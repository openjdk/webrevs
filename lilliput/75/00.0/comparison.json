{"files":[{"patch":"@@ -2316,0 +2316,6 @@\n+    case Op_ExpandBits:\n+    case Op_CompressBits:\n+      if (!(UseSVE > 1 && VM_Version::supports_svebitperm())) {\n+        ret_value = false;\n+      }\n+      break;\n@@ -14116,0 +14122,25 @@\n+\/\/ This pattern is automatically generated from aarch64_ad.m4\n+\/\/ DO NOT EDIT ANYTHING IN THIS SECTION OF THE FILE\n+instruct bits_reverse_I(iRegINoSp dst, iRegIorL2I src)\n+%{\n+  match(Set dst (ReverseI src));\n+  ins_cost(INSN_COST);\n+  format %{ \"rbitw  $dst, $src\" %}\n+  ins_encode %{\n+    __ rbitw($dst$$Register, $src$$Register);\n+  %}\n+  ins_pipe(ialu_reg);\n+%}\n+\n+\/\/ This pattern is automatically generated from aarch64_ad.m4\n+\/\/ DO NOT EDIT ANYTHING IN THIS SECTION OF THE FILE\n+instruct bits_reverse_L(iRegLNoSp dst, iRegL src)\n+%{\n+  match(Set dst (ReverseL src));\n+  ins_cost(INSN_COST);\n+  format %{ \"rbit  $dst, $src\" %}\n+  ins_encode %{\n+    __ rbit($dst$$Register, $src$$Register);\n+  %}\n+  ins_pipe(ialu_reg);\n+%}\n@@ -17385,0 +17416,151 @@\n+\/\/----------------------------- CompressBits\/ExpandBits ------------------------\n+\n+instruct compressBitsI_reg(iRegINoSp dst, iRegIorL2I src, iRegIorL2I mask,\n+                           vRegF tdst, vRegF tsrc, vRegF tmask) %{\n+  match(Set dst (CompressBits src mask));\n+  effect(TEMP tdst, TEMP tsrc, TEMP tmask);\n+  format %{ \"mov    $tsrc, $src\\n\\t\"\n+            \"mov    $tmask, $mask\\n\\t\"\n+            \"bext   $tdst, $tsrc, $tmask\\n\\t\"\n+            \"mov    $dst, $tdst\"\n+          %}\n+  ins_encode %{\n+    __ mov($tsrc$$FloatRegister, __ S, 0, $src$$Register);\n+    __ mov($tmask$$FloatRegister, __ S, 0, $mask$$Register);\n+    __ sve_bext($tdst$$FloatRegister, __ S, $tsrc$$FloatRegister, $tmask$$FloatRegister);\n+    __ mov($dst$$Register, $tdst$$FloatRegister, __ S, 0);\n+  %}\n+  ins_pipe(pipe_slow);\n+%}\n+\n+instruct compressBitsI_memcon(iRegINoSp dst, memory4 mem, immI mask,\n+                           vRegF tdst, vRegF tsrc, vRegF tmask) %{\n+  match(Set dst (CompressBits (LoadI mem) mask));\n+  effect(TEMP tdst, TEMP tsrc, TEMP tmask);\n+  format %{ \"ldrs   $tsrc, $mem\\n\\t\"\n+            \"ldrs   $tmask, $mask\\n\\t\"\n+            \"bext   $tdst, $tsrc, $tmask\\n\\t\"\n+            \"mov    $dst, $tdst\"\n+          %}\n+  ins_encode %{\n+    loadStore(C2_MacroAssembler(&cbuf), &MacroAssembler::ldrs, $tsrc$$FloatRegister, $mem->opcode(),\n+              as_Register($mem$$base), $mem$$index, $mem$$scale, $mem$$disp, 4);\n+    __ ldrs($tmask$$FloatRegister, $constantaddress($mask));\n+    __ sve_bext($tdst$$FloatRegister, __ S, $tsrc$$FloatRegister, $tmask$$FloatRegister);\n+    __ mov($dst$$Register, $tdst$$FloatRegister, __ S, 0);\n+  %}\n+  ins_pipe(pipe_slow);\n+%}\n+\n+instruct compressBitsL_reg(iRegLNoSp dst, iRegL src, iRegL mask,\n+                           vRegD tdst, vRegD tsrc, vRegD tmask) %{\n+  match(Set dst (CompressBits src mask));\n+  effect(TEMP tdst, TEMP tsrc, TEMP tmask);\n+  format %{ \"mov    $tsrc, $src\\n\\t\"\n+            \"mov    $tmask, $mask\\n\\t\"\n+            \"bext   $tdst, $tsrc, $tmask\\n\\t\"\n+            \"mov    $dst, $tdst\"\n+          %}\n+  ins_encode %{\n+    __ mov($tsrc$$FloatRegister, __ D, 0, $src$$Register);\n+    __ mov($tmask$$FloatRegister, __ D, 0, $mask$$Register);\n+    __ sve_bext($tdst$$FloatRegister, __ D, $tsrc$$FloatRegister, $tmask$$FloatRegister);\n+    __ mov($dst$$Register, $tdst$$FloatRegister, __ D, 0);\n+  %}\n+  ins_pipe(pipe_slow);\n+%}\n+\n+instruct compressBitsL_memcon(iRegLNoSp dst, memory8 mem, immL mask,\n+                           vRegF tdst, vRegF tsrc, vRegF tmask) %{\n+  match(Set dst (CompressBits (LoadL mem) mask));\n+  effect(TEMP tdst, TEMP tsrc, TEMP tmask);\n+  format %{ \"ldrd   $tsrc, $mem\\n\\t\"\n+            \"ldrd   $tmask, $mask\\n\\t\"\n+            \"bext   $tdst, $tsrc, $tmask\\n\\t\"\n+            \"mov    $dst, $tdst\"\n+          %}\n+  ins_encode %{\n+    loadStore(C2_MacroAssembler(&cbuf), &MacroAssembler::ldrd, $tsrc$$FloatRegister, $mem->opcode(),\n+              as_Register($mem$$base), $mem$$index, $mem$$scale, $mem$$disp, 8);\n+    __ ldrd($tmask$$FloatRegister, $constantaddress($mask));\n+    __ sve_bext($tdst$$FloatRegister, __ D, $tsrc$$FloatRegister, $tmask$$FloatRegister);\n+    __ mov($dst$$Register, $tdst$$FloatRegister, __ D, 0);\n+  %}\n+  ins_pipe(pipe_slow);\n+%}\n+\n+instruct expandBitsI_reg(iRegINoSp dst, iRegIorL2I src, iRegIorL2I mask,\n+                         vRegF tdst, vRegF tsrc, vRegF tmask) %{\n+  match(Set dst (ExpandBits src mask));\n+  effect(TEMP tdst, TEMP tsrc, TEMP tmask);\n+  format %{ \"mov    $tsrc, $src\\n\\t\"\n+            \"mov    $tmask, $mask\\n\\t\"\n+            \"bdep   $tdst, $tsrc, $tmask\\n\\t\"\n+            \"mov    $dst, $tdst\"\n+          %}\n+  ins_encode %{\n+    __ mov($tsrc$$FloatRegister, __ S, 0, $src$$Register);\n+    __ mov($tmask$$FloatRegister, __ S, 0, $mask$$Register);\n+    __ sve_bdep($tdst$$FloatRegister, __ S, $tsrc$$FloatRegister, $tmask$$FloatRegister);\n+    __ mov($dst$$Register, $tdst$$FloatRegister, __ S, 0);\n+  %}\n+  ins_pipe(pipe_slow);\n+%}\n+\n+instruct expandBitsI_memcon(iRegINoSp dst, memory4 mem, immI mask,\n+                         vRegF tdst, vRegF tsrc, vRegF tmask) %{\n+  match(Set dst (ExpandBits (LoadI mem) mask));\n+  effect(TEMP tdst, TEMP tsrc, TEMP tmask);\n+  format %{ \"ldrs   $tsrc, $mem\\n\\t\"\n+            \"ldrs   $tmask, $mask\\n\\t\"\n+            \"bdep   $tdst, $tsrc, $tmask\\n\\t\"\n+            \"mov    $dst, $tdst\"\n+          %}\n+  ins_encode %{\n+    loadStore(C2_MacroAssembler(&cbuf), &MacroAssembler::ldrs, $tsrc$$FloatRegister, $mem->opcode(),\n+              as_Register($mem$$base), $mem$$index, $mem$$scale, $mem$$disp, 4);\n+    __ ldrs($tmask$$FloatRegister, $constantaddress($mask));\n+    __ sve_bdep($tdst$$FloatRegister, __ S, $tsrc$$FloatRegister, $tmask$$FloatRegister);\n+    __ mov($dst$$Register, $tdst$$FloatRegister, __ S, 0);\n+  %}\n+  ins_pipe(pipe_slow);\n+%}\n+\n+instruct expandBitsL_reg(iRegLNoSp dst, iRegL src, iRegL mask,\n+                         vRegD tdst, vRegD tsrc, vRegD tmask) %{\n+  match(Set dst (ExpandBits src mask));\n+  effect(TEMP tdst, TEMP tsrc, TEMP tmask);\n+  format %{ \"mov    $tsrc, $src\\n\\t\"\n+            \"mov    $tmask, $mask\\n\\t\"\n+            \"bdep   $tdst, $tsrc, $tmask\\n\\t\"\n+            \"mov    $dst, $tdst\"\n+          %}\n+  ins_encode %{\n+    __ mov($tsrc$$FloatRegister, __ D, 0, $src$$Register);\n+    __ mov($tmask$$FloatRegister, __ D, 0, $mask$$Register);\n+    __ sve_bdep($tdst$$FloatRegister, __ D, $tsrc$$FloatRegister, $tmask$$FloatRegister);\n+    __ mov($dst$$Register, $tdst$$FloatRegister, __ D, 0);\n+  %}\n+  ins_pipe(pipe_slow);\n+%}\n+\n+\n+instruct expandBitsL_memcon(iRegINoSp dst, memory8 mem, immL mask,\n+                         vRegF tdst, vRegF tsrc, vRegF tmask) %{\n+  match(Set dst (ExpandBits (LoadL mem) mask));\n+  effect(TEMP tdst, TEMP tsrc, TEMP tmask);\n+  format %{ \"ldrd   $tsrc, $mem\\n\\t\"\n+            \"ldrd   $tmask, $mask\\n\\t\"\n+            \"bdep   $tdst, $tsrc, $tmask\\n\\t\"\n+            \"mov    $dst, $tdst\"\n+          %}\n+  ins_encode %{\n+    loadStore(C2_MacroAssembler(&cbuf), &MacroAssembler::ldrd, $tsrc$$FloatRegister, $mem->opcode(),\n+              as_Register($mem$$base), $mem$$index, $mem$$scale, $mem$$disp, 8);\n+    __ ldrd($tmask$$FloatRegister, $constantaddress($mask));\n+    __ sve_bdep($tdst$$FloatRegister, __ D, $tsrc$$FloatRegister, $tmask$$FloatRegister);\n+    __ mov($dst$$Register, $tdst$$FloatRegister, __ D, 0);\n+  %}\n+  ins_pipe(pipe_slow);\n+%}\n+\n","filename":"src\/hotspot\/cpu\/aarch64\/aarch64.ad","additions":182,"deletions":0,"binary":false,"changes":182,"status":"modified"},{"patch":"@@ -67,12 +67,0 @@\n-RangeCheckStub::RangeCheckStub(CodeEmitInfo* info, LIR_Opr index, LIR_Opr array)\n-  : _index(index), _array(array), _throw_index_out_of_bounds_exception(false) {\n-  assert(info != NULL, \"must have info\");\n-  _info = new CodeEmitInfo(info);\n-}\n-\n-RangeCheckStub::RangeCheckStub(CodeEmitInfo* info, LIR_Opr index)\n-  : _index(index), _array(), _throw_index_out_of_bounds_exception(true) {\n-  assert(info != NULL, \"must have info\");\n-  _info = new CodeEmitInfo(info);\n-}\n-\n@@ -212,8 +200,0 @@\n-\/\/ Implementation of MonitorAccessStubs\n-\n-MonitorEnterStub::MonitorEnterStub(LIR_Opr obj_reg, LIR_Opr lock_reg, CodeEmitInfo* info)\n-: MonitorAccessStub(obj_reg, lock_reg)\n-{\n-  _info = new CodeEmitInfo(info);\n-}\n-\n","filename":"src\/hotspot\/cpu\/aarch64\/c1_CodeStubs_aarch64.cpp","additions":0,"deletions":20,"binary":false,"changes":20,"status":"modified"},{"patch":"@@ -114,1 +114,1 @@\n-    ands(hdr, hdr, aligned_mask - os::vm_page_size());\n+    ands(hdr, hdr, aligned_mask - (int)os::vm_page_size());\n","filename":"src\/hotspot\/cpu\/aarch64\/c1_MacroAssembler_aarch64.cpp","additions":1,"deletions":1,"binary":false,"changes":2,"status":"modified"},{"patch":"@@ -2,1 +2,1 @@\n- * Copyright (c) 2003, 2022, Oracle and\/or its affiliates. All rights reserved.\n+ * Copyright (c) 2003, 2023, Oracle and\/or its affiliates. All rights reserved.\n@@ -810,1 +810,1 @@\n-      ands(swap_reg, swap_reg, (uint64_t) (7 - os::vm_page_size()));\n+      ands(swap_reg, swap_reg, (uint64_t)(7 - (int)os::vm_page_size()));\n","filename":"src\/hotspot\/cpu\/aarch64\/interp_masm_aarch64.cpp","additions":2,"deletions":2,"binary":false,"changes":4,"status":"modified"},{"patch":"@@ -930,2 +930,1 @@\n-  address stub = start_a_stub(NativeInstruction::instruction_size\n-                   + NativeCallTrampolineStub::instruction_size);\n+  address stub = start_a_stub(max_trampoline_stub_size());\n@@ -963,0 +962,5 @@\n+int MacroAssembler::max_trampoline_stub_size() {\n+  \/\/ Max stub size: alignment nop, TrampolineStub.\n+  return NativeInstruction::instruction_size + NativeCallTrampolineStub::instruction_size;\n+}\n+\n@@ -975,0 +979,5 @@\n+int MacroAssembler::static_call_stub_size() {\n+  \/\/ isb; movk; movz; movz; movk; movz; movz; br\n+  return 8 * NativeInstruction::instruction_size;\n+}\n+\n@@ -3598,0 +3607,59 @@\n+void MacroAssembler::kernel_crc32_using_crypto_pmull(Register crc, Register buf,\n+        Register len, Register tmp0, Register tmp1, Register tmp2, Register tmp3) {\n+    Label CRC_by4_loop, CRC_by1_loop, CRC_less128, CRC_by128_pre, CRC_by32_loop, CRC_less32, L_exit;\n+    assert_different_registers(crc, buf, len, tmp0, tmp1, tmp2);\n+\n+    subs(tmp0, len, 384);\n+    mvnw(crc, crc);\n+    br(Assembler::GE, CRC_by128_pre);\n+  BIND(CRC_less128);\n+    subs(len, len, 32);\n+    br(Assembler::GE, CRC_by32_loop);\n+  BIND(CRC_less32);\n+    adds(len, len, 32 - 4);\n+    br(Assembler::GE, CRC_by4_loop);\n+    adds(len, len, 4);\n+    br(Assembler::GT, CRC_by1_loop);\n+    b(L_exit);\n+\n+  BIND(CRC_by32_loop);\n+    ldp(tmp0, tmp1, Address(buf));\n+    crc32x(crc, crc, tmp0);\n+    ldp(tmp2, tmp3, Address(buf, 16));\n+    crc32x(crc, crc, tmp1);\n+    add(buf, buf, 32);\n+    crc32x(crc, crc, tmp2);\n+    subs(len, len, 32);\n+    crc32x(crc, crc, tmp3);\n+    br(Assembler::GE, CRC_by32_loop);\n+    cmn(len, (u1)32);\n+    br(Assembler::NE, CRC_less32);\n+    b(L_exit);\n+\n+  BIND(CRC_by4_loop);\n+    ldrw(tmp0, Address(post(buf, 4)));\n+    subs(len, len, 4);\n+    crc32w(crc, crc, tmp0);\n+    br(Assembler::GE, CRC_by4_loop);\n+    adds(len, len, 4);\n+    br(Assembler::LE, L_exit);\n+  BIND(CRC_by1_loop);\n+    ldrb(tmp0, Address(post(buf, 1)));\n+    subs(len, len, 1);\n+    crc32b(crc, crc, tmp0);\n+    br(Assembler::GT, CRC_by1_loop);\n+    b(L_exit);\n+\n+  BIND(CRC_by128_pre);\n+    kernel_crc32_common_fold_using_crypto_pmull(crc, buf, len, tmp0, tmp1, tmp2,\n+      4*256*sizeof(juint) + 8*sizeof(juint));\n+    mov(crc, 0);\n+    crc32x(crc, crc, tmp0);\n+    crc32x(crc, crc, tmp1);\n+\n+    cbnz(len, CRC_less128);\n+\n+  BIND(L_exit);\n+    mvnw(crc, crc);\n+}\n+\n@@ -3709,0 +3777,5 @@\n+  if (UseCryptoPmullForCRC32) {\n+      kernel_crc32_using_crypto_pmull(crc, buf, len, table0, table1, table2, table3);\n+      return;\n+  }\n+\n@@ -4008,0 +4081,117 @@\n+void MacroAssembler::kernel_crc32_common_fold_using_crypto_pmull(Register crc, Register buf,\n+        Register len, Register tmp0, Register tmp1, Register tmp2, size_t table_offset) {\n+    Label CRC_by128_loop;\n+    assert_different_registers(crc, buf, len, tmp0, tmp1, tmp2);\n+\n+    sub(len, len, 256);\n+    Register table = tmp0;\n+    {\n+      uint64_t offset;\n+      adrp(table, ExternalAddress(StubRoutines::crc_table_addr()), offset);\n+      add(table, table, offset);\n+    }\n+    add(table, table, table_offset);\n+\n+    sub(buf, buf, 0x10);\n+    ldrq(v1, Address(buf, 0x10));\n+    ldrq(v2, Address(buf, 0x20));\n+    ldrq(v3, Address(buf, 0x30));\n+    ldrq(v4, Address(buf, 0x40));\n+    ldrq(v5, Address(buf, 0x50));\n+    ldrq(v6, Address(buf, 0x60));\n+    ldrq(v7, Address(buf, 0x70));\n+    ldrq(v8, Address(pre(buf, 0x80)));\n+\n+    movi(v25, T4S, 0);\n+    mov(v25, S, 0, crc);\n+    eor(v1, T16B, v1, v25);\n+\n+    ldrq(v0, Address(table));\n+    b(CRC_by128_loop);\n+\n+    align(OptoLoopAlignment);\n+  BIND(CRC_by128_loop);\n+    pmull (v9,  T1Q, v1, v0, T1D);\n+    pmull2(v10, T1Q, v1, v0, T2D);\n+    ldrq(v1, Address(buf, 0x10));\n+    eor3(v1, T16B, v9,  v10, v1);\n+\n+    pmull (v11, T1Q, v2, v0, T1D);\n+    pmull2(v12, T1Q, v2, v0, T2D);\n+    ldrq(v2, Address(buf, 0x20));\n+    eor3(v2, T16B, v11, v12, v2);\n+\n+    pmull (v13, T1Q, v3, v0, T1D);\n+    pmull2(v14, T1Q, v3, v0, T2D);\n+    ldrq(v3, Address(buf, 0x30));\n+    eor3(v3, T16B, v13, v14, v3);\n+\n+    pmull (v15, T1Q, v4, v0, T1D);\n+    pmull2(v16, T1Q, v4, v0, T2D);\n+    ldrq(v4, Address(buf, 0x40));\n+    eor3(v4, T16B, v15, v16, v4);\n+\n+    pmull (v17, T1Q, v5, v0, T1D);\n+    pmull2(v18, T1Q, v5, v0, T2D);\n+    ldrq(v5, Address(buf, 0x50));\n+    eor3(v5, T16B, v17, v18, v5);\n+\n+    pmull (v19, T1Q, v6, v0, T1D);\n+    pmull2(v20, T1Q, v6, v0, T2D);\n+    ldrq(v6, Address(buf, 0x60));\n+    eor3(v6, T16B, v19, v20, v6);\n+\n+    pmull (v21, T1Q, v7, v0, T1D);\n+    pmull2(v22, T1Q, v7, v0, T2D);\n+    ldrq(v7, Address(buf, 0x70));\n+    eor3(v7, T16B, v21, v22, v7);\n+\n+    pmull (v23, T1Q, v8, v0, T1D);\n+    pmull2(v24, T1Q, v8, v0, T2D);\n+    ldrq(v8, Address(pre(buf, 0x80)));\n+    eor3(v8, T16B, v23, v24, v8);\n+\n+    subs(len, len, 0x80);\n+    br(Assembler::GE, CRC_by128_loop);\n+\n+    \/\/ fold into 512 bits\n+    ldrq(v0, Address(table, 0x10));\n+\n+    pmull (v10,  T1Q, v1, v0, T1D);\n+    pmull2(v11, T1Q, v1, v0, T2D);\n+    eor3(v1, T16B, v10, v11, v5);\n+\n+    pmull (v12, T1Q, v2, v0, T1D);\n+    pmull2(v13, T1Q, v2, v0, T2D);\n+    eor3(v2, T16B, v12, v13, v6);\n+\n+    pmull (v14, T1Q, v3, v0, T1D);\n+    pmull2(v15, T1Q, v3, v0, T2D);\n+    eor3(v3, T16B, v14, v15, v7);\n+\n+    pmull (v16, T1Q, v4, v0, T1D);\n+    pmull2(v17, T1Q, v4, v0, T2D);\n+    eor3(v4, T16B, v16, v17, v8);\n+\n+    \/\/ fold into 128 bits\n+    ldrq(v5, Address(table, 0x20));\n+    pmull (v10, T1Q, v1, v5, T1D);\n+    pmull2(v11, T1Q, v1, v5, T2D);\n+    eor3(v4, T16B, v4, v10, v11);\n+\n+    ldrq(v6, Address(table, 0x30));\n+    pmull (v12, T1Q, v2, v6, T1D);\n+    pmull2(v13, T1Q, v2, v6, T2D);\n+    eor3(v4, T16B, v4, v12, v13);\n+\n+    ldrq(v7, Address(table, 0x40));\n+    pmull (v14, T1Q, v3, v7, T1D);\n+    pmull2(v15, T1Q, v3, v7, T2D);\n+    eor3(v1, T16B, v4, v14, v15);\n+\n+    add(len, len, 0x80);\n+    add(buf, buf, 0x10);\n+\n+    mov(tmp0, v1, D, 0);\n+    mov(tmp1, v1, D, 1);\n+}\n@@ -4092,0 +4282,5 @@\n+void MacroAssembler::load_klass_check_null(Register dst, Register src) {\n+  null_check(src, oopDesc::klass_offset_in_bytes());\n+  load_klass(dst, src);\n+}\n+\n@@ -4572,1 +4767,1 @@\n-  mov(rscratch1, os::vm_page_size());\n+  mov(rscratch1, (int)os::vm_page_size());\n@@ -4574,1 +4769,1 @@\n-  lea(tmp, Address(tmp, -os::vm_page_size()));\n+  lea(tmp, Address(tmp, -(int)os::vm_page_size()));\n@@ -4585,1 +4780,1 @@\n-  for (int i = 0; i < (int)(StackOverflow::stack_shadow_zone_size() \/ os::vm_page_size()) - 1; i++) {\n+  for (int i = 0; i < (int)(StackOverflow::stack_shadow_zone_size() \/ (int)os::vm_page_size()) - 1; i++) {\n@@ -4588,1 +4783,1 @@\n-    lea(tmp, Address(tmp, -os::vm_page_size()));\n+    lea(tmp, Address(tmp, -(int)os::vm_page_size()));\n","filename":"src\/hotspot\/cpu\/aarch64\/macroAssembler_aarch64.cpp","additions":201,"deletions":6,"binary":false,"changes":207,"status":"modified"},{"patch":"@@ -651,0 +651,1 @@\n+  static int max_trampoline_stub_size();\n@@ -652,0 +653,1 @@\n+  static int static_call_stub_size();\n@@ -855,0 +857,1 @@\n+  void load_klass_check_null(Register dst, Register src);\n@@ -1431,0 +1434,3 @@\n+  void kernel_crc32_using_crypto_pmull(Register crc, Register buf,\n+        Register len, Register tmp0, Register tmp1, Register tmp2,\n+        Register tmp3);\n@@ -1437,0 +1443,3 @@\n+  void kernel_crc32_common_fold_using_crypto_pmull(Register crc, Register buf,\n+        Register len, Register tmp0, Register tmp1, Register tmp2,\n+        size_t table_offset);\n","filename":"src\/hotspot\/cpu\/aarch64\/macroAssembler_aarch64.hpp","additions":9,"deletions":0,"binary":false,"changes":9,"status":"modified"},{"patch":"@@ -1805,1 +1805,1 @@\n-        __ ands(swap_reg, swap_reg, 3 - os::vm_page_size());\n+        __ ands(swap_reg, swap_reg, 3 - (int)os::vm_page_size());\n","filename":"src\/hotspot\/cpu\/aarch64\/sharedRuntime_aarch64.cpp","additions":1,"deletions":1,"binary":false,"changes":2,"status":"modified"},{"patch":"@@ -571,33 +571,2 @@\n-#if INCLUDE_ZGC\n-    if (UseZGC) {\n-      \/\/ Check if mask is good.\n-      \/\/ verifies that ZAddressBadMask & r0 == 0\n-      __ ldr(c_rarg3, Address(rthread, ZThreadLocalData::address_bad_mask_offset()));\n-      __ andr(c_rarg2, r0, c_rarg3);\n-      __ cbnz(c_rarg2, error);\n-    }\n-#endif\n-\n-    \/\/ Check if the oop is in the right area of memory\n-    __ mov(c_rarg3, (intptr_t) Universe::verify_oop_mask());\n-    __ andr(c_rarg2, r0, c_rarg3);\n-    __ mov(c_rarg3, (intptr_t) Universe::verify_oop_bits());\n-\n-    \/\/ Compare c_rarg2 and c_rarg3.  We don't use a compare\n-    \/\/ instruction here because the flags register is live.\n-    __ eor(c_rarg2, c_rarg2, c_rarg3);\n-    __ cbnz(c_rarg2, error);\n-\n-    \/\/ make sure klass is 'reasonable', which is not zero.\n-    \/\/ NOTE: We used to load the Klass* here, and compare that to zero.\n-    \/\/ However, with current Lilliput implementation, that would require\n-    \/\/ checking the locking bits and calling into the runtime, which\n-    \/\/ clobbers the condition flags, which may be live around this call.\n-    \/\/ OTOH, this is a simple NULL-check, and we can simply load the upper\n-    \/\/ 32bit of the header as narrowKlass, and compare that to 0. The\n-    \/\/ worst that can happen (rarely) is that the object is locked and\n-    \/\/ we have lock pointer bits in the upper 32bits. We can't get a false\n-    \/\/ negative.\n-    assert(oopDesc::klass_offset_in_bytes() % 4 == 0, \"must be 4 byte aligned\");\n-    __ ldrw(r0, Address(r0, oopDesc::klass_offset_in_bytes()));  \/\/ get klass\n-    __ cbzw(r0, error);      \/\/ if klass is NULL it is broken\n+    BarrierSetAssembler* bs_asm = BarrierSet::barrier_set()->barrier_set_assembler();\n+    bs_asm->check_oop(_masm, r0, c_rarg2, c_rarg3, error);\n","filename":"src\/hotspot\/cpu\/aarch64\/stubGenerator_aarch64.cpp","additions":2,"deletions":33,"binary":false,"changes":35,"status":"modified"},{"patch":"@@ -2,1 +2,1 @@\n- * Copyright (c) 2003, 2022, Oracle and\/or its affiliates. All rights reserved.\n+ * Copyright (c) 2003, 2023, Oracle and\/or its affiliates. All rights reserved.\n@@ -294,0 +294,12 @@\n+\n+    \/\/ Constants for CRC-32 crypto pmull implementation\n+    0xe88ef372UL, 0x00000001UL,\n+    0x4a7fe880UL, 0x00000001UL,\n+    0x54442bd4UL, 0x00000001UL,\n+    0xc6e41596UL, 0x00000001UL,\n+    0x3db1ecdcUL, 0x00000000UL,\n+    0x74359406UL, 0x00000001UL,\n+    0xf1da05aaUL, 0x00000000UL,\n+    0x5a546366UL, 0x00000001UL,\n+    0x751997d0UL, 0x00000001UL,\n+    0xccaa009eUL, 0x00000000UL,\n","filename":"src\/hotspot\/cpu\/aarch64\/stubRoutines_aarch64.cpp","additions":13,"deletions":1,"binary":false,"changes":14,"status":"modified"},{"patch":"@@ -3239,2 +3239,1 @@\n-  __ null_check(recv, oopDesc::mark_offset_in_bytes());\n-  __ load_klass(r0, recv);\n+  __ load_klass_check_null(r0, recv);\n@@ -3329,2 +3328,1 @@\n-  __ null_check(r2, oopDesc::mark_offset_in_bytes());\n-  __ load_klass(r3, r2);\n+  __ load_klass_check_null(r3, r2);\n@@ -3346,2 +3344,1 @@\n-  __ null_check(r2, oopDesc::mark_offset_in_bytes());\n-  __ load_klass(r3, r2);\n+  __ load_klass_check_null(r3, r2);\n","filename":"src\/hotspot\/cpu\/aarch64\/templateTable_aarch64.cpp","additions":3,"deletions":6,"binary":false,"changes":9,"status":"modified"},{"patch":"@@ -2,1 +2,1 @@\n- * Copyright (c) 2008, 2020, Oracle and\/or its affiliates. All rights reserved.\n+ * Copyright (c) 2008, 2023, Oracle and\/or its affiliates. All rights reserved.\n@@ -56,16 +56,0 @@\n-\n-\/\/ TODO: ARM - is it possible to inline these stubs into the main code stream?\n-\n-\n-RangeCheckStub::RangeCheckStub(CodeEmitInfo* info, LIR_Opr index, LIR_Opr array)\n-  : _index(index), _array(array), _throw_index_out_of_bounds_exception(false) {\n-  assert(info != NULL, \"must have info\");\n-  _info = new CodeEmitInfo(info);\n-}\n-\n-RangeCheckStub::RangeCheckStub(CodeEmitInfo* info, LIR_Opr index)\n-  : _index(index), _array(), _throw_index_out_of_bounds_exception(true) {\n-  assert(info != NULL, \"must have info\");\n-  _info = new CodeEmitInfo(info);\n-}\n-\n@@ -195,10 +179,0 @@\n-\n-\/\/ Implementation of MonitorAccessStubs\n-\n-MonitorEnterStub::MonitorEnterStub(LIR_Opr obj_reg, LIR_Opr lock_reg, CodeEmitInfo* info)\n-: MonitorAccessStub(obj_reg, lock_reg)\n-{\n-  _info = new CodeEmitInfo(info);\n-}\n-\n-\n@@ -352,1 +326,1 @@\n-  address target = NULL;\n+  address target = nullptr;\n","filename":"src\/hotspot\/cpu\/arm\/c1_CodeStubs_arm.cpp","additions":2,"deletions":28,"binary":false,"changes":30,"status":"modified"},{"patch":"@@ -2,1 +2,1 @@\n- * Copyright (c) 2008, 2022, Oracle and\/or its affiliates. All rights reserved.\n+ * Copyright (c) 2008, 2023, Oracle and\/or its affiliates. All rights reserved.\n@@ -203,1 +203,1 @@\n-  if (handler_base == NULL) {\n+  if (handler_base == nullptr) {\n@@ -243,1 +243,1 @@\n-  MonitorExitStub* stub = NULL;\n+  MonitorExitStub* stub = nullptr;\n@@ -256,1 +256,1 @@\n-  if (stub != NULL) {\n+  if (stub != nullptr) {\n@@ -266,1 +266,1 @@\n-  if (handler_base == NULL) {\n+  if (handler_base == nullptr) {\n@@ -405,1 +405,1 @@\n-  assert((src->as_constant_ptr()->type() == T_OBJECT && src->as_constant_ptr()->as_jobject() == NULL),\"cannot handle otherwise\");\n+  assert((src->as_constant_ptr()->type() == T_OBJECT && src->as_constant_ptr()->as_jobject() == nullptr),\"cannot handle otherwise\");\n@@ -411,1 +411,1 @@\n-  if (info != NULL) {\n+  if (info != nullptr) {\n@@ -499,1 +499,1 @@\n-  PatchingStub* patch = NULL;\n+  PatchingStub* patch = nullptr;\n@@ -550,1 +550,1 @@\n-        if (patch != NULL) {\n+        if (patch != nullptr) {\n@@ -559,1 +559,1 @@\n-        if (patch != NULL) {\n+        if (patch != nullptr) {\n@@ -568,1 +568,1 @@\n-        if (patch != NULL) {\n+        if (patch != nullptr) {\n@@ -608,1 +608,1 @@\n-  if (info != NULL) {\n+  if (info != nullptr) {\n@@ -612,1 +612,1 @@\n-  if (patch != NULL) {\n+  if (patch != nullptr) {\n@@ -701,1 +701,1 @@\n-  PatchingStub* patch = NULL;\n+  PatchingStub* patch = nullptr;\n@@ -705,1 +705,1 @@\n-  if (info != NULL) {\n+  if (info != nullptr) {\n@@ -759,1 +759,1 @@\n-        if (patch != NULL) {\n+        if (patch != nullptr) {\n@@ -768,1 +768,1 @@\n-        if (patch != NULL) {\n+        if (patch != nullptr) {\n@@ -777,1 +777,1 @@\n-        if (patch != NULL) {\n+        if (patch != nullptr) {\n@@ -817,1 +817,1 @@\n-  if (patch != NULL) {\n+  if (patch != nullptr) {\n@@ -864,4 +864,4 @@\n-  assert(op->block() == NULL || op->block()->label() == op->label(), \"wrong label\");\n-  if (op->block() != NULL)  _branch_target_blocks.append(op->block());\n-  if (op->ublock() != NULL) _branch_target_blocks.append(op->ublock());\n-  assert(op->info() == NULL, \"CodeEmitInfo?\");\n+  assert(op->block() == nullptr || op->block()->label() == op->label(), \"wrong label\");\n+  if (op->block() != nullptr)  _branch_target_blocks.append(op->block());\n+  if (op->ublock() != nullptr) _branch_target_blocks.append(op->ublock());\n+  assert(op->info() == nullptr, \"CodeEmitInfo?\");\n@@ -1024,1 +1024,1 @@\n-  assert(md != NULL, \"Sanity\");\n+  assert(md != nullptr, \"Sanity\");\n@@ -1026,1 +1026,1 @@\n-  assert(data != NULL,       \"need data for checkcast\");\n+  assert(data != nullptr,       \"need data for checkcast\");\n@@ -1039,1 +1039,1 @@\n-  assert(method != NULL, \"Should have method\");\n+  assert(method != nullptr, \"Should have method\");\n@@ -1514,1 +1514,1 @@\n-  assert(info == NULL, \"unused on this code path\");\n+  assert(info == nullptr, \"unused on this code path\");\n@@ -1791,1 +1791,1 @@\n-          assert(opr2->as_constant_ptr()->as_jobject() == NULL, \"cannot handle otherwise\");\n+          assert(opr2->as_constant_ptr()->as_jobject() == nullptr, \"cannot handle otherwise\");\n@@ -1796,1 +1796,1 @@\n-          assert(opr2->as_constant_ptr()->as_metadata() == NULL, \"cannot handle otherwise\");\n+          assert(opr2->as_constant_ptr()->as_metadata() == nullptr, \"cannot handle otherwise\");\n@@ -1944,1 +1944,1 @@\n-  if (stub == NULL) {\n+  if (stub == nullptr) {\n@@ -1950,1 +1950,1 @@\n-  InlinedMetadata metadata_literal(NULL);\n+  InlinedMetadata metadata_literal(nullptr);\n@@ -2104,1 +2104,1 @@\n-  BasicType basic_type = default_type != NULL ? default_type->element_type()->basic_type() : T_ILLEGAL;\n+  BasicType basic_type = default_type != nullptr ? default_type->element_type()->basic_type() : T_ILLEGAL;\n@@ -2108,1 +2108,1 @@\n-  if (default_type == NULL) {\n+  if (default_type == nullptr) {\n@@ -2117,1 +2117,1 @@\n-    assert(copyfunc_addr != NULL, \"generic arraycopy stub required\");\n+    assert(copyfunc_addr != nullptr, \"generic arraycopy stub required\");\n@@ -2142,1 +2142,1 @@\n-  assert(default_type != NULL && default_type->is_array_klass() && default_type->is_loaded(),\n+  assert(default_type != nullptr && default_type->is_array_klass() && default_type->is_loaded(),\n@@ -2147,1 +2147,1 @@\n-  \/\/ Check for NULL\n+  \/\/ Check for null\n@@ -2255,1 +2255,1 @@\n-      __ check_klass_subtype_fast_path(tmp, tmp2, R6, altFP_7_11, &cont, copyfunc_addr == NULL ? stub->entry() : &slow, NULL);\n+      __ check_klass_subtype_fast_path(tmp, tmp2, R6, altFP_7_11, &cont, copyfunc_addr == nullptr ? stub->entry() : &slow, nullptr);\n@@ -2266,1 +2266,1 @@\n-      if (copyfunc_addr != NULL) { \/\/ use stub if available\n+      if (copyfunc_addr != nullptr) { \/\/ use stub if available\n@@ -2435,1 +2435,1 @@\n-    if (op->info() != NULL) {\n+    if (op->info() != nullptr) {\n@@ -2443,1 +2443,1 @@\n-    if (op->info() != NULL) {\n+    if (op->info() != nullptr) {\n@@ -2459,1 +2459,1 @@\n-  if (info != NULL) {\n+  if (info != nullptr) {\n@@ -2477,1 +2477,1 @@\n-  assert(md != NULL, \"Sanity\");\n+  assert(md != nullptr, \"Sanity\");\n@@ -2479,1 +2479,1 @@\n-  assert(data != NULL && data->is_CounterData(), \"need CounterData for calls\");\n+  assert(data != nullptr && data->is_CounterData(), \"need CounterData for calls\");\n@@ -2505,1 +2505,1 @@\n-    if (C1OptimizeVirtualCallProfiling && known_klass != NULL) {\n+    if (C1OptimizeVirtualCallProfiling && known_klass != nullptr) {\n@@ -2534,1 +2534,1 @@\n-        if (receiver == NULL) {\n+        if (receiver == nullptr) {\n@@ -2637,1 +2637,1 @@\n-  if (info != NULL) {\n+  if (info != nullptr) {\n@@ -2682,1 +2682,1 @@\n-  if (info != NULL) {\n+  if (info != nullptr) {\n@@ -2749,1 +2749,1 @@\n-        assert(cmp != NULL, \"cmp LIR instruction is not an op2\");\n+        assert(cmp != nullptr, \"cmp LIR instruction is not an op2\");\n@@ -2753,1 +2753,1 @@\n-          if (cmove != NULL && cmove->code() == lir_cmove) {\n+          if (cmove != nullptr && cmove->code() == lir_cmove) {\n","filename":"src\/hotspot\/cpu\/arm\/c1_LIRAssembler_arm.cpp","additions":49,"deletions":49,"binary":false,"changes":98,"status":"modified"},{"patch":"@@ -2,1 +2,1 @@\n- * Copyright (c) 2008, 2022, Oracle and\/or its affiliates. All rights reserved.\n+ * Copyright (c) 2008, 2023, Oracle and\/or its affiliates. All rights reserved.\n@@ -265,1 +265,1 @@\n-  \/\/ If hdr is NULL, we've got recursive locking and there's nothing more to do\n+  \/\/ If hdr is null, we've got recursive locking and there's nothing more to do\n","filename":"src\/hotspot\/cpu\/arm\/c1_MacroAssembler_arm.cpp","additions":2,"deletions":2,"binary":false,"changes":4,"status":"modified"},{"patch":"@@ -2,1 +2,1 @@\n- * Copyright (c) 2008, 2019, Oracle and\/or its affiliates. All rights reserved.\n+ * Copyright (c) 2008, 2023, Oracle and\/or its affiliates. All rights reserved.\n@@ -67,1 +67,1 @@\n-  void null_check(Register r, Label *Lnull = NULL) { MacroAssembler::null_check(r); }\n+  void null_check(Register r, Label *Lnull = nullptr) { MacroAssembler::null_check(r); }\n","filename":"src\/hotspot\/cpu\/arm\/c1_MacroAssembler_arm.hpp","additions":2,"deletions":2,"binary":false,"changes":4,"status":"modified"},{"patch":"@@ -67,12 +67,0 @@\n-RangeCheckStub::RangeCheckStub(CodeEmitInfo* info, LIR_Opr index, LIR_Opr array)\n-  : _index(index), _array(array), _throw_index_out_of_bounds_exception(false) {\n-  assert(info != NULL, \"must have info\");\n-  _info = new CodeEmitInfo(info);\n-}\n-\n-RangeCheckStub::RangeCheckStub(CodeEmitInfo* info, LIR_Opr index)\n-  : _index(index), _array(), _throw_index_out_of_bounds_exception(true) {\n-  assert(info != NULL, \"must have info\");\n-  _info = new CodeEmitInfo(info);\n-}\n-\n@@ -285,7 +273,0 @@\n-\n-\/\/ Implementation of MonitorAccessStubs\n-MonitorEnterStub::MonitorEnterStub(LIR_Opr obj_reg, LIR_Opr lock_reg, CodeEmitInfo* info)\n-  : MonitorAccessStub(obj_reg, lock_reg) {\n-  _info = new CodeEmitInfo(info);\n-}\n-\n","filename":"src\/hotspot\/cpu\/ppc\/c1_CodeStubs_ppc.cpp","additions":0,"deletions":19,"binary":false,"changes":19,"status":"modified"},{"patch":"@@ -734,1 +734,4 @@\n-            __ verify_oop(from_reg->as_register(), FILE_AND_LINE);\n+            if (VerifyOops) {\n+              BarrierSetAssembler* bs = BarrierSet::barrier_set()->barrier_set_assembler();\n+              bs->check_oop(_masm, from_reg->as_register(), FILE_AND_LINE); \/\/ kills R0\n+            }\n@@ -775,1 +778,4 @@\n-          __ verify_oop(from_reg->as_register(), FILE_AND_LINE); \/\/ kills R0\n+          if (VerifyOops) {\n+            BarrierSetAssembler* bs = BarrierSet::barrier_set()->barrier_set_assembler();\n+            bs->check_oop(_masm, from_reg->as_register(), FILE_AND_LINE); \/\/ kills R0\n+          }\n@@ -816,1 +822,4 @@\n-          __ verify_oop(to_reg->as_register(), FILE_AND_LINE);\n+          if (VerifyOops) {\n+            BarrierSetAssembler* bs = BarrierSet::barrier_set()->barrier_set_assembler();\n+            bs->check_oop(_masm, to_reg->as_register(), FILE_AND_LINE); \/\/ kills R0\n+          }\n@@ -847,1 +856,4 @@\n-        __ verify_oop(to_reg->as_register(), FILE_AND_LINE);\n+        if (VerifyOops) {\n+          BarrierSetAssembler* bs = BarrierSet::barrier_set()->barrier_set_assembler();\n+          bs->check_oop(_masm, to_reg->as_register(), FILE_AND_LINE); \/\/ kills R0\n+        }\n","filename":"src\/hotspot\/cpu\/ppc\/c1_LIRAssembler_ppc.cpp","additions":16,"deletions":4,"binary":false,"changes":20,"status":"modified"},{"patch":"@@ -2,1 +2,1 @@\n- * Copyright (c) 1999, 2021, Oracle and\/or its affiliates. All rights reserved.\n+ * Copyright (c) 1999, 2023, Oracle and\/or its affiliates. All rights reserved.\n@@ -48,2 +48,1 @@\n-  MacroAssembler::null_check(receiver, oopDesc::klass_offset_in_bytes(), &Lmiss);\n-  load_klass(temp_reg, receiver);\n+  load_klass_check_null(temp_reg, receiver, &Lmiss);\n","filename":"src\/hotspot\/cpu\/ppc\/c1_MacroAssembler_ppc.cpp","additions":2,"deletions":3,"binary":false,"changes":5,"status":"modified"},{"patch":"@@ -2962,2 +2962,0 @@\n-  verify_thread();\n-\n@@ -3064,0 +3062,5 @@\n+void MacroAssembler::load_klass_check_null(Register dst, Register src, Label* is_null) {\n+  null_check(src, oopDesc::klass_offset_in_bytes(), is_null);\n+  load_klass(dst, src);\n+}\n+\n@@ -4234,6 +4237,0 @@\n-void MacroAssembler::verify_thread() {\n-  if (VerifyThread) {\n-    unimplemented(\"'VerifyThread' currently not implemented on PPC\");\n-  }\n-}\n-\n","filename":"src\/hotspot\/cpu\/ppc\/macroAssembler_ppc.cpp","additions":5,"deletions":8,"binary":false,"changes":13,"status":"modified"},{"patch":"@@ -453,1 +453,1 @@\n-      __ mvw(dest->as_register(), c->as_jint());\n+      __ mv(dest->as_register(), c->as_jint());\n@@ -521,1 +521,1 @@\n-        __ mvw(t1, c->as_jint_bits());\n+        __ mv(t1, c->as_jint_bits());\n@@ -1004,1 +1004,1 @@\n-    __ mvw(t1, InstanceKlass::fully_initialized);\n+    __ mv(t1, (u1)InstanceKlass::fully_initialized);\n","filename":"src\/hotspot\/cpu\/riscv\/c1_LIRAssembler_riscv.cpp","additions":3,"deletions":3,"binary":false,"changes":6,"status":"modified"},{"patch":"@@ -2,1 +2,1 @@\n- * Copyright (c) 1999, 2022, Oracle and\/or its affiliates. All rights reserved.\n+ * Copyright (c) 1999, 2023, Oracle and\/or its affiliates. All rights reserved.\n@@ -80,1 +80,1 @@\n-    ori(hdr, hdr, markWord::unlocked_value);\n+    jori(hdr, hdr, markWord::unlocked_value);\n@@ -103,1 +103,1 @@\n-    mv(t0, aligned_mask - os::vm_page_size());\n+    mv(t0, aligned_mask - (int)os::vm_page_size());\n","filename":"src\/hotspot\/cpu\/riscv\/c1_MacroAssembler_riscv.cpp","additions":3,"deletions":3,"binary":false,"changes":6,"status":"modified"},{"patch":"@@ -2,1 +2,1 @@\n- * Copyright (c) 2003, 2022, Oracle and\/or its affiliates. All rights reserved.\n+ * Copyright (c) 2003, 2023, Oracle and\/or its affiliates. All rights reserved.\n@@ -149,1 +149,1 @@\n-  mvw(t0, (int) ilgl);\n+  mv(t0, (int)ilgl);\n@@ -839,1 +839,1 @@\n-      mv(t0, (int64_t)(7 - os::vm_page_size()));\n+      mv(t0, (int64_t)(7 - (int)os::vm_page_size()));\n@@ -1526,2 +1526,2 @@\n-    mvw(reg2, in_bytes(MultiBranchData::per_case_size()));\n-    mvw(t0, in_bytes(MultiBranchData::case_array_offset()));\n+    mv(reg2, in_bytes(MultiBranchData::per_case_size()));\n+    mv(t0, in_bytes(MultiBranchData::case_array_offset()));\n","filename":"src\/hotspot\/cpu\/riscv\/interp_masm_riscv.cpp","additions":5,"deletions":5,"binary":false,"changes":10,"status":"modified"},{"patch":"@@ -4,1 +4,1 @@\n- * Copyright (c) 2020, 2022, Huawei Technologies Co., Ltd. All rights reserved.\n+ * Copyright (c) 2020, 2023, Huawei Technologies Co., Ltd. All rights reserved.\n@@ -212,0 +212,1 @@\n+    InlineSkippedInstructionsCounter skipCounter(this);\n@@ -1061,1 +1062,1 @@\n-  INSN(rdinstret,  CSR_INSTERT);\n+  INSN(rdinstret,  CSR_INSTRET);\n@@ -1935,1 +1936,1 @@\n-  mv(t0, os::vm_page_size());\n+  mv(t0, (int)os::vm_page_size());\n@@ -1949,1 +1950,1 @@\n-  for (int i = 0; i < (int)(StackOverflow::stack_shadow_zone_size() \/ os::vm_page_size()) - 1; i++) {\n+  for (int i = 0; i < (int)(StackOverflow::stack_shadow_zone_size() \/ (int)os::vm_page_size()) - 1; i++) {\n@@ -1952,1 +1953,1 @@\n-    sub(tmp, tmp, os::vm_page_size());\n+    sub(tmp, tmp, (int)os::vm_page_size());\n@@ -2081,0 +2082,5 @@\n+void MacroAssembler::load_klass_check_null(Register dst, Register src, Register tmp) {\n+  null_check(src, oopDesc::klass_offset_in_bytes());\n+  load_klass(dst, src, tmp);\n+}\n+\n@@ -2152,1 +2158,1 @@\n-  if (((uint64_t)(uintptr_t)CompressedKlassPointers::base() & 0xffffffff) == 0 &&\n+  if (((uint64_t)CompressedKlassPointers::base() & 0xffffffff) == 0 &&\n@@ -2164,1 +2170,1 @@\n-  mv(xbase, (intptr_t)CompressedKlassPointers::base());\n+  mv(xbase, (uintptr_t)CompressedKlassPointers::base());\n@@ -2172,1 +2178,1 @@\n-void  MacroAssembler::decode_heap_oop_not_null(Register r) {\n+void MacroAssembler::decode_heap_oop_not_null(Register r) {\n@@ -3144,2 +3150,2 @@\n-  address stub = start_a_stub(NativeInstruction::instruction_size\n-                            + NativeCallTrampolineStub::instruction_size);\n+  \/\/ Max stub size: alignment nop, TrampolineStub.\n+  address stub = start_a_stub(max_trampoline_stub_size());\n@@ -3185,0 +3191,10 @@\n+int MacroAssembler::max_trampoline_stub_size() {\n+  \/\/ Max stub size: alignment nop, TrampolineStub.\n+  return NativeInstruction::instruction_size + NativeCallTrampolineStub::instruction_size;\n+}\n+\n+int MacroAssembler::static_call_stub_size() {\n+  \/\/ (lui, addi, slli, addi, slli, addi) + (lui, addi, slli, addi, slli) + jalr\n+  return 12 * NativeInstruction::instruction_size;\n+}\n+\n","filename":"src\/hotspot\/cpu\/riscv\/macroAssembler_riscv.cpp","additions":26,"deletions":10,"binary":false,"changes":36,"status":"modified"},{"patch":"@@ -198,0 +198,1 @@\n+  void load_klass_check_null(Register dst, Register src, Register tmp = t0);\n@@ -415,0 +416,1 @@\n+  static int max_trampoline_stub_size();\n@@ -416,0 +418,1 @@\n+  static int static_call_stub_size();\n@@ -692,1 +695,1 @@\n-  void li(Register Rd, int64_t imm);  \/\/ optimized load immediate\n+  void li  (Register Rd, int64_t imm);  \/\/ optimized load immediate\n@@ -706,2 +709,0 @@\n-  inline void mvw(Register Rd, int32_t imm32)         { mv(Rd, imm32); }\n-\n","filename":"src\/hotspot\/cpu\/riscv\/macroAssembler_riscv.hpp","additions":4,"deletions":3,"binary":false,"changes":7,"status":"modified"},{"patch":"@@ -947,2 +947,2 @@\n-  int_def FMUL_SINGLE_COST     (  500,  5 * DEFAULT_COST);          \/\/ fadd, fmul, fmadd\n-  int_def FMUL_DOUBLE_COST     (  700,  7 * DEFAULT_COST);          \/\/ fadd, fmul, fmadd\n+  int_def FMUL_SINGLE_COST     (  500,  5 * DEFAULT_COST);          \/\/ fmul, fmadd\n+  int_def FMUL_DOUBLE_COST     (  700,  7 * DEFAULT_COST);          \/\/ fmul, fmadd\n@@ -1925,1 +1925,6 @@\n-    return MaxVectorSize;\n+    int size = MaxVectorSize;\n+    \/\/ Minimum 2 values in vector\n+    if (size < 2 * type2aelembytes(bt)) size = 0;\n+    \/\/ But never < 4\n+    if (size < 4) size = 0;\n+    return size;\n@@ -1934,0 +1939,1 @@\n+\n@@ -1935,1 +1941,9 @@\n-  return max_vector_size(bt);\n+  int max_size = max_vector_size(bt);\n+  \/\/ Limit the min vector size to 8 bytes.\n+  int size = 8 \/ type2aelembytes(bt);\n+  if (bt == T_BOOLEAN) {\n+    \/\/ To support vector api load\/store mask.\n+    size = 2;\n+  }\n+  if (size < 2) size = 2;\n+  return MIN2(size, max_size);\n@@ -3935,3 +3949,3 @@\n-\/\/ (actually a mvw $dst $src) and the downstream instructions consume\n-\/\/ the result of the l2i as an iRegI input. That's a shame since the\n-\/\/ mvw is actually redundant but its not too costly.\n+\/\/ (actually an addiw $dst, $src, 0) and the downstream instructions\n+\/\/ consume the result of the L2I as an iRegI input. That's a shame since\n+\/\/ the addiw is actually redundant but its not too costly.\n@@ -7027,1 +7041,1 @@\n-  ins_pipe(ialu_reg);\n+  ins_pipe(ialu_reg_imm);\n@@ -7039,1 +7053,1 @@\n-  ins_pipe(ialu_reg);\n+  ins_pipe(ialu_reg_imm);\n@@ -7049,1 +7063,1 @@\n-  ins_cost(FMUL_SINGLE_COST);\n+  ins_cost(DEFAULT_COST * 5);\n@@ -7064,1 +7078,1 @@\n-  ins_cost(FMUL_DOUBLE_COST);\n+  ins_cost(DEFAULT_COST * 5);\n@@ -7079,1 +7093,1 @@\n-  ins_cost(FMUL_SINGLE_COST);\n+  ins_cost(DEFAULT_COST * 5);\n@@ -7094,1 +7108,1 @@\n-  ins_cost(FMUL_DOUBLE_COST);\n+  ins_cost(DEFAULT_COST * 5);\n@@ -7297,1 +7311,1 @@\n-  ins_pipe(fp_dop_reg_reg_s);\n+  ins_pipe(pipe_class_default);\n@@ -7313,1 +7327,1 @@\n-  ins_pipe(fp_dop_reg_reg_s);\n+  ins_pipe(pipe_class_default);\n@@ -7329,1 +7343,1 @@\n-  ins_pipe(fp_dop_reg_reg_d);\n+  ins_pipe(pipe_class_default);\n@@ -7345,1 +7359,1 @@\n-  ins_pipe(fp_dop_reg_reg_d);\n+  ins_pipe(pipe_class_default);\n@@ -7349,1 +7363,1 @@\n-instruct isIniniteF_reg_reg(iRegINoSp dst, fRegF src)\n+instruct isInfiniteF_reg_reg(iRegINoSp dst, fRegF src)\n@@ -7352,0 +7366,1 @@\n+\n@@ -7355,1 +7370,1 @@\n-    __ andi(as_Register($dst$$reg), as_Register($dst$$reg), 0b10000001);\n+    __ andi(as_Register($dst$$reg), as_Register($dst$$reg), 0b0010000001);\n@@ -7358,1 +7373,2 @@\n-  ins_pipe(fp_dop_reg_reg_s);\n+\n+  ins_pipe(pipe_class_default);\n@@ -7365,0 +7381,1 @@\n+\n@@ -7368,1 +7385,1 @@\n-    __ andi(as_Register($dst$$reg), as_Register($dst$$reg), 0b10000001);\n+    __ andi(as_Register($dst$$reg), as_Register($dst$$reg), 0b0010000001);\n@@ -7371,1 +7388,2 @@\n-  ins_pipe(fp_dop_reg_reg_d);\n+\n+  ins_pipe(pipe_class_default);\n@@ -7378,0 +7396,1 @@\n+\n@@ -7384,1 +7403,2 @@\n-  ins_pipe(fp_dop_reg_reg_s);\n+\n+  ins_pipe(pipe_class_default);\n@@ -7391,0 +7411,1 @@\n+\n@@ -7397,1 +7418,2 @@\n-  ins_pipe(fp_dop_reg_reg_d);\n+\n+  ins_pipe(pipe_class_default);\n@@ -10081,0 +10103,17 @@\n+instruct cmovI_cmpUL(iRegINoSp dst, iRegI src, iRegL op1, iRegL op2, cmpOpU cop) %{\n+  match(Set dst (CMoveI (Binary cop (CmpUL op1 op2)) (Binary dst src)));\n+  ins_cost(ALU_COST + BRANCH_COST);\n+\n+  format %{\n+    \"CMove $dst, ($op1 $cop $op2), $dst, $src\\t#@cmovI_cmpUL\\n\\t\"\n+  %}\n+\n+  ins_encode %{\n+    __ enc_cmove($cop$$cmpcode | C2_MacroAssembler::unsigned_branch_mask,\n+                 as_Register($op1$$reg), as_Register($op2$$reg),\n+                 as_Register($dst$$reg), as_Register($src$$reg));\n+  %}\n+\n+  ins_pipe(pipe_class_compare);\n+%}\n+\n@@ -10115,2 +10154,2 @@\n-instruct cmovI_cmpUL(iRegINoSp dst, iRegI src, iRegL op1, iRegL op2, cmpOpU cop) %{\n-  match(Set dst (CMoveI (Binary cop (CmpUL op1 op2)) (Binary dst src)));\n+instruct cmovL_cmpI(iRegLNoSp dst, iRegL src, iRegI op1, iRegI op2, cmpOp cop) %{\n+  match(Set dst (CMoveL (Binary cop (CmpI op1 op2)) (Binary dst src)));\n@@ -10120,1 +10159,1 @@\n-    \"CMove $dst, ($op1 $cop $op2), $dst, $src\\t#@cmovI_cmpUL\\n\\t\"\n+    \"CMove $dst, ($op1 $cop $op2), $dst, $src\\t#@cmovL_cmpI\\n\\t\"\n@@ -10124,1 +10163,1 @@\n-    __ enc_cmove($cop$$cmpcode | C2_MacroAssembler::unsigned_branch_mask,\n+    __ enc_cmove($cop$$cmpcode,\n@@ -10132,0 +10171,16 @@\n+instruct cmovL_cmpU(iRegLNoSp dst, iRegL src, iRegI op1, iRegI op2, cmpOpU cop) %{\n+  match(Set dst (CMoveL (Binary cop (CmpU op1 op2)) (Binary dst src)));\n+  ins_cost(ALU_COST + BRANCH_COST);\n+\n+  format %{\n+    \"CMove $dst, ($op1 $cop $op2), $dst, $src\\t#@cmovL_cmpU\\n\\t\"\n+  %}\n+\n+  ins_encode %{\n+    __ enc_cmove($cop$$cmpcode | C2_MacroAssembler::unsigned_branch_mask,\n+                 as_Register($op1$$reg), as_Register($op2$$reg),\n+                 as_Register($dst$$reg), as_Register($src$$reg));\n+  %}\n+\n+  ins_pipe(pipe_class_compare);\n+%}\n","filename":"src\/hotspot\/cpu\/riscv\/riscv.ad","additions":83,"deletions":28,"binary":false,"changes":111,"status":"modified"},{"patch":"@@ -4,1 +4,1 @@\n- * Copyright (c) 2020, 2022, Huawei Technologies Co., Ltd. All rights reserved.\n+ * Copyright (c) 2020, 2023, Huawei Technologies Co., Ltd. All rights reserved.\n@@ -433,1 +433,1 @@\n-        if ( sig_bt[i] == T_LONG || sig_bt[i] == T_DOUBLE) {\n+        if (sig_bt[i] == T_LONG || sig_bt[i] == T_DOUBLE) {\n@@ -889,1 +889,1 @@\n-  __ mvw(t0, ContinuationEntry::cookie_value());\n+  __ mv(t0, ContinuationEntry::cookie_value());\n@@ -1699,1 +1699,1 @@\n-        __ andi(swap_reg, swap_reg, 3 - os::vm_page_size());\n+        __ andi(swap_reg, swap_reg, 3 - (int)os::vm_page_size());\n@@ -2112,1 +2112,1 @@\n-  __ mvw(xcpool, Deoptimization::Unpack_deopt); \/\/ callee-saved\n+  __ mv(xcpool, Deoptimization::Unpack_deopt); \/\/ callee-saved\n@@ -2129,1 +2129,1 @@\n-  __ mvw(xcpool, Deoptimization::Unpack_reexecute); \/\/ callee-saved\n+  __ mv(xcpool, Deoptimization::Unpack_reexecute); \/\/ callee-saved\n@@ -2152,1 +2152,1 @@\n-    __ mvw(t0, -1);\n+    __ mv(t0, -1);\n@@ -2155,1 +2155,1 @@\n-    __ mvw(xcpool, (int32_t)Deoptimization::Unpack_reexecute);\n+    __ mv(xcpool, Deoptimization::Unpack_reexecute);\n@@ -2496,1 +2496,1 @@\n-  __ mvw(c_rarg2, (unsigned)Deoptimization::Unpack_uncommon_trap);\n+  __ mv(c_rarg2, Deoptimization::Unpack_uncommon_trap);\n@@ -2522,1 +2522,1 @@\n-    __ mvw(t1, Deoptimization::Unpack_uncommon_trap);\n+    __ mv(t1, Deoptimization::Unpack_uncommon_trap);\n@@ -2623,1 +2623,1 @@\n-  __ mvw(c_rarg1, (unsigned)Deoptimization::Unpack_uncommon_trap);\n+  __ mv(c_rarg1, Deoptimization::Unpack_uncommon_trap);\n","filename":"src\/hotspot\/cpu\/riscv\/sharedRuntime_riscv.cpp","additions":11,"deletions":11,"binary":false,"changes":22,"status":"modified"},{"patch":"@@ -48,12 +48,0 @@\n-RangeCheckStub::RangeCheckStub(CodeEmitInfo* info, LIR_Opr index, LIR_Opr array)\n-  : _index(index), _array(array), _throw_index_out_of_bounds_exception(false) {\n-  assert(info != NULL, \"must have info\");\n-  _info = new CodeEmitInfo(info);\n-}\n-\n-RangeCheckStub::RangeCheckStub(CodeEmitInfo* info, LIR_Opr index)\n-  : _index(index), _array(), _throw_index_out_of_bounds_exception(true) {\n-  assert(info != NULL, \"must have info\");\n-  _info = new CodeEmitInfo(info);\n-}\n-\n@@ -230,5 +218,0 @@\n-MonitorEnterStub::MonitorEnterStub(LIR_Opr obj_reg, LIR_Opr lock_reg, CodeEmitInfo* info)\n-  : MonitorAccessStub(obj_reg, lock_reg) {\n-  _info = new CodeEmitInfo(info);\n-}\n-\n","filename":"src\/hotspot\/cpu\/s390\/c1_CodeStubs_s390.cpp","additions":0,"deletions":17,"binary":false,"changes":17,"status":"modified"},{"patch":"@@ -2,2 +2,2 @@\n- * Copyright (c) 2016, 2022, Oracle and\/or its affiliates. All rights reserved.\n- * Copyright (c) 2016, 2022 SAP SE. All rights reserved.\n+ * Copyright (c) 2016, 2023, Oracle and\/or its affiliates. All rights reserved.\n+ * Copyright (c) 2016, 2023 SAP SE. All rights reserved.\n@@ -1056,1 +1056,1 @@\n-\/\/ addr: Address descriptor of memory to clear index register will not be used !\n+\/\/ addr: Address descriptor of memory to clear. Index register will not be used!\n@@ -1058,0 +1058,1 @@\n+\/\/ condition code will not be preserved.\n@@ -1060,7 +1061,2 @@\n-void MacroAssembler::clear_mem(const Address& addr, unsigned size) {\n-  guarantee(size <= 256, \"MacroAssembler::clear_mem: size too large\");\n-\n-  if (size == 1) {\n-    z_mvi(addr, 0);\n-    return;\n-  }\n+void MacroAssembler::clear_mem(const Address& addr, unsigned int size) {\n+  guarantee((addr.disp() + size) <= 4096, \"MacroAssembler::clear_mem: size too large\");\n@@ -1069,1 +1065,4 @@\n-    case 2: z_mvhhi(addr, 0);\n+    case 0:\n+      return;\n+    case 1:\n+      z_mvi(addr, 0);\n@@ -1071,1 +1070,2 @@\n-    case 4: z_mvhi(addr, 0);\n+    case 2:\n+      z_mvhhi(addr, 0);\n@@ -1073,1 +1073,5 @@\n-    case 8: z_mvghi(addr, 0);\n+    case 4:\n+      z_mvhi(addr, 0);\n+      return;\n+    case 8:\n+      z_mvghi(addr, 0);\n@@ -1078,1 +1082,15 @@\n-  z_xc(addr, size, addr);\n+  \/\/ Caution: the emitter with Address operands does implicitly decrement the length\n+  if (size <= 256) {\n+    z_xc(addr, size, addr);\n+  } else {\n+    unsigned int offset = addr.disp();\n+    unsigned int incr   = 256;\n+    for (unsigned int i = 0; i <= size-incr; i += incr) {\n+      z_xc(offset, incr - 1, addr.base(), offset, addr.base());\n+      offset += incr;\n+    }\n+    unsigned int rest = size - (offset - addr.disp());\n+    if (size > 0) {\n+      z_xc(offset, rest-1, addr.base(), offset, addr.base());\n+    }\n+  }\n@@ -3356,2 +3374,0 @@\n-  verify_thread();\n-\n@@ -3365,2 +3381,0 @@\n-  verify_thread();\n-\n@@ -3641,0 +3655,5 @@\n+void MacroAssembler::load_klass_check_null(Register klass, Register src_oop, Register tmp) {\n+  null_check(src_oop, tmp, oopDesc::klass_offset_in_bytes());\n+  load_klass(klass, src_oop);\n+}\n+\n@@ -5390,6 +5409,0 @@\n-void MacroAssembler::verify_thread() {\n-  if (VerifyThread) {\n-    unimplemented(\"\", 117);\n-  }\n-}\n-\n","filename":"src\/hotspot\/cpu\/s390\/macroAssembler_s390.cpp","additions":37,"deletions":24,"binary":false,"changes":61,"status":"modified"},{"patch":"@@ -120,12 +120,0 @@\n-RangeCheckStub::RangeCheckStub(CodeEmitInfo* info, LIR_Opr index, LIR_Opr array)\n-  : _index(index), _array(array), _throw_index_out_of_bounds_exception(false) {\n-  assert(info != NULL, \"must have info\");\n-  _info = new CodeEmitInfo(info);\n-}\n-\n-RangeCheckStub::RangeCheckStub(CodeEmitInfo* info, LIR_Opr index)\n-  : _index(index), _array(), _throw_index_out_of_bounds_exception(true) {\n-  assert(info != NULL, \"must have info\");\n-  _info = new CodeEmitInfo(info);\n-}\n-\n@@ -258,10 +246,0 @@\n-\n-\/\/ Implementation of MonitorAccessStubs\n-\n-MonitorEnterStub::MonitorEnterStub(LIR_Opr obj_reg, LIR_Opr lock_reg, CodeEmitInfo* info)\n-: MonitorAccessStub(obj_reg, lock_reg)\n-{\n-  _info = new CodeEmitInfo(info);\n-}\n-\n-\n","filename":"src\/hotspot\/cpu\/x86\/c1_CodeStubs_x86.cpp","additions":0,"deletions":22,"binary":false,"changes":22,"status":"modified"},{"patch":"@@ -2,1 +2,1 @@\n- * Copyright (c) 1999, 2022, Oracle and\/or its affiliates. All rights reserved.\n+ * Copyright (c) 1999, 2023, Oracle and\/or its affiliates. All rights reserved.\n@@ -75,1 +75,0 @@\n-    \/\/ mark header as unlocked\n@@ -100,1 +99,1 @@\n-    andptr(hdr, aligned_mask - os::vm_page_size());\n+    andptr(hdr, aligned_mask - (int)os::vm_page_size());\n","filename":"src\/hotspot\/cpu\/x86\/c1_MacroAssembler_x86.cpp","additions":2,"deletions":3,"binary":false,"changes":5,"status":"modified"},{"patch":"@@ -576,0 +576,1 @@\n+    assert(cx1Reg == noreg, \"\");\n@@ -598,1 +599,1 @@\n-    load_klass(tmpReg, objReg, cx1Reg);\n+    load_klass(tmpReg, objReg, scrReg);\n@@ -647,1 +648,1 @@\n-      andptr(tmpReg, (int32_t) (NOT_LP64(0xFFFFF003) LP64_ONLY(7 - os::vm_page_size())) );\n+      andptr(tmpReg, (int32_t) (NOT_LP64(0xFFFFF003) LP64_ONLY(7 - (int)os::vm_page_size())) );\n@@ -2828,2 +2829,2 @@\n-      andl(result, (os::vm_page_size()-1));\n-      cmpl(result, (os::vm_page_size()-16));\n+      andl(result, ((int)os::vm_page_size()-1));\n+      cmpl(result, ((int)os::vm_page_size()-16));\n@@ -2858,2 +2859,2 @@\n-    andl(result, (os::vm_page_size()-1));\n-    cmpl(result, (os::vm_page_size()-16));\n+    andl(result, ((int)os::vm_page_size()-1));\n+    cmpl(result, ((int)os::vm_page_size()-16));\n","filename":"src\/hotspot\/cpu\/x86\/c2_MacroAssembler_x86.cpp","additions":7,"deletions":6,"binary":false,"changes":13,"status":"modified"},{"patch":"@@ -2,1 +2,1 @@\n- * Copyright (c) 1997, 2022, Oracle and\/or its affiliates. All rights reserved.\n+ * Copyright (c) 1997, 2023, Oracle and\/or its affiliates. All rights reserved.\n@@ -1283,1 +1283,1 @@\n-      andptr(swap_reg, zero_bits - os::vm_page_size());\n+      andptr(swap_reg, zero_bits - (int)os::vm_page_size());\n","filename":"src\/hotspot\/cpu\/x86\/interp_masm_x86.cpp","additions":2,"deletions":2,"binary":false,"changes":4,"status":"modified"},{"patch":"@@ -1277,3 +1277,3 @@\n-  movl(Address(tmp, (-os::vm_page_size())), size );\n-  subptr(tmp, os::vm_page_size());\n-  subl(size, os::vm_page_size());\n+  movl(Address(tmp, (-(int)os::vm_page_size())), size );\n+  subptr(tmp, (int)os::vm_page_size());\n+  subl(size, (int)os::vm_page_size());\n@@ -1288,1 +1288,1 @@\n-  for (int i = 1; i < ((int)StackOverflow::stack_shadow_zone_size() \/ os::vm_page_size()); i++) {\n+  for (int i = 1; i < ((int)StackOverflow::stack_shadow_zone_size() \/ (int)os::vm_page_size()); i++) {\n@@ -1291,1 +1291,1 @@\n-    movptr(Address(tmp, (-i*os::vm_page_size())), size );\n+    movptr(Address(tmp, (-i*(int)os::vm_page_size())), size );\n@@ -5153,1 +5153,1 @@\n-void MacroAssembler::load_klass(Register dst, Register src, Register tmp, bool null_check_src) {\n+void MacroAssembler::load_klass(Register dst, Register src, Register tmp) {\n@@ -5158,3 +5158,0 @@\n-  if (null_check_src) {\n-    null_check(src, oopDesc::mark_offset_in_bytes());\n-  }\n@@ -5164,3 +5161,0 @@\n-  if (null_check_src) {\n-    null_check(src, oopDesc::klass_offset_in_bytes());\n-  }\n@@ -5171,0 +5165,9 @@\n+void MacroAssembler::load_klass_check_null(Register dst, Register src, Register tmp) {\n+#ifdef _LP64\n+  null_check(src, oopDesc::mark_offset_in_bytes());\n+#else\n+  null_check(src, oopDesc::klass_offset_in_bytes());\n+#endif\n+  load_klass(dst, src, tmp);\n+}\n+\n","filename":"src\/hotspot\/cpu\/x86\/macroAssembler_x86.cpp","additions":15,"deletions":12,"binary":false,"changes":27,"status":"modified"},{"patch":"@@ -373,1 +373,0 @@\n-  void load_klass(Register dst, Register src, Register tmp, bool null_check_src = false);\n@@ -379,0 +378,2 @@\n+  void load_klass(Register dst, Register src, Register tmp);\n+  void load_klass_check_null(Register dst, Register src, Register tmp);\n","filename":"src\/hotspot\/cpu\/x86\/macroAssembler_x86.hpp","additions":2,"deletions":1,"binary":false,"changes":3,"status":"modified"},{"patch":"@@ -2,1 +2,1 @@\n- * Copyright (c) 2003, 2022, Oracle and\/or its affiliates. All rights reserved.\n+ * Copyright (c) 2003, 2023, Oracle and\/or its affiliates. All rights reserved.\n@@ -1710,1 +1710,1 @@\n-        __ andptr(swap_reg, 3 - os::vm_page_size());\n+        __ andptr(swap_reg, 3 - (int)os::vm_page_size());\n","filename":"src\/hotspot\/cpu\/x86\/sharedRuntime_x86_32.cpp","additions":2,"deletions":2,"binary":false,"changes":4,"status":"modified"},{"patch":"@@ -2,1 +2,1 @@\n- * Copyright (c) 2003, 2022, Oracle and\/or its affiliates. All rights reserved.\n+ * Copyright (c) 2003, 2023, Oracle and\/or its affiliates. All rights reserved.\n@@ -2179,1 +2179,1 @@\n-        __ andptr(swap_reg, 3 - os::vm_page_size());\n+        __ andptr(swap_reg, 3 - (int)os::vm_page_size());\n","filename":"src\/hotspot\/cpu\/x86\/sharedRuntime_x86_64.cpp","additions":2,"deletions":2,"binary":false,"changes":4,"status":"modified"},{"patch":"@@ -1096,20 +1096,2 @@\n-#if INCLUDE_ZGC\n-  if (UseZGC) {\n-    \/\/ Check if metadata bits indicate a bad oop\n-    __ testptr(rax, Address(r15_thread, ZThreadLocalData::address_bad_mask_offset()));\n-    __ jcc(Assembler::notZero, error);\n-  }\n-#endif\n-\n-  \/\/ Check if the oop is in the right area of memory\n-  __ movptr(c_rarg2, rax);\n-  __ movptr(c_rarg3, (intptr_t) Universe::verify_oop_mask());\n-  __ andptr(c_rarg2, c_rarg3);\n-  __ movptr(c_rarg3, (intptr_t) Universe::verify_oop_bits());\n-  __ cmpptr(c_rarg2, c_rarg3);\n-  __ jcc(Assembler::notZero, error);\n-\n-  \/\/ make sure klass is 'reasonable', which is not zero.\n-  __ load_klass(rax, rax, rscratch1);  \/\/ get klass\n-  __ testptr(rax, rax);\n-  __ jcc(Assembler::zero, error); \/\/ if klass is NULL it is broken\n+   BarrierSetAssembler* bs_asm = BarrierSet::barrier_set()->barrier_set_assembler();\n+   bs_asm->check_oop(_masm, rax, c_rarg2, c_rarg3, error);\n@@ -1663,1 +1645,0 @@\n-  __ cmpl(length, 0);\n@@ -1706,2 +1687,1 @@\n-  if (VM_Version::supports_avx2()\n-      && VM_Version::supports_avx512vlbw()) {\n+  if (VM_Version::supports_avx2()) {\n@@ -1724,0 +1704,1 @@\n+    __ movdl(xmm8, rax);\n@@ -1725,1 +1706,1 @@\n-    __ evpbroadcastd(xmm8, rax, Assembler::AVX_256bit);\n+    __ vpbroadcastd(xmm8, xmm8, Assembler::AVX_256bit);\n@@ -1732,1 +1713,2 @@\n-    __ evpbroadcastd(xmm7, rax, Assembler::AVX_256bit);\n+    __ movdl(xmm7, rax);\n+    __ vpbroadcastd(xmm7, xmm7, Assembler::AVX_256bit);\n@@ -1834,1 +1816,2 @@\n-    __ evpbroadcastd(xmm6, rax, Assembler::AVX_256bit);\n+    __ movdl(xmm6, rax);\n+    __ vpbroadcastd(xmm6, xmm6, Assembler::AVX_256bit);\n@@ -1837,1 +1820,2 @@\n-    __ evpbroadcastd(xmm5, rax, Assembler::AVX_256bit);\n+    __ movdl(xmm5, rax);\n+    __ vpbroadcastd(xmm5, xmm5, Assembler::AVX_256bit);\n@@ -1844,1 +1828,2 @@\n-    __ evpbroadcastd(xmm3, rax, Assembler::AVX_256bit);\n+    __ movdl(xmm3, rax);\n+    __ vpbroadcastd(xmm3, xmm3, Assembler::AVX_256bit);\n@@ -1846,1 +1831,2 @@\n-    __ evpbroadcastd(xmm4, rax, Assembler::AVX_256bit);\n+    __ movdl(xmm4, rax);\n+    __ vpbroadcastd(xmm4, xmm4, Assembler::AVX_256bit);\n@@ -2154,0 +2140,74 @@\n+address StubGenerator::base64_AVX2_decode_tables_addr() {\n+  __ align64();\n+  StubCodeMark mark(this, \"StubRoutines\", \"AVX2_tables_base64\");\n+  address start = __ pc();\n+\n+  assert(((unsigned long long)start & 0x3f) == 0,\n+         \"Alignment problem (0x%08llx)\", (unsigned long long)start);\n+  __ emit_data(0x2f2f2f2f, relocInfo::none, 0);\n+  __ emit_data(0x5f5f5f5f, relocInfo::none, 0);  \/\/ for URL\n+\n+  __ emit_data(0xffffffff, relocInfo::none, 0);\n+  __ emit_data(0xfcfcfcfc, relocInfo::none, 0);  \/\/ for URL\n+\n+  \/\/ Permute table\n+  __ emit_data64(0x0000000100000000, relocInfo::none);\n+  __ emit_data64(0x0000000400000002, relocInfo::none);\n+  __ emit_data64(0x0000000600000005, relocInfo::none);\n+  __ emit_data64(0xffffffffffffffff, relocInfo::none);\n+\n+  \/\/ Shuffle table\n+  __ emit_data64(0x090a040506000102, relocInfo::none);\n+  __ emit_data64(0xffffffff0c0d0e08, relocInfo::none);\n+  __ emit_data64(0x090a040506000102, relocInfo::none);\n+  __ emit_data64(0xffffffff0c0d0e08, relocInfo::none);\n+\n+  \/\/ merge table\n+  __ emit_data(0x01400140, relocInfo::none, 0);\n+\n+  \/\/ merge multiplier\n+  __ emit_data(0x00011000, relocInfo::none, 0);\n+\n+  return start;\n+}\n+\n+address StubGenerator::base64_AVX2_decode_LUT_tables_addr() {\n+  __ align64();\n+  StubCodeMark mark(this, \"StubRoutines\", \"AVX2_tables_URL_base64\");\n+  address start = __ pc();\n+\n+  assert(((unsigned long long)start & 0x3f) == 0,\n+         \"Alignment problem (0x%08llx)\", (unsigned long long)start);\n+  \/\/ lut_lo\n+  __ emit_data64(0x1111111111111115, relocInfo::none);\n+  __ emit_data64(0x1a1b1b1b1a131111, relocInfo::none);\n+  __ emit_data64(0x1111111111111115, relocInfo::none);\n+  __ emit_data64(0x1a1b1b1b1a131111, relocInfo::none);\n+\n+  \/\/ lut_roll\n+  __ emit_data64(0xb9b9bfbf04131000, relocInfo::none);\n+  __ emit_data64(0x0000000000000000, relocInfo::none);\n+  __ emit_data64(0xb9b9bfbf04131000, relocInfo::none);\n+  __ emit_data64(0x0000000000000000, relocInfo::none);\n+\n+  \/\/ lut_lo URL\n+  __ emit_data64(0x1111111111111115, relocInfo::none);\n+  __ emit_data64(0x1b1b1a1b1b131111, relocInfo::none);\n+  __ emit_data64(0x1111111111111115, relocInfo::none);\n+  __ emit_data64(0x1b1b1a1b1b131111, relocInfo::none);\n+\n+  \/\/ lut_roll URL\n+  __ emit_data64(0xb9b9bfbf0411e000, relocInfo::none);\n+  __ emit_data64(0x0000000000000000, relocInfo::none);\n+  __ emit_data64(0xb9b9bfbf0411e000, relocInfo::none);\n+  __ emit_data64(0x0000000000000000, relocInfo::none);\n+\n+  \/\/ lut_hi\n+  __ emit_data64(0x0804080402011010, relocInfo::none);\n+  __ emit_data64(0x1010101010101010, relocInfo::none);\n+  __ emit_data64(0x0804080402011010, relocInfo::none);\n+  __ emit_data64(0x1010101010101010, relocInfo::none);\n+\n+  return start;\n+}\n+\n@@ -2310,1 +2370,1 @@\n-  Label L_forceLoop, L_bottomLoop, L_checkMIME, L_exit_no_vzero;\n+  Label L_forceLoop, L_bottomLoop, L_checkMIME, L_exit_no_vzero, L_lastChunk;\n@@ -2320,2 +2380,2 @@\n-    __ cmpl(length, 128);     \/\/ 128-bytes is break-even for AVX-512\n-    __ jcc(Assembler::lessEqual, L_bruteForce);\n+    __ cmpl(length, 31);     \/\/ 32-bytes is break-even for AVX-512\n+    __ jcc(Assembler::lessEqual, L_lastChunk);\n@@ -2324,1 +2384,1 @@\n-    __ jcc(Assembler::notEqual, L_bruteForce);\n+    __ jcc(Assembler::notEqual, L_lastChunk);\n@@ -2575,0 +2635,83 @@\n+  if (VM_Version::supports_avx2()) {\n+    Label L_tailProc, L_topLoop, L_enterLoop;\n+\n+    __ cmpl(isMIME, 0);\n+    __ jcc(Assembler::notEqual, L_lastChunk);\n+\n+    \/\/ Check for buffer too small (for algorithm)\n+    __ subl(length, 0x2c);\n+    __ jcc(Assembler::less, L_tailProc);\n+\n+    __ shll(isURL, 2);\n+\n+    \/\/ Algorithm adapted from https:\/\/arxiv.org\/abs\/1704.00605, \"Faster Base64\n+    \/\/ Encoding and Decoding using AVX2 Instructions\".  URL modifications added.\n+\n+    \/\/ Set up constants\n+    __ lea(r13, ExternalAddress(StubRoutines::x86::base64_AVX2_decode_tables_addr()));\n+    __ vpbroadcastd(xmm4, Address(r13, isURL, Address::times_1), Assembler::AVX_256bit);  \/\/ 2F or 5F\n+    __ vpbroadcastd(xmm10, Address(r13, isURL, Address::times_1, 0x08), Assembler::AVX_256bit);  \/\/ -1 or -4\n+    __ vmovdqu(xmm12, Address(r13, 0x10));  \/\/ permute\n+    __ vmovdqu(xmm13, Address(r13, 0x30)); \/\/ shuffle\n+    __ vpbroadcastd(xmm7, Address(r13, 0x50), Assembler::AVX_256bit);  \/\/ merge\n+    __ vpbroadcastd(xmm6, Address(r13, 0x54), Assembler::AVX_256bit);  \/\/ merge mult\n+\n+    __ lea(r13, ExternalAddress(StubRoutines::x86::base64_AVX2_decode_LUT_tables_addr()));\n+    __ shll(isURL, 4);\n+    __ vmovdqu(xmm11, Address(r13, isURL, Address::times_1, 0x00));  \/\/ lut_lo\n+    __ vmovdqu(xmm8, Address(r13, isURL, Address::times_1, 0x20)); \/\/ lut_roll\n+    __ shrl(isURL, 6);  \/\/ restore isURL\n+    __ vmovdqu(xmm9, Address(r13, 0x80));  \/\/ lut_hi\n+    __ jmp(L_enterLoop);\n+\n+    __ align32();\n+    __ bind(L_topLoop);\n+    \/\/ Add in the offset value (roll) to get 6-bit out values\n+    __ vpaddb(xmm0, xmm0, xmm2, Assembler::AVX_256bit);\n+    \/\/ Merge and permute the output bits into appropriate output byte lanes\n+    __ vpmaddubsw(xmm0, xmm0, xmm7, Assembler::AVX_256bit);\n+    __ vpmaddwd(xmm0, xmm0, xmm6, Assembler::AVX_256bit);\n+    __ vpshufb(xmm0, xmm0, xmm13, Assembler::AVX_256bit);\n+    __ vpermd(xmm0, xmm12, xmm0, Assembler::AVX_256bit);\n+    \/\/ Store the output bytes\n+    __ vmovdqu(Address(dest, dp, Address::times_1, 0), xmm0);\n+    __ addptr(source, 0x20);\n+    __ addptr(dest, 0x18);\n+    __ subl(length, 0x20);\n+    __ jcc(Assembler::less, L_tailProc);\n+\n+    __ bind(L_enterLoop);\n+\n+    \/\/ Load in encoded string (32 bytes)\n+    __ vmovdqu(xmm2, Address(source, start_offset, Address::times_1, 0x0));\n+    \/\/ Extract the high nibble for indexing into the lut tables.  High 4 bits are don't care.\n+    __ vpsrld(xmm1, xmm2, 0x4, Assembler::AVX_256bit);\n+    __ vpand(xmm1, xmm4, xmm1, Assembler::AVX_256bit);\n+    \/\/ Extract the low nibble. 5F\/2F will isolate the low-order 4 bits.  High 4 bits are don't care.\n+    __ vpand(xmm3, xmm2, xmm4, Assembler::AVX_256bit);\n+    \/\/ Check for special-case (0x2F or 0x5F (URL))\n+    __ vpcmpeqb(xmm0, xmm4, xmm2, Assembler::AVX_256bit);\n+    \/\/ Get the bitset based on the low nibble.  vpshufb uses low-order 4 bits only.\n+    __ vpshufb(xmm3, xmm11, xmm3, Assembler::AVX_256bit);\n+    \/\/ Get the bit value of the high nibble\n+    __ vpshufb(xmm5, xmm9, xmm1, Assembler::AVX_256bit);\n+    \/\/ Make sure 2F \/ 5F shows as valid\n+    __ vpandn(xmm3, xmm0, xmm3, Assembler::AVX_256bit);\n+    \/\/ Make adjustment for roll index.  For non-URL, this is a no-op,\n+    \/\/ for URL, this adjusts by -4.  This is to properly index the\n+    \/\/ roll value for 2F \/ 5F.\n+    __ vpand(xmm0, xmm0, xmm10, Assembler::AVX_256bit);\n+    \/\/ If the and of the two is non-zero, we have an invalid input character\n+    __ vptest(xmm3, xmm5);\n+    \/\/ Extract the \"roll\" value - value to add to the input to get 6-bit out value\n+    __ vpaddb(xmm0, xmm0, xmm1, Assembler::AVX_256bit); \/\/ Handle 2F \/ 5F\n+    __ vpshufb(xmm0, xmm8, xmm0, Assembler::AVX_256bit);\n+    __ jcc(Assembler::equal, L_topLoop);  \/\/ Fall through on error\n+\n+    __ bind(L_tailProc);\n+\n+    __ addl(length, 0x2c);\n+\n+    __ vzeroupper();\n+  }\n+\n@@ -2605,0 +2748,2 @@\n+  __ bind(L_lastChunk);\n+\n@@ -3899,3 +4044,1 @@\n-    if(VM_Version::supports_avx2() &&\n-       VM_Version::supports_avx512bw() &&\n-       VM_Version::supports_avx512vl()) {\n+    if(VM_Version::supports_avx2()) {\n@@ -3905,0 +4048,2 @@\n+      StubRoutines::x86::_avx2_decode_tables_base64 = base64_AVX2_decode_tables_addr();\n+      StubRoutines::x86::_avx2_decode_lut_tables_base64 = base64_AVX2_decode_LUT_tables_addr();\n","filename":"src\/hotspot\/cpu\/x86\/stubGenerator_x86_64.cpp","additions":181,"deletions":36,"binary":false,"changes":217,"status":"modified"},{"patch":"@@ -444,0 +444,2 @@\n+  address base64_AVX2_decode_tables_addr();\n+  address base64_AVX2_decode_LUT_tables_addr();\n","filename":"src\/hotspot\/cpu\/x86\/stubGenerator_x86_64.hpp","additions":2,"deletions":0,"binary":false,"changes":2,"status":"modified"},{"patch":"@@ -74,0 +74,2 @@\n+address StubRoutines::x86::_avx2_decode_tables_base64 = NULL;\n+address StubRoutines::x86::_avx2_decode_lut_tables_base64 = NULL;\n","filename":"src\/hotspot\/cpu\/x86\/stubRoutines_x86.cpp","additions":2,"deletions":0,"binary":false,"changes":2,"status":"modified"},{"patch":"@@ -190,0 +190,2 @@\n+  static address _avx2_decode_tables_base64;\n+  static address _avx2_decode_lut_tables_base64;\n@@ -332,0 +334,2 @@\n+  static address base64_AVX2_decode_tables_addr() { return _avx2_decode_tables_base64; }\n+  static address base64_AVX2_decode_LUT_tables_addr() { return _avx2_decode_lut_tables_base64; }\n","filename":"src\/hotspot\/cpu\/x86\/stubRoutines_x86.hpp","additions":4,"deletions":0,"binary":false,"changes":4,"status":"modified"},{"patch":"@@ -155,1 +155,4 @@\n-  __ store_heap_oop(dst, val, rdx, rbx, LP64_ONLY(r8) NOT_LP64(rsi), decorators);\n+  __ store_heap_oop(dst, val,\n+                    NOT_LP64(rdx) LP64_ONLY(rscratch2),\n+                    NOT_LP64(rbx) LP64_ONLY(r9),\n+                    NOT_LP64(rsi) LP64_ONLY(r8), decorators);\n@@ -3661,1 +3664,1 @@\n-  __ load_klass(rax, recv, rscratch1, true);\n+  __ load_klass_check_null(rax, recv, rscratch1);\n@@ -3752,1 +3755,1 @@\n-  __ load_klass(rlocals, rcx, rscratch1, true);\n+  __ load_klass_check_null(rlocals, rcx, rscratch1);\n@@ -3774,1 +3777,1 @@\n-  __ load_klass(rdx, rcx, rscratch1, true);\n+  __ load_klass_check_null(rdx, rcx, rscratch1);\n","filename":"src\/hotspot\/cpu\/x86\/templateTable_x86.cpp","additions":7,"deletions":4,"binary":false,"changes":11,"status":"modified"},{"patch":"@@ -13387,1 +13387,1 @@\n-instruct cmpFastLock(rFlagsReg cr, rRegP object, rbx_RegP box, rax_RegI tmp, rRegP scr, rRegP cx1) %{\n+instruct cmpFastLock(rFlagsReg cr, rRegP object, rbx_RegP box, rax_RegI tmp, rRegP scr) %{\n@@ -13390,1 +13390,1 @@\n-  effect(TEMP tmp, TEMP scr, TEMP cx1, USE_KILL box);\n+  effect(TEMP tmp, TEMP scr, USE_KILL box);\n@@ -13395,1 +13395,1 @@\n-                 $scr$$Register, $cx1$$Register, noreg, r15_thread, NULL, NULL, NULL, false, false);\n+                 $scr$$Register, noreg, noreg, r15_thread, NULL, NULL, NULL, false, false);\n","filename":"src\/hotspot\/cpu\/x86\/x86_64.ad","additions":3,"deletions":3,"binary":false,"changes":6,"status":"modified"},{"patch":"@@ -2,1 +2,1 @@\n- * Copyright (c) 1999, 2022, Oracle and\/or its affiliates. All rights reserved.\n+ * Copyright (c) 1999, 2023, Oracle and\/or its affiliates. All rights reserved.\n@@ -113,0 +113,2 @@\n+    FrameMap* f = Compilation::current()->frame_map();\n+    f->update_reserved_argument_area_size(2 * BytesPerWord);\n@@ -169,1 +171,7 @@\n-  RangeCheckStub(CodeEmitInfo* info, LIR_Opr index, LIR_Opr array);\n+  RangeCheckStub(CodeEmitInfo* info, LIR_Opr index, LIR_Opr array)\n+    : _index(index), _array(array), _throw_index_out_of_bounds_exception(false) {\n+    assert(info != NULL, \"must have info\");\n+    _info = new CodeEmitInfo(info);\n+    FrameMap* f = Compilation::current()->frame_map();\n+    f->update_reserved_argument_area_size(2 * BytesPerWord);\n+  }\n@@ -171,1 +179,7 @@\n-  RangeCheckStub(CodeEmitInfo* info, LIR_Opr index);\n+  RangeCheckStub(CodeEmitInfo* info, LIR_Opr index)\n+    : _index(index), _array(), _throw_index_out_of_bounds_exception(true) {\n+    assert(info != NULL, \"must have info\");\n+    _info = new CodeEmitInfo(info);\n+    FrameMap* f = Compilation::current()->frame_map();\n+    f->update_reserved_argument_area_size(2 * BytesPerWord);\n+  }\n@@ -338,1 +352,6 @@\n-  MonitorEnterStub(LIR_Opr obj_reg, LIR_Opr lock_reg, CodeEmitInfo* info);\n+  MonitorEnterStub(LIR_Opr obj_reg, LIR_Opr lock_reg, CodeEmitInfo* info)\n+    : MonitorAccessStub(obj_reg, lock_reg) {\n+    _info = new CodeEmitInfo(info);\n+    FrameMap* f = Compilation::current()->frame_map();\n+    f->update_reserved_argument_area_size(2 * BytesPerWord);\n+  }\n@@ -479,1 +498,4 @@\n-    _info(new CodeEmitInfo(info)), _trap_request(Deoptimization::make_trap_request(reason, action)) {}\n+    _info(new CodeEmitInfo(info)), _trap_request(Deoptimization::make_trap_request(reason, action)) {\n+    FrameMap* f = Compilation::current()->frame_map();\n+    f->update_reserved_argument_area_size(2 * BytesPerWord);\n+  }\n@@ -502,0 +524,2 @@\n+    FrameMap* f = Compilation::current()->frame_map();\n+    f->update_reserved_argument_area_size(2 * BytesPerWord);\n@@ -537,1 +561,4 @@\n-  ArrayCopyStub(LIR_OpArrayCopy* op): _op(op) { }\n+  ArrayCopyStub(LIR_OpArrayCopy* op): _op(op) {\n+    FrameMap* f = Compilation::current()->frame_map();\n+    f->update_reserved_argument_area_size(arraycopystub_reserved_argument_area_size * BytesPerWord);\n+  }\n","filename":"src\/hotspot\/share\/c1\/c1_CodeStubs.hpp","additions":33,"deletions":6,"binary":false,"changes":39,"status":"modified"},{"patch":"@@ -2,1 +2,1 @@\n- * Copyright (c) 1999, 2022, Oracle and\/or its affiliates. All rights reserved.\n+ * Copyright (c) 1999, 2023, Oracle and\/or its affiliates. All rights reserved.\n@@ -402,1 +402,1 @@\n-    _frame_map = new FrameMap(method(), hir()->number_of_locks(), MAX2(4, hir()->max_stack()));\n+    _frame_map = new FrameMap(method(), hir()->number_of_locks(), hir()->max_stack());\n","filename":"src\/hotspot\/share\/c1\/c1_Compilation.cpp","additions":2,"deletions":2,"binary":false,"changes":4,"status":"modified"},{"patch":"@@ -27,0 +27,1 @@\n+#include \"cds\/archiveHeapWriter.hpp\"\n@@ -850,3 +851,1 @@\n-\/\/ Update a Java object to point its Klass* to the address whene\n-\/\/ the class would be mapped at runtime.\n-void ArchiveBuilder::relocate_klass_ptr_of_oop(oop o) {\n+narrowKlass ArchiveBuilder::get_requested_narrow_klass(Klass* k) {\n@@ -854,1 +853,1 @@\n-  Klass* k = get_buffered_klass(o->klass());\n+  k = get_buffered_klass(k);\n@@ -856,4 +855,1 @@\n-  narrowKlass nk = CompressedKlassPointers::encode_not_null(requested_k, _requested_static_archive_bottom);\n-#ifdef _LP64\n-  o->set_mark(o->mark().set_narrow_klass(nk));\n-#endif\n+  return CompressedKlassPointers::encode_not_null(requested_k, _requested_static_archive_bottom);\n@@ -1084,2 +1080,1 @@\n-        oop archived_oop = cast_to_oop(start);\n-        oop original_oop = HeapShared::get_original_object(archived_oop);\n+        oop original_oop = ArchiveHeapWriter::buffered_addr_to_source_obj(start);\n@@ -1091,1 +1086,1 @@\n-        } else if (archived_oop == HeapShared::roots()) {\n+        } else if (start == ArchiveHeapWriter::buffered_heap_roots_addr()) {\n@@ -1096,1 +1091,1 @@\n-          byte_size = objArrayOopDesc::object_size(HeapShared::roots()->length()) * BytesPerWord;\n+          byte_size = ArchiveHeapWriter::heap_roots_word_size() * BytesPerWord;\n@@ -1113,1 +1108,1 @@\n-    return HeapShared::to_requested_address(p);\n+    return ArchiveHeapWriter::buffered_addr_to_requested_addr(p);\n","filename":"src\/hotspot\/share\/cds\/archiveBuilder.cpp","additions":8,"deletions":13,"binary":false,"changes":21,"status":"modified"},{"patch":"@@ -457,1 +457,1 @@\n-  void relocate_klass_ptr_of_oop(oop o);\n+  narrowKlass get_requested_narrow_klass(Klass* k);\n","filename":"src\/hotspot\/share\/cds\/archiveBuilder.hpp","additions":1,"deletions":1,"binary":false,"changes":2,"status":"modified"},{"patch":"@@ -0,0 +1,655 @@\n+\/*\n+ * Copyright (c) 2023, Oracle and\/or its affiliates. All rights reserved.\n+ * DO NOT ALTER OR REMOVE COPYRIGHT NOTICES OR THIS FILE HEADER.\n+ *\n+ * This code is free software; you can redistribute it and\/or modify it\n+ * under the terms of the GNU General Public License version 2 only, as\n+ * published by the Free Software Foundation.\n+ *\n+ * This code is distributed in the hope that it will be useful, but WITHOUT\n+ * ANY WARRANTY; without even the implied warranty of MERCHANTABILITY or\n+ * FITNESS FOR A PARTICULAR PURPOSE.  See the GNU General Public License\n+ * version 2 for more details (a copy is included in the LICENSE file that\n+ * accompanied this code).\n+ *\n+ * You should have received a copy of the GNU General Public License version\n+ * 2 along with this work; if not, write to the Free Software Foundation,\n+ * Inc., 51 Franklin St, Fifth Floor, Boston, MA 02110-1301 USA.\n+ *\n+ * Please contact Oracle, 500 Oracle Parkway, Redwood Shores, CA 94065 USA\n+ * or visit www.oracle.com if you need additional information or have any\n+ * questions.\n+ *\n+ *\/\n+\n+#include \"precompiled.hpp\"\n+#include \"cds\/archiveHeapWriter.hpp\"\n+#include \"cds\/filemap.hpp\"\n+#include \"cds\/heapShared.hpp\"\n+#include \"gc\/shared\/collectedHeap.hpp\"\n+#include \"memory\/iterator.inline.hpp\"\n+#include \"memory\/oopFactory.hpp\"\n+#include \"memory\/universe.hpp\"\n+#include \"oops\/compressedOops.hpp\"\n+#include \"oops\/oop.inline.hpp\"\n+#include \"oops\/objArrayOop.inline.hpp\"\n+#include \"oops\/oopHandle.inline.hpp\"\n+#include \"oops\/typeArrayKlass.hpp\"\n+#include \"oops\/typeArrayOop.hpp\"\n+#include \"runtime\/java.hpp\"\n+#include \"runtime\/mutexLocker.hpp\"\n+#include \"utilities\/bitMap.inline.hpp\"\n+\n+#if INCLUDE_G1GC\n+#include \"gc\/g1\/g1CollectedHeap.hpp\"\n+#include \"gc\/g1\/heapRegion.hpp\"\n+#endif\n+\n+#if INCLUDE_CDS_JAVA_HEAP\n+\n+\n+GrowableArrayCHeap<u1, mtClassShared>* ArchiveHeapWriter::_buffer;\n+\n+\/\/ The following are offsets from buffer_bottom()\n+size_t ArchiveHeapWriter::_buffer_top;\n+size_t ArchiveHeapWriter::_open_bottom;\n+size_t ArchiveHeapWriter::_open_top;\n+size_t ArchiveHeapWriter::_closed_bottom;\n+size_t ArchiveHeapWriter::_closed_top;\n+size_t ArchiveHeapWriter::_heap_roots_bottom;\n+\n+size_t ArchiveHeapWriter::_heap_roots_word_size;\n+\n+address ArchiveHeapWriter::_requested_open_region_bottom;\n+address ArchiveHeapWriter::_requested_open_region_top;\n+address ArchiveHeapWriter::_requested_closed_region_bottom;\n+address ArchiveHeapWriter::_requested_closed_region_top;\n+\n+ResourceBitMap* ArchiveHeapWriter::_closed_oopmap;\n+ResourceBitMap* ArchiveHeapWriter::_open_oopmap;\n+\n+GrowableArrayCHeap<ArchiveHeapWriter::NativePointerInfo, mtClassShared>* ArchiveHeapWriter::_native_pointers;\n+GrowableArrayCHeap<oop, mtClassShared>* ArchiveHeapWriter::_source_objs;\n+\n+ArchiveHeapWriter::BufferOffsetToSourceObjectTable*\n+  ArchiveHeapWriter::_buffer_offset_to_source_obj_table = nullptr;\n+\n+void ArchiveHeapWriter::init() {\n+  if (HeapShared::can_write()) {\n+    Universe::heap()->collect(GCCause::_java_lang_system_gc);\n+\n+    _buffer_offset_to_source_obj_table = new BufferOffsetToSourceObjectTable();\n+\n+    _requested_open_region_bottom = nullptr;\n+    _requested_open_region_top = nullptr;\n+    _requested_closed_region_bottom = nullptr;\n+    _requested_closed_region_top = nullptr;\n+\n+    _native_pointers = new GrowableArrayCHeap<NativePointerInfo, mtClassShared>(2048);\n+    _source_objs = new GrowableArrayCHeap<oop, mtClassShared>(10000);\n+\n+    guarantee(UseG1GC, \"implementation limitation\");\n+    guarantee(MIN_GC_REGION_ALIGNMENT <= \/*G1*\/HeapRegion::min_region_size_in_words() * HeapWordSize, \"must be\");\n+  }\n+}\n+\n+void ArchiveHeapWriter::add_source_obj(oop src_obj) {\n+  _source_objs->append(src_obj);\n+}\n+\n+\/\/ For the time being, always support two regions (to be strictly compatible with existing G1\n+\/\/ mapping code. We might eventually use a single region (JDK-8298048).\n+void ArchiveHeapWriter::write(GrowableArrayCHeap<oop, mtClassShared>* roots,\n+                              GrowableArray<MemRegion>* closed_regions, GrowableArray<MemRegion>* open_regions,\n+                              GrowableArray<ArchiveHeapBitmapInfo>* closed_bitmaps,\n+                              GrowableArray<ArchiveHeapBitmapInfo>* open_bitmaps) {\n+  assert(HeapShared::can_write(), \"sanity\");\n+  allocate_buffer();\n+  copy_source_objs_to_buffer(roots);\n+  set_requested_address_for_regions(closed_regions, open_regions);\n+  relocate_embedded_oops(roots, closed_bitmaps, open_bitmaps);\n+}\n+\n+bool ArchiveHeapWriter::is_too_large_to_archive(oop o) {\n+  return is_too_large_to_archive(o->size());\n+}\n+\n+bool ArchiveHeapWriter::is_string_too_large_to_archive(oop string) {\n+  typeArrayOop value = java_lang_String::value_no_keepalive(string);\n+  return is_too_large_to_archive(value);\n+}\n+\n+bool ArchiveHeapWriter::is_too_large_to_archive(size_t size) {\n+  assert(size > 0, \"no zero-size object\");\n+  assert(size * HeapWordSize > size, \"no overflow\");\n+  static_assert(MIN_GC_REGION_ALIGNMENT > 0, \"must be positive\");\n+\n+  size_t byte_size = size * HeapWordSize;\n+  if (byte_size > size_t(MIN_GC_REGION_ALIGNMENT)) {\n+    return true;\n+  } else {\n+    return false;\n+  }\n+}\n+\n+\/\/ Various lookup functions between source_obj, buffered_obj and requested_obj\n+bool ArchiveHeapWriter::is_in_requested_regions(oop o) {\n+  assert(_requested_open_region_bottom != nullptr, \"do not call before this is initialized\");\n+  assert(_requested_closed_region_bottom != nullptr, \"do not call before this is initialized\");\n+\n+  address a = cast_from_oop<address>(o);\n+  return (_requested_open_region_bottom <= a && a < _requested_open_region_top) ||\n+         (_requested_closed_region_bottom <= a && a < _requested_closed_region_top);\n+}\n+\n+oop ArchiveHeapWriter::requested_obj_from_buffer_offset(size_t offset) {\n+  oop req_obj = cast_to_oop(_requested_open_region_bottom + offset);\n+  assert(is_in_requested_regions(req_obj), \"must be\");\n+  return req_obj;\n+}\n+\n+oop ArchiveHeapWriter::source_obj_to_requested_obj(oop src_obj) {\n+  assert(DumpSharedSpaces, \"dump-time only\");\n+  HeapShared::CachedOopInfo* p = HeapShared::archived_object_cache()->get(src_obj);\n+  if (p != nullptr) {\n+    return requested_obj_from_buffer_offset(p->buffer_offset());\n+  } else {\n+    return nullptr;\n+  }\n+}\n+\n+oop ArchiveHeapWriter::buffered_addr_to_source_obj(address buffered_addr) {\n+  oop* p = _buffer_offset_to_source_obj_table->get(buffered_address_to_offset(buffered_addr));\n+  if (p != nullptr) {\n+    return *p;\n+  } else {\n+    return nullptr;\n+  }\n+}\n+\n+address ArchiveHeapWriter::buffered_addr_to_requested_addr(address buffered_addr) {\n+  return _requested_open_region_bottom + buffered_address_to_offset(buffered_addr);\n+}\n+\n+oop ArchiveHeapWriter::heap_roots_requested_address() {\n+  return requested_obj_from_buffer_offset(_heap_roots_bottom);\n+}\n+\n+address ArchiveHeapWriter::heap_region_requested_bottom(int heap_region_idx) {\n+  assert(_buffer != nullptr, \"must be initialized\");\n+  switch (heap_region_idx) {\n+  case MetaspaceShared::first_closed_heap_region:\n+    return _requested_closed_region_bottom;\n+  case MetaspaceShared::first_open_heap_region:\n+    return _requested_open_region_bottom;\n+  default:\n+    ShouldNotReachHere();\n+    return nullptr;\n+  }\n+}\n+\n+void ArchiveHeapWriter::allocate_buffer() {\n+  int initial_buffer_size = 100000;\n+  _buffer = new GrowableArrayCHeap<u1, mtClassShared>(initial_buffer_size);\n+  _open_bottom = _buffer_top = 0;\n+  ensure_buffer_space(1); \/\/ so that buffer_bottom() works\n+}\n+\n+void ArchiveHeapWriter::ensure_buffer_space(size_t min_bytes) {\n+  \/\/ We usually have very small heaps. If we get a huge one it's probably caused by a bug.\n+  guarantee(min_bytes <= max_jint, \"we dont support archiving more than 2G of objects\");\n+  _buffer->at_grow(to_array_index(min_bytes));\n+}\n+\n+void ArchiveHeapWriter::copy_roots_to_buffer(GrowableArrayCHeap<oop, mtClassShared>* roots) {\n+  Klass* k = Universe::objectArrayKlassObj(); \/\/ already relocated to point to archived klass\n+  int length = roots != nullptr ? roots->length() : 0;\n+  _heap_roots_word_size = objArrayOopDesc::object_size(length);\n+  size_t byte_size = _heap_roots_word_size * HeapWordSize;\n+  if (byte_size >= MIN_GC_REGION_ALIGNMENT) {\n+    log_error(cds, heap)(\"roots array is too large. Please reduce the number of classes\");\n+    vm_exit(1);\n+  }\n+\n+  maybe_fill_gc_region_gap(byte_size);\n+\n+  size_t new_top = _buffer_top + byte_size;\n+  ensure_buffer_space(new_top);\n+\n+  HeapWord* mem = offset_to_buffered_address<HeapWord*>(_buffer_top);\n+  memset(mem, 0, byte_size);\n+  {\n+    \/\/ This is copied from MemAllocator::finish\n+    narrowKlass nk = ArchiveBuilder::current()->get_requested_narrow_klass(k);\n+    oopDesc::release_set_mark(mem, markWord::prototype().set_narrow_klass(nk));\n+  }\n+  {\n+    \/\/ This is copied from ObjArrayAllocator::initialize\n+    arrayOopDesc::set_length(mem, length);\n+  }\n+\n+  objArrayOop arrayOop = objArrayOop(cast_to_oop(mem));\n+  for (int i = 0; i < length; i++) {\n+    \/\/ Do not use arrayOop->obj_at_put(i, o) as arrayOop is outside of the real heap!\n+    oop o = roots->at(i);\n+    if (UseCompressedOops) {\n+      * arrayOop->obj_at_addr<narrowOop>(i) = CompressedOops::encode(o);\n+    } else {\n+      * arrayOop->obj_at_addr<oop>(i) = o;\n+    }\n+  }\n+  log_info(cds)(\"archived obj roots[%d] = \" SIZE_FORMAT \" bytes, klass = %p, obj = %p\", length, byte_size, k, mem);\n+\n+  _heap_roots_bottom = _buffer_top;\n+  _buffer_top = new_top;\n+}\n+\n+void ArchiveHeapWriter::copy_source_objs_to_buffer(GrowableArrayCHeap<oop, mtClassShared>* roots) {\n+  copy_source_objs_to_buffer_by_region(\/*copy_open_region=*\/true);\n+  copy_roots_to_buffer(roots);\n+  _open_top = _buffer_top;\n+\n+  \/\/ Align the closed region to the next G1 region\n+  _buffer_top = _closed_bottom = align_up(_buffer_top, HeapRegion::GrainBytes);\n+  copy_source_objs_to_buffer_by_region(\/*copy_open_region=*\/false);\n+  _closed_top = _buffer_top;\n+\n+  log_info(cds, heap)(\"Size of open region   = \" SIZE_FORMAT \" bytes\", _open_top   - _open_bottom);\n+  log_info(cds, heap)(\"Size of closed region = \" SIZE_FORMAT \" bytes\", _closed_top - _closed_bottom);\n+}\n+\n+void ArchiveHeapWriter::copy_source_objs_to_buffer_by_region(bool copy_open_region) {\n+  for (int i = 0; i < _source_objs->length(); i++) {\n+    oop src_obj = _source_objs->at(i);\n+    HeapShared::CachedOopInfo* info = HeapShared::archived_object_cache()->get(src_obj);\n+    assert(info != nullptr, \"must be\");\n+    if (info->in_open_region() == copy_open_region) {\n+      \/\/ For region-based collectors such as G1, we need to make sure that we don't have\n+      \/\/ an object that can possible span across two regions.\n+      size_t buffer_offset = copy_one_source_obj_to_buffer(src_obj);\n+      info->set_buffer_offset(buffer_offset);\n+\n+      _buffer_offset_to_source_obj_table->put(buffer_offset, src_obj);\n+    }\n+  }\n+}\n+\n+size_t ArchiveHeapWriter::filler_array_byte_size(int length) {\n+  size_t byte_size = objArrayOopDesc::object_size(length) * HeapWordSize;\n+  return byte_size;\n+}\n+\n+int ArchiveHeapWriter::filler_array_length(size_t fill_bytes) {\n+  assert(is_object_aligned(fill_bytes), \"must be\");\n+  size_t elemSize = (UseCompressedOops ? sizeof(narrowOop) : sizeof(oop));\n+\n+  int initial_length = to_array_length(fill_bytes \/ elemSize);\n+  for (int length = initial_length; length >= 0; length --) {\n+    size_t array_byte_size = filler_array_byte_size(length);\n+    if (array_byte_size == fill_bytes) {\n+      return length;\n+    }\n+  }\n+\n+  ShouldNotReachHere();\n+  return -1;\n+}\n+\n+void ArchiveHeapWriter::init_filler_array_at_buffer_top(int array_length, size_t fill_bytes) {\n+  assert(UseCompressedClassPointers, \"Archived heap only supported for compressed klasses\");\n+  Klass* oak = Universe::objectArrayKlassObj(); \/\/ already relocated to point to archived klass\n+  HeapWord* mem = offset_to_buffered_address<HeapWord*>(_buffer_top);\n+  memset(mem, 0, fill_bytes);\n+  narrowKlass nk = ArchiveBuilder::current()->get_requested_narrow_klass(oak);\n+  oopDesc::set_mark(mem, markWord::prototype().set_narrow_klass(nk));\n+  arrayOopDesc::set_length(mem, array_length);\n+}\n+\n+void ArchiveHeapWriter::maybe_fill_gc_region_gap(size_t required_byte_size) {\n+  \/\/ We fill only with arrays (so we don't need to use a single HeapWord filler if the\n+  \/\/ leftover space is smaller than a zero-sized array object). Therefore, we need to\n+  \/\/ make sure there's enough space of min_filler_byte_size in the current region after\n+  \/\/ required_byte_size has been allocated. If not, fill the remainder of the current\n+  \/\/ region.\n+  size_t min_filler_byte_size = filler_array_byte_size(0);\n+  size_t new_top = _buffer_top + required_byte_size + min_filler_byte_size;\n+\n+  const size_t cur_min_region_bottom = align_down(_buffer_top, MIN_GC_REGION_ALIGNMENT);\n+  const size_t next_min_region_bottom = align_down(new_top, MIN_GC_REGION_ALIGNMENT);\n+\n+  if (cur_min_region_bottom != next_min_region_bottom) {\n+    \/\/ Make sure that no objects span across MIN_GC_REGION_ALIGNMENT. This way\n+    \/\/ we can map the region in any region-based collector.\n+    assert(next_min_region_bottom > cur_min_region_bottom, \"must be\");\n+    assert(next_min_region_bottom - cur_min_region_bottom == MIN_GC_REGION_ALIGNMENT,\n+           \"no buffered object can be larger than %d bytes\",  MIN_GC_REGION_ALIGNMENT);\n+\n+    const size_t filler_end = next_min_region_bottom;\n+    const size_t fill_bytes = filler_end - _buffer_top;\n+    assert(fill_bytes > 0, \"must be\");\n+    ensure_buffer_space(filler_end);\n+\n+    int array_length = filler_array_length(fill_bytes);\n+    log_info(cds, heap)(\"Inserting filler obj array of %d elements (\" SIZE_FORMAT \" bytes total) @ buffer offset \" SIZE_FORMAT,\n+                        array_length, fill_bytes, _buffer_top);\n+    init_filler_array_at_buffer_top(array_length, fill_bytes);\n+\n+    _buffer_top = filler_end;\n+  }\n+}\n+\n+size_t ArchiveHeapWriter::copy_one_source_obj_to_buffer(oop src_obj) {\n+  assert(!is_too_large_to_archive(src_obj), \"already checked\");\n+  size_t byte_size = src_obj->size() * HeapWordSize;\n+  assert(byte_size > 0, \"no zero-size objects\");\n+\n+  maybe_fill_gc_region_gap(byte_size);\n+\n+  size_t new_top = _buffer_top + byte_size;\n+  assert(new_top > _buffer_top, \"no wrap around\");\n+\n+  size_t cur_min_region_bottom = align_down(_buffer_top, MIN_GC_REGION_ALIGNMENT);\n+  size_t next_min_region_bottom = align_down(new_top, MIN_GC_REGION_ALIGNMENT);\n+  assert(cur_min_region_bottom == next_min_region_bottom, \"no object should cross minimal GC region boundaries\");\n+\n+  ensure_buffer_space(new_top);\n+\n+  address from = cast_from_oop<address>(src_obj);\n+  address to = offset_to_buffered_address<address>(_buffer_top);\n+  assert(is_object_aligned(_buffer_top), \"sanity\");\n+  assert(is_object_aligned(byte_size), \"sanity\");\n+  memcpy(to, from, byte_size);\n+\n+  size_t buffered_obj_offset = _buffer_top;\n+  _buffer_top = new_top;\n+\n+  return buffered_obj_offset;\n+}\n+\n+void ArchiveHeapWriter::set_requested_address_for_regions(GrowableArray<MemRegion>* closed_regions,\n+                                                          GrowableArray<MemRegion>* open_regions) {\n+  assert(closed_regions->length() == 0, \"must be\");\n+  assert(open_regions->length() == 0, \"must be\");\n+\n+  assert(UseG1GC, \"must be\");\n+  address heap_end = (address)G1CollectedHeap::heap()->reserved().end();\n+  log_info(cds, heap)(\"Heap end = %p\", heap_end);\n+\n+  size_t closed_region_byte_size = _closed_top - _closed_bottom;\n+  size_t open_region_byte_size = _open_top - _open_bottom;\n+  assert(closed_region_byte_size > 0, \"must archived at least one object for closed region!\");\n+  assert(open_region_byte_size > 0, \"must archived at least one object for open region!\");\n+\n+  \/\/ The following two asserts are ensured by copy_source_objs_to_buffer_by_region().\n+  assert(is_aligned(_closed_bottom, HeapRegion::GrainBytes), \"sanity\");\n+  assert(is_aligned(_open_bottom, HeapRegion::GrainBytes), \"sanity\");\n+\n+  _requested_closed_region_bottom = align_down(heap_end - closed_region_byte_size, HeapRegion::GrainBytes);\n+  _requested_open_region_bottom = _requested_closed_region_bottom - (_closed_bottom - _open_bottom);\n+\n+  assert(is_aligned(_requested_closed_region_bottom, HeapRegion::GrainBytes), \"sanity\");\n+  assert(is_aligned(_requested_open_region_bottom, HeapRegion::GrainBytes), \"sanity\");\n+\n+  _requested_open_region_top = _requested_open_region_bottom + (_open_top - _open_bottom);\n+  _requested_closed_region_top = _requested_closed_region_bottom + (_closed_top - _closed_bottom);\n+\n+  assert(_requested_open_region_top <= _requested_closed_region_bottom, \"no overlap\");\n+\n+  closed_regions->append(MemRegion(offset_to_buffered_address<HeapWord*>(_closed_bottom),\n+                                   offset_to_buffered_address<HeapWord*>(_closed_top)));\n+  open_regions->append(  MemRegion(offset_to_buffered_address<HeapWord*>(_open_bottom),\n+                                   offset_to_buffered_address<HeapWord*>(_open_top)));\n+}\n+\n+\/\/ Oop relocation\n+\n+template <typename T> T* ArchiveHeapWriter::requested_addr_to_buffered_addr(T* p) {\n+  assert(is_in_requested_regions(cast_to_oop(p)), \"must be\");\n+\n+  address addr = address(p);\n+  assert(addr >= _requested_open_region_bottom, \"must be\");\n+  size_t offset = addr - _requested_open_region_bottom;\n+  return offset_to_buffered_address<T*>(offset);\n+}\n+\n+template <typename T> oop ArchiveHeapWriter::load_source_oop_from_buffer(T* buffered_addr) {\n+  oop o = load_oop_from_buffer(buffered_addr);\n+  assert(!in_buffer(cast_from_oop<address>(o)), \"must point to source oop\");\n+  return o;\n+}\n+\n+template <typename T> void ArchiveHeapWriter::store_requested_oop_in_buffer(T* buffered_addr,\n+                                                                            oop request_oop) {\n+  assert(is_in_requested_regions(request_oop), \"must be\");\n+  store_oop_in_buffer(buffered_addr, request_oop);\n+}\n+\n+void ArchiveHeapWriter::store_oop_in_buffer(oop* buffered_addr, oop requested_obj) {\n+  \/\/ Make heap content deterministic. See comments inside HeapShared::to_requested_address.\n+  *buffered_addr = HeapShared::to_requested_address(requested_obj);\n+}\n+\n+void ArchiveHeapWriter::store_oop_in_buffer(narrowOop* buffered_addr, oop requested_obj) {\n+  \/\/ Note: HeapShared::to_requested_address() is not necessary because\n+  \/\/ the heap always starts at a deterministic address with UseCompressedOops==true.\n+  narrowOop val = CompressedOops::encode_not_null(requested_obj);\n+  *buffered_addr = val;\n+}\n+\n+oop ArchiveHeapWriter::load_oop_from_buffer(oop* buffered_addr) {\n+  return *buffered_addr;\n+}\n+\n+oop ArchiveHeapWriter::load_oop_from_buffer(narrowOop* buffered_addr) {\n+  return CompressedOops::decode(*buffered_addr);\n+}\n+\n+template <typename T> void ArchiveHeapWriter::relocate_field_in_buffer(T* field_addr_in_buffer) {\n+  oop source_referent = load_source_oop_from_buffer<T>(field_addr_in_buffer);\n+  if (!CompressedOops::is_null(source_referent)) {\n+    oop request_referent = source_obj_to_requested_obj(source_referent);\n+    store_requested_oop_in_buffer<T>(field_addr_in_buffer, request_referent);\n+    mark_oop_pointer<T>(field_addr_in_buffer);\n+  }\n+}\n+\n+template <typename T> void ArchiveHeapWriter::mark_oop_pointer(T* buffered_addr) {\n+  T* request_p = (T*)(buffered_addr_to_requested_addr((address)buffered_addr));\n+  ResourceBitMap* oopmap;\n+  address requested_region_bottom;\n+\n+  if (request_p >= (T*)_requested_closed_region_bottom) {\n+    assert(request_p < (T*)_requested_closed_region_top, \"sanity\");\n+    oopmap = _closed_oopmap;\n+    requested_region_bottom = _requested_closed_region_bottom;\n+  } else {\n+    assert(request_p >= (T*)_requested_open_region_bottom, \"sanity\");\n+    assert(request_p <  (T*)_requested_open_region_top, \"sanity\");\n+    oopmap = _open_oopmap;\n+    requested_region_bottom = _requested_open_region_bottom;\n+  }\n+\n+  \/\/ Mark the pointer in the oopmap\n+  T* region_bottom = (T*)requested_region_bottom;\n+  assert(request_p >= region_bottom, \"must be\");\n+  BitMap::idx_t idx = request_p - region_bottom;\n+  assert(idx < oopmap->size(), \"overflow\");\n+  oopmap->set_bit(idx);\n+}\n+\n+void ArchiveHeapWriter::update_header_for_requested_obj(oop requested_obj, oop src_obj,  Klass* src_klass) {\n+  assert(UseCompressedClassPointers, \"Archived heap only supported for compressed klasses\");\n+  narrowKlass nk = ArchiveBuilder::current()->get_requested_narrow_klass(src_klass);\n+  address buffered_addr = requested_addr_to_buffered_addr(cast_from_oop<address>(requested_obj));\n+\n+  oop fake_oop = cast_to_oop(buffered_addr);\n+\n+  \/\/ We need to retain the identity_hash, because it may have been used by some hashtables\n+  \/\/ in the shared heap. This also has the side effect of pre-initializing the\n+  \/\/ identity_hash for all shared objects, so they are less likely to be written\n+  \/\/ into during run time, increasing the potential of memory sharing.\n+  if (src_obj != nullptr) {\n+    int src_hash = src_obj->identity_hash();\n+    fake_oop->set_mark(markWord::prototype().set_narrow_klass(nk).copy_set_hash(src_hash));\n+    assert(fake_oop->mark().is_unlocked(), \"sanity\");\n+\n+    DEBUG_ONLY(int archived_hash = fake_oop->identity_hash());\n+    assert(src_hash == archived_hash, \"Different hash codes: original %x, archived %x\", src_hash, archived_hash);\n+  }\n+}\n+\n+\/\/ Relocate an element in the buffered copy of HeapShared::roots()\n+template <typename T> void ArchiveHeapWriter::relocate_root_at(oop requested_roots, int index) {\n+  size_t offset = (size_t)((objArrayOop)requested_roots)->obj_at_offset<T>(index);\n+  relocate_field_in_buffer<T>((T*)(buffered_heap_roots_addr() + offset));\n+}\n+\n+class ArchiveHeapWriter::EmbeddedOopRelocator: public BasicOopIterateClosure {\n+  oop _src_obj;\n+  address _buffered_obj;\n+\n+public:\n+  EmbeddedOopRelocator(oop src_obj, address buffered_obj) :\n+    _src_obj(src_obj), _buffered_obj(buffered_obj) {}\n+\n+  void do_oop(narrowOop *p) { EmbeddedOopRelocator::do_oop_work(p); }\n+  void do_oop(      oop *p) { EmbeddedOopRelocator::do_oop_work(p); }\n+\n+private:\n+  template <class T> void do_oop_work(T *p) {\n+    size_t field_offset = pointer_delta(p, _src_obj, sizeof(char));\n+    ArchiveHeapWriter::relocate_field_in_buffer<T>((T*)(_buffered_obj + field_offset));\n+  }\n+};\n+\n+\/\/ Update all oop fields embedded in the buffered objects\n+void ArchiveHeapWriter::relocate_embedded_oops(GrowableArrayCHeap<oop, mtClassShared>* roots,\n+                                               GrowableArray<ArchiveHeapBitmapInfo>* closed_bitmaps,\n+                                               GrowableArray<ArchiveHeapBitmapInfo>* open_bitmaps) {\n+  size_t oopmap_unit = (UseCompressedOops ? sizeof(narrowOop) : sizeof(oop));\n+  size_t closed_region_byte_size = _closed_top - _closed_bottom;\n+  size_t open_region_byte_size   = _open_top   - _open_bottom;\n+  ResourceBitMap closed_oopmap(closed_region_byte_size \/ oopmap_unit);\n+  ResourceBitMap open_oopmap  (open_region_byte_size   \/ oopmap_unit);\n+\n+  _closed_oopmap = &closed_oopmap;\n+  _open_oopmap = &open_oopmap;\n+\n+  auto iterator = [&] (oop src_obj, HeapShared::CachedOopInfo& info) {\n+    oop requested_obj = requested_obj_from_buffer_offset(info.buffer_offset());\n+    update_header_for_requested_obj(requested_obj, src_obj, src_obj->klass());\n+\n+    address buffered_obj = offset_to_buffered_address<address>(info.buffer_offset());\n+    EmbeddedOopRelocator relocator(src_obj, buffered_obj);\n+\n+    src_obj->oop_iterate(&relocator);\n+  };\n+  HeapShared::archived_object_cache()->iterate_all(iterator);\n+\n+  \/\/ Relocate HeapShared::roots(), which is created in copy_roots_to_buffer() and\n+  \/\/ doesn't have a corresponding src_obj, so we can't use EmbeddedOopRelocator on it.\n+  oop requested_roots = requested_obj_from_buffer_offset(_heap_roots_bottom);\n+  update_header_for_requested_obj(requested_roots, nullptr, Universe::objectArrayKlassObj());\n+  int length = roots != nullptr ? roots->length() : 0;\n+  for (int i = 0; i < length; i++) {\n+    if (UseCompressedOops) {\n+      relocate_root_at<narrowOop>(requested_roots, i);\n+    } else {\n+      relocate_root_at<oop>(requested_roots, i);\n+    }\n+  }\n+\n+  closed_bitmaps->append(make_bitmap_info(&closed_oopmap, \/*is_open=*\/false, \/*is_oopmap=*\/true));\n+  open_bitmaps  ->append(make_bitmap_info(&open_oopmap,   \/*is_open=*\/false, \/*is_oopmap=*\/true));\n+\n+  closed_bitmaps->append(compute_ptrmap(\/*is_open=*\/false));\n+  open_bitmaps  ->append(compute_ptrmap(\/*is_open=*\/true));\n+\n+  _closed_oopmap = nullptr;\n+  _open_oopmap = nullptr;\n+}\n+\n+ArchiveHeapBitmapInfo ArchiveHeapWriter::make_bitmap_info(ResourceBitMap* bitmap, bool is_open,  bool is_oopmap) {\n+  size_t size_in_bits = bitmap->size();\n+  size_t size_in_bytes;\n+  uintptr_t* buffer;\n+\n+  if (size_in_bits > 0) {\n+    size_in_bytes = bitmap->size_in_bytes();\n+    buffer = (uintptr_t*)NEW_C_HEAP_ARRAY(char, size_in_bytes, mtInternal);\n+    bitmap->write_to(buffer, size_in_bytes);\n+  } else {\n+    size_in_bytes = 0;\n+    buffer = nullptr;\n+  }\n+\n+  log_info(cds, heap)(\"%s @ \" INTPTR_FORMAT \" (\" SIZE_FORMAT_W(6) \" bytes) for %s heap region\",\n+                      is_oopmap ? \"Oopmap\" : \"Ptrmap\",\n+                      p2i(buffer), size_in_bytes,\n+                      is_open? \"open\" : \"closed\");\n+\n+  ArchiveHeapBitmapInfo info;\n+  info._map = (address)buffer;\n+  info._size_in_bits = size_in_bits;\n+  info._size_in_bytes = size_in_bytes;\n+\n+  return info;\n+}\n+\n+void ArchiveHeapWriter::mark_native_pointer(oop src_obj, int field_offset) {\n+  Metadata* ptr = src_obj->metadata_field_acquire(field_offset);\n+  if (ptr != nullptr) {\n+    NativePointerInfo info;\n+    info._src_obj = src_obj;\n+    info._field_offset = field_offset;\n+    _native_pointers->append(info);\n+  }\n+}\n+\n+ArchiveHeapBitmapInfo ArchiveHeapWriter::compute_ptrmap(bool is_open) {\n+  int num_non_null_ptrs = 0;\n+  Metadata** bottom = (Metadata**) (is_open ? _requested_open_region_bottom: _requested_closed_region_bottom);\n+  Metadata** top = (Metadata**) (is_open ? _requested_open_region_top: _requested_closed_region_top); \/\/ exclusive\n+  ResourceBitMap ptrmap(top - bottom);\n+\n+  for (int i = 0; i < _native_pointers->length(); i++) {\n+    NativePointerInfo info = _native_pointers->at(i);\n+    oop src_obj = info._src_obj;\n+    int field_offset = info._field_offset;\n+    HeapShared::CachedOopInfo* p = HeapShared::archived_object_cache()->get(src_obj);\n+    if (p->in_open_region() == is_open) {\n+      \/\/ requested_field_addr = the address of this field in the requested space\n+      oop requested_obj = requested_obj_from_buffer_offset(p->buffer_offset());\n+      Metadata** requested_field_addr = (Metadata**)(cast_from_oop<address>(requested_obj) + field_offset);\n+      assert(bottom <= requested_field_addr && requested_field_addr < top, \"range check\");\n+\n+      \/\/ Mark this field in the bitmap\n+      BitMap::idx_t idx = requested_field_addr - bottom;\n+      ptrmap.set_bit(idx);\n+      num_non_null_ptrs ++;\n+\n+      \/\/ Set the native pointer to the requested address of the metadata (at runtime, the metadata will have\n+      \/\/ this address if the RO\/RW regions are mapped at the default location).\n+\n+      Metadata** buffered_field_addr = requested_addr_to_buffered_addr(requested_field_addr);\n+      Metadata* native_ptr = *buffered_field_addr;\n+      assert(native_ptr != nullptr, \"sanity\");\n+\n+      address buffered_native_ptr = ArchiveBuilder::current()->get_buffered_addr((address)native_ptr);\n+      address requested_native_ptr = ArchiveBuilder::current()->to_requested(buffered_native_ptr);\n+      *buffered_field_addr = (Metadata*)requested_native_ptr;\n+    }\n+  }\n+\n+  log_info(cds, heap)(\"compute_ptrmap: marked %d non-null native pointers for %s heap region\",\n+                      num_non_null_ptrs, is_open ? \"open\" : \"closed\");\n+\n+  if (num_non_null_ptrs == 0) {\n+    ResourceBitMap empty;\n+    return make_bitmap_info(&empty, is_open, \/*is_oopmap=*\/ false);\n+  } else {\n+    return make_bitmap_info(&ptrmap, is_open, \/*is_oopmap=*\/ false);\n+  }\n+}\n+\n+#endif \/\/ INCLUDE_CDS_JAVA_HEAP\n","filename":"src\/hotspot\/share\/cds\/archiveHeapWriter.cpp","additions":655,"deletions":0,"binary":false,"changes":655,"status":"added"},{"patch":"@@ -28,0 +28,1 @@\n+#include \"cds\/archiveHeapWriter.hpp\"\n@@ -65,1 +66,0 @@\n-#include \"gc\/g1\/heapRegion.hpp\"\n@@ -86,0 +86,1 @@\n+bool HeapShared::_copying_open_region_objects = false;\n@@ -87,1 +88,0 @@\n-GrowableArrayCHeap<Metadata**, mtClassShared>* HeapShared::_native_pointers = nullptr;\n@@ -147,8 +147,0 @@\n-#ifdef ASSERT\n-bool HeapShared::is_archived_object_during_dumptime(oop p) {\n-  assert(HeapShared::can_write(), \"must be\");\n-  assert(DumpSharedSpaces, \"this function is only used with -Xshare:dump\");\n-  return Universe::heap()->is_archived_object(p);\n-}\n-#endif\n-\n@@ -224,2 +216,2 @@\n-HeapShared::OriginalObjectTable* HeapShared::_original_object_table = nullptr;\n-oop HeapShared::find_archived_heap_object(oop obj) {\n+\n+bool HeapShared::has_been_archived(oop obj) {\n@@ -227,7 +219,1 @@\n-  ArchivedObjectCache* cache = archived_object_cache();\n-  CachedOopInfo* p = cache->get(obj);\n-  if (p != nullptr) {\n-    return p->_obj;\n-  } else {\n-    return nullptr;\n-  }\n+  return archived_object_cache()->get(obj) != nullptr;\n@@ -267,12 +253,5 @@\n-  if (DumpSharedSpaces) {\n-    assert(Thread::current() == (Thread*)VMThread::vm_thread(), \"should be in vm thread\");\n-    assert(_pending_roots != nullptr, \"sanity\");\n-    return _pending_roots->at(index);\n-  } else {\n-    assert(UseSharedSpaces, \"must be\");\n-    assert(!_roots.is_empty(), \"must have loaded shared heap\");\n-    oop result = roots()->obj_at(index);\n-    if (clear) {\n-      clear_root(index);\n-    }\n-    return result;\n+  assert(!DumpSharedSpaces && UseSharedSpaces, \"runtime only\");\n+  assert(!_roots.is_empty(), \"must have loaded shared heap\");\n+  oop result = roots()->obj_at(index);\n+  if (clear) {\n+    clear_root(index);\n@@ -280,0 +259,1 @@\n+  return result;\n@@ -294,10 +274,1 @@\n-bool HeapShared::is_too_large_to_archive(oop o) {\n-  \/\/ TODO: To make the CDS heap mappable for all collectors, this function should\n-  \/\/ reject objects that may be too large for *any* collector.\n-  assert(UseG1GC, \"implementation limitation\");\n-  size_t sz = align_up(o->size() * HeapWordSize, ObjectAlignmentInBytes);\n-  size_t max = \/*G1*\/HeapRegion::min_region_size_in_words() * HeapWordSize;\n-  return (sz > max);\n-}\n-\n-oop HeapShared::archive_object(oop obj) {\n+bool HeapShared::archive_object(oop obj) {\n@@ -307,5 +278,2 @@\n-\n-  oop ao = find_archived_heap_object(obj);\n-  if (ao != nullptr) {\n-    \/\/ already archived\n-    return ao;\n+  if (has_been_archived(obj)) {\n+    return true;\n@@ -314,2 +282,1 @@\n-  int len = obj->size();\n-  if (G1CollectedHeap::heap()->is_archive_alloc_too_large(len)) {\n+  if (ArchiveHeapWriter::is_too_large_to_archive(obj->size())) {\n@@ -317,3 +284,14 @@\n-                         p2i(obj), (size_t)obj->size());\n-    return nullptr;\n-  }\n+                         p2i(obj), obj->size());\n+    return false;\n+  } else {\n+    count_allocation(obj->size());\n+    ArchiveHeapWriter::add_source_obj(obj);\n+\n+    \/\/ The archived objects are discovered in a predictable order. Compute\n+    \/\/ their identity_hash() as soon as we see them. This ensures that the\n+    \/\/ the identity_hash in the object header will have a predictable value,\n+    \/\/ making the archive reproducible.\n+    obj->identity_hash();\n+    CachedOopInfo info = make_cached_oop_info();\n+    archived_object_cache()->put(obj, info);\n+    mark_native_pointers(obj);\n@@ -321,31 +299,0 @@\n-  oop archived_oop = cast_to_oop(G1CollectedHeap::heap()->archive_mem_allocate(len));\n-  if (archived_oop != nullptr) {\n-    count_allocation(len);\n-    Copy::aligned_disjoint_words(cast_from_oop<HeapWord*>(obj), cast_from_oop<HeapWord*>(archived_oop), len);\n-    \/\/ Reinitialize markword to remove age\/marking\/locking\/etc.\n-    \/\/\n-    \/\/ We need to retain the identity_hash, because it may have been used by some hashtables\n-    \/\/ in the shared heap. This also has the side effect of pre-initializing the\n-    \/\/ identity_hash for all shared objects, so they are less likely to be written\n-    \/\/ into during run time, increasing the potential of memory sharing.\n-    int hash_original = obj->identity_hash();\n-\n-    assert(SafepointSynchronize::is_at_safepoint(), \"resolving displaced headers only at safepoint\");\n-    markWord mark = obj->mark();\n-    if (mark.has_displaced_mark_helper()) {\n-      mark = mark.displaced_mark_helper();\n-    }\n-    narrowKlass nklass = mark.narrow_klass();\n-    archived_oop->set_mark(markWord::prototype().copy_set_hash(hash_original) LP64_ONLY(.set_narrow_klass(nklass)));\n-    assert(archived_oop->mark().is_unlocked(), \"sanity\");\n-\n-    DEBUG_ONLY(int hash_archived = archived_oop->identity_hash());\n-    assert(hash_original == hash_archived, \"Different hash codes: original %x, archived %x\", hash_original, hash_archived);\n-\n-    ArchivedObjectCache* cache = archived_object_cache();\n-    CachedOopInfo info = make_cached_oop_info(archived_oop);\n-    cache->put(obj, info);\n-    if (_original_object_table != nullptr) {\n-      _original_object_table->put(archived_oop, obj);\n-    }\n-    mark_native_pointers(obj, archived_oop);\n@@ -354,2 +301,2 @@\n-      log_debug(cds, heap)(\"Archived heap object \" PTR_FORMAT \" ==> \" PTR_FORMAT \" : %s\",\n-                           p2i(obj), p2i(archived_oop), obj->klass()->external_name());\n+      log_debug(cds, heap)(\"Archived heap object \" PTR_FORMAT \" : %s\",\n+                           p2i(obj), obj->klass()->external_name());\n@@ -357,7 +304,15 @@\n-  } else {\n-    log_error(cds, heap)(\n-      \"Cannot allocate space for object \" PTR_FORMAT \" in archived heap region\",\n-      p2i(obj));\n-    log_error(cds)(\"Out of memory. Please run with a larger Java heap, current MaxHeapSize = \"\n-        SIZE_FORMAT \"M\", MaxHeapSize\/M);\n-    os::_exit(-1);\n+\n+    if (java_lang_Module::is_instance(obj)) {\n+      if (Modules::check_module_oop(obj)) {\n+        Modules::update_oops_in_archived_module(obj, append_root(obj));\n+      }\n+      java_lang_Module::set_module_entry(obj, nullptr);\n+    } else if (java_lang_ClassLoader::is_instance(obj)) {\n+      \/\/ class_data will be restored explicitly at run time.\n+      guarantee(obj == SystemDictionary::java_platform_loader() ||\n+                obj == SystemDictionary::java_system_loader() ||\n+                java_lang_ClassLoader::loader_data(obj) == nullptr, \"must be\");\n+      java_lang_ClassLoader::release_set_loader_data(obj, nullptr);\n+    }\n+\n+    return true;\n@@ -365,1 +320,0 @@\n-  return archived_oop;\n@@ -435,2 +389,2 @@\n-      oop archived_m = archive_reachable_objects_from(1, _default_subgraph_info, m, \/*is_closed_archive=*\/ false);\n-      assert(archived_m != nullptr, \"sanity\");\n+      bool success = archive_reachable_objects_from(1, _default_subgraph_info, m, \/*is_closed_archive=*\/ false);\n+      assert(success, \"sanity\");\n@@ -439,2 +393,2 @@\n-        \"Archived %s mirror object from \" PTR_FORMAT \" ==> \" PTR_FORMAT,\n-        type2name(bt), p2i(m), p2i(archived_m));\n+        \"Archived %s mirror object from \" PTR_FORMAT,\n+        type2name(bt), p2i(m));\n@@ -442,1 +396,1 @@\n-      Universe::set_archived_basic_type_mirror_index(bt, append_root(archived_m));\n+      Universe::set_archived_basic_type_mirror_index(bt, append_root(m));\n@@ -453,3 +407,3 @@\n-      oop archived_m = archive_reachable_objects_from(1, _default_subgraph_info, m, \/*is_closed_archive=*\/ false);\n-      guarantee(archived_m != nullptr, \"scratch mirrors should not point to any unachivable objects\");\n-      buffered_k->set_archived_java_mirror(append_root(archived_m));\n+      bool success = archive_reachable_objects_from(1, _default_subgraph_info, m, \/*is_closed_archive=*\/ false);\n+      guarantee(success, \"scratch mirrors should not point to any unachivable objects\");\n+      buffered_k->set_archived_java_mirror(append_root(m));\n@@ -458,2 +412,2 @@\n-        \"Archived %s mirror object from \" PTR_FORMAT \" ==> \" PTR_FORMAT,\n-        buffered_k->external_name(), p2i(m), p2i(archived_m));\n+        \"Archived %s mirror object from \" PTR_FORMAT,\n+        buffered_k->external_name(), p2i(m));\n@@ -465,5 +419,5 @@\n-        if (rr != nullptr && !is_too_large_to_archive(rr)) {\n-          oop archived_obj = HeapShared::archive_reachable_objects_from(1, _default_subgraph_info, rr,\n-                                                                        \/*is_closed_archive=*\/false);\n-          assert(archived_obj != nullptr,  \"already checked not too large to archive\");\n-          int root_index = append_root(archived_obj);\n+        if (rr != nullptr && !ArchiveHeapWriter::is_too_large_to_archive(rr)) {\n+          bool success = HeapShared::archive_reachable_objects_from(1, _default_subgraph_info, rr,\n+                                                                    \/*is_closed_archive=*\/false);\n+          assert(success, \"must be\");\n+          int root_index = append_root(rr);\n@@ -479,1 +433,1 @@\n-void HeapShared::mark_native_pointers(oop orig_obj, oop archived_obj) {\n+void HeapShared::mark_native_pointers(oop orig_obj) {\n@@ -481,21 +435,2 @@\n-    mark_one_native_pointer(archived_obj, java_lang_Class::klass_offset());\n-    mark_one_native_pointer(archived_obj, java_lang_Class::array_klass_offset());\n-  }\n-}\n-\n-void HeapShared::mark_one_native_pointer(oop archived_obj, int offset) {\n-  Metadata* ptr = archived_obj->metadata_field_acquire(offset);\n-  if (ptr != nullptr) {\n-    \/\/ Set the native pointer to the requested address (at runtime, if the metadata\n-    \/\/ is mapped at the default location, it will be at this address).\n-    address buffer_addr = ArchiveBuilder::current()->get_buffered_addr((address)ptr);\n-    address requested_addr = ArchiveBuilder::current()->to_requested(buffer_addr);\n-    archived_obj->metadata_field_put(offset, (Metadata*)requested_addr);\n-\n-    \/\/ Remember this pointer. At runtime, if the metadata is mapped at a non-default\n-    \/\/ location, the pointer needs to be patched (see ArchiveHeapLoader::patch_native_pointers()).\n-    _native_pointers->append(archived_obj->field_addr<Metadata*>(offset));\n-\n-    log_debug(cds, heap, mirror)(\n-        \"Marked metadata field at %d: \" PTR_FORMAT \" ==> \" PTR_FORMAT,\n-         offset, p2i(ptr), p2i(requested_addr));\n+    ArchiveHeapWriter::mark_native_pointer(orig_obj, java_lang_Class::klass_offset());\n+    ArchiveHeapWriter::mark_native_pointer(orig_obj, java_lang_Class::array_klass_offset());\n@@ -528,0 +463,1 @@\n+  assert(level > 1, \"must never be called at the first (outermost) level\");\n@@ -555,3 +491,4 @@\n-        oop archived_oop_field = archive_reachable_objects_from(level, subgraph_info, oop_field, is_closed_archive);\n-        int root_index = append_root(archived_oop_field);\n-        log_info(cds, heap)(\"Archived enum obj @%d %s::%s (\" INTPTR_FORMAT \" -> \" INTPTR_FORMAT \")\",\n+        bool success = archive_reachable_objects_from(level, subgraph_info, oop_field, is_closed_archive);\n+        assert(success, \"VM should have exited with unarchivable objects for _level > 1\");\n+        int root_index = append_root(oop_field);\n+        log_info(cds, heap)(\"Archived enum obj @%d %s::%s (\" INTPTR_FORMAT \")\",\n@@ -559,1 +496,1 @@\n-                            p2i((oopDesc*)oop_field), p2i((oopDesc*)archived_oop_field));\n+                            p2i((oopDesc*)oop_field));\n@@ -593,23 +530,3 @@\n-void HeapShared::run_full_gc_in_vm_thread() {\n-  if (HeapShared::can_write()) {\n-    \/\/ Avoid fragmentation while archiving heap objects.\n-    \/\/ We do this inside a safepoint, so that no further allocation can happen after GC\n-    \/\/ has finished.\n-    if (GCLocker::is_active()) {\n-      \/\/ Just checking for safety ...\n-      \/\/ This should not happen during -Xshare:dump. If you see this, probably the Java core lib\n-      \/\/ has been modified such that JNI code is executed in some clean up threads after\n-      \/\/ we have finished class loading.\n-      log_warning(cds)(\"GC locker is held, unable to start extra compacting GC. This may produce suboptimal results.\");\n-    } else {\n-      log_info(cds)(\"Run GC ...\");\n-      Universe::heap()->collect_as_vm_thread(GCCause::_archive_time_gc);\n-      log_info(cds)(\"Run GC done\");\n-    }\n-  }\n-}\n-\n-                                 GrowableArray<MemRegion>* open_regions) {\n-\n-  G1HeapVerifier::verify_ready_for_archiving();\n-\n+                                 GrowableArray<MemRegion>* open_regions,\n+                                 GrowableArray<ArchiveHeapBitmapInfo>* closed_bitmaps,\n+                                 GrowableArray<ArchiveHeapBitmapInfo>* open_bitmaps) {\n@@ -623,1 +540,1 @@\n-    create_archived_object_cache(log_is_enabled(Info, cds, map));\n+    create_archived_object_cache();\n@@ -631,1 +548,3 @@\n-    copy_closed_objects(closed_regions);\n+    copy_closed_objects();\n+\n+    _copying_open_region_objects = true;\n@@ -634,1 +553,1 @@\n-    copy_open_objects(open_regions);\n+    copy_open_objects();\n@@ -640,1 +559,1 @@\n-  G1HeapVerifier::verify_archive_regions();\n+  ArchiveHeapWriter::write(_pending_roots, closed_regions, open_regions, closed_bitmaps, open_bitmaps);\n@@ -649,5 +568,4 @@\n-    typeArrayOop value = java_lang_String::value_no_keepalive(s);\n-    if (!HeapShared::is_too_large_to_archive(value)) {\n-      oop archived_s = archive_reachable_objects_from(1, _default_subgraph_info,\n-                                                      s, \/*is_closed_archive=*\/true);\n-      assert(archived_s != nullptr, \"already checked not too large to archive\");\n+    if (!ArchiveHeapWriter::is_string_too_large_to_archive(s)) {\n+      bool success = archive_reachable_objects_from(1, _default_subgraph_info,\n+                                                    s, \/*is_closed_archive=*\/true);\n+      assert(success, \"must be\");\n@@ -656,1 +574,1 @@\n-      java_lang_String::set_deduplication_forbidden(archived_s);\n+      java_lang_String::set_deduplication_forbidden(s);\n@@ -664,1 +582,1 @@\n-void HeapShared::copy_closed_objects(GrowableArray<MemRegion>* closed_regions) {\n+void HeapShared::copy_closed_objects() {\n@@ -667,2 +585,0 @@\n-  G1CollectedHeap::heap()->begin_archive_alloc_range();\n-\n@@ -675,3 +591,0 @@\n-\n-  G1CollectedHeap::heap()->end_archive_alloc_range(closed_regions,\n-                                                   os::vm_allocation_granularity());\n@@ -680,1 +593,1 @@\n-void HeapShared::copy_open_objects(GrowableArray<MemRegion>* open_regions) {\n+void HeapShared::copy_open_objects() {\n@@ -683,2 +596,0 @@\n-  G1CollectedHeap::heap()->begin_archive_alloc_range(true \/* open *\/);\n-\n@@ -696,39 +607,0 @@\n-\n-  copy_roots();\n-\n-  G1CollectedHeap::heap()->end_archive_alloc_range(open_regions,\n-                                                   os::vm_allocation_granularity());\n-}\n-\n-\/\/ Copy _pending_archive_roots into an objArray\n-void HeapShared::copy_roots() {\n-  \/\/ HeapShared::roots() points into an ObjArray in the open archive region. A portion of the\n-  \/\/ objects in this array are discovered during HeapShared::archive_objects(). For example,\n-  \/\/ in HeapShared::archive_reachable_objects_from() ->  HeapShared::check_enum_obj().\n-  \/\/ However, HeapShared::archive_objects() happens inside a safepoint, so we can't\n-  \/\/ allocate a \"regular\" ObjArray and pass the result to HeapShared::archive_object().\n-  \/\/ Instead, we have to roll our own alloc\/copy routine here.\n-  int length = _pending_roots != nullptr ? _pending_roots->length() : 0;\n-  size_t size = objArrayOopDesc::object_size(length);\n-  Klass* k = Universe::objectArrayKlassObj(); \/\/ already relocated to point to archived klass\n-  HeapWord* mem = G1CollectedHeap::heap()->archive_mem_allocate(size);\n-\n-  memset(mem, 0, size * BytesPerWord);\n-  {\n-    \/\/ This is copied from MemAllocator::finish\n-    oopDesc::set_mark(mem, k->prototype_header());\n-#ifndef _LP64\n-    oopDesc::release_set_klass(mem, k);\n-#endif\n-  }\n-  {\n-    \/\/ This is copied from ObjArrayAllocator::initialize\n-    arrayOopDesc::set_length(mem, length);\n-  }\n-\n-  _roots = OopHandle(Universe::vm_global(), cast_to_oop(mem));\n-  for (int i = 0; i < length; i++) {\n-    roots()->obj_at_put(i, _pending_roots->at(i));\n-  }\n-  log_info(cds)(\"archived obj roots[%d] = \" SIZE_FORMAT \" words, klass = %p, obj = %p\", length, size, k, mem);\n-  count_allocation(roots()->size());\n@@ -998,1 +870,3 @@\n-    roots_oop = roots();\n+    if (HeapShared::can_write()) {\n+      roots_oop = ArchiveHeapWriter::heap_roots_requested_address();\n+    }\n@@ -1236,2 +1110,1 @@\n-  oop _orig_referencing_obj;\n-  oop _archived_referencing_obj;\n+  oop _referencing_obj;\n@@ -1248,1 +1121,1 @@\n-                           oop orig, oop archived) :\n+                           oop orig) :\n@@ -1252,1 +1125,1 @@\n-    _orig_referencing_obj(orig), _archived_referencing_obj(archived) {\n+    _referencing_obj(orig) {\n@@ -1266,5 +1139,1 @@\n-      assert(!HeapShared::is_archived_object_during_dumptime(obj),\n-             \"original objects must not point to archived objects\");\n-\n-      size_t field_delta = pointer_delta(p, _orig_referencing_obj, sizeof(char));\n-      T* new_p = (T*)(cast_from_oop<address>(_archived_referencing_obj) + field_delta);\n+      size_t field_delta = pointer_delta(p, _referencing_obj, sizeof(char));\n@@ -1275,1 +1144,1 @@\n-                             _orig_referencing_obj->klass()->external_name(), field_delta,\n+                             _referencing_obj->klass()->external_name(), field_delta,\n@@ -1284,1 +1153,1 @@\n-      oop archived = HeapShared::archive_reachable_objects_from(\n+      bool success = HeapShared::archive_reachable_objects_from(\n@@ -1286,9 +1155,1 @@\n-      assert(archived != nullptr, \"VM should have exited with unarchivable objects for _level > 1\");\n-      assert(HeapShared::is_archived_object_during_dumptime(archived), \"must be\");\n-\n-      if (!_record_klasses_only) {\n-        \/\/ Update the reference in the archived copy of the referencing object.\n-        log_debug(cds, heap)(\"(%d) updating oop @[\" PTR_FORMAT \"] \" PTR_FORMAT \" ==> \" PTR_FORMAT,\n-                             _level, p2i(new_p), p2i(obj), p2i(archived));\n-        RawAccess<IS_NOT_NULL>::oop_store(new_p, archived);\n-      }\n+      assert(success, \"VM should have exited with unarchivable objects for _level > 1\");\n@@ -1300,1 +1161,1 @@\n-  oop orig_referencing_obj()                  { return _orig_referencing_obj; }\n+  oop referencing_obj()                       { return _referencing_obj;      }\n@@ -1306,2 +1167,1 @@\n-HeapShared::CachedOopInfo HeapShared::make_cached_oop_info(oop orig_obj) {\n-  CachedOopInfo info;\n+HeapShared::CachedOopInfo HeapShared::make_cached_oop_info() {\n@@ -1309,6 +1169,2 @@\n-\n-  info._subgraph_info = (walker == nullptr) ? nullptr : walker->subgraph_info();\n-  info._referrer = (walker == nullptr) ? nullptr : walker->orig_referencing_obj();\n-  info._obj = orig_obj;\n-\n-  return info;\n+  oop referrer = (walker == nullptr) ? nullptr : walker->referencing_obj();\n+  return CachedOopInfo(referrer, _copying_open_region_objects);\n@@ -1337,4 +1193,4 @@\n-oop HeapShared::archive_reachable_objects_from(int level,\n-                                               KlassSubGraphInfo* subgraph_info,\n-                                               oop orig_obj,\n-                                               bool is_closed_archive) {\n+bool HeapShared::archive_reachable_objects_from(int level,\n+                                                KlassSubGraphInfo* subgraph_info,\n+                                                oop orig_obj,\n+                                                bool is_closed_archive) {\n@@ -1342,1 +1198,0 @@\n-  assert(!is_archived_object_during_dumptime(orig_obj), \"sanity\");\n@@ -1363,7 +1218,0 @@\n-  oop archived_obj = find_archived_heap_object(orig_obj);\n-  if (java_lang_String::is_instance(orig_obj) && archived_obj != nullptr) {\n-    \/\/ To save time, don't walk strings that are already archived. They just contain\n-    \/\/ pointers to a type array, whose klass doesn't need to be recorded.\n-    return archived_obj;\n-  }\n-\n@@ -1372,1 +1220,1 @@\n-    return archived_obj;\n+    return true;\n@@ -1377,2 +1225,3 @@\n-  bool record_klasses_only = (archived_obj != nullptr);\n-  if (archived_obj == nullptr) {\n+  bool already_archived = has_been_archived(orig_obj);\n+  bool record_klasses_only = already_archived;\n+  if (!already_archived) {\n@@ -1380,2 +1229,1 @@\n-    archived_obj = archive_object(orig_obj);\n-    if (archived_obj == nullptr) {\n+    if (!archive_object(orig_obj)) {\n@@ -1391,1 +1239,1 @@\n-        return nullptr;\n+        return false;\n@@ -1399,13 +1247,0 @@\n-\n-    if (java_lang_Module::is_instance(orig_obj)) {\n-      if (Modules::check_module_oop(orig_obj)) {\n-        Modules::update_oops_in_archived_module(orig_obj, append_root(archived_obj));\n-      }\n-      java_lang_Module::set_module_entry(archived_obj, nullptr);\n-    } else if (java_lang_ClassLoader::is_instance(orig_obj)) {\n-      \/\/ class_data will be restored explicitly at run time.\n-      guarantee(orig_obj == SystemDictionary::java_platform_loader() ||\n-                orig_obj == SystemDictionary::java_system_loader() ||\n-                java_lang_ClassLoader::loader_data(orig_obj) == nullptr, \"must be\");\n-      java_lang_ClassLoader::release_set_loader_data(archived_obj, nullptr);\n-    }\n@@ -1414,1 +1249,0 @@\n-  assert(archived_obj != nullptr, \"must be\");\n@@ -1419,1 +1253,1 @@\n-                                  subgraph_info, orig_obj, archived_obj);\n+                                  subgraph_info, orig_obj);\n@@ -1426,1 +1260,1 @@\n-  return archived_obj;\n+  return true;\n@@ -1485,1 +1319,1 @@\n-    oop af = archive_reachable_objects_from(1, subgraph_info, f, is_closed_archive);\n+    bool success = archive_reachable_objects_from(1, subgraph_info, f, is_closed_archive);\n@@ -1487,1 +1321,1 @@\n-    if (af == nullptr) {\n+    if (!success) {\n@@ -1494,2 +1328,2 @@\n-      subgraph_info->add_subgraph_entry_field(field_offset, af, is_closed_archive);\n-      log_info(cds, heap)(\"Archived field %s::%s => \" PTR_FORMAT, klass_name, field_name, p2i(af));\n+      subgraph_info->add_subgraph_entry_field(field_offset, f, is_closed_archive);\n+      log_info(cds, heap)(\"Archived field %s::%s => \" PTR_FORMAT, klass_name, field_name, p2i(f));\n@@ -1506,5 +1340,0 @@\n- private:\n-  bool _is_archived;\n-\n-  VerifySharedOopClosure(bool is_archived) : _is_archived(is_archived) {}\n-\n@@ -1519,1 +1348,1 @@\n-      HeapShared::verify_reachable_objects_from(obj, _is_archived);\n+      HeapShared::verify_reachable_objects_from(obj);\n@@ -1536,2 +1365,1 @@\n-  oop archived_obj = find_archived_heap_object(orig_obj);\n-  if (archived_obj == nullptr) {\n+  if (!has_been_archived(orig_obj)) {\n@@ -1545,1 +1373,1 @@\n-  verify_reachable_objects_from(orig_obj, false);\n+  verify_reachable_objects_from(orig_obj);\n@@ -1547,8 +1375,0 @@\n-\n-  \/\/ Note: we could also verify that all objects reachable from the archived\n-  \/\/ copy of orig_obj can only point to archived objects, with:\n-  \/\/      init_seen_objects_table();\n-  \/\/      verify_reachable_objects_from(archived_obj, true);\n-  \/\/      init_seen_objects_table();\n-  \/\/ but that's already done in G1HeapVerifier::verify_archive_regions so we\n-  \/\/ won't do it here.\n@@ -1557,1 +1377,1 @@\n-void HeapShared::verify_reachable_objects_from(oop obj, bool is_archived) {\n+void HeapShared::verify_reachable_objects_from(oop obj) {\n@@ -1561,10 +1381,2 @@\n-\n-    if (is_archived) {\n-      assert(is_archived_object_during_dumptime(obj), \"must be\");\n-      assert(find_archived_heap_object(obj) == nullptr, \"must be\");\n-    } else {\n-      assert(!is_archived_object_during_dumptime(obj), \"must be\");\n-      assert(find_archived_heap_object(obj) != nullptr, \"must be\");\n-    }\n-\n-    VerifySharedOopClosure walker(is_archived);\n+    assert(has_been_archived(obj), \"must be\");\n+    VerifySharedOopClosure walker;\n@@ -1824,1 +1636,0 @@\n-    _native_pointers = new GrowableArrayCHeap<Metadata**, mtClassShared>(2048);\n@@ -1890,0 +1701,1 @@\n+  assert(!ArchiveHeapWriter::is_string_too_large_to_archive(string), \"must be\");\n@@ -1894,0 +1706,1 @@\n+#ifndef PRODUCT\n@@ -1925,4 +1738,0 @@\n-      if (DumpSharedSpaces) {\n-        \/\/ Make heap content deterministic.\n-        *p = HeapShared::to_requested_address(*p);\n-      }\n@@ -1936,1 +1745,1 @@\n-\n+#endif\n@@ -1965,0 +1774,1 @@\n+#ifndef PRODUCT\n@@ -1972,1 +1782,0 @@\n-  ArchiveBuilder* builder = DumpSharedSpaces ? ArchiveBuilder::current() : nullptr;\n@@ -1979,3 +1788,0 @@\n-    if (DumpSharedSpaces) {\n-      builder->relocate_klass_ptr_of_oop(o);\n-    }\n@@ -1990,28 +1796,1 @@\n-\n-ResourceBitMap HeapShared::calculate_ptrmap(MemRegion region) {\n-  size_t num_bits = region.byte_size() \/ sizeof(Metadata*);\n-  ResourceBitMap oopmap(num_bits);\n-\n-  Metadata** start = (Metadata**)region.start();\n-  Metadata** end   = (Metadata**)region.end();\n-\n-  int num_non_null_ptrs = 0;\n-  int len = _native_pointers->length();\n-  for (int i = 0; i < len; i++) {\n-    Metadata** p = _native_pointers->at(i);\n-    if (start <= p && p < end) {\n-      assert(*p != nullptr, \"must be non-null\");\n-      num_non_null_ptrs ++;\n-      size_t idx = p - start;\n-      oopmap.set_bit(idx);\n-    }\n-  }\n-\n-  log_info(cds, heap)(\"calculate_ptrmap: marked %d non-null native pointers out of \"\n-                      SIZE_FORMAT \" possible locations\", num_non_null_ptrs, num_bits);\n-  if (num_non_null_ptrs > 0) {\n-    return oopmap;\n-  } else {\n-    return ResourceBitMap(0);\n-  }\n-}\n+#endif \/\/ !PRODUCT\n","filename":"src\/hotspot\/share\/cds\/heapShared.cpp","additions":127,"deletions":348,"binary":false,"changes":475,"status":"modified"},{"patch":"@@ -1457,5 +1457,3 @@\n-    if (!info.is_excluded()) {\n-      bool created;\n-      _cloned_table->put_if_absent(k, info, &created);\n-      assert(created, \"must be\");\n-    }\n+    bool created;\n+    _cloned_table->put_if_absent(k, info, &created);\n+    assert(created, \"must be\");\n","filename":"src\/hotspot\/share\/classfile\/systemDictionaryShared.cpp","additions":3,"deletions":5,"binary":false,"changes":8,"status":"modified"},{"patch":"@@ -53,1 +53,0 @@\n-#include \"gc\/g1\/g1HotCardCache.hpp\"\n@@ -495,34 +494,0 @@\n-void G1CollectedHeap::begin_archive_alloc_range(bool open) {\n-  assert_at_safepoint_on_vm_thread();\n-  assert(_archive_allocator == nullptr, \"should not be initialized\");\n-  _archive_allocator = G1ArchiveAllocator::create_allocator(this, open);\n-}\n-\n-bool G1CollectedHeap::is_archive_alloc_too_large(size_t word_size) {\n-  \/\/ Allocations in archive regions cannot be of a size that would be considered\n-  \/\/ humongous even for a minimum-sized region, because G1 region sizes\/boundaries\n-  \/\/ may be different at archive-restore time.\n-  return word_size >= humongous_threshold_for(HeapRegion::min_region_size_in_words());\n-}\n-\n-HeapWord* G1CollectedHeap::archive_mem_allocate(size_t word_size) {\n-  assert_at_safepoint_on_vm_thread();\n-  assert(_archive_allocator != nullptr, \"_archive_allocator not initialized\");\n-  if (is_archive_alloc_too_large(word_size)) {\n-    return nullptr;\n-  }\n-  return _archive_allocator->archive_mem_allocate(word_size);\n-}\n-\n-void G1CollectedHeap::end_archive_alloc_range(GrowableArray<MemRegion>* ranges,\n-                                              size_t end_alignment_in_bytes) {\n-  assert_at_safepoint_on_vm_thread();\n-  assert(_archive_allocator != nullptr, \"_archive_allocator not initialized\");\n-\n-  \/\/ Call complete_archive to do the real work, filling in the MemRegion\n-  \/\/ array with the archive regions.\n-  _archive_allocator->complete_archive(ranges, end_alignment_in_bytes);\n-  delete _archive_allocator;\n-  _archive_allocator = nullptr;\n-}\n-\n@@ -1015,1 +980,1 @@\n-void G1CollectedHeap::prepare_heap_for_mutators() {\n+void G1CollectedHeap::prepare_for_mutator_after_full_collection() {\n@@ -1030,2 +995,0 @@\n-  \/\/ Start a new incremental collection set for the next pause\n-\n@@ -1040,4 +1003,0 @@\n-  if (G1HotCardCache::use_cache()) {\n-    _hot_card_cache->reset_hot_cache();\n-  }\n-\n@@ -1428,1 +1387,0 @@\n-  _archive_allocator(nullptr),\n@@ -1445,1 +1403,0 @@\n-  _hot_card_cache(NULL),\n@@ -1585,3 +1542,0 @@\n-  \/\/ Create the hot card cache.\n-  _hot_card_cache = new G1HotCardCache(this);\n-\n@@ -1610,1 +1564,1 @@\n-  \/\/ Create storage for the BOT, card table, card counts table (hot card cache) and the bitmap.\n+  \/\/ Create storage for the BOT, card table and the bitmap.\n@@ -1621,5 +1575,0 @@\n-  G1RegionToSpaceMapper* card_counts_storage =\n-    create_aux_memory_mapper(\"Card Counts Table\",\n-                             G1CardCounts::compute_size(heap_rs.size() \/ HeapWordSize),\n-                             G1CardCounts::heap_map_factor());\n-\n@@ -1630,1 +1579,1 @@\n-  _hrm.initialize(heap_storage, bitmap_storage, bot_storage, cardtable_storage, card_counts_storage);\n+  _hrm.initialize(heap_storage, bitmap_storage, bot_storage, cardtable_storage);\n@@ -1633,3 +1582,0 @@\n-  \/\/ Do later initialization work for concurrent refinement.\n-  _hot_card_cache->initialize(card_counts_storage);\n-\n@@ -1646,1 +1592,1 @@\n-  _rem_set = new G1RemSet(this, _card_table, _hot_card_cache);\n+  _rem_set = new G1RemSet(this, _card_table);\n@@ -1826,4 +1772,0 @@\n-void G1CollectedHeap::iterate_hcc_closure(G1CardTableEntryClosure* cl, uint worker_id) {\n-  _hot_card_cache->drain(cl, worker_id);\n-}\n-\n@@ -1833,1 +1775,0 @@\n-  assert(_archive_allocator == nullptr, \"must be, should not contribute to used\");\n@@ -1873,26 +1814,0 @@\n-#ifndef PRODUCT\n-void G1CollectedHeap::allocate_dummy_regions() {\n-  \/\/ Let's fill up most of the region\n-  size_t word_size = HeapRegion::GrainWords - 1024;\n-  \/\/ And as a result the region we'll allocate will be humongous.\n-  guarantee(is_humongous(word_size), \"sanity\");\n-\n-  \/\/ _filler_array_max_size is set to humongous object threshold\n-  \/\/ but temporarily change it to use CollectedHeap::fill_with_object().\n-  AutoModifyRestore<size_t> temporarily(_filler_array_max_size, word_size);\n-\n-  for (uintx i = 0; i < G1DummyRegionsPerGC; ++i) {\n-    \/\/ Let's use the existing mechanism for the allocation\n-    HeapWord* dummy_obj = humongous_obj_allocate(word_size);\n-    if (dummy_obj != NULL) {\n-      MemRegion mr(dummy_obj, word_size);\n-      CollectedHeap::fill_with_object(mr);\n-    } else {\n-      \/\/ If we can't allocate once, we probably cannot allocate\n-      \/\/ again. Let's get out of the loop.\n-      break;\n-    }\n-  }\n-}\n-#endif \/\/ !PRODUCT\n-\n@@ -2638,12 +2553,0 @@\n-class VerifyRegionRemSetClosure : public HeapRegionClosure {\n-  public:\n-    bool do_heap_region(HeapRegion* hr) {\n-      if (!hr->is_archive() && !hr->is_continues_humongous()) {\n-        hr->verify_rem_set();\n-      }\n-      return false;\n-    }\n-};\n-\n-  double start = os::elapsedTime();\n-\n@@ -2661,2 +2564,0 @@\n-\n-  phase_times()->record_start_new_cset_time_ms((os::elapsedTime() - start) * 1000.0);\n@@ -2683,5 +2584,0 @@\n-  if (VerifyRememberedSets) {\n-    log_info(gc, verify)(\"[Verifying RemSets before GC]\");\n-    VerifyRegionRemSetClosure v_cl;\n-    heap_region_iterate(&v_cl);\n-  }\n@@ -2698,5 +2594,0 @@\n-  if (VerifyRememberedSets) {\n-    log_info(gc, verify)(\"[Verifying RemSets after GC]\");\n-    VerifyRegionRemSetClosure v_cl;\n-    heap_region_iterate(&v_cl);\n-  }\n@@ -2772,1 +2663,1 @@\n-void G1CollectedHeap::prepare_tlabs_for_mutator() {\n+void G1CollectedHeap::prepare_for_mutator_after_young_collection() {\n@@ -2778,2 +2669,2 @@\n-  allocate_dummy_regions();\n-\n+  \/\/ Start a new incremental collection set for the mutator phase.\n+  start_new_collection_set();\n@@ -2782,3 +2673,1 @@\n-  resize_all_tlabs();\n-\n-  phase_times()->record_resize_tlab_time_ms((Ticks::now() - start).seconds() * 1000.0);\n+  phase_times()->record_prepare_for_mutator_time_ms((Ticks::now() - start).seconds() * 1000.0);\n@@ -2887,7 +2776,0 @@\n-  \/\/ Clear the card counts for this region.\n-  \/\/ Note: we only need to do this if the region is not young\n-  \/\/ (since we don't refine cards in young regions).\n-  if (!hr->is_young()) {\n-    _hot_card_cache->reset_card_counts(hr);\n-  }\n-\n@@ -3114,1 +2996,0 @@\n-    assert(_archive_allocator == nullptr, \"must be, should not contribute to used\");\n@@ -3313,2 +3194,0 @@\n-\n-    assert(_archive_allocator == nullptr, \"must be, should not contribute to used\");\n@@ -3322,4 +3201,0 @@\n-void G1CollectedHeap::reset_hot_card_cache() {\n-  _hot_card_cache->reset_hot_cache();\n-}\n-\n","filename":"src\/hotspot\/share\/gc\/g1\/g1CollectedHeap.cpp","additions":8,"deletions":133,"binary":false,"changes":141,"status":"modified"},{"patch":"@@ -60,0 +60,1 @@\n+#include \"runtime\/threadSMR.hpp\"\n@@ -69,1 +70,0 @@\n-class G1ArchiveAllocator;\n@@ -78,1 +78,0 @@\n-class G1HotCardCache;\n@@ -124,0 +123,26 @@\n+\/\/ Helper to claim contiguous sets of JavaThread for processing by multiple threads.\n+class G1JavaThreadsListClaimer : public StackObj {\n+  ThreadsListHandle _list;\n+  uint _claim_step;\n+\n+  volatile uint _cur_claim;\n+\n+  \/\/ Attempts to claim _claim_step JavaThreads, returning an array of claimed\n+  \/\/ JavaThread* with count elements. Returns null (and a zero count) if there\n+  \/\/ are no more threads to claim.\n+  JavaThread* const* claim(uint& count);\n+\n+public:\n+  G1JavaThreadsListClaimer(uint claim_step) : _list(), _claim_step(claim_step), _cur_claim(0) {\n+    assert(claim_step > 0, \"must be\");\n+  }\n+\n+  \/\/ Executes the given closure on the elements of the JavaThread list, chunking the\n+  \/\/ JavaThread set in claim_step chunks for each caller to reduce parallelization\n+  \/\/ overhead.\n+  void apply(ThreadClosure* cl);\n+\n+  \/\/ Total number of JavaThreads that can be claimed.\n+  uint length() const { return _list.length(); }\n+};\n+\n@@ -222,3 +247,0 @@\n-  \/\/ Class that handles archive allocation ranges.\n-  G1ArchiveAllocator* _archive_allocator;\n-\n@@ -283,8 +305,0 @@\n-  \/\/ This is a non-product method that is helpful for testing. It is\n-  \/\/ called at the end of a GC and artificially expands the heap by\n-  \/\/ allocating a number of dead regions. This way we can induce very\n-  \/\/ frequent marking cycles and stress the cleanup \/ concurrent\n-  \/\/ cleanup code more (as all the regions that will be allocated by\n-  \/\/ this method will be found dead by the marking cycle).\n-  void allocate_dummy_regions() PRODUCT_RETURN;\n-\n@@ -501,1 +515,1 @@\n-  void prepare_heap_for_mutators();\n+  void prepare_for_mutator_after_full_collection();\n@@ -692,20 +706,0 @@\n-  \/\/ Facility for allocating in 'archive' regions in high heap memory and\n-  \/\/ recording the allocated ranges. These should all be called from the\n-  \/\/ VM thread at safepoints, without the heap lock held. They can be used\n-  \/\/ to create and archive a set of heap regions which can be mapped at the\n-  \/\/ same fixed addresses in a subsequent JVM invocation.\n-  void begin_archive_alloc_range(bool open = false);\n-\n-  \/\/ Check if the requested size would be too large for an archive allocation.\n-  bool is_archive_alloc_too_large(size_t word_size);\n-\n-  \/\/ Allocate memory of the requested size from the archive region. This will\n-  \/\/ return NULL if the size is too large or if no memory is available. It\n-  \/\/ does not trigger a garbage collection.\n-  HeapWord* archive_mem_allocate(size_t word_size);\n-\n-  \/\/ Optionally aligns the end address and returns the allocated ranges in\n-  \/\/ an array of MemRegions in order of ascending addresses.\n-  void end_archive_alloc_range(GrowableArray<MemRegion>* ranges,\n-                               size_t end_alignment_in_bytes = 0);\n-\n@@ -781,1 +775,1 @@\n-  void prepare_tlabs_for_mutator();\n+  void prepare_for_mutator_after_young_collection();\n@@ -790,3 +784,0 @@\n-  \/\/ The hot card cache for remembered set insertion optimization.\n-  G1HotCardCache* _hot_card_cache;\n-\n@@ -946,3 +937,0 @@\n-  \/\/ Apply the given closure on all cards in the Hot Card Cache, emptying it.\n-  void iterate_hcc_closure(G1CardTableEntryClosure* cl, uint worker_id);\n-\n@@ -1076,2 +1064,0 @@\n-  G1HotCardCache* hot_card_cache() const { return _hot_card_cache; }\n-\n@@ -1279,4 +1265,0 @@\n-  \/\/ Reset and re-enable the hot card cache.\n-  \/\/ Note the counts for the cards in the regions in the\n-  \/\/ collection set are reset when the collection set is freed.\n-  void reset_hot_card_cache();\n","filename":"src\/hotspot\/share\/gc\/g1\/g1CollectedHeap.hpp","additions":29,"deletions":47,"binary":false,"changes":76,"status":"modified"},{"patch":"@@ -2,1 +2,1 @@\n- * Copyright (c) 2017, 2022, Oracle and\/or its affiliates. All rights reserved.\n+ * Copyright (c) 2017, 2023, Oracle and\/or its affiliates. All rights reserved.\n@@ -37,0 +37,1 @@\n+#include \"gc\/g1\/g1FullGCResetMetadataTask.hpp\"\n@@ -121,0 +122,1 @@\n+    _has_compaction_targets(false),\n@@ -211,1 +213,2 @@\n-  phase3_adjust_pointers();\n+  if (has_compaction_targets()) {\n+    phase3_adjust_pointers();\n@@ -213,1 +216,8 @@\n-  phase4_do_compaction();\n+    phase4_do_compaction();\n+  } else {\n+    \/\/ All regions have a high live ratio thus will not be compacted.\n+    \/\/ The live ratio is only considered if do_maximal_compaction is false.\n+    log_info(gc, phases) (\"No Regions selected for compaction. Skipping Phase 3: Adjust pointers and Phase 4: Compact heap\");\n+  }\n+\n+  phase5_reset_metadata();\n@@ -232,1 +242,1 @@\n-  _heap->prepare_heap_for_mutators();\n+  _heap->prepare_for_mutator_after_full_collection();\n@@ -328,0 +338,4 @@\n+  if (!has_compaction_targets()) {\n+    return;\n+  }\n+\n@@ -334,3 +348,3 @@\n-  \/\/ if (!has_free_compaction_targets) {\n-  \/\/   phase2c_prepare_serial_compaction();\n-  \/\/ }\n+\/\/  if (scope()->do_maximal_compaction() || !has_free_compaction_targets) {\n+\/\/    phase2c_prepare_serial_compaction();\n+\/\/  }\n@@ -355,37 +369,55 @@\n-void G1FullCollector::phase2c_prepare_serial_compaction() {\n-  ShouldNotReachHere(); \/\/ Disabled in Lilliput.\n-  \/\/GCTraceTime(Debug, gc, phases) debug(\"Phase 2: Prepare serial compaction\", scope()->timer());\n-  \/\/ At this point we know that after parallel compaction there will be no\n-  \/\/ completely free regions. That means that the last region of\n-  \/\/ all compaction queues still have data in them. We try to compact\n-  \/\/ these regions in serial to avoid a premature OOM when the mutator wants\n-  \/\/ to allocate the first eden region after gc.\n-  \/*\n-  for (uint i = 0; i < workers(); i++) {\n-    G1FullGCCompactionPoint* cp = compaction_point(i);\n-    if (cp->has_regions()) {\n-      serial_compaction_point()->add(cp->remove_last());\n-    }\n-  }\n-  *\/\n-\n-  \/\/ Update the forwarding information for the regions in the serial\n-  \/\/ compaction point.\n-  \/*\n-  G1FullGCCompactionPoint* cp = serial_compaction_point();\n-  for (GrowableArrayIterator<HeapRegion*> it = cp->regions()->begin(); it != cp->regions()->end(); ++it) {\n-    HeapRegion* current = *it;\n-    if (!cp->is_initialized()) {\n-      \/\/ Initialize the compaction point. Nothing more is needed for the first heap region\n-      \/\/ since it is already prepared for compaction.\n-      cp->initialize(current);\n-    } else {\n-      assert(!current->is_humongous(), \"Should be no humongous regions in compaction queue\");\n-      G1SerialRePrepareClosure re_prepare(cp, current);\n-      set_compaction_top(current, current->bottom());\n-      current->apply_to_marked_objects(mark_bitmap(), &re_prepare);\n-    }\n-  }\n-  cp->update();\n-  *\/\n-}\n+\/\/uint G1FullCollector::truncate_parallel_cps() {\n+\/\/  uint lowest_current = (uint)-1;\n+\/\/  for (uint i = 0; i < workers(); i++) {\n+\/\/    G1FullGCCompactionPoint* cp = compaction_point(i);\n+\/\/    if (cp->has_regions()) {\n+\/\/      lowest_current = MIN2(lowest_current, cp->current_region()->hrm_index());\n+\/\/    }\n+\/\/  }\n+\n+\/\/  for (uint i = 0; i < workers(); i++) {\n+\/\/    G1FullGCCompactionPoint* cp = compaction_point(i);\n+\/\/    if (cp->has_regions()) {\n+\/\/      cp->remove_at_or_above(lowest_current);\n+\/\/    }\n+\/\/  }\n+\/\/  return lowest_current;\n+\/\/}\n+\n+\/\/void G1FullCollector::phase2c_prepare_serial_compaction() {\n+\/\/  GCTraceTime(Debug, gc, phases) debug(\"Phase 2: Prepare serial compaction\", scope()->timer());\n+\/\/  \/\/ At this point, we know that after parallel compaction there will be regions that\n+\/\/  \/\/ are partially compacted into. Thus, the last compaction region of all\n+\/\/  \/\/ compaction queues still have space in them. We try to re-compact these regions\n+\/\/  \/\/ in serial to avoid a premature OOM when the mutator wants to allocate the first\n+\/\/  \/\/ eden region after gc.\n+\/\/\n+\/\/  \/\/ For maximum compaction, we need to re-prepare all objects above the lowest\n+\/\/  \/\/ region among the current regions for all thread compaction points. It may\n+\/\/  \/\/ happen that due to the uneven distribution of objects to parallel threads, holes\n+\/\/  \/\/ have been created as threads compact to different target regions between the\n+\/\/  \/\/ lowest and the highest region in the tails of the compaction points.\n+\/\/\n+\/\/  uint start_serial = truncate_parallel_cps();\n+\/\/  assert(start_serial < _heap->max_reserved_regions(), \"Called on empty parallel compaction queues\");\n+\/\/\n+\/\/  G1FullGCCompactionPoint* serial_cp = serial_compaction_point();\n+\/\/  assert(!serial_cp->is_initialized(), \"sanity!\");\n+\/\/\n+\/\/  HeapRegion* start_hr = _heap->region_at(start_serial);\n+\/\/  serial_cp->add(start_hr);\n+\/\/  serial_cp->initialize(start_hr);\n+\/\/\n+\/\/  HeapWord* dense_prefix_top = compaction_top(start_hr);\n+\/\/  G1SerialRePrepareClosure re_prepare(serial_cp, dense_prefix_top);\n+\/\/\n+\/\/  for (uint i = start_serial + 1; i < _heap->max_reserved_regions(); i++) {\n+\/\/    if (is_compaction_target(i)) {\n+\/\/      HeapRegion* current = _heap->region_at(i);\n+\/\/      set_compaction_top(current, current->bottom());\n+\/\/      serial_cp->add(current);\n+\/\/      current->apply_to_marked_objects(mark_bitmap(), &re_prepare);\n+\/\/    }\n+\/\/  }\n+\/\/  serial_cp->update();\n+\/\/}\n@@ -413,0 +445,7 @@\n+void G1FullCollector::phase5_reset_metadata() {\n+  \/\/ Clear region metadata that is invalid after GC for all regions.\n+  GCTraceTime(Info, gc, phases) info(\"Phase 5: Reset Metadata\", scope()->timer());\n+  G1FullGCResetMetadataTask task(this);\n+  run_task(&task);\n+}\n+\n","filename":"src\/hotspot\/share\/gc\/g1\/g1FullCollector.cpp","additions":83,"deletions":44,"binary":false,"changes":127,"status":"modified"},{"patch":"@@ -2,1 +2,1 @@\n- * Copyright (c) 2017, 2022, Oracle and\/or its affiliates. All rights reserved.\n+ * Copyright (c) 2017, 2023, Oracle and\/or its affiliates. All rights reserved.\n@@ -38,37 +38,0 @@\n-\/\/ Do work for all skip-compacting regions.\n-class G1ResetSkipCompactingClosure : public HeapRegionClosure {\n-  G1FullCollector* _collector;\n-\n-public:\n-  G1ResetSkipCompactingClosure(G1FullCollector* collector) : _collector(collector) { }\n-\n-  bool do_heap_region(HeapRegion* r) {\n-    uint region_index = r->hrm_index();\n-    \/\/ Only for skip-compaction regions; early return otherwise.\n-    if (!_collector->is_skip_compacting(region_index)) {\n-      return false;\n-    }\n-#ifdef ASSERT\n-    if (r->is_humongous()) {\n-      oop obj = cast_to_oop(r->humongous_start_region()->bottom());\n-      assert(_collector->mark_bitmap()->is_marked(obj), \"must be live\");\n-    } else if (r->is_open_archive()) {\n-      bool is_empty = (_collector->live_words(r->hrm_index()) == 0);\n-      assert(!is_empty, \"should contain at least one live obj\");\n-    } else if (r->is_closed_archive()) {\n-      \/\/ should early-return above\n-      ShouldNotReachHere();\n-    } else {\n-      assert(_collector->live_words(region_index) > _collector->scope()->region_compaction_threshold(),\n-             \"should be quite full\");\n-    }\n-#endif\n-    assert(_collector->compaction_top(r) == nullptr,\n-           \"region %u compaction_top \" PTR_FORMAT \" must not be different from bottom \" PTR_FORMAT,\n-           r->hrm_index(), p2i(_collector->compaction_top(r)), p2i(r->bottom()));\n-\n-    r->reset_skip_compacting_after_full_gc();\n-    return false;\n-  }\n-};\n-\n@@ -128,4 +91,0 @@\n-\n-  G1ResetSkipCompactingClosure hc(collector());\n-  G1CollectedHeap::heap()->heap_region_par_iterate_from_worker_offset(&hc, &_claimer, worker_id);\n-  log_task(\"Compaction task\", worker_id, start);\n","filename":"src\/hotspot\/share\/gc\/g1\/g1FullGCCompactTask.cpp","additions":1,"deletions":42,"binary":false,"changes":43,"status":"modified"},{"patch":"@@ -2,1 +2,1 @@\n- * Copyright (c) 2017, 2022, Oracle and\/or its affiliates. All rights reserved.\n+ * Copyright (c) 2017, 2023, Oracle and\/or its affiliates. All rights reserved.\n@@ -120,2 +120,13 @@\n-HeapRegion* G1FullGCCompactionPoint::remove_last() {\n-  return _compaction_regions->pop();\n+void G1FullGCCompactionPoint::remove_at_or_above(uint bottom) {\n+  HeapRegion* cur = current_region();\n+  assert(cur->hrm_index() >= bottom, \"Sanity!\");\n+\n+  int start_index = 0;\n+  for (HeapRegion* r : *_compaction_regions) {\n+    if (r->hrm_index() < bottom) {\n+      start_index++;\n+    }\n+  }\n+\n+  assert(start_index >= 0, \"Should have at least one region\");\n+  _compaction_regions->trunc_to(start_index);\n","filename":"src\/hotspot\/share\/gc\/g1\/g1FullGCCompactionPoint.cpp","additions":14,"deletions":3,"binary":false,"changes":17,"status":"modified"},{"patch":"@@ -2,1 +2,1 @@\n- * Copyright (c) 2017, 2022, Oracle and\/or its affiliates. All rights reserved.\n+ * Copyright (c) 2017, 2023, Oracle and\/or its affiliates. All rights reserved.\n@@ -59,1 +59,1 @@\n-  HeapRegion* remove_last();\n+  void remove_at_or_above(uint bottom);\n","filename":"src\/hotspot\/share\/gc\/g1\/g1FullGCCompactionPoint.hpp","additions":2,"deletions":2,"binary":false,"changes":4,"status":"modified"},{"patch":"@@ -94,24 +94,0 @@\n-class G1VerifyOopClosure: public BasicOopIterateClosure {\n-private:\n-  G1CollectedHeap* _g1h;\n-  bool             _failures;\n-  oop              _containing_obj;\n-  VerifyOption     _verify_option;\n-\n-public:\n-  int _cc;\n-  G1VerifyOopClosure(VerifyOption option);\n-\n-  void set_containing_obj(oop obj) {\n-    _containing_obj = obj;\n-  }\n-\n-  bool failures() { return _failures; }\n-  void print_object(outputStream* out, oop obj);\n-\n-  template <class T> void do_oop_work(T* p);\n-\n-  void do_oop(oop* p)       { do_oop_work(p); }\n-  void do_oop(narrowOop* p) { do_oop_work(p); }\n-};\n-\n","filename":"src\/hotspot\/share\/gc\/g1\/g1FullGCOopClosures.hpp","additions":0,"deletions":24,"binary":false,"changes":24,"status":"modified"},{"patch":"@@ -2,1 +2,1 @@\n- * Copyright (c) 2017, 2022, Oracle and\/or its affiliates. All rights reserved.\n+ * Copyright (c) 2017, 2023, Oracle and\/or its affiliates. All rights reserved.\n@@ -33,1 +33,0 @@\n-#include \"gc\/g1\/g1HotCardCache.hpp\"\n@@ -100,6 +99,0 @@\n-\n-  \/\/ Clear region metadata that is invalid after GC for all regions.\n-  {\n-    G1ResetMetadataClosure closure(collector());\n-    G1CollectedHeap::heap()->heap_region_par_iterate_from_start(&closure, &_hrclaimer);\n-  }\n@@ -116,29 +109,0 @@\n-G1FullGCPrepareTask::G1ResetMetadataClosure::G1ResetMetadataClosure(G1FullCollector* collector) :\n-  _g1h(G1CollectedHeap::heap()),\n-  _collector(collector) { }\n-\n-void G1FullGCPrepareTask::G1ResetMetadataClosure::reset_region_metadata(HeapRegion* hr) {\n-  hr->rem_set()->clear();\n-  hr->clear_cardtable();\n-\n-  G1HotCardCache* hcc = _g1h->hot_card_cache();\n-  if (G1HotCardCache::use_cache()) {\n-    hcc->reset_card_counts(hr);\n-  }\n-}\n-\n-bool G1FullGCPrepareTask::G1ResetMetadataClosure::do_heap_region(HeapRegion* hr) {\n-  uint const region_idx = hr->hrm_index();\n-  if (!_collector->is_compaction_target(region_idx)) {\n-    assert(!hr->is_free(), \"all free regions should be compaction targets\");\n-    assert(_collector->is_skip_compacting(region_idx) || hr->is_closed_archive(), \"must be\");\n-    if (hr->needs_scrubbing_during_full_gc()) {\n-      scrub_skip_compacting_region(hr, hr->is_young());\n-    }\n-  }\n-\n-  \/\/ Reset data structures not valid after Full GC.\n-  reset_region_metadata(hr);\n-\n-  return false;\n-}\n@@ -161,28 +125,0 @@\n-\n-void G1FullGCPrepareTask::G1ResetMetadataClosure::scrub_skip_compacting_region(HeapRegion* hr, bool update_bot_for_live) {\n-  assert(hr->needs_scrubbing_during_full_gc(), \"must be\");\n-\n-  HeapWord* limit = hr->top();\n-  HeapWord* current_obj = hr->bottom();\n-  G1CMBitMap* bitmap = _collector->mark_bitmap();\n-\n-  while (current_obj < limit) {\n-    if (bitmap->is_marked(current_obj)) {\n-      oop current = cast_to_oop(current_obj);\n-      size_t size = current->size();\n-      if (update_bot_for_live) {\n-        hr->update_bot_for_block(current_obj, current_obj + size);\n-      }\n-      current_obj += size;\n-      continue;\n-    }\n-    \/\/ Found dead object, which is potentially unloaded, scrub to next\n-    \/\/ marked object.\n-    HeapWord* scrub_start = current_obj;\n-    HeapWord* scrub_end = bitmap->get_next_marked_addr(scrub_start, limit);\n-    assert(scrub_start != scrub_end, \"must advance\");\n-    hr->fill_range_with_dead_objects(scrub_start, scrub_end);\n-\n-    current_obj = scrub_end;\n-  }\n-}\n","filename":"src\/hotspot\/share\/gc\/g1\/g1FullGCPrepareTask.cpp","additions":1,"deletions":65,"binary":false,"changes":66,"status":"modified"},{"patch":"@@ -2,1 +2,1 @@\n- * Copyright (c) 2017, 2022, Oracle and\/or its affiliates. All rights reserved.\n+ * Copyright (c) 2017, 2023, Oracle and\/or its affiliates. All rights reserved.\n@@ -94,16 +94,0 @@\n-  class G1ResetMetadataClosure : public HeapRegionClosure {\n-    G1CollectedHeap* _g1h;\n-    G1FullCollector* _collector;\n-\n-    void reset_region_metadata(HeapRegion* hr);\n-    \/\/ Scrub all runs of dead objects within the given region by putting filler\n-    \/\/ objects and updating the corresponding BOT. If update_bot_for_live is true,\n-    \/\/ also update the BOT for live objects.\n-    void scrub_skip_compacting_region(HeapRegion* hr, bool update_bot_for_live);\n-\n-  public:\n-    G1ResetMetadataClosure(G1FullCollector* collector);\n-\n-    bool do_heap_region(HeapRegion* hr);\n-  };\n-\n@@ -124,1 +108,1 @@\n-\/\/  HeapRegion* _current;\n+\/\/  HeapWord* _dense_prefix_top;\n@@ -127,1 +111,1 @@\n-\/\/  G1SerialRePrepareClosure(G1FullGCCompactionPoint* hrcp, HeapRegion* hr) :\n+\/\/  G1SerialRePrepareClosure(G1FullGCCompactionPoint* hrcp, HeapWord* dense_prefix_top) :\n@@ -129,1 +113,1 @@\n-\/\/    _current(hr) { }\n+\/\/    _dense_prefix_top(dense_prefix_top) { }\n","filename":"src\/hotspot\/share\/gc\/g1\/g1FullGCPrepareTask.hpp","additions":4,"deletions":20,"binary":false,"changes":24,"status":"modified"},{"patch":"@@ -2,1 +2,1 @@\n- * Copyright (c) 2022, Oracle and\/or its affiliates. All rights reserved.\n+ * Copyright (c) 2022, 2023, Oracle and\/or its affiliates. All rights reserved.\n@@ -71,0 +71,2 @@\n+  _collector->set_has_compaction_targets();\n+\n@@ -113,4 +115,6 @@\n-\/\/  \/\/ We only re-prepare objects forwarded within the current region, so\n-\/\/  \/\/ skip objects that are already forwarded to another region.\n-\/\/  if (obj->is_forwarded() && !_current->is_in(obj->forwardee())) {\n-\/\/    return obj->size();\n+\/\/  if (obj->is_forwarded()) {\n+\/\/    \/\/ We skip objects compiled into the first region or\n+\/\/    \/\/ into regions not part of the serial compaction point.\n+\/\/    if (cast_from_oop<HeapWord*>(obj->forwardee()) < _dense_prefix_top) {\n+\/\/      return obj->size();\n+\/\/    }\n","filename":"src\/hotspot\/share\/gc\/g1\/g1FullGCPrepareTask.inline.hpp","additions":9,"deletions":5,"binary":false,"changes":14,"status":"modified"},{"patch":"@@ -30,1 +30,1 @@\n-#include \"gc\/g1\/g1CollectedHeap.hpp\"\n+#include \"gc\/g1\/g1CollectedHeap.inline.hpp\"\n@@ -36,0 +36,2 @@\n+#include \"logging\/log.hpp\"\n+#include \"logging\/logStream.hpp\"\n","filename":"src\/hotspot\/share\/gc\/g1\/g1OopClosures.inline.hpp","additions":3,"deletions":1,"binary":false,"changes":4,"status":"modified"},{"patch":"@@ -327,1 +327,1 @@\n-    PSScavenge::card_table()->inline_write_ref_field_gc(p, new_obj);\n+    PSScavenge::card_table()->inline_write_ref_field_gc(p);\n","filename":"src\/hotspot\/share\/gc\/parallel\/psPromotionManager.inline.hpp","additions":1,"deletions":1,"binary":false,"changes":2,"status":"modified"},{"patch":"@@ -26,0 +26,1 @@\n+#include \"gc\/serial\/cardTableRS.hpp\"\n@@ -33,1 +34,0 @@\n-#include \"gc\/shared\/cardTableRS.hpp\"\n@@ -44,1 +44,0 @@\n-#include \"gc\/shared\/genOopClosures.inline.hpp\"\n@@ -66,2 +65,7 @@\n-\/\/\n-\/\/ DefNewGeneration functions.\n+class ScavengeHelper {\n+  DefNewGeneration* _young_gen;\n+  HeapWord*         _young_gen_end;\n+public:\n+  ScavengeHelper(DefNewGeneration* young_gen) :\n+    _young_gen(young_gen),\n+    _young_gen_end(young_gen->reserved().end()) {}\n@@ -69,1 +73,3 @@\n-\/\/ Methods of protected closure types.\n+  bool is_in_young_gen(void* p) const {\n+    return p < _young_gen_end;\n+  }\n@@ -71,3 +77,18 @@\n-DefNewGeneration::IsAliveClosure::IsAliveClosure(Generation* young_gen) : _young_gen(young_gen) {\n-  assert(_young_gen->kind() == Generation::DefNew, \"Expected the young generation here\");\n-}\n+  template <typename T, typename Func>\n+  void try_scavenge(T* p, Func&& f) {\n+    T heap_oop = RawAccess<>::oop_load(p);\n+    \/\/ Should we copy the obj?\n+    if (!CompressedOops::is_null(heap_oop)) {\n+      oop obj = CompressedOops::decode_not_null(heap_oop);\n+      if (is_in_young_gen(obj)) {\n+        assert(!_young_gen->to()->is_in_reserved(obj), \"Scanning field twice?\");\n+        oop new_obj = obj->is_forwarded() ? obj->forwardee()\n+                                          : _young_gen->copy_to_survivor_space(obj);\n+        RawAccess<IS_NOT_NULL>::oop_store(p, new_obj);\n+\n+        \/\/ callback\n+        f(new_obj);\n+      }\n+    }\n+  }\n+};\n@@ -75,3 +96,6 @@\n-bool DefNewGeneration::IsAliveClosure::do_object_b(oop p) {\n-  return cast_from_oop<HeapWord*>(p) >= _young_gen->reserved().end() || p->is_forwarded();\n-}\n+class InHeapScanClosure : public BasicOopIterateClosure {\n+  ScavengeHelper _helper;\n+protected:\n+  bool is_in_young_gen(void* p) const {\n+    return _helper.is_in_young_gen(p);\n+  }\n@@ -79,6 +103,4 @@\n-DefNewGeneration::FastKeepAliveClosure::\n-FastKeepAliveClosure(DefNewGeneration* g, ScanWeakRefClosure* cl) :\n-  _cl(cl) {\n-  _rs = GenCollectedHeap::heap()->rem_set();\n-  _boundary = g->reserved().end();\n-}\n+  template <typename T, typename Func>\n+  void try_scavenge(T* p, Func&& f) {\n+    _helper.try_scavenge(p, f);\n+  }\n@@ -86,2 +108,4 @@\n-void DefNewGeneration::FastKeepAliveClosure::do_oop(oop* p)       { DefNewGeneration::FastKeepAliveClosure::do_oop_work(p); }\n-void DefNewGeneration::FastKeepAliveClosure::do_oop(narrowOop* p) { DefNewGeneration::FastKeepAliveClosure::do_oop_work(p); }\n+  InHeapScanClosure(DefNewGeneration* young_gen) :\n+    BasicOopIterateClosure(young_gen->ref_processor()),\n+    _helper(young_gen) {}\n+};\n@@ -89,7 +113,6 @@\n-DefNewGeneration::FastEvacuateFollowersClosure::\n-FastEvacuateFollowersClosure(SerialHeap* heap,\n-                             DefNewScanClosure* cur,\n-                             DefNewYoungerGenClosure* older) :\n-  _heap(heap), _scan_cur_or_nonheap(cur), _scan_older(older)\n-{\n-}\n+class OffHeapScanClosure : public OopClosure {\n+  ScavengeHelper _helper;\n+protected:\n+  bool is_in_young_gen(void* p) const {\n+    return _helper.is_in_young_gen(p);\n+  }\n@@ -97,6 +120,4 @@\n-void DefNewGeneration::FastEvacuateFollowersClosure::do_void() {\n-  do {\n-    _heap->oop_since_save_marks_iterate(_scan_cur_or_nonheap, _scan_older);\n-  } while (!_heap->no_allocs_since_save_marks());\n-  guarantee(_heap->young_gen()->promo_failure_scan_is_complete(), \"Failed to finish scan\");\n-}\n+  template <typename T, typename Func>\n+  void try_scavenge(T* p, Func&& f) {\n+    _helper.try_scavenge(p, f);\n+  }\n@@ -104,6 +125,2 @@\n-void CLDScanClosure::do_cld(ClassLoaderData* cld) {\n-  NOT_PRODUCT(ResourceMark rm);\n-  log_develop_trace(gc, scavenge)(\"CLDScanClosure::do_cld \" PTR_FORMAT \", %s, dirty: %s\",\n-                                  p2i(cld),\n-                                  cld->loader_name_and_id(),\n-                                  cld->has_modified_oops() ? \"true\" : \"false\");\n+  OffHeapScanClosure(DefNewGeneration* young_gen) :  _helper(young_gen) {}\n+};\n@@ -111,3 +128,17 @@\n-  \/\/ If the cld has not been dirtied we know that there's\n-  \/\/ no references into  the young gen and we can skip it.\n-  if (cld->has_modified_oops()) {\n+class OldGenScanClosure : public InHeapScanClosure {\n+  CardTableRS* _rs;\n+\n+  template <typename T>\n+  void do_oop_work(T* p) {\n+    assert(!is_in_young_gen(p), \"precondition\");\n+\n+    try_scavenge(p, [&] (oop new_obj) {\n+      \/\/ If p points to a younger generation, mark the card.\n+      if (is_in_young_gen(new_obj)) {\n+        _rs->inline_write_ref_field_gc(p);\n+      }\n+    });\n+  }\n+public:\n+  OldGenScanClosure(DefNewGeneration* g) : InHeapScanClosure(g),\n+    _rs(SerialHeap::heap()->rem_set()) {}\n@@ -115,3 +146,3 @@\n-    \/\/ Tell the closure which CLD is being scanned so that it can be dirtied\n-    \/\/ if oops are left pointing into the young gen.\n-    _scavenge_closure->set_scanned_cld(cld);\n+  void do_oop(oop* p)       { do_oop_work(p); }\n+  void do_oop(narrowOop* p) { do_oop_work(p); }\n+};\n@@ -119,2 +150,5 @@\n-    \/\/ Clean the cld since we're going to scavenge all the metadata.\n-    cld->oops_do(_scavenge_closure, ClassLoaderData::_claim_none, \/*clear_modified_oops*\/true);\n+class PromoteFailureClosure : public InHeapScanClosure {\n+  template <typename T>\n+  void do_oop_work(T* p) {\n+    assert(is_in_young_gen(p), \"promote-fail objs must be in young-gen\");\n+    assert(!SerialHeap::heap()->young_gen()->to()->is_in_reserved(p), \"must not be in to-space\");\n@@ -122,1 +156,1 @@\n-    _scavenge_closure->set_scanned_cld(nullptr);\n+    try_scavenge(p, [] (auto) {});\n@@ -124,1 +158,2 @@\n-}\n+public:\n+  PromoteFailureClosure(DefNewGeneration* g) : InHeapScanClosure(g) {}\n@@ -126,5 +161,165 @@\n-ScanWeakRefClosure::ScanWeakRefClosure(DefNewGeneration* g) :\n-  _g(g)\n-{\n-  _boundary = _g->reserved().end();\n-}\n+  void do_oop(oop* p)       { do_oop_work(p); }\n+  void do_oop(narrowOop* p) { do_oop_work(p); }\n+};\n+\n+class YoungGenScanClosure : public InHeapScanClosure {\n+  template <typename T>\n+  void do_oop_work(T* p) {\n+    assert(SerialHeap::heap()->young_gen()->to()->is_in_reserved(p), \"precondition\");\n+\n+    try_scavenge(p, [] (auto) {});\n+  }\n+public:\n+  YoungGenScanClosure(DefNewGeneration* g) : InHeapScanClosure(g) {}\n+\n+  void do_oop(oop* p)       { do_oop_work(p); }\n+  void do_oop(narrowOop* p) { do_oop_work(p); }\n+};\n+\n+class RootScanClosure : public OffHeapScanClosure {\n+  template <typename T>\n+  void do_oop_work(T* p) {\n+    assert(!SerialHeap::heap()->is_in_reserved(p), \"outside the heap\");\n+\n+    try_scavenge(p,  [] (auto) {});\n+  }\n+public:\n+  RootScanClosure(DefNewGeneration* g) : OffHeapScanClosure(g) {}\n+\n+  void do_oop(oop* p)       { do_oop_work(p); }\n+  void do_oop(narrowOop* p) { do_oop_work(p); }\n+};\n+\n+class CLDScanClosure: public CLDClosure {\n+\n+  class CLDOopClosure : public OffHeapScanClosure {\n+    ClassLoaderData* _scanned_cld;\n+\n+    template <typename T>\n+    void do_oop_work(T* p) {\n+      assert(!SerialHeap::heap()->is_in_reserved(p), \"outside the heap\");\n+\n+      try_scavenge(p, [&] (oop new_obj) {\n+        assert(_scanned_cld != nullptr, \"inv\");\n+        if (is_in_young_gen(new_obj) && !_scanned_cld->has_modified_oops()) {\n+          _scanned_cld->record_modified_oops();\n+        }\n+      });\n+    }\n+\n+  public:\n+    CLDOopClosure(DefNewGeneration* g) : OffHeapScanClosure(g),\n+      _scanned_cld(nullptr) {}\n+\n+    void set_scanned_cld(ClassLoaderData* cld) {\n+      assert(cld == nullptr || _scanned_cld == nullptr, \"Must be\");\n+      _scanned_cld = cld;\n+    }\n+\n+    void do_oop(oop* p)       { do_oop_work(p); }\n+    void do_oop(narrowOop* p) { ShouldNotReachHere(); }\n+  };\n+\n+  CLDOopClosure _oop_closure;\n+ public:\n+  CLDScanClosure(DefNewGeneration* g) : _oop_closure(g) {}\n+\n+  void do_cld(ClassLoaderData* cld) {\n+    \/\/ If the cld has not been dirtied we know that there's\n+    \/\/ no references into  the young gen and we can skip it.\n+    if (cld->has_modified_oops()) {\n+\n+      \/\/ Tell the closure which CLD is being scanned so that it can be dirtied\n+      \/\/ if oops are left pointing into the young gen.\n+      _oop_closure.set_scanned_cld(cld);\n+\n+      \/\/ Clean the cld since we're going to scavenge all the metadata.\n+      cld->oops_do(&_oop_closure, ClassLoaderData::_claim_none, \/*clear_modified_oops*\/true);\n+\n+      _oop_closure.set_scanned_cld(nullptr);\n+    }\n+  }\n+};\n+\n+class IsAliveClosure: public BoolObjectClosure {\n+  HeapWord*         _young_gen_end;\n+public:\n+  IsAliveClosure(DefNewGeneration* g): _young_gen_end(g->reserved().end()) {}\n+\n+  bool do_object_b(oop p) {\n+    return cast_from_oop<HeapWord*>(p) >= _young_gen_end || p->is_forwarded();\n+  }\n+};\n+\n+class AdjustWeakRootClosure: public OffHeapScanClosure {\n+  template <class T>\n+  void do_oop_work(T* p) {\n+    DEBUG_ONLY(SerialHeap* heap = SerialHeap::heap();)\n+    assert(!heap->is_in_reserved(p), \"outside the heap\");\n+\n+    oop obj = RawAccess<IS_NOT_NULL>::oop_load(p);\n+    if (is_in_young_gen(obj)) {\n+      assert(!heap->young_gen()->to()->is_in_reserved(obj), \"inv\");\n+      assert(obj->is_forwarded(), \"forwarded before weak-root-processing\");\n+      oop new_obj = obj->forwardee();\n+      RawAccess<IS_NOT_NULL>::oop_store(p, new_obj);\n+    }\n+  }\n+ public:\n+  AdjustWeakRootClosure(DefNewGeneration* g): OffHeapScanClosure(g) {}\n+\n+  void do_oop(oop* p)       { do_oop_work(p); }\n+  void do_oop(narrowOop* p) { ShouldNotReachHere(); }\n+};\n+\n+class KeepAliveClosure: public OopClosure {\n+  DefNewGeneration* _young_gen;\n+  HeapWord*         _young_gen_end;\n+  CardTableRS* _rs;\n+\n+  bool is_in_young_gen(void* p) const {\n+    return p < _young_gen_end;\n+  }\n+\n+  template <class T>\n+  void do_oop_work(T* p) {\n+    oop obj = RawAccess<IS_NOT_NULL>::oop_load(p);\n+\n+    if (is_in_young_gen(obj)) {\n+      oop new_obj = obj->is_forwarded() ? obj->forwardee()\n+                                        : _young_gen->copy_to_survivor_space(obj);\n+      RawAccess<IS_NOT_NULL>::oop_store(p, new_obj);\n+\n+      if (is_in_young_gen(new_obj) && !is_in_young_gen(p)) {\n+        _rs->inline_write_ref_field_gc(p);\n+      }\n+    }\n+  }\n+public:\n+  KeepAliveClosure(DefNewGeneration* g) :\n+    _young_gen(g),\n+    _young_gen_end(g->reserved().end()),\n+    _rs(SerialHeap::heap()->rem_set()) {}\n+\n+  void do_oop(oop* p)       { do_oop_work(p); }\n+  void do_oop(narrowOop* p) { do_oop_work(p); }\n+};\n+\n+class FastEvacuateFollowersClosure: public VoidClosure {\n+  SerialHeap* _heap;\n+  YoungGenScanClosure* _young_cl;\n+  OldGenScanClosure* _old_cl;\n+public:\n+  FastEvacuateFollowersClosure(SerialHeap* heap,\n+                               YoungGenScanClosure* young_cl,\n+                               OldGenScanClosure* old_cl) :\n+    _heap(heap), _young_cl(young_cl), _old_cl(old_cl)\n+  {}\n+\n+  void do_void() {\n+    do {\n+      _heap->oop_since_save_marks_iterate(_young_cl, _old_cl);\n+    } while (!_heap->no_allocs_since_save_marks());\n+    guarantee(_heap->young_gen()->promo_failure_scan_is_complete(), \"Failed to finish scan\");\n+  }\n+};\n@@ -199,0 +394,4 @@\n+  if (eden_size > max_eden_size()) {\n+    eden_size = max_eden_size();\n+    survivor_size = (size - eden_size)\/2;\n+  }\n@@ -547,1 +746,0 @@\n-  ScanWeakRefClosure scan_weak_ref(this);\n@@ -557,2 +755,2 @@\n-  DefNewScanClosure       scan_closure(this);\n-  DefNewYoungerGenClosure younger_gen_closure(this, _old_gen);\n+  YoungGenScanClosure young_gen_cl(this);\n+  OldGenScanClosure   old_gen_cl(this);\n@@ -560,5 +758,2 @@\n-  CLDScanClosure cld_scan_closure(&scan_closure);\n-\n-  set_promo_failure_scan_stack_closure(&scan_closure);\n-                                                  &scan_closure,\n-                                                  &younger_gen_closure);\n+                                                  &young_gen_cl,\n+                                                  &old_gen_cl);\n@@ -572,0 +767,2 @@\n+    RootScanClosure root_cl{this};\n+    CLDScanClosure cld_scan_closure{this};\n@@ -573,2 +770,2 @@\n-    heap->young_process_roots(&scan_closure,\n-                              &younger_gen_closure,\n+    heap->young_process_roots(&root_cl,\n+                              &old_gen_cl,\n@@ -581,9 +778,11 @@\n-  FastKeepAliveClosure keep_alive(this, &scan_weak_ref);\n-  ReferenceProcessor* rp = ref_processor();\n-  ReferenceProcessorPhaseTimes pt(_gc_timer, rp->max_num_queues());\n-  SerialGCRefProcProxyTask task(is_alive, keep_alive, evacuate_followers);\n-  const ReferenceProcessorStats& stats = rp->process_discovered_references(task, pt);\n-  _gc_tracer->report_gc_reference_stats(stats);\n-  _gc_tracer->report_tenuring_threshold(tenuring_threshold());\n-  pt.print_all_references();\n-\n+  {\n+    \/\/ Reference processing\n+    KeepAliveClosure keep_alive(this);\n+    ReferenceProcessor* rp = ref_processor();\n+    ReferenceProcessorPhaseTimes pt(_gc_timer, rp->max_num_queues());\n+    SerialGCRefProcProxyTask task(is_alive, keep_alive, evacuate_followers);\n+    const ReferenceProcessorStats& stats = rp->process_discovered_references(task, pt);\n+    _gc_tracer->report_gc_reference_stats(stats);\n+    _gc_tracer->report_tenuring_threshold(tenuring_threshold());\n+    pt.print_all_references();\n+  }\n@@ -592,1 +791,4 @@\n-  WeakProcessor::weak_oops_do(&is_alive, &keep_alive);\n+  {\n+    AdjustWeakRootClosure cl{this};\n+    WeakProcessor::weak_oops_do(&is_alive, &cl);\n+  }\n@@ -765,0 +967,1 @@\n+  PromoteFailureClosure cl{this};\n@@ -767,1 +970,1 @@\n-     obj->oop_iterate(_promo_failure_scan_stack_closure);\n+     obj->oop_iterate(&cl);\n","filename":"src\/hotspot\/share\/gc\/serial\/defNewGeneration.cpp","additions":277,"deletions":74,"binary":false,"changes":351,"status":"modified"},{"patch":"@@ -35,0 +35,1 @@\n+#include \"gc\/serial\/cardTableRS.hpp\"\n@@ -44,1 +45,0 @@\n-#include \"gc\/shared\/genOopClosures.inline.hpp\"\n","filename":"src\/hotspot\/share\/gc\/serial\/genMarkSweep.cpp","additions":1,"deletions":1,"binary":false,"changes":2,"status":"modified"},{"patch":"@@ -29,1 +29,0 @@\n-#include \"gc\/shared\/genOopClosures.hpp\"\n@@ -91,1 +90,0 @@\n-  friend class VM_MarkSweep;\n","filename":"src\/hotspot\/share\/gc\/serial\/markSweep.hpp","additions":0,"deletions":2,"binary":false,"changes":2,"status":"modified"},{"patch":"@@ -64,1 +64,1 @@\n-Klass* CollectedHeap::_filler_object_klass = NULL;\n+Klass* CollectedHeap::_filler_object_klass = nullptr;\n@@ -102,1 +102,1 @@\n-  _records[index].thread = NULL; \/\/ Its the GC thread so it's not that interesting.\n+  _records[index].thread = nullptr; \/\/ Its the GC thread so it's not that interesting.\n@@ -164,1 +164,1 @@\n-  if (_gc_heap_log != NULL) {\n+  if (_gc_heap_log != nullptr) {\n@@ -178,1 +178,1 @@\n-  if (_gc_heap_log != NULL) {\n+  if (_gc_heap_log != nullptr) {\n@@ -191,1 +191,1 @@\n-  if (bs != NULL) {\n+  if (bs != nullptr) {\n@@ -276,1 +276,1 @@\n-    _gc_heap_log = NULL;\n+    _gc_heap_log = nullptr;\n@@ -299,1 +299,0 @@\n-    case GCCause::_archive_time_gc:\n@@ -321,1 +320,1 @@\n-    if (result != NULL) {\n+    if (result != nullptr) {\n@@ -330,1 +329,1 @@\n-      if (result != NULL) {\n+      if (result != nullptr) {\n@@ -347,1 +346,1 @@\n-        return NULL;\n+        return nullptr;\n@@ -522,1 +521,1 @@\n-  return NULL;\n+  return nullptr;\n@@ -565,1 +564,1 @@\n-  assert(timer != NULL, \"timer is null\");\n+  assert(timer != nullptr, \"timer is null\");\n","filename":"src\/hotspot\/share\/gc\/shared\/collectedHeap.cpp","additions":11,"deletions":12,"binary":false,"changes":23,"status":"modified"},{"patch":"@@ -203,1 +203,1 @@\n-    assert(heap != NULL, \"Uninitialized heap\");\n+    assert(heap != nullptr, \"Uninitialized heap\");\n@@ -280,1 +280,1 @@\n-  DEBUG_ONLY(bool is_in_or_null(const void* p) const { return p == NULL || is_in(p); })\n+  DEBUG_ONLY(bool is_in_or_null(const void* p) const { return p == nullptr || is_in(p); })\n@@ -416,1 +416,1 @@\n-    return NULL;\n+    return nullptr;\n@@ -492,1 +492,1 @@\n-  \/\/ method returns NULL, cleanup tasks are done serially in the VMThread. See\n+  \/\/ method returns null, cleanup tasks are done serially in the VMThread. See\n@@ -500,1 +500,1 @@\n-  virtual WorkerThreads* safepoint_workers() { return NULL; }\n+  virtual WorkerThreads* safepoint_workers() { return nullptr; }\n@@ -515,1 +515,1 @@\n-  virtual HeapWord* allocate_loaded_archive_space(size_t size) { return NULL; }\n+  virtual HeapWord* allocate_loaded_archive_space(size_t size) { return nullptr; }\n","filename":"src\/hotspot\/share\/gc\/shared\/collectedHeap.hpp","additions":6,"deletions":6,"binary":false,"changes":12,"status":"modified"},{"patch":"@@ -2,1 +2,1 @@\n- * Copyright (c) 2000, 2022, Oracle and\/or its affiliates. All rights reserved.\n+ * Copyright (c) 2000, 2023, Oracle and\/or its affiliates. All rights reserved.\n@@ -33,0 +33,1 @@\n+#include \"gc\/serial\/cardTableRS.hpp\"\n@@ -38,1 +39,0 @@\n-#include \"gc\/shared\/cardTableRS.hpp\"\n@@ -52,1 +52,0 @@\n-#include \"gc\/shared\/genOopClosures.inline.hpp\"\n@@ -89,2 +88,2 @@\n-  _young_gen(NULL),\n-  _old_gen(NULL),\n+  _young_gen(nullptr),\n+  _old_gen(nullptr),\n@@ -99,1 +98,1 @@\n-  _rem_set(NULL),\n+  _rem_set(nullptr),\n@@ -101,1 +100,1 @@\n-  _size_policy(NULL),\n+  _size_policy(nullptr),\n@@ -105,2 +104,2 @@\n-  _young_manager(NULL),\n-  _old_manager(NULL) {\n+  _young_manager(nullptr),\n+  _old_manager(nullptr) {\n@@ -274,1 +273,1 @@\n-  HeapWord* result = NULL;\n+  HeapWord* result = nullptr;\n@@ -278,1 +277,1 @@\n-  if (result == NULL) {\n+  if (result == nullptr) {\n@@ -283,1 +282,1 @@\n-  assert(result == NULL || is_in_reserved(result), \"result not in heap\");\n+  assert(result == nullptr || is_in_reserved(result), \"result not in heap\");\n@@ -295,1 +294,1 @@\n-  HeapWord* result = NULL;\n+  HeapWord* result = nullptr;\n@@ -304,1 +303,1 @@\n-      if (result != NULL) {\n+      if (result != nullptr) {\n@@ -318,1 +317,1 @@\n-      if (result != NULL) {\n+      if (result != nullptr) {\n@@ -325,1 +324,1 @@\n-          return NULL;  \/\/ Caller will retry allocating individual object.\n+          return nullptr;  \/\/ Caller will retry allocating individual object.\n@@ -331,1 +330,1 @@\n-          if (result != NULL) {\n+          if (result != nullptr) {\n@@ -337,1 +336,1 @@\n-          return NULL; \/\/ We didn't get to do a GC and we didn't get any memory.\n+          return nullptr; \/\/ We didn't get to do a GC and we didn't get any memory.\n@@ -358,1 +357,1 @@\n-          return NULL;\n+          return nullptr;\n@@ -371,1 +370,1 @@\n-         assert(result == NULL, \"must be NULL if gc_locked() is true\");\n+         assert(result == nullptr, \"must be null if gc_locked() is true\");\n@@ -377,1 +376,1 @@\n-      \/\/ this time, return NULL so that an out-of-memory\n+      \/\/ this time, return null so that an out-of-memory\n@@ -387,1 +386,1 @@\n-        if (op.result() != NULL) {\n+        if (op.result() != nullptr) {\n@@ -390,1 +389,1 @@\n-        return NULL;\n+        return nullptr;\n@@ -392,1 +391,1 @@\n-      assert(result == NULL || is_in_reserved(result),\n+      assert(result == nullptr || is_in_reserved(result),\n@@ -409,1 +408,1 @@\n-  HeapWord* res = NULL;\n+  HeapWord* res = nullptr;\n@@ -413,1 +412,1 @@\n-    if (res != NULL || first_only) {\n+    if (res != nullptr || first_only) {\n@@ -535,1 +534,1 @@\n-    GCTraceTime(Info, gc) t(\"Pause Young\", NULL, gc_cause(), true);\n+    GCTraceTime(Info, gc) t(\"Pause Young\", nullptr, gc_cause(), true);\n@@ -585,1 +584,1 @@\n-    GCTraceTime(Info, gc) t(\"Pause Full\", NULL, gc_cause(), true);\n+    GCTraceTime(Info, gc) t(\"Pause Full\", nullptr, gc_cause(), true);\n@@ -607,1 +606,0 @@\n-    CodeCache::arm_all_nmethods();\n@@ -670,1 +668,1 @@\n-  HeapWord* result = NULL;\n+  HeapWord* result = nullptr;\n@@ -702,1 +700,1 @@\n-  if (result != NULL) {\n+  if (result != nullptr) {\n@@ -709,1 +707,1 @@\n-  if (result != NULL) {\n+  if (result != nullptr) {\n@@ -729,1 +727,1 @@\n-  if (result != NULL) {\n+  if (result != nullptr) {\n@@ -741,1 +739,1 @@\n-  return NULL;\n+  return nullptr;\n@@ -761,1 +759,1 @@\n-  assert(code_roots != NULL, \"code root closure should always be set\");\n+  assert(code_roots != nullptr, \"code root closure should always be set\");\n@@ -766,1 +764,1 @@\n-  CodeBlobToOopClosure* roots_from_code_p = (so & SO_AllCodeCache) ? NULL : code_roots;\n+  CodeBlobToOopClosure* roots_from_code_p = (so & SO_AllCodeCache) ? nullptr : code_roots;\n@@ -773,1 +771,1 @@\n-    assert(code_roots != NULL, \"must supply closure for code cache\");\n+    assert(code_roots != nullptr, \"must supply closure for code cache\");\n@@ -779,1 +777,1 @@\n-    assert(code_roots != NULL, \"must supply closure for code cache\");\n+    assert(code_roots != nullptr, \"must supply closure for code cache\");\n@@ -877,1 +875,1 @@\n-  assert(is_in_reserved(p) || p == NULL,\n+  assert(is_in_reserved(p) || p == nullptr,\n@@ -879,1 +877,1 @@\n-  return p < _young_gen->reserved().end() && p != NULL;\n+  return p < _young_gen->reserved().end() && p != nullptr;\n@@ -895,1 +893,1 @@\n-  if (res != NULL) {\n+  if (res != nullptr) {\n@@ -899,1 +897,1 @@\n-  assert(res != NULL, \"Could not find containing space\");\n+  assert(res != nullptr, \"Could not find containing space\");\n@@ -951,1 +949,1 @@\n-  if (result != NULL) {\n+  if (result != nullptr) {\n@@ -958,1 +956,1 @@\n-\/\/ Requires \"*prev_ptr\" to be non-NULL.  Deletes and a block of minimal size\n+\/\/ Requires \"*prev_ptr\" to be non-null.  Deletes and a block of minimal size\n@@ -984,1 +982,1 @@\n-  ScratchBlock* sorted = NULL;\n+  ScratchBlock* sorted = nullptr;\n@@ -996,1 +994,1 @@\n-  ScratchBlock* res = NULL;\n+  ScratchBlock* res = nullptr;\n@@ -1059,1 +1057,1 @@\n-  if (_young_gen != NULL) {\n+  if (_young_gen != nullptr) {\n@@ -1062,1 +1060,1 @@\n-  if (_old_gen != NULL) {\n+  if (_old_gen != nullptr) {\n","filename":"src\/hotspot\/share\/gc\/shared\/genCollectedHeap.cpp","additions":46,"deletions":48,"binary":false,"changes":94,"status":"modified"},{"patch":"@@ -26,1 +26,1 @@\n-#include \"gc\/shared\/cardTableRS.hpp\"\n+#include \"gc\/serial\/cardTableRS.hpp\"\n","filename":"src\/hotspot\/share\/gc\/shared\/jvmFlagConstraintsGC.cpp","additions":1,"deletions":1,"binary":false,"changes":2,"status":"modified"},{"patch":"@@ -2,1 +2,1 @@\n- * Copyright (c) 2018, 2022, Oracle and\/or its affiliates. All rights reserved.\n+ * Copyright (c) 2018, 2023, Oracle and\/or its affiliates. All rights reserved.\n@@ -105,1 +105,1 @@\n-    *obj_ptr = NULL;\n+    *obj_ptr = nullptr;\n@@ -121,1 +121,1 @@\n-  if (obj() != NULL) {\n+  if (obj() != nullptr) {\n@@ -243,1 +243,1 @@\n-    if (klass != NULL && klass->name() != NULL) {\n+    if (klass != nullptr && klass->name() != nullptr) {\n@@ -259,1 +259,1 @@\n-  if (mem == NULL) {\n+  if (mem == nullptr) {\n@@ -275,1 +275,1 @@\n-  if (mem != NULL) {\n+  if (mem != nullptr) {\n@@ -288,1 +288,1 @@\n-  HeapWord* mem = NULL;\n+  HeapWord* mem = nullptr;\n@@ -299,1 +299,1 @@\n-    if (mem != NULL) {\n+    if (mem != nullptr) {\n@@ -308,1 +308,1 @@\n-    return NULL;\n+    return nullptr;\n@@ -318,1 +318,1 @@\n-    return NULL;\n+    return nullptr;\n@@ -325,1 +325,1 @@\n-  if (mem == NULL) {\n+  if (mem == nullptr) {\n@@ -330,1 +330,1 @@\n-    return NULL;\n+    return nullptr;\n@@ -362,1 +362,1 @@\n-    if (mem != NULL) {\n+    if (mem != nullptr) {\n@@ -374,1 +374,1 @@\n-    if (mem != NULL) {\n+    if (mem != nullptr) {\n@@ -383,1 +383,1 @@\n-  oop obj = NULL;\n+  oop obj = nullptr;\n@@ -387,1 +387,1 @@\n-    if (mem != NULL) {\n+    if (mem != nullptr) {\n@@ -391,2 +391,2 @@\n-      \/\/ so reset it to NULL if mem is NULL.\n-      obj = NULL;\n+      \/\/ so reset it to null if mem is null.\n+      obj = nullptr;\n@@ -399,1 +399,1 @@\n-  assert(mem != NULL, \"cannot initialize NULL object\");\n+  assert(mem != nullptr, \"cannot initialize null object\");\n@@ -406,2 +406,2 @@\n-  assert(mem != NULL, \"NULL object pointer\");\n-  \/\/ object zeroing are visible before setting the klass non-NULL, for\n+  assert(mem != nullptr, \"null object pointer\");\n+  \/\/ object zeroing are visible before setting the klass non-null, for\n@@ -435,1 +435,1 @@\n-  \/\/ non-NULL klass field indicates that the object is parsable by\n+  \/\/ non-null klass field indicates that the object is parsable by\n@@ -447,1 +447,1 @@\n-  \/\/ non-NULL _klass field indicates that the object is parsable by\n+  \/\/ non-null _klass field indicates that the object is parsable by\n","filename":"src\/hotspot\/share\/gc\/shared\/memAllocator.cpp","additions":23,"deletions":23,"binary":false,"changes":46,"status":"modified"},{"patch":"@@ -2,1 +2,1 @@\n- * Copyright (c) 2018, 2021, Oracle and\/or its affiliates. All rights reserved.\n+ * Copyright (c) 2018, 2023, Oracle and\/or its affiliates. All rights reserved.\n@@ -188,1 +188,1 @@\n-  \/\/ Returns NULL if ptr is not in a block or not allocated in that block.\n+  \/\/ Returns null if ptr is not in a block or not allocated in that block.\n@@ -262,1 +262,1 @@\n-    if (v != NULL) {\n+    if (v != nullptr) {\n@@ -267,1 +267,1 @@\n-        *ptr = NULL;            \/\/ Clear dead value.\n+        *ptr = nullptr;            \/\/ Clear dead value.\n@@ -290,1 +290,1 @@\n-    return (*ptr != NULL) ? _f(ptr) : true;\n+    return (*ptr != nullptr) ? _f(ptr) : true;\n","filename":"src\/hotspot\/share\/gc\/shared\/oopStorage.inline.hpp","additions":5,"deletions":5,"binary":false,"changes":10,"status":"modified"},{"patch":"@@ -2,1 +2,1 @@\n- * Copyright (c) 2016, 2022, Oracle and\/or its affiliates. All rights reserved.\n+ * Copyright (c) 2016, 2023, Oracle and\/or its affiliates. All rights reserved.\n@@ -94,2 +94,2 @@\n-  \/\/ This should be != NULL if the stacks have been initialized,\n-  \/\/ or == NULL if they have not.\n+  \/\/ This should be != null if the stacks have been initialized,\n+  \/\/ or == null if they have not.\n@@ -103,1 +103,1 @@\n-    assert(_num > 0 && _stacks != NULL, \"stacks should have been initialized\");\n+    assert(_num > 0 && _stacks != nullptr, \"stacks should have been initialized\");\n@@ -113,1 +113,1 @@\n-  \/\/ is NULL, perform the work serially in the current thread.\n+  \/\/ is null, perform the work serially in the current thread.\n@@ -125,1 +125,1 @@\n-      : _in_c_heap(in_c_heap), _num(0), _stacks(NULL) { }\n+      : _in_c_heap(in_c_heap), _num(0), _stacks(nullptr) { }\n@@ -128,1 +128,1 @@\n-    assert(_stacks == NULL && _num == 0, \"stacks should have been reclaimed\");\n+    assert(_stacks == nullptr && _num == 0, \"stacks should have been reclaimed\");\n","filename":"src\/hotspot\/share\/gc\/shared\/preservedMarks.hpp","additions":7,"deletions":7,"binary":false,"changes":14,"status":"modified"},{"patch":"@@ -2,1 +2,1 @@\n- * Copyright (c) 1997, 2022, Oracle and\/or its affiliates. All rights reserved.\n+ * Copyright (c) 1997, 2023, Oracle and\/or its affiliates. All rights reserved.\n@@ -30,1 +30,0 @@\n-#include \"gc\/shared\/genOopClosures.inline.hpp\"\n@@ -53,1 +52,1 @@\n-  if (top_obj != NULL && top_obj < (_sp->toContiguousSpace())->top()) {\n+  if (top_obj != nullptr && top_obj < (_sp->toContiguousSpace())->top()) {\n@@ -98,1 +97,1 @@\n-  assert(_last_bottom == NULL || top <= _last_bottom,\n+  assert(_last_bottom == nullptr || top <= _last_bottom,\n@@ -114,1 +113,1 @@\n-  if (_min_done != NULL && _min_done < top) {\n+  if (_min_done != nullptr && _min_done < top) {\n@@ -124,1 +123,1 @@\n-         (_min_done == NULL || top <= _min_done),\n+         (_min_done == nullptr || top <= _min_done),\n@@ -172,1 +171,1 @@\n-ContiguousSpace::ContiguousSpace(): CompactibleSpace(), _top(NULL) {\n+ContiguousSpace::ContiguousSpace(): CompactibleSpace(), _top(nullptr) {\n@@ -248,1 +247,1 @@\n-  _next_compaction_space = NULL;\n+  _next_compaction_space = nullptr;\n@@ -266,1 +265,1 @@\n-    if (cp->space == NULL) {\n+    if (cp->space == nullptr) {\n@@ -268,1 +267,1 @@\n-      assert(cp->gen != NULL, \"compaction must succeed\");\n+      assert(cp->gen != nullptr, \"compaction must succeed\");\n@@ -270,1 +269,1 @@\n-      assert(cp->space != NULL, \"generation must have a first compaction space\");\n+      assert(cp->space != nullptr, \"generation must have a first compaction space\");\n@@ -308,2 +307,2 @@\n-  if (cp->space == NULL) {\n-    assert(cp->gen != NULL, \"need a generation\");\n+  if (cp->space == nullptr) {\n+    assert(cp->gen != nullptr, \"need a generation\");\n@@ -321,1 +320,1 @@\n-  HeapWord*  first_dead = NULL; \/\/ The first dead object.\n+  HeapWord*  first_dead = nullptr; \/\/ The first dead object.\n@@ -359,1 +358,1 @@\n-        if (first_dead == NULL) {\n+        if (first_dead == nullptr) {\n@@ -371,1 +370,1 @@\n-  if (first_dead != NULL) {\n+  if (first_dead != nullptr) {\n@@ -399,1 +398,1 @@\n-  debug_only(HeapWord* prev_obj = NULL);\n+  debug_only(HeapWord* prev_obj = nullptr);\n@@ -448,1 +447,1 @@\n-  debug_only(HeapWord* prev_obj = NULL);\n+  debug_only(HeapWord* prev_obj = nullptr);\n@@ -474,1 +473,1 @@\n-      assert(new_obj->klass() != NULL, \"should have a class\");\n+      assert(new_obj->klass() != nullptr, \"should have a class\");\n@@ -519,1 +518,1 @@\n-  HeapWord* prev_p = NULL;\n+  HeapWord* prev_p = nullptr;\n@@ -616,1 +615,1 @@\n-    return NULL;\n+    return nullptr;\n@@ -635,1 +634,1 @@\n-      return NULL;\n+      return nullptr;\n@@ -673,1 +672,1 @@\n-  HeapWord* prev_p = NULL;\n+  HeapWord* prev_p = nullptr;\n","filename":"src\/hotspot\/share\/gc\/shared\/space.cpp","additions":22,"deletions":23,"binary":false,"changes":45,"status":"modified"},{"patch":"@@ -2,1 +2,1 @@\n- * Copyright (c) 1997, 2022, Oracle and\/or its affiliates. All rights reserved.\n+ * Copyright (c) 1997, 2023, Oracle and\/or its affiliates. All rights reserved.\n@@ -81,1 +81,1 @@\n-    _bottom(NULL), _end(NULL) { }\n+    _bottom(nullptr), _end(nullptr) { }\n@@ -180,1 +180,1 @@\n-  \/\/ object or a non-object.  If \"p\" is not in the space, return NULL.\n+  \/\/ object or a non-object.  If \"p\" is not in the space, return null.\n@@ -202,1 +202,1 @@\n-  \/\/ Allocation (return NULL if full).  Assumes the caller has established\n+  \/\/ Allocation (return null if full).  Assumes the caller has established\n@@ -206,1 +206,1 @@\n-  \/\/ Allocation (return NULL if full).  Enforces mutual exclusion internally.\n+  \/\/ Allocation (return null if full).  Enforces mutual exclusion internally.\n@@ -221,1 +221,1 @@\n-  \/\/ IF \"this\" is a ContiguousSpace, return it, else return NULL.\n+  \/\/ IF \"this\" is a ContiguousSpace, return it, else return null.\n@@ -223,1 +223,1 @@\n-    return NULL;\n+    return nullptr;\n@@ -251,1 +251,1 @@\n-                                \/\/ shouldn't be done again.  NULL means infinity.)\n+                                \/\/ shouldn't be done again.  null means infinity.)\n@@ -282,2 +282,2 @@\n-    _cl(cl), _sp(sp), _min_done(NULL) {\n-    NOT_PRODUCT(_last_bottom = NULL);\n+    _cl(cl), _sp(sp), _min_done(nullptr) {\n+    NOT_PRODUCT(_last_bottom = nullptr);\n@@ -296,2 +296,2 @@\n-  CompactPoint(Generation* g = NULL) :\n-    gen(g), space(NULL) {}\n+  CompactPoint(Generation* g = nullptr) :\n+    gen(g), space(nullptr) {}\n@@ -318,1 +318,1 @@\n-   _compaction_top(NULL), _next_compaction_space(NULL) {}\n+   _compaction_top(nullptr), _next_compaction_space(nullptr) {}\n@@ -328,1 +328,1 @@\n-    assert(value == NULL || (value >= bottom() && value <= end()),\n+    assert(value == nullptr || (value >= bottom() && value <= end()),\n@@ -413,1 +413,1 @@\n-  \/\/ Allocation helpers (return NULL if full).\n+  \/\/ Allocation helpers (return null if full).\n@@ -449,1 +449,1 @@\n-  \/\/ This code may be NULL depending on the macro DEBUG_MANGLING.\n+  \/\/ This code may be null depending on the macro DEBUG_MANGLING.\n@@ -462,1 +462,1 @@\n-  \/\/ Allocation (return NULL if full)\n+  \/\/ Allocation (return null if full)\n","filename":"src\/hotspot\/share\/gc\/shared\/space.hpp","additions":17,"deletions":17,"binary":false,"changes":34,"status":"modified"},{"patch":"@@ -2,1 +2,1 @@\n- * Copyright (c) 2000, 2022, Oracle and\/or its affiliates. All rights reserved.\n+ * Copyright (c) 2000, 2023, Oracle and\/or its affiliates. All rights reserved.\n@@ -50,1 +50,1 @@\n-  if (res != NULL) {\n+  if (res != nullptr) {\n@@ -70,1 +70,1 @@\n-  if (res != NULL) {\n+  if (res != nullptr) {\n@@ -141,1 +141,1 @@\n-     HeapWord* prev_obj = NULL;\n+     HeapWord* prev_obj = nullptr;\n@@ -176,1 +176,1 @@\n-  assert(p != NULL, \"expected saved mark\");\n+  assert(p != nullptr, \"expected saved mark\");\n","filename":"src\/hotspot\/share\/gc\/shared\/space.inline.hpp","additions":5,"deletions":5,"binary":false,"changes":10,"status":"modified"},{"patch":"@@ -2,1 +2,1 @@\n- * Copyright (c) 2018, 2020, Oracle and\/or its affiliates. All rights reserved.\n+ * Copyright (c) 2018, 2023, Oracle and\/or its affiliates. All rights reserved.\n@@ -62,1 +62,1 @@\n-    if (obj == NULL) {\n+    if (obj == nullptr) {\n@@ -69,1 +69,1 @@\n-      *p = NULL;\n+      *p = nullptr;\n@@ -94,1 +94,1 @@\n-    if (_times != NULL) {\n+    if (_times != nullptr) {\n","filename":"src\/hotspot\/share\/gc\/shared\/weakProcessor.inline.hpp","additions":4,"deletions":4,"binary":false,"changes":8,"status":"modified"},{"patch":"@@ -44,1 +44,1 @@\n-  if (r != NULL && r->is_committed()) {\n+  if (r != nullptr && r->is_committed()) {\n@@ -102,1 +102,1 @@\n-    if (r != NULL) {\n+    if (r != nullptr) {\n@@ -117,1 +117,1 @@\n-  bool loc_in_heap = (loc != NULL && heap->is_in(loc));\n+  bool loc_in_heap = (loc != nullptr && heap->is_in(loc));\n@@ -122,1 +122,1 @@\n-  if (interior_loc != NULL) {\n+  if (interior_loc != nullptr) {\n@@ -174,1 +174,1 @@\n-    print_failure(_safe_unknown, obj, interior_loc, NULL, \"Shenandoah assert_in_heap failed\",\n+    print_failure(_safe_unknown, obj, interior_loc, nullptr, \"Shenandoah assert_in_heap failed\",\n@@ -183,2 +183,2 @@\n-  if (obj != NULL && !heap->is_in(obj)) {\n-    print_failure(_safe_unknown, obj, interior_loc, NULL, \"Shenandoah assert_in_heap_or_null failed\",\n+  if (obj != nullptr && !heap->is_in(obj)) {\n+    print_failure(_safe_unknown, obj, interior_loc, nullptr, \"Shenandoah assert_in_heap_or_null failed\",\n@@ -196,1 +196,1 @@\n-    print_failure(_safe_unknown, obj, interior_loc, NULL, \"Shenandoah assert_correct failed\",\n+    print_failure(_safe_unknown, obj, interior_loc, nullptr, \"Shenandoah assert_correct failed\",\n@@ -202,3 +202,3 @@\n-  if (obj_klass == NULL) {\n-    print_failure(_safe_unknown, obj, interior_loc, NULL, \"Shenandoah assert_correct failed\",\n-                  \"Object klass pointer should not be NULL\",\n+  if (obj_klass == nullptr) {\n+    print_failure(_safe_unknown, obj, interior_loc, nullptr, \"Shenandoah assert_correct failed\",\n+                  \"Object klass pointer should not be null\",\n@@ -209,1 +209,1 @@\n-    print_failure(_safe_unknown, obj, interior_loc, NULL, \"Shenandoah assert_correct failed\",\n+    print_failure(_safe_unknown, obj, interior_loc, nullptr, \"Shenandoah assert_correct failed\",\n@@ -221,1 +221,1 @@\n-      print_failure(_safe_oop, obj, interior_loc, NULL, \"Shenandoah assert_correct failed\",\n+      print_failure(_safe_oop, obj, interior_loc, nullptr, \"Shenandoah assert_correct failed\",\n@@ -228,1 +228,1 @@\n-      print_failure(_safe_oop, obj, interior_loc, NULL, \"Shenandoah assert_correct failed\",\n+      print_failure(_safe_oop, obj, interior_loc, nullptr, \"Shenandoah assert_correct failed\",\n@@ -234,1 +234,1 @@\n-      print_failure(_safe_oop, obj, interior_loc, NULL, \"Shenandoah assert_correct failed\",\n+      print_failure(_safe_oop, obj, interior_loc, nullptr, \"Shenandoah assert_correct failed\",\n@@ -241,1 +241,1 @@\n-      print_failure(_safe_all, obj, interior_loc, NULL, \"Shenandoah assert_correct failed\",\n+      print_failure(_safe_all, obj, interior_loc, nullptr, \"Shenandoah assert_correct failed\",\n@@ -249,1 +249,1 @@\n-      print_failure(_safe_all, obj, interior_loc, NULL, \"Shenandoah assert_correct failed\",\n+      print_failure(_safe_all, obj, interior_loc, nullptr, \"Shenandoah assert_correct failed\",\n@@ -262,1 +262,1 @@\n-    print_failure(_safe_unknown, obj, interior_loc, NULL, \"Shenandoah assert_in_correct_region failed\",\n+    print_failure(_safe_unknown, obj, interior_loc, nullptr, \"Shenandoah assert_in_correct_region failed\",\n@@ -274,1 +274,1 @@\n-        print_failure(_safe_unknown, obj, interior_loc, NULL, \"Shenandoah assert_in_correct_region failed\",\n+        print_failure(_safe_unknown, obj, interior_loc, nullptr, \"Shenandoah assert_in_correct_region failed\",\n@@ -279,1 +279,1 @@\n-        print_failure(_safe_oop, obj, interior_loc, NULL, \"Shenandoah assert_in_correct_region failed\",\n+        print_failure(_safe_oop, obj, interior_loc, nullptr, \"Shenandoah assert_in_correct_region failed\",\n@@ -292,1 +292,1 @@\n-    print_failure(_safe_all, obj, interior_loc, NULL, \"Shenandoah assert_forwarded failed\",\n+    print_failure(_safe_all, obj, interior_loc, nullptr, \"Shenandoah assert_forwarded failed\",\n@@ -303,1 +303,1 @@\n-    print_failure(_safe_all, obj, interior_loc, NULL, \"Shenandoah assert_not_forwarded failed\",\n+    print_failure(_safe_all, obj, interior_loc, nullptr, \"Shenandoah assert_not_forwarded failed\",\n@@ -314,1 +314,1 @@\n-    print_failure(_safe_all, obj, interior_loc, NULL, \"Shenandoah assert_marked failed\",\n+    print_failure(_safe_all, obj, interior_loc, nullptr, \"Shenandoah assert_marked failed\",\n@@ -325,1 +325,1 @@\n-    print_failure(_safe_all, obj, interior_loc, NULL, \"Shenandoah assert_marked_weak failed\",\n+    print_failure(_safe_all, obj, interior_loc, nullptr, \"Shenandoah assert_marked_weak failed\",\n@@ -336,1 +336,1 @@\n-    print_failure(_safe_all, obj, interior_loc, NULL, \"Shenandoah assert_marked_strong failed\",\n+    print_failure(_safe_all, obj, interior_loc, nullptr, \"Shenandoah assert_marked_strong failed\",\n@@ -347,1 +347,1 @@\n-    print_failure(_safe_all, obj, interior_loc, NULL, \"Shenandoah assert_in_cset failed\",\n+    print_failure(_safe_all, obj, interior_loc, nullptr, \"Shenandoah assert_in_cset failed\",\n@@ -358,1 +358,1 @@\n-    print_failure(_safe_all, obj, interior_loc, NULL, \"Shenandoah assert_not_in_cset failed\",\n+    print_failure(_safe_all, obj, interior_loc, nullptr, \"Shenandoah assert_not_in_cset failed\",\n@@ -367,1 +367,1 @@\n-    print_failure(_safe_unknown, NULL, interior_loc, NULL, \"Shenandoah assert_not_in_cset_loc failed\",\n+    print_failure(_safe_unknown, nullptr, interior_loc, nullptr, \"Shenandoah assert_not_in_cset_loc failed\",\n","filename":"src\/hotspot\/share\/gc\/shenandoah\/shenandoahAsserts.cpp","additions":28,"deletions":28,"binary":false,"changes":56,"status":"modified"},{"patch":"@@ -36,1 +36,1 @@\n-  shenandoah_assert_in_heap(NULL, obj);\n+  shenandoah_assert_in_heap(nullptr, obj);\n@@ -42,1 +42,1 @@\n-  \/\/ On this path, we can encounter the \"marked\" object, but with NULL\n+  \/\/ On this path, we can encounter the \"marked\" object, but with null\n@@ -48,1 +48,1 @@\n-    if (fwdptr != NULL) {\n+    if (fwdptr != nullptr) {\n@@ -56,2 +56,2 @@\n-  \/\/ Same as above, but mutator thread cannot ever see NULL forwardee.\n-  shenandoah_assert_correct(NULL, obj);\n+  \/\/ Same as above, but mutator thread cannot ever see null forwardee.\n+  shenandoah_assert_correct(nullptr, obj);\n@@ -63,1 +63,1 @@\n-    assert(fwdptr != NULL, \"Forwarding pointer is never null here\");\n+    assert(fwdptr != nullptr, \"Forwarding pointer is never null here\");\n@@ -71,1 +71,1 @@\n-  shenandoah_assert_correct(NULL, obj);\n+  shenandoah_assert_correct(nullptr, obj);\n","filename":"src\/hotspot\/share\/gc\/shenandoah\/shenandoahForwarding.inline.hpp","additions":7,"deletions":7,"binary":false,"changes":14,"status":"modified"},{"patch":"@@ -321,1 +321,1 @@\n-    _from_region(NULL),\n+    _from_region(nullptr),\n@@ -329,1 +329,1 @@\n-    assert(_to_region != NULL, \"should not happen\");\n+    assert(_to_region != nullptr, \"should not happen\");\n@@ -342,1 +342,1 @@\n-    assert(_from_region != NULL, \"must set before work\");\n+    assert(_from_region != nullptr, \"must set before work\");\n@@ -361,1 +361,1 @@\n-      assert(new_to_region != NULL, \"must not be NULL\");\n+      assert(new_to_region != nullptr, \"must not be null\");\n@@ -368,1 +368,1 @@\n-    shenandoah_assert_not_forwarded(NULL, p);\n+    shenandoah_assert_not_forwarded(nullptr, p);\n@@ -406,1 +406,1 @@\n-    if (from_region == NULL) {\n+    if (from_region == nullptr) {\n@@ -418,1 +418,1 @@\n-    while (from_region != NULL) {\n+    while (from_region != nullptr) {\n@@ -673,1 +673,1 @@\n-    while (r != NULL) {\n+    while (r != nullptr) {\n@@ -789,1 +789,1 @@\n-    while (r != NULL) {\n+    while (r != nullptr) {\n@@ -884,1 +884,1 @@\n-    while (r != NULL) {\n+    while (r != nullptr) {\n@@ -1030,1 +1030,1 @@\n-    while (region != NULL) {\n+    while (region != nullptr) {\n","filename":"src\/hotspot\/share\/gc\/shenandoah\/shenandoahFullGC.cpp","additions":11,"deletions":11,"binary":false,"changes":22,"status":"modified"},{"patch":"@@ -106,1 +106,1 @@\n-    while (r != NULL) {\n+    while (r != nullptr) {\n@@ -130,1 +130,1 @@\n-    while (r != NULL) {\n+    while (r != nullptr) {\n@@ -182,3 +182,3 @@\n-  size_t heap_page_size   = UseLargePages ? (size_t)os::large_page_size() : (size_t)os::vm_page_size();\n-  size_t bitmap_page_size = UseLargePages ? (size_t)os::large_page_size() : (size_t)os::vm_page_size();\n-  size_t region_page_size = UseLargePages ? (size_t)os::large_page_size() : (size_t)os::vm_page_size();\n+  size_t heap_page_size   = UseLargePages ? os::large_page_size() : os::vm_page_size();\n+  size_t bitmap_page_size = UseLargePages ? os::large_page_size() : os::vm_page_size();\n+  size_t region_page_size = UseLargePages ? os::large_page_size() : os::vm_page_size();\n@@ -317,1 +317,1 @@\n-    if (_collection_set == NULL) {\n+    if (_collection_set == nullptr) {\n@@ -403,1 +403,1 @@\n-    _pacer = NULL;\n+    _pacer = nullptr;\n@@ -414,1 +414,1 @@\n-  if (ShenandoahGCMode != NULL) {\n+  if (ShenandoahGCMode != nullptr) {\n@@ -441,1 +441,1 @@\n-  assert(_gc_mode != NULL, \"Must be initialized\");\n+  assert(_gc_mode != nullptr, \"Must be initialized\");\n@@ -468,2 +468,2 @@\n-  _workers(NULL),\n-  _safepoint_workers(NULL),\n+  _workers(nullptr),\n+  _safepoint_workers(nullptr),\n@@ -472,1 +472,1 @@\n-  _regions(NULL),\n+  _regions(nullptr),\n@@ -474,1 +474,1 @@\n-  _control_thread(NULL),\n+  _control_thread(nullptr),\n@@ -476,8 +476,8 @@\n-  _gc_mode(NULL),\n-  _heuristics(NULL),\n-  _free_set(NULL),\n-  _pacer(NULL),\n-  _verifier(NULL),\n-  _phase_timings(NULL),\n-  _monitoring_support(NULL),\n-  _memory_pool(NULL),\n+  _gc_mode(nullptr),\n+  _heuristics(nullptr),\n+  _free_set(nullptr),\n+  _pacer(nullptr),\n+  _verifier(nullptr),\n+  _phase_timings(nullptr),\n+  _monitoring_support(nullptr),\n+  _memory_pool(nullptr),\n@@ -490,1 +490,1 @@\n-  _marking_context(NULL),\n+  _marking_context(nullptr),\n@@ -496,2 +496,2 @@\n-  _liveness_cache(NULL),\n-  _collection_set(NULL)\n+  _liveness_cache(nullptr),\n+  _collection_set(nullptr)\n@@ -505,1 +505,1 @@\n-  if (_workers == NULL) {\n+  if (_workers == nullptr) {\n@@ -534,1 +534,1 @@\n-    while (region != NULL) {\n+    while (region != nullptr) {\n@@ -589,1 +589,1 @@\n-  if (cset != NULL) {\n+  if (cset != nullptr) {\n@@ -593,1 +593,1 @@\n-    st->print_cr(\" (NULL)\");\n+    st->print_cr(\" (null)\");\n@@ -607,1 +607,1 @@\n-    assert(thread != NULL, \"Sanity\");\n+    assert(thread != nullptr, \"Sanity\");\n@@ -623,1 +623,1 @@\n-  if (_safepoint_workers != NULL) {\n+  if (_safepoint_workers != nullptr) {\n@@ -766,1 +766,1 @@\n-    return NULL;\n+    return nullptr;\n@@ -775,2 +775,2 @@\n-  if (gclab_buf == NULL) {\n-    return NULL;\n+  if (gclab_buf == nullptr) {\n+    return nullptr;\n@@ -803,1 +803,1 @@\n-  if (res != NULL) {\n+  if (res != nullptr) {\n@@ -816,1 +816,1 @@\n-  if (res != NULL) {\n+  if (res != nullptr) {\n@@ -827,1 +827,1 @@\n-  HeapWord* result = NULL;\n+  HeapWord* result = nullptr;\n@@ -851,1 +851,1 @@\n-    while (result == NULL && _progress_last_gc.is_set()) {\n+    while (result == nullptr && _progress_last_gc.is_set()) {\n@@ -857,1 +857,1 @@\n-    while (result == NULL && tries <= ShenandoahFullGCThreshold) {\n+    while (result == nullptr && tries <= ShenandoahFullGCThreshold) {\n@@ -874,1 +874,1 @@\n-  if (result != NULL) {\n+  if (result != nullptr) {\n@@ -923,1 +923,1 @@\n-  if (result != NULL) {\n+  if (result != nullptr) {\n@@ -932,1 +932,1 @@\n-  if (result != NULL) {\n+  if (result != nullptr) {\n@@ -938,1 +938,1 @@\n-  if (result != NULL) {\n+  if (result != nullptr) {\n@@ -943,1 +943,1 @@\n-  return NULL;\n+  return nullptr;\n@@ -955,1 +955,1 @@\n-    shenandoah_assert_marked(NULL, p);\n+    shenandoah_assert_marked(nullptr, p);\n@@ -994,1 +994,1 @@\n-    while ((r =_cs->claim_next()) != NULL) {\n+    while ((r =_cs->claim_next()) != nullptr) {\n@@ -1020,1 +1020,1 @@\n-  while ((r = set->next()) != NULL) {\n+  while ((r = set->next()) != nullptr) {\n@@ -1065,1 +1065,1 @@\n-    assert(gclab != NULL, \"GCLAB should be initialized for %s\", thread->name());\n+    assert(gclab != nullptr, \"GCLAB should be initialized for %s\", thread->name());\n@@ -1077,1 +1077,1 @@\n-    assert(gclab != NULL, \"GCLAB should be initialized for %s\", thread->name());\n+    assert(gclab != nullptr, \"GCLAB should be initialized for %s\", thread->name());\n@@ -1134,1 +1134,1 @@\n-  if (safepoint_workers() != NULL) {\n+  if (safepoint_workers() != nullptr) {\n@@ -1165,1 +1165,1 @@\n-  if (r != NULL) {\n+  if (r != nullptr) {\n@@ -1168,1 +1168,1 @@\n-  return NULL;\n+  return nullptr;\n@@ -1189,1 +1189,1 @@\n-  if (_safepoint_workers != NULL) {\n+  if (_safepoint_workers != nullptr) {\n@@ -1328,1 +1328,1 @@\n-  uint n_workers = safepoint_workers() != NULL ? safepoint_workers()->active_workers() : 1;\n+  uint n_workers = safepoint_workers() != nullptr ? safepoint_workers()->active_workers() : 1;\n@@ -1407,1 +1407,1 @@\n-    if (_task_queues!= NULL) {\n+    if (_task_queues!= nullptr) {\n@@ -1410,1 +1410,1 @@\n-        if (q != NULL) {\n+        if (q != nullptr) {\n@@ -1412,1 +1412,1 @@\n-          _task_queues->register_queue(i, NULL);\n+          _task_queues->register_queue(i, nullptr);\n@@ -1416,1 +1416,1 @@\n-      _task_queues = NULL;\n+      _task_queues = nullptr;\n@@ -1456,1 +1456,1 @@\n-    assert(queue_set != NULL, \"task queue must not be NULL\");\n+    assert(queue_set != nullptr, \"task queue must not be null\");\n@@ -1459,1 +1459,1 @@\n-    assert(q != NULL, \"object iterate queue must not be NULL\");\n+    assert(q != nullptr, \"object iterate queue must not be null\");\n@@ -1482,1 +1482,1 @@\n-  if (is_concurrent_mark_in_progress() && (obj != NULL)) {\n+  if (is_concurrent_mark_in_progress() && (obj != nullptr)) {\n@@ -1879,1 +1879,1 @@\n-  assert(heap->collection_set() != NULL, \"Sanity\");\n+  assert(heap->collection_set() != nullptr, \"Sanity\");\n@@ -1930,1 +1930,1 @@\n-  assert(r != NULL, \"Sanity\");\n+  assert(r != nullptr, \"Sanity\");\n@@ -2012,1 +2012,1 @@\n-  assert (_verifier != NULL, \"sanity\");\n+  assert (_verifier != nullptr, \"sanity\");\n@@ -2045,1 +2045,1 @@\n-    while (r != NULL) {\n+    while (r != nullptr) {\n@@ -2290,1 +2290,1 @@\n-  assert(_liveness_cache != NULL, \"sanity\");\n+  assert(_liveness_cache != nullptr, \"sanity\");\n@@ -2301,1 +2301,1 @@\n-  assert(_liveness_cache != NULL, \"sanity\");\n+  assert(_liveness_cache != nullptr, \"sanity\");\n","filename":"src\/hotspot\/share\/gc\/shenandoah\/shenandoahHeap.cpp","additions":66,"deletions":66,"binary":false,"changes":132,"status":"modified"},{"patch":"@@ -97,1 +97,1 @@\n-  \/\/ Returns next region, or NULL if there are no more regions.\n+  \/\/ Returns next region, or null if there are no more regions.\n","filename":"src\/hotspot\/share\/gc\/shenandoah\/shenandoahHeap.hpp","additions":1,"deletions":1,"binary":false,"changes":2,"status":"modified"},{"patch":"@@ -61,1 +61,1 @@\n-  \/\/ get_region() provides the bounds-check and returns NULL on OOB.\n+  \/\/ get_region() provides the bounds-check and returns null on OOB.\n@@ -223,2 +223,2 @@\n-\/\/ The memory ordering discussion above does not apply for methods that store NULLs:\n-\/\/ then, there is no transitive reads in mutator (as we see NULLs), and we can do\n+\/\/ The memory ordering discussion above does not apply for methods that store nulls:\n+\/\/ then, there is no transitive reads in mutator (as we see nulls), and we can do\n@@ -278,1 +278,1 @@\n-  if (gclab == NULL) {\n+  if (gclab == nullptr) {\n@@ -282,1 +282,1 @@\n-    return NULL;\n+    return nullptr;\n@@ -285,1 +285,1 @@\n-  if (obj != NULL) {\n+  if (obj != nullptr) {\n@@ -306,1 +306,1 @@\n-  HeapWord* copy = NULL;\n+  HeapWord* copy = nullptr;\n@@ -311,1 +311,1 @@\n-        copy = NULL;\n+        copy = nullptr;\n@@ -317,1 +317,1 @@\n-    if (copy == NULL) {\n+    if (copy == nullptr) {\n@@ -326,1 +326,1 @@\n-  if (copy == NULL) {\n+  if (copy == nullptr) {\n@@ -349,1 +349,1 @@\n-    shenandoah_assert_correct(NULL, copy_val);\n+    shenandoah_assert_correct(nullptr, copy_val);\n@@ -367,1 +367,1 @@\n-      shenandoah_assert_correct(NULL, copy_val);\n+      shenandoah_assert_correct(nullptr, copy_val);\n@@ -369,1 +369,1 @@\n-    shenandoah_assert_correct(NULL, result);\n+    shenandoah_assert_correct(nullptr, result);\n@@ -380,1 +380,1 @@\n-  assert(collection_set() != NULL, \"Sanity\");\n+  assert(collection_set() != nullptr, \"Sanity\");\n@@ -385,1 +385,1 @@\n-  assert(collection_set() != NULL, \"Sanity\");\n+  assert(collection_set() != nullptr, \"Sanity\");\n@@ -574,1 +574,1 @@\n-    return NULL;\n+    return nullptr;\n","filename":"src\/hotspot\/share\/gc\/shenandoah\/shenandoahHeap.inline.hpp","additions":16,"deletions":16,"binary":false,"changes":32,"status":"modified"},{"patch":"@@ -82,2 +82,2 @@\n-    _interior_loc(NULL),\n-    _loc(NULL) {\n+    _interior_loc(nullptr),\n+    _loc(nullptr) {\n@@ -134,2 +134,2 @@\n-      check(ShenandoahAsserts::_safe_unknown, obj, obj_klass != NULL,\n-             \"Object klass pointer should not be NULL\");\n+      check(ShenandoahAsserts::_safe_unknown, obj, obj_klass != nullptr,\n+             \"Object klass pointer should not be null\");\n@@ -178,1 +178,1 @@\n-    ShenandoahHeapRegion* fwd_reg = NULL;\n+    ShenandoahHeapRegion* fwd_reg = nullptr;\n@@ -190,2 +190,2 @@\n-      check(ShenandoahAsserts::_safe_oop, obj, fwd_klass != NULL,\n-             \"Forwardee klass pointer should not be NULL\");\n+      check(ShenandoahAsserts::_safe_oop, obj, fwd_klass != nullptr,\n+             \"Forwardee klass pointer should not be null\");\n@@ -288,1 +288,1 @@\n-    _interior_loc = NULL;\n+    _interior_loc = nullptr;\n@@ -298,1 +298,1 @@\n-    _interior_loc = NULL;\n+    _interior_loc = nullptr;\n@@ -300,1 +300,1 @@\n-    _interior_loc = NULL;\n+    _interior_loc = nullptr;\n@@ -311,1 +311,1 @@\n-    _loc = NULL;\n+    _loc = nullptr;\n@@ -925,1 +925,1 @@\n-        ShenandoahAsserts::print_failure(ShenandoahAsserts::_safe_all, obj, p, NULL,\n+        ShenandoahAsserts::print_failure(ShenandoahAsserts::_safe_all, obj, p, nullptr,\n@@ -946,1 +946,1 @@\n-        ShenandoahAsserts::print_failure(ShenandoahAsserts::_safe_all, obj, p, NULL,\n+        ShenandoahAsserts::print_failure(ShenandoahAsserts::_safe_all, obj, p, nullptr,\n@@ -951,1 +951,1 @@\n-        ShenandoahAsserts::print_failure(ShenandoahAsserts::_safe_all, obj, p, NULL,\n+        ShenandoahAsserts::print_failure(ShenandoahAsserts::_safe_all, obj, p, nullptr,\n@@ -957,1 +957,1 @@\n-        ShenandoahAsserts::print_failure(ShenandoahAsserts::_safe_all, obj, p, NULL,\n+        ShenandoahAsserts::print_failure(ShenandoahAsserts::_safe_all, obj, p, nullptr,\n","filename":"src\/hotspot\/share\/gc\/shenandoah\/shenandoahVerifier.cpp","additions":15,"deletions":15,"binary":false,"changes":30,"status":"modified"},{"patch":"@@ -2,1 +2,1 @@\n- * Copyright (c) 2015, 2022, Oracle and\/or its affiliates. All rights reserved.\n+ * Copyright (c) 2015, 2023, Oracle and\/or its affiliates. All rights reserved.\n@@ -566,0 +566,17 @@\n+\n+#ifndef PRODUCT\n+void ZBarrierSetC2::dump_barrier_data(const MachNode* mach, outputStream* st) const {\n+  if ((mach->barrier_data() & ZLoadBarrierStrong) != 0) {\n+    st->print(\"strong \");\n+  }\n+  if ((mach->barrier_data() & ZLoadBarrierWeak) != 0) {\n+    st->print(\"weak \");\n+  }\n+  if ((mach->barrier_data() & ZLoadBarrierPhantom) != 0) {\n+    st->print(\"phantom \");\n+  }\n+  if ((mach->barrier_data() & ZLoadBarrierNoKeepalive) != 0) {\n+    st->print(\"nokeepalive \");\n+  }\n+}\n+#endif \/\/ !PRODUCT\n","filename":"src\/hotspot\/share\/gc\/z\/c2\/zBarrierSetC2.cpp","additions":18,"deletions":1,"binary":false,"changes":19,"status":"modified"},{"patch":"@@ -1134,4 +1134,0 @@\n-  if (java_lang_VirtualThread::notify_jvmti_events()) {\n-    JvmtiExport::check_vthread_and_suspend_at_safepoint(current);\n-  }\n-\n","filename":"src\/hotspot\/share\/interpreter\/interpreterRuntime.cpp","additions":0,"deletions":4,"binary":false,"changes":4,"status":"modified"},{"patch":"@@ -82,1 +82,1 @@\n-  static_field(CompilerToVM::Data,             vm_page_size,                           int)                                          \\\n+  static_field(CompilerToVM::Data,             vm_page_size,                           size_t)                                       \\\n@@ -161,2 +161,2 @@\n-  nonstatic_field(InstanceKlass,               _init_state,                                   InstanceKlass::ClassState)             \\\n-  nonstatic_field(InstanceKlass,               _init_thread,                                  Thread*)                               \\\n+  volatile_nonstatic_field(InstanceKlass,      _init_state,                                   InstanceKlass::ClassState)             \\\n+  volatile_nonstatic_field(InstanceKlass,      _init_thread,                                  JavaThread*)                           \\\n","filename":"src\/hotspot\/share\/jvmci\/vmStructs_jvmci.cpp","additions":3,"deletions":3,"binary":false,"changes":6,"status":"modified"},{"patch":"@@ -177,9 +177,16 @@\n-\/\/ This only exists for JFR and jcmd VM.classloader_stats. We may want to\n-\/\/  change this. Capacity as a stat is of questionable use since it may\n-\/\/  contain committed and uncommitted areas. For now we do this to maintain\n-\/\/  backward compatibility with JFR.\n-void ClassLoaderMetaspace::calculate_jfr_stats(size_t* p_used_bytes, size_t* p_capacity_bytes) const {\n-  \/\/ Implement this using the standard statistics objects.\n-  size_t used_c = 0, cap_c = 0, used_nc = 0, cap_nc = 0;\n-  if (non_class_space_arena() != nullptr) {\n-    non_class_space_arena()->usage_numbers(&used_nc, nullptr, &cap_nc);\n+\/\/ Convenience method to get the most important usage statistics.\n+void ClassLoaderMetaspace::usage_numbers(Metaspace::MetadataType mdType, size_t* p_used_words,\n+                                         size_t* p_committed_words, size_t* p_capacity_words) const {\n+  const MetaspaceArena* arena = (mdType == Metaspace::MetadataType::ClassType) ?\n+      class_space_arena() : non_class_space_arena();\n+  arena->usage_numbers(p_used_words, p_committed_words, p_capacity_words);\n+}\n+\n+\/\/ Convenience method to get total usage numbers\n+void ClassLoaderMetaspace::usage_numbers(size_t* p_used_words, size_t* p_committed_words,\n+                                         size_t* p_capacity_words) const {\n+  size_t used_nc, comm_nc, cap_nc;\n+  usage_numbers(Metaspace::MetadataType::NonClassType, &used_nc, &comm_nc, &cap_nc);\n+  size_t used_c = 0, comm_c = 0, cap_c = 0;\n+  if (Metaspace::using_class_space()) {\n+    usage_numbers(Metaspace::MetadataType::ClassType, &used_c, &comm_c, &cap_c);\n@@ -187,2 +194,2 @@\n-  if (class_space_arena() != nullptr) {\n-    class_space_arena()->usage_numbers(&used_c, nullptr, &cap_c);\n+  if (p_used_words != nullptr) {\n+    (*p_used_words) = used_nc + used_c;\n@@ -190,2 +197,2 @@\n-  if (p_used_bytes != nullptr) {\n-    *p_used_bytes = used_c + used_nc;\n+  if (p_committed_words != nullptr) {\n+    (*p_committed_words) = comm_nc + comm_c;\n@@ -193,2 +200,2 @@\n-  if (p_capacity_bytes != nullptr) {\n-    *p_capacity_bytes = cap_c + cap_nc;\n+  if (p_capacity_words != nullptr) {\n+    (*p_capacity_words) = cap_nc + cap_c;\n@@ -197,1 +204,0 @@\n-\n","filename":"src\/hotspot\/share\/memory\/classLoaderMetaspace.cpp","additions":22,"deletions":16,"binary":false,"changes":38,"status":"modified"},{"patch":"@@ -3,1 +3,1 @@\n- * Copyright (c) 2020 SAP SE. All rights reserved.\n+ * Copyright (c) 2020, 2023 SAP SE. All rights reserved.\n@@ -322,3 +322,0 @@\n-      if (Settings::new_chunks_are_fully_committed()) {\n-        assert(new_chunk->is_fully_committed(), \"Chunk should be fully committed.\");\n-      }\n","filename":"src\/hotspot\/share\/memory\/metaspace\/metaspaceArena.cpp","additions":1,"deletions":4,"binary":false,"changes":5,"status":"modified"},{"patch":"@@ -61,1 +61,1 @@\n-  if (preferred_page_size != (size_t)os::vm_page_size()) {\n+  if (preferred_page_size != os::vm_page_size()) {\n@@ -134,1 +134,1 @@\n-         page_size != (size_t) os::vm_page_size();\n+         page_size != os::vm_page_size();\n@@ -259,1 +259,1 @@\n-    } while (page_size > (size_t) os::vm_page_size());\n+    } while (page_size > os::vm_page_size());\n@@ -264,1 +264,1 @@\n-    assert(page_size == (size_t) os::vm_page_size(), \"inv\");\n+    assert(page_size == os::vm_page_size(), \"inv\");\n@@ -287,1 +287,1 @@\n-  assert(page_size >= (size_t) os::vm_page_size(), \"Invalid page size\");\n+  assert(page_size >= os::vm_page_size(), \"Invalid page size\");\n@@ -297,1 +297,1 @@\n-  alignment = MAX2(alignment, (size_t)os::vm_page_size());\n+  alignment = MAX2(alignment, os::vm_page_size());\n@@ -362,1 +362,1 @@\n-  assert(_alignment >= (size_t)os::vm_page_size(), \"must be at least page size big\");\n+  assert(_alignment >= os::vm_page_size(), \"must be at least page size big\");\n@@ -495,1 +495,1 @@\n-  guarantee(alignment == MAX2(alignment, (size_t)os::vm_page_size()), \"alignment too small\");\n+  guarantee(alignment == MAX2(alignment, os::vm_page_size()), \"alignment too small\");\n","filename":"src\/hotspot\/share\/memory\/virtualspace.cpp","additions":8,"deletions":8,"binary":false,"changes":16,"status":"modified"},{"patch":"@@ -2,1 +2,1 @@\n- * Copyright (c) 1997, 2020, Oracle and\/or its affiliates. All rights reserved.\n+ * Copyright (c) 1997, 2023, Oracle and\/or its affiliates. All rights reserved.\n@@ -105,2 +105,2 @@\n-    if (obj != NULL) {\n-      assert(raw == NULL, \"either raw or in-heap\");\n+    if (obj != nullptr) {\n+      assert(raw == nullptr, \"either raw or in-heap\");\n@@ -110,1 +110,1 @@\n-      assert(raw != NULL, \"either raw or in-heap\");\n+      assert(raw != nullptr, \"either raw or in-heap\");\n","filename":"src\/hotspot\/share\/oops\/arrayOop.hpp","additions":4,"deletions":4,"binary":false,"changes":8,"status":"modified"},{"patch":"@@ -33,1 +33,1 @@\n-address CompressedKlassPointers::_base = NULL;\n+address CompressedKlassPointers::_base = nullptr;\n","filename":"src\/hotspot\/share\/oops\/compressedKlass.cpp","additions":1,"deletions":1,"binary":false,"changes":2,"status":"modified"},{"patch":"@@ -100,1 +100,1 @@\n-  static bool is_null(Klass* v)      { return v == NULL; }\n+  static bool is_null(Klass* v)      { return v == nullptr; }\n","filename":"src\/hotspot\/share\/oops\/compressedKlass.hpp","additions":1,"deletions":1,"binary":false,"changes":2,"status":"modified"},{"patch":"@@ -60,1 +60,1 @@\n-  return is_null(v) ? (Klass*)NULL : decode_not_null(v);\n+  return is_null(v) ? nullptr : decode_not_null(v);\n","filename":"src\/hotspot\/share\/oops\/compressedKlass.inline.hpp","additions":1,"deletions":1,"binary":false,"changes":2,"status":"modified"},{"patch":"@@ -2,1 +2,1 @@\n- * Copyright (c) 2019, 2022, Oracle and\/or its affiliates. All rights reserved.\n+ * Copyright (c) 2019, 2023, Oracle and\/or its affiliates. All rights reserved.\n@@ -38,1 +38,1 @@\n-NarrowPtrStruct CompressedOops::_narrow_oop = { NULL, 0, true };\n+NarrowPtrStruct CompressedOops::_narrow_oop = { nullptr, 0, true };\n@@ -82,2 +82,2 @@\n-  assert((intptr_t)base() <= ((intptr_t)_heap_address_range.start() - os::vm_page_size()) ||\n-         base() == NULL, \"invalid value\");\n+  assert((intptr_t)base() <= ((intptr_t)_heap_address_range.start() - (intptr_t)os::vm_page_size()) ||\n+         base() == nullptr, \"invalid value\");\n@@ -151,1 +151,1 @@\n-  return _narrow_oop._base != NULL && is_disjoint_heap_base_address(_narrow_oop._base);\n+  return _narrow_oop._base != nullptr && is_disjoint_heap_base_address(_narrow_oop._base);\n@@ -158,1 +158,1 @@\n-  return _narrow_oop._base != NULL && !is_disjoint_heap_base_address(_narrow_oop._base);\n+  return _narrow_oop._base != nullptr && !is_disjoint_heap_base_address(_narrow_oop._base);\n","filename":"src\/hotspot\/share\/oops\/compressedOops.cpp","additions":6,"deletions":6,"binary":false,"changes":12,"status":"modified"},{"patch":"@@ -2,1 +2,1 @@\n- * Copyright (c) 2019, 2022, Oracle and\/or its affiliates. All rights reserved.\n+ * Copyright (c) 2019, 2023, Oracle and\/or its affiliates. All rights reserved.\n@@ -39,1 +39,1 @@\n-  \/\/ NULL if using wide oops or zero based narrow oops.\n+  \/\/ null if using wide oops or zero based narrow oops.\n@@ -119,1 +119,1 @@\n-  static bool is_null(oop v)       { return v == NULL; }\n+  static bool is_null(oop v)       { return v == nullptr; }\n@@ -130,0 +130,1 @@\n+  static inline oop decode_raw_not_null(oop v);\n","filename":"src\/hotspot\/share\/oops\/compressedOops.hpp","additions":4,"deletions":3,"binary":false,"changes":7,"status":"modified"},{"patch":"@@ -2,1 +2,1 @@\n- * Copyright (c) 2017, 2021, Oracle and\/or its affiliates. All rights reserved.\n+ * Copyright (c) 2017, 2023, Oracle and\/or its affiliates. All rights reserved.\n@@ -63,1 +63,1 @@\n-  return is_null(v) ? (oop)NULL : decode_not_null(v);\n+  return is_null(v) ? nullptr : decode_not_null(v);\n@@ -81,0 +81,5 @@\n+inline oop CompressedOops::decode_raw_not_null(oop v) {\n+  assert(v != nullptr, \"object is null\");\n+  return v;\n+}\n+\n","filename":"src\/hotspot\/share\/oops\/compressedOops.inline.hpp","additions":7,"deletions":2,"binary":false,"changes":9,"status":"modified"},{"patch":"@@ -88,1 +88,1 @@\n-  if (_name != NULL) _name->increment_refcount();\n+  if (_name != nullptr) _name->increment_refcount();\n@@ -101,1 +101,1 @@\n-  while (t != NULL) {\n+  while (t != nullptr) {\n@@ -109,1 +109,1 @@\n-  if (_name != NULL) _name->decrement_refcount();\n+  if (_name != nullptr) _name->decrement_refcount();\n@@ -136,1 +136,1 @@\n-    if (s == NULL || s->next_sibling() != NULL) \/\/ Oops; wrong count; give up\n+    if (s == nullptr || s->next_sibling() != nullptr) \/\/ Oops; wrong count; give up\n@@ -164,1 +164,1 @@\n-  assert(s != NULL, \"Throw NPE!\");\n+  assert(s != nullptr, \"Throw NPE!\");\n@@ -181,1 +181,1 @@\n-  return NULL;\n+  return nullptr;\n@@ -193,1 +193,1 @@\n-  return NULL;\n+  return nullptr;\n@@ -236,1 +236,1 @@\n-  if (super() == NULL)\n+  if (super() == nullptr)\n@@ -245,2 +245,2 @@\n-  if (k == NULL) {\n-    set_super(NULL);\n+  if (k == nullptr) {\n+    set_super(nullptr);\n@@ -250,1 +250,1 @@\n-    assert(super() == NULL || super() == vmClasses::Object_klass(),\n+    assert(super() == nullptr || super() == vmClasses::Object_klass(),\n@@ -281,1 +281,1 @@\n-        assert(primary_super_of_depth(j1) == NULL, \"super list padding\");\n+        assert(primary_super_of_depth(j1) == nullptr, \"super list padding\");\n@@ -283,1 +283,1 @@\n-      while (t != NULL) {\n+      while (t != nullptr) {\n@@ -293,1 +293,1 @@\n-  if (secondary_supers() == NULL) {\n+  if (secondary_supers() == nullptr) {\n@@ -300,1 +300,1 @@\n-    for (p = super(); !(p == NULL || p->can_be_primary_super()); p = p->super()) {\n+    for (p = super(); !(p == nullptr || p->can_be_primary_super()); p = p->super()) {\n@@ -308,1 +308,1 @@\n-    if (secondaries == NULL) {\n+    if (secondaries == nullptr) {\n@@ -315,1 +315,1 @@\n-    for (p = super(); !(p == NULL || p->can_be_primary_super()); p = p->super()) {\n+    for (p = super(); !(p == nullptr || p->can_be_primary_super()); p = p->super()) {\n@@ -347,1 +347,1 @@\n-      \/\/ We must not copy any NULL placeholders left over from bootstrap.\n+      \/\/ We must not copy any null placeholders left over from bootstrap.\n@@ -349,1 +349,1 @@\n-      assert(s2->at(j) != NULL, \"correct bootstrapping order\");\n+      assert(s2->at(j) != nullptr, \"correct bootstrapping order\");\n@@ -360,1 +360,1 @@\n-  assert(transitive_interfaces == NULL, \"sanity\");\n+  assert(transitive_interfaces == nullptr, \"sanity\");\n@@ -362,1 +362,1 @@\n-  return NULL;\n+  return nullptr;\n@@ -368,2 +368,2 @@\n-  assert(super() == NULL || super()->is_instance_klass(), \"must be instance klass\");\n-  return _super == NULL ? NULL : InstanceKlass::cast(_super);\n+  assert(super() == nullptr || super()->is_instance_klass(), \"must be instance klass\");\n+  return _super == nullptr ? nullptr : InstanceKlass::cast(_super);\n@@ -379,1 +379,1 @@\n-       chain != NULL;\n+       chain != nullptr;\n@@ -393,1 +393,1 @@\n-  return NULL;\n+  return nullptr;\n@@ -400,1 +400,1 @@\n-       chain != NULL;\n+       chain != nullptr;\n@@ -413,1 +413,1 @@\n-  return NULL;\n+  return nullptr;\n@@ -436,1 +436,1 @@\n-  if (super == NULL) return;        \/\/ special case: class Object\n+  if (super == nullptr) return;     \/\/ special case: class Object\n@@ -438,1 +438,1 @@\n-          && (super->superklass() == NULL || !is_interface())),\n+          && (super->superklass() == nullptr || !is_interface())),\n@@ -446,1 +446,1 @@\n-    if (prev_first_subklass != NULL) {\n+    if (prev_first_subklass != nullptr) {\n@@ -465,1 +465,1 @@\n-    if (subklass == NULL || subklass->is_loader_alive()) {\n+    if (subklass == nullptr || subklass->is_loader_alive()) {\n@@ -490,1 +490,1 @@\n-    if (sub != NULL) {\n+    if (sub != nullptr) {\n@@ -497,1 +497,1 @@\n-    if (sibling != NULL) {\n+    if (sibling != nullptr) {\n@@ -508,1 +508,1 @@\n-      while ((ik = ik->previous_versions()) != NULL) {\n+      while ((ik = ik->previous_versions()) != nullptr) {\n@@ -530,1 +530,1 @@\n-    \/\/ to follow these pointers anyway, as they will be set to NULL in\n+    \/\/ to follow these pointers anyway, as they will be set to null in\n@@ -553,3 +553,3 @@\n-  set_subklass(NULL);\n-  set_next_sibling(NULL);\n-  set_next_link(NULL);\n+  set_subklass(nullptr);\n+  set_next_sibling(nullptr);\n+  set_next_link(nullptr);\n@@ -558,1 +558,1 @@\n-  set_class_loader_data(NULL);\n+  set_class_loader_data(nullptr);\n@@ -584,1 +584,1 @@\n-  if (class_loader_data() == NULL) {\n+  if (class_loader_data() == nullptr) {\n@@ -593,1 +593,1 @@\n-  ModuleEntry* module_entry = NULL;\n+  ModuleEntry* module_entry = nullptr;\n@@ -606,1 +606,1 @@\n-  Handle module_handle(THREAD, ((module_entry != NULL) ? module_entry->module() : (oop)NULL));\n+  Handle module_handle(THREAD, ((module_entry != nullptr) ? module_entry->module() : (oop)nullptr));\n@@ -628,1 +628,1 @@\n-  if (java_mirror() == NULL) {\n+  if (java_mirror() == nullptr) {\n@@ -697,1 +697,1 @@\n-  if (name() == NULL)  return \"<unknown>\";\n+  if (name() == nullptr)  return \"<unknown>\";\n@@ -702,1 +702,1 @@\n-  if (name() == NULL)  return \"<unknown>\";\n+  if (name() == nullptr)  return \"<unknown>\";\n@@ -783,1 +783,1 @@\n-  if (super() != NULL) {\n+  if (super() != nullptr) {\n@@ -786,1 +786,1 @@\n-  if (secondary_super_cache() != NULL) {\n+  if (secondary_super_cache() != nullptr) {\n@@ -792,1 +792,1 @@\n-    if (ko != NULL) {\n+    if (ko != nullptr) {\n@@ -797,1 +797,1 @@\n-  if (java_mirror_no_keepalive() != NULL) {\n+  if (java_mirror_no_keepalive() != nullptr) {\n@@ -860,1 +860,1 @@\n-  if (joint_description == NULL) {\n+  if (joint_description == nullptr) {\n@@ -919,1 +919,1 @@\n-  assert(cld != NULL, \"class_loader_data should not be null\");\n+  assert(cld != nullptr, \"class_loader_data should not be null\");\n@@ -933,1 +933,1 @@\n-    if (parent_cld == NULL) {\n+    if (parent_cld == nullptr) {\n@@ -935,1 +935,1 @@\n-      if (cl_name_and_id != NULL) {\n+      if (cl_name_and_id != nullptr) {\n@@ -952,1 +952,1 @@\n-  if (class_description == NULL) {\n+  if (class_description == nullptr) {\n","filename":"src\/hotspot\/share\/oops\/klass.cpp","additions":54,"deletions":54,"binary":false,"changes":108,"status":"modified"},{"patch":"@@ -148,1 +148,1 @@\n-  \/\/ First subclass (NULL if none); _subklass->next_sibling() is next one\n+  \/\/ First subclass (null if none); _subklass->next_sibling() is next one\n@@ -150,1 +150,1 @@\n-  \/\/ Sibling link (or NULL); links all subklasses of a klass\n+  \/\/ Sibling link (or null); links all subklasses of a klass\n@@ -225,1 +225,1 @@\n-  virtual InstanceKlass* java_super() const  { return NULL; }\n+  virtual InstanceKlass* java_super() const  { return nullptr; }\n@@ -237,1 +237,1 @@\n-  \/\/ If there is no such element, return either NULL or this.\n+  \/\/ If there is no such element, return either null or this.\n@@ -241,1 +241,1 @@\n-    assert(super == NULL || super->super_depth() == i, \"correct display\");\n+    assert(super == nullptr || super->super_depth() == i, \"correct display\");\n@@ -270,1 +270,1 @@\n-  oop archived_java_mirror() NOT_CDS_JAVA_HEAP_RETURN_(NULL);\n+  oop archived_java_mirror() NOT_CDS_JAVA_HEAP_RETURN_(nullptr);\n@@ -277,1 +277,1 @@\n-  \/\/ Set java mirror OopHandle to NULL for CDS\n+  \/\/ Set java mirror OopHandle to null for CDS\n@@ -533,1 +533,1 @@\n-  \/\/ These will return NULL instead of allocating on the heap:\n+  \/\/ These will return null instead of allocating on the heap:\n@@ -572,1 +572,1 @@\n-    } else if (_java_mirror.ptr_raw() == NULL) {\n+    } else if (_java_mirror.ptr_raw() == nullptr) {\n","filename":"src\/hotspot\/share\/oops\/klass.hpp","additions":9,"deletions":9,"binary":false,"changes":18,"status":"modified"},{"patch":"@@ -2,1 +2,1 @@\n- * Copyright (c) 1997, 2021, Oracle and\/or its affiliates. All rights reserved.\n+ * Copyright (c) 1997, 2023, Oracle and\/or its affiliates. All rights reserved.\n@@ -73,2 +73,2 @@\n-      if (mon == NULL) {\n-        st->print(\"NULL (this should never be seen!)\");\n+      if (mon == nullptr) {\n+        st->print(\"null (this should never be seen!)\");\n","filename":"src\/hotspot\/share\/oops\/markWord.cpp","additions":3,"deletions":3,"binary":false,"changes":6,"status":"modified"},{"patch":"@@ -2,1 +2,1 @@\n- * Copyright (c) 1997, 2021, Oracle and\/or its affiliates. All rights reserved.\n+ * Copyright (c) 1997, 2023, Oracle and\/or its affiliates. All rights reserved.\n@@ -61,1 +61,1 @@\n-  Klass* super_klass = NULL;\n+  Klass* super_klass = nullptr;\n@@ -64,1 +64,1 @@\n-    if (element_super != NULL) {\n+    if (element_super != nullptr) {\n@@ -67,1 +67,1 @@\n-      bool supers_exist = super_klass != NULL;\n+      bool supers_exist = super_klass != nullptr;\n@@ -73,1 +73,1 @@\n-        if (elem_super->array_klass_or_null() == NULL) {\n+        if (elem_super->array_klass_or_null() == nullptr) {\n@@ -80,1 +80,1 @@\n-        Klass* ek = NULL;\n+        Klass* ek = nullptr;\n@@ -100,1 +100,1 @@\n-  Symbol* name = NULL;\n+  Symbol* name = nullptr;\n@@ -124,1 +124,1 @@\n-  assert(module != NULL, \"No module entry for array\");\n+  assert(module != nullptr, \"No module entry for array\");\n@@ -130,1 +130,1 @@\n-  \/\/ including classes in the bootstrap (NULL) class loader.\n+  \/\/ including classes in the bootstrap (null) class loader.\n@@ -149,1 +149,1 @@\n-  assert(bk != NULL && (bk->is_instance_klass() || bk->is_typeArray_klass()), \"invalid bottom klass\");\n+  assert(bk != nullptr && (bk->is_instance_klass() || bk->is_typeArray_klass()), \"invalid bottom klass\");\n@@ -293,1 +293,1 @@\n-    assert(arrayOopDesc::obj_offset_to_raw<narrowOop>(s, src_offset, NULL) ==\n+    assert(arrayOopDesc::obj_offset_to_raw<narrowOop>(s, src_offset, nullptr) ==\n@@ -295,1 +295,1 @@\n-    assert(arrayOopDesc::obj_offset_to_raw<narrowOop>(d, dst_offset, NULL) ==\n+    assert(arrayOopDesc::obj_offset_to_raw<narrowOop>(d, dst_offset, nullptr) ==\n@@ -301,1 +301,1 @@\n-    assert(arrayOopDesc::obj_offset_to_raw<oop>(s, src_offset, NULL) ==\n+    assert(arrayOopDesc::obj_offset_to_raw<oop>(s, src_offset, nullptr) ==\n@@ -303,1 +303,1 @@\n-    assert(arrayOopDesc::obj_offset_to_raw<oop>(d, dst_offset, NULL) ==\n+    assert(arrayOopDesc::obj_offset_to_raw<oop>(d, dst_offset, nullptr) ==\n@@ -317,1 +317,1 @@\n-  if (higher_dimension_acquire() == NULL) {\n+  if (higher_dimension_acquire() == nullptr) {\n@@ -325,1 +325,1 @@\n-      if (higher_dimension() == NULL) {\n+      if (higher_dimension() == nullptr) {\n@@ -351,2 +351,2 @@\n-  if (higher_dimension_acquire() == NULL) {\n-    return NULL;\n+  if (higher_dimension_acquire() == nullptr) {\n+    return nullptr;\n@@ -377,1 +377,1 @@\n-  assert(transitive_interfaces == NULL, \"sanity\");\n+  assert(transitive_interfaces == nullptr, \"sanity\");\n@@ -380,1 +380,1 @@\n-  int num_elem_supers = elem_supers == NULL ? 0 : elem_supers->length();\n+  int num_elem_supers = elem_supers == nullptr ? 0 : elem_supers->length();\n@@ -385,1 +385,1 @@\n-    return NULL;\n+    return nullptr;\n@@ -393,1 +393,1 @@\n-      assert(array_super != NULL, \"must already have been created\");\n+      assert(array_super != nullptr, \"must already have been created\");\n@@ -412,1 +412,1 @@\n-  if (element_klass() == NULL) {\n+  if (element_klass() == nullptr) {\n@@ -424,1 +424,1 @@\n-  assert(bottom_klass() != NULL, \"ObjArrayKlass returned unexpected NULL bottom_klass\");\n+  assert(bottom_klass() != nullptr, \"ObjArrayKlass returned unexpected null bottom_klass\");\n@@ -430,1 +430,1 @@\n-  assert(bottom_klass() != NULL, \"ObjArrayKlass returned unexpected NULL bottom_klass\");\n+  assert(bottom_klass() != nullptr, \"ObjArrayKlass returned unexpected null bottom_klass\");\n@@ -461,1 +461,1 @@\n-    if (oa->obj_at(index) != NULL) {\n+    if (oa->obj_at(index) != nullptr) {\n@@ -465,1 +465,1 @@\n-      st->print_cr(\"NULL\");\n+      st->print_cr(\"null\");\n@@ -482,1 +482,1 @@\n-  if (obj != NULL) {\n+  if (obj != nullptr) {\n@@ -485,1 +485,1 @@\n-    st->print_cr(\"NULL\");\n+    st->print_cr(\"null\");\n","filename":"src\/hotspot\/share\/oops\/objArrayKlass.cpp","additions":29,"deletions":29,"binary":false,"changes":58,"status":"modified"},{"patch":"@@ -38,0 +38,1 @@\n+  friend class ArchiveHeapWriter;\n","filename":"src\/hotspot\/share\/oops\/objArrayOop.hpp","additions":1,"deletions":0,"binary":false,"changes":1,"status":"modified"},{"patch":"@@ -2,1 +2,1 @@\n- * Copyright (c) 1997, 2022, Oracle and\/or its affiliates. All rights reserved.\n+ * Copyright (c) 1997, 2023, Oracle and\/or its affiliates. All rights reserved.\n@@ -58,0 +58,10 @@\n+void oopDesc::print_name_on(outputStream* st) const {\n+  if (*((juint*)this) == badHeapWordVal) {\n+    st->print_cr(\"BAD WORD\");\n+  } else if (*((juint*)this) == badMetaWordVal) {\n+    st->print_cr(\"BAD META WORD\");\n+  } else {\n+    st->print_cr(\"%s\", klass()->external_name());\n+  }\n+}\n+\n@@ -91,1 +101,1 @@\n-  if (oop_desc != NULL) {\n+  if (oop_desc != nullptr) {\n@@ -128,1 +138,1 @@\n-  return obj == NULL ? true : is_oop(obj, ignore_mark_word);\n+  return obj == nullptr ? true : is_oop(obj, ignore_mark_word);\n@@ -164,1 +174,1 @@\n-    if (CompressedOops::is_null(narrow_oop)) return NULL;\n+    if (CompressedOops::is_null(narrow_oop)) return nullptr;\n","filename":"src\/hotspot\/share\/oops\/oop.cpp","additions":14,"deletions":4,"binary":false,"changes":18,"status":"modified"},{"patch":"@@ -2,1 +2,1 @@\n- * Copyright (c) 1997, 2022, Oracle and\/or its affiliates. All rights reserved.\n+ * Copyright (c) 1997, 2023, Oracle and\/or its affiliates. All rights reserved.\n@@ -91,0 +91,2 @@\n+  \/\/ Get the raw value without any checks.\n+  inline Klass* klass_raw() const;\n@@ -227,2 +229,2 @@\n-  void print_on(outputStream* st) const;        \/\/ First level print\n-  void print_value_on(outputStream* st) const;  \/\/ Second level print.\n+  void print_on(outputStream* st) const;         \/\/ First level print\n+  void print_value_on(outputStream* st) const;   \/\/ Second level print.\n@@ -230,0 +232,1 @@\n+  void print_name_on(outputStream* st) const;    \/\/ External name printing.\n@@ -265,1 +268,1 @@\n-  \/\/ this call returns \"NULL\" for that thread; any other thread has the\n+  \/\/ this call returns null for that thread; any other thread has the\n","filename":"src\/hotspot\/share\/oops\/oop.hpp","additions":7,"deletions":4,"binary":false,"changes":11,"status":"modified"},{"patch":"@@ -142,0 +142,4 @@\n+Klass* oopDesc::klass_raw() const {\n+  return klass();\n+}\n+\n@@ -144,1 +148,1 @@\n-  assert(Universe::is_bootstrapping() || (k != NULL && k->is_klass()), \"incorrect Klass\");\n+  assert(Universe::is_bootstrapping() || (k != nullptr && k->is_klass()), \"incorrect Klass\");\n@@ -149,1 +153,1 @@\n-  assert(Universe::is_bootstrapping() || (k != NULL && k->is_klass()), \"incorrect Klass\");\n+  assert(Universe::is_bootstrapping() || (k != nullptr && k->is_klass()), \"incorrect Klass\");\n@@ -314,1 +318,1 @@\n-    return NULL;\n+    return nullptr;\n@@ -419,1 +423,1 @@\n-  return obj == NULL || obj->klass()->is_subtype_of(klass);\n+  return obj == nullptr || obj->klass()->is_subtype_of(klass);\n","filename":"src\/hotspot\/share\/oops\/oop.inline.hpp","additions":8,"deletions":4,"binary":false,"changes":12,"status":"modified"},{"patch":"@@ -2,1 +2,1 @@\n- * Copyright (c) 1997, 2022, Oracle and\/or its affiliates. All rights reserved.\n+ * Copyright (c) 1997, 2023, Oracle and\/or its affiliates. All rights reserved.\n@@ -68,1 +68,1 @@\n-\/\/ Converting NULL to oop to Handle implicit is no longer accepted by the\n+\/\/ Converting null to oop to Handle implicit is no longer accepted by the\n","filename":"src\/hotspot\/share\/oops\/oopsHierarchy.hpp","additions":2,"deletions":2,"binary":false,"changes":4,"status":"modified"},{"patch":"@@ -2,1 +2,1 @@\n- * Copyright (c) 1997, 2021, Oracle and\/or its affiliates. All rights reserved.\n+ * Copyright (c) 1997, 2023, Oracle and\/or its affiliates. All rights reserved.\n@@ -47,2 +47,2 @@\n-  Symbol* sym = NULL;\n-  if (name_str != NULL) {\n+  Symbol* sym = nullptr;\n+  if (name_str != nullptr) {\n@@ -60,1 +60,1 @@\n-  \/\/ including classes in the bootstrap (NULL) class loader.\n+  \/\/ including classes in the bootstrap (null) class loader.\n@@ -181,1 +181,1 @@\n-  if (higher_dimension_acquire() == NULL) {\n+  if (higher_dimension_acquire() == nullptr) {\n@@ -189,1 +189,1 @@\n-      if (higher_dimension() == NULL) {\n+      if (higher_dimension() == nullptr) {\n@@ -214,2 +214,2 @@\n-  if (higher_dimension_acquire() == NULL) {\n-    return NULL;\n+  if (higher_dimension_acquire() == nullptr) {\n+    return nullptr;\n@@ -252,1 +252,1 @@\n-  return NULL;\n+  return nullptr;\n@@ -374,1 +374,1 @@\n-  return NULL;\n+  return nullptr;\n","filename":"src\/hotspot\/share\/oops\/typeArrayKlass.cpp","additions":10,"deletions":10,"binary":false,"changes":20,"status":"modified"},{"patch":"@@ -759,0 +759,3 @@\n+\n+    gvn.set_type(root(), root()->bottom_type());\n+\n@@ -981,1 +984,0 @@\n-  _type_verify_symmetry = true;\n@@ -984,0 +986,1 @@\n+  _type_verify = NULL;\n","filename":"src\/hotspot\/share\/opto\/compile.cpp","additions":4,"deletions":1,"binary":false,"changes":5,"status":"modified"},{"patch":"@@ -95,0 +95,1 @@\n+class VerifyMeetResult;\n@@ -943,1 +944,2 @@\n-  inline void       record_for_igvn(Node* n);   \/\/ Body is after class Unique_Node_List.\n+  inline void       record_for_igvn(Node* n);   \/\/ Body is after class Unique_Node_List in node.hpp.\n+  inline void       remove_for_igvn(Node* n);   \/\/ Body is after class Unique_Node_List in node.hpp.\n@@ -1219,1 +1221,1 @@\n-  bool _type_verify_symmetry;\n+  VerifyMeetResult* _type_verify;\n","filename":"src\/hotspot\/share\/opto\/compile.hpp","additions":4,"deletions":2,"binary":false,"changes":6,"status":"modified"},{"patch":"@@ -2,1 +2,1 @@\n- * Copyright (c) 1997, 2022, Oracle and\/or its affiliates. All rights reserved.\n+ * Copyright (c) 1997, 2023, Oracle and\/or its affiliates. All rights reserved.\n@@ -548,1 +548,0 @@\n-  gvn().set_type(root(), root()->bottom_type());\n@@ -683,0 +682,1 @@\n+          block->copy_irreducible_status_to(r, jvms());\n@@ -1504,1 +1504,6 @@\n-    if (b->is_loop_head()) tty->print(\"  lphd\");\n+    if (b->is_loop_head()) {\n+      tty->print(\"  lphd\");\n+    }\n+    if (b->is_irreducible_loop_entry()) {\n+      tty->print(\"  irreducible\");\n+    }\n@@ -1693,0 +1698,1 @@\n+      target->copy_irreducible_status_to(r, jvms());\n@@ -1730,1 +1736,1 @@\n-      if (!block()->flow()->is_irreducible_entry()) {\n+      if (!block()->flow()->is_irreducible_loop_secondary_entry()) {\n","filename":"src\/hotspot\/share\/opto\/parse1.cpp","additions":10,"deletions":4,"binary":false,"changes":14,"status":"modified"},{"patch":"@@ -789,2 +789,148 @@\n-void Type::check_symmetrical(const Type* t, const Type* mt) const {\n-  const Type* mt2 = t->xmeet(this);\n+class VerifyMeet;\n+class VerifyMeetResult : public ArenaObj {\n+  friend class VerifyMeet;\n+  friend class Type;\n+private:\n+  class VerifyMeetResultEntry {\n+  private:\n+    const Type* _in1;\n+    const Type* _in2;\n+    const Type* _res;\n+  public:\n+    VerifyMeetResultEntry(const Type* in1, const Type* in2, const Type* res):\n+            _in1(in1), _in2(in2), _res(res) {\n+    }\n+    VerifyMeetResultEntry():\n+            _in1(NULL), _in2(NULL), _res(NULL) {\n+    }\n+\n+    bool operator==(const VerifyMeetResultEntry& rhs) const {\n+      return _in1 == rhs._in1 &&\n+             _in2 == rhs._in2 &&\n+             _res == rhs._res;\n+    }\n+\n+    bool operator!=(const VerifyMeetResultEntry& rhs) const {\n+      return !(rhs == *this);\n+    }\n+\n+    static int compare(const VerifyMeetResultEntry& v1, const VerifyMeetResultEntry& v2) {\n+      if ((intptr_t) v1._in1 < (intptr_t) v2._in1) {\n+        return -1;\n+      } else if (v1._in1 == v2._in1) {\n+        if ((intptr_t) v1._in2 < (intptr_t) v2._in2) {\n+          return -1;\n+        } else if (v1._in2 == v2._in2) {\n+          assert(v1._res == v2._res || v1._res == NULL || v2._res == NULL, \"same inputs should lead to same result\");\n+          return 0;\n+        }\n+        return 1;\n+      }\n+      return 1;\n+    }\n+    const Type* res() const { return _res; }\n+  };\n+  uint _depth;\n+  GrowableArray<VerifyMeetResultEntry> _cache;\n+\n+  \/\/ With verification code, the meet of A and B causes the computation of:\n+  \/\/ 1- meet(A, B)\n+  \/\/ 2- meet(B, A)\n+  \/\/ 3- meet(dual(meet(A, B)), dual(A))\n+  \/\/ 4- meet(dual(meet(A, B)), dual(B))\n+  \/\/ 5- meet(dual(A), dual(B))\n+  \/\/ 6- meet(dual(B), dual(A))\n+  \/\/ 7- meet(dual(meet(dual(A), dual(B))), A)\n+  \/\/ 8- meet(dual(meet(dual(A), dual(B))), B)\n+  \/\/\n+  \/\/ In addition the meet of A[] and B[] requires the computation of the meet of A and B.\n+  \/\/\n+  \/\/ The meet of A[] and B[] triggers the computation of:\n+  \/\/ 1- meet(A[], B[][)\n+  \/\/   1.1- meet(A, B)\n+  \/\/   1.2- meet(B, A)\n+  \/\/   1.3- meet(dual(meet(A, B)), dual(A))\n+  \/\/   1.4- meet(dual(meet(A, B)), dual(B))\n+  \/\/   1.5- meet(dual(A), dual(B))\n+  \/\/   1.6- meet(dual(B), dual(A))\n+  \/\/   1.7- meet(dual(meet(dual(A), dual(B))), A)\n+  \/\/   1.8- meet(dual(meet(dual(A), dual(B))), B)\n+  \/\/ 2- meet(B[], A[])\n+  \/\/   2.1- meet(B, A) = 1.2\n+  \/\/   2.2- meet(A, B) = 1.1\n+  \/\/   2.3- meet(dual(meet(B, A)), dual(B)) = 1.4\n+  \/\/   2.4- meet(dual(meet(B, A)), dual(A)) = 1.3\n+  \/\/   2.5- meet(dual(B), dual(A)) = 1.6\n+  \/\/   2.6- meet(dual(A), dual(B)) = 1.5\n+  \/\/   2.7- meet(dual(meet(dual(B), dual(A))), B) = 1.8\n+  \/\/   2.8- meet(dual(meet(dual(B), dual(A))), B) = 1.7\n+  \/\/ etc.\n+  \/\/ The number of meet operations performed grows exponentially with the number of dimensions of the arrays but the number\n+  \/\/ of different meet operations is linear in the number of dimensions. The function below caches meet results for the\n+  \/\/ duration of the meet at the root of the recursive calls.\n+  \/\/\n+  const Type* meet(const Type* t1, const Type* t2) {\n+    bool found = false;\n+    const VerifyMeetResultEntry meet(t1, t2, NULL);\n+    int pos = _cache.find_sorted<VerifyMeetResultEntry, VerifyMeetResultEntry::compare>(meet, found);\n+    const Type* res = NULL;\n+    if (found) {\n+      res = _cache.at(pos).res();\n+    } else {\n+      res = t1->xmeet(t2);\n+      _cache.insert_sorted<VerifyMeetResultEntry::compare>(VerifyMeetResultEntry(t1, t2, res));\n+      found = false;\n+      _cache.find_sorted<VerifyMeetResultEntry, VerifyMeetResultEntry::compare>(meet, found);\n+      assert(found, \"should be in table after it's added\");\n+    }\n+    return res;\n+  }\n+\n+  void add(const Type* t1, const Type* t2, const Type* res) {\n+    _cache.insert_sorted<VerifyMeetResultEntry::compare>(VerifyMeetResultEntry(t1, t2, res));\n+  }\n+\n+  bool empty_cache() const {\n+    return _cache.length() == 0;\n+  }\n+public:\n+  VerifyMeetResult(Compile* C) :\n+          _depth(0), _cache(C->comp_arena(), 2, 0, VerifyMeetResultEntry()) {\n+  }\n+};\n+\n+void Type::assert_type_verify_empty() const {\n+  assert(Compile::current()->_type_verify == NULL || Compile::current()->_type_verify->empty_cache(), \"cache should have been discarded\");\n+}\n+\n+class VerifyMeet {\n+private:\n+  Compile* _C;\n+public:\n+  VerifyMeet(Compile* C) : _C(C) {\n+    if (C->_type_verify == NULL) {\n+      C->_type_verify = new (C->comp_arena())VerifyMeetResult(C);\n+    }\n+    _C->_type_verify->_depth++;\n+  }\n+\n+  ~VerifyMeet() {\n+    assert(_C->_type_verify->_depth != 0, \"\");\n+    _C->_type_verify->_depth--;\n+    if (_C->_type_verify->_depth == 0) {\n+      _C->_type_verify->_cache.trunc_to(0);\n+    }\n+  }\n+\n+  const Type* meet(const Type* t1, const Type* t2) const {\n+    return _C->_type_verify->meet(t1, t2);\n+  }\n+\n+  void add(const Type* t1, const Type* t2, const Type* res) const {\n+    _C->_type_verify->add(t1, t2, res);\n+  }\n+};\n+\n+void Type::check_symmetrical(const Type* t, const Type* mt, const VerifyMeet& verify) const {\n+  Compile* C = Compile::current();\n+  const Type* mt2 = verify.meet(t, this);\n@@ -801,2 +947,2 @@\n-  const Type* t2t    = dual_join->xmeet(t->_dual);\n-  const Type* t2this = dual_join->xmeet(this->_dual);\n+  const Type* t2t    = verify.meet(dual_join,t->_dual);\n+  const Type* t2this = verify.meet(dual_join,this->_dual);\n@@ -823,1 +969,1 @@\n-#endif\n+#endif\n@@ -839,0 +985,5 @@\n+#ifdef ASSERT\n+  Compile* C = Compile::current();\n+  VerifyMeet verify(C);\n+#endif\n+\n@@ -844,4 +995,2 @@\n-  if (isa_narrowoop() || t->isa_narrowoop()) return mt;\n-  if (isa_narrowklass() || t->isa_narrowklass()) return mt;\n-  Compile* C = Compile::current();\n-  if (!C->_type_verify_symmetry) {\n+  verify.add(this_t, t, mt);\n+  if (isa_narrowoop() || t->isa_narrowoop()) {\n@@ -850,15 +999,6 @@\n-  this_t->check_symmetrical(t, mt);\n-  \/\/ In the case of an array, computing the meet above, caused the\n-  \/\/ computation of the meet of the elements which at verification\n-  \/\/ time caused the computation of the meet of the dual of the\n-  \/\/ elements. Computing the meet of the dual of the arrays here\n-  \/\/ causes the meet of the dual of the elements to be computed which\n-  \/\/ would cause the meet of the dual of the dual of the elements,\n-  \/\/ that is the meet of the elements already computed above to be\n-  \/\/ computed. Avoid redundant computations by requesting no\n-  \/\/ verification.\n-  C->_type_verify_symmetry = false;\n-  const Type *mt_dual = this_t->_dual->xmeet(t->_dual);\n-  this_t->_dual->check_symmetrical(t->_dual, mt_dual);\n-  assert(!C->_type_verify_symmetry, \"shouldn't have changed\");\n-  C->_type_verify_symmetry = true;\n+  if (isa_narrowklass() || t->isa_narrowklass()) {\n+    return mt;\n+  }\n+  this_t->check_symmetrical(t, mt, verify);\n+  const Type *mt_dual = verify.meet(this_t->_dual, t->_dual);\n+  this_t->_dual->check_symmetrical(t->_dual, mt_dual, verify);\n@@ -2821,1 +2961,1 @@\n-      return speculative->klass();\n+      return speculative->exact_klass();\n@@ -4913,0 +5053,5 @@\n+      if (above_centerline(ptr) || ptr == Constant) {\n+        ptr = NotNull;\n+        res_xk = false;\n+        return NOT_SUBTYPE;\n+      }\n@@ -4948,1 +5093,1 @@\n-          res_xk = true;\n+        res_xk = true;\n","filename":"src\/hotspot\/share\/opto\/type.cpp","additions":171,"deletions":26,"binary":false,"changes":197,"status":"modified"},{"patch":"@@ -558,0 +558,3 @@\n+  { \"G1ConcRSLogCacheSize\",    JDK_Version::undefined(), JDK_Version::jdk(21), JDK_Version::undefined() },\n+  { \"G1ConcRSHotCardLimit\",   JDK_Version::undefined(), JDK_Version::jdk(21), JDK_Version::undefined() },\n+\n@@ -1482,1 +1485,1 @@\n-  size_t displacement_due_to_null_page = align_up((size_t)os::vm_page_size(),\n+  size_t displacement_due_to_null_page = align_up(os::vm_page_size(),\n@@ -1536,1 +1539,1 @@\n-                                          (size_t)os::vm_allocation_granularity(),\n+                                          os::vm_allocation_granularity(),\n@@ -3459,1 +3462,2 @@\n-    if (os::same_files((const char*)get_default_shared_archive_path(), ArchiveClassesAtExit)) {\n+    char* shared_archive_path = get_default_shared_archive_path();\n+    if (os::same_files(shared_archive_path, ArchiveClassesAtExit)) {\n@@ -3461,1 +3465,1 @@\n-        \"Cannot specify the default CDS archive for -XX:ArchiveClassesAtExit\", get_default_shared_archive_path());\n+        \"Cannot specify the default CDS archive for -XX:ArchiveClassesAtExit\", shared_archive_path);\n@@ -3463,0 +3467,1 @@\n+    FREE_C_HEAP_ARRAY(char, shared_archive_path);\n@@ -4249,75 +4254,0 @@\n-\n-bool Arguments::parse_malloc_limit_size(const char* s, size_t* out) {\n-  julong limit = 0;\n-  Arguments::ArgsRange range = parse_memory_size(s, &limit, 1, SIZE_MAX);\n-  switch (range) {\n-  case ArgsRange::arg_in_range:\n-    *out = (size_t)limit;\n-    return true;\n-  case ArgsRange::arg_too_big: \/\/ only possible on 32-bit\n-    vm_exit_during_initialization(\"MallocLimit: too large\", s);\n-    break;\n-  case ArgsRange::arg_too_small:\n-    vm_exit_during_initialization(\"MallocLimit: limit must be > 0\");\n-    break;\n-  default:\n-    break;\n-  }\n-  return false;\n-}\n-\n-\/\/ Helper for parse_malloc_limits\n-void Arguments::parse_single_category_limit(char* expression, size_t limits[mt_number_of_types]) {\n-  \/\/ <category>:<limit>\n-  char* colon = ::strchr(expression, ':');\n-  if (colon == nullptr) {\n-    vm_exit_during_initialization(\"MallocLimit: colon missing\", expression);\n-  }\n-  *colon = '\\0';\n-  MEMFLAGS f = NMTUtil::string_to_flag(expression);\n-  if (f == mtNone) {\n-    vm_exit_during_initialization(\"MallocLimit: invalid nmt category\", expression);\n-  }\n-  if (parse_malloc_limit_size(colon + 1, limits + (int)f) == false) {\n-    vm_exit_during_initialization(\"Invalid MallocLimit size\", colon + 1);\n-  }\n-}\n-\n-void Arguments::parse_malloc_limits(size_t* total_limit, size_t limits[mt_number_of_types]) {\n-\n-  \/\/ Reset output to 0\n-  *total_limit = 0;\n-  for (int i = 0; i < mt_number_of_types; i ++) {\n-    limits[i] = 0;\n-  }\n-\n-  \/\/ We are done if the option is not given.\n-  if (MallocLimit == nullptr) {\n-    return;\n-  }\n-\n-  \/\/ Global form?\n-  if (parse_malloc_limit_size(MallocLimit, total_limit)) {\n-    return;\n-  }\n-\n-  \/\/ No. So it must be in category-specific form: MallocLimit=<nmt category>:<size>[,<nmt category>:<size> ..]\n-  char* copy = os::strdup(MallocLimit);\n-  if (copy == nullptr) {\n-    vm_exit_out_of_memory(strlen(MallocLimit), OOM_MALLOC_ERROR, \"MallocLimit\");\n-  }\n-\n-  char* p = copy, *q;\n-  do {\n-    q = p;\n-    p = ::strchr(q, ',');\n-    if (p != nullptr) {\n-      *p = '\\0';\n-      p ++;\n-    }\n-    parse_single_category_limit(q, limits);\n-  } while (p != nullptr);\n-\n-  os::free(copy);\n-\n-}\n","filename":"src\/hotspot\/share\/runtime\/arguments.cpp","additions":9,"deletions":79,"binary":false,"changes":88,"status":"modified"},{"patch":"@@ -1112,1 +1112,1 @@\n-  product_pd(uintx, TypeProfileLevel,                                       \\\n+  product_pd(uint, TypeProfileLevel,                                        \\\n@@ -1189,3 +1189,0 @@\n-  develop(bool, VerifyThread, false,                                        \\\n-          \"Watch the thread register for corruption (SPARC only)\")          \\\n-                                                                            \\\n@@ -1314,3 +1311,0 @@\n-  notproduct(ccstrlist, SuppressErrorAt, \"\",                                \\\n-          \"List of assertions (file:line) to muzzle\")                       \\\n-                                                                            \\\n@@ -1347,7 +1341,2 @@\n-  product(uintx, MallocMaxTestWords,     0, DIAGNOSTIC,                     \\\n-          \"If non-zero, maximum number of words that malloc\/realloc can \"   \\\n-          \"allocate (for testing only)\")                                    \\\n-          range(0, max_uintx)                                               \\\n-                                                                            \\\n-          \"Limit malloc allocation size from VM. Reaching the limit will \"  \\\n-          \"trigger a fatal error. This feature requires \"                   \\\n+          \"Limit malloc allocation size from VM. Reaching a limit will \"    \\\n+          \"trigger an action (see flag). This feature requires \"            \\\n@@ -1357,4 +1346,11 @@\n-          \"- MallocLimit=<size> to set a total limit. \"                     \\\n-          \"- MallocLimit=<NMT category>:<size>[,<NMT category>:<size>...] \" \\\n-          \"  to set one or more category-specific limits.\"                  \\\n-          \"Example: -XX:MallocLimit=compiler:500m\")                         \\\n+          \"\\\"-XX:MallocLimit=<size>[:<flag>]\\\" sets a total limit.\"         \\\n+          \"\\\"-XX:MallocLimit=<category>:<size>[:<flag>][,<category>:<size>[:<flag>] ...]\\\"\" \\\n+          \"sets one or more category-specific limits.\"                      \\\n+          \"<flag> defines the action upon reaching the limit:\"              \\\n+          \"\\\"fatal\\\": end VM with a fatal error at the allocation site\"     \\\n+          \"\\\"oom\\\"  : will mimic a native OOM\"                              \\\n+          \"If <flag> is omitted, \\\"fatal\\\" is the default.\"                 \\\n+          \"Examples:\\n\"                                                     \\\n+          \"-XX:MallocLimit=2g\"                                              \\\n+          \"-XX:MallocLimit=2g:oom\"                                          \\\n+          \"-XX:MallocLimit=compiler:200m:oom,code:100m\")                    \\\n@@ -1426,2 +1422,2 @@\n-  product(ccstr, MetaspaceReclaimPolicy, \"balanced\",                        \\\n-          \"options: balanced, aggressive, none\")                            \\\n+  product(ccstr, MetaspaceReclaimPolicy, \"balanced\", DIAGNOSTIC,            \\\n+          \"options: balanced, aggressive\")                                  \\\n","filename":"src\/hotspot\/share\/runtime\/globals.hpp","additions":16,"deletions":20,"binary":false,"changes":36,"status":"modified"},{"patch":"@@ -161,1 +161,1 @@\n-  _jvmti_vthread = OopHandle(_thread_oop_storage, nullptr);\n+  _jvmti_vthread = OopHandle(_thread_oop_storage, p->is_a(vmClasses::BoundVirtualThread_klass()) ? p : nullptr);\n","filename":"src\/hotspot\/share\/runtime\/javaThread.cpp","additions":1,"deletions":1,"binary":false,"changes":2,"status":"modified"},{"patch":"@@ -64,0 +64,1 @@\n+#include \"sanitizers\/address.hpp\"\n@@ -67,1 +68,1 @@\n-#include \"services\/memTracker.hpp\"\n+#include \"services\/memTracker.inline.hpp\"\n@@ -90,2 +91,0 @@\n-static size_t cur_malloc_words = 0;  \/\/ current size for MallocMaxTestWords\n-\n@@ -610,17 +609,0 @@\n-\/\/\n-\/\/ This function supports testing of the malloc out of memory\n-\/\/ condition without really running the system out of memory.\n-\/\/\n-\n-static bool has_reached_max_malloc_test_peak(size_t alloc_size) {\n-  if (MallocMaxTestWords > 0) {\n-    size_t words = (alloc_size \/ BytesPerWord);\n-\n-    if ((cur_malloc_words + words) > MallocMaxTestWords) {\n-      return true;\n-    }\n-    Atomic::add(&cur_malloc_words, words);\n-  }\n-  return false;\n-}\n-\n@@ -661,2 +643,2 @@\n-  \/\/ For the test flag -XX:MallocMaxTestWords\n-  if (has_reached_max_malloc_test_peak(size)) {\n+  \/\/ Observe MallocLimit\n+  if (MemTracker::check_exceeds_limit(size, memflags)) {\n@@ -698,1 +680,1 @@\n-  if (NMTPreInit::handle_realloc(&rc, memblock, size)) {\n+  if (NMTPreInit::handle_realloc(&rc, memblock, size, memflags)) {\n@@ -713,5 +695,0 @@\n-  \/\/ For the test flag -XX:MallocMaxTestWords\n-  if (has_reached_max_malloc_test_peak(size)) {\n-    return nullptr;\n-  }\n-\n@@ -728,0 +705,7 @@\n+    const size_t old_size = MallocTracker::malloc_header(memblock)->size();\n+\n+    \/\/ Observe MallocLimit\n+    if ((size > old_size) && MemTracker::check_exceeds_limit(size - old_size, memflags)) {\n+      return nullptr;\n+    }\n+\n@@ -730,2 +714,3 @@\n-    MallocHeader* header = MallocTracker::malloc_header(memblock);\n-    header->assert_block_integrity(); \/\/ Assert block hasn't been tampered with.\n+    MallocHeader* header = MallocHeader::resolve_checked(memblock);\n+    assert(memflags == header->flags(), \"weird NMT flags mismatch (new:\\\"%s\\\" != old:\\\"%s\\\")\\n\",\n+           NMTUtil::flag_to_name(memflags), NMTUtil::flag_to_name(header->flags()));\n@@ -733,0 +718,1 @@\n+\n@@ -752,1 +738,1 @@\n-    size_t old_size = free_info.size;\n+    assert(old_size == free_info.size, \"Sanity\");\n@@ -945,0 +931,10 @@\n+ATTRIBUTE_NO_ASAN static void print_hex_readable_pointer(outputStream* st, address p,\n+                                                         int unitsize) {\n+  switch (unitsize) {\n+    case 1: st->print(\"%02x\", *(u1*)p); break;\n+    case 2: st->print(\"%04x\", *(u2*)p); break;\n+    case 4: st->print(\"%08x\", *(u4*)p); break;\n+    case 8: st->print(\"%016\" FORMAT64_MODIFIER \"x\", *(u8*)p); break;\n+  }\n+}\n+\n@@ -963,6 +959,1 @@\n-      switch (unitsize) {\n-        case 1: st->print(\"%02x\", *(u1*)p); break;\n-        case 2: st->print(\"%04x\", *(u2*)p); break;\n-        case 4: st->print(\"%08x\", *(u4*)p); break;\n-        case 8: st->print(\"%016\" FORMAT64_MODIFIER \"x\", *(u8*)p); break;\n-      }\n+      print_hex_readable_pointer(st, p, unitsize);\n","filename":"src\/hotspot\/share\/runtime\/os.cpp","additions":28,"deletions":37,"binary":false,"changes":65,"status":"modified"},{"patch":"@@ -237,2 +237,2 @@\n-  nonstatic_field(InstanceKlass,               _init_state,                                   InstanceKlass::ClassState)             \\\n-  nonstatic_field(InstanceKlass,               _init_thread,                                  Thread*)                               \\\n+  volatile_nonstatic_field(InstanceKlass,      _init_state,                                   InstanceKlass::ClassState)             \\\n+  volatile_nonstatic_field(InstanceKlass,      _init_thread,                                  JavaThread*)                           \\\n","filename":"src\/hotspot\/share\/runtime\/vmStructs.cpp","additions":2,"deletions":2,"binary":false,"changes":4,"status":"modified"},{"patch":"@@ -2,1 +2,1 @@\n- * Copyright (c) 1997, 2022, Oracle and\/or its affiliates. All rights reserved.\n+ * Copyright (c) 1997, 2023, Oracle and\/or its affiliates. All rights reserved.\n","filename":"src\/hotspot\/share\/utilities\/globalDefinitions.hpp","additions":1,"deletions":1,"binary":false,"changes":2,"status":"modified"},{"patch":"@@ -1446,7 +1446,0 @@\n-#if defined(_WINDOWS)\n-    \/\/ If UseOSErrorReporting we call this for each level of the call stack\n-    \/\/ while searching for the exception handler.  Only the first level needs\n-    \/\/ to be reported.\n-    if (UseOSErrorReporting && log_done) return;\n-#endif\n-\n@@ -1682,9 +1675,12 @@\n-  if (WINDOWS_ONLY(!UseOSErrorReporting) NOT_WINDOWS(true)) {\n-    \/\/ os::abort() will call abort hooks, try it first.\n-    static bool skip_os_abort = false;\n-    if (!skip_os_abort) {\n-      skip_os_abort = true;\n-      bool dump_core = should_report_bug(_id);\n-      os::abort(dump_core && CreateCoredumpOnCrash, _siginfo, _context);\n-    }\n-\n+#if defined _WINDOWS\n+  if (UseOSErrorReporting) {\n+    raise_fail_fast(_siginfo, _context);\n+  }\n+#endif \/\/ _WINDOWS\n+\n+  \/\/ os::abort() will call abort hooks, try it first.\n+  static bool skip_os_abort = false;\n+  if (!skip_os_abort) {\n+    skip_os_abort = true;\n+    bool dump_core = should_report_bug(_id);\n+    os::abort(dump_core && CreateCoredumpOnCrash, _siginfo, _context);\n@@ -1692,1 +1688,1 @@\n-    os::die();\n+  os::die();\n","filename":"src\/hotspot\/share\/utilities\/vmError.cpp","additions":13,"deletions":17,"binary":false,"changes":30,"status":"modified"},{"patch":"@@ -2,2 +2,2 @@\n- * Copyright (c) 2020, Oracle and\/or its affiliates. All rights reserved.\n- * Copyright (c) 2020 SAP SE. All rights reserved.\n+ * Copyright (c) 2020, 2023, Oracle and\/or its affiliates. All rights reserved.\n+ * Copyright (c) 2020, 2023 SAP SE. All rights reserved.\n@@ -119,5 +119,1 @@\n-      if (Settings::uncommit_free_chunks()) {\n-        ASSERT_LE(committed_words_after, committed_words_before);\n-      } else {\n-        ASSERT_EQ(committed_words_after, committed_words_before);\n-      }\n+      ASSERT_LE(committed_words_after, committed_words_before);\n@@ -178,6 +174,1 @@\n-      if (Settings::new_chunks_are_fully_committed()) {\n-        ASSERT_LT(possible_expansion, MAX_CHUNK_WORD_SIZE);\n-      } else {\n-        ASSERT_LT(possible_expansion, word_size);\n-      }\n-\n+      ASSERT_LT(possible_expansion, word_size);\n@@ -458,4 +449,0 @@\n-  if (Settings::new_chunks_are_fully_committed()) {\n-    return; \/\/ This would throw off the commit counting in this test.\n-  }\n-\n@@ -566,6 +553,17 @@\n-  if (!(Settings::new_chunks_are_fully_committed() && type == Metaspace::BootMetaspaceType)) {\n-    \/\/ Initial commit charge for the whole context should be one granule\n-    ASSERT_EQ(context.committed_words(), Settings::commit_granule_words());\n-    \/\/ Initial commit number for the arena should be less since - apart from boot loader - no\n-    \/\/  space type has large initial chunks.\n-    ASSERT_LE(committed, Settings::commit_granule_words());\n+  \/\/ What happens when we allocate, commit wise:\n+  \/\/ Arena allocates from current chunk, committing needed memory from the chunk on demand.\n+  \/\/ The chunk asks the underlying vsnode to commit the area it is located in. Since the\n+  \/\/ chunk may be smaller than one commit granule, this may result in surrounding memory\n+  \/\/ also getting committed.\n+  \/\/ In reality we will commit in granule granularity, but arena can only know what its first\n+  \/\/ chunk did commit. So what it thinks was committed depends on the size of its first chunk,\n+  \/\/ which depends on ArenaGrowthPolicy.\n+  {\n+    const chunklevel_t expected_level_for_first_chunk =\n+        ArenaGrowthPolicy::policy_for_space_type(type, is_class)->get_level_at_step(0);\n+    const size_t what_arena_should_think_was_committed =\n+        MIN2(Settings::commit_granule_words(), word_size_for_level(expected_level_for_first_chunk));\n+    const size_t what_should_really_be_committed = Settings::commit_granule_words();\n+\n+    ASSERT_EQ(committed, what_arena_should_think_was_committed);\n+    ASSERT_EQ(context.committed_words(), what_should_really_be_committed);\n@@ -616,1 +614,1 @@\n-    if (committed_jump > 0 && !Settings::new_chunks_are_fully_committed()) {\n+    if (committed_jump > 0) {\n","filename":"test\/hotspot\/gtest\/metaspace\/test_metaspacearena.cpp","additions":22,"deletions":24,"binary":false,"changes":46,"status":"modified"},{"patch":"@@ -2,1 +2,1 @@\n-# Copyright (c) 2016, 2022, Oracle and\/or its affiliates. All rights reserved.\n+# Copyright (c) 2016, 2023, Oracle and\/or its affiliates. All rights reserved.\n@@ -87,1 +87,1 @@\n-runtime\/cds\/appcds\/jigsaw\/modulepath\/ModulePathAndCP_JFR.java 8253437 windows-x64\n+\n@@ -99,1 +99,0 @@\n-runtime\/StackGuardPages\/TestStackGuardPages.java 8293452 linux-all\n@@ -168,3 +167,0 @@\n-vmTestbase\/nsk\/stress\/strace\/strace002.java 8288912 macosx-x64,windows-x64\n-vmTestbase\/nsk\/stress\/strace\/strace003.java 8297824 macosx-x64,windows-x64\n-vmTestbase\/nsk\/stress\/strace\/strace004.java 8297824 macosx-x64,windows-x64\n@@ -173,0 +169,1 @@\n+vmTestbase\/nsk\/jdi\/VMOutOfMemoryException\/VMOutOfMemoryException001\/VMOutOfMemoryException001.java 8303057 generic-all\n","filename":"test\/hotspot\/jtreg\/ProblemList.txt","additions":3,"deletions":6,"binary":false,"changes":9,"status":"modified"},{"patch":"@@ -30,0 +30,4 @@\n+ * @comment Testing compressed class pointers without compressed oops is not possible\n+ *          on MacOS because the heap is given an arbitrary address that occasionally\n+ *          collides with where we would ideally have placed the compressed class space.\n+ * @requires os.family != \"mac\"\n@@ -350,14 +354,6 @@\n-        if (!Platform.isOSX()) {\n-            \/\/ Testing compressed class pointers without compressed oops.\n-            \/\/ This is only possible if the platform supports it. Notably,\n-            \/\/ on macOS, when compressed oops is disabled and the heap is\n-            \/\/ given an arbitrary address, that address occasionally collides\n-            \/\/ with where we would ideally have placed the compressed class\n-            \/\/ space. Therefore, macOS is omitted for now.\n-            \/\/ smallHeapTestNoCoop();\n-            \/\/ smallHeapTestWith1GNoCoop();\n-            \/\/ largeHeapTestNoCoop();\n-            largePagesTestNoCoop();\n-            heapBaseMinAddressTestNoCoop();\n-            sharingTestNoCoop();\n-        }\n+        \/\/ smallHeapTestNoCoop();\n+        \/\/ smallHeapTestWith1GNoCoop();\n+        \/\/ largeHeapTestNoCoop();\n+        largePagesTestNoCoop();\n+        heapBaseMinAddressTestNoCoop();\n+        sharingTestNoCoop();\n","filename":"test\/hotspot\/jtreg\/runtime\/CompressedOops\/CompressedClassPointers.java","additions":10,"deletions":14,"binary":false,"changes":24,"status":"modified"},{"patch":"@@ -39,10 +39,0 @@\n-\/*\n- * @test id=test-64bit-ccs-noreclaim\n- * @summary Test the VM.metaspace command\n- * @requires vm.bits == \"64\"\n- * @library \/test\/lib\n- * @modules java.base\/jdk.internal.misc\n- *          java.management\n- * @run main\/othervm -Dwith-compressed-class-space -XX:MaxMetaspaceSize=201M -Xmx100M -XX:+UseCompressedOops -XX:+UseCompressedClassPointers -XX:MetaspaceReclaimPolicy=none PrintMetaspaceDcmd\n- *\/\n-\n@@ -56,1 +46,1 @@\n- * @run main\/othervm -Dwith-compressed-class-space -XX:MaxMetaspaceSize=201M -Xmx100M -XX:+UseCompressedOops -XX:+UseCompressedClassPointers -XX:MetaspaceReclaimPolicy=aggressive PrintMetaspaceDcmd\n+ * @run main\/othervm -Dwith-compressed-class-space -XX:MaxMetaspaceSize=201M -Xmx100M -XX:+UseCompressedOops -XX:+UseCompressedClassPointers -XX:+UnlockDiagnosticVMOptions -XX:MetaspaceReclaimPolicy=aggressive PrintMetaspaceDcmd\n","filename":"test\/hotspot\/jtreg\/runtime\/Metaspace\/PrintMetaspaceDcmd.java","additions":1,"deletions":11,"binary":false,"changes":12,"status":"modified"},{"patch":"@@ -2,1 +2,1 @@\n- * Copyright (c) 2021, 2022, Oracle and\/or its affiliates. All rights reserved.\n+ * Copyright (c) 2021, 2023, Oracle and\/or its affiliates. All rights reserved.\n@@ -235,1 +235,1 @@\n-            Utils.shouldHaveThrownException(baos.toString());\n+            Asserts.fail(\"Should have thrown exception\");\n@@ -367,1 +367,1 @@\n-            Utils.shouldHaveThrownException(baos.toString());\n+            Asserts.fail(\"Should have thrown exception\");\n","filename":"test\/hotspot\/jtreg\/testlibrary_tests\/ir_framework\/tests\/TestIRMatching.java","additions":3,"deletions":3,"binary":false,"changes":6,"status":"modified"}]}