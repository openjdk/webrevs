{"files":[{"patch":"@@ -61,3 +61,0 @@\n-          - s390x\n-          - ppc64le\n-          - riscv64\n@@ -76,16 +73,0 @@\n-          - target-cpu: s390x\n-            gnu-arch: s390x\n-            debian-arch: s390x\n-            debian-repository: https:\/\/httpredir.debian.org\/debian\/\n-            debian-version: bullseye\n-          - target-cpu: ppc64le\n-            gnu-arch: powerpc64le\n-            debian-arch: ppc64el\n-            debian-repository: https:\/\/httpredir.debian.org\/debian\/\n-            debian-version: bullseye\n-          - target-cpu: riscv64\n-            gnu-arch: riscv64\n-            debian-arch: riscv64\n-            debian-repository: https:\/\/deb.debian.org\/debian-ports\n-            debian-keyring: \/usr\/share\/keyrings\/debian-ports-archive-keyring.gpg\n-            debian-version: sid\n","filename":".github\/workflows\/build-cross-compile.yml","additions":0,"deletions":19,"binary":false,"changes":19,"status":"modified"},{"patch":"@@ -2,1 +2,1 @@\n-project=jdk\n+project=lilliput\n@@ -7,1 +7,1 @@\n-error=author,committer,reviewers,merge,issues,executable,symlink,message,hg-tag,whitespace,problemlists\n+error=author,committer,reviewers,issues,executable,symlink,message,hg-tag,whitespace,problemlists\n@@ -21,3 +21,0 @@\n-[checks \"merge\"]\n-message=Merge\n-\n@@ -25,1 +22,1 @@\n-reviewers=1\n+committers=1\n","filename":".jcheck\/conf","additions":3,"deletions":6,"binary":false,"changes":9,"status":"modified"},{"patch":"@@ -1787,0 +1787,12 @@\n+  int max_monitors = C->method() != NULL ? C->max_monitors() : 0;\n+  if (UseFastLocking && max_monitors > 0) {\n+    C2CheckLockStackStub* stub = new (C->comp_arena()) C2CheckLockStackStub();\n+    C->output()->add_stub(stub);\n+    __ ldr(r9, Address(rthread, JavaThread::lock_stack_current_offset()));\n+    __ ldr(r10, Address(rthread, JavaThread::lock_stack_limit_offset()));\n+    __ add(r9, r9, max_monitors * oopSize);\n+    __ cmp(r9, r10);\n+    __ br(Assembler::GE, stub->entry());\n+    __ bind(stub->continuation());\n+  }\n+\n@@ -3773,1 +3785,1 @@\n-    Label no_count;\n+    Label count, no_count;\n@@ -3791,14 +3803,2 @@\n-      \/\/ Set tmp to be (markWord of object | UNLOCK_VALUE).\n-      __ orr(tmp, disp_hdr, markWord::unlocked_value);\n-\n-      \/\/ Initialize the box. (Must happen before we update the object mark!)\n-      __ str(tmp, Address(box, BasicLock::displaced_header_offset_in_bytes()));\n-\n-      \/\/ Compare object markWord with an unlocked value (tmp) and if\n-      \/\/ equal exchange the stack address of our box with object markWord.\n-      \/\/ On failure disp_hdr contains the possibly locked markWord.\n-      __ cmpxchg(oop, tmp, box, Assembler::xword, \/*acquire*\/ true,\n-                 \/*release*\/ true, \/*weak*\/ false, disp_hdr);\n-      __ br(Assembler::EQ, cont);\n-\n-      assert(oopDesc::mark_offset_in_bytes() == 0, \"offset of _mark is not 0\");\n+      if (UseFastLocking) {\n+        __ fast_lock(oop, disp_hdr, tmp, rscratch1, no_count, false);\n@@ -3806,12 +3806,33 @@\n-      \/\/ If the compare-and-exchange succeeded, then we found an unlocked\n-      \/\/ object, will have now locked it will continue at label cont\n-\n-      \/\/ Check if the owner is self by comparing the value in the\n-      \/\/ markWord of object (disp_hdr) with the stack pointer.\n-      __ mov(rscratch1, sp);\n-      __ sub(disp_hdr, disp_hdr, rscratch1);\n-      __ mov(tmp, (address) (~(os::vm_page_size()-1) | markWord::lock_mask_in_place));\n-      \/\/ If condition is true we are cont and hence we can store 0 as the\n-      \/\/ displaced header in the box, which indicates that it is a recursive lock.\n-      __ ands(tmp\/*==0?*\/, disp_hdr, tmp);   \/\/ Sets flags for result\n-      __ str(tmp\/*==0, perhaps*\/, Address(box, BasicLock::displaced_header_offset_in_bytes()));\n+        \/\/ Indicate success at cont.\n+        __ cmp(oop, oop);\n+        __ b(count);\n+      } else {\n+        \/\/ Set tmp to be (markWord of object | UNLOCK_VALUE).\n+        __ orr(tmp, disp_hdr, markWord::unlocked_value);\n+\n+        \/\/ Initialize the box. (Must happen before we update the object mark!)\n+        __ str(tmp, Address(box, BasicLock::displaced_header_offset_in_bytes()));\n+\n+        \/\/ Compare object markWord with an unlocked value (tmp) and if\n+        \/\/ equal exchange the stack address of our box with object markWord.\n+        \/\/ On failure disp_hdr contains the possibly locked markWord.\n+        __ cmpxchg(oop, tmp, box, Assembler::xword, \/*acquire*\/ true,\n+                   \/*release*\/ true, \/*weak*\/ false, disp_hdr);\n+        __ br(Assembler::EQ, cont);\n+\n+        assert(oopDesc::mark_offset_in_bytes() == 0, \"offset of _mark is not 0\");\n+\n+        \/\/ If the compare-and-exchange succeeded, then we found an unlocked\n+        \/\/ object, will have now locked it will continue at label cont\n+\n+        \/\/ Check if the owner is self by comparing the value in the\n+        \/\/ markWord of object (disp_hdr) with the stack pointer.\n+        __ mov(rscratch1, sp);\n+        __ sub(disp_hdr, disp_hdr, rscratch1);\n+        __ mov(tmp, (address) (~(os::vm_page_size()-1) | markWord::lock_mask_in_place));\n+        \/\/ If condition is true we are cont and hence we can store 0 as the\n+        \/\/ displaced header in the box, which indicates that it is a recursive lock.\n+        __ ands(tmp\/*==0?*\/, disp_hdr, tmp);   \/\/ Sets flags for result\n+        __ str(tmp\/*==0, perhaps*\/, Address(box, BasicLock::displaced_header_offset_in_bytes()));\n+        __ b(cont);\n+      }\n@@ -3820,0 +3841,1 @@\n+      __ b(cont);\n@@ -3821,1 +3843,0 @@\n-    __ b(cont);\n@@ -3834,7 +3855,8 @@\n-    \/\/ Store a non-null value into the box to avoid looking like a re-entrant\n-    \/\/ lock. The fast-path monitor unlock code checks for\n-    \/\/ markWord::monitor_value so use markWord::unused_mark which has the\n-    \/\/ relevant bit set, and also matches ObjectSynchronizer::enter.\n-    __ mov(tmp, (address)markWord::unused_mark().value());\n-    __ str(tmp, Address(box, BasicLock::displaced_header_offset_in_bytes()));\n-\n+    if (!UseFastLocking) {\n+      \/\/ Store a non-null value into the box to avoid looking like a re-entrant\n+      \/\/ lock. The fast-path monitor unlock code checks for\n+      \/\/ markWord::monitor_value so use markWord::unused_mark which has the\n+      \/\/ relevant bit set, and also matches ObjectSynchronizer::enter.\n+      __ mov(tmp, (address)markWord::unused_mark().value());\n+      __ str(tmp, Address(box, BasicLock::displaced_header_offset_in_bytes()));\n+    }\n@@ -3855,0 +3877,1 @@\n+    __ bind(count);\n@@ -3868,1 +3891,1 @@\n-    Label no_count;\n+    Label count, no_count;\n@@ -3872,1 +3895,1 @@\n-    if (!UseHeavyMonitors) {\n+    if (!UseHeavyMonitors && !UseFastLocking) {\n@@ -3886,3 +3909,10 @@\n-      \/\/ Check if it is still a light weight lock, this is is true if we\n-      \/\/ see the stack address of the basicLock in the markWord of the\n-      \/\/ object.\n+      if (UseFastLocking) {\n+        __ fast_unlock(oop, tmp, box, disp_hdr, no_count);\n+\n+        \/\/ Indicate success at cont.\n+        __ cmp(oop, oop);\n+        __ b(count);\n+      } else {\n+        \/\/ Check if it is still a light weight lock, this is is true if we\n+        \/\/ see the stack address of the basicLock in the markWord of the\n+        \/\/ object.\n@@ -3890,2 +3920,4 @@\n-      __ cmpxchg(oop, box, disp_hdr, Assembler::xword, \/*acquire*\/ false,\n-                 \/*release*\/ true, \/*weak*\/ false, tmp);\n+        __ cmpxchg(oop, box, disp_hdr, Assembler::xword, \/*acquire*\/ false,\n+                   \/*release*\/ true, \/*weak*\/ false, tmp);\n+        __ b(cont);\n+      }\n@@ -3894,0 +3926,1 @@\n+      __ b(cont);\n@@ -3895,1 +3928,0 @@\n-    __ b(cont);\n@@ -3903,0 +3935,11 @@\n+\n+    if (UseFastLocking) {\n+      \/\/ If the owner is anonymous, we need to fix it -- in the slow-path.\n+      __ ldr(disp_hdr, Address(tmp, ObjectMonitor::owner_offset_in_bytes()));\n+      \/\/ We cannot use tbnz here: tbnz would leave the condition flags untouched,\n+      \/\/ but we want to carry-over the NE condition to the exit at the cont label,\n+      \/\/ in order to take the slow-path.\n+      __ tst(disp_hdr, (uint64_t)(intptr_t) ANONYMOUS_OWNER);\n+      __ br(Assembler::NE, no_count);\n+    }\n+\n@@ -3929,0 +3972,1 @@\n+    __ bind(count);\n@@ -7191,1 +7235,1 @@\n-instruct loadNKlass(iRegNNoSp dst, memory4 mem)\n+instruct loadNKlass(iRegNNoSp dst, memory4 mem, rFlagsReg cr)\n@@ -7194,0 +7238,1 @@\n+  effect(TEMP_DEF dst, KILL cr);\n@@ -7198,4 +7243,16 @@\n-\n-  ins_encode(aarch64_enc_ldrw(dst, mem));\n-\n-  ins_pipe(iload_reg_mem);\n+  ins_encode %{\n+    assert($mem$$disp == oopDesc::klass_offset_in_bytes(), \"expect correct offset\");\n+    assert($mem$$index$$Register == noreg, \"expect no index\");\n+    Register dst = $dst$$Register;\n+    Register obj = $mem$$base$$Register;\n+    C2LoadNKlassStub* stub = new (Compile::current()->comp_arena()) C2LoadNKlassStub(dst);\n+    Compile::current()->output()->add_stub(stub);\n+    __ ldr(dst, Address(obj, oopDesc::mark_offset_in_bytes()));\n+    \/\/ NOTE: We can't use tbnz here, because the target is sometimes too far away\n+    \/\/ and cannot be encoded.\n+    __ tst(dst, markWord::monitor_value);\n+    __ br(Assembler::NE, stub->entry());\n+    __ bind(stub->continuation());\n+    __ lsr(dst, dst, markWord::klass_shift);\n+  %}\n+  ins_pipe(pipe_slow);\n","filename":"src\/hotspot\/cpu\/aarch64\/aarch64.ad","additions":105,"deletions":48,"binary":false,"changes":153,"status":"modified"},{"patch":"@@ -35,0 +35,1 @@\n+#include \"runtime\/objectMonitor.hpp\"\n@@ -236,0 +237,6 @@\n+void LoadKlassStub::emit_code(LIR_Assembler* ce) {\n+  __ bind(_entry);\n+  Register d = _result->as_register();\n+  __ ldr(d, Address(d, OM_OFFSET_NO_MONITOR_VALUE_TAG(header)));\n+  __ b(_continuation);\n+}\n","filename":"src\/hotspot\/cpu\/aarch64\/c1_CodeStubs_aarch64.cpp","additions":7,"deletions":0,"binary":false,"changes":7,"status":"modified"},{"patch":"@@ -245,1 +245,1 @@\n-  __ build_frame(initial_frame_size_in_bytes(), bang_size_in_bytes());\n+  __ build_frame(initial_frame_size_in_bytes(), bang_size_in_bytes(), compilation()->max_monitors());\n@@ -1232,1 +1232,1 @@\n-                      arrayOopDesc::header_size(op->type()),\n+                      arrayOopDesc::base_offset_in_bytes(op->type()),\n@@ -2290,2 +2290,0 @@\n-  Address src_klass_addr = Address(src, oopDesc::klass_offset_in_bytes());\n-  Address dst_klass_addr = Address(dst, oopDesc::klass_offset_in_bytes());\n@@ -2352,9 +2350,4 @@\n-      if (UseCompressedClassPointers) {\n-        __ ldrw(tmp, src_klass_addr);\n-        __ ldrw(rscratch1, dst_klass_addr);\n-        __ cmpw(tmp, rscratch1);\n-      } else {\n-        __ ldr(tmp, src_klass_addr);\n-        __ ldr(rscratch1, dst_klass_addr);\n-        __ cmp(tmp, rscratch1);\n-      }\n+      assert(UseCompressedClassPointers, \"Lilliput\");\n+      __ load_nklass(tmp, src);\n+      __ load_nklass(rscratch1, dst);\n+      __ cmpw(tmp, rscratch1);\n@@ -2486,0 +2479,1 @@\n+    assert(UseCompressedClassPointers, \"Lilliput\");\n@@ -2487,8 +2481,2 @@\n-\n-      if (UseCompressedClassPointers) {\n-        __ ldrw(rscratch1, dst_klass_addr);\n-        __ cmpw(tmp, rscratch1);\n-      } else {\n-        __ ldr(rscratch1, dst_klass_addr);\n-        __ cmp(tmp, rscratch1);\n-      }\n+      __ load_nklass(rscratch1, dst);\n+      __ cmpw(tmp, rscratch1);\n@@ -2496,7 +2484,2 @@\n-      if (UseCompressedClassPointers) {\n-        __ ldrw(rscratch1, src_klass_addr);\n-        __ cmpw(tmp, rscratch1);\n-      } else {\n-        __ ldr(rscratch1, src_klass_addr);\n-        __ cmp(tmp, rscratch1);\n-      }\n+      __ load_nklass(rscratch1, src);\n+      __ cmpw(tmp, rscratch1);\n@@ -2505,7 +2488,2 @@\n-      if (UseCompressedClassPointers) {\n-        __ ldrw(rscratch1, dst_klass_addr);\n-        __ cmpw(tmp, rscratch1);\n-      } else {\n-        __ ldr(rscratch1, dst_klass_addr);\n-        __ cmp(tmp, rscratch1);\n-      }\n+      __ load_nklass(rscratch1, dst);\n+      __ cmpw(tmp, rscratch1);\n@@ -2591,6 +2569,11 @@\n-  if (UseCompressedClassPointers) {\n-    __ ldrw(result, Address (obj, oopDesc::klass_offset_in_bytes()));\n-    __ decode_klass_not_null(result);\n-  } else {\n-    __ ldr(result, Address (obj, oopDesc::klass_offset_in_bytes()));\n-  }\n+  assert(UseCompressedClassPointers, \"expects UseCompressedClassPointers\");\n+\n+  \/\/ Check if we can take the (common) fast path, if obj is unlocked.\n+  __ ldr(result, Address(obj, oopDesc::mark_offset_in_bytes()));\n+  __ tst(result, markWord::monitor_value);\n+  __ br(Assembler::NE, *op->stub()->entry());\n+  __ bind(*op->stub()->continuation());\n+\n+  \/\/ Shift and decode Klass*.\n+  __ lsr(result, result, markWord::klass_shift);\n+  __ decode_klass_not_null(result);\n","filename":"src\/hotspot\/cpu\/aarch64\/c1_LIRAssembler_aarch64.cpp","additions":24,"deletions":41,"binary":false,"changes":65,"status":"modified"},{"patch":"@@ -86,31 +86,35 @@\n-  \/\/ and mark it as unlocked\n-  orr(hdr, hdr, markWord::unlocked_value);\n-  \/\/ save unlocked object header into the displaced header location on the stack\n-  str(hdr, Address(disp_hdr, 0));\n-  \/\/ test if object header is still the same (i.e. unlocked), and if so, store the\n-  \/\/ displaced header address in the object header - if it is not the same, get the\n-  \/\/ object header instead\n-  lea(rscratch2, Address(obj, hdr_offset));\n-  cmpxchgptr(hdr, disp_hdr, rscratch2, rscratch1, done, \/*fallthough*\/NULL);\n-  \/\/ if the object header was the same, we're done\n-  \/\/ if the object header was not the same, it is now in the hdr register\n-  \/\/ => test if it is a stack pointer into the same stack (recursive locking), i.e.:\n-  \/\/\n-  \/\/ 1) (hdr & aligned_mask) == 0\n-  \/\/ 2) sp <= hdr\n-  \/\/ 3) hdr <= sp + page_size\n-  \/\/\n-  \/\/ these 3 tests can be done by evaluating the following expression:\n-  \/\/\n-  \/\/ (hdr - sp) & (aligned_mask - page_size)\n-  \/\/\n-  \/\/ assuming both the stack pointer and page_size have their least\n-  \/\/ significant 2 bits cleared and page_size is a power of 2\n-  mov(rscratch1, sp);\n-  sub(hdr, hdr, rscratch1);\n-  ands(hdr, hdr, aligned_mask - (int)os::vm_page_size());\n-  \/\/ for recursive locking, the result is zero => save it in the displaced header\n-  \/\/ location (NULL in the displaced hdr location indicates recursive locking)\n-  str(hdr, Address(disp_hdr, 0));\n-  \/\/ otherwise we don't care about the result and handle locking via runtime call\n-  cbnz(hdr, slow_case);\n+  if (UseFastLocking) {\n+    fast_lock(obj, hdr, rscratch1, rscratch2, slow_case, false);\n+  } else {\n+    \/\/ and mark it as unlocked\n+    orr(hdr, hdr, markWord::unlocked_value);\n+    \/\/ save unlocked object header into the displaced header location on the stack\n+    str(hdr, Address(disp_hdr, 0));\n+    \/\/ test if object header is still the same (i.e. unlocked), and if so, store the\n+    \/\/ displaced header address in the object header - if it is not the same, get the\n+    \/\/ object header instead\n+    lea(rscratch2, Address(obj, hdr_offset));\n+    cmpxchgptr(hdr, disp_hdr, rscratch2, rscratch1, done, \/*fallthough*\/NULL);\n+    \/\/ if the object header was the same, we're done\n+    \/\/ if the object header was not the same, it is now in the hdr register\n+    \/\/ => test if it is a stack pointer into the same stack (recursive locking), i.e.:\n+    \/\/\n+    \/\/ 1) (hdr & aligned_mask) == 0\n+    \/\/ 2) sp <= hdr\n+    \/\/ 3) hdr <= sp + page_size\n+    \/\/\n+    \/\/ these 3 tests can be done by evaluating the following expression:\n+    \/\/\n+    \/\/ (hdr - sp) & (aligned_mask - page_size)\n+    \/\/\n+    \/\/ assuming both the stack pointer and page_size have their least\n+    \/\/ significant 2 bits cleared and page_size is a power of 2\n+    mov(rscratch1, sp);\n+    sub(hdr, hdr, rscratch1);\n+    ands(hdr, hdr, aligned_mask - (int)os::vm_page_size());\n+    \/\/ for recursive locking, the result is zero => save it in the displaced header\n+    \/\/ location (NULL in the displaced hdr location indicates recursive locking)\n+    str(hdr, Address(disp_hdr, 0));\n+    \/\/ otherwise we don't care about the result and handle locking via runtime call\n+    cbnz(hdr, slow_case);\n+  }\n@@ -130,5 +134,8 @@\n-  \/\/ load displaced header\n-  ldr(hdr, Address(disp_hdr, 0));\n-  \/\/ if the loaded hdr is NULL we had recursive locking\n-  \/\/ if we had recursive locking, we are done\n-  cbz(hdr, done);\n+  if (!UseFastLocking) {\n+    \/\/ load displaced header\n+    ldr(hdr, Address(disp_hdr, 0));\n+    \/\/ if the loaded hdr is NULL we had recursive locking\n+    \/\/ if we had recursive locking, we are done\n+    cbz(hdr, done);\n+  }\n+\n@@ -138,8 +145,4 @@\n-  \/\/ test if object header is pointing to the displaced header, and if so, restore\n-  \/\/ the displaced header in the object - if the object header is not pointing to\n-  \/\/ the displaced header, get the object header instead\n-  \/\/ if the object header was not pointing to the displaced header,\n-  \/\/ we do unlocking via runtime call\n-  if (hdr_offset) {\n-    lea(rscratch1, Address(obj, hdr_offset));\n-    cmpxchgptr(disp_hdr, hdr, rscratch1, rscratch2, done, &slow_case);\n+\n+  if (UseFastLocking) {\n+    ldr(hdr, Address(obj, oopDesc::mark_offset_in_bytes()));\n+    fast_unlock(obj, hdr, rscratch1, rscratch2, slow_case);\n@@ -147,1 +150,13 @@\n-    cmpxchgptr(disp_hdr, hdr, obj, rscratch2, done, &slow_case);\n+    \/\/ test if object header is pointing to the displaced header, and if so, restore\n+    \/\/ the displaced header in the object - if the object header is not pointing to\n+    \/\/ the displaced header, get the object header instead\n+    \/\/ if the object header was not pointing to the displaced header,\n+    \/\/ we do unlocking via runtime call\n+    if (hdr_offset) {\n+      lea(rscratch1, Address(obj, hdr_offset));\n+      cmpxchgptr(disp_hdr, hdr, rscratch1, rscratch2, done, &slow_case);\n+    } else {\n+      cmpxchgptr(disp_hdr, hdr, obj, rscratch2, done, &slow_case);\n+    }\n+    \/\/ done\n+    bind(done);\n@@ -149,2 +164,0 @@\n-  \/\/ done\n-  bind(done);\n@@ -166,2 +179,1 @@\n-  \/\/ This assumes that all prototype bits fit in an int32_t\n-  mov(t1, (int32_t)(intptr_t)markWord::prototype().value());\n+  ldr(t1, Address(klass, Klass::prototype_header_offset()));\n@@ -170,7 +182,0 @@\n-  if (UseCompressedClassPointers) { \/\/ Take care not to kill klass\n-    encode_klass_not_null(t1, klass);\n-    strw(t1, Address(obj, oopDesc::klass_offset_in_bytes()));\n-  } else {\n-    str(klass, Address(obj, oopDesc::klass_offset_in_bytes()));\n-  }\n-\n@@ -179,2 +184,0 @@\n-  } else if (UseCompressedClassPointers) {\n-    store_klass_gap(obj, zr);\n@@ -198,0 +201,6 @@\n+  \/\/ Zero first 4 bytes, if start offset is not word aligned.\n+  if (!is_aligned(hdr_size_in_bytes, BytesPerWord)) {\n+    strw(zr, Address(obj, hdr_size_in_bytes));\n+    hdr_size_in_bytes += BytesPerInt;\n+  }\n+\n@@ -257,1 +266,1 @@\n-void C1_MacroAssembler::allocate_array(Register obj, Register len, Register t1, Register t2, int header_size, int f, Register klass, Label& slow_case) {\n+void C1_MacroAssembler::allocate_array(Register obj, Register len, Register t1, Register t2, int base_offset_in_bytes, int f, Register klass, Label& slow_case) {\n@@ -270,1 +279,1 @@\n-  mov(arr_size, (int32_t)header_size * BytesPerWord + MinObjAlignmentInBytesMask);\n+  mov(arr_size, (int32_t)base_offset_in_bytes + MinObjAlignmentInBytesMask);\n@@ -279,1 +288,1 @@\n-  initialize_body(obj, arr_size, header_size * BytesPerWord, t1, t2);\n+  initialize_body(obj, arr_size, base_offset_in_bytes, t1, t2);\n@@ -299,1 +308,1 @@\n-  assert(!MacroAssembler::needs_explicit_null_check(oopDesc::klass_offset_in_bytes()), \"must add explicit null check\");\n+  assert(!MacroAssembler::needs_explicit_null_check(oopDesc::mark_offset_in_bytes()), \"must add explicit null check\");\n@@ -305,1 +314,1 @@\n-void C1_MacroAssembler::build_frame(int framesize, int bang_size_in_bytes) {\n+void C1_MacroAssembler::build_frame(int framesize, int bang_size_in_bytes, int max_monitors) {\n@@ -312,0 +321,12 @@\n+  if (UseFastLocking && max_monitors > 0) {\n+    Label ok;\n+    ldr(r9, Address(rthread, JavaThread::lock_stack_current_offset()));\n+    ldr(r10, Address(rthread, JavaThread::lock_stack_limit_offset()));\n+    add(r9, r9, max_monitors * oopSize);\n+    cmp(r9, r10);\n+    br(Assembler::LT, ok);\n+    assert(StubRoutines::aarch64::check_lock_stack() != NULL, \"need runtime call stub\");\n+    far_call(StubRoutines::aarch64::check_lock_stack());\n+    bind(ok);\n+  }\n+\n","filename":"src\/hotspot\/cpu\/aarch64\/c1_MacroAssembler_aarch64.cpp","additions":84,"deletions":63,"binary":false,"changes":147,"status":"modified"},{"patch":"@@ -291,1 +291,0 @@\n-\n","filename":"src\/hotspot\/cpu\/aarch64\/c1_Runtime1_aarch64.cpp","additions":0,"deletions":1,"binary":false,"changes":1,"status":"modified"},{"patch":"@@ -28,0 +28,1 @@\n+#include \"runtime\/objectMonitor.hpp\"\n@@ -66,0 +67,22 @@\n+int C2CheckLockStackStub::max_size() const {\n+  return 20;\n+}\n+\n+void C2CheckLockStackStub::emit(C2_MacroAssembler& masm) {\n+  __ bind(entry());\n+  assert(StubRoutines::aarch64::check_lock_stack() != NULL, \"need runtime call stub\");\n+  __ far_call(StubRoutines::aarch64::check_lock_stack());\n+  __ b(continuation());\n+}\n+\n+int C2LoadNKlassStub::max_size() const {\n+  return 8;\n+}\n+\n+void C2LoadNKlassStub::emit(C2_MacroAssembler& masm) {\n+  __ bind(entry());\n+  Register d = dst();\n+  __ ldr(d, Address(d, OM_OFFSET_NO_MONITOR_VALUE_TAG(header)));\n+  __ b(continuation());\n+}\n+\n","filename":"src\/hotspot\/cpu\/aarch64\/c2_CodeStubs_aarch64.cpp","additions":23,"deletions":0,"binary":false,"changes":23,"status":"modified"},{"patch":"@@ -0,0 +1,48 @@\n+\/*\n+ * Copyright (c) 2021 SAP SE. All rights reserved.\n+ * Copyright (c) 2021, Red Hat Inc. All rights reserved.\n+ * Copyright (c) 2021, Oracle and\/or its affiliates. All rights reserved.\n+ *\n+ * DO NOT ALTER OR REMOVE COPYRIGHT NOTICES OR THIS FILE HEADER.\n+ *\n+ * This code is free software; you can redistribute it and\/or modify it\n+ * under the terms of the GNU General Public License version 2 only, as\n+ * published by the Free Software Foundation.\n+ *\n+ * This code is distributed in the hope that it will be useful, but WITHOUT\n+ * ANY WARRANTY; without even the implied warranty of MERCHANTABILITY or\n+ * FITNESS FOR A PARTICULAR PURPOSE.  See the GNU General Public License\n+ * version 2 for more details (a copy is included in the LICENSE file that\n+ * accompanied this code).\n+ *\n+ * You should have received a copy of the GNU General Public License version\n+ * 2 along with this work; if not, write to the Free Software Foundation,\n+ * Inc., 51 Franklin St, Fifth Floor, Boston, MA 02110-1301 USA.\n+ *\n+ * Please contact Oracle, 500 Oracle Parkway, Redwood Shores, CA 94065 USA\n+ * or visit www.oracle.com if you need additional information or have any\n+ * questions.\n+ *\n+ *\/\n+\n+#include \"precompiled.hpp\"\n+\n+#include \"macroAssembler_aarch64.hpp\"\n+#include \"oops\/compressedKlass.hpp\"\n+#include \"utilities\/ostream.hpp\"\n+#include \"runtime\/globals.hpp\"\n+\n+\/\/ Given an address p, return true if p can be used as an encoding base.\n+\/\/  (Some platforms have restrictions of what constitutes a valid base address).\n+bool CompressedKlassPointers::is_valid_base(address p) {\n+  return MacroAssembler::klass_decode_mode_for_base(p) != MacroAssembler::KlassDecodeNone;\n+}\n+\n+void CompressedKlassPointers::print_mode(outputStream* st) {\n+  st->print_cr(\"Narrow klass base: \" PTR_FORMAT \", Narrow klass shift: %d, \"\n+               \"Narrow klass range: \" UINT64_FORMAT_X\n+               \", Encoding mode %s\",\n+               p2i(base()), shift(), KlassEncodingMetaspaceMax,\n+               MacroAssembler::describe_klass_decode_mode(MacroAssembler::klass_decode_mode()));\n+}\n+\n","filename":"src\/hotspot\/cpu\/aarch64\/compressedKlass_aarch64.cpp","additions":48,"deletions":0,"binary":false,"changes":48,"status":"added"},{"patch":"@@ -761,50 +761,55 @@\n-    \/\/ Load (object->mark() | 1) into swap_reg\n-    ldr(rscratch1, Address(obj_reg, oopDesc::mark_offset_in_bytes()));\n-    orr(swap_reg, rscratch1, 1);\n-\n-    \/\/ Save (object->mark() | 1) into BasicLock's displaced header\n-    str(swap_reg, Address(lock_reg, mark_offset));\n-\n-    assert(lock_offset == 0,\n-           \"displached header must be first word in BasicObjectLock\");\n-\n-    Label fail;\n-    cmpxchg_obj_header(swap_reg, lock_reg, obj_reg, rscratch1, count, \/*fallthrough*\/NULL);\n-\n-    \/\/ Fast check for recursive lock.\n-    \/\/\n-    \/\/ Can apply the optimization only if this is a stack lock\n-    \/\/ allocated in this thread. For efficiency, we can focus on\n-    \/\/ recently allocated stack locks (instead of reading the stack\n-    \/\/ base and checking whether 'mark' points inside the current\n-    \/\/ thread stack):\n-    \/\/  1) (mark & 7) == 0, and\n-    \/\/  2) sp <= mark < mark + os::pagesize()\n-    \/\/\n-    \/\/ Warning: sp + os::pagesize can overflow the stack base. We must\n-    \/\/ neither apply the optimization for an inflated lock allocated\n-    \/\/ just above the thread stack (this is why condition 1 matters)\n-    \/\/ nor apply the optimization if the stack lock is inside the stack\n-    \/\/ of another thread. The latter is avoided even in case of overflow\n-    \/\/ because we have guard pages at the end of all stacks. Hence, if\n-    \/\/ we go over the stack base and hit the stack of another thread,\n-    \/\/ this should not be in a writeable area that could contain a\n-    \/\/ stack lock allocated by that thread. As a consequence, a stack\n-    \/\/ lock less than page size away from sp is guaranteed to be\n-    \/\/ owned by the current thread.\n-    \/\/\n-    \/\/ These 3 tests can be done by evaluating the following\n-    \/\/ expression: ((mark - sp) & (7 - os::vm_page_size())),\n-    \/\/ assuming both stack pointer and pagesize have their\n-    \/\/ least significant 3 bits clear.\n-    \/\/ NOTE: the mark is in swap_reg %r0 as the result of cmpxchg\n-    \/\/ NOTE2: aarch64 does not like to subtract sp from rn so take a\n-    \/\/ copy\n-    mov(rscratch1, sp);\n-    sub(swap_reg, swap_reg, rscratch1);\n-    ands(swap_reg, swap_reg, (uint64_t)(7 - (int)os::vm_page_size()));\n-\n-    \/\/ Save the test result, for recursive case, the result is zero\n-    str(swap_reg, Address(lock_reg, mark_offset));\n-    br(Assembler::EQ, count);\n-\n+    if (UseFastLocking) {\n+      ldr(tmp, Address(obj_reg, oopDesc::mark_offset_in_bytes()));\n+      fast_lock(obj_reg, tmp, rscratch1, rscratch2, slow_case);\n+      b(count);\n+    } else {\n+      \/\/ Load (object->mark() | 1) into swap_reg\n+      ldr(rscratch1, Address(obj_reg, oopDesc::mark_offset_in_bytes()));\n+      orr(swap_reg, rscratch1, 1);\n+\n+      \/\/ Save (object->mark() | 1) into BasicLock's displaced header\n+      str(swap_reg, Address(lock_reg, mark_offset));\n+\n+      assert(lock_offset == 0,\n+             \"displached header must be first word in BasicObjectLock\");\n+\n+      Label fail;\n+      cmpxchg_obj_header(swap_reg, lock_reg, obj_reg, rscratch1, count, \/*fallthrough*\/NULL);\n+\n+      \/\/ Fast check for recursive lock.\n+      \/\/\n+      \/\/ Can apply the optimization only if this is a stack lock\n+      \/\/ allocated in this thread. For efficiency, we can focus on\n+      \/\/ recently allocated stack locks (instead of reading the stack\n+      \/\/ base and checking whether 'mark' points inside the current\n+      \/\/ thread stack):\n+      \/\/  1) (mark & 7) == 0, and\n+      \/\/  2) sp <= mark < mark + os::pagesize()\n+      \/\/\n+      \/\/ Warning: sp + os::pagesize can overflow the stack base. We must\n+      \/\/ neither apply the optimization for an inflated lock allocated\n+      \/\/ just above the thread stack (this is why condition 1 matters)\n+      \/\/ nor apply the optimization if the stack lock is inside the stack\n+      \/\/ of another thread. The latter is avoided even in case of overflow\n+      \/\/ because we have guard pages at the end of all stacks. Hence, if\n+      \/\/ we go over the stack base and hit the stack of another thread,\n+      \/\/ this should not be in a writeable area that could contain a\n+      \/\/ stack lock allocated by that thread. As a consequence, a stack\n+      \/\/ lock less than page size away from sp is guaranteed to be\n+      \/\/ owned by the current thread.\n+      \/\/\n+      \/\/ These 3 tests can be done by evaluating the following\n+      \/\/ expression: ((mark - sp) & (7 - os::vm_page_size())),\n+      \/\/ assuming both stack pointer and pagesize have their\n+      \/\/ least significant 3 bits clear.\n+      \/\/ NOTE: the mark is in swap_reg %r0 as the result of cmpxchg\n+      \/\/ NOTE2: aarch64 does not like to subtract sp from rn so take a\n+      \/\/ copy\n+      mov(rscratch1, sp);\n+      sub(swap_reg, swap_reg, rscratch1);\n+      ands(swap_reg, swap_reg, (uint64_t)(7 - (int)os::vm_page_size()));\n+\n+      \/\/ Save the test result, for recursive case, the result is zero\n+      str(swap_reg, Address(lock_reg, mark_offset));\n+      br(Assembler::EQ, count);\n+    }\n@@ -816,1 +821,1 @@\n-            lock_reg);\n+            UseFastLocking ? obj_reg : lock_reg);\n@@ -853,3 +858,5 @@\n-    \/\/ Convert from BasicObjectLock structure to object and BasicLock\n-    \/\/ structure Store the BasicLock address into %r0\n-    lea(swap_reg, Address(lock_reg, BasicObjectLock::lock_offset_in_bytes()));\n+    if (!UseFastLocking) {\n+      \/\/ Convert from BasicObjectLock structure to object and BasicLock\n+      \/\/ structure Store the BasicLock address into %r0\n+      lea(swap_reg, Address(lock_reg, BasicObjectLock::lock_offset_in_bytes()));\n+    }\n@@ -863,3 +870,2 @@\n-    \/\/ Load the old header from BasicLock structure\n-    ldr(header_reg, Address(swap_reg,\n-                            BasicLock::displaced_header_offset_in_bytes()));\n+    if (UseFastLocking) {\n+      Label slow_case;\n@@ -867,2 +873,14 @@\n-    \/\/ Test for recursion\n-    cbz(header_reg, count);\n+      \/\/ Check for non-symmetric locking. This is allowed by the spec and the interpreter\n+      \/\/ must handle it.\n+      ldr(header_reg, Address(rthread, JavaThread::lock_stack_current_offset()));\n+      cmpoop(header_reg, obj_reg);\n+      br(Assembler::NE, slow_case);\n+\n+      ldr(header_reg, Address(obj_reg, oopDesc::mark_offset_in_bytes()));\n+      fast_unlock(obj_reg, header_reg, swap_reg, rscratch1, slow_case);\n+      b(count);\n+      bind(slow_case);\n+    } else {\n+      \/\/ Load the old header from BasicLock structure\n+      ldr(header_reg, Address(swap_reg,\n+                              BasicLock::displaced_header_offset_in_bytes()));\n@@ -870,2 +888,2 @@\n-    \/\/ Atomic swap back the old header\n-    cmpxchg_obj_header(swap_reg, header_reg, obj_reg, rscratch1, count, \/*fallthrough*\/NULL);\n+      \/\/ Test for recursion\n+      cbz(header_reg, count);\n@@ -873,0 +891,3 @@\n+      \/\/ Atomic swap back the old header\n+      cmpxchg_obj_header(swap_reg, header_reg, obj_reg, rscratch1, count, \/*fallthrough*\/NULL);\n+    }\n","filename":"src\/hotspot\/cpu\/aarch64\/interp_masm_aarch64.cpp","additions":82,"deletions":61,"binary":false,"changes":143,"status":"modified"},{"patch":"@@ -48,0 +48,1 @@\n+#include \"oops\/compressedKlass.inline.hpp\"\n@@ -4255,0 +4256,21 @@\n+\/\/ Loads the obj's Klass* into dst.\n+\/\/ src and dst must be distinct registers\n+\/\/ Preserves all registers (incl src, rscratch1 and rscratch2), but clobbers condition flags\n+void MacroAssembler::load_nklass(Register dst, Register src) {\n+  assert(UseCompressedClassPointers, \"expects UseCompressedClassPointers\");\n+\n+  Label fast;\n+\n+  \/\/ Check if we can take the (common) fast path, if obj is unlocked.\n+  ldr(dst, Address(src, oopDesc::mark_offset_in_bytes()));\n+  tst(dst, markWord::monitor_value);\n+  br(Assembler::EQ, fast);\n+\n+  \/\/ Fetch displaced header\n+  ldr(dst, Address(dst, OM_OFFSET_NO_MONITOR_VALUE_TAG(header)));\n+\n+  \/\/ Fast-path: shift and decode Klass*.\n+  bind(fast);\n+  lsr(dst, dst, markWord::klass_shift);\n+}\n+\n@@ -4256,6 +4278,2 @@\n-  if (UseCompressedClassPointers) {\n-    ldrw(dst, Address(src, oopDesc::klass_offset_in_bytes()));\n-    decode_klass_not_null(dst);\n-  } else {\n-    ldr(dst, Address(src, oopDesc::klass_offset_in_bytes()));\n-  }\n+  load_nklass(dst, src);\n+  decode_klass_not_null(dst);\n@@ -4300,14 +4318,10 @@\n-  if (UseCompressedClassPointers) {\n-    ldrw(tmp, Address(oop, oopDesc::klass_offset_in_bytes()));\n-    if (CompressedKlassPointers::base() == NULL) {\n-      cmp(trial_klass, tmp, LSL, CompressedKlassPointers::shift());\n-      return;\n-    } else if (((uint64_t)CompressedKlassPointers::base() & 0xffffffff) == 0\n-               && CompressedKlassPointers::shift() == 0) {\n-      \/\/ Only the bottom 32 bits matter\n-      cmpw(trial_klass, tmp);\n-      return;\n-    }\n-    decode_klass_not_null(tmp);\n-  } else {\n-    ldr(tmp, Address(oop, oopDesc::klass_offset_in_bytes()));\n+  assert(UseCompressedClassPointers, \"Lilliput\");\n+  load_nklass(tmp, oop);\n+  if (CompressedKlassPointers::base() == NULL) {\n+    cmp(trial_klass, tmp, LSL, CompressedKlassPointers::shift());\n+    return;\n+  } else if (((uint64_t)CompressedKlassPointers::base() & 0xffffffff) == 0\n+             && CompressedKlassPointers::shift() == 0) {\n+    \/\/ Only the bottom 32 bits matter\n+    cmpw(trial_klass, tmp);\n+    return;\n@@ -4315,0 +4329,1 @@\n+  decode_klass_not_null(tmp);\n@@ -4318,18 +4333,0 @@\n-void MacroAssembler::store_klass(Register dst, Register src) {\n-  \/\/ FIXME: Should this be a store release?  concurrent gcs assumes\n-  \/\/ klass length is valid if klass field is not null.\n-  if (UseCompressedClassPointers) {\n-    encode_klass_not_null(src);\n-    strw(src, Address(dst, oopDesc::klass_offset_in_bytes()));\n-  } else {\n-    str(src, Address(dst, oopDesc::klass_offset_in_bytes()));\n-  }\n-}\n-\n-void MacroAssembler::store_klass_gap(Register dst, Register src) {\n-  if (UseCompressedClassPointers) {\n-    \/\/ Store to klass gap in destination\n-    strw(src, Address(dst, oopDesc::klass_gap_offset_in_bytes()));\n-  }\n-}\n-\n@@ -4470,0 +4467,14 @@\n+\/\/ Returns a static string\n+const char* MacroAssembler::describe_klass_decode_mode(MacroAssembler::KlassDecodeMode mode) {\n+  switch (mode) {\n+  case KlassDecodeNone: return \"none\";\n+  case KlassDecodeZero: return \"zero\";\n+  case KlassDecodeXor:  return \"xor\";\n+  case KlassDecodeMovk: return \"movk\";\n+  default:\n+    ShouldNotReachHere();\n+  }\n+  return NULL;\n+}\n+\n+\/\/ Return the current narrow Klass pointer decode mode.\n@@ -4471,2 +4482,4 @@\n-  assert(UseCompressedClassPointers, \"not using compressed class pointers\");\n-  assert(Metaspace::initialized(), \"metaspace not initialized yet\");\n+  if (_klass_decode_mode == KlassDecodeNone) {\n+    \/\/ First time initialization\n+    assert(UseCompressedClassPointers, \"not using compressed class pointers\");\n+    assert(Metaspace::initialized(), \"metaspace not initialized yet\");\n@@ -4474,2 +4487,5 @@\n-  if (_klass_decode_mode != KlassDecodeNone) {\n-    return _klass_decode_mode;\n+    _klass_decode_mode = klass_decode_mode_for_base(CompressedKlassPointers::base());\n+    guarantee(_klass_decode_mode != KlassDecodeNone,\n+              PTR_FORMAT \" is not a valid encoding base on aarch64\",\n+              p2i(CompressedKlassPointers::base()));\n+    log_info(metaspace)(\"klass decode mode initialized: %s\", describe_klass_decode_mode(_klass_decode_mode));\n@@ -4477,0 +4493,2 @@\n+  return _klass_decode_mode;\n+}\n@@ -4478,2 +4496,4 @@\n-  assert(LogKlassAlignmentInBytes == CompressedKlassPointers::shift()\n-         || 0 == CompressedKlassPointers::shift(), \"decode alg wrong\");\n+\/\/ Given an arbitrary base address, return the KlassDecodeMode that would be used. Return KlassDecodeNone\n+\/\/ if base address is not valid for encoding.\n+MacroAssembler::KlassDecodeMode MacroAssembler::klass_decode_mode_for_base(address base) {\n+  assert(CompressedKlassPointers::shift() != 0, \"not lilliput?\");\n@@ -4481,2 +4501,4 @@\n-  if (CompressedKlassPointers::base() == NULL) {\n-    return (_klass_decode_mode = KlassDecodeZero);\n+  const uint64_t base_u64 = (uint64_t) base;\n+\n+  if (base_u64 == 0) {\n+    return KlassDecodeZero;\n@@ -4485,7 +4507,3 @@\n-  if (operand_valid_for_logical_immediate(\n-        \/*is32*\/false, (uint64_t)CompressedKlassPointers::base())) {\n-    const uint64_t range_mask =\n-      (1ULL << log2i(CompressedKlassPointers::range())) - 1;\n-    if (((uint64_t)CompressedKlassPointers::base() & range_mask) == 0) {\n-      return (_klass_decode_mode = KlassDecodeXor);\n-    }\n+  if (operand_valid_for_logical_immediate(false, base_u64) &&\n+      ((base_u64 & (KlassEncodingMetaspaceMax - 1)) == 0)) {\n+    return KlassDecodeXor;\n@@ -4494,4 +4512,4 @@\n-  const uint64_t shifted_base =\n-    (uint64_t)CompressedKlassPointers::base() >> CompressedKlassPointers::shift();\n-  guarantee((shifted_base & 0xffff0000ffffffff) == 0,\n-            \"compressed class base bad alignment\");\n+  const uint64_t shifted_base = base_u64 >> CompressedKlassPointers::shift();\n+  if ((shifted_base & 0xffff0000ffffffff) == 0) {\n+    return KlassDecodeMovk;\n+  }\n@@ -4499,1 +4517,1 @@\n-  return (_klass_decode_mode = KlassDecodeMovk);\n+  return KlassDecodeNone;\n@@ -4503,0 +4521,2 @@\n+  assert (UseCompressedClassPointers, \"should only be used for compressed headers\");\n+  assert(CompressedKlassPointers::shift() != 0, \"not lilliput?\");\n@@ -4505,5 +4525,1 @@\n-    if (CompressedKlassPointers::shift() != 0) {\n-      lsr(dst, src, LogKlassAlignmentInBytes);\n-    } else {\n-      if (dst != src) mov(dst, src);\n-    }\n+    lsr(dst, src, LogKlassAlignmentInBytes);\n@@ -4513,6 +4529,2 @@\n-    if (CompressedKlassPointers::shift() != 0) {\n-      eor(dst, src, (uint64_t)CompressedKlassPointers::base());\n-      lsr(dst, dst, LogKlassAlignmentInBytes);\n-    } else {\n-      eor(dst, src, (uint64_t)CompressedKlassPointers::base());\n-    }\n+    eor(dst, src, (uint64_t)CompressedKlassPointers::base());\n+    lsr(dst, dst, LogKlassAlignmentInBytes);\n@@ -4522,5 +4534,1 @@\n-    if (CompressedKlassPointers::shift() != 0) {\n-      ubfx(dst, src, LogKlassAlignmentInBytes, 32);\n-    } else {\n-      movw(dst, src);\n-    }\n+    ubfx(dst, src, LogKlassAlignmentInBytes, MaxNarrowKlassPointerBits);\n@@ -4542,0 +4550,2 @@\n+  assert(CompressedKlassPointers::shift() != 0, \"not lilliput?\");\n+\n@@ -4544,5 +4554,1 @@\n-    if (CompressedKlassPointers::shift() != 0) {\n-      lsl(dst, src, LogKlassAlignmentInBytes);\n-    } else {\n-      if (dst != src) mov(dst, src);\n-    }\n+    if (dst != src) mov(dst, src);\n@@ -4552,6 +4558,2 @@\n-    if (CompressedKlassPointers::shift() != 0) {\n-      lsl(dst, src, LogKlassAlignmentInBytes);\n-      eor(dst, dst, (uint64_t)CompressedKlassPointers::base());\n-    } else {\n-      eor(dst, src, (uint64_t)CompressedKlassPointers::base());\n-    }\n+    lsl(dst, src, LogKlassAlignmentInBytes);\n+    eor(dst, dst, (uint64_t)CompressedKlassPointers::base());\n@@ -4564,0 +4566,3 @@\n+    \/\/ Invalid base should have been gracefully handled via klass_decode_mode() in VM initialization.\n+    assert((shifted_base & 0xffff0000ffffffff) == 0, \"incompatible base\");\n+\n@@ -4566,5 +4571,1 @@\n-\n-    if (CompressedKlassPointers::shift() != 0) {\n-      lsl(dst, dst, LogKlassAlignmentInBytes);\n-    }\n-\n+    lsl(dst, dst, LogKlassAlignmentInBytes);\n@@ -6152,0 +6153,55 @@\n+\n+\/\/ Attempt to fast-lock an object. Fall-through on success, branch to slow label\n+\/\/ on failure.\n+\/\/ Registers:\n+\/\/  - obj: the object to be locked\n+\/\/  - hdr: the header, already loaded from obj, will be destroyed\n+\/\/  - t1, t2, t3: temporary registers, will be destroyed\n+void MacroAssembler::fast_lock(Register obj, Register hdr, Register t1, Register t2, Label& slow, bool rt_check_stack) {\n+  assert(UseFastLocking, \"only used with fast-locking\");\n+  assert_different_registers(obj, hdr, t1, t2);\n+\n+  if (rt_check_stack) {\n+    \/\/ Check if we would have space on lock-stack for the object.\n+    ldr(t1, Address(rthread, JavaThread::lock_stack_current_offset()));\n+    ldr(t2, Address(rthread, JavaThread::lock_stack_limit_offset()));\n+    cmp(t1, t2);\n+    br(Assembler::GE, slow);\n+  }\n+\n+  \/\/ Load (object->mark() | 1) into hdr\n+  orr(hdr, hdr, markWord::unlocked_value);\n+  \/\/ Clear lock-bits, into t2\n+  eor(t2, hdr, markWord::unlocked_value);\n+  \/\/ Try to swing header from unlocked to locked\n+  cmpxchg(\/*addr*\/ obj, \/*expected*\/ hdr, \/*new*\/ t2, Assembler::xword,\n+          \/*acquire*\/ true, \/*release*\/ true, \/*weak*\/ false, t1);\n+  br(Assembler::NE, slow);\n+\n+  \/\/ After successful lock, push object on lock-stack\n+  ldr(t1, Address(rthread, JavaThread::lock_stack_current_offset()));\n+  str(obj, Address(t1, 0));\n+  add(t1, t1, oopSize);\n+  str(t1, Address(rthread, JavaThread::lock_stack_current_offset()));\n+}\n+\n+void MacroAssembler::fast_unlock(Register obj, Register hdr, Register t1, Register t2, Label& slow) {\n+  assert(UseFastLocking, \"only used with fast-locking\");\n+  assert_different_registers(obj, hdr, t1, t2);\n+\n+  \/\/ Load the expected old header (lock-bits cleared to indicate 'locked') into hdr\n+  andr(hdr, hdr, ~markWord::lock_mask_in_place);\n+\n+  \/\/ Load the new header (unlocked) into t1\n+  orr(t1, hdr, markWord::unlocked_value);\n+\n+  \/\/ Try to swing header from locked to unlocked\n+  cmpxchg(obj, hdr, t1, Assembler::xword,\n+          \/*acquire*\/ true, \/*release*\/ true, \/*weak*\/ false, t2);\n+  br(Assembler::NE, slow);\n+\n+  \/\/ After successful unlock, pop object from lock-stack\n+  ldr(t1, Address(rthread, JavaThread::lock_stack_current_offset()));\n+  sub(t1, t1, oopSize);\n+  str(t1, Address(rthread, JavaThread::lock_stack_current_offset()));\n+}\n","filename":"src\/hotspot\/cpu\/aarch64\/macroAssembler_aarch64.cpp","additions":146,"deletions":90,"binary":false,"changes":236,"status":"modified"},{"patch":"@@ -89,0 +89,2 @@\n+ public:\n+\n@@ -96,1 +98,9 @@\n-  KlassDecodeMode klass_decode_mode();\n+  \/\/ Return the current narrow Klass pointer decode mode. Initialized on first call.\n+  static KlassDecodeMode klass_decode_mode();\n+\n+  \/\/ Given an arbitrary base address, return the KlassDecodeMode that would be used. Return KlassDecodeNone\n+  \/\/ if base address is not valid for encoding.\n+  static KlassDecodeMode klass_decode_mode_for_base(address base);\n+\n+  \/\/ Returns a static string\n+  static const char* describe_klass_decode_mode(KlassDecodeMode mode);\n@@ -845,0 +855,1 @@\n+  void load_nklass(Register dst, Register src);\n@@ -847,1 +858,0 @@\n-  void store_klass(Register dst, Register src);\n@@ -873,2 +883,0 @@\n-  void store_klass_gap(Register dst, Register src);\n-\n@@ -1574,0 +1582,3 @@\n+  void fast_lock(Register obj, Register hdr, Register t1, Register t2, Label& slow, bool rt_check_stack = true);\n+  void fast_unlock(Register obj, Register hdr, Register t1, Register t2, Label& slow);\n+\n","filename":"src\/hotspot\/cpu\/aarch64\/macroAssembler_aarch64.hpp","additions":15,"deletions":4,"binary":false,"changes":19,"status":"modified"},{"patch":"@@ -1778,28 +1778,33 @@\n-      \/\/ Load (object->mark() | 1) into swap_reg %r0\n-      __ ldr(rscratch1, Address(obj_reg, oopDesc::mark_offset_in_bytes()));\n-      __ orr(swap_reg, rscratch1, 1);\n-\n-      \/\/ Save (object->mark() | 1) into BasicLock's displaced header\n-      __ str(swap_reg, Address(lock_reg, mark_word_offset));\n-\n-      \/\/ src -> dest iff dest == r0 else r0 <- dest\n-      __ cmpxchg_obj_header(r0, lock_reg, obj_reg, rscratch1, count, \/*fallthrough*\/NULL);\n-\n-      \/\/ Hmm should this move to the slow path code area???\n-\n-      \/\/ Test if the oopMark is an obvious stack pointer, i.e.,\n-      \/\/  1) (mark & 3) == 0, and\n-      \/\/  2) sp <= mark < mark + os::pagesize()\n-      \/\/ These 3 tests can be done by evaluating the following\n-      \/\/ expression: ((mark - sp) & (3 - os::vm_page_size())),\n-      \/\/ assuming both stack pointer and pagesize have their\n-      \/\/ least significant 2 bits clear.\n-      \/\/ NOTE: the oopMark is in swap_reg %r0 as the result of cmpxchg\n-\n-      __ sub(swap_reg, sp, swap_reg);\n-      __ neg(swap_reg, swap_reg);\n-      __ ands(swap_reg, swap_reg, 3 - (int)os::vm_page_size());\n-\n-      \/\/ Save the test result, for recursive case, the result is zero\n-      __ str(swap_reg, Address(lock_reg, mark_word_offset));\n-      __ br(Assembler::NE, slow_path_lock);\n+      if (UseFastLocking) {\n+        __ ldr(swap_reg, Address(obj_reg, oopDesc::mark_offset_in_bytes()));\n+        __ fast_lock(obj_reg, swap_reg, tmp, rscratch1, slow_path_lock);\n+      } else {\n+        \/\/ Load (object->mark() | 1) into swap_reg %r0\n+        __ ldr(rscratch1, Address(obj_reg, oopDesc::mark_offset_in_bytes()));\n+        __ orr(swap_reg, rscratch1, 1);\n+\n+        \/\/ Save (object->mark() | 1) into BasicLock's displaced header\n+        __ str(swap_reg, Address(lock_reg, mark_word_offset));\n+\n+        \/\/ src -> dest iff dest == r0 else r0 <- dest\n+        __ cmpxchg_obj_header(r0, lock_reg, obj_reg, rscratch1, count, \/*fallthrough*\/NULL);\n+\n+        \/\/ Hmm should this move to the slow path code area???\n+\n+        \/\/ Test if the oopMark is an obvious stack pointer, i.e.,\n+        \/\/  1) (mark & 3) == 0, and\n+        \/\/  2) sp <= mark < mark + os::pagesize()\n+        \/\/ These 3 tests can be done by evaluating the following\n+        \/\/ expression: ((mark - sp) & (3 - os::vm_page_size())),\n+        \/\/ assuming both stack pointer and pagesize have their\n+        \/\/ least significant 2 bits clear.\n+        \/\/ NOTE: the oopMark is in swap_reg %r0 as the result of cmpxchg\n+\n+        __ sub(swap_reg, sp, swap_reg);\n+        __ neg(swap_reg, swap_reg);\n+        __ ands(swap_reg, swap_reg, 3 - (int)os::vm_page_size());\n+\n+        \/\/ Save the test result, for recursive case, the result is zero\n+        __ str(swap_reg, Address(lock_reg, mark_word_offset));\n+        __ br(Assembler::NE, slow_path_lock);\n+      }\n@@ -1916,1 +1921,1 @@\n-    if (!UseHeavyMonitors) {\n+    if (!UseHeavyMonitors && !UseFastLocking) {\n@@ -1932,9 +1937,14 @@\n-      \/\/ get address of the stack lock\n-      __ lea(r0, Address(sp, lock_slot_offset * VMRegImpl::stack_slot_size));\n-      \/\/  get old displaced header\n-      __ ldr(old_hdr, Address(r0, 0));\n-\n-      \/\/ Atomic swap old header if oop still contains the stack lock\n-      Label count;\n-      __ cmpxchg_obj_header(r0, old_hdr, obj_reg, rscratch1, count, &slow_path_unlock);\n-      __ bind(count);\n+      if (UseFastLocking) {\n+        __ ldr(old_hdr, Address(obj_reg, oopDesc::mark_offset_in_bytes()));\n+        __ fast_unlock(obj_reg, old_hdr, swap_reg, rscratch1, slow_path_unlock);\n+      } else {\n+        \/\/ get address of the stack lock\n+        __ lea(r0, Address(sp, lock_slot_offset * VMRegImpl::stack_slot_size));\n+        \/\/  get old displaced header\n+        __ ldr(old_hdr, Address(r0, 0));\n+\n+        \/\/ Atomic swap old header if oop still contains the stack lock\n+        Label count;\n+        __ cmpxchg_obj_header(r0, old_hdr, obj_reg, rscratch1, count, &slow_path_unlock);\n+        __ bind(count);\n+      }\n","filename":"src\/hotspot\/cpu\/aarch64\/sharedRuntime_aarch64.cpp","additions":48,"deletions":38,"binary":false,"changes":86,"status":"modified"},{"patch":"@@ -5360,0 +5360,23 @@\n+  address generate_check_lock_stack() {\n+    __ align(CodeEntryAlignment);\n+    StubCodeMark mark(this, \"StubRoutines\", \"check_lock_stack\");\n+\n+    address start = __ pc();\n+\n+    __ set_last_Java_frame(sp, rfp, lr, rscratch1);\n+    __ enter();\n+    __ push_call_clobbered_registers();\n+\n+    __ mov(c_rarg0, r9);\n+    __ call_VM_leaf(CAST_FROM_FN_PTR(address, LockStack::ensure_lock_stack_size), 1);\n+\n+\n+    __ pop_call_clobbered_registers();\n+    __ leave();\n+    __ reset_last_Java_frame(true);\n+\n+    __ ret(lr);\n+\n+    return start;\n+  }\n+\n@@ -7992,0 +8015,3 @@\n+    if (UseFastLocking) {\n+      StubRoutines::aarch64::_check_lock_stack = generate_check_lock_stack();\n+    }\n","filename":"src\/hotspot\/cpu\/aarch64\/stubGenerator_aarch64.cpp","additions":26,"deletions":0,"binary":false,"changes":26,"status":"modified"},{"patch":"@@ -60,0 +60,1 @@\n+address StubRoutines::aarch64::_check_lock_stack = NULL;\n","filename":"src\/hotspot\/cpu\/aarch64\/stubRoutines_aarch64.cpp","additions":1,"deletions":0,"binary":false,"changes":1,"status":"modified"},{"patch":"@@ -73,0 +73,2 @@\n+  static address _check_lock_stack;\n+\n@@ -183,0 +185,4 @@\n+  static address check_lock_stack() {\n+    return _check_lock_stack;\n+  }\n+\n","filename":"src\/hotspot\/cpu\/aarch64\/stubRoutines_aarch64.hpp","additions":6,"deletions":0,"binary":false,"changes":6,"status":"modified"},{"patch":"@@ -3531,1 +3531,1 @@\n-    __ mov(rscratch1, (intptr_t)markWord::prototype().value());\n+    __ ldr(rscratch1, Address(r4, Klass::prototype_header_offset()));\n@@ -3533,2 +3533,0 @@\n-    __ store_klass_gap(r0, zr);  \/\/ zero klass gap for compressed oops\n-    __ store_klass(r0, r4);      \/\/ store klass last\n","filename":"src\/hotspot\/cpu\/aarch64\/templateTable_aarch64.cpp","additions":1,"deletions":3,"binary":false,"changes":4,"status":"modified"},{"patch":"@@ -220,0 +220,4 @@\n+void LoadKlassStub::emit_code(LIR_Assembler* ce) {\n+  \/\/ Currently not needed.\n+  Unimplemented();\n+}\n","filename":"src\/hotspot\/cpu\/arm\/c1_CodeStubs_arm.cpp","additions":4,"deletions":0,"binary":false,"changes":4,"status":"modified"},{"patch":"@@ -148,1 +148,1 @@\n-  __ build_frame(initial_frame_size_in_bytes(), bang_size_in_bytes());\n+  __ build_frame(initial_frame_size_in_bytes(), bang_size_in_bytes(), compilation()->max_monitors());\n@@ -974,1 +974,1 @@\n-                      arrayOopDesc::header_size(op->type()),\n+                      arrayOopDesc::base_offset_in_bytes(op->type()),\n","filename":"src\/hotspot\/cpu\/arm\/c1_LIRAssembler_arm.cpp","additions":2,"deletions":2,"binary":false,"changes":4,"status":"modified"},{"patch":"@@ -56,1 +56,1 @@\n-void C1_MacroAssembler::build_frame(int frame_size_in_bytes, int bang_size_in_bytes) {\n+void C1_MacroAssembler::build_frame(int frame_size_in_bytes, int bang_size_in_bytes, int max_monitors) {\n@@ -166,1 +166,1 @@\n-                                       int header_size, int element_size,\n+                                       int header_size_in_bytes, int element_size,\n@@ -169,1 +169,0 @@\n-  const int header_size_in_bytes = header_size * BytesPerWord;\n","filename":"src\/hotspot\/cpu\/arm\/c1_MacroAssembler_arm.cpp","additions":2,"deletions":3,"binary":false,"changes":5,"status":"modified"},{"patch":"@@ -55,1 +55,1 @@\n-                      int header_size, int element_size,\n+                      int header_size_in_bytes, int element_size,\n","filename":"src\/hotspot\/cpu\/arm\/c1_MacroAssembler_arm.hpp","additions":1,"deletions":1,"binary":false,"changes":2,"status":"modified"},{"patch":"@@ -301,0 +301,4 @@\n+void LoadKlassStub::emit_code(LIR_Assembler* ce) {\n+  \/\/ Currently not needed.\n+  Unimplemented();\n+}\n","filename":"src\/hotspot\/cpu\/ppc\/c1_CodeStubs_ppc.cpp","additions":4,"deletions":0,"binary":false,"changes":4,"status":"modified"},{"patch":"@@ -116,1 +116,1 @@\n-  __ build_frame(initial_frame_size_in_bytes(), bang_size_in_bytes());\n+  __ build_frame(initial_frame_size_in_bytes(), bang_size_in_bytes(), compilation()->max_monitors());\n","filename":"src\/hotspot\/cpu\/ppc\/c1_LIRAssembler_ppc.cpp","additions":1,"deletions":1,"binary":false,"changes":2,"status":"modified"},{"patch":"@@ -72,1 +72,1 @@\n-void C1_MacroAssembler::build_frame(int frame_size_in_bytes, int bang_size_in_bytes) {\n+void C1_MacroAssembler::build_frame(int frame_size_in_bytes, int bang_size_in_bytes, int max_monitors) {\n","filename":"src\/hotspot\/cpu\/ppc\/c1_MacroAssembler_ppc.cpp","additions":1,"deletions":1,"binary":false,"changes":2,"status":"modified"},{"patch":"@@ -0,0 +1,44 @@\n+\/*\n+ * Copyright (c) 2021 SAP SE. All rights reserved.\n+ * Copyright (c) 2021, Red Hat Inc. All rights reserved.\n+ * Copyright (c) 2021, Oracle and\/or its affiliates. All rights reserved.\n+ *\n+ * DO NOT ALTER OR REMOVE COPYRIGHT NOTICES OR THIS FILE HEADER.\n+ *\n+ * This code is free software; you can redistribute it and\/or modify it\n+ * under the terms of the GNU General Public License version 2 only, as\n+ * published by the Free Software Foundation.\n+ *\n+ * This code is distributed in the hope that it will be useful, but WITHOUT\n+ * ANY WARRANTY; without even the implied warranty of MERCHANTABILITY or\n+ * FITNESS FOR A PARTICULAR PURPOSE.  See the GNU General Public License\n+ * version 2 for more details (a copy is included in the LICENSE file that\n+ * accompanied this code).\n+ *\n+ * You should have received a copy of the GNU General Public License version\n+ * 2 along with this work; if not, write to the Free Software Foundation,\n+ * Inc., 51 Franklin St, Fifth Floor, Boston, MA 02110-1301 USA.\n+ *\n+ * Please contact Oracle, 500 Oracle Parkway, Redwood Shores, CA 94065 USA\n+ * or visit www.oracle.com if you need additional information or have any\n+ * questions.\n+ *\n+ *\/\n+\n+#include \"precompiled.hpp\"\n+\n+#include \"oops\/compressedKlass.hpp\"\n+#include \"utilities\/ostream.hpp\"\n+#include \"runtime\/globals.hpp\"\n+\n+\/\/ Given an address p, return true if p can be used as an encoding base.\n+\/\/  (Some platforms have restrictions of what constitutes a valid base address).\n+bool CompressedKlassPointers::is_valid_base(address p) {\n+  return true;\n+}\n+\n+void CompressedKlassPointers::print_mode(outputStream* st) {\n+  st->print_cr(\"Narrow klass base: \" PTR_FORMAT \", Narrow klass shift: %d, \"\n+               \"Narrow klass range: \" UINT64_FORMAT, p2i(base()), shift(),\n+               KlassEncodingMetaspaceMax);\n+}\n","filename":"src\/hotspot\/cpu\/ppc\/compressedKlass_ppc64.cpp","additions":44,"deletions":0,"binary":false,"changes":44,"status":"added"},{"patch":"@@ -35,0 +35,1 @@\n+#include \"oops\/compressedKlass.inline.hpp\"\n","filename":"src\/hotspot\/cpu\/ppc\/macroAssembler_ppc.cpp","additions":1,"deletions":0,"binary":false,"changes":1,"status":"modified"},{"patch":"@@ -218,1 +218,1 @@\n-  __ build_frame(initial_frame_size_in_bytes(), bang_size_in_bytes());\n+  __ build_frame(initial_frame_size_in_bytes(), bang_size_in_bytes(), compilation()->max_monitors());\n","filename":"src\/hotspot\/cpu\/riscv\/c1_LIRAssembler_riscv.cpp","additions":1,"deletions":1,"binary":false,"changes":2,"status":"modified"},{"patch":"@@ -75,33 +75,38 @@\n-  \/\/ and mark it as unlocked\n-  ori(hdr, hdr, markWord::unlocked_value);\n-  \/\/ save unlocked object header into the displaced header location on the stack\n-  sd(hdr, Address(disp_hdr, 0));\n-  \/\/ test if object header is still the same (i.e. unlocked), and if so, store the\n-  \/\/ displaced header address in the object header - if it is not the same, get the\n-  \/\/ object header instead\n-  la(t1, Address(obj, hdr_offset));\n-  cmpxchgptr(hdr, disp_hdr, t1, t0, done, \/*fallthough*\/NULL);\n-  \/\/ if the object header was the same, we're done\n-  \/\/ if the object header was not the same, it is now in the hdr register\n-  \/\/ => test if it is a stack pointer into the same stack (recursive locking), i.e.:\n-  \/\/\n-  \/\/ 1) (hdr & aligned_mask) == 0\n-  \/\/ 2) sp <= hdr\n-  \/\/ 3) hdr <= sp + page_size\n-  \/\/\n-  \/\/ these 3 tests can be done by evaluating the following expression:\n-  \/\/\n-  \/\/ (hdr -sp) & (aligned_mask - page_size)\n-  \/\/\n-  \/\/ assuming both the stack pointer and page_size have their least\n-  \/\/ significant 2 bits cleared and page_size is a power of 2\n-  sub(hdr, hdr, sp);\n-  mv(t0, aligned_mask - (int)os::vm_page_size());\n-  andr(hdr, hdr, t0);\n-  \/\/ for recursive locking, the result is zero => save it in the displaced header\n-  \/\/ location (NULL in the displaced hdr location indicates recursive locking)\n-  sd(hdr, Address(disp_hdr, 0));\n-  \/\/ otherwise we don't care about the result and handle locking via runtime call\n-  bnez(hdr, slow_case, \/* is_far *\/ true);\n-  \/\/ done\n-  bind(done);\n+\n+  if (UseFastLocking) {\n+    fast_lock(obj, hdr, t0, t1, slow_case);\n+  } else {\n+    \/\/ and mark it as unlocked\n+    jori(hdr, hdr, markWord::unlocked_value);\n+    \/\/ save unlocked object header into the displaced header location on the stack\n+    sd(hdr, Address(disp_hdr, 0));\n+    \/\/ test if object header is still the same (i.e. unlocked), and if so, store the\n+    \/\/ displaced header address in the object header - if it is not the same, get the\n+    \/\/ object header instead\n+    la(t1, Address(obj, hdr_offset));\n+    cmpxchgptr(hdr, disp_hdr, t1, t0, done, \/*fallthough*\/NULL);\n+    \/\/ if the object header was the same, we're done\n+    \/\/ if the object header was not the same, it is now in the hdr register\n+    \/\/ => test if it is a stack pointer into the same stack (recursive locking), i.e.:\n+    \/\/\n+    \/\/ 1) (hdr & aligned_mask) == 0\n+    \/\/ 2) sp <= hdr\n+    \/\/ 3) hdr <= sp + page_size\n+    \/\/\n+    \/\/ these 3 tests can be done by evaluating the following expression:\n+    \/\/\n+    \/\/ (hdr -sp) & (aligned_mask - page_size)\n+    \/\/\n+    \/\/ assuming both the stack pointer and page_size have their least\n+    \/\/ significant 2 bits cleared and page_size is a power of 2\n+    sub(hdr, hdr, sp);\n+    mv(t0, aligned_mask - (int)os::vm_page_size());\n+    andr(hdr, hdr, t0);\n+    \/\/ for recursive locking, the result is zero => save it in the displaced header\n+    \/\/ location (NULL in the displaced hdr location indicates recursive locking)\n+    sd(hdr, Address(disp_hdr, 0));\n+    \/\/ otherwise we don't care about the result and handle locking via runtime call\n+    bnez(hdr, slow_case, \/* is_far *\/ true);\n+    \/\/ done\n+    bind(done);\n+  }\n@@ -118,16 +123,6 @@\n-  \/\/ load displaced header\n-  ld(hdr, Address(disp_hdr, 0));\n-  \/\/ if the loaded hdr is NULL we had recursive locking\n-  \/\/ if we had recursive locking, we are done\n-  beqz(hdr, done);\n-  \/\/ load object\n-  ld(obj, Address(disp_hdr, BasicObjectLock::obj_offset_in_bytes()));\n-  verify_oop(obj);\n-  \/\/ test if object header is pointing to the displaced header, and if so, restore\n-  \/\/ the displaced header in the object - if the object header is not pointing to\n-  \/\/ the displaced header, get the object header instead\n-  \/\/ if the object header was not pointing to the displaced header,\n-  \/\/ we do unlocking via runtime call\n-  if (hdr_offset) {\n-    la(t0, Address(obj, hdr_offset));\n-    cmpxchgptr(disp_hdr, hdr, t0, t1, done, &slow_case);\n+  if (UseFastLocking) {\n+    \/\/ load object\n+    ld(obj, Address(disp_hdr, BasicObjectLock::obj_offset_in_bytes()));\n+    verify_oop(obj);\n+    ld(hdr, Address(obj, oopDesc::mark_offset_in_bytes()));\n+    fast_unlock(obj, hdr, t0, t1, slow_case);\n@@ -135,1 +130,21 @@\n-    cmpxchgptr(disp_hdr, hdr, obj, t1, done, &slow_case);\n+    \/\/ load displaced header\n+    ld(hdr, Address(disp_hdr, 0));\n+    \/\/ if the loaded hdr is NULL we had recursive locking\n+    \/\/ if we had recursive locking, we are done\n+    beqz(hdr, done);\n+    \/\/ load object\n+    ld(obj, Address(disp_hdr, BasicObjectLock::obj_offset_in_bytes()));\n+    verify_oop(obj);\n+    \/\/ test if object header is pointing to the displaced header, and if so, restore\n+    \/\/ the displaced header in the object - if the object header is not pointing to\n+    \/\/ the displaced header, get the object header instead\n+    \/\/ if the object header was not pointing to the displaced header,\n+    \/\/ we do unlocking via runtime call\n+    if (hdr_offset) {\n+      la(t0, Address(obj, hdr_offset));\n+      cmpxchgptr(disp_hdr, hdr, t0, t1, done, &slow_case);\n+    } else {\n+      cmpxchgptr(disp_hdr, hdr, obj, t1, done, &slow_case);\n+    }\n+    \/\/ done\n+    bind(done);\n@@ -137,2 +152,0 @@\n-  \/\/ done\n-  bind(done);\n@@ -308,1 +321,1 @@\n-void C1_MacroAssembler::build_frame(int framesize, int bang_size_in_bytes) {\n+void C1_MacroAssembler::build_frame(int framesize, int bang_size_in_bytes, int max_monitors) {\n","filename":"src\/hotspot\/cpu\/riscv\/c1_MacroAssembler_riscv.cpp","additions":66,"deletions":53,"binary":false,"changes":119,"status":"modified"},{"patch":"@@ -812,28 +812,34 @@\n-    \/\/ Load (object->mark() | 1) into swap_reg\n-    ld(t0, Address(obj_reg, oopDesc::mark_offset_in_bytes()));\n-    ori(swap_reg, t0, 1);\n-\n-    \/\/ Save (object->mark() | 1) into BasicLock's displaced header\n-    sd(swap_reg, Address(lock_reg, mark_offset));\n-\n-    assert(lock_offset == 0,\n-           \"displached header must be first word in BasicObjectLock\");\n-\n-    cmpxchg_obj_header(swap_reg, lock_reg, obj_reg, t0, count, \/*fallthrough*\/NULL);\n-\n-    \/\/ Test if the oopMark is an obvious stack pointer, i.e.,\n-    \/\/  1) (mark & 7) == 0, and\n-    \/\/  2) sp <= mark < mark + os::pagesize()\n-    \/\/\n-    \/\/ These 3 tests can be done by evaluating the following\n-    \/\/ expression: ((mark - sp) & (7 - os::vm_page_size())),\n-    \/\/ assuming both stack pointer and pagesize have their\n-    \/\/ least significant 3 bits clear.\n-    \/\/ NOTE: the oopMark is in swap_reg x10 as the result of cmpxchg\n-    sub(swap_reg, swap_reg, sp);\n-    mv(t0, (int64_t)(7 - (int)os::vm_page_size()));\n-    andr(swap_reg, swap_reg, t0);\n-\n-    \/\/ Save the test result, for recursive case, the result is zero\n-    sd(swap_reg, Address(lock_reg, mark_offset));\n-    beqz(swap_reg, count);\n+    if (UseFastLocking) {\n+      ld(tmp, Address(obj_reg, oopDesc::mark_offset_in_bytes()));\n+      fast_lock(obj_reg, tmp, t0, t1, slow_case);\n+      j(count);\n+    } else {\n+      \/\/ Load (object->mark() | 1) into swap_reg\n+      ld(t0, Address(obj_reg, oopDesc::mark_offset_in_bytes()));\n+      ori(swap_reg, t0, 1);\n+\n+      \/\/ Save (object->mark() | 1) into BasicLock's displaced header\n+      sd(swap_reg, Address(lock_reg, mark_offset));\n+\n+      assert(lock_offset == 0,\n+             \"displached header must be first word in BasicObjectLock\");\n+\n+      cmpxchg_obj_header(swap_reg, lock_reg, obj_reg, t0, count, \/*fallthrough*\/NULL);\n+\n+      \/\/ Test if the oopMark is an obvious stack pointer, i.e.,\n+      \/\/  1) (mark & 7) == 0, and\n+      \/\/  2) sp <= mark < mark + os::pagesize()\n+      \/\/\n+      \/\/ These 3 tests can be done by evaluating the following\n+      \/\/ expression: ((mark - sp) & (7 - os::vm_page_size())),\n+      \/\/ assuming both stack pointer and pagesize have their\n+      \/\/ least significant 3 bits clear.\n+      \/\/ NOTE: the oopMark is in swap_reg x10 as the result of cmpxchg\n+      sub(swap_reg, swap_reg, sp);\n+      mv(t0, (int64_t)(7 - (int)os::vm_page_size()));\n+      andr(swap_reg, swap_reg, t0);\n+\n+      \/\/ Save the test result, for recursive case, the result is zero\n+      sd(swap_reg, Address(lock_reg, mark_offset));\n+      beqz(swap_reg, count);\n+    }\n@@ -846,1 +852,1 @@\n-            lock_reg);\n+            UseFastLocking ? obj_reg : lock_reg);\n@@ -884,3 +890,4 @@\n-    \/\/ Convert from BasicObjectLock structure to object and BasicLock\n-    \/\/ structure Store the BasicLock address into x10\n-    la(swap_reg, Address(lock_reg, BasicObjectLock::lock_offset_in_bytes()));\n+    if (UseFastLocking) {\n+      Label slow_case;\n+      \/\/ Load oop into obj_reg(c_rarg3)\n+      ld(obj_reg, Address(lock_reg, BasicObjectLock::obj_offset_in_bytes()));\n@@ -888,2 +895,2 @@\n-    \/\/ Load oop into obj_reg(c_rarg3)\n-    ld(obj_reg, Address(lock_reg, BasicObjectLock::obj_offset_in_bytes()));\n+      \/\/ Free entry\n+      sd(zr, Address(lock_reg, BasicObjectLock::obj_offset_in_bytes()));\n@@ -891,2 +898,4 @@\n-    \/\/ Free entry\n-    sd(zr, Address(lock_reg, BasicObjectLock::obj_offset_in_bytes()));\n+      \/\/ Check for non-symmetric locking. This is allowed by the spec and the interpreter\n+      \/\/ must handle it.\n+      ld(header_reg, Address(xthread, JavaThread::lock_stack_current_offset()));\n+      bne(header_reg, obj_reg, slow_case);\n@@ -894,3 +903,9 @@\n-    \/\/ Load the old header from BasicLock structure\n-    ld(header_reg, Address(swap_reg,\n-                           BasicLock::displaced_header_offset_in_bytes()));\n+      ld(header_reg, Address(obj_reg, oopDesc::mark_offset_in_bytes()));\n+      fast_unlock(obj_reg, header_reg, swap_reg, t0, slow_case);\n+      j(count);\n+\n+      bind(slow_case);\n+    } else {\n+      \/\/ Convert from BasicObjectLock structure to object and BasicLock\n+      \/\/ structure Store the BasicLock address into x10\n+      la(swap_reg, Address(lock_reg, BasicObjectLock::lock_offset_in_bytes()));\n@@ -898,2 +913,2 @@\n-    \/\/ Test for recursion\n-    beqz(header_reg, count);\n+      \/\/ Load oop into obj_reg(c_rarg3)\n+      ld(obj_reg, Address(lock_reg, BasicObjectLock::obj_offset_in_bytes()));\n@@ -901,2 +916,13 @@\n-    \/\/ Atomic swap back the old header\n-    cmpxchg_obj_header(swap_reg, header_reg, obj_reg, t0, count, \/*fallthrough*\/NULL);\n+      \/\/ Free entry\n+      sd(zr, Address(lock_reg, BasicObjectLock::obj_offset_in_bytes()));\n+\n+      \/\/ Load the old header from BasicLock structure\n+      ld(header_reg, Address(swap_reg,\n+                             BasicLock::displaced_header_offset_in_bytes()));\n+\n+      \/\/ Test for recursion\n+      beqz(header_reg, count);\n+\n+      \/\/ Atomic swap back the old header\n+      cmpxchg_obj_header(swap_reg, header_reg, obj_reg, t0, count, \/*fallthrough*\/NULL);\n+    }\n","filename":"src\/hotspot\/cpu\/riscv\/interp_masm_riscv.cpp","additions":69,"deletions":43,"binary":false,"changes":112,"status":"modified"},{"patch":"@@ -4485,0 +4485,55 @@\n+\n+\/\/ Attempt to fast-lock an object. Fall-through on success, branch to slow label\n+\/\/ on failure.\n+\/\/ Registers:\n+\/\/  - obj: the object to be locked\n+\/\/  - hdr: the header, already loaded from obj, will be destroyed\n+\/\/  - tmp1, tmp2: temporary registers, will be destroyed\n+void MacroAssembler::fast_lock(Register obj, Register hdr, Register tmp1, Register tmp2, Label& slow) {\n+  assert(UseFastLocking, \"only used with fast-locking\");\n+  assert_different_registers(obj, hdr, tmp1, tmp2);\n+\n+  \/\/ Check if we would have space on lock-stack for the object.\n+  ld(tmp1, Address(xthread, JavaThread::lock_stack_current_offset()));\n+  ld(tmp2, Address(xthread, JavaThread::lock_stack_limit_offset()));\n+  bge(tmp1, tmp2, slow, true);\n+\n+  \/\/ Load (object->mark() | 1) into hdr\n+  ori(hdr, hdr, markWord::unlocked_value);\n+  \/\/ Clear lock-bits, into tmp2\n+  xori(tmp2, hdr, markWord::unlocked_value);\n+  \/\/ Try to swing header from unlocked to locked\n+  Label success;\n+  cmpxchgptr(hdr, tmp2, obj, tmp1, success, &slow);\n+  bind(success);\n+\n+  \/\/ After successful lock, push object on lock-stack\n+  \/\/ TODO: Can we avoid re-loading the current offset? The CAS above clobbers it.\n+  \/\/ Maybe we could ensure that we have enough space on the lock stack more cleverly.\n+  ld(tmp1, Address(xthread, JavaThread::lock_stack_current_offset()));\n+  sd(obj, Address(tmp1, 0));\n+  add(tmp1, tmp1, oopSize);\n+  sd(tmp1, Address(xthread, JavaThread::lock_stack_current_offset()));\n+}\n+\n+void MacroAssembler::fast_unlock(Register obj, Register hdr, Register tmp1, Register tmp2, Label& slow) {\n+  assert(UseFastLocking, \"only used with fast-locking\");\n+  assert_different_registers(obj, hdr, tmp1, tmp2);\n+\n+  \/\/ Load the expected old header (lock-bits cleared to indicate 'locked') into hdr\n+  mv(tmp1, ~markWord::lock_mask_in_place);\n+  andr(hdr, hdr, tmp1);\n+\n+  \/\/ Load the new header (unlocked) into tmp1\n+  ori(tmp1, hdr, markWord::unlocked_value);\n+\n+  \/\/ Try to swing header from locked to unlocked\n+  Label success;\n+  cmpxchgptr(hdr, tmp1, obj, tmp2, success, &slow);\n+  bind(success);\n+\n+  \/\/ After successful unlock, pop object from lock-stack\n+  ld(tmp1, Address(xthread, JavaThread::lock_stack_current_offset()));\n+  sub(tmp1, tmp1, oopSize);\n+  sd(tmp1, Address(xthread, JavaThread::lock_stack_current_offset()));\n+}\n","filename":"src\/hotspot\/cpu\/riscv\/macroAssembler_riscv.cpp","additions":55,"deletions":0,"binary":false,"changes":55,"status":"modified"},{"patch":"@@ -1386,0 +1386,4 @@\n+\n+public:\n+  void fast_lock(Register obj, Register hdr, Register tmp1, Register tmp2, Label& slow);\n+  void fast_unlock(Register obj, Register hdr, Register tmp1, Register tmp2, Label& slow);\n","filename":"src\/hotspot\/cpu\/riscv\/macroAssembler_riscv.hpp","additions":4,"deletions":0,"binary":false,"changes":4,"status":"modified"},{"patch":"@@ -2420,30 +2420,40 @@\n-      \/\/ Set tmp to be (markWord of object | UNLOCK_VALUE).\n-      __ ori(tmp, disp_hdr, markWord::unlocked_value);\n-\n-      \/\/ Initialize the box. (Must happen before we update the object mark!)\n-      __ sd(tmp, Address(box, BasicLock::displaced_header_offset_in_bytes()));\n-\n-      \/\/ Compare object markWord with an unlocked value (tmp) and if\n-      \/\/ equal exchange the stack address of our box with object markWord.\n-      \/\/ On failure disp_hdr contains the possibly locked markWord.\n-      __ cmpxchg(\/*memory address*\/oop, \/*expected value*\/tmp, \/*new value*\/box, Assembler::int64, Assembler::aq,\n-                 Assembler::rl, \/*result*\/disp_hdr);\n-      __ mv(flag, zr);\n-      __ beq(disp_hdr, tmp, cont); \/\/ prepare zero flag and goto cont if we won the cas\n-\n-      assert(oopDesc::mark_offset_in_bytes() == 0, \"offset of _mark is not 0\");\n-\n-      \/\/ If the compare-and-exchange succeeded, then we found an unlocked\n-      \/\/ object, will have now locked it will continue at label cont\n-      \/\/ We did not see an unlocked object so try the fast recursive case.\n-\n-      \/\/ Check if the owner is self by comparing the value in the\n-      \/\/ markWord of object (disp_hdr) with the stack pointer.\n-      __ sub(disp_hdr, disp_hdr, sp);\n-      __ mv(tmp, (intptr_t) (~(os::vm_page_size()-1) | (uintptr_t)markWord::lock_mask_in_place));\n-      \/\/ If (mark & lock_mask) == 0 and mark - sp < page_size, we are stack-locking and goto cont,\n-      \/\/ hence we can store 0 as the displaced header in the box, which indicates that it is a\n-      \/\/ recursive lock.\n-      __ andr(tmp\/*==0?*\/, disp_hdr, tmp);\n-      __ sd(tmp\/*==0, perhaps*\/, Address(box, BasicLock::displaced_header_offset_in_bytes()));\n-      __ mv(flag, tmp); \/\/ we can use the value of tmp as the result here\n+      if (UseFastLocking) {\n+        Label slow;\n+        __ fast_lock(oop, disp_hdr, tmp, t0, slow);\n+        \/\/ Indicate success at cont.\n+        __ mv(flag, zr);\n+        __ j(cont);\n+        __ bind(slow);\n+        __ mv(flag, 1); \/\/ Set non-zero flag to indicate 'failure' -> take slow-path\n+      } else {\n+        \/\/ Set tmp to be (markWord of object | UNLOCK_VALUE).\n+        __ ori(tmp, disp_hdr, markWord::unlocked_value);\n+\n+        \/\/ Initialize the box. (Must happen before we update the object mark!)\n+        __ sd(tmp, Address(box, BasicLock::displaced_header_offset_in_bytes()));\n+\n+        \/\/ Compare object markWord with an unlocked value (tmp) and if\n+        \/\/ equal exchange the stack address of our box with object markWord.\n+        \/\/ On failure disp_hdr contains the possibly locked markWord.\n+        __ cmpxchg(\/*memory address*\/oop, \/*expected value*\/tmp, \/*new value*\/box, Assembler::int64, Assembler::aq,\n+                   Assembler::rl, \/*result*\/disp_hdr);\n+        __ mv(flag, zr);\n+        __ beq(disp_hdr, tmp, cont); \/\/ prepare zero flag and goto cont if we won the cas\n+\n+        assert(oopDesc::mark_offset_in_bytes() == 0, \"offset of _mark is not 0\");\n+\n+        \/\/ If the compare-and-exchange succeeded, then we found an unlocked\n+        \/\/ object, will have now locked it will continue at label cont\n+        \/\/ We did not see an unlocked object so try the fast recursive case.\n+\n+        \/\/ Check if the owner is self by comparing the value in the\n+        \/\/ markWord of object (disp_hdr) with the stack pointer.\n+        __ sub(disp_hdr, disp_hdr, sp);\n+        __ mv(tmp, (intptr_t) (~(os::vm_page_size()-1) | (uintptr_t)markWord::lock_mask_in_place));\n+        \/\/ If (mark & lock_mask) == 0 and mark - sp < page_size, we are stack-locking and goto cont,\n+        \/\/ hence we can store 0 as the displaced header in the box, which indicates that it is a\n+        \/\/ recursive lock.\n+        __ andr(tmp\/*==0?*\/, disp_hdr, tmp);\n+        __ sd(tmp\/*==0, perhaps*\/, Address(box, BasicLock::displaced_header_offset_in_bytes()));\n+        __ mv(flag, tmp); \/\/ we can use the value of tmp as the result here\n+      }\n@@ -2466,6 +2476,8 @@\n-    \/\/ Store a non-null value into the box to avoid looking like a re-entrant\n-    \/\/ lock. The fast-path monitor unlock code checks for\n-    \/\/ markWord::monitor_value so use markWord::unused_mark which has the\n-    \/\/ relevant bit set, and also matches ObjectSynchronizer::slow_enter.\n-    __ mv(tmp, (address)markWord::unused_mark().value());\n-    __ sd(tmp, Address(box, BasicLock::displaced_header_offset_in_bytes()));\n+    if (!UseFastLocking) {\n+      \/\/ Store a non-null value into the box to avoid looking like a re-entrant\n+      \/\/ lock. The fast-path monitor unlock code checks for\n+      \/\/ markWord::monitor_value so use markWord::unused_mark which has the\n+      \/\/ relevant bit set, and also matches ObjectSynchronizer::slow_enter.\n+      __ mv(tmp, (address)markWord::unused_mark().value());\n+      __ sd(tmp, Address(box, BasicLock::displaced_header_offset_in_bytes()));\n+    }\n@@ -2504,1 +2516,1 @@\n-    if (!UseHeavyMonitors) {\n+    if (!UseHeavyMonitors && !UseFastLocking) {\n@@ -2519,3 +2531,13 @@\n-      \/\/ Check if it is still a light weight lock, this is true if we\n-      \/\/ see the stack address of the basicLock in the markWord of the\n-      \/\/ object.\n+      if (UseFastLocking) {\n+        Label slow;\n+        __ fast_unlock(oop, tmp, box, disp_hdr, slow);\n+\n+        \/\/ Indicate success at cont.\n+        __ mv(flag, zr);\n+        __ j(cont);\n+        __ bind(slow);\n+        __ mv(flag, 1); \/\/ Set non-zero flag to indicate 'failure' -> take slow path\n+      } else {\n+        \/\/ Check if it is still a light weight lock, this is true if we\n+        \/\/ see the stack address of the basicLock in the markWord of the\n+        \/\/ object.\n@@ -2523,3 +2545,4 @@\n-      __ cmpxchg(\/*memory address*\/oop, \/*expected value*\/box, \/*new value*\/disp_hdr, Assembler::int64, Assembler::relaxed,\n-                 Assembler::rl, \/*result*\/tmp);\n-      __ xorr(flag, box, tmp); \/\/ box == tmp if cas succeeds\n+        __ cmpxchg(\/*memory address*\/oop, \/*expected value*\/box, \/*new value*\/disp_hdr, Assembler::int64, Assembler::relaxed,\n+                   Assembler::rl, \/*result*\/tmp);\n+        __ xorr(flag, box, tmp); \/\/ box == tmp if cas succeeds\n+      }\n@@ -2537,0 +2560,11 @@\n+\n+    if (UseFastLocking) {\n+      Label L;\n+      __ ld(disp_hdr, Address(tmp, ObjectMonitor::owner_offset_in_bytes()));\n+      __ mv(t0, (unsigned char)(intptr_t)ANONYMOUS_OWNER);\n+      __ bne(disp_hdr, t0, L);\n+      __ mv(flag, 1); \/\/ Indicate failure at cont -- dive into slow-path.\n+      __ j(cont);\n+      __ bind(L);\n+    }\n+\n","filename":"src\/hotspot\/cpu\/riscv\/riscv.ad","additions":77,"deletions":43,"binary":false,"changes":120,"status":"modified"},{"patch":"@@ -1675,25 +1675,30 @@\n-      \/\/ Load (object->mark() | 1) into swap_reg % x10\n-      __ ld(t0, Address(obj_reg, oopDesc::mark_offset_in_bytes()));\n-      __ ori(swap_reg, t0, 1);\n-\n-      \/\/ Save (object->mark() | 1) into BasicLock's displaced header\n-      __ sd(swap_reg, Address(lock_reg, mark_word_offset));\n-\n-      \/\/ src -> dest if dest == x10 else x10 <- dest\n-      __ cmpxchg_obj_header(x10, lock_reg, obj_reg, t0, count, \/*fallthrough*\/NULL);\n-\n-      \/\/ Test if the oopMark is an obvious stack pointer, i.e.,\n-      \/\/  1) (mark & 3) == 0, and\n-      \/\/  2) sp <= mark < mark + os::pagesize()\n-      \/\/ These 3 tests can be done by evaluating the following\n-      \/\/ expression: ((mark - sp) & (3 - os::vm_page_size())),\n-      \/\/ assuming both stack pointer and pagesize have their\n-      \/\/ least significant 2 bits clear.\n-      \/\/ NOTE: the oopMark is in swap_reg % 10 as the result of cmpxchg\n-\n-      __ sub(swap_reg, swap_reg, sp);\n-      __ andi(swap_reg, swap_reg, 3 - (int)os::vm_page_size());\n-\n-      \/\/ Save the test result, for recursive case, the result is zero\n-      __ sd(swap_reg, Address(lock_reg, mark_word_offset));\n-      __ bnez(swap_reg, slow_path_lock);\n+      if (UseFastLocking) {\n+        __ ld(swap_reg, Address(obj_reg, oopDesc::mark_offset_in_bytes()));\n+        __ fast_lock(obj_reg, swap_reg, tmp, t0, slow_path_lock);\n+      } else {\n+        \/\/ Load (object->mark() | 1) into swap_reg % x10\n+        __ ld(t0, Address(obj_reg, oopDesc::mark_offset_in_bytes()));\n+        __ ori(swap_reg, t0, 1);\n+\n+        \/\/ Save (object->mark() | 1) into BasicLock's displaced header\n+        __ sd(swap_reg, Address(lock_reg, mark_word_offset));\n+\n+        \/\/ src -> dest if dest == x10 else x10 <- dest\n+        __ cmpxchg_obj_header(x10, lock_reg, obj_reg, t0, count, \/*fallthrough*\/NULL);\n+\n+        \/\/ Test if the oopMark is an obvious stack pointer, i.e.,\n+        \/\/  1) (mark & 3) == 0, and\n+        \/\/  2) sp <= mark < mark + os::pagesize()\n+        \/\/ These 3 tests can be done by evaluating the following\n+        \/\/ expression: ((mark - sp) & (3 - os::vm_page_size())),\n+        \/\/ assuming both stack pointer and pagesize have their\n+        \/\/ least significant 2 bits clear.\n+        \/\/ NOTE: the oopMark is in swap_reg % 10 as the result of cmpxchg\n+\n+        __ sub(swap_reg, swap_reg, sp);\n+        __ andi(swap_reg, swap_reg, 3 - (int)os::vm_page_size());\n+\n+        \/\/ Save the test result, for recursive case, the result is zero\n+        __ sd(swap_reg, Address(lock_reg, mark_word_offset));\n+        __ bnez(swap_reg, slow_path_lock);\n+      }\n@@ -1794,1 +1799,1 @@\n-    if (!UseHeavyMonitors) {\n+    if (!UseHeavyMonitors && !UseFastLocking) {\n@@ -1810,9 +1815,14 @@\n-      \/\/ get address of the stack lock\n-      __ la(x10, Address(sp, lock_slot_offset * VMRegImpl::stack_slot_size));\n-      \/\/  get old displaced header\n-      __ ld(old_hdr, Address(x10, 0));\n-\n-      \/\/ Atomic swap old header if oop still contains the stack lock\n-      Label count;\n-      __ cmpxchg_obj_header(x10, old_hdr, obj_reg, t0, count, &slow_path_unlock);\n-      __ bind(count);\n+      if (UseFastLocking) {\n+        __ ld(old_hdr, Address(obj_reg, oopDesc::mark_offset_in_bytes()));\n+        __ fast_unlock(obj_reg, old_hdr, swap_reg, t0, slow_path_unlock);\n+      } else {\n+        \/\/ get address of the stack lock\n+        __ la(x10, Address(sp, lock_slot_offset * VMRegImpl::stack_slot_size));\n+        \/\/  get old displaced header\n+        __ ld(old_hdr, Address(x10, 0));\n+\n+        \/\/ Atomic swap old header if oop still contains the stack lock\n+        Label count;\n+        __ cmpxchg_obj_header(x10, old_hdr, obj_reg, t0, count, &slow_path_unlock);\n+        __ bind(count);\n+      }\n","filename":"src\/hotspot\/cpu\/riscv\/sharedRuntime_riscv.cpp","additions":45,"deletions":35,"binary":false,"changes":80,"status":"modified"},{"patch":"@@ -256,0 +256,5 @@\n+void LoadKlassStub::emit_code(LIR_Assembler* ce) {\n+  \/\/ Currently not needed.\n+  Unimplemented();\n+}\n+\n","filename":"src\/hotspot\/cpu\/s390\/c1_CodeStubs_s390.cpp","additions":5,"deletions":0,"binary":false,"changes":5,"status":"modified"},{"patch":"@@ -115,1 +115,1 @@\n-  __ build_frame(initial_frame_size_in_bytes(), bang_size_in_bytes());\n+  __ build_frame(initial_frame_size_in_bytes(), bang_size_in_bytes(), compilation()->max_monitors());\n","filename":"src\/hotspot\/cpu\/s390\/c1_LIRAssembler_s390.cpp","additions":1,"deletions":1,"binary":false,"changes":2,"status":"modified"},{"patch":"@@ -72,1 +72,1 @@\n-void C1_MacroAssembler::build_frame(int frame_size_in_bytes, int bang_size_in_bytes) {\n+void C1_MacroAssembler::build_frame(int frame_size_in_bytes, int bang_size_in_bytes, int max_monitors) {\n","filename":"src\/hotspot\/cpu\/s390\/c1_MacroAssembler_s390.cpp","additions":1,"deletions":1,"binary":false,"changes":2,"status":"modified"},{"patch":"@@ -0,0 +1,45 @@\n+\/*\n+ * Copyright (c) 2021 SAP SE. All rights reserved.\n+ * Copyright (c) 2021, Red Hat Inc. All rights reserved.\n+ * Copyright (c) 2021, Oracle and\/or its affiliates. All rights reserved.\n+ *\n+ * DO NOT ALTER OR REMOVE COPYRIGHT NOTICES OR THIS FILE HEADER.\n+ *\n+ * This code is free software; you can redistribute it and\/or modify it\n+ * under the terms of the GNU General Public License version 2 only, as\n+ * published by the Free Software Foundation.\n+ *\n+ * This code is distributed in the hope that it will be useful, but WITHOUT\n+ * ANY WARRANTY; without even the implied warranty of MERCHANTABILITY or\n+ * FITNESS FOR A PARTICULAR PURPOSE.  See the GNU General Public License\n+ * version 2 for more details (a copy is included in the LICENSE file that\n+ * accompanied this code).\n+ *\n+ * You should have received a copy of the GNU General Public License version\n+ * 2 along with this work; if not, write to the Free Software Foundation,\n+ * Inc., 51 Franklin St, Fifth Floor, Boston, MA 02110-1301 USA.\n+ *\n+ * Please contact Oracle, 500 Oracle Parkway, Redwood Shores, CA 94065 USA\n+ * or visit www.oracle.com if you need additional information or have any\n+ * questions.\n+ *\n+ *\/\n+\n+#include \"precompiled.hpp\"\n+\n+#include \"oops\/compressedKlass.hpp\"\n+#include \"utilities\/ostream.hpp\"\n+#include \"runtime\/globals.hpp\"\n+\n+\/\/ Given an address p, return true if p can be used as an encoding base.\n+\/\/  (Some platforms have restrictions of what constitutes a valid base address).\n+bool CompressedKlassPointers::is_valid_base(address p) {\n+  return true;\n+}\n+\n+void CompressedKlassPointers::print_mode(outputStream* st) {\n+  st->print_cr(\"Narrow klass base: \" PTR_FORMAT \", Narrow klass shift: %d, \"\n+               \"Narrow klass range: \" UINT64_FORMAT, p2i(base()), shift(),\n+               KlassEncodingMetaspaceMax);\n+}\n+\n","filename":"src\/hotspot\/cpu\/s390\/compressedKlass_s390.cpp","additions":45,"deletions":0,"binary":false,"changes":45,"status":"added"},{"patch":"@@ -38,0 +38,1 @@\n+#include \"oops\/compressedKlass.inline.hpp\"\n","filename":"src\/hotspot\/cpu\/s390\/macroAssembler_s390.cpp","additions":1,"deletions":0,"binary":false,"changes":1,"status":"modified"},{"patch":"@@ -33,0 +33,1 @@\n+#include \"runtime\/objectMonitor.hpp\"\n@@ -281,0 +282,10 @@\n+void LoadKlassStub::emit_code(LIR_Assembler* ce) {\n+  __ bind(_entry);\n+#ifdef _LP64\n+  Register d = _result->as_register();\n+  __ movq(d, Address(d, OM_OFFSET_NO_MONITOR_VALUE_TAG(header)));\n+  __ jmp(_continuation);\n+#else\n+  __ should_not_reach_here();\n+#endif\n+}\n","filename":"src\/hotspot\/cpu\/x86\/c1_CodeStubs_x86.cpp","additions":11,"deletions":0,"binary":false,"changes":11,"status":"modified"},{"patch":"@@ -288,1 +288,1 @@\n-  __ build_frame(initial_frame_size_in_bytes(), bang_size_in_bytes());\n+  __ build_frame(initial_frame_size_in_bytes(), bang_size_in_bytes(), compilation()->max_monitors());\n@@ -1638,1 +1638,1 @@\n-                      arrayOopDesc::header_size(op->type()),\n+                      arrayOopDesc::base_offset_in_bytes(op->type()),\n@@ -3069,0 +3069,1 @@\n+  Register tmp2 = LP64_ONLY(rscratch2) NOT_LP64(noreg);\n@@ -3193,0 +3194,1 @@\n+#ifndef _LP64\n@@ -3195,1 +3197,1 @@\n-\n+#endif\n@@ -3260,7 +3262,8 @@\n-      if (UseCompressedClassPointers) {\n-        __ movl(tmp, src_klass_addr);\n-        __ cmpl(tmp, dst_klass_addr);\n-      } else {\n-        __ movptr(tmp, src_klass_addr);\n-        __ cmpptr(tmp, dst_klass_addr);\n-      }\n+#ifdef _LP64\n+      __ load_nklass(tmp, src);\n+      __ load_nklass(tmp2, dst);\n+      __ cmpl(tmp, tmp2);\n+#else\n+      __ movptr(tmp, src_klass_addr);\n+      __ cmpptr(tmp, dst_klass_addr);\n+#endif\n@@ -3422,5 +3425,2 @@\n-    if (UseCompressedClassPointers) {\n-      __ encode_klass_not_null(tmp, rscratch1);\n-    }\n-#endif\n-\n+    assert(UseCompressedClassPointers, \"Lilliput\");\n+    __ encode_klass_not_null(tmp, rscratch1);\n@@ -3428,3 +3428,12 @@\n-\n-      if (UseCompressedClassPointers)          __ cmpl(tmp, dst_klass_addr);\n-      else                   __ cmpptr(tmp, dst_klass_addr);\n+      __ load_nklass(tmp2, dst);\n+      __ cmpl(tmp, tmp2);\n+      __ jcc(Assembler::notEqual, halt);\n+      __ load_nklass(tmp2, src);\n+      __ cmpl(tmp, tmp2);\n+      __ jcc(Assembler::equal, known_ok);\n+    } else {\n+      __ load_nklass(tmp2, dst);\n+      __ cmpl(tmp, tmp2);\n+#else\n+    if (basic_type != T_OBJECT) {\n+      __ cmpptr(tmp, dst_klass_addr);\n@@ -3432,2 +3441,1 @@\n-      if (UseCompressedClassPointers)          __ cmpl(tmp, src_klass_addr);\n-      else                   __ cmpptr(tmp, src_klass_addr);\n+      __ cmpptr(tmp, src_klass_addr);\n@@ -3436,2 +3444,2 @@\n-      if (UseCompressedClassPointers)          __ cmpl(tmp, dst_klass_addr);\n-      else                   __ cmpptr(tmp, dst_klass_addr);\n+      __ cmpptr(tmp, dst_klass_addr);\n+#endif\n@@ -3507,0 +3515,1 @@\n+    Register tmp = UseFastLocking ? op->scratch_opr()->as_register() : noreg;\n@@ -3508,1 +3517,1 @@\n-    int null_check_offset = __ lock_object(hdr, obj, lock, *op->stub()->entry());\n+    int null_check_offset = __ lock_object(hdr, obj, lock, tmp, *op->stub()->entry());\n@@ -3530,1 +3539,0 @@\n-\n@@ -3532,4 +3540,16 @@\n-  if (UseCompressedClassPointers) {\n-    __ movl(result, Address(obj, oopDesc::klass_offset_in_bytes()));\n-    __ decode_klass_not_null(result, rscratch1);\n-  } else\n+  Register tmp = rscratch1;\n+  assert_different_registers(tmp, obj);\n+  assert_different_registers(tmp, result);\n+\n+  \/\/ Check if we can take the (common) fast path, if obj is unlocked.\n+  __ movq(result, Address(obj, oopDesc::mark_offset_in_bytes()));\n+  __ testb(result, markWord::monitor_value);\n+  __ jcc(Assembler::notZero, *op->stub()->entry());\n+  __ bind(*op->stub()->continuation());\n+  \/\/ Fast-path: shift and decode Klass*.\n+  __ shrq(result, markWord::klass_shift);\n+  __ decode_klass_not_null(result, tmp);\n+#else\n+  __ movptr(result, Address(obj, oopDesc::klass_offset_in_bytes()));\n+  \/\/ Not really needed, but bind the label anyway to make compiler happy.\n+  __ bind(*op->stub()->continuation());\n@@ -3537,1 +3557,0 @@\n-    __ movptr(result, Address(obj, oopDesc::klass_offset_in_bytes()));\n","filename":"src\/hotspot\/cpu\/x86\/c1_LIRAssembler_x86.cpp","additions":48,"deletions":29,"binary":false,"changes":77,"status":"modified"},{"patch":"@@ -322,1 +322,2 @@\n-  monitor_enter(obj.result(), lock, syncTempOpr(), LIR_OprFact::illegalOpr,\n+  LIR_Opr tmp = UseFastLocking ? new_register(T_INT) : LIR_OprFact::illegalOpr;\n+  monitor_enter(obj.result(), lock, syncTempOpr(), tmp,\n","filename":"src\/hotspot\/cpu\/x86\/c1_LIRGenerator_x86.cpp","additions":2,"deletions":1,"binary":false,"changes":3,"status":"modified"},{"patch":"@@ -41,1 +41,1 @@\n-int C1_MacroAssembler::lock_object(Register hdr, Register obj, Register disp_hdr, Label& slow_case) {\n+int C1_MacroAssembler::lock_object(Register hdr, Register obj, Register disp_hdr, Register tmp, Label& slow_case) {\n@@ -45,2 +45,1 @@\n-  assert(hdr != obj && hdr != disp_hdr && obj != disp_hdr, \"registers must be different\");\n-  Label done;\n+  assert_different_registers(hdr, obj, disp_hdr, tmp);\n@@ -65,33 +64,44 @@\n-  \/\/ and mark it as unlocked\n-  orptr(hdr, markWord::unlocked_value);\n-  \/\/ save unlocked object header into the displaced header location on the stack\n-  movptr(Address(disp_hdr, 0), hdr);\n-  \/\/ test if object header is still the same (i.e. unlocked), and if so, store the\n-  \/\/ displaced header address in the object header - if it is not the same, get the\n-  \/\/ object header instead\n-  MacroAssembler::lock(); \/\/ must be immediately before cmpxchg!\n-  cmpxchgptr(disp_hdr, Address(obj, hdr_offset));\n-  \/\/ if the object header was the same, we're done\n-  jcc(Assembler::equal, done);\n-  \/\/ if the object header was not the same, it is now in the hdr register\n-  \/\/ => test if it is a stack pointer into the same stack (recursive locking), i.e.:\n-  \/\/\n-  \/\/ 1) (hdr & aligned_mask) == 0\n-  \/\/ 2) rsp <= hdr\n-  \/\/ 3) hdr <= rsp + page_size\n-  \/\/\n-  \/\/ these 3 tests can be done by evaluating the following expression:\n-  \/\/\n-  \/\/ (hdr - rsp) & (aligned_mask - page_size)\n-  \/\/\n-  \/\/ assuming both the stack pointer and page_size have their least\n-  \/\/ significant 2 bits cleared and page_size is a power of 2\n-  subptr(hdr, rsp);\n-  andptr(hdr, aligned_mask - (int)os::vm_page_size());\n-  \/\/ for recursive locking, the result is zero => save it in the displaced header\n-  \/\/ location (NULL in the displaced hdr location indicates recursive locking)\n-  movptr(Address(disp_hdr, 0), hdr);\n-  \/\/ otherwise we don't care about the result and handle locking via runtime call\n-  jcc(Assembler::notZero, slow_case);\n-  \/\/ done\n-  bind(done);\n+\n+  if (UseFastLocking) {\n+#ifdef _LP64\n+    const Register thread = r15_thread;\n+#else\n+    const Register thread = disp_hdr;\n+    get_thread(thread);\n+#endif\n+    fast_lock_impl(obj, hdr, thread, tmp, slow_case, LP64_ONLY(false) NOT_LP64(true));\n+  } else {\n+    Label done;\n+    orptr(hdr, markWord::unlocked_value);\n+    \/\/ save unlocked object header into the displaced header location on the stack\n+    movptr(Address(disp_hdr, 0), hdr);\n+    \/\/ test if object header is still the same (i.e. unlocked), and if so, store the\n+    \/\/ displaced header address in the object header - if it is not the same, get the\n+    \/\/ object header instead\n+    MacroAssembler::lock(); \/\/ must be immediately before cmpxchg!\n+    cmpxchgptr(disp_hdr, Address(obj, hdr_offset));\n+    \/\/ if the object header was the same, we're done\n+    jcc(Assembler::equal, done);\n+    \/\/ if the object header was not the same, it is now in the hdr register\n+    \/\/ => test if it is a stack pointer into the same stack (recursive locking), i.e.:\n+    \/\/\n+    \/\/ 1) (hdr & aligned_mask) == 0\n+    \/\/ 2) rsp <= hdr\n+    \/\/ 3) hdr <= rsp + page_size\n+    \/\/\n+    \/\/ these 3 tests can be done by evaluating the following expression:\n+    \/\/\n+    \/\/ (hdr - rsp) & (aligned_mask - page_size)\n+    \/\/\n+    \/\/ assuming both the stack pointer and page_size have their least\n+    \/\/ significant 2 bits cleared and page_size is a power of 2\n+    subptr(hdr, rsp);\n+    andptr(hdr, aligned_mask - (int)os::vm_page_size());\n+    \/\/ for recursive locking, the result is zero => save it in the displaced header\n+    \/\/ location (NULL in the displaced hdr location indicates recursive locking)\n+    movptr(Address(disp_hdr, 0), hdr);\n+    \/\/ otherwise we don't care about the result and handle locking via runtime call\n+    jcc(Assembler::notZero, slow_case);\n+    \/\/ done\n+    bind(done);\n+  }\n@@ -111,6 +121,9 @@\n-  \/\/ load displaced header\n-  movptr(hdr, Address(disp_hdr, 0));\n-  \/\/ if the loaded hdr is NULL we had recursive locking\n-  testptr(hdr, hdr);\n-  \/\/ if we had recursive locking, we are done\n-  jcc(Assembler::zero, done);\n+  if (!UseFastLocking) {\n+    \/\/ load displaced header\n+    movptr(hdr, Address(disp_hdr, 0));\n+    \/\/ if the loaded hdr is NULL we had recursive locking\n+    testptr(hdr, hdr);\n+    \/\/ if we had recursive locking, we are done\n+    jcc(Assembler::zero, done);\n+  }\n+\n@@ -119,1 +132,0 @@\n-\n@@ -121,10 +133,0 @@\n-  \/\/ test if object header is pointing to the displaced header, and if so, restore\n-  \/\/ the displaced header in the object - if the object header is not pointing to\n-  \/\/ the displaced header, get the object header instead\n-  MacroAssembler::lock(); \/\/ must be immediately before cmpxchg!\n-  cmpxchgptr(hdr, Address(obj, hdr_offset));\n-  \/\/ if the object header was not pointing to the displaced header,\n-  \/\/ we do unlocking via runtime call\n-  jcc(Assembler::notEqual, slow_case);\n-  \/\/ done\n-  bind(done);\n@@ -132,0 +134,16 @@\n+  if (UseFastLocking) {\n+    movptr(disp_hdr, Address(obj, hdr_offset));\n+    andptr(disp_hdr, ~(int32_t)markWord::lock_mask_in_place);\n+    fast_unlock_impl(obj, disp_hdr, hdr, slow_case);\n+  } else {\n+    \/\/ test if object header is pointing to the displaced header, and if so, restore\n+    \/\/ the displaced header in the object - if the object header is not pointing to\n+    \/\/ the displaced header, get the object header instead\n+    MacroAssembler::lock(); \/\/ must be immediately before cmpxchg!\n+    cmpxchgptr(hdr, Address(obj, hdr_offset));\n+    \/\/ if the object header was not pointing to the displaced header,\n+    \/\/ we do unlocking via runtime call\n+    jcc(Assembler::notEqual, slow_case);\n+    \/\/ done\n+  }\n+  bind(done);\n@@ -147,8 +165,5 @@\n-  assert_different_registers(obj, klass, len);\n-  movptr(Address(obj, oopDesc::mark_offset_in_bytes()), checked_cast<int32_t>(markWord::prototype().value()));\n-#ifdef _LP64\n-  if (UseCompressedClassPointers) { \/\/ Take care not to kill klass\n-    movptr(t1, klass);\n-    encode_klass_not_null(t1, rscratch1);\n-    movl(Address(obj, oopDesc::klass_offset_in_bytes()), t1);\n-  } else\n+  assert_different_registers(obj, klass, len, t1, t2);\n+  movptr(t1, Address(klass, Klass::prototype_header_offset()));\n+  movptr(Address(obj, oopDesc::mark_offset_in_bytes()), t1);\n+#ifndef _LP64\n+  movptr(Address(obj, oopDesc::klass_offset_in_bytes()), klass);\n@@ -156,3 +171,0 @@\n-  {\n-    movptr(Address(obj, oopDesc::klass_offset_in_bytes()), klass);\n-  }\n@@ -163,6 +175,0 @@\n-#ifdef _LP64\n-  else if (UseCompressedClassPointers) {\n-    xorptr(t1, t1);\n-    store_klass_gap(obj, t1);\n-  }\n-#endif\n@@ -245,1 +251,1 @@\n-void C1_MacroAssembler::allocate_array(Register obj, Register len, Register t1, Register t2, int header_size, Address::ScaleFactor f, Register klass, Label& slow_case) {\n+void C1_MacroAssembler::allocate_array(Register obj, Register len, Register t1, Register t2, int base_offset_in_bytes, Address::ScaleFactor f, Register klass, Label& slow_case) {\n@@ -258,1 +264,1 @@\n-  movptr(arr_size, header_size * BytesPerWord + MinObjAlignmentInBytesMask);\n+  movptr(arr_size, (int32_t)base_offset_in_bytes + MinObjAlignmentInBytesMask);\n@@ -268,1 +274,1 @@\n-  initialize_body(obj, arr_size, header_size * BytesPerWord, len_zero);\n+  initialize_body(obj, arr_size, base_offset_in_bytes, len_zero);\n@@ -302,1 +308,1 @@\n-void C1_MacroAssembler::build_frame(int frame_size_in_bytes, int bang_size_in_bytes) {\n+void C1_MacroAssembler::build_frame(int frame_size_in_bytes, int bang_size_in_bytes, int max_monitors) {\n@@ -323,0 +329,13 @@\n+#ifdef _LP64\n+  if (UseFastLocking && max_monitors > 0) {\n+    Label ok;\n+    movptr(rax, Address(r15_thread, JavaThread::lock_stack_current_offset()));\n+    addptr(rax, max_monitors * wordSize);\n+    cmpptr(rax, Address(r15_thread, JavaThread::lock_stack_limit_offset()));\n+    jcc(Assembler::less, ok);\n+    assert(StubRoutines::x86::check_lock_stack() != NULL, \"need runtime call stub\");\n+    call(RuntimeAddress(StubRoutines::x86::check_lock_stack()));\n+    bind(ok);\n+  }\n+#endif\n+\n","filename":"src\/hotspot\/cpu\/x86\/c1_MacroAssembler_x86.cpp","additions":93,"deletions":74,"binary":false,"changes":167,"status":"modified"},{"patch":"@@ -53,1 +53,1 @@\n-  int lock_object  (Register swap, Register obj, Register disp_hdr, Label& slow_case);\n+  int lock_object  (Register swap, Register obj, Register disp_hdr, Register tmp, Label& slow_case);\n","filename":"src\/hotspot\/cpu\/x86\/c1_MacroAssembler_x86.hpp","additions":1,"deletions":1,"binary":false,"changes":2,"status":"modified"},{"patch":"@@ -28,0 +28,1 @@\n+#include \"runtime\/objectMonitor.hpp\"\n@@ -75,0 +76,24 @@\n+int C2CheckLockStackStub::max_size() const {\n+  return 10;\n+}\n+\n+void C2CheckLockStackStub::emit(C2_MacroAssembler& masm) {\n+  __ bind(entry());\n+  assert(StubRoutines::x86::check_lock_stack() != NULL, \"need runtime call stub\");\n+  __ call(RuntimeAddress(StubRoutines::x86::check_lock_stack()));\n+  __ jmp(continuation(), false \/* maybe_short *\/);\n+}\n+\n+#ifdef _LP64\n+int C2LoadNKlassStub::max_size() const {\n+  return 10;\n+}\n+\n+void C2LoadNKlassStub::emit(C2_MacroAssembler& masm) {\n+  __ bind(entry());\n+  Register d = dst();\n+  __ movq(d, Address(d, OM_OFFSET_NO_MONITOR_VALUE_TAG(header)));\n+  __ jmp(continuation());\n+}\n+#endif\n+\n","filename":"src\/hotspot\/cpu\/x86\/c2_CodeStubs_x86.cpp","additions":25,"deletions":0,"binary":false,"changes":25,"status":"modified"},{"patch":"@@ -48,1 +48,1 @@\n-void C2_MacroAssembler::verified_entry(int framesize, int stack_bang_size, bool fp_mode_24b, bool is_stub) {\n+void C2_MacroAssembler::verified_entry(int framesize, int stack_bang_size, bool fp_mode_24b, bool is_stub, int max_monitors) {\n@@ -130,0 +130,14 @@\n+#ifdef _LP64\n+  if (UseFastLocking && max_monitors > 0) {\n+    C2CheckLockStackStub* stub = new (Compile::current()->comp_arena()) C2CheckLockStackStub();\n+    Compile::current()->output()->add_stub(stub);\n+    assert(!is_stub, \"only methods have monitors\");\n+    Register thread = r15_thread;\n+    movptr(rax, Address(thread, JavaThread::lock_stack_current_offset()));\n+    addptr(rax, max_monitors * oopSize);\n+    cmpptr(rax, Address(thread, JavaThread::lock_stack_limit_offset()));\n+    jcc(Assembler::greaterEqual, stub->entry());\n+    bind(stub->continuation());\n+  }\n+#endif\n+\n@@ -551,1 +565,1 @@\n-                                 Register scrReg, Register cx1Reg, Register cx2Reg,\n+                                 Register scrReg, Register cx1Reg, Register cx2Reg, Register thread,\n@@ -605,14 +619,32 @@\n-    \/\/ Attempt stack-locking ...\n-    orptr (tmpReg, markWord::unlocked_value);\n-    movptr(Address(boxReg, 0), tmpReg);          \/\/ Anticipate successful CAS\n-    lock();\n-    cmpxchgptr(boxReg, Address(objReg, oopDesc::mark_offset_in_bytes()));      \/\/ Updates tmpReg\n-    jcc(Assembler::equal, COUNT);           \/\/ Success\n-\n-    \/\/ Recursive locking.\n-    \/\/ The object is stack-locked: markword contains stack pointer to BasicLock.\n-    \/\/ Locked by current thread if difference with current SP is less than one page.\n-    subptr(tmpReg, rsp);\n-    \/\/ Next instruction set ZFlag == 1 (Success) if difference is less then one page.\n-    andptr(tmpReg, (int32_t) (NOT_LP64(0xFFFFF003) LP64_ONLY(7 - (int)os::vm_page_size())) );\n-    movptr(Address(boxReg, 0), tmpReg);\n+    if (UseFastLocking) {\n+#ifdef _LP64\n+      fast_lock_impl(objReg, tmpReg, thread, scrReg, NO_COUNT, false);\n+      jmp(COUNT);\n+#else\n+      \/\/ We can not emit the lock-stack-check in verified_entry() because we don't have enough\n+      \/\/ registers (for thread ptr). Therefore we have to emit the lock-stack-check in\n+      \/\/ fast_lock_impl(). However, that check can take a slow-path with ZF=1, therefore\n+      \/\/ we need to handle it specially and force ZF=0 before taking the actual slow-path.\n+      Label slow;\n+      fast_lock_impl(objReg, tmpReg, thread, scrReg, slow);\n+      jmp(COUNT);\n+      bind(slow);\n+      testptr(objReg, objReg); \/\/ ZF=0 to indicate failure\n+      jmp(NO_COUNT);\n+#endif\n+    } else {\n+      \/\/ Attempt stack-locking ...\n+      orptr (tmpReg, markWord::unlocked_value);\n+      movptr(Address(boxReg, 0), tmpReg);          \/\/ Anticipate successful CAS\n+      lock();\n+      cmpxchgptr(boxReg, Address(objReg, oopDesc::mark_offset_in_bytes()));      \/\/ Updates tmpReg\n+      jcc(Assembler::equal, COUNT);           \/\/ Success\n+\n+      \/\/ Recursive locking.\n+      \/\/ The object is stack-locked: markword contains stack pointer to BasicLock.\n+      \/\/ Locked by current thread if difference with current SP is less than one page.\n+      subptr(tmpReg, rsp);\n+      \/\/ Next instruction set ZFlag == 1 (Success) if difference is less then one page.\n+      andptr(tmpReg, (int32_t) (NOT_LP64(0xFFFFF003) LP64_ONLY(7 - (int)os::vm_page_size())) );\n+      movptr(Address(boxReg, 0), tmpReg);\n+    }\n@@ -662,1 +694,1 @@\n-  cmpxchgptr(scrReg, Address(boxReg, OM_OFFSET_NO_MONITOR_VALUE_TAG(owner)));\n+  cmpxchgptr(thread, Address(boxReg, OM_OFFSET_NO_MONITOR_VALUE_TAG(owner)));\n@@ -664,7 +696,0 @@\n-  \/\/ If we weren't able to swing _owner from NULL to the BasicLock\n-  \/\/ then take the slow path.\n-  jccb  (Assembler::notZero, NO_COUNT);\n-  \/\/ update _owner from BasicLock to thread\n-  get_thread (scrReg);                    \/\/ beware: clobbers ICCs\n-  movptr(Address(boxReg, OM_OFFSET_NO_MONITOR_VALUE_TAG(owner)), scrReg);\n-  xorptr(boxReg, boxReg);                 \/\/ set icc.ZFlag = 1 to indicate success\n@@ -776,1 +801,1 @@\n-  if (!UseHeavyMonitors) {\n+  if (!UseHeavyMonitors && !UseFastLocking) {\n@@ -783,1 +808,11 @@\n-    jccb   (Assembler::zero, Stacked);\n+#if INCLUDE_RTM_OPT\n+    if (UseFastLocking && use_rtm) {\n+      jcc(Assembler::zero, Stacked);\n+    } else\n+#endif\n+    jccb(Assembler::zero, Stacked);\n+    if (UseFastLocking) {\n+      \/\/ If the owner is ANONYMOUS, we need to fix it - in the slow-path.\n+      testb(Address(tmpReg, OM_OFFSET_NO_MONITOR_VALUE_TAG(owner)), (int32_t) (intptr_t) ANONYMOUS_OWNER);\n+      jcc(Assembler::notEqual, NO_COUNT);\n+    }\n@@ -795,1 +830,1 @@\n-    jmpb(DONE_LABEL);\n+    jmp(DONE_LABEL);\n@@ -909,3 +944,9 @@\n-    movptr(tmpReg, Address (boxReg, 0));      \/\/ re-fetch\n-    lock();\n-    cmpxchgptr(tmpReg, Address(objReg, oopDesc::mark_offset_in_bytes())); \/\/ Uses RAX which is box\n+    if (UseFastLocking) {\n+      mov(boxReg, tmpReg);\n+      fast_unlock_impl(objReg, boxReg, tmpReg, NO_COUNT);\n+      jmp(COUNT);\n+    } else {\n+      movptr(tmpReg, Address (boxReg, 0));      \/\/ re-fetch\n+      lock();\n+      cmpxchgptr(tmpReg, Address(objReg, oopDesc::mark_offset_in_bytes())); \/\/ Uses RAX which is box\n+    }\n","filename":"src\/hotspot\/cpu\/x86\/c2_MacroAssembler_x86.cpp","additions":71,"deletions":30,"binary":false,"changes":101,"status":"modified"},{"patch":"@@ -32,1 +32,1 @@\n-  void verified_entry(int framesize, int stack_bang_size, bool fp_mode_24b, bool is_stub);\n+  void verified_entry(int framesize, int stack_bang_size, bool fp_mode_24b, bool is_stub, int max_monitors);\n@@ -39,1 +39,1 @@\n-                 Register scr, Register cx1, Register cx2,\n+                 Register scr, Register cx1, Register cx2, Register thread,\n","filename":"src\/hotspot\/cpu\/x86\/c2_MacroAssembler_x86.hpp","additions":2,"deletions":2,"binary":false,"changes":4,"status":"modified"},{"patch":"@@ -0,0 +1,52 @@\n+\/*\n+ * Copyright (c) 2021 SAP SE. All rights reserved.\n+ * Copyright (c) 2021, Red Hat Inc. All rights reserved.\n+ * Copyright (c) 2021, Oracle and\/or its affiliates. All rights reserved.\n+ *\n+ * DO NOT ALTER OR REMOVE COPYRIGHT NOTICES OR THIS FILE HEADER.\n+ *\n+ * This code is free software; you can redistribute it and\/or modify it\n+ * under the terms of the GNU General Public License version 2 only, as\n+ * published by the Free Software Foundation.\n+ *\n+ * This code is distributed in the hope that it will be useful, but WITHOUT\n+ * ANY WARRANTY; without even the implied warranty of MERCHANTABILITY or\n+ * FITNESS FOR A PARTICULAR PURPOSE.  See the GNU General Public License\n+ * version 2 for more details (a copy is included in the LICENSE file that\n+ * accompanied this code).\n+ *\n+ * You should have received a copy of the GNU General Public License version\n+ * 2 along with this work; if not, write to the Free Software Foundation,\n+ * Inc., 51 Franklin St, Fifth Floor, Boston, MA 02110-1301 USA.\n+ *\n+ * Please contact Oracle, 500 Oracle Parkway, Redwood Shores, CA 94065 USA\n+ * or visit www.oracle.com if you need additional information or have any\n+ * questions.\n+ *\n+ *\/\n+\n+#include \"precompiled.hpp\"\n+\n+#include \"macroAssembler_x86.hpp\"\n+#include \"oops\/compressedKlass.hpp\"\n+#include \"utilities\/ostream.hpp\"\n+#include \"utilities\/debug.hpp\"\n+#include \"runtime\/globals.hpp\"\n+\n+#ifdef _LP64\n+\n+\/\/ Given an address p, return true if p can be used as an encoding base.\n+\/\/  (Some platforms have restrictions of what constitutes a valid base address).\n+bool CompressedKlassPointers::is_valid_base(address p) {\n+  return MacroAssembler::klass_decode_mode_for_base(p) != MacroAssembler::KlassDecodeNone;\n+}\n+\n+void CompressedKlassPointers::print_mode(outputStream* st) {\n+  st->print_cr(\"Narrow klass base: \" PTR_FORMAT \", Narrow klass shift: %d, \"\n+               \"Narrow klass range: \" UINT64_FORMAT_X\n+               \", Encoding mode %s\",\n+               p2i(base()), shift(), KlassEncodingMetaspaceMax,\n+               MacroAssembler::describe_klass_decode_mode(MacroAssembler::klass_decode_mode()));\n+}\n+\n+#endif \/\/ _LP64\n","filename":"src\/hotspot\/cpu\/x86\/compressedKlass_x86.cpp","additions":52,"deletions":0,"binary":false,"changes":52,"status":"added"},{"patch":"@@ -1226,53 +1226,65 @@\n-    \/\/ Load immediate 1 into swap_reg %rax\n-    movl(swap_reg, 1);\n-\n-    \/\/ Load (object->mark() | 1) into swap_reg %rax\n-    orptr(swap_reg, Address(obj_reg, oopDesc::mark_offset_in_bytes()));\n-\n-    \/\/ Save (object->mark() | 1) into BasicLock's displaced header\n-    movptr(Address(lock_reg, mark_offset), swap_reg);\n-\n-    assert(lock_offset == 0,\n-           \"displaced header must be first word in BasicObjectLock\");\n-\n-    lock();\n-    cmpxchgptr(lock_reg, Address(obj_reg, oopDesc::mark_offset_in_bytes()));\n-    jcc(Assembler::zero, count_locking);\n-\n-    const int zero_bits = LP64_ONLY(7) NOT_LP64(3);\n-\n-    \/\/ Fast check for recursive lock.\n-    \/\/\n-    \/\/ Can apply the optimization only if this is a stack lock\n-    \/\/ allocated in this thread. For efficiency, we can focus on\n-    \/\/ recently allocated stack locks (instead of reading the stack\n-    \/\/ base and checking whether 'mark' points inside the current\n-    \/\/ thread stack):\n-    \/\/  1) (mark & zero_bits) == 0, and\n-    \/\/  2) rsp <= mark < mark + os::pagesize()\n-    \/\/\n-    \/\/ Warning: rsp + os::pagesize can overflow the stack base. We must\n-    \/\/ neither apply the optimization for an inflated lock allocated\n-    \/\/ just above the thread stack (this is why condition 1 matters)\n-    \/\/ nor apply the optimization if the stack lock is inside the stack\n-    \/\/ of another thread. The latter is avoided even in case of overflow\n-    \/\/ because we have guard pages at the end of all stacks. Hence, if\n-    \/\/ we go over the stack base and hit the stack of another thread,\n-    \/\/ this should not be in a writeable area that could contain a\n-    \/\/ stack lock allocated by that thread. As a consequence, a stack\n-    \/\/ lock less than page size away from rsp is guaranteed to be\n-    \/\/ owned by the current thread.\n-    \/\/\n-    \/\/ These 3 tests can be done by evaluating the following\n-    \/\/ expression: ((mark - rsp) & (zero_bits - os::vm_page_size())),\n-    \/\/ assuming both stack pointer and pagesize have their\n-    \/\/ least significant bits clear.\n-    \/\/ NOTE: the mark is in swap_reg %rax as the result of cmpxchg\n-    subptr(swap_reg, rsp);\n-    andptr(swap_reg, zero_bits - (int)os::vm_page_size());\n-\n-    \/\/ Save the test result, for recursive case, the result is zero\n-    movptr(Address(lock_reg, mark_offset), swap_reg);\n-    jcc(Assembler::notZero, slow_case);\n-\n-    bind(count_locking);\n+    if (UseFastLocking) {\n+#ifdef _LP64\n+      const Register thread = r15_thread;\n+#else\n+      const Register thread = lock_reg;\n+      get_thread(thread);\n+#endif\n+      \/\/ Load object header, prepare for CAS from unlocked to locked.\n+      movptr(swap_reg, Address(obj_reg, oopDesc::mark_offset_in_bytes()));\n+      fast_lock_impl(obj_reg, swap_reg, thread, tmp_reg, slow_case);\n+    } else {\n+      \/\/ Load immediate 1 into swap_reg %rax\n+      movl(swap_reg, 1);\n+\n+      \/\/ Load (object->mark() | 1) into swap_reg %rax\n+      orptr(swap_reg, Address(obj_reg, oopDesc::mark_offset_in_bytes()));\n+\n+      \/\/ Save (object->mark() | 1) into BasicLock's displaced header\n+      movptr(Address(lock_reg, mark_offset), swap_reg);\n+\n+      assert(lock_offset == 0,\n+             \"displaced header must be first word in BasicObjectLock\");\n+\n+      lock();\n+      cmpxchgptr(lock_reg, Address(obj_reg, oopDesc::mark_offset_in_bytes()));\n+      jcc(Assembler::zero, count_locking);\n+\n+      const int zero_bits = LP64_ONLY(7) NOT_LP64(3);\n+\n+      \/\/ Fast check for recursive lock.\n+      \/\/\n+      \/\/ Can apply the optimization only if this is a stack lock\n+      \/\/ allocated in this thread. For efficiency, we can focus on\n+      \/\/ recently allocated stack locks (instead of reading the stack\n+      \/\/ base and checking whether 'mark' points inside the current\n+      \/\/ thread stack):\n+      \/\/  1) (mark & zero_bits) == 0, and\n+      \/\/  2) rsp <= mark < mark + os::pagesize()\n+      \/\/\n+      \/\/ Warning: rsp + os::pagesize can overflow the stack base. We must\n+      \/\/ neither apply the optimization for an inflated lock allocated\n+      \/\/ just above the thread stack (this is why condition 1 matters)\n+      \/\/ nor apply the optimization if the stack lock is inside the stack\n+      \/\/ of another thread. The latter is avoided even in case of overflow\n+      \/\/ because we have guard pages at the end of all stacks. Hence, if\n+      \/\/ we go over the stack base and hit the stack of another thread,\n+      \/\/ this should not be in a writeable area that could contain a\n+      \/\/ stack lock allocated by that thread. As a consequence, a stack\n+      \/\/ lock less than page size away from rsp is guaranteed to be\n+      \/\/ owned by the current thread.\n+      \/\/\n+      \/\/ These 3 tests can be done by evaluating the following\n+      \/\/ expression: ((mark - rsp) & (zero_bits - os::vm_page_size())),\n+      \/\/ assuming both stack pointer and pagesize have their\n+      \/\/ least significant bits clear.\n+      \/\/ NOTE: the mark is in swap_reg %rax as the result of cmpxchg\n+      subptr(swap_reg, rsp);\n+      andptr(swap_reg, zero_bits - (int)os::vm_page_size());\n+\n+      \/\/ Save the test result, for recursive case, the result is zero\n+      movptr(Address(lock_reg, mark_offset), swap_reg);\n+      jcc(Assembler::notZero, slow_case);\n+\n+      bind(count_locking);\n+    }\n@@ -1287,1 +1299,1 @@\n-            lock_reg);\n+            UseFastLocking ? obj_reg : lock_reg);\n@@ -1321,3 +1333,5 @@\n-    \/\/ Convert from BasicObjectLock structure to object and BasicLock\n-    \/\/ structure Store the BasicLock address into %rax\n-    lea(swap_reg, Address(lock_reg, BasicObjectLock::lock_offset_in_bytes()));\n+    if (!UseFastLocking) {\n+      \/\/ Convert from BasicObjectLock structure to object and BasicLock\n+      \/\/ structure Store the BasicLock address into %rax\n+      lea(swap_reg, Address(lock_reg, BasicObjectLock::lock_offset_in_bytes()));\n+    }\n@@ -1331,3 +1345,18 @@\n-    \/\/ Load the old header from BasicLock structure\n-    movptr(header_reg, Address(swap_reg,\n-                               BasicLock::displaced_header_offset_in_bytes()));\n+    if (UseFastLocking) {\n+#ifdef _LP64\n+      const Register thread = r15_thread;\n+#else\n+      const Register thread = header_reg;\n+      get_thread(thread);\n+#endif\n+      \/\/ Handle unstructured locking.\n+      cmpptr(obj_reg, Address(thread, JavaThread::lock_stack_current_offset()));\n+      jcc(Assembler::notEqual, slow_case);\n+      \/\/ Try to swing header from locked to unlock.\n+      movptr(swap_reg, Address(obj_reg, oopDesc::mark_offset_in_bytes()));\n+      andptr(swap_reg, ~(int32_t)markWord::lock_mask_in_place);\n+      fast_unlock_impl(obj_reg, swap_reg, header_reg, slow_case);\n+    } else {\n+      \/\/ Load the old header from BasicLock structure\n+      movptr(header_reg, Address(swap_reg,\n+                                 BasicLock::displaced_header_offset_in_bytes()));\n@@ -1335,2 +1364,2 @@\n-    \/\/ Test for recursion\n-    testptr(header_reg, header_reg);\n+      \/\/ Test for recursion\n+      testptr(header_reg, header_reg);\n@@ -1338,2 +1367,2 @@\n-    \/\/ zero for recursive case\n-    jcc(Assembler::zero, count_locking);\n+      \/\/ zero for recursive case\n+      jcc(Assembler::zero, count_locking);\n@@ -1341,3 +1370,3 @@\n-    \/\/ Atomic swap back the old header\n-    lock();\n-    cmpxchgptr(header_reg, Address(obj_reg, oopDesc::mark_offset_in_bytes()));\n+      \/\/ Atomic swap back the old header\n+      lock();\n+      cmpxchgptr(header_reg, Address(obj_reg, oopDesc::mark_offset_in_bytes()));\n@@ -1345,2 +1374,2 @@\n-    \/\/ zero for simple unlock of a stack-lock case\n-    jcc(Assembler::notZero, slow_case);\n+      \/\/ zero for simple unlock of a stack-lock case\n+      jcc(Assembler::notZero, slow_case);\n@@ -1348,1 +1377,2 @@\n-    bind(count_locking);\n+      bind(count_locking);\n+    }\n","filename":"src\/hotspot\/cpu\/x86\/interp_masm_x86.cpp","additions":100,"deletions":70,"binary":false,"changes":170,"status":"modified"},{"patch":"@@ -41,0 +41,1 @@\n+#include \"oops\/compressedKlass.inline.hpp\"\n@@ -4173,1 +4174,1 @@\n-  assert((offset_in_bytes & (BytesPerWord - 1)) == 0, \"offset must be a multiple of BytesPerWord\");\n+  assert((offset_in_bytes & (BytesPerInt - 1)) == 0, \"offset must be a multiple of BytesPerInt\");\n@@ -4179,0 +4180,13 @@\n+  \/\/ Emit single 32bit store to clear leading bytes, if necessary.\n+  xorptr(temp, temp);    \/\/ use _zero reg to clear memory (shorter code)\n+#ifdef _LP64\n+  if (!is_aligned(offset_in_bytes, BytesPerWord)) {\n+    movl(Address(address, offset_in_bytes), temp);\n+    offset_in_bytes += BytesPerInt;\n+    decrement(length_in_bytes, BytesPerInt);\n+  }\n+  assert((offset_in_bytes & (BytesPerWord - 1)) == 0, \"offset must be a multiple of BytesPerWord\");\n+  testptr(length_in_bytes, length_in_bytes);\n+  jcc(Assembler::zero, done);\n+#endif\n+\n@@ -4191,1 +4205,0 @@\n-  xorptr(temp, temp);    \/\/ use _zero reg to clear memory (shorter code)\n@@ -5123,0 +5136,17 @@\n+#ifdef _LP64\n+void MacroAssembler::load_nklass(Register dst, Register src) {\n+  assert(UseCompressedClassPointers, \"expect compressed class pointers\");\n+\n+  Label fast;\n+  movq(dst, Address(src, oopDesc::mark_offset_in_bytes()));\n+  testb(dst, markWord::monitor_value);\n+  jccb(Assembler::zero, fast);\n+\n+  \/\/ Fetch displaced header\n+  movq(dst, Address(dst, OM_OFFSET_NO_MONITOR_VALUE_TAG(header)));\n+\n+  bind(fast);\n+  shrq(dst, markWord::klass_shift);\n+}\n+#endif\n+\n@@ -5127,4 +5157,5 @@\n-  if (UseCompressedClassPointers) {\n-    movl(dst, Address(src, oopDesc::klass_offset_in_bytes()));\n-    decode_klass_not_null(dst, tmp);\n-  } else\n+  assert(UseCompressedClassPointers, \"expect compressed class pointers\");\n+  load_nklass(dst, src);\n+  decode_klass_not_null(dst, tmp);\n+#else\n+  movptr(dst, Address(src, oopDesc::klass_offset_in_bytes()));\n@@ -5132,1 +5163,0 @@\n-    movptr(dst, Address(src, oopDesc::klass_offset_in_bytes()));\n@@ -5136,0 +5166,3 @@\n+#ifdef _LP64\n+  null_check(src, oopDesc::mark_offset_in_bytes());\n+#else\n@@ -5137,0 +5170,1 @@\n+#endif\n@@ -5140,10 +5174,3 @@\n-void MacroAssembler::store_klass(Register dst, Register src, Register tmp) {\n-  assert_different_registers(src, tmp);\n-  assert_different_registers(dst, tmp);\n-#ifdef _LP64\n-  if (UseCompressedClassPointers) {\n-    encode_klass_not_null(src, tmp);\n-    movl(Address(dst, oopDesc::klass_offset_in_bytes()), src);\n-  } else\n-#endif\n-    movptr(Address(dst, oopDesc::klass_offset_in_bytes()), src);\n+#ifndef _LP64\n+void MacroAssembler::store_klass(Register dst, Register src) {\n+  movptr(Address(dst, oopDesc::klass_offset_in_bytes()), src);\n@@ -5151,0 +5178,1 @@\n+#endif\n@@ -5198,7 +5226,0 @@\n-void MacroAssembler::store_klass_gap(Register dst, Register src) {\n-  if (UseCompressedClassPointers) {\n-    \/\/ Store to klass gap in destination\n-    movl(Address(dst, oopDesc::klass_gap_offset_in_bytes()), src);\n-  }\n-}\n-\n@@ -5356,0 +5377,62 @@\n+MacroAssembler::KlassDecodeMode MacroAssembler::_klass_decode_mode = KlassDecodeNone;\n+\n+\/\/ Returns a static string\n+const char* MacroAssembler::describe_klass_decode_mode(MacroAssembler::KlassDecodeMode mode) {\n+  switch (mode) {\n+  case KlassDecodeNone: return \"none\";\n+  case KlassDecodeZero: return \"zero\";\n+  case KlassDecodeXor:  return \"xor\";\n+  case KlassDecodeAdd:  return \"add\";\n+  default:\n+    ShouldNotReachHere();\n+  }\n+  return NULL;\n+}\n+\n+\/\/ Return the current narrow Klass pointer decode mode.\n+MacroAssembler::KlassDecodeMode MacroAssembler::klass_decode_mode() {\n+  if (_klass_decode_mode == KlassDecodeNone) {\n+    \/\/ First time initialization\n+    assert(UseCompressedClassPointers, \"not using compressed class pointers\");\n+    assert(Metaspace::initialized(), \"metaspace not initialized yet\");\n+\n+    _klass_decode_mode = klass_decode_mode_for_base(CompressedKlassPointers::base());\n+    guarantee(_klass_decode_mode != KlassDecodeNone,\n+              PTR_FORMAT \" is not a valid encoding base on aarch64\",\n+              p2i(CompressedKlassPointers::base()));\n+    log_info(metaspace)(\"klass decode mode initialized: %s\", describe_klass_decode_mode(_klass_decode_mode));\n+  }\n+  return _klass_decode_mode;\n+}\n+\n+\/\/ Given an arbitrary base address, return the KlassDecodeMode that would be used. Return KlassDecodeNone\n+\/\/ if base address is not valid for encoding.\n+MacroAssembler::KlassDecodeMode MacroAssembler::klass_decode_mode_for_base(address base) {\n+  assert(CompressedKlassPointers::shift() != 0, \"not lilliput?\");\n+\n+  const uint64_t base_u64 = (uint64_t) base;\n+\n+  if (base_u64 == 0) {\n+    return KlassDecodeZero;\n+  }\n+\n+  if ((base_u64 & (KlassEncodingMetaspaceMax - 1)) == 0) {\n+    return KlassDecodeXor;\n+  }\n+\n+  \/\/ Note that there is no point in optimizing for shift=3 since lilliput\n+  \/\/ will use larger shifts\n+\n+  \/\/ The add+shift mode for decode_and_move_klass_not_null() requires the base to be\n+  \/\/  shiftable-without-loss. So, this is the minimum restriction on x64 for a valid\n+  \/\/  encoding base. This does not matter in reality since the shift values we use for\n+  \/\/  Lilliput, while large, won't be larger than a page size. And the encoding base\n+  \/\/  will be quite likely page aligned since it usually falls to the beginning of\n+  \/\/  either CDS or CCS.\n+  if ((base_u64 & (KlassAlignmentInBytes - 1)) == 0) {\n+    return KlassDecodeAdd;\n+  }\n+\n+  return KlassDecodeNone;\n+}\n+\n@@ -5358,1 +5441,12 @@\n-  if (CompressedKlassPointers::base() != NULL) {\n+  switch (klass_decode_mode()) {\n+  case KlassDecodeZero: {\n+    shrq(r, CompressedKlassPointers::shift());\n+    break;\n+  }\n+  case KlassDecodeXor: {\n+    mov64(tmp, (int64_t)CompressedKlassPointers::base());\n+    xorq(r, tmp);\n+    shrq(r, CompressedKlassPointers::shift());\n+    break;\n+  }\n+  case KlassDecodeAdd: {\n@@ -5361,0 +5455,2 @@\n+    shrq(r, CompressedKlassPointers::shift());\n+    break;\n@@ -5362,3 +5458,2 @@\n-  if (CompressedKlassPointers::shift() != 0) {\n-    assert (LogKlassAlignmentInBytes == CompressedKlassPointers::shift(), \"decode alg wrong\");\n-    shrq(r, LogKlassAlignmentInBytes);\n+  default:\n+    ShouldNotReachHere();\n@@ -5370,1 +5465,13 @@\n-  if (CompressedKlassPointers::base() != NULL) {\n+  switch (klass_decode_mode()) {\n+  case KlassDecodeZero: {\n+    movptr(dst, src);\n+    shrq(dst, CompressedKlassPointers::shift());\n+    break;\n+  }\n+  case KlassDecodeXor: {\n+    mov64(dst, (int64_t)CompressedKlassPointers::base());\n+    xorq(dst, src);\n+    shrq(dst, CompressedKlassPointers::shift());\n+    break;\n+  }\n+  case KlassDecodeAdd: {\n@@ -5373,2 +5480,2 @@\n-  } else {\n-    movptr(dst, src);\n+    shrq(dst, CompressedKlassPointers::shift());\n+    break;\n@@ -5376,3 +5483,2 @@\n-  if (CompressedKlassPointers::shift() != 0) {\n-    assert (LogKlassAlignmentInBytes == CompressedKlassPointers::shift(), \"decode alg wrong\");\n-    shrq(dst, LogKlassAlignmentInBytes);\n+  default:\n+    ShouldNotReachHere();\n@@ -5384,8 +5490,5 @@\n-  \/\/ Note: it will change flags\n-  assert(UseCompressedClassPointers, \"should only be used for compressed headers\");\n-  \/\/ Cannot assert, unverified entry point counts instructions (see .ad file)\n-  \/\/ vtableStubs also counts instructions in pd_code_size_limit.\n-  \/\/ Also do not verify_oop as this is called by verify_oop.\n-  if (CompressedKlassPointers::shift() != 0) {\n-    assert(LogKlassAlignmentInBytes == CompressedKlassPointers::shift(), \"decode alg wrong\");\n-    shlq(r, LogKlassAlignmentInBytes);\n+  const uint64_t base_u64 = (uint64_t)CompressedKlassPointers::base();\n+  switch (klass_decode_mode()) {\n+  case KlassDecodeZero: {\n+    shlq(r, CompressedKlassPointers::shift());\n+    break;\n@@ -5393,2 +5496,11 @@\n-  if (CompressedKlassPointers::base() != NULL) {\n-    mov64(tmp, (int64_t)CompressedKlassPointers::base());\n+  case KlassDecodeXor: {\n+    assert((base_u64 & (KlassEncodingMetaspaceMax - 1)) == 0,\n+           \"base \" UINT64_FORMAT_X \" invalid for xor mode\", base_u64); \/\/ should have been handled at VM init.\n+    shlq(r, CompressedKlassPointers::shift());\n+    mov64(tmp, base_u64);\n+    xorq(r, tmp);\n+    break;\n+  }\n+  case KlassDecodeAdd: {\n+    shlq(r, CompressedKlassPointers::shift());\n+    mov64(tmp, base_u64);\n@@ -5396,0 +5508,4 @@\n+    break;\n+  }\n+  default:\n+    ShouldNotReachHere();\n@@ -5401,3 +5517,1 @@\n-  \/\/ Note: it will change flags\n-  assert (UseCompressedClassPointers, \"should only be used for compressed headers\");\n-  \/\/ Cannot assert, unverified entry point counts instructions (see .ad file)\n+  \/\/ Note: Cannot assert, unverified entry point counts instructions (see .ad file)\n@@ -5407,18 +5521,28 @@\n-  if (CompressedKlassPointers::base() == NULL &&\n-      CompressedKlassPointers::shift() == 0) {\n-    \/\/ The best case scenario is that there is no base or shift. Then it is already\n-    \/\/ a pointer that needs nothing but a register rename.\n-    movl(dst, src);\n-  } else {\n-    if (CompressedKlassPointers::base() != NULL) {\n-      mov64(dst, (int64_t)CompressedKlassPointers::base());\n-    } else {\n-      xorq(dst, dst);\n-    }\n-    if (CompressedKlassPointers::shift() != 0) {\n-      assert(LogKlassAlignmentInBytes == CompressedKlassPointers::shift(), \"decode alg wrong\");\n-      assert(LogKlassAlignmentInBytes == Address::times_8, \"klass not aligned on 64bits?\");\n-      leaq(dst, Address(dst, src, Address::times_8, 0));\n-    } else {\n-      addq(dst, src);\n-    }\n+  const uint64_t base_u64 = (uint64_t)CompressedKlassPointers::base();\n+\n+  switch (klass_decode_mode()) {\n+  case KlassDecodeZero: {\n+    movq(dst, src);\n+    shlq(dst, CompressedKlassPointers::shift());\n+    break;\n+  }\n+  case KlassDecodeXor: {\n+    assert((base_u64 & (KlassEncodingMetaspaceMax - 1)) == 0,\n+           \"base \" UINT64_FORMAT_X \" invalid for xor mode\", base_u64); \/\/ should have been handled at VM init.\n+    const uint64_t base_right_shifted = base_u64 >> CompressedKlassPointers::shift();\n+    mov64(dst, base_right_shifted);\n+    xorq(dst, src);\n+    shlq(dst, CompressedKlassPointers::shift());\n+    break;\n+  }\n+  case KlassDecodeAdd: {\n+    assert((base_u64 & (KlassAlignmentInBytes - 1)) == 0,\n+           \"base \" UINT64_FORMAT_X \" invalid for add mode\", base_u64); \/\/ should have been handled at VM init.\n+    const uint64_t base_right_shifted = base_u64 >> CompressedKlassPointers::shift();\n+    mov64(dst, base_right_shifted);\n+    addq(dst, src);\n+    shlq(dst, CompressedKlassPointers::shift());\n+    break;\n+  }\n+  default:\n+    ShouldNotReachHere();\n@@ -9679,0 +9803,58 @@\n+\n+void MacroAssembler::fast_lock_impl(Register obj, Register hdr, Register thread, Register tmp, Label& slow, bool rt_check_stack) {\n+  assert(hdr == rax, \"header must be in rax for cmpxchg\");\n+  assert_different_registers(obj, hdr, thread, tmp);\n+\n+  \/\/ First we need to check if the lock-stack has room for pushing the object reference.\n+  if (rt_check_stack) {\n+    movptr(tmp, Address(thread, JavaThread::lock_stack_current_offset()));\n+    cmpptr(tmp, Address(thread, JavaThread::lock_stack_limit_offset()));\n+    jcc(Assembler::greaterEqual, slow);\n+  }\n+#ifdef ASSERT\n+  else {\n+    Label ok;\n+    movptr(tmp, Address(thread, JavaThread::lock_stack_current_offset()));\n+    cmpptr(tmp, Address(thread, JavaThread::lock_stack_limit_offset()));\n+    jcc(Assembler::less, ok);\n+    stop(\"Not enough room in lock stack; should have been checked in the method prologue\");\n+    bind(ok);\n+  }\n+#endif\n+\n+  \/\/ Now we attempt to take the fast-lock.\n+  \/\/ Clear lowest two header bits (locked state).\n+  andptr(hdr, ~(int32_t)markWord::lock_mask_in_place);\n+  movptr(tmp, hdr);\n+  \/\/ Set lowest bit (unlocked state).\n+  orptr(hdr, markWord::unlocked_value);\n+  lock();\n+  cmpxchgptr(tmp, Address(obj, oopDesc::mark_offset_in_bytes()));\n+  jcc(Assembler::notEqual, slow);\n+\n+  \/\/ If successful, push object to lock-stack.\n+  movptr(tmp, Address(thread, JavaThread::lock_stack_current_offset()));\n+  movptr(Address(tmp, 0), obj);\n+  increment(tmp, oopSize);\n+  movptr(Address(thread, JavaThread::lock_stack_current_offset()), tmp);\n+}\n+\n+void MacroAssembler::fast_unlock_impl(Register obj, Register hdr, Register tmp, Label& slow) {\n+  assert(hdr == rax, \"header must be in rax for cmpxchg\");\n+  assert_different_registers(obj, hdr, tmp);\n+\n+  \/\/ Mark-word must be 00 now, try to swing it back to 01 (unlocked)\n+  movptr(tmp, hdr); \/\/ The expected old value\n+  orptr(tmp, markWord::unlocked_value);\n+  lock();\n+  cmpxchgptr(tmp, Address(obj, oopDesc::mark_offset_in_bytes()));\n+  jcc(Assembler::notEqual, slow);\n+  \/\/ Pop the lock object from the lock-stack.\n+#ifdef _LP64\n+  const Register thread = r15_thread;\n+#else\n+  const Register thread = rax;\n+  get_thread(rax);\n+#endif\n+  subptr(Address(thread, JavaThread::lock_stack_current_offset()), oopSize);\n+}\n","filename":"src\/hotspot\/cpu\/x86\/macroAssembler_x86.cpp","additions":247,"deletions":65,"binary":false,"changes":312,"status":"modified"},{"patch":"@@ -82,0 +82,22 @@\n+ public:\n+\n+  enum KlassDecodeMode {\n+    KlassDecodeNone,\n+    KlassDecodeZero,\n+    KlassDecodeXor,\n+    KlassDecodeAdd\n+  };\n+\n+  \/\/ Return the current narrow Klass pointer decode mode. Initialized on first call.\n+  static KlassDecodeMode klass_decode_mode();\n+\n+  \/\/ Given an arbitrary base address, return the KlassDecodeMode that would be used. Return KlassDecodeNone\n+  \/\/ if base address is not valid for encoding.\n+  static KlassDecodeMode klass_decode_mode_for_base(address base);\n+\n+  \/\/ Returns a static string\n+  static const char* describe_klass_decode_mode(KlassDecodeMode mode);\n+\n+ private:\n+  static KlassDecodeMode _klass_decode_mode;\n+\n@@ -351,0 +373,5 @@\n+#ifdef _LP64\n+  void load_nklass(Register dst, Register src);\n+#else\n+  void store_klass(Register dst, Register src);\n+#endif\n@@ -353,1 +380,0 @@\n-  void store_klass(Register dst, Register src, Register tmp);\n@@ -372,2 +398,0 @@\n-  void store_klass_gap(Register dst, Register src);\n-\n@@ -1998,0 +2022,2 @@\n+  void fast_lock_impl(Register obj, Register hdr, Register thread, Register tmp, Label& slow, bool rt_check_stack = true);\n+  void fast_unlock_impl(Register obj, Register hdr, Register tmp, Label& slow);\n","filename":"src\/hotspot\/cpu\/x86\/macroAssembler_x86.hpp","additions":29,"deletions":3,"binary":false,"changes":32,"status":"modified"},{"patch":"@@ -1680,30 +1680,36 @@\n-      \/\/ Load immediate 1 into swap_reg %rax,\n-      __ movptr(swap_reg, 1);\n-\n-      \/\/ Load (object->mark() | 1) into swap_reg %rax,\n-      __ orptr(swap_reg, Address(obj_reg, oopDesc::mark_offset_in_bytes()));\n-\n-      \/\/ Save (object->mark() | 1) into BasicLock's displaced header\n-      __ movptr(Address(lock_reg, mark_word_offset), swap_reg);\n-\n-      \/\/ src -> dest iff dest == rax, else rax, <- dest\n-      \/\/ *obj_reg = lock_reg iff *obj_reg == rax, else rax, = *(obj_reg)\n-      __ lock();\n-      __ cmpxchgptr(lock_reg, Address(obj_reg, oopDesc::mark_offset_in_bytes()));\n-      __ jcc(Assembler::equal, count_mon);\n-\n-      \/\/ Test if the oopMark is an obvious stack pointer, i.e.,\n-      \/\/  1) (mark & 3) == 0, and\n-      \/\/  2) rsp <= mark < mark + os::pagesize()\n-      \/\/ These 3 tests can be done by evaluating the following\n-      \/\/ expression: ((mark - rsp) & (3 - os::vm_page_size())),\n-      \/\/ assuming both stack pointer and pagesize have their\n-      \/\/ least significant 2 bits clear.\n-      \/\/ NOTE: the oopMark is in swap_reg %rax, as the result of cmpxchg\n-\n-      __ subptr(swap_reg, rsp);\n-      __ andptr(swap_reg, 3 - (int)os::vm_page_size());\n-\n-      \/\/ Save the test result, for recursive case, the result is zero\n-      __ movptr(Address(lock_reg, mark_word_offset), swap_reg);\n-      __ jcc(Assembler::notEqual, slow_path_lock);\n+      if (UseFastLocking) {\n+        \/\/ Load object header\n+        __ movptr(swap_reg, Address(obj_reg, oopDesc::mark_offset_in_bytes()));\n+        __ fast_lock_impl(obj_reg, swap_reg, thread, lock_reg, slow_path_lock);\n+      } else {\n+        \/\/ Load immediate 1 into swap_reg %rax,\n+        __ movptr(swap_reg, 1);\n+\n+        \/\/ Load (object->mark() | 1) into swap_reg %rax,\n+        __ orptr(swap_reg, Address(obj_reg, oopDesc::mark_offset_in_bytes()));\n+\n+        \/\/ Save (object->mark() | 1) into BasicLock's displaced header\n+        __ movptr(Address(lock_reg, mark_word_offset), swap_reg);\n+\n+        \/\/ src -> dest iff dest == rax, else rax, <- dest\n+        \/\/ *obj_reg = lock_reg iff *obj_reg == rax, else rax, = *(obj_reg)\n+        __ lock();\n+        __ cmpxchgptr(lock_reg, Address(obj_reg, oopDesc::mark_offset_in_bytes()));\n+        __ jcc(Assembler::equal, count_mon);\n+\n+        \/\/ Test if the oopMark is an obvious stack pointer, i.e.,\n+        \/\/  1) (mark & 3) == 0, and\n+        \/\/  2) rsp <= mark < mark + os::pagesize()\n+        \/\/ These 3 tests can be done by evaluating the following\n+        \/\/ expression: ((mark - rsp) & (3 - os::vm_page_size())),\n+        \/\/ assuming both stack pointer and pagesize have their\n+        \/\/ least significant 2 bits clear.\n+        \/\/ NOTE: the oopMark is in swap_reg %rax, as the result of cmpxchg\n+\n+        __ subptr(swap_reg, rsp);\n+        __ andptr(swap_reg, 3 - (int)os::vm_page_size());\n+\n+        \/\/ Save the test result, for recursive case, the result is zero\n+        __ movptr(Address(lock_reg, mark_word_offset), swap_reg);\n+       __ jcc(Assembler::notEqual, slow_path_lock);\n+      }\n@@ -1833,1 +1839,1 @@\n-    if (!UseHeavyMonitors) {\n+    if (!UseHeavyMonitors && !UseFastLocking) {\n@@ -1849,12 +1855,18 @@\n-      \/\/  get old displaced header\n-      __ movptr(rbx, Address(rbp, lock_slot_rbp_offset));\n-\n-      \/\/ get address of the stack lock\n-      __ lea(rax, Address(rbp, lock_slot_rbp_offset));\n-\n-      \/\/ Atomic swap old header if oop still contains the stack lock\n-      \/\/ src -> dest iff dest == rax, else rax, <- dest\n-      \/\/ *obj_reg = rbx, iff *obj_reg == rax, else rax, = *(obj_reg)\n-      __ lock();\n-      __ cmpxchgptr(rbx, Address(obj_reg, oopDesc::mark_offset_in_bytes()));\n-      __ jcc(Assembler::notEqual, slow_path_unlock);\n+      if (UseFastLocking) {\n+        __ movptr(swap_reg, Address(obj_reg, oopDesc::mark_offset_in_bytes()));\n+        __ andptr(swap_reg, ~(int32_t)markWord::lock_mask_in_place);\n+        __ fast_unlock_impl(obj_reg, swap_reg, lock_reg, slow_path_unlock);\n+      } else {\n+        \/\/  get old displaced header\n+        __ movptr(rbx, Address(rbp, lock_slot_rbp_offset));\n+\n+        \/\/ get address of the stack lock\n+        __ lea(rax, Address(rbp, lock_slot_rbp_offset));\n+\n+        \/\/ Atomic swap old header if oop still contains the stack lock\n+        \/\/ src -> dest iff dest == rax, else rax, <- dest\n+        \/\/ *obj_reg = rbx, iff *obj_reg == rax, else rax, = *(obj_reg)\n+        __ lock();\n+        __ cmpxchgptr(rbx, Address(obj_reg, oopDesc::mark_offset_in_bytes()));\n+        __ jcc(Assembler::notEqual, slow_path_unlock);\n+      }\n","filename":"src\/hotspot\/cpu\/x86\/sharedRuntime_x86_32.cpp","additions":55,"deletions":43,"binary":false,"changes":98,"status":"modified"},{"patch":"@@ -2148,0 +2148,7 @@\n+      if (UseFastLocking) {\n+        \/\/ Load object header\n+        __ movptr(swap_reg, Address(obj_reg, oopDesc::mark_offset_in_bytes()));\n+        __ fast_lock_impl(obj_reg, swap_reg, r15_thread, rscratch1, slow_path_lock);\n+      } else {\n+        \/\/ Load immediate 1 into swap_reg %rax\n+        __ movl(swap_reg, 1);\n@@ -2149,5 +2156,2 @@\n-      \/\/ Load immediate 1 into swap_reg %rax\n-      __ movl(swap_reg, 1);\n-\n-      \/\/ Load (object->mark() | 1) into swap_reg %rax\n-      __ orptr(swap_reg, Address(obj_reg, oopDesc::mark_offset_in_bytes()));\n+        \/\/ Load (object->mark() | 1) into swap_reg %rax\n+        __ orptr(swap_reg, Address(obj_reg, oopDesc::mark_offset_in_bytes()));\n@@ -2155,2 +2159,2 @@\n-      \/\/ Save (object->mark() | 1) into BasicLock's displaced header\n-      __ movptr(Address(lock_reg, mark_word_offset), swap_reg);\n+        \/\/ Save (object->mark() | 1) into BasicLock's displaced header\n+        __ movptr(Address(lock_reg, mark_word_offset), swap_reg);\n@@ -2158,4 +2162,4 @@\n-      \/\/ src -> dest iff dest == rax else rax <- dest\n-      __ lock();\n-      __ cmpxchgptr(lock_reg, Address(obj_reg, oopDesc::mark_offset_in_bytes()));\n-      __ jcc(Assembler::equal, count_mon);\n+        \/\/ src -> dest iff dest == rax else rax <- dest\n+        __ lock();\n+        __ cmpxchgptr(lock_reg, Address(obj_reg, oopDesc::mark_offset_in_bytes()));\n+        __ jcc(Assembler::equal, count_mon);\n@@ -2163,1 +2167,1 @@\n-      \/\/ Hmm should this move to the slow path code area???\n+        \/\/ Hmm should this move to the slow path code area???\n@@ -2165,8 +2169,8 @@\n-      \/\/ Test if the oopMark is an obvious stack pointer, i.e.,\n-      \/\/  1) (mark & 3) == 0, and\n-      \/\/  2) rsp <= mark < mark + os::pagesize()\n-      \/\/ These 3 tests can be done by evaluating the following\n-      \/\/ expression: ((mark - rsp) & (3 - os::vm_page_size())),\n-      \/\/ assuming both stack pointer and pagesize have their\n-      \/\/ least significant 2 bits clear.\n-      \/\/ NOTE: the oopMark is in swap_reg %rax as the result of cmpxchg\n+        \/\/ Test if the oopMark is an obvious stack pointer, i.e.,\n+        \/\/  1) (mark & 3) == 0, and\n+        \/\/  2) rsp <= mark < mark + os::pagesize()\n+        \/\/ These 3 tests can be done by evaluating the following\n+        \/\/ expression: ((mark - rsp) & (3 - os::vm_page_size())),\n+        \/\/ assuming both stack pointer and pagesize have their\n+        \/\/ least significant 2 bits clear.\n+        \/\/ NOTE: the oopMark is in swap_reg %rax as the result of cmpxchg\n@@ -2174,2 +2178,2 @@\n-      __ subptr(swap_reg, rsp);\n-      __ andptr(swap_reg, 3 - (int)os::vm_page_size());\n+        __ subptr(swap_reg, rsp);\n+        __ andptr(swap_reg, 3 - (int)os::vm_page_size());\n@@ -2177,3 +2181,4 @@\n-      \/\/ Save the test result, for recursive case, the result is zero\n-      __ movptr(Address(lock_reg, mark_word_offset), swap_reg);\n-      __ jcc(Assembler::notEqual, slow_path_lock);\n+        \/\/ Save the test result, for recursive case, the result is zero\n+        __ movptr(Address(lock_reg, mark_word_offset), swap_reg);\n+        __ jcc(Assembler::notEqual, slow_path_lock);\n+      }\n@@ -2293,1 +2298,1 @@\n-    if (!UseHeavyMonitors) {\n+    if (!UseHeavyMonitors && !UseFastLocking) {\n@@ -2309,9 +2314,15 @@\n-      \/\/ get address of the stack lock\n-      __ lea(rax, Address(rsp, lock_slot_offset * VMRegImpl::stack_slot_size));\n-      \/\/  get old displaced header\n-      __ movptr(old_hdr, Address(rax, 0));\n-\n-      \/\/ Atomic swap old header if oop still contains the stack lock\n-      __ lock();\n-      __ cmpxchgptr(old_hdr, Address(obj_reg, oopDesc::mark_offset_in_bytes()));\n-      __ jcc(Assembler::notEqual, slow_path_unlock);\n+      if (UseFastLocking) {\n+        __ movptr(swap_reg, Address(obj_reg, oopDesc::mark_offset_in_bytes()));\n+        __ andptr(swap_reg, ~(int32_t)markWord::lock_mask_in_place);\n+        __ fast_unlock_impl(obj_reg, swap_reg, lock_reg, slow_path_unlock);\n+      } else {\n+        \/\/ get address of the stack lock\n+        __ lea(rax, Address(rsp, lock_slot_offset * VMRegImpl::stack_slot_size));\n+        \/\/  get old displaced header\n+        __ movptr(old_hdr, Address(rax, 0));\n+\n+        \/\/ Atomic swap old header if oop still contains the stack lock\n+        __ lock();\n+        __ cmpxchgptr(old_hdr, Address(obj_reg, oopDesc::mark_offset_in_bytes()));\n+        __ jcc(Assembler::notEqual, slow_path_unlock);\n+      }\n","filename":"src\/hotspot\/cpu\/x86\/sharedRuntime_x86_64.cpp","additions":46,"deletions":35,"binary":false,"changes":81,"status":"modified"},{"patch":"@@ -3186,0 +3186,49 @@\n+\/\/ Call runtime to ensure lock-stack size.\n+\/\/ Arguments:\n+\/\/ - c_rarg0: the required _limit pointer\n+address StubGenerator::generate_check_lock_stack() {\n+  __ align(CodeEntryAlignment);\n+  StubCodeMark mark(this, \"StubRoutines\", \"check_lock_stack\");\n+  address start = __ pc();\n+\n+  BLOCK_COMMENT(\"Entry:\");\n+  __ enter(); \/\/ save rbp\n+\n+  __ pusha();\n+\n+  \/\/ The method may have floats as arguments, and we must spill them before calling\n+  \/\/ the VM runtime.\n+  assert(Argument::n_float_register_parameters_j == 8, \"Assumption\");\n+  const int xmm_size = wordSize * 2;\n+  const int xmm_spill_size = xmm_size * Argument::n_float_register_parameters_j;\n+  __ subptr(rsp, xmm_spill_size);\n+  __ movdqu(Address(rsp, xmm_size * 7), xmm7);\n+  __ movdqu(Address(rsp, xmm_size * 6), xmm6);\n+  __ movdqu(Address(rsp, xmm_size * 5), xmm5);\n+  __ movdqu(Address(rsp, xmm_size * 4), xmm4);\n+  __ movdqu(Address(rsp, xmm_size * 3), xmm3);\n+  __ movdqu(Address(rsp, xmm_size * 2), xmm2);\n+  __ movdqu(Address(rsp, xmm_size * 1), xmm1);\n+  __ movdqu(Address(rsp, xmm_size * 0), xmm0);\n+\n+  __ call_VM_leaf(CAST_FROM_FN_PTR(address, static_cast<void (*)(oop*)>(LockStack::ensure_lock_stack_size)), rax);\n+\n+  __ movdqu(xmm0, Address(rsp, xmm_size * 0));\n+  __ movdqu(xmm1, Address(rsp, xmm_size * 1));\n+  __ movdqu(xmm2, Address(rsp, xmm_size * 2));\n+  __ movdqu(xmm3, Address(rsp, xmm_size * 3));\n+  __ movdqu(xmm4, Address(rsp, xmm_size * 4));\n+  __ movdqu(xmm5, Address(rsp, xmm_size * 5));\n+  __ movdqu(xmm6, Address(rsp, xmm_size * 6));\n+  __ movdqu(xmm7, Address(rsp, xmm_size * 7));\n+  __ addptr(rsp, xmm_spill_size);\n+\n+  __ popa();\n+\n+  __ leave();\n+\n+  __ ret(0);\n+\n+  return start;\n+}\n+\n@@ -4023,0 +4072,3 @@\n+  if (UseFastLocking) {\n+    StubRoutines::x86::_check_lock_stack = generate_check_lock_stack();\n+  }\n","filename":"src\/hotspot\/cpu\/x86\/stubGenerator_x86_64.cpp","additions":52,"deletions":0,"binary":false,"changes":52,"status":"modified"},{"patch":"@@ -468,0 +468,2 @@\n+  address generate_check_lock_stack();\n+\n","filename":"src\/hotspot\/cpu\/x86\/stubGenerator_x86_64.hpp","additions":2,"deletions":0,"binary":false,"changes":2,"status":"modified"},{"patch":"@@ -87,0 +87,1 @@\n+address StubRoutines::x86::_check_lock_stack = NULL;\n","filename":"src\/hotspot\/cpu\/x86\/stubRoutines_x86.cpp","additions":1,"deletions":0,"binary":false,"changes":1,"status":"modified"},{"patch":"@@ -129,0 +129,2 @@\n+  static address _check_lock_stack;\n+\n@@ -218,0 +220,2 @@\n+  static address check_lock_stack() { return _check_lock_stack; }\n+\n","filename":"src\/hotspot\/cpu\/x86\/stubRoutines_x86.hpp","additions":4,"deletions":0,"binary":false,"changes":4,"status":"modified"},{"patch":"@@ -3999,2 +3999,0 @@\n-    __ movptr(Address(rax, oopDesc::mark_offset_in_bytes()),\n-              (intptr_t)markWord::prototype().value()); \/\/ header\n@@ -4002,3 +4000,4 @@\n-#ifdef _LP64\n-    __ xorl(rsi, rsi); \/\/ use zero reg to clear memory (shorter code)\n-    __ store_klass_gap(rax, rsi);  \/\/ zero klass gap for compressed oops\n+    __ movptr(rbx, Address(rcx, Klass::prototype_header_offset()));\n+    __ movptr(Address(rax, oopDesc::mark_offset_in_bytes ()), rbx);\n+#ifndef _LP64\n+    __ store_klass(rax, rcx);  \/\/ klass\n@@ -4006,1 +4005,0 @@\n-    __ store_klass(rax, rcx, rscratch1);  \/\/ klass\n","filename":"src\/hotspot\/cpu\/x86\/templateTable_x86.cpp","additions":4,"deletions":6,"binary":false,"changes":10,"status":"modified"},{"patch":"@@ -220,1 +220,1 @@\n-  const ptrdiff_t estimate = 136;\n+  const ptrdiff_t estimate = 137;\n","filename":"src\/hotspot\/cpu\/x86\/vtableStubs_x86_64.cpp","additions":1,"deletions":1,"binary":false,"changes":2,"status":"modified"},{"patch":"@@ -617,1 +617,2 @@\n-  __ verified_entry(framesize, C->output()->need_stack_bang(bangsize)?bangsize:0, C->in_24_bit_fp_mode(), C->stub_function() != NULL);\n+  int max_monitors = C->method() != NULL ? C->max_monitors() : 0;\n+  __ verified_entry(framesize, C->output()->need_stack_bang(bangsize)?bangsize:0, C->in_24_bit_fp_mode(), C->stub_function() != NULL, max_monitors);\n@@ -13805,1 +13806,1 @@\n-instruct cmpFastLockRTM(eFlagsReg cr, eRegP object, eBXRegP box, eAXRegI tmp, eDXRegI scr, rRegI cx1, rRegI cx2) %{\n+instruct cmpFastLockRTM(eFlagsReg cr, eRegP object, eBXRegP box, eAXRegI tmp, eDXRegI scr, rRegI cx1, rRegI cx2, eRegP thread) %{\n@@ -13808,1 +13809,1 @@\n-  effect(TEMP tmp, TEMP scr, TEMP cx1, TEMP cx2, USE_KILL box);\n+  effect(TEMP tmp, TEMP scr, TEMP cx1, TEMP cx2, USE_KILL box, TEMP thread);\n@@ -13812,0 +13813,1 @@\n+    __ get_thread($thread$$Register);\n@@ -13813,1 +13815,1 @@\n-                 $scr$$Register, $cx1$$Register, $cx2$$Register,\n+                 $scr$$Register, $cx1$$Register, $cx2$$Register, $thread$$Register,\n@@ -13821,1 +13823,1 @@\n-instruct cmpFastLock(eFlagsReg cr, eRegP object, eBXRegP box, eAXRegI tmp, eRegP scr) %{\n+instruct cmpFastLock(eFlagsReg cr, eRegP object, eBXRegP box, eAXRegI tmp, eRegP scr, eRegP thread) %{\n@@ -13824,1 +13826,1 @@\n-  effect(TEMP tmp, TEMP scr, USE_KILL box);\n+  effect(TEMP tmp, TEMP scr, USE_KILL box, TEMP thread);\n@@ -13828,0 +13830,1 @@\n+    __ get_thread($thread$$Register);\n@@ -13829,1 +13832,1 @@\n-                 $scr$$Register, noreg, noreg, NULL, NULL, NULL, false, false);\n+                 $scr$$Register, noreg, noreg, $thread$$Register, NULL, NULL, NULL, false, false);\n","filename":"src\/hotspot\/cpu\/x86\/x86_32.ad","additions":10,"deletions":7,"binary":false,"changes":17,"status":"modified"},{"patch":"@@ -925,1 +925,2 @@\n-  __ verified_entry(framesize, C->output()->need_stack_bang(bangsize)?bangsize:0, false, C->stub_function() != NULL);\n+  int max_monitors = C->method() != NULL ? C->max_monitors() : 0;\n+  __ verified_entry(framesize, C->output()->need_stack_bang(bangsize)?bangsize:0, false, C->stub_function() != NULL, max_monitors);\n@@ -5315,1 +5316,1 @@\n-instruct loadNKlass(rRegN dst, memory mem)\n+instruct loadNKlass(rRegN dst, indOffset8 mem, rFlagsReg cr)\n@@ -5318,1 +5319,1 @@\n-\n+  effect(TEMP_DEF dst, KILL cr);\n@@ -5322,1 +5323,11 @@\n-    __ movl($dst$$Register, $mem$$Address);\n+    assert($mem$$disp == oopDesc::klass_offset_in_bytes(), \"expect correct offset 4, but got: %d\", $mem$$disp);\n+    assert($mem$$index == 4, \"expect no index register: %d\", $mem$$index);\n+    Register dst = $dst$$Register;\n+    Register obj = $mem$$base$$Register;\n+    C2LoadNKlassStub* stub = new (Compile::current()->comp_arena()) C2LoadNKlassStub(dst);\n+    Compile::current()->output()->add_stub(stub);\n+    __ movq(dst, Address(obj, oopDesc::mark_offset_in_bytes()));\n+    __ testb(dst, markWord::monitor_value);\n+    __ jcc(Assembler::notZero, stub->entry());\n+    __ bind(stub->continuation());\n+    __ shrq(dst, markWord::klass_shift);\n@@ -5324,1 +5335,1 @@\n-  ins_pipe(ialu_reg_mem); \/\/ XXX\n+  ins_pipe(pipe_slow); \/\/ XXX\n@@ -12676,0 +12687,3 @@\n+\/\/ Disabled because the compressed Klass* in header cannot be safely\n+\/\/ accessed. TODO: Re-enable it as soon as synchronization does not\n+\/\/ overload the upper header bits anymore.\n@@ -12678,0 +12692,1 @@\n+  predicate(false);\n@@ -13364,1 +13379,1 @@\n-                 $scr$$Register, $cx1$$Register, $cx2$$Register,\n+                 $scr$$Register, $cx1$$Register, $cx2$$Register, r15_thread,\n@@ -13380,1 +13395,1 @@\n-                 $scr$$Register, noreg, noreg, NULL, NULL, NULL, false, false);\n+                 $scr$$Register, noreg, noreg, r15_thread, NULL, NULL, NULL, false, false);\n","filename":"src\/hotspot\/cpu\/x86\/x86_64.ad","additions":22,"deletions":7,"binary":false,"changes":29,"status":"modified"},{"patch":"@@ -0,0 +1,49 @@\n+\/*\n+ * Copyright (c) 2021 SAP SE. All rights reserved.\n+ * Copyright (c) 2021, Red Hat Inc. All rights reserved.\n+ * Copyright (c) 2021, Oracle and\/or its affiliates. All rights reserved.\n+ *\n+ * DO NOT ALTER OR REMOVE COPYRIGHT NOTICES OR THIS FILE HEADER.\n+ *\n+ * This code is free software; you can redistribute it and\/or modify it\n+ * under the terms of the GNU General Public License version 2 only, as\n+ * published by the Free Software Foundation.\n+ *\n+ * This code is distributed in the hope that it will be useful, but WITHOUT\n+ * ANY WARRANTY; without even the implied warranty of MERCHANTABILITY or\n+ * FITNESS FOR A PARTICULAR PURPOSE.  See the GNU General Public License\n+ * version 2 for more details (a copy is included in the LICENSE file that\n+ * accompanied this code).\n+ *\n+ * You should have received a copy of the GNU General Public License version\n+ * 2 along with this work; if not, write to the Free Software Foundation,\n+ * Inc., 51 Franklin St, Fifth Floor, Boston, MA 02110-1301 USA.\n+ *\n+ * Please contact Oracle, 500 Oracle Parkway, Redwood Shores, CA 94065 USA\n+ * or visit www.oracle.com if you need additional information or have any\n+ * questions.\n+ *\n+ *\/\n+\n+#include \"precompiled.hpp\"\n+\n+#include \"oops\/compressedKlass.hpp\"\n+#include \"utilities\/ostream.hpp\"\n+#include \"runtime\/globals.hpp\"\n+\n+#ifdef _LP64\n+\n+\/\/ Given an address p, return true if p can be used as an encoding base.\n+\/\/  (Some platforms have restrictions of what constitutes a valid base address).\n+bool CompressedKlassPointers::is_valid_base(address p) {\n+  return true;\n+}\n+\n+void CompressedKlassPointers::print_mode(outputStream* st) {\n+  st->print_cr(\"Narrow klass base: \" PTR_FORMAT \", Narrow klass shift: %d, \"\n+               \"Narrow klass range: \" UINT64_FORMAT, p2i(base()), shift(),\n+               KlassEncodingMetaspaceMax);\n+}\n+\n+#endif \/\/ _LP64\n+\n","filename":"src\/hotspot\/cpu\/zero\/compressedKlass_zero.cpp","additions":49,"deletions":0,"binary":false,"changes":49,"status":"added"},{"patch":"@@ -584,0 +584,18 @@\n+class LoadKlassStub: public CodeStub {\n+private:\n+  LIR_Opr          _result;\n+\n+public:\n+  LoadKlassStub(LIR_Opr result) :\n+    CodeStub(), _result(result) {};\n+\n+  virtual void emit_code(LIR_Assembler* e);\n+  virtual void visit(LIR_OpVisitState* visitor) {\n+    visitor->do_temp(_result);\n+    visitor->do_output(_result);\n+  }\n+#ifndef PRODUCT\n+virtual void print_name(outputStream* out) const { out->print(\"LoadKlassStub\"); }\n+#endif \/\/ PRODUCT\n+};\n+\n","filename":"src\/hotspot\/share\/c1\/c1_CodeStubs.hpp","additions":18,"deletions":0,"binary":false,"changes":18,"status":"modified"},{"patch":"@@ -387,0 +387,1 @@\n+    push_monitor();\n@@ -574,0 +575,1 @@\n+, _max_monitors(0)\n","filename":"src\/hotspot\/share\/c1\/c1_Compilation.cpp","additions":2,"deletions":0,"binary":false,"changes":2,"status":"modified"},{"patch":"@@ -86,0 +86,1 @@\n+  int                _max_monitors; \/\/ Max number of active monitors, for fast-locking\n@@ -143,0 +144,1 @@\n+  int max_monitors() const                       { return _max_monitors; }\n@@ -175,0 +177,2 @@\n+  void push_monitor()                            { _max_monitors++; }\n+\n","filename":"src\/hotspot\/share\/c1\/c1_Compilation.hpp","additions":4,"deletions":0,"binary":false,"changes":4,"status":"modified"},{"patch":"@@ -2318,0 +2318,1 @@\n+  compilation()->push_monitor();\n","filename":"src\/hotspot\/share\/c1\/c1_GraphBuilder.cpp","additions":1,"deletions":0,"binary":false,"changes":1,"status":"modified"},{"patch":"@@ -891,0 +891,1 @@\n+      do_stub(opLoadKlass->_stub);\n@@ -1071,0 +1072,1 @@\n+  masm->append_code_stub(stub());\n@@ -2039,0 +2041,1 @@\n+  out->print(\"[lbl:\" INTPTR_FORMAT \"]\", p2i(stub()->entry()));\n","filename":"src\/hotspot\/share\/c1\/c1_LIR.cpp","additions":3,"deletions":0,"binary":false,"changes":3,"status":"modified"},{"patch":"@@ -1904,0 +1904,1 @@\n+  CodeStub* _stub;\n@@ -1905,1 +1906,1 @@\n-  LIR_OpLoadKlass(LIR_Opr obj, LIR_Opr result, CodeEmitInfo* info)\n+  LIR_OpLoadKlass(LIR_Opr obj, LIR_Opr result, CodeEmitInfo* info, CodeStub* stub)\n@@ -1908,1 +1909,1 @@\n-    {}\n+    , _stub(stub) {}\n@@ -1911,0 +1912,1 @@\n+  CodeStub* stub()     const { return _stub; }\n@@ -2374,1 +2376,1 @@\n-  void load_klass(LIR_Opr obj, LIR_Opr result, CodeEmitInfo* info) { append(new LIR_OpLoadKlass(obj, result, info)); }\n+  void load_klass(LIR_Opr obj, LIR_Opr result, CodeEmitInfo* info, CodeStub* stub) { append(new LIR_OpLoadKlass(obj, result, info, stub)); }\n","filename":"src\/hotspot\/share\/c1\/c1_LIR.hpp","additions":5,"deletions":3,"binary":false,"changes":8,"status":"modified"},{"patch":"@@ -774,1 +774,1 @@\n-  _masm->build_frame(initial_frame_size_in_bytes(), bang_size_in_bytes());\n+  _masm->build_frame(initial_frame_size_in_bytes(), bang_size_in_bytes(), compilation()->max_monitors());\n","filename":"src\/hotspot\/share\/c1\/c1_LIRAssembler.cpp","additions":1,"deletions":1,"binary":false,"changes":2,"status":"modified"},{"patch":"@@ -1244,1 +1244,2 @@\n-  __ load_klass(obj, klass, null_check_info);\n+  CodeStub* slow_path = new LoadKlassStub(klass);\n+  __ load_klass(obj, klass, null_check_info, slow_path);\n","filename":"src\/hotspot\/share\/c1\/c1_LIRGenerator.cpp","additions":2,"deletions":1,"binary":false,"changes":3,"status":"modified"},{"patch":"@@ -42,1 +42,1 @@\n-  void build_frame(int frame_size_in_bytes, int bang_size_in_bytes);\n+  void build_frame(int frame_size_in_bytes, int bang_size_in_bytes, int max_monitors);\n","filename":"src\/hotspot\/share\/c1\/c1_MacroAssembler.hpp","additions":1,"deletions":1,"binary":false,"changes":2,"status":"modified"},{"patch":"@@ -760,2 +760,2 @@\n-  assert(obj == lock->obj(), \"must match\");\n-  SharedRuntime::monitor_enter_helper(obj, lock->lock(), current);\n+  assert(UseFastLocking || obj == lock->obj(), \"must match\");\n+  SharedRuntime::monitor_enter_helper(obj, UseFastLocking ? NULL : lock->lock(), current);\n","filename":"src\/hotspot\/share\/c1\/c1_Runtime1.cpp","additions":2,"deletions":2,"binary":false,"changes":4,"status":"modified"},{"patch":"@@ -43,0 +43,1 @@\n+#include \"oops\/compressedKlass.inline.hpp\"\n@@ -44,0 +45,1 @@\n+#include \"oops\/klass.inline.hpp\"\n@@ -223,2 +225,4 @@\n-    \/\/ See RunTimeClassInfo::get_for()\n-    _estimated_metaspaceobj_bytes += align_up(BytesPerWord, SharedSpaceObjectAlignment);\n+    \/\/ See ArchiveBuilder::make_shallow_copies: make sure we have enough space for both maximum\n+    \/\/ Klass alignment as well as the RuntimeInfo* pointer we will embed in front of a Klass.\n+    _estimated_metaspaceobj_bytes += align_up(BytesPerWord, KlassAlignmentInBytes) +\n+        align_up(sizeof(void*), SharedSpaceObjectAlignment);\n@@ -621,4 +625,5 @@\n-    \/\/ Save a pointer immediate in front of an InstanceKlass, so\n-    \/\/ we can do a quick lookup from InstanceKlass* -> RunTimeClassInfo*\n-    \/\/ without building another hashtable. See RunTimeClassInfo::get_for()\n-    \/\/ in systemDictionaryShared.cpp.\n+    \/\/ Reserve space for a pointer immediately in front of an InstanceKlass. That space will\n+    \/\/ later be used to store the RuntimeClassInfo* pointer directly in front of the archived\n+    \/\/ InstanceKlass, in order to have a quick lookup InstanceKlass* -> RunTimeClassInfo*\n+    \/\/ without building another hashtable. See RunTimeClassInfo::get_for()\/::set_for() for\n+    \/\/ details.\n@@ -630,0 +635,3 @@\n+    dest = dump_region->allocate(bytes, KlassAlignmentInBytes);\n+  } else {\n+    dest = dump_region->allocate(bytes);\n@@ -631,1 +639,0 @@\n-  dest = dump_region->allocate(bytes);\n@@ -650,1 +657,2 @@\n-  log_trace(cds)(\"Copy: \" PTR_FORMAT \" ==> \" PTR_FORMAT \" %d\", p2i(src), p2i(dest), bytes);\n+  log_trace(cds)(\"Copy: \" PTR_FORMAT \" ==> \" PTR_FORMAT \" %d (%s)\", p2i(src), p2i(dest), bytes,\n+                 MetaspaceObj::type_name(ref->msotype()));\n@@ -654,0 +662,2 @@\n+\n+  DEBUG_ONLY(_alloc_stats.verify((int)dump_region->used(), src_info->read_only()));\n@@ -750,0 +760,7 @@\n+    Klass* requested_k = to_requested(k);\n+#ifdef _LP64\n+    narrowKlass nk = CompressedKlassPointers::encode_not_null(requested_k, _requested_static_archive_bottom);\n+    k->set_prototype_header(markWord::prototype().set_narrow_klass(nk));\n+#else\n+    k->set_prototype_header(markWord::prototype());\n+#endif\n","filename":"src\/hotspot\/share\/cds\/archiveBuilder.cpp","additions":25,"deletions":8,"binary":false,"changes":33,"status":"modified"},{"patch":"@@ -31,0 +31,1 @@\n+#include \"memory\/metaspace\/metaspaceAlignment.hpp\"\n@@ -46,3 +47,13 @@\n-\/\/ Metaspace::allocate() requires that all blocks must be aligned with KlassAlignmentInBytes.\n-\/\/ We enforce the same alignment rule in blocks allocated from the shared space.\n-const int SharedSpaceObjectAlignment = KlassAlignmentInBytes;\n+\/\/ CDS has three alignments to deal with:\n+\/\/ - SharedSpaceObjectAlignment, always 8 bytes: used for placing arbitrary structures.\n+\/\/   These may contain 64-bit members (not larger, we know that much). Therefore we\n+\/\/   need to use 64-bit alignment on both 32-bit and 64-bit platforms. We reuse metaspace\n+\/\/   minimal alignment for this, which follows the same logic.\n+\/\/ - With CompressedClassPointers=1, we need to store Klass structures with a large\n+\/\/   alignment (Lilliput specific narrow Klass pointer encoding) - KlassAlignmentInBytes.\n+\/\/ - Header data and tags are squeezed in with word alignment, which happens to be 4 bytes\n+\/\/   on 32-bit. See ReadClosure::do_xxx() and DumpRegion::append_intptr().\n+const int SharedSpaceObjectAlignment = metaspace::MetaspaceMinAlignmentBytes;\n+\n+\/\/ standard alignment should be sufficient for storing 64-bit values.\n+STATIC_ASSERT(SharedSpaceObjectAlignment >= sizeof(uint64_t));\n","filename":"src\/hotspot\/share\/cds\/archiveBuilder.hpp","additions":14,"deletions":3,"binary":false,"changes":17,"status":"modified"},{"patch":"@@ -223,2 +223,2 @@\n-    oopDesc::set_mark(mem, markWord::prototype());\n-    oopDesc::release_set_klass(mem, k);\n+    narrowKlass nk = ArchiveBuilder::current()->get_requested_narrow_klass(k);\n+    oopDesc::release_set_mark(mem, markWord::prototype().set_narrow_klass(nk));\n@@ -303,1 +303,0 @@\n-  oopDesc::set_mark(mem, markWord::prototype());\n@@ -305,1 +304,1 @@\n-  cast_to_oop(mem)->set_narrow_klass(nk);\n+  oopDesc::set_mark(mem, markWord::prototype().set_narrow_klass(nk));\n@@ -487,1 +486,0 @@\n-  fake_oop->set_narrow_klass(nk);\n@@ -495,1 +493,1 @@\n-    fake_oop->set_mark(markWord::prototype().copy_set_hash(src_hash));\n+    fake_oop->set_mark(markWord::prototype().set_narrow_klass(nk).copy_set_hash(src_hash));\n","filename":"src\/hotspot\/share\/cds\/archiveHeapWriter.cpp","additions":4,"deletions":6,"binary":false,"changes":10,"status":"modified"},{"patch":"@@ -208,3 +208,13 @@\n-char* DumpRegion::allocate(size_t num_bytes) {\n-  char* p = (char*)align_up(_top, (size_t)SharedSpaceObjectAlignment);\n-  char* newtop = p + align_up(num_bytes, (size_t)SharedSpaceObjectAlignment);\n+char* DumpRegion::allocate(size_t num_bytes, size_t alignment) {\n+  \/\/ We align the starting address of each allocation.\n+  char* p = (char*)align_up(_top, alignment);\n+  char* newtop = p + num_bytes;\n+  \/\/ Leave _top always SharedSpaceObjectAlignment aligned. But not more -\n+  \/\/  if we allocate with large alignments, lets not waste the gaps.\n+  \/\/ Ideally we would not need to align _top to anything here but CDS has\n+  \/\/  a number of implicit alignment assumptions. Leaving this unaligned\n+  \/\/  here will trip of at least ReadClosure (assuming word alignment) and\n+  \/\/  DumpAllocStats (will get confused about counting bytes on 32-bit\n+  \/\/  platforms if we align to anything less than SharedSpaceObjectAlignment\n+  \/\/  here).\n+  newtop = align_up(newtop, SharedSpaceObjectAlignment);\n@@ -212,1 +222,1 @@\n-  memset(p, 0, newtop - p);\n+  memset(p, 0, newtop - p); \/\/ todo: needed? debug_only?\n@@ -216,0 +226,4 @@\n+char* DumpRegion::allocate(size_t num_bytes) {\n+  return allocate(num_bytes, SharedSpaceObjectAlignment);\n+}\n+\n@@ -312,1 +326,1 @@\n-  assert(tag == old_tag, \"old tag doesn't match\");\n+  assert(tag == old_tag, \"tag doesn't match (%d, expected %d)\", old_tag, tag);\n","filename":"src\/hotspot\/share\/cds\/archiveUtils.cpp","additions":19,"deletions":5,"binary":false,"changes":24,"status":"modified"},{"patch":"@@ -149,0 +149,1 @@\n+  \/\/ Allocate with default alignment (SharedSpaceObjectAlignment)\n@@ -150,0 +151,2 @@\n+  \/\/ Allocate with an arbitrary alignment.\n+  char* allocate(size_t num_bytes, size_t alignment);\n","filename":"src\/hotspot\/share\/cds\/archiveUtils.hpp","additions":3,"deletions":0,"binary":false,"changes":3,"status":"modified"},{"patch":"@@ -110,0 +110,12 @@\n+\n+#ifdef ASSERT\n+void DumpAllocStats::verify(int expected_byte_size, bool read_only) const {\n+  int bytes = 0;\n+  const int what = (int)(read_only ? RO : RW);\n+  for (int type = 0; type < int(_number_of_types); type ++) {\n+    bytes += _bytes[what][type];\n+  }\n+  assert(bytes == expected_byte_size, \"counter mismatch (%s: %d vs %d)\",\n+         (read_only ? \"RO\" : \"RW\"), bytes, expected_byte_size);\n+}\n+#endif \/\/ ASSERT\n","filename":"src\/hotspot\/share\/cds\/dumpAllocStats.cpp","additions":12,"deletions":0,"binary":false,"changes":12,"status":"modified"},{"patch":"@@ -91,0 +91,2 @@\n+  DEBUG_ONLY(void verify(int expected_byte_size, bool read_only) const;)\n+\n","filename":"src\/hotspot\/share\/cds\/dumpAllocStats.hpp","additions":2,"deletions":0,"binary":false,"changes":2,"status":"modified"},{"patch":"@@ -59,0 +59,1 @@\n+#include \"runtime\/safepoint.hpp\"\n","filename":"src\/hotspot\/share\/cds\/heapShared.cpp","additions":1,"deletions":0,"binary":false,"changes":1,"status":"modified"},{"patch":"@@ -61,0 +61,1 @@\n+#include \"oops\/compressedKlass.hpp\"\n@@ -1314,0 +1315,5 @@\n+#ifdef ASSERT\n+    if (UseCompressedClassPointers) {\n+      CompressedKlassPointers::verify_klass_pointer(record->_klass);\n+    }\n+#endif\n@@ -1315,1 +1321,0 @@\n-    assert(check_alignment(record->_klass), \"Address not aligned\");\n","filename":"src\/hotspot\/share\/classfile\/systemDictionaryShared.cpp","additions":6,"deletions":1,"binary":false,"changes":7,"status":"modified"},{"patch":"@@ -105,2 +105,2 @@\n-  static const int first_vtableStub_size =  64;\n-  static const int first_itableStub_size = 256;\n+  static const int first_vtableStub_size = 256;\n+  static const int first_itableStub_size = 512;\n","filename":"src\/hotspot\/share\/code\/vtableStubs.cpp","additions":2,"deletions":2,"binary":false,"changes":4,"status":"modified"},{"patch":"@@ -89,0 +89,2 @@\n+#include \"gc\/shared\/slidingForwarding.inline.hpp\"\n+#include \"gc\/shared\/suspendibleThreadSet.hpp\"\n@@ -1523,0 +1525,2 @@\n+  _forwarding = new SlidingForwarding(heap_rs.region(), HeapRegion::LogOfHRGrainBytes - LogHeapWordSize);\n+\n","filename":"src\/hotspot\/share\/gc\/g1\/g1CollectedHeap.cpp","additions":4,"deletions":0,"binary":false,"changes":4,"status":"modified"},{"patch":"@@ -88,0 +88,1 @@\n+class SlidingForwarding;\n@@ -255,0 +256,2 @@\n+  SlidingForwarding* _forwarding;\n+\n@@ -268,0 +271,4 @@\n+  SlidingForwarding* forwarding() const {\n+    return _forwarding;\n+  }\n+\n","filename":"src\/hotspot\/share\/gc\/g1\/g1CollectedHeap.hpp","additions":7,"deletions":0,"binary":false,"changes":7,"status":"modified"},{"patch":"@@ -45,0 +45,1 @@\n+#include \"gc\/shared\/slidingForwarding.hpp\"\n@@ -333,0 +334,2 @@\n+  _heap->forwarding()->clear();\n+\n@@ -344,3 +347,4 @@\n-  if (scope()->do_maximal_compaction() || !has_free_compaction_targets) {\n-    phase2c_prepare_serial_compaction();\n-  }\n+  \/\/ TODO: Disabled for now because it violates sliding-forwarding assumption.\n+\/\/  if (scope()->do_maximal_compaction() || !has_free_compaction_targets) {\n+\/\/    phase2c_prepare_serial_compaction();\n+\/\/  }\n@@ -365,55 +369,55 @@\n-uint G1FullCollector::truncate_parallel_cps() {\n-  uint lowest_current = (uint)-1;\n-  for (uint i = 0; i < workers(); i++) {\n-    G1FullGCCompactionPoint* cp = compaction_point(i);\n-    if (cp->has_regions()) {\n-      lowest_current = MIN2(lowest_current, cp->current_region()->hrm_index());\n-    }\n-  }\n-\n-  for (uint i = 0; i < workers(); i++) {\n-    G1FullGCCompactionPoint* cp = compaction_point(i);\n-    if (cp->has_regions()) {\n-      cp->remove_at_or_above(lowest_current);\n-    }\n-  }\n-  return lowest_current;\n-}\n-\n-void G1FullCollector::phase2c_prepare_serial_compaction() {\n-  GCTraceTime(Debug, gc, phases) debug(\"Phase 2: Prepare serial compaction\", scope()->timer());\n-  \/\/ At this point, we know that after parallel compaction there will be regions that\n-  \/\/ are partially compacted into. Thus, the last compaction region of all\n-  \/\/ compaction queues still have space in them. We try to re-compact these regions\n-  \/\/ in serial to avoid a premature OOM when the mutator wants to allocate the first\n-  \/\/ eden region after gc.\n-\n-  \/\/ For maximum compaction, we need to re-prepare all objects above the lowest\n-  \/\/ region among the current regions for all thread compaction points. It may\n-  \/\/ happen that due to the uneven distribution of objects to parallel threads, holes\n-  \/\/ have been created as threads compact to different target regions between the\n-  \/\/ lowest and the highest region in the tails of the compaction points.\n-\n-  uint start_serial = truncate_parallel_cps();\n-  assert(start_serial < _heap->max_reserved_regions(), \"Called on empty parallel compaction queues\");\n-\n-  G1FullGCCompactionPoint* serial_cp = serial_compaction_point();\n-  assert(!serial_cp->is_initialized(), \"sanity!\");\n-\n-  HeapRegion* start_hr = _heap->region_at(start_serial);\n-  serial_cp->add(start_hr);\n-  serial_cp->initialize(start_hr);\n-\n-  HeapWord* dense_prefix_top = compaction_top(start_hr);\n-  G1SerialRePrepareClosure re_prepare(serial_cp, dense_prefix_top);\n-\n-  for (uint i = start_serial + 1; i < _heap->max_reserved_regions(); i++) {\n-    if (is_compaction_target(i)) {\n-      HeapRegion* current = _heap->region_at(i);\n-      set_compaction_top(current, current->bottom());\n-      serial_cp->add(current);\n-      current->apply_to_marked_objects(mark_bitmap(), &re_prepare);\n-    }\n-  }\n-  serial_cp->update();\n-}\n+\/\/uint G1FullCollector::truncate_parallel_cps() {\n+\/\/  uint lowest_current = (uint)-1;\n+\/\/  for (uint i = 0; i < workers(); i++) {\n+\/\/    G1FullGCCompactionPoint* cp = compaction_point(i);\n+\/\/    if (cp->has_regions()) {\n+\/\/      lowest_current = MIN2(lowest_current, cp->current_region()->hrm_index());\n+\/\/    }\n+\/\/  }\n+\n+\/\/  for (uint i = 0; i < workers(); i++) {\n+\/\/    G1FullGCCompactionPoint* cp = compaction_point(i);\n+\/\/    if (cp->has_regions()) {\n+\/\/      cp->remove_at_or_above(lowest_current);\n+\/\/    }\n+\/\/  }\n+\/\/  return lowest_current;\n+\/\/}\n+\n+\/\/void G1FullCollector::phase2c_prepare_serial_compaction() {\n+\/\/  GCTraceTime(Debug, gc, phases) debug(\"Phase 2: Prepare serial compaction\", scope()->timer());\n+\/\/  \/\/ At this point, we know that after parallel compaction there will be regions that\n+\/\/  \/\/ are partially compacted into. Thus, the last compaction region of all\n+\/\/  \/\/ compaction queues still have space in them. We try to re-compact these regions\n+\/\/  \/\/ in serial to avoid a premature OOM when the mutator wants to allocate the first\n+\/\/  \/\/ eden region after gc.\n+\/\/\n+\/\/  \/\/ For maximum compaction, we need to re-prepare all objects above the lowest\n+\/\/  \/\/ region among the current regions for all thread compaction points. It may\n+\/\/  \/\/ happen that due to the uneven distribution of objects to parallel threads, holes\n+\/\/  \/\/ have been created as threads compact to different target regions between the\n+\/\/  \/\/ lowest and the highest region in the tails of the compaction points.\n+\/\/\n+\/\/  uint start_serial = truncate_parallel_cps();\n+\/\/  assert(start_serial < _heap->max_reserved_regions(), \"Called on empty parallel compaction queues\");\n+\/\/\n+\/\/  G1FullGCCompactionPoint* serial_cp = serial_compaction_point();\n+\/\/  assert(!serial_cp->is_initialized(), \"sanity!\");\n+\/\/\n+\/\/  HeapRegion* start_hr = _heap->region_at(start_serial);\n+\/\/  serial_cp->add(start_hr);\n+\/\/  serial_cp->initialize(start_hr);\n+\/\/\n+\/\/  HeapWord* dense_prefix_top = compaction_top(start_hr);\n+\/\/  G1SerialRePrepareClosure re_prepare(serial_cp, dense_prefix_top);\n+\/\/\n+\/\/  for (uint i = start_serial + 1; i < _heap->max_reserved_regions(); i++) {\n+\/\/    if (is_compaction_target(i)) {\n+\/\/      HeapRegion* current = _heap->region_at(i);\n+\/\/      set_compaction_top(current, current->bottom());\n+\/\/      serial_cp->add(current);\n+\/\/      current->apply_to_marked_objects(mark_bitmap(), &re_prepare);\n+\/\/    }\n+\/\/  }\n+\/\/  serial_cp->update();\n+\/\/}\n","filename":"src\/hotspot\/share\/gc\/g1\/g1FullCollector.cpp","additions":62,"deletions":58,"binary":false,"changes":120,"status":"modified"},{"patch":"@@ -97,1 +97,2 @@\n-  marker->preserved_stack()->adjust_during_full_gc();\n+  const SlidingForwarding* const forwarding = G1CollectedHeap::heap()->forwarding();\n+  marker->preserved_stack()->adjust_during_full_gc(forwarding);\n","filename":"src\/hotspot\/share\/gc\/g1\/g1FullGCAdjustTask.cpp","additions":2,"deletions":1,"binary":false,"changes":3,"status":"modified"},{"patch":"@@ -33,0 +33,1 @@\n+#include \"gc\/shared\/slidingForwarding.inline.hpp\"\n@@ -45,1 +46,1 @@\n-    HeapWord* destination = cast_from_oop<HeapWord*>(obj->forwardee());\n+    HeapWord* destination = cast_from_oop<HeapWord*>(_forwarding->forwardee(obj));\n","filename":"src\/hotspot\/share\/gc\/g1\/g1FullGCCompactTask.cpp","additions":2,"deletions":1,"binary":false,"changes":3,"status":"modified"},{"patch":"@@ -36,0 +36,1 @@\n+class SlidingForwarding;\n@@ -54,0 +55,1 @@\n+    const SlidingForwarding* const _forwarding;\n@@ -56,1 +58,3 @@\n-    G1CompactRegionClosure(G1CMBitMap* bitmap) : _bitmap(bitmap) { }\n+    G1CompactRegionClosure(G1CMBitMap* bitmap) :\n+      _bitmap(bitmap),\n+      _forwarding(G1CollectedHeap::heap()->forwarding()) { }\n","filename":"src\/hotspot\/share\/gc\/g1\/g1FullGCCompactTask.hpp","additions":5,"deletions":1,"binary":false,"changes":6,"status":"modified"},{"patch":"@@ -29,0 +29,1 @@\n+#include \"gc\/shared\/slidingForwarding.inline.hpp\"\n@@ -94,1 +95,1 @@\n-void G1FullGCCompactionPoint::forward(oop object, size_t size) {\n+void G1FullGCCompactionPoint::forward(SlidingForwarding* const forwarding, oop object, size_t size) {\n@@ -104,1 +105,1 @@\n-    object->forward_to(cast_to_oop(_compaction_top));\n+    forwarding->forward_to(object, cast_to_oop(_compaction_top));\n","filename":"src\/hotspot\/share\/gc\/g1\/g1FullGCCompactionPoint.cpp","additions":3,"deletions":2,"binary":false,"changes":5,"status":"modified"},{"patch":"@@ -34,0 +34,1 @@\n+class SlidingForwarding;\n@@ -55,1 +56,1 @@\n-  void forward(oop object, size_t size);\n+  void forward(SlidingForwarding* const forwarding, oop object, size_t size);\n","filename":"src\/hotspot\/share\/gc\/g1\/g1FullGCCompactionPoint.hpp","additions":2,"deletions":1,"binary":false,"changes":3,"status":"modified"},{"patch":"@@ -28,0 +28,1 @@\n+#include \"gc\/g1\/g1CollectedHeap.hpp\"\n@@ -35,0 +36,1 @@\n+class SlidingForwarding;\n@@ -78,0 +80,1 @@\n+  const SlidingForwarding* const _forwarding;\n@@ -81,1 +84,3 @@\n-  G1AdjustClosure(G1FullCollector* collector) : _collector(collector) { }\n+  G1AdjustClosure(G1FullCollector* collector) :\n+    _collector(collector),\n+    _forwarding(G1CollectedHeap::heap()->forwarding()) { }\n","filename":"src\/hotspot\/share\/gc\/g1\/g1FullGCOopClosures.hpp","additions":6,"deletions":1,"binary":false,"changes":7,"status":"modified"},{"patch":"@@ -35,0 +35,1 @@\n+#include \"gc\/shared\/slidingForwarding.inline.hpp\"\n@@ -69,1 +70,1 @@\n-    oop forwardee = obj->forwardee();\n+    oop forwardee = _forwarding->forwardee(obj);\n","filename":"src\/hotspot\/share\/gc\/g1\/g1FullGCOopClosures.inline.hpp","additions":2,"deletions":1,"binary":false,"changes":3,"status":"modified"},{"patch":"@@ -36,0 +36,1 @@\n+#include \"gc\/shared\/slidingForwarding.inline.hpp\"\n@@ -110,1 +111,1 @@\n-    _cp(cp) { }\n+    _cp(cp), _forwarding(G1CollectedHeap::heap()->forwarding()) { }\n@@ -114,1 +115,1 @@\n-  _cp->forward(object, size);\n+  _cp->forward(_forwarding, object, size);\n","filename":"src\/hotspot\/share\/gc\/g1\/g1FullGCPrepareTask.cpp","additions":3,"deletions":2,"binary":false,"changes":5,"status":"modified"},{"patch":"@@ -35,0 +35,1 @@\n+class SlidingForwarding;\n@@ -95,0 +96,1 @@\n+    SlidingForwarding* const _forwarding;\n@@ -104,11 +106,11 @@\n-class G1SerialRePrepareClosure : public StackObj {\n-  G1FullGCCompactionPoint* _cp;\n-  HeapWord* _dense_prefix_top;\n-\n-public:\n-  G1SerialRePrepareClosure(G1FullGCCompactionPoint* hrcp, HeapWord* dense_prefix_top) :\n-    _cp(hrcp),\n-    _dense_prefix_top(dense_prefix_top) { }\n-\n-  inline size_t apply(oop obj);\n-};\n+\/\/class G1SerialRePrepareClosure : public StackObj {\n+\/\/  G1FullGCCompactionPoint* _cp;\n+\/\/  HeapWord* _dense_prefix_top;\n+\/\/\n+\/\/public:\n+\/\/  G1SerialRePrepareClosure(G1FullGCCompactionPoint* hrcp, HeapWord* dense_prefix_top) :\n+\/\/    _cp(hrcp),\n+\/\/    _dense_prefix_top(dense_prefix_top) { }\n+\/\/\n+\/\/  inline size_t apply(oop obj);\n+\/\/};\n","filename":"src\/hotspot\/share\/gc\/g1\/g1FullGCPrepareTask.hpp","additions":13,"deletions":11,"binary":false,"changes":24,"status":"modified"},{"patch":"@@ -114,15 +114,15 @@\n-inline size_t G1SerialRePrepareClosure::apply(oop obj) {\n-  if (obj->is_forwarded()) {\n-    \/\/ We skip objects compiled into the first region or\n-    \/\/ into regions not part of the serial compaction point.\n-    if (cast_from_oop<HeapWord*>(obj->forwardee()) < _dense_prefix_top) {\n-      return obj->size();\n-    }\n-  }\n-\n-  \/\/ Get size and forward.\n-  size_t size = obj->size();\n-  _cp->forward(obj, size);\n-\n-  return size;\n-}\n+\/\/inline size_t G1SerialRePrepareClosure::apply(oop obj) {\n+\/\/  if (obj->is_forwarded()) {\n+\/\/    \/\/ We skip objects compiled into the first region or\n+\/\/    \/\/ into regions not part of the serial compaction point.\n+\/\/    if (cast_from_oop<HeapWord*>(obj->forwardee()) < _dense_prefix_top) {\n+\/\/      return obj->size();\n+\/\/    }\n+\/\/  }\n+\/\/\n+\/\/  \/\/ Get size and forward.\n+\/\/  size_t size = obj->size();\n+\/\/  _cp->forward(obj, size);\n+\/\/\n+\/\/  return size;\n+\/\/}\n","filename":"src\/hotspot\/share\/gc\/g1\/g1FullGCPrepareTask.inline.hpp","additions":15,"deletions":15,"binary":false,"changes":30,"status":"modified"},{"patch":"@@ -231,1 +231,1 @@\n-      forwardee = cast_to_oop(m.decode_pointer());\n+      forwardee = obj->forwardee(m);\n","filename":"src\/hotspot\/share\/gc\/g1\/g1OopClosures.inline.hpp","additions":1,"deletions":1,"binary":false,"changes":2,"status":"modified"},{"patch":"@@ -209,1 +209,1 @@\n-    obj = cast_to_oop(m.decode_pointer());\n+    obj = obj->forwardee(m);\n@@ -223,1 +223,0 @@\n-  assert(from_obj->is_objArray(), \"must be obj array\");\n@@ -253,1 +252,0 @@\n-  assert(from_obj->is_objArray(), \"precondition\");\n@@ -380,1 +378,1 @@\n-                                                  oop const old, size_t word_sz, uint age,\n+                                                  oop const old, Klass* klass, size_t word_sz, uint age,\n@@ -384,1 +382,1 @@\n-    _g1h->gc_tracer_stw()->report_promotion_in_new_plab_event(old->klass(), word_sz * HeapWordSize, age,\n+    _g1h->gc_tracer_stw()->report_promotion_in_new_plab_event(klass, word_sz * HeapWordSize, age,\n@@ -388,1 +386,1 @@\n-    _g1h->gc_tracer_stw()->report_promotion_outside_plab_event(old->klass(), word_sz * HeapWordSize, age,\n+    _g1h->gc_tracer_stw()->report_promotion_outside_plab_event(klass, word_sz * HeapWordSize, age,\n@@ -396,0 +394,1 @@\n+                                                   Klass* klass,\n@@ -418,1 +417,1 @@\n-      report_promotion_event(*dest_attr, old, word_sz, age, obj_ptr, node_index);\n+      report_promotion_event(*dest_attr, old, klass, word_sz, age, obj_ptr, node_index);\n@@ -453,0 +452,4 @@\n+  if (old_mark.is_marked()) {\n+    \/\/ Already forwarded by somebody else, return forwardee.\n+    return old->forwardee(old_mark);\n+  }\n@@ -455,0 +458,3 @@\n+#ifdef _LP64\n+  Klass* klass = old_mark.safe_klass();\n+#else\n@@ -456,0 +462,1 @@\n+#endif\n@@ -468,1 +475,1 @@\n-    obj_ptr = allocate_copy_slow(&dest_attr, old, word_sz, age, node_index);\n+    obj_ptr = allocate_copy_slow(&dest_attr, old, klass, word_sz, age, node_index);\n@@ -621,1 +628,1 @@\n-  oop forward_ptr = old->forward_to_atomic(old, m, memory_order_relaxed);\n+  oop forward_ptr = old->forward_to_self_atomic(m, memory_order_relaxed);\n","filename":"src\/hotspot\/share\/gc\/g1\/g1ParScanThreadState.cpp","additions":16,"deletions":9,"binary":false,"changes":25,"status":"modified"},{"patch":"@@ -164,0 +164,1 @@\n+                               Klass* klass,\n@@ -198,1 +199,1 @@\n-                              oop const old, size_t word_sz, uint age,\n+                              oop const old, Klass* klass, size_t word_sz, uint age,\n","filename":"src\/hotspot\/share\/gc\/g1\/g1ParScanThreadState.hpp","additions":2,"deletions":1,"binary":false,"changes":3,"status":"modified"},{"patch":"@@ -398,1 +398,1 @@\n-    HeapWord* test_addr = cast_from_oop<HeapWord*>(obj) + 1;\n+    HeapWord* test_addr = cast_from_oop<HeapWord*>(obj);\n","filename":"src\/hotspot\/share\/gc\/parallel\/psOldGen.cpp","additions":1,"deletions":1,"binary":false,"changes":2,"status":"modified"},{"patch":"@@ -293,1 +293,0 @@\n-  assert(old->is_objArray(), \"invariant\");\n@@ -331,1 +330,1 @@\n-  if (obj->forward_to_atomic(obj, obj_mark) == nullptr) {\n+  if (obj->forward_to_self_atomic(obj_mark) == nullptr) {\n","filename":"src\/hotspot\/share\/gc\/parallel\/psPromotionManager.cpp","additions":1,"deletions":2,"binary":false,"changes":3,"status":"modified"},{"patch":"@@ -108,1 +108,1 @@\n-  inline void promotion_trace_event(oop new_obj, oop old_obj, size_t obj_size,\n+  inline void promotion_trace_event(oop new_obj, oop old_obj, Klass* klass, size_t obj_size,\n","filename":"src\/hotspot\/share\/gc\/parallel\/psPromotionManager.hpp","additions":1,"deletions":1,"binary":false,"changes":2,"status":"modified"},{"patch":"@@ -66,1 +66,1 @@\n-inline void PSPromotionManager::promotion_trace_event(oop new_obj, oop old_obj,\n+inline void PSPromotionManager::promotion_trace_event(oop new_obj, oop old_obj, Klass* klass,\n@@ -79,1 +79,1 @@\n-        gc_tracer->report_promotion_in_new_plab_event(old_obj->klass(), obj_bytes,\n+        gc_tracer->report_promotion_in_new_plab_event(klass, obj_bytes,\n@@ -86,1 +86,1 @@\n-        gc_tracer->report_promotion_outside_plab_event(old_obj->klass(), obj_bytes,\n+        gc_tracer->report_promotion_outside_plab_event(klass, obj_bytes,\n@@ -150,1 +150,1 @@\n-    return cast_to_oop(m.decode_pointer());\n+    return o->forwardee(m);\n@@ -166,1 +166,6 @@\n-  size_t new_obj_size = o->size();\n+#ifdef _LP64\n+  Klass* klass = test_mark.safe_klass();\n+#else\n+  Klass* klass = o->klass();\n+#endif\n+  size_t new_obj_size = o->size_given_klass(klass);\n@@ -181,1 +186,1 @@\n-          promotion_trace_event(new_obj, o, new_obj_size, age, false, nullptr);\n+          promotion_trace_event(new_obj, o, klass, new_obj_size, age, false, nullptr);\n@@ -191,1 +196,1 @@\n-            promotion_trace_event(new_obj, o, new_obj_size, age, false, &_young_lab);\n+            promotion_trace_event(new_obj, o, klass, new_obj_size, age, false, &_young_lab);\n@@ -217,1 +222,1 @@\n-          promotion_trace_event(new_obj, o, new_obj_size, age, true, nullptr);\n+          promotion_trace_event(new_obj, o, klass, new_obj_size, age, true, nullptr);\n@@ -227,1 +232,1 @@\n-            promotion_trace_event(new_obj, o, new_obj_size, age, true, &_old_lab);\n+            promotion_trace_event(new_obj, o, klass, new_obj_size, age, true, &_old_lab);\n@@ -250,3 +255,5 @@\n-  \/\/ Parallel GC claims with a release - so other threads might access this object\n-  \/\/ after claiming and they should see the \"completed\" object.\n-  ContinuationGCSupport::transform_stack_chunk(new_obj);\n+  if (!new_obj->mark().is_marked()) {\n+    \/\/ Parallel GC claims with a release - so other threads might access this object\n+    \/\/ after claiming and they should see the \"completed\" object.\n+    ContinuationGCSupport::transform_stack_chunk(new_obj);\n+  }\n","filename":"src\/hotspot\/share\/gc\/parallel\/psPromotionManager.inline.hpp","additions":19,"deletions":12,"binary":false,"changes":31,"status":"modified"},{"patch":"@@ -873,0 +873,10 @@\n+#ifdef _LP64\n+        oop forwardee = obj->forwardee();\n+        markWord header = forwardee->mark();\n+        if (header.has_displaced_mark_helper()) {\n+          header = header.displaced_mark_helper();\n+        }\n+        assert(UseCompressedClassPointers, \"assume +UseCompressedClassPointers\");\n+        narrowKlass nklass = header.narrow_klass();\n+        obj->set_mark(markWord::prototype().set_narrow_klass(nklass));\n+#else\n@@ -874,0 +884,1 @@\n+#endif\n@@ -897,1 +908,1 @@\n-  old->forward_to(old);\n+  old->forward_to_self();\n","filename":"src\/hotspot\/share\/gc\/serial\/defNewGeneration.cpp","additions":12,"deletions":1,"binary":false,"changes":13,"status":"modified"},{"patch":"@@ -259,0 +259,2 @@\n+  AdjustPointerClosure adjust_pointer_closure(gch->forwarding());\n+  CLDToOopClosure      adjust_cld_closure(&adjust_pointer_closure, ClassLoaderData::_claim_stw_fullgc_adjust);\n","filename":"src\/hotspot\/share\/gc\/serial\/genMarkSweep.cpp","additions":2,"deletions":0,"binary":false,"changes":2,"status":"modified"},{"patch":"@@ -29,0 +29,1 @@\n+#include \"gc\/shared\/genCollectedHeap.hpp\"\n@@ -66,1 +67,0 @@\n-CLDToOopClosure    MarkSweep::adjust_cld_closure(&adjust_pointer_closure, ClassLoaderData::_claim_stw_fullgc_adjust);\n@@ -148,2 +148,2 @@\n-void PreservedMark::adjust_pointer() {\n-  MarkSweep::adjust_pointer(&_obj);\n+void PreservedMark::adjust_pointer(const SlidingForwarding* const forwarding) {\n+  MarkSweep::adjust_pointer(forwarding, &_obj);\n@@ -183,0 +183,2 @@\n+  ContinuationGCSupport::transform_stack_chunk(obj);\n+\n@@ -186,0 +188,8 @@\n+#ifdef _LP64\n+  markWord real_mark = mark;\n+  if (real_mark.has_displaced_mark_helper()) {\n+    real_mark = real_mark.displaced_mark_helper();\n+  }\n+  Klass* klass = real_mark.klass();\n+  obj->set_mark(klass->prototype_header().set_marked());\n+#else\n@@ -187,2 +197,1 @@\n-\n-  ContinuationGCSupport::transform_stack_chunk(obj);\n+#endif\n@@ -211,2 +220,0 @@\n-AdjustPointerClosure MarkSweep::adjust_pointer_closure;\n-\n@@ -214,0 +221,2 @@\n+  const SlidingForwarding* const forwarding = GenCollectedHeap::heap()->forwarding();\n+\n@@ -216,1 +225,1 @@\n-    _preserved_marks[i].adjust_pointer();\n+    _preserved_marks[i].adjust_pointer(forwarding);\n@@ -223,1 +232,1 @@\n-    p->adjust_pointer();\n+    p->adjust_pointer(forwarding);\n","filename":"src\/hotspot\/share\/gc\/serial\/markSweep.cpp","additions":18,"deletions":9,"binary":false,"changes":27,"status":"modified"},{"patch":"@@ -41,0 +41,1 @@\n+class SlidingForwarding;\n@@ -127,2 +128,0 @@\n-  static AdjustPointerClosure adjust_pointer_closure;\n-  static CLDToOopClosure      adjust_cld_closure;\n@@ -145,1 +144,1 @@\n-  static size_t adjust_pointers(oop obj);\n+  static size_t adjust_pointers(const SlidingForwarding* const forwarding, oop obj);\n@@ -149,1 +148,1 @@\n-  template <class T> static inline void adjust_pointer(T* p);\n+  template <class T> static inline void adjust_pointer(const SlidingForwarding* const forwarding, T* p);\n@@ -183,0 +182,2 @@\n+private:\n+  const SlidingForwarding* const _forwarding;\n@@ -184,0 +185,1 @@\n+  AdjustPointerClosure(const SlidingForwarding* forwarding) : _forwarding(forwarding) {}\n@@ -197,1 +199,1 @@\n-  void adjust_pointer();\n+  void adjust_pointer(const SlidingForwarding* const forwarding);\n","filename":"src\/hotspot\/share\/gc\/serial\/markSweep.hpp","additions":7,"deletions":5,"binary":false,"changes":12,"status":"modified"},{"patch":"@@ -30,0 +30,1 @@\n+#include \"gc\/shared\/slidingForwarding.inline.hpp\"\n@@ -42,1 +43,1 @@\n-template <class T> inline void MarkSweep::adjust_pointer(T* p) {\n+template <class T> inline void MarkSweep::adjust_pointer(const SlidingForwarding* const forwarding, T* p) {\n@@ -48,2 +49,4 @@\n-    if (obj->is_forwarded()) {\n-      oop new_obj = obj->forwardee();\n+    markWord header = obj->mark();\n+    if (header.is_marked()) {\n+      oop new_obj = forwarding->forwardee(obj);\n+      assert(new_obj != NULL, \"must be forwarded\");\n@@ -57,1 +60,1 @@\n-void AdjustPointerClosure::do_oop_work(T* p)           { MarkSweep::adjust_pointer(p); }\n+void AdjustPointerClosure::do_oop_work(T* p)           { MarkSweep::adjust_pointer(_forwarding, p); }\n@@ -61,2 +64,3 @@\n-inline size_t MarkSweep::adjust_pointers(oop obj) {\n-  return obj->oop_iterate_size(&MarkSweep::adjust_pointer_closure);\n+inline size_t MarkSweep::adjust_pointers(const SlidingForwarding* const forwarding, oop obj) {\n+  AdjustPointerClosure cl(forwarding);\n+  return obj->oop_iterate_size(&cl);\n","filename":"src\/hotspot\/share\/gc\/serial\/markSweep.inline.hpp","additions":10,"deletions":6,"binary":false,"changes":16,"status":"modified"},{"patch":"@@ -226,4 +226,0 @@\n-  if (is_in(object->klass_raw())) {\n-    return false;\n-  }\n-\n@@ -256,2 +252,4 @@\n-  _filler_array_max_size = align_object_size(filler_array_hdr_size() +\n-                                             max_len \/ elements_per_word);\n+  int header_size_in_bytes = arrayOopDesc::base_offset_in_bytes(T_INT);\n+  assert(header_size_in_bytes % sizeof(jint) == 0, \"must be aligned to int\");\n+  int header_size_in_ints = header_size_in_bytes \/ sizeof(jint);\n+  _filler_array_max_size = align_object_size((header_size_in_ints + max_len) \/ elements_per_word);\n@@ -418,1 +416,3 @@\n-  size_t max_int_size = typeArrayOopDesc::header_size(T_INT) +\n+  int header_size_in_bytes = typeArrayOopDesc::base_offset_in_bytes(T_INT);\n+  assert(header_size_in_bytes % sizeof(jint) == 0, \"header size must align to int\");\n+  size_t max_int_size = header_size_in_bytes \/ HeapWordSize +\n@@ -424,4 +424,0 @@\n-size_t CollectedHeap::filler_array_hdr_size() {\n-  return align_object_offset(arrayOopDesc::header_size(T_INT)); \/\/ align to Long\n-}\n-\n@@ -429,1 +425,2 @@\n-  return align_object_size(filler_array_hdr_size()); \/\/ align to MinObjAlignment\n+  int aligned_header_size_words = align_up(arrayOopDesc::base_offset_in_bytes(T_INT), HeapWordSize) \/ HeapWordSize;\n+  return align_object_size(aligned_header_size_words); \/\/ align to MinObjAlignment\n@@ -433,2 +430,3 @@\n-  Copy::fill_to_words(start + filler_array_hdr_size(),\n-                      words - filler_array_hdr_size(), value);\n+  int payload_start = align_up(arrayOopDesc::base_offset_in_bytes(T_INT), HeapWordSize) \/ HeapWordSize;\n+  Copy::fill_to_words(start + payload_start,\n+                      words - payload_start, value);\n@@ -458,2 +456,3 @@\n-  const size_t payload_size = words - filler_array_hdr_size();\n-  const size_t len = payload_size * HeapWordSize \/ sizeof(jint);\n+  const size_t payload_size_bytes = words * HeapWordSize - arrayOopDesc::base_offset_in_bytes(T_INT);\n+  assert(payload_size_bytes % sizeof(jint) == 0, \"must be int aligned\");\n+  const size_t len = payload_size_bytes \/ sizeof(jint);\n","filename":"src\/hotspot\/share\/gc\/shared\/collectedHeap.cpp","additions":15,"deletions":16,"binary":false,"changes":31,"status":"modified"},{"patch":"@@ -166,1 +166,0 @@\n-  static inline size_t filler_array_hdr_size();\n","filename":"src\/hotspot\/share\/gc\/shared\/collectedHeap.hpp","additions":0,"deletions":1,"binary":false,"changes":1,"status":"modified"},{"patch":"@@ -57,0 +57,1 @@\n+#include \"gc\/shared\/slidingForwarding.hpp\"\n@@ -119,0 +120,1 @@\n+  _forwarding = new SlidingForwarding(_reserved);\n@@ -1037,0 +1039,1 @@\n+  _forwarding->clear();\n","filename":"src\/hotspot\/share\/gc\/shared\/genCollectedHeap.cpp","additions":3,"deletions":0,"binary":false,"changes":3,"status":"modified"},{"patch":"@@ -38,0 +38,1 @@\n+class SlidingForwarding;\n@@ -90,0 +91,2 @@\n+  SlidingForwarding* _forwarding;\n+\n@@ -315,0 +318,4 @@\n+  SlidingForwarding* forwarding() const {\n+    return _forwarding;\n+  }\n+\n","filename":"src\/hotspot\/share\/gc\/shared\/genCollectedHeap.hpp","additions":7,"deletions":0,"binary":false,"changes":7,"status":"modified"},{"patch":"@@ -34,0 +34,1 @@\n+#include \"memory\/metaspace.hpp\"\n@@ -403,0 +404,15 @@\n+JVMFlag::Error CompressedClassSpaceSizeConstraintFunc(size_t value, bool verbose) {\n+#ifdef _LP64\n+  \/\/ There is no minimal value check, although class space will be transparently enlarged\n+  \/\/ to a multiple of metaspace root chunk size (4m).\n+  \/\/ The max. value of class space size depends on narrow klass pointer encoding range size\n+  \/\/ and CDS, see metaspace.cpp.\n+  if (value > Metaspace::max_class_space_size()) {\n+    JVMFlag::printError(verbose, \"CompressedClassSpaceSize \" SIZE_FORMAT \" too large (max: \" SIZE_FORMAT \")\\n\",\n+                        value, Metaspace::max_class_space_size());\n+    return JVMFlag::VIOLATES_CONSTRAINT;\n+  }\n+#endif\n+  return JVMFlag::SUCCESS;\n+}\n+\n","filename":"src\/hotspot\/share\/gc\/shared\/jvmFlagConstraintsGC.cpp","additions":16,"deletions":0,"binary":false,"changes":16,"status":"modified"},{"patch":"@@ -68,0 +68,1 @@\n+ f(size_t, CompressedClassSpaceSizeConstraintFunc)             \\\n","filename":"src\/hotspot\/share\/gc\/shared\/jvmFlagConstraintsGC.hpp","additions":1,"deletions":0,"binary":false,"changes":1,"status":"modified"},{"patch":"@@ -402,1 +402,0 @@\n-  oopDesc::set_klass_gap(mem, 0);\n@@ -408,2 +407,0 @@\n-  \/\/ May be bootstrapping\n-  oopDesc::set_mark(mem, markWord::prototype());\n@@ -413,0 +410,4 @@\n+#ifdef _LP64\n+  oopDesc::release_set_mark(mem, _klass->prototype_header());\n+#else\n+  oopDesc::set_mark(mem, _klass->prototype_header());\n@@ -414,0 +415,1 @@\n+#endif\n@@ -427,1 +429,1 @@\n-  const size_t hs = arrayOopDesc::header_size(array_klass->element_type());\n+  const size_t hs = align_up(arrayOopDesc::base_offset_in_bytes(array_klass->element_type()), HeapWordSize) \/ HeapWordSize;\n","filename":"src\/hotspot\/share\/gc\/shared\/memAllocator.cpp","additions":6,"deletions":4,"binary":false,"changes":10,"status":"modified"},{"patch":"@@ -32,0 +32,1 @@\n+#include \"runtime\/objectMonitor.hpp\"\n@@ -265,0 +266,1 @@\n+        ObjectMonitor::maybe_deflate_dead(ptr);\n","filename":"src\/hotspot\/share\/gc\/shared\/oopStorage.inline.hpp","additions":2,"deletions":0,"binary":false,"changes":2,"status":"modified"},{"patch":"@@ -27,0 +27,1 @@\n+#include \"gc\/shared\/slidingForwarding.inline.hpp\"\n@@ -43,0 +44,2 @@\n+\/\/ TODO: This method is unused, except in the gunit test. Change the test\n+\/\/ to exercise the updated method below instead, and remove this one.\n@@ -55,0 +58,12 @@\n+void PreservedMarks::adjust_during_full_gc(const SlidingForwarding* const forwarding) {\n+  StackIterator<OopAndMarkWord, mtGC> iter(_stack);\n+  while (!iter.is_empty()) {\n+    OopAndMarkWord* elem = iter.next_addr();\n+\n+    oop obj = elem->get_oop();\n+    if (obj->is_forwarded()) {\n+      elem->set_oop(forwarding->forwardee(obj));\n+    }\n+  }\n+}\n+\n","filename":"src\/hotspot\/share\/gc\/shared\/preservedMarks.cpp","additions":15,"deletions":0,"binary":false,"changes":15,"status":"modified"},{"patch":"@@ -35,0 +35,1 @@\n+class SlidingForwarding;\n@@ -66,0 +67,2 @@\n+  \/\/ TODO: This method is unused, except in the gunit test. Change the test\n+  \/\/ to exercise the updated method below instead, and remove this one.\n@@ -68,0 +71,2 @@\n+  void adjust_during_full_gc(const SlidingForwarding* const forwarding);\n+\n","filename":"src\/hotspot\/share\/gc\/shared\/preservedMarks.hpp","additions":5,"deletions":0,"binary":false,"changes":5,"status":"modified"},{"patch":"@@ -0,0 +1,69 @@\n+\/*\n+ * Copyright (c) 2021, Red Hat, Inc. All rights reserved.\n+ * DO NOT ALTER OR REMOVE COPYRIGHT NOTICES OR THIS FILE HEADER.\n+ *\n+ * This code is free software; you can redistribute it and\/or modify it\n+ * under the terms of the GNU General Public License version 2 only, as\n+ * published by the Free Software Foundation.\n+ *\n+ * This code is distributed in the hope that it will be useful, but WITHOUT\n+ * ANY WARRANTY; without even the implied warranty of MERCHANTABILITY or\n+ * FITNESS FOR A PARTICULAR PURPOSE.  See the GNU General Public License\n+ * version 2 for more details (a copy is included in the LICENSE file that\n+ * accompanied this code).\n+ *\n+ * You should have received a copy of the GNU General Public License version\n+ * 2 along with this work; if not, write to the Free Software Foundation,\n+ * Inc., 51 Franklin St, Fifth Floor, Boston, MA 02110-1301 USA.\n+ *\n+ * Please contact Oracle, 500 Oracle Parkway, Redwood Shores, CA 94065 USA\n+ * or visit www.oracle.com if you need additional information or have any\n+ * questions.\n+ *\n+ *\/\n+\n+#include \"precompiled.hpp\"\n+#include \"gc\/shared\/slidingForwarding.hpp\"\n+\n+#ifdef _LP64\n+HeapWord* const SlidingForwarding::UNUSED_BASE = reinterpret_cast<HeapWord*>(0x1);\n+#endif\n+\n+SlidingForwarding::SlidingForwarding(MemRegion heap)\n+#ifdef _LP64\n+        : _heap_start(heap.start()),\n+          _num_regions(((heap.end() - heap.start()) >> NUM_COMPRESSED_BITS) + 1),\n+          _region_size_words_shift(NUM_COMPRESSED_BITS),\n+          _target_base_table(NEW_C_HEAP_ARRAY(HeapWord*, _num_regions * 2, mtGC)) {\n+  assert(_region_size_words_shift <= NUM_COMPRESSED_BITS, \"regions must not be larger than maximum addressing bits allow\");\n+#else\n+  {\n+#endif\n+}\n+\n+SlidingForwarding::SlidingForwarding(MemRegion heap, size_t region_size_words_shift)\n+#ifdef _LP64\n+        : _heap_start(heap.start()),\n+          _num_regions(((heap.end() - heap.start()) >> region_size_words_shift) + 1),\n+          _region_size_words_shift(region_size_words_shift),\n+          _target_base_table(NEW_C_HEAP_ARRAY(HeapWord*, _num_regions * (ONE << NUM_REGION_BITS), mtGC)) {\n+  assert(region_size_words_shift <= NUM_COMPRESSED_BITS, \"regions must not be larger than maximum addressing bits allow\");\n+#else\n+  {\n+#endif\n+}\n+\n+SlidingForwarding::~SlidingForwarding() {\n+#ifdef _LP64\n+  FREE_C_HEAP_ARRAY(HeapWord*, _target_base_table);\n+#endif\n+}\n+\n+void SlidingForwarding::clear() {\n+#ifdef _LP64\n+  size_t max = _num_regions * (ONE << NUM_REGION_BITS);\n+  for (size_t i = 0; i < max; i++) {\n+    _target_base_table[i] = UNUSED_BASE;\n+  }\n+#endif\n+}\n","filename":"src\/hotspot\/share\/gc\/shared\/slidingForwarding.cpp","additions":69,"deletions":0,"binary":false,"changes":69,"status":"added"},{"patch":"@@ -0,0 +1,112 @@\n+\/*\n+ * Copyright (c) 2021, Red Hat, Inc. All rights reserved.\n+ * DO NOT ALTER OR REMOVE COPYRIGHT NOTICES OR THIS FILE HEADER.\n+ *\n+ * This code is free software; you can redistribute it and\/or modify it\n+ * under the terms of the GNU General Public License version 2 only, as\n+ * published by the Free Software Foundation.\n+ *\n+ * This code is distributed in the hope that it will be useful, but WITHOUT\n+ * ANY WARRANTY; without even the implied warranty of MERCHANTABILITY or\n+ * FITNESS FOR A PARTICULAR PURPOSE.  See the GNU General Public License\n+ * version 2 for more details (a copy is included in the LICENSE file that\n+ * accompanied this code).\n+ *\n+ * You should have received a copy of the GNU General Public License version\n+ * 2 along with this work; if not, write to the Free Software Foundation,\n+ * Inc., 51 Franklin St, Fifth Floor, Boston, MA 02110-1301 USA.\n+ *\n+ * Please contact Oracle, 500 Oracle Parkway, Redwood Shores, CA 94065 USA\n+ * or visit www.oracle.com if you need additional information or have any\n+ * questions.\n+ *\n+ *\/\n+\n+#ifndef SHARE_GC_SHARED_SLIDINGFORWARDING_HPP\n+#define SHARE_GC_SHARED_SLIDINGFORWARDING_HPP\n+\n+#include \"memory\/allocation.hpp\"\n+#include \"memory\/memRegion.hpp\"\n+#include \"oops\/oopsHierarchy.hpp\"\n+\n+\/**\n+ * SlidingForwarding is a method to store forwarding information in a compressed form into the object header,\n+ * that has been specifically designed for sliding compaction GCs.\n+ * It avoids overriding the compressed class pointer in the upper bits of the header, which would otherwise\n+ * be lost. SlidingForwarding requires only small side tables and guarantees constant-time access and modification.\n+ *\n+ * The idea is to use a pointer compression scheme very similar to the one that is used for compressed oops.\n+ * We divide the heap into number of logical regions. Each region spans maximum of 2^NUM_BITS words.\n+ * We take advantage of the fact that sliding compaction can forward objects from one region to a maximum of\n+ * two regions (including itself, but that does not really matter). We need 1 bit to indicate which region is forwarded\n+ * into. We also currently require the two lowest header bits to indicate that the object is forwarded.\n+ *\n+ * For addressing, we need a table with N*2 entries, for N logical regions. For each region, it gives the base\n+ * address of the two target regions, or a special placeholder if not used.\n+ *\n+ * Adding a forwarding then works as follows:\n+ * Given an original address 'orig', and a 'target' address:\n+ * - Look-up first target base of region of orig. If not yet used,\n+ *   establish it to be the base of region of target address. Use that base in step 3.\n+ * - Else, if first target base is already used, check second target base. This must either be unused, or the\n+ *   base of the region of our target address. If unused, establish it to be the base of the region of our target\n+ *   address. Use that base for next step.\n+ * - Now we found a base address. Encode the target address with that base into lowest NUM_BITS bits, and shift\n+ *   that up by 3 bits. Set the 3rd bit if we used the secondary target base, otherwise leave it at 0. Set the\n+ *   lowest two bits to indicate that the object has been forwarded. Store that in the lowest NUM_BITS+3 bits of the\n+ *   original object's header.\n+ *\n+ * Similarily, looking up the target address, given an original object address works as follows:\n+ * - Load lowest NUM_BITS + 3 from original object header. Extract target region bit and compressed address bits.\n+ * - Depending on target region bit, load base address from the target base table by looking up the corresponding entry\n+ *   for the region of the original object.\n+ * - Decode the target address by using the target base address and the compressed address bits.\n+ *\/\n+\n+class SlidingForwarding : public CHeapObj<mtGC> {\n+#ifdef _LP64\n+private:\n+  static const int NUM_REGION_BITS = 1;\n+\n+  static const uintptr_t ONE = 1ULL;\n+\n+  static const size_t NUM_REGIONS = ONE << NUM_REGION_BITS;\n+\n+  \/\/ We need the lowest three bits to indicate a forwarded object and self-forwarding.\n+  static const int BASE_SHIFT = 3;\n+\n+  \/\/ The compressed address bits start here.\n+  static const int COMPRESSED_BITS_SHIFT = BASE_SHIFT + NUM_REGION_BITS;\n+\n+  \/\/ How many bits we use for the compressed pointer (we are going to need one more bit to indicate target region, and\n+  \/\/ two lowest bits to mark objects as forwarded)\n+  static const int NUM_COMPRESSED_BITS = 32 - BASE_SHIFT - NUM_REGION_BITS;\n+\n+  \/\/ Indicates an usused base address in the target base table. We cannot use 0, because that may already be\n+  \/\/ a valid base address in zero-based heaps. 0x1 is safe because heap base addresses must be aligned by 2^X.\n+  static HeapWord* const UNUSED_BASE;\n+\n+  HeapWord*  const _heap_start;\n+  size_t     const _num_regions;\n+  size_t     const _region_size_words_shift;\n+  HeapWord** const _target_base_table;\n+\n+  inline size_t region_index_containing(HeapWord* addr) const;\n+  inline bool region_contains(HeapWord* region_base, HeapWord* addr) const;\n+\n+  inline uintptr_t encode_forwarding(HeapWord* original, HeapWord* target);\n+  inline HeapWord* decode_forwarding(HeapWord* original, uintptr_t encoded) const;\n+\n+#endif\n+\n+public:\n+  SlidingForwarding(MemRegion heap);\n+  SlidingForwarding(MemRegion heap, size_t num_regions);\n+  ~SlidingForwarding();\n+\n+  void clear();\n+  inline void forward_to(oop original, oop target);\n+  inline oop forwardee(oop original) const;\n+};\n+\n+#endif \/\/ SHARE_GC_SHARED_SLIDINGFORWARDING_HPP\n","filename":"src\/hotspot\/share\/gc\/shared\/slidingForwarding.hpp","additions":112,"deletions":0,"binary":false,"changes":112,"status":"added"},{"patch":"@@ -0,0 +1,113 @@\n+\/*\n+ * Copyright (c) 2021, Red Hat, Inc. All rights reserved.\n+ * DO NOT ALTER OR REMOVE COPYRIGHT NOTICES OR THIS FILE HEADER.\n+ *\n+ * This code is free software; you can redistribute it and\/or modify it\n+ * under the terms of the GNU General Public License version 2 only, as\n+ * published by the Free Software Foundation.\n+ *\n+ * This code is distributed in the hope that it will be useful, but WITHOUT\n+ * ANY WARRANTY; without even the implied warranty of MERCHANTABILITY or\n+ * FITNESS FOR A PARTICULAR PURPOSE.  See the GNU General Public License\n+ * version 2 for more details (a copy is included in the LICENSE file that\n+ * accompanied this code).\n+ *\n+ * You should have received a copy of the GNU General Public License version\n+ * 2 along with this work; if not, write to the Free Software Foundation,\n+ * Inc., 51 Franklin St, Fifth Floor, Boston, MA 02110-1301 USA.\n+ *\n+ * Please contact Oracle, 500 Oracle Parkway, Redwood Shores, CA 94065 USA\n+ * or visit www.oracle.com if you need additional information or have any\n+ * questions.\n+ *\/\n+\n+#ifndef SHARE_GC_SHARED_SLIDINGFORWARDING_INLINE_HPP\n+#define SHARE_GC_SHARED_SLIDINGFORWARDING_INLINE_HPP\n+\n+#include \"gc\/shared\/slidingForwarding.hpp\"\n+#include \"oops\/markWord.hpp\"\n+#include \"oops\/oop.inline.hpp\"\n+\n+#ifdef _LP64\n+size_t SlidingForwarding::region_index_containing(HeapWord* addr) const {\n+  assert(addr >= _heap_start, \"sanity: addr: \" PTR_FORMAT \" heap base: \" PTR_FORMAT, p2i(addr), p2i(_heap_start));\n+  size_t index = ((size_t) (addr - _heap_start)) >> _region_size_words_shift;\n+  assert(index < _num_regions, \"Region index is in bounds: \" PTR_FORMAT, p2i(addr));\n+  return index;\n+}\n+\n+bool SlidingForwarding::region_contains(HeapWord* region_base, HeapWord* addr) const {\n+  return uintptr_t(addr - region_base) < (ONE << _region_size_words_shift);\n+}\n+\n+\n+uintptr_t SlidingForwarding::encode_forwarding(HeapWord* original, HeapWord* target) {\n+  size_t orig_idx = region_index_containing(original);\n+  size_t base_table_idx = orig_idx * 2;\n+  size_t target_idx = region_index_containing(target);\n+  HeapWord* encode_base;\n+  uintptr_t region_idx;\n+  for (region_idx = 0; region_idx < NUM_REGIONS; region_idx++) {\n+    encode_base = _target_base_table[base_table_idx + region_idx];\n+    if (encode_base == UNUSED_BASE) {\n+      encode_base = _heap_start + target_idx * (ONE << _region_size_words_shift);\n+      _target_base_table[base_table_idx + region_idx] = encode_base;\n+      break;\n+    } else if (region_contains(encode_base, target)) {\n+      break;\n+    }\n+  }\n+  if (region_idx >= NUM_REGIONS) {\n+    tty->print_cr(\"target: \" PTR_FORMAT, p2i(target));\n+    for (region_idx = 0; region_idx < NUM_REGIONS; region_idx++) {\n+      tty->print_cr(\"region_idx: \" INTPTR_FORMAT \", encode_base: \" PTR_FORMAT, region_idx, p2i(_target_base_table[base_table_idx + region_idx]));\n+    }\n+  }\n+  assert(region_idx < NUM_REGIONS, \"need to have found an encoding base\");\n+  assert(target >= encode_base, \"target must be above encode base, target:\" PTR_FORMAT \", encoded_base: \" PTR_FORMAT \",  target_idx: \" SIZE_FORMAT \", heap start: \" PTR_FORMAT \", region_idx: \" INTPTR_FORMAT,\n+         p2i(target), p2i(encode_base), target_idx, p2i(_heap_start), region_idx);\n+  assert(region_contains(encode_base, target), \"region must contain target: original: \" PTR_FORMAT \", target: \" PTR_FORMAT \", encode_base: \" PTR_FORMAT \", region_idx: \" INTPTR_FORMAT, p2i(original), p2i(target), p2i(encode_base), region_idx);\n+  uintptr_t encoded = (((uintptr_t)(target - encode_base)) << COMPRESSED_BITS_SHIFT) |\n+                      (region_idx << BASE_SHIFT) | markWord::marked_value;\n+  assert(target == decode_forwarding(original, encoded), \"must be reversible\");\n+  return encoded;\n+}\n+\n+HeapWord* SlidingForwarding::decode_forwarding(HeapWord* original, uintptr_t encoded) const {\n+  assert((encoded & markWord::marked_value) == markWord::marked_value, \"must be marked as forwarded\");\n+  size_t orig_idx = region_index_containing(original);\n+  size_t region_idx = (encoded >> BASE_SHIFT) & right_n_bits(NUM_REGION_BITS);\n+  size_t base_table_idx = orig_idx * 2 + region_idx;\n+  HeapWord* decoded = _target_base_table[base_table_idx] + (encoded >> COMPRESSED_BITS_SHIFT);\n+  assert(decoded >= _heap_start, \"must be above heap start, encoded: \" INTPTR_FORMAT \", region_idx: \" SIZE_FORMAT \", base: \" PTR_FORMAT, encoded, region_idx, p2i(_target_base_table[base_table_idx]));\n+  return decoded;\n+}\n+#endif\n+\n+void SlidingForwarding::forward_to(oop original, oop target) {\n+#ifdef _LP64\n+  markWord header = original->mark();\n+  if (header.has_displaced_mark_helper()) {\n+    header = header.displaced_mark_helper();\n+  }\n+  uintptr_t encoded = encode_forwarding(cast_from_oop<HeapWord*>(original), cast_from_oop<HeapWord*>(target));\n+  assert((encoded & markWord::klass_mask_in_place) == 0, \"encoded forwardee must not overlap with Klass*: \" PTR_FORMAT, encoded);\n+  header = markWord((header.value() & markWord::klass_mask_in_place) | encoded);\n+  original->set_mark(header);\n+#else\n+  original->forward_to(target);\n+#endif\n+}\n+\n+oop SlidingForwarding::forwardee(oop original) const {\n+#ifdef _LP64\n+  markWord header = original->mark();\n+  uintptr_t encoded = header.value() & ~markWord::klass_mask_in_place;\n+  HeapWord* forwardee = decode_forwarding(cast_from_oop<HeapWord*>(original), encoded);\n+  return cast_to_oop(forwardee);\n+#else\n+  return original->forwardee();\n+#endif\n+}\n+\n+#endif \/\/ SHARE_GC_SHARED_SLIDINGFORWARDING_INLINE_HPP\n","filename":"src\/hotspot\/share\/gc\/shared\/slidingForwarding.inline.hpp","additions":113,"deletions":0,"binary":false,"changes":113,"status":"added"},{"patch":"@@ -30,0 +30,1 @@\n+#include \"gc\/shared\/slidingForwarding.inline.hpp\"\n@@ -255,1 +256,1 @@\n-                                    CompactPoint* cp, HeapWord* compact_top) {\n+                                    CompactPoint* cp, HeapWord* compact_top, SlidingForwarding* const forwarding) {\n@@ -278,1 +279,1 @@\n-    q->forward_to(cast_to_oop(compact_top));\n+    forwarding->forward_to(q, cast_to_oop(compact_top));\n@@ -326,0 +327,1 @@\n+  SlidingForwarding* const forwarding = GenCollectedHeap::heap()->forwarding();\n@@ -331,1 +333,1 @@\n-      compact_top = cp->space->forward(cast_to_oop(cur_obj), size, cp, compact_top);\n+      compact_top = cp->space->forward(cast_to_oop(cur_obj), size, cp, compact_top, forwarding);\n@@ -347,1 +349,1 @@\n-        compact_top = cp->space->forward(obj, obj->size(), cp, compact_top);\n+        compact_top = cp->space->forward(obj, obj->size(), cp, compact_top, forwarding);\n@@ -390,0 +392,1 @@\n+  const SlidingForwarding* const forwarding = GenCollectedHeap::heap()->forwarding();\n@@ -401,1 +404,1 @@\n-      size_t size = MarkSweep::adjust_pointers(cast_to_oop(cur_obj));\n+      size_t size = MarkSweep::adjust_pointers(forwarding, cast_to_oop(cur_obj));\n@@ -442,0 +445,2 @@\n+  const SlidingForwarding* const forwarding = GenCollectedHeap::heap()->forwarding();\n+\n@@ -455,1 +460,1 @@\n-      HeapWord* compaction_top = cast_from_oop<HeapWord*>(cast_to_oop(cur_obj)->forwardee());\n+      HeapWord* compaction_top = cast_from_oop<HeapWord*>(forwarding->forwardee(cast_to_oop(cur_obj)));\n","filename":"src\/hotspot\/share\/gc\/shared\/space.cpp","additions":11,"deletions":6,"binary":false,"changes":17,"status":"modified"},{"patch":"@@ -59,0 +59,1 @@\n+class SlidingForwarding;\n@@ -387,1 +388,1 @@\n-                    HeapWord* compact_top);\n+                    HeapWord* compact_top, SlidingForwarding* const forwarding);\n","filename":"src\/hotspot\/share\/gc\/shared\/space.hpp","additions":2,"deletions":1,"binary":false,"changes":3,"status":"modified"},{"patch":"@@ -32,0 +32,1 @@\n+#include \"gc\/shared\/genCollectedHeap.hpp\"\n","filename":"src\/hotspot\/share\/gc\/shared\/space.inline.hpp","additions":1,"deletions":0,"binary":false,"changes":1,"status":"modified"},{"patch":"@@ -68,0 +68,1 @@\n+      ObjectMonitor::maybe_deflate_dead(p);\n","filename":"src\/hotspot\/share\/gc\/shared\/weakProcessor.inline.hpp","additions":1,"deletions":0,"binary":false,"changes":1,"status":"modified"},{"patch":"@@ -32,0 +32,1 @@\n+#include \"gc\/shenandoah\/shenandoahObjectUtils.inline.hpp\"\n@@ -200,1 +201,1 @@\n-  Klass* obj_klass = obj->klass_or_null();\n+  Klass* obj_klass = ShenandoahObjectUtils::klass(obj);\n@@ -232,1 +233,1 @@\n-    if (obj_klass != fwd->klass()) {\n+    if (obj_klass != ShenandoahObjectUtils::klass(fwd)) {\n","filename":"src\/hotspot\/share\/gc\/shenandoah\/shenandoahAsserts.cpp","additions":3,"deletions":2,"binary":false,"changes":5,"status":"modified"},{"patch":"@@ -33,0 +33,1 @@\n+#include \"runtime\/synchronizer.hpp\"\n@@ -89,0 +90,1 @@\n+    assert(prev_mark.is_marked(), \"must be forwarded\");\n","filename":"src\/hotspot\/share\/gc\/shenandoah\/shenandoahForwarding.inline.hpp","additions":2,"deletions":0,"binary":false,"changes":2,"status":"modified"},{"patch":"@@ -31,0 +31,1 @@\n+#include \"gc\/shared\/slidingForwarding.inline.hpp\"\n@@ -190,0 +191,1 @@\n+    heap->forwarding()->clear();\n@@ -300,2 +302,3 @@\n-  PreservedMarks*          const _preserved_marks;\n-  ShenandoahHeap*          const _heap;\n+  PreservedMarks*    const _preserved_marks;\n+  SlidingForwarding* const _forwarding;\n+  ShenandoahHeap*    const _heap;\n@@ -313,0 +316,1 @@\n+    _forwarding(ShenandoahHeap::heap()->forwarding()),\n@@ -366,1 +370,1 @@\n-    p->forward_to(cast_to_oop(_compact_point));\n+    _forwarding->forward_to(p, cast_to_oop(_compact_point));\n@@ -440,0 +444,1 @@\n+  SlidingForwarding* forwarding = heap->forwarding();\n@@ -474,1 +479,1 @@\n-        old_obj->forward_to(cast_to_oop(heap->get_region(start)->bottom()));\n+        forwarding->forward_to(old_obj, cast_to_oop(heap->get_region(start)->bottom()));\n@@ -725,1 +730,2 @@\n-  ShenandoahHeap* const _heap;\n+  ShenandoahHeap*           const _heap;\n+  const SlidingForwarding*  const _forwarding;\n@@ -735,1 +741,1 @@\n-        oop forw = obj->forwardee();\n+        oop forw = _forwarding->forwardee(obj);\n@@ -744,0 +750,1 @@\n+    _forwarding(_heap->forwarding()),\n@@ -805,1 +812,2 @@\n-    _preserved_marks->get(worker_id)->adjust_during_full_gc();\n+    const SlidingForwarding* const forwarding = ShenandoahHeap::heap()->forwarding();\n+    _preserved_marks->get(worker_id)->adjust_during_full_gc(forwarding);\n@@ -835,2 +843,3 @@\n-  ShenandoahHeap* const _heap;\n-  uint            const _worker_id;\n+  ShenandoahHeap*          const _heap;\n+  const SlidingForwarding* const _forwarding;\n+  uint                     const _worker_id;\n@@ -840,1 +849,1 @@\n-    _heap(ShenandoahHeap::heap()), _worker_id(worker_id) {}\n+    _heap(ShenandoahHeap::heap()), _forwarding(_heap->forwarding()), _worker_id(worker_id) {}\n@@ -847,1 +856,1 @@\n-      HeapWord* compact_to = cast_from_oop<HeapWord*>(p->forwardee());\n+      HeapWord* compact_to = cast_from_oop<HeapWord*>(_forwarding->forwardee(p));\n@@ -944,0 +953,1 @@\n+  const SlidingForwarding* const forwarding = heap->forwarding();\n@@ -958,1 +968,1 @@\n-      size_t new_start = heap->heap_region_index_containing(old_obj->forwardee());\n+      size_t new_start = heap->heap_region_index_containing(forwarding->forwardee(old_obj));\n","filename":"src\/hotspot\/share\/gc\/shenandoah\/shenandoahFullGC.cpp","additions":22,"deletions":12,"binary":false,"changes":34,"status":"modified"},{"patch":"@@ -36,0 +36,1 @@\n+#include \"gc\/shared\/slidingForwarding.hpp\"\n@@ -197,0 +198,2 @@\n+  _forwarding = new SlidingForwarding(_heap_region, ShenandoahHeapRegion::region_size_words_shift());\n+\n@@ -953,1 +956,1 @@\n-    if (!p->is_forwarded()) {\n+    if (!ShenandoahForwarding::is_forwarded(p)) {\n@@ -1298,0 +1301,1 @@\n+    shenandoah_assert_not_in_cset_except(NULL, obj, cancelled_gc());\n","filename":"src\/hotspot\/share\/gc\/shenandoah\/shenandoahHeap.cpp","additions":5,"deletions":1,"binary":false,"changes":6,"status":"modified"},{"patch":"@@ -65,0 +65,1 @@\n+class SlidingForwarding;\n@@ -231,0 +232,1 @@\n+  SlidingForwarding* _forwarding;\n@@ -247,0 +249,2 @@\n+  SlidingForwarding* forwarding() const { return _forwarding; }\n+\n","filename":"src\/hotspot\/share\/gc\/shenandoah\/shenandoahHeap.hpp","additions":4,"deletions":0,"binary":false,"changes":4,"status":"modified"},{"patch":"@@ -45,0 +45,1 @@\n+#include \"gc\/shenandoah\/shenandoahObjectUtils.inline.hpp\"\n@@ -300,1 +301,1 @@\n-  size_t size = p->size();\n+  size_t size = ShenandoahObjectUtils::size(p);\n@@ -338,2 +339,7 @@\n-  ContinuationGCSupport::relativize_stack_chunk(copy_val);\n-\n+  if (!copy_val->mark().is_marked()) {\n+    \/\/ If we copied a mark-word that indicates 'forwarded' state, then\n+    \/\/ another thread beat us, and this new copy will never be published.\n+    \/\/ ContinuationGCSupport would get a corrupt Klass* in that case,\n+    \/\/ so don't even attempt it.\n+    ContinuationGCSupport::relativize_stack_chunk(copy_val);\n+  }\n@@ -519,1 +525,1 @@\n-    size_t size = obj->size();\n+    size_t size = ShenandoahObjectUtils::size(obj);\n","filename":"src\/hotspot\/share\/gc\/shenandoah\/shenandoahHeap.inline.hpp","additions":10,"deletions":4,"binary":false,"changes":14,"status":"modified"},{"patch":"@@ -0,0 +1,43 @@\n+\/*\n+ * Copyright (c) 2022, Red Hat, Inc. All rights reserved.\n+ * DO NOT ALTER OR REMOVE COPYRIGHT NOTICES OR THIS FILE HEADER.\n+ *\n+ * This code is free software; you can redistribute it and\/or modify it\n+ * under the terms of the GNU General Public License version 2 only, as\n+ * published by the Free Software Foundation.\n+ *\n+ * This code is distributed in the hope that it will be useful, but WITHOUT\n+ * ANY WARRANTY; without even the implied warranty of MERCHANTABILITY or\n+ * FITNESS FOR A PARTICULAR PURPOSE.  See the GNU General Public License\n+ * version 2 for more details (a copy is included in the LICENSE file that\n+ * accompanied this code).\n+ *\n+ * You should have received a copy of the GNU General Public License version\n+ * 2 along with this work; if not, write to the Free Software Foundation,\n+ * Inc., 51 Franklin St, Fifth Floor, Boston, MA 02110-1301 USA.\n+ *\n+ * Please contact Oracle, 500 Oracle Parkway, Redwood Shores, CA 94065 USA\n+ * or visit www.oracle.com if you need additional information or have any\n+ * questions.\n+ *\n+ *\/\n+\n+#ifndef SHARE_GC_SHENANDOAH_SHENANDOAHOBJECTUTILS_HPP\n+#define SHARE_GC_SHENANDOAH_SHENANDOAHOBJECTUTILS_HPP\n+\n+#include \"memory\/allStatic.hpp\"\n+#include \"oops\/markWord.hpp\"\n+#include \"oops\/oopsHierarchy.hpp\"\n+\n+class Klass;\n+\n+class ShenandoahObjectUtils : public AllStatic {\n+public:\n+#ifdef _LP64\n+  static inline markWord stable_mark(oop obj);\n+#endif\n+  static inline Klass* klass(oop obj);\n+  static inline size_t size(oop obj);\n+};\n+\n+#endif \/\/ SHARE_GC_SHENANDOAH_SHENANDOAHOBJECTUTILS_HPP\n","filename":"src\/hotspot\/share\/gc\/shenandoah\/shenandoahObjectUtils.hpp","additions":43,"deletions":0,"binary":false,"changes":43,"status":"added"},{"patch":"@@ -0,0 +1,141 @@\n+\/*\n+ * Copyright (c) 2022, Red Hat, Inc. All rights reserved.\n+ * DO NOT ALTER OR REMOVE COPYRIGHT NOTICES OR THIS FILE HEADER.\n+ *\n+ * This code is free software; you can redistribute it and\/or modify it\n+ * under the terms of the GNU General Public License version 2 only, as\n+ * published by the Free Software Foundation.\n+ *\n+ * This code is distributed in the hope that it will be useful, but WITHOUT\n+ * ANY WARRANTY; without even the implied warranty of MERCHANTABILITY or\n+ * FITNESS FOR A PARTICULAR PURPOSE.  See the GNU General Public License\n+ * version 2 for more details (a copy is included in the LICENSE file that\n+ * accompanied this code).\n+ *\n+ * You should have received a copy of the GNU General Public License version\n+ * 2 along with this work; if not, write to the Free Software Foundation,\n+ * Inc., 51 Franklin St, Fifth Floor, Boston, MA 02110-1301 USA.\n+ *\n+ * Please contact Oracle, 500 Oracle Parkway, Redwood Shores, CA 94065 USA\n+ * or visit www.oracle.com if you need additional information or have any\n+ * questions.\n+ *\n+ *\/\n+\n+#ifndef SHARE_GC_SHENANDOAH_SHENANDOAHOBJECTUTILS_INLINE_HPP\n+#define SHARE_GC_SHENANDOAH_SHENANDOAHOBJECTUTILS_INLINE_HPP\n+\n+#include \"gc\/shenandoah\/shenandoahHeap.inline.hpp\"\n+#include \"gc\/shenandoah\/shenandoahObjectUtils.hpp\"\n+#include \"oops\/klass.hpp\"\n+#include \"oops\/markWord.inline.hpp\"\n+#include \"oops\/oop.inline.hpp\"\n+#include \"runtime\/objectMonitor.inline.hpp\"\n+#include \"runtime\/thread.hpp\"\n+\n+\/\/ This is a variant of ObjectSynchronizer::stable_mark(), which does the same thing, but also\n+\/\/ handles forwarded objects. This is intended to be used by concurrent evacuation only. No other\n+\/\/ code is supposed to observe from-space objects.\n+#ifdef _LP64\n+markWord ShenandoahObjectUtils::stable_mark(oop obj) {\n+  ShenandoahHeap* heap = ShenandoahHeap::heap();\n+  for (;;) {\n+    assert(heap->is_in(obj), \"object not in heap: \" PTR_FORMAT, p2i(obj));\n+    markWord mark = obj->mark_acquire();\n+\n+    \/\/ The mark can be in one of the following states:\n+    \/\/ *  Inflated     - just return mark from inflated monitor\n+    \/\/ *  Stack-locked - coerce it to inflating, and then return displaced mark\n+    \/\/ *  INFLATING    - busy wait for conversion to complete\n+    \/\/ *  Neutral      - return mark\n+    \/\/ *  Marked       - object is forwarded, try again on forwardee\n+\n+    \/\/ Most common case first.\n+    if (mark.is_neutral() || mark.is_fast_locked()) {\n+      return mark;\n+    }\n+\n+    \/\/ If object is already forwarded, then resolve it, and try again.\n+    if (mark.is_marked()) {\n+      if (heap->is_full_gc_move_in_progress()) {\n+        \/\/ In these cases, we want to return the header as-is: the Klass* would not be overloaded.\n+        return mark;\n+      }\n+      obj = cast_to_oop(mark.decode_pointer());\n+      continue;\n+    }\n+\n+    \/\/ CASE: inflated\n+    if (mark.has_monitor()) {\n+      \/\/ It is safe to access the object monitor because all Java and GC worker threads\n+      \/\/ participate in the monitor deflation protocol (i.e, they react to handshakes and STS requests).\n+      ObjectMonitor* inf = mark.monitor();\n+      markWord dmw = inf->header();\n+      assert(dmw.is_neutral(), \"invariant: header=\" INTPTR_FORMAT \", original mark: \" INTPTR_FORMAT, dmw.value(), mark.value());\n+      return dmw;\n+    }\n+\n+    \/\/ CASE: inflating\n+    if (mark.is_being_inflated()) {\n+      \/\/ Interference, try again.\n+      continue;\n+    }\n+\n+    \/\/ CASE: stack-locked\n+    if (mark.has_locker()) {\n+      if (Thread::current()->is_lock_owned((address)mark.locker())) {\n+        \/\/ This thread owns the lock. We can safely access it.\n+        markWord dmw = mark.displaced_mark_helper();\n+        assert(dmw.is_neutral(), \"invariant: header=\" INTPTR_FORMAT \", original mark: \" INTPTR_FORMAT, dmw.value(), mark.value());\n+        return dmw;\n+      }\n+\n+      \/\/ Else we try to install INFLATING into the header. This will (temporarily) prevent other\n+      \/\/ threads from stack-locking or evacuating the object.\n+      markWord cmp = obj->cas_set_mark(markWord::INFLATING(), mark);\n+      if (cmp != mark) {\n+        continue;       \/\/ Interference -- just retry\n+      }\n+\n+      \/\/ We've successfully installed INFLATING (0) into the mark-word.\n+      \/\/ This is the only case where 0 will appear in a mark-word.\n+      \/\/ Only the singular thread that successfully swings the mark-word\n+      \/\/ to 0 can fetch the stack-lock and safely read the displaced header.\n+\n+      \/\/ fetch the displaced mark from the owner's stack.\n+      \/\/ The owner can't die or unwind past the lock while our INFLATING\n+      \/\/ object is in the mark.  Furthermore the owner can't complete\n+      \/\/ an unlock on the object, either. No other thread can do evacuation, either.\n+      markWord dmw = mark.displaced_mark_helper();\n+      \/\/ Catch if the object's header is not neutral (not locked and\n+      \/\/ not marked is what we care about here).\n+      assert(dmw.is_neutral(), \"invariant: header=\" INTPTR_FORMAT, dmw.value());\n+\n+      \/\/ Must preserve store ordering. The monitor state must\n+      \/\/ be stable at the time of publishing the monitor address.\n+      guarantee(obj->mark() == markWord::INFLATING(), \"invariant\");\n+      \/\/ Release semantics so that above set_object() is seen first.\n+      obj->release_set_mark(mark);\n+\n+      return dmw;\n+    }\n+  }\n+}\n+#endif\n+\n+Klass* ShenandoahObjectUtils::klass(oop obj) {\n+#ifdef _LP64\n+  markWord header = stable_mark(obj);\n+  assert(header.narrow_klass() != 0, \"klass must not be NULL: \" INTPTR_FORMAT, header.value());\n+  return header.klass();\n+#else\n+  return obj->klass();\n+#endif\n+}\n+\n+size_t ShenandoahObjectUtils::size(oop obj) {\n+  Klass* kls = klass(obj);\n+  return obj->size_given_klass(kls);\n+}\n+\n+#endif \/\/ SHARE_GC_SHENANDOAH_SHENANDOAHOBJECTUTILS_HPP\n","filename":"src\/hotspot\/share\/gc\/shenandoah\/shenandoahObjectUtils.inline.hpp","additions":141,"deletions":0,"binary":false,"changes":141,"status":"added"},{"patch":"@@ -32,0 +32,1 @@\n+#include \"gc\/shenandoah\/shenandoahObjectUtils.inline.hpp\"\n@@ -101,1 +102,1 @@\n-      if (is_instance_ref_klass(obj->klass())) {\n+      if (is_instance_ref_klass(ShenandoahObjectUtils::klass(obj))) {\n@@ -128,1 +129,1 @@\n-    Klass* obj_klass = obj->klass_or_null();\n+    Klass* obj_klass = ShenandoahObjectUtils::klass(obj);\n@@ -143,1 +144,1 @@\n-        check(ShenandoahAsserts::_safe_unknown, obj, (obj_addr + obj->size()) <= obj_reg->top(),\n+        check(ShenandoahAsserts::_safe_unknown, obj, (obj_addr + ShenandoahObjectUtils::size(obj)) <= obj_reg->top(),\n@@ -147,1 +148,1 @@\n-        size_t humongous_end = humongous_start + (obj->size() >> ShenandoahHeapRegion::region_size_words_shift());\n+        size_t humongous_end = humongous_start + (ShenandoahObjectUtils::size(obj) >> ShenandoahHeapRegion::region_size_words_shift());\n@@ -164,1 +165,1 @@\n-          Atomic::add(&_ld[obj_reg->index()], (uint) obj->size(), memory_order_relaxed);\n+          Atomic::add(&_ld[obj_reg->index()], (uint) ShenandoahObjectUtils::size(obj), memory_order_relaxed);\n@@ -205,1 +206,1 @@\n-      check(ShenandoahAsserts::_safe_oop, obj, (fwd_addr + fwd->size()) <= fwd_reg->top(),\n+      check(ShenandoahAsserts::_safe_oop, obj, (fwd_addr + ShenandoahObjectUtils::size(fwd)) <= fwd_reg->top(),\n@@ -308,1 +309,2 @@\n-    obj->oop_iterate(this);\n+    Klass* klass = ShenandoahObjectUtils::klass(obj);\n+    obj->oop_iterate_backwards(this, klass);\n@@ -588,1 +590,1 @@\n-    if (!is_instance_ref_klass(obj->klass())) {\n+    if (!is_instance_ref_klass(ShenandoahObjectUtils::klass(obj))) {\n","filename":"src\/hotspot\/share\/gc\/shenandoah\/shenandoahVerifier.cpp","additions":10,"deletions":8,"binary":false,"changes":18,"status":"modified"},{"patch":"@@ -337,1 +337,1 @@\n-  product(bool, ShenandoahSuspendibleWorkers, false, EXPERIMENTAL,          \\\n+  product(bool, ShenandoahSuspendibleWorkers, true, EXPERIMENTAL,           \\\n","filename":"src\/hotspot\/share\/gc\/shenandoah\/shenandoah_globals.hpp","additions":1,"deletions":1,"binary":false,"changes":2,"status":"modified"},{"patch":"@@ -301,1 +301,0 @@\n-        assert(!UseCompressedClassPointers, \"should only happen without compressed class pointers\");\n","filename":"src\/hotspot\/share\/gc\/z\/c2\/zBarrierSetC2.cpp","additions":0,"deletions":1,"binary":false,"changes":1,"status":"modified"},{"patch":"@@ -147,4 +147,0 @@\n-    \/\/ Get the size of the object before calling the closure, which\n-    \/\/ might overwrite the object in case we are relocating in-place.\n-    const size_t size = ZUtils::object_size(addr);\n-\n@@ -155,1 +151,1 @@\n-    const uintptr_t next_addr = align_up(addr + size, 1 << page_object_alignment_shift);\n+    const uintptr_t next_addr = align_up(addr + 1, 1 << page_object_alignment_shift);\n","filename":"src\/hotspot\/share\/gc\/z\/zLiveMap.inline.hpp","additions":1,"deletions":5,"binary":false,"changes":6,"status":"modified"},{"patch":"@@ -53,2 +53,9 @@\n-  const size_t header = arrayOopDesc::header_size(element_type);\n-  const size_t payload_size = _word_size - header;\n+  int base_offset = arrayOopDesc::base_offset_in_bytes(element_type);\n+\n+  \/\/ Clear leading 32 bit, if necessary.\n+  if (!is_aligned(base_offset, HeapWordSize)) {\n+    assert(is_aligned(base_offset, BytesPerInt), \"array base must be 32 bit aligned\");\n+    *reinterpret_cast<jint*>(reinterpret_cast<char*>(mem) + base_offset) = 0;\n+    base_offset += BytesPerInt;\n+  }\n+  assert(is_aligned(base_offset, HeapWordSize), \"remaining array base must be 64 bit aligned\");\n@@ -56,0 +63,2 @@\n+  const size_t header = heap_word_size(base_offset);\n+  const size_t payload_size = _word_size - header;\n@@ -66,2 +75,1 @@\n-  arrayOopDesc::set_mark(mem, markWord::prototype());\n-  arrayOopDesc::release_set_klass(mem, _klass);\n+  oopDesc::release_set_mark(mem, _klass->prototype_header());\n","filename":"src\/hotspot\/share\/gc\/z\/zObjArrayAllocator.cpp","additions":12,"deletions":4,"binary":false,"changes":16,"status":"modified"},{"patch":"@@ -26,0 +26,1 @@\n+#include \"gc\/shared\/suspendibleThreadSet.hpp\"\n@@ -335,0 +336,4 @@\n+\n+    if (SuspendibleThreadSet::should_yield()) {\n+      SuspendibleThreadSet::yield();\n+    }\n@@ -406,0 +411,1 @@\n+    SuspendibleThreadSetJoiner sts_joiner;\n","filename":"src\/hotspot\/share\/gc\/z\/zRelocate.cpp","additions":6,"deletions":0,"binary":false,"changes":6,"status":"modified"},{"patch":"@@ -738,0 +738,9 @@\n+  if (!UseHeavyMonitors && UseFastLocking) {\n+    \/\/ This is a hack to get around the limitation of registers in x86_32. We really\n+    \/\/ send an oopDesc* instead of a BasicObjectLock*.\n+    Handle h_obj(current, oop((reinterpret_cast<oopDesc*>(elem))));\n+    assert(Universe::heap()->is_in_or_null(h_obj()),\n+           \"must be NULL or an object\");\n+    ObjectSynchronizer::enter(h_obj, NULL, current);\n+    return;\n+  }\n","filename":"src\/hotspot\/share\/interpreter\/interpreterRuntime.cpp","additions":9,"deletions":0,"binary":false,"changes":9,"status":"modified"},{"patch":"@@ -2002,0 +2002,3 @@\n+#ifdef _LP64\n+              oopDesc::release_set_mark(result, ik->prototype_header());\n+#else\n@@ -2003,1 +2006,0 @@\n-              oopDesc::set_klass_gap(result, 0);\n@@ -2005,1 +2007,1 @@\n-\n+#endif\n","filename":"src\/hotspot\/share\/interpreter\/zero\/bytecodeInterpreter.cpp","additions":4,"deletions":2,"binary":false,"changes":6,"status":"modified"},{"patch":"@@ -69,1 +69,2 @@\n-    _store->push(ObjectSampleMarkWord(obj, obj->mark()));\n+    markWord mark = obj->mark();\n+    _store->push(ObjectSampleMarkWord(obj, mark));\n@@ -73,1 +74,6 @@\n-    obj->set_mark(markWord::prototype().set_marked());\n+#ifdef _LP64\n+    if (mark.has_displaced_mark_helper()) {\n+      mark = mark.displaced_mark_helper();\n+    }\n+#endif\n+    obj->set_mark(markWord::prototype().set_marked() LP64_ONLY(.set_narrow_klass(mark.narrow_klass())));\n","filename":"src\/hotspot\/share\/jfr\/leakprofiler\/chains\/objectSampleMarker.hpp","additions":8,"deletions":2,"binary":false,"changes":10,"status":"modified"},{"patch":"@@ -2207,1 +2207,1 @@\n-  return arrayOopDesc::header_size(type) * HeapWordSize;\n+  return arrayOopDesc::base_offset_in_bytes(type);\n","filename":"src\/hotspot\/share\/jvmci\/jvmciCompilerToVM.cpp","additions":1,"deletions":1,"binary":false,"changes":2,"status":"modified"},{"patch":"@@ -271,1 +271,0 @@\n-  volatile_nonstatic_field(oopDesc,            _metadata._klass,                              Klass*)                                \\\n","filename":"src\/hotspot\/share\/jvmci\/vmStructs_jvmci.cpp","additions":0,"deletions":1,"binary":false,"changes":1,"status":"modified"},{"patch":"@@ -31,0 +31,1 @@\n+#include \"memory\/metaspace\/metaspaceAlignment.hpp\"\n@@ -39,0 +40,2 @@\n+#include \"oops\/compressedKlass.hpp\"\n+#include \"utilities\/align.hpp\"\n@@ -40,0 +43,1 @@\n+#include \"utilities\/globalDefinitions.hpp\"\n@@ -59,0 +63,2 @@\n+  const int klass_alignment_words = KlassAlignmentInBytes \/ BytesPerWord;\n+\n@@ -63,0 +69,1 @@\n+      metaspace::MetaspaceMinAlignmentWords,\n@@ -69,0 +76,1 @@\n+    \/\/ Klass instances live in class space and must be aligned correctly.\n@@ -74,0 +82,1 @@\n+        klass_alignment_words,\n@@ -77,0 +86,5 @@\n+  } else {\n+    \/\/ note for lilliput, this path should be restricted to 32bit only. There, klass alignment\n+    \/\/  should be compatible with metaspace minimal alignment since we store Klass structures\n+    \/\/  in regular metaspace.\n+    NOT_LP64(STATIC_ASSERT(metaspace::MetaspaceMinAlignmentBytes == KlassAlignmentInBytes));\n","filename":"src\/hotspot\/share\/memory\/classLoaderMetaspace.cpp","additions":14,"deletions":0,"binary":false,"changes":14,"status":"modified"},{"patch":"@@ -39,0 +39,1 @@\n+#include \"memory\/metaspace\/metaspaceAlignment.hpp\"\n@@ -870,0 +871,14 @@\n+#ifdef _LP64\n+\/\/ The largest allowed size for class space\n+size_t Metaspace::max_class_space_size() {\n+  \/\/ This is a bit fuzzy. Max value of class space size depends on narrow klass pointer\n+  \/\/ encoding range size and CDS, since class space shares encoding range with CDS. CDS\n+  \/\/ archives are usually pretty small though, so to keep matters simple, for now we\n+  \/\/ just assume a reasonable default (this is hackish; improve!).\n+  const size_t slice_for_cds = M * 128;\n+  assert(KlassEncodingMetaspaceMax >= (slice_for_cds * 2), \"rethink this\");\n+  const size_t max_class_space_size = KlassEncodingMetaspaceMax - slice_for_cds;\n+  return max_class_space_size;\n+}\n+#endif \/\/ _LP64\n+\n","filename":"src\/hotspot\/share\/memory\/metaspace.cpp","additions":15,"deletions":0,"binary":false,"changes":15,"status":"modified"},{"patch":"@@ -112,0 +112,3 @@\n+  \/\/ The largest allowed size for class space\n+  LP64_ONLY(static size_t max_class_space_size();)\n+\n","filename":"src\/hotspot\/share\/memory\/metaspace.hpp","additions":3,"deletions":0,"binary":false,"changes":3,"status":"modified"},{"patch":"@@ -55,0 +55,5 @@\n+    \/\/ Attention alignment: the resulting block must have the right alignment\n+    \/\/  for the enclosing arena. ATM this works, since the arena aligns allocated block\n+    \/\/  size. If we ever switch to a different model (e.g. aligning the start\n+    \/\/  address of allocated blocks instead of the request size) this should be\n+    \/\/  rewritten).\n","filename":"src\/hotspot\/share\/memory\/metaspace\/freeBlocks.cpp","additions":5,"deletions":0,"binary":false,"changes":5,"status":"modified"},{"patch":"@@ -0,0 +1,70 @@\n+\/*\n+ * Copyright (c) 2020, Oracle and\/or its affiliates. All rights reserved.\n+ * Copyright (c) 2020 SAP SE. All rights reserved.\n+ * DO NOT ALTER OR REMOVE COPYRIGHT NOTICES OR THIS FILE HEADER.\n+ *\n+ * This code is free software; you can redistribute it and\/or modify it\n+ * under the terms of the GNU General Public License version 2 only, as\n+ * published by the Free Software Foundation.\n+ *\n+ * This code is distributed in the hope that it will be useful, but WITHOUT\n+ * ANY WARRANTY; without even the implied warranty of MERCHANTABILITY or\n+ * FITNESS FOR A PARTICULAR PURPOSE.  See the GNU General Public License\n+ * version 2 for more details (a copy is included in the LICENSE file that\n+ * accompanied this code).\n+ *\n+ * You should have received a copy of the GNU General Public License version\n+ * 2 along with this work; if not, write to the Free Software Foundation,\n+ * Inc., 51 Franklin St, Fifth Floor, Boston, MA 02110-1301 USA.\n+ *\n+ * Please contact Oracle, 500 Oracle Parkway, Redwood Shores, CA 94065 USA\n+ * or visit www.oracle.com if you need additional information or have any\n+ * questions.\n+ *\n+ *\/\n+\n+#ifndef SHARE_MEMORY_METASPACE_ALIGNMENT_HPP\n+#define SHARE_MEMORY_METASPACE_ALIGNMENT_HPP\n+\n+#include \"memory\/metaspace\/chunklevel.hpp\"\n+#include \"memory\/metaspace\/freeBlocks.hpp\"\n+#include \"utilities\/align.hpp\"\n+#include \"utilities\/powerOfTwo.hpp\"\n+#include \"utilities\/globalDefinitions.hpp\"\n+\n+namespace metaspace {\n+\n+\/\/ The minimal alignment: good enough to store structures with 64bit wide members (also on 32-bit).\n+\/\/ Should we ever store longer values, revise.\n+static const int LogMetaspaceMinimalAlignment = LogBytesPerLong;\n+static const int MetaspaceMinAlignmentBytes = 1 << LogMetaspaceMinimalAlignment;\n+static const int MetaspaceMinAlignmentWords = MetaspaceMinAlignmentBytes \/ BytesPerWord;\n+\n+\/\/ The maximum possible alignment is the smallest chunk size (note that the buddy allocator places\n+\/\/ chunks at chunk-size-aligned boundaries, therefore the start address is guaranteed to be aligned).\n+\/\/ We cannot guarantee allocation alignment beyond this value.\n+static const int MetaspaceMaxAlignmentWords = chunklevel::MIN_CHUNK_WORD_SIZE;\n+\n+\/\/ Given a net allocation word size and an alignment value, return the raw word size we actually\n+\/\/ allocate internally.\n+inline size_t get_raw_word_size_for_requested_word_size(size_t net_word_size,\n+                                                        size_t alignment_words) {\n+\n+  \/\/ The alignment should be between the minimum alignment but cannot be larger than the smallest chunk size\n+  assert(is_power_of_2(alignment_words), \"invalid alignment\");\n+  assert(alignment_words >= MetaspaceMinAlignmentWords &&\n+         alignment_words <= MetaspaceMaxAlignmentWords,\n+         \"invalid alignment (\" SIZE_FORMAT \")\", alignment_words);\n+\n+  \/\/ Deallocated metablocks are kept in a binlist which means blocks need to have\n+  \/\/ a minimal size\n+  size_t raw_word_size = MAX2(net_word_size, FreeBlocks::MinWordSize);\n+\n+  raw_word_size = align_up(raw_word_size, alignment_words);\n+\n+  return raw_word_size;\n+}\n+\n+} \/\/ namespace metaspace\n+\n+#endif \/\/ SHARE_MEMORY_METASPACE_ALIGNMENT_HPP\n","filename":"src\/hotspot\/share\/memory\/metaspace\/metaspaceAlignment.hpp","additions":70,"deletions":0,"binary":false,"changes":70,"status":"added"},{"patch":"@@ -34,0 +34,1 @@\n+#include \"memory\/metaspace\/metaspaceAlignment.hpp\"\n@@ -110,1 +111,1 @@\n-MetaspaceArena::MetaspaceArena(ChunkManager* chunk_manager, const ArenaGrowthPolicy* growth_policy,\n+MetaspaceArena::MetaspaceArena(ChunkManager* chunk_manager, const ArenaGrowthPolicy* growth_policy, int alignment_words,\n@@ -117,0 +118,1 @@\n+  _alignment_words(alignment_words),\n@@ -123,0 +125,1 @@\n+\n@@ -227,1 +230,1 @@\n-  const size_t raw_word_size = get_raw_word_size_for_requested_word_size(requested_word_size);\n+  const size_t raw_word_size = get_raw_word_size_for_requested_word_size(requested_word_size, _alignment_words);\n@@ -271,1 +274,1 @@\n-  const size_t raw_word_size = get_raw_word_size_for_requested_word_size(requested_word_size);\n+  const size_t raw_word_size = get_raw_word_size_for_requested_word_size(requested_word_size, _alignment_words);\n@@ -368,1 +371,1 @@\n-  size_t raw_word_size = get_raw_word_size_for_requested_word_size(word_size);\n+  size_t raw_word_size = get_raw_word_size_for_requested_word_size(word_size, _alignment_words);\n@@ -482,2 +485,2 @@\n-  st->print_cr(\"growth-policy \" PTR_FORMAT \", lock \" PTR_FORMAT \", cm \" PTR_FORMAT \", fbl \" PTR_FORMAT,\n-                p2i(_growth_policy), p2i(_lock), p2i(_chunk_manager), p2i(_fbl));\n+  st->print_cr(\"growth-policy \" PTR_FORMAT \", alignment %d, lock \" PTR_FORMAT \", cm \" PTR_FORMAT \", fbl \" PTR_FORMAT,\n+                p2i(_growth_policy), _alignment_words * BytesPerWord, p2i(_lock), p2i(_chunk_manager), p2i(_fbl));\n","filename":"src\/hotspot\/share\/memory\/metaspace\/metaspaceArena.cpp","additions":9,"deletions":6,"binary":false,"changes":15,"status":"modified"},{"patch":"@@ -97,0 +97,3 @@\n+  \/\/ Alignment alignment, in words.\n+  const int _alignment_words;\n+\n@@ -167,1 +170,1 @@\n-  MetaspaceArena(ChunkManager* chunk_manager, const ArenaGrowthPolicy* growth_policy,\n+  MetaspaceArena(ChunkManager* chunk_manager, const ArenaGrowthPolicy* growth_policy, int alignment_words,\n","filename":"src\/hotspot\/share\/memory\/metaspace\/metaspaceArena.hpp","additions":4,"deletions":1,"binary":false,"changes":5,"status":"modified"},{"patch":"@@ -28,0 +28,1 @@\n+#include \"memory\/metaspace\/metaspaceAlignment.hpp\"\n@@ -171,18 +172,0 @@\n-\/\/ Given a net allocation word size, return the raw word size we actually allocate.\n-\/\/ Note: externally visible for gtests.\n-\/\/static\n-size_t get_raw_word_size_for_requested_word_size(size_t word_size) {\n-  size_t byte_size = word_size * BytesPerWord;\n-\n-  \/\/ Deallocated metablocks are kept in a binlist which limits their minimal\n-  \/\/  size to at least the size of a binlist item (2 words).\n-  byte_size = MAX2(byte_size, FreeBlocks::MinWordSize * BytesPerWord);\n-\n-  \/\/ Metaspace allocations are aligned to word size.\n-  byte_size = align_up(byte_size, AllocationAlignmentByteSize);\n-\n-  size_t raw_word_size = byte_size \/ BytesPerWord;\n-  assert(raw_word_size * BytesPerWord == byte_size, \"Sanity\");\n-  return raw_word_size;\n-}\n-\n","filename":"src\/hotspot\/share\/memory\/metaspace\/metaspaceCommon.cpp","additions":1,"deletions":18,"binary":false,"changes":19,"status":"modified"},{"patch":"@@ -38,21 +38,0 @@\n-\/\/ Metaspace allocation alignment:\n-\n-\/\/ 1) Metaspace allocations have to be aligned such that 64bit values are aligned\n-\/\/  correctly.\n-\/\/\n-\/\/ 2) Klass* structures allocated from Metaspace have to be aligned to KlassAlignmentInBytes.\n-\/\/\n-\/\/ At the moment LogKlassAlignmentInBytes is 3, so KlassAlignmentInBytes == 8,\n-\/\/  so (1) and (2) can both be fulfilled with an alignment of 8. Should we increase\n-\/\/  KlassAlignmentInBytes at any time this will increase the necessary alignment as well. In\n-\/\/  that case we may think about introducing a separate alignment just for the class space\n-\/\/  since that alignment would only be needed for Klass structures.\n-\n-static const size_t AllocationAlignmentByteSize = 8;\n-STATIC_ASSERT(AllocationAlignmentByteSize == (size_t)KlassAlignmentInBytes);\n-\n-static const size_t AllocationAlignmentWordSize = AllocationAlignmentByteSize \/ BytesPerWord;\n-\n-\/\/ Returns the raw word size allocated for a given net allocation\n-size_t get_raw_word_size_for_requested_word_size(size_t word_size);\n-\n","filename":"src\/hotspot\/share\/memory\/metaspace\/metaspaceCommon.hpp","additions":0,"deletions":21,"binary":false,"changes":21,"status":"modified"},{"patch":"@@ -41,0 +41,1 @@\n+#include \"oops\/compressedOops.hpp\"\n@@ -108,0 +109,6 @@\n+    out->cr();\n+    out->print_cr(\"KlassAlignmentInBytes: %d\", KlassAlignmentInBytes);\n+    out->print(\"KlassEncodingMetaspaceMax: \");\n+    print_human_readable_size(out, KlassEncodingMetaspaceMax, scale);\n+    out->cr();\n+    CompressedKlassPointers::print_mode(out);\n@@ -109,1 +116,1 @@\n-    out->print(\"No class space\");\n+    out->print_cr(\"No class space\");\n@@ -111,1 +118,0 @@\n-  out->cr();\n","filename":"src\/hotspot\/share\/memory\/metaspace\/metaspaceReporter.cpp","additions":8,"deletions":2,"binary":false,"changes":10,"status":"modified"},{"patch":"@@ -27,0 +27,1 @@\n+#include \"memory\/metaspace\/metaspaceAlignment.hpp\"\n@@ -100,1 +101,2 @@\n-    arena = new MetaspaceArena(_context->cm(), growth_policy, lock, &_used_words_counter, _name);\n+    arena = new MetaspaceArena(_context->cm(), growth_policy, MetaspaceMinAlignmentWords,\n+                               lock, &_used_words_counter, _name);\n","filename":"src\/hotspot\/share\/memory\/metaspace\/testHelpers.cpp","additions":3,"deletions":1,"binary":false,"changes":4,"status":"modified"},{"patch":"@@ -550,0 +550,4 @@\n+    \/\/ Note Lilliput: the advantages of this strategy were questionable before\n+    \/\/  (since CDS=off + Compressed oops + heap large enough to suffocate us out of lower 32g\n+    \/\/  is rare) and with Lilliput the encoding range drastically shrank. We may just do away\n+    \/\/  with this altogether.\n","filename":"src\/hotspot\/share\/memory\/virtualspace.cpp","additions":4,"deletions":0,"binary":false,"changes":4,"status":"modified"},{"patch":"@@ -53,2 +53,1 @@\n-    size_t hs = align_up(length_offset_in_bytes() + sizeof(int),\n-                              HeapWordSize);\n+    size_t hs = length_offset_in_bytes() + sizeof(int);\n@@ -74,0 +73,5 @@\n+#ifdef _LP64\n+    if (type == T_OBJECT || type == T_ARRAY) {\n+      return !UseCompressedOops;\n+    }\n+#endif\n@@ -82,2 +86,1 @@\n-    return UseCompressedClassPointers ? klass_gap_offset_in_bytes() :\n-                               sizeof(arrayOopDesc);\n+    return sizeof(arrayOopDesc);\n@@ -88,1 +91,4 @@\n-    return header_size(type) * HeapWordSize;\n+    size_t typesize_in_bytes = header_size_in_bytes();\n+    return (int)(element_type_should_be_aligned(type)\n+                 ? align_up(typesize_in_bytes, BytesPerLong)\n+                 : typesize_in_bytes);\n@@ -125,11 +131,0 @@\n-  \/\/ Should only be called with constants as argument\n-  \/\/ (will not constant fold otherwise)\n-  \/\/ Returns the header size in words aligned to the requirements of the\n-  \/\/ array object type.\n-  static int header_size(BasicType type) {\n-    size_t typesize_in_bytes = header_size_in_bytes();\n-    return (int)(element_type_should_be_aligned(type)\n-      ? align_object_offset(typesize_in_bytes\/HeapWordSize)\n-      : typesize_in_bytes\/HeapWordSize);\n-  }\n-\n@@ -144,4 +139,2 @@\n-    const size_t max_element_words_per_size_t =\n-      align_down((SIZE_MAX\/HeapWordSize - header_size(type)), MinObjAlignment);\n-    const size_t max_elements_per_size_t =\n-      HeapWordSize * max_element_words_per_size_t \/ type2aelembytes(type);\n+    const size_t max_size_bytes = align_down(SIZE_MAX - base_offset_in_bytes(type), MinObjAlignmentInBytes);\n+    const size_t max_elements_per_size_t = max_size_bytes \/ type2aelembytes(type);\n@@ -153,1 +146,2 @@\n-      return align_down(max_jint - header_size(type), MinObjAlignment);\n+      int header_size_words = align_up(base_offset_in_bytes(type), HeapWordSize) \/ HeapWordSize;\n+      return align_down(max_jint - header_size_words, MinObjAlignment);\n","filename":"src\/hotspot\/share\/oops\/arrayOop.hpp","additions":15,"deletions":21,"binary":false,"changes":36,"status":"modified"},{"patch":"@@ -0,0 +1,103 @@\n+\/*\n+ * Copyright (c) 2021 SAP SE. All rights reserved.\n+ * Copyright (c) 2021, Oracle and\/or its affiliates. All rights reserved.\n+ *\n+ * DO NOT ALTER OR REMOVE COPYRIGHT NOTICES OR THIS FILE HEADER.\n+ *\n+ * This code is free software; you can redistribute it and\/or modify it\n+ * under the terms of the GNU General Public License version 2 only, as\n+ * published by the Free Software Foundation.\n+ *\n+ * This code is distributed in the hope that it will be useful, but WITHOUT\n+ * ANY WARRANTY; without even the implied warranty of MERCHANTABILITY or\n+ * FITNESS FOR A PARTICULAR PURPOSE.  See the GNU General Public License\n+ * version 2 for more details (a copy is included in the LICENSE file that\n+ * accompanied this code).\n+ *\n+ * You should have received a copy of the GNU General Public License version\n+ * 2 along with this work; if not, write to the Free Software Foundation,\n+ * Inc., 51 Franklin St, Fifth Floor, Boston, MA 02110-1301 USA.\n+ *\n+ * Please contact Oracle, 500 Oracle Parkway, Redwood Shores, CA 94065 USA\n+ * or visit www.oracle.com if you need additional information or have any\n+ * questions.\n+ *\n+ *\/\n+\n+#include \"precompiled.hpp\"\n+#include \"oops\/compressedKlass.hpp\"\n+#include \"utilities\/ostream.hpp\"\n+#include \"utilities\/debug.hpp\"\n+#include \"runtime\/globals.hpp\"\n+\n+address CompressedKlassPointers::_base = nullptr;\n+int CompressedKlassPointers::_shift_copy = 0;\n+\n+\/\/ Given an address range [addr, addr+len) which the encoding is supposed to\n+\/\/  cover, choose base, shift and range.\n+\/\/  The address range is the expected range of uncompressed Klass pointers we\n+\/\/  will encounter (and the implicit promise that there will be no Klass\n+\/\/  structures outside this range).\n+void CompressedKlassPointers::initialize(address addr, size_t len) {\n+#ifdef _LP64\n+  assert(UseCompressedClassPointers, \"Sanity\");\n+\n+  assert(len <= (size_t)KlassEncodingMetaspaceMax, \"Range \" SIZE_FORMAT \" too large \"\n+         \"- cannot be contained fully in narrow Klass pointer encoding range.\", len);\n+\n+  if (UseSharedSpaces || DumpSharedSpaces) {\n+\n+    \/\/ Special requirements if CDS is active:\n+    \/\/ Encoding base and shift must be the same between dump and run time.\n+    \/\/   CDS takes care that the SharedBaseAddress and CompressedClassSpaceSize\n+    \/\/   are the same. Archive size will be probably different at runtime, but\n+    \/\/   it can only be smaller than at, never larger, since archives get\n+    \/\/   shrunk at the end of the dump process.\n+    \/\/   From that it follows that the range [addr, len) we are handed in at\n+    \/\/   runtime will start at the same address then at dumptime, and its len\n+    \/\/   may be smaller at runtime then it was at dump time.\n+    \/\/\n+    \/\/ To be very careful here, we avoid any optimizations and just keep using\n+    \/\/  the same address and shift value. Specifically we avoid using zero-based\n+    \/\/  encoding. We also set the expected value range to 4G (encoding range\n+    \/\/  cannot be larger than that).\n+\n+    _base = addr;\n+\n+  } else {\n+\n+    \/\/ (Note that this case is almost not worth optimizing for. CDS is typically on.)\n+    if ((addr + len) <= (address)KlassEncodingMetaspaceMax) {\n+      _base = 0;\n+    } else {\n+      _base = addr;\n+    }\n+  }\n+\n+  assert(is_valid_base(_base), \"Address \" PTR_FORMAT \" was chosen as encoding base for range [\"\n+                               PTR_FORMAT \", \" PTR_FORMAT \") but is not a valid encoding base\",\n+                               p2i(_base), p2i(addr), p2i(addr + len));\n+\n+  \/\/ For SA\n+  _shift_copy = LogKlassAlignmentInBytes;\n+\n+#else\n+  ShouldNotReachHere(); \/\/ 64-bit only\n+#endif\n+}\n+\n+\/\/ 64-bit platforms define these functions on a per-platform base. They are not needed for\n+\/\/  32-bit (in fact, the whole setup is not needed and could be excluded from compilation,\n+\/\/  but that is a question for another RFE).\n+#ifndef _LP64\n+\/\/ Given an address p, return true if p can be used as an encoding base.\n+\/\/  (Some platforms have restrictions of what constitutes a valid base address).\n+bool CompressedKlassPointers::is_valid_base(address p) {\n+  ShouldNotReachHere(); \/\/ 64-bit only\n+  return false;\n+}\n+\n+void CompressedKlassPointers::print_mode(outputStream* st) {\n+}\n+#endif\n+\n","filename":"src\/hotspot\/share\/oops\/compressedKlass.cpp","additions":103,"deletions":0,"binary":false,"changes":103,"status":"added"},{"patch":"@@ -0,0 +1,114 @@\n+\/*\n+ * Copyright (c) 2021 SAP SE. All rights reserved.\n+ * Copyright (c) 2021, Oracle and\/or its affiliates. All rights reserved.\n+ *\n+ * DO NOT ALTER OR REMOVE COPYRIGHT NOTICES OR THIS FILE HEADER.\n+ *\n+ * This code is free software; you can redistribute it and\/or modify it\n+ * under the terms of the GNU General Public License version 2 only, as\n+ * published by the Free Software Foundation.\n+ *\n+ * This code is distributed in the hope that it will be useful, but WITHOUT\n+ * ANY WARRANTY; without even the implied warranty of MERCHANTABILITY or\n+ * FITNESS FOR A PARTICULAR PURPOSE.  See the GNU General Public License\n+ * version 2 for more details (a copy is included in the LICENSE file that\n+ * accompanied this code).\n+ *\n+ * You should have received a copy of the GNU General Public License version\n+ * 2 along with this work; if not, write to the Free Software Foundation,\n+ * Inc., 51 Franklin St, Fifth Floor, Boston, MA 02110-1301 USA.\n+ *\n+ * Please contact Oracle, 500 Oracle Parkway, Redwood Shores, CA 94065 USA\n+ * or visit www.oracle.com if you need additional information or have any\n+ * questions.\n+ *\n+ *\/\n+\n+#ifndef SHARE_OOPS_COMPRESSEDKLASS_HPP\n+#define SHARE_OOPS_COMPRESSEDKLASS_HPP\n+\n+#include \"memory\/allStatic.hpp\"\n+#include \"utilities\/globalDefinitions.hpp\"\n+\n+class outputStream;\n+class Klass;\n+\n+\/\/ Narrow Klass pointer constants;\n+#ifdef _LP64\n+const int LogKlassAlignmentInBytes = 9; \/\/ 512 byte alignment (Lilliput)\n+#else\n+const int LogKlassAlignmentInBytes = 3; \/\/ traditional 64-bit alignment\n+#endif\n+\n+const int KlassAlignmentInBytes    = 1 << LogKlassAlignmentInBytes;\n+\n+\/\/ Max. allowed size of compressed class pointer, in bits\n+const  int      MaxNarrowKlassPointerBits = 22;\n+\n+\/\/ Mask to mask in the bits which are valid to be set in a narrow Klass pointer\n+const uint64_t  NarrowKlassPointerBitMask = ((((uint64_t)1) << MaxNarrowKlassPointerBits) - 1);\n+\n+\/\/ Maximal size of compressed class pointer encoding range (2G with 22bit class ptr and 9 bit alignment).\n+const  uint64_t KlassEncodingMetaspaceMax = UCONST64(1) << (MaxNarrowKlassPointerBits + LogKlassAlignmentInBytes);\n+\n+\/\/ If compressed klass pointers then use narrowKlass.\n+typedef uint32_t narrowKlass;\n+\n+class CompressedKlassPointers : public AllStatic {\n+  friend class VMStructs;\n+  friend class ArchiveBuilder;\n+\n+  static address _base;\n+\n+  \/\/ Shift is actually a constant; we keep this just for the SA (see vmStructs.cpp and\n+  \/\/ sun\/jvm\/hotspot\/oops\/CompressedKlassPointers.java)\n+  static int _shift_copy;\n+\n+  \/\/ The decode\/encode versions taking an explicit base are for the sole use of CDS\n+  \/\/ (see ArchiveBuilder).\n+  static inline Klass* decode_raw(narrowKlass v, address base);\n+  static inline Klass* decode_not_null(narrowKlass v, address base);\n+  static inline narrowKlass encode_not_null(Klass* v, address base);\n+  DEBUG_ONLY(static inline void verify_klass_pointer(const Klass* v, address base));\n+\n+public:\n+\n+  \/\/ Given an address p, return true if p can be used as an encoding base.\n+  \/\/  (Some platforms have restrictions of what constitutes a valid base\n+  \/\/   address).\n+  static bool is_valid_base(address p);\n+\n+  \/\/ Given an address range [addr, addr+len) which the encoding is supposed to\n+  \/\/  cover, choose base, shift and range.\n+  \/\/  The address range is the expected range of uncompressed Klass pointers we\n+  \/\/  will encounter (and the implicit promise that there will be no Klass\n+  \/\/  structures outside this range).\n+  static void initialize(address addr, size_t len);\n+\n+  static void     print_mode(outputStream* st);\n+\n+  \/\/ The encoding base. Note: this is not necessarily the base address of the\n+  \/\/ class space nor the base address of the CDS archive.\n+  static address  base()             { return  _base; }\n+\n+  \/\/ End of the encoding range.\n+  static address  end()              { return base() + KlassEncodingMetaspaceMax; }\n+\n+  \/\/ Shift == LogKlassAlignmentInBytes (TODO: unify)\n+  static int      shift()            { return  LogKlassAlignmentInBytes; }\n+\n+  static bool is_null(Klass* v)      { return v == nullptr; }\n+  static bool is_null(narrowKlass v) { return v == 0; }\n+\n+  static inline Klass* decode_raw(narrowKlass v);\n+  static inline Klass* decode_not_null(narrowKlass v);\n+  static inline Klass* decode(narrowKlass v);\n+  static inline narrowKlass encode_not_null(Klass* v);\n+  static inline narrowKlass encode(Klass* v);\n+\n+  DEBUG_ONLY(static inline void verify_klass_pointer(const Klass* v));\n+  DEBUG_ONLY(static inline void verify_narrow_klass_pointer(narrowKlass v);)\n+\n+};\n+\n+#endif \/\/ SHARE_OOPS_COMPRESSEDOOPS_HPP\n","filename":"src\/hotspot\/share\/oops\/compressedKlass.hpp","additions":114,"deletions":0,"binary":false,"changes":114,"status":"added"},{"patch":"@@ -0,0 +1,101 @@\n+\/*\n+ * Copyright (c) 2021 SAP SE. All rights reserved.\n+ * Copyright (c) 2021, Oracle and\/or its affiliates. All rights reserved.\n+ *\n+ * DO NOT ALTER OR REMOVE COPYRIGHT NOTICES OR THIS FILE HEADER.\n+ *\n+ * This code is free software; you can redistribute it and\/or modify it\n+ * under the terms of the GNU General Public License version 2 only, as\n+ * published by the Free Software Foundation.\n+ *\n+ * This code is distributed in the hope that it will be useful, but WITHOUT\n+ * ANY WARRANTY; without even the implied warranty of MERCHANTABILITY or\n+ * FITNESS FOR A PARTICULAR PURPOSE.  See the GNU General Public License\n+ * version 2 for more details (a copy is included in the LICENSE file that\n+ * accompanied this code).\n+ *\n+ * You should have received a copy of the GNU General Public License version\n+ * 2 along with this work; if not, write to the Free Software Foundation,\n+ * Inc., 51 Franklin St, Fifth Floor, Boston, MA 02110-1301 USA.\n+ *\n+ * Please contact Oracle, 500 Oracle Parkway, Redwood Shores, CA 94065 USA\n+ * or visit www.oracle.com if you need additional information or have any\n+ * questions.\n+ *\n+ *\/\n+\n+#ifndef SHARE_OOPS_COMPRESSEDKLASS_INLINE_HPP\n+#define SHARE_OOPS_COMPRESSEDKLASS_INLINE_HPP\n+\n+#include \"oops\/compressedKlass.hpp\"\n+#include \"memory\/allStatic.hpp\"\n+#include \"utilities\/align.hpp\"\n+#include \"utilities\/globalDefinitions.hpp\"\n+\n+\n+static inline bool check_alignment(Klass* v) {\n+  return (intptr_t)v % KlassAlignmentInBytes == 0;\n+}\n+\n+inline Klass* CompressedKlassPointers::decode_raw(narrowKlass v) {\n+  return decode_raw(v, base());\n+}\n+\n+inline Klass* CompressedKlassPointers::decode_raw(narrowKlass v, address narrow_base) {\n+  return (Klass*)(void*)((uintptr_t)narrow_base +((uintptr_t)v << shift()));\n+}\n+\n+inline Klass* CompressedKlassPointers::decode_not_null(narrowKlass v) {\n+  return decode_not_null(v, base());\n+}\n+\n+inline Klass* CompressedKlassPointers::decode_not_null(narrowKlass v, address narrow_base) {\n+  assert(!is_null(v), \"narrow klass value can never be zero\");\n+  Klass* result = decode_raw(v, narrow_base);\n+  DEBUG_ONLY(verify_klass_pointer(result, narrow_base));\n+  return result;\n+}\n+\n+inline Klass* CompressedKlassPointers::decode(narrowKlass v) {\n+  return is_null(v) ? nullptr : decode_not_null(v);\n+}\n+\n+inline narrowKlass CompressedKlassPointers::encode_not_null(Klass* v) {\n+  return encode_not_null(v, base());\n+}\n+\n+inline narrowKlass CompressedKlassPointers::encode_not_null(Klass* v, address narrow_base) {\n+  DEBUG_ONLY(verify_klass_pointer(v, narrow_base));\n+  uint64_t v2 = (uint64_t)(pointer_delta((void*)v, narrow_base, 1));\n+  v2 >>= shift();\n+  assert(v2 <= UINT_MAX, \"narrow klass pointer overflow\");\n+  narrowKlass result = (narrowKlass)v2;\n+  DEBUG_ONLY(verify_narrow_klass_pointer(result));\n+  assert(decode_not_null(result, narrow_base) == v, \"reversibility\");\n+  return result;\n+}\n+\n+inline narrowKlass CompressedKlassPointers::encode(Klass* v) {\n+  return is_null(v) ? (narrowKlass)0 : encode_not_null(v);\n+}\n+\n+#ifdef ASSERT\n+inline void CompressedKlassPointers::verify_klass_pointer(const Klass* v, address narrow_base) {\n+  assert(is_aligned(v, KlassAlignmentInBytes), \"misaligned Klass* pointer (\" PTR_FORMAT \")\", p2i(v));\n+  address end = narrow_base + KlassEncodingMetaspaceMax;\n+  assert((address)v >= narrow_base && (address)v < end,\n+         \"Klass (\" PTR_FORMAT \") located outside encoding range [\" PTR_FORMAT \", \" PTR_FORMAT \")\",\n+         p2i(v), p2i(narrow_base), p2i(end));\n+}\n+\n+inline void CompressedKlassPointers::verify_klass_pointer(const Klass* v) {\n+  verify_klass_pointer(v, base());\n+}\n+\n+inline void CompressedKlassPointers::verify_narrow_klass_pointer(narrowKlass v) {\n+  \/\/ Make sure we only use the lower n bits\n+  assert((((uint64_t)v) & ~NarrowKlassPointerBitMask) == 0, \"%x: not a valid narrow klass pointer\", v);\n+}\n+#endif\n+\n+#endif \/\/ SHARE_OOPS_COMPRESSEDOOPS_HPP\n","filename":"src\/hotspot\/share\/oops\/compressedKlass.inline.hpp","additions":101,"deletions":0,"binary":false,"changes":101,"status":"added"},{"patch":"@@ -180,123 +180,0 @@\n-\n-\/\/ For UseCompressedClassPointers.\n-NarrowPtrStruct CompressedKlassPointers::_narrow_klass = { nullptr, 0, true };\n-\n-\/\/ CompressedClassSpaceSize set to 1GB, but appear 3GB away from _narrow_ptrs_base during CDS dump.\n-\/\/ (Todo: we should #ifdef out CompressedKlassPointers for 32bit completely and fix all call sites which\n-\/\/  are compiled for 32bit to LP64_ONLY).\n-size_t CompressedKlassPointers::_range = 0;\n-\n-\n-\/\/ Given an address range [addr, addr+len) which the encoding is supposed to\n-\/\/  cover, choose base, shift and range.\n-\/\/  The address range is the expected range of uncompressed Klass pointers we\n-\/\/  will encounter (and the implicit promise that there will be no Klass\n-\/\/  structures outside this range).\n-void CompressedKlassPointers::initialize(address addr, size_t len) {\n-#ifdef _LP64\n-  assert(is_valid_base(addr), \"Address must be a valid encoding base\");\n-  address const end = addr + len;\n-\n-  address base;\n-  int shift;\n-  size_t range;\n-\n-  if (UseSharedSpaces || DumpSharedSpaces) {\n-\n-    \/\/ Special requirements if CDS is active:\n-    \/\/ Encoding base and shift must be the same between dump and run time.\n-    \/\/   CDS takes care that the SharedBaseAddress and CompressedClassSpaceSize\n-    \/\/   are the same. Archive size will be probably different at runtime, but\n-    \/\/   it can only be smaller than at, never larger, since archives get\n-    \/\/   shrunk at the end of the dump process.\n-    \/\/   From that it follows that the range [addr, len) we are handed in at\n-    \/\/   runtime will start at the same address then at dumptime, and its len\n-    \/\/   may be smaller at runtime then it was at dump time.\n-    \/\/\n-    \/\/ To be very careful here, we avoid any optimizations and just keep using\n-    \/\/  the same address and shift value. Specifically we avoid using zero-based\n-    \/\/  encoding. We also set the expected value range to 4G (encoding range\n-    \/\/  cannot be larger than that).\n-\n-    base = addr;\n-\n-    \/\/ JDK-8265705\n-    \/\/ This is a temporary fix for aarch64: there, if the range-to-be-encoded is located\n-    \/\/  below 32g, either encoding base should be zero or base should be aligned to 4G\n-    \/\/  and shift should be zero. The simplest way to fix this for now is to force\n-    \/\/  shift to zero for both runtime and dumptime.\n-    \/\/ Note however that this is not a perfect solution. Ideally this whole function\n-    \/\/  should be CDS agnostic, that would simplify it - and testing - a lot. See JDK-8267141\n-    \/\/  for details.\n-    shift = 0;\n-\n-    \/\/ This must be true since at dumptime cds+ccs is 4G, at runtime it can\n-    \/\/  only be smaller, see comment above.\n-    assert(len <= 4 * G, \"Encoding range cannot be larger than 4G\");\n-    range = 4 * G;\n-\n-  } else {\n-\n-    \/\/ Otherwise we attempt to use a zero base if the range fits in lower 32G.\n-    if (end <= (address)KlassEncodingMetaspaceMax) {\n-      base = 0;\n-    } else {\n-      base = addr;\n-    }\n-\n-    \/\/ Highest offset a Klass* can ever have in relation to base.\n-    range = end - base;\n-\n-    \/\/ We may not even need a shift if the range fits into 32bit:\n-    const uint64_t UnscaledClassSpaceMax = (uint64_t(max_juint) + 1);\n-    if (range < UnscaledClassSpaceMax) {\n-      shift = 0;\n-    } else {\n-      shift = LogKlassAlignmentInBytes;\n-    }\n-\n-  }\n-\n-  set_base(base);\n-  set_shift(shift);\n-  set_range(range);\n-#else\n-  fatal(\"64bit only.\");\n-#endif\n-}\n-\n-\/\/ Given an address p, return true if p can be used as an encoding base.\n-\/\/  (Some platforms have restrictions of what constitutes a valid base address).\n-bool CompressedKlassPointers::is_valid_base(address p) {\n-#ifdef AARCH64\n-  \/\/ Below 32G, base must be aligned to 4G.\n-  \/\/ Above that point, base must be aligned to 32G\n-  if (p < (address)(32 * G)) {\n-    return is_aligned(p, 4 * G);\n-  }\n-  return is_aligned(p, (4 << LogKlassAlignmentInBytes) * G);\n-#else\n-  return true;\n-#endif\n-}\n-\n-void CompressedKlassPointers::print_mode(outputStream* st) {\n-  st->print_cr(\"Narrow klass base: \" PTR_FORMAT \", Narrow klass shift: %d, \"\n-               \"Narrow klass range: \" SIZE_FORMAT_X, p2i(base()), shift(),\n-               range());\n-}\n-\n-void CompressedKlassPointers::set_base(address base) {\n-  assert(UseCompressedClassPointers, \"no compressed klass ptrs?\");\n-  _narrow_klass._base   = base;\n-}\n-\n-void CompressedKlassPointers::set_shift(int shift)       {\n-  assert(shift == 0 || shift == LogKlassAlignmentInBytes, \"invalid shift for klass ptrs\");\n-  _narrow_klass._shift   = shift;\n-}\n-\n-void CompressedKlassPointers::set_range(size_t range) {\n-  assert(UseCompressedClassPointers, \"no compressed klass ptrs?\");\n-  _range = range;\n-}\n","filename":"src\/hotspot\/share\/oops\/compressedOops.cpp","additions":0,"deletions":123,"binary":false,"changes":123,"status":"modified"},{"patch":"@@ -143,53 +143,0 @@\n-\/\/ For UseCompressedClassPointers.\n-class CompressedKlassPointers : public AllStatic {\n-  friend class VMStructs;\n-\n-  static NarrowPtrStruct _narrow_klass;\n-\n-  \/\/ Together with base, this defines the address range within which Klass\n-  \/\/  structures will be located: [base, base+range). While the maximal\n-  \/\/  possible encoding range is 4|32G for shift 0|3, if we know beforehand\n-  \/\/  the expected range of Klass* pointers will be smaller, a platform\n-  \/\/  could use this info to optimize encoding.\n-  static size_t _range;\n-\n-  static void set_base(address base);\n-  static void set_range(size_t range);\n-\n-public:\n-\n-  static void set_shift(int shift);\n-\n-\n-  \/\/ Given an address p, return true if p can be used as an encoding base.\n-  \/\/  (Some platforms have restrictions of what constitutes a valid base\n-  \/\/   address).\n-  static bool is_valid_base(address p);\n-\n-  \/\/ Given an address range [addr, addr+len) which the encoding is supposed to\n-  \/\/  cover, choose base, shift and range.\n-  \/\/  The address range is the expected range of uncompressed Klass pointers we\n-  \/\/  will encounter (and the implicit promise that there will be no Klass\n-  \/\/  structures outside this range).\n-  static void initialize(address addr, size_t len);\n-\n-  static void     print_mode(outputStream* st);\n-\n-  static address  base()               { return  _narrow_klass._base; }\n-  static size_t   range()              { return  _range; }\n-  static int      shift()              { return  _narrow_klass._shift; }\n-\n-  static bool is_null(Klass* v)      { return v == nullptr; }\n-  static bool is_null(narrowKlass v) { return v == 0; }\n-\n-  static inline Klass* decode_raw(narrowKlass v, address base);\n-  static inline Klass* decode_raw(narrowKlass v);\n-  static inline Klass* decode_not_null(narrowKlass v);\n-  static inline Klass* decode_not_null(narrowKlass v, address base);\n-  static inline Klass* decode(narrowKlass v);\n-  static inline narrowKlass encode_not_null(Klass* v);\n-  static inline narrowKlass encode_not_null(Klass* v, address base);\n-  static inline narrowKlass encode(Klass* v);\n-\n-};\n-\n","filename":"src\/hotspot\/share\/oops\/compressedOops.hpp","additions":0,"deletions":53,"binary":false,"changes":53,"status":"modified"},{"patch":"@@ -121,46 +121,0 @@\n-static inline bool check_alignment(Klass* v) {\n-  return (intptr_t)v % KlassAlignmentInBytes == 0;\n-}\n-\n-inline Klass* CompressedKlassPointers::decode_raw(narrowKlass v) {\n-  return decode_raw(v, base());\n-}\n-\n-inline Klass* CompressedKlassPointers::decode_raw(narrowKlass v, address narrow_base) {\n-  return (Klass*)((uintptr_t)narrow_base +((uintptr_t)v << shift()));\n-}\n-\n-inline Klass* CompressedKlassPointers::decode_not_null(narrowKlass v) {\n-  return decode_not_null(v, base());\n-}\n-\n-inline Klass* CompressedKlassPointers::decode_not_null(narrowKlass v, address narrow_base) {\n-  assert(!is_null(v), \"narrow klass value can never be zero\");\n-  Klass* result = decode_raw(v, narrow_base);\n-  assert(check_alignment(result), \"address not aligned: \" PTR_FORMAT, p2i(result));\n-  return result;\n-}\n-\n-inline Klass* CompressedKlassPointers::decode(narrowKlass v) {\n-  return is_null(v) ? nullptr : decode_not_null(v);\n-}\n-\n-inline narrowKlass CompressedKlassPointers::encode_not_null(Klass* v) {\n-  return encode_not_null(v, base());\n-}\n-\n-inline narrowKlass CompressedKlassPointers::encode_not_null(Klass* v, address narrow_base) {\n-  assert(!is_null(v), \"klass value can never be zero\");\n-  assert(check_alignment(v), \"Address not aligned\");\n-  uint64_t pd = (uint64_t)(pointer_delta(v, narrow_base, 1));\n-  assert(KlassEncodingMetaspaceMax > pd, \"change encoding max if new encoding\");\n-  uint64_t result = pd >> shift();\n-  assert((result & CONST64(0xffffffff00000000)) == 0, \"narrow klass pointer overflow\");\n-  assert(decode_not_null(result, narrow_base) == v, \"reversibility\");\n-  return (narrowKlass)result;\n-}\n-\n-inline narrowKlass CompressedKlassPointers::encode(Klass* v) {\n-  return is_null(v) ? (narrowKlass)0 : encode_not_null(v);\n-}\n-\n","filename":"src\/hotspot\/share\/oops\/compressedOops.inline.hpp","additions":0,"deletions":46,"binary":false,"changes":46,"status":"modified"},{"patch":"@@ -41,4 +41,1 @@\n-    return (UseCompressedClassPointers) ?\n-            klass_gap_offset_in_bytes() :\n-            sizeof(instanceOopDesc);\n-\n+    return sizeof(instanceOopDesc);\n","filename":"src\/hotspot\/share\/oops\/instanceOop.hpp","additions":1,"deletions":4,"binary":false,"changes":5,"status":"modified"},{"patch":"@@ -126,1 +126,1 @@\n-  static void trace_reference_gc(const char *s, oop obj) NOT_DEBUG_RETURN;\n+  void trace_reference_gc(const char *s, oop obj) NOT_DEBUG_RETURN;\n","filename":"src\/hotspot\/share\/oops\/instanceRefKlass.hpp","additions":1,"deletions":1,"binary":false,"changes":2,"status":"modified"},{"patch":"@@ -53,0 +53,1 @@\n+#include \"runtime\/globals.hpp\"\n@@ -54,0 +55,1 @@\n+#include \"utilities\/align.hpp\"\n@@ -195,1 +197,3 @@\n-  return Metaspace::allocate(loader_data, word_size, MetaspaceObj::ClassType, THREAD);\n+  MetaWord* p = Metaspace::allocate(loader_data, word_size, MetaspaceObj::ClassType, THREAD);\n+  assert(is_aligned(p, KlassAlignmentInBytes), \"metaspace returned badly aligned memory.\");\n+  return p;\n@@ -203,0 +207,1 @@\n+                           _prototype_header(markWord::prototype() LP64_ONLY(.set_klass(this))),\n@@ -747,0 +752,2 @@\n+     st->print(BULLET\"prototype_header: \" INTPTR_FORMAT, _prototype_header.value());\n+     st->cr();\n@@ -770,0 +777,4 @@\n+  if (UseCompressedClassPointers) {\n+    assert(is_aligned(this, KlassAlignmentInBytes), \"misaligned Klass structure\");\n+  }\n+\n","filename":"src\/hotspot\/share\/oops\/klass.cpp","additions":12,"deletions":1,"binary":false,"changes":13,"status":"modified"},{"patch":"@@ -168,0 +168,2 @@\n+  markWord _prototype_header;   \/\/ Used to initialize objects' header\n+\n@@ -671,0 +673,4 @@\n+  markWord prototype_header() const      { return _prototype_header; }\n+  inline void set_prototype_header(markWord header);\n+  static ByteSize prototype_header_offset() { return in_ByteSize(offset_of(Klass, _prototype_header)); }\n+\n","filename":"src\/hotspot\/share\/oops\/klass.hpp","additions":6,"deletions":0,"binary":false,"changes":6,"status":"modified"},{"patch":"@@ -55,0 +55,4 @@\n+inline void Klass::set_prototype_header(markWord header) {\n+  _prototype_header = header;\n+}\n+\n","filename":"src\/hotspot\/share\/oops\/klass.inline.hpp","additions":4,"deletions":0,"binary":false,"changes":4,"status":"modified"},{"patch":"@@ -26,0 +26,1 @@\n+#include \"oops\/compressedOops.inline.hpp\"\n","filename":"src\/hotspot\/share\/oops\/markWord.cpp","additions":1,"deletions":0,"binary":false,"changes":1,"status":"modified"},{"patch":"@@ -29,1 +29,1 @@\n-#include \"oops\/oopsHierarchy.hpp\"\n+#include \"oops\/compressedKlass.hpp\"\n@@ -44,1 +44,1 @@\n-\/\/  unused:25 hash:31 -->| unused_gap:1  age:4  unused_gap:1  lock:2 (normal object)\n+\/\/  nklass:32 hash:25 -->| unused_gap:1  age:4  unused_gap:1  lock:2 (normal object)\n@@ -105,4 +105,6 @@\n-  static const int first_unused_gap_bits          = 1;\n-  static const int max_hash_bits                  = BitsPerWord - age_bits - lock_bits - first_unused_gap_bits;\n-  static const int hash_bits                      = max_hash_bits > 31 ? 31 : max_hash_bits;\n-  static const int second_unused_gap_bits         = LP64_ONLY(1) NOT_LP64(0);\n+  static const int self_forwarded_bits            = 1;\n+  static const int max_hash_bits                  = BitsPerWord - age_bits - lock_bits - self_forwarded_bits;\n+  static const int hash_bits                      = max_hash_bits > 25 ? 25 : max_hash_bits;\n+#ifdef _LP64\n+  static const int klass_bits                     = 32;\n+#endif\n@@ -111,2 +113,6 @@\n-  static const int age_shift                      = lock_bits + first_unused_gap_bits;\n-  static const int hash_shift                     = age_shift + age_bits + second_unused_gap_bits;\n+  static const int self_forwarded_shift           = lock_shift + lock_bits;\n+  static const int age_shift                      = self_forwarded_shift + self_forwarded_bits;\n+  static const int hash_shift                     = age_shift + age_bits;\n+#ifdef _LP64\n+  static const int klass_shift                    = hash_shift + hash_bits;\n+#endif\n@@ -116,0 +122,2 @@\n+  static const uintptr_t self_forwarded_mask      = right_n_bits(self_forwarded_bits);\n+  static const uintptr_t self_forwarded_mask_in_place = self_forwarded_mask << self_forwarded_shift;\n@@ -121,0 +129,5 @@\n+#ifdef _LP64\n+  static const uintptr_t klass_mask               = right_n_bits(klass_bits);\n+  static const uintptr_t klass_mask_in_place      = klass_mask << klass_shift;\n+#endif\n+\n@@ -173,1 +186,1 @@\n-    return ((value() & lock_mask_in_place) == locked_value);\n+    return !UseFastLocking && ((value() & lock_mask_in_place) == locked_value);\n@@ -179,0 +192,8 @@\n+\n+  bool is_fast_locked() const {\n+    return UseFastLocking && ((value() & lock_mask_in_place) == locked_value);\n+  }\n+  markWord set_fast_locked() const {\n+    return markWord(value() & ~lock_mask_in_place);\n+  }\n+\n@@ -188,1 +209,3 @@\n-    return ((value() & unlocked_value) == 0);\n+    intptr_t lockbits = value() & lock_mask_in_place;\n+    return UseFastLocking ? lockbits == monitor_value   \/\/ monitor?\n+                    : (lockbits & unlocked_value) == 0; \/\/ monitor | stack-locked?\n@@ -235,0 +258,9 @@\n+#ifdef _LP64\n+  inline Klass* klass() const;\n+  inline Klass* klass_or_null() const;\n+  inline Klass* safe_klass() const;\n+  inline markWord set_klass(const Klass* klass) const;\n+  inline narrowKlass narrow_klass() const;\n+  inline markWord set_narrow_klass(const narrowKlass klass) const;\n+#endif\n+\n@@ -248,0 +280,8 @@\n+\n+  inline bool self_forwarded() const {\n+    return mask_bits(value(), self_forwarded_mask_in_place) != 0;\n+  }\n+\n+  inline markWord set_self_forwarded() const {\n+    return markWord(value() | self_forwarded_mask_in_place | marked_value);\n+  }\n","filename":"src\/hotspot\/share\/oops\/markWord.hpp","additions":50,"deletions":10,"binary":false,"changes":60,"status":"modified"},{"patch":"@@ -0,0 +1,68 @@\n+\/*\n+ * Copyright (c) 2021, Oracle and\/or its affiliates. All rights reserved.\n+ * DO NOT ALTER OR REMOVE COPYRIGHT NOTICES OR THIS FILE HEADER.\n+ *\n+ * This code is free software; you can redistribute it and\/or modify it\n+ * under the terms of the GNU General Public License version 2 only, as\n+ * published by the Free Software Foundation.\n+ *\n+ * This code is distributed in the hope that it will be useful, but WITHOUT\n+ * ANY WARRANTY; without even the implied warranty of MERCHANTABILITY or\n+ * FITNESS FOR A PARTICULAR PURPOSE.  See the GNU General Public License\n+ * version 2 for more details (a copy is included in the LICENSE file that\n+ * accompanied this code).\n+ *\n+ * You should have received a copy of the GNU General Public License version\n+ * 2 along with this work; if not, write to the Free Software Foundation,\n+ * Inc., 51 Franklin St, Fifth Floor, Boston, MA 02110-1301 USA.\n+ *\n+ * Please contact Oracle, 500 Oracle Parkway, Redwood Shores, CA 94065 USA\n+ * or visit www.oracle.com if you need additional information or have any\n+ * questions.\n+ *\n+ *\/\n+\n+#ifndef SHARE_OOPS_MARKWORD_INLINE_HPP\n+#define SHARE_OOPS_MARKWORD_INLINE_HPP\n+\n+#include \"oops\/compressedKlass.inline.hpp\"\n+#include \"oops\/compressedOops.inline.hpp\"\n+#include \"oops\/markWord.hpp\"\n+#include \"runtime\/safepoint.hpp\"\n+\n+#ifdef _LP64\n+narrowKlass markWord::narrow_klass() const {\n+  return narrowKlass(value() >> klass_shift);\n+}\n+\n+Klass* markWord::klass() const {\n+  assert(!CompressedKlassPointers::is_null(narrow_klass()), \"narrow klass must not be null: \" INTPTR_FORMAT, value());\n+  return CompressedKlassPointers::decode_not_null(narrow_klass());\n+}\n+\n+Klass* markWord::klass_or_null() const {\n+  return CompressedKlassPointers::decode(narrow_klass());\n+}\n+\n+markWord markWord::set_narrow_klass(const narrowKlass nklass) const {\n+  return markWord((value() & ~klass_mask_in_place) | ((uintptr_t) nklass << klass_shift));\n+}\n+\n+Klass* markWord::safe_klass() const {\n+  assert(SafepointSynchronize::is_at_safepoint(), \"only call at safepoint\");\n+  markWord m = *this;\n+  if (m.has_displaced_mark_helper()) {\n+    m = m.displaced_mark_helper();\n+  }\n+  return CompressedKlassPointers::decode_not_null(m.narrow_klass());\n+}\n+\n+markWord markWord::set_klass(const Klass* klass) const {\n+  assert(UseCompressedClassPointers, \"expect compressed klass pointers\");\n+  \/\/ TODO: Don't cast to non-const, change CKP::encode() to accept const Klass* instead.\n+  narrowKlass nklass = CompressedKlassPointers::encode(const_cast<Klass*>(klass));\n+  return set_narrow_klass(nklass);\n+}\n+#endif\n+\n+#endif \/\/ SHARE_OOPS_MARKWORD_INLINE_HPP\n","filename":"src\/hotspot\/share\/oops\/markWord.inline.hpp","additions":68,"deletions":0,"binary":false,"changes":68,"status":"added"},{"patch":"@@ -159,1 +159,0 @@\n-  assert(obj->is_objArray(), \"must be object array\");\n","filename":"src\/hotspot\/share\/oops\/objArrayKlass.cpp","additions":0,"deletions":1,"binary":false,"changes":1,"status":"modified"},{"patch":"@@ -73,1 +73,0 @@\n-  assert (obj->is_array(), \"obj must be array\");\n","filename":"src\/hotspot\/share\/oops\/objArrayKlass.inline.hpp","additions":0,"deletions":1,"binary":false,"changes":1,"status":"modified"},{"patch":"@@ -55,23 +55,3 @@\n-  \/\/ Give size of objArrayOop in HeapWords minus the header\n-  static int array_size(int length) {\n-    const uint OopsPerHeapWord = HeapWordSize\/heapOopSize;\n-    assert(OopsPerHeapWord >= 1 && (HeapWordSize % heapOopSize == 0),\n-           \"Else the following (new) computation would be in error\");\n-    uint res = ((uint)length + OopsPerHeapWord - 1)\/OopsPerHeapWord;\n-#ifdef ASSERT\n-    \/\/ The old code is left in for sanity-checking; it'll\n-    \/\/ go away pretty soon. XXX\n-    \/\/ Without UseCompressedOops, this is simply:\n-    \/\/ oop->length() * HeapWordsPerOop;\n-    \/\/ With narrowOops, HeapWordsPerOop is 1\/2 or equal 0 as an integer.\n-    \/\/ The oop elements are aligned up to wordSize\n-    const uint HeapWordsPerOop = heapOopSize\/HeapWordSize;\n-    uint old_res;\n-    if (HeapWordsPerOop > 0) {\n-      old_res = length * HeapWordsPerOop;\n-    } else {\n-      old_res = align_up((uint)length, OopsPerHeapWord)\/OopsPerHeapWord;\n-    }\n-    assert(res == old_res, \"Inconsistency between old and new.\");\n-#endif  \/\/ ASSERT\n-    return res;\n+  \/\/ Give size of objArrayOop in bytes minus the header\n+  static size_t array_size_in_bytes(int length) {\n+    return (size_t)length * heapOopSize;\n@@ -97,1 +77,0 @@\n-  static int header_size()    { return arrayOopDesc::header_size(T_OBJECT); }\n@@ -102,5 +81,5 @@\n-    uint asz = array_size(length);\n-    uint osz = align_object_size(header_size() + asz);\n-    assert(osz >= asz,   \"no overflow\");\n-    assert((int)osz > 0, \"no overflow\");\n-    return (size_t)osz;\n+    size_t asz = array_size_in_bytes(length);\n+    size_t size_words = align_up(base_offset_in_bytes() + asz, HeapWordSize) \/ HeapWordSize;\n+    size_t osz = align_object_size(size_words);\n+    assert(osz < max_jint, \"no overflow\");\n+    return osz;\n","filename":"src\/hotspot\/share\/oops\/objArrayOop.hpp","additions":8,"deletions":29,"binary":false,"changes":37,"status":"modified"},{"patch":"@@ -33,0 +33,1 @@\n+#include \"oops\/compressedKlass.inline.hpp\"\n@@ -37,0 +38,1 @@\n+#include \"runtime\/interfaceSupport.inline.hpp\"\n@@ -125,1 +127,1 @@\n-  if (ignore_mark_word) {\n+  if (ignore_mark_word || UseFastLocking) {\n@@ -157,13 +159,0 @@\n-bool oopDesc::has_klass_gap() {\n-  \/\/ Only has a klass gap when compressed class pointers are used.\n-  return UseCompressedClassPointers;\n-}\n-\n-#if INCLUDE_CDS_JAVA_HEAP\n-void oopDesc::set_narrow_klass(narrowKlass nk) {\n-  assert(DumpSharedSpaces, \"Used by CDS only. Do not abuse!\");\n-  assert(UseCompressedClassPointers, \"must be\");\n-  _metadata._compressed_klass = nk;\n-}\n-#endif\n-\n@@ -171,7 +160,8 @@\n-  if (UseCompressedClassPointers) {\n-    narrowKlass narrow_klass = obj->_metadata._compressed_klass;\n-    if (narrow_klass == 0) return nullptr;\n-    return (void*)CompressedKlassPointers::decode_raw(narrow_klass);\n-  } else {\n-    return obj->_metadata._klass;\n-  }\n+  \/\/ TODO: Remove method altogether and replace with calls to obj->klass() ?\n+  \/\/ OTOH, we may eventually get rid of locking in header, and then no\n+  \/\/ longer have to deal with that anymore.\n+#ifdef _LP64\n+  return obj->klass();\n+#else\n+  return obj->_klass;\n+#endif\n","filename":"src\/hotspot\/share\/oops\/oop.cpp","additions":11,"deletions":21,"binary":false,"changes":32,"status":"modified"},{"patch":"@@ -31,0 +31,1 @@\n+#include \"oops\/compressedKlass.hpp\"\n@@ -57,4 +58,3 @@\n-  union _metadata {\n-    Klass*      _klass;\n-    narrowKlass _compressed_klass;\n-  } _metadata;\n+#ifndef _LP64\n+  Klass*            _klass;\n+#endif\n@@ -78,0 +78,1 @@\n+  static inline void release_set_mark(HeapWord* mem, markWord m);\n@@ -81,0 +82,2 @@\n+  inline markWord resolve_mark() const;\n+\n@@ -91,1 +94,1 @@\n-  void set_narrow_klass(narrowKlass nk) NOT_CDS_JAVA_HEAP_RETURN;\n+#ifndef _LP64\n@@ -94,3 +97,1 @@\n-\n-  \/\/ For klass field compression\n-  static inline void set_klass_gap(HeapWord* mem, int z);\n+#endif\n@@ -263,0 +264,1 @@\n+  inline void forward_to_self();\n@@ -269,0 +271,1 @@\n+  inline oop forward_to_self_atomic(markWord compare, atomic_memory_order order = memory_order_conservative);\n@@ -271,0 +274,1 @@\n+  inline oop forwardee(markWord header) const;\n@@ -310,2 +314,0 @@\n-  static bool has_klass_gap();\n-\n@@ -314,4 +316,7 @@\n-  static int klass_offset_in_bytes()     { return offset_of(oopDesc, _metadata._klass); }\n-  static int klass_gap_offset_in_bytes() {\n-    assert(has_klass_gap(), \"only applicable to compressed klass pointers\");\n-    return klass_offset_in_bytes() + sizeof(narrowKlass);\n+  static int klass_offset_in_bytes()     {\n+#ifdef _LP64\n+    STATIC_ASSERT(markWord::klass_shift % 8 == 0);\n+    return mark_offset_in_bytes() + markWord::klass_shift \/ 8;\n+#else\n+    return offset_of(oopDesc, _klass);\n+#endif\n","filename":"src\/hotspot\/share\/oops\/oop.hpp","additions":19,"deletions":14,"binary":false,"changes":33,"status":"modified"},{"patch":"@@ -35,0 +35,1 @@\n+#include \"oops\/compressedKlass.inline.hpp\"\n@@ -37,1 +38,1 @@\n-#include \"oops\/markWord.hpp\"\n+#include \"oops\/markWord.inline.hpp\"\n@@ -41,0 +42,3 @@\n+#include \"runtime\/objectMonitor.inline.hpp\"\n+#include \"runtime\/safepoint.hpp\"\n+#include \"runtime\/synchronizer.hpp\"\n@@ -73,0 +77,4 @@\n+void oopDesc::release_set_mark(HeapWord* mem, markWord m) {\n+  Atomic::release_store((markWord*)(((char*)mem) + mark_offset_in_bytes()), m);\n+}\n+\n@@ -81,0 +89,9 @@\n+markWord oopDesc::resolve_mark() const {\n+  markWord hdr = mark();\n+  if (hdr.has_monitor()) {\n+    ObjectMonitor* monitor = hdr.monitor();\n+    return monitor->header();\n+  }\n+  return hdr;\n+}\n+\n@@ -82,1 +99,8 @@\n-  set_mark(markWord::prototype());\n+#ifdef _LP64\n+  markWord header = resolve_mark();\n+  assert(UseCompressedClassPointers, \"expect compressed klass pointers\");\n+  header = markWord((header.value() & markWord::klass_mask_in_place) | markWord::prototype().value());\n+#else\n+  markWord header = markWord::prototype();\n+#endif\n+  set_mark(header);\n@@ -86,5 +110,7 @@\n-  if (UseCompressedClassPointers) {\n-    return CompressedKlassPointers::decode_not_null(_metadata._compressed_klass);\n-  } else {\n-    return _metadata._klass;\n-  }\n+#ifdef _LP64\n+  assert(UseCompressedClassPointers, \"only with compressed class pointers\");\n+  markWord header = resolve_mark();\n+  return header.klass();\n+#else\n+  return _klass;\n+#endif\n@@ -94,5 +120,7 @@\n-  if (UseCompressedClassPointers) {\n-    return CompressedKlassPointers::decode(_metadata._compressed_klass);\n-  } else {\n-    return _metadata._klass;\n-  }\n+#ifdef _LP64\n+  assert(UseCompressedClassPointers, \"only with compressed class pointers\");\n+  markWord header = resolve_mark();\n+  return header.klass_or_null();\n+#else\n+  return _klass;\n+#endif\n@@ -102,5 +130,5 @@\n-  if (UseCompressedClassPointers) {\n-    narrowKlass nklass = Atomic::load_acquire(&_metadata._compressed_klass);\n-    return CompressedKlassPointers::decode(nklass);\n-  } else {\n-    return Atomic::load_acquire(&_metadata._klass);\n+#ifdef _LP64\n+  assert(UseCompressedClassPointers, \"only with compressed class pointers\");\n+  markWord header = mark_acquire();\n+  if (header.has_monitor()) {\n+    header = header.monitor()->header();\n@@ -108,0 +136,4 @@\n+  return header.klass_or_null();\n+#else\n+  return Atomic::load_acquire(&_klass);\n+#endif\n@@ -111,5 +143,1 @@\n-  if (UseCompressedClassPointers) {\n-    return CompressedKlassPointers::decode_raw(_metadata._compressed_klass);\n-  } else {\n-    return _metadata._klass;\n-  }\n+  return klass();\n@@ -118,0 +146,1 @@\n+#ifndef _LP64\n@@ -120,5 +149,1 @@\n-  if (UseCompressedClassPointers) {\n-    _metadata._compressed_klass = CompressedKlassPointers::encode_not_null(k);\n-  } else {\n-    _metadata._klass = k;\n-  }\n+  _klass = k;\n@@ -137,6 +162,1 @@\n-\n-void oopDesc::set_klass_gap(HeapWord* mem, int v) {\n-  if (UseCompressedClassPointers) {\n-    *(int*)(((char*)mem) + klass_gap_offset_in_bytes()) = v;\n-  }\n-}\n+#endif\n@@ -271,1 +291,15 @@\n-  assert(m.decode_pointer() == p, \"encoding must be reversible\");\n+  assert(forwardee(m) == p, \"encoding must be reversable\");\n+  set_mark(m);\n+}\n+\n+void oopDesc::forward_to_self() {\n+#ifdef _LP64\n+  verify_forwardee(this);\n+  markWord m = mark();\n+  \/\/ If mark is displaced, we need to preserve the Klass* from real header.\n+  assert(SafepointSynchronize::is_at_safepoint(), \"we can only safely fetch the displaced header at safepoint\");\n+  if (m.has_displaced_mark_helper()) {\n+    m = m.displaced_mark_helper();\n+  }\n+  m = m.set_self_forwarded();\n+  assert(forwardee(m) == cast_to_oop(this), \"encoding must be reversable\");\n@@ -273,0 +307,3 @@\n+#else\n+  forward_to(oop(this));\n+#endif\n@@ -278,1 +315,1 @@\n-  assert(m.decode_pointer() == p, \"encoding must be reversible\");\n+  assert(forwardee(m) == p, \"encoding must be reversable\");\n@@ -283,1 +320,1 @@\n-    return cast_to_oop(old_mark.decode_pointer());\n+    return forwardee(old_mark);\n@@ -287,0 +324,23 @@\n+oop oopDesc::forward_to_self_atomic(markWord compare, atomic_memory_order order) {\n+#ifdef _LP64\n+  verify_forwardee(this);\n+  markWord m = compare;\n+  \/\/ If mark is displaced, we need to preserve the Klass* from real header.\n+  assert(SafepointSynchronize::is_at_safepoint(), \"we can only safely fetch the displaced header at safepoint\");\n+  if (m.has_displaced_mark_helper()) {\n+    m = m.displaced_mark_helper();\n+  }\n+  m = m.set_self_forwarded();\n+  assert(forwardee(m) == cast_to_oop(this), \"encoding must be reversable\");\n+  markWord old_mark = cas_set_mark(m, compare, order);\n+  if (old_mark == compare) {\n+    return NULL;\n+  } else {\n+    assert(old_mark.is_marked(), \"must be marked here\");\n+    return forwardee(old_mark);\n+  }\n+#else\n+  return forward_to_atomic(oop(this), compare, order);\n+#endif\n+}\n+\n@@ -291,2 +351,14 @@\n-  assert(is_forwarded(), \"only decode when actually forwarded\");\n-  return cast_to_oop(mark().decode_pointer());\n+  return forwardee(mark());\n+}\n+\n+oop oopDesc::forwardee(markWord header) const {\n+  assert(header.is_marked(), \"must be forwarded\");\n+#ifdef _LP64\n+  if (header.self_forwarded()) {\n+    return cast_to_oop(this);\n+  } else\n+#endif\n+  {\n+    assert(header.is_marked(), \"only decode when actually forwarded\");\n+    return cast_to_oop(header.decode_pointer());\n+  }\n@@ -347,1 +419,0 @@\n-  assert(k == klass(), \"wrong klass\");\n","filename":"src\/hotspot\/share\/oops\/oop.inline.hpp","additions":110,"deletions":39,"binary":false,"changes":149,"status":"modified"},{"patch":"@@ -40,3 +40,0 @@\n-\/\/ If compressed klass pointers then use narrowKlass.\n-typedef juint  narrowKlass;\n-\n","filename":"src\/hotspot\/share\/oops\/oopsHierarchy.hpp","additions":0,"deletions":3,"binary":false,"changes":3,"status":"modified"},{"patch":"@@ -231,1 +231,0 @@\n-  assert(obj->is_typeArray(),\"must be a type array\");\n","filename":"src\/hotspot\/share\/oops\/typeArrayKlass.cpp","additions":0,"deletions":1,"binary":false,"changes":1,"status":"modified"},{"patch":"@@ -38,1 +38,0 @@\n-  assert(obj->is_typeArray(),\"must be a type array\");\n","filename":"src\/hotspot\/share\/oops\/typeArrayKlass.inline.hpp","additions":0,"deletions":1,"binary":false,"changes":1,"status":"modified"},{"patch":"@@ -89,0 +89,20 @@\n+class C2CheckLockStackStub : public C2CodeStub {\n+public:\n+  C2CheckLockStackStub() : C2CodeStub() {}\n+\n+  int max_size() const;\n+  void emit(C2_MacroAssembler& masm);\n+};\n+\n+#ifdef _LP64\n+class C2LoadNKlassStub : public C2CodeStub {\n+private:\n+  Register _dst;\n+public:\n+  C2LoadNKlassStub(Register dst) : C2CodeStub(), _dst(dst) {}\n+  Register dst() { return _dst; }\n+  int max_size() const;\n+  void emit(C2_MacroAssembler& masm);\n+};\n+#endif\n+\n","filename":"src\/hotspot\/share\/opto\/c2_CodeStubs.hpp","additions":20,"deletions":0,"binary":false,"changes":20,"status":"modified"},{"patch":"@@ -1575,3 +1575,3 @@\n-  Node* mark_node = NULL;\n-  \/\/ For now only enable fast locking for non-array types\n-  mark_node = phase->MakeConX(markWord::prototype().value());\n+  Node* klass_node = in(AllocateNode::KlassNode);\n+  Node* proto_adr = phase->transform(new AddPNode(klass_node, klass_node, phase->MakeConX(in_bytes(Klass::prototype_header_offset()))));\n+  Node* mark_node = LoadNode::make(*phase, control, mem, proto_adr, TypeRawPtr::BOTTOM, TypeX_X, TypeX_X->basic_type(), MemNode::unordered);\n","filename":"src\/hotspot\/share\/opto\/callnode.cpp","additions":3,"deletions":3,"binary":false,"changes":6,"status":"modified"},{"patch":"@@ -1024,0 +1024,1 @@\n+  reset_max_monitors();\n","filename":"src\/hotspot\/share\/opto\/compile.cpp","additions":1,"deletions":0,"binary":false,"changes":1,"status":"modified"},{"patch":"@@ -340,0 +340,1 @@\n+  uint                  _max_monitors;          \/\/ Keep track of maximum number of active monitors in this compilation\n@@ -634,0 +635,4 @@\n+  void          push_monitor() { _max_monitors++; }\n+  void          reset_max_monitors() { _max_monitors = 0; }\n+  uint          max_monitors() { return _max_monitors; }\n+\n","filename":"src\/hotspot\/share\/opto\/compile.hpp","additions":5,"deletions":0,"binary":false,"changes":5,"status":"modified"},{"patch":"@@ -183,0 +183,1 @@\n+  C->push_monitor();\n","filename":"src\/hotspot\/share\/opto\/locknode.cpp","additions":1,"deletions":0,"binary":false,"changes":1,"status":"modified"},{"patch":"@@ -1664,0 +1664,1 @@\n+#ifndef _LP64\n@@ -1665,0 +1666,1 @@\n+#endif\n","filename":"src\/hotspot\/share\/opto\/macro.cpp","additions":2,"deletions":0,"binary":false,"changes":2,"status":"modified"},{"patch":"@@ -426,0 +426,1 @@\n+    C->push_monitor();\n","filename":"src\/hotspot\/share\/opto\/parse1.cpp","additions":1,"deletions":0,"binary":false,"changes":1,"status":"modified"},{"patch":"@@ -311,1 +311,1 @@\n-    const size_t hs = arrayOopDesc::header_size(elem_type);\n+    const size_t hs_bytes = arrayOopDesc::base_offset_in_bytes(elem_type);\n@@ -313,1 +313,1 @@\n-    const size_t aligned_hs = align_object_offset(hs);\n+    const size_t aligned_hs_bytes = align_up(hs_bytes, BytesPerLong);\n@@ -315,2 +315,2 @@\n-    if (aligned_hs > hs) {\n-      Copy::zero_to_words(obj+hs, aligned_hs-hs);\n+    if (aligned_hs_bytes > hs_bytes) {\n+      Copy::zero_to_bytes(obj + hs_bytes, aligned_hs_bytes - hs_bytes);\n@@ -319,0 +319,1 @@\n+    const size_t aligned_hs = aligned_hs_bytes \/ HeapWordSize;\n","filename":"src\/hotspot\/share\/opto\/runtime.cpp","additions":5,"deletions":4,"binary":false,"changes":9,"status":"modified"},{"patch":"@@ -5153,1 +5153,2 @@\n-    int header_size = objArrayOopDesc::header_size() * wordSize;\n+    BasicType basic_elem_type = elem()->basic_type();\n+    int header_size = arrayOopDesc::base_offset_in_bytes(basic_elem_type);\n","filename":"src\/hotspot\/share\/opto\/type.cpp","additions":2,"deletions":1,"binary":false,"changes":3,"status":"modified"},{"patch":"@@ -70,1 +70,1 @@\n-  ( arrayOopDesc::header_size(T_DOUBLE) * HeapWordSize \\\n+  ( arrayOopDesc::base_offset_in_bytes(T_DOUBLE) \\\n","filename":"src\/hotspot\/share\/prims\/unsafe.cpp","additions":1,"deletions":1,"binary":false,"changes":2,"status":"modified"},{"patch":"@@ -1507,3 +1507,0 @@\n-      if (COMPRESSED_CLASS_POINTERS_DEPENDS_ON_COMPRESSED_OOPS) {\n-        FLAG_SET_DEFAULT(UseCompressedClassPointers, false);\n-      }\n@@ -1515,1 +1512,0 @@\n-\n@@ -1520,25 +1516,14 @@\n-  \/\/ On some architectures, the use of UseCompressedClassPointers implies the use of\n-  \/\/ UseCompressedOops. The reason is that the rheap_base register of said platforms\n-  \/\/ is reused to perform some optimized spilling, in order to use rheap_base as a\n-  \/\/ temp register. But by treating it as any other temp register, spilling can typically\n-  \/\/ be completely avoided instead. So it is better not to perform this trick. And by\n-  \/\/ not having that reliance, large heaps, or heaps not supporting compressed oops,\n-  \/\/ can still use compressed class pointers.\n-  if (COMPRESSED_CLASS_POINTERS_DEPENDS_ON_COMPRESSED_OOPS && !UseCompressedOops) {\n-    if (UseCompressedClassPointers) {\n-      warning(\"UseCompressedClassPointers requires UseCompressedOops\");\n-    }\n-    FLAG_SET_DEFAULT(UseCompressedClassPointers, false);\n-  } else {\n-    \/\/ Turn on UseCompressedClassPointers too\n-    if (FLAG_IS_DEFAULT(UseCompressedClassPointers)) {\n-      FLAG_SET_ERGO(UseCompressedClassPointers, true);\n-    }\n-    \/\/ Check the CompressedClassSpaceSize to make sure we use compressed klass ptrs.\n-    if (UseCompressedClassPointers) {\n-      if (CompressedClassSpaceSize > KlassEncodingMetaspaceMax) {\n-        warning(\"CompressedClassSpaceSize is too large for UseCompressedClassPointers\");\n-        FLAG_SET_DEFAULT(UseCompressedClassPointers, false);\n-      }\n-    }\n-  }\n+  if (!UseCompressedClassPointers) {\n+    \/\/ Lilliput requires compressed class pointers. Default shall reflect that.\n+    \/\/ If user specifies -UseCompressedClassPointers, it should be reverted with\n+    \/\/ a warning.\n+    assert(!FLAG_IS_DEFAULT(UseCompressedClassPointers), \"Wrong default for UseCompressedClassPointers\");\n+    warning(\"Lilliput reqires compressed class pointers.\");\n+    FLAG_SET_ERGO(UseCompressedClassPointers, true);\n+  }\n+  \/\/ Assert validity of compressed class space size. User arg should have been checked at this point\n+  \/\/ (see CompressedClassSpaceSizeConstraintFunc()), so no need to be nice about it, this fires in\n+  \/\/ case the default is wrong.\n+  assert(CompressedClassSpaceSize <= Metaspace::max_class_space_size(),\n+         \"CompressedClassSpaceSize \" SIZE_FORMAT \" too large (max: \" SIZE_FORMAT \")\",\n+         CompressedClassSpaceSize, Metaspace::max_class_space_size());\n@@ -1707,3 +1692,0 @@\n-          if (COMPRESSED_CLASS_POINTERS_DEPENDS_ON_COMPRESSED_OOPS) {\n-            FLAG_SET_ERGO(UseCompressedClassPointers, false);\n-          }\n@@ -3128,0 +3110,3 @@\n+  \/\/ Lilliput requires fast-locking.\n+  FLAG_SET_DEFAULT(UseFastLocking, true);\n+\n@@ -4101,0 +4086,1 @@\n+\n","filename":"src\/hotspot\/share\/runtime\/arguments.cpp","additions":18,"deletions":32,"binary":false,"changes":50,"status":"modified"},{"patch":"@@ -128,1 +128,1 @@\n-  product(bool, UseCompressedClassPointers, false,                          \\\n+  product(bool, UseCompressedClassPointers, true,                           \\\n@@ -1059,1 +1059,1 @@\n-  develop(bool, UseHeavyMonitors, false,                                    \\\n+  product(bool, UseHeavyMonitors, false, DIAGNOSTIC,                        \\\n@@ -1416,1 +1416,1 @@\n-          range(1*M, 3*G)                                                   \\\n+          constraint(CompressedClassSpaceSizeConstraintFunc,AtParse)        \\\n@@ -1418,1 +1418,1 @@\n-  develop(size_t, CompressedClassSpaceBaseAddress, 0,                       \\\n+  product(size_t, CompressedClassSpaceBaseAddress, 0, DIAGNOSTIC,           \\\n@@ -1981,0 +1981,9 @@\n+                                                                            \\\n+  product(bool, HeapObjectStats, false, DIAGNOSTIC,                         \\\n+             \"Enable gathering of heap object statistics\")                  \\\n+                                                                            \\\n+  product(size_t, HeapObjectStatsSamplingInterval, 500, DIAGNOSTIC,         \\\n+             \"Heap object statistics sampling interval (ms)\")               \\\n+                                                                            \\\n+  product(bool, UseFastLocking, false, EXPERIMENTAL,                        \\\n+                \"Use fast-locking instead of stack-locking\")                \\\n","filename":"src\/hotspot\/share\/runtime\/globals.hpp","additions":13,"deletions":4,"binary":false,"changes":17,"status":"modified"},{"patch":"@@ -73,0 +73,1 @@\n+#include \"runtime\/lockStack.inline.hpp\"\n@@ -490,2 +491,3 @@\n-  _SleepEvent(ParkEvent::Allocate(this))\n-{\n+  _SleepEvent(ParkEvent::Allocate(this)),\n+\n+  _lock_stack() {\n@@ -988,0 +990,1 @@\n+  assert(!UseFastLocking, \"should not be called with fast-locking\");\n@@ -1381,0 +1384,4 @@\n+\n+  if (!UseHeavyMonitors && UseFastLocking) {\n+    lock_stack().oops_do(f);\n+  }\n","filename":"src\/hotspot\/share\/runtime\/javaThread.cpp","additions":9,"deletions":2,"binary":false,"changes":11,"status":"modified"},{"patch":"@@ -37,0 +37,1 @@\n+#include \"runtime\/lockStack.hpp\"\n@@ -1143,0 +1144,10 @@\n+private:\n+  LockStack _lock_stack;\n+\n+public:\n+  LockStack& lock_stack() { return _lock_stack; }\n+\n+  static ByteSize lock_stack_current_offset()    { return byte_offset_of(JavaThread, _lock_stack) + LockStack::current_offset(); }\n+  static ByteSize lock_stack_limit_offset()    { return byte_offset_of(JavaThread, _lock_stack) + LockStack::limit_offset(); }\n+\n+\n","filename":"src\/hotspot\/share\/runtime\/javaThread.hpp","additions":11,"deletions":0,"binary":false,"changes":11,"status":"modified"},{"patch":"@@ -0,0 +1,78 @@\n+\/*\n+ * Copyright (c) 2022, Oracle and\/or its affiliates. All rights reserved.\n+ * DO NOT ALTER OR REMOVE COPYRIGHT NOTICES OR THIS FILE HEADER.\n+ *\n+ * This code is free software; you can redistribute it and\/or modify it\n+ * under the terms of the GNU General Public License version 2 only, as\n+ * published by the Free Software Foundation.\n+ *\n+ * This code is distributed in the hope that it will be useful, but WITHOUT\n+ * ANY WARRANTY; without even the implied warranty of MERCHANTABILITY or\n+ * FITNESS FOR A PARTICULAR PURPOSE.  See the GNU General Public License\n+ * version 2 for more details (a copy is included in the LICENSE file that\n+ * accompanied this code).\n+ *\n+ * You should have received a copy of the GNU General Public License version\n+ * 2 along with this work; if not, write to the Free Software Foundation,\n+ * Inc., 51 Franklin St, Fifth Floor, Boston, MA 02110-1301 USA.\n+ *\n+ * Please contact Oracle, 500 Oracle Parkway, Redwood Shores, CA 94065 USA\n+ * or visit www.oracle.com if you need additional information or have any\n+ * questions.\n+ *\n+ *\/\n+\n+#include \"precompiled.hpp\"\n+#include \"memory\/allocation.hpp\"\n+#include \"runtime\/lockStack.hpp\"\n+#include \"runtime\/safepoint.hpp\"\n+#include \"runtime\/thread.hpp\"\n+#include \"utilities\/copy.hpp\"\n+#include \"utilities\/ostream.hpp\"\n+\n+LockStack::LockStack() :\n+  _base(UseFastLocking && !UseHeavyMonitors ? NEW_C_HEAP_ARRAY(oop, INITIAL_CAPACITY, mtSynchronizer) : NULL),\n+  _limit(_base + INITIAL_CAPACITY),\n+  _current(_base) {\n+}\n+\n+LockStack::~LockStack() {\n+  if (UseFastLocking && !UseHeavyMonitors) {\n+    FREE_C_HEAP_ARRAY(oop, _base);\n+  }\n+}\n+\n+#ifndef PRODUCT\n+void LockStack::validate(const char* msg) const {\n+  assert(UseFastLocking && !UseHeavyMonitors, \"never use lock-stack when fast-locking is disabled\");\n+  for (oop* loc1 = _base; loc1 < _current - 1; loc1++) {\n+    for (oop* loc2 = loc1 + 1; loc2 < _current; loc2++) {\n+      assert(*loc1 != *loc2, \"entries must be unique: %s\", msg);\n+    }\n+  }\n+}\n+#endif\n+\n+void LockStack::grow(size_t min_capacity) {\n+  \/\/ Grow stack.\n+  assert(_limit > _base, \"invariant\");\n+  size_t capacity = _limit - _base;\n+  size_t index = _current - _base;\n+  size_t new_capacity = MAX2(min_capacity, capacity * 2);\n+  oop* new_stack = NEW_C_HEAP_ARRAY(oop, new_capacity, mtSynchronizer);\n+  for (size_t i = 0; i < index; i++) {\n+    *(new_stack + i) = *(_base + i);\n+  }\n+  FREE_C_HEAP_ARRAY(oop, _base);\n+  _base = new_stack;\n+  _limit = _base + new_capacity;\n+  _current = _base + index;\n+  assert(_current < _limit, \"must fit after growing\");\n+  assert((_limit - _base) >= (ptrdiff_t) min_capacity, \"must grow enough\");\n+}\n+\n+void LockStack::ensure_lock_stack_size(oop* required_limit) {\n+  JavaThread* jt = JavaThread::current();\n+  LockStack& lock_stack = jt->lock_stack();\n+  lock_stack.grow(required_limit - lock_stack._base);\n+}\n","filename":"src\/hotspot\/share\/runtime\/lockStack.cpp","additions":78,"deletions":0,"binary":false,"changes":78,"status":"added"},{"patch":"@@ -0,0 +1,67 @@\n+\/*\n+ * Copyright (c) 2022, Oracle and\/or its affiliates. All rights reserved.\n+ * DO NOT ALTER OR REMOVE COPYRIGHT NOTICES OR THIS FILE HEADER.\n+ *\n+ * This code is free software; you can redistribute it and\/or modify it\n+ * under the terms of the GNU General Public License version 2 only, as\n+ * published by the Free Software Foundation.\n+ *\n+ * This code is distributed in the hope that it will be useful, but WITHOUT\n+ * ANY WARRANTY; without even the implied warranty of MERCHANTABILITY or\n+ * FITNESS FOR A PARTICULAR PURPOSE.  See the GNU General Public License\n+ * version 2 for more details (a copy is included in the LICENSE file that\n+ * accompanied this code).\n+ *\n+ * You should have received a copy of the GNU General Public License version\n+ * 2 along with this work; if not, write to the Free Software Foundation,\n+ * Inc., 51 Franklin St, Fifth Floor, Boston, MA 02110-1301 USA.\n+ *\n+ * Please contact Oracle, 500 Oracle Parkway, Redwood Shores, CA 94065 USA\n+ * or visit www.oracle.com if you need additional information or have any\n+ * questions.\n+ *\n+ *\/\n+\n+#ifndef SHARE_RUNTIME_LOCKSTACK_HPP\n+#define SHARE_RUNTIME_LOCKSTACK_HPP\n+\n+#include \"oops\/oopsHierarchy.hpp\"\n+#include \"utilities\/globalDefinitions.hpp\"\n+#include \"utilities\/sizes.hpp\"\n+\n+class Thread;\n+class OopClosure;\n+\n+class LockStack {\n+  friend class VMStructs;\n+private:\n+  static const size_t INITIAL_CAPACITY = 1;\n+  oop* _base;\n+  oop* _limit;\n+  oop* _current;\n+\n+  void grow(size_t min_capacity);\n+\n+  void validate(const char* msg) const PRODUCT_RETURN;\n+public:\n+  static ByteSize current_offset()    { return byte_offset_of(LockStack, _current); }\n+  static ByteSize base_offset()       { return byte_offset_of(LockStack, _base); }\n+  static ByteSize limit_offset()      { return byte_offset_of(LockStack, _limit); }\n+\n+  static void ensure_lock_stack_size(oop* _required_limit);\n+\n+  LockStack();\n+  ~LockStack();\n+\n+  inline void push(oop o);\n+  inline oop pop();\n+  inline void remove(oop o);\n+\n+  inline bool contains(oop o) const;\n+\n+  \/\/ GC support\n+  inline void oops_do(OopClosure* cl);\n+\n+};\n+\n+#endif \/\/ SHARE_RUNTIME_LOCKSTACK_HPP\n","filename":"src\/hotspot\/share\/runtime\/lockStack.hpp","additions":67,"deletions":0,"binary":false,"changes":67,"status":"added"},{"patch":"@@ -0,0 +1,93 @@\n+\/*\n+ * Copyright (c) 2022, Oracle and\/or its affiliates. All rights reserved.\n+ * DO NOT ALTER OR REMOVE COPYRIGHT NOTICES OR THIS FILE HEADER.\n+ *\n+ * This code is free software; you can redistribute it and\/or modify it\n+ * under the terms of the GNU General Public License version 2 only, as\n+ * published by the Free Software Foundation.\n+ *\n+ * This code is distributed in the hope that it will be useful, but WITHOUT\n+ * ANY WARRANTY; without even the implied warranty of MERCHANTABILITY or\n+ * FITNESS FOR A PARTICULAR PURPOSE.  See the GNU General Public License\n+ * version 2 for more details (a copy is included in the LICENSE file that\n+ * accompanied this code).\n+ *\n+ * You should have received a copy of the GNU General Public License version\n+ * 2 along with this work; if not, write to the Free Software Foundation,\n+ * Inc., 51 Franklin St, Fifth Floor, Boston, MA 02110-1301 USA.\n+ *\n+ * Please contact Oracle, 500 Oracle Parkway, Redwood Shores, CA 94065 USA\n+ * or visit www.oracle.com if you need additional information or have any\n+ * questions.\n+ *\n+ *\/\n+\n+#ifndef SHARE_RUNTIME_LOCKSTACK_INLINE_HPP\n+#define SHARE_RUNTIME_LOCKSTACK_INLINE_HPP\n+\n+#include \"memory\/iterator.hpp\"\n+#include \"runtime\/lockStack.hpp\"\n+\n+inline void LockStack::push(oop o) {\n+  validate(\"pre-push\");\n+  assert(oopDesc::is_oop(o), \"must be\");\n+  assert(!contains(o), \"entries must be unique\");\n+  if (_current >= _limit) {\n+    grow((_limit - _base) + 1);\n+  }\n+  *_current = o;\n+  _current++;\n+  validate(\"post-push\");\n+}\n+\n+inline oop LockStack::pop() {\n+  validate(\"pre-pop\");\n+  oop* new_loc = _current - 1;\n+  assert(new_loc < _current, \"underflow, probably unbalanced push\/pop\");\n+  _current = new_loc;\n+  oop o = *_current;\n+  assert(!contains(o), \"entries must be unique\");\n+  validate(\"post-pop\");\n+  return o;\n+}\n+\n+inline void LockStack::remove(oop o) {\n+  validate(\"pre-remove\");\n+  assert(contains(o), \"entry must be present\");\n+  for (oop* loc = _base; loc < _current; loc++) {\n+    if (*loc == o) {\n+      oop* last = _current - 1;\n+      for (; loc < last; loc++) {\n+        *loc = *(loc + 1);\n+      }\n+      _current--;\n+      break;\n+    }\n+  }\n+  assert(!contains(o), \"entries must be unique: \" PTR_FORMAT, p2i(o));\n+  validate(\"post-remove\");\n+}\n+\n+inline bool LockStack::contains(oop o) const {\n+  validate(\"pre-contains\");\n+  bool found = false;\n+  size_t i = 0;\n+  size_t found_i = 0;\n+  for (oop* loc = _current - 1; loc >= _base; loc--) {\n+    if (*loc == o) {\n+      return true;\n+    }\n+  }\n+  validate(\"post-contains\");\n+  return false;\n+}\n+\n+inline void LockStack::oops_do(OopClosure* cl) {\n+  validate(\"pre-oops-do\");\n+  for (oop* loc = _base; loc < _current; loc++) {\n+    cl->do_oop(loc);\n+  }\n+  validate(\"post-oops-do\");\n+}\n+\n+#endif \/\/ SHARE_RUNTIME_LOCKSTACK_INLINE_HPP\n","filename":"src\/hotspot\/share\/runtime\/lockStack.inline.hpp","additions":93,"deletions":0,"binary":false,"changes":93,"status":"added"},{"patch":"@@ -337,1 +337,1 @@\n-  if (current->is_lock_owned((address)cur)) {\n+  if (!UseFastLocking && current->is_lock_owned((address)cur)) {\n@@ -603,0 +603,16 @@\n+\/\/ We might access the dead object headers for parsable heap walk, make sure\n+\/\/ headers are in correct shape, e.g. monitors deflated.\n+void ObjectMonitor::maybe_deflate_dead(oop* p) {\n+  oop obj = *p;\n+  assert(obj != NULL, \"must not yet been cleared\");\n+  markWord mark = obj->mark();\n+  if (mark.has_monitor()) {\n+    ObjectMonitor* monitor = mark.monitor();\n+    if (p == monitor->_object.ptr_raw()) {\n+      assert(monitor->object_peek() == obj, \"lock object must match\");\n+      markWord dmw = monitor->header();\n+      obj->set_mark(dmw);\n+    }\n+  }\n+}\n+\n@@ -1138,1 +1154,1 @@\n-    if (current->is_lock_owned((address)cur)) {\n+    if (!UseFastLocking && current->is_lock_owned((address)cur)) {\n@@ -1358,1 +1374,1 @@\n-    if (current->is_lock_owned((address)cur)) {\n+    if (!UseFastLocking && current->is_lock_owned((address)cur)) {\n@@ -1407,0 +1423,1 @@\n+  assert(cur != ANONYMOUS_OWNER, \"no anon owner here\");\n@@ -1410,1 +1427,1 @@\n-  if (current->is_lock_owned((address)cur)) {\n+  if (!UseFastLocking && current->is_lock_owned((address)cur)) {\n","filename":"src\/hotspot\/share\/runtime\/objectMonitor.cpp","additions":21,"deletions":4,"binary":false,"changes":25,"status":"modified"},{"patch":"@@ -147,2 +147,10 @@\n-  \/\/ Used by async deflation as a marker in the _owner field:\n-  #define DEFLATER_MARKER reinterpret_cast<void*>(-1)\n+  \/\/ Used by async deflation as a marker in the _owner field.\n+  \/\/ Note that the choice of the two markers is peculiar:\n+  \/\/ - They need to represent values that cannot be pointers. In particular,\n+  \/\/   we achieve this by using the lowest two bits\n+  \/\/ - ANONYMOUS_OWNER should be a small value, it is used in generated code\n+  \/\/   and small values encode much better\n+  \/\/ - We test for anonymous owner by testing for the lowest bit, therefore\n+  \/\/   DEFLATER_MARKER must *not* have that bit set.\n+  #define DEFLATER_MARKER reinterpret_cast<void*>(2)\n+  #define ANONYMOUS_OWNER reinterpret_cast<void*>(1)\n@@ -207,0 +215,1 @@\n+  static int header_offset_in_bytes()      { return offset_of(ObjectMonitor, _header); }\n@@ -266,0 +275,12 @@\n+  void set_owner_anonymous() {\n+    set_owner_from(NULL, ANONYMOUS_OWNER);\n+  }\n+\n+  bool is_owner_anonymous() const {\n+    return owner_raw() == ANONYMOUS_OWNER;\n+  }\n+\n+  void set_owner_from_anonymous(Thread* owner) {\n+    set_owner_from(ANONYMOUS_OWNER, owner);\n+  }\n+\n@@ -326,0 +347,2 @@\n+  static void maybe_deflate_dead(oop* p);\n+\n","filename":"src\/hotspot\/share\/runtime\/objectMonitor.hpp","additions":25,"deletions":2,"binary":false,"changes":27,"status":"modified"},{"patch":"@@ -33,0 +33,1 @@\n+#include \"runtime\/lockStack.inline.hpp\"\n@@ -36,3 +37,11 @@\n-  void* owner = owner_raw();\n-  if (current == owner || current->is_lock_owned((address)owner)) {\n-    return 1;\n+  if (UseFastLocking) {\n+    if (is_owner_anonymous()) {\n+      return current->lock_stack().contains(object()) ? 1 : 0;\n+    } else {\n+      return current == owner_raw() ? 1 : 0;\n+    }\n+  } else {\n+    void* owner = owner_raw();\n+    if (current == owner || current->is_lock_owned((address)owner)) {\n+      return 1;\n+    }\n","filename":"src\/hotspot\/share\/runtime\/objectMonitor.inline.hpp","additions":12,"deletions":3,"binary":false,"changes":15,"status":"modified"},{"patch":"@@ -42,0 +42,1 @@\n+#include \"oops\/compressedKlass.hpp\"\n","filename":"src\/hotspot\/share\/runtime\/os.cpp","additions":1,"deletions":0,"binary":false,"changes":1,"status":"modified"},{"patch":"@@ -28,0 +28,1 @@\n+#include \"gc\/shared\/suspendibleThreadSet.hpp\"\n@@ -42,0 +43,1 @@\n+#include \"runtime\/lockStack.inline.hpp\"\n@@ -314,1 +316,2 @@\n-  if (mark.has_locker() && current->is_lock_owned((address)mark.locker())) {\n+  if ((mark.is_fast_locked() && current->lock_stack().contains(oop(obj))) ||\n+      (mark.has_locker() && current->is_lock_owned((address)mark.locker()))) {\n@@ -397,1 +400,3 @@\n-    lock->set_displaced_header(markWord::unused_mark());\n+    if (!UseFastLocking) {\n+      lock->set_displaced_header(markWord::unused_mark());\n+    }\n@@ -486,6 +491,37 @@\n-    markWord mark = obj->mark();\n-    if (mark.is_neutral()) {\n-      \/\/ Anticipate successful CAS -- the ST of the displaced mark must\n-      \/\/ be visible <= the ST performed by the CAS.\n-      lock->set_displaced_header(mark);\n-      if (mark == obj()->cas_set_mark(markWord::from_pointer(lock), mark)) {\n+    if (UseFastLocking) {\n+      LockStack& lock_stack = current->lock_stack();\n+\n+      markWord header = obj()->mark_acquire();\n+      while (true) {\n+        if (header.is_neutral()) {\n+          assert(!lock_stack.contains(obj()), \"thread must not already hold the lock\");\n+          \/\/ Try to swing into 'fast-locked' state without inflating.\n+          markWord locked_header = header.set_fast_locked();\n+          markWord witness = obj()->cas_set_mark(locked_header, header);\n+          if (witness == header) {\n+            \/\/ Successfully fast-locked, push object to lock-stack and return.\n+            lock_stack.push(obj());\n+            return;\n+          }\n+          \/\/ Otherwise retry.\n+          header = witness;\n+        } else {\n+          \/\/ Fall-through to inflate-enter.\n+          break;\n+        }\n+      }\n+    } else {\n+      markWord mark = obj->mark();\n+      if (mark.is_neutral()) {\n+        \/\/ Anticipate successful CAS -- the ST of the displaced mark must\n+        \/\/ be visible <= the ST performed by the CAS.\n+        lock->set_displaced_header(mark);\n+        if (mark == obj()->cas_set_mark(markWord::from_pointer(lock), mark)) {\n+          return;\n+        }\n+        \/\/ Fall through to inflate() ...\n+      } else if (mark.has_locker() &&\n+                 current->is_lock_owned((address)mark.locker())) {\n+        assert(lock != mark.locker(), \"must not re-lock the same lock\");\n+        assert(lock != (BasicLock*)obj->mark().value(), \"don't relock with same BasicLock\");\n+        lock->set_displaced_header(markWord::from_pointer(nullptr));\n@@ -494,8 +530,0 @@\n-      \/\/ Fall through to inflate() ...\n-    } else if (mark.has_locker() &&\n-               current->is_lock_owned((address)mark.locker())) {\n-      assert(lock != mark.locker(), \"must not re-lock the same lock\");\n-      assert(lock != (BasicLock*)obj->mark().value(), \"don't relock with same BasicLock\");\n-      lock->set_displaced_header(markWord::from_pointer(nullptr));\n-      return;\n-    }\n@@ -503,5 +531,6 @@\n-    \/\/ The object header will never be displaced to this lock,\n-    \/\/ so it does not matter what the value is, except that it\n-    \/\/ must be non-zero to avoid looking like a re-entrant lock,\n-    \/\/ and must not look locked either.\n-    lock->set_displaced_header(markWord::unused_mark());\n+      \/\/ The object header will never be displaced to this lock,\n+      \/\/ so it does not matter what the value is, except that it\n+      \/\/ must be non-zero to avoid looking like a re-entrant lock,\n+      \/\/ and must not look locked either.\n+      lock->set_displaced_header(markWord::unused_mark());\n+    }\n@@ -509,1 +538,1 @@\n-    guarantee(!obj->mark().has_locker(), \"must not be stack-locked\");\n+    guarantee(!obj->mark().has_locker() && !obj->mark().is_fast_locked(), \"must not be stack-locked\");\n@@ -528,25 +557,12 @@\n-\n-    markWord dhw = lock->displaced_header();\n-    if (dhw.value() == 0) {\n-      \/\/ If the displaced header is null, then this exit matches up with\n-      \/\/ a recursive enter. No real work to do here except for diagnostics.\n-#ifndef PRODUCT\n-      if (mark != markWord::INFLATING()) {\n-        \/\/ Only do diagnostics if we are not racing an inflation. Simply\n-        \/\/ exiting a recursive enter of a Java Monitor that is being\n-        \/\/ inflated is safe; see the has_monitor() comment below.\n-        assert(!mark.is_neutral(), \"invariant\");\n-        assert(!mark.has_locker() ||\n-        current->is_lock_owned((address)mark.locker()), \"invariant\");\n-        if (mark.has_monitor()) {\n-          \/\/ The BasicLock's displaced_header is marked as a recursive\n-          \/\/ enter and we have an inflated Java Monitor (ObjectMonitor).\n-          \/\/ This is a special case where the Java Monitor was inflated\n-          \/\/ after this thread entered the stack-lock recursively. When a\n-          \/\/ Java Monitor is inflated, we cannot safely walk the Java\n-          \/\/ Monitor owner's stack and update the BasicLocks because a\n-          \/\/ Java Monitor can be asynchronously inflated by a thread that\n-          \/\/ does not own the Java Monitor.\n-          ObjectMonitor* m = mark.monitor();\n-          assert(m->object()->mark() == mark, \"invariant\");\n-          assert(m->is_entered(current), \"invariant\");\n+    if (UseFastLocking) {\n+      if (mark.is_fast_locked()) {\n+        markWord unlocked_header = mark.set_unlocked();\n+        markWord witness = object->cas_set_mark(unlocked_header, mark);\n+        if (witness != mark) {\n+          \/\/ Another thread beat us, it can only have installed an anonymously locked monitor at this point.\n+          \/\/ Fetch that monitor, set owner correctly to this thread, and exit it (allowing waiting threads to enter).\n+          assert(witness.has_monitor(), \"must have monitor\");\n+          ObjectMonitor* monitor = witness.monitor();\n+          assert(monitor->is_owner_anonymous(), \"must be anonymous owner\");\n+          monitor->set_owner_from_anonymous(current);\n+          monitor->exit(current);\n@@ -554,0 +570,3 @@\n+        LockStack& lock_stack = current->lock_stack();\n+        lock_stack.remove(object);\n+        return;\n@@ -555,0 +574,27 @@\n+    } else {\n+      markWord dhw = lock->displaced_header();\n+      if (dhw.value() == 0) {\n+        \/\/ If the displaced header is null, then this exit matches up with\n+        \/\/ a recursive enter. No real work to do here except for diagnostics.\n+#ifndef PRODUCT\n+        if (mark != markWord::INFLATING()) {\n+          \/\/ Only do diagnostics if we are not racing an inflation. Simply\n+          \/\/ exiting a recursive enter of a Java Monitor that is being\n+          \/\/ inflated is safe; see the has_monitor() comment below.\n+          assert(!mark.is_neutral(), \"invariant\");\n+          assert(!mark.has_locker() ||\n+                 current->is_lock_owned((address)mark.locker()), \"invariant\");\n+          if (mark.has_monitor()) {\n+            \/\/ The BasicLock's displaced_header is marked as a recursive\n+            \/\/ enter and we have an inflated Java Monitor (ObjectMonitor).\n+            \/\/ This is a special case where the Java Monitor was inflated\n+            \/\/ after this thread entered the stack-lock recursively. When a\n+            \/\/ Java Monitor is inflated, we cannot safely walk the Java\n+            \/\/ Monitor owner's stack and update the BasicLocks because a\n+            \/\/ Java Monitor can be asynchronously inflated by a thread that\n+            \/\/ does not own the Java Monitor.\n+            ObjectMonitor* m = mark.monitor();\n+            assert(m->object()->mark() == mark, \"invariant\");\n+            assert(m->is_entered(current), \"invariant\");\n+          }\n+        }\n@@ -556,8 +602,0 @@\n-      return;\n-    }\n-\n-    if (mark == markWord::from_pointer(lock)) {\n-      \/\/ If the object is stack-locked by the current thread, try to\n-      \/\/ swing the displaced header from the BasicLock back to the mark.\n-      assert(dhw.is_neutral(), \"invariant\");\n-      if (object->cas_set_mark(dhw, mark) == mark) {\n@@ -566,0 +604,9 @@\n+\n+      if (mark == markWord::from_pointer(lock)) {\n+        \/\/ If the object is stack-locked by the current thread, try to\n+        \/\/ swing the displaced header from the BasicLock back to the mark.\n+        assert(dhw.is_neutral(), \"invariant\");\n+        if (object->cas_set_mark(dhw, mark) == mark) {\n+          return;\n+        }\n+      }\n@@ -575,0 +622,7 @@\n+  if (UseFastLocking && monitor->is_owner_anonymous()) {\n+    \/\/ It must be us. Pop lock object from lock stack.\n+    LockStack& lock_stack = current->lock_stack();\n+    oop popped = lock_stack.pop();\n+    assert(popped == object, \"must be owned by this thread\");\n+    monitor->set_owner_from_anonymous(current);\n+  }\n@@ -701,1 +755,2 @@\n-  if (mark.has_locker() && current->is_lock_owned((address)mark.locker())) {\n+  if ((mark.is_fast_locked() && current->lock_stack().contains(obj())) ||\n+      (mark.has_locker() && current->is_lock_owned((address)mark.locker()))) {\n@@ -716,1 +771,2 @@\n-  if (mark.has_locker() && current->is_lock_owned((address)mark.locker())) {\n+  if ((mark.is_fast_locked() && current->lock_stack().contains(obj())) ||\n+      (mark.has_locker() && current->is_lock_owned((address)mark.locker()))) {\n@@ -744,1 +800,1 @@\n-  if (!mark.is_being_inflated()) {\n+  if (!mark.is_being_inflated() || UseFastLocking) {\n@@ -859,0 +915,5 @@\n+static bool is_lock_owned(Thread* thread, oop obj) {\n+  assert(UseFastLocking, \"only call this with fast-locking enabled\");\n+  return thread->is_Java_thread() ? reinterpret_cast<JavaThread*>(thread)->lock_stack().contains(obj) : false;\n+}\n+\n@@ -913,1 +974,8 @@\n-    } else if (current->is_lock_owned((address)mark.locker())) {\n+    } else if (mark.is_fast_locked() && is_lock_owned(current, obj)) {\n+      \/\/ This is a fast lock owned by the calling thread so use the\n+      \/\/ markWord from the object.\n+      hash = mark.hash();\n+      if (hash != 0) {                  \/\/ if it has a hash, just return it\n+        return hash;\n+      }\n+    } else if (mark.has_locker() && current->is_lock_owned((address)mark.locker())) {\n@@ -982,0 +1050,6 @@\n+\n+  \/\/ Fast-locking case.\n+  if (mark.is_fast_locked()) {\n+    return current->lock_stack().contains(h_obj());\n+  }\n+\n@@ -996,2 +1070,0 @@\n-  address owner = nullptr;\n-\n@@ -1002,1 +1074,5 @@\n-    owner = (address) mark.locker();\n+    return Threads::owning_thread_from_monitor_owner(t_list, (address) mark.locker());\n+  }\n+\n+  if (mark.is_fast_locked()) {\n+    return Threads::owning_thread_from_object(t_list, h_obj());\n@@ -1006,1 +1082,1 @@\n-  else if (mark.has_monitor()) {\n+  if (mark.has_monitor()) {\n@@ -1011,1 +1087,1 @@\n-    owner = (address) monitor->owner();\n+    return Threads::owning_thread_from_monitor(t_list, monitor);\n@@ -1014,10 +1090,0 @@\n-  if (owner != nullptr) {\n-    \/\/ owning_thread_from_monitor_owner() may also return null here\n-    return Threads::owning_thread_from_monitor_owner(t_list, owner);\n-  }\n-\n-  \/\/ Unlocked case, header in place\n-  \/\/ Cannot have assertion since this object may have been\n-  \/\/ locked by another thread when reaching here.\n-  \/\/ assert(mark.is_neutral(), \"sanity check\");\n-\n@@ -1221,0 +1287,5 @@\n+      if (UseFastLocking && inf->is_owner_anonymous() && is_lock_owned(current, object)) {\n+        inf->set_owner_from_anonymous(current);\n+        assert(current->is_Java_thread(), \"must be Java thread\");\n+        reinterpret_cast<JavaThread*>(current)->lock_stack().remove(object);\n+      }\n@@ -1230,1 +1301,4 @@\n-    if (mark == markWord::INFLATING()) {\n+    \/\/ NOTE: We need to check UseFastLocking here, because with fast-locking, the header\n+    \/\/ may legitimately be zero: cleared lock-bits and all upper header bits zero.\n+    \/\/ With fast-locking, the INFLATING protocol is not used.\n+    if (mark == markWord::INFLATING() && !UseFastLocking) {\n@@ -1246,0 +1320,42 @@\n+    if (mark.is_fast_locked()) {\n+      assert(UseFastLocking, \"can only happen with fast-locking\");\n+      ObjectMonitor* monitor = new ObjectMonitor(object);\n+      monitor->set_header(mark.set_unlocked());\n+      bool own = is_lock_owned(current, object);\n+      if (own) {\n+        \/\/ Owned by us.\n+        monitor->set_owner_from(nullptr, current);\n+      } else {\n+        \/\/ Owned by somebody else.\n+        monitor->set_owner_anonymous();\n+      }\n+      markWord monitor_mark = markWord::encode(monitor);\n+      markWord witness = object->cas_set_mark(monitor_mark, mark);\n+      if (witness == mark) {\n+        \/\/ Success! Return inflated monitor.\n+        if (own) {\n+          assert(current->is_Java_thread(), \"must be: checked in is_lock_owned()\");\n+          reinterpret_cast<JavaThread*>(current)->lock_stack().remove(object);\n+        }\n+        \/\/ Once the ObjectMonitor is configured and object is associated\n+        \/\/ with the ObjectMonitor, it is safe to allow async deflation:\n+        _in_use_list.add(monitor);\n+\n+        \/\/ Hopefully the performance counters are allocated on distinct\n+        \/\/ cache lines to avoid false sharing on MP systems ...\n+        OM_PERFDATA_OP(Inflations, inc());\n+        if (log_is_enabled(Trace, monitorinflation)) {\n+          ResourceMark rm(current);\n+          lsh.print_cr(\"inflate(has_locker): object=\" INTPTR_FORMAT \", mark=\"\n+                       INTPTR_FORMAT \", type='%s'\", p2i(object),\n+                       object->mark().value(), object->klass()->external_name());\n+        }\n+        if (event.should_commit()) {\n+          post_monitor_inflate_event(&event, object, cause);\n+        }\n+        return monitor;\n+      } else {\n+        delete monitor;\n+        continue;\n+      }\n+    }\n@@ -1248,0 +1364,1 @@\n+      assert(!UseFastLocking, \"can not happen with fast-locking\");\n@@ -1461,0 +1578,10 @@\n+class VM_RendezvousGCThreads : public VM_Operation {\n+public:\n+  bool evaluate_at_safepoint() const override { return false; }\n+  VMOp_Type type() const override { return VMOp_RendezvousGCThreads; }\n+  void doit() override {\n+    SuspendibleThreadSet::synchronize();\n+    SuspendibleThreadSet::desynchronize();\n+  };\n+};\n+\n@@ -1514,0 +1641,3 @@\n+      \/\/ Also, we sync and desync GC threads around the handshake, so that they can\n+      \/\/ safely read the mark-word and look-through to the object-monitor, without\n+      \/\/ being afraid that the object-monitor is going away.\n@@ -1516,0 +1646,2 @@\n+      VM_RendezvousGCThreads sync_gc;\n+      VMThread::execute(&sync_gc);\n","filename":"src\/hotspot\/share\/runtime\/synchronizer.cpp","additions":207,"deletions":75,"binary":false,"changes":282,"status":"modified"},{"patch":"@@ -536,0 +536,1 @@\n+  assert(!UseFastLocking, \"should not be called with fast-locking\");\n","filename":"src\/hotspot\/share\/runtime\/thread.cpp","additions":1,"deletions":0,"binary":false,"changes":1,"status":"modified"},{"patch":"@@ -71,0 +71,1 @@\n+#include \"runtime\/lockStack.inline.hpp\"\n@@ -1391,0 +1392,1 @@\n+  assert(!UseFastLocking, \"only with stack-locking\");\n@@ -1420,0 +1422,10 @@\n+JavaThread* Threads::owning_thread_from_object(ThreadsList * t_list, oop obj) {\n+  assert(UseFastLocking, \"Only with fast-locking\");\n+  for (JavaThread* q : *t_list) {\n+    if (q->lock_stack().contains(obj)) {\n+      return q;\n+    }\n+  }\n+  return nullptr;\n+}\n+\n@@ -1421,2 +1433,12 @@\n-  address owner = (address)monitor->owner();\n-  return owning_thread_from_monitor_owner(t_list, owner);\n+  if (UseFastLocking) {\n+    if (monitor->is_owner_anonymous()) {\n+      return owning_thread_from_object(t_list, monitor->object());\n+    } else {\n+      Thread* owner = reinterpret_cast<Thread*>(monitor->owner());\n+      assert(owner == nullptr || owner->is_Java_thread(), \"only JavaThreads own monitors\");\n+      return reinterpret_cast<JavaThread*>(owner);\n+    }\n+  } else {\n+    address owner = (address)monitor->owner();\n+    return owning_thread_from_monitor_owner(t_list, owner);\n+  }\n","filename":"src\/hotspot\/share\/runtime\/threads.cpp","additions":24,"deletions":2,"binary":false,"changes":26,"status":"modified"},{"patch":"@@ -141,0 +141,1 @@\n+  static JavaThread* owning_thread_from_object(ThreadsList* t_list, oop obj);\n","filename":"src\/hotspot\/share\/runtime\/threads.hpp","additions":1,"deletions":0,"binary":false,"changes":1,"status":"modified"},{"patch":"@@ -83,0 +83,1 @@\n+  template(HeapObjectStatistics)                  \\\n@@ -92,0 +93,1 @@\n+  template(RendezvousGCThreads)                   \\\n","filename":"src\/hotspot\/share\/runtime\/vmOperation.hpp","additions":2,"deletions":0,"binary":false,"changes":2,"status":"modified"},{"patch":"@@ -201,2 +201,1 @@\n-  volatile_nonstatic_field(oopDesc,            _metadata._klass,                              Klass*)                                \\\n-  volatile_nonstatic_field(oopDesc,            _metadata._compressed_klass,                   narrowKlass)                           \\\n+  NOT_LP64(volatile_nonstatic_field(oopDesc,   _klass,                                        Klass*))                               \\\n@@ -380,2 +379,2 @@\n-     static_field(CompressedKlassPointers,     _narrow_klass._base,                           address)                               \\\n-     static_field(CompressedKlassPointers,     _narrow_klass._shift,                          int)                                   \\\n+     static_field(CompressedKlassPointers,     _base,                           address)                                             \\\n+     static_field(CompressedKlassPointers,     _shift_copy,                          int)                                            \\\n@@ -702,0 +701,3 @@\n+  nonstatic_field(JavaThread,                  _lock_stack,                                   LockStack)                             \\\n+  nonstatic_field(LockStack,                   _current,                                      oop*)                                  \\\n+  nonstatic_field(LockStack,                   _base,                                         oop*)                                  \\\n@@ -1315,0 +1317,1 @@\n+  declare_toplevel_type(LockStack)                                        \\\n@@ -2605,0 +2608,1 @@\n+  LP64_ONLY(declare_constant(markWord::klass_shift))                      \\\n","filename":"src\/hotspot\/share\/runtime\/vmStructs.cpp","additions":8,"deletions":4,"binary":false,"changes":12,"status":"modified"},{"patch":"@@ -0,0 +1,177 @@\n+\/*\n+ * Copyright (c) 2021, Red Hat, Inc. All rights reserved.\n+ * DO NOT ALTER OR REMOVE COPYRIGHT NOTICES OR THIS FILE HEADER.\n+ *\n+ * This code is free software; you can redistribute it and\/or modify it\n+ * under the terms of the GNU General Public License version 2 only, as\n+ * published by the Free Software Foundation.\n+ *\n+ * This code is distributed in the hope that it will be useful, but WITHOUT\n+ * ANY WARRANTY; without even the implied warranty of MERCHANTABILITY or\n+ * FITNESS FOR A PARTICULAR PURPOSE.  See the GNU General Public License\n+ * version 2 for more details (a copy is included in the LICENSE file that\n+ * accompanied this code).\n+ *\n+ * You should have received a copy of the GNU General Public License version\n+ * 2 along with this work; if not, write to the Free Software Foundation,\n+ * Inc., 51 Franklin St, Fifth Floor, Boston, MA 02110-1301 USA.\n+ *\n+ * Please contact Oracle, 500 Oracle Parkway, Redwood Shores, CA 94065 USA\n+ * or visit www.oracle.com if you need additional information or have any\n+ * questions.\n+ *\n+ *\/\n+\n+#include \"precompiled.hpp\"\n+#include \"gc\/shared\/collectedHeap.hpp\"\n+#include \"logging\/logStream.hpp\"\n+#include \"logging\/logTag.hpp\"\n+#include \"memory\/allocation.hpp\"\n+#include \"memory\/iterator.hpp\"\n+#include \"memory\/resourceArea.hpp\"\n+#include \"memory\/universe.hpp\"\n+#include \"oops\/oop.inline.hpp\"\n+#include \"runtime\/mutexLocker.hpp\"\n+#include \"runtime\/vmThread.hpp\"\n+#include \"services\/heapObjectStatistics.hpp\"\n+#include \"utilities\/copy.hpp\"\n+#include \"utilities\/globalDefinitions.hpp\"\n+#include \"utilities\/ostream.hpp\"\n+\n+HeapObjectStatistics* HeapObjectStatistics::_instance = NULL;\n+\n+class HeapObjectStatsObjectClosure : public ObjectClosure {\n+private:\n+  HeapObjectStatistics* const _stats;\n+public:\n+  HeapObjectStatsObjectClosure() : _stats(HeapObjectStatistics::instance()) {}\n+  void do_object(oop obj) {\n+    _stats->visit_object(obj);\n+  }\n+};\n+\n+class VM_HeapObjectStatistics : public VM_Operation {\n+public:\n+  VMOp_Type type() const { return VMOp_HeapObjectStatistics; }\n+  bool doit_prologue() {\n+    Heap_lock->lock();\n+    return true;\n+  }\n+\n+  void doit_epilogue() {\n+    Heap_lock->unlock();\n+  }\n+\n+  void doit() {\n+    assert(SafepointSynchronize::is_at_safepoint(), \"all threads are stopped\");\n+    assert(Heap_lock->is_locked(), \"should have the Heap_lock\");\n+\n+    CollectedHeap* heap = Universe::heap();\n+    heap->ensure_parsability(false);\n+\n+    HeapObjectStatistics* stats = HeapObjectStatistics::instance();\n+    stats->begin_sample();\n+\n+    HeapObjectStatsObjectClosure cl;\n+    heap->object_iterate(&cl);\n+  }\n+};\n+\n+HeapObjectStatisticsTask::HeapObjectStatisticsTask() : PeriodicTask(HeapObjectStatsSamplingInterval) {}\n+\n+void HeapObjectStatisticsTask::task() {\n+  VM_HeapObjectStatistics vmop;\n+  VMThread::execute(&vmop);\n+}\n+\n+void HeapObjectStatistics::initialize() {\n+  assert(_instance == NULL, \"Don't init twice\");\n+  if (HeapObjectStats) {\n+    _instance = new HeapObjectStatistics();\n+    _instance->start();\n+  }\n+}\n+\n+void HeapObjectStatistics::shutdown() {\n+  if (HeapObjectStats) {\n+    assert(_instance != NULL, \"Must be initialized\");\n+    LogTarget(Info, heap, stats) lt;\n+    if (lt.is_enabled()) {\n+      LogStream ls(lt);\n+      ResourceMark rm;\n+      _instance->print(&ls);\n+    }\n+    _instance->stop();\n+    delete _instance;\n+    _instance = NULL;\n+  }\n+}\n+\n+HeapObjectStatistics* HeapObjectStatistics::instance() {\n+  assert(_instance != NULL, \"Must be initialized\");\n+  return _instance;\n+}\n+\n+void HeapObjectStatistics::increase_counter(uint64_t& counter, uint64_t val) {\n+  uint64_t oldval = counter;\n+  uint64_t newval = counter + val;\n+  if (newval < oldval) {\n+    log_warning(heap, stats)(\"HeapObjectStats counter overflow: resulting statistics will be useless\");\n+  }\n+  counter = newval;\n+}\n+\n+HeapObjectStatistics::HeapObjectStatistics() :\n+  _task(), _num_samples(0), _num_objects(0), _num_ihashed(0), _num_locked(0), _lds(0) { }\n+\n+void HeapObjectStatistics::start() {\n+  _task.enroll();\n+}\n+\n+void HeapObjectStatistics::stop() {\n+  _task.disenroll();\n+}\n+\n+void HeapObjectStatistics::begin_sample() {\n+  _num_samples++;\n+}\n+\n+void HeapObjectStatistics::visit_object(oop obj) {\n+  increase_counter(_num_objects);\n+  markWord mark = obj->mark();\n+  if (!mark.has_no_hash()) {\n+    increase_counter(_num_ihashed);\n+    if (mark.age() > 0) {\n+      increase_counter(_num_ihashed_moved);\n+    }\n+  }\n+  if (mark.is_locked()) {\n+    increase_counter(_num_locked);\n+  }\n+#ifdef ASSERT\n+#ifdef _LP64\n+  if (!mark.has_displaced_mark_helper()) {\n+    assert(mark.narrow_klass() == CompressedKlassPointers::encode(obj->klass_or_null()), \"upper 32 mark bits must be narrow klass: mark: \" INTPTR_FORMAT \", compressed-klass: \" INTPTR_FORMAT, (intptr_t)mark.narrow_klass(), (intptr_t)CompressedKlassPointers::encode(obj->klass_or_null()));\n+  }\n+#endif\n+#endif\n+  increase_counter(_lds, obj->size());\n+}\n+\n+void HeapObjectStatistics::print(outputStream* out) const {\n+  if (!HeapObjectStats) {\n+    return;\n+  }\n+  if (_num_samples == 0 || _num_objects == 0) {\n+    return;\n+  }\n+\n+  out->print_cr(\"Number of samples:  \" UINT64_FORMAT, _num_samples);\n+  out->print_cr(\"Average number of objects: \" UINT64_FORMAT, _num_objects \/ _num_samples);\n+  out->print_cr(\"Average object size: \" UINT64_FORMAT \" bytes, %.1f words\", (_lds * HeapWordSize) \/ _num_objects, (float) _lds \/ _num_objects);\n+  out->print_cr(\"Average number of hashed objects: \" UINT64_FORMAT \" (%.2f%%)\", _num_ihashed \/ _num_samples, (float) (_num_ihashed * 100.0) \/ _num_objects);\n+  out->print_cr(\"Average number of moved hashed objects: \" UINT64_FORMAT \" (%.2f%%)\", _num_ihashed_moved \/ _num_samples, (float) (_num_ihashed_moved * 100.0) \/ _num_objects);\n+  out->print_cr(\"Average number of locked objects: \" UINT64_FORMAT \" (%.2f%%)\", _num_locked \/ _num_samples, (float) (_num_locked * 100) \/ _num_objects);\n+  out->print_cr(\"Average LDS: \" UINT64_FORMAT \" bytes\", _lds * HeapWordSize \/ _num_samples);\n+  out->print_cr(\"Avg LDS with (assumed) 64bit header: \" UINT64_FORMAT \" bytes (%.1f%%)\", (_lds - _num_objects) * HeapWordSize \/ _num_samples, ((float) _lds - _num_objects) * 100.0 \/ _lds);\n+}\n","filename":"src\/hotspot\/share\/services\/heapObjectStatistics.cpp","additions":177,"deletions":0,"binary":false,"changes":177,"status":"added"},{"patch":"@@ -0,0 +1,71 @@\n+\/*\n+ * Copyright (c) 2021, Red Hat, Inc. All rights reserved.\n+ * DO NOT ALTER OR REMOVE COPYRIGHT NOTICES OR THIS FILE HEADER.\n+ *\n+ * This code is free software; you can redistribute it and\/or modify it\n+ * under the terms of the GNU General Public License version 2 only, as\n+ * published by the Free Software Foundation.\n+ *\n+ * This code is distributed in the hope that it will be useful, but WITHOUT\n+ * ANY WARRANTY; without even the implied warranty of MERCHANTABILITY or\n+ * FITNESS FOR A PARTICULAR PURPOSE.  See the GNU General Public License\n+ * version 2 for more details (a copy is included in the LICENSE file that\n+ * accompanied this code).\n+ *\n+ * You should have received a copy of the GNU General Public License version\n+ * 2 along with this work; if not, write to the Free Software Foundation,\n+ * Inc., 51 Franklin St, Fifth Floor, Boston, MA 02110-1301 USA.\n+ *\n+ * Please contact Oracle, 500 Oracle Parkway, Redwood Shores, CA 94065 USA\n+ * or visit www.oracle.com if you need additional information or have any\n+ * questions.\n+ *\n+ *\/\n+\n+#ifndef SHARE_SERVICES_HEAPOBJECTSTATISTICS_HPP\n+#define SHARE_SERVICES_HEAPOBJECTSTATISTICS_HPP\n+\n+#include \"memory\/allocation.hpp\"\n+#include \"oops\/oopsHierarchy.hpp\"\n+#include \"runtime\/task.hpp\"\n+#include \"runtime\/vmOperation.hpp\"\n+\n+class outputStream;\n+\n+class HeapObjectStatisticsTask : public PeriodicTask {\n+public:\n+  HeapObjectStatisticsTask();\n+  void task();\n+};\n+\n+class HeapObjectStatistics : public CHeapObj<mtGC> {\n+private:\n+  static HeapObjectStatistics* _instance;\n+\n+  HeapObjectStatisticsTask _task;\n+  uint64_t _num_samples;\n+  uint64_t _num_objects;\n+  uint64_t _num_ihashed;\n+  uint64_t _num_ihashed_moved;\n+  uint64_t _num_locked;\n+  uint64_t _lds;\n+\n+  static void increase_counter(uint64_t& counter, uint64_t val = 1);\n+\n+  void print(outputStream* out) const;\n+\n+public:\n+  static void initialize();\n+  static void shutdown();\n+\n+  static HeapObjectStatistics* instance();\n+\n+  HeapObjectStatistics();\n+  void start();\n+  void stop();\n+\n+  void begin_sample();\n+  void visit_object(oop object);\n+};\n+\n+#endif \/\/ SHARE_SERVICES_HEAPOBJECTSTATISTICS_HPP\n","filename":"src\/hotspot\/share\/services\/heapObjectStatistics.hpp","additions":71,"deletions":0,"binary":false,"changes":71,"status":"added"},{"patch":"@@ -598,5 +598,0 @@\n-const int LogKlassAlignmentInBytes = 3;\n-const int LogKlassAlignment        = LogKlassAlignmentInBytes - LogHeapWordSize;\n-const int KlassAlignmentInBytes    = 1 << LogKlassAlignmentInBytes;\n-const int KlassAlignment           = KlassAlignmentInBytes \/ HeapWordSize;\n-\n@@ -610,5 +605,0 @@\n-\/\/ Maximal size of compressed class space. Above this limit compression is not possible.\n-\/\/ Also upper bound for placement of zero based class space. (Class space is further limited\n-\/\/ to be < 3G, see arguments.cpp.)\n-const  uint64_t KlassEncodingMetaspaceMax = (uint64_t(max_juint) + 1) << LogKlassAlignmentInBytes;\n-\n","filename":"src\/hotspot\/share\/utilities\/globalDefinitions.hpp","additions":0,"deletions":10,"binary":false,"changes":10,"status":"modified"},{"patch":"@@ -39,0 +39,1 @@\n+#include \"oops\/compressedKlass.hpp\"\n","filename":"src\/hotspot\/share\/utilities\/vmError.cpp","additions":1,"deletions":0,"binary":false,"changes":1,"status":"modified"},{"patch":"@@ -27,0 +27,2 @@\n+import sun.jvm.hotspot.oops.Mark;\n+\n@@ -397,1 +399,10 @@\n-    long value = readCInteger(address, getKlassPtrSize(), true);\n+    \/\/ On 64 bit systems, the compressed Klass* is currently read from the mark\n+    \/\/ word. We need to load the whole mark, and shift the upper parts.\n+    long value = readCInteger(address, machDesc.getAddressSize(), true);\n+    value = value >>> Mark.getKlassShift();\n+\n+    \/\/ Todo: Lilliput: this is a hack. The real problem is the assumption that size\n+    \/\/  of a narrow Klass pointer can be expressed in number of bytes (getKlassPtrSize).\n+    \/\/  That assumption is present in a number of files here. Better would be\n+    \/\/  to change this to getKlassPtrSizeInBits, or to do it some other way.\n+    value &= (1 << 22) - 1; \/\/ narrow klass pointer size is 22 bits.\n","filename":"src\/jdk.hotspot.agent\/share\/classes\/sun\/jvm\/hotspot\/debugger\/DebuggerBase.java","additions":12,"deletions":1,"binary":false,"changes":13,"status":"modified"},{"patch":"@@ -64,6 +64,1 @@\n-    if (VM.getVM().isCompressedKlassPointersEnabled()) {\n-      headerSize = typeSize;\n-    } else {\n-      headerSize = VM.getVM().alignUp(typeSize + VM.getVM().getIntSize(),\n-                                      VM.getVM().getHeapWordSize());\n-    }\n+    headerSize = lengthOffsetInBytes() + VM.getVM().getIntSize();\n@@ -73,9 +68,1 @@\n-  private static long headerSize(BasicType type) {\n-    if (Universe.elementTypeShouldBeAligned(type)) {\n-       return alignObjectSize(headerSizeInBytes())\/VM.getVM().getHeapWordSize();\n-    } else {\n-      return headerSizeInBytes()\/VM.getVM().getHeapWordSize();\n-    }\n-  }\n-\n-  private long lengthOffsetInBytes() {\n+  private static long lengthOffsetInBytes() {\n@@ -85,5 +72,1 @@\n-    if (VM.getVM().isCompressedKlassPointersEnabled()) {\n-      lengthOffsetInBytes = typeSize - VM.getVM().getIntSize();\n-    } else {\n-      lengthOffsetInBytes = typeSize;\n-    }\n+    lengthOffsetInBytes = typeSize;\n@@ -111,1 +94,7 @@\n-    return headerSize(type) * VM.getVM().getHeapWordSize();\n+    long typeSizeInBytes = headerSizeInBytes();\n+    if (Universe.elementTypeShouldBeAligned(type)) {\n+      VM vm = VM.getVM();\n+      return vm.alignUp(typeSizeInBytes, vm.getVM().getHeapWordSize());\n+    } else {\n+      return typeSizeInBytes;\n+    }\n","filename":"src\/jdk.hotspot.agent\/share\/classes\/sun\/jvm\/hotspot\/oops\/Array.java","additions":10,"deletions":21,"binary":false,"changes":31,"status":"modified"},{"patch":"@@ -61,2 +61,2 @@\n-    baseField = type.getAddressField(\"_narrow_klass._base\");\n-    shiftField = type.getCIntegerField(\"_narrow_klass._shift\");\n+    baseField = type.getAddressField(\"_base\");\n+    shiftField = type.getCIntegerField(\"_shift_copy\");\n@@ -72,0 +72,1 @@\n+        System.out.println(\"base: \" + baseField.getValue().minus(null));\n@@ -77,1 +78,3 @@\n-    return (int)shiftField.getValue();\n+\n+      System.out.println(\"shift: \" + (int)shiftField.getValue());\n+      return (int)shiftField.getValue();\n","filename":"src\/jdk.hotspot.agent\/share\/classes\/sun\/jvm\/hotspot\/oops\/CompressedKlassPointers.java","additions":6,"deletions":3,"binary":false,"changes":9,"status":"modified"},{"patch":"@@ -57,0 +57,3 @@\n+    if (VM.getVM().isLP64()) {\n+      klassShift          = db.lookupLongConstant(\"markWord::klass_shift\").longValue();\n+    }\n@@ -85,0 +88,1 @@\n+  private static long klassShift;\n@@ -110,0 +114,4 @@\n+  public static long getKlassShift() {\n+    return klassShift;\n+  }\n+\n@@ -189,0 +197,5 @@\n+  public Klass getKlass() {\n+    assert(!hasMonitor());\n+    return (Klass)Metadata.instantiateWrapperFor(addr.getCompKlassAddressAt(0));\n+  }\n+\n","filename":"src\/jdk.hotspot.agent\/share\/classes\/sun\/jvm\/hotspot\/oops\/Mark.java","additions":13,"deletions":0,"binary":false,"changes":13,"status":"modified"},{"patch":"@@ -49,2 +49,3 @@\n-    klass      = new MetadataField(type.getAddressField(\"_metadata._klass\"), 0);\n-    compressedKlass  = new NarrowKlassField(type.getAddressField(\"_metadata._compressed_klass\"), 0);\n+    if (!VM.getVM().isLP64()) {\n+      klass      = new MetadataField(type.getAddressField(\"_klass\"), 0);\n+    }\n@@ -78,0 +79,9 @@\n+\n+  private static Klass getKlass(Mark mark) {\n+    if (mark.hasMonitor()) {\n+      ObjectMonitor mon = mark.monitor();\n+      mark = mon.header();\n+    }\n+    return mark.getKlass();\n+  }\n+\n@@ -79,2 +89,3 @@\n-    if (VM.getVM().isCompressedKlassPointersEnabled()) {\n-      return (Klass)compressedKlass.getValue(getHandle());\n+    if (VM.getVM().isLP64()) {\n+      assert(VM.getVM().isCompressedKlassPointersEnabled());\n+      return getKlass(getMark());\n@@ -150,3 +161,1 @@\n-      if (VM.getVM().isCompressedKlassPointersEnabled()) {\n-        visitor.doMetadata(compressedKlass, true);\n-      } else {\n+      if (!VM.getVM().isLP64()) {\n@@ -209,2 +218,3 @@\n-    if (VM.getVM().isCompressedKlassPointersEnabled()) {\n-      return (Klass)Metadata.instantiateWrapperFor(handle.getCompKlassAddressAt(compressedKlass.getOffset()));\n+    if (VM.getVM().isLP64()) {\n+      Mark mark = new Mark(handle);\n+      return getKlass(mark);\n","filename":"src\/jdk.hotspot.agent\/share\/classes\/sun\/jvm\/hotspot\/oops\/Oop.java","additions":19,"deletions":9,"binary":false,"changes":28,"status":"modified"},{"patch":"@@ -47,0 +47,2 @@\n+  private static long          lockStackCurrentOffset;\n+  private static long          lockStackBaseOffset;\n@@ -56,0 +58,1 @@\n+  private static long oopPtrSize;\n@@ -88,0 +91,1 @@\n+    Type typeLockStack = db.lookupType(\"LockStack\");\n@@ -101,0 +105,4 @@\n+    lockStackCurrentOffset = type.getField(\"_lock_stack\").getOffset() + typeLockStack.getField(\"_current\").getOffset();\n+    lockStackBaseOffset = type.getField(\"_lock_stack\").getOffset() + typeLockStack.getField(\"_base\").getOffset();\n+    oopPtrSize = VM.getVM().getAddressSize();\n+\n@@ -397,0 +405,13 @@\n+  public boolean isLockOwned(OopHandle obj) {\n+    Address current = addr.getAddressAt(lockStackCurrentOffset);\n+    Address base = addr.getAddressAt(lockStackBaseOffset);\n+    while (base.lessThan(current)) {\n+        Address oop = base.getAddressAt(0);\n+        if (oop.equals(obj)) {\n+            return true;\n+        }\n+        base = base.addOffsetTo(oopPtrSize);\n+    }\n+    return false;\n+  }\n+\n","filename":"src\/jdk.hotspot.agent\/share\/classes\/sun\/jvm\/hotspot\/runtime\/JavaThread.java","additions":21,"deletions":0,"binary":false,"changes":21,"status":"modified"},{"patch":"@@ -85,0 +85,1 @@\n+          mark.monitor().isOwnedAnonymous() ||\n","filename":"src\/jdk.hotspot.agent\/share\/classes\/sun\/jvm\/hotspot\/runtime\/JavaVFrame.java","additions":1,"deletions":0,"binary":false,"changes":1,"status":"modified"},{"patch":"@@ -82,0 +82,4 @@\n+  public boolean isOwnedAnonymous() {\n+    return addr.getAddressAt(ownerFieldOffset).asLongValue() == 1;\n+  }\n+\n","filename":"src\/jdk.hotspot.agent\/share\/classes\/sun\/jvm\/hotspot\/runtime\/ObjectMonitor.java","additions":4,"deletions":0,"binary":false,"changes":4,"status":"modified"},{"patch":"@@ -213,0 +213,1 @@\n+        assert(!VM.getVM().getCommandLineBooleanFlag(\"UseFastLocking\"));\n@@ -230,1 +231,18 @@\n-        return owningThreadFromMonitor(monitor.owner());\n+        if (VM.getVM().getCommandLineBooleanFlag(\"UseFastLocking\")) {\n+            if (monitor.isOwnedAnonymous()) {\n+                OopHandle object = monitor.object();\n+                for (int i = 0; i < getNumberOfThreads(); i++) {\n+                    JavaThread thread = getJavaThreadAt(i);\n+                    if (thread.isLockOwned(object)) {\n+                        return thread;\n+                     }\n+                }\n+                throw new InternalError(\"We should have found a thread that owns the anonymous lock\");\n+            }\n+            \/\/ Owner can only be threads at this point.\n+            Address o = monitor.owner();\n+            if (o == null) return null;\n+            return new JavaThread(o);\n+        } else {\n+            return owningThreadFromMonitor(monitor.owner());\n+        }\n","filename":"src\/jdk.hotspot.agent\/share\/classes\/sun\/jvm\/hotspot\/runtime\/Threads.java","additions":19,"deletions":1,"binary":false,"changes":20,"status":"modified"},{"patch":"@@ -29,0 +29,1 @@\n+import sun.jvm.hotspot.oops.Oop;\n@@ -40,20 +41,0 @@\n-  private static AddressField klassField;\n-\n-  static {\n-    VM.registerVMInitializedObserver(new Observer() {\n-        public void update(Observable o, Object data) {\n-          initialize(VM.getVM().getTypeDataBase());\n-        }\n-      });\n-  }\n-\n-  private static void initialize(TypeDataBase db) {\n-    Type type = db.lookupType(\"oopDesc\");\n-\n-    if (VM.getVM().isCompressedKlassPointersEnabled()) {\n-      klassField = type.getAddressField(\"_metadata._compressed_klass\");\n-    } else {\n-      klassField = type.getAddressField(\"_metadata._klass\");\n-    }\n-  }\n-\n@@ -69,5 +50,1 @@\n-      if (VM.getVM().isCompressedKlassPointersEnabled()) {\n-        Metadata.instantiateWrapperFor(oop.getCompKlassAddressAt(klassField.getOffset()));\n-      } else {\n-        Metadata.instantiateWrapperFor(klassField.getValue(oop));\n-      }\n+      Oop.getKlassForOopHandle(oop);\n","filename":"src\/jdk.hotspot.agent\/share\/classes\/sun\/jvm\/hotspot\/utilities\/RobustOopDeterminator.java","additions":2,"deletions":25,"binary":false,"changes":27,"status":"modified"},{"patch":"@@ -76,1 +76,2 @@\n-    final int hubOffset = getFieldOffset(\"oopDesc::_metadata._klass\", Integer.class, \"Klass*\");\n+    \/\/ TODO: Lilliput. Probably ok.\n+    final int hubOffset = 4; \/\/ getFieldOffset(\"oopDesc::_metadata._klass\", Integer.class, \"Klass*\");\n","filename":"src\/jdk.internal.vm.ci\/share\/classes\/jdk.vm.ci.hotspot\/src\/jdk\/vm\/ci\/hotspot\/HotSpotVMConfig.java","additions":2,"deletions":1,"binary":false,"changes":3,"status":"modified"},{"patch":"@@ -45,1 +45,1 @@\n-  static markWord originalMark() { return markWord(markWord::lock_mask_in_place); }\n+  static markWord originalMark() { return markWord(markWord::unlocked_value); }\n@@ -81,0 +81,3 @@\n+  \/\/ TODO: This is the only use of PM::adjust_during_full_gc().\n+  \/\/ GCs use the variant with a forwarding structure here,\n+  \/\/ test that variant, and remove the method.\n","filename":"test\/hotspot\/gtest\/gc\/shared\/test_preservedMarks.cpp","additions":4,"deletions":1,"binary":false,"changes":5,"status":"modified"},{"patch":"@@ -30,0 +30,1 @@\n+#include \"memory\/metaspace\/metaspaceAlignment.hpp\"\n@@ -53,5 +54,0 @@\n-\/\/ See metaspaceArena.cpp : needed for predicting commit sizes.\n-namespace metaspace {\n-  extern size_t get_raw_word_size_for_requested_word_size(size_t net_word_size);\n-}\n-\n@@ -65,0 +61,1 @@\n+  int _alignment_words;\n@@ -67,1 +64,2 @@\n-  void initialize(const ArenaGrowthPolicy* growth_policy, const char* name = \"gtest-MetaspaceArena\") {\n+  void initialize(const ArenaGrowthPolicy* growth_policy, int alignment_words,\n+                  const char* name = \"gtest-MetaspaceArena\") {\n@@ -74,1 +72,1 @@\n-      _arena = new MetaspaceArena(&_context.cm(), _growth_policy, _lock, &_used_words_counter, name);\n+      _arena = new MetaspaceArena(&_context.cm(), _growth_policy, alignment_words, _lock, &_used_words_counter, name);\n@@ -88,1 +86,1 @@\n-    initialize(ArenaGrowthPolicy::policy_for_space_type(space_type, is_class), name);\n+    initialize(ArenaGrowthPolicy::policy_for_space_type(space_type, is_class), metaspace::MetaspaceMinAlignmentWords, name);\n@@ -96,1 +94,1 @@\n-    initialize(growth_policy, name);\n+    initialize(growth_policy, metaspace::MetaspaceMinAlignmentWords, name);\n@@ -275,1 +273,1 @@\n-    allocated += metaspace::get_raw_word_size_for_requested_word_size(s);\n+    allocated += metaspace::get_raw_word_size_for_requested_word_size(s, metaspace::MetaspaceMinAlignmentWords);\n@@ -332,1 +330,1 @@\n-    allocated += metaspace::get_raw_word_size_for_requested_word_size(s);\n+    allocated += metaspace::get_raw_word_size_for_requested_word_size(s, metaspace::MetaspaceMinAlignmentWords);\n@@ -596,1 +594,1 @@\n-    words_allocated += metaspace::get_raw_word_size_for_requested_word_size(alloc_words);\n+    words_allocated += metaspace::get_raw_word_size_for_requested_word_size(alloc_words, metaspace::MetaspaceMinAlignmentWords);\n","filename":"test\/hotspot\/gtest\/metaspace\/test_metaspacearena.cpp","additions":10,"deletions":12,"binary":false,"changes":22,"status":"modified"},{"patch":"@@ -29,0 +29,1 @@\n+#include \"memory\/metaspace\/metaspaceAlignment.hpp\"\n@@ -55,5 +56,0 @@\n-\/\/ See metaspaceArena.cpp : needed for predicting commit sizes.\n-namespace metaspace {\n-  extern size_t get_raw_word_size_for_requested_word_size(size_t net_word_size);\n-}\n-\n@@ -64,1 +60,2 @@\n-  MetaspaceArena* _arena;\n+  const SizeRange _allocation_range;\n+  const int _alignment_words;\n@@ -66,0 +63,1 @@\n+  MetaspaceArena* _arena;\n@@ -67,2 +65,0 @@\n-\n-  const SizeRange _allocation_range;\n@@ -90,1 +86,2 @@\n-  MemRangeCounter _alloc_count;\n+  MemRangeCounter _alloc_count_net; \/\/ net used bytes\n+  MemRangeCounter _alloc_count_raw; \/\/ net used bytes + internal overhead\n@@ -102,2 +99,2 @@\n-    assert(_dealloc_count.total_size() <= _alloc_count.total_size() &&\n-           _dealloc_count.count() <= _alloc_count.count(), \"Sanity\");\n+    assert(_dealloc_count.total_size() <= _alloc_count_net.total_size() &&\n+           _dealloc_count.count() <= _alloc_count_net.count(), \"Sanity\");\n@@ -119,1 +116,1 @@\n-    const size_t at_least_allocated = _alloc_count.total_size() - _dealloc_count.total_size();\n+    const size_t at_least_allocated = _alloc_count_net.total_size() - _dealloc_count.total_size();\n@@ -122,3 +119,6 @@\n-    const size_t max_word_overhead_per_alloc =\n-        4 + (metaspace::Settings::use_allocation_guard() ? 4 : 0);\n-    const size_t at_most_allocated = _alloc_count.total_size() + max_word_overhead_per_alloc * _alloc_count.count();\n+    size_t max_word_overhead_per_alloc = align_up(4, _alignment_words);\n+    \/\/ Guard fences come as a separate, secondary block\n+    if (metaspace::Settings::use_allocation_guard()) {\n+      max_word_overhead_per_alloc *= 2;\n+    }\n+    const size_t at_most_allocated = _alloc_count_raw.total_size() + max_word_overhead_per_alloc * _alloc_count_raw.count();\n@@ -128,1 +128,0 @@\n-\n@@ -135,1 +134,1 @@\n-  MetaspaceArenaTestBed(ChunkManager* cm, const ArenaGrowthPolicy* alloc_sequence,\n+  MetaspaceArenaTestBed(ChunkManager* cm, const ArenaGrowthPolicy* alloc_sequence, int alignment_words,\n@@ -137,0 +136,2 @@\n+    _allocation_range(allocation_range),\n+    _alignment_words(alignment_words),\n@@ -139,1 +140,0 @@\n-    _allocation_range(allocation_range),\n@@ -142,1 +142,1 @@\n-    _alloc_count(),\n+    _alloc_count_net(),\n@@ -149,1 +149,1 @@\n-    _arena = new MetaspaceArena(cm, alloc_sequence, _lock, used_words_counter, \"gtest-MetaspaceArenaTestBed-sm\");\n+    _arena = new MetaspaceArena(cm, alloc_sequence, alignment_words, _lock, used_words_counter, \"gtest-MetaspaceArenaTestBed-sm\");\n@@ -172,2 +172,2 @@\n-  size_t words_allocated() const        { return _alloc_count.total_size(); }\n-  int num_allocations() const           { return _alloc_count.count(); }\n+  size_t words_allocated() const        { return _alloc_count_net.total_size(); }\n+  int num_allocations() const           { return _alloc_count_net.count(); }\n@@ -177,0 +177,4 @@\n+  size_t calc_expected_usage_for_allocated_words(size_t word_size) {\n+    return metaspace::get_raw_word_size_for_requested_word_size(word_size, _alignment_words);\n+  }\n+\n@@ -182,1 +186,1 @@\n-      EXPECT_TRUE(is_aligned(p, sizeof(MetaWord)));\n+      EXPECT_TRUE(is_aligned(p, _alignment_words * BytesPerWord));\n@@ -189,2 +193,3 @@\n-      _alloc_count.add(word_size);\n-      if ((_alloc_count.count() % 20) == 0) {\n+      _alloc_count_net.add(word_size);\n+      _alloc_count_raw.add(calc_expected_usage_for_allocated_words(word_size));\n+      if ((_alloc_count_net.count() % 20) == 0) {\n@@ -232,1 +237,1 @@\n-  void create_new_test_bed_at(int slotindex, const ArenaGrowthPolicy* growth_policy, SizeRange allocation_range) {\n+  void create_new_test_bed_at(int slotindex, const ArenaGrowthPolicy* growth_policy, int alignment_words, SizeRange allocation_range) {\n@@ -234,1 +239,1 @@\n-    MetaspaceArenaTestBed* bed = new MetaspaceArenaTestBed(&_context.cm(), growth_policy,\n+    MetaspaceArenaTestBed* bed = new MetaspaceArenaTestBed(&_context.cm(), growth_policy, alignment_words,\n@@ -245,1 +250,4 @@\n-    create_new_test_bed_at(slotindex, growth_policy, allocation_range);\n+    const int alignment_bytes =\n+        1 << IntRange(metaspace::LogMetaspaceMinimalAlignment,\n+                      metaspace::LogMetaspaceMinimalAlignment + 7).random_value(); \/\/ zw 8 byte and 1K\n+    create_new_test_bed_at(slotindex, growth_policy, alignment_bytes \/ BytesPerWord, allocation_range);\n@@ -258,9 +266,0 @@\n-  \/\/ Create test beds for all slots\n-  void create_all_test_beds() {\n-    for (int slot = 0; slot < _testbeds.size(); slot++) {\n-      if (_testbeds.slot_is_null(slot)) {\n-        create_random_test_bed_at(slot);\n-      }\n-    }\n-  }\n-\n@@ -302,1 +301,1 @@\n-                metaspace::get_raw_word_size_for_requested_word_size(bed->size_of_last_failed_allocation()));\n+                bed->calc_expected_usage_for_allocated_words(bed->size_of_last_failed_allocation()));\n","filename":"test\/hotspot\/gtest\/metaspace\/test_metaspacearena_stress.cpp","additions":37,"deletions":38,"binary":false,"changes":75,"status":"modified"},{"patch":"@@ -39,0 +39,1 @@\n+#ifndef _LP64\n@@ -40,0 +41,1 @@\n+#endif\n","filename":"test\/hotspot\/gtest\/oops\/test_typeArrayOop.cpp","additions":2,"deletions":0,"binary":false,"changes":2,"status":"modified"},{"patch":"@@ -170,0 +170,12 @@\n+\n+Lilliput temporary:\n+compiler\/c2\/irTests\/TestVectorizationNotRun.java 8301785 generic-all\n+runtime\/CompressedOops\/CompressedClassPointerEncoding.java#x64-area-beyond-encoding-range-use-xor 8302094 windows-all,macosx-all\n+runtime\/CompressedOops\/CompressedClassPointerEncoding.java#x64-area-partly-within-encoding-range-use-add 8302094 windows-all,macosx-all\n+runtime\/CompressedOops\/CompressedClassPointerEncoding.java#x64-area-within-encoding-range-use-zero 8302094 windows-all,macosx-all\n+runtime\/CompressedOops\/CompressedClassPointerEncoding.java#x64-area-far-out-no-low-bits-use-xor 8302094 windows-all,macosx-all\n+runtime\/CompressedOops\/CompressedClassPointerEncoding.java#x64-area-far-out-with-low-bits-use-add 8302094 windows-all,macosx-all\n+runtime\/CompressedOops\/CompressedClassPointerEncoding.java#aarch64-xor 8302094 windows-all,macosx-all\n+runtime\/CompressedOops\/CompressedClassPointerEncoding.java#aarch64-movk-1 8302094 windows-all,macosx-all\n+runtime\/CompressedOops\/CompressedClassPointerEncoding.java#aarch64-movk-2 8302094 windows-all,macosx-all\n+runtime\/CompressedOops\/CompressedClassPointerEncoding.java#aarch64-movk-3 8302094 windows-all,macosx-all\n","filename":"test\/hotspot\/jtreg\/ProblemList.txt","additions":12,"deletions":0,"binary":false,"changes":12,"status":"modified"},{"patch":"@@ -28,1 +28,0 @@\n- * @run main\/othervm -XX:+IgnoreUnrecognizedVMOptions -XX:+TieredCompilation -XX:TieredStopAtLevel=1 -Xcomp -XX:-UseCompressedClassPointers -XX:CompileOnly=TestArrayCopyToFromObject.test TestArrayCopyToFromObject\n","filename":"test\/hotspot\/jtreg\/compiler\/c1\/TestArrayCopyToFromObject.java","additions":0,"deletions":1,"binary":false,"changes":1,"status":"modified"},{"patch":"@@ -36,5 +36,0 @@\n- *                                 -XX:+UseCompressedOops -XX:-UseCompressedClassPointers\n- *                                 -XX:CompileCommand=dontinline,compiler.unsafe.OpaqueAccesses::test*\n- *                                 compiler.unsafe.OpaqueAccesses\n- * @run main\/bootclasspath\/othervm -XX:+IgnoreUnrecognizedVMOptions -XX:+UnlockDiagnosticVMOptions\n- *                                 -XX:-TieredCompilation -Xbatch\n@@ -43,5 +38,0 @@\n- *                                 compiler.unsafe.OpaqueAccesses\n- * @run main\/bootclasspath\/othervm -XX:+IgnoreUnrecognizedVMOptions -XX:+UnlockDiagnosticVMOptions\n- *                                 -XX:-TieredCompilation -Xbatch\n- *                                 -XX:-UseCompressedOops -XX:-UseCompressedClassPointers\n- *                                 -XX:CompileCommand=dontinline,compiler.unsafe.OpaqueAccesses::test*\n","filename":"test\/hotspot\/jtreg\/compiler\/unsafe\/OpaqueAccesses.java","additions":0,"deletions":10,"binary":false,"changes":10,"status":"modified"},{"patch":"@@ -1,56 +0,0 @@\n-\/*\n- * Copyright (c) 2013, 2022, Oracle and\/or its affiliates. All rights reserved.\n- * DO NOT ALTER OR REMOVE COPYRIGHT NOTICES OR THIS FILE HEADER.\n- *\n- * This code is free software; you can redistribute it and\/or modify it\n- * under the terms of the GNU General Public License version 2 only, as\n- * published by the Free Software Foundation.\n- *\n- * This code is distributed in the hope that it will be useful, but WITHOUT\n- * ANY WARRANTY; without even the implied warranty of MERCHANTABILITY or\n- * FITNESS FOR A PARTICULAR PURPOSE.  See the GNU General Public License\n- * version 2 for more details (a copy is included in the LICENSE file that\n- * accompanied this code).\n- *\n- * You should have received a copy of the GNU General Public License version\n- * 2 along with this work; if not, write to the Free Software Foundation,\n- * Inc., 51 Franklin St, Fifth Floor, Boston, MA 02110-1301 USA.\n- *\n- * Please contact Oracle, 500 Oracle Parkway, Redwood Shores, CA 94065 USA\n- * or visit www.oracle.com if you need additional information or have any\n- * questions.\n- *\/\n-\n-package gc.arguments;\n-\n-import jdk.test.lib.process.OutputAnalyzer;\n-import jdk.test.lib.Platform;\n-\n-\/*\n- * @test\n- * @bug 8015107\n- * @summary Tests that VM prints a warning when -XX:CompressedClassSpaceSize\n- *          is used together with -XX:-UseCompressedClassPointers\n- * @library \/test\/lib\n- * @library \/\n- * @modules java.base\/jdk.internal.misc\n- *          java.management\n- * @run driver gc.arguments.TestCompressedClassFlags\n- *\/\n-public class TestCompressedClassFlags {\n-    public static void main(String[] args) throws Exception {\n-        if (Platform.is64bit()) {\n-            OutputAnalyzer output = runJava(\"-XX:CompressedClassSpaceSize=1g\",\n-                                            \"-XX:-UseCompressedClassPointers\",\n-                                            \"-version\");\n-            output.shouldContain(\"warning\");\n-            output.shouldNotContain(\"error\");\n-            output.shouldHaveExitValue(0);\n-        }\n-    }\n-\n-    private static OutputAnalyzer runJava(String ... args) throws Exception {\n-        ProcessBuilder pb = GCArguments.createJavaProcessBuilder(args);\n-        return new OutputAnalyzer(pb.start());\n-    }\n-}\n","filename":"test\/hotspot\/jtreg\/gc\/arguments\/TestCompressedClassFlags.java","additions":0,"deletions":56,"binary":false,"changes":56,"status":"deleted"},{"patch":"@@ -129,1 +129,3 @@\n-    String compressedClassSpaceSizeArg = \"-XX:CompressedClassSpaceSize=\" + 2 * getCompressedClassSpaceSize();\n+    \/\/ Lilliput: do not assume a max. class space size, since that is subject to change. Instead, use a value slightly smaller\n+    \/\/  than what the parent VM runs with (which is the default size).\n+    String compressedClassSpaceSizeArg = \"-XX:CompressedClassSpaceSize=\" + (getCompressedClassSpaceSize() - 1);\n","filename":"test\/hotspot\/jtreg\/gc\/arguments\/TestUseCompressedOopsErgoTools.java","additions":3,"deletions":1,"binary":false,"changes":4,"status":"modified"},{"patch":"@@ -60,1 +60,1 @@\n-        dump_args.addAll(Arrays.asList(new String[] { \"-Xshare:dump\", \"-Xlog:cds\" }));\n+        dump_args.addAll(Arrays.asList(new String[] { \"-Xshare:dump\", \"-Xlog:cds*\", \"-Xlog:metaspace*\" }));\n@@ -64,0 +64,1 @@\n+        output.reportDiagnosticSummary();\n@@ -73,1 +74,1 @@\n-            load_args.addAll(Arrays.asList(new String[] { \"-Xshare:on\", \"-version\" }));\n+            load_args.addAll(Arrays.asList(new String[] { \"-Xshare:on\", \"-Xlog:cds*\", \"-Xlog:metaspace*\", \"-version\" }));\n@@ -77,0 +78,1 @@\n+            output.reportDiagnosticSummary();\n","filename":"test\/hotspot\/jtreg\/gc\/g1\/TestSharedArchiveWithPreTouch.java","additions":4,"deletions":2,"binary":false,"changes":6,"status":"modified"},{"patch":"@@ -48,0 +48,1 @@\n+import jdk.test.lib.Platform;\n@@ -75,1 +76,1 @@\n-    private static final int OBJECT_SIZE_HIGH = 3250;\n+    private static final int OBJECT_SIZE_HIGH = Platform.is64bit() ? 3266 : 3258;\n","filename":"test\/hotspot\/jtreg\/gc\/g1\/plab\/TestPLABPromotion.java","additions":2,"deletions":1,"binary":false,"changes":3,"status":"modified"},{"patch":"@@ -300,1 +300,1 @@\n-        return Platform.is64bit() && InputArguments.contains(\"-XX:+UseCompressedClassPointers\");\n+        return Platform.is64bit();\n","filename":"test\/hotspot\/jtreg\/gc\/metaspace\/TestMetaspacePerfCounters.java","additions":1,"deletions":1,"binary":false,"changes":2,"status":"modified"},{"patch":"@@ -38,2 +38,1 @@\n- * @run driver gc.metaspace.TestSizeTransitions false -XX:+UseSerialGC\n- * @run driver gc.metaspace.TestSizeTransitions true  -XX:+UseSerialGC\n+ * @run driver gc.metaspace.TestSizeTransitions -XX:+UseSerialGC\n@@ -46,2 +45,1 @@\n- * @run driver gc.metaspace.TestSizeTransitions false -XX:+UseParallelGC\n- * @run driver gc.metaspace.TestSizeTransitions true  -XX:+UseParallelGC\n+ * @run driver gc.metaspace.TestSizeTransitions -XX:+UseParallelGC\n@@ -54,2 +52,1 @@\n- * @run driver gc.metaspace.TestSizeTransitions false -XX:+UseG1GC\n- * @run driver gc.metaspace.TestSizeTransitions true  -XX:+UseG1GC\n+ * @run driver gc.metaspace.TestSizeTransitions -XX:+UseG1GC\n@@ -93,2 +90,2 @@\n-    \/\/ args: <use-coops> <gc-arg>\n-    if (args.length != 2) {\n+    \/\/ args: <gc-arg>\n+    if (args.length != 1) {\n@@ -99,8 +96,1 @@\n-    final boolean useCompressedKlassPointers = Boolean.parseBoolean(args[0]);\n-    final String gcArg = args[1];\n-\n-    if (!hasCompressedKlassPointers && useCompressedKlassPointers) {\n-       \/\/ No need to run this configuration.\n-       System.out.println(\"Skipping test.\");\n-       return;\n-    }\n+    final String gcArg = args[0];\n@@ -109,3 +99,0 @@\n-    if (hasCompressedKlassPointers) {\n-      jvmArgs.add(useCompressedKlassPointers ? \"-XX:+UseCompressedClassPointers\" : \"-XX:-UseCompressedClassPointers\");\n-    }\n@@ -127,1 +114,1 @@\n-    if (useCompressedKlassPointers) {\n+    if (hasCompressedKlassPointers) {\n","filename":"test\/hotspot\/jtreg\/gc\/metaspace\/TestSizeTransitions.java","additions":7,"deletions":20,"binary":false,"changes":27,"status":"modified"},{"patch":"@@ -52,1 +52,1 @@\n- *   -Xmx1G -XX:G1HeapRegionSize=8m -XX:MaxGCPauseMillis=1000 gc.stress.TestMultiThreadStressRSet 60 16\n+ *   -Xmx1100M -XX:G1HeapRegionSize=8m -XX:MaxGCPauseMillis=1000 gc.stress.TestMultiThreadStressRSet 60 16\n","filename":"test\/hotspot\/jtreg\/gc\/stress\/TestMultiThreadStressRSet.java","additions":1,"deletions":1,"binary":false,"changes":2,"status":"modified"},{"patch":"@@ -30,11 +30,0 @@\n-\n-\/* @test\n- * @bug 8264008\n- * @summary Run metaspace utils related gtests with compressed class pointers off\n- * @requires vm.bits == 64\n- * @library \/test\/lib\n- * @modules java.base\/jdk.internal.misc\n- *          java.xml\n- * @requires vm.flagless\n- * @run main\/native GTestWrapper --gtest_filter=MetaspaceUtils* -XX:-UseCompressedClassPointers\n- *\/\n","filename":"test\/hotspot\/jtreg\/gtest\/MetaspaceUtilsGtests.java","additions":0,"deletions":11,"binary":false,"changes":11,"status":"modified"},{"patch":"@@ -0,0 +1,404 @@\n+\/*\n+ * Copyright (c) 2013, 2021, Oracle and\/or its affiliates. All rights reserved.\n+ * DO NOT ALTER OR REMOVE COPYRIGHT NOTICES OR THIS FILE HEADER.\n+ *\n+ * This code is free software; you can redistribute it and\/or modify it\n+ * under the terms of the GNU General Public License version 2 only, as\n+ * published by the Free Software Foundation.\n+ *\n+ * This code is distributed in the hope that it will be useful, but WITHOUT\n+ * ANY WARRANTY; without even the implied warranty of MERCHANTABILITY or\n+ * FITNESS FOR A PARTICULAR PURPOSE.  See the GNU General Public License\n+ * version 2 for more details (a copy is included in the LICENSE file that\n+ * accompanied this code).\n+ *\n+ * You should have received a copy of the GNU General Public License version\n+ * 2 along with this work; if not, write to the Free Software Foundation,\n+ * Inc., 51 Franklin St, Fifth Floor, Boston, MA 02110-1301 USA.\n+ *\n+ * Please contact Oracle, 500 Oracle Parkway, Redwood Shores, CA 94065 USA\n+ * or visit www.oracle.com if you need additional information or have any\n+ * questions.\n+ *\/\n+\n+\/\/ These tests test that narrow Klass pointer encoding\/decoding work.\n+\/\/\n+\/\/ Note that we do not enforce the encoding base directly. We enforce base and size of the compressed class space.\n+\/\/ The hotspot then decides on the best encoding range and scheme to chose for the given range.\n+\/\/\n+\/\/ So what we really test here is that for a given range-to-encode:\n+\/\/  - the chosen encoding range and architecture-specific mode makes sense - e.g. if range fits into low address\n+\/\/    space, use base=0 and zero-based encoding.\n+\/\/  - and that the chosen encoding actually works by starting a simple program which loads a bunch of classes.\n+\/\/\n+\/\/  In order for that to work, we have to switch of CDS. Switching off CDS means the hotspot choses the encoding base\n+\/\/  based on the class space base address (we just know this - see CompressedKlassPointers::initialize() - and if this\n+\/\/  changes, we may have to adapt this test).\n+\/\/\n+\/\/  Switching off CDS also means we use the class space much more fully. More Klass structures stored in that range\n+\/\/  and we exercise the ability of Metaspace to allocate Klass structures with the correct alignment, compatible to\n+\/\/  encoding.\n+\n+\n+\/\/ x64\n+\n+\/*\n+ * @test id=x64-area-beyond-encoding-range-use-xor\n+ * @requires os.arch==\"amd64\" | os.arch==\"x86_64\"\n+ * @requires vm.flagless\n+ * @library \/test\/lib\n+ * @modules java.base\/jdk.internal.misc\n+ *          java.management\n+ * @run driver CompressedClassPointerEncoding x64-area-beyond-encoding-range-use-xor\n+ *\/\n+\n+\/*\n+ * @test id=x64-area-partly-within-encoding-range-use-add\n+ * @requires os.arch==\"amd64\" | os.arch==\"x86_64\"\n+ * @requires vm.flagless\n+ * @library \/test\/lib\n+ * @modules java.base\/jdk.internal.misc\n+ *          java.management\n+ * @run driver CompressedClassPointerEncoding x64-area-partly-within-encoding-range-use-add\n+ *\/\n+\n+\/*\n+ * @test id=x64-area-within-encoding-range-use-zero\n+ * @requires os.arch==\"amd64\" | os.arch==\"x86_64\"\n+ * @requires vm.flagless\n+ * @library \/test\/lib\n+ * @modules java.base\/jdk.internal.misc\n+ *          java.management\n+ * @run driver CompressedClassPointerEncoding x64-area-within-encoding-range-use-zero\n+ *\/\n+\n+\/*\n+ * @test id=x64-area-far-out-no-low-bits-use-xor\n+ * @requires os.arch==\"amd64\" | os.arch==\"x86_64\"\n+ * @requires vm.flagless\n+ * @library \/test\/lib\n+ * @modules java.base\/jdk.internal.misc\n+ *          java.management\n+ * @run driver CompressedClassPointerEncoding x64-area-far-out-no-low-bits-use-xor\n+ *\/\n+\n+\/*\n+ * @test id=x64-area-far-out-with-low-bits-use-add\n+ * @requires os.arch==\"amd64\" | os.arch==\"x86_64\"\n+ * @requires vm.flagless\n+ * @library \/test\/lib\n+ * @modules java.base\/jdk.internal.misc\n+ *          java.management\n+ * @run driver CompressedClassPointerEncoding x64-area-far-out-with-low-bits-use-add\n+ *\/\n+\n+\/\/ aarch64\n+\n+\/*\n+ * @test id=aarch64-xor\n+ * @requires os.arch==\"aarch64\"\n+ * @requires vm.flagless\n+ * @library \/test\/lib\n+ * @modules java.base\/jdk.internal.misc\n+ *          java.management\n+ * @run driver CompressedClassPointerEncoding aarch64-xor\n+ *\/\n+\n+\/*\n+ * @test id=aarch64-movk-1\n+ * @requires os.arch==\"aarch64\"\n+ * @requires vm.flagless\n+ * @library \/test\/lib\n+ * @modules java.base\/jdk.internal.misc\n+ *          java.management\n+ * @run driver CompressedClassPointerEncoding aarch64-movk-1\n+ *\/\n+\n+\/*\n+ * @test id=aarch64-movk-2\n+ * @requires os.arch==\"aarch64\"\n+ * @requires vm.flagless\n+ * @library \/test\/lib\n+ * @modules java.base\/jdk.internal.misc\n+ *          java.management\n+ * @run driver CompressedClassPointerEncoding aarch64-movk-2\n+ *\/\n+\n+\/*\n+ * @test id=aarch64-movk-3\n+ * @requires os.arch==\"aarch64\"\n+ * @requires vm.flagless\n+ * @library \/test\/lib\n+ * @modules java.base\/jdk.internal.misc\n+ *          java.management\n+ * @run driver CompressedClassPointerEncoding aarch64-movk-3\n+ *\/\n+\n+import jdk.test.lib.Platform;\n+import jdk.test.lib.process.OutputAnalyzer;\n+import jdk.test.lib.process.ProcessTools;\n+import jtreg.SkippedException;\n+\n+import java.io.IOException;\n+import java.util.Arrays;\n+import java.util.regex.Pattern;\n+\n+public class CompressedClassPointerEncoding {\n+\n+    final static long M = 1024 * 1024;\n+    final static long G = 1024 * M;\n+\n+    final static int narrowKlassBitSize = 22;\n+    final static int narrowKlassShift = 9;\n+    final static long narrowKlassValueSpan = 1L << narrowKlassBitSize;\n+    final static long encodingRangeSpan = narrowKlassValueSpan << narrowKlassShift;\n+\n+    final static long defaultCCSSize = 128 * M;\n+    final static long ccsGranularity = 16 * M; \/\/ root chunk size\n+\n+\n+\n+    static class TestDetails {\n+        public final String name;\n+        public final long ccsBaseAddress;\n+        public final long compressedClassSpaceSize;\n+        public final long expectedEncodingBase;\n+        public final String expectedEncodingMode;\n+        \/\/ The test relies on -XX:CompressedClassSpaceBaseAddress to force the CCS base address to a certain value.\n+        \/\/ That can fail due to ASLR. If tolerateCCSMappingError is true, we tolerate this.\n+        public final boolean tolerateCCSMappingError;\n+\n+        public TestDetails(String name, long ccsBaseAddress,\n+                           long expectedEncodingBase, String expectedEncodingMode, boolean tolerateCCSMappingError) {\n+            this.name = name;\n+            this.ccsBaseAddress = ccsBaseAddress;\n+            this.compressedClassSpaceSize = defaultCCSSize;\n+            this.expectedEncodingBase = expectedEncodingBase;\n+            this.expectedEncodingMode = expectedEncodingMode;\n+            this.tolerateCCSMappingError = tolerateCCSMappingError;\n+        }\n+    };\n+\n+    static void runTestWithName(String name) throws IOException {\n+\n+        \/\/ How this works:\n+        \/\/ 1) we enforce a CCS base address with -XX:CompressedClassSpaceBaseAddress. This bypasses the automatic\n+        \/\/    CCS placement we do in Metaspace\/CDS initialization; CCS begins at that address.\n+        \/\/ 2) We set CCS size to 128M.\n+        \/\/ 3) We expect the zero-based encoding range to be [0...<max encoding span>)\n+        \/\/ 4) Depending on where CCS is located wrt the zero-based encoding range, we expect different encoding\n+        \/\/    platform dependent mechanisms. For example,\n+        \/\/    a) If CCS range fits completely into the zero-based encoding range, we expect zero-based encoding\n+        \/\/    b) If CCs range lies partly or completely outside the zero-based encoding range, zero-base encoding\n+        \/\/       cannot work. Typically, if CCS base address and Klass* offset range can be appended, something like\n+        \/\/       xor would be used, otherwise some for of add+shift\n+\n+        String expectedMode;\n+        long ccsBaseAddress;\n+        long expectedEncodingRangeStart;\n+        boolean tolerateMappingFailure = false;\n+        switch (name) {\n+\n+            \/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\n+            \/\/\/\/\/\/ x64 \/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\n+\n+            case \"x64-area-beyond-encoding-range-use-xor\":\n+                \/\/ CCS base starts just (beyond) zero-based encoding range.\n+                \/\/ - Encoding cannot be zero-based\n+                \/\/ - We should be able to use xor mode since CCS base address and Klass* offset range don't intersect\n+                ccsBaseAddress = encodingRangeSpan;\n+                expectedEncodingRangeStart = ccsBaseAddress;\n+                expectedMode = \"xor\";\n+                break;\n+            case \"x64-area-partly-within-encoding-range-use-add\":\n+                \/\/ CCS partly contained within zero-based encoding range, but last part (a single granule) outside.\n+                \/\/ - Encoding cannot be zero-based\n+                \/\/ - We cannot use xor since CCS base address intersects with max left-shifted narrow Klass pointer\n+                ccsBaseAddress = encodingRangeSpan - defaultCCSSize + ccsGranularity;\n+                expectedEncodingRangeStart = ccsBaseAddress;\n+                expectedMode = \"add\";\n+                break;\n+            case \"x64-area-within-encoding-range-use-zero\":\n+                \/\/ CCS fully contained within zero-based encoding range\n+                \/\/ - Encoding can be zero-based\n+                ccsBaseAddress = encodingRangeSpan - defaultCCSSize;\n+                expectedEncodingRangeStart = 0;\n+                expectedMode = \"zero\";\n+                break;\n+            case \"x64-area-far-out-no-low-bits-use-xor\":\n+                \/\/ CCS located far beyond zero-based encoding range, at a nicely aligned base.\n+                \/\/ - Encoding cannot be zero-based\n+                \/\/ - We should be able to use xor mode since CCS base address does not intersect with\n+                \/\/   max left-shifted narrow Klass pointer\n+                ccsBaseAddress = encodingRangeSpan * 4;\n+                expectedEncodingRangeStart = ccsBaseAddress;\n+                expectedMode = \"xor\";\n+                break;\n+            case \"x64-area-far-out-with-low-bits-use-add\":\n+                \/\/ CCS located far beyond zero-based encoding range, but address is unfit for xor mode.\n+                \/\/ - Encoding cannot be zero-based\n+                \/\/ - We cannot use xor since CCS base address has lower bits set that intersect with\n+                \/\/   max left-shifted narrow Klass pointer\n+                ccsBaseAddress = (encodingRangeSpan * 4) + ccsGranularity;\n+                expectedEncodingRangeStart = ccsBaseAddress;\n+                expectedMode = \"add\";\n+                break;\n+\n+            \/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\n+            \/\/\/\/\/\/ aarch64 \/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\n+\n+            case \"aarch64-xor\":\n+                \/\/ CCS with a base which is a valid immediate, does not intersect the uncompressed klass pointer bits,\n+                \/\/ should use xor+shift\n+                ccsBaseAddress = 0x1000000000L;\n+                expectedEncodingRangeStart = ccsBaseAddress;\n+                expectedMode = \"xor\";\n+                break;\n+\n+            \/\/ Attempt to test movk:\n+            \/\/ Addresses that would need movk mode are quite high for lilliput (9 bit shift), with the lowest\n+            \/\/ needing 40bits. Therefore the following tests may fail because CCS cannot be mapped. This depends on\n+            \/\/ how the kernel was compiled (39, 42 or 48 bit virtual addresses).\n+            \/\/ Therefore we tolerate mapping failures for the following tests.\n+            case \"aarch64-movk-1\":\n+                ccsBaseAddress = 0x00000a0000000000L;\n+                expectedEncodingRangeStart = ccsBaseAddress;\n+                expectedMode = \"movk\";\n+                tolerateMappingFailure = true;\n+                break;\n+\n+            case \"aarch64-movk-2\":\n+                ccsBaseAddress = 0x120000000000L;\n+                expectedEncodingRangeStart = ccsBaseAddress;\n+                expectedMode = \"movk\";\n+                tolerateMappingFailure = true;\n+                break;\n+\n+            case \"aarch64-movk-3\":\n+                ccsBaseAddress = 0x160000000000L;\n+                expectedEncodingRangeStart = ccsBaseAddress;\n+                expectedMode = \"movk\";\n+                tolerateMappingFailure = true;\n+                break;\n+\n+            default:\n+                throw new RuntimeException(\"Bad test name: \" + name);\n+        }\n+\n+        TestDetails details = new TestDetails(name, ccsBaseAddress, expectedEncodingRangeStart, expectedMode, tolerateMappingFailure);\n+        runTest(details);\n+    }\n+\n+    ;\n+\n+    \/\/ Helper function. Given a string, replace $1 ... $n with\n+    \/\/ replacement_strings[0] ... replacement_strings[n]\n+    static private String replacePlaceholdersInString(String original, String ...replacement_strings) {\n+        String result = original;\n+        int repl_id = 1; \/\/ 1 based\n+        for (String replacement : replacement_strings) {\n+            String placeholder = \"$\" + repl_id;\n+            result = result.replace(placeholder, replacement);\n+            repl_id ++;\n+        }\n+        return result;\n+    }\n+\n+    \/\/ Helper function. Given a string array, replace $1 ... $n with\n+    \/\/ replacement_strings[0] ... replacement_strings[n]\n+    static private String[] replacePlaceholdersInArray(String[] original, String ...replacement_strings) {\n+        String[] copy = new String[original.length];\n+        for (int n = 0; n < copy.length; n ++) {\n+            copy[n] = replacePlaceholdersInString(original[n], replacement_strings);\n+        }\n+        return copy;\n+    }\n+\n+    static void runTest(TestDetails details) throws IOException {\n+        System.err.println(\"----------------------------------------------------\");\n+        System.err.println(\"Running Test: \" + details.name);\n+        System.err.println(details);\n+\n+        \/\/ Replace:\n+        \/\/ $1 with force base address\n+        \/\/ $2 with compressed class space size\n+        String[] vmOptionsTemplate = new String[] {\n+                \"-XX:CompressedClassSpaceBaseAddress=$1\",\n+                \"-XX:CompressedClassSpaceSize=$2\",\n+                \"-Xshare:off\",                         \/\/ Disable CDS\n+                \"-Xlog:metaspace*\",                    \/\/ for analysis\n+                \"-XX:+PrintMetaspaceStatisticsAtExit\", \/\/ for analysis\n+                \"-version\"\n+        };\n+\n+        String ccsBaseAddressAsHex = String.format(\"0x%016x\", details.ccsBaseAddress);\n+\n+        \/\/ VM Options: replace:\n+        \/\/ $1 with force base address\n+        \/\/ $2 with compressed class space size\n+        String[] vmOptions = replacePlaceholdersInArray(vmOptionsTemplate,\n+                ccsBaseAddressAsHex,              \/\/ $1\n+                (details.compressedClassSpaceSize \/ M) + \"M\");    \/\/ $2\n+\n+        ProcessBuilder pb = ProcessTools.createJavaProcessBuilder(vmOptions);\n+        OutputAnalyzer output = new OutputAnalyzer(pb.start());\n+\n+        System.err.println(\"----------------------------------------------------\");\n+        System.err.println(Arrays.toString(vmOptions));\n+        output.reportDiagnosticSummary();\n+        System.err.println(\"----------------------------------------------------\");\n+\n+        if (details.tolerateCCSMappingError) {\n+            String template = \"CompressedClassSpaceBaseAddress=$1 given, but reserving class space failed\";\n+            String pat = replacePlaceholdersInString(template, ccsBaseAddressAsHex);\n+            if (output.getStdout().contains(pat)) {\n+                throw new SkippedException(\"Skipping test: failed to force CCS to base address \" + ccsBaseAddressAsHex);\n+            }\n+        }\n+\n+        \/\/ Replace:\n+        \/\/ $1 with expected ccs base address (extended hex printed)\n+        \/\/ $2 with expected encoding base (extended hex printed)\n+        \/\/ $3 with expected encoding shift\n+        \/\/ $4 with expected encoding range\n+        \/\/ $5 with expected encoding mode\n+        String[] expectedOutputTemplate = new String[] {\n+                \"Successfully forced class space address to $1\",\n+                \"CDS archive(s) not mapped\",\n+                \"CompressedClassSpaceSize: 128.00 MB\",\n+                \"KlassAlignmentInBytes: 512\",\n+                \"KlassEncodingMetaspaceMax: 2.00 GB\",\n+                \"Narrow klass base: $2, Narrow klass shift: $3, Narrow klass range: $4, Encoding mode $5\"\n+        };\n+\n+        String expectedOutput[] = replacePlaceholdersInArray(expectedOutputTemplate,\n+                ccsBaseAddressAsHex, \/\/ $1\n+                String.format(\"0x%016x\", details.expectedEncodingBase), \/\/ $2\n+                Long.toString(narrowKlassShift), \/\/ $3\n+                String.format(\"0x%x\", encodingRangeSpan), \/\/ $4\n+                details.expectedEncodingMode  \/\/ $5\n+        );\n+\n+        String[] lines = output.asLines().toArray(new String[0]);\n+        int found = 0;\n+        int lineno = 0;\n+        while (lineno < lines.length && found < expectedOutput.length) {\n+            String pat = expectedOutput[found];\n+            String line = lines[lineno];\n+            if (line.contains(pat)) {\n+                System.out.println(\"Found: \" + pat + \" at line \" + lineno);\n+                found ++;\n+            }\n+            lineno++;\n+        }\n+        if (found < expectedOutput.length) {\n+            throw new RuntimeException(\"Not all expected pattern found. First missing: \" + expectedOutput[found]);\n+        }\n+\n+        output.shouldHaveExitValue(0);\n+    }\n+\n+    public static void main(String[] args) throws Exception {\n+        runTestWithName(args[0]);\n+    }\n+}\n","filename":"test\/hotspot\/jtreg\/runtime\/CompressedOops\/CompressedClassPointerEncoding.java","additions":404,"deletions":0,"binary":false,"changes":404,"status":"added"},{"patch":"@@ -61,0 +61,3 @@\n+    \/* Lilliput: cannot work due to drastically reduced narrow klass pointer range (atm 2g and that may get\n+       smaller still). There is an argument for improving CDS\/CCS reservation and make it more likely to run\n+       zero-based, but that logic has to be rethought.\n@@ -75,0 +78,1 @@\n+     *\/\n@@ -78,0 +82,1 @@\n+    \/* Lilliput: See comment above.\n@@ -92,0 +97,1 @@\n+    *\/\n@@ -96,0 +102,2 @@\n+    \/* Lilliput: I am not sure what the point of this test CCS reservation is independent from\n+       heap. See below the desparate attempts to predict heap reservation on PPC. Why do we even care?\n@@ -116,0 +124,1 @@\n+     *\/\n@@ -121,0 +130,1 @@\n+    \/* Lilliput: narrow klass pointer range drastically reduced. See comments under smallHeapTest().\n@@ -140,0 +150,1 @@\n+    *\/\n@@ -142,0 +153,4 @@\n+    \/* Lilliput: not sure what the point of this test is. The ability to have a class space if heap uses\n+       large pages? Why would that be a problem? Kept alive for now since it makes no problems even with\n+       smaller class pointers.\n+     *\/\n@@ -202,0 +217,1 @@\n+    \/* Lilliput: narrow klass pointer range drastically reduced. See comments under smallHeapTest().\n@@ -217,0 +233,1 @@\n+    *\/\n@@ -218,0 +235,1 @@\n+    \/* Lilliput: narrow klass pointer range drastically reduced. See comments under smallHeapTest().\n@@ -237,0 +255,1 @@\n+    *\/\n@@ -238,0 +257,1 @@\n+    \/* Lilliput: narrow klass pointer range drastically reduced. See comments under smallHeapTest().\n@@ -257,0 +277,1 @@\n+    *\/\n@@ -325,4 +346,4 @@\n-        smallHeapTest();\n-        smallHeapTestWith1G();\n-        largeHeapTest();\n-        largeHeapAbove32GTest();\n+        \/\/ smallHeapTest();\n+        \/\/ smallHeapTestWith1G();\n+        \/\/ largeHeapTest();\n+        \/\/ largeHeapAbove32GTest();\n@@ -333,3 +354,3 @@\n-        smallHeapTestNoCoop();\n-        smallHeapTestWith1GNoCoop();\n-        largeHeapTestNoCoop();\n+        \/\/ smallHeapTestNoCoop();\n+        \/\/ smallHeapTestWith1GNoCoop();\n+        \/\/ largeHeapTestNoCoop();\n","filename":"test\/hotspot\/jtreg\/runtime\/CompressedOops\/CompressedClassPointers.java","additions":28,"deletions":7,"binary":false,"changes":35,"status":"modified"},{"patch":"@@ -41,0 +41,7 @@\n+    \/\/ Sizes beyond this will be rejected by hotspot arg parsing\n+    \/\/ (Lilliput: see Metaspace::max_class_space_size() for details)\n+    static final long max_class_space_size = 2013265920;\n+\n+    \/\/ Below this size class space will be silently enlarged to a multiple of this size\n+    static final long min_class_space_size = 16777216;\n+\n@@ -44,6 +51,0 @@\n-        \/\/ Minimum size is 1MB\n-        pb = ProcessTools.createJavaProcessBuilder(\"-XX:CompressedClassSpaceSize=0\",\n-                                                   \"-version\");\n-        output = new OutputAnalyzer(pb.start());\n-        output.shouldContain(\"outside the allowed range\")\n-              .shouldHaveExitValue(1);\n@@ -58,0 +59,1 @@\n+        \/\/\/\/\/\/\/\/\/\/\/\n@@ -59,2 +61,4 @@\n-        \/\/ Maximum size is 3GB\n-        pb = ProcessTools.createJavaProcessBuilder(\"-XX:CompressedClassSpaceSize=4g\",\n+        \/\/ Going below the minimum size for class space (one root chunk size = atm 4M) should be transparently\n+        \/\/ handled by the hotspot, which should round up class space size and not report an error.\n+        pb = ProcessTools.createJavaProcessBuilder(\"-XX:CompressedClassSpaceSize=1m\",\n+                                                   \"-Xlog:gc+metaspace=trace\",\n@@ -63,2 +67,2 @@\n-        output.shouldContain(\"outside the allowed range\")\n-              .shouldHaveExitValue(1);\n+        output.shouldMatch(\"Compressed class space.*\" + min_class_space_size)\n+              .shouldHaveExitValue(0);\n@@ -66,0 +70,1 @@\n+        \/\/\/\/\/\/\/\/\/\/\/\n@@ -67,7 +72,4 @@\n-        \/\/ Make sure the minimum size is set correctly and printed\n-        \/\/ (Note: ccs size are rounded up to the next larger root chunk boundary (16m).\n-        \/\/ Note that this is **reserved** size and does not affect rss.\n-        pb = ProcessTools.createJavaProcessBuilder(\"-XX:+UnlockDiagnosticVMOptions\",\n-                                                   \"-XX:CompressedClassSpaceSize=1m\",\n-                                                   \"-Xlog:gc+metaspace=trace\",\n-                                                   \"-version\");\n+        \/\/ Try 0. Same result expected.\n+        pb = ProcessTools.createJavaProcessBuilder(\"-XX:CompressedClassSpaceSize=0\",\n+                \"-Xlog:gc+metaspace=trace\",\n+                \"-version\");\n@@ -75,2 +77,2 @@\n-        output.shouldMatch(\"Compressed class space.*16777216\")\n-              .shouldHaveExitValue(0);\n+        output.shouldMatch(\"Compressed class space.*\" + min_class_space_size)\n+                .shouldHaveExitValue(0);\n@@ -78,0 +80,1 @@\n+        \/\/\/\/\/\/\/\/\/\/\/\n@@ -79,1 +82,1 @@\n-        \/\/ Make sure the maximum size is set correctly and printed\n+        \/\/ Try max allowed size, which should be accepted\n@@ -81,1 +84,1 @@\n-                                                   \"-XX:CompressedClassSpaceSize=3g\",\n+                                                   \"-XX:CompressedClassSpaceSize=\" + max_class_space_size,\n@@ -85,1 +88,1 @@\n-        output.shouldMatch(\"Compressed class space.*3221225472\")\n+        output.shouldMatch(\"Compressed class space.*\" + max_class_space_size)\n@@ -88,0 +91,1 @@\n+        \/\/\/\/\/\/\/\/\/\/\/\n@@ -89,3 +93,5 @@\n-        pb = ProcessTools.createJavaProcessBuilder(\"-XX:-UseCompressedClassPointers\",\n-                                                   \"-XX:CompressedClassSpaceSize=1m\",\n-                                                   \"-version\");\n+        \/\/ Set max allowed size + 1, which should graciously fail\n+        pb = ProcessTools.createJavaProcessBuilder(\"-XX:+UnlockDiagnosticVMOptions\",\n+                \"-XX:CompressedClassSpaceSize=\" + (max_class_space_size + 1),\n+                \"-Xlog:gc+metaspace=trace\",\n+                \"-version\");\n@@ -93,2 +99,3 @@\n-        output.shouldContain(\"Setting CompressedClassSpaceSize has no effect when compressed class pointers are not used\")\n-              .shouldHaveExitValue(0);\n+        output.shouldContain(\"CompressedClassSpaceSize \" + (max_class_space_size + 1) + \" too large (max: \" + max_class_space_size)\n+              .shouldHaveExitValue(1);\n+\n","filename":"test\/hotspot\/jtreg\/runtime\/CompressedOops\/CompressedClassSpaceSize.java","additions":34,"deletions":27,"binary":false,"changes":61,"status":"modified"},{"patch":"@@ -0,0 +1,83 @@\n+\/*\n+ * Copyright (c) 2022, Oracle and\/or its affiliates. All rights reserved.\n+ * DO NOT ALTER OR REMOVE COPYRIGHT NOTICES OR THIS FILE HEADER.\n+ *\n+ * This code is free software; you can redistribute it and\/or modify it\n+ * under the terms of the GNU General Public License version 2 only, as\n+ * published by the Free Software Foundation.\n+ *\n+ * This code is distributed in the hope that it will be useful, but WITHOUT\n+ * ANY WARRANTY; without even the implied warranty of MERCHANTABILITY or\n+ * FITNESS FOR A PARTICULAR PURPOSE.  See the GNU General Public License\n+ * version 2 for more details (a copy is included in the LICENSE file that\n+ * accompanied this code).\n+ *\n+ * You should have received a copy of the GNU General Public License version\n+ * 2 along with this work; if not, write to the Free Software Foundation,\n+ * Inc., 51 Franklin St, Fifth Floor, Boston, MA 02110-1301 USA.\n+ *\n+ * Please contact Oracle, 500 Oracle Parkway, Redwood Shores, CA 94065 USA\n+ * or visit www.oracle.com if you need additional information or have any\n+ * questions.\n+ *\/\n+\n+\/*\n+ * @test id=default\n+ * @library \/test\/lib\n+ * @modules java.base\/jdk.internal.misc\n+ *          java.management\n+ * @run main\/othervm BaseOffsets\n+ *\/\n+\/*\n+ * @test id=no-coops\n+ * @library \/test\/lib\n+ * @requires vm.bits == \"64\"\n+ * @modules java.base\/jdk.internal.misc\n+ *          java.management\n+ * @run main\/othervm -XX:-UseCompressedOops BaseOffsets\n+ *\/\n+\n+import java.lang.reflect.Field;\n+import java.util.Arrays;\n+import java.util.Comparator;\n+import jdk.internal.misc.Unsafe;\n+\n+import jdk.test.lib.Asserts;\n+import jdk.test.lib.Platform;\n+\n+public class BaseOffsets {\n+\n+    static class LIClass {\n+        public int i;\n+    }\n+\n+    \/\/ @0:  8 byte header,  @8: int field\n+    static final long INT_OFFSET  = 8L;\n+\n+    static public void main(String[] args) {\n+        Unsafe unsafe = Unsafe.getUnsafe();\n+        Class c = LIClass.class;\n+        Field[] fields = c.getFields();\n+        for (int i = 0; i < fields.length; i++) {\n+            long offset = unsafe.objectFieldOffset(fields[i]);\n+            if (fields[i].getType() == int.class) {\n+                Asserts.assertEquals(offset, INT_OFFSET, \"Misplaced int field\");\n+            } else {\n+                Asserts.fail(\"Unexpected field type\");\n+            }\n+        }\n+\n+        Asserts.assertEquals(unsafe.arrayBaseOffset(boolean[].class), 12, \"Misplaced boolean array base\");\n+        Asserts.assertEquals(unsafe.arrayBaseOffset(byte[].class),    12, \"Misplaced byte    array base\");\n+        Asserts.assertEquals(unsafe.arrayBaseOffset(char[].class),    12, \"Misplaced char    array base\");\n+        Asserts.assertEquals(unsafe.arrayBaseOffset(short[].class),   12, \"Misplaced short   array base\");\n+        Asserts.assertEquals(unsafe.arrayBaseOffset(int[].class),     12, \"Misplaced int     array base\");\n+        Asserts.assertEquals(unsafe.arrayBaseOffset(long[].class),    16, \"Misplaced long    array base\");\n+        Asserts.assertEquals(unsafe.arrayBaseOffset(float[].class),   12, \"Misplaced float   array base\");\n+        Asserts.assertEquals(unsafe.arrayBaseOffset(double[].class),  16, \"Misplaced double  array base\");\n+        boolean narrowOops = System.getProperty(\"java.vm.compressedOopsMode\") != null ||\n+                             !Platform.is64bit();\n+        int expected_objary_offset = narrowOops ? 12 : 16;\n+        Asserts.assertEquals(unsafe.arrayBaseOffset(Object[].class),  expected_objary_offset, \"Misplaced object  array base\");\n+    }\n+}\n","filename":"test\/hotspot\/jtreg\/runtime\/FieldLayout\/BaseOffsets.java","additions":83,"deletions":0,"binary":false,"changes":83,"status":"added"},{"patch":"@@ -41,1 +41,0 @@\n- * @run main\/othervm -XX:+UseCompressedOops -XX:-UseCompressedClassPointers FieldDensityTest\n","filename":"test\/hotspot\/jtreg\/runtime\/FieldLayout\/FieldDensityTest.java","additions":0,"deletions":1,"binary":false,"changes":1,"status":"modified"},{"patch":"@@ -61,2 +61,2 @@\n-    static final long INT_OFFSET  = Platform.is64bit() ? 12L : 16L;\n-    static final long LONG_OFFSET = Platform.is64bit() ? 16L :  8L;\n+    static final long INT_OFFSET  = 16L;\n+    static final long LONG_OFFSET = 8L;\n","filename":"test\/hotspot\/jtreg\/runtime\/FieldLayout\/OldLayoutCheck.java","additions":2,"deletions":2,"binary":false,"changes":4,"status":"modified"},{"patch":"@@ -60,20 +60,0 @@\n-\/*\n- * @test id=test-64bit-noccs\n- * @summary Test the VM.metaspace command\n- * @requires vm.bits == \"64\"\n- * @library \/test\/lib\n- * @modules java.base\/jdk.internal.misc\n- *          java.management\n- * @run main\/othervm -Dwithout-compressed-class-space -XX:MaxMetaspaceSize=201M -Xmx100M -XX:-UseCompressedOops -XX:-UseCompressedClassPointers PrintMetaspaceDcmd\n- *\/\n-\n- \/*\n- * @test id=test-nospecified\n- * @summary Test the VM.metaspace command\n- * @requires vm.bits == \"64\"\n- * @library \/test\/lib\n- * @modules java.base\/jdk.internal.misc\n- *          java.management\n- * @run main\/othervm -Dno-specified-flag -Xmx100M -XX:-UseCompressedOops -XX:-UseCompressedClassPointers PrintMetaspaceDcmd\n- *\/\n-\n","filename":"test\/hotspot\/jtreg\/runtime\/Metaspace\/PrintMetaspaceDcmd.java","additions":0,"deletions":20,"binary":false,"changes":20,"status":"modified"},{"patch":"@@ -48,1 +48,1 @@\n-      processArgs.add(\"-XX:MaxMetaspaceSize=3m\");\n+      processArgs.add(\"-XX:MaxMetaspaceSize=2m\");\n","filename":"test\/hotspot\/jtreg\/runtime\/cds\/MaxMetaspaceSize.java","additions":1,"deletions":1,"binary":false,"changes":2,"status":"modified"},{"patch":"@@ -70,2 +70,0 @@\n-        testTable.add( new TestVector(\"-XX:+UseCompressedClassPointers\", \"-XX:-UseCompressedClassPointers\",\n-           \"The saved state of UseCompressedOops and UseCompressedClassPointers is different from runtime, CDS will be disabled.\", 1) );\n","filename":"test\/hotspot\/jtreg\/runtime\/cds\/appcds\/CommandLineFlagComboNegative.java","additions":0,"deletions":2,"binary":false,"changes":2,"status":"modified"},{"patch":"@@ -49,1 +49,0 @@\n-        public boolean useCompressedClassPointers;   \/\/ UseCompressedClassPointers\n@@ -52,1 +51,1 @@\n-        public ConfArg(boolean useCompressedOops, boolean useCompressedClassPointers, String msg, int code) {\n+        public ConfArg(boolean useCompressedOops, String msg, int code) {\n@@ -54,1 +53,0 @@\n-            this.useCompressedClassPointers = useCompressedClassPointers;\n@@ -69,1 +67,1 @@\n-            *          UseCompressedOops   UseCompressedClassPointers  Result\n+            *          UseCompressedOops   Result\n@@ -71,5 +69,3 @@\n-            *    dump: on                  on\n-            *    test: on                  on                          Pass\n-            *          on                  off                         Fail\n-            *          off                 on                          Fail\n-            *          off                 off                         Fail\n+            *    dump: on\n+            *    test: on                  Pass\n+            *          off                 Fail\n@@ -77,15 +73,3 @@\n-            *    dump: on                  off\n-            *    test: on                  off                         Pass\n-            *          on                  on                          Fail\n-            *          off                 on                          Pass\n-            *          off                 off                         Fail\n-            *    3.\n-            *    dump: off                 on\n-            *    test: off                 on                          Pass\n-            *          on                  on                          Fail\n-            *          on                  off                         Fail\n-            *    4.\n-            *    dump: off                 off\n-            *    test: off                 off                         Pass\n-            *          on                  on                          Fail\n-            *          on                  off                         Fail\n+            *    dump: off\n+            *    test: off                 Pass\n+            *          on                  Fail\n@@ -94,1 +78,1 @@\n-            if (dumpArg.useCompressedOops && dumpArg.useCompressedClassPointers) {\n+            if (dumpArg.useCompressedOops) {\n@@ -96,1 +80,1 @@\n-                    .add(new ConfArg(true, true, HELLO_STRING, PASS));\n+                    .add(new ConfArg(true, HELLO_STRING, PASS));\n@@ -98,15 +82,1 @@\n-                    .add(new ConfArg(true, false, EXEC_ABNORMAL_MSG, FAIL));\n-                execArgs\n-                    .add(new ConfArg(false, true, EXEC_ABNORMAL_MSG, FAIL));\n-                execArgs\n-                    .add(new ConfArg(false, false, EXEC_ABNORMAL_MSG, FAIL));\n-\n-            }  else if(dumpArg.useCompressedOops && !dumpArg.useCompressedClassPointers) {\n-                execArgs\n-                    .add(new ConfArg(true, false, HELLO_STRING, PASS));\n-                execArgs\n-                    .add(new ConfArg(true, true, EXEC_ABNORMAL_MSG, FAIL));\n-                execArgs\n-                    .add(new ConfArg(false, true, EXEC_ABNORMAL_MSG, FAIL));\n-                execArgs\n-                    .add(new ConfArg(false, false, EXEC_ABNORMAL_MSG, FAIL));\n+                    .add(new ConfArg(false, EXEC_ABNORMAL_MSG, FAIL));\n@@ -114,3 +84,1 @@\n-            } else if (!dumpArg.useCompressedOops && dumpArg.useCompressedClassPointers) {\n-                execArgs\n-                    .add(new ConfArg(false, true, HELLO_STRING, PASS));\n+            } else if (!dumpArg.useCompressedOops) {\n@@ -118,1 +86,1 @@\n-                    .add(new ConfArg(true, true, EXEC_ABNORMAL_MSG, FAIL));\n+                    .add(new ConfArg(false, HELLO_STRING, PASS));\n@@ -120,8 +88,1 @@\n-                    .add(new ConfArg(true, false, EXEC_ABNORMAL_MSG, FAIL));\n-            } else if (!dumpArg.useCompressedOops && !dumpArg.useCompressedClassPointers) {\n-                execArgs\n-                    .add(new ConfArg(false, false, HELLO_STRING, PASS));\n-                execArgs\n-                    .add(new ConfArg(true, true, EXEC_ABNORMAL_MSG, FAIL));\n-                execArgs\n-                    .add(new ConfArg(true, false, EXEC_ABNORMAL_MSG, FAIL));\n+                    .add(new ConfArg(true, EXEC_ABNORMAL_MSG, FAIL));\n@@ -137,5 +98,0 @@\n-    public static String getCompressedClassPointersArg(boolean on) {\n-        if (on) return \"-XX:+UseCompressedClassPointers\";\n-        else    return \"-XX:-UseCompressedClassPointers\";\n-    }\n-\n@@ -147,5 +103,1 @@\n-            .add(new RunArg(new ConfArg(true, true, null, PASS)));\n-        runList\n-            .add(new RunArg(new ConfArg(true, false, null, PASS)));\n-        runList\n-            .add(new RunArg(new ConfArg(false, true, null, PASS)));\n+            .add(new RunArg(new ConfArg(true, null, PASS)));\n@@ -153,1 +105,1 @@\n-            .add(new RunArg(new ConfArg(false, false, null, PASS)));\n+            .add(new RunArg(new ConfArg(false, null, PASS)));\n@@ -165,1 +117,0 @@\n-                      getCompressedClassPointersArg(t.dumpArg.useCompressedClassPointers),\n@@ -178,1 +129,0 @@\n-                                      getCompressedClassPointersArg(c.useCompressedClassPointers),\n","filename":"test\/hotspot\/jtreg\/runtime\/cds\/appcds\/TestCombinedCompressedFlags.java","additions":16,"deletions":66,"binary":false,"changes":82,"status":"modified"},{"patch":"@@ -75,13 +75,1 @@\n-         System.out.println(\"3. Run with -UseCompressedOops -UseCompressedClassPointers\");\n-         out = TestCommon\n-                   .exec(helloJar,\n-                         \"-XX:+UseSerialGC\",\n-                         \"-XX:-UseCompressedOops\",\n-                         \"-XX:-UseCompressedClassPointers\",\n-                         \"-Xlog:cds\",\n-                         \"Hello\");\n-         out.shouldContain(UNABLE_TO_USE_ARCHIVE);\n-         out.shouldContain(ERR_MSG);\n-         out.shouldHaveExitValue(1);\n-\n-         System.out.println(\"4. Run with -UseCompressedOops +UseCompressedClassPointers\");\n+         System.out.println(\"3. Run with -UseCompressedOops +UseCompressedClassPointers\");\n@@ -98,13 +86,1 @@\n-         System.out.println(\"5. Run with +UseCompressedOops -UseCompressedClassPointers\");\n-         out = TestCommon\n-                   .exec(helloJar,\n-                         \"-XX:+UseSerialGC\",\n-                         \"-XX:+UseCompressedOops\",\n-                         \"-XX:-UseCompressedClassPointers\",\n-                         \"-Xlog:cds\",\n-                         \"Hello\");\n-         out.shouldContain(UNABLE_TO_USE_ARCHIVE);\n-         out.shouldContain(ERR_MSG);\n-         out.shouldHaveExitValue(1);\n-\n-         System.out.println(\"6. Run with +UseCompressedOops +UseCompressedClassPointers\");\n+         System.out.println(\"4. Run with +UseCompressedOops +UseCompressedClassPointers\");\n@@ -122,12 +98,1 @@\n-         System.out.println(\"7. Dump with -UseCompressedOops -UseCompressedClassPointers\");\n-         out = TestCommon\n-                   .dump(helloJar,\n-                         new String[] {\"Hello\"},\n-                         \"-XX:+UseSerialGC\",\n-                         \"-XX:-UseCompressedOops\",\n-                         \"-XX:+UseCompressedClassPointers\",\n-                         \"-Xlog:cds\");\n-         out.shouldContain(\"Dumping shared data to file:\");\n-         out.shouldHaveExitValue(0);\n-\n-         System.out.println(\"8. Run with ZGC\");\n+         System.out.println(\"5. Run with ZGC\");\n","filename":"test\/hotspot\/jtreg\/runtime\/cds\/appcds\/TestZGCWithCDS.java","additions":3,"deletions":38,"binary":false,"changes":41,"status":"modified"},{"patch":"@@ -101,1 +101,1 @@\n-                       Platform.is64bit() ? 549755813632L: 4294967168L);\n+                       4294967168L);\n","filename":"test\/hotspot\/jtreg\/serviceability\/sa\/ClhsdbLongConstant.java","additions":1,"deletions":1,"binary":false,"changes":2,"status":"modified"},{"patch":"@@ -134,2 +134,1 @@\n-        runCheck(new String[] {\"-XX:+IgnoreUnrecognizedVMOptions\", \"-XX:-UseCompressedClassPointers\"},\n-                 BadFailOnConstraint.create(Loads.class, \"load()\", 1, 1, \"Load\"),\n+        runCheck(BadFailOnConstraint.create(Loads.class, \"load()\", 1, 1, \"Load\"),\n","filename":"test\/hotspot\/jtreg\/testlibrary_tests\/ir_framework\/tests\/TestIRMatching.java","additions":1,"deletions":2,"binary":false,"changes":3,"status":"modified"},{"patch":"@@ -375,1 +375,1 @@\n-        long expected = roundUp(Platform.is64bit() ? 16 : 8, OBJ_ALIGN);\n+        long expected = roundUp(8, OBJ_ALIGN);\n@@ -382,1 +382,1 @@\n-        long expected = roundUp(Platform.is64bit() ? 16 : 8, OBJ_ALIGN);\n+        long expected = roundUp(8, OBJ_ALIGN);\n@@ -392,1 +392,1 @@\n-        long expected = roundUp(Platform.is64bit() ? 16 : 8, OBJ_ALIGN);\n+        long expected = roundUp(8, OBJ_ALIGN);\n","filename":"test\/jdk\/java\/lang\/instrument\/GetObjectSizeIntrinsicsTest.java","additions":3,"deletions":3,"binary":false,"changes":6,"status":"modified"},{"patch":"@@ -73,1 +73,2 @@\n-        int objectHeaderSize = bytesPerWord * 3; \/\/ length will be aligned on 64 bits\n+        \/\/ length will be in klass-gap on 64 bits, extra field on 32 bits.\n+        int objectHeaderSize = bytesPerWord * (runsOn32Bit ? 3 : 2);\n","filename":"test\/jdk\/jdk\/jfr\/event\/gc\/objectcount\/ObjectCountEventVerifier.java","additions":2,"deletions":1,"binary":false,"changes":3,"status":"modified"},{"patch":"@@ -62,0 +62,3 @@\n+\n+\n+jdk\/jshell\/ToolTabSnippetTest.java 1234567 generic-all\n","filename":"test\/langtools\/ProblemList.txt","additions":3,"deletions":0,"binary":false,"changes":3,"status":"modified"}]}