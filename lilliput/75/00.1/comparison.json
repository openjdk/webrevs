{"files":[{"patch":"@@ -1787,0 +1787,12 @@\n+  int max_monitors = C->method() != NULL ? C->max_monitors() : 0;\n+  if (UseFastLocking && max_monitors > 0) {\n+    C2CheckLockStackStub* stub = new (C->comp_arena()) C2CheckLockStackStub();\n+    C->output()->add_stub(stub);\n+    __ ldr(r9, Address(rthread, JavaThread::lock_stack_current_offset()));\n+    __ ldr(r10, Address(rthread, JavaThread::lock_stack_limit_offset()));\n+    __ add(r9, r9, max_monitors * oopSize);\n+    __ cmp(r9, r10);\n+    __ br(Assembler::GE, stub->entry());\n+    __ bind(stub->continuation());\n+  }\n+\n@@ -3773,1 +3785,1 @@\n-    Label no_count;\n+    Label count, no_count;\n@@ -3791,14 +3803,2 @@\n-      \/\/ Set tmp to be (markWord of object | UNLOCK_VALUE).\n-      __ orr(tmp, disp_hdr, markWord::unlocked_value);\n-\n-      \/\/ Initialize the box. (Must happen before we update the object mark!)\n-      __ str(tmp, Address(box, BasicLock::displaced_header_offset_in_bytes()));\n-\n-      \/\/ Compare object markWord with an unlocked value (tmp) and if\n-      \/\/ equal exchange the stack address of our box with object markWord.\n-      \/\/ On failure disp_hdr contains the possibly locked markWord.\n-      __ cmpxchg(oop, tmp, box, Assembler::xword, \/*acquire*\/ true,\n-                 \/*release*\/ true, \/*weak*\/ false, disp_hdr);\n-      __ br(Assembler::EQ, cont);\n-\n-      assert(oopDesc::mark_offset_in_bytes() == 0, \"offset of _mark is not 0\");\n+      if (UseFastLocking) {\n+        __ fast_lock(oop, disp_hdr, tmp, rscratch1, no_count, false);\n@@ -3806,12 +3806,33 @@\n-      \/\/ If the compare-and-exchange succeeded, then we found an unlocked\n-      \/\/ object, will have now locked it will continue at label cont\n-\n-      \/\/ Check if the owner is self by comparing the value in the\n-      \/\/ markWord of object (disp_hdr) with the stack pointer.\n-      __ mov(rscratch1, sp);\n-      __ sub(disp_hdr, disp_hdr, rscratch1);\n-      __ mov(tmp, (address) (~(os::vm_page_size()-1) | markWord::lock_mask_in_place));\n-      \/\/ If condition is true we are cont and hence we can store 0 as the\n-      \/\/ displaced header in the box, which indicates that it is a recursive lock.\n-      __ ands(tmp\/*==0?*\/, disp_hdr, tmp);   \/\/ Sets flags for result\n-      __ str(tmp\/*==0, perhaps*\/, Address(box, BasicLock::displaced_header_offset_in_bytes()));\n+        \/\/ Indicate success at cont.\n+        __ cmp(oop, oop);\n+        __ b(count);\n+      } else {\n+        \/\/ Set tmp to be (markWord of object | UNLOCK_VALUE).\n+        __ orr(tmp, disp_hdr, markWord::unlocked_value);\n+\n+        \/\/ Initialize the box. (Must happen before we update the object mark!)\n+        __ str(tmp, Address(box, BasicLock::displaced_header_offset_in_bytes()));\n+\n+        \/\/ Compare object markWord with an unlocked value (tmp) and if\n+        \/\/ equal exchange the stack address of our box with object markWord.\n+        \/\/ On failure disp_hdr contains the possibly locked markWord.\n+        __ cmpxchg(oop, tmp, box, Assembler::xword, \/*acquire*\/ true,\n+                   \/*release*\/ true, \/*weak*\/ false, disp_hdr);\n+        __ br(Assembler::EQ, cont);\n+\n+        assert(oopDesc::mark_offset_in_bytes() == 0, \"offset of _mark is not 0\");\n+\n+        \/\/ If the compare-and-exchange succeeded, then we found an unlocked\n+        \/\/ object, will have now locked it will continue at label cont\n+\n+        \/\/ Check if the owner is self by comparing the value in the\n+        \/\/ markWord of object (disp_hdr) with the stack pointer.\n+        __ mov(rscratch1, sp);\n+        __ sub(disp_hdr, disp_hdr, rscratch1);\n+        __ mov(tmp, (address) (~(os::vm_page_size()-1) | markWord::lock_mask_in_place));\n+        \/\/ If condition is true we are cont and hence we can store 0 as the\n+        \/\/ displaced header in the box, which indicates that it is a recursive lock.\n+        __ ands(tmp\/*==0?*\/, disp_hdr, tmp);   \/\/ Sets flags for result\n+        __ str(tmp\/*==0, perhaps*\/, Address(box, BasicLock::displaced_header_offset_in_bytes()));\n+        __ b(cont);\n+      }\n@@ -3820,0 +3841,1 @@\n+      __ b(cont);\n@@ -3821,1 +3843,0 @@\n-    __ b(cont);\n@@ -3834,7 +3855,8 @@\n-    \/\/ Store a non-null value into the box to avoid looking like a re-entrant\n-    \/\/ lock. The fast-path monitor unlock code checks for\n-    \/\/ markWord::monitor_value so use markWord::unused_mark which has the\n-    \/\/ relevant bit set, and also matches ObjectSynchronizer::enter.\n-    __ mov(tmp, (address)markWord::unused_mark().value());\n-    __ str(tmp, Address(box, BasicLock::displaced_header_offset_in_bytes()));\n-\n+    if (!UseFastLocking) {\n+      \/\/ Store a non-null value into the box to avoid looking like a re-entrant\n+      \/\/ lock. The fast-path monitor unlock code checks for\n+      \/\/ markWord::monitor_value so use markWord::unused_mark which has the\n+      \/\/ relevant bit set, and also matches ObjectSynchronizer::enter.\n+      __ mov(tmp, (address)markWord::unused_mark().value());\n+      __ str(tmp, Address(box, BasicLock::displaced_header_offset_in_bytes()));\n+    }\n@@ -3855,0 +3877,1 @@\n+    __ bind(count);\n@@ -3868,1 +3891,1 @@\n-    Label no_count;\n+    Label count, no_count;\n@@ -3872,1 +3895,1 @@\n-    if (!UseHeavyMonitors) {\n+    if (!UseHeavyMonitors && !UseFastLocking) {\n@@ -3886,3 +3909,10 @@\n-      \/\/ Check if it is still a light weight lock, this is is true if we\n-      \/\/ see the stack address of the basicLock in the markWord of the\n-      \/\/ object.\n+      if (UseFastLocking) {\n+        __ fast_unlock(oop, tmp, box, disp_hdr, no_count);\n+\n+        \/\/ Indicate success at cont.\n+        __ cmp(oop, oop);\n+        __ b(count);\n+      } else {\n+        \/\/ Check if it is still a light weight lock, this is is true if we\n+        \/\/ see the stack address of the basicLock in the markWord of the\n+        \/\/ object.\n@@ -3890,2 +3920,4 @@\n-      __ cmpxchg(oop, box, disp_hdr, Assembler::xword, \/*acquire*\/ false,\n-                 \/*release*\/ true, \/*weak*\/ false, tmp);\n+        __ cmpxchg(oop, box, disp_hdr, Assembler::xword, \/*acquire*\/ false,\n+                   \/*release*\/ true, \/*weak*\/ false, tmp);\n+        __ b(cont);\n+      }\n@@ -3894,0 +3926,1 @@\n+      __ b(cont);\n@@ -3895,1 +3928,0 @@\n-    __ b(cont);\n@@ -3903,0 +3935,11 @@\n+\n+    if (UseFastLocking) {\n+      \/\/ If the owner is anonymous, we need to fix it -- in the slow-path.\n+      __ ldr(disp_hdr, Address(tmp, ObjectMonitor::owner_offset_in_bytes()));\n+      \/\/ We cannot use tbnz here: tbnz would leave the condition flags untouched,\n+      \/\/ but we want to carry-over the NE condition to the exit at the cont label,\n+      \/\/ in order to take the slow-path.\n+      __ tst(disp_hdr, (uint64_t)(intptr_t) ANONYMOUS_OWNER);\n+      __ br(Assembler::NE, no_count);\n+    }\n+\n@@ -3929,0 +3972,1 @@\n+    __ bind(count);\n@@ -7191,1 +7235,1 @@\n-instruct loadNKlass(iRegNNoSp dst, memory4 mem)\n+instruct loadNKlass(iRegNNoSp dst, memory4 mem, rFlagsReg cr)\n@@ -7194,0 +7238,1 @@\n+  effect(TEMP_DEF dst, KILL cr);\n@@ -7198,4 +7243,16 @@\n-\n-  ins_encode(aarch64_enc_ldrw(dst, mem));\n-\n-  ins_pipe(iload_reg_mem);\n+  ins_encode %{\n+    assert($mem$$disp == oopDesc::klass_offset_in_bytes(), \"expect correct offset\");\n+    assert($mem$$index$$Register == noreg, \"expect no index\");\n+    Register dst = $dst$$Register;\n+    Register obj = $mem$$base$$Register;\n+    C2LoadNKlassStub* stub = new (Compile::current()->comp_arena()) C2LoadNKlassStub(dst);\n+    Compile::current()->output()->add_stub(stub);\n+    __ ldr(dst, Address(obj, oopDesc::mark_offset_in_bytes()));\n+    \/\/ NOTE: We can't use tbnz here, because the target is sometimes too far away\n+    \/\/ and cannot be encoded.\n+    __ tst(dst, markWord::monitor_value);\n+    __ br(Assembler::NE, stub->entry());\n+    __ bind(stub->continuation());\n+    __ lsr(dst, dst, markWord::klass_shift);\n+  %}\n+  ins_pipe(pipe_slow);\n","filename":"src\/hotspot\/cpu\/aarch64\/aarch64.ad","additions":105,"deletions":48,"binary":false,"changes":153,"status":"modified"},{"patch":"@@ -35,0 +35,1 @@\n+#include \"runtime\/objectMonitor.hpp\"\n@@ -236,0 +237,6 @@\n+void LoadKlassStub::emit_code(LIR_Assembler* ce) {\n+  __ bind(_entry);\n+  Register d = _result->as_register();\n+  __ ldr(d, Address(d, OM_OFFSET_NO_MONITOR_VALUE_TAG(header)));\n+  __ b(_continuation);\n+}\n","filename":"src\/hotspot\/cpu\/aarch64\/c1_CodeStubs_aarch64.cpp","additions":7,"deletions":0,"binary":false,"changes":7,"status":"modified"},{"patch":"@@ -86,31 +86,35 @@\n-  \/\/ and mark it as unlocked\n-  orr(hdr, hdr, markWord::unlocked_value);\n-  \/\/ save unlocked object header into the displaced header location on the stack\n-  str(hdr, Address(disp_hdr, 0));\n-  \/\/ test if object header is still the same (i.e. unlocked), and if so, store the\n-  \/\/ displaced header address in the object header - if it is not the same, get the\n-  \/\/ object header instead\n-  lea(rscratch2, Address(obj, hdr_offset));\n-  cmpxchgptr(hdr, disp_hdr, rscratch2, rscratch1, done, \/*fallthough*\/NULL);\n-  \/\/ if the object header was the same, we're done\n-  \/\/ if the object header was not the same, it is now in the hdr register\n-  \/\/ => test if it is a stack pointer into the same stack (recursive locking), i.e.:\n-  \/\/\n-  \/\/ 1) (hdr & aligned_mask) == 0\n-  \/\/ 2) sp <= hdr\n-  \/\/ 3) hdr <= sp + page_size\n-  \/\/\n-  \/\/ these 3 tests can be done by evaluating the following expression:\n-  \/\/\n-  \/\/ (hdr - sp) & (aligned_mask - page_size)\n-  \/\/\n-  \/\/ assuming both the stack pointer and page_size have their least\n-  \/\/ significant 2 bits cleared and page_size is a power of 2\n-  mov(rscratch1, sp);\n-  sub(hdr, hdr, rscratch1);\n-  ands(hdr, hdr, aligned_mask - (int)os::vm_page_size());\n-  \/\/ for recursive locking, the result is zero => save it in the displaced header\n-  \/\/ location (NULL in the displaced hdr location indicates recursive locking)\n-  str(hdr, Address(disp_hdr, 0));\n-  \/\/ otherwise we don't care about the result and handle locking via runtime call\n-  cbnz(hdr, slow_case);\n+  if (UseFastLocking) {\n+    fast_lock(obj, hdr, rscratch1, rscratch2, slow_case, false);\n+  } else {\n+    \/\/ and mark it as unlocked\n+    orr(hdr, hdr, markWord::unlocked_value);\n+    \/\/ save unlocked object header into the displaced header location on the stack\n+    str(hdr, Address(disp_hdr, 0));\n+    \/\/ test if object header is still the same (i.e. unlocked), and if so, store the\n+    \/\/ displaced header address in the object header - if it is not the same, get the\n+    \/\/ object header instead\n+    lea(rscratch2, Address(obj, hdr_offset));\n+    cmpxchgptr(hdr, disp_hdr, rscratch2, rscratch1, done, \/*fallthough*\/NULL);\n+    \/\/ if the object header was the same, we're done\n+    \/\/ if the object header was not the same, it is now in the hdr register\n+    \/\/ => test if it is a stack pointer into the same stack (recursive locking), i.e.:\n+    \/\/\n+    \/\/ 1) (hdr & aligned_mask) == 0\n+    \/\/ 2) sp <= hdr\n+    \/\/ 3) hdr <= sp + page_size\n+    \/\/\n+    \/\/ these 3 tests can be done by evaluating the following expression:\n+    \/\/\n+    \/\/ (hdr - sp) & (aligned_mask - page_size)\n+    \/\/\n+    \/\/ assuming both the stack pointer and page_size have their least\n+    \/\/ significant 2 bits cleared and page_size is a power of 2\n+    mov(rscratch1, sp);\n+    sub(hdr, hdr, rscratch1);\n+    ands(hdr, hdr, aligned_mask - (int)os::vm_page_size());\n+    \/\/ for recursive locking, the result is zero => save it in the displaced header\n+    \/\/ location (NULL in the displaced hdr location indicates recursive locking)\n+    str(hdr, Address(disp_hdr, 0));\n+    \/\/ otherwise we don't care about the result and handle locking via runtime call\n+    cbnz(hdr, slow_case);\n+  }\n@@ -130,5 +134,8 @@\n-  \/\/ load displaced header\n-  ldr(hdr, Address(disp_hdr, 0));\n-  \/\/ if the loaded hdr is NULL we had recursive locking\n-  \/\/ if we had recursive locking, we are done\n-  cbz(hdr, done);\n+  if (!UseFastLocking) {\n+    \/\/ load displaced header\n+    ldr(hdr, Address(disp_hdr, 0));\n+    \/\/ if the loaded hdr is NULL we had recursive locking\n+    \/\/ if we had recursive locking, we are done\n+    cbz(hdr, done);\n+  }\n+\n@@ -138,8 +145,4 @@\n-  \/\/ test if object header is pointing to the displaced header, and if so, restore\n-  \/\/ the displaced header in the object - if the object header is not pointing to\n-  \/\/ the displaced header, get the object header instead\n-  \/\/ if the object header was not pointing to the displaced header,\n-  \/\/ we do unlocking via runtime call\n-  if (hdr_offset) {\n-    lea(rscratch1, Address(obj, hdr_offset));\n-    cmpxchgptr(disp_hdr, hdr, rscratch1, rscratch2, done, &slow_case);\n+\n+  if (UseFastLocking) {\n+    ldr(hdr, Address(obj, oopDesc::mark_offset_in_bytes()));\n+    fast_unlock(obj, hdr, rscratch1, rscratch2, slow_case);\n@@ -147,1 +150,13 @@\n-    cmpxchgptr(disp_hdr, hdr, obj, rscratch2, done, &slow_case);\n+    \/\/ test if object header is pointing to the displaced header, and if so, restore\n+    \/\/ the displaced header in the object - if the object header is not pointing to\n+    \/\/ the displaced header, get the object header instead\n+    \/\/ if the object header was not pointing to the displaced header,\n+    \/\/ we do unlocking via runtime call\n+    if (hdr_offset) {\n+      lea(rscratch1, Address(obj, hdr_offset));\n+      cmpxchgptr(disp_hdr, hdr, rscratch1, rscratch2, done, &slow_case);\n+    } else {\n+      cmpxchgptr(disp_hdr, hdr, obj, rscratch2, done, &slow_case);\n+    }\n+    \/\/ done\n+    bind(done);\n@@ -149,2 +164,0 @@\n-  \/\/ done\n-  bind(done);\n@@ -166,2 +179,1 @@\n-  \/\/ This assumes that all prototype bits fit in an int32_t\n-  mov(t1, (int32_t)(intptr_t)markWord::prototype().value());\n+  ldr(t1, Address(klass, Klass::prototype_header_offset()));\n@@ -170,7 +182,0 @@\n-  if (UseCompressedClassPointers) { \/\/ Take care not to kill klass\n-    encode_klass_not_null(t1, klass);\n-    strw(t1, Address(obj, oopDesc::klass_offset_in_bytes()));\n-  } else {\n-    str(klass, Address(obj, oopDesc::klass_offset_in_bytes()));\n-  }\n-\n@@ -179,2 +184,0 @@\n-  } else if (UseCompressedClassPointers) {\n-    store_klass_gap(obj, zr);\n@@ -198,0 +201,6 @@\n+  \/\/ Zero first 4 bytes, if start offset is not word aligned.\n+  if (!is_aligned(hdr_size_in_bytes, BytesPerWord)) {\n+    strw(zr, Address(obj, hdr_size_in_bytes));\n+    hdr_size_in_bytes += BytesPerInt;\n+  }\n+\n@@ -257,1 +266,1 @@\n-void C1_MacroAssembler::allocate_array(Register obj, Register len, Register t1, Register t2, int header_size, int f, Register klass, Label& slow_case) {\n+void C1_MacroAssembler::allocate_array(Register obj, Register len, Register t1, Register t2, int base_offset_in_bytes, int f, Register klass, Label& slow_case) {\n@@ -270,1 +279,1 @@\n-  mov(arr_size, (int32_t)header_size * BytesPerWord + MinObjAlignmentInBytesMask);\n+  mov(arr_size, (int32_t)base_offset_in_bytes + MinObjAlignmentInBytesMask);\n@@ -279,1 +288,1 @@\n-  initialize_body(obj, arr_size, header_size * BytesPerWord, t1, t2);\n+  initialize_body(obj, arr_size, base_offset_in_bytes, t1, t2);\n@@ -299,1 +308,1 @@\n-  assert(!MacroAssembler::needs_explicit_null_check(oopDesc::klass_offset_in_bytes()), \"must add explicit null check\");\n+  assert(!MacroAssembler::needs_explicit_null_check(oopDesc::mark_offset_in_bytes()), \"must add explicit null check\");\n@@ -305,1 +314,1 @@\n-void C1_MacroAssembler::build_frame(int framesize, int bang_size_in_bytes) {\n+void C1_MacroAssembler::build_frame(int framesize, int bang_size_in_bytes, int max_monitors) {\n@@ -312,0 +321,12 @@\n+  if (UseFastLocking && max_monitors > 0) {\n+    Label ok;\n+    ldr(r9, Address(rthread, JavaThread::lock_stack_current_offset()));\n+    ldr(r10, Address(rthread, JavaThread::lock_stack_limit_offset()));\n+    add(r9, r9, max_monitors * oopSize);\n+    cmp(r9, r10);\n+    br(Assembler::LT, ok);\n+    assert(StubRoutines::aarch64::check_lock_stack() != NULL, \"need runtime call stub\");\n+    far_call(StubRoutines::aarch64::check_lock_stack());\n+    bind(ok);\n+  }\n+\n","filename":"src\/hotspot\/cpu\/aarch64\/c1_MacroAssembler_aarch64.cpp","additions":84,"deletions":63,"binary":false,"changes":147,"status":"modified"},{"patch":"@@ -761,50 +761,55 @@\n-    \/\/ Load (object->mark() | 1) into swap_reg\n-    ldr(rscratch1, Address(obj_reg, oopDesc::mark_offset_in_bytes()));\n-    orr(swap_reg, rscratch1, 1);\n-\n-    \/\/ Save (object->mark() | 1) into BasicLock's displaced header\n-    str(swap_reg, Address(lock_reg, mark_offset));\n-\n-    assert(lock_offset == 0,\n-           \"displached header must be first word in BasicObjectLock\");\n-\n-    Label fail;\n-    cmpxchg_obj_header(swap_reg, lock_reg, obj_reg, rscratch1, count, \/*fallthrough*\/NULL);\n-\n-    \/\/ Fast check for recursive lock.\n-    \/\/\n-    \/\/ Can apply the optimization only if this is a stack lock\n-    \/\/ allocated in this thread. For efficiency, we can focus on\n-    \/\/ recently allocated stack locks (instead of reading the stack\n-    \/\/ base and checking whether 'mark' points inside the current\n-    \/\/ thread stack):\n-    \/\/  1) (mark & 7) == 0, and\n-    \/\/  2) sp <= mark < mark + os::pagesize()\n-    \/\/\n-    \/\/ Warning: sp + os::pagesize can overflow the stack base. We must\n-    \/\/ neither apply the optimization for an inflated lock allocated\n-    \/\/ just above the thread stack (this is why condition 1 matters)\n-    \/\/ nor apply the optimization if the stack lock is inside the stack\n-    \/\/ of another thread. The latter is avoided even in case of overflow\n-    \/\/ because we have guard pages at the end of all stacks. Hence, if\n-    \/\/ we go over the stack base and hit the stack of another thread,\n-    \/\/ this should not be in a writeable area that could contain a\n-    \/\/ stack lock allocated by that thread. As a consequence, a stack\n-    \/\/ lock less than page size away from sp is guaranteed to be\n-    \/\/ owned by the current thread.\n-    \/\/\n-    \/\/ These 3 tests can be done by evaluating the following\n-    \/\/ expression: ((mark - sp) & (7 - os::vm_page_size())),\n-    \/\/ assuming both stack pointer and pagesize have their\n-    \/\/ least significant 3 bits clear.\n-    \/\/ NOTE: the mark is in swap_reg %r0 as the result of cmpxchg\n-    \/\/ NOTE2: aarch64 does not like to subtract sp from rn so take a\n-    \/\/ copy\n-    mov(rscratch1, sp);\n-    sub(swap_reg, swap_reg, rscratch1);\n-    ands(swap_reg, swap_reg, (uint64_t)(7 - (int)os::vm_page_size()));\n-\n-    \/\/ Save the test result, for recursive case, the result is zero\n-    str(swap_reg, Address(lock_reg, mark_offset));\n-    br(Assembler::EQ, count);\n-\n+    if (UseFastLocking) {\n+      ldr(tmp, Address(obj_reg, oopDesc::mark_offset_in_bytes()));\n+      fast_lock(obj_reg, tmp, rscratch1, rscratch2, slow_case);\n+      b(count);\n+    } else {\n+      \/\/ Load (object->mark() | 1) into swap_reg\n+      ldr(rscratch1, Address(obj_reg, oopDesc::mark_offset_in_bytes()));\n+      orr(swap_reg, rscratch1, 1);\n+\n+      \/\/ Save (object->mark() | 1) into BasicLock's displaced header\n+      str(swap_reg, Address(lock_reg, mark_offset));\n+\n+      assert(lock_offset == 0,\n+             \"displached header must be first word in BasicObjectLock\");\n+\n+      Label fail;\n+      cmpxchg_obj_header(swap_reg, lock_reg, obj_reg, rscratch1, count, \/*fallthrough*\/NULL);\n+\n+      \/\/ Fast check for recursive lock.\n+      \/\/\n+      \/\/ Can apply the optimization only if this is a stack lock\n+      \/\/ allocated in this thread. For efficiency, we can focus on\n+      \/\/ recently allocated stack locks (instead of reading the stack\n+      \/\/ base and checking whether 'mark' points inside the current\n+      \/\/ thread stack):\n+      \/\/  1) (mark & 7) == 0, and\n+      \/\/  2) sp <= mark < mark + os::pagesize()\n+      \/\/\n+      \/\/ Warning: sp + os::pagesize can overflow the stack base. We must\n+      \/\/ neither apply the optimization for an inflated lock allocated\n+      \/\/ just above the thread stack (this is why condition 1 matters)\n+      \/\/ nor apply the optimization if the stack lock is inside the stack\n+      \/\/ of another thread. The latter is avoided even in case of overflow\n+      \/\/ because we have guard pages at the end of all stacks. Hence, if\n+      \/\/ we go over the stack base and hit the stack of another thread,\n+      \/\/ this should not be in a writeable area that could contain a\n+      \/\/ stack lock allocated by that thread. As a consequence, a stack\n+      \/\/ lock less than page size away from sp is guaranteed to be\n+      \/\/ owned by the current thread.\n+      \/\/\n+      \/\/ These 3 tests can be done by evaluating the following\n+      \/\/ expression: ((mark - sp) & (7 - os::vm_page_size())),\n+      \/\/ assuming both stack pointer and pagesize have their\n+      \/\/ least significant 3 bits clear.\n+      \/\/ NOTE: the mark is in swap_reg %r0 as the result of cmpxchg\n+      \/\/ NOTE2: aarch64 does not like to subtract sp from rn so take a\n+      \/\/ copy\n+      mov(rscratch1, sp);\n+      sub(swap_reg, swap_reg, rscratch1);\n+      ands(swap_reg, swap_reg, (uint64_t)(7 - (int)os::vm_page_size()));\n+\n+      \/\/ Save the test result, for recursive case, the result is zero\n+      str(swap_reg, Address(lock_reg, mark_offset));\n+      br(Assembler::EQ, count);\n+    }\n@@ -816,1 +821,1 @@\n-            lock_reg);\n+            UseFastLocking ? obj_reg : lock_reg);\n@@ -853,3 +858,5 @@\n-    \/\/ Convert from BasicObjectLock structure to object and BasicLock\n-    \/\/ structure Store the BasicLock address into %r0\n-    lea(swap_reg, Address(lock_reg, BasicObjectLock::lock_offset_in_bytes()));\n+    if (!UseFastLocking) {\n+      \/\/ Convert from BasicObjectLock structure to object and BasicLock\n+      \/\/ structure Store the BasicLock address into %r0\n+      lea(swap_reg, Address(lock_reg, BasicObjectLock::lock_offset_in_bytes()));\n+    }\n@@ -863,3 +870,2 @@\n-    \/\/ Load the old header from BasicLock structure\n-    ldr(header_reg, Address(swap_reg,\n-                            BasicLock::displaced_header_offset_in_bytes()));\n+    if (UseFastLocking) {\n+      Label slow_case;\n@@ -867,2 +873,14 @@\n-    \/\/ Test for recursion\n-    cbz(header_reg, count);\n+      \/\/ Check for non-symmetric locking. This is allowed by the spec and the interpreter\n+      \/\/ must handle it.\n+      ldr(header_reg, Address(rthread, JavaThread::lock_stack_current_offset()));\n+      cmpoop(header_reg, obj_reg);\n+      br(Assembler::NE, slow_case);\n+\n+      ldr(header_reg, Address(obj_reg, oopDesc::mark_offset_in_bytes()));\n+      fast_unlock(obj_reg, header_reg, swap_reg, rscratch1, slow_case);\n+      b(count);\n+      bind(slow_case);\n+    } else {\n+      \/\/ Load the old header from BasicLock structure\n+      ldr(header_reg, Address(swap_reg,\n+                              BasicLock::displaced_header_offset_in_bytes()));\n@@ -870,2 +888,2 @@\n-    \/\/ Atomic swap back the old header\n-    cmpxchg_obj_header(swap_reg, header_reg, obj_reg, rscratch1, count, \/*fallthrough*\/NULL);\n+      \/\/ Test for recursion\n+      cbz(header_reg, count);\n@@ -873,0 +891,3 @@\n+      \/\/ Atomic swap back the old header\n+      cmpxchg_obj_header(swap_reg, header_reg, obj_reg, rscratch1, count, \/*fallthrough*\/NULL);\n+    }\n","filename":"src\/hotspot\/cpu\/aarch64\/interp_masm_aarch64.cpp","additions":82,"deletions":61,"binary":false,"changes":143,"status":"modified"},{"patch":"@@ -48,0 +48,1 @@\n+#include \"oops\/compressedKlass.inline.hpp\"\n@@ -4255,0 +4256,21 @@\n+\/\/ Loads the obj's Klass* into dst.\n+\/\/ src and dst must be distinct registers\n+\/\/ Preserves all registers (incl src, rscratch1 and rscratch2), but clobbers condition flags\n+void MacroAssembler::load_nklass(Register dst, Register src) {\n+  assert(UseCompressedClassPointers, \"expects UseCompressedClassPointers\");\n+\n+  Label fast;\n+\n+  \/\/ Check if we can take the (common) fast path, if obj is unlocked.\n+  ldr(dst, Address(src, oopDesc::mark_offset_in_bytes()));\n+  tst(dst, markWord::monitor_value);\n+  br(Assembler::EQ, fast);\n+\n+  \/\/ Fetch displaced header\n+  ldr(dst, Address(dst, OM_OFFSET_NO_MONITOR_VALUE_TAG(header)));\n+\n+  \/\/ Fast-path: shift and decode Klass*.\n+  bind(fast);\n+  lsr(dst, dst, markWord::klass_shift);\n+}\n+\n@@ -4256,6 +4278,2 @@\n-  if (UseCompressedClassPointers) {\n-    ldrw(dst, Address(src, oopDesc::klass_offset_in_bytes()));\n-    decode_klass_not_null(dst);\n-  } else {\n-    ldr(dst, Address(src, oopDesc::klass_offset_in_bytes()));\n-  }\n+  load_nklass(dst, src);\n+  decode_klass_not_null(dst);\n@@ -4300,14 +4318,10 @@\n-  if (UseCompressedClassPointers) {\n-    ldrw(tmp, Address(oop, oopDesc::klass_offset_in_bytes()));\n-    if (CompressedKlassPointers::base() == NULL) {\n-      cmp(trial_klass, tmp, LSL, CompressedKlassPointers::shift());\n-      return;\n-    } else if (((uint64_t)CompressedKlassPointers::base() & 0xffffffff) == 0\n-               && CompressedKlassPointers::shift() == 0) {\n-      \/\/ Only the bottom 32 bits matter\n-      cmpw(trial_klass, tmp);\n-      return;\n-    }\n-    decode_klass_not_null(tmp);\n-  } else {\n-    ldr(tmp, Address(oop, oopDesc::klass_offset_in_bytes()));\n+  assert(UseCompressedClassPointers, \"Lilliput\");\n+  load_nklass(tmp, oop);\n+  if (CompressedKlassPointers::base() == NULL) {\n+    cmp(trial_klass, tmp, LSL, CompressedKlassPointers::shift());\n+    return;\n+  } else if (((uint64_t)CompressedKlassPointers::base() & 0xffffffff) == 0\n+             && CompressedKlassPointers::shift() == 0) {\n+    \/\/ Only the bottom 32 bits matter\n+    cmpw(trial_klass, tmp);\n+    return;\n@@ -4315,0 +4329,1 @@\n+  decode_klass_not_null(tmp);\n@@ -4318,18 +4333,0 @@\n-void MacroAssembler::store_klass(Register dst, Register src) {\n-  \/\/ FIXME: Should this be a store release?  concurrent gcs assumes\n-  \/\/ klass length is valid if klass field is not null.\n-  if (UseCompressedClassPointers) {\n-    encode_klass_not_null(src);\n-    strw(src, Address(dst, oopDesc::klass_offset_in_bytes()));\n-  } else {\n-    str(src, Address(dst, oopDesc::klass_offset_in_bytes()));\n-  }\n-}\n-\n-void MacroAssembler::store_klass_gap(Register dst, Register src) {\n-  if (UseCompressedClassPointers) {\n-    \/\/ Store to klass gap in destination\n-    strw(src, Address(dst, oopDesc::klass_gap_offset_in_bytes()));\n-  }\n-}\n-\n@@ -4470,0 +4467,14 @@\n+\/\/ Returns a static string\n+const char* MacroAssembler::describe_klass_decode_mode(MacroAssembler::KlassDecodeMode mode) {\n+  switch (mode) {\n+  case KlassDecodeNone: return \"none\";\n+  case KlassDecodeZero: return \"zero\";\n+  case KlassDecodeXor:  return \"xor\";\n+  case KlassDecodeMovk: return \"movk\";\n+  default:\n+    ShouldNotReachHere();\n+  }\n+  return NULL;\n+}\n+\n+\/\/ Return the current narrow Klass pointer decode mode.\n@@ -4471,2 +4482,4 @@\n-  assert(UseCompressedClassPointers, \"not using compressed class pointers\");\n-  assert(Metaspace::initialized(), \"metaspace not initialized yet\");\n+  if (_klass_decode_mode == KlassDecodeNone) {\n+    \/\/ First time initialization\n+    assert(UseCompressedClassPointers, \"not using compressed class pointers\");\n+    assert(Metaspace::initialized(), \"metaspace not initialized yet\");\n@@ -4474,2 +4487,5 @@\n-  if (_klass_decode_mode != KlassDecodeNone) {\n-    return _klass_decode_mode;\n+    _klass_decode_mode = klass_decode_mode_for_base(CompressedKlassPointers::base());\n+    guarantee(_klass_decode_mode != KlassDecodeNone,\n+              PTR_FORMAT \" is not a valid encoding base on aarch64\",\n+              p2i(CompressedKlassPointers::base()));\n+    log_info(metaspace)(\"klass decode mode initialized: %s\", describe_klass_decode_mode(_klass_decode_mode));\n@@ -4477,0 +4493,2 @@\n+  return _klass_decode_mode;\n+}\n@@ -4478,2 +4496,4 @@\n-  assert(LogKlassAlignmentInBytes == CompressedKlassPointers::shift()\n-         || 0 == CompressedKlassPointers::shift(), \"decode alg wrong\");\n+\/\/ Given an arbitrary base address, return the KlassDecodeMode that would be used. Return KlassDecodeNone\n+\/\/ if base address is not valid for encoding.\n+MacroAssembler::KlassDecodeMode MacroAssembler::klass_decode_mode_for_base(address base) {\n+  assert(CompressedKlassPointers::shift() != 0, \"not lilliput?\");\n@@ -4481,2 +4501,4 @@\n-  if (CompressedKlassPointers::base() == NULL) {\n-    return (_klass_decode_mode = KlassDecodeZero);\n+  const uint64_t base_u64 = (uint64_t) base;\n+\n+  if (base_u64 == 0) {\n+    return KlassDecodeZero;\n@@ -4485,7 +4507,3 @@\n-  if (operand_valid_for_logical_immediate(\n-        \/*is32*\/false, (uint64_t)CompressedKlassPointers::base())) {\n-    const uint64_t range_mask =\n-      (1ULL << log2i(CompressedKlassPointers::range())) - 1;\n-    if (((uint64_t)CompressedKlassPointers::base() & range_mask) == 0) {\n-      return (_klass_decode_mode = KlassDecodeXor);\n-    }\n+  if (operand_valid_for_logical_immediate(false, base_u64) &&\n+      ((base_u64 & (KlassEncodingMetaspaceMax - 1)) == 0)) {\n+    return KlassDecodeXor;\n@@ -4494,4 +4512,4 @@\n-  const uint64_t shifted_base =\n-    (uint64_t)CompressedKlassPointers::base() >> CompressedKlassPointers::shift();\n-  guarantee((shifted_base & 0xffff0000ffffffff) == 0,\n-            \"compressed class base bad alignment\");\n+  const uint64_t shifted_base = base_u64 >> CompressedKlassPointers::shift();\n+  if ((shifted_base & 0xffff0000ffffffff) == 0) {\n+    return KlassDecodeMovk;\n+  }\n@@ -4499,1 +4517,1 @@\n-  return (_klass_decode_mode = KlassDecodeMovk);\n+  return KlassDecodeNone;\n@@ -4503,0 +4521,2 @@\n+  assert (UseCompressedClassPointers, \"should only be used for compressed headers\");\n+  assert(CompressedKlassPointers::shift() != 0, \"not lilliput?\");\n@@ -4505,5 +4525,1 @@\n-    if (CompressedKlassPointers::shift() != 0) {\n-      lsr(dst, src, LogKlassAlignmentInBytes);\n-    } else {\n-      if (dst != src) mov(dst, src);\n-    }\n+    lsr(dst, src, LogKlassAlignmentInBytes);\n@@ -4513,6 +4529,2 @@\n-    if (CompressedKlassPointers::shift() != 0) {\n-      eor(dst, src, (uint64_t)CompressedKlassPointers::base());\n-      lsr(dst, dst, LogKlassAlignmentInBytes);\n-    } else {\n-      eor(dst, src, (uint64_t)CompressedKlassPointers::base());\n-    }\n+    eor(dst, src, (uint64_t)CompressedKlassPointers::base());\n+    lsr(dst, dst, LogKlassAlignmentInBytes);\n@@ -4522,5 +4534,1 @@\n-    if (CompressedKlassPointers::shift() != 0) {\n-      ubfx(dst, src, LogKlassAlignmentInBytes, 32);\n-    } else {\n-      movw(dst, src);\n-    }\n+    ubfx(dst, src, LogKlassAlignmentInBytes, MaxNarrowKlassPointerBits);\n@@ -4542,0 +4550,2 @@\n+  assert(CompressedKlassPointers::shift() != 0, \"not lilliput?\");\n+\n@@ -4544,5 +4554,1 @@\n-    if (CompressedKlassPointers::shift() != 0) {\n-      lsl(dst, src, LogKlassAlignmentInBytes);\n-    } else {\n-      if (dst != src) mov(dst, src);\n-    }\n+    if (dst != src) mov(dst, src);\n@@ -4552,6 +4558,2 @@\n-    if (CompressedKlassPointers::shift() != 0) {\n-      lsl(dst, src, LogKlassAlignmentInBytes);\n-      eor(dst, dst, (uint64_t)CompressedKlassPointers::base());\n-    } else {\n-      eor(dst, src, (uint64_t)CompressedKlassPointers::base());\n-    }\n+    lsl(dst, src, LogKlassAlignmentInBytes);\n+    eor(dst, dst, (uint64_t)CompressedKlassPointers::base());\n@@ -4564,0 +4566,3 @@\n+    \/\/ Invalid base should have been gracefully handled via klass_decode_mode() in VM initialization.\n+    assert((shifted_base & 0xffff0000ffffffff) == 0, \"incompatible base\");\n+\n@@ -4566,5 +4571,1 @@\n-\n-    if (CompressedKlassPointers::shift() != 0) {\n-      lsl(dst, dst, LogKlassAlignmentInBytes);\n-    }\n-\n+    lsl(dst, dst, LogKlassAlignmentInBytes);\n@@ -6152,0 +6153,55 @@\n+\n+\/\/ Attempt to fast-lock an object. Fall-through on success, branch to slow label\n+\/\/ on failure.\n+\/\/ Registers:\n+\/\/  - obj: the object to be locked\n+\/\/  - hdr: the header, already loaded from obj, will be destroyed\n+\/\/  - t1, t2, t3: temporary registers, will be destroyed\n+void MacroAssembler::fast_lock(Register obj, Register hdr, Register t1, Register t2, Label& slow, bool rt_check_stack) {\n+  assert(UseFastLocking, \"only used with fast-locking\");\n+  assert_different_registers(obj, hdr, t1, t2);\n+\n+  if (rt_check_stack) {\n+    \/\/ Check if we would have space on lock-stack for the object.\n+    ldr(t1, Address(rthread, JavaThread::lock_stack_current_offset()));\n+    ldr(t2, Address(rthread, JavaThread::lock_stack_limit_offset()));\n+    cmp(t1, t2);\n+    br(Assembler::GE, slow);\n+  }\n+\n+  \/\/ Load (object->mark() | 1) into hdr\n+  orr(hdr, hdr, markWord::unlocked_value);\n+  \/\/ Clear lock-bits, into t2\n+  eor(t2, hdr, markWord::unlocked_value);\n+  \/\/ Try to swing header from unlocked to locked\n+  cmpxchg(\/*addr*\/ obj, \/*expected*\/ hdr, \/*new*\/ t2, Assembler::xword,\n+          \/*acquire*\/ true, \/*release*\/ true, \/*weak*\/ false, t1);\n+  br(Assembler::NE, slow);\n+\n+  \/\/ After successful lock, push object on lock-stack\n+  ldr(t1, Address(rthread, JavaThread::lock_stack_current_offset()));\n+  str(obj, Address(t1, 0));\n+  add(t1, t1, oopSize);\n+  str(t1, Address(rthread, JavaThread::lock_stack_current_offset()));\n+}\n+\n+void MacroAssembler::fast_unlock(Register obj, Register hdr, Register t1, Register t2, Label& slow) {\n+  assert(UseFastLocking, \"only used with fast-locking\");\n+  assert_different_registers(obj, hdr, t1, t2);\n+\n+  \/\/ Load the expected old header (lock-bits cleared to indicate 'locked') into hdr\n+  andr(hdr, hdr, ~markWord::lock_mask_in_place);\n+\n+  \/\/ Load the new header (unlocked) into t1\n+  orr(t1, hdr, markWord::unlocked_value);\n+\n+  \/\/ Try to swing header from locked to unlocked\n+  cmpxchg(obj, hdr, t1, Assembler::xword,\n+          \/*acquire*\/ true, \/*release*\/ true, \/*weak*\/ false, t2);\n+  br(Assembler::NE, slow);\n+\n+  \/\/ After successful unlock, pop object from lock-stack\n+  ldr(t1, Address(rthread, JavaThread::lock_stack_current_offset()));\n+  sub(t1, t1, oopSize);\n+  str(t1, Address(rthread, JavaThread::lock_stack_current_offset()));\n+}\n","filename":"src\/hotspot\/cpu\/aarch64\/macroAssembler_aarch64.cpp","additions":146,"deletions":90,"binary":false,"changes":236,"status":"modified"},{"patch":"@@ -89,0 +89,2 @@\n+ public:\n+\n@@ -96,1 +98,9 @@\n-  KlassDecodeMode klass_decode_mode();\n+  \/\/ Return the current narrow Klass pointer decode mode. Initialized on first call.\n+  static KlassDecodeMode klass_decode_mode();\n+\n+  \/\/ Given an arbitrary base address, return the KlassDecodeMode that would be used. Return KlassDecodeNone\n+  \/\/ if base address is not valid for encoding.\n+  static KlassDecodeMode klass_decode_mode_for_base(address base);\n+\n+  \/\/ Returns a static string\n+  static const char* describe_klass_decode_mode(KlassDecodeMode mode);\n@@ -845,0 +855,1 @@\n+  void load_nklass(Register dst, Register src);\n@@ -847,1 +858,0 @@\n-  void store_klass(Register dst, Register src);\n@@ -873,2 +883,0 @@\n-  void store_klass_gap(Register dst, Register src);\n-\n@@ -1574,0 +1582,3 @@\n+  void fast_lock(Register obj, Register hdr, Register t1, Register t2, Label& slow, bool rt_check_stack = true);\n+  void fast_unlock(Register obj, Register hdr, Register t1, Register t2, Label& slow);\n+\n","filename":"src\/hotspot\/cpu\/aarch64\/macroAssembler_aarch64.hpp","additions":15,"deletions":4,"binary":false,"changes":19,"status":"modified"},{"patch":"@@ -1778,28 +1778,33 @@\n-      \/\/ Load (object->mark() | 1) into swap_reg %r0\n-      __ ldr(rscratch1, Address(obj_reg, oopDesc::mark_offset_in_bytes()));\n-      __ orr(swap_reg, rscratch1, 1);\n-\n-      \/\/ Save (object->mark() | 1) into BasicLock's displaced header\n-      __ str(swap_reg, Address(lock_reg, mark_word_offset));\n-\n-      \/\/ src -> dest iff dest == r0 else r0 <- dest\n-      __ cmpxchg_obj_header(r0, lock_reg, obj_reg, rscratch1, count, \/*fallthrough*\/NULL);\n-\n-      \/\/ Hmm should this move to the slow path code area???\n-\n-      \/\/ Test if the oopMark is an obvious stack pointer, i.e.,\n-      \/\/  1) (mark & 3) == 0, and\n-      \/\/  2) sp <= mark < mark + os::pagesize()\n-      \/\/ These 3 tests can be done by evaluating the following\n-      \/\/ expression: ((mark - sp) & (3 - os::vm_page_size())),\n-      \/\/ assuming both stack pointer and pagesize have their\n-      \/\/ least significant 2 bits clear.\n-      \/\/ NOTE: the oopMark is in swap_reg %r0 as the result of cmpxchg\n-\n-      __ sub(swap_reg, sp, swap_reg);\n-      __ neg(swap_reg, swap_reg);\n-      __ ands(swap_reg, swap_reg, 3 - (int)os::vm_page_size());\n-\n-      \/\/ Save the test result, for recursive case, the result is zero\n-      __ str(swap_reg, Address(lock_reg, mark_word_offset));\n-      __ br(Assembler::NE, slow_path_lock);\n+      if (UseFastLocking) {\n+        __ ldr(swap_reg, Address(obj_reg, oopDesc::mark_offset_in_bytes()));\n+        __ fast_lock(obj_reg, swap_reg, tmp, rscratch1, slow_path_lock);\n+      } else {\n+        \/\/ Load (object->mark() | 1) into swap_reg %r0\n+        __ ldr(rscratch1, Address(obj_reg, oopDesc::mark_offset_in_bytes()));\n+        __ orr(swap_reg, rscratch1, 1);\n+\n+        \/\/ Save (object->mark() | 1) into BasicLock's displaced header\n+        __ str(swap_reg, Address(lock_reg, mark_word_offset));\n+\n+        \/\/ src -> dest iff dest == r0 else r0 <- dest\n+        __ cmpxchg_obj_header(r0, lock_reg, obj_reg, rscratch1, count, \/*fallthrough*\/NULL);\n+\n+        \/\/ Hmm should this move to the slow path code area???\n+\n+        \/\/ Test if the oopMark is an obvious stack pointer, i.e.,\n+        \/\/  1) (mark & 3) == 0, and\n+        \/\/  2) sp <= mark < mark + os::pagesize()\n+        \/\/ These 3 tests can be done by evaluating the following\n+        \/\/ expression: ((mark - sp) & (3 - os::vm_page_size())),\n+        \/\/ assuming both stack pointer and pagesize have their\n+        \/\/ least significant 2 bits clear.\n+        \/\/ NOTE: the oopMark is in swap_reg %r0 as the result of cmpxchg\n+\n+        __ sub(swap_reg, sp, swap_reg);\n+        __ neg(swap_reg, swap_reg);\n+        __ ands(swap_reg, swap_reg, 3 - (int)os::vm_page_size());\n+\n+        \/\/ Save the test result, for recursive case, the result is zero\n+        __ str(swap_reg, Address(lock_reg, mark_word_offset));\n+        __ br(Assembler::NE, slow_path_lock);\n+      }\n@@ -1916,1 +1921,1 @@\n-    if (!UseHeavyMonitors) {\n+    if (!UseHeavyMonitors && !UseFastLocking) {\n@@ -1932,9 +1937,14 @@\n-      \/\/ get address of the stack lock\n-      __ lea(r0, Address(sp, lock_slot_offset * VMRegImpl::stack_slot_size));\n-      \/\/  get old displaced header\n-      __ ldr(old_hdr, Address(r0, 0));\n-\n-      \/\/ Atomic swap old header if oop still contains the stack lock\n-      Label count;\n-      __ cmpxchg_obj_header(r0, old_hdr, obj_reg, rscratch1, count, &slow_path_unlock);\n-      __ bind(count);\n+      if (UseFastLocking) {\n+        __ ldr(old_hdr, Address(obj_reg, oopDesc::mark_offset_in_bytes()));\n+        __ fast_unlock(obj_reg, old_hdr, swap_reg, rscratch1, slow_path_unlock);\n+      } else {\n+        \/\/ get address of the stack lock\n+        __ lea(r0, Address(sp, lock_slot_offset * VMRegImpl::stack_slot_size));\n+        \/\/  get old displaced header\n+        __ ldr(old_hdr, Address(r0, 0));\n+\n+        \/\/ Atomic swap old header if oop still contains the stack lock\n+        Label count;\n+        __ cmpxchg_obj_header(r0, old_hdr, obj_reg, rscratch1, count, &slow_path_unlock);\n+        __ bind(count);\n+      }\n","filename":"src\/hotspot\/cpu\/aarch64\/sharedRuntime_aarch64.cpp","additions":48,"deletions":38,"binary":false,"changes":86,"status":"modified"},{"patch":"@@ -5360,0 +5360,23 @@\n+  address generate_check_lock_stack() {\n+    __ align(CodeEntryAlignment);\n+    StubCodeMark mark(this, \"StubRoutines\", \"check_lock_stack\");\n+\n+    address start = __ pc();\n+\n+    __ set_last_Java_frame(sp, rfp, lr, rscratch1);\n+    __ enter();\n+    __ push_call_clobbered_registers();\n+\n+    __ mov(c_rarg0, r9);\n+    __ call_VM_leaf(CAST_FROM_FN_PTR(address, LockStack::ensure_lock_stack_size), 1);\n+\n+\n+    __ pop_call_clobbered_registers();\n+    __ leave();\n+    __ reset_last_Java_frame(true);\n+\n+    __ ret(lr);\n+\n+    return start;\n+  }\n+\n@@ -7992,0 +8015,3 @@\n+    if (UseFastLocking) {\n+      StubRoutines::aarch64::_check_lock_stack = generate_check_lock_stack();\n+    }\n","filename":"src\/hotspot\/cpu\/aarch64\/stubGenerator_aarch64.cpp","additions":26,"deletions":0,"binary":false,"changes":26,"status":"modified"},{"patch":"@@ -60,0 +60,1 @@\n+address StubRoutines::aarch64::_check_lock_stack = NULL;\n","filename":"src\/hotspot\/cpu\/aarch64\/stubRoutines_aarch64.cpp","additions":1,"deletions":0,"binary":false,"changes":1,"status":"modified"},{"patch":"@@ -3531,1 +3531,1 @@\n-    __ mov(rscratch1, (intptr_t)markWord::prototype().value());\n+    __ ldr(rscratch1, Address(r4, Klass::prototype_header_offset()));\n@@ -3533,2 +3533,0 @@\n-    __ store_klass_gap(r0, zr);  \/\/ zero klass gap for compressed oops\n-    __ store_klass(r0, r4);      \/\/ store klass last\n","filename":"src\/hotspot\/cpu\/aarch64\/templateTable_aarch64.cpp","additions":1,"deletions":3,"binary":false,"changes":4,"status":"modified"},{"patch":"@@ -220,0 +220,4 @@\n+void LoadKlassStub::emit_code(LIR_Assembler* ce) {\n+  \/\/ Currently not needed.\n+  Unimplemented();\n+}\n","filename":"src\/hotspot\/cpu\/arm\/c1_CodeStubs_arm.cpp","additions":4,"deletions":0,"binary":false,"changes":4,"status":"modified"},{"patch":"@@ -148,1 +148,1 @@\n-  __ build_frame(initial_frame_size_in_bytes(), bang_size_in_bytes());\n+  __ build_frame(initial_frame_size_in_bytes(), bang_size_in_bytes(), compilation()->max_monitors());\n@@ -974,1 +974,1 @@\n-                      arrayOopDesc::header_size(op->type()),\n+                      arrayOopDesc::base_offset_in_bytes(op->type()),\n","filename":"src\/hotspot\/cpu\/arm\/c1_LIRAssembler_arm.cpp","additions":2,"deletions":2,"binary":false,"changes":4,"status":"modified"},{"patch":"@@ -56,1 +56,1 @@\n-void C1_MacroAssembler::build_frame(int frame_size_in_bytes, int bang_size_in_bytes) {\n+void C1_MacroAssembler::build_frame(int frame_size_in_bytes, int bang_size_in_bytes, int max_monitors) {\n@@ -166,1 +166,1 @@\n-                                       int header_size, int element_size,\n+                                       int header_size_in_bytes, int element_size,\n@@ -169,1 +169,0 @@\n-  const int header_size_in_bytes = header_size * BytesPerWord;\n","filename":"src\/hotspot\/cpu\/arm\/c1_MacroAssembler_arm.cpp","additions":2,"deletions":3,"binary":false,"changes":5,"status":"modified"},{"patch":"@@ -55,1 +55,1 @@\n-                      int header_size, int element_size,\n+                      int header_size_in_bytes, int element_size,\n","filename":"src\/hotspot\/cpu\/arm\/c1_MacroAssembler_arm.hpp","additions":1,"deletions":1,"binary":false,"changes":2,"status":"modified"},{"patch":"@@ -301,0 +301,4 @@\n+void LoadKlassStub::emit_code(LIR_Assembler* ce) {\n+  \/\/ Currently not needed.\n+  Unimplemented();\n+}\n","filename":"src\/hotspot\/cpu\/ppc\/c1_CodeStubs_ppc.cpp","additions":4,"deletions":0,"binary":false,"changes":4,"status":"modified"},{"patch":"@@ -116,1 +116,1 @@\n-  __ build_frame(initial_frame_size_in_bytes(), bang_size_in_bytes());\n+  __ build_frame(initial_frame_size_in_bytes(), bang_size_in_bytes(), compilation()->max_monitors());\n","filename":"src\/hotspot\/cpu\/ppc\/c1_LIRAssembler_ppc.cpp","additions":1,"deletions":1,"binary":false,"changes":2,"status":"modified"},{"patch":"@@ -72,1 +72,1 @@\n-void C1_MacroAssembler::build_frame(int frame_size_in_bytes, int bang_size_in_bytes) {\n+void C1_MacroAssembler::build_frame(int frame_size_in_bytes, int bang_size_in_bytes, int max_monitors) {\n","filename":"src\/hotspot\/cpu\/ppc\/c1_MacroAssembler_ppc.cpp","additions":1,"deletions":1,"binary":false,"changes":2,"status":"modified"},{"patch":"@@ -35,0 +35,1 @@\n+#include \"oops\/compressedKlass.inline.hpp\"\n","filename":"src\/hotspot\/cpu\/ppc\/macroAssembler_ppc.cpp","additions":1,"deletions":0,"binary":false,"changes":1,"status":"modified"},{"patch":"@@ -218,1 +218,1 @@\n-  __ build_frame(initial_frame_size_in_bytes(), bang_size_in_bytes());\n+  __ build_frame(initial_frame_size_in_bytes(), bang_size_in_bytes(), compilation()->max_monitors());\n","filename":"src\/hotspot\/cpu\/riscv\/c1_LIRAssembler_riscv.cpp","additions":1,"deletions":1,"binary":false,"changes":2,"status":"modified"},{"patch":"@@ -75,33 +75,38 @@\n-  \/\/ and mark it as unlocked\n-  ori(hdr, hdr, markWord::unlocked_value);\n-  \/\/ save unlocked object header into the displaced header location on the stack\n-  sd(hdr, Address(disp_hdr, 0));\n-  \/\/ test if object header is still the same (i.e. unlocked), and if so, store the\n-  \/\/ displaced header address in the object header - if it is not the same, get the\n-  \/\/ object header instead\n-  la(t1, Address(obj, hdr_offset));\n-  cmpxchgptr(hdr, disp_hdr, t1, t0, done, \/*fallthough*\/NULL);\n-  \/\/ if the object header was the same, we're done\n-  \/\/ if the object header was not the same, it is now in the hdr register\n-  \/\/ => test if it is a stack pointer into the same stack (recursive locking), i.e.:\n-  \/\/\n-  \/\/ 1) (hdr & aligned_mask) == 0\n-  \/\/ 2) sp <= hdr\n-  \/\/ 3) hdr <= sp + page_size\n-  \/\/\n-  \/\/ these 3 tests can be done by evaluating the following expression:\n-  \/\/\n-  \/\/ (hdr -sp) & (aligned_mask - page_size)\n-  \/\/\n-  \/\/ assuming both the stack pointer and page_size have their least\n-  \/\/ significant 2 bits cleared and page_size is a power of 2\n-  sub(hdr, hdr, sp);\n-  mv(t0, aligned_mask - (int)os::vm_page_size());\n-  andr(hdr, hdr, t0);\n-  \/\/ for recursive locking, the result is zero => save it in the displaced header\n-  \/\/ location (NULL in the displaced hdr location indicates recursive locking)\n-  sd(hdr, Address(disp_hdr, 0));\n-  \/\/ otherwise we don't care about the result and handle locking via runtime call\n-  bnez(hdr, slow_case, \/* is_far *\/ true);\n-  \/\/ done\n-  bind(done);\n+\n+  if (UseFastLocking) {\n+    fast_lock(obj, hdr, t0, t1, slow_case);\n+  } else {\n+    \/\/ and mark it as unlocked\n+    jori(hdr, hdr, markWord::unlocked_value);\n+    \/\/ save unlocked object header into the displaced header location on the stack\n+    sd(hdr, Address(disp_hdr, 0));\n+    \/\/ test if object header is still the same (i.e. unlocked), and if so, store the\n+    \/\/ displaced header address in the object header - if it is not the same, get the\n+    \/\/ object header instead\n+    la(t1, Address(obj, hdr_offset));\n+    cmpxchgptr(hdr, disp_hdr, t1, t0, done, \/*fallthough*\/NULL);\n+    \/\/ if the object header was the same, we're done\n+    \/\/ if the object header was not the same, it is now in the hdr register\n+    \/\/ => test if it is a stack pointer into the same stack (recursive locking), i.e.:\n+    \/\/\n+    \/\/ 1) (hdr & aligned_mask) == 0\n+    \/\/ 2) sp <= hdr\n+    \/\/ 3) hdr <= sp + page_size\n+    \/\/\n+    \/\/ these 3 tests can be done by evaluating the following expression:\n+    \/\/\n+    \/\/ (hdr -sp) & (aligned_mask - page_size)\n+    \/\/\n+    \/\/ assuming both the stack pointer and page_size have their least\n+    \/\/ significant 2 bits cleared and page_size is a power of 2\n+    sub(hdr, hdr, sp);\n+    mv(t0, aligned_mask - (int)os::vm_page_size());\n+    andr(hdr, hdr, t0);\n+    \/\/ for recursive locking, the result is zero => save it in the displaced header\n+    \/\/ location (NULL in the displaced hdr location indicates recursive locking)\n+    sd(hdr, Address(disp_hdr, 0));\n+    \/\/ otherwise we don't care about the result and handle locking via runtime call\n+    bnez(hdr, slow_case, \/* is_far *\/ true);\n+    \/\/ done\n+    bind(done);\n+  }\n@@ -118,16 +123,6 @@\n-  \/\/ load displaced header\n-  ld(hdr, Address(disp_hdr, 0));\n-  \/\/ if the loaded hdr is NULL we had recursive locking\n-  \/\/ if we had recursive locking, we are done\n-  beqz(hdr, done);\n-  \/\/ load object\n-  ld(obj, Address(disp_hdr, BasicObjectLock::obj_offset_in_bytes()));\n-  verify_oop(obj);\n-  \/\/ test if object header is pointing to the displaced header, and if so, restore\n-  \/\/ the displaced header in the object - if the object header is not pointing to\n-  \/\/ the displaced header, get the object header instead\n-  \/\/ if the object header was not pointing to the displaced header,\n-  \/\/ we do unlocking via runtime call\n-  if (hdr_offset) {\n-    la(t0, Address(obj, hdr_offset));\n-    cmpxchgptr(disp_hdr, hdr, t0, t1, done, &slow_case);\n+  if (UseFastLocking) {\n+    \/\/ load object\n+    ld(obj, Address(disp_hdr, BasicObjectLock::obj_offset_in_bytes()));\n+    verify_oop(obj);\n+    ld(hdr, Address(obj, oopDesc::mark_offset_in_bytes()));\n+    fast_unlock(obj, hdr, t0, t1, slow_case);\n@@ -135,1 +130,21 @@\n-    cmpxchgptr(disp_hdr, hdr, obj, t1, done, &slow_case);\n+    \/\/ load displaced header\n+    ld(hdr, Address(disp_hdr, 0));\n+    \/\/ if the loaded hdr is NULL we had recursive locking\n+    \/\/ if we had recursive locking, we are done\n+    beqz(hdr, done);\n+    \/\/ load object\n+    ld(obj, Address(disp_hdr, BasicObjectLock::obj_offset_in_bytes()));\n+    verify_oop(obj);\n+    \/\/ test if object header is pointing to the displaced header, and if so, restore\n+    \/\/ the displaced header in the object - if the object header is not pointing to\n+    \/\/ the displaced header, get the object header instead\n+    \/\/ if the object header was not pointing to the displaced header,\n+    \/\/ we do unlocking via runtime call\n+    if (hdr_offset) {\n+      la(t0, Address(obj, hdr_offset));\n+      cmpxchgptr(disp_hdr, hdr, t0, t1, done, &slow_case);\n+    } else {\n+      cmpxchgptr(disp_hdr, hdr, obj, t1, done, &slow_case);\n+    }\n+    \/\/ done\n+    bind(done);\n@@ -137,2 +152,0 @@\n-  \/\/ done\n-  bind(done);\n@@ -308,1 +321,1 @@\n-void C1_MacroAssembler::build_frame(int framesize, int bang_size_in_bytes) {\n+void C1_MacroAssembler::build_frame(int framesize, int bang_size_in_bytes, int max_monitors) {\n","filename":"src\/hotspot\/cpu\/riscv\/c1_MacroAssembler_riscv.cpp","additions":66,"deletions":53,"binary":false,"changes":119,"status":"modified"},{"patch":"@@ -812,28 +812,34 @@\n-    \/\/ Load (object->mark() | 1) into swap_reg\n-    ld(t0, Address(obj_reg, oopDesc::mark_offset_in_bytes()));\n-    ori(swap_reg, t0, 1);\n-\n-    \/\/ Save (object->mark() | 1) into BasicLock's displaced header\n-    sd(swap_reg, Address(lock_reg, mark_offset));\n-\n-    assert(lock_offset == 0,\n-           \"displached header must be first word in BasicObjectLock\");\n-\n-    cmpxchg_obj_header(swap_reg, lock_reg, obj_reg, t0, count, \/*fallthrough*\/NULL);\n-\n-    \/\/ Test if the oopMark is an obvious stack pointer, i.e.,\n-    \/\/  1) (mark & 7) == 0, and\n-    \/\/  2) sp <= mark < mark + os::pagesize()\n-    \/\/\n-    \/\/ These 3 tests can be done by evaluating the following\n-    \/\/ expression: ((mark - sp) & (7 - os::vm_page_size())),\n-    \/\/ assuming both stack pointer and pagesize have their\n-    \/\/ least significant 3 bits clear.\n-    \/\/ NOTE: the oopMark is in swap_reg x10 as the result of cmpxchg\n-    sub(swap_reg, swap_reg, sp);\n-    mv(t0, (int64_t)(7 - (int)os::vm_page_size()));\n-    andr(swap_reg, swap_reg, t0);\n-\n-    \/\/ Save the test result, for recursive case, the result is zero\n-    sd(swap_reg, Address(lock_reg, mark_offset));\n-    beqz(swap_reg, count);\n+    if (UseFastLocking) {\n+      ld(tmp, Address(obj_reg, oopDesc::mark_offset_in_bytes()));\n+      fast_lock(obj_reg, tmp, t0, t1, slow_case);\n+      j(count);\n+    } else {\n+      \/\/ Load (object->mark() | 1) into swap_reg\n+      ld(t0, Address(obj_reg, oopDesc::mark_offset_in_bytes()));\n+      ori(swap_reg, t0, 1);\n+\n+      \/\/ Save (object->mark() | 1) into BasicLock's displaced header\n+      sd(swap_reg, Address(lock_reg, mark_offset));\n+\n+      assert(lock_offset == 0,\n+             \"displached header must be first word in BasicObjectLock\");\n+\n+      cmpxchg_obj_header(swap_reg, lock_reg, obj_reg, t0, count, \/*fallthrough*\/NULL);\n+\n+      \/\/ Test if the oopMark is an obvious stack pointer, i.e.,\n+      \/\/  1) (mark & 7) == 0, and\n+      \/\/  2) sp <= mark < mark + os::pagesize()\n+      \/\/\n+      \/\/ These 3 tests can be done by evaluating the following\n+      \/\/ expression: ((mark - sp) & (7 - os::vm_page_size())),\n+      \/\/ assuming both stack pointer and pagesize have their\n+      \/\/ least significant 3 bits clear.\n+      \/\/ NOTE: the oopMark is in swap_reg x10 as the result of cmpxchg\n+      sub(swap_reg, swap_reg, sp);\n+      mv(t0, (int64_t)(7 - (int)os::vm_page_size()));\n+      andr(swap_reg, swap_reg, t0);\n+\n+      \/\/ Save the test result, for recursive case, the result is zero\n+      sd(swap_reg, Address(lock_reg, mark_offset));\n+      beqz(swap_reg, count);\n+    }\n@@ -846,1 +852,1 @@\n-            lock_reg);\n+            UseFastLocking ? obj_reg : lock_reg);\n@@ -884,3 +890,4 @@\n-    \/\/ Convert from BasicObjectLock structure to object and BasicLock\n-    \/\/ structure Store the BasicLock address into x10\n-    la(swap_reg, Address(lock_reg, BasicObjectLock::lock_offset_in_bytes()));\n+    if (UseFastLocking) {\n+      Label slow_case;\n+      \/\/ Load oop into obj_reg(c_rarg3)\n+      ld(obj_reg, Address(lock_reg, BasicObjectLock::obj_offset_in_bytes()));\n@@ -888,2 +895,2 @@\n-    \/\/ Load oop into obj_reg(c_rarg3)\n-    ld(obj_reg, Address(lock_reg, BasicObjectLock::obj_offset_in_bytes()));\n+      \/\/ Free entry\n+      sd(zr, Address(lock_reg, BasicObjectLock::obj_offset_in_bytes()));\n@@ -891,2 +898,4 @@\n-    \/\/ Free entry\n-    sd(zr, Address(lock_reg, BasicObjectLock::obj_offset_in_bytes()));\n+      \/\/ Check for non-symmetric locking. This is allowed by the spec and the interpreter\n+      \/\/ must handle it.\n+      ld(header_reg, Address(xthread, JavaThread::lock_stack_current_offset()));\n+      bne(header_reg, obj_reg, slow_case);\n@@ -894,3 +903,9 @@\n-    \/\/ Load the old header from BasicLock structure\n-    ld(header_reg, Address(swap_reg,\n-                           BasicLock::displaced_header_offset_in_bytes()));\n+      ld(header_reg, Address(obj_reg, oopDesc::mark_offset_in_bytes()));\n+      fast_unlock(obj_reg, header_reg, swap_reg, t0, slow_case);\n+      j(count);\n+\n+      bind(slow_case);\n+    } else {\n+      \/\/ Convert from BasicObjectLock structure to object and BasicLock\n+      \/\/ structure Store the BasicLock address into x10\n+      la(swap_reg, Address(lock_reg, BasicObjectLock::lock_offset_in_bytes()));\n@@ -898,2 +913,2 @@\n-    \/\/ Test for recursion\n-    beqz(header_reg, count);\n+      \/\/ Load oop into obj_reg(c_rarg3)\n+      ld(obj_reg, Address(lock_reg, BasicObjectLock::obj_offset_in_bytes()));\n@@ -901,2 +916,13 @@\n-    \/\/ Atomic swap back the old header\n-    cmpxchg_obj_header(swap_reg, header_reg, obj_reg, t0, count, \/*fallthrough*\/NULL);\n+      \/\/ Free entry\n+      sd(zr, Address(lock_reg, BasicObjectLock::obj_offset_in_bytes()));\n+\n+      \/\/ Load the old header from BasicLock structure\n+      ld(header_reg, Address(swap_reg,\n+                             BasicLock::displaced_header_offset_in_bytes()));\n+\n+      \/\/ Test for recursion\n+      beqz(header_reg, count);\n+\n+      \/\/ Atomic swap back the old header\n+      cmpxchg_obj_header(swap_reg, header_reg, obj_reg, t0, count, \/*fallthrough*\/NULL);\n+    }\n","filename":"src\/hotspot\/cpu\/riscv\/interp_masm_riscv.cpp","additions":69,"deletions":43,"binary":false,"changes":112,"status":"modified"},{"patch":"@@ -4485,0 +4485,55 @@\n+\n+\/\/ Attempt to fast-lock an object. Fall-through on success, branch to slow label\n+\/\/ on failure.\n+\/\/ Registers:\n+\/\/  - obj: the object to be locked\n+\/\/  - hdr: the header, already loaded from obj, will be destroyed\n+\/\/  - tmp1, tmp2: temporary registers, will be destroyed\n+void MacroAssembler::fast_lock(Register obj, Register hdr, Register tmp1, Register tmp2, Label& slow) {\n+  assert(UseFastLocking, \"only used with fast-locking\");\n+  assert_different_registers(obj, hdr, tmp1, tmp2);\n+\n+  \/\/ Check if we would have space on lock-stack for the object.\n+  ld(tmp1, Address(xthread, JavaThread::lock_stack_current_offset()));\n+  ld(tmp2, Address(xthread, JavaThread::lock_stack_limit_offset()));\n+  bge(tmp1, tmp2, slow, true);\n+\n+  \/\/ Load (object->mark() | 1) into hdr\n+  ori(hdr, hdr, markWord::unlocked_value);\n+  \/\/ Clear lock-bits, into tmp2\n+  xori(tmp2, hdr, markWord::unlocked_value);\n+  \/\/ Try to swing header from unlocked to locked\n+  Label success;\n+  cmpxchgptr(hdr, tmp2, obj, tmp1, success, &slow);\n+  bind(success);\n+\n+  \/\/ After successful lock, push object on lock-stack\n+  \/\/ TODO: Can we avoid re-loading the current offset? The CAS above clobbers it.\n+  \/\/ Maybe we could ensure that we have enough space on the lock stack more cleverly.\n+  ld(tmp1, Address(xthread, JavaThread::lock_stack_current_offset()));\n+  sd(obj, Address(tmp1, 0));\n+  add(tmp1, tmp1, oopSize);\n+  sd(tmp1, Address(xthread, JavaThread::lock_stack_current_offset()));\n+}\n+\n+void MacroAssembler::fast_unlock(Register obj, Register hdr, Register tmp1, Register tmp2, Label& slow) {\n+  assert(UseFastLocking, \"only used with fast-locking\");\n+  assert_different_registers(obj, hdr, tmp1, tmp2);\n+\n+  \/\/ Load the expected old header (lock-bits cleared to indicate 'locked') into hdr\n+  mv(tmp1, ~markWord::lock_mask_in_place);\n+  andr(hdr, hdr, tmp1);\n+\n+  \/\/ Load the new header (unlocked) into tmp1\n+  ori(tmp1, hdr, markWord::unlocked_value);\n+\n+  \/\/ Try to swing header from locked to unlocked\n+  Label success;\n+  cmpxchgptr(hdr, tmp1, obj, tmp2, success, &slow);\n+  bind(success);\n+\n+  \/\/ After successful unlock, pop object from lock-stack\n+  ld(tmp1, Address(xthread, JavaThread::lock_stack_current_offset()));\n+  sub(tmp1, tmp1, oopSize);\n+  sd(tmp1, Address(xthread, JavaThread::lock_stack_current_offset()));\n+}\n","filename":"src\/hotspot\/cpu\/riscv\/macroAssembler_riscv.cpp","additions":55,"deletions":0,"binary":false,"changes":55,"status":"modified"},{"patch":"@@ -1386,0 +1386,4 @@\n+\n+public:\n+  void fast_lock(Register obj, Register hdr, Register tmp1, Register tmp2, Label& slow);\n+  void fast_unlock(Register obj, Register hdr, Register tmp1, Register tmp2, Label& slow);\n","filename":"src\/hotspot\/cpu\/riscv\/macroAssembler_riscv.hpp","additions":4,"deletions":0,"binary":false,"changes":4,"status":"modified"},{"patch":"@@ -2420,30 +2420,40 @@\n-      \/\/ Set tmp to be (markWord of object | UNLOCK_VALUE).\n-      __ ori(tmp, disp_hdr, markWord::unlocked_value);\n-\n-      \/\/ Initialize the box. (Must happen before we update the object mark!)\n-      __ sd(tmp, Address(box, BasicLock::displaced_header_offset_in_bytes()));\n-\n-      \/\/ Compare object markWord with an unlocked value (tmp) and if\n-      \/\/ equal exchange the stack address of our box with object markWord.\n-      \/\/ On failure disp_hdr contains the possibly locked markWord.\n-      __ cmpxchg(\/*memory address*\/oop, \/*expected value*\/tmp, \/*new value*\/box, Assembler::int64, Assembler::aq,\n-                 Assembler::rl, \/*result*\/disp_hdr);\n-      __ mv(flag, zr);\n-      __ beq(disp_hdr, tmp, cont); \/\/ prepare zero flag and goto cont if we won the cas\n-\n-      assert(oopDesc::mark_offset_in_bytes() == 0, \"offset of _mark is not 0\");\n-\n-      \/\/ If the compare-and-exchange succeeded, then we found an unlocked\n-      \/\/ object, will have now locked it will continue at label cont\n-      \/\/ We did not see an unlocked object so try the fast recursive case.\n-\n-      \/\/ Check if the owner is self by comparing the value in the\n-      \/\/ markWord of object (disp_hdr) with the stack pointer.\n-      __ sub(disp_hdr, disp_hdr, sp);\n-      __ mv(tmp, (intptr_t) (~(os::vm_page_size()-1) | (uintptr_t)markWord::lock_mask_in_place));\n-      \/\/ If (mark & lock_mask) == 0 and mark - sp < page_size, we are stack-locking and goto cont,\n-      \/\/ hence we can store 0 as the displaced header in the box, which indicates that it is a\n-      \/\/ recursive lock.\n-      __ andr(tmp\/*==0?*\/, disp_hdr, tmp);\n-      __ sd(tmp\/*==0, perhaps*\/, Address(box, BasicLock::displaced_header_offset_in_bytes()));\n-      __ mv(flag, tmp); \/\/ we can use the value of tmp as the result here\n+      if (UseFastLocking) {\n+        Label slow;\n+        __ fast_lock(oop, disp_hdr, tmp, t0, slow);\n+        \/\/ Indicate success at cont.\n+        __ mv(flag, zr);\n+        __ j(cont);\n+        __ bind(slow);\n+        __ mv(flag, 1); \/\/ Set non-zero flag to indicate 'failure' -> take slow-path\n+      } else {\n+        \/\/ Set tmp to be (markWord of object | UNLOCK_VALUE).\n+        __ ori(tmp, disp_hdr, markWord::unlocked_value);\n+\n+        \/\/ Initialize the box. (Must happen before we update the object mark!)\n+        __ sd(tmp, Address(box, BasicLock::displaced_header_offset_in_bytes()));\n+\n+        \/\/ Compare object markWord with an unlocked value (tmp) and if\n+        \/\/ equal exchange the stack address of our box with object markWord.\n+        \/\/ On failure disp_hdr contains the possibly locked markWord.\n+        __ cmpxchg(\/*memory address*\/oop, \/*expected value*\/tmp, \/*new value*\/box, Assembler::int64, Assembler::aq,\n+                   Assembler::rl, \/*result*\/disp_hdr);\n+        __ mv(flag, zr);\n+        __ beq(disp_hdr, tmp, cont); \/\/ prepare zero flag and goto cont if we won the cas\n+\n+        assert(oopDesc::mark_offset_in_bytes() == 0, \"offset of _mark is not 0\");\n+\n+        \/\/ If the compare-and-exchange succeeded, then we found an unlocked\n+        \/\/ object, will have now locked it will continue at label cont\n+        \/\/ We did not see an unlocked object so try the fast recursive case.\n+\n+        \/\/ Check if the owner is self by comparing the value in the\n+        \/\/ markWord of object (disp_hdr) with the stack pointer.\n+        __ sub(disp_hdr, disp_hdr, sp);\n+        __ mv(tmp, (intptr_t) (~(os::vm_page_size()-1) | (uintptr_t)markWord::lock_mask_in_place));\n+        \/\/ If (mark & lock_mask) == 0 and mark - sp < page_size, we are stack-locking and goto cont,\n+        \/\/ hence we can store 0 as the displaced header in the box, which indicates that it is a\n+        \/\/ recursive lock.\n+        __ andr(tmp\/*==0?*\/, disp_hdr, tmp);\n+        __ sd(tmp\/*==0, perhaps*\/, Address(box, BasicLock::displaced_header_offset_in_bytes()));\n+        __ mv(flag, tmp); \/\/ we can use the value of tmp as the result here\n+      }\n@@ -2466,6 +2476,8 @@\n-    \/\/ Store a non-null value into the box to avoid looking like a re-entrant\n-    \/\/ lock. The fast-path monitor unlock code checks for\n-    \/\/ markWord::monitor_value so use markWord::unused_mark which has the\n-    \/\/ relevant bit set, and also matches ObjectSynchronizer::slow_enter.\n-    __ mv(tmp, (address)markWord::unused_mark().value());\n-    __ sd(tmp, Address(box, BasicLock::displaced_header_offset_in_bytes()));\n+    if (!UseFastLocking) {\n+      \/\/ Store a non-null value into the box to avoid looking like a re-entrant\n+      \/\/ lock. The fast-path monitor unlock code checks for\n+      \/\/ markWord::monitor_value so use markWord::unused_mark which has the\n+      \/\/ relevant bit set, and also matches ObjectSynchronizer::slow_enter.\n+      __ mv(tmp, (address)markWord::unused_mark().value());\n+      __ sd(tmp, Address(box, BasicLock::displaced_header_offset_in_bytes()));\n+    }\n@@ -2504,1 +2516,1 @@\n-    if (!UseHeavyMonitors) {\n+    if (!UseHeavyMonitors && !UseFastLocking) {\n@@ -2519,3 +2531,13 @@\n-      \/\/ Check if it is still a light weight lock, this is true if we\n-      \/\/ see the stack address of the basicLock in the markWord of the\n-      \/\/ object.\n+      if (UseFastLocking) {\n+        Label slow;\n+        __ fast_unlock(oop, tmp, box, disp_hdr, slow);\n+\n+        \/\/ Indicate success at cont.\n+        __ mv(flag, zr);\n+        __ j(cont);\n+        __ bind(slow);\n+        __ mv(flag, 1); \/\/ Set non-zero flag to indicate 'failure' -> take slow path\n+      } else {\n+        \/\/ Check if it is still a light weight lock, this is true if we\n+        \/\/ see the stack address of the basicLock in the markWord of the\n+        \/\/ object.\n@@ -2523,3 +2545,4 @@\n-      __ cmpxchg(\/*memory address*\/oop, \/*expected value*\/box, \/*new value*\/disp_hdr, Assembler::int64, Assembler::relaxed,\n-                 Assembler::rl, \/*result*\/tmp);\n-      __ xorr(flag, box, tmp); \/\/ box == tmp if cas succeeds\n+        __ cmpxchg(\/*memory address*\/oop, \/*expected value*\/box, \/*new value*\/disp_hdr, Assembler::int64, Assembler::relaxed,\n+                   Assembler::rl, \/*result*\/tmp);\n+        __ xorr(flag, box, tmp); \/\/ box == tmp if cas succeeds\n+      }\n@@ -2537,0 +2560,11 @@\n+\n+    if (UseFastLocking) {\n+      Label L;\n+      __ ld(disp_hdr, Address(tmp, ObjectMonitor::owner_offset_in_bytes()));\n+      __ mv(t0, (unsigned char)(intptr_t)ANONYMOUS_OWNER);\n+      __ bne(disp_hdr, t0, L);\n+      __ mv(flag, 1); \/\/ Indicate failure at cont -- dive into slow-path.\n+      __ j(cont);\n+      __ bind(L);\n+    }\n+\n","filename":"src\/hotspot\/cpu\/riscv\/riscv.ad","additions":77,"deletions":43,"binary":false,"changes":120,"status":"modified"},{"patch":"@@ -1675,25 +1675,30 @@\n-      \/\/ Load (object->mark() | 1) into swap_reg % x10\n-      __ ld(t0, Address(obj_reg, oopDesc::mark_offset_in_bytes()));\n-      __ ori(swap_reg, t0, 1);\n-\n-      \/\/ Save (object->mark() | 1) into BasicLock's displaced header\n-      __ sd(swap_reg, Address(lock_reg, mark_word_offset));\n-\n-      \/\/ src -> dest if dest == x10 else x10 <- dest\n-      __ cmpxchg_obj_header(x10, lock_reg, obj_reg, t0, count, \/*fallthrough*\/NULL);\n-\n-      \/\/ Test if the oopMark is an obvious stack pointer, i.e.,\n-      \/\/  1) (mark & 3) == 0, and\n-      \/\/  2) sp <= mark < mark + os::pagesize()\n-      \/\/ These 3 tests can be done by evaluating the following\n-      \/\/ expression: ((mark - sp) & (3 - os::vm_page_size())),\n-      \/\/ assuming both stack pointer and pagesize have their\n-      \/\/ least significant 2 bits clear.\n-      \/\/ NOTE: the oopMark is in swap_reg % 10 as the result of cmpxchg\n-\n-      __ sub(swap_reg, swap_reg, sp);\n-      __ andi(swap_reg, swap_reg, 3 - (int)os::vm_page_size());\n-\n-      \/\/ Save the test result, for recursive case, the result is zero\n-      __ sd(swap_reg, Address(lock_reg, mark_word_offset));\n-      __ bnez(swap_reg, slow_path_lock);\n+      if (UseFastLocking) {\n+        __ ld(swap_reg, Address(obj_reg, oopDesc::mark_offset_in_bytes()));\n+        __ fast_lock(obj_reg, swap_reg, tmp, t0, slow_path_lock);\n+      } else {\n+        \/\/ Load (object->mark() | 1) into swap_reg % x10\n+        __ ld(t0, Address(obj_reg, oopDesc::mark_offset_in_bytes()));\n+        __ ori(swap_reg, t0, 1);\n+\n+        \/\/ Save (object->mark() | 1) into BasicLock's displaced header\n+        __ sd(swap_reg, Address(lock_reg, mark_word_offset));\n+\n+        \/\/ src -> dest if dest == x10 else x10 <- dest\n+        __ cmpxchg_obj_header(x10, lock_reg, obj_reg, t0, count, \/*fallthrough*\/NULL);\n+\n+        \/\/ Test if the oopMark is an obvious stack pointer, i.e.,\n+        \/\/  1) (mark & 3) == 0, and\n+        \/\/  2) sp <= mark < mark + os::pagesize()\n+        \/\/ These 3 tests can be done by evaluating the following\n+        \/\/ expression: ((mark - sp) & (3 - os::vm_page_size())),\n+        \/\/ assuming both stack pointer and pagesize have their\n+        \/\/ least significant 2 bits clear.\n+        \/\/ NOTE: the oopMark is in swap_reg % 10 as the result of cmpxchg\n+\n+        __ sub(swap_reg, swap_reg, sp);\n+        __ andi(swap_reg, swap_reg, 3 - (int)os::vm_page_size());\n+\n+        \/\/ Save the test result, for recursive case, the result is zero\n+        __ sd(swap_reg, Address(lock_reg, mark_word_offset));\n+        __ bnez(swap_reg, slow_path_lock);\n+      }\n@@ -1794,1 +1799,1 @@\n-    if (!UseHeavyMonitors) {\n+    if (!UseHeavyMonitors && !UseFastLocking) {\n@@ -1810,9 +1815,14 @@\n-      \/\/ get address of the stack lock\n-      __ la(x10, Address(sp, lock_slot_offset * VMRegImpl::stack_slot_size));\n-      \/\/  get old displaced header\n-      __ ld(old_hdr, Address(x10, 0));\n-\n-      \/\/ Atomic swap old header if oop still contains the stack lock\n-      Label count;\n-      __ cmpxchg_obj_header(x10, old_hdr, obj_reg, t0, count, &slow_path_unlock);\n-      __ bind(count);\n+      if (UseFastLocking) {\n+        __ ld(old_hdr, Address(obj_reg, oopDesc::mark_offset_in_bytes()));\n+        __ fast_unlock(obj_reg, old_hdr, swap_reg, t0, slow_path_unlock);\n+      } else {\n+        \/\/ get address of the stack lock\n+        __ la(x10, Address(sp, lock_slot_offset * VMRegImpl::stack_slot_size));\n+        \/\/  get old displaced header\n+        __ ld(old_hdr, Address(x10, 0));\n+\n+        \/\/ Atomic swap old header if oop still contains the stack lock\n+        Label count;\n+        __ cmpxchg_obj_header(x10, old_hdr, obj_reg, t0, count, &slow_path_unlock);\n+        __ bind(count);\n+      }\n","filename":"src\/hotspot\/cpu\/riscv\/sharedRuntime_riscv.cpp","additions":45,"deletions":35,"binary":false,"changes":80,"status":"modified"},{"patch":"@@ -256,0 +256,5 @@\n+void LoadKlassStub::emit_code(LIR_Assembler* ce) {\n+  \/\/ Currently not needed.\n+  Unimplemented();\n+}\n+\n","filename":"src\/hotspot\/cpu\/s390\/c1_CodeStubs_s390.cpp","additions":5,"deletions":0,"binary":false,"changes":5,"status":"modified"},{"patch":"@@ -38,0 +38,1 @@\n+#include \"oops\/compressedKlass.inline.hpp\"\n","filename":"src\/hotspot\/cpu\/s390\/macroAssembler_s390.cpp","additions":1,"deletions":0,"binary":false,"changes":1,"status":"modified"},{"patch":"@@ -33,0 +33,1 @@\n+#include \"runtime\/objectMonitor.hpp\"\n@@ -281,0 +282,10 @@\n+void LoadKlassStub::emit_code(LIR_Assembler* ce) {\n+  __ bind(_entry);\n+#ifdef _LP64\n+  Register d = _result->as_register();\n+  __ movq(d, Address(d, OM_OFFSET_NO_MONITOR_VALUE_TAG(header)));\n+  __ jmp(_continuation);\n+#else\n+  __ should_not_reach_here();\n+#endif\n+}\n","filename":"src\/hotspot\/cpu\/x86\/c1_CodeStubs_x86.cpp","additions":11,"deletions":0,"binary":false,"changes":11,"status":"modified"},{"patch":"@@ -41,1 +41,1 @@\n-int C1_MacroAssembler::lock_object(Register hdr, Register obj, Register disp_hdr, Label& slow_case) {\n+int C1_MacroAssembler::lock_object(Register hdr, Register obj, Register disp_hdr, Register tmp, Label& slow_case) {\n@@ -45,2 +45,1 @@\n-  assert(hdr != obj && hdr != disp_hdr && obj != disp_hdr, \"registers must be different\");\n-  Label done;\n+  assert_different_registers(hdr, obj, disp_hdr, tmp);\n@@ -65,33 +64,44 @@\n-  \/\/ and mark it as unlocked\n-  orptr(hdr, markWord::unlocked_value);\n-  \/\/ save unlocked object header into the displaced header location on the stack\n-  movptr(Address(disp_hdr, 0), hdr);\n-  \/\/ test if object header is still the same (i.e. unlocked), and if so, store the\n-  \/\/ displaced header address in the object header - if it is not the same, get the\n-  \/\/ object header instead\n-  MacroAssembler::lock(); \/\/ must be immediately before cmpxchg!\n-  cmpxchgptr(disp_hdr, Address(obj, hdr_offset));\n-  \/\/ if the object header was the same, we're done\n-  jcc(Assembler::equal, done);\n-  \/\/ if the object header was not the same, it is now in the hdr register\n-  \/\/ => test if it is a stack pointer into the same stack (recursive locking), i.e.:\n-  \/\/\n-  \/\/ 1) (hdr & aligned_mask) == 0\n-  \/\/ 2) rsp <= hdr\n-  \/\/ 3) hdr <= rsp + page_size\n-  \/\/\n-  \/\/ these 3 tests can be done by evaluating the following expression:\n-  \/\/\n-  \/\/ (hdr - rsp) & (aligned_mask - page_size)\n-  \/\/\n-  \/\/ assuming both the stack pointer and page_size have their least\n-  \/\/ significant 2 bits cleared and page_size is a power of 2\n-  subptr(hdr, rsp);\n-  andptr(hdr, aligned_mask - (int)os::vm_page_size());\n-  \/\/ for recursive locking, the result is zero => save it in the displaced header\n-  \/\/ location (NULL in the displaced hdr location indicates recursive locking)\n-  movptr(Address(disp_hdr, 0), hdr);\n-  \/\/ otherwise we don't care about the result and handle locking via runtime call\n-  jcc(Assembler::notZero, slow_case);\n-  \/\/ done\n-  bind(done);\n+\n+  if (UseFastLocking) {\n+#ifdef _LP64\n+    const Register thread = r15_thread;\n+#else\n+    const Register thread = disp_hdr;\n+    get_thread(thread);\n+#endif\n+    fast_lock_impl(obj, hdr, thread, tmp, slow_case, LP64_ONLY(false) NOT_LP64(true));\n+  } else {\n+    Label done;\n+    orptr(hdr, markWord::unlocked_value);\n+    \/\/ save unlocked object header into the displaced header location on the stack\n+    movptr(Address(disp_hdr, 0), hdr);\n+    \/\/ test if object header is still the same (i.e. unlocked), and if so, store the\n+    \/\/ displaced header address in the object header - if it is not the same, get the\n+    \/\/ object header instead\n+    MacroAssembler::lock(); \/\/ must be immediately before cmpxchg!\n+    cmpxchgptr(disp_hdr, Address(obj, hdr_offset));\n+    \/\/ if the object header was the same, we're done\n+    jcc(Assembler::equal, done);\n+    \/\/ if the object header was not the same, it is now in the hdr register\n+    \/\/ => test if it is a stack pointer into the same stack (recursive locking), i.e.:\n+    \/\/\n+    \/\/ 1) (hdr & aligned_mask) == 0\n+    \/\/ 2) rsp <= hdr\n+    \/\/ 3) hdr <= rsp + page_size\n+    \/\/\n+    \/\/ these 3 tests can be done by evaluating the following expression:\n+    \/\/\n+    \/\/ (hdr - rsp) & (aligned_mask - page_size)\n+    \/\/\n+    \/\/ assuming both the stack pointer and page_size have their least\n+    \/\/ significant 2 bits cleared and page_size is a power of 2\n+    subptr(hdr, rsp);\n+    andptr(hdr, aligned_mask - (int)os::vm_page_size());\n+    \/\/ for recursive locking, the result is zero => save it in the displaced header\n+    \/\/ location (NULL in the displaced hdr location indicates recursive locking)\n+    movptr(Address(disp_hdr, 0), hdr);\n+    \/\/ otherwise we don't care about the result and handle locking via runtime call\n+    jcc(Assembler::notZero, slow_case);\n+    \/\/ done\n+    bind(done);\n+  }\n@@ -111,6 +121,9 @@\n-  \/\/ load displaced header\n-  movptr(hdr, Address(disp_hdr, 0));\n-  \/\/ if the loaded hdr is NULL we had recursive locking\n-  testptr(hdr, hdr);\n-  \/\/ if we had recursive locking, we are done\n-  jcc(Assembler::zero, done);\n+  if (!UseFastLocking) {\n+    \/\/ load displaced header\n+    movptr(hdr, Address(disp_hdr, 0));\n+    \/\/ if the loaded hdr is NULL we had recursive locking\n+    testptr(hdr, hdr);\n+    \/\/ if we had recursive locking, we are done\n+    jcc(Assembler::zero, done);\n+  }\n+\n@@ -119,11 +132,16 @@\n-\n-  \/\/ test if object header is pointing to the displaced header, and if so, restore\n-  \/\/ the displaced header in the object - if the object header is not pointing to\n-  \/\/ the displaced header, get the object header instead\n-  MacroAssembler::lock(); \/\/ must be immediately before cmpxchg!\n-  cmpxchgptr(hdr, Address(obj, hdr_offset));\n-  \/\/ if the object header was not pointing to the displaced header,\n-  \/\/ we do unlocking via runtime call\n-  jcc(Assembler::notEqual, slow_case);\n-  \/\/ done\n-  bind(done);\n+  if (UseFastLocking) {\n+    movptr(disp_hdr, Address(obj, hdr_offset));\n+    andptr(disp_hdr, ~(int32_t)markWord::lock_mask_in_place);\n+    fast_unlock_impl(obj, disp_hdr, hdr, slow_case);\n+  } else {\n+    \/\/ test if object header is pointing to the displaced header, and if so, restore\n+    \/\/ the displaced header in the object - if the object header is not pointing to\n+    \/\/ the displaced header, get the object header instead\n+    MacroAssembler::lock(); \/\/ must be immediately before cmpxchg!\n+    cmpxchgptr(hdr, Address(obj, hdr_offset));\n+    \/\/ if the object header was not pointing to the displaced header,\n+    \/\/ we do unlocking via runtime call\n+    jcc(Assembler::notEqual, slow_case);\n+    \/\/ done\n+  }\n+  bind(done);\n@@ -147,8 +165,5 @@\n-  assert_different_registers(obj, klass, len);\n-  movptr(Address(obj, oopDesc::mark_offset_in_bytes()), checked_cast<int32_t>(markWord::prototype().value()));\n-#ifdef _LP64\n-  if (UseCompressedClassPointers) { \/\/ Take care not to kill klass\n-    movptr(t1, klass);\n-    encode_klass_not_null(t1, rscratch1);\n-    movl(Address(obj, oopDesc::klass_offset_in_bytes()), t1);\n-  } else\n+  assert_different_registers(obj, klass, len, t1, t2);\n+  movptr(t1, Address(klass, Klass::prototype_header_offset()));\n+  movptr(Address(obj, oopDesc::mark_offset_in_bytes()), t1);\n+#ifndef _LP64\n+  movptr(Address(obj, oopDesc::klass_offset_in_bytes()), klass);\n@@ -156,3 +171,0 @@\n-  {\n-    movptr(Address(obj, oopDesc::klass_offset_in_bytes()), klass);\n-  }\n@@ -163,6 +175,0 @@\n-#ifdef _LP64\n-  else if (UseCompressedClassPointers) {\n-    xorptr(t1, t1);\n-    store_klass_gap(obj, t1);\n-  }\n-#endif\n@@ -245,1 +251,1 @@\n-void C1_MacroAssembler::allocate_array(Register obj, Register len, Register t1, Register t2, int header_size, Address::ScaleFactor f, Register klass, Label& slow_case) {\n+void C1_MacroAssembler::allocate_array(Register obj, Register len, Register t1, Register t2, int base_offset_in_bytes, Address::ScaleFactor f, Register klass, Label& slow_case) {\n@@ -258,1 +264,1 @@\n-  movptr(arr_size, header_size * BytesPerWord + MinObjAlignmentInBytesMask);\n+  movptr(arr_size, (int32_t)base_offset_in_bytes + MinObjAlignmentInBytesMask);\n@@ -268,1 +274,1 @@\n-  initialize_body(obj, arr_size, header_size * BytesPerWord, len_zero);\n+  initialize_body(obj, arr_size, base_offset_in_bytes, len_zero);\n@@ -302,1 +308,1 @@\n-void C1_MacroAssembler::build_frame(int frame_size_in_bytes, int bang_size_in_bytes) {\n+void C1_MacroAssembler::build_frame(int frame_size_in_bytes, int bang_size_in_bytes, int max_monitors) {\n@@ -323,0 +329,13 @@\n+#ifdef _LP64\n+  if (UseFastLocking && max_monitors > 0) {\n+    Label ok;\n+    movptr(rax, Address(r15_thread, JavaThread::lock_stack_current_offset()));\n+    addptr(rax, max_monitors * wordSize);\n+    cmpptr(rax, Address(r15_thread, JavaThread::lock_stack_limit_offset()));\n+    jcc(Assembler::less, ok);\n+    assert(StubRoutines::x86::check_lock_stack() != NULL, \"need runtime call stub\");\n+    call(RuntimeAddress(StubRoutines::x86::check_lock_stack()));\n+    bind(ok);\n+  }\n+#endif\n+\n","filename":"src\/hotspot\/cpu\/x86\/c1_MacroAssembler_x86.cpp","additions":93,"deletions":74,"binary":false,"changes":167,"status":"modified"},{"patch":"@@ -48,1 +48,1 @@\n-void C2_MacroAssembler::verified_entry(int framesize, int stack_bang_size, bool fp_mode_24b, bool is_stub) {\n+void C2_MacroAssembler::verified_entry(int framesize, int stack_bang_size, bool fp_mode_24b, bool is_stub, int max_monitors) {\n@@ -130,0 +130,14 @@\n+#ifdef _LP64\n+  if (UseFastLocking && max_monitors > 0) {\n+    C2CheckLockStackStub* stub = new (Compile::current()->comp_arena()) C2CheckLockStackStub();\n+    Compile::current()->output()->add_stub(stub);\n+    assert(!is_stub, \"only methods have monitors\");\n+    Register thread = r15_thread;\n+    movptr(rax, Address(thread, JavaThread::lock_stack_current_offset()));\n+    addptr(rax, max_monitors * oopSize);\n+    cmpptr(rax, Address(thread, JavaThread::lock_stack_limit_offset()));\n+    jcc(Assembler::greaterEqual, stub->entry());\n+    bind(stub->continuation());\n+  }\n+#endif\n+\n@@ -551,1 +565,1 @@\n-                                 Register scrReg, Register cx1Reg, Register cx2Reg,\n+                                 Register scrReg, Register cx1Reg, Register cx2Reg, Register thread,\n@@ -605,14 +619,32 @@\n-    \/\/ Attempt stack-locking ...\n-    orptr (tmpReg, markWord::unlocked_value);\n-    movptr(Address(boxReg, 0), tmpReg);          \/\/ Anticipate successful CAS\n-    lock();\n-    cmpxchgptr(boxReg, Address(objReg, oopDesc::mark_offset_in_bytes()));      \/\/ Updates tmpReg\n-    jcc(Assembler::equal, COUNT);           \/\/ Success\n-\n-    \/\/ Recursive locking.\n-    \/\/ The object is stack-locked: markword contains stack pointer to BasicLock.\n-    \/\/ Locked by current thread if difference with current SP is less than one page.\n-    subptr(tmpReg, rsp);\n-    \/\/ Next instruction set ZFlag == 1 (Success) if difference is less then one page.\n-    andptr(tmpReg, (int32_t) (NOT_LP64(0xFFFFF003) LP64_ONLY(7 - (int)os::vm_page_size())) );\n-    movptr(Address(boxReg, 0), tmpReg);\n+    if (UseFastLocking) {\n+#ifdef _LP64\n+      fast_lock_impl(objReg, tmpReg, thread, scrReg, NO_COUNT, false);\n+      jmp(COUNT);\n+#else\n+      \/\/ We can not emit the lock-stack-check in verified_entry() because we don't have enough\n+      \/\/ registers (for thread ptr). Therefore we have to emit the lock-stack-check in\n+      \/\/ fast_lock_impl(). However, that check can take a slow-path with ZF=1, therefore\n+      \/\/ we need to handle it specially and force ZF=0 before taking the actual slow-path.\n+      Label slow;\n+      fast_lock_impl(objReg, tmpReg, thread, scrReg, slow);\n+      jmp(COUNT);\n+      bind(slow);\n+      testptr(objReg, objReg); \/\/ ZF=0 to indicate failure\n+      jmp(NO_COUNT);\n+#endif\n+    } else {\n+      \/\/ Attempt stack-locking ...\n+      orptr (tmpReg, markWord::unlocked_value);\n+      movptr(Address(boxReg, 0), tmpReg);          \/\/ Anticipate successful CAS\n+      lock();\n+      cmpxchgptr(boxReg, Address(objReg, oopDesc::mark_offset_in_bytes()));      \/\/ Updates tmpReg\n+      jcc(Assembler::equal, COUNT);           \/\/ Success\n+\n+      \/\/ Recursive locking.\n+      \/\/ The object is stack-locked: markword contains stack pointer to BasicLock.\n+      \/\/ Locked by current thread if difference with current SP is less than one page.\n+      subptr(tmpReg, rsp);\n+      \/\/ Next instruction set ZFlag == 1 (Success) if difference is less then one page.\n+      andptr(tmpReg, (int32_t) (NOT_LP64(0xFFFFF003) LP64_ONLY(7 - (int)os::vm_page_size())) );\n+      movptr(Address(boxReg, 0), tmpReg);\n+    }\n@@ -662,1 +694,1 @@\n-  cmpxchgptr(scrReg, Address(boxReg, OM_OFFSET_NO_MONITOR_VALUE_TAG(owner)));\n+  cmpxchgptr(thread, Address(boxReg, OM_OFFSET_NO_MONITOR_VALUE_TAG(owner)));\n@@ -664,7 +696,0 @@\n-  \/\/ If we weren't able to swing _owner from NULL to the BasicLock\n-  \/\/ then take the slow path.\n-  jccb  (Assembler::notZero, NO_COUNT);\n-  \/\/ update _owner from BasicLock to thread\n-  get_thread (scrReg);                    \/\/ beware: clobbers ICCs\n-  movptr(Address(boxReg, OM_OFFSET_NO_MONITOR_VALUE_TAG(owner)), scrReg);\n-  xorptr(boxReg, boxReg);                 \/\/ set icc.ZFlag = 1 to indicate success\n@@ -776,1 +801,1 @@\n-  if (!UseHeavyMonitors) {\n+  if (!UseHeavyMonitors && !UseFastLocking) {\n@@ -783,1 +808,11 @@\n-    jccb   (Assembler::zero, Stacked);\n+#if INCLUDE_RTM_OPT\n+    if (UseFastLocking && use_rtm) {\n+      jcc(Assembler::zero, Stacked);\n+    } else\n+#endif\n+    jccb(Assembler::zero, Stacked);\n+    if (UseFastLocking) {\n+      \/\/ If the owner is ANONYMOUS, we need to fix it - in the slow-path.\n+      testb(Address(tmpReg, OM_OFFSET_NO_MONITOR_VALUE_TAG(owner)), (int32_t) (intptr_t) ANONYMOUS_OWNER);\n+      jcc(Assembler::notEqual, NO_COUNT);\n+    }\n@@ -795,1 +830,1 @@\n-    jmpb(DONE_LABEL);\n+    jmp(DONE_LABEL);\n@@ -909,3 +944,9 @@\n-    movptr(tmpReg, Address (boxReg, 0));      \/\/ re-fetch\n-    lock();\n-    cmpxchgptr(tmpReg, Address(objReg, oopDesc::mark_offset_in_bytes())); \/\/ Uses RAX which is box\n+    if (UseFastLocking) {\n+      mov(boxReg, tmpReg);\n+      fast_unlock_impl(objReg, boxReg, tmpReg, NO_COUNT);\n+      jmp(COUNT);\n+    } else {\n+      movptr(tmpReg, Address (boxReg, 0));      \/\/ re-fetch\n+      lock();\n+      cmpxchgptr(tmpReg, Address(objReg, oopDesc::mark_offset_in_bytes())); \/\/ Uses RAX which is box\n+    }\n","filename":"src\/hotspot\/cpu\/x86\/c2_MacroAssembler_x86.cpp","additions":71,"deletions":30,"binary":false,"changes":101,"status":"modified"},{"patch":"@@ -1226,53 +1226,65 @@\n-    \/\/ Load immediate 1 into swap_reg %rax\n-    movl(swap_reg, 1);\n-\n-    \/\/ Load (object->mark() | 1) into swap_reg %rax\n-    orptr(swap_reg, Address(obj_reg, oopDesc::mark_offset_in_bytes()));\n-\n-    \/\/ Save (object->mark() | 1) into BasicLock's displaced header\n-    movptr(Address(lock_reg, mark_offset), swap_reg);\n-\n-    assert(lock_offset == 0,\n-           \"displaced header must be first word in BasicObjectLock\");\n-\n-    lock();\n-    cmpxchgptr(lock_reg, Address(obj_reg, oopDesc::mark_offset_in_bytes()));\n-    jcc(Assembler::zero, count_locking);\n-\n-    const int zero_bits = LP64_ONLY(7) NOT_LP64(3);\n-\n-    \/\/ Fast check for recursive lock.\n-    \/\/\n-    \/\/ Can apply the optimization only if this is a stack lock\n-    \/\/ allocated in this thread. For efficiency, we can focus on\n-    \/\/ recently allocated stack locks (instead of reading the stack\n-    \/\/ base and checking whether 'mark' points inside the current\n-    \/\/ thread stack):\n-    \/\/  1) (mark & zero_bits) == 0, and\n-    \/\/  2) rsp <= mark < mark + os::pagesize()\n-    \/\/\n-    \/\/ Warning: rsp + os::pagesize can overflow the stack base. We must\n-    \/\/ neither apply the optimization for an inflated lock allocated\n-    \/\/ just above the thread stack (this is why condition 1 matters)\n-    \/\/ nor apply the optimization if the stack lock is inside the stack\n-    \/\/ of another thread. The latter is avoided even in case of overflow\n-    \/\/ because we have guard pages at the end of all stacks. Hence, if\n-    \/\/ we go over the stack base and hit the stack of another thread,\n-    \/\/ this should not be in a writeable area that could contain a\n-    \/\/ stack lock allocated by that thread. As a consequence, a stack\n-    \/\/ lock less than page size away from rsp is guaranteed to be\n-    \/\/ owned by the current thread.\n-    \/\/\n-    \/\/ These 3 tests can be done by evaluating the following\n-    \/\/ expression: ((mark - rsp) & (zero_bits - os::vm_page_size())),\n-    \/\/ assuming both stack pointer and pagesize have their\n-    \/\/ least significant bits clear.\n-    \/\/ NOTE: the mark is in swap_reg %rax as the result of cmpxchg\n-    subptr(swap_reg, rsp);\n-    andptr(swap_reg, zero_bits - (int)os::vm_page_size());\n-\n-    \/\/ Save the test result, for recursive case, the result is zero\n-    movptr(Address(lock_reg, mark_offset), swap_reg);\n-    jcc(Assembler::notZero, slow_case);\n-\n-    bind(count_locking);\n+    if (UseFastLocking) {\n+#ifdef _LP64\n+      const Register thread = r15_thread;\n+#else\n+      const Register thread = lock_reg;\n+      get_thread(thread);\n+#endif\n+      \/\/ Load object header, prepare for CAS from unlocked to locked.\n+      movptr(swap_reg, Address(obj_reg, oopDesc::mark_offset_in_bytes()));\n+      fast_lock_impl(obj_reg, swap_reg, thread, tmp_reg, slow_case);\n+    } else {\n+      \/\/ Load immediate 1 into swap_reg %rax\n+      movl(swap_reg, 1);\n+\n+      \/\/ Load (object->mark() | 1) into swap_reg %rax\n+      orptr(swap_reg, Address(obj_reg, oopDesc::mark_offset_in_bytes()));\n+\n+      \/\/ Save (object->mark() | 1) into BasicLock's displaced header\n+      movptr(Address(lock_reg, mark_offset), swap_reg);\n+\n+      assert(lock_offset == 0,\n+             \"displaced header must be first word in BasicObjectLock\");\n+\n+      lock();\n+      cmpxchgptr(lock_reg, Address(obj_reg, oopDesc::mark_offset_in_bytes()));\n+      jcc(Assembler::zero, count_locking);\n+\n+      const int zero_bits = LP64_ONLY(7) NOT_LP64(3);\n+\n+      \/\/ Fast check for recursive lock.\n+      \/\/\n+      \/\/ Can apply the optimization only if this is a stack lock\n+      \/\/ allocated in this thread. For efficiency, we can focus on\n+      \/\/ recently allocated stack locks (instead of reading the stack\n+      \/\/ base and checking whether 'mark' points inside the current\n+      \/\/ thread stack):\n+      \/\/  1) (mark & zero_bits) == 0, and\n+      \/\/  2) rsp <= mark < mark + os::pagesize()\n+      \/\/\n+      \/\/ Warning: rsp + os::pagesize can overflow the stack base. We must\n+      \/\/ neither apply the optimization for an inflated lock allocated\n+      \/\/ just above the thread stack (this is why condition 1 matters)\n+      \/\/ nor apply the optimization if the stack lock is inside the stack\n+      \/\/ of another thread. The latter is avoided even in case of overflow\n+      \/\/ because we have guard pages at the end of all stacks. Hence, if\n+      \/\/ we go over the stack base and hit the stack of another thread,\n+      \/\/ this should not be in a writeable area that could contain a\n+      \/\/ stack lock allocated by that thread. As a consequence, a stack\n+      \/\/ lock less than page size away from rsp is guaranteed to be\n+      \/\/ owned by the current thread.\n+      \/\/\n+      \/\/ These 3 tests can be done by evaluating the following\n+      \/\/ expression: ((mark - rsp) & (zero_bits - os::vm_page_size())),\n+      \/\/ assuming both stack pointer and pagesize have their\n+      \/\/ least significant bits clear.\n+      \/\/ NOTE: the mark is in swap_reg %rax as the result of cmpxchg\n+      subptr(swap_reg, rsp);\n+      andptr(swap_reg, zero_bits - (int)os::vm_page_size());\n+\n+      \/\/ Save the test result, for recursive case, the result is zero\n+      movptr(Address(lock_reg, mark_offset), swap_reg);\n+      jcc(Assembler::notZero, slow_case);\n+\n+      bind(count_locking);\n+    }\n@@ -1287,1 +1299,1 @@\n-            lock_reg);\n+            UseFastLocking ? obj_reg : lock_reg);\n@@ -1321,3 +1333,5 @@\n-    \/\/ Convert from BasicObjectLock structure to object and BasicLock\n-    \/\/ structure Store the BasicLock address into %rax\n-    lea(swap_reg, Address(lock_reg, BasicObjectLock::lock_offset_in_bytes()));\n+    if (!UseFastLocking) {\n+      \/\/ Convert from BasicObjectLock structure to object and BasicLock\n+      \/\/ structure Store the BasicLock address into %rax\n+      lea(swap_reg, Address(lock_reg, BasicObjectLock::lock_offset_in_bytes()));\n+    }\n@@ -1331,3 +1345,18 @@\n-    \/\/ Load the old header from BasicLock structure\n-    movptr(header_reg, Address(swap_reg,\n-                               BasicLock::displaced_header_offset_in_bytes()));\n+    if (UseFastLocking) {\n+#ifdef _LP64\n+      const Register thread = r15_thread;\n+#else\n+      const Register thread = header_reg;\n+      get_thread(thread);\n+#endif\n+      \/\/ Handle unstructured locking.\n+      cmpptr(obj_reg, Address(thread, JavaThread::lock_stack_current_offset()));\n+      jcc(Assembler::notEqual, slow_case);\n+      \/\/ Try to swing header from locked to unlock.\n+      movptr(swap_reg, Address(obj_reg, oopDesc::mark_offset_in_bytes()));\n+      andptr(swap_reg, ~(int32_t)markWord::lock_mask_in_place);\n+      fast_unlock_impl(obj_reg, swap_reg, header_reg, slow_case);\n+    } else {\n+      \/\/ Load the old header from BasicLock structure\n+      movptr(header_reg, Address(swap_reg,\n+                                 BasicLock::displaced_header_offset_in_bytes()));\n@@ -1335,2 +1364,2 @@\n-    \/\/ Test for recursion\n-    testptr(header_reg, header_reg);\n+      \/\/ Test for recursion\n+      testptr(header_reg, header_reg);\n@@ -1338,2 +1367,2 @@\n-    \/\/ zero for recursive case\n-    jcc(Assembler::zero, count_locking);\n+      \/\/ zero for recursive case\n+      jcc(Assembler::zero, count_locking);\n@@ -1341,3 +1370,3 @@\n-    \/\/ Atomic swap back the old header\n-    lock();\n-    cmpxchgptr(header_reg, Address(obj_reg, oopDesc::mark_offset_in_bytes()));\n+      \/\/ Atomic swap back the old header\n+      lock();\n+      cmpxchgptr(header_reg, Address(obj_reg, oopDesc::mark_offset_in_bytes()));\n@@ -1345,2 +1374,2 @@\n-    \/\/ zero for simple unlock of a stack-lock case\n-    jcc(Assembler::notZero, slow_case);\n+      \/\/ zero for simple unlock of a stack-lock case\n+      jcc(Assembler::notZero, slow_case);\n@@ -1348,1 +1377,2 @@\n-    bind(count_locking);\n+      bind(count_locking);\n+    }\n","filename":"src\/hotspot\/cpu\/x86\/interp_masm_x86.cpp","additions":100,"deletions":70,"binary":false,"changes":170,"status":"modified"},{"patch":"@@ -41,0 +41,1 @@\n+#include \"oops\/compressedKlass.inline.hpp\"\n@@ -4173,1 +4174,1 @@\n-  assert((offset_in_bytes & (BytesPerWord - 1)) == 0, \"offset must be a multiple of BytesPerWord\");\n+  assert((offset_in_bytes & (BytesPerInt - 1)) == 0, \"offset must be a multiple of BytesPerInt\");\n@@ -4179,0 +4180,13 @@\n+  \/\/ Emit single 32bit store to clear leading bytes, if necessary.\n+  xorptr(temp, temp);    \/\/ use _zero reg to clear memory (shorter code)\n+#ifdef _LP64\n+  if (!is_aligned(offset_in_bytes, BytesPerWord)) {\n+    movl(Address(address, offset_in_bytes), temp);\n+    offset_in_bytes += BytesPerInt;\n+    decrement(length_in_bytes, BytesPerInt);\n+  }\n+  assert((offset_in_bytes & (BytesPerWord - 1)) == 0, \"offset must be a multiple of BytesPerWord\");\n+  testptr(length_in_bytes, length_in_bytes);\n+  jcc(Assembler::zero, done);\n+#endif\n+\n@@ -4191,1 +4205,0 @@\n-  xorptr(temp, temp);    \/\/ use _zero reg to clear memory (shorter code)\n@@ -5123,0 +5136,17 @@\n+#ifdef _LP64\n+void MacroAssembler::load_nklass(Register dst, Register src) {\n+  assert(UseCompressedClassPointers, \"expect compressed class pointers\");\n+\n+  Label fast;\n+  movq(dst, Address(src, oopDesc::mark_offset_in_bytes()));\n+  testb(dst, markWord::monitor_value);\n+  jccb(Assembler::zero, fast);\n+\n+  \/\/ Fetch displaced header\n+  movq(dst, Address(dst, OM_OFFSET_NO_MONITOR_VALUE_TAG(header)));\n+\n+  bind(fast);\n+  shrq(dst, markWord::klass_shift);\n+}\n+#endif\n+\n@@ -5127,4 +5157,5 @@\n-  if (UseCompressedClassPointers) {\n-    movl(dst, Address(src, oopDesc::klass_offset_in_bytes()));\n-    decode_klass_not_null(dst, tmp);\n-  } else\n+  assert(UseCompressedClassPointers, \"expect compressed class pointers\");\n+  load_nklass(dst, src);\n+  decode_klass_not_null(dst, tmp);\n+#else\n+  movptr(dst, Address(src, oopDesc::klass_offset_in_bytes()));\n@@ -5132,1 +5163,0 @@\n-    movptr(dst, Address(src, oopDesc::klass_offset_in_bytes()));\n@@ -5136,0 +5166,3 @@\n+#ifdef _LP64\n+  null_check(src, oopDesc::mark_offset_in_bytes());\n+#else\n@@ -5137,0 +5170,1 @@\n+#endif\n@@ -5140,10 +5174,3 @@\n-void MacroAssembler::store_klass(Register dst, Register src, Register tmp) {\n-  assert_different_registers(src, tmp);\n-  assert_different_registers(dst, tmp);\n-#ifdef _LP64\n-  if (UseCompressedClassPointers) {\n-    encode_klass_not_null(src, tmp);\n-    movl(Address(dst, oopDesc::klass_offset_in_bytes()), src);\n-  } else\n-#endif\n-    movptr(Address(dst, oopDesc::klass_offset_in_bytes()), src);\n+#ifndef _LP64\n+void MacroAssembler::store_klass(Register dst, Register src) {\n+  movptr(Address(dst, oopDesc::klass_offset_in_bytes()), src);\n@@ -5151,0 +5178,1 @@\n+#endif\n@@ -5198,7 +5226,0 @@\n-void MacroAssembler::store_klass_gap(Register dst, Register src) {\n-  if (UseCompressedClassPointers) {\n-    \/\/ Store to klass gap in destination\n-    movl(Address(dst, oopDesc::klass_gap_offset_in_bytes()), src);\n-  }\n-}\n-\n@@ -5356,0 +5377,62 @@\n+MacroAssembler::KlassDecodeMode MacroAssembler::_klass_decode_mode = KlassDecodeNone;\n+\n+\/\/ Returns a static string\n+const char* MacroAssembler::describe_klass_decode_mode(MacroAssembler::KlassDecodeMode mode) {\n+  switch (mode) {\n+  case KlassDecodeNone: return \"none\";\n+  case KlassDecodeZero: return \"zero\";\n+  case KlassDecodeXor:  return \"xor\";\n+  case KlassDecodeAdd:  return \"add\";\n+  default:\n+    ShouldNotReachHere();\n+  }\n+  return NULL;\n+}\n+\n+\/\/ Return the current narrow Klass pointer decode mode.\n+MacroAssembler::KlassDecodeMode MacroAssembler::klass_decode_mode() {\n+  if (_klass_decode_mode == KlassDecodeNone) {\n+    \/\/ First time initialization\n+    assert(UseCompressedClassPointers, \"not using compressed class pointers\");\n+    assert(Metaspace::initialized(), \"metaspace not initialized yet\");\n+\n+    _klass_decode_mode = klass_decode_mode_for_base(CompressedKlassPointers::base());\n+    guarantee(_klass_decode_mode != KlassDecodeNone,\n+              PTR_FORMAT \" is not a valid encoding base on aarch64\",\n+              p2i(CompressedKlassPointers::base()));\n+    log_info(metaspace)(\"klass decode mode initialized: %s\", describe_klass_decode_mode(_klass_decode_mode));\n+  }\n+  return _klass_decode_mode;\n+}\n+\n+\/\/ Given an arbitrary base address, return the KlassDecodeMode that would be used. Return KlassDecodeNone\n+\/\/ if base address is not valid for encoding.\n+MacroAssembler::KlassDecodeMode MacroAssembler::klass_decode_mode_for_base(address base) {\n+  assert(CompressedKlassPointers::shift() != 0, \"not lilliput?\");\n+\n+  const uint64_t base_u64 = (uint64_t) base;\n+\n+  if (base_u64 == 0) {\n+    return KlassDecodeZero;\n+  }\n+\n+  if ((base_u64 & (KlassEncodingMetaspaceMax - 1)) == 0) {\n+    return KlassDecodeXor;\n+  }\n+\n+  \/\/ Note that there is no point in optimizing for shift=3 since lilliput\n+  \/\/ will use larger shifts\n+\n+  \/\/ The add+shift mode for decode_and_move_klass_not_null() requires the base to be\n+  \/\/  shiftable-without-loss. So, this is the minimum restriction on x64 for a valid\n+  \/\/  encoding base. This does not matter in reality since the shift values we use for\n+  \/\/  Lilliput, while large, won't be larger than a page size. And the encoding base\n+  \/\/  will be quite likely page aligned since it usually falls to the beginning of\n+  \/\/  either CDS or CCS.\n+  if ((base_u64 & (KlassAlignmentInBytes - 1)) == 0) {\n+    return KlassDecodeAdd;\n+  }\n+\n+  return KlassDecodeNone;\n+}\n+\n@@ -5358,1 +5441,12 @@\n-  if (CompressedKlassPointers::base() != NULL) {\n+  switch (klass_decode_mode()) {\n+  case KlassDecodeZero: {\n+    shrq(r, CompressedKlassPointers::shift());\n+    break;\n+  }\n+  case KlassDecodeXor: {\n+    mov64(tmp, (int64_t)CompressedKlassPointers::base());\n+    xorq(r, tmp);\n+    shrq(r, CompressedKlassPointers::shift());\n+    break;\n+  }\n+  case KlassDecodeAdd: {\n@@ -5361,0 +5455,2 @@\n+    shrq(r, CompressedKlassPointers::shift());\n+    break;\n@@ -5362,3 +5458,2 @@\n-  if (CompressedKlassPointers::shift() != 0) {\n-    assert (LogKlassAlignmentInBytes == CompressedKlassPointers::shift(), \"decode alg wrong\");\n-    shrq(r, LogKlassAlignmentInBytes);\n+  default:\n+    ShouldNotReachHere();\n@@ -5370,1 +5465,13 @@\n-  if (CompressedKlassPointers::base() != NULL) {\n+  switch (klass_decode_mode()) {\n+  case KlassDecodeZero: {\n+    movptr(dst, src);\n+    shrq(dst, CompressedKlassPointers::shift());\n+    break;\n+  }\n+  case KlassDecodeXor: {\n+    mov64(dst, (int64_t)CompressedKlassPointers::base());\n+    xorq(dst, src);\n+    shrq(dst, CompressedKlassPointers::shift());\n+    break;\n+  }\n+  case KlassDecodeAdd: {\n@@ -5373,2 +5480,2 @@\n-  } else {\n-    movptr(dst, src);\n+    shrq(dst, CompressedKlassPointers::shift());\n+    break;\n@@ -5376,3 +5483,2 @@\n-  if (CompressedKlassPointers::shift() != 0) {\n-    assert (LogKlassAlignmentInBytes == CompressedKlassPointers::shift(), \"decode alg wrong\");\n-    shrq(dst, LogKlassAlignmentInBytes);\n+  default:\n+    ShouldNotReachHere();\n@@ -5384,8 +5490,5 @@\n-  \/\/ Note: it will change flags\n-  assert(UseCompressedClassPointers, \"should only be used for compressed headers\");\n-  \/\/ Cannot assert, unverified entry point counts instructions (see .ad file)\n-  \/\/ vtableStubs also counts instructions in pd_code_size_limit.\n-  \/\/ Also do not verify_oop as this is called by verify_oop.\n-  if (CompressedKlassPointers::shift() != 0) {\n-    assert(LogKlassAlignmentInBytes == CompressedKlassPointers::shift(), \"decode alg wrong\");\n-    shlq(r, LogKlassAlignmentInBytes);\n+  const uint64_t base_u64 = (uint64_t)CompressedKlassPointers::base();\n+  switch (klass_decode_mode()) {\n+  case KlassDecodeZero: {\n+    shlq(r, CompressedKlassPointers::shift());\n+    break;\n@@ -5393,2 +5496,11 @@\n-  if (CompressedKlassPointers::base() != NULL) {\n-    mov64(tmp, (int64_t)CompressedKlassPointers::base());\n+  case KlassDecodeXor: {\n+    assert((base_u64 & (KlassEncodingMetaspaceMax - 1)) == 0,\n+           \"base \" UINT64_FORMAT_X \" invalid for xor mode\", base_u64); \/\/ should have been handled at VM init.\n+    shlq(r, CompressedKlassPointers::shift());\n+    mov64(tmp, base_u64);\n+    xorq(r, tmp);\n+    break;\n+  }\n+  case KlassDecodeAdd: {\n+    shlq(r, CompressedKlassPointers::shift());\n+    mov64(tmp, base_u64);\n@@ -5396,0 +5508,4 @@\n+    break;\n+  }\n+  default:\n+    ShouldNotReachHere();\n@@ -5401,3 +5517,1 @@\n-  \/\/ Note: it will change flags\n-  assert (UseCompressedClassPointers, \"should only be used for compressed headers\");\n-  \/\/ Cannot assert, unverified entry point counts instructions (see .ad file)\n+  \/\/ Note: Cannot assert, unverified entry point counts instructions (see .ad file)\n@@ -5407,18 +5521,28 @@\n-  if (CompressedKlassPointers::base() == NULL &&\n-      CompressedKlassPointers::shift() == 0) {\n-    \/\/ The best case scenario is that there is no base or shift. Then it is already\n-    \/\/ a pointer that needs nothing but a register rename.\n-    movl(dst, src);\n-  } else {\n-    if (CompressedKlassPointers::base() != NULL) {\n-      mov64(dst, (int64_t)CompressedKlassPointers::base());\n-    } else {\n-      xorq(dst, dst);\n-    }\n-    if (CompressedKlassPointers::shift() != 0) {\n-      assert(LogKlassAlignmentInBytes == CompressedKlassPointers::shift(), \"decode alg wrong\");\n-      assert(LogKlassAlignmentInBytes == Address::times_8, \"klass not aligned on 64bits?\");\n-      leaq(dst, Address(dst, src, Address::times_8, 0));\n-    } else {\n-      addq(dst, src);\n-    }\n+  const uint64_t base_u64 = (uint64_t)CompressedKlassPointers::base();\n+\n+  switch (klass_decode_mode()) {\n+  case KlassDecodeZero: {\n+    movq(dst, src);\n+    shlq(dst, CompressedKlassPointers::shift());\n+    break;\n+  }\n+  case KlassDecodeXor: {\n+    assert((base_u64 & (KlassEncodingMetaspaceMax - 1)) == 0,\n+           \"base \" UINT64_FORMAT_X \" invalid for xor mode\", base_u64); \/\/ should have been handled at VM init.\n+    const uint64_t base_right_shifted = base_u64 >> CompressedKlassPointers::shift();\n+    mov64(dst, base_right_shifted);\n+    xorq(dst, src);\n+    shlq(dst, CompressedKlassPointers::shift());\n+    break;\n+  }\n+  case KlassDecodeAdd: {\n+    assert((base_u64 & (KlassAlignmentInBytes - 1)) == 0,\n+           \"base \" UINT64_FORMAT_X \" invalid for add mode\", base_u64); \/\/ should have been handled at VM init.\n+    const uint64_t base_right_shifted = base_u64 >> CompressedKlassPointers::shift();\n+    mov64(dst, base_right_shifted);\n+    addq(dst, src);\n+    shlq(dst, CompressedKlassPointers::shift());\n+    break;\n+  }\n+  default:\n+    ShouldNotReachHere();\n@@ -9679,0 +9803,58 @@\n+\n+void MacroAssembler::fast_lock_impl(Register obj, Register hdr, Register thread, Register tmp, Label& slow, bool rt_check_stack) {\n+  assert(hdr == rax, \"header must be in rax for cmpxchg\");\n+  assert_different_registers(obj, hdr, thread, tmp);\n+\n+  \/\/ First we need to check if the lock-stack has room for pushing the object reference.\n+  if (rt_check_stack) {\n+    movptr(tmp, Address(thread, JavaThread::lock_stack_current_offset()));\n+    cmpptr(tmp, Address(thread, JavaThread::lock_stack_limit_offset()));\n+    jcc(Assembler::greaterEqual, slow);\n+  }\n+#ifdef ASSERT\n+  else {\n+    Label ok;\n+    movptr(tmp, Address(thread, JavaThread::lock_stack_current_offset()));\n+    cmpptr(tmp, Address(thread, JavaThread::lock_stack_limit_offset()));\n+    jcc(Assembler::less, ok);\n+    stop(\"Not enough room in lock stack; should have been checked in the method prologue\");\n+    bind(ok);\n+  }\n+#endif\n+\n+  \/\/ Now we attempt to take the fast-lock.\n+  \/\/ Clear lowest two header bits (locked state).\n+  andptr(hdr, ~(int32_t)markWord::lock_mask_in_place);\n+  movptr(tmp, hdr);\n+  \/\/ Set lowest bit (unlocked state).\n+  orptr(hdr, markWord::unlocked_value);\n+  lock();\n+  cmpxchgptr(tmp, Address(obj, oopDesc::mark_offset_in_bytes()));\n+  jcc(Assembler::notEqual, slow);\n+\n+  \/\/ If successful, push object to lock-stack.\n+  movptr(tmp, Address(thread, JavaThread::lock_stack_current_offset()));\n+  movptr(Address(tmp, 0), obj);\n+  increment(tmp, oopSize);\n+  movptr(Address(thread, JavaThread::lock_stack_current_offset()), tmp);\n+}\n+\n+void MacroAssembler::fast_unlock_impl(Register obj, Register hdr, Register tmp, Label& slow) {\n+  assert(hdr == rax, \"header must be in rax for cmpxchg\");\n+  assert_different_registers(obj, hdr, tmp);\n+\n+  \/\/ Mark-word must be 00 now, try to swing it back to 01 (unlocked)\n+  movptr(tmp, hdr); \/\/ The expected old value\n+  orptr(tmp, markWord::unlocked_value);\n+  lock();\n+  cmpxchgptr(tmp, Address(obj, oopDesc::mark_offset_in_bytes()));\n+  jcc(Assembler::notEqual, slow);\n+  \/\/ Pop the lock object from the lock-stack.\n+#ifdef _LP64\n+  const Register thread = r15_thread;\n+#else\n+  const Register thread = rax;\n+  get_thread(rax);\n+#endif\n+  subptr(Address(thread, JavaThread::lock_stack_current_offset()), oopSize);\n+}\n","filename":"src\/hotspot\/cpu\/x86\/macroAssembler_x86.cpp","additions":247,"deletions":65,"binary":false,"changes":312,"status":"modified"},{"patch":"@@ -82,0 +82,22 @@\n+ public:\n+\n+  enum KlassDecodeMode {\n+    KlassDecodeNone,\n+    KlassDecodeZero,\n+    KlassDecodeXor,\n+    KlassDecodeAdd\n+  };\n+\n+  \/\/ Return the current narrow Klass pointer decode mode. Initialized on first call.\n+  static KlassDecodeMode klass_decode_mode();\n+\n+  \/\/ Given an arbitrary base address, return the KlassDecodeMode that would be used. Return KlassDecodeNone\n+  \/\/ if base address is not valid for encoding.\n+  static KlassDecodeMode klass_decode_mode_for_base(address base);\n+\n+  \/\/ Returns a static string\n+  static const char* describe_klass_decode_mode(KlassDecodeMode mode);\n+\n+ private:\n+  static KlassDecodeMode _klass_decode_mode;\n+\n@@ -351,0 +373,5 @@\n+#ifdef _LP64\n+  void load_nklass(Register dst, Register src);\n+#else\n+  void store_klass(Register dst, Register src);\n+#endif\n@@ -353,1 +380,0 @@\n-  void store_klass(Register dst, Register src, Register tmp);\n@@ -372,2 +398,0 @@\n-  void store_klass_gap(Register dst, Register src);\n-\n@@ -1998,0 +2022,2 @@\n+  void fast_lock_impl(Register obj, Register hdr, Register thread, Register tmp, Label& slow, bool rt_check_stack = true);\n+  void fast_unlock_impl(Register obj, Register hdr, Register tmp, Label& slow);\n","filename":"src\/hotspot\/cpu\/x86\/macroAssembler_x86.hpp","additions":29,"deletions":3,"binary":false,"changes":32,"status":"modified"},{"patch":"@@ -1680,30 +1680,36 @@\n-      \/\/ Load immediate 1 into swap_reg %rax,\n-      __ movptr(swap_reg, 1);\n-\n-      \/\/ Load (object->mark() | 1) into swap_reg %rax,\n-      __ orptr(swap_reg, Address(obj_reg, oopDesc::mark_offset_in_bytes()));\n-\n-      \/\/ Save (object->mark() | 1) into BasicLock's displaced header\n-      __ movptr(Address(lock_reg, mark_word_offset), swap_reg);\n-\n-      \/\/ src -> dest iff dest == rax, else rax, <- dest\n-      \/\/ *obj_reg = lock_reg iff *obj_reg == rax, else rax, = *(obj_reg)\n-      __ lock();\n-      __ cmpxchgptr(lock_reg, Address(obj_reg, oopDesc::mark_offset_in_bytes()));\n-      __ jcc(Assembler::equal, count_mon);\n-\n-      \/\/ Test if the oopMark is an obvious stack pointer, i.e.,\n-      \/\/  1) (mark & 3) == 0, and\n-      \/\/  2) rsp <= mark < mark + os::pagesize()\n-      \/\/ These 3 tests can be done by evaluating the following\n-      \/\/ expression: ((mark - rsp) & (3 - os::vm_page_size())),\n-      \/\/ assuming both stack pointer and pagesize have their\n-      \/\/ least significant 2 bits clear.\n-      \/\/ NOTE: the oopMark is in swap_reg %rax, as the result of cmpxchg\n-\n-      __ subptr(swap_reg, rsp);\n-      __ andptr(swap_reg, 3 - (int)os::vm_page_size());\n-\n-      \/\/ Save the test result, for recursive case, the result is zero\n-      __ movptr(Address(lock_reg, mark_word_offset), swap_reg);\n-      __ jcc(Assembler::notEqual, slow_path_lock);\n+      if (UseFastLocking) {\n+        \/\/ Load object header\n+        __ movptr(swap_reg, Address(obj_reg, oopDesc::mark_offset_in_bytes()));\n+        __ fast_lock_impl(obj_reg, swap_reg, thread, lock_reg, slow_path_lock);\n+      } else {\n+        \/\/ Load immediate 1 into swap_reg %rax,\n+        __ movptr(swap_reg, 1);\n+\n+        \/\/ Load (object->mark() | 1) into swap_reg %rax,\n+        __ orptr(swap_reg, Address(obj_reg, oopDesc::mark_offset_in_bytes()));\n+\n+        \/\/ Save (object->mark() | 1) into BasicLock's displaced header\n+        __ movptr(Address(lock_reg, mark_word_offset), swap_reg);\n+\n+        \/\/ src -> dest iff dest == rax, else rax, <- dest\n+        \/\/ *obj_reg = lock_reg iff *obj_reg == rax, else rax, = *(obj_reg)\n+        __ lock();\n+        __ cmpxchgptr(lock_reg, Address(obj_reg, oopDesc::mark_offset_in_bytes()));\n+        __ jcc(Assembler::equal, count_mon);\n+\n+        \/\/ Test if the oopMark is an obvious stack pointer, i.e.,\n+        \/\/  1) (mark & 3) == 0, and\n+        \/\/  2) rsp <= mark < mark + os::pagesize()\n+        \/\/ These 3 tests can be done by evaluating the following\n+        \/\/ expression: ((mark - rsp) & (3 - os::vm_page_size())),\n+        \/\/ assuming both stack pointer and pagesize have their\n+        \/\/ least significant 2 bits clear.\n+        \/\/ NOTE: the oopMark is in swap_reg %rax, as the result of cmpxchg\n+\n+        __ subptr(swap_reg, rsp);\n+        __ andptr(swap_reg, 3 - (int)os::vm_page_size());\n+\n+        \/\/ Save the test result, for recursive case, the result is zero\n+        __ movptr(Address(lock_reg, mark_word_offset), swap_reg);\n+       __ jcc(Assembler::notEqual, slow_path_lock);\n+      }\n@@ -1833,1 +1839,1 @@\n-    if (!UseHeavyMonitors) {\n+    if (!UseHeavyMonitors && !UseFastLocking) {\n@@ -1849,12 +1855,18 @@\n-      \/\/  get old displaced header\n-      __ movptr(rbx, Address(rbp, lock_slot_rbp_offset));\n-\n-      \/\/ get address of the stack lock\n-      __ lea(rax, Address(rbp, lock_slot_rbp_offset));\n-\n-      \/\/ Atomic swap old header if oop still contains the stack lock\n-      \/\/ src -> dest iff dest == rax, else rax, <- dest\n-      \/\/ *obj_reg = rbx, iff *obj_reg == rax, else rax, = *(obj_reg)\n-      __ lock();\n-      __ cmpxchgptr(rbx, Address(obj_reg, oopDesc::mark_offset_in_bytes()));\n-      __ jcc(Assembler::notEqual, slow_path_unlock);\n+      if (UseFastLocking) {\n+        __ movptr(swap_reg, Address(obj_reg, oopDesc::mark_offset_in_bytes()));\n+        __ andptr(swap_reg, ~(int32_t)markWord::lock_mask_in_place);\n+        __ fast_unlock_impl(obj_reg, swap_reg, lock_reg, slow_path_unlock);\n+      } else {\n+        \/\/  get old displaced header\n+        __ movptr(rbx, Address(rbp, lock_slot_rbp_offset));\n+\n+        \/\/ get address of the stack lock\n+        __ lea(rax, Address(rbp, lock_slot_rbp_offset));\n+\n+        \/\/ Atomic swap old header if oop still contains the stack lock\n+        \/\/ src -> dest iff dest == rax, else rax, <- dest\n+        \/\/ *obj_reg = rbx, iff *obj_reg == rax, else rax, = *(obj_reg)\n+        __ lock();\n+        __ cmpxchgptr(rbx, Address(obj_reg, oopDesc::mark_offset_in_bytes()));\n+        __ jcc(Assembler::notEqual, slow_path_unlock);\n+      }\n","filename":"src\/hotspot\/cpu\/x86\/sharedRuntime_x86_32.cpp","additions":55,"deletions":43,"binary":false,"changes":98,"status":"modified"},{"patch":"@@ -2148,0 +2148,7 @@\n+      if (UseFastLocking) {\n+        \/\/ Load object header\n+        __ movptr(swap_reg, Address(obj_reg, oopDesc::mark_offset_in_bytes()));\n+        __ fast_lock_impl(obj_reg, swap_reg, r15_thread, rscratch1, slow_path_lock);\n+      } else {\n+        \/\/ Load immediate 1 into swap_reg %rax\n+        __ movl(swap_reg, 1);\n@@ -2149,5 +2156,2 @@\n-      \/\/ Load immediate 1 into swap_reg %rax\n-      __ movl(swap_reg, 1);\n-\n-      \/\/ Load (object->mark() | 1) into swap_reg %rax\n-      __ orptr(swap_reg, Address(obj_reg, oopDesc::mark_offset_in_bytes()));\n+        \/\/ Load (object->mark() | 1) into swap_reg %rax\n+        __ orptr(swap_reg, Address(obj_reg, oopDesc::mark_offset_in_bytes()));\n@@ -2155,2 +2159,2 @@\n-      \/\/ Save (object->mark() | 1) into BasicLock's displaced header\n-      __ movptr(Address(lock_reg, mark_word_offset), swap_reg);\n+        \/\/ Save (object->mark() | 1) into BasicLock's displaced header\n+        __ movptr(Address(lock_reg, mark_word_offset), swap_reg);\n@@ -2158,4 +2162,4 @@\n-      \/\/ src -> dest iff dest == rax else rax <- dest\n-      __ lock();\n-      __ cmpxchgptr(lock_reg, Address(obj_reg, oopDesc::mark_offset_in_bytes()));\n-      __ jcc(Assembler::equal, count_mon);\n+        \/\/ src -> dest iff dest == rax else rax <- dest\n+        __ lock();\n+        __ cmpxchgptr(lock_reg, Address(obj_reg, oopDesc::mark_offset_in_bytes()));\n+        __ jcc(Assembler::equal, count_mon);\n@@ -2163,1 +2167,1 @@\n-      \/\/ Hmm should this move to the slow path code area???\n+        \/\/ Hmm should this move to the slow path code area???\n@@ -2165,8 +2169,8 @@\n-      \/\/ Test if the oopMark is an obvious stack pointer, i.e.,\n-      \/\/  1) (mark & 3) == 0, and\n-      \/\/  2) rsp <= mark < mark + os::pagesize()\n-      \/\/ These 3 tests can be done by evaluating the following\n-      \/\/ expression: ((mark - rsp) & (3 - os::vm_page_size())),\n-      \/\/ assuming both stack pointer and pagesize have their\n-      \/\/ least significant 2 bits clear.\n-      \/\/ NOTE: the oopMark is in swap_reg %rax as the result of cmpxchg\n+        \/\/ Test if the oopMark is an obvious stack pointer, i.e.,\n+        \/\/  1) (mark & 3) == 0, and\n+        \/\/  2) rsp <= mark < mark + os::pagesize()\n+        \/\/ These 3 tests can be done by evaluating the following\n+        \/\/ expression: ((mark - rsp) & (3 - os::vm_page_size())),\n+        \/\/ assuming both stack pointer and pagesize have their\n+        \/\/ least significant 2 bits clear.\n+        \/\/ NOTE: the oopMark is in swap_reg %rax as the result of cmpxchg\n@@ -2174,2 +2178,2 @@\n-      __ subptr(swap_reg, rsp);\n-      __ andptr(swap_reg, 3 - (int)os::vm_page_size());\n+        __ subptr(swap_reg, rsp);\n+        __ andptr(swap_reg, 3 - (int)os::vm_page_size());\n@@ -2177,3 +2181,4 @@\n-      \/\/ Save the test result, for recursive case, the result is zero\n-      __ movptr(Address(lock_reg, mark_word_offset), swap_reg);\n-      __ jcc(Assembler::notEqual, slow_path_lock);\n+        \/\/ Save the test result, for recursive case, the result is zero\n+        __ movptr(Address(lock_reg, mark_word_offset), swap_reg);\n+        __ jcc(Assembler::notEqual, slow_path_lock);\n+      }\n@@ -2293,1 +2298,1 @@\n-    if (!UseHeavyMonitors) {\n+    if (!UseHeavyMonitors && !UseFastLocking) {\n@@ -2309,9 +2314,15 @@\n-      \/\/ get address of the stack lock\n-      __ lea(rax, Address(rsp, lock_slot_offset * VMRegImpl::stack_slot_size));\n-      \/\/  get old displaced header\n-      __ movptr(old_hdr, Address(rax, 0));\n-\n-      \/\/ Atomic swap old header if oop still contains the stack lock\n-      __ lock();\n-      __ cmpxchgptr(old_hdr, Address(obj_reg, oopDesc::mark_offset_in_bytes()));\n-      __ jcc(Assembler::notEqual, slow_path_unlock);\n+      if (UseFastLocking) {\n+        __ movptr(swap_reg, Address(obj_reg, oopDesc::mark_offset_in_bytes()));\n+        __ andptr(swap_reg, ~(int32_t)markWord::lock_mask_in_place);\n+        __ fast_unlock_impl(obj_reg, swap_reg, lock_reg, slow_path_unlock);\n+      } else {\n+        \/\/ get address of the stack lock\n+        __ lea(rax, Address(rsp, lock_slot_offset * VMRegImpl::stack_slot_size));\n+        \/\/  get old displaced header\n+        __ movptr(old_hdr, Address(rax, 0));\n+\n+        \/\/ Atomic swap old header if oop still contains the stack lock\n+        __ lock();\n+        __ cmpxchgptr(old_hdr, Address(obj_reg, oopDesc::mark_offset_in_bytes()));\n+        __ jcc(Assembler::notEqual, slow_path_unlock);\n+      }\n","filename":"src\/hotspot\/cpu\/x86\/sharedRuntime_x86_64.cpp","additions":46,"deletions":35,"binary":false,"changes":81,"status":"modified"},{"patch":"@@ -3186,0 +3186,49 @@\n+\/\/ Call runtime to ensure lock-stack size.\n+\/\/ Arguments:\n+\/\/ - c_rarg0: the required _limit pointer\n+address StubGenerator::generate_check_lock_stack() {\n+  __ align(CodeEntryAlignment);\n+  StubCodeMark mark(this, \"StubRoutines\", \"check_lock_stack\");\n+  address start = __ pc();\n+\n+  BLOCK_COMMENT(\"Entry:\");\n+  __ enter(); \/\/ save rbp\n+\n+  __ pusha();\n+\n+  \/\/ The method may have floats as arguments, and we must spill them before calling\n+  \/\/ the VM runtime.\n+  assert(Argument::n_float_register_parameters_j == 8, \"Assumption\");\n+  const int xmm_size = wordSize * 2;\n+  const int xmm_spill_size = xmm_size * Argument::n_float_register_parameters_j;\n+  __ subptr(rsp, xmm_spill_size);\n+  __ movdqu(Address(rsp, xmm_size * 7), xmm7);\n+  __ movdqu(Address(rsp, xmm_size * 6), xmm6);\n+  __ movdqu(Address(rsp, xmm_size * 5), xmm5);\n+  __ movdqu(Address(rsp, xmm_size * 4), xmm4);\n+  __ movdqu(Address(rsp, xmm_size * 3), xmm3);\n+  __ movdqu(Address(rsp, xmm_size * 2), xmm2);\n+  __ movdqu(Address(rsp, xmm_size * 1), xmm1);\n+  __ movdqu(Address(rsp, xmm_size * 0), xmm0);\n+\n+  __ call_VM_leaf(CAST_FROM_FN_PTR(address, static_cast<void (*)(oop*)>(LockStack::ensure_lock_stack_size)), rax);\n+\n+  __ movdqu(xmm0, Address(rsp, xmm_size * 0));\n+  __ movdqu(xmm1, Address(rsp, xmm_size * 1));\n+  __ movdqu(xmm2, Address(rsp, xmm_size * 2));\n+  __ movdqu(xmm3, Address(rsp, xmm_size * 3));\n+  __ movdqu(xmm4, Address(rsp, xmm_size * 4));\n+  __ movdqu(xmm5, Address(rsp, xmm_size * 5));\n+  __ movdqu(xmm6, Address(rsp, xmm_size * 6));\n+  __ movdqu(xmm7, Address(rsp, xmm_size * 7));\n+  __ addptr(rsp, xmm_spill_size);\n+\n+  __ popa();\n+\n+  __ leave();\n+\n+  __ ret(0);\n+\n+  return start;\n+}\n+\n@@ -4023,0 +4072,3 @@\n+  if (UseFastLocking) {\n+    StubRoutines::x86::_check_lock_stack = generate_check_lock_stack();\n+  }\n","filename":"src\/hotspot\/cpu\/x86\/stubGenerator_x86_64.cpp","additions":52,"deletions":0,"binary":false,"changes":52,"status":"modified"},{"patch":"@@ -468,0 +468,2 @@\n+  address generate_check_lock_stack();\n+\n","filename":"src\/hotspot\/cpu\/x86\/stubGenerator_x86_64.hpp","additions":2,"deletions":0,"binary":false,"changes":2,"status":"modified"},{"patch":"@@ -87,0 +87,1 @@\n+address StubRoutines::x86::_check_lock_stack = NULL;\n","filename":"src\/hotspot\/cpu\/x86\/stubRoutines_x86.cpp","additions":1,"deletions":0,"binary":false,"changes":1,"status":"modified"},{"patch":"@@ -129,0 +129,2 @@\n+  static address _check_lock_stack;\n+\n@@ -218,0 +220,2 @@\n+  static address check_lock_stack() { return _check_lock_stack; }\n+\n","filename":"src\/hotspot\/cpu\/x86\/stubRoutines_x86.hpp","additions":4,"deletions":0,"binary":false,"changes":4,"status":"modified"},{"patch":"@@ -3999,5 +3999,4 @@\n-    __ movptr(Address(rax, oopDesc::mark_offset_in_bytes()),\n-              (intptr_t)markWord::prototype().value()); \/\/ header\n-#ifdef _LP64\n-    __ xorl(rsi, rsi); \/\/ use zero reg to clear memory (shorter code)\n-    __ store_klass_gap(rax, rsi);  \/\/ zero klass gap for compressed oops\n+    __ movptr(rbx, Address(rcx, Klass::prototype_header_offset()));\n+    __ movptr(Address(rax, oopDesc::mark_offset_in_bytes ()), rbx);\n+#ifndef _LP64\n+    __ store_klass(rax, rcx);  \/\/ klass\n@@ -4006,1 +4005,0 @@\n-    __ store_klass(rax, rcx, rscratch1);  \/\/ klass\n","filename":"src\/hotspot\/cpu\/x86\/templateTable_x86.cpp","additions":4,"deletions":6,"binary":false,"changes":10,"status":"modified"},{"patch":"@@ -925,1 +925,2 @@\n-  __ verified_entry(framesize, C->output()->need_stack_bang(bangsize)?bangsize:0, false, C->stub_function() != NULL);\n+  int max_monitors = C->method() != NULL ? C->max_monitors() : 0;\n+  __ verified_entry(framesize, C->output()->need_stack_bang(bangsize)?bangsize:0, false, C->stub_function() != NULL, max_monitors);\n@@ -5315,1 +5316,1 @@\n-instruct loadNKlass(rRegN dst, memory mem)\n+instruct loadNKlass(rRegN dst, indOffset8 mem, rFlagsReg cr)\n@@ -5318,1 +5319,1 @@\n-\n+  effect(TEMP_DEF dst, KILL cr);\n@@ -5322,1 +5323,11 @@\n-    __ movl($dst$$Register, $mem$$Address);\n+    assert($mem$$disp == oopDesc::klass_offset_in_bytes(), \"expect correct offset 4, but got: %d\", $mem$$disp);\n+    assert($mem$$index == 4, \"expect no index register: %d\", $mem$$index);\n+    Register dst = $dst$$Register;\n+    Register obj = $mem$$base$$Register;\n+    C2LoadNKlassStub* stub = new (Compile::current()->comp_arena()) C2LoadNKlassStub(dst);\n+    Compile::current()->output()->add_stub(stub);\n+    __ movq(dst, Address(obj, oopDesc::mark_offset_in_bytes()));\n+    __ testb(dst, markWord::monitor_value);\n+    __ jcc(Assembler::notZero, stub->entry());\n+    __ bind(stub->continuation());\n+    __ shrq(dst, markWord::klass_shift);\n@@ -5324,1 +5335,1 @@\n-  ins_pipe(ialu_reg_mem); \/\/ XXX\n+  ins_pipe(pipe_slow); \/\/ XXX\n@@ -12676,0 +12687,3 @@\n+\/\/ Disabled because the compressed Klass* in header cannot be safely\n+\/\/ accessed. TODO: Re-enable it as soon as synchronization does not\n+\/\/ overload the upper header bits anymore.\n@@ -12678,0 +12692,1 @@\n+  predicate(false);\n@@ -13364,1 +13379,1 @@\n-                 $scr$$Register, $cx1$$Register, $cx2$$Register,\n+                 $scr$$Register, $cx1$$Register, $cx2$$Register, r15_thread,\n@@ -13380,1 +13395,1 @@\n-                 $scr$$Register, noreg, noreg, NULL, NULL, NULL, false, false);\n+                 $scr$$Register, noreg, noreg, r15_thread, NULL, NULL, NULL, false, false);\n","filename":"src\/hotspot\/cpu\/x86\/x86_64.ad","additions":22,"deletions":7,"binary":false,"changes":29,"status":"modified"},{"patch":"@@ -584,0 +584,18 @@\n+class LoadKlassStub: public CodeStub {\n+private:\n+  LIR_Opr          _result;\n+\n+public:\n+  LoadKlassStub(LIR_Opr result) :\n+    CodeStub(), _result(result) {};\n+\n+  virtual void emit_code(LIR_Assembler* e);\n+  virtual void visit(LIR_OpVisitState* visitor) {\n+    visitor->do_temp(_result);\n+    visitor->do_output(_result);\n+  }\n+#ifndef PRODUCT\n+virtual void print_name(outputStream* out) const { out->print(\"LoadKlassStub\"); }\n+#endif \/\/ PRODUCT\n+};\n+\n","filename":"src\/hotspot\/share\/c1\/c1_CodeStubs.hpp","additions":18,"deletions":0,"binary":false,"changes":18,"status":"modified"},{"patch":"@@ -387,0 +387,1 @@\n+    push_monitor();\n@@ -574,0 +575,1 @@\n+, _max_monitors(0)\n","filename":"src\/hotspot\/share\/c1\/c1_Compilation.cpp","additions":2,"deletions":0,"binary":false,"changes":2,"status":"modified"},{"patch":"@@ -43,0 +43,1 @@\n+#include \"oops\/compressedKlass.inline.hpp\"\n@@ -44,0 +45,1 @@\n+#include \"oops\/klass.inline.hpp\"\n@@ -223,2 +225,4 @@\n-    \/\/ See RunTimeClassInfo::get_for()\n-    _estimated_metaspaceobj_bytes += align_up(BytesPerWord, SharedSpaceObjectAlignment);\n+    \/\/ See ArchiveBuilder::make_shallow_copies: make sure we have enough space for both maximum\n+    \/\/ Klass alignment as well as the RuntimeInfo* pointer we will embed in front of a Klass.\n+    _estimated_metaspaceobj_bytes += align_up(BytesPerWord, KlassAlignmentInBytes) +\n+        align_up(sizeof(void*), SharedSpaceObjectAlignment);\n@@ -621,4 +625,5 @@\n-    \/\/ Save a pointer immediate in front of an InstanceKlass, so\n-    \/\/ we can do a quick lookup from InstanceKlass* -> RunTimeClassInfo*\n-    \/\/ without building another hashtable. See RunTimeClassInfo::get_for()\n-    \/\/ in systemDictionaryShared.cpp.\n+    \/\/ Reserve space for a pointer immediately in front of an InstanceKlass. That space will\n+    \/\/ later be used to store the RuntimeClassInfo* pointer directly in front of the archived\n+    \/\/ InstanceKlass, in order to have a quick lookup InstanceKlass* -> RunTimeClassInfo*\n+    \/\/ without building another hashtable. See RunTimeClassInfo::get_for()\/::set_for() for\n+    \/\/ details.\n@@ -630,0 +635,3 @@\n+    dest = dump_region->allocate(bytes, KlassAlignmentInBytes);\n+  } else {\n+    dest = dump_region->allocate(bytes);\n@@ -631,1 +639,0 @@\n-  dest = dump_region->allocate(bytes);\n@@ -650,1 +657,2 @@\n-  log_trace(cds)(\"Copy: \" PTR_FORMAT \" ==> \" PTR_FORMAT \" %d\", p2i(src), p2i(dest), bytes);\n+  log_trace(cds)(\"Copy: \" PTR_FORMAT \" ==> \" PTR_FORMAT \" %d (%s)\", p2i(src), p2i(dest), bytes,\n+                 MetaspaceObj::type_name(ref->msotype()));\n@@ -654,0 +662,2 @@\n+\n+  DEBUG_ONLY(_alloc_stats.verify((int)dump_region->used(), src_info->read_only()));\n@@ -750,0 +760,7 @@\n+    Klass* requested_k = to_requested(k);\n+#ifdef _LP64\n+    narrowKlass nk = CompressedKlassPointers::encode_not_null(requested_k, _requested_static_archive_bottom);\n+    k->set_prototype_header(markWord::prototype().set_narrow_klass(nk));\n+#else\n+    k->set_prototype_header(markWord::prototype());\n+#endif\n","filename":"src\/hotspot\/share\/cds\/archiveBuilder.cpp","additions":25,"deletions":8,"binary":false,"changes":33,"status":"modified"},{"patch":"@@ -31,0 +31,1 @@\n+#include \"memory\/metaspace\/metaspaceAlignment.hpp\"\n@@ -46,3 +47,13 @@\n-\/\/ Metaspace::allocate() requires that all blocks must be aligned with KlassAlignmentInBytes.\n-\/\/ We enforce the same alignment rule in blocks allocated from the shared space.\n-const int SharedSpaceObjectAlignment = KlassAlignmentInBytes;\n+\/\/ CDS has three alignments to deal with:\n+\/\/ - SharedSpaceObjectAlignment, always 8 bytes: used for placing arbitrary structures.\n+\/\/   These may contain 64-bit members (not larger, we know that much). Therefore we\n+\/\/   need to use 64-bit alignment on both 32-bit and 64-bit platforms. We reuse metaspace\n+\/\/   minimal alignment for this, which follows the same logic.\n+\/\/ - With CompressedClassPointers=1, we need to store Klass structures with a large\n+\/\/   alignment (Lilliput specific narrow Klass pointer encoding) - KlassAlignmentInBytes.\n+\/\/ - Header data and tags are squeezed in with word alignment, which happens to be 4 bytes\n+\/\/   on 32-bit. See ReadClosure::do_xxx() and DumpRegion::append_intptr().\n+const int SharedSpaceObjectAlignment = metaspace::MetaspaceMinAlignmentBytes;\n+\n+\/\/ standard alignment should be sufficient for storing 64-bit values.\n+STATIC_ASSERT(SharedSpaceObjectAlignment >= sizeof(uint64_t));\n","filename":"src\/hotspot\/share\/cds\/archiveBuilder.hpp","additions":14,"deletions":3,"binary":false,"changes":17,"status":"modified"},{"patch":"@@ -223,2 +223,2 @@\n-    oopDesc::set_mark(mem, markWord::prototype());\n-    oopDesc::release_set_klass(mem, k);\n+    narrowKlass nk = ArchiveBuilder::current()->get_requested_narrow_klass(k);\n+    oopDesc::release_set_mark(mem, markWord::prototype().set_narrow_klass(nk));\n@@ -303,1 +303,0 @@\n-  oopDesc::set_mark(mem, markWord::prototype());\n@@ -305,1 +304,1 @@\n-  cast_to_oop(mem)->set_narrow_klass(nk);\n+  oopDesc::set_mark(mem, markWord::prototype().set_narrow_klass(nk));\n@@ -487,1 +486,0 @@\n-  fake_oop->set_narrow_klass(nk);\n@@ -495,1 +493,1 @@\n-    fake_oop->set_mark(markWord::prototype().copy_set_hash(src_hash));\n+    fake_oop->set_mark(markWord::prototype().set_narrow_klass(nk).copy_set_hash(src_hash));\n","filename":"src\/hotspot\/share\/cds\/archiveHeapWriter.cpp","additions":4,"deletions":6,"binary":false,"changes":10,"status":"modified"},{"patch":"@@ -59,0 +59,1 @@\n+#include \"runtime\/safepoint.hpp\"\n","filename":"src\/hotspot\/share\/cds\/heapShared.cpp","additions":1,"deletions":0,"binary":false,"changes":1,"status":"modified"},{"patch":"@@ -61,0 +61,1 @@\n+#include \"oops\/compressedKlass.hpp\"\n@@ -1314,0 +1315,5 @@\n+#ifdef ASSERT\n+    if (UseCompressedClassPointers) {\n+      CompressedKlassPointers::verify_klass_pointer(record->_klass);\n+    }\n+#endif\n@@ -1315,1 +1321,0 @@\n-    assert(check_alignment(record->_klass), \"Address not aligned\");\n","filename":"src\/hotspot\/share\/classfile\/systemDictionaryShared.cpp","additions":6,"deletions":1,"binary":false,"changes":7,"status":"modified"},{"patch":"@@ -89,0 +89,2 @@\n+#include \"gc\/shared\/slidingForwarding.inline.hpp\"\n+#include \"gc\/shared\/suspendibleThreadSet.hpp\"\n@@ -1523,0 +1525,2 @@\n+  _forwarding = new SlidingForwarding(heap_rs.region(), HeapRegion::LogOfHRGrainBytes - LogHeapWordSize);\n+\n","filename":"src\/hotspot\/share\/gc\/g1\/g1CollectedHeap.cpp","additions":4,"deletions":0,"binary":false,"changes":4,"status":"modified"},{"patch":"@@ -88,0 +88,1 @@\n+class SlidingForwarding;\n@@ -255,0 +256,2 @@\n+  SlidingForwarding* _forwarding;\n+\n@@ -268,0 +271,4 @@\n+  SlidingForwarding* forwarding() const {\n+    return _forwarding;\n+  }\n+\n","filename":"src\/hotspot\/share\/gc\/g1\/g1CollectedHeap.hpp","additions":7,"deletions":0,"binary":false,"changes":7,"status":"modified"},{"patch":"@@ -45,0 +45,1 @@\n+#include \"gc\/shared\/slidingForwarding.hpp\"\n@@ -333,0 +334,2 @@\n+  _heap->forwarding()->clear();\n+\n@@ -344,3 +347,4 @@\n-  if (scope()->do_maximal_compaction() || !has_free_compaction_targets) {\n-    phase2c_prepare_serial_compaction();\n-  }\n+  \/\/ TODO: Disabled for now because it violates sliding-forwarding assumption.\n+\/\/  if (scope()->do_maximal_compaction() || !has_free_compaction_targets) {\n+\/\/    phase2c_prepare_serial_compaction();\n+\/\/  }\n@@ -365,55 +369,55 @@\n-uint G1FullCollector::truncate_parallel_cps() {\n-  uint lowest_current = (uint)-1;\n-  for (uint i = 0; i < workers(); i++) {\n-    G1FullGCCompactionPoint* cp = compaction_point(i);\n-    if (cp->has_regions()) {\n-      lowest_current = MIN2(lowest_current, cp->current_region()->hrm_index());\n-    }\n-  }\n-\n-  for (uint i = 0; i < workers(); i++) {\n-    G1FullGCCompactionPoint* cp = compaction_point(i);\n-    if (cp->has_regions()) {\n-      cp->remove_at_or_above(lowest_current);\n-    }\n-  }\n-  return lowest_current;\n-}\n-\n-void G1FullCollector::phase2c_prepare_serial_compaction() {\n-  GCTraceTime(Debug, gc, phases) debug(\"Phase 2: Prepare serial compaction\", scope()->timer());\n-  \/\/ At this point, we know that after parallel compaction there will be regions that\n-  \/\/ are partially compacted into. Thus, the last compaction region of all\n-  \/\/ compaction queues still have space in them. We try to re-compact these regions\n-  \/\/ in serial to avoid a premature OOM when the mutator wants to allocate the first\n-  \/\/ eden region after gc.\n-\n-  \/\/ For maximum compaction, we need to re-prepare all objects above the lowest\n-  \/\/ region among the current regions for all thread compaction points. It may\n-  \/\/ happen that due to the uneven distribution of objects to parallel threads, holes\n-  \/\/ have been created as threads compact to different target regions between the\n-  \/\/ lowest and the highest region in the tails of the compaction points.\n-\n-  uint start_serial = truncate_parallel_cps();\n-  assert(start_serial < _heap->max_reserved_regions(), \"Called on empty parallel compaction queues\");\n-\n-  G1FullGCCompactionPoint* serial_cp = serial_compaction_point();\n-  assert(!serial_cp->is_initialized(), \"sanity!\");\n-\n-  HeapRegion* start_hr = _heap->region_at(start_serial);\n-  serial_cp->add(start_hr);\n-  serial_cp->initialize(start_hr);\n-\n-  HeapWord* dense_prefix_top = compaction_top(start_hr);\n-  G1SerialRePrepareClosure re_prepare(serial_cp, dense_prefix_top);\n-\n-  for (uint i = start_serial + 1; i < _heap->max_reserved_regions(); i++) {\n-    if (is_compaction_target(i)) {\n-      HeapRegion* current = _heap->region_at(i);\n-      set_compaction_top(current, current->bottom());\n-      serial_cp->add(current);\n-      current->apply_to_marked_objects(mark_bitmap(), &re_prepare);\n-    }\n-  }\n-  serial_cp->update();\n-}\n+\/\/uint G1FullCollector::truncate_parallel_cps() {\n+\/\/  uint lowest_current = (uint)-1;\n+\/\/  for (uint i = 0; i < workers(); i++) {\n+\/\/    G1FullGCCompactionPoint* cp = compaction_point(i);\n+\/\/    if (cp->has_regions()) {\n+\/\/      lowest_current = MIN2(lowest_current, cp->current_region()->hrm_index());\n+\/\/    }\n+\/\/  }\n+\n+\/\/  for (uint i = 0; i < workers(); i++) {\n+\/\/    G1FullGCCompactionPoint* cp = compaction_point(i);\n+\/\/    if (cp->has_regions()) {\n+\/\/      cp->remove_at_or_above(lowest_current);\n+\/\/    }\n+\/\/  }\n+\/\/  return lowest_current;\n+\/\/}\n+\n+\/\/void G1FullCollector::phase2c_prepare_serial_compaction() {\n+\/\/  GCTraceTime(Debug, gc, phases) debug(\"Phase 2: Prepare serial compaction\", scope()->timer());\n+\/\/  \/\/ At this point, we know that after parallel compaction there will be regions that\n+\/\/  \/\/ are partially compacted into. Thus, the last compaction region of all\n+\/\/  \/\/ compaction queues still have space in them. We try to re-compact these regions\n+\/\/  \/\/ in serial to avoid a premature OOM when the mutator wants to allocate the first\n+\/\/  \/\/ eden region after gc.\n+\/\/\n+\/\/  \/\/ For maximum compaction, we need to re-prepare all objects above the lowest\n+\/\/  \/\/ region among the current regions for all thread compaction points. It may\n+\/\/  \/\/ happen that due to the uneven distribution of objects to parallel threads, holes\n+\/\/  \/\/ have been created as threads compact to different target regions between the\n+\/\/  \/\/ lowest and the highest region in the tails of the compaction points.\n+\/\/\n+\/\/  uint start_serial = truncate_parallel_cps();\n+\/\/  assert(start_serial < _heap->max_reserved_regions(), \"Called on empty parallel compaction queues\");\n+\/\/\n+\/\/  G1FullGCCompactionPoint* serial_cp = serial_compaction_point();\n+\/\/  assert(!serial_cp->is_initialized(), \"sanity!\");\n+\/\/\n+\/\/  HeapRegion* start_hr = _heap->region_at(start_serial);\n+\/\/  serial_cp->add(start_hr);\n+\/\/  serial_cp->initialize(start_hr);\n+\/\/\n+\/\/  HeapWord* dense_prefix_top = compaction_top(start_hr);\n+\/\/  G1SerialRePrepareClosure re_prepare(serial_cp, dense_prefix_top);\n+\/\/\n+\/\/  for (uint i = start_serial + 1; i < _heap->max_reserved_regions(); i++) {\n+\/\/    if (is_compaction_target(i)) {\n+\/\/      HeapRegion* current = _heap->region_at(i);\n+\/\/      set_compaction_top(current, current->bottom());\n+\/\/      serial_cp->add(current);\n+\/\/      current->apply_to_marked_objects(mark_bitmap(), &re_prepare);\n+\/\/    }\n+\/\/  }\n+\/\/  serial_cp->update();\n+\/\/}\n","filename":"src\/hotspot\/share\/gc\/g1\/g1FullCollector.cpp","additions":62,"deletions":58,"binary":false,"changes":120,"status":"modified"},{"patch":"@@ -33,0 +33,1 @@\n+#include \"gc\/shared\/slidingForwarding.inline.hpp\"\n@@ -45,1 +46,1 @@\n-    HeapWord* destination = cast_from_oop<HeapWord*>(obj->forwardee());\n+    HeapWord* destination = cast_from_oop<HeapWord*>(_forwarding->forwardee(obj));\n","filename":"src\/hotspot\/share\/gc\/g1\/g1FullGCCompactTask.cpp","additions":2,"deletions":1,"binary":false,"changes":3,"status":"modified"},{"patch":"@@ -29,0 +29,1 @@\n+#include \"gc\/shared\/slidingForwarding.inline.hpp\"\n@@ -94,1 +95,1 @@\n-void G1FullGCCompactionPoint::forward(oop object, size_t size) {\n+void G1FullGCCompactionPoint::forward(SlidingForwarding* const forwarding, oop object, size_t size) {\n@@ -104,1 +105,1 @@\n-    object->forward_to(cast_to_oop(_compaction_top));\n+    forwarding->forward_to(object, cast_to_oop(_compaction_top));\n","filename":"src\/hotspot\/share\/gc\/g1\/g1FullGCCompactionPoint.cpp","additions":3,"deletions":2,"binary":false,"changes":5,"status":"modified"},{"patch":"@@ -34,0 +34,1 @@\n+class SlidingForwarding;\n@@ -55,1 +56,1 @@\n-  void forward(oop object, size_t size);\n+  void forward(SlidingForwarding* const forwarding, oop object, size_t size);\n","filename":"src\/hotspot\/share\/gc\/g1\/g1FullGCCompactionPoint.hpp","additions":2,"deletions":1,"binary":false,"changes":3,"status":"modified"},{"patch":"@@ -28,0 +28,1 @@\n+#include \"gc\/g1\/g1CollectedHeap.hpp\"\n@@ -35,0 +36,1 @@\n+class SlidingForwarding;\n@@ -78,0 +80,1 @@\n+  const SlidingForwarding* const _forwarding;\n@@ -81,1 +84,3 @@\n-  G1AdjustClosure(G1FullCollector* collector) : _collector(collector) { }\n+  G1AdjustClosure(G1FullCollector* collector) :\n+    _collector(collector),\n+    _forwarding(G1CollectedHeap::heap()->forwarding()) { }\n","filename":"src\/hotspot\/share\/gc\/g1\/g1FullGCOopClosures.hpp","additions":6,"deletions":1,"binary":false,"changes":7,"status":"modified"},{"patch":"@@ -36,0 +36,1 @@\n+#include \"gc\/shared\/slidingForwarding.inline.hpp\"\n@@ -110,1 +111,1 @@\n-    _cp(cp) { }\n+    _cp(cp), _forwarding(G1CollectedHeap::heap()->forwarding()) { }\n@@ -114,1 +115,1 @@\n-  _cp->forward(object, size);\n+  _cp->forward(_forwarding, object, size);\n","filename":"src\/hotspot\/share\/gc\/g1\/g1FullGCPrepareTask.cpp","additions":3,"deletions":2,"binary":false,"changes":5,"status":"modified"},{"patch":"@@ -35,0 +35,1 @@\n+class SlidingForwarding;\n@@ -95,0 +96,1 @@\n+    SlidingForwarding* const _forwarding;\n@@ -104,11 +106,11 @@\n-class G1SerialRePrepareClosure : public StackObj {\n-  G1FullGCCompactionPoint* _cp;\n-  HeapWord* _dense_prefix_top;\n-\n-public:\n-  G1SerialRePrepareClosure(G1FullGCCompactionPoint* hrcp, HeapWord* dense_prefix_top) :\n-    _cp(hrcp),\n-    _dense_prefix_top(dense_prefix_top) { }\n-\n-  inline size_t apply(oop obj);\n-};\n+\/\/class G1SerialRePrepareClosure : public StackObj {\n+\/\/  G1FullGCCompactionPoint* _cp;\n+\/\/  HeapWord* _dense_prefix_top;\n+\/\/\n+\/\/public:\n+\/\/  G1SerialRePrepareClosure(G1FullGCCompactionPoint* hrcp, HeapWord* dense_prefix_top) :\n+\/\/    _cp(hrcp),\n+\/\/    _dense_prefix_top(dense_prefix_top) { }\n+\/\/\n+\/\/  inline size_t apply(oop obj);\n+\/\/};\n","filename":"src\/hotspot\/share\/gc\/g1\/g1FullGCPrepareTask.hpp","additions":13,"deletions":11,"binary":false,"changes":24,"status":"modified"},{"patch":"@@ -114,15 +114,15 @@\n-inline size_t G1SerialRePrepareClosure::apply(oop obj) {\n-  if (obj->is_forwarded()) {\n-    \/\/ We skip objects compiled into the first region or\n-    \/\/ into regions not part of the serial compaction point.\n-    if (cast_from_oop<HeapWord*>(obj->forwardee()) < _dense_prefix_top) {\n-      return obj->size();\n-    }\n-  }\n-\n-  \/\/ Get size and forward.\n-  size_t size = obj->size();\n-  _cp->forward(obj, size);\n-\n-  return size;\n-}\n+\/\/inline size_t G1SerialRePrepareClosure::apply(oop obj) {\n+\/\/  if (obj->is_forwarded()) {\n+\/\/    \/\/ We skip objects compiled into the first region or\n+\/\/    \/\/ into regions not part of the serial compaction point.\n+\/\/    if (cast_from_oop<HeapWord*>(obj->forwardee()) < _dense_prefix_top) {\n+\/\/      return obj->size();\n+\/\/    }\n+\/\/  }\n+\/\/\n+\/\/  \/\/ Get size and forward.\n+\/\/  size_t size = obj->size();\n+\/\/  _cp->forward(obj, size);\n+\/\/\n+\/\/  return size;\n+\/\/}\n","filename":"src\/hotspot\/share\/gc\/g1\/g1FullGCPrepareTask.inline.hpp","additions":15,"deletions":15,"binary":false,"changes":30,"status":"modified"},{"patch":"@@ -231,1 +231,1 @@\n-      forwardee = cast_to_oop(m.decode_pointer());\n+      forwardee = obj->forwardee(m);\n","filename":"src\/hotspot\/share\/gc\/g1\/g1OopClosures.inline.hpp","additions":1,"deletions":1,"binary":false,"changes":2,"status":"modified"},{"patch":"@@ -66,1 +66,1 @@\n-inline void PSPromotionManager::promotion_trace_event(oop new_obj, oop old_obj,\n+inline void PSPromotionManager::promotion_trace_event(oop new_obj, oop old_obj, Klass* klass,\n@@ -79,1 +79,1 @@\n-        gc_tracer->report_promotion_in_new_plab_event(old_obj->klass(), obj_bytes,\n+        gc_tracer->report_promotion_in_new_plab_event(klass, obj_bytes,\n@@ -86,1 +86,1 @@\n-        gc_tracer->report_promotion_outside_plab_event(old_obj->klass(), obj_bytes,\n+        gc_tracer->report_promotion_outside_plab_event(klass, obj_bytes,\n@@ -150,1 +150,1 @@\n-    return cast_to_oop(m.decode_pointer());\n+    return o->forwardee(m);\n@@ -166,1 +166,6 @@\n-  size_t new_obj_size = o->size();\n+#ifdef _LP64\n+  Klass* klass = test_mark.safe_klass();\n+#else\n+  Klass* klass = o->klass();\n+#endif\n+  size_t new_obj_size = o->size_given_klass(klass);\n@@ -181,1 +186,1 @@\n-          promotion_trace_event(new_obj, o, new_obj_size, age, false, nullptr);\n+          promotion_trace_event(new_obj, o, klass, new_obj_size, age, false, nullptr);\n@@ -191,1 +196,1 @@\n-            promotion_trace_event(new_obj, o, new_obj_size, age, false, &_young_lab);\n+            promotion_trace_event(new_obj, o, klass, new_obj_size, age, false, &_young_lab);\n@@ -217,1 +222,1 @@\n-          promotion_trace_event(new_obj, o, new_obj_size, age, true, nullptr);\n+          promotion_trace_event(new_obj, o, klass, new_obj_size, age, true, nullptr);\n@@ -227,1 +232,1 @@\n-            promotion_trace_event(new_obj, o, new_obj_size, age, true, &_old_lab);\n+            promotion_trace_event(new_obj, o, klass, new_obj_size, age, true, &_old_lab);\n@@ -250,3 +255,5 @@\n-  \/\/ Parallel GC claims with a release - so other threads might access this object\n-  \/\/ after claiming and they should see the \"completed\" object.\n-  ContinuationGCSupport::transform_stack_chunk(new_obj);\n+  if (!new_obj->mark().is_marked()) {\n+    \/\/ Parallel GC claims with a release - so other threads might access this object\n+    \/\/ after claiming and they should see the \"completed\" object.\n+    ContinuationGCSupport::transform_stack_chunk(new_obj);\n+  }\n","filename":"src\/hotspot\/share\/gc\/parallel\/psPromotionManager.inline.hpp","additions":19,"deletions":12,"binary":false,"changes":31,"status":"modified"},{"patch":"@@ -873,0 +873,10 @@\n+#ifdef _LP64\n+        oop forwardee = obj->forwardee();\n+        markWord header = forwardee->mark();\n+        if (header.has_displaced_mark_helper()) {\n+          header = header.displaced_mark_helper();\n+        }\n+        assert(UseCompressedClassPointers, \"assume +UseCompressedClassPointers\");\n+        narrowKlass nklass = header.narrow_klass();\n+        obj->set_mark(markWord::prototype().set_narrow_klass(nklass));\n+#else\n@@ -874,0 +884,1 @@\n+#endif\n@@ -897,1 +908,1 @@\n-  old->forward_to(old);\n+  old->forward_to_self();\n","filename":"src\/hotspot\/share\/gc\/serial\/defNewGeneration.cpp","additions":12,"deletions":1,"binary":false,"changes":13,"status":"modified"},{"patch":"@@ -259,0 +259,2 @@\n+  AdjustPointerClosure adjust_pointer_closure(gch->forwarding());\n+  CLDToOopClosure      adjust_cld_closure(&adjust_pointer_closure, ClassLoaderData::_claim_stw_fullgc_adjust);\n","filename":"src\/hotspot\/share\/gc\/serial\/genMarkSweep.cpp","additions":2,"deletions":0,"binary":false,"changes":2,"status":"modified"},{"patch":"@@ -41,0 +41,1 @@\n+class SlidingForwarding;\n@@ -127,2 +128,0 @@\n-  static AdjustPointerClosure adjust_pointer_closure;\n-  static CLDToOopClosure      adjust_cld_closure;\n@@ -145,1 +144,1 @@\n-  static size_t adjust_pointers(oop obj);\n+  static size_t adjust_pointers(const SlidingForwarding* const forwarding, oop obj);\n@@ -149,1 +148,1 @@\n-  template <class T> static inline void adjust_pointer(T* p);\n+  template <class T> static inline void adjust_pointer(const SlidingForwarding* const forwarding, T* p);\n@@ -183,0 +182,2 @@\n+private:\n+  const SlidingForwarding* const _forwarding;\n@@ -184,0 +185,1 @@\n+  AdjustPointerClosure(const SlidingForwarding* forwarding) : _forwarding(forwarding) {}\n@@ -197,1 +199,1 @@\n-  void adjust_pointer();\n+  void adjust_pointer(const SlidingForwarding* const forwarding);\n","filename":"src\/hotspot\/share\/gc\/serial\/markSweep.hpp","additions":7,"deletions":5,"binary":false,"changes":12,"status":"modified"},{"patch":"@@ -226,4 +226,0 @@\n-  if (is_in(object->klass_raw())) {\n-    return false;\n-  }\n-\n@@ -256,2 +252,4 @@\n-  _filler_array_max_size = align_object_size(filler_array_hdr_size() +\n-                                             max_len \/ elements_per_word);\n+  int header_size_in_bytes = arrayOopDesc::base_offset_in_bytes(T_INT);\n+  assert(header_size_in_bytes % sizeof(jint) == 0, \"must be aligned to int\");\n+  int header_size_in_ints = header_size_in_bytes \/ sizeof(jint);\n+  _filler_array_max_size = align_object_size((header_size_in_ints + max_len) \/ elements_per_word);\n@@ -418,1 +416,3 @@\n-  size_t max_int_size = typeArrayOopDesc::header_size(T_INT) +\n+  int header_size_in_bytes = typeArrayOopDesc::base_offset_in_bytes(T_INT);\n+  assert(header_size_in_bytes % sizeof(jint) == 0, \"header size must align to int\");\n+  size_t max_int_size = header_size_in_bytes \/ HeapWordSize +\n@@ -424,5 +424,2 @@\n-size_t CollectedHeap::filler_array_hdr_size() {\n-  return align_object_offset(arrayOopDesc::header_size(T_INT)); \/\/ align to Long\n-}\n-\n-  return align_object_size(filler_array_hdr_size()); \/\/ align to MinObjAlignment\n+  int aligned_header_size_words = align_up(arrayOopDesc::base_offset_in_bytes(T_INT), HeapWordSize) \/ HeapWordSize;\n+  return align_object_size(aligned_header_size_words); \/\/ align to MinObjAlignment\n@@ -433,2 +430,3 @@\n-  Copy::fill_to_words(start + filler_array_hdr_size(),\n-                      words - filler_array_hdr_size(), value);\n+  int payload_start = align_up(arrayOopDesc::base_offset_in_bytes(T_INT), HeapWordSize) \/ HeapWordSize;\n+  Copy::fill_to_words(start + payload_start,\n+                      words - payload_start, value);\n@@ -458,2 +456,3 @@\n-  const size_t payload_size = words - filler_array_hdr_size();\n-  const size_t len = payload_size * HeapWordSize \/ sizeof(jint);\n+  const size_t payload_size_bytes = words * HeapWordSize - arrayOopDesc::base_offset_in_bytes(T_INT);\n+  assert(payload_size_bytes % sizeof(jint) == 0, \"must be int aligned\");\n+  const size_t len = payload_size_bytes \/ sizeof(jint);\n","filename":"src\/hotspot\/share\/gc\/shared\/collectedHeap.cpp","additions":15,"deletions":16,"binary":false,"changes":31,"status":"modified"},{"patch":"@@ -166,1 +166,0 @@\n-  static inline size_t filler_array_hdr_size();\n","filename":"src\/hotspot\/share\/gc\/shared\/collectedHeap.hpp","additions":0,"deletions":1,"binary":false,"changes":1,"status":"modified"},{"patch":"@@ -57,0 +57,1 @@\n+#include \"gc\/shared\/slidingForwarding.hpp\"\n@@ -119,0 +120,1 @@\n+  _forwarding = new SlidingForwarding(_reserved);\n@@ -1037,0 +1039,1 @@\n+  _forwarding->clear();\n","filename":"src\/hotspot\/share\/gc\/shared\/genCollectedHeap.cpp","additions":3,"deletions":0,"binary":false,"changes":3,"status":"modified"},{"patch":"@@ -34,0 +34,1 @@\n+#include \"memory\/metaspace.hpp\"\n@@ -403,0 +404,15 @@\n+JVMFlag::Error CompressedClassSpaceSizeConstraintFunc(size_t value, bool verbose) {\n+#ifdef _LP64\n+  \/\/ There is no minimal value check, although class space will be transparently enlarged\n+  \/\/ to a multiple of metaspace root chunk size (4m).\n+  \/\/ The max. value of class space size depends on narrow klass pointer encoding range size\n+  \/\/ and CDS, see metaspace.cpp.\n+  if (value > Metaspace::max_class_space_size()) {\n+    JVMFlag::printError(verbose, \"CompressedClassSpaceSize \" SIZE_FORMAT \" too large (max: \" SIZE_FORMAT \")\\n\",\n+                        value, Metaspace::max_class_space_size());\n+    return JVMFlag::VIOLATES_CONSTRAINT;\n+  }\n+#endif\n+  return JVMFlag::SUCCESS;\n+}\n+\n","filename":"src\/hotspot\/share\/gc\/shared\/jvmFlagConstraintsGC.cpp","additions":16,"deletions":0,"binary":false,"changes":16,"status":"modified"},{"patch":"@@ -402,1 +402,0 @@\n-  oopDesc::set_klass_gap(mem, 0);\n@@ -408,2 +407,0 @@\n-  \/\/ May be bootstrapping\n-  oopDesc::set_mark(mem, markWord::prototype());\n@@ -413,0 +410,4 @@\n+#ifdef _LP64\n+  oopDesc::release_set_mark(mem, _klass->prototype_header());\n+#else\n+  oopDesc::set_mark(mem, _klass->prototype_header());\n@@ -414,0 +415,1 @@\n+#endif\n@@ -427,1 +429,1 @@\n-  const size_t hs = arrayOopDesc::header_size(array_klass->element_type());\n+  const size_t hs = align_up(arrayOopDesc::base_offset_in_bytes(array_klass->element_type()), HeapWordSize) \/ HeapWordSize;\n","filename":"src\/hotspot\/share\/gc\/shared\/memAllocator.cpp","additions":6,"deletions":4,"binary":false,"changes":10,"status":"modified"},{"patch":"@@ -32,0 +32,1 @@\n+#include \"runtime\/objectMonitor.hpp\"\n@@ -265,0 +266,1 @@\n+        ObjectMonitor::maybe_deflate_dead(ptr);\n","filename":"src\/hotspot\/share\/gc\/shared\/oopStorage.inline.hpp","additions":2,"deletions":0,"binary":false,"changes":2,"status":"modified"},{"patch":"@@ -35,0 +35,1 @@\n+class SlidingForwarding;\n@@ -66,0 +67,2 @@\n+  \/\/ TODO: This method is unused, except in the gunit test. Change the test\n+  \/\/ to exercise the updated method below instead, and remove this one.\n@@ -68,0 +71,2 @@\n+  void adjust_during_full_gc(const SlidingForwarding* const forwarding);\n+\n","filename":"src\/hotspot\/share\/gc\/shared\/preservedMarks.hpp","additions":5,"deletions":0,"binary":false,"changes":5,"status":"modified"},{"patch":"@@ -30,0 +30,1 @@\n+#include \"gc\/shared\/slidingForwarding.inline.hpp\"\n@@ -255,1 +256,1 @@\n-                                    CompactPoint* cp, HeapWord* compact_top) {\n+                                    CompactPoint* cp, HeapWord* compact_top, SlidingForwarding* const forwarding) {\n@@ -278,1 +279,1 @@\n-    q->forward_to(cast_to_oop(compact_top));\n+    forwarding->forward_to(q, cast_to_oop(compact_top));\n@@ -326,0 +327,1 @@\n+  SlidingForwarding* const forwarding = GenCollectedHeap::heap()->forwarding();\n@@ -331,1 +333,1 @@\n-      compact_top = cp->space->forward(cast_to_oop(cur_obj), size, cp, compact_top);\n+      compact_top = cp->space->forward(cast_to_oop(cur_obj), size, cp, compact_top, forwarding);\n@@ -347,1 +349,1 @@\n-        compact_top = cp->space->forward(obj, obj->size(), cp, compact_top);\n+        compact_top = cp->space->forward(obj, obj->size(), cp, compact_top, forwarding);\n@@ -390,0 +392,1 @@\n+  const SlidingForwarding* const forwarding = GenCollectedHeap::heap()->forwarding();\n@@ -401,1 +404,1 @@\n-      size_t size = MarkSweep::adjust_pointers(cast_to_oop(cur_obj));\n+      size_t size = MarkSweep::adjust_pointers(forwarding, cast_to_oop(cur_obj));\n@@ -442,0 +445,2 @@\n+  const SlidingForwarding* const forwarding = GenCollectedHeap::heap()->forwarding();\n+\n@@ -455,1 +460,1 @@\n-      HeapWord* compaction_top = cast_from_oop<HeapWord*>(cast_to_oop(cur_obj)->forwardee());\n+      HeapWord* compaction_top = cast_from_oop<HeapWord*>(forwarding->forwardee(cast_to_oop(cur_obj)));\n","filename":"src\/hotspot\/share\/gc\/shared\/space.cpp","additions":11,"deletions":6,"binary":false,"changes":17,"status":"modified"},{"patch":"@@ -59,0 +59,1 @@\n+class SlidingForwarding;\n@@ -387,1 +388,1 @@\n-                    HeapWord* compact_top);\n+                    HeapWord* compact_top, SlidingForwarding* const forwarding);\n","filename":"src\/hotspot\/share\/gc\/shared\/space.hpp","additions":2,"deletions":1,"binary":false,"changes":3,"status":"modified"},{"patch":"@@ -32,0 +32,1 @@\n+#include \"gc\/shared\/genCollectedHeap.hpp\"\n","filename":"src\/hotspot\/share\/gc\/shared\/space.inline.hpp","additions":1,"deletions":0,"binary":false,"changes":1,"status":"modified"},{"patch":"@@ -68,0 +68,1 @@\n+      ObjectMonitor::maybe_deflate_dead(p);\n","filename":"src\/hotspot\/share\/gc\/shared\/weakProcessor.inline.hpp","additions":1,"deletions":0,"binary":false,"changes":1,"status":"modified"},{"patch":"@@ -32,0 +32,1 @@\n+#include \"gc\/shenandoah\/shenandoahObjectUtils.inline.hpp\"\n@@ -200,1 +201,1 @@\n-  Klass* obj_klass = obj->klass_or_null();\n+  Klass* obj_klass = ShenandoahObjectUtils::klass(obj);\n@@ -232,1 +233,1 @@\n-    if (obj_klass != fwd->klass()) {\n+    if (obj_klass != ShenandoahObjectUtils::klass(fwd)) {\n","filename":"src\/hotspot\/share\/gc\/shenandoah\/shenandoahAsserts.cpp","additions":3,"deletions":2,"binary":false,"changes":5,"status":"modified"},{"patch":"@@ -33,0 +33,1 @@\n+#include \"runtime\/synchronizer.hpp\"\n@@ -89,0 +90,1 @@\n+    assert(prev_mark.is_marked(), \"must be forwarded\");\n","filename":"src\/hotspot\/share\/gc\/shenandoah\/shenandoahForwarding.inline.hpp","additions":2,"deletions":0,"binary":false,"changes":2,"status":"modified"},{"patch":"@@ -31,0 +31,1 @@\n+#include \"gc\/shared\/slidingForwarding.inline.hpp\"\n@@ -190,0 +191,1 @@\n+    heap->forwarding()->clear();\n@@ -300,2 +302,3 @@\n-  PreservedMarks*          const _preserved_marks;\n-  ShenandoahHeap*          const _heap;\n+  PreservedMarks*    const _preserved_marks;\n+  SlidingForwarding* const _forwarding;\n+  ShenandoahHeap*    const _heap;\n@@ -313,0 +316,1 @@\n+    _forwarding(ShenandoahHeap::heap()->forwarding()),\n@@ -366,1 +370,1 @@\n-    p->forward_to(cast_to_oop(_compact_point));\n+    _forwarding->forward_to(p, cast_to_oop(_compact_point));\n@@ -440,0 +444,1 @@\n+  SlidingForwarding* forwarding = heap->forwarding();\n@@ -474,1 +479,1 @@\n-        old_obj->forward_to(cast_to_oop(heap->get_region(start)->bottom()));\n+        forwarding->forward_to(old_obj, cast_to_oop(heap->get_region(start)->bottom()));\n@@ -725,1 +730,2 @@\n-  ShenandoahHeap* const _heap;\n+  ShenandoahHeap*           const _heap;\n+  const SlidingForwarding*  const _forwarding;\n@@ -735,1 +741,1 @@\n-        oop forw = obj->forwardee();\n+        oop forw = _forwarding->forwardee(obj);\n@@ -744,0 +750,1 @@\n+    _forwarding(_heap->forwarding()),\n@@ -805,1 +812,2 @@\n-    _preserved_marks->get(worker_id)->adjust_during_full_gc();\n+    const SlidingForwarding* const forwarding = ShenandoahHeap::heap()->forwarding();\n+    _preserved_marks->get(worker_id)->adjust_during_full_gc(forwarding);\n@@ -835,2 +843,3 @@\n-  ShenandoahHeap* const _heap;\n-  uint            const _worker_id;\n+  ShenandoahHeap*          const _heap;\n+  const SlidingForwarding* const _forwarding;\n+  uint                     const _worker_id;\n@@ -840,1 +849,1 @@\n-    _heap(ShenandoahHeap::heap()), _worker_id(worker_id) {}\n+    _heap(ShenandoahHeap::heap()), _forwarding(_heap->forwarding()), _worker_id(worker_id) {}\n@@ -847,1 +856,1 @@\n-      HeapWord* compact_to = cast_from_oop<HeapWord*>(p->forwardee());\n+      HeapWord* compact_to = cast_from_oop<HeapWord*>(_forwarding->forwardee(p));\n@@ -944,0 +953,1 @@\n+  const SlidingForwarding* const forwarding = heap->forwarding();\n@@ -958,1 +968,1 @@\n-      size_t new_start = heap->heap_region_index_containing(old_obj->forwardee());\n+      size_t new_start = heap->heap_region_index_containing(forwarding->forwardee(old_obj));\n","filename":"src\/hotspot\/share\/gc\/shenandoah\/shenandoahFullGC.cpp","additions":22,"deletions":12,"binary":false,"changes":34,"status":"modified"},{"patch":"@@ -36,0 +36,1 @@\n+#include \"gc\/shared\/slidingForwarding.hpp\"\n@@ -197,0 +198,2 @@\n+  _forwarding = new SlidingForwarding(_heap_region, ShenandoahHeapRegion::region_size_words_shift());\n+\n@@ -953,1 +956,1 @@\n-    if (!p->is_forwarded()) {\n+    if (!ShenandoahForwarding::is_forwarded(p)) {\n@@ -1298,0 +1301,1 @@\n+    shenandoah_assert_not_in_cset_except(NULL, obj, cancelled_gc());\n","filename":"src\/hotspot\/share\/gc\/shenandoah\/shenandoahHeap.cpp","additions":5,"deletions":1,"binary":false,"changes":6,"status":"modified"},{"patch":"@@ -65,0 +65,1 @@\n+class SlidingForwarding;\n@@ -231,0 +232,1 @@\n+  SlidingForwarding* _forwarding;\n@@ -247,0 +249,2 @@\n+  SlidingForwarding* forwarding() const { return _forwarding; }\n+\n","filename":"src\/hotspot\/share\/gc\/shenandoah\/shenandoahHeap.hpp","additions":4,"deletions":0,"binary":false,"changes":4,"status":"modified"},{"patch":"@@ -45,0 +45,1 @@\n+#include \"gc\/shenandoah\/shenandoahObjectUtils.inline.hpp\"\n@@ -300,1 +301,1 @@\n-  size_t size = p->size();\n+  size_t size = ShenandoahObjectUtils::size(p);\n@@ -338,2 +339,7 @@\n-  ContinuationGCSupport::relativize_stack_chunk(copy_val);\n-\n+  if (!copy_val->mark().is_marked()) {\n+    \/\/ If we copied a mark-word that indicates 'forwarded' state, then\n+    \/\/ another thread beat us, and this new copy will never be published.\n+    \/\/ ContinuationGCSupport would get a corrupt Klass* in that case,\n+    \/\/ so don't even attempt it.\n+    ContinuationGCSupport::relativize_stack_chunk(copy_val);\n+  }\n@@ -519,1 +525,1 @@\n-    size_t size = obj->size();\n+    size_t size = ShenandoahObjectUtils::size(obj);\n","filename":"src\/hotspot\/share\/gc\/shenandoah\/shenandoahHeap.inline.hpp","additions":10,"deletions":4,"binary":false,"changes":14,"status":"modified"},{"patch":"@@ -32,0 +32,1 @@\n+#include \"gc\/shenandoah\/shenandoahObjectUtils.inline.hpp\"\n@@ -101,1 +102,1 @@\n-      if (is_instance_ref_klass(obj->klass())) {\n+      if (is_instance_ref_klass(ShenandoahObjectUtils::klass(obj))) {\n@@ -128,1 +129,1 @@\n-    Klass* obj_klass = obj->klass_or_null();\n+    Klass* obj_klass = ShenandoahObjectUtils::klass(obj);\n@@ -143,1 +144,1 @@\n-        check(ShenandoahAsserts::_safe_unknown, obj, (obj_addr + obj->size()) <= obj_reg->top(),\n+        check(ShenandoahAsserts::_safe_unknown, obj, (obj_addr + ShenandoahObjectUtils::size(obj)) <= obj_reg->top(),\n@@ -147,1 +148,1 @@\n-        size_t humongous_end = humongous_start + (obj->size() >> ShenandoahHeapRegion::region_size_words_shift());\n+        size_t humongous_end = humongous_start + (ShenandoahObjectUtils::size(obj) >> ShenandoahHeapRegion::region_size_words_shift());\n@@ -164,1 +165,1 @@\n-          Atomic::add(&_ld[obj_reg->index()], (uint) obj->size(), memory_order_relaxed);\n+          Atomic::add(&_ld[obj_reg->index()], (uint) ShenandoahObjectUtils::size(obj), memory_order_relaxed);\n@@ -205,1 +206,1 @@\n-      check(ShenandoahAsserts::_safe_oop, obj, (fwd_addr + fwd->size()) <= fwd_reg->top(),\n+      check(ShenandoahAsserts::_safe_oop, obj, (fwd_addr + ShenandoahObjectUtils::size(fwd)) <= fwd_reg->top(),\n@@ -308,1 +309,2 @@\n-    obj->oop_iterate(this);\n+    Klass* klass = ShenandoahObjectUtils::klass(obj);\n+    obj->oop_iterate_backwards(this, klass);\n@@ -588,1 +590,1 @@\n-    if (!is_instance_ref_klass(obj->klass())) {\n+    if (!is_instance_ref_klass(ShenandoahObjectUtils::klass(obj))) {\n","filename":"src\/hotspot\/share\/gc\/shenandoah\/shenandoahVerifier.cpp","additions":10,"deletions":8,"binary":false,"changes":18,"status":"modified"},{"patch":"@@ -301,1 +301,0 @@\n-        assert(!UseCompressedClassPointers, \"should only happen without compressed class pointers\");\n","filename":"src\/hotspot\/share\/gc\/z\/c2\/zBarrierSetC2.cpp","additions":0,"deletions":1,"binary":false,"changes":1,"status":"modified"},{"patch":"@@ -738,0 +738,9 @@\n+  if (!UseHeavyMonitors && UseFastLocking) {\n+    \/\/ This is a hack to get around the limitation of registers in x86_32. We really\n+    \/\/ send an oopDesc* instead of a BasicObjectLock*.\n+    Handle h_obj(current, oop((reinterpret_cast<oopDesc*>(elem))));\n+    assert(Universe::heap()->is_in_or_null(h_obj()),\n+           \"must be NULL or an object\");\n+    ObjectSynchronizer::enter(h_obj, NULL, current);\n+    return;\n+  }\n","filename":"src\/hotspot\/share\/interpreter\/interpreterRuntime.cpp","additions":9,"deletions":0,"binary":false,"changes":9,"status":"modified"},{"patch":"@@ -271,1 +271,0 @@\n-  volatile_nonstatic_field(oopDesc,            _metadata._klass,                              Klass*)                                \\\n","filename":"src\/hotspot\/share\/jvmci\/vmStructs_jvmci.cpp","additions":0,"deletions":1,"binary":false,"changes":1,"status":"modified"},{"patch":"@@ -31,0 +31,1 @@\n+#include \"memory\/metaspace\/metaspaceAlignment.hpp\"\n@@ -39,0 +40,2 @@\n+#include \"oops\/compressedKlass.hpp\"\n+#include \"utilities\/align.hpp\"\n@@ -40,0 +43,1 @@\n+#include \"utilities\/globalDefinitions.hpp\"\n@@ -59,0 +63,2 @@\n+  const int klass_alignment_words = KlassAlignmentInBytes \/ BytesPerWord;\n+\n@@ -63,0 +69,1 @@\n+      metaspace::MetaspaceMinAlignmentWords,\n@@ -69,0 +76,1 @@\n+    \/\/ Klass instances live in class space and must be aligned correctly.\n@@ -74,0 +82,1 @@\n+        klass_alignment_words,\n@@ -77,0 +86,5 @@\n+  } else {\n+    \/\/ note for lilliput, this path should be restricted to 32bit only. There, klass alignment\n+    \/\/  should be compatible with metaspace minimal alignment since we store Klass structures\n+    \/\/  in regular metaspace.\n+    NOT_LP64(STATIC_ASSERT(metaspace::MetaspaceMinAlignmentBytes == KlassAlignmentInBytes));\n","filename":"src\/hotspot\/share\/memory\/classLoaderMetaspace.cpp","additions":14,"deletions":0,"binary":false,"changes":14,"status":"modified"},{"patch":"@@ -34,0 +34,1 @@\n+#include \"memory\/metaspace\/metaspaceAlignment.hpp\"\n@@ -110,1 +111,1 @@\n-MetaspaceArena::MetaspaceArena(ChunkManager* chunk_manager, const ArenaGrowthPolicy* growth_policy,\n+MetaspaceArena::MetaspaceArena(ChunkManager* chunk_manager, const ArenaGrowthPolicy* growth_policy, int alignment_words,\n@@ -117,0 +118,1 @@\n+  _alignment_words(alignment_words),\n@@ -123,0 +125,1 @@\n+\n@@ -227,1 +230,1 @@\n-  const size_t raw_word_size = get_raw_word_size_for_requested_word_size(requested_word_size);\n+  const size_t raw_word_size = get_raw_word_size_for_requested_word_size(requested_word_size, _alignment_words);\n@@ -271,1 +274,1 @@\n-  const size_t raw_word_size = get_raw_word_size_for_requested_word_size(requested_word_size);\n+  const size_t raw_word_size = get_raw_word_size_for_requested_word_size(requested_word_size, _alignment_words);\n@@ -368,1 +371,1 @@\n-  size_t raw_word_size = get_raw_word_size_for_requested_word_size(word_size);\n+  size_t raw_word_size = get_raw_word_size_for_requested_word_size(word_size, _alignment_words);\n@@ -482,2 +485,2 @@\n-  st->print_cr(\"growth-policy \" PTR_FORMAT \", lock \" PTR_FORMAT \", cm \" PTR_FORMAT \", fbl \" PTR_FORMAT,\n-                p2i(_growth_policy), p2i(_lock), p2i(_chunk_manager), p2i(_fbl));\n+  st->print_cr(\"growth-policy \" PTR_FORMAT \", alignment %d, lock \" PTR_FORMAT \", cm \" PTR_FORMAT \", fbl \" PTR_FORMAT,\n+                p2i(_growth_policy), _alignment_words * BytesPerWord, p2i(_lock), p2i(_chunk_manager), p2i(_fbl));\n","filename":"src\/hotspot\/share\/memory\/metaspace\/metaspaceArena.cpp","additions":9,"deletions":6,"binary":false,"changes":15,"status":"modified"},{"patch":"@@ -550,0 +550,4 @@\n+    \/\/ Note Lilliput: the advantages of this strategy were questionable before\n+    \/\/  (since CDS=off + Compressed oops + heap large enough to suffocate us out of lower 32g\n+    \/\/  is rare) and with Lilliput the encoding range drastically shrank. We may just do away\n+    \/\/  with this altogether.\n","filename":"src\/hotspot\/share\/memory\/virtualspace.cpp","additions":4,"deletions":0,"binary":false,"changes":4,"status":"modified"},{"patch":"@@ -53,2 +53,1 @@\n-    size_t hs = align_up(length_offset_in_bytes() + sizeof(int),\n-                              HeapWordSize);\n+    size_t hs = length_offset_in_bytes() + sizeof(int);\n@@ -74,0 +73,5 @@\n+#ifdef _LP64\n+    if (type == T_OBJECT || type == T_ARRAY) {\n+      return !UseCompressedOops;\n+    }\n+#endif\n@@ -82,2 +86,1 @@\n-    return UseCompressedClassPointers ? klass_gap_offset_in_bytes() :\n-                               sizeof(arrayOopDesc);\n+    return sizeof(arrayOopDesc);\n@@ -88,1 +91,4 @@\n-    return header_size(type) * HeapWordSize;\n+    size_t typesize_in_bytes = header_size_in_bytes();\n+    return (int)(element_type_should_be_aligned(type)\n+                 ? align_up(typesize_in_bytes, BytesPerLong)\n+                 : typesize_in_bytes);\n@@ -125,11 +131,0 @@\n-  \/\/ Should only be called with constants as argument\n-  \/\/ (will not constant fold otherwise)\n-  \/\/ Returns the header size in words aligned to the requirements of the\n-  \/\/ array object type.\n-  static int header_size(BasicType type) {\n-    size_t typesize_in_bytes = header_size_in_bytes();\n-    return (int)(element_type_should_be_aligned(type)\n-      ? align_object_offset(typesize_in_bytes\/HeapWordSize)\n-      : typesize_in_bytes\/HeapWordSize);\n-  }\n-\n@@ -144,4 +139,2 @@\n-    const size_t max_element_words_per_size_t =\n-      align_down((SIZE_MAX\/HeapWordSize - header_size(type)), MinObjAlignment);\n-    const size_t max_elements_per_size_t =\n-      HeapWordSize * max_element_words_per_size_t \/ type2aelembytes(type);\n+    const size_t max_size_bytes = align_down(SIZE_MAX - base_offset_in_bytes(type), MinObjAlignmentInBytes);\n+    const size_t max_elements_per_size_t = max_size_bytes \/ type2aelembytes(type);\n@@ -153,1 +146,2 @@\n-      return align_down(max_jint - header_size(type), MinObjAlignment);\n+      int header_size_words = align_up(base_offset_in_bytes(type), HeapWordSize) \/ HeapWordSize;\n+      return align_down(max_jint - header_size_words, MinObjAlignment);\n","filename":"src\/hotspot\/share\/oops\/arrayOop.hpp","additions":15,"deletions":21,"binary":false,"changes":36,"status":"modified"},{"patch":"@@ -0,0 +1,103 @@\n+\/*\n+ * Copyright (c) 2021 SAP SE. All rights reserved.\n+ * Copyright (c) 2021, Oracle and\/or its affiliates. All rights reserved.\n+ *\n+ * DO NOT ALTER OR REMOVE COPYRIGHT NOTICES OR THIS FILE HEADER.\n+ *\n+ * This code is free software; you can redistribute it and\/or modify it\n+ * under the terms of the GNU General Public License version 2 only, as\n+ * published by the Free Software Foundation.\n+ *\n+ * This code is distributed in the hope that it will be useful, but WITHOUT\n+ * ANY WARRANTY; without even the implied warranty of MERCHANTABILITY or\n+ * FITNESS FOR A PARTICULAR PURPOSE.  See the GNU General Public License\n+ * version 2 for more details (a copy is included in the LICENSE file that\n+ * accompanied this code).\n+ *\n+ * You should have received a copy of the GNU General Public License version\n+ * 2 along with this work; if not, write to the Free Software Foundation,\n+ * Inc., 51 Franklin St, Fifth Floor, Boston, MA 02110-1301 USA.\n+ *\n+ * Please contact Oracle, 500 Oracle Parkway, Redwood Shores, CA 94065 USA\n+ * or visit www.oracle.com if you need additional information or have any\n+ * questions.\n+ *\n+ *\/\n+\n+#include \"precompiled.hpp\"\n+#include \"oops\/compressedKlass.hpp\"\n+#include \"utilities\/ostream.hpp\"\n+#include \"utilities\/debug.hpp\"\n+#include \"runtime\/globals.hpp\"\n+\n+address CompressedKlassPointers::_base = nullptr;\n+int CompressedKlassPointers::_shift_copy = 0;\n+\n+\/\/ Given an address range [addr, addr+len) which the encoding is supposed to\n+\/\/  cover, choose base, shift and range.\n+\/\/  The address range is the expected range of uncompressed Klass pointers we\n+\/\/  will encounter (and the implicit promise that there will be no Klass\n+\/\/  structures outside this range).\n+void CompressedKlassPointers::initialize(address addr, size_t len) {\n+#ifdef _LP64\n+  assert(UseCompressedClassPointers, \"Sanity\");\n+\n+  assert(len <= (size_t)KlassEncodingMetaspaceMax, \"Range \" SIZE_FORMAT \" too large \"\n+         \"- cannot be contained fully in narrow Klass pointer encoding range.\", len);\n+\n+  if (UseSharedSpaces || DumpSharedSpaces) {\n+\n+    \/\/ Special requirements if CDS is active:\n+    \/\/ Encoding base and shift must be the same between dump and run time.\n+    \/\/   CDS takes care that the SharedBaseAddress and CompressedClassSpaceSize\n+    \/\/   are the same. Archive size will be probably different at runtime, but\n+    \/\/   it can only be smaller than at, never larger, since archives get\n+    \/\/   shrunk at the end of the dump process.\n+    \/\/   From that it follows that the range [addr, len) we are handed in at\n+    \/\/   runtime will start at the same address then at dumptime, and its len\n+    \/\/   may be smaller at runtime then it was at dump time.\n+    \/\/\n+    \/\/ To be very careful here, we avoid any optimizations and just keep using\n+    \/\/  the same address and shift value. Specifically we avoid using zero-based\n+    \/\/  encoding. We also set the expected value range to 4G (encoding range\n+    \/\/  cannot be larger than that).\n+\n+    _base = addr;\n+\n+  } else {\n+\n+    \/\/ (Note that this case is almost not worth optimizing for. CDS is typically on.)\n+    if ((addr + len) <= (address)KlassEncodingMetaspaceMax) {\n+      _base = 0;\n+    } else {\n+      _base = addr;\n+    }\n+  }\n+\n+  assert(is_valid_base(_base), \"Address \" PTR_FORMAT \" was chosen as encoding base for range [\"\n+                               PTR_FORMAT \", \" PTR_FORMAT \") but is not a valid encoding base\",\n+                               p2i(_base), p2i(addr), p2i(addr + len));\n+\n+  \/\/ For SA\n+  _shift_copy = LogKlassAlignmentInBytes;\n+\n+#else\n+  ShouldNotReachHere(); \/\/ 64-bit only\n+#endif\n+}\n+\n+\/\/ 64-bit platforms define these functions on a per-platform base. They are not needed for\n+\/\/  32-bit (in fact, the whole setup is not needed and could be excluded from compilation,\n+\/\/  but that is a question for another RFE).\n+#ifndef _LP64\n+\/\/ Given an address p, return true if p can be used as an encoding base.\n+\/\/  (Some platforms have restrictions of what constitutes a valid base address).\n+bool CompressedKlassPointers::is_valid_base(address p) {\n+  ShouldNotReachHere(); \/\/ 64-bit only\n+  return false;\n+}\n+\n+void CompressedKlassPointers::print_mode(outputStream* st) {\n+}\n+#endif\n+\n","filename":"src\/hotspot\/share\/oops\/compressedKlass.cpp","additions":103,"deletions":0,"binary":false,"changes":103,"status":"added"},{"patch":"@@ -0,0 +1,114 @@\n+\/*\n+ * Copyright (c) 2021 SAP SE. All rights reserved.\n+ * Copyright (c) 2021, Oracle and\/or its affiliates. All rights reserved.\n+ *\n+ * DO NOT ALTER OR REMOVE COPYRIGHT NOTICES OR THIS FILE HEADER.\n+ *\n+ * This code is free software; you can redistribute it and\/or modify it\n+ * under the terms of the GNU General Public License version 2 only, as\n+ * published by the Free Software Foundation.\n+ *\n+ * This code is distributed in the hope that it will be useful, but WITHOUT\n+ * ANY WARRANTY; without even the implied warranty of MERCHANTABILITY or\n+ * FITNESS FOR A PARTICULAR PURPOSE.  See the GNU General Public License\n+ * version 2 for more details (a copy is included in the LICENSE file that\n+ * accompanied this code).\n+ *\n+ * You should have received a copy of the GNU General Public License version\n+ * 2 along with this work; if not, write to the Free Software Foundation,\n+ * Inc., 51 Franklin St, Fifth Floor, Boston, MA 02110-1301 USA.\n+ *\n+ * Please contact Oracle, 500 Oracle Parkway, Redwood Shores, CA 94065 USA\n+ * or visit www.oracle.com if you need additional information or have any\n+ * questions.\n+ *\n+ *\/\n+\n+#ifndef SHARE_OOPS_COMPRESSEDKLASS_HPP\n+#define SHARE_OOPS_COMPRESSEDKLASS_HPP\n+\n+#include \"memory\/allStatic.hpp\"\n+#include \"utilities\/globalDefinitions.hpp\"\n+\n+class outputStream;\n+class Klass;\n+\n+\/\/ Narrow Klass pointer constants;\n+#ifdef _LP64\n+const int LogKlassAlignmentInBytes = 9; \/\/ 512 byte alignment (Lilliput)\n+#else\n+const int LogKlassAlignmentInBytes = 3; \/\/ traditional 64-bit alignment\n+#endif\n+\n+const int KlassAlignmentInBytes    = 1 << LogKlassAlignmentInBytes;\n+\n+\/\/ Max. allowed size of compressed class pointer, in bits\n+const  int      MaxNarrowKlassPointerBits = 22;\n+\n+\/\/ Mask to mask in the bits which are valid to be set in a narrow Klass pointer\n+const uint64_t  NarrowKlassPointerBitMask = ((((uint64_t)1) << MaxNarrowKlassPointerBits) - 1);\n+\n+\/\/ Maximal size of compressed class pointer encoding range (2G with 22bit class ptr and 9 bit alignment).\n+const  uint64_t KlassEncodingMetaspaceMax = UCONST64(1) << (MaxNarrowKlassPointerBits + LogKlassAlignmentInBytes);\n+\n+\/\/ If compressed klass pointers then use narrowKlass.\n+typedef uint32_t narrowKlass;\n+\n+class CompressedKlassPointers : public AllStatic {\n+  friend class VMStructs;\n+  friend class ArchiveBuilder;\n+\n+  static address _base;\n+\n+  \/\/ Shift is actually a constant; we keep this just for the SA (see vmStructs.cpp and\n+  \/\/ sun\/jvm\/hotspot\/oops\/CompressedKlassPointers.java)\n+  static int _shift_copy;\n+\n+  \/\/ The decode\/encode versions taking an explicit base are for the sole use of CDS\n+  \/\/ (see ArchiveBuilder).\n+  static inline Klass* decode_raw(narrowKlass v, address base);\n+  static inline Klass* decode_not_null(narrowKlass v, address base);\n+  static inline narrowKlass encode_not_null(Klass* v, address base);\n+  DEBUG_ONLY(static inline void verify_klass_pointer(const Klass* v, address base));\n+\n+public:\n+\n+  \/\/ Given an address p, return true if p can be used as an encoding base.\n+  \/\/  (Some platforms have restrictions of what constitutes a valid base\n+  \/\/   address).\n+  static bool is_valid_base(address p);\n+\n+  \/\/ Given an address range [addr, addr+len) which the encoding is supposed to\n+  \/\/  cover, choose base, shift and range.\n+  \/\/  The address range is the expected range of uncompressed Klass pointers we\n+  \/\/  will encounter (and the implicit promise that there will be no Klass\n+  \/\/  structures outside this range).\n+  static void initialize(address addr, size_t len);\n+\n+  static void     print_mode(outputStream* st);\n+\n+  \/\/ The encoding base. Note: this is not necessarily the base address of the\n+  \/\/ class space nor the base address of the CDS archive.\n+  static address  base()             { return  _base; }\n+\n+  \/\/ End of the encoding range.\n+  static address  end()              { return base() + KlassEncodingMetaspaceMax; }\n+\n+  \/\/ Shift == LogKlassAlignmentInBytes (TODO: unify)\n+  static int      shift()            { return  LogKlassAlignmentInBytes; }\n+\n+  static bool is_null(Klass* v)      { return v == nullptr; }\n+  static bool is_null(narrowKlass v) { return v == 0; }\n+\n+  static inline Klass* decode_raw(narrowKlass v);\n+  static inline Klass* decode_not_null(narrowKlass v);\n+  static inline Klass* decode(narrowKlass v);\n+  static inline narrowKlass encode_not_null(Klass* v);\n+  static inline narrowKlass encode(Klass* v);\n+\n+  DEBUG_ONLY(static inline void verify_klass_pointer(const Klass* v));\n+  DEBUG_ONLY(static inline void verify_narrow_klass_pointer(narrowKlass v);)\n+\n+};\n+\n+#endif \/\/ SHARE_OOPS_COMPRESSEDOOPS_HPP\n","filename":"src\/hotspot\/share\/oops\/compressedKlass.hpp","additions":114,"deletions":0,"binary":false,"changes":114,"status":"added"},{"patch":"@@ -0,0 +1,101 @@\n+\/*\n+ * Copyright (c) 2021 SAP SE. All rights reserved.\n+ * Copyright (c) 2021, Oracle and\/or its affiliates. All rights reserved.\n+ *\n+ * DO NOT ALTER OR REMOVE COPYRIGHT NOTICES OR THIS FILE HEADER.\n+ *\n+ * This code is free software; you can redistribute it and\/or modify it\n+ * under the terms of the GNU General Public License version 2 only, as\n+ * published by the Free Software Foundation.\n+ *\n+ * This code is distributed in the hope that it will be useful, but WITHOUT\n+ * ANY WARRANTY; without even the implied warranty of MERCHANTABILITY or\n+ * FITNESS FOR A PARTICULAR PURPOSE.  See the GNU General Public License\n+ * version 2 for more details (a copy is included in the LICENSE file that\n+ * accompanied this code).\n+ *\n+ * You should have received a copy of the GNU General Public License version\n+ * 2 along with this work; if not, write to the Free Software Foundation,\n+ * Inc., 51 Franklin St, Fifth Floor, Boston, MA 02110-1301 USA.\n+ *\n+ * Please contact Oracle, 500 Oracle Parkway, Redwood Shores, CA 94065 USA\n+ * or visit www.oracle.com if you need additional information or have any\n+ * questions.\n+ *\n+ *\/\n+\n+#ifndef SHARE_OOPS_COMPRESSEDKLASS_INLINE_HPP\n+#define SHARE_OOPS_COMPRESSEDKLASS_INLINE_HPP\n+\n+#include \"oops\/compressedKlass.hpp\"\n+#include \"memory\/allStatic.hpp\"\n+#include \"utilities\/align.hpp\"\n+#include \"utilities\/globalDefinitions.hpp\"\n+\n+\n+static inline bool check_alignment(Klass* v) {\n+  return (intptr_t)v % KlassAlignmentInBytes == 0;\n+}\n+\n+inline Klass* CompressedKlassPointers::decode_raw(narrowKlass v) {\n+  return decode_raw(v, base());\n+}\n+\n+inline Klass* CompressedKlassPointers::decode_raw(narrowKlass v, address narrow_base) {\n+  return (Klass*)(void*)((uintptr_t)narrow_base +((uintptr_t)v << shift()));\n+}\n+\n+inline Klass* CompressedKlassPointers::decode_not_null(narrowKlass v) {\n+  return decode_not_null(v, base());\n+}\n+\n+inline Klass* CompressedKlassPointers::decode_not_null(narrowKlass v, address narrow_base) {\n+  assert(!is_null(v), \"narrow klass value can never be zero\");\n+  Klass* result = decode_raw(v, narrow_base);\n+  DEBUG_ONLY(verify_klass_pointer(result, narrow_base));\n+  return result;\n+}\n+\n+inline Klass* CompressedKlassPointers::decode(narrowKlass v) {\n+  return is_null(v) ? nullptr : decode_not_null(v);\n+}\n+\n+inline narrowKlass CompressedKlassPointers::encode_not_null(Klass* v) {\n+  return encode_not_null(v, base());\n+}\n+\n+inline narrowKlass CompressedKlassPointers::encode_not_null(Klass* v, address narrow_base) {\n+  DEBUG_ONLY(verify_klass_pointer(v, narrow_base));\n+  uint64_t v2 = (uint64_t)(pointer_delta((void*)v, narrow_base, 1));\n+  v2 >>= shift();\n+  assert(v2 <= UINT_MAX, \"narrow klass pointer overflow\");\n+  narrowKlass result = (narrowKlass)v2;\n+  DEBUG_ONLY(verify_narrow_klass_pointer(result));\n+  assert(decode_not_null(result, narrow_base) == v, \"reversibility\");\n+  return result;\n+}\n+\n+inline narrowKlass CompressedKlassPointers::encode(Klass* v) {\n+  return is_null(v) ? (narrowKlass)0 : encode_not_null(v);\n+}\n+\n+#ifdef ASSERT\n+inline void CompressedKlassPointers::verify_klass_pointer(const Klass* v, address narrow_base) {\n+  assert(is_aligned(v, KlassAlignmentInBytes), \"misaligned Klass* pointer (\" PTR_FORMAT \")\", p2i(v));\n+  address end = narrow_base + KlassEncodingMetaspaceMax;\n+  assert((address)v >= narrow_base && (address)v < end,\n+         \"Klass (\" PTR_FORMAT \") located outside encoding range [\" PTR_FORMAT \", \" PTR_FORMAT \")\",\n+         p2i(v), p2i(narrow_base), p2i(end));\n+}\n+\n+inline void CompressedKlassPointers::verify_klass_pointer(const Klass* v) {\n+  verify_klass_pointer(v, base());\n+}\n+\n+inline void CompressedKlassPointers::verify_narrow_klass_pointer(narrowKlass v) {\n+  \/\/ Make sure we only use the lower n bits\n+  assert((((uint64_t)v) & ~NarrowKlassPointerBitMask) == 0, \"%x: not a valid narrow klass pointer\", v);\n+}\n+#endif\n+\n+#endif \/\/ SHARE_OOPS_COMPRESSEDOOPS_HPP\n","filename":"src\/hotspot\/share\/oops\/compressedKlass.inline.hpp","additions":101,"deletions":0,"binary":false,"changes":101,"status":"added"},{"patch":"@@ -180,123 +180,0 @@\n-\n-\/\/ For UseCompressedClassPointers.\n-NarrowPtrStruct CompressedKlassPointers::_narrow_klass = { nullptr, 0, true };\n-\n-\/\/ CompressedClassSpaceSize set to 1GB, but appear 3GB away from _narrow_ptrs_base during CDS dump.\n-\/\/ (Todo: we should #ifdef out CompressedKlassPointers for 32bit completely and fix all call sites which\n-\/\/  are compiled for 32bit to LP64_ONLY).\n-size_t CompressedKlassPointers::_range = 0;\n-\n-\n-\/\/ Given an address range [addr, addr+len) which the encoding is supposed to\n-\/\/  cover, choose base, shift and range.\n-\/\/  The address range is the expected range of uncompressed Klass pointers we\n-\/\/  will encounter (and the implicit promise that there will be no Klass\n-\/\/  structures outside this range).\n-void CompressedKlassPointers::initialize(address addr, size_t len) {\n-#ifdef _LP64\n-  assert(is_valid_base(addr), \"Address must be a valid encoding base\");\n-  address const end = addr + len;\n-\n-  address base;\n-  int shift;\n-  size_t range;\n-\n-  if (UseSharedSpaces || DumpSharedSpaces) {\n-\n-    \/\/ Special requirements if CDS is active:\n-    \/\/ Encoding base and shift must be the same between dump and run time.\n-    \/\/   CDS takes care that the SharedBaseAddress and CompressedClassSpaceSize\n-    \/\/   are the same. Archive size will be probably different at runtime, but\n-    \/\/   it can only be smaller than at, never larger, since archives get\n-    \/\/   shrunk at the end of the dump process.\n-    \/\/   From that it follows that the range [addr, len) we are handed in at\n-    \/\/   runtime will start at the same address then at dumptime, and its len\n-    \/\/   may be smaller at runtime then it was at dump time.\n-    \/\/\n-    \/\/ To be very careful here, we avoid any optimizations and just keep using\n-    \/\/  the same address and shift value. Specifically we avoid using zero-based\n-    \/\/  encoding. We also set the expected value range to 4G (encoding range\n-    \/\/  cannot be larger than that).\n-\n-    base = addr;\n-\n-    \/\/ JDK-8265705\n-    \/\/ This is a temporary fix for aarch64: there, if the range-to-be-encoded is located\n-    \/\/  below 32g, either encoding base should be zero or base should be aligned to 4G\n-    \/\/  and shift should be zero. The simplest way to fix this for now is to force\n-    \/\/  shift to zero for both runtime and dumptime.\n-    \/\/ Note however that this is not a perfect solution. Ideally this whole function\n-    \/\/  should be CDS agnostic, that would simplify it - and testing - a lot. See JDK-8267141\n-    \/\/  for details.\n-    shift = 0;\n-\n-    \/\/ This must be true since at dumptime cds+ccs is 4G, at runtime it can\n-    \/\/  only be smaller, see comment above.\n-    assert(len <= 4 * G, \"Encoding range cannot be larger than 4G\");\n-    range = 4 * G;\n-\n-  } else {\n-\n-    \/\/ Otherwise we attempt to use a zero base if the range fits in lower 32G.\n-    if (end <= (address)KlassEncodingMetaspaceMax) {\n-      base = 0;\n-    } else {\n-      base = addr;\n-    }\n-\n-    \/\/ Highest offset a Klass* can ever have in relation to base.\n-    range = end - base;\n-\n-    \/\/ We may not even need a shift if the range fits into 32bit:\n-    const uint64_t UnscaledClassSpaceMax = (uint64_t(max_juint) + 1);\n-    if (range < UnscaledClassSpaceMax) {\n-      shift = 0;\n-    } else {\n-      shift = LogKlassAlignmentInBytes;\n-    }\n-\n-  }\n-\n-  set_base(base);\n-  set_shift(shift);\n-  set_range(range);\n-#else\n-  fatal(\"64bit only.\");\n-#endif\n-}\n-\n-\/\/ Given an address p, return true if p can be used as an encoding base.\n-\/\/  (Some platforms have restrictions of what constitutes a valid base address).\n-bool CompressedKlassPointers::is_valid_base(address p) {\n-#ifdef AARCH64\n-  \/\/ Below 32G, base must be aligned to 4G.\n-  \/\/ Above that point, base must be aligned to 32G\n-  if (p < (address)(32 * G)) {\n-    return is_aligned(p, 4 * G);\n-  }\n-  return is_aligned(p, (4 << LogKlassAlignmentInBytes) * G);\n-#else\n-  return true;\n-#endif\n-}\n-\n-void CompressedKlassPointers::print_mode(outputStream* st) {\n-  st->print_cr(\"Narrow klass base: \" PTR_FORMAT \", Narrow klass shift: %d, \"\n-               \"Narrow klass range: \" SIZE_FORMAT_X, p2i(base()), shift(),\n-               range());\n-}\n-\n-void CompressedKlassPointers::set_base(address base) {\n-  assert(UseCompressedClassPointers, \"no compressed klass ptrs?\");\n-  _narrow_klass._base   = base;\n-}\n-\n-void CompressedKlassPointers::set_shift(int shift)       {\n-  assert(shift == 0 || shift == LogKlassAlignmentInBytes, \"invalid shift for klass ptrs\");\n-  _narrow_klass._shift   = shift;\n-}\n-\n-void CompressedKlassPointers::set_range(size_t range) {\n-  assert(UseCompressedClassPointers, \"no compressed klass ptrs?\");\n-  _range = range;\n-}\n","filename":"src\/hotspot\/share\/oops\/compressedOops.cpp","additions":0,"deletions":123,"binary":false,"changes":123,"status":"modified"},{"patch":"@@ -143,53 +143,0 @@\n-\/\/ For UseCompressedClassPointers.\n-class CompressedKlassPointers : public AllStatic {\n-  friend class VMStructs;\n-\n-  static NarrowPtrStruct _narrow_klass;\n-\n-  \/\/ Together with base, this defines the address range within which Klass\n-  \/\/  structures will be located: [base, base+range). While the maximal\n-  \/\/  possible encoding range is 4|32G for shift 0|3, if we know beforehand\n-  \/\/  the expected range of Klass* pointers will be smaller, a platform\n-  \/\/  could use this info to optimize encoding.\n-  static size_t _range;\n-\n-  static void set_base(address base);\n-  static void set_range(size_t range);\n-\n-public:\n-\n-  static void set_shift(int shift);\n-\n-\n-  \/\/ Given an address p, return true if p can be used as an encoding base.\n-  \/\/  (Some platforms have restrictions of what constitutes a valid base\n-  \/\/   address).\n-  static bool is_valid_base(address p);\n-\n-  \/\/ Given an address range [addr, addr+len) which the encoding is supposed to\n-  \/\/  cover, choose base, shift and range.\n-  \/\/  The address range is the expected range of uncompressed Klass pointers we\n-  \/\/  will encounter (and the implicit promise that there will be no Klass\n-  \/\/  structures outside this range).\n-  static void initialize(address addr, size_t len);\n-\n-  static void     print_mode(outputStream* st);\n-\n-  static address  base()               { return  _narrow_klass._base; }\n-  static size_t   range()              { return  _range; }\n-  static int      shift()              { return  _narrow_klass._shift; }\n-\n-  static bool is_null(Klass* v)      { return v == nullptr; }\n-  static bool is_null(narrowKlass v) { return v == 0; }\n-\n-  static inline Klass* decode_raw(narrowKlass v, address base);\n-  static inline Klass* decode_raw(narrowKlass v);\n-  static inline Klass* decode_not_null(narrowKlass v);\n-  static inline Klass* decode_not_null(narrowKlass v, address base);\n-  static inline Klass* decode(narrowKlass v);\n-  static inline narrowKlass encode_not_null(Klass* v);\n-  static inline narrowKlass encode_not_null(Klass* v, address base);\n-  static inline narrowKlass encode(Klass* v);\n-\n-};\n-\n","filename":"src\/hotspot\/share\/oops\/compressedOops.hpp","additions":0,"deletions":53,"binary":false,"changes":53,"status":"modified"},{"patch":"@@ -121,46 +121,0 @@\n-static inline bool check_alignment(Klass* v) {\n-  return (intptr_t)v % KlassAlignmentInBytes == 0;\n-}\n-\n-inline Klass* CompressedKlassPointers::decode_raw(narrowKlass v) {\n-  return decode_raw(v, base());\n-}\n-\n-inline Klass* CompressedKlassPointers::decode_raw(narrowKlass v, address narrow_base) {\n-  return (Klass*)((uintptr_t)narrow_base +((uintptr_t)v << shift()));\n-}\n-\n-inline Klass* CompressedKlassPointers::decode_not_null(narrowKlass v) {\n-  return decode_not_null(v, base());\n-}\n-\n-inline Klass* CompressedKlassPointers::decode_not_null(narrowKlass v, address narrow_base) {\n-  assert(!is_null(v), \"narrow klass value can never be zero\");\n-  Klass* result = decode_raw(v, narrow_base);\n-  assert(check_alignment(result), \"address not aligned: \" PTR_FORMAT, p2i(result));\n-  return result;\n-}\n-\n-inline Klass* CompressedKlassPointers::decode(narrowKlass v) {\n-  return is_null(v) ? nullptr : decode_not_null(v);\n-}\n-\n-inline narrowKlass CompressedKlassPointers::encode_not_null(Klass* v) {\n-  return encode_not_null(v, base());\n-}\n-\n-inline narrowKlass CompressedKlassPointers::encode_not_null(Klass* v, address narrow_base) {\n-  assert(!is_null(v), \"klass value can never be zero\");\n-  assert(check_alignment(v), \"Address not aligned\");\n-  uint64_t pd = (uint64_t)(pointer_delta(v, narrow_base, 1));\n-  assert(KlassEncodingMetaspaceMax > pd, \"change encoding max if new encoding\");\n-  uint64_t result = pd >> shift();\n-  assert((result & CONST64(0xffffffff00000000)) == 0, \"narrow klass pointer overflow\");\n-  assert(decode_not_null(result, narrow_base) == v, \"reversibility\");\n-  return (narrowKlass)result;\n-}\n-\n-inline narrowKlass CompressedKlassPointers::encode(Klass* v) {\n-  return is_null(v) ? (narrowKlass)0 : encode_not_null(v);\n-}\n-\n","filename":"src\/hotspot\/share\/oops\/compressedOops.inline.hpp","additions":0,"deletions":46,"binary":false,"changes":46,"status":"modified"},{"patch":"@@ -53,0 +53,1 @@\n+#include \"runtime\/globals.hpp\"\n@@ -54,0 +55,1 @@\n+#include \"utilities\/align.hpp\"\n@@ -195,1 +197,3 @@\n-  return Metaspace::allocate(loader_data, word_size, MetaspaceObj::ClassType, THREAD);\n+  MetaWord* p = Metaspace::allocate(loader_data, word_size, MetaspaceObj::ClassType, THREAD);\n+  assert(is_aligned(p, KlassAlignmentInBytes), \"metaspace returned badly aligned memory.\");\n+  return p;\n@@ -203,0 +207,1 @@\n+                           _prototype_header(markWord::prototype() LP64_ONLY(.set_klass(this))),\n@@ -747,0 +752,2 @@\n+     st->print(BULLET\"prototype_header: \" INTPTR_FORMAT, _prototype_header.value());\n+     st->cr();\n@@ -770,0 +777,4 @@\n+  if (UseCompressedClassPointers) {\n+    assert(is_aligned(this, KlassAlignmentInBytes), \"misaligned Klass structure\");\n+  }\n+\n","filename":"src\/hotspot\/share\/oops\/klass.cpp","additions":12,"deletions":1,"binary":false,"changes":13,"status":"modified"},{"patch":"@@ -168,0 +168,2 @@\n+  markWord _prototype_header;   \/\/ Used to initialize objects' header\n+\n@@ -671,0 +673,4 @@\n+  markWord prototype_header() const      { return _prototype_header; }\n+  inline void set_prototype_header(markWord header);\n+  static ByteSize prototype_header_offset() { return in_ByteSize(offset_of(Klass, _prototype_header)); }\n+\n","filename":"src\/hotspot\/share\/oops\/klass.hpp","additions":6,"deletions":0,"binary":false,"changes":6,"status":"modified"},{"patch":"@@ -26,0 +26,1 @@\n+#include \"oops\/compressedOops.inline.hpp\"\n","filename":"src\/hotspot\/share\/oops\/markWord.cpp","additions":1,"deletions":0,"binary":false,"changes":1,"status":"modified"},{"patch":"@@ -159,1 +159,0 @@\n-  assert(obj->is_objArray(), \"must be object array\");\n","filename":"src\/hotspot\/share\/oops\/objArrayKlass.cpp","additions":0,"deletions":1,"binary":false,"changes":1,"status":"modified"},{"patch":"@@ -55,23 +55,3 @@\n-  \/\/ Give size of objArrayOop in HeapWords minus the header\n-  static int array_size(int length) {\n-    const uint OopsPerHeapWord = HeapWordSize\/heapOopSize;\n-    assert(OopsPerHeapWord >= 1 && (HeapWordSize % heapOopSize == 0),\n-           \"Else the following (new) computation would be in error\");\n-    uint res = ((uint)length + OopsPerHeapWord - 1)\/OopsPerHeapWord;\n-#ifdef ASSERT\n-    \/\/ The old code is left in for sanity-checking; it'll\n-    \/\/ go away pretty soon. XXX\n-    \/\/ Without UseCompressedOops, this is simply:\n-    \/\/ oop->length() * HeapWordsPerOop;\n-    \/\/ With narrowOops, HeapWordsPerOop is 1\/2 or equal 0 as an integer.\n-    \/\/ The oop elements are aligned up to wordSize\n-    const uint HeapWordsPerOop = heapOopSize\/HeapWordSize;\n-    uint old_res;\n-    if (HeapWordsPerOop > 0) {\n-      old_res = length * HeapWordsPerOop;\n-    } else {\n-      old_res = align_up((uint)length, OopsPerHeapWord)\/OopsPerHeapWord;\n-    }\n-    assert(res == old_res, \"Inconsistency between old and new.\");\n-#endif  \/\/ ASSERT\n-    return res;\n+  \/\/ Give size of objArrayOop in bytes minus the header\n+  static size_t array_size_in_bytes(int length) {\n+    return (size_t)length * heapOopSize;\n@@ -97,1 +77,0 @@\n-  static int header_size()    { return arrayOopDesc::header_size(T_OBJECT); }\n@@ -102,5 +81,5 @@\n-    uint asz = array_size(length);\n-    uint osz = align_object_size(header_size() + asz);\n-    assert(osz >= asz,   \"no overflow\");\n-    assert((int)osz > 0, \"no overflow\");\n-    return (size_t)osz;\n+    size_t asz = array_size_in_bytes(length);\n+    size_t size_words = align_up(base_offset_in_bytes() + asz, HeapWordSize) \/ HeapWordSize;\n+    size_t osz = align_object_size(size_words);\n+    assert(osz < max_jint, \"no overflow\");\n+    return osz;\n","filename":"src\/hotspot\/share\/oops\/objArrayOop.hpp","additions":8,"deletions":29,"binary":false,"changes":37,"status":"modified"},{"patch":"@@ -33,0 +33,1 @@\n+#include \"oops\/compressedKlass.inline.hpp\"\n@@ -37,0 +38,1 @@\n+#include \"runtime\/interfaceSupport.inline.hpp\"\n@@ -125,1 +127,1 @@\n-  if (ignore_mark_word) {\n+  if (ignore_mark_word || UseFastLocking) {\n@@ -157,20 +159,8 @@\n-bool oopDesc::has_klass_gap() {\n-  \/\/ Only has a klass gap when compressed class pointers are used.\n-  return UseCompressedClassPointers;\n-}\n-\n-#if INCLUDE_CDS_JAVA_HEAP\n-void oopDesc::set_narrow_klass(narrowKlass nk) {\n-  assert(DumpSharedSpaces, \"Used by CDS only. Do not abuse!\");\n-  assert(UseCompressedClassPointers, \"must be\");\n-  _metadata._compressed_klass = nk;\n-}\n-#endif\n-\n-  if (UseCompressedClassPointers) {\n-    narrowKlass narrow_klass = obj->_metadata._compressed_klass;\n-    if (narrow_klass == 0) return nullptr;\n-    return (void*)CompressedKlassPointers::decode_raw(narrow_klass);\n-  } else {\n-    return obj->_metadata._klass;\n-  }\n+  \/\/ TODO: Remove method altogether and replace with calls to obj->klass() ?\n+  \/\/ OTOH, we may eventually get rid of locking in header, and then no\n+  \/\/ longer have to deal with that anymore.\n+#ifdef _LP64\n+  return obj->klass();\n+#else\n+  return obj->_klass;\n+#endif\n","filename":"src\/hotspot\/share\/oops\/oop.cpp","additions":11,"deletions":21,"binary":false,"changes":32,"status":"modified"},{"patch":"@@ -31,0 +31,1 @@\n+#include \"oops\/compressedKlass.hpp\"\n@@ -57,4 +58,3 @@\n-  union _metadata {\n-    Klass*      _klass;\n-    narrowKlass _compressed_klass;\n-  } _metadata;\n+#ifndef _LP64\n+  Klass*            _klass;\n+#endif\n@@ -78,0 +78,1 @@\n+  static inline void release_set_mark(HeapWord* mem, markWord m);\n@@ -81,0 +82,2 @@\n+  inline markWord resolve_mark() const;\n+\n@@ -91,1 +94,1 @@\n-  void set_narrow_klass(narrowKlass nk) NOT_CDS_JAVA_HEAP_RETURN;\n+#ifndef _LP64\n@@ -94,3 +97,1 @@\n-\n-  \/\/ For klass field compression\n-  static inline void set_klass_gap(HeapWord* mem, int z);\n+#endif\n@@ -263,0 +264,1 @@\n+  inline void forward_to_self();\n@@ -269,0 +271,1 @@\n+  inline oop forward_to_self_atomic(markWord compare, atomic_memory_order order = memory_order_conservative);\n@@ -271,0 +274,1 @@\n+  inline oop forwardee(markWord header) const;\n@@ -310,2 +314,0 @@\n-  static bool has_klass_gap();\n-\n@@ -314,4 +316,7 @@\n-  static int klass_offset_in_bytes()     { return offset_of(oopDesc, _metadata._klass); }\n-  static int klass_gap_offset_in_bytes() {\n-    assert(has_klass_gap(), \"only applicable to compressed klass pointers\");\n-    return klass_offset_in_bytes() + sizeof(narrowKlass);\n+  static int klass_offset_in_bytes()     {\n+#ifdef _LP64\n+    STATIC_ASSERT(markWord::klass_shift % 8 == 0);\n+    return mark_offset_in_bytes() + markWord::klass_shift \/ 8;\n+#else\n+    return offset_of(oopDesc, _klass);\n+#endif\n","filename":"src\/hotspot\/share\/oops\/oop.hpp","additions":19,"deletions":14,"binary":false,"changes":33,"status":"modified"},{"patch":"@@ -35,0 +35,1 @@\n+#include \"oops\/compressedKlass.inline.hpp\"\n@@ -37,1 +38,1 @@\n-#include \"oops\/markWord.hpp\"\n+#include \"oops\/markWord.inline.hpp\"\n@@ -41,0 +42,3 @@\n+#include \"runtime\/objectMonitor.inline.hpp\"\n+#include \"runtime\/safepoint.hpp\"\n+#include \"runtime\/synchronizer.hpp\"\n@@ -73,0 +77,4 @@\n+void oopDesc::release_set_mark(HeapWord* mem, markWord m) {\n+  Atomic::release_store((markWord*)(((char*)mem) + mark_offset_in_bytes()), m);\n+}\n+\n@@ -81,0 +89,9 @@\n+markWord oopDesc::resolve_mark() const {\n+  markWord hdr = mark();\n+  if (hdr.has_monitor()) {\n+    ObjectMonitor* monitor = hdr.monitor();\n+    return monitor->header();\n+  }\n+  return hdr;\n+}\n+\n@@ -82,1 +99,8 @@\n-  set_mark(markWord::prototype());\n+#ifdef _LP64\n+  markWord header = resolve_mark();\n+  assert(UseCompressedClassPointers, \"expect compressed klass pointers\");\n+  header = markWord((header.value() & markWord::klass_mask_in_place) | markWord::prototype().value());\n+#else\n+  markWord header = markWord::prototype();\n+#endif\n+  set_mark(header);\n@@ -86,5 +110,7 @@\n-  if (UseCompressedClassPointers) {\n-    return CompressedKlassPointers::decode_not_null(_metadata._compressed_klass);\n-  } else {\n-    return _metadata._klass;\n-  }\n+#ifdef _LP64\n+  assert(UseCompressedClassPointers, \"only with compressed class pointers\");\n+  markWord header = resolve_mark();\n+  return header.klass();\n+#else\n+  return _klass;\n+#endif\n@@ -94,5 +120,7 @@\n-  if (UseCompressedClassPointers) {\n-    return CompressedKlassPointers::decode(_metadata._compressed_klass);\n-  } else {\n-    return _metadata._klass;\n-  }\n+#ifdef _LP64\n+  assert(UseCompressedClassPointers, \"only with compressed class pointers\");\n+  markWord header = resolve_mark();\n+  return header.klass_or_null();\n+#else\n+  return _klass;\n+#endif\n@@ -102,5 +130,5 @@\n-  if (UseCompressedClassPointers) {\n-    narrowKlass nklass = Atomic::load_acquire(&_metadata._compressed_klass);\n-    return CompressedKlassPointers::decode(nklass);\n-  } else {\n-    return Atomic::load_acquire(&_metadata._klass);\n+#ifdef _LP64\n+  assert(UseCompressedClassPointers, \"only with compressed class pointers\");\n+  markWord header = mark_acquire();\n+  if (header.has_monitor()) {\n+    header = header.monitor()->header();\n@@ -108,0 +136,4 @@\n+  return header.klass_or_null();\n+#else\n+  return Atomic::load_acquire(&_klass);\n+#endif\n@@ -111,5 +143,1 @@\n-  if (UseCompressedClassPointers) {\n-    return CompressedKlassPointers::decode_raw(_metadata._compressed_klass);\n-  } else {\n-    return _metadata._klass;\n-  }\n+  return klass();\n@@ -118,0 +146,1 @@\n+#ifndef _LP64\n@@ -120,5 +149,1 @@\n-  if (UseCompressedClassPointers) {\n-    _metadata._compressed_klass = CompressedKlassPointers::encode_not_null(k);\n-  } else {\n-    _metadata._klass = k;\n-  }\n+  _klass = k;\n@@ -137,6 +162,1 @@\n-\n-void oopDesc::set_klass_gap(HeapWord* mem, int v) {\n-  if (UseCompressedClassPointers) {\n-    *(int*)(((char*)mem) + klass_gap_offset_in_bytes()) = v;\n-  }\n-}\n+#endif\n@@ -271,1 +291,15 @@\n-  assert(m.decode_pointer() == p, \"encoding must be reversible\");\n+  assert(forwardee(m) == p, \"encoding must be reversable\");\n+  set_mark(m);\n+}\n+\n+void oopDesc::forward_to_self() {\n+#ifdef _LP64\n+  verify_forwardee(this);\n+  markWord m = mark();\n+  \/\/ If mark is displaced, we need to preserve the Klass* from real header.\n+  assert(SafepointSynchronize::is_at_safepoint(), \"we can only safely fetch the displaced header at safepoint\");\n+  if (m.has_displaced_mark_helper()) {\n+    m = m.displaced_mark_helper();\n+  }\n+  m = m.set_self_forwarded();\n+  assert(forwardee(m) == cast_to_oop(this), \"encoding must be reversable\");\n@@ -273,0 +307,3 @@\n+#else\n+  forward_to(oop(this));\n+#endif\n@@ -278,1 +315,1 @@\n-  assert(m.decode_pointer() == p, \"encoding must be reversible\");\n+  assert(forwardee(m) == p, \"encoding must be reversable\");\n@@ -283,1 +320,1 @@\n-    return cast_to_oop(old_mark.decode_pointer());\n+    return forwardee(old_mark);\n@@ -287,0 +324,23 @@\n+oop oopDesc::forward_to_self_atomic(markWord compare, atomic_memory_order order) {\n+#ifdef _LP64\n+  verify_forwardee(this);\n+  markWord m = compare;\n+  \/\/ If mark is displaced, we need to preserve the Klass* from real header.\n+  assert(SafepointSynchronize::is_at_safepoint(), \"we can only safely fetch the displaced header at safepoint\");\n+  if (m.has_displaced_mark_helper()) {\n+    m = m.displaced_mark_helper();\n+  }\n+  m = m.set_self_forwarded();\n+  assert(forwardee(m) == cast_to_oop(this), \"encoding must be reversable\");\n+  markWord old_mark = cas_set_mark(m, compare, order);\n+  if (old_mark == compare) {\n+    return NULL;\n+  } else {\n+    assert(old_mark.is_marked(), \"must be marked here\");\n+    return forwardee(old_mark);\n+  }\n+#else\n+  return forward_to_atomic(oop(this), compare, order);\n+#endif\n+}\n+\n@@ -291,2 +351,14 @@\n-  assert(is_forwarded(), \"only decode when actually forwarded\");\n-  return cast_to_oop(mark().decode_pointer());\n+  return forwardee(mark());\n+}\n+\n+oop oopDesc::forwardee(markWord header) const {\n+  assert(header.is_marked(), \"must be forwarded\");\n+#ifdef _LP64\n+  if (header.self_forwarded()) {\n+    return cast_to_oop(this);\n+  } else\n+#endif\n+  {\n+    assert(header.is_marked(), \"only decode when actually forwarded\");\n+    return cast_to_oop(header.decode_pointer());\n+  }\n@@ -347,1 +419,0 @@\n-  assert(k == klass(), \"wrong klass\");\n","filename":"src\/hotspot\/share\/oops\/oop.inline.hpp","additions":110,"deletions":39,"binary":false,"changes":149,"status":"modified"},{"patch":"@@ -40,3 +40,0 @@\n-\/\/ If compressed klass pointers then use narrowKlass.\n-typedef juint  narrowKlass;\n-\n","filename":"src\/hotspot\/share\/oops\/oopsHierarchy.hpp","additions":0,"deletions":3,"binary":false,"changes":3,"status":"modified"},{"patch":"@@ -231,1 +231,0 @@\n-  assert(obj->is_typeArray(),\"must be a type array\");\n","filename":"src\/hotspot\/share\/oops\/typeArrayKlass.cpp","additions":0,"deletions":1,"binary":false,"changes":1,"status":"modified"},{"patch":"@@ -1024,0 +1024,1 @@\n+  reset_max_monitors();\n","filename":"src\/hotspot\/share\/opto\/compile.cpp","additions":1,"deletions":0,"binary":false,"changes":1,"status":"modified"},{"patch":"@@ -340,0 +340,1 @@\n+  uint                  _max_monitors;          \/\/ Keep track of maximum number of active monitors in this compilation\n@@ -634,0 +635,4 @@\n+  void          push_monitor() { _max_monitors++; }\n+  void          reset_max_monitors() { _max_monitors = 0; }\n+  uint          max_monitors() { return _max_monitors; }\n+\n","filename":"src\/hotspot\/share\/opto\/compile.hpp","additions":5,"deletions":0,"binary":false,"changes":5,"status":"modified"},{"patch":"@@ -426,0 +426,1 @@\n+    C->push_monitor();\n","filename":"src\/hotspot\/share\/opto\/parse1.cpp","additions":1,"deletions":0,"binary":false,"changes":1,"status":"modified"},{"patch":"@@ -5153,1 +5153,2 @@\n-    int header_size = objArrayOopDesc::header_size() * wordSize;\n+    BasicType basic_elem_type = elem()->basic_type();\n+    int header_size = arrayOopDesc::base_offset_in_bytes(basic_elem_type);\n","filename":"src\/hotspot\/share\/opto\/type.cpp","additions":2,"deletions":1,"binary":false,"changes":3,"status":"modified"},{"patch":"@@ -1507,3 +1507,0 @@\n-      if (COMPRESSED_CLASS_POINTERS_DEPENDS_ON_COMPRESSED_OOPS) {\n-        FLAG_SET_DEFAULT(UseCompressedClassPointers, false);\n-      }\n@@ -1515,1 +1512,0 @@\n-\n@@ -1520,25 +1516,14 @@\n-  \/\/ On some architectures, the use of UseCompressedClassPointers implies the use of\n-  \/\/ UseCompressedOops. The reason is that the rheap_base register of said platforms\n-  \/\/ is reused to perform some optimized spilling, in order to use rheap_base as a\n-  \/\/ temp register. But by treating it as any other temp register, spilling can typically\n-  \/\/ be completely avoided instead. So it is better not to perform this trick. And by\n-  \/\/ not having that reliance, large heaps, or heaps not supporting compressed oops,\n-  \/\/ can still use compressed class pointers.\n-  if (COMPRESSED_CLASS_POINTERS_DEPENDS_ON_COMPRESSED_OOPS && !UseCompressedOops) {\n-    if (UseCompressedClassPointers) {\n-      warning(\"UseCompressedClassPointers requires UseCompressedOops\");\n-    }\n-    FLAG_SET_DEFAULT(UseCompressedClassPointers, false);\n-  } else {\n-    \/\/ Turn on UseCompressedClassPointers too\n-    if (FLAG_IS_DEFAULT(UseCompressedClassPointers)) {\n-      FLAG_SET_ERGO(UseCompressedClassPointers, true);\n-    }\n-    \/\/ Check the CompressedClassSpaceSize to make sure we use compressed klass ptrs.\n-    if (UseCompressedClassPointers) {\n-      if (CompressedClassSpaceSize > KlassEncodingMetaspaceMax) {\n-        warning(\"CompressedClassSpaceSize is too large for UseCompressedClassPointers\");\n-        FLAG_SET_DEFAULT(UseCompressedClassPointers, false);\n-      }\n-    }\n-  }\n+  if (!UseCompressedClassPointers) {\n+    \/\/ Lilliput requires compressed class pointers. Default shall reflect that.\n+    \/\/ If user specifies -UseCompressedClassPointers, it should be reverted with\n+    \/\/ a warning.\n+    assert(!FLAG_IS_DEFAULT(UseCompressedClassPointers), \"Wrong default for UseCompressedClassPointers\");\n+    warning(\"Lilliput reqires compressed class pointers.\");\n+    FLAG_SET_ERGO(UseCompressedClassPointers, true);\n+  }\n+  \/\/ Assert validity of compressed class space size. User arg should have been checked at this point\n+  \/\/ (see CompressedClassSpaceSizeConstraintFunc()), so no need to be nice about it, this fires in\n+  \/\/ case the default is wrong.\n+  assert(CompressedClassSpaceSize <= Metaspace::max_class_space_size(),\n+         \"CompressedClassSpaceSize \" SIZE_FORMAT \" too large (max: \" SIZE_FORMAT \")\",\n+         CompressedClassSpaceSize, Metaspace::max_class_space_size());\n@@ -1707,3 +1692,0 @@\n-          if (COMPRESSED_CLASS_POINTERS_DEPENDS_ON_COMPRESSED_OOPS) {\n-            FLAG_SET_ERGO(UseCompressedClassPointers, false);\n-          }\n@@ -3128,0 +3110,3 @@\n+  \/\/ Lilliput requires fast-locking.\n+  FLAG_SET_DEFAULT(UseFastLocking, true);\n+\n@@ -4101,0 +4086,1 @@\n+\n","filename":"src\/hotspot\/share\/runtime\/arguments.cpp","additions":18,"deletions":32,"binary":false,"changes":50,"status":"modified"},{"patch":"@@ -128,1 +128,1 @@\n-  product(bool, UseCompressedClassPointers, false,                          \\\n+  product(bool, UseCompressedClassPointers, true,                           \\\n@@ -1059,1 +1059,1 @@\n-  develop(bool, UseHeavyMonitors, false,                                    \\\n+  product(bool, UseHeavyMonitors, false, DIAGNOSTIC,                        \\\n@@ -1416,1 +1416,1 @@\n-          range(1*M, 3*G)                                                   \\\n+          constraint(CompressedClassSpaceSizeConstraintFunc,AtParse)        \\\n@@ -1418,1 +1418,1 @@\n-  develop(size_t, CompressedClassSpaceBaseAddress, 0,                       \\\n+  product(size_t, CompressedClassSpaceBaseAddress, 0, DIAGNOSTIC,           \\\n@@ -1981,0 +1981,9 @@\n+                                                                            \\\n+  product(bool, HeapObjectStats, false, DIAGNOSTIC,                         \\\n+             \"Enable gathering of heap object statistics\")                  \\\n+                                                                            \\\n+  product(size_t, HeapObjectStatsSamplingInterval, 500, DIAGNOSTIC,         \\\n+             \"Heap object statistics sampling interval (ms)\")               \\\n+                                                                            \\\n+  product(bool, UseFastLocking, false, EXPERIMENTAL,                        \\\n+                \"Use fast-locking instead of stack-locking\")                \\\n","filename":"src\/hotspot\/share\/runtime\/globals.hpp","additions":13,"deletions":4,"binary":false,"changes":17,"status":"modified"},{"patch":"@@ -73,0 +73,1 @@\n+#include \"runtime\/lockStack.inline.hpp\"\n@@ -490,2 +491,3 @@\n-  _SleepEvent(ParkEvent::Allocate(this))\n-{\n+  _SleepEvent(ParkEvent::Allocate(this)),\n+\n+  _lock_stack() {\n@@ -988,0 +990,1 @@\n+  assert(!UseFastLocking, \"should not be called with fast-locking\");\n@@ -1381,0 +1384,4 @@\n+\n+  if (!UseHeavyMonitors && UseFastLocking) {\n+    lock_stack().oops_do(f);\n+  }\n","filename":"src\/hotspot\/share\/runtime\/javaThread.cpp","additions":9,"deletions":2,"binary":false,"changes":11,"status":"modified"},{"patch":"@@ -42,0 +42,1 @@\n+#include \"oops\/compressedKlass.hpp\"\n","filename":"src\/hotspot\/share\/runtime\/os.cpp","additions":1,"deletions":0,"binary":false,"changes":1,"status":"modified"},{"patch":"@@ -201,2 +201,1 @@\n-  volatile_nonstatic_field(oopDesc,            _metadata._klass,                              Klass*)                                \\\n-  volatile_nonstatic_field(oopDesc,            _metadata._compressed_klass,                   narrowKlass)                           \\\n+  NOT_LP64(volatile_nonstatic_field(oopDesc,   _klass,                                        Klass*))                               \\\n@@ -380,2 +379,2 @@\n-     static_field(CompressedKlassPointers,     _narrow_klass._base,                           address)                               \\\n-     static_field(CompressedKlassPointers,     _narrow_klass._shift,                          int)                                   \\\n+     static_field(CompressedKlassPointers,     _base,                           address)                                             \\\n+     static_field(CompressedKlassPointers,     _shift_copy,                          int)                                            \\\n@@ -702,0 +701,3 @@\n+  nonstatic_field(JavaThread,                  _lock_stack,                                   LockStack)                             \\\n+  nonstatic_field(LockStack,                   _current,                                      oop*)                                  \\\n+  nonstatic_field(LockStack,                   _base,                                         oop*)                                  \\\n@@ -1315,0 +1317,1 @@\n+  declare_toplevel_type(LockStack)                                        \\\n@@ -2605,0 +2608,1 @@\n+  LP64_ONLY(declare_constant(markWord::klass_shift))                      \\\n","filename":"src\/hotspot\/share\/runtime\/vmStructs.cpp","additions":8,"deletions":4,"binary":false,"changes":12,"status":"modified"},{"patch":"@@ -598,5 +598,0 @@\n-const int LogKlassAlignmentInBytes = 3;\n-const int LogKlassAlignment        = LogKlassAlignmentInBytes - LogHeapWordSize;\n-const int KlassAlignmentInBytes    = 1 << LogKlassAlignmentInBytes;\n-const int KlassAlignment           = KlassAlignmentInBytes \/ HeapWordSize;\n-\n@@ -610,5 +605,0 @@\n-\/\/ Maximal size of compressed class space. Above this limit compression is not possible.\n-\/\/ Also upper bound for placement of zero based class space. (Class space is further limited\n-\/\/ to be < 3G, see arguments.cpp.)\n-const  uint64_t KlassEncodingMetaspaceMax = (uint64_t(max_juint) + 1) << LogKlassAlignmentInBytes;\n-\n","filename":"src\/hotspot\/share\/utilities\/globalDefinitions.hpp","additions":0,"deletions":10,"binary":false,"changes":10,"status":"modified"},{"patch":"@@ -39,0 +39,1 @@\n+#include \"oops\/compressedKlass.hpp\"\n","filename":"src\/hotspot\/share\/utilities\/vmError.cpp","additions":1,"deletions":0,"binary":false,"changes":1,"status":"modified"},{"patch":"@@ -30,0 +30,1 @@\n+#include \"memory\/metaspace\/metaspaceAlignment.hpp\"\n@@ -53,5 +54,0 @@\n-\/\/ See metaspaceArena.cpp : needed for predicting commit sizes.\n-namespace metaspace {\n-  extern size_t get_raw_word_size_for_requested_word_size(size_t net_word_size);\n-}\n-\n@@ -65,0 +61,1 @@\n+  int _alignment_words;\n@@ -67,1 +64,2 @@\n-  void initialize(const ArenaGrowthPolicy* growth_policy, const char* name = \"gtest-MetaspaceArena\") {\n+  void initialize(const ArenaGrowthPolicy* growth_policy, int alignment_words,\n+                  const char* name = \"gtest-MetaspaceArena\") {\n@@ -74,1 +72,1 @@\n-      _arena = new MetaspaceArena(&_context.cm(), _growth_policy, _lock, &_used_words_counter, name);\n+      _arena = new MetaspaceArena(&_context.cm(), _growth_policy, alignment_words, _lock, &_used_words_counter, name);\n@@ -88,1 +86,1 @@\n-    initialize(ArenaGrowthPolicy::policy_for_space_type(space_type, is_class), name);\n+    initialize(ArenaGrowthPolicy::policy_for_space_type(space_type, is_class), metaspace::MetaspaceMinAlignmentWords, name);\n@@ -96,1 +94,1 @@\n-    initialize(growth_policy, name);\n+    initialize(growth_policy, metaspace::MetaspaceMinAlignmentWords, name);\n@@ -275,1 +273,1 @@\n-    allocated += metaspace::get_raw_word_size_for_requested_word_size(s);\n+    allocated += metaspace::get_raw_word_size_for_requested_word_size(s, metaspace::MetaspaceMinAlignmentWords);\n@@ -332,1 +330,1 @@\n-    allocated += metaspace::get_raw_word_size_for_requested_word_size(s);\n+    allocated += metaspace::get_raw_word_size_for_requested_word_size(s, metaspace::MetaspaceMinAlignmentWords);\n@@ -596,1 +594,1 @@\n-    words_allocated += metaspace::get_raw_word_size_for_requested_word_size(alloc_words);\n+    words_allocated += metaspace::get_raw_word_size_for_requested_word_size(alloc_words, metaspace::MetaspaceMinAlignmentWords);\n","filename":"test\/hotspot\/gtest\/metaspace\/test_metaspacearena.cpp","additions":10,"deletions":12,"binary":false,"changes":22,"status":"modified"},{"patch":"@@ -170,0 +170,12 @@\n+\n+Lilliput temporary:\n+compiler\/c2\/irTests\/TestVectorizationNotRun.java 8301785 generic-all\n+runtime\/CompressedOops\/CompressedClassPointerEncoding.java#x64-area-beyond-encoding-range-use-xor 8302094 windows-all,macosx-all\n+runtime\/CompressedOops\/CompressedClassPointerEncoding.java#x64-area-partly-within-encoding-range-use-add 8302094 windows-all,macosx-all\n+runtime\/CompressedOops\/CompressedClassPointerEncoding.java#x64-area-within-encoding-range-use-zero 8302094 windows-all,macosx-all\n+runtime\/CompressedOops\/CompressedClassPointerEncoding.java#x64-area-far-out-no-low-bits-use-xor 8302094 windows-all,macosx-all\n+runtime\/CompressedOops\/CompressedClassPointerEncoding.java#x64-area-far-out-with-low-bits-use-add 8302094 windows-all,macosx-all\n+runtime\/CompressedOops\/CompressedClassPointerEncoding.java#aarch64-xor 8302094 windows-all,macosx-all\n+runtime\/CompressedOops\/CompressedClassPointerEncoding.java#aarch64-movk-1 8302094 windows-all,macosx-all\n+runtime\/CompressedOops\/CompressedClassPointerEncoding.java#aarch64-movk-2 8302094 windows-all,macosx-all\n+runtime\/CompressedOops\/CompressedClassPointerEncoding.java#aarch64-movk-3 8302094 windows-all,macosx-all\n","filename":"test\/hotspot\/jtreg\/ProblemList.txt","additions":12,"deletions":0,"binary":false,"changes":12,"status":"modified"},{"patch":"@@ -61,0 +61,3 @@\n+    \/* Lilliput: cannot work due to drastically reduced narrow klass pointer range (atm 2g and that may get\n+       smaller still). There is an argument for improving CDS\/CCS reservation and make it more likely to run\n+       zero-based, but that logic has to be rethought.\n@@ -75,0 +78,1 @@\n+     *\/\n@@ -78,0 +82,1 @@\n+    \/* Lilliput: See comment above.\n@@ -92,0 +97,1 @@\n+    *\/\n@@ -96,0 +102,2 @@\n+    \/* Lilliput: I am not sure what the point of this test CCS reservation is independent from\n+       heap. See below the desparate attempts to predict heap reservation on PPC. Why do we even care?\n@@ -116,0 +124,1 @@\n+     *\/\n@@ -121,0 +130,1 @@\n+    \/* Lilliput: narrow klass pointer range drastically reduced. See comments under smallHeapTest().\n@@ -140,0 +150,1 @@\n+    *\/\n@@ -142,0 +153,4 @@\n+    \/* Lilliput: not sure what the point of this test is. The ability to have a class space if heap uses\n+       large pages? Why would that be a problem? Kept alive for now since it makes no problems even with\n+       smaller class pointers.\n+     *\/\n@@ -202,0 +217,1 @@\n+    \/* Lilliput: narrow klass pointer range drastically reduced. See comments under smallHeapTest().\n@@ -217,0 +233,1 @@\n+    *\/\n@@ -218,0 +235,1 @@\n+    \/* Lilliput: narrow klass pointer range drastically reduced. See comments under smallHeapTest().\n@@ -237,0 +255,1 @@\n+    *\/\n@@ -238,0 +257,1 @@\n+    \/* Lilliput: narrow klass pointer range drastically reduced. See comments under smallHeapTest().\n@@ -257,0 +277,1 @@\n+    *\/\n@@ -325,4 +346,4 @@\n-        smallHeapTest();\n-        smallHeapTestWith1G();\n-        largeHeapTest();\n-        largeHeapAbove32GTest();\n+        \/\/ smallHeapTest();\n+        \/\/ smallHeapTestWith1G();\n+        \/\/ largeHeapTest();\n+        \/\/ largeHeapAbove32GTest();\n@@ -333,3 +354,3 @@\n-        smallHeapTestNoCoop();\n-        smallHeapTestWith1GNoCoop();\n-        largeHeapTestNoCoop();\n+        \/\/ smallHeapTestNoCoop();\n+        \/\/ smallHeapTestWith1GNoCoop();\n+        \/\/ largeHeapTestNoCoop();\n","filename":"test\/hotspot\/jtreg\/runtime\/CompressedOops\/CompressedClassPointers.java","additions":28,"deletions":7,"binary":false,"changes":35,"status":"modified"},{"patch":"@@ -60,20 +60,0 @@\n-\/*\n- * @test id=test-64bit-noccs\n- * @summary Test the VM.metaspace command\n- * @requires vm.bits == \"64\"\n- * @library \/test\/lib\n- * @modules java.base\/jdk.internal.misc\n- *          java.management\n- * @run main\/othervm -Dwithout-compressed-class-space -XX:MaxMetaspaceSize=201M -Xmx100M -XX:-UseCompressedOops -XX:-UseCompressedClassPointers PrintMetaspaceDcmd\n- *\/\n-\n- \/*\n- * @test id=test-nospecified\n- * @summary Test the VM.metaspace command\n- * @requires vm.bits == \"64\"\n- * @library \/test\/lib\n- * @modules java.base\/jdk.internal.misc\n- *          java.management\n- * @run main\/othervm -Dno-specified-flag -Xmx100M -XX:-UseCompressedOops -XX:-UseCompressedClassPointers PrintMetaspaceDcmd\n- *\/\n-\n","filename":"test\/hotspot\/jtreg\/runtime\/Metaspace\/PrintMetaspaceDcmd.java","additions":0,"deletions":20,"binary":false,"changes":20,"status":"modified"},{"patch":"@@ -134,2 +134,1 @@\n-        runCheck(new String[] {\"-XX:+IgnoreUnrecognizedVMOptions\", \"-XX:-UseCompressedClassPointers\"},\n-                 BadFailOnConstraint.create(Loads.class, \"load()\", 1, 1, \"Load\"),\n+        runCheck(BadFailOnConstraint.create(Loads.class, \"load()\", 1, 1, \"Load\"),\n","filename":"test\/hotspot\/jtreg\/testlibrary_tests\/ir_framework\/tests\/TestIRMatching.java","additions":1,"deletions":2,"binary":false,"changes":3,"status":"modified"}]}