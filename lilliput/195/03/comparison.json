{"files":[{"patch":"@@ -119,1 +119,1 @@\n-  const idx_t res_bit = _beg_bits.find_last_set_bit_aligned_left(beg_bit, end_bit);\n+  const idx_t res_bit = _beg_bits.find_last_set_bit(beg_bit, end_bit);\n","filename":"src\/hotspot\/share\/gc\/parallel\/parMarkBitMap.inline.hpp","additions":1,"deletions":1,"binary":false,"changes":2,"status":"modified"},{"patch":"@@ -32,0 +32,1 @@\n+#include \"gc\/parallel\/psParallelCompactNew.inline.hpp\"\n@@ -123,2 +124,8 @@\n-  if (!PSParallelCompact::initialize_aux_data()) {\n-    return JNI_ENOMEM;\n+  if (UseCompactObjectHeaders) {\n+    if (!PSParallelCompactNew::initialize_aux_data()) {\n+      return JNI_ENOMEM;\n+    }\n+  } else {\n+    if (!PSParallelCompact::initialize_aux_data()) {\n+      return JNI_ENOMEM;\n+    }\n@@ -187,1 +194,5 @@\n-  PSParallelCompact::post_initialize();\n+  if (UseCompactObjectHeaders) {\n+    PSParallelCompactNew::post_initialize();\n+  } else {\n+    PSParallelCompact::post_initialize();\n+  }\n@@ -431,1 +442,5 @@\n-  PSParallelCompact::invoke(clear_all_soft_refs);\n+  if (UseCompactObjectHeaders) {\n+    PSParallelCompactNew::invoke(clear_all_soft_refs, false \/* serial *\/);\n+  } else {\n+    PSParallelCompact::invoke(clear_all_soft_refs);\n+  }\n@@ -474,1 +489,5 @@\n-    PSParallelCompact::invoke(clear_all_soft_refs);\n+    if (UseCompactObjectHeaders) {\n+      PSParallelCompactNew::invoke(clear_all_soft_refs, false \/* serial *\/);\n+    } else {\n+      PSParallelCompact::invoke(clear_all_soft_refs);\n+    }\n@@ -485,0 +504,9 @@\n+  if (UseCompactObjectHeaders) {\n+    PSParallelCompactNew::invoke(true \/* clear_soft_refs *\/, true \/* serial *\/);\n+  }\n+\n+  result = expand_heap_and_allocate(size, is_tlab);\n+  if (result != nullptr) {\n+    return result;\n+  }\n+\n@@ -597,1 +625,5 @@\n-  PSParallelCompact::invoke(clear_soft_refs);\n+  if (UseCompactObjectHeaders) {\n+    PSParallelCompactNew::invoke(clear_soft_refs, false \/* serial *\/);\n+  } else {\n+    PSParallelCompact::invoke(clear_soft_refs);\n+  }\n@@ -738,1 +770,5 @@\n-  PSParallelCompact::print_on_error(st);\n+  if (UseCompactObjectHeaders) {\n+    PSParallelCompactNew::print_on_error(st);\n+  } else {\n+    PSParallelCompact::print_on_error(st);\n+  }\n@@ -748,1 +784,5 @@\n-  log_debug(gc, heap, exit)(\"Accumulated old generation GC time %3.7f secs\", PSParallelCompact::accumulated_time()->seconds());\n+  if (UseCompactObjectHeaders) {\n+    log_debug(gc, heap, exit)(\"Accumulated old generation GC time %3.7f secs\", PSParallelCompactNew::accumulated_time()->seconds());\n+  } else {\n+    log_debug(gc, heap, exit)(\"Accumulated old generation GC time %3.7f secs\", PSParallelCompact::accumulated_time()->seconds());\n+  }\n","filename":"src\/hotspot\/share\/gc\/parallel\/parallelScavengeHeap.cpp","additions":48,"deletions":8,"binary":false,"changes":56,"status":"modified"},{"patch":"@@ -0,0 +1,167 @@\n+\/*\n+ * Copyright (c) 2005, 2025, Oracle and\/or its affiliates. All rights reserved.\n+ * Copyright Amazon.com Inc. or its affiliates. All Rights Reserved.\n+ * DO NOT ALTER OR REMOVE COPYRIGHT NOTICES OR THIS FILE HEADER.\n+ *\n+ * This code is free software; you can redistribute it and\/or modify it\n+ * under the terms of the GNU General Public License version 2 only, as\n+ * published by the Free Software Foundation.\n+ *\n+ * This code is distributed in the hope that it will be useful, but WITHOUT\n+ * ANY WARRANTY; without even the implied warranty of MERCHANTABILITY or\n+ * FITNESS FOR A PARTICULAR PURPOSE.  See the GNU General Public License\n+ * version 2 for more details (a copy is included in the LICENSE file that\n+ * accompanied this code).\n+ *\n+ * You should have received a copy of the GNU General Public License version\n+ * 2 along with this work; if not, write to the Free Software Foundation,\n+ * Inc., 51 Franklin St, Fifth Floor, Boston, MA 02110-1301 USA.\n+ *\n+ * Please contact Oracle, 500 Oracle Parkway, Redwood Shores, CA 94065 USA\n+ * or visit www.oracle.com if you need additional information or have any\n+ * questions.\n+ *\n+ *\/\n+\n+#include \"gc\/parallel\/objectStartArray.hpp\"\n+#include \"gc\/parallel\/parMarkBitMap.inline.hpp\"\n+#include \"gc\/parallel\/parallelScavengeHeap.hpp\"\n+#include \"gc\/parallel\/psCompactionManagerNew.inline.hpp\"\n+#include \"gc\/parallel\/psOldGen.hpp\"\n+#include \"gc\/parallel\/psParallelCompactNew.inline.hpp\"\n+#include \"gc\/shared\/partialArraySplitter.inline.hpp\"\n+#include \"gc\/shared\/partialArrayState.hpp\"\n+#include \"gc\/shared\/preservedMarks.inline.hpp\"\n+#include \"memory\/iterator.inline.hpp\"\n+\n+PSOldGen*                  ParCompactionManagerNew::_old_gen = nullptr;\n+ParCompactionManagerNew**  ParCompactionManagerNew::_manager_array = nullptr;\n+\n+ParCompactionManagerNew::PSMarkTasksQueueSet*  ParCompactionManagerNew::_marking_stacks = nullptr;\n+PartialArrayStateManager* ParCompactionManagerNew::_partial_array_state_manager = nullptr;\n+\n+ObjectStartArray*    ParCompactionManagerNew::_start_array = nullptr;\n+ParMarkBitMap*       ParCompactionManagerNew::_mark_bitmap = nullptr;\n+\n+PreservedMarksSet* ParCompactionManagerNew::_preserved_marks_set = nullptr;\n+\n+ParCompactionManagerNew::ParCompactionManagerNew(PreservedMarks* preserved_marks,\n+                                           ReferenceProcessor* ref_processor,\n+                                           uint parallel_gc_threads)\n+  :_partial_array_splitter(_partial_array_state_manager, parallel_gc_threads),\n+   _mark_and_push_closure(this, ref_processor) {\n+\n+  _old_gen = ParallelScavengeHeap::old_gen();\n+  _start_array = old_gen()->start_array();\n+\n+  _preserved_marks = preserved_marks;\n+}\n+\n+void ParCompactionManagerNew::initialize(ParMarkBitMap* mbm) {\n+  assert(ParallelScavengeHeap::heap() != nullptr, \"Needed for initialization\");\n+  assert(PSParallelCompactNew::ref_processor() != nullptr, \"precondition\");\n+  assert(ParallelScavengeHeap::heap()->workers().max_workers() != 0, \"Not initialized?\");\n+\n+  _mark_bitmap = mbm;\n+\n+  uint parallel_gc_threads = ParallelScavengeHeap::heap()->workers().max_workers();\n+\n+  assert(_manager_array == nullptr, \"Attempt to initialize twice\");\n+  _manager_array = NEW_C_HEAP_ARRAY(ParCompactionManagerNew*, parallel_gc_threads, mtGC);\n+\n+  assert(_partial_array_state_manager == nullptr, \"Attempt to initialize twice\");\n+  _partial_array_state_manager\n+    = new PartialArrayStateManager(parallel_gc_threads);\n+  _marking_stacks = new PSMarkTasksQueueSet(parallel_gc_threads);\n+\n+  _preserved_marks_set = new PreservedMarksSet(true);\n+  _preserved_marks_set->init(parallel_gc_threads);\n+\n+  \/\/ Create and register the ParCompactionManagerNew(s) for the worker threads.\n+  for(uint i=0; i<parallel_gc_threads; i++) {\n+    _manager_array[i] = new ParCompactionManagerNew(_preserved_marks_set->get(i),\n+                                                 PSParallelCompactNew::ref_processor(),\n+                                                 parallel_gc_threads);\n+    marking_stacks()->register_queue(i, _manager_array[i]->marking_stack());\n+  }\n+}\n+\n+void ParCompactionManagerNew::flush_all_string_dedup_requests() {\n+  uint parallel_gc_threads = ParallelScavengeHeap::heap()->workers().max_workers();\n+  for (uint i=0; i<parallel_gc_threads; i++) {\n+    _manager_array[i]->flush_string_dedup_requests();\n+  }\n+}\n+\n+ParCompactionManagerNew*\n+ParCompactionManagerNew::gc_thread_compaction_manager(uint index) {\n+  assert(index < ParallelGCThreads, \"index out of range\");\n+  assert(_manager_array != nullptr, \"Sanity\");\n+  return _manager_array[index];\n+}\n+\n+void ParCompactionManagerNew::push_objArray(oop obj) {\n+  assert(obj->is_objArray(), \"precondition\");\n+  _mark_and_push_closure.do_klass(obj->klass());\n+\n+  objArrayOop obj_array = objArrayOop(obj);\n+  size_t array_length = obj_array->length();\n+  size_t initial_chunk_size =\n+    _partial_array_splitter.start(&_marking_stack, obj_array, nullptr, array_length);\n+  follow_array(obj_array, 0, initial_chunk_size);\n+}\n+\n+void ParCompactionManagerNew::process_array_chunk(PartialArrayState* state, bool stolen) {\n+  \/\/ Access before release by claim().\n+  oop obj = state->source();\n+  PartialArraySplitter::Claim claim =\n+    _partial_array_splitter.claim(state, &_marking_stack, stolen);\n+  follow_array(objArrayOop(obj), claim._start, claim._end);\n+}\n+\n+void ParCompactionManagerNew::follow_marking_stacks() {\n+  ScannerTask task;\n+  do {\n+    \/\/ First, try to move tasks from the overflow stack into the shared buffer, so\n+    \/\/ that other threads can steal. Otherwise process the overflow stack first.\n+    while (marking_stack()->pop_overflow(task)) {\n+      if (!marking_stack()->try_push_to_taskqueue(task)) {\n+        follow_contents(task, false);\n+      }\n+    }\n+    while (marking_stack()->pop_local(task)) {\n+      follow_contents(task, false);\n+    }\n+  } while (!marking_stack_empty());\n+\n+  assert(marking_stack_empty(), \"Sanity\");\n+}\n+\n+#if TASKQUEUE_STATS\n+void ParCompactionManagerNew::print_and_reset_taskqueue_stats() {\n+  marking_stacks()->print_and_reset_taskqueue_stats(\"Marking Stacks\");\n+\n+  auto get_pa_stats = [&](uint i) {\n+    return _manager_array[i]->partial_array_task_stats();\n+  };\n+  PartialArrayTaskStats::log_set(ParallelGCThreads, get_pa_stats,\n+                                 \"Partial Array Task Stats\");\n+  uint parallel_gc_threads = ParallelScavengeHeap::heap()->workers().max_workers();\n+  for (uint i = 0; i < parallel_gc_threads; ++i) {\n+    get_pa_stats(i)->reset();\n+  }\n+}\n+\n+PartialArrayTaskStats* ParCompactionManagerNew::partial_array_task_stats() {\n+  return _partial_array_splitter.stats();\n+}\n+#endif \/\/ TASKQUEUE_STATS\n+\n+#ifdef ASSERT\n+void ParCompactionManagerNew::verify_all_marking_stack_empty() {\n+  uint parallel_gc_threads = ParallelGCThreads;\n+  for (uint i = 0; i < parallel_gc_threads; i++) {\n+    assert(_manager_array[i]->marking_stack_empty(), \"Marking stack should be empty\");\n+  }\n+}\n+#endif\n","filename":"src\/hotspot\/share\/gc\/parallel\/psCompactionManagerNew.cpp","additions":167,"deletions":0,"binary":false,"changes":167,"status":"added"},{"patch":"@@ -0,0 +1,165 @@\n+\/*\n+ * Copyright (c) 2005, 2024, Oracle and\/or its affiliates. All rights reserved.\n+ * Copyright Amazon.com Inc. or its affiliates. All Rights Reserved.\n+ * DO NOT ALTER OR REMOVE COPYRIGHT NOTICES OR THIS FILE HEADER.\n+ *\n+ * This code is free software; you can redistribute it and\/or modify it\n+ * under the terms of the GNU General Public License version 2 only, as\n+ * published by the Free Software Foundation.\n+ *\n+ * This code is distributed in the hope that it will be useful, but WITHOUT\n+ * ANY WARRANTY; without even the implied warranty of MERCHANTABILITY or\n+ * FITNESS FOR A PARTICULAR PURPOSE.  See the GNU General Public License\n+ * version 2 for more details (a copy is included in the LICENSE file that\n+ * accompanied this code).\n+ *\n+ * You should have received a copy of the GNU General Public License version\n+ * 2 along with this work; if not, write to the Free Software Foundation,\n+ * Inc., 51 Franklin St, Fifth Floor, Boston, MA 02110-1301 USA.\n+ *\n+ * Please contact Oracle, 500 Oracle Parkway, Redwood Shores, CA 94065 USA\n+ * or visit www.oracle.com if you need additional information or have any\n+ * questions.\n+ *\n+ *\/\n+\n+#ifndef SHARE_GC_PARALLEL_PSCOMPACTIONMANAGERNEW_HPP\n+#define SHARE_GC_PARALLEL_PSCOMPACTIONMANAGERNEW_HPP\n+\n+#include \"classfile\/classLoaderData.hpp\"\n+#include \"gc\/parallel\/psParallelCompactNew.hpp\"\n+#include \"gc\/shared\/partialArraySplitter.hpp\"\n+#include \"gc\/shared\/partialArrayTaskStats.hpp\"\n+#include \"gc\/shared\/partialArrayState.hpp\"\n+#include \"gc\/shared\/preservedMarks.hpp\"\n+#include \"gc\/shared\/stringdedup\/stringDedup.hpp\"\n+#include \"gc\/shared\/taskqueue.hpp\"\n+#include \"gc\/shared\/taskTerminator.hpp\"\n+#include \"memory\/allocation.hpp\"\n+#include \"utilities\/stack.hpp\"\n+\n+class MutableSpace;\n+class PSOldGen;\n+class ParCompactionManagerNew;\n+class ObjectStartArray;\n+class ParMarkBitMap;\n+\n+class PCMarkAndPushClosureNew: public ClaimMetadataVisitingOopIterateClosure {\n+  ParCompactionManagerNew* _compaction_manager;\n+\n+  template <typename T> void do_oop_work(T* p);\n+public:\n+  PCMarkAndPushClosureNew(ParCompactionManagerNew* cm, ReferenceProcessor* rp) :\n+    ClaimMetadataVisitingOopIterateClosure(ClassLoaderData::_claim_stw_fullgc_mark, rp),\n+    _compaction_manager(cm) { }\n+\n+  void do_oop(oop* p) final               { do_oop_work(p); }\n+  void do_oop(narrowOop* p) final         { do_oop_work(p); }\n+};\n+\n+class ParCompactionManagerNew : public CHeapObj<mtGC> {\n+  friend class MarkFromRootsTaskNew;\n+  friend class ParallelCompactRefProcProxyTaskNew;\n+  friend class ParallelScavengeRefProcProxyTask;\n+  friend class ParMarkBitMap;\n+  friend class PSParallelCompactNew;\n+  friend class PCAddThreadRootsMarkingTaskClosureNew;\n+\n+ private:\n+  typedef OverflowTaskQueue<ScannerTask, mtGC>           PSMarkTaskQueue;\n+  typedef GenericTaskQueueSet<PSMarkTaskQueue, mtGC>     PSMarkTasksQueueSet;\n+\n+  static ParCompactionManagerNew** _manager_array;\n+  static PSMarkTasksQueueSet*   _marking_stacks;\n+  static ObjectStartArray*      _start_array;\n+  static PSOldGen*              _old_gen;\n+\n+  static PartialArrayStateManager*  _partial_array_state_manager;\n+  PartialArraySplitter              _partial_array_splitter;\n+\n+  PSMarkTaskQueue               _marking_stack;\n+\n+  PCMarkAndPushClosureNew _mark_and_push_closure;\n+\n+  static PreservedMarksSet* _preserved_marks_set;\n+  PreservedMarks* _preserved_marks;\n+\n+  static ParMarkBitMap* _mark_bitmap;\n+\n+  StringDedup::Requests _string_dedup_requests;\n+\n+  static PSOldGen* old_gen()             { return _old_gen; }\n+  static ObjectStartArray* start_array() { return _start_array; }\n+  static PSMarkTasksQueueSet* marking_stacks()  { return _marking_stacks; }\n+\n+  static void initialize(ParMarkBitMap* mbm);\n+\n+  ParCompactionManagerNew(PreservedMarks* preserved_marks,\n+                       ReferenceProcessor* ref_processor,\n+                       uint parallel_gc_threads);\n+\n+  inline PSMarkTaskQueue*  marking_stack() { return &_marking_stack; }\n+  inline void push(PartialArrayState* stat);\n+  void push_objArray(oop obj);\n+\n+#if TASKQUEUE_STATS\n+  static void print_and_reset_taskqueue_stats();\n+  PartialArrayTaskStats* partial_array_task_stats();\n+#endif \/\/ TASKQUEUE_STATS\n+\n+public:\n+  void flush_string_dedup_requests() {\n+    _string_dedup_requests.flush();\n+  }\n+\n+  static void flush_all_string_dedup_requests();\n+\n+  \/\/ Get the compaction manager when doing evacuation work from the VM thread.\n+  \/\/ Simply use the first compaction manager here.\n+  static ParCompactionManagerNew* get_vmthread_cm() { return _manager_array[0]; }\n+\n+  PreservedMarks* preserved_marks() const {\n+    return _preserved_marks;\n+  }\n+\n+  static ParMarkBitMap* mark_bitmap() { return _mark_bitmap; }\n+\n+  \/\/ Save for later processing.  Must not fail.\n+  inline void push(oop obj);\n+\n+  \/\/ Check mark and maybe push on marking stack.\n+  template <typename T> inline void mark_and_push(T* p);\n+\n+  \/\/ Access function for compaction managers\n+  static ParCompactionManagerNew* gc_thread_compaction_manager(uint index);\n+\n+  static bool steal(uint queue_num, ScannerTask& t);\n+\n+  \/\/ Process tasks remaining on marking stack\n+  void follow_marking_stacks();\n+  inline bool marking_stack_empty() const;\n+\n+  inline void follow_contents(const ScannerTask& task, bool stolen);\n+  inline void follow_array(objArrayOop array, size_t start, size_t end);\n+  void process_array_chunk(PartialArrayState* state, bool stolen);\n+\n+  class FollowStackClosure: public VoidClosure {\n+   private:\n+    ParCompactionManagerNew* _compaction_manager;\n+    TaskTerminator* _terminator;\n+    uint _worker_id;\n+   public:\n+    FollowStackClosure(ParCompactionManagerNew* cm, TaskTerminator* terminator, uint worker_id)\n+      : _compaction_manager(cm), _terminator(terminator), _worker_id(worker_id) { }\n+    void do_void() final;\n+  };\n+\n+  \/\/ Called after marking.\n+  static void verify_all_marking_stack_empty() NOT_DEBUG_RETURN;\n+};\n+\n+bool ParCompactionManagerNew::marking_stack_empty() const {\n+  return _marking_stack.is_empty();\n+}\n+\n+#endif \/\/ SHARE_GC_PARALLEL_PSCOMPACTIONMANAGERNEW_HPP\n","filename":"src\/hotspot\/share\/gc\/parallel\/psCompactionManagerNew.hpp","additions":165,"deletions":0,"binary":false,"changes":165,"status":"added"},{"patch":"@@ -0,0 +1,127 @@\n+\/*\n+ * Copyright (c) 2010, 2024, Oracle and\/or its affiliates. All rights reserved.\n+ * Copyright Amazon.com Inc. or its affiliates. All Rights Reserved.\n+ * DO NOT ALTER OR REMOVE COPYRIGHT NOTICES OR THIS FILE HEADER.\n+ *\n+ * This code is free software; you can redistribute it and\/or modify it\n+ * under the terms of the GNU General Public License version 2 only, as\n+ * published by the Free Software Foundation.\n+ *\n+ * This code is distributed in the hope that it will be useful, but WITHOUT\n+ * ANY WARRANTY; without even the implied warranty of MERCHANTABILITY or\n+ * FITNESS FOR A PARTICULAR PURPOSE.  See the GNU General Public License\n+ * version 2 for more details (a copy is included in the LICENSE file that\n+ * accompanied this code).\n+ *\n+ * You should have received a copy of the GNU General Public License version\n+ * 2 along with this work; if not, write to the Free Software Foundation,\n+ * Inc., 51 Franklin St, Fifth Floor, Boston, MA 02110-1301 USA.\n+ *\n+ * Please contact Oracle, 500 Oracle Parkway, Redwood Shores, CA 94065 USA\n+ * or visit www.oracle.com if you need additional information or have any\n+ * questions.\n+ *\n+ *\/\n+\n+#ifndef SHARE_GC_PARALLEL_PSCOMPACTIONMANAGERNEW_INLINE_HPP\n+#define SHARE_GC_PARALLEL_PSCOMPACTIONMANAGERNEW_INLINE_HPP\n+\n+#include \"gc\/parallel\/psCompactionManagerNew.hpp\"\n+\n+#include \"classfile\/classLoaderData.hpp\"\n+#include \"classfile\/javaClasses.inline.hpp\"\n+#include \"gc\/parallel\/parMarkBitMap.hpp\"\n+#include \"gc\/parallel\/psParallelCompactNew.inline.hpp\"\n+#include \"gc\/parallel\/psStringDedup.hpp\"\n+#include \"gc\/shared\/partialArrayState.hpp\"\n+#include \"gc\/shared\/partialArrayTaskStepper.inline.hpp\"\n+#include \"gc\/shared\/taskqueue.inline.hpp\"\n+#include \"oops\/access.inline.hpp\"\n+#include \"oops\/arrayOop.hpp\"\n+#include \"oops\/compressedOops.inline.hpp\"\n+#include \"oops\/objArrayOop.inline.hpp\"\n+#include \"oops\/oop.inline.hpp\"\n+#include \"utilities\/debug.hpp\"\n+#include \"utilities\/globalDefinitions.hpp\"\n+\n+template <typename T>\n+inline void PCMarkAndPushClosureNew::do_oop_work(T* p) {\n+  _compaction_manager->mark_and_push(p);\n+}\n+\n+inline bool ParCompactionManagerNew::steal(uint queue_num, ScannerTask& t) {\n+  return marking_stacks()->steal(queue_num, t);\n+}\n+\n+inline void ParCompactionManagerNew::push(oop obj) {\n+  marking_stack()->push(ScannerTask(obj));\n+}\n+\n+inline void ParCompactionManagerNew::push(PartialArrayState* stat) {\n+  marking_stack()->push(ScannerTask(stat));\n+}\n+\n+template <typename T>\n+inline void ParCompactionManagerNew::mark_and_push(T* p) {\n+  T heap_oop = RawAccess<>::oop_load(p);\n+  if (!CompressedOops::is_null(heap_oop)) {\n+    oop obj = CompressedOops::decode_not_null(heap_oop);\n+    assert(ParallelScavengeHeap::heap()->is_in(obj), \"should be in heap\");\n+\n+    if (mark_bitmap()->mark_obj(obj)) {\n+      if (StringDedup::is_enabled() &&\n+          java_lang_String::is_instance(obj) &&\n+          psStringDedup::is_candidate_from_mark(obj)) {\n+        _string_dedup_requests.add(obj);\n+      }\n+\n+      ContinuationGCSupport::transform_stack_chunk(obj);\n+      push(obj);\n+    }\n+  }\n+}\n+\n+inline void ParCompactionManagerNew::FollowStackClosure::do_void() {\n+  _compaction_manager->follow_marking_stacks();\n+  if (_terminator != nullptr) {\n+    steal_marking_work_new(*_terminator, _worker_id);\n+  }\n+}\n+\n+template <typename T>\n+inline void follow_array_specialized(objArrayOop obj, size_t start, size_t end, ParCompactionManagerNew* cm) {\n+  assert(start <= end, \"invariant\");\n+  T* const base = (T*)obj->base();\n+  T* const beg = base + start;\n+  T* const chunk_end = base + end;\n+\n+  \/\/ Push the non-null elements of the next stride on the marking stack.\n+  for (T* e = beg; e < chunk_end; e++) {\n+    cm->mark_and_push<T>(e);\n+  }\n+}\n+\n+inline void ParCompactionManagerNew::follow_array(objArrayOop obj, size_t start, size_t end) {\n+  if (UseCompressedOops) {\n+    follow_array_specialized<narrowOop>(obj, start, end, this);\n+  } else {\n+    follow_array_specialized<oop>(obj, start, end, this);\n+  }\n+}\n+\n+inline void ParCompactionManagerNew::follow_contents(const ScannerTask& task, bool stolen) {\n+  if (task.is_partial_array_state()) {\n+    assert(PSParallelCompactNew::mark_bitmap()->is_marked(task.to_partial_array_state()->source()), \"should be marked\");\n+    process_array_chunk(task.to_partial_array_state(), stolen);\n+  } else {\n+    oop obj = task.to_oop();\n+    assert(PSParallelCompactNew::mark_bitmap()->is_marked(obj), \"should be marked\");\n+    if (obj->is_objArray()) {\n+      push_objArray(obj);\n+    } else {\n+      obj->oop_iterate(&_mark_and_push_closure);\n+    }\n+  }\n+}\n+\n+#endif \/\/ SHARE_GC_PARALLEL_PSCOMPACTIONMANAGERNEW_INLINE_HPP\n","filename":"src\/hotspot\/share\/gc\/parallel\/psCompactionManagerNew.inline.hpp","additions":127,"deletions":0,"binary":false,"changes":127,"status":"added"},{"patch":"@@ -551,1 +551,1 @@\n-class PCAdjustPointerClosure: public BasicOopIterateClosure {\n+class PCAdjustPointerClosureNew: public BasicOopIterateClosure {\n@@ -562,1 +562,1 @@\n-static PCAdjustPointerClosure pc_adjust_pointer_closure;\n+static PCAdjustPointerClosureNew pc_adjust_pointer_closure;\n","filename":"src\/hotspot\/share\/gc\/parallel\/psParallelCompact.cpp","additions":2,"deletions":2,"binary":false,"changes":4,"status":"modified"},{"patch":"@@ -0,0 +1,1151 @@\n+\/*\n+ * Copyright (c) 2005, 2025, Oracle and\/or its affiliates. All rights reserved.\n+ * Copyright Amazon.com Inc. or its affiliates. All Rights Reserved.\n+ * DO NOT ALTER OR REMOVE COPYRIGHT NOTICES OR THIS FILE HEADER.\n+ *\n+ * This code is free software; you can redistribute it and\/or modify it\n+ * under the terms of the GNU General Public License version 2 only, as\n+ * published by the Free Software Foundation.\n+ *\n+ * This code is distributed in the hope that it will be useful, but WITHOUT\n+ * ANY WARRANTY; without even the implied warranty of MERCHANTABILITY or\n+ * FITNESS FOR A PARTICULAR PURPOSE.  See the GNU General Public License\n+ * version 2 for more details (a copy is included in the LICENSE file that\n+ * accompanied this code).\n+ *\n+ * You should have received a copy of the GNU General Public License version\n+ * 2 along with this work; if not, write to the Free Software Foundation,\n+ * Inc., 51 Franklin St, Fifth Floor, Boston, MA 02110-1301 USA.\n+ *\n+ * Please contact Oracle, 500 Oracle Parkway, Redwood Shores, CA 94065 USA\n+ * or visit www.oracle.com if you need additional information or have any\n+ * questions.\n+ *\n+ *\/\n+\n+#include \"classfile\/classLoaderDataGraph.hpp\"\n+#include \"classfile\/symbolTable.hpp\"\n+#include \"classfile\/systemDictionary.hpp\"\n+#include \"code\/codeCache.hpp\"\n+#include \"compiler\/oopMap.hpp\"\n+#include \"gc\/parallel\/objectStartArray.inline.hpp\"\n+#include \"gc\/parallel\/parallelArguments.hpp\"\n+#include \"gc\/parallel\/parallelScavengeHeap.inline.hpp\"\n+#include \"gc\/parallel\/parMarkBitMap.inline.hpp\"\n+#include \"gc\/parallel\/psAdaptiveSizePolicy.hpp\"\n+#include \"gc\/parallel\/psCompactionManagerNew.inline.hpp\"\n+#include \"gc\/parallel\/psOldGen.hpp\"\n+#include \"gc\/parallel\/psParallelCompactNew.inline.hpp\"\n+#include \"gc\/parallel\/psPromotionManager.inline.hpp\"\n+#include \"gc\/parallel\/psScavenge.hpp\"\n+#include \"gc\/parallel\/psYoungGen.hpp\"\n+#include \"gc\/shared\/classUnloadingContext.hpp\"\n+#include \"gc\/shared\/fullGCForwarding.inline.hpp\"\n+#include \"gc\/shared\/gcCause.hpp\"\n+#include \"gc\/shared\/gcHeapSummary.hpp\"\n+#include \"gc\/shared\/gcId.hpp\"\n+#include \"gc\/shared\/gcLocker.hpp\"\n+#include \"gc\/shared\/gcTimer.hpp\"\n+#include \"gc\/shared\/gcTrace.hpp\"\n+#include \"gc\/shared\/gcTraceTime.inline.hpp\"\n+#include \"gc\/shared\/gcVMOperations.hpp\"\n+#include \"gc\/shared\/isGCActiveMark.hpp\"\n+#include \"gc\/shared\/oopStorageSetParState.inline.hpp\"\n+#include \"gc\/shared\/preservedMarks.inline.hpp\"\n+#include \"gc\/shared\/referencePolicy.hpp\"\n+#include \"gc\/shared\/referenceProcessor.hpp\"\n+#include \"gc\/shared\/referenceProcessorPhaseTimes.hpp\"\n+#include \"gc\/shared\/spaceDecorator.hpp\"\n+#include \"gc\/shared\/strongRootsScope.hpp\"\n+#include \"gc\/shared\/taskTerminator.hpp\"\n+#include \"gc\/shared\/weakProcessor.inline.hpp\"\n+#include \"gc\/shared\/workerPolicy.hpp\"\n+#include \"gc\/shared\/workerThread.hpp\"\n+#include \"gc\/shared\/workerUtils.hpp\"\n+#include \"logging\/log.hpp\"\n+#include \"memory\/iterator.inline.hpp\"\n+#include \"memory\/memoryReserver.hpp\"\n+#include \"memory\/metaspaceUtils.hpp\"\n+#include \"memory\/resourceArea.hpp\"\n+#include \"memory\/universe.hpp\"\n+#include \"nmt\/memTracker.hpp\"\n+#include \"oops\/methodData.hpp\"\n+#include \"runtime\/java.hpp\"\n+#include \"runtime\/safepoint.hpp\"\n+#include \"runtime\/threads.hpp\"\n+#include \"runtime\/vmThread.hpp\"\n+#include \"services\/memoryService.hpp\"\n+#include \"utilities\/align.hpp\"\n+#include \"utilities\/debug.hpp\"\n+#include \"utilities\/events.hpp\"\n+#include \"utilities\/formatBuffer.hpp\"\n+#include \"utilities\/macros.hpp\"\n+#if INCLUDE_JVMCI\n+#include \"jvmci\/jvmci.hpp\"\n+#endif\n+\n+SpaceInfoNew PSParallelCompactNew::_space_info[PSParallelCompactNew::last_space_id];\n+\n+size_t PSParallelCompactNew::_num_regions;\n+PCRegionData* PSParallelCompactNew::_region_data_array;\n+size_t PSParallelCompactNew::_num_regions_serial;\n+PCRegionData* PSParallelCompactNew::_region_data_array_serial;\n+PCRegionData** PSParallelCompactNew::_per_worker_region_data;\n+bool PSParallelCompactNew::_serial = false;\n+\n+SpanSubjectToDiscoveryClosure PSParallelCompactNew::_span_based_discoverer;\n+ReferenceProcessor* PSParallelCompactNew::_ref_processor = nullptr;\n+\n+void PSParallelCompactNew::print_on_error(outputStream* st) {\n+  _mark_bitmap.print_on_error(st);\n+}\n+\n+STWGCTimer          PSParallelCompactNew::_gc_timer;\n+ParallelOldTracer   PSParallelCompactNew::_gc_tracer;\n+elapsedTimer        PSParallelCompactNew::_accumulated_time;\n+unsigned int        PSParallelCompactNew::_maximum_compaction_gc_num = 0;\n+CollectorCounters*  PSParallelCompactNew::_counters = nullptr;\n+ParMarkBitMap       PSParallelCompactNew::_mark_bitmap;\n+\n+PSParallelCompactNew::IsAliveClosure PSParallelCompactNew::_is_alive_closure;\n+\n+class PCAdjustPointerClosure: public BasicOopIterateClosure {\n+  template <typename T>\n+  void do_oop_work(T* p) { PSParallelCompactNew::adjust_pointer(p); }\n+\n+public:\n+  void do_oop(oop* p) final          { do_oop_work(p); }\n+  void do_oop(narrowOop* p) final    { do_oop_work(p); }\n+\n+  ReferenceIterationMode reference_iteration_mode() final { return DO_FIELDS; }\n+};\n+\n+static PCAdjustPointerClosure pc_adjust_pointer_closure;\n+\n+class IsAliveClosure: public BoolObjectClosure {\n+public:\n+  bool do_object_b(oop p) final;\n+};\n+\n+\n+bool PSParallelCompactNew::IsAliveClosure::do_object_b(oop p) { return mark_bitmap()->is_marked(p); }\n+\n+void PSParallelCompactNew::post_initialize() {\n+  ParallelScavengeHeap* heap = ParallelScavengeHeap::heap();\n+  _span_based_discoverer.set_span(heap->reserved_region());\n+  _ref_processor =\n+    new ReferenceProcessor(&_span_based_discoverer,\n+                           ParallelGCThreads,   \/\/ mt processing degree\n+                           ParallelGCThreads,   \/\/ mt discovery degree\n+                           false,               \/\/ concurrent_discovery\n+                           &_is_alive_closure); \/\/ non-header is alive closure\n+\n+  _counters = new CollectorCounters(\"Parallel full collection pauses\", 1);\n+\n+  \/\/ Initialize static fields in ParCompactionManager.\n+  ParCompactionManagerNew::initialize(mark_bitmap());\n+}\n+\n+bool PSParallelCompactNew::initialize_aux_data() {\n+  ParallelScavengeHeap* heap = ParallelScavengeHeap::heap();\n+  MemRegion mr = heap->reserved_region();\n+  assert(mr.byte_size() != 0, \"heap should be reserved\");\n+\n+  initialize_space_info();\n+\n+  if (!_mark_bitmap.initialize(mr)) {\n+    vm_shutdown_during_initialization(\n+      err_msg(\"Unable to allocate %zuKB bitmaps for parallel \"\n+      \"garbage collection for the requested %zuKB heap.\",\n+      _mark_bitmap.reserved_byte_size()\/K, mr.byte_size()\/K));\n+    return false;\n+  }\n+\n+  return true;\n+}\n+\n+void PSParallelCompactNew::initialize_space_info()\n+{\n+  memset(&_space_info, 0, sizeof(_space_info));\n+\n+  PSYoungGen* young_gen = ParallelScavengeHeap::young_gen();\n+\n+  _space_info[old_space_id].set_space(ParallelScavengeHeap::old_gen()->object_space());\n+  _space_info[eden_space_id].set_space(young_gen->eden_space());\n+  _space_info[from_space_id].set_space(young_gen->from_space());\n+  _space_info[to_space_id].set_space(young_gen->to_space());\n+\n+  _space_info[old_space_id].set_start_array(ParallelScavengeHeap::old_gen()->start_array());\n+}\n+\n+void\n+PSParallelCompactNew::clear_data_covering_space(SpaceId id)\n+{\n+  \/\/ At this point, top is the value before GC, new_top() is the value that will\n+  \/\/ be set at the end of GC.  The marking bitmap is cleared to top; nothing\n+  \/\/ should be marked above top.\n+  MutableSpace* const space = _space_info[id].space();\n+  HeapWord* const bot = space->bottom();\n+  HeapWord* const top = space->top();\n+\n+  _mark_bitmap.clear_range(bot, top);\n+}\n+\n+void PSParallelCompactNew::pre_compact()\n+{\n+  \/\/ Update the from & to space pointers in space_info, since they are swapped\n+  \/\/ at each young gen gc.  Do the update unconditionally (even though a\n+  \/\/ promotion failure does not swap spaces) because an unknown number of young\n+  \/\/ collections will have swapped the spaces an unknown number of times.\n+  GCTraceTime(Debug, gc, phases) tm(\"Pre Compact\", &_gc_timer);\n+  ParallelScavengeHeap* heap = ParallelScavengeHeap::heap();\n+  _space_info[from_space_id].set_space(ParallelScavengeHeap::young_gen()->from_space());\n+  _space_info[to_space_id].set_space(ParallelScavengeHeap::young_gen()->to_space());\n+\n+  \/\/ Increment the invocation count\n+  heap->increment_total_collections(true);\n+\n+  CodeCache::on_gc_marking_cycle_start();\n+\n+  heap->print_heap_before_gc();\n+  heap->trace_heap_before_gc(&_gc_tracer);\n+\n+  \/\/ Fill in TLABs\n+  heap->ensure_parsability(true);  \/\/ retire TLABs\n+\n+  if (VerifyBeforeGC && heap->total_collections() >= VerifyGCStartAt) {\n+    Universe::verify(\"Before GC\");\n+  }\n+\n+  DEBUG_ONLY(mark_bitmap()->verify_clear();)\n+}\n+\n+void PSParallelCompactNew::post_compact()\n+{\n+  GCTraceTime(Info, gc, phases) tm(\"Post Compact\", &_gc_timer);\n+\n+  CodeCache::on_gc_marking_cycle_finish();\n+  CodeCache::arm_all_nmethods();\n+\n+  for (unsigned int id = old_space_id; id < last_space_id; ++id) {\n+    \/\/ Clear the marking bitmap, summary data and split info.\n+    clear_data_covering_space(SpaceId(id));\n+  }\n+\n+  {\n+    PCRegionData* last_live[last_space_id];\n+    for (uint i = old_space_id; i < last_space_id; ++i) {\n+      last_live[i] = nullptr;\n+    }\n+\n+    \/\/ Figure out last region in each space that has live data.\n+    uint space_id = old_space_id;\n+    MutableSpace* space = _space_info[space_id].space();\n+    size_t num_regions = get_num_regions();\n+    PCRegionData* region_data_array = get_region_data_array();\n+    last_live[space_id] = &region_data_array[0];\n+    for (size_t idx = 0; idx < num_regions; idx++) {\n+      PCRegionData* rd = region_data_array + idx;\n+      if(!space->contains(rd->bottom())) {\n+        ++space_id;\n+        assert(space_id < last_space_id, \"invariant\");\n+        space = _space_info[space_id].space();\n+        log_develop_trace(gc, compaction)(\"Last live for space: %u: %zu\", space_id, idx);\n+        last_live[space_id] = rd;\n+      }\n+      assert(space->contains(rd->bottom()), \"next space should contain next region\");\n+      log_develop_trace(gc, compaction)(\"post-compact region: idx: %zu, bottom: \" PTR_FORMAT \", new_top: \" PTR_FORMAT \", end: \" PTR_FORMAT, rd->idx(), p2i(rd->bottom()), p2i(rd->new_top()), p2i(rd->end()));\n+      if (rd->new_top() > rd->bottom()) {\n+        last_live[space_id] = rd;\n+        log_develop_trace(gc, compaction)(\"Bump last live for space: %u\", space_id);\n+      }\n+    }\n+\n+    for (uint i = old_space_id; i < last_space_id; ++i) {\n+      PCRegionData* rd = last_live[i];\n+        log_develop_trace(gc, compaction)(\n+                \"Last live region in space: %u, compaction region, \" PTR_FORMAT \", #%zu: [\" PTR_FORMAT \", \" PTR_FORMAT \"), new_top: \" PTR_FORMAT,\n+                i, p2i(rd), rd->idx(),\n+                p2i(rd->bottom()), p2i(rd->end()), p2i(rd->new_top()));\n+    }\n+\n+    \/\/ Fill all gaps and update the space boundaries.\n+    space_id = old_space_id;\n+    space = _space_info[space_id].space();\n+    size_t total_live = 0;\n+    size_t total_waste = 0;\n+    for (size_t idx = 0; idx < num_regions; idx++) {\n+      PCRegionData* rd = &region_data_array[idx];\n+      PCRegionData* last_live_in_space = last_live[space_id];\n+      assert(last_live_in_space != nullptr, \"last live must not be null\");\n+      if (rd != last_live_in_space) {\n+        if (rd->new_top() < rd->end()) {\n+          ObjectStartArray* sa = start_array(SpaceId(space_id));\n+          if (sa != nullptr) {\n+            sa->update_for_block(rd->new_top(), rd->end());\n+          }\n+          ParallelScavengeHeap::heap()->fill_with_dummy_object(rd->new_top(), rd->end(), false);\n+        }\n+        size_t live = pointer_delta(rd->new_top(), rd->bottom());\n+        size_t waste = pointer_delta(rd->end(), rd->new_top());\n+        total_live += live;\n+        total_waste += waste;\n+        log_develop_trace(gc, compaction)(\n+                \"Live compaction region, #%zu: [\" PTR_FORMAT \", \" PTR_FORMAT \"), new_top: \" PTR_FORMAT \", live: %zu, waste: %zu\",\n+                rd->idx(),\n+                p2i(rd->bottom()), p2i(rd->end()), p2i(rd->new_top()), live, waste);\n+      } else {\n+        \/\/ Update top of space.\n+        space->set_top(rd->new_top());\n+        size_t live = pointer_delta(rd->new_top(), rd->bottom());\n+        total_live += live;\n+        log_develop_trace(gc, compaction)(\n+                \"Live compaction region, #%zu: [\" PTR_FORMAT \", \" PTR_FORMAT \"), new_top: \" PTR_FORMAT \", live: %zu, waste: %zu\",\n+                rd->idx(),\n+                p2i(rd->bottom()), p2i(rd->end()), p2i(rd->new_top()), live, size_t(0));\n+\n+        \/\/ Fast-Forward to next space.\n+        for (; idx < num_regions - 1; idx++) {\n+          rd = &region_data_array[idx + 1];\n+          if (!space->contains(rd->bottom())) {\n+            space_id++;\n+            assert(space_id < last_space_id, \"must be\");\n+            space = _space_info[space_id].space();\n+            assert(space->contains(rd->bottom()), \"space must contain region\");\n+            break;\n+          }\n+        }\n+      }\n+    }\n+    log_develop_debug(gc, compaction)(\"total live: %zu, total waste: %zu, ratio: %f\", total_live, total_waste, ((float)total_waste)\/((float)(total_live + total_waste)));\n+  }\n+  {\n+    FREE_C_HEAP_ARRAY(PCRegionData*, _per_worker_region_data);\n+    FREE_C_HEAP_ARRAY(PCRegionData, _region_data_array);\n+    FREE_C_HEAP_ARRAY(PCRegionData, _region_data_array_serial);\n+  }\n+#ifdef ASSERT\n+  {\n+    mark_bitmap()->verify_clear();\n+  }\n+#endif\n+\n+  ParCompactionManagerNew::flush_all_string_dedup_requests();\n+\n+  MutableSpace* const eden_space = _space_info[eden_space_id].space();\n+  MutableSpace* const from_space = _space_info[from_space_id].space();\n+  MutableSpace* const to_space   = _space_info[to_space_id].space();\n+\n+  ParallelScavengeHeap* heap = ParallelScavengeHeap::heap();\n+  bool eden_empty = eden_space->is_empty();\n+\n+  \/\/ Update heap occupancy information which is used as input to the soft ref\n+  \/\/ clearing policy at the next gc.\n+  Universe::heap()->update_capacity_and_used_at_gc();\n+\n+  bool young_gen_empty = eden_empty && from_space->is_empty() &&\n+    to_space->is_empty();\n+\n+  PSCardTable* ct = heap->card_table();\n+  MemRegion old_mr = ParallelScavengeHeap::old_gen()->committed();\n+  if (young_gen_empty) {\n+    ct->clear_MemRegion(old_mr);\n+  } else {\n+    ct->dirty_MemRegion(old_mr);\n+  }\n+\n+  {\n+    \/\/ Delete metaspaces for unloaded class loaders and clean up loader_data graph\n+    GCTraceTime(Debug, gc, phases) t(\"Purge Class Loader Data\", gc_timer());\n+    ClassLoaderDataGraph::purge(true \/* at_safepoint *\/);\n+    DEBUG_ONLY(MetaspaceUtils::verify();)\n+  }\n+\n+  \/\/ Need to clear claim bits for the next mark.\n+  ClassLoaderDataGraph::clear_claimed_marks();\n+\n+  heap->prune_scavengable_nmethods();\n+\n+#if COMPILER2_OR_JVMCI\n+  DerivedPointerTable::update_pointers();\n+#endif\n+\n+  \/\/ Signal that we have completed a visit to all live objects.\n+  Universe::heap()->record_whole_heap_examined_timestamp();\n+}\n+\n+void PSParallelCompactNew::setup_regions_parallel() {\n+  static const size_t REGION_SIZE_WORDS = (SpaceAlignment \/ HeapWordSize);\n+  size_t num_regions = 0;\n+  for (uint i = old_space_id; i < last_space_id; ++i) {\n+    MutableSpace* const space = _space_info[i].space();\n+    size_t const space_size_words = space->capacity_in_words();\n+    num_regions += align_up(space_size_words, REGION_SIZE_WORDS) \/ REGION_SIZE_WORDS;\n+  }\n+  _region_data_array = NEW_C_HEAP_ARRAY(PCRegionData, num_regions, mtGC);\n+\n+  size_t region_idx = 0;\n+  for (uint i = old_space_id; i < last_space_id; ++i) {\n+    const MutableSpace* space = _space_info[i].space();\n+    HeapWord* addr = space->bottom();\n+    HeapWord* sp_end = space->end();\n+    HeapWord* sp_top = space->top();\n+    while (addr < sp_end) {\n+      HeapWord* end = MIN2(align_up(addr + REGION_SIZE_WORDS, REGION_SIZE_WORDS), space->end());\n+      if (addr < sp_top) {\n+        HeapWord* prev_obj_start = _mark_bitmap.find_obj_beg_reverse(addr, end);\n+        if (prev_obj_start < end) {\n+          HeapWord* prev_obj_end = prev_obj_start + cast_to_oop(prev_obj_start)->size();\n+          if (end < prev_obj_end) {\n+            \/\/ Object crosses region boundary, adjust end to be after object's last word.\n+            end = prev_obj_end;\n+          }\n+        }\n+      }\n+      assert(region_idx < num_regions, \"must not exceed number of regions: region_idx: %zu, num_regions: %zu\", region_idx, num_regions);\n+      HeapWord* top;\n+      if (sp_top < addr) {\n+        top = addr;\n+      } else if (sp_top >= end) {\n+        top = end;\n+      } else {\n+        top = sp_top;\n+      }\n+      assert(ParallelScavengeHeap::heap()->is_in_reserved(addr), \"addr must be in heap: \" PTR_FORMAT, p2i(addr));\n+      new (_region_data_array + region_idx) PCRegionData(region_idx, addr, top, end);\n+      addr = end;\n+      region_idx++;\n+    }\n+  }\n+  _num_regions = region_idx;\n+  log_info(gc)(\"Number of regions: %zu\", _num_regions);\n+}\n+\n+void PSParallelCompactNew::setup_regions_serial() {\n+  _num_regions_serial = last_space_id;\n+  _region_data_array_serial = NEW_C_HEAP_ARRAY(PCRegionData, _num_regions_serial, mtGC);\n+  new (_region_data_array_serial + old_space_id)  PCRegionData(old_space_id, space(old_space_id)->bottom(), space(old_space_id)->top(), space(old_space_id)->end());\n+  new (_region_data_array_serial + eden_space_id) PCRegionData(eden_space_id, space(eden_space_id)->bottom(), space(eden_space_id)->top(), space(eden_space_id)->end());\n+  new (_region_data_array_serial + from_space_id) PCRegionData(from_space_id, space(from_space_id)->bottom(), space(from_space_id)->top(), space(from_space_id)->end());\n+  new (_region_data_array_serial + to_space_id)   PCRegionData(to_space_id, space(to_space_id)->bottom(), space(to_space_id)->top(), space(to_space_id)->end());\n+}\n+\n+bool PSParallelCompactNew::check_maximum_compaction() {\n+\n+  ParallelScavengeHeap* heap = ParallelScavengeHeap::heap();\n+\n+  \/\/ Check System.GC\n+  bool is_max_on_system_gc = UseMaximumCompactionOnSystemGC\n+                          && GCCause::is_user_requested_gc(heap->gc_cause());\n+\n+  \/\/ JVM flags\n+  const uint total_invocations = heap->total_full_collections();\n+  assert(total_invocations >= _maximum_compaction_gc_num, \"sanity\");\n+  const size_t gcs_since_max = total_invocations - _maximum_compaction_gc_num;\n+  const bool is_interval_ended = gcs_since_max > HeapMaximumCompactionInterval;\n+\n+  if (is_max_on_system_gc || is_interval_ended) {\n+    _maximum_compaction_gc_num = total_invocations;\n+    return true;\n+  }\n+\n+  return false;\n+}\n+\n+void PSParallelCompactNew::summary_phase() {\n+  GCTraceTime(Info, gc, phases) tm(\"Summary Phase\", &_gc_timer);\n+\n+  setup_regions_serial();\n+  setup_regions_parallel();\n+\n+#ifndef PRODUCT\n+  for (size_t idx = 0; idx < _num_regions; idx++) {\n+    PCRegionData* rd = &_region_data_array[idx];\n+    log_develop_trace(gc, compaction)(\"Compaction region #%zu: [\" PTR_FORMAT \", \" PTR_FORMAT \")\", rd->idx(), p2i(\n+            rd->bottom()), p2i(rd->end()));\n+  }\n+#endif\n+}\n+\n+\/\/ This method should contain all heap-specific policy for invoking a full\n+\/\/ collection.  invoke_no_policy() will only attempt to compact the heap; it\n+\/\/ will do nothing further.  If we need to bail out for policy reasons, scavenge\n+\/\/ before full gc, or any other specialized behavior, it needs to be added here.\n+\/\/\n+\/\/ Note that this method should only be called from the vm_thread while at a\n+\/\/ safepoint.\n+\/\/\n+\/\/ Note that the all_soft_refs_clear flag in the soft ref policy\n+\/\/ may be true because this method can be called without intervening\n+\/\/ activity.  For example when the heap space is tight and full measure\n+\/\/ are being taken to free space.\n+bool PSParallelCompactNew::invoke(bool clear_all_soft_refs, bool serial) {\n+  assert(SafepointSynchronize::is_at_safepoint(), \"should be at safepoint\");\n+  assert(Thread::current() == (Thread*)VMThread::vm_thread(),\n+         \"should be in vm thread\");\n+\n+  SvcGCMarker sgcm(SvcGCMarker::FULL);\n+  IsSTWGCActiveMark mark;\n+\n+  ParallelScavengeHeap* heap = ParallelScavengeHeap::heap();\n+  clear_all_soft_refs = clear_all_soft_refs\n+                     || heap->soft_ref_policy()->should_clear_all_soft_refs();\n+\n+  return PSParallelCompactNew::invoke_no_policy(clear_all_soft_refs, serial);\n+}\n+\n+\/\/ This method contains no policy. You should probably\n+\/\/ be calling invoke() instead.\n+bool PSParallelCompactNew::invoke_no_policy(bool clear_all_soft_refs, bool serial) {\n+  assert(SafepointSynchronize::is_at_safepoint(), \"must be at a safepoint\");\n+  assert(ref_processor() != nullptr, \"Sanity\");\n+\n+  if (GCLocker::check_active_before_gc()) {\n+    return false;\n+  }\n+\n+  ParallelScavengeHeap* heap = ParallelScavengeHeap::heap();\n+\n+  GCIdMark gc_id_mark;\n+  _gc_timer.register_gc_start();\n+  _gc_tracer.report_gc_start(heap->gc_cause(), _gc_timer.gc_start());\n+\n+  GCCause::Cause gc_cause = heap->gc_cause();\n+  PSYoungGen* young_gen = ParallelScavengeHeap::young_gen();\n+  PSOldGen* old_gen = ParallelScavengeHeap::old_gen();\n+  PSAdaptiveSizePolicy* size_policy = heap->size_policy();\n+\n+  \/\/ The scope of casr should end after code that can change\n+  \/\/ SoftRefPolicy::_should_clear_all_soft_refs.\n+  ClearedAllSoftRefs casr(clear_all_soft_refs,\n+                          heap->soft_ref_policy());\n+\n+  \/\/ Make sure data structures are sane, make the heap parsable, and do other\n+  \/\/ miscellaneous bookkeeping.\n+  pre_compact();\n+\n+  const PreGenGCValues pre_gc_values = heap->get_pre_gc_values();\n+\n+  {\n+    const uint active_workers =\n+      WorkerPolicy::calc_active_workers(ParallelScavengeHeap::heap()->workers().max_workers(),\n+                                        ParallelScavengeHeap::heap()->workers().active_workers(),\n+                                        Threads::number_of_non_daemon_threads());\n+    ParallelScavengeHeap::heap()->workers().set_active_workers(active_workers);\n+\n+    if (serial || check_maximum_compaction()) {\n+      \/\/ Serial compaction executes the forwarding and compaction phases serially,\n+      \/\/ thus achieving perfect compaction.\n+      \/\/ Marking and ajust-references would still be executed in parallel threads.\n+      _serial = true;\n+    } else {\n+      _serial = false;\n+    }\n+\n+    GCTraceCPUTime tcpu(&_gc_tracer);\n+    GCTraceTime(Info, gc) tm(\"Pause Full\", nullptr, gc_cause, true);\n+\n+    heap->pre_full_gc_dump(&_gc_timer);\n+\n+    TraceCollectorStats tcs(counters());\n+    TraceMemoryManagerStats tms(heap->old_gc_manager(), gc_cause, \"end of major GC\");\n+\n+    if (log_is_enabled(Debug, gc, heap, exit)) {\n+      accumulated_time()->start();\n+    }\n+\n+    \/\/ Let the size policy know we're starting\n+    size_policy->major_collection_begin();\n+\n+#if COMPILER2_OR_JVMCI\n+    DerivedPointerTable::clear();\n+#endif\n+\n+    ref_processor()->start_discovery(clear_all_soft_refs);\n+\n+    ClassUnloadingContext ctx(1 \/* num_nmethod_unlink_workers *\/,\n+                              false \/* unregister_nmethods_during_purge *\/,\n+                              false \/* lock_nmethod_free_separately *\/);\n+\n+    marking_phase(&_gc_tracer);\n+\n+    summary_phase();\n+\n+#if COMPILER2_OR_JVMCI\n+    assert(DerivedPointerTable::is_active(), \"Sanity\");\n+    DerivedPointerTable::set_active(false);\n+#endif\n+\n+    FullGCForwarding::begin();\n+\n+    forward_to_new_addr();\n+\n+    adjust_pointers();\n+\n+    compact();\n+\n+    FullGCForwarding::end();\n+\n+    ParCompactionManagerNew::_preserved_marks_set->restore(&ParallelScavengeHeap::heap()->workers());\n+\n+    \/\/ Reset the mark bitmap, summary data, and do other bookkeeping.  Must be\n+    \/\/ done before resizing.\n+    post_compact();\n+\n+    \/\/ Let the size policy know we're done\n+    size_policy->major_collection_end(old_gen->used_in_bytes(), gc_cause);\n+\n+    if (UseAdaptiveSizePolicy) {\n+      log_debug(gc, ergo)(\"AdaptiveSizeStart: collection: %d \", heap->total_collections());\n+      log_trace(gc, ergo)(\"old_gen_capacity: %zu young_gen_capacity: %zu\",\n+                          old_gen->capacity_in_bytes(), young_gen->capacity_in_bytes());\n+\n+      \/\/ Don't check if the size_policy is ready here.  Let\n+      \/\/ the size_policy check that internally.\n+      if (UseAdaptiveGenerationSizePolicyAtMajorCollection &&\n+          AdaptiveSizePolicy::should_update_promo_stats(gc_cause)) {\n+        \/\/ Swap the survivor spaces if from_space is empty. The\n+        \/\/ resize_young_gen() called below is normally used after\n+        \/\/ a successful young GC and swapping of survivor spaces;\n+        \/\/ otherwise, it will fail to resize the young gen with\n+        \/\/ the current implementation.\n+        if (young_gen->from_space()->is_empty()) {\n+          young_gen->from_space()->clear(SpaceDecorator::Mangle);\n+          young_gen->swap_spaces();\n+        }\n+\n+        \/\/ Calculate optimal free space amounts\n+        assert(young_gen->max_gen_size() >\n+          young_gen->from_space()->capacity_in_bytes() +\n+          young_gen->to_space()->capacity_in_bytes(),\n+          \"Sizes of space in young gen are out-of-bounds\");\n+\n+        size_t young_live = young_gen->used_in_bytes();\n+        size_t eden_live = young_gen->eden_space()->used_in_bytes();\n+        size_t old_live = old_gen->used_in_bytes();\n+        size_t cur_eden = young_gen->eden_space()->capacity_in_bytes();\n+        size_t max_old_gen_size = old_gen->max_gen_size();\n+        size_t max_eden_size = young_gen->max_gen_size() -\n+          young_gen->from_space()->capacity_in_bytes() -\n+          young_gen->to_space()->capacity_in_bytes();\n+\n+        \/\/ Used for diagnostics\n+        size_policy->clear_generation_free_space_flags();\n+\n+        size_policy->compute_generations_free_space(young_live,\n+                                                    eden_live,\n+                                                    old_live,\n+                                                    cur_eden,\n+                                                    max_old_gen_size,\n+                                                    max_eden_size,\n+                                                    true \/* full gc*\/);\n+\n+        size_policy->check_gc_overhead_limit(eden_live,\n+                                             max_old_gen_size,\n+                                             max_eden_size,\n+                                             true \/* full gc*\/,\n+                                             gc_cause,\n+                                             heap->soft_ref_policy());\n+\n+        size_policy->decay_supplemental_growth(true \/* full gc*\/);\n+\n+        heap->resize_old_gen(\n+          size_policy->calculated_old_free_size_in_bytes());\n+\n+        heap->resize_young_gen(size_policy->calculated_eden_size_in_bytes(),\n+                               size_policy->calculated_survivor_size_in_bytes());\n+      }\n+\n+      log_debug(gc, ergo)(\"AdaptiveSizeStop: collection: %d \", heap->total_collections());\n+    }\n+\n+    if (UsePerfData) {\n+      PSGCAdaptivePolicyCounters* const counters = ParallelScavengeHeap::gc_policy_counters();\n+      counters->update_counters();\n+      counters->update_old_capacity(old_gen->capacity_in_bytes());\n+      counters->update_young_capacity(young_gen->capacity_in_bytes());\n+    }\n+\n+    heap->resize_all_tlabs();\n+\n+    \/\/ Resize the metaspace capacity after a collection\n+    MetaspaceGC::compute_new_size();\n+\n+    if (log_is_enabled(Debug, gc, heap, exit)) {\n+      accumulated_time()->stop();\n+    }\n+\n+    heap->print_heap_change(pre_gc_values);\n+\n+    \/\/ Track memory usage and detect low memory\n+    MemoryService::track_memory_usage();\n+    heap->update_counters();\n+\n+    heap->post_full_gc_dump(&_gc_timer);\n+  }\n+\n+  if (VerifyAfterGC && heap->total_collections() >= VerifyGCStartAt) {\n+    Universe::verify(\"After GC\");\n+  }\n+\n+  heap->print_heap_after_gc();\n+  heap->trace_heap_after_gc(&_gc_tracer);\n+\n+  AdaptiveSizePolicyOutput::print(size_policy, heap->total_collections());\n+\n+  _gc_timer.register_gc_end();\n+\n+  _gc_tracer.report_gc_end(_gc_timer.gc_end(), _gc_timer.time_partitions());\n+\n+  return true;\n+}\n+\n+class PCAddThreadRootsMarkingTaskClosureNew : public ThreadClosure {\n+private:\n+  uint _worker_id;\n+\n+public:\n+  explicit PCAddThreadRootsMarkingTaskClosureNew(uint worker_id) : _worker_id(worker_id) { }\n+  void do_thread(Thread* thread) final {\n+    assert(ParallelScavengeHeap::heap()->is_stw_gc_active(), \"called outside gc\");\n+\n+    ResourceMark rm;\n+\n+    ParCompactionManagerNew* cm = ParCompactionManagerNew::gc_thread_compaction_manager(_worker_id);\n+\n+    MarkingNMethodClosure mark_and_push_in_blobs(&cm->_mark_and_push_closure,\n+                                                 !NMethodToOopClosure::FixRelocations,\n+                                                 true \/* keepalive nmethods *\/);\n+\n+    thread->oops_do(&cm->_mark_and_push_closure, &mark_and_push_in_blobs);\n+\n+    \/\/ Do the real work\n+    cm->follow_marking_stacks();\n+  }\n+};\n+\n+void steal_marking_work_new(TaskTerminator& terminator, uint worker_id) {\n+  assert(ParallelScavengeHeap::heap()->is_stw_gc_active(), \"called outside gc\");\n+\n+  ParCompactionManagerNew* cm =\n+    ParCompactionManagerNew::gc_thread_compaction_manager(worker_id);\n+\n+  do {\n+    ScannerTask task;\n+    if (ParCompactionManagerNew::steal(worker_id, task)) {\n+      cm->follow_contents(task, true);\n+    }\n+    cm->follow_marking_stacks();\n+  } while (!terminator.offer_termination());\n+}\n+\n+class MarkFromRootsTaskNew : public WorkerTask {\n+  StrongRootsScope _strong_roots_scope; \/\/ needed for Threads::possibly_parallel_threads_do\n+  OopStorageSetStrongParState<false \/* concurrent *\/, false \/* is_const *\/> _oop_storage_set_par_state;\n+  TaskTerminator _terminator;\n+  uint _active_workers;\n+\n+public:\n+  explicit MarkFromRootsTaskNew(uint active_workers) :\n+      WorkerTask(\"MarkFromRootsTaskNew\"),\n+      _strong_roots_scope(active_workers),\n+      _terminator(active_workers, ParCompactionManagerNew::marking_stacks()),\n+      _active_workers(active_workers) {}\n+\n+  void work(uint worker_id) final {\n+    ParCompactionManagerNew* cm = ParCompactionManagerNew::gc_thread_compaction_manager(worker_id);\n+    {\n+      CLDToOopClosure cld_closure(&cm->_mark_and_push_closure, ClassLoaderData::_claim_stw_fullgc_mark);\n+      ClassLoaderDataGraph::always_strong_cld_do(&cld_closure);\n+\n+      \/\/ Do the real work\n+      cm->follow_marking_stacks();\n+    }\n+\n+    {\n+      PCAddThreadRootsMarkingTaskClosureNew closure(worker_id);\n+      Threads::possibly_parallel_threads_do(_active_workers > 1 \/* is_par *\/, &closure);\n+    }\n+\n+    \/\/ Mark from OopStorages\n+    {\n+      _oop_storage_set_par_state.oops_do(&cm->_mark_and_push_closure);\n+      \/\/ Do the real work\n+      cm->follow_marking_stacks();\n+    }\n+\n+    if (_active_workers > 1) {\n+      steal_marking_work_new(_terminator, worker_id);\n+    }\n+  }\n+};\n+\n+class ParallelCompactRefProcProxyTaskNew : public RefProcProxyTask {\n+  TaskTerminator _terminator;\n+\n+public:\n+  explicit ParallelCompactRefProcProxyTaskNew(uint max_workers)\n+    : RefProcProxyTask(\"ParallelCompactRefProcProxyTaskNew\", max_workers),\n+      _terminator(_max_workers, ParCompactionManagerNew::marking_stacks()) {}\n+\n+  void work(uint worker_id) final {\n+    assert(worker_id < _max_workers, \"sanity\");\n+    ParCompactionManagerNew* cm = (_tm == RefProcThreadModel::Single) ? ParCompactionManagerNew::get_vmthread_cm() : ParCompactionManagerNew::gc_thread_compaction_manager(worker_id);\n+    BarrierEnqueueDiscoveredFieldClosure enqueue;\n+    ParCompactionManagerNew::FollowStackClosure complete_gc(cm, (_tm == RefProcThreadModel::Single) ? nullptr : &_terminator, worker_id);\n+    _rp_task->rp_work(worker_id, PSParallelCompactNew::is_alive_closure(), &cm->_mark_and_push_closure, &enqueue, &complete_gc);\n+  }\n+\n+  void prepare_run_task_hook() final {\n+    _terminator.reset_for_reuse(_queue_count);\n+  }\n+};\n+\n+void PSParallelCompactNew::marking_phase(ParallelOldTracer *gc_tracer) {\n+  \/\/ Recursively traverse all live objects and mark them\n+  GCTraceTime(Info, gc, phases) tm(\"Marking Phase\", &_gc_timer);\n+\n+  uint active_gc_threads = ParallelScavengeHeap::heap()->workers().active_workers();\n+\n+  ClassLoaderDataGraph::verify_claimed_marks_cleared(ClassLoaderData::_claim_stw_fullgc_mark);\n+  {\n+    GCTraceTime(Debug, gc, phases) pm_tm(\"Par Mark\", &_gc_timer);\n+\n+    MarkFromRootsTaskNew task(active_gc_threads);\n+    ParallelScavengeHeap::heap()->workers().run_task(&task);\n+  }\n+\n+  \/\/ Process reference objects found during marking\n+  {\n+    GCTraceTime(Debug, gc, phases) rp_tm(\"Reference Processing\", &_gc_timer);\n+\n+    ReferenceProcessorStats stats;\n+    ReferenceProcessorPhaseTimes pt(&_gc_timer, ref_processor()->max_num_queues());\n+\n+    ref_processor()->set_active_mt_degree(active_gc_threads);\n+    ParallelCompactRefProcProxyTaskNew task(ref_processor()->max_num_queues());\n+    stats = ref_processor()->process_discovered_references(task, pt);\n+\n+    gc_tracer->report_gc_reference_stats(stats);\n+    pt.print_all_references();\n+  }\n+\n+  \/\/ This is the point where the entire marking should have completed.\n+  ParCompactionManagerNew::verify_all_marking_stack_empty();\n+\n+  {\n+    GCTraceTime(Debug, gc, phases) wp_tm(\"Weak Processing\", &_gc_timer);\n+    WeakProcessor::weak_oops_do(&ParallelScavengeHeap::heap()->workers(),\n+                                is_alive_closure(),\n+                                &do_nothing_cl,\n+                                1);\n+  }\n+\n+  {\n+    GCTraceTime(Debug, gc, phases) tm_m(\"Class Unloading\", &_gc_timer);\n+\n+    ClassUnloadingContext* ctx = ClassUnloadingContext::context();\n+\n+    bool unloading_occurred;\n+    {\n+      CodeCache::UnlinkingScope scope(is_alive_closure());\n+\n+      \/\/ Follow system dictionary roots and unload classes.\n+      unloading_occurred = SystemDictionary::do_unloading(&_gc_timer);\n+\n+      \/\/ Unload nmethods.\n+      CodeCache::do_unloading(unloading_occurred);\n+    }\n+\n+    {\n+      GCTraceTime(Debug, gc, phases) t(\"Purge Unlinked NMethods\", gc_timer());\n+      \/\/ Release unloaded nmethod's memory.\n+      ctx->purge_nmethods();\n+    }\n+    {\n+      GCTraceTime(Debug, gc, phases) ur(\"Unregister NMethods\", &_gc_timer);\n+      ParallelScavengeHeap::heap()->prune_unlinked_nmethods();\n+    }\n+    {\n+      GCTraceTime(Debug, gc, phases) t(\"Free Code Blobs\", gc_timer());\n+      ctx->free_nmethods();\n+    }\n+\n+    \/\/ Prune dead klasses from subklass\/sibling\/implementor lists.\n+    Klass::clean_weak_klass_links(unloading_occurred);\n+\n+    \/\/ Clean JVMCI metadata handles.\n+    JVMCI_ONLY(JVMCI::do_unloading(unloading_occurred));\n+  }\n+\n+  {\n+    GCTraceTime(Debug, gc, phases) roc_tm(\"Report Object Count\", &_gc_timer);\n+    _gc_tracer.report_object_count_after_gc(is_alive_closure(), &ParallelScavengeHeap::heap()->workers());\n+  }\n+#if TASKQUEUE_STATS\n+  ParCompactionManagerNew::print_and_reset_taskqueue_stats();\n+#endif\n+}\n+\n+void PSParallelCompactNew::adjust_pointers_in_spaces(uint worker_id) {\n+  auto start_time = Ticks::now();\n+  for (size_t i = 0; i < _num_regions; i++) {\n+    PCRegionData* region = &_region_data_array[i];\n+    if (!region->claim()) {\n+      continue;\n+    }\n+    log_trace(gc, compaction)(\"Adjusting pointers in region: %zu (worker_id: %u)\", region->idx(), worker_id);\n+    HeapWord* end = region->top();\n+    HeapWord* current = _mark_bitmap.find_obj_beg(region->bottom(), end);\n+    while (current < end) {\n+      assert(_mark_bitmap.is_marked(current), \"must be marked\");\n+      oop obj = cast_to_oop(current);\n+      size_t size = obj->size();\n+      obj->oop_iterate(&pc_adjust_pointer_closure);\n+      current = _mark_bitmap.find_obj_beg(current + size, end);\n+    }\n+  }\n+  log_trace(gc, phases)(\"adjust_pointers_in_spaces worker %u: %.3f ms\", worker_id, (Ticks::now() - start_time).seconds() * 1000);\n+}\n+\n+class PSAdjustTaskNew final : public WorkerTask {\n+  SubTasksDone                               _sub_tasks;\n+  WeakProcessor::Task                        _weak_proc_task;\n+  OopStorageSetStrongParState<false, false>  _oop_storage_iter;\n+  uint                                       _nworkers;\n+\n+  enum PSAdjustSubTask {\n+    PSAdjustSubTask_code_cache,\n+\n+    PSAdjustSubTask_num_elements\n+  };\n+\n+public:\n+  explicit PSAdjustTaskNew(uint nworkers) :\n+    WorkerTask(\"PSAdjust task\"),\n+    _sub_tasks(PSAdjustSubTask_num_elements),\n+    _weak_proc_task(nworkers),\n+    _nworkers(nworkers) {\n+\n+    ClassLoaderDataGraph::verify_claimed_marks_cleared(ClassLoaderData::_claim_stw_fullgc_adjust);\n+    if (nworkers > 1) {\n+      Threads::change_thread_claim_token();\n+    }\n+  }\n+\n+  ~PSAdjustTaskNew() {\n+    Threads::assert_all_threads_claimed();\n+  }\n+\n+  void work(uint worker_id) final {\n+    ParCompactionManagerNew* cm = ParCompactionManagerNew::gc_thread_compaction_manager(worker_id);\n+    cm->preserved_marks()->adjust_during_full_gc();\n+    {\n+      \/\/ adjust pointers in all spaces\n+      PSParallelCompactNew::adjust_pointers_in_spaces(worker_id);\n+    }\n+    {\n+      ResourceMark rm;\n+      Threads::possibly_parallel_oops_do(_nworkers > 1, &pc_adjust_pointer_closure, nullptr);\n+    }\n+    _oop_storage_iter.oops_do(&pc_adjust_pointer_closure);\n+    {\n+      CLDToOopClosure cld_closure(&pc_adjust_pointer_closure, ClassLoaderData::_claim_stw_fullgc_adjust);\n+      ClassLoaderDataGraph::cld_do(&cld_closure);\n+    }\n+    {\n+      AlwaysTrueClosure always_alive;\n+      _weak_proc_task.work(worker_id, &always_alive, &pc_adjust_pointer_closure);\n+    }\n+    if (_sub_tasks.try_claim_task(PSAdjustSubTask_code_cache)) {\n+      NMethodToOopClosure adjust_code(&pc_adjust_pointer_closure, NMethodToOopClosure::FixRelocations);\n+      CodeCache::nmethods_do(&adjust_code);\n+    }\n+    _sub_tasks.all_tasks_claimed();\n+  }\n+};\n+\n+void PSParallelCompactNew::adjust_pointers() {\n+  \/\/ Adjust the pointers to reflect the new locations\n+  GCTraceTime(Info, gc, phases) tm(\"Adjust Pointers\", &_gc_timer);\n+  uint num_workers = ParallelScavengeHeap::heap()->workers().active_workers();\n+  PSAdjustTaskNew task(num_workers);\n+  ParallelScavengeHeap::heap()->workers().run_task(&task);\n+}\n+\n+void PSParallelCompactNew::forward_to_new_addr() {\n+  GCTraceTime(Info, gc, phases) tm(\"Forward\", &_gc_timer);\n+  uint num_workers = get_num_workers();\n+  _per_worker_region_data = NEW_C_HEAP_ARRAY(PCRegionData*, num_workers, mtGC);\n+  for (uint i = 0; i < num_workers; i++) {\n+    _per_worker_region_data[i] = nullptr;\n+  }\n+\n+  class ForwardState {\n+    uint _worker_id;\n+    PCRegionData* _compaction_region;\n+    HeapWord* _compaction_point;\n+\n+    void ensure_compaction_point() {\n+      if (_compaction_point == nullptr) {\n+        assert(_compaction_region == nullptr, \"invariant\");\n+        _compaction_region = _per_worker_region_data[_worker_id];\n+        assert(_compaction_region != nullptr, \"invariant\");\n+        _compaction_point = _compaction_region->bottom();\n+      }\n+    }\n+  public:\n+    explicit ForwardState(uint worker_id) :\n+            _worker_id(worker_id),\n+            _compaction_region(nullptr),\n+            _compaction_point(nullptr) {\n+    }\n+\n+    size_t available() const {\n+      return pointer_delta(_compaction_region->end(), _compaction_point);\n+    }\n+\n+    void forward_objs_in_region(ParCompactionManagerNew* cm, PCRegionData* region) {\n+      ensure_compaction_point();\n+      HeapWord* end = region->end();\n+      HeapWord* current = _mark_bitmap.find_obj_beg(region->bottom(), end);\n+      while (current < end) {\n+        assert(_mark_bitmap.is_marked(current), \"must be marked\");\n+        oop obj = cast_to_oop(current);\n+        assert(region->contains(obj), \"object must not cross region boundary: obj: \" PTR_FORMAT \", obj_end: \" PTR_FORMAT \", region start: \" PTR_FORMAT \", region end: \" PTR_FORMAT, p2i(obj), p2i(cast_from_oop<HeapWord*>(obj) + obj->size()), p2i(region->bottom()), p2i(region->end()));\n+        size_t old_size = obj->size();\n+        size_t new_size = obj->copy_size(old_size, obj->mark());\n+        size_t size = (current == _compaction_point) ? old_size : new_size;\n+        while (size > available()) {\n+          _compaction_region->set_new_top(_compaction_point);\n+          _compaction_region = _compaction_region->local_next();\n+          assert(_compaction_region != nullptr, \"must find a compaction region\");\n+          _compaction_point = _compaction_region->bottom();\n+          size = (current == _compaction_point) ? old_size : new_size;\n+        }\n+        \/\/log_develop_trace(gc, compaction)(\"Forwarding obj: \" PTR_FORMAT \", to: \" PTR_FORMAT, p2i(obj), p2i(_compaction_point));\n+        if (current != _compaction_point) {\n+          cm->preserved_marks()->push_if_necessary(obj, obj->mark());\n+          FullGCForwarding::forward_to(obj, cast_to_oop(_compaction_point));\n+        }\n+        _compaction_point += size;\n+        assert(_compaction_point <= _compaction_region->end(), \"object must fit in region\");\n+        current += old_size;\n+        assert(current <= end, \"object must not cross region boundary\");\n+        current = _mark_bitmap.find_obj_beg(current, end);\n+      }\n+    }\n+    void finish() {\n+      if (_compaction_region != nullptr) {\n+        _compaction_region->set_new_top(_compaction_point);\n+      }\n+    }\n+  };\n+\n+  struct ForwardTask final : public WorkerTask {\n+    ForwardTask() : WorkerTask(\"PSForward task\") {}\n+\n+    void work(uint worker_id) override {\n+      ParCompactionManagerNew* cm = ParCompactionManagerNew::gc_thread_compaction_manager(worker_id);\n+      ForwardState state(worker_id);\n+      PCRegionData** last_link = &_per_worker_region_data[worker_id];\n+      size_t idx = worker_id;\n+      uint num_workers = get_num_workers();\n+      size_t num_regions = get_num_regions();\n+      PCRegionData* region_data_array = get_region_data_array();\n+      while (idx < num_regions) {\n+        PCRegionData* region = region_data_array + idx;\n+        *last_link = region;\n+        last_link = region->local_next_addr();\n+        state.forward_objs_in_region(cm, region);\n+        idx += num_workers;\n+      }\n+      state.finish();\n+    }\n+  } task;\n+\n+  uint par_workers = ParallelScavengeHeap::heap()->workers().active_workers();\n+  ParallelScavengeHeap::heap()->workers().set_active_workers(num_workers);\n+  ParallelScavengeHeap::heap()->workers().run_task(&task);\n+  ParallelScavengeHeap::heap()->workers().set_active_workers(par_workers);\n+\n+#ifndef PRODUCT\n+  for (uint wid = 0; wid < num_workers; wid++) {\n+    for (PCRegionData* rd = _per_worker_region_data[wid]; rd != nullptr; rd = rd->local_next()) {\n+      log_develop_trace(gc, compaction)(\"Per worker compaction region, worker: %d, #%zu: [\" PTR_FORMAT \", \" PTR_FORMAT \"), new_top: \" PTR_FORMAT, wid, rd->idx(),\n+                                        p2i(rd->bottom()), p2i(rd->end()), p2i(rd->new_top()));\n+    }\n+  }\n+#endif\n+}\n+\n+void PSParallelCompactNew::compact() {\n+  GCTraceTime(Info, gc, phases) tm(\"Compaction Phase\", &_gc_timer);\n+  class CompactTask final : public WorkerTask {\n+    static void compact_region(PCRegionData* region) {\n+      HeapWord* bottom = region->bottom();\n+      HeapWord* end = region->top();\n+      if (bottom == end) {\n+        return;\n+      }\n+      HeapWord* current = _mark_bitmap.find_obj_beg(bottom, end);\n+      while (current < end) {\n+        oop obj = cast_to_oop(current);\n+        size_t size = obj->size();\n+        if (FullGCForwarding::is_forwarded(obj)) {\n+          oop fwd = FullGCForwarding::forwardee(obj);\n+          auto* dst = cast_from_oop<HeapWord*>(fwd);\n+          ObjectStartArray* sa = start_array(space_id(dst));\n+          if (sa != nullptr) {\n+            assert(dst != current, \"expect moving object\");\n+            size_t new_words = obj->copy_size(size, obj->mark());\n+            sa->update_for_block(dst, dst + new_words);\n+          }\n+\n+          Copy::aligned_conjoint_words(current, dst, size);\n+          fwd->init_mark();\n+          fwd->initialize_hash_if_necessary(obj);\n+        } else {\n+          \/\/ The start_array must be updated even if the object is not moving.\n+          ObjectStartArray* sa = start_array(space_id(current));\n+          if (sa != nullptr) {\n+            sa->update_for_block(current, current + size);\n+          }\n+        }\n+        current = _mark_bitmap.find_obj_beg(current + size, end);\n+      }\n+    }\n+  public:\n+    explicit CompactTask() : WorkerTask(\"PSCompact task\") {}\n+    void work(uint worker_id) override {\n+      PCRegionData* region = _per_worker_region_data[worker_id];\n+      while (region != nullptr) {\n+        log_trace(gc)(\"Compact worker: %u, compacting region: %zu\", worker_id, region->idx());\n+        compact_region(region);\n+        region = region->local_next();\n+      }\n+    }\n+  } task;\n+\n+  uint num_workers = get_num_workers();\n+  uint par_workers = ParallelScavengeHeap::heap()->workers().active_workers();\n+  ParallelScavengeHeap::heap()->workers().set_active_workers(num_workers);\n+  ParallelScavengeHeap::heap()->workers().run_task(&task);\n+  ParallelScavengeHeap::heap()->workers().set_active_workers(par_workers);\n+}\n+\n+\/\/ Return the SpaceId for the space containing addr.  If addr is not in the\n+\/\/ heap, last_space_id is returned.  In debug mode it expects the address to be\n+\/\/ in the heap and asserts such.\n+PSParallelCompactNew::SpaceId PSParallelCompactNew::space_id(HeapWord* addr) {\n+  assert(ParallelScavengeHeap::heap()->is_in_reserved(addr), \"addr not in the heap\");\n+\n+  for (unsigned int id = old_space_id; id < last_space_id; ++id) {\n+    if (_space_info[id].space()->contains(addr)) {\n+      return SpaceId(id);\n+    }\n+  }\n+\n+  assert(false, \"no space contains the addr\");\n+  return last_space_id;\n+}\n","filename":"src\/hotspot\/share\/gc\/parallel\/psParallelCompactNew.cpp","additions":1151,"deletions":0,"binary":false,"changes":1151,"status":"added"},{"patch":"@@ -0,0 +1,338 @@\n+\/*\n+ * Copyright (c) 2005, 2024, Oracle and\/or its affiliates. All rights reserved.\n+ * Copyright Amazon.com Inc. or its affiliates. All Rights Reserved.\n+ * * DO NOT ALTER OR REMOVE COPYRIGHT NOTICES OR THIS FILE HEADER.\n+ *\n+ * This code is free software; you can redistribute it and\/or modify it\n+ * under the terms of the GNU General Public License version 2 only, as\n+ * published by the Free Software Foundation.\n+ *\n+ * This code is distributed in the hope that it will be useful, but WITHOUT\n+ * ANY WARRANTY; without even the implied warranty of MERCHANTABILITY or\n+ * FITNESS FOR A PARTICULAR PURPOSE.  See the GNU General Public License\n+ * version 2 for more details (a copy is included in the LICENSE file that\n+ * accompanied this code).\n+ *\n+ * You should have received a copy of the GNU General Public License version\n+ * 2 along with this work; if not, write to the Free Software Foundation,\n+ * Inc., 51 Franklin St, Fifth Floor, Boston, MA 02110-1301 USA.\n+ *\n+ * Please contact Oracle, 500 Oracle Parkway, Redwood Shores, CA 94065 USA\n+ * or visit www.oracle.com if you need additional information or have any\n+ * questions.\n+ *\n+ *\/\n+\n+#ifndef SHARE_GC_PARALLEL_PSPARALLELCOMPACTNEW_HPP\n+#define SHARE_GC_PARALLEL_PSPARALLELCOMPACTNEW_HPP\n+\n+#include \"gc\/parallel\/mutableSpace.hpp\"\n+#include \"gc\/parallel\/objectStartArray.hpp\"\n+#include \"gc\/parallel\/parallelScavengeHeap.hpp\"\n+#include \"gc\/parallel\/parMarkBitMap.hpp\"\n+#include \"gc\/shared\/collectedHeap.hpp\"\n+#include \"gc\/shared\/collectorCounters.hpp\"\n+#include \"gc\/shared\/referenceProcessor.hpp\"\n+#include \"gc\/shared\/taskTerminator.hpp\"\n+#include \"oops\/oop.hpp\"\n+#include \"runtime\/atomic.hpp\"\n+#include \"runtime\/orderAccess.hpp\"\n+\n+class ParallelScavengeHeap;\n+class PSAdaptiveSizePolicy;\n+class PSYoungGen;\n+class PSOldGen;\n+class ParCompactionManagerNew;\n+class PSParallelCompactNew;\n+class ParallelOldTracer;\n+class STWGCTimer;\n+\n+class SpaceInfoNew\n+{\n+public:\n+  MutableSpace* space() const { return _space; }\n+\n+  \/\/ The start array for the (generation containing the) space, or null if there\n+  \/\/ is no start array.\n+  ObjectStartArray* start_array() const { return _start_array; }\n+\n+  void set_space(MutableSpace* s)           { _space = s; }\n+  void set_start_array(ObjectStartArray* s) { _start_array = s; }\n+\n+private:\n+  MutableSpace*     _space;\n+  ObjectStartArray* _start_array;\n+};\n+\n+\/\/ The Parallel compaction collector is a stop-the-world garbage collector that\n+\/\/ does parts of the collection using parallel threads.  The collection includes\n+\/\/ the tenured generation and the young generation.\n+\/\/\n+\/\/ A collection consists of the following phases.\n+\/\/\n+\/\/      - marking phase\n+\/\/      - summary phase (single-threaded)\n+\/\/      - forward (to new address) phase\n+\/\/      - adjust pointers phase\n+\/\/      - compacting phase\n+\/\/      - clean up phase\n+\/\/\n+\/\/ Roughly speaking these phases correspond, respectively, to\n+\/\/\n+\/\/      - mark all the live objects\n+\/\/      - set-up temporary regions to enable parallelism in following phases\n+\/\/      - calculate the destination of each object at the end of the collection\n+\/\/      - adjust pointers to reflect new destination of objects\n+\/\/      - move the objects to their destination\n+\/\/      - update some references and reinitialize some variables\n+\/\/\n+\/\/ A space that is being collected is divided into regions and with each region\n+\/\/ is associated an object of type PCRegionData. Regions are targeted to be of\n+\/\/ a mostly uniform size, but if an object would cross a region boundary, then\n+\/\/ the boundary is adjusted to be after the end of that object.\n+\/\/\n+\/\/ The marking phase does a complete marking of all live objects in the heap.\n+\/\/ The marking phase uses multiple GC threads and marking is done in a bit\n+\/\/ array of type ParMarkBitMap.  The marking of the bit map is done atomically.\n+\/\/\n+\/\/ The summary phase sets up the regions, such that region covers roughly\n+\/\/ uniform memory regions (currently same size as SpaceAlignment). However,\n+\/\/ if that would result in an object crossing a region boundary, then\n+\/\/ the upper bounds is adjusted such that the region ends after that object.\n+\/\/ This way we can ensure that a GC worker thread can fully 'own' a region\n+\/\/ during the forwarding, adjustment and compaction phases, without worrying\n+\/\/ about other threads messing with parts of the object. The summary phase\n+\/\/ also sets up an alternative set of regions, where each region covers\n+\/\/ a single space. This is used for a serial compaction mode which achieves\n+\/\/ maximum compaction at the expense of parallelism during the forwarding\n+\/\/ compaction phases.\n+\/\/\n+\/\/ The forwarding phase calculates the new address of each live\n+\/\/ object and records old-addr-to-new-addr association. It does this using\n+\/\/ multiple GC threads. Each thread 'claims' a source region and appends it to a\n+\/\/ local work-list. The region is also set as the current compaction region\n+\/\/ for that thread. All live objects in the region are then visited and its\n+\/\/ new location calculated by tracking the compaction point in the compaction\n+\/\/ region. Once the source region is exhausted, the next source region is\n+\/\/ claimed from the global pool and appended to the end of the local work-list.\n+\/\/ Once the compaction region is exhausted, the top of the old compaction region\n+\/\/ is recorded, and the next compaction region is fetched from the front of the\n+\/\/ local work-list (which is guaranteed to already have finished processing, or\n+\/\/ is the same as the source region). This way, each worker forms a local\n+\/\/ list of regions in which the worker can compact as if it were a serial\n+\/\/ compaction.\n+\/\/\n+\/\/ The adjust pointers phase remaps all pointers to reflect the new address of each\n+\/\/ object. Again, this uses multiple GC worker threads. Each thread claims\n+\/\/ a region, processes all references in all live objects of that region. Then\n+\/\/ the thread proceeds to claim the next region from the global pool, until\n+\/\/ all regions have been processed.\n+\/\/\n+\/\/ The compaction phase moves objects to their new location. Again, this uses\n+\/\/ multiple GC worker threads. Each worker processes the local work-list that\n+\/\/ has been set-up during the forwarding phase and processes it from bottom\n+\/\/ to top, copying each live object to its new location (which is guaranteed\n+\/\/ to be lower in that threads parts of the heap, and thus would never overwrite\n+\/\/ other objects).\n+\/\/\n+\/\/ This algorithm will usually leave gaps of non-fillable memory at the end\n+\/\/ of regions, and potentially whole empty regions at the end of compaction.\n+\/\/ The post-compaction phase fills those gaps with filler objects to ensure\n+\/\/ that the heap remains parsable.\n+\/\/\n+\/\/ In some situations, this inefficiency of leaving gaps can lead to a\n+\/\/ situation where after a full GC, it is still not possible to satisfy an\n+\/\/ allocation, even though there should be enough memory available. When\n+\/\/ that happens, the collector switches to a serial mode, where we only\n+\/\/ have 4 regions which correspond exaxtly to the 4 spaces, and the forwarding\n+\/\/ and compaction phases are executed using only a single thread. This\n+\/\/ achieves maximum compaction. This serial mode is also invoked when\n+\/\/ System.gc() is called *and* UseMaximumCompactionOnSystemGC is set to\n+\/\/ true (which is the default), or when the number of full GCs exceeds\n+\/\/ the HeapMaximumCompactionInterval.\n+\/\/\n+\/\/ Possible improvements to the algorithm include:\n+\/\/ - Identify and ignore a 'dense prefix'. This requires that we collect\n+\/\/   liveness data during marking, or that we scan the prefix object-by-object\n+\/\/   during the summary phase.\n+\/\/ - When an object does not fit into a remaining gap of a region, and the\n+\/\/   object is rather large, we could attempt to forward\/compact subsequent\n+\/\/   objects 'around' that large object in an attempt to minimize the\n+\/\/   resulting gap. This could be achieved by reconfiguring the regions\n+\/\/   to exclude the large object.\n+\/\/ - Instead of finding out *after* the whole compaction that an allocation\n+\/\/   can still not be satisfied, and then re-running the whole compaction\n+\/\/   serially, we could determine that after the forwarding phase, and\n+\/\/   re-do only forwarding serially, thus avoiding running marking,\n+\/\/   adjusting references and compaction twice.\n+class PCRegionData \/*: public CHeapObj<mtGC> *\/ {\n+  \/\/ A region index\n+  size_t const _idx;\n+\n+  \/\/ The start of the region\n+  HeapWord* const _bottom;\n+  \/\/ The top of the region. (first word after last live object in containing space)\n+  HeapWord* const _top;\n+  \/\/ The end of the region (first word after last word of the region)\n+  HeapWord* const _end;\n+\n+  \/\/ The next compaction address\n+  HeapWord* _new_top;\n+\n+  \/\/ Points to the next region in the GC-worker-local work-list\n+  PCRegionData* _local_next;\n+\n+  \/\/ Parallel workers claiming protocol, used during adjust-references phase.\n+  volatile bool _claimed;\n+\n+public:\n+\n+  PCRegionData(size_t idx, HeapWord* bottom, HeapWord* top, HeapWord* end) :\n+    _idx(idx), _bottom(bottom), _top(top), _end(end), _new_top(bottom),\n+          _local_next(nullptr), _claimed(false) {}\n+\n+  size_t idx() const { return _idx; };\n+\n+  HeapWord* bottom() const { return _bottom; }\n+  HeapWord* top() const { return _top; }\n+  HeapWord* end()   const { return _end;   }\n+\n+  PCRegionData*  local_next() const { return _local_next; }\n+  PCRegionData** local_next_addr() { return &_local_next; }\n+\n+  HeapWord* new_top() const {\n+    return _new_top;\n+  }\n+  void set_new_top(HeapWord* new_top) {\n+    _new_top = new_top;\n+  }\n+\n+  bool contains(oop obj) {\n+    auto* obj_start = cast_from_oop<HeapWord*>(obj);\n+    HeapWord* obj_end = obj_start + obj->size();\n+    return _bottom <= obj_start && obj_start < _end && _bottom < obj_end && obj_end <= _end;\n+  }\n+\n+  bool claim() {\n+    bool claimed =  _claimed;\n+    if (claimed) {\n+      return false;\n+    }\n+    return !Atomic::cmpxchg(&_claimed, false, true);\n+  }\n+};\n+\n+class PSParallelCompactNew : AllStatic {\n+public:\n+  typedef enum {\n+    old_space_id, eden_space_id,\n+    from_space_id, to_space_id, last_space_id\n+  } SpaceId;\n+\n+public:\n+  class IsAliveClosure: public BoolObjectClosure {\n+  public:\n+    bool do_object_b(oop p) final;\n+  };\n+\n+private:\n+  static STWGCTimer           _gc_timer;\n+  static ParallelOldTracer    _gc_tracer;\n+  static elapsedTimer         _accumulated_time;\n+  static unsigned int         _maximum_compaction_gc_num;\n+  static CollectorCounters*   _counters;\n+  static ParMarkBitMap        _mark_bitmap;\n+  static IsAliveClosure       _is_alive_closure;\n+  static SpaceInfoNew         _space_info[last_space_id];\n+\n+  \/\/ The head of the global region data list.\n+  static size_t               _num_regions;\n+  static PCRegionData*        _region_data_array;\n+  static PCRegionData**       _per_worker_region_data;\n+\n+  static size_t               _num_regions_serial;\n+  static PCRegionData*        _region_data_array_serial;\n+  static bool                 _serial;\n+\n+  \/\/ Reference processing (used in ...follow_contents)\n+  static SpanSubjectToDiscoveryClosure  _span_based_discoverer;\n+  static ReferenceProcessor*  _ref_processor;\n+\n+  static uint get_num_workers() { return _serial ? 1 : ParallelScavengeHeap::heap()->workers().active_workers(); }\n+  static size_t get_num_regions() { return _serial ? _num_regions_serial : _num_regions; }\n+  static PCRegionData* get_region_data_array() { return _serial ? _region_data_array_serial : _region_data_array; }\n+\n+public:\n+  static ParallelOldTracer* gc_tracer() { return &_gc_tracer; }\n+\n+private:\n+\n+  static void initialize_space_info();\n+\n+  \/\/ Clear the marking bitmap and summary data that cover the specified space.\n+  static void clear_data_covering_space(SpaceId id);\n+\n+  static void pre_compact();\n+\n+  static void post_compact();\n+\n+  static bool check_maximum_compaction();\n+\n+  \/\/ Mark live objects\n+  static void marking_phase(ParallelOldTracer *gc_tracer);\n+\n+  static void summary_phase();\n+  static void setup_regions_parallel();\n+  static void setup_regions_serial();\n+\n+  static void adjust_pointers();\n+  static void forward_to_new_addr();\n+\n+  \/\/ Move objects to new locations.\n+  static void compact();\n+\n+public:\n+  static bool invoke(bool maximum_heap_compaction, bool serial);\n+  static bool invoke_no_policy(bool maximum_heap_compaction, bool serial);\n+\n+  static void adjust_pointers_in_spaces(uint worker_id);\n+\n+  static void post_initialize();\n+  \/\/ Perform initialization for PSParallelCompactNew that requires\n+  \/\/ allocations.  This should be called during the VM initialization\n+  \/\/ at a pointer where it would be appropriate to return a JNI_ENOMEM\n+  \/\/ in the event of a failure.\n+  static bool initialize_aux_data();\n+\n+  \/\/ Closure accessors\n+  static BoolObjectClosure* is_alive_closure()     { return &_is_alive_closure; }\n+\n+  \/\/ Public accessors\n+  static elapsedTimer* accumulated_time() { return &_accumulated_time; }\n+\n+  static CollectorCounters* counters()    { return _counters; }\n+\n+  static inline bool is_marked(oop obj);\n+\n+  template <class T> static inline void adjust_pointer(T* p);\n+\n+  \/\/ Convenience wrappers for per-space data kept in _space_info.\n+  static inline MutableSpace*     space(SpaceId space_id);\n+  static inline ObjectStartArray* start_array(SpaceId space_id);\n+\n+  static ParMarkBitMap* mark_bitmap() { return &_mark_bitmap; }\n+\n+  \/\/ Reference Processing\n+  static ReferenceProcessor* ref_processor() { return _ref_processor; }\n+\n+  static STWGCTimer* gc_timer() { return &_gc_timer; }\n+\n+  \/\/ Return the SpaceId for the given address.\n+  static SpaceId space_id(HeapWord* addr);\n+\n+  static void print_on_error(outputStream* st);\n+};\n+\n+void steal_marking_work_new(TaskTerminator& terminator, uint worker_id);\n+\n+#endif \/\/ SHARE_GC_PARALLEL_PSPARALLELCOMPACTNEW_HPP\n","filename":"src\/hotspot\/share\/gc\/parallel\/psParallelCompactNew.hpp","additions":338,"deletions":0,"binary":false,"changes":338,"status":"added"},{"patch":"@@ -0,0 +1,74 @@\n+\/*\n+ * Copyright (c) 2016, 2024, Oracle and\/or its affiliates. All rights reserved.\n+ * Copyright Amazon.com Inc. or its affiliates. All Rights Reserved.\n+ * DO NOT ALTER OR REMOVE COPYRIGHT NOTICES OR THIS FILE HEADER.\n+ *\n+ * This code is free software; you can redistribute it and\/or modify it\n+ * under the terms of the GNU General Public License version 2 only, as\n+ * published by the Free Software Foundation.\n+ *\n+ * This code is distributed in the hope that it will be useful, but WITHOUT\n+ * ANY WARRANTY; without even the implied warranty of MERCHANTABILITY or\n+ * FITNESS FOR A PARTICULAR PURPOSE.  See the GNU General Public License\n+ * version 2 for more details (a copy is included in the LICENSE file that\n+ * accompanied this code).\n+ *\n+ * You should have received a copy of the GNU General Public License version\n+ * 2 along with this work; if not, write to the Free Software Foundation,\n+ * Inc., 51 Franklin St, Fifth Floor, Boston, MA 02110-1301 USA.\n+ *\n+ * Please contact Oracle, 500 Oracle Parkway, Redwood Shores, CA 94065 USA\n+ * or visit www.oracle.com if you need additional information or have any\n+ * questions.\n+ *\n+ *\/\n+\n+#ifndef SHARE_GC_PARALLEL_PSPARALLELCOMPACTNEW_INLINE_HPP\n+#define SHARE_GC_PARALLEL_PSPARALLELCOMPACTNEW_INLINE_HPP\n+\n+#include \"gc\/parallel\/psParallelCompactNew.hpp\"\n+\n+#include \"gc\/parallel\/parallelScavengeHeap.hpp\"\n+#include \"gc\/parallel\/parMarkBitMap.inline.hpp\"\n+#include \"gc\/shared\/collectedHeap.hpp\"\n+#include \"gc\/shared\/continuationGCSupport.inline.hpp\"\n+#include \"gc\/shared\/fullGCForwarding.inline.hpp\"\n+#include \"oops\/access.inline.hpp\"\n+#include \"oops\/compressedOops.inline.hpp\"\n+#include \"oops\/klass.hpp\"\n+#include \"oops\/oop.inline.hpp\"\n+\n+inline bool PSParallelCompactNew::is_marked(oop obj) {\n+  return mark_bitmap()->is_marked(obj);\n+}\n+\n+inline MutableSpace* PSParallelCompactNew::space(SpaceId id) {\n+  assert(id < last_space_id, \"id out of range\");\n+  return _space_info[id].space();\n+}\n+\n+inline ObjectStartArray* PSParallelCompactNew::start_array(SpaceId id) {\n+  assert(id < last_space_id, \"id out of range\");\n+  return _space_info[id].start_array();\n+}\n+\n+template <class T>\n+inline void PSParallelCompactNew::adjust_pointer(T* p) {\n+  T heap_oop = RawAccess<>::oop_load(p);\n+  if (!CompressedOops::is_null(heap_oop)) {\n+    oop obj = CompressedOops::decode_not_null(heap_oop);\n+    assert(ParallelScavengeHeap::heap()->is_in(obj), \"should be in heap\");\n+    assert(is_marked(obj), \"must be marked\");\n+    if (!FullGCForwarding::is_forwarded(obj)) {\n+      return;\n+    }\n+    oop new_obj = FullGCForwarding::forwardee(obj);\n+    assert(new_obj != nullptr, \"non-null address for live objects\");\n+    assert(new_obj != obj, \"inv\");\n+    assert(ParallelScavengeHeap::heap()->is_in_reserved(new_obj),\n+           \"should be in object space\");\n+    RawAccess<IS_NOT_NULL>::oop_store(p, new_obj);\n+  }\n+}\n+\n+#endif \/\/ SHARE_GC_PARALLEL_PSPARALLELCOMPACTNEW_INLINE_HPP\n","filename":"src\/hotspot\/share\/gc\/parallel\/psParallelCompactNew.inline.hpp","additions":74,"deletions":0,"binary":false,"changes":74,"status":"added"},{"patch":"@@ -283,0 +283,2 @@\n+    new_obj->initialize_hash_if_necessary(o);\n+\n","filename":"src\/hotspot\/share\/gc\/parallel\/psPromotionManager.inline.hpp","additions":2,"deletions":0,"binary":false,"changes":2,"status":"modified"},{"patch":"@@ -33,0 +33,1 @@\n+#include \"gc\/parallel\/psCompactionManagerNew.hpp\"\n@@ -207,1 +208,1 @@\n-      _terminator(max_workers, ParCompactionManager::marking_stacks()) {}\n+      _terminator(max_workers, UseCompactObjectHeaders ? ParCompactionManagerNew::marking_stacks() : ParCompactionManager::marking_stacks()) {}\n","filename":"src\/hotspot\/share\/gc\/parallel\/psScavenge.cpp","additions":2,"deletions":1,"binary":false,"changes":3,"status":"modified"},{"patch":"@@ -126,1 +126,1 @@\n-        if (fwd_mark.is_expanded() && klass->expand_for_hash(fwd)) {\n+        if (fwd_mark.is_expanded() && klass->expand_for_hash(fwd, fwd_mark)) {\n","filename":"src\/hotspot\/share\/gc\/shenandoah\/shenandoahForwarding.inline.hpp","additions":1,"deletions":1,"binary":false,"changes":2,"status":"modified"},{"patch":"@@ -1339,1 +1339,1 @@\n-bool Klass::expand_for_hash(oop obj) const {\n+bool Klass::expand_for_hash(oop obj, markWord m) const {\n@@ -1341,2 +1341,2 @@\n-  assert((size_t)hash_offset_in_bytes(obj) <= (obj->base_size_given_klass(obj->mark(), this) * HeapWordSize), \"hash offset must be eq or lt base size: hash offset: %d, base size: %zu\", hash_offset_in_bytes(obj), obj->base_size_given_klass(obj->mark(), this) * HeapWordSize);\n-  return obj->base_size_given_klass(obj->mark(), this) * HeapWordSize - hash_offset_in_bytes(obj) < (int)sizeof(uint32_t);\n+  assert((size_t)hash_offset_in_bytes(obj) <= (obj->base_size_given_klass(m, this) * HeapWordSize), \"hash offset must be eq or lt base size: hash offset: %d, base size: %zu\", hash_offset_in_bytes(obj), obj->base_size_given_klass(m, this) * HeapWordSize);\n+  return obj->base_size_given_klass(m, this) * HeapWordSize - hash_offset_in_bytes(obj) < (int)sizeof(uint32_t);\n","filename":"src\/hotspot\/share\/oops\/klass.cpp","additions":3,"deletions":3,"binary":false,"changes":6,"status":"modified"},{"patch":"@@ -797,1 +797,1 @@\n-  bool expand_for_hash(oop obj) const;\n+  bool expand_for_hash(oop obj, markWord m) const;\n","filename":"src\/hotspot\/share\/oops\/klass.hpp","additions":1,"deletions":1,"binary":false,"changes":2,"status":"modified"},{"patch":"@@ -205,1 +205,1 @@\n-    if (mrk.is_expanded() && kls->expand_for_hash(cast_to_oop(this))) {\n+    if (mrk.is_expanded() && kls->expand_for_hash(cast_to_oop(this), mrk)) {\n@@ -216,1 +216,1 @@\n-    if (mark.is_hashed_not_expanded() && klass->expand_for_hash(cast_to_oop(this))) {\n+    if (mark.is_hashed_not_expanded() && klass->expand_for_hash(cast_to_oop(this), mark)) {\n@@ -228,1 +228,1 @@\n-    if (mark.is_hashed_not_expanded() && klass->expand_for_hash(cast_to_oop(this))) {\n+    if (mark.is_hashed_not_expanded() && klass->expand_for_hash(cast_to_oop(this), mark)) {\n@@ -231,1 +231,1 @@\n-    if (mark.is_not_hashed_expanded() && klass->expand_for_hash(cast_to_oop(this))) {\n+    if (mark.is_not_hashed_expanded() && klass->expand_for_hash(cast_to_oop(this), mark)) {\n","filename":"src\/hotspot\/share\/oops\/oop.inline.hpp","additions":4,"deletions":4,"binary":false,"changes":8,"status":"modified"},{"patch":"@@ -3663,4 +3663,0 @@\n-    FLAG_SET_DEFAULT(UseCompactObjectHeaders, false);\n-  }\n-  if (UseCompactObjectHeaders && UseParallelGC) {\n-    warning(\"Parallel GC is currently not compatible with compact object headers (due to identity hash-code). Disabling compact object headers.\");\n","filename":"src\/hotspot\/share\/runtime\/arguments.cpp","additions":0,"deletions":4,"binary":false,"changes":4,"status":"modified"}]}