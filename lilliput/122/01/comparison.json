{"files":[{"patch":"@@ -0,0 +1,631 @@\n+\n+#include \"precompiled.hpp\"\n+\n+#include \"classfile\/classLoaderData.hpp\"\n+#include \"classfile\/classLoaderDataGraph.hpp\"\n+#include \"classfile\/systemDictionary.hpp\"\n+#include \"code\/codeCache.hpp\"\n+#include \"gc\/serial\/cardTableRS.hpp\"\n+#include \"gc\/serial\/generation.hpp\"\n+#include \"gc\/serial\/serialCompressor.hpp\"\n+#include \"gc\/serial\/serialGcRefProcProxyTask.hpp\"\n+#include \"gc\/serial\/serialHeap.hpp\"\n+#include \"gc\/serial\/serialStringDedup.hpp\"\n+#include \"gc\/shared\/classUnloadingContext.hpp\"\n+#include \"gc\/shared\/continuationGCSupport.hpp\"\n+#include \"gc\/shared\/gcTrace.hpp\"\n+#include \"gc\/shared\/gcTraceTime.inline.hpp\"\n+#include \"gc\/shared\/markBitMap.inline.hpp\"\n+#include \"gc\/shared\/referenceProcessor.hpp\"\n+#include \"gc\/shared\/referenceProcessorPhaseTimes.hpp\"\n+#include \"gc\/shared\/space.hpp\"\n+#include \"gc\/shared\/strongRootsScope.hpp\"\n+#include \"gc\/shared\/taskqueue.hpp\"\n+#include \"gc\/shared\/weakProcessor.hpp\"\n+#include \"memory\/iterator.hpp\"\n+#include \"memory\/universe.hpp\"\n+#include \"oops\/access.inline.hpp\"\n+#include \"oops\/compressedOops.inline.hpp\"\n+#include \"utilities\/copy.hpp\"\n+#include \"utilities\/stack.inline.hpp\"\n+\n+#if INCLUDE_JVMCI\n+#include \"jvmci\/jvmci.hpp\"\n+#endif\n+\n+static HeapWord** allocate_table() {\n+  MemRegion covered = SerialHeap::heap()->reserved_region();\n+  HeapWord* start = covered.start();\n+  HeapWord* end = covered.end();\n+  int words_per_block = SCBlockOffsetTable::words_per_block();\n+  size_t num_blocks = align_up(pointer_delta(end, start), words_per_block) \/ words_per_block;\n+  return NEW_C_HEAP_ARRAY(HeapWord*, num_blocks, mtGC);\n+}\n+\n+SCBlockOffsetTable::SCBlockOffsetTable(MarkBitMap& mark_bitmap) :\n+  _table(allocate_table()),\n+  _mark_bitmap(mark_bitmap),\n+  _covered(SerialHeap::heap()->reserved_region()) { }\n+\n+SCBlockOffsetTable::~SCBlockOffsetTable() {\n+  FREE_C_HEAP_ARRAY(HeapWord*, _table);\n+}\n+\n+inline size_t SCBlockOffsetTable::addr_to_block_idx(HeapWord* addr) const {\n+  assert(addr >= _covered.start() && addr <= _covered.end(), \"address must be in heap\");\n+  return pointer_delta(addr, _covered.start()) \/ words_per_block();\n+}\n+\n+void SCBlockOffsetTable::build_table_for_space(ContiguousSpace* space, CompactPoint& cp) {\n+  HeapWord* bottom = space->bottom();\n+  HeapWord* top = space->top();\n+\n+  \/\/ We're sure to be here before any objects are compacted into this\n+  \/\/ space, so this is a good time to initialize this:\n+  space->set_compaction_top(bottom);\n+\n+  \/\/ Init compaction-space, if necessary.\n+  if (cp.space == nullptr) {\n+    assert(cp.gen != nullptr, \"need a generation\");\n+    assert(cp.gen->first_compaction_space() == space, \"just checking\");\n+    cp.space = cp.gen->first_compaction_space();\n+    cp.space->set_compaction_top(cp.space->bottom());\n+  }\n+\n+  \/\/ Clear table.\n+  size_t bottom_block = addr_to_block_idx(bottom);\n+  size_t top_block = addr_to_block_idx(MIN2(space->end(), align_up(space->top(), words_per_block() * BytesPerWord)));\n+  Copy::fill_to_words(reinterpret_cast<HeapWord*>(&_table[bottom_block]), top_block - bottom_block);\n+\n+  HeapWord* compact_top = cp.space->compaction_top();\n+  HeapWord* current = _mark_bitmap.get_next_marked_addr(bottom, top);\n+  \/\/ Scan all live objects in the space.\n+  while (current < top) {\n+    size_t idx = addr_to_block_idx(current);\n+    assert(_table[idx] == nullptr, \"must be new block\");\n+    HeapWord* block_end = MIN2(top, align_up(current + 1, words_per_block() * BytesPerWord));\n+    size_t live_in_block = 0;\n+    HeapWord* first_in_block = current;\n+    HeapWord* compact = compact_top;\n+    \/\/ Scan all live objects in the block to calculate the number of live words of all\n+    \/\/ objects in the block. Note that this can be larger than the block when a\n+    \/\/ trailing object spans into subsequent block(s). This is intentional:\n+    \/\/ All objects which start in a block will share the same block-base-address,\n+    \/\/ and thus must be compacted into the same destinatioin space.\n+    while (current < block_end) {\n+      oop obj = cast_to_oop(current);\n+      assert(oopDesc::is_oop(obj), \"must be oop start\");\n+      size_t obj_size = obj->size();\n+      live_in_block += obj_size;\n+\n+      \/\/ Advance to next live object.\n+      current = _mark_bitmap.get_next_marked_addr(current + obj_size, top);\n+    }\n+\n+    \/\/ Check if block fits into current compaction space, and switch to next,\n+    \/\/ if necessary. The compaction space must have enough space left to\n+    \/\/ accomodate all objects that start in the block.\n+    while (live_in_block > pointer_delta(cp.space->end(), compact_top)) {\n+      cp.space->set_compaction_top(compact_top);\n+      cp.space = cp.space->next_compaction_space();\n+      if (cp.space == nullptr) {\n+        cp.gen = GenCollectedHeap::heap()->young_gen();\n+        assert(cp.gen != nullptr, \"compaction must succeed\");\n+        cp.space = cp.gen->first_compaction_space();\n+        assert(cp.space != nullptr, \"generation must have a first compaction space\");\n+      }\n+      compact_top = cp.space->bottom();\n+      cp.space->set_compaction_top(compact_top);\n+    }\n+\n+    \/\/ Record address of the first live word in this block.\n+    HeapWord* block_start = align_down(first_in_block, words_per_block() * BytesPerWord);\n+    \/\/ Count number of live words preceding the first object in the block. This must\n+    \/\/ be subtracted, because the BOT stores the forwarding address of the first live\n+    \/\/ *word*, not the first live *object* in the block.\n+    size_t num_live = _mark_bitmap.count_marked_words(block_start, first_in_block);\n+    \/\/ Note that we only record the address for blocks where objects start. That\n+    \/\/ is ok, because we only ask for forwarding address of first word of objects.\n+    _table[idx] = compact_top - num_live;\n+    assert(forwardee(first_in_block) == compact_top, \"must match\");\n+\n+    compact_top += live_in_block;\n+  }\n+  cp.space->set_compaction_top(compact_top);\n+}\n+\n+inline HeapWord* SCBlockOffsetTable::forwardee(HeapWord* addr) const {\n+  assert(_mark_bitmap.is_marked(addr), \"must be marked\");\n+  HeapWord* block_base = align_down(addr, words_per_block() * BytesPerWord);\n+  size_t block = addr_to_block_idx(addr);\n+  assert(_table[block] != nullptr, \"must have initialized BOT entry\");\n+  return _table[block] + _mark_bitmap.count_marked_words(block_base, addr);\n+}\n+\n+SerialCompressor::SerialCompressor(STWGCTimer* gc_timer):\n+  _mark_bitmap(),\n+  _marking_stack(),\n+  _objarray_stack(),\n+  _bot(_mark_bitmap),\n+  _string_dedup_requests(),\n+  _gc_timer(gc_timer),\n+  _gc_tracer() {\n+  \/\/ Initialize underlying marking bitmap.\n+  SerialHeap* heap = SerialHeap::heap();\n+  MemRegion reserved = heap->reserved_region();\n+  size_t bitmap_size = MarkBitMap::compute_size(reserved.byte_size());\n+  ReservedSpace bitmap(bitmap_size, MAX2(os::vm_page_size(), (size_t)SCBlockOffsetTable::words_per_block() * BytesPerWord));\n+  _mark_bitmap_region = MemRegion((HeapWord*) bitmap.base(), bitmap.size() \/ HeapWordSize);\n+  os::commit_memory_or_exit((char *)_mark_bitmap_region.start(), _mark_bitmap_region.byte_size(), false,\n+                            \"Cannot commit bitmap memory\");\n+  _mark_bitmap.initialize(heap->reserved_region(), _mark_bitmap_region);\n+}\n+\n+SerialCompressor::~SerialCompressor() {\n+  os::release_memory((char*)_mark_bitmap_region.start(), _mark_bitmap_region.byte_size());\n+}\n+\n+\/\/ Entry point.\n+void SerialCompressor::invoke_at_safepoint(bool clear_all_softrefs) {\n+  assert(SafepointSynchronize::is_at_safepoint(), \"must be at a safepoint\");\n+\n+  SerialHeap* gch = SerialHeap::heap();\n+#ifdef ASSERT\n+  if (gch->soft_ref_policy()->should_clear_all_soft_refs()) {\n+    assert(clear_all_softrefs, \"Policy should have been checked earlier\");\n+  }\n+#endif\n+\n+  gch->trace_heap_before_gc(&_gc_tracer);\n+\n+  \/\/ Capture used regions for each generation that will be\n+  \/\/ subject to collection, so that card table adjustments can\n+  \/\/ be made intelligently (see clear \/ invalidate further below).\n+  gch->save_used_regions();\n+\n+  phase1_mark(clear_all_softrefs);\n+  phase2_build_bot();\n+\n+  \/\/ Don't add any more derived pointers during phase3\n+#if COMPILER2_OR_JVMCI\n+  assert(DerivedPointerTable::is_active(), \"Sanity\");\n+  DerivedPointerTable::set_active(false);\n+#endif\n+\n+  phase3_compact_and_update();\n+\n+  \/\/ Set saved marks for allocation profiler (and other things? -- dld)\n+  \/\/ (Should this be in general part?)\n+  gch->save_marks();\n+\n+  bool is_young_gen_empty = (gch->young_gen()->used() == 0);\n+  gch->rem_set()->maintain_old_to_young_invariant(gch->old_gen(), is_young_gen_empty);\n+\n+  gch->prune_scavengable_nmethods();\n+\n+  \/\/ Update heap occupancy information which is used as\n+  \/\/ input to soft ref clearing policy at the next gc.\n+  Universe::heap()->update_capacity_and_used_at_gc();\n+\n+  \/\/ Signal that we have completed a visit to all live objects.\n+  Universe::heap()->record_whole_heap_examined_timestamp();\n+\n+  gch->trace_heap_after_gc(&_gc_tracer);\n+}\n+\n+template<class T>\n+void SerialCompressor::mark_and_push(T* p) {\n+  T heap_oop = RawAccess<>::oop_load(p);\n+  if (!CompressedOops::is_null(heap_oop)) {\n+    oop obj = CompressedOops::decode_not_null(heap_oop);\n+    if (mark_object(obj)) {\n+      _marking_stack.push(obj);\n+    }\n+  }\n+}\n+\n+class SCMarkAndPushClosure: public ClaimMetadataVisitingOopIterateClosure {\n+private:\n+  SerialCompressor& _compressor;\n+\n+  template<typename T>\n+  void do_oop_work(T* p) {\n+    _compressor.mark_and_push(p);\n+  }\n+\n+public:\n+  SCMarkAndPushClosure(int claim, SerialCompressor& compressor) :\n+    ClaimMetadataVisitingOopIterateClosure(claim),\n+    _compressor(compressor) {\n+    set_ref_discoverer_internal(compressor.ref_processor());\n+  }\n+\n+  void do_oop(oop* p)       { do_oop_work(p); }\n+  void do_oop(narrowOop* p) { do_oop_work(p); }\n+};\n+\n+class SCFollowRootClosure: public BasicOopIterateClosure {\n+private:\n+  SerialCompressor& _compressor;\n+\n+  template<class T>\n+  void follow_root(T* p) {\n+    assert(!Universe::heap()->is_in(p), \"roots shouldn't be things within the heap\");\n+    _compressor.mark_and_push(p);\n+    _compressor.follow_stack();\n+  }\n+\n+public:\n+  SCFollowRootClosure(SerialCompressor& compressor) :\n+    _compressor(compressor) { }\n+\n+  void do_oop(oop* p)       { follow_root(p); }\n+  void do_oop(narrowOop* p) { follow_root(p); }\n+};\n+\n+class SCFollowStackClosure: public VoidClosure {\n+private:\n+  SerialCompressor& _compressor;\n+public:\n+  SCFollowStackClosure(SerialCompressor& compressor) :\n+    _compressor(compressor) { }\n+  void do_void() {\n+    _compressor.follow_stack();\n+  }\n+};\n+\n+class SCIsAliveClosure: public BoolObjectClosure {\n+  MarkBitMap& _mark_bitmap;\n+public:\n+  SCIsAliveClosure(MarkBitMap& mark_bitmap) :\n+    _mark_bitmap(mark_bitmap) { }\n+  bool do_object_b(oop p) {\n+    return _mark_bitmap.is_marked(p);\n+  }\n+};\n+\n+class SCKeepAliveClosure: public OopClosure {\n+private:\n+  SerialCompressor& _compressor;\n+  template<class T>\n+  void do_oop_work(T* p) {\n+    _compressor.mark_and_push(p);\n+  }\n+public:\n+  SCKeepAliveClosure(SerialCompressor& compressor) :\n+    _compressor(compressor) { }\n+  void do_oop(oop* p)       { do_oop_work(p); }\n+  void do_oop(narrowOop* p) { do_oop_work(p); }\n+};\n+\n+bool SerialCompressor::mark_object(oop obj) {\n+  HeapWord* addr = cast_from_oop<HeapWord*>(obj);\n+  if (!_mark_bitmap.is_marked(addr)) {\n+    if (StringDedup::is_enabled() &&\n+        java_lang_String::is_instance(obj) &&\n+        SerialStringDedup::is_candidate_from_mark(obj)) {\n+      _string_dedup_requests.add(obj);\n+    }\n+\n+    \/\/ Do the transform while we still have the header intact,\n+    \/\/ which might include important class information.\n+    ContinuationGCSupport::transform_stack_chunk(obj);\n+\n+    _mark_bitmap.mark_range(addr, obj->size());\n+    return true;\n+  } else {\n+    return false;\n+  }\n+}\n+\n+void SerialCompressor::push_objarray(objArrayOop array, size_t index) {\n+  ObjArrayTask task(array, index);\n+  assert(task.is_valid(), \"bad ObjArrayTask\");\n+  _objarray_stack.push(task);\n+}\n+\n+void SerialCompressor::follow_array(objArrayOop array) {\n+  SCMarkAndPushClosure mark_and_push_closure(ClassLoaderData::_claim_stw_fullgc_mark, *this);\n+  mark_and_push_closure.do_klass(array->klass());\n+\n+  if (array->length() > 0) {\n+    push_objarray(array, 0);\n+  }\n+}\n+\n+void SerialCompressor::follow_object(oop obj) {\n+  assert(_mark_bitmap.is_marked(obj), \"p must be marked\");\n+  if (obj->is_objArray()) {\n+    follow_array((objArrayOop)obj);\n+  } else {\n+    SCMarkAndPushClosure mark_and_push_closure(ClassLoaderData::_claim_stw_fullgc_mark, *this);\n+    obj->oop_iterate(&mark_and_push_closure);\n+  }\n+}\n+\n+void SerialCompressor::follow_array_chunk(objArrayOop array, int index) {\n+  const int len = array->length();\n+  const int beg_index = index;\n+  assert(beg_index < len || len == 0, \"index too large\");\n+\n+  const int stride = MIN2(len - beg_index, (int) ObjArrayMarkingStride);\n+  const int end_index = beg_index + stride;\n+\n+  SCMarkAndPushClosure mark_and_push_closure(ClassLoaderData::_claim_stw_fullgc_mark, *this);\n+  array->oop_iterate_range(&mark_and_push_closure, beg_index, end_index);\n+\n+  if (end_index < len) {\n+    push_objarray(array, end_index); \/\/ Push the continuation.\n+  }\n+}\n+\n+void SerialCompressor::follow_stack() {\n+  do {\n+    while (!_marking_stack.is_empty()) {\n+      oop obj = _marking_stack.pop();\n+      assert(_mark_bitmap.is_marked(obj), \"p must be marked\");\n+      follow_object(obj);\n+    }\n+    \/\/ Process ObjArrays one at a time to avoid marking stack bloat.\n+    if (!_objarray_stack.is_empty()) {\n+      ObjArrayTask task = _objarray_stack.pop();\n+      follow_array_chunk(objArrayOop(task.obj()), task.index());\n+    }\n+  } while (!_marking_stack.is_empty() || !_objarray_stack.is_empty());\n+}\n+\n+void SerialCompressor::phase1_mark(bool clear_all_softrefs) {\n+  \/\/ Recursively traverse all live objects and mark them\n+  GCTraceTime(Info, gc, phases) tm(\"Phase 1: Mark live objects\", _gc_timer);\n+\n+  SerialHeap* gch = SerialHeap::heap();\n+\n+  ClassLoaderDataGraph::verify_claimed_marks_cleared(ClassLoaderData::_claim_stw_fullgc_mark);\n+\n+  AlwaysTrueClosure always_true_closure;\n+  _ref_processor = new ReferenceProcessor(&always_true_closure);\n+  _ref_processor->start_discovery(clear_all_softrefs);\n+\n+  {\n+    StrongRootsScope srs(0);\n+    SCMarkAndPushClosure mark_and_push_closure(ClassLoaderData::_claim_stw_fullgc_mark, *this);\n+    CLDToOopClosure follow_cld_closure(&mark_and_push_closure, ClassLoaderData::_claim_stw_fullgc_mark);\n+    SCFollowRootClosure follow_root_closure(*this);\n+\n+    CLDClosure* weak_cld_closure = ClassUnloading ? nullptr : &follow_cld_closure;\n+    MarkingCodeBlobClosure mark_code_closure(&follow_root_closure, !CodeBlobToOopClosure::FixRelocations, true);\n+    gch->process_roots(SerialHeap::SO_None,\n+                       &follow_root_closure,\n+                       &follow_cld_closure,\n+                       weak_cld_closure,\n+                       &mark_code_closure);\n+  }\n+\n+  SCIsAliveClosure is_alive(_mark_bitmap);\n+\n+  \/\/ Process reference objects found during marking\n+  {\n+    GCTraceTime(Debug, gc, phases) tm_m(\"Reference Processing\", _gc_timer);\n+\n+    SCKeepAliveClosure keep_alive(*this);\n+    SCFollowStackClosure follow_stack_closure(*this);\n+    ReferenceProcessorPhaseTimes pt(_gc_timer, _ref_processor->max_num_queues());\n+    SerialGCRefProcProxyTask task(is_alive, keep_alive, follow_stack_closure);\n+    const ReferenceProcessorStats& stats = _ref_processor->process_discovered_references(task, pt);\n+    pt.print_all_references();\n+    _gc_tracer.report_gc_reference_stats(stats);\n+  }\n+\n+  \/\/ This is the point where the entire marking should have completed.\n+  assert(_marking_stack.is_empty(), \"Marking should have completed\");\n+\n+  {\n+    GCTraceTime(Debug, gc, phases) tm_m(\"Weak Processing\", _gc_timer);\n+    WeakProcessor::weak_oops_do(&is_alive, &do_nothing_cl);\n+  }\n+\n+  {\n+    GCTraceTime(Debug, gc, phases) tm_m(\"Class Unloading\", _gc_timer);\n+\n+    ClassUnloadingContext* ctx = ClassUnloadingContext::context();\n+\n+    bool unloading_occurred;\n+    {\n+      CodeCache::UnlinkingScope scope(&is_alive);\n+\n+      \/\/ Unload classes and purge the SystemDictionary.\n+      unloading_occurred = SystemDictionary::do_unloading(_gc_timer);\n+\n+      \/\/ Unload nmethods.\n+      CodeCache::do_unloading(unloading_occurred);\n+    }\n+\n+    {\n+      GCTraceTime(Debug, gc, phases) t(\"Purge Unlinked NMethods\", _gc_timer);\n+      \/\/ Release unloaded nmethod's memory.\n+      ctx->purge_nmethods();\n+    }\n+    {\n+      GCTraceTime(Debug, gc, phases) t(\"Free Code Blobs\", _gc_timer);\n+      ctx->free_code_blobs();\n+    }\n+\n+    \/\/ Prune dead klasses from subklass\/sibling\/implementor lists.\n+    Klass::clean_weak_klass_links(unloading_occurred);\n+\n+    \/\/ Clean JVMCI metadata handles.\n+    JVMCI_ONLY(JVMCI::do_unloading(unloading_occurred));\n+  }\n+\n+  {\n+    GCTraceTime(Debug, gc, phases) tm_m(\"Report Object Count\", _gc_timer);\n+    _gc_tracer.report_object_count_after_gc(&is_alive, nullptr);\n+  }\n+\n+  delete _ref_processor;\n+}\n+\n+void SerialCompressor::iterate_spaces_of_generation(CSpaceClosure& cl, Generation* gen) const {\n+  ContiguousSpace* space = gen->first_compaction_space();\n+  while (space != nullptr) {\n+    cl.do_space(space);\n+    space = space->next_compaction_space();\n+  }\n+}\n+\n+void SerialCompressor::iterate_spaces(CSpaceClosure& cl) const {\n+  SerialHeap* heap = SerialHeap::heap();\n+  iterate_spaces_of_generation(cl, heap->old_gen());\n+  iterate_spaces_of_generation(cl, heap->young_gen());\n+}\n+\n+class BuildBOTClosure : public CSpaceClosure {\n+private:\n+  SCBlockOffsetTable& _bot;\n+  CompactPoint _compact_point;\n+public:\n+  BuildBOTClosure(SCBlockOffsetTable& bot) :\n+    _bot(bot),\n+    _compact_point(SerialHeap::heap()->old_gen()) { }\n+\n+  void do_space(ContiguousSpace* space) override {\n+    _bot.build_table_for_space(space, _compact_point);\n+  }\n+};\n+\n+void SerialCompressor::phase2_build_bot() {\n+  GCTraceTime(Info, gc, phases) tm(\"Phase 2: Build block-offset-table\", _gc_timer);\n+  BuildBOTClosure cl(_bot);\n+  iterate_spaces(cl);\n+}\n+\n+class SCUpdateRefsClosure : public BasicOopIterateClosure {\n+private:\n+  const SCBlockOffsetTable& _bot;\n+\n+  template<class T>\n+  void do_oop_work(T* p) {\n+    T heap_oop = RawAccess<>::oop_load(p);\n+    if (!CompressedOops::is_null(heap_oop)) {\n+      oop obj = CompressedOops::decode_raw_not_null(heap_oop);\n+      assert(SerialHeap::heap()->is_in_reserved(obj), \"should be in heap\");\n+      oop forwardee = cast_to_oop(_bot.forwardee(cast_from_oop<HeapWord*>(obj)));\n+      if (forwardee != obj) {\n+        RawAccess<IS_NOT_NULL>::oop_store(p, forwardee);\n+      }\n+    }\n+  }\n+public:\n+  SCUpdateRefsClosure(const SCBlockOffsetTable& bot) :\n+    _bot(bot) {}\n+\n+  void do_oop(oop* p) {\n+    do_oop_work(p);\n+  }\n+  void do_oop(narrowOop* p) {\n+    do_oop_work(p);\n+  }\n+};\n+\n+\/\/ Copied from space.inline.hpp to avoid exposing the method.\n+static void clear_empty_region(ContiguousSpace* space) {\n+  \/\/ Let's remember if we were empty before we did the compaction.\n+  bool was_empty = space->used_region().is_empty();\n+  \/\/ Reset space after compaction is complete\n+  space->reset_after_compaction();\n+  \/\/ We do this clear, below, since it has overloaded meanings for some\n+  \/\/ space subtypes.  For example, TenuredSpace's that were\n+  \/\/ compacted into will have had their offset table thresholds updated\n+  \/\/ continuously, but those that weren't need to have their thresholds\n+  \/\/ re-initialized.  Also mangles unused area for debugging.\n+  if (space->used_region().is_empty()) {\n+    if (!was_empty) space->clear(SpaceDecorator::Mangle);\n+  } else {\n+    if (ZapUnusedHeapArea) space->mangle_unused_area();\n+  }\n+}\n+\n+\/\/ Find space that contains the address.\n+\/\/ Note: we could use a CSpaceClosure, but then we would\n+\/\/ not get the fast return.\n+static ContiguousSpace* space_containing(HeapWord* addr) {\n+  SerialHeap* heap = SerialHeap::heap();\n+  Generation* gen = heap->old_gen();\n+  if (!gen->is_in_reserved(addr)) {\n+    gen = heap->young_gen();\n+  }\n+  assert(gen->is_in_reserved(addr), \"must be\");\n+  ContiguousSpace* space = gen->first_compaction_space();\n+  while (!space->is_in_reserved(addr)) {\n+    assert(space != nullptr, \"must succeed\");\n+    space = space->next_compaction_space();\n+  }\n+  return space;\n+}\n+\n+\/\/ Compact live objects in a space.\n+void SerialCompressor::compact_space(ContiguousSpace* space) const {\n+  HeapWord* bottom = space->bottom();\n+  HeapWord* top = space->top();\n+  HeapWord* current = _mark_bitmap.get_next_marked_addr(bottom, top);\n+  SCUpdateRefsClosure cl(_bot);\n+  \/\/ Visit all live objects in the space.\n+  while (current < top) {\n+    oop obj = cast_to_oop(current);\n+    assert(oopDesc::is_oop(obj), \"must be oop\");\n+    size_t size_in_words = obj->size();\n+\n+    \/\/ Update references of object.\n+    obj->oop_iterate(&cl);\n+\n+    \/\/ Copy object itself.\n+    HeapWord* forwardee = _bot.forwardee(current);\n+    if (current != forwardee) {\n+      Copy::aligned_conjoint_words(current, forwardee, size_in_words);\n+    }\n+\n+    \/\/ We need to update the offset table so that the beginnings of objects can be\n+    \/\/ found during scavenge.  Note that we are updating the offset table based on\n+    \/\/ where the object will be once the compaction phase finishes.\n+    space_containing(forwardee)->update_for_block(forwardee, forwardee + size_in_words);\n+\n+    \/\/ Advance to next live object.\n+    current = _mark_bitmap.get_next_marked_addr(current + size_in_words, top);\n+  }\n+  clear_empty_region(space);\n+}\n+\n+\/\/ Update all GC roots.\n+void SerialCompressor::update_roots() {\n+  ClassLoaderDataGraph::verify_claimed_marks_cleared(ClassLoaderData::_claim_stw_fullgc_adjust);\n+  SerialHeap* heap = SerialHeap::heap();\n+  SCUpdateRefsClosure adjust_pointer_closure(_bot);\n+  CLDToOopClosure adjust_cld_closure(&adjust_pointer_closure, ClassLoaderData::_claim_stw_fullgc_adjust);\n+  CodeBlobToOopClosure code_closure(&adjust_pointer_closure, CodeBlobToOopClosure::FixRelocations);\n+  heap->process_roots(SerialHeap::SO_AllCodeCache,\n+                      &adjust_pointer_closure,\n+                      &adjust_cld_closure,\n+                      &adjust_cld_closure,\n+                      &code_closure);\n+\n+  heap->gen_process_weak_roots(&adjust_pointer_closure);\n+}\n+\n+class SCCompactClosure : public CSpaceClosure {\n+private:\n+  const SerialCompressor& _compressor;\n+public:\n+  SCCompactClosure(const SerialCompressor& compressor) :\n+    _compressor(compressor) {}\n+\n+  void do_space(ContiguousSpace* space) override {\n+    _compressor.compact_space(space);\n+  }\n+};\n+\n+void SerialCompressor::phase3_compact_and_update() {\n+  GCTraceTime(Info, gc, phases) tm(\"Phase 3: Compact heap\", _gc_timer);\n+  update_roots();\n+  SCCompactClosure compact(*this);\n+  iterate_spaces(compact);\n+}\n","filename":"src\/hotspot\/share\/gc\/serial\/serialCompressor.cpp","additions":631,"deletions":0,"binary":false,"changes":631,"status":"added"},{"patch":"@@ -0,0 +1,177 @@\n+\n+#ifndef SHARE_GC_SERIAL_SERIALCOMPRESSOR_HPP\n+#define SHARE_GC_SERIAL_SERIALCOMPRESSOR_HPP\n+\n+#include \"gc\/shared\/gcTrace.hpp\"\n+#include \"gc\/shared\/markBitMap.hpp\"\n+#include \"gc\/shared\/space.hpp\"\n+#include \"gc\/shared\/stringdedup\/stringDedup.hpp\"\n+#include \"gc\/shared\/taskqueue.hpp\"\n+#include \"memory\/allocation.hpp\"\n+#include \"memory\/iterator.hpp\"\n+#include \"utilities\/stack.hpp\"\n+\n+class ContiguousSpace;\n+class Generation;\n+class SCCompactClosure;\n+class SCFollowRootClosure;\n+class SCFollowStackClosure;\n+class SCKeepAliveClosure;\n+class SCMarkAndPushClosure;\n+class Space;\n+class STWGCTimer;\n+\n+\/**\n+ * Implements compacting full-GC for the Serial GC. This is based on\n+ * Abuaiadh et al. [2004] and Kermany and Petrank [2006], as described in\n+ * The Garbage Collection Handbook, Second Edition by Jones, Hosking and Moss [2023].\n+ *\n+ * The Full GC is carried out in 3 phases:\n+ * 1. Marking\n+ * 2. Preparation\n+ * 3. Compaction\n+ *\n+ * The algorithm uses 2 major data-structures:\n+ * - A marking bitmap. Each bit represents one word of the heap (or larger blocks\n+ *   according to MinObjAlignment).\n+ * - A block-offset-table. Each word of the table stores the destination address\n+ *   of each block of the heap. A block spans as many words (or larger blocks,\n+ *   according to MinObjAlignment) as can be represented in a single word in\n+ *   the marking bitmap. For example, on a 64-bit system and the default\n+ *   1-word-alignment, each block would span 64 words of the heap. Note that the\n+ *   sizes have been chosen such that we achieve a reasonable compromise between\n+ *   the size of the table (1\/64th of the heap size) and performance (for each\n+ *   forwarding, we only need to scan at most 64 bits - which can be done very\n+ *   efficiently, see population_count.hpp. It could be done even more efficiently\n+ *   once we allow ~SSE4.2 ISA and use the popcount instruction on x86.\n+ *\n+ * The algorithm then works as follows:\n+ *\n+ * 1. Marking: This is pretty much textbook marking algorithm, with the difference\n+ *    that we are setting one bit for each live object in the heap, not only one\n+ *    bit per object. We are going to use this information to calculate the\n+ *    forwarding pointers of each object.\n+ * 2. Preparation: Here we are building the block-offset-table. The basic idea\n+ *    is to scan the heap bottom to top, keep track of compaction-top for each\n+ *    block and record the compaction target for the first live word of each\n+ *    block in the block-offset-table. (Notice that the first live word of a block\n+ *    will often be from an object that is overlapping from a previous block.)\n+ *    Later (during compaction) we can easily calculate the forwarding address\n+ *    of each object by finding its block, loading the corresponding\n+ *    block-destination, and adding the number of live words preceding the object\n+ *    in its block:\n+ *    forwarding(obj) = bot[block(obj)] + count_live_words(block_base(obj), obj)\n+ * 3. Compaction: This compacts the heap and updates all references in a single\n+ *    sequential pass over the heap.\n+ *    Scan heap bottom to top, for each live object:\n+ *    - Update all its references to point to their forwarded locations\n+ *    - Copy the object itself to its forwarded location\n+ *\n+ * Notice that the actual implementation is more complex than this description.\n+ * In particular, during marking, we also need to take care of reference-processing,\n+ * class-unloading, string-deduplication. The preparation phase is complicated by\n+ * the heap being divided into generations and spaces - we need to ensure that whole\n+ * blocks are compacted into the same space, and that all its objects, including\n+ * the tails that overlap into an adjacent block, fit into the destination space.\n+ *\/\n+\n+class CSpaceClosure : public StackObj {\n+public:\n+  virtual void do_space(ContiguousSpace* space) = 0;\n+};\n+\n+\/**\n+ * The block-offset-table stores the forwarding address for the first live\n+ * word in each heap block.\n+ *\/\n+class SCBlockOffsetTable {\n+private:\n+  \/\/ The block offset table.\n+  HeapWord** _table;\n+\n+  \/\/ The marking bitmap.\n+  const MarkBitMap& _mark_bitmap;\n+\n+  \/\/ The heap region covered by this BOT.\n+  const MemRegion _covered;\n+\n+  \/\/ For a given heap address, compute the index of the\n+  \/\/ corresponding block in the table.\n+  inline size_t addr_to_block_idx(HeapWord* addr) const;\n+\n+public:\n+  \/\/ Return the number of heap words covered by each block.\n+  static int words_per_block() {\n+    return BitsPerWord << LogMinObjAlignment;\n+  }\n+\n+  SCBlockOffsetTable(MarkBitMap& mark_bitmap);\n+  ~SCBlockOffsetTable();\n+\n+  void build_table_for_space(ContiguousSpace* space, CompactPoint& cp);\n+\n+  inline HeapWord* forwardee(HeapWord* addr) const;\n+};\n+\n+class SerialCompressor : public StackObj {\n+  friend class SCCompactClosure;\n+  friend class SCFollowRootClosure;\n+  friend class SCFollowStackClosure;\n+  friend class SCKeepAliveClosure;\n+  friend class SCMarkAndPushClosure;\n+private:\n+\n+  \/\/ Memory area of the underlying marking bitmap.\n+  MemRegion  _mark_bitmap_region;\n+  \/\/ The marking bitmap.\n+  MarkBitMap _mark_bitmap;\n+  \/\/ The marking stack.\n+  Stack<oop,mtGC> _marking_stack;\n+  \/\/ Separate marking stack for object-array-chunks.\n+  Stack<ObjArrayTask, mtGC> _objarray_stack;\n+\n+  \/\/ The block-offset table.\n+  SCBlockOffsetTable _bot;\n+\n+  \/\/ String-dedup support.\n+  StringDedup::Requests _string_dedup_requests;\n+\n+  STWGCTimer* _gc_timer;\n+  SerialOldTracer _gc_tracer;\n+  ReferenceProcessor* _ref_processor;\n+\n+  \/\/ Space iteration support.\n+  void iterate_spaces_of_generation(CSpaceClosure& cl, Generation* gen) const;\n+  void iterate_spaces(CSpaceClosure& cl) const;\n+\n+  \/\/ Phase 1: Marking.\n+  void phase1_mark(bool clear_all_softrefs);\n+  \/\/ Phase 2: Building the block-offset-table.\n+  void phase2_build_bot();\n+  \/\/ Phase 3: Compacting and updating references.\n+  void phase3_compact_and_update();\n+\n+  \/\/ Various markingsupport methods.\n+  bool mark_object(oop obj);\n+  void follow_array(objArrayOop array);\n+  void follow_array_chunk(objArrayOop array, int index);\n+  void follow_object(oop obj);\n+  void push_objarray(objArrayOop array, size_t index);\n+  ReferenceProcessor* ref_processor() const { return _ref_processor; }\n+  void follow_stack();\n+  template<class T>\n+  void mark_and_push(T* p);\n+\n+  \/\/ Update GC roots.\n+  void update_roots();\n+  void compact_space(ContiguousSpace* space) const;\n+\n+public:\n+  SerialCompressor(STWGCTimer* gc_timer);\n+  ~SerialCompressor();\n+\n+  \/\/ Entry point.\n+  void invoke_at_safepoint(bool clear_all_softrefs);\n+};\n+\n+#endif \/\/ SHARE_GC_SERIAL_SERIALCOMPRESSOR_HPP\n","filename":"src\/hotspot\/share\/gc\/serial\/serialCompressor.hpp","additions":177,"deletions":0,"binary":false,"changes":177,"status":"added"},{"patch":"@@ -29,0 +29,1 @@\n+#include \"gc\/serial\/serialCompressor.hpp\"\n@@ -437,1 +438,6 @@\n-  GenMarkSweep::invoke_at_safepoint(clear_all_soft_refs);\n+  if (UseCompressorFullGC) {\n+    SerialCompressor full_gc = SerialCompressor(gc_timer);\n+    full_gc.invoke_at_safepoint(clear_all_soft_refs);\n+  } else {\n+    GenMarkSweep::invoke_at_safepoint(clear_all_soft_refs);\n+  }\n","filename":"src\/hotspot\/share\/gc\/serial\/tenuredGeneration.cpp","additions":7,"deletions":1,"binary":false,"changes":8,"status":"modified"},{"patch":"@@ -699,0 +699,3 @@\n+                                                                            \\\n+  product(bool, UseCompressorFullGC, false, EXPERIMENTAL,                   \\\n+          \"Use compressor-style full GC\")                                   \\\n","filename":"src\/hotspot\/share\/gc\/shared\/gc_globals.hpp","additions":3,"deletions":0,"binary":false,"changes":3,"status":"modified"},{"patch":"@@ -97,0 +97,2 @@\n+  inline void mark_range(HeapWord* addr, size_t num_words);\n+\n@@ -101,0 +103,3 @@\n+\n+  \/\/ Count the number of marked words in the range [start, end).\n+  inline size_t count_marked_words(HeapWord* start, HeapWord* end) const;\n","filename":"src\/hotspot\/share\/gc\/shared\/markBitMap.hpp","additions":5,"deletions":0,"binary":false,"changes":5,"status":"modified"},{"patch":"@@ -55,0 +55,6 @@\n+inline void MarkBitMap::mark_range(HeapWord* addr, size_t num_words) {\n+  size_t beg = addr_to_offset(addr);\n+  size_t end = addr_to_offset(addr + num_words);\n+  _bm.set_range(beg, end);\n+}\n+\n@@ -77,0 +83,4 @@\n+inline size_t MarkBitMap::count_marked_words(HeapWord* start, HeapWord* end) const {\n+  return _bm.count_one_bits_within_word(addr_to_offset(start), addr_to_offset(end)) << LogMinObjAlignment;\n+}\n+\n","filename":"src\/hotspot\/share\/gc\/shared\/markBitMap.inline.hpp","additions":10,"deletions":0,"binary":false,"changes":10,"status":"modified"},{"patch":"@@ -245,4 +245,0 @@\n-  \/\/ This the function to invoke when an allocation of an object covering\n-  \/\/ \"start\" to \"end\" occurs to update other internal data structures.\n-  virtual void update_for_block(HeapWord* start, HeapWord* the_end) { }\n-\n@@ -405,0 +401,4 @@\n+  \/\/ This the function to invoke when an allocation of an object covering\n+  \/\/ \"start\" to \"end\" occurs to update other internal data structures.\n+  virtual void update_for_block(HeapWord* start, HeapWord* the_end) { }\n+\n","filename":"src\/hotspot\/share\/gc\/shared\/space.hpp","additions":4,"deletions":4,"binary":false,"changes":8,"status":"modified"},{"patch":"@@ -2995,0 +2995,3 @@\n+  if (UseCompactObjectHeaders && UseSerialGC && !UseCompressorFullGC) {\n+    FLAG_SET_DEFAULT(UseCompressorFullGC, true);\n+  }\n","filename":"src\/hotspot\/share\/runtime\/arguments.cpp","additions":3,"deletions":0,"binary":false,"changes":3,"status":"modified"},{"patch":"@@ -607,11 +607,0 @@\n-BitMap::idx_t BitMap::count_one_bits_within_word(idx_t beg, idx_t end) const {\n-  if (beg != end) {\n-    assert(end > beg, \"must be\");\n-    bm_word_t mask = ~inverted_bit_mask_for_range(beg, end);\n-    bm_word_t w = *word_addr(beg);\n-    w &= mask;\n-    return population_count(w);\n-  }\n-  return 0;\n-}\n-\n","filename":"src\/hotspot\/share\/utilities\/bitMap.cpp","additions":0,"deletions":11,"binary":false,"changes":11,"status":"modified"},{"patch":"@@ -177,2 +177,3 @@\n-\n-  idx_t count_one_bits_within_word(idx_t beg, idx_t end) const;\n+public:\n+  inline idx_t count_one_bits_within_word(idx_t beg, idx_t end) const;\n+protected:\n","filename":"src\/hotspot\/share\/utilities\/bitMap.hpp","additions":3,"deletions":2,"binary":false,"changes":5,"status":"modified"},{"patch":"@@ -34,0 +34,1 @@\n+#include \"utilities\/population_count.hpp\"\n@@ -589,0 +590,12 @@\n+inline BitMap::idx_t BitMap::count_one_bits_within_word(idx_t beg, idx_t end) const {\n+  if (beg != end) {\n+    assert(end > beg, \"must be\");\n+    bm_word_t mask = ~inverted_bit_mask_for_range(beg, end);\n+    bm_word_t w = *word_addr(beg);\n+    w &= mask;\n+    return population_count(w);\n+  }\n+  return 0;\n+}\n+\n+\n","filename":"src\/hotspot\/share\/utilities\/bitMap.inline.hpp","additions":13,"deletions":0,"binary":false,"changes":13,"status":"modified"}]}