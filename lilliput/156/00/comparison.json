{"files":[{"patch":"@@ -599,0 +599,1 @@\n+  assert(LockingMode != LM_LIGHTWEIGHT, \"Use LightweightSynchronizer\");\n@@ -607,55 +608,1 @@\n-    if (LockingMode == LM_LIGHTWEIGHT) {\n-      \/\/ Fast-locking does not use the 'lock' argument.\n-      LockStack& lock_stack = locking_thread->lock_stack();\n-      if (lock_stack.is_full()) {\n-        \/\/ We unconditionally make room on the lock stack by inflating\n-        \/\/ the least recently locked object on the lock stack.\n-\n-        \/\/ About the choice to inflate least recently locked object.\n-        \/\/ First we must chose to inflate a lock, either some lock on\n-        \/\/ the lock-stack or the lock that is currently being entered\n-        \/\/ (which may or may not be on the lock-stack).\n-        \/\/ Second the best lock to inflate is a lock which is entered\n-        \/\/ in a control flow where there are only a very few locks being\n-        \/\/ used, as the costly part of inflated locking is inflation,\n-        \/\/ not locking. But this property is entirely program dependent.\n-        \/\/ Third inflating the lock currently being entered on when it\n-        \/\/ is not present on the lock-stack will result in a still full\n-        \/\/ lock-stack. This creates a scenario where every deeper nested\n-        \/\/ monitorenter must call into the runtime.\n-        \/\/ The rational here is as follows:\n-        \/\/ Because we cannot (currently) figure out the second, and want\n-        \/\/ to avoid the third, we inflate a lock on the lock-stack.\n-        \/\/ The least recently locked lock is chosen as it is the lock\n-        \/\/ with the longest critical section.\n-\n-        log_info(monitorinflation)(\"LockStack capacity exceeded, inflating.\");\n-        ObjectMonitor* monitor = inflate_for(locking_thread, lock_stack.bottom(), inflate_cause_vm_internal);\n-        assert(monitor->owner() == Thread::current(), \"must be owner=\" PTR_FORMAT \" current=\" PTR_FORMAT \" mark=\" PTR_FORMAT,\n-               p2i(monitor->owner()), p2i(Thread::current()), monitor->object()->mark_acquire().value());\n-        assert(!lock_stack.is_full(), \"must have made room here\");\n-      }\n-\n-      markWord mark = obj()->mark_acquire();\n-      while (mark.is_unlocked()) {\n-        \/\/ Retry until a lock state change has been observed. cas_set_mark() may collide with non lock bits modifications.\n-        \/\/ Try to swing into 'fast-locked' state.\n-        assert(!lock_stack.contains(obj()), \"thread must not already hold the lock\");\n-        const markWord locked_mark = mark.set_fast_locked();\n-        const markWord old_mark = obj()->cas_set_mark(locked_mark, mark);\n-        if (old_mark == mark) {\n-          \/\/ Successfully fast-locked, push object to lock-stack and return.\n-          lock_stack.push(obj());\n-          return true;\n-        }\n-        mark = old_mark;\n-      }\n-\n-      if (mark.is_fast_locked() && lock_stack.try_recursive_enter(obj())) {\n-        \/\/ Recursive lock successful.\n-        return true;\n-      }\n-\n-      \/\/ Failed to fast lock.\n-      return false;\n-    } else if (LockingMode == LM_LEGACY) {\n+    if (LockingMode == LM_LEGACY) {\n","filename":"src\/hotspot\/share\/runtime\/synchronizer.cpp","additions":2,"deletions":55,"binary":false,"changes":57,"status":"modified"}]}