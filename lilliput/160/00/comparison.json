{"files":[{"patch":"@@ -761,10 +761,3 @@\n-    if (LockingMode == LM_LIGHTWEIGHT) {\n-      \/\/ TODO[OMWorld]: Clean this monitorenter_obj up. We still want to use the lock_reg for lightweight\n-      call_VM(noreg,\n-              CAST_FROM_FN_PTR(address, InterpreterRuntime::monitorenter_obj),\n-              obj_reg);\n-    } else {\n-      call_VM(noreg,\n-              CAST_FROM_FN_PTR(address, InterpreterRuntime::monitorenter),\n-              lock_reg);\n-    }\n+    call_VM(noreg,\n+            CAST_FROM_FN_PTR(address, InterpreterRuntime::monitorenter),\n+            lock_reg);\n","filename":"src\/hotspot\/cpu\/aarch64\/interp_masm_aarch64.cpp","additions":3,"deletions":10,"binary":false,"changes":13,"status":"modified"},{"patch":"@@ -993,9 +993,1 @@\n-    if (LockingMode == LM_LIGHTWEIGHT) {\n-      \/\/ Pass oop, not lock, in fast lock case. call_VM wants R1 though.\n-      push(R1);\n-      mov(R1, Robj);\n-      call_VM(noreg, CAST_FROM_FN_PTR(address, InterpreterRuntime::monitorenter_obj), R1);\n-      pop(R1);\n-    } else {\n-      call_VM(noreg, CAST_FROM_FN_PTR(address, InterpreterRuntime::monitorenter), Rlock);\n-    }\n+    call_VM(noreg, CAST_FROM_FN_PTR(address, InterpreterRuntime::monitorenter), Rlock);\n","filename":"src\/hotspot\/cpu\/arm\/interp_masm_arm.cpp","additions":1,"deletions":9,"binary":false,"changes":10,"status":"modified"},{"patch":"@@ -1048,5 +1048,1 @@\n-    if (LockingMode == LM_LIGHTWEIGHT) {\n-      call_VM(noreg, CAST_FROM_FN_PTR(address, InterpreterRuntime::monitorenter_obj), object);\n-    } else {\n-      call_VM(noreg, CAST_FROM_FN_PTR(address, InterpreterRuntime::monitorenter), monitor);\n-    }\n+    call_VM(noreg, CAST_FROM_FN_PTR(address, InterpreterRuntime::monitorenter), monitor);\n","filename":"src\/hotspot\/cpu\/ppc\/interp_masm_ppc_64.cpp","additions":1,"deletions":5,"binary":false,"changes":6,"status":"modified"},{"patch":"@@ -802,9 +802,3 @@\n-    if (LockingMode == LM_LIGHTWEIGHT) {\n-      call_VM(noreg,\n-              CAST_FROM_FN_PTR(address, InterpreterRuntime::monitorenter_obj),\n-              obj_reg);\n-    } else {\n-      call_VM(noreg,\n-              CAST_FROM_FN_PTR(address, InterpreterRuntime::monitorenter),\n-              lock_reg);\n-    }\n+    call_VM(noreg,\n+            CAST_FROM_FN_PTR(address, InterpreterRuntime::monitorenter),\n+            lock_reg);\n","filename":"src\/hotspot\/cpu\/riscv\/interp_masm_riscv.cpp","additions":3,"deletions":9,"binary":false,"changes":12,"status":"modified"},{"patch":"@@ -1081,10 +1081,3 @@\n-  if (LockingMode == LM_LIGHTWEIGHT) {\n-    \/\/ for lightweight locking we need to use monitorenter_obj, see interpreterRuntime.cpp\n-    call_VM(noreg,\n-            CAST_FROM_FN_PTR(address, InterpreterRuntime::monitorenter_obj),\n-            object);\n-  } else {\n-    call_VM(noreg,\n-            CAST_FROM_FN_PTR(address, InterpreterRuntime::monitorenter),\n-            monitor);\n-  }\n+  call_VM(noreg,\n+          CAST_FROM_FN_PTR(address, InterpreterRuntime::monitorenter),\n+          monitor);\n","filename":"src\/hotspot\/cpu\/s390\/interp_masm_s390.cpp","additions":3,"deletions":10,"binary":false,"changes":13,"status":"modified"},{"patch":"@@ -44,0 +44,1 @@\n+#include \"utilities\/macros.hpp\"\n@@ -69,0 +70,1 @@\n+    lightweight_lock(disp_hdr, obj, hdr, thread, tmp, slow_case);\n@@ -70,1 +72,3 @@\n-    const Register thread = disp_hdr;\n+    movptr(hdr, Address(obj, oopDesc::mark_offset_in_bytes()));\n+    \/\/ Lacking registers and thread on x86_32. Always take slow path.\n+    jmp(slow_case);\n@@ -72,1 +76,0 @@\n-    lightweight_lock(disp_hdr, obj, hdr, thread, tmp, slow_case);\n","filename":"src\/hotspot\/cpu\/x86\/c1_MacroAssembler_x86.cpp","additions":5,"deletions":2,"binary":false,"changes":7,"status":"modified"},{"patch":"@@ -1211,6 +1211,0 @@\n-#ifndef _LP64\n-        \/\/ TODO[OMWorld]: Unify 32 with 64. Should just be a straight up use 64 on 32. We have the registers here.\n-        \/\/ Check if recursive.\n-        xorptr(reg_rax, reg_rax);\n-        orptr(reg_rax, Address(monitor, ObjectMonitor::recursions_offset()));\n-        jcc(Assembler::notZero, check_successor);\n@@ -1218,4 +1212,1 @@\n-        \/\/ Check if the entry lists are empty.\n-        movptr(reg_rax, Address(monitor, ObjectMonitor::EntryList_offset()));\n-        orptr(reg_rax, Address(monitor, ObjectMonitor::cxq_offset()));\n-        jcc(Assembler::notZero, check_successor);\n+      Label recursive;\n@@ -1223,4 +1214,3 @@\n-        \/\/ Release lock.\n-        movptr(Address(monitor, ObjectMonitor::owner_offset()), NULL_WORD);\n-#else \/\/ _LP64\n-        Label recursive;\n+      \/\/ Check if recursive.\n+      cmpptr(Address(monitor,ObjectMonitor::recursions_offset()),0);\n+      jccb(Assembler::notEqual, recursive);\n@@ -1228,18 +1218,13 @@\n-        \/\/ Check if recursive.\n-        cmpptr(Address(monitor,ObjectMonitor::recursions_offset()),0);\n-        jccb(Assembler::notEqual, recursive);\n-\n-        \/\/ Check if the entry lists are empty.\n-        movptr(reg_rax, Address(monitor, ObjectMonitor::cxq_offset()));\n-        orptr(reg_rax, Address(monitor, ObjectMonitor::EntryList_offset()));\n-        jcc(Assembler::notZero, check_successor);\n-\n-        \/\/ Release lock.\n-        movptr(Address(monitor, ObjectMonitor::owner_offset()), NULL_WORD);\n-        jmpb(unlocked);\n-\n-        \/\/ Recursive unlock.\n-        bind(recursive);\n-        decrement(Address(monitor, ObjectMonitor::recursions_offset()));\n-        xorl(t, t);\n-#endif\n+      \/\/ Check if the entry lists are empty.\n+      movptr(reg_rax, Address(monitor, ObjectMonitor::cxq_offset()));\n+      orptr(reg_rax, Address(monitor, ObjectMonitor::EntryList_offset()));\n+      jcc(Assembler::notZero, check_successor);\n+\n+      \/\/ Release lock.\n+      movptr(Address(monitor, ObjectMonitor::owner_offset()), NULL_WORD);\n+      jmpb(unlocked);\n+\n+      \/\/ Recursive unlock.\n+      bind(recursive);\n+      decrement(Address(monitor, ObjectMonitor::recursions_offset()));\n+      xorl(t, t);\n","filename":"src\/hotspot\/cpu\/x86\/c2_MacroAssembler_x86.cpp","additions":17,"deletions":32,"binary":false,"changes":49,"status":"modified"},{"patch":"@@ -1191,0 +1191,1 @@\n+      lightweight_lock(lock_reg, obj_reg, swap_reg, thread, tmp_reg, slow_case);\n@@ -1192,1 +1193,2 @@\n-      const Register thread = lock_reg;\n+      \/\/ Lacking registers and thread on x86_32. Always take slow path.\n+      jmp(slow_case);\n@@ -1194,1 +1196,0 @@\n-      lightweight_lock(lock_reg, obj_reg, swap_reg, thread, tmp_reg, slow_case);\n@@ -1256,10 +1257,3 @@\n-    if (LockingMode == LM_LIGHTWEIGHT) {\n-      \/\/ TODO[OMWorld]: Clean this monitorenter_obj up. We still want to use the lock_reg for lightweight\n-      call_VM(noreg,\n-              CAST_FROM_FN_PTR(address, InterpreterRuntime::monitorenter_obj),\n-              obj_reg);\n-    } else {\n-      call_VM(noreg,\n-              CAST_FROM_FN_PTR(address, InterpreterRuntime::monitorenter),\n-              lock_reg);\n-    }\n+    call_VM(noreg,\n+            CAST_FROM_FN_PTR(address, InterpreterRuntime::monitorenter),\n+            lock_reg);\n","filename":"src\/hotspot\/cpu\/x86\/interp_masm_x86.cpp","additions":6,"deletions":12,"binary":false,"changes":18,"status":"modified"},{"patch":"@@ -761,3 +761,2 @@\n-  const bool no_lock = NOT_LP64(LockingMode == LM_LIGHTWEIGHT) LP64_ONLY(false);\n-  assert(no_lock || obj == lock->obj(), \"must match\");\n-  SharedRuntime::monitor_enter_helper(obj, no_lock ? nullptr : lock->lock(), current);\n+  assert(obj == lock->obj(), \"must match: \" PTR_FORMAT, p2i(lock));\n+  SharedRuntime::monitor_enter_helper(obj, lock->lock(), current);\n","filename":"src\/hotspot\/share\/c1\/c1_Runtime1.cpp","additions":2,"deletions":3,"binary":false,"changes":5,"status":"modified"},{"patch":"@@ -738,1 +738,0 @@\n-  assert(LockingMode != LM_LIGHTWEIGHT, \"Should call monitorenter_obj() when using the new lightweight locking\");\n@@ -753,17 +752,0 @@\n-\/\/ NOTE: We provide a separate implementation for the new lightweight locking to workaround a limitation\n-\/\/ of registers in x86_32. This entry point accepts an oop instead of a BasicObjectLock*.\n-\/\/ The problem is that we would need to preserve the register that holds the BasicObjectLock,\n-\/\/ but we are using that register to hold the thread. We don't have enough registers to\n-\/\/ also keep the BasicObjectLock, but we don't really need it anyway, we only need\n-\/\/ the object. See also InterpreterMacroAssembler::lock_object().\n-\/\/ As soon as legacy stack-locking goes away we could remove the other monitorenter() entry\n-\/\/ point, and only use oop-accepting entries (same for monitorexit() below).\n-JRT_ENTRY_NO_ASYNC(void, InterpreterRuntime::monitorenter_obj(JavaThread* current, oopDesc* obj))\n-  assert(LockingMode == LM_LIGHTWEIGHT, \"Should call monitorenter() when not using the new lightweight locking\");\n-  Handle h_obj(current, cast_to_oop(obj));\n-  assert(Universe::heap()->is_in_or_null(h_obj()),\n-         \"must be null or an object\");\n-  ObjectSynchronizer::enter(h_obj, nullptr, current);\n-  return;\n-JRT_END\n-\n","filename":"src\/hotspot\/share\/interpreter\/interpreterRuntime.cpp","additions":0,"deletions":18,"binary":false,"changes":18,"status":"modified"},{"patch":"@@ -251,1 +251,2 @@\n-  assert(this == current(), \"only set own thread locals\");\n+  assert(this == current() || monitor->owner_raw() == this, \"only add owned monitors for other threads\");\n+  assert(this == current() || is_obj_deopt_suspend(), \"thread must not run concurrently\");\n","filename":"src\/hotspot\/share\/runtime\/javaThread.inline.hpp","additions":2,"deletions":1,"binary":false,"changes":3,"status":"modified"},{"patch":"@@ -496,2 +496,3 @@\n-void LightweightSynchronizer::ensure_lock_stack_space(JavaThread* locking_thread, JavaThread* current) {\n-  LockStack& lock_stack = locking_thread->lock_stack();\n+void LightweightSynchronizer::ensure_lock_stack_space(JavaThread* current) {\n+  assert(current == JavaThread::current(), \"must be\");\n+  LockStack& lock_stack = current->lock_stack();\n@@ -502,1 +503,1 @@\n-    LockStackInflateContendedLocks().inflate(locking_thread, current);\n+    LockStackInflateContendedLocks().inflate(current, current);\n@@ -505,1 +506,1 @@\n-      inflate_fast_locked_object(lock_stack.bottom(), locking_thread, current, ObjectSynchronizer::inflate_cause_vm_internal);\n+      inflate_fast_locked_object(lock_stack.bottom(), current, current, ObjectSynchronizer::inflate_cause_vm_internal);\n@@ -510,0 +511,29 @@\n+class LightweightSynchronizer::CacheSetter : StackObj {\n+  JavaThread* const _thread;\n+  BasicLock* const _lock;\n+  ObjectMonitor* _monitor;\n+\n+  NONCOPYABLE(CacheSetter);\n+\n+public:\n+  CacheSetter(JavaThread* thread, BasicLock* lock) :\n+    _thread(thread),\n+    _lock(lock),\n+    _monitor(nullptr) {}\n+\n+  ~CacheSetter() {\n+    if (_monitor != nullptr) {\n+      _thread->om_set_monitor_cache(_monitor);\n+      _lock->set_displaced_header(_monitor);\n+    } else {\n+      _lock->clear_displaced_header();\n+    }\n+  }\n+\n+  void set_monitor(ObjectMonitor* monitor) {\n+    assert(_monitor == nullptr, \"only set once\");\n+    _monitor = monitor;\n+  }\n+\n+};\n+\n@@ -533,4 +563,0 @@\n-  enter(obj, lock, locking_thread, JavaThread::current());\n-}\n-\n-void LightweightSynchronizer::enter(Handle obj, BasicLock* lock, JavaThread* locking_thread, JavaThread* current) {\n@@ -538,0 +564,1 @@\n+  JavaThread* current = JavaThread::current();\n@@ -540,0 +567,1 @@\n+  \/\/ TODO[OMWorld]: Is this necessary?\n@@ -541,1 +569,1 @@\n-    ObjectSynchronizer::handle_sync_on_value_based_class(obj, locking_thread);\n+    ObjectSynchronizer::handle_sync_on_value_based_class(obj, current);\n@@ -546,9 +574,27 @@\n-  if (lock != nullptr) {\n-    \/\/ This is cleared in the interpreter\n-    \/\/ TODO[OMWorld]: All paths should have cleared this, assert it is 0\n-    \/\/                instead of clearing it here. Should maybe only be for\n-    \/\/                c++ ObjectLocks and compiler re-lock (check this)\n-    \/\/                Also double check JNI interactions, JNI does not have\n-    \/\/                a slot, so no cache, but is there a problem if JNI first\n-    \/\/                followed by recursive monitor enter exit\n-    lock->clear_displaced_header();\n+  CacheSetter cache_setter(locking_thread, lock);\n+\n+  LockStack& lock_stack = locking_thread->lock_stack();\n+\n+  ObjectMonitor* monitor = nullptr;\n+  if (lock_stack.contains(obj())) {\n+    monitor = inflate_fast_locked_object(obj(), locking_thread, current, ObjectSynchronizer::inflate_cause_monitor_enter);\n+    bool entered = monitor->enter_for(locking_thread);\n+    assert(entered, \"recursive ObjectMonitor::enter_for must succeed\");\n+  } else {\n+    \/\/ It is assumed that enter_for must enter on an object without contention.\n+    \/\/ TODO[OMWorld]: We also assume that this re-lock is on either a new never\n+    \/\/                inflated monitor, or one that is already locked by the\n+    \/\/                locking_thread. Should we have this stricter restriction?\n+    monitor = inflate_and_enter(obj(), locking_thread, current, ObjectSynchronizer::inflate_cause_monitor_enter);\n+  }\n+\n+  assert(monitor != nullptr, \"LightweightSynchronizer::enter_for must succeed\");\n+  cache_setter.set_monitor(monitor);\n+}\n+\n+void LightweightSynchronizer::enter(Handle obj, BasicLock* lock, JavaThread* current) {\n+  assert(LockingMode == LM_LIGHTWEIGHT, \"must be\");\n+  assert(current == JavaThread::current(), \"must be\");\n+\n+  if (obj->klass()->is_value_based()) {\n+    ObjectSynchronizer::handle_sync_on_value_based_class(obj, current);\n@@ -557,0 +603,4 @@\n+  current->inc_held_monitor_count();\n+\n+  CacheSetter cache_setter(current, lock);\n+\n@@ -560,3 +610,1 @@\n-  LockStack& lock_stack = locking_thread->lock_stack();\n-\n-  \/\/ TODO[OMWorld]: Cleanup locking_thread != current\n+  LockStack& lock_stack = current->lock_stack();\n@@ -575,12 +623,2 @@\n-    ObjectMonitor* mon = inflate_fast_locked_object(obj(), locking_thread, current, ObjectSynchronizer::inflate_cause_monitor_enter);\n-    \/\/ TODO[OMWorld]: Cleanup enter_for\n-    bool entered = false;\n-    if (locking_thread == current) {\n-      entered = mon->enter(locking_thread);\n-      locking_thread->om_set_monitor_cache(mon);\n-    } else {\n-      entered = mon->enter_for(locking_thread);\n-    }\n-    if (lock != nullptr) {\n-      lock->set_displaced_header(mon);\n-    }\n+    ObjectMonitor* monitor = inflate_fast_locked_object(obj(), current, current, ObjectSynchronizer::inflate_cause_monitor_enter);\n+    bool entered = monitor->enter(current);\n@@ -588,0 +626,1 @@\n+    cache_setter.set_monitor(monitor);\n@@ -602,1 +641,1 @@\n-        ensure_lock_stack_space(locking_thread, current);\n+        ensure_lock_stack_space(current);\n@@ -624,1 +663,3 @@\n-    if (inflate_and_enter(obj(), lock, locking_thread, current, ObjectSynchronizer::inflate_cause_monitor_enter)) {\n+    ObjectMonitor* monitor = inflate_and_enter(obj(), current, current, ObjectSynchronizer::inflate_cause_monitor_enter);\n+    if (monitor != nullptr) {\n+      cache_setter.set_monitor(monitor);\n@@ -793,1 +834,1 @@\n-bool LightweightSynchronizer::inflate_and_enter(oop object, BasicLock* lock, JavaThread* locking_thread, JavaThread* current, const ObjectSynchronizer::InflateCause cause) {\n+ObjectMonitor* LightweightSynchronizer::inflate_and_enter(oop object, JavaThread* locking_thread, JavaThread* current, const ObjectSynchronizer::InflateCause cause) {\n@@ -818,6 +859,2 @@\n-  if (current == locking_thread && monitor->try_enter(locking_thread)) {\n-    current->om_set_monitor_cache(monitor);\n-    if (lock != nullptr) {\n-      lock->set_displaced_header(monitor);\n-    }\n-    return true;\n+  if (monitor->try_enter(locking_thread)) {\n+    return monitor;\n@@ -827,1 +864,1 @@\n-  ObjectMonitorContentionMark mark(monitor);\n+  ObjectMonitorContentionMark contention_mark(monitor);\n@@ -853,1 +890,1 @@\n-    return false;\n+    return nullptr;\n@@ -918,5 +955,1 @@\n-    \/\/ Update the thread-local cache\n-    if (current == locking_thread) {\n-      current->om_set_monitor_cache(monitor);\n-      current->_unlocked_inflation++;\n-    }\n+    locking_thread->_unlocked_inflation++;\n@@ -924,1 +957,1 @@\n-    return true;\n+    return monitor;\n@@ -927,2 +960,7 @@\n-  if (current == locking_thread && monitor->has_owner() && monitor->owner_raw() != locking_thread) {\n-    \/\/ Someone else owns the lock, take the time befor entering to fix the lock stack\n+  if (current == locking_thread) {\n+    \/\/ One round of spinning\n+    if (monitor->spin_enter(locking_thread)) {\n+      return monitor;\n+    }\n+\n+    \/\/ Monitor is contended, take the time befor entering to fix the lock stack.\n@@ -936,11 +974,4 @@\n-  \/\/ TODO[OMWorld]: Fix this enter_for\n-  if ((current == locking_thread && monitor->enter(locking_thread)) || monitor->enter_for(locking_thread)) {\n-    \/\/ Update the thread-local cache\n-    if (current == locking_thread) {\n-      current->om_set_monitor_cache(monitor);\n-    }\n-    if (lock != nullptr) {\n-      lock->set_displaced_header(monitor);\n-    }\n-\n-    return true;\n+  if (current == locking_thread) {\n+    monitor->enter_with_contention_mark(locking_thread, contention_mark);\n+  } else {\n+    monitor->enter_for_with_contention_mark(locking_thread, contention_mark);\n@@ -949,1 +980,1 @@\n-  return false;\n+  return monitor;\n","filename":"src\/hotspot\/share\/runtime\/lightweightSynchronizer.cpp","additions":95,"deletions":64,"binary":false,"changes":159,"status":"modified"},{"patch":"@@ -47,1 +47,3 @@\n-  static void ensure_lock_stack_space(JavaThread* locking_thread, JavaThread* current);\n+  static void ensure_lock_stack_space(JavaThread* current);\n+\n+  class CacheSetter;\n@@ -57,1 +59,1 @@\n-  static void enter(Handle obj, BasicLock* lock,  JavaThread* locking_thread, JavaThread* current);\n+  static void enter(Handle obj, BasicLock* lock, JavaThread* current);\n@@ -62,1 +64,1 @@\n-  static bool inflate_and_enter(oop object, BasicLock* lock, JavaThread* locking_thread, JavaThread* current, const ObjectSynchronizer::InflateCause cause);\n+  static ObjectMonitor* inflate_and_enter(oop object, JavaThread* locking_thread, JavaThread* current, const ObjectSynchronizer::InflateCause cause);\n","filename":"src\/hotspot\/share\/runtime\/lightweightSynchronizer.hpp","additions":5,"deletions":3,"binary":false,"changes":8,"status":"modified"},{"patch":"@@ -320,6 +320,0 @@\n-bool ObjectMonitor::try_enter(JavaThread* current) {\n-  void* cur = try_set_owner_from(nullptr, current);\n-  if (cur == nullptr) {\n-    assert(_recursions == 0, \"invariant\");\n-    return true;\n-  }\n@@ -327,2 +321,10 @@\n-  if (cur == current) {\n-    _recursions++;\n+bool ObjectMonitor::enter_is_async_deflating() {\n+  if (is_being_async_deflated()) {\n+    if (LockingMode != LM_LIGHTWEIGHT) {\n+      const oop l_object = object();\n+      if (l_object != nullptr) {\n+        \/\/ Attempt to restore the header\/dmw to the object's header so that\n+        \/\/ we only retry once if the deflater thread happens to be slow.\n+        install_displaced_markword_in_object(l_object);\n+      }\n+    }\n@@ -335,1 +337,1 @@\n-bool ObjectMonitor::enter_for(JavaThread* locking_thread) {\n+void ObjectMonitor::enter_for_with_contention_mark(JavaThread* locking_thread, ObjectMonitorContentionMark& contention_mark) {\n@@ -340,0 +342,2 @@\n+  assert(contention_mark._monitor == this, \"must be\");\n+  assert(!is_being_async_deflated(), \"must be\");\n@@ -341,2 +345,2 @@\n-  \/\/ Block out deflation as soon as possible.\n-  add_to_contentions(1);\n+\n+  void* prev_owner = try_set_owner_from(nullptr, locking_thread);\n@@ -345,2 +349,0 @@\n-  if (!is_being_async_deflated()) {\n-    void* prev_owner = try_set_owner_from(nullptr, locking_thread);\n@@ -348,25 +350,12 @@\n-    if (prev_owner == nullptr) {\n-      assert(_recursions == 0, \"invariant\");\n-      success = true;\n-    } else if (prev_owner == locking_thread) {\n-      _recursions++;\n-      success = true;\n-    } else if (prev_owner == DEFLATER_MARKER) {\n-      \/\/ Racing with deflation.\n-      prev_owner = try_set_owner_from(DEFLATER_MARKER, locking_thread);\n-      if (prev_owner == DEFLATER_MARKER) {\n-        \/\/ Cancelled deflation. Increment contentions as part of the deflation protocol.\n-        add_to_contentions(1);\n-        success = true;\n-      } else if (prev_owner == nullptr) {\n-        \/\/ At this point we cannot race with deflation as we have both incremented\n-        \/\/ contentions, seen contention > 0 and seen a DEFLATER_MARKER.\n-        \/\/ success will only be false if this races with something other than\n-        \/\/ deflation.\n-        prev_owner = try_set_owner_from(nullptr, locking_thread);\n-        success = prev_owner == nullptr;\n-      }\n-    } else if (LockingMode == LM_LEGACY && locking_thread->is_lock_owned((address)prev_owner)) {\n-      assert(_recursions == 0, \"must be\");\n-      _recursions = 1;\n-      set_owner_from_BasicLock(prev_owner, locking_thread);\n+  if (prev_owner == nullptr) {\n+    assert(_recursions == 0, \"invariant\");\n+    success = true;\n+  } else if (prev_owner == locking_thread) {\n+    _recursions++;\n+    success = true;\n+  } else if (prev_owner == DEFLATER_MARKER) {\n+    \/\/ Racing with deflation.\n+    prev_owner = try_set_owner_from(DEFLATER_MARKER, locking_thread);\n+    if (prev_owner == DEFLATER_MARKER) {\n+      \/\/ Cancelled deflation. Increment contentions as part of the deflation protocol.\n+      add_to_contentions(1);\n@@ -374,0 +363,7 @@\n+    } else if (prev_owner == nullptr) {\n+      \/\/ At this point we cannot race with deflation as we have both incremented\n+      \/\/ contentions, seen contention > 0 and seen a DEFLATER_MARKER.\n+      \/\/ success will only be false if this races with something other than\n+      \/\/ deflation.\n+      prev_owner = try_set_owner_from(nullptr, locking_thread);\n+      success = prev_owner == nullptr;\n@@ -375,13 +371,5 @@\n-    assert(success, \"Failed to enter_for: locking_thread=\" INTPTR_FORMAT\n-           \", this=\" INTPTR_FORMAT \"{owner=\" INTPTR_FORMAT \"}, observed owner: \" INTPTR_FORMAT,\n-           p2i(locking_thread), p2i(this), p2i(owner_raw()), p2i(prev_owner));\n-  } else {\n-    \/\/ Async deflation is in progress and our contentions increment\n-    \/\/ above lost the race to async deflation. Undo the work and\n-    \/\/ force the caller to retry.\n-    const oop l_object = object();\n-    if (l_object != nullptr) {\n-      \/\/ Attempt to restore the header\/dmw to the object's header so that\n-      \/\/ we only retry once if the deflater thread happens to be slow.\n-      install_displaced_markword_in_object(l_object);\n-    }\n+  } else if (LockingMode == LM_LEGACY && locking_thread->is_lock_owned((address)prev_owner)) {\n+    assert(_recursions == 0, \"must be\");\n+    _recursions = 1;\n+    set_owner_from_BasicLock(prev_owner, locking_thread);\n+    success = true;\n@@ -389,0 +377,4 @@\n+  assert(success, \"Failed to enter_for: locking_thread=\" INTPTR_FORMAT\n+          \", this=\" INTPTR_FORMAT \"{owner=\" INTPTR_FORMAT \"}, observed owner: \" INTPTR_FORMAT,\n+          p2i(locking_thread), p2i(this), p2i(owner_raw()), p2i(prev_owner));\n+}\n@@ -390,1 +382,1 @@\n-  add_to_contentions(-1);\n+bool ObjectMonitor::enter_for(JavaThread* locking_thread) {\n@@ -392,1 +384,2 @@\n-  assert(!success || owner_raw() == locking_thread, \"must be\");\n+  \/\/ Block out deflation as soon as possible.\n+  ObjectMonitorContentionMark contention_mark(this);\n@@ -394,2 +387,4 @@\n-  return success;\n-}\n+  \/\/ Check for deflation.\n+  if (enter_is_async_deflating()) {\n+    return false;\n+  }\n@@ -397,4 +392,4 @@\n-bool ObjectMonitor::enter(JavaThread* current) {\n-  assert(current == JavaThread::current(), \"must be\");\n-  \/\/ The following code is ordered to check the most common cases first\n-  \/\/ and to reduce RTS->RTO cache line upgrades on SPARC and IA32 processors.\n+  enter_for_with_contention_mark(locking_thread, contention_mark);\n+  assert(owner_raw() == locking_thread, \"must be\");\n+  return true;\n+}\n@@ -402,0 +397,1 @@\n+bool ObjectMonitor::try_enter(JavaThread* current) {\n@@ -409,1 +405,0 @@\n-    \/\/ TODO-FIXME: check for integer overflow!  BUGID 6557169.\n@@ -421,0 +416,16 @@\n+  return false;\n+}\n+\n+bool ObjectMonitor::spin_enter(JavaThread* current) {\n+  assert(current == JavaThread::current(), \"must be\");\n+\n+  \/\/ Check for recursion.\n+  if (try_enter(current)) {\n+    return true;\n+  }\n+\n+  \/\/ Check for deflation.\n+  if (enter_is_async_deflating()) {\n+    return false;\n+  }\n+\n@@ -423,3 +434,1 @@\n-  \/\/ Try one round of spinning *before* enqueueing current\n-  \/\/ and before going through the awkward and expensive state\n-  \/\/ transitions.  The following spin is strictly optional ...\n+  \/\/ Do one round of spinning.\n@@ -435,0 +444,10 @@\n+  return false;\n+}\n+\n+bool ObjectMonitor::enter(JavaThread* current) {\n+  assert(current == JavaThread::current(), \"must be\");\n+\n+  if (spin_enter(current)) {\n+    return true;\n+  }\n+\n@@ -440,13 +459,5 @@\n-  \/\/ Keep track of contention for JVM\/TI and M&M queries.\n-  add_to_contentions(1);\n-  if (is_being_async_deflated()) {\n-    \/\/ Async deflation is in progress and our contentions increment\n-    \/\/ above lost the race to async deflation. Undo the work and\n-    \/\/ force the caller to retry.\n-    const oop l_object = object();\n-    if (LockingMode != LM_LIGHTWEIGHT && l_object != nullptr) {\n-      \/\/ Attempt to restore the header\/dmw to the object's header so that\n-      \/\/ we only retry once if the deflater thread happens to be slow.\n-      install_displaced_markword_in_object(l_object);\n-    }\n-    add_to_contentions(-1);\n+  \/\/ Keep is_being_async_deflated stable across the rest of enter\n+  ObjectMonitorContentionMark contention_mark(this);\n+\n+  \/\/ Check for deflation.\n+  if (enter_is_async_deflating()) {\n@@ -456,0 +467,11 @@\n+  \/\/ At this point this ObjectMonitor cannot be deflated, finish contended enter\n+  enter_with_contention_mark(current, contention_mark);\n+  return true;\n+}\n+\n+void ObjectMonitor::enter_with_contention_mark(JavaThread *current, ObjectMonitorContentionMark &cm) {\n+  assert(current == JavaThread::current(), \"must be\");\n+  assert(owner_raw() != current, \"must be\");\n+  assert(cm._monitor == this, \"must be\");\n+  assert(!is_being_async_deflated(), \"must be\");\n+\n@@ -512,1 +534,0 @@\n-  add_to_contentions(-1);\n@@ -548,1 +569,0 @@\n-  return true;\n","filename":"src\/hotspot\/share\/runtime\/objectMonitor.cpp","additions":98,"deletions":78,"binary":false,"changes":176,"status":"modified"},{"patch":"@@ -36,0 +36,1 @@\n+class ObjectMonitorContentionMark;\n@@ -352,0 +353,2 @@\n+\n+  bool      enter_is_async_deflating();\n@@ -353,1 +356,1 @@\n-  bool      try_enter(JavaThread* current);\n+  void      enter_for_with_contention_mark(JavaThread* locking_thread, ObjectMonitorContentionMark& contention_mark);\n@@ -356,0 +359,3 @@\n+  bool      try_enter(JavaThread* current);\n+  bool      spin_enter(JavaThread* current);\n+  void      enter_with_contention_mark(JavaThread* current, ObjectMonitorContentionMark& contention_mark);\n@@ -392,1 +398,3 @@\n-class ObjectMonitorContentionMark {\n+class ObjectMonitorContentionMark : StackObj {\n+  DEBUG_ONLY(friend class ObjectMonitor;)\n+\n@@ -395,0 +403,2 @@\n+  NONCOPYABLE(ObjectMonitorContentionMark);\n+\n@@ -396,1 +406,1 @@\n-  ObjectMonitorContentionMark(ObjectMonitor* monitor);\n+  explicit ObjectMonitorContentionMark(ObjectMonitor* monitor);\n","filename":"src\/hotspot\/share\/runtime\/objectMonitor.hpp","additions":13,"deletions":3,"binary":false,"changes":16,"status":"modified"},{"patch":"@@ -577,1 +577,1 @@\n-    return LightweightSynchronizer::enter(obj, lock, current, current);\n+    return LightweightSynchronizer::enter(obj, lock, current);\n@@ -797,1 +797,1 @@\n-      entered = LightweightSynchronizer::inflate_and_enter(obj(), nullptr, current, current, inflate_cause_jni_enter);\n+      entered = LightweightSynchronizer::inflate_and_enter(obj(), current, current, inflate_cause_jni_enter) != nullptr;\n","filename":"src\/hotspot\/share\/runtime\/synchronizer.cpp","additions":2,"deletions":2,"binary":false,"changes":4,"status":"modified"}]}