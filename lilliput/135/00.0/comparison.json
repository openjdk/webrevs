{"files":[{"patch":"@@ -16450,0 +16450,1 @@\n+  predicate(LockingMode != LM_LIGHTWEIGHT);\n@@ -16453,3 +16454,1 @@\n-  \/\/ TODO\n-  \/\/ identify correct cost\n-  format %{ \"fastlock $object,$box\\t! kills $tmp,$tmp2\" %}\n+  format %{ \"fastlock $object,$box\\t! kills $tmp,$tmp2,$tmp3\" %}\n@@ -16467,0 +16466,1 @@\n+  predicate(LockingMode != LM_LIGHTWEIGHT);\n@@ -16480,0 +16480,31 @@\n+instruct cmpFastLockLightweight(rFlagsReg cr, iRegP object, iRegP box, iRegPNoSp tmp, iRegPNoSp tmp2)\n+%{\n+  predicate(LockingMode == LM_LIGHTWEIGHT);\n+  match(Set cr (FastLock object box));\n+  effect(TEMP tmp, TEMP tmp2);\n+\n+  ins_cost(5 * INSN_COST);\n+  format %{ \"fastlock $object,$box\\t! kills $tmp,$tmp2\" %}\n+\n+  ins_encode %{\n+    __ fast_lock_lightweight($object$$Register, $box$$Register, $tmp$$Register, $tmp2$$Register);\n+  %}\n+\n+  ins_pipe(pipe_serial);\n+%}\n+\n+instruct cmpFastUnlockLightweight(rFlagsReg cr, iRegP object, iRegP box, iRegPNoSp tmp, iRegPNoSp tmp2)\n+%{\n+  predicate(LockingMode == LM_LIGHTWEIGHT);\n+  match(Set cr (FastUnlock object box));\n+  effect(TEMP tmp, TEMP tmp2);\n+\n+  ins_cost(5 * INSN_COST);\n+  format %{ \"fastunlock $object,$box\\t! kills $tmp, $tmp2\" %}\n+\n+  ins_encode %{\n+    __ fast_unlock_lightweight($object$$Register, $box$$Register, $tmp$$Register, $tmp2$$Register);\n+  %}\n+\n+  ins_pipe(pipe_serial);\n+%}\n","filename":"src\/hotspot\/cpu\/aarch64\/aarch64.ad","additions":34,"deletions":3,"binary":false,"changes":37,"status":"modified"},{"patch":"@@ -2,1 +2,1 @@\n- * Copyright (c) 1999, 2023, Oracle and\/or its affiliates. All rights reserved.\n+ * Copyright (c) 1999, 2024, Oracle and\/or its affiliates. All rights reserved.\n@@ -83,2 +83,0 @@\n-  \/\/ Load object header\n-  ldr(hdr, Address(obj, hdr_offset));\n@@ -89,0 +87,2 @@\n+    \/\/ Load object header\n+    ldr(hdr, Address(obj, hdr_offset));\n@@ -147,5 +147,0 @@\n-    ldr(hdr, Address(obj, oopDesc::mark_offset_in_bytes()));\n-    \/\/ We cannot use tbnz here, the target might be too far away and cannot\n-    \/\/ be encoded.\n-    tst(hdr, markWord::monitor_value);\n-    br(Assembler::NE, slow_case);\n","filename":"src\/hotspot\/cpu\/aarch64\/c1_MacroAssembler_aarch64.cpp","additions":3,"deletions":8,"binary":false,"changes":11,"status":"modified"},{"patch":"@@ -35,0 +35,1 @@\n+#include \"utilities\/globalDefinitions.hpp\"\n@@ -58,0 +59,1 @@\n+  assert(LockingMode != LM_LIGHTWEIGHT, \"lightweight locking should use fast_lock_lightweight\");\n@@ -76,1 +78,2 @@\n-  } else if (LockingMode == LM_LEGACY) {\n+  } else {\n+    assert(LockingMode == LM_LEGACY, \"must be\");\n@@ -105,4 +108,0 @@\n-  } else {\n-    assert(LockingMode == LM_LIGHTWEIGHT, \"must be\");\n-    lightweight_lock(oop, disp_hdr, tmp, tmp3Reg, no_count);\n-    b(count);\n@@ -122,8 +121,7 @@\n-  if (LockingMode != LM_LIGHTWEIGHT) {\n-    \/\/ Store a non-null value into the box to avoid looking like a re-entrant\n-    \/\/ lock. The fast-path monitor unlock code checks for\n-    \/\/ markWord::monitor_value so use markWord::unused_mark which has the\n-    \/\/ relevant bit set, and also matches ObjectSynchronizer::enter.\n-    mov(tmp, (address)markWord::unused_mark().value());\n-    str(tmp, Address(box, BasicLock::displaced_header_offset_in_bytes()));\n-  }\n+  \/\/ Store a non-null value into the box to avoid looking like a re-entrant\n+  \/\/ lock. The fast-path monitor unlock code checks for\n+  \/\/ markWord::monitor_value so use markWord::unused_mark which has the\n+  \/\/ relevant bit set, and also matches ObjectSynchronizer::enter.\n+  mov(tmp, (address)markWord::unused_mark().value());\n+  str(tmp, Address(box, BasicLock::displaced_header_offset_in_bytes()));\n+\n@@ -160,0 +158,1 @@\n+  assert(LockingMode != LM_LIGHTWEIGHT, \"lightweight locking should use fast_unlock_lightweight\");\n@@ -178,1 +177,2 @@\n-  } else if (LockingMode == LM_LEGACY) {\n+  } else {\n+    assert(LockingMode == LM_LEGACY, \"must be\");\n@@ -186,4 +186,0 @@\n-  } else {\n-    assert(LockingMode == LM_LIGHTWEIGHT, \"must be\");\n-    lightweight_unlock(oop, tmp, box, disp_hdr, no_count);\n-    b(count);\n@@ -199,13 +195,0 @@\n-  if (LockingMode == LM_LIGHTWEIGHT) {\n-    \/\/ If the owner is anonymous, we need to fix it -- in an outline stub.\n-    Register tmp2 = disp_hdr;\n-    ldr(tmp2, Address(tmp, ObjectMonitor::owner_offset()));\n-    \/\/ We cannot use tbnz here, the target might be too far away and cannot\n-    \/\/ be encoded.\n-    tst(tmp2, (uint64_t)ObjectMonitor::ANONYMOUS_OWNER);\n-    C2HandleAnonOMOwnerStub* stub = new (Compile::current()->comp_arena()) C2HandleAnonOMOwnerStub(tmp, tmp2);\n-    Compile::current()->output()->add_stub(stub);\n-    br(Assembler::NE, stub->entry());\n-    bind(stub->continuation());\n-  }\n-\n@@ -244,0 +227,256 @@\n+void C2_MacroAssembler::fast_lock_lightweight(Register obj, Register t1,\n+                                              Register t2, Register t3) {\n+  assert(LockingMode == LM_LIGHTWEIGHT, \"must be\");\n+  assert_different_registers(obj, t1, t2, t3);\n+\n+  \/\/ Handle inflated monitor.\n+  Label inflated;\n+  \/\/ Finish fast lock successfully. MUST branch to with flag == EQ\n+  Label locked;\n+  \/\/ Finish fast lock unsuccessfully. MUST branch to with flag == NE\n+  Label slow_path;\n+\n+  if (DiagnoseSyncOnValueBasedClasses != 0) {\n+    load_klass(t1, obj);\n+    ldrw(t1, Address(t1, Klass::access_flags_offset()));\n+    tstw(t1, JVM_ACC_IS_VALUE_BASED_CLASS);\n+    br(Assembler::NE, slow_path);\n+  }\n+\n+  const Register t1_mark = t1;\n+\n+  { \/\/ Lightweight locking\n+\n+    \/\/ Push lock to the lock stack and finish successfully. MUST branch to with flag == EQ\n+    Label push;\n+\n+    const Register t2_top = t2;\n+    const Register t3_t = t3;\n+\n+    \/\/ Check if lock-stack is full.\n+    ldrw(t2_top, Address(rthread, JavaThread::lock_stack_top_offset()));\n+    cmpw(t2_top, (unsigned)LockStack::end_offset() - 1);\n+    br(Assembler::GT, slow_path);\n+\n+    \/\/ Check if recursive.\n+    subw(t3_t, t2_top, oopSize);\n+    ldr(t3_t, Address(rthread, t3_t));\n+    cmp(obj, t3_t);\n+    br(Assembler::EQ, push);\n+\n+    \/\/ Relaxed normal load to check for monitor. Optimization for monitor case.\n+    ldr(t1_mark, Address(obj, oopDesc::mark_offset_in_bytes()));\n+    tbnz(t1_mark, exact_log2(markWord::monitor_value), inflated);\n+\n+    \/\/ Not inflated\n+    assert(oopDesc::mark_offset_in_bytes() == 0, \"required to avoid a lea\");\n+\n+    \/\/ Try to lock. Transition lock-bits 0b01 => 0b00\n+    orr(t1_mark, t1_mark, markWord::unlocked_value);\n+    eor(t3_t, t1_mark, markWord::unlocked_value);\n+    cmpxchg(\/*addr*\/ obj, \/*expected*\/ t1_mark, \/*new*\/ t3_t, Assembler::xword,\n+            \/*acquire*\/ true, \/*release*\/ false, \/*weak*\/ false, noreg);\n+    br(Assembler::NE, slow_path);\n+\n+    bind(push);\n+    \/\/ After successful lock, push object on lock-stack.\n+    str(obj, Address(rthread, t2_top));\n+    addw(t2_top, t2_top, oopSize);\n+    strw(t2_top, Address(rthread, JavaThread::lock_stack_top_offset()));\n+    b(locked);\n+  }\n+\n+  { \/\/ Handle inflated monitor.\n+    bind(inflated);\n+\n+    \/\/ mark contains the tagged ObjectMonitor*.\n+    const Register t1_tagged_monitor = t1_mark;\n+    const uintptr_t monitor_tag = markWord::monitor_value;\n+    const Register t2_owner_addr = t2;\n+    const Register t3_owner = t3;\n+\n+    \/\/ Compute owner address.\n+    lea(t2_owner_addr, Address(t1_tagged_monitor, (in_bytes(ObjectMonitor::owner_offset()) - monitor_tag)));\n+\n+    \/\/ CAS owner (null => current thread).\n+    cmpxchg(t2_owner_addr, zr, rthread, Assembler::xword, \/*acquire*\/ true,\n+            \/*release*\/ false, \/*weak*\/ false, t3_owner);\n+    br(Assembler::EQ, locked);\n+\n+    \/\/ Check if recursive.\n+    cmp(t3_owner, rthread);\n+    br(Assembler::NE, slow_path);\n+\n+    \/\/ Recursive.\n+    increment(Address(t1_tagged_monitor, in_bytes(ObjectMonitor::recursions_offset()) - monitor_tag), 1);\n+  }\n+\n+  bind(locked);\n+  increment(Address(rthread, JavaThread::held_monitor_count_offset()));\n+\n+#ifdef ASSERT\n+  \/\/ Check that locked label is reached with Flags == EQ.\n+  Label flag_correct;\n+  br(Assembler::EQ, flag_correct);\n+  stop(\"Fast Lock Flag != EQ\");\n+#endif\n+\n+  bind(slow_path);\n+#ifdef ASSERT\n+  \/\/ Check that slow_path label is reached with Flags == NE.\n+  br(Assembler::NE, flag_correct);\n+  stop(\"Fast Lock Flag != NE\");\n+  bind(flag_correct);\n+#endif\n+  \/\/ C2 uses the value of Flags (NE vs EQ) to determine the continuation.\n+}\n+\n+void C2_MacroAssembler::fast_unlock_lightweight(Register obj, Register t1, Register t2,\n+                                                Register t3) {\n+  assert(LockingMode == LM_LIGHTWEIGHT, \"must be\");\n+  assert_different_registers(obj, t1, t2, t3);\n+\n+  \/\/ Handle inflated monitor.\n+  Label inflated, inflated_load_monitor;\n+  \/\/ Finish fast unlock successfully. MUST branch to with flag == EQ\n+  Label unlocked;\n+  \/\/ Finish fast unlock unsuccessfully. MUST branch to with flag == NE\n+  Label slow_path;\n+\n+  const Register t1_mark = t1;\n+  const Register t2_top = t2;\n+  const Register t3_t = t3;\n+\n+  { \/\/ Lightweight unlock\n+\n+    \/\/ Check if obj is top of lock-stack.\n+    ldrw(t2_top, Address(rthread, JavaThread::lock_stack_top_offset()));\n+    subw(t2_top, t2_top, oopSize);\n+    ldr(t3_t, Address(rthread, t2_top));\n+    cmp(obj, t3_t);\n+    \/\/ Top of lock stack was not obj. Must be monitor.\n+    br(Assembler::NE, inflated_load_monitor);\n+\n+    \/\/ Pop lock-stack.\n+    DEBUG_ONLY(str(zr, Address(rthread, t2_top));)\n+    strw(t2_top, Address(rthread, JavaThread::lock_stack_top_offset()));\n+\n+    \/\/ Check if recursive.\n+    subw(t3_t, t2_top, oopSize);\n+    ldr(t3_t, Address(rthread, t3_t));\n+    cmp(obj, t3_t);\n+    br(Assembler::EQ, unlocked);\n+\n+    \/\/ Not recursive.\n+    \/\/ Load Mark.\n+    ldr(t1_mark, Address(obj, oopDesc::mark_offset_in_bytes()));\n+\n+    \/\/ Check header for monitor (0b10).\n+    tbnz(t1_mark, exact_log2(markWord::monitor_value), inflated);\n+\n+    \/\/ Try to unlock. Transition lock bits 0b00 => 0b01\n+    assert(oopDesc::mark_offset_in_bytes() == 0, \"required to avoid lea\");\n+    orr(t3_t, t1_mark, markWord::unlocked_value);\n+    cmpxchg(\/*addr*\/ obj, \/*expected*\/ t1_mark, \/*new*\/ t3_t, Assembler::xword,\n+            \/*acquire*\/ false, \/*release*\/ true, \/*weak*\/ false, noreg);\n+    br(Assembler::EQ, unlocked);\n+\n+    \/\/ Compare and exchange failed.\n+    \/\/ Restore lock-stack and handle the unlock in runtime.\n+    DEBUG_ONLY(str(obj, Address(rthread, t2_top));)\n+    addw(t2_top, t2_top, oopSize);\n+    str(t2_top, Address(rthread, JavaThread::lock_stack_top_offset()));\n+    b(slow_path);\n+  }\n+\n+\n+  { \/\/ Handle inflated monitor.\n+    bind(inflated_load_monitor);\n+    ldr(t1_mark, Address(obj, oopDesc::mark_offset_in_bytes()));\n+#ifdef ASSERT\n+    tbnz(t1_mark, exact_log2(markWord::monitor_value), inflated);\n+    stop(\"Fast Unlock not monitor\");\n+#endif\n+\n+    bind(inflated);\n+\n+#ifdef ASSERT\n+    Label check_done;\n+    subw(t2_top, t2_top, oopSize);\n+    cmpw(t2_top, in_bytes(JavaThread::lock_stack_base_offset()));\n+    br(Assembler::LT, check_done);\n+    ldr(t3_t, Address(rthread, t2_top));\n+    cmp(obj, t3_t);\n+    br(Assembler::NE, inflated);\n+    stop(\"Fast Unlock lock on stack\");\n+    bind(check_done);\n+#endif\n+\n+    \/\/ mark contains the tagged ObjectMonitor*.\n+    const Register t1_monitor = t1_mark;\n+    const uintptr_t monitor_tag = markWord::monitor_value;\n+\n+    \/\/ Untag the monitor.\n+    sub(t1_monitor, t1_mark, monitor_tag);\n+\n+    const Register t2_recursions = t2;\n+    Label not_recursive;\n+\n+    \/\/ Check if recursive.\n+    ldr(t2_recursions, Address(t1_monitor, ObjectMonitor::recursions_offset()));\n+    cbz(t2_recursions, not_recursive);\n+\n+    \/\/ Recursive unlock.\n+    sub(t2_recursions, t2_recursions, 1u);\n+    str(t2_recursions, Address(t1_monitor, ObjectMonitor::recursions_offset()));\n+    \/\/ Set flag == EQ\n+    cmp(t2_recursions, t2_recursions);\n+    b(unlocked);\n+\n+    bind(not_recursive);\n+\n+    Label release;\n+    const Register t2_owner_addr = t2;\n+\n+    \/\/ Compute owner address.\n+    lea(t2_owner_addr, Address(t1_monitor, ObjectMonitor::owner_offset()));\n+\n+    \/\/ Check if the entry lists are empty.\n+    ldr(rscratch1, Address(t1_monitor, ObjectMonitor::EntryList_offset()));\n+    ldr(t3_t, Address(t1_monitor, ObjectMonitor::cxq_offset()));\n+    orr(rscratch1, rscratch1, t3_t);\n+    cmp(rscratch1, zr);\n+    br(Assembler::EQ, release);\n+\n+    \/\/ The owner may be anonymous and we removed the last obj entry in\n+    \/\/ the lock-stack. This loses the information about the owner.\n+    \/\/ Write the thread to the owner field so the runtime knows the owner.\n+    str(rthread, Address(t2_owner_addr));\n+    b(slow_path);\n+\n+    bind(release);\n+    \/\/ Set owner to null.\n+    \/\/ Release to satisfy the JMM\n+    stlr(zr, t2_owner_addr);\n+  }\n+\n+  bind(unlocked);\n+  decrement(Address(rthread, JavaThread::held_monitor_count_offset()));\n+\n+#ifdef ASSERT\n+  \/\/ Check that unlocked label is reached with Flags == EQ.\n+  Label flag_correct;\n+  br(Assembler::EQ, flag_correct);\n+  stop(\"Fast Unlock Flag != EQ\");\n+#endif\n+\n+  bind(slow_path);\n+#ifdef ASSERT\n+  \/\/ Check that slow_path label is reached with Flags == NE.\n+  br(Assembler::NE, flag_correct);\n+  stop(\"Fast Unlock Flag != NE\");\n+  bind(flag_correct);\n+#endif\n+  \/\/ C2 uses the value of Flags (NE vs EQ) to determine the continuation.\n+}\n+\n","filename":"src\/hotspot\/cpu\/aarch64\/c2_MacroAssembler_aarch64.cpp","additions":270,"deletions":31,"binary":false,"changes":301,"status":"modified"},{"patch":"@@ -2,1 +2,1 @@\n- * Copyright (c) 2020, 2023, Oracle and\/or its affiliates. All rights reserved.\n+ * Copyright (c) 2020, 2024, Oracle and\/or its affiliates. All rights reserved.\n@@ -39,1 +39,0 @@\n-  \/\/ See full description in macroAssembler_aarch64.cpp.\n@@ -42,0 +41,3 @@\n+  \/\/ Code used by cmpFastLockLightweight and cmpFastUnlockLightweight mach instructions in .ad file.\n+  void fast_lock_lightweight(Register object, Register t1, Register t2, Register t3);\n+  void fast_unlock_lightweight(Register object, Register t1, Register t2, Register t3);\n","filename":"src\/hotspot\/cpu\/aarch64\/c2_MacroAssembler_aarch64.hpp","additions":4,"deletions":2,"binary":false,"changes":6,"status":"modified"},{"patch":"@@ -2,1 +2,1 @@\n- * Copyright (c) 1997, 2023, Oracle and\/or its affiliates. All rights reserved.\n+ * Copyright (c) 1997, 2024, Oracle and\/or its affiliates. All rights reserved.\n@@ -26,2 +26,0 @@\n-#include <sys\/types.h>\n-\n@@ -59,0 +57,1 @@\n+#include \"utilities\/globalDefinitions.hpp\"\n@@ -70,0 +69,2 @@\n+#include <sys\/types.h>\n+\n@@ -6429,2 +6430,0 @@\n-\/\/ Branches to slow upon failure to lock the object, with ZF cleared.\n-\/\/ Falls through upon success with ZF set.\n@@ -6433,3 +6432,3 @@\n-\/\/  - hdr: the header, already loaded from obj, will be destroyed\n-\/\/  - t1, t2: temporary registers, will be destroyed\n-void MacroAssembler::lightweight_lock(Register obj, Register hdr, Register t1, Register t2, Label& slow) {\n+\/\/  - t1, t2, t3: temporary registers, will be destroyed\n+\/\/  - slow: branched to if locking fails, absolute offset may larger than 32KB (imm14 encoding).\n+void MacroAssembler::lightweight_lock(Register obj, Register t1, Register t2, Register t3, Label& slow) {\n@@ -6437,15 +6436,24 @@\n-  assert_different_registers(obj, hdr, t1, t2, rscratch1);\n-\n-  \/\/ Check if we would have space on lock-stack for the object.\n-  ldrw(t1, Address(rthread, JavaThread::lock_stack_top_offset()));\n-  cmpw(t1, (unsigned)LockStack::end_offset() - 1);\n-  br(Assembler::GT, slow);\n-\n-  \/\/ Load (object->mark() | 1) into hdr\n-  orr(hdr, hdr, markWord::unlocked_value);\n-  \/\/ Clear lock-bits, into t2\n-  eor(t2, hdr, markWord::unlocked_value);\n-  \/\/ Try to swing header from unlocked to locked\n-  \/\/ Clobbers rscratch1 when UseLSE is false\n-  cmpxchg(\/*addr*\/ obj, \/*expected*\/ hdr, \/*new*\/ t2, Assembler::xword,\n-          \/*acquire*\/ true, \/*release*\/ true, \/*weak*\/ false, t1);\n+  assert_different_registers(obj, t1, t2, t3, rscratch1);\n+\n+  Label push;\n+  const Register top = t1;\n+  const Register mark = t2;\n+  const Register t = t3;\n+\n+  \/\/ Preload the markWord. It is important that this is the first\n+  \/\/ instruction emitted as it is part of C1's null check semantics.\n+  ldr(mark, Address(obj, oopDesc::mark_offset_in_bytes()));\n+\n+  \/\/ Check if the lock-stack is full.\n+  ldrw(top, Address(rthread, JavaThread::lock_stack_top_offset()));\n+  cmpw(top, (unsigned)LockStack::end_offset());\n+  br(Assembler::GE, slow);\n+\n+  \/\/ Check for recursion.\n+  subw(t, top, oopSize);\n+  ldr(t, Address(rthread, t));\n+  cmp(obj, t);\n+  br(Assembler::EQ, push);\n+\n+  \/\/ Check header for monitor (0b10).\n+  tst(mark, markWord::monitor_value);\n@@ -6454,5 +6462,13 @@\n-  \/\/ After successful lock, push object on lock-stack\n-  ldrw(t1, Address(rthread, JavaThread::lock_stack_top_offset()));\n-  str(obj, Address(rthread, t1));\n-  addw(t1, t1, oopSize);\n-  strw(t1, Address(rthread, JavaThread::lock_stack_top_offset()));\n+  \/\/ Try to lock. Transition lock bits 0b01 => 0b00\n+  assert(oopDesc::mark_offset_in_bytes() == 0, \"required to avoid lea\");\n+  orr(mark, mark, markWord::unlocked_value);\n+  eor(t, mark, markWord::unlocked_value);\n+  cmpxchg(\/*addr*\/ obj, \/*expected*\/ mark, \/*new*\/ t, Assembler::xword,\n+          \/*acquire*\/ true, \/*release*\/ false, \/*weak*\/ false, noreg);\n+  br(Assembler::NE, slow);\n+\n+  bind(push);\n+  \/\/ After successful lock, push object on lock-stack.\n+  str(obj, Address(rthread, top));\n+  addw(top, top, oopSize);\n+  strw(top, Address(rthread, JavaThread::lock_stack_top_offset()));\n@@ -6462,2 +6478,0 @@\n-\/\/ Branches to slow upon failure, with ZF cleared.\n-\/\/ Falls through upon success, with ZF set.\n@@ -6466,3 +6480,3 @@\n-\/\/ - hdr: the (pre-loaded) header of the object\n-\/\/ - t1, t2: temporary registers\n-void MacroAssembler::lightweight_unlock(Register obj, Register hdr, Register t1, Register t2, Label& slow) {\n+\/\/ - t1, t2, t3: temporary registers\n+\/\/ - slow: branched to if unlocking fails, absolute offset may larger than 32KB (imm14 encoding).\n+void MacroAssembler::lightweight_unlock(Register obj, Register t1, Register t2, Register t3, Label& slow) {\n@@ -6470,1 +6484,2 @@\n-  assert_different_registers(obj, hdr, t1, t2, rscratch1);\n+  \/\/ cmpxchg clobbers rscratch1.\n+  assert_different_registers(obj, t1, t2, t3, rscratch1);\n@@ -6474,4 +6489,0 @@\n-    \/\/ The following checks rely on the fact that LockStack is only ever modified by\n-    \/\/ its owning thread, even if the lock got inflated concurrently; removal of LockStack\n-    \/\/ entries after inflation will happen delayed in that case.\n-\n@@ -6482,1 +6493,1 @@\n-    br(Assembler::GT, stack_ok);\n+    br(Assembler::GE, stack_ok);\n@@ -6486,18 +6497,0 @@\n-  {\n-    \/\/ Check if the top of the lock-stack matches the unlocked object.\n-    Label tos_ok;\n-    subw(t1, t1, oopSize);\n-    ldr(t1, Address(rthread, t1));\n-    cmpoop(t1, obj);\n-    br(Assembler::EQ, tos_ok);\n-    STOP(\"Top of lock-stack does not match the unlocked object\");\n-    bind(tos_ok);\n-  }\n-  {\n-    \/\/ Check that hdr is fast-locked.\n-    Label hdr_ok;\n-    tst(hdr, markWord::lock_mask_in_place);\n-    br(Assembler::EQ, hdr_ok);\n-    STOP(\"Header is not fast-locked\");\n-    bind(hdr_ok);\n-  }\n@@ -6506,2 +6499,4 @@\n-  \/\/ Load the new header (unlocked) into t1\n-  orr(t1, hdr, markWord::unlocked_value);\n+  Label unlocked, push_and_slow;\n+  const Register top = t1;\n+  const Register mark = t2;\n+  const Register t = t3;\n@@ -6509,4 +6504,5 @@\n-  \/\/ Try to swing header from locked to unlocked\n-  \/\/ Clobbers rscratch1 when UseLSE is false\n-  cmpxchg(obj, hdr, t1, Assembler::xword,\n-          \/*acquire*\/ true, \/*release*\/ true, \/*weak*\/ false, t2);\n+  \/\/ Check if obj is top of lock-stack.\n+  ldrw(top, Address(rthread, JavaThread::lock_stack_top_offset()));\n+  subw(top, top, oopSize);\n+  ldr(t, Address(rthread, top));\n+  cmp(obj, t);\n@@ -6515,3 +6511,14 @@\n-  \/\/ After successful unlock, pop object from lock-stack\n-  ldrw(t1, Address(rthread, JavaThread::lock_stack_top_offset()));\n-  subw(t1, t1, oopSize);\n+  \/\/ Pop lock-stack.\n+  DEBUG_ONLY(str(zr, Address(rthread, top));)\n+  strw(top, Address(rthread, JavaThread::lock_stack_top_offset()));\n+\n+  \/\/ Check if recursive.\n+  subw(t, top, oopSize);\n+  ldr(t, Address(rthread, t));\n+  cmp(obj, t);\n+  br(Assembler::EQ, unlocked);\n+\n+  \/\/ Not recursive. Check header for monitor (0b10).\n+  ldr(mark, Address(obj, oopDesc::mark_offset_in_bytes()));\n+  tbnz(mark, log2i_exact(markWord::monitor_value), push_and_slow);\n+\n@@ -6519,1 +6526,5 @@\n-  str(zr, Address(rthread, t1));\n+  \/\/ Check header not unlocked (0b01).\n+  Label not_unlocked;\n+  tbz(mark, log2i_exact(markWord::unlocked_value), not_unlocked);\n+  stop(\"lightweight_unlock already unlocked\");\n+  bind(not_unlocked);\n@@ -6521,1 +6532,16 @@\n-  strw(t1, Address(rthread, JavaThread::lock_stack_top_offset()));\n+\n+  \/\/ Try to unlock. Transition lock bits 0b00 => 0b01\n+  assert(oopDesc::mark_offset_in_bytes() == 0, \"required to avoid lea\");\n+  orr(t, mark, markWord::unlocked_value);\n+  cmpxchg(obj, mark, t, Assembler::xword,\n+          \/*acquire*\/ false, \/*release*\/ true, \/*weak*\/ false, noreg);\n+  br(Assembler::EQ, unlocked);\n+\n+  bind(push_and_slow);\n+  \/\/ Restore lock-stack and handle the unlock in runtime.\n+  DEBUG_ONLY(str(obj, Address(rthread, top));)\n+  addw(top, top, oopSize);\n+  strw(top, Address(rthread, JavaThread::lock_stack_top_offset()));\n+  b(slow);\n+\n+  bind(unlocked);\n","filename":"src\/hotspot\/cpu\/aarch64\/macroAssembler_aarch64.cpp","additions":94,"deletions":68,"binary":false,"changes":162,"status":"modified"},{"patch":"@@ -2,1 +2,1 @@\n- * Copyright (c) 1997, 2023, Oracle and\/or its affiliates. All rights reserved.\n+ * Copyright (c) 1997, 2024, Oracle and\/or its affiliates. All rights reserved.\n@@ -1607,2 +1607,2 @@\n-  void lightweight_lock(Register obj, Register hdr, Register t1, Register t2, Label& slow);\n-  void lightweight_unlock(Register obj, Register hdr, Register t1, Register t2, Label& slow);\n+  void lightweight_lock(Register obj, Register t1, Register t2, Register t3, Label& slow);\n+  void lightweight_unlock(Register obj, Register t1, Register t2, Register t3, Label& slow);\n","filename":"src\/hotspot\/cpu\/aarch64\/macroAssembler_aarch64.hpp","additions":3,"deletions":3,"binary":false,"changes":6,"status":"modified"},{"patch":"@@ -463,0 +463,3 @@\n+  if (ce->compilation()->bailed_out()) {\n+    return; \/\/ CodeCache is full\n+  }\n","filename":"src\/hotspot\/cpu\/ppc\/c1_CodeStubs_ppc.cpp","additions":3,"deletions":0,"binary":false,"changes":3,"status":"modified"},{"patch":"@@ -2,2 +2,2 @@\n- * Copyright (c) 2016, 2023, Oracle and\/or its affiliates. All rights reserved.\n- * Copyright (c) 2016, 2018 SAP SE. All rights reserved.\n+ * Copyright (c) 2016, 2024, Oracle and\/or its affiliates. All rights reserved.\n+ * Copyright (c) 2016, 2024 SAP SE. All rights reserved.\n@@ -436,0 +436,1 @@\n+  CHECK_BAILOUT();\n","filename":"src\/hotspot\/cpu\/s390\/c1_CodeStubs_s390.cpp","additions":3,"deletions":2,"binary":false,"changes":5,"status":"modified"},{"patch":"@@ -1695,3 +1695,6 @@\n-  char *base = os::remap_memory(_fd, _full_path, r->file_offset(),\n-                                addr, size, false \/* !read_only *\/,\n-                                r->allow_exec());\n+  \/\/ This path should not be reached for Windows; see JDK-8222379.\n+  assert(WINDOWS_ONLY(false) NOT_WINDOWS(true), \"Don't call on Windows\");\n+  \/\/ Replace old mapping with new one that is writable.\n+  char *base = os::map_memory(_fd, _full_path, r->file_offset(),\n+                              addr, size, false \/* !read_only *\/,\n+                              r->allow_exec());\n","filename":"src\/hotspot\/share\/cds\/filemap.cpp","additions":6,"deletions":3,"binary":false,"changes":9,"status":"modified"},{"patch":"@@ -195,1 +195,1 @@\n-  _rem_set = create_rem_set(heap_rs.region());\n+  _rem_set = new CardTableRS(heap_rs.region());\n@@ -212,5 +212,0 @@\n-\n-CardTableRS* SerialHeap::create_rem_set(const MemRegion& reserved_region) {\n-  return new CardTableRS(reserved_region);\n-}\n-\n@@ -527,1 +522,1 @@\n-    if (run_verification && VerifyGCLevel <= 0 && VerifyBeforeGC) {\n+    if (run_verification && VerifyBeforeGC) {\n@@ -539,1 +534,1 @@\n-                       run_verification && VerifyGCLevel <= 0,\n+                       run_verification,\n@@ -577,2 +572,1 @@\n-    if (!prepared_for_verification && run_verification &&\n-        VerifyGCLevel <= 1 && VerifyBeforeGC) {\n+    if (!prepared_for_verification && run_verification && VerifyBeforeGC) {\n@@ -604,1 +598,1 @@\n-                       run_verification && VerifyGCLevel <= 1,\n+                       run_verification,\n@@ -957,12 +951,2 @@\n-void SerialHeap::generation_iterate(GenClosure* cl,\n-                                    bool old_to_young) {\n-  if (old_to_young) {\n-    cl->do_generation(_old_gen);\n-    cl->do_generation(_young_gen);\n-  } else {\n-    cl->do_generation(_young_gen);\n-    cl->do_generation(_old_gen);\n-  }\n-}\n-\n-  return _young_gen->is_maximal_no_gc() && _old_gen->is_maximal_no_gc();\n+  \/\/ We don't expand young-gen except at a GC.\n+  return _old_gen->is_maximal_no_gc();\n@@ -1064,8 +1048,0 @@\n-class GenGCSaveTopsBeforeGCClosure: public SerialHeap::GenClosure {\n- private:\n- public:\n-  void do_generation(Generation* gen) {\n-    gen->record_spaces_top();\n-  }\n-};\n-\n@@ -1074,2 +1050,2 @@\n-    GenGCSaveTopsBeforeGCClosure blk;\n-    generation_iterate(&blk, false);  \/\/ not old-to-young.\n+    _young_gen->record_spaces_top();\n+    _old_gen->record_spaces_top();\n","filename":"src\/hotspot\/share\/gc\/serial\/serialHeap.cpp","additions":9,"deletions":33,"binary":false,"changes":42,"status":"modified"},{"patch":"@@ -620,4 +620,0 @@\n-  product(int, VerifyGCLevel,     0, DIAGNOSTIC,                            \\\n-          \"Generation level at which to start +VerifyBefore\/AfterGC\")       \\\n-          range(0, 1)                                                       \\\n-                                                                            \\\n","filename":"src\/hotspot\/share\/gc\/shared\/gc_globals.hpp","additions":0,"deletions":4,"binary":false,"changes":4,"status":"modified"},{"patch":"@@ -2,1 +2,1 @@\n- * Copyright (c) 2018, 2023, Oracle and\/or its affiliates. All rights reserved.\n+ * Copyright (c) 2018, 2024, Oracle and\/or its affiliates. All rights reserved.\n@@ -44,0 +44,1 @@\n+#include \"utilities\/globalDefinitions.hpp\"\n@@ -416,0 +417,1 @@\n+    mem_zap_end_padding(mem);\n@@ -421,0 +423,18 @@\n+#ifndef PRODUCT\n+void ObjArrayAllocator::mem_zap_end_padding(HeapWord* mem) const {\n+  const size_t length_in_bytes = static_cast<size_t>(_length) << ArrayKlass::cast(_klass)->log2_element_size();\n+  const BasicType element_type = ArrayKlass::cast(_klass)->element_type();\n+  const size_t base_offset_in_bytes = arrayOopDesc::base_offset_in_bytes(element_type);\n+  const size_t size_in_bytes = _word_size * BytesPerWord;\n+\n+  const address obj_end = reinterpret_cast<address>(mem) + size_in_bytes;\n+  const address base = reinterpret_cast<address>(mem) + base_offset_in_bytes;\n+  const address elements_end = base + length_in_bytes;\n+  assert(elements_end <= obj_end, \"payload must fit in object\");\n+  if (elements_end < obj_end) {\n+    const size_t padding_in_bytes = obj_end - elements_end;\n+    Copy::fill_to_bytes(elements_end, padding_in_bytes, heapPaddingByteVal);\n+  }\n+}\n+#endif\n+\n","filename":"src\/hotspot\/share\/gc\/shared\/memAllocator.cpp","additions":21,"deletions":1,"binary":false,"changes":22,"status":"modified"},{"patch":"@@ -2,1 +2,1 @@\n- * Copyright (c) 2023, Oracle and\/or its affiliates. All rights reserved.\n+ * Copyright (c) 2023, 2024, Oracle and\/or its affiliates. All rights reserved.\n@@ -666,1 +666,1 @@\n-  JFR_ONLY(ShenandoahJFRSupport::register_jfr_type_serializers());\n+  JFR_ONLY(ShenandoahJFRSupport::register_jfr_type_serializers();)\n@@ -1009,1 +1009,5 @@\n-  ShenandoahHeapLocker locker(lock());\n+  \/\/ If we are dealing with mutator allocation, then we may need to block for safepoint.\n+  \/\/ We cannot block for safepoint for GC allocations, because there is a high chance\n+  \/\/ we are already running at safepoint or from stack watermark machinery, and we cannot\n+  \/\/ block again.\n+  ShenandoahHeapLocker locker(lock(), req.is_mutator_alloc());\n","filename":"src\/hotspot\/share\/gc\/shenandoah\/shenandoahHeap.cpp","additions":7,"deletions":3,"binary":false,"changes":10,"status":"modified"},{"patch":"@@ -2,1 +2,1 @@\n- * Copyright (c) 2019, 2023, Oracle and\/or its affiliates. All rights reserved.\n+ * Copyright (c) 2019, 2024, Oracle and\/or its affiliates. All rights reserved.\n@@ -148,0 +148,2 @@\n+  mem_zap_end_padding(mem);\n+\n","filename":"src\/hotspot\/share\/gc\/z\/zObjArrayAllocator.cpp","additions":3,"deletions":1,"binary":false,"changes":4,"status":"modified"},{"patch":"@@ -2,1 +2,1 @@\n- * Copyright (c) 1997, 2023, Oracle and\/or its affiliates. All rights reserved.\n+ * Copyright (c) 1997, 2024, Oracle and\/or its affiliates. All rights reserved.\n@@ -1994,1 +1994,1 @@\n-  product(uint, TrimNativeHeapInterval, 0, EXPERIMENTAL,                    \\\n+  product(uint, TrimNativeHeapInterval, 0,                                  \\\n","filename":"src\/hotspot\/share\/runtime\/globals.hpp","additions":2,"deletions":2,"binary":false,"changes":4,"status":"modified"},{"patch":"@@ -2,1 +2,1 @@\n- * Copyright (c) 2016, 2022, Oracle and\/or its affiliates. All rights reserved.\n+ * Copyright (c) 2016, 2024, Oracle and\/or its affiliates. All rights reserved.\n@@ -29,1 +29,1 @@\n- * @requires !vm.flightRecorder\n+ * @requires vm.flagless\n","filename":"test\/hotspot\/jtreg\/gc\/g1\/plab\/TestPLABPromotion.java","additions":2,"deletions":2,"binary":false,"changes":4,"status":"modified"},{"patch":"@@ -2,1 +2,1 @@\n- * Copyright (c) 2016, 2023, Oracle and\/or its affiliates. All rights reserved.\n+ * Copyright (c) 2016, 2024, Oracle and\/or its affiliates. All rights reserved.\n@@ -50,1 +50,0 @@\n-import java.util.stream.Collectors;\n@@ -87,4 +86,0 @@\n-\n-        public void putAll(Map<String, String> map) {\n-            map.entrySet().forEach(e -> put(e.getKey(), () -> e.getValue()));\n-        }\n@@ -143,1 +138,0 @@\n-        map.putAll(xOptFlags()); \/\/ -Xmx4g -> @requires vm.opt.x.Xmx == \"4g\" )\n@@ -726,21 +720,0 @@\n-    \/**\n-     * Parses extra options, options that start with -X excluding the\n-     * bare -X option (as it is not considered an extra option).\n-     * Ignores extra options not starting with -X\n-     *\n-     * This could be improved to handle extra options not starting\n-     * with -X as well as \"standard\" options.\n-     *\/\n-    private Map<String, String> xOptFlags() {\n-        return allFlags()\n-            .filter(s -> s.startsWith(\"-X\") && !s.startsWith(\"-XX:\") && !s.equals(\"-X\"))\n-            .map(s -> s.replaceFirst(\"-\", \"\"))\n-            .map(flag -> flag.splitWithDelimiters(\"[:0123456789]\", 2))\n-            .collect(Collectors.toMap(a -> \"vm.opt.x.\" + a[0],\n-                                      a -> (a.length == 1)\n-                                      ? \"true\" \/\/ -Xnoclassgc\n-                                      : (a[1].equals(\":\")\n-                                         ? a[2]            \/\/ [\"-XshowSettings\", \":\", \"system\"]\n-                                         : a[1] + a[2]))); \/\/ [\"-Xmx\", \"4\", \"g\"]\n-    }\n-\n","filename":"test\/jtreg-ext\/requires\/VMProps.java","additions":1,"deletions":28,"binary":false,"changes":29,"status":"modified"}]}