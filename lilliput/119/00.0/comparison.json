{"files":[{"patch":"@@ -85,0 +85,4 @@\n+      - name: 'Get GTest'\n+        id: gtest\n+        uses: .\/.github\/actions\/get-gtest\n+\n@@ -140,0 +144,1 @@\n+          --with-gtest=${{ steps.gtest.outputs.path }}\n","filename":".github\/workflows\/build-cross-compile.yml","additions":5,"deletions":0,"binary":false,"changes":5,"status":"modified"},{"patch":"@@ -360,1 +360,1 @@\n-    $(subst .sh,,$(subst .html,,$(subst .java,,$(suffix $(notdir $1)))))\n+    $(subst .jasm,,$(subst .sh,,$(subst .html,,$(subst .java,,$(suffix $(notdir $1))))))\n","filename":"make\/RunTests.gmk","additions":1,"deletions":1,"binary":false,"changes":2,"status":"modified"},{"patch":"@@ -2590,1 +2590,1 @@\n-  if (UseSVE == 0 || !VectorNode::is_invariant_vector(m)) {\n+  if (UseSVE == 0 || m->Opcode() != Op_Replicate) {\n","filename":"src\/hotspot\/cpu\/aarch64\/aarch64.ad","additions":1,"deletions":1,"binary":false,"changes":2,"status":"modified"},{"patch":"@@ -58,1 +58,1 @@\n-  __ movptr(rscratch1, (uintptr_t) StubRoutines::aarch64::method_entry_barrier());\n+  __ lea(rscratch1, RuntimeAddress(StubRoutines::method_entry_barrier()));\n","filename":"src\/hotspot\/cpu\/aarch64\/c2_CodeStubs_aarch64.cpp","additions":1,"deletions":1,"binary":false,"changes":2,"status":"modified"},{"patch":"@@ -120,1 +120,1 @@\n-          \/*release*\/ true, \/*weak*\/ false, rscratch1); \/\/ Sets flags for result\n+          \/*release*\/ true, \/*weak*\/ false, tmp3Reg); \/\/ Sets flags for result\n@@ -132,1 +132,1 @@\n-  cmp(rscratch1, rthread);\n+  cmp(tmp3Reg, rthread);\n","filename":"src\/hotspot\/cpu\/aarch64\/c2_MacroAssembler_aarch64.cpp","additions":2,"deletions":2,"binary":false,"changes":4,"status":"modified"},{"patch":"@@ -574,0 +574,13 @@\n+  \/\/ FPCR : op1 == 011\n+  \/\/        CRn == 0100\n+  \/\/        CRm == 0100\n+  \/\/        op2 == 000\n+\n+  inline void get_fpcr(Register reg) {\n+    mrs(0b11, 0b0100, 0b0100, 0b000, reg);\n+  }\n+\n+  inline void set_fpcr(Register reg) {\n+    msr(0b011, 0b0100, 0b0100, 0b000, reg);\n+  }\n+\n","filename":"src\/hotspot\/cpu\/aarch64\/macroAssembler_aarch64.hpp","additions":13,"deletions":0,"binary":false,"changes":13,"status":"modified"},{"patch":"@@ -43,0 +43,1 @@\n+#include \"oops\/resolvedMethodEntry.hpp\"\n@@ -491,1 +492,1 @@\n-  __ andw(off, off, ConstantPoolCacheEntry::field_index_mask);\n+  __ andw(off, off, ConstantPoolCache::field_index_mask);\n@@ -498,2 +499,2 @@\n-  __ ubfxw(flags, flags, ConstantPoolCacheEntry::tos_state_shift,\n-           ConstantPoolCacheEntry::tos_state_bits);\n+  __ ubfxw(flags, flags, ConstantPoolCache::tos_state_shift,\n+           ConstantPoolCache::tos_state_bits);\n@@ -2260,1 +2261,1 @@\n-void TemplateTable::resolve_cache_and_index(int byte_no,\n+void TemplateTable::resolve_cache_and_index_for_method(int byte_no,\n@@ -2262,2 +2263,1 @@\n-                                            Register index,\n-                                            size_t index_size) {\n+                                            Register index) {\n@@ -2266,0 +2266,1 @@\n+  assert(byte_no == f1_byte || byte_no == f2_byte, \"byte_no out of range\");\n@@ -2270,3 +2271,11 @@\n-\n-  assert(byte_no == f1_byte || byte_no == f2_byte, \"byte_no out of range\");\n-  __ get_cache_and_index_and_bytecode_at_bcp(Rcache, index, temp, byte_no, 1, index_size);\n+  __ load_method_entry(Rcache, index);\n+  switch(byte_no) {\n+    case f1_byte:\n+      __ lea(temp, Address(Rcache, in_bytes(ResolvedMethodEntry::bytecode1_offset())));\n+      break;\n+    case f2_byte:\n+      __ lea(temp, Address(Rcache, in_bytes(ResolvedMethodEntry::bytecode2_offset())));\n+      break;\n+  }\n+  \/\/ Load-acquire the bytecode to match store-release in InterpreterRuntime\n+  __ ldarb(temp, temp);\n@@ -2284,1 +2293,1 @@\n-  __ get_cache_and_index_at_bcp(Rcache, index, 1, index_size);\n+  __ load_method_entry(Rcache, index);\n@@ -2291,1 +2300,1 @@\n-    __ load_resolved_method_at_index(byte_no, temp, Rcache);\n+    __ ldr(temp, Address(Rcache, in_bytes(ResolvedMethodEntry::method_offset())));\n@@ -2360,9 +2369,3 @@\n-\/\/ The Rcache and index registers must be set before call\n-\/\/ n.b unlike x86 cache already includes the index offset\n-void TemplateTable::load_field_cp_cache_entry(Register obj,\n-                                              Register cache,\n-                                              Register index,\n-                                              Register off,\n-                                              Register flags,\n-                                              bool is_static = false) {\n-  assert_different_registers(cache, index, flags, off);\n+void TemplateTable::load_resolved_method_entry_special_or_static(Register cache,\n+                                                                 Register method,\n+                                                                 Register flags) {\n@@ -2370,7 +2373,3 @@\n-  ByteSize cp_base_offset = ConstantPoolCache::base_offset();\n-  \/\/ Field offset\n-  __ ldr(off, Address(cache, in_bytes(cp_base_offset +\n-                                          ConstantPoolCacheEntry::f2_offset())));\n-  \/\/ Flags\n-  __ ldrw(flags, Address(cache, in_bytes(cp_base_offset +\n-                                           ConstantPoolCacheEntry::flags_offset())));\n+  \/\/ setup registers\n+  const Register index = flags;\n+  assert_different_registers(method, cache, flags);\n@@ -2378,8 +2377,89 @@\n-  \/\/ klass overwrite register\n-  if (is_static) {\n-    __ ldr(obj, Address(cache, in_bytes(cp_base_offset +\n-                                        ConstantPoolCacheEntry::f1_offset())));\n-    const int mirror_offset = in_bytes(Klass::java_mirror_offset());\n-    __ ldr(obj, Address(obj, mirror_offset));\n-    __ resolve_oop_handle(obj, r5, rscratch2);\n-  }\n+  \/\/ determine constant pool cache field offsets\n+  resolve_cache_and_index_for_method(f1_byte, cache, index);\n+  __ load_unsigned_byte(flags, Address(cache, in_bytes(ResolvedMethodEntry::flags_offset())));\n+  __ ldr(method, Address(cache, in_bytes(ResolvedMethodEntry::method_offset())));\n+}\n+\n+void TemplateTable::load_resolved_method_entry_handle(Register cache,\n+                                                      Register method,\n+                                                      Register ref_index,\n+                                                      Register flags) {\n+  \/\/ setup registers\n+  const Register index = ref_index;\n+  assert_different_registers(method, flags);\n+  assert_different_registers(method, cache, index);\n+\n+  \/\/ determine constant pool cache field offsets\n+  resolve_cache_and_index_for_method(f1_byte, cache, index);\n+  __ load_unsigned_byte(flags, Address(cache, in_bytes(ResolvedMethodEntry::flags_offset())));\n+\n+  \/\/ maybe push appendix to arguments (just before return address)\n+  Label L_no_push;\n+  __ tbz(flags, ResolvedMethodEntry::has_appendix_shift, L_no_push);\n+  \/\/ invokehandle uses an index into the resolved references array\n+  __ load_unsigned_short(ref_index, Address(cache, in_bytes(ResolvedMethodEntry::resolved_references_index_offset())));\n+  \/\/ Push the appendix as a trailing parameter.\n+  \/\/ This must be done before we get the receiver,\n+  \/\/ since the parameter_size includes it.\n+  Register appendix = method;\n+  __ load_resolved_reference_at_index(appendix, ref_index);\n+  __ push(appendix);  \/\/ push appendix (MethodType, CallSite, etc.)\n+  __ bind(L_no_push);\n+\n+  __ ldr(method, Address(cache, in_bytes(ResolvedMethodEntry::method_offset())));\n+}\n+\n+void TemplateTable::load_resolved_method_entry_interface(Register cache,\n+                                                         Register klass,\n+                                                         Register method_or_table_index,\n+                                                         Register flags) {\n+  \/\/ setup registers\n+  const Register index = method_or_table_index;\n+  assert_different_registers(method_or_table_index, cache, flags);\n+\n+  \/\/ determine constant pool cache field offsets\n+  resolve_cache_and_index_for_method(f1_byte, cache, index);\n+  __ load_unsigned_byte(flags, Address(cache, in_bytes(ResolvedMethodEntry::flags_offset())));\n+\n+  \/\/ Invokeinterface can behave in different ways:\n+  \/\/ If calling a method from java.lang.Object, the forced virtual flag is true so the invocation will\n+  \/\/ behave like an invokevirtual call. The state of the virtual final flag will determine whether a method or\n+  \/\/ vtable index is placed in the register.\n+  \/\/ Otherwise, the registers will be populated with the klass and method.\n+\n+  Label NotVirtual; Label NotVFinal; Label Done;\n+  __ tbz(flags, ResolvedMethodEntry::is_forced_virtual_shift, NotVirtual);\n+  __ tbz(flags, ResolvedMethodEntry::is_vfinal_shift, NotVFinal);\n+  __ ldr(method_or_table_index, Address(cache, in_bytes(ResolvedMethodEntry::method_offset())));\n+  __ b(Done);\n+\n+  __ bind(NotVFinal);\n+  __ load_unsigned_short(method_or_table_index, Address(cache, in_bytes(ResolvedMethodEntry::table_index_offset())));\n+  __ b(Done);\n+\n+  __ bind(NotVirtual);\n+  __ ldr(method_or_table_index, Address(cache, in_bytes(ResolvedMethodEntry::method_offset())));\n+  __ ldr(klass, Address(cache, in_bytes(ResolvedMethodEntry::klass_offset())));\n+  __ bind(Done);\n+}\n+\n+void TemplateTable::load_resolved_method_entry_virtual(Register cache,\n+                                                       Register method_or_table_index,\n+                                                       Register flags) {\n+  \/\/ setup registers\n+  const Register index = flags;\n+  assert_different_registers(method_or_table_index, cache, flags);\n+\n+  \/\/ determine constant pool cache field offsets\n+  resolve_cache_and_index_for_method(f2_byte, cache, index);\n+  __ load_unsigned_byte(flags, Address(cache, in_bytes(ResolvedMethodEntry::flags_offset())));\n+\n+  \/\/ method_or_table_index can either be an itable index or a method depending on the virtual final flag\n+  Label NotVFinal; Label Done;\n+  __ tbz(flags, ResolvedMethodEntry::is_vfinal_shift, NotVFinal);\n+  __ ldr(method_or_table_index, Address(cache, in_bytes(ResolvedMethodEntry::method_offset())));\n+  __ b(Done);\n+\n+  __ bind(NotVFinal);\n+  __ load_unsigned_short(method_or_table_index, Address(cache, in_bytes(ResolvedMethodEntry::table_index_offset())));\n+  __ bind(Done);\n@@ -2457,38 +2537,0 @@\n-void TemplateTable::load_invoke_cp_cache_entry(int byte_no,\n-                                               Register method,\n-                                               Register itable_index,\n-                                               Register flags,\n-                                               bool is_invokevirtual,\n-                                               bool is_invokevfinal, \/*unused*\/\n-                                               bool is_invokedynamic \/*unused*\/) {\n-  \/\/ setup registers\n-  const Register cache = rscratch2;\n-  const Register index = r4;\n-  assert_different_registers(method, flags);\n-  assert_different_registers(method, cache, index);\n-  assert_different_registers(itable_index, flags);\n-  assert_different_registers(itable_index, cache, index);\n-  \/\/ determine constant pool cache field offsets\n-  assert(is_invokevirtual == (byte_no == f2_byte), \"is_invokevirtual flag redundant\");\n-  const int method_offset = in_bytes(\n-    ConstantPoolCache::base_offset() +\n-      (is_invokevirtual\n-       ? ConstantPoolCacheEntry::f2_offset()\n-       : ConstantPoolCacheEntry::f1_offset()));\n-  const int flags_offset = in_bytes(ConstantPoolCache::base_offset() +\n-                                    ConstantPoolCacheEntry::flags_offset());\n-  \/\/ access constant pool cache fields\n-  const int index_offset = in_bytes(ConstantPoolCache::base_offset() +\n-                                    ConstantPoolCacheEntry::f2_offset());\n-\n-  size_t index_size = sizeof(u2);\n-  resolve_cache_and_index(byte_no, cache, index, index_size);\n-  __ ldr(method, Address(cache, method_offset));\n-\n-  if (itable_index != noreg) {\n-    __ ldr(itable_index, Address(cache, index_offset));\n-  }\n-  __ ldrw(flags, Address(cache, flags_offset));\n-}\n-\n-\n@@ -3245,7 +3287,2 @@\n-void TemplateTable::prepare_invoke(int byte_no,\n-                                   Register method, \/\/ linked method (or i-klass)\n-                                   Register index,  \/\/ itable index, MethodType, etc.\n-                                   Register recv,   \/\/ if caller wants to see it\n-                                   Register flags   \/\/ if caller wants to test it\n-                                   ) {\n-  \/\/ determine flags\n+void TemplateTable::prepare_invoke(Register cache, Register recv) {\n+\n@@ -3253,16 +3290,1 @@\n-  const bool is_invokeinterface  = code == Bytecodes::_invokeinterface;\n-  const bool is_invokedynamic    = code == Bytecodes::_invokedynamic;\n-  const bool is_invokehandle     = code == Bytecodes::_invokehandle;\n-  const bool is_invokevirtual    = code == Bytecodes::_invokevirtual;\n-  const bool is_invokespecial    = code == Bytecodes::_invokespecial;\n-  const bool load_receiver       = (recv  != noreg);\n-  const bool save_flags          = (flags != noreg);\n-  assert(load_receiver == (code != Bytecodes::_invokestatic && code != Bytecodes::_invokedynamic), \"\");\n-  assert(save_flags    == (is_invokeinterface || is_invokevirtual), \"need flags for vfinal\");\n-  assert(flags == noreg || flags == r3, \"\");\n-  assert(recv  == noreg || recv  == r2, \"\");\n-\n-  \/\/ setup registers & access constant pool cache\n-  if (recv  == noreg)  recv  = r2;\n-  if (flags == noreg)  flags = r3;\n-  assert_different_registers(method, index, recv, flags);\n+  const bool load_receiver       = (code != Bytecodes::_invokestatic) && (code != Bytecodes::_invokedynamic);\n@@ -3273,16 +3295,2 @@\n-  load_invoke_cp_cache_entry(byte_no, method, index, flags, is_invokevirtual, false, is_invokedynamic);\n-\n-  \/\/ maybe push appendix to arguments (just before return address)\n-  if (is_invokehandle) {\n-    Label L_no_push;\n-    __ tbz(flags, ConstantPoolCacheEntry::has_appendix_shift, L_no_push);\n-    \/\/ Push the appendix as a trailing parameter.\n-    \/\/ This must be done before we get the receiver,\n-    \/\/ since the parameter_size includes it.\n-    __ push(r19);\n-    __ mov(r19, index);\n-    __ load_resolved_reference_at_index(index, r19);\n-    __ pop(r19);\n-    __ push(index);  \/\/ push appendix (MethodType, CallSite, etc.)\n-    __ bind(L_no_push);\n-  }\n+  \/\/ Load TOS state for later\n+  __ load_unsigned_byte(rscratch2, Address(cache, in_bytes(ResolvedMethodEntry::type_offset())));\n@@ -3292,7 +3300,2 @@\n-    __ andw(recv, flags, ConstantPoolCacheEntry::parameter_size_mask);\n-    \/\/ FIXME -- is this actually correct? looks like it should be 2\n-    \/\/ const int no_return_pc_pushed_yet = -1;  \/\/ argument slot correction before we push return address\n-    \/\/ const int receiver_is_at_end      = -1;  \/\/ back off one slot to get receiver\n-    \/\/ Address recv_addr = __ argument_address(recv, no_return_pc_pushed_yet + receiver_is_at_end);\n-    \/\/ __ movptr(recv, recv_addr);\n-    __ add(rscratch1, esp, recv, ext::uxtx, 3); \/\/ FIXME: uxtb here?\n+    __ load_unsigned_short(recv, Address(cache, in_bytes(ResolvedMethodEntry::num_parameters_offset())));\n+    __ add(rscratch1, esp, recv, ext::uxtx, 3);\n@@ -3303,4 +3306,0 @@\n-  \/\/ compute return type\n-  \/\/ x86 uses a shift and mask or wings it with a shift plus assert\n-  \/\/ the mask is not needed. aarch64 just uses bitfield extract\n-  __ ubfxw(rscratch2, flags, ConstantPoolCacheEntry::tos_state_shift,  ConstantPoolCacheEntry::tos_state_bits);\n@@ -3324,1 +3323,1 @@\n-  __ tbz(flags, ConstantPoolCacheEntry::is_vfinal_shift, notFinal);\n+  __ tbz(flags, ResolvedMethodEntry::is_vfinal_shift, notFinal);\n@@ -3363,1 +3362,4 @@\n-  prepare_invoke(byte_no, rmethod, noreg, r2, r3);\n+  load_resolved_method_entry_virtual(r2,      \/\/ ResolvedMethodEntry*\n+                                     rmethod, \/\/ Method* or itable index\n+                                     r3);     \/\/ flags\n+  prepare_invoke(r2, r2); \/\/ recv\n@@ -3377,2 +3379,4 @@\n-  prepare_invoke(byte_no, rmethod, noreg,  \/\/ get f1 Method*\n-                 r2);  \/\/ get receiver also for null check\n+  load_resolved_method_entry_special_or_static(r2,      \/\/ ResolvedMethodEntry*\n+                                               rmethod, \/\/ Method*\n+                                               r3);     \/\/ flags\n+  prepare_invoke(r2, r2);  \/\/ get receiver also for null check\n@@ -3392,1 +3396,5 @@\n-  prepare_invoke(byte_no, rmethod);  \/\/ get f1 Method*\n+  load_resolved_method_entry_special_or_static(r2,      \/\/ ResolvedMethodEntry*\n+                                               rmethod, \/\/ Method*\n+                                               r3);     \/\/ flags\n+  prepare_invoke(r2, r2);  \/\/ get receiver also for null check\n+\n@@ -3408,2 +3416,5 @@\n-  prepare_invoke(byte_no, r0, rmethod,  \/\/ get f1 Klass*, f2 Method*\n-                 r2, r3); \/\/ recv, flags\n+  load_resolved_method_entry_interface(r2,      \/\/ ResolvedMethodEntry*\n+                                       r0,      \/\/ Klass*\n+                                       rmethod, \/\/ Method* or itable\/vtable index\n+                                       r3);     \/\/ flags\n+  prepare_invoke(r2, r2); \/\/ receiver\n@@ -3422,1 +3433,1 @@\n-  __ tbz(r3, ConstantPoolCacheEntry::is_forced_virtual_shift, notObjectMethod);\n+  __ tbz(r3, ResolvedMethodEntry::is_forced_virtual_shift, notObjectMethod);\n@@ -3431,1 +3442,1 @@\n-  __ tbz(r3, ConstantPoolCacheEntry::is_vfinal_shift, notVFinal);\n+  __ tbz(r3, ResolvedMethodEntry::is_vfinal_shift, notVFinal);\n@@ -3528,1 +3539,6 @@\n-  prepare_invoke(byte_no, rmethod, r0, r2);\n+  load_resolved_method_entry_handle(r2,      \/\/ ResolvedMethodEntry*\n+                                    rmethod, \/\/ Method*\n+                                    r0,      \/\/ Resolved reference\n+                                    r3);     \/\/ flags\n+  prepare_invoke(r2, r2);\n+\n@@ -3550,1 +3566,1 @@\n-  \/\/ rmethod: MH.linkToCallSite method (from f2)\n+  \/\/ rmethod: MH.linkToCallSite method\n@@ -3552,1 +3568,1 @@\n-  \/\/ Note:  r0_callsite is already pushed by prepare_invoke\n+  \/\/ Note:  r0_callsite is already pushed\n","filename":"src\/hotspot\/cpu\/aarch64\/templateTable_aarch64.cpp","additions":151,"deletions":135,"binary":false,"changes":286,"status":"modified"},{"patch":"@@ -697,2 +697,0 @@\n-                                         VMRegPair *regs2,\n-  assert(regs2 == nullptr, \"not needed on riscv\");\n@@ -1348,1 +1346,1 @@\n-  int out_arg_slots = c_calling_convention(out_sig_bt, out_regs, nullptr, total_c_args);\n+  int out_arg_slots = c_calling_convention(out_sig_bt, out_regs, total_c_args);\n@@ -1653,0 +1651,1 @@\n+  const Register lock_tmp = x31;  \/\/ Temporary used by lightweight_lock\/unlock\n@@ -1704,1 +1703,1 @@\n-      __ lightweight_lock(obj_reg, swap_reg, tmp, t0, slow_path_lock);\n+      __ lightweight_lock(obj_reg, swap_reg, tmp, lock_tmp, slow_path_lock);\n@@ -1832,1 +1831,1 @@\n-      __ lightweight_unlock(obj_reg, old_hdr, swap_reg, t0, slow_path_unlock);\n+      __ lightweight_unlock(obj_reg, old_hdr, swap_reg, lock_tmp, slow_path_unlock);\n","filename":"src\/hotspot\/cpu\/riscv\/sharedRuntime_riscv.cpp","additions":4,"deletions":5,"binary":false,"changes":9,"status":"modified"},{"patch":"@@ -72,1 +72,1 @@\n-  __ call(RuntimeAddress(StubRoutines::x86::method_entry_barrier()));\n+  __ call(RuntimeAddress(StubRoutines::method_entry_barrier()));\n","filename":"src\/hotspot\/cpu\/x86\/c2_CodeStubs_x86.cpp","additions":1,"deletions":1,"binary":false,"changes":2,"status":"modified"},{"patch":"@@ -3856,2 +3856,0 @@\n-    Register tmp3_aliased = len;\n-\n@@ -3861,2 +3859,2 @@\n-    andl(tmp1, 64 - 1);   \/\/ tail count (in chars) 0x3F\n-    andl(len, ~(64 - 1));    \/\/ vector count (in chars)\n+    andl(tmp1, 0x0000003f); \/\/ tail count (in chars) 0x3F\n+    andl(len,  0xffffffc0); \/\/ vector count (in chars)\n@@ -3882,0 +3880,2 @@\n+\n+    \/\/ check the tail for absense of negatives\n@@ -3884,4 +3884,7 @@\n-    mov64(tmp3_aliased, 0xFFFFFFFFFFFFFFFF);\n-    shlxq(tmp3_aliased, tmp3_aliased, tmp1);\n-    notq(tmp3_aliased);\n-    kmovql(mask2, tmp3_aliased);\n+    {\n+      Register tmp3_aliased = len;\n+      mov64(tmp3_aliased, 0xFFFFFFFFFFFFFFFF);\n+      shlxq(tmp3_aliased, tmp3_aliased, tmp1);\n+      notq(tmp3_aliased);\n+      kmovql(mask2, tmp3_aliased);\n+    }\n@@ -3919,0 +3922,5 @@\n+    \/\/ do a full check for negative registers in the tail\n+    movl(len, tmp1); \/\/ tmp1 holds low 6-bit from original len;\n+                     \/\/ ary1 already pointing to the right place\n+    jmpb(TAIL_START);\n+\n@@ -3920,1 +3928,1 @@\n-    \/\/ At least one byte in the last 64 bytes is negative.\n+    \/\/ At least one byte in the last 64 byte block was negative.\n","filename":"src\/hotspot\/cpu\/x86\/c2_MacroAssembler_x86.cpp","additions":17,"deletions":9,"binary":false,"changes":26,"status":"modified"},{"patch":"@@ -1342,0 +1342,4 @@\n+#ifdef _LP64\n+  \/\/ Needs full 64-bit immediate for later patching.\n+  mov64(rax, (intptr_t)Universe::non_oop_word());\n+#else\n@@ -1343,0 +1347,1 @@\n+#endif\n@@ -1869,0 +1874,86 @@\n+void MacroAssembler::cvtss2sd(XMMRegister dst, XMMRegister src) {\n+  if ((UseAVX > 0) && (dst != src)) {\n+    xorpd(dst, dst);\n+  }\n+  Assembler::cvtss2sd(dst, src);\n+}\n+\n+void MacroAssembler::cvtss2sd(XMMRegister dst, Address src) {\n+  if (UseAVX > 0) {\n+    xorpd(dst, dst);\n+  }\n+  Assembler::cvtss2sd(dst, src);\n+}\n+\n+void MacroAssembler::cvtsd2ss(XMMRegister dst, XMMRegister src) {\n+  if ((UseAVX > 0) && (dst != src)) {\n+    xorps(dst, dst);\n+  }\n+  Assembler::cvtsd2ss(dst, src);\n+}\n+\n+void MacroAssembler::cvtsd2ss(XMMRegister dst, Address src) {\n+  if (UseAVX > 0) {\n+    xorps(dst, dst);\n+  }\n+  Assembler::cvtsd2ss(dst, src);\n+}\n+\n+void MacroAssembler::cvtsi2sdl(XMMRegister dst, Register src) {\n+  if (UseAVX > 0) {\n+    xorpd(dst, dst);\n+  }\n+  Assembler::cvtsi2sdl(dst, src);\n+}\n+\n+void MacroAssembler::cvtsi2sdl(XMMRegister dst, Address src) {\n+  if (UseAVX > 0) {\n+    xorpd(dst, dst);\n+  }\n+  Assembler::cvtsi2sdl(dst, src);\n+}\n+\n+void MacroAssembler::cvtsi2ssl(XMMRegister dst, Register src) {\n+  if (UseAVX > 0) {\n+    xorps(dst, dst);\n+  }\n+  Assembler::cvtsi2ssl(dst, src);\n+}\n+\n+void MacroAssembler::cvtsi2ssl(XMMRegister dst, Address src) {\n+  if (UseAVX > 0) {\n+    xorps(dst, dst);\n+  }\n+  Assembler::cvtsi2ssl(dst, src);\n+}\n+\n+#ifdef _LP64\n+void MacroAssembler::cvtsi2sdq(XMMRegister dst, Register src) {\n+  if (UseAVX > 0) {\n+    xorpd(dst, dst);\n+  }\n+  Assembler::cvtsi2sdq(dst, src);\n+}\n+\n+void MacroAssembler::cvtsi2sdq(XMMRegister dst, Address src) {\n+  if (UseAVX > 0) {\n+    xorpd(dst, dst);\n+  }\n+  Assembler::cvtsi2sdq(dst, src);\n+}\n+\n+void MacroAssembler::cvtsi2ssq(XMMRegister dst, Register src) {\n+  if (UseAVX > 0) {\n+    xorps(dst, dst);\n+  }\n+  Assembler::cvtsi2ssq(dst, src);\n+}\n+\n+void MacroAssembler::cvtsi2ssq(XMMRegister dst, Address src) {\n+  if (UseAVX > 0) {\n+    xorps(dst, dst);\n+  }\n+  Assembler::cvtsi2ssq(dst, src);\n+}\n+#endif  \/\/ _LP64\n+\n@@ -2565,1 +2656,9 @@\n-  LP64_ONLY(mov64(dst, src)) NOT_LP64(movl(dst, src));\n+#ifdef _LP64\n+  if (is_simm32(src)) {\n+    movq(dst, checked_cast<int32_t>(src));\n+  } else {\n+    mov64(dst, src);\n+  }\n+#else\n+  movl(dst, src);\n+#endif\n@@ -9340,0 +9439,11 @@\n+void MacroAssembler::vpshufb(XMMRegister dst, XMMRegister nds, AddressLiteral src, int vector_len, Register rscratch) {\n+  assert(rscratch != noreg || always_reachable(src), \"missing\");\n+\n+  if (reachable(src)) {\n+    vpshufb(dst, nds, as_Address(src), vector_len);\n+  } else {\n+    lea(rscratch, src);\n+    vpshufb(dst, nds, Address(rscratch, 0), vector_len);\n+  }\n+}\n+\n","filename":"src\/hotspot\/cpu\/x86\/macroAssembler_x86.cpp","additions":111,"deletions":1,"binary":false,"changes":112,"status":"modified"},{"patch":"@@ -814,0 +814,17 @@\n+\n+  \/\/ cvt instructions\n+  void cvtss2sd(XMMRegister dst, XMMRegister src);\n+  void cvtss2sd(XMMRegister dst, Address src);\n+  void cvtsd2ss(XMMRegister dst, XMMRegister src);\n+  void cvtsd2ss(XMMRegister dst, Address src);\n+  void cvtsi2sdl(XMMRegister dst, Register src);\n+  void cvtsi2sdl(XMMRegister dst, Address src);\n+  void cvtsi2ssl(XMMRegister dst, Register src);\n+  void cvtsi2ssl(XMMRegister dst, Address src);\n+#ifdef _LP64\n+  void cvtsi2sdq(XMMRegister dst, Register src);\n+  void cvtsi2sdq(XMMRegister dst, Address src);\n+  void cvtsi2ssq(XMMRegister dst, Register src);\n+  void cvtsi2ssq(XMMRegister dst, Address src);\n+#endif\n+\n@@ -1809,0 +1826,3 @@\n+  using Assembler::vpshufb;\n+  void vpshufb(XMMRegister dst, XMMRegister nds, AddressLiteral src, int vector_len, Register rscratch = noreg);\n+\n","filename":"src\/hotspot\/cpu\/x86\/macroAssembler_x86.hpp","additions":20,"deletions":0,"binary":false,"changes":20,"status":"modified"},{"patch":"@@ -42,0 +42,1 @@\n+#include \"oops\/resolvedMethodEntry.hpp\"\n@@ -527,1 +528,1 @@\n-  __ andl(off, ConstantPoolCacheEntry::field_index_mask);\n+  __ andl(off, ConstantPoolCache::field_index_mask);\n@@ -531,2 +532,2 @@\n-  __ shrl(flags, ConstantPoolCacheEntry::tos_state_shift);\n-  __ andl(flags, ConstantPoolCacheEntry::tos_state_mask);\n+  __ shrl(flags, ConstantPoolCache::tos_state_shift);\n+  __ andl(flags, ConstantPoolCache::tos_state_mask);\n@@ -2656,4 +2657,3 @@\n-void TemplateTable::resolve_cache_and_index(int byte_no,\n-                                            Register cache,\n-                                            Register index,\n-                                            size_t index_size) {\n+void TemplateTable::resolve_cache_and_index_for_method(int byte_no,\n+                                                       Register cache,\n+                                                       Register index) {\n@@ -2669,1 +2669,12 @@\n-  __ get_cache_and_index_and_bytecode_at_bcp(cache, index, temp, byte_no, 1, index_size);\n+\n+  __ load_method_entry(cache, index);\n+  switch(byte_no) {\n+    case f1_byte:\n+      __ load_unsigned_byte(temp, Address(cache, in_bytes(ResolvedMethodEntry::bytecode1_offset())));\n+      break;\n+    case f2_byte:\n+      __ load_unsigned_byte(temp, Address(cache, in_bytes(ResolvedMethodEntry::bytecode2_offset())));\n+      break;\n+    default:\n+      ShouldNotReachHere();\n+  }\n@@ -2680,1 +2691,1 @@\n-  __ get_cache_and_index_at_bcp(cache, index, 1, index_size);\n+  __ load_method_entry(cache, index);\n@@ -2691,1 +2702,1 @@\n-    __ load_resolved_method_at_index(byte_no, method, cache, index);\n+    __ movptr(method, Address(cache, in_bytes(ResolvedMethodEntry::method_offset())));\n@@ -2759,30 +2770,0 @@\n-\/\/ The cache and index registers must be set before call\n-void TemplateTable::load_field_cp_cache_entry(Register obj,\n-                                              Register cache,\n-                                              Register index,\n-                                              Register off,\n-                                              Register flags,\n-                                              bool is_static = false) {\n-  assert_different_registers(cache, index, flags, off);\n-\n-  ByteSize cp_base_offset = ConstantPoolCache::base_offset();\n-  \/\/ Field offset\n-  __ movptr(off, Address(cache, index, Address::times_ptr,\n-                         in_bytes(cp_base_offset +\n-                                  ConstantPoolCacheEntry::f2_offset())));\n-  \/\/ Flags\n-  __ movl(flags, Address(cache, index, Address::times_ptr,\n-                         in_bytes(cp_base_offset +\n-                                  ConstantPoolCacheEntry::flags_offset())));\n-\n-  \/\/ klass overwrite register\n-  if (is_static) {\n-    __ movptr(obj, Address(cache, index, Address::times_ptr,\n-                           in_bytes(cp_base_offset +\n-                                    ConstantPoolCacheEntry::f1_offset())));\n-    const int mirror_offset = in_bytes(Klass::java_mirror_offset());\n-    __ movptr(obj, Address(obj, mirror_offset));\n-    __ resolve_oop_handle(obj, rscratch2);\n-  }\n-}\n-\n@@ -2857,1 +2838,15 @@\n-void TemplateTable::load_invoke_cp_cache_entry(int byte_no,\n+void TemplateTable::load_resolved_method_entry_special_or_static(Register cache,\n+                                                                 Register method,\n+                                                                 Register flags) {\n+  \/\/ setup registers\n+  const Register index = rdx;\n+  assert_different_registers(cache, index);\n+  assert_different_registers(method, cache, flags);\n+\n+  \/\/ determine constant pool cache field offsets\n+  resolve_cache_and_index_for_method(f1_byte, cache, index);\n+  __ load_unsigned_byte(flags, Address(cache, in_bytes(ResolvedMethodEntry::flags_offset())));\n+  __ movptr(method, Address(cache, in_bytes(ResolvedMethodEntry::method_offset())));\n+}\n+\n+void TemplateTable::load_resolved_method_entry_handle(Register cache,\n@@ -2859,5 +2854,2 @@\n-                                               Register itable_index,\n-                                               Register flags,\n-                                               bool is_invokevirtual,\n-                                               bool is_invokevfinal, \/*unused*\/\n-                                               bool is_invokedynamic \/*unused*\/) {\n+                                               Register ref_index,\n+                                               Register flags) {\n@@ -2865,5 +2857,3 @@\n-  const Register cache = rcx;\n-  assert_different_registers(method, flags);\n-  assert_different_registers(method, cache, index);\n-  assert_different_registers(itable_index, flags);\n-  assert_different_registers(itable_index, cache, index);\n+  assert_different_registers(cache, index);\n+  assert_different_registers(cache, method, ref_index, flags);\n+\n@@ -2872,6 +2862,2 @@\n-  assert(is_invokevirtual == (byte_no == f2_byte), \"is_invokevirtual flag redundant\");\n-  const int flags_offset = in_bytes(ConstantPoolCache::base_offset() +\n-                                    ConstantPoolCacheEntry::flags_offset());\n-  \/\/ access constant pool cache fields\n-  const int index_offset = in_bytes(ConstantPoolCache::base_offset() +\n-                                    ConstantPoolCacheEntry::f2_offset());\n+  resolve_cache_and_index_for_method(f1_byte, cache, index);\n+  __ load_unsigned_byte(flags, Address(cache, in_bytes(ResolvedMethodEntry::flags_offset())));\n@@ -2879,3 +2865,13 @@\n-  size_t index_size = sizeof(u2);\n-  resolve_cache_and_index(byte_no, cache, index, index_size);\n-  __ load_resolved_method_at_index(byte_no, method, cache, index);\n+  \/\/ Maybe push appendix\n+  Label L_no_push;\n+  __ testl(flags, (1 << ResolvedMethodEntry::has_appendix_shift));\n+  __ jcc(Assembler::zero, L_no_push);\n+  \/\/ invokehandle uses an index into the resolved references array\n+  __ load_unsigned_short(ref_index, Address(cache, in_bytes(ResolvedMethodEntry::resolved_references_index_offset())));\n+  \/\/ Push the appendix as a trailing parameter.\n+  \/\/ This must be done before we get the receiver,\n+  \/\/ since the parameter_size includes it.\n+  Register appendix = method;\n+  __ load_resolved_reference_at_index(appendix, ref_index);\n+  __ push(appendix);  \/\/ push appendix (MethodType, CallSite, etc.)\n+  __ bind(L_no_push);\n@@ -2883,5 +2879,60 @@\n-  if (itable_index != noreg) {\n-    \/\/ pick up itable or appendix index from f2 also:\n-    __ movptr(itable_index, Address(cache, index, Address::times_ptr, index_offset));\n-  }\n-  __ movl(flags, Address(cache, index, Address::times_ptr, flags_offset));\n+  __ movptr(method, Address(cache, in_bytes(ResolvedMethodEntry::method_offset())));\n+}\n+\n+void TemplateTable::load_resolved_method_entry_interface(Register cache,\n+                                                         Register klass,\n+                                                         Register method_or_table_index,\n+                                                         Register flags) {\n+  \/\/ setup registers\n+  const Register index = rdx;\n+  assert_different_registers(cache, klass, method_or_table_index, flags);\n+\n+  \/\/ determine constant pool cache field offsets\n+  resolve_cache_and_index_for_method(f1_byte, cache, index);\n+  __ load_unsigned_byte(flags, Address(cache, in_bytes(ResolvedMethodEntry::flags_offset())));\n+\n+  \/\/ Invokeinterface can behave in different ways:\n+  \/\/ If calling a method from java.lang.Object, the forced virtual flag is true so the invocation will\n+  \/\/ behave like an invokevirtual call. The state of the virtual final flag will determine whether a method or\n+  \/\/ vtable index is placed in the register.\n+  \/\/ Otherwise, the registers will be populated with the klass and method.\n+\n+  Label NotVirtual; Label NotVFinal; Label Done;\n+  __ testl(flags, 1 << ResolvedMethodEntry::is_forced_virtual_shift);\n+  __ jcc(Assembler::zero, NotVirtual);\n+  __ testl(flags, (1 << ResolvedMethodEntry::is_vfinal_shift));\n+  __ jcc(Assembler::zero, NotVFinal);\n+  __ movptr(method_or_table_index, Address(cache, in_bytes(ResolvedMethodEntry::method_offset())));\n+  __ jmp(Done);\n+\n+  __ bind(NotVFinal);\n+  __ load_unsigned_short(method_or_table_index, Address(cache, in_bytes(ResolvedMethodEntry::table_index_offset())));\n+  __ jmp(Done);\n+\n+  __ bind(NotVirtual);\n+  __ movptr(method_or_table_index, Address(cache, in_bytes(ResolvedMethodEntry::method_offset())));\n+  __ movptr(klass, Address(cache, in_bytes(ResolvedMethodEntry::klass_offset())));\n+  __ bind(Done);\n+}\n+\n+void TemplateTable::load_resolved_method_entry_virtual(Register cache,\n+                                                       Register method_or_table_index,\n+                                                       Register flags) {\n+  \/\/ setup registers\n+  const Register index = rdx;\n+  assert_different_registers(index, cache);\n+  assert_different_registers(method_or_table_index, cache, flags);\n+\n+  \/\/ determine constant pool cache field offsets\n+  resolve_cache_and_index_for_method(f2_byte, cache, index);\n+  __ load_unsigned_byte(flags, Address(cache, in_bytes(ResolvedMethodEntry::flags_offset())));\n+\n+  \/\/ method_or_table_index can either be an itable index or a method depending on the virtual final flag\n+  Label isVFinal; Label Done;\n+  __ testl(flags, (1 << ResolvedMethodEntry::is_vfinal_shift));\n+  __ jcc(Assembler::notZero, isVFinal);\n+  __ load_unsigned_short(method_or_table_index, Address(cache, in_bytes(ResolvedMethodEntry::table_index_offset())));\n+  __ jmp(Done);\n+  __ bind(isVFinal);\n+  __ movptr(method_or_table_index, Address(cache, in_bytes(ResolvedMethodEntry::method_offset())));\n+  __ bind(Done);\n@@ -3634,6 +3685,1 @@\n-void TemplateTable::prepare_invoke(int byte_no,\n-                                   Register method,  \/\/ linked method (or i-klass)\n-                                   Register index,   \/\/ itable index, MethodType, etc.\n-                                   Register recv,    \/\/ if caller wants to see it\n-                                   Register flags    \/\/ if caller wants to test it\n-                                   ) {\n+void TemplateTable::prepare_invoke(Register cache, Register recv, Register flags) {\n@@ -3642,16 +3688,2 @@\n-  const bool is_invokeinterface  = code == Bytecodes::_invokeinterface;\n-  const bool is_invokedynamic    = code == Bytecodes::_invokedynamic;\n-  const bool is_invokehandle     = code == Bytecodes::_invokehandle;\n-  const bool is_invokevirtual    = code == Bytecodes::_invokevirtual;\n-  const bool is_invokespecial    = code == Bytecodes::_invokespecial;\n-  const bool load_receiver       = (recv  != noreg);\n-  const bool save_flags          = (flags != noreg);\n-  assert(load_receiver == (code != Bytecodes::_invokestatic && code != Bytecodes::_invokedynamic), \"\");\n-  assert(save_flags    == (is_invokeinterface || is_invokevirtual), \"need flags for vfinal\");\n-  assert(flags == noreg || flags == rdx, \"\");\n-  assert(recv  == noreg || recv  == rcx, \"\");\n-\n-  \/\/ setup registers & access constant pool cache\n-  if (recv  == noreg)  recv  = rcx;\n-  if (flags == noreg)  flags = rdx;\n-  assert_different_registers(method, index, recv, flags);\n+  const bool load_receiver       = (code != Bytecodes::_invokestatic) && (code != Bytecodes::_invokedynamic);\n+  assert_different_registers(recv, flags);\n@@ -3662,17 +3694,3 @@\n-  load_invoke_cp_cache_entry(byte_no, method, index, flags, is_invokevirtual, false, is_invokedynamic);\n-\n-  \/\/ maybe push appendix to arguments (just before return address)\n-  if (is_invokehandle) {\n-    Label L_no_push;\n-    __ testl(flags, (1 << ConstantPoolCacheEntry::has_appendix_shift));\n-    __ jcc(Assembler::zero, L_no_push);\n-    \/\/ Push the appendix as a trailing parameter.\n-    \/\/ This must be done before we get the receiver,\n-    \/\/ since the parameter_size includes it.\n-    __ push(rbx);\n-    __ mov(rbx, index);\n-    __ load_resolved_reference_at_index(index, rbx);\n-    __ pop(rbx);\n-    __ push(index);  \/\/ push appendix (MethodType, CallSite, etc.)\n-    __ bind(L_no_push);\n-  }\n+  \/\/ Save flags and load TOS\n+  __ movl(rbcp, flags);\n+  __ load_unsigned_byte(flags, Address(cache, in_bytes(ResolvedMethodEntry::type_offset())));\n@@ -3683,2 +3701,1 @@\n-    __ movl(recv, flags);\n-    __ andl(recv, ConstantPoolCacheEntry::parameter_size_mask);\n+    __ load_unsigned_short(recv, Address(cache, in_bytes(ResolvedMethodEntry::num_parameters_offset())));\n@@ -3692,8 +3709,0 @@\n-  if (save_flags) {\n-    __ movl(rbcp, flags);\n-  }\n-\n-  \/\/ compute return type\n-  __ shrl(flags, ConstantPoolCacheEntry::tos_state_shift);\n-  \/\/ Make sure we don't need to mask flags after the above shift\n-  ConstantPoolCacheEntry::verify_tos_state_shift();\n@@ -3715,1 +3724,1 @@\n-  \/\/ Restore flags value from the constant pool cache, and restore rsi\n+  \/\/ Restore flags value from the constant pool cache entry, and restore rsi\n@@ -3717,4 +3726,2 @@\n-  if (save_flags) {\n-    __ movl(flags, rbcp);\n-    __ restore_bcp();\n-  }\n+  __ movl(flags, rbcp);\n+  __ restore_bcp();\n@@ -3734,1 +3741,1 @@\n-  __ andl(rax, (1 << ConstantPoolCacheEntry::is_vfinal_shift));\n+  __ andl(rax, (1 << ResolvedMethodEntry::is_vfinal_shift));\n@@ -3770,4 +3777,7 @@\n-  prepare_invoke(byte_no,\n-                 rbx,    \/\/ method or vtable index\n-                 noreg,  \/\/ unused itable index\n-                 rcx, rdx); \/\/ recv, flags\n+\n+  load_resolved_method_entry_virtual(rcx,  \/\/ ResolvedMethodEntry*\n+                                     rbx,  \/\/ Method or itable index\n+                                     rdx); \/\/ Flags\n+  prepare_invoke(rcx,  \/\/ ResolvedMethodEntry*\n+                 rcx,  \/\/ Receiver\n+                 rdx); \/\/ flags\n@@ -3778,1 +3788,0 @@\n-\n@@ -3785,2 +3794,8 @@\n-  prepare_invoke(byte_no, rbx, noreg,  \/\/ get f1 Method*\n-                 rcx);  \/\/ get receiver also for null check\n+\n+  load_resolved_method_entry_special_or_static(rcx,  \/\/ ResolvedMethodEntry*\n+                                               rbx,  \/\/ Method*\n+                                               rdx); \/\/ flags\n+  prepare_invoke(rcx,\n+                 rcx,  \/\/ get receiver also for null check\n+                 rdx); \/\/ flags\n+\n@@ -3798,1 +3813,7 @@\n-  prepare_invoke(byte_no, rbx);  \/\/ get f1 Method*\n+\n+  load_resolved_method_entry_special_or_static(rcx, \/\/ ResolvedMethodEntry*\n+                                               rbx, \/\/ Method*\n+                                               rdx  \/\/ flags\n+                                               );\n+  prepare_invoke(rcx, rcx, rdx);  \/\/ cache and flags\n+\n@@ -3816,6 +3837,5 @@\n-  prepare_invoke(byte_no, rax, rbx,  \/\/ get f1 Klass*, f2 Method*\n-                 rcx, rdx); \/\/ recv, flags\n-  \/\/ rax: reference klass (from f1) if interface method\n-  \/\/ rbx: method (from f2)\n-  \/\/ rcx: receiver\n-  \/\/ rdx: flags\n+  load_resolved_method_entry_interface(rcx,  \/\/ ResolvedMethodEntry*\n+                                       rax,  \/\/ Klass*\n+                                       rbx,  \/\/ Method* or itable\/vtable index\n+                                       rdx); \/\/ flags\n+  prepare_invoke(rcx, rcx, rdx); \/\/ receiver, flags\n@@ -3831,1 +3851,1 @@\n-  __ andl(rlocals, (1 << ConstantPoolCacheEntry::is_forced_virtual_shift));\n+  __ andl(rlocals, (1 << ResolvedMethodEntry::is_forced_virtual_shift));\n@@ -3833,0 +3853,1 @@\n+\n@@ -3843,1 +3864,1 @@\n-  __ andl(rlocals, (1 << ConstantPoolCacheEntry::is_vfinal_shift));\n+  __ andl(rlocals, (1 << ResolvedMethodEntry::is_vfinal_shift));\n@@ -3964,1 +3985,3 @@\n-  prepare_invoke(byte_no, rbx_method, rax_mtype, rcx_recv);\n+  load_resolved_method_entry_handle(rcx, rbx_method, rax_mtype, rdx_flags);\n+  prepare_invoke(rcx, rcx_recv, rdx_flags);\n+\n@@ -3970,1 +3993,1 @@\n-  \/\/ rbx: MH.invokeExact_MT method (from f2)\n+  \/\/ rbx: MH.invokeExact_MT method\n@@ -3972,1 +3995,1 @@\n-  \/\/ Note:  rax_mtype is already pushed (if necessary) by prepare_invoke\n+  \/\/ Note:  rax_mtype is already pushed (if necessary)\n@@ -3989,2 +4012,2 @@\n-  \/\/ rax: CallSite object (from cpool->resolved_references[f1])\n-  \/\/ rbx: MH.linkToCallSite method (from f2)\n+  \/\/ rax: CallSite object (from cpool->resolved_references[])\n+  \/\/ rbx: MH.linkToCallSite method\n@@ -3992,1 +4015,1 @@\n-  \/\/ Note:  rax_callsite is already pushed by prepare_invoke\n+  \/\/ Note:  rax_callsite is already pushed\n","filename":"src\/hotspot\/cpu\/x86\/templateTable_x86.cpp","additions":165,"deletions":142,"binary":false,"changes":307,"status":"modified"},{"patch":"@@ -521,181 +521,0 @@\n-\/\/ EMIT_RM()\n-void emit_rm(CodeBuffer &cbuf, int f1, int f2, int f3) {\n-  unsigned char c = (unsigned char) ((f1 << 6) | (f2 << 3) | f3);\n-  cbuf.insts()->emit_int8(c);\n-}\n-\n-\/\/ EMIT_CC()\n-void emit_cc(CodeBuffer &cbuf, int f1, int f2) {\n-  unsigned char c = (unsigned char) (f1 | f2);\n-  cbuf.insts()->emit_int8(c);\n-}\n-\n-\/\/ EMIT_OPCODE()\n-void emit_opcode(CodeBuffer &cbuf, int code) {\n-  cbuf.insts()->emit_int8((unsigned char) code);\n-}\n-\n-\/\/ EMIT_OPCODE() w\/ relocation information\n-void emit_opcode(CodeBuffer &cbuf,\n-                 int code, relocInfo::relocType reloc, int offset, int format)\n-{\n-  cbuf.relocate(cbuf.insts_mark() + offset, reloc, format);\n-  emit_opcode(cbuf, code);\n-}\n-\n-\/\/ EMIT_D8()\n-void emit_d8(CodeBuffer &cbuf, int d8) {\n-  cbuf.insts()->emit_int8((unsigned char) d8);\n-}\n-\n-\/\/ EMIT_D16()\n-void emit_d16(CodeBuffer &cbuf, int d16) {\n-  cbuf.insts()->emit_int16(d16);\n-}\n-\n-\/\/ EMIT_D32()\n-void emit_d32(CodeBuffer &cbuf, int d32) {\n-  cbuf.insts()->emit_int32(d32);\n-}\n-\n-\/\/ EMIT_D64()\n-void emit_d64(CodeBuffer &cbuf, int64_t d64) {\n-  cbuf.insts()->emit_int64(d64);\n-}\n-\n-\/\/ emit 32 bit value and construct relocation entry from relocInfo::relocType\n-void emit_d32_reloc(CodeBuffer& cbuf,\n-                    int d32,\n-                    relocInfo::relocType reloc,\n-                    int format)\n-{\n-  assert(reloc != relocInfo::external_word_type, \"use 2-arg emit_d32_reloc\");\n-  cbuf.relocate(cbuf.insts_mark(), reloc, format);\n-  cbuf.insts()->emit_int32(d32);\n-}\n-\n-\/\/ emit 32 bit value and construct relocation entry from RelocationHolder\n-void emit_d32_reloc(CodeBuffer& cbuf, int d32, RelocationHolder const& rspec, int format) {\n-#ifdef ASSERT\n-  if (rspec.reloc()->type() == relocInfo::oop_type &&\n-      d32 != 0 && d32 != (intptr_t) Universe::non_oop_word()) {\n-    assert(Universe::heap()->is_in((address)(intptr_t)d32), \"should be real oop\");\n-    assert(oopDesc::is_oop(cast_to_oop((intptr_t)d32)), \"cannot embed broken oops in code\");\n-  }\n-#endif\n-  cbuf.relocate(cbuf.insts_mark(), rspec, format);\n-  cbuf.insts()->emit_int32(d32);\n-}\n-\n-void emit_d32_reloc(CodeBuffer& cbuf, address addr) {\n-  address next_ip = cbuf.insts_end() + 4;\n-  emit_d32_reloc(cbuf, (int) (addr - next_ip),\n-                 external_word_Relocation::spec(addr),\n-                 RELOC_DISP32);\n-}\n-\n-\n-\/\/ emit 64 bit value and construct relocation entry from relocInfo::relocType\n-void emit_d64_reloc(CodeBuffer& cbuf, int64_t d64, relocInfo::relocType reloc, int format) {\n-  cbuf.relocate(cbuf.insts_mark(), reloc, format);\n-  cbuf.insts()->emit_int64(d64);\n-}\n-\n-\/\/ emit 64 bit value and construct relocation entry from RelocationHolder\n-void emit_d64_reloc(CodeBuffer& cbuf, int64_t d64, RelocationHolder const& rspec, int format) {\n-#ifdef ASSERT\n-  if (rspec.reloc()->type() == relocInfo::oop_type &&\n-      d64 != 0 && d64 != (int64_t) Universe::non_oop_word()) {\n-    assert(Universe::heap()->is_in((address)d64), \"should be real oop\");\n-    assert(oopDesc::is_oop(cast_to_oop(d64)), \"cannot embed broken oops in code\");\n-  }\n-#endif\n-  cbuf.relocate(cbuf.insts_mark(), rspec, format);\n-  cbuf.insts()->emit_int64(d64);\n-}\n-\n-\/\/ Access stack slot for load or store\n-void store_to_stackslot(CodeBuffer &cbuf, int opcode, int rm_field, int disp)\n-{\n-  emit_opcode(cbuf, opcode);                  \/\/ (e.g., FILD   [RSP+src])\n-  if (-0x80 <= disp && disp < 0x80) {\n-    emit_rm(cbuf, 0x01, rm_field, RSP_enc);   \/\/ R\/M byte\n-    emit_rm(cbuf, 0x00, RSP_enc, RSP_enc);    \/\/ SIB byte\n-    emit_d8(cbuf, disp);     \/\/ Displacement  \/\/ R\/M byte\n-  } else {\n-    emit_rm(cbuf, 0x02, rm_field, RSP_enc);   \/\/ R\/M byte\n-    emit_rm(cbuf, 0x00, RSP_enc, RSP_enc);    \/\/ SIB byte\n-    emit_d32(cbuf, disp);     \/\/ Displacement \/\/ R\/M byte\n-  }\n-}\n-\n-   \/\/ rRegI ereg, memory mem) %{    \/\/ emit_reg_mem\n-void encode_RegMem(CodeBuffer &cbuf,\n-                   int reg,\n-                   int base, int index, int scale, int disp, relocInfo::relocType disp_reloc)\n-{\n-  assert(disp_reloc == relocInfo::none, \"cannot have disp\");\n-  int regenc = reg & 7;\n-  int baseenc = base & 7;\n-  int indexenc = index & 7;\n-\n-  \/\/ There is no index & no scale, use form without SIB byte\n-  if (index == 0x4 && scale == 0 && base != RSP_enc && base != R12_enc) {\n-    \/\/ If no displacement, mode is 0x0; unless base is [RBP] or [R13]\n-    if (disp == 0 && base != RBP_enc && base != R13_enc) {\n-      emit_rm(cbuf, 0x0, regenc, baseenc); \/\/ *\n-    } else if (-0x80 <= disp && disp < 0x80 && disp_reloc == relocInfo::none) {\n-      \/\/ If 8-bit displacement, mode 0x1\n-      emit_rm(cbuf, 0x1, regenc, baseenc); \/\/ *\n-      emit_d8(cbuf, disp);\n-    } else {\n-      \/\/ If 32-bit displacement\n-      if (base == -1) { \/\/ Special flag for absolute address\n-        emit_rm(cbuf, 0x0, regenc, 0x5); \/\/ *\n-        if (disp_reloc != relocInfo::none) {\n-          emit_d32_reloc(cbuf, disp, relocInfo::oop_type, RELOC_DISP32);\n-        } else {\n-          emit_d32(cbuf, disp);\n-        }\n-      } else {\n-        \/\/ Normal base + offset\n-        emit_rm(cbuf, 0x2, regenc, baseenc); \/\/ *\n-        if (disp_reloc != relocInfo::none) {\n-          emit_d32_reloc(cbuf, disp, relocInfo::oop_type, RELOC_DISP32);\n-        } else {\n-          emit_d32(cbuf, disp);\n-        }\n-      }\n-    }\n-  } else {\n-    \/\/ Else, encode with the SIB byte\n-    \/\/ If no displacement, mode is 0x0; unless base is [RBP] or [R13]\n-    if (disp == 0 && base != RBP_enc && base != R13_enc) {\n-      \/\/ If no displacement\n-      emit_rm(cbuf, 0x0, regenc, 0x4); \/\/ *\n-      emit_rm(cbuf, scale, indexenc, baseenc);\n-    } else {\n-      if (-0x80 <= disp && disp < 0x80 && disp_reloc == relocInfo::none) {\n-        \/\/ If 8-bit displacement, mode 0x1\n-        emit_rm(cbuf, 0x1, regenc, 0x4); \/\/ *\n-        emit_rm(cbuf, scale, indexenc, baseenc);\n-        emit_d8(cbuf, disp);\n-      } else {\n-        \/\/ If 32-bit displacement\n-        if (base == 0x04 ) {\n-          emit_rm(cbuf, 0x2, regenc, 0x4);\n-          emit_rm(cbuf, scale, indexenc, 0x04); \/\/ XXX is this valid???\n-        } else {\n-          emit_rm(cbuf, 0x2, regenc, 0x4);\n-          emit_rm(cbuf, scale, indexenc, baseenc); \/\/ *\n-        }\n-        if (disp_reloc != relocInfo::none) {\n-          emit_d32_reloc(cbuf, disp, relocInfo::oop_type, RELOC_DISP32);\n-        } else {\n-          emit_d32(cbuf, disp);\n-        }\n-      }\n-    }\n-  }\n-}\n-\n@@ -999,10 +818,1 @@\n-    emit_opcode(cbuf, Assembler::REX_W);\n-    if (framesize < 0x80) {\n-      emit_opcode(cbuf, 0x83); \/\/ addq rsp, #framesize\n-      emit_rm(cbuf, 0x3, 0x00, RSP_enc);\n-      emit_d8(cbuf, framesize);\n-    } else {\n-      emit_opcode(cbuf, 0x81); \/\/ addq rsp, #framesize\n-      emit_rm(cbuf, 0x3, 0x00, RSP_enc);\n-      emit_d32(cbuf, framesize);\n-    }\n+    __ addq(rsp, framesize);\n@@ -1011,2 +821,1 @@\n-  \/\/ popq rbp\n-  emit_opcode(cbuf, 0x58 | RBP_enc);\n+  __ popq(rbp);\n@@ -1649,13 +1458,3 @@\n-  if (offset >= 0x80) {\n-    emit_opcode(cbuf, reg < 8 ? Assembler::REX_W : Assembler::REX_WR);\n-    emit_opcode(cbuf, 0x8D); \/\/ LEA  reg,[SP+offset]\n-    emit_rm(cbuf, 0x2, reg & 7, 0x04);\n-    emit_rm(cbuf, 0x0, 0x04, RSP_enc);\n-    emit_d32(cbuf, offset);\n-  } else {\n-    emit_opcode(cbuf, reg < 8 ? Assembler::REX_W : Assembler::REX_WR);\n-    emit_opcode(cbuf, 0x8D); \/\/ LEA  reg,[SP+offset]\n-    emit_rm(cbuf, 0x1, reg & 7, 0x04);\n-    emit_rm(cbuf, 0x0, 0x04, RSP_enc);\n-    emit_d8(cbuf, offset);\n-  }\n+\n+  MacroAssembler masm(&cbuf);\n+  masm.lea(as_Register(reg), Address(rsp, offset));\n@@ -1860,54 +1659,0 @@\n-  \/\/ Build emit functions for each basic byte or larger field in the\n-  \/\/ intel encoding scheme (opcode, rm, sib, immediate), and call them\n-  \/\/ from C++ code in the enc_class source block.  Emit functions will\n-  \/\/ live in the main source block for now.  In future, we can\n-  \/\/ generalize this by adding a syntax that specifies the sizes of\n-  \/\/ fields in an order, so that the adlc can build the emit functions\n-  \/\/ automagically\n-\n-  \/\/ Emit primary opcode\n-  enc_class OpcP\n-  %{\n-    emit_opcode(cbuf, $primary);\n-  %}\n-\n-  \/\/ Emit secondary opcode\n-  enc_class OpcS\n-  %{\n-    emit_opcode(cbuf, $secondary);\n-  %}\n-\n-  \/\/ Emit tertiary opcode\n-  enc_class OpcT\n-  %{\n-    emit_opcode(cbuf, $tertiary);\n-  %}\n-\n-  \/\/ Emit opcode directly\n-  enc_class Opcode(immI d8)\n-  %{\n-    emit_opcode(cbuf, $d8$$constant);\n-  %}\n-\n-  \/\/ Emit size prefix\n-  enc_class SizePrefix\n-  %{\n-    emit_opcode(cbuf, 0x66);\n-  %}\n-\n-  enc_class reg(rRegI reg)\n-  %{\n-    emit_rm(cbuf, 0x3, 0, $reg$$reg & 7);\n-  %}\n-\n-  enc_class reg_reg(rRegI dst, rRegI src)\n-  %{\n-    emit_rm(cbuf, 0x3, $dst$$reg & 7, $src$$reg & 7);\n-  %}\n-\n-  enc_class opc_reg_reg(immI opcode, rRegI dst, rRegI src)\n-  %{\n-    emit_opcode(cbuf, $opcode$$constant);\n-    emit_rm(cbuf, 0x3, $dst$$reg & 7, $src$$reg & 7);\n-  %}\n-\n@@ -2031,91 +1776,0 @@\n-  \/\/ Opcde enc_class for 8\/32 bit immediate instructions with sign-extension\n-  enc_class OpcSE(immI imm)\n-  %{\n-    \/\/ Emit primary opcode and set sign-extend bit\n-    \/\/ Check for 8-bit immediate, and set sign extend bit in opcode\n-    if (-0x80 <= $imm$$constant && $imm$$constant < 0x80) {\n-      emit_opcode(cbuf, $primary | 0x02);\n-    } else {\n-      \/\/ 32-bit immediate\n-      emit_opcode(cbuf, $primary);\n-    }\n-  %}\n-\n-  enc_class OpcSErm(rRegI dst, immI imm)\n-  %{\n-    \/\/ OpcSEr\/m\n-    int dstenc = $dst$$reg;\n-    if (dstenc >= 8) {\n-      emit_opcode(cbuf, Assembler::REX_B);\n-      dstenc -= 8;\n-    }\n-    \/\/ Emit primary opcode and set sign-extend bit\n-    \/\/ Check for 8-bit immediate, and set sign extend bit in opcode\n-    if (-0x80 <= $imm$$constant && $imm$$constant < 0x80) {\n-      emit_opcode(cbuf, $primary | 0x02);\n-    } else {\n-      \/\/ 32-bit immediate\n-      emit_opcode(cbuf, $primary);\n-    }\n-    \/\/ Emit r\/m byte with secondary opcode, after primary opcode.\n-    emit_rm(cbuf, 0x3, $secondary, dstenc);\n-  %}\n-\n-  enc_class OpcSErm_wide(rRegL dst, immI imm)\n-  %{\n-    \/\/ OpcSEr\/m\n-    int dstenc = $dst$$reg;\n-    if (dstenc < 8) {\n-      emit_opcode(cbuf, Assembler::REX_W);\n-    } else {\n-      emit_opcode(cbuf, Assembler::REX_WB);\n-      dstenc -= 8;\n-    }\n-    \/\/ Emit primary opcode and set sign-extend bit\n-    \/\/ Check for 8-bit immediate, and set sign extend bit in opcode\n-    if (-0x80 <= $imm$$constant && $imm$$constant < 0x80) {\n-      emit_opcode(cbuf, $primary | 0x02);\n-    } else {\n-      \/\/ 32-bit immediate\n-      emit_opcode(cbuf, $primary);\n-    }\n-    \/\/ Emit r\/m byte with secondary opcode, after primary opcode.\n-    emit_rm(cbuf, 0x3, $secondary, dstenc);\n-  %}\n-\n-  enc_class Con8or32(immI imm)\n-  %{\n-    \/\/ Check for 8-bit immediate, and set sign extend bit in opcode\n-    if (-0x80 <= $imm$$constant && $imm$$constant < 0x80) {\n-      $$$emit8$imm$$constant;\n-    } else {\n-      \/\/ 32-bit immediate\n-      $$$emit32$imm$$constant;\n-    }\n-  %}\n-\n-  enc_class opc2_reg(rRegI dst)\n-  %{\n-    \/\/ BSWAP\n-    emit_cc(cbuf, $secondary, $dst$$reg);\n-  %}\n-\n-  enc_class opc3_reg(rRegI dst)\n-  %{\n-    \/\/ BSWAP\n-    emit_cc(cbuf, $tertiary, $dst$$reg);\n-  %}\n-\n-  enc_class reg_opc(rRegI div)\n-  %{\n-    \/\/ INC, DEC, IDIV, IMOD, JMP indirect, ...\n-    emit_rm(cbuf, 0x3, $secondary, $div$$reg & 7);\n-  %}\n-\n-  enc_class enc_cmov(cmpOp cop)\n-  %{\n-    \/\/ CMOV\n-    $$$emit8$primary;\n-    emit_cc(cbuf, $secondary, $cop$$cmpcode);\n-  %}\n-\n@@ -2168,1 +1822,0 @@\n-    cbuf.set_insts_mark();\n@@ -2171,4 +1824,1 @@\n-      $$$emit8$primary;\n-      emit_d32_reloc(cbuf, (int) ($meth$$method - ((intptr_t) cbuf.insts_end()) - 4),\n-                     runtime_call_Relocation::spec(),\n-                     RELOC_DISP32);\n+      __ call(RuntimeAddress(CAST_FROM_FN_PTR(address, $meth$$method)));\n@@ -2181,1 +1831,0 @@\n-      $$$emit8$primary;\n@@ -2185,3 +1834,3 @@\n-      emit_d32_reloc(cbuf, (int) ($meth$$method - ((intptr_t) cbuf.insts_end()) - 4),\n-                     rspec, RELOC_DISP32);\n-      address mark = cbuf.insts_mark();\n+      address mark = __ pc();\n+      int call_offset = __ offset();\n+      __ call(AddressLiteral(CAST_FROM_FN_PTR(address, $meth$$method), rspec));\n@@ -2191,1 +1840,1 @@\n-        cbuf.shared_stub_to_interp_for(_method, cbuf.insts()->mark_off());\n+        cbuf.shared_stub_to_interp_for(_method, call_offset);\n@@ -2201,1 +1850,0 @@\n-    _masm.clear_inst_mark();\n@@ -2211,523 +1859,0 @@\n-  enc_class reg_opc_imm(rRegI dst, immI8 shift)\n-  %{\n-    \/\/ SAL, SAR, SHR\n-    int dstenc = $dst$$reg;\n-    if (dstenc >= 8) {\n-      emit_opcode(cbuf, Assembler::REX_B);\n-      dstenc -= 8;\n-    }\n-    $$$emit8$primary;\n-    emit_rm(cbuf, 0x3, $secondary, dstenc);\n-    $$$emit8$shift$$constant;\n-  %}\n-\n-  enc_class reg_opc_imm_wide(rRegL dst, immI8 shift)\n-  %{\n-    \/\/ SAL, SAR, SHR\n-    int dstenc = $dst$$reg;\n-    if (dstenc < 8) {\n-      emit_opcode(cbuf, Assembler::REX_W);\n-    } else {\n-      emit_opcode(cbuf, Assembler::REX_WB);\n-      dstenc -= 8;\n-    }\n-    $$$emit8$primary;\n-    emit_rm(cbuf, 0x3, $secondary, dstenc);\n-    $$$emit8$shift$$constant;\n-  %}\n-\n-  enc_class load_immI(rRegI dst, immI src)\n-  %{\n-    int dstenc = $dst$$reg;\n-    if (dstenc >= 8) {\n-      emit_opcode(cbuf, Assembler::REX_B);\n-      dstenc -= 8;\n-    }\n-    emit_opcode(cbuf, 0xB8 | dstenc);\n-    $$$emit32$src$$constant;\n-  %}\n-\n-  enc_class load_immL(rRegL dst, immL src)\n-  %{\n-    int dstenc = $dst$$reg;\n-    if (dstenc < 8) {\n-      emit_opcode(cbuf, Assembler::REX_W);\n-    } else {\n-      emit_opcode(cbuf, Assembler::REX_WB);\n-      dstenc -= 8;\n-    }\n-    emit_opcode(cbuf, 0xB8 | dstenc);\n-    emit_d64(cbuf, $src$$constant);\n-  %}\n-\n-  enc_class load_immUL32(rRegL dst, immUL32 src)\n-  %{\n-    \/\/ same as load_immI, but this time we care about zeroes in the high word\n-    int dstenc = $dst$$reg;\n-    if (dstenc >= 8) {\n-      emit_opcode(cbuf, Assembler::REX_B);\n-      dstenc -= 8;\n-    }\n-    emit_opcode(cbuf, 0xB8 | dstenc);\n-    $$$emit32$src$$constant;\n-  %}\n-\n-  enc_class load_immL32(rRegL dst, immL32 src)\n-  %{\n-    int dstenc = $dst$$reg;\n-    if (dstenc < 8) {\n-      emit_opcode(cbuf, Assembler::REX_W);\n-    } else {\n-      emit_opcode(cbuf, Assembler::REX_WB);\n-      dstenc -= 8;\n-    }\n-    emit_opcode(cbuf, 0xC7);\n-    emit_rm(cbuf, 0x03, 0x00, dstenc);\n-    $$$emit32$src$$constant;\n-  %}\n-\n-  enc_class load_immP31(rRegP dst, immP32 src)\n-  %{\n-    \/\/ same as load_immI, but this time we care about zeroes in the high word\n-    int dstenc = $dst$$reg;\n-    if (dstenc >= 8) {\n-      emit_opcode(cbuf, Assembler::REX_B);\n-      dstenc -= 8;\n-    }\n-    emit_opcode(cbuf, 0xB8 | dstenc);\n-    $$$emit32$src$$constant;\n-  %}\n-\n-  enc_class load_immP(rRegP dst, immP src)\n-  %{\n-    int dstenc = $dst$$reg;\n-    if (dstenc < 8) {\n-      emit_opcode(cbuf, Assembler::REX_W);\n-    } else {\n-      emit_opcode(cbuf, Assembler::REX_WB);\n-      dstenc -= 8;\n-    }\n-    emit_opcode(cbuf, 0xB8 | dstenc);\n-    \/\/ This next line should be generated from ADLC\n-    if ($src->constant_reloc() != relocInfo::none) {\n-      emit_d64_reloc(cbuf, $src$$constant, $src->constant_reloc(), RELOC_IMM64);\n-    } else {\n-      emit_d64(cbuf, $src$$constant);\n-    }\n-  %}\n-\n-  enc_class Con32(immI src)\n-  %{\n-    \/\/ Output immediate\n-    $$$emit32$src$$constant;\n-  %}\n-\n-  enc_class Con32F_as_bits(immF src)\n-  %{\n-    \/\/ Output Float immediate bits\n-    jfloat jf = $src$$constant;\n-    jint jf_as_bits = jint_cast(jf);\n-    emit_d32(cbuf, jf_as_bits);\n-  %}\n-\n-  enc_class Con16(immI src)\n-  %{\n-    \/\/ Output immediate\n-    $$$emit16$src$$constant;\n-  %}\n-\n-  \/\/ How is this different from Con32??? XXX\n-  enc_class Con_d32(immI src)\n-  %{\n-    emit_d32(cbuf,$src$$constant);\n-  %}\n-\n-  enc_class conmemref (rRegP t1) %{    \/\/ Con32(storeImmI)\n-    \/\/ Output immediate memory reference\n-    emit_rm(cbuf, 0x00, $t1$$reg, 0x05 );\n-    emit_d32(cbuf, 0x00);\n-  %}\n-\n-  enc_class lock_prefix()\n-  %{\n-    emit_opcode(cbuf, 0xF0); \/\/ lock\n-  %}\n-\n-  enc_class REX_mem(memory mem)\n-  %{\n-    if ($mem$$base >= 8) {\n-      if ($mem$$index < 8) {\n-        emit_opcode(cbuf, Assembler::REX_B);\n-      } else {\n-        emit_opcode(cbuf, Assembler::REX_XB);\n-      }\n-    } else {\n-      if ($mem$$index >= 8) {\n-        emit_opcode(cbuf, Assembler::REX_X);\n-      }\n-    }\n-  %}\n-\n-  enc_class REX_mem_wide(memory mem)\n-  %{\n-    if ($mem$$base >= 8) {\n-      if ($mem$$index < 8) {\n-        emit_opcode(cbuf, Assembler::REX_WB);\n-      } else {\n-        emit_opcode(cbuf, Assembler::REX_WXB);\n-      }\n-    } else {\n-      if ($mem$$index < 8) {\n-        emit_opcode(cbuf, Assembler::REX_W);\n-      } else {\n-        emit_opcode(cbuf, Assembler::REX_WX);\n-      }\n-    }\n-  %}\n-\n-  \/\/ for byte regs\n-  enc_class REX_breg(rRegI reg)\n-  %{\n-    if ($reg$$reg >= 4) {\n-      emit_opcode(cbuf, $reg$$reg < 8 ? Assembler::REX : Assembler::REX_B);\n-    }\n-  %}\n-\n-  \/\/ for byte regs\n-  enc_class REX_reg_breg(rRegI dst, rRegI src)\n-  %{\n-    if ($dst$$reg < 8) {\n-      if ($src$$reg >= 4) {\n-        emit_opcode(cbuf, $src$$reg < 8 ? Assembler::REX : Assembler::REX_B);\n-      }\n-    } else {\n-      if ($src$$reg < 8) {\n-        emit_opcode(cbuf, Assembler::REX_R);\n-      } else {\n-        emit_opcode(cbuf, Assembler::REX_RB);\n-      }\n-    }\n-  %}\n-\n-  \/\/ for byte regs\n-  enc_class REX_breg_mem(rRegI reg, memory mem)\n-  %{\n-    if ($reg$$reg < 8) {\n-      if ($mem$$base < 8) {\n-        if ($mem$$index >= 8) {\n-          emit_opcode(cbuf, Assembler::REX_X);\n-        } else if ($reg$$reg >= 4) {\n-          emit_opcode(cbuf, Assembler::REX);\n-        }\n-      } else {\n-        if ($mem$$index < 8) {\n-          emit_opcode(cbuf, Assembler::REX_B);\n-        } else {\n-          emit_opcode(cbuf, Assembler::REX_XB);\n-        }\n-      }\n-    } else {\n-      if ($mem$$base < 8) {\n-        if ($mem$$index < 8) {\n-          emit_opcode(cbuf, Assembler::REX_R);\n-        } else {\n-          emit_opcode(cbuf, Assembler::REX_RX);\n-        }\n-      } else {\n-        if ($mem$$index < 8) {\n-          emit_opcode(cbuf, Assembler::REX_RB);\n-        } else {\n-          emit_opcode(cbuf, Assembler::REX_RXB);\n-        }\n-      }\n-    }\n-  %}\n-\n-  enc_class REX_reg(rRegI reg)\n-  %{\n-    if ($reg$$reg >= 8) {\n-      emit_opcode(cbuf, Assembler::REX_B);\n-    }\n-  %}\n-\n-  enc_class REX_reg_wide(rRegI reg)\n-  %{\n-    if ($reg$$reg < 8) {\n-      emit_opcode(cbuf, Assembler::REX_W);\n-    } else {\n-      emit_opcode(cbuf, Assembler::REX_WB);\n-    }\n-  %}\n-\n-  enc_class REX_reg_reg(rRegI dst, rRegI src)\n-  %{\n-    if ($dst$$reg < 8) {\n-      if ($src$$reg >= 8) {\n-        emit_opcode(cbuf, Assembler::REX_B);\n-      }\n-    } else {\n-      if ($src$$reg < 8) {\n-        emit_opcode(cbuf, Assembler::REX_R);\n-      } else {\n-        emit_opcode(cbuf, Assembler::REX_RB);\n-      }\n-    }\n-  %}\n-\n-  enc_class REX_reg_reg_wide(rRegI dst, rRegI src)\n-  %{\n-    if ($dst$$reg < 8) {\n-      if ($src$$reg < 8) {\n-        emit_opcode(cbuf, Assembler::REX_W);\n-      } else {\n-        emit_opcode(cbuf, Assembler::REX_WB);\n-      }\n-    } else {\n-      if ($src$$reg < 8) {\n-        emit_opcode(cbuf, Assembler::REX_WR);\n-      } else {\n-        emit_opcode(cbuf, Assembler::REX_WRB);\n-      }\n-    }\n-  %}\n-\n-  enc_class REX_reg_mem(rRegI reg, memory mem)\n-  %{\n-    if ($reg$$reg < 8) {\n-      if ($mem$$base < 8) {\n-        if ($mem$$index >= 8) {\n-          emit_opcode(cbuf, Assembler::REX_X);\n-        }\n-      } else {\n-        if ($mem$$index < 8) {\n-          emit_opcode(cbuf, Assembler::REX_B);\n-        } else {\n-          emit_opcode(cbuf, Assembler::REX_XB);\n-        }\n-      }\n-    } else {\n-      if ($mem$$base < 8) {\n-        if ($mem$$index < 8) {\n-          emit_opcode(cbuf, Assembler::REX_R);\n-        } else {\n-          emit_opcode(cbuf, Assembler::REX_RX);\n-        }\n-      } else {\n-        if ($mem$$index < 8) {\n-          emit_opcode(cbuf, Assembler::REX_RB);\n-        } else {\n-          emit_opcode(cbuf, Assembler::REX_RXB);\n-        }\n-      }\n-    }\n-  %}\n-\n-  enc_class REX_reg_mem_wide(rRegL reg, memory mem)\n-  %{\n-    if ($reg$$reg < 8) {\n-      if ($mem$$base < 8) {\n-        if ($mem$$index < 8) {\n-          emit_opcode(cbuf, Assembler::REX_W);\n-        } else {\n-          emit_opcode(cbuf, Assembler::REX_WX);\n-        }\n-      } else {\n-        if ($mem$$index < 8) {\n-          emit_opcode(cbuf, Assembler::REX_WB);\n-        } else {\n-          emit_opcode(cbuf, Assembler::REX_WXB);\n-        }\n-      }\n-    } else {\n-      if ($mem$$base < 8) {\n-        if ($mem$$index < 8) {\n-          emit_opcode(cbuf, Assembler::REX_WR);\n-        } else {\n-          emit_opcode(cbuf, Assembler::REX_WRX);\n-        }\n-      } else {\n-        if ($mem$$index < 8) {\n-          emit_opcode(cbuf, Assembler::REX_WRB);\n-        } else {\n-          emit_opcode(cbuf, Assembler::REX_WRXB);\n-        }\n-      }\n-    }\n-  %}\n-\n-  enc_class reg_mem(rRegI ereg, memory mem)\n-  %{\n-    \/\/ High registers handle in encode_RegMem\n-    int reg = $ereg$$reg;\n-    int base = $mem$$base;\n-    int index = $mem$$index;\n-    int scale = $mem$$scale;\n-    int disp = $mem$$disp;\n-    relocInfo::relocType disp_reloc = $mem->disp_reloc();\n-\n-    encode_RegMem(cbuf, reg, base, index, scale, disp, disp_reloc);\n-  %}\n-\n-  enc_class RM_opc_mem(immI rm_opcode, memory mem)\n-  %{\n-    int rm_byte_opcode = $rm_opcode$$constant;\n-\n-    \/\/ High registers handle in encode_RegMem\n-    int base = $mem$$base;\n-    int index = $mem$$index;\n-    int scale = $mem$$scale;\n-    int displace = $mem$$disp;\n-\n-    relocInfo::relocType disp_reloc = $mem->disp_reloc();       \/\/ disp-as-oop when\n-                                            \/\/ working with static\n-                                            \/\/ globals\n-    encode_RegMem(cbuf, rm_byte_opcode, base, index, scale, displace,\n-                  disp_reloc);\n-  %}\n-\n-  enc_class reg_lea(rRegI dst, rRegI src0, immI src1)\n-  %{\n-    int reg_encoding = $dst$$reg;\n-    int base         = $src0$$reg;      \/\/ 0xFFFFFFFF indicates no base\n-    int index        = 0x04;            \/\/ 0x04 indicates no index\n-    int scale        = 0x00;            \/\/ 0x00 indicates no scale\n-    int displace     = $src1$$constant; \/\/ 0x00 indicates no displacement\n-    relocInfo::relocType disp_reloc = relocInfo::none;\n-    encode_RegMem(cbuf, reg_encoding, base, index, scale, displace,\n-                  disp_reloc);\n-  %}\n-\n-  enc_class neg_reg(rRegI dst)\n-  %{\n-    int dstenc = $dst$$reg;\n-    if (dstenc >= 8) {\n-      emit_opcode(cbuf, Assembler::REX_B);\n-      dstenc -= 8;\n-    }\n-    \/\/ NEG $dst\n-    emit_opcode(cbuf, 0xF7);\n-    emit_rm(cbuf, 0x3, 0x03, dstenc);\n-  %}\n-\n-  enc_class neg_reg_wide(rRegI dst)\n-  %{\n-    int dstenc = $dst$$reg;\n-    if (dstenc < 8) {\n-      emit_opcode(cbuf, Assembler::REX_W);\n-    } else {\n-      emit_opcode(cbuf, Assembler::REX_WB);\n-      dstenc -= 8;\n-    }\n-    \/\/ NEG $dst\n-    emit_opcode(cbuf, 0xF7);\n-    emit_rm(cbuf, 0x3, 0x03, dstenc);\n-  %}\n-\n-  enc_class setLT_reg(rRegI dst)\n-  %{\n-    int dstenc = $dst$$reg;\n-    if (dstenc >= 8) {\n-      emit_opcode(cbuf, Assembler::REX_B);\n-      dstenc -= 8;\n-    } else if (dstenc >= 4) {\n-      emit_opcode(cbuf, Assembler::REX);\n-    }\n-    \/\/ SETLT $dst\n-    emit_opcode(cbuf, 0x0F);\n-    emit_opcode(cbuf, 0x9C);\n-    emit_rm(cbuf, 0x3, 0x0, dstenc);\n-  %}\n-\n-  enc_class setNZ_reg(rRegI dst)\n-  %{\n-    int dstenc = $dst$$reg;\n-    if (dstenc >= 8) {\n-      emit_opcode(cbuf, Assembler::REX_B);\n-      dstenc -= 8;\n-    } else if (dstenc >= 4) {\n-      emit_opcode(cbuf, Assembler::REX);\n-    }\n-    \/\/ SETNZ $dst\n-    emit_opcode(cbuf, 0x0F);\n-    emit_opcode(cbuf, 0x95);\n-    emit_rm(cbuf, 0x3, 0x0, dstenc);\n-  %}\n-\n-\n-  \/\/ Compare the lonogs and set -1, 0, or 1 into dst\n-  enc_class cmpl3_flag(rRegL src1, rRegL src2, rRegI dst)\n-  %{\n-    int src1enc = $src1$$reg;\n-    int src2enc = $src2$$reg;\n-    int dstenc = $dst$$reg;\n-\n-    \/\/ cmpq $src1, $src2\n-    if (src1enc < 8) {\n-      if (src2enc < 8) {\n-        emit_opcode(cbuf, Assembler::REX_W);\n-      } else {\n-        emit_opcode(cbuf, Assembler::REX_WB);\n-      }\n-    } else {\n-      if (src2enc < 8) {\n-        emit_opcode(cbuf, Assembler::REX_WR);\n-      } else {\n-        emit_opcode(cbuf, Assembler::REX_WRB);\n-      }\n-    }\n-    emit_opcode(cbuf, 0x3B);\n-    emit_rm(cbuf, 0x3, src1enc & 7, src2enc & 7);\n-\n-    \/\/ movl $dst, -1\n-    if (dstenc >= 8) {\n-      emit_opcode(cbuf, Assembler::REX_B);\n-    }\n-    emit_opcode(cbuf, 0xB8 | (dstenc & 7));\n-    emit_d32(cbuf, -1);\n-\n-    \/\/ jl,s done\n-    emit_opcode(cbuf, 0x7C);\n-    emit_d8(cbuf, dstenc < 4 ? 0x06 : 0x08);\n-\n-    \/\/ setne $dst\n-    if (dstenc >= 4) {\n-      emit_opcode(cbuf, dstenc < 8 ? Assembler::REX : Assembler::REX_B);\n-    }\n-    emit_opcode(cbuf, 0x0F);\n-    emit_opcode(cbuf, 0x95);\n-    emit_opcode(cbuf, 0xC0 | (dstenc & 7));\n-\n-    \/\/ movzbl $dst, $dst\n-    if (dstenc >= 4) {\n-      emit_opcode(cbuf, dstenc < 8 ? Assembler::REX : Assembler::REX_RB);\n-    }\n-    emit_opcode(cbuf, 0x0F);\n-    emit_opcode(cbuf, 0xB6);\n-    emit_rm(cbuf, 0x3, dstenc & 7, dstenc & 7);\n-  %}\n-\n-  enc_class Push_ResultXD(regD dst) %{\n-    MacroAssembler _masm(&cbuf);\n-    __ fstp_d(Address(rsp, 0));\n-    __ movdbl($dst$$XMMRegister, Address(rsp, 0));\n-    __ addptr(rsp, 8);\n-  %}\n-\n-  enc_class Push_SrcXD(regD src) %{\n-    MacroAssembler _masm(&cbuf);\n-    __ subptr(rsp, 8);\n-    __ movdbl(Address(rsp, 0), $src$$XMMRegister);\n-    __ fld_d(Address(rsp, 0));\n-  %}\n-\n-\n-  enc_class enc_rethrow()\n-  %{\n-    cbuf.set_insts_mark();\n-    emit_opcode(cbuf, 0xE9); \/\/ jmp entry\n-    emit_d32_reloc(cbuf,\n-                   (int) (OptoRuntime::rethrow_stub() - cbuf.insts_end() - 4),\n-                   runtime_call_Relocation::spec(),\n-                   RELOC_DISP32);\n-  %}\n-\n@@ -4591,11 +3716,0 @@\n-\/\/ XXX\n-\/\/ \/\/ Conditional move double reg-reg\n-\/\/ pipe_class pipe_cmovD_reg( rFlagsReg cr, regDPR1 dst, regD src)\n-\/\/ %{\n-\/\/     single_instruction;\n-\/\/     dst    : S4(write);\n-\/\/     src    : S3(read);\n-\/\/     cr     : S3(read);\n-\/\/     DECODE : S0;     \/\/ any decoder\n-\/\/ %}\n-\n@@ -5928,1 +5042,1 @@\n-    __ xorpd ($dst$$XMMRegister, $dst$$XMMRegister);\n+    __ xorpd($dst$$XMMRegister, $dst$$XMMRegister);\n@@ -5939,2 +5053,3 @@\n-  opcode(0x8B);\n-  ins_encode(REX_reg_mem(dst, src), OpcP, reg_mem(dst, src));\n+  ins_encode %{\n+    __ movl($dst$$Register, $src$$Address);\n+  %}\n@@ -5950,2 +5065,3 @@\n-  opcode(0x8B);\n-  ins_encode(REX_reg_mem_wide(dst, src), OpcP, reg_mem(dst, src));\n+  ins_encode %{\n+    __ movq($dst$$Register, $src$$Address);\n+  %}\n@@ -5961,2 +5077,3 @@\n-  opcode(0x8B);\n-  ins_encode(REX_reg_mem_wide(dst, src), OpcP, reg_mem(dst, src));\n+  ins_encode %{\n+    __ movq($dst$$Register, $src$$Address);\n+  %}\n@@ -6420,2 +5537,3 @@\n-  opcode(0x89);\n-  ins_encode(REX_reg_mem(src, dst), OpcP, reg_mem(src, dst));\n+  ins_encode %{\n+    __ movl($dst$$Address, $src$$Register);\n+  %}\n@@ -6431,2 +5549,3 @@\n-  opcode(0x89);\n-  ins_encode(REX_reg_mem_wide(src, dst), OpcP, reg_mem(src, dst));\n+  ins_encode %{\n+    __ movq($dst$$Address, $src$$Register);\n+  %}\n@@ -6442,2 +5561,3 @@\n-  opcode(0x89);\n-  ins_encode(REX_reg_mem_wide(src, dst), OpcP, reg_mem(src, dst));\n+  ins_encode %{\n+    __ movq($dst$$Address, $src$$Register);\n+  %}\n@@ -7398,27 +6518,0 @@\n-\/\/ DISABLED: Requires the ADLC to emit a bottom_type call that\n-\/\/ correctly meets the two pointer arguments; one is an incoming\n-\/\/ register but the other is a memory operand.  ALSO appears to\n-\/\/ be buggy with implicit null checks.\n-\/\/\n-\/\/\/\/ Conditional move\n-\/\/instruct cmovP_mem(cmpOp cop, rFlagsReg cr, rRegP dst, memory src)\n-\/\/%{\n-\/\/  match(Set dst (CMoveP (Binary cop cr) (Binary dst (LoadP src))));\n-\/\/  ins_cost(250);\n-\/\/  format %{ \"CMOV$cop $dst,$src\\t# ptr\" %}\n-\/\/  opcode(0x0F,0x40);\n-\/\/  ins_encode( enc_cmov(cop), reg_mem( dst, src ) );\n-\/\/  ins_pipe( pipe_cmov_mem );\n-\/\/%}\n-\/\/\n-\/\/\/\/ Conditional move\n-\/\/instruct cmovP_memU(cmpOpU cop, rFlagsRegU cr, rRegP dst, memory src)\n-\/\/%{\n-\/\/  match(Set dst (CMoveP (Binary cop cr) (Binary dst (LoadP src))));\n-\/\/  ins_cost(250);\n-\/\/  format %{ \"CMOV$cop $dst,$src\\t# ptr\" %}\n-\/\/  opcode(0x0F,0x40);\n-\/\/  ins_encode( enc_cmov(cop), reg_mem( dst, src ) );\n-\/\/  ins_pipe( pipe_cmov_mem );\n-\/\/%}\n-\n@@ -7579,12 +6672,0 @@\n-\/\/ instruct cmovF_mem(cmpOp cop, rFlagsReg cr, regF dst, memory src)\n-\/\/ %{\n-\/\/   match(Set dst (CMoveF (Binary cop cr) (Binary dst (LoadL src))));\n-\n-\/\/   ins_cost(200); \/\/ XXX\n-\/\/   format %{ \"jn$cop    skip\\t# signed cmove float\\n\\t\"\n-\/\/             \"movss     $dst, $src\\n\"\n-\/\/     \"skip:\" %}\n-\/\/   ins_encode(enc_cmovf_mem_branch(cop, dst, src));\n-\/\/   ins_pipe(pipe_slow);\n-\/\/ %}\n-\n@@ -8671,2 +7752,3 @@\n-  opcode(0x2B);\n-  ins_encode(REX_reg_reg_wide(dst, src), OpcP, reg_reg(dst, src));\n+  ins_encode %{\n+    __ subq($dst$$Register, $src$$Register);\n+  %}\n@@ -11086,1 +10168,1 @@\n-\n+  effect(TEMP dst);\n@@ -11108,1 +10190,1 @@\n-\n+  effect(TEMP dst);\n@@ -11324,17 +10406,0 @@\n-\/\/ instruct convI2L_reg_reg_foo(rRegL dst, rRegI src)\n-\/\/ %{\n-\/\/   match(Set dst (ConvI2L src));\n-\/\/ \/\/   predicate(_kids[0]->_leaf->as_Type()->type()->is_int()->_lo >= 0 &&\n-\/\/ \/\/             _kids[0]->_leaf->as_Type()->type()->is_int()->_hi >= 0);\n-\/\/   predicate(((const TypeNode*) n)->type()->is_long()->_hi ==\n-\/\/             (unsigned int) ((const TypeNode*) n)->type()->is_long()->_hi &&\n-\/\/             ((const TypeNode*) n)->type()->is_long()->_lo ==\n-\/\/             (unsigned int) ((const TypeNode*) n)->type()->is_long()->_lo);\n-\n-\/\/   format %{ \"movl    $dst, $src\\t# unsigned i2l\" %}\n-\/\/   ins_encode(enc_copy(dst, src));\n-\/\/ \/\/   opcode(0x63); \/\/ needs REX.W\n-\/\/ \/\/   ins_encode(REX_reg_reg_wide(dst, src), OpcP, reg_reg(dst,src));\n-\/\/   ins_pipe(ialu_reg_reg);\n-\/\/ %}\n-\n@@ -12619,11 +11684,0 @@\n-\/\/ \/\/ \/\/ Cisc-spilled version of cmpU_rReg\n-\/\/ \/\/instruct compU_mem_rReg(rFlagsRegU cr, memory op1, rRegI op2)\n-\/\/ \/\/%{\n-\/\/ \/\/  match(Set cr (CmpU (LoadI op1) op2));\n-\/\/ \/\/\n-\/\/ \/\/  format %{ \"CMPu   $op1,$op2\" %}\n-\/\/ \/\/  ins_cost(500);\n-\/\/ \/\/  opcode(0x39);  \/* Opcode 39 \/r *\/\n-\/\/ \/\/  ins_encode( OpcP, reg_mem( op1, op2) );\n-\/\/ \/\/%}\n-\n@@ -12665,11 +11719,0 @@\n-\/\/ \/\/ \/\/ Cisc-spilled version of cmpP_rReg\n-\/\/ \/\/instruct compP_mem_rReg(rFlagsRegU cr, memory op1, rRegP op2)\n-\/\/ \/\/%{\n-\/\/ \/\/  match(Set cr (CmpP (LoadP op1) op2));\n-\/\/ \/\/\n-\/\/ \/\/  format %{ \"CMPu   $op1,$op2\" %}\n-\/\/ \/\/  ins_cost(500);\n-\/\/ \/\/  opcode(0x39);  \/* Opcode 39 \/r *\/\n-\/\/ \/\/  ins_encode( OpcP, reg_mem( op1, op2) );\n-\/\/ \/\/%}\n-\n@@ -13653,1 +12696,3 @@\n-  ins_encode(enc_rethrow);\n+  ins_encode %{\n+    __ jump(RuntimeAddress(OptoRuntime::rethrow_stub()), noreg);\n+  %}\n","filename":"src\/hotspot\/cpu\/x86\/x86_64.ad","additions":37,"deletions":992,"binary":false,"changes":1029,"status":"modified"},{"patch":"@@ -29,0 +29,1 @@\n+#include \"cds\/cdsConfig.hpp\"\n@@ -190,4 +191,0 @@\n-bool ArchiveBuilder::is_dumping_full_module_graph() {\n-  return DumpSharedSpaces && MetaspaceShared::use_full_module_graph();\n-}\n-\n@@ -239,1 +236,1 @@\n-  if (is_dumping_full_module_graph()) {\n+  if (CDSConfig::is_dumping_full_module_graph()) {\n@@ -245,1 +242,1 @@\n-  if (DumpSharedSpaces) {\n+  if (CDSConfig::is_dumping_static_archive()) {\n@@ -345,1 +342,1 @@\n-  if (DumpSharedSpaces) {\n+  if (CDSConfig::is_dumping_static_archive()) {\n@@ -373,1 +370,1 @@\n-  if (DumpSharedSpaces) {\n+  if (CDSConfig::is_dumping_static_archive()) {\n@@ -518,1 +515,1 @@\n-      assert(DynamicDumpSharedSpaces, \"sanity\");\n+      assert(CDSConfig::is_dumping_dynamic_archive(), \"sanity\");\n@@ -576,0 +573,6 @@\n+char* ArchiveBuilder::ro_strdup(const char* s) {\n+  char* archived_str = ro_region_alloc((int)strlen(s) + 1);\n+  strcpy(archived_str, s);\n+  return archived_str;\n+}\n+\n@@ -582,1 +585,1 @@\n-  if (is_dumping_full_module_graph()) {\n+  if (CDSConfig::is_dumping_full_module_graph()) {\n@@ -599,1 +602,1 @@\n-  if (is_dumping_full_module_graph()) {\n+  if (CDSConfig::is_dumping_full_module_graph()) {\n@@ -642,1 +645,1 @@\n-  if (DumpSharedSpaces && (src_info->msotype() == MetaspaceObj::SymbolType)) {\n+  if (CDSConfig::is_dumping_static_archive() && (src_info->msotype() == MetaspaceObj::SymbolType)) {\n@@ -749,1 +752,1 @@\n-      if (DynamicDumpSharedSpaces) {\n+      if (CDSConfig::is_dumping_dynamic_archive()) {\n@@ -822,1 +825,1 @@\n-    assert(DynamicDumpSharedSpaces, \"must be\");\n+    assert(CDSConfig::is_dumping_dynamic_archive(), \"must be\");\n@@ -834,1 +837,1 @@\n-  assert(DumpSharedSpaces, \"sanity\");\n+  assert(CDSConfig::is_dumping_heap(), \"sanity\");\n@@ -925,1 +928,1 @@\n-  if (DumpSharedSpaces) {\n+  if (CDSConfig::is_dumping_static_archive()) {\n@@ -930,1 +933,1 @@\n-    assert(DynamicDumpSharedSpaces, \"must be\");\n+    assert(CDSConfig::is_dumping_dynamic_archive(), \"must be\");\n@@ -1145,0 +1148,1 @@\n+      assert(requested_obj != nullptr, \"Attempting to load field from null oop\");\n@@ -1246,1 +1250,1 @@\n-    log_info(cds, map)(\"%s CDS archive map for %s\", DumpSharedSpaces ? \"Static\" : \"Dynamic\", mapinfo->full_path());\n+    log_info(cds, map)(\"%s CDS archive map for %s\", CDSConfig::is_dumping_static_archive() ? \"Static\" : \"Dynamic\", mapinfo->full_path());\n","filename":"src\/hotspot\/share\/cds\/archiveBuilder.cpp","additions":22,"deletions":18,"binary":false,"changes":40,"status":"modified"},{"patch":"@@ -254,1 +254,0 @@\n-  bool is_dumping_full_module_graph();\n@@ -391,0 +390,2 @@\n+  char* ro_strdup(const char* s);\n+\n","filename":"src\/hotspot\/share\/cds\/archiveBuilder.hpp","additions":2,"deletions":1,"binary":false,"changes":3,"status":"modified"},{"patch":"@@ -27,0 +27,1 @@\n+#include \"cds\/cdsConfig.hpp\"\n@@ -141,1 +142,1 @@\n-  assert(DumpSharedSpaces, \"dump-time only\");\n+  assert(CDSConfig::is_dumping_heap(), \"dump-time only\");\n","filename":"src\/hotspot\/share\/cds\/archiveHeapWriter.cpp","additions":2,"deletions":1,"binary":false,"changes":3,"status":"modified"},{"patch":"@@ -31,0 +31,1 @@\n+#include \"cds\/cdsConfig.hpp\"\n@@ -47,1 +48,1 @@\n-#include \"logging\/logStream.hpp\"\n+#include \"logging\/logStream.hpp\"\n@@ -54,0 +55,1 @@\n+#include \"nmt\/memTracker.hpp\"\n@@ -65,1 +67,0 @@\n-#include \"services\/memTracker.hpp\"\n@@ -196,1 +197,1 @@\n-  set_magic(DynamicDumpSharedSpaces ? CDS_DYNAMIC_ARCHIVE_MAGIC : CDS_ARCHIVE_MAGIC);\n+  set_magic(CDSConfig::is_dumping_dynamic_archive() ? CDS_DYNAMIC_ARCHIVE_MAGIC : CDS_ARCHIVE_MAGIC);\n@@ -207,1 +208,1 @@\n-  if (DumpSharedSpaces && HeapShared::can_write()) {\n+  if (CDSConfig::is_dumping_heap()) {\n@@ -216,1 +217,1 @@\n-  _use_full_module_graph = MetaspaceShared::use_full_module_graph();\n+  _has_full_module_graph = CDSConfig::is_dumping_full_module_graph();\n@@ -238,1 +239,1 @@\n-  if (!DynamicDumpSharedSpaces) {\n+  if (!CDSConfig::is_dumping_dynamic_archive()) {\n@@ -295,1 +296,1 @@\n-  st->print_cr(\"- use_full_module_graph           %d\", _use_full_module_graph);\n+  st->print_cr(\"- has_full_module_graph           %d\", _has_full_module_graph);\n@@ -307,1 +308,1 @@\n-  Arguments::assert_is_dumping_archive();\n+  assert(CDSConfig::is_dumping_archive(), \"sanity\");\n@@ -467,1 +468,1 @@\n-  Arguments::assert_is_dumping_archive();\n+  assert(CDSConfig::is_dumping_archive(), \"sanity\");\n@@ -514,1 +515,1 @@\n-  Arguments::assert_is_dumping_archive();\n+  assert(CDSConfig::is_dumping_archive(), \"sanity\");\n@@ -541,1 +542,1 @@\n-  Arguments::assert_is_dumping_archive();\n+  assert(CDSConfig::is_dumping_archive(), \"sanity\");\n@@ -550,1 +551,1 @@\n-  Arguments::assert_is_dumping_archive();\n+  assert(CDSConfig::is_dumping_archive(), \"sanity\");\n@@ -691,1 +692,1 @@\n-  Arguments::assert_is_dumping_archive();\n+  assert(CDSConfig::is_dumping_archive(), \"sanity\");\n@@ -936,1 +937,1 @@\n-  if (DynamicDumpSharedSpaces) {\n+  if (CDSConfig::is_dumping_dynamic_archive()) {\n@@ -946,1 +947,1 @@\n-      DynamicDumpSharedSpaces = false;\n+      CDSConfig::disable_dumping_dynamic_archive();\n@@ -952,1 +953,1 @@\n-        DynamicDumpSharedSpaces = false;\n+        CDSConfig::disable_dumping_dynamic_archive();\n@@ -1513,1 +1514,1 @@\n-  Arguments::assert_is_dumping_archive();\n+  assert(CDSConfig::is_dumping_archive(), \"sanity\");\n@@ -1527,1 +1528,1 @@\n-    assert(!DynamicDumpSharedSpaces, \"must be\");\n+    assert(!CDSConfig::is_dumping_dynamic_archive(), \"must be\");\n@@ -1972,1 +1973,1 @@\n-    MetaspaceShared::disable_full_module_graph();\n+    CDSConfig::disable_loading_full_module_graph();\n@@ -2276,1 +2277,1 @@\n-        DynamicDumpSharedSpaces = true;\n+        CDSConfig::enable_dumping_dynamic_archive();\n@@ -2399,3 +2400,3 @@\n-  if (!_use_full_module_graph) {\n-    MetaspaceShared::disable_full_module_graph();\n-    log_info(cds)(\"full module graph: disabled because archive was created without full module graph\");\n+  if (is_static() && !_has_full_module_graph) {\n+    \/\/ Only the static archive can contain the full module graph.\n+    CDSConfig::disable_loading_full_module_graph(\"archive was created without full module graph\");\n","filename":"src\/hotspot\/share\/cds\/filemap.cpp","additions":24,"deletions":23,"binary":false,"changes":47,"status":"modified"},{"patch":"@@ -227,1 +227,1 @@\n-  bool   _use_full_module_graph;        \/\/ Can we use the full archived module graph?\n+  bool   _has_full_module_graph;        \/\/ Does this CDS archive contain the full archived module graph?\n@@ -253,0 +253,1 @@\n+  bool is_static()                         const { return magic() == CDS_ARCHIVE_MAGIC; }\n","filename":"src\/hotspot\/share\/cds\/filemap.hpp","additions":2,"deletions":1,"binary":false,"changes":3,"status":"modified"},{"patch":"@@ -30,0 +30,1 @@\n+#include \"cds\/cdsConfig.hpp\"\n@@ -31,1 +32,1 @@\n-#include \"cds\/classListWriter.hpp\"\n+#include \"cds\/cds_globals.hpp\"\n@@ -33,0 +34,1 @@\n+#include \"cds\/classListWriter.hpp\"\n@@ -46,0 +48,1 @@\n+#include \"classfile\/modules.hpp\"\n@@ -47,1 +50,1 @@\n-#include \"classfile\/symbolTable.hpp\"\n+#include \"classfile\/symbolTable.hpp\"\n@@ -65,0 +68,1 @@\n+#include \"nmt\/memTracker.hpp\"\n@@ -79,1 +83,1 @@\n-#include \"runtime\/vmThread.hpp\"\n+#include \"runtime\/vmThread.hpp\"\n@@ -82,1 +86,0 @@\n-#include \"services\/memTracker.hpp\"\n@@ -85,1 +88,1 @@\n-#include \"utilities\/ostream.hpp\"\n+#include \"utilities\/ostream.hpp\"\n@@ -97,1 +100,0 @@\n-bool MetaspaceShared::_use_full_module_graph = true;\n@@ -245,1 +247,1 @@\n-  assert(DumpSharedSpaces, \"should be called for dump time only\");\n+  assert(CDSConfig::is_dumping_static_archive(), \"sanity\");\n@@ -279,1 +281,1 @@\n-      if (!DynamicDumpSharedSpaces) {\n+      if (!CDSConfig::is_dumping_dynamic_archive()) {\n@@ -388,0 +390,1 @@\n+  CDS_JAVA_HEAP_ONLY(Modules::serialize(soc);)\n@@ -480,0 +483,2 @@\n+  \/\/ Write module name into archive\n+  CDS_JAVA_HEAP_ONLY(Modules::dump_main_module_name();)\n@@ -581,1 +586,1 @@\n-  if (DynamicDumpSharedSpaces && ik->is_shared_unregistered_class()) {\n+  if (CDSConfig::is_dumping_dynamic_archive() && ik->is_shared_unregistered_class()) {\n@@ -643,1 +648,1 @@\n-  Arguments::assert_is_dumping_archive();\n+  assert(CDSConfig::is_dumping_archive(), \"sanity\");\n@@ -670,1 +675,1 @@\n-  if (!DumpSharedSpaces || UseCompressedOops) {\n+  if (!CDSConfig::is_dumping_heap() || UseCompressedOops) {\n@@ -764,2 +769,0 @@\n-  HeapShared::init_for_dumping(CHECK);\n-\n@@ -777,4 +780,11 @@\n-  StringTable::allocate_shared_strings_array(CHECK);\n-  ArchiveHeapWriter::init();\n-  if (use_full_module_graph()) {\n-    HeapShared::reset_archived_object_states(CHECK);\n+  if (CDSConfig::is_dumping_heap()) {\n+    StringTable::allocate_shared_strings_array(CHECK);\n+    if (!HeapShared::is_archived_boot_layer_available(THREAD)) {\n+      log_info(cds)(\"archivedBootLayer not available, disabling full module graph\");\n+      CDSConfig::disable_dumping_full_module_graph();\n+    }\n+    HeapShared::init_for_dumping(CHECK);\n+    ArchiveHeapWriter::init();\n+    if (CDSConfig::is_dumping_full_module_graph()) {\n+      HeapShared::reset_archived_object_states(CHECK);\n+    }\n@@ -792,1 +802,1 @@\n-  Arguments::assert_is_dumping_archive();\n+  assert(CDSConfig::is_dumping_archive(), \"sanity\");\n@@ -941,1 +951,1 @@\n-    if (DynamicDumpSharedSpaces) {\n+    if (CDSConfig::is_dumping_dynamic_archive()) {\n@@ -947,1 +957,1 @@\n-    DynamicDumpSharedSpaces = false;\n+    CDSConfig::disable_dumping_dynamic_archive();\n@@ -983,1 +993,1 @@\n-  if (DynamicDumpSharedSpaces) {\n+  if (CDSConfig::is_dumping_dynamic_archive()) {\n@@ -1162,2 +1172,2 @@\n-    log_info(cds)(\"optimized module handling: %s\", MetaspaceShared::use_optimized_module_handling() ? \"enabled\" : \"disabled\");\n-    log_info(cds)(\"full module graph: %s\", MetaspaceShared::use_full_module_graph() ? \"enabled\" : \"disabled\");\n+    log_info(cds)(\"initial optimized module handling: %s\", MetaspaceShared::use_optimized_module_handling() ? \"enabled\" : \"disabled\");\n+    log_info(cds)(\"initial full module graph: %s\", CDSConfig::is_loading_full_module_graph() ? \"enabled\" : \"disabled\");\n@@ -1478,1 +1488,1 @@\n-  if (DynamicDumpSharedSpaces) {\n+  if (CDSConfig::is_dumping_dynamic_archive()) {\n@@ -1537,23 +1547,0 @@\n-bool MetaspaceShared::use_full_module_graph() {\n-#if INCLUDE_CDS_JAVA_HEAP\n-  if (ClassLoaderDataShared::is_full_module_graph_loaded()) {\n-    return true;\n-  }\n-#endif\n-  bool result = _use_optimized_module_handling && _use_full_module_graph;\n-  if (DumpSharedSpaces) {\n-    result &= HeapShared::can_write();\n-  } else if (UseSharedSpaces) {\n-    result &= ArchiveHeapLoader::can_use();\n-  } else {\n-    result = false;\n-  }\n-\n-  if (result && UseSharedSpaces) {\n-    \/\/ Classes used by the archived full module graph are loaded in JVMTI early phase.\n-    assert(!(JvmtiExport::should_post_class_file_load_hook() && JvmtiExport::has_early_class_hook_env()),\n-           \"CDS should be disabled if early class hooks are enabled\");\n-  }\n-  return result;\n-}\n-\n","filename":"src\/hotspot\/share\/cds\/metaspaceShared.cpp","additions":34,"deletions":47,"binary":false,"changes":81,"status":"modified"},{"patch":"@@ -98,2 +98,1 @@\n-VtableStub* VtableStubs::_table[VtableStubs::N];\n-int VtableStubs::_number_of_vtable_stubs = 0;\n+VtableStub* volatile VtableStubs::_table[VtableStubs::N];\n@@ -129,0 +128,2 @@\n+  assert(VtableStub::_receiver_location == VMRegImpl::Bad(), \"initialized multiple times?\");\n+\n@@ -132,3 +133,1 @@\n-    assert(_number_of_vtable_stubs == 0, \"potential performance bug: VtableStubs initialized more than once\");\n-    assert(is_power_of_2(int(N)), \"N must be a power of 2\");\n-      _table[i] = nullptr;\n+      Atomic::store(&_table[i], (VtableStub*)nullptr);\n@@ -262,1 +261,1 @@\n-  VtableStub* s = _table[hash];\n+  VtableStub* s = Atomic::load(&_table[hash]);\n@@ -272,4 +271,4 @@\n-  \/\/ enter s at the beginning of the corresponding list\n-  s->set_next(_table[h]);\n-  _table[h] = s;\n-  _number_of_vtable_stubs++;\n+  \/\/ Insert s at the beginning of the corresponding list.\n+  s->set_next(Atomic::load(&_table[h]));\n+  \/\/ Make sure that concurrent readers not taking the mutex observe the writing of \"next\".\n+  Atomic::release_store(&_table[h], s);\n@@ -283,1 +282,1 @@\n-  for (s = _table[hash]; s != nullptr && s != stub; s = s->next()) {}\n+  for (s = Atomic::load(&_table[hash]); s != nullptr && s != stub; s = s->next()) {}\n@@ -287,0 +286,7 @@\n+bool VtableStubs::is_icholder_entry(address pc) {\n+  assert(contains(pc), \"must contain all vtable blobs\");\n+  VtableStub* stub = (VtableStub*)(pc - VtableStub::entry_offset());\n+  \/\/ itable stubs use CompiledICHolder.\n+  return stub->is_itable_stub();\n+}\n+\n@@ -295,4 +301,1 @@\n-  \/\/ Note: No locking needed since any change to the data structure\n-  \/\/       happens with an atomic store into it (we don't care about\n-  \/\/       consistency with the _number_of_vtable_stubs counter).\n-    for (VtableStub* s = _table[i]; s != nullptr; s = s->next()) {\n+    for (VtableStub* s = Atomic::load_acquire(&_table[i]); s != nullptr; s = s->next()) {\n@@ -311,4 +314,3 @@\n-    for (int i = 0; i < N; i++) {\n-        for (VtableStub* s = _table[i]; s != nullptr; s = s->next()) {\n-            f(s);\n-        }\n+  for (int i = 0; i < N; i++) {\n+    for (VtableStub* s = Atomic::load_acquire(&_table[i]); s != nullptr; s = s->next()) {\n+      f(s);\n@@ -316,0 +318,1 @@\n+  }\n","filename":"src\/hotspot\/share\/code\/vtableStubs.cpp","additions":22,"deletions":19,"binary":false,"changes":41,"status":"modified"},{"patch":"@@ -327,4 +327,7 @@\n-    CodeCache::UnloadingScope unloading_scope(&_is_alive);\n-    \/\/ Unload classes and purge the SystemDictionary.\n-    bool purged_class = SystemDictionary::do_unloading(scope()->timer());\n-    _heap->complete_cleaning(purged_class);\n+    {\n+      CodeCache::UnlinkingScope unloading_scope(&_is_alive);\n+      \/\/ Unload classes and purge the SystemDictionary.\n+      bool unloading_occurred = SystemDictionary::do_unloading(scope()->timer());\n+      _heap->complete_cleaning(unloading_occurred);\n+    }\n+    CodeCache::flush_unlinked_nmethods();\n","filename":"src\/hotspot\/share\/gc\/g1\/g1FullCollector.cpp","additions":7,"deletions":4,"binary":false,"changes":11,"status":"modified"},{"patch":"@@ -237,8 +237,3 @@\n-      \/\/ In PSCardTable::scavenge_contents_parallel(), when work is distributed\n-      \/\/ among different workers, an object is never split between multiple workers.\n-      \/\/ Therefore, if a worker gets owned a large objArray, it may accumulate\n-      \/\/ many tasks (corresponding to every element in this array) in its\n-      \/\/ task queue. When there are too many overflow tasks, publishing them\n-      \/\/ (via try_push_to_taskqueue()) can incur noticeable overhead in Young GC\n-      \/\/ pause, so it is better to process them locally until large-objArray-splitting is implemented.\n-      process_popped_location_depth(task);\n+      if (!tq->try_push_to_taskqueue(task)) {\n+        process_popped_location_depth(task);\n+      }\n","filename":"src\/hotspot\/share\/gc\/parallel\/psPromotionManager.cpp","additions":3,"deletions":8,"binary":false,"changes":11,"status":"modified"},{"patch":"@@ -136,5 +136,0 @@\n-  bool young_gen_is_full()             { return _young_gen_is_full; }\n-\n-  bool old_gen_is_full()               { return _old_gen_is_full; }\n-  void set_old_gen_is_full(bool state) { _old_gen_is_full = state; }\n-\n@@ -180,0 +175,1 @@\n+  void push_contents_bounded(oop obj, HeapWord* left, HeapWord* right);\n","filename":"src\/hotspot\/share\/gc\/parallel\/psPromotionManager.hpp","additions":1,"deletions":5,"binary":false,"changes":6,"status":"modified"},{"patch":"@@ -134,0 +134,5 @@\n+inline void PSPromotionManager::push_contents_bounded(oop obj, HeapWord* left, HeapWord* right) {\n+  PSPushContentsClosure pcc(this);\n+  obj->oop_iterate(&pcc, MemRegion(left, right));\n+}\n+\n","filename":"src\/hotspot\/share\/gc\/parallel\/psPromotionManager.inline.hpp","additions":5,"deletions":0,"binary":false,"changes":5,"status":"modified"},{"patch":"@@ -37,0 +37,1 @@\n+#include \"gc\/serial\/generation.hpp\"\n@@ -45,1 +46,0 @@\n-#include \"gc\/shared\/generation.hpp\"\n@@ -207,3 +207,3 @@\n-    CodeCache::UnloadingScope scope(&is_alive);\n-    \/\/ Unload classes and purge the SystemDictionary.\n-    bool purged_class = SystemDictionary::do_unloading(gc_timer());\n+    bool unloading_occurred;\n+    {\n+      CodeCache::UnlinkingScope scope(&is_alive);\n@@ -212,2 +212,9 @@\n-    \/\/ Unload nmethods.\n-    CodeCache::do_unloading(purged_class);\n+      \/\/ Unload classes and purge the SystemDictionary.\n+      unloading_occurred = SystemDictionary::do_unloading(gc_timer());\n+\n+      \/\/ Unload nmethods.\n+      CodeCache::do_unloading(unloading_occurred);\n+    }\n+\n+    \/\/ Release unloaded nmethod's memory.\n+    CodeCache::flush_unlinked_nmethods();\n@@ -216,1 +223,1 @@\n-    Klass::clean_weak_klass_links(purged_class);\n+    Klass::clean_weak_klass_links(unloading_occurred);\n@@ -219,1 +226,1 @@\n-    JVMCI_ONLY(JVMCI::do_unloading(purged_class));\n+    JVMCI_ONLY(JVMCI::do_unloading(unloading_occurred));\n","filename":"src\/hotspot\/share\/gc\/serial\/genMarkSweep.cpp","additions":15,"deletions":8,"binary":false,"changes":23,"status":"modified"},{"patch":"@@ -26,0 +26,1 @@\n+#include \"cds\/cdsConfig.hpp\"\n@@ -467,1 +468,1 @@\n-  if (DumpSharedSpaces) {\n+  if (CDSConfig::is_dumping_heap()) {\n","filename":"src\/hotspot\/share\/gc\/shared\/collectedHeap.cpp","additions":2,"deletions":1,"binary":false,"changes":3,"status":"modified"},{"patch":"@@ -50,18 +50,0 @@\n-void Space::initialize(MemRegion mr,\n-                       bool clear_space,\n-                       bool mangle_space) {\n-  HeapWord* bottom = mr.start();\n-  HeapWord* end    = mr.end();\n-  assert(Universe::on_page_boundary(bottom) && Universe::on_page_boundary(end),\n-         \"invalid space boundaries\");\n-  set_bottom(bottom);\n-  set_end(end);\n-  if (clear_space) clear(mangle_space);\n-}\n-\n-void Space::clear(bool mangle_space) {\n-  if (ZapUnusedHeapArea && mangle_space) {\n-    mangle_unused_area();\n-  }\n-}\n-\n@@ -83,2 +65,10 @@\n-  Space::initialize(mr, clear_space, mangle_space);\n-  set_compaction_top(bottom());\n+  HeapWord* bottom = mr.start();\n+  HeapWord* end    = mr.end();\n+  assert(Universe::on_page_boundary(bottom) && Universe::on_page_boundary(end),\n+         \"invalid space boundaries\");\n+  set_bottom(bottom);\n+  set_end(end);\n+  if (clear_space) {\n+    clear(mangle_space);\n+  }\n+  set_compaction_top(bottom);\n@@ -91,1 +81,3 @@\n-  Space::clear(mangle_space);\n+  if (ZapUnusedHeapArea && mangle_space) {\n+    mangle_unused_area();\n+  }\n@@ -99,19 +91,0 @@\n-#if INCLUDE_SERIALGC\n-void TenuredSpace::clear(bool mangle_space) {\n-  ContiguousSpace::clear(mangle_space);\n-  _offsets.initialize_threshold();\n-}\n-\n-void TenuredSpace::set_bottom(HeapWord* new_bottom) {\n-  Space::set_bottom(new_bottom);\n-  _offsets.set_bottom(new_bottom);\n-}\n-\n-void TenuredSpace::set_end(HeapWord* new_end) {\n-  \/\/ Space should not advertise an increase in size\n-  \/\/ until after the underlying offset table has been enlarged.\n-  _offsets.resize(pointer_delta(new_end, bottom()));\n-  Space::set_end(new_end);\n-}\n-#endif \/\/ INCLUDE_SERIALGC\n-\n@@ -164,1 +137,0 @@\n-    cp->space->initialize_threshold();\n@@ -184,1 +156,1 @@\n-  cp->space->alloc_block(compact_top - size, compact_top);\n+  cp->space->update_for_block(compact_top - size, compact_top);\n@@ -203,1 +175,0 @@\n-    cp->space->initialize_threshold();\n@@ -423,3 +394,2 @@\n-  st->print_cr(\" [\" PTR_FORMAT \", \" PTR_FORMAT \", \"\n-                PTR_FORMAT \", \" PTR_FORMAT \")\",\n-              p2i(bottom()), p2i(top()), p2i(_offsets.threshold()), p2i(end()));\n+  st->print_cr(\" [\" PTR_FORMAT \", \" PTR_FORMAT \", \" PTR_FORMAT \")\",\n+              p2i(bottom()), p2i(top()), p2i(end()));\n@@ -432,1 +402,0 @@\n-  HeapWord* prev_p = nullptr;\n@@ -435,1 +404,0 @@\n-    prev_p = p;\n@@ -439,5 +407,0 @@\n-  if (top() != end()) {\n-    guarantee(top() == block_start_const(end()-1) &&\n-              top() == block_start_const(top()),\n-              \"top should be start of unallocated block, if it exists\");\n-  }\n@@ -549,2 +512,2 @@\n-void TenuredSpace::initialize_threshold() {\n-  _offsets.initialize_threshold();\n+void TenuredSpace::update_for_block(HeapWord* start, HeapWord* end) {\n+  _offsets.update_for_block(start, end);\n@@ -553,2 +516,14 @@\n-void TenuredSpace::alloc_block(HeapWord* start, HeapWord* end) {\n-  _offsets.alloc_block(start, end);\n+HeapWord* TenuredSpace::block_start_const(const void* addr) const {\n+  HeapWord* cur_block = _offsets.block_start_reaching_into_card(addr);\n+\n+  while (true) {\n+    HeapWord* next_block = cur_block + cast_to_oop(cur_block)->size();\n+    if (next_block > addr) {\n+      assert(cur_block <= addr, \"postcondition\");\n+      return cur_block;\n+    }\n+    cur_block = next_block;\n+    \/\/ Because the BOT is precise, we should never step into the next card\n+    \/\/ (i.e. crossing the card boundary).\n+    assert(!SerialBlockOffsetTable::is_crossing_card_boundary(cur_block, (HeapWord*)addr), \"must be\");\n+  }\n@@ -557,1 +532,1 @@\n-TenuredSpace::TenuredSpace(BlockOffsetSharedArray* sharedOffsetArray,\n+TenuredSpace::TenuredSpace(SerialBlockOffsetSharedArray* sharedOffsetArray,\n@@ -559,2 +534,1 @@\n-  _offsets(sharedOffsetArray, mr),\n-  _par_alloc_lock(Mutex::safepoint, \"TenuredSpaceParAlloc_lock\", true)\n+  _offsets(sharedOffsetArray)\n@@ -562,1 +536,0 @@\n-  _offsets.set_contig_space(this);\n@@ -566,38 +539,0 @@\n-#define OBJ_SAMPLE_INTERVAL 0\n-#define BLOCK_SAMPLE_INTERVAL 100\n-\n-void TenuredSpace::verify() const {\n-  HeapWord* p = bottom();\n-  HeapWord* prev_p = nullptr;\n-  int objs = 0;\n-  int blocks = 0;\n-\n-  if (VerifyObjectStartArray) {\n-    _offsets.verify();\n-  }\n-\n-  while (p < top()) {\n-    size_t size = cast_to_oop(p)->size();\n-    \/\/ For a sampling of objects in the space, find it using the\n-    \/\/ block offset table.\n-    if (blocks == BLOCK_SAMPLE_INTERVAL) {\n-      guarantee(p == block_start_const(p + (size\/2)),\n-                \"check offset computation\");\n-      blocks = 0;\n-    } else {\n-      blocks++;\n-    }\n-\n-    if (objs == OBJ_SAMPLE_INTERVAL) {\n-      oopDesc::verify(cast_to_oop(p));\n-      objs = 0;\n-    } else {\n-      objs++;\n-    }\n-    prev_p = p;\n-    p += size;\n-  }\n-  guarantee(p == top(), \"end of last object must match end of space\");\n-}\n-\n-\n","filename":"src\/hotspot\/share\/gc\/shared\/space.cpp","additions":34,"deletions":99,"binary":false,"changes":133,"status":"modified"},{"patch":"@@ -50,5 +50,0 @@\n-#if INCLUDE_SERIALGC\n-class BlockOffsetArray;\n-class BlockOffsetArrayContigSpace;\n-class BlockOffsetTable;\n-#endif\n@@ -89,1 +84,1 @@\n-  virtual HeapWord* saved_mark_word() const  { return _saved_mark_word; }\n+  HeapWord* saved_mark_word() const  { return _saved_mark_word; }\n@@ -95,1 +90,1 @@\n-  virtual bool obj_allocated_since_save_marks(const oop obj) const {\n+  bool obj_allocated_since_save_marks(const oop obj) const {\n@@ -115,11 +110,0 @@\n-  \/\/ Initialization.\n-  \/\/ \"initialize\" should be called once on a space, before it is used for\n-  \/\/ any purpose.  The \"mr\" arguments gives the bounds of the space, and\n-  \/\/ the \"clear_space\" argument should be true unless the memory in \"mr\" is\n-  \/\/ known to be zeroed.\n-  virtual void initialize(MemRegion mr, bool clear_space, bool mangle_space);\n-\n-  \/\/ The \"clear\" method must be called on a region that may have\n-  \/\/ had allocation performed in it, but is now to be considered empty.\n-  virtual void clear(bool mangle_space);\n-\n@@ -181,1 +165,1 @@\n-  virtual HeapWord* block_start(const void* p);\n+  HeapWord* block_start(const void* p);\n@@ -194,1 +178,1 @@\n-  virtual bool obj_is_alive(const HeapWord* addr) const;\n+  bool obj_is_alive(const HeapWord* addr) const;\n@@ -209,1 +193,1 @@\n-  virtual void print() const;\n+  void print() const;\n@@ -211,5 +195,2 @@\n-  virtual void print_short() const;\n-  virtual void print_short_on(outputStream* st) const;\n-\n-  \/\/ Debugging\n-  virtual void verify() const = 0;\n+  void print_short() const;\n+  void print_short_on(outputStream* st) const;\n@@ -266,1 +247,1 @@\n-  virtual void alloc_block(HeapWord* start, HeapWord* the_end) { }\n+  virtual void update_for_block(HeapWord* start, HeapWord* the_end) { }\n@@ -278,1 +259,6 @@\n-  void initialize(MemRegion mr, bool clear_space, bool mangle_space) override;\n+  \/\/ Initialization.\n+  \/\/ \"initialize\" should be called once on a space, before it is used for\n+  \/\/ any purpose.  The \"mr\" arguments gives the bounds of the space, and\n+  \/\/ the \"clear_space\" argument should be true unless the memory in \"mr\" is\n+  \/\/ known to be zeroed.\n+  void initialize(MemRegion mr, bool clear_space, bool mangle_space);\n@@ -280,1 +266,3 @@\n-  void clear(bool mangle_space) override;\n+  \/\/ The \"clear\" method must be called on a region that may have\n+  \/\/ had allocation performed in it, but is now to be considered empty.\n+  virtual void clear(bool mangle_space);\n@@ -326,5 +314,0 @@\n-  \/\/ Some contiguous spaces may maintain some data structures that should\n-  \/\/ be updated whenever an allocation crosses a boundary.  This function\n-  \/\/ initializes these data structures for further updates.\n-  virtual void initialize_threshold() { }\n-\n@@ -340,1 +323,1 @@\n-  \/\/ Invokes the \"alloc_block\" function of the then-current compaction\n+  \/\/ Invokes the \"update_for_block\" function of the then-current compaction\n@@ -425,1 +408,1 @@\n-  void verify() const override;\n+  void verify() const;\n@@ -431,2 +414,1 @@\n-\/\/ \"block_start\" operation via a BlockOffsetArray (whose BlockOffsetSharedArray\n-\/\/ may be shared with other spaces.)\n+\/\/ \"block_start\" operation via a SerialBlockOffsetTable.\n@@ -437,2 +419,1 @@\n-  BlockOffsetArrayContigSpace _offsets;\n-  Mutex _par_alloc_lock;\n+  SerialBlockOffsetTable _offsets;\n@@ -444,1 +425,1 @@\n-  TenuredSpace(BlockOffsetSharedArray* sharedOffsetArray,\n+  TenuredSpace(SerialBlockOffsetSharedArray* sharedOffsetArray,\n@@ -447,6 +428,1 @@\n-  void set_bottom(HeapWord* value) override;\n-  void set_end(HeapWord* value) override;\n-\n-  void clear(bool mangle_space) override;\n-\n-  inline HeapWord* block_start_const(const void* p) const override;\n+  HeapWord* block_start_const(const void* addr) const override;\n@@ -459,2 +435,1 @@\n-  void initialize_threshold() override;\n-  void alloc_block(HeapWord* start, HeapWord* end) override;\n+  void update_for_block(HeapWord* start, HeapWord* end) override;\n@@ -463,3 +438,0 @@\n-\n-  \/\/ Debugging\n-  void verify() const override;\n","filename":"src\/hotspot\/share\/gc\/shared\/space.hpp","additions":24,"deletions":52,"binary":false,"changes":76,"status":"modified"},{"patch":"@@ -80,0 +80,2 @@\n+#include \"nmt\/mallocTracker.hpp\"\n+#include \"nmt\/memTracker.hpp\"\n@@ -89,2 +91,0 @@\n-#include \"services\/mallocTracker.hpp\"\n-#include \"services\/memTracker.hpp\"\n@@ -509,0 +509,1 @@\n+  _gc_no_progress_count(0),\n@@ -879,1 +880,13 @@\n-    \/\/ Allocation failed, block until control thread reacted, then retry allocation.\n+    \/\/ Check that gc overhead is not exceeded.\n+    \/\/\n+    \/\/ Shenandoah will grind along for quite a while allocating one\n+    \/\/ object at a time using shared (non-tlab) allocations. This check\n+    \/\/ is testing that the GC overhead limit has not been exceeded.\n+    \/\/ This will notify the collector to start a cycle, but will raise\n+    \/\/ an OOME to the mutator if the last Full GCs have not made progress.\n+    if (result == nullptr && !req.is_lab_alloc() && get_gc_no_progress_count() > ShenandoahNoProgressThreshold) {\n+      control_thread()->handle_alloc_failure(req, false);\n+      return nullptr;\n+    }\n+\n+    \/\/ Block until control thread reacted, then retry allocation.\n@@ -888,1 +901,1 @@\n-        && (_progress_last_gc.is_set() || original_count == shenandoah_policy()->full_gc_count())) {\n+        && (get_gc_no_progress_count() == 0 || original_count == shenandoah_policy()->full_gc_count())) {\n@@ -892,0 +905,6 @@\n+\n+    if (log_is_enabled(Debug, gc, alloc)) {\n+      ResourceMark rm;\n+      log_debug(gc, alloc)(\"Thread: %s, Result: \" PTR_FORMAT \", Request: %s, Size: \" SIZE_FORMAT \", Original: \" SIZE_FORMAT \", Latest: \" SIZE_FORMAT,\n+                           Thread::current()->name(), p2i(result), req.type_string(), req.size(), original_count, get_gc_no_progress_count());\n+    }\n@@ -1817,8 +1836,12 @@\n-    CodeCache::UnloadingScope scope(is_alive.is_alive_closure());\n-    ShenandoahGCPhase gc_phase(phase);\n-    ShenandoahGCWorkerPhase worker_phase(phase);\n-    bool purged_class = SystemDictionary::do_unloading(gc_timer());\n-\n-    uint num_workers = _workers->active_workers();\n-    ShenandoahClassUnloadingTask unlink_task(phase, num_workers, purged_class);\n-    _workers->run_task(&unlink_task);\n+    {\n+      CodeCache::UnlinkingScope scope(is_alive.is_alive_closure());\n+      ShenandoahGCPhase gc_phase(phase);\n+      ShenandoahGCWorkerPhase worker_phase(phase);\n+      bool unloading_occurred = SystemDictionary::do_unloading(gc_timer());\n+\n+      uint num_workers = _workers->active_workers();\n+      ShenandoahClassUnloadingTask unlink_task(phase, num_workers, unloading_occurred);\n+      _workers->run_task(&unlink_task);\n+    }\n+    \/\/ Release unloaded nmethods's memory.\n+    CodeCache::flush_unlinked_nmethods();\n@@ -1899,8 +1922,0 @@\n-address ShenandoahHeap::cancelled_gc_addr() {\n-  return (address) ShenandoahHeap::heap()->_cancelled_gc.addr_of();\n-}\n-\n-address ShenandoahHeap::gc_state_addr() {\n-  return (address) ShenandoahHeap::heap()->_gc_state.addr_of();\n-}\n-\n","filename":"src\/hotspot\/share\/gc\/shenandoah\/shenandoahHeap.cpp","additions":35,"deletions":20,"binary":false,"changes":55,"status":"modified"},{"patch":"@@ -76,0 +76,12 @@\n+inline void ShenandoahHeap::notify_gc_progress() {\n+  Atomic::store(&_gc_no_progress_count, (size_t) 0);\n+\n+}\n+inline void ShenandoahHeap::notify_gc_no_progress() {\n+  Atomic::inc(&_gc_no_progress_count);\n+}\n+\n+inline size_t ShenandoahHeap::get_gc_no_progress_count() const {\n+  return Atomic::load(&_gc_no_progress_count);\n+}\n+\n@@ -399,4 +411,0 @@\n-inline bool ShenandoahHeap::is_gc_in_progress_mask(uint mask) const {\n-  return _gc_state.is_set(mask);\n-}\n-\n","filename":"src\/hotspot\/share\/gc\/shenandoah\/shenandoahHeap.inline.hpp","additions":12,"deletions":4,"binary":false,"changes":16,"status":"modified"},{"patch":"@@ -26,0 +26,1 @@\n+#include \"cds\/cdsConfig.hpp\"\n@@ -50,0 +51,1 @@\n+#include \"oops\/resolvedMethodEntry.hpp\"\n@@ -53,1 +55,0 @@\n-#include \"runtime\/arguments.hpp\"\n@@ -2281,1 +2282,1 @@\n-        ConstantPoolCacheEntry* cache = cp->entry_at(index);\n+        ResolvedMethodEntry* entry = cp->resolved_method_entry_at(index);\n@@ -2283,1 +2284,1 @@\n-        if (! cache->is_resolved((Bytecodes::Code) opcode)) {\n+        if (! entry->is_resolved((Bytecodes::Code) opcode)) {\n@@ -2286,1 +2287,1 @@\n-          cache = cp->entry_at(index);\n+          entry = cp->resolved_method_entry_at(index);\n@@ -2289,1 +2290,1 @@\n-        Method* method = cache->f1_as_method();\n+        Method* method = entry->method();\n@@ -2292,1 +2293,1 @@\n-        if (cache->has_appendix()) {\n+        if (entry->has_appendix()) {\n@@ -2294,1 +2295,1 @@\n-          SET_STACK_OBJECT(cache->appendix_if_resolved(cp), 0);\n+          SET_STACK_OBJECT(cp->cache()->appendix_if_resolved(entry), 0);\n@@ -2312,2 +2313,2 @@\n-        ConstantPoolCacheEntry* cache = cp->entry_at(index);\n-        if (!cache->is_resolved((Bytecodes::Code)opcode)) {\n+        ResolvedMethodEntry* entry = cp->resolved_method_entry_at(index);\n+        if (!entry->is_resolved((Bytecodes::Code)opcode)) {\n@@ -2316,1 +2317,0 @@\n-          cache = cp->entry_at(index);\n@@ -2324,4 +2324,4 @@\n-        if (cache->is_forced_virtual()) {\n-          CHECK_NULL(STACK_OBJECT(-(cache->parameter_size())));\n-          if (cache->is_vfinal()) {\n-            callee = cache->f2_as_vfinal_method();\n+        if (entry->is_forced_virtual()) {\n+          CHECK_NULL(STACK_OBJECT(-(entry->number_of_parameters())));\n+          if (entry->is_vfinal()) {\n+            callee = entry->method();\n@@ -2330,1 +2330,1 @@\n-            int parms = cache->parameter_size();\n+            int parms = entry->number_of_parameters();\n@@ -2335,1 +2335,1 @@\n-            callee = (Method*) rcvrKlass->method_at_vtable(cache->f2_as_index());\n+            callee = (Method*) rcvrKlass->method_at_vtable(entry->table_index());\n@@ -2337,1 +2337,1 @@\n-        } else if (cache->is_vfinal()) {\n+        } else if (entry->is_vfinal()) {\n@@ -2344,1 +2344,1 @@\n-          int parms = cache->parameter_size();\n+          int parms = entry->number_of_parameters();\n@@ -2348,1 +2348,1 @@\n-          Klass* resolved_klass = cache->f1_as_klass();\n+          Klass* resolved_klass = entry->interface_klass();\n@@ -2357,1 +2357,1 @@\n-          callee = cache->f2_as_vfinal_method();\n+          callee = entry->method();\n@@ -2370,1 +2370,1 @@\n-        Method *interface_method = cache->f2_as_interface_method();\n+        Method *interface_method = entry->method();\n@@ -2374,1 +2374,1 @@\n-        int parms = cache->parameter_size();\n+        int parms = entry->number_of_parameters();\n@@ -2381,1 +2381,1 @@\n-          Klass* refc = cache->f1_as_klass();\n+          Klass* refc = entry->interface_klass();\n@@ -2434,1 +2434,1 @@\n-        ConstantPoolCacheEntry* cache = cp->entry_at(index);\n+        ResolvedMethodEntry* entry = cp->resolved_method_entry_at(index);\n@@ -2438,1 +2438,1 @@\n-        if (!cache->is_resolved((Bytecodes::Code)opcode)) {\n+        if (!entry->is_resolved((Bytecodes::Code)opcode)) {\n@@ -2441,1 +2441,1 @@\n-          cache = cp->entry_at(index);\n+          entry = cp->resolved_method_entry_at(index);\n@@ -2448,4 +2448,4 @@\n-            CHECK_NULL(STACK_OBJECT(-(cache->parameter_size())));\n-            if (cache->is_vfinal()) {\n-              callee = cache->f2_as_vfinal_method();\n-              if (REWRITE_BYTECODES && !UseSharedSpaces && !Arguments::is_dumping_archive()) {\n+            CHECK_NULL(STACK_OBJECT(-(entry->number_of_parameters())));\n+            if (entry->is_vfinal()) {\n+              callee = entry->method();\n+              if (REWRITE_BYTECODES && !UseSharedSpaces && !CDSConfig::is_dumping_archive()) {\n@@ -2457,1 +2457,1 @@\n-              int parms = cache->parameter_size();\n+              int parms = entry->number_of_parameters();\n@@ -2483,1 +2483,1 @@\n-              callee = (Method*) rcvrKlass->method_at_vtable(cache->f2_as_index());\n+              callee = (Method*) rcvrKlass->method_at_vtable(entry->table_index());\n@@ -2487,1 +2487,1 @@\n-              CHECK_NULL(STACK_OBJECT(-(cache->parameter_size())));\n+              CHECK_NULL(STACK_OBJECT(-(entry->number_of_parameters())));\n@@ -2489,1 +2489,1 @@\n-            callee = cache->f1_as_method();\n+            callee = entry->method();\n@@ -2894,1 +2894,1 @@\n-        ConstantPoolCacheEntry* cache = cp->entry_at(index);\n+        ResolvedMethodEntry* entry = cp->resolved_method_entry_at(index);\n@@ -2896,1 +2896,1 @@\n-        assert(cache->is_resolved(Bytecodes::_invokevirtual), \"Should be resolved before rewriting\");\n+        assert(entry->is_resolved(Bytecodes::_invokevirtual), \"Should be resolved before rewriting\");\n@@ -2900,2 +2900,2 @@\n-        CHECK_NULL(STACK_OBJECT(-(cache->parameter_size())));\n-        Method* callee = cache->f2_as_vfinal_method();\n+        CHECK_NULL(STACK_OBJECT(-(entry->number_of_parameters())));\n+        Method* callee = entry->method();\n","filename":"src\/hotspot\/share\/interpreter\/zero\/bytecodeInterpreter.cpp","additions":38,"deletions":38,"binary":false,"changes":76,"status":"modified"},{"patch":"@@ -168,1 +168,1 @@\n-  JavaThread* THREAD = thread;                                        \\\n+  JavaThread* THREAD = thread;                                    \\\n@@ -172,4 +172,9 @@\n-#define C2V_BLOCK(result_type, name, signature)      \\\n-  JVMCI_VM_ENTRY_MARK;                               \\\n-  ResourceMark rm;                                   \\\n-  JVMCIENV_FROM_JNI(JVMCI::compilation_tick(thread), env);\n+\/\/ Note: CompilerThreadCanCallJava must precede JVMCIENV_FROM_JNI so that\n+\/\/ the translation of an uncaught exception in the JVMCIEnv does not make\n+\/\/ a Java call when __is_hotspot == false.\n+#define C2V_BLOCK(result_type, name, signature)            \\\n+  JVMCI_VM_ENTRY_MARK;                                     \\\n+  ResourceMark rm;                                         \\\n+  bool __is_hotspot = env == thread->jni_environment();    \\\n+  CompilerThreadCanCallJava ccj(thread, __is_hotspot);     \\\n+  JVMCIENV_FROM_JNI(JVMCI::compilation_tick(thread), env); \\\n@@ -191,1 +196,1 @@\n-  if (thread == nullptr) {                                  \\\n+  if (thread == nullptr) {                               \\\n@@ -202,1 +207,1 @@\n-  if (thread == nullptr) {                                  \\\n+  if (thread == nullptr) {                               \\\n@@ -224,1 +229,1 @@\n-      tty->print_cr(\"Throwing \" #name \" in \" caller \" returned %d\", __throw_res); \\\n+      JVMCI_event_1(\"Throwing \" #name \" in \" caller \" returned %d\", __throw_res); \\\n@@ -232,1 +237,1 @@\n-      tty->print_cr(\"Throwing \" #name \" in \" caller \" returned %d\", __throw_res); \\\n+      JVMCI_event_1(\"Throwing \" #name \" in \" caller \" returned %d\", __throw_res); \\\n@@ -586,0 +591,1 @@\n+  CompilerThreadCanCallJava canCallJava(thread, resolve); \/\/ Resolution requires Java calls\n@@ -599,1 +605,1 @@\n-    } else if (strstr(val, str) != nullptr) {\n+    } else if (strstr(str, val) != nullptr) {\n@@ -935,5 +941,0 @@\n-C2V_VMENTRY_0(jint, constantPoolRemapInstructionOperandFromCache, (JNIEnv* env, jobject, ARGUMENT_PAIR(cp), jint index))\n-  constantPoolHandle cp(THREAD, UNPACK_PAIR(ConstantPool, cp));\n-  return cp->remap_instruction_operand_from_cache(index);\n-C2V_END\n-\n@@ -945,0 +946,11 @@\n+\n+  Bytecodes::Code bc = (Bytecodes::Code) (((int) opcode) & 0xFF);\n+  int holder_index = cp->klass_ref_index_at(index, bc);\n+  if (!cp->tag_at(holder_index).is_klass() && !THREAD->can_call_java()) {\n+    \/\/ If the holder is not resolved in the constant pool and the current\n+    \/\/ thread cannot call Java, return null. This avoids a Java call\n+    \/\/ in LinkInfo to load the holder.\n+    Symbol* klass_name = cp->klass_ref_at_noresolve(index, bc);\n+    return nullptr;\n+  }\n+\n@@ -1650,0 +1662,8 @@\n+C2V_VMENTRY_0(int, decodeMethodIndexToCPIndex, (JNIEnv* env, jobject, ARGUMENT_PAIR(cp), jint method_index))\n+  constantPoolHandle cp(THREAD, UNPACK_PAIR(ConstantPool, cp));\n+  if (method_index < 0 || method_index >= cp->resolved_method_entries_length()) {\n+    JVMCI_THROW_MSG_0(IllegalStateException, err_msg(\"invalid method index %d\", method_index));\n+  }\n+  return cp->resolved_method_entry_at(method_index)->constant_pool_index();\n+C2V_END\n+\n@@ -1657,2 +1677,1 @@\n-    ConstantPoolCacheEntry* cp_cache_entry = cp->cache()->entry_at(cp->decode_cpcache_index(index));\n-    cp_cache_entry->set_method_handle(cp, callInfo);\n+    cp->cache()->set_method_handle(index, callInfo);\n@@ -1664,2 +1683,2 @@\n-  ConstantPoolCacheEntry* cp_cache_entry = cp->cache()->entry_at(cp->decode_cpcache_index(index));\n-  if (cp_cache_entry->is_resolved(Bytecodes::_invokehandle)) {\n+  ResolvedMethodEntry* entry = cp->cache()->resolved_method_entry_at(index);\n+  if (entry->is_resolved(Bytecodes::_invokehandle)) {\n@@ -1678,1 +1697,1 @@\n-    methodHandle adapter_method(THREAD, cp_cache_entry->f1_as_method());\n+    methodHandle adapter_method(THREAD, entry->method());\n@@ -1687,1 +1706,1 @@\n-      vmassert(cp_cache_entry->appendix_if_resolved(cp) == nullptr, \"!\");\n+      vmassert(cp->cache()->appendix_if_resolved(entry) == nullptr, \"!\");\n@@ -1697,1 +1716,1 @@\n-    if (cp->resolved_indy_entry_at(cp->decode_cpcache_index(index))->is_resolved()) {\n+    if (cp->resolved_indy_entry_at(cp->decode_invokedynamic_index(index))->is_resolved()) {\n@@ -1968,0 +1987,1 @@\n+  CompilerThreadCanCallJava canCallJava(thread, true); \/\/ Linking requires Java calls\n@@ -2443,0 +2463,10 @@\n+C2V_VMENTRY(void, clearOopHandle, (JNIEnv* env, jobject, jlong oop_handle))\n+  if (oop_handle == 0L) {\n+    JVMCI_THROW(NullPointerException);\n+  }\n+  \/\/ Assert before nulling out, for better debugging.\n+  assert(JVMCIRuntime::is_oop_handle(oop_handle), \"precondition\");\n+  oop* oop_ptr = (oop*) oop_handle;\n+  NativeAccess<>::oop_store(oop_ptr, (oop) nullptr);\n+C2V_END\n+\n@@ -2723,0 +2753,1 @@\n+  CompilerThreadCanCallJava canCallJava(thread, PEER_JVMCIENV->is_hotspot());\n@@ -2776,2 +2807,2 @@\n-        \/\/ Only HotSpotNmethod instances in the HotSpot heap are tracked directly by the runtime.\n-        if (PEER_JVMCIENV->is_hotspot()) {\n+        \/\/ Only non-default HotSpotNmethod instances in the HotSpot heap are tracked directly by the runtime.\n+        if (!isDefault && PEER_JVMCIENV->is_hotspot()) {\n@@ -2780,1 +2811,1 @@\n-            JVMCI_THROW_MSG_0(IllegalArgumentException, \"Cannot set HotSpotNmethod mirror for default nmethod\");\n+            JVMCI_THROW_MSG_0(IllegalArgumentException, \"Missing HotSpotNmethod data\");\n@@ -2942,0 +2973,1 @@\n+  CompilerThreadCanCallJava canCallJava(thread, true); \/\/ Requires Java support\n@@ -2948,0 +2980,1 @@\n+  CompilerThreadCanCallJava canCallJava(thread, true); \/\/ Requires Java support\n@@ -2954,0 +2987,1 @@\n+  CompilerThreadCanCallJava canCallJava(thread, true); \/\/ Requires Java support\n@@ -3010,0 +3044,1 @@\n+  CompilerThreadCanCallJava canCallJava(thread, true);\n@@ -3194,1 +3229,0 @@\n-  {CC \"constantPoolRemapInstructionOperandFromCache\", CC \"(\" HS_CONSTANT_POOL2 \"I)I\",                                                       FN_PTR(constantPoolRemapInstructionOperandFromCache)},\n@@ -3201,0 +3235,1 @@\n+  {CC \"decodeMethodIndexToCPIndex\",                   CC \"(\" HS_CONSTANT_POOL2 \"I)I\",                                                       FN_PTR(decodeMethodIndexToCPIndex)},\n@@ -3267,0 +3302,1 @@\n+  {CC \"clearOopHandle\",                               CC \"(J)V\",                                                                            FN_PTR(clearOopHandle)},\n","filename":"src\/hotspot\/share\/jvmci\/jvmciCompilerToVM.cpp","additions":62,"deletions":26,"binary":false,"changes":88,"status":"modified"},{"patch":"@@ -27,0 +27,1 @@\n+#include \"cds\/cdsConfig.hpp\"\n@@ -242,1 +243,1 @@\n-  assert(DumpSharedSpaces, \"dump-time only\");\n+  assert(CDSConfig::is_dumping_heap(), \"sanity\");\n@@ -491,1 +492,1 @@\n-    if (DumpSharedSpaces) {\n+    if (CDSConfig::is_dumping_heap()) {\n@@ -811,2 +812,0 @@\n-  \/\/ Create memory for metadata.  Must be after initializing heap for\n-  \/\/ DumpSharedSpaces.\n@@ -830,1 +829,1 @@\n-  if (Arguments::is_dumping_archive()) {\n+  if (CDSConfig::is_dumping_archive()) {\n","filename":"src\/hotspot\/share\/memory\/universe.cpp","additions":4,"deletions":5,"binary":false,"changes":9,"status":"modified"},{"patch":"@@ -27,0 +27,1 @@\n+#include \"cds\/cdsConfig.hpp\"\n@@ -51,1 +52,0 @@\n-#include \"runtime\/arguments.hpp\"\n@@ -88,1 +88,1 @@\n-  if (Arguments::is_dumping_archive() && is_instance_klass()) {\n+  if (CDSConfig::is_dumping_archive() && is_instance_klass()) {\n@@ -208,0 +208,4 @@\n+Klass::Klass() : _kind(UnknownKlassKind) {\n+  assert(CDSConfig::is_dumping_static_archive() || UseSharedSpaces, \"only for cds\");\n+}\n+\n@@ -534,1 +538,1 @@\n-  if (!Arguments::is_dumping_archive()) {\n+  if (!CDSConfig::is_dumping_archive()) {\n@@ -551,1 +555,1 @@\n-  assert (Arguments::is_dumping_archive(),\n+  assert(CDSConfig::is_dumping_archive(),\n@@ -569,1 +573,1 @@\n-  Arguments::assert_is_dumping_archive();\n+  assert(CDSConfig::is_dumping_archive(), \"sanity\");\n@@ -659,1 +663,1 @@\n-  assert(DumpSharedSpaces, \"called only during dumptime\");\n+  assert(CDSConfig::is_dumping_heap(), \"sanity\");\n","filename":"src\/hotspot\/share\/oops\/klass.cpp","additions":10,"deletions":6,"binary":false,"changes":16,"status":"modified"},{"patch":"@@ -198,2 +198,1 @@\n-  \/\/ Constructor\n-  Klass() : _kind(UnknownKlassKind) { assert(DumpSharedSpaces || UseSharedSpaces, \"only for cds\"); }\n+  Klass();\n","filename":"src\/hotspot\/share\/oops\/klass.hpp","additions":1,"deletions":2,"binary":false,"changes":3,"status":"modified"},{"patch":"@@ -26,0 +26,1 @@\n+#include \"cds\/cdsConfig.hpp\"\n@@ -166,1 +167,1 @@\n-  assert(DumpSharedSpaces, \"Used by CDS only. Do not abuse!\");\n+  assert(CDSConfig::is_dumping_heap(), \"Used by CDS only. Do not abuse!\");\n","filename":"src\/hotspot\/share\/oops\/oop.cpp","additions":2,"deletions":1,"binary":false,"changes":3,"status":"modified"},{"patch":"@@ -1235,1 +1235,1 @@\n-  SharedRuntime::c_calling_convention(sig_bt, parm_regs, \/*regs2=*\/nullptr, argcnt);\n+  SharedRuntime::c_calling_convention(sig_bt, parm_regs, argcnt);\n","filename":"src\/hotspot\/share\/opto\/callnode.cpp","additions":1,"deletions":1,"binary":false,"changes":2,"status":"modified"},{"patch":"@@ -32,0 +32,1 @@\n+#include \"compiler\/compilationMemoryStatistic.hpp\"\n@@ -288,0 +289,1 @@\n+    PhaseIterGVN::add_users_of_use_to_worklist(nn, use, *_igvn_worklist);\n@@ -649,2 +651,2 @@\n-                  _node_arena_one(mtCompiler),\n-                  _node_arena_two(mtCompiler),\n+                  _node_arena_one(mtCompiler, Arena::Tag::tag_node),\n+                  _node_arena_two(mtCompiler, Arena::Tag::tag_node),\n@@ -664,0 +666,1 @@\n+                  _oom(false),\n@@ -813,2 +816,0 @@\n-    print_method(PHASE_BEFORE_REMOVEUSELESS, 3);\n-\n@@ -841,1 +842,1 @@\n-  if (StressLCM || StressGCM || StressIGVN || StressCCP) {\n+  if (StressLCM || StressGCM || StressIGVN || StressCCP || StressIncrementalInlining) {\n@@ -941,0 +942,1 @@\n+    _oom(false),\n@@ -1861,0 +1863,1 @@\n+    if (failing()) return;\n@@ -2004,1 +2007,0 @@\n-    PhaseGVN* gvn = initial_gvn();\n@@ -2076,0 +2078,1 @@\n+    if (failing()) return;\n@@ -2252,0 +2255,2 @@\n+  if (failing())  return;\n+\n@@ -2262,1 +2267,3 @@\n-    if (AlwaysIncrementalInline) {\n+    if (failing())  return;\n+\n+    if (AlwaysIncrementalInline || StressIncrementalInlining) {\n@@ -2277,0 +2284,2 @@\n+  if (failing())  return;\n+\n@@ -2281,0 +2290,2 @@\n+  if (failing())  return;\n+\n@@ -2286,1 +2297,1 @@\n-\n+    if (failing())  return;\n@@ -2300,0 +2311,1 @@\n+    if (failing()) return;\n@@ -2306,0 +2318,2 @@\n+  if (failing())  return;\n+\n@@ -2327,1 +2341,1 @@\n-      if (failing())  return;\n+      if (failing()) return;\n@@ -2337,0 +2351,2 @@\n+        if (failing()) return;\n+\n@@ -2418,0 +2434,2 @@\n+  if (failing())  return;\n+\n@@ -2444,0 +2462,1 @@\n+    if (failing()) return;\n@@ -2456,0 +2475,1 @@\n+    if (failing())  return;\n@@ -4016,2 +4036,0 @@\n-        DEBUG_ONLY( n->dump_bfs(1, 0, \"-\"); );\n-        assert(false, \"malformed control flow\");\n@@ -4362,0 +4380,1 @@\n+  if (_compile->failing()) return;\n@@ -4934,0 +4953,1 @@\n+      if (failing())  return;\n@@ -5078,0 +5098,1 @@\n+  if (failing()) { return; }\n@@ -5248,0 +5269,3 @@\n+void Compile::record_method_not_compilable_oom() {\n+  record_method_not_compilable(CompilationMemoryStatistic::failure_reason_memlimit());\n+}\n","filename":"src\/hotspot\/share\/opto\/compile.cpp","additions":35,"deletions":11,"binary":false,"changes":46,"status":"modified"},{"patch":"@@ -1115,0 +1115,1 @@\n+  clear_upper_avx();\n@@ -1369,0 +1370,1 @@\n+  clear_upper_avx();\n@@ -5370,2 +5372,0 @@\n-  const char *stubName = \"array_partition_stub\";\n-\n@@ -5380,15 +5380,1 @@\n-  const TypeInstPtr* elem_klass = gvn().type(elementType)->isa_instptr();\n-  ciType* elem_type = elem_klass->const_oop()->as_instance()->java_mirror_type();\n-  BasicType bt = elem_type->basic_type();\n-  address stubAddr = nullptr;\n-  stubAddr = StubRoutines::select_array_partition_function();\n-  \/\/ stub not loaded\n-  if (stubAddr == nullptr) {\n-    return false;\n-  }\n-  \/\/ get the address of the array\n-  const TypeAryPtr* obj_t = _gvn.type(obj)->isa_aryptr();\n-  if (obj_t == nullptr || obj_t->elem() == Type::BOTTOM ) {\n-    return false; \/\/ failed input validation\n-  }\n-  Node* obj_adr = make_unsafe_address(obj, offset);\n+  Node* pivotIndices = nullptr;\n@@ -5396,7 +5382,4 @@\n-  \/\/ create the pivotIndices array of type int and size = 2\n-  Node* size = intcon(2);\n-  Node* klass_node = makecon(TypeKlassPtr::make(ciTypeArrayKlass::make(T_INT)));\n-  Node* pivotIndices = new_array(klass_node, size, 0);  \/\/ no arguments to push\n-  AllocateArrayNode* alloc = tightly_coupled_allocation(pivotIndices);\n-  guarantee(alloc != nullptr, \"created above\");\n-  Node* pivotIndices_adr = basic_plus_adr(pivotIndices, arrayOopDesc::base_offset_in_bytes(T_INT));\n+  \/\/ Set the original stack and the reexecute bit for the interpreter to reexecute\n+  \/\/ the bytecode that invokes DualPivotQuicksort.partition() if deoptimization happens.\n+  { PreserveReexecuteState preexecs(this);\n+    jvms()->set_should_reexecute(true);\n@@ -5404,2 +5387,15 @@\n-  \/\/ pass the basic type enum to the stub\n-  Node* elemType = intcon(bt);\n+    const TypeInstPtr* elem_klass = gvn().type(elementType)->isa_instptr();\n+    ciType* elem_type = elem_klass->const_oop()->as_instance()->java_mirror_type();\n+    BasicType bt = elem_type->basic_type();\n+    address stubAddr = nullptr;\n+    stubAddr = StubRoutines::select_array_partition_function();\n+    \/\/ stub not loaded\n+    if (stubAddr == nullptr) {\n+      return false;\n+    }\n+    \/\/ get the address of the array\n+    const TypeAryPtr* obj_t = _gvn.type(obj)->isa_aryptr();\n+    if (obj_t == nullptr || obj_t->elem() == Type::BOTTOM ) {\n+      return false; \/\/ failed input validation\n+    }\n+    Node* obj_adr = make_unsafe_address(obj, offset);\n@@ -5407,5 +5403,19 @@\n-  \/\/ Call the stub\n-  make_runtime_call(RC_LEAF|RC_NO_FP, OptoRuntime::array_partition_Type(),\n-                    stubAddr, stubName, TypePtr::BOTTOM,\n-                    obj_adr, elemType, fromIndex, toIndex, pivotIndices_adr,\n-                    indexPivot1, indexPivot2);\n+    \/\/ create the pivotIndices array of type int and size = 2\n+    Node* size = intcon(2);\n+    Node* klass_node = makecon(TypeKlassPtr::make(ciTypeArrayKlass::make(T_INT)));\n+    pivotIndices = new_array(klass_node, size, 0);  \/\/ no arguments to push\n+    AllocateArrayNode* alloc = tightly_coupled_allocation(pivotIndices);\n+    guarantee(alloc != nullptr, \"created above\");\n+    Node* pivotIndices_adr = basic_plus_adr(pivotIndices, arrayOopDesc::base_offset_in_bytes(T_INT));\n+\n+    \/\/ pass the basic type enum to the stub\n+    Node* elemType = intcon(bt);\n+\n+    \/\/ Call the stub\n+    const char *stubName = \"array_partition_stub\";\n+    make_runtime_call(RC_LEAF|RC_NO_FP, OptoRuntime::array_partition_Type(),\n+                      stubAddr, stubName, TypePtr::BOTTOM,\n+                      obj_adr, elemType, fromIndex, toIndex, pivotIndices_adr,\n+                      indexPivot1, indexPivot2);\n+\n+  } \/\/ original reexecute is set back here\n@@ -5424,3 +5434,0 @@\n-  const char *stubName;\n-  stubName = \"arraysort_stub\";\n-\n@@ -5454,0 +5461,1 @@\n+  const char *stubName = \"arraysort_stub\";\n","filename":"src\/hotspot\/share\/opto\/library_call.cpp","additions":42,"deletions":34,"binary":false,"changes":76,"status":"modified"},{"patch":"@@ -600,1 +600,1 @@\n-          if (!n->is_Store() && n->Opcode() != Op_CastP2X && !bs->is_gc_pre_barrier_node(n)) {\n+          if (!n->is_Store() && n->Opcode() != Op_CastP2X && !bs->is_gc_pre_barrier_node(n) && !reduce_merge_precheck) {\n@@ -678,0 +678,5 @@\n+\n+  if (TraceReduceAllocationMerges && !can_eliminate && reduce_merge_precheck) {\n+    tty->print_cr(\"\\tCan't eliminate allocation because '%s': \", fail_eliminate != nullptr ? fail_eliminate : \"\");\n+    DEBUG_ONLY(if (disq_node != nullptr) disq_node->dump();)\n+  }\n","filename":"src\/hotspot\/share\/opto\/macro.cpp","additions":6,"deletions":1,"binary":false,"changes":7,"status":"modified"},{"patch":"@@ -65,0 +65,1 @@\n+  { Bad,             T_ARRAY,      \"interfaces:\",   false, Node::NotAMachineReg, relocInfo::none          },  \/\/ Interfaces\n@@ -124,2 +125,2 @@\n-const TypePtr::InterfaceSet* TypeAryPtr::_array_interfaces = nullptr;\n-const TypePtr::InterfaceSet* TypeAryKlassPtr::_array_interfaces = nullptr;\n+const TypeInterfaces* TypeAryPtr::_array_interfaces = nullptr;\n+const TypeInterfaces* TypeAryKlassPtr::_array_interfaces = nullptr;\n@@ -575,1 +576,1 @@\n-  TypeAryPtr::_array_interfaces = new TypePtr::InterfaceSet(&array_interfaces);\n+  TypeAryPtr::_array_interfaces = TypeInterfaces::make(&array_interfaces);\n@@ -3256,2 +3257,2 @@\n-TypePtr::InterfaceSet::InterfaceSet()\n-        : _list(Compile::current()->type_arena(), 0, 0, nullptr),\n+TypeInterfaces::TypeInterfaces()\n+        : Type(Interfaces), _list(Compile::current()->type_arena(), 0, 0, nullptr),\n@@ -3262,2 +3263,2 @@\n-TypePtr::InterfaceSet::InterfaceSet(GrowableArray<ciInstanceKlass*>* interfaces)\n-        : _list(Compile::current()->type_arena(), interfaces->length(), 0, nullptr),\n+TypeInterfaces::TypeInterfaces(GrowableArray<ciInstanceKlass*>* interfaces)\n+        : Type(Interfaces), _list(Compile::current()->type_arena(), interfaces->length(), 0, nullptr),\n@@ -3271,1 +3272,6 @@\n-void TypePtr::InterfaceSet::initialize() {\n+const TypeInterfaces* TypeInterfaces::make(GrowableArray<ciInstanceKlass*>* interfaces) {\n+  TypeInterfaces* result = (interfaces == nullptr) ? new TypeInterfaces() : new TypeInterfaces(interfaces);\n+  return (const TypeInterfaces*)result->hashcons();\n+}\n+\n+void TypeInterfaces::initialize() {\n@@ -3277,1 +3283,1 @@\n-int TypePtr::InterfaceSet::compare(ciKlass* const& k1, ciKlass* const& k2) {\n+int TypeInterfaces::compare(ciInstanceKlass* const& k1, ciInstanceKlass* const& k2) {\n@@ -3286,1 +3292,1 @@\n-void TypePtr::InterfaceSet::add(ciKlass* interface) {\n+void TypeInterfaces::add(ciInstanceKlass* interface) {\n@@ -3292,7 +3298,3 @@\n-void TypePtr::InterfaceSet::raw_add(ciKlass* interface) {\n-  assert(interface->is_interface(), \"for interfaces only\");\n-  _list.push(interface);\n-}\n-\n-bool TypePtr::InterfaceSet::eq(const InterfaceSet& other) const {\n-  if (_list.length() != other._list.length()) {\n+bool TypeInterfaces::eq(const Type* t) const {\n+  const TypeInterfaces* other = (const TypeInterfaces*)t;\n+  if (_list.length() != other->_list.length()) {\n@@ -3303,1 +3305,1 @@\n-    ciKlass* k2 = other._list.at(i);\n+    ciKlass* k2 = other->_list.at(i);\n@@ -3311,1 +3313,1 @@\n-bool TypePtr::InterfaceSet::eq(ciInstanceKlass* k) const {\n+bool TypeInterfaces::eq(ciInstanceKlass* k) const {\n@@ -3313,1 +3315,1 @@\n-  GrowableArray<ciInstanceKlass *>* interfaces = k->as_instance_klass()->transitive_interfaces();\n+  GrowableArray<ciInstanceKlass *>* interfaces = k->transitive_interfaces();\n@@ -3319,1 +3321,1 @@\n-    _list.find_sorted<ciKlass*, compare>(interfaces->at(i), found);\n+    _list.find_sorted<ciInstanceKlass*, compare>(interfaces->at(i), found);\n@@ -3328,1 +3330,1 @@\n-uint TypePtr::InterfaceSet::hash() const {\n+uint TypeInterfaces::hash() const {\n@@ -3333,1 +3335,5 @@\n-void TypePtr::InterfaceSet::compute_hash() {\n+const Type* TypeInterfaces::xdual() const {\n+  return this;\n+}\n+\n+void TypeInterfaces::compute_hash() {\n@@ -3342,1 +3348,1 @@\n-static int compare_interfaces(ciKlass** k1, ciKlass** k2) {\n+static int compare_interfaces(ciInstanceKlass** k1, ciInstanceKlass** k2) {\n@@ -3346,1 +3352,1 @@\n-void TypePtr::InterfaceSet::dump(outputStream* st) const {\n+void TypeInterfaces::dump(outputStream* st) const {\n@@ -3352,1 +3358,1 @@\n-  GrowableArray<ciKlass*> interfaces;\n+  GrowableArray<ciInstanceKlass*> interfaces;\n@@ -3367,1 +3373,1 @@\n-void TypePtr::InterfaceSet::verify() const {\n+void TypeInterfaces::verify() const {\n@@ -3369,2 +3375,2 @@\n-    ciKlass* k1 = _list.at(i-1);\n-    ciKlass* k2 = _list.at(i);\n+    ciInstanceKlass* k1 = _list.at(i-1);\n+    ciInstanceKlass* k2 = _list.at(i);\n@@ -3377,2 +3383,2 @@\n-TypePtr::InterfaceSet TypeOopPtr::InterfaceSet::union_with(const InterfaceSet& other) const {\n-  InterfaceSet result;\n+const TypeInterfaces* TypeInterfaces::union_with(const TypeInterfaces* other) const {\n+  GrowableArray<ciInstanceKlass*> result_list;\n@@ -3381,1 +3387,1 @@\n-  while (i < _list.length() || j < other._list.length()) {\n+  while (i < _list.length() || j < other->_list.length()) {\n@@ -3383,3 +3389,3 @@\n-           (j >= other._list.length() ||\n-            compare(_list.at(i), other._list.at(j)) < 0)) {\n-      result.raw_add(_list.at(i));\n+           (j >= other->_list.length() ||\n+            compare(_list.at(i), other->_list.at(j)) < 0)) {\n+      result_list.push(_list.at(i));\n@@ -3388,1 +3394,1 @@\n-    while (j < other._list.length() &&\n+    while (j < other->_list.length() &&\n@@ -3390,2 +3396,2 @@\n-            compare(other._list.at(j), _list.at(i)) < 0)) {\n-      result.raw_add(other._list.at(j));\n+            compare(other->_list.at(j), _list.at(i)) < 0)) {\n+      result_list.push(other->_list.at(j));\n@@ -3395,3 +3401,3 @@\n-        j < other._list.length() &&\n-        _list.at(i) == other._list.at(j)) {\n-      result.raw_add(_list.at(i));\n+        j < other->_list.length() &&\n+        _list.at(i) == other->_list.at(j)) {\n+      result_list.push(_list.at(i));\n@@ -3402,1 +3408,1 @@\n-  result.initialize();\n+  const TypeInterfaces* result = TypeInterfaces::make(&result_list);\n@@ -3404,1 +3410,1 @@\n-  result.verify();\n+  result->verify();\n@@ -3406,1 +3412,1 @@\n-    assert(result._list.contains(_list.at(i)), \"missing\");\n+    assert(result->_list.contains(_list.at(i)), \"missing\");\n@@ -3408,2 +3414,2 @@\n-  for (int i = 0; i < other._list.length(); i++) {\n-    assert(result._list.contains(other._list.at(i)), \"missing\");\n+  for (int i = 0; i < other->_list.length(); i++) {\n+    assert(result->_list.contains(other->_list.at(i)), \"missing\");\n@@ -3411,2 +3417,2 @@\n-  for (int i = 0; i < result._list.length(); i++) {\n-    assert(_list.contains(result._list.at(i)) || other._list.contains(result._list.at(i)), \"missing\");\n+  for (int i = 0; i < result->_list.length(); i++) {\n+    assert(_list.contains(result->_list.at(i)) || other->_list.contains(result->_list.at(i)), \"missing\");\n@@ -3418,2 +3424,2 @@\n-TypePtr::InterfaceSet TypeOopPtr::InterfaceSet::intersection_with(const InterfaceSet& other) const {\n-  InterfaceSet result;\n+const TypeInterfaces* TypeInterfaces::intersection_with(const TypeInterfaces* other) const {\n+  GrowableArray<ciInstanceKlass*> result_list;\n@@ -3422,1 +3428,1 @@\n-  while (i < _list.length() || j < other._list.length()) {\n+  while (i < _list.length() || j < other->_list.length()) {\n@@ -3424,2 +3430,2 @@\n-           (j >= other._list.length() ||\n-            compare(_list.at(i), other._list.at(j)) < 0)) {\n+           (j >= other->_list.length() ||\n+            compare(_list.at(i), other->_list.at(j)) < 0)) {\n@@ -3428,1 +3434,1 @@\n-    while (j < other._list.length() &&\n+    while (j < other->_list.length() &&\n@@ -3430,1 +3436,1 @@\n-            compare(other._list.at(j), _list.at(i)) < 0)) {\n+            compare(other->_list.at(j), _list.at(i)) < 0)) {\n@@ -3434,3 +3440,3 @@\n-        j < other._list.length() &&\n-        _list.at(i) == other._list.at(j)) {\n-      result.raw_add(_list.at(i));\n+        j < other->_list.length() &&\n+        _list.at(i) == other->_list.at(j)) {\n+      result_list.push(_list.at(i));\n@@ -3441,1 +3447,1 @@\n-  result.initialize();\n+  const TypeInterfaces* result = TypeInterfaces::make(&result_list);\n@@ -3443,1 +3449,1 @@\n-  result.verify();\n+  result->verify();\n@@ -3445,1 +3451,1 @@\n-    assert(!other._list.contains(_list.at(i)) || result._list.contains(_list.at(i)), \"missing\");\n+    assert(!other->_list.contains(_list.at(i)) || result->_list.contains(_list.at(i)), \"missing\");\n@@ -3447,2 +3453,2 @@\n-  for (int i = 0; i < other._list.length(); i++) {\n-    assert(!_list.contains(other._list.at(i)) || result._list.contains(other._list.at(i)), \"missing\");\n+  for (int i = 0; i < other->_list.length(); i++) {\n+    assert(!_list.contains(other->_list.at(i)) || result->_list.contains(other->_list.at(i)), \"missing\");\n@@ -3450,2 +3456,2 @@\n-  for (int i = 0; i < result._list.length(); i++) {\n-    assert(_list.contains(result._list.at(i)) && other._list.contains(result._list.at(i)), \"missing\");\n+  for (int i = 0; i < result->_list.length(); i++) {\n+    assert(_list.contains(result->_list.at(i)) && other->_list.contains(result->_list.at(i)), \"missing\");\n@@ -3458,1 +3464,1 @@\n-ciKlass* TypePtr::InterfaceSet::exact_klass() const {\n+ciInstanceKlass* TypeInterfaces::exact_klass() const {\n@@ -3463,1 +3469,1 @@\n-void TypePtr::InterfaceSet::compute_exact_klass() {\n+void TypeInterfaces::compute_exact_klass() {\n@@ -3468,1 +3474,1 @@\n-  ciKlass* res = nullptr;\n+  ciInstanceKlass* res = nullptr;\n@@ -3470,1 +3476,1 @@\n-    ciInstanceKlass* interface = _list.at(i)->as_instance_klass();\n+    ciInstanceKlass* interface = _list.at(i);\n@@ -3480,1 +3486,1 @@\n-void TypePtr::InterfaceSet::verify_is_loaded() const {\n+void TypeInterfaces::verify_is_loaded() const {\n@@ -3488,0 +3494,11 @@\n+\/\/ Can't be implemented because there's no way to know if the type is above or below the center line.\n+const Type* TypeInterfaces::xmeet(const Type* t) const {\n+  ShouldNotReachHere();\n+  return Type::xmeet(t);\n+}\n+\n+bool TypeInterfaces::singleton(void) const {\n+  ShouldNotReachHere();\n+  return Type::singleton();\n+}\n+\n@@ -3489,1 +3506,1 @@\n-TypeOopPtr::TypeOopPtr(TYPES t, PTR ptr, ciKlass* k, const InterfaceSet& interfaces, bool xk, ciObject* o, int offset,\n+TypeOopPtr::TypeOopPtr(TYPES t, PTR ptr, ciKlass* k, const TypeInterfaces* interfaces, bool xk, ciObject* o, int offset,\n@@ -3501,1 +3518,1 @@\n-    interfaces.verify_is_loaded();\n+    interfaces->verify_is_loaded();\n@@ -3578,1 +3595,2 @@\n-  return (TypeOopPtr*)(new TypeOopPtr(OopPtr, ptr, k, InterfaceSet(), xk, o, offset, instance_id, speculative, inline_depth))->hashcons();\n+  const TypeInterfaces* interfaces = TypeInterfaces::make();\n+  return (TypeOopPtr*)(new TypeOopPtr(OopPtr, ptr, k, interfaces, xk, o, offset, instance_id, speculative, inline_depth))->hashcons();\n@@ -3723,1 +3741,1 @@\n-    const TypePtr::InterfaceSet interfaces = TypePtr::interfaces(klass, true, true, false, interface_handling);\n+    const TypeInterfaces* interfaces = TypePtr::interfaces(klass, true, true, false, interface_handling);\n@@ -3956,1 +3974,1 @@\n-TypePtr::InterfaceSet TypeOopPtr::meet_interfaces(const TypeOopPtr* other) const {\n+const TypeInterfaces* TypeOopPtr::meet_interfaces(const TypeOopPtr* other) const {\n@@ -3958,1 +3976,1 @@\n-    return _interfaces.union_with(other->_interfaces);\n+    return _interfaces->union_with(other->_interfaces);\n@@ -3964,1 +3982,1 @@\n-  return _interfaces.intersection_with(other->_interfaces);\n+  return _interfaces->intersection_with(other->_interfaces);\n@@ -3993,1 +4011,1 @@\n-  if (_interfaces.empty()) {\n+  if (_interfaces->empty()) {\n@@ -3997,1 +4015,1 @@\n-    if (_interfaces.eq(_klass->as_instance_klass())) {\n+    if (_interfaces->eq(_klass->as_instance_klass())) {\n@@ -4002,1 +4020,1 @@\n-  return _interfaces.exact_klass();\n+  return _interfaces->exact_klass();\n@@ -4006,1 +4024,1 @@\n-TypeInstPtr::TypeInstPtr(PTR ptr, ciKlass* k, const InterfaceSet& interfaces, bool xk, ciObject* o, int off,\n+TypeInstPtr::TypeInstPtr(PTR ptr, ciKlass* k, const TypeInterfaces* interfaces, bool xk, ciObject* o, int off,\n@@ -4018,1 +4036,1 @@\n-                                     const InterfaceSet& interfaces,\n+                                     const TypeInterfaces* interfaces,\n@@ -4050,1 +4068,1 @@\n-TypePtr::InterfaceSet TypePtr::interfaces(ciKlass*& k, bool klass, bool interface, bool array, InterfaceHandling interface_handling) {\n+const TypeInterfaces* TypePtr::interfaces(ciKlass*& k, bool klass, bool interface, bool array, InterfaceHandling interface_handling) {\n@@ -4056,1 +4074,1 @@\n-        InterfaceSet interfaces;\n+        const TypeInterfaces* interfaces = TypeInterfaces::make();\n@@ -4060,1 +4078,1 @@\n-      InterfaceSet interfaces(k_interfaces);\n+      const TypeInterfaces* interfaces = TypeInterfaces::make(k_interfaces);\n@@ -4069,1 +4087,1 @@\n-    InterfaceSet interfaces;\n+    const TypeInterfaces* interfaces = TypeInterfaces::make();\n@@ -4080,1 +4098,1 @@\n-  return *TypeAryPtr::_array_interfaces;\n+  return TypeAryPtr::_array_interfaces;\n@@ -4134,1 +4152,1 @@\n-const TypeInstPtr *TypeInstPtr::xmeet_unloaded(const TypeInstPtr *tinst, const InterfaceSet& interfaces) const {\n+const TypeInstPtr *TypeInstPtr::xmeet_unloaded(const TypeInstPtr *tinst, const TypeInterfaces* interfaces) const {\n@@ -4291,1 +4309,1 @@\n-    InterfaceSet interfaces = meet_interfaces(tinst);\n+    const TypeInterfaces* interfaces = meet_interfaces(tinst);\n@@ -4351,2 +4369,2 @@\n-template<class T> TypePtr::MeetResult TypePtr::meet_instptr(PTR& ptr, InterfaceSet& interfaces, const T* this_type, const T* other_type,\n-                      ciKlass*& res_klass, bool& res_xk) {\n+template<class T> TypePtr::MeetResult TypePtr::meet_instptr(PTR& ptr, const TypeInterfaces*& interfaces, const T* this_type, const T* other_type,\n+                                                            ciKlass*& res_klass, bool& res_xk) {\n@@ -4359,2 +4377,2 @@\n-  InterfaceSet this_interfaces = this_type->interfaces();\n-  InterfaceSet other_interfaces = other_type->interfaces();\n+  const TypeInterfaces* this_interfaces = this_type->interfaces();\n+  const TypeInterfaces* other_interfaces = other_type->interfaces();\n@@ -4445,1 +4463,1 @@\n-  interfaces = this_interfaces.intersection_with(other_interfaces);\n+  interfaces = this_interfaces->intersection_with(other_interfaces);\n@@ -4481,1 +4499,1 @@\n-    _interfaces.eq(p->_interfaces) &&\n+    _interfaces->eq(p->_interfaces) &&\n@@ -4488,1 +4506,1 @@\n-  return klass()->hash() + TypeOopPtr::hash() + _interfaces.hash();\n+  return klass()->hash() + TypeOopPtr::hash() + _interfaces->hash();\n@@ -4511,1 +4529,1 @@\n-  _interfaces.dump(st);\n+  _interfaces->dump(st);\n@@ -4595,1 +4613,1 @@\n-    if (_interfaces.eq(ik)) {\n+    if (_interfaces->eq(ik)) {\n@@ -4612,1 +4630,1 @@\n-  if (other->klass() == ciEnv::current()->Object_klass() && other->_interfaces.empty()) {\n+  if (other->klass() == ciEnv::current()->Object_klass() && other->_interfaces->empty()) {\n@@ -4617,1 +4635,1 @@\n-         (!this_xk || this_one->_interfaces.contains(other->_interfaces));\n+         (!this_xk || this_one->_interfaces->contains(other->_interfaces));\n@@ -4627,1 +4645,1 @@\n-  if (other->klass() == ciEnv::current()->Object_klass() && other->_interfaces.empty()) {\n+  if (other->klass() == ciEnv::current()->Object_klass() && other->_interfaces->empty()) {\n@@ -4632,1 +4650,1 @@\n-    return other->klass() == ciEnv::current()->Object_klass() && this_one->_interfaces.contains(other->_interfaces);\n+    return other->klass() == ciEnv::current()->Object_klass() && this_one->_interfaces->contains(other->_interfaces);\n@@ -4975,3 +4993,3 @@\n-    InterfaceSet interfaces = meet_interfaces(tp);\n-    InterfaceSet tp_interfaces = tp->_interfaces;\n-    InterfaceSet this_interfaces = _interfaces;\n+    const TypeInterfaces* interfaces = meet_interfaces(tp);\n+    const TypeInterfaces* tp_interfaces = tp->_interfaces;\n+    const TypeInterfaces* this_interfaces = _interfaces;\n@@ -4985,1 +5003,1 @@\n-      if (tp->klass()->equals(ciEnv::current()->Object_klass()) && this_interfaces.contains(tp_interfaces) && !tp->klass_is_exact()) {\n+      if (tp->klass()->equals(ciEnv::current()->Object_klass()) && this_interfaces->contains(tp_interfaces) && !tp->klass_is_exact()) {\n@@ -4991,1 +5009,1 @@\n-        interfaces = this_interfaces.intersection_with(tp_interfaces);\n+        interfaces = this_interfaces->intersection_with(tp_interfaces);\n@@ -5004,1 +5022,1 @@\n-        if (tp->klass()->equals(ciEnv::current()->Object_klass()) && this_interfaces.contains(tp_interfaces) && !tp->klass_is_exact()) {\n+        if (tp->klass()->equals(ciEnv::current()->Object_klass()) && this_interfaces->contains(tp_interfaces) && !tp->klass_is_exact()) {\n@@ -5018,1 +5036,1 @@\n-      interfaces = this_interfaces.intersection_with(tp_interfaces);\n+      interfaces = this_interfaces->intersection_with(tp_interfaces);\n@@ -5132,1 +5150,1 @@\n-  _interfaces.dump(st);\n+  _interfaces->dump(st);\n@@ -5582,1 +5600,1 @@\n-    const InterfaceSet interfaces = TypePtr::interfaces(klass, true, true, false, interface_handling);\n+    const TypeInterfaces* interfaces = TypePtr::interfaces(klass, true, true, false, interface_handling);\n@@ -5590,1 +5608,1 @@\n-TypeKlassPtr::TypeKlassPtr(TYPES t, PTR ptr, ciKlass* klass, const InterfaceSet& interfaces, int offset)\n+TypeKlassPtr::TypeKlassPtr(TYPES t, PTR ptr, ciKlass* klass, const TypeInterfaces* interfaces, int offset)\n@@ -5599,1 +5617,1 @@\n-  if (_interfaces.empty()) {\n+  if (_interfaces->empty()) {\n@@ -5603,1 +5621,1 @@\n-    if (_interfaces.eq(_klass->as_instance_klass())) {\n+    if (_interfaces->eq(_klass->as_instance_klass())) {\n@@ -5608,1 +5626,1 @@\n-  return _interfaces.exact_klass();\n+  return _interfaces->exact_klass();\n@@ -5616,1 +5634,1 @@\n-    _interfaces.eq(p->_interfaces) &&\n+    _interfaces->eq(p->_interfaces) &&\n@@ -5623,1 +5641,1 @@\n-  return TypePtr::hash() + _interfaces.hash();\n+  return TypePtr::hash() + _interfaces->hash();\n@@ -5650,1 +5668,1 @@\n-TypePtr::InterfaceSet TypeKlassPtr::meet_interfaces(const TypeKlassPtr* other) const {\n+const TypeInterfaces* TypeKlassPtr::meet_interfaces(const TypeKlassPtr* other) const {\n@@ -5652,1 +5670,1 @@\n-    return _interfaces.union_with(other->_interfaces);\n+    return _interfaces->union_with(other->_interfaces);\n@@ -5658,1 +5676,1 @@\n-  return _interfaces.intersection_with(other->_interfaces);\n+  return _interfaces->intersection_with(other->_interfaces);\n@@ -5698,1 +5716,1 @@\n-      _interfaces.dump(st);\n+      _interfaces->dump(st);\n@@ -5739,1 +5757,1 @@\n-const TypeInstKlassPtr *TypeInstKlassPtr::make(PTR ptr, ciKlass* k, const InterfaceSet& interfaces, int offset) {\n+const TypeInstKlassPtr *TypeInstKlassPtr::make(PTR ptr, ciKlass* k, const TypeInterfaces* interfaces, int offset) {\n@@ -5791,1 +5809,1 @@\n-  TypePtr::InterfaceSet interfaces = _interfaces;\n+  const TypeInterfaces* interfaces = _interfaces;\n@@ -5800,1 +5818,1 @@\n-        if (_interfaces.eq(sub)) {\n+        if (_interfaces->eq(sub)) {\n@@ -5884,1 +5902,1 @@\n-    InterfaceSet interfaces = meet_interfaces(tkls);\n+    const TypeInterfaces* interfaces = meet_interfaces(tkls);\n@@ -5907,3 +5925,3 @@\n-    InterfaceSet interfaces = meet_interfaces(tp);\n-    InterfaceSet tp_interfaces = tp->_interfaces;\n-    InterfaceSet this_interfaces = _interfaces;\n+    const TypeInterfaces* interfaces = meet_interfaces(tp);\n+    const TypeInterfaces* tp_interfaces = tp->_interfaces;\n+    const TypeInterfaces* this_interfaces = _interfaces;\n@@ -5917,1 +5935,1 @@\n-      if (klass()->equals(ciEnv::current()->Object_klass()) && tp_interfaces.contains(this_interfaces) && !klass_is_exact()) {\n+      if (klass()->equals(ciEnv::current()->Object_klass()) && tp_interfaces->contains(this_interfaces) && !klass_is_exact()) {\n@@ -5922,1 +5940,1 @@\n-        interfaces = _interfaces.intersection_with(tp->_interfaces);\n+        interfaces = _interfaces->intersection_with(tp->_interfaces);\n@@ -5935,1 +5953,1 @@\n-        if (klass()->equals(ciEnv::current()->Object_klass()) && tp_interfaces.contains(this_interfaces) && !klass_is_exact()) {\n+        if (klass()->equals(ciEnv::current()->Object_klass()) && tp_interfaces->contains(this_interfaces) && !klass_is_exact()) {\n@@ -5945,1 +5963,1 @@\n-      interfaces = this_interfaces.intersection_with(tp_interfaces);\n+      interfaces = this_interfaces->intersection_with(tp_interfaces);\n@@ -5974,1 +5992,1 @@\n-  if (other->klass()->equals(ciEnv::current()->Object_klass()) && other->_interfaces.empty()) {\n+  if (other->klass()->equals(ciEnv::current()->Object_klass()) && other->_interfaces->empty()) {\n@@ -5978,1 +5996,1 @@\n-  return this_one->_klass->is_subtype_of(other->_klass) && this_one->_interfaces.contains(other->_interfaces);\n+  return this_one->_klass->is_subtype_of(other->_klass) && this_one->_interfaces->contains(other->_interfaces);\n@@ -5993,1 +6011,1 @@\n-  return this_one->_klass->equals(other->_klass) && this_one->_interfaces.eq(other->_interfaces);\n+  return this_one->_klass->equals(other->_klass) && this_one->_interfaces->eq(other->_interfaces);\n@@ -6007,1 +6025,1 @@\n-    return !this_exact && this_one->_klass->equals(ciEnv::current()->Object_klass())  && other->_interfaces.contains(this_one->_interfaces);\n+    return !this_exact && this_one->_klass->equals(ciEnv::current()->Object_klass())  && other->_interfaces->contains(this_one->_interfaces);\n@@ -6021,1 +6039,1 @@\n-    return this_one->_klass->is_subtype_of(other->_klass) && this_one->_interfaces.contains(other->_interfaces);\n+    return this_one->_klass->is_subtype_of(other->_klass) && this_one->_interfaces->contains(other->_interfaces);\n@@ -6039,1 +6057,1 @@\n-  TypePtr::InterfaceSet interfaces = _interfaces;\n+  const TypeInterfaces* interfaces = _interfaces;\n@@ -6047,1 +6065,1 @@\n-        if (_interfaces.eq(sub)) {\n+        if (_interfaces->eq(sub)) {\n@@ -6341,3 +6359,3 @@\n-    InterfaceSet interfaces = meet_interfaces(tp);\n-    InterfaceSet tp_interfaces = tp->_interfaces;\n-    InterfaceSet this_interfaces = _interfaces;\n+    const TypeInterfaces* interfaces = meet_interfaces(tp);\n+    const TypeInterfaces* tp_interfaces = tp->_interfaces;\n+    const TypeInterfaces* this_interfaces = _interfaces;\n@@ -6351,1 +6369,1 @@\n-      if (tp->klass()->equals(ciEnv::current()->Object_klass()) && this_interfaces.intersection_with(tp_interfaces).eq(tp_interfaces) && !tp->klass_is_exact()) {\n+      if (tp->klass()->equals(ciEnv::current()->Object_klass()) && this_interfaces->intersection_with(tp_interfaces)->eq(tp_interfaces) && !tp->klass_is_exact()) {\n@@ -6356,1 +6374,1 @@\n-        interfaces = this_interfaces.intersection_with(tp->_interfaces);\n+        interfaces = this_interfaces->intersection_with(tp->_interfaces);\n@@ -6369,1 +6387,1 @@\n-        if (tp->klass()->equals(ciEnv::current()->Object_klass()) && this_interfaces.intersection_with(tp_interfaces).eq(tp_interfaces) && !tp->klass_is_exact()) {\n+        if (tp->klass()->equals(ciEnv::current()->Object_klass()) && this_interfaces->intersection_with(tp_interfaces)->eq(tp_interfaces) && !tp->klass_is_exact()) {\n@@ -6378,1 +6396,1 @@\n-      interfaces = this_interfaces.intersection_with(tp_interfaces);\n+      interfaces = this_interfaces->intersection_with(tp_interfaces);\n@@ -6391,1 +6409,1 @@\n-  if (other->klass() == ciEnv::current()->Object_klass() && other->_interfaces.empty() && other_exact) {\n+  if (other->klass() == ciEnv::current()->Object_klass() && other->_interfaces->empty() && other_exact) {\n@@ -6403,1 +6421,1 @@\n-    return other->klass() == ciEnv::current()->Object_klass() && other->_interfaces.intersection_with(this_one->_interfaces).eq(other->_interfaces) && other_exact;\n+    return other->klass() == ciEnv::current()->Object_klass() && other->_interfaces->intersection_with(this_one->_interfaces)->eq(other->_interfaces) && other_exact;\n@@ -6463,1 +6481,1 @@\n-  if (other->klass() == ciEnv::current()->Object_klass() && other->_interfaces.empty() && other_exact) {\n+  if (other->klass() == ciEnv::current()->Object_klass() && other->_interfaces->empty() && other_exact) {\n@@ -6472,1 +6490,1 @@\n-    return other->_klass->equals(ciEnv::current()->Object_klass()) && other->_interfaces.intersection_with(this_one->_interfaces).eq(other->_interfaces);\n+    return other->_klass->equals(ciEnv::current()->Object_klass()) && other->_interfaces->intersection_with(this_one->_interfaces)->eq(other->_interfaces);\n@@ -6547,1 +6565,1 @@\n-      _interfaces.dump(st);\n+      _interfaces->dump(st);\n","filename":"src\/hotspot\/share\/opto\/type.cpp","additions":169,"deletions":151,"binary":false,"changes":320,"status":"modified"},{"patch":"@@ -27,0 +27,1 @@\n+#include \"cds\/cdsConfig.hpp\"\n@@ -44,0 +45,1 @@\n+#include \"nmt\/nmtCommon.hpp\"\n@@ -61,1 +63,0 @@\n-#include \"services\/nmtCommon.hpp\"\n@@ -133,0 +134,3 @@\n+\/\/ True if -Xint\/-Xmixed\/-Xcomp were specified\n+static bool mode_flag_cmd_line = false;\n+\n@@ -528,0 +532,1 @@\n+  { \"UseCounterDecay\",              JDK_Version::undefined(), JDK_Version::jdk(22), JDK_Version::jdk(23) },\n@@ -1261,2 +1266,1 @@\n-  if (is_internal_module_property(key) ||\n-      strcmp(key, \"jdk.module.main\") == 0) {\n+  if (is_internal_module_property(key)) {\n@@ -1269,1 +1273,2 @@\n-    MetaspaceShared::disable_full_module_graph();\n+    CDSConfig::disable_loading_full_module_graph();\n+    CDSConfig::disable_dumping_full_module_graph();\n@@ -1335,1 +1340,1 @@\n-  assert(is_dumping_archive(),\n+  assert(CDSConfig::is_dumping_archive(),\n@@ -2602,0 +2607,1 @@\n+          mode_flag_cmd_line = true;\n@@ -2605,0 +2611,1 @@\n+          mode_flag_cmd_line = true;\n@@ -2609,0 +2616,1 @@\n+          mode_flag_cmd_line = true;\n@@ -2611,1 +2619,1 @@\n-      DumpSharedSpaces = true;\n+      CDSConfig::enable_dumping_static_archive();\n@@ -3032,9 +3040,13 @@\n-  if (DumpSharedSpaces) {\n-    \/\/ Compiler threads may concurrently update the class metadata (such as method entries), so it's\n-    \/\/ unsafe with -Xshare:dump (which modifies the class metadata in place). Let's disable\n-    \/\/ compiler just to be safe.\n-    \/\/\n-    \/\/ Note: this is not a concern for dynamically dumping shared spaces, which makes a copy of the\n-    \/\/ class metadata instead of modifying them in place. The copy is inaccessible to the compiler.\n-    \/\/ TODO: revisit the following for the static archive case.\n-    set_mode_flags(_int);\n+  if (CDSConfig::is_dumping_static_archive()) {\n+    if (!mode_flag_cmd_line) {\n+      \/\/ By default, -Xshare:dump runs in interpreter-only mode, which is required for deterministic archive.\n+      \/\/\n+      \/\/ If your classlist is large and you don't care about deterministic dumping, you can use\n+      \/\/ -Xshare:dump -Xmixed to improve dumping speed.\n+      set_mode_flags(_int);\n+    } else if (_mode == _comp) {\n+      \/\/ -Xcomp may use excessive CPU for the test tiers. Also, -Xshare:dump runs a small and fixed set of\n+      \/\/ Java code, so there's not much benefit in running -Xcomp.\n+      log_info(cds)(\"reduced -Xcomp to -Xmixed for static dumping\");\n+      set_mode_flags(_mixed);\n+    }\n@@ -3056,1 +3068,1 @@\n-    DynamicDumpSharedSpaces = false;\n+    CDSConfig::disable_dumping_dynamic_archive();\n@@ -3058,1 +3070,1 @@\n-    DynamicDumpSharedSpaces = true;\n+    CDSConfig::enable_dumping_dynamic_archive();\n@@ -3075,1 +3087,1 @@\n-  if (UseSharedSpaces && !DumpSharedSpaces && check_unsupported_cds_runtime_properties()) {\n+  if (UseSharedSpaces && check_unsupported_cds_runtime_properties()) {\n@@ -3079,1 +3091,1 @@\n-  if (DumpSharedSpaces || DynamicDumpSharedSpaces) {\n+  if (CDSConfig::is_dumping_archive()) {\n@@ -3378,1 +3390,1 @@\n-  if (DumpSharedSpaces) {\n+  if (CDSConfig::is_dumping_static_archive()) {\n@@ -3389,1 +3401,1 @@\n-  if (DumpSharedSpaces || UseSharedSpaces) {\n+  if (CDSConfig::is_dumping_static_archive() || UseSharedSpaces) {\n@@ -3459,1 +3471,1 @@\n-    if (DumpSharedSpaces) {\n+    if (CDSConfig::is_dumping_static_archive()) {\n@@ -3476,1 +3488,1 @@\n-    if (is_dumping_archive() && archives > 1) {\n+    if (CDSConfig::is_dumping_archive() && archives > 1) {\n@@ -3481,1 +3493,1 @@\n-    if (DumpSharedSpaces) {\n+    if (CDSConfig::is_dumping_static_archive()) {\n@@ -3508,1 +3520,1 @@\n-            DynamicDumpSharedSpaces = true;\n+            CDSConfig::enable_dumping_dynamic_archive();\n@@ -3923,6 +3935,0 @@\n-  if (CountCompiledCalls) {\n-    if (UseCounterDecay) {\n-      warning(\"UseCounterDecay disabled because CountCalls is set\");\n-      UseCounterDecay = false;\n-    }\n-  }\n@@ -3946,1 +3952,1 @@\n-  if (DumpSharedSpaces || RequireSharedSpaces) {\n+  if (CDSConfig::is_dumping_static_archive() || RequireSharedSpaces) {\n","filename":"src\/hotspot\/share\/runtime\/arguments.cpp","additions":38,"deletions":32,"binary":false,"changes":70,"status":"modified"},{"patch":"@@ -1227,3 +1227,0 @@\n-  product(bool, UseCounterDecay, true,                                      \\\n-          \"Adjust recompilation counters\")                                  \\\n-                                                                            \\\n@@ -1233,4 +1230,0 @@\n-  develop(intx, CounterDecayMinIntervalLength,   500,                       \\\n-          \"The minimum interval (in milliseconds) between invocation of \"   \\\n-          \"CounterDecay\")                                                   \\\n-                                                                            \\\n@@ -1985,1 +1978,1 @@\n-  product(int, LockingMode, LM_LIGHTWEIGHT,                                 \\\n+  product(int, LockingMode, LM_LEGACY,                                      \\\n@@ -1988,2 +1981,2 @@\n-          \"1: monitors & legacy stack-locking (LM_LEGACY), \"                \\\n-          \"2: monitors & new lightweight locking (LM_LIGHTWEIGHT, default)\") \\\n+          \"1: monitors & legacy stack-locking (LM_LEGACY, default), \"       \\\n+          \"2: monitors & new lightweight locking (LM_LIGHTWEIGHT)\")         \\\n","filename":"src\/hotspot\/share\/runtime\/globals.hpp","additions":3,"deletions":10,"binary":false,"changes":13,"status":"modified"},{"patch":"@@ -66,39 +66,0 @@\n-class ObjectMonitorsHashtable::PtrList :\n-  public LinkedListImpl<ObjectMonitor*,\n-                        AnyObj::C_HEAP, mtThread,\n-                        AllocFailStrategy::RETURN_NULL> {};\n-\n-class CleanupObjectMonitorsHashtable: StackObj {\n- public:\n-  bool do_entry(void*& key, ObjectMonitorsHashtable::PtrList*& list) {\n-    list->clear();  \/\/ clear the LinkListNodes\n-    delete list;    \/\/ then delete the LinkedList\n-    return true;\n-  }\n-};\n-\n-ObjectMonitorsHashtable::~ObjectMonitorsHashtable() {\n-  CleanupObjectMonitorsHashtable cleanup;\n-  _ptrs->unlink(&cleanup);  \/\/ cleanup the LinkedLists\n-  delete _ptrs;             \/\/ then delete the hash table\n-}\n-\n-void ObjectMonitorsHashtable::add_entry(void* key, ObjectMonitor* om) {\n-  ObjectMonitorsHashtable::PtrList* list = get_entry(key);\n-  if (list == nullptr) {\n-    \/\/ Create new list and add it to the hash table:\n-    list = new (mtThread) ObjectMonitorsHashtable::PtrList;\n-    add_entry(key, list);\n-  }\n-  list->add(om);  \/\/ Add the ObjectMonitor to the list.\n-  _om_count++;\n-}\n-\n-bool ObjectMonitorsHashtable::has_entry(void* key, ObjectMonitor* om) {\n-  ObjectMonitorsHashtable::PtrList* list = get_entry(key);\n-  if (list == nullptr || list->find(om) == nullptr) {\n-    return false;\n-  }\n-  return true;\n-}\n-\n@@ -1102,6 +1063,3 @@\n-\/\/ Iterate ObjectMonitors where the owner == thread; this does NOT include\n-\/\/ ObjectMonitors where owner is set to a stack-lock address in thread.\n-\/\/\n-\/\/ This version of monitors_iterate() works with the in-use monitor list.\n-\/\/\n-void ObjectSynchronizer::monitors_iterate(MonitorClosure* closure, JavaThread* thread) {\n+\/\/ Iterate over all ObjectMonitors.\n+template <typename Function>\n+void ObjectSynchronizer::monitors_iterate(Function function) {\n@@ -1110,17 +1068,2 @@\n-    ObjectMonitor* mid = iter.next();\n-    if (mid->owner() != thread) {\n-      \/\/ Not owned by the target thread and intentionally skips when owner\n-      \/\/ is set to a stack-lock address in the target thread.\n-      continue;\n-    }\n-    if (!mid->is_being_async_deflated() && mid->object_peek() != nullptr) {\n-      \/\/ Only process with closure if the object is set.\n-\n-      \/\/ monitors_iterate() is only called at a safepoint or when the\n-      \/\/ target thread is suspended or when the target thread is\n-      \/\/ operating on itself. The current closures in use today are\n-      \/\/ only interested in an owned ObjectMonitor and ownership\n-      \/\/ cannot be dropped under the calling contexts so the\n-      \/\/ ObjectMonitor cannot be async deflated.\n-      closure->do_monitor(mid);\n-    }\n+    ObjectMonitor* monitor = iter.next();\n+    function(monitor);\n@@ -1130,21 +1073,16 @@\n-\/\/ This version of monitors_iterate() works with the specified linked list.\n-\/\/\n-void ObjectSynchronizer::monitors_iterate(MonitorClosure* closure,\n-                                          ObjectMonitorsHashtable::PtrList* list,\n-                                          JavaThread* thread) {\n-  typedef LinkedListIterator<ObjectMonitor*> ObjectMonitorIterator;\n-  ObjectMonitorIterator iter(list->head());\n-  while (!iter.is_empty()) {\n-    ObjectMonitor* mid = *iter.next();\n-    \/\/ Owner set to a stack-lock address in thread should never be seen here:\n-    assert(mid->owner() == thread, \"must be\");\n-    if (!mid->is_being_async_deflated() && mid->object_peek() != nullptr) {\n-      \/\/ Only process with closure if the object is set.\n-\n-      \/\/ monitors_iterate() is only called at a safepoint or when the\n-      \/\/ target thread is suspended or when the target thread is\n-      \/\/ operating on itself. The current closures in use today are\n-      \/\/ only interested in an owned ObjectMonitor and ownership\n-      \/\/ cannot be dropped under the calling contexts so the\n-      \/\/ ObjectMonitor cannot be async deflated.\n-      closure->do_monitor(mid);\n+\/\/ Iterate ObjectMonitors owned by any thread and where the owner `filter`\n+\/\/ returns true.\n+template <typename OwnerFilter>\n+void ObjectSynchronizer::owned_monitors_iterate_filtered(MonitorClosure* closure, OwnerFilter filter) {\n+  monitors_iterate([&](ObjectMonitor* monitor) {\n+    \/\/ This function is only called at a safepoint or when the\n+    \/\/ target thread is suspended or when the target thread is\n+    \/\/ operating on itself. The current closures in use today are\n+    \/\/ only interested in an owned ObjectMonitor and ownership\n+    \/\/ cannot be dropped under the calling contexts so the\n+    \/\/ ObjectMonitor cannot be async deflated.\n+    if (monitor->has_owner() && filter(monitor->owner_raw())) {\n+      assert(!monitor->is_being_async_deflated(), \"Owned monitors should not be deflating\");\n+      assert(monitor->object_peek() != nullptr, \"Owned monitors should not have a dead object\");\n+\n+      closure->do_monitor(monitor);\n@@ -1152,1 +1090,14 @@\n-  }\n+  });\n+}\n+\n+\/\/ Iterate ObjectMonitors where the owner == thread; this does NOT include\n+\/\/ ObjectMonitors where owner is set to a stack-lock address in thread.\n+void ObjectSynchronizer::owned_monitors_iterate(MonitorClosure* closure, JavaThread* thread) {\n+  auto thread_filter = [&](void* owner) { return owner == thread; };\n+  return owned_monitors_iterate_filtered(closure, thread_filter);\n+}\n+\n+\/\/ Iterate ObjectMonitors owned by any thread.\n+void ObjectSynchronizer::owned_monitors_iterate(MonitorClosure* closure) {\n+  auto all_filter = [&](void* owner) { return true; };\n+  return owned_monitors_iterate_filtered(closure, all_filter);\n@@ -1259,1 +1210,7 @@\n-bool ObjectSynchronizer::request_deflate_idle_monitors() {\n+void ObjectSynchronizer::request_deflate_idle_monitors() {\n+  MonitorLocker ml(MonitorDeflation_lock, Mutex::_no_safepoint_check_flag);\n+  set_is_async_deflation_requested(true);\n+  ml.notify_all();\n+}\n+\n+bool ObjectSynchronizer::request_deflate_idle_monitors_from_wb() {\n@@ -1264,5 +1221,3 @@\n-  set_is_async_deflation_requested(true);\n-  {\n-    MonitorLocker ml(MonitorDeflation_lock, Mutex::_no_safepoint_check_flag);\n-    ml.notify_all();\n-  }\n+\n+  request_deflate_idle_monitors();\n+\n@@ -1585,9 +1540,1 @@\n-\/\/ If table != nullptr, we gather owned ObjectMonitors indexed by the\n-\/\/ owner in the table. Please note that ObjectMonitors where the owner\n-\/\/ is set to a stack-lock address are NOT associated with the JavaThread\n-\/\/ that holds that stack-lock. All of the current consumers of\n-\/\/ ObjectMonitorsHashtable info only care about JNI locked monitors and\n-\/\/ those do not have the owner set to a stack-lock address.\n-\/\/\n-                                                elapsedTimer* timer_p,\n-                                                ObjectMonitorsHashtable* table) {\n+                                                elapsedTimer* timer_p) {\n@@ -1605,12 +1552,0 @@\n-    } else if (table != nullptr) {\n-      \/\/ The caller is interested in the owned ObjectMonitors. This does\n-      \/\/ not include when owner is set to a stack-lock address in thread.\n-      \/\/ This also does not capture unowned ObjectMonitors that cannot be\n-      \/\/ deflated because of a waiter.\n-      void* key = mid->owner();\n-      \/\/ Since deflate_idle_monitors() and deflate_monitor_list() can be\n-      \/\/ called more than once, we have to make sure the entry has not\n-      \/\/ already been added.\n-      if (key != nullptr && !table->has_entry(key, mid)) {\n-        table->add_entry(key, mid);\n-      }\n@@ -1660,3 +1595,2 @@\n-\/\/ ObjectMonitors. It is also called via do_final_audit_and_print_stats()\n-\/\/ and VM_ThreadDump::doit() by the VMThread.\n-size_t ObjectSynchronizer::deflate_idle_monitors(ObjectMonitorsHashtable* table) {\n+\/\/ ObjectMonitors.\n+size_t ObjectSynchronizer::deflate_idle_monitors() {\n@@ -1687,1 +1621,1 @@\n-  size_t deflated_count = deflate_monitor_list(current, ls, &timer, table);\n+  size_t deflated_count = deflate_monitor_list(current, ls, &timer);\n@@ -1690,5 +1624,2 @@\n-  if (deflated_count > 0 || is_final_audit()) {\n-    \/\/ There are ObjectMonitors that have been deflated or this is the\n-    \/\/ final audit and all the remaining ObjectMonitors have been\n-    \/\/ deflated, BUT the MonitorDeflationThread blocked for the final\n-    \/\/ safepoint during unlinking.\n+  if (deflated_count > 0) {\n+    \/\/ There are ObjectMonitors that have been deflated.\n@@ -1769,4 +1700,0 @@\n-    if (table != nullptr) {\n-      ls->print_cr(\"ObjectMonitorsHashtable: key_count=\" SIZE_FORMAT \", om_count=\" SIZE_FORMAT,\n-                   table->key_count(), table->om_count());\n-    }\n@@ -1825,1 +1752,1 @@\n-  ObjectSynchronizer::monitors_iterate(&rjmc, current);\n+  ObjectSynchronizer::owned_monitors_iterate(&rjmc, current);\n@@ -1879,6 +1806,0 @@\n-    \/\/ Do deflations in order to reduce the in-use monitor population\n-    \/\/ that is reported by ObjectSynchronizer::log_in_use_monitor_details()\n-    \/\/ which is called by ObjectSynchronizer::audit_and_print_stats().\n-    while (deflate_idle_monitors(\/* ObjectMonitorsHashtable is not needed here *\/ nullptr) > 0) {\n-      ; \/\/ empty\n-    }\n@@ -1933,1 +1854,1 @@\n-    log_in_use_monitor_details(ls);\n+    log_in_use_monitor_details(ls, !on_exit \/* log_all *\/);\n@@ -2013,2 +1934,1 @@\n-void ObjectSynchronizer::log_in_use_monitor_details(outputStream* out) {\n-  stringStream ss;\n+void ObjectSynchronizer::log_in_use_monitor_details(outputStream* out, bool log_all) {\n@@ -2016,0 +1936,1 @@\n+    stringStream ss;\n@@ -2021,12 +1942,18 @@\n-    MonitorList::Iterator iter = _in_use_list.iterator();\n-    while (iter.has_next()) {\n-      ObjectMonitor* mid = iter.next();\n-      const oop obj = mid->object_peek();\n-      const markWord mark = mid->header();\n-      ResourceMark rm;\n-      out->print(INTPTR_FORMAT \"  %d%d%d  \" INTPTR_FORMAT \"  %s\", p2i(mid),\n-                 mid->is_busy(), mark.hash() != 0, mid->owner() != nullptr,\n-                 p2i(obj), obj == nullptr ? \"\" : obj->klass()->external_name());\n-      if (mid->is_busy()) {\n-        out->print(\" (%s)\", mid->is_busy_to_string(&ss));\n-        ss.reset();\n+\n+    auto is_interesting = [&](ObjectMonitor* monitor) {\n+      return log_all || monitor->has_owner() || monitor->is_busy();\n+    };\n+\n+    monitors_iterate([&](ObjectMonitor* monitor) {\n+      if (is_interesting(monitor)) {\n+        const oop obj = monitor->object_peek();\n+        const markWord mark = monitor->header();\n+        ResourceMark rm;\n+        out->print(INTPTR_FORMAT \"  %d%d%d  \" INTPTR_FORMAT \"  %s\", p2i(monitor),\n+                   monitor->is_busy(), mark.hash() != 0, monitor->owner() != nullptr,\n+                   p2i(obj), obj == nullptr ? \"\" : obj->klass()->external_name());\n+        if (monitor->is_busy()) {\n+          out->print(\" (%s)\", monitor->is_busy_to_string(&ss));\n+          ss.reset();\n+        }\n+        out->cr();\n@@ -2034,2 +1961,1 @@\n-      out->cr();\n-    }\n+    });\n","filename":"src\/hotspot\/share\/runtime\/synchronizer.cpp","additions":74,"deletions":148,"binary":false,"changes":222,"status":"modified"},{"patch":"@@ -28,0 +28,1 @@\n+#include \"cds\/cdsConfig.hpp\"\n@@ -55,0 +56,1 @@\n+#include \"nmt\/memTracker.hpp\"\n@@ -59,1 +61,1 @@\n-#include \"prims\/jvmtiAgentList.hpp\"\n+#include \"prims\/jvmtiAgentList.hpp\"\n@@ -64,1 +66,1 @@\n-#include \"runtime\/handles.inline.hpp\"\n+#include \"runtime\/handles.inline.hpp\"\n@@ -83,0 +85,1 @@\n+#include \"runtime\/stackWatermarkSet.inline.hpp\"\n@@ -86,1 +89,1 @@\n-#include \"runtime\/threads.hpp\"\n+#include \"runtime\/threads.hpp\"\n@@ -95,1 +98,0 @@\n-#include \"services\/memTracker.hpp\"\n@@ -827,1 +829,1 @@\n-  if (DumpSharedSpaces) {\n+  if (CDSConfig::is_dumping_static_archive()) {\n@@ -1243,0 +1245,6 @@\n+    \/\/ Need to start processing before accessing oops in the thread.\n+    StackWatermark* watermark = StackWatermarkSet::get(q, StackWatermarkKind::gc);\n+    if (watermark != nullptr) {\n+      watermark->start_processing();\n+    }\n+\n","filename":"src\/hotspot\/share\/runtime\/threads.cpp","additions":13,"deletions":5,"binary":false,"changes":18,"status":"modified"},{"patch":"@@ -88,0 +88,1 @@\n+#include \"oops\/resolvedMethodEntry.hpp\"\n@@ -237,1 +238,0 @@\n-  nonstatic_field(ConstantPoolCache,           _length,                                       int)                                   \\\n@@ -241,0 +241,2 @@\n+  nonstatic_field(ConstantPoolCache,           _resolved_method_entries,                      Array<ResolvedMethodEntry>*)           \\\n+  nonstatic_field(ResolvedMethodEntry,         _cpool_index,                                  u2)                                    \\\n@@ -350,9 +352,0 @@\n-  \/***********************\/                                                                                                          \\\n-  \/* Constant Pool Cache *\/                                                                                                          \\\n-  \/***********************\/                                                                                                          \\\n-                                                                                                                                     \\\n-  volatile_nonstatic_field(ConstantPoolCacheEntry,      _indices,                             intx)                                  \\\n-  volatile_nonstatic_field(ConstantPoolCacheEntry,      _f1,                                  Metadata*)                             \\\n-  volatile_nonstatic_field(ConstantPoolCacheEntry,      _f2,                                  intx)                                  \\\n-  volatile_nonstatic_field(ConstantPoolCacheEntry,      _flags,                               intx)                                  \\\n-                                                                                                                                     \\\n@@ -501,0 +494,2 @@\n+  nonstatic_field(Array<ResolvedMethodEntry>,  _length,                                       int)                                   \\\n+  nonstatic_field(Array<ResolvedMethodEntry>,  _data[0],                                      ResolvedMethodEntry)                   \\\n@@ -988,0 +983,1 @@\n+  unchecked_nonstatic_field(Array<ResolvedMethodEntry>,_data,                                 sizeof(ResolvedMethodEntry))           \\\n@@ -1508,0 +1504,1 @@\n+  declare_c2_type(ConvertNode, TypeNode)                                  \\\n@@ -1763,6 +1760,1 @@\n-  declare_c2_type(ReplicateBNode, VectorNode)                             \\\n-  declare_c2_type(ReplicateSNode, VectorNode)                             \\\n-  declare_c2_type(ReplicateINode, VectorNode)                             \\\n-  declare_c2_type(ReplicateLNode, VectorNode)                             \\\n-  declare_c2_type(ReplicateFNode, VectorNode)                             \\\n-  declare_c2_type(ReplicateDNode, VectorNode)                             \\\n+  declare_c2_type(ReplicateNode, VectorNode)                              \\\n@@ -1922,0 +1914,1 @@\n+            declare_type(Array<ResolvedMethodEntry>, MetaspaceObj)        \\\n@@ -1939,1 +1932,1 @@\n-  declare_toplevel_type(ConstantPoolCacheEntry)                           \\\n+  declare_toplevel_type(ResolvedMethodEntry)                              \\\n@@ -2217,12 +2210,0 @@\n-  declare_constant(ConstantPool::CPCACHE_INDEX_TAG)                       \\\n-                                                                          \\\n-  \/********************************\/                                      \\\n-  \/* ConstantPoolCacheEntry enums *\/                                      \\\n-  \/********************************\/                                      \\\n-                                                                          \\\n-  declare_constant(ConstantPoolCacheEntry::is_volatile_shift)             \\\n-  declare_constant(ConstantPoolCacheEntry::is_final_shift)                \\\n-  declare_constant(ConstantPoolCacheEntry::is_forced_virtual_shift)       \\\n-  declare_constant(ConstantPoolCacheEntry::is_vfinal_shift)               \\\n-  declare_constant(ConstantPoolCacheEntry::is_field_entry_shift)          \\\n-  declare_constant(ConstantPoolCacheEntry::tos_state_shift)               \\\n","filename":"src\/hotspot\/share\/runtime\/vmStructs.cpp","additions":10,"deletions":29,"binary":false,"changes":39,"status":"modified"},{"patch":"@@ -50,0 +50,2 @@\n+import java.util.stream.Collectors;\n+import java.util.stream.Stream;\n@@ -52,1 +54,0 @@\n-\n@@ -86,0 +87,4 @@\n+\n+        public void putAll(Map<String, String> map) {\n+            map.entrySet().forEach(e -> put(e.getKey(), () -> e.getValue()));\n+        }\n@@ -138,0 +143,1 @@\n+        map.putAll(xOptFlags()); \/\/ -Xmx4g -> @requires vm.opt.x.Xmx == \"4g\" )\n@@ -605,3 +611,4 @@\n-        ProcessBuilder pb = new ProcessBuilder(Container.ENGINE_COMMAND, \"ps\");\n-        Map<String, String> logFileNames = redirectOutputToLogFile(\"checkDockerSupport(): <container> ps\",\n-                                                      pb, \"container-ps\");\n+        ProcessBuilder pb = new ProcessBuilder(\"which\", Container.ENGINE_COMMAND);\n+        Map<String, String> logFileNames =\n+            redirectOutputToLogFile(\"checkDockerSupport(): which \" + Container.ENGINE_COMMAND,\n+                                                      pb, \"which-container\");\n@@ -664,3 +671,1 @@\n-        List<String> allFlags = new ArrayList<String>();\n-        Collections.addAll(allFlags, System.getProperty(\"test.vm.opts\", \"\").trim().split(\"\\\\s+\"));\n-        Collections.addAll(allFlags, System.getProperty(\"test.java.opts\", \"\").trim().split(\"\\\\s+\"));\n+        List<String> allFlags = allFlags().toList();\n@@ -717,0 +722,25 @@\n+    private Stream<String> allFlags() {\n+        return Stream.of((System.getProperty(\"test.vm.opts\", \"\") + \" \" + System.getProperty(\"test.java.opts\", \"\")).trim().split(\"\\\\s+\"));\n+    }\n+\n+    \/**\n+     * Parses extra options, options that start with -X excluding the\n+     * bare -X option (as it is not considered an extra option).\n+     * Ignores extra options not starting with -X\n+     *\n+     * This could be improved to handle extra options not starting\n+     * with -X as well as \"standard\" options.\n+     *\/\n+    private Map<String, String> xOptFlags() {\n+        return allFlags()\n+            .filter(s -> s.startsWith(\"-X\") && !s.startsWith(\"-XX:\") && !s.equals(\"-X\"))\n+            .map(s -> s.replaceFirst(\"-\", \"\"))\n+            .map(flag -> flag.splitWithDelimiters(\"[:0123456789]\", 2))\n+            .collect(Collectors.toMap(a -> \"vm.opt.x.\" + a[0],\n+                                      a -> (a.length == 1)\n+                                      ? \"true\" \/\/ -Xnoclassgc\n+                                      : (a[1].equals(\":\")\n+                                         ? a[2]            \/\/ [\"-XshowSettings\", \":\", \"system\"]\n+                                         : a[1] + a[2]))); \/\/ [\"-Xmx\", \"4\", \"g\"]\n+    }\n+\n","filename":"test\/jtreg-ext\/requires\/VMProps.java","additions":37,"deletions":7,"binary":false,"changes":44,"status":"modified"}]}