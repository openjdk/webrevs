{"files":[{"patch":"@@ -623,0 +623,4 @@\n+# Microbenchmarks are executed from the root of the test image directory.\n+# This enables JMH tests to add dependencies using relative paths such as\n+# -Djava.library.path=micro\/native\n+\n@@ -628,0 +632,1 @@\n+\t    $$(CD) $$(TEST_IMAGE_DIR) && \\\n","filename":"make\/RunTests.gmk","additions":5,"deletions":0,"binary":false,"changes":5,"status":"modified"},{"patch":"@@ -2,1 +2,1 @@\n-\/\/ Copyright (c) 2003, 2023, Oracle and\/or its affiliates. All rights reserved.\n+\/\/ Copyright (c) 2003, 2024, Oracle and\/or its affiliates. All rights reserved.\n@@ -8254,0 +8254,18 @@\n+\/\/ ============================================================================\n+\/\/ VerifyVectorAlignment Instruction\n+\n+instruct verify_vector_alignment(iRegP addr, immL_positive_bitmaskI mask, rFlagsReg cr) %{\n+  match(Set addr (VerifyVectorAlignment addr mask));\n+  effect(KILL cr);\n+  format %{ \"verify_vector_alignment $addr $mask \\t! verify alignment\" %}\n+  ins_encode %{\n+    Label Lskip;\n+    \/\/ check if masked bits of addr are zero\n+    __ tst($addr$$Register, $mask$$constant);\n+    __ br(Assembler::EQ, Lskip);\n+    __ stop(\"verify_vector_alignment found a misaligned vector memory access\");\n+    __ bind(Lskip);\n+  %}\n+  ins_pipe(pipe_slow);\n+%}\n+\n","filename":"src\/hotspot\/cpu\/aarch64\/aarch64.ad","additions":19,"deletions":1,"binary":false,"changes":20,"status":"modified"},{"patch":"@@ -2,1 +2,1 @@\n- * Copyright (c) 2000, 2023, Oracle and\/or its affiliates. All rights reserved.\n+ * Copyright (c) 2000, 2024, Oracle and\/or its affiliates. All rights reserved.\n@@ -285,1 +285,2 @@\n-      __ ldp(r19, r20, Address(OSR_buf, slot_offset));\n+      __ ldr(r19, Address(OSR_buf, slot_offset));\n+      __ ldr(r20, Address(OSR_buf, slot_offset + BytesPerWord));\n","filename":"src\/hotspot\/cpu\/aarch64\/c1_LIRAssembler_aarch64.cpp","additions":3,"deletions":2,"binary":false,"changes":5,"status":"modified"},{"patch":"@@ -3606,5 +3606,3 @@\n-  \/\/ make sure klass is initialized & doesn't have finalizer\n-  \/\/ make sure klass is fully initialized\n-  __ ldrb(rscratch1, Address(r4, InstanceKlass::init_state_offset()));\n-  __ cmp(rscratch1, (u1)InstanceKlass::fully_initialized);\n-  __ br(Assembler::NE, slow_case);\n+  \/\/ make sure klass is initialized\n+  assert(VM_Version::supports_fast_class_init_checks(), \"Optimization requires support for fast class initialization checks\");\n+  __ clinit_barrier(r4, rscratch1, nullptr \/*L_fast_path*\/, &slow_case);\n","filename":"src\/hotspot\/cpu\/aarch64\/templateTable_aarch64.cpp","additions":3,"deletions":5,"binary":false,"changes":8,"status":"modified"},{"patch":"@@ -2,1 +2,1 @@\n- * Copyright (c) 2008, 2023, Oracle and\/or its affiliates. All rights reserved.\n+ * Copyright (c) 2008, 2024, Oracle and\/or its affiliates. All rights reserved.\n@@ -2643,2 +2643,2 @@\n-  assert(src->is_double_cpu() && dest->is_address() ||\n-         src->is_address() && dest->is_double_cpu(),\n+  assert((src->is_double_cpu() && dest->is_address()) ||\n+         (src->is_address() && dest->is_double_cpu()),\n","filename":"src\/hotspot\/cpu\/arm\/c1_LIRAssembler_arm.cpp","additions":3,"deletions":3,"binary":false,"changes":6,"status":"modified"},{"patch":"@@ -2,1 +2,1 @@\n- * Copyright (c) 2020, 2023, Oracle and\/or its affiliates. All rights reserved.\n+ * Copyright (c) 2020, 2024, Oracle and\/or its affiliates. All rights reserved.\n@@ -1092,1 +1092,2 @@\n-  assert_different_registers(a, b, tmp, atmp, btmp);\n+  assert_different_registers(a, tmp, atmp, btmp);\n+  assert_different_registers(b, tmp, atmp, btmp);\n@@ -1179,1 +1180,2 @@\n-  assert_different_registers(dst, a, b, atmp, btmp);\n+  assert_different_registers(dst, a, atmp, btmp);\n+  assert_different_registers(dst, b, atmp, btmp);\n@@ -5226,2 +5228,2 @@\n-  assert(vec_enc == AVX_128bit && VM_Version::supports_avx() ||\n-         vec_enc == AVX_256bit && (VM_Version::supports_avx2() || type2aelembytes(bt) >= 4), \"\");\n+  assert((vec_enc == AVX_128bit && VM_Version::supports_avx()) ||\n+         (vec_enc == AVX_256bit && (VM_Version::supports_avx2() || type2aelembytes(bt) >= 4)), \"\");\n","filename":"src\/hotspot\/cpu\/x86\/c2_MacroAssembler_x86.cpp","additions":7,"deletions":5,"binary":false,"changes":12,"status":"modified"},{"patch":"@@ -1874,86 +1874,0 @@\n-void MacroAssembler::cvtss2sd(XMMRegister dst, XMMRegister src) {\n-  if ((UseAVX > 0) && (dst != src)) {\n-    xorpd(dst, dst);\n-  }\n-  Assembler::cvtss2sd(dst, src);\n-}\n-\n-void MacroAssembler::cvtss2sd(XMMRegister dst, Address src) {\n-  if (UseAVX > 0) {\n-    xorpd(dst, dst);\n-  }\n-  Assembler::cvtss2sd(dst, src);\n-}\n-\n-void MacroAssembler::cvtsd2ss(XMMRegister dst, XMMRegister src) {\n-  if ((UseAVX > 0) && (dst != src)) {\n-    xorps(dst, dst);\n-  }\n-  Assembler::cvtsd2ss(dst, src);\n-}\n-\n-void MacroAssembler::cvtsd2ss(XMMRegister dst, Address src) {\n-  if (UseAVX > 0) {\n-    xorps(dst, dst);\n-  }\n-  Assembler::cvtsd2ss(dst, src);\n-}\n-\n-void MacroAssembler::cvtsi2sdl(XMMRegister dst, Register src) {\n-  if (UseAVX > 0) {\n-    xorpd(dst, dst);\n-  }\n-  Assembler::cvtsi2sdl(dst, src);\n-}\n-\n-void MacroAssembler::cvtsi2sdl(XMMRegister dst, Address src) {\n-  if (UseAVX > 0) {\n-    xorpd(dst, dst);\n-  }\n-  Assembler::cvtsi2sdl(dst, src);\n-}\n-\n-void MacroAssembler::cvtsi2ssl(XMMRegister dst, Register src) {\n-  if (UseAVX > 0) {\n-    xorps(dst, dst);\n-  }\n-  Assembler::cvtsi2ssl(dst, src);\n-}\n-\n-void MacroAssembler::cvtsi2ssl(XMMRegister dst, Address src) {\n-  if (UseAVX > 0) {\n-    xorps(dst, dst);\n-  }\n-  Assembler::cvtsi2ssl(dst, src);\n-}\n-\n-#ifdef _LP64\n-void MacroAssembler::cvtsi2sdq(XMMRegister dst, Register src) {\n-  if (UseAVX > 0) {\n-    xorpd(dst, dst);\n-  }\n-  Assembler::cvtsi2sdq(dst, src);\n-}\n-\n-void MacroAssembler::cvtsi2sdq(XMMRegister dst, Address src) {\n-  if (UseAVX > 0) {\n-    xorpd(dst, dst);\n-  }\n-  Assembler::cvtsi2sdq(dst, src);\n-}\n-\n-void MacroAssembler::cvtsi2ssq(XMMRegister dst, Register src) {\n-  if (UseAVX > 0) {\n-    xorps(dst, dst);\n-  }\n-  Assembler::cvtsi2ssq(dst, src);\n-}\n-\n-void MacroAssembler::cvtsi2ssq(XMMRegister dst, Address src) {\n-  if (UseAVX > 0) {\n-    xorps(dst, dst);\n-  }\n-  Assembler::cvtsi2ssq(dst, src);\n-}\n-#endif  \/\/ _LP64\n-\n","filename":"src\/hotspot\/cpu\/x86\/macroAssembler_x86.cpp","additions":0,"deletions":86,"binary":false,"changes":86,"status":"modified"},{"patch":"@@ -114,2 +114,2 @@\n-        op == 0x0F && (branch[1] & 0xF0) == 0x80 \/* jcc *\/ ||\n-        op == 0xC7 && branch[1] == 0xF8 \/* xbegin *\/,\n+        (op == 0x0F && (branch[1] & 0xF0) == 0x80) \/* jcc *\/ ||\n+        (op == 0xC7 && branch[1] == 0xF8) \/* xbegin *\/,\n@@ -814,17 +814,0 @@\n-\n-  \/\/ cvt instructions\n-  void cvtss2sd(XMMRegister dst, XMMRegister src);\n-  void cvtss2sd(XMMRegister dst, Address src);\n-  void cvtsd2ss(XMMRegister dst, XMMRegister src);\n-  void cvtsd2ss(XMMRegister dst, Address src);\n-  void cvtsi2sdl(XMMRegister dst, Register src);\n-  void cvtsi2sdl(XMMRegister dst, Address src);\n-  void cvtsi2ssl(XMMRegister dst, Register src);\n-  void cvtsi2ssl(XMMRegister dst, Address src);\n-#ifdef _LP64\n-  void cvtsi2sdq(XMMRegister dst, Register src);\n-  void cvtsi2sdq(XMMRegister dst, Address src);\n-  void cvtsi2ssq(XMMRegister dst, Register src);\n-  void cvtsi2ssq(XMMRegister dst, Address src);\n-#endif\n-\n","filename":"src\/hotspot\/cpu\/x86\/macroAssembler_x86.hpp","additions":2,"deletions":19,"binary":false,"changes":21,"status":"modified"},{"patch":"@@ -29,0 +29,1 @@\n+#include \"utilities\/globalDefinitions.hpp\"\n@@ -62,3 +63,10 @@\n-  \/\/ check if locked\n-  __ testptr(result, markWord::unlocked_value);\n-  __ jcc(Assembler::zero, slowCase);\n+\n+  if (LockingMode == LM_LIGHTWEIGHT) {\n+    \/\/ check if monitor\n+    __ testptr(result, markWord::monitor_value);\n+    __ jcc(Assembler::notZero, slowCase);\n+  } else {\n+    \/\/ check if locked\n+    __ testptr(result, markWord::unlocked_value);\n+    __ jcc(Assembler::zero, slowCase);\n+  }\n","filename":"src\/hotspot\/cpu\/x86\/sharedRuntime_x86.cpp","additions":11,"deletions":3,"binary":false,"changes":14,"status":"modified"},{"patch":"@@ -4051,2 +4051,5 @@\n-  \/\/ make sure klass is initialized & doesn't have finalizer\n-  \/\/ make sure klass is fully initialized\n+  \/\/ make sure klass is initialized\n+#ifdef _LP64\n+  assert(VM_Version::supports_fast_class_init_checks(), \"must support fast class initialization checks\");\n+  __ clinit_barrier(rcx, r15_thread, nullptr \/*L_fast_path*\/, &slow_case);\n+#else\n@@ -4055,0 +4058,1 @@\n+#endif\n@@ -4056,0 +4060,1 @@\n+  \/\/ make sure klass doesn't have finalizer\n","filename":"src\/hotspot\/cpu\/x86\/templateTable_x86.cpp","additions":7,"deletions":2,"binary":false,"changes":9,"status":"modified"},{"patch":"@@ -10114,1 +10114,1 @@\n-  effect(TEMP dst);\n+\n@@ -10136,1 +10136,1 @@\n-  effect(TEMP dst);\n+\n","filename":"src\/hotspot\/cpu\/x86\/x86_64.ad","additions":2,"deletions":2,"binary":false,"changes":4,"status":"modified"},{"patch":"@@ -2,1 +2,1 @@\n- * Copyright (c) 2003, 2023, Oracle and\/or its affiliates. All rights reserved.\n+ * Copyright (c) 2003, 2024, Oracle and\/or its affiliates. All rights reserved.\n@@ -581,6 +581,8 @@\n-    assert(ent->in_named_module(), \"must be\");\n-    bool cond = strcmp(file, ent->name()) == 0;\n-    log_debug(class, path)(\"get_module_shared_path_index (%d) %s : %s = %s\", i,\n-                           location->as_C_string(), ent->name(), cond ? \"same\" : \"different\");\n-    if (cond) {\n-      return i;\n+    if (!ent->is_non_existent()) {\n+      assert(ent->in_named_module(), \"must be\");\n+      bool cond = strcmp(file, ent->name()) == 0;\n+      log_debug(class, path)(\"get_module_shared_path_index (%d) %s : %s = %s\", i,\n+                             location->as_C_string(), ent->name(), cond ? \"same\" : \"different\");\n+      if (cond) {\n+        return i;\n+      }\n@@ -1470,1 +1472,1 @@\n-bool FileMapRegion::check_region_crc() const {\n+bool FileMapRegion::check_region_crc(char* base) const {\n@@ -1479,2 +1481,2 @@\n-  assert(mapped_base() != nullptr, \"must be initialized\");\n-  int crc = ClassLoader::crc32(0, mapped_base(), (jint)sz);\n+  assert(base != nullptr, \"must be initialized\");\n+  int crc = ClassLoader::crc32(0, base, (jint)sz);\n@@ -1765,4 +1767,1 @@\n-  r->set_mapped_from_file(false);\n-  r->set_mapped_base(base);\n-\n-  if (VerifySharedSpaces && !r->check_region_crc()) {\n+  if (VerifySharedSpaces && !r->check_region_crc(base)) {\n@@ -1772,0 +1771,3 @@\n+  r->set_mapped_from_file(false);\n+  r->set_mapped_base(base);\n+\n@@ -1808,0 +1810,1 @@\n+      return MAP_ARCHIVE_SUCCESS;\n@@ -1822,0 +1825,5 @@\n+\n+    if (VerifySharedSpaces && !r->check_region_crc(requested_addr)) {\n+      return MAP_ARCHIVE_OTHER_FAILURE;\n+    }\n+\n@@ -1824,3 +1832,1 @@\n-  }\n-  if (VerifySharedSpaces && !r->check_region_crc()) {\n-    return MAP_ARCHIVE_OTHER_FAILURE;\n+    return MAP_ARCHIVE_SUCCESS;\n@@ -1829,2 +1835,0 @@\n-\n-  return MAP_ARCHIVE_SUCCESS;\n@@ -1848,2 +1852,1 @@\n-  r->set_mapped_base(bitmap_base);\n-  if (VerifySharedSpaces && !r->check_region_crc()) {\n+  if (VerifySharedSpaces && !r->check_region_crc(bitmap_base)) {\n@@ -1858,0 +1861,1 @@\n+  r->set_mapped_base(bitmap_base);\n@@ -2133,2 +2137,1 @@\n-  r->set_mapped_base(base);\n-  if (VerifySharedSpaces && !r->check_region_crc()) {\n+  if (VerifySharedSpaces && !r->check_region_crc(base)) {\n@@ -2140,0 +2143,2 @@\n+  r->set_mapped_base(base);\n+\n","filename":"src\/hotspot\/share\/cds\/filemap.cpp","additions":28,"deletions":23,"binary":false,"changes":51,"status":"modified"},{"patch":"@@ -2,1 +2,1 @@\n- * Copyright (c) 2003, 2023, Oracle and\/or its affiliates. All rights reserved.\n+ * Copyright (c) 2003, 2024, Oracle and\/or its affiliates. All rights reserved.\n@@ -92,0 +92,1 @@\n+  bool is_non_existent()  const { return _type == non_existent_entry; }\n@@ -173,1 +174,1 @@\n-  bool check_region_crc() const;\n+  bool check_region_crc(char* base) const;\n","filename":"src\/hotspot\/share\/cds\/filemap.hpp","additions":3,"deletions":2,"binary":false,"changes":5,"status":"modified"},{"patch":"@@ -2523,0 +2523,1 @@\n+                            false \/* unregister_nmethods_during_purge *\/,\n@@ -2534,0 +2535,4 @@\n+  {\n+    GCTraceTime(Debug, gc, phases) ur(\"Unregister NMethods\", timer);\n+    G1CollectedHeap::heap()->bulk_unregister_nmethods();\n+  }\n@@ -2545,0 +2550,27 @@\n+class G1BulkUnregisterNMethodTask : public WorkerTask {\n+  HeapRegionClaimer _hrclaimer;\n+\n+  class UnregisterNMethodsHeapRegionClosure : public HeapRegionClosure {\n+  public:\n+\n+    bool do_heap_region(HeapRegion* hr) {\n+      hr->rem_set()->bulk_remove_code_roots();\n+      return false;\n+    }\n+  } _cl;\n+\n+public:\n+  G1BulkUnregisterNMethodTask(uint num_workers)\n+  : WorkerTask(\"G1 Remove Unlinked NMethods From Code Root Set Task\"),\n+    _hrclaimer(num_workers) { }\n+\n+  void work(uint worker_id) {\n+    G1CollectedHeap::heap()->heap_region_par_iterate_from_worker_offset(&_cl, &_hrclaimer, worker_id);\n+  }\n+};\n+\n+void G1CollectedHeap::bulk_unregister_nmethods() {\n+  uint num_workers = workers()->active_workers();\n+  G1BulkUnregisterNMethodTask t(num_workers);\n+  workers()->run_task(&t);\n+}\n@@ -2969,25 +3001,0 @@\n-class UnregisterNMethodOopClosure: public OopClosure {\n-  G1CollectedHeap* _g1h;\n-  nmethod* _nm;\n-\n-public:\n-  UnregisterNMethodOopClosure(G1CollectedHeap* g1h, nmethod* nm) :\n-    _g1h(g1h), _nm(nm) {}\n-\n-  void do_oop(oop* p) {\n-    oop heap_oop = RawAccess<>::oop_load(p);\n-    if (!CompressedOops::is_null(heap_oop)) {\n-      oop obj = CompressedOops::decode_not_null(heap_oop);\n-      HeapRegion* hr = _g1h->heap_region_containing(obj);\n-      assert(!hr->is_continues_humongous(),\n-             \"trying to remove code root \" PTR_FORMAT \" in continuation of humongous region \" HR_FORMAT\n-             \" starting at \" HR_FORMAT,\n-             p2i(_nm), HR_FORMAT_PARAMS(hr), HR_FORMAT_PARAMS(hr->humongous_start_region()));\n-\n-      hr->remove_code_root(_nm);\n-    }\n-  }\n-\n-  void do_oop(narrowOop* p) { ShouldNotReachHere(); }\n-};\n-\n@@ -3001,3 +3008,2 @@\n-  guarantee(nm != nullptr, \"sanity\");\n-  UnregisterNMethodOopClosure reg_cl(this, nm);\n-  nm->oops_do(&reg_cl, true);\n+  \/\/ We always unregister nmethods in bulk during code unloading only.\n+  ShouldNotReachHere();\n","filename":"src\/hotspot\/share\/gc\/g1\/g1CollectedHeap.cpp","additions":34,"deletions":28,"binary":false,"changes":62,"status":"modified"},{"patch":"@@ -98,1 +98,1 @@\n-  const size_t padding_elem_num = (DEFAULT_CACHE_LINE_SIZE \/ sizeof(size_t));\n+  const size_t padding_elem_num = (DEFAULT_PADDING_SIZE \/ sizeof(size_t));\n","filename":"src\/hotspot\/share\/gc\/g1\/g1ParScanThreadState.cpp","additions":1,"deletions":1,"binary":false,"changes":2,"status":"modified"},{"patch":"@@ -176,2 +176,0 @@\n-  ParallelScavengeHeap* heap = ParallelScavengeHeap::heap();\n-\n@@ -181,2 +179,0 @@\n-  uint queue_size = claimed_stack_depth()->max_elems();\n-\n@@ -279,3 +275,1 @@\n-    if (PSScavenge::should_scavenge(p)) {\n-      claim_or_forward_depth(p);\n-    }\n+    claim_or_forward_depth(p);\n","filename":"src\/hotspot\/share\/gc\/parallel\/psPromotionManager.cpp","additions":1,"deletions":7,"binary":false,"changes":8,"status":"modified"},{"patch":"@@ -59,4 +59,7 @@\n-  assert(should_scavenge(p, true), \"revisiting object?\");\n-  oop obj = RawAccess<IS_NOT_NULL>::oop_load(p);\n-  Prefetch::write(obj->mark_addr(), 0);\n-  push_depth(ScannerTask(p));\n+  T heap_oop = RawAccess<>::oop_load(p);\n+  if (PSScavenge::is_obj_in_young(heap_oop)) {\n+    oop obj = CompressedOops::decode_not_null(heap_oop);\n+    assert(!PSScavenge::is_obj_in_to_space(obj), \"revisiting object?\");\n+    Prefetch::write(obj->mark_addr(), 0);\n+    push_depth(ScannerTask(p));\n+  }\n@@ -98,4 +101,2 @@\n-  template <typename T> void do_oop_nv(T* p) {\n-    if (PSScavenge::should_scavenge(p)) {\n-      _pm->claim_or_forward_depth(p);\n-    }\n+  template <typename T> void do_oop_work(T* p) {\n+    _pm->claim_or_forward_depth(p);\n@@ -104,2 +105,2 @@\n-  virtual void do_oop(oop* p)       { do_oop_nv(p); }\n-  virtual void do_oop(narrowOop* p) { do_oop_nv(p); }\n+  virtual void do_oop(oop* p)       { do_oop_work(p); }\n+  virtual void do_oop(narrowOop* p) { do_oop_work(p); }\n","filename":"src\/hotspot\/share\/gc\/parallel\/psPromotionManager.inline.hpp","additions":11,"deletions":10,"binary":false,"changes":21,"status":"modified"},{"patch":"@@ -1107,5 +1107,0 @@\n-\/\/ Moved from inline file as they are not called inline\n-ContiguousSpace* DefNewGeneration::first_compaction_space() const {\n-  return eden();\n-}\n-\n","filename":"src\/hotspot\/share\/gc\/serial\/defNewGeneration.cpp","additions":0,"deletions":5,"binary":false,"changes":5,"status":"modified"},{"patch":"@@ -39,0 +39,1 @@\n+#include \"gc\/serial\/markSweep.inline.hpp\"\n@@ -51,2 +52,2 @@\n-#include \"gc\/shared\/slidingForwarding.hpp\"\n-#include \"gc\/shared\/space.hpp\"\n+#include \"gc\/shared\/slidingForwarding.inline.hpp\"\n+#include \"gc\/shared\/space.inline.hpp\"\n@@ -61,0 +62,1 @@\n+#include \"runtime\/prefetch.inline.hpp\"\n@@ -70,2 +72,4 @@\n-void GenMarkSweep::invoke_at_safepoint(bool clear_all_softrefs) {\n-  assert(SafepointSynchronize::is_at_safepoint(), \"must be at a safepoint\");\n+class DeadSpacer : StackObj {\n+  size_t _allowed_deadspace_words;\n+  bool _active;\n+  ContiguousSpace* _space;\n@@ -73,4 +77,16 @@\n-  SerialHeap* gch = SerialHeap::heap();\n-#ifdef ASSERT\n-  if (gch->soft_ref_policy()->should_clear_all_soft_refs()) {\n-    assert(clear_all_softrefs, \"Policy should have been checked earlier\");\n+public:\n+  DeadSpacer(ContiguousSpace* space) : _allowed_deadspace_words(0), _space(space) {\n+    size_t ratio = _space->allowed_dead_ratio();\n+    _active = ratio > 0;\n+\n+    if (_active) {\n+      \/\/ We allow some amount of garbage towards the bottom of the space, so\n+      \/\/ we don't start compacting before there is a significant gain to be made.\n+      \/\/ Occasionally, we want to ensure a full compaction, which is determined\n+      \/\/ by the MarkSweepAlwaysCompactCount parameter.\n+      if ((MarkSweep::total_invocations() % MarkSweepAlwaysCompactCount) != 0) {\n+        _allowed_deadspace_words = (space->capacity() * ratio \/ 100) \/ HeapWordSize;\n+      } else {\n+        _active = false;\n+      }\n+    }\n@@ -78,5 +94,4 @@\n-#endif\n-  gch->trace_heap_before_gc(_gc_tracer);\n-\n-  \/\/ Increment the invocation count\n-  _total_invocations++;\n+  bool insert_deadspace(HeapWord* dead_start, HeapWord* dead_end) {\n+    if (!_active) {\n+      return false;\n+    }\n@@ -85,4 +100,18 @@\n-  \/\/ Capture used regions for each generation that will be\n-  \/\/ subject to collection, so that card table adjustments can\n-  \/\/ be made intelligently (see clear \/ invalidate further below).\n-  gch->save_used_regions();\n+    size_t dead_length = pointer_delta(dead_end, dead_start);\n+    if (_allowed_deadspace_words >= dead_length) {\n+      _allowed_deadspace_words -= dead_length;\n+      CollectedHeap::fill_with_object(dead_start, dead_length);\n+      oop obj = cast_to_oop(dead_start);\n+      \/\/ obj->set_mark(obj->mark().set_marked());\n+\n+      assert(dead_length == obj->size(), \"bad filler object size\");\n+      log_develop_trace(gc, compaction)(\"Inserting object to dead space: \" PTR_FORMAT \", \" PTR_FORMAT \", \" SIZE_FORMAT \"b\",\n+                                        p2i(dead_start), p2i(dead_end), dead_length * HeapWordSize);\n+\n+      return true;\n+    } else {\n+      _active = false;\n+      return false;\n+    }\n+  }\n+};\n@@ -90,1 +119,20 @@\n-  allocate_stacks();\n+\/\/ Implement the \"compaction\" part of the mark-compact GC algorithm.\n+class Compacter {\n+  \/\/ There are four spaces in total, but only the first three can be used after\n+  \/\/ compact. IOW, old and eden\/from must be enough for all live objs\n+  static constexpr uint max_num_spaces = 4;\n+\n+  struct CompactionSpace {\n+    ContiguousSpace* _space;\n+    \/\/ Will be the new top after compaction is complete.\n+    HeapWord* _compaction_top;\n+    \/\/ The first dead word in this contiguous space. It's an optimization to\n+    \/\/ skip large chunk of live objects at the beginning.\n+    HeapWord* _first_dead;\n+\n+    void init(ContiguousSpace* space) {\n+      _space = space;\n+      _compaction_top = space->bottom();\n+      _first_dead = nullptr;\n+    }\n+  };\n@@ -92,1 +140,3 @@\n-  mark_sweep_phase1(clear_all_softrefs);\n+  CompactionSpace _spaces[max_num_spaces];\n+  \/\/ The num of spaces to be compacted, i.e. containing live objs.\n+  uint _num_spaces;\n@@ -94,1 +144,1 @@\n-  SlidingForwarding::begin();\n+  uint _index;\n@@ -96,1 +146,3 @@\n-  mark_sweep_phase2();\n+  HeapWord* get_compaction_top(uint index) const {\n+    return _spaces[index]._compaction_top;\n+  }\n@@ -98,5 +150,3 @@\n-  \/\/ Don't add any more derived pointers during phase3\n-#if COMPILER2_OR_JVMCI\n-  assert(DerivedPointerTable::is_active(), \"Sanity\");\n-  DerivedPointerTable::set_active(false);\n-#endif\n+  HeapWord* get_first_dead(uint index) const {\n+    return _spaces[index]._first_dead;\n+  }\n@@ -104,1 +154,3 @@\n-  mark_sweep_phase3();\n+  ContiguousSpace* get_space(uint index) const {\n+    return _spaces[index]._space;\n+  }\n@@ -106,1 +158,4 @@\n-  mark_sweep_phase4();\n+  void record_first_dead(uint index, HeapWord* first_dead) {\n+    assert(_spaces[index]._first_dead == nullptr, \"should write only once\");\n+    _spaces[index]._first_dead = first_dead;\n+  }\n@@ -108,1 +163,18 @@\n-  SlidingForwarding::end();\n+  HeapWord* alloc(size_t words) {\n+    while (true) {\n+      if (words <= pointer_delta(_spaces[_index]._space->end(),\n+                                 _spaces[_index]._compaction_top)) {\n+        HeapWord* result = _spaces[_index]._compaction_top;\n+        _spaces[_index]._compaction_top += words;\n+        if (_index == 0) {\n+          \/\/ old-gen requires BOT update\n+          static_cast<TenuredSpace*>(_spaces[0]._space)->update_for_block(result, result + words);\n+        }\n+        return result;\n+      }\n+\n+      \/\/ out-of-memory in this space\n+      _index++;\n+      assert(_index < max_num_spaces - 1, \"the last space should not be used\");\n+    }\n+  }\n@@ -110,1 +182,5 @@\n-  restore_marks();\n+  static void prefetch_read_scan(void* p) {\n+    if (PrefetchScanIntervalInBytes >= 0) {\n+      Prefetch::read(p, PrefetchScanIntervalInBytes);\n+    }\n+  }\n@@ -112,3 +188,5 @@\n-  \/\/ Set saved marks for allocation profiler (and other things? -- dld)\n-  \/\/ (Should this be in general part?)\n-  gch->save_marks();\n+  static void prefetch_write_scan(void* p) {\n+    if (PrefetchScanIntervalInBytes >= 0) {\n+      Prefetch::write(p, PrefetchScanIntervalInBytes);\n+    }\n+  }\n@@ -116,1 +194,5 @@\n-  deallocate_stacks();\n+  static void prefetch_write_copy(void* p) {\n+    if (PrefetchCopyIntervalInBytes >= 0) {\n+      Prefetch::write(p, PrefetchCopyIntervalInBytes);\n+    }\n+  }\n@@ -118,1 +200,11 @@\n-  MarkSweep::_string_dedup_requests->flush();\n+  template <bool ALT_FWD>\n+  static void forward_obj(oop obj, HeapWord* new_addr) {\n+    prefetch_write_scan(obj);\n+    if (cast_from_oop<HeapWord*>(obj) != new_addr) {\n+      SlidingForwarding::forward_to<ALT_FWD>(obj, cast_to_oop(new_addr));\n+    } else {\n+      assert(obj->is_gc_marked(), \"inv\");\n+      \/\/ This obj will stay in-place. Fix the markword.\n+      obj->init_mark();\n+    }\n+  }\n@@ -120,2 +212,11 @@\n-  bool is_young_gen_empty = (gch->young_gen()->used() == 0);\n-  gch->rem_set()->maintain_old_to_young_invariant(gch->old_gen(), is_young_gen_empty);\n+  static HeapWord* find_next_live_addr(HeapWord* start, HeapWord* end) {\n+    for (HeapWord* i_addr = start; i_addr < end; \/* empty *\/) {\n+      prefetch_read_scan(i_addr);\n+      oop obj = cast_to_oop(i_addr);\n+      if (obj->is_gc_marked()) {\n+        return i_addr;\n+      }\n+      i_addr += obj->size();\n+    }\n+    return end;\n+  };\n@@ -123,1 +224,4 @@\n-  gch->prune_scavengable_nmethods();\n+  template <bool ALT_FWD>\n+  static size_t relocate(HeapWord* addr) {\n+    \/\/ Prefetch source and destination\n+    prefetch_read_scan(addr);\n@@ -125,3 +229,5 @@\n-  \/\/ Update heap occupancy information which is used as\n-  \/\/ input to soft ref clearing policy at the next gc.\n-  Universe::heap()->update_capacity_and_used_at_gc();\n+    oop obj = cast_to_oop(addr);\n+    oop new_obj = SlidingForwarding::forwardee<ALT_FWD>(obj);\n+    HeapWord* new_addr = cast_from_oop<HeapWord*>(new_obj);\n+    assert(addr != new_addr, \"inv\");\n+    prefetch_write_copy(new_addr);\n@@ -129,2 +235,3 @@\n-  \/\/ Signal that we have completed a visit to all live objects.\n-  Universe::heap()->record_whole_heap_examined_timestamp();\n+    size_t obj_size = obj->size();\n+    Copy::aligned_conjoint_words(addr, new_addr, obj_size);\n+    new_obj->init_mark();\n@@ -132,2 +239,2 @@\n-  gch->trace_heap_after_gc(_gc_tracer);\n-}\n+    return obj_size;\n+  }\n@@ -135,5 +242,16 @@\n-void GenMarkSweep::allocate_stacks() {\n-  void* scratch = nullptr;\n-  size_t num_words;\n-  DefNewGeneration* young_gen = (DefNewGeneration*)SerialHeap::heap()->young_gen();\n-  young_gen->contribute_scratch(scratch, num_words);\n+public:\n+  explicit Compacter(SerialHeap* heap) {\n+    \/\/ In this order so that heap is compacted towards old-gen.\n+    _spaces[0].init(heap->old_gen()->space());\n+    _spaces[1].init(heap->young_gen()->eden());\n+    _spaces[2].init(heap->young_gen()->from());\n+\n+    bool is_promotion_failed = (heap->young_gen()->from()->next_compaction_space() != nullptr);\n+    if (is_promotion_failed) {\n+      _spaces[3].init(heap->young_gen()->to());\n+      _num_spaces = 4;\n+    } else {\n+      _num_spaces = 3;\n+    }\n+    _index = 0;\n+  }\n@@ -141,4 +259,39 @@\n-  if (scratch != nullptr) {\n-    _preserved_count_max = num_words * HeapWordSize \/ sizeof(PreservedMark);\n-  } else {\n-    _preserved_count_max = 0;\n+  template <bool ALT_FWD>\n+  void phase2_calculate_new_addr() {\n+    for (uint i = 0; i < _num_spaces; ++i) {\n+      ContiguousSpace* space = get_space(i);\n+      HeapWord* cur_addr = space->bottom();\n+      HeapWord* top = space->top();\n+\n+      bool record_first_dead_done = false;\n+\n+      DeadSpacer dead_spacer(space);\n+\n+      while (cur_addr < top) {\n+        oop obj = cast_to_oop(cur_addr);\n+        size_t obj_size = obj->size();\n+        if (obj->is_gc_marked()) {\n+          HeapWord* new_addr = alloc(obj_size);\n+          forward_obj<ALT_FWD>(obj, new_addr);\n+          cur_addr += obj_size;\n+        } else {\n+          \/\/ Skipping the current known-unmarked obj\n+          HeapWord* next_live_addr = find_next_live_addr(cur_addr + obj_size, top);\n+          if (dead_spacer.insert_deadspace(cur_addr, next_live_addr)) {\n+            \/\/ Register space for the filler obj\n+            alloc(pointer_delta(next_live_addr, cur_addr));\n+          } else {\n+            if (!record_first_dead_done) {\n+              record_first_dead(i, cur_addr);\n+              record_first_dead_done = true;\n+            }\n+            *(HeapWord**)cur_addr = next_live_addr;\n+          }\n+          cur_addr = next_live_addr;\n+        }\n+      }\n+\n+      if (!record_first_dead_done) {\n+        record_first_dead(i, top);\n+      }\n+    }\n@@ -147,2 +300,7 @@\n-  _preserved_marks = (PreservedMark*)scratch;\n-  _preserved_count = 0;\n+  void phase2_calculate_new_addr() {\n+    if (UseAltGCForwarding) {\n+      phase2_calculate_new_addr<true>();\n+    } else {\n+      phase2_calculate_new_addr<false>();\n+    }\n+  }\n@@ -150,2 +308,20 @@\n-  _preserved_overflow_stack_set.init(1);\n-}\n+  template <bool ALT_FWD>\n+  void phase3_adjust_pointers() {\n+    for (uint i = 0; i < _num_spaces; ++i) {\n+      ContiguousSpace* space = get_space(i);\n+      HeapWord* cur_addr = space->bottom();\n+      HeapWord* const top = space->top();\n+      HeapWord* const first_dead = get_first_dead(i);\n+\n+      while (cur_addr < top) {\n+        prefetch_write_scan(cur_addr);\n+        if (cur_addr < first_dead || cast_to_oop(cur_addr)->is_gc_marked()) {\n+          size_t size = MarkSweep::adjust_pointers<ALT_FWD>(cast_to_oop(cur_addr));\n+          cur_addr += size;\n+        } else {\n+          assert(*(HeapWord**)cur_addr > cur_addr, \"forward progress\");\n+          cur_addr = *(HeapWord**)cur_addr;\n+        }\n+      }\n+    }\n+  }\n@@ -153,0 +329,7 @@\n+  void phase3_adjust_pointers() {\n+    if (UseAltGCForwarding) {\n+      phase3_adjust_pointers<true>();\n+    } else {\n+      phase3_adjust_pointers<false>();\n+    }\n+  }\n@@ -154,4 +337,27 @@\n-void GenMarkSweep::deallocate_stacks() {\n-  if (_preserved_count_max != 0) {\n-    DefNewGeneration* young_gen = (DefNewGeneration*)SerialHeap::heap()->young_gen();\n-    young_gen->reset_scratch();\n+  template <bool ALT_FWD>\n+  void phase4_compact() {\n+    for (uint i = 0; i < _num_spaces; ++i) {\n+      ContiguousSpace* space = get_space(i);\n+      HeapWord* cur_addr = space->bottom();\n+      HeapWord* top = space->top();\n+\n+      \/\/ Check if the first obj inside this space is forwarded.\n+      if (SlidingForwarding::is_not_forwarded(cast_to_oop(cur_addr))) {\n+        \/\/ Jump over consecutive (in-place) live-objs-chunk\n+        cur_addr = get_first_dead(i);\n+      }\n+\n+      while (cur_addr < top) {\n+        if (SlidingForwarding::is_not_forwarded(cast_to_oop(cur_addr))) {\n+          cur_addr = *(HeapWord**) cur_addr;\n+          continue;\n+        }\n+        cur_addr += relocate<ALT_FWD>(cur_addr);\n+      }\n+\n+      \/\/ Reset top and unused memory\n+      space->set_top(get_compaction_top(i));\n+      if (ZapUnusedHeapArea) {\n+        space->mangle_unused_area();\n+      }\n+    }\n@@ -160,4 +366,8 @@\n-  _preserved_overflow_stack_set.reclaim();\n-  _marking_stack.clear();\n-  _objarray_stack.clear(true);\n-}\n+  void phase4_compact() {\n+    if (UseAltGCForwarding) {\n+      phase4_compact<true>();\n+    } else {\n+      phase4_compact<false>();\n+    }\n+  }\n+};\n@@ -165,1 +375,1 @@\n-void GenMarkSweep::mark_sweep_phase1(bool clear_all_softrefs) {\n+void GenMarkSweep::phase1_mark(bool clear_all_softrefs) {\n@@ -227,0 +437,4 @@\n+    {\n+      GCTraceTime(Debug, gc, phases) ur(\"Unregister NMethods\", gc_timer());\n+      gch->prune_unlinked_nmethods();\n+    }\n@@ -245,0 +459,11 @@\n+void GenMarkSweep::invoke_at_safepoint(bool clear_all_softrefs) {\n+  assert(SafepointSynchronize::is_at_safepoint(), \"must be at a safepoint\");\n+\n+  SerialHeap* gch = SerialHeap::heap();\n+#ifdef ASSERT\n+  if (gch->soft_ref_policy()->should_clear_all_soft_refs()) {\n+    assert(clear_all_softrefs, \"Policy should have been checked earlier\");\n+  }\n+#endif\n+\n+  gch->trace_heap_before_gc(_gc_tracer);\n@@ -246,3 +471,2 @@\n-void GenMarkSweep::mark_sweep_phase2() {\n-  \/\/ Now all live objects are marked, compute the new object addresses.\n-  GCTraceTime(Info, gc, phases) tm(\"Phase 2: Compute new object addresses\", _gc_timer);\n+  \/\/ Increment the invocation count\n+  _total_invocations++;\n@@ -250,2 +474,4 @@\n-  SerialHeap::heap()->prepare_for_compaction();\n-}\n+  \/\/ Capture used regions for each generation that will be\n+  \/\/ subject to collection, so that card table adjustments can\n+  \/\/ be made intelligently (see clear \/ invalidate further below).\n+  gch->save_used_regions();\n@@ -253,4 +479,13 @@\n-class GenAdjustPointersClosure: public SerialHeap::GenClosure {\n-public:\n-  void do_generation(Generation* gen) {\n-    gen->adjust_pointers();\n+  allocate_stacks();\n+\n+  phase1_mark(clear_all_softrefs);\n+\n+  SlidingForwarding::begin();\n+\n+  Compacter compacter{gch};\n+\n+  {\n+    \/\/ Now all live objects are marked, compute the new object addresses.\n+    GCTraceTime(Info, gc, phases) tm(\"Phase 2: Compute new object addresses\", _gc_timer);\n+\n+    compacter.phase2_calculate_new_addr();\n@@ -258,3 +493,5 @@\n-};\n-void GenMarkSweep::mark_sweep_phase3() {\n-  SerialHeap* gch = SerialHeap::heap();\n+  \/\/ Don't add any more derived pointers during phase3\n+#if COMPILER2_OR_JVMCI\n+  assert(DerivedPointerTable::is_active(), \"Sanity\");\n+  DerivedPointerTable::set_active(false);\n+#endif\n@@ -263,2 +500,29 @@\n-  \/\/ Adjust the pointers to reflect the new locations\n-  GCTraceTime(Info, gc, phases) tm(\"Phase 3: Adjust pointers\", gc_timer());\n+  {\n+    \/\/ Adjust the pointers to reflect the new locations\n+    GCTraceTime(Info, gc, phases) tm(\"Phase 3: Adjust pointers\", gc_timer());\n+\n+    ClassLoaderDataGraph::verify_claimed_marks_cleared(ClassLoaderData::_claim_stw_fullgc_adjust);\n+\n+    if (UseAltGCForwarding) {\n+      AdjustPointerClosure<true> adjust_pointer_closure;\n+      CLDToOopClosure adjust_cld_closure(&adjust_pointer_closure, ClassLoaderData::_claim_stw_fullgc_adjust);\n+      CodeBlobToOopClosure code_closure(&adjust_pointer_closure, CodeBlobToOopClosure::FixRelocations);\n+      gch->process_roots(SerialHeap::SO_AllCodeCache,\n+                         &adjust_pointer_closure,\n+                         &adjust_cld_closure,\n+                         &adjust_cld_closure,\n+                         &code_closure);\n+\n+      WeakProcessor::oops_do(&adjust_pointer_closure);\n+    } else {\n+      AdjustPointerClosure<false> adjust_pointer_closure;\n+      CLDToOopClosure adjust_cld_closure(&adjust_pointer_closure, ClassLoaderData::_claim_stw_fullgc_adjust);\n+      CodeBlobToOopClosure code_closure(&adjust_pointer_closure, CodeBlobToOopClosure::FixRelocations);\n+      gch->process_roots(SerialHeap::SO_AllCodeCache,\n+                         &adjust_pointer_closure,\n+                         &adjust_cld_closure,\n+                         &adjust_cld_closure,\n+                         &code_closure);\n+\n+      WeakProcessor::oops_do(&adjust_pointer_closure);\n+    }\n@@ -266,1 +530,3 @@\n-  ClassLoaderDataGraph::verify_claimed_marks_cleared(ClassLoaderData::_claim_stw_fullgc_adjust);\n+    adjust_marks();\n+    compacter.phase3_adjust_pointers();\n+  }\n@@ -268,9 +534,3 @@\n-  if (UseAltGCForwarding) {\n-    AdjustPointerClosure<true> adjust_pointer_closure;\n-    CLDToOopClosure adjust_cld_closure(&adjust_pointer_closure, ClassLoaderData::_claim_stw_fullgc_adjust);\n-    CodeBlobToOopClosure code_closure(&adjust_pointer_closure, CodeBlobToOopClosure::FixRelocations);\n-    gch->process_roots(SerialHeap::SO_AllCodeCache,\n-                       &adjust_pointer_closure,\n-                       &adjust_cld_closure,\n-                       &adjust_cld_closure,\n-                       &code_closure);\n+  {\n+    \/\/ All pointers are now adjusted, move objects accordingly\n+    GCTraceTime(Info, gc, phases) tm(\"Phase 4: Move objects\", _gc_timer);\n@@ -278,12 +538,1 @@\n-    gch->gen_process_weak_roots(&adjust_pointer_closure);\n-  } else {\n-    AdjustPointerClosure<false> adjust_pointer_closure;\n-    CLDToOopClosure adjust_cld_closure(&adjust_pointer_closure, ClassLoaderData::_claim_stw_fullgc_adjust);\n-    CodeBlobToOopClosure code_closure(&adjust_pointer_closure, CodeBlobToOopClosure::FixRelocations);\n-    gch->process_roots(SerialHeap::SO_AllCodeCache,\n-                       &adjust_pointer_closure,\n-                       &adjust_cld_closure,\n-                       &adjust_cld_closure,\n-                       &code_closure);\n-\n-    gch->gen_process_weak_roots(&adjust_pointer_closure);\n+    compacter.phase4_compact();\n@@ -291,3 +540,26 @@\n-  adjust_marks();\n-  GenAdjustPointersClosure blk;\n-  gch->generation_iterate(&blk, true);\n+\n+  SlidingForwarding::end();\n+\n+  restore_marks();\n+\n+  \/\/ Set saved marks for allocation profiler (and other things? -- dld)\n+  \/\/ (Should this be in general part?)\n+  gch->save_marks();\n+\n+  deallocate_stacks();\n+\n+  MarkSweep::_string_dedup_requests->flush();\n+\n+  bool is_young_gen_empty = (gch->young_gen()->used() == 0);\n+  gch->rem_set()->maintain_old_to_young_invariant(gch->old_gen(), is_young_gen_empty);\n+\n+  gch->prune_scavengable_nmethods();\n+\n+  \/\/ Update heap occupancy information which is used as\n+  \/\/ input to soft ref clearing policy at the next gc.\n+  Universe::heap()->update_capacity_and_used_at_gc();\n+\n+  \/\/ Signal that we have completed a visit to all live objects.\n+  Universe::heap()->record_whole_heap_examined_timestamp();\n+\n+  gch->trace_heap_after_gc(_gc_tracer);\n@@ -296,4 +568,10 @@\n-class GenCompactClosure: public SerialHeap::GenClosure {\n-public:\n-  void do_generation(Generation* gen) {\n-    gen->compact();\n+void GenMarkSweep::allocate_stacks() {\n+  void* scratch = nullptr;\n+  size_t num_words;\n+  DefNewGeneration* young_gen = (DefNewGeneration*)SerialHeap::heap()->young_gen();\n+  young_gen->contribute_scratch(scratch, num_words);\n+\n+  if (scratch != nullptr) {\n+    _preserved_count_max = num_words * HeapWordSize \/ sizeof(PreservedMark);\n+  } else {\n+    _preserved_count_max = 0;\n@@ -301,4 +579,2 @@\n-};\n-void GenMarkSweep::mark_sweep_phase4() {\n-  \/\/ All pointers are now adjusted, move objects accordingly\n-  GCTraceTime(Info, gc, phases) tm(\"Phase 4: Move objects\", _gc_timer);\n+  _preserved_marks = (PreservedMark*)scratch;\n+  _preserved_count = 0;\n@@ -307,2 +583,12 @@\n-  GenCompactClosure blk;\n-  SerialHeap::heap()->generation_iterate(&blk, true);\n+  _preserved_overflow_stack_set.init(1);\n+}\n+\n+void GenMarkSweep::deallocate_stacks() {\n+  if (_preserved_count_max != 0) {\n+    DefNewGeneration* young_gen = (DefNewGeneration*)SerialHeap::heap()->young_gen();\n+    young_gen->reset_scratch();\n+  }\n+\n+  _preserved_overflow_stack_set.reclaim();\n+  _marking_stack.clear();\n+  _objarray_stack.clear(true);\n","filename":"src\/hotspot\/share\/gc\/serial\/genMarkSweep.cpp","additions":402,"deletions":116,"binary":false,"changes":518,"status":"modified"},{"patch":"@@ -2,1 +2,1 @@\n- * Copyright (c) 2017, 2023, Oracle and\/or its affiliates. All rights reserved.\n+ * Copyright (c) 2017, 2024, Oracle and\/or its affiliates. All rights reserved.\n@@ -26,0 +26,8 @@\n+#include \"classfile\/classLoaderDataGraph.hpp\"\n+#include \"classfile\/stringTable.hpp\"\n+#include \"classfile\/symbolTable.hpp\"\n+#include \"classfile\/vmSymbols.hpp\"\n+#include \"code\/codeCache.hpp\"\n+#include \"code\/icBuffer.hpp\"\n+#include \"compiler\/oopMap.hpp\"\n+#include \"gc\/serial\/cardTableRS.hpp\"\n@@ -27,0 +35,2 @@\n+#include \"gc\/serial\/genMarkSweep.hpp\"\n+#include \"gc\/serial\/markSweep.hpp\"\n@@ -28,0 +38,2 @@\n+#include \"gc\/serial\/serialMemoryPools.hpp\"\n+#include \"gc\/serial\/serialVMOperations.hpp\"\n@@ -29,0 +41,7 @@\n+#include \"gc\/shared\/cardTableBarrierSet.hpp\"\n+#include \"gc\/shared\/classUnloadingContext.hpp\"\n+#include \"gc\/shared\/collectedHeap.inline.hpp\"\n+#include \"gc\/shared\/collectorCounters.hpp\"\n+#include \"gc\/shared\/continuationGCSupport.inline.hpp\"\n+#include \"gc\/shared\/gcId.hpp\"\n+#include \"gc\/shared\/gcInitLogger.hpp\"\n@@ -30,1 +49,12 @@\n-#include \"gc\/shared\/genMemoryPools.hpp\"\n+#include \"gc\/shared\/gcPolicyCounters.hpp\"\n+#include \"gc\/shared\/gcTrace.hpp\"\n+#include \"gc\/shared\/gcTraceTime.inline.hpp\"\n+#include \"gc\/shared\/gcVMOperations.hpp\"\n+#include \"gc\/shared\/genArguments.hpp\"\n+#include \"gc\/shared\/locationPrinter.inline.hpp\"\n+#include \"gc\/shared\/oopStorage.inline.hpp\"\n+#include \"gc\/shared\/oopStorageParState.inline.hpp\"\n+#include \"gc\/shared\/oopStorageSet.inline.hpp\"\n+#include \"gc\/shared\/scavengableNMethods.hpp\"\n+#include \"gc\/shared\/slidingForwarding.hpp\"\n+#include \"gc\/shared\/space.hpp\"\n@@ -33,0 +63,6 @@\n+#include \"gc\/shared\/weakProcessor.hpp\"\n+#include \"gc\/shared\/workerThread.hpp\"\n+#include \"memory\/iterator.hpp\"\n+#include \"memory\/metaspaceCounters.hpp\"\n+#include \"memory\/metaspaceUtils.hpp\"\n+#include \"memory\/resourceArea.hpp\"\n@@ -34,0 +70,4 @@\n+#include \"oops\/oop.inline.hpp\"\n+#include \"runtime\/handles.hpp\"\n+#include \"runtime\/handles.inline.hpp\"\n+#include \"runtime\/java.hpp\"\n@@ -35,0 +75,2 @@\n+#include \"runtime\/threads.hpp\"\n+#include \"runtime\/vmThread.hpp\"\n@@ -36,0 +78,10 @@\n+#include \"services\/memoryService.hpp\"\n+#include \"utilities\/autoRestore.hpp\"\n+#include \"utilities\/debug.hpp\"\n+#include \"utilities\/formatBuffer.hpp\"\n+#include \"utilities\/macros.hpp\"\n+#include \"utilities\/stack.inline.hpp\"\n+#include \"utilities\/vmError.hpp\"\n+#if INCLUDE_JVMCI\n+#include \"jvmci\/jvmci.hpp\"\n+#endif\n@@ -42,3 +94,9 @@\n-    GenCollectedHeap(Generation::DefNew,\n-                     Generation::MarkSweepCompact,\n-                     \"Copy:MSC\"),\n+    CollectedHeap(),\n+    _young_gen(nullptr),\n+    _old_gen(nullptr),\n+    _rem_set(nullptr),\n+    _soft_ref_policy(),\n+    _gc_policy_counters(new GCPolicyCounters(\"Copy:MSC\", 2, 2)),\n+    _incremental_collection_failed(false),\n+    _young_manager(nullptr),\n+    _old_manager(nullptr),\n@@ -53,1 +111,0 @@\n-\n@@ -77,1 +134,0 @@\n-\n@@ -135,0 +191,927 @@\n+\n+jint SerialHeap::initialize() {\n+  \/\/ Allocate space for the heap.\n+\n+  ReservedHeapSpace heap_rs = allocate(HeapAlignment);\n+\n+  if (!heap_rs.is_reserved()) {\n+    vm_shutdown_during_initialization(\n+      \"Could not reserve enough space for object heap\");\n+    return JNI_ENOMEM;\n+  }\n+\n+  initialize_reserved_region(heap_rs);\n+\n+  ReservedSpace young_rs = heap_rs.first_part(MaxNewSize);\n+  ReservedSpace old_rs = heap_rs.last_part(MaxNewSize);\n+\n+  _rem_set = create_rem_set(heap_rs.region());\n+  _rem_set->initialize(young_rs.base(), old_rs.base());\n+\n+  CardTableBarrierSet *bs = new CardTableBarrierSet(_rem_set);\n+  bs->initialize();\n+  BarrierSet::set_barrier_set(bs);\n+\n+  _young_gen = new DefNewGeneration(young_rs, NewSize, MinNewSize, MaxNewSize);\n+  _old_gen = new TenuredGeneration(old_rs, OldSize, MinOldSize, MaxOldSize, rem_set());\n+\n+  GCInitLogger::print();\n+\n+  SlidingForwarding::initialize(_reserved, SpaceAlignment \/ HeapWordSize);\n+\n+  return JNI_OK;\n+}\n+\n+\n+CardTableRS* SerialHeap::create_rem_set(const MemRegion& reserved_region) {\n+  return new CardTableRS(reserved_region);\n+}\n+\n+ReservedHeapSpace SerialHeap::allocate(size_t alignment) {\n+  \/\/ Now figure out the total size.\n+  const size_t pageSize = UseLargePages ? os::large_page_size() : os::vm_page_size();\n+  assert(alignment % pageSize == 0, \"Must be\");\n+\n+  \/\/ Check for overflow.\n+  size_t total_reserved = MaxNewSize + MaxOldSize;\n+  if (total_reserved < MaxNewSize) {\n+    vm_exit_during_initialization(\"The size of the object heap + VM data exceeds \"\n+                                  \"the maximum representable size\");\n+  }\n+  assert(total_reserved % alignment == 0,\n+         \"Gen size; total_reserved=\" SIZE_FORMAT \", alignment=\"\n+         SIZE_FORMAT, total_reserved, alignment);\n+\n+  ReservedHeapSpace heap_rs = Universe::reserve_heap(total_reserved, alignment);\n+  size_t used_page_size = heap_rs.page_size();\n+\n+  os::trace_page_sizes(\"Heap\",\n+                       MinHeapSize,\n+                       total_reserved,\n+                       heap_rs.base(),\n+                       heap_rs.size(),\n+                       used_page_size);\n+\n+  return heap_rs;\n+}\n+\n+class GenIsScavengable : public BoolObjectClosure {\n+public:\n+  bool do_object_b(oop obj) {\n+    return SerialHeap::heap()->is_in_young(obj);\n+  }\n+};\n+\n+static GenIsScavengable _is_scavengable;\n+\n+void SerialHeap::post_initialize() {\n+  CollectedHeap::post_initialize();\n+\n+  DefNewGeneration* def_new_gen = (DefNewGeneration*)_young_gen;\n+\n+  def_new_gen->ref_processor_init();\n+\n+  MarkSweep::initialize();\n+\n+  ScavengableNMethods::initialize(&_is_scavengable);\n+}\n+\n+PreGenGCValues SerialHeap::get_pre_gc_values() const {\n+  const DefNewGeneration* const def_new_gen = (DefNewGeneration*) young_gen();\n+\n+  return PreGenGCValues(def_new_gen->used(),\n+                        def_new_gen->capacity(),\n+                        def_new_gen->eden()->used(),\n+                        def_new_gen->eden()->capacity(),\n+                        def_new_gen->from()->used(),\n+                        def_new_gen->from()->capacity(),\n+                        old_gen()->used(),\n+                        old_gen()->capacity());\n+}\n+\n+size_t SerialHeap::capacity() const {\n+  return _young_gen->capacity() + _old_gen->capacity();\n+}\n+\n+size_t SerialHeap::used() const {\n+  return _young_gen->used() + _old_gen->used();\n+}\n+\n+void SerialHeap::save_used_regions() {\n+  _old_gen->save_used_region();\n+  _young_gen->save_used_region();\n+}\n+\n+size_t SerialHeap::max_capacity() const {\n+  return _young_gen->max_capacity() + _old_gen->max_capacity();\n+}\n+\n+\/\/ Return true if any of the following is true:\n+\/\/ . the allocation won't fit into the current young gen heap\n+\/\/ . gc locker is occupied (jni critical section)\n+\/\/ . heap memory is tight -- the most recent previous collection\n+\/\/   was a full collection because a partial collection (would\n+\/\/   have) failed and is likely to fail again\n+bool SerialHeap::should_try_older_generation_allocation(size_t word_size) const {\n+  size_t young_capacity = _young_gen->capacity_before_gc();\n+  return    (word_size > heap_word_size(young_capacity))\n+         || GCLocker::is_active_and_needs_gc()\n+         || incremental_collection_failed();\n+}\n+\n+HeapWord* SerialHeap::expand_heap_and_allocate(size_t size, bool is_tlab) {\n+  HeapWord* result = nullptr;\n+  if (_old_gen->should_allocate(size, is_tlab)) {\n+    result = _old_gen->expand_and_allocate(size, is_tlab);\n+  }\n+  if (result == nullptr) {\n+    if (_young_gen->should_allocate(size, is_tlab)) {\n+      result = _young_gen->expand_and_allocate(size, is_tlab);\n+    }\n+  }\n+  assert(result == nullptr || is_in_reserved(result), \"result not in heap\");\n+  return result;\n+}\n+\n+HeapWord* SerialHeap::mem_allocate_work(size_t size,\n+                                        bool is_tlab) {\n+\n+  HeapWord* result = nullptr;\n+\n+  \/\/ Loop until the allocation is satisfied, or unsatisfied after GC.\n+  for (uint try_count = 1, gclocker_stalled_count = 0; \/* return or throw *\/; try_count += 1) {\n+\n+    \/\/ First allocation attempt is lock-free.\n+    Generation *young = _young_gen;\n+    if (young->should_allocate(size, is_tlab)) {\n+      result = young->par_allocate(size, is_tlab);\n+      if (result != nullptr) {\n+        assert(is_in_reserved(result), \"result not in heap\");\n+        return result;\n+      }\n+    }\n+    uint gc_count_before;  \/\/ Read inside the Heap_lock locked region.\n+    {\n+      MutexLocker ml(Heap_lock);\n+      log_trace(gc, alloc)(\"SerialHeap::mem_allocate_work: attempting locked slow path allocation\");\n+      \/\/ Note that only large objects get a shot at being\n+      \/\/ allocated in later generations.\n+      bool first_only = !should_try_older_generation_allocation(size);\n+\n+      result = attempt_allocation(size, is_tlab, first_only);\n+      if (result != nullptr) {\n+        assert(is_in_reserved(result), \"result not in heap\");\n+        return result;\n+      }\n+\n+      if (GCLocker::is_active_and_needs_gc()) {\n+        if (is_tlab) {\n+          return nullptr;  \/\/ Caller will retry allocating individual object.\n+        }\n+        if (!is_maximal_no_gc()) {\n+          \/\/ Try and expand heap to satisfy request.\n+          result = expand_heap_and_allocate(size, is_tlab);\n+          \/\/ Result could be null if we are out of space.\n+          if (result != nullptr) {\n+            return result;\n+          }\n+        }\n+\n+        if (gclocker_stalled_count > GCLockerRetryAllocationCount) {\n+          return nullptr; \/\/ We didn't get to do a GC and we didn't get any memory.\n+        }\n+\n+        \/\/ If this thread is not in a jni critical section, we stall\n+        \/\/ the requestor until the critical section has cleared and\n+        \/\/ GC allowed. When the critical section clears, a GC is\n+        \/\/ initiated by the last thread exiting the critical section; so\n+        \/\/ we retry the allocation sequence from the beginning of the loop,\n+        \/\/ rather than causing more, now probably unnecessary, GC attempts.\n+        JavaThread* jthr = JavaThread::current();\n+        if (!jthr->in_critical()) {\n+          MutexUnlocker mul(Heap_lock);\n+          \/\/ Wait for JNI critical section to be exited\n+          GCLocker::stall_until_clear();\n+          gclocker_stalled_count += 1;\n+          continue;\n+        } else {\n+          if (CheckJNICalls) {\n+            fatal(\"Possible deadlock due to allocating while\"\n+                  \" in jni critical section\");\n+          }\n+          return nullptr;\n+        }\n+      }\n+\n+      \/\/ Read the gc count while the heap lock is held.\n+      gc_count_before = total_collections();\n+    }\n+\n+    VM_GenCollectForAllocation op(size, is_tlab, gc_count_before);\n+    VMThread::execute(&op);\n+    if (op.prologue_succeeded()) {\n+      result = op.result();\n+      if (op.gc_locked()) {\n+         assert(result == nullptr, \"must be null if gc_locked() is true\");\n+         continue;  \/\/ Retry and\/or stall as necessary.\n+      }\n+\n+      assert(result == nullptr || is_in_reserved(result),\n+             \"result not in heap\");\n+      return result;\n+    }\n+\n+    \/\/ Give a warning if we seem to be looping forever.\n+    if ((QueuedAllocationWarningCount > 0) &&\n+        (try_count % QueuedAllocationWarningCount == 0)) {\n+          log_warning(gc, ergo)(\"SerialHeap::mem_allocate_work retries %d times,\"\n+                                \" size=\" SIZE_FORMAT \" %s\", try_count, size, is_tlab ? \"(TLAB)\" : \"\");\n+    }\n+  }\n+}\n+\n+HeapWord* SerialHeap::attempt_allocation(size_t size,\n+                                         bool is_tlab,\n+                                         bool first_only) {\n+  HeapWord* res = nullptr;\n+\n+  if (_young_gen->should_allocate(size, is_tlab)) {\n+    res = _young_gen->allocate(size, is_tlab);\n+    if (res != nullptr || first_only) {\n+      return res;\n+    }\n+  }\n+\n+  if (_old_gen->should_allocate(size, is_tlab)) {\n+    res = _old_gen->allocate(size, is_tlab);\n+  }\n+\n+  return res;\n+}\n+\n+HeapWord* SerialHeap::mem_allocate(size_t size,\n+                                   bool* gc_overhead_limit_was_exceeded) {\n+  return mem_allocate_work(size,\n+                           false \/* is_tlab *\/);\n+}\n+\n+bool SerialHeap::must_clear_all_soft_refs() {\n+  return _gc_cause == GCCause::_metadata_GC_clear_soft_refs ||\n+         _gc_cause == GCCause::_wb_full_gc;\n+}\n+\n+void SerialHeap::collect_generation(Generation* gen, bool full, size_t size,\n+                                    bool is_tlab, bool run_verification, bool clear_soft_refs) {\n+  FormatBuffer<> title(\"Collect gen: %s\", gen->short_name());\n+  GCTraceTime(Trace, gc, phases) t1(title);\n+  TraceCollectorStats tcs(gen->counters());\n+  TraceMemoryManagerStats tmms(gen->gc_manager(), gc_cause(), heap()->is_young_gen(gen) ? \"end of minor GC\" : \"end of major GC\");\n+\n+  gen->stat_record()->invocations++;\n+  gen->stat_record()->accumulated_time.start();\n+\n+  \/\/ Must be done anew before each collection because\n+  \/\/ a previous collection will do mangling and will\n+  \/\/ change top of some spaces.\n+  record_gen_tops_before_GC();\n+\n+  log_trace(gc)(\"%s invoke=%d size=\" SIZE_FORMAT, heap()->is_young_gen(gen) ? \"Young\" : \"Old\", gen->stat_record()->invocations, size * HeapWordSize);\n+\n+  if (run_verification && VerifyBeforeGC) {\n+    Universe::verify(\"Before GC\");\n+  }\n+  COMPILER2_OR_JVMCI_PRESENT(DerivedPointerTable::clear());\n+\n+  \/\/ Do collection work\n+  {\n+    save_marks();   \/\/ save marks for all gens\n+\n+    gen->collect(full, clear_soft_refs, size, is_tlab);\n+  }\n+\n+  COMPILER2_OR_JVMCI_PRESENT(DerivedPointerTable::update_pointers());\n+\n+  gen->stat_record()->accumulated_time.stop();\n+\n+  update_gc_stats(gen, full);\n+\n+  if (run_verification && VerifyAfterGC) {\n+    Universe::verify(\"After GC\");\n+  }\n+}\n+\n+void SerialHeap::do_collection(bool full,\n+                               bool clear_all_soft_refs,\n+                               size_t size,\n+                               bool is_tlab,\n+                               GenerationType max_generation) {\n+  ResourceMark rm;\n+  DEBUG_ONLY(Thread* my_thread = Thread::current();)\n+\n+  assert(SafepointSynchronize::is_at_safepoint(), \"should be at safepoint\");\n+  assert(my_thread->is_VM_thread(), \"only VM thread\");\n+  assert(Heap_lock->is_locked(),\n+         \"the requesting thread should have the Heap_lock\");\n+  guarantee(!is_gc_active(), \"collection is not reentrant\");\n+\n+  if (GCLocker::check_active_before_gc()) {\n+    return; \/\/ GC is disabled (e.g. JNI GetXXXCritical operation)\n+  }\n+\n+  const bool do_clear_all_soft_refs = clear_all_soft_refs ||\n+                          soft_ref_policy()->should_clear_all_soft_refs();\n+\n+  ClearedAllSoftRefs casr(do_clear_all_soft_refs, soft_ref_policy());\n+\n+  AutoModifyRestore<bool> temporarily(_is_gc_active, true);\n+\n+  bool complete = full && (max_generation == OldGen);\n+  bool old_collects_young = complete && !ScavengeBeforeFullGC;\n+  bool do_young_collection = !old_collects_young && _young_gen->should_collect(full, size, is_tlab);\n+\n+  const PreGenGCValues pre_gc_values = get_pre_gc_values();\n+\n+  bool run_verification = total_collections() >= VerifyGCStartAt;\n+  bool prepared_for_verification = false;\n+  bool do_full_collection = false;\n+\n+  if (do_young_collection) {\n+    GCIdMark gc_id_mark;\n+    GCTraceCPUTime tcpu(((DefNewGeneration*)_young_gen)->gc_tracer());\n+    GCTraceTime(Info, gc) t(\"Pause Young\", nullptr, gc_cause(), true);\n+\n+    print_heap_before_gc();\n+\n+    if (run_verification && VerifyGCLevel <= 0 && VerifyBeforeGC) {\n+      prepare_for_verify();\n+      prepared_for_verification = true;\n+    }\n+\n+    gc_prologue(complete);\n+    increment_total_collections(complete);\n+\n+    collect_generation(_young_gen,\n+                       full,\n+                       size,\n+                       is_tlab,\n+                       run_verification && VerifyGCLevel <= 0,\n+                       do_clear_all_soft_refs);\n+\n+    if (size > 0 && (!is_tlab || _young_gen->supports_tlab_allocation()) &&\n+        size * HeapWordSize <= _young_gen->unsafe_max_alloc_nogc()) {\n+      \/\/ Allocation request was met by young GC.\n+      size = 0;\n+    }\n+\n+    \/\/ Ask if young collection is enough. If so, do the final steps for young collection,\n+    \/\/ and fallthrough to the end.\n+    do_full_collection = should_do_full_collection(size, full, is_tlab, max_generation);\n+    if (!do_full_collection) {\n+      \/\/ Adjust generation sizes.\n+      _young_gen->compute_new_size();\n+\n+      print_heap_change(pre_gc_values);\n+\n+      \/\/ Track memory usage and detect low memory after GC finishes\n+      MemoryService::track_memory_usage();\n+\n+      gc_epilogue(complete);\n+    }\n+\n+    print_heap_after_gc();\n+\n+  } else {\n+    \/\/ No young collection, ask if we need to perform Full collection.\n+    do_full_collection = should_do_full_collection(size, full, is_tlab, max_generation);\n+  }\n+\n+  if (do_full_collection) {\n+    GCIdMark gc_id_mark;\n+    GCTraceCPUTime tcpu(GenMarkSweep::gc_tracer());\n+    GCTraceTime(Info, gc) t(\"Pause Full\", nullptr, gc_cause(), true);\n+\n+    print_heap_before_gc();\n+\n+    if (!prepared_for_verification && run_verification &&\n+        VerifyGCLevel <= 1 && VerifyBeforeGC) {\n+      prepare_for_verify();\n+    }\n+\n+    if (!do_young_collection) {\n+      gc_prologue(complete);\n+      increment_total_collections(complete);\n+    }\n+\n+    \/\/ Accounting quirk: total full collections would be incremented when \"complete\"\n+    \/\/ is set, by calling increment_total_collections above. However, we also need to\n+    \/\/ account Full collections that had \"complete\" unset.\n+    if (!complete) {\n+      increment_total_full_collections();\n+    }\n+\n+    CodeCache::on_gc_marking_cycle_start();\n+\n+    ClassUnloadingContext ctx(1 \/* num_nmethod_unlink_workers *\/,\n+                              false \/* unregister_nmethods_during_purge *\/,\n+                              false \/* lock_codeblob_free_separately *\/);\n+\n+    collect_generation(_old_gen,\n+                       full,\n+                       size,\n+                       is_tlab,\n+                       run_verification && VerifyGCLevel <= 1,\n+                       do_clear_all_soft_refs);\n+\n+    CodeCache::on_gc_marking_cycle_finish();\n+    CodeCache::arm_all_nmethods();\n+\n+    \/\/ Adjust generation sizes.\n+    _old_gen->compute_new_size();\n+    _young_gen->compute_new_size();\n+\n+    \/\/ Delete metaspaces for unloaded class loaders and clean up loader_data graph\n+    ClassLoaderDataGraph::purge(\/*at_safepoint*\/true);\n+    DEBUG_ONLY(MetaspaceUtils::verify();)\n+\n+    \/\/ Need to clear claim bits for the next mark.\n+    ClassLoaderDataGraph::clear_claimed_marks();\n+\n+    \/\/ Resize the metaspace capacity after full collections\n+    MetaspaceGC::compute_new_size();\n+\n+    print_heap_change(pre_gc_values);\n+\n+    \/\/ Track memory usage and detect low memory after GC finishes\n+    MemoryService::track_memory_usage();\n+\n+    \/\/ Need to tell the epilogue code we are done with Full GC, regardless what was\n+    \/\/ the initial value for \"complete\" flag.\n+    gc_epilogue(true);\n+\n+    print_heap_after_gc();\n+  }\n+}\n+\n+bool SerialHeap::should_do_full_collection(size_t size, bool full, bool is_tlab,\n+                                           SerialHeap::GenerationType max_gen) const {\n+  return max_gen == OldGen && _old_gen->should_collect(full, size, is_tlab);\n+}\n+\n+void SerialHeap::register_nmethod(nmethod* nm) {\n+  ScavengableNMethods::register_nmethod(nm);\n+}\n+\n+void SerialHeap::unregister_nmethod(nmethod* nm) {\n+  ScavengableNMethods::unregister_nmethod(nm);\n+}\n+\n+void SerialHeap::verify_nmethod(nmethod* nm) {\n+  ScavengableNMethods::verify_nmethod(nm);\n+}\n+\n+void SerialHeap::prune_scavengable_nmethods() {\n+  ScavengableNMethods::prune_nmethods_not_into_young();\n+}\n+\n+void SerialHeap::prune_unlinked_nmethods() {\n+  ScavengableNMethods::prune_unlinked_nmethods();\n+}\n+\n+HeapWord* SerialHeap::satisfy_failed_allocation(size_t size, bool is_tlab) {\n+  GCCauseSetter x(this, GCCause::_allocation_failure);\n+  HeapWord* result = nullptr;\n+\n+  assert(size != 0, \"Precondition violated\");\n+  if (GCLocker::is_active_and_needs_gc()) {\n+    \/\/ GC locker is active; instead of a collection we will attempt\n+    \/\/ to expand the heap, if there's room for expansion.\n+    if (!is_maximal_no_gc()) {\n+      result = expand_heap_and_allocate(size, is_tlab);\n+    }\n+    return result;   \/\/ Could be null if we are out of space.\n+  } else if (!incremental_collection_will_fail(false \/* don't consult_young *\/)) {\n+    \/\/ Do an incremental collection.\n+    do_collection(false,                     \/\/ full\n+                  false,                     \/\/ clear_all_soft_refs\n+                  size,                      \/\/ size\n+                  is_tlab,                   \/\/ is_tlab\n+                  SerialHeap::OldGen); \/\/ max_generation\n+  } else {\n+    log_trace(gc)(\" :: Trying full because partial may fail :: \");\n+    \/\/ Try a full collection; see delta for bug id 6266275\n+    \/\/ for the original code and why this has been simplified\n+    \/\/ with from-space allocation criteria modified and\n+    \/\/ such allocation moved out of the safepoint path.\n+    do_collection(true,                      \/\/ full\n+                  false,                     \/\/ clear_all_soft_refs\n+                  size,                      \/\/ size\n+                  is_tlab,                   \/\/ is_tlab\n+                  SerialHeap::OldGen); \/\/ max_generation\n+  }\n+\n+  result = attempt_allocation(size, is_tlab, false \/*first_only*\/);\n+\n+  if (result != nullptr) {\n+    assert(is_in_reserved(result), \"result not in heap\");\n+    return result;\n+  }\n+\n+  \/\/ OK, collection failed, try expansion.\n+  result = expand_heap_and_allocate(size, is_tlab);\n+  if (result != nullptr) {\n+    return result;\n+  }\n+\n+  \/\/ If we reach this point, we're really out of memory. Try every trick\n+  \/\/ we can to reclaim memory. Force collection of soft references. Force\n+  \/\/ a complete compaction of the heap. Any additional methods for finding\n+  \/\/ free memory should be here, especially if they are expensive. If this\n+  \/\/ attempt fails, an OOM exception will be thrown.\n+  {\n+    UIntFlagSetting flag_change(MarkSweepAlwaysCompactCount, 1); \/\/ Make sure the heap is fully compacted\n+\n+    do_collection(true,                      \/\/ full\n+                  true,                      \/\/ clear_all_soft_refs\n+                  size,                      \/\/ size\n+                  is_tlab,                   \/\/ is_tlab\n+                  SerialHeap::OldGen); \/\/ max_generation\n+  }\n+\n+  result = attempt_allocation(size, is_tlab, false \/* first_only *\/);\n+  if (result != nullptr) {\n+    assert(is_in_reserved(result), \"result not in heap\");\n+    return result;\n+  }\n+\n+  assert(!soft_ref_policy()->should_clear_all_soft_refs(),\n+    \"Flag should have been handled and cleared prior to this point\");\n+\n+  \/\/ What else?  We might try synchronous finalization later.  If the total\n+  \/\/ space available is large enough for the allocation, then a more\n+  \/\/ complete compaction phase than we've tried so far might be\n+  \/\/ appropriate.\n+  return nullptr;\n+}\n+\n+#ifdef ASSERT\n+class AssertNonScavengableClosure: public OopClosure {\n+public:\n+  virtual void do_oop(oop* p) {\n+    assert(!SerialHeap::heap()->is_in_partial_collection(*p),\n+      \"Referent should not be scavengable.\");  }\n+  virtual void do_oop(narrowOop* p) { ShouldNotReachHere(); }\n+};\n+static AssertNonScavengableClosure assert_is_non_scavengable_closure;\n+#endif\n+\n+void SerialHeap::process_roots(ScanningOption so,\n+                               OopClosure* strong_roots,\n+                               CLDClosure* strong_cld_closure,\n+                               CLDClosure* weak_cld_closure,\n+                               CodeBlobToOopClosure* code_roots) {\n+  \/\/ General roots.\n+  assert(code_roots != nullptr, \"code root closure should always be set\");\n+\n+  ClassLoaderDataGraph::roots_cld_do(strong_cld_closure, weak_cld_closure);\n+\n+  \/\/ Only process code roots from thread stacks if we aren't visiting the entire CodeCache anyway\n+  CodeBlobToOopClosure* roots_from_code_p = (so & SO_AllCodeCache) ? nullptr : code_roots;\n+\n+  Threads::oops_do(strong_roots, roots_from_code_p);\n+\n+  OopStorageSet::strong_oops_do(strong_roots);\n+\n+  if (so & SO_ScavengeCodeCache) {\n+    assert(code_roots != nullptr, \"must supply closure for code cache\");\n+\n+    \/\/ We only visit parts of the CodeCache when scavenging.\n+    ScavengableNMethods::nmethods_do(code_roots);\n+  }\n+  if (so & SO_AllCodeCache) {\n+    assert(code_roots != nullptr, \"must supply closure for code cache\");\n+\n+    \/\/ CMSCollector uses this to do intermediate-strength collections.\n+    \/\/ We scan the entire code cache, since CodeCache::do_unloading is not called.\n+    CodeCache::blobs_do(code_roots);\n+  }\n+  \/\/ Verify that the code cache contents are not subject to\n+  \/\/ movement by a scavenging collection.\n+  DEBUG_ONLY(CodeBlobToOopClosure assert_code_is_non_scavengable(&assert_is_non_scavengable_closure, !CodeBlobToOopClosure::FixRelocations));\n+  DEBUG_ONLY(ScavengableNMethods::asserted_non_scavengable_nmethods_do(&assert_code_is_non_scavengable));\n+}\n+\n+bool SerialHeap::no_allocs_since_save_marks() {\n+  return _young_gen->no_allocs_since_save_marks() &&\n+         _old_gen->no_allocs_since_save_marks();\n+}\n+\n+\/\/ public collection interfaces\n+void SerialHeap::collect(GCCause::Cause cause) {\n+  \/\/ The caller doesn't have the Heap_lock\n+  assert(!Heap_lock->owned_by_self(), \"this thread should not own the Heap_lock\");\n+\n+  unsigned int gc_count_before;\n+  unsigned int full_gc_count_before;\n+\n+  {\n+    MutexLocker ml(Heap_lock);\n+    \/\/ Read the GC count while holding the Heap_lock\n+    gc_count_before      = total_collections();\n+    full_gc_count_before = total_full_collections();\n+  }\n+\n+  if (GCLocker::should_discard(cause, gc_count_before)) {\n+    return;\n+  }\n+\n+  bool should_run_young_gc =  (cause == GCCause::_wb_young_gc)\n+                           || (cause == GCCause::_gc_locker)\n+                DEBUG_ONLY(|| (cause == GCCause::_scavenge_alot));\n+\n+  const GenerationType max_generation = should_run_young_gc\n+                                      ? YoungGen\n+                                      : OldGen;\n+\n+  while (true) {\n+    VM_GenCollectFull op(gc_count_before, full_gc_count_before,\n+                         cause, max_generation);\n+    VMThread::execute(&op);\n+\n+    if (!GCCause::is_explicit_full_gc(cause)) {\n+      return;\n+    }\n+\n+    {\n+      MutexLocker ml(Heap_lock);\n+      \/\/ Read the GC count while holding the Heap_lock\n+      if (full_gc_count_before != total_full_collections()) {\n+        return;\n+      }\n+    }\n+\n+    if (GCLocker::is_active_and_needs_gc()) {\n+      \/\/ If GCLocker is active, wait until clear before retrying.\n+      GCLocker::stall_until_clear();\n+    }\n+  }\n+}\n+\n+void SerialHeap::do_full_collection(bool clear_all_soft_refs) {\n+   do_full_collection(clear_all_soft_refs, OldGen);\n+}\n+\n+void SerialHeap::do_full_collection(bool clear_all_soft_refs,\n+                                    GenerationType last_generation) {\n+  do_collection(true,                   \/\/ full\n+                clear_all_soft_refs,    \/\/ clear_all_soft_refs\n+                0,                      \/\/ size\n+                false,                  \/\/ is_tlab\n+                last_generation);       \/\/ last_generation\n+  \/\/ Hack XXX FIX ME !!!\n+  \/\/ A scavenge may not have been attempted, or may have\n+  \/\/ been attempted and failed, because the old gen was too full\n+  if (gc_cause() == GCCause::_gc_locker && incremental_collection_failed()) {\n+    log_debug(gc, jni)(\"GC locker: Trying a full collection because scavenge failed\");\n+    \/\/ This time allow the old gen to be collected as well\n+    do_collection(true,                \/\/ full\n+                  clear_all_soft_refs, \/\/ clear_all_soft_refs\n+                  0,                   \/\/ size\n+                  false,               \/\/ is_tlab\n+                  OldGen);             \/\/ last_generation\n+  }\n+}\n+\n+bool SerialHeap::is_in_young(const void* p) const {\n+  bool result = p < _old_gen->reserved().start();\n+  assert(result == _young_gen->is_in_reserved(p),\n+         \"incorrect test - result=%d, p=\" PTR_FORMAT, result, p2i(p));\n+  return result;\n+}\n+\n+bool SerialHeap::requires_barriers(stackChunkOop obj) const {\n+  return !is_in_young(obj);\n+}\n+\n+\/\/ Returns \"TRUE\" iff \"p\" points into the committed areas of the heap.\n+bool SerialHeap::is_in(const void* p) const {\n+  return _young_gen->is_in(p) || _old_gen->is_in(p);\n+}\n+\n+#ifdef ASSERT\n+\/\/ Don't implement this by using is_in_young().  This method is used\n+\/\/ in some cases to check that is_in_young() is correct.\n+bool SerialHeap::is_in_partial_collection(const void* p) {\n+  assert(is_in_reserved(p) || p == nullptr,\n+    \"Does not work if address is non-null and outside of the heap\");\n+  return p < _young_gen->reserved().end() && p != nullptr;\n+}\n+#endif\n+\n+void SerialHeap::object_iterate(ObjectClosure* cl) {\n+  _young_gen->object_iterate(cl);\n+  _old_gen->object_iterate(cl);\n+}\n+\n+HeapWord* SerialHeap::block_start(const void* addr) const {\n+  assert(is_in_reserved(addr), \"block_start of address outside of heap\");\n+  if (_young_gen->is_in_reserved(addr)) {\n+    assert(_young_gen->is_in(addr), \"addr should be in allocated part of generation\");\n+    return _young_gen->block_start(addr);\n+  }\n+\n+  assert(_old_gen->is_in_reserved(addr), \"Some generation should contain the address\");\n+  assert(_old_gen->is_in(addr), \"addr should be in allocated part of generation\");\n+  return _old_gen->block_start(addr);\n+}\n+\n+bool SerialHeap::block_is_obj(const HeapWord* addr) const {\n+  assert(is_in_reserved(addr), \"block_is_obj of address outside of heap\");\n+  assert(block_start(addr) == addr, \"addr must be a block start\");\n+  if (_young_gen->is_in_reserved(addr)) {\n+    return _young_gen->block_is_obj(addr);\n+  }\n+\n+  assert(_old_gen->is_in_reserved(addr), \"Some generation should contain the address\");\n+  return _old_gen->block_is_obj(addr);\n+}\n+\n+size_t SerialHeap::tlab_capacity(Thread* thr) const {\n+  assert(!_old_gen->supports_tlab_allocation(), \"Old gen supports TLAB allocation?!\");\n+  assert(_young_gen->supports_tlab_allocation(), \"Young gen doesn't support TLAB allocation?!\");\n+  return _young_gen->tlab_capacity();\n+}\n+\n+size_t SerialHeap::tlab_used(Thread* thr) const {\n+  assert(!_old_gen->supports_tlab_allocation(), \"Old gen supports TLAB allocation?!\");\n+  assert(_young_gen->supports_tlab_allocation(), \"Young gen doesn't support TLAB allocation?!\");\n+  return _young_gen->tlab_used();\n+}\n+\n+size_t SerialHeap::unsafe_max_tlab_alloc(Thread* thr) const {\n+  assert(!_old_gen->supports_tlab_allocation(), \"Old gen supports TLAB allocation?!\");\n+  assert(_young_gen->supports_tlab_allocation(), \"Young gen doesn't support TLAB allocation?!\");\n+  return _young_gen->unsafe_max_tlab_alloc();\n+}\n+\n+HeapWord* SerialHeap::allocate_new_tlab(size_t min_size,\n+                                        size_t requested_size,\n+                                        size_t* actual_size) {\n+  HeapWord* result = mem_allocate_work(requested_size \/* size *\/,\n+                                       true \/* is_tlab *\/);\n+  if (result != nullptr) {\n+    *actual_size = requested_size;\n+  }\n+\n+  return result;\n+}\n+\n+void SerialHeap::prepare_for_verify() {\n+  ensure_parsability(false);        \/\/ no need to retire TLABs\n+}\n+\n+void SerialHeap::generation_iterate(GenClosure* cl,\n+                                    bool old_to_young) {\n+  if (old_to_young) {\n+    cl->do_generation(_old_gen);\n+    cl->do_generation(_young_gen);\n+  } else {\n+    cl->do_generation(_young_gen);\n+    cl->do_generation(_old_gen);\n+  }\n+}\n+\n+bool SerialHeap::is_maximal_no_gc() const {\n+  return _young_gen->is_maximal_no_gc() && _old_gen->is_maximal_no_gc();\n+}\n+\n+void SerialHeap::save_marks() {\n+  _young_gen->save_marks();\n+  _old_gen->save_marks();\n+}\n+\n+void SerialHeap::verify(VerifyOption option \/* ignored *\/) {\n+  log_debug(gc, verify)(\"%s\", _old_gen->name());\n+  _old_gen->verify();\n+\n+  log_debug(gc, verify)(\"%s\", _young_gen->name());\n+  _young_gen->verify();\n+\n+  log_debug(gc, verify)(\"RemSet\");\n+  rem_set()->verify();\n+}\n+\n+void SerialHeap::print_on(outputStream* st) const {\n+  if (_young_gen != nullptr) {\n+    _young_gen->print_on(st);\n+  }\n+  if (_old_gen != nullptr) {\n+    _old_gen->print_on(st);\n+  }\n+  MetaspaceUtils::print_on(st);\n+}\n+\n+void SerialHeap::gc_threads_do(ThreadClosure* tc) const {\n+}\n+\n+bool SerialHeap::print_location(outputStream* st, void* addr) const {\n+  return BlockLocationPrinter<SerialHeap>::print_location(st, addr);\n+}\n+\n+void SerialHeap::print_tracing_info() const {\n+  if (log_is_enabled(Debug, gc, heap, exit)) {\n+    LogStreamHandle(Debug, gc, heap, exit) lsh;\n+    _young_gen->print_summary_info_on(&lsh);\n+    _old_gen->print_summary_info_on(&lsh);\n+  }\n+}\n+\n+void SerialHeap::print_heap_change(const PreGenGCValues& pre_gc_values) const {\n+  const DefNewGeneration* const def_new_gen = (DefNewGeneration*) young_gen();\n+\n+  log_info(gc, heap)(HEAP_CHANGE_FORMAT\" \"\n+                     HEAP_CHANGE_FORMAT\" \"\n+                     HEAP_CHANGE_FORMAT,\n+                     HEAP_CHANGE_FORMAT_ARGS(def_new_gen->short_name(),\n+                                             pre_gc_values.young_gen_used(),\n+                                             pre_gc_values.young_gen_capacity(),\n+                                             def_new_gen->used(),\n+                                             def_new_gen->capacity()),\n+                     HEAP_CHANGE_FORMAT_ARGS(\"Eden\",\n+                                             pre_gc_values.eden_used(),\n+                                             pre_gc_values.eden_capacity(),\n+                                             def_new_gen->eden()->used(),\n+                                             def_new_gen->eden()->capacity()),\n+                     HEAP_CHANGE_FORMAT_ARGS(\"From\",\n+                                             pre_gc_values.from_used(),\n+                                             pre_gc_values.from_capacity(),\n+                                             def_new_gen->from()->used(),\n+                                             def_new_gen->from()->capacity()));\n+  log_info(gc, heap)(HEAP_CHANGE_FORMAT,\n+                     HEAP_CHANGE_FORMAT_ARGS(old_gen()->short_name(),\n+                                             pre_gc_values.old_gen_used(),\n+                                             pre_gc_values.old_gen_capacity(),\n+                                             old_gen()->used(),\n+                                             old_gen()->capacity()));\n+  MetaspaceUtils::print_metaspace_change(pre_gc_values.metaspace_sizes());\n+}\n+\n+class GenGCPrologueClosure: public SerialHeap::GenClosure {\n+ private:\n+  bool _full;\n+ public:\n+  void do_generation(Generation* gen) {\n+    gen->gc_prologue(_full);\n+  }\n+  GenGCPrologueClosure(bool full) : _full(full) {};\n+};\n+\n+void SerialHeap::gc_prologue(bool full) {\n+  assert(InlineCacheBuffer::is_empty(), \"should have cleaned up ICBuffer\");\n+\n+  \/\/ Fill TLAB's and such\n+  ensure_parsability(true);   \/\/ retire TLABs\n+\n+  \/\/ Walk generations\n+  GenGCPrologueClosure blk(full);\n+  generation_iterate(&blk, false);  \/\/ not old-to-young.\n+};\n+\n+class GenGCEpilogueClosure: public SerialHeap::GenClosure {\n+ private:\n+  bool _full;\n+ public:\n+  void do_generation(Generation* gen) {\n+    gen->gc_epilogue(_full);\n+  }\n+  GenGCEpilogueClosure(bool full) : _full(full) {};\n+};\n+\n+void SerialHeap::gc_epilogue(bool full) {\n+#if COMPILER2_OR_JVMCI\n+  assert(DerivedPointerTable::is_empty(), \"derived pointer present\");\n+#endif \/\/ COMPILER2_OR_JVMCI\n+\n+  resize_all_tlabs();\n+\n+  GenGCEpilogueClosure blk(full);\n+  generation_iterate(&blk, false);  \/\/ not old-to-young.\n+\n+  MetaspaceCounters::update_performance_counters();\n+};\n+\n+#ifndef PRODUCT\n+class GenGCSaveTopsBeforeGCClosure: public SerialHeap::GenClosure {\n+ private:\n+ public:\n+  void do_generation(Generation* gen) {\n+    gen->record_spaces_top();\n+  }\n+};\n+\n+void SerialHeap::record_gen_tops_before_GC() {\n+  if (ZapUnusedHeapArea) {\n+    GenGCSaveTopsBeforeGCClosure blk;\n+    generation_iterate(&blk, false);  \/\/ not old-to-young.\n+  }\n+}\n+#endif  \/\/ not PRODUCT\n","filename":"src\/hotspot\/share\/gc\/serial\/serialHeap.cpp","additions":990,"deletions":7,"binary":false,"changes":997,"status":"modified"},{"patch":"@@ -570,0 +570,1 @@\n+  static uint count = 0;\n@@ -571,2 +572,5 @@\n-    GCTraceTime(Info, gc) tm(before ? \"Heap Dump (before full gc)\" : \"Heap Dump (after full gc)\", timer);\n-    HeapDumper::dump_heap();\n+    if (FullGCHeapDumpLimit == 0 || count < FullGCHeapDumpLimit) {\n+      GCTraceTime(Info, gc) tm(before ? \"Heap Dump (before full gc)\" : \"Heap Dump (after full gc)\", timer);\n+      HeapDumper::dump_heap();\n+      count++;\n+    }\n","filename":"src\/hotspot\/share\/gc\/shared\/collectedHeap.cpp","additions":6,"deletions":2,"binary":false,"changes":8,"status":"modified"},{"patch":"@@ -2,1 +2,1 @@\n- * Copyright (c) 2001, 2023, Oracle and\/or its affiliates. All rights reserved.\n+ * Copyright (c) 2001, 2024, Oracle and\/or its affiliates. All rights reserved.\n@@ -87,2 +87,1 @@\n-\/\/   GenCollectedHeap\n-\/\/     SerialHeap\n+\/\/   SerialHeap\n","filename":"src\/hotspot\/share\/gc\/shared\/collectedHeap.hpp","additions":2,"deletions":3,"binary":false,"changes":5,"status":"modified"},{"patch":"@@ -411,5 +411,0 @@\n-  product(uint, AdaptiveSizePolicyCollectionCostMargin, 50,                 \\\n-          \"If collection costs are within margin, reduce both by full \"     \\\n-          \"delta\")                                                          \\\n-          range(0, 100)                                                     \\\n-                                                                            \\\n","filename":"src\/hotspot\/share\/gc\/shared\/gc_globals.hpp","additions":0,"deletions":5,"binary":false,"changes":5,"status":"modified"},{"patch":"@@ -974,0 +974,3 @@\n+      if (ZapUnusedHeapArea) {\n+        SpaceMangler::mangle_region(MemRegion(r->top(), r->end()));\n+      }\n","filename":"src\/hotspot\/share\/gc\/shenandoah\/shenandoahFullGC.cpp","additions":3,"deletions":0,"binary":false,"changes":3,"status":"modified"},{"patch":"@@ -510,0 +510,1 @@\n+  _gc_state_changed(false),\n@@ -1747,3 +1748,8 @@\n-void ShenandoahHeap::set_gc_state_all_threads(char state) {\n-  for (JavaThreadIteratorWithHandle jtiwh; JavaThread *t = jtiwh.next(); ) {\n-    ShenandoahThreadLocalData::set_gc_state(t, state);\n+void ShenandoahHeap::propagate_gc_state_to_java_threads() {\n+  assert(ShenandoahSafepoint::is_at_shenandoah_safepoint(), \"Must be at Shenandoah safepoint\");\n+  if (_gc_state_changed) {\n+    _gc_state_changed = false;\n+    char state = gc_state();\n+    for (JavaThreadIteratorWithHandle jtiwh; JavaThread *t = jtiwh.next(); ) {\n+      ShenandoahThreadLocalData::set_gc_state(t, state);\n+    }\n@@ -1753,2 +1759,2 @@\n-void ShenandoahHeap::set_gc_state_mask(uint mask, bool value) {\n-  assert(ShenandoahSafepoint::is_at_shenandoah_safepoint(), \"Should really be Shenandoah safepoint\");\n+void ShenandoahHeap::set_gc_state(uint mask, bool value) {\n+  assert(ShenandoahSafepoint::is_at_shenandoah_safepoint(), \"Must be at Shenandoah safepoint\");\n@@ -1756,1 +1762,1 @@\n-  set_gc_state_all_threads(_gc_state.raw_value());\n+  _gc_state_changed = true;\n@@ -1761,1 +1767,1 @@\n-  set_gc_state_mask(MARKING, in_progress);\n+  set_gc_state(MARKING, in_progress);\n@@ -1767,1 +1773,1 @@\n-  set_gc_state_mask(EVACUATION, in_progress);\n+  set_gc_state(EVACUATION, in_progress);\n@@ -1779,1 +1785,1 @@\n-  set_gc_state_mask(WEAK_ROOTS, cond);\n+  set_gc_state(WEAK_ROOTS, cond);\n@@ -1828,0 +1834,1 @@\n+                            true \/* unregister_nmethods_during_purge *\/,\n@@ -1905,1 +1912,1 @@\n-  set_gc_state_mask(HAS_FORWARDED, cond);\n+  set_gc_state(HAS_FORWARDED, cond);\n@@ -1944,1 +1951,1 @@\n-  set_gc_state_mask(UPDATEREFS, in_progress);\n+  set_gc_state(UPDATEREFS, in_progress);\n","filename":"src\/hotspot\/share\/gc\/shenandoah\/shenandoahHeap.cpp","additions":18,"deletions":11,"binary":false,"changes":29,"status":"modified"},{"patch":"@@ -624,0 +624,2 @@\n+  ShenandoahHeap::heap()->propagate_gc_state_to_java_threads();\n+\n","filename":"src\/hotspot\/share\/gc\/shenandoah\/shenandoahVerifier.cpp","additions":2,"deletions":0,"binary":false,"changes":2,"status":"modified"},{"patch":"@@ -2,1 +2,1 @@\n- * Copyright (c) 2015, 2023, Oracle and\/or its affiliates. All rights reserved.\n+ * Copyright (c) 2015, 2024, Oracle and\/or its affiliates. All rights reserved.\n@@ -323,0 +323,14 @@\n+uint ZBarrierSetC2::estimated_barrier_size(const Node* node) const {\n+  uint8_t barrier_data = MemNode::barrier_data(node);\n+  assert(barrier_data != 0, \"should be a barrier node\");\n+  uint uncolor_or_color_size = node->is_Load() ? 1 : 2;\n+  if ((barrier_data & ZBarrierElided) != 0) {\n+    return uncolor_or_color_size;\n+  }\n+  \/\/ A compare and branch corresponds to approximately four fast-path Ideal\n+  \/\/ nodes (Cmp, Bool, If, If projection). The slow path (If projection and\n+  \/\/ runtime call) is excluded since the corresponding code is laid out\n+  \/\/ separately and does not directly affect performance.\n+  return uncolor_or_color_size + 4;\n+}\n+\n","filename":"src\/hotspot\/share\/gc\/z\/c2\/zBarrierSetC2.cpp","additions":15,"deletions":1,"binary":false,"changes":16,"status":"modified"},{"patch":"@@ -90,0 +90,1 @@\n+    _is_active(false),\n@@ -106,0 +107,14 @@\n+void ZRelocateQueue::activate(uint nworkers) {\n+  _is_active = true;\n+  join(nworkers);\n+}\n+\n+void ZRelocateQueue::deactivate() {\n+  Atomic::store(&_is_active, false);\n+  clear();\n+}\n+\n+bool ZRelocateQueue::is_active() const {\n+  return Atomic::load(&_is_active);\n+}\n+\n@@ -330,1 +345,1 @@\n-  _queue.join(workers()->active_workers());\n+  _queue.activate(workers()->active_workers());\n@@ -1092,0 +1107,3 @@\n+\n+    \/\/ Signal that we're not using the queue anymore. Used mostly for asserts.\n+    _queue->deactivate();\n@@ -1236,2 +1254,0 @@\n-\n-  _queue.clear();\n@@ -1320,0 +1336,4 @@\n+\n+bool ZRelocate::is_queue_active() const {\n+  return _queue.is_active();\n+}\n","filename":"src\/hotspot\/share\/gc\/z\/zRelocate.cpp","additions":23,"deletions":3,"binary":false,"changes":26,"status":"modified"},{"patch":"@@ -3044,0 +3044,7 @@\n+  if (!JVMCIENV->is_hotspot()) {\n+    \/\/ It's generally not safe to call Java code before the module system is initialized\n+    if (!Universe::is_module_initialized()) {\n+      JVMCI_event_1(\"callSystemExit(%d) before Universe::is_module_initialized() -> direct VM exit\", status);\n+      vm_exit_during_initialization();\n+    }\n+  }\n","filename":"src\/hotspot\/share\/jvmci\/jvmciCompilerToVM.cpp","additions":7,"deletions":0,"binary":false,"changes":7,"status":"modified"},{"patch":"@@ -346,1 +346,1 @@\n-      _fillerArrayKlassObj = TypeArrayKlass::create_klass(T_INT, \"Ljdk\/internal\/vm\/FillerArray;\", CHECK);\n+      _fillerArrayKlassObj = TypeArrayKlass::create_klass(T_INT, \"[Ljdk\/internal\/vm\/FillerElement;\", CHECK);\n","filename":"src\/hotspot\/share\/memory\/universe.cpp","additions":1,"deletions":1,"binary":false,"changes":2,"status":"modified"},{"patch":"@@ -48,2 +48,0 @@\n-class FilteringClosure;\n-\n","filename":"src\/hotspot\/share\/oops\/oop.hpp","additions":0,"deletions":2,"binary":false,"changes":2,"status":"modified"},{"patch":"@@ -2,1 +2,1 @@\n- * Copyright (c) 1997, 2023, Oracle and\/or its affiliates. All rights reserved.\n+ * Copyright (c) 1997, 2024, Oracle and\/or its affiliates. All rights reserved.\n@@ -980,2 +980,2 @@\n-    assert(is_CallStaticJava()  && cg->is_mh_late_inline() ||\n-           is_CallDynamicJava() && cg->is_virtual_late_inline(), \"mismatch\");\n+    assert((is_CallStaticJava()  && cg->is_mh_late_inline()) ||\n+           (is_CallDynamicJava() && cg->is_virtual_late_inline()), \"mismatch\");\n@@ -1647,2 +1647,2 @@\n-             length_type->is_con() && narrow_length_type->is_con() &&\n-                (narrow_length_type->_hi <= length_type->_lo) ||\n+             (length_type->is_con() && narrow_length_type->is_con() &&\n+              (narrow_length_type->_hi <= length_type->_lo)) ||\n@@ -1661,2 +1661,1 @@\n-        length = new CastIINode(length, narrow_length_type);\n-        length->set_req(TypeFunc::Control, init->proj_out_or_null(TypeFunc::Control));\n+        length = new CastIINode(init->proj_out_or_null(TypeFunc::Control), length, narrow_length_type);\n","filename":"src\/hotspot\/share\/opto\/callnode.cpp","additions":6,"deletions":7,"binary":false,"changes":13,"status":"modified"},{"patch":"@@ -2,1 +2,1 @@\n- * Copyright (c) 1997, 2023, Oracle and\/or its affiliates. All rights reserved.\n+ * Copyright (c) 1997, 2024, Oracle and\/or its affiliates. All rights reserved.\n@@ -32,0 +32,1 @@\n+#include \"compiler\/compilationFailureInfo.hpp\"\n@@ -35,0 +36,1 @@\n+#include \"compiler\/compiler_globals.hpp\"\n@@ -640,0 +642,1 @@\n+                  _first_failure_details(nullptr),\n@@ -743,1 +746,1 @@\n-    initial_gvn()->transform_no_reclaim(top());\n+    initial_gvn()->transform(top());\n@@ -929,0 +932,1 @@\n+    _first_failure_details(nullptr),\n@@ -981,1 +985,1 @@\n-    gvn.transform_no_reclaim(top());\n+    gvn.transform(top());\n@@ -992,0 +996,5 @@\n+Compile::~Compile() {\n+  delete _print_inlining_stream;\n+  delete _first_failure_details;\n+};\n+\n@@ -1044,0 +1053,4 @@\n+#ifndef PRODUCT\n+  Copy::zero_to_bytes(_igv_phase_iter, sizeof(_igv_phase_iter));\n+#endif\n+\n@@ -1055,1 +1068,1 @@\n-    if (has_method() && (_directive->VectorizeOption || _directive->VectorizeDebugOption)) {\n+    if (has_method() && _directive->VectorizeOption) {\n@@ -2404,0 +2417,1 @@\n+  print_method(PHASE_BEFORE_CCP1, 2);\n@@ -2979,0 +2993,2 @@\n+\n+    print_method(PHASE_REGISTER_ALLOCATION, 2);\n@@ -2996,0 +3012,1 @@\n+    print_method(PHASE_BLOCK_ORDERING, 3);\n@@ -3003,0 +3020,1 @@\n+    print_method(PHASE_PEEPHOLE, 3);\n@@ -3009,0 +3027,1 @@\n+    print_method(PHASE_POSTALLOC_EXPAND, 3);\n@@ -3137,2 +3156,2 @@\n-            n->is_Load() && (n->as_Load()->bottom_type()->isa_oopptr() ||\n-                             LoadNode::is_immutable_value(n->in(MemNode::Address))),\n+            (n->is_Load() && (n->as_Load()->bottom_type()->isa_oopptr() ||\n+                              LoadNode::is_immutable_value(n->in(MemNode::Address)))),\n@@ -3695,0 +3714,25 @@\n+#ifdef ASSERT\n+    \/\/ Add VerifyVectorAlignment node between adr and load \/ store.\n+    if (VerifyAlignVector && Matcher::has_match_rule(Op_VerifyVectorAlignment)) {\n+      bool must_verify_alignment = n->is_LoadVector() ? n->as_LoadVector()->must_verify_alignment() :\n+                                                        n->as_StoreVector()->must_verify_alignment();\n+      if (must_verify_alignment) {\n+        jlong vector_width = n->is_LoadVector() ? n->as_LoadVector()->memory_size() :\n+                                                  n->as_StoreVector()->memory_size();\n+        \/\/ The memory access should be aligned to the vector width in bytes.\n+        \/\/ However, the underlying array is possibly less well aligned, but at least\n+        \/\/ to ObjectAlignmentInBytes. Hence, even if multiple arrays are accessed in\n+        \/\/ a loop we can expect at least the following alignment:\n+        jlong guaranteed_alignment = MIN2(vector_width, (jlong)ObjectAlignmentInBytes);\n+        assert(2 <= guaranteed_alignment && guaranteed_alignment <= 64, \"alignment must be in range\");\n+        assert(is_power_of_2(guaranteed_alignment), \"alignment must be power of 2\");\n+        \/\/ Create mask from alignment. e.g. 0b1000 -> 0b0111\n+        jlong mask = guaranteed_alignment - 1;\n+        Node* mask_con = ConLNode::make(mask);\n+        VerifyVectorAlignmentNode* va = new VerifyVectorAlignmentNode(n->in(MemNode::Address), mask_con);\n+        n->set_req(MemNode::Address, va);\n+      }\n+    }\n+#endif\n+    break;\n+\n@@ -4355,0 +4399,3 @@\n+    if (CaptureBailoutInformation) {\n+      _first_failure_details = new CompilationFailureInfo(reason);\n+    }\n@@ -4468,1 +4515,0 @@\n-    value = new CastIINode(value, itype, carry_dependency ? ConstraintCastNode::StrongDependency : ConstraintCastNode::RegularDependency, true \/* range check dependency *\/);\n@@ -4473,1 +4519,1 @@\n-    value->set_req(0, ctrl);\n+    value = new CastIINode(ctrl, value, itype, carry_dependency ? ConstraintCastNode::StrongDependency : ConstraintCastNode::RegularDependency, true \/* range check dependency *\/);\n@@ -5109,0 +5155,4 @@\n+  int iter = ++_igv_phase_iter[cpt];\n+  if (iter > 1) {\n+    ss.print(\" %d\", iter);\n+  }\n","filename":"src\/hotspot\/share\/opto\/compile.cpp","additions":58,"deletions":8,"binary":false,"changes":66,"status":"modified"},{"patch":"@@ -495,1 +495,2 @@\n-  case vmIntrinsics::_notifyJvmtiVThreadHideFrames: return inline_native_notify_jvmti_hide();\n+  case vmIntrinsics::_notifyJvmtiVThreadHideFrames:     return inline_native_notify_jvmti_hide();\n+  case vmIntrinsics::_notifyJvmtiVThreadDisableSuspend: return inline_native_notify_jvmti_sync();\n@@ -875,2 +876,1 @@\n-    Node* ccast = new CastIINode(index, TypeInt::POS);\n-    ccast->set_req(0, control());\n+    Node* ccast = new CastIINode(control(), index, TypeInt::POS);\n@@ -1143,1 +1143,3 @@\n-  Node* casted_length = ConstraintCastNode::make(control(), length, TypeInteger::make(0, upper_bound, Type::WidenMax, bt), ConstraintCastNode::RegularDependency, bt);\n+  Node* casted_length = ConstraintCastNode::make_cast_for_basic_type(\n+      control(), length, TypeInteger::make(0, upper_bound, Type::WidenMax, bt),\n+      ConstraintCastNode::RegularDependency, bt);\n@@ -1171,1 +1173,3 @@\n-  Node* result = ConstraintCastNode::make(control(), index, TypeInteger::make(0, upper_bound, Type::WidenMax, bt), ConstraintCastNode::RegularDependency, bt);\n+  Node* result = ConstraintCastNode::make_cast_for_basic_type(\n+      control(), index, TypeInteger::make(0, upper_bound, Type::WidenMax, bt),\n+      ConstraintCastNode::RegularDependency, bt);\n@@ -2953,0 +2957,23 @@\n+\/\/ Always update the is_disable_suspend bit.\n+bool LibraryCallKit::inline_native_notify_jvmti_sync() {\n+  if (!DoJVMTIVirtualThreadTransitions) {\n+    return true;\n+  }\n+  IdealKit ideal(this);\n+\n+  {\n+    \/\/ unconditionally update the is_disable_suspend bit in current JavaThread\n+    Node* thread = ideal.thread();\n+    Node* arg = _gvn.transform(argument(1)); \/\/ argument for notification\n+    Node* addr = basic_plus_adr(thread, in_bytes(JavaThread::is_disable_suspend_offset()));\n+    const TypePtr *addr_type = _gvn.type(addr)->isa_ptr();\n+\n+    sync_kit(ideal);\n+    access_store_at(nullptr, addr, addr_type, arg, _gvn.type(arg), T_BOOLEAN, IN_NATIVE | MO_UNORDERED);\n+    ideal.sync_kit(this);\n+  }\n+  final_sync(ideal);\n+\n+  return true;\n+}\n+\n@@ -4291,2 +4318,1 @@\n-      Node* cast = new CastPPNode(klass_node, akls);\n-      cast->init_req(0, control());\n+      Node* cast = new CastPPNode(control(), klass_node, akls);\n@@ -4549,1 +4575,1 @@\n-  \/\/ Test the header to see if it is unlocked.\n+  \/\/ Test the header to see if it is safe to read w.r.t. locking.\n@@ -4552,3 +4578,10 @@\n-  Node *unlocked_val   = _gvn.MakeConX(markWord::unlocked_value);\n-  Node *chk_unlocked   = _gvn.transform(new CmpXNode( lmasked_header, unlocked_val));\n-  Node *test_unlocked  = _gvn.transform(new BoolNode( chk_unlocked, BoolTest::ne));\n+  if (LockingMode == LM_LIGHTWEIGHT) {\n+    Node *monitor_val   = _gvn.MakeConX(markWord::monitor_value);\n+    Node *chk_monitor   = _gvn.transform(new CmpXNode(lmasked_header, monitor_val));\n+    Node *test_monitor  = _gvn.transform(new BoolNode(chk_monitor, BoolTest::eq));\n+\n+    generate_slow_guard(test_monitor, slow_region);\n+  } else {\n+    Node *unlocked_val      = _gvn.MakeConX(markWord::unlocked_value);\n+    Node *chk_unlocked      = _gvn.transform(new CmpXNode(lmasked_header, unlocked_val));\n+    Node *test_not_unlocked = _gvn.transform(new BoolNode(chk_unlocked, BoolTest::ne));\n@@ -4556,1 +4589,2 @@\n-  generate_slow_guard(test_unlocked, slow_region);\n+    generate_slow_guard(test_not_unlocked, slow_region);\n+  }\n@@ -5390,0 +5424,4 @@\n+    \/\/ Disable the intrinsic if the CPU does not support SIMD sort\n+    if (!Matcher::supports_simd_sort(bt)) {\n+      return false;\n+    }\n@@ -5443,0 +5481,4 @@\n+  \/\/ Disable the intrinsic if the CPU does not support SIMD sort\n+  if (!Matcher::supports_simd_sort(bt)) {\n+    return false;\n+  }\n@@ -5911,2 +5953,1 @@\n-       Node *cast = new CastPPNode(z, TypePtr::NOTNULL);\n-       cast->init_req(0, control());\n+       Node* cast = new CastPPNode(control(), z, TypePtr::NOTNULL);\n","filename":"src\/hotspot\/share\/opto\/library_call.cpp","additions":55,"deletions":14,"binary":false,"changes":69,"status":"modified"},{"patch":"@@ -2,1 +2,1 @@\n- * Copyright (c) 1997, 2023, Oracle and\/or its affiliates. All rights reserved.\n+ * Copyright (c) 1997, 2024, Oracle and\/or its affiliates. All rights reserved.\n@@ -842,0 +842,9 @@\n+uint8_t MemNode::barrier_data(const Node* n) {\n+  if (n->is_LoadStore()) {\n+    return n->as_LoadStore()->barrier_data();\n+  } else if (n->is_Mem()) {\n+    return n->as_Mem()->barrier_data();\n+  }\n+  return 0;\n+}\n+\n@@ -848,2 +857,6 @@\n-bool LoadNode::cmp( const Node &n ) const\n-{ return !Type::cmp( _type, ((LoadNode&)n)._type ); }\n+bool LoadNode::cmp(const Node &n) const {\n+  LoadNode& load = (LoadNode &)n;\n+  return !Type::cmp(_type, load._type) &&\n+         _control_dependency == load._control_dependency &&\n+         _mo == load._mo;\n+}\n@@ -986,0 +999,8 @@\n+LoadNode* LoadNode::pin_array_access_node() const {\n+  const TypePtr* adr_type = this->adr_type();\n+  if (adr_type != nullptr && adr_type->isa_aryptr()) {\n+    return clone_pinned();\n+  }\n+  return nullptr;\n+}\n+\n@@ -1005,1 +1026,2 @@\n-    LoadNode* ld = clone()->as_Load();\n+    \/\/ load depends on the tests that validate the arraycopy\n+    LoadNode* ld = clone_pinned();\n@@ -1047,2 +1069,0 @@\n-    \/\/ load depends on the tests that validate the arraycopy\n-    ld->_control_dependency = UnknownControl;\n@@ -2516,0 +2536,6 @@\n+LoadNode* LoadNode::clone_pinned() const {\n+  LoadNode* ld = clone()->as_Load();\n+  ld->_control_dependency = UnknownControl;\n+  return ld;\n+}\n+\n@@ -4972,1 +4998,1 @@\n-         alias_idx == Compile::AliasIdxBot && !Compile::current()->do_aliasing(),\n+         (alias_idx == Compile::AliasIdxBot && !Compile::current()->do_aliasing()),\n","filename":"src\/hotspot\/share\/opto\/memnode.cpp","additions":33,"deletions":7,"binary":false,"changes":40,"status":"modified"},{"patch":"@@ -4667,1 +4667,1 @@\n-    return this_one->_klass->is_subtype_of(other->_klass);\n+    return this_one->klass()->is_subtype_of(other->klass());\n@@ -5996,1 +5996,1 @@\n-  return this_one->_klass->is_subtype_of(other->_klass) && this_one->_interfaces->contains(other->_interfaces);\n+  return this_one->klass()->is_subtype_of(other->klass()) && this_one->_interfaces->contains(other->_interfaces);\n@@ -6011,1 +6011,1 @@\n-  return this_one->_klass->equals(other->_klass) && this_one->_interfaces->eq(other->_interfaces);\n+  return this_one->klass()->equals(other->klass()) && this_one->_interfaces->eq(other->_interfaces);\n@@ -6025,1 +6025,1 @@\n-    return !this_exact && this_one->_klass->equals(ciEnv::current()->Object_klass())  && other->_interfaces->contains(this_one->_interfaces);\n+    return !this_exact && this_one->klass()->equals(ciEnv::current()->Object_klass())  && other->_interfaces->contains(this_one->_interfaces);\n@@ -6034,1 +6034,1 @@\n-  if (!this_one->_klass->is_subtype_of(other->_klass) && !other->_klass->is_subtype_of(this_one->_klass)) {\n+  if (!this_one->klass()->is_subtype_of(other->klass()) && !other->klass()->is_subtype_of(this_one->klass())) {\n@@ -6039,1 +6039,1 @@\n-    return this_one->_klass->is_subtype_of(other->_klass) && this_one->_interfaces->contains(other->_interfaces);\n+    return this_one->klass()->is_subtype_of(other->klass()) && this_one->_interfaces->contains(other->_interfaces);\n@@ -6119,1 +6119,1 @@\n-ciKlass* TypeAryPtr::compute_klass(DEBUG_ONLY(bool verify)) const {\n+ciKlass* TypeAryPtr::compute_klass() const {\n@@ -6140,22 +6140,1 @@\n-    \/\/ Cannot compute array klass directly from basic type,\n-    \/\/ since subtypes of TypeInt all have basic type T_INT.\n-#ifdef ASSERT\n-    if (verify && el->isa_int()) {\n-      \/\/ Check simple cases when verifying klass.\n-      BasicType bt = T_ILLEGAL;\n-      if (el == TypeInt::BYTE) {\n-        bt = T_BYTE;\n-      } else if (el == TypeInt::SHORT) {\n-        bt = T_SHORT;\n-      } else if (el == TypeInt::CHAR) {\n-        bt = T_CHAR;\n-      } else if (el == TypeInt::INT) {\n-        bt = T_INT;\n-      } else {\n-        return _klass; \/\/ just return specified klass\n-      }\n-      return ciTypeArrayKlass::make(bt);\n-    }\n-#endif\n-    assert(!el->isa_int(),\n-           \"integral arrays must be pre-equipped with a class\");\n+    assert(!el->isa_int(), \"integral arrays must be pre-equipped with a class\");\n@@ -6437,1 +6416,1 @@\n-    return this_one->_klass->is_subtype_of(other->_klass);\n+    return this_one->klass()->is_subtype_of(other->klass());\n@@ -6469,2 +6448,1 @@\n-    assert(this_one->_klass != nullptr && other->_klass != nullptr, \"\");\n-    return this_one->_klass->equals(other->_klass);\n+    return this_one->klass()->equals(other->klass());\n@@ -6490,1 +6468,1 @@\n-    return other->_klass->equals(ciEnv::current()->Object_klass()) && other->_interfaces->intersection_with(this_one->_interfaces)->eq(other->_interfaces);\n+    return other->klass()->equals(ciEnv::current()->Object_klass()) && other->_interfaces->intersection_with(this_one->_interfaces)->eq(other->_interfaces);\n@@ -6509,1 +6487,1 @@\n-    return this_one->_klass->is_subtype_of(other->_klass);\n+    return this_one->klass()->is_subtype_of(other->klass());\n","filename":"src\/hotspot\/share\/opto\/type.cpp","additions":12,"deletions":34,"binary":false,"changes":46,"status":"modified"},{"patch":"@@ -393,1 +393,4 @@\n-  Copy::fill_to_memory_atomic(p, sz, value);\n+  {\n+    GuardUnsafeAccess guard(thread);\n+    Copy::fill_to_memory_atomic(p, sz, value);\n+  }\n","filename":"src\/hotspot\/share\/prims\/unsafe.cpp","additions":4,"deletions":1,"binary":false,"changes":5,"status":"modified"},{"patch":"@@ -318,1 +318,0 @@\n-  assert((strncmp(property, \"-D\", 2) != 0), \"Unexpected leading -D\");\n@@ -528,7 +527,1 @@\n-  { \"DoReserveCopyInSuperWord\",     JDK_Version::undefined(), JDK_Version::jdk(22), JDK_Version::jdk(23) },\n-  { \"UseCounterDecay\",              JDK_Version::undefined(), JDK_Version::jdk(22), JDK_Version::jdk(23) },\n-\n-#ifdef LINUX\n-  { \"UseHugeTLBFS\",                 JDK_Version::undefined(), JDK_Version::jdk(22), JDK_Version::jdk(23) },\n-  { \"UseSHM\",                       JDK_Version::undefined(), JDK_Version::jdk(22), JDK_Version::jdk(23) },\n-#endif\n+  { \"AdaptiveSizePolicyCollectionCostMargin\",   JDK_Version::undefined(), JDK_Version::jdk(23), JDK_Version::jdk(24) },\n@@ -2225,4 +2218,0 @@\n-#if INCLUDE_CDS\n-      MetaspaceShared::disable_optimized_module_handling();\n-      log_info(cds)(\"optimized module handling: disabled because bootclasspath was appended\");\n-#endif\n@@ -2594,3 +2583,0 @@\n-    \/\/ -Xnoagent\n-    } else if (match_option(option, \"-Xnoagent\")) {\n-      warning(\"Option -Xnoagent was deprecated in JDK 22 and will likely be removed in a future release.\");\n","filename":"src\/hotspot\/share\/runtime\/arguments.cpp","additions":1,"deletions":15,"binary":false,"changes":16,"status":"modified"},{"patch":"@@ -553,1 +553,2 @@\n-          \"Dump heap to file before any major stop-the-world GC\")           \\\n+          \"Dump heap to file before any major stop-the-world GC \"           \\\n+          \"(also see FullGCHeapDumpLimit)\")                                 \\\n@@ -556,1 +557,7 @@\n-          \"Dump heap to file after any major stop-the-world GC\")            \\\n+          \"Dump heap to file after any major stop-the-world GC \"            \\\n+          \"(also see FullGCHeapDumpLimit)\")                                 \\\n+                                                                            \\\n+  product(uint, FullGCHeapDumpLimit, 0, MANAGEABLE,                         \\\n+          \"Limit the number of heap dumps triggered by \"                    \\\n+          \"HeapDumpBeforeFullGC or HeapDumpAfterFullGC \"                    \\\n+          \"(0 means no limit)\")                                             \\\n@@ -877,3 +884,0 @@\n-  develop(bool, TraceICs, false,                                            \\\n-          \"Trace inline cache changes\")                                     \\\n-                                                                            \\\n","filename":"src\/hotspot\/share\/runtime\/globals.hpp","additions":9,"deletions":5,"binary":false,"changes":14,"status":"modified"},{"patch":"@@ -124,4 +124,0 @@\n-#ifndef OM_CACHE_LINE_SIZE\n-\/\/ Use DEFAULT_CACHE_LINE_SIZE if not already specified for\n-\/\/ the current build platform.\n-#endif\n@@ -361,1 +357,0 @@\n-  int       NotRunnable(JavaThread* current, JavaThread* Owner);\n","filename":"src\/hotspot\/share\/runtime\/objectMonitor.hpp","additions":0,"deletions":5,"binary":false,"changes":5,"status":"modified"},{"patch":"@@ -522,2 +522,2 @@\n-        if (mark.is_neutral()) {\n-          assert(!lock_stack.contains(obj()), \"thread must not already hold the lock\");\n+        while (mark.is_neutral()) {\n+          \/\/ Retry until a lock state change has been observed.  cas_set_mark() may collide with non lock bits modifications.\n@@ -525,2 +525,3 @@\n-          markWord locked_mark = mark.set_fast_locked();\n-          markWord old_mark = obj()->cas_set_mark(locked_mark, mark);\n+          assert(!lock_stack.contains(obj()), \"thread must not already hold the lock\");\n+          const markWord locked_mark = mark.set_fast_locked();\n+          const markWord old_mark = obj()->cas_set_mark(locked_mark, mark);\n@@ -532,0 +533,1 @@\n+          mark = old_mark;\n@@ -581,13 +583,7 @@\n-      if (mark.is_fast_locked()) {\n-        markWord unlocked_mark = mark.set_unlocked();\n-        markWord old_mark = object->cas_set_mark(unlocked_mark, mark);\n-        if (old_mark != mark) {\n-          \/\/ Another thread won the CAS, it must have inflated the monitor.\n-          \/\/ It can only have installed an anonymously locked monitor at this point.\n-          \/\/ Fetch that monitor, set owner correctly to this thread, and\n-          \/\/ exit it (allowing waiting threads to enter).\n-          assert(old_mark.has_monitor(), \"must have monitor\");\n-          ObjectMonitor* monitor = old_mark.monitor();\n-          assert(monitor->is_owner_anonymous(), \"must be anonymous owner\");\n-          monitor->set_owner_from_anonymous(current);\n-          monitor->exit(current);\n+      while (mark.is_fast_locked()) {\n+        \/\/ Retry until a lock state change has been observed.  cas_set_mark() may collide with non lock bits modifications.\n+        const markWord unlocked_mark = mark.set_unlocked();\n+        const markWord old_mark = object->cas_set_mark(unlocked_mark, mark);\n+        if (old_mark == mark) {\n+          current->lock_stack().remove(object);\n+          return;\n@@ -595,3 +591,1 @@\n-        LockStack& lock_stack = current->lock_stack();\n-        lock_stack.remove(object);\n-        return;\n+        mark = old_mark;\n@@ -911,7 +905,0 @@\n-\/\/ Can be called from non JavaThreads (e.g., VMThread) for FastHashCode\n-\/\/ calculations as part of JVM\/TI tagging.\n-static bool is_lock_owned(Thread* thread, oop obj) {\n-  assert(LockingMode == LM_LIGHTWEIGHT, \"only call this with new lightweight locking enabled\");\n-  return thread->is_Java_thread() ? JavaThread::cast(thread)->lock_stack().contains(obj) : false;\n-}\n-\n@@ -929,1 +916,1 @@\n-    if (mark.is_neutral()) {               \/\/ if this is a normal header\n+    if (mark.is_neutral() || (LockingMode == LM_LIGHTWEIGHT && mark.is_fast_locked())) {\n@@ -941,0 +928,4 @@\n+      if (LockingMode == LM_LIGHTWEIGHT) {\n+        \/\/ CAS failed, retry\n+        continue;\n+      }\n@@ -972,7 +963,0 @@\n-    } else if (LockingMode == LM_LIGHTWEIGHT && mark.is_fast_locked() && is_lock_owned(current, obj)) {\n-      \/\/ This is a fast-lock owned by the calling thread so use the\n-      \/\/ markWord from the object.\n-      hash = mark.hash();\n-      if (hash != 0) {                  \/\/ if it has a hash, just return it\n-        return hash;\n-      }\n@@ -1308,0 +1292,7 @@\n+\/\/ Can be called from non JavaThreads (e.g., VMThread) for FastHashCode\n+\/\/ calculations as part of JVM\/TI tagging.\n+static bool is_lock_owned(Thread* thread, oop obj) {\n+  assert(LockingMode == LM_LIGHTWEIGHT, \"only call this with new lightweight locking enabled\");\n+  return thread->is_Java_thread() ? JavaThread::cast(thread)->lock_stack().contains(obj) : false;\n+}\n+\n","filename":"src\/hotspot\/share\/runtime\/synchronizer.cpp","additions":26,"deletions":35,"binary":false,"changes":61,"status":"modified"},{"patch":"@@ -123,1 +123,1 @@\n-            OutputAnalyzer out = ProcessTools.executeTestJvm(options);\n+            OutputAnalyzer out = ProcessTools.executeTestJava(options);\n","filename":"test\/hotspot\/jtreg\/gc\/g1\/plab\/TestPLABPromotion.java","additions":1,"deletions":1,"binary":false,"changes":2,"status":"modified"}]}