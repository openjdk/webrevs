{"files":[{"patch":"@@ -61,0 +61,5 @@\n+  if (UseCompactObjectHeaders) {\n+    \/\/ Don't generate anything else and always take the slow-path for now.\n+    return;\n+  }\n+\n","filename":"src\/hotspot\/cpu\/x86\/sharedRuntime_x86.cpp","additions":5,"deletions":0,"binary":false,"changes":5,"status":"modified"},{"patch":"@@ -1301,1 +1301,3 @@\n-        byte_size = source_oop->size() * BytesPerWord;\n+        size_t old_size = source_oop->size();\n+        size_t new_size = source_oop->copy_size_cds(old_size, source_oop->mark());\n+        byte_size = new_size * BytesPerWord;\n","filename":"src\/hotspot\/share\/cds\/archiveBuilder.cpp","additions":3,"deletions":1,"binary":false,"changes":4,"status":"modified"},{"patch":"@@ -420,1 +420,3 @@\n-  size_t byte_size = src_obj->size() * HeapWordSize;\n+  size_t old_size = src_obj->size();\n+  size_t new_size = src_obj->copy_size_cds(old_size, src_obj->mark());\n+  size_t byte_size = new_size * HeapWordSize;\n@@ -441,1 +443,1 @@\n-  memcpy(to, from, byte_size);\n+  memcpy(to, from, old_size * HeapWordSize);\n@@ -577,0 +579,1 @@\n+    assert(fake_oop->mark().narrow_klass() != 0, \"must not be null\");\n@@ -589,1 +592,7 @@\n-      fake_oop->set_mark(markWord::prototype().set_narrow_klass(nk).copy_set_hash(src_hash));\n+      markWord m = markWord::prototype().set_narrow_klass(nk);\n+      m = m.copy_hashctrl_from(src_obj->mark());\n+      fake_oop->set_mark(m);\n+      if (m.is_hashed_not_expanded()) {\n+        fake_oop->initialize_hash_if_necessary(src_obj, src_klass, m);\n+      }\n+      assert(!fake_oop->mark().is_not_hashed_expanded() && !fake_oop->mark().is_hashed_not_expanded(), \"must not be not-hashed-moved and not be hashed-not-moved\");\n@@ -592,0 +601,2 @@\n+      DEBUG_ONLY(intptr_t archived_hash = fake_oop->identity_hash());\n+      assert(src_hash == archived_hash, \"Different hash codes: original \" INTPTR_FORMAT \", archived \" INTPTR_FORMAT, src_hash, archived_hash);\n@@ -594,3 +605,0 @@\n-\n-    DEBUG_ONLY(intptr_t archived_hash = fake_oop->identity_hash());\n-    assert(src_hash == archived_hash, \"Different hash codes: original \" INTPTR_FORMAT \", archived \" INTPTR_FORMAT, src_hash, archived_hash);\n","filename":"src\/hotspot\/share\/cds\/archiveHeapWriter.cpp","additions":14,"deletions":6,"binary":false,"changes":20,"status":"modified"},{"patch":"@@ -381,1 +381,1 @@\n-      oop m = java_lang_Class::create_basic_type_mirror(type2name(bt), bt, CHECK);\n+      oop m = java_lang_Class::create_basic_type_mirror(type2name(bt), bt, true, CHECK);\n@@ -533,0 +533,1 @@\n+  assert(!UseCompactObjectHeaders || scratch_m->mark().is_not_hashed_expanded(), \"scratch mirror must have not-hashed-expanded state\");\n@@ -534,0 +535,1 @@\n+    intptr_t orig_mark = orig_mirror->mark().value();\n@@ -536,2 +538,17 @@\n-      narrowKlass nk = CompressedKlassPointers::encode(orig_mirror->klass());\n-      scratch_m->set_mark(markWord::prototype().set_narrow_klass(nk).copy_set_hash(src_hash));\n+      \/\/ We leave the cases not_hashed\/not_hashed_expanded as they are.\n+      assert(orig_mirror->mark().is_hashed_not_expanded() || orig_mirror->mark().is_hashed_expanded(), \"must be hashed\");\n+      Klass* orig_klass = orig_mirror->klass();\n+      narrowKlass nk = CompressedKlassPointers::encode(orig_klass);\n+      markWord mark = markWord::prototype().set_narrow_klass(nk);\n+      mark = mark.copy_hashctrl_from(orig_mirror->mark());\n+      if (mark.is_hashed_not_expanded()) {\n+        scratch_m->initialize_hash_if_necessary(orig_mirror, orig_klass, mark);\n+      } else {\n+        assert(mark.is_hashed_expanded(), \"must be hashed & moved\");\n+        int offset = orig_klass->hash_offset_in_bytes(orig_mirror);\n+        assert(offset >= 8, \"hash offset must not be in header\");\n+        scratch_m->int_field_put(offset, (jint) src_hash);\n+        scratch_m->set_mark(mark);\n+      }\n+      assert(scratch_m->mark().is_hashed_expanded(), \"must be hashed & moved\");\n+      assert(scratch_m->mark().is_not_hashed_expanded() || scratch_m->mark().is_hashed_expanded(), \"must be not hashed and expanded\");\n@@ -540,0 +557,2 @@\n+      DEBUG_ONLY(intptr_t archived_hash = scratch_m->identity_hash());\n+      assert(src_hash == archived_hash, \"Different hash codes: original \" INTPTR_FORMAT \", archived \" INTPTR_FORMAT, src_hash, archived_hash);\n@@ -542,3 +561,0 @@\n-\n-    DEBUG_ONLY(intptr_t archived_hash = scratch_m->identity_hash());\n-    assert(src_hash == archived_hash, \"Different hash codes: original \" INTPTR_FORMAT \", archived \" INTPTR_FORMAT, src_hash, archived_hash);\n","filename":"src\/hotspot\/share\/cds\/heapShared.cpp","additions":22,"deletions":6,"binary":false,"changes":28,"status":"modified"},{"patch":"@@ -2,1 +2,1 @@\n- * Copyright (c) 1999, 2023, Oracle and\/or its affiliates. All rights reserved.\n+ * Copyrigt (c) 1999, 2023, Oracle and\/or its affiliates. All rights reserved.\n@@ -299,0 +299,4 @@\n+  int hash_offset_in_bytes() const {\n+    return get_instanceKlass()->hash_offset_in_bytes(nullptr);\n+  }\n+\n","filename":"src\/hotspot\/share\/ci\/ciInstanceKlass.hpp","additions":5,"deletions":1,"binary":false,"changes":6,"status":"modified"},{"patch":"@@ -78,0 +78,3 @@\n+  bool is_mirror_instance_klass() { return get_Klass()->is_mirror_instance_klass(); }\n+  bool is_reference_instance_klass() { return get_Klass()->is_reference_instance_klass(); }\n+\n","filename":"src\/hotspot\/share\/ci\/ciKlass.hpp","additions":3,"deletions":0,"binary":false,"changes":3,"status":"modified"},{"patch":"@@ -59,0 +59,3 @@\n+  if (UseCompactObjectHeaders) {\n+    return os::random();\n+  }\n","filename":"src\/hotspot\/share\/classfile\/altHashing.cpp","additions":3,"deletions":0,"binary":false,"changes":3,"status":"modified"},{"patch":"@@ -4889,0 +4889,4 @@\n+int ClassFileParser::hash_offset() const {\n+  return _field_info->_hash_offset;\n+}\n+\n","filename":"src\/hotspot\/share\/classfile\/classFileParser.cpp","additions":4,"deletions":0,"binary":false,"changes":4,"status":"modified"},{"patch":"@@ -75,0 +75,1 @@\n+  int _hash_offset;\n@@ -503,0 +504,1 @@\n+  int hash_offset() const;\n","filename":"src\/hotspot\/share\/classfile\/classFileParser.hpp","additions":2,"deletions":0,"binary":false,"changes":2,"status":"modified"},{"patch":"@@ -212,0 +212,17 @@\n+\/\/ Finds a slot for the identity hash-code.\n+\/\/ Same basic algorithm as above add() method, but simplified\n+\/\/ and does not actually insert the field.\n+int FieldLayout::find_hash_offset() {\n+  LayoutRawBlock* start = this->_start;\n+  LayoutRawBlock* last = last_block();\n+  LayoutRawBlock* cursor = start;\n+  while (cursor != last) {\n+    assert(cursor != nullptr, \"Sanity check\");\n+    if (cursor->kind() == LayoutRawBlock::EMPTY && cursor->fit(4, 1)) {\n+      break;\n+    }\n+    cursor = cursor->next_block();\n+  }\n+  return cursor->offset();\n+}\n+\n@@ -677,0 +694,3 @@\n+  if (UseCompactObjectHeaders) {\n+    _info->_hash_offset   = _layout->find_hash_offset();\n+  }\n","filename":"src\/hotspot\/share\/classfile\/fieldLayoutBuilder.cpp","additions":20,"deletions":0,"binary":false,"changes":20,"status":"modified"},{"patch":"@@ -188,0 +188,1 @@\n+  int find_hash_offset();\n","filename":"src\/hotspot\/share\/classfile\/fieldLayoutBuilder.hpp","additions":1,"deletions":0,"binary":false,"changes":1,"status":"modified"},{"patch":"@@ -1058,1 +1058,1 @@\n-  oop mirror_oop = InstanceMirrorKlass::cast(vmClasses::Class_klass())->allocate_instance(k, CHECK);\n+  oop mirror_oop = InstanceMirrorKlass::cast(vmClasses::Class_klass())->allocate_instance(k, is_scratch, CHECK);\n@@ -1352,1 +1352,1 @@\n-oop java_lang_Class::create_basic_type_mirror(const char* basic_type_name, BasicType type, TRAPS) {\n+oop java_lang_Class::create_basic_type_mirror(const char* basic_type_name, BasicType type, bool is_scratch, TRAPS) {\n@@ -1355,1 +1355,1 @@\n-  oop java_class = InstanceMirrorKlass::cast(vmClasses::Class_klass())->allocate_instance(nullptr, CHECK_NULL);\n+  oop java_class = InstanceMirrorKlass::cast(vmClasses::Class_klass())->allocate_instance(nullptr, is_scratch, CHECK_NULL);\n","filename":"src\/hotspot\/share\/classfile\/javaClasses.cpp","additions":3,"deletions":3,"binary":false,"changes":6,"status":"modified"},{"patch":"@@ -283,1 +283,1 @@\n-  static oop  create_basic_type_mirror(const char* basic_type_name, BasicType type, TRAPS);\n+  static oop  create_basic_type_mirror(const char* basic_type_name, BasicType type, bool is_scratch, TRAPS);\n","filename":"src\/hotspot\/share\/classfile\/javaClasses.hpp","additions":1,"deletions":1,"binary":false,"changes":2,"status":"modified"},{"patch":"@@ -37,1 +37,0 @@\n-#include \"gc\/shared\/fullGCForwarding.hpp\"\n@@ -247,2 +246,0 @@\n-\n-  FullGCForwarding::initialize_flags(heap_reserved_size_bytes());\n","filename":"src\/hotspot\/share\/gc\/g1\/g1Arguments.cpp","additions":0,"deletions":3,"binary":false,"changes":3,"status":"modified"},{"patch":"@@ -215,0 +215,2 @@\n+  FullGCForwarding::begin();\n+\n@@ -227,0 +229,2 @@\n+  FullGCForwarding::end();\n+\n","filename":"src\/hotspot\/share\/gc\/g1\/g1FullCollector.cpp","additions":4,"deletions":0,"binary":false,"changes":4,"status":"modified"},{"patch":"@@ -63,0 +63,1 @@\n+  assert(obj_addr != destination, \"only copy actually-moving objects\");\n@@ -67,0 +68,1 @@\n+  cast_to_oop(destination)->initialize_hash_if_necessary(obj);\n","filename":"src\/hotspot\/share\/gc\/g1\/g1FullGCCompactTask.cpp","additions":2,"deletions":0,"binary":false,"changes":2,"status":"modified"},{"patch":"@@ -100,0 +100,4 @@\n+  size_t old_size = size;\n+  size_t new_size = object->copy_size(old_size, object->mark());\n+  size = cast_from_oop<HeapWord*>(object) != _compaction_top ? new_size : old_size;\n+\n@@ -103,0 +107,1 @@\n+    size = cast_from_oop<HeapWord*>(object) != _compaction_top ? new_size : old_size;\n@@ -157,2 +162,3 @@\n-  size_t obj_size = obj->size();\n-  uint num_regions = (uint)G1CollectedHeap::humongous_obj_size_in_regions(obj_size);\n+  size_t old_size = obj->size();\n+  size_t new_size = obj->copy_size(old_size, obj->mark());\n+  uint num_regions = (uint)G1CollectedHeap::humongous_obj_size_in_regions(new_size);\n@@ -176,0 +182,1 @@\n+  assert(hr->bottom() != dest_hr->bottom(), \"assuming actual humongous move\");\n","filename":"src\/hotspot\/share\/gc\/g1\/g1FullGCCompactionPoint.cpp","additions":9,"deletions":2,"binary":false,"changes":11,"status":"modified"},{"patch":"@@ -481,1 +481,2 @@\n-  const size_t word_sz = old->size_given_klass(klass);\n+  const size_t old_size = old->size_given_mark_and_klass(old_mark, klass);\n+  const size_t word_sz = old->copy_size(old_size, old_mark);\n@@ -519,1 +520,1 @@\n-  Copy::aligned_disjoint_words(cast_from_oop<HeapWord*>(old), obj_ptr, word_sz);\n+  Copy::aligned_disjoint_words(cast_from_oop<HeapWord*>(old), obj_ptr, old_size);\n@@ -536,0 +537,2 @@\n+    obj->initialize_hash_if_necessary(old);\n+\n","filename":"src\/hotspot\/share\/gc\/g1\/g1ParScanThreadState.cpp","additions":5,"deletions":2,"binary":false,"changes":7,"status":"modified"},{"patch":"@@ -30,1 +30,0 @@\n-#include \"gc\/shared\/fullGCForwarding.hpp\"\n@@ -86,2 +85,0 @@\n-\n-  FullGCForwarding::initialize_flags(heap_reserved_size_bytes());\n","filename":"src\/hotspot\/share\/gc\/parallel\/parallelArguments.cpp","additions":0,"deletions":3,"binary":false,"changes":3,"status":"modified"},{"patch":"@@ -1052,0 +1052,2 @@\n+    FullGCForwarding::begin();\n+\n@@ -1058,0 +1060,2 @@\n+    FullGCForwarding::end();\n+\n","filename":"src\/hotspot\/share\/gc\/parallel\/psParallelCompact.cpp","additions":4,"deletions":0,"binary":false,"changes":4,"status":"modified"},{"patch":"@@ -180,1 +180,2 @@\n-  size_t new_obj_size = o->size_given_klass(klass);\n+  size_t old_obj_size = o->size_given_mark_and_klass(test_mark, klass);\n+  size_t new_obj_size = o->copy_size(old_obj_size, test_mark);\n","filename":"src\/hotspot\/share\/gc\/parallel\/psPromotionManager.inline.hpp","additions":2,"deletions":1,"binary":false,"changes":3,"status":"modified"},{"patch":"@@ -740,1 +740,3 @@\n-  size_t s = old->size();\n+  size_t old_size = old->size();\n+  size_t s = old->copy_size(old_size, old->mark());\n+\n@@ -765,1 +767,1 @@\n-  Copy::aligned_disjoint_words(cast_from_oop<HeapWord*>(old), cast_from_oop<HeapWord*>(obj), s);\n+  Copy::aligned_disjoint_words(cast_from_oop<HeapWord*>(old), cast_from_oop<HeapWord*>(obj), old_size);\n@@ -775,0 +777,2 @@\n+  obj->initialize_hash_if_necessary(old);\n+\n","filename":"src\/hotspot\/share\/gc\/serial\/defNewGeneration.cpp","additions":6,"deletions":2,"binary":false,"changes":8,"status":"modified"},{"patch":"@@ -26,2 +26,0 @@\n-#include \"gc\/shared\/fullGCForwarding.hpp\"\n-#include \"gc\/shared\/gcArguments.hpp\"\n@@ -31,5 +29,0 @@\n-void SerialArguments::initialize() {\n-  GCArguments::initialize();\n-  FullGCForwarding::initialize_flags(MaxHeapSize);\n-}\n-\n","filename":"src\/hotspot\/share\/gc\/serial\/serialArguments.cpp","additions":0,"deletions":7,"binary":false,"changes":7,"status":"modified"},{"patch":"@@ -34,1 +34,0 @@\n-  virtual void initialize();\n","filename":"src\/hotspot\/share\/gc\/serial\/serialArguments.hpp","additions":0,"deletions":1,"binary":false,"changes":1,"status":"modified"},{"patch":"@@ -194,1 +194,2 @@\n-  HeapWord* alloc(size_t words) {\n+  HeapWord* alloc(size_t old_size, size_t new_size, HeapWord* old_obj) {\n+    size_t words = (old_obj == _spaces[_index]._compaction_top) ? old_size : new_size;\n@@ -210,0 +211,1 @@\n+      words = (old_obj == _spaces[_index]._compaction_top) ? old_size : new_size;\n@@ -261,2 +263,0 @@\n-    assert(addr != new_addr, \"inv\");\n-    prefetch_write_copy(new_addr);\n@@ -265,1 +265,4 @@\n-    Copy::aligned_conjoint_words(addr, new_addr, obj_size);\n+    if (addr != new_addr) {\n+      prefetch_write_copy(new_addr);\n+      Copy::aligned_conjoint_words(addr, new_addr, obj_size);\n+    }\n@@ -267,0 +270,3 @@\n+    if (addr != new_addr) {\n+      new_obj->initialize_hash_if_necessary(obj);\n+    }\n@@ -302,0 +308,1 @@\n+        size_t new_size = obj->copy_size(obj_size, obj->mark());\n@@ -303,1 +310,1 @@\n-          HeapWord* new_addr = alloc(obj_size);\n+          HeapWord* new_addr = alloc(obj_size, new_size, cur_addr);\n@@ -305,0 +312,1 @@\n+          assert(obj->size() == obj_size, \"size must not change after forwarding\");\n@@ -311,1 +319,2 @@\n-            alloc(pointer_delta(next_live_addr, cur_addr));\n+            size_t size = pointer_delta(next_live_addr, cur_addr);\n+            alloc(size, size, cur_addr);\n@@ -597,1 +606,1 @@\n-  obj->set_mark(obj->prototype_mark().set_marked());\n+  obj->set_mark(mark.set_marked());\n@@ -700,0 +709,2 @@\n+  FullGCForwarding::begin();\n+\n@@ -743,0 +754,2 @@\n+  FullGCForwarding::end();\n+\n","filename":"src\/hotspot\/share\/gc\/serial\/serialFullGC.cpp","additions":20,"deletions":7,"binary":false,"changes":27,"status":"modified"},{"patch":"@@ -394,1 +394,1 @@\n-  assert(obj_size == obj->size(), \"bad obj_size passed in\");\n+  assert(obj_size == obj->size() || UseCompactObjectHeaders, \"bad obj_size passed in\");\n","filename":"src\/hotspot\/share\/gc\/serial\/tenuredGeneration.cpp","additions":1,"deletions":1,"binary":false,"changes":2,"status":"modified"},{"patch":"@@ -285,1 +285,1 @@\n-  oop class_allocate(Klass* klass, size_t size, TRAPS);\n+  oop class_allocate(Klass* klass, size_t size, size_t base_size, TRAPS);\n","filename":"src\/hotspot\/share\/gc\/shared\/collectedHeap.hpp","additions":1,"deletions":1,"binary":false,"changes":2,"status":"modified"},{"patch":"@@ -44,2 +44,2 @@\n-inline oop CollectedHeap::class_allocate(Klass* klass, size_t size, TRAPS) {\n-  ClassAllocator allocator(klass, size, THREAD);\n+inline oop CollectedHeap::class_allocate(Klass* klass, size_t size, size_t base_size, TRAPS) {\n+  ClassAllocator allocator(klass, size, base_size, THREAD);\n","filename":"src\/hotspot\/share\/gc\/shared\/collectedHeap.inline.hpp","additions":2,"deletions":2,"binary":false,"changes":4,"status":"modified"},{"patch":"@@ -27,2 +27,6 @@\n-#include \"memory\/memRegion.hpp\"\n-#include \"runtime\/globals_extension.hpp\"\n+#include \"logging\/log.hpp\"\n+#include \"nmt\/memTag.hpp\"\n+#include \"utilities\/ostream.hpp\"\n+#include \"utilities\/concurrentHashTable.inline.hpp\"\n+#include \"utilities\/fastHash.hpp\"\n+#include \"utilities\/powerOfTwo.hpp\"\n@@ -30,2 +34,5 @@\n-HeapWord* FullGCForwarding::_heap_base = nullptr;\n-int FullGCForwarding::_num_low_bits = 0;\n+static uintx hash(HeapWord* const& addr) {\n+  uint64_t val = reinterpret_cast<uint64_t>(addr);\n+  uint32_t hash = FastHash::get_hash32((uint32_t)val, (uint32_t)(val >> 32));\n+  return hash;\n+}\n@@ -33,11 +40,29 @@\n-void FullGCForwarding::initialize_flags(size_t max_heap_size) {\n-#ifdef _LP64\n-  size_t max_narrow_heap_size = right_n_bits(NumLowBitsNarrow - Shift);\n-  if (UseCompactObjectHeaders && max_heap_size > max_narrow_heap_size * HeapWordSize) {\n-    warning(\"Compact object headers require a java heap size smaller than \" SIZE_FORMAT\n-            \"%s (given: \" SIZE_FORMAT \"%s). Disabling compact object headers.\",\n-            byte_size_in_proper_unit(max_narrow_heap_size * HeapWordSize),\n-            proper_unit_for_byte_size(max_narrow_heap_size * HeapWordSize),\n-            byte_size_in_proper_unit(max_heap_size),\n-            proper_unit_for_byte_size(max_heap_size));\n-    FLAG_SET_ERGO(UseCompactObjectHeaders, false);\n+struct ForwardingEntry {\n+  HeapWord* _from;\n+  HeapWord* _to;\n+  ForwardingEntry(HeapWord* from, HeapWord* to) : _from(from), _to(to) {}\n+};\n+\n+struct FallbackTableConfig {\n+  using Value = ForwardingEntry;\n+  static uintx get_hash(Value const& entry, bool* is_dead) {\n+    return hash(entry._from);\n+  }\n+  static void* allocate_node(void* context, size_t size, Value const& value) {\n+    return AllocateHeap(size, mtGC);\n+  }\n+  static void free_node(void* context, void* memory, Value const& value) {\n+    FreeHeap(memory);\n+  }\n+};\n+\n+class FallbackTable : public ConcurrentHashTable<FallbackTableConfig, mtGC> {\n+\n+};\n+\n+class FallbackTableLookup : public StackObj {\n+  ForwardingEntry const _entry;\n+public:\n+  explicit FallbackTableLookup(HeapWord* from) : _entry(from, nullptr) {}\n+  uintx get_hash() const {\n+    return hash(_entry._from);\n@@ -45,0 +70,20 @@\n+  bool equals(ForwardingEntry* value) {\n+    return _entry._from == value->_from;\n+  }\n+  bool is_dead(ForwardingEntry* value) { return false; }\n+};\n+\n+\/\/ We cannot use 0, because that may already be a valid base address in zero-based heaps.\n+\/\/ 0x1 is safe because heap base addresses must be aligned by much larger alignment\n+HeapWord* const FullGCForwarding::UNUSED_BASE = reinterpret_cast<HeapWord*>(0x1);\n+\n+HeapWord* FullGCForwarding::_heap_start = nullptr;\n+size_t FullGCForwarding::_heap_start_region_bias = 0;\n+size_t FullGCForwarding::_num_regions = 0;\n+uintptr_t FullGCForwarding::_region_mask = 0;\n+HeapWord** FullGCForwarding::_biased_bases = nullptr;\n+HeapWord** FullGCForwarding::_bases_table = nullptr;\n+FallbackTable* FullGCForwarding::_fallback_table = nullptr;\n+#ifndef PRODUCT\n+volatile uint64_t FullGCForwarding::_num_forwardings = 0;\n+volatile uint64_t FullGCForwarding::_num_fallback_forwardings = 0;\n@@ -46,1 +91,0 @@\n-}\n@@ -50,5 +94,32 @@\n-  _heap_base = heap.start();\n-  if (UseCompactObjectHeaders) {\n-    _num_low_bits = NumLowBitsNarrow;\n-  } else {\n-    _num_low_bits = NumLowBitsWide;\n+  _heap_start = heap.start();\n+\n+  size_t rounded_heap_size = round_up_power_of_2(heap.byte_size());\n+\n+  _num_regions = (rounded_heap_size \/ BytesPerWord) \/ BLOCK_SIZE_WORDS;\n+\n+  _heap_start_region_bias = (uintptr_t)_heap_start >> BLOCK_SIZE_BYTES_SHIFT;\n+  _region_mask = ~((uintptr_t(1) << BLOCK_SIZE_BYTES_SHIFT) - 1);\n+\n+  assert(_bases_table == nullptr, \"should not be initialized yet\");\n+  assert(_fallback_table == nullptr, \"should not be initialized yet\");\n+#endif\n+}\n+\n+void FullGCForwarding::begin() {\n+#ifdef _LP64\n+  assert(_bases_table == nullptr, \"should not be initialized yet\");\n+  assert(_fallback_table == nullptr, \"should not be initialized yet\");\n+\n+  _fallback_table = new FallbackTable();\n+\n+#ifndef PRODUCT\n+  _num_forwardings = 0;\n+  _num_fallback_forwardings = 0;\n+#endif\n+\n+  size_t max = _num_regions;\n+  _bases_table = NEW_C_HEAP_ARRAY(HeapWord*, max, mtGC);\n+  HeapWord** biased_start = _bases_table - _heap_start_region_bias;\n+  _biased_bases = biased_start;\n+  for (size_t i = 0; i < max; i++) {\n+    _bases_table[i] = UNUSED_BASE;\n@@ -58,0 +129,59 @@\n+\n+void FullGCForwarding::end() {\n+#ifndef PRODUCT\n+  log_info(gc)(\"Total forwardings: \" UINT64_FORMAT \", fallback forwardings: \" UINT64_FORMAT\n+                \", ratio: %f, memory used by fallback table: \" SIZE_FORMAT \"%s, memory used by bases table: \" SIZE_FORMAT \"%s\",\n+               _num_forwardings, _num_fallback_forwardings, (float)_num_forwardings\/(float)_num_fallback_forwardings,\n+               byte_size_in_proper_unit(_fallback_table->get_mem_size(Thread::current())),\n+               proper_unit_for_byte_size(_fallback_table->get_mem_size(Thread::current())),\n+               byte_size_in_proper_unit(sizeof(HeapWord*) * _num_regions),\n+               proper_unit_for_byte_size(sizeof(HeapWord*) * _num_regions));\n+#endif\n+#ifdef _LP64\n+  assert(_bases_table != nullptr, \"should be initialized\");\n+  FREE_C_HEAP_ARRAY(HeapWord*, _bases_table);\n+  _bases_table = nullptr;\n+  delete _fallback_table;\n+  _fallback_table = nullptr;\n+#endif\n+}\n+\n+void FullGCForwarding::fallback_forward_to(HeapWord* from, HeapWord* to) {\n+  assert(to != nullptr, \"no null forwarding\");\n+  assert(_fallback_table != nullptr, \"should be initialized\");\n+  FallbackTableLookup lookup_f(from);\n+  ForwardingEntry entry(from, to);\n+  auto found_f = [&](ForwardingEntry* found) {\n+    \/\/ If dupe has been found, override it with new value.\n+    \/\/ This is also called when new entry is succussfully inserted.\n+    if (found->_to != to) {\n+      found->_to = to;\n+    }\n+  };\n+  Thread* current_thread = Thread::current();\n+  bool grow;\n+  bool added = _fallback_table->insert_get(current_thread, lookup_f, entry, found_f, &grow);\n+  NOT_PRODUCT(Atomic::inc(&_num_fallback_forwardings);)\n+#ifdef ASSERT\n+  assert(fallback_forwardee(from) != nullptr, \"must have entered forwarding\");\n+  assert(fallback_forwardee(from) == to, \"forwarding must be correct, added: %s, from: \" PTR_FORMAT \", to: \" PTR_FORMAT \", fwd: \" PTR_FORMAT, BOOL_TO_STR(added), p2i(from), p2i(to), p2i(fallback_forwardee(from)));\n+#endif\n+  if (grow) {\n+    _fallback_table->grow(current_thread);\n+    tty->print_cr(\"grow fallback table to size: \" SIZE_FORMAT \" bytes\",\n+                  _fallback_table->get_mem_size(current_thread));\n+  }\n+}\n+\n+HeapWord* FullGCForwarding::fallback_forwardee(HeapWord* from) {\n+  assert(_fallback_table != nullptr, \"fallback table must be present\");\n+  HeapWord* result;\n+  FallbackTableLookup lookup_f(from);\n+  auto found_f = [&](ForwardingEntry* found) {\n+    result = found->_to;\n+  };\n+  bool found = _fallback_table->get(Thread::current(), lookup_f, found_f);\n+  assert(found, \"something must have been found\");\n+  assert(result != nullptr, \"must have found forwarding\");\n+  return result;\n+}\n","filename":"src\/hotspot\/share\/gc\/shared\/fullGCForwarding.cpp","additions":151,"deletions":21,"binary":false,"changes":172,"status":"modified"},{"patch":"@@ -28,1 +28,1 @@\n-#include \"memory\/allStatic.hpp\"\n+#include \"memory\/allocation.hpp\"\n@@ -33,10 +33,85 @@\n-\/*\n- * Implements forwarding for the Full GCs of Serial, Parallel, G1 and Shenandoah in\n- * a way that preserves upper N bits of object mark-words, which contain crucial\n- * Klass* information when running with compact headers. The encoding is similar to\n- * compressed-oops encoding: it basically subtracts the forwardee address from the\n- * heap-base, shifts that difference into the right place, and sets the lowest two\n- * bits (to indicate 'forwarded' state as usual).\n- * With compact-headers, we have 40 bits to encode forwarding pointers. This is\n- * enough to address 8TB of heap. If the heap size exceeds that limit, we turn off\n- * compact headers.\n+class FallbackTable;\n+class Mutex;\n+\n+\/**\n+ * FullGCForwarding is a method to store forwarding information in a compressed form into the object header,\n+ * that has been specifically designed for sliding compacting GCs and compact object headers. With compact object\n+ * headers, we store the compressed class pointer in the header, which would be overwritten by full forwarding\n+ * pointers, if we allow the legacy forwarding code to act. This would lose the class information for the object,\n+ * which is required later in GC cycle to iterate the reference fields and get the object size for copying.\n+ *\n+ * FullGCForwarding requires only small side tables and guarantees constant-time access and modification.\n+ *\n+ * The key advantage of sliding compaction for encoding efficiency:\n+ * - It forwards objects linearily, starting at the heap bottom and moving up to the top, sliding\n+ *   live objects towards the bottom of the heap. (The reality in parallel or regionalized GCs is a bit more\n+ *   complex, but conceptually it is the same.)\n+ * - Objects starting in any one block can only be forwarded to a memory region that is not larger than\n+ *   a block. (There are exceptions to this rule which are discussed below.)\n+ *\n+ * This is an intuitive property: when we slide the compact block full of data, it can not take up more\n+ * memory afterwards.\n+ * This property allows us to use a side table to record the addresses of the target memory region for\n+ * each block. The table holds N entries for N blocks. For each block, it gives the base\n+ * address of the target regions, or a special placeholder if not used.\n+ *\n+ * This encoding efficiency allows to store the forwarding information in the object header _together_ with the\n+ * compressed class pointer.\n+ *\n+ * The idea is to use a pointer compression scheme very similar to the one that is used for compressed oops.\n+ * We divide the heap into number of equal-sized blocks. Each block spans a maximum of 2^NUM_OFFSET_BITS words.\n+ * We maintain a side-table of target-base-addresses, with one address entry per block.\n+ *\n+ * When recording the sliding forwarding, the mark word would look roughly like this:\n+ *\n+ *   32                               0\n+ *    [.....................OOOOOOOOOTT]\n+ *                                    ^------ tag-bits, indicates 'forwarded'\n+ *                                  ^-------- in-region offset\n+ *                         ^----------------- protected area, *not touched* by this code, useful for\n+ *                                            compressed class pointer with compact object headers\n+ *\n+ * Adding a forwarding then generally works as follows:\n+ *   1. Compute the index of the block of the \"from\" address.\n+ *   2. Load the target-base-offset of the from-block from the side-table.\n+ *   3. If the base-offset is not-yet set, set it to the to-address of the forwarding.\n+ *      (In other words, the first forwarding of a block determines the target base-offset.)\n+ *   4. Compute the offset of the to-address in the target region.\n+ *   4. Store offset in the object header.\n+ *\n+ * Similarly, looking up the target address, given an original object address generally works as follows:\n+ *   1. Compute the index of the block of the \"from\" address.\n+ *   2. Load the target-base-offset of the from-block from the side-table.\n+ *   3. Extract the offset from the object header.\n+ *   4. Compute the \"to\" address from \"to\" region base and \"offset\"\n+ *\n+ * We reserve one special value for the offset:\n+ *  - 111111111: Indicates an exceptional forwarding (see below), for which a fallback hash-table\n+ *               is used to look up the target address.\n+ *\n+ * In order to support this, we need to make a change to the above algorithm:\n+ *  - Forwardings that would use offsets >= 111111111 (i.e. the last slot)\n+ *    would also need to use the fallback-table. We expect that to be relatively rare for two reasons:\n+ *    1. It only affects 1 out of 512 possible offsets, in other words, 1\/512th of all situations in an equal\n+ *       distribution.\n+ *    2. Forwardings are not equally-distributed, because normally we 'skip' unreachable objects,\n+ *       thus compacting the block. Forwardings tend to cluster at the beginning of the target region,\n+ *       and become less likely towards the end of the possible encodable target address range.\n+ *       Which means in reality it will be much less frequent than 1\/512.\n+ *\n+ * There are several conditions when the above algorithm would be broken because the assumption that\n+ * 'objects from each block can only get forwarded to a region of block-size' is violated:\n+ * - G1 last-ditch serial compaction: there, object from a single region can be forwarded to multiple,\n+ *   more than two regions. G1 serial compaction is not very common - it is the last-last-ditch GC\n+ *   that is used when the JVM is scrambling to squeeze more space out of the heap, and at that point,\n+ *   ultimate performance is no longer the main concern.\n+ * - When forwarding hits a space (or G1\/Shenandoah region) boundary, then latter objects of a block\n+ *   need to be forwarded to a different address range than earlier objects in the same block.\n+ *   This is rare.\n+ * - With compact identity hash-code, objects can grow, and in the worst case use up more memory in\n+ *   the target block than we can address. We expect that to be rare.\n+ *\n+ * To deal with that, we initialize a fallback-hashtable for storing those extra forwardings, and use a special\n+ * offset pattern (0b11...1) to indicate that the forwardee is not encoded but should be looked-up in the hashtable.\n+ * This implies that this particular offset (the last word of a block) can not be used directly as forwarding,\n+ * but also has to be handled by the fallback-table.\n@@ -45,3 +120,54 @@\n-  static const int NumLowBitsNarrow = LP64_ONLY(markWord::klass_shift) NOT_LP64(0 \/*unused*\/);\n-  static const int NumLowBitsWide   = BitsPerWord;\n-  static const int Shift            = markWord::lock_bits + markWord::lock_shift;\n+private:\n+  static constexpr int AVAILABLE_LOW_BITS       = 11;\n+  static constexpr int AVAILABLE_BITS_MASK      = right_n_bits(AVAILABLE_LOW_BITS);\n+  \/\/ The offset bits start after the lock-bits, which are currently used by Serial GC\n+  \/\/ for marking objects. Could be 1 for Serial GC when being clever with the bits,\n+  \/\/ and 0 for all other GCs.\n+  static constexpr int OFFSET_BITS_SHIFT = markWord::lock_shift + markWord::lock_bits;\n+\n+  \/\/ How many bits we use for the offset\n+  static constexpr int NUM_OFFSET_BITS = AVAILABLE_LOW_BITS - OFFSET_BITS_SHIFT;\n+  static constexpr size_t BLOCK_SIZE_WORDS = 1 << NUM_OFFSET_BITS;\n+  static constexpr int BLOCK_SIZE_BYTES_SHIFT = NUM_OFFSET_BITS + LogHeapWordSize;\n+  static constexpr size_t MAX_OFFSET = BLOCK_SIZE_WORDS - 2;\n+  static constexpr uintptr_t OFFSET_MASK = right_n_bits(NUM_OFFSET_BITS) << OFFSET_BITS_SHIFT;\n+\n+  \/\/ This offset bit-pattern indicates that the actual mapping is handled by the\n+  \/\/ fallback-table. This also implies that this cannot be used as a valid offset,\n+  \/\/ and we must also use the fallback-table for mappings to the last word of a\n+  \/\/ block.\n+  static constexpr uintptr_t FALLBACK_PATTERN = right_n_bits(NUM_OFFSET_BITS);\n+  static constexpr uintptr_t FALLBACK_PATTERN_IN_PLACE = FALLBACK_PATTERN << OFFSET_BITS_SHIFT;\n+\n+  \/\/ Indicates an unused base address in the target base table.\n+  static HeapWord* const UNUSED_BASE;\n+\n+  static HeapWord*      _heap_start;\n+\n+  static size_t         _heap_start_region_bias;\n+  static size_t         _num_regions;\n+  static uintptr_t      _region_mask;\n+\n+  \/\/ The target base table memory.\n+  static HeapWord**     _bases_table;\n+  \/\/ Entries into the target base tables, biased to the start of the heap.\n+  static HeapWord**     _biased_bases;\n+\n+  static FallbackTable* _fallback_table;\n+\n+#ifndef PRODUCT\n+  static volatile uint64_t _num_forwardings;\n+  static volatile uint64_t _num_fallback_forwardings;\n+#endif\n+\n+  static inline size_t biased_region_index_containing(HeapWord* addr);\n+\n+  static inline bool is_fallback(uintptr_t encoded);\n+  static inline uintptr_t encode_forwarding(HeapWord* from, HeapWord* to);\n+  static inline HeapWord* decode_forwarding(HeapWord* from, uintptr_t encoded);\n+\n+  static void fallback_forward_to(HeapWord* from, HeapWord* to);\n+  static HeapWord* fallback_forwardee(HeapWord* from);\n+\n+  static inline void forward_to_impl(oop from, oop to);\n+  static inline oop forwardee_impl(oop from);\n@@ -49,2 +175,0 @@\n-  static HeapWord* _heap_base;\n-  static int _num_low_bits;\n@@ -52,1 +176,0 @@\n-  static void initialize_flags(size_t max_heap_size);\n@@ -54,0 +177,7 @@\n+\n+  static void begin();\n+  static void end();\n+\n+  static inline bool is_forwarded(oop obj);\n+  static inline bool is_not_forwarded(oop obj);\n+\n@@ -56,1 +186,0 @@\n-  static inline bool is_forwarded(oop obj);\n","filename":"src\/hotspot\/share\/gc\/shared\/fullGCForwarding.hpp","additions":147,"deletions":18,"binary":false,"changes":165,"status":"modified"},{"patch":"@@ -22,1 +22,0 @@\n- *\n@@ -25,2 +24,2 @@\n-#ifndef GC_SHARED_FULLGCFORWARDING_INLINE_HPP\n-#define GC_SHARED_FULLGCFORWARDING_INLINE_HPP\n+#ifndef SHARE_GC_SHARED_FULLGCFORWARDING_INLINE_HPP\n+#define SHARE_GC_SHARED_FULLGCFORWARDING_INLINE_HPP\n@@ -28,0 +27,1 @@\n+#include \"gc\/shared\/gc_globals.hpp\"\n@@ -29,1 +29,1 @@\n-\n+#include \"oops\/markWord.hpp\"\n@@ -31,1 +31,36 @@\n-#include \"utilities\/globalDefinitions.hpp\"\n+#include \"utilities\/macros.hpp\"\n+\n+inline bool FullGCForwarding::is_forwarded(oop obj) {\n+  return obj->is_forwarded();\n+}\n+\n+size_t FullGCForwarding::biased_region_index_containing(HeapWord* addr) {\n+  return (uintptr_t)addr >> BLOCK_SIZE_BYTES_SHIFT;\n+}\n+\n+bool FullGCForwarding::is_fallback(uintptr_t encoded) {\n+  return (encoded & OFFSET_MASK) == FALLBACK_PATTERN_IN_PLACE;\n+}\n+\n+uintptr_t FullGCForwarding::encode_forwarding(HeapWord* from, HeapWord* to) {\n+  size_t from_block_idx = biased_region_index_containing(from);\n+\n+  HeapWord* to_region_base = _biased_bases[from_block_idx];\n+  if (to_region_base == UNUSED_BASE) {\n+    _biased_bases[from_block_idx] = to_region_base = to;\n+  }\n+\n+  \/\/ Avoid pointer_delta() on purpose: using an unsigned subtraction,\n+  \/\/ we get an underflow when to < to_region_base, which means\n+  \/\/ we can use a single comparison instead of:\n+  \/\/ if (to_region_base > to || (to - to_region_base) > MAX_OFFSET) { .. }\n+  size_t offset = size_t(to - to_region_base);\n+  if (offset > MAX_OFFSET) {\n+    offset = FALLBACK_PATTERN;\n+  }\n+  uintptr_t encoded = (offset << OFFSET_BITS_SHIFT) | markWord::marked_value;\n+\n+  assert(is_fallback(encoded) || to == decode_forwarding(from, encoded), \"must be reversible\");\n+  assert((encoded & ~AVAILABLE_BITS_MASK) == 0, \"must encode to available bits\");\n+  return encoded;\n+}\n@@ -33,1 +68,34 @@\n-void FullGCForwarding::forward_to(oop from, oop to) {\n+HeapWord* FullGCForwarding::decode_forwarding(HeapWord* from, uintptr_t encoded) {\n+  assert(!is_fallback(encoded), \"must not be fallback-forwarded, encoded: \" INTPTR_FORMAT \", OFFSET_MASK: \" INTPTR_FORMAT \", FALLBACK_PATTERN_IN_PLACE: \" INTPTR_FORMAT, encoded, OFFSET_MASK, FALLBACK_PATTERN_IN_PLACE);\n+  assert((encoded & ~AVAILABLE_BITS_MASK) == 0, \"must decode from available bits, encoded: \" INTPTR_FORMAT, encoded);\n+  uintptr_t offset = (encoded >> OFFSET_BITS_SHIFT);\n+\n+  size_t from_idx = biased_region_index_containing(from);\n+  HeapWord* base = _biased_bases[from_idx];\n+  assert(base != UNUSED_BASE, \"must not be unused base: encoded: \" INTPTR_FORMAT, encoded);\n+  HeapWord* decoded = base + offset;\n+  assert(decoded >= _heap_start,\n+         \"Address must be above heap start. encoded: \" INTPTR_FORMAT \", base: \" PTR_FORMAT,\n+          encoded, p2i(base));\n+\n+  return decoded;\n+}\n+\n+inline void FullGCForwarding::forward_to_impl(oop from, oop to) {\n+  assert(_bases_table != nullptr, \"call begin() before forwarding\");\n+\n+  markWord from_header = from->mark();\n+  HeapWord* from_hw = cast_from_oop<HeapWord*>(from);\n+  HeapWord* to_hw   = cast_from_oop<HeapWord*>(to);\n+  uintptr_t encoded = encode_forwarding(from_hw, to_hw);\n+  markWord new_header = markWord((from_header.value() & ~OFFSET_MASK) | encoded);\n+  from->set_mark(new_header);\n+\n+  if (is_fallback(encoded)) {\n+    fallback_forward_to(from_hw, to_hw);\n+  }\n+  NOT_PRODUCT(Atomic::inc(&_num_forwardings);)\n+}\n+\n+inline void FullGCForwarding::forward_to(oop obj, oop fwd) {\n+  assert(fwd != nullptr, \"no null forwarding\");\n@@ -35,6 +103,3 @@\n-  uintptr_t encoded = pointer_delta(cast_from_oop<HeapWord*>(to), _heap_base) << Shift;\n-  assert(encoded <= static_cast<uintptr_t>(right_n_bits(_num_low_bits)), \"encoded forwardee must fit\");\n-  uintptr_t mark = from->mark().value();\n-  mark &= ~right_n_bits(_num_low_bits);\n-  mark |= (encoded | markWord::marked_value);\n-  from->set_mark(markWord(mark));\n+  assert(_bases_table != nullptr, \"expect sliding forwarding initialized\");\n+  forward_to_impl(obj, fwd);\n+  assert(forwardee(obj) == fwd, \"must be forwarded to correct forwardee, obj: \" PTR_FORMAT \", forwardee(obj): \" PTR_FORMAT \", fwd: \" PTR_FORMAT \", mark: \" INTPTR_FORMAT, p2i(obj), p2i(forwardee(obj)), p2i(fwd), obj->mark().value());\n@@ -42,1 +107,1 @@\n-  from->forward_to(to);\n+  obj->forward_to(fwd);\n@@ -46,1 +111,15 @@\n-oop FullGCForwarding::forwardee(oop from) {\n+inline oop FullGCForwarding::forwardee_impl(oop from) {\n+  assert(_bases_table != nullptr, \"call begin() before asking for forwarding\");\n+\n+  markWord header = from->mark();\n+  HeapWord* from_hw = cast_from_oop<HeapWord*>(from);\n+  if (is_fallback(header.value())) {\n+    HeapWord* to = fallback_forwardee(from_hw);\n+    return cast_to_oop(to);\n+  }\n+  uintptr_t encoded = header.value() & OFFSET_MASK;\n+  HeapWord* to = decode_forwarding(from_hw, encoded);\n+  return cast_to_oop(to);\n+}\n+\n+inline oop FullGCForwarding::forwardee(oop obj) {\n@@ -48,3 +127,2 @@\n-  uintptr_t mark = from->mark().value();\n-  HeapWord* decoded = _heap_base + ((mark & right_n_bits(_num_low_bits)) >> Shift);\n-  return cast_to_oop(decoded);\n+  assert(_bases_table != nullptr, \"expect sliding forwarding initialized\");\n+  return forwardee_impl(obj);\n@@ -52,1 +130,1 @@\n-  return from->forwardee();\n+  return obj->forwardee();\n@@ -56,5 +134,1 @@\n-bool FullGCForwarding::is_forwarded(oop obj) {\n-  return obj->mark().is_forwarded();\n-}\n-\n-#endif \/\/ GC_SHARED_FULLGCFORWARDING_INLINE_HPP\n+#endif \/\/ SHARE_GC_SHARED_FULLGCFORWARDING_INLINE_HPP\n","filename":"src\/hotspot\/share\/gc\/shared\/fullGCForwarding.inline.hpp","additions":97,"deletions":23,"binary":false,"changes":120,"status":"modified"},{"patch":"@@ -439,1 +439,1 @@\n-  assert(_word_size > 0, \"oop_size must be positive.\");\n+  assert(_base_size > 0, \"oop_size must be positive.\");\n@@ -441,1 +441,1 @@\n-  java_lang_Class::set_oop_size(mem, _word_size);\n+  java_lang_Class::set_oop_size(mem, _base_size);\n","filename":"src\/hotspot\/share\/gc\/shared\/memAllocator.cpp","additions":2,"deletions":2,"binary":false,"changes":4,"status":"modified"},{"patch":"@@ -111,0 +111,1 @@\n+  size_t _base_size;\n@@ -112,2 +113,3 @@\n-  ClassAllocator(Klass* klass, size_t word_size, Thread* thread = Thread::current())\n-    : MemAllocator(klass, word_size, thread) {}\n+  ClassAllocator(Klass* klass, size_t word_size, size_t base_size, Thread* thread = Thread::current())\n+    : MemAllocator(klass, word_size, thread),\n+    _base_size(base_size) {}\n","filename":"src\/hotspot\/share\/gc\/shared\/memAllocator.hpp","additions":4,"deletions":2,"binary":false,"changes":6,"status":"modified"},{"patch":"@@ -60,1 +60,1 @@\n-  _o->set_mark(_m);\n+  _o->set_mark(_m.copy_hashctrl_from(_o->mark()));\n","filename":"src\/hotspot\/share\/gc\/shared\/preservedMarks.inline.hpp","additions":1,"deletions":1,"binary":false,"changes":2,"status":"modified"},{"patch":"@@ -27,1 +27,0 @@\n-#include \"gc\/shared\/fullGCForwarding.hpp\"\n@@ -189,2 +188,0 @@\n-\n-  FullGCForwarding::initialize_flags(MaxHeapSize);\n","filename":"src\/hotspot\/share\/gc\/shenandoah\/shenandoahArguments.cpp","additions":0,"deletions":3,"binary":false,"changes":3,"status":"modified"},{"patch":"@@ -32,0 +32,3 @@\n+private:\n+  static const uintptr_t FWDED_HASH_TRANSITION = 0b111;\n+\n@@ -54,0 +57,1 @@\n+  static inline bool is_forwarded(markWord m);\n","filename":"src\/hotspot\/share\/gc\/shenandoah\/shenandoahForwarding.hpp","additions":4,"deletions":0,"binary":false,"changes":4,"status":"modified"},{"patch":"@@ -31,0 +31,1 @@\n+#include \"oops\/klass.hpp\"\n@@ -39,0 +40,3 @@\n+static HeapWord* to_forwardee(markWord mark) {\n+  return reinterpret_cast<HeapWord*>(mark.value() & ~(markWord::lock_mask_in_place | markWord::self_fwd_mask_in_place));\n+}\n@@ -46,1 +50,1 @@\n-    HeapWord* fwdptr = (HeapWord*) mark.clear_lock_bits().to_pointer();\n+    HeapWord* fwdptr = to_forwardee(mark);\n@@ -61,1 +65,1 @@\n-    HeapWord* fwdptr = (HeapWord*) mark.clear_lock_bits().to_pointer();\n+    HeapWord* fwdptr = to_forwardee(mark);\n@@ -74,0 +78,4 @@\n+inline bool ShenandoahForwarding::is_forwarded(markWord m) {\n+  return (m.value() & (markWord::lock_mask_in_place | markWord::self_fwd_mask_in_place)) > markWord::monitor_value;\n+}\n+\n@@ -75,1 +83,1 @@\n-  return obj->mark().is_marked();\n+  return is_forwarded(obj->mark());\n@@ -80,2 +88,2 @@\n-  if (old_mark.is_marked()) {\n-    return cast_to_oop(old_mark.clear_lock_bits().to_pointer());\n+  if (is_forwarded(old_mark)) {\n+    return cast_to_oop(to_forwardee(old_mark));\n@@ -85,0 +93,3 @@\n+  if (UseCompactObjectHeaders && old_mark.is_hashed_not_expanded()) {\n+    new_mark = markWord(new_mark.value() | FWDED_HASH_TRANSITION);\n+  }\n@@ -89,1 +100,1 @@\n-    return cast_to_oop(prev_mark.clear_lock_bits().to_pointer());\n+    return cast_to_oop(to_forwardee(prev_mark));\n@@ -97,1 +108,1 @@\n-      oop fwd = cast_to_oop(mark.clear_lock_bits().to_pointer());\n+      oop fwd = cast_to_oop(to_forwardee(mark));\n@@ -107,1 +118,18 @@\n-  return obj->size_given_klass(klass(obj));\n+  markWord mark = obj->mark();\n+  if (is_forwarded(mark)) {\n+    oop fwd = cast_to_oop(to_forwardee(mark));\n+    markWord fwd_mark = fwd->mark();\n+    Klass* klass = UseCompactObjectHeaders ? fwd_mark.klass() : fwd->klass();\n+    size_t size = fwd->base_size_given_klass(klass);\n+    if (UseCompactObjectHeaders) {\n+      if ((mark.value() & FWDED_HASH_TRANSITION) != FWDED_HASH_TRANSITION) {\n+        if (fwd_mark.is_expanded() && klass->expand_for_hash(fwd)) {\n+          size = align_object_size(size + 1);\n+        }\n+      }\n+    }\n+    return size;\n+  } else {\n+    Klass* klass = UseCompactObjectHeaders ? mark.klass() : obj->klass();\n+    return obj->size_given_mark_and_klass(mark, klass);\n+  }\n","filename":"src\/hotspot\/share\/gc\/shenandoah\/shenandoahForwarding.inline.hpp","additions":36,"deletions":8,"binary":false,"changes":44,"status":"modified"},{"patch":"@@ -248,0 +248,2 @@\n+    FullGCForwarding::begin();\n+\n@@ -257,0 +259,2 @@\n+\n+    FullGCForwarding::end();\n@@ -369,1 +373,3 @@\n-    size_t obj_size = p->size();\n+    size_t old_size = p->size();\n+    size_t new_size = p->copy_size(old_size, p->mark());\n+    size_t obj_size = _compact_point == cast_from_oop<HeapWord*>(p) ? old_size : new_size;\n@@ -387,0 +393,1 @@\n+      obj_size = _compact_point == cast_from_oop<HeapWord*>(p) ? old_size : new_size;\n@@ -906,0 +913,1 @@\n+      new_obj->initialize_hash_if_necessary(p);\n","filename":"src\/hotspot\/share\/gc\/shenandoah\/shenandoahFullGC.cpp","additions":9,"deletions":1,"binary":false,"changes":10,"status":"modified"},{"patch":"@@ -278,1 +278,2 @@\n-  size_t obj_size = p->size();\n+  size_t old_size = p->size();\n+  size_t new_size = p->copy_size(old_size, p->mark());\n@@ -285,1 +286,1 @@\n-    if ((_old_to_region != nullptr) && (_old_compact_point + obj_size > _old_to_region->end())) {\n+    if ((_old_to_region != nullptr) && (_old_compact_point + new_size > _old_to_region->end())) {\n@@ -307,0 +308,1 @@\n+    size_t obj_size = _old_compact_point == cast_from_oop<HeapWord*>(p) ? old_size : new_size;\n@@ -331,0 +333,1 @@\n+      obj_size = _old_compact_point == cast_from_oop<HeapWord*>(p) ? old_size : new_size;\n@@ -354,0 +357,1 @@\n+    size_t obj_size = _young_compact_point == cast_from_oop<HeapWord*>(p) ? old_size : new_size;\n@@ -377,0 +381,1 @@\n+      obj_size = _young_compact_point == cast_from_oop<HeapWord*>(p) ? old_size : new_size;\n","filename":"src\/hotspot\/share\/gc\/shenandoah\/shenandoahGenerationalFullGC.cpp","additions":7,"deletions":2,"binary":false,"changes":9,"status":"modified"},{"patch":"@@ -243,1 +243,8 @@\n-  size_t size = ShenandoahForwarding::size(p);\n+\n+  markWord mark = p->mark();\n+  if (ShenandoahForwarding::is_forwarded(mark)) {\n+    return ShenandoahForwarding::get_forwardee(p);\n+  }\n+  size_t old_size = ShenandoahForwarding::size(p);\n+  size_t size = p->copy_size(old_size, mark);\n+\n@@ -335,1 +342,1 @@\n-  Copy::aligned_disjoint_words(cast_from_oop<HeapWord*>(p), copy, size);\n+  Copy::aligned_disjoint_words(cast_from_oop<HeapWord*>(p), copy, old_size);\n@@ -353,0 +360,1 @@\n+    copy_val->initialize_hash_if_necessary(p);\n","filename":"src\/hotspot\/share\/gc\/shenandoah\/shenandoahGenerationalHeap.cpp","additions":10,"deletions":2,"binary":false,"changes":12,"status":"modified"},{"patch":"@@ -1207,1 +1207,7 @@\n-  size_t size = ShenandoahForwarding::size(p);\n+\n+  markWord mark = p->mark();\n+  if (ShenandoahForwarding::is_forwarded(mark)) {\n+    return ShenandoahForwarding::get_forwardee(p);\n+  }\n+  size_t old_size = ShenandoahForwarding::size(p);\n+  size_t size = p->copy_size(old_size, mark);\n@@ -1237,1 +1243,1 @@\n-  Copy::aligned_disjoint_words(cast_from_oop<HeapWord*>(p), copy, size);\n+  Copy::aligned_disjoint_words(cast_from_oop<HeapWord*>(p), copy, old_size);\n@@ -1244,0 +1250,1 @@\n+    copy_val->initialize_hash_if_necessary(p);\n","filename":"src\/hotspot\/share\/gc\/shenandoah\/shenandoahHeap.cpp","additions":9,"deletions":2,"binary":false,"changes":11,"status":"modified"},{"patch":"@@ -327,1 +327,2 @@\n-  const size_t size = ZUtils::object_size(from_addr);\n+  const size_t old_size = ZUtils::object_size(from_addr);\n+  const size_t size = ZUtils::copy_size(from_addr, old_size);\n@@ -337,0 +338,1 @@\n+  assert(to_addr != from_addr, \"addresses must be different\");\n@@ -339,1 +341,2 @@\n-  ZUtils::object_copy_disjoint(from_addr, to_addr, size);\n+  ZUtils::object_copy_disjoint(from_addr, to_addr, old_size);\n+  ZUtils::initialize_hash_if_necessary(to_addr, from_addr);\n@@ -591,1 +594,1 @@\n-  zaddress try_relocate_object_inner(zaddress from_addr) {\n+  zaddress try_relocate_object_inner(zaddress from_addr, size_t old_size) {\n@@ -593,2 +596,0 @@\n-\n-    const size_t size = ZUtils::object_size(from_addr);\n@@ -596,0 +597,4 @@\n+    zoffset_end from_offset = to_zoffset_end(ZAddress::offset(from_addr));\n+    zoffset_end top = to_page != nullptr ? to_page->top() : to_zoffset_end(0);\n+    const size_t new_size = ZUtils::copy_size(from_addr, old_size);\n+    const size_t size = top == from_offset ? old_size : new_size;\n@@ -613,0 +618,4 @@\n+    if (old_size != new_size && ((top == from_offset) != (allocated_addr == from_addr))) {\n+      _allocator->undo_alloc_object(to_page, allocated_addr, size);\n+      return zaddress::null;\n+    }\n@@ -616,2 +625,2 @@\n-    if (_forwarding->in_place_relocation() && allocated_addr + size > from_addr) {\n-      ZUtils::object_copy_conjoint(from_addr, allocated_addr, size);\n+    if (_forwarding->in_place_relocation() && allocated_addr + old_size > from_addr) {\n+      ZUtils::object_copy_conjoint(from_addr, allocated_addr, old_size);\n@@ -619,1 +628,4 @@\n-      ZUtils::object_copy_disjoint(from_addr, allocated_addr, size);\n+      ZUtils::object_copy_disjoint(from_addr, allocated_addr, old_size);\n+    }\n+    if (from_addr != allocated_addr) {\n+      ZUtils::initialize_hash_if_necessary(allocated_addr, from_addr);\n@@ -633,1 +645,1 @@\n-  void update_remset_old_to_old(zaddress from_addr, zaddress to_addr) const {\n+  void update_remset_old_to_old(zaddress from_addr, zaddress to_addr, size_t size) const {\n@@ -655,4 +667,2 @@\n-\n-    \/\/ Read the size from the to-object, since the from-object\n-    \/\/ could have been overwritten during in-place relocation.\n-    const size_t size = ZUtils::object_size(to_addr);\n+    assert(size <= ZUtils::object_size(to_addr), \"old size must be <= new size\");\n+    assert(size > 0, \"size must be set\");\n@@ -783,1 +793,1 @@\n-  void update_remset_for_fields(zaddress from_addr, zaddress to_addr) const {\n+  void update_remset_for_fields(zaddress from_addr, zaddress to_addr, size_t size) const {\n@@ -791,1 +801,1 @@\n-      update_remset_old_to_old(from_addr, to_addr);\n+      update_remset_old_to_old(from_addr, to_addr, size);\n@@ -800,1 +810,2 @@\n-    const zaddress to_addr = try_relocate_object_inner(from_addr);\n+    size_t size = ZUtils::object_size(from_addr);\n+    const zaddress to_addr = try_relocate_object_inner(from_addr, size);\n@@ -806,1 +817,1 @@\n-    update_remset_for_fields(from_addr, to_addr);\n+    update_remset_for_fields(from_addr, to_addr, size);\n","filename":"src\/hotspot\/share\/gc\/z\/zRelocate.cpp","additions":28,"deletions":17,"binary":false,"changes":45,"status":"modified"},{"patch":"@@ -45,0 +45,3 @@\n+  static size_t copy_size(zaddress addr, size_t size);\n+  static void initialize_hash_if_necessary(zaddress to_addr, zaddress from_addr);\n+\n","filename":"src\/hotspot\/share\/gc\/z\/zUtils.hpp","additions":3,"deletions":0,"binary":false,"changes":3,"status":"modified"},{"patch":"@@ -62,0 +62,9 @@\n+inline size_t ZUtils::copy_size(zaddress addr, size_t old_size) {\n+  oop obj = to_oop(addr);\n+  return words_to_bytes(obj->copy_size(bytes_to_words(old_size), obj->mark()));\n+}\n+\n+inline void ZUtils::initialize_hash_if_necessary(zaddress to_addr, zaddress from_addr) {\n+  to_oop(to_addr)->initialize_hash_if_necessary(to_oop(from_addr));\n+}\n+\n","filename":"src\/hotspot\/share\/gc\/z\/zUtils.inline.hpp","additions":9,"deletions":0,"binary":false,"changes":9,"status":"modified"},{"patch":"@@ -568,1 +568,1 @@\n-          oop m = java_lang_Class::create_basic_type_mirror(type2name(bt), bt, CHECK);\n+          oop m = java_lang_Class::create_basic_type_mirror(type2name(bt), bt, false, CHECK);\n","filename":"src\/hotspot\/share\/memory\/universe.cpp","additions":1,"deletions":1,"binary":false,"changes":2,"status":"modified"},{"patch":"@@ -308,0 +308,7 @@\n+\n+int ArrayKlass::hash_offset_in_bytes(oop obj) const {\n+  assert(UseCompactObjectHeaders, \"only with compact i-hash\");\n+  arrayOop ary = arrayOop(obj);\n+  BasicType type = element_type();\n+  return ary->base_offset_in_bytes(type) + (ary->length() << log2_element_size());\n+}\n","filename":"src\/hotspot\/share\/oops\/arrayKlass.cpp","additions":7,"deletions":0,"binary":false,"changes":7,"status":"modified"},{"patch":"@@ -125,0 +125,2 @@\n+  int hash_offset_in_bytes(oop obj) const;\n+\n","filename":"src\/hotspot\/share\/oops\/arrayKlass.hpp","additions":2,"deletions":0,"binary":false,"changes":2,"status":"modified"},{"patch":"@@ -532,1 +532,2 @@\n-  _init_thread(nullptr)\n+  _init_thread(nullptr),\n+  _hash_offset(parser.hash_offset())\n","filename":"src\/hotspot\/share\/oops\/instanceKlass.cpp","additions":2,"deletions":1,"binary":false,"changes":3,"status":"modified"},{"patch":"@@ -241,0 +241,2 @@\n+  int             _hash_offset;             \/\/ Offset of hidden field for i-hash\n+\n@@ -950,0 +952,9 @@\n+  virtual int hash_offset_in_bytes(oop obj) const {\n+    assert(UseCompactObjectHeaders, \"only with compact i-hash\");\n+    return _hash_offset;\n+  }\n+  static int hash_offset_offset_in_bytes() {\n+    assert(UseCompactObjectHeaders, \"only with compact i-hash\");\n+    return (int)offset_of(InstanceKlass, _hash_offset);\n+  }\n+\n","filename":"src\/hotspot\/share\/oops\/instanceKlass.hpp","additions":11,"deletions":0,"binary":false,"changes":11,"status":"modified"},{"patch":"@@ -54,1 +54,1 @@\n-instanceOop InstanceMirrorKlass::allocate_instance(Klass* k, TRAPS) {\n+instanceOop InstanceMirrorKlass::allocate_instance(Klass* k, bool extend, TRAPS) {\n@@ -56,2 +56,6 @@\n-  size_t size = instance_size(k);\n-  assert(size > 0, \"total object size must be non-zero: \" SIZE_FORMAT, size);\n+  size_t base_size = instance_size(k);\n+  size_t size = base_size;\n+  if (extend && UseCompactObjectHeaders) {\n+    size = align_object_size(size + 1);\n+  }\n+  assert(base_size > 0, \"base object size must be non-zero: \" SIZE_FORMAT, base_size);\n@@ -61,1 +65,5 @@\n-  return (instanceOop)Universe::heap()->class_allocate(this, size, THREAD);\n+  instanceOop obj = (instanceOop)Universe::heap()->class_allocate(this, size, base_size, THREAD);\n+  if (extend && UseCompactObjectHeaders) {\n+    obj->set_mark(obj->mark().set_not_hashed_expanded());\n+  }\n+  return obj;\n@@ -76,0 +84,10 @@\n+int InstanceMirrorKlass::hash_offset_in_bytes(oop obj) const {\n+  assert(UseCompactObjectHeaders, \"only with compact i-hash\");\n+  \/\/ TODO: There may be gaps that we could use, e.g. in the fields of Class,\n+  \/\/ between the fields of Class and the static fields or in or at the end of\n+  \/\/ the static fields block.\n+  \/\/ When implementing any change here, make sure that allocate_instance()\n+  \/\/ and corresponding code in InstanceMirrorKlass.java are in sync.\n+  return obj->base_size_given_klass(this) * BytesPerWord;\n+}\n+\n","filename":"src\/hotspot\/share\/oops\/instanceMirrorKlass.cpp","additions":22,"deletions":4,"binary":false,"changes":26,"status":"modified"},{"patch":"@@ -69,0 +69,1 @@\n+  int hash_offset_in_bytes(oop obj) const;\n@@ -92,1 +93,1 @@\n-  instanceOop allocate_instance(Klass* k, TRAPS);\n+  instanceOop allocate_instance(Klass* k, bool extend, TRAPS);\n","filename":"src\/hotspot\/share\/oops\/instanceMirrorKlass.hpp","additions":2,"deletions":1,"binary":false,"changes":3,"status":"modified"},{"patch":"@@ -1327,0 +1327,6 @@\n+\n+bool Klass::expand_for_hash(oop obj) const {\n+  assert(UseCompactObjectHeaders, \"only with compact i-hash\");\n+  assert((size_t)hash_offset_in_bytes(obj) <= (obj->base_size_given_klass(this) * HeapWordSize), \"hash offset must be eq or lt base size: hash offset: %d, base size: \" SIZE_FORMAT, hash_offset_in_bytes(obj), obj->base_size_given_klass(this) * HeapWordSize);\n+  return obj->base_size_given_klass(this) * HeapWordSize - hash_offset_in_bytes(obj) < (int)sizeof(uint32_t);\n+}\n","filename":"src\/hotspot\/share\/oops\/klass.cpp","additions":6,"deletions":0,"binary":false,"changes":6,"status":"modified"},{"patch":"@@ -792,0 +792,4 @@\n+  virtual int hash_offset_in_bytes(oop obj) const = 0;\n+  static int kind_offset_in_bytes() { return (int)offset_of(Klass, _kind); }\n+\n+  bool expand_for_hash(oop obj) const;\n","filename":"src\/hotspot\/share\/oops\/klass.hpp","additions":4,"deletions":0,"binary":false,"changes":4,"status":"modified"},{"patch":"@@ -94,0 +94,3 @@\n+      } else if (UseCompactObjectHeaders) {\n+        st->print(\" hash is-hashed=%s is-copied=%s\", BOOL_TO_STR(is_hashed_not_expanded()), BOOL_TO_STR(\n+                is_hashed_expanded()));\n","filename":"src\/hotspot\/share\/oops\/markWord.cpp","additions":3,"deletions":0,"binary":false,"changes":3,"status":"modified"},{"patch":"@@ -49,1 +49,1 @@\n-\/\/  klass:22  hash:31 -->| unused_gap:4  age:4  self-fwd:1  lock:2 (normal object)\n+\/\/  klass:22  unused_gap:29 hash:2 -->| unused_gap:4  age:4  self-fwd:1  lock:2 (normal object)\n@@ -56,0 +56,13 @@\n+\/\/  - With +UseCompactObjectHeaders:\n+\/\/    hashctrl bits indicate if object has been hashed:\n+\/\/    00 - never hashed\n+\/\/    01 - hashed, but not expanded by GC: will recompute hash\n+\/\/    10 - not hashed, but expanded; special state used only by CDS to deal with scratch classes\n+\/\/    11 - hashed and expanded by GC, and hashcode has been installed in hidden field\n+\/\/\n+\/\/    When identityHashCode() is called, the transitions work as follows:\n+\/\/    00 - set the hashctrl bits to 01, and compute the identity hash\n+\/\/    01 - recompute idendity hash. When GC encounters 01 when moving an object, it will allocate an extra word, if\n+\/\/         necessary, for the object copy, and install 11.\n+\/\/    11 - read hashcode from field\n+\/\/\n@@ -116,0 +129,1 @@\n+  static const int hashctrl_bits                  = 2;\n@@ -121,0 +135,1 @@\n+  static const int hashctrl_shift                 = age_shift + age_bits + unused_gap_bits;\n@@ -130,0 +145,4 @@\n+  static const uintptr_t hashctrl_mask            = right_n_bits(hashctrl_bits);\n+  static const uintptr_t hashctrl_mask_in_place   = hashctrl_mask << hashctrl_shift;\n+  static const uintptr_t hashctrl_hashed_mask_in_place    = ((uintptr_t)1) << hashctrl_shift;\n+  static const uintptr_t hashctrl_expanded_mask_in_place  = ((uintptr_t)2) << hashctrl_shift;\n@@ -167,1 +186,1 @@\n-    return (mask_bits(value(), lock_mask_in_place) == marked_value);\n+    return (value() & (self_fwd_mask_in_place | lock_mask_in_place)) > monitor_value;\n@@ -192,1 +211,1 @@\n-    return (!is_unlocked() || !has_no_hash());\n+    return UseCompactObjectHeaders ? !is_unlocked() : (!is_unlocked() || !has_no_hash());\n@@ -239,0 +258,1 @@\n+    assert(!UseCompactObjectHeaders, \"Do not use with compact i-hash\");\n@@ -279,0 +299,1 @@\n+    assert(!UseCompactObjectHeaders, \"only without compact i-hash\");\n@@ -283,1 +304,57 @@\n-    return hash() == no_hash;\n+    if (UseCompactObjectHeaders) {\n+      return !is_hashed();\n+    } else {\n+      return hash() == no_hash;\n+    }\n+  }\n+\n+  inline bool is_hashed_not_expanded() const {\n+    assert(UseCompactObjectHeaders, \"only with compact i-hash\");\n+    return (value() & hashctrl_mask_in_place) == hashctrl_hashed_mask_in_place;\n+  }\n+  inline markWord set_hashed_not_expanded() const {\n+    assert(UseCompactObjectHeaders, \"only with compact i-hash\");\n+    return markWord((value() & ~hashctrl_mask_in_place) | hashctrl_hashed_mask_in_place);\n+  }\n+\n+  inline bool is_hashed_expanded() const {\n+    assert(UseCompactObjectHeaders, \"only with compact i-hash\");\n+    return (value() & hashctrl_mask_in_place) == (hashctrl_hashed_mask_in_place | hashctrl_expanded_mask_in_place);\n+  }\n+  inline markWord set_hashed_expanded() const {\n+    assert(UseCompactObjectHeaders, \"only with compact i-hash\");\n+    return markWord((value() & ~hashctrl_mask_in_place) | (hashctrl_hashed_mask_in_place | hashctrl_expanded_mask_in_place));\n+  }\n+\n+  \/\/ This is a special hashctrl state (11) that is only used\n+  \/\/ during CDS archive dumping. There we allocate 'scratch mirrors' for\n+  \/\/ each real mirror klass. We allocate those scratch mirrors\n+  \/\/ in a pre-extended form, but without being hashed. When the\n+  \/\/ real mirror gets hashed, then we turn the scratch mirror into\n+  \/\/ hashed_moved state, otherwise we leave it in that special state\n+  \/\/ which indicates that the archived copy will be allocated in the\n+  \/\/ unhashed form.\n+  inline bool is_not_hashed_expanded() const {\n+    assert(UseCompactObjectHeaders, \"only with compact i-hash\");\n+    return (value() & hashctrl_mask_in_place) == hashctrl_expanded_mask_in_place;\n+  }\n+  inline markWord set_not_hashed_expanded() const {\n+    assert(UseCompactObjectHeaders, \"only with compact i-hash\");\n+    return markWord((value() & ~hashctrl_mask_in_place) | hashctrl_expanded_mask_in_place);\n+  }\n+  \/\/ Return true when object is either hashed_moved or not_hashed_moved.\n+  inline bool is_expanded() const {\n+    assert(UseCompactObjectHeaders, \"only with compact i-hash\");\n+    return (value() & hashctrl_expanded_mask_in_place) != 0;\n+  }\n+  inline bool is_hashed() const {\n+    assert(UseCompactObjectHeaders, \"only with compact i-hash\");\n+    return (value() & hashctrl_hashed_mask_in_place) != 0;\n+  }\n+\n+  inline markWord copy_hashctrl_from(markWord m) const {\n+    if (UseCompactObjectHeaders) {\n+      return markWord((value() & ~hashctrl_mask_in_place) | (m.value() & hashctrl_mask_in_place));\n+    } else {\n+      return markWord(value());\n+    }\n@@ -294,1 +371,5 @@\n-    return markWord( no_hash_in_place | no_lock_in_place );\n+    if (UseCompactObjectHeaders) {\n+      return markWord(no_lock_in_place);\n+    } else {\n+      return markWord(no_hash_in_place | no_lock_in_place);\n+    }\n@@ -308,1 +389,2 @@\n-    return mask_bits(value(), self_fwd_mask_in_place) != 0;\n+    \/\/ Match 100, 101, 110 but not 111.\n+    return mask_bits(value() + 1, (lock_mask_in_place | self_fwd_mask_in_place)) > 4;\n","filename":"src\/hotspot\/share\/oops\/markWord.hpp","additions":88,"deletions":6,"binary":false,"changes":94,"status":"modified"},{"patch":"@@ -41,0 +41,1 @@\n+#include \"runtime\/lightweightSynchronizer.hpp\"\n@@ -132,0 +133,17 @@\n+void oopDesc::initialize_hash_if_necessary(oop obj, Klass* k, markWord m) {\n+  assert(UseCompactObjectHeaders, \"only with compact object headers\");\n+  assert(!m.has_displaced_mark_helper(), \"must not be displaced header\");\n+  assert(m.is_hashed_not_expanded(), \"must be hashed but not moved\");\n+  assert(!m.is_hashed_expanded(), \"must not be moved: \" INTPTR_FORMAT, m.value());\n+  uint32_t hash = static_cast<uint32_t>(ObjectSynchronizer::get_next_hash(nullptr, obj));\n+  int offset = k->hash_offset_in_bytes(cast_to_oop(this));\n+  assert(offset >= 8, \"hash offset must not be in header\");\n+  \/\/log_info(gc)(\"Initializing hash for \" PTR_FORMAT \", old: \" PTR_FORMAT \", hash: %d, offset: %d\", p2i(this), p2i(obj), hash, offset);\n+  int_field_put(offset, (jint) hash);\n+  m = m.set_hashed_expanded();\n+  assert(static_cast<uint32_t>(LightweightSynchronizer::get_hash(m, cast_to_oop(this), k)) == hash,\n+         \"hash must remain the same\");\n+  assert(m.narrow_klass() != 0, \"must not be null\");\n+  set_mark(m);\n+}\n+\n","filename":"src\/hotspot\/share\/oops\/oop.cpp","additions":18,"deletions":0,"binary":false,"changes":18,"status":"modified"},{"patch":"@@ -119,0 +119,22 @@\n+  \/\/ Returns the size that a copy of this object requires, in machine words.\n+  \/\/ It can be 1 word larger than its current size to accomodate\n+  \/\/ an additional 4-byte-field for the identity hash-code.\n+  \/\/\n+  \/\/ size: the current size of this object, we're passing this here for performance\n+  \/\/       reasons, because all callers compute this anyway, and we want to avoid\n+  \/\/       recomputing it.\n+  \/\/ mark: the mark-word of this object. Some callers (e.g. G1ParScanThreadState::do_copy_to_survivor_space())\n+  \/\/       need to use a known markWord because of racing GC threads that can change\n+  \/\/       the markWord at any time.\n+  inline size_t copy_size(size_t size, markWord mark) const;\n+  \/\/ Special version to deal with scratch classes in CDS. There we allocate\n+  \/\/ temporary scratch classes (which are skeleton versions of InstanceMirrorKlass,\n+  \/\/ which represent java.lang.Class objects in the CDS archive). At that point, we\n+  \/\/ don't know whether or not the final archived version will be hashed or expanded,\n+  \/\/ and therefore we allocate them in the special state not-hashed-but-expanded.\n+  \/\/ When creating the final copy of those objects, we either populate the hidden hash\n+  \/\/ field and make the object 'expanded', or we turn it back to 'not-hashed'\n+  \/\/ and reduce the object's size. We do this by providing a separate method for CDS\n+  \/\/ so that we don't affect GC performance.\n+  inline size_t copy_size_cds(size_t size, markWord mark) const;\n+\n@@ -121,1 +143,2 @@\n-  inline size_t size_given_klass(Klass* klass);\n+  inline size_t base_size_given_klass(const Klass* klass);\n+  inline size_t size_given_mark_and_klass(markWord mrk, const Klass* kls);\n@@ -319,0 +342,6 @@\n+  \/\/ Initialize identity hash code in hash word of object copy from original object.\n+  \/\/ Returns true if the object has been expanded, false otherwise.\n+  inline void initialize_hash_if_necessary(oop obj);\n+  \/\/ For CDS only.\n+  void initialize_hash_if_necessary(oop obj, Klass* k, markWord m);\n+\n","filename":"src\/hotspot\/share\/oops\/oop.hpp","additions":30,"deletions":1,"binary":false,"changes":31,"status":"modified"},{"patch":"@@ -95,1 +95,7 @@\n-  set_mark(prototype_mark());\n+  if (UseCompactObjectHeaders) {\n+    markWord m = prototype_mark().copy_hashctrl_from(mark());\n+    assert(m.is_neutral(), \"must be neutral\");\n+    set_mark(m);\n+  } else {\n+    set_mark(prototype_mark());\n+  }\n@@ -175,0 +181,38 @@\n+size_t oopDesc::size_given_mark_and_klass(markWord mrk, const Klass* kls) {\n+  size_t sz = base_size_given_klass(kls);\n+  if (UseCompactObjectHeaders) {\n+    assert(!mrk.has_displaced_mark_helper(), \"must not be displaced\");\n+    if (mrk.is_expanded() && kls->expand_for_hash(cast_to_oop(this))) {\n+      sz = align_object_size(sz + 1);\n+    }\n+  }\n+  return sz;\n+}\n+\n+size_t oopDesc::copy_size(size_t size, markWord mark) const {\n+  if (UseCompactObjectHeaders) {\n+    assert(!mark.has_displaced_mark_helper(), \"must not be displaced\");\n+    Klass* klass = mark.klass();\n+    if (mark.is_hashed_not_expanded() && klass->expand_for_hash(cast_to_oop(this))) {\n+      size = align_object_size(size + 1);\n+    }\n+  }\n+  assert(is_object_aligned(size), \"Oop size is not properly aligned: \" SIZE_FORMAT, size);\n+  return size;\n+}\n+\n+size_t oopDesc::copy_size_cds(size_t size, markWord mark) const {\n+  if (UseCompactObjectHeaders) {\n+    assert(!mark.has_displaced_mark_helper(), \"must not be displaced\");\n+    Klass* klass = mark.klass();\n+    if (mark.is_hashed_not_expanded() && klass->expand_for_hash(cast_to_oop(this))) {\n+      size = align_object_size(size + 1);\n+    }\n+    if (mark.is_not_hashed_expanded() && klass->expand_for_hash(cast_to_oop(this))) {\n+      size = align_object_size(size - ObjectAlignmentInBytes \/ HeapWordSize);\n+    }\n+  }\n+  assert(is_object_aligned(size), \"Oop size is not properly aligned: \" SIZE_FORMAT, size);\n+  return size;\n+}\n+\n@@ -176,1 +220,1 @@\n-  return size_given_klass(klass());\n+  return size_given_mark_and_klass(mark(), klass());\n@@ -179,1 +223,1 @@\n-size_t oopDesc::size_given_klass(Klass* klass)  {\n+size_t oopDesc::base_size_given_klass(const Klass* klass)  {\n@@ -388,1 +432,1 @@\n-  size_t size = size_given_klass(k);\n+  size_t size = size_given_mark_and_klass(mark(), k);\n@@ -396,1 +440,1 @@\n-  size_t size = size_given_klass(k);\n+  size_t size = size_given_mark_and_klass(mark(), k);\n@@ -420,5 +464,7 @@\n-  markWord mrk = mark();\n-  if (mrk.is_unlocked() && !mrk.has_no_hash()) {\n-    return mrk.hash();\n-  } else if (mrk.is_marked()) {\n-    return mrk.hash();\n+  if (UseCompactObjectHeaders) {\n+    markWord mrk = mark();\n+    if (mrk.is_hashed_expanded()) {\n+      Klass* klass = mrk.klass();\n+      return int_field(klass->hash_offset_in_bytes(cast_to_oop(this)));\n+    }\n+    \/\/ Fall-through to slow-case.\n@@ -426,1 +472,7 @@\n-    return slow_identity_hash();\n+    markWord mrk = mark();\n+    if (mrk.is_unlocked() && !mrk.has_no_hash()) {\n+      return mrk.hash();\n+    } else if (mrk.is_marked()) {\n+      return mrk.hash();\n+    }\n+    \/\/ Fall-through to slow-case.\n@@ -428,0 +480,1 @@\n+  return slow_identity_hash();\n@@ -435,1 +488,1 @@\n-  return mrk.is_unlocked() && mrk.has_no_hash();\n+  return (UseCompactObjectHeaders || mrk.is_unlocked()) && mrk.has_no_hash();\n@@ -458,0 +511,12 @@\n+inline void oopDesc::initialize_hash_if_necessary(oop obj) {\n+  if (!UseCompactObjectHeaders) {\n+    return;\n+  }\n+  markWord m = mark();\n+  assert(!m.has_displaced_mark_helper(), \"must not be displaced header\");\n+  if (m.is_hashed_not_expanded()) {\n+    initialize_hash_if_necessary(obj, m.klass(), m);\n+  }\n+}\n+\n+\n","filename":"src\/hotspot\/share\/oops\/oop.inline.hpp","additions":77,"deletions":12,"binary":false,"changes":89,"status":"modified"},{"patch":"@@ -4668,1 +4668,1 @@\n-  enum { _slow_path = 1, _fast_path, _null_path, PATH_LIMIT };\n+  enum { _slow_path = 1, _null_path, _fast_path, _fast_path2, PATH_LIMIT };\n@@ -4716,6 +4716,34 @@\n-  \/\/ Get the header out of the object, use LoadMarkNode when available\n-  Node* header_addr = basic_plus_adr(obj, oopDesc::mark_offset_in_bytes());\n-  \/\/ The control of the load must be null. Otherwise, the load can move before\n-  \/\/ the null check after castPP removal.\n-  Node* no_ctrl = nullptr;\n-  Node* header = make_load(no_ctrl, header_addr, TypeX_X, TypeX_X->basic_type(), MemNode::unordered);\n+  if (UseCompactObjectHeaders) {\n+    \/\/ Get the header out of the object.\n+    Node* header_addr = basic_plus_adr(obj, oopDesc::mark_offset_in_bytes());\n+    \/\/ The control of the load must be null. Otherwise, the load can move before\n+    \/\/ the null check after castPP removal.\n+    Node* no_ctrl = nullptr;\n+    Node* header = make_load(no_ctrl, header_addr, TypeX_X, TypeX_X->basic_type(), MemNode::unordered);\n+\n+    \/\/ Test the header to see if the object is in hashed or copied state.\n+    Node* hashctrl_mask  = _gvn.MakeConX(markWord::hashctrl_mask_in_place);\n+    Node* masked_header  = _gvn.transform(new AndXNode(header, hashctrl_mask));\n+\n+    \/\/ Take slow-path when the object has not been hashed.\n+    Node* not_hashed_val = _gvn.MakeConX(0);\n+    Node* chk_hashed     = _gvn.transform(new CmpXNode(masked_header, not_hashed_val));\n+    Node* test_hashed    = _gvn.transform(new BoolNode(chk_hashed, BoolTest::eq));\n+\n+    generate_slow_guard(test_hashed, slow_region);\n+\n+    \/\/ Test whether the object is hashed or hashed&copied.\n+    Node* hashed_copied = _gvn.MakeConX(markWord::hashctrl_expanded_mask_in_place | markWord::hashctrl_hashed_mask_in_place);\n+    Node* chk_copied    = _gvn.transform(new CmpXNode(masked_header, hashed_copied));\n+    \/\/ If true, then object has been hashed&copied, otherwise it's only hashed.\n+    Node* test_copied   = _gvn.transform(new BoolNode(chk_copied, BoolTest::eq));\n+    IfNode* if_copied   = create_and_map_if(control(), test_copied, PROB_FAIR, COUNT_UNKNOWN);\n+    Node* if_true = _gvn.transform(new IfTrueNode(if_copied));\n+    Node* if_false = _gvn.transform(new IfFalseNode(if_copied));\n+\n+    \/\/ Hashed&Copied path: read hash-code out of the object.\n+    set_control(if_true);\n+    \/\/ result_val->del_req(_fast_path2);\n+    \/\/ result_reg->del_req(_fast_path2);\n+    \/\/ result_io->del_req(_fast_path2);\n+    \/\/ result_mem->del_req(_fast_path2);\n@@ -4723,8 +4751,19 @@\n-  if (!UseObjectMonitorTable) {\n-    \/\/ Test the header to see if it is safe to read w.r.t. locking.\n-    Node *lock_mask      = _gvn.MakeConX(markWord::lock_mask_in_place);\n-    Node *lmasked_header = _gvn.transform(new AndXNode(header, lock_mask));\n-    if (LockingMode == LM_LIGHTWEIGHT) {\n-      Node *monitor_val   = _gvn.MakeConX(markWord::monitor_value);\n-      Node *chk_monitor   = _gvn.transform(new CmpXNode(lmasked_header, monitor_val));\n-      Node *test_monitor  = _gvn.transform(new BoolNode(chk_monitor, BoolTest::eq));\n+    Node* obj_klass = load_object_klass(obj);\n+    Node* hash_addr;\n+    const TypeKlassPtr* klass_t = _gvn.type(obj_klass)->isa_klassptr();\n+    bool load_offset_runtime = true;\n+\n+    if (klass_t != nullptr) {\n+      if (klass_t->klass_is_exact()  && klass_t->isa_instklassptr()) {\n+        ciInstanceKlass* ciKlass = reinterpret_cast<ciInstanceKlass*>(klass_t->is_instklassptr()->exact_klass());\n+        if (!ciKlass->is_mirror_instance_klass() && !ciKlass->is_reference_instance_klass()) {\n+          \/\/ We know the InstanceKlass, load hash_offset from there at compile-time.\n+          int hash_offset = ciKlass->hash_offset_in_bytes();\n+          hash_addr = basic_plus_adr(obj, hash_offset);\n+          Node* loaded_hash = make_load(control(), hash_addr, TypeInt::INT, T_INT, MemNode::unordered);\n+          result_val->init_req(_fast_path2, loaded_hash);\n+          result_reg->init_req(_fast_path2, control());\n+          load_offset_runtime = false;\n+        }\n+      }\n+    }\n@@ -4732,5 +4771,22 @@\n-      generate_slow_guard(test_monitor, slow_region);\n-    } else {\n-      Node *unlocked_val      = _gvn.MakeConX(markWord::unlocked_value);\n-      Node *chk_unlocked      = _gvn.transform(new CmpXNode(lmasked_header, unlocked_val));\n-      Node *test_not_unlocked = _gvn.transform(new BoolNode(chk_unlocked, BoolTest::ne));\n+    \/\/tty->print_cr(\"Load hash-offset at runtime: %s\", BOOL_TO_STR(load_offset_runtime));\n+\n+    if (load_offset_runtime) {\n+      \/\/ We don't know if it is an array or an exact type, figure it out at run-time.\n+      \/\/ If not an ordinary instance, then we need to take slow-path.\n+      Node* kind_addr = basic_plus_adr(obj_klass, Klass::kind_offset_in_bytes());\n+      Node* kind = make_load(control(), kind_addr, TypeInt::INT, T_INT, MemNode::unordered);\n+      Node* instance_val = _gvn.intcon(Klass::InstanceKlassKind);\n+      Node* chk_inst     = _gvn.transform(new CmpINode(kind, instance_val));\n+      Node* test_inst    = _gvn.transform(new BoolNode(chk_inst, BoolTest::ne));\n+      generate_slow_guard(test_inst, slow_region);\n+\n+      \/\/ Otherwise it's an instance and we can read the hash_offset from the InstanceKlass.\n+      Node* hash_offset_addr = basic_plus_adr(obj_klass, InstanceKlass::hash_offset_offset_in_bytes());\n+      Node* hash_offset = make_load(control(), hash_offset_addr, TypeInt::INT, T_INT, MemNode::unordered);\n+      \/\/ hash_offset->dump();\n+      Node* hash_addr = basic_plus_adr(obj, ConvI2X(hash_offset));\n+      Compile::current()->set_has_unsafe_access(true);\n+      Node* loaded_hash = make_load(control(), hash_addr, TypeInt::INT, T_INT, MemNode::unordered);\n+      result_val->init_req(_fast_path2, loaded_hash);\n+      result_reg->init_req(_fast_path2, control());\n+    }\n@@ -4738,1 +4794,78 @@\n-      generate_slow_guard(test_not_unlocked, slow_region);\n+    \/\/ Hashed-only path: recompute hash-code from object address.\n+    set_control(if_false);\n+    \/\/ Our constants.\n+    Node* M = _gvn.intcon(0x337954D5);\n+    Node* A = _gvn.intcon(0xAAAAAAAA);\n+    \/\/ Split object address into lo and hi 32 bits.\n+    Node* obj_addr = _gvn.transform(new CastP2XNode(nullptr, obj));\n+    Node* x = _gvn.transform(new ConvL2INode(obj_addr));\n+    Node* upper_addr = _gvn.transform(new URShiftLNode(obj_addr, _gvn.intcon(32)));\n+    Node* y = _gvn.transform(new ConvL2INode(upper_addr));\n+\n+    Node* H0 = _gvn.transform(new XorINode(x, y));\n+    Node* L0 = _gvn.transform(new XorINode(x, A));\n+\n+    \/\/ Full multiplication of two 32 bit values L0 and M into a hi\/lo result in two 32 bit values V0 and U0.\n+    Node* L0_64 = _gvn.transform(new ConvI2LNode(L0));\n+    L0_64 = _gvn.transform(new AndLNode(L0_64, _gvn.longcon(0xFFFFFFFF)));\n+    Node* M_64 = _gvn.transform(new ConvI2LNode(M));\n+    \/\/ M_64 = _gvn.transform(new AndLNode(M_64, _gvn.longcon(0xFFFFFFFF)));\n+    Node* prod64 = _gvn.transform(new MulLNode(L0_64, M_64));\n+    Node* V0 = _gvn.transform(new ConvL2INode(prod64));\n+    Node* prod_upper = _gvn.transform(new URShiftLNode(prod64, _gvn.intcon(32)));\n+    Node* U0 = _gvn.transform(new ConvL2INode(prod_upper));\n+\n+    Node* Q0 = _gvn.transform(new MulINode(H0, M));\n+    Node* L1 = _gvn.transform(new XorINode(Q0, U0));\n+\n+    \/\/ Full multiplication of two 32 bit values L1 and M into a hi\/lo result in two 32 bit values V1 and U1.\n+    Node* L1_64 = _gvn.transform(new ConvI2LNode(L1));\n+    L1_64 = _gvn.transform(new AndLNode(L1_64, _gvn.longcon(0xFFFFFFFF)));\n+    prod64 = _gvn.transform(new MulLNode(L1_64, M_64));\n+    Node* V1 = _gvn.transform(new ConvL2INode(prod64));\n+    prod_upper = _gvn.transform(new URShiftLNode(prod64, _gvn.intcon(32)));\n+    Node* U1 = _gvn.transform(new ConvL2INode(prod_upper));\n+\n+    Node* P1 = _gvn.transform(new XorINode(V0, M));\n+\n+    \/\/ Right rotate P1 by distance L1.\n+    Node* distance = _gvn.transform(new AndINode(L1, _gvn.intcon(32 - 1)));\n+    Node* inverse_distance = _gvn.transform(new SubINode(_gvn.intcon(32), distance));\n+    Node* ror_part1 = _gvn.transform(new URShiftINode(P1, distance));\n+    Node* ror_part2 = _gvn.transform(new LShiftINode(P1, inverse_distance));\n+    Node* Q1 = _gvn.transform(new OrINode(ror_part1, ror_part2));\n+\n+    Node* L2 = _gvn.transform(new XorINode(Q1, U1));\n+    Node* hash = _gvn.transform(new XorINode(V1, L2));\n+    Node* hash_truncated = _gvn.transform(new AndINode(hash, _gvn.intcon(markWord::hash_mask)));\n+\n+    \/\/ TODO: We could generate a fast case here under the following conditions:\n+    \/\/ - The hashctrl is set to hash_is_copied (see markWord::hash_is_copied())\n+    \/\/ - The type of the object is known\n+    \/\/ Then we can load the identity hashcode from the int field at Klass::hash_offset_in_bytes() of the object.\n+    result_val->init_req(_fast_path, hash_truncated);\n+  } else {\n+    \/\/ Get the header out of the object, use LoadMarkNode when available\n+    Node* header_addr = basic_plus_adr(obj, oopDesc::mark_offset_in_bytes());\n+    \/\/ The control of the load must be null. Otherwise, the load can move before\n+    \/\/ the null check after castPP removal.\n+    Node* no_ctrl = nullptr;\n+    Node* header = make_load(no_ctrl, header_addr, TypeX_X, TypeX_X->basic_type(), MemNode::unordered);\n+\n+    if (!UseObjectMonitorTable) {\n+      \/\/ Test the header to see if it is safe to read w.r.t. locking.\n+      Node *lock_mask      = _gvn.MakeConX(markWord::lock_mask_in_place);\n+      Node *lmasked_header = _gvn.transform(new AndXNode(header, lock_mask));\n+      if (LockingMode == LM_LIGHTWEIGHT) {\n+        Node *monitor_val   = _gvn.MakeConX(markWord::monitor_value);\n+        Node *chk_monitor   = _gvn.transform(new CmpXNode(lmasked_header, monitor_val));\n+        Node *test_monitor  = _gvn.transform(new BoolNode(chk_monitor, BoolTest::eq));\n+\n+        generate_slow_guard(test_monitor, slow_region);\n+      } else {\n+        Node *unlocked_val      = _gvn.MakeConX(markWord::unlocked_value);\n+        Node *chk_unlocked      = _gvn.transform(new CmpXNode(lmasked_header, unlocked_val));\n+        Node *test_not_unlocked = _gvn.transform(new BoolNode(chk_unlocked, BoolTest::ne));\n+\n+        generate_slow_guard(test_not_unlocked, slow_region);\n+      }\n@@ -4740,1 +4873,0 @@\n-  }\n@@ -4742,13 +4874,13 @@\n-  \/\/ Get the hash value and check to see that it has been properly assigned.\n-  \/\/ We depend on hash_mask being at most 32 bits and avoid the use of\n-  \/\/ hash_mask_in_place because it could be larger than 32 bits in a 64-bit\n-  \/\/ vm: see markWord.hpp.\n-  Node *hash_mask      = _gvn.intcon(markWord::hash_mask);\n-  Node *hash_shift     = _gvn.intcon(markWord::hash_shift);\n-  Node *hshifted_header= _gvn.transform(new URShiftXNode(header, hash_shift));\n-  \/\/ This hack lets the hash bits live anywhere in the mark object now, as long\n-  \/\/ as the shift drops the relevant bits into the low 32 bits.  Note that\n-  \/\/ Java spec says that HashCode is an int so there's no point in capturing\n-  \/\/ an 'X'-sized hashcode (32 in 32-bit build or 64 in 64-bit build).\n-  hshifted_header      = ConvX2I(hshifted_header);\n-  Node *hash_val       = _gvn.transform(new AndINode(hshifted_header, hash_mask));\n+    \/\/ Get the hash value and check to see that it has been properly assigned.\n+    \/\/ We depend on hash_mask being at most 32 bits and avoid the use of\n+    \/\/ hash_mask_in_place because it could be larger than 32 bits in a 64-bit\n+    \/\/ vm: see markWord.hpp.\n+    Node *hash_mask      = _gvn.intcon(markWord::hash_mask);\n+    Node *hash_shift     = _gvn.intcon(markWord::hash_shift);\n+    Node *hshifted_header= _gvn.transform(new URShiftXNode(header, hash_shift));\n+    \/\/ This hack lets the hash bits live anywhere in the mark object now, as long\n+    \/\/ as the shift drops the relevant bits into the low 32 bits.  Note that\n+    \/\/ Java spec says that HashCode is an int so there's no point in capturing\n+    \/\/ an 'X'-sized hashcode (32 in 32-bit build or 64 in 64-bit build).\n+    hshifted_header      = ConvX2I(hshifted_header);\n+    Node *hash_val       = _gvn.transform(new AndINode(hshifted_header, hash_mask));\n@@ -4756,3 +4888,3 @@\n-  Node *no_hash_val    = _gvn.intcon(markWord::no_hash);\n-  Node *chk_assigned   = _gvn.transform(new CmpINode( hash_val, no_hash_val));\n-  Node *test_assigned  = _gvn.transform(new BoolNode( chk_assigned, BoolTest::eq));\n+    Node *no_hash_val    = _gvn.intcon(markWord::no_hash);\n+    Node *chk_assigned   = _gvn.transform(new CmpINode( hash_val, no_hash_val));\n+    Node *test_assigned  = _gvn.transform(new BoolNode( chk_assigned, BoolTest::eq));\n@@ -4760,1 +4892,10 @@\n-  generate_slow_guard(test_assigned, slow_region);\n+    generate_slow_guard(test_assigned, slow_region);\n+\n+    result_val->init_req(_fast_path, hash_val);\n+\n+    \/\/ _fast_path2 is not used here.\n+    result_val->del_req(_fast_path2);\n+    result_reg->del_req(_fast_path2);\n+    result_io->del_req(_fast_path2);\n+    result_mem->del_req(_fast_path2);\n+  }\n@@ -4767,1 +4908,0 @@\n-  result_val->init_req(_fast_path, hash_val);\n@@ -4772,0 +4912,5 @@\n+  if (UseCompactObjectHeaders) {\n+    result_io->init_req(_fast_path2, i_o());\n+    result_mem->init_req(_fast_path2, init_mem);\n+  }\n+\n@@ -4773,0 +4918,1 @@\n+  assert(slow_region != nullptr, \"must have slow_region\");\n","filename":"src\/hotspot\/share\/opto\/library_call.cpp","additions":186,"deletions":40,"binary":false,"changes":226,"status":"modified"},{"patch":"@@ -3674,0 +3674,4 @@\n+  if (UseCompactObjectHeaders && UseParallelGC) {\n+    warning(\"Parallel GC is currently not compatible with compact object headers (due to identity hash-code). Disabling compact object headers.\");\n+    FLAG_SET_DEFAULT(UseCompactObjectHeaders, false);\n+  }\n@@ -3693,0 +3697,3 @@\n+  if (UseCompactObjectHeaders && FLAG_IS_DEFAULT(hashCode)) {\n+    hashCode = 6;\n+  }\n","filename":"src\/hotspot\/share\/runtime\/arguments.cpp","additions":7,"deletions":0,"binary":false,"changes":7,"status":"modified"},{"patch":"@@ -54,0 +54,12 @@\n+static uintx objhash(oop obj) {\n+  if (UseCompactObjectHeaders) {\n+    uintx hash = LightweightSynchronizer::get_hash(obj->mark(), obj);\n+    assert(hash != 0, \"should have a hash\");\n+    return hash;\n+  } else {\n+    uintx hash = obj->mark().hash();\n+    assert(hash != 0, \"should have a hash\");\n+    return hash;\n+  }\n+}\n+\n@@ -84,3 +96,1 @@\n-      uintx hash = _obj->mark().hash();\n-      assert(hash != 0, \"should have a hash\");\n-      return hash;\n+      return objhash(_obj);\n@@ -288,0 +298,1 @@\n+      assert(objhash(obj) == (uintx)(*found)->hash(), \"hash must match\");\n@@ -320,1 +331,1 @@\n-       assert(obj->mark().hash() == om->hash(), \"hash must match\");\n+       assert(objhash(obj) == (uintx)om->hash(), \"hash must match\");\n@@ -409,1 +420,1 @@\n-  intptr_t hash = obj->mark().hash();\n+  intptr_t hash = objhash(obj);\n@@ -1220,0 +1231,18 @@\n+\n+uint32_t LightweightSynchronizer::get_hash(markWord mark, oop obj, Klass* klass) {\n+  assert(UseCompactObjectHeaders, \"Only with compact i-hash\");\n+  \/\/assert(mark.is_neutral() | mark.is_fast_locked(), \"only from neutral or fast-locked mark: \" INTPTR_FORMAT, mark.value());\n+  assert(mark.is_hashed(), \"only from hashed or copied object\");\n+  if (mark.is_hashed_expanded()) {\n+    return obj->int_field(klass->hash_offset_in_bytes(obj));\n+  } else {\n+    assert(mark.is_hashed_not_expanded(), \"must be hashed\");\n+    assert(hashCode == 6 || hashCode == 2, \"must have idempotent hashCode\");\n+    \/\/ Already marked as hashed, but not yet copied. Recompute hash and return it.\n+    return ObjectSynchronizer::get_next_hash(nullptr, obj); \/\/ recompute hash\n+  }\n+}\n+\n+uint32_t LightweightSynchronizer::get_hash(markWord mark, oop obj) {\n+  return get_hash(mark, obj, mark.klass());\n+}\n","filename":"src\/hotspot\/share\/runtime\/lightweightSynchronizer.cpp","additions":34,"deletions":5,"binary":false,"changes":39,"status":"modified"},{"patch":"@@ -78,0 +78,5 @@\n+\n+  \/\/ NOTE: May not cause monitor inflation\n+  static uint32_t get_hash(markWord mark, oop obj);\n+  \/\/ For CDS path.\n+  static uint32_t get_hash(markWord mark, oop obj, Klass* klass);\n","filename":"src\/hotspot\/share\/runtime\/lightweightSynchronizer.hpp","additions":5,"deletions":0,"binary":false,"changes":5,"status":"modified"},{"patch":"@@ -67,0 +67,1 @@\n+#include \"utilities\/fastHash.hpp\"\n@@ -935,1 +936,1 @@\n-static intptr_t get_next_hash(Thread* current, oop obj) {\n+intptr_t ObjectSynchronizer::get_next_hash(Thread* current, oop obj) {\n@@ -954,1 +955,1 @@\n-  } else {\n+  } else if (hashCode == 5) {\n@@ -967,0 +968,10 @@\n+  } else {\n+    assert(UseCompactObjectHeaders, \"Only with compact i-hash\");\n+#ifdef _LP64\n+    uint64_t val = cast_from_oop<uint64_t>(obj);\n+    uint32_t hash = FastHash::get_hash32((uint32_t)val, (uint32_t)(val >> 32));\n+#else\n+    uint32_t val = cast_from_oop<uint32_t>(obj);\n+    uint32_t hash = FastHash::get_hash32(val, UCONST64(0xAAAAAAAA));\n+#endif\n+    value= static_cast<intptr_t>(hash);\n@@ -970,2 +981,2 @@\n-  if (value == 0) value = 0xBAD;\n-  assert(value != markWord::no_hash, \"invariant\");\n+  if (hashCode != 6 && value == 0) value = 0xBAD;\n+  assert(value != markWord::no_hash || hashCode == 6, \"invariant\");\n@@ -980,4 +991,23 @@\n-    intptr_t hash = mark.hash();\n-    if (hash != 0) {\n-      return hash;\n-    }\n+    if (UseCompactObjectHeaders) {\n+      if (mark.is_hashed()) {\n+        return LightweightSynchronizer::get_hash(mark, obj);\n+      }\n+      intptr_t hash = ObjectSynchronizer::get_next_hash(current, obj);  \/\/ get a new hash\n+      markWord new_mark;\n+      if (mark.is_not_hashed_expanded()) {\n+        new_mark = mark.set_hashed_expanded();\n+        int offset = mark.klass()->hash_offset_in_bytes(obj);\n+        obj->int_field_put(offset, (jint) hash);\n+      } else {\n+        new_mark = mark.set_hashed_not_expanded();\n+      }\n+      markWord old_mark = obj->cas_set_mark(new_mark, mark);\n+      if (old_mark == mark) {\n+        return hash;\n+      }\n+      mark = old_mark;\n+    } else {\n+      intptr_t hash = mark.hash();\n+      if (hash != 0) {\n+        return hash;\n+      }\n@@ -985,3 +1015,3 @@\n-    hash = get_next_hash(current, obj);\n-    const markWord old_mark = mark;\n-    const markWord new_mark = old_mark.copy_set_hash(hash);\n+      hash = ObjectSynchronizer::get_next_hash(current, obj);\n+      const markWord old_mark = mark;\n+      const markWord new_mark = old_mark.copy_set_hash(hash);\n@@ -989,3 +1019,4 @@\n-    mark = obj->cas_set_mark(new_mark, old_mark);\n-    if (old_mark == mark) {\n-      return hash;\n+      mark = obj->cas_set_mark(new_mark, old_mark);\n+      if (old_mark == mark) {\n+        return hash;\n+      }\n","filename":"src\/hotspot\/share\/runtime\/synchronizer.cpp","additions":45,"deletions":14,"binary":false,"changes":59,"status":"modified"},{"patch":"@@ -212,0 +212,2 @@\n+  static intptr_t get_next_hash(Thread* current, oop obj);\n+\n","filename":"src\/hotspot\/share\/runtime\/synchronizer.hpp","additions":2,"deletions":0,"binary":false,"changes":2,"status":"modified"},{"patch":"@@ -251,0 +251,1 @@\n+  nonstatic_field(InstanceKlass,               _hash_offset,                                  int)                                    \\\n@@ -2514,0 +2515,1 @@\n+  declare_constant(markWord::hashctrl_bits)                               \\\n@@ -2518,0 +2520,1 @@\n+  declare_constant(markWord::hashctrl_shift)                              \\\n@@ -2526,0 +2529,4 @@\n+  declare_constant(markWord::hashctrl_mask)                               \\\n+  declare_constant(markWord::hashctrl_mask_in_place)                      \\\n+  declare_constant(markWord::hashctrl_hashed_mask_in_place)               \\\n+  declare_constant(markWord::hashctrl_expanded_mask_in_place)             \\\n","filename":"src\/hotspot\/share\/runtime\/vmStructs.cpp","additions":7,"deletions":0,"binary":false,"changes":7,"status":"modified"},{"patch":"@@ -0,0 +1,102 @@\n+\/*\n+ * Copyright (c) 2023, Oracle and\/or its affiliates. All rights reserved.\n+ * DO NOT ALTER OR REMOVE COPYRIGHT NOTICES OR THIS FILE HEADER.\n+ *\n+ * This code is free software; you can redistribute it and\/or modify it\n+ * under the terms of the GNU General Public License version 2 only, as\n+ * published by the Free Software Foundation.\n+ *\n+ * This code is distributed in the hope that it will be useful, but WITHOUT\n+ * ANY WARRANTY; without even the implied warranty of MERCHANTABILITY or\n+ * FITNESS FOR A PARTICULAR PURPOSE.  See the GNU General Public License\n+ * version 2 for more details (a copy is included in the LICENSE file that\n+ * accompanied this code).\n+ *\n+ * You should have received a copy of the GNU General Public License version\n+ * 2 along with this work; if not, write to the Free Software Foundation,\n+ * Inc., 51 Franklin St, Fifth Floor, Boston, MA 02110-1301 USA.\n+ *\n+ * Please contact Oracle, 500 Oracle Parkway, Redwood Shores, CA 94065 USA\n+ * or visit www.oracle.com if you need additional information or have any\n+ * questions.\n+ *\n+ *\/\n+\n+#ifndef SHARE_UTILITIES_FASTHASH_HPP\n+#define SHARE_UTILITIES_FASTHASH_HPP\n+\n+#include \"memory\/allStatic.hpp\"\n+\n+class FastHash : public AllStatic {\n+private:\n+  static void fullmul64(uint64_t& hi, uint64_t& lo, uint64_t op1, uint64_t op2) {\n+#if defined(__SIZEOF_INT128__)\n+    __uint128_t prod = static_cast<__uint128_t>(op1) * static_cast<__uint128_t>(op2);\n+    hi = static_cast<uint64_t>(prod >> 64);\n+    lo = static_cast<uint64_t>(prod >>  0);\n+#else\n+    \/* First calculate all of the cross products. *\/\n+    uint64_t lo_lo = (op1 & 0xFFFFFFFF) * (op2 & 0xFFFFFFFF);\n+    uint64_t hi_lo = (op1 >> 32)        * (op2 & 0xFFFFFFFF);\n+    uint64_t lo_hi = (op1 & 0xFFFFFFFF) * (op2 >> 32);\n+    uint64_t hi_hi = (op1 >> 32)        * (op2 >> 32);\n+\n+    \/* Now add the products together. These will never overflow. *\/\n+    uint64_t cross = (lo_lo >> 32) + (hi_lo & 0xFFFFFFFF) + lo_hi;\n+    uint64_t upper = (hi_lo >> 32) + (cross >> 32)        + hi_hi;\n+    hi = upper;\n+    lo = (cross << 32) | (lo_lo & 0xFFFFFFFF);\n+#endif\n+  }\n+\n+  static void fullmul32(uint32_t& hi, uint32_t& lo, uint32_t op1, uint32_t op2) {\n+    uint64_t x64 = op1, y64 = op2, xy64 = x64 * y64;\n+    hi = (uint32_t)(xy64 >> 32);\n+    lo = (uint32_t)(xy64 >>  0);\n+  }\n+\n+  static uint64_t ror64(uint64_t x, uint64_t distance) {\n+    distance = distance & (64 - 1);\n+    return (x >> distance) | (x << (64 - distance));\n+  }\n+\n+  static uint32_t ror32(uint32_t x, uint32_t distance) {\n+    distance = distance & (32 - 1);\n+    return (x >> distance) | (x << (32 - distance));\n+  }\n+\n+public:\n+  static uint64_t get_hash64(uint64_t x, uint64_t y) {\n+    const uint64_t M  = 0x8ADAE89C337954D5;\n+    const uint64_t A  = 0xAAAAAAAAAAAAAAAA; \/\/ REPAA\n+    const uint64_t H0 = (x ^ y), L0 = (x ^ A);\n+\n+    uint64_t U0, V0; fullmul64(U0, V0, L0, M);\n+    const uint64_t Q0 = (H0 * M);\n+    const uint64_t L1 = (Q0 ^ U0);\n+\n+    uint64_t U1, V1; fullmul64(U1, V1, L1, M);\n+    const uint64_t P1 = (V0 ^ M);\n+    const uint64_t Q1 = ror64(P1, L1);\n+    const uint64_t L2 = (Q1 ^ U1);\n+    return V1 ^ L2;\n+  }\n+\n+  static uint32_t get_hash32(uint32_t x, uint32_t y) {\n+    const uint32_t M  = 0x337954D5;\n+    const uint32_t A  = 0xAAAAAAAA; \/\/ REPAA\n+    const uint32_t H0 = (x ^ y), L0 = (x ^ A);\n+\n+    uint32_t U0, V0; fullmul32(U0, V0, L0, M);\n+    const uint32_t Q0 = (H0 * M);\n+    const uint32_t L1 = (Q0 ^ U0);\n+\n+    uint32_t U1, V1; fullmul32(U1, V1, L1, M);\n+    const uint32_t P1 = (V0 ^ M);\n+    const uint32_t Q1 = ror32(P1, L1);\n+    const uint32_t L2 = (Q1 ^ U1);\n+    return V1 ^ L2;\n+  }\n+};\n+\n+#endif\/\/ SHARE_UTILITIES_FASTHASH_HPP\n","filename":"src\/hotspot\/share\/utilities\/fastHash.hpp","additions":102,"deletions":0,"binary":false,"changes":102,"status":"added"},{"patch":"@@ -107,0 +107,7 @@\n+    if (VM.getVM().isCompactObjectHeadersEnabled()) {\n+      Mark mark = getMark();\n+      if (mark.isExpanded()) {\n+        \/\/ Needs extra 4 bytes for identity hash-code.\n+        s += 4;\n+      }\n+    }\n","filename":"src\/jdk.hotspot.agent\/share\/classes\/sun\/jvm\/hotspot\/oops\/Array.java","additions":7,"deletions":0,"binary":false,"changes":7,"status":"modified"},{"patch":"@@ -87,0 +87,1 @@\n+    hashOffset           = new CIntField(type.getCIntegerField(\"_hash_offset\"), 0);\n@@ -153,0 +154,1 @@\n+  private static CIntField hashOffset;\n@@ -243,1 +245,9 @@\n-    return getSizeHelper() * VM.getVM().getAddressSize();\n+    long baseSize = getSizeHelper() * VM.getVM().getAddressSize();\n+    if (VM.getVM().isCompactObjectHeadersEnabled()) {\n+      Mark mark = object.getMark();\n+      if (mark.isExpanded() && (getHashOffset() + 4 \/* size of hash field *\/) > baseSize) {\n+        \/\/ Needs extra word for identity hash-code.\n+        return baseSize + VM.getVM().getBytesPerWord();\n+      }\n+    }\n+    return baseSize;\n@@ -377,0 +387,1 @@\n+  public long      getHashOffset()          { return                hashOffset.getValue(this); }\n","filename":"src\/jdk.hotspot.agent\/share\/classes\/sun\/jvm\/hotspot\/oops\/InstanceKlass.java","additions":12,"deletions":1,"binary":false,"changes":13,"status":"modified"},{"patch":"@@ -58,1 +58,9 @@\n-    return java_lang_Class.getOopSize(o) * VM.getVM().getAddressSize();\n+    long s = java_lang_Class.getOopSize(o) * VM.getVM().getAddressSize();\n+    if (VM.getVM().isCompactObjectHeadersEnabled()) {\n+      Mark mark = o.getMark();\n+      if (mark.isExpanded()) {\n+        \/\/ Needs extra 4 bytes for identity hash-code (and align-up to whole word).\n+        s += VM.getVM().getAddressSize();\n+      }\n+    }\n+    return s;\n","filename":"src\/jdk.hotspot.agent\/share\/classes\/sun\/jvm\/hotspot\/oops\/InstanceMirrorKlass.java","additions":9,"deletions":1,"binary":false,"changes":10,"status":"modified"},{"patch":"@@ -54,0 +54,1 @@\n+    hashCtrlBits        = db.lookupLongConstant(\"markWord::hashctrl_bits\").longValue();\n@@ -57,0 +58,1 @@\n+    hashCtrlShift       = db.lookupLongConstant(\"markWord::hashctrl_shift\").longValue();\n@@ -66,0 +68,4 @@\n+    hashCtrlMask        = db.lookupLongConstant(\"markWord::hashctrl_mask\").longValue();\n+    hashCtrlMaskInPlace = db.lookupLongConstant(\"markWord::hashctrl_mask_in_place\").longValue();\n+    hashCtrlHashedMaskInPlace =   db.lookupLongConstant(\"markWord::hashctrl_hashed_mask_in_place\").longValue();\n+    hashCtrlExpandedMaskInPlace = db.lookupLongConstant(\"markWord::hashctrl_expanded_mask_in_place\").longValue();\n@@ -84,0 +90,1 @@\n+  private static long hashCtrlBits;\n@@ -88,0 +95,1 @@\n+  private static long hashCtrlShift;\n@@ -96,0 +104,4 @@\n+  private static long hashCtrlMask;\n+  private static long hashCtrlMaskInPlace;\n+  private static long hashCtrlHashedMaskInPlace;\n+  private static long hashCtrlExpandedMaskInPlace;\n@@ -195,1 +207,6 @@\n-    return Bits.maskBitsLong(value() >> hashShift, hashMask);\n+    if (VM.getVM().isCompactObjectHeadersEnabled()) {\n+      System.exit(-23);\n+      throw new RuntimeException(\"Compact I-Hash not yet implemented\");\n+    } else {\n+      return Bits.maskBitsLong(value() >> hashShift, hashMask);\n+    }\n@@ -202,0 +219,5 @@\n+  public boolean isExpanded() {\n+    assert(VM.getVM().isCompactObjectHeadersEnabled());\n+    return Bits.maskBitsLong(value(), hashCtrlExpandedMaskInPlace) != 0;\n+  }\n+\n","filename":"src\/jdk.hotspot.agent\/share\/classes\/sun\/jvm\/hotspot\/oops\/Mark.java","additions":23,"deletions":1,"binary":false,"changes":24,"status":"modified"},{"patch":"@@ -135,0 +135,4 @@\n+    if (VM.getVM().isCompactObjectHeadersEnabled()) {\n+      System.exit(-23);\n+        throw new InternalError(\"Not yet implemented\");\n+    }\n","filename":"src\/jdk.hotspot.agent\/share\/classes\/sun\/jvm\/hotspot\/oops\/Oop.java","additions":4,"deletions":0,"binary":false,"changes":4,"status":"modified"},{"patch":"@@ -31,1 +31,1 @@\n-static markWord changedMark()  { return markWord(0x4711); }\n+static markWord changedMark()  { return markWord(0x4712); }\n@@ -59,0 +59,2 @@\n+  FullGCForwarding::begin();\n+\n@@ -77,0 +79,2 @@\n+\n+  FullGCForwarding::end();\n","filename":"test\/hotspot\/gtest\/gc\/shared\/test_preservedMarks.cpp","additions":5,"deletions":1,"binary":false,"changes":6,"status":"modified"},{"patch":"@@ -95,1 +95,5 @@\n-  assert_test_pattern(h_obj, \"is_unlocked hash=0x\");\n+  if (UseCompactObjectHeaders) {\n+    assert_test_pattern(h_obj, \"is_unlocked hash is-hashed=true is-copied=false\");\n+  } else {\n+    assert_test_pattern(h_obj, \"is_unlocked hash=0x\");\n+  }\n","filename":"test\/hotspot\/gtest\/oops\/test_markWord.cpp","additions":5,"deletions":1,"binary":false,"changes":6,"status":"modified"},{"patch":"@@ -77,0 +77,2 @@\n+            baseArgs.add(\"-XX:+UnlockExperimentalVMOptions\");\n+            baseArgs.add(\"-XX:-UseCompactObjectHeaders\");\n","filename":"test\/hotspot\/jtreg\/runtime\/cds\/DeterministicDump.java","additions":2,"deletions":0,"binary":false,"changes":2,"status":"modified"},{"patch":"@@ -30,0 +30,1 @@\n+ * @requires vm.opt.final.UseCompactObjectHeaders == false\n@@ -51,0 +52,1 @@\n+ * @requires vm.opt.final.UseCompactObjectHeaders == false\n","filename":"test\/hotspot\/jtreg\/runtime\/cds\/appcds\/TestParallelGCWithCDS.java","additions":2,"deletions":0,"binary":false,"changes":2,"status":"modified"},{"patch":"@@ -32,1 +32,1 @@\n- * @requires (vm.gc==\"null\")\n+ * @requires (vm.gc==\"null\") & vm.opt.final.UseCompactObjectHeaders == false\n","filename":"test\/hotspot\/jtreg\/runtime\/cds\/appcds\/cacheObject\/OpenArchiveRegion.java","additions":1,"deletions":1,"binary":false,"changes":2,"status":"modified"}]}