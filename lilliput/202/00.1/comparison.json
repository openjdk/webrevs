{"files":[{"patch":"@@ -5802,0 +5802,3 @@\n+opclass memory_noindex(indirect,\n+                       indOffI1, indOffL1,indOffI2, indOffL2, indOffI4, indOffL4, indOffI8, indOffL8,\n+                       indirectN, indOffIN, indOffLN, indirectX2P, indOffX2P);\n@@ -6739,1 +6742,1 @@\n-instruct loadNKlassCompactHeaders(iRegNNoSp dst, memory4 mem)\n+instruct loadNKlassCompactHeaders(iRegNNoSp dst, memory_noindex mem)\n@@ -6747,1 +6750,1 @@\n-    \"lsrw  $dst, $dst, markWord::klass_shift_at_offset\"\n+    \"lsrw  $dst, $dst, markWord::klass_shift\"\n@@ -6750,4 +6753,5 @@\n-    \/\/ inlined aarch64_enc_ldrw\n-    loadStore(masm, &MacroAssembler::ldrw, $dst$$Register, $mem->opcode(),\n-              as_Register($mem$$base), $mem$$index, $mem$$scale, $mem$$disp, 4);\n-    __ lsrw($dst$$Register, $dst$$Register, markWord::klass_shift_at_offset);\n+    assert($mem$$index$$Register == noreg, \"must not have indexed address\");\n+    \/\/ The incoming address is pointing into obj-start + klass_offset_in_bytes. We need to extract\n+    \/\/ obj-start, so that we can load from the object's mark-word instead.\n+    __ ldrw($dst$$Register, Address($mem$$base$$Register, $mem$$disp - Type::klass_offset()));\n+    __ lsrw($dst$$Register, $dst$$Register, markWord::klass_shift);\n","filename":"src\/hotspot\/cpu\/aarch64\/aarch64.ad","additions":10,"deletions":6,"binary":false,"changes":16,"status":"modified"},{"patch":"@@ -5043,2 +5043,2 @@\n-  ldr(dst, Address(src, oopDesc::mark_offset_in_bytes()));\n-  lsr(dst, dst, markWord::klass_shift);\n+  ldrw(dst, Address(src, oopDesc::mark_offset_in_bytes()));\n+  lsrw(dst, dst, markWord::klass_shift);\n@@ -5898,0 +5898,1 @@\n+  Register cnt2 = tmp2;  \/\/ cnt2 only used in array length compare\n@@ -5900,1 +5901,0 @@\n-  int klass_offset  = arrayOopDesc::klass_offset_in_bytes();\n@@ -5904,10 +5904,0 @@\n-  \/\/ When the length offset is not aligned to 8 bytes,\n-  \/\/ then we align it down. This is valid because the new\n-  \/\/ offset will always be the klass which is the same\n-  \/\/ for type arrays.\n-  int start_offset = align_down(length_offset, BytesPerWord);\n-  int extra_length = base_offset - start_offset;\n-  assert(start_offset == length_offset || start_offset == klass_offset,\n-         \"start offset must be 8-byte-aligned or be the klass offset\");\n-  assert(base_offset != start_offset, \"must include the length field\");\n-  extra_length = extra_length \/ elem_size; \/\/ We count in elements, not bytes.\n@@ -5947,4 +5937,5 @@\n-    \/\/ Increase loop counter by diff between base- and actual start-offset.\n-    addw(cnt1, cnt1, extra_length);\n-    lea(a1, Address(a1, start_offset));\n-    lea(a2, Address(a2, start_offset));\n+    ldrw(cnt2, Address(a2, length_offset));\n+    eorw(tmp5, cnt1, cnt2);\n+    cbnzw(tmp5, DONE);\n+    lea(a1, Address(a1, base_offset));\n+    lea(a2, Address(a2, base_offset));\n@@ -6013,3 +6004,1 @@\n-    \/\/ Increase loop counter by diff between base- and actual start-offset.\n-    addw(cnt1, cnt1, extra_length);\n-\n+    ldrw(cnt2, Address(a2, length_offset));\n@@ -6020,1 +6009,1 @@\n-    ldr(tmp3, Address(pre(a1, start_offset)));\n+    ldr(tmp3, Address(pre(a1, base_offset)));\n@@ -6023,1 +6012,1 @@\n-    ldr(tmp4, Address(pre(a2, start_offset)));\n+    ldr(tmp4, Address(pre(a2, base_offset)));\n@@ -6025,0 +6014,2 @@\n+    cmp(cnt2, cnt1);\n+    br(NE, DONE);\n@@ -6056,1 +6047,3 @@\n-    ldr(tmp4, Address(pre(a2, start_offset)));\n+    ldr(tmp4, Address(pre(a2, base_offset)));\n+    cmp(cnt2, cnt1);\n+    br(NE, DONE);\n@@ -6077,0 +6070,3 @@\n+    cmp(cnt2, cnt1);\n+    br(NE, DONE);\n+    cbz(cnt1, SAME);\n@@ -6078,2 +6074,2 @@\n-    ldr(tmp3, Address(a1, start_offset));\n-    ldr(tmp4, Address(a2, start_offset));\n+    ldr(tmp3, Address(a1, base_offset));\n+    ldr(tmp4, Address(a2, base_offset));\n","filename":"src\/hotspot\/cpu\/aarch64\/macroAssembler_aarch64.cpp","additions":21,"deletions":25,"binary":false,"changes":46,"status":"modified"},{"patch":"@@ -3638,1 +3638,0 @@\n-    assert(is_aligned(header_size, BytesPerLong), \"oop header size must be 8-byte-aligned\");\n","filename":"src\/hotspot\/cpu\/aarch64\/templateTable_aarch64.cpp","additions":0,"deletions":1,"binary":false,"changes":1,"status":"modified"},{"patch":"@@ -4709,2 +4709,3 @@\n-    __ lwu(as_Register($dst$$reg), Address(as_Register($mem$$base), $mem$$disp));\n-    __ srli(as_Register($dst$$reg), as_Register($dst$$reg), (unsigned) markWord::klass_shift_at_offset);\n+    Unimplemented();\n+    \/\/ __ lwu(as_Register($dst$$reg), Address(as_Register($mem$$base), $mem$$disp));\n+    \/\/ __ srli(as_Register($dst$$reg), as_Register($dst$$reg), (unsigned) markWord::klass_shift_at_offset);\n","filename":"src\/hotspot\/cpu\/riscv\/riscv.ad","additions":3,"deletions":2,"binary":false,"changes":5,"status":"modified"},{"patch":"@@ -1,1 +1,1 @@\n-\/*\n+ \/*\n@@ -37,1 +37,1 @@\n-         CompressedKlassPointers::narrow_klass_pointer_bits() == 22, \"Rethink if we ever use different nKlass bit sizes\");\n+         CompressedKlassPointers::narrow_klass_pointer_bits() == 19, \"Rethink if we ever use different nKlass bit sizes\");\n","filename":"src\/hotspot\/cpu\/x86\/compressedKlass_x86.cpp","additions":2,"deletions":2,"binary":false,"changes":4,"status":"modified"},{"patch":"@@ -5139,2 +5139,2 @@\n-  movq(dst, Address(src, oopDesc::mark_offset_in_bytes()));\n-  shrq(dst, markWord::klass_shift);\n+  movl(dst, Address(src, oopDesc::mark_offset_in_bytes()));\n+  shrl(dst, markWord::klass_shift);\n","filename":"src\/hotspot\/cpu\/x86\/macroAssembler_x86.cpp","additions":2,"deletions":2,"binary":false,"changes":4,"status":"modified"},{"patch":"@@ -60,0 +60,5 @@\n+  if (UseCompactObjectHeaders) {\n+    \/\/ Don't generate anything else and always take the slow-path for now.\n+    return;\n+  }\n+\n","filename":"src\/hotspot\/cpu\/x86\/sharedRuntime_x86.cpp","additions":5,"deletions":0,"binary":false,"changes":5,"status":"modified"},{"patch":"@@ -3569,2 +3569,1 @@\n-      assert(is_aligned(oopDesc::base_offset_in_bytes(), BytesPerLong), \"oop base offset must be 8-byte-aligned\");\n-      __ decrement(rdx, oopDesc::base_offset_in_bytes());\n+      __ decrement(rdx, align_up(oopDesc::base_offset_in_bytes(), BytesPerLong));\n","filename":"src\/hotspot\/cpu\/x86\/templateTable_x86.cpp","additions":1,"deletions":2,"binary":false,"changes":3,"status":"modified"},{"patch":"@@ -4399,1 +4399,1 @@\n-    \"shrl    $dst, markWord::klass_shift_at_offset\"\n+    \"shrl    $dst, markWord::klass_shift\"\n@@ -4402,0 +4402,4 @@\n+    \/\/ The incoming address is pointing into obj-start + Type::klass_offset(). We need to extract\n+    \/\/ obj-start, so that we can load from the object's mark-word instead.\n+    Register d = $dst$$Register;\n+    Address  s = ($mem$$Address).plus_disp(-Type::klass_offset());\n@@ -4403,5 +4407,4 @@\n-      __ eshrl($dst$$Register, $mem$$Address, markWord::klass_shift_at_offset, false);\n-    }\n-    else {\n-      __ movl($dst$$Register, $mem$$Address);\n-      __ shrl($dst$$Register, markWord::klass_shift_at_offset);\n+      __ eshrl(d, s, markWord::klass_shift, false);\n+    } else {\n+      __ movl(d, s);\n+      __ shrl(d, markWord::klass_shift);\n","filename":"src\/hotspot\/cpu\/x86\/x86_64.ad","additions":9,"deletions":6,"binary":false,"changes":15,"status":"modified"},{"patch":"@@ -1333,1 +1333,3 @@\n-        byte_size = source_oop->size() * BytesPerWord;\n+        size_t old_size = source_oop->size();\n+        size_t new_size = source_oop->copy_size_cds(old_size, source_oop->mark());\n+        byte_size = new_size * BytesPerWord;\n","filename":"src\/hotspot\/share\/cds\/archiveBuilder.cpp","additions":3,"deletions":1,"binary":false,"changes":4,"status":"modified"},{"patch":"@@ -426,1 +426,3 @@\n-  size_t byte_size = src_obj->size() * HeapWordSize;\n+  size_t old_size = src_obj->size();\n+  size_t new_size = src_obj->copy_size_cds(old_size, src_obj->mark());\n+  size_t byte_size = new_size * HeapWordSize;\n@@ -447,1 +449,1 @@\n-  memcpy(to, from, byte_size);\n+  memcpy(to, from, old_size * HeapWordSize);\n@@ -587,0 +589,1 @@\n+    assert(fake_oop->mark().narrow_klass() != 0, \"must not be null\");\n@@ -599,1 +602,7 @@\n-      fake_oop->set_mark(markWord::prototype().set_narrow_klass(nk).copy_set_hash(src_hash));\n+      markWord m = markWord::prototype().set_narrow_klass(nk);\n+      m = m.copy_hashctrl_from(src_obj->mark());\n+      fake_oop->set_mark(m);\n+      if (m.is_hashed_not_expanded()) {\n+        fake_oop->initialize_hash_if_necessary(src_obj, src_klass, m);\n+      }\n+      assert(!fake_oop->mark().is_not_hashed_expanded() && !fake_oop->mark().is_hashed_not_expanded(), \"must not be not-hashed-moved and not be hashed-not-moved\");\n@@ -602,0 +611,2 @@\n+      DEBUG_ONLY(intptr_t archived_hash = fake_oop->identity_hash());\n+      assert(src_hash == archived_hash, \"Different hash codes: original \" INTPTR_FORMAT \", archived \" INTPTR_FORMAT, src_hash, archived_hash);\n@@ -604,3 +615,0 @@\n-\n-    DEBUG_ONLY(intptr_t archived_hash = fake_oop->identity_hash());\n-    assert(src_hash == archived_hash, \"Different hash codes: original \" INTPTR_FORMAT \", archived \" INTPTR_FORMAT, src_hash, archived_hash);\n","filename":"src\/hotspot\/share\/cds\/archiveHeapWriter.cpp","additions":14,"deletions":6,"binary":false,"changes":20,"status":"modified"},{"patch":"@@ -419,1 +419,1 @@\n-      oop m = java_lang_Class::create_basic_type_mirror(type2name(bt), bt, CHECK);\n+      oop m = java_lang_Class::create_basic_type_mirror(type2name(bt), bt, true, CHECK);\n@@ -594,0 +594,1 @@\n+  assert(!UseCompactObjectHeaders || scratch_m->mark().is_not_hashed_expanded(), \"scratch mirror must have not-hashed-expanded state\");\n@@ -595,0 +596,1 @@\n+    intptr_t orig_mark = orig_mirror->mark().value();\n@@ -597,2 +599,17 @@\n-      narrowKlass nk = CompressedKlassPointers::encode(orig_mirror->klass());\n-      scratch_m->set_mark(markWord::prototype().set_narrow_klass(nk).copy_set_hash(src_hash));\n+      \/\/ We leave the cases not_hashed\/not_hashed_expanded as they are.\n+      assert(orig_mirror->mark().is_hashed_not_expanded() || orig_mirror->mark().is_hashed_expanded(), \"must be hashed\");\n+      Klass* orig_klass = orig_mirror->klass();\n+      narrowKlass nk = CompressedKlassPointers::encode(orig_klass);\n+      markWord mark = markWord::prototype().set_narrow_klass(nk);\n+      mark = mark.copy_hashctrl_from(orig_mirror->mark());\n+      if (mark.is_hashed_not_expanded()) {\n+        scratch_m->initialize_hash_if_necessary(orig_mirror, orig_klass, mark);\n+      } else {\n+        assert(mark.is_hashed_expanded(), \"must be hashed & moved\");\n+        int offset = orig_klass->hash_offset_in_bytes(orig_mirror, mark);\n+        assert(offset >= 8, \"hash offset must not be in header\");\n+        scratch_m->int_field_put(offset, (jint) src_hash);\n+        scratch_m->set_mark(mark);\n+      }\n+      assert(scratch_m->mark().is_hashed_expanded(), \"must be hashed & moved\");\n+      assert(scratch_m->mark().is_not_hashed_expanded() || scratch_m->mark().is_hashed_expanded(), \"must be not hashed and expanded\");\n@@ -601,0 +618,2 @@\n+      DEBUG_ONLY(intptr_t archived_hash = scratch_m->identity_hash());\n+      assert(src_hash == archived_hash, \"Different hash codes: original \" INTPTR_FORMAT \", archived \" INTPTR_FORMAT, src_hash, archived_hash);\n@@ -603,3 +622,0 @@\n-\n-    DEBUG_ONLY(intptr_t archived_hash = scratch_m->identity_hash());\n-    assert(src_hash == archived_hash, \"Different hash codes: original \" INTPTR_FORMAT \", archived \" INTPTR_FORMAT, src_hash, archived_hash);\n","filename":"src\/hotspot\/share\/cds\/heapShared.cpp","additions":22,"deletions":6,"binary":false,"changes":28,"status":"modified"},{"patch":"@@ -298,0 +298,4 @@\n+  int hash_offset_in_bytes() const {\n+    return get_instanceKlass()->hash_offset_in_bytes(nullptr, markWord(0));\n+  }\n+\n","filename":"src\/hotspot\/share\/ci\/ciInstanceKlass.hpp","additions":4,"deletions":0,"binary":false,"changes":4,"status":"modified"},{"patch":"@@ -75,0 +75,3 @@\n+  bool is_mirror_instance_klass() { return get_Klass()->is_mirror_instance_klass(); }\n+  bool is_reference_instance_klass() { return get_Klass()->is_reference_instance_klass(); }\n+\n","filename":"src\/hotspot\/share\/ci\/ciKlass.hpp","additions":3,"deletions":0,"binary":false,"changes":3,"status":"modified"},{"patch":"@@ -58,0 +58,3 @@\n+  if (UseCompactObjectHeaders) {\n+    return os::random();\n+  }\n","filename":"src\/hotspot\/share\/classfile\/altHashing.cpp","additions":3,"deletions":0,"binary":false,"changes":3,"status":"modified"},{"patch":"@@ -4899,0 +4899,4 @@\n+int ClassFileParser::hash_offset() const {\n+  return _field_info->_hash_offset;\n+}\n+\n","filename":"src\/hotspot\/share\/classfile\/classFileParser.cpp","additions":4,"deletions":0,"binary":false,"changes":4,"status":"modified"},{"patch":"@@ -75,0 +75,1 @@\n+  int _hash_offset;\n@@ -508,0 +509,1 @@\n+  int hash_offset() const;\n","filename":"src\/hotspot\/share\/classfile\/classFileParser.hpp","additions":2,"deletions":0,"binary":false,"changes":2,"status":"modified"},{"patch":"@@ -213,0 +213,17 @@\n+\/\/ Finds a slot for the identity hash-code.\n+\/\/ Same basic algorithm as above add() method, but simplified\n+\/\/ and does not actually insert the field.\n+int FieldLayout::find_hash_offset() {\n+  LayoutRawBlock* start = this->_start;\n+  LayoutRawBlock* last = last_block();\n+  LayoutRawBlock* cursor = start;\n+  while (cursor != last) {\n+    assert(cursor != nullptr, \"Sanity check\");\n+    if (cursor->kind() == LayoutRawBlock::EMPTY && cursor->fit(4, 1)) {\n+      break;\n+    }\n+    cursor = cursor->next_block();\n+  }\n+  return cursor->offset();\n+}\n+\n@@ -700,0 +717,3 @@\n+  if (UseCompactObjectHeaders) {\n+    _info->_hash_offset   = _layout->find_hash_offset();\n+  }\n","filename":"src\/hotspot\/share\/classfile\/fieldLayoutBuilder.cpp","additions":20,"deletions":0,"binary":false,"changes":20,"status":"modified"},{"patch":"@@ -188,0 +188,1 @@\n+  int find_hash_offset();\n","filename":"src\/hotspot\/share\/classfile\/fieldLayoutBuilder.hpp","additions":1,"deletions":0,"binary":false,"changes":1,"status":"modified"},{"patch":"@@ -1067,1 +1067,1 @@\n-  oop mirror_oop = InstanceMirrorKlass::cast(vmClasses::Class_klass())->allocate_instance(k, CHECK);\n+  oop mirror_oop = InstanceMirrorKlass::cast(vmClasses::Class_klass())->allocate_instance(k, is_scratch, CHECK);\n@@ -1368,1 +1368,1 @@\n-oop java_lang_Class::create_basic_type_mirror(const char* basic_type_name, BasicType type, TRAPS) {\n+oop java_lang_Class::create_basic_type_mirror(const char* basic_type_name, BasicType type, bool is_scratch, TRAPS) {\n@@ -1370,1 +1370,1 @@\n-  oop java_class = InstanceMirrorKlass::cast(vmClasses::Class_klass())->allocate_instance(nullptr, CHECK_NULL);\n+  oop java_class = InstanceMirrorKlass::cast(vmClasses::Class_klass())->allocate_instance(nullptr, is_scratch, CHECK_NULL);\n","filename":"src\/hotspot\/share\/classfile\/javaClasses.cpp","additions":3,"deletions":3,"binary":false,"changes":6,"status":"modified"},{"patch":"@@ -284,1 +284,1 @@\n-  static oop  create_basic_type_mirror(const char* basic_type_name, BasicType type, TRAPS);\n+  static oop  create_basic_type_mirror(const char* basic_type_name, BasicType type, bool is_scratch, TRAPS);\n","filename":"src\/hotspot\/share\/classfile\/javaClasses.hpp","additions":1,"deletions":1,"binary":false,"changes":2,"status":"modified"},{"patch":"@@ -288,1 +288,1 @@\n-  assert(java_lang_Class::is_instance(java_class), \"must be a Class object\");\n+  \/\/assert(java_lang_Class::is_instance(java_class), \"must be a Class object\");\n","filename":"src\/hotspot\/share\/classfile\/javaClasses.inline.hpp","additions":1,"deletions":1,"binary":false,"changes":2,"status":"modified"},{"patch":"@@ -36,1 +36,0 @@\n-#include \"gc\/shared\/fullGCForwarding.hpp\"\n@@ -249,2 +248,0 @@\n-\n-  FullGCForwarding::initialize_flags(heap_reserved_size_bytes());\n","filename":"src\/hotspot\/share\/gc\/g1\/g1Arguments.cpp","additions":0,"deletions":3,"binary":false,"changes":3,"status":"modified"},{"patch":"@@ -79,1 +79,1 @@\n-#include \"gc\/shared\/fullGCForwarding.hpp\"\n+#include \"gc\/shared\/fullGCForwarding.inline.hpp\"\n","filename":"src\/hotspot\/share\/gc\/g1\/g1CollectedHeap.cpp","additions":1,"deletions":1,"binary":false,"changes":2,"status":"modified"},{"patch":"@@ -214,0 +214,2 @@\n+  FullGCForwarding::begin();\n+\n@@ -226,0 +228,2 @@\n+  FullGCForwarding::end();\n+\n","filename":"src\/hotspot\/share\/gc\/g1\/g1FullCollector.cpp","additions":4,"deletions":0,"binary":false,"changes":4,"status":"modified"},{"patch":"@@ -62,0 +62,1 @@\n+  assert(obj_addr != destination, \"only copy actually-moving objects\");\n@@ -66,0 +67,1 @@\n+  cast_to_oop(destination)->initialize_hash_if_necessary(obj);\n","filename":"src\/hotspot\/share\/gc\/g1\/g1FullGCCompactTask.cpp","additions":2,"deletions":0,"binary":false,"changes":2,"status":"modified"},{"patch":"@@ -99,0 +99,4 @@\n+  size_t old_size = size;\n+  size_t new_size = object->copy_size(old_size, object->mark());\n+  size = cast_from_oop<HeapWord*>(object) != _compaction_top ? new_size : old_size;\n+\n@@ -102,0 +106,1 @@\n+    size = cast_from_oop<HeapWord*>(object) != _compaction_top ? new_size : old_size;\n@@ -156,2 +161,3 @@\n-  size_t obj_size = obj->size();\n-  uint num_regions = (uint)G1CollectedHeap::humongous_obj_size_in_regions(obj_size);\n+  size_t old_size = obj->size();\n+  size_t new_size = obj->copy_size(old_size, obj->mark());\n+  uint num_regions = (uint)G1CollectedHeap::humongous_obj_size_in_regions(new_size);\n@@ -175,0 +181,1 @@\n+  assert(hr->bottom() != dest_hr->bottom(), \"assuming actual humongous move\");\n","filename":"src\/hotspot\/share\/gc\/g1\/g1FullGCCompactionPoint.cpp","additions":9,"deletions":2,"binary":false,"changes":11,"status":"modified"},{"patch":"@@ -478,1 +478,2 @@\n-  const size_t word_sz = old->size_given_klass(klass);\n+  const size_t old_size = old->size_given_mark_and_klass(old_mark, klass);\n+  const size_t word_sz = old->copy_size(old_size, old_mark);\n@@ -516,1 +517,1 @@\n-  Copy::aligned_disjoint_words(cast_from_oop<HeapWord*>(old), obj_ptr, word_sz);\n+  Copy::aligned_disjoint_words(cast_from_oop<HeapWord*>(old), obj_ptr, old_size);\n@@ -533,0 +534,2 @@\n+    obj->initialize_hash_if_necessary(old);\n+\n","filename":"src\/hotspot\/share\/gc\/g1\/g1ParScanThreadState.cpp","additions":5,"deletions":2,"binary":false,"changes":7,"status":"modified"},{"patch":"@@ -119,1 +119,1 @@\n-  const idx_t res_bit = _beg_bits.find_last_set_bit_aligned_left(beg_bit, end_bit);\n+  const idx_t res_bit = _beg_bits.find_last_set_bit(beg_bit, end_bit);\n","filename":"src\/hotspot\/share\/gc\/parallel\/parMarkBitMap.inline.hpp","additions":1,"deletions":1,"binary":false,"changes":2,"status":"modified"},{"patch":"@@ -29,1 +29,0 @@\n-#include \"gc\/shared\/fullGCForwarding.hpp\"\n@@ -97,2 +96,0 @@\n-\n-  FullGCForwarding::initialize_flags(heap_reserved_size_bytes());\n","filename":"src\/hotspot\/share\/gc\/parallel\/parallelArguments.cpp","additions":0,"deletions":3,"binary":false,"changes":3,"status":"modified"},{"patch":"@@ -32,0 +32,1 @@\n+#include \"gc\/parallel\/psParallelCompactNew.inline.hpp\"\n@@ -122,2 +123,8 @@\n-  if (!PSParallelCompact::initialize_aux_data()) {\n-    return JNI_ENOMEM;\n+  if (UseCompactObjectHeaders) {\n+    if (!PSParallelCompactNew::initialize_aux_data()) {\n+      return JNI_ENOMEM;\n+    }\n+  } else {\n+    if (!PSParallelCompact::initialize_aux_data()) {\n+      return JNI_ENOMEM;\n+    }\n@@ -186,1 +193,5 @@\n-  PSParallelCompact::post_initialize();\n+  if (UseCompactObjectHeaders) {\n+    PSParallelCompactNew::post_initialize();\n+  } else {\n+    PSParallelCompact::post_initialize();\n+  }\n@@ -387,1 +398,5 @@\n-  PSParallelCompact::invoke(clear_all_soft_refs);\n+  if (UseCompactObjectHeaders) {\n+    PSParallelCompactNew::invoke(clear_all_soft_refs, false \/* serial *\/);\n+  } else {\n+    PSParallelCompact::invoke(clear_all_soft_refs);\n+  }\n@@ -425,1 +440,5 @@\n-    PSParallelCompact::invoke(clear_all_soft_refs);\n+    if (UseCompactObjectHeaders) {\n+      PSParallelCompactNew::invoke(clear_all_soft_refs, false \/* serial *\/);\n+    } else {\n+      PSParallelCompact::invoke(clear_all_soft_refs);\n+    }\n@@ -436,0 +455,9 @@\n+  if (UseCompactObjectHeaders) {\n+    PSParallelCompactNew::invoke(true \/* clear_soft_refs *\/, true \/* serial *\/);\n+  }\n+\n+  result = expand_heap_and_allocate(size, is_tlab);\n+  if (result != nullptr) {\n+    return result;\n+  }\n+\n@@ -518,1 +546,5 @@\n-  PSParallelCompact::invoke(clear_soft_refs);\n+  if (UseCompactObjectHeaders) {\n+    PSParallelCompactNew::invoke(clear_soft_refs, false \/* serial *\/);\n+  } else {\n+    PSParallelCompact::invoke(clear_soft_refs);\n+  }\n@@ -661,1 +693,5 @@\n-  PSParallelCompact::print_on(st);\n+  if (UseCompactObjectHeaders) {\n+    PSParallelCompactNew::print_on(st);\n+  } else {\n+    PSParallelCompact::print_on(st);\n+  }\n@@ -671,1 +707,5 @@\n-  log_debug(gc, heap, exit)(\"Accumulated old generation GC time %3.7f secs\", PSParallelCompact::accumulated_time()->seconds());\n+  if (UseCompactObjectHeaders) {\n+    log_debug(gc, heap, exit)(\"Accumulated old generation GC time %3.7f secs\", PSParallelCompactNew::accumulated_time()->seconds());\n+  } else {\n+    log_debug(gc, heap, exit)(\"Accumulated old generation GC time %3.7f secs\", PSParallelCompact::accumulated_time()->seconds());\n+  }\n","filename":"src\/hotspot\/share\/gc\/parallel\/parallelScavengeHeap.cpp","additions":48,"deletions":8,"binary":false,"changes":56,"status":"modified"},{"patch":"@@ -0,0 +1,167 @@\n+\/*\n+ * Copyright (c) 2005, 2025, Oracle and\/or its affiliates. All rights reserved.\n+ * Copyright Amazon.com Inc. or its affiliates. All Rights Reserved.\n+ * DO NOT ALTER OR REMOVE COPYRIGHT NOTICES OR THIS FILE HEADER.\n+ *\n+ * This code is free software; you can redistribute it and\/or modify it\n+ * under the terms of the GNU General Public License version 2 only, as\n+ * published by the Free Software Foundation.\n+ *\n+ * This code is distributed in the hope that it will be useful, but WITHOUT\n+ * ANY WARRANTY; without even the implied warranty of MERCHANTABILITY or\n+ * FITNESS FOR A PARTICULAR PURPOSE.  See the GNU General Public License\n+ * version 2 for more details (a copy is included in the LICENSE file that\n+ * accompanied this code).\n+ *\n+ * You should have received a copy of the GNU General Public License version\n+ * 2 along with this work; if not, write to the Free Software Foundation,\n+ * Inc., 51 Franklin St, Fifth Floor, Boston, MA 02110-1301 USA.\n+ *\n+ * Please contact Oracle, 500 Oracle Parkway, Redwood Shores, CA 94065 USA\n+ * or visit www.oracle.com if you need additional information or have any\n+ * questions.\n+ *\n+ *\/\n+\n+#include \"gc\/parallel\/objectStartArray.hpp\"\n+#include \"gc\/parallel\/parMarkBitMap.inline.hpp\"\n+#include \"gc\/parallel\/parallelScavengeHeap.hpp\"\n+#include \"gc\/parallel\/psCompactionManagerNew.inline.hpp\"\n+#include \"gc\/parallel\/psOldGen.hpp\"\n+#include \"gc\/parallel\/psParallelCompactNew.inline.hpp\"\n+#include \"gc\/shared\/partialArraySplitter.inline.hpp\"\n+#include \"gc\/shared\/partialArrayState.hpp\"\n+#include \"gc\/shared\/preservedMarks.inline.hpp\"\n+#include \"memory\/iterator.inline.hpp\"\n+\n+PSOldGen*                  ParCompactionManagerNew::_old_gen = nullptr;\n+ParCompactionManagerNew**  ParCompactionManagerNew::_manager_array = nullptr;\n+\n+ParCompactionManagerNew::PSMarkTasksQueueSet*  ParCompactionManagerNew::_marking_stacks = nullptr;\n+PartialArrayStateManager* ParCompactionManagerNew::_partial_array_state_manager = nullptr;\n+\n+ObjectStartArray*    ParCompactionManagerNew::_start_array = nullptr;\n+ParMarkBitMap*       ParCompactionManagerNew::_mark_bitmap = nullptr;\n+\n+PreservedMarksSet* ParCompactionManagerNew::_preserved_marks_set = nullptr;\n+\n+ParCompactionManagerNew::ParCompactionManagerNew(PreservedMarks* preserved_marks,\n+                                           ReferenceProcessor* ref_processor,\n+                                           uint parallel_gc_threads)\n+  :_partial_array_splitter(_partial_array_state_manager, parallel_gc_threads, ObjArrayMarkingStride),\n+   _mark_and_push_closure(this, ref_processor) {\n+\n+  _old_gen = ParallelScavengeHeap::old_gen();\n+  _start_array = old_gen()->start_array();\n+\n+  _preserved_marks = preserved_marks;\n+}\n+\n+void ParCompactionManagerNew::initialize(ParMarkBitMap* mbm) {\n+  assert(ParallelScavengeHeap::heap() != nullptr, \"Needed for initialization\");\n+  assert(PSParallelCompactNew::ref_processor() != nullptr, \"precondition\");\n+  assert(ParallelScavengeHeap::heap()->workers().max_workers() != 0, \"Not initialized?\");\n+\n+  _mark_bitmap = mbm;\n+\n+  uint parallel_gc_threads = ParallelScavengeHeap::heap()->workers().max_workers();\n+\n+  assert(_manager_array == nullptr, \"Attempt to initialize twice\");\n+  _manager_array = NEW_C_HEAP_ARRAY(ParCompactionManagerNew*, parallel_gc_threads, mtGC);\n+\n+  assert(_partial_array_state_manager == nullptr, \"Attempt to initialize twice\");\n+  _partial_array_state_manager\n+    = new PartialArrayStateManager(parallel_gc_threads);\n+  _marking_stacks = new PSMarkTasksQueueSet(parallel_gc_threads);\n+\n+  _preserved_marks_set = new PreservedMarksSet(true);\n+  _preserved_marks_set->init(parallel_gc_threads);\n+\n+  \/\/ Create and register the ParCompactionManagerNew(s) for the worker threads.\n+  for(uint i=0; i<parallel_gc_threads; i++) {\n+    _manager_array[i] = new ParCompactionManagerNew(_preserved_marks_set->get(i),\n+                                                 PSParallelCompactNew::ref_processor(),\n+                                                 parallel_gc_threads);\n+    marking_stacks()->register_queue(i, _manager_array[i]->marking_stack());\n+  }\n+}\n+\n+void ParCompactionManagerNew::flush_all_string_dedup_requests() {\n+  uint parallel_gc_threads = ParallelScavengeHeap::heap()->workers().max_workers();\n+  for (uint i=0; i<parallel_gc_threads; i++) {\n+    _manager_array[i]->flush_string_dedup_requests();\n+  }\n+}\n+\n+ParCompactionManagerNew*\n+ParCompactionManagerNew::gc_thread_compaction_manager(uint index) {\n+  assert(index < ParallelGCThreads, \"index out of range\");\n+  assert(_manager_array != nullptr, \"Sanity\");\n+  return _manager_array[index];\n+}\n+\n+void ParCompactionManagerNew::push_objArray(oop obj) {\n+  assert(obj->is_objArray(), \"precondition\");\n+  _mark_and_push_closure.do_klass(obj->klass());\n+\n+  objArrayOop obj_array = objArrayOop(obj);\n+  size_t array_length = obj_array->length();\n+  size_t initial_chunk_size =\n+    _partial_array_splitter.start(&_marking_stack, obj_array, nullptr, array_length);\n+  follow_array(obj_array, 0, initial_chunk_size);\n+}\n+\n+void ParCompactionManagerNew::process_array_chunk(PartialArrayState* state, bool stolen) {\n+  \/\/ Access before release by claim().\n+  oop obj = state->source();\n+  PartialArraySplitter::Claim claim =\n+    _partial_array_splitter.claim(state, &_marking_stack, stolen);\n+  follow_array(objArrayOop(obj), claim._start, claim._end);\n+}\n+\n+void ParCompactionManagerNew::follow_marking_stacks() {\n+  ScannerTask task;\n+  do {\n+    \/\/ First, try to move tasks from the overflow stack into the shared buffer, so\n+    \/\/ that other threads can steal. Otherwise process the overflow stack first.\n+    while (marking_stack()->pop_overflow(task)) {\n+      if (!marking_stack()->try_push_to_taskqueue(task)) {\n+        follow_contents(task, false);\n+      }\n+    }\n+    while (marking_stack()->pop_local(task)) {\n+      follow_contents(task, false);\n+    }\n+  } while (!marking_stack_empty());\n+\n+  assert(marking_stack_empty(), \"Sanity\");\n+}\n+\n+#if TASKQUEUE_STATS\n+void ParCompactionManagerNew::print_and_reset_taskqueue_stats() {\n+  marking_stacks()->print_and_reset_taskqueue_stats(\"Marking Stacks\");\n+\n+  auto get_pa_stats = [&](uint i) {\n+    return _manager_array[i]->partial_array_task_stats();\n+  };\n+  PartialArrayTaskStats::log_set(ParallelGCThreads, get_pa_stats,\n+                                 \"Partial Array Task Stats\");\n+  uint parallel_gc_threads = ParallelScavengeHeap::heap()->workers().max_workers();\n+  for (uint i = 0; i < parallel_gc_threads; ++i) {\n+    get_pa_stats(i)->reset();\n+  }\n+}\n+\n+PartialArrayTaskStats* ParCompactionManagerNew::partial_array_task_stats() {\n+  return _partial_array_splitter.stats();\n+}\n+#endif \/\/ TASKQUEUE_STATS\n+\n+#ifdef ASSERT\n+void ParCompactionManagerNew::verify_all_marking_stack_empty() {\n+  uint parallel_gc_threads = ParallelGCThreads;\n+  for (uint i = 0; i < parallel_gc_threads; i++) {\n+    assert(_manager_array[i]->marking_stack_empty(), \"Marking stack should be empty\");\n+  }\n+}\n+#endif\n","filename":"src\/hotspot\/share\/gc\/parallel\/psCompactionManagerNew.cpp","additions":167,"deletions":0,"binary":false,"changes":167,"status":"added"},{"patch":"@@ -0,0 +1,165 @@\n+\/*\n+ * Copyright (c) 2005, 2024, Oracle and\/or its affiliates. All rights reserved.\n+ * Copyright Amazon.com Inc. or its affiliates. All Rights Reserved.\n+ * DO NOT ALTER OR REMOVE COPYRIGHT NOTICES OR THIS FILE HEADER.\n+ *\n+ * This code is free software; you can redistribute it and\/or modify it\n+ * under the terms of the GNU General Public License version 2 only, as\n+ * published by the Free Software Foundation.\n+ *\n+ * This code is distributed in the hope that it will be useful, but WITHOUT\n+ * ANY WARRANTY; without even the implied warranty of MERCHANTABILITY or\n+ * FITNESS FOR A PARTICULAR PURPOSE.  See the GNU General Public License\n+ * version 2 for more details (a copy is included in the LICENSE file that\n+ * accompanied this code).\n+ *\n+ * You should have received a copy of the GNU General Public License version\n+ * 2 along with this work; if not, write to the Free Software Foundation,\n+ * Inc., 51 Franklin St, Fifth Floor, Boston, MA 02110-1301 USA.\n+ *\n+ * Please contact Oracle, 500 Oracle Parkway, Redwood Shores, CA 94065 USA\n+ * or visit www.oracle.com if you need additional information or have any\n+ * questions.\n+ *\n+ *\/\n+\n+#ifndef SHARE_GC_PARALLEL_PSCOMPACTIONMANAGERNEW_HPP\n+#define SHARE_GC_PARALLEL_PSCOMPACTIONMANAGERNEW_HPP\n+\n+#include \"classfile\/classLoaderData.hpp\"\n+#include \"gc\/parallel\/psParallelCompactNew.hpp\"\n+#include \"gc\/shared\/partialArraySplitter.hpp\"\n+#include \"gc\/shared\/partialArrayTaskStats.hpp\"\n+#include \"gc\/shared\/partialArrayState.hpp\"\n+#include \"gc\/shared\/preservedMarks.hpp\"\n+#include \"gc\/shared\/stringdedup\/stringDedup.hpp\"\n+#include \"gc\/shared\/taskqueue.hpp\"\n+#include \"gc\/shared\/taskTerminator.hpp\"\n+#include \"memory\/allocation.hpp\"\n+#include \"utilities\/stack.hpp\"\n+\n+class MutableSpace;\n+class PSOldGen;\n+class ParCompactionManagerNew;\n+class ObjectStartArray;\n+class ParMarkBitMap;\n+\n+class PCMarkAndPushClosureNew: public ClaimMetadataVisitingOopIterateClosure {\n+  ParCompactionManagerNew* _compaction_manager;\n+\n+  template <typename T> void do_oop_work(T* p);\n+public:\n+  PCMarkAndPushClosureNew(ParCompactionManagerNew* cm, ReferenceProcessor* rp) :\n+    ClaimMetadataVisitingOopIterateClosure(ClassLoaderData::_claim_stw_fullgc_mark, rp),\n+    _compaction_manager(cm) { }\n+\n+  void do_oop(oop* p) final               { do_oop_work(p); }\n+  void do_oop(narrowOop* p) final         { do_oop_work(p); }\n+};\n+\n+class ParCompactionManagerNew : public CHeapObj<mtGC> {\n+  friend class MarkFromRootsTaskNew;\n+  friend class ParallelCompactRefProcProxyTaskNew;\n+  friend class ParallelScavengeRefProcProxyTask;\n+  friend class ParMarkBitMap;\n+  friend class PSParallelCompactNew;\n+  friend class PCAddThreadRootsMarkingTaskClosureNew;\n+\n+ private:\n+  typedef OverflowTaskQueue<ScannerTask, mtGC>           PSMarkTaskQueue;\n+  typedef GenericTaskQueueSet<PSMarkTaskQueue, mtGC>     PSMarkTasksQueueSet;\n+\n+  static ParCompactionManagerNew** _manager_array;\n+  static PSMarkTasksQueueSet*   _marking_stacks;\n+  static ObjectStartArray*      _start_array;\n+  static PSOldGen*              _old_gen;\n+\n+  static PartialArrayStateManager*  _partial_array_state_manager;\n+  PartialArraySplitter              _partial_array_splitter;\n+\n+  PSMarkTaskQueue               _marking_stack;\n+\n+  PCMarkAndPushClosureNew _mark_and_push_closure;\n+\n+  static PreservedMarksSet* _preserved_marks_set;\n+  PreservedMarks* _preserved_marks;\n+\n+  static ParMarkBitMap* _mark_bitmap;\n+\n+  StringDedup::Requests _string_dedup_requests;\n+\n+  static PSOldGen* old_gen()             { return _old_gen; }\n+  static ObjectStartArray* start_array() { return _start_array; }\n+  static PSMarkTasksQueueSet* marking_stacks()  { return _marking_stacks; }\n+\n+  static void initialize(ParMarkBitMap* mbm);\n+\n+  ParCompactionManagerNew(PreservedMarks* preserved_marks,\n+                       ReferenceProcessor* ref_processor,\n+                       uint parallel_gc_threads);\n+\n+  inline PSMarkTaskQueue*  marking_stack() { return &_marking_stack; }\n+  inline void push(PartialArrayState* stat);\n+  void push_objArray(oop obj);\n+\n+#if TASKQUEUE_STATS\n+  static void print_and_reset_taskqueue_stats();\n+  PartialArrayTaskStats* partial_array_task_stats();\n+#endif \/\/ TASKQUEUE_STATS\n+\n+public:\n+  void flush_string_dedup_requests() {\n+    _string_dedup_requests.flush();\n+  }\n+\n+  static void flush_all_string_dedup_requests();\n+\n+  \/\/ Get the compaction manager when doing evacuation work from the VM thread.\n+  \/\/ Simply use the first compaction manager here.\n+  static ParCompactionManagerNew* get_vmthread_cm() { return _manager_array[0]; }\n+\n+  PreservedMarks* preserved_marks() const {\n+    return _preserved_marks;\n+  }\n+\n+  static ParMarkBitMap* mark_bitmap() { return _mark_bitmap; }\n+\n+  \/\/ Save for later processing.  Must not fail.\n+  inline void push(oop obj);\n+\n+  \/\/ Check mark and maybe push on marking stack.\n+  template <typename T> inline void mark_and_push(T* p);\n+\n+  \/\/ Access function for compaction managers\n+  static ParCompactionManagerNew* gc_thread_compaction_manager(uint index);\n+\n+  static bool steal(uint queue_num, ScannerTask& t);\n+\n+  \/\/ Process tasks remaining on marking stack\n+  void follow_marking_stacks();\n+  inline bool marking_stack_empty() const;\n+\n+  inline void follow_contents(const ScannerTask& task, bool stolen);\n+  inline void follow_array(objArrayOop array, size_t start, size_t end);\n+  void process_array_chunk(PartialArrayState* state, bool stolen);\n+\n+  class FollowStackClosure: public VoidClosure {\n+   private:\n+    ParCompactionManagerNew* _compaction_manager;\n+    TaskTerminator* _terminator;\n+    uint _worker_id;\n+   public:\n+    FollowStackClosure(ParCompactionManagerNew* cm, TaskTerminator* terminator, uint worker_id)\n+      : _compaction_manager(cm), _terminator(terminator), _worker_id(worker_id) { }\n+    void do_void() final;\n+  };\n+\n+  \/\/ Called after marking.\n+  static void verify_all_marking_stack_empty() NOT_DEBUG_RETURN;\n+};\n+\n+bool ParCompactionManagerNew::marking_stack_empty() const {\n+  return _marking_stack.is_empty();\n+}\n+\n+#endif \/\/ SHARE_GC_PARALLEL_PSCOMPACTIONMANAGERNEW_HPP\n","filename":"src\/hotspot\/share\/gc\/parallel\/psCompactionManagerNew.hpp","additions":165,"deletions":0,"binary":false,"changes":165,"status":"added"},{"patch":"@@ -0,0 +1,127 @@\n+\/*\n+ * Copyright (c) 2010, 2024, Oracle and\/or its affiliates. All rights reserved.\n+ * Copyright Amazon.com Inc. or its affiliates. All Rights Reserved.\n+ * DO NOT ALTER OR REMOVE COPYRIGHT NOTICES OR THIS FILE HEADER.\n+ *\n+ * This code is free software; you can redistribute it and\/or modify it\n+ * under the terms of the GNU General Public License version 2 only, as\n+ * published by the Free Software Foundation.\n+ *\n+ * This code is distributed in the hope that it will be useful, but WITHOUT\n+ * ANY WARRANTY; without even the implied warranty of MERCHANTABILITY or\n+ * FITNESS FOR A PARTICULAR PURPOSE.  See the GNU General Public License\n+ * version 2 for more details (a copy is included in the LICENSE file that\n+ * accompanied this code).\n+ *\n+ * You should have received a copy of the GNU General Public License version\n+ * 2 along with this work; if not, write to the Free Software Foundation,\n+ * Inc., 51 Franklin St, Fifth Floor, Boston, MA 02110-1301 USA.\n+ *\n+ * Please contact Oracle, 500 Oracle Parkway, Redwood Shores, CA 94065 USA\n+ * or visit www.oracle.com if you need additional information or have any\n+ * questions.\n+ *\n+ *\/\n+\n+#ifndef SHARE_GC_PARALLEL_PSCOMPACTIONMANAGERNEW_INLINE_HPP\n+#define SHARE_GC_PARALLEL_PSCOMPACTIONMANAGERNEW_INLINE_HPP\n+\n+#include \"gc\/parallel\/psCompactionManagerNew.hpp\"\n+\n+#include \"classfile\/classLoaderData.hpp\"\n+#include \"classfile\/javaClasses.inline.hpp\"\n+#include \"gc\/parallel\/parMarkBitMap.hpp\"\n+#include \"gc\/parallel\/psParallelCompactNew.inline.hpp\"\n+#include \"gc\/parallel\/psStringDedup.hpp\"\n+#include \"gc\/shared\/partialArrayState.hpp\"\n+#include \"gc\/shared\/partialArrayTaskStepper.inline.hpp\"\n+#include \"gc\/shared\/taskqueue.inline.hpp\"\n+#include \"oops\/access.inline.hpp\"\n+#include \"oops\/arrayOop.hpp\"\n+#include \"oops\/compressedOops.inline.hpp\"\n+#include \"oops\/objArrayOop.inline.hpp\"\n+#include \"oops\/oop.inline.hpp\"\n+#include \"utilities\/debug.hpp\"\n+#include \"utilities\/globalDefinitions.hpp\"\n+\n+template <typename T>\n+inline void PCMarkAndPushClosureNew::do_oop_work(T* p) {\n+  _compaction_manager->mark_and_push(p);\n+}\n+\n+inline bool ParCompactionManagerNew::steal(uint queue_num, ScannerTask& t) {\n+  return marking_stacks()->steal(queue_num, t);\n+}\n+\n+inline void ParCompactionManagerNew::push(oop obj) {\n+  marking_stack()->push(ScannerTask(obj));\n+}\n+\n+inline void ParCompactionManagerNew::push(PartialArrayState* stat) {\n+  marking_stack()->push(ScannerTask(stat));\n+}\n+\n+template <typename T>\n+inline void ParCompactionManagerNew::mark_and_push(T* p) {\n+  T heap_oop = RawAccess<>::oop_load(p);\n+  if (!CompressedOops::is_null(heap_oop)) {\n+    oop obj = CompressedOops::decode_not_null(heap_oop);\n+    assert(ParallelScavengeHeap::heap()->is_in(obj), \"should be in heap\");\n+\n+    if (mark_bitmap()->mark_obj(obj)) {\n+      if (StringDedup::is_enabled() &&\n+          java_lang_String::is_instance(obj) &&\n+          psStringDedup::is_candidate_from_mark(obj)) {\n+        _string_dedup_requests.add(obj);\n+      }\n+\n+      ContinuationGCSupport::transform_stack_chunk(obj);\n+      push(obj);\n+    }\n+  }\n+}\n+\n+inline void ParCompactionManagerNew::FollowStackClosure::do_void() {\n+  _compaction_manager->follow_marking_stacks();\n+  if (_terminator != nullptr) {\n+    steal_marking_work_new(*_terminator, _worker_id);\n+  }\n+}\n+\n+template <typename T>\n+inline void follow_array_specialized(objArrayOop obj, size_t start, size_t end, ParCompactionManagerNew* cm) {\n+  assert(start <= end, \"invariant\");\n+  T* const base = (T*)obj->base();\n+  T* const beg = base + start;\n+  T* const chunk_end = base + end;\n+\n+  \/\/ Push the non-null elements of the next stride on the marking stack.\n+  for (T* e = beg; e < chunk_end; e++) {\n+    cm->mark_and_push<T>(e);\n+  }\n+}\n+\n+inline void ParCompactionManagerNew::follow_array(objArrayOop obj, size_t start, size_t end) {\n+  if (UseCompressedOops) {\n+    follow_array_specialized<narrowOop>(obj, start, end, this);\n+  } else {\n+    follow_array_specialized<oop>(obj, start, end, this);\n+  }\n+}\n+\n+inline void ParCompactionManagerNew::follow_contents(const ScannerTask& task, bool stolen) {\n+  if (task.is_partial_array_state()) {\n+    assert(PSParallelCompactNew::mark_bitmap()->is_marked(task.to_partial_array_state()->source()), \"should be marked\");\n+    process_array_chunk(task.to_partial_array_state(), stolen);\n+  } else {\n+    oop obj = task.to_oop();\n+    assert(PSParallelCompactNew::mark_bitmap()->is_marked(obj), \"should be marked\");\n+    if (obj->is_objArray()) {\n+      push_objArray(obj);\n+    } else {\n+      obj->oop_iterate(&_mark_and_push_closure);\n+    }\n+  }\n+}\n+\n+#endif \/\/ SHARE_GC_PARALLEL_PSCOMPACTIONMANAGERNEW_INLINE_HPP\n","filename":"src\/hotspot\/share\/gc\/parallel\/psCompactionManagerNew.inline.hpp","additions":127,"deletions":0,"binary":false,"changes":127,"status":"added"},{"patch":"@@ -552,1 +552,1 @@\n-class PCAdjustPointerClosure: public BasicOopIterateClosure {\n+class PCAdjustPointerClosureNew: public BasicOopIterateClosure {\n@@ -563,1 +563,1 @@\n-static PCAdjustPointerClosure pc_adjust_pointer_closure;\n+static PCAdjustPointerClosureNew pc_adjust_pointer_closure;\n@@ -1060,0 +1060,2 @@\n+    FullGCForwarding::begin();\n+\n@@ -1066,0 +1068,2 @@\n+    FullGCForwarding::end();\n+\n","filename":"src\/hotspot\/share\/gc\/parallel\/psParallelCompact.cpp","additions":6,"deletions":2,"binary":false,"changes":8,"status":"modified"},{"patch":"@@ -0,0 +1,1146 @@\n+\/*\n+ * Copyright (c) 2005, 2025, Oracle and\/or its affiliates. All rights reserved.\n+ * Copyright Amazon.com Inc. or its affiliates. All Rights Reserved.\n+ * DO NOT ALTER OR REMOVE COPYRIGHT NOTICES OR THIS FILE HEADER.\n+ *\n+ * This code is free software; you can redistribute it and\/or modify it\n+ * under the terms of the GNU General Public License version 2 only, as\n+ * published by the Free Software Foundation.\n+ *\n+ * This code is distributed in the hope that it will be useful, but WITHOUT\n+ * ANY WARRANTY; without even the implied warranty of MERCHANTABILITY or\n+ * FITNESS FOR A PARTICULAR PURPOSE.  See the GNU General Public License\n+ * version 2 for more details (a copy is included in the LICENSE file that\n+ * accompanied this code).\n+ *\n+ * You should have received a copy of the GNU General Public License version\n+ * 2 along with this work; if not, write to the Free Software Foundation,\n+ * Inc., 51 Franklin St, Fifth Floor, Boston, MA 02110-1301 USA.\n+ *\n+ * Please contact Oracle, 500 Oracle Parkway, Redwood Shores, CA 94065 USA\n+ * or visit www.oracle.com if you need additional information or have any\n+ * questions.\n+ *\n+ *\/\n+\n+#include \"classfile\/classLoaderDataGraph.hpp\"\n+#include \"classfile\/symbolTable.hpp\"\n+#include \"classfile\/systemDictionary.hpp\"\n+#include \"code\/codeCache.hpp\"\n+#include \"compiler\/oopMap.hpp\"\n+#include \"gc\/parallel\/objectStartArray.inline.hpp\"\n+#include \"gc\/parallel\/parallelArguments.hpp\"\n+#include \"gc\/parallel\/parallelScavengeHeap.inline.hpp\"\n+#include \"gc\/parallel\/parMarkBitMap.inline.hpp\"\n+#include \"gc\/parallel\/psAdaptiveSizePolicy.hpp\"\n+#include \"gc\/parallel\/psCompactionManagerNew.inline.hpp\"\n+#include \"gc\/parallel\/psOldGen.hpp\"\n+#include \"gc\/parallel\/psParallelCompactNew.inline.hpp\"\n+#include \"gc\/parallel\/psPromotionManager.inline.hpp\"\n+#include \"gc\/parallel\/psScavenge.hpp\"\n+#include \"gc\/parallel\/psYoungGen.hpp\"\n+#include \"gc\/shared\/classUnloadingContext.hpp\"\n+#include \"gc\/shared\/fullGCForwarding.inline.hpp\"\n+#include \"gc\/shared\/gcCause.hpp\"\n+#include \"gc\/shared\/gcHeapSummary.hpp\"\n+#include \"gc\/shared\/gcId.hpp\"\n+#include \"gc\/shared\/gcLocker.hpp\"\n+#include \"gc\/shared\/gcTimer.hpp\"\n+#include \"gc\/shared\/gcTrace.hpp\"\n+#include \"gc\/shared\/gcTraceTime.inline.hpp\"\n+#include \"gc\/shared\/gcVMOperations.hpp\"\n+#include \"gc\/shared\/isGCActiveMark.hpp\"\n+#include \"gc\/shared\/oopStorageSetParState.inline.hpp\"\n+#include \"gc\/shared\/preservedMarks.inline.hpp\"\n+#include \"gc\/shared\/referencePolicy.hpp\"\n+#include \"gc\/shared\/referenceProcessor.hpp\"\n+#include \"gc\/shared\/referenceProcessorPhaseTimes.hpp\"\n+#include \"gc\/shared\/spaceDecorator.hpp\"\n+#include \"gc\/shared\/strongRootsScope.hpp\"\n+#include \"gc\/shared\/taskTerminator.hpp\"\n+#include \"gc\/shared\/weakProcessor.inline.hpp\"\n+#include \"gc\/shared\/workerPolicy.hpp\"\n+#include \"gc\/shared\/workerThread.hpp\"\n+#include \"gc\/shared\/workerUtils.hpp\"\n+#include \"logging\/log.hpp\"\n+#include \"memory\/iterator.inline.hpp\"\n+#include \"memory\/memoryReserver.hpp\"\n+#include \"memory\/metaspaceUtils.hpp\"\n+#include \"memory\/resourceArea.hpp\"\n+#include \"memory\/universe.hpp\"\n+#include \"nmt\/memTracker.hpp\"\n+#include \"oops\/methodData.hpp\"\n+#include \"runtime\/java.hpp\"\n+#include \"runtime\/safepoint.hpp\"\n+#include \"runtime\/threads.hpp\"\n+#include \"runtime\/vmThread.hpp\"\n+#include \"services\/memoryService.hpp\"\n+#include \"utilities\/align.hpp\"\n+#include \"utilities\/debug.hpp\"\n+#include \"utilities\/events.hpp\"\n+#include \"utilities\/formatBuffer.hpp\"\n+#include \"utilities\/macros.hpp\"\n+#if INCLUDE_JVMCI\n+#include \"jvmci\/jvmci.hpp\"\n+#endif\n+\n+SpaceInfoNew PSParallelCompactNew::_space_info[PSParallelCompactNew::last_space_id];\n+\n+size_t PSParallelCompactNew::_num_regions;\n+PCRegionData* PSParallelCompactNew::_region_data_array;\n+size_t PSParallelCompactNew::_num_regions_serial;\n+PCRegionData* PSParallelCompactNew::_region_data_array_serial;\n+PCRegionData** PSParallelCompactNew::_per_worker_region_data;\n+bool PSParallelCompactNew::_serial = false;\n+\n+SpanSubjectToDiscoveryClosure PSParallelCompactNew::_span_based_discoverer;\n+ReferenceProcessor* PSParallelCompactNew::_ref_processor = nullptr;\n+\n+void PSParallelCompactNew::print_on(outputStream* st) {\n+  _mark_bitmap.print_on(st);\n+}\n+\n+STWGCTimer          PSParallelCompactNew::_gc_timer;\n+ParallelOldTracer   PSParallelCompactNew::_gc_tracer;\n+elapsedTimer        PSParallelCompactNew::_accumulated_time;\n+unsigned int        PSParallelCompactNew::_maximum_compaction_gc_num = 0;\n+CollectorCounters*  PSParallelCompactNew::_counters = nullptr;\n+ParMarkBitMap       PSParallelCompactNew::_mark_bitmap;\n+\n+PSParallelCompactNew::IsAliveClosure PSParallelCompactNew::_is_alive_closure;\n+\n+class PCAdjustPointerClosure: public BasicOopIterateClosure {\n+  template <typename T>\n+  void do_oop_work(T* p) { PSParallelCompactNew::adjust_pointer(p); }\n+\n+public:\n+  void do_oop(oop* p) final          { do_oop_work(p); }\n+  void do_oop(narrowOop* p) final    { do_oop_work(p); }\n+\n+  ReferenceIterationMode reference_iteration_mode() final { return DO_FIELDS; }\n+};\n+\n+static PCAdjustPointerClosure pc_adjust_pointer_closure;\n+\n+class IsAliveClosure: public BoolObjectClosure {\n+public:\n+  bool do_object_b(oop p) final;\n+};\n+\n+\n+bool PSParallelCompactNew::IsAliveClosure::do_object_b(oop p) { return mark_bitmap()->is_marked(p); }\n+\n+void PSParallelCompactNew::post_initialize() {\n+  ParallelScavengeHeap* heap = ParallelScavengeHeap::heap();\n+  _span_based_discoverer.set_span(heap->reserved_region());\n+  _ref_processor =\n+    new ReferenceProcessor(&_span_based_discoverer,\n+                           ParallelGCThreads,   \/\/ mt processing degree\n+                           ParallelGCThreads,   \/\/ mt discovery degree\n+                           false,               \/\/ concurrent_discovery\n+                           &_is_alive_closure); \/\/ non-header is alive closure\n+\n+  _counters = new CollectorCounters(\"Parallel full collection pauses\", 1);\n+\n+  \/\/ Initialize static fields in ParCompactionManager.\n+  ParCompactionManagerNew::initialize(mark_bitmap());\n+}\n+\n+bool PSParallelCompactNew::initialize_aux_data() {\n+  ParallelScavengeHeap* heap = ParallelScavengeHeap::heap();\n+  MemRegion mr = heap->reserved_region();\n+  assert(mr.byte_size() != 0, \"heap should be reserved\");\n+\n+  initialize_space_info();\n+\n+  if (!_mark_bitmap.initialize(mr)) {\n+    vm_shutdown_during_initialization(\n+      err_msg(\"Unable to allocate %zuKB bitmaps for parallel \"\n+      \"garbage collection for the requested %zuKB heap.\",\n+      _mark_bitmap.reserved_byte_size()\/K, mr.byte_size()\/K));\n+    return false;\n+  }\n+\n+  return true;\n+}\n+\n+void PSParallelCompactNew::initialize_space_info()\n+{\n+  memset(&_space_info, 0, sizeof(_space_info));\n+\n+  PSYoungGen* young_gen = ParallelScavengeHeap::young_gen();\n+\n+  _space_info[old_space_id].set_space(ParallelScavengeHeap::old_gen()->object_space());\n+  _space_info[eden_space_id].set_space(young_gen->eden_space());\n+  _space_info[from_space_id].set_space(young_gen->from_space());\n+  _space_info[to_space_id].set_space(young_gen->to_space());\n+\n+  _space_info[old_space_id].set_start_array(ParallelScavengeHeap::old_gen()->start_array());\n+}\n+\n+void\n+PSParallelCompactNew::clear_data_covering_space(SpaceId id)\n+{\n+  \/\/ At this point, top is the value before GC, new_top() is the value that will\n+  \/\/ be set at the end of GC.  The marking bitmap is cleared to top; nothing\n+  \/\/ should be marked above top.\n+  MutableSpace* const space = _space_info[id].space();\n+  HeapWord* const bot = space->bottom();\n+  HeapWord* const top = space->top();\n+\n+  _mark_bitmap.clear_range(bot, top);\n+}\n+\n+void PSParallelCompactNew::pre_compact()\n+{\n+  \/\/ Update the from & to space pointers in space_info, since they are swapped\n+  \/\/ at each young gen gc.  Do the update unconditionally (even though a\n+  \/\/ promotion failure does not swap spaces) because an unknown number of young\n+  \/\/ collections will have swapped the spaces an unknown number of times.\n+  GCTraceTime(Debug, gc, phases) tm(\"Pre Compact\", &_gc_timer);\n+  ParallelScavengeHeap* heap = ParallelScavengeHeap::heap();\n+  _space_info[from_space_id].set_space(ParallelScavengeHeap::young_gen()->from_space());\n+  _space_info[to_space_id].set_space(ParallelScavengeHeap::young_gen()->to_space());\n+\n+  \/\/ Increment the invocation count\n+  heap->increment_total_collections(true);\n+\n+  CodeCache::on_gc_marking_cycle_start();\n+\n+  heap->print_before_gc();\n+  heap->trace_heap_before_gc(&_gc_tracer);\n+\n+  \/\/ Fill in TLABs\n+  heap->ensure_parsability(true);  \/\/ retire TLABs\n+\n+  if (VerifyBeforeGC && heap->total_collections() >= VerifyGCStartAt) {\n+    Universe::verify(\"Before GC\");\n+  }\n+\n+  DEBUG_ONLY(mark_bitmap()->verify_clear();)\n+}\n+\n+void PSParallelCompactNew::post_compact()\n+{\n+  GCTraceTime(Info, gc, phases) tm(\"Post Compact\", &_gc_timer);\n+\n+  CodeCache::on_gc_marking_cycle_finish();\n+  CodeCache::arm_all_nmethods();\n+\n+  for (unsigned int id = old_space_id; id < last_space_id; ++id) {\n+    \/\/ Clear the marking bitmap, summary data and split info.\n+    clear_data_covering_space(SpaceId(id));\n+  }\n+\n+  {\n+    PCRegionData* last_live[last_space_id];\n+    for (uint i = old_space_id; i < last_space_id; ++i) {\n+      last_live[i] = nullptr;\n+    }\n+\n+    \/\/ Figure out last region in each space that has live data.\n+    uint space_id = old_space_id;\n+    MutableSpace* space = _space_info[space_id].space();\n+    size_t num_regions = get_num_regions();\n+    PCRegionData* region_data_array = get_region_data_array();\n+    last_live[space_id] = &region_data_array[0];\n+    for (size_t idx = 0; idx < num_regions; idx++) {\n+      PCRegionData* rd = region_data_array + idx;\n+      if(!space->contains(rd->bottom())) {\n+        ++space_id;\n+        assert(space_id < last_space_id, \"invariant\");\n+        space = _space_info[space_id].space();\n+        log_develop_trace(gc, compaction)(\"Last live for space: %u: %zu\", space_id, idx);\n+        last_live[space_id] = rd;\n+      }\n+      assert(space->contains(rd->bottom()), \"next space should contain next region\");\n+      log_develop_trace(gc, compaction)(\"post-compact region: idx: %zu, bottom: \" PTR_FORMAT \", new_top: \" PTR_FORMAT \", end: \" PTR_FORMAT, rd->idx(), p2i(rd->bottom()), p2i(rd->new_top()), p2i(rd->end()));\n+      if (rd->new_top() > rd->bottom()) {\n+        last_live[space_id] = rd;\n+        log_develop_trace(gc, compaction)(\"Bump last live for space: %u\", space_id);\n+      }\n+    }\n+\n+    for (uint i = old_space_id; i < last_space_id; ++i) {\n+      PCRegionData* rd = last_live[i];\n+        log_develop_trace(gc, compaction)(\n+                \"Last live region in space: %u, compaction region, \" PTR_FORMAT \", #%zu: [\" PTR_FORMAT \", \" PTR_FORMAT \"), new_top: \" PTR_FORMAT,\n+                i, p2i(rd), rd->idx(),\n+                p2i(rd->bottom()), p2i(rd->end()), p2i(rd->new_top()));\n+    }\n+\n+    \/\/ Fill all gaps and update the space boundaries.\n+    space_id = old_space_id;\n+    space = _space_info[space_id].space();\n+    size_t total_live = 0;\n+    size_t total_waste = 0;\n+    for (size_t idx = 0; idx < num_regions; idx++) {\n+      PCRegionData* rd = &region_data_array[idx];\n+      PCRegionData* last_live_in_space = last_live[space_id];\n+      assert(last_live_in_space != nullptr, \"last live must not be null\");\n+      if (rd != last_live_in_space) {\n+        if (rd->new_top() < rd->end()) {\n+          ObjectStartArray* sa = start_array(SpaceId(space_id));\n+          if (sa != nullptr) {\n+            sa->update_for_block(rd->new_top(), rd->end());\n+          }\n+          ParallelScavengeHeap::heap()->fill_with_dummy_object(rd->new_top(), rd->end(), false);\n+        }\n+        size_t live = pointer_delta(rd->new_top(), rd->bottom());\n+        size_t waste = pointer_delta(rd->end(), rd->new_top());\n+        total_live += live;\n+        total_waste += waste;\n+        log_develop_trace(gc, compaction)(\n+                \"Live compaction region, #%zu: [\" PTR_FORMAT \", \" PTR_FORMAT \"), new_top: \" PTR_FORMAT \", live: %zu, waste: %zu\",\n+                rd->idx(),\n+                p2i(rd->bottom()), p2i(rd->end()), p2i(rd->new_top()), live, waste);\n+      } else {\n+        \/\/ Update top of space.\n+        space->set_top(rd->new_top());\n+        size_t live = pointer_delta(rd->new_top(), rd->bottom());\n+        total_live += live;\n+        log_develop_trace(gc, compaction)(\n+                \"Live compaction region, #%zu: [\" PTR_FORMAT \", \" PTR_FORMAT \"), new_top: \" PTR_FORMAT \", live: %zu, waste: %zu\",\n+                rd->idx(),\n+                p2i(rd->bottom()), p2i(rd->end()), p2i(rd->new_top()), live, size_t(0));\n+\n+        \/\/ Fast-Forward to next space.\n+        for (; idx < num_regions - 1; idx++) {\n+          rd = &region_data_array[idx + 1];\n+          if (!space->contains(rd->bottom())) {\n+            space_id++;\n+            assert(space_id < last_space_id, \"must be\");\n+            space = _space_info[space_id].space();\n+            assert(space->contains(rd->bottom()), \"space must contain region\");\n+            break;\n+          }\n+        }\n+      }\n+    }\n+    log_develop_debug(gc, compaction)(\"total live: %zu, total waste: %zu, ratio: %f\", total_live, total_waste, ((float)total_waste)\/((float)(total_live + total_waste)));\n+  }\n+  {\n+    FREE_C_HEAP_ARRAY(PCRegionData*, _per_worker_region_data);\n+    FREE_C_HEAP_ARRAY(PCRegionData, _region_data_array);\n+    FREE_C_HEAP_ARRAY(PCRegionData, _region_data_array_serial);\n+  }\n+#ifdef ASSERT\n+  {\n+    mark_bitmap()->verify_clear();\n+  }\n+#endif\n+\n+  ParCompactionManagerNew::flush_all_string_dedup_requests();\n+\n+  MutableSpace* const eden_space = _space_info[eden_space_id].space();\n+  MutableSpace* const from_space = _space_info[from_space_id].space();\n+  MutableSpace* const to_space   = _space_info[to_space_id].space();\n+\n+  ParallelScavengeHeap* heap = ParallelScavengeHeap::heap();\n+  bool eden_empty = eden_space->is_empty();\n+\n+  \/\/ Update heap occupancy information which is used as input to the soft ref\n+  \/\/ clearing policy at the next gc.\n+  Universe::heap()->update_capacity_and_used_at_gc();\n+\n+  bool young_gen_empty = eden_empty && from_space->is_empty() &&\n+    to_space->is_empty();\n+\n+  PSCardTable* ct = heap->card_table();\n+  MemRegion old_mr = ParallelScavengeHeap::old_gen()->committed();\n+  if (young_gen_empty) {\n+    ct->clear_MemRegion(old_mr);\n+  } else {\n+    ct->dirty_MemRegion(old_mr);\n+  }\n+\n+  {\n+    \/\/ Delete metaspaces for unloaded class loaders and clean up loader_data graph\n+    GCTraceTime(Debug, gc, phases) t(\"Purge Class Loader Data\", gc_timer());\n+    ClassLoaderDataGraph::purge(true \/* at_safepoint *\/);\n+    DEBUG_ONLY(MetaspaceUtils::verify();)\n+  }\n+\n+  \/\/ Need to clear claim bits for the next mark.\n+  ClassLoaderDataGraph::clear_claimed_marks();\n+\n+  heap->prune_scavengable_nmethods();\n+\n+#if COMPILER2_OR_JVMCI\n+  DerivedPointerTable::update_pointers();\n+#endif\n+\n+  \/\/ Signal that we have completed a visit to all live objects.\n+  Universe::heap()->record_whole_heap_examined_timestamp();\n+}\n+\n+void PSParallelCompactNew::setup_regions_parallel() {\n+  static const size_t REGION_SIZE_WORDS = (SpaceAlignment \/ HeapWordSize);\n+  size_t num_regions = 0;\n+  for (uint i = old_space_id; i < last_space_id; ++i) {\n+    MutableSpace* const space = _space_info[i].space();\n+    size_t const space_size_words = space->capacity_in_words();\n+    num_regions += align_up(space_size_words, REGION_SIZE_WORDS) \/ REGION_SIZE_WORDS;\n+  }\n+  _region_data_array = NEW_C_HEAP_ARRAY(PCRegionData, num_regions, mtGC);\n+\n+  size_t region_idx = 0;\n+  for (uint i = old_space_id; i < last_space_id; ++i) {\n+    const MutableSpace* space = _space_info[i].space();\n+    HeapWord* addr = space->bottom();\n+    HeapWord* sp_end = space->end();\n+    HeapWord* sp_top = space->top();\n+    while (addr < sp_end) {\n+      HeapWord* end = MIN2(align_up(addr + REGION_SIZE_WORDS, REGION_SIZE_WORDS), space->end());\n+      if (addr < sp_top) {\n+        HeapWord* prev_obj_start = _mark_bitmap.find_obj_beg_reverse(addr, end);\n+        if (prev_obj_start < end) {\n+          HeapWord* prev_obj_end = prev_obj_start + cast_to_oop(prev_obj_start)->size();\n+          if (end < prev_obj_end) {\n+            \/\/ Object crosses region boundary, adjust end to be after object's last word.\n+            end = prev_obj_end;\n+          }\n+        }\n+      }\n+      assert(region_idx < num_regions, \"must not exceed number of regions: region_idx: %zu, num_regions: %zu\", region_idx, num_regions);\n+      HeapWord* top;\n+      if (sp_top < addr) {\n+        top = addr;\n+      } else if (sp_top >= end) {\n+        top = end;\n+      } else {\n+        top = sp_top;\n+      }\n+      assert(ParallelScavengeHeap::heap()->is_in_reserved(addr), \"addr must be in heap: \" PTR_FORMAT, p2i(addr));\n+      new (_region_data_array + region_idx) PCRegionData(region_idx, addr, top, end);\n+      addr = end;\n+      region_idx++;\n+    }\n+  }\n+  _num_regions = region_idx;\n+  log_info(gc)(\"Number of regions: %zu\", _num_regions);\n+}\n+\n+void PSParallelCompactNew::setup_regions_serial() {\n+  _num_regions_serial = last_space_id;\n+  _region_data_array_serial = NEW_C_HEAP_ARRAY(PCRegionData, _num_regions_serial, mtGC);\n+  new (_region_data_array_serial + old_space_id)  PCRegionData(old_space_id, space(old_space_id)->bottom(), space(old_space_id)->top(), space(old_space_id)->end());\n+  new (_region_data_array_serial + eden_space_id) PCRegionData(eden_space_id, space(eden_space_id)->bottom(), space(eden_space_id)->top(), space(eden_space_id)->end());\n+  new (_region_data_array_serial + from_space_id) PCRegionData(from_space_id, space(from_space_id)->bottom(), space(from_space_id)->top(), space(from_space_id)->end());\n+  new (_region_data_array_serial + to_space_id)   PCRegionData(to_space_id, space(to_space_id)->bottom(), space(to_space_id)->top(), space(to_space_id)->end());\n+}\n+\n+bool PSParallelCompactNew::check_maximum_compaction() {\n+\n+  ParallelScavengeHeap* heap = ParallelScavengeHeap::heap();\n+\n+  \/\/ Check System.GC\n+  bool is_max_on_system_gc = UseMaximumCompactionOnSystemGC\n+                          && GCCause::is_user_requested_gc(heap->gc_cause());\n+\n+  \/\/ JVM flags\n+  const uint total_invocations = heap->total_full_collections();\n+  assert(total_invocations >= _maximum_compaction_gc_num, \"sanity\");\n+  const size_t gcs_since_max = total_invocations - _maximum_compaction_gc_num;\n+  const bool is_interval_ended = gcs_since_max > HeapMaximumCompactionInterval;\n+\n+  if (is_max_on_system_gc || is_interval_ended) {\n+    _maximum_compaction_gc_num = total_invocations;\n+    return true;\n+  }\n+\n+  return false;\n+}\n+\n+void PSParallelCompactNew::summary_phase() {\n+  GCTraceTime(Info, gc, phases) tm(\"Summary Phase\", &_gc_timer);\n+\n+  setup_regions_serial();\n+  setup_regions_parallel();\n+\n+#ifndef PRODUCT\n+  for (size_t idx = 0; idx < _num_regions; idx++) {\n+    PCRegionData* rd = &_region_data_array[idx];\n+    log_develop_trace(gc, compaction)(\"Compaction region #%zu: [\" PTR_FORMAT \", \" PTR_FORMAT \")\", rd->idx(), p2i(\n+            rd->bottom()), p2i(rd->end()));\n+  }\n+#endif\n+}\n+\n+\/\/ This method should contain all heap-specific policy for invoking a full\n+\/\/ collection.  invoke_no_policy() will only attempt to compact the heap; it\n+\/\/ will do nothing further.  If we need to bail out for policy reasons, scavenge\n+\/\/ before full gc, or any other specialized behavior, it needs to be added here.\n+\/\/\n+\/\/ Note that this method should only be called from the vm_thread while at a\n+\/\/ safepoint.\n+\/\/\n+\/\/ Note that the all_soft_refs_clear flag in the soft ref policy\n+\/\/ may be true because this method can be called without intervening\n+\/\/ activity.  For example when the heap space is tight and full measure\n+\/\/ are being taken to free space.\n+bool PSParallelCompactNew::invoke(bool clear_all_soft_refs, bool serial) {\n+  assert(SafepointSynchronize::is_at_safepoint(), \"should be at safepoint\");\n+  assert(Thread::current() == (Thread*)VMThread::vm_thread(),\n+         \"should be in vm thread\");\n+\n+  SvcGCMarker sgcm(SvcGCMarker::FULL);\n+  IsSTWGCActiveMark mark;\n+\n+  ParallelScavengeHeap* heap = ParallelScavengeHeap::heap();\n+  clear_all_soft_refs = clear_all_soft_refs\n+                     || heap->soft_ref_policy()->should_clear_all_soft_refs();\n+\n+  return PSParallelCompactNew::invoke_no_policy(clear_all_soft_refs, serial);\n+}\n+\n+\/\/ This method contains no policy. You should probably\n+\/\/ be calling invoke() instead.\n+bool PSParallelCompactNew::invoke_no_policy(bool clear_all_soft_refs, bool serial) {\n+  assert(SafepointSynchronize::is_at_safepoint(), \"must be at a safepoint\");\n+  assert(ref_processor() != nullptr, \"Sanity\");\n+\n+  ParallelScavengeHeap* heap = ParallelScavengeHeap::heap();\n+\n+  GCIdMark gc_id_mark;\n+  _gc_timer.register_gc_start();\n+  _gc_tracer.report_gc_start(heap->gc_cause(), _gc_timer.gc_start());\n+\n+  GCCause::Cause gc_cause = heap->gc_cause();\n+  PSYoungGen* young_gen = ParallelScavengeHeap::young_gen();\n+  PSOldGen* old_gen = ParallelScavengeHeap::old_gen();\n+  PSAdaptiveSizePolicy* size_policy = heap->size_policy();\n+\n+  \/\/ The scope of casr should end after code that can change\n+  \/\/ SoftRefPolicy::_should_clear_all_soft_refs.\n+  ClearedAllSoftRefs casr(clear_all_soft_refs,\n+                          heap->soft_ref_policy());\n+\n+  \/\/ Make sure data structures are sane, make the heap parsable, and do other\n+  \/\/ miscellaneous bookkeeping.\n+  pre_compact();\n+\n+  const PreGenGCValues pre_gc_values = heap->get_pre_gc_values();\n+\n+  {\n+    const uint active_workers =\n+      WorkerPolicy::calc_active_workers(ParallelScavengeHeap::heap()->workers().max_workers(),\n+                                        ParallelScavengeHeap::heap()->workers().active_workers(),\n+                                        Threads::number_of_non_daemon_threads());\n+    ParallelScavengeHeap::heap()->workers().set_active_workers(active_workers);\n+\n+    if (serial || check_maximum_compaction()) {\n+      \/\/ Serial compaction executes the forwarding and compaction phases serially,\n+      \/\/ thus achieving perfect compaction.\n+      \/\/ Marking and ajust-references would still be executed in parallel threads.\n+      _serial = true;\n+    } else {\n+      _serial = false;\n+    }\n+\n+    GCTraceCPUTime tcpu(&_gc_tracer);\n+    GCTraceTime(Info, gc) tm(\"Pause Full\", nullptr, gc_cause, true);\n+\n+    heap->pre_full_gc_dump(&_gc_timer);\n+\n+    TraceCollectorStats tcs(counters());\n+    TraceMemoryManagerStats tms(heap->old_gc_manager(), gc_cause, \"end of major GC\");\n+\n+    if (log_is_enabled(Debug, gc, heap, exit)) {\n+      accumulated_time()->start();\n+    }\n+\n+    \/\/ Let the size policy know we're starting\n+    size_policy->major_collection_begin();\n+\n+#if COMPILER2_OR_JVMCI\n+    DerivedPointerTable::clear();\n+#endif\n+\n+    ref_processor()->start_discovery(clear_all_soft_refs);\n+\n+    ClassUnloadingContext ctx(1 \/* num_nmethod_unlink_workers *\/,\n+                              false \/* unregister_nmethods_during_purge *\/,\n+                              false \/* lock_nmethod_free_separately *\/);\n+\n+    marking_phase(&_gc_tracer);\n+\n+    summary_phase();\n+\n+#if COMPILER2_OR_JVMCI\n+    assert(DerivedPointerTable::is_active(), \"Sanity\");\n+    DerivedPointerTable::set_active(false);\n+#endif\n+\n+    FullGCForwarding::begin();\n+\n+    forward_to_new_addr();\n+\n+    adjust_pointers();\n+\n+    compact();\n+\n+    FullGCForwarding::end();\n+\n+    ParCompactionManagerNew::_preserved_marks_set->restore(&ParallelScavengeHeap::heap()->workers());\n+\n+    \/\/ Reset the mark bitmap, summary data, and do other bookkeeping.  Must be\n+    \/\/ done before resizing.\n+    post_compact();\n+\n+    \/\/ Let the size policy know we're done\n+    size_policy->major_collection_end(old_gen->used_in_bytes(), gc_cause);\n+\n+    if (UseAdaptiveSizePolicy) {\n+      log_debug(gc, ergo)(\"AdaptiveSizeStart: collection: %d \", heap->total_collections());\n+      log_trace(gc, ergo)(\"old_gen_capacity: %zu young_gen_capacity: %zu\",\n+                          old_gen->capacity_in_bytes(), young_gen->capacity_in_bytes());\n+\n+      \/\/ Don't check if the size_policy is ready here.  Let\n+      \/\/ the size_policy check that internally.\n+      if (UseAdaptiveGenerationSizePolicyAtMajorCollection &&\n+          AdaptiveSizePolicy::should_update_promo_stats(gc_cause)) {\n+        \/\/ Swap the survivor spaces if from_space is empty. The\n+        \/\/ resize_young_gen() called below is normally used after\n+        \/\/ a successful young GC and swapping of survivor spaces;\n+        \/\/ otherwise, it will fail to resize the young gen with\n+        \/\/ the current implementation.\n+        if (young_gen->from_space()->is_empty()) {\n+          young_gen->from_space()->clear(SpaceDecorator::Mangle);\n+          young_gen->swap_spaces();\n+        }\n+\n+        \/\/ Calculate optimal free space amounts\n+        assert(young_gen->max_gen_size() >\n+          young_gen->from_space()->capacity_in_bytes() +\n+          young_gen->to_space()->capacity_in_bytes(),\n+          \"Sizes of space in young gen are out-of-bounds\");\n+\n+        size_t young_live = young_gen->used_in_bytes();\n+        size_t eden_live = young_gen->eden_space()->used_in_bytes();\n+        size_t old_live = old_gen->used_in_bytes();\n+        size_t cur_eden = young_gen->eden_space()->capacity_in_bytes();\n+        size_t max_old_gen_size = old_gen->max_gen_size();\n+        size_t max_eden_size = young_gen->max_gen_size() -\n+          young_gen->from_space()->capacity_in_bytes() -\n+          young_gen->to_space()->capacity_in_bytes();\n+\n+        \/\/ Used for diagnostics\n+        size_policy->clear_generation_free_space_flags();\n+\n+        size_policy->compute_generations_free_space(young_live,\n+                                                    eden_live,\n+                                                    old_live,\n+                                                    cur_eden,\n+                                                    max_old_gen_size,\n+                                                    max_eden_size,\n+                                                    true \/* full gc*\/);\n+\n+        size_policy->check_gc_overhead_limit(eden_live,\n+                                             max_old_gen_size,\n+                                             max_eden_size,\n+                                             true \/* full gc*\/,\n+                                             gc_cause,\n+                                             heap->soft_ref_policy());\n+\n+        size_policy->decay_supplemental_growth(true \/* full gc*\/);\n+\n+        heap->resize_old_gen(\n+          size_policy->calculated_old_free_size_in_bytes());\n+\n+        heap->resize_young_gen(size_policy->calculated_eden_size_in_bytes(),\n+                               size_policy->calculated_survivor_size_in_bytes());\n+      }\n+\n+      log_debug(gc, ergo)(\"AdaptiveSizeStop: collection: %d \", heap->total_collections());\n+    }\n+\n+    if (UsePerfData) {\n+      PSGCAdaptivePolicyCounters* const counters = ParallelScavengeHeap::gc_policy_counters();\n+      counters->update_counters();\n+      counters->update_old_capacity(old_gen->capacity_in_bytes());\n+      counters->update_young_capacity(young_gen->capacity_in_bytes());\n+    }\n+\n+    heap->resize_all_tlabs();\n+\n+    \/\/ Resize the metaspace capacity after a collection\n+    MetaspaceGC::compute_new_size();\n+\n+    if (log_is_enabled(Debug, gc, heap, exit)) {\n+      accumulated_time()->stop();\n+    }\n+\n+    heap->print_heap_change(pre_gc_values);\n+\n+    \/\/ Track memory usage and detect low memory\n+    MemoryService::track_memory_usage();\n+    heap->update_counters();\n+\n+    heap->post_full_gc_dump(&_gc_timer);\n+  }\n+\n+  if (VerifyAfterGC && heap->total_collections() >= VerifyGCStartAt) {\n+    Universe::verify(\"After GC\");\n+  }\n+\n+  heap->print_after_gc();\n+  heap->trace_heap_after_gc(&_gc_tracer);\n+\n+  AdaptiveSizePolicyOutput::print(size_policy, heap->total_collections());\n+\n+  _gc_timer.register_gc_end();\n+\n+  _gc_tracer.report_gc_end(_gc_timer.gc_end(), _gc_timer.time_partitions());\n+\n+  return true;\n+}\n+\n+class PCAddThreadRootsMarkingTaskClosureNew : public ThreadClosure {\n+private:\n+  uint _worker_id;\n+\n+public:\n+  explicit PCAddThreadRootsMarkingTaskClosureNew(uint worker_id) : _worker_id(worker_id) { }\n+  void do_thread(Thread* thread) final {\n+    assert(ParallelScavengeHeap::heap()->is_stw_gc_active(), \"called outside gc\");\n+\n+    ResourceMark rm;\n+\n+    ParCompactionManagerNew* cm = ParCompactionManagerNew::gc_thread_compaction_manager(_worker_id);\n+\n+    MarkingNMethodClosure mark_and_push_in_blobs(&cm->_mark_and_push_closure,\n+                                                 !NMethodToOopClosure::FixRelocations,\n+                                                 true \/* keepalive nmethods *\/);\n+\n+    thread->oops_do(&cm->_mark_and_push_closure, &mark_and_push_in_blobs);\n+\n+    \/\/ Do the real work\n+    cm->follow_marking_stacks();\n+  }\n+};\n+\n+void steal_marking_work_new(TaskTerminator& terminator, uint worker_id) {\n+  assert(ParallelScavengeHeap::heap()->is_stw_gc_active(), \"called outside gc\");\n+\n+  ParCompactionManagerNew* cm =\n+    ParCompactionManagerNew::gc_thread_compaction_manager(worker_id);\n+\n+  do {\n+    ScannerTask task;\n+    if (ParCompactionManagerNew::steal(worker_id, task)) {\n+      cm->follow_contents(task, true);\n+    }\n+    cm->follow_marking_stacks();\n+  } while (!terminator.offer_termination());\n+}\n+\n+class MarkFromRootsTaskNew : public WorkerTask {\n+  StrongRootsScope _strong_roots_scope; \/\/ needed for Threads::possibly_parallel_threads_do\n+  OopStorageSetStrongParState<false \/* concurrent *\/, false \/* is_const *\/> _oop_storage_set_par_state;\n+  TaskTerminator _terminator;\n+  uint _active_workers;\n+\n+public:\n+  explicit MarkFromRootsTaskNew(uint active_workers) :\n+      WorkerTask(\"MarkFromRootsTaskNew\"),\n+      _strong_roots_scope(active_workers),\n+      _terminator(active_workers, ParCompactionManagerNew::marking_stacks()),\n+      _active_workers(active_workers) {}\n+\n+  void work(uint worker_id) final {\n+    ParCompactionManagerNew* cm = ParCompactionManagerNew::gc_thread_compaction_manager(worker_id);\n+    {\n+      CLDToOopClosure cld_closure(&cm->_mark_and_push_closure, ClassLoaderData::_claim_stw_fullgc_mark);\n+      ClassLoaderDataGraph::always_strong_cld_do(&cld_closure);\n+\n+      \/\/ Do the real work\n+      cm->follow_marking_stacks();\n+    }\n+\n+    {\n+      PCAddThreadRootsMarkingTaskClosureNew closure(worker_id);\n+      Threads::possibly_parallel_threads_do(_active_workers > 1 \/* is_par *\/, &closure);\n+    }\n+\n+    \/\/ Mark from OopStorages\n+    {\n+      _oop_storage_set_par_state.oops_do(&cm->_mark_and_push_closure);\n+      \/\/ Do the real work\n+      cm->follow_marking_stacks();\n+    }\n+\n+    if (_active_workers > 1) {\n+      steal_marking_work_new(_terminator, worker_id);\n+    }\n+  }\n+};\n+\n+class ParallelCompactRefProcProxyTaskNew : public RefProcProxyTask {\n+  TaskTerminator _terminator;\n+\n+public:\n+  explicit ParallelCompactRefProcProxyTaskNew(uint max_workers)\n+    : RefProcProxyTask(\"ParallelCompactRefProcProxyTaskNew\", max_workers),\n+      _terminator(_max_workers, ParCompactionManagerNew::marking_stacks()) {}\n+\n+  void work(uint worker_id) final {\n+    assert(worker_id < _max_workers, \"sanity\");\n+    ParCompactionManagerNew* cm = (_tm == RefProcThreadModel::Single) ? ParCompactionManagerNew::get_vmthread_cm() : ParCompactionManagerNew::gc_thread_compaction_manager(worker_id);\n+    BarrierEnqueueDiscoveredFieldClosure enqueue;\n+    ParCompactionManagerNew::FollowStackClosure complete_gc(cm, (_tm == RefProcThreadModel::Single) ? nullptr : &_terminator, worker_id);\n+    _rp_task->rp_work(worker_id, PSParallelCompactNew::is_alive_closure(), &cm->_mark_and_push_closure, &enqueue, &complete_gc);\n+  }\n+\n+  void prepare_run_task_hook() final {\n+    _terminator.reset_for_reuse(_queue_count);\n+  }\n+};\n+\n+void PSParallelCompactNew::marking_phase(ParallelOldTracer *gc_tracer) {\n+  \/\/ Recursively traverse all live objects and mark them\n+  GCTraceTime(Info, gc, phases) tm(\"Marking Phase\", &_gc_timer);\n+\n+  uint active_gc_threads = ParallelScavengeHeap::heap()->workers().active_workers();\n+\n+  ClassLoaderDataGraph::verify_claimed_marks_cleared(ClassLoaderData::_claim_stw_fullgc_mark);\n+  {\n+    GCTraceTime(Debug, gc, phases) pm_tm(\"Par Mark\", &_gc_timer);\n+\n+    MarkFromRootsTaskNew task(active_gc_threads);\n+    ParallelScavengeHeap::heap()->workers().run_task(&task);\n+  }\n+\n+  \/\/ Process reference objects found during marking\n+  {\n+    GCTraceTime(Debug, gc, phases) rp_tm(\"Reference Processing\", &_gc_timer);\n+\n+    ReferenceProcessorStats stats;\n+    ReferenceProcessorPhaseTimes pt(&_gc_timer, ref_processor()->max_num_queues());\n+\n+    ParallelCompactRefProcProxyTaskNew task(ref_processor()->max_num_queues());\n+    stats = ref_processor()->process_discovered_references(task, &ParallelScavengeHeap::heap()->workers(), pt);\n+\n+    gc_tracer->report_gc_reference_stats(stats);\n+    pt.print_all_references();\n+  }\n+\n+  \/\/ This is the point where the entire marking should have completed.\n+  ParCompactionManagerNew::verify_all_marking_stack_empty();\n+\n+  {\n+    GCTraceTime(Debug, gc, phases) wp_tm(\"Weak Processing\", &_gc_timer);\n+    WeakProcessor::weak_oops_do(&ParallelScavengeHeap::heap()->workers(),\n+                                is_alive_closure(),\n+                                &do_nothing_cl,\n+                                1);\n+  }\n+\n+  {\n+    GCTraceTime(Debug, gc, phases) tm_m(\"Class Unloading\", &_gc_timer);\n+\n+    ClassUnloadingContext* ctx = ClassUnloadingContext::context();\n+\n+    bool unloading_occurred;\n+    {\n+      CodeCache::UnlinkingScope scope(is_alive_closure());\n+\n+      \/\/ Follow system dictionary roots and unload classes.\n+      unloading_occurred = SystemDictionary::do_unloading(&_gc_timer);\n+\n+      \/\/ Unload nmethods.\n+      CodeCache::do_unloading(unloading_occurred);\n+    }\n+\n+    {\n+      GCTraceTime(Debug, gc, phases) t(\"Purge Unlinked NMethods\", gc_timer());\n+      \/\/ Release unloaded nmethod's memory.\n+      ctx->purge_nmethods();\n+    }\n+    {\n+      GCTraceTime(Debug, gc, phases) ur(\"Unregister NMethods\", &_gc_timer);\n+      ParallelScavengeHeap::heap()->prune_unlinked_nmethods();\n+    }\n+    {\n+      GCTraceTime(Debug, gc, phases) t(\"Free Code Blobs\", gc_timer());\n+      ctx->free_nmethods();\n+    }\n+\n+    \/\/ Prune dead klasses from subklass\/sibling\/implementor lists.\n+    Klass::clean_weak_klass_links(unloading_occurred);\n+\n+    \/\/ Clean JVMCI metadata handles.\n+    JVMCI_ONLY(JVMCI::do_unloading(unloading_occurred));\n+  }\n+\n+  {\n+    GCTraceTime(Debug, gc, phases) roc_tm(\"Report Object Count\", &_gc_timer);\n+    _gc_tracer.report_object_count_after_gc(is_alive_closure(), &ParallelScavengeHeap::heap()->workers());\n+  }\n+#if TASKQUEUE_STATS\n+  ParCompactionManagerNew::print_and_reset_taskqueue_stats();\n+#endif\n+}\n+\n+void PSParallelCompactNew::adjust_pointers_in_spaces(uint worker_id) {\n+  auto start_time = Ticks::now();\n+  for (size_t i = 0; i < _num_regions; i++) {\n+    PCRegionData* region = &_region_data_array[i];\n+    if (!region->claim()) {\n+      continue;\n+    }\n+    log_trace(gc, compaction)(\"Adjusting pointers in region: %zu (worker_id: %u)\", region->idx(), worker_id);\n+    HeapWord* end = region->top();\n+    HeapWord* current = _mark_bitmap.find_obj_beg(region->bottom(), end);\n+    while (current < end) {\n+      assert(_mark_bitmap.is_marked(current), \"must be marked\");\n+      oop obj = cast_to_oop(current);\n+      size_t size = obj->size();\n+      obj->oop_iterate(&pc_adjust_pointer_closure);\n+      current = _mark_bitmap.find_obj_beg(current + size, end);\n+    }\n+  }\n+  log_trace(gc, phases)(\"adjust_pointers_in_spaces worker %u: %.3f ms\", worker_id, (Ticks::now() - start_time).seconds() * 1000);\n+}\n+\n+class PSAdjustTaskNew final : public WorkerTask {\n+  SubTasksDone                               _sub_tasks;\n+  WeakProcessor::Task                        _weak_proc_task;\n+  OopStorageSetStrongParState<false, false>  _oop_storage_iter;\n+  uint                                       _nworkers;\n+\n+  enum PSAdjustSubTask {\n+    PSAdjustSubTask_code_cache,\n+\n+    PSAdjustSubTask_num_elements\n+  };\n+\n+public:\n+  explicit PSAdjustTaskNew(uint nworkers) :\n+    WorkerTask(\"PSAdjust task\"),\n+    _sub_tasks(PSAdjustSubTask_num_elements),\n+    _weak_proc_task(nworkers),\n+    _nworkers(nworkers) {\n+\n+    ClassLoaderDataGraph::verify_claimed_marks_cleared(ClassLoaderData::_claim_stw_fullgc_adjust);\n+    if (nworkers > 1) {\n+      Threads::change_thread_claim_token();\n+    }\n+  }\n+\n+  ~PSAdjustTaskNew() {\n+    Threads::assert_all_threads_claimed();\n+  }\n+\n+  void work(uint worker_id) final {\n+    ParCompactionManagerNew* cm = ParCompactionManagerNew::gc_thread_compaction_manager(worker_id);\n+    cm->preserved_marks()->adjust_during_full_gc();\n+    {\n+      \/\/ adjust pointers in all spaces\n+      PSParallelCompactNew::adjust_pointers_in_spaces(worker_id);\n+    }\n+    {\n+      ResourceMark rm;\n+      Threads::possibly_parallel_oops_do(_nworkers > 1, &pc_adjust_pointer_closure, nullptr);\n+    }\n+    _oop_storage_iter.oops_do(&pc_adjust_pointer_closure);\n+    {\n+      CLDToOopClosure cld_closure(&pc_adjust_pointer_closure, ClassLoaderData::_claim_stw_fullgc_adjust);\n+      ClassLoaderDataGraph::cld_do(&cld_closure);\n+    }\n+    {\n+      AlwaysTrueClosure always_alive;\n+      _weak_proc_task.work(worker_id, &always_alive, &pc_adjust_pointer_closure);\n+    }\n+    if (_sub_tasks.try_claim_task(PSAdjustSubTask_code_cache)) {\n+      NMethodToOopClosure adjust_code(&pc_adjust_pointer_closure, NMethodToOopClosure::FixRelocations);\n+      CodeCache::nmethods_do(&adjust_code);\n+    }\n+    _sub_tasks.all_tasks_claimed();\n+  }\n+};\n+\n+void PSParallelCompactNew::adjust_pointers() {\n+  \/\/ Adjust the pointers to reflect the new locations\n+  GCTraceTime(Info, gc, phases) tm(\"Adjust Pointers\", &_gc_timer);\n+  uint num_workers = ParallelScavengeHeap::heap()->workers().active_workers();\n+  PSAdjustTaskNew task(num_workers);\n+  ParallelScavengeHeap::heap()->workers().run_task(&task);\n+}\n+\n+void PSParallelCompactNew::forward_to_new_addr() {\n+  GCTraceTime(Info, gc, phases) tm(\"Forward\", &_gc_timer);\n+  uint num_workers = get_num_workers();\n+  _per_worker_region_data = NEW_C_HEAP_ARRAY(PCRegionData*, num_workers, mtGC);\n+  for (uint i = 0; i < num_workers; i++) {\n+    _per_worker_region_data[i] = nullptr;\n+  }\n+\n+  class ForwardState {\n+    uint _worker_id;\n+    PCRegionData* _compaction_region;\n+    HeapWord* _compaction_point;\n+\n+    void ensure_compaction_point() {\n+      if (_compaction_point == nullptr) {\n+        assert(_compaction_region == nullptr, \"invariant\");\n+        _compaction_region = _per_worker_region_data[_worker_id];\n+        assert(_compaction_region != nullptr, \"invariant\");\n+        _compaction_point = _compaction_region->bottom();\n+      }\n+    }\n+  public:\n+    explicit ForwardState(uint worker_id) :\n+            _worker_id(worker_id),\n+            _compaction_region(nullptr),\n+            _compaction_point(nullptr) {\n+    }\n+\n+    size_t available() const {\n+      return pointer_delta(_compaction_region->end(), _compaction_point);\n+    }\n+\n+    void forward_objs_in_region(ParCompactionManagerNew* cm, PCRegionData* region) {\n+      ensure_compaction_point();\n+      HeapWord* end = region->end();\n+      HeapWord* current = _mark_bitmap.find_obj_beg(region->bottom(), end);\n+      while (current < end) {\n+        assert(_mark_bitmap.is_marked(current), \"must be marked\");\n+        oop obj = cast_to_oop(current);\n+        assert(region->contains(obj), \"object must not cross region boundary: obj: \" PTR_FORMAT \", obj_end: \" PTR_FORMAT \", region start: \" PTR_FORMAT \", region end: \" PTR_FORMAT, p2i(obj), p2i(cast_from_oop<HeapWord*>(obj) + obj->size()), p2i(region->bottom()), p2i(region->end()));\n+        size_t old_size = obj->size();\n+        size_t new_size = obj->copy_size(old_size, obj->mark());\n+        size_t size = (current == _compaction_point) ? old_size : new_size;\n+        while (size > available()) {\n+          _compaction_region->set_new_top(_compaction_point);\n+          _compaction_region = _compaction_region->local_next();\n+          assert(_compaction_region != nullptr, \"must find a compaction region\");\n+          _compaction_point = _compaction_region->bottom();\n+          size = (current == _compaction_point) ? old_size : new_size;\n+        }\n+        \/\/log_develop_trace(gc, compaction)(\"Forwarding obj: \" PTR_FORMAT \", to: \" PTR_FORMAT, p2i(obj), p2i(_compaction_point));\n+        if (current != _compaction_point) {\n+          cm->preserved_marks()->push_if_necessary(obj, obj->mark());\n+          FullGCForwarding::forward_to(obj, cast_to_oop(_compaction_point));\n+        }\n+        _compaction_point += size;\n+        assert(_compaction_point <= _compaction_region->end(), \"object must fit in region\");\n+        current += old_size;\n+        assert(current <= end, \"object must not cross region boundary\");\n+        current = _mark_bitmap.find_obj_beg(current, end);\n+      }\n+    }\n+    void finish() {\n+      if (_compaction_region != nullptr) {\n+        _compaction_region->set_new_top(_compaction_point);\n+      }\n+    }\n+  };\n+\n+  struct ForwardTask final : public WorkerTask {\n+    ForwardTask() : WorkerTask(\"PSForward task\") {}\n+\n+    void work(uint worker_id) override {\n+      ParCompactionManagerNew* cm = ParCompactionManagerNew::gc_thread_compaction_manager(worker_id);\n+      ForwardState state(worker_id);\n+      PCRegionData** last_link = &_per_worker_region_data[worker_id];\n+      size_t idx = worker_id;\n+      uint num_workers = get_num_workers();\n+      size_t num_regions = get_num_regions();\n+      PCRegionData* region_data_array = get_region_data_array();\n+      while (idx < num_regions) {\n+        PCRegionData* region = region_data_array + idx;\n+        *last_link = region;\n+        last_link = region->local_next_addr();\n+        state.forward_objs_in_region(cm, region);\n+        idx += num_workers;\n+      }\n+      state.finish();\n+    }\n+  } task;\n+\n+  uint par_workers = ParallelScavengeHeap::heap()->workers().active_workers();\n+  ParallelScavengeHeap::heap()->workers().set_active_workers(num_workers);\n+  ParallelScavengeHeap::heap()->workers().run_task(&task);\n+  ParallelScavengeHeap::heap()->workers().set_active_workers(par_workers);\n+\n+#ifndef PRODUCT\n+  for (uint wid = 0; wid < num_workers; wid++) {\n+    for (PCRegionData* rd = _per_worker_region_data[wid]; rd != nullptr; rd = rd->local_next()) {\n+      log_develop_trace(gc, compaction)(\"Per worker compaction region, worker: %d, #%zu: [\" PTR_FORMAT \", \" PTR_FORMAT \"), new_top: \" PTR_FORMAT, wid, rd->idx(),\n+                                        p2i(rd->bottom()), p2i(rd->end()), p2i(rd->new_top()));\n+    }\n+  }\n+#endif\n+}\n+\n+void PSParallelCompactNew::compact() {\n+  GCTraceTime(Info, gc, phases) tm(\"Compaction Phase\", &_gc_timer);\n+  class CompactTask final : public WorkerTask {\n+    static void compact_region(PCRegionData* region) {\n+      HeapWord* bottom = region->bottom();\n+      HeapWord* end = region->top();\n+      if (bottom == end) {\n+        return;\n+      }\n+      HeapWord* current = _mark_bitmap.find_obj_beg(bottom, end);\n+      while (current < end) {\n+        oop obj = cast_to_oop(current);\n+        size_t size = obj->size();\n+        if (FullGCForwarding::is_forwarded(obj)) {\n+          oop fwd = FullGCForwarding::forwardee(obj);\n+          auto* dst = cast_from_oop<HeapWord*>(fwd);\n+          ObjectStartArray* sa = start_array(space_id(dst));\n+          if (sa != nullptr) {\n+            assert(dst != current, \"expect moving object\");\n+            size_t new_words = obj->copy_size(size, obj->mark());\n+            sa->update_for_block(dst, dst + new_words);\n+          }\n+\n+          Copy::aligned_conjoint_words(current, dst, size);\n+          fwd->init_mark();\n+          fwd->initialize_hash_if_necessary(obj);\n+        } else {\n+          \/\/ The start_array must be updated even if the object is not moving.\n+          ObjectStartArray* sa = start_array(space_id(current));\n+          if (sa != nullptr) {\n+            sa->update_for_block(current, current + size);\n+          }\n+        }\n+        current = _mark_bitmap.find_obj_beg(current + size, end);\n+      }\n+    }\n+  public:\n+    explicit CompactTask() : WorkerTask(\"PSCompact task\") {}\n+    void work(uint worker_id) override {\n+      PCRegionData* region = _per_worker_region_data[worker_id];\n+      while (region != nullptr) {\n+        log_trace(gc)(\"Compact worker: %u, compacting region: %zu\", worker_id, region->idx());\n+        compact_region(region);\n+        region = region->local_next();\n+      }\n+    }\n+  } task;\n+\n+  uint num_workers = get_num_workers();\n+  uint par_workers = ParallelScavengeHeap::heap()->workers().active_workers();\n+  ParallelScavengeHeap::heap()->workers().set_active_workers(num_workers);\n+  ParallelScavengeHeap::heap()->workers().run_task(&task);\n+  ParallelScavengeHeap::heap()->workers().set_active_workers(par_workers);\n+}\n+\n+\/\/ Return the SpaceId for the space containing addr.  If addr is not in the\n+\/\/ heap, last_space_id is returned.  In debug mode it expects the address to be\n+\/\/ in the heap and asserts such.\n+PSParallelCompactNew::SpaceId PSParallelCompactNew::space_id(HeapWord* addr) {\n+  assert(ParallelScavengeHeap::heap()->is_in_reserved(addr), \"addr not in the heap\");\n+\n+  for (unsigned int id = old_space_id; id < last_space_id; ++id) {\n+    if (_space_info[id].space()->contains(addr)) {\n+      return SpaceId(id);\n+    }\n+  }\n+\n+  assert(false, \"no space contains the addr\");\n+  return last_space_id;\n+}\n","filename":"src\/hotspot\/share\/gc\/parallel\/psParallelCompactNew.cpp","additions":1146,"deletions":0,"binary":false,"changes":1146,"status":"added"},{"patch":"@@ -0,0 +1,338 @@\n+\/*\n+ * Copyright (c) 2005, 2024, Oracle and\/or its affiliates. All rights reserved.\n+ * Copyright Amazon.com Inc. or its affiliates. All Rights Reserved.\n+ * * DO NOT ALTER OR REMOVE COPYRIGHT NOTICES OR THIS FILE HEADER.\n+ *\n+ * This code is free software; you can redistribute it and\/or modify it\n+ * under the terms of the GNU General Public License version 2 only, as\n+ * published by the Free Software Foundation.\n+ *\n+ * This code is distributed in the hope that it will be useful, but WITHOUT\n+ * ANY WARRANTY; without even the implied warranty of MERCHANTABILITY or\n+ * FITNESS FOR A PARTICULAR PURPOSE.  See the GNU General Public License\n+ * version 2 for more details (a copy is included in the LICENSE file that\n+ * accompanied this code).\n+ *\n+ * You should have received a copy of the GNU General Public License version\n+ * 2 along with this work; if not, write to the Free Software Foundation,\n+ * Inc., 51 Franklin St, Fifth Floor, Boston, MA 02110-1301 USA.\n+ *\n+ * Please contact Oracle, 500 Oracle Parkway, Redwood Shores, CA 94065 USA\n+ * or visit www.oracle.com if you need additional information or have any\n+ * questions.\n+ *\n+ *\/\n+\n+#ifndef SHARE_GC_PARALLEL_PSPARALLELCOMPACTNEW_HPP\n+#define SHARE_GC_PARALLEL_PSPARALLELCOMPACTNEW_HPP\n+\n+#include \"gc\/parallel\/mutableSpace.hpp\"\n+#include \"gc\/parallel\/objectStartArray.hpp\"\n+#include \"gc\/parallel\/parallelScavengeHeap.hpp\"\n+#include \"gc\/parallel\/parMarkBitMap.hpp\"\n+#include \"gc\/shared\/collectedHeap.hpp\"\n+#include \"gc\/shared\/collectorCounters.hpp\"\n+#include \"gc\/shared\/referenceProcessor.hpp\"\n+#include \"gc\/shared\/taskTerminator.hpp\"\n+#include \"oops\/oop.hpp\"\n+#include \"runtime\/atomic.hpp\"\n+#include \"runtime\/orderAccess.hpp\"\n+\n+class ParallelScavengeHeap;\n+class PSAdaptiveSizePolicy;\n+class PSYoungGen;\n+class PSOldGen;\n+class ParCompactionManagerNew;\n+class PSParallelCompactNew;\n+class ParallelOldTracer;\n+class STWGCTimer;\n+\n+class SpaceInfoNew\n+{\n+public:\n+  MutableSpace* space() const { return _space; }\n+\n+  \/\/ The start array for the (generation containing the) space, or null if there\n+  \/\/ is no start array.\n+  ObjectStartArray* start_array() const { return _start_array; }\n+\n+  void set_space(MutableSpace* s)           { _space = s; }\n+  void set_start_array(ObjectStartArray* s) { _start_array = s; }\n+\n+private:\n+  MutableSpace*     _space;\n+  ObjectStartArray* _start_array;\n+};\n+\n+\/\/ The Parallel compaction collector is a stop-the-world garbage collector that\n+\/\/ does parts of the collection using parallel threads.  The collection includes\n+\/\/ the tenured generation and the young generation.\n+\/\/\n+\/\/ A collection consists of the following phases.\n+\/\/\n+\/\/      - marking phase\n+\/\/      - summary phase (single-threaded)\n+\/\/      - forward (to new address) phase\n+\/\/      - adjust pointers phase\n+\/\/      - compacting phase\n+\/\/      - clean up phase\n+\/\/\n+\/\/ Roughly speaking these phases correspond, respectively, to\n+\/\/\n+\/\/      - mark all the live objects\n+\/\/      - set-up temporary regions to enable parallelism in following phases\n+\/\/      - calculate the destination of each object at the end of the collection\n+\/\/      - adjust pointers to reflect new destination of objects\n+\/\/      - move the objects to their destination\n+\/\/      - update some references and reinitialize some variables\n+\/\/\n+\/\/ A space that is being collected is divided into regions and with each region\n+\/\/ is associated an object of type PCRegionData. Regions are targeted to be of\n+\/\/ a mostly uniform size, but if an object would cross a region boundary, then\n+\/\/ the boundary is adjusted to be after the end of that object.\n+\/\/\n+\/\/ The marking phase does a complete marking of all live objects in the heap.\n+\/\/ The marking phase uses multiple GC threads and marking is done in a bit\n+\/\/ array of type ParMarkBitMap.  The marking of the bit map is done atomically.\n+\/\/\n+\/\/ The summary phase sets up the regions, such that region covers roughly\n+\/\/ uniform memory regions (currently same size as SpaceAlignment). However,\n+\/\/ if that would result in an object crossing a region boundary, then\n+\/\/ the upper bounds is adjusted such that the region ends after that object.\n+\/\/ This way we can ensure that a GC worker thread can fully 'own' a region\n+\/\/ during the forwarding, adjustment and compaction phases, without worrying\n+\/\/ about other threads messing with parts of the object. The summary phase\n+\/\/ also sets up an alternative set of regions, where each region covers\n+\/\/ a single space. This is used for a serial compaction mode which achieves\n+\/\/ maximum compaction at the expense of parallelism during the forwarding\n+\/\/ compaction phases.\n+\/\/\n+\/\/ The forwarding phase calculates the new address of each live\n+\/\/ object and records old-addr-to-new-addr association. It does this using\n+\/\/ multiple GC threads. Each thread 'claims' a source region and appends it to a\n+\/\/ local work-list. The region is also set as the current compaction region\n+\/\/ for that thread. All live objects in the region are then visited and its\n+\/\/ new location calculated by tracking the compaction point in the compaction\n+\/\/ region. Once the source region is exhausted, the next source region is\n+\/\/ claimed from the global pool and appended to the end of the local work-list.\n+\/\/ Once the compaction region is exhausted, the top of the old compaction region\n+\/\/ is recorded, and the next compaction region is fetched from the front of the\n+\/\/ local work-list (which is guaranteed to already have finished processing, or\n+\/\/ is the same as the source region). This way, each worker forms a local\n+\/\/ list of regions in which the worker can compact as if it were a serial\n+\/\/ compaction.\n+\/\/\n+\/\/ The adjust pointers phase remaps all pointers to reflect the new address of each\n+\/\/ object. Again, this uses multiple GC worker threads. Each thread claims\n+\/\/ a region, processes all references in all live objects of that region. Then\n+\/\/ the thread proceeds to claim the next region from the global pool, until\n+\/\/ all regions have been processed.\n+\/\/\n+\/\/ The compaction phase moves objects to their new location. Again, this uses\n+\/\/ multiple GC worker threads. Each worker processes the local work-list that\n+\/\/ has been set-up during the forwarding phase and processes it from bottom\n+\/\/ to top, copying each live object to its new location (which is guaranteed\n+\/\/ to be lower in that threads parts of the heap, and thus would never overwrite\n+\/\/ other objects).\n+\/\/\n+\/\/ This algorithm will usually leave gaps of non-fillable memory at the end\n+\/\/ of regions, and potentially whole empty regions at the end of compaction.\n+\/\/ The post-compaction phase fills those gaps with filler objects to ensure\n+\/\/ that the heap remains parsable.\n+\/\/\n+\/\/ In some situations, this inefficiency of leaving gaps can lead to a\n+\/\/ situation where after a full GC, it is still not possible to satisfy an\n+\/\/ allocation, even though there should be enough memory available. When\n+\/\/ that happens, the collector switches to a serial mode, where we only\n+\/\/ have 4 regions which correspond exaxtly to the 4 spaces, and the forwarding\n+\/\/ and compaction phases are executed using only a single thread. This\n+\/\/ achieves maximum compaction. This serial mode is also invoked when\n+\/\/ System.gc() is called *and* UseMaximumCompactionOnSystemGC is set to\n+\/\/ true (which is the default), or when the number of full GCs exceeds\n+\/\/ the HeapMaximumCompactionInterval.\n+\/\/\n+\/\/ Possible improvements to the algorithm include:\n+\/\/ - Identify and ignore a 'dense prefix'. This requires that we collect\n+\/\/   liveness data during marking, or that we scan the prefix object-by-object\n+\/\/   during the summary phase.\n+\/\/ - When an object does not fit into a remaining gap of a region, and the\n+\/\/   object is rather large, we could attempt to forward\/compact subsequent\n+\/\/   objects 'around' that large object in an attempt to minimize the\n+\/\/   resulting gap. This could be achieved by reconfiguring the regions\n+\/\/   to exclude the large object.\n+\/\/ - Instead of finding out *after* the whole compaction that an allocation\n+\/\/   can still not be satisfied, and then re-running the whole compaction\n+\/\/   serially, we could determine that after the forwarding phase, and\n+\/\/   re-do only forwarding serially, thus avoiding running marking,\n+\/\/   adjusting references and compaction twice.\n+class PCRegionData \/*: public CHeapObj<mtGC> *\/ {\n+  \/\/ A region index\n+  size_t const _idx;\n+\n+  \/\/ The start of the region\n+  HeapWord* const _bottom;\n+  \/\/ The top of the region. (first word after last live object in containing space)\n+  HeapWord* const _top;\n+  \/\/ The end of the region (first word after last word of the region)\n+  HeapWord* const _end;\n+\n+  \/\/ The next compaction address\n+  HeapWord* _new_top;\n+\n+  \/\/ Points to the next region in the GC-worker-local work-list\n+  PCRegionData* _local_next;\n+\n+  \/\/ Parallel workers claiming protocol, used during adjust-references phase.\n+  volatile bool _claimed;\n+\n+public:\n+\n+  PCRegionData(size_t idx, HeapWord* bottom, HeapWord* top, HeapWord* end) :\n+    _idx(idx), _bottom(bottom), _top(top), _end(end), _new_top(bottom),\n+          _local_next(nullptr), _claimed(false) {}\n+\n+  size_t idx() const { return _idx; };\n+\n+  HeapWord* bottom() const { return _bottom; }\n+  HeapWord* top() const { return _top; }\n+  HeapWord* end()   const { return _end;   }\n+\n+  PCRegionData*  local_next() const { return _local_next; }\n+  PCRegionData** local_next_addr() { return &_local_next; }\n+\n+  HeapWord* new_top() const {\n+    return _new_top;\n+  }\n+  void set_new_top(HeapWord* new_top) {\n+    _new_top = new_top;\n+  }\n+\n+  bool contains(oop obj) {\n+    auto* obj_start = cast_from_oop<HeapWord*>(obj);\n+    HeapWord* obj_end = obj_start + obj->size();\n+    return _bottom <= obj_start && obj_start < _end && _bottom < obj_end && obj_end <= _end;\n+  }\n+\n+  bool claim() {\n+    bool claimed =  _claimed;\n+    if (claimed) {\n+      return false;\n+    }\n+    return !Atomic::cmpxchg(&_claimed, false, true);\n+  }\n+};\n+\n+class PSParallelCompactNew : AllStatic {\n+public:\n+  typedef enum {\n+    old_space_id, eden_space_id,\n+    from_space_id, to_space_id, last_space_id\n+  } SpaceId;\n+\n+public:\n+  class IsAliveClosure: public BoolObjectClosure {\n+  public:\n+    bool do_object_b(oop p) final;\n+  };\n+\n+private:\n+  static STWGCTimer           _gc_timer;\n+  static ParallelOldTracer    _gc_tracer;\n+  static elapsedTimer         _accumulated_time;\n+  static unsigned int         _maximum_compaction_gc_num;\n+  static CollectorCounters*   _counters;\n+  static ParMarkBitMap        _mark_bitmap;\n+  static IsAliveClosure       _is_alive_closure;\n+  static SpaceInfoNew         _space_info[last_space_id];\n+\n+  \/\/ The head of the global region data list.\n+  static size_t               _num_regions;\n+  static PCRegionData*        _region_data_array;\n+  static PCRegionData**       _per_worker_region_data;\n+\n+  static size_t               _num_regions_serial;\n+  static PCRegionData*        _region_data_array_serial;\n+  static bool                 _serial;\n+\n+  \/\/ Reference processing (used in ...follow_contents)\n+  static SpanSubjectToDiscoveryClosure  _span_based_discoverer;\n+  static ReferenceProcessor*  _ref_processor;\n+\n+  static uint get_num_workers() { return _serial ? 1 : ParallelScavengeHeap::heap()->workers().active_workers(); }\n+  static size_t get_num_regions() { return _serial ? _num_regions_serial : _num_regions; }\n+  static PCRegionData* get_region_data_array() { return _serial ? _region_data_array_serial : _region_data_array; }\n+\n+public:\n+  static ParallelOldTracer* gc_tracer() { return &_gc_tracer; }\n+\n+private:\n+\n+  static void initialize_space_info();\n+\n+  \/\/ Clear the marking bitmap and summary data that cover the specified space.\n+  static void clear_data_covering_space(SpaceId id);\n+\n+  static void pre_compact();\n+\n+  static void post_compact();\n+\n+  static bool check_maximum_compaction();\n+\n+  \/\/ Mark live objects\n+  static void marking_phase(ParallelOldTracer *gc_tracer);\n+\n+  static void summary_phase();\n+  static void setup_regions_parallel();\n+  static void setup_regions_serial();\n+\n+  static void adjust_pointers();\n+  static void forward_to_new_addr();\n+\n+  \/\/ Move objects to new locations.\n+  static void compact();\n+\n+public:\n+  static bool invoke(bool maximum_heap_compaction, bool serial);\n+  static bool invoke_no_policy(bool maximum_heap_compaction, bool serial);\n+\n+  static void adjust_pointers_in_spaces(uint worker_id);\n+\n+  static void post_initialize();\n+  \/\/ Perform initialization for PSParallelCompactNew that requires\n+  \/\/ allocations.  This should be called during the VM initialization\n+  \/\/ at a pointer where it would be appropriate to return a JNI_ENOMEM\n+  \/\/ in the event of a failure.\n+  static bool initialize_aux_data();\n+\n+  \/\/ Closure accessors\n+  static BoolObjectClosure* is_alive_closure()     { return &_is_alive_closure; }\n+\n+  \/\/ Public accessors\n+  static elapsedTimer* accumulated_time() { return &_accumulated_time; }\n+\n+  static CollectorCounters* counters()    { return _counters; }\n+\n+  static inline bool is_marked(oop obj);\n+\n+  template <class T> static inline void adjust_pointer(T* p);\n+\n+  \/\/ Convenience wrappers for per-space data kept in _space_info.\n+  static inline MutableSpace*     space(SpaceId space_id);\n+  static inline ObjectStartArray* start_array(SpaceId space_id);\n+\n+  static ParMarkBitMap* mark_bitmap() { return &_mark_bitmap; }\n+\n+  \/\/ Reference Processing\n+  static ReferenceProcessor* ref_processor() { return _ref_processor; }\n+\n+  static STWGCTimer* gc_timer() { return &_gc_timer; }\n+\n+  \/\/ Return the SpaceId for the given address.\n+  static SpaceId space_id(HeapWord* addr);\n+\n+  static void print_on(outputStream* st);\n+};\n+\n+void steal_marking_work_new(TaskTerminator& terminator, uint worker_id);\n+\n+#endif \/\/ SHARE_GC_PARALLEL_PSPARALLELCOMPACTNEW_HPP\n","filename":"src\/hotspot\/share\/gc\/parallel\/psParallelCompactNew.hpp","additions":338,"deletions":0,"binary":false,"changes":338,"status":"added"},{"patch":"@@ -0,0 +1,74 @@\n+\/*\n+ * Copyright (c) 2016, 2024, Oracle and\/or its affiliates. All rights reserved.\n+ * Copyright Amazon.com Inc. or its affiliates. All Rights Reserved.\n+ * DO NOT ALTER OR REMOVE COPYRIGHT NOTICES OR THIS FILE HEADER.\n+ *\n+ * This code is free software; you can redistribute it and\/or modify it\n+ * under the terms of the GNU General Public License version 2 only, as\n+ * published by the Free Software Foundation.\n+ *\n+ * This code is distributed in the hope that it will be useful, but WITHOUT\n+ * ANY WARRANTY; without even the implied warranty of MERCHANTABILITY or\n+ * FITNESS FOR A PARTICULAR PURPOSE.  See the GNU General Public License\n+ * version 2 for more details (a copy is included in the LICENSE file that\n+ * accompanied this code).\n+ *\n+ * You should have received a copy of the GNU General Public License version\n+ * 2 along with this work; if not, write to the Free Software Foundation,\n+ * Inc., 51 Franklin St, Fifth Floor, Boston, MA 02110-1301 USA.\n+ *\n+ * Please contact Oracle, 500 Oracle Parkway, Redwood Shores, CA 94065 USA\n+ * or visit www.oracle.com if you need additional information or have any\n+ * questions.\n+ *\n+ *\/\n+\n+#ifndef SHARE_GC_PARALLEL_PSPARALLELCOMPACTNEW_INLINE_HPP\n+#define SHARE_GC_PARALLEL_PSPARALLELCOMPACTNEW_INLINE_HPP\n+\n+#include \"gc\/parallel\/psParallelCompactNew.hpp\"\n+\n+#include \"gc\/parallel\/parallelScavengeHeap.hpp\"\n+#include \"gc\/parallel\/parMarkBitMap.inline.hpp\"\n+#include \"gc\/shared\/collectedHeap.hpp\"\n+#include \"gc\/shared\/continuationGCSupport.inline.hpp\"\n+#include \"gc\/shared\/fullGCForwarding.inline.hpp\"\n+#include \"oops\/access.inline.hpp\"\n+#include \"oops\/compressedOops.inline.hpp\"\n+#include \"oops\/klass.hpp\"\n+#include \"oops\/oop.inline.hpp\"\n+\n+inline bool PSParallelCompactNew::is_marked(oop obj) {\n+  return mark_bitmap()->is_marked(obj);\n+}\n+\n+inline MutableSpace* PSParallelCompactNew::space(SpaceId id) {\n+  assert(id < last_space_id, \"id out of range\");\n+  return _space_info[id].space();\n+}\n+\n+inline ObjectStartArray* PSParallelCompactNew::start_array(SpaceId id) {\n+  assert(id < last_space_id, \"id out of range\");\n+  return _space_info[id].start_array();\n+}\n+\n+template <class T>\n+inline void PSParallelCompactNew::adjust_pointer(T* p) {\n+  T heap_oop = RawAccess<>::oop_load(p);\n+  if (!CompressedOops::is_null(heap_oop)) {\n+    oop obj = CompressedOops::decode_not_null(heap_oop);\n+    assert(ParallelScavengeHeap::heap()->is_in(obj), \"should be in heap\");\n+    assert(is_marked(obj), \"must be marked\");\n+    if (!FullGCForwarding::is_forwarded(obj)) {\n+      return;\n+    }\n+    oop new_obj = FullGCForwarding::forwardee(obj);\n+    assert(new_obj != nullptr, \"non-null address for live objects\");\n+    assert(new_obj != obj, \"inv\");\n+    assert(ParallelScavengeHeap::heap()->is_in_reserved(new_obj),\n+           \"should be in object space\");\n+    RawAccess<IS_NOT_NULL>::oop_store(p, new_obj);\n+  }\n+}\n+\n+#endif \/\/ SHARE_GC_PARALLEL_PSPARALLELCOMPACTNEW_INLINE_HPP\n","filename":"src\/hotspot\/share\/gc\/parallel\/psParallelCompactNew.inline.hpp","additions":74,"deletions":0,"binary":false,"changes":74,"status":"added"},{"patch":"@@ -180,1 +180,2 @@\n-  size_t new_obj_size = o->size_given_klass(klass);\n+  size_t old_obj_size = o->size_given_mark_and_klass(test_mark, klass);\n+  size_t new_obj_size = o->copy_size(old_obj_size, test_mark);\n@@ -282,0 +283,2 @@\n+    new_obj->initialize_hash_if_necessary(o);\n+\n","filename":"src\/hotspot\/share\/gc\/parallel\/psPromotionManager.inline.hpp","additions":4,"deletions":1,"binary":false,"changes":5,"status":"modified"},{"patch":"@@ -33,0 +33,1 @@\n+#include \"gc\/parallel\/psCompactionManagerNew.hpp\"\n@@ -206,1 +207,1 @@\n-      _terminator(max_workers, ParCompactionManager::marking_stacks()) {}\n+      _terminator(max_workers, UseCompactObjectHeaders ? ParCompactionManagerNew::marking_stacks() : ParCompactionManager::marking_stacks()) {}\n","filename":"src\/hotspot\/share\/gc\/parallel\/psScavenge.cpp","additions":2,"deletions":1,"binary":false,"changes":3,"status":"modified"},{"patch":"@@ -691,7 +691,1 @@\n-      if (obj->is_self_forwarded()) {\n-        obj->unset_self_forwarded();\n-      } else if (obj->is_forwarded()) {\n-        \/\/ To restore the klass-bits in the header.\n-        \/\/ Needed for object iteration to work properly.\n-        obj->set_mark(obj->forwardee()->prototype_mark());\n-      }\n+      obj->reset_forwarded();\n@@ -728,1 +722,3 @@\n-  size_t s = old->size();\n+  size_t old_size = old->size();\n+  size_t s = old->copy_size(old_size, old->mark());\n+\n@@ -753,1 +749,1 @@\n-  Copy::aligned_disjoint_words(cast_from_oop<HeapWord*>(old), cast_from_oop<HeapWord*>(obj), s);\n+  Copy::aligned_disjoint_words(cast_from_oop<HeapWord*>(old), cast_from_oop<HeapWord*>(obj), old_size);\n@@ -763,0 +759,2 @@\n+  obj->initialize_hash_if_necessary(old);\n+\n","filename":"src\/hotspot\/share\/gc\/serial\/defNewGeneration.cpp","additions":7,"deletions":9,"binary":false,"changes":16,"status":"modified"},{"patch":"@@ -27,1 +27,0 @@\n-#include \"gc\/shared\/fullGCForwarding.hpp\"\n@@ -30,5 +29,0 @@\n-void SerialArguments::initialize() {\n-  GCArguments::initialize();\n-  FullGCForwarding::initialize_flags(MaxHeapSize);\n-}\n-\n","filename":"src\/hotspot\/share\/gc\/serial\/serialArguments.cpp","additions":0,"deletions":6,"binary":false,"changes":6,"status":"modified"},{"patch":"@@ -34,1 +34,0 @@\n-  virtual void initialize();\n","filename":"src\/hotspot\/share\/gc\/serial\/serialArguments.hpp","additions":0,"deletions":1,"binary":false,"changes":1,"status":"modified"},{"patch":"@@ -193,1 +193,2 @@\n-  HeapWord* alloc(size_t words) {\n+  HeapWord* alloc(size_t old_size, size_t new_size, HeapWord* old_obj) {\n+    size_t words = (old_obj == _spaces[_index]._compaction_top) ? old_size : new_size;\n@@ -209,0 +210,1 @@\n+      words = (old_obj == _spaces[_index]._compaction_top) ? old_size : new_size;\n@@ -230,1 +232,1 @@\n-  static void forward_obj(oop obj, HeapWord* new_addr) {\n+  static void forward_obj(oop obj, HeapWord* new_addr, bool after_first_dead) {\n@@ -236,2 +238,7 @@\n-      \/\/ This obj will stay in-place. Fix the markword.\n-      obj->init_mark();\n+      if (!after_first_dead) {\n+        \/\/ This obj will stay in-place and we'll not see it during relocation.\n+        \/\/ Fix the markword.\n+        obj->init_mark();\n+      } else {\n+        FullGCForwarding::forward_to(obj, cast_to_oop(new_addr));\n+      }\n@@ -260,2 +267,0 @@\n-    assert(addr != new_addr, \"inv\");\n-    prefetch_write_copy(new_addr);\n@@ -264,1 +269,4 @@\n-    Copy::aligned_conjoint_words(addr, new_addr, obj_size);\n+    if (addr != new_addr) {\n+      prefetch_write_copy(new_addr);\n+      Copy::aligned_conjoint_words(addr, new_addr, obj_size);\n+    }\n@@ -266,0 +274,3 @@\n+    if (addr != new_addr) {\n+      new_obj->initialize_hash_if_necessary(obj);\n+    }\n@@ -301,0 +312,1 @@\n+        size_t new_size = obj->copy_size(obj_size, obj->mark());\n@@ -302,2 +314,3 @@\n-          HeapWord* new_addr = alloc(obj_size);\n-          forward_obj(obj, new_addr);\n+          HeapWord* new_addr = alloc(obj_size, new_size, cur_addr);\n+          forward_obj(obj, new_addr, record_first_dead_done);\n+          assert(obj->size() == obj_size, \"size must not change after forwarding\");\n@@ -310,1 +323,2 @@\n-            alloc(pointer_delta(next_live_addr, cur_addr));\n+            size_t size = pointer_delta(next_live_addr, cur_addr);\n+            alloc(size, size, cur_addr);\n@@ -596,1 +610,1 @@\n-  obj->set_mark(obj->prototype_mark().set_marked());\n+  obj->set_mark(mark.set_marked());\n@@ -699,0 +713,2 @@\n+  FullGCForwarding::begin();\n+\n@@ -742,0 +758,2 @@\n+  FullGCForwarding::end();\n+\n","filename":"src\/hotspot\/share\/gc\/serial\/serialFullGC.cpp","additions":29,"deletions":11,"binary":false,"changes":40,"status":"modified"},{"patch":"@@ -42,1 +42,1 @@\n-#include \"gc\/shared\/fullGCForwarding.hpp\"\n+#include \"gc\/shared\/fullGCForwarding.inline.hpp\"\n","filename":"src\/hotspot\/share\/gc\/serial\/serialHeap.cpp","additions":1,"deletions":1,"binary":false,"changes":2,"status":"modified"},{"patch":"@@ -390,1 +390,1 @@\n-  assert(obj_size == obj->size(), \"bad obj_size passed in\");\n+  assert(obj_size == obj->size() || UseCompactObjectHeaders, \"bad obj_size passed in\");\n","filename":"src\/hotspot\/share\/gc\/serial\/tenuredGeneration.cpp","additions":1,"deletions":1,"binary":false,"changes":2,"status":"modified"},{"patch":"@@ -707,1 +707,2 @@\n-  \/\/ 8  - 32-bit VM or 64-bit VM, compact headers\n+  \/\/ 4  - compact headers\n+  \/\/ 8  - 32-bit VM\n@@ -712,1 +713,0 @@\n-    assert(!UseCompactObjectHeaders, \"\");\n@@ -717,2 +717,6 @@\n-      \/\/ Include klass to copy by 8 bytes words.\n-      base_off = instanceOopDesc::klass_offset_in_bytes();\n+      if (UseCompactObjectHeaders) {\n+        base_off = 0; \/* FIXME *\/\n+      } else {\n+        \/\/ Include klass to copy by 8 bytes words.\n+        base_off = instanceOopDesc::klass_offset_in_bytes();\n+      }\n@@ -720,1 +724,1 @@\n-    assert(base_off % BytesPerLong == 0, \"expect 8 bytes alignment\");\n+    assert(base_off % BytesPerLong == 0 || UseCompactObjectHeaders, \"expect 8 bytes alignment\");\n","filename":"src\/hotspot\/share\/gc\/shared\/c2\/barrierSetC2.cpp","additions":9,"deletions":5,"binary":false,"changes":14,"status":"modified"},{"patch":"@@ -292,1 +292,1 @@\n-  oop class_allocate(Klass* klass, size_t size, TRAPS);\n+  oop class_allocate(Klass* klass, size_t size, size_t base_size, TRAPS);\n","filename":"src\/hotspot\/share\/gc\/shared\/collectedHeap.hpp","additions":1,"deletions":1,"binary":false,"changes":2,"status":"modified"},{"patch":"@@ -44,2 +44,2 @@\n-inline oop CollectedHeap::class_allocate(Klass* klass, size_t size, TRAPS) {\n-  ClassAllocator allocator(klass, size, THREAD);\n+inline oop CollectedHeap::class_allocate(Klass* klass, size_t size, size_t base_size, TRAPS) {\n+  ClassAllocator allocator(klass, size, base_size, THREAD);\n","filename":"src\/hotspot\/share\/gc\/shared\/collectedHeap.inline.hpp","additions":2,"deletions":2,"binary":false,"changes":4,"status":"modified"},{"patch":"@@ -1,56 +1,0 @@\n-\/*\n- * Copyright Amazon.com Inc. or its affiliates. All Rights Reserved.\n- * DO NOT ALTER OR REMOVE COPYRIGHT NOTICES OR THIS FILE HEADER.\n- *\n- * This code is free software; you can redistribute it and\/or modify it\n- * under the terms of the GNU General Public License version 2 only, as\n- * published by the Free Software Foundation.\n- *\n- * This code is distributed in the hope that it will be useful, but WITHOUT\n- * ANY WARRANTY; without even the implied warranty of MERCHANTABILITY or\n- * FITNESS FOR A PARTICULAR PURPOSE.  See the GNU General Public License\n- * version 2 for more details (a copy is included in the LICENSE file that\n- * accompanied this code).\n- *\n- * You should have received a copy of the GNU General Public License version\n- * 2 along with this work; if not, write to the Free Software Foundation,\n- * Inc., 51 Franklin St, Fifth Floor, Boston, MA 02110-1301 USA.\n- *\n- * Please contact Oracle, 500 Oracle Parkway, Redwood Shores, CA 94065 USA\n- * or visit www.oracle.com if you need additional information or have any\n- * questions.\n- *\n- *\/\n-\n-#include \"gc\/shared\/fullGCForwarding.hpp\"\n-#include \"memory\/memRegion.hpp\"\n-#include \"runtime\/globals_extension.hpp\"\n-\n-HeapWord* FullGCForwarding::_heap_base = nullptr;\n-int FullGCForwarding::_num_low_bits = 0;\n-\n-void FullGCForwarding::initialize_flags(size_t max_heap_size) {\n-#ifdef _LP64\n-  size_t max_narrow_heap_size = right_n_bits(NumLowBitsNarrow - Shift);\n-  if (UseCompactObjectHeaders && max_heap_size > max_narrow_heap_size * HeapWordSize) {\n-    warning(\"Compact object headers require a java heap size smaller than %zu\"\n-            \"%s (given: %zu%s). Disabling compact object headers.\",\n-            byte_size_in_proper_unit(max_narrow_heap_size * HeapWordSize),\n-            proper_unit_for_byte_size(max_narrow_heap_size * HeapWordSize),\n-            byte_size_in_proper_unit(max_heap_size),\n-            proper_unit_for_byte_size(max_heap_size));\n-    FLAG_SET_ERGO(UseCompactObjectHeaders, false);\n-  }\n-#endif\n-}\n-\n-void FullGCForwarding::initialize(MemRegion heap) {\n-#ifdef _LP64\n-  _heap_base = heap.start();\n-  if (UseCompactObjectHeaders) {\n-    _num_low_bits = NumLowBitsNarrow;\n-  } else {\n-    _num_low_bits = NumLowBitsWide;\n-  }\n-#endif\n-}\n","filename":"src\/hotspot\/share\/gc\/shared\/fullGCForwarding.cpp","additions":0,"deletions":56,"binary":false,"changes":56,"status":"deleted"},{"patch":"@@ -28,1 +28,1 @@\n-#include \"memory\/allStatic.hpp\"\n+#include \"memory\/allocation.hpp\"\n@@ -33,10 +33,84 @@\n-\/*\n- * Implements forwarding for the Full GCs of Serial, Parallel, G1 and Shenandoah in\n- * a way that preserves upper N bits of object mark-words, which contain crucial\n- * Klass* information when running with compact headers. The encoding is similar to\n- * compressed-oops encoding: it basically subtracts the forwardee address from the\n- * heap-base, shifts that difference into the right place, and sets the lowest two\n- * bits (to indicate 'forwarded' state as usual).\n- * With compact-headers, we have 40 bits to encode forwarding pointers. This is\n- * enough to address 8TB of heap. If the heap size exceeds that limit, we turn off\n- * compact headers.\n+class FallbackTable;\n+\n+\/**\n+ * FullGCForwarding is a method to store forwarding information in a compressed form into the object header,\n+ * that has been specifically designed for sliding compacting GCs and compact object headers. With compact object\n+ * headers, we store the compressed class pointer in the header, which would be overwritten by full forwarding\n+ * pointers, if we allow the legacy forwarding code to act. This would lose the class information for the object,\n+ * which is required later in GC cycle to iterate the reference fields and get the object size for copying.\n+ *\n+ * FullGCForwarding requires only small side tables and guarantees constant-time access and modification.\n+ *\n+ * The key advantage of sliding compaction for encoding efficiency:\n+ * - It forwards objects linearily, starting at the heap bottom and moving up to the top, sliding\n+ *   live objects towards the bottom of the heap. (The reality in parallel or regionalized GCs is a bit more\n+ *   complex, but conceptually it is the same.)\n+ * - Objects starting in any one block can only be forwarded to a memory region that is not larger than\n+ *   a block. (There are exceptions to this rule which are discussed below.)\n+ *\n+ * This is an intuitive property: when we slide the compact block full of data, it can not take up more\n+ * memory afterwards.\n+ * This property allows us to use a side table to record the addresses of the target memory region for\n+ * each block. The table holds N entries for N blocks. For each block, it gives the base\n+ * address of the target regions, or a special placeholder if not used.\n+ *\n+ * This encoding efficiency allows to store the forwarding information in the object header _together_ with the\n+ * compressed class pointer.\n+ *\n+ * The idea is to use a pointer compression scheme very similar to the one that is used for compressed oops.\n+ * We divide the heap into number of equal-sized blocks. Each block spans a maximum of 2^NUM_OFFSET_BITS words.\n+ * We maintain a side-table of target-base-addresses, with one address entry per block.\n+ *\n+ * When recording the sliding forwarding, the mark word would look roughly like this:\n+ *\n+ *   32                               0\n+ *    [.....................OOOOOOOOOTT]\n+ *                                    ^------ tag-bits, indicates 'forwarded'\n+ *                                  ^-------- in-region offset\n+ *                         ^----------------- protected area, *not touched* by this code, useful for\n+ *                                            compressed class pointer with compact object headers\n+ *\n+ * Adding a forwarding then generally works as follows:\n+ *   1. Compute the index of the block of the \"from\" address.\n+ *   2. Load the target-base-offset of the from-block from the side-table.\n+ *   3. If the base-offset is not-yet set, set it to the to-address of the forwarding.\n+ *      (In other words, the first forwarding of a block determines the target base-offset.)\n+ *   4. Compute the offset of the to-address in the target region.\n+ *   4. Store offset in the object header.\n+ *\n+ * Similarly, looking up the target address, given an original object address generally works as follows:\n+ *   1. Compute the index of the block of the \"from\" address.\n+ *   2. Load the target-base-offset of the from-block from the side-table.\n+ *   3. Extract the offset from the object header.\n+ *   4. Compute the \"to\" address from \"to\" region base and \"offset\"\n+ *\n+ * We reserve one special value for the offset:\n+ *  - 111111111: Indicates an exceptional forwarding (see below), for which a fallback hash-table\n+ *               is used to look up the target address.\n+ *\n+ * In order to support this, we need to make a change to the above algorithm:\n+ *  - Forwardings that would use offsets >= 111111111 (i.e. the last slot)\n+ *    would also need to use the fallback-table. We expect that to be relatively rare for two reasons:\n+ *    1. It only affects 1 out of 512 possible offsets, in other words, 1\/512th of all situations in an equal\n+ *       distribution.\n+ *    2. Forwardings are not equally-distributed, because normally we 'skip' unreachable objects,\n+ *       thus compacting the block. Forwardings tend to cluster at the beginning of the target region,\n+ *       and become less likely towards the end of the possible encodable target address range.\n+ *       Which means in reality it will be much less frequent than 1\/512.\n+ *\n+ * There are several conditions when the above algorithm would be broken because the assumption that\n+ * 'objects from each block can only get forwarded to a region of block-size' is violated:\n+ * - G1 last-ditch serial compaction: there, object from a single region can be forwarded to multiple,\n+ *   more than two regions. G1 serial compaction is not very common - it is the last-last-ditch GC\n+ *   that is used when the JVM is scrambling to squeeze more space out of the heap, and at that point,\n+ *   ultimate performance is no longer the main concern.\n+ * - When forwarding hits a space (or G1\/Shenandoah region) boundary, then latter objects of a block\n+ *   need to be forwarded to a different address range than earlier objects in the same block.\n+ *   This is rare.\n+ * - With compact identity hash-code, objects can grow, and in the worst case use up more memory in\n+ *   the target block than we can address. We expect that to be rare.\n+ *\n+ * To deal with that, we initialize a fallback-hashtable for storing those extra forwardings, and use a special\n+ * offset pattern (0b11...1) to indicate that the forwardee is not encoded but should be looked-up in the hashtable.\n+ * This implies that this particular offset (the last word of a block) can not be used directly as forwarding,\n+ * but also has to be handled by the fallback-table.\n@@ -44,4 +118,16 @@\n-class FullGCForwarding : public AllStatic {\n-  static const int NumLowBitsNarrow = LP64_ONLY(markWord::klass_shift) NOT_LP64(0 \/*unused*\/);\n-  static const int NumLowBitsWide   = BitsPerWord;\n-  static const int Shift            = markWord::lock_bits + markWord::lock_shift;\n+template <int BITS>\n+class FullGCForwardingImpl : public AllStatic {\n+  friend class FullGCForwardingTest;\n+  static constexpr int AVAILABLE_LOW_BITS        = BITS;\n+  static constexpr uintptr_t AVAILABLE_BITS_MASK = right_n_bits(AVAILABLE_LOW_BITS);\n+  \/\/ The offset bits start after the lock-bits, which are currently used by Serial GC\n+  \/\/ for marking objects. Could be 1 for Serial GC when being clever with the bits,\n+  \/\/ and 0 for all other GCs.\n+  static constexpr int OFFSET_BITS_SHIFT = markWord::lock_shift + markWord::lock_bits;\n+\n+  \/\/ How many bits we use for the offset\n+  static constexpr int NUM_OFFSET_BITS = AVAILABLE_LOW_BITS - OFFSET_BITS_SHIFT;\n+  static constexpr size_t BLOCK_SIZE_WORDS = 1ll << NUM_OFFSET_BITS;\n+  static constexpr int BLOCK_SIZE_BYTES_SHIFT = NUM_OFFSET_BITS + LogHeapWordSize;\n+  static constexpr size_t MAX_OFFSET = BLOCK_SIZE_WORDS - 2;\n+  static constexpr uintptr_t OFFSET_MASK = right_n_bits(NUM_OFFSET_BITS) << OFFSET_BITS_SHIFT;\n@@ -49,2 +135,48 @@\n-  static HeapWord* _heap_base;\n-  static int _num_low_bits;\n+  \/\/ This offset bit-pattern indicates that the actual mapping is handled by the\n+  \/\/ fallback-table. This also implies that this cannot be used as a valid offset,\n+  \/\/ and we must also use the fallback-table for mappings to the last word of a\n+  \/\/ block.\n+  static constexpr uintptr_t FALLBACK_PATTERN = right_n_bits(NUM_OFFSET_BITS);\n+  static constexpr uintptr_t FALLBACK_PATTERN_IN_PLACE = FALLBACK_PATTERN << OFFSET_BITS_SHIFT;\n+\n+  \/\/ Indicates an unused base address in the target base table.\n+  static HeapWord* const UNUSED_BASE;\n+\n+  static HeapWord*      _heap_start;\n+\n+  static size_t         _heap_start_region_bias;\n+  static size_t         _num_regions;\n+  static uintptr_t      _region_mask;\n+\n+  \/\/ The target base table memory.\n+  static HeapWord**     _bases_table;\n+  \/\/ Entries into the target base tables, biased to the start of the heap.\n+  static HeapWord**     _biased_bases;\n+\n+  static size_t _fallback_table_log2_start_size;\n+  static FallbackTable* _fallback_table;\n+\n+#ifndef PRODUCT\n+  static volatile uint64_t _num_forwardings;\n+  static volatile uint64_t _num_fallback_forwardings;\n+#endif\n+\n+  static size_t biased_region_index_containing(HeapWord* addr);\n+\n+  static bool is_fallback(uintptr_t encoded);\n+  static uintptr_t encode_forwarding(HeapWord* from, HeapWord* to);\n+  static HeapWord* decode_forwarding(HeapWord* from, uintptr_t encoded);\n+\n+  static void maybe_init_fallback_table();\n+  static void fallback_forward_to(HeapWord* from, HeapWord* to);\n+  static HeapWord* fallback_forwardee(HeapWord* from);\n+\n+  static void forward_to_impl(oop from, oop to);\n+  static oop forwardee_impl(oop from);\n+\n+  FullGCForwardingImpl() = delete;\n+\n+  \/\/ Used in unit-test, so that we can test fallback-table-growth.\n+  static void set_fallback_table_log2_start_size(size_t fallback_table_log2_start_size) {\n+    _fallback_table_log2_start_size = fallback_table_log2_start_size;\n+  }\n@@ -52,4 +184,8 @@\n-  static void initialize_flags(size_t max_heap_size);\n-  static inline void forward_to(oop from, oop to);\n-  static inline oop forwardee(oop from);\n-  static inline bool is_forwarded(oop obj);\n+\n+  static void begin();\n+  static void end();\n+\n+  static bool is_forwarded(oop obj);\n+\n+  static void forward_to(oop from, oop to);\n+  static oop forwardee(oop from);\n@@ -59,0 +195,8 @@\n+#ifdef _LP64\n+using FullGCForwarding = FullGCForwardingImpl<markWord::hashctrl_shift>;\n+#else\n+\/\/ On 32 bit, the BITS template argument is not used, but we still need\n+\/\/ to pass a value.\n+using FullGCForwarding = FullGCForwardingImpl<0>;\n+#endif\n+\n","filename":"src\/hotspot\/share\/gc\/shared\/fullGCForwarding.hpp","additions":165,"deletions":21,"binary":false,"changes":186,"status":"modified"},{"patch":"@@ -22,1 +22,0 @@\n- *\n@@ -25,2 +24,2 @@\n-#ifndef GC_SHARED_FULLGCFORWARDING_INLINE_HPP\n-#define GC_SHARED_FULLGCFORWARDING_INLINE_HPP\n+#ifndef SHARE_GC_SHARED_FULLGCFORWARDING_INLINE_HPP\n+#define SHARE_GC_SHARED_FULLGCFORWARDING_INLINE_HPP\n@@ -29,1 +28,3 @@\n-\n+#include \"logging\/log.hpp\"\n+#include \"nmt\/memTag.hpp\"\n+#include \"oops\/markWord.hpp\"\n@@ -31,1 +32,76 @@\n-#include \"utilities\/globalDefinitions.hpp\"\n+#include \"utilities\/concurrentHashTable.inline.hpp\"\n+#include \"utilities\/fastHash.hpp\"\n+#include \"utilities\/macros.hpp\"\n+#include \"utilities\/powerOfTwo.hpp\"\n+\n+\/\/ We cannot use 0, because that may already be a valid base address in zero-based heaps.\n+\/\/ 0x1 is safe because heap base addresses must be aligned by much larger alignment\n+template <int BITS>\n+HeapWord* const FullGCForwardingImpl<BITS>::UNUSED_BASE = reinterpret_cast<HeapWord*>(0x1);\n+\n+template <int BITS>\n+HeapWord* FullGCForwardingImpl<BITS>::_heap_start = nullptr;\n+template <int BITS>\n+size_t FullGCForwardingImpl<BITS>::_heap_start_region_bias = 0;\n+template <int BITS>\n+size_t FullGCForwardingImpl<BITS>::_num_regions = 0;\n+template <int BITS>\n+uintptr_t FullGCForwardingImpl<BITS>::_region_mask = 0;\n+template <int BITS>\n+HeapWord** FullGCForwardingImpl<BITS>::_biased_bases = nullptr;\n+template <int BITS>\n+HeapWord** FullGCForwardingImpl<BITS>::_bases_table = nullptr;\n+template <int BITS>\n+size_t FullGCForwardingImpl<BITS>::_fallback_table_log2_start_size = 9; \/\/ 512 entries.\n+template <int BITS>\n+FallbackTable* FullGCForwardingImpl<BITS>::_fallback_table = nullptr;\n+#ifndef PRODUCT\n+template <int BITS>\n+volatile uint64_t FullGCForwardingImpl<BITS>::_num_forwardings = 0;\n+template <int BITS>\n+volatile uint64_t FullGCForwardingImpl<BITS>::_num_fallback_forwardings = 0;\n+#endif\n+\n+template <int BITS>\n+bool FullGCForwardingImpl<BITS>::is_forwarded(oop obj) {\n+  return obj->is_forwarded();\n+}\n+\n+template <int BITS>\n+size_t FullGCForwardingImpl<BITS>::biased_region_index_containing(HeapWord* addr) {\n+  return reinterpret_cast<uintptr_t>(addr) >> BLOCK_SIZE_BYTES_SHIFT;\n+}\n+\n+template <int BITS>\n+bool FullGCForwardingImpl<BITS>::is_fallback(uintptr_t encoded) {\n+  return (encoded & OFFSET_MASK) == FALLBACK_PATTERN_IN_PLACE;\n+}\n+\n+template <int BITS>\n+uintptr_t FullGCForwardingImpl<BITS>::encode_forwarding(HeapWord* from, HeapWord* to) {\n+  size_t from_block_idx = biased_region_index_containing(from);\n+\n+  HeapWord* to_region_base = _biased_bases[from_block_idx];\n+  if (to_region_base == UNUSED_BASE) {\n+    HeapWord* prev = Atomic::cmpxchg(&_biased_bases[from_block_idx], UNUSED_BASE, to);\n+    if (prev == UNUSED_BASE) {\n+      to_region_base = to;\n+    } else {\n+      to_region_base = prev;\n+    }\n+    \/\/ _biased_bases[from_block_idx] = to_region_base = to;\n+  }\n+  \/\/ Avoid pointer_delta() on purpose: using an unsigned subtraction,\n+  \/\/ we get an underflow when to < to_region_base, which means\n+  \/\/ we can use a single comparison instead of:\n+  \/\/ if (to_region_base > to || (to - to_region_base) > MAX_OFFSET) { .. }\n+  size_t offset = static_cast<size_t>(to - to_region_base);\n+  if (offset > MAX_OFFSET) {\n+    offset = FALLBACK_PATTERN;\n+  }\n+  uintptr_t encoded = (offset << OFFSET_BITS_SHIFT) | markWord::marked_value;\n+\n+  assert(is_fallback(encoded) || to == decode_forwarding(from, encoded), \"must be reversible: \" PTR_FORMAT \" -> \" PTR_FORMAT \", reversed: \" PTR_FORMAT \", encoded: \" INTPTR_FORMAT \", to_region_base: \" PTR_FORMAT \", from_block_idx: %lu\", p2i(from), p2i(to), p2i(decode_forwarding(from, encoded)), encoded, p2i(to_region_base), from_block_idx);\n+  assert((encoded & ~AVAILABLE_BITS_MASK) == 0, \"must encode to available bits\");\n+  return encoded;\n+}\n@@ -33,1 +109,37 @@\n-void FullGCForwarding::forward_to(oop from, oop to) {\n+template <int BITS>\n+HeapWord* FullGCForwardingImpl<BITS>::decode_forwarding(HeapWord* from, uintptr_t encoded) {\n+  assert(!is_fallback(encoded), \"must not be fallback-forwarded, encoded: \" INTPTR_FORMAT \", OFFSET_MASK: \" INTPTR_FORMAT \", FALLBACK_PATTERN_IN_PLACE: \" INTPTR_FORMAT, encoded, OFFSET_MASK, FALLBACK_PATTERN_IN_PLACE);\n+  assert((encoded & ~AVAILABLE_BITS_MASK) == 0, \"must decode from available bits, encoded: \" INTPTR_FORMAT, encoded);\n+  uintptr_t offset = (encoded >> OFFSET_BITS_SHIFT);\n+\n+  size_t from_idx = biased_region_index_containing(from);\n+  HeapWord* base = _biased_bases[from_idx];\n+  assert(base != UNUSED_BASE, \"must not be unused base: encoded: \" INTPTR_FORMAT, encoded);\n+  HeapWord* decoded = base + offset;\n+  assert(decoded >= _heap_start,\n+         \"Address must be above heap start. encoded: \" INTPTR_FORMAT \", base: \" PTR_FORMAT,\n+          encoded, p2i(base));\n+\n+  return decoded;\n+}\n+\n+template <int BITS>\n+void FullGCForwardingImpl<BITS>::forward_to_impl(oop from, oop to) {\n+  assert(_bases_table != nullptr, \"call begin() before forwarding\");\n+\n+  markWord from_header = from->mark();\n+  HeapWord* from_hw = cast_from_oop<HeapWord*>(from);\n+  HeapWord* to_hw   = cast_from_oop<HeapWord*>(to);\n+  uintptr_t encoded = encode_forwarding(from_hw, to_hw);\n+  markWord new_header = markWord((from_header.value() & ~OFFSET_MASK) | encoded);\n+  from->set_mark(new_header);\n+\n+  if (is_fallback(encoded)) {\n+    fallback_forward_to(from_hw, to_hw);\n+  }\n+  NOT_PRODUCT(Atomic::inc(&_num_forwardings);)\n+}\n+\n+template <int BITS>\n+void FullGCForwardingImpl<BITS>::forward_to(oop obj, oop fwd) {\n+  assert(fwd != nullptr, \"no null forwarding\");\n@@ -35,6 +147,3 @@\n-  uintptr_t encoded = pointer_delta(cast_from_oop<HeapWord*>(to), _heap_base) << Shift;\n-  assert(encoded <= static_cast<uintptr_t>(right_n_bits(_num_low_bits)), \"encoded forwardee must fit\");\n-  uintptr_t mark = from->mark().value();\n-  mark &= ~right_n_bits(_num_low_bits);\n-  mark |= (encoded | markWord::marked_value);\n-  from->set_mark(markWord(mark));\n+  assert(_bases_table != nullptr, \"expect sliding forwarding initialized\");\n+  forward_to_impl(obj, fwd);\n+  \/\/ assert(forwardee(obj) == fwd, \"must be forwarded to correct forwardee, obj: \" PTR_FORMAT \", forwardee(obj): \" PTR_FORMAT \", fwd: \" PTR_FORMAT \", mark: \" INTPTR_FORMAT \", num-regions: %lu, base: \" PTR_FORMAT \", OFFSET_MASK: \" INTPTR_FORMAT \", encoded: \" PTR_FORMAT \", biased-base: \" PTR_FORMAT \", heap-start: \" PTR_FORMAT, p2i(obj), p2i(forwardee(obj)), p2i(fwd), obj->mark().value(), _num_regions, p2i(_bases_table[0]), OFFSET_MASK, encode_forwarding(cast_from_oop<HeapWord*>(obj), cast_from_oop<HeapWord*>(fwd)), p2i(_biased_bases[biased_region_index_containing(cast_from_oop<HeapWord*>(obj))]), p2i(_heap_start));\n@@ -42,1 +151,1 @@\n-  from->forward_to(to);\n+  obj->forward_to(fwd);\n@@ -46,1 +155,17 @@\n-oop FullGCForwarding::forwardee(oop from) {\n+template <int BITS>\n+oop FullGCForwardingImpl<BITS>::forwardee_impl(oop from) {\n+  assert(_bases_table != nullptr, \"call begin() before asking for forwarding\");\n+\n+  markWord header = from->mark();\n+  HeapWord* from_hw = cast_from_oop<HeapWord*>(from);\n+  if (is_fallback(header.value())) {\n+    HeapWord* to = fallback_forwardee(from_hw);\n+    return cast_to_oop(to);\n+  }\n+  uintptr_t encoded = header.value() & OFFSET_MASK;\n+  HeapWord* to = decode_forwarding(from_hw, encoded);\n+  return cast_to_oop(to);\n+}\n+\n+template <int BITS>\n+oop FullGCForwardingImpl<BITS>::forwardee(oop obj) {\n@@ -48,3 +173,2 @@\n-  uintptr_t mark = from->mark().value();\n-  HeapWord* decoded = _heap_base + ((mark & right_n_bits(_num_low_bits)) >> Shift);\n-  return cast_to_oop(decoded);\n+  assert(_bases_table != nullptr, \"expect sliding forwarding initialized\");\n+  return forwardee_impl(obj);\n@@ -52,1 +176,152 @@\n-  return from->forwardee();\n+  return obj->forwardee();\n+#endif\n+}\n+\n+static uintx hash(HeapWord* const& addr) {\n+  uint64_t val = reinterpret_cast<uint64_t>(addr);\n+  uint32_t hash = FastHash::get_hash32(static_cast<uint32_t>(val), static_cast<uint32_t>(val >> 32));\n+  return hash;\n+}\n+\n+struct ForwardingEntry {\n+  HeapWord* _from;\n+  HeapWord* _to;\n+  ForwardingEntry(HeapWord* from, HeapWord* to) : _from(from), _to(to) {}\n+};\n+\n+struct FallbackTableConfig {\n+  using Value = ForwardingEntry;\n+  static uintx get_hash(Value const& entry, bool* is_dead) {\n+    return hash(entry._from);\n+  }\n+  static void* allocate_node(void* context, size_t size, Value const& value) {\n+    return AllocateHeap(size, mtGC);\n+  }\n+  static void free_node(void* context, void* memory, Value const& value) {\n+    FreeHeap(memory);\n+  }\n+};\n+\n+class FallbackTable : public ConcurrentHashTable<FallbackTableConfig, mtGC> {\n+public:\n+  explicit FallbackTable(size_t log2size) : ConcurrentHashTable(log2size) {}\n+};\n+\n+class FallbackTableLookup : public StackObj {\n+  ForwardingEntry const _entry;\n+public:\n+  explicit FallbackTableLookup(HeapWord* from) : _entry(from, nullptr) {}\n+  uintx get_hash() const {\n+    return hash(_entry._from);\n+  }\n+  bool equals(const ForwardingEntry* value) const {\n+    return _entry._from == value->_from;\n+  }\n+  static bool is_dead(ForwardingEntry* value) { return false; }\n+};\n+\n+template <int BITS>\n+void FullGCForwardingImpl<BITS>::initialize(MemRegion heap) {\n+#ifdef _LP64\n+  _heap_start = heap.start();\n+\n+  size_t rounded_heap_size = MAX2(round_up_power_of_2(heap.byte_size()) \/ BytesPerWord, BLOCK_SIZE_WORDS);\n+\n+  _num_regions = rounded_heap_size \/ BLOCK_SIZE_WORDS;\n+\n+  _heap_start_region_bias = reinterpret_cast<uintptr_t>(_heap_start) >> BLOCK_SIZE_BYTES_SHIFT;\n+  _region_mask = ~((static_cast<uintptr_t>(1) << BLOCK_SIZE_BYTES_SHIFT) - 1);\n+\n+  assert(_bases_table == nullptr, \"should not be initialized yet\");\n+  assert(_fallback_table == nullptr, \"should not be initialized yet\");\n+#endif\n+}\n+\n+template <int BITS>\n+void FullGCForwardingImpl<BITS>::begin() {\n+#ifdef _LP64\n+  assert(_bases_table == nullptr, \"should not be initialized yet\");\n+  assert(_fallback_table == nullptr, \"should not be initialized yet\");\n+\n+  _fallback_table = nullptr;\n+\n+#ifndef PRODUCT\n+  _num_forwardings = 0;\n+  _num_fallback_forwardings = 0;\n+#endif\n+\n+  size_t max = _num_regions;\n+  _bases_table = NEW_C_HEAP_ARRAY(HeapWord*, max, mtGC);\n+  HeapWord** biased_start = _bases_table - _heap_start_region_bias;\n+  _biased_bases = biased_start;\n+  if (max == 1) {\n+    \/\/ Optimize the case when the block-size >= heap-size.\n+    \/\/ In this case we can use the heap-start as block-start,\n+    \/\/ and don't risk that competing GC threads set a higher\n+    \/\/ address as block-start, which would lead to unnecessary\n+    \/\/ fallback-usage.\n+    _bases_table[0] = _heap_start;\n+  } else {\n+    for (size_t i = 0; i < max; i++) {\n+      _bases_table[i] = UNUSED_BASE;\n+    }\n+  }\n+#endif\n+}\n+\n+template <int BITS>\n+void FullGCForwardingImpl<BITS>::end() {\n+#ifndef PRODUCT\n+  size_t fallback_table_size = _fallback_table != nullptr ? _fallback_table->get_mem_size(Thread::current()) : 0;\n+  log_info(gc)(\"Total forwardings: \" UINT64_FORMAT \", fallback forwardings: \" UINT64_FORMAT\n+                \", ratio: %f, memory used by fallback table: %zu%s, memory used by bases table: %zu%s\",\n+               _num_forwardings, _num_fallback_forwardings, static_cast<float>(_num_forwardings) \/ static_cast<float>(_num_fallback_forwardings),\n+               byte_size_in_proper_unit(fallback_table_size),\n+               proper_unit_for_byte_size(fallback_table_size),\n+               byte_size_in_proper_unit(sizeof(HeapWord*) * _num_regions),\n+               proper_unit_for_byte_size(sizeof(HeapWord*) * _num_regions));\n+#endif\n+#ifdef _LP64\n+  assert(_bases_table != nullptr, \"should be initialized\");\n+  FREE_C_HEAP_ARRAY(HeapWord*, _bases_table);\n+  _bases_table = nullptr;\n+  if (_fallback_table != nullptr) {\n+    delete _fallback_table;\n+    _fallback_table = nullptr;\n+  }\n+#endif\n+}\n+\n+template <int BITS>\n+void FullGCForwardingImpl<BITS>::maybe_init_fallback_table() {\n+  if (_fallback_table == nullptr) {\n+    FallbackTable* fallback_table = new FallbackTable(_fallback_table_log2_start_size);\n+    FallbackTable* prev = Atomic::cmpxchg(&_fallback_table, static_cast<FallbackTable*>(nullptr), fallback_table);\n+    if (prev != nullptr) {\n+      \/\/ Another thread won, discard our table.\n+      delete fallback_table;\n+    }\n+  }\n+}\n+\n+template <int BITS>\n+void FullGCForwardingImpl<BITS>::fallback_forward_to(HeapWord* from, HeapWord* to) {\n+  assert(to != nullptr, \"no null forwarding\");\n+  maybe_init_fallback_table();\n+  assert(_fallback_table != nullptr, \"should be initialized\");\n+  FallbackTableLookup lookup_f(from);\n+  ForwardingEntry entry(from, to);\n+  auto found_f = [&](ForwardingEntry* found) {\n+    \/\/ If dupe has been found, override it with new value.\n+    \/\/ This is also called when new entry is succussfully inserted.\n+    if (found->_to != to) {\n+      found->_to = to;\n+    }\n+  };\n+  Thread* current_thread = Thread::current();\n+  bool grow;\n+  bool added = _fallback_table->insert_get(current_thread, lookup_f, entry, found_f, &grow);\n+  NOT_PRODUCT(Atomic::inc(&_num_fallback_forwardings);)\n+#ifdef ASSERT\n+  assert(fallback_forwardee(from) != nullptr, \"must have entered forwarding\");\n+  assert(fallback_forwardee(from) == to, \"forwarding must be correct, added: %s, from: \" PTR_FORMAT \", to: \" PTR_FORMAT \", fwd: \" PTR_FORMAT, BOOL_TO_STR(added), p2i(from), p2i(to), p2i(fallback_forwardee(from)));\n@@ -54,0 +329,4 @@\n+  if (grow) {\n+    _fallback_table->grow(current_thread);\n+    log_debug(gc)(\"grow fallback table to size: %zu bytes\", _fallback_table->get_mem_size(current_thread));\n+  }\n@@ -56,2 +335,12 @@\n-bool FullGCForwarding::is_forwarded(oop obj) {\n-  return obj->mark().is_forwarded();\n+template <int BITS>\n+HeapWord* FullGCForwardingImpl<BITS>::fallback_forwardee(HeapWord* from) {\n+  assert(_fallback_table != nullptr, \"fallback table must be present\");\n+  HeapWord* result;\n+  FallbackTableLookup lookup_f(from);\n+  auto found_f = [&](const ForwardingEntry* found) {\n+    result = found->_to;\n+  };\n+  bool found = _fallback_table->get(Thread::current(), lookup_f, found_f);\n+  assert(found, \"something must have been found\");\n+  assert(result != nullptr, \"must have found forwarding\");\n+  return result;\n@@ -60,1 +349,1 @@\n-#endif \/\/ GC_SHARED_FULLGCFORWARDING_INLINE_HPP\n+#endif \/\/ SHARE_GC_SHARED_FULLGCFORWARDING_INLINE_HPP\n","filename":"src\/hotspot\/share\/gc\/shared\/fullGCForwarding.inline.hpp","additions":310,"deletions":21,"binary":false,"changes":331,"status":"modified"},{"patch":"@@ -449,1 +449,1 @@\n-  assert(_word_size > 0, \"oop_size must be positive.\");\n+  assert(_base_size > 0, \"oop_size must be positive.\");\n@@ -451,1 +451,1 @@\n-  java_lang_Class::set_oop_size(mem, _word_size);\n+  java_lang_Class::set_oop_size(mem, _base_size);\n","filename":"src\/hotspot\/share\/gc\/shared\/memAllocator.cpp","additions":2,"deletions":2,"binary":false,"changes":4,"status":"modified"},{"patch":"@@ -111,0 +111,1 @@\n+  size_t _base_size;\n@@ -112,2 +113,3 @@\n-  ClassAllocator(Klass* klass, size_t word_size, Thread* thread = Thread::current())\n-    : MemAllocator(klass, word_size, thread) {}\n+  ClassAllocator(Klass* klass, size_t word_size, size_t base_size, Thread* thread = Thread::current())\n+    : MemAllocator(klass, word_size, thread),\n+    _base_size(base_size) {}\n","filename":"src\/hotspot\/share\/gc\/shared\/memAllocator.hpp","additions":4,"deletions":2,"binary":false,"changes":6,"status":"modified"},{"patch":"@@ -60,1 +60,1 @@\n-  _o->set_mark(_m);\n+  _o->set_mark(_m.copy_hashctrl_from(_o->mark()));\n","filename":"src\/hotspot\/share\/gc\/shared\/preservedMarks.inline.hpp","additions":1,"deletions":1,"binary":false,"changes":2,"status":"modified"},{"patch":"@@ -27,1 +27,0 @@\n-#include \"gc\/shared\/fullGCForwarding.hpp\"\n@@ -196,2 +195,0 @@\n-\n-  FullGCForwarding::initialize_flags(MaxHeapSize);\n","filename":"src\/hotspot\/share\/gc\/shenandoah\/shenandoahArguments.cpp","additions":0,"deletions":3,"binary":false,"changes":3,"status":"modified"},{"patch":"@@ -32,0 +32,3 @@\n+private:\n+  static const uintptr_t FWDED_HASH_TRANSITION = 0b111;\n+\n@@ -54,0 +57,1 @@\n+  static inline bool is_forwarded(markWord m);\n","filename":"src\/hotspot\/share\/gc\/shenandoah\/shenandoahForwarding.hpp","additions":4,"deletions":0,"binary":false,"changes":4,"status":"modified"},{"patch":"@@ -31,0 +31,1 @@\n+#include \"oops\/klass.hpp\"\n@@ -39,0 +40,3 @@\n+static HeapWord* to_forwardee(markWord mark) {\n+  return reinterpret_cast<HeapWord*>(mark.value() & ~(markWord::lock_mask_in_place | markWord::self_fwd_mask_in_place));\n+}\n@@ -46,1 +50,1 @@\n-    HeapWord* fwdptr = (HeapWord*) mark.clear_lock_bits().to_pointer();\n+    HeapWord* fwdptr = to_forwardee(mark);\n@@ -61,1 +65,1 @@\n-    HeapWord* fwdptr = (HeapWord*) mark.clear_lock_bits().to_pointer();\n+    HeapWord* fwdptr = to_forwardee(mark);\n@@ -74,0 +78,4 @@\n+inline bool ShenandoahForwarding::is_forwarded(markWord m) {\n+  return (m.value() & (markWord::lock_mask_in_place | markWord::self_fwd_mask_in_place)) > markWord::monitor_value;\n+}\n+\n@@ -75,1 +83,1 @@\n-  return obj->mark().is_marked();\n+  return is_forwarded(obj->mark());\n@@ -80,2 +88,2 @@\n-  if (old_mark.is_marked()) {\n-    return cast_to_oop(old_mark.clear_lock_bits().to_pointer());\n+  if (is_forwarded(old_mark)) {\n+    return cast_to_oop(to_forwardee(old_mark));\n@@ -85,0 +93,3 @@\n+  if (UseCompactObjectHeaders && old_mark.is_hashed_not_expanded()) {\n+    new_mark = markWord(new_mark.value() | FWDED_HASH_TRANSITION);\n+  }\n@@ -89,1 +100,1 @@\n-    return cast_to_oop(prev_mark.clear_lock_bits().to_pointer());\n+    return cast_to_oop(to_forwardee(prev_mark));\n@@ -97,1 +108,1 @@\n-      oop fwd = cast_to_oop(mark.clear_lock_bits().to_pointer());\n+      oop fwd = cast_to_oop(to_forwardee(mark));\n@@ -107,1 +118,18 @@\n-  return obj->size_given_klass(klass(obj));\n+  markWord mark = obj->mark();\n+  if (is_forwarded(mark)) {\n+    oop fwd = cast_to_oop(to_forwardee(mark));\n+    markWord fwd_mark = fwd->mark();\n+    Klass* klass = UseCompactObjectHeaders ? fwd_mark.klass() : fwd->klass();\n+    size_t size = fwd->base_size_given_klass(fwd_mark, klass);\n+    if (UseCompactObjectHeaders) {\n+      if ((mark.value() & FWDED_HASH_TRANSITION) != FWDED_HASH_TRANSITION) {\n+        if (fwd_mark.is_expanded() && klass->expand_for_hash(fwd, fwd_mark)) {\n+          size = align_object_size(size + 1);\n+        }\n+      }\n+    }\n+    return size;\n+  } else {\n+    Klass* klass = UseCompactObjectHeaders ? mark.klass() : obj->klass();\n+    return obj->size_given_mark_and_klass(mark, klass);\n+  }\n","filename":"src\/hotspot\/share\/gc\/shenandoah\/shenandoahForwarding.inline.hpp","additions":36,"deletions":8,"binary":false,"changes":44,"status":"modified"},{"patch":"@@ -248,0 +248,2 @@\n+    FullGCForwarding::begin();\n+\n@@ -257,0 +259,2 @@\n+\n+    FullGCForwarding::end();\n@@ -361,1 +365,3 @@\n-    size_t obj_size = p->size();\n+    size_t old_size = p->size();\n+    size_t new_size = p->copy_size(old_size, p->mark());\n+    size_t obj_size = _compact_point == cast_from_oop<HeapWord*>(p) ? old_size : new_size;\n@@ -379,0 +385,1 @@\n+      obj_size = _compact_point == cast_from_oop<HeapWord*>(p) ? old_size : new_size;\n@@ -898,0 +905,1 @@\n+      new_obj->initialize_hash_if_necessary(p);\n","filename":"src\/hotspot\/share\/gc\/shenandoah\/shenandoahFullGC.cpp","additions":9,"deletions":1,"binary":false,"changes":10,"status":"modified"},{"patch":"@@ -276,1 +276,2 @@\n-  size_t obj_size = p->size();\n+  size_t old_size = p->size();\n+  size_t new_size = p->copy_size(old_size, p->mark());\n@@ -283,1 +284,1 @@\n-    if ((_old_to_region != nullptr) && (_old_compact_point + obj_size > _old_to_region->end())) {\n+    if ((_old_to_region != nullptr) && (_old_compact_point + new_size > _old_to_region->end())) {\n@@ -305,0 +306,1 @@\n+    size_t obj_size = _old_compact_point == cast_from_oop<HeapWord*>(p) ? old_size : new_size;\n@@ -329,0 +331,1 @@\n+      obj_size = _old_compact_point == cast_from_oop<HeapWord*>(p) ? old_size : new_size;\n@@ -352,0 +355,1 @@\n+    size_t obj_size = _young_compact_point == cast_from_oop<HeapWord*>(p) ? old_size : new_size;\n@@ -375,0 +379,1 @@\n+      obj_size = _young_compact_point == cast_from_oop<HeapWord*>(p) ? old_size : new_size;\n","filename":"src\/hotspot\/share\/gc\/shenandoah\/shenandoahGenerationalFullGC.cpp","additions":7,"deletions":2,"binary":false,"changes":9,"status":"modified"},{"patch":"@@ -250,1 +250,8 @@\n-  size_t size = ShenandoahForwarding::size(p);\n+\n+  markWord mark = p->mark();\n+  if (ShenandoahForwarding::is_forwarded(mark)) {\n+    return ShenandoahForwarding::get_forwardee(p);\n+  }\n+  size_t old_size = ShenandoahForwarding::size(p);\n+  size_t size = p->copy_size(old_size, mark);\n+\n@@ -342,1 +349,1 @@\n-  Copy::aligned_disjoint_words(cast_from_oop<HeapWord*>(p), copy, size);\n+  Copy::aligned_disjoint_words(cast_from_oop<HeapWord*>(p), copy, old_size);\n@@ -360,0 +367,1 @@\n+    copy_val->initialize_hash_if_necessary(p);\n","filename":"src\/hotspot\/share\/gc\/shenandoah\/shenandoahGenerationalHeap.cpp","additions":10,"deletions":2,"binary":false,"changes":12,"status":"modified"},{"patch":"@@ -31,1 +31,1 @@\n-#include \"gc\/shared\/fullGCForwarding.hpp\"\n+#include \"gc\/shared\/fullGCForwarding.inline.hpp\"\n@@ -1363,1 +1363,7 @@\n-  size_t size = ShenandoahForwarding::size(p);\n+\n+  markWord mark = p->mark();\n+  if (ShenandoahForwarding::is_forwarded(mark)) {\n+    return ShenandoahForwarding::get_forwardee(p);\n+  }\n+  size_t old_size = ShenandoahForwarding::size(p);\n+  size_t size = p->copy_size(old_size, mark);\n@@ -1393,1 +1399,1 @@\n-  Copy::aligned_disjoint_words(cast_from_oop<HeapWord*>(p), copy, size);\n+  Copy::aligned_disjoint_words(cast_from_oop<HeapWord*>(p), copy, old_size);\n@@ -1400,0 +1406,1 @@\n+    copy_val->initialize_hash_if_necessary(p);\n","filename":"src\/hotspot\/share\/gc\/shenandoah\/shenandoahHeap.cpp","additions":10,"deletions":3,"binary":false,"changes":13,"status":"modified"},{"patch":"@@ -327,1 +327,2 @@\n-  const size_t size = ZUtils::object_size(from_addr);\n+  const size_t old_size = ZUtils::object_size(from_addr);\n+  const size_t size = ZUtils::copy_size(from_addr, old_size);\n@@ -337,0 +338,1 @@\n+  assert(to_addr != from_addr, \"addresses must be different\");\n@@ -339,1 +341,2 @@\n-  ZUtils::object_copy_disjoint(from_addr, to_addr, size);\n+  ZUtils::object_copy_disjoint(from_addr, to_addr, old_size);\n+  ZUtils::initialize_hash_if_necessary(to_addr, from_addr);\n@@ -593,1 +596,1 @@\n-  zaddress try_relocate_object_inner(zaddress from_addr) {\n+  zaddress try_relocate_object_inner(zaddress from_addr, size_t old_size) {\n@@ -595,2 +598,4 @@\n-\n-    const size_t size = ZUtils::object_size(from_addr);\n+    zoffset_end from_offset = to_zoffset_end(ZAddress::offset(from_addr));\n+    zoffset_end top = to_page != nullptr ? to_page->top() : to_zoffset_end(0);\n+    const size_t new_size = ZUtils::copy_size(from_addr, old_size);\n+    const size_t size = top == from_offset ? old_size : new_size;\n@@ -615,0 +620,4 @@\n+    if (old_size != new_size && ((top == from_offset) != (allocated_addr == from_addr))) {\n+      _allocator->undo_alloc_object(to_page, allocated_addr, size);\n+      return zaddress::null;\n+    }\n@@ -618,2 +627,2 @@\n-    if (_forwarding->in_place_relocation() && allocated_addr + size > from_addr) {\n-      ZUtils::object_copy_conjoint(from_addr, allocated_addr, size);\n+    if (_forwarding->in_place_relocation() && allocated_addr + old_size > from_addr) {\n+      ZUtils::object_copy_conjoint(from_addr, allocated_addr, old_size);\n@@ -621,1 +630,4 @@\n-      ZUtils::object_copy_disjoint(from_addr, allocated_addr, size);\n+      ZUtils::object_copy_disjoint(from_addr, allocated_addr, old_size);\n+    }\n+    if (from_addr != allocated_addr) {\n+      ZUtils::initialize_hash_if_necessary(allocated_addr, from_addr);\n@@ -635,1 +647,1 @@\n-  void update_remset_old_to_old(zaddress from_addr, zaddress to_addr) const {\n+  void update_remset_old_to_old(zaddress from_addr, zaddress to_addr, size_t size) const {\n@@ -657,4 +669,2 @@\n-\n-    \/\/ Read the size from the to-object, since the from-object\n-    \/\/ could have been overwritten during in-place relocation.\n-    const size_t size = ZUtils::object_size(to_addr);\n+    assert(size <= ZUtils::object_size(to_addr), \"old size must be <= new size\");\n+    assert(size > 0, \"size must be set\");\n@@ -785,1 +795,1 @@\n-  void update_remset_for_fields(zaddress from_addr, zaddress to_addr) const {\n+  void update_remset_for_fields(zaddress from_addr, zaddress to_addr, size_t size) const {\n@@ -793,1 +803,1 @@\n-      update_remset_old_to_old(from_addr, to_addr);\n+      update_remset_old_to_old(from_addr, to_addr, size);\n@@ -809,1 +819,2 @@\n-    const zaddress to_addr = try_relocate_object_inner(from_addr);\n+    size_t size = ZUtils::object_size(from_addr);\n+    const zaddress to_addr = try_relocate_object_inner(from_addr, size);\n@@ -815,1 +826,1 @@\n-    update_remset_for_fields(from_addr, to_addr);\n+    update_remset_for_fields(from_addr, to_addr, size);\n","filename":"src\/hotspot\/share\/gc\/z\/zRelocate.cpp","additions":28,"deletions":17,"binary":false,"changes":45,"status":"modified"},{"patch":"@@ -45,0 +45,3 @@\n+  static size_t copy_size(zaddress addr, size_t size);\n+  static void initialize_hash_if_necessary(zaddress to_addr, zaddress from_addr);\n+\n","filename":"src\/hotspot\/share\/gc\/z\/zUtils.hpp","additions":3,"deletions":0,"binary":false,"changes":3,"status":"modified"},{"patch":"@@ -62,0 +62,9 @@\n+inline size_t ZUtils::copy_size(zaddress addr, size_t old_size) {\n+  oop obj = to_oop(addr);\n+  return words_to_bytes(obj->copy_size(bytes_to_words(old_size), obj->mark()));\n+}\n+\n+inline void ZUtils::initialize_hash_if_necessary(zaddress to_addr, zaddress from_addr) {\n+  to_oop(to_addr)->initialize_hash_if_necessary(to_oop(from_addr));\n+}\n+\n","filename":"src\/hotspot\/share\/gc\/z\/zUtils.inline.hpp","additions":9,"deletions":0,"binary":false,"changes":9,"status":"modified"},{"patch":"@@ -455,1 +455,1 @@\n-  if (offset == oopDesc::klass_offset_in_bytes()) {\n+  if (offset == 1 \/*oopDesc::klass_offset_in_bytes()*\/) {\n","filename":"src\/hotspot\/share\/jvmci\/jvmciCompilerToVM.cpp","additions":1,"deletions":1,"binary":false,"changes":2,"status":"modified"},{"patch":"@@ -162,1 +162,1 @@\n-  oopDesc_klass_offset_in_bytes = oopDesc::klass_offset_in_bytes();\n+  oopDesc_klass_offset_in_bytes = 1; \/\/oopDesc::klass_offset_in_bytes();\n","filename":"src\/hotspot\/share\/jvmci\/jvmciCompilerToVMInit.cpp","additions":1,"deletions":1,"binary":false,"changes":2,"status":"modified"},{"patch":"@@ -98,0 +98,1 @@\n+  NOT_PRODUCT(LOG_TAG(ihash)) \\\n","filename":"src\/hotspot\/share\/logging\/logTag.hpp","additions":1,"deletions":0,"binary":false,"changes":1,"status":"modified"},{"patch":"@@ -571,1 +571,1 @@\n-          oop m = java_lang_Class::create_basic_type_mirror(type2name(bt), bt, CHECK);\n+          oop m = java_lang_Class::create_basic_type_mirror(type2name(bt), bt, false, CHECK);\n","filename":"src\/hotspot\/share\/memory\/universe.cpp","additions":1,"deletions":1,"binary":false,"changes":2,"status":"modified"},{"patch":"@@ -303,0 +303,8 @@\n+\n+int ArrayKlass::hash_offset_in_bytes(oop obj, markWord m) const {\n+  assert(UseCompactObjectHeaders, \"only with compact i-hash\");\n+  arrayOop ary = arrayOop(obj);\n+  BasicType type = element_type();\n+  int length = LP64_ONLY(m.array_length()) NOT_LP64(ary->length());\n+  return ary->base_offset_in_bytes(type) + (length << log2_element_size());\n+}\n","filename":"src\/hotspot\/share\/oops\/arrayKlass.cpp","additions":8,"deletions":0,"binary":false,"changes":8,"status":"modified"},{"patch":"@@ -121,0 +121,2 @@\n+  int hash_offset_in_bytes(oop obj, markWord m) const;\n+\n","filename":"src\/hotspot\/share\/oops\/arrayKlass.hpp","additions":2,"deletions":0,"binary":false,"changes":2,"status":"modified"},{"patch":"@@ -78,0 +78,1 @@\n+    assert(!UseCompactObjectHeaders || hs == 8, \"array header must be 8 bytes\");\n@@ -86,0 +87,1 @@\n+    assert(!UseCompactObjectHeaders || oopDesc::base_offset_in_bytes() == 4, \"array length must be at 4 bytes\");\n","filename":"src\/hotspot\/share\/oops\/arrayOop.hpp","additions":2,"deletions":0,"binary":false,"changes":2,"status":"modified"},{"patch":"@@ -79,0 +79,4 @@\n+  \/\/ There is no technical reason preventing us from using other klass pointer bit lengths,\n+  \/\/ but it should be a deliberate choice\n+  ASSERT_HERE(_narrow_klass_pointer_bits == 32 || _narrow_klass_pointer_bits == 19);\n+\n@@ -225,1 +229,6 @@\n-    \/\/ In compact object header mode, with 22-bit narrowKlass, we don't attempt for\n+    \/\/ This handles the case that we - experimentally - reduce the number of\n+    \/\/ class pointer bits further, such that (shift + num bits) < 32.\n+    assert(len <= (size_t)nth_bit(narrow_klass_pointer_bits() + max_shift()),\n+           \"klass range size exceeds encoding, len: %zu, narrow_klass_pointer_bits: %d, max_shift: %d\", len, narrow_klass_pointer_bits(), max_shift());\n+\n+    \/\/ In compact object header mode, with 19-bit narrowKlass, we don't attempt for\n","filename":"src\/hotspot\/share\/oops\/compressedKlass.cpp","additions":10,"deletions":1,"binary":false,"changes":11,"status":"modified"},{"patch":"@@ -111,1 +111,1 @@\n-  static constexpr int narrow_klass_pointer_bits_coh = 22;\n+  static constexpr int narrow_klass_pointer_bits_coh = 19;\n","filename":"src\/hotspot\/share\/oops\/compressedKlass.hpp","additions":1,"deletions":1,"binary":false,"changes":2,"status":"modified"},{"patch":"@@ -548,1 +548,2 @@\n-  _init_thread(nullptr)\n+  _init_thread(nullptr),\n+  _hash_offset(parser.hash_offset())\n@@ -3825,1 +3826,1 @@\n-  st->print_cr(BULLET\"---- fields (total size %zu words):\", oop_size(obj));\n+  st->print_cr(BULLET\"---- fields (total size %zu words):\", oop_size(obj, obj->mark()));\n","filename":"src\/hotspot\/share\/oops\/instanceKlass.cpp","additions":3,"deletions":2,"binary":false,"changes":5,"status":"modified"},{"patch":"@@ -241,0 +241,2 @@\n+  int             _hash_offset;             \/\/ Offset of hidden field for i-hash\n+\n@@ -890,1 +892,1 @@\n-  size_t oop_size(oop obj)  const             { return size_helper(); }\n+  size_t oop_size(oop obj, markWord mark) const { return size_helper(); }\n@@ -951,0 +953,9 @@\n+  virtual int hash_offset_in_bytes(oop obj, markWord m) const {\n+    assert(UseCompactObjectHeaders, \"only with compact i-hash\");\n+    return _hash_offset;\n+  }\n+  static int hash_offset_offset_in_bytes() {\n+    assert(UseCompactObjectHeaders, \"only with compact i-hash\");\n+    return (int)offset_of(InstanceKlass, _hash_offset);\n+  }\n+\n","filename":"src\/hotspot\/share\/oops\/instanceKlass.hpp","additions":12,"deletions":1,"binary":false,"changes":13,"status":"modified"},{"patch":"@@ -53,1 +53,1 @@\n-instanceOop InstanceMirrorKlass::allocate_instance(Klass* k, TRAPS) {\n+instanceOop InstanceMirrorKlass::allocate_instance(Klass* k, bool extend, TRAPS) {\n@@ -55,2 +55,6 @@\n-  size_t size = instance_size(k);\n-  assert(size > 0, \"total object size must be non-zero: %zu\", size);\n+  size_t base_size = instance_size(k);\n+  size_t size = base_size;\n+  if (extend && UseCompactObjectHeaders) {\n+    size = align_object_size(size + 1);\n+  }\n+  assert(base_size > 0, \"base object size must be non-zero: %zu\", base_size);\n@@ -60,1 +64,6 @@\n-  return (instanceOop)Universe::heap()->class_allocate(this, size, THREAD);\n+  instanceOop obj = (instanceOop)Universe::heap()->class_allocate(this, size, base_size, THREAD);\n+  if (extend && UseCompactObjectHeaders) {\n+    obj->set_mark(obj->mark().set_not_hashed_expanded());\n+    assert(expand_for_hash(obj, obj->mark()), \"must expand for hash\");\n+  }\n+  return obj;\n@@ -63,1 +72,1 @@\n-size_t InstanceMirrorKlass::oop_size(oop obj) const {\n+size_t InstanceMirrorKlass::oop_size(oop obj, markWord mark) const {\n@@ -75,0 +84,10 @@\n+int InstanceMirrorKlass::hash_offset_in_bytes(oop obj, markWord m) const {\n+  assert(UseCompactObjectHeaders, \"only with compact i-hash\");\n+  \/\/ TODO: There may be gaps that we could use, e.g. in the fields of Class,\n+  \/\/ between the fields of Class and the static fields or in or at the end of\n+  \/\/ the static fields block.\n+  \/\/ When implementing any change here, make sure that allocate_instance()\n+  \/\/ and corresponding code in InstanceMirrorKlass.java are in sync.\n+  return checked_cast<int>(obj->base_size_given_klass(m, this) * BytesPerWord);\n+}\n+\n","filename":"src\/hotspot\/share\/oops\/instanceMirrorKlass.cpp","additions":24,"deletions":5,"binary":false,"changes":29,"status":"modified"},{"patch":"@@ -68,1 +68,2 @@\n-  virtual size_t oop_size(oop obj) const;\n+  size_t oop_size(oop obj, markWord mark) const;\n+  int hash_offset_in_bytes(oop obj, markWord m) const;\n@@ -92,1 +93,1 @@\n-  instanceOop allocate_instance(Klass* k, TRAPS);\n+  instanceOop allocate_instance(Klass* k, bool extend, TRAPS);\n","filename":"src\/hotspot\/share\/oops\/instanceMirrorKlass.hpp","additions":3,"deletions":2,"binary":false,"changes":5,"status":"modified"},{"patch":"@@ -66,1 +66,1 @@\n-size_t InstanceStackChunkKlass::oop_size(oop obj) const {\n+size_t InstanceStackChunkKlass::oop_size(oop obj, markWord mark) const {\n","filename":"src\/hotspot\/share\/oops\/instanceStackChunkKlass.cpp","additions":1,"deletions":1,"binary":false,"changes":2,"status":"modified"},{"patch":"@@ -131,1 +131,1 @@\n-  virtual size_t oop_size(oop obj) const override;\n+  size_t oop_size(oop obj, markWord mark) const override;\n","filename":"src\/hotspot\/share\/oops\/instanceStackChunkKlass.hpp","additions":1,"deletions":1,"binary":false,"changes":2,"status":"modified"},{"patch":"@@ -57,0 +57,1 @@\n+#include \"utilities\/numberSeq.hpp\"\n@@ -1362,0 +1363,13 @@\n+\n+static int expanded = 0;\n+static int not_expanded = 0;\n+static NumberSeq seq = NumberSeq();\n+\n+bool Klass::expand_for_hash(oop obj, markWord m) const {\n+  assert(UseCompactObjectHeaders, \"only with compact i-hash\");\n+  {\n+    ResourceMark rm;\n+    assert((size_t)hash_offset_in_bytes(obj,m ) <= (obj->base_size_given_klass(m, this) * HeapWordSize), \"hash offset must be eq or lt base size: hash offset: %d, base size: %zu, class-name: %s\", hash_offset_in_bytes(obj, m), obj->base_size_given_klass(m, this) * HeapWordSize, external_name());\n+  }\n+  return obj->base_size_given_klass(m, this) * HeapWordSize - hash_offset_in_bytes(obj, m) < (int)sizeof(uint32_t);\n+}\n","filename":"src\/hotspot\/share\/oops\/klass.cpp","additions":14,"deletions":0,"binary":false,"changes":14,"status":"modified"},{"patch":"@@ -641,1 +641,2 @@\n-  virtual size_t oop_size(oop obj) const = 0;\n+  virtual size_t oop_size(oop obj, markWord mark) const = 0;\n+  size_t oop_size(oop obj) const;\n@@ -792,0 +793,4 @@\n+  virtual int hash_offset_in_bytes(oop obj, markWord m) const = 0;\n+  static int kind_offset_in_bytes() { return (int)offset_of(Klass, _kind); }\n+\n+  bool expand_for_hash(oop obj, markWord m) const;\n","filename":"src\/hotspot\/share\/oops\/klass.hpp","additions":6,"deletions":1,"binary":false,"changes":7,"status":"modified"},{"patch":"@@ -187,0 +187,5 @@\n+\n+inline size_t Klass::oop_size(oop obj) const {\n+  return oop_size(obj, obj->mark());\n+}\n+\n","filename":"src\/hotspot\/share\/oops\/klass.inline.hpp","additions":5,"deletions":0,"binary":false,"changes":5,"status":"modified"},{"patch":"@@ -32,1 +32,1 @@\n-STATIC_ASSERT(markWord::klass_shift + markWord::klass_bits == 64);\n+STATIC_ASSERT(markWord::klass_shift + markWord::klass_bits == 32);\n@@ -34,1 +34,1 @@\n-STATIC_ASSERT(markWord::klass_shift == markWord::hash_bits + markWord::hash_shift);\n+STATIC_ASSERT(markWord::klass_shift == markWord::hashctrl_bits + markWord::hashctrl_shift);\n@@ -93,0 +93,3 @@\n+      } else if (UseCompactObjectHeaders) {\n+        st->print(\" hash is-hashed=%s is-copied=%s\", BOOL_TO_STR(is_hashed_not_expanded()), BOOL_TO_STR(\n+                is_hashed_expanded()));\n","filename":"src\/hotspot\/share\/oops\/markWord.cpp","additions":5,"deletions":2,"binary":false,"changes":7,"status":"modified"},{"patch":"@@ -49,1 +49,1 @@\n-\/\/  klass:22  hash:31 -->| unused_gap:4  age:4  self-fwd:1  lock:2 (normal object)\n+\/\/  klass:22  unused_gap:29 hash:2 -->| unused_gap:4  age:4  self-fwd:1  lock:2 (normal object)\n@@ -56,0 +56,13 @@\n+\/\/  - With +UseCompactObjectHeaders:\n+\/\/    hashctrl bits indicate if object has been hashed:\n+\/\/    00 - never hashed\n+\/\/    01 - hashed, but not expanded by GC: will recompute hash\n+\/\/    10 - not hashed, but expanded; special state used only by CDS to deal with scratch classes\n+\/\/    11 - hashed and expanded by GC, and hashcode has been installed in hidden field\n+\/\/\n+\/\/    When identityHashCode() is called, the transitions work as follows:\n+\/\/    00 - set the hashctrl bits to 01, and compute the identity hash\n+\/\/    01 - recompute idendity hash. When GC encounters 01 when moving an object, it will allocate an extra word, if\n+\/\/         necessary, for the object copy, and install 11.\n+\/\/    11 - read hashcode from field\n+\/\/\n@@ -108,0 +121,1 @@\n+  uint32_t value32() const { return (uint32_t)_value; }\n@@ -116,0 +130,1 @@\n+  static const int hashctrl_bits                  = 2;\n@@ -121,0 +136,1 @@\n+  static const int hashctrl_shift                 = age_shift + age_bits + unused_gap_bits;\n@@ -130,0 +146,4 @@\n+  static const uintptr_t hashctrl_mask            = right_n_bits(hashctrl_bits);\n+  static const uintptr_t hashctrl_mask_in_place   = hashctrl_mask << hashctrl_shift;\n+  static const uintptr_t hashctrl_hashed_mask_in_place    = ((uintptr_t)1) << hashctrl_shift;\n+  static const uintptr_t hashctrl_expanded_mask_in_place  = ((uintptr_t)2) << hashctrl_shift;\n@@ -136,4 +156,2 @@\n-  static constexpr int klass_offset_in_bytes      = 4;\n-  static constexpr int klass_shift                = hash_shift + hash_bits;\n-  static constexpr int klass_shift_at_offset      = klass_shift - klass_offset_in_bytes * BitsPerByte;\n-  static constexpr int klass_bits                 = 22;\n+  static constexpr int klass_shift                = hashctrl_shift + hashctrl_bits;\n+  static constexpr int klass_bits                 = 19;\n@@ -149,0 +167,1 @@\n+  static const uintptr_t forward_expanded_value   = 0b111;\n@@ -167,1 +186,1 @@\n-    return (mask_bits(value(), lock_mask_in_place) == marked_value);\n+    return (value() & (self_fwd_mask_in_place | lock_mask_in_place)) > monitor_value;\n@@ -177,0 +196,8 @@\n+  markWord set_forward_expanded() {\n+    assert((value() & (lock_mask_in_place | self_fwd_mask_in_place)) == marked_value, \"must be normal-forwarded here\");\n+    return markWord(value() | forward_expanded_value);\n+  }\n+  bool is_forward_expanded() {\n+    return (value() & (lock_mask_in_place | self_fwd_mask_in_place)) == forward_expanded_value;\n+  }\n+\n@@ -192,1 +219,1 @@\n-    return (!is_unlocked() || !has_no_hash());\n+    return UseCompactObjectHeaders ? !is_unlocked() : (!is_unlocked() || !has_no_hash());\n@@ -239,0 +266,1 @@\n+    assert(!UseCompactObjectHeaders, \"Do not use with compact i-hash\");\n@@ -264,1 +292,1 @@\n-  markWord clear_lock_bits() const { return markWord(value() & ~lock_mask_in_place); }\n+  markWord clear_lock_bits() const { return markWord(value() & ~(lock_mask_in_place | self_fwd_mask_in_place)); }\n@@ -279,0 +307,1 @@\n+    assert(!UseCompactObjectHeaders, \"only without compact i-hash\");\n@@ -283,1 +312,57 @@\n-    return hash() == no_hash;\n+    if (UseCompactObjectHeaders) {\n+      return !is_hashed();\n+    } else {\n+      return hash() == no_hash;\n+    }\n+  }\n+\n+  inline bool is_hashed_not_expanded() const {\n+    assert(UseCompactObjectHeaders, \"only with compact i-hash\");\n+    return (value() & hashctrl_mask_in_place) == hashctrl_hashed_mask_in_place;\n+  }\n+  inline markWord set_hashed_not_expanded() const {\n+    assert(UseCompactObjectHeaders, \"only with compact i-hash\");\n+    return markWord((value() & ~hashctrl_mask_in_place) | hashctrl_hashed_mask_in_place);\n+  }\n+\n+  inline bool is_hashed_expanded() const {\n+    assert(UseCompactObjectHeaders, \"only with compact i-hash\");\n+    return (value() & hashctrl_mask_in_place) == (hashctrl_hashed_mask_in_place | hashctrl_expanded_mask_in_place);\n+  }\n+  inline markWord set_hashed_expanded() const {\n+    assert(UseCompactObjectHeaders, \"only with compact i-hash\");\n+    return markWord((value() & ~hashctrl_mask_in_place) | (hashctrl_hashed_mask_in_place | hashctrl_expanded_mask_in_place));\n+  }\n+\n+  \/\/ This is a special hashctrl state (11) that is only used\n+  \/\/ during CDS archive dumping. There we allocate 'scratch mirrors' for\n+  \/\/ each real mirror klass. We allocate those scratch mirrors\n+  \/\/ in a pre-extended form, but without being hashed. When the\n+  \/\/ real mirror gets hashed, then we turn the scratch mirror into\n+  \/\/ hashed_moved state, otherwise we leave it in that special state\n+  \/\/ which indicates that the archived copy will be allocated in the\n+  \/\/ unhashed form.\n+  inline bool is_not_hashed_expanded() const {\n+    assert(UseCompactObjectHeaders, \"only with compact i-hash\");\n+    return (value() & hashctrl_mask_in_place) == hashctrl_expanded_mask_in_place;\n+  }\n+  inline markWord set_not_hashed_expanded() const {\n+    assert(UseCompactObjectHeaders, \"only with compact i-hash\");\n+    return markWord((value() & ~hashctrl_mask_in_place) | hashctrl_expanded_mask_in_place);\n+  }\n+  \/\/ Return true when object is either hashed_moved or not_hashed_moved.\n+  inline bool is_expanded() const {\n+    assert(UseCompactObjectHeaders, \"only with compact i-hash\");\n+    return (value() & hashctrl_expanded_mask_in_place) != 0;\n+  }\n+  inline bool is_hashed() const {\n+    assert(UseCompactObjectHeaders, \"only with compact i-hash\");\n+    return (value() & hashctrl_hashed_mask_in_place) != 0;\n+  }\n+\n+  inline markWord copy_hashctrl_from(markWord m) const {\n+    if (UseCompactObjectHeaders) {\n+      return markWord((value() & ~hashctrl_mask_in_place) | (m.value() & hashctrl_mask_in_place));\n+    } else {\n+      return markWord(value());\n+    }\n@@ -292,0 +377,4 @@\n+#ifdef _LP64\n+  inline int array_length() { return checked_cast<int>(value() >> 32); }\n+#endif\n+\n@@ -294,1 +383,5 @@\n-    return markWord( no_hash_in_place | no_lock_in_place );\n+    if (UseCompactObjectHeaders) {\n+      return markWord(no_lock_in_place);\n+    } else {\n+      return markWord(no_hash_in_place | no_lock_in_place);\n+    }\n@@ -308,1 +401,2 @@\n-    return mask_bits(value(), self_fwd_mask_in_place) != 0;\n+    \/\/ Match 100, 101, 110 but not 111.\n+    return mask_bits(value() + 1, (lock_mask_in_place | self_fwd_mask_in_place)) > 4;\n","filename":"src\/hotspot\/share\/oops\/markWord.hpp","additions":105,"deletions":11,"binary":false,"changes":116,"status":"modified"},{"patch":"@@ -34,1 +34,1 @@\n-  return narrowKlass(value() >> klass_shift);\n+  return narrowKlass(value32() >> klass_shift);\n","filename":"src\/hotspot\/share\/oops\/markWord.inline.hpp","additions":1,"deletions":1,"binary":false,"changes":2,"status":"modified"},{"patch":"@@ -144,1 +144,1 @@\n-size_t ObjArrayKlass::oop_size(oop obj) const {\n+size_t ObjArrayKlass::oop_size(oop obj, markWord mark) const {\n@@ -149,1 +149,2 @@\n-  return objArrayOop(obj)->object_size();\n+  int length = LP64_ONLY(UseCompactObjectHeaders ? mark.array_length() :) objArrayOop(obj)->length();\n+  return objArrayOop(obj)->object_size(length);\n","filename":"src\/hotspot\/share\/oops\/objArrayKlass.cpp","additions":3,"deletions":2,"binary":false,"changes":5,"status":"modified"},{"patch":"@@ -79,1 +79,1 @@\n-  size_t oop_size(oop obj) const;\n+  size_t oop_size(oop obj, markWord) const;\n","filename":"src\/hotspot\/share\/oops\/objArrayKlass.hpp","additions":1,"deletions":1,"binary":false,"changes":2,"status":"modified"},{"patch":"@@ -38,2 +38,2 @@\n-    _oop_base_offset_in_bytes = sizeof(markWord);\n-    _oop_has_klass_gap = false;\n+    _oop_base_offset_in_bytes = sizeof(uint32_t);\n+    _oop_has_klass_gap = true;\n","filename":"src\/hotspot\/share\/oops\/objLayout.cpp","additions":2,"deletions":2,"binary":false,"changes":4,"status":"modified"},{"patch":"@@ -40,0 +40,1 @@\n+#include \"runtime\/lightweightSynchronizer.hpp\"\n@@ -131,0 +132,17 @@\n+void oopDesc::initialize_hash_if_necessary(oop obj, Klass* k, markWord m) {\n+  assert(UseCompactObjectHeaders, \"only with compact object headers\");\n+  assert(!m.has_displaced_mark_helper(), \"must not be displaced header\");\n+  assert(m.is_hashed_not_expanded(), \"must be hashed but not moved\");\n+  assert(!m.is_hashed_expanded(), \"must not be moved: \" INTPTR_FORMAT, m.value());\n+  uint32_t hash = static_cast<uint32_t>(ObjectSynchronizer::get_next_hash(nullptr, obj));\n+  int offset = k->hash_offset_in_bytes(cast_to_oop(this), m);\n+  assert(offset >= 4, \"hash offset must not be in header\");\n+  log_develop_trace(gc)(\"Initializing hash for \" PTR_FORMAT \", old: \" PTR_FORMAT \", hash: %d, offset: %d, is_mirror: %s\", p2i(this), p2i(obj), hash, offset, BOOL_TO_STR(k->is_mirror_instance_klass()));\n+  int_field_put(offset, (jint) hash);\n+  m = m.set_hashed_expanded();\n+  assert(static_cast<uint32_t>(LightweightSynchronizer::get_hash(m, cast_to_oop(this), k)) == hash,\n+         \"hash must remain the same\");\n+  assert(m.narrow_klass() != 0, \"must not be null\");\n+  set_mark(m);\n+}\n+\n","filename":"src\/hotspot\/share\/oops\/oop.cpp","additions":18,"deletions":0,"binary":false,"changes":18,"status":"modified"},{"patch":"@@ -73,0 +73,1 @@\n+  inline void set_mark_full(markWord m);\n@@ -114,0 +115,22 @@\n+  \/\/ Returns the size that a copy of this object requires, in machine words.\n+  \/\/ It can be 1 word larger than its current size to accomodate\n+  \/\/ an additional 4-byte-field for the identity hash-code.\n+  \/\/\n+  \/\/ size: the current size of this object, we're passing this here for performance\n+  \/\/       reasons, because all callers compute this anyway, and we want to avoid\n+  \/\/       recomputing it.\n+  \/\/ mark: the mark-word of this object. Some callers (e.g. G1ParScanThreadState::do_copy_to_survivor_space())\n+  \/\/       need to use a known markWord because of racing GC threads that can change\n+  \/\/       the markWord at any time.\n+  inline size_t copy_size(size_t size, markWord mark) const;\n+  \/\/ Special version to deal with scratch classes in CDS. There we allocate\n+  \/\/ temporary scratch classes (which are skeleton versions of InstanceMirrorKlass,\n+  \/\/ which represent java.lang.Class objects in the CDS archive). At that point, we\n+  \/\/ don't know whether or not the final archived version will be hashed or expanded,\n+  \/\/ and therefore we allocate them in the special state not-hashed-but-expanded.\n+  \/\/ When creating the final copy of those objects, we either populate the hidden hash\n+  \/\/ field and make the object 'expanded', or we turn it back to 'not-hashed'\n+  \/\/ and reduce the object's size. We do this by providing a separate method for CDS\n+  \/\/ so that we don't affect GC performance.\n+  inline size_t copy_size_cds(size_t size, markWord mark) const;\n+\n@@ -116,1 +139,2 @@\n-  inline size_t size_given_klass(Klass* klass);\n+  inline size_t base_size_given_klass(markWord m, const Klass* klass);\n+  inline size_t size_given_mark_and_klass(markWord mrk, const Klass* kls);\n@@ -284,0 +308,1 @@\n+  inline void reset_forwarded();\n@@ -314,0 +339,6 @@\n+  \/\/ Initialize identity hash code in hash word of object copy from original object.\n+  \/\/ Returns true if the object has been expanded, false otherwise.\n+  inline void initialize_hash_if_necessary(oop obj);\n+  \/\/ For CDS only.\n+  void initialize_hash_if_necessary(oop obj, Klass* k, markWord m);\n+\n@@ -330,10 +361,2 @@\n-#ifdef _LP64\n-    if (UseCompactObjectHeaders) {\n-      \/\/ NOTE: The only places where this is used with compact headers are the C2\n-      \/\/ compiler and JVMCI.\n-      return mark_offset_in_bytes() + markWord::klass_offset_in_bytes;\n-    } else\n-#endif\n-    {\n-      return (int)offset_of(oopDesc, _metadata._klass);\n-    }\n+    assert(!UseCompactObjectHeaders, \"don't use this with compact headers\");\n+    return (int)offset_of(oopDesc, _metadata._klass);\n@@ -343,1 +366,5 @@\n-    return klass_offset_in_bytes() + sizeof(narrowKlass);\n+    if (UseCompactObjectHeaders) {\n+      return base_offset_in_bytes();\n+    } else {\n+      return klass_offset_in_bytes() + sizeof(narrowKlass);\n+    }\n","filename":"src\/hotspot\/share\/oops\/oop.hpp","additions":39,"deletions":12,"binary":false,"changes":51,"status":"modified"},{"patch":"@@ -63,0 +63,8 @@\n+  if (UseCompactObjectHeaders) {\n+    Atomic::store(reinterpret_cast<uint32_t volatile*>(&_mark), m.value32());\n+  } else {\n+    Atomic::store(&_mark, m);\n+  }\n+}\n+\n+void oopDesc::set_mark_full(markWord m) {\n@@ -67,1 +75,5 @@\n-  *(markWord*)(((char*)mem) + mark_offset_in_bytes()) = m;\n+  if (UseCompactObjectHeaders) {\n+    *(uint32_t*)(((char*)mem) + mark_offset_in_bytes()) = m.value32();\n+  } else {\n+    *(markWord*)(((char*)mem) + mark_offset_in_bytes()) = m;\n+  }\n@@ -71,1 +83,5 @@\n-  Atomic::release_store((markWord*)(((char*)mem) + mark_offset_in_bytes()), m);\n+  if (UseCompactObjectHeaders) {\n+    Atomic::release_store((uint32_t*)(((char*)mem) + mark_offset_in_bytes()), m.value32());\n+  } else {\n+    Atomic::release_store((markWord*)(((char*)mem) + mark_offset_in_bytes()), m);\n+  }\n@@ -75,1 +91,5 @@\n-  Atomic::release_store(&_mark, m);\n+  if (UseCompactObjectHeaders) {\n+    Atomic::release_store(reinterpret_cast<uint32_t volatile*>(&_mark), m.value32());\n+  } else {\n+    Atomic::release_store(&_mark, m);\n+  }\n@@ -95,1 +115,7 @@\n-  set_mark(prototype_mark());\n+  if (UseCompactObjectHeaders) {\n+    markWord m = prototype_mark().copy_hashctrl_from(mark());\n+    assert(m.is_neutral(), \"must be neutral\");\n+    set_mark(m);\n+  } else {\n+    set_mark(prototype_mark());\n+  }\n@@ -175,0 +201,38 @@\n+size_t oopDesc::size_given_mark_and_klass(markWord mrk, const Klass* kls) {\n+  size_t sz = base_size_given_klass(mrk, kls);\n+  if (UseCompactObjectHeaders) {\n+    assert(!mrk.has_displaced_mark_helper(), \"must not be displaced\");\n+    if (mrk.is_expanded() && kls->expand_for_hash(cast_to_oop(this), mrk)) {\n+      sz = align_object_size(sz + 1);\n+    }\n+  }\n+  return sz;\n+}\n+\n+size_t oopDesc::copy_size(size_t size, markWord mark) const {\n+  if (UseCompactObjectHeaders) {\n+    assert(!mark.has_displaced_mark_helper(), \"must not be displaced\");\n+    Klass* klass = mark.klass();\n+    if (mark.is_hashed_not_expanded() && klass->expand_for_hash(cast_to_oop(this), mark)) {\n+      size = align_object_size(size + 1);\n+    }\n+  }\n+  assert(is_object_aligned(size), \"Oop size is not properly aligned: %zu\", size);\n+  return size;\n+}\n+\n+size_t oopDesc::copy_size_cds(size_t size, markWord mark) const {\n+  if (UseCompactObjectHeaders) {\n+    assert(!mark.has_displaced_mark_helper(), \"must not be displaced\");\n+    Klass* klass = mark.klass();\n+    if (mark.is_hashed_not_expanded() && klass->expand_for_hash(cast_to_oop(this), mark)) {\n+      size = align_object_size(size + 1);\n+    }\n+    if (mark.is_not_hashed_expanded() && klass->expand_for_hash(cast_to_oop(this), mark)) {\n+      size = align_object_size(size - ObjectAlignmentInBytes \/ HeapWordSize);\n+    }\n+  }\n+  assert(is_object_aligned(size), \"Oop size is not properly aligned: %zu\", size);\n+  return size;\n+}\n+\n@@ -176,1 +240,1 @@\n-  return size_given_klass(klass());\n+  return size_given_mark_and_klass(mark(), klass());\n@@ -179,1 +243,1 @@\n-size_t oopDesc::size_given_klass(Klass* klass)  {\n+size_t oopDesc::base_size_given_klass(markWord mrk, const Klass* klass)  {\n@@ -198,1 +262,1 @@\n-      s = klass->oop_size(this);\n+      s = klass->oop_size(this, mrk);\n@@ -207,1 +271,8 @@\n-      size_t array_length = (size_t) ((arrayOop)this)->length();\n+      size_t array_length;\n+#ifdef _LP64\n+      if (UseCompactObjectHeaders) {\n+        array_length = (size_t) mrk.array_length();\n+      } else\n+#endif\n+        array_length = (size_t)((arrayOop)this)->length();\n+\n@@ -215,2 +286,6 @@\n-\n-      assert(s == klass->oop_size(this), \"wrong array object size\");\n+      if (s != klass->oop_size(this, mrk)) {\n+        tty->print_cr(\"length: %zu\", array_length);\n+        tty->print_cr(\"log element size: %d\", Klass::layout_helper_log2_element_size(lh));\n+        tty->print_cr(\"is_objArray: %s\", BOOL_TO_STR(klass->is_objArray_klass()));\n+      }\n+      assert(s == klass->oop_size(this, mrk), \"wrong array object size, s: %zu, oop_size: %zu\", s, klass->oop_size(this, mrk));\n@@ -219,1 +294,1 @@\n-      s = klass->oop_size(this);\n+      s = klass->oop_size(this, mrk);\n@@ -302,0 +377,3 @@\n+  if (UseCompactObjectHeaders && p->mark().is_expanded() && !mark().is_expanded()) {\n+    m = m.set_forward_expanded();\n+  }\n@@ -303,1 +381,1 @@\n-  set_mark(m);\n+  set_mark_full(m);\n@@ -310,0 +388,16 @@\n+void oopDesc::reset_forwarded() {\n+  markWord m = mark();\n+  if (m.is_self_forwarded()) {\n+    unset_self_forwarded();\n+  } else if (m.is_forwarded()) {\n+    \/\/ Restore Klass* and hash-bits in the header,\n+    \/\/ for correct iteration.\n+    markWord fwd_mark = forwardee()->mark();\n+    if (m.is_forward_expanded()) {\n+      \/\/ Un-expand original object.\n+      fwd_mark = fwd_mark.set_hashed_expanded();\n+    }\n+    set_mark_full(fwd_mark);\n+  }\n+}\n+\n@@ -388,1 +482,1 @@\n-  size_t size = size_given_klass(k);\n+  size_t size = size_given_mark_and_klass(mark(), k);\n@@ -396,1 +490,1 @@\n-  size_t size = size_given_klass(k);\n+  size_t size = size_given_mark_and_klass(mark(), k);\n@@ -420,5 +514,7 @@\n-  markWord mrk = mark();\n-  if (mrk.is_unlocked() && !mrk.has_no_hash()) {\n-    return mrk.hash();\n-  } else if (mrk.is_marked()) {\n-    return mrk.hash();\n+  if (UseCompactObjectHeaders) {\n+    markWord mrk = mark();\n+    if (mrk.is_hashed_expanded()) {\n+      Klass* klass = mrk.klass();\n+      return int_field(klass->hash_offset_in_bytes(cast_to_oop(this), mrk));\n+    }\n+    \/\/ Fall-through to slow-case.\n@@ -426,1 +522,7 @@\n-    return slow_identity_hash();\n+    markWord mrk = mark();\n+    if (mrk.is_unlocked() && !mrk.has_no_hash()) {\n+      return mrk.hash();\n+    } else if (mrk.is_marked()) {\n+      return mrk.hash();\n+    }\n+    \/\/ Fall-through to slow-case.\n@@ -428,0 +530,1 @@\n+  return slow_identity_hash();\n@@ -435,1 +538,1 @@\n-  return mrk.is_unlocked() && mrk.has_no_hash();\n+  return (UseCompactObjectHeaders || mrk.is_unlocked()) && mrk.has_no_hash();\n@@ -458,0 +561,13 @@\n+inline void oopDesc::initialize_hash_if_necessary(oop obj) {\n+  if (!UseCompactObjectHeaders) {\n+    return;\n+  }\n+  markWord m = mark();\n+  assert(!m.has_displaced_mark_helper(), \"must not be displaced header\");\n+  assert(!m.is_forwarded(), \"must not be forwarded header\");\n+  if (m.is_hashed_not_expanded()) {\n+    initialize_hash_if_necessary(obj, m.klass(), m);\n+  }\n+}\n+\n+\n","filename":"src\/hotspot\/share\/oops\/oop.inline.hpp","additions":137,"deletions":21,"binary":false,"changes":158,"status":"modified"},{"patch":"@@ -176,1 +176,1 @@\n-size_t TypeArrayKlass::oop_size(oop obj) const {\n+size_t TypeArrayKlass::oop_size(oop obj, markWord mark) const {\n@@ -180,1 +180,2 @@\n-  return t->object_size(this);\n+  int length = LP64_ONLY(UseCompactObjectHeaders ? mark.array_length() :) t->length();\n+  return t->object_size(this, length);\n","filename":"src\/hotspot\/share\/oops\/typeArrayKlass.cpp","additions":3,"deletions":2,"binary":false,"changes":5,"status":"modified"},{"patch":"@@ -71,1 +71,1 @@\n-  size_t oop_size(oop obj) const;\n+  size_t oop_size(oop obj, markWord mark) const;\n","filename":"src\/hotspot\/share\/oops\/typeArrayKlass.hpp","additions":1,"deletions":1,"binary":false,"changes":2,"status":"modified"},{"patch":"@@ -136,0 +136,1 @@\n+  inline size_t object_size(const TypeArrayKlass* tk, int length) const;\n","filename":"src\/hotspot\/share\/oops\/typeArrayOop.hpp","additions":1,"deletions":0,"binary":false,"changes":1,"status":"modified"},{"patch":"@@ -34,0 +34,4 @@\n+size_t typeArrayOopDesc::object_size(const TypeArrayKlass* tk, int length) const {\n+  return object_size(tk->layout_helper(), length);\n+}\n+\n@@ -35,1 +39,1 @@\n-  return object_size(tk->layout_helper(), length());\n+  return object_size(tk, length());\n","filename":"src\/hotspot\/share\/oops\/typeArrayOop.inline.hpp","additions":5,"deletions":1,"binary":false,"changes":6,"status":"modified"},{"patch":"@@ -1383,1 +1383,1 @@\n-      } else if( offset == oopDesc::klass_offset_in_bytes() ) {\n+      } else if( offset == Type::klass_offset() ) {\n@@ -1556,1 +1556,1 @@\n-          (offset == oopDesc::klass_offset_in_bytes() && tj->base() == Type::AryPtr) ||\n+          (offset == Type::klass_offset() && tj->base() == Type::AryPtr) ||\n","filename":"src\/hotspot\/share\/opto\/compile.cpp","additions":2,"deletions":2,"binary":false,"changes":4,"status":"modified"},{"patch":"@@ -947,1 +947,1 @@\n-    Node* p = basic_plus_adr( ex_node, ex_node, oopDesc::klass_offset_in_bytes());\n+    Node* p = basic_plus_adr( ex_node, ex_node, Type::klass_offset());\n@@ -965,2 +965,2 @@\n-        Node* p = basic_plus_adr(ex_in, ex_in, oopDesc::klass_offset_in_bytes());\n-        Node* k = _gvn.transform(LoadKlassNode::make(_gvn, immutable_memory(), p, TypeInstPtr::KLASS, TypeInstKlassPtr::OBJECT));\n+        Node* p = basic_plus_adr(ex_in, ex_in, Type::klass_offset());\n+        Node* k = _gvn.transform( LoadKlassNode::make(_gvn, immutable_memory(), p, TypeInstPtr::KLASS, TypeInstKlassPtr::OBJECT));\n","filename":"src\/hotspot\/share\/opto\/doCall.cpp","additions":3,"deletions":3,"binary":false,"changes":6,"status":"modified"},{"patch":"@@ -3443,1 +3443,1 @@\n-  } else if (offset != oopDesc::klass_offset_in_bytes()) {\n+  } else if (offset != Type::klass_offset()) {\n@@ -4442,1 +4442,1 @@\n-      _compile->get_alias_index(tinst->add_offset(oopDesc::klass_offset_in_bytes()));\n+      _compile->get_alias_index(tinst->add_offset(Type::klass_offset()));\n","filename":"src\/hotspot\/share\/opto\/escape.cpp","additions":2,"deletions":2,"binary":false,"changes":4,"status":"modified"},{"patch":"@@ -1217,1 +1217,1 @@\n-  Node* k_adr = basic_plus_adr(obj, oopDesc::klass_offset_in_bytes());\n+  Node* k_adr = basic_plus_adr(obj, Type::klass_offset());\n@@ -3632,1 +3632,1 @@\n-    set_memory(minit_out, C->get_alias_index(oop_type->add_offset(oopDesc::klass_offset_in_bytes())));\n+    set_memory(minit_out, C->get_alias_index(oop_type->add_offset(Type::klass_offset())));\n","filename":"src\/hotspot\/share\/opto\/graphKit.cpp","additions":2,"deletions":2,"binary":false,"changes":4,"status":"modified"},{"patch":"@@ -4725,1 +4725,1 @@\n-  enum { _slow_path = 1, _fast_path, _null_path, PATH_LIMIT };\n+  enum { _slow_path = 1, _null_path, _fast_path, _fast_path2, PATH_LIMIT };\n@@ -4773,6 +4773,34 @@\n-  \/\/ Get the header out of the object, use LoadMarkNode when available\n-  Node* header_addr = basic_plus_adr(obj, oopDesc::mark_offset_in_bytes());\n-  \/\/ The control of the load must be null. Otherwise, the load can move before\n-  \/\/ the null check after castPP removal.\n-  Node* no_ctrl = nullptr;\n-  Node* header = make_load(no_ctrl, header_addr, TypeX_X, TypeX_X->basic_type(), MemNode::unordered);\n+  if (UseCompactObjectHeaders) {\n+    \/\/ Get the header out of the object.\n+    Node* header_addr = basic_plus_adr(obj, oopDesc::mark_offset_in_bytes());\n+    \/\/ The control of the load must be null. Otherwise, the load can move before\n+    \/\/ the null check after castPP removal.\n+    Node* no_ctrl = nullptr;\n+    Node* header = make_load(no_ctrl, header_addr, TypeX_X, TypeX_X->basic_type(), MemNode::unordered);\n+\n+    \/\/ Test the header to see if the object is in hashed or copied state.\n+    Node* hashctrl_mask  = _gvn.MakeConX(markWord::hashctrl_mask_in_place);\n+    Node* masked_header  = _gvn.transform(new AndXNode(header, hashctrl_mask));\n+\n+    \/\/ Take slow-path when the object has not been hashed.\n+    Node* not_hashed_val = _gvn.MakeConX(0);\n+    Node* chk_hashed     = _gvn.transform(new CmpXNode(masked_header, not_hashed_val));\n+    Node* test_hashed    = _gvn.transform(new BoolNode(chk_hashed, BoolTest::eq));\n+\n+    generate_slow_guard(test_hashed, slow_region);\n+\n+    \/\/ Test whether the object is hashed or hashed&copied.\n+    Node* hashed_copied = _gvn.MakeConX(markWord::hashctrl_expanded_mask_in_place | markWord::hashctrl_hashed_mask_in_place);\n+    Node* chk_copied    = _gvn.transform(new CmpXNode(masked_header, hashed_copied));\n+    \/\/ If true, then object has been hashed&copied, otherwise it's only hashed.\n+    Node* test_copied   = _gvn.transform(new BoolNode(chk_copied, BoolTest::eq));\n+    IfNode* if_copied   = create_and_map_if(control(), test_copied, PROB_FAIR, COUNT_UNKNOWN);\n+    Node* if_true = _gvn.transform(new IfTrueNode(if_copied));\n+    Node* if_false = _gvn.transform(new IfFalseNode(if_copied));\n+\n+    \/\/ Hashed&Copied path: read hash-code out of the object.\n+    set_control(if_true);\n+    \/\/ result_val->del_req(_fast_path2);\n+    \/\/ result_reg->del_req(_fast_path2);\n+    \/\/ result_io->del_req(_fast_path2);\n+    \/\/ result_mem->del_req(_fast_path2);\n@@ -4780,8 +4808,19 @@\n-  if (!UseObjectMonitorTable) {\n-    \/\/ Test the header to see if it is safe to read w.r.t. locking.\n-    Node *lock_mask      = _gvn.MakeConX(markWord::lock_mask_in_place);\n-    Node *lmasked_header = _gvn.transform(new AndXNode(header, lock_mask));\n-    if (LockingMode == LM_LIGHTWEIGHT) {\n-      Node *monitor_val   = _gvn.MakeConX(markWord::monitor_value);\n-      Node *chk_monitor   = _gvn.transform(new CmpXNode(lmasked_header, monitor_val));\n-      Node *test_monitor  = _gvn.transform(new BoolNode(chk_monitor, BoolTest::eq));\n+    Node* obj_klass = load_object_klass(obj);\n+    Node* hash_addr;\n+    const TypeKlassPtr* klass_t = _gvn.type(obj_klass)->isa_klassptr();\n+    bool load_offset_runtime = true;\n+\n+    if (klass_t != nullptr) {\n+      if (klass_t->klass_is_exact()  && klass_t->isa_instklassptr()) {\n+        ciInstanceKlass* ciKlass = reinterpret_cast<ciInstanceKlass*>(klass_t->is_instklassptr()->exact_klass());\n+        if (!ciKlass->is_mirror_instance_klass() && !ciKlass->is_reference_instance_klass()) {\n+          \/\/ We know the InstanceKlass, load hash_offset from there at compile-time.\n+          int hash_offset = ciKlass->hash_offset_in_bytes();\n+          hash_addr = basic_plus_adr(obj, hash_offset);\n+          Node* loaded_hash = make_load(control(), hash_addr, TypeInt::INT, T_INT, MemNode::unordered);\n+          result_val->init_req(_fast_path2, loaded_hash);\n+          result_reg->init_req(_fast_path2, control());\n+          load_offset_runtime = false;\n+        }\n+      }\n+    }\n@@ -4789,5 +4828,22 @@\n-      generate_slow_guard(test_monitor, slow_region);\n-    } else {\n-      Node *unlocked_val      = _gvn.MakeConX(markWord::unlocked_value);\n-      Node *chk_unlocked      = _gvn.transform(new CmpXNode(lmasked_header, unlocked_val));\n-      Node *test_not_unlocked = _gvn.transform(new BoolNode(chk_unlocked, BoolTest::ne));\n+    \/\/tty->print_cr(\"Load hash-offset at runtime: %s\", BOOL_TO_STR(load_offset_runtime));\n+\n+    if (load_offset_runtime) {\n+      \/\/ We don't know if it is an array or an exact type, figure it out at run-time.\n+      \/\/ If not an ordinary instance, then we need to take slow-path.\n+      Node* kind_addr = basic_plus_adr(obj_klass, Klass::kind_offset_in_bytes());\n+      Node* kind = make_load(control(), kind_addr, TypeInt::INT, T_INT, MemNode::unordered);\n+      Node* instance_val = _gvn.intcon(Klass::InstanceKlassKind);\n+      Node* chk_inst     = _gvn.transform(new CmpINode(kind, instance_val));\n+      Node* test_inst    = _gvn.transform(new BoolNode(chk_inst, BoolTest::ne));\n+      generate_slow_guard(test_inst, slow_region);\n+\n+      \/\/ Otherwise it's an instance and we can read the hash_offset from the InstanceKlass.\n+      Node* hash_offset_addr = basic_plus_adr(obj_klass, InstanceKlass::hash_offset_offset_in_bytes());\n+      Node* hash_offset = make_load(control(), hash_offset_addr, TypeInt::INT, T_INT, MemNode::unordered);\n+      \/\/ hash_offset->dump();\n+      Node* hash_addr = basic_plus_adr(obj, ConvI2X(hash_offset));\n+      Compile::current()->set_has_unsafe_access(true);\n+      Node* loaded_hash = make_load(control(), hash_addr, TypeInt::INT, T_INT, MemNode::unordered);\n+      result_val->init_req(_fast_path2, loaded_hash);\n+      result_reg->init_req(_fast_path2, control());\n+    }\n@@ -4795,1 +4851,78 @@\n-      generate_slow_guard(test_not_unlocked, slow_region);\n+    \/\/ Hashed-only path: recompute hash-code from object address.\n+    set_control(if_false);\n+    if (hashCode == 6) {\n+      \/\/ Our constants.\n+      Node* M = _gvn.intcon(0x337954D5);\n+      Node* A = _gvn.intcon(0xAAAAAAAA);\n+      \/\/ Split object address into lo and hi 32 bits.\n+      Node* obj_addr = _gvn.transform(new CastP2XNode(nullptr, obj));\n+      Node* x = _gvn.transform(new ConvL2INode(obj_addr));\n+      Node* upper_addr = _gvn.transform(new URShiftLNode(obj_addr, _gvn.intcon(32)));\n+      Node* y = _gvn.transform(new ConvL2INode(upper_addr));\n+\n+      Node* H0 = _gvn.transform(new XorINode(x, y));\n+      Node* L0 = _gvn.transform(new XorINode(x, A));\n+\n+      \/\/ Full multiplication of two 32 bit values L0 and M into a hi\/lo result in two 32 bit values V0 and U0.\n+      Node* L0_64 = _gvn.transform(new ConvI2LNode(L0));\n+      L0_64 = _gvn.transform(new AndLNode(L0_64, _gvn.longcon(0xFFFFFFFF)));\n+      Node* M_64 = _gvn.transform(new ConvI2LNode(M));\n+      \/\/ M_64 = _gvn.transform(new AndLNode(M_64, _gvn.longcon(0xFFFFFFFF)));\n+      Node* prod64 = _gvn.transform(new MulLNode(L0_64, M_64));\n+      Node* V0 = _gvn.transform(new ConvL2INode(prod64));\n+      Node* prod_upper = _gvn.transform(new URShiftLNode(prod64, _gvn.intcon(32)));\n+      Node* U0 = _gvn.transform(new ConvL2INode(prod_upper));\n+\n+      Node* Q0 = _gvn.transform(new MulINode(H0, M));\n+      Node* L1 = _gvn.transform(new XorINode(Q0, U0));\n+\n+      \/\/ Full multiplication of two 32 bit values L1 and M into a hi\/lo result in two 32 bit values V1 and U1.\n+      Node* L1_64 = _gvn.transform(new ConvI2LNode(L1));\n+      L1_64 = _gvn.transform(new AndLNode(L1_64, _gvn.longcon(0xFFFFFFFF)));\n+      prod64 = _gvn.transform(new MulLNode(L1_64, M_64));\n+      Node* V1 = _gvn.transform(new ConvL2INode(prod64));\n+      prod_upper = _gvn.transform(new URShiftLNode(prod64, _gvn.intcon(32)));\n+      Node* U1 = _gvn.transform(new ConvL2INode(prod_upper));\n+\n+      Node* P1 = _gvn.transform(new XorINode(V0, M));\n+\n+      \/\/ Right rotate P1 by distance L1.\n+      Node* distance = _gvn.transform(new AndINode(L1, _gvn.intcon(32 - 1)));\n+      Node* inverse_distance = _gvn.transform(new SubINode(_gvn.intcon(32), distance));\n+      Node* ror_part1 = _gvn.transform(new URShiftINode(P1, distance));\n+      Node* ror_part2 = _gvn.transform(new LShiftINode(P1, inverse_distance));\n+      Node* Q1 = _gvn.transform(new OrINode(ror_part1, ror_part2));\n+\n+      Node* L2 = _gvn.transform(new XorINode(Q1, U1));\n+      Node* hash = _gvn.transform(new XorINode(V1, L2));\n+      Node* hash_truncated = _gvn.transform(new AndINode(hash, _gvn.intcon(markWord::hash_mask)));\n+\n+      result_val->init_req(_fast_path, hash_truncated);\n+    } else if (hashCode == 2) {\n+      result_val->init_req(_fast_path, _gvn.intcon(1));\n+    }\n+  } else {\n+    \/\/ Get the header out of the object, use LoadMarkNode when available\n+    Node* header_addr = basic_plus_adr(obj, oopDesc::mark_offset_in_bytes());\n+    \/\/ The control of the load must be null. Otherwise, the load can move before\n+    \/\/ the null check after castPP removal.\n+    Node* no_ctrl = nullptr;\n+    Node* header = make_load(no_ctrl, header_addr, TypeX_X, TypeX_X->basic_type(), MemNode::unordered);\n+\n+    if (!UseObjectMonitorTable) {\n+      \/\/ Test the header to see if it is safe to read w.r.t. locking.\n+      Node *lock_mask      = _gvn.MakeConX(markWord::lock_mask_in_place);\n+      Node *lmasked_header = _gvn.transform(new AndXNode(header, lock_mask));\n+      if (LockingMode == LM_LIGHTWEIGHT) {\n+        Node *monitor_val   = _gvn.MakeConX(markWord::monitor_value);\n+        Node *chk_monitor   = _gvn.transform(new CmpXNode(lmasked_header, monitor_val));\n+        Node *test_monitor  = _gvn.transform(new BoolNode(chk_monitor, BoolTest::eq));\n+\n+        generate_slow_guard(test_monitor, slow_region);\n+      } else {\n+        Node *unlocked_val      = _gvn.MakeConX(markWord::unlocked_value);\n+        Node *chk_unlocked      = _gvn.transform(new CmpXNode(lmasked_header, unlocked_val));\n+        Node *test_not_unlocked = _gvn.transform(new BoolNode(chk_unlocked, BoolTest::ne));\n+\n+        generate_slow_guard(test_not_unlocked, slow_region);\n+      }\n@@ -4797,1 +4930,0 @@\n-  }\n@@ -4799,13 +4931,13 @@\n-  \/\/ Get the hash value and check to see that it has been properly assigned.\n-  \/\/ We depend on hash_mask being at most 32 bits and avoid the use of\n-  \/\/ hash_mask_in_place because it could be larger than 32 bits in a 64-bit\n-  \/\/ vm: see markWord.hpp.\n-  Node *hash_mask      = _gvn.intcon(markWord::hash_mask);\n-  Node *hash_shift     = _gvn.intcon(markWord::hash_shift);\n-  Node *hshifted_header= _gvn.transform(new URShiftXNode(header, hash_shift));\n-  \/\/ This hack lets the hash bits live anywhere in the mark object now, as long\n-  \/\/ as the shift drops the relevant bits into the low 32 bits.  Note that\n-  \/\/ Java spec says that HashCode is an int so there's no point in capturing\n-  \/\/ an 'X'-sized hashcode (32 in 32-bit build or 64 in 64-bit build).\n-  hshifted_header      = ConvX2I(hshifted_header);\n-  Node *hash_val       = _gvn.transform(new AndINode(hshifted_header, hash_mask));\n+    \/\/ Get the hash value and check to see that it has been properly assigned.\n+    \/\/ We depend on hash_mask being at most 32 bits and avoid the use of\n+    \/\/ hash_mask_in_place because it could be larger than 32 bits in a 64-bit\n+    \/\/ vm: see markWord.hpp.\n+    Node *hash_mask      = _gvn.intcon(markWord::hash_mask);\n+    Node *hash_shift     = _gvn.intcon(markWord::hash_shift);\n+    Node *hshifted_header= _gvn.transform(new URShiftXNode(header, hash_shift));\n+    \/\/ This hack lets the hash bits live anywhere in the mark object now, as long\n+    \/\/ as the shift drops the relevant bits into the low 32 bits.  Note that\n+    \/\/ Java spec says that HashCode is an int so there's no point in capturing\n+    \/\/ an 'X'-sized hashcode (32 in 32-bit build or 64 in 64-bit build).\n+    hshifted_header      = ConvX2I(hshifted_header);\n+    Node *hash_val       = _gvn.transform(new AndINode(hshifted_header, hash_mask));\n@@ -4813,3 +4945,3 @@\n-  Node *no_hash_val    = _gvn.intcon(markWord::no_hash);\n-  Node *chk_assigned   = _gvn.transform(new CmpINode( hash_val, no_hash_val));\n-  Node *test_assigned  = _gvn.transform(new BoolNode( chk_assigned, BoolTest::eq));\n+    Node *no_hash_val    = _gvn.intcon(markWord::no_hash);\n+    Node *chk_assigned   = _gvn.transform(new CmpINode( hash_val, no_hash_val));\n+    Node *test_assigned  = _gvn.transform(new BoolNode( chk_assigned, BoolTest::eq));\n@@ -4817,1 +4949,10 @@\n-  generate_slow_guard(test_assigned, slow_region);\n+    generate_slow_guard(test_assigned, slow_region);\n+\n+    result_val->init_req(_fast_path, hash_val);\n+\n+    \/\/ _fast_path2 is not used here.\n+    result_val->del_req(_fast_path2);\n+    result_reg->del_req(_fast_path2);\n+    result_io->del_req(_fast_path2);\n+    result_mem->del_req(_fast_path2);\n+  }\n@@ -4824,1 +4965,0 @@\n-  result_val->init_req(_fast_path, hash_val);\n@@ -4829,0 +4969,5 @@\n+  if (UseCompactObjectHeaders) {\n+    result_io->init_req(_fast_path2, i_o());\n+    result_mem->init_req(_fast_path2, init_mem);\n+  }\n+\n@@ -4830,0 +4975,1 @@\n+  assert(slow_region != nullptr, \"must have slow_region\");\n","filename":"src\/hotspot\/share\/opto\/library_call.cpp","additions":186,"deletions":40,"binary":false,"changes":226,"status":"modified"},{"patch":"@@ -1702,1 +1702,1 @@\n-    rawmem = make_store(control, rawmem, object, oopDesc::klass_offset_in_bytes(), klass_node, T_METADATA);\n+    rawmem = make_store(control, rawmem, object, Type::klass_offset(), klass_node, T_METADATA);\n@@ -2329,1 +2329,1 @@\n-      Node* k_adr = basic_plus_adr(obj_or_subklass, oopDesc::klass_offset_in_bytes());\n+      Node* k_adr = basic_plus_adr(obj_or_subklass, Type::klass_offset());\n","filename":"src\/hotspot\/share\/opto\/macro.cpp","additions":2,"deletions":2,"binary":false,"changes":4,"status":"modified"},{"patch":"@@ -265,1 +265,1 @@\n-          adr_check->offset() == oopDesc::klass_offset_in_bytes() ||\n+          adr_check->offset() == Type::klass_offset() ||\n@@ -953,1 +953,1 @@\n-           adr_type->offset() == oopDesc::klass_offset_in_bytes()),\n+           adr_type->offset() == Type::klass_offset()),\n@@ -2464,1 +2464,1 @@\n-    if (offset == oopDesc::klass_offset_in_bytes()) {\n+    if (offset == Type::klass_offset()) {\n@@ -2472,1 +2472,1 @@\n-      tary->offset() == oopDesc::klass_offset_in_bytes()) {\n+      tary->offset() == Type::klass_offset()) {\n@@ -2538,1 +2538,1 @@\n-  if (offset == oopDesc::klass_offset_in_bytes()) {\n+  if (offset == Type::klass_offset()) {\n","filename":"src\/hotspot\/share\/opto\/memnode.cpp","additions":5,"deletions":5,"binary":false,"changes":10,"status":"modified"},{"patch":"@@ -2122,1 +2122,1 @@\n-  Node* klass_addr = basic_plus_adr( receiver, receiver, oopDesc::klass_offset_in_bytes() );\n+  Node* klass_addr = basic_plus_adr( receiver, receiver, Type::klass_offset() );\n","filename":"src\/hotspot\/share\/opto\/parse1.cpp","additions":1,"deletions":1,"binary":false,"changes":2,"status":"modified"},{"patch":"@@ -1713,1 +1713,1 @@\n-  if (obj == nullptr || off != oopDesc::klass_offset_in_bytes()) \/\/ loading oopDesc::_klass?\n+  if (obj == nullptr || off != Type::klass_offset()) \/\/ loading oopDesc::_klass?\n","filename":"src\/hotspot\/share\/opto\/parse2.cpp","additions":1,"deletions":1,"binary":false,"changes":2,"status":"modified"},{"patch":"@@ -156,1 +156,1 @@\n-  int klass_offset = oopDesc::klass_offset_in_bytes();\n+  int klass_offset = Type::klass_offset();\n","filename":"src\/hotspot\/share\/opto\/parseHelper.cpp","additions":1,"deletions":1,"binary":false,"changes":2,"status":"modified"},{"patch":"@@ -427,1 +427,1 @@\n-    const size_t size = TypeArrayKlass::cast(array_type)->oop_size(result);\n+    const size_t size = TypeArrayKlass::cast(array_type)->oop_size(result, result->mark());\n","filename":"src\/hotspot\/share\/opto\/runtime.cpp","additions":1,"deletions":1,"binary":false,"changes":2,"status":"modified"},{"patch":"@@ -1181,1 +1181,1 @@\n-  if (con2 == oopDesc::klass_offset_in_bytes()) {\n+  if (con2 == Type::klass_offset()) {\n","filename":"src\/hotspot\/share\/opto\/subnode.cpp","additions":1,"deletions":1,"binary":false,"changes":2,"status":"modified"},{"patch":"@@ -96,1 +96,1 @@\n-    if (con == oopDesc::klass_offset_in_bytes() && obj != nullptr) {\n+    if (con == Type::klass_offset() && obj != nullptr) {\n@@ -220,1 +220,1 @@\n-    Node* adr = phase->transform(new AddPNode(obj_or_subklass, obj_or_subklass, phase->MakeConX(oopDesc::klass_offset_in_bytes())));\n+    Node* adr = phase->transform(new AddPNode(obj_or_subklass, obj_or_subklass, phase->MakeConX(Type::klass_offset())));\n@@ -246,1 +246,1 @@\n-#endif\n+#endif\n","filename":"src\/hotspot\/share\/opto\/subtypenode.cpp","additions":3,"deletions":3,"binary":false,"changes":6,"status":"modified"},{"patch":"@@ -578,1 +578,1 @@\n-                                           false, nullptr, oopDesc::klass_offset_in_bytes());\n+                                           false, nullptr, Type::klass_offset());\n@@ -3453,1 +3453,1 @@\n-    if (_offset == oopDesc::klass_offset_in_bytes()) {\n+    if (_offset == Type::klass_offset()) {\n","filename":"src\/hotspot\/share\/opto\/type.cpp","additions":2,"deletions":2,"binary":false,"changes":4,"status":"modified"},{"patch":"@@ -203,0 +203,11 @@\n+  \/\/ This is used as a marker to identify narrow Klass* loads, which\n+  \/\/ are really extracted from the mark-word, but we still want to\n+  \/\/ distinguish it.\n+  static int klass_offset() {\n+    if (UseCompactObjectHeaders) {\n+      return 1;\n+    } else {\n+      return oopDesc::klass_offset_in_bytes();\n+    }\n+  }\n+\n@@ -1622,1 +1633,1 @@\n-        _offset != arrayOopDesc::klass_offset_in_bytes()) {\n+        _offset != Type::klass_offset()) {\n","filename":"src\/hotspot\/share\/opto\/type.hpp","additions":12,"deletions":1,"binary":false,"changes":13,"status":"modified"},{"patch":"@@ -3768,0 +3768,3 @@\n+  if (UseCompactObjectHeaders && FLAG_IS_DEFAULT(hashCode)) {\n+    hashCode = 6;\n+  }\n","filename":"src\/hotspot\/share\/runtime\/arguments.cpp","additions":3,"deletions":0,"binary":false,"changes":3,"status":"modified"},{"patch":"@@ -1404,1 +1404,1 @@\n-  product(size_t, CompressedClassSpaceSize, 1*G,                            \\\n+  product(size_t, CompressedClassSpaceSize, 128*M,                          \\\n","filename":"src\/hotspot\/share\/runtime\/globals.hpp","additions":1,"deletions":1,"binary":false,"changes":2,"status":"modified"},{"patch":"@@ -51,0 +51,12 @@\n+static uintx objhash(oop obj) {\n+  if (UseCompactObjectHeaders) {\n+    uintx hash = LightweightSynchronizer::get_hash(obj->mark(), obj);\n+    assert(hash != 0, \"should have a hash\");\n+    return hash;\n+  } else {\n+    uintx hash = obj->mark().hash();\n+    assert(hash != 0, \"should have a hash\");\n+    return hash;\n+  }\n+}\n+\n@@ -81,3 +93,1 @@\n-      uintx hash = _obj->mark().hash();\n-      assert(hash != 0, \"should have a hash\");\n-      return hash;\n+      return objhash(_obj);\n@@ -286,0 +296,1 @@\n+      assert(objhash(obj) == (uintx)(*found)->hash(), \"hash must match\");\n@@ -318,1 +329,1 @@\n-       assert(obj->mark().hash() == om->hash(), \"hash must match\");\n+       assert(objhash(obj) == (uintx)om->hash(), \"hash must match\");\n@@ -408,1 +419,1 @@\n-  intptr_t hash = obj->mark().hash();\n+  intptr_t hash = objhash(obj);\n@@ -1244,0 +1255,18 @@\n+\n+uint32_t LightweightSynchronizer::get_hash(markWord mark, oop obj, Klass* klass) {\n+  assert(UseCompactObjectHeaders, \"Only with compact i-hash\");\n+  \/\/assert(mark.is_neutral() | mark.is_fast_locked(), \"only from neutral or fast-locked mark: \" INTPTR_FORMAT, mark.value());\n+  assert(mark.is_hashed(), \"only from hashed or copied object\");\n+  if (mark.is_hashed_expanded()) {\n+    return obj->int_field(klass->hash_offset_in_bytes(obj, mark));\n+  } else {\n+    assert(mark.is_hashed_not_expanded(), \"must be hashed\");\n+    assert(hashCode == 6 || hashCode == 2, \"must have idempotent hashCode\");\n+    \/\/ Already marked as hashed, but not yet copied. Recompute hash and return it.\n+    return ObjectSynchronizer::get_next_hash(nullptr, obj); \/\/ recompute hash\n+  }\n+}\n+\n+uint32_t LightweightSynchronizer::get_hash(markWord mark, oop obj) {\n+  return get_hash(mark, obj, mark.klass());\n+}\n","filename":"src\/hotspot\/share\/runtime\/lightweightSynchronizer.cpp","additions":34,"deletions":5,"binary":false,"changes":39,"status":"modified"},{"patch":"@@ -78,0 +78,5 @@\n+\n+  \/\/ NOTE: May not cause monitor inflation\n+  static uint32_t get_hash(markWord mark, oop obj);\n+  \/\/ For CDS path.\n+  static uint32_t get_hash(markWord mark, oop obj, Klass* klass);\n","filename":"src\/hotspot\/share\/runtime\/lightweightSynchronizer.hpp","additions":5,"deletions":0,"binary":false,"changes":5,"status":"modified"},{"patch":"@@ -66,0 +66,1 @@\n+#include \"utilities\/fastHash.hpp\"\n@@ -928,1 +929,1 @@\n-static intptr_t get_next_hash(Thread* current, oop obj) {\n+intptr_t ObjectSynchronizer::get_next_hash(Thread* current, oop obj) {\n@@ -947,1 +948,1 @@\n-  } else {\n+  } else if (hashCode == 5) {\n@@ -960,0 +961,10 @@\n+  } else {\n+    assert(UseCompactObjectHeaders, \"Only with compact i-hash\");\n+#ifdef _LP64\n+    uint64_t val = cast_from_oop<uint64_t>(obj);\n+    uint32_t hash = FastHash::get_hash32((uint32_t)val, (uint32_t)(val >> 32));\n+#else\n+    uint32_t val = cast_from_oop<uint32_t>(obj);\n+    uint32_t hash = FastHash::get_hash32(val, UCONST64(0xAAAAAAAA));\n+#endif\n+    value= static_cast<intptr_t>(hash);\n@@ -963,2 +974,2 @@\n-  if (value == 0) value = 0xBAD;\n-  assert(value != markWord::no_hash, \"invariant\");\n+  if (hashCode != 6 && value == 0) value = 0xBAD;\n+  assert(value != markWord::no_hash || hashCode == 6, \"invariant\");\n@@ -973,4 +984,23 @@\n-    intptr_t hash = mark.hash();\n-    if (hash != 0) {\n-      return hash;\n-    }\n+    if (UseCompactObjectHeaders) {\n+      if (mark.is_hashed()) {\n+        return LightweightSynchronizer::get_hash(mark, obj);\n+      }\n+      intptr_t hash = ObjectSynchronizer::get_next_hash(current, obj);  \/\/ get a new hash\n+      markWord new_mark;\n+      if (mark.is_not_hashed_expanded()) {\n+        new_mark = mark.set_hashed_expanded();\n+        int offset = mark.klass()->hash_offset_in_bytes(obj, mark);\n+        obj->int_field_put(offset, (jint) hash);\n+      } else {\n+        new_mark = mark.set_hashed_not_expanded();\n+      }\n+      markWord old_mark = obj->cas_set_mark(new_mark, mark);\n+      if (old_mark == mark) {\n+        return hash;\n+      }\n+      mark = old_mark;\n+    } else {\n+      intptr_t hash = mark.hash();\n+      if (hash != 0) {\n+        return hash;\n+      }\n@@ -978,3 +1008,3 @@\n-    hash = get_next_hash(current, obj);\n-    const markWord old_mark = mark;\n-    const markWord new_mark = old_mark.copy_set_hash(hash);\n+      hash = ObjectSynchronizer::get_next_hash(current, obj);\n+      const markWord old_mark = mark;\n+      const markWord new_mark = old_mark.copy_set_hash(hash);\n@@ -982,3 +1012,4 @@\n-    mark = obj->cas_set_mark(new_mark, old_mark);\n-    if (old_mark == mark) {\n-      return hash;\n+      mark = obj->cas_set_mark(new_mark, old_mark);\n+      if (old_mark == mark) {\n+        return hash;\n+      }\n","filename":"src\/hotspot\/share\/runtime\/synchronizer.cpp","additions":45,"deletions":14,"binary":false,"changes":59,"status":"modified"},{"patch":"@@ -215,0 +215,2 @@\n+  static intptr_t get_next_hash(Thread* current, oop obj);\n+\n","filename":"src\/hotspot\/share\/runtime\/synchronizer.hpp","additions":2,"deletions":0,"binary":false,"changes":2,"status":"modified"},{"patch":"@@ -212,0 +212,1 @@\n+  nonstatic_field(InstanceKlass,               _hash_offset,                                  int)                                    \\\n@@ -1822,0 +1823,1 @@\n+  declare_constant(markWord::hashctrl_bits)                               \\\n@@ -1826,0 +1828,1 @@\n+  declare_constant(markWord::hashctrl_shift)                              \\\n@@ -1834,0 +1837,4 @@\n+  declare_constant(markWord::hashctrl_mask)                               \\\n+  declare_constant(markWord::hashctrl_mask_in_place)                      \\\n+  declare_constant(markWord::hashctrl_hashed_mask_in_place)               \\\n+  declare_constant(markWord::hashctrl_expanded_mask_in_place)             \\\n","filename":"src\/hotspot\/share\/runtime\/vmStructs.cpp","additions":7,"deletions":0,"binary":false,"changes":7,"status":"modified"},{"patch":"@@ -0,0 +1,102 @@\n+\/*\n+ * Copyright (c) 2023, Oracle and\/or its affiliates. All rights reserved.\n+ * DO NOT ALTER OR REMOVE COPYRIGHT NOTICES OR THIS FILE HEADER.\n+ *\n+ * This code is free software; you can redistribute it and\/or modify it\n+ * under the terms of the GNU General Public License version 2 only, as\n+ * published by the Free Software Foundation.\n+ *\n+ * This code is distributed in the hope that it will be useful, but WITHOUT\n+ * ANY WARRANTY; without even the implied warranty of MERCHANTABILITY or\n+ * FITNESS FOR A PARTICULAR PURPOSE.  See the GNU General Public License\n+ * version 2 for more details (a copy is included in the LICENSE file that\n+ * accompanied this code).\n+ *\n+ * You should have received a copy of the GNU General Public License version\n+ * 2 along with this work; if not, write to the Free Software Foundation,\n+ * Inc., 51 Franklin St, Fifth Floor, Boston, MA 02110-1301 USA.\n+ *\n+ * Please contact Oracle, 500 Oracle Parkway, Redwood Shores, CA 94065 USA\n+ * or visit www.oracle.com if you need additional information or have any\n+ * questions.\n+ *\n+ *\/\n+\n+#ifndef SHARE_UTILITIES_FASTHASH_HPP\n+#define SHARE_UTILITIES_FASTHASH_HPP\n+\n+#include \"memory\/allStatic.hpp\"\n+\n+class FastHash : public AllStatic {\n+private:\n+  static void fullmul64(uint64_t& hi, uint64_t& lo, uint64_t op1, uint64_t op2) {\n+#if defined(__SIZEOF_INT128__)\n+    __uint128_t prod = static_cast<__uint128_t>(op1) * static_cast<__uint128_t>(op2);\n+    hi = static_cast<uint64_t>(prod >> 64);\n+    lo = static_cast<uint64_t>(prod >>  0);\n+#else\n+    \/* First calculate all of the cross products. *\/\n+    uint64_t lo_lo = (op1 & 0xFFFFFFFF) * (op2 & 0xFFFFFFFF);\n+    uint64_t hi_lo = (op1 >> 32)        * (op2 & 0xFFFFFFFF);\n+    uint64_t lo_hi = (op1 & 0xFFFFFFFF) * (op2 >> 32);\n+    uint64_t hi_hi = (op1 >> 32)        * (op2 >> 32);\n+\n+    \/* Now add the products together. These will never overflow. *\/\n+    uint64_t cross = (lo_lo >> 32) + (hi_lo & 0xFFFFFFFF) + lo_hi;\n+    uint64_t upper = (hi_lo >> 32) + (cross >> 32)        + hi_hi;\n+    hi = upper;\n+    lo = (cross << 32) | (lo_lo & 0xFFFFFFFF);\n+#endif\n+  }\n+\n+  static void fullmul32(uint32_t& hi, uint32_t& lo, uint32_t op1, uint32_t op2) {\n+    uint64_t x64 = op1, y64 = op2, xy64 = x64 * y64;\n+    hi = (uint32_t)(xy64 >> 32);\n+    lo = (uint32_t)(xy64 >>  0);\n+  }\n+\n+  static uint64_t ror64(uint64_t x, uint64_t distance) {\n+    distance = distance & (64 - 1);\n+    return (x >> distance) | (x << (64 - distance));\n+  }\n+\n+  static uint32_t ror32(uint32_t x, uint32_t distance) {\n+    distance = distance & (32 - 1);\n+    return (x >> distance) | (x << (32 - distance));\n+  }\n+\n+public:\n+  static uint64_t get_hash64(uint64_t x, uint64_t y) {\n+    const uint64_t M  = 0x8ADAE89C337954D5;\n+    const uint64_t A  = 0xAAAAAAAAAAAAAAAA; \/\/ REPAA\n+    const uint64_t H0 = (x ^ y), L0 = (x ^ A);\n+\n+    uint64_t U0, V0; fullmul64(U0, V0, L0, M);\n+    const uint64_t Q0 = (H0 * M);\n+    const uint64_t L1 = (Q0 ^ U0);\n+\n+    uint64_t U1, V1; fullmul64(U1, V1, L1, M);\n+    const uint64_t P1 = (V0 ^ M);\n+    const uint64_t Q1 = ror64(P1, L1);\n+    const uint64_t L2 = (Q1 ^ U1);\n+    return V1 ^ L2;\n+  }\n+\n+  static uint32_t get_hash32(uint32_t x, uint32_t y) {\n+    const uint32_t M  = 0x337954D5;\n+    const uint32_t A  = 0xAAAAAAAA; \/\/ REPAA\n+    const uint32_t H0 = (x ^ y), L0 = (x ^ A);\n+\n+    uint32_t U0, V0; fullmul32(U0, V0, L0, M);\n+    const uint32_t Q0 = (H0 * M);\n+    const uint32_t L1 = (Q0 ^ U0);\n+\n+    uint32_t U1, V1; fullmul32(U1, V1, L1, M);\n+    const uint32_t P1 = (V0 ^ M);\n+    const uint32_t Q1 = ror32(P1, L1);\n+    const uint32_t L2 = (Q1 ^ U1);\n+    return V1 ^ L2;\n+  }\n+};\n+\n+#endif\/\/ SHARE_UTILITIES_FASTHASH_HPP\n","filename":"src\/hotspot\/share\/utilities\/fastHash.hpp","additions":102,"deletions":0,"binary":false,"changes":102,"status":"added"},{"patch":"@@ -404,1 +404,1 @@\n-      value = readCInteger(address, machDesc.getAddressSize(), true);\n+      value = readCInteger(address, jintSize, true);\n","filename":"src\/jdk.hotspot.agent\/share\/classes\/sun\/jvm\/hotspot\/debugger\/DebuggerBase.java","additions":1,"deletions":1,"binary":false,"changes":2,"status":"modified"},{"patch":"@@ -107,0 +107,7 @@\n+    if (VM.getVM().isCompactObjectHeadersEnabled()) {\n+      Mark mark = getMark();\n+      if (mark.isExpanded()) {\n+        \/\/ Needs extra 4 bytes for identity hash-code.\n+        s += 4;\n+      }\n+    }\n","filename":"src\/jdk.hotspot.agent\/share\/classes\/sun\/jvm\/hotspot\/oops\/Array.java","additions":7,"deletions":0,"binary":false,"changes":7,"status":"modified"},{"patch":"@@ -87,0 +87,1 @@\n+    hashOffset           = new CIntField(type.getCIntegerField(\"_hash_offset\"), 0);\n@@ -153,0 +154,1 @@\n+  private static CIntField hashOffset;\n@@ -243,1 +245,9 @@\n-    return getSizeHelper() * VM.getVM().getAddressSize();\n+    long baseSize = getSizeHelper() * VM.getVM().getAddressSize();\n+    if (VM.getVM().isCompactObjectHeadersEnabled()) {\n+      Mark mark = object.getMark();\n+      if (mark.isExpanded() && (getHashOffset() + 4 \/* size of hash field *\/) > baseSize) {\n+        \/\/ Needs extra word for identity hash-code.\n+        return baseSize + VM.getVM().getBytesPerWord();\n+      }\n+    }\n+    return baseSize;\n@@ -377,0 +387,1 @@\n+  public long      getHashOffset()          { return                hashOffset.getValue(this); }\n","filename":"src\/jdk.hotspot.agent\/share\/classes\/sun\/jvm\/hotspot\/oops\/InstanceKlass.java","additions":12,"deletions":1,"binary":false,"changes":13,"status":"modified"},{"patch":"@@ -58,1 +58,9 @@\n-    return java_lang_Class.getOopSize(o) * VM.getVM().getAddressSize();\n+    long s = java_lang_Class.getOopSize(o) * VM.getVM().getAddressSize();\n+    if (VM.getVM().isCompactObjectHeadersEnabled()) {\n+      Mark mark = o.getMark();\n+      if (mark.isExpanded()) {\n+        \/\/ Needs extra 4 bytes for identity hash-code (and align-up to whole word).\n+        s += VM.getVM().getAddressSize();\n+      }\n+    }\n+    return s;\n","filename":"src\/jdk.hotspot.agent\/share\/classes\/sun\/jvm\/hotspot\/oops\/InstanceMirrorKlass.java","additions":9,"deletions":1,"binary":false,"changes":10,"status":"modified"},{"patch":"@@ -54,0 +54,1 @@\n+    hashCtrlBits        = db.lookupLongConstant(\"markWord::hashctrl_bits\").longValue();\n@@ -57,0 +58,1 @@\n+    hashCtrlShift       = db.lookupLongConstant(\"markWord::hashctrl_shift\").longValue();\n@@ -66,0 +68,4 @@\n+    hashCtrlMask        = db.lookupLongConstant(\"markWord::hashctrl_mask\").longValue();\n+    hashCtrlMaskInPlace = db.lookupLongConstant(\"markWord::hashctrl_mask_in_place\").longValue();\n+    hashCtrlHashedMaskInPlace =   db.lookupLongConstant(\"markWord::hashctrl_hashed_mask_in_place\").longValue();\n+    hashCtrlExpandedMaskInPlace = db.lookupLongConstant(\"markWord::hashctrl_expanded_mask_in_place\").longValue();\n@@ -84,0 +90,1 @@\n+  private static long hashCtrlBits;\n@@ -88,0 +95,1 @@\n+  private static long hashCtrlShift;\n@@ -96,0 +104,4 @@\n+  private static long hashCtrlMask;\n+  private static long hashCtrlMaskInPlace;\n+  private static long hashCtrlHashedMaskInPlace;\n+  private static long hashCtrlExpandedMaskInPlace;\n@@ -195,1 +207,6 @@\n-    return Bits.maskBitsLong(value() >> hashShift, hashMask);\n+    if (VM.getVM().isCompactObjectHeadersEnabled()) {\n+      System.exit(-23);\n+      throw new RuntimeException(\"Compact I-Hash not yet implemented\");\n+    } else {\n+      return Bits.maskBitsLong(value() >> hashShift, hashMask);\n+    }\n@@ -202,0 +219,5 @@\n+  public boolean isExpanded() {\n+    assert(VM.getVM().isCompactObjectHeadersEnabled());\n+    return Bits.maskBitsLong(value(), hashCtrlExpandedMaskInPlace) != 0;\n+  }\n+\n","filename":"src\/jdk.hotspot.agent\/share\/classes\/sun\/jvm\/hotspot\/oops\/Mark.java","additions":23,"deletions":1,"binary":false,"changes":24,"status":"modified"},{"patch":"@@ -50,2 +50,1 @@\n-      Type markType = db.lookupType(\"markWord\");\n-      headerSize = markType.getSize();\n+      headerSize = 4;\n@@ -135,0 +134,4 @@\n+    if (VM.getVM().isCompactObjectHeadersEnabled()) {\n+      System.exit(-23);\n+        throw new InternalError(\"Not yet implemented\");\n+    }\n","filename":"src\/jdk.hotspot.agent\/share\/classes\/sun\/jvm\/hotspot\/oops\/Oop.java","additions":5,"deletions":2,"binary":false,"changes":7,"status":"modified"},{"patch":"@@ -30,1 +30,1 @@\n-static markWord changedMark()  { return markWord(0x4711); }\n+static markWord changedMark()  { return markWord(0x4712); }\n@@ -58,0 +58,2 @@\n+  FullGCForwarding::begin();\n+\n@@ -76,0 +78,2 @@\n+\n+  FullGCForwarding::end();\n","filename":"test\/hotspot\/gtest\/gc\/shared\/test_preservedMarks.cpp","additions":5,"deletions":1,"binary":false,"changes":6,"status":"modified"},{"patch":"@@ -85,8 +85,8 @@\n-    EXPECT_EQ(arrayOopDesc::base_offset_in_bytes(T_BOOLEAN), 12);\n-    EXPECT_EQ(arrayOopDesc::base_offset_in_bytes(T_BYTE),    12);\n-    EXPECT_EQ(arrayOopDesc::base_offset_in_bytes(T_SHORT),   12);\n-    EXPECT_EQ(arrayOopDesc::base_offset_in_bytes(T_CHAR),    12);\n-    EXPECT_EQ(arrayOopDesc::base_offset_in_bytes(T_INT),     12);\n-    EXPECT_EQ(arrayOopDesc::base_offset_in_bytes(T_FLOAT),   12);\n-    EXPECT_EQ(arrayOopDesc::base_offset_in_bytes(T_LONG),    16);\n-    EXPECT_EQ(arrayOopDesc::base_offset_in_bytes(T_DOUBLE),  16);\n+    EXPECT_EQ(arrayOopDesc::base_offset_in_bytes(T_BOOLEAN), 8);\n+    EXPECT_EQ(arrayOopDesc::base_offset_in_bytes(T_BYTE),    8);\n+    EXPECT_EQ(arrayOopDesc::base_offset_in_bytes(T_SHORT),   8);\n+    EXPECT_EQ(arrayOopDesc::base_offset_in_bytes(T_CHAR),    8);\n+    EXPECT_EQ(arrayOopDesc::base_offset_in_bytes(T_INT),     8);\n+    EXPECT_EQ(arrayOopDesc::base_offset_in_bytes(T_FLOAT),   8);\n+    EXPECT_EQ(arrayOopDesc::base_offset_in_bytes(T_LONG),    8);\n+    EXPECT_EQ(arrayOopDesc::base_offset_in_bytes(T_DOUBLE),  8);\n@@ -94,2 +94,2 @@\n-      EXPECT_EQ(arrayOopDesc::base_offset_in_bytes(T_OBJECT), 12);\n-      EXPECT_EQ(arrayOopDesc::base_offset_in_bytes(T_ARRAY),  12);\n+      EXPECT_EQ(arrayOopDesc::base_offset_in_bytes(T_OBJECT), 8);\n+      EXPECT_EQ(arrayOopDesc::base_offset_in_bytes(T_ARRAY),  8);\n@@ -97,2 +97,2 @@\n-      EXPECT_EQ(arrayOopDesc::base_offset_in_bytes(T_OBJECT), 16);\n-      EXPECT_EQ(arrayOopDesc::base_offset_in_bytes(T_ARRAY),  16);\n+      EXPECT_EQ(arrayOopDesc::base_offset_in_bytes(T_OBJECT), 8);\n+      EXPECT_EQ(arrayOopDesc::base_offset_in_bytes(T_ARRAY),  8);\n","filename":"test\/hotspot\/gtest\/oops\/test_arrayOop.cpp","additions":12,"deletions":12,"binary":false,"changes":24,"status":"modified"},{"patch":"@@ -94,1 +94,5 @@\n-  assert_test_pattern(h_obj, \"is_unlocked hash=0x\");\n+  if (UseCompactObjectHeaders) {\n+    assert_test_pattern(h_obj, \"is_unlocked hash is-hashed=true is-copied=false\");\n+  } else {\n+    assert_test_pattern(h_obj, \"is_unlocked hash=0x\");\n+  }\n","filename":"test\/hotspot\/gtest\/oops\/test_markWord.cpp","additions":5,"deletions":1,"binary":false,"changes":6,"status":"modified"},{"patch":"@@ -79,1 +79,0 @@\n-        applyIfOr = {\"UseCompactObjectHeaders\", \"false\", \"AlignVector\", \"false\"},\n@@ -87,5 +86,0 @@\n-            \/\/ With AlignVector, we need 8-byte alignment of vector loads\/stores.\n-            \/\/ UseCompactObjectHeaders=false                  UseCompactObjectHeaders=true\n-            \/\/ I_adr = base + 16 + 4*i  ->  i % 2 = 0         B_adr = base + 12 + 4*i  ->  i % 2 = 1\n-            \/\/ N_adr = base      + 4*i  ->  i % 2 = 0         N_adr = base      + 4*i  ->  i % 2 = 0\n-            \/\/ -> vectorize                                   -> no vectorization\n","filename":"test\/hotspot\/jtreg\/compiler\/c2\/TestCastX2NotProcessedIGVN.java","additions":0,"deletions":6,"binary":false,"changes":6,"status":"modified"},{"patch":"@@ -411,1 +411,0 @@\n-        applyIfOr = {\"UseCompactObjectHeaders\", \"false\", \"AlignVector\", \"false\"},\n@@ -417,4 +416,0 @@\n-            \/\/ With AlignVector, we need 8-byte alignment of vector loads\/stores.\n-            \/\/ UseCompactObjectHeaders=false                        UseCompactObjectHeaders=true\n-            \/\/ adr = base + 16 + 8*i      ->  always                adr = base + 12 + 8*i      ->  never\n-            \/\/ -> vectorize                                         -> no vectorization\n@@ -429,1 +424,0 @@\n-        applyIfOr = {\"UseCompactObjectHeaders\", \"false\", \"AlignVector\", \"false\"},\n@@ -435,4 +429,0 @@\n-            \/\/ With AlignVector, we need 8-byte alignment of vector loads\/stores.\n-            \/\/ UseCompactObjectHeaders=false                        UseCompactObjectHeaders=true\n-            \/\/ adr = base + 16 + 8*i      ->  always                adr = base + 12 + 8*i      ->  never\n-            \/\/ -> vectorize                                         -> no vectorization\n","filename":"test\/hotspot\/jtreg\/compiler\/c2\/irTests\/TestVectorConditionalMove.java","additions":0,"deletions":10,"binary":false,"changes":10,"status":"modified"},{"patch":"@@ -168,1 +168,0 @@\n-        applyIfOr = {\"UseCompactObjectHeaders\", \"false\", \"AlignVector\", \"false\"},\n@@ -176,5 +175,0 @@\n-            \/\/ With AlignVector, we need 8-byte alignment of vector loads\/stores.\n-            \/\/ UseCompactObjectHeaders=false                  UseCompactObjectHeaders=true\n-            \/\/ B_adr = base + 16 + 8*i  ->  always            B_adr = base + 12 + 8*i  ->  never\n-            \/\/ L_adr = base + 16 + 8*i  ->  always            L_adr = base + 16 + 8*i  ->  always\n-            \/\/ -> vectorize                                   -> no vectorization\n@@ -186,1 +180,0 @@\n-        applyIfOr = {\"UseCompactObjectHeaders\", \"false\", \"AlignVector\", \"false\"},\n@@ -193,5 +186,0 @@\n-            \/\/ With AlignVector, we need 8-byte alignment of vector loads\/stores.\n-            \/\/ UseCompactObjectHeaders=false                  UseCompactObjectHeaders=true\n-            \/\/ B_adr = base + 16 + 8*i  ->  always            B_adr = base + 12 + 8*i  ->  never\n-            \/\/ L_adr = base + 16 + 8*i  ->  always            L_adr = base + 16 + 8*i  ->  always\n-            \/\/ -> vectorize                                   -> no vectorization\n@@ -208,5 +196,0 @@\n-            \/\/ With AlignVector, we need 8-byte alignment of vector loads\/stores.\n-            \/\/ UseCompactObjectHeaders=false                  UseCompactObjectHeaders=true\n-            \/\/ B_adr = base + 64 + 8*i  ->  always            B_adr = base + 64 + 8*i  ->  always\n-            \/\/ L_adr = base + 16 + 8*i  ->  always            L_adr = base + 16 + 8*i  ->  always\n-            \/\/ -> vectorize                                   -> vectorize\n@@ -225,5 +208,0 @@\n-            \/\/ With AlignVector, we need 8-byte alignment of vector loads\/stores.\n-            \/\/ UseCompactObjectHeaders=false                  UseCompactObjectHeaders=true\n-            \/\/ B_adr = base + 64 + 8*i  ->  always            B_adr = base + 64 + 8*i  ->  always\n-            \/\/ L_adr = base + 16 + 8*i  ->  always            L_adr = base + 16 + 8*i  ->  always\n-            \/\/ -> vectorize                                   -> vectorize\n@@ -243,1 +221,0 @@\n-        applyIfOr = {\"UseCompactObjectHeaders\", \"false\", \"AlignVector\", \"false\"},\n@@ -251,5 +228,0 @@\n-            \/\/ With AlignVector, we need 8-byte alignment of vector loads\/stores.\n-            \/\/ UseCompactObjectHeaders=false                      UseCompactObjectHeaders=true\n-            \/\/ B_adr = base + 16 + 8*(i-1)  ->  always            B_adr = base + 12 + 8*(i-1)  ->  never\n-            \/\/ L_adr = base + 16 + 8*i      ->  always            L_adr = base + 16 + 8*i      ->  always\n-            \/\/ -> vectorize                                       -> no vectorization\n@@ -261,1 +233,0 @@\n-        applyIfOr = {\"UseCompactObjectHeaders\", \"false\", \"AlignVector\", \"false\"},\n@@ -268,5 +239,0 @@\n-            \/\/ With AlignVector, we need 8-byte alignment of vector loads\/stores.\n-            \/\/ UseCompactObjectHeaders=false                      UseCompactObjectHeaders=true\n-            \/\/ B_adr = base + 16 + 8*(i-1)  ->  always            B_adr = base + 12 + 8*(i-1)  ->  never\n-            \/\/ L_adr = base + 16 + 8*i      ->  always            L_adr = base + 16 + 8*i      ->  always\n-            \/\/ -> vectorize                                       -> no vectorization\n@@ -284,1 +250,0 @@\n-        applyIfOr = {\"UseCompactObjectHeaders\", \"false\", \"AlignVector\", \"false\"},\n@@ -292,5 +257,0 @@\n-            \/\/ With AlignVector, we need 8-byte alignment of vector loads\/stores.\n-            \/\/ UseCompactObjectHeaders=false                      UseCompactObjectHeaders=true\n-            \/\/ B_adr = base + 16 + 8*(i+1)  ->  always            B_adr = base + 12 + 8*(i+1)  ->  never\n-            \/\/ L_adr = base + 16 + 8*i      ->  always            L_adr = base + 16 + 8*i      ->  always\n-            \/\/ -> vectorize                                       -> no vectorization\n@@ -302,1 +262,0 @@\n-        applyIfOr = {\"UseCompactObjectHeaders\", \"false\", \"AlignVector\", \"false\"},\n@@ -309,5 +268,0 @@\n-            \/\/ With AlignVector, we need 8-byte alignment of vector loads\/stores.\n-            \/\/ UseCompactObjectHeaders=false                      UseCompactObjectHeaders=true\n-            \/\/ B_adr = base + 16 + 8*(i+1)  ->  always            B_adr = base + 12 + 8*(i+1)  ->  never\n-            \/\/ L_adr = base + 16 + 8*i      ->  always            L_adr = base + 16 + 8*i      ->  always\n-            \/\/ -> vectorize                                       -> no vectorization\n@@ -359,1 +313,0 @@\n-        applyIfOr = {\"UseCompactObjectHeaders\", \"false\", \"AlignVector\", \"false\"},\n@@ -367,5 +320,0 @@\n-            \/\/ With AlignVector, we need 8-byte alignment of vector loads\/stores.\n-            \/\/ UseCompactObjectHeaders=false                      UseCompactObjectHeaders=true\n-            \/\/ B_adr = base + 16 + 8*(i+x)  ->  always            B_adr = base + 12 + 8*(i+x)  ->  never\n-            \/\/ L_adr = base + 16 + 8*i      ->  always            L_adr = base + 16 + 8*i      ->  always\n-            \/\/ -> vectorize                                       -> no vectorization\n@@ -377,1 +325,0 @@\n-        applyIfOr = {\"UseCompactObjectHeaders\", \"false\", \"AlignVector\", \"false\"},\n@@ -384,5 +331,0 @@\n-            \/\/ With AlignVector, we need 8-byte alignment of vector loads\/stores.\n-            \/\/ UseCompactObjectHeaders=false                      UseCompactObjectHeaders=true\n-            \/\/ B_adr = base + 16 + 8*(i+x)  ->  always            B_adr = base + 12 + 8*(i+x)  ->  never\n-            \/\/ L_adr = base + 16 + 8*i      ->  always            L_adr = base + 16 + 8*i      ->  always\n-            \/\/ -> vectorize                                       -> no vectorization\n@@ -401,1 +343,0 @@\n-        applyIfOr = {\"UseCompactObjectHeaders\", \"false\", \"AlignVector\", \"false\"},\n@@ -409,5 +350,0 @@\n-            \/\/ With AlignVector, we need 8-byte alignment of vector loads\/stores.\n-            \/\/ UseCompactObjectHeaders=false                    UseCompactObjectHeaders=true\n-            \/\/ src_adr = base + 16 + 8*i  ->  always            src_adr = base + 12 + 8*i  ->  never\n-            \/\/ dst_adr = base + 16 + 8*i  ->  always            dst_adr = base + 12 + 8*i  ->  never\n-            \/\/ -> vectorize                                     -> no vectorization\n@@ -419,1 +355,0 @@\n-        applyIfOr = {\"UseCompactObjectHeaders\", \"false\", \"AlignVector\", \"false\"},\n@@ -426,5 +361,0 @@\n-            \/\/ With AlignVector, we need 8-byte alignment of vector loads\/stores.\n-            \/\/ UseCompactObjectHeaders=false                    UseCompactObjectHeaders=true\n-            \/\/ src_adr = base + 16 + 8*i  ->  always            src_adr = base + 12 + 8*i  ->  never\n-            \/\/ dst_adr = base + 16 + 8*i  ->  always            dst_adr = base + 12 + 8*i  ->  never\n-            \/\/ -> vectorize                                     -> no vectorization\n@@ -442,1 +372,0 @@\n-        applyIfOr = {\"UseCompactObjectHeaders\", \"false\", \"AlignVector\", \"false\"},\n@@ -450,5 +379,0 @@\n-            \/\/ With AlignVector, we need 8-byte alignment of vector loads\/stores.\n-            \/\/ UseCompactObjectHeaders=false                        UseCompactObjectHeaders=true\n-            \/\/ src_adr = base + 16 + 8*i      ->  always            src_adr = base + 12 + 8*i      ->  never\n-            \/\/ dst_adr = base + 16 + 8*(i-1)  ->  always            dst_adr = base + 12 + 8*(i-1)  ->  never\n-            \/\/ -> vectorize                                         -> no vectorization\n@@ -460,1 +384,0 @@\n-        applyIfOr = {\"UseCompactObjectHeaders\", \"false\", \"AlignVector\", \"false\"},\n@@ -467,5 +390,0 @@\n-            \/\/ With AlignVector, we need 8-byte alignment of vector loads\/stores.\n-            \/\/ UseCompactObjectHeaders=false                        UseCompactObjectHeaders=true\n-            \/\/ src_adr = base + 16 + 8*i      ->  always            src_adr = base + 12 + 8*i      ->  never\n-            \/\/ dst_adr = base + 16 + 8*(i-1)  ->  always            dst_adr = base + 12 + 8*(i-1)  ->  never\n-            \/\/ -> vectorize                                         -> no vectorization\n","filename":"test\/hotspot\/jtreg\/compiler\/c2\/irTests\/TestVectorizationMismatchedAccess.java","additions":0,"deletions":82,"binary":false,"changes":82,"status":"modified"},{"patch":"@@ -66,1 +66,0 @@\n-        applyIfOr = { \"UseCompactObjectHeaders\", \"false\", \"AlignVector\", \"false\" },\n@@ -75,4 +74,0 @@\n-            \/\/ For UseCompactObjectHeaders and AlignVector, we must 8-byte align all vector loads\/stores.\n-            \/\/ But the long-stores to the byte-array are never aligned:\n-            \/\/ adr = base + UNSAFE.ARRAY_BYTE_BASE_OFFSET + 8*iter\n-            \/\/              = 16 (or 12 if UseCompactObjectHeaders=true)\n","filename":"test\/hotspot\/jtreg\/compiler\/c2\/irTests\/TestVectorizationNotRun.java","additions":0,"deletions":5,"binary":false,"changes":5,"status":"modified"},{"patch":"@@ -64,2 +64,1 @@\n-    @IR(applyIf = {\"UseCompactObjectHeaders\", \"false\"},\n-        counts = {IRNode.CALL_OF, \"arrayof_jshort_disjoint_arraycopy\", \">0\"})\n+    @IR(counts = {IRNode.CALL_OF, \"arrayof_jshort_disjoint_arraycopy\", \">0\"})\n@@ -74,12 +73,1 @@\n-    @IR(applyIf = {\"UseCompactObjectHeaders\", \"true\"},\n-        counts = {IRNode.CALL_OF, \"arrayof_jshort_disjoint_arraycopy\", \"0\"})\n-    static void testSBToStringUnAligned() {\n-        \/\/ Exercise the StringBuilder.toString API\n-        StringBuilder sb = new StringBuilder(input_strU);\n-        output_strU = sb.append(input_strU).toString();\n-    }\n-\n-    @Test\n-    @Warmup(10000)\n-    @IR(applyIf = {\"UseCompactObjectHeaders\", \"false\"},\n-        counts = {IRNode.CALL_OF, \"arrayof_jshort_disjoint_arraycopy\", \">0\"})\n+    @IR(counts = {IRNode.CALL_OF, \"arrayof_jshort_disjoint_arraycopy\", \">0\"})\n@@ -93,11 +81,1 @@\n-    @IR(applyIf = {\"UseCompactObjectHeaders\", \"true\"},\n-        counts = {IRNode.CALL_OF, \"arrayof_jshort_disjoint_arraycopy\", \"0\"})\n-    static void testStrUGetCharsUnAligned() {\n-        \/\/ Exercise the StringUTF16.getChars API\n-        output_arrU = input_strU.toCharArray();\n-    }\n-\n-    @Test\n-    @Warmup(10000)\n-    @IR(applyIf = {\"UseCompactObjectHeaders\", \"false\"},\n-        counts = {IRNode.CALL_OF, \"arrayof_jshort_disjoint_arraycopy\", \">0\"})\n+    @IR(counts = {IRNode.CALL_OF, \"arrayof_jshort_disjoint_arraycopy\", \">0\"})\n@@ -109,9 +87,0 @@\n-    @Test\n-    @Warmup(10000)\n-    @IR(applyIf = {\"UseCompactObjectHeaders\", \"true\"},\n-        counts = {IRNode.CALL_OF, \"arrayof_jshort_disjoint_arraycopy\", \"0\"})\n-    static void testStrUtoBytesUnAligned() {\n-        \/\/ Exercise the StringUTF16.toBytes API\n-        output_strU = String.valueOf(input_arrU);\n-    }\n-\n","filename":"test\/hotspot\/jtreg\/compiler\/c2\/irTests\/stringopts\/TestArrayCopySelect.java","additions":3,"deletions":34,"binary":false,"changes":37,"status":"modified"},{"patch":"@@ -140,2 +140,1 @@\n-        tests.put(\"test1a\",      () -> { return test1a(aB.clone(), bB.clone(), mB); });\n-        tests.put(\"test1b\",      () -> { return test1b(aB.clone(), bB.clone(), mB); });\n+        tests.put(\"test1\",       () -> { return test1(aB.clone(), bB.clone(), mB); });\n@@ -156,1 +155,0 @@\n-        tests.put(\"test10e\",     () -> { return test10e(aS.clone(), bS.clone(), mS); });\n@@ -226,2 +224,1 @@\n-                 \"test1a\",\n-                 \"test1b\",\n+                 \"test1\",\n@@ -240,1 +237,0 @@\n-                 \"test10e\",\n@@ -431,3 +427,0 @@\n-        applyIfOr = {\"UseCompactObjectHeaders\", \"false\", \"AlignVector\", \"false\"},\n-        \/\/ UNSAFE.ARRAY_BYTE_BASE_OFFSET = 16, but with compact object headers UNSAFE.ARRAY_BYTE_BASE_OFFSET=12.\n-        \/\/ If AlignVector=true, we need the offset to be 8-byte aligned, else the vectors are filtered out.\n@@ -436,1 +429,1 @@\n-    static Object[] test1a(byte[] a, byte[] b, byte mask) {\n+    static Object[] test1(byte[] a, byte[] b, byte mask) {\n@@ -450,23 +443,0 @@\n-    @Test\n-    @IR(counts = {IRNode.LOAD_VECTOR_B, \"> 0\",\n-                  IRNode.AND_VB, \"> 0\",\n-                  IRNode.STORE_VECTOR, \"> 0\"},\n-        applyIfOr = {\"UseCompactObjectHeaders\", \"true\", \"AlignVector\", \"false\"},\n-        \/\/ UNSAFE.ARRAY_BYTE_BASE_OFFSET = 16, but with compact object headers UNSAFE.ARRAY_BYTE_BASE_OFFSET=12.\n-        \/\/ If AlignVector=true, we need the offset to be 8-byte aligned, else the vectors are filtered out.\n-        applyIfPlatform = {\"64-bit\", \"true\"},\n-        applyIfCPUFeatureOr = {\"avx2\", \"true\", \"asimd\", \"true\", \"rvv\", \"true\"})\n-    static Object[] test1b(byte[] a, byte[] b, byte mask) {\n-        for (int i = 4; i < RANGE-8; i+=8) {\n-            b[i+0] = (byte)(a[i+0] & mask); \/\/ adr = base + UNSAFE.ARRAY_BYTE_BASE_OFFSET + 4 + iter*8\n-            b[i+1] = (byte)(a[i+1] & mask);\n-            b[i+2] = (byte)(a[i+2] & mask);\n-            b[i+3] = (byte)(a[i+3] & mask);\n-            b[i+4] = (byte)(a[i+4] & mask);\n-            b[i+5] = (byte)(a[i+5] & mask);\n-            b[i+6] = (byte)(a[i+6] & mask);\n-            b[i+7] = (byte)(a[i+7] & mask);\n-        }\n-        return new Object[]{ a, b };\n-    }\n-\n@@ -764,3 +734,1 @@\n-        applyIfAnd = {\"MaxVectorSize\", \">=16\", \"UseCompactObjectHeaders\", \"false\"},\n-        \/\/ UNSAFE.ARRAY_BYTE_BASE_OFFSET = 16, but with compact object headers UNSAFE.ARRAY_BYTE_BASE_OFFSET=12.\n-        \/\/ If AlignVector=true, we need the offset to be 8-byte aligned, else the vectors are filtered out.\n+        applyIf = {\"MaxVectorSize\", \">=16\"},\n@@ -780,20 +748,0 @@\n-    @Test\n-    @IR(counts = {IRNode.LOAD_VECTOR_S, IRNode.VECTOR_SIZE_4, \"> 0\",\n-                  IRNode.AND_VS,        IRNode.VECTOR_SIZE_4, \"> 0\",\n-                  IRNode.STORE_VECTOR, \"> 0\"},\n-        applyIfAnd = {\"MaxVectorSize\", \">=16\", \"UseCompactObjectHeaders\", \"true\"},\n-        \/\/ UNSAFE.ARRAY_BYTE_BASE_OFFSET = 16, but with compact object headers UNSAFE.ARRAY_BYTE_BASE_OFFSET=12.\n-        \/\/ If AlignVector=true, we need the offset to be 8-byte aligned, else the vectors are filtered out.\n-        applyIfPlatform = {\"64-bit\", \"true\"},\n-        applyIfCPUFeatureOr = {\"avx2\", \"true\", \"asimd\", \"true\", \"rvv\", \"true\"})\n-    static Object[] test10e(short[] a, short[] b, short mask) {\n-        for (int i = 11; i < RANGE-16; i+=8) {\n-            \/\/ adr = base + UNSAFE.ARRAY_SHORT_BASE_OFFSET + 2*(3 + 11) + iter*16\n-            b[i+0+3] = (short)(a[i+0+3] & mask);\n-            b[i+1+3] = (short)(a[i+1+3] & mask);\n-            b[i+2+3] = (short)(a[i+2+3] & mask);\n-            b[i+3+3] = (short)(a[i+3+3] & mask);\n-        }\n-        return new Object[]{ a, b };\n-    }\n-\n@@ -1082,1 +1030,0 @@\n-        applyIfOr = {\"UseCompactObjectHeaders\", \"false\", \"AlignVector\", \"false\"},\n@@ -1087,13 +1034,0 @@\n-            \/\/ adr = base + UNSAFE.ARRAY_BYTE_BASE_OFFSET + 1*iter\n-            \/\/              = 16 (or 12 if UseCompactObjectHeaders=true)\n-            \/\/ adr = base + UNSAFE.ARRAY_INT_BASE_OFFSET  + 4*iter\n-            \/\/              = 16 (or 12 if UseCompactObjectHeaders=true)\n-            \/\/ For AlignVector, all adr must be 8-byte aligned. Let's see for which iteration this can hold:\n-            \/\/ If UseCompactObjectHeaders=false:\n-            \/\/   a: 0, 8, 16, 24, 32, ...\n-            \/\/   b: 0, 2,  4,  6,  8, ...\n-            \/\/   -> Ok, aligns every 8th iteration.\n-            \/\/ If UseCompactObjectHeaders=true:\n-            \/\/   a: 4, 12, 20, 28, 36, ...\n-            \/\/   b: 1,  3,  5,  7,  9, ...\n-            \/\/   -> we can never align both vectors!\n@@ -1112,1 +1046,0 @@\n-        applyIfOr = {\"UseCompactObjectHeaders\", \"false\", \"AlignVector\", \"false\"},\n@@ -1117,13 +1050,0 @@\n-            \/\/ adr = base + UNSAFE.ARRAY_BYTE_BASE_OFFSET + 4*iter\n-            \/\/              = 16 (or 12 if UseCompactObjectHeaders=true)\n-            \/\/ adr = base + UNSAFE.ARRAY_SHORT_BASE_OFFSET + 2*iter\n-            \/\/              = 16 (or 12 if UseCompactObjectHeaders=true)\n-            \/\/ For AlignVector, all adr must be 8-byte aligned. Let's see for which iteration this can hold:\n-            \/\/ If UseCompactObjectHeaders=false:\n-            \/\/   a: iter % 2 == 0\n-            \/\/   b: iter % 4 == 0\n-            \/\/   -> Ok, aligns every 4th iteration.\n-            \/\/ If UseCompactObjectHeaders=true:\n-            \/\/   a: iter % 2 = 1\n-            \/\/   b: iter % 4 = 2\n-            \/\/   -> we can never align both vectors!\n@@ -1146,1 +1066,0 @@\n-        applyIfOr = {\"UseCompactObjectHeaders\", \"false\", \"AlignVector\", \"false\"},\n@@ -1151,12 +1070,0 @@\n-            \/\/ adr = base + UNSAFE.ARRAY_BYTE_BASE_OFFSET + 1*iter\n-            \/\/              = 16 (or 12 if UseCompactObjectHeaders=true)\n-            \/\/ adr = base + UNSAFE.ARRAY_SHORT_BASE_OFFSET + 2*iter\n-            \/\/              = 16 (or 12 if UseCompactObjectHeaders=true)\n-            \/\/ adr = base + UNSAFE.ARRAY_INT_BASE_OFFSET + 4*iter\n-            \/\/              = 16 (or 12 if UseCompactObjectHeaders=true)\n-            \/\/ adr = base + UNSAFE.ARRAY_LONG_BASE_OFFSET + 8*iter\n-            \/\/              = 16 (always)\n-            \/\/ If AlignVector and UseCompactObjectHeaders, and we want all adr 8-byte aligned:\n-            \/\/   a: iter % 8 = 4\n-            \/\/   c: iter % 2 = 1\n-            \/\/   -> can never align both vectors!\n@@ -1194,1 +1101,0 @@\n-        applyIfOr = {\"UseCompactObjectHeaders\", \"false\", \"AlignVector\", \"false\"},\n@@ -1199,8 +1105,0 @@\n-            \/\/ adr = base + UNSAFE.ARRAY_INT_BASE_OFFSET + 4 + 4*iter\n-            \/\/              = 16 (or 12 if UseCompactObjectHeaders=true)\n-            \/\/ adr = base + UNSAFE.ARRAY_BYTE_BASE_OFFSET + 1 + 1*iter\n-            \/\/              = 16 (or 12 if UseCompactObjectHeaders=true)\n-            \/\/ If AlignVector and UseCompactObjectHeaders, and we want all adr 8-byte aligned:\n-            \/\/   a: iter % 2 = 0\n-            \/\/   b: iter % 8 = 3\n-            \/\/   -> can never align both vectors!\n@@ -1219,1 +1117,0 @@\n-        applyIfOr = {\"UseCompactObjectHeaders\", \"false\", \"AlignVector\", \"false\"},\n@@ -1224,8 +1121,0 @@\n-            \/\/ adr = base + UNSAFE.ARRAY_INT_BASE_OFFSET + 4 + 4*iter\n-            \/\/              = 16 (or 12 if UseCompactObjectHeaders=true)\n-            \/\/ adr = base + UNSAFE.ARRAY_SHORT_BASE_OFFSET + 2 + 2*iter\n-            \/\/              = 16 (or 12 if UseCompactObjectHeaders=true)\n-            \/\/ If AlignVector and UseCompactObjectHeaders, and we want all adr 8-byte aligned:\n-            \/\/   a: iter % 2 = 0\n-            \/\/   b: iter % 4 = 1\n-            \/\/   -> can never align both vectors!\n@@ -1248,1 +1137,0 @@\n-        applyIfOr = {\"UseCompactObjectHeaders\", \"false\", \"AlignVector\", \"false\"},\n@@ -1253,12 +1141,0 @@\n-            \/\/ adr = base + UNSAFE.ARRAY_BYTE_BASE_OFFSET + 1 + 1*iter\n-            \/\/              = 16 (or 12 if UseCompactObjectHeaders=true)\n-            \/\/ adr = base + UNSAFE.ARRAY_SHORT_BASE_OFFSET + 2 + 2*iter\n-            \/\/              = 16 (or 12 if UseCompactObjectHeaders=true)\n-            \/\/ adr = base + UNSAFE.ARRAY_INT_BASE_OFFSET + 4 + 4*iter\n-            \/\/              = 16 (or 12 if UseCompactObjectHeaders=true)\n-            \/\/ adr = base + UNSAFE.ARRAY_LONG_BASE_OFFSET + 8 + 8*iter\n-            \/\/              = 16 (always)\n-            \/\/ If AlignVector and UseCompactObjectHeaders, and we want all adr 8-byte aligned:\n-            \/\/   a: iter % 8 = 3\n-            \/\/   c: iter % 2 = 0\n-            \/\/   -> can never align both vectors!\n","filename":"test\/hotspot\/jtreg\/compiler\/loopopts\/superword\/TestAlignVector.java","additions":4,"deletions":128,"binary":false,"changes":132,"status":"modified"},{"patch":"@@ -133,1 +133,0 @@\n-        applyIfOr = {\"UseCompactObjectHeaders\", \"false\", \"AlignVector\", \"false\"},\n@@ -143,4 +142,0 @@\n-            \/\/ With AlignVector, we need 8-byte alignment of vector loads\/stores.\n-            \/\/ UseCompactObjectHeaders=false                 UseCompactObjectHeaders=true\n-            \/\/ adr = base + 16 + 8*i   ->  always            adr = base + 12 + 8*i   ->  never\n-            \/\/ -> vectorize                                  -> no vectorization\n@@ -163,1 +158,0 @@\n-        applyIfOr = {\"UseCompactObjectHeaders\", \"false\", \"AlignVector\", \"false\"},\n@@ -173,4 +167,0 @@\n-            \/\/ With AlignVector, we need 8-byte alignment of vector loads\/stores.\n-            \/\/ UseCompactObjectHeaders=false                 UseCompactObjectHeaders=true\n-            \/\/ adr = base + 16 + 8*i   ->  always            adr = base + 12 + 8*i   ->  never\n-            \/\/ -> vectorize                                  -> no vectorization\n@@ -192,1 +182,0 @@\n-        applyIfOr = {\"UseCompactObjectHeaders\", \"false\", \"AlignVector\", \"false\"},\n@@ -202,4 +191,0 @@\n-            \/\/ With AlignVector, we need 8-byte alignment of vector loads\/stores.\n-            \/\/ UseCompactObjectHeaders=false                 UseCompactObjectHeaders=true\n-            \/\/ adr = base + 16 + 8*i   ->  always            adr = base + 12 + 8*i   ->  never\n-            \/\/ -> vectorize                                  -> no vectorization\n@@ -222,1 +207,0 @@\n-        applyIfOr = {\"UseCompactObjectHeaders\", \"false\", \"AlignVector\", \"false\"},\n@@ -234,4 +218,0 @@\n-            \/\/ With AlignVector, we need 8-byte alignment of vector loads\/stores.\n-            \/\/ UseCompactObjectHeaders=false                 UseCompactObjectHeaders=true\n-            \/\/ adr = base + 16 + 8*i   ->  always            adr = base + 12 + 8*i   ->  never\n-            \/\/ -> vectorize                                  -> no vectorization\n@@ -304,1 +284,0 @@\n-        applyIfOr = {\"UseCompactObjectHeaders\", \"false\", \"AlignVector\", \"false\"},\n@@ -323,4 +302,0 @@\n-            \/\/ With AlignVector, we need 8-byte alignment of vector loads\/stores.\n-            \/\/ UseCompactObjectHeaders=false                 UseCompactObjectHeaders=true\n-            \/\/ adr = base + 16 + 8*i   ->  always            adr = base + 12 + 8*i   ->  never\n-            \/\/ -> vectorize                                  -> no vectorization\n","filename":"test\/hotspot\/jtreg\/compiler\/loopopts\/superword\/TestIndependentPacksWithCyclicDependency.java","additions":0,"deletions":25,"binary":false,"changes":25,"status":"modified"},{"patch":"@@ -168,1 +168,0 @@\n-        applyIfOr = { \"UseCompactObjectHeaders\", \"false\", \"AlignVector\", \"false\" },\n@@ -171,1 +170,1 @@\n-        applyIfAnd = {\"MaxVectorSize\", \"16\", \"UseCompactObjectHeaders\", \"false\"}, \/\/ AD file requires vector_length = 16\n+        applyIf = {\"MaxVectorSize\", \"16\"}, \/\/ AD file requires vector_length = 16\n@@ -174,1 +173,0 @@\n-        applyIfOr = { \"UseCompactObjectHeaders\", \"false\", \"AlignVector\", \"false\" },\n@@ -181,7 +179,0 @@\n-            \/\/ Hand-unrolling can mess with AlignVector and UseCompactObjectHeaders.\n-            \/\/ We need all addresses 8-byte aligned.\n-            \/\/\n-            \/\/ out:\n-            \/\/   adr = base + UNSAFE.ARRAY_INT_BASE_OFFSET + 8*iter\n-            \/\/                = 16 (or 12 if UseCompactObjectHeaders=true)\n-            \/\/ -> never aligned!\n@@ -195,1 +186,0 @@\n-        applyIfOr = { \"UseCompactObjectHeaders\", \"false\", \"AlignVector\", \"false\" },\n@@ -198,1 +188,1 @@\n-        applyIfAnd = {\"MaxVectorSize\", \"16\", \"UseCompactObjectHeaders\", \"false\" }, \/\/ AD file requires vector_length = 16\n+        applyIf = {\"MaxVectorSize\", \"16\"}, \/\/ AD file requires vector_length = 16\n@@ -201,1 +191,0 @@\n-        applyIfOr = { \"UseCompactObjectHeaders\", \"false\", \"AlignVector\", \"false\" },\n@@ -208,7 +197,0 @@\n-            \/\/ Hand-unrolling can mess with AlignVector and UseCompactObjectHeaders.\n-            \/\/ We need all addresses 8-byte aligned.\n-            \/\/\n-            \/\/ out:\n-            \/\/   adr = base + UNSAFE.ARRAY_INT_BASE_OFFSET + 8*iter\n-            \/\/                = 16 (or 12 if UseCompactObjectHeaders=true)\n-            \/\/ -> never aligned!\n@@ -222,1 +204,0 @@\n-        applyIfOr = { \"UseCompactObjectHeaders\", \"false\", \"AlignVector\", \"false\" },\n@@ -225,1 +206,1 @@\n-        applyIfAnd = {\"MaxVectorSize\", \"16\", \"UseCompactObjectHeaders\", \"false\" }, \/\/ AD file requires vector_length = 16\n+        applyIf = {\"MaxVectorSize\", \"16\"}, \/\/ AD file requires vector_length = 16\n@@ -228,1 +209,0 @@\n-        applyIfOr = { \"UseCompactObjectHeaders\", \"false\", \"AlignVector\", \"false\" },\n@@ -235,7 +215,0 @@\n-            \/\/ Hand-unrolling can mess with AlignVector and UseCompactObjectHeaders.\n-            \/\/ We need all addresses 8-byte aligned.\n-            \/\/\n-            \/\/ out:\n-            \/\/   adr = base + UNSAFE.ARRAY_INT_BASE_OFFSET + 8*iter\n-            \/\/                = 16 (or 12 if UseCompactObjectHeaders=true)\n-            \/\/ -> never aligned!\n@@ -249,1 +222,0 @@\n-        applyIfOr = { \"UseCompactObjectHeaders\", \"false\", \"AlignVector\", \"false\" },\n@@ -252,1 +224,1 @@\n-        applyIfAnd = {\"MaxVectorSize\", \"16\", \"UseCompactObjectHeaders\", \"false\" }, \/\/ AD file requires vector_length = 16\n+        applyIf = {\"MaxVectorSize\", \"16\"}, \/\/ AD file requires vector_length = 16\n@@ -255,1 +227,0 @@\n-        applyIfOr = { \"UseCompactObjectHeaders\", \"false\", \"AlignVector\", \"false\" },\n@@ -262,7 +233,0 @@\n-            \/\/ Hand-unrolling can mess with AlignVector and UseCompactObjectHeaders.\n-            \/\/ We need all addresses 8-byte aligned.\n-            \/\/\n-            \/\/ out:\n-            \/\/   adr = base + UNSAFE.ARRAY_INT_BASE_OFFSET + 8*iter\n-            \/\/                = 16 (or 12 if UseCompactObjectHeaders=true)\n-            \/\/ -> never aligned!\n@@ -276,1 +240,0 @@\n-        applyIfOr = { \"UseCompactObjectHeaders\", \"false\", \"AlignVector\", \"false\" },\n@@ -279,1 +242,1 @@\n-        applyIfAnd = {\"MaxVectorSize\", \"16\", \"UseCompactObjectHeaders\", \"false\" }, \/\/ AD file requires vector_length = 16\n+        applyIf = {\"MaxVectorSize\", \"16\"}, \/\/ AD file requires vector_length = 16\n@@ -282,1 +245,0 @@\n-        applyIfOr = { \"UseCompactObjectHeaders\", \"false\", \"AlignVector\", \"false\" },\n@@ -289,7 +251,0 @@\n-            \/\/ Hand-unrolling can mess with AlignVector and UseCompactObjectHeaders.\n-            \/\/ We need all addresses 8-byte aligned.\n-            \/\/\n-            \/\/ out:\n-            \/\/   adr = base + UNSAFE.ARRAY_INT_BASE_OFFSET + 8*iter\n-            \/\/                = 16 (or 12 if UseCompactObjectHeaders=true)\n-            \/\/ -> never aligned!\n","filename":"test\/hotspot\/jtreg\/compiler\/loopopts\/superword\/TestMulAddS2I.java","additions":5,"deletions":50,"binary":false,"changes":55,"status":"modified"},{"patch":"@@ -94,1 +94,0 @@\n-        applyIfOr = {\"UseCompactObjectHeaders\", \"false\", \"AlignVector\", \"false\"},\n@@ -119,4 +118,0 @@\n-            \/\/ With AlignVector, we need 8-byte alignment of vector loads\/stores.\n-            \/\/ UseCompactObjectHeaders=false                 UseCompactObjectHeaders=true\n-            \/\/ adr = base + 16 + 8*i   ->  always            adr = base + 12 + 8*i   ->  never\n-            \/\/ -> vectorize                                  -> no vectorization\n@@ -139,1 +134,0 @@\n-        applyIfOr = {\"UseCompactObjectHeaders\", \"false\", \"AlignVector\", \"false\"},\n@@ -149,4 +143,0 @@\n-            \/\/ With AlignVector, we need 8-byte alignment of vector loads\/stores.\n-            \/\/ UseCompactObjectHeaders=false                 UseCompactObjectHeaders=true\n-            \/\/ adr = base + 16 + 8*i   ->  always            adr = base + 12 + 8*i   ->  never\n-            \/\/ -> vectorize                                  -> no vectorization\n","filename":"test\/hotspot\/jtreg\/compiler\/loopopts\/superword\/TestScheduleReordersScalarMemops.java","additions":0,"deletions":10,"binary":false,"changes":10,"status":"modified"},{"patch":"@@ -281,9 +281,1 @@\n-        applyIfAnd = {\"MaxVectorSize\", \">=32\", \"AlignVector\", \"false\"},\n-        applyIfPlatform = {\"64-bit\", \"true\"},\n-        applyIfCPUFeatureOr = {\"sse4.1\", \"true\", \"asimd\", \"true\", \"rvv\", \"true\"})\n-    @IR(counts = {IRNode.LOAD_VECTOR_I, IRNode.VECTOR_SIZE_2, \"> 0\",\n-                  IRNode.LOAD_VECTOR_I, IRNode.VECTOR_SIZE_4, \"> 0\",\n-                  IRNode.AND_VI,        IRNode.VECTOR_SIZE_2, \"> 0\",\n-                  IRNode.AND_VI,        IRNode.VECTOR_SIZE_4, \"> 0\",\n-                  IRNode.STORE_VECTOR, \"> 0\"},\n-        applyIfAnd = {\"MaxVectorSize\", \">=32\", \"UseCompactObjectHeaders\", \"false\"},\n+        applyIf = {\"MaxVectorSize\", \">=32\"},\n@@ -314,4 +306,0 @@\n-            \/\/ With AlignVector, we need 8-byte alignment of vector loads\/stores.\n-            \/\/ UseCompactObjectHeaders=false                 UseCompactObjectHeaders=true\n-            \/\/ adr = base + 16 + 32*i  ->  always            adr = base + 12 + 32*i  ->  never\n-            \/\/ -> vectorize                                  -> no vectorization\n@@ -328,9 +316,1 @@\n-        applyIfAnd = {\"MaxVectorSize\", \">=32\", \"AlignVector\", \"false\"},\n-        applyIfPlatform = {\"64-bit\", \"true\"},\n-        applyIfCPUFeatureOr = {\"sse4.1\", \"true\", \"asimd\", \"true\", \"rvv\", \"true\"})\n-    @IR(counts = {IRNode.LOAD_VECTOR_I, IRNode.VECTOR_SIZE_2, \"> 0\",\n-                  IRNode.LOAD_VECTOR_I, IRNode.VECTOR_SIZE_4, \"> 0\",\n-                  IRNode.ADD_VI,        IRNode.VECTOR_SIZE_4, \"> 0\",\n-                  IRNode.MUL_VI,        IRNode.VECTOR_SIZE_2, \"> 0\",\n-                  IRNode.STORE_VECTOR, \"> 0\"},\n-        applyIfAnd = {\"MaxVectorSize\", \">=32\", \"UseCompactObjectHeaders\", \"false\"},\n+        applyIf = {\"MaxVectorSize\", \">=32\"},\n@@ -349,4 +329,0 @@\n-            \/\/ With AlignVector, we need 8-byte alignment of vector loads\/stores.\n-            \/\/ UseCompactObjectHeaders=false                 UseCompactObjectHeaders=true\n-            \/\/ adr = base + 16 + 32*i  ->  always            adr = base + 12 + 32*i  ->  never\n-            \/\/ -> vectorize                                  -> no vectorization\n@@ -363,9 +339,1 @@\n-        applyIfAnd = {\"MaxVectorSize\", \">=32\", \"AlignVector\", \"false\"},\n-        applyIfPlatform = {\"64-bit\", \"true\"},\n-        applyIfCPUFeatureOr = {\"sse4.1\", \"true\", \"asimd\", \"true\", \"rvv\", \"true\"})\n-    @IR(counts = {IRNode.LOAD_VECTOR_I, IRNode.VECTOR_SIZE_2, \"> 0\",\n-                  IRNode.LOAD_VECTOR_I, IRNode.VECTOR_SIZE_4, \"> 0\",\n-                  IRNode.ADD_VI,        IRNode.VECTOR_SIZE_2, \"> 0\",\n-                  IRNode.MUL_VI,        IRNode.VECTOR_SIZE_4, \"> 0\",\n-                  IRNode.STORE_VECTOR, \"> 0\"},\n-        applyIfAnd = {\"MaxVectorSize\", \">=32\", \"UseCompactObjectHeaders\", \"false\"},\n+        applyIf = {\"MaxVectorSize\", \">=32\"},\n@@ -384,4 +352,0 @@\n-            \/\/ With AlignVector, we need 8-byte alignment of vector loads\/stores.\n-            \/\/ UseCompactObjectHeaders=false                 UseCompactObjectHeaders=true\n-            \/\/ adr = base + 16 + 32*i  ->  always            adr = base + 12 + 32*i  ->  never\n-            \/\/ -> vectorize                                  -> no vectorization\n@@ -398,9 +362,1 @@\n-        applyIfAnd = {\"MaxVectorSize\", \">=32\", \"AlignVector\", \"false\"},\n-        applyIfPlatform = {\"64-bit\", \"true\"},\n-        applyIfCPUFeatureOr = {\"avx2\", \"true\", \"asimd\", \"true\", \"rvv\", \"true\"})\n-    @IR(counts = {IRNode.LOAD_VECTOR_I, IRNode.VECTOR_SIZE_2, \"> 0\",\n-                  IRNode.LOAD_VECTOR_I, IRNode.VECTOR_SIZE_4, \"> 0\",\n-                  IRNode.ADD_VI,        IRNode.VECTOR_SIZE_2, \"> 0\",\n-                  IRNode.MUL_VI,        IRNode.VECTOR_SIZE_4, \"> 0\",\n-                  IRNode.STORE_VECTOR, \"> 0\"},\n-        applyIfAnd = {\"MaxVectorSize\", \">=32\", \"UseCompactObjectHeaders\", \"false\"},\n+        applyIf = {\"MaxVectorSize\", \">=32\"},\n@@ -419,4 +375,0 @@\n-            \/\/ With AlignVector, we need 8-byte alignment of vector loads\/stores.\n-            \/\/ UseCompactObjectHeaders=false                 UseCompactObjectHeaders=true\n-            \/\/ adr = base + 16 + 32*i  ->  always            adr = base + 12 + 32*i  ->  never\n-            \/\/ -> vectorize                                  -> no vectorization\n@@ -433,9 +385,1 @@\n-        applyIfAnd = {\"MaxVectorSize\", \">=32\", \"AlignVector\", \"false\"},\n-        applyIfPlatform = {\"64-bit\", \"true\"},\n-        applyIfCPUFeatureOr = {\"avx2\", \"true\", \"asimd\", \"true\", \"rvv\", \"true\"})\n-    @IR(counts = {IRNode.LOAD_VECTOR_I, IRNode.VECTOR_SIZE_2, \"> 0\",\n-                  IRNode.LOAD_VECTOR_I, IRNode.VECTOR_SIZE_4, \"> 0\",\n-                  IRNode.ADD_VI,        IRNode.VECTOR_SIZE_4, \"> 0\",\n-                  IRNode.MUL_VI,        IRNode.VECTOR_SIZE_2, \"> 0\",\n-                  IRNode.STORE_VECTOR, \"> 0\"},\n-        applyIfAnd = {\"MaxVectorSize\", \">=32\", \"UseCompactObjectHeaders\", \"false\"},\n+        applyIf = {\"MaxVectorSize\", \">=32\"},\n@@ -454,4 +398,0 @@\n-            \/\/ With AlignVector, we need 8-byte alignment of vector loads\/stores.\n-            \/\/ UseCompactObjectHeaders=false                 UseCompactObjectHeaders=true\n-            \/\/ adr = base + 16 + 32*i  ->  always            adr = base + 12 + 32*i  ->  never\n-            \/\/ -> vectorize                                  -> no vectorization\n@@ -468,9 +408,1 @@\n-        applyIfAnd = {\"MaxVectorSize\", \">=32\", \"AlignVector\", \"false\"},\n-        applyIfPlatform = {\"64-bit\", \"true\"},\n-        applyIfCPUFeatureOr = {\"sse4.1\", \"true\", \"asimd\", \"true\", \"rvv\", \"true\"})\n-    @IR(counts = {IRNode.LOAD_VECTOR_I, IRNode.VECTOR_SIZE_2, \"> 0\",\n-                  IRNode.LOAD_VECTOR_I, IRNode.VECTOR_SIZE_4, \"> 0\",\n-                  IRNode.AND_VI,        IRNode.VECTOR_SIZE_2, \"> 0\",\n-                  IRNode.AND_VI,        IRNode.VECTOR_SIZE_4, \"> 0\",\n-                  IRNode.STORE_VECTOR, \"> 0\"},\n-        applyIfAnd = {\"MaxVectorSize\", \">=32\", \"UseCompactObjectHeaders\", \"false\"},\n+        applyIf = {\"MaxVectorSize\", \">=32\"},\n@@ -503,4 +435,0 @@\n-            \/\/ With AlignVector, we need 8-byte alignment of vector loads\/stores.\n-            \/\/ UseCompactObjectHeaders=false                 UseCompactObjectHeaders=true\n-            \/\/ adr = base + 16 + 32*i  ->  always            adr = base + 12 + 32*i  ->  never\n-            \/\/ -> vectorize                                  -> no vectorization\n@@ -517,9 +445,1 @@\n-        applyIfAnd = {\"MaxVectorSize\", \">=32\", \"AlignVector\", \"false\"},\n-        applyIfPlatform = {\"64-bit\", \"true\"},\n-        applyIfCPUFeatureOr = {\"sse4.1\", \"true\", \"asimd\", \"true\", \"rvv\", \"true\"})\n-    @IR(counts = {IRNode.LOAD_VECTOR_I, IRNode.VECTOR_SIZE_2, \"> 0\",\n-                  IRNode.LOAD_VECTOR_I, IRNode.VECTOR_SIZE_4, \"> 0\",\n-                  IRNode.AND_VI,        IRNode.VECTOR_SIZE_2, \"> 0\",\n-                  IRNode.AND_VI,        IRNode.VECTOR_SIZE_4, \"> 0\",\n-                  IRNode.STORE_VECTOR, \"> 0\"},\n-        applyIfAnd = {\"MaxVectorSize\", \">=32\", \"UseCompactObjectHeaders\", \"false\"},\n+        applyIf = {\"MaxVectorSize\", \">=32\"},\n@@ -552,4 +472,0 @@\n-            \/\/ With AlignVector, we need 8-byte alignment of vector loads\/stores.\n-            \/\/ UseCompactObjectHeaders=false                 UseCompactObjectHeaders=true\n-            \/\/ adr = base + 16 + 32*i  ->  always            adr = base + 12 + 32*i  ->  never\n-            \/\/ -> vectorize                                  -> no vectorization\n@@ -566,9 +482,1 @@\n-        applyIfAnd = {\"MaxVectorSize\", \">=32\", \"AlignVector\", \"false\"},\n-        applyIfPlatform = {\"64-bit\", \"true\"},\n-        applyIfCPUFeatureOr = {\"sse4.1\", \"true\", \"asimd\", \"true\", \"rvv\", \"true\"})\n-    @IR(counts = {IRNode.LOAD_VECTOR_I, IRNode.VECTOR_SIZE_2, \"> 0\",\n-                  IRNode.LOAD_VECTOR_I, IRNode.VECTOR_SIZE_4, \"> 0\",\n-                  IRNode.AND_VI,        IRNode.VECTOR_SIZE_2, \"> 0\",\n-                  IRNode.AND_VI,        IRNode.VECTOR_SIZE_4, \"> 0\",\n-                  IRNode.STORE_VECTOR, \"> 0\"},\n-        applyIfAnd = {\"MaxVectorSize\", \">=32\", \"UseCompactObjectHeaders\", \"false\"},\n+        applyIf = {\"MaxVectorSize\", \">=32\"},\n@@ -601,4 +509,0 @@\n-            \/\/ With AlignVector, we need 8-byte alignment of vector loads\/stores.\n-            \/\/ UseCompactObjectHeaders=false                 UseCompactObjectHeaders=true\n-            \/\/ adr = base + 16 + 32*i  ->  always            adr = base + 12 + 32*i  ->  never\n-            \/\/ -> vectorize                                  -> no vectorization\n@@ -615,9 +519,1 @@\n-        applyIfAnd = {\"MaxVectorSize\", \">=32\", \"AlignVector\", \"false\"},\n-        applyIfPlatform = {\"64-bit\", \"true\"},\n-        applyIfCPUFeatureOr = {\"sse4.1\", \"true\", \"asimd\", \"true\", \"rvv\", \"true\"})\n-    @IR(counts = {IRNode.LOAD_VECTOR_I, IRNode.VECTOR_SIZE_2, \"> 0\",\n-                  IRNode.LOAD_VECTOR_I, IRNode.VECTOR_SIZE_4, \"> 0\",\n-                  IRNode.AND_VI,        IRNode.VECTOR_SIZE_2, \"> 0\",\n-                  IRNode.AND_VI,        IRNode.VECTOR_SIZE_4, \"> 0\",\n-                  IRNode.STORE_VECTOR, \"> 0\"},\n-        applyIfAnd = {\"MaxVectorSize\", \">=32\", \"UseCompactObjectHeaders\", \"false\"},\n+        applyIf = {\"MaxVectorSize\", \">=32\"},\n@@ -650,4 +546,0 @@\n-            \/\/ With AlignVector, we need 8-byte alignment of vector loads\/stores.\n-            \/\/ UseCompactObjectHeaders=false                 UseCompactObjectHeaders=true\n-            \/\/ adr = base + 16 + 32*i  ->  always            adr = base + 12 + 32*i  ->  never\n-            \/\/ -> vectorize                                  -> no vectorization\n@@ -661,6 +553,1 @@\n-        applyIfAnd = {\"MaxVectorSize\", \">=32\", \"AlignVector\", \"false\"},\n-        applyIfPlatform = {\"64-bit\", \"true\"},\n-        applyIfCPUFeatureOr = {\"sse4.1\", \"true\", \"asimd\", \"true\", \"rvv\", \"true\"})\n-    @IR(counts = {IRNode.LOAD_VECTOR_S, IRNode.VECTOR_SIZE_4, \"> 0\",\n-                  IRNode.STORE_VECTOR, \"> 0\"},\n-        applyIfAnd = {\"MaxVectorSize\", \">=32\", \"UseCompactObjectHeaders\", \"false\"},\n+        applyIf = {\"MaxVectorSize\", \">=32\"},\n@@ -703,5 +590,0 @@\n-\n-            \/\/ With AlignVector, we need 8-byte alignment of vector loads\/stores.\n-            \/\/ UseCompactObjectHeaders=false                 UseCompactObjectHeaders=true\n-            \/\/ adr = base + 16 + 8 + 32*i  ->  always        adr = base + 12 + 8 + 32*i  ->  never\n-            \/\/ -> vectorize                                  -> no vectorization\n@@ -861,9 +743,1 @@\n-        applyIfAnd = {\"MaxVectorSize\", \">=32\", \"AlignVector\", \"false\"},\n-        applyIfPlatform = {\"64-bit\", \"true\"},\n-        applyIfCPUFeatureOr = {\"sse4.1\", \"true\", \"asimd\", \"true\", \"rvv\", \"true\"})\n-    @IR(counts = {IRNode.LOAD_VECTOR_I,   IRNode.VECTOR_SIZE_4, \"> 0\",\n-                  IRNode.MUL_VI,          IRNode.VECTOR_SIZE_4, \"> 0\",\n-                  IRNode.AND_VI,          IRNode.VECTOR_SIZE_4, \"> 0\",\n-                  IRNode.ADD_VI,          IRNode.VECTOR_SIZE_4, \"> 0\", \/\/ reduction moved out of loop\n-                  IRNode.ADD_REDUCTION_V,                       \"> 0\"},\n-        applyIfAnd = {\"MaxVectorSize\", \">=32\", \"UseCompactObjectHeaders\", \"false\"},\n+        applyIf = {\"MaxVectorSize\", \">=32\"},\n@@ -885,4 +759,0 @@\n-            \/\/ With AlignVector, we need 8-byte alignment of vector loads\/stores.\n-            \/\/ UseCompactObjectHeaders=false                 UseCompactObjectHeaders=true\n-            \/\/ adr = base + 16 + 32*i  ->  always            adr = base + 12 + 32*i  ->  never\n-            \/\/ -> vectorize                                  -> no vectorization\n","filename":"test\/hotspot\/jtreg\/compiler\/loopopts\/superword\/TestSplitPacks.java","additions":11,"deletions":141,"binary":false,"changes":152,"status":"modified"},{"patch":"@@ -67,1 +67,0 @@\n-        applyIfOr = {\"AlignVector\", \"false\", \"UseCompactObjectHeaders\", \"false\"},\n@@ -95,5 +94,0 @@\n-\n-            \/\/ With AlignVector, we need 8-byte alignment of vector loads\/stores.\n-            \/\/ UseCompactObjectHeaders=false                 UseCompactObjectHeaders=true\n-            \/\/ adr = base + 16 + 8*i  ->  always             adr = base + 12 + 8*i  ->  never\n-            \/\/ -> vectorize                                  -> no vectorization\n","filename":"test\/hotspot\/jtreg\/compiler\/loopopts\/superword\/TestUnorderedReductionPartialVectorization.java","additions":0,"deletions":6,"binary":false,"changes":6,"status":"modified"},{"patch":"@@ -65,1 +65,0 @@\n-        applyIfOr = {\"UseCompactObjectHeaders\", \"false\", \"AlignVector\", \"false\"},\n@@ -71,5 +70,0 @@\n-            \/\/ With AlignVector, we need 8-byte alignment of vector loads\/stores.\n-            \/\/ UseCompactObjectHeaders=false                 UseCompactObjectHeaders=true\n-            \/\/ F_adr = base + 16 + 4*i   ->  i % 2 = 0       F_adr = base + 12 + 4*i   ->  i % 2 = 1\n-            \/\/ S_adr = base + 16 + 2*i   ->  i % 4 = 0       S_adr = base + 12 + 2*i   ->  i % 4 = 2\n-            \/\/ -> vectorize                                  -> no vectorization\n@@ -134,1 +128,0 @@\n-        applyIfOr = {\"UseCompactObjectHeaders\", \"false\", \"AlignVector\", \"false\"},\n@@ -140,5 +133,0 @@\n-            \/\/ With AlignVector, we need 8-byte alignment of vector loads\/stores.\n-            \/\/ UseCompactObjectHeaders=false                 UseCompactObjectHeaders=true\n-            \/\/ F_adr = base + 16 + 4*i   ->  i % 2 = 0       F_adr = base + 12 + 4*i   ->  i % 2 = 1\n-            \/\/ S_adr = base + 16 + 2*i   ->  i % 4 = 0       S_adr = base + 12 + 2*i   ->  i % 4 = 2\n-            \/\/ -> vectorize                                  -> no vectorization\n","filename":"test\/hotspot\/jtreg\/compiler\/vectorization\/TestFloatConversionsVector.java","additions":0,"deletions":12,"binary":false,"changes":12,"status":"modified"},{"patch":"@@ -68,1 +68,0 @@\n-        applyIfOr = {\"UseCompactObjectHeaders\", \"false\", \"AlignVector\", \"false\"})\n@@ -72,5 +71,0 @@\n-            \/\/ With AlignVector, we need 8-byte alignment of vector loads\/stores.\n-            \/\/ UseCompactObjectHeaders=false                 UseCompactObjectHeaders=true\n-            \/\/ F_adr = base + 16 + 4*i   ->  i % 2 = 0       F_adr = base + 12 + 4*i   ->  i % 2 = 1\n-            \/\/ S_adr = base + 16 + 2*i   ->  i % 4 = 0       S_adr = base + 12 + 2*i   ->  i % 4 = 2\n-            \/\/ -> vectorize                                  -> no vectorization\n@@ -149,2 +143,1 @@\n-    @IR(counts = {IRNode.VECTOR_CAST_HF2F, IRNode.VECTOR_SIZE + \"min(max_float, max_short)\", \"> 0\"},\n-        applyIfOr = {\"UseCompactObjectHeaders\", \"false\", \"AlignVector\", \"false\"})\n+    @IR(counts = {IRNode.VECTOR_CAST_HF2F, IRNode.VECTOR_SIZE + \"min(max_float, max_short)\", \"> 0\"})\n@@ -154,5 +147,0 @@\n-            \/\/ With AlignVector, we need 8-byte alignment of vector loads\/stores.\n-            \/\/ UseCompactObjectHeaders=false                 UseCompactObjectHeaders=true\n-            \/\/ F_adr = base + 16 + 4*i   ->  i % 2 = 0       F_adr = base + 12 + 4*i   ->  i % 2 = 1\n-            \/\/ S_adr = base + 16 + 2*i   ->  i % 4 = 0       S_adr = base + 12 + 2*i   ->  i % 4 = 2\n-            \/\/ -> vectorize                                  -> no vectorization\n","filename":"test\/hotspot\/jtreg\/compiler\/vectorization\/TestFloatConversionsVectorNaN.java","additions":1,"deletions":13,"binary":false,"changes":14,"status":"modified"},{"patch":"@@ -64,1 +64,1 @@\n-\/\/ This means it affects the alignment constraints.\n+\/\/ It should, however, not affect the alignment constraints.\n@@ -237,1 +237,0 @@\n-        applyIfOr = {\"AlignVector\", \"false\", \"UseCompactObjectHeaders\", \"false\"},\n@@ -243,7 +242,0 @@\n-            \/\/ AlignVector=true requires that all vector load\/store are 8-byte aligned.\n-            \/\/ F_adr = base + UNSAFE.ARRAY_FLOAT_BASE_OFFSET + 4*i\n-            \/\/                = 16 (UseCompactObjectHeaders=false)    -> i % 2 = 0\n-            \/\/                = 12 (UseCompactObjectHeaders=true )    -> i % 2 = 1\n-            \/\/ S_adr = base + UNSAFE.ARRAY_SHORT_BASE_OFFSET + 2*i\n-            \/\/                = 16 (UseCompactObjectHeaders=false)    -> i % 4 = 0  -> can align both\n-            \/\/                = 12 (UseCompactObjectHeaders=true )    -> i % 4 = 2  -> cannot align both\n@@ -340,1 +332,0 @@\n-        applyIfOr = {\"AlignVector\", \"false\", \"UseCompactObjectHeaders\", \"false\"},\n@@ -346,7 +337,0 @@\n-            \/\/ AlignVector=true requires that all vector load\/store are 8-byte aligned.\n-            \/\/ F_adr = base + UNSAFE.ARRAY_FLOAT_BASE_OFFSET + 4*i\n-            \/\/                = 16 (UseCompactObjectHeaders=false)    -> i % 2 = 0\n-            \/\/                = 12 (UseCompactObjectHeaders=true )    -> i % 2 = 1\n-            \/\/ S_adr = base + UNSAFE.ARRAY_SHORT_BASE_OFFSET + 2*i\n-            \/\/                = 16 (UseCompactObjectHeaders=false)    -> i % 4 = 0  -> can align both\n-            \/\/                = 12 (UseCompactObjectHeaders=true )    -> i % 4 = 2  -> cannot align both\n@@ -359,1 +343,0 @@\n-        applyIfOr = {\"AlignVector\", \"false\", \"UseCompactObjectHeaders\", \"false\"},\n@@ -365,7 +348,0 @@\n-            \/\/ AlignVector=true requires that all vector load\/store are 8-byte aligned.\n-            \/\/ F_adr = base + UNSAFE.ARRAY_FLOAT_BASE_OFFSET + 4*i\n-            \/\/                = 16 (UseCompactObjectHeaders=false)    -> i % 2 = 0\n-            \/\/                = 12 (UseCompactObjectHeaders=true )    -> i % 2 = 1\n-            \/\/ S_adr = base + UNSAFE.ARRAY_SHORT_BASE_OFFSET + 2*i\n-            \/\/                = 16 (UseCompactObjectHeaders=false)    -> i % 4 = 0  -> can align both\n-            \/\/                = 12 (UseCompactObjectHeaders=true )    -> i % 4 = 2  -> cannot align both\n","filename":"test\/hotspot\/jtreg\/compiler\/vectorization\/runner\/ArrayTypeConvertTest.java","additions":1,"deletions":25,"binary":false,"changes":26,"status":"modified"},{"patch":"@@ -210,1 +210,0 @@\n-        applyIfOr = { \"UseCompactObjectHeaders\", \"false\", \"AlignVector\", \"false\"},\n@@ -220,17 +219,0 @@\n-            \/\/ We have a mix of int and short loads\/stores.\n-            \/\/ With UseCompactObjectHeaders and AlignVector,\n-            \/\/ we must 8-byte align all vector loads\/stores.\n-            \/\/\n-            \/\/ int:\n-            \/\/ adr = base + UNSAFE.ARRAY_INT_BASE_OFFSET + 4*iter\n-            \/\/              = 16 (or 12 if UseCompactObjectHeaders=true)\n-            \/\/ If UseCompactObjectHeaders=false: iter % 2 = 0\n-            \/\/ If UseCompactObjectHeaders=true:  iter % 2 = 1\n-            \/\/\n-            \/\/ byte:\n-            \/\/ adr = base + UNSAFE.ARRAY_BYTE_BASE_OFFSET + 1*iter\n-            \/\/              = 16 (or 12 if UseCompactObjectHeaders=true)\n-            \/\/ If UseCompactObjectHeaders=false: iter % 8 = 0\n-            \/\/ If UseCompactObjectHeaders=true:  iter % 8 = 4\n-            \/\/\n-            \/\/ -> we cannot align both if UseCompactObjectHeaders=true.\n@@ -243,1 +225,0 @@\n-        applyIfOr = { \"UseCompactObjectHeaders\", \"false\", \"AlignVector\", \"false\"},\n@@ -255,17 +236,0 @@\n-            \/\/ We have a mix of int and short loads\/stores.\n-            \/\/ With UseCompactObjectHeaders and AlignVector,\n-            \/\/ we must 8-byte align all vector loads\/stores.\n-            \/\/\n-            \/\/ int:\n-            \/\/ adr = base + UNSAFE.ARRAY_INT_BASE_OFFSET + 4*iter\n-            \/\/              = 16 (or 12 if UseCompactObjectHeaders=true)\n-            \/\/ If UseCompactObjectHeaders=false: iter % 2 = 0\n-            \/\/ If UseCompactObjectHeaders=true:  iter % 2 = 1\n-            \/\/\n-            \/\/ byte:\n-            \/\/ adr = base + UNSAFE.ARRAY_BYTE_BASE_OFFSET + 1*iter\n-            \/\/              = 16 (or 12 if UseCompactObjectHeaders=true)\n-            \/\/ If UseCompactObjectHeaders=false: iter % 8 = 0\n-            \/\/ If UseCompactObjectHeaders=true:  iter % 8 = 4\n-            \/\/\n-            \/\/ -> we cannot align both if UseCompactObjectHeaders=true.\n@@ -293,1 +257,0 @@\n-        applyIfOr = { \"UseCompactObjectHeaders\", \"false\", \"AlignVector\", \"false\"},\n@@ -303,17 +266,0 @@\n-            \/\/ We have a mix of int and short loads\/stores.\n-            \/\/ With UseCompactObjectHeaders and AlignVector,\n-            \/\/ we must 8-byte align all vector loads\/stores.\n-            \/\/\n-            \/\/ int:\n-            \/\/ adr = base + UNSAFE.ARRAY_INT_BASE_OFFSET + 4*iter\n-            \/\/              = 16 (or 12 if UseCompactObjectHeaders=true)\n-            \/\/ If UseCompactObjectHeaders=false: iter % 2 = 0\n-            \/\/ If UseCompactObjectHeaders=true:  iter % 2 = 1\n-            \/\/\n-            \/\/ byte:\n-            \/\/ adr = base + UNSAFE.ARRAY_BYTE_BASE_OFFSET + 1*iter\n-            \/\/              = 16 (or 12 if UseCompactObjectHeaders=true)\n-            \/\/ If UseCompactObjectHeaders=false: iter % 8 = 0\n-            \/\/ If UseCompactObjectHeaders=true:  iter % 8 = 4\n-            \/\/\n-            \/\/ -> we cannot align both if UseCompactObjectHeaders=true.\n@@ -326,1 +272,0 @@\n-        applyIfOr = { \"UseCompactObjectHeaders\", \"false\", \"AlignVector\", \"false\"},\n@@ -336,17 +281,0 @@\n-            \/\/ We have a mix of int and short loads\/stores.\n-            \/\/ With UseCompactObjectHeaders and AlignVector,\n-            \/\/ we must 8-byte align all vector loads\/stores.\n-            \/\/\n-            \/\/ int:\n-            \/\/ adr = base + UNSAFE.ARRAY_INT_BASE_OFFSET + 4*iter\n-            \/\/              = 16 (or 12 if UseCompactObjectHeaders=true)\n-            \/\/ If UseCompactObjectHeaders=false: iter % 2 = 0\n-            \/\/ If UseCompactObjectHeaders=true:  iter % 2 = 1\n-            \/\/\n-            \/\/ byte:\n-            \/\/ adr = base + UNSAFE.ARRAY_BYTE_BASE_OFFSET + 1*iter\n-            \/\/              = 16 (or 12 if UseCompactObjectHeaders=true)\n-            \/\/ If UseCompactObjectHeaders=false: iter % 8 = 0\n-            \/\/ If UseCompactObjectHeaders=true:  iter % 8 = 4\n-            \/\/\n-            \/\/ -> we cannot align both if UseCompactObjectHeaders=true.\n@@ -359,1 +287,0 @@\n-        applyIfOr = { \"UseCompactObjectHeaders\", \"false\", \"AlignVector\", \"false\"},\n@@ -369,1 +296,0 @@\n-            \/\/ same argument as in multipleOpsWith2DifferentTypesAndInvariant.\n@@ -376,1 +302,0 @@\n-        applyIfOr = { \"UseCompactObjectHeaders\", \"false\", \"AlignVector\", \"false\"},\n@@ -387,17 +312,0 @@\n-            \/\/ We have a mix of int and short loads\/stores.\n-            \/\/ With UseCompactObjectHeaders and AlignVector,\n-            \/\/ we must 8-byte align all vector loads\/stores.\n-            \/\/\n-            \/\/ int:\n-            \/\/ adr = base + UNSAFE.ARRAY_INT_BASE_OFFSET + 4*iter\n-            \/\/              = 16 (or 12 if UseCompactObjectHeaders=true)\n-            \/\/ If UseCompactObjectHeaders=false: iter % 2 = 0\n-            \/\/ If UseCompactObjectHeaders=true:  iter % 2 = 1\n-            \/\/\n-            \/\/ byte:\n-            \/\/ adr = base + UNSAFE.ARRAY_BYTE_BASE_OFFSET + 1*iter\n-            \/\/              = 16 (or 12 if UseCompactObjectHeaders=true)\n-            \/\/ If UseCompactObjectHeaders=false: iter % 8 = 0\n-            \/\/ If UseCompactObjectHeaders=true:  iter % 8 = 4\n-            \/\/\n-            \/\/ -> we cannot align both if UseCompactObjectHeaders=true.\n@@ -450,1 +358,0 @@\n-        applyIfOr = { \"UseCompactObjectHeaders\", \"false\", \"AlignVector\", \"false\"},\n@@ -457,9 +364,0 @@\n-            \/\/ Hand-unrolling can mess with alignment!\n-            \/\/\n-            \/\/ With UseCompactObjectHeaders and AlignVector,\n-            \/\/ we must 8-byte align all vector loads\/stores.\n-            \/\/\n-            \/\/ adr = base + UNSAFE.ARRAY_INT_BASE_OFFSET + 8*iter\n-            \/\/              = 16 (or 12 if UseCompactObjectHeaders=true)\n-            \/\/ If UseCompactObjectHeaders=false: 16 divisible by 8 -> vectorize\n-            \/\/ If UseCompactObjectHeaders=true:  12 not divisibly by 8 -> not vectorize\n","filename":"test\/hotspot\/jtreg\/compiler\/vectorization\/runner\/LoopCombinedOpTest.java","additions":0,"deletions":102,"binary":false,"changes":102,"status":"modified"},{"patch":"@@ -140,1 +140,1 @@\n-            int expectedShift = 6;\n+            int expectedShift = 9;\n@@ -144,1 +144,1 @@\n-            expectedShift = 8;\n+            expectedShift = 10;\n@@ -148,1 +148,1 @@\n-            expectedShift = 9;\n+            expectedShift = 10;\n","filename":"test\/hotspot\/jtreg\/runtime\/CompressedOops\/CompressedClassPointersEncodingScheme.java","additions":3,"deletions":3,"binary":false,"changes":6,"status":"modified"},{"patch":"@@ -77,1 +77,1 @@\n-    final static long maxClassSpaceSize = 4096 * MB;\n+    final static long maxClassSpaceSize = 512 * MB;\n@@ -142,5 +142,6 @@\n-                \/\/ Create archive\n-                pb = ProcessTools.createLimitedTestJavaProcessBuilder(\n-                        \"-XX:SharedArchiveFile=.\/abc.jsa\", \"-Xshare:dump\", \"-version\");\n-                output = new OutputAnalyzer(pb.start());\n-                output.shouldHaveExitValue(0);\n+                \/\/ TODO: Does not currently work with +UseCompactObjectHeaders.\n+                \/\/ \/\/ Create archive\n+                \/\/ pb = ProcessTools.createLimitedTestJavaProcessBuilder(\n+                \/\/         \"-XX:SharedArchiveFile=.\/abc.jsa\", \"-Xshare:dump\", \"-version\");\n+                \/\/ output = new OutputAnalyzer(pb.start());\n+                \/\/ output.shouldHaveExitValue(0);\n@@ -148,12 +149,12 @@\n-                \/\/ With CDS, class space should fill whatever the CDS archive leaves us (modulo alignment)\n-                pb = ProcessTools.createLimitedTestJavaProcessBuilder(\"-XX:CompressedClassSpaceSize=\" + maxClassSpaceSize,\n-                        \"-XX:SharedArchiveFile=.\/abc.jsa\", \"-Xshare:on\", \"-Xlog:metaspace*\", \"-version\");\n-                output = new OutputAnalyzer(pb.start());\n-                output.shouldHaveExitValue(0);\n-                long reducedSize = Long.parseLong(\n-                        output.firstMatch(\"reducing class space size from \" + maxClassSpaceSize + \" to (\\\\d+)\", 1));\n-                if (reducedSize < (maxClassSpaceSize - maxExpectedArchiveSize)) {\n-                    output.reportDiagnosticSummary();\n-                    throw new RuntimeException(\"Class space size too small?\");\n-                }\n-                output.shouldMatch(\"Compressed class space.*\" + reducedSize);\n+                \/\/ \/\/ With CDS, class space should fill whatever the CDS archive leaves us (modulo alignment)\n+                \/\/ pb = ProcessTools.createLimitedTestJavaProcessBuilder(\"-XX:CompressedClassSpaceSize=\" + maxClassSpaceSize,\n+                \/\/         \"-XX:SharedArchiveFile=.\/abc.jsa\", \"-Xshare:on\", \"-Xlog:metaspace*\", \"-version\");\n+                \/\/ output = new OutputAnalyzer(pb.start());\n+                \/\/ output.shouldHaveExitValue(0);\n+                \/\/ long reducedSize = Long.parseLong(\n+                \/\/         output.firstMatch(\"reducing class space size from \" + maxClassSpaceSize + \" to (\\\\d+)\", 1));\n+                \/\/ if (reducedSize < (maxClassSpaceSize - maxExpectedArchiveSize)) {\n+                \/\/     output.reportDiagnosticSummary();\n+                \/\/     throw new RuntimeException(\"Class space size too small?\");\n+                \/\/ }\n+                \/\/ output.shouldMatch(\"Compressed class space.*\" + reducedSize);\n","filename":"test\/hotspot\/jtreg\/runtime\/CompressedOops\/CompressedClassSpaceSize.java","additions":19,"deletions":18,"binary":false,"changes":37,"status":"modified"},{"patch":"@@ -117,3 +117,3 @@\n-            INT_OFFSET = 8;\n-            INT_ARRAY_OFFSET = 12;\n-            LONG_ARRAY_OFFSET = 16;\n+            INT_OFFSET = 4;\n+            INT_ARRAY_OFFSET = 8;\n+            LONG_ARRAY_OFFSET = 8;\n","filename":"test\/hotspot\/jtreg\/runtime\/FieldLayout\/BaseOffsets.java","additions":3,"deletions":3,"binary":false,"changes":6,"status":"modified"},{"patch":"@@ -101,1 +101,1 @@\n-                HEADER_SIZE_IN_BYTES = 8;\n+                HEADER_SIZE_IN_BYTES = 4;\n@@ -197,1 +197,1 @@\n-            (OOP_SIZE_IN_BYTES == 8 && HEADER_SIZE_IN_BYTES == 12)\n+            (OOP_SIZE_IN_BYTES == 8 && (HEADER_SIZE_IN_BYTES == 12 || HEADER_SIZE_IN_BYTES == 4))\n","filename":"test\/hotspot\/jtreg\/runtime\/FieldLayout\/TestOopMapSizeMinimal.java","additions":2,"deletions":2,"binary":false,"changes":4,"status":"modified"},{"patch":"@@ -77,0 +77,2 @@\n+            baseArgs.add(\"-XX:+UnlockExperimentalVMOptions\");\n+            baseArgs.add(\"-XX:-UseCompactObjectHeaders\");\n","filename":"test\/hotspot\/jtreg\/runtime\/cds\/DeterministicDump.java","additions":2,"deletions":0,"binary":false,"changes":2,"status":"modified"},{"patch":"@@ -30,0 +30,1 @@\n+ * @requires vm.opt.final.UseCompactObjectHeaders == false\n@@ -51,0 +52,1 @@\n+ * @requires vm.opt.final.UseCompactObjectHeaders == false\n","filename":"test\/hotspot\/jtreg\/runtime\/cds\/appcds\/TestParallelGCWithCDS.java","additions":2,"deletions":0,"binary":false,"changes":2,"status":"modified"},{"patch":"@@ -32,1 +32,1 @@\n- * @requires (vm.gc==\"null\")\n+ * @requires (vm.gc==\"null\") & vm.opt.final.UseCompactObjectHeaders == false\n","filename":"test\/hotspot\/jtreg\/runtime\/cds\/appcds\/cacheObject\/OpenArchiveRegion.java","additions":1,"deletions":1,"binary":false,"changes":2,"status":"modified"}]}