{"files":[{"patch":"@@ -37,1 +37,0 @@\n-#include \"gc\/shared\/fullGCForwarding.hpp\"\n@@ -247,2 +246,0 @@\n-\n-  FullGCForwarding::initialize_flags(heap_reserved_size_bytes());\n","filename":"src\/hotspot\/share\/gc\/g1\/g1Arguments.cpp","additions":0,"deletions":3,"binary":false,"changes":3,"status":"modified"},{"patch":"@@ -215,0 +215,2 @@\n+  FullGCForwarding::begin();\n+\n@@ -227,0 +229,2 @@\n+  FullGCForwarding::end();\n+\n","filename":"src\/hotspot\/share\/gc\/g1\/g1FullCollector.cpp","additions":4,"deletions":0,"binary":false,"changes":4,"status":"modified"},{"patch":"@@ -30,1 +30,0 @@\n-#include \"gc\/shared\/fullGCForwarding.hpp\"\n@@ -86,2 +85,0 @@\n-\n-  FullGCForwarding::initialize_flags(heap_reserved_size_bytes());\n","filename":"src\/hotspot\/share\/gc\/parallel\/parallelArguments.cpp","additions":0,"deletions":3,"binary":false,"changes":3,"status":"modified"},{"patch":"@@ -1052,0 +1052,2 @@\n+    FullGCForwarding::begin();\n+\n@@ -1058,0 +1060,2 @@\n+    FullGCForwarding::end();\n+\n","filename":"src\/hotspot\/share\/gc\/parallel\/psParallelCompact.cpp","additions":4,"deletions":0,"binary":false,"changes":4,"status":"modified"},{"patch":"@@ -26,2 +26,0 @@\n-#include \"gc\/shared\/fullGCForwarding.hpp\"\n-#include \"gc\/shared\/gcArguments.hpp\"\n@@ -31,5 +29,0 @@\n-void SerialArguments::initialize() {\n-  GCArguments::initialize();\n-  FullGCForwarding::initialize_flags(MaxHeapSize);\n-}\n-\n","filename":"src\/hotspot\/share\/gc\/serial\/serialArguments.cpp","additions":0,"deletions":7,"binary":false,"changes":7,"status":"modified"},{"patch":"@@ -34,1 +34,0 @@\n-  virtual void initialize();\n","filename":"src\/hotspot\/share\/gc\/serial\/serialArguments.hpp","additions":0,"deletions":1,"binary":false,"changes":1,"status":"modified"},{"patch":"@@ -700,0 +700,2 @@\n+  FullGCForwarding::begin();\n+\n@@ -743,0 +745,2 @@\n+  FullGCForwarding::end();\n+\n","filename":"src\/hotspot\/share\/gc\/serial\/serialFullGC.cpp","additions":4,"deletions":0,"binary":false,"changes":4,"status":"modified"},{"patch":"@@ -27,2 +27,6 @@\n-#include \"memory\/memRegion.hpp\"\n-#include \"runtime\/globals_extension.hpp\"\n+#include \"logging\/log.hpp\"\n+#include \"nmt\/memTag.hpp\"\n+#include \"utilities\/ostream.hpp\"\n+#include \"utilities\/concurrentHashTable.inline.hpp\"\n+#include \"utilities\/fastHash.hpp\"\n+#include \"utilities\/powerOfTwo.hpp\"\n@@ -30,2 +34,5 @@\n-HeapWord* FullGCForwarding::_heap_base = nullptr;\n-int FullGCForwarding::_num_low_bits = 0;\n+static uintx hash(HeapWord* const& addr) {\n+  uint64_t val = reinterpret_cast<uint64_t>(addr);\n+  uint32_t hash = FastHash::get_hash32((uint32_t)val, (uint32_t)(val >> 32));\n+  return hash;\n+}\n@@ -33,11 +40,29 @@\n-void FullGCForwarding::initialize_flags(size_t max_heap_size) {\n-#ifdef _LP64\n-  size_t max_narrow_heap_size = right_n_bits(NumLowBitsNarrow - Shift);\n-  if (UseCompactObjectHeaders && max_heap_size > max_narrow_heap_size * HeapWordSize) {\n-    warning(\"Compact object headers require a java heap size smaller than \" SIZE_FORMAT\n-            \"%s (given: \" SIZE_FORMAT \"%s). Disabling compact object headers.\",\n-            byte_size_in_proper_unit(max_narrow_heap_size * HeapWordSize),\n-            proper_unit_for_byte_size(max_narrow_heap_size * HeapWordSize),\n-            byte_size_in_proper_unit(max_heap_size),\n-            proper_unit_for_byte_size(max_heap_size));\n-    FLAG_SET_ERGO(UseCompactObjectHeaders, false);\n+struct ForwardingEntry {\n+  HeapWord* _from;\n+  HeapWord* _to;\n+  ForwardingEntry(HeapWord* from, HeapWord* to) : _from(from), _to(to) {}\n+};\n+\n+struct FallbackTableConfig {\n+  using Value = ForwardingEntry;\n+  static uintx get_hash(Value const& entry, bool* is_dead) {\n+    return hash(entry._from);\n+  }\n+  static void* allocate_node(void* context, size_t size, Value const& value) {\n+    return AllocateHeap(size, mtGC);\n+  }\n+  static void free_node(void* context, void* memory, Value const& value) {\n+    FreeHeap(memory);\n+  }\n+};\n+\n+class FallbackTable : public ConcurrentHashTable<FallbackTableConfig, mtGC> {\n+\n+};\n+\n+class FallbackTableLookup : public StackObj {\n+  ForwardingEntry const _entry;\n+public:\n+  explicit FallbackTableLookup(HeapWord* from) : _entry(from, nullptr) {}\n+  uintx get_hash() const {\n+    return hash(_entry._from);\n@@ -45,0 +70,20 @@\n+  bool equals(ForwardingEntry* value) {\n+    return _entry._from == value->_from;\n+  }\n+  bool is_dead(ForwardingEntry* value) { return false; }\n+};\n+\n+\/\/ We cannot use 0, because that may already be a valid base address in zero-based heaps.\n+\/\/ 0x1 is safe because heap base addresses must be aligned by much larger alignment\n+HeapWord* const FullGCForwarding::UNUSED_BASE = reinterpret_cast<HeapWord*>(0x1);\n+\n+HeapWord* FullGCForwarding::_heap_start = nullptr;\n+size_t FullGCForwarding::_heap_start_region_bias = 0;\n+size_t FullGCForwarding::_num_regions = 0;\n+uintptr_t FullGCForwarding::_region_mask = 0;\n+HeapWord** FullGCForwarding::_biased_bases = nullptr;\n+HeapWord** FullGCForwarding::_bases_table = nullptr;\n+FallbackTable* FullGCForwarding::_fallback_table = nullptr;\n+#ifndef PRODUCT\n+volatile uint64_t FullGCForwarding::_num_forwardings = 0;\n+volatile uint64_t FullGCForwarding::_num_fallback_forwardings = 0;\n@@ -46,1 +91,0 @@\n-}\n@@ -50,5 +94,32 @@\n-  _heap_base = heap.start();\n-  if (UseCompactObjectHeaders) {\n-    _num_low_bits = NumLowBitsNarrow;\n-  } else {\n-    _num_low_bits = NumLowBitsWide;\n+  _heap_start = heap.start();\n+\n+  size_t rounded_heap_size = round_up_power_of_2(heap.byte_size());\n+\n+  _num_regions = (rounded_heap_size \/ BytesPerWord) \/ BLOCK_SIZE_WORDS;\n+\n+  _heap_start_region_bias = (uintptr_t)_heap_start >> BLOCK_SIZE_BYTES_SHIFT;\n+  _region_mask = ~((uintptr_t(1) << BLOCK_SIZE_BYTES_SHIFT) - 1);\n+\n+  assert(_bases_table == nullptr, \"should not be initialized yet\");\n+  assert(_fallback_table == nullptr, \"should not be initialized yet\");\n+#endif\n+}\n+\n+void FullGCForwarding::begin() {\n+#ifdef _LP64\n+  assert(_bases_table == nullptr, \"should not be initialized yet\");\n+  assert(_fallback_table == nullptr, \"should not be initialized yet\");\n+\n+  _fallback_table = new FallbackTable();\n+\n+#ifndef PRODUCT\n+  _num_forwardings = 0;\n+  _num_fallback_forwardings = 0;\n+#endif\n+\n+  size_t max = _num_regions;\n+  _bases_table = NEW_C_HEAP_ARRAY(HeapWord*, max, mtGC);\n+  HeapWord** biased_start = _bases_table - _heap_start_region_bias;\n+  _biased_bases = biased_start;\n+  for (size_t i = 0; i < max; i++) {\n+    _bases_table[i] = UNUSED_BASE;\n@@ -58,0 +129,59 @@\n+\n+void FullGCForwarding::end() {\n+#ifndef PRODUCT\n+  log_info(gc)(\"Total forwardings: \" UINT64_FORMAT \", fallback forwardings: \" UINT64_FORMAT\n+                \", ratio: %f, memory used by fallback table: \" SIZE_FORMAT \"%s, memory used by bases table: \" SIZE_FORMAT \"%s\",\n+               _num_forwardings, _num_fallback_forwardings, (float)_num_forwardings\/(float)_num_fallback_forwardings,\n+               byte_size_in_proper_unit(_fallback_table->get_mem_size(Thread::current())),\n+               proper_unit_for_byte_size(_fallback_table->get_mem_size(Thread::current())),\n+               byte_size_in_proper_unit(sizeof(HeapWord*) * _num_regions),\n+               proper_unit_for_byte_size(sizeof(HeapWord*) * _num_regions));\n+#endif\n+#ifdef _LP64\n+  assert(_bases_table != nullptr, \"should be initialized\");\n+  FREE_C_HEAP_ARRAY(HeapWord*, _bases_table);\n+  _bases_table = nullptr;\n+  delete _fallback_table;\n+  _fallback_table = nullptr;\n+#endif\n+}\n+\n+void FullGCForwarding::fallback_forward_to(HeapWord* from, HeapWord* to) {\n+  assert(to != nullptr, \"no null forwarding\");\n+  assert(_fallback_table != nullptr, \"should be initialized\");\n+  FallbackTableLookup lookup_f(from);\n+  ForwardingEntry entry(from, to);\n+  auto found_f = [&](ForwardingEntry* found) {\n+    \/\/ If dupe has been found, override it with new value.\n+    \/\/ This is also called when new entry is succussfully inserted.\n+    if (found->_to != to) {\n+      found->_to = to;\n+    }\n+  };\n+  Thread* current_thread = Thread::current();\n+  bool grow;\n+  bool added = _fallback_table->insert_get(current_thread, lookup_f, entry, found_f, &grow);\n+  NOT_PRODUCT(Atomic::inc(&_num_fallback_forwardings);)\n+#ifdef ASSERT\n+  assert(fallback_forwardee(from) != nullptr, \"must have entered forwarding\");\n+  assert(fallback_forwardee(from) == to, \"forwarding must be correct, added: %s, from: \" PTR_FORMAT \", to: \" PTR_FORMAT \", fwd: \" PTR_FORMAT, BOOL_TO_STR(added), p2i(from), p2i(to), p2i(fallback_forwardee(from)));\n+#endif\n+  if (grow) {\n+    _fallback_table->grow(current_thread);\n+    tty->print_cr(\"grow fallback table to size: \" SIZE_FORMAT \" bytes\",\n+                  _fallback_table->get_mem_size(current_thread));\n+  }\n+}\n+\n+HeapWord* FullGCForwarding::fallback_forwardee(HeapWord* from) {\n+  assert(_fallback_table != nullptr, \"fallback table must be present\");\n+  HeapWord* result;\n+  FallbackTableLookup lookup_f(from);\n+  auto found_f = [&](ForwardingEntry* found) {\n+    result = found->_to;\n+  };\n+  bool found = _fallback_table->get(Thread::current(), lookup_f, found_f);\n+  assert(found, \"something must have been found\");\n+  assert(result != nullptr, \"must have found forwarding\");\n+  return result;\n+}\n","filename":"src\/hotspot\/share\/gc\/shared\/fullGCForwarding.cpp","additions":151,"deletions":21,"binary":false,"changes":172,"status":"modified"},{"patch":"@@ -28,1 +28,1 @@\n-#include \"memory\/allStatic.hpp\"\n+#include \"memory\/allocation.hpp\"\n@@ -33,10 +33,85 @@\n-\/*\n- * Implements forwarding for the Full GCs of Serial, Parallel, G1 and Shenandoah in\n- * a way that preserves upper N bits of object mark-words, which contain crucial\n- * Klass* information when running with compact headers. The encoding is similar to\n- * compressed-oops encoding: it basically subtracts the forwardee address from the\n- * heap-base, shifts that difference into the right place, and sets the lowest two\n- * bits (to indicate 'forwarded' state as usual).\n- * With compact-headers, we have 40 bits to encode forwarding pointers. This is\n- * enough to address 8TB of heap. If the heap size exceeds that limit, we turn off\n- * compact headers.\n+class FallbackTable;\n+class Mutex;\n+\n+\/**\n+ * FullGCForwarding is a method to store forwarding information in a compressed form into the object header,\n+ * that has been specifically designed for sliding compacting GCs and compact object headers. With compact object\n+ * headers, we store the compressed class pointer in the header, which would be overwritten by full forwarding\n+ * pointers, if we allow the legacy forwarding code to act. This would lose the class information for the object,\n+ * which is required later in GC cycle to iterate the reference fields and get the object size for copying.\n+ *\n+ * FullGCForwarding requires only small side tables and guarantees constant-time access and modification.\n+ *\n+ * The key advantage of sliding compaction for encoding efficiency:\n+ * - It forwards objects linearily, starting at the heap bottom and moving up to the top, sliding\n+ *   live objects towards the bottom of the heap. (The reality in parallel or regionalized GCs is a bit more\n+ *   complex, but conceptually it is the same.)\n+ * - Objects starting in any one block can only be forwarded to a memory region that is not larger than\n+ *   a block. (There are exceptions to this rule which are discussed below.)\n+ *\n+ * This is an intuitive property: when we slide the compact block full of data, it can not take up more\n+ * memory afterwards.\n+ * This property allows us to use a side table to record the addresses of the target memory region for\n+ * each block. The table holds N entries for N blocks. For each block, it gives the base\n+ * address of the target regions, or a special placeholder if not used.\n+ *\n+ * This encoding efficiency allows to store the forwarding information in the object header _together_ with the\n+ * compressed class pointer.\n+ *\n+ * The idea is to use a pointer compression scheme very similar to the one that is used for compressed oops.\n+ * We divide the heap into number of equal-sized blocks. Each block spans a maximum of 2^NUM_OFFSET_BITS words.\n+ * We maintain a side-table of target-base-addresses, with one address entry per block.\n+ *\n+ * When recording the sliding forwarding, the mark word would look roughly like this:\n+ *\n+ *   32                               0\n+ *    [.....................OOOOOOOOOTT]\n+ *                                    ^------ tag-bits, indicates 'forwarded'\n+ *                                  ^-------- in-region offset\n+ *                         ^----------------- protected area, *not touched* by this code, useful for\n+ *                                            compressed class pointer with compact object headers\n+ *\n+ * Adding a forwarding then generally works as follows:\n+ *   1. Compute the index of the block of the \"from\" address.\n+ *   2. Load the target-base-offset of the from-block from the side-table.\n+ *   3. If the base-offset is not-yet set, set it to the to-address of the forwarding.\n+ *      (In other words, the first forwarding of a block determines the target base-offset.)\n+ *   4. Compute the offset of the to-address in the target region.\n+ *   4. Store offset in the object header.\n+ *\n+ * Similarly, looking up the target address, given an original object address generally works as follows:\n+ *   1. Compute the index of the block of the \"from\" address.\n+ *   2. Load the target-base-offset of the from-block from the side-table.\n+ *   3. Extract the offset from the object header.\n+ *   4. Compute the \"to\" address from \"to\" region base and \"offset\"\n+ *\n+ * We reserve one special value for the offset:\n+ *  - 111111111: Indicates an exceptional forwarding (see below), for which a fallback hash-table\n+ *               is used to look up the target address.\n+ *\n+ * In order to support this, we need to make a change to the above algorithm:\n+ *  - Forwardings that would use offsets >= 111111111 (i.e. the last slot)\n+ *    would also need to use the fallback-table. We expect that to be relatively rare for two reasons:\n+ *    1. It only affects 1 out of 512 possible offsets, in other words, 1\/512th of all situations in an equal\n+ *       distribution.\n+ *    2. Forwardings are not equally-distributed, because normally we 'skip' unreachable objects,\n+ *       thus compacting the block. Forwardings tend to cluster at the beginning of the target region,\n+ *       and become less likely towards the end of the possible encodable target address range.\n+ *       Which means in reality it will be much less frequent than 1\/512.\n+ *\n+ * There are several conditions when the above algorithm would be broken because the assumption that\n+ * 'objects from each block can only get forwarded to a region of block-size' is violated:\n+ * - G1 last-ditch serial compaction: there, object from a single region can be forwarded to multiple,\n+ *   more than two regions. G1 serial compaction is not very common - it is the last-last-ditch GC\n+ *   that is used when the JVM is scrambling to squeeze more space out of the heap, and at that point,\n+ *   ultimate performance is no longer the main concern.\n+ * - When forwarding hits a space (or G1\/Shenandoah region) boundary, then latter objects of a block\n+ *   need to be forwarded to a different address range than earlier objects in the same block.\n+ *   This is rare.\n+ * - With compact identity hash-code, objects can grow, and in the worst case use up more memory in\n+ *   the target block than we can address. We expect that to be rare.\n+ *\n+ * To deal with that, we initialize a fallback-hashtable for storing those extra forwardings, and use a special\n+ * offset pattern (0b11...1) to indicate that the forwardee is not encoded but should be looked-up in the hashtable.\n+ * This implies that this particular offset (the last word of a block) can not be used directly as forwarding,\n+ * but also has to be handled by the fallback-table.\n@@ -45,3 +120,54 @@\n-  static const int NumLowBitsNarrow = LP64_ONLY(markWord::klass_shift) NOT_LP64(0 \/*unused*\/);\n-  static const int NumLowBitsWide   = BitsPerWord;\n-  static const int Shift            = markWord::lock_bits + markWord::lock_shift;\n+private:\n+  static const int AVAILABLE_LOW_BITS       = 11;\n+  static const int AVAILABLE_BITS_MASK      = right_n_bits(AVAILABLE_LOW_BITS);\n+  \/\/ The offset bits start after the lock-bits, which are currently used by Serial GC\n+  \/\/ for marking objects. Could be 1 for Serial GC when being clever with the bits,\n+  \/\/ and 0 for all other GCs.\n+  static const int OFFSET_BITS_SHIFT = markWord::lock_shift + markWord::lock_bits;\n+\n+  \/\/ How many bits we use for the offset\n+  static const int NUM_OFFSET_BITS = AVAILABLE_LOW_BITS - OFFSET_BITS_SHIFT;\n+  static const size_t BLOCK_SIZE_WORDS = 1 << NUM_OFFSET_BITS;\n+  static const int BLOCK_SIZE_BYTES_SHIFT = NUM_OFFSET_BITS + LogHeapWordSize;\n+  static const size_t MAX_OFFSET = BLOCK_SIZE_WORDS - 2;\n+  static const uintptr_t OFFSET_MASK = right_n_bits(NUM_OFFSET_BITS) << OFFSET_BITS_SHIFT;\n+\n+  \/\/ This offset bit-pattern indicates that the actual mapping is handled by the\n+  \/\/ fallback-table. This also implies that this cannot be used as a valid offset,\n+  \/\/ and we must also use the fallback-table for mappings to the last word of a\n+  \/\/ block.\n+  static const uintptr_t FALLBACK_PATTERN = right_n_bits(NUM_OFFSET_BITS);\n+  static const uintptr_t FALLBACK_PATTERN_IN_PLACE = FALLBACK_PATTERN << OFFSET_BITS_SHIFT;\n+\n+  \/\/ Indicates an unused base address in the target base table.\n+  static HeapWord* const UNUSED_BASE;\n+\n+  static HeapWord*      _heap_start;\n+\n+  static size_t         _heap_start_region_bias;\n+  static size_t         _num_regions;\n+  static uintptr_t      _region_mask;\n+\n+  \/\/ The target base table memory.\n+  static HeapWord**     _bases_table;\n+  \/\/ Entries into the target base tables, biased to the start of the heap.\n+  static HeapWord**     _biased_bases;\n+\n+  static FallbackTable* _fallback_table;\n+\n+#ifndef PRODUCT\n+  static volatile uint64_t _num_forwardings;\n+  static volatile uint64_t _num_fallback_forwardings;\n+#endif\n+\n+  static inline size_t biased_region_index_containing(HeapWord* addr);\n+\n+  static inline bool is_fallback(uintptr_t encoded);\n+  static inline uintptr_t encode_forwarding(HeapWord* from, HeapWord* to);\n+  static inline HeapWord* decode_forwarding(HeapWord* from, uintptr_t encoded);\n+\n+  static void fallback_forward_to(HeapWord* from, HeapWord* to);\n+  static HeapWord* fallback_forwardee(HeapWord* from);\n+\n+  static inline void forward_to_impl(oop from, oop to);\n+  static inline oop forwardee_impl(oop from);\n@@ -49,2 +175,0 @@\n-  static HeapWord* _heap_base;\n-  static int _num_low_bits;\n@@ -52,1 +176,0 @@\n-  static void initialize_flags(size_t max_heap_size);\n@@ -54,0 +177,7 @@\n+\n+  static void begin();\n+  static void end();\n+\n+  static inline bool is_forwarded(oop obj);\n+  static inline bool is_not_forwarded(oop obj);\n+\n@@ -56,1 +186,0 @@\n-  static inline bool is_forwarded(oop obj);\n","filename":"src\/hotspot\/share\/gc\/shared\/fullGCForwarding.hpp","additions":147,"deletions":18,"binary":false,"changes":165,"status":"modified"},{"patch":"@@ -22,1 +22,0 @@\n- *\n@@ -25,2 +24,2 @@\n-#ifndef GC_SHARED_FULLGCFORWARDING_INLINE_HPP\n-#define GC_SHARED_FULLGCFORWARDING_INLINE_HPP\n+#ifndef SHARE_GC_SHARED_FULLGCFORWARDING_INLINE_HPP\n+#define SHARE_GC_SHARED_FULLGCFORWARDING_INLINE_HPP\n@@ -28,0 +27,1 @@\n+#include \"gc\/shared\/gc_globals.hpp\"\n@@ -29,1 +29,1 @@\n-\n+#include \"oops\/markWord.hpp\"\n@@ -31,1 +31,36 @@\n-#include \"utilities\/globalDefinitions.hpp\"\n+#include \"utilities\/macros.hpp\"\n+\n+inline bool FullGCForwarding::is_forwarded(oop obj) {\n+  return obj->is_forwarded();\n+}\n+\n+size_t FullGCForwarding::biased_region_index_containing(HeapWord* addr) {\n+  return (uintptr_t)addr >> BLOCK_SIZE_BYTES_SHIFT;\n+}\n+\n+bool FullGCForwarding::is_fallback(uintptr_t encoded) {\n+  return (encoded & OFFSET_MASK) == FALLBACK_PATTERN_IN_PLACE;\n+}\n+\n+uintptr_t FullGCForwarding::encode_forwarding(HeapWord* from, HeapWord* to) {\n+  size_t from_block_idx = biased_region_index_containing(from);\n+\n+  HeapWord* to_region_base = _biased_bases[from_block_idx];\n+  if (to_region_base == UNUSED_BASE) {\n+    _biased_bases[from_block_idx] = to_region_base = to;\n+  }\n+\n+  \/\/ Avoid pointer_delta() on purpose: using an unsigned subtraction,\n+  \/\/ we get an underflow when to < to_region_base, which means\n+  \/\/ we can use a single comparison instead of:\n+  \/\/ if (to_region_base > to || (to - to_region_base) > MAX_OFFSET) { .. }\n+  size_t offset = size_t(to - to_region_base);\n+  if (offset > MAX_OFFSET) {\n+    offset = FALLBACK_PATTERN;\n+  }\n+  uintptr_t encoded = (offset << OFFSET_BITS_SHIFT) | markWord::marked_value;\n+\n+  assert(is_fallback(encoded) || to == decode_forwarding(from, encoded), \"must be reversible\");\n+  assert((encoded & ~AVAILABLE_BITS_MASK) == 0, \"must encode to available bits\");\n+  return encoded;\n+}\n@@ -33,1 +68,34 @@\n-void FullGCForwarding::forward_to(oop from, oop to) {\n+HeapWord* FullGCForwarding::decode_forwarding(HeapWord* from, uintptr_t encoded) {\n+  assert(!is_fallback(encoded), \"must not be fallback-forwarded, encoded: \" INTPTR_FORMAT \", OFFSET_MASK: \" INTPTR_FORMAT \", FALLBACK_PATTERN_IN_PLACE: \" INTPTR_FORMAT, encoded, OFFSET_MASK, FALLBACK_PATTERN_IN_PLACE);\n+  assert((encoded & ~AVAILABLE_BITS_MASK) == 0, \"must decode from available bits, encoded: \" INTPTR_FORMAT, encoded);\n+  uintptr_t offset = (encoded >> OFFSET_BITS_SHIFT);\n+\n+  size_t from_idx = biased_region_index_containing(from);\n+  HeapWord* base = _biased_bases[from_idx];\n+  assert(base != UNUSED_BASE, \"must not be unused base: encoded: \" INTPTR_FORMAT, encoded);\n+  HeapWord* decoded = base + offset;\n+  assert(decoded >= _heap_start,\n+         \"Address must be above heap start. encoded: \" INTPTR_FORMAT \", base: \" PTR_FORMAT,\n+          encoded, p2i(base));\n+\n+  return decoded;\n+}\n+\n+inline void FullGCForwarding::forward_to_impl(oop from, oop to) {\n+  assert(_bases_table != nullptr, \"call begin() before forwarding\");\n+\n+  markWord from_header = from->mark();\n+  HeapWord* from_hw = cast_from_oop<HeapWord*>(from);\n+  HeapWord* to_hw   = cast_from_oop<HeapWord*>(to);\n+  uintptr_t encoded = encode_forwarding(from_hw, to_hw);\n+  markWord new_header = markWord((from_header.value() & ~OFFSET_MASK) | encoded);\n+  from->set_mark(new_header);\n+\n+  if (is_fallback(encoded)) {\n+    fallback_forward_to(from_hw, to_hw);\n+  }\n+  NOT_PRODUCT(Atomic::inc(&_num_forwardings);)\n+}\n+\n+inline void FullGCForwarding::forward_to(oop obj, oop fwd) {\n+  assert(fwd != nullptr, \"no null forwarding\");\n@@ -35,6 +103,3 @@\n-  uintptr_t encoded = pointer_delta(cast_from_oop<HeapWord*>(to), _heap_base) << Shift;\n-  assert(encoded <= static_cast<uintptr_t>(right_n_bits(_num_low_bits)), \"encoded forwardee must fit\");\n-  uintptr_t mark = from->mark().value();\n-  mark &= ~right_n_bits(_num_low_bits);\n-  mark |= (encoded | markWord::marked_value);\n-  from->set_mark(markWord(mark));\n+  assert(_bases_table != nullptr, \"expect sliding forwarding initialized\");\n+  forward_to_impl(obj, fwd);\n+  assert(forwardee(obj) == fwd, \"must be forwarded to correct forwardee, obj: \" PTR_FORMAT \", forwardee(obj): \" PTR_FORMAT \", fwd: \" PTR_FORMAT \", mark: \" INTPTR_FORMAT, p2i(obj), p2i(forwardee(obj)), p2i(fwd), obj->mark().value());\n@@ -42,1 +107,1 @@\n-  from->forward_to(to);\n+  obj->forward_to(fwd);\n@@ -46,1 +111,15 @@\n-oop FullGCForwarding::forwardee(oop from) {\n+inline oop FullGCForwarding::forwardee_impl(oop from) {\n+  assert(_bases_table != nullptr, \"call begin() before asking for forwarding\");\n+\n+  markWord header = from->mark();\n+  HeapWord* from_hw = cast_from_oop<HeapWord*>(from);\n+  if (is_fallback(header.value())) {\n+    HeapWord* to = fallback_forwardee(from_hw);\n+    return cast_to_oop(to);\n+  }\n+  uintptr_t encoded = header.value() & OFFSET_MASK;\n+  HeapWord* to = decode_forwarding(from_hw, encoded);\n+  return cast_to_oop(to);\n+}\n+\n+inline oop FullGCForwarding::forwardee(oop obj) {\n@@ -48,3 +127,2 @@\n-  uintptr_t mark = from->mark().value();\n-  HeapWord* decoded = _heap_base + ((mark & right_n_bits(_num_low_bits)) >> Shift);\n-  return cast_to_oop(decoded);\n+  assert(_bases_table != nullptr, \"expect sliding forwarding initialized\");\n+  return forwardee_impl(obj);\n@@ -52,1 +130,1 @@\n-  return from->forwardee();\n+  return obj->forwardee();\n@@ -56,5 +134,1 @@\n-bool FullGCForwarding::is_forwarded(oop obj) {\n-  return obj->mark().is_forwarded();\n-}\n-\n-#endif \/\/ GC_SHARED_FULLGCFORWARDING_INLINE_HPP\n+#endif \/\/ SHARE_GC_SHARED_FULLGCFORWARDING_INLINE_HPP\n","filename":"src\/hotspot\/share\/gc\/shared\/fullGCForwarding.inline.hpp","additions":97,"deletions":23,"binary":false,"changes":120,"status":"modified"},{"patch":"@@ -27,1 +27,0 @@\n-#include \"gc\/shared\/fullGCForwarding.hpp\"\n@@ -189,2 +188,0 @@\n-\n-  FullGCForwarding::initialize_flags(MaxHeapSize);\n","filename":"src\/hotspot\/share\/gc\/shenandoah\/shenandoahArguments.cpp","additions":0,"deletions":3,"binary":false,"changes":3,"status":"modified"},{"patch":"@@ -248,0 +248,2 @@\n+    FullGCForwarding::begin();\n+\n@@ -257,0 +259,2 @@\n+\n+    FullGCForwarding::end();\n","filename":"src\/hotspot\/share\/gc\/shenandoah\/shenandoahFullGC.cpp","additions":4,"deletions":0,"binary":false,"changes":4,"status":"modified"},{"patch":"@@ -0,0 +1,102 @@\n+\/*\n+ * Copyright (c) 2023, Oracle and\/or its affiliates. All rights reserved.\n+ * DO NOT ALTER OR REMOVE COPYRIGHT NOTICES OR THIS FILE HEADER.\n+ *\n+ * This code is free software; you can redistribute it and\/or modify it\n+ * under the terms of the GNU General Public License version 2 only, as\n+ * published by the Free Software Foundation.\n+ *\n+ * This code is distributed in the hope that it will be useful, but WITHOUT\n+ * ANY WARRANTY; without even the implied warranty of MERCHANTABILITY or\n+ * FITNESS FOR A PARTICULAR PURPOSE.  See the GNU General Public License\n+ * version 2 for more details (a copy is included in the LICENSE file that\n+ * accompanied this code).\n+ *\n+ * You should have received a copy of the GNU General Public License version\n+ * 2 along with this work; if not, write to the Free Software Foundation,\n+ * Inc., 51 Franklin St, Fifth Floor, Boston, MA 02110-1301 USA.\n+ *\n+ * Please contact Oracle, 500 Oracle Parkway, Redwood Shores, CA 94065 USA\n+ * or visit www.oracle.com if you need additional information or have any\n+ * questions.\n+ *\n+ *\/\n+\n+#ifndef SHARE_UTILITIES_FASTHASH_HPP\n+#define SHARE_UTILITIES_FASTHASH_HPP\n+\n+#include \"memory\/allStatic.hpp\"\n+\n+class FastHash : public AllStatic {\n+private:\n+  static void fullmul64(uint64_t& hi, uint64_t& lo, uint64_t op1, uint64_t op2) {\n+#if defined(__SIZEOF_INT128__)\n+    __uint128_t prod = static_cast<__uint128_t>(op1) * static_cast<__uint128_t>(op2);\n+    hi = static_cast<uint64_t>(prod >> 64);\n+    lo = static_cast<uint64_t>(prod >>  0);\n+#else\n+    \/* First calculate all of the cross products. *\/\n+    uint64_t lo_lo = (op1 & 0xFFFFFFFF) * (op2 & 0xFFFFFFFF);\n+    uint64_t hi_lo = (op1 >> 32)        * (op2 & 0xFFFFFFFF);\n+    uint64_t lo_hi = (op1 & 0xFFFFFFFF) * (op2 >> 32);\n+    uint64_t hi_hi = (op1 >> 32)        * (op2 >> 32);\n+\n+    \/* Now add the products together. These will never overflow. *\/\n+    uint64_t cross = (lo_lo >> 32) + (hi_lo & 0xFFFFFFFF) + lo_hi;\n+    uint64_t upper = (hi_lo >> 32) + (cross >> 32)        + hi_hi;\n+    hi = upper;\n+    lo = (cross << 32) | (lo_lo & 0xFFFFFFFF);\n+#endif\n+  }\n+\n+  static void fullmul32(uint32_t& hi, uint32_t& lo, uint32_t op1, uint32_t op2) {\n+    uint64_t x64 = op1, y64 = op2, xy64 = x64 * y64;\n+    hi = (uint32_t)(xy64 >> 32);\n+    lo = (uint32_t)(xy64 >>  0);\n+  }\n+\n+  static uint64_t ror64(uint64_t x, uint64_t distance) {\n+    distance = distance & (64 - 1);\n+    return (x >> distance) | (x << (64 - distance));\n+  }\n+\n+  static uint32_t ror32(uint32_t x, uint32_t distance) {\n+    distance = distance & (32 - 1);\n+    return (x >> distance) | (x << (32 - distance));\n+  }\n+\n+public:\n+  static uint64_t get_hash64(uint64_t x, uint64_t y) {\n+    const uint64_t M  = 0x8ADAE89C337954D5;\n+    const uint64_t A  = 0xAAAAAAAAAAAAAAAA; \/\/ REPAA\n+    const uint64_t H0 = (x ^ y), L0 = (x ^ A);\n+\n+    uint64_t U0, V0; fullmul64(U0, V0, L0, M);\n+    const uint64_t Q0 = (H0 * M);\n+    const uint64_t L1 = (Q0 ^ U0);\n+\n+    uint64_t U1, V1; fullmul64(U1, V1, L1, M);\n+    const uint64_t P1 = (V0 ^ M);\n+    const uint64_t Q1 = ror64(P1, L1);\n+    const uint64_t L2 = (Q1 ^ U1);\n+    return V1 ^ L2;\n+  }\n+\n+  static uint32_t get_hash32(uint32_t x, uint32_t y) {\n+    const uint32_t M  = 0x337954D5;\n+    const uint32_t A  = 0xAAAAAAAA; \/\/ REPAA\n+    const uint32_t H0 = (x ^ y), L0 = (x ^ A);\n+\n+    uint32_t U0, V0; fullmul32(U0, V0, L0, M);\n+    const uint32_t Q0 = (H0 * M);\n+    const uint32_t L1 = (Q0 ^ U0);\n+\n+    uint32_t U1, V1; fullmul32(U1, V1, L1, M);\n+    const uint32_t P1 = (V0 ^ M);\n+    const uint32_t Q1 = ror32(P1, L1);\n+    const uint32_t L2 = (Q1 ^ U1);\n+    return V1 ^ L2;\n+  }\n+};\n+\n+#endif\/\/ SHARE_UTILITIES_FASTHASH_HPP\n","filename":"src\/hotspot\/share\/utilities\/fastHash.hpp","additions":102,"deletions":0,"binary":false,"changes":102,"status":"added"},{"patch":"@@ -59,0 +59,2 @@\n+  FullGCForwarding::begin();\n+\n@@ -77,0 +79,2 @@\n+\n+  FullGCForwarding::end();\n","filename":"test\/hotspot\/gtest\/gc\/shared\/test_preservedMarks.cpp","additions":4,"deletions":0,"binary":false,"changes":4,"status":"modified"}]}