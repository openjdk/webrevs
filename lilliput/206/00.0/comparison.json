{"files":[{"patch":"@@ -4,0 +4,1 @@\n+\/\/ Copyright 2025 Arm Limited and\/or its affiliates.\n@@ -884,0 +885,40 @@\n+\/\/ Class for vector register V10\n+reg_class v10_veca_reg(\n+    V10, V10_H, V10_J, V10_K\n+);\n+\n+\/\/ Class for vector register V11\n+reg_class v11_veca_reg(\n+    V11, V11_H, V11_J, V11_K\n+);\n+\n+\/\/ Class for vector register V12\n+reg_class v12_veca_reg(\n+    V12, V12_H, V12_J, V12_K\n+);\n+\n+\/\/ Class for vector register V13\n+reg_class v13_veca_reg(\n+    V13, V13_H, V13_J, V13_K\n+);\n+\n+\/\/ Class for vector register V17\n+reg_class v17_veca_reg(\n+    V17, V17_H, V17_J, V17_K\n+);\n+\n+\/\/ Class for vector register V18\n+reg_class v18_veca_reg(\n+    V18, V18_H, V18_J, V18_K\n+);\n+\n+\/\/ Class for vector register V23\n+reg_class v23_veca_reg(\n+    V23, V23_H, V23_J, V23_K\n+);\n+\n+\/\/ Class for vector register V24\n+reg_class v24_veca_reg(\n+    V24, V24_H, V24_J, V24_K\n+);\n+\n@@ -1157,1 +1198,0 @@\n-  static int emit_exception_handler(C2_MacroAssembler *masm);\n@@ -1160,5 +1200,1 @@\n-  static uint size_exception_handler() {\n-    return MacroAssembler::far_codestub_branch_size();\n-  }\n-\n-    \/\/ count one adr and one far branch instruction\n+    \/\/ count one branch instruction and one far call instruction sequence\n@@ -1229,2 +1265,2 @@\n-    _ANY_REG32_mask = _ALL_REG32_mask;\n-    _ANY_REG32_mask.Remove(OptoReg::as_OptoReg(r31_sp->as_VMReg()));\n+    _ANY_REG32_mask.assignFrom(_ALL_REG32_mask);\n+    _ANY_REG32_mask.remove(OptoReg::as_OptoReg(r31_sp->as_VMReg()));\n@@ -1232,1 +1268,1 @@\n-    _ANY_REG_mask = _ALL_REG_mask;\n+    _ANY_REG_mask.assignFrom(_ALL_REG_mask);\n@@ -1234,1 +1270,1 @@\n-    _PTR_REG_mask = _ALL_REG_mask;\n+    _PTR_REG_mask.assignFrom(_ALL_REG_mask);\n@@ -1236,2 +1272,2 @@\n-    _NO_SPECIAL_REG32_mask = _ALL_REG32_mask;\n-    _NO_SPECIAL_REG32_mask.SUBTRACT(_NON_ALLOCATABLE_REG32_mask);\n+    _NO_SPECIAL_REG32_mask.assignFrom(_ALL_REG32_mask);\n+    _NO_SPECIAL_REG32_mask.subtract(_NON_ALLOCATABLE_REG32_mask);\n@@ -1239,2 +1275,2 @@\n-    _NO_SPECIAL_REG_mask = _ALL_REG_mask;\n-    _NO_SPECIAL_REG_mask.SUBTRACT(_NON_ALLOCATABLE_REG_mask);\n+    _NO_SPECIAL_REG_mask.assignFrom(_ALL_REG_mask);\n+    _NO_SPECIAL_REG_mask.subtract(_NON_ALLOCATABLE_REG_mask);\n@@ -1242,2 +1278,2 @@\n-    _NO_SPECIAL_PTR_REG_mask = _ALL_REG_mask;\n-    _NO_SPECIAL_PTR_REG_mask.SUBTRACT(_NON_ALLOCATABLE_REG_mask);\n+    _NO_SPECIAL_PTR_REG_mask.assignFrom(_ALL_REG_mask);\n+    _NO_SPECIAL_PTR_REG_mask.subtract(_NON_ALLOCATABLE_REG_mask);\n@@ -1248,3 +1284,3 @@\n-      _NO_SPECIAL_REG32_mask.Remove(OptoReg::as_OptoReg(r27->as_VMReg()));\n-      _NO_SPECIAL_REG_mask.Remove(OptoReg::as_OptoReg(r27->as_VMReg()));\n-      _NO_SPECIAL_PTR_REG_mask.Remove(OptoReg::as_OptoReg(r27->as_VMReg()));\n+      _NO_SPECIAL_REG32_mask.remove(OptoReg::as_OptoReg(r27->as_VMReg()));\n+      _NO_SPECIAL_REG_mask.remove(OptoReg::as_OptoReg(r27->as_VMReg()));\n+      _NO_SPECIAL_PTR_REG_mask.remove(OptoReg::as_OptoReg(r27->as_VMReg()));\n@@ -1255,3 +1291,3 @@\n-      _NO_SPECIAL_REG32_mask.Remove(OptoReg::as_OptoReg(r29->as_VMReg()));\n-      _NO_SPECIAL_REG_mask.Remove(OptoReg::as_OptoReg(r29->as_VMReg()));\n-      _NO_SPECIAL_PTR_REG_mask.Remove(OptoReg::as_OptoReg(r29->as_VMReg()));\n+      _NO_SPECIAL_REG32_mask.remove(OptoReg::as_OptoReg(r29->as_VMReg()));\n+      _NO_SPECIAL_REG_mask.remove(OptoReg::as_OptoReg(r29->as_VMReg()));\n+      _NO_SPECIAL_PTR_REG_mask.remove(OptoReg::as_OptoReg(r29->as_VMReg()));\n@@ -1260,2 +1296,2 @@\n-    _NO_SPECIAL_NO_RFP_PTR_REG_mask = _NO_SPECIAL_PTR_REG_mask;\n-    _NO_SPECIAL_NO_RFP_PTR_REG_mask.Remove(OptoReg::as_OptoReg(r29->as_VMReg()));\n+    _NO_SPECIAL_NO_RFP_PTR_REG_mask.assignFrom(_NO_SPECIAL_PTR_REG_mask);\n+    _NO_SPECIAL_NO_RFP_PTR_REG_mask.remove(OptoReg::as_OptoReg(r29->as_VMReg()));\n@@ -1697,1 +1733,1 @@\n-const RegMask& MachConstantBaseNode::_out_RegMask = RegMask::Empty;\n+const RegMask& MachConstantBaseNode::_out_RegMask = RegMask::EMPTY;\n@@ -2224,19 +2260,0 @@\n-\/\/ Emit exception handler code.\n-int HandlerImpl::emit_exception_handler(C2_MacroAssembler* masm)\n-{\n-  \/\/ mov rscratch1 #exception_blob_entry_point\n-  \/\/ br rscratch1\n-  \/\/ Note that the code buffer's insts_mark is always relative to insts.\n-  \/\/ That's why we must use the macroassembler to generate a handler.\n-  address base = __ start_a_stub(size_exception_handler());\n-  if (base == nullptr) {\n-    ciEnv::current()->record_failure(\"CodeCache is full\");\n-    return 0;  \/\/ CodeBuffer::expand failed\n-  }\n-  int offset = __ offset();\n-  __ far_jump(RuntimeAddress(OptoRuntime::exception_blob()->entry_point()));\n-  assert(__ offset() - offset <= (int) size_exception_handler(), \"overflow\");\n-  __ end_a_stub();\n-  return offset;\n-}\n-\n@@ -2253,0 +2270,1 @@\n+\n@@ -2254,0 +2272,3 @@\n+  Label start;\n+  __ bind(start);\n+  __ far_call(RuntimeAddress(SharedRuntime::deopt_blob()->unpack()));\n@@ -2255,2 +2276,2 @@\n-  __ adr(lr, __ pc());\n-  __ far_jump(RuntimeAddress(SharedRuntime::deopt_blob()->unpack()));\n+  int entry_offset = __ offset();\n+  __ b(start);\n@@ -2259,0 +2280,2 @@\n+  assert(__ offset() - entry_offset >= NativePostCallNop::first_check_size,\n+         \"out of bounds read in post-call NOP check\");\n@@ -2260,1 +2283,1 @@\n-  return offset;\n+  return entry_offset;\n@@ -2365,9 +2388,25 @@\n-  int max_size = max_vector_size(bt);\n-  \/\/ Limit the min vector size to 8 bytes.\n-  int size = 8 \/ type2aelembytes(bt);\n-  if (bt == T_BYTE) {\n-    \/\/ To support vector api shuffle\/rearrange.\n-    size = 4;\n-  } else if (bt == T_BOOLEAN) {\n-    \/\/ To support vector api load\/store mask.\n-    size = 2;\n+  \/\/ Usually, the shortest vector length supported by AArch64 ISA and\n+  \/\/ Vector API species is 64 bits. However, we allow 32-bit or 16-bit\n+  \/\/ vectors in a few special cases.\n+  int size;\n+  switch(bt) {\n+    case T_BOOLEAN:\n+      \/\/ Load\/store a vector mask with only 2 elements for vector types\n+      \/\/ such as \"2I\/2F\/2L\/2D\".\n+      size = 2;\n+      break;\n+    case T_BYTE:\n+      \/\/ Generate a \"4B\" vector, to support vector cast between \"8B\/16B\"\n+      \/\/ and \"4S\/4I\/4L\/4F\/4D\".\n+      size = 4;\n+      break;\n+    case T_SHORT:\n+      \/\/ Generate a \"2S\" vector, to support vector cast between \"4S\/8S\"\n+      \/\/ and \"2I\/2L\/2F\/2D\".\n+      size = 2;\n+      break;\n+    default:\n+      \/\/ Limit the min vector length to 64-bit.\n+      size = 8 \/ type2aelembytes(bt);\n+      \/\/ The number of elements in a vector should be at least 2.\n+      size = MAX2(size, 2);\n@@ -2375,1 +2414,2 @@\n-  if (size < 2) size = 2;\n+\n+  int max_size = max_vector_size(bt);\n@@ -2466,1 +2506,1 @@\n-  \/\/ (_NO_SPECIAL_REG32_mask.Size() minus 1) forces CallNode to become\n+  \/\/ (_NO_SPECIAL_REG32_mask.size() minus 1) forces CallNode to become\n@@ -2469,1 +2509,1 @@\n-  uint default_int_pressure_threshold = _NO_SPECIAL_REG32_mask.Size() - 1;\n+  uint default_int_pressure_threshold = _NO_SPECIAL_REG32_mask.size() - 1;\n@@ -2484,1 +2524,1 @@\n-  return (FLOATPRESSURE == -1) ? _FLOAT_REG_mask.Size() : FLOATPRESSURE;\n+  return (FLOATPRESSURE == -1) ? _FLOAT_REG_mask.size() : FLOATPRESSURE;\n@@ -2491,1 +2531,1 @@\n-RegMask Matcher::divI_proj_mask() {\n+const RegMask& Matcher::divI_proj_mask() {\n@@ -2493,1 +2533,1 @@\n-  return RegMask();\n+  return RegMask::EMPTY;\n@@ -2497,1 +2537,1 @@\n-RegMask Matcher::modI_proj_mask() {\n+const RegMask& Matcher::modI_proj_mask() {\n@@ -2499,1 +2539,1 @@\n-  return RegMask();\n+  return RegMask::EMPTY;\n@@ -2503,1 +2543,1 @@\n-RegMask Matcher::divL_proj_mask() {\n+const RegMask& Matcher::divL_proj_mask() {\n@@ -2505,1 +2545,1 @@\n-  return RegMask();\n+  return RegMask::EMPTY;\n@@ -2509,1 +2549,1 @@\n-RegMask Matcher::modL_proj_mask() {\n+const RegMask& Matcher::modL_proj_mask() {\n@@ -2511,5 +2551,1 @@\n-  return RegMask();\n-}\n-\n-const RegMask Matcher::method_handle_invoke_SP_save_mask() {\n-  return FP_REG_mask();\n+  return RegMask::EMPTY;\n@@ -3338,1 +3374,1 @@\n-  enc_class aarch64_enc_cmpxchg_acq(memory mem, iRegLNoSp oldval, iRegLNoSp newval) %{\n+  enc_class aarch64_enc_cmpxchg_acq(memory mem, iRegL oldval, iRegL newval) %{\n@@ -3345,1 +3381,1 @@\n-  enc_class aarch64_enc_cmpxchgw_acq(memory mem, iRegINoSp oldval, iRegINoSp newval) %{\n+  enc_class aarch64_enc_cmpxchgw_acq(memory mem, iRegI oldval, iRegI newval) %{\n@@ -3352,1 +3388,1 @@\n-  enc_class aarch64_enc_cmpxchgs_acq(memory mem, iRegINoSp oldval, iRegINoSp newval) %{\n+  enc_class aarch64_enc_cmpxchgs_acq(memory mem, iRegI oldval, iRegI newval) %{\n@@ -3359,1 +3395,1 @@\n-  enc_class aarch64_enc_cmpxchgb_acq(memory mem, iRegINoSp oldval, iRegINoSp newval) %{\n+  enc_class aarch64_enc_cmpxchgb_acq(memory mem, iRegI oldval, iRegI newval) %{\n@@ -3367,1 +3403,1 @@\n-  enc_class aarch64_enc_cset_eq(iRegINoSp res) %{\n+  enc_class aarch64_enc_cset_eq(iRegI res) %{\n@@ -3453,4 +3489,0 @@\n-  enc_class aarch64_enc_mov_byte_map_base(iRegP dst, immByteMapBase src) %{\n-    __ load_byte_map_base($dst$$Register);\n-  %}\n-\n@@ -4362,1 +4394,1 @@\n-operand immI8_shift8()\n+operand immIDupV()\n@@ -4364,2 +4396,1 @@\n-  predicate((n->get_int() <= 127 && n->get_int() >= -128) ||\n-            (n->get_int() <= 32512 && n->get_int() >= -32768 && (n->get_int() & 0xff) == 0));\n+  predicate(Assembler::operand_valid_for_sve_dup_immediate((int64_t)n->get_int()));\n@@ -4374,1 +4405,1 @@\n-operand immL8_shift8()\n+operand immLDupV()\n@@ -4376,2 +4407,1 @@\n-  predicate((n->get_long() <= 127 && n->get_long() >= -128) ||\n-            (n->get_long() <= 32512 && n->get_long() >= -32768 && (n->get_long() & 0xff) == 0));\n+  predicate(Assembler::operand_valid_for_sve_dup_immediate(n->get_long()));\n@@ -4385,0 +4415,11 @@\n+\/\/ 8 bit signed value (simm8), or #simm8 LSL 8.\n+operand immHDupV()\n+%{\n+  predicate(Assembler::operand_valid_for_sve_dup_immediate((int64_t)n->geth()));\n+  match(ConH);\n+\n+  op_cost(0);\n+  format %{ %}\n+  interface(CONST_INTER);\n+%}\n+\n@@ -4557,14 +4598,0 @@\n-\/\/ Card Table Byte Map Base\n-operand immByteMapBase()\n-%{\n-  \/\/ Get base of card map\n-  predicate(BarrierSet::barrier_set()->is_a(BarrierSet::CardTableBarrierSet) &&\n-            SHENANDOAHGC_ONLY(!BarrierSet::barrier_set()->is_a(BarrierSet::ShenandoahBarrierSet) &&)\n-            (CardTable::CardValue*)n->get_ptr() == ((CardTableBarrierSet*)(BarrierSet::barrier_set()))->card_table()->byte_map_base());\n-  match(ConP);\n-\n-  op_cost(0);\n-  format %{ %}\n-  interface(CONST_INTER);\n-%}\n-\n@@ -4973,0 +5000,80 @@\n+operand vReg_V10()\n+%{\n+  constraint(ALLOC_IN_RC(v10_veca_reg));\n+  match(vReg);\n+\n+  op_cost(0);\n+  format %{ %}\n+  interface(REG_INTER);\n+%}\n+\n+operand vReg_V11()\n+%{\n+  constraint(ALLOC_IN_RC(v11_veca_reg));\n+  match(vReg);\n+\n+  op_cost(0);\n+  format %{ %}\n+  interface(REG_INTER);\n+%}\n+\n+operand vReg_V12()\n+%{\n+  constraint(ALLOC_IN_RC(v12_veca_reg));\n+  match(vReg);\n+\n+  op_cost(0);\n+  format %{ %}\n+  interface(REG_INTER);\n+%}\n+\n+operand vReg_V13()\n+%{\n+  constraint(ALLOC_IN_RC(v13_veca_reg));\n+  match(vReg);\n+\n+  op_cost(0);\n+  format %{ %}\n+  interface(REG_INTER);\n+%}\n+\n+operand vReg_V17()\n+%{\n+  constraint(ALLOC_IN_RC(v17_veca_reg));\n+  match(vReg);\n+\n+  op_cost(0);\n+  format %{ %}\n+  interface(REG_INTER);\n+%}\n+\n+operand vReg_V18()\n+%{\n+  constraint(ALLOC_IN_RC(v18_veca_reg));\n+  match(vReg);\n+\n+  op_cost(0);\n+  format %{ %}\n+  interface(REG_INTER);\n+%}\n+\n+operand vReg_V23()\n+%{\n+  constraint(ALLOC_IN_RC(v23_veca_reg));\n+  match(vReg);\n+\n+  op_cost(0);\n+  format %{ %}\n+  interface(REG_INTER);\n+%}\n+\n+operand vReg_V24()\n+%{\n+  constraint(ALLOC_IN_RC(v24_veca_reg));\n+  match(vReg);\n+\n+  op_cost(0);\n+  format %{ %}\n+  interface(REG_INTER);\n+%}\n+\n@@ -5843,3 +5950,0 @@\n-\n-  \/\/ List of nop instructions\n-  nops( MachNop );\n@@ -6861,14 +6965,0 @@\n-\/\/ Load Byte Map Base Constant\n-\n-instruct loadByteMapBase(iRegPNoSp dst, immByteMapBase con)\n-%{\n-  match(Set dst con);\n-\n-  ins_cost(INSN_COST);\n-  format %{ \"adr  $dst, $con\\t# Byte Map Base\" %}\n-\n-  ins_encode(aarch64_enc_mov_byte_map_base(dst, con));\n-\n-  ins_pipe(ialu_imm);\n-%}\n-\n@@ -6979,3 +7069,0 @@\n-\/\/ The \"ldr\" instruction loads a 32-bit word from the constant pool into a\n-\/\/ 32-bit register but only the bottom half will be populated and the top\n-\/\/ 16 bits are zero.\n@@ -6984,3 +7071,3 @@\n-  format %{\n-    \"ldrs $dst, [$constantaddress]\\t# load from constant table: half float=$con\\n\\t\"\n-  %}\n+  format %{ \"mov    rscratch1, $con\\n\\t\"\n+            \"fmov   $dst, rscratch1\"\n+         %}\n@@ -6988,1 +7075,2 @@\n-    __ ldrs(as_FloatRegister($dst$$reg), $constantaddress($con));\n+    __ movw(rscratch1, (uint32_t)$con$$constant);\n+    __ fmovs($dst$$FloatRegister, rscratch1);\n@@ -6990,1 +7078,1 @@\n-  ins_pipe(fp_load_constant_s);\n+  ins_pipe(pipe_class_default);\n@@ -8305,1 +8393,1 @@\n-instruct compareAndSwapB(iRegINoSp res, indirect mem, iRegINoSp oldval, iRegINoSp newval, rFlagsReg cr) %{\n+instruct compareAndSwapB(iRegINoSp res, indirect mem, iRegI oldval, iRegI newval, rFlagsReg cr) %{\n@@ -8323,1 +8411,1 @@\n-instruct compareAndSwapS(iRegINoSp res, indirect mem, iRegINoSp oldval, iRegINoSp newval, rFlagsReg cr) %{\n+instruct compareAndSwapS(iRegINoSp res, indirect mem, iRegI oldval, iRegI newval, rFlagsReg cr) %{\n@@ -8341,1 +8429,1 @@\n-instruct compareAndSwapI(iRegINoSp res, indirect mem, iRegINoSp oldval, iRegINoSp newval, rFlagsReg cr) %{\n+instruct compareAndSwapI(iRegINoSp res, indirect mem, iRegI oldval, iRegI newval, rFlagsReg cr) %{\n@@ -8359,1 +8447,1 @@\n-instruct compareAndSwapL(iRegINoSp res, indirect mem, iRegLNoSp oldval, iRegLNoSp newval, rFlagsReg cr) %{\n+instruct compareAndSwapL(iRegINoSp res, indirect mem, iRegL oldval, iRegL newval, rFlagsReg cr) %{\n@@ -8396,1 +8484,1 @@\n-instruct compareAndSwapN(iRegINoSp res, indirect mem, iRegNNoSp oldval, iRegNNoSp newval, rFlagsReg cr) %{\n+instruct compareAndSwapN(iRegINoSp res, indirect mem, iRegN oldval, iRegN newval, rFlagsReg cr) %{\n@@ -8417,1 +8505,1 @@\n-instruct compareAndSwapBAcq(iRegINoSp res, indirect mem, iRegINoSp oldval, iRegINoSp newval, rFlagsReg cr) %{\n+instruct compareAndSwapBAcq(iRegINoSp res, indirect mem, iRegI oldval, iRegI newval, rFlagsReg cr) %{\n@@ -8436,1 +8524,1 @@\n-instruct compareAndSwapSAcq(iRegINoSp res, indirect mem, iRegINoSp oldval, iRegINoSp newval, rFlagsReg cr) %{\n+instruct compareAndSwapSAcq(iRegINoSp res, indirect mem, iRegI oldval, iRegI newval, rFlagsReg cr) %{\n@@ -8455,1 +8543,1 @@\n-instruct compareAndSwapIAcq(iRegINoSp res, indirect mem, iRegINoSp oldval, iRegINoSp newval, rFlagsReg cr) %{\n+instruct compareAndSwapIAcq(iRegINoSp res, indirect mem, iRegI oldval, iRegI newval, rFlagsReg cr) %{\n@@ -8474,1 +8562,1 @@\n-instruct compareAndSwapLAcq(iRegINoSp res, indirect mem, iRegLNoSp oldval, iRegLNoSp newval, rFlagsReg cr) %{\n+instruct compareAndSwapLAcq(iRegINoSp res, indirect mem, iRegL oldval, iRegL newval, rFlagsReg cr) %{\n@@ -8512,1 +8600,1 @@\n-instruct compareAndSwapNAcq(iRegINoSp res, indirect mem, iRegNNoSp oldval, iRegNNoSp newval, rFlagsReg cr) %{\n+instruct compareAndSwapNAcq(iRegINoSp res, indirect mem, iRegN oldval, iRegN newval, rFlagsReg cr) %{\n@@ -16185,1 +16273,0 @@\n-  predicate(LockingMode != LM_LIGHTWEIGHT);\n@@ -16199,33 +16286,1 @@\n-instruct cmpFastUnlock(rFlagsReg cr, iRegP object, iRegP box, iRegPNoSp tmp, iRegPNoSp tmp2)\n-%{\n-  predicate(LockingMode != LM_LIGHTWEIGHT);\n-  match(Set cr (FastUnlock object box));\n-  effect(TEMP tmp, TEMP tmp2);\n-\n-  ins_cost(5 * INSN_COST);\n-  format %{ \"fastunlock $object,$box\\t! kills $tmp, $tmp2\" %}\n-\n-  ins_encode %{\n-    __ fast_unlock($object$$Register, $box$$Register, $tmp$$Register, $tmp2$$Register);\n-  %}\n-\n-  ins_pipe(pipe_serial);\n-%}\n-\n-instruct cmpFastLockLightweight(rFlagsReg cr, iRegP object, iRegP box, iRegPNoSp tmp, iRegPNoSp tmp2, iRegPNoSp tmp3)\n-%{\n-  predicate(LockingMode == LM_LIGHTWEIGHT);\n-  match(Set cr (FastLock object box));\n-  effect(TEMP tmp, TEMP tmp2, TEMP tmp3);\n-\n-  ins_cost(5 * INSN_COST);\n-  format %{ \"fastlock $object,$box\\t! kills $tmp,$tmp2,$tmp3\" %}\n-\n-  ins_encode %{\n-    __ fast_lock_lightweight($object$$Register, $box$$Register, $tmp$$Register, $tmp2$$Register, $tmp3$$Register);\n-  %}\n-\n-  ins_pipe(pipe_serial);\n-%}\n-\n-instruct cmpFastUnlockLightweight(rFlagsReg cr, iRegP object, iRegP box, iRegPNoSp tmp, iRegPNoSp tmp2, iRegPNoSp tmp3)\n+instruct cmpFastUnlock(rFlagsReg cr, iRegP object, iRegP box, iRegPNoSp tmp, iRegPNoSp tmp2, iRegPNoSp tmp3)\n@@ -16233,1 +16288,0 @@\n-  predicate(LockingMode == LM_LIGHTWEIGHT);\n@@ -16241,1 +16295,1 @@\n-    __ fast_unlock_lightweight($object$$Register, $box$$Register, $tmp$$Register, $tmp2$$Register, $tmp3$$Register);\n+    __ fast_unlock($object$$Register, $box$$Register, $tmp$$Register, $tmp2$$Register, $tmp3$$Register);\n","filename":"src\/hotspot\/cpu\/aarch64\/aarch64.ad","additions":226,"deletions":172,"binary":false,"changes":398,"status":"modified"},{"patch":"@@ -151,9 +151,3 @@\n-class RelocActions {\n-protected:\n-  typedef int (*reloc_insn)(address insn_addr, address &target);\n-  virtual reloc_insn adrpMem() = 0;\n-  virtual reloc_insn adrpAdd() = 0;\n-  virtual reloc_insn adrpMovk() = 0;\n-\n-  const address _insn_addr;\n-  const uint32_t _insn;\n+static uint32_t insn_at(address insn_addr, int n) {\n+  return ((uint32_t*)insn_addr)[n];\n+}\n@@ -162,6 +156,2 @@\n-  static uint32_t insn_at(address insn_addr, int n) {\n-    return ((uint32_t*)insn_addr)[n];\n-  }\n-  uint32_t insn_at(int n) const {\n-    return insn_at(_insn_addr, n);\n-  }\n+template<typename T>\n+class RelocActions : public AllStatic {\n@@ -171,14 +161,1 @@\n-  RelocActions(address insn_addr) : _insn_addr(insn_addr), _insn(insn_at(insn_addr, 0)) {}\n-  RelocActions(address insn_addr, uint32_t insn)\n-    :  _insn_addr(insn_addr), _insn(insn) {}\n-\n-  virtual int unconditionalBranch(address insn_addr, address &target) = 0;\n-  virtual int conditionalBranch(address insn_addr, address &target) = 0;\n-  virtual int testAndBranch(address insn_addr, address &target) = 0;\n-  virtual int loadStore(address insn_addr, address &target) = 0;\n-  virtual int adr(address insn_addr, address &target) = 0;\n-  virtual int adrp(address insn_addr, address &target, reloc_insn inner) = 0;\n-  virtual int immediate(address insn_addr, address &target) = 0;\n-  virtual void verify(address insn_addr, address &target) = 0;\n-\n-  int ALWAYSINLINE run(address insn_addr, address &target) {\n+  static int ALWAYSINLINE run(address insn_addr, address &target) {\n@@ -186,0 +163,1 @@\n+    uint32_t insn = insn_at(insn_addr, 0);\n@@ -187,1 +165,1 @@\n-    uint32_t dispatch = Instruction_aarch64::extract(_insn, 30, 25);\n+    uint32_t dispatch = Instruction_aarch64::extract(insn, 30, 25);\n@@ -191,1 +169,1 @@\n-        instructions = unconditionalBranch(insn_addr, target);\n+        instructions = T::unconditionalBranch(insn_addr, target);\n@@ -196,2 +174,2 @@\n-        instructions = conditionalBranch(insn_addr, target);\n-          break;\n+        instructions = T::conditionalBranch(insn_addr, target);\n+        break;\n@@ -200,1 +178,1 @@\n-        instructions = testAndBranch(insn_addr, target);\n+        instructions = T::testAndBranch(insn_addr, target);\n@@ -212,1 +190,1 @@\n-        if ((Instruction_aarch64::extract(_insn, 29, 24) & 0b111011) == 0b011000) {\n+        if ((Instruction_aarch64::extract(insn, 29, 24) & 0b111011) == 0b011000) {\n@@ -214,1 +192,1 @@\n-          instructions = loadStore(insn_addr, target);\n+          instructions = T::loadStore(insn_addr, target);\n@@ -227,2 +205,2 @@\n-        assert(Instruction_aarch64::extract(_insn, 28, 24) == 0b10000, \"must be\");\n-        int shift = Instruction_aarch64::extract(_insn, 31, 31);\n+        assert(Instruction_aarch64::extract(insn, 28, 24) == 0b10000, \"must be\");\n+        int shift = Instruction_aarch64::extract(insn, 31, 31);\n@@ -230,1 +208,1 @@\n-          uint32_t insn2 = insn_at(1);\n+          uint32_t insn2 = insn_at(insn_addr, 1);\n@@ -232,1 +210,1 @@\n-              Instruction_aarch64::extract(_insn, 4, 0) ==\n+              Instruction_aarch64::extract(insn, 4, 0) ==\n@@ -234,1 +212,1 @@\n-            instructions = adrp(insn_addr, target, adrpMem());\n+            instructions = T::adrp(insn_addr, target, T::adrpMem);\n@@ -236,1 +214,1 @@\n-                     Instruction_aarch64::extract(_insn, 4, 0) ==\n+                     Instruction_aarch64::extract(insn, 4, 0) ==\n@@ -238,1 +216,1 @@\n-            instructions = adrp(insn_addr, target, adrpAdd());\n+            instructions = T::adrp(insn_addr, target, T::adrpAdd);\n@@ -240,1 +218,1 @@\n-                     Instruction_aarch64::extract(_insn, 4, 0) ==\n+                     Instruction_aarch64::extract(insn, 4, 0) ==\n@@ -242,1 +220,1 @@\n-            instructions = adrp(insn_addr, target, adrpMovk());\n+            instructions = T::adrp(insn_addr, target, T::adrpMovk);\n@@ -247,1 +225,1 @@\n-          instructions = adr(insn_addr, target);\n+          instructions = T::adr(insn_addr, target);\n@@ -255,1 +233,1 @@\n-        instructions = immediate(insn_addr, target);\n+        instructions = T::immediate(insn_addr, target);\n@@ -263,1 +241,1 @@\n-    verify(insn_addr, target);\n+    T::verify(insn_addr, target);\n@@ -268,5 +246,1 @@\n-class Patcher : public RelocActions {\n-  virtual reloc_insn adrpMem() { return &Patcher::adrpMem_impl; }\n-  virtual reloc_insn adrpAdd() { return &Patcher::adrpAdd_impl; }\n-  virtual reloc_insn adrpMovk() { return &Patcher::adrpMovk_impl; }\n-\n+class Patcher : public AllStatic {\n@@ -274,3 +248,1 @@\n-  Patcher(address insn_addr) : RelocActions(insn_addr) {}\n-\n-  virtual int unconditionalBranch(address insn_addr, address &target) {\n+  static int unconditionalBranch(address insn_addr, address &target) {\n@@ -281,1 +253,1 @@\n-  virtual int conditionalBranch(address insn_addr, address &target) {\n+  static int conditionalBranch(address insn_addr, address &target) {\n@@ -286,1 +258,1 @@\n-  virtual int testAndBranch(address insn_addr, address &target) {\n+  static int testAndBranch(address insn_addr, address &target) {\n@@ -291,1 +263,1 @@\n-  virtual int loadStore(address insn_addr, address &target) {\n+  static int loadStore(address insn_addr, address &target) {\n@@ -296,1 +268,1 @@\n-  virtual int adr(address insn_addr, address &target) {\n+  static int adr(address insn_addr, address &target) {\n@@ -298,1 +270,1 @@\n-    assert(Instruction_aarch64::extract(_insn, 28, 24) == 0b10000, \"must be\");\n+    assert(Instruction_aarch64::extract(insn_at(insn_addr, 0), 28, 24) == 0b10000, \"must be\");\n@@ -308,1 +280,2 @@\n-  virtual int adrp(address insn_addr, address &target, reloc_insn inner) {\n+  template<typename U>\n+  static int adrp(address insn_addr, address &target, U inner) {\n@@ -311,1 +284,1 @@\n-    assert(Instruction_aarch64::extract(_insn, 28, 24) == 0b10000, \"must be\");\n+    assert(Instruction_aarch64::extract(insn_at(insn_addr, 0), 28, 24) == 0b10000, \"must be\");\n@@ -318,1 +291,1 @@\n-    instructions = (*inner)(insn_addr, adjusted_target);\n+    instructions = inner(insn_addr, adjusted_target);\n@@ -328,1 +301,1 @@\n-  static int adrpMem_impl(address insn_addr, address &target) {\n+  static int adrpMem(address insn_addr, address &target) {\n@@ -337,1 +310,1 @@\n-  static int adrpAdd_impl(address insn_addr, address &target) {\n+  static int adrpAdd(address insn_addr, address &target) {\n@@ -343,1 +316,1 @@\n-  static int adrpMovk_impl(address insn_addr, address &target) {\n+  static int adrpMovk(address insn_addr, address &target) {\n@@ -350,2 +323,2 @@\n-  virtual int immediate(address insn_addr, address &target) {\n-    assert(Instruction_aarch64::extract(_insn, 31, 21) == 0b11010010100, \"must be\");\n+  static int immediate(address insn_addr, address &target) {\n+    assert(Instruction_aarch64::extract(insn_at(insn_addr, 0), 31, 21) == 0b11010010100, \"must be\");\n@@ -361,1 +334,1 @@\n-  virtual void verify(address insn_addr, address &target) {\n+  static void verify(address insn_addr, address &target) {\n@@ -395,5 +368,1 @@\n-class AArch64Decoder : public RelocActions {\n-  virtual reloc_insn adrpMem() { return &AArch64Decoder::adrpMem_impl; }\n-  virtual reloc_insn adrpAdd() { return &AArch64Decoder::adrpAdd_impl; }\n-  virtual reloc_insn adrpMovk() { return &AArch64Decoder::adrpMovk_impl; }\n-\n+class AArch64Decoder : public AllStatic {\n@@ -401,3 +370,2 @@\n-  AArch64Decoder(address insn_addr, uint32_t insn) : RelocActions(insn_addr, insn) {}\n-  virtual int loadStore(address insn_addr, address &target) {\n-    intptr_t offset = Instruction_aarch64::sextract(_insn, 23, 5);\n+  static int loadStore(address insn_addr, address &target) {\n+    intptr_t offset = Instruction_aarch64::sextract(insn_at(insn_addr, 0), 23, 5);\n@@ -408,2 +376,2 @@\n-  virtual int unconditionalBranch(address insn_addr, address &target) {\n-    intptr_t offset = Instruction_aarch64::sextract(_insn, 25, 0);\n+  static int unconditionalBranch(address insn_addr, address &target) {\n+    intptr_t offset = Instruction_aarch64::sextract(insn_at(insn_addr, 0), 25, 0);\n@@ -413,2 +381,2 @@\n-  virtual int conditionalBranch(address insn_addr, address &target) {\n-    intptr_t offset = Instruction_aarch64::sextract(_insn, 23, 5);\n+  static int conditionalBranch(address insn_addr, address &target) {\n+    intptr_t offset = Instruction_aarch64::sextract(insn_at(insn_addr, 0), 23, 5);\n@@ -418,2 +386,2 @@\n-  virtual int testAndBranch(address insn_addr, address &target) {\n-    intptr_t offset = Instruction_aarch64::sextract(_insn, 18, 5);\n+  static int testAndBranch(address insn_addr, address &target) {\n+    intptr_t offset = Instruction_aarch64::sextract(insn_at(insn_addr, 0), 18, 5);\n@@ -423,1 +391,1 @@\n-  virtual int adr(address insn_addr, address &target) {\n+  static int adr(address insn_addr, address &target) {\n@@ -425,2 +393,3 @@\n-    intptr_t offset = Instruction_aarch64::extract(_insn, 30, 29);\n-    offset |= Instruction_aarch64::sextract(_insn, 23, 5) << 2;\n+    uint32_t insn = insn_at(insn_addr, 0);\n+    intptr_t offset = Instruction_aarch64::extract(insn, 30, 29);\n+    offset |= Instruction_aarch64::sextract(insn, 23, 5) << 2;\n@@ -430,4 +399,6 @@\n-  virtual int adrp(address insn_addr, address &target, reloc_insn inner) {\n-    assert(Instruction_aarch64::extract(_insn, 28, 24) == 0b10000, \"must be\");\n-    intptr_t offset = Instruction_aarch64::extract(_insn, 30, 29);\n-    offset |= Instruction_aarch64::sextract(_insn, 23, 5) << 2;\n+  template<typename U>\n+  static int adrp(address insn_addr, address &target, U inner) {\n+    uint32_t insn = insn_at(insn_addr, 0);\n+    assert(Instruction_aarch64::extract(insn, 28, 24) == 0b10000, \"must be\");\n+    intptr_t offset = Instruction_aarch64::extract(insn, 30, 29);\n+    offset |= Instruction_aarch64::sextract(insn, 23, 5) << 2;\n@@ -438,1 +409,1 @@\n-    uint32_t insn2 = insn_at(1);\n+    uint32_t insn2 = insn_at(insn_addr, 1);\n@@ -441,1 +412,1 @@\n-    (*inner)(insn_addr, target);\n+    inner(insn_addr, target);\n@@ -444,1 +415,1 @@\n-  static int adrpMem_impl(address insn_addr, address &target) {\n+  static int adrpMem(address insn_addr, address &target) {\n@@ -453,1 +424,1 @@\n-  static int adrpAdd_impl(address insn_addr, address &target) {\n+  static int adrpAdd(address insn_addr, address &target) {\n@@ -460,1 +431,1 @@\n-  static int adrpMovk_impl(address insn_addr, address &target) {\n+  static int adrpMovk(address insn_addr, address &target) {\n@@ -479,1 +450,1 @@\n-  virtual int immediate(address insn_addr, address &target) {\n+  static int immediate(address insn_addr, address &target) {\n@@ -481,1 +452,1 @@\n-    assert(Instruction_aarch64::extract(_insn, 31, 21) == 0b11010010100, \"must be\");\n+    assert(Instruction_aarch64::extract(insns[0], 31, 21) == 0b11010010100, \"must be\");\n@@ -485,3 +456,3 @@\n-    target = address(uint64_t(Instruction_aarch64::extract(_insn, 20, 5))\n-                 + (uint64_t(Instruction_aarch64::extract(insns[1], 20, 5)) << 16)\n-                 + (uint64_t(Instruction_aarch64::extract(insns[2], 20, 5)) << 32));\n+    target = address(uint64_t(Instruction_aarch64::extract(insns[0], 20, 5))\n+                  + (uint64_t(Instruction_aarch64::extract(insns[1], 20, 5)) << 16)\n+                  + (uint64_t(Instruction_aarch64::extract(insns[2], 20, 5)) << 32));\n@@ -492,1 +463,1 @@\n-  virtual void verify(address insn_addr, address &target) {\n+  static void verify(address insn_addr, address &target) {\n@@ -496,2 +467,1 @@\n-address MacroAssembler::target_addr_for_insn(address insn_addr, uint32_t insn) {\n-  AArch64Decoder decoder(insn_addr, insn);\n+address MacroAssembler::target_addr_for_insn(address insn_addr) {\n@@ -499,1 +469,1 @@\n-  decoder.run(insn_addr, target);\n+  RelocActions<AArch64Decoder>::run(insn_addr, target);\n@@ -506,2 +476,1 @@\n-  Patcher patcher(insn_addr);\n-  return patcher.run(insn_addr, target);\n+  return RelocActions<Patcher>::run(insn_addr, target);\n@@ -549,2 +518,2 @@\n-address MacroAssembler::target_addr_for_insn_or_null(address insn_addr, unsigned insn) {\n-  if (NativeInstruction::is_ldrw_to_zr(address(&insn))) {\n+address MacroAssembler::target_addr_for_insn_or_null(address insn_addr) {\n+  if (NativeInstruction::is_ldrw_to_zr(insn_addr)) {\n@@ -553,1 +522,1 @@\n-  return MacroAssembler::target_addr_for_insn(insn_addr, insn);\n+  return MacroAssembler::target_addr_for_insn(insn_addr);\n@@ -637,2 +606,0 @@\n-  str(last_java_sp, Address(rthread, JavaThread::last_Java_sp_offset()));\n-\n@@ -643,0 +610,3 @@\n+\n+  \/\/ We must set sp last.\n+  str(last_java_sp, Address(rthread, JavaThread::last_Java_sp_offset()));\n@@ -776,4 +746,0 @@\n-static bool is_preemptable(address entry_point) {\n-  return entry_point == CAST_FROM_FN_PTR(address, InterpreterRuntime::monitorenter);\n-}\n-\n@@ -783,0 +749,1 @@\n+                                  Label*   return_pc,\n@@ -815,6 +782,1 @@\n-  if (is_preemptable(entry_point)) {\n-    \/\/ skip setting last_pc since we already set it to desired value.\n-    set_last_Java_frame(last_java_sp, rfp, noreg, rscratch1);\n-  } else {\n-    set_last_Java_frame(last_java_sp, rfp, l, rscratch1);\n-  }\n+  set_last_Java_frame(last_java_sp, rfp, return_pc != nullptr ? *return_pc : l, rscratch1);\n@@ -855,1 +817,1 @@\n-  call_VM_base(oop_result, noreg, noreg, entry_point, number_of_arguments, check_exceptions);\n+  call_VM_base(oop_result, noreg, noreg, nullptr, entry_point, number_of_arguments, check_exceptions);\n@@ -1113,1 +1075,1 @@\n-  call_VM_base(oop_result, rthread, last_java_sp, entry_point, number_of_arguments, check_exceptions);\n+  call_VM_base(oop_result, rthread, last_java_sp, nullptr, entry_point, number_of_arguments, check_exceptions);\n@@ -2262,1 +2224,1 @@\n-    snprintf(buffer, sizeof(buffer), \"0x%\" PRIX64, (uint64_t)imm64);\n+    os::snprintf_checked(buffer, sizeof(buffer), \"0x%\" PRIX64, (uint64_t)imm64);\n@@ -2320,1 +2282,1 @@\n-    snprintf(buffer, sizeof(buffer), \"0x%\" PRIX64, imm64);\n+    os::snprintf_checked(buffer, sizeof(buffer), \"0x%\" PRIX64, imm64);\n@@ -2433,1 +2395,1 @@\n-      snprintf(buffer, sizeof(buffer), \"0x%\" PRIX32, imm32);\n+      os::snprintf_checked(buffer, sizeof(buffer), \"0x%\" PRIX32, imm32);\n@@ -2905,1 +2867,1 @@\n-      snprintf(buffer, sizeof(buffer), \"push_fp: %d SVE registers\", count);\n+      os::snprintf_checked(buffer, sizeof(buffer), \"push_fp: %d SVE registers\", count);\n@@ -2907,1 +2869,1 @@\n-      snprintf(buffer, sizeof(buffer), \"push_fp: %d Neon registers\", count);\n+      os::snprintf_checked(buffer, sizeof(buffer), \"push_fp: %d Neon registers\", count);\n@@ -2909,1 +2871,1 @@\n-      snprintf(buffer, sizeof(buffer), \"push_fp: %d fp registers\", count);\n+      os::snprintf_checked(buffer, sizeof(buffer), \"push_fp: %d fp registers\", count);\n@@ -3017,1 +2979,1 @@\n-      snprintf(buffer, sizeof(buffer), \"pop_fp: %d SVE registers\", count);\n+      os::snprintf_checked(buffer, sizeof(buffer), \"pop_fp: %d SVE registers\", count);\n@@ -3019,1 +2981,1 @@\n-      snprintf(buffer, sizeof(buffer), \"pop_fp: %d Neon registers\", count);\n+      os::snprintf_checked(buffer, sizeof(buffer), \"pop_fp: %d Neon registers\", count);\n@@ -3021,1 +2983,1 @@\n-      snprintf(buffer, sizeof(buffer), \"pop_fp: %d fp registers\", count);\n+      os::snprintf_checked(buffer, sizeof(buffer), \"pop_fp: %d fp registers\", count);\n@@ -3356,91 +3318,0 @@\n-\/\/ this simulates the behaviour of the x86 cmpxchg instruction using a\n-\/\/ load linked\/store conditional pair. we use the acquire\/release\n-\/\/ versions of these instructions so that we flush pending writes as\n-\/\/ per Java semantics.\n-\n-\/\/ n.b the x86 version assumes the old value to be compared against is\n-\/\/ in rax and updates rax with the value located in memory if the\n-\/\/ cmpxchg fails. we supply a register for the old value explicitly\n-\n-\/\/ the aarch64 load linked\/store conditional instructions do not\n-\/\/ accept an offset. so, unlike x86, we must provide a plain register\n-\/\/ to identify the memory word to be compared\/exchanged rather than a\n-\/\/ register+offset Address.\n-\n-void MacroAssembler::cmpxchgptr(Register oldv, Register newv, Register addr, Register tmp,\n-                                Label &succeed, Label *fail) {\n-  \/\/ oldv holds comparison value\n-  \/\/ newv holds value to write in exchange\n-  \/\/ addr identifies memory word to compare against\/update\n-  if (UseLSE) {\n-    mov(tmp, oldv);\n-    casal(Assembler::xword, oldv, newv, addr);\n-    cmp(tmp, oldv);\n-    br(Assembler::EQ, succeed);\n-    membar(AnyAny);\n-  } else {\n-    Label retry_load, nope;\n-    prfm(Address(addr), PSTL1STRM);\n-    bind(retry_load);\n-    \/\/ flush and load exclusive from the memory location\n-    \/\/ and fail if it is not what we expect\n-    ldaxr(tmp, addr);\n-    cmp(tmp, oldv);\n-    br(Assembler::NE, nope);\n-    \/\/ if we store+flush with no intervening write tmp will be zero\n-    stlxr(tmp, newv, addr);\n-    cbzw(tmp, succeed);\n-    \/\/ retry so we only ever return after a load fails to compare\n-    \/\/ ensures we don't return a stale value after a failed write.\n-    b(retry_load);\n-    \/\/ if the memory word differs we return it in oldv and signal a fail\n-    bind(nope);\n-    membar(AnyAny);\n-    mov(oldv, tmp);\n-  }\n-  if (fail)\n-    b(*fail);\n-}\n-\n-void MacroAssembler::cmpxchg_obj_header(Register oldv, Register newv, Register obj, Register tmp,\n-                                        Label &succeed, Label *fail) {\n-  assert(oopDesc::mark_offset_in_bytes() == 0, \"assumption\");\n-  cmpxchgptr(oldv, newv, obj, tmp, succeed, fail);\n-}\n-\n-void MacroAssembler::cmpxchgw(Register oldv, Register newv, Register addr, Register tmp,\n-                                Label &succeed, Label *fail) {\n-  \/\/ oldv holds comparison value\n-  \/\/ newv holds value to write in exchange\n-  \/\/ addr identifies memory word to compare against\/update\n-  \/\/ tmp returns 0\/1 for success\/failure\n-  if (UseLSE) {\n-    mov(tmp, oldv);\n-    casal(Assembler::word, oldv, newv, addr);\n-    cmp(tmp, oldv);\n-    br(Assembler::EQ, succeed);\n-    membar(AnyAny);\n-  } else {\n-    Label retry_load, nope;\n-    prfm(Address(addr), PSTL1STRM);\n-    bind(retry_load);\n-    \/\/ flush and load exclusive from the memory location\n-    \/\/ and fail if it is not what we expect\n-    ldaxrw(tmp, addr);\n-    cmp(tmp, oldv);\n-    br(Assembler::NE, nope);\n-    \/\/ if we store+flush with no intervening write tmp will be zero\n-    stlxrw(tmp, newv, addr);\n-    cbzw(tmp, succeed);\n-    \/\/ retry so we only ever return after a load fails to compare\n-    \/\/ ensures we don't return a stale value after a failed write.\n-    b(retry_load);\n-    \/\/ if the memory word differs we return it in oldv and signal a fail\n-    bind(nope);\n-    membar(AnyAny);\n-    mov(oldv, tmp);\n-  }\n-  if (fail)\n-    b(*fail);\n-}\n-\n@@ -5633,32 +5504,0 @@\n-void MacroAssembler::inc_held_monitor_count(Register tmp) {\n-  Address dst(rthread, JavaThread::held_monitor_count_offset());\n-#ifdef ASSERT\n-  ldr(tmp, dst);\n-  increment(tmp);\n-  str(tmp, dst);\n-  Label ok;\n-  tbz(tmp, 63, ok);\n-  STOP(\"assert(held monitor count underflow)\");\n-  should_not_reach_here();\n-  bind(ok);\n-#else\n-  increment(dst);\n-#endif\n-}\n-\n-void MacroAssembler::dec_held_monitor_count(Register tmp) {\n-  Address dst(rthread, JavaThread::held_monitor_count_offset());\n-#ifdef ASSERT\n-  ldr(tmp, dst);\n-  decrement(tmp);\n-  str(tmp, dst);\n-  Label ok;\n-  tbz(tmp, 63, ok);\n-  STOP(\"assert(held monitor count underflow)\");\n-  should_not_reach_here();\n-  bind(ok);\n-#else\n-  decrement(dst);\n-#endif\n-}\n-\n@@ -5913,1 +5752,1 @@\n-    snprintf(comment, sizeof comment, \"array_equals%c{\", kind);\n+    os::snprintf_checked(comment, sizeof comment, \"array_equals%c{\", kind);\n@@ -6117,1 +5956,1 @@\n-    snprintf(comment, sizeof comment, \"{string_equalsL\");\n+    os::snprintf_checked(comment, sizeof comment, \"{string_equalsL\");\n@@ -6265,1 +6104,1 @@\n-      snprintf(buf, sizeof buf, \"zero_words (count = %\" PRIu64 \") {\", cnt);\n+      os::snprintf_checked(buf, sizeof buf, \"zero_words (count = %\" PRIu64 \") {\", cnt);\n@@ -6420,4 +6259,8 @@\n-\/\/ - sun\/nio\/cs\/ISO_8859_1$Encoder.implEncodeISOArray\n-\/\/     return the number of characters copied.\n-\/\/ - java\/lang\/StringUTF16.compress\n-\/\/     return index of non-latin1 character if copy fails, otherwise 'len'.\n+\/\/ - sun.nio.cs.ISO_8859_1.Encoder#encodeISOArray0(byte[] sa, int sp, byte[] da, int dp, int len)\n+\/\/   Encodes char[] to byte[] in ISO-8859-1\n+\/\/\n+\/\/ - java.lang.StringCoding#encodeISOArray0(byte[] sa, int sp, byte[] da, int dp, int len)\n+\/\/   Encodes byte[] (containing UTF-16) to byte[] in ISO-8859-1\n+\/\/\n+\/\/ - java.lang.StringCoding#encodeAsciiArray0(char[] sa, int sp, byte[] da, int dp, int len)\n+\/\/   Encodes char[] to byte[] in ASCII\n@@ -6815,0 +6658,1 @@\n+        assert(VM_Version::supports_sb(), \"current CPU does not support SB instruction\");\n@@ -7089,1 +6933,1 @@\n-\/\/ Implements lightweight-locking.\n+\/\/ Implements fast-locking.\n@@ -7094,2 +6938,1 @@\n-void MacroAssembler::lightweight_lock(Register basic_lock, Register obj, Register t1, Register t2, Register t3, Label& slow) {\n-  assert(LockingMode == LM_LIGHTWEIGHT, \"only used with new lightweight locking\");\n+void MacroAssembler::fast_lock(Register basic_lock, Register obj, Register t1, Register t2, Register t3, Label& slow) {\n@@ -7149,1 +6992,1 @@\n-\/\/ Implements lightweight-unlocking.\n+\/\/ Implements fast-unlocking.\n@@ -7154,2 +6997,1 @@\n-void MacroAssembler::lightweight_unlock(Register obj, Register t1, Register t2, Register t3, Label& slow) {\n-  assert(LockingMode == LM_LIGHTWEIGHT, \"only used with new lightweight locking\");\n+void MacroAssembler::fast_unlock(Register obj, Register t1, Register t2, Register t3, Label& slow) {\n@@ -7201,1 +7043,1 @@\n-  stop(\"lightweight_unlock already unlocked\");\n+  stop(\"fast_unlock already unlocked\");\n","filename":"src\/hotspot\/cpu\/aarch64\/macroAssembler_aarch64.cpp","additions":112,"deletions":270,"binary":false,"changes":382,"status":"modified"},{"patch":"@@ -171,0 +171,1 @@\n+  assert_different_registers(bc_reg, temp_reg);\n@@ -234,3 +235,6 @@\n-\n-  \/\/ patch bytecode\n-  __ strb(bc_reg, at_bcp(0));\n+  \/\/ Patch bytecode with release store to coordinate with ResolvedFieldEntry loads\n+  \/\/ in fast bytecode codelets. load_field_entry has a memory barrier that gains\n+  \/\/ the needed ordering, together with control dependency on entering the fast codelet\n+  \/\/ itself.\n+  __ lea(temp_reg, at_bcp(0));\n+  __ stlrb(bc_reg, temp_reg);\n@@ -2272,1 +2276,1 @@\n-  Label resolved, clinit_barrier_slow;\n+  Label L_clinit_barrier_slow, L_done;\n@@ -2287,1 +2291,11 @@\n-  __ br(Assembler::EQ, resolved);\n+\n+  \/\/ Class initialization barrier for static methods\n+  if (VM_Version::supports_fast_class_init_checks() && bytecode() == Bytecodes::_invokestatic) {\n+    __ br(Assembler::NE, L_clinit_barrier_slow);\n+    __ ldr(temp, Address(Rcache, in_bytes(ResolvedMethodEntry::method_offset())));\n+    __ load_method_holder(temp, temp);\n+    __ clinit_barrier(temp, rscratch1, &L_done, \/*L_slow_path*\/ nullptr);\n+    __ bind(L_clinit_barrier_slow);\n+  } else {\n+    __ br(Assembler::EQ, L_done);\n+  }\n@@ -2291,1 +2305,0 @@\n-  __ bind(clinit_barrier_slow);\n@@ -2294,1 +2307,1 @@\n-  __ call_VM(noreg, entry, temp);\n+  __ call_VM_preemptable(noreg, entry, temp);\n@@ -2300,8 +2313,1 @@\n-  __ bind(resolved);\n-\n-  \/\/ Class initialization barrier for static methods\n-  if (VM_Version::supports_fast_class_init_checks() && bytecode() == Bytecodes::_invokestatic) {\n-    __ ldr(temp, Address(Rcache, in_bytes(ResolvedMethodEntry::method_offset())));\n-    __ load_method_holder(temp, temp);\n-    __ clinit_barrier(temp, rscratch1, nullptr, &clinit_barrier_slow);\n-  }\n+  __ bind(L_done);\n@@ -2316,1 +2322,1 @@\n-  Label resolved;\n+  Label L_clinit_barrier_slow, L_done;\n@@ -2335,1 +2341,13 @@\n-  __ br(Assembler::EQ, resolved);\n+\n+  \/\/ Class initialization barrier for static fields\n+  if (VM_Version::supports_fast_class_init_checks() &&\n+      (bytecode() == Bytecodes::_getstatic || bytecode() == Bytecodes::_putstatic)) {\n+    const Register field_holder = temp;\n+\n+    __ br(Assembler::NE, L_clinit_barrier_slow);\n+    __ ldr(field_holder, Address(Rcache, in_bytes(ResolvedFieldEntry::field_holder_offset())));\n+    __ clinit_barrier(field_holder, rscratch1, &L_done, \/*L_slow_path*\/ nullptr);\n+    __ bind(L_clinit_barrier_slow);\n+  } else {\n+    __ br(Assembler::EQ, L_done);\n+  }\n@@ -2338,0 +2356,1 @@\n+  \/\/ Class initialization barrier slow path lands here as well.\n@@ -2340,1 +2359,1 @@\n-  __ call_VM(noreg, entry, temp);\n+  __ call_VM_preemptable(noreg, entry, temp);\n@@ -2344,1 +2363,1 @@\n-  __ bind(resolved);\n+  __ bind(L_done);\n@@ -3082,0 +3101,1 @@\n+  __ verify_field_offset(r1);\n@@ -3171,0 +3191,2 @@\n+  __ verify_field_offset(r1);\n+\n@@ -3237,0 +3259,1 @@\n+\n@@ -3238,0 +3261,1 @@\n+  __ verify_field_offset(r1);\n@@ -3678,1 +3702,1 @@\n-  call_VM(r0, CAST_FROM_FN_PTR(address, InterpreterRuntime::_new), c_rarg1, c_rarg2);\n+  __ call_VM_preemptable(r0, CAST_FROM_FN_PTR(address, InterpreterRuntime::_new), c_rarg1, c_rarg2);\n","filename":"src\/hotspot\/cpu\/aarch64\/templateTable_aarch64.cpp","additions":44,"deletions":20,"binary":false,"changes":64,"status":"modified"},{"patch":"@@ -2,1 +2,1 @@\n-\/\/ Copyright (c) 2003, 2024, Oracle and\/or its affiliates. All rights reserved.\n+\/\/ Copyright (c) 2003, 2025, Oracle and\/or its affiliates. All rights reserved.\n@@ -1052,1 +1052,0 @@\n-  static int emit_exception_handler(C2_MacroAssembler *masm);\n@@ -1055,5 +1054,1 @@\n-  static uint size_exception_handler() {\n-    return MacroAssembler::far_branch_size();\n-  }\n-\n-    \/\/ count auipc + far branch\n+    \/\/ count far call + j\n@@ -1095,2 +1090,2 @@\n-  _ANY_REG32_mask = _ALL_REG32_mask;\n-  _ANY_REG32_mask.Remove(OptoReg::as_OptoReg(x0->as_VMReg()));\n+  _ANY_REG32_mask.assignFrom(_ALL_REG32_mask);\n+  _ANY_REG32_mask.remove(OptoReg::as_OptoReg(x0->as_VMReg()));\n@@ -1098,2 +1093,2 @@\n-  _ANY_REG_mask = _ALL_REG_mask;\n-  _ANY_REG_mask.SUBTRACT(_ZR_REG_mask);\n+  _ANY_REG_mask.assignFrom(_ALL_REG_mask);\n+  _ANY_REG_mask.subtract(_ZR_REG_mask);\n@@ -1101,2 +1096,2 @@\n-  _PTR_REG_mask = _ALL_REG_mask;\n-  _PTR_REG_mask.SUBTRACT(_ZR_REG_mask);\n+  _PTR_REG_mask.assignFrom(_ALL_REG_mask);\n+  _PTR_REG_mask.subtract(_ZR_REG_mask);\n@@ -1104,2 +1099,2 @@\n-  _NO_SPECIAL_REG32_mask = _ALL_REG32_mask;\n-  _NO_SPECIAL_REG32_mask.SUBTRACT(_NON_ALLOCATABLE_REG32_mask);\n+  _NO_SPECIAL_REG32_mask.assignFrom(_ALL_REG32_mask);\n+  _NO_SPECIAL_REG32_mask.subtract(_NON_ALLOCATABLE_REG32_mask);\n@@ -1107,2 +1102,2 @@\n-  _NO_SPECIAL_REG_mask = _ALL_REG_mask;\n-  _NO_SPECIAL_REG_mask.SUBTRACT(_NON_ALLOCATABLE_REG_mask);\n+  _NO_SPECIAL_REG_mask.assignFrom(_ALL_REG_mask);\n+  _NO_SPECIAL_REG_mask.subtract(_NON_ALLOCATABLE_REG_mask);\n@@ -1110,2 +1105,2 @@\n-  _NO_SPECIAL_PTR_REG_mask = _ALL_REG_mask;\n-  _NO_SPECIAL_PTR_REG_mask.SUBTRACT(_NON_ALLOCATABLE_REG_mask);\n+  _NO_SPECIAL_PTR_REG_mask.assignFrom(_ALL_REG_mask);\n+  _NO_SPECIAL_PTR_REG_mask.subtract(_NON_ALLOCATABLE_REG_mask);\n@@ -1115,3 +1110,3 @@\n-    _NO_SPECIAL_REG32_mask.Remove(OptoReg::as_OptoReg(x27->as_VMReg()));\n-    _NO_SPECIAL_REG_mask.Remove(OptoReg::as_OptoReg(x27->as_VMReg()));\n-    _NO_SPECIAL_PTR_REG_mask.Remove(OptoReg::as_OptoReg(x27->as_VMReg()));\n+    _NO_SPECIAL_REG32_mask.remove(OptoReg::as_OptoReg(x27->as_VMReg()));\n+    _NO_SPECIAL_REG_mask.remove(OptoReg::as_OptoReg(x27->as_VMReg()));\n+    _NO_SPECIAL_PTR_REG_mask.remove(OptoReg::as_OptoReg(x27->as_VMReg()));\n@@ -1122,3 +1117,3 @@\n-    _NO_SPECIAL_REG32_mask.Remove(OptoReg::as_OptoReg(x8->as_VMReg()));\n-    _NO_SPECIAL_REG_mask.Remove(OptoReg::as_OptoReg(x8->as_VMReg()));\n-    _NO_SPECIAL_PTR_REG_mask.Remove(OptoReg::as_OptoReg(x8->as_VMReg()));\n+    _NO_SPECIAL_REG32_mask.remove(OptoReg::as_OptoReg(x8->as_VMReg()));\n+    _NO_SPECIAL_REG_mask.remove(OptoReg::as_OptoReg(x8->as_VMReg()));\n+    _NO_SPECIAL_PTR_REG_mask.remove(OptoReg::as_OptoReg(x8->as_VMReg()));\n@@ -1127,2 +1122,2 @@\n-  _NO_SPECIAL_NO_FP_PTR_REG_mask = _NO_SPECIAL_PTR_REG_mask;\n-  _NO_SPECIAL_NO_FP_PTR_REG_mask.Remove(OptoReg::as_OptoReg(x8->as_VMReg()));\n+  _NO_SPECIAL_NO_FP_PTR_REG_mask.assignFrom(_NO_SPECIAL_PTR_REG_mask);\n+  _NO_SPECIAL_NO_FP_PTR_REG_mask.remove(OptoReg::as_OptoReg(x8->as_VMReg()));\n@@ -1187,0 +1182,2 @@\n+constexpr uint64_t MAJIK_DWORD = 0xabbaabbaabbaabbaull;\n+\n@@ -1272,0 +1269,20 @@\n+int CallRuntimeDirectNode::compute_padding(int current_offset) const\n+{\n+  return align_up(current_offset, alignment_required()) - current_offset;\n+}\n+\n+int CallLeafDirectNode::compute_padding(int current_offset) const\n+{\n+  return align_up(current_offset, alignment_required()) - current_offset;\n+}\n+\n+int CallLeafDirectVectorNode::compute_padding(int current_offset) const\n+{\n+  return align_up(current_offset, alignment_required()) - current_offset;\n+}\n+\n+int CallLeafNoFPDirectNode::compute_padding(int current_offset) const\n+{\n+  return align_up(current_offset, alignment_required()) - current_offset;\n+}\n+\n@@ -1309,1 +1326,1 @@\n-const RegMask& MachConstantBaseNode::_out_RegMask = RegMask::Empty;\n+const RegMask& MachConstantBaseNode::_out_RegMask = RegMask::EMPTY;\n@@ -1346,3 +1363,8 @@\n-  st->print(\"sd  fp, [sp, #%d]\\n\\t\", - 2 * wordSize);\n-  st->print(\"sd  ra, [sp, #%d]\\n\\t\", - wordSize);\n-  if (PreserveFramePointer) { st->print(\"sub  fp, sp, #%d\\n\\t\", 2 * wordSize); }\n+  st->print(\"sd  fp, [sp, #%d]\\n\\t\", framesize - 2 * wordSize);\n+  st->print(\"sd  ra, [sp, #%d]\\n\\t\", framesize - wordSize);\n+  if (PreserveFramePointer) { st->print(\"add fp, sp, #%d\\n\\t\", framesize); }\n+\n+  if (VerifyStackAtCalls) {\n+    st->print(\"mv  t2, %ld\\n\\t\", MAJIK_DWORD);\n+    st->print(\"sd  t2, [sp, #%d]\\n\\t\", framesize - 3 * wordSize);\n+  }\n@@ -1391,0 +1413,5 @@\n+  if (VerifyStackAtCalls) {\n+    __ mv(t2, MAJIK_DWORD);\n+    __ sd(t2, Address(sp, framesize - 3 * wordSize));\n+  }\n+\n@@ -1412,4 +1439,0 @@\n-  if (VerifyStackAtCalls) {\n-    Unimplemented();\n-  }\n-\n@@ -1813,19 +1836,0 @@\n-\/\/ Emit exception handler code.\n-int HandlerImpl::emit_exception_handler(C2_MacroAssembler* masm)\n-{\n-  \/\/ auipc t1, #exception_blob_entry_point\n-  \/\/ jr (offset)t1\n-  \/\/ Note that the code buffer's insts_mark is always relative to insts.\n-  \/\/ That's why we must use the macroassembler to generate a handler.\n-  address base = __ start_a_stub(size_exception_handler());\n-  if (base == nullptr) {\n-    ciEnv::current()->record_failure(\"CodeCache is full\");\n-    return 0;  \/\/ CodeBuffer::expand failed\n-  }\n-  int offset = __ offset();\n-  __ far_jump(RuntimeAddress(OptoRuntime::exception_blob()->entry_point()));\n-  assert(__ offset() - offset <= (int) size_exception_handler(), \"overflow\");\n-  __ end_a_stub();\n-  return offset;\n-}\n-\n@@ -1842,2 +1846,7 @@\n-  __ auipc(ra, 0);\n-  __ far_jump(RuntimeAddress(SharedRuntime::deopt_blob()->unpack()));\n+  Label start;\n+  __ bind(start);\n+\n+  __ far_call(RuntimeAddress(SharedRuntime::deopt_blob()->unpack()));\n+\n+  int entry_offset = __ offset();\n+  __ j(start);\n@@ -1846,0 +1855,2 @@\n+  assert(__ offset() - entry_offset >= NativePostCallNop::first_check_size,\n+         \"out of bounds read in post-call NOP check\");\n@@ -1847,1 +1858,1 @@\n-  return offset;\n+  return entry_offset;\n@@ -2087,1 +2098,1 @@\n-  \/\/ (_NO_SPECIAL_REG32_mask.Size() minus 1) forces CallNode to become\n+  \/\/ (_NO_SPECIAL_REG32_mask.size() minus 1) forces CallNode to become\n@@ -2090,1 +2101,1 @@\n-  uint default_int_pressure_threshold = _NO_SPECIAL_REG32_mask.Size() - 1;\n+  uint default_int_pressure_threshold = _NO_SPECIAL_REG32_mask.size() - 1;\n@@ -2105,1 +2116,1 @@\n-  return (FLOATPRESSURE == -1) ? _FLOAT_REG_mask.Size() : FLOATPRESSURE;\n+  return (FLOATPRESSURE == -1) ? _FLOAT_REG_mask.size() : FLOATPRESSURE;\n@@ -2112,1 +2123,1 @@\n-RegMask Matcher::divI_proj_mask() {\n+const RegMask& Matcher::divI_proj_mask() {\n@@ -2114,1 +2125,1 @@\n-  return RegMask();\n+  return RegMask::EMPTY;\n@@ -2118,1 +2129,1 @@\n-RegMask Matcher::modI_proj_mask() {\n+const RegMask& Matcher::modI_proj_mask() {\n@@ -2120,1 +2131,1 @@\n-  return RegMask();\n+  return RegMask::EMPTY;\n@@ -2124,1 +2135,1 @@\n-RegMask Matcher::divL_proj_mask() {\n+const RegMask& Matcher::divL_proj_mask() {\n@@ -2126,1 +2137,1 @@\n-  return RegMask();\n+  return RegMask::EMPTY;\n@@ -2130,1 +2141,1 @@\n-RegMask Matcher::modL_proj_mask() {\n+const RegMask& Matcher::modL_proj_mask() {\n@@ -2132,5 +2143,1 @@\n-  return RegMask();\n-}\n-\n-const RegMask Matcher::method_handle_invoke_SP_save_mask() {\n-  return FP_REG_mask();\n+  return RegMask::EMPTY;\n@@ -2279,4 +2286,0 @@\n-  enc_class riscv_enc_mov_byte_map_base(iRegP dst) %{\n-    __ load_byte_map_base($dst$$Register);\n-  %}\n-\n@@ -2422,1 +2425,7 @@\n-      __ call_Unimplemented();\n+      int framesize = ra_->reg2offset_unchecked(OptoReg::add(ra_->_matcher._old_SP, -3 * VMRegImpl::slots_per_word));\n+      Label stack_ok;\n+      __ ld(t1, Address(sp, framesize));\n+      __ mv(t2, MAJIK_DWORD);\n+      __ beq(t2, t1, stack_ok);\n+      __ stop(\"MAJIK_DWORD not found\");\n+      __ bind(stack_ok);\n@@ -2837,15 +2846,0 @@\n-\/\/ Card Table Byte Map Base\n-operand immByteMapBase()\n-%{\n-  \/\/ Get base of card map\n-  predicate(BarrierSet::barrier_set()->is_a(BarrierSet::CardTableBarrierSet) &&\n-            SHENANDOAHGC_ONLY(!BarrierSet::barrier_set()->is_a(BarrierSet::ShenandoahBarrierSet) &&)\n-            (CardTable::CardValue*)n->get_ptr() ==\n-             ((CardTableBarrierSet*)(BarrierSet::barrier_set()))->card_table()->byte_map_base());\n-  match(ConP);\n-\n-  op_cost(0);\n-  format %{ %}\n-  interface(CONST_INTER);\n-%}\n-\n@@ -3855,7 +3849,3 @@\n-  \/\/ RISC-V instructions are of fixed length\n-  fixed_size_instructions;           \/\/ Fixed size instructions TODO does\n-  max_instructions_per_bundle = 2;   \/\/ Generic RISC-V 1, Sifive Series 7 2\n-  \/\/ RISC-V instructions come in 32-bit word units\n-  instruction_unit_size = 4;         \/\/ An instruction is 4 bytes long\n-  instruction_fetch_unit_size = 64;  \/\/ The processor fetches one line\n-  instruction_fetch_units = 1;       \/\/ of 64 bytes\n+  \/\/ RISC-V instructions are of length 2 or 4 bytes.\n+  variable_size_instructions;\n+  instruction_unit_size = 2;\n@@ -3863,2 +3853,8 @@\n-  \/\/ List of nop instructions\n-  nops( MachNop );\n+  \/\/ Up to 4 instructions per bundle\n+  max_instructions_per_bundle = 4;\n+\n+  \/\/ The RISC-V processor fetches 64 bytes...\n+  instruction_fetch_unit_size = 64;\n+\n+  \/\/ ...in one line.\n+  instruction_fetch_units = 1;\n@@ -4812,12 +4808,0 @@\n-\/\/ Load Byte Map Base Constant\n-instruct loadByteMapBase(iRegPNoSp dst, immByteMapBase con)\n-%{\n-  match(Set dst con);\n-  ins_cost(ALU_COST);\n-  format %{ \"mv  $dst, $con\\t# Byte Map Base, #@loadByteMapBase\" %}\n-\n-  ins_encode(riscv_enc_mov_byte_map_base(dst));\n-\n-  ins_pipe(ialu_imm);\n-%}\n-\n@@ -8208,1 +8192,1 @@\n- \n+\n@@ -8589,1 +8573,1 @@\n-  format %{ \"fmv.h.x $dst, $src\" %}\n+  format %{ \"fmv.h.x $dst, $src\\t# reinterpretS2HF\" %}\n@@ -8609,1 +8593,1 @@\n-  format %{ \"fmv.x.h $dst, $src\" %}\n+  format %{ \"fmv.x.h $dst, $src\\t# reinterpretHF2S\" %}\n@@ -8971,1 +8955,1 @@\n-  effect(TEMP tmp);\n+  effect(TEMP_DEF dst, TEMP tmp);\n@@ -10542,0 +10526,1 @@\n+  ins_alignment(4);\n@@ -10559,0 +10544,1 @@\n+  ins_alignment(4);\n@@ -10576,0 +10562,1 @@\n+  ins_alignment(4);\n@@ -10593,0 +10580,1 @@\n+  ins_alignment(4);\n@@ -11004,0 +10992,1 @@\n+  predicate(!UseRVV);\n@@ -11059,1 +11048,0 @@\n-  predicate(LockingMode != LM_LIGHTWEIGHT);\n@@ -11075,36 +11063,2 @@\n-instruct cmpFastUnlock(rFlagsReg cr, iRegP object, iRegP box, iRegPNoSp tmp1, iRegPNoSp tmp2)\n-%{\n-  predicate(LockingMode != LM_LIGHTWEIGHT);\n-  match(Set cr (FastUnlock object box));\n-  effect(TEMP tmp1, TEMP tmp2);\n-\n-  ins_cost(10 * DEFAULT_COST);\n-  format %{ \"fastunlock $object,$box\\t! kills $tmp1, $tmp2, #@cmpFastUnlock\" %}\n-\n-  ins_encode %{\n-    __ fast_unlock($object$$Register, $box$$Register, $tmp1$$Register, $tmp2$$Register);\n-  %}\n-\n-  ins_pipe(pipe_serial);\n-%}\n-\n-instruct cmpFastLockLightweight(rFlagsReg cr, iRegP object, iRegP box,\n-                                iRegPNoSp tmp1, iRegPNoSp tmp2, iRegPNoSp tmp3, iRegPNoSp tmp4)\n-%{\n-  predicate(LockingMode == LM_LIGHTWEIGHT);\n-  match(Set cr (FastLock object box));\n-  effect(TEMP tmp1, TEMP tmp2, TEMP tmp3, TEMP tmp4);\n-\n-  ins_cost(10 * DEFAULT_COST);\n-  format %{ \"fastlock $object,$box\\t! kills $tmp1,$tmp2,$tmp3,$tmp4 #@cmpFastLockLightweight\" %}\n-\n-  ins_encode %{\n-    __ fast_lock_lightweight($object$$Register, $box$$Register,\n-                             $tmp1$$Register, $tmp2$$Register, $tmp3$$Register, $tmp4$$Register);\n-  %}\n-\n-  ins_pipe(pipe_serial);\n-%}\n-\n-instruct cmpFastUnlockLightweight(rFlagsReg cr, iRegP object, iRegP box,\n-                                  iRegPNoSp tmp1, iRegPNoSp tmp2, iRegPNoSp tmp3)\n+instruct cmpFastUnlock(rFlagsReg cr, iRegP object, iRegP box,\n+                       iRegPNoSp tmp1, iRegPNoSp tmp2, iRegPNoSp tmp3)\n@@ -11112,1 +11066,0 @@\n-  predicate(LockingMode == LM_LIGHTWEIGHT);\n@@ -11117,1 +11070,1 @@\n-  format %{ \"fastunlock $object,$box\\t! kills $tmp1,$tmp2,$tmp3 #@cmpFastUnlockLightweight\" %}\n+  format %{ \"fastunlock $object,$box\\t! kills $tmp1,$tmp2,$tmp3 #@cmpFastUnlock\" %}\n@@ -11120,2 +11073,2 @@\n-    __ fast_unlock_lightweight($object$$Register, $box$$Register,\n-                               $tmp1$$Register, $tmp2$$Register, $tmp3$$Register);\n+    __ fast_unlock($object$$Register, $box$$Register,\n+                   $tmp1$$Register, $tmp2$$Register, $tmp3$$Register);\n","filename":"src\/hotspot\/cpu\/riscv\/riscv.ad","additions":111,"deletions":158,"binary":false,"changes":269,"status":"modified"},{"patch":"@@ -26,2 +26,0 @@\n-#ifdef _LP64\n-\n@@ -57,2 +55,0 @@\n-\n-#endif \/\/ _LP64\n","filename":"src\/hotspot\/cpu\/x86\/compressedKlass_x86.cpp","additions":0,"deletions":4,"binary":false,"changes":4,"status":"modified"},{"patch":"@@ -798,0 +798,16 @@\n+void MacroAssembler::push_ppx(Register src) {\n+  if (VM_Version::supports_apx_f()) {\n+    pushp(src);\n+  } else {\n+    Assembler::push(src);\n+  }\n+}\n+\n+void MacroAssembler::pop_ppx(Register dst) {\n+  if (VM_Version::supports_apx_f()) {\n+    popp(dst);\n+  } else {\n+    Assembler::pop(dst);\n+  }\n+}\n+\n@@ -2418,8 +2434,0 @@\n-void MacroAssembler::inc_held_monitor_count() {\n-  incrementq(Address(r15_thread, JavaThread::held_monitor_count_offset()));\n-}\n-\n-void MacroAssembler::dec_held_monitor_count() {\n-  decrementq(Address(r15_thread, JavaThread::held_monitor_count_offset()));\n-}\n-\n@@ -2884,1 +2892,2 @@\n-  bool blend_emulation = EnableX86ECoreOpts && UseAVX > 1;\n+  bool blend_emulation = EnableX86ECoreOpts && UseAVX > 1 &&\n+                         !(VM_Version::is_intel_darkmont() && (dst == src1)); \/\/ partially fixed on Darkmont\n@@ -2908,1 +2917,2 @@\n-  bool blend_emulation = EnableX86ECoreOpts && UseAVX > 1;\n+  bool blend_emulation = EnableX86ECoreOpts && UseAVX > 1 &&\n+                         !(VM_Version::is_intel_darkmont() && (dst == src1)); \/\/ partially fixed on Darkmont\n@@ -5832,1 +5842,1 @@\n-  cmpptr(count, 2<<shift); \/\/ Short arrays (< 8 bytes) fill by element\n+  cmpptr(count, 8 << shift); \/\/ Short arrays (< 32 bytes) fill by element\n@@ -5895,1 +5905,0 @@\n-        Label L_fill_64_bytes_loop;\n@@ -5900,1 +5909,25 @@\n-        align(16);\n+        \/\/ align data for 64-byte chunks\n+        Label L_fill_64_bytes_loop, L_align_64_bytes_loop;\n+        if (EnableX86ECoreOpts) {\n+            \/\/ align 'big' arrays to cache lines to minimize split_stores\n+            cmpptr(count, 96 << shift);\n+            jcc(Assembler::below, L_fill_64_bytes_loop);\n+\n+            \/\/ Find the bytes needed for alignment\n+            movptr(rtmp, to);\n+            andptr(rtmp, 0x1c);\n+            jcc(Assembler::zero, L_fill_64_bytes_loop);\n+            negptr(rtmp);           \/\/ number of bytes to fill 32-rtmp. it filled by 2 mov by 32\n+            addptr(rtmp, 32);\n+            shrptr(rtmp, 2 - shift);\/\/ get number of elements from bytes\n+            subptr(count, rtmp);    \/\/ adjust count by number of elements\n+\n+            align(16);\n+            BIND(L_align_64_bytes_loop);\n+            movdl(Address(to, 0), xtmp);\n+            addptr(to, 4);\n+            subptr(rtmp, 1 << shift);\n+            jcc(Assembler::greater, L_align_64_bytes_loop);\n+        }\n+\n+        align(16);\n@@ -5909,0 +5942,1 @@\n+        align(16);\n@@ -5953,0 +5987,1 @@\n+      align(16);\n@@ -5961,3 +5996,3 @@\n-  \/\/ fill trailing 4 bytes\n-  BIND(L_fill_4_bytes);\n-  testl(count, 1<<shift);\n+\n+  Label L_fill_4_bytes_loop;\n+  testl(count, 1 << shift);\n@@ -5965,0 +6000,3 @@\n+\n+  align(16);\n+  BIND(L_fill_4_bytes_loop);\n@@ -5966,0 +6004,6 @@\n+  addptr(to, 4);\n+\n+  BIND(L_fill_4_bytes);\n+  subptr(count, 1 << shift);\n+  jccb(Assembler::greaterEqual, L_fill_4_bytes_loop);\n+\n@@ -5968,1 +6012,0 @@\n-    addptr(to, 4);\n@@ -6014,26 +6057,40 @@\n-\/\/ encode char[] to byte[] in ISO_8859_1 or ASCII\n-   \/\/@IntrinsicCandidate\n-   \/\/private static int implEncodeISOArray(byte[] sa, int sp,\n-   \/\/byte[] da, int dp, int len) {\n-   \/\/  int i = 0;\n-   \/\/  for (; i < len; i++) {\n-   \/\/    char c = StringUTF16.getChar(sa, sp++);\n-   \/\/    if (c > '\\u00FF')\n-   \/\/      break;\n-   \/\/    da[dp++] = (byte)c;\n-   \/\/  }\n-   \/\/  return i;\n-   \/\/}\n-   \/\/\n-   \/\/@IntrinsicCandidate\n-   \/\/private static int implEncodeAsciiArray(char[] sa, int sp,\n-   \/\/    byte[] da, int dp, int len) {\n-   \/\/  int i = 0;\n-   \/\/  for (; i < len; i++) {\n-   \/\/    char c = sa[sp++];\n-   \/\/    if (c >= '\\u0080')\n-   \/\/      break;\n-   \/\/    da[dp++] = (byte)c;\n-   \/\/  }\n-   \/\/  return i;\n-   \/\/}\n+\/\/ Encode given char[]\/byte[] to byte[] in ISO_8859_1 or ASCII\n+\/\/\n+\/\/ @IntrinsicCandidate\n+\/\/ int sun.nio.cs.ISO_8859_1.Encoder#encodeISOArray0(\n+\/\/         char[] sa, int sp, byte[] da, int dp, int len) {\n+\/\/     int i = 0;\n+\/\/     for (; i < len; i++) {\n+\/\/         char c = sa[sp++];\n+\/\/         if (c > '\\u00FF')\n+\/\/             break;\n+\/\/         da[dp++] = (byte) c;\n+\/\/     }\n+\/\/     return i;\n+\/\/ }\n+\/\/\n+\/\/ @IntrinsicCandidate\n+\/\/ int java.lang.StringCoding.encodeISOArray0(\n+\/\/         byte[] sa, int sp, byte[] da, int dp, int len) {\n+\/\/   int i = 0;\n+\/\/   for (; i < len; i++) {\n+\/\/     char c = StringUTF16.getChar(sa, sp++);\n+\/\/     if (c > '\\u00FF')\n+\/\/       break;\n+\/\/     da[dp++] = (byte) c;\n+\/\/   }\n+\/\/   return i;\n+\/\/ }\n+\/\/\n+\/\/ @IntrinsicCandidate\n+\/\/ int java.lang.StringCoding.encodeAsciiArray0(\n+\/\/         char[] sa, int sp, byte[] da, int dp, int len) {\n+\/\/   int i = 0;\n+\/\/   for (; i < len; i++) {\n+\/\/     char c = sa[sp++];\n+\/\/     if (c >= '\\u0080')\n+\/\/       break;\n+\/\/     da[dp++] = (byte) c;\n+\/\/   }\n+\/\/   return i;\n+\/\/ }\n@@ -9599,1 +9656,1 @@\n-\/\/ Implements lightweight-locking.\n+\/\/ Implements fast-locking.\n@@ -9605,1 +9662,1 @@\n-void MacroAssembler::lightweight_lock(Register basic_lock, Register obj, Register reg_rax, Register tmp, Label& slow) {\n+void MacroAssembler::fast_lock(Register basic_lock, Register obj, Register reg_rax, Register tmp, Label& slow) {\n@@ -9661,1 +9718,1 @@\n-\/\/ Implements lightweight-unlocking.\n+\/\/ Implements fast-unlocking.\n@@ -9667,1 +9724,1 @@\n-void MacroAssembler::lightweight_unlock(Register obj, Register reg_rax, Register tmp, Label& slow) {\n+void MacroAssembler::fast_unlock(Register obj, Register reg_rax, Register tmp, Label& slow) {\n@@ -9699,1 +9756,1 @@\n-  stop(\"lightweight_unlock already unlocked\");\n+  stop(\"fast_unlock already unlocked\");\n","filename":"src\/hotspot\/cpu\/x86\/macroAssembler_x86.cpp","additions":105,"deletions":48,"binary":false,"changes":153,"status":"modified"},{"patch":"@@ -67,11 +67,4 @@\n-\n-  if (LockingMode == LM_LIGHTWEIGHT) {\n-    if (!UseObjectMonitorTable) {\n-      \/\/ check if monitor\n-      __ testptr(result, markWord::monitor_value);\n-      __ jcc(Assembler::notZero, slowCase);\n-    }\n-  } else {\n-    \/\/ check if locked\n-    __ testptr(result, markWord::unlocked_value);\n-    __ jcc(Assembler::zero, slowCase);\n+  if (!UseObjectMonitorTable) {\n+    \/\/ check if monitor\n+    __ testptr(result, markWord::monitor_value);\n+    __ jcc(Assembler::notZero, slowCase);\n","filename":"src\/hotspot\/cpu\/x86\/sharedRuntime_x86.cpp","additions":4,"deletions":11,"binary":false,"changes":15,"status":"modified"},{"patch":"@@ -2199,2 +2199,1 @@\n-  Label L_clinit_barrier_slow;\n-  Label resolved;\n+  Label L_clinit_barrier_slow, L_done;\n@@ -2218,12 +2217,0 @@\n-  __ jcc(Assembler::equal, resolved);\n-\n-  \/\/ resolve first time through\n-  \/\/ Class initialization barrier slow path lands here as well.\n-  __ bind(L_clinit_barrier_slow);\n-  address entry = CAST_FROM_FN_PTR(address, InterpreterRuntime::resolve_from_cache);\n-  __ movl(temp, code);\n-  __ call_VM(noreg, entry, temp);\n-  \/\/ Update registers with resolved info\n-  __ load_method_entry(cache, index);\n-\n-  __ bind(resolved);\n@@ -2236,0 +2223,1 @@\n+    __ jcc(Assembler::notEqual, L_clinit_barrier_slow);\n@@ -2238,1 +2226,4 @@\n-    __ clinit_barrier(klass, nullptr \/*L_fast_path*\/, &L_clinit_barrier_slow);\n+    __ clinit_barrier(klass, &L_done, \/*L_slow_path*\/ nullptr);\n+    __ bind(L_clinit_barrier_slow);\n+  } else {\n+    __ jcc(Assembler::equal, L_done);\n@@ -2240,0 +2231,9 @@\n+\n+  \/\/ resolve first time through\n+  \/\/ Class initialization barrier slow path lands here as well.\n+  address entry = CAST_FROM_FN_PTR(address, InterpreterRuntime::resolve_from_cache);\n+  __ movl(temp, code);\n+  __ call_VM_preemptable(noreg, entry, temp);\n+  \/\/ Update registers with resolved info\n+  __ load_method_entry(cache, index);\n+  __ bind(L_done);\n@@ -2243,2 +2243,2 @@\n-                                            Register cache,\n-                                            Register index) {\n+                                                      Register cache,\n+                                                      Register index) {\n@@ -2248,1 +2248,1 @@\n-  Label resolved;\n+  Label L_clinit_barrier_slow, L_done;\n@@ -2265,1 +2265,13 @@\n-  __ jcc(Assembler::equal, resolved);\n+\n+  \/\/ Class initialization barrier for static fields\n+  if (VM_Version::supports_fast_class_init_checks() &&\n+      (bytecode() == Bytecodes::_getstatic || bytecode() == Bytecodes::_putstatic)) {\n+    const Register field_holder = temp;\n+\n+    __ jcc(Assembler::notEqual, L_clinit_barrier_slow);\n+    __ movptr(field_holder, Address(cache, in_bytes(ResolvedFieldEntry::field_holder_offset())));\n+    __ clinit_barrier(field_holder, &L_done, \/*L_slow_path*\/ nullptr);\n+    __ bind(L_clinit_barrier_slow);\n+  } else {\n+    __ jcc(Assembler::equal, L_done);\n+  }\n@@ -2268,0 +2280,1 @@\n+  \/\/ Class initialization barrier slow path lands here as well.\n@@ -2270,1 +2283,1 @@\n-  __ call_VM(noreg, entry, temp);\n+  __ call_VM_preemptable(noreg, entry, temp);\n@@ -2273,2 +2286,1 @@\n-\n-  __ bind(resolved);\n+  __ bind(L_done);\n@@ -3634,2 +3646,2 @@\n-  call_VM(rax, CAST_FROM_FN_PTR(address, InterpreterRuntime::_new), c_rarg1, c_rarg2);\n-   __ verify_oop(rax);\n+  __ call_VM_preemptable(rax, CAST_FROM_FN_PTR(address, InterpreterRuntime::_new), c_rarg1, c_rarg2);\n+  __ verify_oop(rax);\n","filename":"src\/hotspot\/cpu\/x86\/templateTable_x86.cpp","additions":36,"deletions":24,"binary":false,"changes":60,"status":"modified"},{"patch":"@@ -25,1 +25,1 @@\n-\/\/ X86 Common Architecture Description File\n+\/\/ X86 AMD64 Architecture Description File\n@@ -62,0 +62,160 @@\n+\/\/ General Registers\n+\/\/ R8-R15 must be encoded with REX.  (RSP, RBP, RSI, RDI need REX when\n+\/\/ used as byte registers)\n+\n+\/\/ Previously set RBX, RSI, and RDI as save-on-entry for java code\n+\/\/ Turn off SOE in java-code due to frequent use of uncommon-traps.\n+\/\/ Now that allocator is better, turn on RSI and RDI as SOE registers.\n+\n+reg_def RAX  (SOC, SOC, Op_RegI,  0, rax->as_VMReg());\n+reg_def RAX_H(SOC, SOC, Op_RegI,  0, rax->as_VMReg()->next());\n+\n+reg_def RCX  (SOC, SOC, Op_RegI,  1, rcx->as_VMReg());\n+reg_def RCX_H(SOC, SOC, Op_RegI,  1, rcx->as_VMReg()->next());\n+\n+reg_def RDX  (SOC, SOC, Op_RegI,  2, rdx->as_VMReg());\n+reg_def RDX_H(SOC, SOC, Op_RegI,  2, rdx->as_VMReg()->next());\n+\n+reg_def RBX  (SOC, SOE, Op_RegI,  3, rbx->as_VMReg());\n+reg_def RBX_H(SOC, SOE, Op_RegI,  3, rbx->as_VMReg()->next());\n+\n+reg_def RSP  (NS,  NS,  Op_RegI,  4, rsp->as_VMReg());\n+reg_def RSP_H(NS,  NS,  Op_RegI,  4, rsp->as_VMReg()->next());\n+\n+\/\/ now that adapter frames are gone RBP is always saved and restored by the prolog\/epilog code\n+reg_def RBP  (NS, SOE, Op_RegI,  5, rbp->as_VMReg());\n+reg_def RBP_H(NS, SOE, Op_RegI,  5, rbp->as_VMReg()->next());\n+\n+#ifdef _WIN64\n+\n+reg_def RSI  (SOC, SOE, Op_RegI,  6, rsi->as_VMReg());\n+reg_def RSI_H(SOC, SOE, Op_RegI,  6, rsi->as_VMReg()->next());\n+\n+reg_def RDI  (SOC, SOE, Op_RegI,  7, rdi->as_VMReg());\n+reg_def RDI_H(SOC, SOE, Op_RegI,  7, rdi->as_VMReg()->next());\n+\n+#else\n+\n+reg_def RSI  (SOC, SOC, Op_RegI,  6, rsi->as_VMReg());\n+reg_def RSI_H(SOC, SOC, Op_RegI,  6, rsi->as_VMReg()->next());\n+\n+reg_def RDI  (SOC, SOC, Op_RegI,  7, rdi->as_VMReg());\n+reg_def RDI_H(SOC, SOC, Op_RegI,  7, rdi->as_VMReg()->next());\n+\n+#endif\n+\n+reg_def R8   (SOC, SOC, Op_RegI,  8, r8->as_VMReg());\n+reg_def R8_H (SOC, SOC, Op_RegI,  8, r8->as_VMReg()->next());\n+\n+reg_def R9   (SOC, SOC, Op_RegI,  9, r9->as_VMReg());\n+reg_def R9_H (SOC, SOC, Op_RegI,  9, r9->as_VMReg()->next());\n+\n+reg_def R10  (SOC, SOC, Op_RegI, 10, r10->as_VMReg());\n+reg_def R10_H(SOC, SOC, Op_RegI, 10, r10->as_VMReg()->next());\n+\n+reg_def R11  (SOC, SOC, Op_RegI, 11, r11->as_VMReg());\n+reg_def R11_H(SOC, SOC, Op_RegI, 11, r11->as_VMReg()->next());\n+\n+reg_def R12  (SOC, SOE, Op_RegI, 12, r12->as_VMReg());\n+reg_def R12_H(SOC, SOE, Op_RegI, 12, r12->as_VMReg()->next());\n+\n+reg_def R13  (SOC, SOE, Op_RegI, 13, r13->as_VMReg());\n+reg_def R13_H(SOC, SOE, Op_RegI, 13, r13->as_VMReg()->next());\n+\n+reg_def R14  (SOC, SOE, Op_RegI, 14, r14->as_VMReg());\n+reg_def R14_H(SOC, SOE, Op_RegI, 14, r14->as_VMReg()->next());\n+\n+reg_def R15  (SOC, SOE, Op_RegI, 15, r15->as_VMReg());\n+reg_def R15_H(SOC, SOE, Op_RegI, 15, r15->as_VMReg()->next());\n+\n+reg_def R16  (SOC, SOC, Op_RegI, 16, r16->as_VMReg());\n+reg_def R16_H(SOC, SOC, Op_RegI, 16, r16->as_VMReg()->next());\n+\n+reg_def R17  (SOC, SOC, Op_RegI, 17, r17->as_VMReg());\n+reg_def R17_H(SOC, SOC, Op_RegI, 17, r17->as_VMReg()->next());\n+\n+reg_def R18  (SOC, SOC, Op_RegI, 18, r18->as_VMReg());\n+reg_def R18_H(SOC, SOC, Op_RegI, 18, r18->as_VMReg()->next());\n+\n+reg_def R19  (SOC, SOC, Op_RegI, 19, r19->as_VMReg());\n+reg_def R19_H(SOC, SOC, Op_RegI, 19, r19->as_VMReg()->next());\n+\n+reg_def R20  (SOC, SOC, Op_RegI, 20, r20->as_VMReg());\n+reg_def R20_H(SOC, SOC, Op_RegI, 20, r20->as_VMReg()->next());\n+\n+reg_def R21  (SOC, SOC, Op_RegI, 21, r21->as_VMReg());\n+reg_def R21_H(SOC, SOC, Op_RegI, 21, r21->as_VMReg()->next());\n+\n+reg_def R22  (SOC, SOC, Op_RegI, 22, r22->as_VMReg());\n+reg_def R22_H(SOC, SOC, Op_RegI, 22, r22->as_VMReg()->next());\n+\n+reg_def R23  (SOC, SOC, Op_RegI, 23, r23->as_VMReg());\n+reg_def R23_H(SOC, SOC, Op_RegI, 23, r23->as_VMReg()->next());\n+\n+reg_def R24  (SOC, SOC, Op_RegI, 24, r24->as_VMReg());\n+reg_def R24_H(SOC, SOC, Op_RegI, 24, r24->as_VMReg()->next());\n+\n+reg_def R25  (SOC, SOC, Op_RegI, 25, r25->as_VMReg());\n+reg_def R25_H(SOC, SOC, Op_RegI, 25, r25->as_VMReg()->next());\n+\n+reg_def R26  (SOC, SOC, Op_RegI, 26, r26->as_VMReg());\n+reg_def R26_H(SOC, SOC, Op_RegI, 26, r26->as_VMReg()->next());\n+\n+reg_def R27  (SOC, SOC, Op_RegI, 27, r27->as_VMReg());\n+reg_def R27_H(SOC, SOC, Op_RegI, 27, r27->as_VMReg()->next());\n+\n+reg_def R28  (SOC, SOC, Op_RegI, 28, r28->as_VMReg());\n+reg_def R28_H(SOC, SOC, Op_RegI, 28, r28->as_VMReg()->next());\n+\n+reg_def R29  (SOC, SOC, Op_RegI, 29, r29->as_VMReg());\n+reg_def R29_H(SOC, SOC, Op_RegI, 29, r29->as_VMReg()->next());\n+\n+reg_def R30  (SOC, SOC, Op_RegI, 30, r30->as_VMReg());\n+reg_def R30_H(SOC, SOC, Op_RegI, 30, r30->as_VMReg()->next());\n+\n+reg_def R31  (SOC, SOC, Op_RegI, 31, r31->as_VMReg());\n+reg_def R31_H(SOC, SOC, Op_RegI, 31, r31->as_VMReg()->next());\n+\n+\/\/ Floating Point Registers\n+\n+\/\/ Specify priority of register selection within phases of register\n+\/\/ allocation.  Highest priority is first.  A useful heuristic is to\n+\/\/ give registers a low priority when they are required by machine\n+\/\/ instructions, like EAX and EDX on I486, and choose no-save registers\n+\/\/ before save-on-call, & save-on-call before save-on-entry.  Registers\n+\/\/ which participate in fixed calling sequences should come last.\n+\/\/ Registers which are used as pairs must fall on an even boundary.\n+\n+alloc_class chunk0(R10,         R10_H,\n+                   R11,         R11_H,\n+                   R8,          R8_H,\n+                   R9,          R9_H,\n+                   R12,         R12_H,\n+                   RCX,         RCX_H,\n+                   RBX,         RBX_H,\n+                   RDI,         RDI_H,\n+                   RDX,         RDX_H,\n+                   RSI,         RSI_H,\n+                   RAX,         RAX_H,\n+                   RBP,         RBP_H,\n+                   R13,         R13_H,\n+                   R14,         R14_H,\n+                   R15,         R15_H,\n+                   R16,         R16_H,\n+                   R17,         R17_H,\n+                   R18,         R18_H,\n+                   R19,         R19_H,\n+                   R20,         R20_H,\n+                   R21,         R21_H,\n+                   R22,         R22_H,\n+                   R23,         R23_H,\n+                   R24,         R24_H,\n+                   R25,         R25_H,\n+                   R26,         R26_H,\n+                   R27,         R27_H,\n+                   R28,         R28_H,\n+                   R29,         R29_H,\n+                   R30,         R30_H,\n+                   R31,         R31_H,\n+                   RSP,         RSP_H);\n+\n@@ -646,0 +806,192 @@\n+\/\/----------Architecture Description Register Classes--------------------------\n+\/\/ Several register classes are automatically defined based upon information in\n+\/\/ this architecture description.\n+\/\/ 1) reg_class inline_cache_reg           ( \/* as def'd in frame section *\/ )\n+\/\/ 2) reg_class stack_slots( \/* one chunk of stack-based \"registers\" *\/ )\n+\/\/\n+\n+\/\/ Empty register class.\n+reg_class no_reg();\n+\n+\/\/ Class for all pointer\/long registers including APX extended GPRs.\n+reg_class all_reg(RAX, RAX_H,\n+                  RDX, RDX_H,\n+                  RBP, RBP_H,\n+                  RDI, RDI_H,\n+                  RSI, RSI_H,\n+                  RCX, RCX_H,\n+                  RBX, RBX_H,\n+                  RSP, RSP_H,\n+                  R8,  R8_H,\n+                  R9,  R9_H,\n+                  R10, R10_H,\n+                  R11, R11_H,\n+                  R12, R12_H,\n+                  R13, R13_H,\n+                  R14, R14_H,\n+                  R15, R15_H,\n+                  R16, R16_H,\n+                  R17, R17_H,\n+                  R18, R18_H,\n+                  R19, R19_H,\n+                  R20, R20_H,\n+                  R21, R21_H,\n+                  R22, R22_H,\n+                  R23, R23_H,\n+                  R24, R24_H,\n+                  R25, R25_H,\n+                  R26, R26_H,\n+                  R27, R27_H,\n+                  R28, R28_H,\n+                  R29, R29_H,\n+                  R30, R30_H,\n+                  R31, R31_H);\n+\n+\/\/ Class for all int registers including APX extended GPRs.\n+reg_class all_int_reg(RAX\n+                      RDX,\n+                      RBP,\n+                      RDI,\n+                      RSI,\n+                      RCX,\n+                      RBX,\n+                      R8,\n+                      R9,\n+                      R10,\n+                      R11,\n+                      R12,\n+                      R13,\n+                      R14,\n+                      R16,\n+                      R17,\n+                      R18,\n+                      R19,\n+                      R20,\n+                      R21,\n+                      R22,\n+                      R23,\n+                      R24,\n+                      R25,\n+                      R26,\n+                      R27,\n+                      R28,\n+                      R29,\n+                      R30,\n+                      R31);\n+\n+\/\/ Class for all pointer registers\n+reg_class any_reg %{\n+  return _ANY_REG_mask;\n+%}\n+\n+\/\/ Class for all pointer registers (excluding RSP)\n+reg_class ptr_reg %{\n+  return _PTR_REG_mask;\n+%}\n+\n+\/\/ Class for all pointer registers (excluding RSP and RBP)\n+reg_class ptr_reg_no_rbp %{\n+  return _PTR_REG_NO_RBP_mask;\n+%}\n+\n+\/\/ Class for all pointer registers (excluding RAX and RSP)\n+reg_class ptr_no_rax_reg %{\n+  return _PTR_NO_RAX_REG_mask;\n+%}\n+\n+\/\/ Class for all pointer registers (excluding RAX, RBX, and RSP)\n+reg_class ptr_no_rax_rbx_reg %{\n+  return _PTR_NO_RAX_RBX_REG_mask;\n+%}\n+\n+\/\/ Class for all long registers (excluding RSP)\n+reg_class long_reg %{\n+  return _LONG_REG_mask;\n+%}\n+\n+\/\/ Class for all long registers (excluding RAX, RDX and RSP)\n+reg_class long_no_rax_rdx_reg %{\n+  return _LONG_NO_RAX_RDX_REG_mask;\n+%}\n+\n+\/\/ Class for all long registers (excluding RCX and RSP)\n+reg_class long_no_rcx_reg %{\n+  return _LONG_NO_RCX_REG_mask;\n+%}\n+\n+\/\/ Class for all long registers (excluding RBP and R13)\n+reg_class long_no_rbp_r13_reg %{\n+  return _LONG_NO_RBP_R13_REG_mask;\n+%}\n+\n+\/\/ Class for all int registers (excluding RSP)\n+reg_class int_reg %{\n+  return _INT_REG_mask;\n+%}\n+\n+\/\/ Class for all int registers (excluding RAX, RDX, and RSP)\n+reg_class int_no_rax_rdx_reg %{\n+  return _INT_NO_RAX_RDX_REG_mask;\n+%}\n+\n+\/\/ Class for all int registers (excluding RCX and RSP)\n+reg_class int_no_rcx_reg %{\n+  return _INT_NO_RCX_REG_mask;\n+%}\n+\n+\/\/ Class for all int registers (excluding RBP and R13)\n+reg_class int_no_rbp_r13_reg %{\n+  return _INT_NO_RBP_R13_REG_mask;\n+%}\n+\n+\/\/ Singleton class for RAX pointer register\n+reg_class ptr_rax_reg(RAX, RAX_H);\n+\n+\/\/ Singleton class for RBX pointer register\n+reg_class ptr_rbx_reg(RBX, RBX_H);\n+\n+\/\/ Singleton class for RSI pointer register\n+reg_class ptr_rsi_reg(RSI, RSI_H);\n+\n+\/\/ Singleton class for RBP pointer register\n+reg_class ptr_rbp_reg(RBP, RBP_H);\n+\n+\/\/ Singleton class for RDI pointer register\n+reg_class ptr_rdi_reg(RDI, RDI_H);\n+\n+\/\/ Singleton class for stack pointer\n+reg_class ptr_rsp_reg(RSP, RSP_H);\n+\n+\/\/ Singleton class for TLS pointer\n+reg_class ptr_r15_reg(R15, R15_H);\n+\n+\/\/ Singleton class for RAX long register\n+reg_class long_rax_reg(RAX, RAX_H);\n+\n+\/\/ Singleton class for RCX long register\n+reg_class long_rcx_reg(RCX, RCX_H);\n+\n+\/\/ Singleton class for RDX long register\n+reg_class long_rdx_reg(RDX, RDX_H);\n+\n+\/\/ Singleton class for R11 long register\n+reg_class long_r11_reg(R11, R11_H);\n+\n+\/\/ Singleton class for RAX int register\n+reg_class int_rax_reg(RAX);\n+\n+\/\/ Singleton class for RBX int register\n+reg_class int_rbx_reg(RBX);\n+\n+\/\/ Singleton class for RCX int register\n+reg_class int_rcx_reg(RCX);\n+\n+\/\/ Singleton class for RDX int register\n+reg_class int_rdx_reg(RDX);\n+\n+\/\/ Singleton class for RDI int register\n+reg_class int_rdi_reg(RDI);\n+\n+\/\/ Singleton class for instruction pointer\n+\/\/ reg_class ip_reg(RIP);\n+\n@@ -706,1 +1058,0 @@\n-\n@@ -1096,0 +1447,1 @@\n+\n@@ -1104,7 +1456,1 @@\n-\/\/ Header information of the source block.\n-\/\/ Method declarations\/definitions which are used outside\n-\/\/ the ad-scope can conveniently be defined here.\n-\/\/\n-\/\/ To keep related declarations\/definitions\/uses close together,\n-\/\/ we switch between source %{ }% and source_hpp %{ }% freely as needed.\n-#include \"runtime\/vm_version.hpp\"\n+#include \"peephole_x86_64.hpp\"\n@@ -1113,1 +1459,1 @@\n-class NativeJump;\n+bool castLL_is_imm32(const Node* n);\n@@ -1115,1 +1461,1 @@\n-class CallStubImpl {\n+%}\n@@ -1117,3 +1463,1 @@\n-  \/\/--------------------------------------------------------------\n-  \/\/---<  Used for optimization in Compile::shorten_branches  >---\n-  \/\/--------------------------------------------------------------\n+source %{\n@@ -1121,5 +1465,5 @@\n- public:\n-  \/\/ Size of call trampoline stub.\n-  static uint size_call_trampoline() {\n-    return 0; \/\/ no call trampolines on this platform\n-  }\n+bool castLL_is_imm32(const Node* n) {\n+  assert(n->is_CastLL(), \"must be a CastLL\");\n+  const TypeLong* t = n->bottom_type()->is_long();\n+  return (t->_lo == min_jlong || Assembler::is_simm32(t->_lo)) && (t->_hi == max_jlong || Assembler::is_simm32(t->_hi));\n+}\n@@ -1127,5 +1471,1 @@\n-  \/\/ number of relocations needed by a call trampoline stub\n-  static uint reloc_call_trampoline() {\n-    return 0; \/\/ no call trampolines on this platform\n-  }\n-};\n+%}\n@@ -1133,1 +1473,2 @@\n-class HandlerImpl {\n+\/\/ Register masks\n+source_hpp %{\n@@ -1135,1 +1476,14 @@\n- public:\n+extern RegMask _ANY_REG_mask;\n+extern RegMask _PTR_REG_mask;\n+extern RegMask _PTR_REG_NO_RBP_mask;\n+extern RegMask _PTR_NO_RAX_REG_mask;\n+extern RegMask _PTR_NO_RAX_RBX_REG_mask;\n+extern RegMask _LONG_REG_mask;\n+extern RegMask _LONG_NO_RAX_RDX_REG_mask;\n+extern RegMask _LONG_NO_RCX_REG_mask;\n+extern RegMask _LONG_NO_RBP_R13_REG_mask;\n+extern RegMask _INT_REG_mask;\n+extern RegMask _INT_NO_RAX_RDX_REG_mask;\n+extern RegMask _INT_NO_RCX_REG_mask;\n+extern RegMask _INT_NO_RBP_R13_REG_mask;\n+extern RegMask _FLOAT_REG_mask;\n@@ -1137,2 +1491,37 @@\n-  static int emit_exception_handler(C2_MacroAssembler *masm);\n-  static int emit_deopt_handler(C2_MacroAssembler* masm);\n+extern RegMask _STACK_OR_PTR_REG_mask;\n+extern RegMask _STACK_OR_LONG_REG_mask;\n+extern RegMask _STACK_OR_INT_REG_mask;\n+\n+inline const RegMask& STACK_OR_PTR_REG_mask()  { return _STACK_OR_PTR_REG_mask;  }\n+inline const RegMask& STACK_OR_LONG_REG_mask() { return _STACK_OR_LONG_REG_mask; }\n+inline const RegMask& STACK_OR_INT_REG_mask()  { return _STACK_OR_INT_REG_mask;  }\n+\n+%}\n+\n+source %{\n+#define   RELOC_IMM64    Assembler::imm_operand\n+#define   RELOC_DISP32   Assembler::disp32_operand\n+\n+#define __ masm->\n+\n+RegMask _ANY_REG_mask;\n+RegMask _PTR_REG_mask;\n+RegMask _PTR_REG_NO_RBP_mask;\n+RegMask _PTR_NO_RAX_REG_mask;\n+RegMask _PTR_NO_RAX_RBX_REG_mask;\n+RegMask _LONG_REG_mask;\n+RegMask _LONG_NO_RAX_RDX_REG_mask;\n+RegMask _LONG_NO_RCX_REG_mask;\n+RegMask _LONG_NO_RBP_R13_REG_mask;\n+RegMask _INT_REG_mask;\n+RegMask _INT_NO_RAX_RDX_REG_mask;\n+RegMask _INT_NO_RCX_REG_mask;\n+RegMask _INT_NO_RBP_R13_REG_mask;\n+RegMask _FLOAT_REG_mask;\n+RegMask _STACK_OR_PTR_REG_mask;\n+RegMask _STACK_OR_LONG_REG_mask;\n+RegMask _STACK_OR_INT_REG_mask;\n+\n+static bool need_r12_heapbase() {\n+  return UseCompressedOops;\n+}\n@@ -1140,7 +1529,14 @@\n-  static uint size_exception_handler() {\n-    \/\/ NativeCall instruction size is the same as NativeJump.\n-    \/\/ exception handler starts out as jump and can be patched to\n-    \/\/ a call be deoptimization.  (4932387)\n-    \/\/ Note that this value is also credited (in output.cpp) to\n-    \/\/ the size of the code section.\n-    return NativeJump::instruction_size;\n+void reg_mask_init() {\n+  constexpr Register egprs[] = {r16, r17, r18, r19, r20, r21, r22, r23, r24, r25, r26, r27, r28, r29, r30, r31};\n+\n+  \/\/ _ALL_REG_mask is generated by adlc from the all_reg register class below.\n+  \/\/ We derive a number of subsets from it.\n+  _ANY_REG_mask.assignFrom(_ALL_REG_mask);\n+\n+  if (PreserveFramePointer) {\n+    _ANY_REG_mask.remove(OptoReg::as_OptoReg(rbp->as_VMReg()));\n+    _ANY_REG_mask.remove(OptoReg::as_OptoReg(rbp->as_VMReg()->next()));\n+  }\n+  if (need_r12_heapbase()) {\n+    _ANY_REG_mask.remove(OptoReg::as_OptoReg(r12->as_VMReg()));\n+    _ANY_REG_mask.remove(OptoReg::as_OptoReg(r12->as_VMReg()->next()));\n@@ -1149,3 +1545,10 @@\n-  static uint size_deopt_handler() {\n-    \/\/ three 5 byte instructions plus one move for unreachable address.\n-    return 15+3;\n+  _PTR_REG_mask.assignFrom(_ANY_REG_mask);\n+  _PTR_REG_mask.remove(OptoReg::as_OptoReg(rsp->as_VMReg()));\n+  _PTR_REG_mask.remove(OptoReg::as_OptoReg(rsp->as_VMReg()->next()));\n+  _PTR_REG_mask.remove(OptoReg::as_OptoReg(r15->as_VMReg()));\n+  _PTR_REG_mask.remove(OptoReg::as_OptoReg(r15->as_VMReg()->next()));\n+  if (!UseAPX) {\n+    for (uint i = 0; i < sizeof(egprs)\/sizeof(Register); i++) {\n+      _PTR_REG_mask.remove(OptoReg::as_OptoReg(egprs[i]->as_VMReg()));\n+      _PTR_REG_mask.remove(OptoReg::as_OptoReg(egprs[i]->as_VMReg()->next()));\n+    }\n@@ -1153,8 +1556,2 @@\n-};\n-inline Assembler::AvxVectorLen vector_length_encoding(int bytes) {\n-  switch(bytes) {\n-    case  4: \/\/ fall-through\n-    case  8: \/\/ fall-through\n-    case 16: return Assembler::AVX_128bit;\n-    case 32: return Assembler::AVX_256bit;\n-    case 64: return Assembler::AVX_512bit;\n+  _STACK_OR_PTR_REG_mask.assignFrom(_PTR_REG_mask);\n+  _STACK_OR_PTR_REG_mask.or_with(STACK_OR_STACK_SLOTS_mask());\n@@ -1163,3 +1560,37 @@\n-    default: {\n-      ShouldNotReachHere();\n-      return Assembler::AVX_NoVec;\n+  _PTR_REG_NO_RBP_mask.assignFrom(_PTR_REG_mask);\n+  _PTR_REG_NO_RBP_mask.remove(OptoReg::as_OptoReg(rbp->as_VMReg()));\n+  _PTR_REG_NO_RBP_mask.remove(OptoReg::as_OptoReg(rbp->as_VMReg()->next()));\n+\n+  _PTR_NO_RAX_REG_mask.assignFrom(_PTR_REG_mask);\n+  _PTR_NO_RAX_REG_mask.remove(OptoReg::as_OptoReg(rax->as_VMReg()));\n+  _PTR_NO_RAX_REG_mask.remove(OptoReg::as_OptoReg(rax->as_VMReg()->next()));\n+\n+  _PTR_NO_RAX_RBX_REG_mask.assignFrom(_PTR_NO_RAX_REG_mask);\n+  _PTR_NO_RAX_RBX_REG_mask.remove(OptoReg::as_OptoReg(rbx->as_VMReg()));\n+  _PTR_NO_RAX_RBX_REG_mask.remove(OptoReg::as_OptoReg(rbx->as_VMReg()->next()));\n+\n+\n+  _LONG_REG_mask.assignFrom(_PTR_REG_mask);\n+  _STACK_OR_LONG_REG_mask.assignFrom(_LONG_REG_mask);\n+  _STACK_OR_LONG_REG_mask.or_with(STACK_OR_STACK_SLOTS_mask());\n+\n+  _LONG_NO_RAX_RDX_REG_mask.assignFrom(_LONG_REG_mask);\n+  _LONG_NO_RAX_RDX_REG_mask.remove(OptoReg::as_OptoReg(rax->as_VMReg()));\n+  _LONG_NO_RAX_RDX_REG_mask.remove(OptoReg::as_OptoReg(rax->as_VMReg()->next()));\n+  _LONG_NO_RAX_RDX_REG_mask.remove(OptoReg::as_OptoReg(rdx->as_VMReg()));\n+  _LONG_NO_RAX_RDX_REG_mask.remove(OptoReg::as_OptoReg(rdx->as_VMReg()->next()));\n+\n+  _LONG_NO_RCX_REG_mask.assignFrom(_LONG_REG_mask);\n+  _LONG_NO_RCX_REG_mask.remove(OptoReg::as_OptoReg(rcx->as_VMReg()));\n+  _LONG_NO_RCX_REG_mask.remove(OptoReg::as_OptoReg(rcx->as_VMReg()->next()));\n+\n+  _LONG_NO_RBP_R13_REG_mask.assignFrom(_LONG_REG_mask);\n+  _LONG_NO_RBP_R13_REG_mask.remove(OptoReg::as_OptoReg(rbp->as_VMReg()));\n+  _LONG_NO_RBP_R13_REG_mask.remove(OptoReg::as_OptoReg(rbp->as_VMReg()->next()));\n+  _LONG_NO_RBP_R13_REG_mask.remove(OptoReg::as_OptoReg(r13->as_VMReg()));\n+  _LONG_NO_RBP_R13_REG_mask.remove(OptoReg::as_OptoReg(r13->as_VMReg()->next()));\n+\n+  _INT_REG_mask.assignFrom(_ALL_INT_REG_mask);\n+  if (!UseAPX) {\n+    for (uint i = 0; i < sizeof(egprs)\/sizeof(Register); i++) {\n+      _INT_REG_mask.remove(OptoReg::as_OptoReg(egprs[i]->as_VMReg()));\n@@ -1168,0 +1599,25 @@\n+\n+  if (PreserveFramePointer) {\n+    _INT_REG_mask.remove(OptoReg::as_OptoReg(rbp->as_VMReg()));\n+  }\n+  if (need_r12_heapbase()) {\n+    _INT_REG_mask.remove(OptoReg::as_OptoReg(r12->as_VMReg()));\n+  }\n+\n+  _STACK_OR_INT_REG_mask.assignFrom(_INT_REG_mask);\n+  _STACK_OR_INT_REG_mask.or_with(STACK_OR_STACK_SLOTS_mask());\n+\n+  _INT_NO_RAX_RDX_REG_mask.assignFrom(_INT_REG_mask);\n+  _INT_NO_RAX_RDX_REG_mask.remove(OptoReg::as_OptoReg(rax->as_VMReg()));\n+  _INT_NO_RAX_RDX_REG_mask.remove(OptoReg::as_OptoReg(rdx->as_VMReg()));\n+\n+  _INT_NO_RCX_REG_mask.assignFrom(_INT_REG_mask);\n+  _INT_NO_RCX_REG_mask.remove(OptoReg::as_OptoReg(rcx->as_VMReg()));\n+\n+  _INT_NO_RBP_R13_REG_mask.assignFrom(_INT_REG_mask);\n+  _INT_NO_RBP_R13_REG_mask.remove(OptoReg::as_OptoReg(rbp->as_VMReg()));\n+  _INT_NO_RBP_R13_REG_mask.remove(OptoReg::as_OptoReg(r13->as_VMReg()));\n+\n+  \/\/ _FLOAT_REG_LEGACY_mask\/_FLOAT_REG_EVEX_mask is generated by adlc\n+  \/\/ from the float_reg_legacy\/float_reg_evex register class.\n+  _FLOAT_REG_mask.assignFrom(VM_Version::supports_evex() ? _FLOAT_REG_EVEX_mask : _FLOAT_REG_LEGACY_mask);\n@@ -1170,2 +1626,2 @@\n-static inline Assembler::AvxVectorLen vector_length_encoding(const Node* n) {\n-  return vector_length_encoding(Matcher::vector_length_in_bytes(n));\n+static bool generate_vzeroupper(Compile* C) {\n+  return (VM_Version::supports_vzeroupper() && (C->max_vector_size() > 16 || C->clear_upper_avx() == true)) ? true: false;  \/\/ Generate vzeroupper\n@@ -1174,4 +1630,2 @@\n-static inline Assembler::AvxVectorLen vector_length_encoding(const MachNode* use, MachOper* opnd) {\n-  uint def_idx = use->operand_index(opnd);\n-  Node* def = use->in(def_idx);\n-  return vector_length_encoding(def);\n+static int clear_avx_size() {\n+  return generate_vzeroupper(Compile::current()) ? 3: 0;  \/\/ vzeroupper\n@@ -1180,3 +1634,8 @@\n-static inline bool is_vector_popcount_predicate(BasicType bt) {\n-  return (is_subword_type(bt) && VM_Version::supports_avx512_bitalg()) ||\n-         (is_non_subword_integral_type(bt) && VM_Version::supports_avx512_vpopcntdq());\n+\/\/ !!!!! Special hack to get all types of calls to specify the byte offset\n+\/\/       from the start of the call to the point where the return address\n+\/\/       will point.\n+int MachCallStaticJavaNode::ret_addr_offset()\n+{\n+  int offset = 5; \/\/ 5 bytes from start of call to where return address points\n+  offset += clear_avx_size();\n+  return offset;\n@@ -1185,3 +1644,5 @@\n-static inline bool is_clz_non_subword_predicate_evex(BasicType bt, int vlen_bytes) {\n-  return is_non_subword_integral_type(bt) && VM_Version::supports_avx512cd() &&\n-           (VM_Version::supports_avx512vl() || vlen_bytes == 64);\n+int MachCallDynamicJavaNode::ret_addr_offset()\n+{\n+  int offset = 15; \/\/ 15 bytes from start of call to where return address points\n+  offset += clear_avx_size();\n+  return offset;\n@@ -1190,17 +1651,10 @@\n-class Node::PD {\n-public:\n-  enum NodeFlags {\n-    Flag_intel_jcc_erratum    = Node::_last_flag << 1,\n-    Flag_sets_carry_flag      = Node::_last_flag << 2,\n-    Flag_sets_parity_flag     = Node::_last_flag << 3,\n-    Flag_sets_zero_flag       = Node::_last_flag << 4,\n-    Flag_sets_overflow_flag   = Node::_last_flag << 5,\n-    Flag_sets_sign_flag       = Node::_last_flag << 6,\n-    Flag_clears_carry_flag    = Node::_last_flag << 7,\n-    Flag_clears_parity_flag   = Node::_last_flag << 8,\n-    Flag_clears_zero_flag     = Node::_last_flag << 9,\n-    Flag_clears_overflow_flag = Node::_last_flag << 10,\n-    Flag_clears_sign_flag     = Node::_last_flag << 11,\n-    _last_flag                = Flag_clears_sign_flag\n-  };\n-};\n+int MachCallRuntimeNode::ret_addr_offset() {\n+  int offset = 13; \/\/ movq r10,#addr; callq (r10)\n+  if (this->ideal_Opcode() != Op_CallLeafVector) {\n+    offset += clear_avx_size();\n+  }\n+  return offset;\n+}\n+\/\/\n+\/\/ Compute padding required for nodes which need alignment\n+\/\/\n@@ -1208,1 +1662,8 @@\n-%} \/\/ end source_hpp\n+\/\/ The address of the call instruction needs to be 4-byte aligned to\n+\/\/ ensure that it does not span a cache line so that it can be patched.\n+int CallStaticJavaDirectNode::compute_padding(int current_offset) const\n+{\n+  current_offset += clear_avx_size(); \/\/ skip vzeroupper\n+  current_offset += 1; \/\/ skip call opcode byte\n+  return align_up(current_offset, alignment_required()) - current_offset;\n+}\n@@ -1210,1 +1671,8 @@\n-source %{\n+\/\/ The address of the call instruction needs to be 4-byte aligned to\n+\/\/ ensure that it does not span a cache line so that it can be patched.\n+int CallDynamicJavaDirectNode::compute_padding(int current_offset) const\n+{\n+  current_offset += clear_avx_size(); \/\/ skip vzeroupper\n+  current_offset += 11; \/\/ skip movq instruction + call opcode byte\n+  return align_up(current_offset, alignment_required()) - current_offset;\n+}\n@@ -1212,2 +1680,20 @@\n-#include \"opto\/addnode.hpp\"\n-#include \"c2_intelJccErratum_x86.hpp\"\n+\/\/ This could be in MacroAssembler but it's fairly C2 specific\n+static void emit_cmpfp_fixup(MacroAssembler* masm) {\n+  Label exit;\n+  __ jccb(Assembler::noParity, exit);\n+  __ pushf();\n+  \/\/\n+  \/\/ comiss\/ucomiss instructions set ZF,PF,CF flags and\n+  \/\/ zero OF,AF,SF for NaN values.\n+  \/\/ Fixup flags by zeroing ZF,PF so that compare of NaN\n+  \/\/ values returns 'less than' result (CF is set).\n+  \/\/ Leave the rest of flags unchanged.\n+  \/\/\n+  \/\/    7 6 5 4 3 2 1 0\n+  \/\/   |S|Z|r|A|r|P|r|C|  (r - reserved bit)\n+  \/\/    0 0 1 0 1 0 1 1   (0x2B)\n+  \/\/\n+  __ andq(Address(rsp, 0), 0xffffff2b);\n+  __ popf();\n+  __ bind(exit);\n+}\n@@ -1215,5 +1701,7 @@\n-void PhaseOutput::pd_perform_mach_node_analysis() {\n-  if (VM_Version::has_intel_jcc_erratum()) {\n-    int extra_padding = IntelJccErratum::tag_affected_machnodes(C, C->cfg(), C->regalloc());\n-    _buf_sizes._code += extra_padding;\n-  }\n+static void emit_cmpfp3(MacroAssembler* masm, Register dst) {\n+  Label done;\n+  __ movl(dst, -1);\n+  __ jcc(Assembler::parity, done);\n+  __ jcc(Assembler::below, done);\n+  __ setcc(Assembler::notEqual, dst);\n+  __ bind(done);\n@@ -1222,6 +1710,37 @@\n-int MachNode::pd_alignment_required() const {\n-  if (VM_Version::has_intel_jcc_erratum() && IntelJccErratum::is_jcc_erratum_branch(this)) {\n-    \/\/ Conservatively add worst case padding. We assume that relocInfo::addr_unit() is 1 on x86.\n-    return IntelJccErratum::largest_jcc_size() + 1;\n-  } else {\n-    return 1;\n+\/\/ Math.min()    # Math.max()\n+\/\/ --------------------------\n+\/\/ ucomis[s\/d]   #\n+\/\/ ja   -> b     # a\n+\/\/ jp   -> NaN   # NaN\n+\/\/ jb   -> a     # b\n+\/\/ je            #\n+\/\/ |-jz -> a | b # a & b\n+\/\/ |    -> a     #\n+static void emit_fp_min_max(MacroAssembler* masm, XMMRegister dst,\n+                            XMMRegister a, XMMRegister b,\n+                            XMMRegister xmmt, Register rt,\n+                            bool min, bool single) {\n+\n+  Label nan, zero, below, above, done;\n+\n+  if (single)\n+    __ ucomiss(a, b);\n+  else\n+    __ ucomisd(a, b);\n+\n+  if (dst->encoding() != (min ? b : a)->encoding())\n+    __ jccb(Assembler::above, above); \/\/ CF=0 & ZF=0\n+  else\n+    __ jccb(Assembler::above, done);\n+\n+  __ jccb(Assembler::parity, nan);  \/\/ PF=1\n+  __ jccb(Assembler::below, below); \/\/ CF=1\n+\n+  \/\/ equal\n+  __ vpxor(xmmt, xmmt, xmmt, Assembler::AVX_128bit);\n+  if (single) {\n+    __ ucomiss(a, xmmt);\n+    __ jccb(Assembler::equal, zero);\n+\n+    __ movflt(dst, a);\n+    __ jmp(done);\n@@ -1229,1 +1748,3 @@\n-}\n+  else {\n+    __ ucomisd(a, xmmt);\n+    __ jccb(Assembler::equal, zero);\n@@ -1231,9 +1752,2 @@\n-int MachNode::compute_padding(int current_offset) const {\n-  if (flags() & Node::PD::Flag_intel_jcc_erratum) {\n-    Compile* C = Compile::current();\n-    PhaseOutput* output = C->output();\n-    Block* block = output->block();\n-    int index = output->index();\n-    return IntelJccErratum::compute_padding(current_offset, this, block, index, C->regalloc());\n-  } else {\n-    return 0;\n+    __ movdbl(dst, a);\n+    __ jmp(done);\n@@ -1241,4 +1755,5 @@\n-}\n-\/\/ Emit exception handler code.\n-\/\/ Stuff framesize into a register and call a VM stub routine.\n-int HandlerImpl::emit_exception_handler(C2_MacroAssembler* masm) {\n+  __ bind(zero);\n+  if (min)\n+    __ vpor(dst, a, b, Assembler::AVX_128bit);\n+  else\n+    __ vpand(dst, a, b, Assembler::AVX_128bit);\n@@ -1247,6 +1762,14 @@\n-  \/\/ Note that the code buffer's insts_mark is always relative to insts.\n-  \/\/ That's why we must use the macroassembler to generate a handler.\n-  address base = __ start_a_stub(size_exception_handler());\n-  if (base == nullptr) {\n-    ciEnv::current()->record_failure(\"CodeCache is full\");\n-    return 0;  \/\/ CodeBuffer::expand failed\n+  __ jmp(done);\n+\n+  __ bind(above);\n+  if (single)\n+    __ movflt(dst, min ? b : a);\n+  else\n+    __ movdbl(dst, min ? b : a);\n+\n+  __ jmp(done);\n+\n+  __ bind(nan);\n+  if (single) {\n+    __ movl(rt, 0x7fc00000); \/\/ Float.NaN\n+    __ movdl(dst, rt);\n@@ -1254,5 +1777,13 @@\n-  int offset = __ offset();\n-  __ jump(RuntimeAddress(OptoRuntime::exception_blob()->entry_point()));\n-  assert(__ offset() - offset <= (int) size_exception_handler(), \"overflow\");\n-  __ end_a_stub();\n-  return offset;\n+  else {\n+    __ mov64(rt, 0x7ff8000000000000L); \/\/ Double.NaN\n+    __ movdq(dst, rt);\n+  }\n+  __ jmp(done);\n+\n+  __ bind(below);\n+  if (single)\n+    __ movflt(dst, min ? a : b);\n+  else\n+    __ movdbl(dst, min ? a : b);\n+\n+  __ bind(done);\n@@ -1261,2 +1792,2 @@\n-\/\/ Emit deopt handler code.\n-int HandlerImpl::emit_deopt_handler(C2_MacroAssembler* masm) {\n+\/\/=============================================================================\n+const RegMask& MachConstantBaseNode::_out_RegMask = RegMask::EMPTY;\n@@ -1264,8 +1795,3 @@\n-  \/\/ Note that the code buffer's insts_mark is always relative to insts.\n-  \/\/ That's why we must use the macroassembler to generate a handler.\n-  address base = __ start_a_stub(size_deopt_handler());\n-  if (base == nullptr) {\n-    ciEnv::current()->record_failure(\"CodeCache is full\");\n-    return 0;  \/\/ CodeBuffer::expand failed\n-  }\n-  int offset = __ offset();\n+int ConstantTable::calculate_table_base_offset() const {\n+  return 0;  \/\/ absolute addressing, no offset\n+}\n@@ -1273,4 +1799,4 @@\n-  address the_pc = (address) __ pc();\n-  Label next;\n-  \/\/ push a \"the_pc\" on the stack without destroying any registers\n-  \/\/ as they all may be live.\n+bool MachConstantBaseNode::requires_postalloc_expand() const { return false; }\n+void MachConstantBaseNode::postalloc_expand(GrowableArray <Node *> *nodes, PhaseRegAlloc *ra_) {\n+  ShouldNotReachHere();\n+}\n@@ -1278,5 +1804,3 @@\n-  \/\/ push address of \"next\"\n-  __ call(next, relocInfo::none); \/\/ reloc none is fine since it is a disp32\n-  __ bind(next);\n-  \/\/ adjust it so it matches \"the_pc\"\n-  __ subptr(Address(rsp, 0), __ offset() - offset);\n+void MachConstantBaseNode::emit(C2_MacroAssembler* masm, PhaseRegAlloc* ra_) const {\n+  \/\/ Empty encoding\n+}\n@@ -1284,4 +1808,2 @@\n-  __ jump(RuntimeAddress(SharedRuntime::deopt_blob()->unpack()));\n-  assert(__ offset() - offset <= (int) size_deopt_handler(), \"overflow %d\", (__ offset() - offset));\n-  __ end_a_stub();\n-  return offset;\n+uint MachConstantBaseNode::size(PhaseRegAlloc* ra_) const {\n+  return 0;\n@@ -1290,11 +1812,3 @@\n-static Assembler::Width widthForType(BasicType bt) {\n-  if (bt == T_BYTE) {\n-    return Assembler::B;\n-  } else if (bt == T_SHORT) {\n-    return Assembler::W;\n-  } else if (bt == T_INT) {\n-    return Assembler::D;\n-  } else {\n-    assert(bt == T_LONG, \"not a long: %s\", type2name(bt));\n-    return Assembler::Q;\n-  }\n+#ifndef PRODUCT\n+void MachConstantBaseNode::format(PhaseRegAlloc* ra_, outputStream* st) const {\n+  st->print(\"# MachConstantBaseNode (empty encoding)\");\n@@ -1302,0 +1816,2 @@\n+#endif\n+\n@@ -1304,0 +1820,37 @@\n+#ifndef PRODUCT\n+void MachPrologNode::format(PhaseRegAlloc* ra_, outputStream* st) const {\n+  Compile* C = ra_->C;\n+\n+  int framesize = C->output()->frame_size_in_bytes();\n+  int bangsize = C->output()->bang_size_in_bytes();\n+  assert((framesize & (StackAlignmentInBytes-1)) == 0, \"frame size not aligned\");\n+  \/\/ Remove wordSize for return addr which is already pushed.\n+  framesize -= wordSize;\n+\n+  if (C->output()->need_stack_bang(bangsize)) {\n+    framesize -= wordSize;\n+    st->print(\"# stack bang (%d bytes)\", bangsize);\n+    st->print(\"\\n\\t\");\n+    st->print(\"pushq   rbp\\t# Save rbp\");\n+    if (PreserveFramePointer) {\n+        st->print(\"\\n\\t\");\n+        st->print(\"movq    rbp, rsp\\t# Save the caller's SP into rbp\");\n+    }\n+    if (framesize) {\n+      st->print(\"\\n\\t\");\n+      st->print(\"subq    rsp, #%d\\t# Create frame\",framesize);\n+    }\n+  } else {\n+    st->print(\"subq    rsp, #%d\\t# Create frame\",framesize);\n+    st->print(\"\\n\\t\");\n+    framesize -= wordSize;\n+    st->print(\"movq    [rsp + #%d], rbp\\t# Save rbp\",framesize);\n+    if (PreserveFramePointer) {\n+      st->print(\"\\n\\t\");\n+      st->print(\"movq    rbp, rsp\\t# Save the caller's SP into rbp\");\n+      if (framesize > 0) {\n+        st->print(\"\\n\\t\");\n+        st->print(\"addq    rbp, #%d\", framesize);\n+      }\n+    }\n+  }\n@@ -1305,20 +1858,65 @@\n-  \/\/ Float masks come from different places depending on platform.\n-  static address float_signmask()  { return StubRoutines::x86::float_sign_mask(); }\n-  static address float_signflip()  { return StubRoutines::x86::float_sign_flip(); }\n-  static address double_signmask() { return StubRoutines::x86::double_sign_mask(); }\n-  static address double_signflip() { return StubRoutines::x86::double_sign_flip(); }\n-  static address vector_short_to_byte_mask() { return StubRoutines::x86::vector_short_to_byte_mask(); }\n-  static address vector_int_to_byte_mask() { return StubRoutines::x86::vector_int_to_byte_mask(); }\n-  static address vector_byte_perm_mask() { return StubRoutines::x86::vector_byte_perm_mask(); }\n-  static address vector_long_sign_mask() { return StubRoutines::x86::vector_long_sign_mask(); }\n-  static address vector_all_bits_set() { return StubRoutines::x86::vector_all_bits_set(); }\n-  static address vector_int_mask_cmp_bits() { return StubRoutines::x86::vector_int_mask_cmp_bits(); }\n-  static address vector_int_to_short_mask() { return StubRoutines::x86::vector_int_to_short_mask(); }\n-  static address vector_byte_shufflemask() { return StubRoutines::x86::vector_byte_shuffle_mask(); }\n-  static address vector_short_shufflemask() { return StubRoutines::x86::vector_short_shuffle_mask(); }\n-  static address vector_int_shufflemask() { return StubRoutines::x86::vector_int_shuffle_mask(); }\n-  static address vector_long_shufflemask() { return StubRoutines::x86::vector_long_shuffle_mask(); }\n-  static address vector_32_bit_mask() { return StubRoutines::x86::vector_32_bit_mask(); }\n-  static address vector_64_bit_mask() { return StubRoutines::x86::vector_64_bit_mask(); }\n-  static address vector_float_signflip() { return StubRoutines::x86::vector_float_sign_flip();}\n-  static address vector_double_signflip() { return StubRoutines::x86::vector_double_sign_flip();}\n+  if (VerifyStackAtCalls) {\n+    st->print(\"\\n\\t\");\n+    framesize -= wordSize;\n+    st->print(\"movq    [rsp + #%d], 0xbadb100d\\t# Majik cookie for stack depth check\",framesize);\n+#ifdef ASSERT\n+    st->print(\"\\n\\t\");\n+    st->print(\"# stack alignment check\");\n+#endif\n+  }\n+  if (C->stub_function() != nullptr) {\n+    st->print(\"\\n\\t\");\n+    st->print(\"cmpl    [r15_thread + #disarmed_guard_value_offset], #disarmed_guard_value\\t\");\n+    st->print(\"\\n\\t\");\n+    st->print(\"je      fast_entry\\t\");\n+    st->print(\"\\n\\t\");\n+    st->print(\"call    #nmethod_entry_barrier_stub\\t\");\n+    st->print(\"\\n\\tfast_entry:\");\n+  }\n+  st->cr();\n+}\n+#endif\n+\n+void MachPrologNode::emit(C2_MacroAssembler *masm, PhaseRegAlloc *ra_) const {\n+  Compile* C = ra_->C;\n+\n+  int framesize = C->output()->frame_size_in_bytes();\n+  int bangsize = C->output()->bang_size_in_bytes();\n+\n+  if (C->clinit_barrier_on_entry()) {\n+    assert(VM_Version::supports_fast_class_init_checks(), \"sanity\");\n+    assert(!C->method()->holder()->is_not_initialized(), \"initialization should have been started\");\n+\n+    Label L_skip_barrier;\n+    Register klass = rscratch1;\n+\n+    __ mov_metadata(klass, C->method()->holder()->constant_encoding());\n+    __ clinit_barrier(klass, &L_skip_barrier \/*L_fast_path*\/);\n+\n+    __ jump(RuntimeAddress(SharedRuntime::get_handle_wrong_method_stub())); \/\/ slow path\n+\n+    __ bind(L_skip_barrier);\n+  }\n+\n+  __ verified_entry(framesize, C->output()->need_stack_bang(bangsize)?bangsize:0, false, C->stub_function() != nullptr);\n+\n+  C->output()->set_frame_complete(__ offset());\n+\n+  if (C->has_mach_constant_base_node()) {\n+    \/\/ NOTE: We set the table base offset here because users might be\n+    \/\/ emitted before MachConstantBaseNode.\n+    ConstantTable& constant_table = C->output()->constant_table();\n+    constant_table.set_table_base_offset(constant_table.calculate_table_base_offset());\n+  }\n+}\n+\n+uint MachPrologNode::size(PhaseRegAlloc* ra_) const\n+{\n+  return MachNode::size(ra_); \/\/ too many variables; just compute it\n+                              \/\/ the hard way\n+}\n+\n+int MachPrologNode::reloc() const\n+{\n+  return 0; \/\/ a large enough number\n+}\n@@ -1327,3 +1925,7 @@\n-bool Matcher::match_rule_supported(int opcode) {\n-  if (!has_match_rule(opcode)) {\n-    return false; \/\/ no match rule present\n+#ifndef PRODUCT\n+void MachEpilogNode::format(PhaseRegAlloc* ra_, outputStream* st) const\n+{\n+  Compile* C = ra_->C;\n+  if (generate_vzeroupper(C)) {\n+    st->print(\"vzeroupper\");\n+    st->cr(); st->print(\"\\t\");\n@@ -1331,12 +1933,121 @@\n-  switch (opcode) {\n-    case Op_AbsVL:\n-    case Op_StoreVectorScatter:\n-      if (UseAVX < 3) {\n-        return false;\n-      }\n-      break;\n-    case Op_PopCountI:\n-    case Op_PopCountL:\n-      if (!UsePopCountInstruction) {\n-        return false;\n-      }\n+\n+  int framesize = C->output()->frame_size_in_bytes();\n+  assert((framesize & (StackAlignmentInBytes-1)) == 0, \"frame size not aligned\");\n+  \/\/ Remove word for return adr already pushed\n+  \/\/ and RBP\n+  framesize -= 2*wordSize;\n+\n+  if (framesize) {\n+    st->print_cr(\"addq    rsp, %d\\t# Destroy frame\", framesize);\n+    st->print(\"\\t\");\n+  }\n+\n+  st->print_cr(\"popq    rbp\");\n+  if (do_polling() && C->is_method_compilation()) {\n+    st->print(\"\\t\");\n+    st->print_cr(\"cmpq    rsp, poll_offset[r15_thread] \\n\\t\"\n+                 \"ja      #safepoint_stub\\t\"\n+                 \"# Safepoint: poll for GC\");\n+  }\n+}\n+#endif\n+\n+void MachEpilogNode::emit(C2_MacroAssembler* masm, PhaseRegAlloc* ra_) const\n+{\n+  Compile* C = ra_->C;\n+\n+  if (generate_vzeroupper(C)) {\n+    \/\/ Clear upper bits of YMM registers when current compiled code uses\n+    \/\/ wide vectors to avoid AVX <-> SSE transition penalty during call.\n+    __ vzeroupper();\n+  }\n+\n+  int framesize = C->output()->frame_size_in_bytes();\n+  assert((framesize & (StackAlignmentInBytes-1)) == 0, \"frame size not aligned\");\n+  \/\/ Remove word for return adr already pushed\n+  \/\/ and RBP\n+  framesize -= 2*wordSize;\n+\n+  \/\/ Note that VerifyStackAtCalls' Majik cookie does not change the frame size popped here\n+\n+  if (framesize) {\n+    __ addq(rsp, framesize);\n+  }\n+\n+  __ popq(rbp);\n+\n+  if (StackReservedPages > 0 && C->has_reserved_stack_access()) {\n+    __ reserved_stack_check();\n+  }\n+\n+  if (do_polling() && C->is_method_compilation()) {\n+    Label dummy_label;\n+    Label* code_stub = &dummy_label;\n+    if (!C->output()->in_scratch_emit_size()) {\n+      C2SafepointPollStub* stub = new (C->comp_arena()) C2SafepointPollStub(__ offset());\n+      C->output()->add_stub(stub);\n+      code_stub = &stub->entry();\n+    }\n+    __ relocate(relocInfo::poll_return_type);\n+    __ safepoint_poll(*code_stub, true \/* at_return *\/, true \/* in_nmethod *\/);\n+  }\n+}\n+\n+uint MachEpilogNode::size(PhaseRegAlloc* ra_) const\n+{\n+  return MachNode::size(ra_); \/\/ too many variables; just compute it\n+                              \/\/ the hard way\n+}\n+\n+int MachEpilogNode::reloc() const\n+{\n+  return 2; \/\/ a large enough number\n+}\n+\n+const Pipeline* MachEpilogNode::pipeline() const\n+{\n+  return MachNode::pipeline_class();\n+}\n+\n+\/\/=============================================================================\n+\n+enum RC {\n+  rc_bad,\n+  rc_int,\n+  rc_kreg,\n+  rc_float,\n+  rc_stack\n+};\n+\n+static enum RC rc_class(OptoReg::Name reg)\n+{\n+  if( !OptoReg::is_valid(reg)  ) return rc_bad;\n+\n+  if (OptoReg::is_stack(reg)) return rc_stack;\n+\n+  VMReg r = OptoReg::as_VMReg(reg);\n+\n+  if (r->is_Register()) return rc_int;\n+\n+  if (r->is_KRegister()) return rc_kreg;\n+\n+  assert(r->is_XMMRegister(), \"must be\");\n+  return rc_float;\n+}\n+\n+\/\/ Next two methods are shared by 32- and 64-bit VM. They are defined in x86.ad.\n+static void vec_mov_helper(C2_MacroAssembler *masm, int src_lo, int dst_lo,\n+                          int src_hi, int dst_hi, uint ireg, outputStream* st);\n+\n+void vec_spill_helper(C2_MacroAssembler *masm, bool is_load,\n+                     int stack_offset, int reg, uint ireg, outputStream* st);\n+\n+static void vec_stack_to_stack_helper(C2_MacroAssembler *masm, int src_offset,\n+                                      int dst_offset, uint ireg, outputStream* st) {\n+  if (masm) {\n+    switch (ireg) {\n+    case Op_VecS:\n+      __ movq(Address(rsp, -8), rax);\n+      __ movl(rax, Address(rsp, src_offset));\n+      __ movl(Address(rsp, dst_offset), rax);\n+      __ movq(rax, Address(rsp, -8));\n@@ -1344,4 +2055,3 @@\n-    case Op_PopCountVI:\n-      if (UseAVX < 2) {\n-        return false;\n-      }\n+    case Op_VecD:\n+      __ pushq(Address(rsp, src_offset));\n+      __ popq (Address(rsp, dst_offset));\n@@ -1349,6 +2059,5 @@\n-    case Op_CompressV:\n-    case Op_ExpandV:\n-    case Op_PopCountVL:\n-      if (UseAVX < 2) {\n-        return false;\n-      }\n+    case Op_VecX:\n+      __ pushq(Address(rsp, src_offset));\n+      __ popq (Address(rsp, dst_offset));\n+      __ pushq(Address(rsp, src_offset+8));\n+      __ popq (Address(rsp, dst_offset+8));\n@@ -1356,4 +2065,5 @@\n-    case Op_MulVI:\n-      if ((UseSSE < 4) && (UseAVX < 1)) { \/\/ only with SSE4_1 or AVX\n-        return false;\n-      }\n+    case Op_VecY:\n+      __ vmovdqu(Address(rsp, -32), xmm0);\n+      __ vmovdqu(xmm0, Address(rsp, src_offset));\n+      __ vmovdqu(Address(rsp, dst_offset), xmm0);\n+      __ vmovdqu(xmm0, Address(rsp, -32));\n@@ -1361,4 +2071,5 @@\n-    case Op_MulVL:\n-      if (UseSSE < 4) { \/\/ only with SSE4_1 or AVX\n-        return false;\n-      }\n+    case Op_VecZ:\n+      __ evmovdquq(Address(rsp, -64), xmm0, 2);\n+      __ evmovdquq(xmm0, Address(rsp, src_offset), 2);\n+      __ evmovdquq(Address(rsp, dst_offset), xmm0, 2);\n+      __ evmovdquq(xmm0, Address(rsp, -64), 2);\n@@ -1366,4 +2077,12 @@\n-    case Op_MulReductionVL:\n-      if (VM_Version::supports_avx512dq() == false) {\n-        return false;\n-      }\n+    default:\n+      ShouldNotReachHere();\n+    }\n+#ifndef PRODUCT\n+  } else {\n+    switch (ireg) {\n+    case Op_VecS:\n+      st->print(\"movq    [rsp - #8], rax\\t# 32-bit mem-mem spill\\n\\t\"\n+                \"movl    rax, [rsp + #%d]\\n\\t\"\n+                \"movl    [rsp + #%d], rax\\n\\t\"\n+                \"movq    rax, [rsp - #8]\",\n+                src_offset, dst_offset);\n@@ -1371,10 +2090,4 @@\n-    case Op_AbsVB:\n-    case Op_AbsVS:\n-    case Op_AbsVI:\n-    case Op_AddReductionVI:\n-    case Op_AndReductionV:\n-    case Op_OrReductionV:\n-    case Op_XorReductionV:\n-      if (UseSSE < 3) { \/\/ requires at least SSSE3\n-        return false;\n-      }\n+    case Op_VecD:\n+      st->print(\"pushq   [rsp + #%d]\\t# 64-bit mem-mem spill\\n\\t\"\n+                \"popq    [rsp + #%d]\",\n+                src_offset, dst_offset);\n@@ -1382,16 +2095,6 @@\n-    case Op_MaxHF:\n-    case Op_MinHF:\n-      if (!VM_Version::supports_avx512vlbw()) {\n-        return false;\n-      }  \/\/ fallthrough\n-    case Op_AddHF:\n-    case Op_DivHF:\n-    case Op_FmaHF:\n-    case Op_MulHF:\n-    case Op_ReinterpretS2HF:\n-    case Op_ReinterpretHF2S:\n-    case Op_SubHF:\n-    case Op_SqrtHF:\n-      if (!VM_Version::supports_avx512_fp16()) {\n-        return false;\n-      }\n+     case Op_VecX:\n+      st->print(\"pushq   [rsp + #%d]\\t# 128-bit mem-mem spill\\n\\t\"\n+                \"popq    [rsp + #%d]\\n\\t\"\n+                \"pushq   [rsp + #%d]\\n\\t\"\n+                \"popq    [rsp + #%d]\",\n+                src_offset, dst_offset, src_offset+8, dst_offset+8);\n@@ -1399,6 +2102,6 @@\n-    case Op_VectorLoadShuffle:\n-    case Op_VectorRearrange:\n-    case Op_MulReductionVI:\n-      if (UseSSE < 4) { \/\/ requires at least SSE4\n-        return false;\n-      }\n+    case Op_VecY:\n+      st->print(\"vmovdqu [rsp - #32], xmm0\\t# 256-bit mem-mem spill\\n\\t\"\n+                \"vmovdqu xmm0, [rsp + #%d]\\n\\t\"\n+                \"vmovdqu [rsp + #%d], xmm0\\n\\t\"\n+                \"vmovdqu xmm0, [rsp - #32]\",\n+                src_offset, dst_offset);\n@@ -1406,5 +2109,6 @@\n-    case Op_IsInfiniteF:\n-    case Op_IsInfiniteD:\n-      if (!VM_Version::supports_avx512dq()) {\n-        return false;\n-      }\n+    case Op_VecZ:\n+      st->print(\"vmovdqu [rsp - #64], xmm0\\t# 512-bit mem-mem spill\\n\\t\"\n+                \"vmovdqu xmm0, [rsp + #%d]\\n\\t\"\n+                \"vmovdqu [rsp + #%d], xmm0\\n\\t\"\n+                \"vmovdqu xmm0, [rsp - #64]\",\n+                src_offset, dst_offset);\n@@ -1412,15 +2116,93 @@\n-    case Op_SqrtVD:\n-    case Op_SqrtVF:\n-    case Op_VectorMaskCmp:\n-    case Op_VectorCastB2X:\n-    case Op_VectorCastS2X:\n-    case Op_VectorCastI2X:\n-    case Op_VectorCastL2X:\n-    case Op_VectorCastF2X:\n-    case Op_VectorCastD2X:\n-    case Op_VectorUCastB2X:\n-    case Op_VectorUCastS2X:\n-    case Op_VectorUCastI2X:\n-    case Op_VectorMaskCast:\n-      if (UseAVX < 1) { \/\/ enabled for AVX only\n-        return false;\n+    default:\n+      ShouldNotReachHere();\n+    }\n+#endif\n+  }\n+}\n+\n+uint MachSpillCopyNode::implementation(C2_MacroAssembler* masm,\n+                                       PhaseRegAlloc* ra_,\n+                                       bool do_size,\n+                                       outputStream* st) const {\n+  assert(masm != nullptr || st  != nullptr, \"sanity\");\n+  \/\/ Get registers to move\n+  OptoReg::Name src_second = ra_->get_reg_second(in(1));\n+  OptoReg::Name src_first = ra_->get_reg_first(in(1));\n+  OptoReg::Name dst_second = ra_->get_reg_second(this);\n+  OptoReg::Name dst_first = ra_->get_reg_first(this);\n+\n+  enum RC src_second_rc = rc_class(src_second);\n+  enum RC src_first_rc = rc_class(src_first);\n+  enum RC dst_second_rc = rc_class(dst_second);\n+  enum RC dst_first_rc = rc_class(dst_first);\n+\n+  assert(OptoReg::is_valid(src_first) && OptoReg::is_valid(dst_first),\n+         \"must move at least 1 register\" );\n+\n+  if (src_first == dst_first && src_second == dst_second) {\n+    \/\/ Self copy, no move\n+    return 0;\n+  }\n+  if (bottom_type()->isa_vect() != nullptr && bottom_type()->isa_vectmask() == nullptr) {\n+    uint ireg = ideal_reg();\n+    assert((src_first_rc != rc_int && dst_first_rc != rc_int), \"sanity\");\n+    assert((ireg == Op_VecS || ireg == Op_VecD || ireg == Op_VecX || ireg == Op_VecY || ireg == Op_VecZ ), \"sanity\");\n+    if( src_first_rc == rc_stack && dst_first_rc == rc_stack ) {\n+      \/\/ mem -> mem\n+      int src_offset = ra_->reg2offset(src_first);\n+      int dst_offset = ra_->reg2offset(dst_first);\n+      vec_stack_to_stack_helper(masm, src_offset, dst_offset, ireg, st);\n+    } else if (src_first_rc == rc_float && dst_first_rc == rc_float ) {\n+      vec_mov_helper(masm, src_first, dst_first, src_second, dst_second, ireg, st);\n+    } else if (src_first_rc == rc_float && dst_first_rc == rc_stack ) {\n+      int stack_offset = ra_->reg2offset(dst_first);\n+      vec_spill_helper(masm, false, stack_offset, src_first, ireg, st);\n+    } else if (src_first_rc == rc_stack && dst_first_rc == rc_float ) {\n+      int stack_offset = ra_->reg2offset(src_first);\n+      vec_spill_helper(masm, true,  stack_offset, dst_first, ireg, st);\n+    } else {\n+      ShouldNotReachHere();\n+    }\n+    return 0;\n+  }\n+  if (src_first_rc == rc_stack) {\n+    \/\/ mem ->\n+    if (dst_first_rc == rc_stack) {\n+      \/\/ mem -> mem\n+      assert(src_second != dst_first, \"overlap\");\n+      if ((src_first & 1) == 0 && src_first + 1 == src_second &&\n+          (dst_first & 1) == 0 && dst_first + 1 == dst_second) {\n+        \/\/ 64-bit\n+        int src_offset = ra_->reg2offset(src_first);\n+        int dst_offset = ra_->reg2offset(dst_first);\n+        if (masm) {\n+          __ pushq(Address(rsp, src_offset));\n+          __ popq (Address(rsp, dst_offset));\n+#ifndef PRODUCT\n+        } else {\n+          st->print(\"pushq   [rsp + #%d]\\t# 64-bit mem-mem spill\\n\\t\"\n+                    \"popq    [rsp + #%d]\",\n+                     src_offset, dst_offset);\n+#endif\n+        }\n+      } else {\n+        \/\/ 32-bit\n+        assert(!((src_first & 1) == 0 && src_first + 1 == src_second), \"no transform\");\n+        assert(!((dst_first & 1) == 0 && dst_first + 1 == dst_second), \"no transform\");\n+        \/\/ No pushl\/popl, so:\n+        int src_offset = ra_->reg2offset(src_first);\n+        int dst_offset = ra_->reg2offset(dst_first);\n+        if (masm) {\n+          __ movq(Address(rsp, -8), rax);\n+          __ movl(rax, Address(rsp, src_offset));\n+          __ movl(Address(rsp, dst_offset), rax);\n+          __ movq(rax, Address(rsp, -8));\n+#ifndef PRODUCT\n+        } else {\n+          st->print(\"movq    [rsp - #8], rax\\t# 32-bit mem-mem spill\\n\\t\"\n+                    \"movl    rax, [rsp + #%d]\\n\\t\"\n+                    \"movl    [rsp + #%d], rax\\n\\t\"\n+                    \"movq    rax, [rsp - #8]\",\n+                     src_offset, dst_offset);\n+#endif\n+        }\n@@ -1428,4 +2210,30 @@\n-      break;\n-    case Op_PopulateIndex:\n-      if (UseAVX < 2) {\n-        return false;\n+      return 0;\n+    } else if (dst_first_rc == rc_int) {\n+      \/\/ mem -> gpr\n+      if ((src_first & 1) == 0 && src_first + 1 == src_second &&\n+          (dst_first & 1) == 0 && dst_first + 1 == dst_second) {\n+        \/\/ 64-bit\n+        int offset = ra_->reg2offset(src_first);\n+        if (masm) {\n+          __ movq(as_Register(Matcher::_regEncode[dst_first]), Address(rsp, offset));\n+#ifndef PRODUCT\n+        } else {\n+          st->print(\"movq    %s, [rsp + #%d]\\t# spill\",\n+                     Matcher::regName[dst_first],\n+                     offset);\n+#endif\n+        }\n+      } else {\n+        \/\/ 32-bit\n+        assert(!((src_first & 1) == 0 && src_first + 1 == src_second), \"no transform\");\n+        assert(!((dst_first & 1) == 0 && dst_first + 1 == dst_second), \"no transform\");\n+        int offset = ra_->reg2offset(src_first);\n+        if (masm) {\n+          __ movl(as_Register(Matcher::_regEncode[dst_first]), Address(rsp, offset));\n+#ifndef PRODUCT\n+        } else {\n+          st->print(\"movl    %s, [rsp + #%d]\\t# spill\",\n+                     Matcher::regName[dst_first],\n+                     offset);\n+#endif\n+        }\n@@ -1433,4 +2241,31 @@\n-      break;\n-    case Op_RoundVF:\n-      if (UseAVX < 2) { \/\/ enabled for AVX2 only\n-        return false;\n+      return 0;\n+    } else if (dst_first_rc == rc_float) {\n+      \/\/ mem-> xmm\n+      if ((src_first & 1) == 0 && src_first + 1 == src_second &&\n+          (dst_first & 1) == 0 && dst_first + 1 == dst_second) {\n+        \/\/ 64-bit\n+        int offset = ra_->reg2offset(src_first);\n+        if (masm) {\n+          __ movdbl( as_XMMRegister(Matcher::_regEncode[dst_first]), Address(rsp, offset));\n+#ifndef PRODUCT\n+        } else {\n+          st->print(\"%s  %s, [rsp + #%d]\\t# spill\",\n+                     UseXmmLoadAndClearUpper ? \"movsd \" : \"movlpd\",\n+                     Matcher::regName[dst_first],\n+                     offset);\n+#endif\n+        }\n+      } else {\n+        \/\/ 32-bit\n+        assert(!((src_first & 1) == 0 && src_first + 1 == src_second), \"no transform\");\n+        assert(!((dst_first & 1) == 0 && dst_first + 1 == dst_second), \"no transform\");\n+        int offset = ra_->reg2offset(src_first);\n+        if (masm) {\n+          __ movflt( as_XMMRegister(Matcher::_regEncode[dst_first]), Address(rsp, offset));\n+#ifndef PRODUCT\n+        } else {\n+          st->print(\"movss   %s, [rsp + #%d]\\t# spill\",\n+                     Matcher::regName[dst_first],\n+                     offset);\n+#endif\n+        }\n@@ -1438,4 +2273,16 @@\n-      break;\n-    case Op_RoundVD:\n-      if (UseAVX < 3) {\n-        return false;  \/\/ enabled for AVX3 only\n+      return 0;\n+    } else if (dst_first_rc == rc_kreg) {\n+      \/\/ mem -> kreg\n+      if ((src_first & 1) == 0 && src_first + 1 == src_second &&\n+          (dst_first & 1) == 0 && dst_first + 1 == dst_second) {\n+        \/\/ 64-bit\n+        int offset = ra_->reg2offset(src_first);\n+        if (masm) {\n+          __ kmov(as_KRegister(Matcher::_regEncode[dst_first]), Address(rsp, offset));\n+#ifndef PRODUCT\n+        } else {\n+          st->print(\"kmovq   %s, [rsp + #%d]\\t# spill\",\n+                     Matcher::regName[dst_first],\n+                     offset);\n+#endif\n+        }\n@@ -1443,71 +2290,33 @@\n-      break;\n-    case Op_CompareAndSwapL:\n-    case Op_CompareAndSwapP:\n-      break;\n-    case Op_StrIndexOf:\n-      if (!UseSSE42Intrinsics) {\n-        return false;\n-      }\n-      break;\n-    case Op_StrIndexOfChar:\n-      if (!UseSSE42Intrinsics) {\n-        return false;\n-      }\n-      break;\n-    case Op_OnSpinWait:\n-      if (VM_Version::supports_on_spin_wait() == false) {\n-        return false;\n-      }\n-      break;\n-    case Op_MulVB:\n-    case Op_LShiftVB:\n-    case Op_RShiftVB:\n-    case Op_URShiftVB:\n-    case Op_VectorInsert:\n-    case Op_VectorLoadMask:\n-    case Op_VectorStoreMask:\n-    case Op_VectorBlend:\n-      if (UseSSE < 4) {\n-        return false;\n-      }\n-      break;\n-    case Op_MaxD:\n-    case Op_MaxF:\n-    case Op_MinD:\n-    case Op_MinF:\n-      if (UseAVX < 1) { \/\/ enabled for AVX only\n-        return false;\n-      }\n-      break;\n-    case Op_CacheWB:\n-    case Op_CacheWBPreSync:\n-    case Op_CacheWBPostSync:\n-      if (!VM_Version::supports_data_cache_line_flush()) {\n-        return false;\n-      }\n-      break;\n-    case Op_ExtractB:\n-    case Op_ExtractL:\n-    case Op_ExtractI:\n-    case Op_RoundDoubleMode:\n-      if (UseSSE < 4) {\n-        return false;\n-      }\n-      break;\n-    case Op_RoundDoubleModeV:\n-      if (VM_Version::supports_avx() == false) {\n-        return false; \/\/ 128bit vroundpd is not available\n-      }\n-      break;\n-    case Op_LoadVectorGather:\n-    case Op_LoadVectorGatherMasked:\n-      if (UseAVX < 2) {\n-        return false;\n-      }\n-      break;\n-    case Op_FmaF:\n-    case Op_FmaD:\n-    case Op_FmaVD:\n-    case Op_FmaVF:\n-      if (!UseFMA) {\n-        return false;\n+      return 0;\n+    }\n+  } else if (src_first_rc == rc_int) {\n+    \/\/ gpr ->\n+    if (dst_first_rc == rc_stack) {\n+      \/\/ gpr -> mem\n+      if ((src_first & 1) == 0 && src_first + 1 == src_second &&\n+          (dst_first & 1) == 0 && dst_first + 1 == dst_second) {\n+        \/\/ 64-bit\n+        int offset = ra_->reg2offset(dst_first);\n+        if (masm) {\n+          __ movq(Address(rsp, offset), as_Register(Matcher::_regEncode[src_first]));\n+#ifndef PRODUCT\n+        } else {\n+          st->print(\"movq    [rsp + #%d], %s\\t# spill\",\n+                     offset,\n+                     Matcher::regName[src_first]);\n+#endif\n+        }\n+      } else {\n+        \/\/ 32-bit\n+        assert(!((src_first & 1) == 0 && src_first + 1 == src_second), \"no transform\");\n+        assert(!((dst_first & 1) == 0 && dst_first + 1 == dst_second), \"no transform\");\n+        int offset = ra_->reg2offset(dst_first);\n+        if (masm) {\n+          __ movl(Address(rsp, offset), as_Register(Matcher::_regEncode[src_first]));\n+#ifndef PRODUCT\n+        } else {\n+          st->print(\"movl    [rsp + #%d], %s\\t# spill\",\n+                     offset,\n+                     Matcher::regName[src_first]);\n+#endif\n+        }\n@@ -1515,4 +2324,32 @@\n-      break;\n-    case Op_MacroLogicV:\n-      if (UseAVX < 3 || !UseVectorMacroLogic) {\n-        return false;\n+      return 0;\n+    } else if (dst_first_rc == rc_int) {\n+      \/\/ gpr -> gpr\n+      if ((src_first & 1) == 0 && src_first + 1 == src_second &&\n+          (dst_first & 1) == 0 && dst_first + 1 == dst_second) {\n+        \/\/ 64-bit\n+        if (masm) {\n+          __ movq(as_Register(Matcher::_regEncode[dst_first]),\n+                  as_Register(Matcher::_regEncode[src_first]));\n+#ifndef PRODUCT\n+        } else {\n+          st->print(\"movq    %s, %s\\t# spill\",\n+                     Matcher::regName[dst_first],\n+                     Matcher::regName[src_first]);\n+#endif\n+        }\n+        return 0;\n+      } else {\n+        \/\/ 32-bit\n+        assert(!((src_first & 1) == 0 && src_first + 1 == src_second), \"no transform\");\n+        assert(!((dst_first & 1) == 0 && dst_first + 1 == dst_second), \"no transform\");\n+        if (masm) {\n+          __ movl(as_Register(Matcher::_regEncode[dst_first]),\n+                  as_Register(Matcher::_regEncode[src_first]));\n+#ifndef PRODUCT\n+        } else {\n+          st->print(\"movl    %s, %s\\t# spill\",\n+                     Matcher::regName[dst_first],\n+                     Matcher::regName[src_first]);\n+#endif\n+        }\n+        return 0;\n@@ -1520,6 +2357,27 @@\n-      break;\n-\n-    case Op_VectorCmpMasked:\n-    case Op_VectorMaskGen:\n-      if (UseAVX < 3 || !VM_Version::supports_bmi2()) {\n-        return false;\n+    } else if (dst_first_rc == rc_float) {\n+      \/\/ gpr -> xmm\n+      if ((src_first & 1) == 0 && src_first + 1 == src_second &&\n+          (dst_first & 1) == 0 && dst_first + 1 == dst_second) {\n+        \/\/ 64-bit\n+        if (masm) {\n+          __ movdq( as_XMMRegister(Matcher::_regEncode[dst_first]), as_Register(Matcher::_regEncode[src_first]));\n+#ifndef PRODUCT\n+        } else {\n+          st->print(\"movdq   %s, %s\\t# spill\",\n+                     Matcher::regName[dst_first],\n+                     Matcher::regName[src_first]);\n+#endif\n+        }\n+      } else {\n+        \/\/ 32-bit\n+        assert(!((src_first & 1) == 0 && src_first + 1 == src_second), \"no transform\");\n+        assert(!((dst_first & 1) == 0 && dst_first + 1 == dst_second), \"no transform\");\n+        if (masm) {\n+          __ movdl( as_XMMRegister(Matcher::_regEncode[dst_first]), as_Register(Matcher::_regEncode[src_first]));\n+#ifndef PRODUCT\n+        } else {\n+          st->print(\"movdl   %s, %s\\t# spill\",\n+                     Matcher::regName[dst_first],\n+                     Matcher::regName[src_first]);\n+#endif\n+        }\n@@ -1527,7 +2385,14 @@\n-      break;\n-    case Op_VectorMaskFirstTrue:\n-    case Op_VectorMaskLastTrue:\n-    case Op_VectorMaskTrueCount:\n-    case Op_VectorMaskToLong:\n-      if (UseAVX < 1) {\n-         return false;\n+      return 0;\n+    } else if (dst_first_rc == rc_kreg) {\n+      if ((src_first & 1) == 0 && src_first + 1 == src_second &&\n+          (dst_first & 1) == 0 && dst_first + 1 == dst_second) {\n+        \/\/ 64-bit\n+        if (masm) {\n+          __ kmov(as_KRegister(Matcher::_regEncode[dst_first]), as_Register(Matcher::_regEncode[src_first]));\n+  #ifndef PRODUCT\n+        } else {\n+           st->print(\"kmovq   %s, %s\\t# spill\",\n+                       Matcher::regName[dst_first],\n+                       Matcher::regName[src_first]);\n+  #endif\n+        }\n@@ -1535,8 +2400,34 @@\n-      break;\n-    case Op_RoundF:\n-    case Op_RoundD:\n-      break;\n-    case Op_CopySignD:\n-    case Op_CopySignF:\n-      if (UseAVX < 3)  {\n-        return false;\n+      Unimplemented();\n+      return 0;\n+    }\n+  } else if (src_first_rc == rc_float) {\n+    \/\/ xmm ->\n+    if (dst_first_rc == rc_stack) {\n+      \/\/ xmm -> mem\n+      if ((src_first & 1) == 0 && src_first + 1 == src_second &&\n+          (dst_first & 1) == 0 && dst_first + 1 == dst_second) {\n+        \/\/ 64-bit\n+        int offset = ra_->reg2offset(dst_first);\n+        if (masm) {\n+          __ movdbl( Address(rsp, offset), as_XMMRegister(Matcher::_regEncode[src_first]));\n+#ifndef PRODUCT\n+        } else {\n+          st->print(\"movsd   [rsp + #%d], %s\\t# spill\",\n+                     offset,\n+                     Matcher::regName[src_first]);\n+#endif\n+        }\n+      } else {\n+        \/\/ 32-bit\n+        assert(!((src_first & 1) == 0 && src_first + 1 == src_second), \"no transform\");\n+        assert(!((dst_first & 1) == 0 && dst_first + 1 == dst_second), \"no transform\");\n+        int offset = ra_->reg2offset(dst_first);\n+        if (masm) {\n+          __ movflt(Address(rsp, offset), as_XMMRegister(Matcher::_regEncode[src_first]));\n+#ifndef PRODUCT\n+        } else {\n+          st->print(\"movss   [rsp + #%d], %s\\t# spill\",\n+                     offset,\n+                     Matcher::regName[src_first]);\n+#endif\n+        }\n@@ -1544,2 +2435,28 @@\n-      if (!VM_Version::supports_avx512vl()) {\n-        return false;\n+      return 0;\n+    } else if (dst_first_rc == rc_int) {\n+      \/\/ xmm -> gpr\n+      if ((src_first & 1) == 0 && src_first + 1 == src_second &&\n+          (dst_first & 1) == 0 && dst_first + 1 == dst_second) {\n+        \/\/ 64-bit\n+        if (masm) {\n+          __ movdq( as_Register(Matcher::_regEncode[dst_first]), as_XMMRegister(Matcher::_regEncode[src_first]));\n+#ifndef PRODUCT\n+        } else {\n+          st->print(\"movdq   %s, %s\\t# spill\",\n+                     Matcher::regName[dst_first],\n+                     Matcher::regName[src_first]);\n+#endif\n+        }\n+      } else {\n+        \/\/ 32-bit\n+        assert(!((src_first & 1) == 0 && src_first + 1 == src_second), \"no transform\");\n+        assert(!((dst_first & 1) == 0 && dst_first + 1 == dst_second), \"no transform\");\n+        if (masm) {\n+          __ movdl( as_Register(Matcher::_regEncode[dst_first]), as_XMMRegister(Matcher::_regEncode[src_first]));\n+#ifndef PRODUCT\n+        } else {\n+          st->print(\"movdl   %s, %s\\t# spill\",\n+                     Matcher::regName[dst_first],\n+                     Matcher::regName[src_first]);\n+#endif\n+        }\n@@ -1547,5 +2464,30 @@\n-      break;\n-    case Op_CompressBits:\n-    case Op_ExpandBits:\n-      if (!VM_Version::supports_bmi2()) {\n-        return false;\n+      return 0;\n+    } else if (dst_first_rc == rc_float) {\n+      \/\/ xmm -> xmm\n+      if ((src_first & 1) == 0 && src_first + 1 == src_second &&\n+          (dst_first & 1) == 0 && dst_first + 1 == dst_second) {\n+        \/\/ 64-bit\n+        if (masm) {\n+          __ movdbl( as_XMMRegister(Matcher::_regEncode[dst_first]), as_XMMRegister(Matcher::_regEncode[src_first]));\n+#ifndef PRODUCT\n+        } else {\n+          st->print(\"%s  %s, %s\\t# spill\",\n+                     UseXmmRegToRegMoveAll ? \"movapd\" : \"movsd \",\n+                     Matcher::regName[dst_first],\n+                     Matcher::regName[src_first]);\n+#endif\n+        }\n+      } else {\n+        \/\/ 32-bit\n+        assert(!((src_first & 1) == 0 && src_first + 1 == src_second), \"no transform\");\n+        assert(!((dst_first & 1) == 0 && dst_first + 1 == dst_second), \"no transform\");\n+        if (masm) {\n+          __ movflt( as_XMMRegister(Matcher::_regEncode[dst_first]), as_XMMRegister(Matcher::_regEncode[src_first]));\n+#ifndef PRODUCT\n+        } else {\n+          st->print(\"%s  %s, %s\\t# spill\",\n+                     UseXmmRegToRegMoveAll ? \"movaps\" : \"movss \",\n+                     Matcher::regName[dst_first],\n+                     Matcher::regName[src_first]);\n+#endif\n+        }\n@@ -1553,4 +2495,21 @@\n-      break;\n-    case Op_CompressM:\n-      if (!VM_Version::supports_avx512vl() || !VM_Version::supports_bmi2()) {\n-        return false;\n+      return 0;\n+    } else if (dst_first_rc == rc_kreg) {\n+      assert(false, \"Illegal spilling\");\n+      return 0;\n+    }\n+  } else if (src_first_rc == rc_kreg) {\n+    if (dst_first_rc == rc_stack) {\n+      \/\/ mem -> kreg\n+      if ((src_first & 1) == 0 && src_first + 1 == src_second &&\n+          (dst_first & 1) == 0 && dst_first + 1 == dst_second) {\n+        \/\/ 64-bit\n+        int offset = ra_->reg2offset(dst_first);\n+        if (masm) {\n+          __ kmov(Address(rsp, offset), as_KRegister(Matcher::_regEncode[src_first]));\n+#ifndef PRODUCT\n+        } else {\n+          st->print(\"kmovq   [rsp + #%d] , %s\\t# spill\",\n+                     offset,\n+                     Matcher::regName[src_first]);\n+#endif\n+        }\n@@ -1558,5 +2517,14 @@\n-      break;\n-    case Op_ConvF2HF:\n-    case Op_ConvHF2F:\n-      if (!VM_Version::supports_float16()) {\n-        return false;\n+      return 0;\n+    } else if (dst_first_rc == rc_int) {\n+      if ((src_first & 1) == 0 && src_first + 1 == src_second &&\n+          (dst_first & 1) == 0 && dst_first + 1 == dst_second) {\n+        \/\/ 64-bit\n+        if (masm) {\n+          __ kmov(as_Register(Matcher::_regEncode[dst_first]), as_KRegister(Matcher::_regEncode[src_first]));\n+#ifndef PRODUCT\n+        } else {\n+         st->print(\"kmovq   %s, %s\\t# spill\",\n+                     Matcher::regName[dst_first],\n+                     Matcher::regName[src_first]);\n+#endif\n+        }\n@@ -1564,5 +2532,15 @@\n-      break;\n-    case Op_VectorCastF2HF:\n-    case Op_VectorCastHF2F:\n-      if (!VM_Version::supports_f16c() && !VM_Version::supports_evex()) {\n-        return false;\n+      Unimplemented();\n+      return 0;\n+    } else if (dst_first_rc == rc_kreg) {\n+      if ((src_first & 1) == 0 && src_first + 1 == src_second &&\n+          (dst_first & 1) == 0 && dst_first + 1 == dst_second) {\n+        \/\/ 64-bit\n+        if (masm) {\n+          __ kmov(as_KRegister(Matcher::_regEncode[dst_first]), as_KRegister(Matcher::_regEncode[src_first]));\n+#ifndef PRODUCT\n+        } else {\n+         st->print(\"kmovq   %s, %s\\t# spill\",\n+                     Matcher::regName[dst_first],\n+                     Matcher::regName[src_first]);\n+#endif\n+        }\n@@ -1570,1 +2548,5 @@\n-      break;\n+      return 0;\n+    } else if (dst_first_rc == rc_float) {\n+      assert(false, \"Illegal spill\");\n+      return 0;\n+    }\n@@ -1572,1 +2554,4 @@\n-  return true;  \/\/ Match rules are supported by default.\n+\n+  assert(0,\" foo \");\n+  Unimplemented();\n+  return 0;\n@@ -1575,1 +2560,5 @@\n-\/\/------------------------------------------------------------------------\n+#ifndef PRODUCT\n+void MachSpillCopyNode::format(PhaseRegAlloc *ra_, outputStream* st) const {\n+  implementation(nullptr, ra_, false, st);\n+}\n+#endif\n@@ -1577,3 +2566,2 @@\n-static inline bool is_pop_count_instr_target(BasicType bt) {\n-  return (is_subword_type(bt) && VM_Version::supports_avx512_bitalg()) ||\n-         (is_non_subword_integral_type(bt) && VM_Version::supports_avx512_vpopcntdq());\n+void MachSpillCopyNode::emit(C2_MacroAssembler *masm, PhaseRegAlloc *ra_) const {\n+  implementation(masm, ra_, false, nullptr);\n@@ -1582,2 +2570,2 @@\n-bool Matcher::match_rule_supported_auto_vectorization(int opcode, int vlen, BasicType bt) {\n-  return match_rule_supported_vector(opcode, vlen, bt);\n+uint MachSpillCopyNode::size(PhaseRegAlloc *ra_) const {\n+  return MachNode::size(ra_);\n@@ -1586,5 +2574,26 @@\n-\/\/ Identify extra cases that we might want to provide match rules for vector nodes and\n-\/\/ other intrinsics guarded with vector length (vlen) and element type (bt).\n-bool Matcher::match_rule_supported_vector(int opcode, int vlen, BasicType bt) {\n-  if (!match_rule_supported(opcode)) {\n-    return false;\n+\/\/=============================================================================\n+#ifndef PRODUCT\n+void BoxLockNode::format(PhaseRegAlloc* ra_, outputStream* st) const\n+{\n+  int offset = ra_->reg2offset(in_RegMask(0).find_first_elem());\n+  int reg = ra_->get_reg_first(this);\n+  st->print(\"leaq    %s, [rsp + #%d]\\t# box lock\",\n+            Matcher::regName[reg], offset);\n+}\n+#endif\n+\n+void BoxLockNode::emit(C2_MacroAssembler* masm, PhaseRegAlloc* ra_) const\n+{\n+  int offset = ra_->reg2offset(in_RegMask(0).find_first_elem());\n+  int reg = ra_->get_encode(this);\n+\n+  __ lea(as_Register(reg), Address(rsp, offset));\n+}\n+\n+uint BoxLockNode::size(PhaseRegAlloc *ra_) const\n+{\n+  int offset = ra_->reg2offset(in_RegMask(0).find_first_elem());\n+  if (ra_->get_encode(this) > 15) {\n+    return (offset < 0x80) ? 6 : 9; \/\/ REX2\n+  } else {\n+    return (offset < 0x80) ? 5 : 8; \/\/ REX\n@@ -1592,10 +2601,12 @@\n-  \/\/ Matcher::vector_size_supported() restricts vector sizes in the following way (see Matcher::vector_width_in_bytes):\n-  \/\/   * SSE2 supports 128bit vectors for all types;\n-  \/\/   * AVX1 supports 256bit vectors only for FLOAT and DOUBLE types;\n-  \/\/   * AVX2 supports 256bit vectors for all types;\n-  \/\/   * AVX512F supports 512bit vectors only for INT, FLOAT, and DOUBLE types;\n-  \/\/   * AVX512BW supports 512bit vectors for BYTE, SHORT, and CHAR types.\n-  \/\/ There's also a limit on minimum vector size supported: 2 elements (or 4 bytes for BYTE).\n-  \/\/ And MaxVectorSize is taken into account as well.\n-  if (!vector_size_supported(bt, vlen)) {\n-    return false;\n+}\n+\n+\/\/=============================================================================\n+#ifndef PRODUCT\n+void MachUEPNode::format(PhaseRegAlloc* ra_, outputStream* st) const\n+{\n+  if (UseCompressedClassPointers) {\n+    st->print_cr(\"movl    rscratch1, [j_rarg0 + oopDesc::klass_offset_in_bytes()]\\t# compressed klass\");\n+    st->print_cr(\"\\tcmpl    rscratch1, [rax + CompiledICData::speculated_klass_offset()]\\t # Inline cache check\");\n+  } else {\n+    st->print_cr(\"movq    rscratch1, [j_rarg0 + oopDesc::klass_offset_in_bytes()]\\t# compressed klass\");\n+    st->print_cr(\"\\tcmpq    rscratch1, [rax + CompiledICData::speculated_klass_offset()]\\t # Inline cache check\");\n@@ -1603,18 +2614,325 @@\n-  \/\/ Special cases which require vector length follow:\n-  \/\/   * implementation limitations\n-  \/\/   * some 512bit vector operations on FLOAT and DOUBLE types require AVX512DQ\n-  \/\/   * 128bit vroundpd instruction is present only in AVX1\n-  int size_in_bits = vlen * type2aelembytes(bt) * BitsPerByte;\n-  switch (opcode) {\n-    case Op_MaxVHF:\n-    case Op_MinVHF:\n-      if (!VM_Version::supports_avx512bw()) {\n-        return false;\n-      }\n-    case Op_AddVHF:\n-    case Op_DivVHF:\n-    case Op_FmaVHF:\n-    case Op_MulVHF:\n-    case Op_SubVHF:\n-    case Op_SqrtVHF:\n-      if (size_in_bits < 512 && !VM_Version::supports_avx512vl()) {\n+  st->print_cr(\"\\tjne     SharedRuntime::_ic_miss_stub\");\n+}\n+#endif\n+\n+void MachUEPNode::emit(C2_MacroAssembler* masm, PhaseRegAlloc* ra_) const\n+{\n+  __ ic_check(InteriorEntryAlignment);\n+}\n+\n+uint MachUEPNode::size(PhaseRegAlloc* ra_) const\n+{\n+  return MachNode::size(ra_); \/\/ too many variables; just compute it\n+                              \/\/ the hard way\n+}\n+\n+\n+\/\/=============================================================================\n+\n+bool Matcher::supports_vector_calling_convention(void) {\n+  return EnableVectorSupport;\n+}\n+\n+OptoRegPair Matcher::vector_return_value(uint ideal_reg) {\n+  assert(EnableVectorSupport, \"sanity\");\n+  int lo = XMM0_num;\n+  int hi = XMM0b_num;\n+  if (ideal_reg == Op_VecX) hi = XMM0d_num;\n+  else if (ideal_reg == Op_VecY) hi = XMM0h_num;\n+  else if (ideal_reg == Op_VecZ) hi = XMM0p_num;\n+  return OptoRegPair(hi, lo);\n+}\n+\n+\/\/ Is this branch offset short enough that a short branch can be used?\n+\/\/\n+\/\/ NOTE: If the platform does not provide any short branch variants, then\n+\/\/       this method should return false for offset 0.\n+bool Matcher::is_short_branch_offset(int rule, int br_size, int offset) {\n+  \/\/ The passed offset is relative to address of the branch.\n+  \/\/ On 86 a branch displacement is calculated relative to address\n+  \/\/ of a next instruction.\n+  offset -= br_size;\n+\n+  \/\/ the short version of jmpConUCF2 contains multiple branches,\n+  \/\/ making the reach slightly less\n+  if (rule == jmpConUCF2_rule)\n+    return (-126 <= offset && offset <= 125);\n+  return (-128 <= offset && offset <= 127);\n+}\n+\n+\/\/ Return whether or not this register is ever used as an argument.\n+\/\/ This function is used on startup to build the trampoline stubs in\n+\/\/ generateOptoStub.  Registers not mentioned will be killed by the VM\n+\/\/ call in the trampoline, and arguments in those registers not be\n+\/\/ available to the callee.\n+bool Matcher::can_be_java_arg(int reg)\n+{\n+  return\n+    reg ==  RDI_num || reg == RDI_H_num ||\n+    reg ==  RSI_num || reg == RSI_H_num ||\n+    reg ==  RDX_num || reg == RDX_H_num ||\n+    reg ==  RCX_num || reg == RCX_H_num ||\n+    reg ==   R8_num || reg ==  R8_H_num ||\n+    reg ==   R9_num || reg ==  R9_H_num ||\n+    reg ==  R12_num || reg == R12_H_num ||\n+    reg == XMM0_num || reg == XMM0b_num ||\n+    reg == XMM1_num || reg == XMM1b_num ||\n+    reg == XMM2_num || reg == XMM2b_num ||\n+    reg == XMM3_num || reg == XMM3b_num ||\n+    reg == XMM4_num || reg == XMM4b_num ||\n+    reg == XMM5_num || reg == XMM5b_num ||\n+    reg == XMM6_num || reg == XMM6b_num ||\n+    reg == XMM7_num || reg == XMM7b_num;\n+}\n+\n+bool Matcher::is_spillable_arg(int reg)\n+{\n+  return can_be_java_arg(reg);\n+}\n+\n+uint Matcher::int_pressure_limit()\n+{\n+  return (INTPRESSURE == -1) ? _INT_REG_mask.size() : INTPRESSURE;\n+}\n+\n+uint Matcher::float_pressure_limit()\n+{\n+  \/\/ After experiment around with different values, the following default threshold\n+  \/\/ works best for LCM's register pressure scheduling on x64.\n+  uint dec_count  = VM_Version::supports_evex() ? 4 : 2;\n+  uint default_float_pressure_threshold = _FLOAT_REG_mask.size() - dec_count;\n+  return (FLOATPRESSURE == -1) ? default_float_pressure_threshold : FLOATPRESSURE;\n+}\n+\n+bool Matcher::use_asm_for_ldiv_by_con( jlong divisor ) {\n+  \/\/ In 64 bit mode a code which use multiply when\n+  \/\/ devisor is constant is faster than hardware\n+  \/\/ DIV instruction (it uses MulHiL).\n+  return false;\n+}\n+\n+\/\/ Register for DIVI projection of divmodI\n+const RegMask& Matcher::divI_proj_mask() {\n+  return INT_RAX_REG_mask();\n+}\n+\n+\/\/ Register for MODI projection of divmodI\n+const RegMask& Matcher::modI_proj_mask() {\n+  return INT_RDX_REG_mask();\n+}\n+\n+\/\/ Register for DIVL projection of divmodL\n+const RegMask& Matcher::divL_proj_mask() {\n+  return LONG_RAX_REG_mask();\n+}\n+\n+\/\/ Register for MODL projection of divmodL\n+const RegMask& Matcher::modL_proj_mask() {\n+  return LONG_RDX_REG_mask();\n+}\n+\n+%}\n+\n+source_hpp %{\n+\/\/ Header information of the source block.\n+\/\/ Method declarations\/definitions which are used outside\n+\/\/ the ad-scope can conveniently be defined here.\n+\/\/\n+\/\/ To keep related declarations\/definitions\/uses close together,\n+\/\/ we switch between source %{ }% and source_hpp %{ }% freely as needed.\n+\n+#include \"runtime\/vm_version.hpp\"\n+\n+class NativeJump;\n+\n+class CallStubImpl {\n+\n+  \/\/--------------------------------------------------------------\n+  \/\/---<  Used for optimization in Compile::shorten_branches  >---\n+  \/\/--------------------------------------------------------------\n+\n+ public:\n+  \/\/ Size of call trampoline stub.\n+  static uint size_call_trampoline() {\n+    return 0; \/\/ no call trampolines on this platform\n+  }\n+\n+  \/\/ number of relocations needed by a call trampoline stub\n+  static uint reloc_call_trampoline() {\n+    return 0; \/\/ no call trampolines on this platform\n+  }\n+};\n+\n+class HandlerImpl {\n+\n+ public:\n+\n+  static int emit_deopt_handler(C2_MacroAssembler* masm);\n+\n+  static uint size_deopt_handler() {\n+    \/\/ one call and one jmp.\n+    return 7;\n+  }\n+};\n+\n+inline Assembler::AvxVectorLen vector_length_encoding(int bytes) {\n+  switch(bytes) {\n+    case  4: \/\/ fall-through\n+    case  8: \/\/ fall-through\n+    case 16: return Assembler::AVX_128bit;\n+    case 32: return Assembler::AVX_256bit;\n+    case 64: return Assembler::AVX_512bit;\n+\n+    default: {\n+      ShouldNotReachHere();\n+      return Assembler::AVX_NoVec;\n+    }\n+  }\n+}\n+\n+static inline Assembler::AvxVectorLen vector_length_encoding(const Node* n) {\n+  return vector_length_encoding(Matcher::vector_length_in_bytes(n));\n+}\n+\n+static inline Assembler::AvxVectorLen vector_length_encoding(const MachNode* use, MachOper* opnd) {\n+  uint def_idx = use->operand_index(opnd);\n+  Node* def = use->in(def_idx);\n+  return vector_length_encoding(def);\n+}\n+\n+static inline bool is_vector_popcount_predicate(BasicType bt) {\n+  return (is_subword_type(bt) && VM_Version::supports_avx512_bitalg()) ||\n+         (is_non_subword_integral_type(bt) && VM_Version::supports_avx512_vpopcntdq());\n+}\n+\n+static inline bool is_clz_non_subword_predicate_evex(BasicType bt, int vlen_bytes) {\n+  return is_non_subword_integral_type(bt) && VM_Version::supports_avx512cd() &&\n+           (VM_Version::supports_avx512vl() || vlen_bytes == 64);\n+}\n+\n+class Node::PD {\n+public:\n+  enum NodeFlags {\n+    Flag_intel_jcc_erratum    = Node::_last_flag << 1,\n+    Flag_sets_carry_flag      = Node::_last_flag << 2,\n+    Flag_sets_parity_flag     = Node::_last_flag << 3,\n+    Flag_sets_zero_flag       = Node::_last_flag << 4,\n+    Flag_sets_overflow_flag   = Node::_last_flag << 5,\n+    Flag_sets_sign_flag       = Node::_last_flag << 6,\n+    Flag_clears_carry_flag    = Node::_last_flag << 7,\n+    Flag_clears_parity_flag   = Node::_last_flag << 8,\n+    Flag_clears_zero_flag     = Node::_last_flag << 9,\n+    Flag_clears_overflow_flag = Node::_last_flag << 10,\n+    Flag_clears_sign_flag     = Node::_last_flag << 11,\n+    _last_flag                = Flag_clears_sign_flag\n+  };\n+};\n+\n+%} \/\/ end source_hpp\n+\n+source %{\n+\n+#include \"opto\/addnode.hpp\"\n+#include \"c2_intelJccErratum_x86.hpp\"\n+\n+void PhaseOutput::pd_perform_mach_node_analysis() {\n+  if (VM_Version::has_intel_jcc_erratum()) {\n+    int extra_padding = IntelJccErratum::tag_affected_machnodes(C, C->cfg(), C->regalloc());\n+    _buf_sizes._code += extra_padding;\n+  }\n+}\n+\n+int MachNode::pd_alignment_required() const {\n+  if (VM_Version::has_intel_jcc_erratum() && IntelJccErratum::is_jcc_erratum_branch(this)) {\n+    \/\/ Conservatively add worst case padding. We assume that relocInfo::addr_unit() is 1 on x86.\n+    return IntelJccErratum::largest_jcc_size() + 1;\n+  } else {\n+    return 1;\n+  }\n+}\n+\n+int MachNode::compute_padding(int current_offset) const {\n+  if (flags() & Node::PD::Flag_intel_jcc_erratum) {\n+    Compile* C = Compile::current();\n+    PhaseOutput* output = C->output();\n+    Block* block = output->block();\n+    int index = output->index();\n+    return IntelJccErratum::compute_padding(current_offset, this, block, index, C->regalloc());\n+  } else {\n+    return 0;\n+  }\n+}\n+\n+\/\/ Emit deopt handler code.\n+int HandlerImpl::emit_deopt_handler(C2_MacroAssembler* masm) {\n+\n+  \/\/ Note that the code buffer's insts_mark is always relative to insts.\n+  \/\/ That's why we must use the macroassembler to generate a handler.\n+  address base = __ start_a_stub(size_deopt_handler());\n+  if (base == nullptr) {\n+    ciEnv::current()->record_failure(\"CodeCache is full\");\n+    return 0;  \/\/ CodeBuffer::expand failed\n+  }\n+  int offset = __ offset();\n+\n+  Label start;\n+  __ bind(start);\n+\n+  __ call(RuntimeAddress(SharedRuntime::deopt_blob()->unpack()));\n+\n+  int entry_offset = __ offset();\n+\n+  __ jmp(start);\n+\n+  assert(__ offset() - offset <= (int) size_deopt_handler(), \"overflow %d\", (__ offset() - offset));\n+  assert(__ offset() - entry_offset >= NativePostCallNop::first_check_size,\n+         \"out of bounds read in post-call NOP check\");\n+  __ end_a_stub();\n+  return entry_offset;\n+}\n+\n+static Assembler::Width widthForType(BasicType bt) {\n+  if (bt == T_BYTE) {\n+    return Assembler::B;\n+  } else if (bt == T_SHORT) {\n+    return Assembler::W;\n+  } else if (bt == T_INT) {\n+    return Assembler::D;\n+  } else {\n+    assert(bt == T_LONG, \"not a long: %s\", type2name(bt));\n+    return Assembler::Q;\n+  }\n+}\n+\n+\/\/=============================================================================\n+\n+  \/\/ Float masks come from different places depending on platform.\n+  static address float_signmask()  { return StubRoutines::x86::float_sign_mask(); }\n+  static address float_signflip()  { return StubRoutines::x86::float_sign_flip(); }\n+  static address double_signmask() { return StubRoutines::x86::double_sign_mask(); }\n+  static address double_signflip() { return StubRoutines::x86::double_sign_flip(); }\n+  static address vector_short_to_byte_mask() { return StubRoutines::x86::vector_short_to_byte_mask(); }\n+  static address vector_int_to_byte_mask() { return StubRoutines::x86::vector_int_to_byte_mask(); }\n+  static address vector_byte_perm_mask() { return StubRoutines::x86::vector_byte_perm_mask(); }\n+  static address vector_long_sign_mask() { return StubRoutines::x86::vector_long_sign_mask(); }\n+  static address vector_all_bits_set() { return StubRoutines::x86::vector_all_bits_set(); }\n+  static address vector_int_mask_cmp_bits() { return StubRoutines::x86::vector_int_mask_cmp_bits(); }\n+  static address vector_int_to_short_mask() { return StubRoutines::x86::vector_int_to_short_mask(); }\n+  static address vector_byte_shufflemask() { return StubRoutines::x86::vector_byte_shuffle_mask(); }\n+  static address vector_short_shufflemask() { return StubRoutines::x86::vector_short_shuffle_mask(); }\n+  static address vector_int_shufflemask() { return StubRoutines::x86::vector_int_shuffle_mask(); }\n+  static address vector_long_shufflemask() { return StubRoutines::x86::vector_long_shuffle_mask(); }\n+  static address vector_32_bit_mask() { return StubRoutines::x86::vector_32_bit_mask(); }\n+  static address vector_64_bit_mask() { return StubRoutines::x86::vector_64_bit_mask(); }\n+  static address vector_float_signflip() { return StubRoutines::x86::vector_float_sign_flip();}\n+  static address vector_double_signflip() { return StubRoutines::x86::vector_double_sign_flip();}\n+\n+\/\/=============================================================================\n+bool Matcher::match_rule_supported(int opcode) {\n+  if (!has_match_rule(opcode)) {\n+    return false; \/\/ no match rule present\n+  }\n+  switch (opcode) {\n+    case Op_AbsVL:\n+    case Op_StoreVectorScatter:\n+      if (UseAVX < 3) {\n@@ -1623,1 +2941,4 @@\n-      if (!VM_Version::supports_avx512_fp16()) {\n+      break;\n+    case Op_PopCountI:\n+    case Op_PopCountL:\n+      if (!UsePopCountInstruction) {\n@@ -1627,4 +2948,3 @@\n-    case Op_AbsVF:\n-    case Op_NegVF:\n-      if ((vlen == 16) && (VM_Version::supports_avx512dq() == false)) {\n-        return false; \/\/ 512bit vandps and vxorps are not available\n+    case Op_PopCountVI:\n+      if (UseAVX < 2) {\n+        return false;\n@@ -1633,4 +2953,5 @@\n-    case Op_AbsVD:\n-    case Op_NegVD:\n-      if ((vlen == 8) && (VM_Version::supports_avx512dq() == false)) {\n-        return false; \/\/ 512bit vpmullq, vandpd and vxorpd are not available\n+    case Op_CompressV:\n+    case Op_ExpandV:\n+    case Op_PopCountVL:\n+      if (UseAVX < 2) {\n+        return false;\n@@ -1639,8 +2960,2 @@\n-    case Op_RotateRightV:\n-    case Op_RotateLeftV:\n-      if (bt != T_INT && bt != T_LONG) {\n-        return false;\n-      } \/\/ fallthrough\n-    case Op_MacroLogicV:\n-      if (!VM_Version::supports_evex() ||\n-          ((size_in_bits != 512) && !VM_Version::supports_avx512vl())) {\n+    case Op_MulVI:\n+      if ((UseSSE < 4) && (UseAVX < 1)) { \/\/ only with SSE4_1 or AVX\n@@ -1650,4 +2965,2 @@\n-    case Op_ClearArray:\n-    case Op_VectorMaskGen:\n-    case Op_VectorCmpMasked:\n-      if (!VM_Version::supports_avx512bw()) {\n+    case Op_MulVL:\n+      if (UseSSE < 4) { \/\/ only with SSE4_1 or AVX\n@@ -1656,1 +2969,3 @@\n-      if ((size_in_bits != 512) && !VM_Version::supports_avx512vl()) {\n+      break;\n+    case Op_MulReductionVL:\n+      if (VM_Version::supports_avx512dq() == false) {\n@@ -1660,3 +2975,8 @@\n-    case Op_LoadVectorMasked:\n-    case Op_StoreVectorMasked:\n-      if (!VM_Version::supports_avx512bw() && (is_subword_type(bt) || UseAVX < 1)) {\n+    case Op_AbsVB:\n+    case Op_AbsVS:\n+    case Op_AbsVI:\n+    case Op_AddReductionVI:\n+    case Op_AndReductionV:\n+    case Op_OrReductionV:\n+    case Op_XorReductionV:\n+      if (UseSSE < 3) { \/\/ requires at least SSSE3\n@@ -1666,3 +2986,14 @@\n-    case Op_UMinV:\n-    case Op_UMaxV:\n-      if (UseAVX == 0) {\n+    case Op_MaxHF:\n+    case Op_MinHF:\n+      if (!VM_Version::supports_avx512vlbw()) {\n+        return false;\n+      }  \/\/ fallthrough\n+    case Op_AddHF:\n+    case Op_DivHF:\n+    case Op_FmaHF:\n+    case Op_MulHF:\n+    case Op_ReinterpretS2HF:\n+    case Op_ReinterpretHF2S:\n+    case Op_SubHF:\n+    case Op_SqrtHF:\n+      if (!VM_Version::supports_avx512_fp16()) {\n@@ -1672,3 +3003,4 @@\n-    case Op_MaxV:\n-    case Op_MinV:\n-      if (UseSSE < 4 && is_integral_type(bt)) {\n+    case Op_VectorLoadShuffle:\n+    case Op_VectorRearrange:\n+    case Op_MulReductionVI:\n+      if (UseSSE < 4) { \/\/ requires at least SSE4\n@@ -1677,11 +3009,3 @@\n-      if ((bt == T_FLOAT || bt == T_DOUBLE)) {\n-          \/\/ Float\/Double intrinsics are enabled for AVX family currently.\n-          if (UseAVX == 0) {\n-            return false;\n-          }\n-          if (UseAVX > 2 && (!VM_Version::supports_avx512dq() && size_in_bits == 512)) { \/\/ 512 bit Float\/Double intrinsics need AVX512DQ\n-            return false;\n-          }\n-      }\n-    case Op_CallLeafVector:\n-      if (size_in_bits == 512 && !VM_Version::supports_avx512vlbwdq()) {\n+    case Op_IsInfiniteF:\n+    case Op_IsInfiniteD:\n+      if (!VM_Version::supports_avx512dq()) {\n@@ -1692,2 +3016,14 @@\n-    case Op_AddReductionVI:\n-      if (bt == T_INT && (UseSSE < 3 || !VM_Version::supports_ssse3())) {\n+    case Op_SqrtVD:\n+    case Op_SqrtVF:\n+    case Op_VectorMaskCmp:\n+    case Op_VectorCastB2X:\n+    case Op_VectorCastS2X:\n+    case Op_VectorCastI2X:\n+    case Op_VectorCastL2X:\n+    case Op_VectorCastF2X:\n+    case Op_VectorCastD2X:\n+    case Op_VectorUCastB2X:\n+    case Op_VectorUCastS2X:\n+    case Op_VectorUCastI2X:\n+    case Op_VectorMaskCast:\n+      if (UseAVX < 1) { \/\/ enabled for AVX only\n@@ -1696,5 +3032,3 @@\n-      \/\/ fallthrough\n-    case Op_AndReductionV:\n-    case Op_OrReductionV:\n-    case Op_XorReductionV:\n-      if (is_subword_type(bt) && (UseSSE < 4)) {\n+      break;\n+    case Op_PopulateIndex:\n+      if (UseAVX < 2) {\n@@ -1704,5 +3038,2 @@\n-    case Op_MinReductionV:\n-    case Op_MaxReductionV:\n-      if ((bt == T_INT || is_subword_type(bt)) && UseSSE < 4) {\n-        return false;\n-      } else if (bt == T_LONG && (UseAVX < 3 || !VM_Version::supports_avx512vlbwdq())) {\n+    case Op_RoundVF:\n+      if (UseAVX < 2) { \/\/ enabled for AVX2 only\n@@ -1711,3 +3042,4 @@\n-      \/\/ Float\/Double intrinsics enabled for AVX family.\n-      if (UseAVX == 0 && (bt == T_FLOAT || bt == T_DOUBLE)) {\n-        return false;\n+      break;\n+    case Op_RoundVD:\n+      if (UseAVX < 3) {\n+        return false;  \/\/ enabled for AVX3 only\n@@ -1715,1 +3047,6 @@\n-      if (UseAVX > 2 && (!VM_Version::supports_avx512dq() && size_in_bits == 512)) {\n+      break;\n+    case Op_CompareAndSwapL:\n+    case Op_CompareAndSwapP:\n+      break;\n+    case Op_StrIndexOf:\n+      if (!UseSSE42Intrinsics) {\n@@ -1719,5 +3056,3 @@\n-    case Op_VectorTest:\n-      if (UseSSE < 4) {\n-        return false; \/\/ Implementation limitation\n-      } else if (size_in_bits < 32) {\n-        return false; \/\/ Implementation limitation\n+    case Op_StrIndexOfChar:\n+      if (!UseSSE42Intrinsics) {\n+        return false;\n@@ -1726,6 +3061,3 @@\n-    case Op_VectorLoadShuffle:\n-    case Op_VectorRearrange:\n-      if(vlen == 2) {\n-        return false; \/\/ Implementation limitation due to how shuffle is loaded\n-      } else if (size_in_bits == 256 && UseAVX < 2) {\n-        return false; \/\/ Implementation limitation\n+    case Op_OnSpinWait:\n+      if (VM_Version::supports_on_spin_wait() == false) {\n+        return false;\n@@ -1734,0 +3066,5 @@\n+    case Op_MulVB:\n+    case Op_LShiftVB:\n+    case Op_RShiftVB:\n+    case Op_URShiftVB:\n+    case Op_VectorInsert:\n@@ -1735,7 +3072,3 @@\n-    case Op_VectorMaskCast:\n-      if (size_in_bits == 256 && UseAVX < 2) {\n-        return false; \/\/ Implementation limitation\n-      }\n-      \/\/ fallthrough\n-      if (vlen == 2) {\n-        return false; \/\/ Implementation limitation\n+    case Op_VectorBlend:\n+      if (UseSSE < 4) {\n+        return false;\n@@ -1745,2 +3078,5 @@\n-    case Op_PopulateIndex:\n-      if (size_in_bits > 256 && !VM_Version::supports_avx512bw()) {\n+    case Op_MaxD:\n+    case Op_MaxF:\n+    case Op_MinD:\n+    case Op_MinF:\n+      if (UseAVX < 1) { \/\/ enabled for AVX only\n@@ -1750,4 +3086,4 @@\n-    case Op_VectorCastB2X:\n-    case Op_VectorCastS2X:\n-    case Op_VectorCastI2X:\n-      if (bt != T_DOUBLE && size_in_bits == 256 && UseAVX < 2) {\n+    case Op_CacheWB:\n+    case Op_CacheWBPreSync:\n+    case Op_CacheWBPostSync:\n+      if (!VM_Version::supports_data_cache_line_flush()) {\n@@ -1757,4 +3093,5 @@\n-    case Op_VectorCastL2X:\n-      if (is_integral_type(bt) && size_in_bits == 256 && UseAVX < 2) {\n-        return false;\n-      } else if (!is_integral_type(bt) && !VM_Version::supports_avx512dq()) {\n+    case Op_ExtractB:\n+    case Op_ExtractL:\n+    case Op_ExtractI:\n+    case Op_RoundDoubleMode:\n+      if (UseSSE < 4) {\n@@ -1764,8 +3101,3 @@\n-    case Op_VectorCastF2X: {\n-        \/\/ As per JLS section 5.1.3 narrowing conversion to sub-word types\n-        \/\/ happen after intermediate conversion to integer and special handling\n-        \/\/ code needs AVX2 vpcmpeqd instruction for 256 bit vectors.\n-        int src_size_in_bits = type2aelembytes(T_FLOAT) * vlen * BitsPerByte;\n-        if (is_integral_type(bt) && src_size_in_bits == 256 && UseAVX < 2) {\n-          return false;\n-        }\n+    case Op_RoundDoubleModeV:\n+      if (VM_Version::supports_avx() == false) {\n+        return false; \/\/ 128bit vroundpd is not available\n@@ -1773,3 +3105,4 @@\n-      \/\/ fallthrough\n-    case Op_VectorCastD2X:\n-      if (bt == T_LONG && !VM_Version::supports_avx512dq()) {\n+      break;\n+    case Op_LoadVectorGather:\n+    case Op_LoadVectorGatherMasked:\n+      if (UseAVX < 2) {\n@@ -1779,5 +3112,5 @@\n-    case Op_VectorCastF2HF:\n-    case Op_VectorCastHF2F:\n-      if (!VM_Version::supports_f16c() &&\n-         ((!VM_Version::supports_evex() ||\n-         ((size_in_bits != 512) && !VM_Version::supports_avx512vl())))) {\n+    case Op_FmaF:\n+    case Op_FmaD:\n+    case Op_FmaVD:\n+    case Op_FmaVF:\n+      if (!UseFMA) {\n@@ -1787,2 +3120,2 @@\n-    case Op_RoundVD:\n-      if (!VM_Version::supports_avx512dq()) {\n+    case Op_MacroLogicV:\n+      if (UseAVX < 3 || !UseVectorMacroLogic) {\n@@ -1792,2 +3125,4 @@\n-    case Op_MulReductionVI:\n-      if (bt == T_BYTE && size_in_bits == 512 && !VM_Version::supports_avx512bw()) {\n+\n+    case Op_VectorCmpMasked:\n+    case Op_VectorMaskGen:\n+      if (UseAVX < 3 || !VM_Version::supports_bmi2()) {\n@@ -1797,2 +3132,14 @@\n-    case Op_LoadVectorGatherMasked:\n-      if (!is_subword_type(bt) && size_in_bits < 512 && !VM_Version::supports_avx512vl()) {\n+    case Op_VectorMaskFirstTrue:\n+    case Op_VectorMaskLastTrue:\n+    case Op_VectorMaskTrueCount:\n+    case Op_VectorMaskToLong:\n+      if (UseAVX < 1) {\n+         return false;\n+      }\n+      break;\n+    case Op_RoundF:\n+    case Op_RoundD:\n+      break;\n+    case Op_CopySignD:\n+    case Op_CopySignF:\n+      if (UseAVX < 3)  {\n@@ -1801,4 +3148,1 @@\n-      if (is_subword_type(bt) &&\n-         ((size_in_bits > 256 && !VM_Version::supports_avx512bw()) ||\n-          (size_in_bits < 64)                                      ||\n-          (bt == T_SHORT && !VM_Version::supports_bmi2()))) {\n+      if (!VM_Version::supports_avx512vl()) {\n@@ -1808,5 +3152,3 @@\n-    case Op_StoreVectorScatterMasked:\n-    case Op_StoreVectorScatter:\n-      if (is_subword_type(bt)) {\n-        return false;\n-      } else if (size_in_bits < 512 && !VM_Version::supports_avx512vl()) {\n+    case Op_CompressBits:\n+    case Op_ExpandBits:\n+      if (!VM_Version::supports_bmi2()) {\n@@ -1815,3 +3157,3 @@\n-      \/\/ fallthrough\n-    case Op_LoadVectorGather:\n-      if (!is_subword_type(bt) && size_in_bits == 64) {\n+      break;\n+    case Op_CompressM:\n+      if (!VM_Version::supports_avx512vl() || !VM_Version::supports_bmi2()) {\n@@ -1820,1 +3162,4 @@\n-      if (is_subword_type(bt) && size_in_bits < 64) {\n+      break;\n+    case Op_ConvF2HF:\n+    case Op_ConvHF2F:\n+      if (!VM_Version::supports_float16()) {\n@@ -1824,6 +3169,3 @@\n-    case Op_SaturatingAddV:\n-    case Op_SaturatingSubV:\n-      if (UseAVX < 1) {\n-        return false; \/\/ Implementation limitation\n-      }\n-      if (is_subword_type(bt) && size_in_bits == 512 && !VM_Version::supports_avx512bw()) {\n+    case Op_VectorCastF2HF:\n+    case Op_VectorCastHF2F:\n+      if (!VM_Version::supports_f16c() && !VM_Version::supports_evex()) {\n@@ -1833,16 +3175,41 @@\n-    case Op_SelectFromTwoVector:\n-       if (size_in_bits < 128 || (size_in_bits < 512 && !VM_Version::supports_avx512vl())) {\n-         return false;\n-       }\n-       if (bt == T_SHORT && !VM_Version::supports_avx512bw()) {\n-         return false;\n-       }\n-       if (bt == T_BYTE && !VM_Version::supports_avx512_vbmi()) {\n-         return false;\n-       }\n-       if ((bt == T_INT || bt == T_FLOAT || bt == T_DOUBLE) && !VM_Version::supports_evex()) {\n-         return false;\n-       }\n-       break;\n-    case Op_MaskAll:\n-      if (!VM_Version::supports_evex()) {\n+  }\n+  return true;  \/\/ Match rules are supported by default.\n+}\n+\n+\/\/------------------------------------------------------------------------\n+\n+static inline bool is_pop_count_instr_target(BasicType bt) {\n+  return (is_subword_type(bt) && VM_Version::supports_avx512_bitalg()) ||\n+         (is_non_subword_integral_type(bt) && VM_Version::supports_avx512_vpopcntdq());\n+}\n+\n+bool Matcher::match_rule_supported_auto_vectorization(int opcode, int vlen, BasicType bt) {\n+  return match_rule_supported_vector(opcode, vlen, bt);\n+}\n+\n+\/\/ Identify extra cases that we might want to provide match rules for vector nodes and\n+\/\/ other intrinsics guarded with vector length (vlen) and element type (bt).\n+bool Matcher::match_rule_supported_vector(int opcode, int vlen, BasicType bt) {\n+  if (!match_rule_supported(opcode)) {\n+    return false;\n+  }\n+  \/\/ Matcher::vector_size_supported() restricts vector sizes in the following way (see Matcher::vector_width_in_bytes):\n+  \/\/   * SSE2 supports 128bit vectors for all types;\n+  \/\/   * AVX1 supports 256bit vectors only for FLOAT and DOUBLE types;\n+  \/\/   * AVX2 supports 256bit vectors for all types;\n+  \/\/   * AVX512F supports 512bit vectors only for INT, FLOAT, and DOUBLE types;\n+  \/\/   * AVX512BW supports 512bit vectors for BYTE, SHORT, and CHAR types.\n+  \/\/ There's also a limit on minimum vector size supported: 2 elements (or 4 bytes for BYTE).\n+  \/\/ And MaxVectorSize is taken into account as well.\n+  if (!vector_size_supported(bt, vlen)) {\n+    return false;\n+  }\n+  \/\/ Special cases which require vector length follow:\n+  \/\/   * implementation limitations\n+  \/\/   * some 512bit vector operations on FLOAT and DOUBLE types require AVX512DQ\n+  \/\/   * 128bit vroundpd instruction is present only in AVX1\n+  int size_in_bits = vlen * type2aelembytes(bt) * BitsPerByte;\n+  switch (opcode) {\n+    case Op_MaxVHF:\n+    case Op_MinVHF:\n+      if (!VM_Version::supports_avx512bw()) {\n@@ -1851,1 +3218,7 @@\n-      if ((vlen > 16 || is_subword_type(bt)) && !VM_Version::supports_avx512bw()) {\n+    case Op_AddVHF:\n+    case Op_DivVHF:\n+    case Op_FmaVHF:\n+    case Op_MulVHF:\n+    case Op_SubVHF:\n+    case Op_SqrtVHF:\n+      if (size_in_bits < 512 && !VM_Version::supports_avx512vl()) {\n@@ -1854,1 +3227,1 @@\n-      if (size_in_bits < 512 && !VM_Version::supports_avx512vl()) {\n+      if (!VM_Version::supports_avx512_fp16()) {\n@@ -1858,2 +3231,20 @@\n-    case Op_VectorMaskCmp:\n-      if (vlen < 2 || size_in_bits < 32) {\n+    case Op_AbsVF:\n+    case Op_NegVF:\n+      if ((vlen == 16) && (VM_Version::supports_avx512dq() == false)) {\n+        return false; \/\/ 512bit vandps and vxorps are not available\n+      }\n+      break;\n+    case Op_AbsVD:\n+    case Op_NegVD:\n+      if ((vlen == 8) && (VM_Version::supports_avx512dq() == false)) {\n+        return false; \/\/ 512bit vpmullq, vandpd and vxorpd are not available\n+      }\n+      break;\n+    case Op_RotateRightV:\n+    case Op_RotateLeftV:\n+      if (bt != T_INT && bt != T_LONG) {\n+        return false;\n+      } \/\/ fallthrough\n+    case Op_MacroLogicV:\n+      if (!VM_Version::supports_evex() ||\n+          ((size_in_bits != 512) && !VM_Version::supports_avx512vl())) {\n@@ -1863,2 +3254,7 @@\n-    case Op_CompressM:\n-      if (UseAVX < 3 || !VM_Version::supports_bmi2()) {\n+    case Op_ClearArray:\n+    case Op_VectorMaskGen:\n+    case Op_VectorCmpMasked:\n+      if (!VM_Version::supports_avx512bw()) {\n+        return false;\n+      }\n+      if ((size_in_bits != 512) && !VM_Version::supports_avx512vl()) {\n@@ -1868,3 +3264,3 @@\n-    case Op_CompressV:\n-    case Op_ExpandV:\n-      if (is_subword_type(bt) && !VM_Version::supports_avx512_vbmi2()) {\n+    case Op_LoadVectorMasked:\n+    case Op_StoreVectorMasked:\n+      if (!VM_Version::supports_avx512bw() && (is_subword_type(bt) || UseAVX < 1)) {\n@@ -1873,1 +3269,4 @@\n-      if (size_in_bits < 128 ) {\n+      break;\n+    case Op_UMinV:\n+    case Op_UMaxV:\n+      if (UseAVX == 0) {\n@@ -1876,2 +3275,4 @@\n-    case Op_VectorLongToMask:\n-      if (UseAVX < 1) {\n+      break;\n+    case Op_MaxV:\n+    case Op_MinV:\n+      if (UseSSE < 4 && is_integral_type(bt)) {\n@@ -1880,1 +3281,12 @@\n-      if (UseAVX < 3 && !VM_Version::supports_bmi2()) {\n+      if ((bt == T_FLOAT || bt == T_DOUBLE)) {\n+          \/\/ Float\/Double intrinsics are enabled for AVX family currently.\n+          if (UseAVX == 0) {\n+            return false;\n+          }\n+          if (UseAVX > 2 && (!VM_Version::supports_avx512dq() && size_in_bits == 512)) { \/\/ 512 bit Float\/Double intrinsics need AVX512DQ\n+            return false;\n+          }\n+      }\n+      break;\n+    case Op_CallLeafVector:\n+      if (size_in_bits == 512 && !VM_Version::supports_avx512vlbwdq()) {\n@@ -1884,3 +3296,9 @@\n-    case Op_SignumVD:\n-    case Op_SignumVF:\n-      if (UseAVX < 1) {\n+    case Op_AddReductionVI:\n+      if (bt == T_INT && (UseSSE < 3 || !VM_Version::supports_ssse3())) {\n+        return false;\n+      }\n+      \/\/ fallthrough\n+    case Op_AndReductionV:\n+    case Op_OrReductionV:\n+    case Op_XorReductionV:\n+      if (is_subword_type(bt) && (UseSSE < 4)) {\n@@ -1890,4 +3308,66 @@\n-    case Op_PopCountVI:\n-    case Op_PopCountVL: {\n-        if (!is_pop_count_instr_target(bt) &&\n-            (size_in_bits == 512) && !VM_Version::supports_avx512bw()) {\n+    case Op_MinReductionV:\n+    case Op_MaxReductionV:\n+      if ((bt == T_INT || is_subword_type(bt)) && UseSSE < 4) {\n+        return false;\n+      } else if (bt == T_LONG && (UseAVX < 3 || !VM_Version::supports_avx512vlbwdq())) {\n+        return false;\n+      }\n+      \/\/ Float\/Double intrinsics enabled for AVX family.\n+      if (UseAVX == 0 && (bt == T_FLOAT || bt == T_DOUBLE)) {\n+        return false;\n+      }\n+      if (UseAVX > 2 && (!VM_Version::supports_avx512dq() && size_in_bits == 512)) {\n+        return false;\n+      }\n+      break;\n+    case Op_VectorTest:\n+      if (UseSSE < 4) {\n+        return false; \/\/ Implementation limitation\n+      } else if (size_in_bits < 32) {\n+        return false; \/\/ Implementation limitation\n+      }\n+      break;\n+    case Op_VectorLoadShuffle:\n+    case Op_VectorRearrange:\n+      if(vlen == 2) {\n+        return false; \/\/ Implementation limitation due to how shuffle is loaded\n+      } else if (size_in_bits == 256 && UseAVX < 2) {\n+        return false; \/\/ Implementation limitation\n+      }\n+      break;\n+    case Op_VectorLoadMask:\n+    case Op_VectorMaskCast:\n+      if (size_in_bits == 256 && UseAVX < 2) {\n+        return false; \/\/ Implementation limitation\n+      }\n+      \/\/ fallthrough\n+    case Op_VectorStoreMask:\n+      if (vlen == 2) {\n+        return false; \/\/ Implementation limitation\n+      }\n+      break;\n+    case Op_PopulateIndex:\n+      if (size_in_bits > 256 && !VM_Version::supports_avx512bw()) {\n+        return false;\n+      }\n+      break;\n+    case Op_VectorCastB2X:\n+    case Op_VectorCastS2X:\n+    case Op_VectorCastI2X:\n+      if (bt != T_DOUBLE && size_in_bits == 256 && UseAVX < 2) {\n+        return false;\n+      }\n+      break;\n+    case Op_VectorCastL2X:\n+      if (is_integral_type(bt) && size_in_bits == 256 && UseAVX < 2) {\n+        return false;\n+      } else if (!is_integral_type(bt) && !VM_Version::supports_avx512dq()) {\n+        return false;\n+      }\n+      break;\n+    case Op_VectorCastF2X: {\n+        \/\/ As per JLS section 5.1.3 narrowing conversion to sub-word types\n+        \/\/ happen after intermediate conversion to integer and special handling\n+        \/\/ code needs AVX2 vpcmpeqd instruction for 256 bit vectors.\n+        int src_size_in_bits = type2aelembytes(T_FLOAT) * vlen * BitsPerByte;\n+        if (is_integral_type(bt) && src_size_in_bits == 256 && UseAVX < 2) {\n@@ -1897,0 +3377,5 @@\n+      \/\/ fallthrough\n+    case Op_VectorCastD2X:\n+      if (bt == T_LONG && !VM_Version::supports_avx512dq()) {\n+        return false;\n+      }\n@@ -1898,3 +3383,5 @@\n-    case Op_ReverseV:\n-    case Op_ReverseBytesV:\n-      if (UseAVX < 2) {\n+    case Op_VectorCastF2HF:\n+    case Op_VectorCastHF2F:\n+      if (!VM_Version::supports_f16c() &&\n+         ((!VM_Version::supports_evex() ||\n+         ((size_in_bits != 512) && !VM_Version::supports_avx512vl())))) {\n@@ -1904,3 +3391,2 @@\n-    case Op_CountTrailingZerosV:\n-    case Op_CountLeadingZerosV:\n-      if (UseAVX < 2) {\n+    case Op_RoundVD:\n+      if (!VM_Version::supports_avx512dq()) {\n@@ -1910,3 +3396,12807 @@\n-  }\n-  return true;  \/\/ Per default match rules are supported.\n-}\n+    case Op_MulReductionVI:\n+      if (bt == T_BYTE && size_in_bits == 512 && !VM_Version::supports_avx512bw()) {\n+        return false;\n+      }\n+      break;\n+    case Op_LoadVectorGatherMasked:\n+      if (!is_subword_type(bt) && size_in_bits < 512 && !VM_Version::supports_avx512vl()) {\n+        return false;\n+      }\n+      if (is_subword_type(bt) &&\n+         ((size_in_bits > 256 && !VM_Version::supports_avx512bw()) ||\n+          (size_in_bits < 64)                                      ||\n+          (bt == T_SHORT && !VM_Version::supports_bmi2()))) {\n+        return false;\n+      }\n+      break;\n+    case Op_StoreVectorScatterMasked:\n+    case Op_StoreVectorScatter:\n+      if (is_subword_type(bt)) {\n+        return false;\n+      } else if (size_in_bits < 512 && !VM_Version::supports_avx512vl()) {\n+        return false;\n+      }\n+      \/\/ fallthrough\n+    case Op_LoadVectorGather:\n+      if (!is_subword_type(bt) && size_in_bits == 64) {\n+        return false;\n+      }\n+      if (is_subword_type(bt) && size_in_bits < 64) {\n+        return false;\n+      }\n+      break;\n+    case Op_SaturatingAddV:\n+    case Op_SaturatingSubV:\n+      if (UseAVX < 1) {\n+        return false; \/\/ Implementation limitation\n+      }\n+      if (is_subword_type(bt) && size_in_bits == 512 && !VM_Version::supports_avx512bw()) {\n+        return false;\n+      }\n+      break;\n+    case Op_SelectFromTwoVector:\n+       if (size_in_bits < 128) {\n+         return false;\n+       }\n+       if (size_in_bits < 512 && !VM_Version::supports_avx512vl()) {\n+         return false;\n+       }\n+       if (bt == T_SHORT && !VM_Version::supports_avx512bw()) {\n+         return false;\n+       }\n+       if (bt == T_BYTE && !VM_Version::supports_avx512_vbmi()) {\n+         return false;\n+       }\n+       if ((bt == T_INT || bt == T_FLOAT || bt == T_DOUBLE) && !VM_Version::supports_evex()) {\n+         return false;\n+       }\n+       break;\n+    case Op_MaskAll:\n+      if (!VM_Version::supports_evex()) {\n+        return false;\n+      }\n+      if ((vlen > 16 || is_subword_type(bt)) && !VM_Version::supports_avx512bw()) {\n+        return false;\n+      }\n+      if (size_in_bits < 512 && !VM_Version::supports_avx512vl()) {\n+        return false;\n+      }\n+      break;\n+    case Op_VectorMaskCmp:\n+      if (vlen < 2 || size_in_bits < 32) {\n+        return false;\n+      }\n+      break;\n+    case Op_CompressM:\n+      if (UseAVX < 3 || !VM_Version::supports_bmi2()) {\n+        return false;\n+      }\n+      break;\n+    case Op_CompressV:\n+    case Op_ExpandV:\n+      if (is_subword_type(bt) && !VM_Version::supports_avx512_vbmi2()) {\n+        return false;\n+      }\n+      if (size_in_bits < 128 ) {\n+        return false;\n+      }\n+    case Op_VectorLongToMask:\n+      if (UseAVX < 1) {\n+        return false;\n+      }\n+      if (UseAVX < 3 && !VM_Version::supports_bmi2()) {\n+        return false;\n+      }\n+      break;\n+    case Op_SignumVD:\n+    case Op_SignumVF:\n+      if (UseAVX < 1) {\n+        return false;\n+      }\n+      break;\n+    case Op_PopCountVI:\n+    case Op_PopCountVL: {\n+        if (!is_pop_count_instr_target(bt) &&\n+            (size_in_bits == 512) && !VM_Version::supports_avx512bw()) {\n+          return false;\n+        }\n+      }\n+      break;\n+    case Op_ReverseV:\n+    case Op_ReverseBytesV:\n+      if (UseAVX < 2) {\n+        return false;\n+      }\n+      break;\n+    case Op_CountTrailingZerosV:\n+    case Op_CountLeadingZerosV:\n+      if (UseAVX < 2) {\n+        return false;\n+      }\n+      break;\n+  }\n+  return true;  \/\/ Per default match rules are supported.\n+}\n+\n+bool Matcher::match_rule_supported_vector_masked(int opcode, int vlen, BasicType bt) {\n+  \/\/ ADLC based match_rule_supported routine checks for the existence of pattern based\n+  \/\/ on IR opcode. Most of the unary\/binary\/ternary masked operation share the IR nodes\n+  \/\/ of their non-masked counterpart with mask edge being the differentiator.\n+  \/\/ This routine does a strict check on the existence of masked operation patterns\n+  \/\/ by returning a default false value for all the other opcodes apart from the\n+  \/\/ ones whose masked instruction patterns are defined in this file.\n+  if (!match_rule_supported_vector(opcode, vlen, bt)) {\n+    return false;\n+  }\n+\n+  int size_in_bits = vlen * type2aelembytes(bt) * BitsPerByte;\n+  if (size_in_bits != 512 && !VM_Version::supports_avx512vl()) {\n+    return false;\n+  }\n+  switch(opcode) {\n+    \/\/ Unary masked operations\n+    case Op_AbsVB:\n+    case Op_AbsVS:\n+      if(!VM_Version::supports_avx512bw()) {\n+        return false;  \/\/ Implementation limitation\n+      }\n+    case Op_AbsVI:\n+    case Op_AbsVL:\n+      return true;\n+\n+    \/\/ Ternary masked operations\n+    case Op_FmaVF:\n+    case Op_FmaVD:\n+      return true;\n+\n+    case Op_MacroLogicV:\n+      if(bt != T_INT && bt != T_LONG) {\n+        return false;\n+      }\n+      return true;\n+\n+    \/\/ Binary masked operations\n+    case Op_AddVB:\n+    case Op_AddVS:\n+    case Op_SubVB:\n+    case Op_SubVS:\n+    case Op_MulVS:\n+    case Op_LShiftVS:\n+    case Op_RShiftVS:\n+    case Op_URShiftVS:\n+      assert(size_in_bits == 512 || VM_Version::supports_avx512vl(), \"\");\n+      if (!VM_Version::supports_avx512bw()) {\n+        return false;  \/\/ Implementation limitation\n+      }\n+      return true;\n+\n+    case Op_MulVL:\n+      assert(size_in_bits == 512 || VM_Version::supports_avx512vl(), \"\");\n+      if (!VM_Version::supports_avx512dq()) {\n+        return false;  \/\/ Implementation limitation\n+      }\n+      return true;\n+\n+    case Op_AndV:\n+    case Op_OrV:\n+    case Op_XorV:\n+    case Op_RotateRightV:\n+    case Op_RotateLeftV:\n+      if (bt != T_INT && bt != T_LONG) {\n+        return false; \/\/ Implementation limitation\n+      }\n+      return true;\n+\n+    case Op_VectorLoadMask:\n+      assert(size_in_bits == 512 || VM_Version::supports_avx512vl(), \"\");\n+      if (is_subword_type(bt) && !VM_Version::supports_avx512bw()) {\n+        return false;\n+      }\n+      return true;\n+\n+    case Op_AddVI:\n+    case Op_AddVL:\n+    case Op_AddVF:\n+    case Op_AddVD:\n+    case Op_SubVI:\n+    case Op_SubVL:\n+    case Op_SubVF:\n+    case Op_SubVD:\n+    case Op_MulVI:\n+    case Op_MulVF:\n+    case Op_MulVD:\n+    case Op_DivVF:\n+    case Op_DivVD:\n+    case Op_SqrtVF:\n+    case Op_SqrtVD:\n+    case Op_LShiftVI:\n+    case Op_LShiftVL:\n+    case Op_RShiftVI:\n+    case Op_RShiftVL:\n+    case Op_URShiftVI:\n+    case Op_URShiftVL:\n+    case Op_LoadVectorMasked:\n+    case Op_StoreVectorMasked:\n+    case Op_LoadVectorGatherMasked:\n+    case Op_StoreVectorScatterMasked:\n+      return true;\n+\n+    case Op_UMinV:\n+    case Op_UMaxV:\n+      if (size_in_bits < 512 && !VM_Version::supports_avx512vl()) {\n+        return false;\n+      } \/\/ fallthrough\n+    case Op_MaxV:\n+    case Op_MinV:\n+      if (is_subword_type(bt) && !VM_Version::supports_avx512bw()) {\n+        return false; \/\/ Implementation limitation\n+      }\n+      if (is_floating_point_type(bt) && !VM_Version::supports_avx10_2()) {\n+        return false; \/\/ Implementation limitation\n+      }\n+      return true;\n+    case Op_SaturatingAddV:\n+    case Op_SaturatingSubV:\n+      if (!is_subword_type(bt)) {\n+        return false;\n+      }\n+      if (size_in_bits < 128 || !VM_Version::supports_avx512bw()) {\n+        return false; \/\/ Implementation limitation\n+      }\n+      return true;\n+\n+    case Op_VectorMaskCmp:\n+      if (is_subword_type(bt) && !VM_Version::supports_avx512bw()) {\n+        return false; \/\/ Implementation limitation\n+      }\n+      return true;\n+\n+    case Op_VectorRearrange:\n+      if (bt == T_SHORT && !VM_Version::supports_avx512bw()) {\n+        return false; \/\/ Implementation limitation\n+      }\n+      if (bt == T_BYTE && !VM_Version::supports_avx512_vbmi()) {\n+        return false; \/\/ Implementation limitation\n+      } else if ((bt == T_INT || bt == T_FLOAT) && size_in_bits < 256) {\n+        return false; \/\/ Implementation limitation\n+      }\n+      return true;\n+\n+    \/\/ Binary Logical operations\n+    case Op_AndVMask:\n+    case Op_OrVMask:\n+    case Op_XorVMask:\n+      if (vlen > 16 && !VM_Version::supports_avx512bw()) {\n+        return false; \/\/ Implementation limitation\n+      }\n+      return true;\n+\n+    case Op_PopCountVI:\n+    case Op_PopCountVL:\n+      if (!is_pop_count_instr_target(bt)) {\n+        return false;\n+      }\n+      return true;\n+\n+    case Op_MaskAll:\n+      return true;\n+\n+    case Op_CountLeadingZerosV:\n+      if (is_non_subword_integral_type(bt) && VM_Version::supports_avx512cd()) {\n+        return true;\n+      }\n+    default:\n+      return false;\n+  }\n+}\n+\n+bool Matcher::vector_needs_partial_operations(Node* node, const TypeVect* vt) {\n+  return false;\n+}\n+\n+\/\/ Return true if Vector::rearrange needs preparation of the shuffle argument\n+bool Matcher::vector_rearrange_requires_load_shuffle(BasicType elem_bt, int vlen) {\n+  switch (elem_bt) {\n+    case T_BYTE:  return false;\n+    case T_SHORT: return !VM_Version::supports_avx512bw();\n+    case T_INT:   return !VM_Version::supports_avx();\n+    case T_LONG:  return vlen < 8 && !VM_Version::supports_avx512vl();\n+    default:\n+      ShouldNotReachHere();\n+      return false;\n+  }\n+}\n+\n+bool Matcher::mask_op_prefers_predicate(int opcode, const TypeVect* vt) {\n+  \/\/ Prefer predicate if the mask type is \"TypeVectMask\".\n+  return vt->isa_vectmask() != nullptr;\n+}\n+\n+MachOper* Matcher::pd_specialize_generic_vector_operand(MachOper* generic_opnd, uint ideal_reg, bool is_temp) {\n+  assert(Matcher::is_generic_vector(generic_opnd), \"not generic\");\n+  bool legacy = (generic_opnd->opcode() == LEGVEC);\n+  if (!VM_Version::supports_avx512vlbwdq() && \/\/ KNL\n+      is_temp && !legacy && (ideal_reg == Op_VecZ)) {\n+    \/\/ Conservatively specialize 512bit vec TEMP operands to legVecZ (zmm0-15) on KNL.\n+    return new legVecZOper();\n+  }\n+  if (legacy) {\n+    switch (ideal_reg) {\n+      case Op_VecS: return new legVecSOper();\n+      case Op_VecD: return new legVecDOper();\n+      case Op_VecX: return new legVecXOper();\n+      case Op_VecY: return new legVecYOper();\n+      case Op_VecZ: return new legVecZOper();\n+    }\n+  } else {\n+    switch (ideal_reg) {\n+      case Op_VecS: return new vecSOper();\n+      case Op_VecD: return new vecDOper();\n+      case Op_VecX: return new vecXOper();\n+      case Op_VecY: return new vecYOper();\n+      case Op_VecZ: return new vecZOper();\n+    }\n+  }\n+  ShouldNotReachHere();\n+  return nullptr;\n+}\n+\n+bool Matcher::is_reg2reg_move(MachNode* m) {\n+  switch (m->rule()) {\n+    case MoveVec2Leg_rule:\n+    case MoveLeg2Vec_rule:\n+    case MoveF2VL_rule:\n+    case MoveF2LEG_rule:\n+    case MoveVL2F_rule:\n+    case MoveLEG2F_rule:\n+    case MoveD2VL_rule:\n+    case MoveD2LEG_rule:\n+    case MoveVL2D_rule:\n+    case MoveLEG2D_rule:\n+      return true;\n+    default:\n+      return false;\n+  }\n+}\n+\n+bool Matcher::is_generic_vector(MachOper* opnd) {\n+  switch (opnd->opcode()) {\n+    case VEC:\n+    case LEGVEC:\n+      return true;\n+    default:\n+      return false;\n+  }\n+}\n+\n+\/\/------------------------------------------------------------------------\n+\n+const RegMask* Matcher::predicate_reg_mask(void) {\n+  return &_VECTMASK_REG_mask;\n+}\n+\n+\/\/ Max vector size in bytes. 0 if not supported.\n+int Matcher::vector_width_in_bytes(BasicType bt) {\n+  assert(is_java_primitive(bt), \"only primitive type vectors\");\n+  \/\/ SSE2 supports 128bit vectors for all types.\n+  \/\/ AVX2 supports 256bit vectors for all types.\n+  \/\/ AVX2\/EVEX supports 512bit vectors for all types.\n+  int size = (UseAVX > 1) ? (1 << UseAVX) * 8 : 16;\n+  \/\/ AVX1 supports 256bit vectors only for FLOAT and DOUBLE.\n+  if (UseAVX > 0 && (bt == T_FLOAT || bt == T_DOUBLE))\n+    size = (UseAVX > 2) ? 64 : 32;\n+  if (UseAVX > 2 && (bt == T_BYTE || bt == T_SHORT || bt == T_CHAR))\n+    size = (VM_Version::supports_avx512bw()) ? 64 : 32;\n+  \/\/ Use flag to limit vector size.\n+  size = MIN2(size,(int)MaxVectorSize);\n+  \/\/ Minimum 2 values in vector (or 4 for bytes).\n+  switch (bt) {\n+  case T_DOUBLE:\n+  case T_LONG:\n+    if (size < 16) return 0;\n+    break;\n+  case T_FLOAT:\n+  case T_INT:\n+    if (size < 8) return 0;\n+    break;\n+  case T_BOOLEAN:\n+    if (size < 4) return 0;\n+    break;\n+  case T_CHAR:\n+    if (size < 4) return 0;\n+    break;\n+  case T_BYTE:\n+    if (size < 4) return 0;\n+    break;\n+  case T_SHORT:\n+    if (size < 4) return 0;\n+    break;\n+  default:\n+    ShouldNotReachHere();\n+  }\n+  return size;\n+}\n+\n+\/\/ Limits on vector size (number of elements) loaded into vector.\n+int Matcher::max_vector_size(const BasicType bt) {\n+  return vector_width_in_bytes(bt)\/type2aelembytes(bt);\n+}\n+int Matcher::min_vector_size(const BasicType bt) {\n+  int max_size = max_vector_size(bt);\n+  \/\/ Min size which can be loaded into vector is 4 bytes.\n+  int size = (type2aelembytes(bt) == 1) ? 4 : 2;\n+  \/\/ Support for calling svml double64 vectors\n+  if (bt == T_DOUBLE) {\n+    size = 1;\n+  }\n+  return MIN2(size,max_size);\n+}\n+\n+int Matcher::max_vector_size_auto_vectorization(const BasicType bt) {\n+  \/\/ Limit the max vector size for auto vectorization to 256 bits (32 bytes)\n+  \/\/ by default on Cascade Lake\n+  if (VM_Version::is_default_intel_cascade_lake()) {\n+    return MIN2(Matcher::max_vector_size(bt), 32 \/ type2aelembytes(bt));\n+  }\n+  return Matcher::max_vector_size(bt);\n+}\n+\n+int Matcher::scalable_vector_reg_size(const BasicType bt) {\n+  return -1;\n+}\n+\n+\/\/ Vector ideal reg corresponding to specified size in bytes\n+uint Matcher::vector_ideal_reg(int size) {\n+  assert(MaxVectorSize >= size, \"\");\n+  switch(size) {\n+    case  4: return Op_VecS;\n+    case  8: return Op_VecD;\n+    case 16: return Op_VecX;\n+    case 32: return Op_VecY;\n+    case 64: return Op_VecZ;\n+  }\n+  ShouldNotReachHere();\n+  return 0;\n+}\n+\n+\/\/ Check for shift by small constant as well\n+static bool clone_shift(Node* shift, Matcher* matcher, Matcher::MStack& mstack, VectorSet& address_visited) {\n+  if (shift->Opcode() == Op_LShiftX && shift->in(2)->is_Con() &&\n+      shift->in(2)->get_int() <= 3 &&\n+      \/\/ Are there other uses besides address expressions?\n+      !matcher->is_visited(shift)) {\n+    address_visited.set(shift->_idx); \/\/ Flag as address_visited\n+    mstack.push(shift->in(2), Matcher::Visit);\n+    Node *conv = shift->in(1);\n+    \/\/ Allow Matcher to match the rule which bypass\n+    \/\/ ConvI2L operation for an array index on LP64\n+    \/\/ if the index value is positive.\n+    if (conv->Opcode() == Op_ConvI2L &&\n+        conv->as_Type()->type()->is_long()->_lo >= 0 &&\n+        \/\/ Are there other uses besides address expressions?\n+        !matcher->is_visited(conv)) {\n+      address_visited.set(conv->_idx); \/\/ Flag as address_visited\n+      mstack.push(conv->in(1), Matcher::Pre_Visit);\n+    } else {\n+      mstack.push(conv, Matcher::Pre_Visit);\n+    }\n+    return true;\n+  }\n+  return false;\n+}\n+\n+\/\/ This function identifies sub-graphs in which a 'load' node is\n+\/\/ input to two different nodes, and such that it can be matched\n+\/\/ with BMI instructions like blsi, blsr, etc.\n+\/\/ Example : for b = -a[i] & a[i] can be matched to blsi r32, m32.\n+\/\/ The graph is (AndL (SubL Con0 LoadL*) LoadL*), where LoadL*\n+\/\/ refers to the same node.\n+\/\/\n+\/\/ Match the generic fused operations pattern (op1 (op2 Con{ConType} mop) mop)\n+\/\/ This is a temporary solution until we make DAGs expressible in ADL.\n+template<typename ConType>\n+class FusedPatternMatcher {\n+  Node* _op1_node;\n+  Node* _mop_node;\n+  int _con_op;\n+\n+  static int match_next(Node* n, int next_op, int next_op_idx) {\n+    if (n->in(1) == nullptr || n->in(2) == nullptr) {\n+      return -1;\n+    }\n+\n+    if (next_op_idx == -1) { \/\/ n is commutative, try rotations\n+      if (n->in(1)->Opcode() == next_op) {\n+        return 1;\n+      } else if (n->in(2)->Opcode() == next_op) {\n+        return 2;\n+      }\n+    } else {\n+      assert(next_op_idx > 0 && next_op_idx <= 2, \"Bad argument index\");\n+      if (n->in(next_op_idx)->Opcode() == next_op) {\n+        return next_op_idx;\n+      }\n+    }\n+    return -1;\n+  }\n+\n+ public:\n+  FusedPatternMatcher(Node* op1_node, Node* mop_node, int con_op) :\n+    _op1_node(op1_node), _mop_node(mop_node), _con_op(con_op) { }\n+\n+  bool match(int op1, int op1_op2_idx,  \/\/ op1 and the index of the op1->op2 edge, -1 if op1 is commutative\n+             int op2, int op2_con_idx,  \/\/ op2 and the index of the op2->con edge, -1 if op2 is commutative\n+             typename ConType::NativeType con_value) {\n+    if (_op1_node->Opcode() != op1) {\n+      return false;\n+    }\n+    if (_mop_node->outcnt() > 2) {\n+      return false;\n+    }\n+    op1_op2_idx = match_next(_op1_node, op2, op1_op2_idx);\n+    if (op1_op2_idx == -1) {\n+      return false;\n+    }\n+    \/\/ Memory operation must be the other edge\n+    int op1_mop_idx = (op1_op2_idx & 1) + 1;\n+\n+    \/\/ Check that the mop node is really what we want\n+    if (_op1_node->in(op1_mop_idx) == _mop_node) {\n+      Node* op2_node = _op1_node->in(op1_op2_idx);\n+      if (op2_node->outcnt() > 1) {\n+        return false;\n+      }\n+      assert(op2_node->Opcode() == op2, \"Should be\");\n+      op2_con_idx = match_next(op2_node, _con_op, op2_con_idx);\n+      if (op2_con_idx == -1) {\n+        return false;\n+      }\n+      \/\/ Memory operation must be the other edge\n+      int op2_mop_idx = (op2_con_idx & 1) + 1;\n+      \/\/ Check that the memory operation is the same node\n+      if (op2_node->in(op2_mop_idx) == _mop_node) {\n+        \/\/ Now check the constant\n+        const Type* con_type = op2_node->in(op2_con_idx)->bottom_type();\n+        if (con_type != Type::TOP && ConType::as_self(con_type)->get_con() == con_value) {\n+          return true;\n+        }\n+      }\n+    }\n+    return false;\n+  }\n+};\n+\n+static bool is_bmi_pattern(Node* n, Node* m) {\n+  assert(UseBMI1Instructions, \"sanity\");\n+  if (n != nullptr && m != nullptr) {\n+    if (m->Opcode() == Op_LoadI) {\n+      FusedPatternMatcher<TypeInt> bmii(n, m, Op_ConI);\n+      return bmii.match(Op_AndI, -1, Op_SubI,  1,  0)  ||\n+             bmii.match(Op_AndI, -1, Op_AddI, -1, -1)  ||\n+             bmii.match(Op_XorI, -1, Op_AddI, -1, -1);\n+    } else if (m->Opcode() == Op_LoadL) {\n+      FusedPatternMatcher<TypeLong> bmil(n, m, Op_ConL);\n+      return bmil.match(Op_AndL, -1, Op_SubL,  1,  0) ||\n+             bmil.match(Op_AndL, -1, Op_AddL, -1, -1) ||\n+             bmil.match(Op_XorL, -1, Op_AddL, -1, -1);\n+    }\n+  }\n+  return false;\n+}\n+\n+\/\/ Should the matcher clone input 'm' of node 'n'?\n+bool Matcher::pd_clone_node(Node* n, Node* m, Matcher::MStack& mstack) {\n+  \/\/ If 'n' and 'm' are part of a graph for BMI instruction, clone the input 'm'.\n+  if (UseBMI1Instructions && is_bmi_pattern(n, m)) {\n+    mstack.push(m, Visit);\n+    return true;\n+  }\n+  if (is_vshift_con_pattern(n, m)) { \/\/ ShiftV src (ShiftCntV con)\n+    mstack.push(m, Visit);           \/\/ m = ShiftCntV\n+    return true;\n+  }\n+  if (is_encode_and_store_pattern(n, m)) {\n+    mstack.push(m, Visit);\n+    return true;\n+  }\n+  return false;\n+}\n+\n+\/\/ Should the Matcher clone shifts on addressing modes, expecting them\n+\/\/ to be subsumed into complex addressing expressions or compute them\n+\/\/ into registers?\n+bool Matcher::pd_clone_address_expressions(AddPNode* m, Matcher::MStack& mstack, VectorSet& address_visited) {\n+  Node *off = m->in(AddPNode::Offset);\n+  if (off->is_Con()) {\n+    address_visited.test_set(m->_idx); \/\/ Flag as address_visited\n+    Node *adr = m->in(AddPNode::Address);\n+\n+    \/\/ Intel can handle 2 adds in addressing mode, with one of them using an immediate offset.\n+    \/\/ AtomicAdd is not an addressing expression.\n+    \/\/ Cheap to find it by looking for screwy base.\n+    if (adr->is_AddP() &&\n+        !adr->in(AddPNode::Base)->is_top() &&\n+        !adr->in(AddPNode::Offset)->is_Con() &&\n+        off->get_long() == (int) (off->get_long()) && \/\/ immL32\n+        \/\/ Are there other uses besides address expressions?\n+        !is_visited(adr)) {\n+      address_visited.set(adr->_idx); \/\/ Flag as address_visited\n+      Node *shift = adr->in(AddPNode::Offset);\n+      if (!clone_shift(shift, this, mstack, address_visited)) {\n+        mstack.push(shift, Pre_Visit);\n+      }\n+      mstack.push(adr->in(AddPNode::Address), Pre_Visit);\n+      mstack.push(adr->in(AddPNode::Base), Pre_Visit);\n+    } else {\n+      mstack.push(adr, Pre_Visit);\n+    }\n+\n+    \/\/ Clone X+offset as it also folds into most addressing expressions\n+    mstack.push(off, Visit);\n+    mstack.push(m->in(AddPNode::Base), Pre_Visit);\n+    return true;\n+  } else if (clone_shift(off, this, mstack, address_visited)) {\n+    address_visited.test_set(m->_idx); \/\/ Flag as address_visited\n+    mstack.push(m->in(AddPNode::Address), Pre_Visit);\n+    mstack.push(m->in(AddPNode::Base), Pre_Visit);\n+    return true;\n+  }\n+  return false;\n+}\n+\n+static inline Assembler::ComparisonPredicate booltest_pred_to_comparison_pred(int bt) {\n+  switch (bt) {\n+    case BoolTest::eq:\n+      return Assembler::eq;\n+    case BoolTest::ne:\n+      return Assembler::neq;\n+    case BoolTest::le:\n+    case BoolTest::ule:\n+      return Assembler::le;\n+    case BoolTest::ge:\n+    case BoolTest::uge:\n+      return Assembler::nlt;\n+    case BoolTest::lt:\n+    case BoolTest::ult:\n+      return Assembler::lt;\n+    case BoolTest::gt:\n+    case BoolTest::ugt:\n+      return Assembler::nle;\n+    default : ShouldNotReachHere(); return Assembler::_false;\n+  }\n+}\n+\n+static inline Assembler::ComparisonPredicateFP booltest_pred_to_comparison_pred_fp(int bt) {\n+  switch (bt) {\n+  case BoolTest::eq: return Assembler::EQ_OQ;  \/\/ ordered non-signaling\n+  \/\/ As per JLS 15.21.1, != of NaNs is true. Thus use unordered compare.\n+  case BoolTest::ne: return Assembler::NEQ_UQ; \/\/ unordered non-signaling\n+  case BoolTest::le: return Assembler::LE_OQ;  \/\/ ordered non-signaling\n+  case BoolTest::ge: return Assembler::GE_OQ;  \/\/ ordered non-signaling\n+  case BoolTest::lt: return Assembler::LT_OQ;  \/\/ ordered non-signaling\n+  case BoolTest::gt: return Assembler::GT_OQ;  \/\/ ordered non-signaling\n+  default: ShouldNotReachHere(); return Assembler::FALSE_OS;\n+  }\n+}\n+\n+\/\/ Helper methods for MachSpillCopyNode::implementation().\n+static void vec_mov_helper(C2_MacroAssembler *masm, int src_lo, int dst_lo,\n+                          int src_hi, int dst_hi, uint ireg, outputStream* st) {\n+  assert(ireg == Op_VecS || \/\/ 32bit vector\n+         ((src_lo & 1) == 0 && (src_lo + 1) == src_hi &&\n+          (dst_lo & 1) == 0 && (dst_lo + 1) == dst_hi),\n+         \"no non-adjacent vector moves\" );\n+  if (masm) {\n+    switch (ireg) {\n+    case Op_VecS: \/\/ copy whole register\n+    case Op_VecD:\n+    case Op_VecX:\n+      if ((UseAVX < 3) || VM_Version::supports_avx512vl()) {\n+        __ movdqu(as_XMMRegister(Matcher::_regEncode[dst_lo]), as_XMMRegister(Matcher::_regEncode[src_lo]));\n+      } else {\n+        __ vextractf32x4(as_XMMRegister(Matcher::_regEncode[dst_lo]), as_XMMRegister(Matcher::_regEncode[src_lo]), 0x0);\n+     }\n+      break;\n+    case Op_VecY:\n+      if ((UseAVX < 3) || VM_Version::supports_avx512vl()) {\n+        __ vmovdqu(as_XMMRegister(Matcher::_regEncode[dst_lo]), as_XMMRegister(Matcher::_regEncode[src_lo]));\n+      } else {\n+        __ vextractf64x4(as_XMMRegister(Matcher::_regEncode[dst_lo]), as_XMMRegister(Matcher::_regEncode[src_lo]), 0x0);\n+     }\n+      break;\n+    case Op_VecZ:\n+      __ evmovdquq(as_XMMRegister(Matcher::_regEncode[dst_lo]), as_XMMRegister(Matcher::_regEncode[src_lo]), 2);\n+      break;\n+    default:\n+      ShouldNotReachHere();\n+    }\n+#ifndef PRODUCT\n+  } else {\n+    switch (ireg) {\n+    case Op_VecS:\n+    case Op_VecD:\n+    case Op_VecX:\n+      st->print(\"movdqu  %s,%s\\t# spill\",Matcher::regName[dst_lo],Matcher::regName[src_lo]);\n+      break;\n+    case Op_VecY:\n+    case Op_VecZ:\n+      st->print(\"vmovdqu %s,%s\\t# spill\",Matcher::regName[dst_lo],Matcher::regName[src_lo]);\n+      break;\n+    default:\n+      ShouldNotReachHere();\n+    }\n+#endif\n+  }\n+}\n+\n+void vec_spill_helper(C2_MacroAssembler *masm, bool is_load,\n+                     int stack_offset, int reg, uint ireg, outputStream* st) {\n+  if (masm) {\n+    if (is_load) {\n+      switch (ireg) {\n+      case Op_VecS:\n+        __ movdl(as_XMMRegister(Matcher::_regEncode[reg]), Address(rsp, stack_offset));\n+        break;\n+      case Op_VecD:\n+        __ movq(as_XMMRegister(Matcher::_regEncode[reg]), Address(rsp, stack_offset));\n+        break;\n+      case Op_VecX:\n+        if ((UseAVX < 3) || VM_Version::supports_avx512vl()) {\n+          __ movdqu(as_XMMRegister(Matcher::_regEncode[reg]), Address(rsp, stack_offset));\n+        } else {\n+          __ vpxor(as_XMMRegister(Matcher::_regEncode[reg]), as_XMMRegister(Matcher::_regEncode[reg]), as_XMMRegister(Matcher::_regEncode[reg]), 2);\n+          __ vinsertf32x4(as_XMMRegister(Matcher::_regEncode[reg]), as_XMMRegister(Matcher::_regEncode[reg]), Address(rsp, stack_offset),0x0);\n+        }\n+        break;\n+      case Op_VecY:\n+        if ((UseAVX < 3) || VM_Version::supports_avx512vl()) {\n+          __ vmovdqu(as_XMMRegister(Matcher::_regEncode[reg]), Address(rsp, stack_offset));\n+        } else {\n+          __ vpxor(as_XMMRegister(Matcher::_regEncode[reg]), as_XMMRegister(Matcher::_regEncode[reg]), as_XMMRegister(Matcher::_regEncode[reg]), 2);\n+          __ vinsertf64x4(as_XMMRegister(Matcher::_regEncode[reg]), as_XMMRegister(Matcher::_regEncode[reg]), Address(rsp, stack_offset),0x0);\n+        }\n+        break;\n+      case Op_VecZ:\n+        __ evmovdquq(as_XMMRegister(Matcher::_regEncode[reg]), Address(rsp, stack_offset), 2);\n+        break;\n+      default:\n+        ShouldNotReachHere();\n+      }\n+    } else { \/\/ store\n+      switch (ireg) {\n+      case Op_VecS:\n+        __ movdl(Address(rsp, stack_offset), as_XMMRegister(Matcher::_regEncode[reg]));\n+        break;\n+      case Op_VecD:\n+        __ movq(Address(rsp, stack_offset), as_XMMRegister(Matcher::_regEncode[reg]));\n+        break;\n+      case Op_VecX:\n+        if ((UseAVX < 3) || VM_Version::supports_avx512vl()) {\n+          __ movdqu(Address(rsp, stack_offset), as_XMMRegister(Matcher::_regEncode[reg]));\n+        }\n+        else {\n+          __ vextractf32x4(Address(rsp, stack_offset), as_XMMRegister(Matcher::_regEncode[reg]), 0x0);\n+        }\n+        break;\n+      case Op_VecY:\n+        if ((UseAVX < 3) || VM_Version::supports_avx512vl()) {\n+          __ vmovdqu(Address(rsp, stack_offset), as_XMMRegister(Matcher::_regEncode[reg]));\n+        }\n+        else {\n+          __ vextractf64x4(Address(rsp, stack_offset), as_XMMRegister(Matcher::_regEncode[reg]), 0x0);\n+        }\n+        break;\n+      case Op_VecZ:\n+        __ evmovdquq(Address(rsp, stack_offset), as_XMMRegister(Matcher::_regEncode[reg]), 2);\n+        break;\n+      default:\n+        ShouldNotReachHere();\n+      }\n+    }\n+#ifndef PRODUCT\n+  } else {\n+    if (is_load) {\n+      switch (ireg) {\n+      case Op_VecS:\n+        st->print(\"movd    %s,[rsp + %d]\\t# spill\", Matcher::regName[reg], stack_offset);\n+        break;\n+      case Op_VecD:\n+        st->print(\"movq    %s,[rsp + %d]\\t# spill\", Matcher::regName[reg], stack_offset);\n+        break;\n+       case Op_VecX:\n+        st->print(\"movdqu  %s,[rsp + %d]\\t# spill\", Matcher::regName[reg], stack_offset);\n+        break;\n+      case Op_VecY:\n+      case Op_VecZ:\n+        st->print(\"vmovdqu %s,[rsp + %d]\\t# spill\", Matcher::regName[reg], stack_offset);\n+        break;\n+      default:\n+        ShouldNotReachHere();\n+      }\n+    } else { \/\/ store\n+      switch (ireg) {\n+      case Op_VecS:\n+        st->print(\"movd    [rsp + %d],%s\\t# spill\", stack_offset, Matcher::regName[reg]);\n+        break;\n+      case Op_VecD:\n+        st->print(\"movq    [rsp + %d],%s\\t# spill\", stack_offset, Matcher::regName[reg]);\n+        break;\n+       case Op_VecX:\n+        st->print(\"movdqu  [rsp + %d],%s\\t# spill\", stack_offset, Matcher::regName[reg]);\n+        break;\n+      case Op_VecY:\n+      case Op_VecZ:\n+        st->print(\"vmovdqu [rsp + %d],%s\\t# spill\", stack_offset, Matcher::regName[reg]);\n+        break;\n+      default:\n+        ShouldNotReachHere();\n+      }\n+    }\n+#endif\n+  }\n+}\n+\n+template <class T>\n+static inline GrowableArray<jbyte>* vreplicate_imm(BasicType bt, T con, int len) {\n+  int size = type2aelembytes(bt) * len;\n+  GrowableArray<jbyte>* val = new GrowableArray<jbyte>(size, size, 0);\n+  for (int i = 0; i < len; i++) {\n+    int offset = i * type2aelembytes(bt);\n+    switch (bt) {\n+      case T_BYTE: val->at(i) = con; break;\n+      case T_SHORT: {\n+        jshort c = con;\n+        memcpy(val->adr_at(offset), &c, sizeof(jshort));\n+        break;\n+      }\n+      case T_INT: {\n+        jint c = con;\n+        memcpy(val->adr_at(offset), &c, sizeof(jint));\n+        break;\n+      }\n+      case T_LONG: {\n+        jlong c = con;\n+        memcpy(val->adr_at(offset), &c, sizeof(jlong));\n+        break;\n+      }\n+      case T_FLOAT: {\n+        jfloat c = con;\n+        memcpy(val->adr_at(offset), &c, sizeof(jfloat));\n+        break;\n+      }\n+      case T_DOUBLE: {\n+        jdouble c = con;\n+        memcpy(val->adr_at(offset), &c, sizeof(jdouble));\n+        break;\n+      }\n+      default: assert(false, \"%s\", type2name(bt));\n+    }\n+  }\n+  return val;\n+}\n+\n+static inline jlong high_bit_set(BasicType bt) {\n+  switch (bt) {\n+    case T_BYTE:  return 0x8080808080808080;\n+    case T_SHORT: return 0x8000800080008000;\n+    case T_INT:   return 0x8000000080000000;\n+    case T_LONG:  return 0x8000000000000000;\n+    default:\n+      ShouldNotReachHere();\n+      return 0;\n+  }\n+}\n+\n+#ifndef PRODUCT\n+  void MachNopNode::format(PhaseRegAlloc*, outputStream* st) const {\n+    st->print(\"nop \\t# %d bytes pad for loops and calls\", _count);\n+  }\n+#endif\n+\n+  void MachNopNode::emit(C2_MacroAssembler *masm, PhaseRegAlloc*) const {\n+    __ nop(_count);\n+  }\n+\n+  uint MachNopNode::size(PhaseRegAlloc*) const {\n+    return _count;\n+  }\n+\n+#ifndef PRODUCT\n+  void MachBreakpointNode::format(PhaseRegAlloc*, outputStream* st) const {\n+    st->print(\"# breakpoint\");\n+  }\n+#endif\n+\n+  void MachBreakpointNode::emit(C2_MacroAssembler *masm, PhaseRegAlloc* ra_) const {\n+    __ int3();\n+  }\n+\n+  uint MachBreakpointNode::size(PhaseRegAlloc* ra_) const {\n+    return MachNode::size(ra_);\n+  }\n+\n+%}\n+\n+\/\/----------ENCODING BLOCK-----------------------------------------------------\n+\/\/ This block specifies the encoding classes used by the compiler to\n+\/\/ output byte streams.  Encoding classes are parameterized macros\n+\/\/ used by Machine Instruction Nodes in order to generate the bit\n+\/\/ encoding of the instruction.  Operands specify their base encoding\n+\/\/ interface with the interface keyword.  There are currently\n+\/\/ supported four interfaces, REG_INTER, CONST_INTER, MEMORY_INTER, &\n+\/\/ COND_INTER.  REG_INTER causes an operand to generate a function\n+\/\/ which returns its register number when queried.  CONST_INTER causes\n+\/\/ an operand to generate a function which returns the value of the\n+\/\/ constant when queried.  MEMORY_INTER causes an operand to generate\n+\/\/ four functions which return the Base Register, the Index Register,\n+\/\/ the Scale Value, and the Offset Value of the operand when queried.\n+\/\/ COND_INTER causes an operand to generate six functions which return\n+\/\/ the encoding code (ie - encoding bits for the instruction)\n+\/\/ associated with each basic boolean condition for a conditional\n+\/\/ instruction.\n+\/\/\n+\/\/ Instructions specify two basic values for encoding.  Again, a\n+\/\/ function is available to check if the constant displacement is an\n+\/\/ oop. They use the ins_encode keyword to specify their encoding\n+\/\/ classes (which must be a sequence of enc_class names, and their\n+\/\/ parameters, specified in the encoding block), and they use the\n+\/\/ opcode keyword to specify, in order, their primary, secondary, and\n+\/\/ tertiary opcode.  Only the opcode sections which a particular\n+\/\/ instruction needs for encoding need to be specified.\n+encode %{\n+  enc_class cdql_enc(no_rax_rdx_RegI div)\n+  %{\n+    \/\/ Full implementation of Java idiv and irem; checks for\n+    \/\/ special case as described in JVM spec., p.243 & p.271.\n+    \/\/\n+    \/\/         normal case                           special case\n+    \/\/\n+    \/\/ input : rax: dividend                         min_int\n+    \/\/         reg: divisor                          -1\n+    \/\/\n+    \/\/ output: rax: quotient  (= rax idiv reg)       min_int\n+    \/\/         rdx: remainder (= rax irem reg)       0\n+    \/\/\n+    \/\/  Code sequnce:\n+    \/\/\n+    \/\/    0:   3d 00 00 00 80          cmp    $0x80000000,%eax\n+    \/\/    5:   75 07\/08                jne    e <normal>\n+    \/\/    7:   33 d2                   xor    %edx,%edx\n+    \/\/  [div >= 8 -> offset + 1]\n+    \/\/  [REX_B]\n+    \/\/    9:   83 f9 ff                cmp    $0xffffffffffffffff,$div\n+    \/\/    c:   74 03\/04                je     11 <done>\n+    \/\/ 000000000000000e <normal>:\n+    \/\/    e:   99                      cltd\n+    \/\/  [div >= 8 -> offset + 1]\n+    \/\/  [REX_B]\n+    \/\/    f:   f7 f9                   idiv   $div\n+    \/\/ 0000000000000011 <done>:\n+    Label normal;\n+    Label done;\n+\n+    \/\/ cmp    $0x80000000,%eax\n+    __ cmpl(as_Register(RAX_enc), 0x80000000);\n+\n+    \/\/ jne    e <normal>\n+    __ jccb(Assembler::notEqual, normal);\n+\n+    \/\/ xor    %edx,%edx\n+    __ xorl(as_Register(RDX_enc), as_Register(RDX_enc));\n+\n+    \/\/ cmp    $0xffffffffffffffff,%ecx\n+    __ cmpl($div$$Register, -1);\n+\n+    \/\/ je     11 <done>\n+    __ jccb(Assembler::equal, done);\n+\n+    \/\/ <normal>\n+    \/\/ cltd\n+    __ bind(normal);\n+    __ cdql();\n+\n+    \/\/ idivl\n+    \/\/ <done>\n+    __ idivl($div$$Register);\n+    __ bind(done);\n+  %}\n+\n+  enc_class cdqq_enc(no_rax_rdx_RegL div)\n+  %{\n+    \/\/ Full implementation of Java ldiv and lrem; checks for\n+    \/\/ special case as described in JVM spec., p.243 & p.271.\n+    \/\/\n+    \/\/         normal case                           special case\n+    \/\/\n+    \/\/ input : rax: dividend                         min_long\n+    \/\/         reg: divisor                          -1\n+    \/\/\n+    \/\/ output: rax: quotient  (= rax idiv reg)       min_long\n+    \/\/         rdx: remainder (= rax irem reg)       0\n+    \/\/\n+    \/\/  Code sequnce:\n+    \/\/\n+    \/\/    0:   48 ba 00 00 00 00 00    mov    $0x8000000000000000,%rdx\n+    \/\/    7:   00 00 80\n+    \/\/    a:   48 39 d0                cmp    %rdx,%rax\n+    \/\/    d:   75 08                   jne    17 <normal>\n+    \/\/    f:   33 d2                   xor    %edx,%edx\n+    \/\/   11:   48 83 f9 ff             cmp    $0xffffffffffffffff,$div\n+    \/\/   15:   74 05                   je     1c <done>\n+    \/\/ 0000000000000017 <normal>:\n+    \/\/   17:   48 99                   cqto\n+    \/\/   19:   48 f7 f9                idiv   $div\n+    \/\/ 000000000000001c <done>:\n+    Label normal;\n+    Label done;\n+\n+    \/\/ mov    $0x8000000000000000,%rdx\n+    __ mov64(as_Register(RDX_enc), 0x8000000000000000);\n+\n+    \/\/ cmp    %rdx,%rax\n+    __ cmpq(as_Register(RAX_enc), as_Register(RDX_enc));\n+\n+    \/\/ jne    17 <normal>\n+    __ jccb(Assembler::notEqual, normal);\n+\n+    \/\/ xor    %edx,%edx\n+    __ xorl(as_Register(RDX_enc), as_Register(RDX_enc));\n+\n+    \/\/ cmp    $0xffffffffffffffff,$div\n+    __ cmpq($div$$Register, -1);\n+\n+    \/\/ je     1e <done>\n+    __ jccb(Assembler::equal, done);\n+\n+    \/\/ <normal>\n+    \/\/ cqto\n+    __ bind(normal);\n+    __ cdqq();\n+\n+    \/\/ idivq (note: must be emitted by the user of this rule)\n+    \/\/ <done>\n+    __ idivq($div$$Register);\n+    __ bind(done);\n+  %}\n+\n+  enc_class clear_avx %{\n+    DEBUG_ONLY(int off0 = __ offset());\n+    if (generate_vzeroupper(Compile::current())) {\n+      \/\/ Clear upper bits of YMM registers to avoid AVX <-> SSE transition penalty\n+      \/\/ Clear upper bits of YMM registers when current compiled code uses\n+      \/\/ wide vectors to avoid AVX <-> SSE transition penalty during call.\n+      __ vzeroupper();\n+    }\n+    DEBUG_ONLY(int off1 = __ offset());\n+    assert(off1 - off0 == clear_avx_size(), \"correct size prediction\");\n+  %}\n+\n+  enc_class Java_To_Runtime(method meth) %{\n+    __ lea(r10, RuntimeAddress((address)$meth$$method));\n+    __ call(r10);\n+    __ post_call_nop();\n+  %}\n+\n+  enc_class Java_Static_Call(method meth)\n+  %{\n+    \/\/ JAVA STATIC CALL\n+    \/\/ CALL to fixup routine.  Fixup routine uses ScopeDesc info to\n+    \/\/ determine who we intended to call.\n+    if (!_method) {\n+      __ call(RuntimeAddress(CAST_FROM_FN_PTR(address, $meth$$method)));\n+    } else if (_method->intrinsic_id() == vmIntrinsicID::_ensureMaterializedForStackWalk) {\n+      \/\/ The NOP here is purely to ensure that eliding a call to\n+      \/\/ JVM_EnsureMaterializedForStackWalk doesn't change the code size.\n+      __ addr_nop_5();\n+      __ block_comment(\"call JVM_EnsureMaterializedForStackWalk (elided)\");\n+    } else {\n+      int method_index = resolved_method_index(masm);\n+      RelocationHolder rspec = _optimized_virtual ? opt_virtual_call_Relocation::spec(method_index)\n+                                                  : static_call_Relocation::spec(method_index);\n+      address mark = __ pc();\n+      int call_offset = __ offset();\n+      __ call(AddressLiteral(CAST_FROM_FN_PTR(address, $meth$$method), rspec));\n+      if (CodeBuffer::supports_shared_stubs() && _method->can_be_statically_bound()) {\n+        \/\/ Calls of the same statically bound method can share\n+        \/\/ a stub to the interpreter.\n+        __ code()->shared_stub_to_interp_for(_method, call_offset);\n+      } else {\n+        \/\/ Emit stubs for static call.\n+        address stub = CompiledDirectCall::emit_to_interp_stub(masm, mark);\n+        __ clear_inst_mark();\n+        if (stub == nullptr) {\n+          ciEnv::current()->record_failure(\"CodeCache is full\");\n+          return;\n+        }\n+      }\n+    }\n+    __ post_call_nop();\n+  %}\n+\n+  enc_class Java_Dynamic_Call(method meth) %{\n+    __ ic_call((address)$meth$$method, resolved_method_index(masm));\n+    __ post_call_nop();\n+  %}\n+\n+  enc_class call_epilog %{\n+    if (VerifyStackAtCalls) {\n+      \/\/ Check that stack depth is unchanged: find majik cookie on stack\n+      int framesize = ra_->reg2offset_unchecked(OptoReg::add(ra_->_matcher._old_SP, -3*VMRegImpl::slots_per_word));\n+      Label L;\n+      __ cmpptr(Address(rsp, framesize), (int32_t)0xbadb100d);\n+      __ jccb(Assembler::equal, L);\n+      \/\/ Die if stack mismatch\n+      __ int3();\n+      __ bind(L);\n+    }\n+  %}\n+\n+%}\n+\n+\/\/----------FRAME--------------------------------------------------------------\n+\/\/ Definition of frame structure and management information.\n+\/\/\n+\/\/  S T A C K   L A Y O U T    Allocators stack-slot number\n+\/\/                             |   (to get allocators register number\n+\/\/  G  Owned by    |        |  v    add OptoReg::stack0())\n+\/\/  r   CALLER     |        |\n+\/\/  o     |        +--------+      pad to even-align allocators stack-slot\n+\/\/  w     V        |  pad0  |        numbers; owned by CALLER\n+\/\/  t   -----------+--------+----> Matcher::_in_arg_limit, unaligned\n+\/\/  h     ^        |   in   |  5\n+\/\/        |        |  args  |  4   Holes in incoming args owned by SELF\n+\/\/  |     |        |        |  3\n+\/\/  |     |        +--------+\n+\/\/  V     |        | old out|      Empty on Intel, window on Sparc\n+\/\/        |    old |preserve|      Must be even aligned.\n+\/\/        |     SP-+--------+----> Matcher::_old_SP, even aligned\n+\/\/        |        |   in   |  3   area for Intel ret address\n+\/\/     Owned by    |preserve|      Empty on Sparc.\n+\/\/       SELF      +--------+\n+\/\/        |        |  pad2  |  2   pad to align old SP\n+\/\/        |        +--------+  1\n+\/\/        |        | locks  |  0\n+\/\/        |        +--------+----> OptoReg::stack0(), even aligned\n+\/\/        |        |  pad1  | 11   pad to align new SP\n+\/\/        |        +--------+\n+\/\/        |        |        | 10\n+\/\/        |        | spills |  9   spills\n+\/\/        V        |        |  8   (pad0 slot for callee)\n+\/\/      -----------+--------+----> Matcher::_out_arg_limit, unaligned\n+\/\/        ^        |  out   |  7\n+\/\/        |        |  args  |  6   Holes in outgoing args owned by CALLEE\n+\/\/     Owned by    +--------+\n+\/\/      CALLEE     | new out|  6   Empty on Intel, window on Sparc\n+\/\/        |    new |preserve|      Must be even-aligned.\n+\/\/        |     SP-+--------+----> Matcher::_new_SP, even aligned\n+\/\/        |        |        |\n+\/\/\n+\/\/ Note 1: Only region 8-11 is determined by the allocator.  Region 0-5 is\n+\/\/         known from SELF's arguments and the Java calling convention.\n+\/\/         Region 6-7 is determined per call site.\n+\/\/ Note 2: If the calling convention leaves holes in the incoming argument\n+\/\/         area, those holes are owned by SELF.  Holes in the outgoing area\n+\/\/         are owned by the CALLEE.  Holes should not be necessary in the\n+\/\/         incoming area, as the Java calling convention is completely under\n+\/\/         the control of the AD file.  Doubles can be sorted and packed to\n+\/\/         avoid holes.  Holes in the outgoing arguments may be necessary for\n+\/\/         varargs C calling conventions.\n+\/\/ Note 3: Region 0-3 is even aligned, with pad2 as needed.  Region 3-5 is\n+\/\/         even aligned with pad0 as needed.\n+\/\/         Region 6 is even aligned.  Region 6-7 is NOT even aligned;\n+\/\/         region 6-11 is even aligned; it may be padded out more so that\n+\/\/         the region from SP to FP meets the minimum stack alignment.\n+\/\/ Note 4: For I2C adapters, the incoming FP may not meet the minimum stack\n+\/\/         alignment.  Region 11, pad1, may be dynamically extended so that\n+\/\/         SP meets the minimum alignment.\n+\n+frame\n+%{\n+  \/\/ These three registers define part of the calling convention\n+  \/\/ between compiled code and the interpreter.\n+  inline_cache_reg(RAX);                \/\/ Inline Cache Register\n+\n+  \/\/ Optional: name the operand used by cisc-spilling to access\n+  \/\/ [stack_pointer + offset]\n+  cisc_spilling_operand_name(indOffset32);\n+\n+  \/\/ Number of stack slots consumed by locking an object\n+  sync_stack_slots(2);\n+\n+  \/\/ Compiled code's Frame Pointer\n+  frame_pointer(RSP);\n+\n+  \/\/ Interpreter stores its frame pointer in a register which is\n+  \/\/ stored to the stack by I2CAdaptors.\n+  \/\/ I2CAdaptors convert from interpreted java to compiled java.\n+  interpreter_frame_pointer(RBP);\n+\n+  \/\/ Stack alignment requirement\n+  stack_alignment(StackAlignmentInBytes); \/\/ Alignment size in bytes (128-bit -> 16 bytes)\n+\n+  \/\/ Number of outgoing stack slots killed above the out_preserve_stack_slots\n+  \/\/ for calls to C.  Supports the var-args backing area for register parms.\n+  varargs_C_out_slots_killed(frame::arg_reg_save_area_bytes\/BytesPerInt);\n+\n+  \/\/ The after-PROLOG location of the return address.  Location of\n+  \/\/ return address specifies a type (REG or STACK) and a number\n+  \/\/ representing the register number (i.e. - use a register name) or\n+  \/\/ stack slot.\n+  \/\/ Ret Addr is on stack in slot 0 if no locks or verification or alignment.\n+  \/\/ Otherwise, it is above the locks and verification slot and alignment word\n+  return_addr(STACK - 2 +\n+              align_up((Compile::current()->in_preserve_stack_slots() +\n+                        Compile::current()->fixed_slots()),\n+                       stack_alignment_in_slots()));\n+\n+  \/\/ Location of compiled Java return values.  Same as C for now.\n+  return_value\n+  %{\n+    assert(ideal_reg >= Op_RegI && ideal_reg <= Op_RegL,\n+           \"only return normal values\");\n+\n+    static const int lo[Op_RegL + 1] = {\n+      0,\n+      0,\n+      RAX_num,  \/\/ Op_RegN\n+      RAX_num,  \/\/ Op_RegI\n+      RAX_num,  \/\/ Op_RegP\n+      XMM0_num, \/\/ Op_RegF\n+      XMM0_num, \/\/ Op_RegD\n+      RAX_num   \/\/ Op_RegL\n+    };\n+    static const int hi[Op_RegL + 1] = {\n+      0,\n+      0,\n+      OptoReg::Bad, \/\/ Op_RegN\n+      OptoReg::Bad, \/\/ Op_RegI\n+      RAX_H_num,    \/\/ Op_RegP\n+      OptoReg::Bad, \/\/ Op_RegF\n+      XMM0b_num,    \/\/ Op_RegD\n+      RAX_H_num     \/\/ Op_RegL\n+    };\n+    \/\/ Excluded flags and vector registers.\n+    assert(ARRAY_SIZE(hi) == _last_machine_leaf - 8, \"missing type\");\n+    return OptoRegPair(hi[ideal_reg], lo[ideal_reg]);\n+  %}\n+%}\n+\n+\/\/----------ATTRIBUTES---------------------------------------------------------\n+\/\/----------Operand Attributes-------------------------------------------------\n+op_attrib op_cost(0);        \/\/ Required cost attribute\n+\n+\/\/----------Instruction Attributes---------------------------------------------\n+ins_attrib ins_cost(100);       \/\/ Required cost attribute\n+ins_attrib ins_size(8);         \/\/ Required size attribute (in bits)\n+ins_attrib ins_short_branch(0); \/\/ Required flag: is this instruction\n+                                \/\/ a non-matching short branch variant\n+                                \/\/ of some long branch?\n+ins_attrib ins_alignment(1);    \/\/ Required alignment attribute (must\n+                                \/\/ be a power of 2) specifies the\n+                                \/\/ alignment that some part of the\n+                                \/\/ instruction (not necessarily the\n+                                \/\/ start) requires.  If > 1, a\n+                                \/\/ compute_padding() function must be\n+                                \/\/ provided for the instruction\n+\n+\/\/ Whether this node is expanded during code emission into a sequence of\n+\/\/ instructions and the first instruction can perform an implicit null check.\n+ins_attrib ins_is_late_expanded_null_check_candidate(false);\n+\n+\/\/----------OPERANDS-----------------------------------------------------------\n+\/\/ Operand definitions must precede instruction definitions for correct parsing\n+\/\/ in the ADLC because operands constitute user defined types which are used in\n+\/\/ instruction definitions.\n+\n+\/\/----------Simple Operands----------------------------------------------------\n+\/\/ Immediate Operands\n+\/\/ Integer Immediate\n+operand immI()\n+%{\n+  match(ConI);\n+\n+  op_cost(10);\n+  format %{ %}\n+  interface(CONST_INTER);\n+%}\n+\n+\/\/ Constant for test vs zero\n+operand immI_0()\n+%{\n+  predicate(n->get_int() == 0);\n+  match(ConI);\n+\n+  op_cost(0);\n+  format %{ %}\n+  interface(CONST_INTER);\n+%}\n+\n+\/\/ Constant for increment\n+operand immI_1()\n+%{\n+  predicate(n->get_int() == 1);\n+  match(ConI);\n+\n+  op_cost(0);\n+  format %{ %}\n+  interface(CONST_INTER);\n+%}\n+\n+\/\/ Constant for decrement\n+operand immI_M1()\n+%{\n+  predicate(n->get_int() == -1);\n+  match(ConI);\n+\n+  op_cost(0);\n+  format %{ %}\n+  interface(CONST_INTER);\n+%}\n+\n+operand immI_2()\n+%{\n+  predicate(n->get_int() == 2);\n+  match(ConI);\n+\n+  op_cost(0);\n+  format %{ %}\n+  interface(CONST_INTER);\n+%}\n+\n+operand immI_4()\n+%{\n+  predicate(n->get_int() == 4);\n+  match(ConI);\n+\n+  op_cost(0);\n+  format %{ %}\n+  interface(CONST_INTER);\n+%}\n+\n+operand immI_8()\n+%{\n+  predicate(n->get_int() == 8);\n+  match(ConI);\n+\n+  op_cost(0);\n+  format %{ %}\n+  interface(CONST_INTER);\n+%}\n+\n+\/\/ Valid scale values for addressing modes\n+operand immI2()\n+%{\n+  predicate(0 <= n->get_int() && (n->get_int() <= 3));\n+  match(ConI);\n+\n+  format %{ %}\n+  interface(CONST_INTER);\n+%}\n+\n+operand immU7()\n+%{\n+  predicate((0 <= n->get_int()) && (n->get_int() <= 0x7F));\n+  match(ConI);\n+\n+  op_cost(5);\n+  format %{ %}\n+  interface(CONST_INTER);\n+%}\n+\n+operand immI8()\n+%{\n+  predicate((-0x80 <= n->get_int()) && (n->get_int() < 0x80));\n+  match(ConI);\n+\n+  op_cost(5);\n+  format %{ %}\n+  interface(CONST_INTER);\n+%}\n+\n+operand immU8()\n+%{\n+  predicate((0 <= n->get_int()) && (n->get_int() <= 255));\n+  match(ConI);\n+\n+  op_cost(5);\n+  format %{ %}\n+  interface(CONST_INTER);\n+%}\n+\n+operand immI16()\n+%{\n+  predicate((-32768 <= n->get_int()) && (n->get_int() <= 32767));\n+  match(ConI);\n+\n+  op_cost(10);\n+  format %{ %}\n+  interface(CONST_INTER);\n+%}\n+\n+\/\/ Int Immediate non-negative\n+operand immU31()\n+%{\n+  predicate(n->get_int() >= 0);\n+  match(ConI);\n+\n+  op_cost(0);\n+  format %{ %}\n+  interface(CONST_INTER);\n+%}\n+\n+\/\/ Pointer Immediate\n+operand immP()\n+%{\n+  match(ConP);\n+\n+  op_cost(10);\n+  format %{ %}\n+  interface(CONST_INTER);\n+%}\n+\n+\/\/ Null Pointer Immediate\n+operand immP0()\n+%{\n+  predicate(n->get_ptr() == 0);\n+  match(ConP);\n+\n+  op_cost(5);\n+  format %{ %}\n+  interface(CONST_INTER);\n+%}\n+\n+\/\/ Pointer Immediate\n+operand immN() %{\n+  match(ConN);\n+\n+  op_cost(10);\n+  format %{ %}\n+  interface(CONST_INTER);\n+%}\n+\n+operand immNKlass() %{\n+  match(ConNKlass);\n+\n+  op_cost(10);\n+  format %{ %}\n+  interface(CONST_INTER);\n+%}\n+\n+\/\/ Null Pointer Immediate\n+operand immN0() %{\n+  predicate(n->get_narrowcon() == 0);\n+  match(ConN);\n+\n+  op_cost(5);\n+  format %{ %}\n+  interface(CONST_INTER);\n+%}\n+\n+operand immP31()\n+%{\n+  predicate(n->as_Type()->type()->reloc() == relocInfo::none\n+            && (n->get_ptr() >> 31) == 0);\n+  match(ConP);\n+\n+  op_cost(5);\n+  format %{ %}\n+  interface(CONST_INTER);\n+%}\n+\n+\n+\/\/ Long Immediate\n+operand immL()\n+%{\n+  match(ConL);\n+\n+  op_cost(20);\n+  format %{ %}\n+  interface(CONST_INTER);\n+%}\n+\n+\/\/ Long Immediate 8-bit\n+operand immL8()\n+%{\n+  predicate(-0x80L <= n->get_long() && n->get_long() < 0x80L);\n+  match(ConL);\n+\n+  op_cost(5);\n+  format %{ %}\n+  interface(CONST_INTER);\n+%}\n+\n+\/\/ Long Immediate 32-bit unsigned\n+operand immUL32()\n+%{\n+  predicate(n->get_long() == (unsigned int) (n->get_long()));\n+  match(ConL);\n+\n+  op_cost(10);\n+  format %{ %}\n+  interface(CONST_INTER);\n+%}\n+\n+\/\/ Long Immediate 32-bit signed\n+operand immL32()\n+%{\n+  predicate(n->get_long() == (int) (n->get_long()));\n+  match(ConL);\n+\n+  op_cost(15);\n+  format %{ %}\n+  interface(CONST_INTER);\n+%}\n+\n+operand immL_Pow2()\n+%{\n+  predicate(is_power_of_2((julong)n->get_long()));\n+  match(ConL);\n+\n+  op_cost(15);\n+  format %{ %}\n+  interface(CONST_INTER);\n+%}\n+\n+operand immL_NotPow2()\n+%{\n+  predicate(is_power_of_2((julong)~n->get_long()));\n+  match(ConL);\n+\n+  op_cost(15);\n+  format %{ %}\n+  interface(CONST_INTER);\n+%}\n+\n+\/\/ Long Immediate zero\n+operand immL0()\n+%{\n+  predicate(n->get_long() == 0L);\n+  match(ConL);\n+\n+  op_cost(10);\n+  format %{ %}\n+  interface(CONST_INTER);\n+%}\n+\n+\/\/ Constant for increment\n+operand immL1()\n+%{\n+  predicate(n->get_long() == 1);\n+  match(ConL);\n+\n+  format %{ %}\n+  interface(CONST_INTER);\n+%}\n+\n+\/\/ Constant for decrement\n+operand immL_M1()\n+%{\n+  predicate(n->get_long() == -1);\n+  match(ConL);\n+\n+  format %{ %}\n+  interface(CONST_INTER);\n+%}\n+\n+\/\/ Long Immediate: low 32-bit mask\n+operand immL_32bits()\n+%{\n+  predicate(n->get_long() == 0xFFFFFFFFL);\n+  match(ConL);\n+  op_cost(20);\n+\n+  format %{ %}\n+  interface(CONST_INTER);\n+%}\n+\n+\/\/ Int Immediate: 2^n-1, positive\n+operand immI_Pow2M1()\n+%{\n+  predicate((n->get_int() > 0)\n+            && is_power_of_2((juint)n->get_int() + 1));\n+  match(ConI);\n+\n+  op_cost(20);\n+  format %{ %}\n+  interface(CONST_INTER);\n+%}\n+\n+\/\/ Float Immediate zero\n+operand immF0()\n+%{\n+  predicate(jint_cast(n->getf()) == 0);\n+  match(ConF);\n+\n+  op_cost(5);\n+  format %{ %}\n+  interface(CONST_INTER);\n+%}\n+\n+\/\/ Float Immediate\n+operand immF()\n+%{\n+  match(ConF);\n+\n+  op_cost(15);\n+  format %{ %}\n+  interface(CONST_INTER);\n+%}\n+\n+\/\/ Half Float Immediate\n+operand immH()\n+%{\n+  match(ConH);\n+\n+  op_cost(15);\n+  format %{ %}\n+  interface(CONST_INTER);\n+%}\n+\n+\/\/ Double Immediate zero\n+operand immD0()\n+%{\n+  predicate(jlong_cast(n->getd()) == 0);\n+  match(ConD);\n+\n+  op_cost(5);\n+  format %{ %}\n+  interface(CONST_INTER);\n+%}\n+\n+\/\/ Double Immediate\n+operand immD()\n+%{\n+  match(ConD);\n+\n+  op_cost(15);\n+  format %{ %}\n+  interface(CONST_INTER);\n+%}\n+\n+\/\/ Immediates for special shifts (sign extend)\n+\n+\/\/ Constants for increment\n+operand immI_16()\n+%{\n+  predicate(n->get_int() == 16);\n+  match(ConI);\n+\n+  format %{ %}\n+  interface(CONST_INTER);\n+%}\n+\n+operand immI_24()\n+%{\n+  predicate(n->get_int() == 24);\n+  match(ConI);\n+\n+  format %{ %}\n+  interface(CONST_INTER);\n+%}\n+\n+\/\/ Constant for byte-wide masking\n+operand immI_255()\n+%{\n+  predicate(n->get_int() == 255);\n+  match(ConI);\n+\n+  format %{ %}\n+  interface(CONST_INTER);\n+%}\n+\n+\/\/ Constant for short-wide masking\n+operand immI_65535()\n+%{\n+  predicate(n->get_int() == 65535);\n+  match(ConI);\n+\n+  format %{ %}\n+  interface(CONST_INTER);\n+%}\n+\n+\/\/ Constant for byte-wide masking\n+operand immL_255()\n+%{\n+  predicate(n->get_long() == 255);\n+  match(ConL);\n+\n+  format %{ %}\n+  interface(CONST_INTER);\n+%}\n+\n+\/\/ Constant for short-wide masking\n+operand immL_65535()\n+%{\n+  predicate(n->get_long() == 65535);\n+  match(ConL);\n+\n+  format %{ %}\n+  interface(CONST_INTER);\n+%}\n+\n+operand kReg()\n+%{\n+  constraint(ALLOC_IN_RC(vectmask_reg));\n+  match(RegVectMask);\n+  format %{%}\n+  interface(REG_INTER);\n+%}\n+\n+\/\/ Register Operands\n+\/\/ Integer Register\n+operand rRegI()\n+%{\n+  constraint(ALLOC_IN_RC(int_reg));\n+  match(RegI);\n+\n+  match(rax_RegI);\n+  match(rbx_RegI);\n+  match(rcx_RegI);\n+  match(rdx_RegI);\n+  match(rdi_RegI);\n+\n+  format %{ %}\n+  interface(REG_INTER);\n+%}\n+\n+\/\/ Special Registers\n+operand rax_RegI()\n+%{\n+  constraint(ALLOC_IN_RC(int_rax_reg));\n+  match(RegI);\n+  match(rRegI);\n+\n+  format %{ \"RAX\" %}\n+  interface(REG_INTER);\n+%}\n+\n+\/\/ Special Registers\n+operand rbx_RegI()\n+%{\n+  constraint(ALLOC_IN_RC(int_rbx_reg));\n+  match(RegI);\n+  match(rRegI);\n+\n+  format %{ \"RBX\" %}\n+  interface(REG_INTER);\n+%}\n+\n+operand rcx_RegI()\n+%{\n+  constraint(ALLOC_IN_RC(int_rcx_reg));\n+  match(RegI);\n+  match(rRegI);\n+\n+  format %{ \"RCX\" %}\n+  interface(REG_INTER);\n+%}\n+\n+operand rdx_RegI()\n+%{\n+  constraint(ALLOC_IN_RC(int_rdx_reg));\n+  match(RegI);\n+  match(rRegI);\n+\n+  format %{ \"RDX\" %}\n+  interface(REG_INTER);\n+%}\n+\n+operand rdi_RegI()\n+%{\n+  constraint(ALLOC_IN_RC(int_rdi_reg));\n+  match(RegI);\n+  match(rRegI);\n+\n+  format %{ \"RDI\" %}\n+  interface(REG_INTER);\n+%}\n+\n+operand no_rax_rdx_RegI()\n+%{\n+  constraint(ALLOC_IN_RC(int_no_rax_rdx_reg));\n+  match(RegI);\n+  match(rbx_RegI);\n+  match(rcx_RegI);\n+  match(rdi_RegI);\n+\n+  format %{ %}\n+  interface(REG_INTER);\n+%}\n+\n+operand no_rbp_r13_RegI()\n+%{\n+  constraint(ALLOC_IN_RC(int_no_rbp_r13_reg));\n+  match(RegI);\n+  match(rRegI);\n+  match(rax_RegI);\n+  match(rbx_RegI);\n+  match(rcx_RegI);\n+  match(rdx_RegI);\n+  match(rdi_RegI);\n+\n+  format %{ %}\n+  interface(REG_INTER);\n+%}\n+\n+\/\/ Pointer Register\n+operand any_RegP()\n+%{\n+  constraint(ALLOC_IN_RC(any_reg));\n+  match(RegP);\n+  match(rax_RegP);\n+  match(rbx_RegP);\n+  match(rdi_RegP);\n+  match(rsi_RegP);\n+  match(rbp_RegP);\n+  match(r15_RegP);\n+  match(rRegP);\n+\n+  format %{ %}\n+  interface(REG_INTER);\n+%}\n+\n+operand rRegP()\n+%{\n+  constraint(ALLOC_IN_RC(ptr_reg));\n+  match(RegP);\n+  match(rax_RegP);\n+  match(rbx_RegP);\n+  match(rdi_RegP);\n+  match(rsi_RegP);\n+  match(rbp_RegP);  \/\/ See Q&A below about\n+  match(r15_RegP);  \/\/ r15_RegP and rbp_RegP.\n+\n+  format %{ %}\n+  interface(REG_INTER);\n+%}\n+\n+operand rRegN() %{\n+  constraint(ALLOC_IN_RC(int_reg));\n+  match(RegN);\n+\n+  format %{ %}\n+  interface(REG_INTER);\n+%}\n+\n+\/\/ Question: Why is r15_RegP (the read-only TLS register) a match for rRegP?\n+\/\/ Answer: Operand match rules govern the DFA as it processes instruction inputs.\n+\/\/ It's fine for an instruction input that expects rRegP to match a r15_RegP.\n+\/\/ The output of an instruction is controlled by the allocator, which respects\n+\/\/ register class masks, not match rules.  Unless an instruction mentions\n+\/\/ r15_RegP or any_RegP explicitly as its output, r15 will not be considered\n+\/\/ by the allocator as an input.\n+\/\/ The same logic applies to rbp_RegP being a match for rRegP: If PreserveFramePointer==true,\n+\/\/ the RBP is used as a proper frame pointer and is not included in ptr_reg. As a\n+\/\/ result, RBP is not included in the output of the instruction either.\n+\n+\/\/ This operand is not allowed to use RBP even if\n+\/\/ RBP is not used to hold the frame pointer.\n+operand no_rbp_RegP()\n+%{\n+  constraint(ALLOC_IN_RC(ptr_reg_no_rbp));\n+  match(RegP);\n+  match(rbx_RegP);\n+  match(rsi_RegP);\n+  match(rdi_RegP);\n+\n+  format %{ %}\n+  interface(REG_INTER);\n+%}\n+\n+\/\/ Special Registers\n+\/\/ Return a pointer value\n+operand rax_RegP()\n+%{\n+  constraint(ALLOC_IN_RC(ptr_rax_reg));\n+  match(RegP);\n+  match(rRegP);\n+\n+  format %{ %}\n+  interface(REG_INTER);\n+%}\n+\n+\/\/ Special Registers\n+\/\/ Return a compressed pointer value\n+operand rax_RegN()\n+%{\n+  constraint(ALLOC_IN_RC(int_rax_reg));\n+  match(RegN);\n+  match(rRegN);\n+\n+  format %{ %}\n+  interface(REG_INTER);\n+%}\n+\n+\/\/ Used in AtomicAdd\n+operand rbx_RegP()\n+%{\n+  constraint(ALLOC_IN_RC(ptr_rbx_reg));\n+  match(RegP);\n+  match(rRegP);\n+\n+  format %{ %}\n+  interface(REG_INTER);\n+%}\n+\n+operand rsi_RegP()\n+%{\n+  constraint(ALLOC_IN_RC(ptr_rsi_reg));\n+  match(RegP);\n+  match(rRegP);\n+\n+  format %{ %}\n+  interface(REG_INTER);\n+%}\n+\n+operand rbp_RegP()\n+%{\n+  constraint(ALLOC_IN_RC(ptr_rbp_reg));\n+  match(RegP);\n+  match(rRegP);\n+\n+  format %{ %}\n+  interface(REG_INTER);\n+%}\n+\n+\/\/ Used in rep stosq\n+operand rdi_RegP()\n+%{\n+  constraint(ALLOC_IN_RC(ptr_rdi_reg));\n+  match(RegP);\n+  match(rRegP);\n+\n+  format %{ %}\n+  interface(REG_INTER);\n+%}\n+\n+operand r15_RegP()\n+%{\n+  constraint(ALLOC_IN_RC(ptr_r15_reg));\n+  match(RegP);\n+  match(rRegP);\n+\n+  format %{ %}\n+  interface(REG_INTER);\n+%}\n+\n+operand rRegL()\n+%{\n+  constraint(ALLOC_IN_RC(long_reg));\n+  match(RegL);\n+  match(rax_RegL);\n+  match(rdx_RegL);\n+\n+  format %{ %}\n+  interface(REG_INTER);\n+%}\n+\n+\/\/ Special Registers\n+operand no_rax_rdx_RegL()\n+%{\n+  constraint(ALLOC_IN_RC(long_no_rax_rdx_reg));\n+  match(RegL);\n+  match(rRegL);\n+\n+  format %{ %}\n+  interface(REG_INTER);\n+%}\n+\n+operand rax_RegL()\n+%{\n+  constraint(ALLOC_IN_RC(long_rax_reg));\n+  match(RegL);\n+  match(rRegL);\n+\n+  format %{ \"RAX\" %}\n+  interface(REG_INTER);\n+%}\n+\n+operand rcx_RegL()\n+%{\n+  constraint(ALLOC_IN_RC(long_rcx_reg));\n+  match(RegL);\n+  match(rRegL);\n+\n+  format %{ %}\n+  interface(REG_INTER);\n+%}\n+\n+operand rdx_RegL()\n+%{\n+  constraint(ALLOC_IN_RC(long_rdx_reg));\n+  match(RegL);\n+  match(rRegL);\n+\n+  format %{ %}\n+  interface(REG_INTER);\n+%}\n+\n+operand r11_RegL()\n+%{\n+  constraint(ALLOC_IN_RC(long_r11_reg));\n+  match(RegL);\n+  match(rRegL);\n+\n+  format %{ %}\n+  interface(REG_INTER);\n+%}\n+\n+operand no_rbp_r13_RegL()\n+%{\n+  constraint(ALLOC_IN_RC(long_no_rbp_r13_reg));\n+  match(RegL);\n+  match(rRegL);\n+  match(rax_RegL);\n+  match(rcx_RegL);\n+  match(rdx_RegL);\n+\n+  format %{ %}\n+  interface(REG_INTER);\n+%}\n+\n+\/\/ Flags register, used as output of compare instructions\n+operand rFlagsReg()\n+%{\n+  constraint(ALLOC_IN_RC(int_flags));\n+  match(RegFlags);\n+\n+  format %{ \"RFLAGS\" %}\n+  interface(REG_INTER);\n+%}\n+\n+\/\/ Flags register, used as output of FLOATING POINT compare instructions\n+operand rFlagsRegU()\n+%{\n+  constraint(ALLOC_IN_RC(int_flags));\n+  match(RegFlags);\n+\n+  format %{ \"RFLAGS_U\" %}\n+  interface(REG_INTER);\n+%}\n+\n+operand rFlagsRegUCF() %{\n+  constraint(ALLOC_IN_RC(int_flags));\n+  match(RegFlags);\n+  predicate(false);\n+\n+  format %{ \"RFLAGS_U_CF\" %}\n+  interface(REG_INTER);\n+%}\n+\n+\/\/ Float register operands\n+operand regF() %{\n+   constraint(ALLOC_IN_RC(float_reg));\n+   match(RegF);\n+\n+   format %{ %}\n+   interface(REG_INTER);\n+%}\n+\n+\/\/ Float register operands\n+operand legRegF() %{\n+   constraint(ALLOC_IN_RC(float_reg_legacy));\n+   match(RegF);\n+\n+   format %{ %}\n+   interface(REG_INTER);\n+%}\n+\n+\/\/ Float register operands\n+operand vlRegF() %{\n+   constraint(ALLOC_IN_RC(float_reg_vl));\n+   match(RegF);\n+\n+   format %{ %}\n+   interface(REG_INTER);\n+%}\n+\n+\/\/ Double register operands\n+operand regD() %{\n+   constraint(ALLOC_IN_RC(double_reg));\n+   match(RegD);\n+\n+   format %{ %}\n+   interface(REG_INTER);\n+%}\n+\n+\/\/ Double register operands\n+operand legRegD() %{\n+   constraint(ALLOC_IN_RC(double_reg_legacy));\n+   match(RegD);\n+\n+   format %{ %}\n+   interface(REG_INTER);\n+%}\n+\n+\/\/ Double register operands\n+operand vlRegD() %{\n+   constraint(ALLOC_IN_RC(double_reg_vl));\n+   match(RegD);\n+\n+   format %{ %}\n+   interface(REG_INTER);\n+%}\n+\n+\/\/----------Memory Operands----------------------------------------------------\n+\/\/ Direct Memory Operand\n+\/\/ operand direct(immP addr)\n+\/\/ %{\n+\/\/   match(addr);\n+\n+\/\/   format %{ \"[$addr]\" %}\n+\/\/   interface(MEMORY_INTER) %{\n+\/\/     base(0xFFFFFFFF);\n+\/\/     index(0x4);\n+\/\/     scale(0x0);\n+\/\/     disp($addr);\n+\/\/   %}\n+\/\/ %}\n+\n+\/\/ Indirect Memory Operand\n+operand indirect(any_RegP reg)\n+%{\n+  constraint(ALLOC_IN_RC(ptr_reg));\n+  match(reg);\n+\n+  format %{ \"[$reg]\" %}\n+  interface(MEMORY_INTER) %{\n+    base($reg);\n+    index(0x4);\n+    scale(0x0);\n+    disp(0x0);\n+  %}\n+%}\n+\n+\/\/ Indirect Memory Plus Short Offset Operand\n+operand indOffset8(any_RegP reg, immL8 off)\n+%{\n+  constraint(ALLOC_IN_RC(ptr_reg));\n+  match(AddP reg off);\n+\n+  format %{ \"[$reg + $off (8-bit)]\" %}\n+  interface(MEMORY_INTER) %{\n+    base($reg);\n+    index(0x4);\n+    scale(0x0);\n+    disp($off);\n+  %}\n+%}\n+\n+\/\/ Indirect Memory Plus Long Offset Operand\n+operand indOffset32(any_RegP reg, immL32 off)\n+%{\n+  constraint(ALLOC_IN_RC(ptr_reg));\n+  match(AddP reg off);\n+\n+  format %{ \"[$reg + $off (32-bit)]\" %}\n+  interface(MEMORY_INTER) %{\n+    base($reg);\n+    index(0x4);\n+    scale(0x0);\n+    disp($off);\n+  %}\n+%}\n+\n+\/\/ Indirect Memory Plus Index Register Plus Offset Operand\n+operand indIndexOffset(any_RegP reg, rRegL lreg, immL32 off)\n+%{\n+  constraint(ALLOC_IN_RC(ptr_reg));\n+  match(AddP (AddP reg lreg) off);\n+\n+  op_cost(10);\n+  format %{\"[$reg + $off + $lreg]\" %}\n+  interface(MEMORY_INTER) %{\n+    base($reg);\n+    index($lreg);\n+    scale(0x0);\n+    disp($off);\n+  %}\n+%}\n+\n+\/\/ Indirect Memory Plus Index Register Plus Offset Operand\n+operand indIndex(any_RegP reg, rRegL lreg)\n+%{\n+  constraint(ALLOC_IN_RC(ptr_reg));\n+  match(AddP reg lreg);\n+\n+  op_cost(10);\n+  format %{\"[$reg + $lreg]\" %}\n+  interface(MEMORY_INTER) %{\n+    base($reg);\n+    index($lreg);\n+    scale(0x0);\n+    disp(0x0);\n+  %}\n+%}\n+\n+\/\/ Indirect Memory Times Scale Plus Index Register\n+operand indIndexScale(any_RegP reg, rRegL lreg, immI2 scale)\n+%{\n+  constraint(ALLOC_IN_RC(ptr_reg));\n+  match(AddP reg (LShiftL lreg scale));\n+\n+  op_cost(10);\n+  format %{\"[$reg + $lreg << $scale]\" %}\n+  interface(MEMORY_INTER) %{\n+    base($reg);\n+    index($lreg);\n+    scale($scale);\n+    disp(0x0);\n+  %}\n+%}\n+\n+operand indPosIndexScale(any_RegP reg, rRegI idx, immI2 scale)\n+%{\n+  constraint(ALLOC_IN_RC(ptr_reg));\n+  predicate(n->in(3)->in(1)->as_Type()->type()->is_long()->_lo >= 0);\n+  match(AddP reg (LShiftL (ConvI2L idx) scale));\n+\n+  op_cost(10);\n+  format %{\"[$reg + pos $idx << $scale]\" %}\n+  interface(MEMORY_INTER) %{\n+    base($reg);\n+    index($idx);\n+    scale($scale);\n+    disp(0x0);\n+  %}\n+%}\n+\n+\/\/ Indirect Memory Times Scale Plus Index Register Plus Offset Operand\n+operand indIndexScaleOffset(any_RegP reg, immL32 off, rRegL lreg, immI2 scale)\n+%{\n+  constraint(ALLOC_IN_RC(ptr_reg));\n+  match(AddP (AddP reg (LShiftL lreg scale)) off);\n+\n+  op_cost(10);\n+  format %{\"[$reg + $off + $lreg << $scale]\" %}\n+  interface(MEMORY_INTER) %{\n+    base($reg);\n+    index($lreg);\n+    scale($scale);\n+    disp($off);\n+  %}\n+%}\n+\n+\/\/ Indirect Memory Plus Positive Index Register Plus Offset Operand\n+operand indPosIndexOffset(any_RegP reg, immL32 off, rRegI idx)\n+%{\n+  constraint(ALLOC_IN_RC(ptr_reg));\n+  predicate(n->in(2)->in(3)->as_Type()->type()->is_long()->_lo >= 0);\n+  match(AddP (AddP reg (ConvI2L idx)) off);\n+\n+  op_cost(10);\n+  format %{\"[$reg + $off + $idx]\" %}\n+  interface(MEMORY_INTER) %{\n+    base($reg);\n+    index($idx);\n+    scale(0x0);\n+    disp($off);\n+  %}\n+%}\n+\n+\/\/ Indirect Memory Times Scale Plus Positive Index Register Plus Offset Operand\n+operand indPosIndexScaleOffset(any_RegP reg, immL32 off, rRegI idx, immI2 scale)\n+%{\n+  constraint(ALLOC_IN_RC(ptr_reg));\n+  predicate(n->in(2)->in(3)->in(1)->as_Type()->type()->is_long()->_lo >= 0);\n+  match(AddP (AddP reg (LShiftL (ConvI2L idx) scale)) off);\n+\n+  op_cost(10);\n+  format %{\"[$reg + $off + $idx << $scale]\" %}\n+  interface(MEMORY_INTER) %{\n+    base($reg);\n+    index($idx);\n+    scale($scale);\n+    disp($off);\n+  %}\n+%}\n+\n+\/\/ Indirect Narrow Oop Plus Offset Operand\n+\/\/ Note: x86 architecture doesn't support \"scale * index + offset\" without a base\n+\/\/ we can't free r12 even with CompressedOops::base() == nullptr.\n+operand indCompressedOopOffset(rRegN reg, immL32 off) %{\n+  predicate(UseCompressedOops && (CompressedOops::shift() == Address::times_8));\n+  constraint(ALLOC_IN_RC(ptr_reg));\n+  match(AddP (DecodeN reg) off);\n+\n+  op_cost(10);\n+  format %{\"[R12 + $reg << 3 + $off] (compressed oop addressing)\" %}\n+  interface(MEMORY_INTER) %{\n+    base(0xc); \/\/ R12\n+    index($reg);\n+    scale(0x3);\n+    disp($off);\n+  %}\n+%}\n+\n+\/\/ Indirect Memory Operand\n+operand indirectNarrow(rRegN reg)\n+%{\n+  predicate(CompressedOops::shift() == 0);\n+  constraint(ALLOC_IN_RC(ptr_reg));\n+  match(DecodeN reg);\n+\n+  format %{ \"[$reg]\" %}\n+  interface(MEMORY_INTER) %{\n+    base($reg);\n+    index(0x4);\n+    scale(0x0);\n+    disp(0x0);\n+  %}\n+%}\n+\n+\/\/ Indirect Memory Plus Short Offset Operand\n+operand indOffset8Narrow(rRegN reg, immL8 off)\n+%{\n+  predicate(CompressedOops::shift() == 0);\n+  constraint(ALLOC_IN_RC(ptr_reg));\n+  match(AddP (DecodeN reg) off);\n+\n+  format %{ \"[$reg + $off (8-bit)]\" %}\n+  interface(MEMORY_INTER) %{\n+    base($reg);\n+    index(0x4);\n+    scale(0x0);\n+    disp($off);\n+  %}\n+%}\n+\n+\/\/ Indirect Memory Plus Long Offset Operand\n+operand indOffset32Narrow(rRegN reg, immL32 off)\n+%{\n+  predicate(CompressedOops::shift() == 0);\n+  constraint(ALLOC_IN_RC(ptr_reg));\n+  match(AddP (DecodeN reg) off);\n+\n+  format %{ \"[$reg + $off (32-bit)]\" %}\n+  interface(MEMORY_INTER) %{\n+    base($reg);\n+    index(0x4);\n+    scale(0x0);\n+    disp($off);\n+  %}\n+%}\n+\n+\/\/ Indirect Memory Plus Index Register Plus Offset Operand\n+operand indIndexOffsetNarrow(rRegN reg, rRegL lreg, immL32 off)\n+%{\n+  predicate(CompressedOops::shift() == 0);\n+  constraint(ALLOC_IN_RC(ptr_reg));\n+  match(AddP (AddP (DecodeN reg) lreg) off);\n+\n+  op_cost(10);\n+  format %{\"[$reg + $off + $lreg]\" %}\n+  interface(MEMORY_INTER) %{\n+    base($reg);\n+    index($lreg);\n+    scale(0x0);\n+    disp($off);\n+  %}\n+%}\n+\n+\/\/ Indirect Memory Plus Index Register Plus Offset Operand\n+operand indIndexNarrow(rRegN reg, rRegL lreg)\n+%{\n+  predicate(CompressedOops::shift() == 0);\n+  constraint(ALLOC_IN_RC(ptr_reg));\n+  match(AddP (DecodeN reg) lreg);\n+\n+  op_cost(10);\n+  format %{\"[$reg + $lreg]\" %}\n+  interface(MEMORY_INTER) %{\n+    base($reg);\n+    index($lreg);\n+    scale(0x0);\n+    disp(0x0);\n+  %}\n+%}\n+\n+\/\/ Indirect Memory Times Scale Plus Index Register\n+operand indIndexScaleNarrow(rRegN reg, rRegL lreg, immI2 scale)\n+%{\n+  predicate(CompressedOops::shift() == 0);\n+  constraint(ALLOC_IN_RC(ptr_reg));\n+  match(AddP (DecodeN reg) (LShiftL lreg scale));\n+\n+  op_cost(10);\n+  format %{\"[$reg + $lreg << $scale]\" %}\n+  interface(MEMORY_INTER) %{\n+    base($reg);\n+    index($lreg);\n+    scale($scale);\n+    disp(0x0);\n+  %}\n+%}\n+\n+\/\/ Indirect Memory Times Scale Plus Index Register Plus Offset Operand\n+operand indIndexScaleOffsetNarrow(rRegN reg, immL32 off, rRegL lreg, immI2 scale)\n+%{\n+  predicate(CompressedOops::shift() == 0);\n+  constraint(ALLOC_IN_RC(ptr_reg));\n+  match(AddP (AddP (DecodeN reg) (LShiftL lreg scale)) off);\n+\n+  op_cost(10);\n+  format %{\"[$reg + $off + $lreg << $scale]\" %}\n+  interface(MEMORY_INTER) %{\n+    base($reg);\n+    index($lreg);\n+    scale($scale);\n+    disp($off);\n+  %}\n+%}\n+\n+\/\/ Indirect Memory Times Plus Positive Index Register Plus Offset Operand\n+operand indPosIndexOffsetNarrow(rRegN reg, immL32 off, rRegI idx)\n+%{\n+  constraint(ALLOC_IN_RC(ptr_reg));\n+  predicate(CompressedOops::shift() == 0 && n->in(2)->in(3)->as_Type()->type()->is_long()->_lo >= 0);\n+  match(AddP (AddP (DecodeN reg) (ConvI2L idx)) off);\n+\n+  op_cost(10);\n+  format %{\"[$reg + $off + $idx]\" %}\n+  interface(MEMORY_INTER) %{\n+    base($reg);\n+    index($idx);\n+    scale(0x0);\n+    disp($off);\n+  %}\n+%}\n+\n+\/\/ Indirect Memory Times Scale Plus Positive Index Register Plus Offset Operand\n+operand indPosIndexScaleOffsetNarrow(rRegN reg, immL32 off, rRegI idx, immI2 scale)\n+%{\n+  constraint(ALLOC_IN_RC(ptr_reg));\n+  predicate(CompressedOops::shift() == 0 && n->in(2)->in(3)->in(1)->as_Type()->type()->is_long()->_lo >= 0);\n+  match(AddP (AddP (DecodeN reg) (LShiftL (ConvI2L idx) scale)) off);\n+\n+  op_cost(10);\n+  format %{\"[$reg + $off + $idx << $scale]\" %}\n+  interface(MEMORY_INTER) %{\n+    base($reg);\n+    index($idx);\n+    scale($scale);\n+    disp($off);\n+  %}\n+%}\n+\n+\/\/----------Special Memory Operands--------------------------------------------\n+\/\/ Stack Slot Operand - This operand is used for loading and storing temporary\n+\/\/                      values on the stack where a match requires a value to\n+\/\/                      flow through memory.\n+operand stackSlotP(sRegP reg)\n+%{\n+  constraint(ALLOC_IN_RC(stack_slots));\n+  \/\/ No match rule because this operand is only generated in matching\n+\n+  format %{ \"[$reg]\" %}\n+  interface(MEMORY_INTER) %{\n+    base(0x4);   \/\/ RSP\n+    index(0x4);  \/\/ No Index\n+    scale(0x0);  \/\/ No Scale\n+    disp($reg);  \/\/ Stack Offset\n+  %}\n+%}\n+\n+operand stackSlotI(sRegI reg)\n+%{\n+  constraint(ALLOC_IN_RC(stack_slots));\n+  \/\/ No match rule because this operand is only generated in matching\n+\n+  format %{ \"[$reg]\" %}\n+  interface(MEMORY_INTER) %{\n+    base(0x4);   \/\/ RSP\n+    index(0x4);  \/\/ No Index\n+    scale(0x0);  \/\/ No Scale\n+    disp($reg);  \/\/ Stack Offset\n+  %}\n+%}\n+\n+operand stackSlotF(sRegF reg)\n+%{\n+  constraint(ALLOC_IN_RC(stack_slots));\n+  \/\/ No match rule because this operand is only generated in matching\n+\n+  format %{ \"[$reg]\" %}\n+  interface(MEMORY_INTER) %{\n+    base(0x4);   \/\/ RSP\n+    index(0x4);  \/\/ No Index\n+    scale(0x0);  \/\/ No Scale\n+    disp($reg);  \/\/ Stack Offset\n+  %}\n+%}\n+\n+operand stackSlotD(sRegD reg)\n+%{\n+  constraint(ALLOC_IN_RC(stack_slots));\n+  \/\/ No match rule because this operand is only generated in matching\n+\n+  format %{ \"[$reg]\" %}\n+  interface(MEMORY_INTER) %{\n+    base(0x4);   \/\/ RSP\n+    index(0x4);  \/\/ No Index\n+    scale(0x0);  \/\/ No Scale\n+    disp($reg);  \/\/ Stack Offset\n+  %}\n+%}\n+operand stackSlotL(sRegL reg)\n+%{\n+  constraint(ALLOC_IN_RC(stack_slots));\n+  \/\/ No match rule because this operand is only generated in matching\n+\n+  format %{ \"[$reg]\" %}\n+  interface(MEMORY_INTER) %{\n+    base(0x4);   \/\/ RSP\n+    index(0x4);  \/\/ No Index\n+    scale(0x0);  \/\/ No Scale\n+    disp($reg);  \/\/ Stack Offset\n+  %}\n+%}\n+\n+\/\/----------Conditional Branch Operands----------------------------------------\n+\/\/ Comparison Op  - This is the operation of the comparison, and is limited to\n+\/\/                  the following set of codes:\n+\/\/                  L (<), LE (<=), G (>), GE (>=), E (==), NE (!=)\n+\/\/\n+\/\/ Other attributes of the comparison, such as unsignedness, are specified\n+\/\/ by the comparison instruction that sets a condition code flags register.\n+\/\/ That result is represented by a flags operand whose subtype is appropriate\n+\/\/ to the unsignedness (etc.) of the comparison.\n+\/\/\n+\/\/ Later, the instruction which matches both the Comparison Op (a Bool) and\n+\/\/ the flags (produced by the Cmp) specifies the coding of the comparison op\n+\/\/ by matching a specific subtype of Bool operand below, such as cmpOpU.\n+\n+\/\/ Comparison Code\n+operand cmpOp()\n+%{\n+  match(Bool);\n+\n+  format %{ \"\" %}\n+  interface(COND_INTER) %{\n+    equal(0x4, \"e\");\n+    not_equal(0x5, \"ne\");\n+    less(0xC, \"l\");\n+    greater_equal(0xD, \"ge\");\n+    less_equal(0xE, \"le\");\n+    greater(0xF, \"g\");\n+    overflow(0x0, \"o\");\n+    no_overflow(0x1, \"no\");\n+  %}\n+%}\n+\n+\/\/ Comparison Code, unsigned compare.  Used by FP also, with\n+\/\/ C2 (unordered) turned into GT or LT already.  The other bits\n+\/\/ C0 and C3 are turned into Carry & Zero flags.\n+operand cmpOpU()\n+%{\n+  match(Bool);\n+\n+  format %{ \"\" %}\n+  interface(COND_INTER) %{\n+    equal(0x4, \"e\");\n+    not_equal(0x5, \"ne\");\n+    less(0x2, \"b\");\n+    greater_equal(0x3, \"ae\");\n+    less_equal(0x6, \"be\");\n+    greater(0x7, \"a\");\n+    overflow(0x0, \"o\");\n+    no_overflow(0x1, \"no\");\n+  %}\n+%}\n+\n+\n+\/\/ Floating comparisons that don't require any fixup for the unordered case,\n+\/\/ If both inputs of the comparison are the same, ZF is always set so we\n+\/\/ don't need to use cmpOpUCF2 for eq\/ne\n+operand cmpOpUCF() %{\n+  match(Bool);\n+  predicate(n->as_Bool()->_test._test == BoolTest::lt ||\n+            n->as_Bool()->_test._test == BoolTest::ge ||\n+            n->as_Bool()->_test._test == BoolTest::le ||\n+            n->as_Bool()->_test._test == BoolTest::gt ||\n+            n->in(1)->in(1) == n->in(1)->in(2));\n+  format %{ \"\" %}\n+  interface(COND_INTER) %{\n+    equal(0xb, \"np\");\n+    not_equal(0xa, \"p\");\n+    less(0x2, \"b\");\n+    greater_equal(0x3, \"ae\");\n+    less_equal(0x6, \"be\");\n+    greater(0x7, \"a\");\n+    overflow(0x0, \"o\");\n+    no_overflow(0x1, \"no\");\n+  %}\n+%}\n+\n+\n+\/\/ Floating comparisons that can be fixed up with extra conditional jumps\n+operand cmpOpUCF2() %{\n+  match(Bool);\n+  predicate((n->as_Bool()->_test._test == BoolTest::ne ||\n+             n->as_Bool()->_test._test == BoolTest::eq) &&\n+            n->in(1)->in(1) != n->in(1)->in(2));\n+  format %{ \"\" %}\n+  interface(COND_INTER) %{\n+    equal(0x4, \"e\");\n+    not_equal(0x5, \"ne\");\n+    less(0x2, \"b\");\n+    greater_equal(0x3, \"ae\");\n+    less_equal(0x6, \"be\");\n+    greater(0x7, \"a\");\n+    overflow(0x0, \"o\");\n+    no_overflow(0x1, \"no\");\n+  %}\n+%}\n+\n+\/\/ Operands for bound floating pointer register arguments\n+operand rxmm0() %{\n+  constraint(ALLOC_IN_RC(xmm0_reg));\n+  match(VecX);\n+  format%{%}\n+  interface(REG_INTER);\n+%}\n+\n+\/\/ Vectors\n+\n+\/\/ Dummy generic vector class. Should be used for all vector operands.\n+\/\/ Replaced with vec[SDXYZ] during post-selection pass.\n+operand vec() %{\n+  constraint(ALLOC_IN_RC(dynamic));\n+  match(VecX);\n+  match(VecY);\n+  match(VecZ);\n+  match(VecS);\n+  match(VecD);\n+\n+  format %{ %}\n+  interface(REG_INTER);\n+%}\n+\n+\/\/ Dummy generic legacy vector class. Should be used for all legacy vector operands.\n+\/\/ Replaced with legVec[SDXYZ] during post-selection cleanup.\n+\/\/ Note: legacy register class is used to avoid extra (unneeded in 32-bit VM)\n+\/\/ runtime code generation via reg_class_dynamic.\n+operand legVec() %{\n+  constraint(ALLOC_IN_RC(dynamic));\n+  match(VecX);\n+  match(VecY);\n+  match(VecZ);\n+  match(VecS);\n+  match(VecD);\n+\n+  format %{ %}\n+  interface(REG_INTER);\n+%}\n+\n+\/\/ Replaces vec during post-selection cleanup. See above.\n+operand vecS() %{\n+  constraint(ALLOC_IN_RC(vectors_reg_vlbwdq));\n+  match(VecS);\n+\n+  format %{ %}\n+  interface(REG_INTER);\n+%}\n+\n+\/\/ Replaces legVec during post-selection cleanup. See above.\n+operand legVecS() %{\n+  constraint(ALLOC_IN_RC(vectors_reg_legacy));\n+  match(VecS);\n+\n+  format %{ %}\n+  interface(REG_INTER);\n+%}\n+\n+\/\/ Replaces vec during post-selection cleanup. See above.\n+operand vecD() %{\n+  constraint(ALLOC_IN_RC(vectord_reg_vlbwdq));\n+  match(VecD);\n+\n+  format %{ %}\n+  interface(REG_INTER);\n+%}\n+\n+\/\/ Replaces legVec during post-selection cleanup. See above.\n+operand legVecD() %{\n+  constraint(ALLOC_IN_RC(vectord_reg_legacy));\n+  match(VecD);\n+\n+  format %{ %}\n+  interface(REG_INTER);\n+%}\n+\n+\/\/ Replaces vec during post-selection cleanup. See above.\n+operand vecX() %{\n+  constraint(ALLOC_IN_RC(vectorx_reg_vlbwdq));\n+  match(VecX);\n+\n+  format %{ %}\n+  interface(REG_INTER);\n+%}\n+\n+\/\/ Replaces legVec during post-selection cleanup. See above.\n+operand legVecX() %{\n+  constraint(ALLOC_IN_RC(vectorx_reg_legacy));\n+  match(VecX);\n+\n+  format %{ %}\n+  interface(REG_INTER);\n+%}\n+\n+\/\/ Replaces vec during post-selection cleanup. See above.\n+operand vecY() %{\n+  constraint(ALLOC_IN_RC(vectory_reg_vlbwdq));\n+  match(VecY);\n+\n+  format %{ %}\n+  interface(REG_INTER);\n+%}\n+\n+\/\/ Replaces legVec during post-selection cleanup. See above.\n+operand legVecY() %{\n+  constraint(ALLOC_IN_RC(vectory_reg_legacy));\n+  match(VecY);\n+\n+  format %{ %}\n+  interface(REG_INTER);\n+%}\n+\n+\/\/ Replaces vec during post-selection cleanup. See above.\n+operand vecZ() %{\n+  constraint(ALLOC_IN_RC(vectorz_reg));\n+  match(VecZ);\n+\n+  format %{ %}\n+  interface(REG_INTER);\n+%}\n+\n+\/\/ Replaces legVec during post-selection cleanup. See above.\n+operand legVecZ() %{\n+  constraint(ALLOC_IN_RC(vectorz_reg_legacy));\n+  match(VecZ);\n+\n+  format %{ %}\n+  interface(REG_INTER);\n+%}\n+\n+\/\/----------OPERAND CLASSES----------------------------------------------------\n+\/\/ Operand Classes are groups of operands that are used as to simplify\n+\/\/ instruction definitions by not requiring the AD writer to specify separate\n+\/\/ instructions for every form of operand when the instruction accepts\n+\/\/ multiple operand types with the same basic encoding and format.  The classic\n+\/\/ case of this is memory operands.\n+\n+opclass memory(indirect, indOffset8, indOffset32, indIndexOffset, indIndex,\n+               indIndexScale, indPosIndexScale, indIndexScaleOffset, indPosIndexOffset, indPosIndexScaleOffset,\n+               indCompressedOopOffset,\n+               indirectNarrow, indOffset8Narrow, indOffset32Narrow,\n+               indIndexOffsetNarrow, indIndexNarrow, indIndexScaleNarrow,\n+               indIndexScaleOffsetNarrow, indPosIndexOffsetNarrow, indPosIndexScaleOffsetNarrow);\n+\n+\/\/----------PIPELINE-----------------------------------------------------------\n+\/\/ Rules which define the behavior of the target architectures pipeline.\n+pipeline %{\n+\n+\/\/----------ATTRIBUTES---------------------------------------------------------\n+attributes %{\n+  variable_size_instructions;        \/\/ Fixed size instructions\n+  max_instructions_per_bundle = 3;   \/\/ Up to 3 instructions per bundle\n+  instruction_unit_size = 1;         \/\/ An instruction is 1 bytes long\n+  instruction_fetch_unit_size = 16;  \/\/ The processor fetches one line\n+  instruction_fetch_units = 1;       \/\/ of 16 bytes\n+%}\n+\n+\/\/----------RESOURCES----------------------------------------------------------\n+\/\/ Resources are the functional units available to the machine\n+\n+\/\/ Generic P2\/P3 pipeline\n+\/\/ 3 decoders, only D0 handles big operands; a \"bundle\" is the limit of\n+\/\/ 3 instructions decoded per cycle.\n+\/\/ 2 load\/store ops per cycle, 1 branch, 1 FPU,\n+\/\/ 3 ALU op, only ALU0 handles mul instructions.\n+resources( D0, D1, D2, DECODE = D0 | D1 | D2,\n+           MS0, MS1, MS2, MEM = MS0 | MS1 | MS2,\n+           BR, FPU,\n+           ALU0, ALU1, ALU2, ALU = ALU0 | ALU1 | ALU2);\n+\n+\/\/----------PIPELINE DESCRIPTION-----------------------------------------------\n+\/\/ Pipeline Description specifies the stages in the machine's pipeline\n+\n+\/\/ Generic P2\/P3 pipeline\n+pipe_desc(S0, S1, S2, S3, S4, S5);\n+\n+\/\/----------PIPELINE CLASSES---------------------------------------------------\n+\/\/ Pipeline Classes describe the stages in which input and output are\n+\/\/ referenced by the hardware pipeline.\n+\n+\/\/ Naming convention: ialu or fpu\n+\/\/ Then: _reg\n+\/\/ Then: _reg if there is a 2nd register\n+\/\/ Then: _long if it's a pair of instructions implementing a long\n+\/\/ Then: _fat if it requires the big decoder\n+\/\/   Or: _mem if it requires the big decoder and a memory unit.\n+\n+\/\/ Integer ALU reg operation\n+pipe_class ialu_reg(rRegI dst)\n+%{\n+    single_instruction;\n+    dst    : S4(write);\n+    dst    : S3(read);\n+    DECODE : S0;        \/\/ any decoder\n+    ALU    : S3;        \/\/ any alu\n+%}\n+\n+\/\/ Long ALU reg operation\n+pipe_class ialu_reg_long(rRegL dst)\n+%{\n+    instruction_count(2);\n+    dst    : S4(write);\n+    dst    : S3(read);\n+    DECODE : S0(2);     \/\/ any 2 decoders\n+    ALU    : S3(2);     \/\/ both alus\n+%}\n+\n+\/\/ Integer ALU reg operation using big decoder\n+pipe_class ialu_reg_fat(rRegI dst)\n+%{\n+    single_instruction;\n+    dst    : S4(write);\n+    dst    : S3(read);\n+    D0     : S0;        \/\/ big decoder only\n+    ALU    : S3;        \/\/ any alu\n+%}\n+\n+\/\/ Integer ALU reg-reg operation\n+pipe_class ialu_reg_reg(rRegI dst, rRegI src)\n+%{\n+    single_instruction;\n+    dst    : S4(write);\n+    src    : S3(read);\n+    DECODE : S0;        \/\/ any decoder\n+    ALU    : S3;        \/\/ any alu\n+%}\n+\n+\/\/ Integer ALU reg-reg operation\n+pipe_class ialu_reg_reg_fat(rRegI dst, memory src)\n+%{\n+    single_instruction;\n+    dst    : S4(write);\n+    src    : S3(read);\n+    D0     : S0;        \/\/ big decoder only\n+    ALU    : S3;        \/\/ any alu\n+%}\n+\n+\/\/ Integer ALU reg-mem operation\n+pipe_class ialu_reg_mem(rRegI dst, memory mem)\n+%{\n+    single_instruction;\n+    dst    : S5(write);\n+    mem    : S3(read);\n+    D0     : S0;        \/\/ big decoder only\n+    ALU    : S4;        \/\/ any alu\n+    MEM    : S3;        \/\/ any mem\n+%}\n+\n+\/\/ Integer mem operation (prefetch)\n+pipe_class ialu_mem(memory mem)\n+%{\n+    single_instruction;\n+    mem    : S3(read);\n+    D0     : S0;        \/\/ big decoder only\n+    MEM    : S3;        \/\/ any mem\n+%}\n+\n+\/\/ Integer Store to Memory\n+pipe_class ialu_mem_reg(memory mem, rRegI src)\n+%{\n+    single_instruction;\n+    mem    : S3(read);\n+    src    : S5(read);\n+    D0     : S0;        \/\/ big decoder only\n+    ALU    : S4;        \/\/ any alu\n+    MEM    : S3;\n+%}\n+\n+\/\/ \/\/ Long Store to Memory\n+\/\/ pipe_class ialu_mem_long_reg(memory mem, rRegL src)\n+\/\/ %{\n+\/\/     instruction_count(2);\n+\/\/     mem    : S3(read);\n+\/\/     src    : S5(read);\n+\/\/     D0     : S0(2);          \/\/ big decoder only; twice\n+\/\/     ALU    : S4(2);     \/\/ any 2 alus\n+\/\/     MEM    : S3(2);  \/\/ Both mems\n+\/\/ %}\n+\n+\/\/ Integer Store to Memory\n+pipe_class ialu_mem_imm(memory mem)\n+%{\n+    single_instruction;\n+    mem    : S3(read);\n+    D0     : S0;        \/\/ big decoder only\n+    ALU    : S4;        \/\/ any alu\n+    MEM    : S3;\n+%}\n+\n+\/\/ Integer ALU0 reg-reg operation\n+pipe_class ialu_reg_reg_alu0(rRegI dst, rRegI src)\n+%{\n+    single_instruction;\n+    dst    : S4(write);\n+    src    : S3(read);\n+    D0     : S0;        \/\/ Big decoder only\n+    ALU0   : S3;        \/\/ only alu0\n+%}\n+\n+\/\/ Integer ALU0 reg-mem operation\n+pipe_class ialu_reg_mem_alu0(rRegI dst, memory mem)\n+%{\n+    single_instruction;\n+    dst    : S5(write);\n+    mem    : S3(read);\n+    D0     : S0;        \/\/ big decoder only\n+    ALU0   : S4;        \/\/ ALU0 only\n+    MEM    : S3;        \/\/ any mem\n+%}\n+\n+\/\/ Integer ALU reg-reg operation\n+pipe_class ialu_cr_reg_reg(rFlagsReg cr, rRegI src1, rRegI src2)\n+%{\n+    single_instruction;\n+    cr     : S4(write);\n+    src1   : S3(read);\n+    src2   : S3(read);\n+    DECODE : S0;        \/\/ any decoder\n+    ALU    : S3;        \/\/ any alu\n+%}\n+\n+\/\/ Integer ALU reg-imm operation\n+pipe_class ialu_cr_reg_imm(rFlagsReg cr, rRegI src1)\n+%{\n+    single_instruction;\n+    cr     : S4(write);\n+    src1   : S3(read);\n+    DECODE : S0;        \/\/ any decoder\n+    ALU    : S3;        \/\/ any alu\n+%}\n+\n+\/\/ Integer ALU reg-mem operation\n+pipe_class ialu_cr_reg_mem(rFlagsReg cr, rRegI src1, memory src2)\n+%{\n+    single_instruction;\n+    cr     : S4(write);\n+    src1   : S3(read);\n+    src2   : S3(read);\n+    D0     : S0;        \/\/ big decoder only\n+    ALU    : S4;        \/\/ any alu\n+    MEM    : S3;\n+%}\n+\n+\/\/ Conditional move reg-reg\n+pipe_class pipe_cmplt( rRegI p, rRegI q, rRegI y)\n+%{\n+    instruction_count(4);\n+    y      : S4(read);\n+    q      : S3(read);\n+    p      : S3(read);\n+    DECODE : S0(4);     \/\/ any decoder\n+%}\n+\n+\/\/ Conditional move reg-reg\n+pipe_class pipe_cmov_reg( rRegI dst, rRegI src, rFlagsReg cr)\n+%{\n+    single_instruction;\n+    dst    : S4(write);\n+    src    : S3(read);\n+    cr     : S3(read);\n+    DECODE : S0;        \/\/ any decoder\n+%}\n+\n+\/\/ Conditional move reg-mem\n+pipe_class pipe_cmov_mem( rFlagsReg cr, rRegI dst, memory src)\n+%{\n+    single_instruction;\n+    dst    : S4(write);\n+    src    : S3(read);\n+    cr     : S3(read);\n+    DECODE : S0;        \/\/ any decoder\n+    MEM    : S3;\n+%}\n+\n+\/\/ Conditional move reg-reg long\n+pipe_class pipe_cmov_reg_long( rFlagsReg cr, rRegL dst, rRegL src)\n+%{\n+    single_instruction;\n+    dst    : S4(write);\n+    src    : S3(read);\n+    cr     : S3(read);\n+    DECODE : S0(2);     \/\/ any 2 decoders\n+%}\n+\n+\/\/ Float reg-reg operation\n+pipe_class fpu_reg(regD dst)\n+%{\n+    instruction_count(2);\n+    dst    : S3(read);\n+    DECODE : S0(2);     \/\/ any 2 decoders\n+    FPU    : S3;\n+%}\n+\n+\/\/ Float reg-reg operation\n+pipe_class fpu_reg_reg(regD dst, regD src)\n+%{\n+    instruction_count(2);\n+    dst    : S4(write);\n+    src    : S3(read);\n+    DECODE : S0(2);     \/\/ any 2 decoders\n+    FPU    : S3;\n+%}\n+\n+\/\/ Float reg-reg operation\n+pipe_class fpu_reg_reg_reg(regD dst, regD src1, regD src2)\n+%{\n+    instruction_count(3);\n+    dst    : S4(write);\n+    src1   : S3(read);\n+    src2   : S3(read);\n+    DECODE : S0(3);     \/\/ any 3 decoders\n+    FPU    : S3(2);\n+%}\n+\n+\/\/ Float reg-reg operation\n+pipe_class fpu_reg_reg_reg_reg(regD dst, regD src1, regD src2, regD src3)\n+%{\n+    instruction_count(4);\n+    dst    : S4(write);\n+    src1   : S3(read);\n+    src2   : S3(read);\n+    src3   : S3(read);\n+    DECODE : S0(4);     \/\/ any 3 decoders\n+    FPU    : S3(2);\n+%}\n+\n+\/\/ Float reg-reg operation\n+pipe_class fpu_reg_mem_reg_reg(regD dst, memory src1, regD src2, regD src3)\n+%{\n+    instruction_count(4);\n+    dst    : S4(write);\n+    src1   : S3(read);\n+    src2   : S3(read);\n+    src3   : S3(read);\n+    DECODE : S1(3);     \/\/ any 3 decoders\n+    D0     : S0;        \/\/ Big decoder only\n+    FPU    : S3(2);\n+    MEM    : S3;\n+%}\n+\n+\/\/ Float reg-mem operation\n+pipe_class fpu_reg_mem(regD dst, memory mem)\n+%{\n+    instruction_count(2);\n+    dst    : S5(write);\n+    mem    : S3(read);\n+    D0     : S0;        \/\/ big decoder only\n+    DECODE : S1;        \/\/ any decoder for FPU POP\n+    FPU    : S4;\n+    MEM    : S3;        \/\/ any mem\n+%}\n+\n+\/\/ Float reg-mem operation\n+pipe_class fpu_reg_reg_mem(regD dst, regD src1, memory mem)\n+%{\n+    instruction_count(3);\n+    dst    : S5(write);\n+    src1   : S3(read);\n+    mem    : S3(read);\n+    D0     : S0;        \/\/ big decoder only\n+    DECODE : S1(2);     \/\/ any decoder for FPU POP\n+    FPU    : S4;\n+    MEM    : S3;        \/\/ any mem\n+%}\n+\n+\/\/ Float mem-reg operation\n+pipe_class fpu_mem_reg(memory mem, regD src)\n+%{\n+    instruction_count(2);\n+    src    : S5(read);\n+    mem    : S3(read);\n+    DECODE : S0;        \/\/ any decoder for FPU PUSH\n+    D0     : S1;        \/\/ big decoder only\n+    FPU    : S4;\n+    MEM    : S3;        \/\/ any mem\n+%}\n+\n+pipe_class fpu_mem_reg_reg(memory mem, regD src1, regD src2)\n+%{\n+    instruction_count(3);\n+    src1   : S3(read);\n+    src2   : S3(read);\n+    mem    : S3(read);\n+    DECODE : S0(2);     \/\/ any decoder for FPU PUSH\n+    D0     : S1;        \/\/ big decoder only\n+    FPU    : S4;\n+    MEM    : S3;        \/\/ any mem\n+%}\n+\n+pipe_class fpu_mem_reg_mem(memory mem, regD src1, memory src2)\n+%{\n+    instruction_count(3);\n+    src1   : S3(read);\n+    src2   : S3(read);\n+    mem    : S4(read);\n+    DECODE : S0;        \/\/ any decoder for FPU PUSH\n+    D0     : S0(2);     \/\/ big decoder only\n+    FPU    : S4;\n+    MEM    : S3(2);     \/\/ any mem\n+%}\n+\n+pipe_class fpu_mem_mem(memory dst, memory src1)\n+%{\n+    instruction_count(2);\n+    src1   : S3(read);\n+    dst    : S4(read);\n+    D0     : S0(2);     \/\/ big decoder only\n+    MEM    : S3(2);     \/\/ any mem\n+%}\n+\n+pipe_class fpu_mem_mem_mem(memory dst, memory src1, memory src2)\n+%{\n+    instruction_count(3);\n+    src1   : S3(read);\n+    src2   : S3(read);\n+    dst    : S4(read);\n+    D0     : S0(3);     \/\/ big decoder only\n+    FPU    : S4;\n+    MEM    : S3(3);     \/\/ any mem\n+%}\n+\n+pipe_class fpu_mem_reg_con(memory mem, regD src1)\n+%{\n+    instruction_count(3);\n+    src1   : S4(read);\n+    mem    : S4(read);\n+    DECODE : S0;        \/\/ any decoder for FPU PUSH\n+    D0     : S0(2);     \/\/ big decoder only\n+    FPU    : S4;\n+    MEM    : S3(2);     \/\/ any mem\n+%}\n+\n+\/\/ Float load constant\n+pipe_class fpu_reg_con(regD dst)\n+%{\n+    instruction_count(2);\n+    dst    : S5(write);\n+    D0     : S0;        \/\/ big decoder only for the load\n+    DECODE : S1;        \/\/ any decoder for FPU POP\n+    FPU    : S4;\n+    MEM    : S3;        \/\/ any mem\n+%}\n+\n+\/\/ Float load constant\n+pipe_class fpu_reg_reg_con(regD dst, regD src)\n+%{\n+    instruction_count(3);\n+    dst    : S5(write);\n+    src    : S3(read);\n+    D0     : S0;        \/\/ big decoder only for the load\n+    DECODE : S1(2);     \/\/ any decoder for FPU POP\n+    FPU    : S4;\n+    MEM    : S3;        \/\/ any mem\n+%}\n+\n+\/\/ UnConditional branch\n+pipe_class pipe_jmp(label labl)\n+%{\n+    single_instruction;\n+    BR   : S3;\n+%}\n+\n+\/\/ Conditional branch\n+pipe_class pipe_jcc(cmpOp cmp, rFlagsReg cr, label labl)\n+%{\n+    single_instruction;\n+    cr    : S1(read);\n+    BR    : S3;\n+%}\n+\n+\/\/ Allocation idiom\n+pipe_class pipe_cmpxchg(rRegP dst, rRegP heap_ptr)\n+%{\n+    instruction_count(1); force_serialization;\n+    fixed_latency(6);\n+    heap_ptr : S3(read);\n+    DECODE   : S0(3);\n+    D0       : S2;\n+    MEM      : S3;\n+    ALU      : S3(2);\n+    dst      : S5(write);\n+    BR       : S5;\n+%}\n+\n+\/\/ Generic big\/slow expanded idiom\n+pipe_class pipe_slow()\n+%{\n+    instruction_count(10); multiple_bundles; force_serialization;\n+    fixed_latency(100);\n+    D0  : S0(2);\n+    MEM : S3(2);\n+%}\n+\n+\/\/ The real do-nothing guy\n+pipe_class empty()\n+%{\n+    instruction_count(0);\n+%}\n+\n+\/\/ Define the class for the Nop node\n+define\n+%{\n+   MachNop = empty;\n+%}\n+\n+%}\n+\n+\/\/----------INSTRUCTIONS-------------------------------------------------------\n+\/\/\n+\/\/ match      -- States which machine-independent subtree may be replaced\n+\/\/               by this instruction.\n+\/\/ ins_cost   -- The estimated cost of this instruction is used by instruction\n+\/\/               selection to identify a minimum cost tree of machine\n+\/\/               instructions that matches a tree of machine-independent\n+\/\/               instructions.\n+\/\/ format     -- A string providing the disassembly for this instruction.\n+\/\/               The value of an instruction's operand may be inserted\n+\/\/               by referring to it with a '$' prefix.\n+\/\/ opcode     -- Three instruction opcodes may be provided.  These are referred\n+\/\/               to within an encode class as $primary, $secondary, and $tertiary\n+\/\/               rrspectively.  The primary opcode is commonly used to\n+\/\/               indicate the type of machine instruction, while secondary\n+\/\/               and tertiary are often used for prefix options or addressing\n+\/\/               modes.\n+\/\/ ins_encode -- A list of encode classes with parameters. The encode class\n+\/\/               name must have been defined in an 'enc_class' specification\n+\/\/               in the encode section of the architecture description.\n+\n+\/\/ ============================================================================\n+\n+instruct ShouldNotReachHere() %{\n+  match(Halt);\n+  format %{ \"stop\\t# ShouldNotReachHere\" %}\n+  ins_encode %{\n+    if (is_reachable()) {\n+      const char* str = __ code_string(_halt_reason);\n+      __ stop(str);\n+    }\n+  %}\n+  ins_pipe(pipe_slow);\n+%}\n+\n+\/\/ ============================================================================\n+\n+\/\/ Dummy reg-to-reg vector moves. Removed during post-selection cleanup.\n+\/\/ Load Float\n+instruct MoveF2VL(vlRegF dst, regF src) %{\n+  match(Set dst src);\n+  format %{ \"movss $dst,$src\\t! load float (4 bytes)\" %}\n+  ins_encode %{\n+    ShouldNotReachHere();\n+  %}\n+  ins_pipe( fpu_reg_reg );\n+%}\n+\n+\/\/ Load Float\n+instruct MoveF2LEG(legRegF dst, regF src) %{\n+  match(Set dst src);\n+  format %{ \"movss $dst,$src\\t# if src != dst load float (4 bytes)\" %}\n+  ins_encode %{\n+    ShouldNotReachHere();\n+  %}\n+  ins_pipe( fpu_reg_reg );\n+%}\n+\n+\/\/ Load Float\n+instruct MoveVL2F(regF dst, vlRegF src) %{\n+  match(Set dst src);\n+  format %{ \"movss $dst,$src\\t! load float (4 bytes)\" %}\n+  ins_encode %{\n+    ShouldNotReachHere();\n+  %}\n+  ins_pipe( fpu_reg_reg );\n+%}\n+\n+\/\/ Load Float\n+instruct MoveLEG2F(regF dst, legRegF src) %{\n+  match(Set dst src);\n+  format %{ \"movss $dst,$src\\t# if src != dst load float (4 bytes)\" %}\n+  ins_encode %{\n+    ShouldNotReachHere();\n+  %}\n+  ins_pipe( fpu_reg_reg );\n+%}\n+\n+\/\/ Load Double\n+instruct MoveD2VL(vlRegD dst, regD src) %{\n+  match(Set dst src);\n+  format %{ \"movsd $dst,$src\\t! load double (8 bytes)\" %}\n+  ins_encode %{\n+    ShouldNotReachHere();\n+  %}\n+  ins_pipe( fpu_reg_reg );\n+%}\n+\n+\/\/ Load Double\n+instruct MoveD2LEG(legRegD dst, regD src) %{\n+  match(Set dst src);\n+  format %{ \"movsd $dst,$src\\t# if src != dst load double (8 bytes)\" %}\n+  ins_encode %{\n+    ShouldNotReachHere();\n+  %}\n+  ins_pipe( fpu_reg_reg );\n+%}\n+\n+\/\/ Load Double\n+instruct MoveVL2D(regD dst, vlRegD src) %{\n+  match(Set dst src);\n+  format %{ \"movsd $dst,$src\\t! load double (8 bytes)\" %}\n+  ins_encode %{\n+    ShouldNotReachHere();\n+  %}\n+  ins_pipe( fpu_reg_reg );\n+%}\n+\n+\/\/ Load Double\n+instruct MoveLEG2D(regD dst, legRegD src) %{\n+  match(Set dst src);\n+  format %{ \"movsd $dst,$src\\t# if src != dst load double (8 bytes)\" %}\n+  ins_encode %{\n+    ShouldNotReachHere();\n+  %}\n+  ins_pipe( fpu_reg_reg );\n+%}\n+\n+\/\/----------Load\/Store\/Move Instructions---------------------------------------\n+\/\/----------Load Instructions--------------------------------------------------\n+\n+\/\/ Load Byte (8 bit signed)\n+instruct loadB(rRegI dst, memory mem)\n+%{\n+  match(Set dst (LoadB mem));\n+\n+  ins_cost(125);\n+  format %{ \"movsbl  $dst, $mem\\t# byte\" %}\n+\n+  ins_encode %{\n+    __ movsbl($dst$$Register, $mem$$Address);\n+  %}\n+\n+  ins_pipe(ialu_reg_mem);\n+%}\n+\n+\/\/ Load Byte (8 bit signed) into Long Register\n+instruct loadB2L(rRegL dst, memory mem)\n+%{\n+  match(Set dst (ConvI2L (LoadB mem)));\n+\n+  ins_cost(125);\n+  format %{ \"movsbq  $dst, $mem\\t# byte -> long\" %}\n+\n+  ins_encode %{\n+    __ movsbq($dst$$Register, $mem$$Address);\n+  %}\n+\n+  ins_pipe(ialu_reg_mem);\n+%}\n+\n+\/\/ Load Unsigned Byte (8 bit UNsigned)\n+instruct loadUB(rRegI dst, memory mem)\n+%{\n+  match(Set dst (LoadUB mem));\n+\n+  ins_cost(125);\n+  format %{ \"movzbl  $dst, $mem\\t# ubyte\" %}\n+\n+  ins_encode %{\n+    __ movzbl($dst$$Register, $mem$$Address);\n+  %}\n+\n+  ins_pipe(ialu_reg_mem);\n+%}\n+\n+\/\/ Load Unsigned Byte (8 bit UNsigned) into Long Register\n+instruct loadUB2L(rRegL dst, memory mem)\n+%{\n+  match(Set dst (ConvI2L (LoadUB mem)));\n+\n+  ins_cost(125);\n+  format %{ \"movzbq  $dst, $mem\\t# ubyte -> long\" %}\n+\n+  ins_encode %{\n+    __ movzbq($dst$$Register, $mem$$Address);\n+  %}\n+\n+  ins_pipe(ialu_reg_mem);\n+%}\n+\n+\/\/ Load Unsigned Byte (8 bit UNsigned) with 32-bit mask into Long Register\n+instruct loadUB2L_immI(rRegL dst, memory mem, immI mask, rFlagsReg cr) %{\n+  match(Set dst (ConvI2L (AndI (LoadUB mem) mask)));\n+  effect(KILL cr);\n+\n+  format %{ \"movzbq  $dst, $mem\\t# ubyte & 32-bit mask -> long\\n\\t\"\n+            \"andl    $dst, right_n_bits($mask, 8)\" %}\n+  ins_encode %{\n+    Register Rdst = $dst$$Register;\n+    __ movzbq(Rdst, $mem$$Address);\n+    __ andl(Rdst, $mask$$constant & right_n_bits(8));\n+  %}\n+  ins_pipe(ialu_reg_mem);\n+%}\n+\n+\/\/ Load Short (16 bit signed)\n+instruct loadS(rRegI dst, memory mem)\n+%{\n+  match(Set dst (LoadS mem));\n+\n+  ins_cost(125);\n+  format %{ \"movswl $dst, $mem\\t# short\" %}\n+\n+  ins_encode %{\n+    __ movswl($dst$$Register, $mem$$Address);\n+  %}\n+\n+  ins_pipe(ialu_reg_mem);\n+%}\n+\n+\/\/ Load Short (16 bit signed) to Byte (8 bit signed)\n+instruct loadS2B(rRegI dst, memory mem, immI_24 twentyfour) %{\n+  match(Set dst (RShiftI (LShiftI (LoadS mem) twentyfour) twentyfour));\n+\n+  ins_cost(125);\n+  format %{ \"movsbl $dst, $mem\\t# short -> byte\" %}\n+  ins_encode %{\n+    __ movsbl($dst$$Register, $mem$$Address);\n+  %}\n+  ins_pipe(ialu_reg_mem);\n+%}\n+\n+\/\/ Load Short (16 bit signed) into Long Register\n+instruct loadS2L(rRegL dst, memory mem)\n+%{\n+  match(Set dst (ConvI2L (LoadS mem)));\n+\n+  ins_cost(125);\n+  format %{ \"movswq $dst, $mem\\t# short -> long\" %}\n+\n+  ins_encode %{\n+    __ movswq($dst$$Register, $mem$$Address);\n+  %}\n+\n+  ins_pipe(ialu_reg_mem);\n+%}\n+\n+\/\/ Load Unsigned Short\/Char (16 bit UNsigned)\n+instruct loadUS(rRegI dst, memory mem)\n+%{\n+  match(Set dst (LoadUS mem));\n+\n+  ins_cost(125);\n+  format %{ \"movzwl  $dst, $mem\\t# ushort\/char\" %}\n+\n+  ins_encode %{\n+    __ movzwl($dst$$Register, $mem$$Address);\n+  %}\n+\n+  ins_pipe(ialu_reg_mem);\n+%}\n+\n+\/\/ Load Unsigned Short\/Char (16 bit UNsigned) to Byte (8 bit signed)\n+instruct loadUS2B(rRegI dst, memory mem, immI_24 twentyfour) %{\n+  match(Set dst (RShiftI (LShiftI (LoadUS mem) twentyfour) twentyfour));\n+\n+  ins_cost(125);\n+  format %{ \"movsbl $dst, $mem\\t# ushort -> byte\" %}\n+  ins_encode %{\n+    __ movsbl($dst$$Register, $mem$$Address);\n+  %}\n+  ins_pipe(ialu_reg_mem);\n+%}\n+\n+\/\/ Load Unsigned Short\/Char (16 bit UNsigned) into Long Register\n+instruct loadUS2L(rRegL dst, memory mem)\n+%{\n+  match(Set dst (ConvI2L (LoadUS mem)));\n+\n+  ins_cost(125);\n+  format %{ \"movzwq  $dst, $mem\\t# ushort\/char -> long\" %}\n+\n+  ins_encode %{\n+    __ movzwq($dst$$Register, $mem$$Address);\n+  %}\n+\n+  ins_pipe(ialu_reg_mem);\n+%}\n+\n+\/\/ Load Unsigned Short\/Char (16 bit UNsigned) with mask 0xFF into Long Register\n+instruct loadUS2L_immI_255(rRegL dst, memory mem, immI_255 mask) %{\n+  match(Set dst (ConvI2L (AndI (LoadUS mem) mask)));\n+\n+  format %{ \"movzbq  $dst, $mem\\t# ushort\/char & 0xFF -> long\" %}\n+  ins_encode %{\n+    __ movzbq($dst$$Register, $mem$$Address);\n+  %}\n+  ins_pipe(ialu_reg_mem);\n+%}\n+\n+\/\/ Load Unsigned Short\/Char (16 bit UNsigned) with 32-bit mask into Long Register\n+instruct loadUS2L_immI(rRegL dst, memory mem, immI mask, rFlagsReg cr) %{\n+  match(Set dst (ConvI2L (AndI (LoadUS mem) mask)));\n+  effect(KILL cr);\n+\n+  format %{ \"movzwq  $dst, $mem\\t# ushort\/char & 32-bit mask -> long\\n\\t\"\n+            \"andl    $dst, right_n_bits($mask, 16)\" %}\n+  ins_encode %{\n+    Register Rdst = $dst$$Register;\n+    __ movzwq(Rdst, $mem$$Address);\n+    __ andl(Rdst, $mask$$constant & right_n_bits(16));\n+  %}\n+  ins_pipe(ialu_reg_mem);\n+%}\n+\n+\/\/ Load Integer\n+instruct loadI(rRegI dst, memory mem)\n+%{\n+  match(Set dst (LoadI mem));\n+\n+  ins_cost(125);\n+  format %{ \"movl    $dst, $mem\\t# int\" %}\n+\n+  ins_encode %{\n+    __ movl($dst$$Register, $mem$$Address);\n+  %}\n+\n+  ins_pipe(ialu_reg_mem);\n+%}\n+\n+\/\/ Load Integer (32 bit signed) to Byte (8 bit signed)\n+instruct loadI2B(rRegI dst, memory mem, immI_24 twentyfour) %{\n+  match(Set dst (RShiftI (LShiftI (LoadI mem) twentyfour) twentyfour));\n+\n+  ins_cost(125);\n+  format %{ \"movsbl  $dst, $mem\\t# int -> byte\" %}\n+  ins_encode %{\n+    __ movsbl($dst$$Register, $mem$$Address);\n+  %}\n+  ins_pipe(ialu_reg_mem);\n+%}\n+\n+\/\/ Load Integer (32 bit signed) to Unsigned Byte (8 bit UNsigned)\n+instruct loadI2UB(rRegI dst, memory mem, immI_255 mask) %{\n+  match(Set dst (AndI (LoadI mem) mask));\n+\n+  ins_cost(125);\n+  format %{ \"movzbl  $dst, $mem\\t# int -> ubyte\" %}\n+  ins_encode %{\n+    __ movzbl($dst$$Register, $mem$$Address);\n+  %}\n+  ins_pipe(ialu_reg_mem);\n+%}\n+\n+\/\/ Load Integer (32 bit signed) to Short (16 bit signed)\n+instruct loadI2S(rRegI dst, memory mem, immI_16 sixteen) %{\n+  match(Set dst (RShiftI (LShiftI (LoadI mem) sixteen) sixteen));\n+\n+  ins_cost(125);\n+  format %{ \"movswl  $dst, $mem\\t# int -> short\" %}\n+  ins_encode %{\n+    __ movswl($dst$$Register, $mem$$Address);\n+  %}\n+  ins_pipe(ialu_reg_mem);\n+%}\n+\n+\/\/ Load Integer (32 bit signed) to Unsigned Short\/Char (16 bit UNsigned)\n+instruct loadI2US(rRegI dst, memory mem, immI_65535 mask) %{\n+  match(Set dst (AndI (LoadI mem) mask));\n+\n+  ins_cost(125);\n+  format %{ \"movzwl  $dst, $mem\\t# int -> ushort\/char\" %}\n+  ins_encode %{\n+    __ movzwl($dst$$Register, $mem$$Address);\n+  %}\n+  ins_pipe(ialu_reg_mem);\n+%}\n+\n+\/\/ Load Integer into Long Register\n+instruct loadI2L(rRegL dst, memory mem)\n+%{\n+  match(Set dst (ConvI2L (LoadI mem)));\n+\n+  ins_cost(125);\n+  format %{ \"movslq  $dst, $mem\\t# int -> long\" %}\n+\n+  ins_encode %{\n+    __ movslq($dst$$Register, $mem$$Address);\n+  %}\n+\n+  ins_pipe(ialu_reg_mem);\n+%}\n+\n+\/\/ Load Integer with mask 0xFF into Long Register\n+instruct loadI2L_immI_255(rRegL dst, memory mem, immI_255 mask) %{\n+  match(Set dst (ConvI2L (AndI (LoadI mem) mask)));\n+\n+  format %{ \"movzbq  $dst, $mem\\t# int & 0xFF -> long\" %}\n+  ins_encode %{\n+    __ movzbq($dst$$Register, $mem$$Address);\n+  %}\n+  ins_pipe(ialu_reg_mem);\n+%}\n+\n+\/\/ Load Integer with mask 0xFFFF into Long Register\n+instruct loadI2L_immI_65535(rRegL dst, memory mem, immI_65535 mask) %{\n+  match(Set dst (ConvI2L (AndI (LoadI mem) mask)));\n+\n+  format %{ \"movzwq  $dst, $mem\\t# int & 0xFFFF -> long\" %}\n+  ins_encode %{\n+    __ movzwq($dst$$Register, $mem$$Address);\n+  %}\n+  ins_pipe(ialu_reg_mem);\n+%}\n+\n+\/\/ Load Integer with a 31-bit mask into Long Register\n+instruct loadI2L_immU31(rRegL dst, memory mem, immU31 mask, rFlagsReg cr) %{\n+  match(Set dst (ConvI2L (AndI (LoadI mem) mask)));\n+  effect(KILL cr);\n+\n+  format %{ \"movl    $dst, $mem\\t# int & 31-bit mask -> long\\n\\t\"\n+            \"andl    $dst, $mask\" %}\n+  ins_encode %{\n+    Register Rdst = $dst$$Register;\n+    __ movl(Rdst, $mem$$Address);\n+    __ andl(Rdst, $mask$$constant);\n+  %}\n+  ins_pipe(ialu_reg_mem);\n+%}\n+\n+\/\/ Load Unsigned Integer into Long Register\n+instruct loadUI2L(rRegL dst, memory mem, immL_32bits mask)\n+%{\n+  match(Set dst (AndL (ConvI2L (LoadI mem)) mask));\n+\n+  ins_cost(125);\n+  format %{ \"movl    $dst, $mem\\t# uint -> long\" %}\n+\n+  ins_encode %{\n+    __ movl($dst$$Register, $mem$$Address);\n+  %}\n+\n+  ins_pipe(ialu_reg_mem);\n+%}\n+\n+\/\/ Load Long\n+instruct loadL(rRegL dst, memory mem)\n+%{\n+  match(Set dst (LoadL mem));\n+\n+  ins_cost(125);\n+  format %{ \"movq    $dst, $mem\\t# long\" %}\n+\n+  ins_encode %{\n+    __ movq($dst$$Register, $mem$$Address);\n+  %}\n+\n+  ins_pipe(ialu_reg_mem); \/\/ XXX\n+%}\n+\n+\/\/ Load Range\n+instruct loadRange(rRegI dst, memory mem)\n+%{\n+  match(Set dst (LoadRange mem));\n+\n+  ins_cost(125); \/\/ XXX\n+  format %{ \"movl    $dst, $mem\\t# range\" %}\n+  ins_encode %{\n+    __ movl($dst$$Register, $mem$$Address);\n+  %}\n+  ins_pipe(ialu_reg_mem);\n+%}\n+\n+\/\/ Load Pointer\n+instruct loadP(rRegP dst, memory mem)\n+%{\n+  match(Set dst (LoadP mem));\n+  predicate(n->as_Load()->barrier_data() == 0);\n+\n+  ins_cost(125); \/\/ XXX\n+  format %{ \"movq    $dst, $mem\\t# ptr\" %}\n+  ins_encode %{\n+    __ movq($dst$$Register, $mem$$Address);\n+  %}\n+  ins_pipe(ialu_reg_mem); \/\/ XXX\n+%}\n+\n+\/\/ Load Compressed Pointer\n+instruct loadN(rRegN dst, memory mem)\n+%{\n+   predicate(n->as_Load()->barrier_data() == 0);\n+   match(Set dst (LoadN mem));\n+\n+   ins_cost(125); \/\/ XXX\n+   format %{ \"movl    $dst, $mem\\t# compressed ptr\" %}\n+   ins_encode %{\n+     __ movl($dst$$Register, $mem$$Address);\n+   %}\n+   ins_pipe(ialu_reg_mem); \/\/ XXX\n+%}\n+\n+\n+\/\/ Load Klass Pointer\n+instruct loadKlass(rRegP dst, memory mem)\n+%{\n+  match(Set dst (LoadKlass mem));\n+\n+  ins_cost(125); \/\/ XXX\n+  format %{ \"movq    $dst, $mem\\t# class\" %}\n+  ins_encode %{\n+    __ movq($dst$$Register, $mem$$Address);\n+  %}\n+  ins_pipe(ialu_reg_mem); \/\/ XXX\n+%}\n+\n+\/\/ Load narrow Klass Pointer\n+instruct loadNKlass(rRegN dst, memory mem)\n+%{\n+  predicate(!UseCompactObjectHeaders);\n+  match(Set dst (LoadNKlass mem));\n+\n+  ins_cost(125); \/\/ XXX\n+  format %{ \"movl    $dst, $mem\\t# compressed klass ptr\" %}\n+  ins_encode %{\n+    __ movl($dst$$Register, $mem$$Address);\n+  %}\n+  ins_pipe(ialu_reg_mem); \/\/ XXX\n+%}\n+\n+instruct loadNKlassCompactHeaders(rRegN dst, memory mem, rFlagsReg cr)\n+%{\n+  predicate(UseCompactObjectHeaders);\n+  match(Set dst (LoadNKlass mem));\n+  effect(KILL cr);\n+  ins_cost(125);\n+  format %{\n+    \"movl    $dst, $mem\\t# compressed klass ptr, shifted\\n\\t\"\n+    \"shrl    $dst, markWord::klass_shift\"\n+  %}\n+  ins_encode %{\n+    \/\/ The incoming address is pointing into obj-start + Type::klass_offset(). We need to extract\n+    \/\/ obj-start, so that we can load from the object's mark-word instead.\n+    Register d = $dst$$Register;\n+    Address  s = ($mem$$Address).plus_disp(-Type::klass_offset());\n+    if (UseAPX) {\n+      __ eshrl(d, s, markWord::klass_shift, false);\n+    } else {\n+      __ movl(d, s);\n+      __ shrl(d, markWord::klass_shift);\n+    }\n+  %}\n+  ins_pipe(ialu_reg_mem);\n+%}\n+\n+\/\/ Load Float\n+instruct loadF(regF dst, memory mem)\n+%{\n+  match(Set dst (LoadF mem));\n+\n+  ins_cost(145); \/\/ XXX\n+  format %{ \"movss   $dst, $mem\\t# float\" %}\n+  ins_encode %{\n+    __ movflt($dst$$XMMRegister, $mem$$Address);\n+  %}\n+  ins_pipe(pipe_slow); \/\/ XXX\n+%}\n+\n+\/\/ Load Double\n+instruct loadD_partial(regD dst, memory mem)\n+%{\n+  predicate(!UseXmmLoadAndClearUpper);\n+  match(Set dst (LoadD mem));\n+\n+  ins_cost(145); \/\/ XXX\n+  format %{ \"movlpd  $dst, $mem\\t# double\" %}\n+  ins_encode %{\n+    __ movdbl($dst$$XMMRegister, $mem$$Address);\n+  %}\n+  ins_pipe(pipe_slow); \/\/ XXX\n+%}\n+\n+instruct loadD(regD dst, memory mem)\n+%{\n+  predicate(UseXmmLoadAndClearUpper);\n+  match(Set dst (LoadD mem));\n+\n+  ins_cost(145); \/\/ XXX\n+  format %{ \"movsd   $dst, $mem\\t# double\" %}\n+  ins_encode %{\n+    __ movdbl($dst$$XMMRegister, $mem$$Address);\n+  %}\n+  ins_pipe(pipe_slow); \/\/ XXX\n+%}\n+\n+\/\/ max = java.lang.Math.max(float a, float b)\n+instruct maxF_avx10_reg(regF dst, regF a, regF b) %{\n+  predicate(VM_Version::supports_avx10_2());\n+  match(Set dst (MaxF a b));\n+  format %{ \"maxF $dst, $a, $b\" %}\n+  ins_encode %{\n+    __ eminmaxss($dst$$XMMRegister, $a$$XMMRegister, $b$$XMMRegister, AVX10_MINMAX_MAX_COMPARE_SIGN);\n+  %}\n+  ins_pipe( pipe_slow );\n+%}\n+\n+\/\/ max = java.lang.Math.max(float a, float b)\n+instruct maxF_reg(legRegF dst, legRegF a, legRegF b, legRegF tmp, legRegF atmp, legRegF btmp) %{\n+  predicate(!VM_Version::supports_avx10_2() && UseAVX > 0 && !VLoopReductions::is_reduction(n));\n+  match(Set dst (MaxF a b));\n+  effect(USE a, USE b, TEMP tmp, TEMP atmp, TEMP btmp);\n+  format %{ \"maxF $dst, $a, $b \\t! using $tmp, $atmp and $btmp as TEMP\" %}\n+  ins_encode %{\n+    __ vminmax_fp(Op_MaxV, T_FLOAT, $dst$$XMMRegister, $a$$XMMRegister, $b$$XMMRegister, $tmp$$XMMRegister, $atmp$$XMMRegister, $btmp$$XMMRegister, Assembler::AVX_128bit);\n+  %}\n+  ins_pipe( pipe_slow );\n+%}\n+\n+instruct maxF_reduction_reg(legRegF dst, legRegF a, legRegF b, legRegF xtmp, rRegI rtmp, rFlagsReg cr) %{\n+  predicate(!VM_Version::supports_avx10_2() && UseAVX > 0 && VLoopReductions::is_reduction(n));\n+  match(Set dst (MaxF a b));\n+  effect(USE a, USE b, TEMP xtmp, TEMP rtmp, KILL cr);\n+\n+  format %{ \"maxF_reduction $dst, $a, $b \\t!using $xtmp and $rtmp as TEMP\" %}\n+  ins_encode %{\n+    emit_fp_min_max(masm, $dst$$XMMRegister, $a$$XMMRegister, $b$$XMMRegister, $xtmp$$XMMRegister, $rtmp$$Register,\n+                    false \/*min*\/, true \/*single*\/);\n+  %}\n+  ins_pipe( pipe_slow );\n+%}\n+\n+\/\/ max = java.lang.Math.max(double a, double b)\n+instruct maxD_avx10_reg(regD dst, regD a, regD b) %{\n+  predicate(VM_Version::supports_avx10_2());\n+  match(Set dst (MaxD a b));\n+  format %{ \"maxD $dst, $a, $b\" %}\n+  ins_encode %{\n+    __ eminmaxsd($dst$$XMMRegister, $a$$XMMRegister, $b$$XMMRegister, AVX10_MINMAX_MAX_COMPARE_SIGN);\n+  %}\n+  ins_pipe( pipe_slow );\n+%}\n+\n+\/\/ max = java.lang.Math.max(double a, double b)\n+instruct maxD_reg(legRegD dst, legRegD a, legRegD b, legRegD tmp, legRegD atmp, legRegD btmp) %{\n+  predicate(!VM_Version::supports_avx10_2() && UseAVX > 0 && !VLoopReductions::is_reduction(n));\n+  match(Set dst (MaxD a b));\n+  effect(USE a, USE b, TEMP atmp, TEMP btmp, TEMP tmp);\n+  format %{ \"maxD $dst, $a, $b \\t! using $tmp, $atmp and $btmp as TEMP\" %}\n+  ins_encode %{\n+    __ vminmax_fp(Op_MaxV, T_DOUBLE, $dst$$XMMRegister, $a$$XMMRegister, $b$$XMMRegister, $tmp$$XMMRegister, $atmp$$XMMRegister, $btmp$$XMMRegister, Assembler::AVX_128bit);\n+  %}\n+  ins_pipe( pipe_slow );\n+%}\n+\n+instruct maxD_reduction_reg(legRegD dst, legRegD a, legRegD b, legRegD xtmp, rRegL rtmp, rFlagsReg cr) %{\n+  predicate(!VM_Version::supports_avx10_2() && UseAVX > 0 && VLoopReductions::is_reduction(n));\n+  match(Set dst (MaxD a b));\n+  effect(USE a, USE b, TEMP xtmp, TEMP rtmp, KILL cr);\n+\n+  format %{ \"maxD_reduction $dst, $a, $b \\t! using $xtmp and $rtmp as TEMP\" %}\n+  ins_encode %{\n+    emit_fp_min_max(masm, $dst$$XMMRegister, $a$$XMMRegister, $b$$XMMRegister, $xtmp$$XMMRegister, $rtmp$$Register,\n+                    false \/*min*\/, false \/*single*\/);\n+  %}\n+  ins_pipe( pipe_slow );\n+%}\n+\n+\/\/ max = java.lang.Math.min(float a, float b)\n+instruct minF_avx10_reg(regF dst, regF a, regF b) %{\n+  predicate(VM_Version::supports_avx10_2());\n+  match(Set dst (MinF a b));\n+  format %{ \"minF $dst, $a, $b\" %}\n+  ins_encode %{\n+    __ eminmaxss($dst$$XMMRegister, $a$$XMMRegister, $b$$XMMRegister, AVX10_MINMAX_MIN_COMPARE_SIGN);\n+  %}\n+  ins_pipe( pipe_slow );\n+%}\n+\n+\/\/ min = java.lang.Math.min(float a, float b)\n+instruct minF_reg(legRegF dst, legRegF a, legRegF b, legRegF tmp, legRegF atmp, legRegF btmp) %{\n+  predicate(!VM_Version::supports_avx10_2() && UseAVX > 0 && !VLoopReductions::is_reduction(n));\n+  match(Set dst (MinF a b));\n+  effect(USE a, USE b, TEMP tmp, TEMP atmp, TEMP btmp);\n+  format %{ \"minF $dst, $a, $b \\t! using $tmp, $atmp and $btmp as TEMP\" %}\n+  ins_encode %{\n+    __ vminmax_fp(Op_MinV, T_FLOAT, $dst$$XMMRegister, $a$$XMMRegister, $b$$XMMRegister, $tmp$$XMMRegister, $atmp$$XMMRegister, $btmp$$XMMRegister, Assembler::AVX_128bit);\n+  %}\n+  ins_pipe( pipe_slow );\n+%}\n+\n+instruct minF_reduction_reg(legRegF dst, legRegF a, legRegF b, legRegF xtmp, rRegI rtmp, rFlagsReg cr) %{\n+  predicate(!VM_Version::supports_avx10_2() && UseAVX > 0 && VLoopReductions::is_reduction(n));\n+  match(Set dst (MinF a b));\n+  effect(USE a, USE b, TEMP xtmp, TEMP rtmp, KILL cr);\n+\n+  format %{ \"minF_reduction $dst, $a, $b \\t! using $xtmp and $rtmp as TEMP\" %}\n+  ins_encode %{\n+    emit_fp_min_max(masm, $dst$$XMMRegister, $a$$XMMRegister, $b$$XMMRegister, $xtmp$$XMMRegister, $rtmp$$Register,\n+                    true \/*min*\/, true \/*single*\/);\n+  %}\n+  ins_pipe( pipe_slow );\n+%}\n+\n+\/\/ max = java.lang.Math.min(double a, double b)\n+instruct minD_avx10_reg(regD dst, regD a, regD b) %{\n+  predicate(VM_Version::supports_avx10_2());\n+  match(Set dst (MinD a b));\n+  format %{ \"minD $dst, $a, $b\" %}\n+  ins_encode %{\n+    __ eminmaxsd($dst$$XMMRegister, $a$$XMMRegister, $b$$XMMRegister, AVX10_MINMAX_MIN_COMPARE_SIGN);\n+  %}\n+  ins_pipe( pipe_slow );\n+%}\n+\n+\/\/ min = java.lang.Math.min(double a, double b)\n+instruct minD_reg(legRegD dst, legRegD a, legRegD b, legRegD tmp, legRegD atmp, legRegD btmp) %{\n+  predicate(!VM_Version::supports_avx10_2() && UseAVX > 0 && !VLoopReductions::is_reduction(n));\n+  match(Set dst (MinD a b));\n+  effect(USE a, USE b, TEMP tmp, TEMP atmp, TEMP btmp);\n+    format %{ \"minD $dst, $a, $b \\t! using $tmp, $atmp and $btmp as TEMP\" %}\n+  ins_encode %{\n+    __ vminmax_fp(Op_MinV, T_DOUBLE, $dst$$XMMRegister, $a$$XMMRegister, $b$$XMMRegister, $tmp$$XMMRegister, $atmp$$XMMRegister, $btmp$$XMMRegister, Assembler::AVX_128bit);\n+  %}\n+  ins_pipe( pipe_slow );\n+%}\n+\n+instruct minD_reduction_reg(legRegD dst, legRegD a, legRegD b, legRegD xtmp, rRegL rtmp, rFlagsReg cr) %{\n+  predicate(!VM_Version::supports_avx10_2() && UseAVX > 0 && VLoopReductions::is_reduction(n));\n+  match(Set dst (MinD a b));\n+  effect(USE a, USE b, TEMP xtmp, TEMP rtmp, KILL cr);\n+\n+  format %{ \"maxD_reduction $dst, $a, $b \\t! using $xtmp and $rtmp as TEMP\" %}\n+  ins_encode %{\n+    emit_fp_min_max(masm, $dst$$XMMRegister, $a$$XMMRegister, $b$$XMMRegister, $xtmp$$XMMRegister, $rtmp$$Register,\n+                    true \/*min*\/, false \/*single*\/);\n+  %}\n+  ins_pipe( pipe_slow );\n+%}\n+\n+\/\/ Load Effective Address\n+instruct leaP8(rRegP dst, indOffset8 mem)\n+%{\n+  match(Set dst mem);\n+\n+  ins_cost(110); \/\/ XXX\n+  format %{ \"leaq    $dst, $mem\\t# ptr 8\" %}\n+  ins_encode %{\n+    __ leaq($dst$$Register, $mem$$Address);\n+  %}\n+  ins_pipe(ialu_reg_reg_fat);\n+%}\n+\n+instruct leaP32(rRegP dst, indOffset32 mem)\n+%{\n+  match(Set dst mem);\n+\n+  ins_cost(110);\n+  format %{ \"leaq    $dst, $mem\\t# ptr 32\" %}\n+  ins_encode %{\n+    __ leaq($dst$$Register, $mem$$Address);\n+  %}\n+  ins_pipe(ialu_reg_reg_fat);\n+%}\n+\n+instruct leaPIdxOff(rRegP dst, indIndexOffset mem)\n+%{\n+  match(Set dst mem);\n+\n+  ins_cost(110);\n+  format %{ \"leaq    $dst, $mem\\t# ptr idxoff\" %}\n+  ins_encode %{\n+    __ leaq($dst$$Register, $mem$$Address);\n+  %}\n+  ins_pipe(ialu_reg_reg_fat);\n+%}\n+\n+instruct leaPIdxScale(rRegP dst, indIndexScale mem)\n+%{\n+  match(Set dst mem);\n+\n+  ins_cost(110);\n+  format %{ \"leaq    $dst, $mem\\t# ptr idxscale\" %}\n+  ins_encode %{\n+    __ leaq($dst$$Register, $mem$$Address);\n+  %}\n+  ins_pipe(ialu_reg_reg_fat);\n+%}\n+\n+instruct leaPPosIdxScale(rRegP dst, indPosIndexScale mem)\n+%{\n+  match(Set dst mem);\n+\n+  ins_cost(110);\n+  format %{ \"leaq    $dst, $mem\\t# ptr idxscale\" %}\n+  ins_encode %{\n+    __ leaq($dst$$Register, $mem$$Address);\n+  %}\n+  ins_pipe(ialu_reg_reg_fat);\n+%}\n+\n+instruct leaPIdxScaleOff(rRegP dst, indIndexScaleOffset mem)\n+%{\n+  match(Set dst mem);\n+\n+  ins_cost(110);\n+  format %{ \"leaq    $dst, $mem\\t# ptr idxscaleoff\" %}\n+  ins_encode %{\n+    __ leaq($dst$$Register, $mem$$Address);\n+  %}\n+  ins_pipe(ialu_reg_reg_fat);\n+%}\n+\n+instruct leaPPosIdxOff(rRegP dst, indPosIndexOffset mem)\n+%{\n+  match(Set dst mem);\n+\n+  ins_cost(110);\n+  format %{ \"leaq    $dst, $mem\\t# ptr posidxoff\" %}\n+  ins_encode %{\n+    __ leaq($dst$$Register, $mem$$Address);\n+  %}\n+  ins_pipe(ialu_reg_reg_fat);\n+%}\n+\n+instruct leaPPosIdxScaleOff(rRegP dst, indPosIndexScaleOffset mem)\n+%{\n+  match(Set dst mem);\n+\n+  ins_cost(110);\n+  format %{ \"leaq    $dst, $mem\\t# ptr posidxscaleoff\" %}\n+  ins_encode %{\n+    __ leaq($dst$$Register, $mem$$Address);\n+  %}\n+  ins_pipe(ialu_reg_reg_fat);\n+%}\n+\n+\/\/ Load Effective Address which uses Narrow (32-bits) oop\n+instruct leaPCompressedOopOffset(rRegP dst, indCompressedOopOffset mem)\n+%{\n+  predicate(UseCompressedOops && (CompressedOops::shift() != 0));\n+  match(Set dst mem);\n+\n+  ins_cost(110);\n+  format %{ \"leaq    $dst, $mem\\t# ptr compressedoopoff32\" %}\n+  ins_encode %{\n+    __ leaq($dst$$Register, $mem$$Address);\n+  %}\n+  ins_pipe(ialu_reg_reg_fat);\n+%}\n+\n+instruct leaP8Narrow(rRegP dst, indOffset8Narrow mem)\n+%{\n+  predicate(CompressedOops::shift() == 0);\n+  match(Set dst mem);\n+\n+  ins_cost(110); \/\/ XXX\n+  format %{ \"leaq    $dst, $mem\\t# ptr off8narrow\" %}\n+  ins_encode %{\n+    __ leaq($dst$$Register, $mem$$Address);\n+  %}\n+  ins_pipe(ialu_reg_reg_fat);\n+%}\n+\n+instruct leaP32Narrow(rRegP dst, indOffset32Narrow mem)\n+%{\n+  predicate(CompressedOops::shift() == 0);\n+  match(Set dst mem);\n+\n+  ins_cost(110);\n+  format %{ \"leaq    $dst, $mem\\t# ptr off32narrow\" %}\n+  ins_encode %{\n+    __ leaq($dst$$Register, $mem$$Address);\n+  %}\n+  ins_pipe(ialu_reg_reg_fat);\n+%}\n+\n+instruct leaPIdxOffNarrow(rRegP dst, indIndexOffsetNarrow mem)\n+%{\n+  predicate(CompressedOops::shift() == 0);\n+  match(Set dst mem);\n+\n+  ins_cost(110);\n+  format %{ \"leaq    $dst, $mem\\t# ptr idxoffnarrow\" %}\n+  ins_encode %{\n+    __ leaq($dst$$Register, $mem$$Address);\n+  %}\n+  ins_pipe(ialu_reg_reg_fat);\n+%}\n+\n+instruct leaPIdxScaleNarrow(rRegP dst, indIndexScaleNarrow mem)\n+%{\n+  predicate(CompressedOops::shift() == 0);\n+  match(Set dst mem);\n+\n+  ins_cost(110);\n+  format %{ \"leaq    $dst, $mem\\t# ptr idxscalenarrow\" %}\n+  ins_encode %{\n+    __ leaq($dst$$Register, $mem$$Address);\n+  %}\n+  ins_pipe(ialu_reg_reg_fat);\n+%}\n+\n+instruct leaPIdxScaleOffNarrow(rRegP dst, indIndexScaleOffsetNarrow mem)\n+%{\n+  predicate(CompressedOops::shift() == 0);\n+  match(Set dst mem);\n+\n+  ins_cost(110);\n+  format %{ \"leaq    $dst, $mem\\t# ptr idxscaleoffnarrow\" %}\n+  ins_encode %{\n+    __ leaq($dst$$Register, $mem$$Address);\n+  %}\n+  ins_pipe(ialu_reg_reg_fat);\n+%}\n+\n+instruct leaPPosIdxOffNarrow(rRegP dst, indPosIndexOffsetNarrow mem)\n+%{\n+  predicate(CompressedOops::shift() == 0);\n+  match(Set dst mem);\n+\n+  ins_cost(110);\n+  format %{ \"leaq    $dst, $mem\\t# ptr posidxoffnarrow\" %}\n+  ins_encode %{\n+    __ leaq($dst$$Register, $mem$$Address);\n+  %}\n+  ins_pipe(ialu_reg_reg_fat);\n+%}\n+\n+instruct leaPPosIdxScaleOffNarrow(rRegP dst, indPosIndexScaleOffsetNarrow mem)\n+%{\n+  predicate(CompressedOops::shift() == 0);\n+  match(Set dst mem);\n+\n+  ins_cost(110);\n+  format %{ \"leaq    $dst, $mem\\t# ptr posidxscaleoffnarrow\" %}\n+  ins_encode %{\n+    __ leaq($dst$$Register, $mem$$Address);\n+  %}\n+  ins_pipe(ialu_reg_reg_fat);\n+%}\n+\n+instruct loadConI(rRegI dst, immI src)\n+%{\n+  match(Set dst src);\n+\n+  format %{ \"movl    $dst, $src\\t# int\" %}\n+  ins_encode %{\n+    __ movl($dst$$Register, $src$$constant);\n+  %}\n+  ins_pipe(ialu_reg_fat); \/\/ XXX\n+%}\n+\n+instruct loadConI0(rRegI dst, immI_0 src, rFlagsReg cr)\n+%{\n+  match(Set dst src);\n+  effect(KILL cr);\n+\n+  ins_cost(50);\n+  format %{ \"xorl    $dst, $dst\\t# int\" %}\n+  ins_encode %{\n+    __ xorl($dst$$Register, $dst$$Register);\n+  %}\n+  ins_pipe(ialu_reg);\n+%}\n+\n+instruct loadConL(rRegL dst, immL src)\n+%{\n+  match(Set dst src);\n+\n+  ins_cost(150);\n+  format %{ \"movq    $dst, $src\\t# long\" %}\n+  ins_encode %{\n+    __ mov64($dst$$Register, $src$$constant);\n+  %}\n+  ins_pipe(ialu_reg);\n+%}\n+\n+instruct loadConL0(rRegL dst, immL0 src, rFlagsReg cr)\n+%{\n+  match(Set dst src);\n+  effect(KILL cr);\n+\n+  ins_cost(50);\n+  format %{ \"xorl    $dst, $dst\\t# long\" %}\n+  ins_encode %{\n+    __ xorl($dst$$Register, $dst$$Register);\n+  %}\n+  ins_pipe(ialu_reg); \/\/ XXX\n+%}\n+\n+instruct loadConUL32(rRegL dst, immUL32 src)\n+%{\n+  match(Set dst src);\n+\n+  ins_cost(60);\n+  format %{ \"movl    $dst, $src\\t# long (unsigned 32-bit)\" %}\n+  ins_encode %{\n+    __ movl($dst$$Register, $src$$constant);\n+  %}\n+  ins_pipe(ialu_reg);\n+%}\n+\n+instruct loadConL32(rRegL dst, immL32 src)\n+%{\n+  match(Set dst src);\n+\n+  ins_cost(70);\n+  format %{ \"movq    $dst, $src\\t# long (32-bit)\" %}\n+  ins_encode %{\n+    __ movq($dst$$Register, $src$$constant);\n+  %}\n+  ins_pipe(ialu_reg);\n+%}\n+\n+instruct loadConP(rRegP dst, immP con) %{\n+  match(Set dst con);\n+\n+  format %{ \"movq    $dst, $con\\t# ptr\" %}\n+  ins_encode %{\n+    __ mov64($dst$$Register, $con$$constant, $con->constant_reloc(), RELOC_IMM64);\n+  %}\n+  ins_pipe(ialu_reg_fat); \/\/ XXX\n+%}\n+\n+instruct loadConP0(rRegP dst, immP0 src, rFlagsReg cr)\n+%{\n+  match(Set dst src);\n+  effect(KILL cr);\n+\n+  ins_cost(50);\n+  format %{ \"xorl    $dst, $dst\\t# ptr\" %}\n+  ins_encode %{\n+    __ xorl($dst$$Register, $dst$$Register);\n+  %}\n+  ins_pipe(ialu_reg);\n+%}\n+\n+instruct loadConP31(rRegP dst, immP31 src, rFlagsReg cr)\n+%{\n+  match(Set dst src);\n+  effect(KILL cr);\n+\n+  ins_cost(60);\n+  format %{ \"movl    $dst, $src\\t# ptr (positive 32-bit)\" %}\n+  ins_encode %{\n+    __ movl($dst$$Register, $src$$constant);\n+  %}\n+  ins_pipe(ialu_reg);\n+%}\n+\n+instruct loadConF(regF dst, immF con) %{\n+  match(Set dst con);\n+  ins_cost(125);\n+  format %{ \"movss   $dst, [$constantaddress]\\t# load from constant table: float=$con\" %}\n+  ins_encode %{\n+    __ movflt($dst$$XMMRegister, $constantaddress($con));\n+  %}\n+  ins_pipe(pipe_slow);\n+%}\n+\n+instruct loadConH(regF dst, immH con) %{\n+  match(Set dst con);\n+  ins_cost(125);\n+  format %{ \"movss   $dst, [$constantaddress]\\t# load from constant table: halffloat=$con\" %}\n+  ins_encode %{\n+    __ movflt($dst$$XMMRegister, $constantaddress($con));\n+  %}\n+  ins_pipe(pipe_slow);\n+%}\n+\n+instruct loadConN0(rRegN dst, immN0 src, rFlagsReg cr) %{\n+  match(Set dst src);\n+  effect(KILL cr);\n+  format %{ \"xorq    $dst, $src\\t# compressed null pointer\" %}\n+  ins_encode %{\n+    __ xorq($dst$$Register, $dst$$Register);\n+  %}\n+  ins_pipe(ialu_reg);\n+%}\n+\n+instruct loadConN(rRegN dst, immN src) %{\n+  match(Set dst src);\n+\n+  ins_cost(125);\n+  format %{ \"movl    $dst, $src\\t# compressed ptr\" %}\n+  ins_encode %{\n+    address con = (address)$src$$constant;\n+    if (con == nullptr) {\n+      ShouldNotReachHere();\n+    } else {\n+      __ set_narrow_oop($dst$$Register, (jobject)$src$$constant);\n+    }\n+  %}\n+  ins_pipe(ialu_reg_fat); \/\/ XXX\n+%}\n+\n+instruct loadConNKlass(rRegN dst, immNKlass src) %{\n+  match(Set dst src);\n+\n+  ins_cost(125);\n+  format %{ \"movl    $dst, $src\\t# compressed klass ptr\" %}\n+  ins_encode %{\n+    address con = (address)$src$$constant;\n+    if (con == nullptr) {\n+      ShouldNotReachHere();\n+    } else {\n+      __ set_narrow_klass($dst$$Register, (Klass*)$src$$constant);\n+    }\n+  %}\n+  ins_pipe(ialu_reg_fat); \/\/ XXX\n+%}\n+\n+instruct loadConF0(regF dst, immF0 src)\n+%{\n+  match(Set dst src);\n+  ins_cost(100);\n+\n+  format %{ \"xorps   $dst, $dst\\t# float 0.0\" %}\n+  ins_encode %{\n+    __ xorps($dst$$XMMRegister, $dst$$XMMRegister);\n+  %}\n+  ins_pipe(pipe_slow);\n+%}\n+\n+\/\/ Use the same format since predicate() can not be used here.\n+instruct loadConD(regD dst, immD con) %{\n+  match(Set dst con);\n+  ins_cost(125);\n+  format %{ \"movsd   $dst, [$constantaddress]\\t# load from constant table: double=$con\" %}\n+  ins_encode %{\n+    __ movdbl($dst$$XMMRegister, $constantaddress($con));\n+  %}\n+  ins_pipe(pipe_slow);\n+%}\n+\n+instruct loadConD0(regD dst, immD0 src)\n+%{\n+  match(Set dst src);\n+  ins_cost(100);\n+\n+  format %{ \"xorpd   $dst, $dst\\t# double 0.0\" %}\n+  ins_encode %{\n+    __ xorpd($dst$$XMMRegister, $dst$$XMMRegister);\n+  %}\n+  ins_pipe(pipe_slow);\n+%}\n+\n+instruct loadSSI(rRegI dst, stackSlotI src)\n+%{\n+  match(Set dst src);\n+\n+  ins_cost(125);\n+  format %{ \"movl    $dst, $src\\t# int stk\" %}\n+  ins_encode %{\n+    __ movl($dst$$Register, $src$$Address);\n+  %}\n+  ins_pipe(ialu_reg_mem);\n+%}\n+\n+instruct loadSSL(rRegL dst, stackSlotL src)\n+%{\n+  match(Set dst src);\n+\n+  ins_cost(125);\n+  format %{ \"movq    $dst, $src\\t# long stk\" %}\n+  ins_encode %{\n+    __ movq($dst$$Register, $src$$Address);\n+  %}\n+  ins_pipe(ialu_reg_mem);\n+%}\n+\n+instruct loadSSP(rRegP dst, stackSlotP src)\n+%{\n+  match(Set dst src);\n+\n+  ins_cost(125);\n+  format %{ \"movq    $dst, $src\\t# ptr stk\" %}\n+  ins_encode %{\n+    __ movq($dst$$Register, $src$$Address);\n+  %}\n+  ins_pipe(ialu_reg_mem);\n+%}\n+\n+instruct loadSSF(regF dst, stackSlotF src)\n+%{\n+  match(Set dst src);\n+\n+  ins_cost(125);\n+  format %{ \"movss   $dst, $src\\t# float stk\" %}\n+  ins_encode %{\n+    __ movflt($dst$$XMMRegister, Address(rsp, $src$$disp));\n+  %}\n+  ins_pipe(pipe_slow); \/\/ XXX\n+%}\n+\n+\/\/ Use the same format since predicate() can not be used here.\n+instruct loadSSD(regD dst, stackSlotD src)\n+%{\n+  match(Set dst src);\n+\n+  ins_cost(125);\n+  format %{ \"movsd   $dst, $src\\t# double stk\" %}\n+  ins_encode  %{\n+    __ movdbl($dst$$XMMRegister, Address(rsp, $src$$disp));\n+  %}\n+  ins_pipe(pipe_slow); \/\/ XXX\n+%}\n+\n+\/\/ Prefetch instructions for allocation.\n+\/\/ Must be safe to execute with invalid address (cannot fault).\n+\n+instruct prefetchAlloc( memory mem ) %{\n+  predicate(AllocatePrefetchInstr==3);\n+  match(PrefetchAllocation mem);\n+  ins_cost(125);\n+\n+  format %{ \"PREFETCHW $mem\\t# Prefetch allocation into level 1 cache and mark modified\" %}\n+  ins_encode %{\n+    __ prefetchw($mem$$Address);\n+  %}\n+  ins_pipe(ialu_mem);\n+%}\n+\n+instruct prefetchAllocNTA( memory mem ) %{\n+  predicate(AllocatePrefetchInstr==0);\n+  match(PrefetchAllocation mem);\n+  ins_cost(125);\n+\n+  format %{ \"PREFETCHNTA $mem\\t# Prefetch allocation to non-temporal cache for write\" %}\n+  ins_encode %{\n+    __ prefetchnta($mem$$Address);\n+  %}\n+  ins_pipe(ialu_mem);\n+%}\n+\n+instruct prefetchAllocT0( memory mem ) %{\n+  predicate(AllocatePrefetchInstr==1);\n+  match(PrefetchAllocation mem);\n+  ins_cost(125);\n+\n+  format %{ \"PREFETCHT0 $mem\\t# Prefetch allocation to level 1 and 2 caches for write\" %}\n+  ins_encode %{\n+    __ prefetcht0($mem$$Address);\n+  %}\n+  ins_pipe(ialu_mem);\n+%}\n+\n+instruct prefetchAllocT2( memory mem ) %{\n+  predicate(AllocatePrefetchInstr==2);\n+  match(PrefetchAllocation mem);\n+  ins_cost(125);\n+\n+  format %{ \"PREFETCHT2 $mem\\t# Prefetch allocation to level 2 cache for write\" %}\n+  ins_encode %{\n+    __ prefetcht2($mem$$Address);\n+  %}\n+  ins_pipe(ialu_mem);\n+%}\n+\n+\/\/----------Store Instructions-------------------------------------------------\n+\n+\/\/ Store Byte\n+instruct storeB(memory mem, rRegI src)\n+%{\n+  match(Set mem (StoreB mem src));\n+\n+  ins_cost(125); \/\/ XXX\n+  format %{ \"movb    $mem, $src\\t# byte\" %}\n+  ins_encode %{\n+    __ movb($mem$$Address, $src$$Register);\n+  %}\n+  ins_pipe(ialu_mem_reg);\n+%}\n+\n+\/\/ Store Char\/Short\n+instruct storeC(memory mem, rRegI src)\n+%{\n+  match(Set mem (StoreC mem src));\n+\n+  ins_cost(125); \/\/ XXX\n+  format %{ \"movw    $mem, $src\\t# char\/short\" %}\n+  ins_encode %{\n+    __ movw($mem$$Address, $src$$Register);\n+  %}\n+  ins_pipe(ialu_mem_reg);\n+%}\n+\n+\/\/ Store Integer\n+instruct storeI(memory mem, rRegI src)\n+%{\n+  match(Set mem (StoreI mem src));\n+\n+  ins_cost(125); \/\/ XXX\n+  format %{ \"movl    $mem, $src\\t# int\" %}\n+  ins_encode %{\n+    __ movl($mem$$Address, $src$$Register);\n+  %}\n+  ins_pipe(ialu_mem_reg);\n+%}\n+\n+\/\/ Store Long\n+instruct storeL(memory mem, rRegL src)\n+%{\n+  match(Set mem (StoreL mem src));\n+\n+  ins_cost(125); \/\/ XXX\n+  format %{ \"movq    $mem, $src\\t# long\" %}\n+  ins_encode %{\n+    __ movq($mem$$Address, $src$$Register);\n+  %}\n+  ins_pipe(ialu_mem_reg); \/\/ XXX\n+%}\n+\n+\/\/ Store Pointer\n+instruct storeP(memory mem, any_RegP src)\n+%{\n+  predicate(n->as_Store()->barrier_data() == 0);\n+  match(Set mem (StoreP mem src));\n+\n+  ins_cost(125); \/\/ XXX\n+  format %{ \"movq    $mem, $src\\t# ptr\" %}\n+  ins_encode %{\n+    __ movq($mem$$Address, $src$$Register);\n+  %}\n+  ins_pipe(ialu_mem_reg);\n+%}\n+\n+instruct storeImmP0(memory mem, immP0 zero)\n+%{\n+  predicate(UseCompressedOops && (CompressedOops::base() == nullptr) && n->as_Store()->barrier_data() == 0);\n+  match(Set mem (StoreP mem zero));\n+\n+  ins_cost(125); \/\/ XXX\n+  format %{ \"movq    $mem, R12\\t# ptr (R12_heapbase==0)\" %}\n+  ins_encode %{\n+    __ movq($mem$$Address, r12);\n+  %}\n+  ins_pipe(ialu_mem_reg);\n+%}\n+\n+\/\/ Store Null Pointer, mark word, or other simple pointer constant.\n+instruct storeImmP(memory mem, immP31 src)\n+%{\n+  predicate(n->as_Store()->barrier_data() == 0);\n+  match(Set mem (StoreP mem src));\n+\n+  ins_cost(150); \/\/ XXX\n+  format %{ \"movq    $mem, $src\\t# ptr\" %}\n+  ins_encode %{\n+    __ movq($mem$$Address, $src$$constant);\n+  %}\n+  ins_pipe(ialu_mem_imm);\n+%}\n+\n+\/\/ Store Compressed Pointer\n+instruct storeN(memory mem, rRegN src)\n+%{\n+  predicate(n->as_Store()->barrier_data() == 0);\n+  match(Set mem (StoreN mem src));\n+\n+  ins_cost(125); \/\/ XXX\n+  format %{ \"movl    $mem, $src\\t# compressed ptr\" %}\n+  ins_encode %{\n+    __ movl($mem$$Address, $src$$Register);\n+  %}\n+  ins_pipe(ialu_mem_reg);\n+%}\n+\n+instruct storeNKlass(memory mem, rRegN src)\n+%{\n+  match(Set mem (StoreNKlass mem src));\n+\n+  ins_cost(125); \/\/ XXX\n+  format %{ \"movl    $mem, $src\\t# compressed klass ptr\" %}\n+  ins_encode %{\n+    __ movl($mem$$Address, $src$$Register);\n+  %}\n+  ins_pipe(ialu_mem_reg);\n+%}\n+\n+instruct storeImmN0(memory mem, immN0 zero)\n+%{\n+  predicate(CompressedOops::base() == nullptr && n->as_Store()->barrier_data() == 0);\n+  match(Set mem (StoreN mem zero));\n+\n+  ins_cost(125); \/\/ XXX\n+  format %{ \"movl    $mem, R12\\t# compressed ptr (R12_heapbase==0)\" %}\n+  ins_encode %{\n+    __ movl($mem$$Address, r12);\n+  %}\n+  ins_pipe(ialu_mem_reg);\n+%}\n+\n+instruct storeImmN(memory mem, immN src)\n+%{\n+  predicate(n->as_Store()->barrier_data() == 0);\n+  match(Set mem (StoreN mem src));\n+\n+  ins_cost(150); \/\/ XXX\n+  format %{ \"movl    $mem, $src\\t# compressed ptr\" %}\n+  ins_encode %{\n+    address con = (address)$src$$constant;\n+    if (con == nullptr) {\n+      __ movl($mem$$Address, 0);\n+    } else {\n+      __ set_narrow_oop($mem$$Address, (jobject)$src$$constant);\n+    }\n+  %}\n+  ins_pipe(ialu_mem_imm);\n+%}\n+\n+instruct storeImmNKlass(memory mem, immNKlass src)\n+%{\n+  match(Set mem (StoreNKlass mem src));\n+\n+  ins_cost(150); \/\/ XXX\n+  format %{ \"movl    $mem, $src\\t# compressed klass ptr\" %}\n+  ins_encode %{\n+    __ set_narrow_klass($mem$$Address, (Klass*)$src$$constant);\n+  %}\n+  ins_pipe(ialu_mem_imm);\n+%}\n+\n+\/\/ Store Integer Immediate\n+instruct storeImmI0(memory mem, immI_0 zero)\n+%{\n+  predicate(UseCompressedOops && (CompressedOops::base() == nullptr));\n+  match(Set mem (StoreI mem zero));\n+\n+  ins_cost(125); \/\/ XXX\n+  format %{ \"movl    $mem, R12\\t# int (R12_heapbase==0)\" %}\n+  ins_encode %{\n+    __ movl($mem$$Address, r12);\n+  %}\n+  ins_pipe(ialu_mem_reg);\n+%}\n+\n+instruct storeImmI(memory mem, immI src)\n+%{\n+  match(Set mem (StoreI mem src));\n+\n+  ins_cost(150);\n+  format %{ \"movl    $mem, $src\\t# int\" %}\n+  ins_encode %{\n+    __ movl($mem$$Address, $src$$constant);\n+  %}\n+  ins_pipe(ialu_mem_imm);\n+%}\n+\n+\/\/ Store Long Immediate\n+instruct storeImmL0(memory mem, immL0 zero)\n+%{\n+  predicate(UseCompressedOops && (CompressedOops::base() == nullptr));\n+  match(Set mem (StoreL mem zero));\n+\n+  ins_cost(125); \/\/ XXX\n+  format %{ \"movq    $mem, R12\\t# long (R12_heapbase==0)\" %}\n+  ins_encode %{\n+    __ movq($mem$$Address, r12);\n+  %}\n+  ins_pipe(ialu_mem_reg);\n+%}\n+\n+instruct storeImmL(memory mem, immL32 src)\n+%{\n+  match(Set mem (StoreL mem src));\n+\n+  ins_cost(150);\n+  format %{ \"movq    $mem, $src\\t# long\" %}\n+  ins_encode %{\n+    __ movq($mem$$Address, $src$$constant);\n+  %}\n+  ins_pipe(ialu_mem_imm);\n+%}\n+\n+\/\/ Store Short\/Char Immediate\n+instruct storeImmC0(memory mem, immI_0 zero)\n+%{\n+  predicate(UseCompressedOops && (CompressedOops::base() == nullptr));\n+  match(Set mem (StoreC mem zero));\n+\n+  ins_cost(125); \/\/ XXX\n+  format %{ \"movw    $mem, R12\\t# short\/char (R12_heapbase==0)\" %}\n+  ins_encode %{\n+    __ movw($mem$$Address, r12);\n+  %}\n+  ins_pipe(ialu_mem_reg);\n+%}\n+\n+instruct storeImmI16(memory mem, immI16 src)\n+%{\n+  predicate(UseStoreImmI16);\n+  match(Set mem (StoreC mem src));\n+\n+  ins_cost(150);\n+  format %{ \"movw    $mem, $src\\t# short\/char\" %}\n+  ins_encode %{\n+    __ movw($mem$$Address, $src$$constant);\n+  %}\n+  ins_pipe(ialu_mem_imm);\n+%}\n+\n+\/\/ Store Byte Immediate\n+instruct storeImmB0(memory mem, immI_0 zero)\n+%{\n+  predicate(UseCompressedOops && (CompressedOops::base() == nullptr));\n+  match(Set mem (StoreB mem zero));\n+\n+  ins_cost(125); \/\/ XXX\n+  format %{ \"movb    $mem, R12\\t# short\/char (R12_heapbase==0)\" %}\n+  ins_encode %{\n+    __ movb($mem$$Address, r12);\n+  %}\n+  ins_pipe(ialu_mem_reg);\n+%}\n+\n+instruct storeImmB(memory mem, immI8 src)\n+%{\n+  match(Set mem (StoreB mem src));\n+\n+  ins_cost(150); \/\/ XXX\n+  format %{ \"movb    $mem, $src\\t# byte\" %}\n+  ins_encode %{\n+    __ movb($mem$$Address, $src$$constant);\n+  %}\n+  ins_pipe(ialu_mem_imm);\n+%}\n+\n+\/\/ Store Float\n+instruct storeF(memory mem, regF src)\n+%{\n+  match(Set mem (StoreF mem src));\n+\n+  ins_cost(95); \/\/ XXX\n+  format %{ \"movss   $mem, $src\\t# float\" %}\n+  ins_encode %{\n+    __ movflt($mem$$Address, $src$$XMMRegister);\n+  %}\n+  ins_pipe(pipe_slow); \/\/ XXX\n+%}\n+\n+\/\/ Store immediate Float value (it is faster than store from XMM register)\n+instruct storeF0(memory mem, immF0 zero)\n+%{\n+  predicate(UseCompressedOops && (CompressedOops::base() == nullptr));\n+  match(Set mem (StoreF mem zero));\n+\n+  ins_cost(25); \/\/ XXX\n+  format %{ \"movl    $mem, R12\\t# float 0. (R12_heapbase==0)\" %}\n+  ins_encode %{\n+    __ movl($mem$$Address, r12);\n+  %}\n+  ins_pipe(ialu_mem_reg);\n+%}\n+\n+instruct storeF_imm(memory mem, immF src)\n+%{\n+  match(Set mem (StoreF mem src));\n+\n+  ins_cost(50);\n+  format %{ \"movl    $mem, $src\\t# float\" %}\n+  ins_encode %{\n+    __ movl($mem$$Address, jint_cast($src$$constant));\n+  %}\n+  ins_pipe(ialu_mem_imm);\n+%}\n+\n+\/\/ Store Double\n+instruct storeD(memory mem, regD src)\n+%{\n+  match(Set mem (StoreD mem src));\n+\n+  ins_cost(95); \/\/ XXX\n+  format %{ \"movsd   $mem, $src\\t# double\" %}\n+  ins_encode %{\n+    __ movdbl($mem$$Address, $src$$XMMRegister);\n+  %}\n+  ins_pipe(pipe_slow); \/\/ XXX\n+%}\n+\n+\/\/ Store immediate double 0.0 (it is faster than store from XMM register)\n+instruct storeD0_imm(memory mem, immD0 src)\n+%{\n+  predicate(!UseCompressedOops || (CompressedOops::base() != nullptr));\n+  match(Set mem (StoreD mem src));\n+\n+  ins_cost(50);\n+  format %{ \"movq    $mem, $src\\t# double 0.\" %}\n+  ins_encode %{\n+    __ movq($mem$$Address, $src$$constant);\n+  %}\n+  ins_pipe(ialu_mem_imm);\n+%}\n+\n+instruct storeD0(memory mem, immD0 zero)\n+%{\n+  predicate(UseCompressedOops && (CompressedOops::base() == nullptr));\n+  match(Set mem (StoreD mem zero));\n+\n+  ins_cost(25); \/\/ XXX\n+  format %{ \"movq    $mem, R12\\t# double 0. (R12_heapbase==0)\" %}\n+  ins_encode %{\n+    __ movq($mem$$Address, r12);\n+  %}\n+  ins_pipe(ialu_mem_reg);\n+%}\n+\n+instruct storeSSI(stackSlotI dst, rRegI src)\n+%{\n+  match(Set dst src);\n+\n+  ins_cost(100);\n+  format %{ \"movl    $dst, $src\\t# int stk\" %}\n+  ins_encode %{\n+    __ movl($dst$$Address, $src$$Register);\n+  %}\n+  ins_pipe( ialu_mem_reg );\n+%}\n+\n+instruct storeSSL(stackSlotL dst, rRegL src)\n+%{\n+  match(Set dst src);\n+\n+  ins_cost(100);\n+  format %{ \"movq    $dst, $src\\t# long stk\" %}\n+  ins_encode %{\n+    __ movq($dst$$Address, $src$$Register);\n+  %}\n+  ins_pipe(ialu_mem_reg);\n+%}\n+\n+instruct storeSSP(stackSlotP dst, rRegP src)\n+%{\n+  match(Set dst src);\n+\n+  ins_cost(100);\n+  format %{ \"movq    $dst, $src\\t# ptr stk\" %}\n+  ins_encode %{\n+    __ movq($dst$$Address, $src$$Register);\n+  %}\n+  ins_pipe(ialu_mem_reg);\n+%}\n+\n+instruct storeSSF(stackSlotF dst, regF src)\n+%{\n+  match(Set dst src);\n+\n+  ins_cost(95); \/\/ XXX\n+  format %{ \"movss   $dst, $src\\t# float stk\" %}\n+  ins_encode %{\n+    __ movflt(Address(rsp, $dst$$disp), $src$$XMMRegister);\n+  %}\n+  ins_pipe(pipe_slow); \/\/ XXX\n+%}\n+\n+instruct storeSSD(stackSlotD dst, regD src)\n+%{\n+  match(Set dst src);\n+\n+  ins_cost(95); \/\/ XXX\n+  format %{ \"movsd   $dst, $src\\t# double stk\" %}\n+  ins_encode %{\n+    __ movdbl(Address(rsp, $dst$$disp), $src$$XMMRegister);\n+  %}\n+  ins_pipe(pipe_slow); \/\/ XXX\n+%}\n+\n+instruct cacheWB(indirect addr)\n+%{\n+  predicate(VM_Version::supports_data_cache_line_flush());\n+  match(CacheWB addr);\n+\n+  ins_cost(100);\n+  format %{\"cache wb $addr\" %}\n+  ins_encode %{\n+    assert($addr->index_position() < 0, \"should be\");\n+    assert($addr$$disp == 0, \"should be\");\n+    __ cache_wb(Address($addr$$base$$Register, 0));\n+  %}\n+  ins_pipe(pipe_slow); \/\/ XXX\n+%}\n+\n+instruct cacheWBPreSync()\n+%{\n+  predicate(VM_Version::supports_data_cache_line_flush());\n+  match(CacheWBPreSync);\n+\n+  ins_cost(100);\n+  format %{\"cache wb presync\" %}\n+  ins_encode %{\n+    __ cache_wbsync(true);\n+  %}\n+  ins_pipe(pipe_slow); \/\/ XXX\n+%}\n+\n+instruct cacheWBPostSync()\n+%{\n+  predicate(VM_Version::supports_data_cache_line_flush());\n+  match(CacheWBPostSync);\n+\n+  ins_cost(100);\n+  format %{\"cache wb postsync\" %}\n+  ins_encode %{\n+    __ cache_wbsync(false);\n+  %}\n+  ins_pipe(pipe_slow); \/\/ XXX\n+%}\n+\n+\/\/----------BSWAP Instructions-------------------------------------------------\n+instruct bytes_reverse_int(rRegI dst) %{\n+  match(Set dst (ReverseBytesI dst));\n+\n+  format %{ \"bswapl  $dst\" %}\n+  ins_encode %{\n+    __ bswapl($dst$$Register);\n+  %}\n+  ins_pipe( ialu_reg );\n+%}\n+\n+instruct bytes_reverse_long(rRegL dst) %{\n+  match(Set dst (ReverseBytesL dst));\n+\n+  format %{ \"bswapq  $dst\" %}\n+  ins_encode %{\n+    __ bswapq($dst$$Register);\n+  %}\n+  ins_pipe( ialu_reg);\n+%}\n+\n+instruct bytes_reverse_unsigned_short(rRegI dst, rFlagsReg cr) %{\n+  match(Set dst (ReverseBytesUS dst));\n+  effect(KILL cr);\n+\n+  format %{ \"bswapl  $dst\\n\\t\"\n+            \"shrl    $dst,16\\n\\t\" %}\n+  ins_encode %{\n+    __ bswapl($dst$$Register);\n+    __ shrl($dst$$Register, 16);\n+  %}\n+  ins_pipe( ialu_reg );\n+%}\n+\n+instruct bytes_reverse_short(rRegI dst, rFlagsReg cr) %{\n+  match(Set dst (ReverseBytesS dst));\n+  effect(KILL cr);\n+\n+  format %{ \"bswapl  $dst\\n\\t\"\n+            \"sar     $dst,16\\n\\t\" %}\n+  ins_encode %{\n+    __ bswapl($dst$$Register);\n+    __ sarl($dst$$Register, 16);\n+  %}\n+  ins_pipe( ialu_reg );\n+%}\n+\n+\/\/---------- Zeros Count Instructions ------------------------------------------\n+\n+instruct countLeadingZerosI(rRegI dst, rRegI src, rFlagsReg cr) %{\n+  predicate(UseCountLeadingZerosInstruction);\n+  match(Set dst (CountLeadingZerosI src));\n+  effect(KILL cr);\n+\n+  format %{ \"lzcntl  $dst, $src\\t# count leading zeros (int)\" %}\n+  ins_encode %{\n+    __ lzcntl($dst$$Register, $src$$Register);\n+  %}\n+  ins_pipe(ialu_reg);\n+%}\n+\n+instruct countLeadingZerosI_mem(rRegI dst, memory src, rFlagsReg cr) %{\n+  predicate(UseCountLeadingZerosInstruction);\n+  match(Set dst (CountLeadingZerosI (LoadI src)));\n+  effect(KILL cr);\n+  ins_cost(175);\n+  format %{ \"lzcntl  $dst, $src\\t# count leading zeros (int)\" %}\n+  ins_encode %{\n+    __ lzcntl($dst$$Register, $src$$Address);\n+  %}\n+  ins_pipe(ialu_reg_mem);\n+%}\n+\n+instruct countLeadingZerosI_bsr(rRegI dst, rRegI src, rFlagsReg cr) %{\n+  predicate(!UseCountLeadingZerosInstruction);\n+  match(Set dst (CountLeadingZerosI src));\n+  effect(KILL cr);\n+\n+  format %{ \"bsrl    $dst, $src\\t# count leading zeros (int)\\n\\t\"\n+            \"jnz     skip\\n\\t\"\n+            \"movl    $dst, -1\\n\"\n+      \"skip:\\n\\t\"\n+            \"negl    $dst\\n\\t\"\n+            \"addl    $dst, 31\" %}\n+  ins_encode %{\n+    Register Rdst = $dst$$Register;\n+    Register Rsrc = $src$$Register;\n+    Label skip;\n+    __ bsrl(Rdst, Rsrc);\n+    __ jccb(Assembler::notZero, skip);\n+    __ movl(Rdst, -1);\n+    __ bind(skip);\n+    __ negl(Rdst);\n+    __ addl(Rdst, BitsPerInt - 1);\n+  %}\n+  ins_pipe(ialu_reg);\n+%}\n+\n+instruct countLeadingZerosL(rRegI dst, rRegL src, rFlagsReg cr) %{\n+  predicate(UseCountLeadingZerosInstruction);\n+  match(Set dst (CountLeadingZerosL src));\n+  effect(KILL cr);\n+\n+  format %{ \"lzcntq  $dst, $src\\t# count leading zeros (long)\" %}\n+  ins_encode %{\n+    __ lzcntq($dst$$Register, $src$$Register);\n+  %}\n+  ins_pipe(ialu_reg);\n+%}\n+\n+instruct countLeadingZerosL_mem(rRegI dst, memory src, rFlagsReg cr) %{\n+  predicate(UseCountLeadingZerosInstruction);\n+  match(Set dst (CountLeadingZerosL (LoadL src)));\n+  effect(KILL cr);\n+  ins_cost(175);\n+  format %{ \"lzcntq  $dst, $src\\t# count leading zeros (long)\" %}\n+  ins_encode %{\n+    __ lzcntq($dst$$Register, $src$$Address);\n+  %}\n+  ins_pipe(ialu_reg_mem);\n+%}\n+\n+instruct countLeadingZerosL_bsr(rRegI dst, rRegL src, rFlagsReg cr) %{\n+  predicate(!UseCountLeadingZerosInstruction);\n+  match(Set dst (CountLeadingZerosL src));\n+  effect(KILL cr);\n+\n+  format %{ \"bsrq    $dst, $src\\t# count leading zeros (long)\\n\\t\"\n+            \"jnz     skip\\n\\t\"\n+            \"movl    $dst, -1\\n\"\n+      \"skip:\\n\\t\"\n+            \"negl    $dst\\n\\t\"\n+            \"addl    $dst, 63\" %}\n+  ins_encode %{\n+    Register Rdst = $dst$$Register;\n+    Register Rsrc = $src$$Register;\n+    Label skip;\n+    __ bsrq(Rdst, Rsrc);\n+    __ jccb(Assembler::notZero, skip);\n+    __ movl(Rdst, -1);\n+    __ bind(skip);\n+    __ negl(Rdst);\n+    __ addl(Rdst, BitsPerLong - 1);\n+  %}\n+  ins_pipe(ialu_reg);\n+%}\n+\n+instruct countTrailingZerosI(rRegI dst, rRegI src, rFlagsReg cr) %{\n+  predicate(UseCountTrailingZerosInstruction);\n+  match(Set dst (CountTrailingZerosI src));\n+  effect(KILL cr);\n+\n+  format %{ \"tzcntl    $dst, $src\\t# count trailing zeros (int)\" %}\n+  ins_encode %{\n+    __ tzcntl($dst$$Register, $src$$Register);\n+  %}\n+  ins_pipe(ialu_reg);\n+%}\n+\n+instruct countTrailingZerosI_mem(rRegI dst, memory src, rFlagsReg cr) %{\n+  predicate(UseCountTrailingZerosInstruction);\n+  match(Set dst (CountTrailingZerosI (LoadI src)));\n+  effect(KILL cr);\n+  ins_cost(175);\n+  format %{ \"tzcntl    $dst, $src\\t# count trailing zeros (int)\" %}\n+  ins_encode %{\n+    __ tzcntl($dst$$Register, $src$$Address);\n+  %}\n+  ins_pipe(ialu_reg_mem);\n+%}\n+\n+instruct countTrailingZerosI_bsf(rRegI dst, rRegI src, rFlagsReg cr) %{\n+  predicate(!UseCountTrailingZerosInstruction);\n+  match(Set dst (CountTrailingZerosI src));\n+  effect(KILL cr);\n+\n+  format %{ \"bsfl    $dst, $src\\t# count trailing zeros (int)\\n\\t\"\n+            \"jnz     done\\n\\t\"\n+            \"movl    $dst, 32\\n\"\n+      \"done:\" %}\n+  ins_encode %{\n+    Register Rdst = $dst$$Register;\n+    Label done;\n+    __ bsfl(Rdst, $src$$Register);\n+    __ jccb(Assembler::notZero, done);\n+    __ movl(Rdst, BitsPerInt);\n+    __ bind(done);\n+  %}\n+  ins_pipe(ialu_reg);\n+%}\n+\n+instruct countTrailingZerosL(rRegI dst, rRegL src, rFlagsReg cr) %{\n+  predicate(UseCountTrailingZerosInstruction);\n+  match(Set dst (CountTrailingZerosL src));\n+  effect(KILL cr);\n+\n+  format %{ \"tzcntq    $dst, $src\\t# count trailing zeros (long)\" %}\n+  ins_encode %{\n+    __ tzcntq($dst$$Register, $src$$Register);\n+  %}\n+  ins_pipe(ialu_reg);\n+%}\n+\n+instruct countTrailingZerosL_mem(rRegI dst, memory src, rFlagsReg cr) %{\n+  predicate(UseCountTrailingZerosInstruction);\n+  match(Set dst (CountTrailingZerosL (LoadL src)));\n+  effect(KILL cr);\n+  ins_cost(175);\n+  format %{ \"tzcntq    $dst, $src\\t# count trailing zeros (long)\" %}\n+  ins_encode %{\n+    __ tzcntq($dst$$Register, $src$$Address);\n+  %}\n+  ins_pipe(ialu_reg_mem);\n+%}\n+\n+instruct countTrailingZerosL_bsf(rRegI dst, rRegL src, rFlagsReg cr) %{\n+  predicate(!UseCountTrailingZerosInstruction);\n+  match(Set dst (CountTrailingZerosL src));\n+  effect(KILL cr);\n+\n+  format %{ \"bsfq    $dst, $src\\t# count trailing zeros (long)\\n\\t\"\n+            \"jnz     done\\n\\t\"\n+            \"movl    $dst, 64\\n\"\n+      \"done:\" %}\n+  ins_encode %{\n+    Register Rdst = $dst$$Register;\n+    Label done;\n+    __ bsfq(Rdst, $src$$Register);\n+    __ jccb(Assembler::notZero, done);\n+    __ movl(Rdst, BitsPerLong);\n+    __ bind(done);\n+  %}\n+  ins_pipe(ialu_reg);\n+%}\n+\n+\/\/--------------- Reverse Operation Instructions ----------------\n+instruct bytes_reversebit_int(rRegI dst, rRegI src, rRegI rtmp, rFlagsReg cr) %{\n+  predicate(!VM_Version::supports_gfni());\n+  match(Set dst (ReverseI src));\n+  effect(TEMP dst, TEMP rtmp, KILL cr);\n+  format %{ \"reverse_int $dst $src\\t! using $rtmp as TEMP\" %}\n+  ins_encode %{\n+    __ reverseI($dst$$Register, $src$$Register, xnoreg, xnoreg, $rtmp$$Register);\n+  %}\n+  ins_pipe( ialu_reg );\n+%}\n+\n+instruct bytes_reversebit_int_gfni(rRegI dst, rRegI src, vlRegF xtmp1, vlRegF xtmp2, rRegL rtmp, rFlagsReg cr) %{\n+  predicate(VM_Version::supports_gfni());\n+  match(Set dst (ReverseI src));\n+  effect(TEMP dst, TEMP xtmp1, TEMP xtmp2, TEMP rtmp, KILL cr);\n+  format %{ \"reverse_int $dst $src\\t! using $rtmp, $xtmp1 and $xtmp2 as TEMP\" %}\n+  ins_encode %{\n+    __ reverseI($dst$$Register, $src$$Register, $xtmp1$$XMMRegister, $xtmp2$$XMMRegister, $rtmp$$Register);\n+  %}\n+  ins_pipe( ialu_reg );\n+%}\n+\n+instruct bytes_reversebit_long(rRegL dst, rRegL src, rRegL rtmp1, rRegL rtmp2, rFlagsReg cr) %{\n+  predicate(!VM_Version::supports_gfni());\n+  match(Set dst (ReverseL src));\n+  effect(TEMP dst, TEMP rtmp1, TEMP rtmp2, KILL cr);\n+  format %{ \"reverse_long $dst $src\\t! using $rtmp1 and $rtmp2 as TEMP\" %}\n+  ins_encode %{\n+    __ reverseL($dst$$Register, $src$$Register, xnoreg, xnoreg, $rtmp1$$Register, $rtmp2$$Register);\n+  %}\n+  ins_pipe( ialu_reg );\n+%}\n+\n+instruct bytes_reversebit_long_gfni(rRegL dst, rRegL src, vlRegD xtmp1, vlRegD xtmp2, rRegL rtmp, rFlagsReg cr) %{\n+  predicate(VM_Version::supports_gfni());\n+  match(Set dst (ReverseL src));\n+  effect(TEMP dst, TEMP xtmp1, TEMP xtmp2, TEMP rtmp, KILL cr);\n+  format %{ \"reverse_long $dst $src\\t! using $rtmp, $xtmp1 and $xtmp2 as TEMP\" %}\n+  ins_encode %{\n+    __ reverseL($dst$$Register, $src$$Register, $xtmp1$$XMMRegister, $xtmp2$$XMMRegister, $rtmp$$Register, noreg);\n+  %}\n+  ins_pipe( ialu_reg );\n+%}\n+\n+\/\/---------- Population Count Instructions -------------------------------------\n+\n+instruct popCountI(rRegI dst, rRegI src, rFlagsReg cr) %{\n+  predicate(UsePopCountInstruction);\n+  match(Set dst (PopCountI src));\n+  effect(KILL cr);\n+\n+  format %{ \"popcnt  $dst, $src\" %}\n+  ins_encode %{\n+    __ popcntl($dst$$Register, $src$$Register);\n+  %}\n+  ins_pipe(ialu_reg);\n+%}\n+\n+instruct popCountI_mem(rRegI dst, memory mem, rFlagsReg cr) %{\n+  predicate(UsePopCountInstruction);\n+  match(Set dst (PopCountI (LoadI mem)));\n+  effect(KILL cr);\n+\n+  format %{ \"popcnt  $dst, $mem\" %}\n+  ins_encode %{\n+    __ popcntl($dst$$Register, $mem$$Address);\n+  %}\n+  ins_pipe(ialu_reg);\n+%}\n+\n+\/\/ Note: Long.bitCount(long) returns an int.\n+instruct popCountL(rRegI dst, rRegL src, rFlagsReg cr) %{\n+  predicate(UsePopCountInstruction);\n+  match(Set dst (PopCountL src));\n+  effect(KILL cr);\n+\n+  format %{ \"popcnt  $dst, $src\" %}\n+  ins_encode %{\n+    __ popcntq($dst$$Register, $src$$Register);\n+  %}\n+  ins_pipe(ialu_reg);\n+%}\n+\n+\/\/ Note: Long.bitCount(long) returns an int.\n+instruct popCountL_mem(rRegI dst, memory mem, rFlagsReg cr) %{\n+  predicate(UsePopCountInstruction);\n+  match(Set dst (PopCountL (LoadL mem)));\n+  effect(KILL cr);\n+\n+  format %{ \"popcnt  $dst, $mem\" %}\n+  ins_encode %{\n+    __ popcntq($dst$$Register, $mem$$Address);\n+  %}\n+  ins_pipe(ialu_reg);\n+%}\n+\n+\n+\/\/----------MemBar Instructions-----------------------------------------------\n+\/\/ Memory barrier flavors\n+\n+instruct membar_acquire()\n+%{\n+  match(MemBarAcquire);\n+  match(LoadFence);\n+  ins_cost(0);\n+\n+  size(0);\n+  format %{ \"MEMBAR-acquire ! (empty encoding)\" %}\n+  ins_encode();\n+  ins_pipe(empty);\n+%}\n+\n+instruct membar_acquire_lock()\n+%{\n+  match(MemBarAcquireLock);\n+  ins_cost(0);\n+\n+  size(0);\n+  format %{ \"MEMBAR-acquire (prior CMPXCHG in FastLock so empty encoding)\" %}\n+  ins_encode();\n+  ins_pipe(empty);\n+%}\n+\n+instruct membar_release()\n+%{\n+  match(MemBarRelease);\n+  match(StoreFence);\n+  ins_cost(0);\n+\n+  size(0);\n+  format %{ \"MEMBAR-release ! (empty encoding)\" %}\n+  ins_encode();\n+  ins_pipe(empty);\n+%}\n+\n+instruct membar_release_lock()\n+%{\n+  match(MemBarReleaseLock);\n+  ins_cost(0);\n+\n+  size(0);\n+  format %{ \"MEMBAR-release (a FastUnlock follows so empty encoding)\" %}\n+  ins_encode();\n+  ins_pipe(empty);\n+%}\n+\n+instruct membar_volatile(rFlagsReg cr) %{\n+  match(MemBarVolatile);\n+  effect(KILL cr);\n+  ins_cost(400);\n+\n+  format %{\n+    $$template\n+    $$emit$$\"lock addl [rsp + #0], 0\\t! membar_volatile\"\n+  %}\n+  ins_encode %{\n+    __ membar(Assembler::StoreLoad);\n+  %}\n+  ins_pipe(pipe_slow);\n+%}\n+\n+instruct unnecessary_membar_volatile()\n+%{\n+  match(MemBarVolatile);\n+  predicate(Matcher::post_store_load_barrier(n));\n+  ins_cost(0);\n+\n+  size(0);\n+  format %{ \"MEMBAR-volatile (unnecessary so empty encoding)\" %}\n+  ins_encode();\n+  ins_pipe(empty);\n+%}\n+\n+instruct membar_storestore() %{\n+  match(MemBarStoreStore);\n+  match(StoreStoreFence);\n+  ins_cost(0);\n+\n+  size(0);\n+  format %{ \"MEMBAR-storestore (empty encoding)\" %}\n+  ins_encode( );\n+  ins_pipe(empty);\n+%}\n+\n+\/\/----------Move Instructions--------------------------------------------------\n+\n+instruct castX2P(rRegP dst, rRegL src)\n+%{\n+  match(Set dst (CastX2P src));\n+\n+  format %{ \"movq    $dst, $src\\t# long->ptr\" %}\n+  ins_encode %{\n+    if ($dst$$reg != $src$$reg) {\n+      __ movptr($dst$$Register, $src$$Register);\n+    }\n+  %}\n+  ins_pipe(ialu_reg_reg); \/\/ XXX\n+%}\n+\n+instruct castP2X(rRegL dst, rRegP src)\n+%{\n+  match(Set dst (CastP2X src));\n+\n+  format %{ \"movq    $dst, $src\\t# ptr -> long\" %}\n+  ins_encode %{\n+    if ($dst$$reg != $src$$reg) {\n+      __ movptr($dst$$Register, $src$$Register);\n+    }\n+  %}\n+  ins_pipe(ialu_reg_reg); \/\/ XXX\n+%}\n+\n+\/\/ Convert oop into int for vectors alignment masking\n+instruct convP2I(rRegI dst, rRegP src)\n+%{\n+  match(Set dst (ConvL2I (CastP2X src)));\n+\n+  format %{ \"movl    $dst, $src\\t# ptr -> int\" %}\n+  ins_encode %{\n+    __ movl($dst$$Register, $src$$Register);\n+  %}\n+  ins_pipe(ialu_reg_reg); \/\/ XXX\n+%}\n+\n+\/\/ Convert compressed oop into int for vectors alignment masking\n+\/\/ in case of 32bit oops (heap < 4Gb).\n+instruct convN2I(rRegI dst, rRegN src)\n+%{\n+  predicate(CompressedOops::shift() == 0);\n+  match(Set dst (ConvL2I (CastP2X (DecodeN src))));\n+\n+  format %{ \"movl    $dst, $src\\t# compressed ptr -> int\" %}\n+  ins_encode %{\n+    __ movl($dst$$Register, $src$$Register);\n+  %}\n+  ins_pipe(ialu_reg_reg); \/\/ XXX\n+%}\n+\n+\/\/ Convert oop pointer into compressed form\n+instruct encodeHeapOop(rRegN dst, rRegP src, rFlagsReg cr) %{\n+  predicate(n->bottom_type()->make_ptr()->ptr() != TypePtr::NotNull);\n+  match(Set dst (EncodeP src));\n+  effect(KILL cr);\n+  format %{ \"encode_heap_oop $dst,$src\" %}\n+  ins_encode %{\n+    Register s = $src$$Register;\n+    Register d = $dst$$Register;\n+    if (s != d) {\n+      __ movq(d, s);\n+    }\n+    __ encode_heap_oop(d);\n+  %}\n+  ins_pipe(ialu_reg_long);\n+%}\n+\n+instruct encodeHeapOop_not_null(rRegN dst, rRegP src, rFlagsReg cr) %{\n+  predicate(n->bottom_type()->make_ptr()->ptr() == TypePtr::NotNull);\n+  match(Set dst (EncodeP src));\n+  effect(KILL cr);\n+  format %{ \"encode_heap_oop_not_null $dst,$src\" %}\n+  ins_encode %{\n+    __ encode_heap_oop_not_null($dst$$Register, $src$$Register);\n+  %}\n+  ins_pipe(ialu_reg_long);\n+%}\n+\n+instruct decodeHeapOop(rRegP dst, rRegN src, rFlagsReg cr) %{\n+  predicate(n->bottom_type()->is_ptr()->ptr() != TypePtr::NotNull &&\n+            n->bottom_type()->is_ptr()->ptr() != TypePtr::Constant);\n+  match(Set dst (DecodeN src));\n+  effect(KILL cr);\n+  format %{ \"decode_heap_oop $dst,$src\" %}\n+  ins_encode %{\n+    Register s = $src$$Register;\n+    Register d = $dst$$Register;\n+    if (s != d) {\n+      __ movq(d, s);\n+    }\n+    __ decode_heap_oop(d);\n+  %}\n+  ins_pipe(ialu_reg_long);\n+%}\n+\n+instruct decodeHeapOop_not_null(rRegP dst, rRegN src, rFlagsReg cr) %{\n+  predicate(n->bottom_type()->is_ptr()->ptr() == TypePtr::NotNull ||\n+            n->bottom_type()->is_ptr()->ptr() == TypePtr::Constant);\n+  match(Set dst (DecodeN src));\n+  effect(KILL cr);\n+  format %{ \"decode_heap_oop_not_null $dst,$src\" %}\n+  ins_encode %{\n+    Register s = $src$$Register;\n+    Register d = $dst$$Register;\n+    if (s != d) {\n+      __ decode_heap_oop_not_null(d, s);\n+    } else {\n+      __ decode_heap_oop_not_null(d);\n+    }\n+  %}\n+  ins_pipe(ialu_reg_long);\n+%}\n+\n+instruct encodeKlass_not_null(rRegN dst, rRegP src, rFlagsReg cr) %{\n+  match(Set dst (EncodePKlass src));\n+  effect(TEMP dst, KILL cr);\n+  format %{ \"encode_and_move_klass_not_null $dst,$src\" %}\n+  ins_encode %{\n+    __ encode_and_move_klass_not_null($dst$$Register, $src$$Register);\n+  %}\n+  ins_pipe(ialu_reg_long);\n+%}\n+\n+instruct decodeKlass_not_null(rRegP dst, rRegN src, rFlagsReg cr) %{\n+  match(Set dst (DecodeNKlass src));\n+  effect(TEMP dst, KILL cr);\n+  format %{ \"decode_and_move_klass_not_null $dst,$src\" %}\n+  ins_encode %{\n+    __ decode_and_move_klass_not_null($dst$$Register, $src$$Register);\n+  %}\n+  ins_pipe(ialu_reg_long);\n+%}\n+\n+\/\/----------Conditional Move---------------------------------------------------\n+\/\/ Jump\n+\/\/ dummy instruction for generating temp registers\n+instruct jumpXtnd_offset(rRegL switch_val, immI2 shift, rRegI dest) %{\n+  match(Jump (LShiftL switch_val shift));\n+  ins_cost(350);\n+  predicate(false);\n+  effect(TEMP dest);\n+\n+  format %{ \"leaq    $dest, [$constantaddress]\\n\\t\"\n+            \"jmp     [$dest + $switch_val << $shift]\\n\\t\" %}\n+  ins_encode %{\n+    \/\/ We could use jump(ArrayAddress) except that the macro assembler needs to use r10\n+    \/\/ to do that and the compiler is using that register as one it can allocate.\n+    \/\/ So we build it all by hand.\n+    \/\/ Address index(noreg, switch_reg, (Address::ScaleFactor)$shift$$constant);\n+    \/\/ ArrayAddress dispatch(table, index);\n+    Address dispatch($dest$$Register, $switch_val$$Register, (Address::ScaleFactor) $shift$$constant);\n+    __ lea($dest$$Register, $constantaddress);\n+    __ jmp(dispatch);\n+  %}\n+  ins_pipe(pipe_jmp);\n+%}\n+\n+instruct jumpXtnd_addr(rRegL switch_val, immI2 shift, immL32 offset, rRegI dest) %{\n+  match(Jump (AddL (LShiftL switch_val shift) offset));\n+  ins_cost(350);\n+  effect(TEMP dest);\n+\n+  format %{ \"leaq    $dest, [$constantaddress]\\n\\t\"\n+            \"jmp     [$dest + $switch_val << $shift + $offset]\\n\\t\" %}\n+  ins_encode %{\n+    \/\/ We could use jump(ArrayAddress) except that the macro assembler needs to use r10\n+    \/\/ to do that and the compiler is using that register as one it can allocate.\n+    \/\/ So we build it all by hand.\n+    \/\/ Address index(noreg, switch_reg, (Address::ScaleFactor) $shift$$constant, (int) $offset$$constant);\n+    \/\/ ArrayAddress dispatch(table, index);\n+    Address dispatch($dest$$Register, $switch_val$$Register, (Address::ScaleFactor) $shift$$constant, (int) $offset$$constant);\n+    __ lea($dest$$Register, $constantaddress);\n+    __ jmp(dispatch);\n+  %}\n+  ins_pipe(pipe_jmp);\n+%}\n+\n+instruct jumpXtnd(rRegL switch_val, rRegI dest) %{\n+  match(Jump switch_val);\n+  ins_cost(350);\n+  effect(TEMP dest);\n+\n+  format %{ \"leaq    $dest, [$constantaddress]\\n\\t\"\n+            \"jmp     [$dest + $switch_val]\\n\\t\" %}\n+  ins_encode %{\n+    \/\/ We could use jump(ArrayAddress) except that the macro assembler needs to use r10\n+    \/\/ to do that and the compiler is using that register as one it can allocate.\n+    \/\/ So we build it all by hand.\n+    \/\/ Address index(noreg, switch_reg, Address::times_1);\n+    \/\/ ArrayAddress dispatch(table, index);\n+    Address dispatch($dest$$Register, $switch_val$$Register, Address::times_1);\n+    __ lea($dest$$Register, $constantaddress);\n+    __ jmp(dispatch);\n+  %}\n+  ins_pipe(pipe_jmp);\n+%}\n+\n+\/\/ Conditional move\n+instruct cmovI_imm_01(rRegI dst, immI_1 src, rFlagsReg cr, cmpOp cop)\n+%{\n+  predicate(n->in(2)->in(2)->is_Con() && n->in(2)->in(2)->get_int() == 0);\n+  match(Set dst (CMoveI (Binary cop cr) (Binary src dst)));\n+\n+  ins_cost(100); \/\/ XXX\n+  format %{ \"setbn$cop $dst\\t# signed, int\" %}\n+  ins_encode %{\n+    Assembler::Condition cond = (Assembler::Condition)($cop$$cmpcode);\n+    __ setb(MacroAssembler::negate_condition(cond), $dst$$Register);\n+  %}\n+  ins_pipe(ialu_reg);\n+%}\n+\n+instruct cmovI_reg(rRegI dst, rRegI src, rFlagsReg cr, cmpOp cop)\n+%{\n+  predicate(!UseAPX);\n+  match(Set dst (CMoveI (Binary cop cr) (Binary dst src)));\n+\n+  ins_cost(200); \/\/ XXX\n+  format %{ \"cmovl$cop $dst, $src\\t# signed, int\" %}\n+  ins_encode %{\n+    __ cmovl((Assembler::Condition)($cop$$cmpcode), $dst$$Register, $src$$Register);\n+  %}\n+  ins_pipe(pipe_cmov_reg);\n+%}\n+\n+instruct cmovI_reg_ndd(rRegI dst, rRegI src1, rRegI src2, rFlagsReg cr, cmpOp cop)\n+%{\n+  predicate(UseAPX);\n+  match(Set dst (CMoveI (Binary cop cr) (Binary src1 src2)));\n+\n+  ins_cost(200);\n+  format %{ \"ecmovl$cop $dst, $src1, $src2\\t# signed, int ndd\" %}\n+  ins_encode %{\n+    __ ecmovl((Assembler::Condition)($cop$$cmpcode), $dst$$Register, $src1$$Register, $src2$$Register);\n+  %}\n+  ins_pipe(pipe_cmov_reg);\n+%}\n+\n+instruct cmovI_imm_01U(rRegI dst, immI_1 src, rFlagsRegU cr, cmpOpU cop)\n+%{\n+  predicate(n->in(2)->in(2)->is_Con() && n->in(2)->in(2)->get_int() == 0);\n+  match(Set dst (CMoveI (Binary cop cr) (Binary src dst)));\n+\n+  ins_cost(100); \/\/ XXX\n+  format %{ \"setbn$cop $dst\\t# unsigned, int\" %}\n+  ins_encode %{\n+    Assembler::Condition cond = (Assembler::Condition)($cop$$cmpcode);\n+    __ setb(MacroAssembler::negate_condition(cond), $dst$$Register);\n+  %}\n+  ins_pipe(ialu_reg);\n+%}\n+\n+instruct cmovI_regU(cmpOpU cop, rFlagsRegU cr, rRegI dst, rRegI src) %{\n+  predicate(!UseAPX);\n+  match(Set dst (CMoveI (Binary cop cr) (Binary dst src)));\n+\n+  ins_cost(200); \/\/ XXX\n+  format %{ \"cmovl$cop $dst, $src\\t# unsigned, int\" %}\n+  ins_encode %{\n+    __ cmovl((Assembler::Condition)($cop$$cmpcode), $dst$$Register, $src$$Register);\n+  %}\n+  ins_pipe(pipe_cmov_reg);\n+%}\n+\n+instruct cmovI_regU_ndd(rRegI dst, cmpOpU cop, rFlagsRegU cr, rRegI src1, rRegI src2) %{\n+  predicate(UseAPX);\n+  match(Set dst (CMoveI (Binary cop cr) (Binary src1 src2)));\n+\n+  ins_cost(200);\n+  format %{ \"ecmovl$cop $dst, $src1, $src2\\t# unsigned, int ndd\" %}\n+  ins_encode %{\n+    __ ecmovl((Assembler::Condition)($cop$$cmpcode), $dst$$Register, $src1$$Register, $src2$$Register);\n+  %}\n+  ins_pipe(pipe_cmov_reg);\n+%}\n+\n+instruct cmovI_imm_01UCF(rRegI dst, immI_1 src, rFlagsRegUCF cr, cmpOpUCF cop)\n+%{\n+  predicate(n->in(2)->in(2)->is_Con() && n->in(2)->in(2)->get_int() == 0);\n+  match(Set dst (CMoveI (Binary cop cr) (Binary src dst)));\n+\n+  ins_cost(100); \/\/ XXX\n+  format %{ \"setbn$cop $dst\\t# unsigned, int\" %}\n+  ins_encode %{\n+    Assembler::Condition cond = (Assembler::Condition)($cop$$cmpcode);\n+    __ setb(MacroAssembler::negate_condition(cond), $dst$$Register);\n+  %}\n+  ins_pipe(ialu_reg);\n+%}\n+\n+instruct cmovI_regUCF(cmpOpUCF cop, rFlagsRegUCF cr, rRegI dst, rRegI src) %{\n+  predicate(!UseAPX);\n+  match(Set dst (CMoveI (Binary cop cr) (Binary dst src)));\n+  ins_cost(200);\n+  expand %{\n+    cmovI_regU(cop, cr, dst, src);\n+  %}\n+%}\n+\n+instruct cmovI_regUCF_ndd(rRegI dst, cmpOpUCF cop, rFlagsRegUCF cr, rRegI src1, rRegI src2) %{\n+  predicate(UseAPX);\n+  match(Set dst (CMoveI (Binary cop cr) (Binary src1 src2)));\n+  ins_cost(200);\n+  format %{ \"ecmovl$cop $dst, $src1, $src2\\t# unsigned, int ndd\" %}\n+  ins_encode %{\n+    __ ecmovl((Assembler::Condition)($cop$$cmpcode), $dst$$Register, $src1$$Register, $src2$$Register);\n+  %}\n+  ins_pipe(pipe_cmov_reg);\n+%}\n+\n+instruct cmovI_regUCF2_ne(cmpOpUCF2 cop, rFlagsRegUCF cr, rRegI dst, rRegI src) %{\n+  predicate(!UseAPX && n->in(1)->in(1)->as_Bool()->_test._test == BoolTest::ne);\n+  match(Set dst (CMoveI (Binary cop cr) (Binary dst src)));\n+\n+  ins_cost(200); \/\/ XXX\n+  format %{ \"cmovpl  $dst, $src\\n\\t\"\n+            \"cmovnel $dst, $src\" %}\n+  ins_encode %{\n+    __ cmovl(Assembler::parity, $dst$$Register, $src$$Register);\n+    __ cmovl(Assembler::notEqual, $dst$$Register, $src$$Register);\n+  %}\n+  ins_pipe(pipe_cmov_reg);\n+%}\n+\n+instruct cmovI_regUCF2_ne_ndd(cmpOpUCF2 cop, rFlagsRegUCF cr, rRegI dst, rRegI src1, rRegI src2) %{\n+  predicate(UseAPX && n->in(1)->in(1)->as_Bool()->_test._test == BoolTest::ne);\n+  match(Set dst (CMoveI (Binary cop cr) (Binary src1 src2)));\n+  effect(TEMP dst);\n+\n+  ins_cost(200);\n+  format %{ \"ecmovpl  $dst, $src1, $src2\\n\\t\"\n+            \"cmovnel  $dst, $src2\" %}\n+  ins_encode %{\n+    __ ecmovl(Assembler::parity, $dst$$Register, $src1$$Register, $src2$$Register);\n+    __ cmovl(Assembler::notEqual, $dst$$Register, $src2$$Register);\n+  %}\n+  ins_pipe(pipe_cmov_reg);\n+%}\n+\n+\/\/ Since (x == y) == !(x != y), we can flip the sense of the test by flipping the\n+\/\/ inputs of the CMove\n+instruct cmovI_regUCF2_eq(cmpOpUCF2 cop, rFlagsRegUCF cr, rRegI dst, rRegI src) %{\n+  predicate(!UseAPX && n->in(1)->in(1)->as_Bool()->_test._test == BoolTest::eq);\n+  match(Set dst (CMoveI (Binary cop cr) (Binary src dst)));\n+  effect(TEMP dst);\n+\n+  ins_cost(200); \/\/ XXX\n+  format %{ \"cmovpl  $dst, $src\\n\\t\"\n+            \"cmovnel $dst, $src\" %}\n+  ins_encode %{\n+    __ cmovl(Assembler::parity, $dst$$Register, $src$$Register);\n+    __ cmovl(Assembler::notEqual, $dst$$Register, $src$$Register);\n+  %}\n+  ins_pipe(pipe_cmov_reg);\n+%}\n+\n+\/\/ We need this special handling for only eq \/ neq comparison since NaN == NaN is false,\n+\/\/ and parity flag bit is set if any of the operand is a NaN.\n+instruct cmovI_regUCF2_eq_ndd(cmpOpUCF2 cop, rFlagsRegUCF cr, rRegI dst, rRegI src1, rRegI src2) %{\n+  predicate(UseAPX && n->in(1)->in(1)->as_Bool()->_test._test == BoolTest::eq);\n+  match(Set dst (CMoveI (Binary cop cr) (Binary src2 src1)));\n+  effect(TEMP dst);\n+\n+  ins_cost(200);\n+  format %{ \"ecmovpl  $dst, $src1, $src2\\n\\t\"\n+            \"cmovnel  $dst, $src2\" %}\n+  ins_encode %{\n+    __ ecmovl(Assembler::parity, $dst$$Register, $src1$$Register, $src2$$Register);\n+    __ cmovl(Assembler::notEqual, $dst$$Register, $src2$$Register);\n+  %}\n+  ins_pipe(pipe_cmov_reg);\n+%}\n+\n+\/\/ Conditional move\n+instruct cmovI_mem(cmpOp cop, rFlagsReg cr, rRegI dst, memory src) %{\n+  predicate(!UseAPX);\n+  match(Set dst (CMoveI (Binary cop cr) (Binary dst (LoadI src))));\n+\n+  ins_cost(250); \/\/ XXX\n+  format %{ \"cmovl$cop $dst, $src\\t# signed, int\" %}\n+  ins_encode %{\n+    __ cmovl((Assembler::Condition)($cop$$cmpcode), $dst$$Register, $src$$Address);\n+  %}\n+  ins_pipe(pipe_cmov_mem);\n+%}\n+\n+\/\/ Conditional move\n+instruct cmovI_rReg_rReg_mem_ndd(rRegI dst, cmpOp cop, rFlagsReg cr, rRegI src1, memory src2)\n+%{\n+  predicate(UseAPX);\n+  match(Set dst (CMoveI (Binary cop cr) (Binary src1 (LoadI src2))));\n+\n+  ins_cost(250);\n+  format %{ \"ecmovl$cop $dst, $src1, $src2\\t# signed, int ndd\" %}\n+  ins_encode %{\n+    __ ecmovl((Assembler::Condition)($cop$$cmpcode), $dst$$Register, $src1$$Register, $src2$$Address);\n+  %}\n+  ins_pipe(pipe_cmov_mem);\n+%}\n+\n+\/\/ Conditional move\n+instruct cmovI_memU(cmpOpU cop, rFlagsRegU cr, rRegI dst, memory src)\n+%{\n+  predicate(!UseAPX);\n+  match(Set dst (CMoveI (Binary cop cr) (Binary dst (LoadI src))));\n+\n+  ins_cost(250); \/\/ XXX\n+  format %{ \"cmovl$cop $dst, $src\\t# unsigned, int\" %}\n+  ins_encode %{\n+    __ cmovl((Assembler::Condition)($cop$$cmpcode), $dst$$Register, $src$$Address);\n+  %}\n+  ins_pipe(pipe_cmov_mem);\n+%}\n+\n+instruct cmovI_memUCF(cmpOpUCF cop, rFlagsRegUCF cr, rRegI dst, memory src) %{\n+  predicate(!UseAPX);\n+  match(Set dst (CMoveI (Binary cop cr) (Binary dst (LoadI src))));\n+  ins_cost(250);\n+  expand %{\n+    cmovI_memU(cop, cr, dst, src);\n+  %}\n+%}\n+\n+instruct cmovI_rReg_rReg_memU_ndd(rRegI dst, cmpOpU cop, rFlagsRegU cr, rRegI src1, memory src2)\n+%{\n+  predicate(UseAPX);\n+  match(Set dst (CMoveI (Binary cop cr) (Binary src1 (LoadI src2))));\n+\n+  ins_cost(250);\n+  format %{ \"ecmovl$cop $dst, $src1, $src2\\t# unsigned, int ndd\" %}\n+  ins_encode %{\n+    __ ecmovl((Assembler::Condition)($cop$$cmpcode), $dst$$Register, $src1$$Register, $src2$$Address);\n+  %}\n+  ins_pipe(pipe_cmov_mem);\n+%}\n+\n+instruct cmovI_rReg_rReg_memUCF_ndd(rRegI dst, cmpOpUCF cop, rFlagsRegUCF cr, rRegI src1, memory src2)\n+%{\n+  predicate(UseAPX);\n+  match(Set dst (CMoveI (Binary cop cr) (Binary src1 (LoadI src2))));\n+  ins_cost(250);\n+  format %{ \"ecmovl$cop $dst, $src1, $src2\\t# unsigned, int ndd\" %}\n+  ins_encode %{\n+    __ ecmovl((Assembler::Condition)($cop$$cmpcode), $dst$$Register, $src1$$Register, $src2$$Address);\n+  %}\n+  ins_pipe(pipe_cmov_mem);\n+%}\n+\n+\/\/ Conditional move\n+instruct cmovN_reg(rRegN dst, rRegN src, rFlagsReg cr, cmpOp cop)\n+%{\n+  predicate(!UseAPX);\n+  match(Set dst (CMoveN (Binary cop cr) (Binary dst src)));\n+\n+  ins_cost(200); \/\/ XXX\n+  format %{ \"cmovl$cop $dst, $src\\t# signed, compressed ptr\" %}\n+  ins_encode %{\n+    __ cmovl((Assembler::Condition)($cop$$cmpcode), $dst$$Register, $src$$Register);\n+  %}\n+  ins_pipe(pipe_cmov_reg);\n+%}\n+\n+\/\/ Conditional move ndd\n+instruct cmovN_reg_ndd(rRegN dst, rRegN src1, rRegN src2, rFlagsReg cr, cmpOp cop)\n+%{\n+  predicate(UseAPX);\n+  match(Set dst (CMoveN (Binary cop cr) (Binary src1 src2)));\n+\n+  ins_cost(200);\n+  format %{ \"ecmovl$cop $dst, $src1, $src2\\t# signed, compressed ptr ndd\" %}\n+  ins_encode %{\n+    __ ecmovl((Assembler::Condition)($cop$$cmpcode), $dst$$Register, $src1$$Register, $src2$$Register);\n+  %}\n+  ins_pipe(pipe_cmov_reg);\n+%}\n+\n+\/\/ Conditional move\n+instruct cmovN_regU(cmpOpU cop, rFlagsRegU cr, rRegN dst, rRegN src)\n+%{\n+  predicate(!UseAPX);\n+  match(Set dst (CMoveN (Binary cop cr) (Binary dst src)));\n+\n+  ins_cost(200); \/\/ XXX\n+  format %{ \"cmovl$cop $dst, $src\\t# unsigned, compressed ptr\" %}\n+  ins_encode %{\n+    __ cmovl((Assembler::Condition)($cop$$cmpcode), $dst$$Register, $src$$Register);\n+  %}\n+  ins_pipe(pipe_cmov_reg);\n+%}\n+\n+instruct cmovN_regUCF(cmpOpUCF cop, rFlagsRegUCF cr, rRegN dst, rRegN src) %{\n+  predicate(!UseAPX);\n+  match(Set dst (CMoveN (Binary cop cr) (Binary dst src)));\n+  ins_cost(200);\n+  expand %{\n+    cmovN_regU(cop, cr, dst, src);\n+  %}\n+%}\n+\n+\/\/ Conditional move ndd\n+instruct cmovN_regU_ndd(rRegN dst, cmpOpU cop, rFlagsRegU cr, rRegN src1, rRegN src2)\n+%{\n+  predicate(UseAPX);\n+  match(Set dst (CMoveN (Binary cop cr) (Binary src1 src2)));\n+\n+  ins_cost(200);\n+  format %{ \"ecmovl$cop $dst, $src1, $src2\\t# unsigned, compressed ptr ndd\" %}\n+  ins_encode %{\n+    __ ecmovl((Assembler::Condition)($cop$$cmpcode), $dst$$Register, $src1$$Register, $src2$$Register);\n+  %}\n+  ins_pipe(pipe_cmov_reg);\n+%}\n+\n+instruct cmovN_regUCF_ndd(rRegN dst, cmpOpUCF cop, rFlagsRegUCF cr, rRegN src1, rRegN src2) %{\n+  predicate(UseAPX);\n+  match(Set dst (CMoveN (Binary cop cr) (Binary src1 src2)));\n+  ins_cost(200);\n+  format %{ \"ecmovl$cop $dst, $src1, $src2\\t# unsigned, compressed ptr ndd\" %}\n+  ins_encode %{\n+    __ ecmovl((Assembler::Condition)($cop$$cmpcode), $dst$$Register, $src1$$Register, $src2$$Register);\n+  %}\n+  ins_pipe(pipe_cmov_reg);\n+%}\n+\n+instruct cmovN_regUCF2_ne(cmpOpUCF2 cop, rFlagsRegUCF cr, rRegN dst, rRegN src) %{\n+  predicate(n->in(1)->in(1)->as_Bool()->_test._test == BoolTest::ne);\n+  match(Set dst (CMoveN (Binary cop cr) (Binary dst src)));\n+\n+  ins_cost(200); \/\/ XXX\n+  format %{ \"cmovpl  $dst, $src\\n\\t\"\n+            \"cmovnel $dst, $src\" %}\n+  ins_encode %{\n+    __ cmovl(Assembler::parity, $dst$$Register, $src$$Register);\n+    __ cmovl(Assembler::notEqual, $dst$$Register, $src$$Register);\n+  %}\n+  ins_pipe(pipe_cmov_reg);\n+%}\n+\n+\/\/ Since (x == y) == !(x != y), we can flip the sense of the test by flipping the\n+\/\/ inputs of the CMove\n+instruct cmovN_regUCF2_eq(cmpOpUCF2 cop, rFlagsRegUCF cr, rRegN dst, rRegN src) %{\n+  predicate(n->in(1)->in(1)->as_Bool()->_test._test == BoolTest::eq);\n+  match(Set dst (CMoveN (Binary cop cr) (Binary src dst)));\n+\n+  ins_cost(200); \/\/ XXX\n+  format %{ \"cmovpl  $dst, $src\\n\\t\"\n+            \"cmovnel $dst, $src\" %}\n+  ins_encode %{\n+    __ cmovl(Assembler::parity, $dst$$Register, $src$$Register);\n+    __ cmovl(Assembler::notEqual, $dst$$Register, $src$$Register);\n+  %}\n+  ins_pipe(pipe_cmov_reg);\n+%}\n+\n+\/\/ Conditional move\n+instruct cmovP_reg(rRegP dst, rRegP src, rFlagsReg cr, cmpOp cop)\n+%{\n+  predicate(!UseAPX);\n+  match(Set dst (CMoveP (Binary cop cr) (Binary dst src)));\n+\n+  ins_cost(200); \/\/ XXX\n+  format %{ \"cmovq$cop $dst, $src\\t# signed, ptr\" %}\n+  ins_encode %{\n+    __ cmovq((Assembler::Condition)($cop$$cmpcode), $dst$$Register, $src$$Register);\n+  %}\n+  ins_pipe(pipe_cmov_reg);  \/\/ XXX\n+%}\n+\n+\/\/ Conditional move ndd\n+instruct cmovP_reg_ndd(rRegP dst, rRegP src1, rRegP src2, rFlagsReg cr, cmpOp cop)\n+%{\n+  predicate(UseAPX);\n+  match(Set dst (CMoveP (Binary cop cr) (Binary src1 src2)));\n+\n+  ins_cost(200);\n+  format %{ \"ecmovq$cop $dst, $src1, $src2\\t# signed, ptr ndd\" %}\n+  ins_encode %{\n+    __ ecmovq((Assembler::Condition)($cop$$cmpcode), $dst$$Register, $src1$$Register, $src2$$Register);\n+  %}\n+  ins_pipe(pipe_cmov_reg);\n+%}\n+\n+\/\/ Conditional move\n+instruct cmovP_regU(cmpOpU cop, rFlagsRegU cr, rRegP dst, rRegP src)\n+%{\n+  predicate(!UseAPX);\n+  match(Set dst (CMoveP (Binary cop cr) (Binary dst src)));\n+\n+  ins_cost(200); \/\/ XXX\n+  format %{ \"cmovq$cop $dst, $src\\t# unsigned, ptr\" %}\n+  ins_encode %{\n+    __ cmovq((Assembler::Condition)($cop$$cmpcode), $dst$$Register, $src$$Register);\n+  %}\n+  ins_pipe(pipe_cmov_reg); \/\/ XXX\n+%}\n+\n+\/\/ Conditional move ndd\n+instruct cmovP_regU_ndd(rRegP dst, cmpOpU cop, rFlagsRegU cr, rRegP src1, rRegP src2)\n+%{\n+  predicate(UseAPX);\n+  match(Set dst (CMoveP (Binary cop cr) (Binary src1 src2)));\n+\n+  ins_cost(200);\n+  format %{ \"ecmovq$cop $dst, $src1, $src2\\t# unsigned, ptr ndd\" %}\n+  ins_encode %{\n+    __ ecmovq((Assembler::Condition)($cop$$cmpcode), $dst$$Register, $src1$$Register, $src2$$Register);\n+  %}\n+  ins_pipe(pipe_cmov_reg);\n+%}\n+\n+instruct cmovP_regUCF(cmpOpUCF cop, rFlagsRegUCF cr, rRegP dst, rRegP src) %{\n+  predicate(!UseAPX);\n+  match(Set dst (CMoveP (Binary cop cr) (Binary dst src)));\n+  ins_cost(200);\n+  expand %{\n+    cmovP_regU(cop, cr, dst, src);\n+  %}\n+%}\n+\n+instruct cmovP_regUCF_ndd(rRegP dst, cmpOpUCF cop, rFlagsRegUCF cr, rRegP src1, rRegP src2) %{\n+  predicate(UseAPX);\n+  match(Set dst (CMoveP (Binary cop cr) (Binary src1 src2)));\n+  ins_cost(200);\n+  format %{ \"ecmovq$cop $dst, $src1, $src2\\t# unsigned, ptr ndd\" %}\n+  ins_encode %{\n+    __ ecmovq((Assembler::Condition)($cop$$cmpcode), $dst$$Register, $src1$$Register, $src2$$Register);\n+  %}\n+  ins_pipe(pipe_cmov_reg);\n+%}\n+\n+instruct cmovP_regUCF2_ne(cmpOpUCF2 cop, rFlagsRegUCF cr, rRegP dst, rRegP src) %{\n+  predicate(!UseAPX && n->in(1)->in(1)->as_Bool()->_test._test == BoolTest::ne);\n+  match(Set dst (CMoveP (Binary cop cr) (Binary dst src)));\n+\n+  ins_cost(200); \/\/ XXX\n+  format %{ \"cmovpq  $dst, $src\\n\\t\"\n+            \"cmovneq $dst, $src\" %}\n+  ins_encode %{\n+    __ cmovq(Assembler::parity, $dst$$Register, $src$$Register);\n+    __ cmovq(Assembler::notEqual, $dst$$Register, $src$$Register);\n+  %}\n+  ins_pipe(pipe_cmov_reg);\n+%}\n+\n+instruct cmovP_regUCF2_ne_ndd(cmpOpUCF2 cop, rFlagsRegUCF cr, rRegP dst, rRegP src1, rRegP src2) %{\n+  predicate(UseAPX && n->in(1)->in(1)->as_Bool()->_test._test == BoolTest::ne);\n+  match(Set dst (CMoveP (Binary cop cr) (Binary src1 src2)));\n+  effect(TEMP dst);\n+\n+  ins_cost(200);\n+  format %{ \"ecmovpq  $dst, $src1, $src2\\n\\t\"\n+            \"cmovneq  $dst, $src2\" %}\n+  ins_encode %{\n+    __ ecmovq(Assembler::parity, $dst$$Register, $src1$$Register, $src2$$Register);\n+    __ cmovq(Assembler::notEqual, $dst$$Register, $src2$$Register);\n+  %}\n+  ins_pipe(pipe_cmov_reg);\n+%}\n+\n+\/\/ Since (x == y) == !(x != y), we can flip the sense of the test by flipping the\n+\/\/ inputs of the CMove\n+instruct cmovP_regUCF2_eq(cmpOpUCF2 cop, rFlagsRegUCF cr, rRegP dst, rRegP src) %{\n+  predicate(!UseAPX && n->in(1)->in(1)->as_Bool()->_test._test == BoolTest::eq);\n+  match(Set dst (CMoveP (Binary cop cr) (Binary src dst)));\n+\n+  ins_cost(200); \/\/ XXX\n+  format %{ \"cmovpq  $dst, $src\\n\\t\"\n+            \"cmovneq $dst, $src\" %}\n+  ins_encode %{\n+    __ cmovq(Assembler::parity, $dst$$Register, $src$$Register);\n+    __ cmovq(Assembler::notEqual, $dst$$Register, $src$$Register);\n+  %}\n+  ins_pipe(pipe_cmov_reg);\n+%}\n+\n+instruct cmovP_regUCF2_eq_ndd(cmpOpUCF2 cop, rFlagsRegUCF cr, rRegP dst, rRegP src1, rRegP src2) %{\n+  predicate(UseAPX && n->in(1)->in(1)->as_Bool()->_test._test == BoolTest::eq);\n+  match(Set dst (CMoveP (Binary cop cr) (Binary src2 src1)));\n+  effect(TEMP dst);\n+\n+  ins_cost(200);\n+  format %{ \"ecmovpq  $dst, $src1, $src2\\n\\t\"\n+            \"cmovneq  $dst, $src2\" %}\n+  ins_encode %{\n+    __ ecmovq(Assembler::parity, $dst$$Register, $src1$$Register, $src2$$Register);\n+    __ cmovq(Assembler::notEqual, $dst$$Register, $src2$$Register);\n+  %}\n+  ins_pipe(pipe_cmov_reg);\n+%}\n+\n+instruct cmovL_imm_01(rRegL dst, immL1 src, rFlagsReg cr, cmpOp cop)\n+%{\n+  predicate(n->in(2)->in(2)->is_Con() && n->in(2)->in(2)->get_long() == 0);\n+  match(Set dst (CMoveL (Binary cop cr) (Binary src dst)));\n+\n+  ins_cost(100); \/\/ XXX\n+  format %{ \"setbn$cop $dst\\t# signed, long\" %}\n+  ins_encode %{\n+    Assembler::Condition cond = (Assembler::Condition)($cop$$cmpcode);\n+    __ setb(MacroAssembler::negate_condition(cond), $dst$$Register);\n+  %}\n+  ins_pipe(ialu_reg);\n+%}\n+\n+instruct cmovL_reg(cmpOp cop, rFlagsReg cr, rRegL dst, rRegL src)\n+%{\n+  predicate(!UseAPX);\n+  match(Set dst (CMoveL (Binary cop cr) (Binary dst src)));\n+\n+  ins_cost(200); \/\/ XXX\n+  format %{ \"cmovq$cop $dst, $src\\t# signed, long\" %}\n+  ins_encode %{\n+    __ cmovq((Assembler::Condition)($cop$$cmpcode), $dst$$Register, $src$$Register);\n+  %}\n+  ins_pipe(pipe_cmov_reg);  \/\/ XXX\n+%}\n+\n+instruct cmovL_reg_ndd(rRegL dst, cmpOp cop, rFlagsReg cr, rRegL src1, rRegL src2)\n+%{\n+  predicate(UseAPX);\n+  match(Set dst (CMoveL (Binary cop cr) (Binary src1 src2)));\n+\n+  ins_cost(200);\n+  format %{ \"ecmovq$cop $dst, $src1, $src2\\t# signed, long ndd\" %}\n+  ins_encode %{\n+    __ ecmovq((Assembler::Condition)($cop$$cmpcode), $dst$$Register, $src1$$Register, $src2$$Register);\n+  %}\n+  ins_pipe(pipe_cmov_reg);\n+%}\n+\n+instruct cmovL_mem(cmpOp cop, rFlagsReg cr, rRegL dst, memory src)\n+%{\n+  predicate(!UseAPX);\n+  match(Set dst (CMoveL (Binary cop cr) (Binary dst (LoadL src))));\n+\n+  ins_cost(200); \/\/ XXX\n+  format %{ \"cmovq$cop $dst, $src\\t# signed, long\" %}\n+  ins_encode %{\n+    __ cmovq((Assembler::Condition)($cop$$cmpcode), $dst$$Register, $src$$Address);\n+  %}\n+  ins_pipe(pipe_cmov_mem);  \/\/ XXX\n+%}\n+\n+instruct cmovL_rReg_rReg_mem_ndd(rRegL dst, cmpOp cop, rFlagsReg cr, rRegL src1, memory src2)\n+%{\n+  predicate(UseAPX);\n+  match(Set dst (CMoveL (Binary cop cr) (Binary src1 (LoadL src2))));\n+\n+  ins_cost(200);\n+  format %{ \"ecmovq$cop $dst, $src1, $src2\\t# signed, long ndd\" %}\n+  ins_encode %{\n+    __ ecmovq((Assembler::Condition)($cop$$cmpcode), $dst$$Register, $src1$$Register, $src2$$Address);\n+  %}\n+  ins_pipe(pipe_cmov_mem);\n+%}\n+\n+instruct cmovL_imm_01U(rRegL dst, immL1 src, rFlagsRegU cr, cmpOpU cop)\n+%{\n+  predicate(n->in(2)->in(2)->is_Con() && n->in(2)->in(2)->get_long() == 0);\n+  match(Set dst (CMoveL (Binary cop cr) (Binary src dst)));\n+\n+  ins_cost(100); \/\/ XXX\n+  format %{ \"setbn$cop $dst\\t# unsigned, long\" %}\n+  ins_encode %{\n+    Assembler::Condition cond = (Assembler::Condition)($cop$$cmpcode);\n+    __ setb(MacroAssembler::negate_condition(cond), $dst$$Register);\n+  %}\n+  ins_pipe(ialu_reg);\n+%}\n+\n+instruct cmovL_regU(cmpOpU cop, rFlagsRegU cr, rRegL dst, rRegL src)\n+%{\n+  predicate(!UseAPX);\n+  match(Set dst (CMoveL (Binary cop cr) (Binary dst src)));\n+\n+  ins_cost(200); \/\/ XXX\n+  format %{ \"cmovq$cop $dst, $src\\t# unsigned, long\" %}\n+  ins_encode %{\n+    __ cmovq((Assembler::Condition)($cop$$cmpcode), $dst$$Register, $src$$Register);\n+  %}\n+  ins_pipe(pipe_cmov_reg); \/\/ XXX\n+%}\n+\n+instruct cmovL_regU_ndd(rRegL dst, cmpOpU cop, rFlagsRegU cr, rRegL src1, rRegL src2)\n+%{\n+  predicate(UseAPX);\n+  match(Set dst (CMoveL (Binary cop cr) (Binary src1 src2)));\n+\n+  ins_cost(200);\n+  format %{ \"ecmovq$cop $dst, $src1, $src2\\t# unsigned, long ndd\" %}\n+  ins_encode %{\n+    __ ecmovq((Assembler::Condition)($cop$$cmpcode), $dst$$Register, $src1$$Register, $src2$$Register);\n+  %}\n+  ins_pipe(pipe_cmov_reg);\n+%}\n+\n+instruct cmovL_imm_01UCF(rRegL dst, immL1 src, rFlagsRegUCF cr, cmpOpUCF cop)\n+%{\n+  predicate(n->in(2)->in(2)->is_Con() && n->in(2)->in(2)->get_long() == 0);\n+  match(Set dst (CMoveL (Binary cop cr) (Binary src dst)));\n+\n+  ins_cost(100); \/\/ XXX\n+  format %{ \"setbn$cop $dst\\t# unsigned, long\" %}\n+  ins_encode %{\n+    Assembler::Condition cond = (Assembler::Condition)($cop$$cmpcode);\n+    __ setb(MacroAssembler::negate_condition(cond), $dst$$Register);\n+  %}\n+  ins_pipe(ialu_reg);\n+%}\n+\n+instruct cmovL_regUCF(cmpOpUCF cop, rFlagsRegUCF cr, rRegL dst, rRegL src) %{\n+  predicate(!UseAPX);\n+  match(Set dst (CMoveL (Binary cop cr) (Binary dst src)));\n+  ins_cost(200);\n+  expand %{\n+    cmovL_regU(cop, cr, dst, src);\n+  %}\n+%}\n+\n+instruct cmovL_regUCF_ndd(rRegL dst, cmpOpUCF cop, rFlagsRegUCF cr, rRegL src1, rRegL src2)\n+%{\n+  predicate(UseAPX);\n+  match(Set dst (CMoveL (Binary cop cr) (Binary src1 src2)));\n+  ins_cost(200);\n+  format %{ \"ecmovq$cop $dst, $src1, $src2\\t# unsigned, long ndd\" %}\n+  ins_encode %{\n+    __ ecmovq((Assembler::Condition)($cop$$cmpcode), $dst$$Register, $src1$$Register, $src2$$Register);\n+  %}\n+  ins_pipe(pipe_cmov_reg);\n+%}\n+\n+instruct cmovL_regUCF2_ne(cmpOpUCF2 cop, rFlagsRegUCF cr, rRegL dst, rRegL src) %{\n+  predicate(!UseAPX && n->in(1)->in(1)->as_Bool()->_test._test == BoolTest::ne);\n+  match(Set dst (CMoveL (Binary cop cr) (Binary dst src)));\n+\n+  ins_cost(200); \/\/ XXX\n+  format %{ \"cmovpq  $dst, $src\\n\\t\"\n+            \"cmovneq $dst, $src\" %}\n+  ins_encode %{\n+    __ cmovq(Assembler::parity, $dst$$Register, $src$$Register);\n+    __ cmovq(Assembler::notEqual, $dst$$Register, $src$$Register);\n+  %}\n+  ins_pipe(pipe_cmov_reg);\n+%}\n+\n+instruct cmovL_regUCF2_ne_ndd(cmpOpUCF2 cop, rFlagsRegUCF cr, rRegL dst, rRegL src1, rRegL src2) %{\n+  predicate(UseAPX && n->in(1)->in(1)->as_Bool()->_test._test == BoolTest::ne);\n+  match(Set dst (CMoveL (Binary cop cr) (Binary src1 src2)));\n+  effect(TEMP dst);\n+\n+  ins_cost(200);\n+  format %{ \"ecmovpq  $dst, $src1, $src2\\n\\t\"\n+            \"cmovneq  $dst, $src2\" %}\n+  ins_encode %{\n+    __ ecmovq(Assembler::parity, $dst$$Register, $src1$$Register, $src2$$Register);\n+    __ cmovq(Assembler::notEqual, $dst$$Register, $src2$$Register);\n+  %}\n+  ins_pipe(pipe_cmov_reg);\n+%}\n+\n+\/\/ Since (x == y) == !(x != y), we can flip the sense of the test by flipping the\n+\/\/ inputs of the CMove\n+instruct cmovL_regUCF2_eq(cmpOpUCF2 cop, rFlagsRegUCF cr, rRegL dst, rRegL src) %{\n+  predicate(!UseAPX && n->in(1)->in(1)->as_Bool()->_test._test == BoolTest::eq);\n+  match(Set dst (CMoveL (Binary cop cr) (Binary src dst)));\n+\n+  ins_cost(200); \/\/ XXX\n+  format %{ \"cmovpq  $dst, $src\\n\\t\"\n+            \"cmovneq $dst, $src\" %}\n+  ins_encode %{\n+    __ cmovq(Assembler::parity, $dst$$Register, $src$$Register);\n+    __ cmovq(Assembler::notEqual, $dst$$Register, $src$$Register);\n+  %}\n+  ins_pipe(pipe_cmov_reg);\n+%}\n+\n+instruct cmovL_regUCF2_eq_ndd(cmpOpUCF2 cop, rFlagsRegUCF cr, rRegL dst, rRegL src1, rRegL src2) %{\n+  predicate(UseAPX && n->in(1)->in(1)->as_Bool()->_test._test == BoolTest::eq);\n+  match(Set dst (CMoveL (Binary cop cr) (Binary src2 src1)));\n+  effect(TEMP dst);\n+\n+  ins_cost(200);\n+  format %{ \"ecmovpq  $dst, $src1, $src2\\n\\t\"\n+            \"cmovneq $dst, $src2\" %}\n+  ins_encode %{\n+    __ ecmovq(Assembler::parity, $dst$$Register, $src1$$Register, $src2$$Register);\n+    __ cmovq(Assembler::notEqual, $dst$$Register, $src2$$Register);\n+  %}\n+  ins_pipe(pipe_cmov_reg);\n+%}\n+\n+instruct cmovL_memU(cmpOpU cop, rFlagsRegU cr, rRegL dst, memory src)\n+%{\n+  predicate(!UseAPX);\n+  match(Set dst (CMoveL (Binary cop cr) (Binary dst (LoadL src))));\n+\n+  ins_cost(200); \/\/ XXX\n+  format %{ \"cmovq$cop $dst, $src\\t# unsigned, long\" %}\n+  ins_encode %{\n+    __ cmovq((Assembler::Condition)($cop$$cmpcode), $dst$$Register, $src$$Address);\n+  %}\n+  ins_pipe(pipe_cmov_mem); \/\/ XXX\n+%}\n+\n+instruct cmovL_memUCF(cmpOpUCF cop, rFlagsRegUCF cr, rRegL dst, memory src) %{\n+  predicate(!UseAPX);\n+  match(Set dst (CMoveL (Binary cop cr) (Binary dst (LoadL src))));\n+  ins_cost(200);\n+  expand %{\n+    cmovL_memU(cop, cr, dst, src);\n+  %}\n+%}\n+\n+instruct cmovL_rReg_rReg_memU_ndd(rRegL dst, cmpOpU cop, rFlagsRegU cr, rRegL src1, memory src2)\n+%{\n+  predicate(UseAPX);\n+  match(Set dst (CMoveL (Binary cop cr) (Binary src1 (LoadL src2))));\n+\n+  ins_cost(200);\n+  format %{ \"ecmovq$cop $dst, $src1, $src2\\t# unsigned, long ndd\" %}\n+  ins_encode %{\n+    __ ecmovq((Assembler::Condition)($cop$$cmpcode), $dst$$Register, $src1$$Register, $src2$$Address);\n+  %}\n+  ins_pipe(pipe_cmov_mem);\n+%}\n+\n+instruct cmovL_rReg_rReg_memUCF_ndd(rRegL dst, cmpOpUCF cop, rFlagsRegUCF cr, rRegL src1, memory src2)\n+%{\n+  predicate(UseAPX);\n+  match(Set dst (CMoveL (Binary cop cr) (Binary src1 (LoadL src2))));\n+  ins_cost(200);\n+  format %{ \"ecmovq$cop $dst, $src1, $src2\\t# unsigned, long ndd\" %}\n+  ins_encode %{\n+    __ ecmovq((Assembler::Condition)($cop$$cmpcode), $dst$$Register, $src1$$Register, $src2$$Address);\n+  %}\n+  ins_pipe(pipe_cmov_mem);\n+%}\n+\n+instruct cmovF_reg(cmpOp cop, rFlagsReg cr, regF dst, regF src)\n+%{\n+  match(Set dst (CMoveF (Binary cop cr) (Binary dst src)));\n+\n+  ins_cost(200); \/\/ XXX\n+  format %{ \"jn$cop    skip\\t# signed cmove float\\n\\t\"\n+            \"movss     $dst, $src\\n\"\n+    \"skip:\" %}\n+  ins_encode %{\n+    Label Lskip;\n+    \/\/ Invert sense of branch from sense of CMOV\n+    __ jccb((Assembler::Condition)($cop$$cmpcode^1), Lskip);\n+    __ movflt($dst$$XMMRegister, $src$$XMMRegister);\n+    __ bind(Lskip);\n+  %}\n+  ins_pipe(pipe_slow);\n+%}\n+\n+instruct cmovF_regU(cmpOpU cop, rFlagsRegU cr, regF dst, regF src)\n+%{\n+  match(Set dst (CMoveF (Binary cop cr) (Binary dst src)));\n+\n+  ins_cost(200); \/\/ XXX\n+  format %{ \"jn$cop    skip\\t# unsigned cmove float\\n\\t\"\n+            \"movss     $dst, $src\\n\"\n+    \"skip:\" %}\n+  ins_encode %{\n+    Label Lskip;\n+    \/\/ Invert sense of branch from sense of CMOV\n+    __ jccb((Assembler::Condition)($cop$$cmpcode^1), Lskip);\n+    __ movflt($dst$$XMMRegister, $src$$XMMRegister);\n+    __ bind(Lskip);\n+  %}\n+  ins_pipe(pipe_slow);\n+%}\n+\n+instruct cmovF_regUCF(cmpOpUCF cop, rFlagsRegUCF cr, regF dst, regF src) %{\n+  match(Set dst (CMoveF (Binary cop cr) (Binary dst src)));\n+  ins_cost(200);\n+  expand %{\n+    cmovF_regU(cop, cr, dst, src);\n+  %}\n+%}\n+\n+instruct cmovD_reg(cmpOp cop, rFlagsReg cr, regD dst, regD src)\n+%{\n+  match(Set dst (CMoveD (Binary cop cr) (Binary dst src)));\n+\n+  ins_cost(200); \/\/ XXX\n+  format %{ \"jn$cop    skip\\t# signed cmove double\\n\\t\"\n+            \"movsd     $dst, $src\\n\"\n+    \"skip:\" %}\n+  ins_encode %{\n+    Label Lskip;\n+    \/\/ Invert sense of branch from sense of CMOV\n+    __ jccb((Assembler::Condition)($cop$$cmpcode^1), Lskip);\n+    __ movdbl($dst$$XMMRegister, $src$$XMMRegister);\n+    __ bind(Lskip);\n+  %}\n+  ins_pipe(pipe_slow);\n+%}\n+\n+instruct cmovD_regU(cmpOpU cop, rFlagsRegU cr, regD dst, regD src)\n+%{\n+  match(Set dst (CMoveD (Binary cop cr) (Binary dst src)));\n+\n+  ins_cost(200); \/\/ XXX\n+  format %{ \"jn$cop    skip\\t# unsigned cmove double\\n\\t\"\n+            \"movsd     $dst, $src\\n\"\n+    \"skip:\" %}\n+  ins_encode %{\n+    Label Lskip;\n+    \/\/ Invert sense of branch from sense of CMOV\n+    __ jccb((Assembler::Condition)($cop$$cmpcode^1), Lskip);\n+    __ movdbl($dst$$XMMRegister, $src$$XMMRegister);\n+    __ bind(Lskip);\n+  %}\n+  ins_pipe(pipe_slow);\n+%}\n+\n+instruct cmovD_regUCF(cmpOpUCF cop, rFlagsRegUCF cr, regD dst, regD src) %{\n+  match(Set dst (CMoveD (Binary cop cr) (Binary dst src)));\n+  ins_cost(200);\n+  expand %{\n+    cmovD_regU(cop, cr, dst, src);\n+  %}\n+%}\n+\n+\/\/----------Arithmetic Instructions--------------------------------------------\n+\/\/----------Addition Instructions----------------------------------------------\n+\n+instruct addI_rReg(rRegI dst, rRegI src, rFlagsReg cr)\n+%{\n+  predicate(!UseAPX);\n+  match(Set dst (AddI dst src));\n+  effect(KILL cr);\n+  flag(PD::Flag_sets_overflow_flag, PD::Flag_sets_sign_flag, PD::Flag_sets_zero_flag, PD::Flag_sets_carry_flag, PD::Flag_sets_parity_flag);\n+  format %{ \"addl    $dst, $src\\t# int\" %}\n+  ins_encode %{\n+    __ addl($dst$$Register, $src$$Register);\n+  %}\n+  ins_pipe(ialu_reg_reg);\n+%}\n+\n+instruct addI_rReg_ndd(rRegI dst, rRegI src1, rRegI src2, rFlagsReg cr)\n+%{\n+  predicate(UseAPX);\n+  match(Set dst (AddI src1 src2));\n+  effect(KILL cr);\n+  flag(PD::Flag_sets_overflow_flag, PD::Flag_sets_sign_flag, PD::Flag_sets_zero_flag, PD::Flag_sets_carry_flag, PD::Flag_sets_parity_flag);\n+\n+  format %{ \"eaddl    $dst, $src1, $src2\\t# int ndd\" %}\n+  ins_encode %{\n+    __ eaddl($dst$$Register, $src1$$Register, $src2$$Register, false);\n+  %}\n+  ins_pipe(ialu_reg_reg);\n+%}\n+\n+instruct addI_rReg_imm(rRegI dst, immI src, rFlagsReg cr)\n+%{\n+  predicate(!UseAPX);\n+  match(Set dst (AddI dst src));\n+  effect(KILL cr);\n+  flag(PD::Flag_sets_overflow_flag, PD::Flag_sets_sign_flag, PD::Flag_sets_zero_flag, PD::Flag_sets_carry_flag, PD::Flag_sets_parity_flag);\n+\n+  format %{ \"addl    $dst, $src\\t# int\" %}\n+  ins_encode %{\n+    __ addl($dst$$Register, $src$$constant);\n+  %}\n+  ins_pipe( ialu_reg );\n+%}\n+\n+instruct addI_rReg_rReg_imm_ndd(rRegI dst, rRegI src1, immI src2, rFlagsReg cr)\n+%{\n+  predicate(UseAPX);\n+  match(Set dst (AddI src1 src2));\n+  effect(KILL cr);\n+  flag(PD::Flag_sets_overflow_flag, PD::Flag_sets_sign_flag, PD::Flag_sets_zero_flag, PD::Flag_sets_carry_flag, PD::Flag_sets_parity_flag);\n+\n+  format %{ \"eaddl    $dst, $src1, $src2\\t# int ndd\" %}\n+  ins_encode %{\n+    __ eaddl($dst$$Register, $src1$$Register, $src2$$constant, false);\n+  %}\n+  ins_pipe( ialu_reg );\n+%}\n+\n+instruct addI_rReg_mem_imm_ndd(rRegI dst, memory src1, immI src2, rFlagsReg cr)\n+%{\n+  predicate(UseAPX);\n+  match(Set dst (AddI (LoadI src1) src2));\n+  effect(KILL cr);\n+  flag(PD::Flag_sets_overflow_flag, PD::Flag_sets_sign_flag, PD::Flag_sets_zero_flag, PD::Flag_sets_carry_flag, PD::Flag_sets_parity_flag);\n+\n+  format %{ \"eaddl    $dst, $src1, $src2\\t# int ndd\" %}\n+  ins_encode %{\n+    __ eaddl($dst$$Register, $src1$$Address, $src2$$constant, false);\n+  %}\n+  ins_pipe( ialu_reg );\n+%}\n+\n+instruct addI_rReg_mem(rRegI dst, memory src, rFlagsReg cr)\n+%{\n+  predicate(!UseAPX);\n+  match(Set dst (AddI dst (LoadI src)));\n+  effect(KILL cr);\n+  flag(PD::Flag_sets_overflow_flag, PD::Flag_sets_sign_flag, PD::Flag_sets_zero_flag, PD::Flag_sets_carry_flag, PD::Flag_sets_parity_flag);\n+\n+  ins_cost(150); \/\/ XXX\n+  format %{ \"addl    $dst, $src\\t# int\" %}\n+  ins_encode %{\n+    __ addl($dst$$Register, $src$$Address);\n+  %}\n+  ins_pipe(ialu_reg_mem);\n+%}\n+\n+instruct addI_rReg_rReg_mem_ndd(rRegI dst, rRegI src1, memory src2, rFlagsReg cr)\n+%{\n+  predicate(UseAPX);\n+  match(Set dst (AddI src1 (LoadI src2)));\n+  effect(KILL cr);\n+  flag(PD::Flag_sets_overflow_flag, PD::Flag_sets_sign_flag, PD::Flag_sets_zero_flag, PD::Flag_sets_carry_flag, PD::Flag_sets_parity_flag);\n+\n+  ins_cost(150);\n+  format %{ \"eaddl    $dst, $src1, $src2\\t# int ndd\" %}\n+  ins_encode %{\n+    __ eaddl($dst$$Register, $src1$$Register, $src2$$Address, false);\n+  %}\n+  ins_pipe(ialu_reg_mem);\n+%}\n+\n+instruct addI_mem_rReg(memory dst, rRegI src, rFlagsReg cr)\n+%{\n+  match(Set dst (StoreI dst (AddI (LoadI dst) src)));\n+  effect(KILL cr);\n+  flag(PD::Flag_sets_overflow_flag, PD::Flag_sets_sign_flag, PD::Flag_sets_zero_flag, PD::Flag_sets_carry_flag, PD::Flag_sets_parity_flag);\n+\n+  ins_cost(150); \/\/ XXX\n+  format %{ \"addl    $dst, $src\\t# int\" %}\n+  ins_encode %{\n+    __ addl($dst$$Address, $src$$Register);\n+  %}\n+  ins_pipe(ialu_mem_reg);\n+%}\n+\n+instruct addI_mem_imm(memory dst, immI src, rFlagsReg cr)\n+%{\n+  match(Set dst (StoreI dst (AddI (LoadI dst) src)));\n+  effect(KILL cr);\n+  flag(PD::Flag_sets_overflow_flag, PD::Flag_sets_sign_flag, PD::Flag_sets_zero_flag, PD::Flag_sets_carry_flag, PD::Flag_sets_parity_flag);\n+\n+\n+  ins_cost(125); \/\/ XXX\n+  format %{ \"addl    $dst, $src\\t# int\" %}\n+  ins_encode %{\n+    __ addl($dst$$Address, $src$$constant);\n+  %}\n+  ins_pipe(ialu_mem_imm);\n+%}\n+\n+instruct incI_rReg(rRegI dst, immI_1 src, rFlagsReg cr)\n+%{\n+  predicate(!UseAPX && UseIncDec);\n+  match(Set dst (AddI dst src));\n+  effect(KILL cr);\n+\n+  format %{ \"incl    $dst\\t# int\" %}\n+  ins_encode %{\n+    __ incrementl($dst$$Register);\n+  %}\n+  ins_pipe(ialu_reg);\n+%}\n+\n+instruct incI_rReg_ndd(rRegI dst, rRegI src, immI_1 val, rFlagsReg cr)\n+%{\n+  predicate(UseAPX && UseIncDec);\n+  match(Set dst (AddI src val));\n+  effect(KILL cr);\n+\n+  format %{ \"eincl    $dst, $src\\t# int ndd\" %}\n+  ins_encode %{\n+    __ eincl($dst$$Register, $src$$Register, false);\n+  %}\n+  ins_pipe(ialu_reg);\n+%}\n+\n+instruct incI_rReg_mem_ndd(rRegI dst, memory src, immI_1 val, rFlagsReg cr)\n+%{\n+  predicate(UseAPX && UseIncDec);\n+  match(Set dst (AddI (LoadI src) val));\n+  effect(KILL cr);\n+\n+  format %{ \"eincl    $dst, $src\\t# int ndd\" %}\n+  ins_encode %{\n+    __ eincl($dst$$Register, $src$$Address, false);\n+  %}\n+  ins_pipe(ialu_reg);\n+%}\n+\n+instruct incI_mem(memory dst, immI_1 src, rFlagsReg cr)\n+%{\n+  predicate(UseIncDec);\n+  match(Set dst (StoreI dst (AddI (LoadI dst) src)));\n+  effect(KILL cr);\n+\n+  ins_cost(125); \/\/ XXX\n+  format %{ \"incl    $dst\\t# int\" %}\n+  ins_encode %{\n+    __ incrementl($dst$$Address);\n+  %}\n+  ins_pipe(ialu_mem_imm);\n+%}\n+\n+\/\/ XXX why does that use AddI\n+instruct decI_rReg(rRegI dst, immI_M1 src, rFlagsReg cr)\n+%{\n+  predicate(!UseAPX && UseIncDec);\n+  match(Set dst (AddI dst src));\n+  effect(KILL cr);\n+\n+  format %{ \"decl    $dst\\t# int\" %}\n+  ins_encode %{\n+    __ decrementl($dst$$Register);\n+  %}\n+  ins_pipe(ialu_reg);\n+%}\n+\n+instruct decI_rReg_ndd(rRegI dst, rRegI src, immI_M1 val, rFlagsReg cr)\n+%{\n+  predicate(UseAPX && UseIncDec);\n+  match(Set dst (AddI src val));\n+  effect(KILL cr);\n+\n+  format %{ \"edecl    $dst, $src\\t# int ndd\" %}\n+  ins_encode %{\n+    __ edecl($dst$$Register, $src$$Register, false);\n+  %}\n+  ins_pipe(ialu_reg);\n+%}\n+\n+instruct decI_rReg_mem_ndd(rRegI dst, memory src, immI_M1 val, rFlagsReg cr)\n+%{\n+  predicate(UseAPX && UseIncDec);\n+  match(Set dst (AddI (LoadI src) val));\n+  effect(KILL cr);\n+\n+  format %{ \"edecl    $dst, $src\\t# int ndd\" %}\n+  ins_encode %{\n+    __ edecl($dst$$Register, $src$$Address, false);\n+  %}\n+  ins_pipe(ialu_reg);\n+%}\n+\n+\/\/ XXX why does that use AddI\n+instruct decI_mem(memory dst, immI_M1 src, rFlagsReg cr)\n+%{\n+  predicate(UseIncDec);\n+  match(Set dst (StoreI dst (AddI (LoadI dst) src)));\n+  effect(KILL cr);\n+\n+  ins_cost(125); \/\/ XXX\n+  format %{ \"decl    $dst\\t# int\" %}\n+  ins_encode %{\n+    __ decrementl($dst$$Address);\n+  %}\n+  ins_pipe(ialu_mem_imm);\n+%}\n+\n+instruct leaI_rReg_immI2_immI(rRegI dst, rRegI index, immI2 scale, immI disp)\n+%{\n+  predicate(VM_Version::supports_fast_2op_lea());\n+  match(Set dst (AddI (LShiftI index scale) disp));\n+\n+  format %{ \"leal $dst, [$index << $scale + $disp]\\t# int\" %}\n+  ins_encode %{\n+    Address::ScaleFactor scale = static_cast<Address::ScaleFactor>($scale$$constant);\n+    __ leal($dst$$Register, Address(noreg, $index$$Register, scale, $disp$$constant));\n+  %}\n+  ins_pipe(ialu_reg_reg);\n+%}\n+\n+instruct leaI_rReg_rReg_immI(rRegI dst, rRegI base, rRegI index, immI disp)\n+%{\n+  predicate(VM_Version::supports_fast_3op_lea());\n+  match(Set dst (AddI (AddI base index) disp));\n+\n+  format %{ \"leal $dst, [$base + $index + $disp]\\t# int\" %}\n+  ins_encode %{\n+    __ leal($dst$$Register, Address($base$$Register, $index$$Register, Address::times_1, $disp$$constant));\n+  %}\n+  ins_pipe(ialu_reg_reg);\n+%}\n+\n+instruct leaI_rReg_rReg_immI2(rRegI dst, no_rbp_r13_RegI base, rRegI index, immI2 scale)\n+%{\n+  predicate(VM_Version::supports_fast_2op_lea());\n+  match(Set dst (AddI base (LShiftI index scale)));\n+\n+  format %{ \"leal $dst, [$base + $index << $scale]\\t# int\" %}\n+  ins_encode %{\n+    Address::ScaleFactor scale = static_cast<Address::ScaleFactor>($scale$$constant);\n+    __ leal($dst$$Register, Address($base$$Register, $index$$Register, scale));\n+  %}\n+  ins_pipe(ialu_reg_reg);\n+%}\n+\n+instruct leaI_rReg_rReg_immI2_immI(rRegI dst, rRegI base, rRegI index, immI2 scale, immI disp)\n+%{\n+  predicate(VM_Version::supports_fast_3op_lea());\n+  match(Set dst (AddI (AddI base (LShiftI index scale)) disp));\n+\n+  format %{ \"leal $dst, [$base + $index << $scale + $disp]\\t# int\" %}\n+  ins_encode %{\n+    Address::ScaleFactor scale = static_cast<Address::ScaleFactor>($scale$$constant);\n+    __ leal($dst$$Register, Address($base$$Register, $index$$Register, scale, $disp$$constant));\n+  %}\n+  ins_pipe(ialu_reg_reg);\n+%}\n+\n+instruct addL_rReg(rRegL dst, rRegL src, rFlagsReg cr)\n+%{\n+  predicate(!UseAPX);\n+  match(Set dst (AddL dst src));\n+  effect(KILL cr);\n+  flag(PD::Flag_sets_overflow_flag, PD::Flag_sets_sign_flag, PD::Flag_sets_zero_flag, PD::Flag_sets_carry_flag, PD::Flag_sets_parity_flag);\n+\n+  format %{ \"addq    $dst, $src\\t# long\" %}\n+  ins_encode %{\n+    __ addq($dst$$Register, $src$$Register);\n+  %}\n+  ins_pipe(ialu_reg_reg);\n+%}\n+\n+instruct addL_rReg_ndd(rRegL dst, rRegL src1, rRegL src2, rFlagsReg cr)\n+%{\n+  predicate(UseAPX);\n+  match(Set dst (AddL src1 src2));\n+  effect(KILL cr);\n+  flag(PD::Flag_sets_overflow_flag, PD::Flag_sets_sign_flag, PD::Flag_sets_zero_flag, PD::Flag_sets_carry_flag, PD::Flag_sets_parity_flag);\n+\n+  format %{ \"eaddq    $dst, $src1, $src2\\t# long ndd\" %}\n+  ins_encode %{\n+    __ eaddq($dst$$Register, $src1$$Register, $src2$$Register, false);\n+  %}\n+  ins_pipe(ialu_reg_reg);\n+%}\n+\n+instruct addL_rReg_imm(rRegL dst, immL32 src, rFlagsReg cr)\n+%{\n+  predicate(!UseAPX);\n+  match(Set dst (AddL dst src));\n+  effect(KILL cr);\n+  flag(PD::Flag_sets_overflow_flag, PD::Flag_sets_sign_flag, PD::Flag_sets_zero_flag, PD::Flag_sets_carry_flag, PD::Flag_sets_parity_flag);\n+\n+  format %{ \"addq    $dst, $src\\t# long\" %}\n+  ins_encode %{\n+    __ addq($dst$$Register, $src$$constant);\n+  %}\n+  ins_pipe( ialu_reg );\n+%}\n+\n+instruct addL_rReg_rReg_imm_ndd(rRegL dst, rRegL src1, immL32 src2, rFlagsReg cr)\n+%{\n+  predicate(UseAPX);\n+  match(Set dst (AddL src1 src2));\n+  effect(KILL cr);\n+  flag(PD::Flag_sets_overflow_flag, PD::Flag_sets_sign_flag, PD::Flag_sets_zero_flag, PD::Flag_sets_carry_flag, PD::Flag_sets_parity_flag);\n+\n+  format %{ \"eaddq    $dst, $src1, $src2\\t# long ndd\" %}\n+  ins_encode %{\n+    __ eaddq($dst$$Register, $src1$$Register, $src2$$constant, false);\n+  %}\n+  ins_pipe( ialu_reg );\n+%}\n+\n+instruct addL_rReg_mem_imm_ndd(rRegL dst, memory src1, immL32 src2, rFlagsReg cr)\n+%{\n+  predicate(UseAPX);\n+  match(Set dst (AddL (LoadL src1) src2));\n+  effect(KILL cr);\n+  flag(PD::Flag_sets_overflow_flag, PD::Flag_sets_sign_flag, PD::Flag_sets_zero_flag, PD::Flag_sets_carry_flag, PD::Flag_sets_parity_flag);\n+\n+  format %{ \"eaddq    $dst, $src1, $src2\\t# long ndd\" %}\n+  ins_encode %{\n+    __ eaddq($dst$$Register, $src1$$Address, $src2$$constant, false);\n+  %}\n+  ins_pipe( ialu_reg );\n+%}\n+\n+instruct addL_rReg_mem(rRegL dst, memory src, rFlagsReg cr)\n+%{\n+  predicate(!UseAPX);\n+  match(Set dst (AddL dst (LoadL src)));\n+  effect(KILL cr);\n+  flag(PD::Flag_sets_overflow_flag, PD::Flag_sets_sign_flag, PD::Flag_sets_zero_flag, PD::Flag_sets_carry_flag, PD::Flag_sets_parity_flag);\n+\n+  ins_cost(150); \/\/ XXX\n+  format %{ \"addq    $dst, $src\\t# long\" %}\n+  ins_encode %{\n+    __ addq($dst$$Register, $src$$Address);\n+  %}\n+  ins_pipe(ialu_reg_mem);\n+%}\n+\n+instruct addL_rReg_rReg_mem_ndd(rRegL dst, rRegL src1, memory src2, rFlagsReg cr)\n+%{\n+  predicate(UseAPX);\n+  match(Set dst (AddL src1 (LoadL src2)));\n+  effect(KILL cr);\n+  flag(PD::Flag_sets_overflow_flag, PD::Flag_sets_sign_flag, PD::Flag_sets_zero_flag, PD::Flag_sets_carry_flag, PD::Flag_sets_parity_flag);\n+\n+  ins_cost(150);\n+  format %{ \"eaddq    $dst, $src1, $src2\\t# long ndd\" %}\n+  ins_encode %{\n+    __ eaddq($dst$$Register, $src1$$Register, $src2$$Address, false);\n+  %}\n+  ins_pipe(ialu_reg_mem);\n+%}\n+\n+instruct addL_mem_rReg(memory dst, rRegL src, rFlagsReg cr)\n+%{\n+  match(Set dst (StoreL dst (AddL (LoadL dst) src)));\n+  effect(KILL cr);\n+  flag(PD::Flag_sets_overflow_flag, PD::Flag_sets_sign_flag, PD::Flag_sets_zero_flag, PD::Flag_sets_carry_flag, PD::Flag_sets_parity_flag);\n+\n+  ins_cost(150); \/\/ XXX\n+  format %{ \"addq    $dst, $src\\t# long\" %}\n+  ins_encode %{\n+    __ addq($dst$$Address, $src$$Register);\n+  %}\n+  ins_pipe(ialu_mem_reg);\n+%}\n+\n+instruct addL_mem_imm(memory dst, immL32 src, rFlagsReg cr)\n+%{\n+  match(Set dst (StoreL dst (AddL (LoadL dst) src)));\n+  effect(KILL cr);\n+  flag(PD::Flag_sets_overflow_flag, PD::Flag_sets_sign_flag, PD::Flag_sets_zero_flag, PD::Flag_sets_carry_flag, PD::Flag_sets_parity_flag);\n+\n+  ins_cost(125); \/\/ XXX\n+  format %{ \"addq    $dst, $src\\t# long\" %}\n+  ins_encode %{\n+    __ addq($dst$$Address, $src$$constant);\n+  %}\n+  ins_pipe(ialu_mem_imm);\n+%}\n+\n+instruct incL_rReg(rRegL dst, immL1 src, rFlagsReg cr)\n+%{\n+  predicate(!UseAPX && UseIncDec);\n+  match(Set dst (AddL dst src));\n+  effect(KILL cr);\n+\n+  format %{ \"incq    $dst\\t# long\" %}\n+  ins_encode %{\n+    __ incrementq($dst$$Register);\n+  %}\n+  ins_pipe(ialu_reg);\n+%}\n+\n+instruct incL_rReg_ndd(rRegL dst, rRegI src, immL1 val, rFlagsReg cr)\n+%{\n+  predicate(UseAPX && UseIncDec);\n+  match(Set dst (AddL src val));\n+  effect(KILL cr);\n+\n+  format %{ \"eincq    $dst, $src\\t# long ndd\" %}\n+  ins_encode %{\n+    __ eincq($dst$$Register, $src$$Register, false);\n+  %}\n+  ins_pipe(ialu_reg);\n+%}\n+\n+instruct incL_rReg_mem_ndd(rRegL dst, memory src, immL1 val, rFlagsReg cr)\n+%{\n+  predicate(UseAPX && UseIncDec);\n+  match(Set dst (AddL (LoadL src) val));\n+  effect(KILL cr);\n+\n+  format %{ \"eincq    $dst, $src\\t# long ndd\" %}\n+  ins_encode %{\n+    __ eincq($dst$$Register, $src$$Address, false);\n+  %}\n+  ins_pipe(ialu_reg);\n+%}\n+\n+instruct incL_mem(memory dst, immL1 src, rFlagsReg cr)\n+%{\n+  predicate(UseIncDec);\n+  match(Set dst (StoreL dst (AddL (LoadL dst) src)));\n+  effect(KILL cr);\n+\n+  ins_cost(125); \/\/ XXX\n+  format %{ \"incq    $dst\\t# long\" %}\n+  ins_encode %{\n+    __ incrementq($dst$$Address);\n+  %}\n+  ins_pipe(ialu_mem_imm);\n+%}\n+\n+\/\/ XXX why does that use AddL\n+instruct decL_rReg(rRegL dst, immL_M1 src, rFlagsReg cr)\n+%{\n+  predicate(!UseAPX && UseIncDec);\n+  match(Set dst (AddL dst src));\n+  effect(KILL cr);\n+\n+  format %{ \"decq    $dst\\t# long\" %}\n+  ins_encode %{\n+    __ decrementq($dst$$Register);\n+  %}\n+  ins_pipe(ialu_reg);\n+%}\n+\n+instruct decL_rReg_ndd(rRegL dst, rRegL src, immL_M1 val, rFlagsReg cr)\n+%{\n+  predicate(UseAPX && UseIncDec);\n+  match(Set dst (AddL src val));\n+  effect(KILL cr);\n+\n+  format %{ \"edecq    $dst, $src\\t# long ndd\" %}\n+  ins_encode %{\n+    __ edecq($dst$$Register, $src$$Register, false);\n+  %}\n+  ins_pipe(ialu_reg);\n+%}\n+\n+instruct decL_rReg_mem_ndd(rRegL dst, memory src, immL_M1 val, rFlagsReg cr)\n+%{\n+  predicate(UseAPX && UseIncDec);\n+  match(Set dst (AddL (LoadL src) val));\n+  effect(KILL cr);\n+\n+  format %{ \"edecq    $dst, $src\\t# long ndd\" %}\n+  ins_encode %{\n+    __ edecq($dst$$Register, $src$$Address, false);\n+  %}\n+  ins_pipe(ialu_reg);\n+%}\n+\n+\/\/ XXX why does that use AddL\n+instruct decL_mem(memory dst, immL_M1 src, rFlagsReg cr)\n+%{\n+  predicate(UseIncDec);\n+  match(Set dst (StoreL dst (AddL (LoadL dst) src)));\n+  effect(KILL cr);\n+\n+  ins_cost(125); \/\/ XXX\n+  format %{ \"decq    $dst\\t# long\" %}\n+  ins_encode %{\n+    __ decrementq($dst$$Address);\n+  %}\n+  ins_pipe(ialu_mem_imm);\n+%}\n+\n+instruct leaL_rReg_immI2_immL32(rRegL dst, rRegL index, immI2 scale, immL32 disp)\n+%{\n+  predicate(VM_Version::supports_fast_2op_lea());\n+  match(Set dst (AddL (LShiftL index scale) disp));\n+\n+  format %{ \"leaq $dst, [$index << $scale + $disp]\\t# long\" %}\n+  ins_encode %{\n+    Address::ScaleFactor scale = static_cast<Address::ScaleFactor>($scale$$constant);\n+    __ leaq($dst$$Register, Address(noreg, $index$$Register, scale, $disp$$constant));\n+  %}\n+  ins_pipe(ialu_reg_reg);\n+%}\n+\n+instruct leaL_rReg_rReg_immL32(rRegL dst, rRegL base, rRegL index, immL32 disp)\n+%{\n+  predicate(VM_Version::supports_fast_3op_lea());\n+  match(Set dst (AddL (AddL base index) disp));\n+\n+  format %{ \"leaq $dst, [$base + $index + $disp]\\t# long\" %}\n+  ins_encode %{\n+    __ leaq($dst$$Register, Address($base$$Register, $index$$Register, Address::times_1, $disp$$constant));\n+  %}\n+  ins_pipe(ialu_reg_reg);\n+%}\n+\n+instruct leaL_rReg_rReg_immI2(rRegL dst, no_rbp_r13_RegL base, rRegL index, immI2 scale)\n+%{\n+  predicate(VM_Version::supports_fast_2op_lea());\n+  match(Set dst (AddL base (LShiftL index scale)));\n+\n+  format %{ \"leaq $dst, [$base + $index << $scale]\\t# long\" %}\n+  ins_encode %{\n+    Address::ScaleFactor scale = static_cast<Address::ScaleFactor>($scale$$constant);\n+    __ leaq($dst$$Register, Address($base$$Register, $index$$Register, scale));\n+  %}\n+  ins_pipe(ialu_reg_reg);\n+%}\n+\n+instruct leaL_rReg_rReg_immI2_immL32(rRegL dst, rRegL base, rRegL index, immI2 scale, immL32 disp)\n+%{\n+  predicate(VM_Version::supports_fast_3op_lea());\n+  match(Set dst (AddL (AddL base (LShiftL index scale)) disp));\n+\n+  format %{ \"leaq $dst, [$base + $index << $scale + $disp]\\t# long\" %}\n+  ins_encode %{\n+    Address::ScaleFactor scale = static_cast<Address::ScaleFactor>($scale$$constant);\n+    __ leaq($dst$$Register, Address($base$$Register, $index$$Register, scale, $disp$$constant));\n+  %}\n+  ins_pipe(ialu_reg_reg);\n+%}\n+\n+instruct addP_rReg(rRegP dst, rRegL src, rFlagsReg cr)\n+%{\n+  match(Set dst (AddP dst src));\n+  effect(KILL cr);\n+  flag(PD::Flag_sets_overflow_flag, PD::Flag_sets_sign_flag, PD::Flag_sets_zero_flag, PD::Flag_sets_carry_flag, PD::Flag_sets_parity_flag);\n+\n+  format %{ \"addq    $dst, $src\\t# ptr\" %}\n+  ins_encode %{\n+    __ addq($dst$$Register, $src$$Register);\n+  %}\n+  ins_pipe(ialu_reg_reg);\n+%}\n+\n+instruct addP_rReg_imm(rRegP dst, immL32 src, rFlagsReg cr)\n+%{\n+  match(Set dst (AddP dst src));\n+  effect(KILL cr);\n+  flag(PD::Flag_sets_overflow_flag, PD::Flag_sets_sign_flag, PD::Flag_sets_zero_flag, PD::Flag_sets_carry_flag, PD::Flag_sets_parity_flag);\n+\n+  format %{ \"addq    $dst, $src\\t# ptr\" %}\n+  ins_encode %{\n+    __ addq($dst$$Register, $src$$constant);\n+  %}\n+  ins_pipe( ialu_reg );\n+%}\n+\n+\/\/ XXX addP mem ops ????\n+\n+instruct checkCastPP(rRegP dst)\n+%{\n+  match(Set dst (CheckCastPP dst));\n+\n+  size(0);\n+  format %{ \"# checkcastPP of $dst\" %}\n+  ins_encode(\/* empty encoding *\/);\n+  ins_pipe(empty);\n+%}\n+\n+instruct castPP(rRegP dst)\n+%{\n+  match(Set dst (CastPP dst));\n+\n+  size(0);\n+  format %{ \"# castPP of $dst\" %}\n+  ins_encode(\/* empty encoding *\/);\n+  ins_pipe(empty);\n+%}\n+\n+instruct castII(rRegI dst)\n+%{\n+  predicate(VerifyConstraintCasts == 0);\n+  match(Set dst (CastII dst));\n+\n+  size(0);\n+  format %{ \"# castII of $dst\" %}\n+  ins_encode(\/* empty encoding *\/);\n+  ins_cost(0);\n+  ins_pipe(empty);\n+%}\n+\n+instruct castII_checked(rRegI dst, rFlagsReg cr)\n+%{\n+  predicate(VerifyConstraintCasts > 0);\n+  match(Set dst (CastII dst));\n+\n+  effect(KILL cr);\n+  format %{ \"# cast_checked_II $dst\" %}\n+  ins_encode %{\n+    __ verify_int_in_range(_idx, bottom_type()->is_int(), $dst$$Register);\n+  %}\n+  ins_pipe(pipe_slow);\n+%}\n+\n+instruct castLL(rRegL dst)\n+%{\n+  predicate(VerifyConstraintCasts == 0);\n+  match(Set dst (CastLL dst));\n+\n+  size(0);\n+  format %{ \"# castLL of $dst\" %}\n+  ins_encode(\/* empty encoding *\/);\n+  ins_cost(0);\n+  ins_pipe(empty);\n+%}\n+\n+instruct castLL_checked_L32(rRegL dst, rFlagsReg cr)\n+%{\n+  predicate(VerifyConstraintCasts > 0 && castLL_is_imm32(n));\n+  match(Set dst (CastLL dst));\n+\n+  effect(KILL cr);\n+  format %{ \"# cast_checked_LL $dst\" %}\n+  ins_encode %{\n+    __ verify_long_in_range(_idx, bottom_type()->is_long(), $dst$$Register, noreg);\n+  %}\n+  ins_pipe(pipe_slow);\n+%}\n+\n+instruct castLL_checked(rRegL dst, rRegL tmp, rFlagsReg cr)\n+%{\n+  predicate(VerifyConstraintCasts > 0 && !castLL_is_imm32(n));\n+  match(Set dst (CastLL dst));\n+\n+  effect(KILL cr, TEMP tmp);\n+  format %{ \"# cast_checked_LL $dst\\tusing $tmp as TEMP\" %}\n+  ins_encode %{\n+    __ verify_long_in_range(_idx, bottom_type()->is_long(), $dst$$Register, $tmp$$Register);\n+  %}\n+  ins_pipe(pipe_slow);\n+%}\n+\n+instruct castFF(regF dst)\n+%{\n+  match(Set dst (CastFF dst));\n+\n+  size(0);\n+  format %{ \"# castFF of $dst\" %}\n+  ins_encode(\/* empty encoding *\/);\n+  ins_cost(0);\n+  ins_pipe(empty);\n+%}\n+\n+instruct castHH(regF dst)\n+%{\n+  match(Set dst (CastHH dst));\n+\n+  size(0);\n+  format %{ \"# castHH of $dst\" %}\n+  ins_encode(\/* empty encoding *\/);\n+  ins_cost(0);\n+  ins_pipe(empty);\n+%}\n+\n+instruct castDD(regD dst)\n+%{\n+  match(Set dst (CastDD dst));\n+\n+  size(0);\n+  format %{ \"# castDD of $dst\" %}\n+  ins_encode(\/* empty encoding *\/);\n+  ins_cost(0);\n+  ins_pipe(empty);\n+%}\n+\n+\/\/ XXX No flag versions for CompareAndSwap{P,I,L} because matcher can't match them\n+instruct compareAndSwapP(rRegI res,\n+                         memory mem_ptr,\n+                         rax_RegP oldval, rRegP newval,\n+                         rFlagsReg cr)\n+%{\n+  predicate(n->as_LoadStore()->barrier_data() == 0);\n+  match(Set res (CompareAndSwapP mem_ptr (Binary oldval newval)));\n+  match(Set res (WeakCompareAndSwapP mem_ptr (Binary oldval newval)));\n+  effect(KILL cr, KILL oldval);\n+\n+  format %{ \"cmpxchgq $mem_ptr,$newval\\t# \"\n+            \"If rax == $mem_ptr then store $newval into $mem_ptr\\n\\t\"\n+            \"setcc $res \\t# emits sete + movzbl or setzue for APX\" %}\n+  ins_encode %{\n+    __ lock();\n+    __ cmpxchgq($newval$$Register, $mem_ptr$$Address);\n+    __ setcc(Assembler::equal, $res$$Register);\n+  %}\n+  ins_pipe( pipe_cmpxchg );\n+%}\n+\n+instruct compareAndSwapL(rRegI res,\n+                         memory mem_ptr,\n+                         rax_RegL oldval, rRegL newval,\n+                         rFlagsReg cr)\n+%{\n+  match(Set res (CompareAndSwapL mem_ptr (Binary oldval newval)));\n+  match(Set res (WeakCompareAndSwapL mem_ptr (Binary oldval newval)));\n+  effect(KILL cr, KILL oldval);\n+\n+  format %{ \"cmpxchgq $mem_ptr,$newval\\t# \"\n+            \"If rax == $mem_ptr then store $newval into $mem_ptr\\n\\t\"\n+            \"setcc $res \\t# emits sete + movzbl or setzue for APX\" %}\n+  ins_encode %{\n+    __ lock();\n+    __ cmpxchgq($newval$$Register, $mem_ptr$$Address);\n+    __ setcc(Assembler::equal, $res$$Register);\n+  %}\n+  ins_pipe( pipe_cmpxchg );\n+%}\n+\n+instruct compareAndSwapI(rRegI res,\n+                         memory mem_ptr,\n+                         rax_RegI oldval, rRegI newval,\n+                         rFlagsReg cr)\n+%{\n+  match(Set res (CompareAndSwapI mem_ptr (Binary oldval newval)));\n+  match(Set res (WeakCompareAndSwapI mem_ptr (Binary oldval newval)));\n+  effect(KILL cr, KILL oldval);\n+\n+  format %{ \"cmpxchgl $mem_ptr,$newval\\t# \"\n+            \"If rax == $mem_ptr then store $newval into $mem_ptr\\n\\t\"\n+            \"setcc $res \\t# emits sete + movzbl or setzue for APX\" %}\n+  ins_encode %{\n+    __ lock();\n+    __ cmpxchgl($newval$$Register, $mem_ptr$$Address);\n+    __ setcc(Assembler::equal, $res$$Register);\n+  %}\n+  ins_pipe( pipe_cmpxchg );\n+%}\n+\n+instruct compareAndSwapB(rRegI res,\n+                         memory mem_ptr,\n+                         rax_RegI oldval, rRegI newval,\n+                         rFlagsReg cr)\n+%{\n+  match(Set res (CompareAndSwapB mem_ptr (Binary oldval newval)));\n+  match(Set res (WeakCompareAndSwapB mem_ptr (Binary oldval newval)));\n+  effect(KILL cr, KILL oldval);\n+\n+  format %{ \"cmpxchgb $mem_ptr,$newval\\t# \"\n+            \"If rax == $mem_ptr then store $newval into $mem_ptr\\n\\t\"\n+            \"setcc $res \\t# emits sete + movzbl or setzue for APX\" %}\n+  ins_encode %{\n+    __ lock();\n+    __ cmpxchgb($newval$$Register, $mem_ptr$$Address);\n+    __ setcc(Assembler::equal, $res$$Register);\n+  %}\n+  ins_pipe( pipe_cmpxchg );\n+%}\n+\n+instruct compareAndSwapS(rRegI res,\n+                         memory mem_ptr,\n+                         rax_RegI oldval, rRegI newval,\n+                         rFlagsReg cr)\n+%{\n+  match(Set res (CompareAndSwapS mem_ptr (Binary oldval newval)));\n+  match(Set res (WeakCompareAndSwapS mem_ptr (Binary oldval newval)));\n+  effect(KILL cr, KILL oldval);\n+\n+  format %{ \"cmpxchgw $mem_ptr,$newval\\t# \"\n+            \"If rax == $mem_ptr then store $newval into $mem_ptr\\n\\t\"\n+            \"setcc $res \\t# emits sete + movzbl or setzue for APX\" %}\n+  ins_encode %{\n+    __ lock();\n+    __ cmpxchgw($newval$$Register, $mem_ptr$$Address);\n+    __ setcc(Assembler::equal, $res$$Register);\n+  %}\n+  ins_pipe( pipe_cmpxchg );\n+%}\n+\n+instruct compareAndSwapN(rRegI res,\n+                          memory mem_ptr,\n+                          rax_RegN oldval, rRegN newval,\n+                          rFlagsReg cr) %{\n+  predicate(n->as_LoadStore()->barrier_data() == 0);\n+  match(Set res (CompareAndSwapN mem_ptr (Binary oldval newval)));\n+  match(Set res (WeakCompareAndSwapN mem_ptr (Binary oldval newval)));\n+  effect(KILL cr, KILL oldval);\n+\n+  format %{ \"cmpxchgl $mem_ptr,$newval\\t# \"\n+            \"If rax == $mem_ptr then store $newval into $mem_ptr\\n\\t\"\n+            \"setcc $res \\t# emits sete + movzbl or setzue for APX\" %}\n+  ins_encode %{\n+    __ lock();\n+    __ cmpxchgl($newval$$Register, $mem_ptr$$Address);\n+    __ setcc(Assembler::equal, $res$$Register);\n+  %}\n+  ins_pipe( pipe_cmpxchg );\n+%}\n+\n+instruct compareAndExchangeB(\n+                         memory mem_ptr,\n+                         rax_RegI oldval, rRegI newval,\n+                         rFlagsReg cr)\n+%{\n+  match(Set oldval (CompareAndExchangeB mem_ptr (Binary oldval newval)));\n+  effect(KILL cr);\n+\n+  format %{ \"cmpxchgb $mem_ptr,$newval\\t# \"\n+            \"If rax == $mem_ptr then store $newval into $mem_ptr\\n\\t\"  %}\n+  ins_encode %{\n+    __ lock();\n+    __ cmpxchgb($newval$$Register, $mem_ptr$$Address);\n+  %}\n+  ins_pipe( pipe_cmpxchg );\n+%}\n+\n+instruct compareAndExchangeS(\n+                         memory mem_ptr,\n+                         rax_RegI oldval, rRegI newval,\n+                         rFlagsReg cr)\n+%{\n+  match(Set oldval (CompareAndExchangeS mem_ptr (Binary oldval newval)));\n+  effect(KILL cr);\n+\n+  format %{ \"cmpxchgw $mem_ptr,$newval\\t# \"\n+            \"If rax == $mem_ptr then store $newval into $mem_ptr\\n\\t\"  %}\n+  ins_encode %{\n+    __ lock();\n+    __ cmpxchgw($newval$$Register, $mem_ptr$$Address);\n+  %}\n+  ins_pipe( pipe_cmpxchg );\n+%}\n+\n+instruct compareAndExchangeI(\n+                         memory mem_ptr,\n+                         rax_RegI oldval, rRegI newval,\n+                         rFlagsReg cr)\n+%{\n+  match(Set oldval (CompareAndExchangeI mem_ptr (Binary oldval newval)));\n+  effect(KILL cr);\n+\n+  format %{ \"cmpxchgl $mem_ptr,$newval\\t# \"\n+            \"If rax == $mem_ptr then store $newval into $mem_ptr\\n\\t\"  %}\n+  ins_encode %{\n+    __ lock();\n+    __ cmpxchgl($newval$$Register, $mem_ptr$$Address);\n+  %}\n+  ins_pipe( pipe_cmpxchg );\n+%}\n+\n+instruct compareAndExchangeL(\n+                         memory mem_ptr,\n+                         rax_RegL oldval, rRegL newval,\n+                         rFlagsReg cr)\n+%{\n+  match(Set oldval (CompareAndExchangeL mem_ptr (Binary oldval newval)));\n+  effect(KILL cr);\n+\n+  format %{ \"cmpxchgq $mem_ptr,$newval\\t# \"\n+            \"If rax == $mem_ptr then store $newval into $mem_ptr\\n\\t\"  %}\n+  ins_encode %{\n+    __ lock();\n+    __ cmpxchgq($newval$$Register, $mem_ptr$$Address);\n+  %}\n+  ins_pipe( pipe_cmpxchg );\n+%}\n+\n+instruct compareAndExchangeN(\n+                          memory mem_ptr,\n+                          rax_RegN oldval, rRegN newval,\n+                          rFlagsReg cr) %{\n+  predicate(n->as_LoadStore()->barrier_data() == 0);\n+  match(Set oldval (CompareAndExchangeN mem_ptr (Binary oldval newval)));\n+  effect(KILL cr);\n+\n+  format %{ \"cmpxchgl $mem_ptr,$newval\\t# \"\n+            \"If rax == $mem_ptr then store $newval into $mem_ptr\\n\\t\" %}\n+  ins_encode %{\n+    __ lock();\n+    __ cmpxchgl($newval$$Register, $mem_ptr$$Address);\n+  %}\n+  ins_pipe( pipe_cmpxchg );\n+%}\n+\n+instruct compareAndExchangeP(\n+                         memory mem_ptr,\n+                         rax_RegP oldval, rRegP newval,\n+                         rFlagsReg cr)\n+%{\n+  predicate(n->as_LoadStore()->barrier_data() == 0);\n+  match(Set oldval (CompareAndExchangeP mem_ptr (Binary oldval newval)));\n+  effect(KILL cr);\n+\n+  format %{ \"cmpxchgq $mem_ptr,$newval\\t# \"\n+            \"If rax == $mem_ptr then store $newval into $mem_ptr\\n\\t\" %}\n+  ins_encode %{\n+    __ lock();\n+    __ cmpxchgq($newval$$Register, $mem_ptr$$Address);\n+  %}\n+  ins_pipe( pipe_cmpxchg );\n+%}\n+\n+instruct xaddB_reg_no_res(memory mem, Universe dummy, rRegI add, rFlagsReg cr) %{\n+  predicate(n->as_LoadStore()->result_not_used());\n+  match(Set dummy (GetAndAddB mem add));\n+  effect(KILL cr);\n+  format %{ \"addb_lock   $mem, $add\" %}\n+  ins_encode %{\n+    __ lock();\n+    __ addb($mem$$Address, $add$$Register);\n+  %}\n+  ins_pipe(pipe_cmpxchg);\n+%}\n+\n+instruct xaddB_imm_no_res(memory mem, Universe dummy, immI add, rFlagsReg cr) %{\n+  predicate(n->as_LoadStore()->result_not_used());\n+  match(Set dummy (GetAndAddB mem add));\n+  effect(KILL cr);\n+  format %{ \"addb_lock   $mem, $add\" %}\n+  ins_encode %{\n+    __ lock();\n+    __ addb($mem$$Address, $add$$constant);\n+  %}\n+  ins_pipe(pipe_cmpxchg);\n+%}\n+\n+instruct xaddB(memory mem, rRegI newval, rFlagsReg cr) %{\n+  predicate(!n->as_LoadStore()->result_not_used());\n+  match(Set newval (GetAndAddB mem newval));\n+  effect(KILL cr);\n+  format %{ \"xaddb_lock  $mem, $newval\" %}\n+  ins_encode %{\n+    __ lock();\n+    __ xaddb($mem$$Address, $newval$$Register);\n+  %}\n+  ins_pipe(pipe_cmpxchg);\n+%}\n+\n+instruct xaddS_reg_no_res(memory mem, Universe dummy, rRegI add, rFlagsReg cr) %{\n+  predicate(n->as_LoadStore()->result_not_used());\n+  match(Set dummy (GetAndAddS mem add));\n+  effect(KILL cr);\n+  format %{ \"addw_lock   $mem, $add\" %}\n+  ins_encode %{\n+    __ lock();\n+    __ addw($mem$$Address, $add$$Register);\n+  %}\n+  ins_pipe(pipe_cmpxchg);\n+%}\n+\n+instruct xaddS_imm_no_res(memory mem, Universe dummy, immI add, rFlagsReg cr) %{\n+  predicate(UseStoreImmI16 && n->as_LoadStore()->result_not_used());\n+  match(Set dummy (GetAndAddS mem add));\n+  effect(KILL cr);\n+  format %{ \"addw_lock   $mem, $add\" %}\n+  ins_encode %{\n+    __ lock();\n+    __ addw($mem$$Address, $add$$constant);\n+  %}\n+  ins_pipe(pipe_cmpxchg);\n+%}\n+\n+instruct xaddS(memory mem, rRegI newval, rFlagsReg cr) %{\n+  predicate(!n->as_LoadStore()->result_not_used());\n+  match(Set newval (GetAndAddS mem newval));\n+  effect(KILL cr);\n+  format %{ \"xaddw_lock  $mem, $newval\" %}\n+  ins_encode %{\n+    __ lock();\n+    __ xaddw($mem$$Address, $newval$$Register);\n+  %}\n+  ins_pipe(pipe_cmpxchg);\n+%}\n+\n+instruct xaddI_reg_no_res(memory mem, Universe dummy, rRegI add, rFlagsReg cr) %{\n+  predicate(n->as_LoadStore()->result_not_used());\n+  match(Set dummy (GetAndAddI mem add));\n+  effect(KILL cr);\n+  format %{ \"addl_lock   $mem, $add\" %}\n+  ins_encode %{\n+    __ lock();\n+    __ addl($mem$$Address, $add$$Register);\n+  %}\n+  ins_pipe(pipe_cmpxchg);\n+%}\n+\n+instruct xaddI_imm_no_res(memory mem, Universe dummy, immI add, rFlagsReg cr) %{\n+  predicate(n->as_LoadStore()->result_not_used());\n+  match(Set dummy (GetAndAddI mem add));\n+  effect(KILL cr);\n+  format %{ \"addl_lock   $mem, $add\" %}\n+  ins_encode %{\n+    __ lock();\n+    __ addl($mem$$Address, $add$$constant);\n+  %}\n+  ins_pipe(pipe_cmpxchg);\n+%}\n+\n+instruct xaddI(memory mem, rRegI newval, rFlagsReg cr) %{\n+  predicate(!n->as_LoadStore()->result_not_used());\n+  match(Set newval (GetAndAddI mem newval));\n+  effect(KILL cr);\n+  format %{ \"xaddl_lock  $mem, $newval\" %}\n+  ins_encode %{\n+    __ lock();\n+    __ xaddl($mem$$Address, $newval$$Register);\n+  %}\n+  ins_pipe(pipe_cmpxchg);\n+%}\n+\n+instruct xaddL_reg_no_res(memory mem, Universe dummy, rRegL add, rFlagsReg cr) %{\n+  predicate(n->as_LoadStore()->result_not_used());\n+  match(Set dummy (GetAndAddL mem add));\n+  effect(KILL cr);\n+  format %{ \"addq_lock   $mem, $add\" %}\n+  ins_encode %{\n+    __ lock();\n+    __ addq($mem$$Address, $add$$Register);\n+  %}\n+  ins_pipe(pipe_cmpxchg);\n+%}\n+\n+instruct xaddL_imm_no_res(memory mem, Universe dummy, immL32 add, rFlagsReg cr) %{\n+  predicate(n->as_LoadStore()->result_not_used());\n+  match(Set dummy (GetAndAddL mem add));\n+  effect(KILL cr);\n+  format %{ \"addq_lock   $mem, $add\" %}\n+  ins_encode %{\n+    __ lock();\n+    __ addq($mem$$Address, $add$$constant);\n+  %}\n+  ins_pipe(pipe_cmpxchg);\n+%}\n+\n+instruct xaddL(memory mem, rRegL newval, rFlagsReg cr) %{\n+  predicate(!n->as_LoadStore()->result_not_used());\n+  match(Set newval (GetAndAddL mem newval));\n+  effect(KILL cr);\n+  format %{ \"xaddq_lock  $mem, $newval\" %}\n+  ins_encode %{\n+    __ lock();\n+    __ xaddq($mem$$Address, $newval$$Register);\n+  %}\n+  ins_pipe(pipe_cmpxchg);\n+%}\n+\n+instruct xchgB( memory mem, rRegI newval) %{\n+  match(Set newval (GetAndSetB mem newval));\n+  format %{ \"XCHGB  $newval,[$mem]\" %}\n+  ins_encode %{\n+    __ xchgb($newval$$Register, $mem$$Address);\n+  %}\n+  ins_pipe( pipe_cmpxchg );\n+%}\n+\n+instruct xchgS( memory mem, rRegI newval) %{\n+  match(Set newval (GetAndSetS mem newval));\n+  format %{ \"XCHGW  $newval,[$mem]\" %}\n+  ins_encode %{\n+    __ xchgw($newval$$Register, $mem$$Address);\n+  %}\n+  ins_pipe( pipe_cmpxchg );\n+%}\n+\n+instruct xchgI( memory mem, rRegI newval) %{\n+  match(Set newval (GetAndSetI mem newval));\n+  format %{ \"XCHGL  $newval,[$mem]\" %}\n+  ins_encode %{\n+    __ xchgl($newval$$Register, $mem$$Address);\n+  %}\n+  ins_pipe( pipe_cmpxchg );\n+%}\n+\n+instruct xchgL( memory mem, rRegL newval) %{\n+  match(Set newval (GetAndSetL mem newval));\n+  format %{ \"XCHGL  $newval,[$mem]\" %}\n+  ins_encode %{\n+    __ xchgq($newval$$Register, $mem$$Address);\n+  %}\n+  ins_pipe( pipe_cmpxchg );\n+%}\n+\n+instruct xchgP( memory mem, rRegP newval) %{\n+  match(Set newval (GetAndSetP mem newval));\n+  predicate(n->as_LoadStore()->barrier_data() == 0);\n+  format %{ \"XCHGQ  $newval,[$mem]\" %}\n+  ins_encode %{\n+    __ xchgq($newval$$Register, $mem$$Address);\n+  %}\n+  ins_pipe( pipe_cmpxchg );\n+%}\n+\n+instruct xchgN( memory mem, rRegN newval) %{\n+  predicate(n->as_LoadStore()->barrier_data() == 0);\n+  match(Set newval (GetAndSetN mem newval));\n+  format %{ \"XCHGL  $newval,$mem]\" %}\n+  ins_encode %{\n+    __ xchgl($newval$$Register, $mem$$Address);\n+  %}\n+  ins_pipe( pipe_cmpxchg );\n+%}\n+\n+\/\/----------Abs Instructions-------------------------------------------\n+\n+\/\/ Integer Absolute Instructions\n+instruct absI_rReg(rRegI dst, rRegI src, rFlagsReg cr)\n+%{\n+  match(Set dst (AbsI src));\n+  effect(TEMP dst, KILL cr);\n+  format %{ \"xorl    $dst, $dst\\t# abs int\\n\\t\"\n+            \"subl    $dst, $src\\n\\t\"\n+            \"cmovll  $dst, $src\" %}\n+  ins_encode %{\n+    __ xorl($dst$$Register, $dst$$Register);\n+    __ subl($dst$$Register, $src$$Register);\n+    __ cmovl(Assembler::less, $dst$$Register, $src$$Register);\n+  %}\n+\n+  ins_pipe(ialu_reg_reg);\n+%}\n+\n+\/\/ Long Absolute Instructions\n+instruct absL_rReg(rRegL dst, rRegL src, rFlagsReg cr)\n+%{\n+  match(Set dst (AbsL src));\n+  effect(TEMP dst, KILL cr);\n+  format %{ \"xorl    $dst, $dst\\t# abs long\\n\\t\"\n+            \"subq    $dst, $src\\n\\t\"\n+            \"cmovlq  $dst, $src\" %}\n+  ins_encode %{\n+    __ xorl($dst$$Register, $dst$$Register);\n+    __ subq($dst$$Register, $src$$Register);\n+    __ cmovq(Assembler::less, $dst$$Register, $src$$Register);\n+  %}\n+\n+  ins_pipe(ialu_reg_reg);\n+%}\n+\n+\/\/----------Subtraction Instructions-------------------------------------------\n+\n+\/\/ Integer Subtraction Instructions\n+instruct subI_rReg(rRegI dst, rRegI src, rFlagsReg cr)\n+%{\n+  predicate(!UseAPX);\n+  match(Set dst (SubI dst src));\n+  effect(KILL cr);\n+  flag(PD::Flag_sets_overflow_flag, PD::Flag_sets_sign_flag, PD::Flag_sets_zero_flag, PD::Flag_sets_carry_flag, PD::Flag_sets_parity_flag);\n+\n+  format %{ \"subl    $dst, $src\\t# int\" %}\n+  ins_encode %{\n+    __ subl($dst$$Register, $src$$Register);\n+  %}\n+  ins_pipe(ialu_reg_reg);\n+%}\n+\n+instruct subI_rReg_ndd(rRegI dst, rRegI src1, rRegI src2, rFlagsReg cr)\n+%{\n+  predicate(UseAPX);\n+  match(Set dst (SubI src1 src2));\n+  effect(KILL cr);\n+  flag(PD::Flag_sets_overflow_flag, PD::Flag_sets_sign_flag, PD::Flag_sets_zero_flag, PD::Flag_sets_carry_flag, PD::Flag_sets_parity_flag);\n+\n+  format %{ \"esubl    $dst, $src1, $src2\\t# int ndd\" %}\n+  ins_encode %{\n+    __ esubl($dst$$Register, $src1$$Register, $src2$$Register, false);\n+  %}\n+  ins_pipe(ialu_reg_reg);\n+%}\n+\n+instruct subI_rReg_rReg_imm_ndd(rRegI dst, rRegI src1, immI src2, rFlagsReg cr)\n+%{\n+  predicate(UseAPX);\n+  match(Set dst (SubI src1 src2));\n+  effect(KILL cr);\n+  flag(PD::Flag_sets_overflow_flag, PD::Flag_sets_sign_flag, PD::Flag_sets_zero_flag, PD::Flag_sets_carry_flag, PD::Flag_sets_parity_flag);\n+\n+  format %{ \"esubl    $dst, $src1, $src2\\t# int ndd\" %}\n+  ins_encode %{\n+    __ esubl($dst$$Register, $src1$$Register, $src2$$constant, false);\n+  %}\n+  ins_pipe(ialu_reg_reg);\n+%}\n+\n+instruct subI_rReg_mem_imm_ndd(rRegI dst, memory src1, immI src2, rFlagsReg cr)\n+%{\n+  predicate(UseAPX);\n+  match(Set dst (SubI (LoadI src1) src2));\n+  effect(KILL cr);\n+  flag(PD::Flag_sets_overflow_flag, PD::Flag_sets_sign_flag, PD::Flag_sets_zero_flag, PD::Flag_sets_carry_flag, PD::Flag_sets_parity_flag);\n+\n+  format %{ \"esubl    $dst, $src1, $src2\\t# int ndd\" %}\n+  ins_encode %{\n+    __ esubl($dst$$Register, $src1$$Address, $src2$$constant, false);\n+  %}\n+  ins_pipe(ialu_reg_reg);\n+%}\n+\n+instruct subI_rReg_mem(rRegI dst, memory src, rFlagsReg cr)\n+%{\n+  predicate(!UseAPX);\n+  match(Set dst (SubI dst (LoadI src)));\n+  effect(KILL cr);\n+  flag(PD::Flag_sets_overflow_flag, PD::Flag_sets_sign_flag, PD::Flag_sets_zero_flag, PD::Flag_sets_carry_flag, PD::Flag_sets_parity_flag);\n+\n+  ins_cost(150);\n+  format %{ \"subl    $dst, $src\\t# int\" %}\n+  ins_encode %{\n+    __ subl($dst$$Register, $src$$Address);\n+  %}\n+  ins_pipe(ialu_reg_mem);\n+%}\n+\n+instruct subI_rReg_rReg_mem_ndd(rRegI dst, rRegI src1, memory src2, rFlagsReg cr)\n+%{\n+  predicate(UseAPX);\n+  match(Set dst (SubI src1 (LoadI src2)));\n+  effect(KILL cr);\n+  flag(PD::Flag_sets_overflow_flag, PD::Flag_sets_sign_flag, PD::Flag_sets_zero_flag, PD::Flag_sets_carry_flag, PD::Flag_sets_parity_flag);\n+\n+  ins_cost(150);\n+  format %{ \"esubl    $dst, $src1, $src2\\t# int ndd\" %}\n+  ins_encode %{\n+    __ esubl($dst$$Register, $src1$$Register, $src2$$Address, false);\n+  %}\n+  ins_pipe(ialu_reg_mem);\n+%}\n+\n+instruct subI_rReg_mem_rReg_ndd(rRegI dst, memory src1, rRegI src2, rFlagsReg cr)\n+%{\n+  predicate(UseAPX);\n+  match(Set dst (SubI (LoadI src1) src2));\n+  effect(KILL cr);\n+  flag(PD::Flag_sets_overflow_flag, PD::Flag_sets_sign_flag, PD::Flag_sets_zero_flag, PD::Flag_sets_carry_flag, PD::Flag_sets_parity_flag);\n+\n+  ins_cost(150);\n+  format %{ \"esubl    $dst, $src1, $src2\\t# int ndd\" %}\n+  ins_encode %{\n+    __ esubl($dst$$Register, $src1$$Address, $src2$$Register, false);\n+  %}\n+  ins_pipe(ialu_reg_mem);\n+%}\n+\n+instruct subI_mem_rReg(memory dst, rRegI src, rFlagsReg cr)\n+%{\n+  match(Set dst (StoreI dst (SubI (LoadI dst) src)));\n+  effect(KILL cr);\n+  flag(PD::Flag_sets_overflow_flag, PD::Flag_sets_sign_flag, PD::Flag_sets_zero_flag, PD::Flag_sets_carry_flag, PD::Flag_sets_parity_flag);\n+\n+  ins_cost(150);\n+  format %{ \"subl    $dst, $src\\t# int\" %}\n+  ins_encode %{\n+    __ subl($dst$$Address, $src$$Register);\n+  %}\n+  ins_pipe(ialu_mem_reg);\n+%}\n+\n+instruct subL_rReg(rRegL dst, rRegL src, rFlagsReg cr)\n+%{\n+  predicate(!UseAPX);\n+  match(Set dst (SubL dst src));\n+  effect(KILL cr);\n+  flag(PD::Flag_sets_overflow_flag, PD::Flag_sets_sign_flag, PD::Flag_sets_zero_flag, PD::Flag_sets_carry_flag, PD::Flag_sets_parity_flag);\n+\n+  format %{ \"subq    $dst, $src\\t# long\" %}\n+  ins_encode %{\n+    __ subq($dst$$Register, $src$$Register);\n+  %}\n+  ins_pipe(ialu_reg_reg);\n+%}\n+\n+instruct subL_rReg_ndd(rRegL dst, rRegL src1, rRegL src2, rFlagsReg cr)\n+%{\n+  predicate(UseAPX);\n+  match(Set dst (SubL src1 src2));\n+  effect(KILL cr);\n+  flag(PD::Flag_sets_overflow_flag, PD::Flag_sets_sign_flag, PD::Flag_sets_zero_flag, PD::Flag_sets_carry_flag, PD::Flag_sets_parity_flag);\n+\n+  format %{ \"esubq    $dst, $src1, $src2\\t# long ndd\" %}\n+  ins_encode %{\n+    __ esubq($dst$$Register, $src1$$Register, $src2$$Register, false);\n+  %}\n+  ins_pipe(ialu_reg_reg);\n+%}\n+\n+instruct subL_rReg_rReg_imm_ndd(rRegL dst, rRegL src1, immL32 src2, rFlagsReg cr)\n+%{\n+  predicate(UseAPX);\n+  match(Set dst (SubL src1 src2));\n+  effect(KILL cr);\n+  flag(PD::Flag_sets_overflow_flag, PD::Flag_sets_sign_flag, PD::Flag_sets_zero_flag, PD::Flag_sets_carry_flag, PD::Flag_sets_parity_flag);\n+\n+  format %{ \"esubq    $dst, $src1, $src2\\t# long ndd\" %}\n+  ins_encode %{\n+    __ esubq($dst$$Register, $src1$$Register, $src2$$constant, false);\n+  %}\n+  ins_pipe(ialu_reg_reg);\n+%}\n+\n+instruct subL_rReg_mem_imm_ndd(rRegL dst, memory src1, immL32 src2, rFlagsReg cr)\n+%{\n+  predicate(UseAPX);\n+  match(Set dst (SubL (LoadL src1) src2));\n+  effect(KILL cr);\n+  flag(PD::Flag_sets_overflow_flag, PD::Flag_sets_sign_flag, PD::Flag_sets_zero_flag, PD::Flag_sets_carry_flag, PD::Flag_sets_parity_flag);\n+\n+  format %{ \"esubq    $dst, $src1, $src2\\t# long ndd\" %}\n+  ins_encode %{\n+    __ esubq($dst$$Register, $src1$$Address, $src2$$constant, false);\n+  %}\n+  ins_pipe(ialu_reg_reg);\n+%}\n+\n+instruct subL_rReg_mem(rRegL dst, memory src, rFlagsReg cr)\n+%{\n+  predicate(!UseAPX);\n+  match(Set dst (SubL dst (LoadL src)));\n+  effect(KILL cr);\n+  flag(PD::Flag_sets_overflow_flag, PD::Flag_sets_sign_flag, PD::Flag_sets_zero_flag, PD::Flag_sets_carry_flag, PD::Flag_sets_parity_flag);\n+\n+  ins_cost(150);\n+  format %{ \"subq    $dst, $src\\t# long\" %}\n+  ins_encode %{\n+    __ subq($dst$$Register, $src$$Address);\n+  %}\n+  ins_pipe(ialu_reg_mem);\n+%}\n+\n+instruct subL_rReg_rReg_mem_ndd(rRegL dst, rRegL src1, memory src2, rFlagsReg cr)\n+%{\n+  predicate(UseAPX);\n+  match(Set dst (SubL src1 (LoadL src2)));\n+  effect(KILL cr);\n+  flag(PD::Flag_sets_overflow_flag, PD::Flag_sets_sign_flag, PD::Flag_sets_zero_flag, PD::Flag_sets_carry_flag, PD::Flag_sets_parity_flag);\n+\n+  ins_cost(150);\n+  format %{ \"esubq    $dst, $src1, $src2\\t# long ndd\" %}\n+  ins_encode %{\n+    __ esubq($dst$$Register, $src1$$Register, $src2$$Address, false);\n+  %}\n+  ins_pipe(ialu_reg_mem);\n+%}\n+\n+instruct subL_rReg_mem_rReg_ndd(rRegL dst, memory src1, rRegL src2, rFlagsReg cr)\n+%{\n+  predicate(UseAPX);\n+  match(Set dst (SubL (LoadL src1) src2));\n+  effect(KILL cr);\n+  flag(PD::Flag_sets_overflow_flag, PD::Flag_sets_sign_flag, PD::Flag_sets_zero_flag, PD::Flag_sets_carry_flag, PD::Flag_sets_parity_flag);\n+\n+  ins_cost(150);\n+  format %{ \"esubq    $dst, $src1, $src2\\t# long ndd\" %}\n+  ins_encode %{\n+    __ esubq($dst$$Register, $src1$$Address, $src2$$Register, false);\n+  %}\n+  ins_pipe(ialu_reg_mem);\n+%}\n+\n+instruct subL_mem_rReg(memory dst, rRegL src, rFlagsReg cr)\n+%{\n+  match(Set dst (StoreL dst (SubL (LoadL dst) src)));\n+  effect(KILL cr);\n+  flag(PD::Flag_sets_overflow_flag, PD::Flag_sets_sign_flag, PD::Flag_sets_zero_flag, PD::Flag_sets_carry_flag, PD::Flag_sets_parity_flag);\n+\n+  ins_cost(150);\n+  format %{ \"subq    $dst, $src\\t# long\" %}\n+  ins_encode %{\n+    __ subq($dst$$Address, $src$$Register);\n+  %}\n+  ins_pipe(ialu_mem_reg);\n+%}\n+\n+\/\/ Subtract from a pointer\n+\/\/ XXX hmpf???\n+instruct subP_rReg(rRegP dst, rRegI src, immI_0 zero, rFlagsReg cr)\n+%{\n+  match(Set dst (AddP dst (SubI zero src)));\n+  effect(KILL cr);\n+\n+  format %{ \"subq    $dst, $src\\t# ptr - int\" %}\n+  ins_encode %{\n+    __ subq($dst$$Register, $src$$Register);\n+  %}\n+  ins_pipe(ialu_reg_reg);\n+%}\n+\n+instruct negI_rReg(rRegI dst, immI_0 zero, rFlagsReg cr)\n+%{\n+  predicate(!UseAPX);\n+  match(Set dst (SubI zero dst));\n+  effect(KILL cr);\n+  flag(PD::Flag_sets_overflow_flag, PD::Flag_sets_sign_flag, PD::Flag_sets_zero_flag, PD::Flag_sets_parity_flag);\n+\n+  format %{ \"negl    $dst\\t# int\" %}\n+  ins_encode %{\n+    __ negl($dst$$Register);\n+  %}\n+  ins_pipe(ialu_reg);\n+%}\n+\n+instruct negI_rReg_ndd(rRegI dst, rRegI src, immI_0 zero, rFlagsReg cr)\n+%{\n+  predicate(UseAPX);\n+  match(Set dst (SubI zero src));\n+  effect(KILL cr);\n+  flag(PD::Flag_sets_overflow_flag, PD::Flag_sets_sign_flag, PD::Flag_sets_zero_flag, PD::Flag_sets_parity_flag);\n+\n+  format %{ \"enegl    $dst, $src\\t# int ndd\" %}\n+  ins_encode %{\n+    __ enegl($dst$$Register, $src$$Register, false);\n+  %}\n+  ins_pipe(ialu_reg);\n+%}\n+\n+instruct negI_rReg_2(rRegI dst, rFlagsReg cr)\n+%{\n+  predicate(!UseAPX);\n+  match(Set dst (NegI dst));\n+  effect(KILL cr);\n+  flag(PD::Flag_sets_overflow_flag, PD::Flag_sets_sign_flag, PD::Flag_sets_zero_flag, PD::Flag_sets_parity_flag);\n+\n+  format %{ \"negl    $dst\\t# int\" %}\n+  ins_encode %{\n+    __ negl($dst$$Register);\n+  %}\n+  ins_pipe(ialu_reg);\n+%}\n+\n+instruct negI_rReg_2_ndd(rRegI dst, rRegI src, rFlagsReg cr)\n+%{\n+  predicate(UseAPX);\n+  match(Set dst (NegI src));\n+  effect(KILL cr);\n+  flag(PD::Flag_sets_overflow_flag, PD::Flag_sets_sign_flag, PD::Flag_sets_zero_flag, PD::Flag_sets_parity_flag);\n+\n+  format %{ \"enegl    $dst, $src\\t# int ndd\" %}\n+  ins_encode %{\n+    __ enegl($dst$$Register, $src$$Register, false);\n+  %}\n+  ins_pipe(ialu_reg);\n+%}\n+\n+instruct negI_mem(memory dst, immI_0 zero, rFlagsReg cr)\n+%{\n+  match(Set dst (StoreI dst (SubI zero (LoadI dst))));\n+  effect(KILL cr);\n+  flag(PD::Flag_sets_overflow_flag, PD::Flag_sets_sign_flag, PD::Flag_sets_zero_flag, PD::Flag_sets_parity_flag);\n+\n+  format %{ \"negl    $dst\\t# int\" %}\n+  ins_encode %{\n+    __ negl($dst$$Address);\n+  %}\n+  ins_pipe(ialu_reg);\n+%}\n+\n+instruct negL_rReg(rRegL dst, immL0 zero, rFlagsReg cr)\n+%{\n+  predicate(!UseAPX);\n+  match(Set dst (SubL zero dst));\n+  effect(KILL cr);\n+  flag(PD::Flag_sets_overflow_flag, PD::Flag_sets_sign_flag, PD::Flag_sets_zero_flag, PD::Flag_sets_parity_flag);\n+\n+  format %{ \"negq    $dst\\t# long\" %}\n+  ins_encode %{\n+    __ negq($dst$$Register);\n+  %}\n+  ins_pipe(ialu_reg);\n+%}\n+\n+instruct negL_rReg_ndd(rRegL dst, rRegL src, immL0 zero, rFlagsReg cr)\n+%{\n+  predicate(UseAPX);\n+  match(Set dst (SubL zero src));\n+  effect(KILL cr);\n+  flag(PD::Flag_sets_overflow_flag, PD::Flag_sets_sign_flag, PD::Flag_sets_zero_flag, PD::Flag_sets_parity_flag);\n+\n+  format %{ \"enegq    $dst, $src\\t# long ndd\" %}\n+  ins_encode %{\n+    __ enegq($dst$$Register, $src$$Register, false);\n+  %}\n+  ins_pipe(ialu_reg);\n+%}\n+\n+instruct negL_rReg_2(rRegL dst, rFlagsReg cr)\n+%{\n+  predicate(!UseAPX);\n+  match(Set dst (NegL dst));\n+  effect(KILL cr);\n+  flag(PD::Flag_sets_overflow_flag, PD::Flag_sets_sign_flag, PD::Flag_sets_zero_flag, PD::Flag_sets_parity_flag);\n+\n+  format %{ \"negq    $dst\\t# int\" %}\n+  ins_encode %{\n+    __ negq($dst$$Register);\n+  %}\n+  ins_pipe(ialu_reg);\n+%}\n+\n+instruct negL_rReg_2_ndd(rRegL dst, rRegL src, rFlagsReg cr)\n+%{\n+  predicate(UseAPX);\n+  match(Set dst (NegL src));\n+  effect(KILL cr);\n+  flag(PD::Flag_sets_overflow_flag, PD::Flag_sets_sign_flag, PD::Flag_sets_zero_flag, PD::Flag_sets_parity_flag);\n+\n+  format %{ \"enegq    $dst, $src\\t# long ndd\" %}\n+  ins_encode %{\n+    __ enegq($dst$$Register, $src$$Register, false);\n+  %}\n+  ins_pipe(ialu_reg);\n+%}\n+\n+instruct negL_mem(memory dst, immL0 zero, rFlagsReg cr)\n+%{\n+  match(Set dst (StoreL dst (SubL zero (LoadL dst))));\n+  effect(KILL cr);\n+  flag(PD::Flag_sets_overflow_flag, PD::Flag_sets_sign_flag, PD::Flag_sets_zero_flag, PD::Flag_sets_parity_flag);\n+\n+  format %{ \"negq    $dst\\t# long\" %}\n+  ins_encode %{\n+    __ negq($dst$$Address);\n+  %}\n+  ins_pipe(ialu_reg);\n+%}\n+\n+\/\/----------Multiplication\/Division Instructions-------------------------------\n+\/\/ Integer Multiplication Instructions\n+\/\/ Multiply Register\n+\n+instruct mulI_rReg(rRegI dst, rRegI src, rFlagsReg cr)\n+%{\n+  predicate(!UseAPX);\n+  match(Set dst (MulI dst src));\n+  effect(KILL cr);\n+\n+  ins_cost(300);\n+  format %{ \"imull   $dst, $src\\t# int\" %}\n+  ins_encode %{\n+    __ imull($dst$$Register, $src$$Register);\n+  %}\n+  ins_pipe(ialu_reg_reg_alu0);\n+%}\n+\n+instruct mulI_rReg_ndd(rRegI dst, rRegI src1, rRegI src2, rFlagsReg cr)\n+%{\n+  predicate(UseAPX);\n+  match(Set dst (MulI src1 src2));\n+  effect(KILL cr);\n+\n+  ins_cost(300);\n+  format %{ \"eimull   $dst, $src1, $src2\\t# int ndd\" %}\n+  ins_encode %{\n+    __ eimull($dst$$Register, $src1$$Register, $src2$$Register, false);\n+  %}\n+  ins_pipe(ialu_reg_reg_alu0);\n+%}\n+\n+instruct mulI_rReg_imm(rRegI dst, rRegI src, immI imm, rFlagsReg cr)\n+%{\n+  match(Set dst (MulI src imm));\n+  effect(KILL cr);\n+\n+  ins_cost(300);\n+  format %{ \"imull   $dst, $src, $imm\\t# int\" %}\n+  ins_encode %{\n+    __ imull($dst$$Register, $src$$Register, $imm$$constant);\n+  %}\n+  ins_pipe(ialu_reg_reg_alu0);\n+%}\n+\n+instruct mulI_mem(rRegI dst, memory src, rFlagsReg cr)\n+%{\n+  predicate(!UseAPX);\n+  match(Set dst (MulI dst (LoadI src)));\n+  effect(KILL cr);\n+\n+  ins_cost(350);\n+  format %{ \"imull   $dst, $src\\t# int\" %}\n+  ins_encode %{\n+    __ imull($dst$$Register, $src$$Address);\n+  %}\n+  ins_pipe(ialu_reg_mem_alu0);\n+%}\n+\n+instruct mulI_rReg_rReg_mem_ndd(rRegI dst, rRegI src1, memory src2, rFlagsReg cr)\n+%{\n+  predicate(UseAPX);\n+  match(Set dst (MulI src1 (LoadI src2)));\n+  effect(KILL cr);\n+\n+  ins_cost(350);\n+  format %{ \"eimull   $dst, $src1, $src2\\t# int ndd\" %}\n+  ins_encode %{\n+    __ eimull($dst$$Register, $src1$$Register, $src2$$Address, false);\n+  %}\n+  ins_pipe(ialu_reg_mem_alu0);\n+%}\n+\n+instruct mulI_mem_imm(rRegI dst, memory src, immI imm, rFlagsReg cr)\n+%{\n+  match(Set dst (MulI (LoadI src) imm));\n+  effect(KILL cr);\n+\n+  ins_cost(300);\n+  format %{ \"imull   $dst, $src, $imm\\t# int\" %}\n+  ins_encode %{\n+    __ imull($dst$$Register, $src$$Address, $imm$$constant);\n+  %}\n+  ins_pipe(ialu_reg_mem_alu0);\n+%}\n+\n+instruct mulAddS2I_rReg(rRegI dst, rRegI src1, rRegI src2, rRegI src3, rFlagsReg cr)\n+%{\n+  match(Set dst (MulAddS2I (Binary dst src1) (Binary src2 src3)));\n+  effect(KILL cr, KILL src2);\n+\n+  expand %{ mulI_rReg(dst, src1, cr);\n+           mulI_rReg(src2, src3, cr);\n+           addI_rReg(dst, src2, cr); %}\n+%}\n+\n+instruct mulL_rReg(rRegL dst, rRegL src, rFlagsReg cr)\n+%{\n+  predicate(!UseAPX);\n+  match(Set dst (MulL dst src));\n+  effect(KILL cr);\n+\n+  ins_cost(300);\n+  format %{ \"imulq   $dst, $src\\t# long\" %}\n+  ins_encode %{\n+    __ imulq($dst$$Register, $src$$Register);\n+  %}\n+  ins_pipe(ialu_reg_reg_alu0);\n+%}\n+\n+instruct mulL_rReg_ndd(rRegL dst, rRegL src1, rRegL src2, rFlagsReg cr)\n+%{\n+  predicate(UseAPX);\n+  match(Set dst (MulL src1 src2));\n+  effect(KILL cr);\n+\n+  ins_cost(300);\n+  format %{ \"eimulq   $dst, $src1, $src2\\t# long ndd\" %}\n+  ins_encode %{\n+    __ eimulq($dst$$Register, $src1$$Register, $src2$$Register, false);\n+  %}\n+  ins_pipe(ialu_reg_reg_alu0);\n+%}\n+\n+instruct mulL_rReg_imm(rRegL dst, rRegL src, immL32 imm, rFlagsReg cr)\n+%{\n+  match(Set dst (MulL src imm));\n+  effect(KILL cr);\n+\n+  ins_cost(300);\n+  format %{ \"imulq   $dst, $src, $imm\\t# long\" %}\n+  ins_encode %{\n+    __ imulq($dst$$Register, $src$$Register, $imm$$constant);\n+  %}\n+  ins_pipe(ialu_reg_reg_alu0);\n+%}\n+\n+instruct mulL_mem(rRegL dst, memory src, rFlagsReg cr)\n+%{\n+  predicate(!UseAPX);\n+  match(Set dst (MulL dst (LoadL src)));\n+  effect(KILL cr);\n+\n+  ins_cost(350);\n+  format %{ \"imulq   $dst, $src\\t# long\" %}\n+  ins_encode %{\n+    __ imulq($dst$$Register, $src$$Address);\n+  %}\n+  ins_pipe(ialu_reg_mem_alu0);\n+%}\n+\n+instruct mulL_rReg_rReg_mem_ndd(rRegL dst, rRegL src1, memory src2, rFlagsReg cr)\n+%{\n+  predicate(UseAPX);\n+  match(Set dst (MulL src1 (LoadL src2)));\n+  effect(KILL cr);\n+\n+  ins_cost(350);\n+  format %{ \"eimulq   $dst, $src1, $src2 \\t# long\" %}\n+  ins_encode %{\n+    __ eimulq($dst$$Register, $src1$$Register, $src2$$Address, false);\n+  %}\n+  ins_pipe(ialu_reg_mem_alu0);\n+%}\n+\n+instruct mulL_mem_imm(rRegL dst, memory src, immL32 imm, rFlagsReg cr)\n+%{\n+  match(Set dst (MulL (LoadL src) imm));\n+  effect(KILL cr);\n+\n+  ins_cost(300);\n+  format %{ \"imulq   $dst, $src, $imm\\t# long\" %}\n+  ins_encode %{\n+    __ imulq($dst$$Register, $src$$Address, $imm$$constant);\n+  %}\n+  ins_pipe(ialu_reg_mem_alu0);\n+%}\n+\n+instruct mulHiL_rReg(rdx_RegL dst, rRegL src, rax_RegL rax, rFlagsReg cr)\n+%{\n+  match(Set dst (MulHiL src rax));\n+  effect(USE_KILL rax, KILL cr);\n+\n+  ins_cost(300);\n+  format %{ \"imulq   RDX:RAX, RAX, $src\\t# mulhi\" %}\n+  ins_encode %{\n+    __ imulq($src$$Register);\n+  %}\n+  ins_pipe(ialu_reg_reg_alu0);\n+%}\n+\n+instruct umulHiL_rReg(rdx_RegL dst, rRegL src, rax_RegL rax, rFlagsReg cr)\n+%{\n+  match(Set dst (UMulHiL src rax));\n+  effect(USE_KILL rax, KILL cr);\n+\n+  ins_cost(300);\n+  format %{ \"mulq   RDX:RAX, RAX, $src\\t# umulhi\" %}\n+  ins_encode %{\n+    __ mulq($src$$Register);\n+  %}\n+  ins_pipe(ialu_reg_reg_alu0);\n+%}\n+\n+instruct divI_rReg(rax_RegI rax, rdx_RegI rdx, no_rax_rdx_RegI div,\n+                   rFlagsReg cr)\n+%{\n+  match(Set rax (DivI rax div));\n+  effect(KILL rdx, KILL cr);\n+\n+  ins_cost(30*100+10*100); \/\/ XXX\n+  format %{ \"cmpl    rax, 0x80000000\\t# idiv\\n\\t\"\n+            \"jne,s   normal\\n\\t\"\n+            \"xorl    rdx, rdx\\n\\t\"\n+            \"cmpl    $div, -1\\n\\t\"\n+            \"je,s    done\\n\"\n+    \"normal: cdql\\n\\t\"\n+            \"idivl   $div\\n\"\n+    \"done:\"        %}\n+  ins_encode(cdql_enc(div));\n+  ins_pipe(ialu_reg_reg_alu0);\n+%}\n+\n+instruct divL_rReg(rax_RegL rax, rdx_RegL rdx, no_rax_rdx_RegL div,\n+                   rFlagsReg cr)\n+%{\n+  match(Set rax (DivL rax div));\n+  effect(KILL rdx, KILL cr);\n+\n+  ins_cost(30*100+10*100); \/\/ XXX\n+  format %{ \"movq    rdx, 0x8000000000000000\\t# ldiv\\n\\t\"\n+            \"cmpq    rax, rdx\\n\\t\"\n+            \"jne,s   normal\\n\\t\"\n+            \"xorl    rdx, rdx\\n\\t\"\n+            \"cmpq    $div, -1\\n\\t\"\n+            \"je,s    done\\n\"\n+    \"normal: cdqq\\n\\t\"\n+            \"idivq   $div\\n\"\n+    \"done:\"        %}\n+  ins_encode(cdqq_enc(div));\n+  ins_pipe(ialu_reg_reg_alu0);\n+%}\n+\n+instruct udivI_rReg(rax_RegI rax, rdx_RegI rdx, no_rax_rdx_RegI div, rFlagsReg cr)\n+%{\n+  match(Set rax (UDivI rax div));\n+  effect(KILL rdx, KILL cr);\n+\n+  ins_cost(300);\n+  format %{ \"udivl $rax,$rax,$div\\t# UDivI\\n\" %}\n+  ins_encode %{\n+    __ udivI($rax$$Register, $div$$Register, $rdx$$Register);\n+  %}\n+  ins_pipe(ialu_reg_reg_alu0);\n+%}\n+\n+instruct udivL_rReg(rax_RegL rax, rdx_RegL rdx, no_rax_rdx_RegL div, rFlagsReg cr)\n+%{\n+  match(Set rax (UDivL rax div));\n+  effect(KILL rdx, KILL cr);\n+\n+  ins_cost(300);\n+  format %{ \"udivq $rax,$rax,$div\\t# UDivL\\n\" %}\n+  ins_encode %{\n+     __ udivL($rax$$Register, $div$$Register, $rdx$$Register);\n+  %}\n+  ins_pipe(ialu_reg_reg_alu0);\n+%}\n+\n+\/\/ Integer DIVMOD with Register, both quotient and mod results\n+instruct divModI_rReg_divmod(rax_RegI rax, rdx_RegI rdx, no_rax_rdx_RegI div,\n+                             rFlagsReg cr)\n+%{\n+  match(DivModI rax div);\n+  effect(KILL cr);\n+\n+  ins_cost(30*100+10*100); \/\/ XXX\n+  format %{ \"cmpl    rax, 0x80000000\\t# idiv\\n\\t\"\n+            \"jne,s   normal\\n\\t\"\n+            \"xorl    rdx, rdx\\n\\t\"\n+            \"cmpl    $div, -1\\n\\t\"\n+            \"je,s    done\\n\"\n+    \"normal: cdql\\n\\t\"\n+            \"idivl   $div\\n\"\n+    \"done:\"        %}\n+  ins_encode(cdql_enc(div));\n+  ins_pipe(pipe_slow);\n+%}\n+\n+\/\/ Long DIVMOD with Register, both quotient and mod results\n+instruct divModL_rReg_divmod(rax_RegL rax, rdx_RegL rdx, no_rax_rdx_RegL div,\n+                             rFlagsReg cr)\n+%{\n+  match(DivModL rax div);\n+  effect(KILL cr);\n+\n+  ins_cost(30*100+10*100); \/\/ XXX\n+  format %{ \"movq    rdx, 0x8000000000000000\\t# ldiv\\n\\t\"\n+            \"cmpq    rax, rdx\\n\\t\"\n+            \"jne,s   normal\\n\\t\"\n+            \"xorl    rdx, rdx\\n\\t\"\n+            \"cmpq    $div, -1\\n\\t\"\n+            \"je,s    done\\n\"\n+    \"normal: cdqq\\n\\t\"\n+            \"idivq   $div\\n\"\n+    \"done:\"        %}\n+  ins_encode(cdqq_enc(div));\n+  ins_pipe(pipe_slow);\n+%}\n+\n+\/\/ Unsigned integer DIVMOD with Register, both quotient and mod results\n+instruct udivModI_rReg_divmod(rax_RegI rax, no_rax_rdx_RegI tmp, rdx_RegI rdx,\n+                              no_rax_rdx_RegI div, rFlagsReg cr)\n+%{\n+  match(UDivModI rax div);\n+  effect(TEMP tmp, KILL cr);\n+\n+  ins_cost(300);\n+  format %{ \"udivl $rax,$rax,$div\\t# begin UDivModI\\n\\t\"\n+            \"umodl $rdx,$rax,$div\\t! using $tmp as TEMP # end UDivModI\\n\"\n+          %}\n+  ins_encode %{\n+    __ udivmodI($rax$$Register, $div$$Register, $rdx$$Register, $tmp$$Register);\n+  %}\n+  ins_pipe(pipe_slow);\n+%}\n+\n+\/\/ Unsigned long DIVMOD with Register, both quotient and mod results\n+instruct udivModL_rReg_divmod(rax_RegL rax, no_rax_rdx_RegL tmp, rdx_RegL rdx,\n+                              no_rax_rdx_RegL div, rFlagsReg cr)\n+%{\n+  match(UDivModL rax div);\n+  effect(TEMP tmp, KILL cr);\n+\n+  ins_cost(300);\n+  format %{ \"udivq $rax,$rax,$div\\t# begin UDivModL\\n\\t\"\n+            \"umodq $rdx,$rax,$div\\t! using $tmp as TEMP # end UDivModL\\n\"\n+          %}\n+  ins_encode %{\n+    __ udivmodL($rax$$Register, $div$$Register, $rdx$$Register, $tmp$$Register);\n+  %}\n+  ins_pipe(pipe_slow);\n+%}\n+\n+instruct modI_rReg(rdx_RegI rdx, rax_RegI rax, no_rax_rdx_RegI div,\n+                   rFlagsReg cr)\n+%{\n+  match(Set rdx (ModI rax div));\n+  effect(KILL rax, KILL cr);\n+\n+  ins_cost(300); \/\/ XXX\n+  format %{ \"cmpl    rax, 0x80000000\\t# irem\\n\\t\"\n+            \"jne,s   normal\\n\\t\"\n+            \"xorl    rdx, rdx\\n\\t\"\n+            \"cmpl    $div, -1\\n\\t\"\n+            \"je,s    done\\n\"\n+    \"normal: cdql\\n\\t\"\n+            \"idivl   $div\\n\"\n+    \"done:\"        %}\n+  ins_encode(cdql_enc(div));\n+  ins_pipe(ialu_reg_reg_alu0);\n+%}\n+\n+instruct modL_rReg(rdx_RegL rdx, rax_RegL rax, no_rax_rdx_RegL div,\n+                   rFlagsReg cr)\n+%{\n+  match(Set rdx (ModL rax div));\n+  effect(KILL rax, KILL cr);\n+\n+  ins_cost(300); \/\/ XXX\n+  format %{ \"movq    rdx, 0x8000000000000000\\t# lrem\\n\\t\"\n+            \"cmpq    rax, rdx\\n\\t\"\n+            \"jne,s   normal\\n\\t\"\n+            \"xorl    rdx, rdx\\n\\t\"\n+            \"cmpq    $div, -1\\n\\t\"\n+            \"je,s    done\\n\"\n+    \"normal: cdqq\\n\\t\"\n+            \"idivq   $div\\n\"\n+    \"done:\"        %}\n+  ins_encode(cdqq_enc(div));\n+  ins_pipe(ialu_reg_reg_alu0);\n+%}\n+\n+instruct umodI_rReg(rdx_RegI rdx, rax_RegI rax, no_rax_rdx_RegI div, rFlagsReg cr)\n+%{\n+  match(Set rdx (UModI rax div));\n+  effect(KILL rax, KILL cr);\n+\n+  ins_cost(300);\n+  format %{ \"umodl $rdx,$rax,$div\\t# UModI\\n\" %}\n+  ins_encode %{\n+    __ umodI($rax$$Register, $div$$Register, $rdx$$Register);\n+  %}\n+  ins_pipe(ialu_reg_reg_alu0);\n+%}\n+\n+instruct umodL_rReg(rdx_RegL rdx, rax_RegL rax, no_rax_rdx_RegL div, rFlagsReg cr)\n+%{\n+  match(Set rdx (UModL rax div));\n+  effect(KILL rax, KILL cr);\n+\n+  ins_cost(300);\n+  format %{ \"umodq $rdx,$rax,$div\\t# UModL\\n\" %}\n+  ins_encode %{\n+    __ umodL($rax$$Register, $div$$Register, $rdx$$Register);\n+  %}\n+  ins_pipe(ialu_reg_reg_alu0);\n+%}\n+\n+\/\/ Integer Shift Instructions\n+\/\/ Shift Left by one, two, three\n+instruct salI_rReg_immI2(rRegI dst, immI2 shift, rFlagsReg cr)\n+%{\n+  predicate(!UseAPX);\n+  match(Set dst (LShiftI dst shift));\n+  effect(KILL cr);\n+\n+  format %{ \"sall    $dst, $shift\" %}\n+  ins_encode %{\n+    __ sall($dst$$Register, $shift$$constant);\n+  %}\n+  ins_pipe(ialu_reg);\n+%}\n+\n+\/\/ Shift Left by one, two, three\n+instruct salI_rReg_immI2_ndd(rRegI dst, rRegI src, immI2 shift, rFlagsReg cr)\n+%{\n+  predicate(UseAPX);\n+  match(Set dst (LShiftI src shift));\n+  effect(KILL cr);\n+\n+  format %{ \"esall    $dst, $src, $shift\\t# int(ndd)\" %}\n+  ins_encode %{\n+    __ esall($dst$$Register, $src$$Register, $shift$$constant, false);\n+  %}\n+  ins_pipe(ialu_reg);\n+%}\n+\n+\/\/ Shift Left by 8-bit immediate\n+instruct salI_rReg_imm(rRegI dst, immI8 shift, rFlagsReg cr)\n+%{\n+  predicate(!UseAPX);\n+  match(Set dst (LShiftI dst shift));\n+  effect(KILL cr);\n+\n+  format %{ \"sall    $dst, $shift\" %}\n+  ins_encode %{\n+    __ sall($dst$$Register, $shift$$constant);\n+  %}\n+  ins_pipe(ialu_reg);\n+%}\n+\n+\/\/ Shift Left by 8-bit immediate\n+instruct salI_rReg_imm_ndd(rRegI dst, rRegI src, immI8 shift, rFlagsReg cr)\n+%{\n+  predicate(UseAPX);\n+  match(Set dst (LShiftI src shift));\n+  effect(KILL cr);\n+\n+  format %{ \"esall    $dst, $src, $shift\\t# int (ndd)\" %}\n+  ins_encode %{\n+    __ esall($dst$$Register, $src$$Register, $shift$$constant, false);\n+  %}\n+  ins_pipe(ialu_reg);\n+%}\n+\n+instruct salI_rReg_mem_imm_ndd(rRegI dst, memory src, immI8 shift, rFlagsReg cr)\n+%{\n+  predicate(UseAPX);\n+  match(Set dst (LShiftI (LoadI src) shift));\n+  effect(KILL cr);\n+\n+  format %{ \"esall    $dst, $src, $shift\\t# int (ndd)\" %}\n+  ins_encode %{\n+    __ esall($dst$$Register, $src$$Address, $shift$$constant, false);\n+  %}\n+  ins_pipe(ialu_reg);\n+%}\n+\n+\/\/ Shift Left by 8-bit immediate\n+instruct salI_mem_imm(memory dst, immI8 shift, rFlagsReg cr)\n+%{\n+  match(Set dst (StoreI dst (LShiftI (LoadI dst) shift)));\n+  effect(KILL cr);\n+\n+  format %{ \"sall    $dst, $shift\" %}\n+  ins_encode %{\n+    __ sall($dst$$Address, $shift$$constant);\n+  %}\n+  ins_pipe(ialu_mem_imm);\n+%}\n+\n+\/\/ Shift Left by variable\n+instruct salI_rReg_CL(rRegI dst, rcx_RegI shift, rFlagsReg cr)\n+%{\n+  predicate(!VM_Version::supports_bmi2());\n+  match(Set dst (LShiftI dst shift));\n+  effect(KILL cr);\n+\n+  format %{ \"sall    $dst, $shift\" %}\n+  ins_encode %{\n+    __ sall($dst$$Register);\n+  %}\n+  ins_pipe(ialu_reg_reg);\n+%}\n+\n+\/\/ Shift Left by variable\n+instruct salI_mem_CL(memory dst, rcx_RegI shift, rFlagsReg cr)\n+%{\n+  predicate(!VM_Version::supports_bmi2());\n+  match(Set dst (StoreI dst (LShiftI (LoadI dst) shift)));\n+  effect(KILL cr);\n+\n+  format %{ \"sall    $dst, $shift\" %}\n+  ins_encode %{\n+    __ sall($dst$$Address);\n+  %}\n+  ins_pipe(ialu_mem_reg);\n+%}\n+\n+instruct salI_rReg_rReg(rRegI dst, rRegI src, rRegI shift)\n+%{\n+  predicate(VM_Version::supports_bmi2());\n+  match(Set dst (LShiftI src shift));\n+\n+  format %{ \"shlxl   $dst, $src, $shift\" %}\n+  ins_encode %{\n+    __ shlxl($dst$$Register, $src$$Register, $shift$$Register);\n+  %}\n+  ins_pipe(ialu_reg_reg);\n+%}\n+\n+instruct salI_mem_rReg(rRegI dst, memory src, rRegI shift)\n+%{\n+  predicate(VM_Version::supports_bmi2());\n+  match(Set dst (LShiftI (LoadI src) shift));\n+  ins_cost(175);\n+  format %{ \"shlxl   $dst, $src, $shift\" %}\n+  ins_encode %{\n+    __ shlxl($dst$$Register, $src$$Address, $shift$$Register);\n+  %}\n+  ins_pipe(ialu_reg_mem);\n+%}\n+\n+\/\/ Arithmetic Shift Right by 8-bit immediate\n+instruct sarI_rReg_imm(rRegI dst, immI8 shift, rFlagsReg cr)\n+%{\n+  predicate(!UseAPX);\n+  match(Set dst (RShiftI dst shift));\n+  effect(KILL cr);\n+\n+  format %{ \"sarl    $dst, $shift\" %}\n+  ins_encode %{\n+    __ sarl($dst$$Register, $shift$$constant);\n+  %}\n+  ins_pipe(ialu_mem_imm);\n+%}\n+\n+\/\/ Arithmetic Shift Right by 8-bit immediate\n+instruct sarI_rReg_imm_ndd(rRegI dst, rRegI src, immI8 shift, rFlagsReg cr)\n+%{\n+  predicate(UseAPX);\n+  match(Set dst (RShiftI src shift));\n+  effect(KILL cr);\n+\n+  format %{ \"esarl    $dst, $src, $shift\\t# int (ndd)\" %}\n+  ins_encode %{\n+    __ esarl($dst$$Register, $src$$Register, $shift$$constant, false);\n+  %}\n+  ins_pipe(ialu_mem_imm);\n+%}\n+\n+instruct sarI_rReg_mem_imm_ndd(rRegI dst, memory src, immI8 shift, rFlagsReg cr)\n+%{\n+  predicate(UseAPX);\n+  match(Set dst (RShiftI (LoadI src) shift));\n+  effect(KILL cr);\n+\n+  format %{ \"esarl    $dst, $src, $shift\\t# int (ndd)\" %}\n+  ins_encode %{\n+    __ esarl($dst$$Register, $src$$Address, $shift$$constant, false);\n+  %}\n+  ins_pipe(ialu_mem_imm);\n+%}\n+\n+\/\/ Arithmetic Shift Right by 8-bit immediate\n+instruct sarI_mem_imm(memory dst, immI8 shift, rFlagsReg cr)\n+%{\n+  match(Set dst (StoreI dst (RShiftI (LoadI dst) shift)));\n+  effect(KILL cr);\n+\n+  format %{ \"sarl    $dst, $shift\" %}\n+  ins_encode %{\n+    __ sarl($dst$$Address, $shift$$constant);\n+  %}\n+  ins_pipe(ialu_mem_imm);\n+%}\n+\n+\/\/ Arithmetic Shift Right by variable\n+instruct sarI_rReg_CL(rRegI dst, rcx_RegI shift, rFlagsReg cr)\n+%{\n+  predicate(!VM_Version::supports_bmi2());\n+  match(Set dst (RShiftI dst shift));\n+  effect(KILL cr);\n+\n+  format %{ \"sarl    $dst, $shift\" %}\n+  ins_encode %{\n+    __ sarl($dst$$Register);\n+  %}\n+  ins_pipe(ialu_reg_reg);\n+%}\n+\n+\/\/ Arithmetic Shift Right by variable\n+instruct sarI_mem_CL(memory dst, rcx_RegI shift, rFlagsReg cr)\n+%{\n+  predicate(!VM_Version::supports_bmi2());\n+  match(Set dst (StoreI dst (RShiftI (LoadI dst) shift)));\n+  effect(KILL cr);\n+\n+  format %{ \"sarl    $dst, $shift\" %}\n+  ins_encode %{\n+    __ sarl($dst$$Address);\n+  %}\n+  ins_pipe(ialu_mem_reg);\n+%}\n+\n+instruct sarI_rReg_rReg(rRegI dst, rRegI src, rRegI shift)\n+%{\n+  predicate(VM_Version::supports_bmi2());\n+  match(Set dst (RShiftI src shift));\n+\n+  format %{ \"sarxl   $dst, $src, $shift\" %}\n+  ins_encode %{\n+    __ sarxl($dst$$Register, $src$$Register, $shift$$Register);\n+  %}\n+  ins_pipe(ialu_reg_reg);\n+%}\n+\n+instruct sarI_mem_rReg(rRegI dst, memory src, rRegI shift)\n+%{\n+  predicate(VM_Version::supports_bmi2());\n+  match(Set dst (RShiftI (LoadI src) shift));\n+  ins_cost(175);\n+  format %{ \"sarxl   $dst, $src, $shift\" %}\n+  ins_encode %{\n+    __ sarxl($dst$$Register, $src$$Address, $shift$$Register);\n+  %}\n+  ins_pipe(ialu_reg_mem);\n+%}\n+\n+\/\/ Logical Shift Right by 8-bit immediate\n+instruct shrI_rReg_imm(rRegI dst, immI8 shift, rFlagsReg cr)\n+%{\n+  predicate(!UseAPX);\n+  match(Set dst (URShiftI dst shift));\n+  effect(KILL cr);\n+\n+  format %{ \"shrl    $dst, $shift\" %}\n+  ins_encode %{\n+    __ shrl($dst$$Register, $shift$$constant);\n+  %}\n+  ins_pipe(ialu_reg);\n+%}\n+\n+\/\/ Logical Shift Right by 8-bit immediate\n+instruct shrI_rReg_imm_ndd(rRegI dst, rRegI src, immI8 shift, rFlagsReg cr)\n+%{\n+  predicate(UseAPX);\n+  match(Set dst (URShiftI src shift));\n+  effect(KILL cr);\n+\n+  format %{ \"eshrl    $dst, $src, $shift\\t # int (ndd)\" %}\n+  ins_encode %{\n+    __ eshrl($dst$$Register, $src$$Register, $shift$$constant, false);\n+  %}\n+  ins_pipe(ialu_reg);\n+%}\n+\n+instruct shrI_rReg_mem_imm_ndd(rRegI dst, memory src, immI8 shift, rFlagsReg cr)\n+%{\n+  predicate(UseAPX);\n+  match(Set dst (URShiftI (LoadI src) shift));\n+  effect(KILL cr);\n+\n+  format %{ \"eshrl    $dst, $src, $shift\\t # int (ndd)\" %}\n+  ins_encode %{\n+    __ eshrl($dst$$Register, $src$$Address, $shift$$constant, false);\n+  %}\n+  ins_pipe(ialu_reg);\n+%}\n+\n+\/\/ Logical Shift Right by 8-bit immediate\n+instruct shrI_mem_imm(memory dst, immI8 shift, rFlagsReg cr)\n+%{\n+  match(Set dst (StoreI dst (URShiftI (LoadI dst) shift)));\n+  effect(KILL cr);\n+\n+  format %{ \"shrl    $dst, $shift\" %}\n+  ins_encode %{\n+    __ shrl($dst$$Address, $shift$$constant);\n+  %}\n+  ins_pipe(ialu_mem_imm);\n+%}\n+\n+\/\/ Logical Shift Right by variable\n+instruct shrI_rReg_CL(rRegI dst, rcx_RegI shift, rFlagsReg cr)\n+%{\n+  predicate(!VM_Version::supports_bmi2());\n+  match(Set dst (URShiftI dst shift));\n+  effect(KILL cr);\n+\n+  format %{ \"shrl    $dst, $shift\" %}\n+  ins_encode %{\n+    __ shrl($dst$$Register);\n+  %}\n+  ins_pipe(ialu_reg_reg);\n+%}\n+\n+\/\/ Logical Shift Right by variable\n+instruct shrI_mem_CL(memory dst, rcx_RegI shift, rFlagsReg cr)\n+%{\n+  predicate(!VM_Version::supports_bmi2());\n+  match(Set dst (StoreI dst (URShiftI (LoadI dst) shift)));\n+  effect(KILL cr);\n+\n+  format %{ \"shrl    $dst, $shift\" %}\n+  ins_encode %{\n+    __ shrl($dst$$Address);\n+  %}\n+  ins_pipe(ialu_mem_reg);\n+%}\n+\n+instruct shrI_rReg_rReg(rRegI dst, rRegI src, rRegI shift)\n+%{\n+  predicate(VM_Version::supports_bmi2());\n+  match(Set dst (URShiftI src shift));\n+\n+  format %{ \"shrxl   $dst, $src, $shift\" %}\n+  ins_encode %{\n+    __ shrxl($dst$$Register, $src$$Register, $shift$$Register);\n+  %}\n+  ins_pipe(ialu_reg_reg);\n+%}\n+\n+instruct shrI_mem_rReg(rRegI dst, memory src, rRegI shift)\n+%{\n+  predicate(VM_Version::supports_bmi2());\n+  match(Set dst (URShiftI (LoadI src) shift));\n+  ins_cost(175);\n+  format %{ \"shrxl   $dst, $src, $shift\" %}\n+  ins_encode %{\n+    __ shrxl($dst$$Register, $src$$Address, $shift$$Register);\n+  %}\n+  ins_pipe(ialu_reg_mem);\n+%}\n+\n+\/\/ Long Shift Instructions\n+\/\/ Shift Left by one, two, three\n+instruct salL_rReg_immI2(rRegL dst, immI2 shift, rFlagsReg cr)\n+%{\n+  predicate(!UseAPX);\n+  match(Set dst (LShiftL dst shift));\n+  effect(KILL cr);\n+\n+  format %{ \"salq    $dst, $shift\" %}\n+  ins_encode %{\n+    __ salq($dst$$Register, $shift$$constant);\n+  %}\n+  ins_pipe(ialu_reg);\n+%}\n+\n+\/\/ Shift Left by one, two, three\n+instruct salL_rReg_immI2_ndd(rRegL dst, rRegL src, immI2 shift, rFlagsReg cr)\n+%{\n+  predicate(UseAPX);\n+  match(Set dst (LShiftL src shift));\n+  effect(KILL cr);\n+\n+  format %{ \"esalq    $dst, $src, $shift\\t# long (ndd)\" %}\n+  ins_encode %{\n+    __ esalq($dst$$Register, $src$$Register, $shift$$constant, false);\n+  %}\n+  ins_pipe(ialu_reg);\n+%}\n+\n+\/\/ Shift Left by 8-bit immediate\n+instruct salL_rReg_imm(rRegL dst, immI8 shift, rFlagsReg cr)\n+%{\n+  predicate(!UseAPX);\n+  match(Set dst (LShiftL dst shift));\n+  effect(KILL cr);\n+\n+  format %{ \"salq    $dst, $shift\" %}\n+  ins_encode %{\n+    __ salq($dst$$Register, $shift$$constant);\n+  %}\n+  ins_pipe(ialu_reg);\n+%}\n+\n+\/\/ Shift Left by 8-bit immediate\n+instruct salL_rReg_imm_ndd(rRegL dst, rRegL src, immI8 shift, rFlagsReg cr)\n+%{\n+  predicate(UseAPX);\n+  match(Set dst (LShiftL src shift));\n+  effect(KILL cr);\n+\n+  format %{ \"esalq    $dst, $src, $shift\\t# long (ndd)\" %}\n+  ins_encode %{\n+    __ esalq($dst$$Register, $src$$Register, $shift$$constant, false);\n+  %}\n+  ins_pipe(ialu_reg);\n+%}\n+\n+instruct salL_rReg_mem_imm_ndd(rRegL dst, memory src, immI8 shift, rFlagsReg cr)\n+%{\n+  predicate(UseAPX);\n+  match(Set dst (LShiftL (LoadL src) shift));\n+  effect(KILL cr);\n+\n+  format %{ \"esalq    $dst, $src, $shift\\t# long (ndd)\" %}\n+  ins_encode %{\n+    __ esalq($dst$$Register, $src$$Address, $shift$$constant, false);\n+  %}\n+  ins_pipe(ialu_reg);\n+%}\n+\n+\/\/ Shift Left by 8-bit immediate\n+instruct salL_mem_imm(memory dst, immI8 shift, rFlagsReg cr)\n+%{\n+  match(Set dst (StoreL dst (LShiftL (LoadL dst) shift)));\n+  effect(KILL cr);\n+\n+  format %{ \"salq    $dst, $shift\" %}\n+  ins_encode %{\n+    __ salq($dst$$Address, $shift$$constant);\n+  %}\n+  ins_pipe(ialu_mem_imm);\n+%}\n+\n+\/\/ Shift Left by variable\n+instruct salL_rReg_CL(rRegL dst, rcx_RegI shift, rFlagsReg cr)\n+%{\n+  predicate(!VM_Version::supports_bmi2());\n+  match(Set dst (LShiftL dst shift));\n+  effect(KILL cr);\n+\n+  format %{ \"salq    $dst, $shift\" %}\n+  ins_encode %{\n+    __ salq($dst$$Register);\n+  %}\n+  ins_pipe(ialu_reg_reg);\n+%}\n+\n+\/\/ Shift Left by variable\n+instruct salL_mem_CL(memory dst, rcx_RegI shift, rFlagsReg cr)\n+%{\n+  predicate(!VM_Version::supports_bmi2());\n+  match(Set dst (StoreL dst (LShiftL (LoadL dst) shift)));\n+  effect(KILL cr);\n+\n+  format %{ \"salq    $dst, $shift\" %}\n+  ins_encode %{\n+    __ salq($dst$$Address);\n+  %}\n+  ins_pipe(ialu_mem_reg);\n+%}\n+\n+instruct salL_rReg_rReg(rRegL dst, rRegL src, rRegI shift)\n+%{\n+  predicate(VM_Version::supports_bmi2());\n+  match(Set dst (LShiftL src shift));\n+\n+  format %{ \"shlxq   $dst, $src, $shift\" %}\n+  ins_encode %{\n+    __ shlxq($dst$$Register, $src$$Register, $shift$$Register);\n+  %}\n+  ins_pipe(ialu_reg_reg);\n+%}\n+\n+instruct salL_mem_rReg(rRegL dst, memory src, rRegI shift)\n+%{\n+  predicate(VM_Version::supports_bmi2());\n+  match(Set dst (LShiftL (LoadL src) shift));\n+  ins_cost(175);\n+  format %{ \"shlxq   $dst, $src, $shift\" %}\n+  ins_encode %{\n+    __ shlxq($dst$$Register, $src$$Address, $shift$$Register);\n+  %}\n+  ins_pipe(ialu_reg_mem);\n+%}\n+\n+\/\/ Arithmetic Shift Right by 8-bit immediate\n+instruct sarL_rReg_imm(rRegL dst, immI shift, rFlagsReg cr)\n+%{\n+  predicate(!UseAPX);\n+  match(Set dst (RShiftL dst shift));\n+  effect(KILL cr);\n+\n+  format %{ \"sarq    $dst, $shift\" %}\n+  ins_encode %{\n+    __ sarq($dst$$Register, (unsigned char)($shift$$constant & 0x3F));\n+  %}\n+  ins_pipe(ialu_mem_imm);\n+%}\n+\n+\/\/ Arithmetic Shift Right by 8-bit immediate\n+instruct sarL_rReg_imm_ndd(rRegL dst, rRegL src, immI shift, rFlagsReg cr)\n+%{\n+  predicate(UseAPX);\n+  match(Set dst (RShiftL src shift));\n+  effect(KILL cr);\n+\n+  format %{ \"esarq    $dst, $src, $shift\\t# long (ndd)\" %}\n+  ins_encode %{\n+    __ esarq($dst$$Register, $src$$Register, (unsigned char)($shift$$constant & 0x3F), false);\n+  %}\n+  ins_pipe(ialu_mem_imm);\n+%}\n+\n+instruct sarL_rReg_mem_imm_ndd(rRegL dst, memory src, immI shift, rFlagsReg cr)\n+%{\n+  predicate(UseAPX);\n+  match(Set dst (RShiftL (LoadL src) shift));\n+  effect(KILL cr);\n+\n+  format %{ \"esarq    $dst, $src, $shift\\t# long (ndd)\" %}\n+  ins_encode %{\n+    __ esarq($dst$$Register, $src$$Address, (unsigned char)($shift$$constant & 0x3F), false);\n+  %}\n+  ins_pipe(ialu_mem_imm);\n+%}\n+\n+\/\/ Arithmetic Shift Right by 8-bit immediate\n+instruct sarL_mem_imm(memory dst, immI shift, rFlagsReg cr)\n+%{\n+  match(Set dst (StoreL dst (RShiftL (LoadL dst) shift)));\n+  effect(KILL cr);\n+\n+  format %{ \"sarq    $dst, $shift\" %}\n+  ins_encode %{\n+    __ sarq($dst$$Address, (unsigned char)($shift$$constant & 0x3F));\n+  %}\n+  ins_pipe(ialu_mem_imm);\n+%}\n+\n+\/\/ Arithmetic Shift Right by variable\n+instruct sarL_rReg_CL(rRegL dst, rcx_RegI shift, rFlagsReg cr)\n+%{\n+  predicate(!VM_Version::supports_bmi2());\n+  match(Set dst (RShiftL dst shift));\n+  effect(KILL cr);\n+\n+  format %{ \"sarq    $dst, $shift\" %}\n+  ins_encode %{\n+    __ sarq($dst$$Register);\n+  %}\n+  ins_pipe(ialu_reg_reg);\n+%}\n+\n+\/\/ Arithmetic Shift Right by variable\n+instruct sarL_mem_CL(memory dst, rcx_RegI shift, rFlagsReg cr)\n+%{\n+  predicate(!VM_Version::supports_bmi2());\n+  match(Set dst (StoreL dst (RShiftL (LoadL dst) shift)));\n+  effect(KILL cr);\n+\n+  format %{ \"sarq    $dst, $shift\" %}\n+  ins_encode %{\n+    __ sarq($dst$$Address);\n+  %}\n+  ins_pipe(ialu_mem_reg);\n+%}\n+\n+instruct sarL_rReg_rReg(rRegL dst, rRegL src, rRegI shift)\n+%{\n+  predicate(VM_Version::supports_bmi2());\n+  match(Set dst (RShiftL src shift));\n+\n+  format %{ \"sarxq   $dst, $src, $shift\" %}\n+  ins_encode %{\n+    __ sarxq($dst$$Register, $src$$Register, $shift$$Register);\n+  %}\n+  ins_pipe(ialu_reg_reg);\n+%}\n+\n+instruct sarL_mem_rReg(rRegL dst, memory src, rRegI shift)\n+%{\n+  predicate(VM_Version::supports_bmi2());\n+  match(Set dst (RShiftL (LoadL src) shift));\n+  ins_cost(175);\n+  format %{ \"sarxq   $dst, $src, $shift\" %}\n+  ins_encode %{\n+    __ sarxq($dst$$Register, $src$$Address, $shift$$Register);\n+  %}\n+  ins_pipe(ialu_reg_mem);\n+%}\n+\n+\/\/ Logical Shift Right by 8-bit immediate\n+instruct shrL_rReg_imm(rRegL dst, immI8 shift, rFlagsReg cr)\n+%{\n+  predicate(!UseAPX);\n+  match(Set dst (URShiftL dst shift));\n+  effect(KILL cr);\n+\n+  format %{ \"shrq    $dst, $shift\" %}\n+  ins_encode %{\n+    __ shrq($dst$$Register, $shift$$constant);\n+  %}\n+  ins_pipe(ialu_reg);\n+%}\n+\n+\/\/ Logical Shift Right by 8-bit immediate\n+instruct shrL_rReg_imm_ndd(rRegL dst, rRegL src, immI8 shift, rFlagsReg cr)\n+%{\n+  predicate(UseAPX);\n+  match(Set dst (URShiftL src shift));\n+  effect(KILL cr);\n+\n+  format %{ \"eshrq    $dst, $src, $shift\\t# long (ndd)\" %}\n+  ins_encode %{\n+    __ eshrq($dst$$Register, $src$$Register, $shift$$constant, false);\n+  %}\n+  ins_pipe(ialu_reg);\n+%}\n+\n+instruct shrL_rReg_mem_imm_ndd(rRegL dst, memory src, immI8 shift, rFlagsReg cr)\n+%{\n+  predicate(UseAPX);\n+  match(Set dst (URShiftL (LoadL src) shift));\n+  effect(KILL cr);\n+\n+  format %{ \"eshrq    $dst, $src, $shift\\t# long (ndd)\" %}\n+  ins_encode %{\n+    __ eshrq($dst$$Register, $src$$Address, $shift$$constant, false);\n+  %}\n+  ins_pipe(ialu_reg);\n+%}\n+\n+\/\/ Logical Shift Right by 8-bit immediate\n+instruct shrL_mem_imm(memory dst, immI8 shift, rFlagsReg cr)\n+%{\n+  match(Set dst (StoreL dst (URShiftL (LoadL dst) shift)));\n+  effect(KILL cr);\n+\n+  format %{ \"shrq    $dst, $shift\" %}\n+  ins_encode %{\n+    __ shrq($dst$$Address, $shift$$constant);\n+  %}\n+  ins_pipe(ialu_mem_imm);\n+%}\n+\n+\/\/ Logical Shift Right by variable\n+instruct shrL_rReg_CL(rRegL dst, rcx_RegI shift, rFlagsReg cr)\n+%{\n+  predicate(!VM_Version::supports_bmi2());\n+  match(Set dst (URShiftL dst shift));\n+  effect(KILL cr);\n+\n+  format %{ \"shrq    $dst, $shift\" %}\n+  ins_encode %{\n+    __ shrq($dst$$Register);\n+  %}\n+  ins_pipe(ialu_reg_reg);\n+%}\n+\n+\/\/ Logical Shift Right by variable\n+instruct shrL_mem_CL(memory dst, rcx_RegI shift, rFlagsReg cr)\n+%{\n+  predicate(!VM_Version::supports_bmi2());\n+  match(Set dst (StoreL dst (URShiftL (LoadL dst) shift)));\n+  effect(KILL cr);\n+\n+  format %{ \"shrq    $dst, $shift\" %}\n+  ins_encode %{\n+    __ shrq($dst$$Address);\n+  %}\n+  ins_pipe(ialu_mem_reg);\n+%}\n+\n+instruct shrL_rReg_rReg(rRegL dst, rRegL src, rRegI shift)\n+%{\n+  predicate(VM_Version::supports_bmi2());\n+  match(Set dst (URShiftL src shift));\n+\n+  format %{ \"shrxq   $dst, $src, $shift\" %}\n+  ins_encode %{\n+    __ shrxq($dst$$Register, $src$$Register, $shift$$Register);\n+  %}\n+  ins_pipe(ialu_reg_reg);\n+%}\n+\n+instruct shrL_mem_rReg(rRegL dst, memory src, rRegI shift)\n+%{\n+  predicate(VM_Version::supports_bmi2());\n+  match(Set dst (URShiftL (LoadL src) shift));\n+  ins_cost(175);\n+  format %{ \"shrxq   $dst, $src, $shift\" %}\n+  ins_encode %{\n+    __ shrxq($dst$$Register, $src$$Address, $shift$$Register);\n+  %}\n+  ins_pipe(ialu_reg_mem);\n+%}\n+\n+\/\/ Logical Shift Right by 24, followed by Arithmetic Shift Left by 24.\n+\/\/ This idiom is used by the compiler for the i2b bytecode.\n+instruct i2b(rRegI dst, rRegI src, immI_24 twentyfour)\n+%{\n+  match(Set dst (RShiftI (LShiftI src twentyfour) twentyfour));\n+\n+  format %{ \"movsbl  $dst, $src\\t# i2b\" %}\n+  ins_encode %{\n+    __ movsbl($dst$$Register, $src$$Register);\n+  %}\n+  ins_pipe(ialu_reg_reg);\n+%}\n+\n+\/\/ Logical Shift Right by 16, followed by Arithmetic Shift Left by 16.\n+\/\/ This idiom is used by the compiler the i2s bytecode.\n+instruct i2s(rRegI dst, rRegI src, immI_16 sixteen)\n+%{\n+  match(Set dst (RShiftI (LShiftI src sixteen) sixteen));\n+\n+  format %{ \"movswl  $dst, $src\\t# i2s\" %}\n+  ins_encode %{\n+    __ movswl($dst$$Register, $src$$Register);\n+  %}\n+  ins_pipe(ialu_reg_reg);\n+%}\n+\n+\/\/ ROL\/ROR instructions\n+\n+\/\/ Rotate left by constant.\n+instruct rolI_immI8_legacy(rRegI dst, immI8 shift, rFlagsReg cr)\n+%{\n+  predicate(!VM_Version::supports_bmi2() && n->bottom_type()->basic_type() == T_INT);\n+  match(Set dst (RotateLeft dst shift));\n+  effect(KILL cr);\n+  format %{ \"roll    $dst, $shift\" %}\n+  ins_encode %{\n+    __ roll($dst$$Register, $shift$$constant);\n+  %}\n+  ins_pipe(ialu_reg);\n+%}\n+\n+instruct rolI_immI8(rRegI dst, rRegI src, immI8 shift)\n+%{\n+  predicate(!UseAPX && VM_Version::supports_bmi2() && n->bottom_type()->basic_type() == T_INT);\n+  match(Set dst (RotateLeft src shift));\n+  format %{ \"rolxl   $dst, $src, $shift\" %}\n+  ins_encode %{\n+    int shift = 32 - ($shift$$constant & 31);\n+    __ rorxl($dst$$Register, $src$$Register, shift);\n+  %}\n+  ins_pipe(ialu_reg_reg);\n+%}\n+\n+instruct rolI_mem_immI8(rRegI dst, memory src, immI8 shift)\n+%{\n+  predicate(VM_Version::supports_bmi2() && n->bottom_type()->basic_type() == T_INT);\n+  match(Set dst (RotateLeft (LoadI src) shift));\n+  ins_cost(175);\n+  format %{ \"rolxl   $dst, $src, $shift\" %}\n+  ins_encode %{\n+    int shift = 32 - ($shift$$constant & 31);\n+    __ rorxl($dst$$Register, $src$$Address, shift);\n+  %}\n+  ins_pipe(ialu_reg_mem);\n+%}\n+\n+\/\/ Rotate Left by variable\n+instruct rolI_rReg_Var(rRegI dst, rcx_RegI shift, rFlagsReg cr)\n+%{\n+  predicate(!UseAPX && n->bottom_type()->basic_type() == T_INT);\n+  match(Set dst (RotateLeft dst shift));\n+  effect(KILL cr);\n+  format %{ \"roll    $dst, $shift\" %}\n+  ins_encode %{\n+    __ roll($dst$$Register);\n+  %}\n+  ins_pipe(ialu_reg_reg);\n+%}\n+\n+\/\/ Rotate Left by variable\n+instruct rolI_rReg_Var_ndd(rRegI dst, rRegI src, rcx_RegI shift, rFlagsReg cr)\n+%{\n+  predicate(UseAPX && n->bottom_type()->basic_type() == T_INT);\n+  match(Set dst (RotateLeft src shift));\n+  effect(KILL cr);\n+\n+  format %{ \"eroll    $dst, $src, $shift\\t# rotate left (int ndd)\" %}\n+  ins_encode %{\n+    __ eroll($dst$$Register, $src$$Register, false);\n+  %}\n+  ins_pipe(ialu_reg_reg);\n+%}\n+\n+\/\/ Rotate Right by constant.\n+instruct rorI_immI8_legacy(rRegI dst, immI8 shift, rFlagsReg cr)\n+%{\n+  predicate(!VM_Version::supports_bmi2() && n->bottom_type()->basic_type() == T_INT);\n+  match(Set dst (RotateRight dst shift));\n+  effect(KILL cr);\n+  format %{ \"rorl    $dst, $shift\" %}\n+  ins_encode %{\n+    __ rorl($dst$$Register, $shift$$constant);\n+  %}\n+  ins_pipe(ialu_reg);\n+%}\n+\n+\/\/ Rotate Right by constant.\n+instruct rorI_immI8(rRegI dst, rRegI src, immI8 shift)\n+%{\n+  predicate(!UseAPX && VM_Version::supports_bmi2() && n->bottom_type()->basic_type() == T_INT);\n+  match(Set dst (RotateRight src shift));\n+  format %{ \"rorxl   $dst, $src, $shift\" %}\n+  ins_encode %{\n+    __ rorxl($dst$$Register, $src$$Register, $shift$$constant);\n+  %}\n+  ins_pipe(ialu_reg_reg);\n+%}\n+\n+instruct rorI_mem_immI8(rRegI dst, memory src, immI8 shift)\n+%{\n+  predicate(VM_Version::supports_bmi2() && n->bottom_type()->basic_type() == T_INT);\n+  match(Set dst (RotateRight (LoadI src) shift));\n+  ins_cost(175);\n+  format %{ \"rorxl   $dst, $src, $shift\" %}\n+  ins_encode %{\n+    __ rorxl($dst$$Register, $src$$Address, $shift$$constant);\n+  %}\n+  ins_pipe(ialu_reg_mem);\n+%}\n+\n+\/\/ Rotate Right by variable\n+instruct rorI_rReg_Var(rRegI dst, rcx_RegI shift, rFlagsReg cr)\n+%{\n+  predicate(!UseAPX && n->bottom_type()->basic_type() == T_INT);\n+  match(Set dst (RotateRight dst shift));\n+  effect(KILL cr);\n+  format %{ \"rorl    $dst, $shift\" %}\n+  ins_encode %{\n+    __ rorl($dst$$Register);\n+  %}\n+  ins_pipe(ialu_reg_reg);\n+%}\n+\n+\/\/ Rotate Right by variable\n+instruct rorI_rReg_Var_ndd(rRegI dst, rRegI src, rcx_RegI shift, rFlagsReg cr)\n+%{\n+  predicate(UseAPX && n->bottom_type()->basic_type() == T_INT);\n+  match(Set dst (RotateRight src shift));\n+  effect(KILL cr);\n+\n+  format %{ \"erorl    $dst, $src, $shift\\t# rotate right(int ndd)\" %}\n+  ins_encode %{\n+    __ erorl($dst$$Register, $src$$Register, false);\n+  %}\n+  ins_pipe(ialu_reg_reg);\n+%}\n+\n+\/\/ Rotate Left by constant.\n+instruct rolL_immI8_legacy(rRegL dst, immI8 shift, rFlagsReg cr)\n+%{\n+  predicate(!VM_Version::supports_bmi2() && n->bottom_type()->basic_type() == T_LONG);\n+  match(Set dst (RotateLeft dst shift));\n+  effect(KILL cr);\n+  format %{ \"rolq    $dst, $shift\" %}\n+  ins_encode %{\n+    __ rolq($dst$$Register, $shift$$constant);\n+  %}\n+  ins_pipe(ialu_reg);\n+%}\n+\n+instruct rolL_immI8(rRegL dst, rRegL src, immI8 shift)\n+%{\n+  predicate(!UseAPX && VM_Version::supports_bmi2() && n->bottom_type()->basic_type() == T_LONG);\n+  match(Set dst (RotateLeft src shift));\n+  format %{ \"rolxq   $dst, $src, $shift\" %}\n+  ins_encode %{\n+    int shift = 64 - ($shift$$constant & 63);\n+    __ rorxq($dst$$Register, $src$$Register, shift);\n+  %}\n+  ins_pipe(ialu_reg_reg);\n+%}\n+\n+instruct rolL_mem_immI8(rRegL dst, memory src, immI8 shift)\n+%{\n+  predicate(VM_Version::supports_bmi2() && n->bottom_type()->basic_type() == T_LONG);\n+  match(Set dst (RotateLeft (LoadL src) shift));\n+  ins_cost(175);\n+  format %{ \"rolxq   $dst, $src, $shift\" %}\n+  ins_encode %{\n+    int shift = 64 - ($shift$$constant & 63);\n+    __ rorxq($dst$$Register, $src$$Address, shift);\n+  %}\n+  ins_pipe(ialu_reg_mem);\n+%}\n+\n+\/\/ Rotate Left by variable\n+instruct rolL_rReg_Var(rRegL dst, rcx_RegI shift, rFlagsReg cr)\n+%{\n+  predicate(!UseAPX && n->bottom_type()->basic_type() == T_LONG);\n+  match(Set dst (RotateLeft dst shift));\n+  effect(KILL cr);\n+  format %{ \"rolq    $dst, $shift\" %}\n+  ins_encode %{\n+    __ rolq($dst$$Register);\n+  %}\n+  ins_pipe(ialu_reg_reg);\n+%}\n+\n+\/\/ Rotate Left by variable\n+instruct rolL_rReg_Var_ndd(rRegL dst, rRegL src, rcx_RegI shift, rFlagsReg cr)\n+%{\n+  predicate(UseAPX && n->bottom_type()->basic_type() == T_LONG);\n+  match(Set dst (RotateLeft src shift));\n+  effect(KILL cr);\n+\n+  format %{ \"erolq    $dst, $src, $shift\\t# rotate left(long ndd)\" %}\n+  ins_encode %{\n+    __ erolq($dst$$Register, $src$$Register, false);\n+  %}\n+  ins_pipe(ialu_reg_reg);\n+%}\n+\n+\/\/ Rotate Right by constant.\n+instruct rorL_immI8_legacy(rRegL dst, immI8 shift, rFlagsReg cr)\n+%{\n+  predicate(!VM_Version::supports_bmi2() && n->bottom_type()->basic_type() == T_LONG);\n+  match(Set dst (RotateRight dst shift));\n+  effect(KILL cr);\n+  format %{ \"rorq    $dst, $shift\" %}\n+  ins_encode %{\n+    __ rorq($dst$$Register, $shift$$constant);\n+  %}\n+  ins_pipe(ialu_reg);\n+%}\n+\n+\/\/ Rotate Right by constant\n+instruct rorL_immI8(rRegL dst, rRegL src, immI8 shift)\n+%{\n+  predicate(VM_Version::supports_bmi2() && n->bottom_type()->basic_type() == T_LONG);\n+  match(Set dst (RotateRight src shift));\n+  format %{ \"rorxq   $dst, $src, $shift\" %}\n+  ins_encode %{\n+    __ rorxq($dst$$Register, $src$$Register, $shift$$constant);\n+  %}\n+  ins_pipe(ialu_reg_reg);\n+%}\n+\n+instruct rorL_mem_immI8(rRegL dst, memory src, immI8 shift)\n+%{\n+  predicate(VM_Version::supports_bmi2() && n->bottom_type()->basic_type() == T_LONG);\n+  match(Set dst (RotateRight (LoadL src) shift));\n+  ins_cost(175);\n+  format %{ \"rorxq   $dst, $src, $shift\" %}\n+  ins_encode %{\n+    __ rorxq($dst$$Register, $src$$Address, $shift$$constant);\n+  %}\n+  ins_pipe(ialu_reg_mem);\n+%}\n+\n+\/\/ Rotate Right by variable\n+instruct rorL_rReg_Var(rRegL dst, rcx_RegI shift, rFlagsReg cr)\n+%{\n+  predicate(!UseAPX && n->bottom_type()->basic_type() == T_LONG);\n+  match(Set dst (RotateRight dst shift));\n+  effect(KILL cr);\n+  format %{ \"rorq    $dst, $shift\" %}\n+  ins_encode %{\n+    __ rorq($dst$$Register);\n+  %}\n+  ins_pipe(ialu_reg_reg);\n+%}\n+\n+\/\/ Rotate Right by variable\n+instruct rorL_rReg_Var_ndd(rRegL dst, rRegL src, rcx_RegI shift, rFlagsReg cr)\n+%{\n+  predicate(UseAPX && n->bottom_type()->basic_type() == T_LONG);\n+  match(Set dst (RotateRight src shift));\n+  effect(KILL cr);\n+\n+  format %{ \"erorq    $dst, $src, $shift\\t# rotate right(long ndd)\" %}\n+  ins_encode %{\n+    __ erorq($dst$$Register, $src$$Register, false);\n+  %}\n+  ins_pipe(ialu_reg_reg);\n+%}\n+\n+\/\/----------------------------- CompressBits\/ExpandBits ------------------------\n+\n+instruct compressBitsL_reg(rRegL dst, rRegL src, rRegL mask) %{\n+  predicate(n->bottom_type()->isa_long());\n+  match(Set dst (CompressBits src mask));\n+  format %{ \"pextq  $dst, $src, $mask\\t! parallel bit extract\" %}\n+  ins_encode %{\n+    __ pextq($dst$$Register, $src$$Register, $mask$$Register);\n+  %}\n+  ins_pipe( pipe_slow );\n+%}\n+\n+instruct expandBitsL_reg(rRegL dst, rRegL src, rRegL mask) %{\n+  predicate(n->bottom_type()->isa_long());\n+  match(Set dst (ExpandBits src mask));\n+  format %{ \"pdepq  $dst, $src, $mask\\t! parallel bit deposit\" %}\n+  ins_encode %{\n+    __ pdepq($dst$$Register, $src$$Register, $mask$$Register);\n+  %}\n+  ins_pipe( pipe_slow );\n+%}\n+\n+instruct compressBitsL_mem(rRegL dst, rRegL src, memory mask) %{\n+  predicate(n->bottom_type()->isa_long());\n+  match(Set dst (CompressBits src (LoadL mask)));\n+  format %{ \"pextq  $dst, $src, $mask\\t! parallel bit extract\" %}\n+  ins_encode %{\n+    __ pextq($dst$$Register, $src$$Register, $mask$$Address);\n+  %}\n+  ins_pipe( pipe_slow );\n+%}\n+\n+instruct expandBitsL_mem(rRegL dst, rRegL src, memory mask) %{\n+  predicate(n->bottom_type()->isa_long());\n+  match(Set dst (ExpandBits src (LoadL mask)));\n+  format %{ \"pdepq  $dst, $src, $mask\\t! parallel bit deposit\" %}\n+  ins_encode %{\n+    __ pdepq($dst$$Register, $src$$Register, $mask$$Address);\n+  %}\n+  ins_pipe( pipe_slow );\n+%}\n+\n+\n+\/\/ Logical Instructions\n+\n+\/\/ Integer Logical Instructions\n+\n+\/\/ And Instructions\n+\/\/ And Register with Register\n+instruct andI_rReg(rRegI dst, rRegI src, rFlagsReg cr)\n+%{\n+  predicate(!UseAPX);\n+  match(Set dst (AndI dst src));\n+  effect(KILL cr);\n+  flag(PD::Flag_sets_sign_flag, PD::Flag_sets_zero_flag, PD::Flag_sets_parity_flag, PD::Flag_clears_overflow_flag, PD::Flag_clears_carry_flag);\n+\n+  format %{ \"andl    $dst, $src\\t# int\" %}\n+  ins_encode %{\n+    __ andl($dst$$Register, $src$$Register);\n+  %}\n+  ins_pipe(ialu_reg_reg);\n+%}\n+\n+\/\/ And Register with Register using New Data Destination (NDD)\n+instruct andI_rReg_ndd(rRegI dst, rRegI src1, rRegI src2, rFlagsReg cr)\n+%{\n+  predicate(UseAPX);\n+  match(Set dst (AndI src1 src2));\n+  effect(KILL cr);\n+  flag(PD::Flag_sets_sign_flag, PD::Flag_sets_zero_flag, PD::Flag_sets_parity_flag, PD::Flag_clears_overflow_flag, PD::Flag_clears_carry_flag);\n+\n+  format %{ \"eandl     $dst, $src1, $src2\\t# int ndd\" %}\n+  ins_encode %{\n+    __ eandl($dst$$Register, $src1$$Register, $src2$$Register, false);\n+\n+  %}\n+  ins_pipe(ialu_reg_reg);\n+%}\n+\n+\/\/ And Register with Immediate 255\n+instruct andI_rReg_imm255(rRegI dst, rRegI src, immI_255 mask)\n+%{\n+  match(Set dst (AndI src mask));\n+\n+  format %{ \"movzbl  $dst, $src\\t# int & 0xFF\" %}\n+  ins_encode %{\n+    __ movzbl($dst$$Register, $src$$Register);\n+  %}\n+  ins_pipe(ialu_reg);\n+%}\n+\n+\/\/ And Register with Immediate 255 and promote to long\n+instruct andI2L_rReg_imm255(rRegL dst, rRegI src, immI_255 mask)\n+%{\n+  match(Set dst (ConvI2L (AndI src mask)));\n+\n+  format %{ \"movzbl  $dst, $src\\t# int & 0xFF -> long\" %}\n+  ins_encode %{\n+    __ movzbl($dst$$Register, $src$$Register);\n+  %}\n+  ins_pipe(ialu_reg);\n+%}\n+\n+\/\/ And Register with Immediate 65535\n+instruct andI_rReg_imm65535(rRegI dst, rRegI src, immI_65535 mask)\n+%{\n+  match(Set dst (AndI src mask));\n+\n+  format %{ \"movzwl  $dst, $src\\t# int & 0xFFFF\" %}\n+  ins_encode %{\n+    __ movzwl($dst$$Register, $src$$Register);\n+  %}\n+  ins_pipe(ialu_reg);\n+%}\n+\n+\/\/ And Register with Immediate 65535 and promote to long\n+instruct andI2L_rReg_imm65535(rRegL dst, rRegI src, immI_65535 mask)\n+%{\n+  match(Set dst (ConvI2L (AndI src mask)));\n+\n+  format %{ \"movzwl  $dst, $src\\t# int & 0xFFFF -> long\" %}\n+  ins_encode %{\n+    __ movzwl($dst$$Register, $src$$Register);\n+  %}\n+  ins_pipe(ialu_reg);\n+%}\n+\n+\/\/ Can skip int2long conversions after AND with small bitmask\n+instruct convI2LAndI_reg_immIbitmask(rRegL dst, rRegI src,  immI_Pow2M1 mask, rRegI tmp, rFlagsReg cr)\n+%{\n+  predicate(VM_Version::supports_bmi2());\n+  ins_cost(125);\n+  effect(TEMP tmp, KILL cr);\n+  match(Set dst (ConvI2L (AndI src mask)));\n+  format %{ \"bzhiq $dst, $src, $mask \\t# using $tmp as TEMP, int &  immI_Pow2M1 -> long\" %}\n+  ins_encode %{\n+    __ movl($tmp$$Register, exact_log2($mask$$constant + 1));\n+    __ bzhiq($dst$$Register, $src$$Register, $tmp$$Register);\n+  %}\n+  ins_pipe(ialu_reg_reg);\n+%}\n+\n+\/\/ And Register with Immediate\n+instruct andI_rReg_imm(rRegI dst, immI src, rFlagsReg cr)\n+%{\n+  predicate(!UseAPX);\n+  match(Set dst (AndI dst src));\n+  effect(KILL cr);\n+  flag(PD::Flag_sets_sign_flag, PD::Flag_sets_zero_flag, PD::Flag_sets_parity_flag, PD::Flag_clears_overflow_flag, PD::Flag_clears_carry_flag);\n+\n+  format %{ \"andl    $dst, $src\\t# int\" %}\n+  ins_encode %{\n+    __ andl($dst$$Register, $src$$constant);\n+  %}\n+  ins_pipe(ialu_reg);\n+%}\n+\n+instruct andI_rReg_rReg_imm_ndd(rRegI dst, rRegI src1, immI src2, rFlagsReg cr)\n+%{\n+  predicate(UseAPX);\n+  match(Set dst (AndI src1 src2));\n+  effect(KILL cr);\n+  flag(PD::Flag_sets_sign_flag, PD::Flag_sets_zero_flag, PD::Flag_sets_parity_flag, PD::Flag_clears_overflow_flag, PD::Flag_clears_carry_flag);\n+\n+  format %{ \"eandl    $dst, $src1, $src2\\t# int ndd\" %}\n+  ins_encode %{\n+    __ eandl($dst$$Register, $src1$$Register, $src2$$constant, false);\n+  %}\n+  ins_pipe(ialu_reg);\n+%}\n+\n+instruct andI_rReg_mem_imm_ndd(rRegI dst, memory src1, immI src2, rFlagsReg cr)\n+%{\n+  predicate(UseAPX);\n+  match(Set dst (AndI (LoadI src1) src2));\n+  effect(KILL cr);\n+  flag(PD::Flag_sets_sign_flag, PD::Flag_sets_zero_flag, PD::Flag_sets_parity_flag, PD::Flag_clears_overflow_flag, PD::Flag_clears_carry_flag);\n+\n+  format %{ \"eandl    $dst, $src1, $src2\\t# int ndd\" %}\n+  ins_encode %{\n+    __ eandl($dst$$Register, $src1$$Address, $src2$$constant, false);\n+  %}\n+  ins_pipe(ialu_reg);\n+%}\n+\n+\/\/ And Register with Memory\n+instruct andI_rReg_mem(rRegI dst, memory src, rFlagsReg cr)\n+%{\n+  predicate(!UseAPX);\n+  match(Set dst (AndI dst (LoadI src)));\n+  effect(KILL cr);\n+  flag(PD::Flag_sets_sign_flag, PD::Flag_sets_zero_flag, PD::Flag_sets_parity_flag, PD::Flag_clears_overflow_flag, PD::Flag_clears_carry_flag);\n+\n+  ins_cost(150);\n+  format %{ \"andl    $dst, $src\\t# int\" %}\n+  ins_encode %{\n+    __ andl($dst$$Register, $src$$Address);\n+  %}\n+  ins_pipe(ialu_reg_mem);\n+%}\n+\n+instruct andI_rReg_rReg_mem_ndd(rRegI dst, rRegI src1, memory src2, rFlagsReg cr)\n+%{\n+  predicate(UseAPX);\n+  match(Set dst (AndI src1 (LoadI src2)));\n+  effect(KILL cr);\n+  flag(PD::Flag_sets_sign_flag, PD::Flag_sets_zero_flag, PD::Flag_sets_parity_flag, PD::Flag_clears_overflow_flag, PD::Flag_clears_carry_flag);\n+\n+  ins_cost(150);\n+  format %{ \"eandl    $dst, $src1, $src2\\t# int ndd\" %}\n+  ins_encode %{\n+    __ eandl($dst$$Register, $src1$$Register, $src2$$Address, false);\n+  %}\n+  ins_pipe(ialu_reg_mem);\n+%}\n+\n+\/\/ And Memory with Register\n+instruct andB_mem_rReg(memory dst, rRegI src, rFlagsReg cr)\n+%{\n+  match(Set dst (StoreB dst (AndI (LoadB dst) src)));\n+  effect(KILL cr);\n+  flag(PD::Flag_sets_sign_flag, PD::Flag_sets_zero_flag, PD::Flag_sets_parity_flag, PD::Flag_clears_overflow_flag, PD::Flag_clears_carry_flag);\n+\n+  ins_cost(150);\n+  format %{ \"andb    $dst, $src\\t# byte\" %}\n+  ins_encode %{\n+    __ andb($dst$$Address, $src$$Register);\n+  %}\n+  ins_pipe(ialu_mem_reg);\n+%}\n+\n+instruct andI_mem_rReg(memory dst, rRegI src, rFlagsReg cr)\n+%{\n+  match(Set dst (StoreI dst (AndI (LoadI dst) src)));\n+  effect(KILL cr);\n+  flag(PD::Flag_sets_sign_flag, PD::Flag_sets_zero_flag, PD::Flag_sets_parity_flag, PD::Flag_clears_overflow_flag, PD::Flag_clears_carry_flag);\n+\n+  ins_cost(150);\n+  format %{ \"andl    $dst, $src\\t# int\" %}\n+  ins_encode %{\n+    __ andl($dst$$Address, $src$$Register);\n+  %}\n+  ins_pipe(ialu_mem_reg);\n+%}\n+\n+\/\/ And Memory with Immediate\n+instruct andI_mem_imm(memory dst, immI src, rFlagsReg cr)\n+%{\n+  match(Set dst (StoreI dst (AndI (LoadI dst) src)));\n+  effect(KILL cr);\n+  flag(PD::Flag_sets_sign_flag, PD::Flag_sets_zero_flag, PD::Flag_sets_parity_flag, PD::Flag_clears_overflow_flag, PD::Flag_clears_carry_flag);\n+\n+  ins_cost(125);\n+  format %{ \"andl    $dst, $src\\t# int\" %}\n+  ins_encode %{\n+    __ andl($dst$$Address, $src$$constant);\n+  %}\n+  ins_pipe(ialu_mem_imm);\n+%}\n+\n+\/\/ BMI1 instructions\n+instruct andnI_rReg_rReg_mem(rRegI dst, rRegI src1, memory src2, immI_M1 minus_1, rFlagsReg cr) %{\n+  match(Set dst (AndI (XorI src1 minus_1) (LoadI src2)));\n+  predicate(UseBMI1Instructions);\n+  effect(KILL cr);\n+  flag(PD::Flag_sets_sign_flag, PD::Flag_sets_zero_flag, PD::Flag_clears_overflow_flag, PD::Flag_clears_carry_flag);\n+\n+  ins_cost(125);\n+  format %{ \"andnl  $dst, $src1, $src2\" %}\n+\n+  ins_encode %{\n+    __ andnl($dst$$Register, $src1$$Register, $src2$$Address);\n+  %}\n+  ins_pipe(ialu_reg_mem);\n+%}\n+\n+instruct andnI_rReg_rReg_rReg(rRegI dst, rRegI src1, rRegI src2, immI_M1 minus_1, rFlagsReg cr) %{\n+  match(Set dst (AndI (XorI src1 minus_1) src2));\n+  predicate(UseBMI1Instructions);\n+  effect(KILL cr);\n+  flag(PD::Flag_sets_sign_flag, PD::Flag_sets_zero_flag, PD::Flag_clears_overflow_flag, PD::Flag_clears_carry_flag);\n+\n+  format %{ \"andnl  $dst, $src1, $src2\" %}\n+\n+  ins_encode %{\n+    __ andnl($dst$$Register, $src1$$Register, $src2$$Register);\n+  %}\n+  ins_pipe(ialu_reg);\n+%}\n+\n+instruct blsiI_rReg_rReg(rRegI dst, rRegI src, immI_0 imm_zero, rFlagsReg cr) %{\n+  match(Set dst (AndI (SubI imm_zero src) src));\n+  predicate(UseBMI1Instructions);\n+  effect(KILL cr);\n+  flag(PD::Flag_sets_sign_flag, PD::Flag_sets_zero_flag, PD::Flag_clears_overflow_flag);\n+\n+  format %{ \"blsil  $dst, $src\" %}\n+\n+  ins_encode %{\n+    __ blsil($dst$$Register, $src$$Register);\n+  %}\n+  ins_pipe(ialu_reg);\n+%}\n+\n+instruct blsiI_rReg_mem(rRegI dst, memory src, immI_0 imm_zero, rFlagsReg cr) %{\n+  match(Set dst (AndI (SubI imm_zero (LoadI src) ) (LoadI src) ));\n+  predicate(UseBMI1Instructions);\n+  effect(KILL cr);\n+  flag(PD::Flag_sets_sign_flag, PD::Flag_sets_zero_flag, PD::Flag_clears_overflow_flag);\n+\n+  ins_cost(125);\n+  format %{ \"blsil  $dst, $src\" %}\n+\n+  ins_encode %{\n+    __ blsil($dst$$Register, $src$$Address);\n+  %}\n+  ins_pipe(ialu_reg_mem);\n+%}\n+\n+instruct blsmskI_rReg_mem(rRegI dst, memory src, immI_M1 minus_1, rFlagsReg cr)\n+%{\n+  match(Set dst (XorI (AddI (LoadI src) minus_1) (LoadI src) ) );\n+  predicate(UseBMI1Instructions);\n+  effect(KILL cr);\n+  flag(PD::Flag_sets_sign_flag, PD::Flag_clears_zero_flag, PD::Flag_clears_overflow_flag);\n+\n+  ins_cost(125);\n+  format %{ \"blsmskl $dst, $src\" %}\n+\n+  ins_encode %{\n+    __ blsmskl($dst$$Register, $src$$Address);\n+  %}\n+  ins_pipe(ialu_reg_mem);\n+%}\n+\n+instruct blsmskI_rReg_rReg(rRegI dst, rRegI src, immI_M1 minus_1, rFlagsReg cr)\n+%{\n+  match(Set dst (XorI (AddI src minus_1) src));\n+  predicate(UseBMI1Instructions);\n+  effect(KILL cr);\n+  flag(PD::Flag_sets_sign_flag, PD::Flag_clears_zero_flag, PD::Flag_clears_overflow_flag);\n+\n+  format %{ \"blsmskl $dst, $src\" %}\n+\n+  ins_encode %{\n+    __ blsmskl($dst$$Register, $src$$Register);\n+  %}\n+\n+  ins_pipe(ialu_reg);\n+%}\n+\n+instruct blsrI_rReg_rReg(rRegI dst, rRegI src, immI_M1 minus_1, rFlagsReg cr)\n+%{\n+  match(Set dst (AndI (AddI src minus_1) src) );\n+  predicate(UseBMI1Instructions);\n+  effect(KILL cr);\n+  flag(PD::Flag_sets_sign_flag, PD::Flag_sets_zero_flag, PD::Flag_clears_overflow_flag);\n+\n+  format %{ \"blsrl  $dst, $src\" %}\n+\n+  ins_encode %{\n+    __ blsrl($dst$$Register, $src$$Register);\n+  %}\n+\n+  ins_pipe(ialu_reg_mem);\n+%}\n+\n+instruct blsrI_rReg_mem(rRegI dst, memory src, immI_M1 minus_1, rFlagsReg cr)\n+%{\n+  match(Set dst (AndI (AddI (LoadI src) minus_1) (LoadI src) ) );\n+  predicate(UseBMI1Instructions);\n+  effect(KILL cr);\n+  flag(PD::Flag_sets_sign_flag, PD::Flag_sets_zero_flag, PD::Flag_clears_overflow_flag);\n+\n+  ins_cost(125);\n+  format %{ \"blsrl  $dst, $src\" %}\n+\n+  ins_encode %{\n+    __ blsrl($dst$$Register, $src$$Address);\n+  %}\n+\n+  ins_pipe(ialu_reg);\n+%}\n+\n+\/\/ Or Instructions\n+\/\/ Or Register with Register\n+instruct orI_rReg(rRegI dst, rRegI src, rFlagsReg cr)\n+%{\n+  predicate(!UseAPX);\n+  match(Set dst (OrI dst src));\n+  effect(KILL cr);\n+  flag(PD::Flag_sets_sign_flag, PD::Flag_sets_zero_flag, PD::Flag_sets_parity_flag, PD::Flag_clears_overflow_flag, PD::Flag_clears_carry_flag);\n+\n+  format %{ \"orl     $dst, $src\\t# int\" %}\n+  ins_encode %{\n+    __ orl($dst$$Register, $src$$Register);\n+  %}\n+  ins_pipe(ialu_reg_reg);\n+%}\n+\n+\/\/ Or Register with Register using New Data Destination (NDD)\n+instruct orI_rReg_ndd(rRegI dst, rRegI src1, rRegI src2, rFlagsReg cr)\n+%{\n+  predicate(UseAPX);\n+  match(Set dst (OrI src1 src2));\n+  effect(KILL cr);\n+  flag(PD::Flag_sets_sign_flag, PD::Flag_sets_zero_flag, PD::Flag_sets_parity_flag, PD::Flag_clears_overflow_flag, PD::Flag_clears_carry_flag);\n+\n+  format %{ \"eorl     $dst, $src1, $src2\\t# int ndd\" %}\n+  ins_encode %{\n+    __ eorl($dst$$Register, $src1$$Register, $src2$$Register, false);\n+  %}\n+  ins_pipe(ialu_reg_reg);\n+%}\n+\n+\/\/ Or Register with Immediate\n+instruct orI_rReg_imm(rRegI dst, immI src, rFlagsReg cr)\n+%{\n+  predicate(!UseAPX);\n+  match(Set dst (OrI dst src));\n+  effect(KILL cr);\n+  flag(PD::Flag_sets_sign_flag, PD::Flag_sets_zero_flag, PD::Flag_sets_parity_flag, PD::Flag_clears_overflow_flag, PD::Flag_clears_carry_flag);\n+\n+  format %{ \"orl     $dst, $src\\t# int\" %}\n+  ins_encode %{\n+    __ orl($dst$$Register, $src$$constant);\n+  %}\n+  ins_pipe(ialu_reg);\n+%}\n+\n+instruct orI_rReg_rReg_imm_ndd(rRegI dst, rRegI src1, immI src2, rFlagsReg cr)\n+%{\n+  predicate(UseAPX);\n+  match(Set dst (OrI src1 src2));\n+  effect(KILL cr);\n+  flag(PD::Flag_sets_sign_flag, PD::Flag_sets_zero_flag, PD::Flag_sets_parity_flag, PD::Flag_clears_overflow_flag, PD::Flag_clears_carry_flag);\n+\n+  format %{ \"eorl     $dst, $src1, $src2\\t# int ndd\" %}\n+  ins_encode %{\n+    __ eorl($dst$$Register, $src1$$Register, $src2$$constant, false);\n+  %}\n+  ins_pipe(ialu_reg);\n+%}\n+\n+instruct orI_rReg_imm_rReg_ndd(rRegI dst, immI src1, rRegI src2, rFlagsReg cr)\n+%{\n+  predicate(UseAPX);\n+  match(Set dst (OrI src1 src2));\n+  effect(KILL cr);\n+  flag(PD::Flag_sets_sign_flag, PD::Flag_sets_zero_flag, PD::Flag_sets_parity_flag, PD::Flag_clears_overflow_flag, PD::Flag_clears_carry_flag);\n+\n+  format %{ \"eorl     $dst, $src2, $src1\\t# int ndd\" %}\n+  ins_encode %{\n+    __ eorl($dst$$Register, $src2$$Register, $src1$$constant, false);\n+  %}\n+  ins_pipe(ialu_reg);\n+%}\n+\n+instruct orI_rReg_mem_imm_ndd(rRegI dst, memory src1, immI src2, rFlagsReg cr)\n+%{\n+  predicate(UseAPX);\n+  match(Set dst (OrI (LoadI src1) src2));\n+  effect(KILL cr);\n+  flag(PD::Flag_sets_sign_flag, PD::Flag_sets_zero_flag, PD::Flag_sets_parity_flag, PD::Flag_clears_overflow_flag, PD::Flag_clears_carry_flag);\n+\n+  format %{ \"eorl     $dst, $src1, $src2\\t# int ndd\" %}\n+  ins_encode %{\n+    __ eorl($dst$$Register, $src1$$Address, $src2$$constant, false);\n+  %}\n+  ins_pipe(ialu_reg);\n+%}\n+\n+\/\/ Or Register with Memory\n+instruct orI_rReg_mem(rRegI dst, memory src, rFlagsReg cr)\n+%{\n+  predicate(!UseAPX);\n+  match(Set dst (OrI dst (LoadI src)));\n+  effect(KILL cr);\n+  flag(PD::Flag_sets_sign_flag, PD::Flag_sets_zero_flag, PD::Flag_sets_parity_flag, PD::Flag_clears_overflow_flag, PD::Flag_clears_carry_flag);\n+\n+  ins_cost(150);\n+  format %{ \"orl     $dst, $src\\t# int\" %}\n+  ins_encode %{\n+    __ orl($dst$$Register, $src$$Address);\n+  %}\n+  ins_pipe(ialu_reg_mem);\n+%}\n+\n+instruct orI_rReg_rReg_mem_ndd(rRegI dst, rRegI src1, memory src2, rFlagsReg cr)\n+%{\n+  predicate(UseAPX);\n+  match(Set dst (OrI src1 (LoadI src2)));\n+  effect(KILL cr);\n+  flag(PD::Flag_sets_sign_flag, PD::Flag_sets_zero_flag, PD::Flag_sets_parity_flag, PD::Flag_clears_overflow_flag, PD::Flag_clears_carry_flag);\n+\n+  ins_cost(150);\n+  format %{ \"eorl     $dst, $src1, $src2\\t# int ndd\" %}\n+  ins_encode %{\n+    __ eorl($dst$$Register, $src1$$Register, $src2$$Address, false);\n+  %}\n+  ins_pipe(ialu_reg_mem);\n+%}\n+\n+\/\/ Or Memory with Register\n+instruct orB_mem_rReg(memory dst, rRegI src, rFlagsReg cr)\n+%{\n+  match(Set dst (StoreB dst (OrI (LoadB dst) src)));\n+  effect(KILL cr);\n+  flag(PD::Flag_sets_sign_flag, PD::Flag_sets_zero_flag, PD::Flag_sets_parity_flag, PD::Flag_clears_overflow_flag, PD::Flag_clears_carry_flag);\n+\n+  ins_cost(150);\n+  format %{ \"orb    $dst, $src\\t# byte\" %}\n+  ins_encode %{\n+    __ orb($dst$$Address, $src$$Register);\n+  %}\n+  ins_pipe(ialu_mem_reg);\n+%}\n+\n+instruct orI_mem_rReg(memory dst, rRegI src, rFlagsReg cr)\n+%{\n+  match(Set dst (StoreI dst (OrI (LoadI dst) src)));\n+  effect(KILL cr);\n+  flag(PD::Flag_sets_sign_flag, PD::Flag_sets_zero_flag, PD::Flag_sets_parity_flag, PD::Flag_clears_overflow_flag, PD::Flag_clears_carry_flag);\n+\n+  ins_cost(150);\n+  format %{ \"orl     $dst, $src\\t# int\" %}\n+  ins_encode %{\n+    __ orl($dst$$Address, $src$$Register);\n+  %}\n+  ins_pipe(ialu_mem_reg);\n+%}\n+\n+\/\/ Or Memory with Immediate\n+instruct orI_mem_imm(memory dst, immI src, rFlagsReg cr)\n+%{\n+  match(Set dst (StoreI dst (OrI (LoadI dst) src)));\n+  effect(KILL cr);\n+  flag(PD::Flag_sets_sign_flag, PD::Flag_sets_zero_flag, PD::Flag_sets_parity_flag, PD::Flag_clears_overflow_flag, PD::Flag_clears_carry_flag);\n+\n+  ins_cost(125);\n+  format %{ \"orl     $dst, $src\\t# int\" %}\n+  ins_encode %{\n+    __ orl($dst$$Address, $src$$constant);\n+  %}\n+  ins_pipe(ialu_mem_imm);\n+%}\n+\n+\/\/ Xor Instructions\n+\/\/ Xor Register with Register\n+instruct xorI_rReg(rRegI dst, rRegI src, rFlagsReg cr)\n+%{\n+  predicate(!UseAPX);\n+  match(Set dst (XorI dst src));\n+  effect(KILL cr);\n+  flag(PD::Flag_sets_sign_flag, PD::Flag_sets_zero_flag, PD::Flag_sets_parity_flag, PD::Flag_clears_overflow_flag, PD::Flag_clears_carry_flag);\n+\n+  format %{ \"xorl    $dst, $src\\t# int\" %}\n+  ins_encode %{\n+    __ xorl($dst$$Register, $src$$Register);\n+  %}\n+  ins_pipe(ialu_reg_reg);\n+%}\n+\n+\/\/ Xor Register with Register using New Data Destination (NDD)\n+instruct xorI_rReg_ndd(rRegI dst, rRegI src1, rRegI src2, rFlagsReg cr)\n+%{\n+  predicate(UseAPX);\n+  match(Set dst (XorI src1 src2));\n+  effect(KILL cr);\n+  flag(PD::Flag_sets_sign_flag, PD::Flag_sets_zero_flag, PD::Flag_sets_parity_flag, PD::Flag_clears_overflow_flag, PD::Flag_clears_carry_flag);\n+\n+  format %{ \"exorl    $dst, $src1, $src2\\t# int ndd\" %}\n+  ins_encode %{\n+    __ exorl($dst$$Register, $src1$$Register, $src2$$Register, false);\n+  %}\n+  ins_pipe(ialu_reg_reg);\n+%}\n+\n+\/\/ Xor Register with Immediate -1\n+instruct xorI_rReg_im1(rRegI dst, immI_M1 imm)\n+%{\n+  predicate(!UseAPX);\n+  match(Set dst (XorI dst imm));\n+\n+  format %{ \"notl    $dst\" %}\n+  ins_encode %{\n+     __ notl($dst$$Register);\n+  %}\n+  ins_pipe(ialu_reg);\n+%}\n+\n+instruct xorI_rReg_im1_ndd(rRegI dst, rRegI src, immI_M1 imm)\n+%{\n+  match(Set dst (XorI src imm));\n+  predicate(UseAPX);\n+\n+  format %{ \"enotl    $dst, $src\" %}\n+  ins_encode %{\n+     __ enotl($dst$$Register, $src$$Register);\n+  %}\n+  ins_pipe(ialu_reg);\n+%}\n+\n+\/\/ Xor Register with Immediate\n+instruct xorI_rReg_imm(rRegI dst, immI src, rFlagsReg cr)\n+%{\n+  \/\/ Strict predicate check to make selection of xorI_rReg_im1 cost agnostic if immI src is -1.\n+  predicate(!UseAPX && n->in(2)->bottom_type()->is_int()->get_con() != -1);\n+  match(Set dst (XorI dst src));\n+  effect(KILL cr);\n+  flag(PD::Flag_sets_sign_flag, PD::Flag_sets_zero_flag, PD::Flag_sets_parity_flag, PD::Flag_clears_overflow_flag, PD::Flag_clears_carry_flag);\n+\n+  format %{ \"xorl    $dst, $src\\t# int\" %}\n+  ins_encode %{\n+    __ xorl($dst$$Register, $src$$constant);\n+  %}\n+  ins_pipe(ialu_reg);\n+%}\n+\n+instruct xorI_rReg_rReg_imm_ndd(rRegI dst, rRegI src1, immI src2, rFlagsReg cr)\n+%{\n+  \/\/ Strict predicate check to make selection of xorI_rReg_im1_ndd cost agnostic if immI src2 is -1.\n+  predicate(UseAPX && n->in(2)->bottom_type()->is_int()->get_con() != -1);\n+  match(Set dst (XorI src1 src2));\n+  effect(KILL cr);\n+  flag(PD::Flag_sets_sign_flag, PD::Flag_sets_zero_flag, PD::Flag_sets_parity_flag, PD::Flag_clears_overflow_flag, PD::Flag_clears_carry_flag);\n+\n+  format %{ \"exorl    $dst, $src1, $src2\\t# int ndd\" %}\n+  ins_encode %{\n+    __ exorl($dst$$Register, $src1$$Register, $src2$$constant, false);\n+  %}\n+  ins_pipe(ialu_reg);\n+%}\n+\n+\/\/ Xor Memory with Immediate\n+instruct xorI_rReg_mem_imm_ndd(rRegI dst, memory src1, immI src2, rFlagsReg cr)\n+%{\n+  predicate(UseAPX);\n+  match(Set dst (XorI (LoadI src1) src2));\n+  effect(KILL cr);\n+  ins_cost(150);\n+  flag(PD::Flag_sets_sign_flag, PD::Flag_sets_zero_flag, PD::Flag_sets_parity_flag, PD::Flag_clears_overflow_flag, PD::Flag_clears_carry_flag);\n+\n+  format %{ \"exorl    $dst, $src1, $src2\\t# int ndd\" %}\n+  ins_encode %{\n+    __ exorl($dst$$Register, $src1$$Address, $src2$$constant, false);\n+  %}\n+  ins_pipe(ialu_reg);\n+%}\n+\n+\/\/ Xor Register with Memory\n+instruct xorI_rReg_mem(rRegI dst, memory src, rFlagsReg cr)\n+%{\n+  predicate(!UseAPX);\n+  match(Set dst (XorI dst (LoadI src)));\n+  effect(KILL cr);\n+  flag(PD::Flag_sets_sign_flag, PD::Flag_sets_zero_flag, PD::Flag_sets_parity_flag, PD::Flag_clears_overflow_flag, PD::Flag_clears_carry_flag);\n+\n+  ins_cost(150);\n+  format %{ \"xorl    $dst, $src\\t# int\" %}\n+  ins_encode %{\n+    __ xorl($dst$$Register, $src$$Address);\n+  %}\n+  ins_pipe(ialu_reg_mem);\n+%}\n+\n+instruct xorI_rReg_rReg_mem_ndd(rRegI dst, rRegI src1, memory src2, rFlagsReg cr)\n+%{\n+  predicate(UseAPX);\n+  match(Set dst (XorI src1 (LoadI src2)));\n+  effect(KILL cr);\n+  flag(PD::Flag_sets_sign_flag, PD::Flag_sets_zero_flag, PD::Flag_sets_parity_flag, PD::Flag_clears_overflow_flag, PD::Flag_clears_carry_flag);\n+\n+  ins_cost(150);\n+  format %{ \"exorl    $dst, $src1, $src2\\t# int ndd\" %}\n+  ins_encode %{\n+    __ exorl($dst$$Register, $src1$$Register, $src2$$Address, false);\n+  %}\n+  ins_pipe(ialu_reg_mem);\n+%}\n+\n+\/\/ Xor Memory with Register\n+instruct xorB_mem_rReg(memory dst, rRegI src, rFlagsReg cr)\n+%{\n+  match(Set dst (StoreB dst (XorI (LoadB dst) src)));\n+  effect(KILL cr);\n+  flag(PD::Flag_sets_sign_flag, PD::Flag_sets_zero_flag, PD::Flag_sets_parity_flag, PD::Flag_clears_overflow_flag, PD::Flag_clears_carry_flag);\n+\n+  ins_cost(150);\n+  format %{ \"xorb    $dst, $src\\t# byte\" %}\n+  ins_encode %{\n+    __ xorb($dst$$Address, $src$$Register);\n+  %}\n+  ins_pipe(ialu_mem_reg);\n+%}\n+\n+instruct xorI_mem_rReg(memory dst, rRegI src, rFlagsReg cr)\n+%{\n+  match(Set dst (StoreI dst (XorI (LoadI dst) src)));\n+  effect(KILL cr);\n+  flag(PD::Flag_sets_sign_flag, PD::Flag_sets_zero_flag, PD::Flag_sets_parity_flag, PD::Flag_clears_overflow_flag, PD::Flag_clears_carry_flag);\n+\n+  ins_cost(150);\n+  format %{ \"xorl    $dst, $src\\t# int\" %}\n+  ins_encode %{\n+    __ xorl($dst$$Address, $src$$Register);\n+  %}\n+  ins_pipe(ialu_mem_reg);\n+%}\n+\n+\/\/ Xor Memory with Immediate\n+instruct xorI_mem_imm(memory dst, immI src, rFlagsReg cr)\n+%{\n+  match(Set dst (StoreI dst (XorI (LoadI dst) src)));\n+  effect(KILL cr);\n+  flag(PD::Flag_sets_sign_flag, PD::Flag_sets_zero_flag, PD::Flag_sets_parity_flag, PD::Flag_clears_overflow_flag, PD::Flag_clears_carry_flag);\n+\n+  ins_cost(125);\n+  format %{ \"xorl    $dst, $src\\t# int\" %}\n+  ins_encode %{\n+    __ xorl($dst$$Address, $src$$constant);\n+  %}\n+  ins_pipe(ialu_mem_imm);\n+%}\n+\n+\n+\/\/ Long Logical Instructions\n+\n+\/\/ And Instructions\n+\/\/ And Register with Register\n+instruct andL_rReg(rRegL dst, rRegL src, rFlagsReg cr)\n+%{\n+  predicate(!UseAPX);\n+  match(Set dst (AndL dst src));\n+  effect(KILL cr);\n+  flag(PD::Flag_sets_sign_flag, PD::Flag_sets_zero_flag, PD::Flag_sets_parity_flag, PD::Flag_clears_overflow_flag, PD::Flag_clears_carry_flag);\n+\n+  format %{ \"andq    $dst, $src\\t# long\" %}\n+  ins_encode %{\n+    __ andq($dst$$Register, $src$$Register);\n+  %}\n+  ins_pipe(ialu_reg_reg);\n+%}\n+\n+\/\/ And Register with Register using New Data Destination (NDD)\n+instruct andL_rReg_ndd(rRegL dst, rRegL src1, rRegL src2, rFlagsReg cr)\n+%{\n+  predicate(UseAPX);\n+  match(Set dst (AndL src1 src2));\n+  effect(KILL cr);\n+  flag(PD::Flag_sets_sign_flag, PD::Flag_sets_zero_flag, PD::Flag_sets_parity_flag, PD::Flag_clears_overflow_flag, PD::Flag_clears_carry_flag);\n+\n+  format %{ \"eandq     $dst, $src1, $src2\\t# long ndd\" %}\n+  ins_encode %{\n+    __ eandq($dst$$Register, $src1$$Register, $src2$$Register, false);\n+\n+  %}\n+  ins_pipe(ialu_reg_reg);\n+%}\n+\n+\/\/ And Register with Immediate 255\n+instruct andL_rReg_imm255(rRegL dst, rRegL src, immL_255 mask)\n+%{\n+  match(Set dst (AndL src mask));\n+\n+  format %{ \"movzbl  $dst, $src\\t# long & 0xFF\" %}\n+  ins_encode %{\n+    \/\/ movzbl zeroes out the upper 32-bit and does not need REX.W\n+    __ movzbl($dst$$Register, $src$$Register);\n+  %}\n+  ins_pipe(ialu_reg);\n+%}\n+\n+\/\/ And Register with Immediate 65535\n+instruct andL_rReg_imm65535(rRegL dst, rRegL src, immL_65535 mask)\n+%{\n+  match(Set dst (AndL src mask));\n+\n+  format %{ \"movzwl  $dst, $src\\t# long & 0xFFFF\" %}\n+  ins_encode %{\n+    \/\/ movzwl zeroes out the upper 32-bit and does not need REX.W\n+    __ movzwl($dst$$Register, $src$$Register);\n+  %}\n+  ins_pipe(ialu_reg);\n+%}\n+\n+\/\/ And Register with Immediate\n+instruct andL_rReg_imm(rRegL dst, immL32 src, rFlagsReg cr)\n+%{\n+  predicate(!UseAPX);\n+  match(Set dst (AndL dst src));\n+  effect(KILL cr);\n+  flag(PD::Flag_sets_sign_flag, PD::Flag_sets_zero_flag, PD::Flag_sets_parity_flag, PD::Flag_clears_overflow_flag, PD::Flag_clears_carry_flag);\n+\n+  format %{ \"andq    $dst, $src\\t# long\" %}\n+  ins_encode %{\n+    __ andq($dst$$Register, $src$$constant);\n+  %}\n+  ins_pipe(ialu_reg);\n+%}\n+\n+instruct andL_rReg_rReg_imm_ndd(rRegL dst, rRegL src1, immL32 src2, rFlagsReg cr)\n+%{\n+  predicate(UseAPX);\n+  match(Set dst (AndL src1 src2));\n+  effect(KILL cr);\n+  flag(PD::Flag_sets_sign_flag, PD::Flag_sets_zero_flag, PD::Flag_sets_parity_flag, PD::Flag_clears_overflow_flag, PD::Flag_clears_carry_flag);\n+\n+  format %{ \"eandq    $dst, $src1, $src2\\t# long ndd\" %}\n+  ins_encode %{\n+    __ eandq($dst$$Register, $src1$$Register, $src2$$constant, false);\n+  %}\n+  ins_pipe(ialu_reg);\n+%}\n+\n+instruct andL_rReg_mem_imm_ndd(rRegL dst, memory src1, immL32 src2, rFlagsReg cr)\n+%{\n+  predicate(UseAPX);\n+  match(Set dst (AndL (LoadL src1) src2));\n+  effect(KILL cr);\n+  flag(PD::Flag_sets_sign_flag, PD::Flag_sets_zero_flag, PD::Flag_sets_parity_flag, PD::Flag_clears_overflow_flag, PD::Flag_clears_carry_flag);\n+\n+  format %{ \"eandq    $dst, $src1, $src2\\t# long ndd\" %}\n+  ins_encode %{\n+    __ eandq($dst$$Register, $src1$$Address, $src2$$constant, false);\n+  %}\n+  ins_pipe(ialu_reg);\n+%}\n+\n+\/\/ And Register with Memory\n+instruct andL_rReg_mem(rRegL dst, memory src, rFlagsReg cr)\n+%{\n+  predicate(!UseAPX);\n+  match(Set dst (AndL dst (LoadL src)));\n+  effect(KILL cr);\n+  flag(PD::Flag_sets_sign_flag, PD::Flag_sets_zero_flag, PD::Flag_sets_parity_flag, PD::Flag_clears_overflow_flag, PD::Flag_clears_carry_flag);\n+\n+  ins_cost(150);\n+  format %{ \"andq    $dst, $src\\t# long\" %}\n+  ins_encode %{\n+    __ andq($dst$$Register, $src$$Address);\n+  %}\n+  ins_pipe(ialu_reg_mem);\n+%}\n+\n+instruct andL_rReg_rReg_mem_ndd(rRegL dst, rRegL src1, memory src2, rFlagsReg cr)\n+%{\n+  predicate(UseAPX);\n+  match(Set dst (AndL src1 (LoadL src2)));\n+  effect(KILL cr);\n+  flag(PD::Flag_sets_sign_flag, PD::Flag_sets_zero_flag, PD::Flag_sets_parity_flag, PD::Flag_clears_overflow_flag, PD::Flag_clears_carry_flag);\n+\n+  ins_cost(150);\n+  format %{ \"eandq    $dst, $src1, $src2\\t# long ndd\" %}\n+  ins_encode %{\n+    __ eandq($dst$$Register, $src1$$Register, $src2$$Address, false);\n+  %}\n+  ins_pipe(ialu_reg_mem);\n+%}\n+\n+\/\/ And Memory with Register\n+instruct andL_mem_rReg(memory dst, rRegL src, rFlagsReg cr)\n+%{\n+  match(Set dst (StoreL dst (AndL (LoadL dst) src)));\n+  effect(KILL cr);\n+  flag(PD::Flag_sets_sign_flag, PD::Flag_sets_zero_flag, PD::Flag_sets_parity_flag, PD::Flag_clears_overflow_flag, PD::Flag_clears_carry_flag);\n+\n+  ins_cost(150);\n+  format %{ \"andq    $dst, $src\\t# long\" %}\n+  ins_encode %{\n+    __ andq($dst$$Address, $src$$Register);\n+  %}\n+  ins_pipe(ialu_mem_reg);\n+%}\n+\n+\/\/ And Memory with Immediate\n+instruct andL_mem_imm(memory dst, immL32 src, rFlagsReg cr)\n+%{\n+  match(Set dst (StoreL dst (AndL (LoadL dst) src)));\n+  effect(KILL cr);\n+  flag(PD::Flag_sets_sign_flag, PD::Flag_sets_zero_flag, PD::Flag_sets_parity_flag, PD::Flag_clears_overflow_flag, PD::Flag_clears_carry_flag);\n+\n+  ins_cost(125);\n+  format %{ \"andq    $dst, $src\\t# long\" %}\n+  ins_encode %{\n+    __ andq($dst$$Address, $src$$constant);\n+  %}\n+  ins_pipe(ialu_mem_imm);\n+%}\n+\n+instruct btrL_mem_imm(memory dst, immL_NotPow2 con, rFlagsReg cr)\n+%{\n+  \/\/ con should be a pure 64-bit immediate given that not(con) is a power of 2\n+  \/\/ because AND\/OR works well enough for 8\/32-bit values.\n+  predicate(log2i_graceful(~n->in(3)->in(2)->get_long()) > 30);\n+\n+  match(Set dst (StoreL dst (AndL (LoadL dst) con)));\n+  effect(KILL cr);\n+\n+  ins_cost(125);\n+  format %{ \"btrq    $dst, log2(not($con))\\t# long\" %}\n+  ins_encode %{\n+    __ btrq($dst$$Address, log2i_exact((julong)~$con$$constant));\n+  %}\n+  ins_pipe(ialu_mem_imm);\n+%}\n+\n+\/\/ BMI1 instructions\n+instruct andnL_rReg_rReg_mem(rRegL dst, rRegL src1, memory src2, immL_M1 minus_1, rFlagsReg cr) %{\n+  match(Set dst (AndL (XorL src1 minus_1) (LoadL src2)));\n+  predicate(UseBMI1Instructions);\n+  effect(KILL cr);\n+  flag(PD::Flag_sets_sign_flag, PD::Flag_sets_zero_flag, PD::Flag_clears_overflow_flag, PD::Flag_clears_carry_flag);\n+\n+  ins_cost(125);\n+  format %{ \"andnq  $dst, $src1, $src2\" %}\n+\n+  ins_encode %{\n+    __ andnq($dst$$Register, $src1$$Register, $src2$$Address);\n+  %}\n+  ins_pipe(ialu_reg_mem);\n+%}\n+\n+instruct andnL_rReg_rReg_rReg(rRegL dst, rRegL src1, rRegL src2, immL_M1 minus_1, rFlagsReg cr) %{\n+  match(Set dst (AndL (XorL src1 minus_1) src2));\n+  predicate(UseBMI1Instructions);\n+  effect(KILL cr);\n+  flag(PD::Flag_sets_sign_flag, PD::Flag_sets_zero_flag, PD::Flag_clears_overflow_flag, PD::Flag_clears_carry_flag);\n+\n+  format %{ \"andnq  $dst, $src1, $src2\" %}\n+\n+  ins_encode %{\n+  __ andnq($dst$$Register, $src1$$Register, $src2$$Register);\n+  %}\n+  ins_pipe(ialu_reg_mem);\n+%}\n+\n+instruct blsiL_rReg_rReg(rRegL dst, rRegL src, immL0 imm_zero, rFlagsReg cr) %{\n+  match(Set dst (AndL (SubL imm_zero src) src));\n+  predicate(UseBMI1Instructions);\n+  effect(KILL cr);\n+  flag(PD::Flag_sets_sign_flag, PD::Flag_sets_zero_flag, PD::Flag_clears_overflow_flag);\n+\n+  format %{ \"blsiq  $dst, $src\" %}\n+\n+  ins_encode %{\n+    __ blsiq($dst$$Register, $src$$Register);\n+  %}\n+  ins_pipe(ialu_reg);\n+%}\n+\n+instruct blsiL_rReg_mem(rRegL dst, memory src, immL0 imm_zero, rFlagsReg cr) %{\n+  match(Set dst (AndL (SubL imm_zero (LoadL src) ) (LoadL src) ));\n+  predicate(UseBMI1Instructions);\n+  effect(KILL cr);\n+  flag(PD::Flag_sets_sign_flag, PD::Flag_sets_zero_flag, PD::Flag_clears_overflow_flag);\n+\n+  ins_cost(125);\n+  format %{ \"blsiq  $dst, $src\" %}\n+\n+  ins_encode %{\n+    __ blsiq($dst$$Register, $src$$Address);\n+  %}\n+  ins_pipe(ialu_reg_mem);\n+%}\n+\n+instruct blsmskL_rReg_mem(rRegL dst, memory src, immL_M1 minus_1, rFlagsReg cr)\n+%{\n+  match(Set dst (XorL (AddL (LoadL src) minus_1) (LoadL src) ) );\n+  predicate(UseBMI1Instructions);\n+  effect(KILL cr);\n+  flag(PD::Flag_sets_sign_flag, PD::Flag_clears_zero_flag, PD::Flag_clears_overflow_flag);\n+\n+  ins_cost(125);\n+  format %{ \"blsmskq $dst, $src\" %}\n+\n+  ins_encode %{\n+    __ blsmskq($dst$$Register, $src$$Address);\n+  %}\n+  ins_pipe(ialu_reg_mem);\n+%}\n+\n+instruct blsmskL_rReg_rReg(rRegL dst, rRegL src, immL_M1 minus_1, rFlagsReg cr)\n+%{\n+  match(Set dst (XorL (AddL src minus_1) src));\n+  predicate(UseBMI1Instructions);\n+  effect(KILL cr);\n+  flag(PD::Flag_sets_sign_flag, PD::Flag_clears_zero_flag, PD::Flag_clears_overflow_flag);\n+\n+  format %{ \"blsmskq $dst, $src\" %}\n+\n+  ins_encode %{\n+    __ blsmskq($dst$$Register, $src$$Register);\n+  %}\n+\n+  ins_pipe(ialu_reg);\n+%}\n+\n+instruct blsrL_rReg_rReg(rRegL dst, rRegL src, immL_M1 minus_1, rFlagsReg cr)\n+%{\n+  match(Set dst (AndL (AddL src minus_1) src) );\n+  predicate(UseBMI1Instructions);\n+  effect(KILL cr);\n+  flag(PD::Flag_sets_sign_flag, PD::Flag_sets_zero_flag, PD::Flag_clears_overflow_flag);\n+\n+  format %{ \"blsrq  $dst, $src\" %}\n+\n+  ins_encode %{\n+    __ blsrq($dst$$Register, $src$$Register);\n+  %}\n+\n+  ins_pipe(ialu_reg);\n+%}\n+\n+instruct blsrL_rReg_mem(rRegL dst, memory src, immL_M1 minus_1, rFlagsReg cr)\n+%{\n+  match(Set dst (AndL (AddL (LoadL src) minus_1) (LoadL src)) );\n+  predicate(UseBMI1Instructions);\n+  effect(KILL cr);\n+  flag(PD::Flag_sets_sign_flag, PD::Flag_sets_zero_flag, PD::Flag_clears_overflow_flag);\n+\n+  ins_cost(125);\n+  format %{ \"blsrq  $dst, $src\" %}\n+\n+  ins_encode %{\n+    __ blsrq($dst$$Register, $src$$Address);\n+  %}\n+\n+  ins_pipe(ialu_reg);\n+%}\n+\n+\/\/ Or Instructions\n+\/\/ Or Register with Register\n+instruct orL_rReg(rRegL dst, rRegL src, rFlagsReg cr)\n+%{\n+  predicate(!UseAPX);\n+  match(Set dst (OrL dst src));\n+  effect(KILL cr);\n+  flag(PD::Flag_sets_sign_flag, PD::Flag_sets_zero_flag, PD::Flag_sets_parity_flag, PD::Flag_clears_overflow_flag, PD::Flag_clears_carry_flag);\n+\n+  format %{ \"orq     $dst, $src\\t# long\" %}\n+  ins_encode %{\n+    __ orq($dst$$Register, $src$$Register);\n+  %}\n+  ins_pipe(ialu_reg_reg);\n+%}\n+\n+\/\/ Or Register with Register using New Data Destination (NDD)\n+instruct orL_rReg_ndd(rRegL dst, rRegL src1, rRegL src2, rFlagsReg cr)\n+%{\n+  predicate(UseAPX);\n+  match(Set dst (OrL src1 src2));\n+  effect(KILL cr);\n+  flag(PD::Flag_sets_sign_flag, PD::Flag_sets_zero_flag, PD::Flag_sets_parity_flag, PD::Flag_clears_overflow_flag, PD::Flag_clears_carry_flag);\n+\n+  format %{ \"eorq     $dst, $src1, $src2\\t# long ndd\" %}\n+  ins_encode %{\n+    __ eorq($dst$$Register, $src1$$Register, $src2$$Register, false);\n+\n+  %}\n+  ins_pipe(ialu_reg_reg);\n+%}\n+\n+\/\/ Use any_RegP to match R15 (TLS register) without spilling.\n+instruct orL_rReg_castP2X(rRegL dst, any_RegP src, rFlagsReg cr) %{\n+  match(Set dst (OrL dst (CastP2X src)));\n+  effect(KILL cr);\n+  flag(PD::Flag_sets_sign_flag, PD::Flag_sets_zero_flag, PD::Flag_sets_parity_flag, PD::Flag_clears_overflow_flag, PD::Flag_clears_carry_flag);\n+\n+  format %{ \"orq     $dst, $src\\t# long\" %}\n+  ins_encode %{\n+    __ orq($dst$$Register, $src$$Register);\n+  %}\n+  ins_pipe(ialu_reg_reg);\n+%}\n+\n+instruct orL_rReg_castP2X_ndd(rRegL dst, any_RegP src1, any_RegP src2, rFlagsReg cr) %{\n+  match(Set dst (OrL src1 (CastP2X src2)));\n+  effect(KILL cr);\n+  flag(PD::Flag_sets_sign_flag, PD::Flag_sets_zero_flag, PD::Flag_sets_parity_flag, PD::Flag_clears_overflow_flag, PD::Flag_clears_carry_flag);\n+\n+  format %{ \"eorq     $dst, $src1, $src2\\t# long ndd\" %}\n+  ins_encode %{\n+    __ eorq($dst$$Register, $src1$$Register, $src2$$Register, false);\n+  %}\n+  ins_pipe(ialu_reg_reg);\n+%}\n+\n+\/\/ Or Register with Immediate\n+instruct orL_rReg_imm(rRegL dst, immL32 src, rFlagsReg cr)\n+%{\n+  predicate(!UseAPX);\n+  match(Set dst (OrL dst src));\n+  effect(KILL cr);\n+  flag(PD::Flag_sets_sign_flag, PD::Flag_sets_zero_flag, PD::Flag_sets_parity_flag, PD::Flag_clears_overflow_flag, PD::Flag_clears_carry_flag);\n+\n+  format %{ \"orq     $dst, $src\\t# long\" %}\n+  ins_encode %{\n+    __ orq($dst$$Register, $src$$constant);\n+  %}\n+  ins_pipe(ialu_reg);\n+%}\n+\n+instruct orL_rReg_rReg_imm_ndd(rRegL dst, rRegL src1, immL32 src2, rFlagsReg cr)\n+%{\n+  predicate(UseAPX);\n+  match(Set dst (OrL src1 src2));\n+  effect(KILL cr);\n+  flag(PD::Flag_sets_sign_flag, PD::Flag_sets_zero_flag, PD::Flag_sets_parity_flag, PD::Flag_clears_overflow_flag, PD::Flag_clears_carry_flag);\n+\n+  format %{ \"eorq     $dst, $src1, $src2\\t# long ndd\" %}\n+  ins_encode %{\n+    __ eorq($dst$$Register, $src1$$Register, $src2$$constant, false);\n+  %}\n+  ins_pipe(ialu_reg);\n+%}\n+\n+instruct orL_rReg_imm_rReg_ndd(rRegL dst, immL32 src1, rRegL src2, rFlagsReg cr)\n+%{\n+  predicate(UseAPX);\n+  match(Set dst (OrL src1 src2));\n+  effect(KILL cr);\n+  flag(PD::Flag_sets_sign_flag, PD::Flag_sets_zero_flag, PD::Flag_sets_parity_flag, PD::Flag_clears_overflow_flag, PD::Flag_clears_carry_flag);\n+\n+  format %{ \"eorq     $dst, $src2, $src1\\t# long ndd\" %}\n+  ins_encode %{\n+    __ eorq($dst$$Register, $src2$$Register, $src1$$constant, false);\n+  %}\n+  ins_pipe(ialu_reg);\n+%}\n+\n+\/\/ Or Memory with Immediate\n+instruct orL_rReg_mem_imm_ndd(rRegL dst, memory src1, immL32 src2, rFlagsReg cr)\n+%{\n+  predicate(UseAPX);\n+  match(Set dst (OrL (LoadL src1) src2));\n+  effect(KILL cr);\n+  flag(PD::Flag_sets_sign_flag, PD::Flag_sets_zero_flag, PD::Flag_sets_parity_flag, PD::Flag_clears_overflow_flag, PD::Flag_clears_carry_flag);\n+\n+  format %{ \"eorq     $dst, $src1, $src2\\t# long ndd\" %}\n+  ins_encode %{\n+    __ eorq($dst$$Register, $src1$$Address, $src2$$constant, false);\n+  %}\n+  ins_pipe(ialu_reg);\n+%}\n+\n+\/\/ Or Register with Memory\n+instruct orL_rReg_mem(rRegL dst, memory src, rFlagsReg cr)\n+%{\n+  predicate(!UseAPX);\n+  match(Set dst (OrL dst (LoadL src)));\n+  effect(KILL cr);\n+  flag(PD::Flag_sets_sign_flag, PD::Flag_sets_zero_flag, PD::Flag_sets_parity_flag, PD::Flag_clears_overflow_flag, PD::Flag_clears_carry_flag);\n+\n+  ins_cost(150);\n+  format %{ \"orq     $dst, $src\\t# long\" %}\n+  ins_encode %{\n+    __ orq($dst$$Register, $src$$Address);\n+  %}\n+  ins_pipe(ialu_reg_mem);\n+%}\n+\n+instruct orL_rReg_rReg_mem_ndd(rRegL dst, rRegL src1, memory src2, rFlagsReg cr)\n+%{\n+  predicate(UseAPX);\n+  match(Set dst (OrL src1 (LoadL src2)));\n+  effect(KILL cr);\n+  flag(PD::Flag_sets_sign_flag, PD::Flag_sets_zero_flag, PD::Flag_sets_parity_flag, PD::Flag_clears_overflow_flag, PD::Flag_clears_carry_flag);\n+\n+  ins_cost(150);\n+  format %{ \"eorq     $dst, $src1, $src2\\t# long ndd\" %}\n+  ins_encode %{\n+    __ eorq($dst$$Register, $src1$$Register, $src2$$Address, false);\n+  %}\n+  ins_pipe(ialu_reg_mem);\n+%}\n+\n+\/\/ Or Memory with Register\n+instruct orL_mem_rReg(memory dst, rRegL src, rFlagsReg cr)\n+%{\n+  match(Set dst (StoreL dst (OrL (LoadL dst) src)));\n+  effect(KILL cr);\n+  flag(PD::Flag_sets_sign_flag, PD::Flag_sets_zero_flag, PD::Flag_sets_parity_flag, PD::Flag_clears_overflow_flag, PD::Flag_clears_carry_flag);\n+\n+  ins_cost(150);\n+  format %{ \"orq     $dst, $src\\t# long\" %}\n+  ins_encode %{\n+    __ orq($dst$$Address, $src$$Register);\n+  %}\n+  ins_pipe(ialu_mem_reg);\n+%}\n+\n+\/\/ Or Memory with Immediate\n+instruct orL_mem_imm(memory dst, immL32 src, rFlagsReg cr)\n+%{\n+  match(Set dst (StoreL dst (OrL (LoadL dst) src)));\n+  effect(KILL cr);\n+  flag(PD::Flag_sets_sign_flag, PD::Flag_sets_zero_flag, PD::Flag_sets_parity_flag, PD::Flag_clears_overflow_flag, PD::Flag_clears_carry_flag);\n+\n+  ins_cost(125);\n+  format %{ \"orq     $dst, $src\\t# long\" %}\n+  ins_encode %{\n+    __ orq($dst$$Address, $src$$constant);\n+  %}\n+  ins_pipe(ialu_mem_imm);\n+%}\n+\n+instruct btsL_mem_imm(memory dst, immL_Pow2 con, rFlagsReg cr)\n+%{\n+  \/\/ con should be a pure 64-bit power of 2 immediate\n+  \/\/ because AND\/OR works well enough for 8\/32-bit values.\n+  predicate(log2i_graceful(n->in(3)->in(2)->get_long()) > 31);\n+\n+  match(Set dst (StoreL dst (OrL (LoadL dst) con)));\n+  effect(KILL cr);\n+\n+  ins_cost(125);\n+  format %{ \"btsq    $dst, log2($con)\\t# long\" %}\n+  ins_encode %{\n+    __ btsq($dst$$Address, log2i_exact((julong)$con$$constant));\n+  %}\n+  ins_pipe(ialu_mem_imm);\n+%}\n+\n+\/\/ Xor Instructions\n+\/\/ Xor Register with Register\n+instruct xorL_rReg(rRegL dst, rRegL src, rFlagsReg cr)\n+%{\n+  predicate(!UseAPX);\n+  match(Set dst (XorL dst src));\n+  effect(KILL cr);\n+  flag(PD::Flag_sets_sign_flag, PD::Flag_sets_zero_flag, PD::Flag_sets_parity_flag, PD::Flag_clears_overflow_flag, PD::Flag_clears_carry_flag);\n+\n+  format %{ \"xorq    $dst, $src\\t# long\" %}\n+  ins_encode %{\n+    __ xorq($dst$$Register, $src$$Register);\n+  %}\n+  ins_pipe(ialu_reg_reg);\n+%}\n+\n+\/\/ Xor Register with Register using New Data Destination (NDD)\n+instruct xorL_rReg_ndd(rRegL dst, rRegL src1, rRegL src2, rFlagsReg cr)\n+%{\n+  predicate(UseAPX);\n+  match(Set dst (XorL src1 src2));\n+  effect(KILL cr);\n+  flag(PD::Flag_sets_sign_flag, PD::Flag_sets_zero_flag, PD::Flag_sets_parity_flag, PD::Flag_clears_overflow_flag, PD::Flag_clears_carry_flag);\n+\n+  format %{ \"exorq    $dst, $src1, $src2\\t# long ndd\" %}\n+  ins_encode %{\n+    __ exorq($dst$$Register, $src1$$Register, $src2$$Register, false);\n+  %}\n+  ins_pipe(ialu_reg_reg);\n+%}\n+\n+\/\/ Xor Register with Immediate -1\n+instruct xorL_rReg_im1(rRegL dst, immL_M1 imm)\n+%{\n+  predicate(!UseAPX);\n+  match(Set dst (XorL dst imm));\n+\n+  format %{ \"notq   $dst\" %}\n+  ins_encode %{\n+     __ notq($dst$$Register);\n+  %}\n+  ins_pipe(ialu_reg);\n+%}\n+\n+instruct xorL_rReg_im1_ndd(rRegL dst,rRegL src, immL_M1 imm)\n+%{\n+  predicate(UseAPX);\n+  match(Set dst (XorL src imm));\n+\n+  format %{ \"enotq   $dst, $src\" %}\n+  ins_encode %{\n+    __ enotq($dst$$Register, $src$$Register);\n+  %}\n+  ins_pipe(ialu_reg);\n+%}\n+\n+\/\/ Xor Register with Immediate\n+instruct xorL_rReg_imm(rRegL dst, immL32 src, rFlagsReg cr)\n+%{\n+  \/\/ Strict predicate check to make selection of xorL_rReg_im1 cost agnostic if immL32 src is -1.\n+  predicate(!UseAPX && n->in(2)->bottom_type()->is_long()->get_con() != -1L);\n+  match(Set dst (XorL dst src));\n+  effect(KILL cr);\n+  flag(PD::Flag_sets_sign_flag, PD::Flag_sets_zero_flag, PD::Flag_sets_parity_flag, PD::Flag_clears_overflow_flag, PD::Flag_clears_carry_flag);\n+\n+  format %{ \"xorq    $dst, $src\\t# long\" %}\n+  ins_encode %{\n+    __ xorq($dst$$Register, $src$$constant);\n+  %}\n+  ins_pipe(ialu_reg);\n+%}\n+\n+instruct xorL_rReg_rReg_imm(rRegL dst, rRegL src1, immL32 src2, rFlagsReg cr)\n+%{\n+  \/\/ Strict predicate check to make selection of xorL_rReg_im1_ndd cost agnostic if immL32 src2 is -1.\n+  predicate(UseAPX && n->in(2)->bottom_type()->is_long()->get_con() != -1L);\n+  match(Set dst (XorL src1 src2));\n+  effect(KILL cr);\n+  flag(PD::Flag_sets_sign_flag, PD::Flag_sets_zero_flag, PD::Flag_sets_parity_flag, PD::Flag_clears_overflow_flag, PD::Flag_clears_carry_flag);\n+\n+  format %{ \"exorq    $dst, $src1, $src2\\t# long ndd\" %}\n+  ins_encode %{\n+    __ exorq($dst$$Register, $src1$$Register, $src2$$constant, false);\n+  %}\n+  ins_pipe(ialu_reg);\n+%}\n+\n+\/\/ Xor Memory with Immediate\n+instruct xorL_rReg_mem_imm(rRegL dst, memory src1, immL32 src2, rFlagsReg cr)\n+%{\n+  predicate(UseAPX);\n+  match(Set dst (XorL (LoadL src1) src2));\n+  effect(KILL cr);\n+  flag(PD::Flag_sets_sign_flag, PD::Flag_sets_zero_flag, PD::Flag_sets_parity_flag, PD::Flag_clears_overflow_flag, PD::Flag_clears_carry_flag);\n+  ins_cost(150);\n+\n+  format %{ \"exorq    $dst, $src1, $src2\\t# long ndd\" %}\n+  ins_encode %{\n+    __ exorq($dst$$Register, $src1$$Address, $src2$$constant, false);\n+  %}\n+  ins_pipe(ialu_reg);\n+%}\n+\n+\/\/ Xor Register with Memory\n+instruct xorL_rReg_mem(rRegL dst, memory src, rFlagsReg cr)\n+%{\n+  predicate(!UseAPX);\n+  match(Set dst (XorL dst (LoadL src)));\n+  effect(KILL cr);\n+  flag(PD::Flag_sets_sign_flag, PD::Flag_sets_zero_flag, PD::Flag_sets_parity_flag, PD::Flag_clears_overflow_flag, PD::Flag_clears_carry_flag);\n+\n+  ins_cost(150);\n+  format %{ \"xorq    $dst, $src\\t# long\" %}\n+  ins_encode %{\n+    __ xorq($dst$$Register, $src$$Address);\n+  %}\n+  ins_pipe(ialu_reg_mem);\n+%}\n+\n+instruct xorL_rReg_rReg_mem_ndd(rRegL dst, rRegL src1, memory src2, rFlagsReg cr)\n+%{\n+  predicate(UseAPX);\n+  match(Set dst (XorL src1 (LoadL src2)));\n+  effect(KILL cr);\n+  flag(PD::Flag_sets_sign_flag, PD::Flag_sets_zero_flag, PD::Flag_sets_parity_flag, PD::Flag_clears_overflow_flag, PD::Flag_clears_carry_flag);\n+\n+  ins_cost(150);\n+  format %{ \"exorq    $dst, $src1, $src2\\t# long ndd\" %}\n+  ins_encode %{\n+    __ exorq($dst$$Register, $src1$$Register, $src2$$Address, false);\n+  %}\n+  ins_pipe(ialu_reg_mem);\n+%}\n+\n+\/\/ Xor Memory with Register\n+instruct xorL_mem_rReg(memory dst, rRegL src, rFlagsReg cr)\n+%{\n+  match(Set dst (StoreL dst (XorL (LoadL dst) src)));\n+  effect(KILL cr);\n+  flag(PD::Flag_sets_sign_flag, PD::Flag_sets_zero_flag, PD::Flag_sets_parity_flag, PD::Flag_clears_overflow_flag, PD::Flag_clears_carry_flag);\n+\n+  ins_cost(150);\n+  format %{ \"xorq    $dst, $src\\t# long\" %}\n+  ins_encode %{\n+    __ xorq($dst$$Address, $src$$Register);\n+  %}\n+  ins_pipe(ialu_mem_reg);\n+%}\n+\n+\/\/ Xor Memory with Immediate\n+instruct xorL_mem_imm(memory dst, immL32 src, rFlagsReg cr)\n+%{\n+  match(Set dst (StoreL dst (XorL (LoadL dst) src)));\n+  effect(KILL cr);\n+  flag(PD::Flag_sets_sign_flag, PD::Flag_sets_zero_flag, PD::Flag_sets_parity_flag, PD::Flag_clears_overflow_flag, PD::Flag_clears_carry_flag);\n+\n+  ins_cost(125);\n+  format %{ \"xorq    $dst, $src\\t# long\" %}\n+  ins_encode %{\n+    __ xorq($dst$$Address, $src$$constant);\n+  %}\n+  ins_pipe(ialu_mem_imm);\n+%}\n+\n+instruct cmpLTMask(rRegI dst, rRegI p, rRegI q, rFlagsReg cr)\n+%{\n+  match(Set dst (CmpLTMask p q));\n+  effect(KILL cr);\n+\n+  ins_cost(400);\n+  format %{ \"cmpl    $p, $q\\t# cmpLTMask\\n\\t\"\n+            \"setcc   $dst \\t# emits setlt + movzbl or setzul for APX\"\n+            \"negl    $dst\" %}\n+  ins_encode %{\n+    __ cmpl($p$$Register, $q$$Register);\n+    __ setcc(Assembler::less, $dst$$Register);\n+    __ negl($dst$$Register);\n+  %}\n+  ins_pipe(pipe_slow);\n+%}\n+\n+instruct cmpLTMask0(rRegI dst, immI_0 zero, rFlagsReg cr)\n+%{\n+  match(Set dst (CmpLTMask dst zero));\n+  effect(KILL cr);\n+\n+  ins_cost(100);\n+  format %{ \"sarl    $dst, #31\\t# cmpLTMask0\" %}\n+  ins_encode %{\n+    __ sarl($dst$$Register, 31);\n+  %}\n+  ins_pipe(ialu_reg);\n+%}\n+\n+\/* Better to save a register than avoid a branch *\/\n+instruct cadd_cmpLTMask(rRegI p, rRegI q, rRegI y, rFlagsReg cr)\n+%{\n+  match(Set p (AddI (AndI (CmpLTMask p q) y) (SubI p q)));\n+  effect(KILL cr);\n+  ins_cost(300);\n+  format %{ \"subl    $p,$q\\t# cadd_cmpLTMask\\n\\t\"\n+            \"jge     done\\n\\t\"\n+            \"addl    $p,$y\\n\"\n+            \"done:   \" %}\n+  ins_encode %{\n+    Register Rp = $p$$Register;\n+    Register Rq = $q$$Register;\n+    Register Ry = $y$$Register;\n+    Label done;\n+    __ subl(Rp, Rq);\n+    __ jccb(Assembler::greaterEqual, done);\n+    __ addl(Rp, Ry);\n+    __ bind(done);\n+  %}\n+  ins_pipe(pipe_cmplt);\n+%}\n+\n+\/* Better to save a register than avoid a branch *\/\n+instruct and_cmpLTMask(rRegI p, rRegI q, rRegI y, rFlagsReg cr)\n+%{\n+  match(Set y (AndI (CmpLTMask p q) y));\n+  effect(KILL cr);\n+\n+  ins_cost(300);\n+\n+  format %{ \"cmpl    $p, $q\\t# and_cmpLTMask\\n\\t\"\n+            \"jlt     done\\n\\t\"\n+            \"xorl    $y, $y\\n\"\n+            \"done:   \" %}\n+  ins_encode %{\n+    Register Rp = $p$$Register;\n+    Register Rq = $q$$Register;\n+    Register Ry = $y$$Register;\n+    Label done;\n+    __ cmpl(Rp, Rq);\n+    __ jccb(Assembler::less, done);\n+    __ xorl(Ry, Ry);\n+    __ bind(done);\n+  %}\n+  ins_pipe(pipe_cmplt);\n+%}\n+\n+\n+\/\/---------- FP Instructions------------------------------------------------\n+\n+\/\/ Really expensive, avoid\n+instruct cmpF_cc_reg(rFlagsRegU cr, regF src1, regF src2)\n+%{\n+  match(Set cr (CmpF src1 src2));\n+\n+  ins_cost(500);\n+  format %{ \"ucomiss $src1, $src2\\n\\t\"\n+            \"jnp,s   exit\\n\\t\"\n+            \"pushfq\\t# saw NaN, set CF\\n\\t\"\n+            \"andq    [rsp], #0xffffff2b\\n\\t\"\n+            \"popfq\\n\"\n+    \"exit:\" %}\n+  ins_encode %{\n+    __ ucomiss($src1$$XMMRegister, $src2$$XMMRegister);\n+    emit_cmpfp_fixup(masm);\n+  %}\n+  ins_pipe(pipe_slow);\n+%}\n+\n+instruct cmpF_cc_reg_CF(rFlagsRegUCF cr, regF src1, regF src2) %{\n+  match(Set cr (CmpF src1 src2));\n+\n+  ins_cost(100);\n+  format %{ \"ucomiss $src1, $src2\" %}\n+  ins_encode %{\n+    __ ucomiss($src1$$XMMRegister, $src2$$XMMRegister);\n+  %}\n+  ins_pipe(pipe_slow);\n+%}\n+\n+instruct cmpF_cc_memCF(rFlagsRegUCF cr, regF src1, memory src2) %{\n+  match(Set cr (CmpF src1 (LoadF src2)));\n+\n+  ins_cost(100);\n+  format %{ \"ucomiss $src1, $src2\" %}\n+  ins_encode %{\n+    __ ucomiss($src1$$XMMRegister, $src2$$Address);\n+  %}\n+  ins_pipe(pipe_slow);\n+%}\n+\n+instruct cmpF_cc_immCF(rFlagsRegUCF cr, regF src, immF con) %{\n+  match(Set cr (CmpF src con));\n+  ins_cost(100);\n+  format %{ \"ucomiss $src, [$constantaddress]\\t# load from constant table: float=$con\" %}\n+  ins_encode %{\n+    __ ucomiss($src$$XMMRegister, $constantaddress($con));\n+  %}\n+  ins_pipe(pipe_slow);\n+%}\n+\n+\/\/ Really expensive, avoid\n+instruct cmpD_cc_reg(rFlagsRegU cr, regD src1, regD src2)\n+%{\n+  match(Set cr (CmpD src1 src2));\n+\n+  ins_cost(500);\n+  format %{ \"ucomisd $src1, $src2\\n\\t\"\n+            \"jnp,s   exit\\n\\t\"\n+            \"pushfq\\t# saw NaN, set CF\\n\\t\"\n+            \"andq    [rsp], #0xffffff2b\\n\\t\"\n+            \"popfq\\n\"\n+    \"exit:\" %}\n+  ins_encode %{\n+    __ ucomisd($src1$$XMMRegister, $src2$$XMMRegister);\n+    emit_cmpfp_fixup(masm);\n+  %}\n+  ins_pipe(pipe_slow);\n+%}\n+\n+instruct cmpD_cc_reg_CF(rFlagsRegUCF cr, regD src1, regD src2) %{\n+  match(Set cr (CmpD src1 src2));\n+\n+  ins_cost(100);\n+  format %{ \"ucomisd $src1, $src2 test\" %}\n+  ins_encode %{\n+    __ ucomisd($src1$$XMMRegister, $src2$$XMMRegister);\n+  %}\n+  ins_pipe(pipe_slow);\n+%}\n+\n+instruct cmpD_cc_memCF(rFlagsRegUCF cr, regD src1, memory src2) %{\n+  match(Set cr (CmpD src1 (LoadD src2)));\n+\n+  ins_cost(100);\n+  format %{ \"ucomisd $src1, $src2\" %}\n+  ins_encode %{\n+    __ ucomisd($src1$$XMMRegister, $src2$$Address);\n+  %}\n+  ins_pipe(pipe_slow);\n+%}\n+\n+instruct cmpD_cc_immCF(rFlagsRegUCF cr, regD src, immD con) %{\n+  match(Set cr (CmpD src con));\n+  ins_cost(100);\n+  format %{ \"ucomisd $src, [$constantaddress]\\t# load from constant table: double=$con\" %}\n+  ins_encode %{\n+    __ ucomisd($src$$XMMRegister, $constantaddress($con));\n+  %}\n+  ins_pipe(pipe_slow);\n+%}\n+\n+\/\/ Compare into -1,0,1\n+instruct cmpF_reg(rRegI dst, regF src1, regF src2, rFlagsReg cr)\n+%{\n+  match(Set dst (CmpF3 src1 src2));\n+  effect(KILL cr);\n+\n+  ins_cost(275);\n+  format %{ \"ucomiss $src1, $src2\\n\\t\"\n+            \"movl    $dst, #-1\\n\\t\"\n+            \"jp,s    done\\n\\t\"\n+            \"jb,s    done\\n\\t\"\n+            \"setne   $dst\\n\\t\"\n+            \"movzbl  $dst, $dst\\n\"\n+    \"done:\" %}\n+  ins_encode %{\n+    __ ucomiss($src1$$XMMRegister, $src2$$XMMRegister);\n+    emit_cmpfp3(masm, $dst$$Register);\n+  %}\n+  ins_pipe(pipe_slow);\n+%}\n+\n+\/\/ Compare into -1,0,1\n+instruct cmpF_mem(rRegI dst, regF src1, memory src2, rFlagsReg cr)\n+%{\n+  match(Set dst (CmpF3 src1 (LoadF src2)));\n+  effect(KILL cr);\n+\n+  ins_cost(275);\n+  format %{ \"ucomiss $src1, $src2\\n\\t\"\n+            \"movl    $dst, #-1\\n\\t\"\n+            \"jp,s    done\\n\\t\"\n+            \"jb,s    done\\n\\t\"\n+            \"setne   $dst\\n\\t\"\n+            \"movzbl  $dst, $dst\\n\"\n+    \"done:\" %}\n+  ins_encode %{\n+    __ ucomiss($src1$$XMMRegister, $src2$$Address);\n+    emit_cmpfp3(masm, $dst$$Register);\n+  %}\n+  ins_pipe(pipe_slow);\n+%}\n+\n+\/\/ Compare into -1,0,1\n+instruct cmpF_imm(rRegI dst, regF src, immF con, rFlagsReg cr) %{\n+  match(Set dst (CmpF3 src con));\n+  effect(KILL cr);\n+\n+  ins_cost(275);\n+  format %{ \"ucomiss $src, [$constantaddress]\\t# load from constant table: float=$con\\n\\t\"\n+            \"movl    $dst, #-1\\n\\t\"\n+            \"jp,s    done\\n\\t\"\n+            \"jb,s    done\\n\\t\"\n+            \"setne   $dst\\n\\t\"\n+            \"movzbl  $dst, $dst\\n\"\n+    \"done:\" %}\n+  ins_encode %{\n+    __ ucomiss($src$$XMMRegister, $constantaddress($con));\n+    emit_cmpfp3(masm, $dst$$Register);\n+  %}\n+  ins_pipe(pipe_slow);\n+%}\n+\n+\/\/ Compare into -1,0,1\n+instruct cmpD_reg(rRegI dst, regD src1, regD src2, rFlagsReg cr)\n+%{\n+  match(Set dst (CmpD3 src1 src2));\n+  effect(KILL cr);\n+\n+  ins_cost(275);\n+  format %{ \"ucomisd $src1, $src2\\n\\t\"\n+            \"movl    $dst, #-1\\n\\t\"\n+            \"jp,s    done\\n\\t\"\n+            \"jb,s    done\\n\\t\"\n+            \"setne   $dst\\n\\t\"\n+            \"movzbl  $dst, $dst\\n\"\n+    \"done:\" %}\n+  ins_encode %{\n+    __ ucomisd($src1$$XMMRegister, $src2$$XMMRegister);\n+    emit_cmpfp3(masm, $dst$$Register);\n+  %}\n+  ins_pipe(pipe_slow);\n+%}\n+\n+\/\/ Compare into -1,0,1\n+instruct cmpD_mem(rRegI dst, regD src1, memory src2, rFlagsReg cr)\n+%{\n+  match(Set dst (CmpD3 src1 (LoadD src2)));\n+  effect(KILL cr);\n+\n+  ins_cost(275);\n+  format %{ \"ucomisd $src1, $src2\\n\\t\"\n+            \"movl    $dst, #-1\\n\\t\"\n+            \"jp,s    done\\n\\t\"\n+            \"jb,s    done\\n\\t\"\n+            \"setne   $dst\\n\\t\"\n+            \"movzbl  $dst, $dst\\n\"\n+    \"done:\" %}\n+  ins_encode %{\n+    __ ucomisd($src1$$XMMRegister, $src2$$Address);\n+    emit_cmpfp3(masm, $dst$$Register);\n+  %}\n+  ins_pipe(pipe_slow);\n+%}\n+\n+\/\/ Compare into -1,0,1\n+instruct cmpD_imm(rRegI dst, regD src, immD con, rFlagsReg cr) %{\n+  match(Set dst (CmpD3 src con));\n+  effect(KILL cr);\n+\n+  ins_cost(275);\n+  format %{ \"ucomisd $src, [$constantaddress]\\t# load from constant table: double=$con\\n\\t\"\n+            \"movl    $dst, #-1\\n\\t\"\n+            \"jp,s    done\\n\\t\"\n+            \"jb,s    done\\n\\t\"\n+            \"setne   $dst\\n\\t\"\n+            \"movzbl  $dst, $dst\\n\"\n+    \"done:\" %}\n+  ins_encode %{\n+    __ ucomisd($src$$XMMRegister, $constantaddress($con));\n+    emit_cmpfp3(masm, $dst$$Register);\n+  %}\n+  ins_pipe(pipe_slow);\n+%}\n+\n+\/\/----------Arithmetic Conversion Instructions---------------------------------\n+\n+instruct convF2D_reg_reg(regD dst, regF src)\n+%{\n+  match(Set dst (ConvF2D src));\n+\n+  format %{ \"cvtss2sd $dst, $src\" %}\n+  ins_encode %{\n+    __ cvtss2sd ($dst$$XMMRegister, $src$$XMMRegister);\n+  %}\n+  ins_pipe(pipe_slow); \/\/ XXX\n+%}\n+\n+instruct convF2D_reg_mem(regD dst, memory src)\n+%{\n+  predicate(UseAVX == 0);\n+  match(Set dst (ConvF2D (LoadF src)));\n+\n+  format %{ \"cvtss2sd $dst, $src\" %}\n+  ins_encode %{\n+    __ cvtss2sd ($dst$$XMMRegister, $src$$Address);\n+  %}\n+  ins_pipe(pipe_slow); \/\/ XXX\n+%}\n+\n+instruct convD2F_reg_reg(regF dst, regD src)\n+%{\n+  match(Set dst (ConvD2F src));\n+\n+  format %{ \"cvtsd2ss $dst, $src\" %}\n+  ins_encode %{\n+    __ cvtsd2ss ($dst$$XMMRegister, $src$$XMMRegister);\n+  %}\n+  ins_pipe(pipe_slow); \/\/ XXX\n+%}\n+\n+instruct convD2F_reg_mem(regF dst, memory src)\n+%{\n+  predicate(UseAVX == 0);\n+  match(Set dst (ConvD2F (LoadD src)));\n+\n+  format %{ \"cvtsd2ss $dst, $src\" %}\n+  ins_encode %{\n+    __ cvtsd2ss ($dst$$XMMRegister, $src$$Address);\n+  %}\n+  ins_pipe(pipe_slow); \/\/ XXX\n+%}\n+\n+\/\/ XXX do mem variants\n+instruct convF2I_reg_reg(rRegI dst, regF src, rFlagsReg cr)\n+%{\n+  predicate(!VM_Version::supports_avx10_2());\n+  match(Set dst (ConvF2I src));\n+  effect(KILL cr);\n+  format %{ \"convert_f2i $dst, $src\" %}\n+  ins_encode %{\n+    __ convertF2I(T_INT, T_FLOAT, $dst$$Register, $src$$XMMRegister);\n+  %}\n+  ins_pipe(pipe_slow);\n+%}\n+\n+instruct convF2I_reg_reg_avx10(rRegI dst, regF src)\n+%{\n+  predicate(VM_Version::supports_avx10_2());\n+  match(Set dst (ConvF2I src));\n+  format %{ \"evcvttss2sisl $dst, $src\" %}\n+  ins_encode %{\n+    __ evcvttss2sisl($dst$$Register, $src$$XMMRegister);\n+  %}\n+  ins_pipe(pipe_slow);\n+%}\n+\n+instruct convF2I_reg_mem_avx10(rRegI dst, memory src)\n+%{\n+  predicate(VM_Version::supports_avx10_2());\n+  match(Set dst (ConvF2I (LoadF src)));\n+  format %{ \"evcvttss2sisl $dst, $src\" %}\n+  ins_encode %{\n+    __ evcvttss2sisl($dst$$Register, $src$$Address);\n+  %}\n+  ins_pipe(pipe_slow);\n+%}\n+\n+instruct convF2L_reg_reg(rRegL dst, regF src, rFlagsReg cr)\n+%{\n+  predicate(!VM_Version::supports_avx10_2());\n+  match(Set dst (ConvF2L src));\n+  effect(KILL cr);\n+  format %{ \"convert_f2l $dst, $src\"%}\n+  ins_encode %{\n+    __ convertF2I(T_LONG, T_FLOAT, $dst$$Register, $src$$XMMRegister);\n+  %}\n+  ins_pipe(pipe_slow);\n+%}\n+\n+instruct convF2L_reg_reg_avx10(rRegL dst, regF src)\n+%{\n+  predicate(VM_Version::supports_avx10_2());\n+  match(Set dst (ConvF2L src));\n+  format %{ \"evcvttss2sisq $dst, $src\" %}\n+  ins_encode %{\n+    __ evcvttss2sisq($dst$$Register, $src$$XMMRegister);\n+  %}\n+  ins_pipe(pipe_slow);\n+%}\n+\n+instruct convF2L_reg_mem_avx10(rRegL dst, memory src)\n+%{\n+  predicate(VM_Version::supports_avx10_2());\n+  match(Set dst (ConvF2L (LoadF src)));\n+  format %{ \"evcvttss2sisq $dst, $src\" %}\n+  ins_encode %{\n+    __ evcvttss2sisq($dst$$Register, $src$$Address);\n+  %}\n+  ins_pipe(pipe_slow);\n+%}\n+\n+instruct convD2I_reg_reg(rRegI dst, regD src, rFlagsReg cr)\n+%{\n+  predicate(!VM_Version::supports_avx10_2());\n+  match(Set dst (ConvD2I src));\n+  effect(KILL cr);\n+  format %{ \"convert_d2i $dst, $src\"%}\n+  ins_encode %{\n+    __ convertF2I(T_INT, T_DOUBLE, $dst$$Register, $src$$XMMRegister);\n+  %}\n+  ins_pipe(pipe_slow);\n+%}\n+\n+instruct convD2I_reg_reg_avx10(rRegI dst, regD src)\n+%{\n+  predicate(VM_Version::supports_avx10_2());\n+  match(Set dst (ConvD2I src));\n+  format %{ \"evcvttsd2sisl $dst, $src\" %}\n+  ins_encode %{\n+    __ evcvttsd2sisl($dst$$Register, $src$$XMMRegister);\n+  %}\n+  ins_pipe(pipe_slow);\n+%}\n+\n+instruct convD2I_reg_mem_avx10(rRegI dst, memory src)\n+%{\n+  predicate(VM_Version::supports_avx10_2());\n+  match(Set dst (ConvD2I (LoadD src)));\n+  format %{ \"evcvttsd2sisl $dst, $src\" %}\n+  ins_encode %{\n+    __ evcvttsd2sisl($dst$$Register, $src$$Address);\n+  %}\n+  ins_pipe(pipe_slow);\n+%}\n+\n+instruct convD2L_reg_reg(rRegL dst, regD src, rFlagsReg cr)\n+%{\n+  predicate(!VM_Version::supports_avx10_2());\n+  match(Set dst (ConvD2L src));\n+  effect(KILL cr);\n+  format %{ \"convert_d2l $dst, $src\"%}\n+  ins_encode %{\n+    __ convertF2I(T_LONG, T_DOUBLE, $dst$$Register, $src$$XMMRegister);\n+  %}\n+  ins_pipe(pipe_slow);\n+%}\n+\n+instruct convD2L_reg_reg_avx10(rRegL dst, regD src)\n+%{\n+  predicate(VM_Version::supports_avx10_2());\n+  match(Set dst (ConvD2L src));\n+  format %{ \"evcvttsd2sisq $dst, $src\" %}\n+  ins_encode %{\n+    __ evcvttsd2sisq($dst$$Register, $src$$XMMRegister);\n+  %}\n+  ins_pipe(pipe_slow);\n+%}\n+\n+instruct convD2L_reg_mem_avx10(rRegL dst, memory src)\n+%{\n+  predicate(VM_Version::supports_avx10_2());\n+  match(Set dst (ConvD2L (LoadD src)));\n+  format %{ \"evcvttsd2sisq $dst, $src\" %}\n+  ins_encode %{\n+    __ evcvttsd2sisq($dst$$Register, $src$$Address);\n+  %}\n+  ins_pipe(pipe_slow);\n+%}\n+\n+instruct round_double_reg(rRegL dst, regD src, rRegL rtmp, rcx_RegL rcx, rFlagsReg cr)\n+%{\n+  match(Set dst (RoundD src));\n+  effect(TEMP dst, TEMP rtmp, TEMP rcx, KILL cr);\n+  format %{ \"round_double $dst,$src \\t! using $rtmp and $rcx as TEMP\"%}\n+  ins_encode %{\n+    __ round_double($dst$$Register, $src$$XMMRegister, $rtmp$$Register, $rcx$$Register);\n+  %}\n+  ins_pipe(pipe_slow);\n+%}\n+\n+instruct round_float_reg(rRegI dst, regF src, rRegL rtmp, rcx_RegL rcx, rFlagsReg cr)\n+%{\n+  match(Set dst (RoundF src));\n+  effect(TEMP dst, TEMP rtmp, TEMP rcx, KILL cr);\n+  format %{ \"round_float $dst,$src\" %}\n+  ins_encode %{\n+    __ round_float($dst$$Register, $src$$XMMRegister, $rtmp$$Register, $rcx$$Register);\n+  %}\n+  ins_pipe(pipe_slow);\n+%}\n+\n+instruct convI2F_reg_reg(vlRegF dst, rRegI src)\n+%{\n+  predicate(!UseXmmI2F);\n+  match(Set dst (ConvI2F src));\n+\n+  format %{ \"cvtsi2ssl $dst, $src\\t# i2f\" %}\n+  ins_encode %{\n+    if (UseAVX > 0) {\n+      __ pxor($dst$$XMMRegister, $dst$$XMMRegister);\n+    }\n+    __ cvtsi2ssl ($dst$$XMMRegister, $src$$Register);\n+  %}\n+  ins_pipe(pipe_slow); \/\/ XXX\n+%}\n+\n+instruct convI2F_reg_mem(regF dst, memory src)\n+%{\n+  predicate(UseAVX == 0);\n+  match(Set dst (ConvI2F (LoadI src)));\n+\n+  format %{ \"cvtsi2ssl $dst, $src\\t# i2f\" %}\n+  ins_encode %{\n+    __ cvtsi2ssl ($dst$$XMMRegister, $src$$Address);\n+  %}\n+  ins_pipe(pipe_slow); \/\/ XXX\n+%}\n+\n+instruct convI2D_reg_reg(vlRegD dst, rRegI src)\n+%{\n+  predicate(!UseXmmI2D);\n+  match(Set dst (ConvI2D src));\n+\n+  format %{ \"cvtsi2sdl $dst, $src\\t# i2d\" %}\n+  ins_encode %{\n+    if (UseAVX > 0) {\n+      __ pxor($dst$$XMMRegister, $dst$$XMMRegister);\n+    }\n+    __ cvtsi2sdl ($dst$$XMMRegister, $src$$Register);\n+  %}\n+  ins_pipe(pipe_slow); \/\/ XXX\n+%}\n+\n+instruct convI2D_reg_mem(regD dst, memory src)\n+%{\n+  predicate(UseAVX == 0);\n+  match(Set dst (ConvI2D (LoadI src)));\n+\n+  format %{ \"cvtsi2sdl $dst, $src\\t# i2d\" %}\n+  ins_encode %{\n+    __ cvtsi2sdl ($dst$$XMMRegister, $src$$Address);\n+  %}\n+  ins_pipe(pipe_slow); \/\/ XXX\n+%}\n+\n+instruct convXI2F_reg(regF dst, rRegI src)\n+%{\n+  predicate(UseXmmI2F);\n+  match(Set dst (ConvI2F src));\n+\n+  format %{ \"movdl $dst, $src\\n\\t\"\n+            \"cvtdq2psl $dst, $dst\\t# i2f\" %}\n+  ins_encode %{\n+    __ movdl($dst$$XMMRegister, $src$$Register);\n+    __ cvtdq2ps($dst$$XMMRegister, $dst$$XMMRegister);\n+  %}\n+  ins_pipe(pipe_slow); \/\/ XXX\n+%}\n+\n+instruct convXI2D_reg(regD dst, rRegI src)\n+%{\n+  predicate(UseXmmI2D);\n+  match(Set dst (ConvI2D src));\n+\n+  format %{ \"movdl $dst, $src\\n\\t\"\n+            \"cvtdq2pdl $dst, $dst\\t# i2d\" %}\n+  ins_encode %{\n+    __ movdl($dst$$XMMRegister, $src$$Register);\n+    __ cvtdq2pd($dst$$XMMRegister, $dst$$XMMRegister);\n+  %}\n+  ins_pipe(pipe_slow); \/\/ XXX\n+%}\n+\n+instruct convL2F_reg_reg(vlRegF dst, rRegL src)\n+%{\n+  match(Set dst (ConvL2F src));\n+\n+  format %{ \"cvtsi2ssq $dst, $src\\t# l2f\" %}\n+  ins_encode %{\n+    if (UseAVX > 0) {\n+      __ pxor($dst$$XMMRegister, $dst$$XMMRegister);\n+    }\n+    __ cvtsi2ssq ($dst$$XMMRegister, $src$$Register);\n+  %}\n+  ins_pipe(pipe_slow); \/\/ XXX\n+%}\n+\n+instruct convL2F_reg_mem(regF dst, memory src)\n+%{\n+  predicate(UseAVX == 0);\n+  match(Set dst (ConvL2F (LoadL src)));\n+\n+  format %{ \"cvtsi2ssq $dst, $src\\t# l2f\" %}\n+  ins_encode %{\n+    __ cvtsi2ssq ($dst$$XMMRegister, $src$$Address);\n+  %}\n+  ins_pipe(pipe_slow); \/\/ XXX\n+%}\n+\n+instruct convL2D_reg_reg(vlRegD dst, rRegL src)\n+%{\n+  match(Set dst (ConvL2D src));\n+\n+  format %{ \"cvtsi2sdq $dst, $src\\t# l2d\" %}\n+  ins_encode %{\n+    if (UseAVX > 0) {\n+      __ pxor($dst$$XMMRegister, $dst$$XMMRegister);\n+    }\n+    __ cvtsi2sdq ($dst$$XMMRegister, $src$$Register);\n+  %}\n+  ins_pipe(pipe_slow); \/\/ XXX\n+%}\n+\n+instruct convL2D_reg_mem(regD dst, memory src)\n+%{\n+  predicate(UseAVX == 0);\n+  match(Set dst (ConvL2D (LoadL src)));\n+\n+  format %{ \"cvtsi2sdq $dst, $src\\t# l2d\" %}\n+  ins_encode %{\n+    __ cvtsi2sdq ($dst$$XMMRegister, $src$$Address);\n+  %}\n+  ins_pipe(pipe_slow); \/\/ XXX\n+%}\n+\n+instruct convI2L_reg_reg(rRegL dst, rRegI src)\n+%{\n+  match(Set dst (ConvI2L src));\n+\n+  ins_cost(125);\n+  format %{ \"movslq  $dst, $src\\t# i2l\" %}\n+  ins_encode %{\n+    __ movslq($dst$$Register, $src$$Register);\n+  %}\n+  ins_pipe(ialu_reg_reg);\n+%}\n+\n+\/\/ Zero-extend convert int to long\n+instruct convI2L_reg_reg_zex(rRegL dst, rRegI src, immL_32bits mask)\n+%{\n+  match(Set dst (AndL (ConvI2L src) mask));\n+\n+  format %{ \"movl    $dst, $src\\t# i2l zero-extend\\n\\t\" %}\n+  ins_encode %{\n+    if ($dst$$reg != $src$$reg) {\n+      __ movl($dst$$Register, $src$$Register);\n+    }\n+  %}\n+  ins_pipe(ialu_reg_reg);\n+%}\n+\n+\/\/ Zero-extend convert int to long\n+instruct convI2L_reg_mem_zex(rRegL dst, memory src, immL_32bits mask)\n+%{\n+  match(Set dst (AndL (ConvI2L (LoadI src)) mask));\n+\n+  format %{ \"movl    $dst, $src\\t# i2l zero-extend\\n\\t\" %}\n+  ins_encode %{\n+    __ movl($dst$$Register, $src$$Address);\n+  %}\n+  ins_pipe(ialu_reg_mem);\n+%}\n+\n+instruct zerox_long_reg_reg(rRegL dst, rRegL src, immL_32bits mask)\n+%{\n+  match(Set dst (AndL src mask));\n+\n+  format %{ \"movl    $dst, $src\\t# zero-extend long\" %}\n+  ins_encode %{\n+    __ movl($dst$$Register, $src$$Register);\n+  %}\n+  ins_pipe(ialu_reg_reg);\n+%}\n+\n+instruct convL2I_reg_reg(rRegI dst, rRegL src)\n+%{\n+  match(Set dst (ConvL2I src));\n+\n+  format %{ \"movl    $dst, $src\\t# l2i\" %}\n+  ins_encode %{\n+    __ movl($dst$$Register, $src$$Register);\n+  %}\n+  ins_pipe(ialu_reg_reg);\n+%}\n+\n+\n+instruct MoveF2I_stack_reg(rRegI dst, stackSlotF src) %{\n+  match(Set dst (MoveF2I src));\n+  effect(DEF dst, USE src);\n+\n+  ins_cost(125);\n+  format %{ \"movl    $dst, $src\\t# MoveF2I_stack_reg\" %}\n+  ins_encode %{\n+    __ movl($dst$$Register, Address(rsp, $src$$disp));\n+  %}\n+  ins_pipe(ialu_reg_mem);\n+%}\n+\n+instruct MoveI2F_stack_reg(regF dst, stackSlotI src) %{\n+  match(Set dst (MoveI2F src));\n+  effect(DEF dst, USE src);\n+\n+  ins_cost(125);\n+  format %{ \"movss   $dst, $src\\t# MoveI2F_stack_reg\" %}\n+  ins_encode %{\n+    __ movflt($dst$$XMMRegister, Address(rsp, $src$$disp));\n+  %}\n+  ins_pipe(pipe_slow);\n+%}\n+\n+instruct MoveD2L_stack_reg(rRegL dst, stackSlotD src) %{\n+  match(Set dst (MoveD2L src));\n+  effect(DEF dst, USE src);\n+\n+  ins_cost(125);\n+  format %{ \"movq    $dst, $src\\t# MoveD2L_stack_reg\" %}\n+  ins_encode %{\n+    __ movq($dst$$Register, Address(rsp, $src$$disp));\n+  %}\n+  ins_pipe(ialu_reg_mem);\n+%}\n+\n+instruct MoveL2D_stack_reg_partial(regD dst, stackSlotL src) %{\n+  predicate(!UseXmmLoadAndClearUpper);\n+  match(Set dst (MoveL2D src));\n+  effect(DEF dst, USE src);\n+\n+  ins_cost(125);\n+  format %{ \"movlpd  $dst, $src\\t# MoveL2D_stack_reg\" %}\n+  ins_encode %{\n+    __ movdbl($dst$$XMMRegister, Address(rsp, $src$$disp));\n+  %}\n+  ins_pipe(pipe_slow);\n+%}\n+\n+instruct MoveL2D_stack_reg(regD dst, stackSlotL src) %{\n+  predicate(UseXmmLoadAndClearUpper);\n+  match(Set dst (MoveL2D src));\n+  effect(DEF dst, USE src);\n+\n+  ins_cost(125);\n+  format %{ \"movsd   $dst, $src\\t# MoveL2D_stack_reg\" %}\n+  ins_encode %{\n+    __ movdbl($dst$$XMMRegister, Address(rsp, $src$$disp));\n+  %}\n+  ins_pipe(pipe_slow);\n+%}\n+\n+\n+instruct MoveF2I_reg_stack(stackSlotI dst, regF src) %{\n+  match(Set dst (MoveF2I src));\n+  effect(DEF dst, USE src);\n+\n+  ins_cost(95); \/\/ XXX\n+  format %{ \"movss   $dst, $src\\t# MoveF2I_reg_stack\" %}\n+  ins_encode %{\n+    __ movflt(Address(rsp, $dst$$disp), $src$$XMMRegister);\n+  %}\n+  ins_pipe(pipe_slow);\n+%}\n+\n+instruct MoveI2F_reg_stack(stackSlotF dst, rRegI src) %{\n+  match(Set dst (MoveI2F src));\n+  effect(DEF dst, USE src);\n+\n+  ins_cost(100);\n+  format %{ \"movl    $dst, $src\\t# MoveI2F_reg_stack\" %}\n+  ins_encode %{\n+    __ movl(Address(rsp, $dst$$disp), $src$$Register);\n+  %}\n+  ins_pipe( ialu_mem_reg );\n+%}\n+\n+instruct MoveD2L_reg_stack(stackSlotL dst, regD src) %{\n+  match(Set dst (MoveD2L src));\n+  effect(DEF dst, USE src);\n+\n+  ins_cost(95); \/\/ XXX\n+  format %{ \"movsd   $dst, $src\\t# MoveL2D_reg_stack\" %}\n+  ins_encode %{\n+    __ movdbl(Address(rsp, $dst$$disp), $src$$XMMRegister);\n+  %}\n+  ins_pipe(pipe_slow);\n+%}\n+\n+instruct MoveL2D_reg_stack(stackSlotD dst, rRegL src) %{\n+  match(Set dst (MoveL2D src));\n+  effect(DEF dst, USE src);\n+\n+  ins_cost(100);\n+  format %{ \"movq    $dst, $src\\t# MoveL2D_reg_stack\" %}\n+  ins_encode %{\n+    __ movq(Address(rsp, $dst$$disp), $src$$Register);\n+  %}\n+  ins_pipe(ialu_mem_reg);\n+%}\n+\n+instruct MoveF2I_reg_reg(rRegI dst, regF src) %{\n+  match(Set dst (MoveF2I src));\n+  effect(DEF dst, USE src);\n+  ins_cost(85);\n+  format %{ \"movd    $dst,$src\\t# MoveF2I\" %}\n+  ins_encode %{\n+    __ movdl($dst$$Register, $src$$XMMRegister);\n+  %}\n+  ins_pipe( pipe_slow );\n+%}\n+\n+instruct MoveD2L_reg_reg(rRegL dst, regD src) %{\n+  match(Set dst (MoveD2L src));\n+  effect(DEF dst, USE src);\n+  ins_cost(85);\n+  format %{ \"movd    $dst,$src\\t# MoveD2L\" %}\n+  ins_encode %{\n+    __ movdq($dst$$Register, $src$$XMMRegister);\n+  %}\n+  ins_pipe( pipe_slow );\n+%}\n+\n+instruct MoveI2F_reg_reg(regF dst, rRegI src) %{\n+  match(Set dst (MoveI2F src));\n+  effect(DEF dst, USE src);\n+  ins_cost(100);\n+  format %{ \"movd    $dst,$src\\t# MoveI2F\" %}\n+  ins_encode %{\n+    __ movdl($dst$$XMMRegister, $src$$Register);\n+  %}\n+  ins_pipe( pipe_slow );\n+%}\n+\n+instruct MoveL2D_reg_reg(regD dst, rRegL src) %{\n+  match(Set dst (MoveL2D src));\n+  effect(DEF dst, USE src);\n+  ins_cost(100);\n+  format %{ \"movd    $dst,$src\\t# MoveL2D\" %}\n+  ins_encode %{\n+     __ movdq($dst$$XMMRegister, $src$$Register);\n+  %}\n+  ins_pipe( pipe_slow );\n+%}\n+\n+\/\/ Fast clearing of an array\n+\/\/ Small non-constant lenght ClearArray for non-AVX512 targets.\n+instruct rep_stos(rcx_RegL cnt, rdi_RegP base, regD tmp, rax_RegI zero,\n+                  Universe dummy, rFlagsReg cr)\n+%{\n+  predicate(!((ClearArrayNode*)n)->is_large() && (UseAVX <= 2));\n+  match(Set dummy (ClearArray cnt base));\n+  effect(USE_KILL cnt, USE_KILL base, TEMP tmp, KILL zero, KILL cr);\n+\n+  format %{ $$template\n+    $$emit$$\"xorq    rax, rax\\t# ClearArray:\\n\\t\"\n+    $$emit$$\"cmp     InitArrayShortSize,rcx\\n\\t\"\n+    $$emit$$\"jg      LARGE\\n\\t\"\n+    $$emit$$\"dec     rcx\\n\\t\"\n+    $$emit$$\"js      DONE\\t# Zero length\\n\\t\"\n+    $$emit$$\"mov     rax,(rdi,rcx,8)\\t# LOOP\\n\\t\"\n+    $$emit$$\"dec     rcx\\n\\t\"\n+    $$emit$$\"jge     LOOP\\n\\t\"\n+    $$emit$$\"jmp     DONE\\n\\t\"\n+    $$emit$$\"# LARGE:\\n\\t\"\n+    if (UseFastStosb) {\n+       $$emit$$\"shlq    rcx,3\\t# Convert doublewords to bytes\\n\\t\"\n+       $$emit$$\"rep     stosb\\t# Store rax to *rdi++ while rcx--\\n\\t\"\n+    } else if (UseXMMForObjInit) {\n+       $$emit$$\"mov     rdi,rax\\n\\t\"\n+       $$emit$$\"vpxor   ymm0,ymm0,ymm0\\n\\t\"\n+       $$emit$$\"jmpq    L_zero_64_bytes\\n\\t\"\n+       $$emit$$\"# L_loop:\\t# 64-byte LOOP\\n\\t\"\n+       $$emit$$\"vmovdqu ymm0,(rax)\\n\\t\"\n+       $$emit$$\"vmovdqu ymm0,0x20(rax)\\n\\t\"\n+       $$emit$$\"add     0x40,rax\\n\\t\"\n+       $$emit$$\"# L_zero_64_bytes:\\n\\t\"\n+       $$emit$$\"sub     0x8,rcx\\n\\t\"\n+       $$emit$$\"jge     L_loop\\n\\t\"\n+       $$emit$$\"add     0x4,rcx\\n\\t\"\n+       $$emit$$\"jl      L_tail\\n\\t\"\n+       $$emit$$\"vmovdqu ymm0,(rax)\\n\\t\"\n+       $$emit$$\"add     0x20,rax\\n\\t\"\n+       $$emit$$\"sub     0x4,rcx\\n\\t\"\n+       $$emit$$\"# L_tail:\\t# Clearing tail bytes\\n\\t\"\n+       $$emit$$\"add     0x4,rcx\\n\\t\"\n+       $$emit$$\"jle     L_end\\n\\t\"\n+       $$emit$$\"dec     rcx\\n\\t\"\n+       $$emit$$\"# L_sloop:\\t# 8-byte short loop\\n\\t\"\n+       $$emit$$\"vmovq   xmm0,(rax)\\n\\t\"\n+       $$emit$$\"add     0x8,rax\\n\\t\"\n+       $$emit$$\"dec     rcx\\n\\t\"\n+       $$emit$$\"jge     L_sloop\\n\\t\"\n+       $$emit$$\"# L_end:\\n\\t\"\n+    } else {\n+       $$emit$$\"rep     stosq\\t# Store rax to *rdi++ while rcx--\\n\\t\"\n+    }\n+    $$emit$$\"# DONE\"\n+  %}\n+  ins_encode %{\n+    __ clear_mem($base$$Register, $cnt$$Register, $zero$$Register,\n+                 $tmp$$XMMRegister, false, knoreg);\n+  %}\n+  ins_pipe(pipe_slow);\n+%}\n+\n+\/\/ Small non-constant length ClearArray for AVX512 targets.\n+instruct rep_stos_evex(rcx_RegL cnt, rdi_RegP base, legRegD tmp, kReg ktmp, rax_RegI zero,\n+                       Universe dummy, rFlagsReg cr)\n+%{\n+  predicate(!((ClearArrayNode*)n)->is_large() && (UseAVX > 2));\n+  match(Set dummy (ClearArray cnt base));\n+  ins_cost(125);\n+  effect(USE_KILL cnt, USE_KILL base, TEMP tmp, TEMP ktmp, KILL zero, KILL cr);\n+\n+  format %{ $$template\n+    $$emit$$\"xorq    rax, rax\\t# ClearArray:\\n\\t\"\n+    $$emit$$\"cmp     InitArrayShortSize,rcx\\n\\t\"\n+    $$emit$$\"jg      LARGE\\n\\t\"\n+    $$emit$$\"dec     rcx\\n\\t\"\n+    $$emit$$\"js      DONE\\t# Zero length\\n\\t\"\n+    $$emit$$\"mov     rax,(rdi,rcx,8)\\t# LOOP\\n\\t\"\n+    $$emit$$\"dec     rcx\\n\\t\"\n+    $$emit$$\"jge     LOOP\\n\\t\"\n+    $$emit$$\"jmp     DONE\\n\\t\"\n+    $$emit$$\"# LARGE:\\n\\t\"\n+    if (UseFastStosb) {\n+       $$emit$$\"shlq    rcx,3\\t# Convert doublewords to bytes\\n\\t\"\n+       $$emit$$\"rep     stosb\\t# Store rax to *rdi++ while rcx--\\n\\t\"\n+    } else if (UseXMMForObjInit) {\n+       $$emit$$\"mov     rdi,rax\\n\\t\"\n+       $$emit$$\"vpxor   ymm0,ymm0,ymm0\\n\\t\"\n+       $$emit$$\"jmpq    L_zero_64_bytes\\n\\t\"\n+       $$emit$$\"# L_loop:\\t# 64-byte LOOP\\n\\t\"\n+       $$emit$$\"vmovdqu ymm0,(rax)\\n\\t\"\n+       $$emit$$\"vmovdqu ymm0,0x20(rax)\\n\\t\"\n+       $$emit$$\"add     0x40,rax\\n\\t\"\n+       $$emit$$\"# L_zero_64_bytes:\\n\\t\"\n+       $$emit$$\"sub     0x8,rcx\\n\\t\"\n+       $$emit$$\"jge     L_loop\\n\\t\"\n+       $$emit$$\"add     0x4,rcx\\n\\t\"\n+       $$emit$$\"jl      L_tail\\n\\t\"\n+       $$emit$$\"vmovdqu ymm0,(rax)\\n\\t\"\n+       $$emit$$\"add     0x20,rax\\n\\t\"\n+       $$emit$$\"sub     0x4,rcx\\n\\t\"\n+       $$emit$$\"# L_tail:\\t# Clearing tail bytes\\n\\t\"\n+       $$emit$$\"add     0x4,rcx\\n\\t\"\n+       $$emit$$\"jle     L_end\\n\\t\"\n+       $$emit$$\"dec     rcx\\n\\t\"\n+       $$emit$$\"# L_sloop:\\t# 8-byte short loop\\n\\t\"\n+       $$emit$$\"vmovq   xmm0,(rax)\\n\\t\"\n+       $$emit$$\"add     0x8,rax\\n\\t\"\n+       $$emit$$\"dec     rcx\\n\\t\"\n+       $$emit$$\"jge     L_sloop\\n\\t\"\n+       $$emit$$\"# L_end:\\n\\t\"\n+    } else {\n+       $$emit$$\"rep     stosq\\t# Store rax to *rdi++ while rcx--\\n\\t\"\n+    }\n+    $$emit$$\"# DONE\"\n+  %}\n+  ins_encode %{\n+    __ clear_mem($base$$Register, $cnt$$Register, $zero$$Register,\n+                 $tmp$$XMMRegister, false, $ktmp$$KRegister);\n+  %}\n+  ins_pipe(pipe_slow);\n+%}\n+\n+\/\/ Large non-constant length ClearArray for non-AVX512 targets.\n+instruct rep_stos_large(rcx_RegL cnt, rdi_RegP base, regD tmp, rax_RegI zero,\n+                        Universe dummy, rFlagsReg cr)\n+%{\n+  predicate((UseAVX <=2) && ((ClearArrayNode*)n)->is_large());\n+  match(Set dummy (ClearArray cnt base));\n+  effect(USE_KILL cnt, USE_KILL base, TEMP tmp, KILL zero, KILL cr);\n+\n+  format %{ $$template\n+    if (UseFastStosb) {\n+       $$emit$$\"xorq    rax, rax\\t# ClearArray:\\n\\t\"\n+       $$emit$$\"shlq    rcx,3\\t# Convert doublewords to bytes\\n\\t\"\n+       $$emit$$\"rep     stosb\\t# Store rax to *rdi++ while rcx--\"\n+    } else if (UseXMMForObjInit) {\n+       $$emit$$\"mov     rdi,rax\\t# ClearArray:\\n\\t\"\n+       $$emit$$\"vpxor   ymm0,ymm0,ymm0\\n\\t\"\n+       $$emit$$\"jmpq    L_zero_64_bytes\\n\\t\"\n+       $$emit$$\"# L_loop:\\t# 64-byte LOOP\\n\\t\"\n+       $$emit$$\"vmovdqu ymm0,(rax)\\n\\t\"\n+       $$emit$$\"vmovdqu ymm0,0x20(rax)\\n\\t\"\n+       $$emit$$\"add     0x40,rax\\n\\t\"\n+       $$emit$$\"# L_zero_64_bytes:\\n\\t\"\n+       $$emit$$\"sub     0x8,rcx\\n\\t\"\n+       $$emit$$\"jge     L_loop\\n\\t\"\n+       $$emit$$\"add     0x4,rcx\\n\\t\"\n+       $$emit$$\"jl      L_tail\\n\\t\"\n+       $$emit$$\"vmovdqu ymm0,(rax)\\n\\t\"\n+       $$emit$$\"add     0x20,rax\\n\\t\"\n+       $$emit$$\"sub     0x4,rcx\\n\\t\"\n+       $$emit$$\"# L_tail:\\t# Clearing tail bytes\\n\\t\"\n+       $$emit$$\"add     0x4,rcx\\n\\t\"\n+       $$emit$$\"jle     L_end\\n\\t\"\n+       $$emit$$\"dec     rcx\\n\\t\"\n+       $$emit$$\"# L_sloop:\\t# 8-byte short loop\\n\\t\"\n+       $$emit$$\"vmovq   xmm0,(rax)\\n\\t\"\n+       $$emit$$\"add     0x8,rax\\n\\t\"\n+       $$emit$$\"dec     rcx\\n\\t\"\n+       $$emit$$\"jge     L_sloop\\n\\t\"\n+       $$emit$$\"# L_end:\\n\\t\"\n+    } else {\n+       $$emit$$\"xorq    rax, rax\\t# ClearArray:\\n\\t\"\n+       $$emit$$\"rep     stosq\\t# Store rax to *rdi++ while rcx--\"\n+    }\n+  %}\n+  ins_encode %{\n+    __ clear_mem($base$$Register, $cnt$$Register, $zero$$Register,\n+                 $tmp$$XMMRegister, true, knoreg);\n+  %}\n+  ins_pipe(pipe_slow);\n+%}\n+\n+\/\/ Large non-constant length ClearArray for AVX512 targets.\n+instruct rep_stos_large_evex(rcx_RegL cnt, rdi_RegP base, legRegD tmp, kReg ktmp, rax_RegI zero,\n+                             Universe dummy, rFlagsReg cr)\n+%{\n+  predicate((UseAVX > 2) && ((ClearArrayNode*)n)->is_large());\n+  match(Set dummy (ClearArray cnt base));\n+  effect(USE_KILL cnt, USE_KILL base, TEMP tmp, TEMP ktmp, KILL zero, KILL cr);\n+\n+  format %{ $$template\n+    if (UseFastStosb) {\n+       $$emit$$\"xorq    rax, rax\\t# ClearArray:\\n\\t\"\n+       $$emit$$\"shlq    rcx,3\\t# Convert doublewords to bytes\\n\\t\"\n+       $$emit$$\"rep     stosb\\t# Store rax to *rdi++ while rcx--\"\n+    } else if (UseXMMForObjInit) {\n+       $$emit$$\"mov     rdi,rax\\t# ClearArray:\\n\\t\"\n+       $$emit$$\"vpxor   ymm0,ymm0,ymm0\\n\\t\"\n+       $$emit$$\"jmpq    L_zero_64_bytes\\n\\t\"\n+       $$emit$$\"# L_loop:\\t# 64-byte LOOP\\n\\t\"\n+       $$emit$$\"vmovdqu ymm0,(rax)\\n\\t\"\n+       $$emit$$\"vmovdqu ymm0,0x20(rax)\\n\\t\"\n+       $$emit$$\"add     0x40,rax\\n\\t\"\n+       $$emit$$\"# L_zero_64_bytes:\\n\\t\"\n+       $$emit$$\"sub     0x8,rcx\\n\\t\"\n+       $$emit$$\"jge     L_loop\\n\\t\"\n+       $$emit$$\"add     0x4,rcx\\n\\t\"\n+       $$emit$$\"jl      L_tail\\n\\t\"\n+       $$emit$$\"vmovdqu ymm0,(rax)\\n\\t\"\n+       $$emit$$\"add     0x20,rax\\n\\t\"\n+       $$emit$$\"sub     0x4,rcx\\n\\t\"\n+       $$emit$$\"# L_tail:\\t# Clearing tail bytes\\n\\t\"\n+       $$emit$$\"add     0x4,rcx\\n\\t\"\n+       $$emit$$\"jle     L_end\\n\\t\"\n+       $$emit$$\"dec     rcx\\n\\t\"\n+       $$emit$$\"# L_sloop:\\t# 8-byte short loop\\n\\t\"\n+       $$emit$$\"vmovq   xmm0,(rax)\\n\\t\"\n+       $$emit$$\"add     0x8,rax\\n\\t\"\n+       $$emit$$\"dec     rcx\\n\\t\"\n+       $$emit$$\"jge     L_sloop\\n\\t\"\n+       $$emit$$\"# L_end:\\n\\t\"\n+    } else {\n+       $$emit$$\"xorq    rax, rax\\t# ClearArray:\\n\\t\"\n+       $$emit$$\"rep     stosq\\t# Store rax to *rdi++ while rcx--\"\n+    }\n+  %}\n+  ins_encode %{\n+    __ clear_mem($base$$Register, $cnt$$Register, $zero$$Register,\n+                 $tmp$$XMMRegister, true, $ktmp$$KRegister);\n+  %}\n+  ins_pipe(pipe_slow);\n+%}\n+\n+\/\/ Small constant length ClearArray for AVX512 targets.\n+instruct rep_stos_im(immL cnt, rRegP base, regD tmp, rRegI zero, kReg ktmp, Universe dummy, rFlagsReg cr)\n+%{\n+  predicate(!((ClearArrayNode*)n)->is_large() && (MaxVectorSize >= 32) && VM_Version::supports_avx512vl());\n+  match(Set dummy (ClearArray cnt base));\n+  ins_cost(100);\n+  effect(TEMP tmp, TEMP zero, TEMP ktmp, KILL cr);\n+  format %{ \"clear_mem_imm $base , $cnt  \\n\\t\" %}\n+  ins_encode %{\n+   __ clear_mem($base$$Register, $cnt$$constant, $zero$$Register, $tmp$$XMMRegister, $ktmp$$KRegister);\n+  %}\n+  ins_pipe(pipe_slow);\n+%}\n+\n+instruct string_compareL(rdi_RegP str1, rcx_RegI cnt1, rsi_RegP str2, rdx_RegI cnt2,\n+                         rax_RegI result, legRegD tmp1, rFlagsReg cr)\n+%{\n+  predicate(!VM_Version::supports_avx512vlbw() && ((StrCompNode*)n)->encoding() == StrIntrinsicNode::LL);\n+  match(Set result (StrComp (Binary str1 cnt1) (Binary str2 cnt2)));\n+  effect(TEMP tmp1, USE_KILL str1, USE_KILL str2, USE_KILL cnt1, USE_KILL cnt2, KILL cr);\n+\n+  format %{ \"String Compare byte[] $str1,$cnt1,$str2,$cnt2 -> $result   \/\/ KILL $tmp1\" %}\n+  ins_encode %{\n+    __ string_compare($str1$$Register, $str2$$Register,\n+                      $cnt1$$Register, $cnt2$$Register, $result$$Register,\n+                      $tmp1$$XMMRegister, StrIntrinsicNode::LL, knoreg);\n+  %}\n+  ins_pipe( pipe_slow );\n+%}\n+\n+instruct string_compareL_evex(rdi_RegP str1, rcx_RegI cnt1, rsi_RegP str2, rdx_RegI cnt2,\n+                              rax_RegI result, legRegD tmp1, kReg ktmp, rFlagsReg cr)\n+%{\n+  predicate(VM_Version::supports_avx512vlbw() && ((StrCompNode*)n)->encoding() == StrIntrinsicNode::LL);\n+  match(Set result (StrComp (Binary str1 cnt1) (Binary str2 cnt2)));\n+  effect(TEMP tmp1, TEMP ktmp, USE_KILL str1, USE_KILL str2, USE_KILL cnt1, USE_KILL cnt2, KILL cr);\n+\n+  format %{ \"String Compare byte[] $str1,$cnt1,$str2,$cnt2 -> $result   \/\/ KILL $tmp1\" %}\n+  ins_encode %{\n+    __ string_compare($str1$$Register, $str2$$Register,\n+                      $cnt1$$Register, $cnt2$$Register, $result$$Register,\n+                      $tmp1$$XMMRegister, StrIntrinsicNode::LL, $ktmp$$KRegister);\n+  %}\n+  ins_pipe( pipe_slow );\n+%}\n+\n+instruct string_compareU(rdi_RegP str1, rcx_RegI cnt1, rsi_RegP str2, rdx_RegI cnt2,\n+                         rax_RegI result, legRegD tmp1, rFlagsReg cr)\n+%{\n+  predicate(!VM_Version::supports_avx512vlbw() && ((StrCompNode*)n)->encoding() == StrIntrinsicNode::UU);\n+  match(Set result (StrComp (Binary str1 cnt1) (Binary str2 cnt2)));\n+  effect(TEMP tmp1, USE_KILL str1, USE_KILL str2, USE_KILL cnt1, USE_KILL cnt2, KILL cr);\n+\n+  format %{ \"String Compare char[] $str1,$cnt1,$str2,$cnt2 -> $result   \/\/ KILL $tmp1\" %}\n+  ins_encode %{\n+    __ string_compare($str1$$Register, $str2$$Register,\n+                      $cnt1$$Register, $cnt2$$Register, $result$$Register,\n+                      $tmp1$$XMMRegister, StrIntrinsicNode::UU, knoreg);\n+  %}\n+  ins_pipe( pipe_slow );\n+%}\n+\n+instruct string_compareU_evex(rdi_RegP str1, rcx_RegI cnt1, rsi_RegP str2, rdx_RegI cnt2,\n+                              rax_RegI result, legRegD tmp1, kReg ktmp, rFlagsReg cr)\n+%{\n+  predicate(VM_Version::supports_avx512vlbw() && ((StrCompNode*)n)->encoding() == StrIntrinsicNode::UU);\n+  match(Set result (StrComp (Binary str1 cnt1) (Binary str2 cnt2)));\n+  effect(TEMP tmp1, TEMP ktmp, USE_KILL str1, USE_KILL str2, USE_KILL cnt1, USE_KILL cnt2, KILL cr);\n+\n+  format %{ \"String Compare char[] $str1,$cnt1,$str2,$cnt2 -> $result   \/\/ KILL $tmp1\" %}\n+  ins_encode %{\n+    __ string_compare($str1$$Register, $str2$$Register,\n+                      $cnt1$$Register, $cnt2$$Register, $result$$Register,\n+                      $tmp1$$XMMRegister, StrIntrinsicNode::UU, $ktmp$$KRegister);\n+  %}\n+  ins_pipe( pipe_slow );\n+%}\n+\n+instruct string_compareLU(rdi_RegP str1, rcx_RegI cnt1, rsi_RegP str2, rdx_RegI cnt2,\n+                          rax_RegI result, legRegD tmp1, rFlagsReg cr)\n+%{\n+  predicate(!VM_Version::supports_avx512vlbw() && ((StrCompNode*)n)->encoding() == StrIntrinsicNode::LU);\n+  match(Set result (StrComp (Binary str1 cnt1) (Binary str2 cnt2)));\n+  effect(TEMP tmp1, USE_KILL str1, USE_KILL str2, USE_KILL cnt1, USE_KILL cnt2, KILL cr);\n+\n+  format %{ \"String Compare byte[] $str1,$cnt1,$str2,$cnt2 -> $result   \/\/ KILL $tmp1\" %}\n+  ins_encode %{\n+    __ string_compare($str1$$Register, $str2$$Register,\n+                      $cnt1$$Register, $cnt2$$Register, $result$$Register,\n+                      $tmp1$$XMMRegister, StrIntrinsicNode::LU, knoreg);\n+  %}\n+  ins_pipe( pipe_slow );\n+%}\n+\n+instruct string_compareLU_evex(rdi_RegP str1, rcx_RegI cnt1, rsi_RegP str2, rdx_RegI cnt2,\n+                               rax_RegI result, legRegD tmp1, kReg ktmp, rFlagsReg cr)\n+%{\n+  predicate(VM_Version::supports_avx512vlbw() && ((StrCompNode*)n)->encoding() == StrIntrinsicNode::LU);\n+  match(Set result (StrComp (Binary str1 cnt1) (Binary str2 cnt2)));\n+  effect(TEMP tmp1, TEMP ktmp, USE_KILL str1, USE_KILL str2, USE_KILL cnt1, USE_KILL cnt2, KILL cr);\n+\n+  format %{ \"String Compare byte[] $str1,$cnt1,$str2,$cnt2 -> $result   \/\/ KILL $tmp1\" %}\n+  ins_encode %{\n+    __ string_compare($str1$$Register, $str2$$Register,\n+                      $cnt1$$Register, $cnt2$$Register, $result$$Register,\n+                      $tmp1$$XMMRegister, StrIntrinsicNode::LU, $ktmp$$KRegister);\n+  %}\n+  ins_pipe( pipe_slow );\n+%}\n+\n+instruct string_compareUL(rsi_RegP str1, rdx_RegI cnt1, rdi_RegP str2, rcx_RegI cnt2,\n+                          rax_RegI result, legRegD tmp1, rFlagsReg cr)\n+%{\n+  predicate(!VM_Version::supports_avx512vlbw() && ((StrCompNode*)n)->encoding() == StrIntrinsicNode::UL);\n+  match(Set result (StrComp (Binary str1 cnt1) (Binary str2 cnt2)));\n+  effect(TEMP tmp1, USE_KILL str1, USE_KILL str2, USE_KILL cnt1, USE_KILL cnt2, KILL cr);\n+\n+  format %{ \"String Compare byte[] $str1,$cnt1,$str2,$cnt2 -> $result   \/\/ KILL $tmp1\" %}\n+  ins_encode %{\n+    __ string_compare($str2$$Register, $str1$$Register,\n+                      $cnt2$$Register, $cnt1$$Register, $result$$Register,\n+                      $tmp1$$XMMRegister, StrIntrinsicNode::UL, knoreg);\n+  %}\n+  ins_pipe( pipe_slow );\n+%}\n+\n+instruct string_compareUL_evex(rsi_RegP str1, rdx_RegI cnt1, rdi_RegP str2, rcx_RegI cnt2,\n+                               rax_RegI result, legRegD tmp1, kReg ktmp, rFlagsReg cr)\n+%{\n+  predicate(VM_Version::supports_avx512vlbw() && ((StrCompNode*)n)->encoding() == StrIntrinsicNode::UL);\n+  match(Set result (StrComp (Binary str1 cnt1) (Binary str2 cnt2)));\n+  effect(TEMP tmp1, TEMP ktmp, USE_KILL str1, USE_KILL str2, USE_KILL cnt1, USE_KILL cnt2, KILL cr);\n+\n+  format %{ \"String Compare byte[] $str1,$cnt1,$str2,$cnt2 -> $result   \/\/ KILL $tmp1\" %}\n+  ins_encode %{\n+    __ string_compare($str2$$Register, $str1$$Register,\n+                      $cnt2$$Register, $cnt1$$Register, $result$$Register,\n+                      $tmp1$$XMMRegister, StrIntrinsicNode::UL, $ktmp$$KRegister);\n+  %}\n+  ins_pipe( pipe_slow );\n+%}\n+\n+\/\/ fast search of substring with known size.\n+instruct string_indexof_conL(rdi_RegP str1, rdx_RegI cnt1, rsi_RegP str2, immI int_cnt2,\n+                             rbx_RegI result, legRegD tmp_vec, rax_RegI cnt2, rcx_RegI tmp, rFlagsReg cr)\n+%{\n+  predicate(UseSSE42Intrinsics && (((StrIndexOfNode*)n)->encoding() == StrIntrinsicNode::LL));\n+  match(Set result (StrIndexOf (Binary str1 cnt1) (Binary str2 int_cnt2)));\n+  effect(TEMP tmp_vec, USE_KILL str1, USE_KILL str2, USE_KILL cnt1, KILL cnt2, KILL tmp, KILL cr);\n+\n+  format %{ \"String IndexOf byte[] $str1,$cnt1,$str2,$int_cnt2 -> $result   \/\/ KILL $tmp_vec, $cnt1, $cnt2, $tmp\" %}\n+  ins_encode %{\n+    int icnt2 = (int)$int_cnt2$$constant;\n+    if (icnt2 >= 16) {\n+      \/\/ IndexOf for constant substrings with size >= 16 elements\n+      \/\/ which don't need to be loaded through stack.\n+      __ string_indexofC8($str1$$Register, $str2$$Register,\n+                          $cnt1$$Register, $cnt2$$Register,\n+                          icnt2, $result$$Register,\n+                          $tmp_vec$$XMMRegister, $tmp$$Register, StrIntrinsicNode::LL);\n+    } else {\n+      \/\/ Small strings are loaded through stack if they cross page boundary.\n+      __ string_indexof($str1$$Register, $str2$$Register,\n+                        $cnt1$$Register, $cnt2$$Register,\n+                        icnt2, $result$$Register,\n+                        $tmp_vec$$XMMRegister, $tmp$$Register, StrIntrinsicNode::LL);\n+    }\n+  %}\n+  ins_pipe( pipe_slow );\n+%}\n+\n+\/\/ fast search of substring with known size.\n+instruct string_indexof_conU(rdi_RegP str1, rdx_RegI cnt1, rsi_RegP str2, immI int_cnt2,\n+                             rbx_RegI result, legRegD tmp_vec, rax_RegI cnt2, rcx_RegI tmp, rFlagsReg cr)\n+%{\n+  predicate(UseSSE42Intrinsics && (((StrIndexOfNode*)n)->encoding() == StrIntrinsicNode::UU));\n+  match(Set result (StrIndexOf (Binary str1 cnt1) (Binary str2 int_cnt2)));\n+  effect(TEMP tmp_vec, USE_KILL str1, USE_KILL str2, USE_KILL cnt1, KILL cnt2, KILL tmp, KILL cr);\n+\n+  format %{ \"String IndexOf char[] $str1,$cnt1,$str2,$int_cnt2 -> $result   \/\/ KILL $tmp_vec, $cnt1, $cnt2, $tmp\" %}\n+  ins_encode %{\n+    int icnt2 = (int)$int_cnt2$$constant;\n+    if (icnt2 >= 8) {\n+      \/\/ IndexOf for constant substrings with size >= 8 elements\n+      \/\/ which don't need to be loaded through stack.\n+      __ string_indexofC8($str1$$Register, $str2$$Register,\n+                          $cnt1$$Register, $cnt2$$Register,\n+                          icnt2, $result$$Register,\n+                          $tmp_vec$$XMMRegister, $tmp$$Register, StrIntrinsicNode::UU);\n+    } else {\n+      \/\/ Small strings are loaded through stack if they cross page boundary.\n+      __ string_indexof($str1$$Register, $str2$$Register,\n+                        $cnt1$$Register, $cnt2$$Register,\n+                        icnt2, $result$$Register,\n+                        $tmp_vec$$XMMRegister, $tmp$$Register, StrIntrinsicNode::UU);\n+    }\n+  %}\n+  ins_pipe( pipe_slow );\n+%}\n+\n+\/\/ fast search of substring with known size.\n+instruct string_indexof_conUL(rdi_RegP str1, rdx_RegI cnt1, rsi_RegP str2, immI int_cnt2,\n+                              rbx_RegI result, legRegD tmp_vec, rax_RegI cnt2, rcx_RegI tmp, rFlagsReg cr)\n+%{\n+  predicate(UseSSE42Intrinsics && (((StrIndexOfNode*)n)->encoding() == StrIntrinsicNode::UL));\n+  match(Set result (StrIndexOf (Binary str1 cnt1) (Binary str2 int_cnt2)));\n+  effect(TEMP tmp_vec, USE_KILL str1, USE_KILL str2, USE_KILL cnt1, KILL cnt2, KILL tmp, KILL cr);\n+\n+  format %{ \"String IndexOf char[] $str1,$cnt1,$str2,$int_cnt2 -> $result   \/\/ KILL $tmp_vec, $cnt1, $cnt2, $tmp\" %}\n+  ins_encode %{\n+    int icnt2 = (int)$int_cnt2$$constant;\n+    if (icnt2 >= 8) {\n+      \/\/ IndexOf for constant substrings with size >= 8 elements\n+      \/\/ which don't need to be loaded through stack.\n+      __ string_indexofC8($str1$$Register, $str2$$Register,\n+                          $cnt1$$Register, $cnt2$$Register,\n+                          icnt2, $result$$Register,\n+                          $tmp_vec$$XMMRegister, $tmp$$Register, StrIntrinsicNode::UL);\n+    } else {\n+      \/\/ Small strings are loaded through stack if they cross page boundary.\n+      __ string_indexof($str1$$Register, $str2$$Register,\n+                        $cnt1$$Register, $cnt2$$Register,\n+                        icnt2, $result$$Register,\n+                        $tmp_vec$$XMMRegister, $tmp$$Register, StrIntrinsicNode::UL);\n+    }\n+  %}\n+  ins_pipe( pipe_slow );\n+%}\n+\n+instruct string_indexofL(rdi_RegP str1, rdx_RegI cnt1, rsi_RegP str2, rax_RegI cnt2,\n+                         rbx_RegI result, legRegD tmp_vec, rcx_RegI tmp, rFlagsReg cr)\n+%{\n+  predicate(UseSSE42Intrinsics && (((StrIndexOfNode*)n)->encoding() == StrIntrinsicNode::LL));\n+  match(Set result (StrIndexOf (Binary str1 cnt1) (Binary str2 cnt2)));\n+  effect(TEMP tmp_vec, USE_KILL str1, USE_KILL str2, USE_KILL cnt1, USE_KILL cnt2, KILL tmp, KILL cr);\n+\n+  format %{ \"String IndexOf byte[] $str1,$cnt1,$str2,$cnt2 -> $result   \/\/ KILL all\" %}\n+  ins_encode %{\n+    __ string_indexof($str1$$Register, $str2$$Register,\n+                      $cnt1$$Register, $cnt2$$Register,\n+                      (-1), $result$$Register,\n+                      $tmp_vec$$XMMRegister, $tmp$$Register, StrIntrinsicNode::LL);\n+  %}\n+  ins_pipe( pipe_slow );\n+%}\n+\n+instruct string_indexofU(rdi_RegP str1, rdx_RegI cnt1, rsi_RegP str2, rax_RegI cnt2,\n+                         rbx_RegI result, legRegD tmp_vec, rcx_RegI tmp, rFlagsReg cr)\n+%{\n+  predicate(UseSSE42Intrinsics && (((StrIndexOfNode*)n)->encoding() == StrIntrinsicNode::UU));\n+  match(Set result (StrIndexOf (Binary str1 cnt1) (Binary str2 cnt2)));\n+  effect(TEMP tmp_vec, USE_KILL str1, USE_KILL str2, USE_KILL cnt1, USE_KILL cnt2, KILL tmp, KILL cr);\n+\n+  format %{ \"String IndexOf char[] $str1,$cnt1,$str2,$cnt2 -> $result   \/\/ KILL all\" %}\n+  ins_encode %{\n+    __ string_indexof($str1$$Register, $str2$$Register,\n+                      $cnt1$$Register, $cnt2$$Register,\n+                      (-1), $result$$Register,\n+                      $tmp_vec$$XMMRegister, $tmp$$Register, StrIntrinsicNode::UU);\n+  %}\n+  ins_pipe( pipe_slow );\n+%}\n+\n+instruct string_indexofUL(rdi_RegP str1, rdx_RegI cnt1, rsi_RegP str2, rax_RegI cnt2,\n+                          rbx_RegI result, legRegD tmp_vec, rcx_RegI tmp, rFlagsReg cr)\n+%{\n+  predicate(UseSSE42Intrinsics && (((StrIndexOfNode*)n)->encoding() == StrIntrinsicNode::UL));\n+  match(Set result (StrIndexOf (Binary str1 cnt1) (Binary str2 cnt2)));\n+  effect(TEMP tmp_vec, USE_KILL str1, USE_KILL str2, USE_KILL cnt1, USE_KILL cnt2, KILL tmp, KILL cr);\n+\n+  format %{ \"String IndexOf char[] $str1,$cnt1,$str2,$cnt2 -> $result   \/\/ KILL all\" %}\n+  ins_encode %{\n+    __ string_indexof($str1$$Register, $str2$$Register,\n+                      $cnt1$$Register, $cnt2$$Register,\n+                      (-1), $result$$Register,\n+                      $tmp_vec$$XMMRegister, $tmp$$Register, StrIntrinsicNode::UL);\n+  %}\n+  ins_pipe( pipe_slow );\n+%}\n+\n+instruct string_indexof_char(rdi_RegP str1, rdx_RegI cnt1, rax_RegI ch,\n+                              rbx_RegI result, legRegD tmp_vec1, legRegD tmp_vec2, legRegD tmp_vec3, rcx_RegI tmp, rFlagsReg cr)\n+%{\n+  predicate(UseSSE42Intrinsics && (((StrIndexOfCharNode*)n)->encoding() == StrIntrinsicNode::U));\n+  match(Set result (StrIndexOfChar (Binary str1 cnt1) ch));\n+  effect(TEMP tmp_vec1, TEMP tmp_vec2, TEMP tmp_vec3, USE_KILL str1, USE_KILL cnt1, USE_KILL ch, TEMP tmp, KILL cr);\n+  format %{ \"StringUTF16 IndexOf char[] $str1,$cnt1,$ch -> $result   \/\/ KILL all\" %}\n+  ins_encode %{\n+    __ string_indexof_char($str1$$Register, $cnt1$$Register, $ch$$Register, $result$$Register,\n+                           $tmp_vec1$$XMMRegister, $tmp_vec2$$XMMRegister, $tmp_vec3$$XMMRegister, $tmp$$Register);\n+  %}\n+  ins_pipe( pipe_slow );\n+%}\n+\n+instruct stringL_indexof_char(rdi_RegP str1, rdx_RegI cnt1, rax_RegI ch,\n+                              rbx_RegI result, legRegD tmp_vec1, legRegD tmp_vec2, legRegD tmp_vec3, rcx_RegI tmp, rFlagsReg cr)\n+%{\n+  predicate(UseSSE42Intrinsics && (((StrIndexOfCharNode*)n)->encoding() == StrIntrinsicNode::L));\n+  match(Set result (StrIndexOfChar (Binary str1 cnt1) ch));\n+  effect(TEMP tmp_vec1, TEMP tmp_vec2, TEMP tmp_vec3, USE_KILL str1, USE_KILL cnt1, USE_KILL ch, TEMP tmp, KILL cr);\n+  format %{ \"StringLatin1 IndexOf char[] $str1,$cnt1,$ch -> $result   \/\/ KILL all\" %}\n+  ins_encode %{\n+    __ stringL_indexof_char($str1$$Register, $cnt1$$Register, $ch$$Register, $result$$Register,\n+                           $tmp_vec1$$XMMRegister, $tmp_vec2$$XMMRegister, $tmp_vec3$$XMMRegister, $tmp$$Register);\n+  %}\n+  ins_pipe( pipe_slow );\n+%}\n+\n+\/\/ fast string equals\n+instruct string_equals(rdi_RegP str1, rsi_RegP str2, rcx_RegI cnt, rax_RegI result,\n+                       legRegD tmp1, legRegD tmp2, rbx_RegI tmp3, rFlagsReg cr)\n+%{\n+  predicate(!VM_Version::supports_avx512vlbw());\n+  match(Set result (StrEquals (Binary str1 str2) cnt));\n+  effect(TEMP tmp1, TEMP tmp2, USE_KILL str1, USE_KILL str2, USE_KILL cnt, KILL tmp3, KILL cr);\n+\n+  format %{ \"String Equals $str1,$str2,$cnt -> $result    \/\/ KILL $tmp1, $tmp2, $tmp3\" %}\n+  ins_encode %{\n+    __ arrays_equals(false, $str1$$Register, $str2$$Register,\n+                     $cnt$$Register, $result$$Register, $tmp3$$Register,\n+                     $tmp1$$XMMRegister, $tmp2$$XMMRegister, false \/* char *\/, knoreg);\n+  %}\n+  ins_pipe( pipe_slow );\n+%}\n+\n+instruct string_equals_evex(rdi_RegP str1, rsi_RegP str2, rcx_RegI cnt, rax_RegI result,\n+                           legRegD tmp1, legRegD tmp2, kReg ktmp, rbx_RegI tmp3, rFlagsReg cr)\n+%{\n+  predicate(VM_Version::supports_avx512vlbw());\n+  match(Set result (StrEquals (Binary str1 str2) cnt));\n+  effect(TEMP tmp1, TEMP tmp2, TEMP ktmp, USE_KILL str1, USE_KILL str2, USE_KILL cnt, KILL tmp3, KILL cr);\n+\n+  format %{ \"String Equals $str1,$str2,$cnt -> $result    \/\/ KILL $tmp1, $tmp2, $tmp3\" %}\n+  ins_encode %{\n+    __ arrays_equals(false, $str1$$Register, $str2$$Register,\n+                     $cnt$$Register, $result$$Register, $tmp3$$Register,\n+                     $tmp1$$XMMRegister, $tmp2$$XMMRegister, false \/* char *\/, $ktmp$$KRegister);\n+  %}\n+  ins_pipe( pipe_slow );\n+%}\n+\n+\/\/ fast array equals\n+instruct array_equalsB(rdi_RegP ary1, rsi_RegP ary2, rax_RegI result,\n+                       legRegD tmp1, legRegD tmp2, rcx_RegI tmp3, rbx_RegI tmp4, rFlagsReg cr)\n+%{\n+  predicate(!VM_Version::supports_avx512vlbw() && ((AryEqNode*)n)->encoding() == StrIntrinsicNode::LL);\n+  match(Set result (AryEq ary1 ary2));\n+  effect(TEMP tmp1, TEMP tmp2, USE_KILL ary1, USE_KILL ary2, KILL tmp3, KILL tmp4, KILL cr);\n+\n+  format %{ \"Array Equals byte[] $ary1,$ary2 -> $result   \/\/ KILL $tmp1, $tmp2, $tmp3, $tmp4\" %}\n+  ins_encode %{\n+    __ arrays_equals(true, $ary1$$Register, $ary2$$Register,\n+                     $tmp3$$Register, $result$$Register, $tmp4$$Register,\n+                     $tmp1$$XMMRegister, $tmp2$$XMMRegister, false \/* char *\/, knoreg);\n+  %}\n+  ins_pipe( pipe_slow );\n+%}\n+\n+instruct array_equalsB_evex(rdi_RegP ary1, rsi_RegP ary2, rax_RegI result,\n+                            legRegD tmp1, legRegD tmp2, kReg ktmp, rcx_RegI tmp3, rbx_RegI tmp4, rFlagsReg cr)\n+%{\n+  predicate(VM_Version::supports_avx512vlbw() && ((AryEqNode*)n)->encoding() == StrIntrinsicNode::LL);\n+  match(Set result (AryEq ary1 ary2));\n+  effect(TEMP tmp1, TEMP tmp2, TEMP ktmp, USE_KILL ary1, USE_KILL ary2, KILL tmp3, KILL tmp4, KILL cr);\n+\n+  format %{ \"Array Equals byte[] $ary1,$ary2 -> $result   \/\/ KILL $tmp1, $tmp2, $tmp3, $tmp4\" %}\n+  ins_encode %{\n+    __ arrays_equals(true, $ary1$$Register, $ary2$$Register,\n+                     $tmp3$$Register, $result$$Register, $tmp4$$Register,\n+                     $tmp1$$XMMRegister, $tmp2$$XMMRegister, false \/* char *\/, $ktmp$$KRegister);\n+  %}\n+  ins_pipe( pipe_slow );\n+%}\n+\n+instruct array_equalsC(rdi_RegP ary1, rsi_RegP ary2, rax_RegI result,\n+                       legRegD tmp1, legRegD tmp2, rcx_RegI tmp3, rbx_RegI tmp4, rFlagsReg cr)\n+%{\n+  predicate(!VM_Version::supports_avx512vlbw() && ((AryEqNode*)n)->encoding() == StrIntrinsicNode::UU);\n+  match(Set result (AryEq ary1 ary2));\n+  effect(TEMP tmp1, TEMP tmp2, USE_KILL ary1, USE_KILL ary2, KILL tmp3, KILL tmp4, KILL cr);\n+\n+  format %{ \"Array Equals char[] $ary1,$ary2 -> $result   \/\/ KILL $tmp1, $tmp2, $tmp3, $tmp4\" %}\n+  ins_encode %{\n+    __ arrays_equals(true, $ary1$$Register, $ary2$$Register,\n+                     $tmp3$$Register, $result$$Register, $tmp4$$Register,\n+                     $tmp1$$XMMRegister, $tmp2$$XMMRegister, true \/* char *\/, knoreg);\n+  %}\n+  ins_pipe( pipe_slow );\n+%}\n+\n+instruct array_equalsC_evex(rdi_RegP ary1, rsi_RegP ary2, rax_RegI result,\n+                            legRegD tmp1, legRegD tmp2, kReg ktmp, rcx_RegI tmp3, rbx_RegI tmp4, rFlagsReg cr)\n+%{\n+  predicate(VM_Version::supports_avx512vlbw() && ((AryEqNode*)n)->encoding() == StrIntrinsicNode::UU);\n+  match(Set result (AryEq ary1 ary2));\n+  effect(TEMP tmp1, TEMP tmp2, TEMP ktmp, USE_KILL ary1, USE_KILL ary2, KILL tmp3, KILL tmp4, KILL cr);\n+\n+  format %{ \"Array Equals char[] $ary1,$ary2 -> $result   \/\/ KILL $tmp1, $tmp2, $tmp3, $tmp4\" %}\n+  ins_encode %{\n+    __ arrays_equals(true, $ary1$$Register, $ary2$$Register,\n+                     $tmp3$$Register, $result$$Register, $tmp4$$Register,\n+                     $tmp1$$XMMRegister, $tmp2$$XMMRegister, true \/* char *\/, $ktmp$$KRegister);\n+  %}\n+  ins_pipe( pipe_slow );\n+%}\n+\n+instruct arrays_hashcode(rdi_RegP ary1, rdx_RegI cnt1, rbx_RegI result, immU8 basic_type,\n+                         legRegD tmp_vec1, legRegD tmp_vec2, legRegD tmp_vec3, legRegD tmp_vec4,\n+                         legRegD tmp_vec5, legRegD tmp_vec6, legRegD tmp_vec7, legRegD tmp_vec8,\n+                         legRegD tmp_vec9, legRegD tmp_vec10, legRegD tmp_vec11, legRegD tmp_vec12,\n+                         legRegD tmp_vec13, rRegI tmp1, rRegI tmp2, rRegI tmp3, rFlagsReg cr)\n+%{\n+  predicate(UseAVX >= 2);\n+  match(Set result (VectorizedHashCode (Binary ary1 cnt1) (Binary result basic_type)));\n+  effect(TEMP tmp_vec1, TEMP tmp_vec2, TEMP tmp_vec3, TEMP tmp_vec4, TEMP tmp_vec5, TEMP tmp_vec6,\n+         TEMP tmp_vec7, TEMP tmp_vec8, TEMP tmp_vec9, TEMP tmp_vec10, TEMP tmp_vec11, TEMP tmp_vec12,\n+         TEMP tmp_vec13, TEMP tmp1, TEMP tmp2, TEMP tmp3, USE_KILL ary1, USE_KILL cnt1,\n+         USE basic_type, KILL cr);\n+\n+  format %{ \"Array HashCode array[] $ary1,$cnt1,$result,$basic_type -> $result   \/\/ KILL all\" %}\n+  ins_encode %{\n+    __ arrays_hashcode($ary1$$Register, $cnt1$$Register, $result$$Register,\n+                       $tmp1$$Register, $tmp2$$Register, $tmp3$$Register,\n+                       $tmp_vec1$$XMMRegister, $tmp_vec2$$XMMRegister, $tmp_vec3$$XMMRegister,\n+                       $tmp_vec4$$XMMRegister, $tmp_vec5$$XMMRegister, $tmp_vec6$$XMMRegister,\n+                       $tmp_vec7$$XMMRegister, $tmp_vec8$$XMMRegister, $tmp_vec9$$XMMRegister,\n+                       $tmp_vec10$$XMMRegister, $tmp_vec11$$XMMRegister, $tmp_vec12$$XMMRegister,\n+                       $tmp_vec13$$XMMRegister, (BasicType)$basic_type$$constant);\n+  %}\n+  ins_pipe( pipe_slow );\n+%}\n+\n+instruct count_positives(rsi_RegP ary1, rcx_RegI len, rax_RegI result,\n+                         legRegD tmp1, legRegD tmp2, rbx_RegI tmp3, rFlagsReg cr,)\n+%{\n+  predicate(!VM_Version::supports_avx512vlbw() || !VM_Version::supports_bmi2());\n+  match(Set result (CountPositives ary1 len));\n+  effect(TEMP tmp1, TEMP tmp2, USE_KILL ary1, USE_KILL len, KILL tmp3, KILL cr);\n+\n+  format %{ \"countPositives byte[] $ary1,$len -> $result   \/\/ KILL $tmp1, $tmp2, $tmp3\" %}\n+  ins_encode %{\n+    __ count_positives($ary1$$Register, $len$$Register,\n+                       $result$$Register, $tmp3$$Register,\n+                       $tmp1$$XMMRegister, $tmp2$$XMMRegister, knoreg, knoreg);\n+  %}\n+  ins_pipe( pipe_slow );\n+%}\n+\n+instruct count_positives_evex(rsi_RegP ary1, rcx_RegI len, rax_RegI result,\n+                              legRegD tmp1, legRegD tmp2, kReg ktmp1, kReg ktmp2, rbx_RegI tmp3, rFlagsReg cr,)\n+%{\n+  predicate(VM_Version::supports_avx512vlbw() && VM_Version::supports_bmi2());\n+  match(Set result (CountPositives ary1 len));\n+  effect(TEMP tmp1, TEMP tmp2, TEMP ktmp1, TEMP ktmp2, USE_KILL ary1, USE_KILL len, KILL tmp3, KILL cr);\n+\n+  format %{ \"countPositives byte[] $ary1,$len -> $result   \/\/ KILL $tmp1, $tmp2, $tmp3\" %}\n+  ins_encode %{\n+    __ count_positives($ary1$$Register, $len$$Register,\n+                       $result$$Register, $tmp3$$Register,\n+                       $tmp1$$XMMRegister, $tmp2$$XMMRegister, $ktmp1$$KRegister, $ktmp2$$KRegister);\n+  %}\n+  ins_pipe( pipe_slow );\n+%}\n+\n+\/\/ fast char[] to byte[] compression\n+instruct string_compress(rsi_RegP src, rdi_RegP dst, rdx_RegI len, legRegD tmp1, legRegD tmp2, legRegD tmp3,\n+                         legRegD tmp4, rcx_RegI tmp5, rax_RegI result, rFlagsReg cr) %{\n+  predicate(!VM_Version::supports_avx512vlbw() || !VM_Version::supports_bmi2());\n+  match(Set result (StrCompressedCopy src (Binary dst len)));\n+  effect(TEMP tmp1, TEMP tmp2, TEMP tmp3, TEMP tmp4, USE_KILL src, USE_KILL dst,\n+         USE_KILL len, KILL tmp5, KILL cr);\n+\n+  format %{ \"String Compress $src,$dst -> $result    \/\/ KILL RAX, RCX, RDX\" %}\n+  ins_encode %{\n+    __ char_array_compress($src$$Register, $dst$$Register, $len$$Register,\n+                           $tmp1$$XMMRegister, $tmp2$$XMMRegister, $tmp3$$XMMRegister,\n+                           $tmp4$$XMMRegister, $tmp5$$Register, $result$$Register,\n+                           knoreg, knoreg);\n+  %}\n+  ins_pipe( pipe_slow );\n+%}\n+\n+instruct string_compress_evex(rsi_RegP src, rdi_RegP dst, rdx_RegI len, legRegD tmp1, legRegD tmp2, legRegD tmp3,\n+                              legRegD tmp4, kReg ktmp1, kReg ktmp2, rcx_RegI tmp5, rax_RegI result, rFlagsReg cr) %{\n+  predicate(VM_Version::supports_avx512vlbw() && VM_Version::supports_bmi2());\n+  match(Set result (StrCompressedCopy src (Binary dst len)));\n+  effect(TEMP tmp1, TEMP tmp2, TEMP tmp3, TEMP tmp4, TEMP ktmp1, TEMP ktmp2, USE_KILL src, USE_KILL dst,\n+         USE_KILL len, KILL tmp5, KILL cr);\n+\n+  format %{ \"String Compress $src,$dst -> $result    \/\/ KILL RAX, RCX, RDX\" %}\n+  ins_encode %{\n+    __ char_array_compress($src$$Register, $dst$$Register, $len$$Register,\n+                           $tmp1$$XMMRegister, $tmp2$$XMMRegister, $tmp3$$XMMRegister,\n+                           $tmp4$$XMMRegister, $tmp5$$Register, $result$$Register,\n+                           $ktmp1$$KRegister, $ktmp2$$KRegister);\n+  %}\n+  ins_pipe( pipe_slow );\n+%}\n+\/\/ fast byte[] to char[] inflation\n+instruct string_inflate(Universe dummy, rsi_RegP src, rdi_RegP dst, rdx_RegI len,\n+                        legRegD tmp1, rcx_RegI tmp2, rFlagsReg cr) %{\n+  predicate(!VM_Version::supports_avx512vlbw() || !VM_Version::supports_bmi2());\n+  match(Set dummy (StrInflatedCopy src (Binary dst len)));\n+  effect(TEMP tmp1, TEMP tmp2, USE_KILL src, USE_KILL dst, USE_KILL len, KILL cr);\n+\n+  format %{ \"String Inflate $src,$dst    \/\/ KILL $tmp1, $tmp2\" %}\n+  ins_encode %{\n+    __ byte_array_inflate($src$$Register, $dst$$Register, $len$$Register,\n+                          $tmp1$$XMMRegister, $tmp2$$Register, knoreg);\n+  %}\n+  ins_pipe( pipe_slow );\n+%}\n+\n+instruct string_inflate_evex(Universe dummy, rsi_RegP src, rdi_RegP dst, rdx_RegI len,\n+                             legRegD tmp1, kReg ktmp, rcx_RegI tmp2, rFlagsReg cr) %{\n+  predicate(VM_Version::supports_avx512vlbw() && VM_Version::supports_bmi2());\n+  match(Set dummy (StrInflatedCopy src (Binary dst len)));\n+  effect(TEMP tmp1, TEMP tmp2, TEMP ktmp, USE_KILL src, USE_KILL dst, USE_KILL len, KILL cr);\n+\n+  format %{ \"String Inflate $src,$dst    \/\/ KILL $tmp1, $tmp2\" %}\n+  ins_encode %{\n+    __ byte_array_inflate($src$$Register, $dst$$Register, $len$$Register,\n+                          $tmp1$$XMMRegister, $tmp2$$Register, $ktmp$$KRegister);\n+  %}\n+  ins_pipe( pipe_slow );\n+%}\n+\n+\/\/ encode char[] to byte[] in ISO_8859_1\n+instruct encode_iso_array(rsi_RegP src, rdi_RegP dst, rdx_RegI len,\n+                          legRegD tmp1, legRegD tmp2, legRegD tmp3, legRegD tmp4,\n+                          rcx_RegI tmp5, rax_RegI result, rFlagsReg cr) %{\n+  predicate(!((EncodeISOArrayNode*)n)->is_ascii());\n+  match(Set result (EncodeISOArray src (Binary dst len)));\n+  effect(TEMP tmp1, TEMP tmp2, TEMP tmp3, TEMP tmp4, USE_KILL src, USE_KILL dst, USE_KILL len, KILL tmp5, KILL cr);\n+\n+  format %{ \"Encode iso array $src,$dst,$len -> $result    \/\/ KILL RCX, RDX, $tmp1, $tmp2, $tmp3, $tmp4, RSI, RDI \" %}\n+  ins_encode %{\n+    __ encode_iso_array($src$$Register, $dst$$Register, $len$$Register,\n+                        $tmp1$$XMMRegister, $tmp2$$XMMRegister, $tmp3$$XMMRegister,\n+                        $tmp4$$XMMRegister, $tmp5$$Register, $result$$Register, false);\n+  %}\n+  ins_pipe( pipe_slow );\n+%}\n+\n+\/\/ encode char[] to byte[] in ASCII\n+instruct encode_ascii_array(rsi_RegP src, rdi_RegP dst, rdx_RegI len,\n+                            legRegD tmp1, legRegD tmp2, legRegD tmp3, legRegD tmp4,\n+                            rcx_RegI tmp5, rax_RegI result, rFlagsReg cr) %{\n+  predicate(((EncodeISOArrayNode*)n)->is_ascii());\n+  match(Set result (EncodeISOArray src (Binary dst len)));\n+  effect(TEMP tmp1, TEMP tmp2, TEMP tmp3, TEMP tmp4, USE_KILL src, USE_KILL dst, USE_KILL len, KILL tmp5, KILL cr);\n+\n+  format %{ \"Encode ascii array $src,$dst,$len -> $result    \/\/ KILL RCX, RDX, $tmp1, $tmp2, $tmp3, $tmp4, RSI, RDI \" %}\n+  ins_encode %{\n+    __ encode_iso_array($src$$Register, $dst$$Register, $len$$Register,\n+                        $tmp1$$XMMRegister, $tmp2$$XMMRegister, $tmp3$$XMMRegister,\n+                        $tmp4$$XMMRegister, $tmp5$$Register, $result$$Register, true);\n+  %}\n+  ins_pipe( pipe_slow );\n+%}\n+\n+\/\/----------Overflow Math Instructions-----------------------------------------\n+\n+instruct overflowAddI_rReg(rFlagsReg cr, rax_RegI op1, rRegI op2)\n+%{\n+  match(Set cr (OverflowAddI op1 op2));\n+  effect(DEF cr, USE_KILL op1, USE op2);\n+\n+  format %{ \"addl    $op1, $op2\\t# overflow check int\" %}\n+\n+  ins_encode %{\n+    __ addl($op1$$Register, $op2$$Register);\n+  %}\n+  ins_pipe(ialu_reg_reg);\n+%}\n+\n+instruct overflowAddI_rReg_imm(rFlagsReg cr, rax_RegI op1, immI op2)\n+%{\n+  match(Set cr (OverflowAddI op1 op2));\n+  effect(DEF cr, USE_KILL op1, USE op2);\n+\n+  format %{ \"addl    $op1, $op2\\t# overflow check int\" %}\n+\n+  ins_encode %{\n+    __ addl($op1$$Register, $op2$$constant);\n+  %}\n+  ins_pipe(ialu_reg_reg);\n+%}\n+\n+instruct overflowAddL_rReg(rFlagsReg cr, rax_RegL op1, rRegL op2)\n+%{\n+  match(Set cr (OverflowAddL op1 op2));\n+  effect(DEF cr, USE_KILL op1, USE op2);\n+\n+  format %{ \"addq    $op1, $op2\\t# overflow check long\" %}\n+  ins_encode %{\n+    __ addq($op1$$Register, $op2$$Register);\n+  %}\n+  ins_pipe(ialu_reg_reg);\n+%}\n+\n+instruct overflowAddL_rReg_imm(rFlagsReg cr, rax_RegL op1, immL32 op2)\n+%{\n+  match(Set cr (OverflowAddL op1 op2));\n+  effect(DEF cr, USE_KILL op1, USE op2);\n+\n+  format %{ \"addq    $op1, $op2\\t# overflow check long\" %}\n+  ins_encode %{\n+    __ addq($op1$$Register, $op2$$constant);\n+  %}\n+  ins_pipe(ialu_reg_reg);\n+%}\n+\n+instruct overflowSubI_rReg(rFlagsReg cr, rRegI op1, rRegI op2)\n+%{\n+  match(Set cr (OverflowSubI op1 op2));\n+\n+  format %{ \"cmpl    $op1, $op2\\t# overflow check int\" %}\n+  ins_encode %{\n+    __ cmpl($op1$$Register, $op2$$Register);\n+  %}\n+  ins_pipe(ialu_reg_reg);\n+%}\n+\n+instruct overflowSubI_rReg_imm(rFlagsReg cr, rRegI op1, immI op2)\n+%{\n+  match(Set cr (OverflowSubI op1 op2));\n+\n+  format %{ \"cmpl    $op1, $op2\\t# overflow check int\" %}\n+  ins_encode %{\n+    __ cmpl($op1$$Register, $op2$$constant);\n+  %}\n+  ins_pipe(ialu_reg_reg);\n+%}\n+\n+instruct overflowSubL_rReg(rFlagsReg cr, rRegL op1, rRegL op2)\n+%{\n+  match(Set cr (OverflowSubL op1 op2));\n+\n+  format %{ \"cmpq    $op1, $op2\\t# overflow check long\" %}\n+  ins_encode %{\n+    __ cmpq($op1$$Register, $op2$$Register);\n+  %}\n+  ins_pipe(ialu_reg_reg);\n+%}\n+\n+instruct overflowSubL_rReg_imm(rFlagsReg cr, rRegL op1, immL32 op2)\n+%{\n+  match(Set cr (OverflowSubL op1 op2));\n+\n+  format %{ \"cmpq    $op1, $op2\\t# overflow check long\" %}\n+  ins_encode %{\n+    __ cmpq($op1$$Register, $op2$$constant);\n+  %}\n+  ins_pipe(ialu_reg_reg);\n+%}\n+\n+instruct overflowNegI_rReg(rFlagsReg cr, immI_0 zero, rax_RegI op2)\n+%{\n+  match(Set cr (OverflowSubI zero op2));\n+  effect(DEF cr, USE_KILL op2);\n+\n+  format %{ \"negl    $op2\\t# overflow check int\" %}\n+  ins_encode %{\n+    __ negl($op2$$Register);\n+  %}\n+  ins_pipe(ialu_reg_reg);\n+%}\n+\n+instruct overflowNegL_rReg(rFlagsReg cr, immL0 zero, rax_RegL op2)\n+%{\n+  match(Set cr (OverflowSubL zero op2));\n+  effect(DEF cr, USE_KILL op2);\n+\n+  format %{ \"negq    $op2\\t# overflow check long\" %}\n+  ins_encode %{\n+    __ negq($op2$$Register);\n+  %}\n+  ins_pipe(ialu_reg_reg);\n+%}\n+\n+instruct overflowMulI_rReg(rFlagsReg cr, rax_RegI op1, rRegI op2)\n+%{\n+  match(Set cr (OverflowMulI op1 op2));\n+  effect(DEF cr, USE_KILL op1, USE op2);\n+\n+  format %{ \"imull    $op1, $op2\\t# overflow check int\" %}\n+  ins_encode %{\n+    __ imull($op1$$Register, $op2$$Register);\n+  %}\n+  ins_pipe(ialu_reg_reg_alu0);\n+%}\n+\n+instruct overflowMulI_rReg_imm(rFlagsReg cr, rRegI op1, immI op2, rRegI tmp)\n+%{\n+  match(Set cr (OverflowMulI op1 op2));\n+  effect(DEF cr, TEMP tmp, USE op1, USE op2);\n+\n+  format %{ \"imull    $tmp, $op1, $op2\\t# overflow check int\" %}\n+  ins_encode %{\n+    __ imull($tmp$$Register, $op1$$Register, $op2$$constant);\n+  %}\n+  ins_pipe(ialu_reg_reg_alu0);\n+%}\n+\n+instruct overflowMulL_rReg(rFlagsReg cr, rax_RegL op1, rRegL op2)\n+%{\n+  match(Set cr (OverflowMulL op1 op2));\n+  effect(DEF cr, USE_KILL op1, USE op2);\n+\n+  format %{ \"imulq    $op1, $op2\\t# overflow check long\" %}\n+  ins_encode %{\n+    __ imulq($op1$$Register, $op2$$Register);\n+  %}\n+  ins_pipe(ialu_reg_reg_alu0);\n+%}\n+\n+instruct overflowMulL_rReg_imm(rFlagsReg cr, rRegL op1, immL32 op2, rRegL tmp)\n+%{\n+  match(Set cr (OverflowMulL op1 op2));\n+  effect(DEF cr, TEMP tmp, USE op1, USE op2);\n+\n+  format %{ \"imulq    $tmp, $op1, $op2\\t# overflow check long\" %}\n+  ins_encode %{\n+    __ imulq($tmp$$Register, $op1$$Register, $op2$$constant);\n+  %}\n+  ins_pipe(ialu_reg_reg_alu0);\n+%}\n+\n+\n+\/\/----------Control Flow Instructions------------------------------------------\n+\/\/ Signed compare Instructions\n+\n+\/\/ XXX more variants!!\n+instruct compI_rReg(rFlagsReg cr, rRegI op1, rRegI op2)\n+%{\n+  match(Set cr (CmpI op1 op2));\n+  effect(DEF cr, USE op1, USE op2);\n+\n+  format %{ \"cmpl    $op1, $op2\" %}\n+  ins_encode %{\n+    __ cmpl($op1$$Register, $op2$$Register);\n+  %}\n+  ins_pipe(ialu_cr_reg_reg);\n+%}\n+\n+instruct compI_rReg_imm(rFlagsReg cr, rRegI op1, immI op2)\n+%{\n+  match(Set cr (CmpI op1 op2));\n+\n+  format %{ \"cmpl    $op1, $op2\" %}\n+  ins_encode %{\n+    __ cmpl($op1$$Register, $op2$$constant);\n+  %}\n+  ins_pipe(ialu_cr_reg_imm);\n+%}\n+\n+instruct compI_rReg_mem(rFlagsReg cr, rRegI op1, memory op2)\n+%{\n+  match(Set cr (CmpI op1 (LoadI op2)));\n+\n+  ins_cost(500); \/\/ XXX\n+  format %{ \"cmpl    $op1, $op2\" %}\n+  ins_encode %{\n+    __ cmpl($op1$$Register, $op2$$Address);\n+  %}\n+  ins_pipe(ialu_cr_reg_mem);\n+%}\n+\n+instruct testI_reg(rFlagsReg cr, rRegI src, immI_0 zero)\n+%{\n+  match(Set cr (CmpI src zero));\n+\n+  format %{ \"testl   $src, $src\" %}\n+  ins_encode %{\n+    __ testl($src$$Register, $src$$Register);\n+  %}\n+  ins_pipe(ialu_cr_reg_imm);\n+%}\n+\n+instruct testI_reg_imm(rFlagsReg cr, rRegI src, immI con, immI_0 zero)\n+%{\n+  match(Set cr (CmpI (AndI src con) zero));\n+\n+  format %{ \"testl   $src, $con\" %}\n+  ins_encode %{\n+    __ testl($src$$Register, $con$$constant);\n+  %}\n+  ins_pipe(ialu_cr_reg_imm);\n+%}\n+\n+instruct testI_reg_reg(rFlagsReg cr, rRegI src1, rRegI src2, immI_0 zero)\n+%{\n+  match(Set cr (CmpI (AndI src1 src2) zero));\n+\n+  format %{ \"testl   $src1, $src2\" %}\n+  ins_encode %{\n+    __ testl($src1$$Register, $src2$$Register);\n+  %}\n+  ins_pipe(ialu_cr_reg_imm);\n+%}\n+\n+instruct testI_reg_mem(rFlagsReg cr, rRegI src, memory mem, immI_0 zero)\n+%{\n+  match(Set cr (CmpI (AndI src (LoadI mem)) zero));\n+\n+  format %{ \"testl   $src, $mem\" %}\n+  ins_encode %{\n+    __ testl($src$$Register, $mem$$Address);\n+  %}\n+  ins_pipe(ialu_cr_reg_mem);\n+%}\n+\n+\/\/ Unsigned compare Instructions; really, same as signed except they\n+\/\/ produce an rFlagsRegU instead of rFlagsReg.\n+instruct compU_rReg(rFlagsRegU cr, rRegI op1, rRegI op2)\n+%{\n+  match(Set cr (CmpU op1 op2));\n+\n+  format %{ \"cmpl    $op1, $op2\\t# unsigned\" %}\n+  ins_encode %{\n+    __ cmpl($op1$$Register, $op2$$Register);\n+  %}\n+  ins_pipe(ialu_cr_reg_reg);\n+%}\n+\n+instruct compU_rReg_imm(rFlagsRegU cr, rRegI op1, immI op2)\n+%{\n+  match(Set cr (CmpU op1 op2));\n+\n+  format %{ \"cmpl    $op1, $op2\\t# unsigned\" %}\n+  ins_encode %{\n+    __ cmpl($op1$$Register, $op2$$constant);\n+  %}\n+  ins_pipe(ialu_cr_reg_imm);\n+%}\n+\n+instruct compU_rReg_mem(rFlagsRegU cr, rRegI op1, memory op2)\n+%{\n+  match(Set cr (CmpU op1 (LoadI op2)));\n+\n+  ins_cost(500); \/\/ XXX\n+  format %{ \"cmpl    $op1, $op2\\t# unsigned\" %}\n+  ins_encode %{\n+    __ cmpl($op1$$Register, $op2$$Address);\n+  %}\n+  ins_pipe(ialu_cr_reg_mem);\n+%}\n+\n+instruct testU_reg(rFlagsRegU cr, rRegI src, immI_0 zero)\n+%{\n+  match(Set cr (CmpU src zero));\n+\n+  format %{ \"testl   $src, $src\\t# unsigned\" %}\n+  ins_encode %{\n+    __ testl($src$$Register, $src$$Register);\n+  %}\n+  ins_pipe(ialu_cr_reg_imm);\n+%}\n+\n+instruct compP_rReg(rFlagsRegU cr, rRegP op1, rRegP op2)\n+%{\n+  match(Set cr (CmpP op1 op2));\n+\n+  format %{ \"cmpq    $op1, $op2\\t# ptr\" %}\n+  ins_encode %{\n+    __ cmpq($op1$$Register, $op2$$Register);\n+  %}\n+  ins_pipe(ialu_cr_reg_reg);\n+%}\n+\n+instruct compP_rReg_mem(rFlagsRegU cr, rRegP op1, memory op2)\n+%{\n+  match(Set cr (CmpP op1 (LoadP op2)));\n+  predicate(n->in(2)->as_Load()->barrier_data() == 0);\n+\n+  ins_cost(500); \/\/ XXX\n+  format %{ \"cmpq    $op1, $op2\\t# ptr\" %}\n+  ins_encode %{\n+    __ cmpq($op1$$Register, $op2$$Address);\n+  %}\n+  ins_pipe(ialu_cr_reg_mem);\n+%}\n+\n+\/\/ XXX this is generalized by compP_rReg_mem???\n+\/\/ Compare raw pointer (used in out-of-heap check).\n+\/\/ Only works because non-oop pointers must be raw pointers\n+\/\/ and raw pointers have no anti-dependencies.\n+instruct compP_mem_rReg(rFlagsRegU cr, rRegP op1, memory op2)\n+%{\n+  predicate(n->in(2)->in(2)->bottom_type()->reloc() == relocInfo::none &&\n+            n->in(2)->as_Load()->barrier_data() == 0);\n+  match(Set cr (CmpP op1 (LoadP op2)));\n+\n+  format %{ \"cmpq    $op1, $op2\\t# raw ptr\" %}\n+  ins_encode %{\n+    __ cmpq($op1$$Register, $op2$$Address);\n+  %}\n+  ins_pipe(ialu_cr_reg_mem);\n+%}\n+\n+\/\/ This will generate a signed flags result. This should be OK since\n+\/\/ any compare to a zero should be eq\/neq.\n+instruct testP_reg(rFlagsReg cr, rRegP src, immP0 zero)\n+%{\n+  match(Set cr (CmpP src zero));\n+\n+  format %{ \"testq   $src, $src\\t# ptr\" %}\n+  ins_encode %{\n+    __ testq($src$$Register, $src$$Register);\n+  %}\n+  ins_pipe(ialu_cr_reg_imm);\n+%}\n+\n+\/\/ This will generate a signed flags result. This should be OK since\n+\/\/ any compare to a zero should be eq\/neq.\n+instruct testP_mem(rFlagsReg cr, memory op, immP0 zero)\n+%{\n+  predicate((!UseCompressedOops || (CompressedOops::base() != nullptr)) &&\n+            n->in(1)->as_Load()->barrier_data() == 0);\n+  match(Set cr (CmpP (LoadP op) zero));\n+\n+  ins_cost(500); \/\/ XXX\n+  format %{ \"testq   $op, 0xffffffffffffffff\\t# ptr\" %}\n+  ins_encode %{\n+    __ testq($op$$Address, 0xFFFFFFFF);\n+  %}\n+  ins_pipe(ialu_cr_reg_imm);\n+%}\n+\n+instruct testP_mem_reg0(rFlagsReg cr, memory mem, immP0 zero)\n+%{\n+  predicate(UseCompressedOops && (CompressedOops::base() == nullptr) &&\n+            n->in(1)->as_Load()->barrier_data() == 0);\n+  match(Set cr (CmpP (LoadP mem) zero));\n+\n+  format %{ \"cmpq    R12, $mem\\t# ptr (R12_heapbase==0)\" %}\n+  ins_encode %{\n+    __ cmpq(r12, $mem$$Address);\n+  %}\n+  ins_pipe(ialu_cr_reg_mem);\n+%}\n+\n+instruct compN_rReg(rFlagsRegU cr, rRegN op1, rRegN op2)\n+%{\n+  match(Set cr (CmpN op1 op2));\n+\n+  format %{ \"cmpl    $op1, $op2\\t# compressed ptr\" %}\n+  ins_encode %{ __ cmpl($op1$$Register, $op2$$Register); %}\n+  ins_pipe(ialu_cr_reg_reg);\n+%}\n+\n+instruct compN_rReg_mem(rFlagsRegU cr, rRegN src, memory mem)\n+%{\n+  predicate(n->in(2)->as_Load()->barrier_data() == 0);\n+  match(Set cr (CmpN src (LoadN mem)));\n+\n+  format %{ \"cmpl    $src, $mem\\t# compressed ptr\" %}\n+  ins_encode %{\n+    __ cmpl($src$$Register, $mem$$Address);\n+  %}\n+  ins_pipe(ialu_cr_reg_mem);\n+%}\n+\n+instruct compN_rReg_imm(rFlagsRegU cr, rRegN op1, immN op2) %{\n+  match(Set cr (CmpN op1 op2));\n+\n+  format %{ \"cmpl    $op1, $op2\\t# compressed ptr\" %}\n+  ins_encode %{\n+    __ cmp_narrow_oop($op1$$Register, (jobject)$op2$$constant);\n+  %}\n+  ins_pipe(ialu_cr_reg_imm);\n+%}\n+\n+instruct compN_mem_imm(rFlagsRegU cr, memory mem, immN src)\n+%{\n+  predicate(n->in(2)->as_Load()->barrier_data() == 0);\n+  match(Set cr (CmpN src (LoadN mem)));\n+\n+  format %{ \"cmpl    $mem, $src\\t# compressed ptr\" %}\n+  ins_encode %{\n+    __ cmp_narrow_oop($mem$$Address, (jobject)$src$$constant);\n+  %}\n+  ins_pipe(ialu_cr_reg_mem);\n+%}\n@@ -1914,10 +16204,2 @@\n-bool Matcher::match_rule_supported_vector_masked(int opcode, int vlen, BasicType bt) {\n-  \/\/ ADLC based match_rule_supported routine checks for the existence of pattern based\n-  \/\/ on IR opcode. Most of the unary\/binary\/ternary masked operation share the IR nodes\n-  \/\/ of their non-masked counterpart with mask edge being the differentiator.\n-  \/\/ This routine does a strict check on the existence of masked operation patterns\n-  \/\/ by returning a default false value for all the other opcodes apart from the\n-  \/\/ ones whose masked instruction patterns are defined in this file.\n-  if (!match_rule_supported_vector(opcode, vlen, bt)) {\n-    return false;\n-  }\n+instruct compN_rReg_imm_klass(rFlagsRegU cr, rRegN op1, immNKlass op2) %{\n+  match(Set cr (CmpN op1 op2));\n@@ -1925,14 +16207,6 @@\n-  int size_in_bits = vlen * type2aelembytes(bt) * BitsPerByte;\n-  if (size_in_bits != 512 && !VM_Version::supports_avx512vl()) {\n-    return false;\n-  }\n-  switch(opcode) {\n-    \/\/ Unary masked operations\n-    case Op_AbsVB:\n-    case Op_AbsVS:\n-      if(!VM_Version::supports_avx512bw()) {\n-        return false;  \/\/ Implementation limitation\n-      }\n-    case Op_AbsVI:\n-    case Op_AbsVL:\n-      return true;\n+  format %{ \"cmpl    $op1, $op2\\t# compressed klass ptr\" %}\n+  ins_encode %{\n+    __ cmp_narrow_klass($op1$$Register, (Klass*)$op2$$constant);\n+  %}\n+  ins_pipe(ialu_cr_reg_imm);\n+%}\n@@ -1940,4 +16214,4 @@\n-    \/\/ Ternary masked operations\n-    case Op_FmaVF:\n-    case Op_FmaVD:\n-      return true;\n+instruct compN_mem_imm_klass(rFlagsRegU cr, memory mem, immNKlass src)\n+%{\n+  predicate(!UseCompactObjectHeaders);\n+  match(Set cr (CmpN src (LoadNKlass mem)));\n@@ -1945,5 +16219,6 @@\n-    case Op_MacroLogicV:\n-      if(bt != T_INT && bt != T_LONG) {\n-        return false;\n-      }\n-      return true;\n+  format %{ \"cmpl    $mem, $src\\t# compressed klass ptr\" %}\n+  ins_encode %{\n+    __ cmp_narrow_klass($mem$$Address, (Klass*)$src$$constant);\n+  %}\n+  ins_pipe(ialu_cr_reg_mem);\n+%}\n@@ -1951,14 +16226,2 @@\n-    \/\/ Binary masked operations\n-    case Op_AddVB:\n-    case Op_AddVS:\n-    case Op_SubVB:\n-    case Op_SubVS:\n-    case Op_MulVS:\n-    case Op_LShiftVS:\n-    case Op_RShiftVS:\n-    case Op_URShiftVS:\n-      assert(size_in_bits == 512 || VM_Version::supports_avx512vl(), \"\");\n-      if (!VM_Version::supports_avx512bw()) {\n-        return false;  \/\/ Implementation limitation\n-      }\n-      return true;\n+instruct testN_reg(rFlagsReg cr, rRegN src, immN0 zero) %{\n+  match(Set cr (CmpN src zero));\n@@ -1966,6 +16229,4 @@\n-    case Op_MulVL:\n-      assert(size_in_bits == 512 || VM_Version::supports_avx512vl(), \"\");\n-      if (!VM_Version::supports_avx512dq()) {\n-        return false;  \/\/ Implementation limitation\n-      }\n-      return true;\n+  format %{ \"testl   $src, $src\\t# compressed ptr\" %}\n+  ins_encode %{ __ testl($src$$Register, $src$$Register); %}\n+  ins_pipe(ialu_cr_reg_imm);\n+%}\n@@ -1973,9 +16234,5 @@\n-    case Op_AndV:\n-    case Op_OrV:\n-    case Op_XorV:\n-    case Op_RotateRightV:\n-    case Op_RotateLeftV:\n-      if (bt != T_INT && bt != T_LONG) {\n-        return false; \/\/ Implementation limitation\n-      }\n-      return true;\n+instruct testN_mem(rFlagsReg cr, memory mem, immN0 zero)\n+%{\n+  predicate(CompressedOops::base() != nullptr &&\n+            n->in(1)->as_Load()->barrier_data() == 0);\n+  match(Set cr (CmpN (LoadN mem) zero));\n@@ -1983,6 +16240,7 @@\n-    case Op_VectorLoadMask:\n-      assert(size_in_bits == 512 || VM_Version::supports_avx512vl(), \"\");\n-      if (is_subword_type(bt) && !VM_Version::supports_avx512bw()) {\n-        return false;\n-      }\n-      return true;\n+  ins_cost(500); \/\/ XXX\n+  format %{ \"testl   $mem, 0xffffffff\\t# compressed ptr\" %}\n+  ins_encode %{\n+    __ cmpl($mem$$Address, (int)0xFFFFFFFF);\n+  %}\n+  ins_pipe(ialu_cr_reg_mem);\n+%}\n@@ -1990,26 +16248,5 @@\n-    case Op_AddVI:\n-    case Op_AddVL:\n-    case Op_AddVF:\n-    case Op_AddVD:\n-    case Op_SubVI:\n-    case Op_SubVL:\n-    case Op_SubVF:\n-    case Op_SubVD:\n-    case Op_MulVI:\n-    case Op_MulVF:\n-    case Op_MulVD:\n-    case Op_DivVF:\n-    case Op_DivVD:\n-    case Op_SqrtVF:\n-    case Op_SqrtVD:\n-    case Op_LShiftVI:\n-    case Op_LShiftVL:\n-    case Op_RShiftVI:\n-    case Op_RShiftVL:\n-    case Op_URShiftVI:\n-    case Op_URShiftVL:\n-    case Op_LoadVectorMasked:\n-    case Op_StoreVectorMasked:\n-    case Op_LoadVectorGatherMasked:\n-    case Op_StoreVectorScatterMasked:\n-      return true;\n+instruct testN_mem_reg0(rFlagsReg cr, memory mem, immN0 zero)\n+%{\n+  predicate(CompressedOops::base() == nullptr &&\n+            n->in(1)->as_Load()->barrier_data() == 0);\n+  match(Set cr (CmpN (LoadN mem) zero));\n@@ -2017,23 +16254,6 @@\n-    case Op_UMinV:\n-    case Op_UMaxV:\n-      if (size_in_bits < 512 && !VM_Version::supports_avx512vl()) {\n-        return false;\n-      } \/\/ fallthrough\n-    case Op_MaxV:\n-    case Op_MinV:\n-      if (is_subword_type(bt) && !VM_Version::supports_avx512bw()) {\n-        return false; \/\/ Implementation limitation\n-      }\n-      if (is_floating_point_type(bt) && !VM_Version::supports_avx10_2()) {\n-        return false; \/\/ Implementation limitation\n-      }\n-      return true;\n-    case Op_SaturatingAddV:\n-    case Op_SaturatingSubV:\n-      if (!is_subword_type(bt)) {\n-        return false;\n-      }\n-      if (size_in_bits < 128 || !VM_Version::supports_avx512bw()) {\n-        return false; \/\/ Implementation limitation\n-      }\n-      return true;\n+  format %{ \"cmpl    R12, $mem\\t# compressed ptr (R12_heapbase==0)\" %}\n+  ins_encode %{\n+    __ cmpl(r12, $mem$$Address);\n+  %}\n+  ins_pipe(ialu_cr_reg_mem);\n+%}\n@@ -2041,5 +16261,2 @@\n-    case Op_VectorMaskCmp:\n-      if (is_subword_type(bt) && !VM_Version::supports_avx512bw()) {\n-        return false; \/\/ Implementation limitation\n-      }\n-      return true;\n+\/\/ Yanked all unsigned pointer compare operations.\n+\/\/ Pointer compares are done with CmpP which is already unsigned.\n@@ -2047,10 +16264,3 @@\n-    case Op_VectorRearrange:\n-      if (bt == T_SHORT && !VM_Version::supports_avx512bw()) {\n-        return false; \/\/ Implementation limitation\n-      }\n-      if (bt == T_BYTE && !VM_Version::supports_avx512_vbmi()) {\n-        return false; \/\/ Implementation limitation\n-      } else if ((bt == T_INT || bt == T_FLOAT) && size_in_bits < 256) {\n-        return false; \/\/ Implementation limitation\n-      }\n-      return true;\n+instruct compL_rReg(rFlagsReg cr, rRegL op1, rRegL op2)\n+%{\n+  match(Set cr (CmpL op1 op2));\n@@ -2058,8 +16268,6 @@\n-    \/\/ Binary Logical operations\n-    case Op_AndVMask:\n-    case Op_OrVMask:\n-    case Op_XorVMask:\n-      if (vlen > 16 && !VM_Version::supports_avx512bw()) {\n-        return false; \/\/ Implementation limitation\n-      }\n-      return true;\n+  format %{ \"cmpq    $op1, $op2\" %}\n+  ins_encode %{\n+    __ cmpq($op1$$Register, $op2$$Register);\n+  %}\n+  ins_pipe(ialu_cr_reg_reg);\n+%}\n@@ -2067,6 +16275,3 @@\n-    case Op_PopCountVI:\n-    case Op_PopCountVL:\n-      if (!is_pop_count_instr_target(bt)) {\n-        return false;\n-      }\n-      return true;\n+instruct compL_rReg_imm(rFlagsReg cr, rRegL op1, immL32 op2)\n+%{\n+  match(Set cr (CmpL op1 op2));\n@@ -2074,2 +16279,6 @@\n-    case Op_MaskAll:\n-      return true;\n+  format %{ \"cmpq    $op1, $op2\" %}\n+  ins_encode %{\n+    __ cmpq($op1$$Register, $op2$$constant);\n+  %}\n+  ins_pipe(ialu_cr_reg_imm);\n+%}\n@@ -2077,8 +16286,3 @@\n-    case Op_CountLeadingZerosV:\n-      if (is_non_subword_integral_type(bt) && VM_Version::supports_avx512cd()) {\n-        return true;\n-      }\n-    default:\n-      return false;\n-  }\n-}\n+instruct compL_rReg_mem(rFlagsReg cr, rRegL op1, memory op2)\n+%{\n+  match(Set cr (CmpL op1 (LoadL op2)));\n@@ -2086,3 +16290,6 @@\n-bool Matcher::vector_needs_partial_operations(Node* node, const TypeVect* vt) {\n-  return false;\n-}\n+  format %{ \"cmpq    $op1, $op2\" %}\n+  ins_encode %{\n+    __ cmpq($op1$$Register, $op2$$Address);\n+  %}\n+  ins_pipe(ialu_cr_reg_mem);\n+%}\n@@ -2090,12 +16297,3 @@\n-\/\/ Return true if Vector::rearrange needs preparation of the shuffle argument\n-bool Matcher::vector_rearrange_requires_load_shuffle(BasicType elem_bt, int vlen) {\n-  switch (elem_bt) {\n-    case T_BYTE:  return false;\n-    case T_SHORT: return !VM_Version::supports_avx512bw();\n-    case T_INT:   return !VM_Version::supports_avx();\n-    case T_LONG:  return vlen < 8 && !VM_Version::supports_avx512vl();\n-    default:\n-      ShouldNotReachHere();\n-      return false;\n-  }\n-}\n+instruct testL_reg(rFlagsReg cr, rRegL src, immL0 zero)\n+%{\n+  match(Set cr (CmpL src zero));\n@@ -2103,28 +16301,50 @@\n-MachOper* Matcher::pd_specialize_generic_vector_operand(MachOper* generic_opnd, uint ideal_reg, bool is_temp) {\n-  assert(Matcher::is_generic_vector(generic_opnd), \"not generic\");\n-  bool legacy = (generic_opnd->opcode() == LEGVEC);\n-  if (!VM_Version::supports_avx512vlbwdq() && \/\/ KNL\n-      is_temp && !legacy && (ideal_reg == Op_VecZ)) {\n-    \/\/ Conservatively specialize 512bit vec TEMP operands to legVecZ (zmm0-15) on KNL.\n-    return new legVecZOper();\n-  }\n-  if (legacy) {\n-    switch (ideal_reg) {\n-      case Op_VecS: return new legVecSOper();\n-      case Op_VecD: return new legVecDOper();\n-      case Op_VecX: return new legVecXOper();\n-      case Op_VecY: return new legVecYOper();\n-      case Op_VecZ: return new legVecZOper();\n-    }\n-  } else {\n-    switch (ideal_reg) {\n-      case Op_VecS: return new vecSOper();\n-      case Op_VecD: return new vecDOper();\n-      case Op_VecX: return new vecXOper();\n-      case Op_VecY: return new vecYOper();\n-      case Op_VecZ: return new vecZOper();\n-    }\n-  }\n-  ShouldNotReachHere();\n-  return nullptr;\n-}\n+  format %{ \"testq   $src, $src\" %}\n+  ins_encode %{\n+    __ testq($src$$Register, $src$$Register);\n+  %}\n+  ins_pipe(ialu_cr_reg_imm);\n+%}\n+\n+instruct testL_reg_imm(rFlagsReg cr, rRegL src, immL32 con, immL0 zero)\n+%{\n+  match(Set cr (CmpL (AndL src con) zero));\n+\n+  format %{ \"testq   $src, $con\\t# long\" %}\n+  ins_encode %{\n+    __ testq($src$$Register, $con$$constant);\n+  %}\n+  ins_pipe(ialu_cr_reg_imm);\n+%}\n+\n+instruct testL_reg_reg(rFlagsReg cr, rRegL src1, rRegL src2, immL0 zero)\n+%{\n+  match(Set cr (CmpL (AndL src1 src2) zero));\n+\n+  format %{ \"testq   $src1, $src2\\t# long\" %}\n+  ins_encode %{\n+    __ testq($src1$$Register, $src2$$Register);\n+  %}\n+  ins_pipe(ialu_cr_reg_imm);\n+%}\n+\n+instruct testL_reg_mem(rFlagsReg cr, rRegL src, memory mem, immL0 zero)\n+%{\n+  match(Set cr (CmpL (AndL src (LoadL mem)) zero));\n+\n+  format %{ \"testq   $src, $mem\" %}\n+  ins_encode %{\n+    __ testq($src$$Register, $mem$$Address);\n+  %}\n+  ins_pipe(ialu_cr_reg_mem);\n+%}\n+\n+instruct testL_reg_mem2(rFlagsReg cr, rRegP src, memory mem, immL0 zero)\n+%{\n+  match(Set cr (CmpL (AndL (CastP2X src) (LoadL mem)) zero));\n+\n+  format %{ \"testq   $src, $mem\" %}\n+  ins_encode %{\n+    __ testq($src$$Register, $mem$$Address);\n+  %}\n+  ins_pipe(ialu_cr_reg_mem);\n+%}\n@@ -2132,17 +16352,23 @@\n-bool Matcher::is_reg2reg_move(MachNode* m) {\n-  switch (m->rule()) {\n-    case MoveVec2Leg_rule:\n-    case MoveLeg2Vec_rule:\n-    case MoveF2VL_rule:\n-    case MoveF2LEG_rule:\n-    case MoveVL2F_rule:\n-    case MoveLEG2F_rule:\n-    case MoveD2VL_rule:\n-    case MoveD2LEG_rule:\n-    case MoveVL2D_rule:\n-    case MoveLEG2D_rule:\n-      return true;\n-    default:\n-      return false;\n-  }\n-}\n+\/\/ Manifest a CmpU result in an integer register.  Very painful.\n+\/\/ This is the test to avoid.\n+instruct cmpU3_reg_reg(rRegI dst, rRegI src1, rRegI src2, rFlagsReg flags)\n+%{\n+  match(Set dst (CmpU3 src1 src2));\n+  effect(KILL flags);\n+\n+  ins_cost(275); \/\/ XXX\n+  format %{ \"cmpl    $src1, $src2\\t# CmpL3\\n\\t\"\n+            \"movl    $dst, -1\\n\\t\"\n+            \"jb,u    done\\n\\t\"\n+            \"setcc   $dst \\t# emits setne + movzbl or setzune for APX\"\n+    \"done:\" %}\n+  ins_encode %{\n+    Label done;\n+    __ cmpl($src1$$Register, $src2$$Register);\n+    __ movl($dst$$Register, -1);\n+    __ jccb(Assembler::below, done);\n+    __ setcc(Assembler::notZero, $dst$$Register);\n+    __ bind(done);\n+  %}\n+  ins_pipe(pipe_slow);\n+%}\n@@ -2150,9 +16376,23 @@\n-bool Matcher::is_generic_vector(MachOper* opnd) {\n-  switch (opnd->opcode()) {\n-    case VEC:\n-    case LEGVEC:\n-      return true;\n-    default:\n-      return false;\n-  }\n-}\n+\/\/ Manifest a CmpL result in an integer register.  Very painful.\n+\/\/ This is the test to avoid.\n+instruct cmpL3_reg_reg(rRegI dst, rRegL src1, rRegL src2, rFlagsReg flags)\n+%{\n+  match(Set dst (CmpL3 src1 src2));\n+  effect(KILL flags);\n+\n+  ins_cost(275); \/\/ XXX\n+  format %{ \"cmpq    $src1, $src2\\t# CmpL3\\n\\t\"\n+            \"movl    $dst, -1\\n\\t\"\n+            \"jl,s    done\\n\\t\"\n+            \"setcc   $dst \\t# emits setne + movzbl or setzune for APX\"\n+    \"done:\" %}\n+  ins_encode %{\n+    Label done;\n+    __ cmpq($src1$$Register, $src2$$Register);\n+    __ movl($dst$$Register, -1);\n+    __ jccb(Assembler::less, done);\n+    __ setcc(Assembler::notZero, $dst$$Register);\n+    __ bind(done);\n+  %}\n+  ins_pipe(pipe_slow);\n+%}\n@@ -2160,1 +16400,23 @@\n-\/\/------------------------------------------------------------------------\n+\/\/ Manifest a CmpUL result in an integer register.  Very painful.\n+\/\/ This is the test to avoid.\n+instruct cmpUL3_reg_reg(rRegI dst, rRegL src1, rRegL src2, rFlagsReg flags)\n+%{\n+  match(Set dst (CmpUL3 src1 src2));\n+  effect(KILL flags);\n+\n+  ins_cost(275); \/\/ XXX\n+  format %{ \"cmpq    $src1, $src2\\t# CmpL3\\n\\t\"\n+            \"movl    $dst, -1\\n\\t\"\n+            \"jb,u    done\\n\\t\"\n+            \"setcc   $dst \\t# emits setne + movzbl or setzune for APX\"\n+    \"done:\" %}\n+  ins_encode %{\n+    Label done;\n+    __ cmpq($src1$$Register, $src2$$Register);\n+    __ movl($dst$$Register, -1);\n+    __ jccb(Assembler::below, done);\n+    __ setcc(Assembler::notZero, $dst$$Register);\n+    __ bind(done);\n+  %}\n+  ins_pipe(pipe_slow);\n+%}\n@@ -2162,3 +16424,5 @@\n-const RegMask* Matcher::predicate_reg_mask(void) {\n-  return &_VECTMASK_REG_mask;\n-}\n+\/\/ Unsigned long compare Instructions; really, same as signed long except they\n+\/\/ produce an rFlagsRegU instead of rFlagsReg.\n+instruct compUL_rReg(rFlagsRegU cr, rRegL op1, rRegL op2)\n+%{\n+  match(Set cr (CmpUL op1 op2));\n@@ -2166,41 +16430,6 @@\n-\/\/ Max vector size in bytes. 0 if not supported.\n-int Matcher::vector_width_in_bytes(BasicType bt) {\n-  assert(is_java_primitive(bt), \"only primitive type vectors\");\n-  \/\/ SSE2 supports 128bit vectors for all types.\n-  \/\/ AVX2 supports 256bit vectors for all types.\n-  \/\/ AVX2\/EVEX supports 512bit vectors for all types.\n-  int size = (UseAVX > 1) ? (1 << UseAVX) * 8 : 16;\n-  \/\/ AVX1 supports 256bit vectors only for FLOAT and DOUBLE.\n-  if (UseAVX > 0 && (bt == T_FLOAT || bt == T_DOUBLE))\n-    size = (UseAVX > 2) ? 64 : 32;\n-  if (UseAVX > 2 && (bt == T_BYTE || bt == T_SHORT || bt == T_CHAR))\n-    size = (VM_Version::supports_avx512bw()) ? 64 : 32;\n-  \/\/ Use flag to limit vector size.\n-  size = MIN2(size,(int)MaxVectorSize);\n-  \/\/ Minimum 2 values in vector (or 4 for bytes).\n-  switch (bt) {\n-  case T_DOUBLE:\n-  case T_LONG:\n-    if (size < 16) return 0;\n-    break;\n-  case T_FLOAT:\n-  case T_INT:\n-    if (size < 8) return 0;\n-    break;\n-  case T_BOOLEAN:\n-    if (size < 4) return 0;\n-    break;\n-  case T_CHAR:\n-    if (size < 4) return 0;\n-    break;\n-  case T_BYTE:\n-    if (size < 4) return 0;\n-    break;\n-  case T_SHORT:\n-    if (size < 4) return 0;\n-    break;\n-  default:\n-    ShouldNotReachHere();\n-  }\n-  return size;\n-}\n+  format %{ \"cmpq    $op1, $op2\\t# unsigned\" %}\n+  ins_encode %{\n+    __ cmpq($op1$$Register, $op2$$Register);\n+  %}\n+  ins_pipe(ialu_cr_reg_reg);\n+%}\n@@ -2208,14 +16437,3 @@\n-\/\/ Limits on vector size (number of elements) loaded into vector.\n-int Matcher::max_vector_size(const BasicType bt) {\n-  return vector_width_in_bytes(bt)\/type2aelembytes(bt);\n-}\n-int Matcher::min_vector_size(const BasicType bt) {\n-  int max_size = max_vector_size(bt);\n-  \/\/ Min size which can be loaded into vector is 4 bytes.\n-  int size = (type2aelembytes(bt) == 1) ? 4 : 2;\n-  \/\/ Support for calling svml double64 vectors\n-  if (bt == T_DOUBLE) {\n-    size = 1;\n-  }\n-  return MIN2(size,max_size);\n-}\n+instruct compUL_rReg_imm(rFlagsRegU cr, rRegL op1, immL32 op2)\n+%{\n+  match(Set cr (CmpUL op1 op2));\n@@ -2223,8 +16441,6 @@\n-int Matcher::max_vector_size_auto_vectorization(const BasicType bt) {\n-  \/\/ Limit the max vector size for auto vectorization to 256 bits (32 bytes)\n-  \/\/ by default on Cascade Lake\n-  if (VM_Version::is_default_intel_cascade_lake()) {\n-    return MIN2(Matcher::max_vector_size(bt), 32 \/ type2aelembytes(bt));\n-  }\n-  return Matcher::max_vector_size(bt);\n-}\n+  format %{ \"cmpq    $op1, $op2\\t# unsigned\" %}\n+  ins_encode %{\n+    __ cmpq($op1$$Register, $op2$$constant);\n+  %}\n+  ins_pipe(ialu_cr_reg_imm);\n+%}\n@@ -2232,3 +16448,3 @@\n-int Matcher::scalable_vector_reg_size(const BasicType bt) {\n-  return -1;\n-}\n+instruct compUL_rReg_mem(rFlagsRegU cr, rRegL op1, memory op2)\n+%{\n+  match(Set cr (CmpUL op1 (LoadL op2)));\n@@ -2236,13 +16452,6 @@\n-\/\/ Vector ideal reg corresponding to specified size in bytes\n-uint Matcher::vector_ideal_reg(int size) {\n-  assert(MaxVectorSize >= size, \"\");\n-  switch(size) {\n-    case  4: return Op_VecS;\n-    case  8: return Op_VecD;\n-    case 16: return Op_VecX;\n-    case 32: return Op_VecY;\n-    case 64: return Op_VecZ;\n-  }\n-  ShouldNotReachHere();\n-  return 0;\n-}\n+  format %{ \"cmpq    $op1, $op2\\t# unsigned\" %}\n+  ins_encode %{\n+    __ cmpq($op1$$Register, $op2$$Address);\n+  %}\n+  ins_pipe(ialu_cr_reg_mem);\n+%}\n@@ -2250,25 +16459,3 @@\n-\/\/ Check for shift by small constant as well\n-static bool clone_shift(Node* shift, Matcher* matcher, Matcher::MStack& mstack, VectorSet& address_visited) {\n-  if (shift->Opcode() == Op_LShiftX && shift->in(2)->is_Con() &&\n-      shift->in(2)->get_int() <= 3 &&\n-      \/\/ Are there other uses besides address expressions?\n-      !matcher->is_visited(shift)) {\n-    address_visited.set(shift->_idx); \/\/ Flag as address_visited\n-    mstack.push(shift->in(2), Matcher::Visit);\n-    Node *conv = shift->in(1);\n-    \/\/ Allow Matcher to match the rule which bypass\n-    \/\/ ConvI2L operation for an array index on LP64\n-    \/\/ if the index value is positive.\n-    if (conv->Opcode() == Op_ConvI2L &&\n-        conv->as_Type()->type()->is_long()->_lo >= 0 &&\n-        \/\/ Are there other uses besides address expressions?\n-        !matcher->is_visited(conv)) {\n-      address_visited.set(conv->_idx); \/\/ Flag as address_visited\n-      mstack.push(conv->in(1), Matcher::Pre_Visit);\n-    } else {\n-      mstack.push(conv, Matcher::Pre_Visit);\n-    }\n-    return true;\n-  }\n-  return false;\n-}\n+instruct testUL_reg(rFlagsRegU cr, rRegL src, immL0 zero)\n+%{\n+  match(Set cr (CmpUL src zero));\n@@ -2276,14 +16463,6 @@\n-\/\/ This function identifies sub-graphs in which a 'load' node is\n-\/\/ input to two different nodes, and such that it can be matched\n-\/\/ with BMI instructions like blsi, blsr, etc.\n-\/\/ Example : for b = -a[i] & a[i] can be matched to blsi r32, m32.\n-\/\/ The graph is (AndL (SubL Con0 LoadL*) LoadL*), where LoadL*\n-\/\/ refers to the same node.\n-\/\/\n-\/\/ Match the generic fused operations pattern (op1 (op2 Con{ConType} mop) mop)\n-\/\/ This is a temporary solution until we make DAGs expressible in ADL.\n-template<typename ConType>\n-class FusedPatternMatcher {\n-  Node* _op1_node;\n-  Node* _mop_node;\n-  int _con_op;\n+  format %{ \"testq   $src, $src\\t# unsigned\" %}\n+  ins_encode %{\n+    __ testq($src$$Register, $src$$Register);\n+  %}\n+  ins_pipe(ialu_cr_reg_imm);\n+%}\n@@ -2291,4 +16470,3 @@\n-  static int match_next(Node* n, int next_op, int next_op_idx) {\n-    if (n->in(1) == nullptr || n->in(2) == nullptr) {\n-      return -1;\n-    }\n+instruct compB_mem_imm(rFlagsReg cr, memory mem, immI8 imm)\n+%{\n+  match(Set cr (CmpI (LoadB mem) imm));\n@@ -2296,14 +16474,5 @@\n-    if (next_op_idx == -1) { \/\/ n is commutative, try rotations\n-      if (n->in(1)->Opcode() == next_op) {\n-        return 1;\n-      } else if (n->in(2)->Opcode() == next_op) {\n-        return 2;\n-      }\n-    } else {\n-      assert(next_op_idx > 0 && next_op_idx <= 2, \"Bad argument index\");\n-      if (n->in(next_op_idx)->Opcode() == next_op) {\n-        return next_op_idx;\n-      }\n-    }\n-    return -1;\n-  }\n+  ins_cost(125);\n+  format %{ \"cmpb    $mem, $imm\" %}\n+  ins_encode %{ __ cmpb($mem$$Address, $imm$$constant); %}\n+  ins_pipe(ialu_cr_reg_mem);\n+%}\n@@ -2311,3 +16480,3 @@\n- public:\n-  FusedPatternMatcher(Node* op1_node, Node* mop_node, int con_op) :\n-    _op1_node(op1_node), _mop_node(mop_node), _con_op(con_op) { }\n+instruct testUB_mem_imm(rFlagsReg cr, memory mem, immU7 imm, immI_0 zero)\n+%{\n+  match(Set cr (CmpI (AndI (LoadUB mem) imm) zero));\n@@ -2315,15 +16484,5 @@\n-  bool match(int op1, int op1_op2_idx,  \/\/ op1 and the index of the op1->op2 edge, -1 if op1 is commutative\n-             int op2, int op2_con_idx,  \/\/ op2 and the index of the op2->con edge, -1 if op2 is commutative\n-             typename ConType::NativeType con_value) {\n-    if (_op1_node->Opcode() != op1) {\n-      return false;\n-    }\n-    if (_mop_node->outcnt() > 2) {\n-      return false;\n-    }\n-    op1_op2_idx = match_next(_op1_node, op2, op1_op2_idx);\n-    if (op1_op2_idx == -1) {\n-      return false;\n-    }\n-    \/\/ Memory operation must be the other edge\n-    int op1_mop_idx = (op1_op2_idx & 1) + 1;\n+  ins_cost(125);\n+  format %{ \"testb   $mem, $imm\\t# ubyte\" %}\n+  ins_encode %{ __ testb($mem$$Address, $imm$$constant); %}\n+  ins_pipe(ialu_cr_reg_mem);\n+%}\n@@ -2331,25 +16490,24 @@\n-    \/\/ Check that the mop node is really what we want\n-    if (_op1_node->in(op1_mop_idx) == _mop_node) {\n-      Node* op2_node = _op1_node->in(op1_op2_idx);\n-      if (op2_node->outcnt() > 1) {\n-        return false;\n-      }\n-      assert(op2_node->Opcode() == op2, \"Should be\");\n-      op2_con_idx = match_next(op2_node, _con_op, op2_con_idx);\n-      if (op2_con_idx == -1) {\n-        return false;\n-      }\n-      \/\/ Memory operation must be the other edge\n-      int op2_mop_idx = (op2_con_idx & 1) + 1;\n-      \/\/ Check that the memory operation is the same node\n-      if (op2_node->in(op2_mop_idx) == _mop_node) {\n-        \/\/ Now check the constant\n-        const Type* con_type = op2_node->in(op2_con_idx)->bottom_type();\n-        if (con_type != Type::TOP && ConType::as_self(con_type)->get_con() == con_value) {\n-          return true;\n-        }\n-      }\n-    }\n-    return false;\n-  }\n-};\n+instruct testB_mem_imm(rFlagsReg cr, memory mem, immI8 imm, immI_0 zero)\n+%{\n+  match(Set cr (CmpI (AndI (LoadB mem) imm) zero));\n+\n+  ins_cost(125);\n+  format %{ \"testb   $mem, $imm\\t# byte\" %}\n+  ins_encode %{ __ testb($mem$$Address, $imm$$constant); %}\n+  ins_pipe(ialu_cr_reg_mem);\n+%}\n+\n+\/\/----------Max and Min--------------------------------------------------------\n+\/\/ Min Instructions\n+\n+instruct cmovI_reg_g(rRegI dst, rRegI src, rFlagsReg cr)\n+%{\n+  predicate(!UseAPX);\n+  effect(USE_DEF dst, USE src, USE cr);\n+\n+  format %{ \"cmovlgt $dst, $src\\t# min\" %}\n+  ins_encode %{\n+    __ cmovl(Assembler::greater, $dst$$Register, $src$$Register);\n+  %}\n+  ins_pipe(pipe_cmov_reg);\n+%}\n@@ -2357,17 +16515,4 @@\n-static bool is_bmi_pattern(Node* n, Node* m) {\n-  assert(UseBMI1Instructions, \"sanity\");\n-  if (n != nullptr && m != nullptr) {\n-    if (m->Opcode() == Op_LoadI) {\n-      FusedPatternMatcher<TypeInt> bmii(n, m, Op_ConI);\n-      return bmii.match(Op_AndI, -1, Op_SubI,  1,  0)  ||\n-             bmii.match(Op_AndI, -1, Op_AddI, -1, -1)  ||\n-             bmii.match(Op_XorI, -1, Op_AddI, -1, -1);\n-    } else if (m->Opcode() == Op_LoadL) {\n-      FusedPatternMatcher<TypeLong> bmil(n, m, Op_ConL);\n-      return bmil.match(Op_AndL, -1, Op_SubL,  1,  0) ||\n-             bmil.match(Op_AndL, -1, Op_AddL, -1, -1) ||\n-             bmil.match(Op_XorL, -1, Op_AddL, -1, -1);\n-    }\n-  }\n-  return false;\n-}\n+instruct cmovI_reg_g_ndd(rRegI dst, rRegI src1, rRegI src2, rFlagsReg cr)\n+%{\n+  predicate(UseAPX);\n+  effect(DEF dst, USE src1, USE src2, USE cr);\n@@ -2375,17 +16520,6 @@\n-\/\/ Should the matcher clone input 'm' of node 'n'?\n-bool Matcher::pd_clone_node(Node* n, Node* m, Matcher::MStack& mstack) {\n-  \/\/ If 'n' and 'm' are part of a graph for BMI instruction, clone the input 'm'.\n-  if (UseBMI1Instructions && is_bmi_pattern(n, m)) {\n-    mstack.push(m, Visit);\n-    return true;\n-  }\n-  if (is_vshift_con_pattern(n, m)) { \/\/ ShiftV src (ShiftCntV con)\n-    mstack.push(m, Visit);           \/\/ m = ShiftCntV\n-    return true;\n-  }\n-  if (is_encode_and_store_pattern(n, m)) {\n-    mstack.push(m, Visit);\n-    return true;\n-  }\n-  return false;\n-}\n+  format %{ \"ecmovlgt $dst, $src1, $src2\\t# min ndd\" %}\n+  ins_encode %{\n+    __ ecmovl(Assembler::greater, $dst$$Register, $src1$$Register, $src2$$Register);\n+  %}\n+  ins_pipe(pipe_cmov_reg);\n+%}\n@@ -2393,8 +16527,4 @@\n-\/\/ Should the Matcher clone shifts on addressing modes, expecting them\n-\/\/ to be subsumed into complex addressing expressions or compute them\n-\/\/ into registers?\n-bool Matcher::pd_clone_address_expressions(AddPNode* m, Matcher::MStack& mstack, VectorSet& address_visited) {\n-  Node *off = m->in(AddPNode::Offset);\n-  if (off->is_Con()) {\n-    address_visited.test_set(m->_idx); \/\/ Flag as address_visited\n-    Node *adr = m->in(AddPNode::Address);\n+instruct minI_rReg(rRegI dst, rRegI src)\n+%{\n+  predicate(!UseAPX);\n+  match(Set dst (MinI dst src));\n@@ -2402,19 +16532,7 @@\n-    \/\/ Intel can handle 2 adds in addressing mode, with one of them using an immediate offset.\n-    \/\/ AtomicAdd is not an addressing expression.\n-    \/\/ Cheap to find it by looking for screwy base.\n-    if (adr->is_AddP() &&\n-        !adr->in(AddPNode::Base)->is_top() &&\n-        !adr->in(AddPNode::Offset)->is_Con() &&\n-        off->get_long() == (int) (off->get_long()) && \/\/ immL32\n-        \/\/ Are there other uses besides address expressions?\n-        !is_visited(adr)) {\n-      address_visited.set(adr->_idx); \/\/ Flag as address_visited\n-      Node *shift = adr->in(AddPNode::Offset);\n-      if (!clone_shift(shift, this, mstack, address_visited)) {\n-        mstack.push(shift, Pre_Visit);\n-      }\n-      mstack.push(adr->in(AddPNode::Address), Pre_Visit);\n-      mstack.push(adr->in(AddPNode::Base), Pre_Visit);\n-    } else {\n-      mstack.push(adr, Pre_Visit);\n-    }\n+  ins_cost(200);\n+  expand %{\n+    rFlagsReg cr;\n+    compI_rReg(cr, dst, src);\n+    cmovI_reg_g(dst, src, cr);\n+  %}\n+%}\n@@ -2422,12 +16540,5 @@\n-    \/\/ Clone X+offset as it also folds into most addressing expressions\n-    mstack.push(off, Visit);\n-    mstack.push(m->in(AddPNode::Base), Pre_Visit);\n-    return true;\n-  } else if (clone_shift(off, this, mstack, address_visited)) {\n-    address_visited.test_set(m->_idx); \/\/ Flag as address_visited\n-    mstack.push(m->in(AddPNode::Address), Pre_Visit);\n-    mstack.push(m->in(AddPNode::Base), Pre_Visit);\n-    return true;\n-  }\n-  return false;\n-}\n+instruct minI_rReg_ndd(rRegI dst, rRegI src1, rRegI src2)\n+%{\n+  predicate(UseAPX);\n+  match(Set dst (MinI src1 src2));\n+  effect(DEF dst, USE src1, USE src2);\n@@ -2435,21 +16546,7 @@\n-static inline Assembler::ComparisonPredicate booltest_pred_to_comparison_pred(int bt) {\n-  switch (bt) {\n-    case BoolTest::eq:\n-      return Assembler::eq;\n-    case BoolTest::ne:\n-      return Assembler::neq;\n-    case BoolTest::le:\n-    case BoolTest::ule:\n-      return Assembler::le;\n-    case BoolTest::ge:\n-    case BoolTest::uge:\n-      return Assembler::nlt;\n-    case BoolTest::lt:\n-    case BoolTest::ult:\n-      return Assembler::lt;\n-    case BoolTest::gt:\n-    case BoolTest::ugt:\n-      return Assembler::nle;\n-    default : ShouldNotReachHere(); return Assembler::_false;\n-  }\n-}\n+  ins_cost(200);\n+  expand %{\n+    rFlagsReg cr;\n+    compI_rReg(cr, src1, src2);\n+    cmovI_reg_g_ndd(dst, src1, src2, cr);\n+  %}\n+%}\n@@ -2457,12 +16554,4 @@\n-static inline Assembler::ComparisonPredicateFP booltest_pred_to_comparison_pred_fp(int bt) {\n-  switch (bt) {\n-  case BoolTest::eq: return Assembler::EQ_OQ;  \/\/ ordered non-signaling\n-  \/\/ As per JLS 15.21.1, != of NaNs is true. Thus use unordered compare.\n-  case BoolTest::ne: return Assembler::NEQ_UQ; \/\/ unordered non-signaling\n-  case BoolTest::le: return Assembler::LE_OQ;  \/\/ ordered non-signaling\n-  case BoolTest::ge: return Assembler::GE_OQ;  \/\/ ordered non-signaling\n-  case BoolTest::lt: return Assembler::LT_OQ;  \/\/ ordered non-signaling\n-  case BoolTest::gt: return Assembler::GT_OQ;  \/\/ ordered non-signaling\n-  default: ShouldNotReachHere(); return Assembler::FALSE_OS;\n-  }\n-}\n+instruct cmovI_reg_l(rRegI dst, rRegI src, rFlagsReg cr)\n+%{\n+  predicate(!UseAPX);\n+  effect(USE_DEF dst, USE src, USE cr);\n@@ -2470,49 +16559,6 @@\n-\/\/ Helper methods for MachSpillCopyNode::implementation().\n-static void vec_mov_helper(C2_MacroAssembler *masm, int src_lo, int dst_lo,\n-                          int src_hi, int dst_hi, uint ireg, outputStream* st) {\n-  assert(ireg == Op_VecS || \/\/ 32bit vector\n-         ((src_lo & 1) == 0 && (src_lo + 1) == src_hi &&\n-          (dst_lo & 1) == 0 && (dst_lo + 1) == dst_hi),\n-         \"no non-adjacent vector moves\" );\n-  if (masm) {\n-    switch (ireg) {\n-    case Op_VecS: \/\/ copy whole register\n-    case Op_VecD:\n-    case Op_VecX:\n-      if ((UseAVX < 3) || VM_Version::supports_avx512vl()) {\n-        __ movdqu(as_XMMRegister(Matcher::_regEncode[dst_lo]), as_XMMRegister(Matcher::_regEncode[src_lo]));\n-      } else {\n-        __ vextractf32x4(as_XMMRegister(Matcher::_regEncode[dst_lo]), as_XMMRegister(Matcher::_regEncode[src_lo]), 0x0);\n-     }\n-      break;\n-    case Op_VecY:\n-      if ((UseAVX < 3) || VM_Version::supports_avx512vl()) {\n-        __ vmovdqu(as_XMMRegister(Matcher::_regEncode[dst_lo]), as_XMMRegister(Matcher::_regEncode[src_lo]));\n-      } else {\n-        __ vextractf64x4(as_XMMRegister(Matcher::_regEncode[dst_lo]), as_XMMRegister(Matcher::_regEncode[src_lo]), 0x0);\n-     }\n-      break;\n-    case Op_VecZ:\n-      __ evmovdquq(as_XMMRegister(Matcher::_regEncode[dst_lo]), as_XMMRegister(Matcher::_regEncode[src_lo]), 2);\n-      break;\n-    default:\n-      ShouldNotReachHere();\n-    }\n-#ifndef PRODUCT\n-  } else {\n-    switch (ireg) {\n-    case Op_VecS:\n-    case Op_VecD:\n-    case Op_VecX:\n-      st->print(\"movdqu  %s,%s\\t# spill\",Matcher::regName[dst_lo],Matcher::regName[src_lo]);\n-      break;\n-    case Op_VecY:\n-    case Op_VecZ:\n-      st->print(\"vmovdqu %s,%s\\t# spill\",Matcher::regName[dst_lo],Matcher::regName[src_lo]);\n-      break;\n-    default:\n-      ShouldNotReachHere();\n-    }\n-#endif\n-  }\n-}\n+  format %{ \"cmovllt $dst, $src\\t# max\" %}\n+  ins_encode %{\n+    __ cmovl(Assembler::less, $dst$$Register, $src$$Register);\n+  %}\n+  ins_pipe(pipe_cmov_reg);\n+%}\n@@ -2520,63 +16566,132 @@\n-void vec_spill_helper(C2_MacroAssembler *masm, bool is_load,\n-                     int stack_offset, int reg, uint ireg, outputStream* st) {\n-  if (masm) {\n-    if (is_load) {\n-      switch (ireg) {\n-      case Op_VecS:\n-        __ movdl(as_XMMRegister(Matcher::_regEncode[reg]), Address(rsp, stack_offset));\n-        break;\n-      case Op_VecD:\n-        __ movq(as_XMMRegister(Matcher::_regEncode[reg]), Address(rsp, stack_offset));\n-        break;\n-      case Op_VecX:\n-        if ((UseAVX < 3) || VM_Version::supports_avx512vl()) {\n-          __ movdqu(as_XMMRegister(Matcher::_regEncode[reg]), Address(rsp, stack_offset));\n-        } else {\n-          __ vpxor(as_XMMRegister(Matcher::_regEncode[reg]), as_XMMRegister(Matcher::_regEncode[reg]), as_XMMRegister(Matcher::_regEncode[reg]), 2);\n-          __ vinsertf32x4(as_XMMRegister(Matcher::_regEncode[reg]), as_XMMRegister(Matcher::_regEncode[reg]), Address(rsp, stack_offset),0x0);\n-        }\n-        break;\n-      case Op_VecY:\n-        if ((UseAVX < 3) || VM_Version::supports_avx512vl()) {\n-          __ vmovdqu(as_XMMRegister(Matcher::_regEncode[reg]), Address(rsp, stack_offset));\n-        } else {\n-          __ vpxor(as_XMMRegister(Matcher::_regEncode[reg]), as_XMMRegister(Matcher::_regEncode[reg]), as_XMMRegister(Matcher::_regEncode[reg]), 2);\n-          __ vinsertf64x4(as_XMMRegister(Matcher::_regEncode[reg]), as_XMMRegister(Matcher::_regEncode[reg]), Address(rsp, stack_offset),0x0);\n-        }\n-        break;\n-      case Op_VecZ:\n-        __ evmovdquq(as_XMMRegister(Matcher::_regEncode[reg]), Address(rsp, stack_offset), 2);\n-        break;\n-      default:\n-        ShouldNotReachHere();\n-      }\n-    } else { \/\/ store\n-      switch (ireg) {\n-      case Op_VecS:\n-        __ movdl(Address(rsp, stack_offset), as_XMMRegister(Matcher::_regEncode[reg]));\n-        break;\n-      case Op_VecD:\n-        __ movq(Address(rsp, stack_offset), as_XMMRegister(Matcher::_regEncode[reg]));\n-        break;\n-      case Op_VecX:\n-        if ((UseAVX < 3) || VM_Version::supports_avx512vl()) {\n-          __ movdqu(Address(rsp, stack_offset), as_XMMRegister(Matcher::_regEncode[reg]));\n-        }\n-        else {\n-          __ vextractf32x4(Address(rsp, stack_offset), as_XMMRegister(Matcher::_regEncode[reg]), 0x0);\n-        }\n-        break;\n-      case Op_VecY:\n-        if ((UseAVX < 3) || VM_Version::supports_avx512vl()) {\n-          __ vmovdqu(Address(rsp, stack_offset), as_XMMRegister(Matcher::_regEncode[reg]));\n-        }\n-        else {\n-          __ vextractf64x4(Address(rsp, stack_offset), as_XMMRegister(Matcher::_regEncode[reg]), 0x0);\n-        }\n-        break;\n-      case Op_VecZ:\n-        __ evmovdquq(Address(rsp, stack_offset), as_XMMRegister(Matcher::_regEncode[reg]), 2);\n-        break;\n-      default:\n-        ShouldNotReachHere();\n-      }\n+instruct cmovI_reg_l_ndd(rRegI dst, rRegI src1, rRegI src2, rFlagsReg cr)\n+%{\n+  predicate(UseAPX);\n+  effect(DEF dst, USE src1, USE src2, USE cr);\n+\n+  format %{ \"ecmovllt $dst, $src1, $src2\\t# max ndd\" %}\n+  ins_encode %{\n+    __ ecmovl(Assembler::less, $dst$$Register, $src1$$Register, $src2$$Register);\n+  %}\n+  ins_pipe(pipe_cmov_reg);\n+%}\n+\n+instruct maxI_rReg(rRegI dst, rRegI src)\n+%{\n+  predicate(!UseAPX);\n+  match(Set dst (MaxI dst src));\n+\n+  ins_cost(200);\n+  expand %{\n+    rFlagsReg cr;\n+    compI_rReg(cr, dst, src);\n+    cmovI_reg_l(dst, src, cr);\n+  %}\n+%}\n+\n+instruct maxI_rReg_ndd(rRegI dst, rRegI src1, rRegI src2)\n+%{\n+  predicate(UseAPX);\n+  match(Set dst (MaxI src1 src2));\n+  effect(DEF dst, USE src1, USE src2);\n+\n+  ins_cost(200);\n+  expand %{\n+    rFlagsReg cr;\n+    compI_rReg(cr, src1, src2);\n+    cmovI_reg_l_ndd(dst, src1, src2, cr);\n+  %}\n+%}\n+\n+\/\/ ============================================================================\n+\/\/ Branch Instructions\n+\n+\/\/ Jump Direct - Label defines a relative address from JMP+1\n+instruct jmpDir(label labl)\n+%{\n+  match(Goto);\n+  effect(USE labl);\n+\n+  ins_cost(300);\n+  format %{ \"jmp     $labl\" %}\n+  size(5);\n+  ins_encode %{\n+    Label* L = $labl$$label;\n+    __ jmp(*L, false); \/\/ Always long jump\n+  %}\n+  ins_pipe(pipe_jmp);\n+%}\n+\n+\/\/ Jump Direct Conditional - Label defines a relative address from Jcc+1\n+instruct jmpCon(cmpOp cop, rFlagsReg cr, label labl)\n+%{\n+  match(If cop cr);\n+  effect(USE labl);\n+\n+  ins_cost(300);\n+  format %{ \"j$cop     $labl\" %}\n+  size(6);\n+  ins_encode %{\n+    Label* L = $labl$$label;\n+    __ jcc((Assembler::Condition)($cop$$cmpcode), *L, false); \/\/ Always long jump\n+  %}\n+  ins_pipe(pipe_jcc);\n+%}\n+\n+\/\/ Jump Direct Conditional - Label defines a relative address from Jcc+1\n+instruct jmpLoopEnd(cmpOp cop, rFlagsReg cr, label labl)\n+%{\n+  match(CountedLoopEnd cop cr);\n+  effect(USE labl);\n+\n+  ins_cost(300);\n+  format %{ \"j$cop     $labl\\t# loop end\" %}\n+  size(6);\n+  ins_encode %{\n+    Label* L = $labl$$label;\n+    __ jcc((Assembler::Condition)($cop$$cmpcode), *L, false); \/\/ Always long jump\n+  %}\n+  ins_pipe(pipe_jcc);\n+%}\n+\n+\/\/ Jump Direct Conditional - using unsigned comparison\n+instruct jmpConU(cmpOpU cop, rFlagsRegU cmp, label labl) %{\n+  match(If cop cmp);\n+  effect(USE labl);\n+\n+  ins_cost(300);\n+  format %{ \"j$cop,u   $labl\" %}\n+  size(6);\n+  ins_encode %{\n+    Label* L = $labl$$label;\n+    __ jcc((Assembler::Condition)($cop$$cmpcode), *L, false); \/\/ Always long jump\n+  %}\n+  ins_pipe(pipe_jcc);\n+%}\n+\n+instruct jmpConUCF(cmpOpUCF cop, rFlagsRegUCF cmp, label labl) %{\n+  match(If cop cmp);\n+  effect(USE labl);\n+\n+  ins_cost(200);\n+  format %{ \"j$cop,u   $labl\" %}\n+  size(6);\n+  ins_encode %{\n+    Label* L = $labl$$label;\n+    __ jcc((Assembler::Condition)($cop$$cmpcode), *L, false); \/\/ Always long jump\n+  %}\n+  ins_pipe(pipe_jcc);\n+%}\n+\n+instruct jmpConUCF2(cmpOpUCF2 cop, rFlagsRegUCF cmp, label labl) %{\n+  match(If cop cmp);\n+  effect(USE labl);\n+\n+  ins_cost(200);\n+  format %{ $$template\n+    if ($cop$$cmpcode == Assembler::notEqual) {\n+      $$emit$$\"jp,u    $labl\\n\\t\"\n+      $$emit$$\"j$cop,u   $labl\"\n+    } else {\n+      $$emit$$\"jp,u    done\\n\\t\"\n+      $$emit$$\"j$cop,u   $labl\\n\\t\"\n+      $$emit$$\"done:\"\n@@ -2584,38 +16699,13 @@\n-#ifndef PRODUCT\n-  } else {\n-    if (is_load) {\n-      switch (ireg) {\n-      case Op_VecS:\n-        st->print(\"movd    %s,[rsp + %d]\\t# spill\", Matcher::regName[reg], stack_offset);\n-        break;\n-      case Op_VecD:\n-        st->print(\"movq    %s,[rsp + %d]\\t# spill\", Matcher::regName[reg], stack_offset);\n-        break;\n-       case Op_VecX:\n-        st->print(\"movdqu  %s,[rsp + %d]\\t# spill\", Matcher::regName[reg], stack_offset);\n-        break;\n-      case Op_VecY:\n-      case Op_VecZ:\n-        st->print(\"vmovdqu %s,[rsp + %d]\\t# spill\", Matcher::regName[reg], stack_offset);\n-        break;\n-      default:\n-        ShouldNotReachHere();\n-      }\n-    } else { \/\/ store\n-      switch (ireg) {\n-      case Op_VecS:\n-        st->print(\"movd    [rsp + %d],%s\\t# spill\", stack_offset, Matcher::regName[reg]);\n-        break;\n-      case Op_VecD:\n-        st->print(\"movq    [rsp + %d],%s\\t# spill\", stack_offset, Matcher::regName[reg]);\n-        break;\n-       case Op_VecX:\n-        st->print(\"movdqu  [rsp + %d],%s\\t# spill\", stack_offset, Matcher::regName[reg]);\n-        break;\n-      case Op_VecY:\n-      case Op_VecZ:\n-        st->print(\"vmovdqu [rsp + %d],%s\\t# spill\", stack_offset, Matcher::regName[reg]);\n-        break;\n-      default:\n-        ShouldNotReachHere();\n-      }\n+  %}\n+  ins_encode %{\n+    Label* l = $labl$$label;\n+    if ($cop$$cmpcode == Assembler::notEqual) {\n+      __ jcc(Assembler::parity, *l, false);\n+      __ jcc(Assembler::notEqual, *l, false);\n+    } else if ($cop$$cmpcode == Assembler::equal) {\n+      Label done;\n+      __ jccb(Assembler::parity, done);\n+      __ jcc(Assembler::equal, *l, false);\n+      __ bind(done);\n+    } else {\n+       ShouldNotReachHere();\n@@ -2623,3 +16713,3 @@\n-#endif\n-  }\n-}\n+  %}\n+  ins_pipe(pipe_jcc);\n+%}\n@@ -2627,34 +16717,87 @@\n-template <class T>\n-static inline GrowableArray<jbyte>* vreplicate_imm(BasicType bt, T con, int len) {\n-  int size = type2aelembytes(bt) * len;\n-  GrowableArray<jbyte>* val = new GrowableArray<jbyte>(size, size, 0);\n-  for (int i = 0; i < len; i++) {\n-    int offset = i * type2aelembytes(bt);\n-    switch (bt) {\n-      case T_BYTE: val->at(i) = con; break;\n-      case T_SHORT: {\n-        jshort c = con;\n-        memcpy(val->adr_at(offset), &c, sizeof(jshort));\n-        break;\n-      }\n-      case T_INT: {\n-        jint c = con;\n-        memcpy(val->adr_at(offset), &c, sizeof(jint));\n-        break;\n-      }\n-      case T_LONG: {\n-        jlong c = con;\n-        memcpy(val->adr_at(offset), &c, sizeof(jlong));\n-        break;\n-      }\n-      case T_FLOAT: {\n-        jfloat c = con;\n-        memcpy(val->adr_at(offset), &c, sizeof(jfloat));\n-        break;\n-      }\n-      case T_DOUBLE: {\n-        jdouble c = con;\n-        memcpy(val->adr_at(offset), &c, sizeof(jdouble));\n-        break;\n-      }\n-      default: assert(false, \"%s\", type2name(bt));\n+\/\/ ============================================================================\n+\/\/ The 2nd slow-half of a subtype check.  Scan the subklass's 2ndary\n+\/\/ superklass array for an instance of the superklass.  Set a hidden\n+\/\/ internal cache on a hit (cache is checked with exposed code in\n+\/\/ gen_subtype_check()).  Return NZ for a miss or zero for a hit.  The\n+\/\/ encoding ALSO sets flags.\n+\n+instruct partialSubtypeCheck(rdi_RegP result,\n+                             rsi_RegP sub, rax_RegP super, rcx_RegI rcx,\n+                             rFlagsReg cr)\n+%{\n+  match(Set result (PartialSubtypeCheck sub super));\n+  predicate(!UseSecondarySupersTable);\n+  effect(KILL rcx, KILL cr);\n+\n+  ins_cost(1100);  \/\/ slightly larger than the next version\n+  format %{ \"movq    rdi, [$sub + in_bytes(Klass::secondary_supers_offset())]\\n\\t\"\n+            \"movl    rcx, [rdi + Array<Klass*>::length_offset_in_bytes()]\\t# length to scan\\n\\t\"\n+            \"addq    rdi, Array<Klass*>::base_offset_in_bytes()\\t# Skip to start of data; set NZ in case count is zero\\n\\t\"\n+            \"repne   scasq\\t# Scan *rdi++ for a match with rax while rcx--\\n\\t\"\n+            \"jne,s   miss\\t\\t# Missed: rdi not-zero\\n\\t\"\n+            \"movq    [$sub + in_bytes(Klass::secondary_super_cache_offset())], $super\\t# Hit: update cache\\n\\t\"\n+            \"xorq    $result, $result\\t\\t Hit: rdi zero\\n\\t\"\n+    \"miss:\\t\" %}\n+\n+  ins_encode %{\n+    Label miss;\n+    \/\/ NB: Callers may assume that, when $result is a valid register,\n+    \/\/ check_klass_subtype_slow_path_linear sets it to a nonzero\n+    \/\/ value.\n+    __ check_klass_subtype_slow_path_linear($sub$$Register, $super$$Register,\n+                                            $rcx$$Register, $result$$Register,\n+                                            nullptr, &miss,\n+                                            \/*set_cond_codes:*\/ true);\n+    __ xorptr($result$$Register, $result$$Register);\n+    __ bind(miss);\n+  %}\n+\n+  ins_pipe(pipe_slow);\n+%}\n+\n+\/\/ ============================================================================\n+\/\/ Two versions of hashtable-based partialSubtypeCheck, both used when\n+\/\/ we need to search for a super class in the secondary supers array.\n+\/\/ The first is used when we don't know _a priori_ the class being\n+\/\/ searched for. The second, far more common, is used when we do know:\n+\/\/ this is used for instanceof, checkcast, and any case where C2 can\n+\/\/ determine it by constant propagation.\n+\n+instruct partialSubtypeCheckVarSuper(rsi_RegP sub, rax_RegP super, rdi_RegP result,\n+                                       rdx_RegL temp1, rcx_RegL temp2, rbx_RegP temp3, r11_RegL temp4,\n+                                       rFlagsReg cr)\n+%{\n+  match(Set result (PartialSubtypeCheck sub super));\n+  predicate(UseSecondarySupersTable);\n+  effect(KILL cr, TEMP temp1, TEMP temp2, TEMP temp3, TEMP temp4);\n+\n+  ins_cost(1000);\n+  format %{ \"partialSubtypeCheck $result, $sub, $super\" %}\n+\n+  ins_encode %{\n+    __ lookup_secondary_supers_table_var($sub$$Register, $super$$Register, $temp1$$Register, $temp2$$Register,\n+\t\t\t\t\t $temp3$$Register, $temp4$$Register, $result$$Register);\n+  %}\n+\n+  ins_pipe(pipe_slow);\n+%}\n+\n+instruct partialSubtypeCheckConstSuper(rsi_RegP sub, rax_RegP super_reg, immP super_con, rdi_RegP result,\n+                                       rdx_RegL temp1, rcx_RegL temp2, rbx_RegP temp3, r11_RegL temp4,\n+                                       rFlagsReg cr)\n+%{\n+  match(Set result (PartialSubtypeCheck sub (Binary super_reg super_con)));\n+  predicate(UseSecondarySupersTable);\n+  effect(KILL cr, TEMP temp1, TEMP temp2, TEMP temp3, TEMP temp4);\n+\n+  ins_cost(700);  \/\/ smaller than the next version\n+  format %{ \"partialSubtypeCheck $result, $sub, $super_reg, $super_con\" %}\n+\n+  ins_encode %{\n+    u1 super_klass_slot = ((Klass*)$super_con$$constant)->hash_slot();\n+    if (InlineSecondarySupersTest) {\n+      __ lookup_secondary_supers_table_const($sub$$Register, $super_reg$$Register, $temp1$$Register, $temp2$$Register,\n+                                       $temp3$$Register, $temp4$$Register, $result$$Register,\n+                                       super_klass_slot);\n+    } else {\n+      __ call(RuntimeAddress(StubRoutines::lookup_secondary_supers_table_stub(super_klass_slot)));\n@@ -2662,3 +16805,1 @@\n-  }\n-  return val;\n-}\n+  %}\n@@ -2666,11 +16807,2 @@\n-static inline jlong high_bit_set(BasicType bt) {\n-  switch (bt) {\n-    case T_BYTE:  return 0x8080808080808080;\n-    case T_SHORT: return 0x8000800080008000;\n-    case T_INT:   return 0x8000000080000000;\n-    case T_LONG:  return 0x8000000000000000;\n-    default:\n-      ShouldNotReachHere();\n-      return 0;\n-  }\n-}\n+  ins_pipe(pipe_slow);\n+%}\n@@ -2678,5 +16810,11 @@\n-#ifndef PRODUCT\n-  void MachNopNode::format(PhaseRegAlloc*, outputStream* st) const {\n-    st->print(\"nop \\t# %d bytes pad for loops and calls\", _count);\n-  }\n-#endif\n+\/\/ ============================================================================\n+\/\/ Branch Instructions -- short offset versions\n+\/\/\n+\/\/ These instructions are used to replace jumps of a long offset (the default\n+\/\/ match) with jumps of a shorter offset.  These instructions are all tagged\n+\/\/ with the ins_short_branch attribute, which causes the ADLC to suppress the\n+\/\/ match rules in general matching.  Instead, the ADLC generates a conversion\n+\/\/ method in the MachNode which can be used to do in-place replacement of the\n+\/\/ long variant with the shorter variant.  The compiler will determine if a\n+\/\/ branch can be taken by the is_short_branch_offset() predicate in the machine\n+\/\/ specific code section of the file.\n@@ -2684,3 +16822,4 @@\n-  void MachNopNode::emit(C2_MacroAssembler *masm, PhaseRegAlloc*) const {\n-    __ nop(_count);\n-  }\n+\/\/ Jump Direct - Label defines a relative address from JMP+1\n+instruct jmpDir_short(label labl) %{\n+  match(Goto);\n+  effect(USE labl);\n@@ -2688,3 +16827,10 @@\n-  uint MachNopNode::size(PhaseRegAlloc*) const {\n-    return _count;\n-  }\n+  ins_cost(300);\n+  format %{ \"jmp,s   $labl\" %}\n+  size(2);\n+  ins_encode %{\n+    Label* L = $labl$$label;\n+    __ jmpb(*L);\n+  %}\n+  ins_pipe(pipe_jmp);\n+  ins_short_branch(1);\n+%}\n@@ -2692,5 +16838,4 @@\n-#ifndef PRODUCT\n-  void MachBreakpointNode::format(PhaseRegAlloc*, outputStream* st) const {\n-    st->print(\"# breakpoint\");\n-  }\n-#endif\n+\/\/ Jump Direct Conditional - Label defines a relative address from Jcc+1\n+instruct jmpCon_short(cmpOp cop, rFlagsReg cr, label labl) %{\n+  match(If cop cr);\n+  effect(USE labl);\n@@ -2698,3 +16843,10 @@\n-  void MachBreakpointNode::emit(C2_MacroAssembler *masm, PhaseRegAlloc* ra_) const {\n-    __ int3();\n-  }\n+  ins_cost(300);\n+  format %{ \"j$cop,s   $labl\" %}\n+  size(2);\n+  ins_encode %{\n+    Label* L = $labl$$label;\n+    __ jccb((Assembler::Condition)($cop$$cmpcode), *L);\n+  %}\n+  ins_pipe(pipe_jcc);\n+  ins_short_branch(1);\n+%}\n@@ -2702,3 +16854,4 @@\n-  uint MachBreakpointNode::size(PhaseRegAlloc* ra_) const {\n-    return MachNode::size(ra_);\n-  }\n+\/\/ Jump Direct Conditional - Label defines a relative address from Jcc+1\n+instruct jmpLoopEnd_short(cmpOp cop, rFlagsReg cr, label labl) %{\n+  match(CountedLoopEnd cop cr);\n+  effect(USE labl);\n@@ -2706,0 +16859,9 @@\n+  ins_cost(300);\n+  format %{ \"j$cop,s   $labl\\t# loop end\" %}\n+  size(2);\n+  ins_encode %{\n+    Label* L = $labl$$label;\n+    __ jccb((Assembler::Condition)($cop$$cmpcode), *L);\n+  %}\n+  ins_pipe(pipe_jcc);\n+  ins_short_branch(1);\n@@ -2708,1 +16870,4 @@\n-encode %{\n+\/\/ Jump Direct Conditional - using unsigned comparison\n+instruct jmpConU_short(cmpOpU cop, rFlagsRegU cmp, label labl) %{\n+  match(If cop cmp);\n+  effect(USE labl);\n@@ -2710,10 +16875,54 @@\n-  enc_class call_epilog %{\n-    if (VerifyStackAtCalls) {\n-      \/\/ Check that stack depth is unchanged: find majik cookie on stack\n-      int framesize = ra_->reg2offset_unchecked(OptoReg::add(ra_->_matcher._old_SP, -3*VMRegImpl::slots_per_word));\n-      Label L;\n-      __ cmpptr(Address(rsp, framesize), (int32_t)0xbadb100d);\n-      __ jccb(Assembler::equal, L);\n-      \/\/ Die if stack mismatch\n-      __ int3();\n-      __ bind(L);\n+  ins_cost(300);\n+  format %{ \"j$cop,us  $labl\" %}\n+  size(2);\n+  ins_encode %{\n+    Label* L = $labl$$label;\n+    __ jccb((Assembler::Condition)($cop$$cmpcode), *L);\n+  %}\n+  ins_pipe(pipe_jcc);\n+  ins_short_branch(1);\n+%}\n+\n+instruct jmpConUCF_short(cmpOpUCF cop, rFlagsRegUCF cmp, label labl) %{\n+  match(If cop cmp);\n+  effect(USE labl);\n+\n+  ins_cost(300);\n+  format %{ \"j$cop,us  $labl\" %}\n+  size(2);\n+  ins_encode %{\n+    Label* L = $labl$$label;\n+    __ jccb((Assembler::Condition)($cop$$cmpcode), *L);\n+  %}\n+  ins_pipe(pipe_jcc);\n+  ins_short_branch(1);\n+%}\n+\n+instruct jmpConUCF2_short(cmpOpUCF2 cop, rFlagsRegUCF cmp, label labl) %{\n+  match(If cop cmp);\n+  effect(USE labl);\n+\n+  ins_cost(300);\n+  format %{ $$template\n+    if ($cop$$cmpcode == Assembler::notEqual) {\n+      $$emit$$\"jp,u,s  $labl\\n\\t\"\n+      $$emit$$\"j$cop,u,s  $labl\"\n+    } else {\n+      $$emit$$\"jp,u,s  done\\n\\t\"\n+      $$emit$$\"j$cop,u,s  $labl\\n\\t\"\n+      $$emit$$\"done:\"\n+    }\n+  %}\n+  size(4);\n+  ins_encode %{\n+    Label* l = $labl$$label;\n+    if ($cop$$cmpcode == Assembler::notEqual) {\n+      __ jccb(Assembler::parity, *l);\n+      __ jccb(Assembler::notEqual, *l);\n+    } else if ($cop$$cmpcode == Assembler::equal) {\n+      Label done;\n+      __ jccb(Assembler::parity, done);\n+      __ jccb(Assembler::equal, *l);\n+      __ bind(done);\n+    } else {\n+       ShouldNotReachHere();\n@@ -2722,0 +16931,6 @@\n+  ins_pipe(pipe_jcc);\n+  ins_short_branch(1);\n+%}\n+\n+\/\/ ============================================================================\n+\/\/ inlined locking and unlocking\n@@ -2723,0 +16938,9 @@\n+instruct cmpFastLock(rFlagsReg cr, rRegP object, rbx_RegP box, rax_RegI rax_reg, rRegP tmp) %{\n+  match(Set cr (FastLock object box));\n+  effect(TEMP rax_reg, TEMP tmp, USE_KILL box);\n+  ins_cost(300);\n+  format %{ \"fastlock $object,$box\\t! kills $box,$rax_reg,$tmp\" %}\n+  ins_encode %{\n+    __ fast_lock($object$$Register, $box$$Register, $rax_reg$$Register, $tmp$$Register, r15_thread);\n+  %}\n+  ins_pipe(pipe_slow);\n@@ -2725,6 +16949,9 @@\n-\/\/ Operands for bound floating pointer register arguments\n-operand rxmm0() %{\n-  constraint(ALLOC_IN_RC(xmm0_reg));\n-  match(VecX);\n-  format%{%}\n-  interface(REG_INTER);\n+instruct cmpFastUnlock(rFlagsReg cr, rRegP object, rax_RegP rax_reg, rRegP tmp) %{\n+  match(Set cr (FastUnlock object rax_reg));\n+  effect(TEMP tmp, USE_KILL rax_reg);\n+  ins_cost(300);\n+  format %{ \"fastunlock $object,$rax_reg\\t! kills $rax_reg,$tmp\" %}\n+  ins_encode %{\n+    __ fast_unlock($object$$Register, $rax_reg$$Register, $tmp$$Register, r15_thread);\n+  %}\n+  ins_pipe(pipe_slow);\n@@ -2733,5 +16960,6 @@\n-\/\/----------OPERANDS-----------------------------------------------------------\n-\/\/ Operand definitions must precede instruction definitions for correct parsing\n-\/\/ in the ADLC because operands constitute user defined types which are used in\n-\/\/ instruction definitions.\n-\/\/ Vectors\n+\/\/ ============================================================================\n+\/\/ Safepoint Instructions\n+instruct safePoint_poll_tls(rFlagsReg cr, rRegP poll)\n+%{\n+  match(SafePoint poll);\n+  effect(KILL cr, USE poll);\n@@ -2740,9 +16968,11 @@\n-\/\/ Dummy generic vector class. Should be used for all vector operands.\n-\/\/ Replaced with vec[SDXYZ] during post-selection pass.\n-operand vec() %{\n-  constraint(ALLOC_IN_RC(dynamic));\n-  match(VecX);\n-  match(VecY);\n-  match(VecZ);\n-  match(VecS);\n-  match(VecD);\n+  format %{ \"testl   rax, [$poll]\\t\"\n+            \"# Safepoint: poll for GC\" %}\n+  ins_cost(125);\n+  ins_encode %{\n+    __ relocate(relocInfo::poll_type);\n+    address pre_pc = __ pc();\n+    __ testl(rax, Address($poll$$Register, 0));\n+    assert(nativeInstruction_at(pre_pc)->is_safepoint_poll(), \"must emit test %%eax [reg]\");\n+  %}\n+  ins_pipe(ialu_reg_mem);\n+%}\n@@ -2750,2 +16980,21 @@\n-  format %{ %}\n-  interface(REG_INTER);\n+instruct mask_all_evexL(kReg dst, rRegL src) %{\n+  match(Set dst (MaskAll src));\n+  format %{ \"mask_all_evexL $dst, $src \\t! mask all operation\" %}\n+  ins_encode %{\n+    int mask_len = Matcher::vector_length(this);\n+    __ vector_maskall_operation($dst$$KRegister, $src$$Register, mask_len);\n+  %}\n+  ins_pipe( pipe_slow );\n+%}\n+\n+instruct mask_all_evexI_GT32(kReg dst, rRegI src, rRegL tmp) %{\n+  predicate(Matcher::vector_length(n) > 32);\n+  match(Set dst (MaskAll src));\n+  effect(TEMP tmp);\n+  format %{ \"mask_all_evexI_GT32 $dst, $src \\t! using $tmp as TEMP\" %}\n+  ins_encode %{\n+    int mask_len = Matcher::vector_length(this);\n+    __ movslq($tmp$$Register, $src$$Register);\n+    __ vector_maskall_operation($dst$$KRegister, $tmp$$Register, mask_len);\n+  %}\n+  ins_pipe( pipe_slow );\n@@ -2754,14 +17003,15 @@\n-\/\/ Dummy generic legacy vector class. Should be used for all legacy vector operands.\n-\/\/ Replaced with legVec[SDXYZ] during post-selection cleanup.\n-\/\/ Note: legacy register class is used to avoid extra (unneeded in 32-bit VM)\n-\/\/ runtime code generation via reg_class_dynamic.\n-operand legVec() %{\n-  constraint(ALLOC_IN_RC(dynamic));\n-  match(VecX);\n-  match(VecY);\n-  match(VecZ);\n-  match(VecS);\n-  match(VecD);\n-\n-  format %{ %}\n-  interface(REG_INTER);\n+\/\/ ============================================================================\n+\/\/ Procedure Call\/Return Instructions\n+\/\/ Call Java Static Instruction\n+\/\/ Note: If this code changes, the corresponding ret_addr_offset() and\n+\/\/       compute_padding() functions will have to be adjusted.\n+instruct CallStaticJavaDirect(method meth) %{\n+  match(CallStaticJava);\n+  effect(USE meth);\n+\n+  ins_cost(300);\n+  format %{ \"call,static \" %}\n+  opcode(0xE8); \/* E8 cd *\/\n+  ins_encode(clear_avx, Java_Static_Call(meth), call_epilog);\n+  ins_pipe(pipe_slow);\n+  ins_alignment(4);\n@@ -2770,4 +17020,7 @@\n-\/\/ Replaces vec during post-selection cleanup. See above.\n-operand vecS() %{\n-  constraint(ALLOC_IN_RC(vectors_reg_vlbwdq));\n-  match(VecS);\n+\/\/ Call Java Dynamic Instruction\n+\/\/ Note: If this code changes, the corresponding ret_addr_offset() and\n+\/\/       compute_padding() functions will have to be adjusted.\n+instruct CallDynamicJavaDirect(method meth)\n+%{\n+  match(CallDynamicJava);\n+  effect(USE meth);\n@@ -2775,2 +17028,6 @@\n-  format %{ %}\n-  interface(REG_INTER);\n+  ins_cost(300);\n+  format %{ \"movq    rax, #Universe::non_oop_word()\\n\\t\"\n+            \"call,dynamic \" %}\n+  ins_encode(clear_avx, Java_Dynamic_Call(meth), call_epilog);\n+  ins_pipe(pipe_slow);\n+  ins_alignment(4);\n@@ -2779,4 +17036,5 @@\n-\/\/ Replaces legVec during post-selection cleanup. See above.\n-operand legVecS() %{\n-  constraint(ALLOC_IN_RC(vectors_reg_legacy));\n-  match(VecS);\n+\/\/ Call Runtime Instruction\n+instruct CallRuntimeDirect(method meth)\n+%{\n+  match(CallRuntime);\n+  effect(USE meth);\n@@ -2784,2 +17042,4 @@\n-  format %{ %}\n-  interface(REG_INTER);\n+  ins_cost(300);\n+  format %{ \"call,runtime \" %}\n+  ins_encode(clear_avx, Java_To_Runtime(meth));\n+  ins_pipe(pipe_slow);\n@@ -2788,4 +17048,5 @@\n-\/\/ Replaces vec during post-selection cleanup. See above.\n-operand vecD() %{\n-  constraint(ALLOC_IN_RC(vectord_reg_vlbwdq));\n-  match(VecD);\n+\/\/ Call runtime without safepoint\n+instruct CallLeafDirect(method meth)\n+%{\n+  match(CallLeaf);\n+  effect(USE meth);\n@@ -2793,2 +17054,4 @@\n-  format %{ %}\n-  interface(REG_INTER);\n+  ins_cost(300);\n+  format %{ \"call_leaf,runtime \" %}\n+  ins_encode(clear_avx, Java_To_Runtime(meth));\n+  ins_pipe(pipe_slow);\n@@ -2797,4 +17060,5 @@\n-\/\/ Replaces legVec during post-selection cleanup. See above.\n-operand legVecD() %{\n-  constraint(ALLOC_IN_RC(vectord_reg_legacy));\n-  match(VecD);\n+\/\/ Call runtime without safepoint and with vector arguments\n+instruct CallLeafDirectVector(method meth)\n+%{\n+  match(CallLeafVector);\n+  effect(USE meth);\n@@ -2802,2 +17066,4 @@\n-  format %{ %}\n-  interface(REG_INTER);\n+  ins_cost(300);\n+  format %{ \"call_leaf,vector \" %}\n+  ins_encode(Java_To_Runtime(meth));\n+  ins_pipe(pipe_slow);\n@@ -2806,4 +17072,5 @@\n-\/\/ Replaces vec during post-selection cleanup. See above.\n-operand vecX() %{\n-  constraint(ALLOC_IN_RC(vectorx_reg_vlbwdq));\n-  match(VecX);\n+\/\/ Call runtime without safepoint\n+instruct CallLeafNoFPDirect(method meth)\n+%{\n+  match(CallLeafNoFP);\n+  effect(USE meth);\n@@ -2811,2 +17078,4 @@\n-  format %{ %}\n-  interface(REG_INTER);\n+  ins_cost(300);\n+  format %{ \"call_leaf_nofp,runtime \" %}\n+  ins_encode(clear_avx, Java_To_Runtime(meth));\n+  ins_pipe(pipe_slow);\n@@ -2815,4 +17084,7 @@\n-\/\/ Replaces legVec during post-selection cleanup. See above.\n-operand legVecX() %{\n-  constraint(ALLOC_IN_RC(vectorx_reg_legacy));\n-  match(VecX);\n+\/\/ Return Instruction\n+\/\/ Remove the return address & jump to it.\n+\/\/ Notice: We always emit a nop after a ret to make sure there is room\n+\/\/ for safepoint patching\n+instruct Ret()\n+%{\n+  match(Return);\n@@ -2820,2 +17092,5 @@\n-  format %{ %}\n-  interface(REG_INTER);\n+  format %{ \"ret\" %}\n+  ins_encode %{\n+    __ ret(0);\n+  %}\n+  ins_pipe(pipe_jmp);\n@@ -2824,4 +17099,9 @@\n-\/\/ Replaces vec during post-selection cleanup. See above.\n-operand vecY() %{\n-  constraint(ALLOC_IN_RC(vectory_reg_vlbwdq));\n-  match(VecY);\n+\/\/ Tail Call; Jump from runtime stub to Java code.\n+\/\/ Also known as an 'interprocedural jump'.\n+\/\/ Target of jump will eventually return to caller.\n+\/\/ TailJump below removes the return address.\n+\/\/ Don't use rbp for 'jump_target' because a MachEpilogNode has already been\n+\/\/ emitted just above the TailCall which has reset rbp to the caller state.\n+instruct TailCalljmpInd(no_rbp_RegP jump_target, rbx_RegP method_ptr)\n+%{\n+  match(TailCall jump_target method_ptr);\n@@ -2829,2 +17109,6 @@\n-  format %{ %}\n-  interface(REG_INTER);\n+  ins_cost(300);\n+  format %{ \"jmp     $jump_target\\t# rbx holds method\" %}\n+  ins_encode %{\n+    __ jmp($jump_target$$Register);\n+  %}\n+  ins_pipe(pipe_jmp);\n@@ -2833,4 +17117,5 @@\n-\/\/ Replaces legVec during post-selection cleanup. See above.\n-operand legVecY() %{\n-  constraint(ALLOC_IN_RC(vectory_reg_legacy));\n-  match(VecY);\n+\/\/ Tail Jump; remove the return address; jump to target.\n+\/\/ TailCall above leaves the return address around.\n+instruct tailjmpInd(no_rbp_RegP jump_target, rax_RegP ex_oop)\n+%{\n+  match(TailJump jump_target ex_oop);\n@@ -2838,2 +17123,8 @@\n-  format %{ %}\n-  interface(REG_INTER);\n+  ins_cost(300);\n+  format %{ \"popq    rdx\\t# pop return address\\n\\t\"\n+            \"jmp     $jump_target\" %}\n+  ins_encode %{\n+    __ popq(as_Register(RDX_enc));\n+    __ jmp($jump_target$$Register);\n+  %}\n+  ins_pipe(pipe_jmp);\n@@ -2842,4 +17133,4 @@\n-\/\/ Replaces vec during post-selection cleanup. See above.\n-operand vecZ() %{\n-  constraint(ALLOC_IN_RC(vectorz_reg));\n-  match(VecZ);\n+\/\/ Forward exception.\n+instruct ForwardExceptionjmp()\n+%{\n+  match(ForwardException);\n@@ -2847,2 +17138,5 @@\n-  format %{ %}\n-  interface(REG_INTER);\n+  format %{ \"jmp     forward_exception_stub\" %}\n+  ins_encode %{\n+    __ jump(RuntimeAddress(StubRoutines::forward_exception_entry()), noreg);\n+  %}\n+  ins_pipe(pipe_jmp);\n@@ -2851,4 +17145,6 @@\n-\/\/ Replaces legVec during post-selection cleanup. See above.\n-operand legVecZ() %{\n-  constraint(ALLOC_IN_RC(vectorz_reg_legacy));\n-  match(VecZ);\n+\/\/ Create exception oop: created by stack-crawling runtime code.\n+\/\/ Created exception is now available to this handler, and is setup\n+\/\/ just prior to jumping to this handler.  No code emitted.\n+instruct CreateException(rax_RegP ex_oop)\n+%{\n+  match(Set ex_oop (CreateEx));\n@@ -2856,2 +17152,5 @@\n-  format %{ %}\n-  interface(REG_INTER);\n+  size(0);\n+  \/\/ use the following format syntax\n+  format %{ \"# exception oop is in rax; no code emitted\" %}\n+  ins_encode();\n+  ins_pipe(empty);\n@@ -2860,3 +17159,6 @@\n-\/\/ INSTRUCTIONS -- Platform independent definitions (same for 32- and 64-bit)\n-\n-\/\/ ============================================================================\n+\/\/ Rethrow exception:\n+\/\/ The exception oop will come in the first argument position.\n+\/\/ Then JUMP (not call) to the rethrow stub code.\n+instruct RethrowException()\n+%{\n+  match(Rethrow);\n@@ -2864,3 +17166,2 @@\n-instruct ShouldNotReachHere() %{\n-  match(Halt);\n-  format %{ \"stop\\t# ShouldNotReachHere\" %}\n+  \/\/ use the following format syntax\n+  format %{ \"jmp     rethrow_stub\" %}\n@@ -2868,4 +17169,1 @@\n-    if (is_reachable()) {\n-      const char* str = __ code_string(_halt_reason);\n-      __ stop(str);\n-    }\n+    __ jump(RuntimeAddress(OptoRuntime::rethrow_stub()), noreg);\n@@ -2873,1 +17171,1 @@\n-  ins_pipe(pipe_slow);\n+  ins_pipe(pipe_jmp);\n@@ -2877,0 +17175,12 @@\n+\/\/ This name is KNOWN by the ADLC and cannot be changed.\n+\/\/ The ADLC forces a 'TypeRawPtr::BOTTOM' output type\n+\/\/ for this guy.\n+instruct tlsLoadP(r15_RegP dst) %{\n+  match(Set dst (ThreadLocal));\n+  effect(DEF dst);\n+\n+  size(0);\n+  format %{ \"# TLS is in R15\" %}\n+  ins_encode( \/*empty encoding*\/ );\n+  ins_pipe(ialu_reg_reg);\n+%}\n@@ -7664,2 +21974,5 @@\n-  predicate(!VM_Version::supports_avx512vl() && Matcher::vector_length_in_bytes(n->in(1)) < 64 &&\n-            type2aelembytes(Matcher::vector_element_basic_type(n)) <= 4);\n+  predicate(!VM_Version::supports_avx10_2() &&\n+            !VM_Version::supports_avx512vl() &&\n+            Matcher::vector_length_in_bytes(n->in(1)) < 64 &&\n+            type2aelembytes(Matcher::vector_element_basic_type(n)) <= 4 &&\n+            is_integral_type(Matcher::vector_element_basic_type(n)));\n@@ -7687,1 +22000,2 @@\n-  predicate((VM_Version::supports_avx512vl() || Matcher::vector_length_in_bytes(n->in(1)) == 64) &&\n+  predicate(!VM_Version::supports_avx10_2() &&\n+            (VM_Version::supports_avx512vl() || Matcher::vector_length_in_bytes(n->in(1)) == 64) &&\n@@ -7709,0 +22023,27 @@\n+instruct castFtoX_reg_avx10(vec dst, vec src) %{\n+  predicate(VM_Version::supports_avx10_2() &&\n+            is_integral_type(Matcher::vector_element_basic_type(n)));\n+  match(Set dst (VectorCastF2X src));\n+  format %{ \"vector_cast_f2x_avx10 $dst, $src\\t!\" %}\n+  ins_encode %{\n+    BasicType to_elem_bt = Matcher::vector_element_basic_type(this);\n+    int vlen_enc = (to_elem_bt == T_LONG) ? vector_length_encoding(this) : vector_length_encoding(this, $src);\n+    __ vector_castF2X_avx10(to_elem_bt, $dst$$XMMRegister, $src$$XMMRegister, vlen_enc);\n+  %}\n+  ins_pipe( pipe_slow );\n+%}\n+\n+instruct castFtoX_mem_avx10(vec dst, memory src) %{\n+  predicate(VM_Version::supports_avx10_2() &&\n+            is_integral_type(Matcher::vector_element_basic_type(n)));\n+  match(Set dst (VectorCastF2X (LoadVector src)));\n+  format %{ \"vector_cast_f2x_avx10 $dst, $src\\t!\" %}\n+  ins_encode %{\n+    int vlen = Matcher::vector_length(this);\n+    BasicType to_elem_bt = Matcher::vector_element_basic_type(this);\n+    int vlen_enc = (to_elem_bt == T_LONG) ? vector_length_encoding(this) : vector_length_encoding(vlen * sizeof(jfloat));\n+    __ vector_castF2X_avx10(to_elem_bt, $dst$$XMMRegister, $src$$Address, vlen_enc);\n+  %}\n+  ins_pipe( pipe_slow );\n+%}\n+\n@@ -7721,1 +22062,3 @@\n-  predicate(!VM_Version::supports_avx512vl() && Matcher::vector_length_in_bytes(n->in(1)) < 64 &&\n+  predicate(!VM_Version::supports_avx10_2() &&\n+            !VM_Version::supports_avx512vl() &&\n+            Matcher::vector_length_in_bytes(n->in(1)) < 64 &&\n@@ -7737,1 +22080,2 @@\n-  predicate((VM_Version::supports_avx512vl() || Matcher::vector_length_in_bytes(n->in(1)) == 64) &&\n+  predicate(!VM_Version::supports_avx10_2() &&\n+            (VM_Version::supports_avx512vl() || Matcher::vector_length_in_bytes(n->in(1)) == 64) &&\n@@ -7753,0 +22097,27 @@\n+instruct castDtoX_reg_avx10(vec dst, vec src) %{\n+  predicate(VM_Version::supports_avx10_2() &&\n+            is_integral_type(Matcher::vector_element_basic_type(n)));\n+  match(Set dst (VectorCastD2X src));\n+  format %{ \"vector_cast_d2x_avx10 $dst, $src\\t!\" %}\n+  ins_encode %{\n+    int vlen_enc = vector_length_encoding(this, $src);\n+    BasicType to_elem_bt = Matcher::vector_element_basic_type(this);\n+    __ vector_castD2X_avx10(to_elem_bt, $dst$$XMMRegister, $src$$XMMRegister, vlen_enc);\n+  %}\n+  ins_pipe( pipe_slow );\n+%}\n+\n+instruct castDtoX_mem_avx10(vec dst, memory src) %{\n+  predicate(VM_Version::supports_avx10_2() &&\n+            is_integral_type(Matcher::vector_element_basic_type(n)));\n+  match(Set dst (VectorCastD2X (LoadVector src)));\n+  format %{ \"vector_cast_d2x_avx10 $dst, $src\\t!\" %}\n+  ins_encode %{\n+    int vlen = Matcher::vector_length(this);\n+    int vlen_enc = vector_length_encoding(vlen * sizeof(jdouble));\n+    BasicType to_elem_bt = Matcher::vector_element_basic_type(this);\n+    __ vector_castD2X_avx10(to_elem_bt, $dst$$XMMRegister, $src$$Address, vlen_enc);\n+  %}\n+  ins_pipe( pipe_slow );\n+%}\n+\n@@ -10878,0 +25249,326 @@\n+\n+\/\/----------PEEPHOLE RULES-----------------------------------------------------\n+\/\/ These must follow all instruction definitions as they use the names\n+\/\/ defined in the instructions definitions.\n+\/\/\n+\/\/ peeppredicate ( rule_predicate );\n+\/\/ \/\/ the predicate unless which the peephole rule will be ignored\n+\/\/\n+\/\/ peepmatch ( root_instr_name [preceding_instruction]* );\n+\/\/\n+\/\/ peepprocedure ( procedure_name );\n+\/\/ \/\/ provide a procedure name to perform the optimization, the procedure should\n+\/\/ \/\/ reside in the architecture dependent peephole file, the method has the\n+\/\/ \/\/ signature of MachNode* (Block*, int, PhaseRegAlloc*, (MachNode*)(*)(), int...)\n+\/\/ \/\/ with the arguments being the basic block, the current node index inside the\n+\/\/ \/\/ block, the register allocator, the functions upon invoked return a new node\n+\/\/ \/\/ defined in peepreplace, and the rules of the nodes appearing in the\n+\/\/ \/\/ corresponding peepmatch, the function return true if successful, else\n+\/\/ \/\/ return false\n+\/\/\n+\/\/ peepconstraint %{\n+\/\/ (instruction_number.operand_name relational_op instruction_number.operand_name\n+\/\/  [, ...] );\n+\/\/ \/\/ instruction numbers are zero-based using left to right order in peepmatch\n+\/\/\n+\/\/ peepreplace ( instr_name  ( [instruction_number.operand_name]* ) );\n+\/\/ \/\/ provide an instruction_number.operand_name for each operand that appears\n+\/\/ \/\/ in the replacement instruction's match rule\n+\/\/\n+\/\/ ---------VM FLAGS---------------------------------------------------------\n+\/\/\n+\/\/ All peephole optimizations can be turned off using -XX:-OptoPeephole\n+\/\/\n+\/\/ Each peephole rule is given an identifying number starting with zero and\n+\/\/ increasing by one in the order seen by the parser.  An individual peephole\n+\/\/ can be enabled, and all others disabled, by using -XX:OptoPeepholeAt=#\n+\/\/ on the command-line.\n+\/\/\n+\/\/ ---------CURRENT LIMITATIONS----------------------------------------------\n+\/\/\n+\/\/ Only transformations inside a basic block (do we need more for peephole)\n+\/\/\n+\/\/ ---------EXAMPLE----------------------------------------------------------\n+\/\/\n+\/\/ \/\/ pertinent parts of existing instructions in architecture description\n+\/\/ instruct movI(rRegI dst, rRegI src)\n+\/\/ %{\n+\/\/   match(Set dst (CopyI src));\n+\/\/ %}\n+\/\/\n+\/\/ instruct incI_rReg(rRegI dst, immI_1 src, rFlagsReg cr)\n+\/\/ %{\n+\/\/   match(Set dst (AddI dst src));\n+\/\/   effect(KILL cr);\n+\/\/ %}\n+\/\/\n+\/\/ instruct leaI_rReg_immI(rRegI dst, immI_1 src)\n+\/\/ %{\n+\/\/   match(Set dst (AddI dst src));\n+\/\/ %}\n+\/\/\n+\/\/ 1. Simple replacement\n+\/\/ - Only match adjacent instructions in same basic block\n+\/\/ - Only equality constraints\n+\/\/ - Only constraints between operands, not (0.dest_reg == RAX_enc)\n+\/\/ - Only one replacement instruction\n+\/\/\n+\/\/ \/\/ Change (inc mov) to lea\n+\/\/ peephole %{\n+\/\/   \/\/ lea should only be emitted when beneficial\n+\/\/   peeppredicate( VM_Version::supports_fast_2op_lea() );\n+\/\/   \/\/ increment preceded by register-register move\n+\/\/   peepmatch ( incI_rReg movI );\n+\/\/   \/\/ require that the destination register of the increment\n+\/\/   \/\/ match the destination register of the move\n+\/\/   peepconstraint ( 0.dst == 1.dst );\n+\/\/   \/\/ construct a replacement instruction that sets\n+\/\/   \/\/ the destination to ( move's source register + one )\n+\/\/   peepreplace ( leaI_rReg_immI( 0.dst 1.src 0.src ) );\n+\/\/ %}\n+\/\/\n+\/\/ 2. Procedural replacement\n+\/\/ - More flexible finding relevent nodes\n+\/\/ - More flexible constraints\n+\/\/ - More flexible transformations\n+\/\/ - May utilise architecture-dependent API more effectively\n+\/\/ - Currently only one replacement instruction due to adlc parsing capabilities\n+\/\/\n+\/\/ \/\/ Change (inc mov) to lea\n+\/\/ peephole %{\n+\/\/   \/\/ lea should only be emitted when beneficial\n+\/\/   peeppredicate( VM_Version::supports_fast_2op_lea() );\n+\/\/   \/\/ the rule numbers of these nodes inside are passed into the function below\n+\/\/   peepmatch ( incI_rReg movI );\n+\/\/   \/\/ the method that takes the responsibility of transformation\n+\/\/   peepprocedure ( inc_mov_to_lea );\n+\/\/   \/\/ the replacement is a leaI_rReg_immI, a lambda upon invoked creating this\n+\/\/   \/\/ node is passed into the function above\n+\/\/   peepreplace ( leaI_rReg_immI() );\n+\/\/ %}\n+\n+\/\/ These instructions is not matched by the matcher but used by the peephole\n+instruct leaI_rReg_rReg_peep(rRegI dst, rRegI src1, rRegI src2)\n+%{\n+  predicate(false);\n+  match(Set dst (AddI src1 src2));\n+  format %{ \"leal    $dst, [$src1 + $src2]\" %}\n+  ins_encode %{\n+    Register dst = $dst$$Register;\n+    Register src1 = $src1$$Register;\n+    Register src2 = $src2$$Register;\n+    if (src1 != rbp && src1 != r13) {\n+      __ leal(dst, Address(src1, src2, Address::times_1));\n+    } else {\n+      assert(src2 != rbp && src2 != r13, \"\");\n+      __ leal(dst, Address(src2, src1, Address::times_1));\n+    }\n+  %}\n+  ins_pipe(ialu_reg_reg);\n+%}\n+\n+instruct leaI_rReg_immI_peep(rRegI dst, rRegI src1, immI src2)\n+%{\n+  predicate(false);\n+  match(Set dst (AddI src1 src2));\n+  format %{ \"leal    $dst, [$src1 + $src2]\" %}\n+  ins_encode %{\n+    __ leal($dst$$Register, Address($src1$$Register, $src2$$constant));\n+  %}\n+  ins_pipe(ialu_reg_reg);\n+%}\n+\n+instruct leaI_rReg_immI2_peep(rRegI dst, rRegI src, immI2 shift)\n+%{\n+  predicate(false);\n+  match(Set dst (LShiftI src shift));\n+  format %{ \"leal    $dst, [$src << $shift]\" %}\n+  ins_encode %{\n+    Address::ScaleFactor scale = static_cast<Address::ScaleFactor>($shift$$constant);\n+    Register src = $src$$Register;\n+    if (scale == Address::times_2 && src != rbp && src != r13) {\n+      __ leal($dst$$Register, Address(src, src, Address::times_1));\n+    } else {\n+      __ leal($dst$$Register, Address(noreg, src, scale));\n+    }\n+  %}\n+  ins_pipe(ialu_reg_reg);\n+%}\n+\n+instruct leaL_rReg_rReg_peep(rRegL dst, rRegL src1, rRegL src2)\n+%{\n+  predicate(false);\n+  match(Set dst (AddL src1 src2));\n+  format %{ \"leaq    $dst, [$src1 + $src2]\" %}\n+  ins_encode %{\n+    Register dst = $dst$$Register;\n+    Register src1 = $src1$$Register;\n+    Register src2 = $src2$$Register;\n+    if (src1 != rbp && src1 != r13) {\n+      __ leaq(dst, Address(src1, src2, Address::times_1));\n+    } else {\n+      assert(src2 != rbp && src2 != r13, \"\");\n+      __ leaq(dst, Address(src2, src1, Address::times_1));\n+    }\n+  %}\n+  ins_pipe(ialu_reg_reg);\n+%}\n+\n+instruct leaL_rReg_immL32_peep(rRegL dst, rRegL src1, immL32 src2)\n+%{\n+  predicate(false);\n+  match(Set dst (AddL src1 src2));\n+  format %{ \"leaq    $dst, [$src1 + $src2]\" %}\n+  ins_encode %{\n+    __ leaq($dst$$Register, Address($src1$$Register, $src2$$constant));\n+  %}\n+  ins_pipe(ialu_reg_reg);\n+%}\n+\n+instruct leaL_rReg_immI2_peep(rRegL dst, rRegL src, immI2 shift)\n+%{\n+  predicate(false);\n+  match(Set dst (LShiftL src shift));\n+  format %{ \"leaq    $dst, [$src << $shift]\" %}\n+  ins_encode %{\n+    Address::ScaleFactor scale = static_cast<Address::ScaleFactor>($shift$$constant);\n+    Register src = $src$$Register;\n+    if (scale == Address::times_2 && src != rbp && src != r13) {\n+      __ leaq($dst$$Register, Address(src, src, Address::times_1));\n+    } else {\n+      __ leaq($dst$$Register, Address(noreg, src, scale));\n+    }\n+  %}\n+  ins_pipe(ialu_reg_reg);\n+%}\n+\n+\/\/ These peephole rules replace mov + I pairs (where I is one of {add, inc, dec,\n+\/\/ sal}) with lea instructions. The {add, sal} rules are beneficial in\n+\/\/ processors with at least partial ALU support for lea\n+\/\/ (supports_fast_2op_lea()), whereas the {inc, dec} rules are only generally\n+\/\/ beneficial for processors with full ALU support\n+\/\/ (VM_Version::supports_fast_3op_lea()) and Intel Cascade Lake.\n+\n+peephole\n+%{\n+  peeppredicate(VM_Version::supports_fast_2op_lea());\n+  peepmatch (addI_rReg);\n+  peepprocedure (lea_coalesce_reg);\n+  peepreplace (leaI_rReg_rReg_peep());\n+%}\n+\n+peephole\n+%{\n+  peeppredicate(VM_Version::supports_fast_2op_lea());\n+  peepmatch (addI_rReg_imm);\n+  peepprocedure (lea_coalesce_imm);\n+  peepreplace (leaI_rReg_immI_peep());\n+%}\n+\n+peephole\n+%{\n+  peeppredicate(VM_Version::supports_fast_3op_lea() ||\n+                VM_Version::is_intel_cascade_lake());\n+  peepmatch (incI_rReg);\n+  peepprocedure (lea_coalesce_imm);\n+  peepreplace (leaI_rReg_immI_peep());\n+%}\n+\n+peephole\n+%{\n+  peeppredicate(VM_Version::supports_fast_3op_lea() ||\n+                VM_Version::is_intel_cascade_lake());\n+  peepmatch (decI_rReg);\n+  peepprocedure (lea_coalesce_imm);\n+  peepreplace (leaI_rReg_immI_peep());\n+%}\n+\n+peephole\n+%{\n+  peeppredicate(VM_Version::supports_fast_2op_lea());\n+  peepmatch (salI_rReg_immI2);\n+  peepprocedure (lea_coalesce_imm);\n+  peepreplace (leaI_rReg_immI2_peep());\n+%}\n+\n+peephole\n+%{\n+  peeppredicate(VM_Version::supports_fast_2op_lea());\n+  peepmatch (addL_rReg);\n+  peepprocedure (lea_coalesce_reg);\n+  peepreplace (leaL_rReg_rReg_peep());\n+%}\n+\n+peephole\n+%{\n+  peeppredicate(VM_Version::supports_fast_2op_lea());\n+  peepmatch (addL_rReg_imm);\n+  peepprocedure (lea_coalesce_imm);\n+  peepreplace (leaL_rReg_immL32_peep());\n+%}\n+\n+peephole\n+%{\n+  peeppredicate(VM_Version::supports_fast_3op_lea() ||\n+                VM_Version::is_intel_cascade_lake());\n+  peepmatch (incL_rReg);\n+  peepprocedure (lea_coalesce_imm);\n+  peepreplace (leaL_rReg_immL32_peep());\n+%}\n+\n+peephole\n+%{\n+  peeppredicate(VM_Version::supports_fast_3op_lea() ||\n+                VM_Version::is_intel_cascade_lake());\n+  peepmatch (decL_rReg);\n+  peepprocedure (lea_coalesce_imm);\n+  peepreplace (leaL_rReg_immL32_peep());\n+%}\n+\n+peephole\n+%{\n+  peeppredicate(VM_Version::supports_fast_2op_lea());\n+  peepmatch (salL_rReg_immI2);\n+  peepprocedure (lea_coalesce_imm);\n+  peepreplace (leaL_rReg_immI2_peep());\n+%}\n+\n+peephole\n+%{\n+  peepmatch (leaPCompressedOopOffset);\n+  peepprocedure (lea_remove_redundant);\n+%}\n+\n+peephole\n+%{\n+  peepmatch (leaP8Narrow);\n+  peepprocedure (lea_remove_redundant);\n+%}\n+\n+peephole\n+%{\n+  peepmatch (leaP32Narrow);\n+  peepprocedure (lea_remove_redundant);\n+%}\n+\n+\/\/ These peephole rules matches instructions which set flags and are followed by a testI\/L_reg\n+\/\/ The test instruction is redudanent in case the downstream instuctions (like JCC or CMOV) only use flags that are already set by the previous instruction\n+\n+\/\/int variant\n+peephole\n+%{\n+  peepmatch (testI_reg);\n+  peepprocedure (test_may_remove);\n+%}\n+\n+\/\/long variant\n+peephole\n+%{\n+  peepmatch (testL_reg);\n+  peepprocedure (test_may_remove);\n+%}\n+\n+\n+\/\/----------SMARTSPILL RULES---------------------------------------------------\n+\/\/ These must follow all instruction definitions as they use the names\n+\/\/ defined in the instructions definitions.\n","filename":"src\/hotspot\/cpu\/x86\/x86.ad","additions":16160,"deletions":1463,"binary":false,"changes":17623,"status":"modified"},{"patch":"@@ -0,0 +1,1039 @@\n+\/*\n+ * Copyright (c) 2023, 2025, Oracle and\/or its affiliates. All rights reserved.\n+ * DO NOT ALTER OR REMOVE COPYRIGHT NOTICES OR THIS FILE HEADER.\n+ *\n+ * This code is free software; you can redistribute it and\/or modify it\n+ * under the terms of the GNU General Public License version 2 only, as\n+ * published by the Free Software Foundation.\n+ *\n+ * This code is distributed in the hope that it will be useful, but WITHOUT\n+ * ANY WARRANTY; without even the implied warranty of MERCHANTABILITY or\n+ * FITNESS FOR A PARTICULAR PURPOSE.  See the GNU General Public License\n+ * version 2 for more details (a copy is included in the LICENSE file that\n+ * accompanied this code).\n+ *\n+ * You should have received a copy of the GNU General Public License version\n+ * 2 along with this work; if not, write to the Free Software Foundation,\n+ * Inc., 51 Franklin St, Fifth Floor, Boston, MA 02110-1301 USA.\n+ *\n+ * Please contact Oracle, 500 Oracle Parkway, Redwood Shores, CA 94065 USA\n+ * or visit www.oracle.com if you need additional information or have any\n+ * questions.\n+ *\n+ *\/\n+\n+#include \"cds\/aotMappedHeapLoader.hpp\"\n+#include \"cds\/aotMappedHeapWriter.hpp\"\n+#include \"cds\/aotReferenceObjSupport.hpp\"\n+#include \"cds\/cdsConfig.hpp\"\n+#include \"cds\/filemap.hpp\"\n+#include \"cds\/heapShared.inline.hpp\"\n+#include \"cds\/regeneratedClasses.hpp\"\n+#include \"classfile\/javaClasses.hpp\"\n+#include \"classfile\/modules.hpp\"\n+#include \"classfile\/systemDictionary.hpp\"\n+#include \"gc\/shared\/collectedHeap.hpp\"\n+#include \"memory\/allocation.inline.hpp\"\n+#include \"memory\/iterator.inline.hpp\"\n+#include \"memory\/oopFactory.hpp\"\n+#include \"memory\/universe.hpp\"\n+#include \"oops\/compressedOops.hpp\"\n+#include \"oops\/objArrayOop.inline.hpp\"\n+#include \"oops\/oop.inline.hpp\"\n+#include \"oops\/oopHandle.inline.hpp\"\n+#include \"oops\/typeArrayKlass.hpp\"\n+#include \"oops\/typeArrayOop.hpp\"\n+#include \"runtime\/java.hpp\"\n+#include \"runtime\/mutexLocker.hpp\"\n+#include \"utilities\/bitMap.inline.hpp\"\n+#if INCLUDE_G1GC\n+#include \"gc\/g1\/g1CollectedHeap.hpp\"\n+#include \"gc\/g1\/g1HeapRegion.hpp\"\n+#endif\n+\n+#if INCLUDE_CDS_JAVA_HEAP\n+\n+GrowableArrayCHeap<u1, mtClassShared>* AOTMappedHeapWriter::_buffer = nullptr;\n+\n+bool AOTMappedHeapWriter::_is_writing_deterministic_heap = false;\n+size_t AOTMappedHeapWriter::_buffer_used;\n+\n+\/\/ Heap root segments\n+HeapRootSegments AOTMappedHeapWriter::_heap_root_segments;\n+\n+address AOTMappedHeapWriter::_requested_bottom;\n+address AOTMappedHeapWriter::_requested_top;\n+\n+GrowableArrayCHeap<AOTMappedHeapWriter::NativePointerInfo, mtClassShared>* AOTMappedHeapWriter::_native_pointers;\n+GrowableArrayCHeap<oop, mtClassShared>* AOTMappedHeapWriter::_source_objs;\n+GrowableArrayCHeap<AOTMappedHeapWriter::HeapObjOrder, mtClassShared>* AOTMappedHeapWriter::_source_objs_order;\n+\n+AOTMappedHeapWriter::BufferOffsetToSourceObjectTable*\n+AOTMappedHeapWriter::_buffer_offset_to_source_obj_table = nullptr;\n+\n+DumpedInternedStrings *AOTMappedHeapWriter::_dumped_interned_strings = nullptr;\n+\n+typedef HashTable<\n+      size_t,    \/\/ offset of a filler from AOTMappedHeapWriter::buffer_bottom()\n+      size_t,    \/\/ size of this filler (in bytes)\n+      127,       \/\/ prime number\n+      AnyObj::C_HEAP,\n+      mtClassShared> FillersTable;\n+static FillersTable* _fillers;\n+static int _num_native_ptrs = 0;\n+\n+void AOTMappedHeapWriter::init() {\n+  if (CDSConfig::is_dumping_heap()) {\n+    Universe::heap()->collect(GCCause::_java_lang_system_gc);\n+\n+    _buffer_offset_to_source_obj_table = new BufferOffsetToSourceObjectTable(\/*size (prime)*\/36137, \/*max size*\/1 * M);\n+    _dumped_interned_strings = new (mtClass)DumpedInternedStrings(INITIAL_TABLE_SIZE, MAX_TABLE_SIZE);\n+    _fillers = new FillersTable();\n+    _requested_bottom = nullptr;\n+    _requested_top = nullptr;\n+\n+    _native_pointers = new GrowableArrayCHeap<NativePointerInfo, mtClassShared>(2048);\n+    _source_objs = new GrowableArrayCHeap<oop, mtClassShared>(10000);\n+\n+    guarantee(MIN_GC_REGION_ALIGNMENT <= G1HeapRegion::min_region_size_in_words() * HeapWordSize, \"must be\");\n+\n+    if (CDSConfig::old_cds_flags_used()) {\n+      \/\/ With the old CDS workflow, we can guatantee determninistic output: given\n+      \/\/ the same classlist file, we can generate the same static CDS archive.\n+      \/\/ To ensure determinism, we always use the same compressed oop encoding\n+      \/\/ (zero-based, no shift). See set_requested_address_range().\n+      _is_writing_deterministic_heap = true;\n+    } else {\n+      \/\/ Determninistic output is not supported by the new AOT workflow, so\n+      \/\/ we don't force the (zero-based, no shift) encoding. This way, it is more\n+      \/\/ likely that we can avoid oop relocation in the production run.\n+      _is_writing_deterministic_heap = false;\n+    }\n+  }\n+}\n+\n+\/\/ For AOTMappedHeapWriter::narrow_oop_{mode, base, shift}(), see comments\n+\/\/ in AOTMappedHeapWriter::set_requested_address_range(),\n+CompressedOops::Mode AOTMappedHeapWriter::narrow_oop_mode() {\n+  if (is_writing_deterministic_heap()) {\n+    return CompressedOops::UnscaledNarrowOop;\n+  } else {\n+    return CompressedOops::mode();\n+  }\n+}\n+\n+address AOTMappedHeapWriter::narrow_oop_base() {\n+  if (is_writing_deterministic_heap()) {\n+    return (address)0;\n+  } else {\n+    return CompressedOops::base();\n+  }\n+}\n+\n+int AOTMappedHeapWriter::narrow_oop_shift() {\n+  if (is_writing_deterministic_heap()) {\n+    return 0;\n+  } else {\n+    return CompressedOops::shift();\n+  }\n+}\n+\n+void AOTMappedHeapWriter::delete_tables_with_raw_oops() {\n+  delete _source_objs;\n+  _source_objs = nullptr;\n+\n+  delete _dumped_interned_strings;\n+  _dumped_interned_strings = nullptr;\n+}\n+\n+void AOTMappedHeapWriter::add_source_obj(oop src_obj) {\n+  _source_objs->append(src_obj);\n+}\n+\n+void AOTMappedHeapWriter::write(GrowableArrayCHeap<oop, mtClassShared>* roots,\n+                                ArchiveMappedHeapInfo* heap_info) {\n+  assert(CDSConfig::is_dumping_heap(), \"sanity\");\n+  allocate_buffer();\n+  copy_source_objs_to_buffer(roots);\n+  set_requested_address_range(heap_info);\n+  relocate_embedded_oops(roots, heap_info);\n+}\n+\n+bool AOTMappedHeapWriter::is_too_large_to_archive(oop o) {\n+  size_t size = o->size();\n+  size = o->copy_size_cds(size, o->mark());\n+  return is_too_large_to_archive(size);\n+}\n+\n+bool AOTMappedHeapWriter::is_string_too_large_to_archive(oop string) {\n+  typeArrayOop value = java_lang_String::value_no_keepalive(string);\n+  return is_too_large_to_archive(value);\n+}\n+\n+bool AOTMappedHeapWriter::is_too_large_to_archive(size_t size) {\n+  assert(size > 0, \"no zero-size object\");\n+  assert(size * HeapWordSize > size, \"no overflow\");\n+  static_assert(MIN_GC_REGION_ALIGNMENT > 0, \"must be positive\");\n+\n+  size_t byte_size = size * HeapWordSize;\n+  if (byte_size > size_t(MIN_GC_REGION_ALIGNMENT)) {\n+    return true;\n+  } else {\n+    return false;\n+  }\n+}\n+\n+\/\/ Keep track of the contents of the archived interned string table. This table\n+\/\/ is used only by CDSHeapVerifier.\n+void AOTMappedHeapWriter::add_to_dumped_interned_strings(oop string) {\n+  assert_at_safepoint(); \/\/ DumpedInternedStrings uses raw oops\n+  assert(!is_string_too_large_to_archive(string), \"must be\");\n+  bool created;\n+  _dumped_interned_strings->put_if_absent(string, true, &created);\n+  if (created) {\n+    \/\/ Prevent string deduplication from changing the value field to\n+    \/\/ something not in the archive.\n+    java_lang_String::set_deduplication_forbidden(string);\n+    _dumped_interned_strings->maybe_grow();\n+  }\n+}\n+\n+bool AOTMappedHeapWriter::is_dumped_interned_string(oop o) {\n+  return _dumped_interned_strings->get(o) != nullptr;\n+}\n+\n+\/\/ Various lookup functions between source_obj, buffered_obj and requested_obj\n+bool AOTMappedHeapWriter::is_in_requested_range(oop o) {\n+  assert(_requested_bottom != nullptr, \"do not call before _requested_bottom is initialized\");\n+  address a = cast_from_oop<address>(o);\n+  return (_requested_bottom <= a && a < _requested_top);\n+}\n+\n+oop AOTMappedHeapWriter::requested_obj_from_buffer_offset(size_t offset) {\n+  oop req_obj = cast_to_oop(_requested_bottom + offset);\n+  assert(is_in_requested_range(req_obj), \"must be\");\n+  return req_obj;\n+}\n+\n+oop AOTMappedHeapWriter::source_obj_to_requested_obj(oop src_obj) {\n+  assert(CDSConfig::is_dumping_heap(), \"dump-time only\");\n+  HeapShared::CachedOopInfo* p = HeapShared::get_cached_oop_info(src_obj);\n+  if (p != nullptr) {\n+    return requested_obj_from_buffer_offset(p->buffer_offset());\n+  } else {\n+    return nullptr;\n+  }\n+}\n+\n+oop AOTMappedHeapWriter::buffered_addr_to_source_obj(address buffered_addr) {\n+  OopHandle* oh = _buffer_offset_to_source_obj_table->get(buffered_address_to_offset(buffered_addr));\n+  if (oh != nullptr) {\n+    return oh->resolve();\n+  } else {\n+    return nullptr;\n+  }\n+}\n+\n+Klass* AOTMappedHeapWriter::real_klass_of_buffered_oop(address buffered_addr) {\n+  oop p = buffered_addr_to_source_obj(buffered_addr);\n+  if (p != nullptr) {\n+    return p->klass();\n+  } else if (get_filler_size_at(buffered_addr) > 0) {\n+    return Universe::fillerArrayKlass();\n+  } else {\n+    \/\/ This is one of the root segments\n+    return Universe::objectArrayKlass();\n+  }\n+}\n+\n+size_t AOTMappedHeapWriter::size_of_buffered_oop(address buffered_addr) {\n+  oop p = buffered_addr_to_source_obj(buffered_addr);\n+  if (p != nullptr) {\n+    size_t size = p->size();\n+    return p->copy_size_cds(size, p->mark());\n+  }\n+\n+  size_t nbytes = get_filler_size_at(buffered_addr);\n+  if (nbytes > 0) {\n+    assert((nbytes % BytesPerWord) == 0, \"should be aligned\");\n+    return nbytes \/ BytesPerWord;\n+  }\n+\n+  address hrs = buffer_bottom();\n+  for (size_t seg_idx = 0; seg_idx < _heap_root_segments.count(); seg_idx++) {\n+    nbytes = _heap_root_segments.size_in_bytes(seg_idx);\n+    if (hrs == buffered_addr) {\n+      assert((nbytes % BytesPerWord) == 0, \"should be aligned\");\n+      return nbytes \/ BytesPerWord;\n+    }\n+    hrs += nbytes;\n+  }\n+\n+  ShouldNotReachHere();\n+  return 0;\n+}\n+\n+address AOTMappedHeapWriter::buffered_addr_to_requested_addr(address buffered_addr) {\n+  return _requested_bottom + buffered_address_to_offset(buffered_addr);\n+}\n+\n+address AOTMappedHeapWriter::requested_address() {\n+  assert(_buffer != nullptr, \"must be initialized\");\n+  return _requested_bottom;\n+}\n+\n+void AOTMappedHeapWriter::allocate_buffer() {\n+  int initial_buffer_size = 100000;\n+  _buffer = new GrowableArrayCHeap<u1, mtClassShared>(initial_buffer_size);\n+  _buffer_used = 0;\n+  ensure_buffer_space(1); \/\/ so that buffer_bottom() works\n+}\n+\n+void AOTMappedHeapWriter::ensure_buffer_space(size_t min_bytes) {\n+  \/\/ We usually have very small heaps. If we get a huge one it's probably caused by a bug.\n+  guarantee(min_bytes <= max_jint, \"we dont support archiving more than 2G of objects\");\n+  _buffer->at_grow(to_array_index(min_bytes));\n+}\n+\n+objArrayOop AOTMappedHeapWriter::allocate_root_segment(size_t offset, int element_count) {\n+  HeapWord* mem = offset_to_buffered_address<HeapWord *>(offset);\n+  memset(mem, 0, objArrayOopDesc::object_size(element_count));\n+\n+  \/\/ The initialization code is copied from MemAllocator::finish and ObjArrayAllocator::initialize.\n+  if (UseCompactObjectHeaders) {\n+    oopDesc::release_set_mark(mem, Universe::objectArrayKlass()->prototype_header());\n+  } else {\n+    oopDesc::set_mark(mem, markWord::prototype());\n+    oopDesc::release_set_klass(mem, Universe::objectArrayKlass());\n+  }\n+  arrayOopDesc::set_length(mem, element_count);\n+  return objArrayOop(cast_to_oop(mem));\n+}\n+\n+void AOTMappedHeapWriter::root_segment_at_put(objArrayOop segment, int index, oop root) {\n+  \/\/ Do not use arrayOop->obj_at_put(i, o) as arrayOop is outside the real heap!\n+  if (UseCompressedOops) {\n+    *segment->obj_at_addr<narrowOop>(index) = CompressedOops::encode(root);\n+  } else {\n+    *segment->obj_at_addr<oop>(index) = root;\n+  }\n+}\n+\n+void AOTMappedHeapWriter::copy_roots_to_buffer(GrowableArrayCHeap<oop, mtClassShared>* roots) {\n+  \/\/ Depending on the number of classes we are archiving, a single roots array may be\n+  \/\/ larger than MIN_GC_REGION_ALIGNMENT. Roots are allocated first in the buffer, which\n+  \/\/ allows us to chop the large array into a series of \"segments\". Current layout\n+  \/\/ starts with zero or more segments exactly fitting MIN_GC_REGION_ALIGNMENT, and end\n+  \/\/ with a single segment that may be smaller than MIN_GC_REGION_ALIGNMENT.\n+  \/\/ This is simple and efficient. We do not need filler objects anywhere between the segments,\n+  \/\/ or immediately after the last segment. This allows starting the object dump immediately\n+  \/\/ after the roots.\n+\n+  assert((_buffer_used % MIN_GC_REGION_ALIGNMENT) == 0,\n+         \"Pre-condition: Roots start at aligned boundary: %zu\", _buffer_used);\n+\n+  int max_elem_count = ((MIN_GC_REGION_ALIGNMENT - arrayOopDesc::header_size_in_bytes()) \/ heapOopSize);\n+  assert(objArrayOopDesc::object_size(max_elem_count)*HeapWordSize == MIN_GC_REGION_ALIGNMENT,\n+         \"Should match exactly\");\n+\n+  HeapRootSegments segments(_buffer_used,\n+                            roots->length(),\n+                            MIN_GC_REGION_ALIGNMENT,\n+                            max_elem_count);\n+\n+  int root_index = 0;\n+  for (size_t seg_idx = 0; seg_idx < segments.count(); seg_idx++) {\n+    int size_elems = segments.size_in_elems(seg_idx);\n+    size_t size_bytes = segments.size_in_bytes(seg_idx);\n+\n+    size_t oop_offset = _buffer_used;\n+    _buffer_used = oop_offset + size_bytes;\n+    ensure_buffer_space(_buffer_used);\n+\n+    assert((oop_offset % MIN_GC_REGION_ALIGNMENT) == 0,\n+           \"Roots segment %zu start is not aligned: %zu\",\n+           segments.count(), oop_offset);\n+\n+    objArrayOop seg_oop = allocate_root_segment(oop_offset, size_elems);\n+    for (int i = 0; i < size_elems; i++) {\n+      root_segment_at_put(seg_oop, i, roots->at(root_index++));\n+    }\n+\n+    log_info(aot, heap)(\"archived obj root segment [%d] = %zu bytes, obj = \" PTR_FORMAT,\n+                        size_elems, size_bytes, p2i(seg_oop));\n+  }\n+\n+  assert(root_index == roots->length(), \"Post-condition: All roots are handled\");\n+\n+  _heap_root_segments = segments;\n+}\n+\n+\/\/ The goal is to sort the objects in increasing order of:\n+\/\/ - objects that have only oop pointers\n+\/\/ - objects that have both native and oop pointers\n+\/\/ - objects that have only native pointers\n+\/\/ - objects that have no pointers\n+static int oop_sorting_rank(oop o) {\n+  bool has_oop_ptr, has_native_ptr;\n+  HeapShared::get_pointer_info(o, has_oop_ptr, has_native_ptr);\n+\n+  if (has_oop_ptr) {\n+    if (!has_native_ptr) {\n+      return 0;\n+    } else {\n+      return 1;\n+    }\n+  } else {\n+    if (has_native_ptr) {\n+      return 2;\n+    } else {\n+      return 3;\n+    }\n+  }\n+}\n+\n+int AOTMappedHeapWriter::compare_objs_by_oop_fields(HeapObjOrder* a, HeapObjOrder* b) {\n+  int rank_a = a->_rank;\n+  int rank_b = b->_rank;\n+\n+  if (rank_a != rank_b) {\n+    return rank_a - rank_b;\n+  } else {\n+    \/\/ If they are the same rank, sort them by their position in the _source_objs array\n+    return a->_index - b->_index;\n+  }\n+}\n+\n+void AOTMappedHeapWriter::sort_source_objs() {\n+  log_info(aot)(\"sorting heap objects\");\n+  int len = _source_objs->length();\n+  _source_objs_order = new GrowableArrayCHeap<HeapObjOrder, mtClassShared>(len);\n+\n+  for (int i = 0; i < len; i++) {\n+    oop o = _source_objs->at(i);\n+    int rank = oop_sorting_rank(o);\n+    HeapObjOrder os = {i, rank};\n+    _source_objs_order->append(os);\n+  }\n+  log_info(aot)(\"computed ranks\");\n+  _source_objs_order->sort(compare_objs_by_oop_fields);\n+  log_info(aot)(\"sorting heap objects done\");\n+}\n+\n+void AOTMappedHeapWriter::copy_source_objs_to_buffer(GrowableArrayCHeap<oop, mtClassShared>* roots) {\n+  \/\/ There could be multiple root segments, which we want to be aligned by region.\n+  \/\/ Putting them ahead of objects makes sure we waste no space.\n+  copy_roots_to_buffer(roots);\n+\n+  sort_source_objs();\n+  for (int i = 0; i < _source_objs_order->length(); i++) {\n+    int src_obj_index = _source_objs_order->at(i)._index;\n+    oop src_obj = _source_objs->at(src_obj_index);\n+    HeapShared::CachedOopInfo* info = HeapShared::get_cached_oop_info(src_obj);\n+    assert(info != nullptr, \"must be\");\n+    size_t buffer_offset = copy_one_source_obj_to_buffer(src_obj);\n+    info->set_buffer_offset(buffer_offset);\n+\n+    OopHandle handle(Universe::vm_global(), src_obj);\n+    _buffer_offset_to_source_obj_table->put_when_absent(buffer_offset, handle);\n+    _buffer_offset_to_source_obj_table->maybe_grow();\n+\n+    if (java_lang_Module::is_instance(src_obj)) {\n+      Modules::check_archived_module_oop(src_obj);\n+    }\n+  }\n+\n+  log_info(aot)(\"Size of heap region = %zu bytes, %d objects, %d roots, %d native ptrs\",\n+                _buffer_used, _source_objs->length() + 1, roots->length(), _num_native_ptrs);\n+}\n+\n+size_t AOTMappedHeapWriter::filler_array_byte_size(int length) {\n+  size_t byte_size = objArrayOopDesc::object_size(length) * HeapWordSize;\n+  return byte_size;\n+}\n+\n+int AOTMappedHeapWriter::filler_array_length(size_t fill_bytes) {\n+  assert(is_object_aligned(fill_bytes), \"must be\");\n+  size_t elemSize = (UseCompressedOops ? sizeof(narrowOop) : sizeof(oop));\n+\n+  int initial_length = to_array_length(fill_bytes \/ elemSize);\n+  for (int length = initial_length; length >= 0; length --) {\n+    size_t array_byte_size = filler_array_byte_size(length);\n+    if (array_byte_size == fill_bytes) {\n+      return length;\n+    }\n+  }\n+\n+  ShouldNotReachHere();\n+  return -1;\n+}\n+\n+HeapWord* AOTMappedHeapWriter::init_filler_array_at_buffer_top(int array_length, size_t fill_bytes) {\n+  assert(UseCompressedClassPointers, \"Archived heap only supported for compressed klasses\");\n+  Klass* oak = Universe::objectArrayKlass(); \/\/ already relocated to point to archived klass\n+  HeapWord* mem = offset_to_buffered_address<HeapWord*>(_buffer_used);\n+  memset(mem, 0, fill_bytes);\n+  narrowKlass nk = ArchiveBuilder::current()->get_requested_narrow_klass(oak);\n+  if (UseCompactObjectHeaders) {\n+    oopDesc::release_set_mark(mem, markWord::prototype().set_narrow_klass(nk));\n+  } else {\n+    oopDesc::set_mark(mem, markWord::prototype());\n+    cast_to_oop(mem)->set_narrow_klass(nk);\n+  }\n+  arrayOopDesc::set_length(mem, array_length);\n+  return mem;\n+}\n+\n+void AOTMappedHeapWriter::maybe_fill_gc_region_gap(size_t required_byte_size) {\n+  \/\/ We fill only with arrays (so we don't need to use a single HeapWord filler if the\n+  \/\/ leftover space is smaller than a zero-sized array object). Therefore, we need to\n+  \/\/ make sure there's enough space of min_filler_byte_size in the current region after\n+  \/\/ required_byte_size has been allocated. If not, fill the remainder of the current\n+  \/\/ region.\n+  size_t min_filler_byte_size = filler_array_byte_size(0);\n+  size_t new_used = _buffer_used + required_byte_size + min_filler_byte_size;\n+\n+  const size_t cur_min_region_bottom = align_down(_buffer_used, MIN_GC_REGION_ALIGNMENT);\n+  const size_t next_min_region_bottom = align_down(new_used, MIN_GC_REGION_ALIGNMENT);\n+\n+  if (cur_min_region_bottom != next_min_region_bottom) {\n+    \/\/ Make sure that no objects span across MIN_GC_REGION_ALIGNMENT. This way\n+    \/\/ we can map the region in any region-based collector.\n+    assert(next_min_region_bottom > cur_min_region_bottom, \"must be\");\n+    assert(next_min_region_bottom - cur_min_region_bottom == MIN_GC_REGION_ALIGNMENT,\n+           \"no buffered object can be larger than %d bytes\",  MIN_GC_REGION_ALIGNMENT);\n+\n+    const size_t filler_end = next_min_region_bottom;\n+    const size_t fill_bytes = filler_end - _buffer_used;\n+    assert(fill_bytes > 0, \"must be\");\n+    ensure_buffer_space(filler_end);\n+\n+    int array_length = filler_array_length(fill_bytes);\n+    log_info(aot, heap)(\"Inserting filler obj array of %d elements (%zu bytes total) @ buffer offset %zu\",\n+                        array_length, fill_bytes, _buffer_used);\n+    HeapWord* filler = init_filler_array_at_buffer_top(array_length, fill_bytes);\n+    _buffer_used = filler_end;\n+    _fillers->put(buffered_address_to_offset((address)filler), fill_bytes);\n+  }\n+}\n+\n+size_t AOTMappedHeapWriter::get_filler_size_at(address buffered_addr) {\n+  size_t* p = _fillers->get(buffered_address_to_offset(buffered_addr));\n+  if (p != nullptr) {\n+    assert(*p > 0, \"filler must be larger than zero bytes\");\n+    return *p;\n+  } else {\n+    return 0; \/\/ buffered_addr is not a filler\n+  }\n+}\n+\n+template <typename T>\n+void update_buffered_object_field(address buffered_obj, int field_offset, T value) {\n+  T* field_addr = cast_to_oop(buffered_obj)->field_addr<T>(field_offset);\n+  *field_addr = value;\n+}\n+\n+size_t AOTMappedHeapWriter::copy_one_source_obj_to_buffer(oop src_obj) {\n+  assert(!is_too_large_to_archive(src_obj), \"already checked\");\n+  size_t old_size = src_obj->size();\n+  size_t new_size = src_obj->copy_size_cds(old_size, src_obj->mark());\n+  size_t byte_size = new_size * HeapWordSize;\n+  assert(byte_size > 0, \"no zero-size objects\");\n+\n+  \/\/ For region-based collectors such as G1, the archive heap may be mapped into\n+  \/\/ multiple regions. We need to make sure that we don't have an object that can possible\n+  \/\/ span across two regions.\n+  maybe_fill_gc_region_gap(byte_size);\n+\n+  size_t new_used = _buffer_used + byte_size;\n+  assert(new_used > _buffer_used, \"no wrap around\");\n+\n+  size_t cur_min_region_bottom = align_down(_buffer_used, MIN_GC_REGION_ALIGNMENT);\n+  size_t next_min_region_bottom = align_down(new_used, MIN_GC_REGION_ALIGNMENT);\n+  assert(cur_min_region_bottom == next_min_region_bottom, \"no object should cross minimal GC region boundaries\");\n+\n+  ensure_buffer_space(new_used);\n+\n+  address from = cast_from_oop<address>(src_obj);\n+  address to = offset_to_buffered_address<address>(_buffer_used);\n+  assert(is_object_aligned(_buffer_used), \"sanity\");\n+  assert(is_object_aligned(byte_size), \"sanity\");\n+  memcpy(to, from, MIN2(new_size, old_size) * HeapWordSize);\n+\n+  \/\/ These native pointers will be restored explicitly at run time.\n+  if (java_lang_Module::is_instance(src_obj)) {\n+    update_buffered_object_field<ModuleEntry*>(to, java_lang_Module::module_entry_offset(), nullptr);\n+  } else if (java_lang_ClassLoader::is_instance(src_obj)) {\n+#ifdef ASSERT\n+    \/\/ We only archive these loaders\n+    if (src_obj != SystemDictionary::java_platform_loader() &&\n+        src_obj != SystemDictionary::java_system_loader()) {\n+      assert(src_obj->klass()->name()->equals(\"jdk\/internal\/loader\/ClassLoaders$BootClassLoader\"), \"must be\");\n+    }\n+#endif\n+    update_buffered_object_field<ClassLoaderData*>(to, java_lang_ClassLoader::loader_data_offset(), nullptr);\n+  }\n+\n+  size_t buffered_obj_offset = _buffer_used;\n+  _buffer_used = new_used;\n+\n+  return buffered_obj_offset;\n+}\n+\n+\/\/ Set the range [_requested_bottom, _requested_top), the requested address range of all\n+\/\/ the archived heap objects in the production run.\n+\/\/\n+\/\/ (1) UseCompressedOops == true && !is_writing_deterministic_heap()\n+\/\/\n+\/\/     The archived objects are stored using the COOPS encoding of the assembly phase.\n+\/\/     We pick a range within the heap used by the assembly phase.\n+\/\/\n+\/\/     In the production run, if different COOPS encodings are used:\n+\/\/         - The heap contents needs to be relocated.\n+\/\/\n+\/\/ (2) UseCompressedOops == true && is_writing_deterministic_heap()\n+\/\/\n+\/\/     We always use zero-based, zero-shift encoding. _requested_top is aligned to 0x10000000.\n+\/\/\n+\/\/ (3) UseCompressedOops == false:\n+\/\/\n+\/\/     In the production run, the heap range is usually picked (randomly) by the OS, so we\n+\/\/     will almost always need to perform relocation, regardless of how we pick the requested\n+\/\/     address range.\n+\/\/\n+\/\/     So we just hard code it to NOCOOPS_REQUESTED_BASE.\n+\/\/\n+void AOTMappedHeapWriter::set_requested_address_range(ArchiveMappedHeapInfo* info) {\n+  assert(!info->is_used(), \"only set once\");\n+\n+  size_t heap_region_byte_size = _buffer_used;\n+  assert(heap_region_byte_size > 0, \"must archived at least one object!\");\n+\n+  if (UseCompressedOops) {\n+    if (is_writing_deterministic_heap()) {\n+      \/\/ Pick a heap range so that requested addresses can be encoded with zero-base\/no shift.\n+      \/\/ We align the requested bottom to at least 1 MB: if the production run uses G1 with a small\n+      \/\/ heap (e.g., -Xmx256m), it's likely that we can map the archived objects at the\n+      \/\/ requested location to avoid relocation.\n+      \/\/\n+      \/\/ For other collectors or larger heaps, relocation is unavoidable, but is usually\n+      \/\/ quite cheap. If you really want to avoid relocation, use the AOT workflow instead.\n+      address heap_end = (address)0x100000000;\n+      size_t alignment = MAX2(MIN_GC_REGION_ALIGNMENT, 1024 * 1024);\n+      if (align_up(heap_region_byte_size, alignment) >= (size_t)heap_end) {\n+        log_error(aot, heap)(\"cached heap space is too large: %zu bytes\", heap_region_byte_size);\n+        AOTMetaspace::unrecoverable_writing_error();\n+      }\n+      _requested_bottom = align_down(heap_end - heap_region_byte_size, alignment);\n+    } else if (UseG1GC) {\n+      \/\/ For G1, pick the range at the top of the current heap. If the exact same heap sizes\n+      \/\/ are used in the production run, it's likely that we can map the archived objects\n+      \/\/ at the requested location to avoid relocation.\n+      address heap_end = (address)G1CollectedHeap::heap()->reserved().end();\n+      log_info(aot, heap)(\"Heap end = %p\", heap_end);\n+      _requested_bottom = align_down(heap_end - heap_region_byte_size, G1HeapRegion::GrainBytes);\n+      _requested_bottom = align_down(_requested_bottom, MIN_GC_REGION_ALIGNMENT);\n+      assert(is_aligned(_requested_bottom, G1HeapRegion::GrainBytes), \"sanity\");\n+    } else {\n+      _requested_bottom = align_up(CompressedOops::begin(), MIN_GC_REGION_ALIGNMENT);\n+    }\n+  } else {\n+    \/\/ We always write the objects as if the heap started at this address. This\n+    \/\/ makes the contents of the archive heap deterministic.\n+    \/\/\n+    \/\/ Note that at runtime, the heap address is selected by the OS, so the archive\n+    \/\/ heap will not be mapped at 0x10000000, and the contents need to be patched.\n+    _requested_bottom = align_up((address)NOCOOPS_REQUESTED_BASE, MIN_GC_REGION_ALIGNMENT);\n+  }\n+\n+  assert(is_aligned(_requested_bottom, MIN_GC_REGION_ALIGNMENT), \"sanity\");\n+\n+  _requested_top = _requested_bottom + _buffer_used;\n+\n+  info->set_buffer_region(MemRegion(offset_to_buffered_address<HeapWord*>(0),\n+                                    offset_to_buffered_address<HeapWord*>(_buffer_used)));\n+  info->set_root_segments(_heap_root_segments);\n+}\n+\n+\/\/ Oop relocation\n+\n+template <typename T> T* AOTMappedHeapWriter::requested_addr_to_buffered_addr(T* p) {\n+  assert(is_in_requested_range(cast_to_oop(p)), \"must be\");\n+\n+  address addr = address(p);\n+  assert(addr >= _requested_bottom, \"must be\");\n+  size_t offset = addr - _requested_bottom;\n+  return offset_to_buffered_address<T*>(offset);\n+}\n+\n+template <typename T> oop AOTMappedHeapWriter::load_source_oop_from_buffer(T* buffered_addr) {\n+  oop o = load_oop_from_buffer(buffered_addr);\n+  assert(!in_buffer(cast_from_oop<address>(o)), \"must point to source oop\");\n+  return o;\n+}\n+\n+template <typename T> void AOTMappedHeapWriter::store_requested_oop_in_buffer(T* buffered_addr,\n+                                                                                   oop request_oop) {\n+  assert(request_oop == nullptr || is_in_requested_range(request_oop), \"must be\");\n+  store_oop_in_buffer(buffered_addr, request_oop);\n+}\n+\n+inline void AOTMappedHeapWriter::store_oop_in_buffer(oop* buffered_addr, oop requested_obj) {\n+  *buffered_addr = requested_obj;\n+}\n+\n+inline void AOTMappedHeapWriter::store_oop_in_buffer(narrowOop* buffered_addr, oop requested_obj) {\n+  narrowOop val = CompressedOops::encode(requested_obj);\n+  *buffered_addr = val;\n+}\n+\n+oop AOTMappedHeapWriter::load_oop_from_buffer(oop* buffered_addr) {\n+  return *buffered_addr;\n+}\n+\n+oop AOTMappedHeapWriter::load_oop_from_buffer(narrowOop* buffered_addr) {\n+  return CompressedOops::decode(*buffered_addr);\n+}\n+\n+template <typename T> void AOTMappedHeapWriter::relocate_field_in_buffer(T* field_addr_in_buffer, oop source_referent, CHeapBitMap* oopmap) {\n+  oop request_referent = source_obj_to_requested_obj(source_referent);\n+  if (UseCompressedOops && is_writing_deterministic_heap()) {\n+    \/\/ We use zero-based, 0-shift encoding, so the narrowOop is just the lower\n+    \/\/ 32 bits of request_referent\n+    intptr_t addr = cast_from_oop<intptr_t>(request_referent);\n+    *((narrowOop*)field_addr_in_buffer) = checked_cast<narrowOop>(addr);\n+  } else {\n+    store_requested_oop_in_buffer<T>(field_addr_in_buffer, request_referent);\n+  }\n+  if (request_referent != nullptr) {\n+    mark_oop_pointer<T>(field_addr_in_buffer, oopmap);\n+  }\n+}\n+\n+template <typename T> void AOTMappedHeapWriter::mark_oop_pointer(T* buffered_addr, CHeapBitMap* oopmap) {\n+  T* request_p = (T*)(buffered_addr_to_requested_addr((address)buffered_addr));\n+  address requested_region_bottom;\n+\n+  assert(request_p >= (T*)_requested_bottom, \"sanity\");\n+  assert(request_p <  (T*)_requested_top, \"sanity\");\n+  requested_region_bottom = _requested_bottom;\n+\n+  \/\/ Mark the pointer in the oopmap\n+  T* region_bottom = (T*)requested_region_bottom;\n+  assert(request_p >= region_bottom, \"must be\");\n+  BitMap::idx_t idx = request_p - region_bottom;\n+  assert(idx < oopmap->size(), \"overflow\");\n+  oopmap->set_bit(idx);\n+}\n+\n+void AOTMappedHeapWriter::update_header_for_requested_obj(oop requested_obj, oop src_obj,  Klass* src_klass) {\n+  assert(UseCompressedClassPointers, \"Archived heap only supported for compressed klasses\");\n+  narrowKlass nk = ArchiveBuilder::current()->get_requested_narrow_klass(src_klass);\n+  address buffered_addr = requested_addr_to_buffered_addr(cast_from_oop<address>(requested_obj));\n+\n+  oop fake_oop = cast_to_oop(buffered_addr);\n+  if (UseCompactObjectHeaders) {\n+    fake_oop->set_mark(markWord::prototype().set_narrow_klass(nk));\n+    assert(fake_oop->mark().narrow_klass() != 0, \"must not be null\");\n+  } else {\n+    fake_oop->set_narrow_klass(nk);\n+  }\n+\n+  if (src_obj == nullptr) {\n+    return;\n+  }\n+  \/\/ We need to retain the identity_hash, because it may have been used by some hashtables\n+  \/\/ in the shared heap.\n+  if (!src_obj->fast_no_hash_check()) {\n+    intptr_t src_hash = src_obj->identity_hash();\n+    if (UseCompactObjectHeaders) {\n+      markWord m = markWord::prototype().set_narrow_klass(nk);\n+      m = m.copy_hashctrl_from(src_obj->mark());\n+      fake_oop->set_mark(m);\n+      if (m.is_hashed_not_expanded()) {\n+        fake_oop->set_mark(fake_oop->initialize_hash_if_necessary(src_obj, src_klass, m));\n+      } else if (m.is_not_hashed_expanded()) {\n+        fake_oop->set_mark(m.set_not_hashed_not_expanded());\n+      }\n+      assert(!fake_oop->mark().is_not_hashed_expanded() && !fake_oop->mark().is_hashed_not_expanded(), \"must not be not-hashed-moved and not be hashed-not-moved\");\n+    } else {\n+      fake_oop->set_mark(markWord::prototype().copy_set_hash(src_hash));\n+      DEBUG_ONLY(intptr_t archived_hash = fake_oop->identity_hash());\n+      assert(src_hash == archived_hash, \"Different hash codes: original \" INTPTR_FORMAT \", archived \" INTPTR_FORMAT, src_hash, archived_hash);\n+    }\n+    assert(fake_oop->mark().is_unlocked(), \"sanity\");\n+  }\n+  \/\/ Strip age bits.\n+  fake_oop->set_mark(fake_oop->mark().set_age(0));\n+}\n+\n+class AOTMappedHeapWriter::EmbeddedOopRelocator: public BasicOopIterateClosure {\n+  oop _src_obj;\n+  address _buffered_obj;\n+  CHeapBitMap* _oopmap;\n+  bool _is_java_lang_ref;\n+public:\n+  EmbeddedOopRelocator(oop src_obj, address buffered_obj, CHeapBitMap* oopmap) :\n+    _src_obj(src_obj), _buffered_obj(buffered_obj), _oopmap(oopmap)\n+  {\n+    _is_java_lang_ref = AOTReferenceObjSupport::check_if_ref_obj(src_obj);\n+  }\n+\n+  void do_oop(narrowOop *p) { EmbeddedOopRelocator::do_oop_work(p); }\n+  void do_oop(      oop *p) { EmbeddedOopRelocator::do_oop_work(p); }\n+\n+private:\n+  template <class T> void do_oop_work(T *p) {\n+    int field_offset = pointer_delta_as_int((char*)p, cast_from_oop<char*>(_src_obj));\n+    T* field_addr = (T*)(_buffered_obj + field_offset);\n+    oop referent = load_source_oop_from_buffer<T>(field_addr);\n+    referent = HeapShared::maybe_remap_referent(_is_java_lang_ref, field_offset, referent);\n+    AOTMappedHeapWriter::relocate_field_in_buffer<T>(field_addr, referent, _oopmap);\n+  }\n+};\n+\n+static void log_bitmap_usage(const char* which, BitMap* bitmap, size_t total_bits) {\n+  \/\/ The whole heap is covered by total_bits, but there are only non-zero bits within [start ... end).\n+  size_t start = bitmap->find_first_set_bit(0);\n+  size_t end = bitmap->size();\n+  log_info(aot)(\"%s = %7zu ... %7zu (%3zu%% ... %3zu%% = %3zu%%)\", which,\n+                start, end,\n+                start * 100 \/ total_bits,\n+                end * 100 \/ total_bits,\n+                (end - start) * 100 \/ total_bits);\n+}\n+\n+\/\/ Update all oop fields embedded in the buffered objects\n+void AOTMappedHeapWriter::relocate_embedded_oops(GrowableArrayCHeap<oop, mtClassShared>* roots,\n+                                                      ArchiveMappedHeapInfo* heap_info) {\n+  size_t oopmap_unit = (UseCompressedOops ? sizeof(narrowOop) : sizeof(oop));\n+  size_t heap_region_byte_size = _buffer_used;\n+  heap_info->oopmap()->resize(heap_region_byte_size   \/ oopmap_unit);\n+\n+  for (int i = 0; i < _source_objs_order->length(); i++) {\n+    int src_obj_index = _source_objs_order->at(i)._index;\n+    oop src_obj = _source_objs->at(src_obj_index);\n+    HeapShared::CachedOopInfo* info = HeapShared::get_cached_oop_info(src_obj);\n+    assert(info != nullptr, \"must be\");\n+    oop requested_obj = requested_obj_from_buffer_offset(info->buffer_offset());\n+    update_header_for_requested_obj(requested_obj, src_obj, src_obj->klass());\n+    address buffered_obj = offset_to_buffered_address<address>(info->buffer_offset());\n+    EmbeddedOopRelocator relocator(src_obj, buffered_obj, heap_info->oopmap());\n+    src_obj->oop_iterate(&relocator);\n+    mark_native_pointers(src_obj);\n+  };\n+\n+  \/\/ Relocate HeapShared::roots(), which is created in copy_roots_to_buffer() and\n+  \/\/ doesn't have a corresponding src_obj, so we can't use EmbeddedOopRelocator on it.\n+  for (size_t seg_idx = 0; seg_idx < _heap_root_segments.count(); seg_idx++) {\n+    size_t seg_offset = _heap_root_segments.segment_offset(seg_idx);\n+\n+    objArrayOop requested_obj = (objArrayOop)requested_obj_from_buffer_offset(seg_offset);\n+    update_header_for_requested_obj(requested_obj, nullptr, Universe::objectArrayKlass());\n+    address buffered_obj = offset_to_buffered_address<address>(seg_offset);\n+    int length = _heap_root_segments.size_in_elems(seg_idx);\n+\n+    size_t elem_size = UseCompressedOops ? sizeof(narrowOop) : sizeof(oop);\n+\n+    for (int i = 0; i < length; i++) {\n+      \/\/ There is no source object; these are native oops - load, translate and\n+      \/\/ write back\n+      size_t elem_offset = objArrayOopDesc::base_offset_in_bytes() + elem_size * i;\n+      HeapWord* elem_addr = (HeapWord*)(buffered_obj + elem_offset);\n+      oop obj = NativeAccess<>::oop_load(elem_addr);\n+      obj = HeapShared::maybe_remap_referent(false \/* is_reference_field *\/, elem_offset, obj);\n+      if (UseCompressedOops) {\n+        relocate_field_in_buffer<narrowOop>((narrowOop*)elem_addr, obj, heap_info->oopmap());\n+      } else {\n+        relocate_field_in_buffer<oop>((oop*)elem_addr, obj, heap_info->oopmap());\n+      }\n+    }\n+  }\n+\n+  compute_ptrmap(heap_info);\n+\n+  size_t total_bytes = (size_t)_buffer->length();\n+  log_bitmap_usage(\"oopmap\", heap_info->oopmap(), total_bytes \/ (UseCompressedOops ? sizeof(narrowOop) : sizeof(oop)));\n+  log_bitmap_usage(\"ptrmap\", heap_info->ptrmap(), total_bytes \/ sizeof(address));\n+}\n+\n+void AOTMappedHeapWriter::mark_native_pointer(oop src_obj, int field_offset) {\n+  Metadata* ptr = src_obj->metadata_field_acquire(field_offset);\n+  if (ptr != nullptr) {\n+    NativePointerInfo info;\n+    info._src_obj = src_obj;\n+    info._field_offset = field_offset;\n+    _native_pointers->append(info);\n+    HeapShared::set_has_native_pointers(src_obj);\n+    _num_native_ptrs ++;\n+  }\n+}\n+\n+void AOTMappedHeapWriter::mark_native_pointers(oop orig_obj) {\n+  HeapShared::do_metadata_offsets(orig_obj, [&](int offset) {\n+    mark_native_pointer(orig_obj, offset);\n+  });\n+}\n+\n+void AOTMappedHeapWriter::compute_ptrmap(ArchiveMappedHeapInfo* heap_info) {\n+  int num_non_null_ptrs = 0;\n+  Metadata** bottom = (Metadata**) _requested_bottom;\n+  Metadata** top = (Metadata**) _requested_top; \/\/ exclusive\n+  heap_info->ptrmap()->resize(top - bottom);\n+\n+  BitMap::idx_t max_idx = 32; \/\/ paranoid - don't make it too small\n+  for (int i = 0; i < _native_pointers->length(); i++) {\n+    NativePointerInfo info = _native_pointers->at(i);\n+    oop src_obj = info._src_obj;\n+    int field_offset = info._field_offset;\n+    HeapShared::CachedOopInfo* p = HeapShared::get_cached_oop_info(src_obj);\n+    \/\/ requested_field_addr = the address of this field in the requested space\n+    oop requested_obj = requested_obj_from_buffer_offset(p->buffer_offset());\n+    Metadata** requested_field_addr = (Metadata**)(cast_from_oop<address>(requested_obj) + field_offset);\n+    assert(bottom <= requested_field_addr && requested_field_addr < top, \"range check\");\n+\n+    \/\/ Mark this field in the bitmap\n+    BitMap::idx_t idx = requested_field_addr - bottom;\n+    heap_info->ptrmap()->set_bit(idx);\n+    num_non_null_ptrs ++;\n+    max_idx = MAX2(max_idx, idx);\n+\n+    \/\/ Set the native pointer to the requested address of the metadata (at runtime, the metadata will have\n+    \/\/ this address if the RO\/RW regions are mapped at the default location).\n+\n+    Metadata** buffered_field_addr = requested_addr_to_buffered_addr(requested_field_addr);\n+    Metadata* native_ptr = *buffered_field_addr;\n+    guarantee(native_ptr != nullptr, \"sanity\");\n+\n+    if (RegeneratedClasses::has_been_regenerated(native_ptr)) {\n+      native_ptr = RegeneratedClasses::get_regenerated_object(native_ptr);\n+    }\n+\n+    guarantee(ArchiveBuilder::current()->has_been_archived((address)native_ptr),\n+              \"Metadata %p should have been archived\", native_ptr);\n+\n+    address buffered_native_ptr = ArchiveBuilder::current()->get_buffered_addr((address)native_ptr);\n+    address requested_native_ptr = ArchiveBuilder::current()->to_requested(buffered_native_ptr);\n+    *buffered_field_addr = (Metadata*)requested_native_ptr;\n+  }\n+\n+  heap_info->ptrmap()->resize(max_idx + 1);\n+  log_info(aot, heap)(\"calculate_ptrmap: marked %d non-null native pointers for heap region (%zu bits)\",\n+                      num_non_null_ptrs, size_t(heap_info->ptrmap()->size()));\n+}\n+\n+AOTMapLogger::OopDataIterator* AOTMappedHeapWriter::oop_iterator(ArchiveMappedHeapInfo* heap_info) {\n+  class MappedWriterOopIterator : public AOTMapLogger::OopDataIterator {\n+  private:\n+    address _current;\n+    address _next;\n+\n+    address _buffer_start;\n+    address _buffer_end;\n+    uint64_t _buffer_start_narrow_oop;\n+    intptr_t _buffer_to_requested_delta;\n+    int _requested_shift;\n+\n+    size_t _num_root_segments;\n+    size_t _num_obj_arrays_logged;\n+\n+  public:\n+    MappedWriterOopIterator(address buffer_start,\n+                            address buffer_end,\n+                            uint64_t buffer_start_narrow_oop,\n+                            intptr_t buffer_to_requested_delta,\n+                            int requested_shift,\n+                            size_t num_root_segments)\n+      : _current(nullptr),\n+        _next(buffer_start),\n+        _buffer_start(buffer_start),\n+        _buffer_end(buffer_end),\n+        _buffer_start_narrow_oop(buffer_start_narrow_oop),\n+        _buffer_to_requested_delta(buffer_to_requested_delta),\n+        _requested_shift(requested_shift),\n+        _num_root_segments(num_root_segments),\n+        _num_obj_arrays_logged(0) {\n+    }\n+\n+    AOTMapLogger::OopData capture(address buffered_addr) {\n+      oopDesc* raw_oop = (oopDesc*)buffered_addr;\n+      size_t size = size_of_buffered_oop(buffered_addr);\n+      address requested_addr = buffered_addr_to_requested_addr(buffered_addr);\n+      intptr_t target_location = (intptr_t)requested_addr;\n+      uint64_t pd = (uint64_t)(pointer_delta(buffered_addr, _buffer_start, 1));\n+      uint32_t narrow_location = checked_cast<uint32_t>(_buffer_start_narrow_oop + (pd >> _requested_shift));\n+      Klass* klass = real_klass_of_buffered_oop(buffered_addr);\n+\n+      return { buffered_addr,\n+               requested_addr,\n+               target_location,\n+               narrow_location,\n+               raw_oop,\n+               klass,\n+               size,\n+               false };\n+    }\n+\n+    bool has_next() override {\n+      return _next < _buffer_end;\n+    }\n+\n+    AOTMapLogger::OopData next() override {\n+      _current = _next;\n+      AOTMapLogger::OopData result = capture(_current);\n+      if (result._klass->is_objArray_klass()) {\n+        result._is_root_segment = _num_obj_arrays_logged++ < _num_root_segments;\n+      }\n+      _next = _current + result._size * BytesPerWord;\n+      return result;\n+    }\n+\n+    AOTMapLogger::OopData obj_at(narrowOop* addr) override {\n+      uint64_t n = (uint64_t)(*addr);\n+      if (n == 0) {\n+        return null_data();\n+      } else {\n+        precond(n >= _buffer_start_narrow_oop);\n+        address buffer_addr = _buffer_start + ((n - _buffer_start_narrow_oop) << _requested_shift);\n+        return capture(buffer_addr);\n+      }\n+    }\n+\n+    AOTMapLogger::OopData obj_at(oop* addr) override {\n+      address requested_value = cast_from_oop<address>(*addr);\n+      if (requested_value == nullptr) {\n+        return null_data();\n+      } else {\n+        address buffer_addr = requested_value - _buffer_to_requested_delta;\n+        return capture(buffer_addr);\n+      }\n+    }\n+\n+    GrowableArrayCHeap<AOTMapLogger::OopData, mtClass>* roots() override {\n+      return new GrowableArrayCHeap<AOTMapLogger::OopData, mtClass>();\n+    }\n+  };\n+\n+  MemRegion r = heap_info->buffer_region();\n+  address buffer_start = address(r.start());\n+  address buffer_end = address(r.end());\n+\n+  address requested_base = UseCompressedOops ? AOTMappedHeapWriter::narrow_oop_base() : (address)AOTMappedHeapWriter::NOCOOPS_REQUESTED_BASE;\n+  address requested_start = UseCompressedOops ? AOTMappedHeapWriter::buffered_addr_to_requested_addr(buffer_start) : requested_base;\n+  int requested_shift = AOTMappedHeapWriter::narrow_oop_shift();\n+  intptr_t buffer_to_requested_delta = requested_start - buffer_start;\n+  uint64_t buffer_start_narrow_oop = 0xdeadbeed;\n+  if (UseCompressedOops) {\n+    buffer_start_narrow_oop = (uint64_t)(pointer_delta(requested_start, requested_base, 1)) >> requested_shift;\n+    assert(buffer_start_narrow_oop < 0xffffffff, \"sanity\");\n+  }\n+\n+  return new MappedWriterOopIterator(buffer_start,\n+                                     buffer_end,\n+                                     buffer_start_narrow_oop,\n+                                     buffer_to_requested_delta,\n+                                     requested_shift,\n+                                     heap_info->root_segments().count());\n+}\n+\n+#endif \/\/ INCLUDE_CDS_JAVA_HEAP\n","filename":"src\/hotspot\/share\/cds\/aotMappedHeapWriter.cpp","additions":1039,"deletions":0,"binary":false,"changes":1039,"status":"added"},{"patch":"@@ -0,0 +1,2256 @@\n+\/*\n+ * Copyright (c) 2012, 2025, Oracle and\/or its affiliates. All rights reserved.\n+ * DO NOT ALTER OR REMOVE COPYRIGHT NOTICES OR THIS FILE HEADER.\n+ *\n+ * This code is free software; you can redistribute it and\/or modify it\n+ * under the terms of the GNU General Public License version 2 only, as\n+ * published by the Free Software Foundation.\n+ *\n+ * This code is distributed in the hope that it will be useful, but WITHOUT\n+ * ANY WARRANTY; without even the implied warranty of MERCHANTABILITY or\n+ * FITNESS FOR A PARTICULAR PURPOSE.  See the GNU General Public License\n+ * version 2 for more details (a copy is included in the LICENSE file that\n+ * accompanied this code).\n+ *\n+ * You should have received a copy of the GNU General Public License version\n+ * 2 along with this work; if not, write to the Free Software Foundation,\n+ * Inc., 51 Franklin St, Fifth Floor, Boston, MA 02110-1301 USA.\n+ *\n+ * Please contact Oracle, 500 Oracle Parkway, Redwood Shores, CA 94065 USA\n+ * or visit www.oracle.com if you need additional information or have any\n+ * questions.\n+ *\n+ *\/\n+\n+#include \"cds\/aotArtifactFinder.hpp\"\n+#include \"cds\/aotClassInitializer.hpp\"\n+#include \"cds\/aotClassLinker.hpp\"\n+#include \"cds\/aotClassLocation.hpp\"\n+#include \"cds\/aotConstantPoolResolver.hpp\"\n+#include \"cds\/aotLinkedClassBulkLoader.hpp\"\n+#include \"cds\/aotLogging.hpp\"\n+#include \"cds\/aotMapLogger.hpp\"\n+#include \"cds\/aotMappedHeapLoader.hpp\"\n+#include \"cds\/aotMetaspace.hpp\"\n+#include \"cds\/aotReferenceObjSupport.hpp\"\n+#include \"cds\/archiveBuilder.hpp\"\n+#include \"cds\/cds_globals.hpp\"\n+#include \"cds\/cdsConfig.hpp\"\n+#include \"cds\/cdsProtectionDomain.hpp\"\n+#include \"cds\/classListParser.hpp\"\n+#include \"cds\/classListWriter.hpp\"\n+#include \"cds\/cppVtables.hpp\"\n+#include \"cds\/dumpAllocStats.hpp\"\n+#include \"cds\/dynamicArchive.hpp\"\n+#include \"cds\/filemap.hpp\"\n+#include \"cds\/finalImageRecipes.hpp\"\n+#include \"cds\/heapShared.inline.hpp\"\n+#include \"cds\/lambdaFormInvokers.hpp\"\n+#include \"cds\/lambdaProxyClassDictionary.hpp\"\n+#include \"classfile\/classLoaderDataGraph.hpp\"\n+#include \"classfile\/classLoaderDataShared.hpp\"\n+#include \"classfile\/javaClasses.inline.hpp\"\n+#include \"classfile\/loaderConstraints.hpp\"\n+#include \"classfile\/modules.hpp\"\n+#include \"classfile\/placeholders.hpp\"\n+#include \"classfile\/stringTable.hpp\"\n+#include \"classfile\/symbolTable.hpp\"\n+#include \"classfile\/systemDictionary.hpp\"\n+#include \"classfile\/systemDictionaryShared.hpp\"\n+#include \"classfile\/vmClasses.hpp\"\n+#include \"classfile\/vmSymbols.hpp\"\n+#include \"code\/aotCodeCache.hpp\"\n+#include \"code\/codeCache.hpp\"\n+#include \"gc\/shared\/gcVMOperations.hpp\"\n+#include \"interpreter\/bytecodes.hpp\"\n+#include \"interpreter\/bytecodeStream.hpp\"\n+#include \"jvm_io.h\"\n+#include \"logging\/log.hpp\"\n+#include \"logging\/logMessage.hpp\"\n+#include \"logging\/logStream.hpp\"\n+#include \"memory\/memoryReserver.hpp\"\n+#include \"memory\/metaspace.hpp\"\n+#include \"memory\/metaspaceClosure.hpp\"\n+#include \"memory\/oopFactory.hpp\"\n+#include \"memory\/resourceArea.hpp\"\n+#include \"memory\/universe.hpp\"\n+#include \"nmt\/memTracker.hpp\"\n+#include \"oops\/compressedKlass.hpp\"\n+#include \"oops\/constantPool.inline.hpp\"\n+#include \"oops\/instanceMirrorKlass.hpp\"\n+#include \"oops\/klass.inline.hpp\"\n+#include \"oops\/objArrayOop.hpp\"\n+#include \"oops\/oop.inline.hpp\"\n+#include \"oops\/oopHandle.hpp\"\n+#include \"oops\/resolvedFieldEntry.hpp\"\n+#include \"oops\/trainingData.hpp\"\n+#include \"prims\/jvmtiExport.hpp\"\n+#include \"runtime\/arguments.hpp\"\n+#include \"runtime\/globals.hpp\"\n+#include \"runtime\/globals_extension.hpp\"\n+#include \"runtime\/handles.inline.hpp\"\n+#include \"runtime\/javaCalls.hpp\"\n+#include \"runtime\/os.inline.hpp\"\n+#include \"runtime\/safepointVerifiers.hpp\"\n+#include \"runtime\/sharedRuntime.hpp\"\n+#include \"runtime\/vmOperations.hpp\"\n+#include \"runtime\/vmThread.hpp\"\n+#include \"sanitizers\/leak.hpp\"\n+#include \"utilities\/align.hpp\"\n+#include \"utilities\/bitMap.inline.hpp\"\n+#include \"utilities\/defaultStream.hpp\"\n+#include \"utilities\/hashTable.hpp\"\n+#include \"utilities\/macros.hpp\"\n+#include \"utilities\/ostream.hpp\"\n+\n+#include <sys\/stat.h>\n+\n+ReservedSpace AOTMetaspace::_symbol_rs;\n+VirtualSpace AOTMetaspace::_symbol_vs;\n+bool AOTMetaspace::_archive_loading_failed = false;\n+bool AOTMetaspace::_remapped_readwrite = false;\n+void* AOTMetaspace::_aot_metaspace_static_top = nullptr;\n+intx AOTMetaspace::_relocation_delta;\n+char* AOTMetaspace::_requested_base_address;\n+Array<Method*>* AOTMetaspace::_archived_method_handle_intrinsics = nullptr;\n+bool AOTMetaspace::_use_optimized_module_handling = true;\n+int volatile AOTMetaspace::_preimage_static_archive_dumped = 0;\n+FileMapInfo* AOTMetaspace::_output_mapinfo = nullptr;\n+\n+\/\/ The CDS archive is divided into the following regions:\n+\/\/     rw  - read-write metadata\n+\/\/     ro  - read-only metadata and read-only tables\n+\/\/     hp  - heap region\n+\/\/     bm  - bitmap for relocating the above 7 regions.\n+\/\/\n+\/\/ The rw and ro regions are linearly allocated, in the order of rw->ro.\n+\/\/ These regions are aligned with AOTMetaspace::core_region_alignment().\n+\/\/\n+\/\/ These 2 regions are populated in the following steps:\n+\/\/ [0] All classes are loaded in AOTMetaspace::load_classes(). All metadata are\n+\/\/     temporarily allocated outside of the shared regions.\n+\/\/ [1] We enter a safepoint and allocate a buffer for the rw\/ro regions.\n+\/\/ [2] C++ vtables are copied into the rw region.\n+\/\/ [3] ArchiveBuilder copies RW metadata into the rw region.\n+\/\/ [4] ArchiveBuilder copies RO metadata into the ro region.\n+\/\/ [5] SymbolTable, StringTable, SystemDictionary, and a few other read-only data\n+\/\/     are copied into the ro region as read-only tables.\n+\/\/\n+\/\/ The heap region is written by HeapShared::write_heap().\n+\/\/\n+\/\/ The bitmap region is used to relocate the ro\/rw\/hp regions.\n+\n+static DumpRegion _symbol_region(\"symbols\");\n+\n+char* AOTMetaspace::symbol_space_alloc(size_t num_bytes) {\n+  return _symbol_region.allocate(num_bytes);\n+}\n+\n+\/\/ os::vm_allocation_granularity() is usually 4K for most OSes. However, some platforms\n+\/\/ such as linux-aarch64 and macos-x64 ...\n+\/\/ it can be either 4K or 64K and on macos-aarch64 it is 16K. To generate archives that are\n+\/\/ compatible for both settings, an alternative cds core region alignment can be enabled\n+\/\/ at building time:\n+\/\/   --enable-compactible-cds-alignment\n+\/\/ Upon successful configuration, the compactible alignment then can be defined in:\n+\/\/   os_linux_aarch64.cpp\n+\/\/   os_bsd_x86.cpp\n+size_t AOTMetaspace::core_region_alignment() {\n+  return os::cds_core_region_alignment();\n+}\n+\n+size_t AOTMetaspace::protection_zone_size() {\n+  return os::cds_core_region_alignment();\n+}\n+\n+static bool shared_base_valid(char* shared_base) {\n+  \/\/ We check user input for SharedBaseAddress at dump time.\n+\n+  \/\/ At CDS runtime, \"shared_base\" will be the (attempted) mapping start. It will also\n+  \/\/ be the encoding base, since the headers of archived base objects (and with Lilliput,\n+  \/\/ the prototype mark words) carry pre-computed narrow Klass IDs that refer to the mapping\n+  \/\/ start as base.\n+  \/\/\n+  \/\/ On AARCH64, The \"shared_base\" may not be later usable as encoding base, depending on the\n+  \/\/ total size of the reserved area and the precomputed_narrow_klass_shift. This is checked\n+  \/\/ before reserving memory.  Here we weed out values already known to be invalid later.\n+  return AARCH64_ONLY(is_aligned(shared_base, 4 * G)) NOT_AARCH64(true);\n+}\n+\n+class DumpClassListCLDClosure : public CLDClosure {\n+  static const int INITIAL_TABLE_SIZE = 1987;\n+  static const int MAX_TABLE_SIZE = 61333;\n+\n+  fileStream *_stream;\n+  ResizeableHashTable<InstanceKlass*, bool,\n+                              AnyObj::C_HEAP, mtClassShared> _dumped_classes;\n+\n+  void dump(InstanceKlass* ik) {\n+    bool created;\n+    _dumped_classes.put_if_absent(ik, &created);\n+    if (!created) {\n+      return;\n+    }\n+    if (_dumped_classes.maybe_grow()) {\n+      log_info(aot, hashtables)(\"Expanded _dumped_classes table to %d\", _dumped_classes.table_size());\n+    }\n+    if (ik->super()) {\n+      dump(ik->super());\n+    }\n+    Array<InstanceKlass*>* interfaces = ik->local_interfaces();\n+    int len = interfaces->length();\n+    for (int i = 0; i < len; i++) {\n+      dump(interfaces->at(i));\n+    }\n+    ClassListWriter::write_to_stream(ik, _stream);\n+  }\n+\n+public:\n+  DumpClassListCLDClosure(fileStream* f)\n+  : CLDClosure(), _dumped_classes(INITIAL_TABLE_SIZE, MAX_TABLE_SIZE) {\n+    _stream = f;\n+  }\n+\n+  void do_cld(ClassLoaderData* cld) {\n+    for (Klass* klass = cld->klasses(); klass != nullptr; klass = klass->next_link()) {\n+      if (klass->is_instance_klass()) {\n+        dump(InstanceKlass::cast(klass));\n+      }\n+    }\n+  }\n+};\n+\n+void AOTMetaspace::dump_loaded_classes(const char* file_name, TRAPS) {\n+  fileStream stream(file_name, \"w\");\n+  if (stream.is_open()) {\n+    MutexLocker lock(ClassLoaderDataGraph_lock);\n+    MutexLocker lock2(ClassListFile_lock, Mutex::_no_safepoint_check_flag);\n+    DumpClassListCLDClosure collect_classes(&stream);\n+    ClassLoaderDataGraph::loaded_cld_do(&collect_classes);\n+  } else {\n+    THROW_MSG(vmSymbols::java_io_IOException(), \"Failed to open file\");\n+  }\n+}\n+\n+static bool shared_base_too_high(char* specified_base, char* aligned_base, size_t cds_max) {\n+  \/\/ Caller should have checked that aligned_base was successfully aligned and is not nullptr.\n+  \/\/ Comparing specified_base with nullptr is UB.\n+  assert(aligned_base != nullptr, \"sanity\");\n+  assert(aligned_base >= specified_base, \"sanity\");\n+\n+  if (max_uintx - uintx(aligned_base) < uintx(cds_max)) {\n+    \/\/ Not enough address space to hold an archive of cds_max bytes from aligned_base.\n+    return true;\n+  } else {\n+    return false;\n+  }\n+}\n+\n+static char* compute_shared_base(size_t cds_max) {\n+  char* specified_base = (char*)SharedBaseAddress;\n+  size_t alignment = AOTMetaspace::core_region_alignment();\n+  if (UseCompressedClassPointers && CompressedKlassPointers::needs_class_space()) {\n+    alignment = MAX2(alignment, Metaspace::reserve_alignment());\n+  }\n+\n+  if (SharedBaseAddress == 0) {\n+    \/\/ Special meaning of -XX:SharedBaseAddress=0 -> Always map archive at os-selected address.\n+    return specified_base;\n+  }\n+\n+  char* aligned_base = can_align_up(specified_base, alignment)\n+                           ? align_up(specified_base, alignment)\n+                           : nullptr;\n+\n+  if (aligned_base != specified_base) {\n+    aot_log_info(aot)(\"SharedBaseAddress (\" INTPTR_FORMAT \") aligned up to \" INTPTR_FORMAT,\n+                   p2i(specified_base), p2i(aligned_base));\n+  }\n+\n+  const char* err = nullptr;\n+  if (aligned_base == nullptr) {\n+    err = \"too high\";\n+  } else if (shared_base_too_high(specified_base, aligned_base, cds_max)) {\n+    err = \"too high\";\n+  } else if (!shared_base_valid(aligned_base)) {\n+    err = \"invalid for this platform\";\n+  } else {\n+    return aligned_base;\n+  }\n+\n+  \/\/ Arguments::default_SharedBaseAddress() is hard-coded in cds_globals.hpp. It must be carefully\n+  \/\/ picked that (a) the align_up() below will always return a valid value; (b) none of\n+  \/\/ the following asserts will fail.\n+  aot_log_warning(aot)(\"SharedBaseAddress (\" INTPTR_FORMAT \") is %s. Reverted to \" INTPTR_FORMAT,\n+                   p2i((void*)SharedBaseAddress), err,\n+                   p2i((void*)Arguments::default_SharedBaseAddress()));\n+\n+  specified_base = (char*)Arguments::default_SharedBaseAddress();\n+  aligned_base = align_up(specified_base, alignment);\n+\n+  \/\/ Make sure the default value of SharedBaseAddress specified in globals.hpp is sane.\n+  assert(!shared_base_too_high(specified_base, aligned_base, cds_max), \"Sanity\");\n+  assert(shared_base_valid(aligned_base), \"Sanity\");\n+  return aligned_base;\n+}\n+\n+void AOTMetaspace::initialize_for_static_dump() {\n+  assert(CDSConfig::is_dumping_static_archive(), \"sanity\");\n+  aot_log_info(aot)(\"Core region alignment: %zu\", core_region_alignment());\n+  \/\/ The max allowed size for CDS archive. We use this to limit SharedBaseAddress\n+  \/\/ to avoid address space wrap around.\n+  size_t cds_max;\n+  const size_t reserve_alignment = core_region_alignment();\n+\n+#ifdef _LP64\n+  const uint64_t UnscaledClassSpaceMax = (uint64_t(max_juint) + 1);\n+  cds_max = align_down(UnscaledClassSpaceMax, reserve_alignment);\n+#else\n+  \/\/ We don't support archives larger than 256MB on 32-bit due to limited\n+  \/\/  virtual address space.\n+  cds_max = align_down(256*M, reserve_alignment);\n+#endif\n+\n+  _requested_base_address = compute_shared_base(cds_max);\n+  SharedBaseAddress = (size_t)_requested_base_address;\n+\n+  size_t symbol_rs_size = LP64_ONLY(3 * G) NOT_LP64(128 * M);\n+  _symbol_rs = MemoryReserver::reserve(symbol_rs_size,\n+                                       os::vm_allocation_granularity(),\n+                                       os::vm_page_size(),\n+                                       mtClassShared);\n+  if (!_symbol_rs.is_reserved()) {\n+    aot_log_error(aot)(\"Unable to reserve memory for symbols: %zu bytes.\", symbol_rs_size);\n+    AOTMetaspace::unrecoverable_writing_error();\n+  }\n+  _symbol_region.init(&_symbol_rs, &_symbol_vs);\n+  if (CDSConfig::is_dumping_preimage_static_archive()) {\n+    \/\/ We are in the AOT training run. User code is executed.\n+    \/\/\n+    \/\/ On Windows, if the user code closes System.out and we open the AOT config file for output\n+    \/\/ only at VM exit, we might get back the same file HANDLE as stdout, and the AOT config\n+    \/\/ file may get corrupted by UL logs. By opening early, we ensure that the output\n+    \/\/ HANDLE is different than stdout so we can avoid such corruption.\n+    open_output_mapinfo();\n+  } else {\n+    \/\/ No need for the above as we won't execute any user code.\n+  }\n+}\n+\n+void AOTMetaspace::open_output_mapinfo() {\n+  const char* static_archive = CDSConfig::output_archive_path();\n+  assert(static_archive != nullptr, \"sanity\");\n+  _output_mapinfo = new FileMapInfo(static_archive, true);\n+  _output_mapinfo->open_as_output();\n+}\n+\n+\/\/ Called by universe_post_init()\n+void AOTMetaspace::post_initialize(TRAPS) {\n+  if (CDSConfig::is_using_archive()) {\n+    FileMapInfo *static_mapinfo = FileMapInfo::current_info();\n+    FileMapInfo *dynamic_mapinfo = FileMapInfo::dynamic_info();\n+\n+    if (AOTMapLogger::is_logging_at_bootstrap()) {\n+      \/\/ The map logging needs to be done here, as it requires some stubs on Windows,\n+      \/\/ which are not generated until the end of init_globals().\n+      AOTMapLogger::runtime_log(static_mapinfo, dynamic_mapinfo);\n+    }\n+\n+    \/\/ Close any open file descriptors. However, mmap'ed pages will remain in memory.\n+    static_mapinfo->close();\n+\n+    if (HeapShared::is_loading() && HeapShared::is_loading_mapping_mode()) {\n+      static_mapinfo->unmap_region(AOTMetaspace::bm);\n+    }\n+\n+    if (dynamic_mapinfo != nullptr) {\n+      dynamic_mapinfo->close();\n+      dynamic_mapinfo->unmap_region(AOTMetaspace::bm);\n+    }\n+\n+    int size = AOTClassLocationConfig::runtime()->length();\n+    if (size > 0) {\n+      CDSProtectionDomain::allocate_shared_data_arrays(size, CHECK);\n+    }\n+  }\n+}\n+\n+\/\/ Extra java.lang.Strings to be added to the archive\n+static GrowableArrayCHeap<OopHandle, mtClassShared>* _extra_interned_strings = nullptr;\n+\/\/ Extra Symbols to be added to the archive\n+static GrowableArrayCHeap<Symbol*, mtClassShared>* _extra_symbols = nullptr;\n+\/\/ Methods managed by SystemDictionary::find_method_handle_intrinsic() to be added to the archive\n+static GrowableArray<Method*>* _pending_method_handle_intrinsics = nullptr;\n+\n+void AOTMetaspace::read_extra_data(JavaThread* current, const char* filename) {\n+  _extra_interned_strings = new GrowableArrayCHeap<OopHandle, mtClassShared>(10000);\n+  _extra_symbols = new GrowableArrayCHeap<Symbol*, mtClassShared>(1000);\n+\n+  HashtableTextDump reader(filename);\n+  reader.check_version(\"VERSION: 1.0\");\n+\n+  while (reader.remain() > 0) {\n+    int utf8_length;\n+    int prefix_type = reader.scan_prefix(&utf8_length);\n+    ResourceMark rm(current);\n+    if (utf8_length == 0x7fffffff) {\n+      \/\/ buf_len will overflown 32-bit value.\n+      aot_log_error(aot)(\"string length too large: %d\", utf8_length);\n+      AOTMetaspace::unrecoverable_loading_error();\n+    }\n+    int buf_len = utf8_length+1;\n+    char* utf8_buffer = NEW_RESOURCE_ARRAY(char, buf_len);\n+    reader.get_utf8(utf8_buffer, utf8_length);\n+    utf8_buffer[utf8_length] = '\\0';\n+\n+    if (prefix_type == HashtableTextDump::SymbolPrefix) {\n+      _extra_symbols->append(SymbolTable::new_permanent_symbol(utf8_buffer));\n+    } else{\n+      assert(prefix_type == HashtableTextDump::StringPrefix, \"Sanity\");\n+      ExceptionMark em(current);\n+      JavaThread* THREAD = current; \/\/ For exception macros.\n+      oop str = StringTable::intern(utf8_buffer, THREAD);\n+\n+      if (HAS_PENDING_EXCEPTION) {\n+        log_warning(aot, heap)(\"[line %d] extra interned string allocation failed; size too large: %d\",\n+                               reader.last_line_no(), utf8_length);\n+        CLEAR_PENDING_EXCEPTION;\n+      } else {\n+#if INCLUDE_CDS_JAVA_HEAP\n+        if (HeapShared::is_string_too_large_to_archive(str)) {\n+          log_warning(aot, heap)(\"[line %d] extra interned string ignored; size too large: %d\",\n+                                 reader.last_line_no(), utf8_length);\n+          continue;\n+        }\n+        \/\/ Make sure this string is included in the dumped interned string table.\n+        assert(str != nullptr, \"must succeed\");\n+        _extra_interned_strings->append(OopHandle(Universe::vm_global(), str));\n+#endif\n+      }\n+    }\n+  }\n+}\n+\n+void AOTMetaspace::make_method_handle_intrinsics_shareable() {\n+  for (int i = 0; i < _pending_method_handle_intrinsics->length(); i++) {\n+    Method* m = ArchiveBuilder::current()->get_buffered_addr(_pending_method_handle_intrinsics->at(i));\n+    m->remove_unshareable_info();\n+    \/\/ Each method has its own constant pool (which is distinct from m->method_holder()->constants());\n+    m->constants()->remove_unshareable_info();\n+  }\n+}\n+\n+void AOTMetaspace::write_method_handle_intrinsics() {\n+  int len = _pending_method_handle_intrinsics->length();\n+  _archived_method_handle_intrinsics = ArchiveBuilder::new_ro_array<Method*>(len);\n+  int word_size = _archived_method_handle_intrinsics->size();\n+  for (int i = 0; i < len; i++) {\n+    Method* m = _pending_method_handle_intrinsics->at(i);\n+    ArchiveBuilder::current()->write_pointer_in_buffer(_archived_method_handle_intrinsics->adr_at(i), m);\n+    word_size += m->size() + m->constMethod()->size() + m->constants()->size();\n+    if (m->constants()->cache() != nullptr) {\n+      word_size += m->constants()->cache()->size();\n+    }\n+  }\n+  log_info(aot)(\"Archived %d method handle intrinsics (%d bytes)\", len, word_size * BytesPerWord);\n+}\n+\n+\/\/ About \"serialize\" --\n+\/\/\n+\/\/ This is (probably a badly named) way to read\/write a data stream of pointers and\n+\/\/ miscellaneous data from\/to the shared archive file. The usual code looks like this:\n+\/\/\n+\/\/     \/\/ These two global C++ variables are initialized during dump time.\n+\/\/     static int _archived_int;\n+\/\/     static MetaspaceObj* archived_ptr;\n+\/\/\n+\/\/     void MyClass::serialize(SerializeClosure* soc) {\n+\/\/         soc->do_int(&_archived_int);\n+\/\/         soc->do_int(&_archived_ptr);\n+\/\/     }\n+\/\/\n+\/\/     At dumptime, these two variables are stored into the CDS archive.\n+\/\/     At runtime, these two variables are loaded from the CDS archive.\n+\/\/     In addition, the pointer is relocated as necessary.\n+\/\/\n+\/\/ Some of the xxx::serialize() functions may have side effects and assume that\n+\/\/ the archive is already mapped. For example, SymbolTable::serialize_shared_table_header()\n+\/\/ unconditionally makes the set of archived symbols available. Therefore, we put most\n+\/\/ of these xxx::serialize() functions inside AOTMetaspace::serialize(), which\n+\/\/ is called AFTER we made the decision to map the archive.\n+\/\/\n+\/\/ However, some of the \"serialized\" data are used to decide whether an archive should\n+\/\/ be mapped or not (e.g., for checking if the -Djdk.module.main property is compatible\n+\/\/ with the archive). The xxx::serialize() functions for these data must be put inside\n+\/\/ AOTMetaspace::early_serialize(). Such functions must not produce side effects that\n+\/\/ assume we will always decides to map the archive.\n+\n+void AOTMetaspace::early_serialize(SerializeClosure* soc) {\n+  int tag = 0;\n+  soc->do_tag(--tag);\n+  CDS_JAVA_HEAP_ONLY(Modules::serialize_archived_module_info(soc);)\n+  soc->do_tag(666);\n+}\n+\n+void AOTMetaspace::serialize(SerializeClosure* soc) {\n+  int tag = 0;\n+  soc->do_tag(--tag);\n+\n+  \/\/ Verify the sizes of various metadata in the system.\n+  soc->do_tag(sizeof(Method));\n+  soc->do_tag(sizeof(ConstMethod));\n+  soc->do_tag(arrayOopDesc::base_offset_in_bytes(T_BYTE));\n+  soc->do_tag(sizeof(ConstantPool));\n+  soc->do_tag(sizeof(ConstantPoolCache));\n+  soc->do_tag(objArrayOopDesc::base_offset_in_bytes());\n+  soc->do_tag(typeArrayOopDesc::base_offset_in_bytes(T_BYTE));\n+  soc->do_tag(sizeof(Symbol));\n+\n+  \/\/ Need to do this first, as subsequent steps may call virtual functions\n+  \/\/ in archived Metadata objects.\n+  CppVtables::serialize(soc);\n+  soc->do_tag(--tag);\n+\n+  \/\/ Dump\/restore miscellaneous metadata.\n+  JavaClasses::serialize_offsets(soc);\n+  Universe::serialize(soc);\n+  soc->do_tag(--tag);\n+\n+  \/\/ Dump\/restore references to commonly used names and signatures.\n+  vmSymbols::serialize(soc);\n+  soc->do_tag(--tag);\n+\n+  \/\/ Dump\/restore the symbol\/string\/subgraph_info tables\n+  SymbolTable::serialize_shared_table_header(soc);\n+  StringTable::serialize_shared_table_header(soc);\n+  HeapShared::serialize_tables(soc);\n+  SystemDictionaryShared::serialize_dictionary_headers(soc);\n+  AOTLinkedClassBulkLoader::serialize(soc);\n+  FinalImageRecipes::serialize(soc);\n+  TrainingData::serialize(soc);\n+  InstanceMirrorKlass::serialize_offsets(soc);\n+\n+  \/\/ Dump\/restore well known classes (pointers)\n+  SystemDictionaryShared::serialize_vm_classes(soc);\n+  soc->do_tag(--tag);\n+\n+  CDS_JAVA_HEAP_ONLY(ClassLoaderDataShared::serialize(soc);)\n+  soc->do_ptr((void**)&_archived_method_handle_intrinsics);\n+\n+  LambdaFormInvokers::serialize(soc);\n+  AdapterHandlerLibrary::serialize_shared_table_header(soc);\n+\n+  soc->do_tag(666);\n+}\n+\n+\/\/ In AOTCache workflow, when dumping preimage, the constant pool entries are stored in unresolved state.\n+\/\/ So the fast version of getfield\/putfield needs to be converted to nofast version.\n+\/\/ When dumping the final image in the assembly phase, these nofast versions are converted back to fast versions\n+\/\/ if the constant pool entry refered by these bytecodes is stored in resolved state.\n+\/\/ Same principle applies to static and dynamic archives. If the constant pool entry is in resolved state, then\n+\/\/ the fast version of the bytecodes can be preserved, else use the nofast version.\n+\/\/\n+\/\/ The fast versions of aload_0 (i.e. _fast_Xaccess_0) merges the bytecode pair (aload_0, fast_Xgetfield).\n+\/\/ If the fast version of aload_0 is preserved in AOTCache, then the JVMTI notifications for field access and\n+\/\/ breakpoint events will be skipped for the second bytecode (fast_Xgetfield) in the pair.\n+\/\/ Same holds for fast versions of iload_0. So for these bytecodes, nofast version is used.\n+static void rewrite_bytecodes(const methodHandle& method) {\n+  ConstantPool* cp = method->constants();\n+  BytecodeStream bcs(method);\n+  Bytecodes::Code new_code;\n+\n+  LogStreamHandle(Trace, aot, resolve) lsh;\n+  if (lsh.is_enabled()) {\n+    lsh.print(\"Rewriting bytecodes for \");\n+    method()->print_external_name(&lsh);\n+    lsh.print(\"\\n\");\n+  }\n+\n+  while (!bcs.is_last_bytecode()) {\n+    Bytecodes::Code opcode = bcs.next();\n+    \/\/ Use current opcode as the default value of new_code\n+    new_code = opcode;\n+    switch(opcode) {\n+    case Bytecodes::_getfield: {\n+      uint rfe_index = bcs.get_index_u2();\n+      bool is_resolved = cp->is_resolved(rfe_index, opcode);\n+      if (is_resolved) {\n+        assert(!CDSConfig::is_dumping_preimage_static_archive(), \"preimage should not have resolved field references\");\n+        ResolvedFieldEntry* rfe = cp->resolved_field_entry_at(bcs.get_index_u2());\n+        switch(rfe->tos_state()) {\n+        case btos:\n+          \/\/ fallthrough\n+        case ztos: new_code = Bytecodes::_fast_bgetfield; break;\n+        case atos: new_code = Bytecodes::_fast_agetfield; break;\n+        case itos: new_code = Bytecodes::_fast_igetfield; break;\n+        case ctos: new_code = Bytecodes::_fast_cgetfield; break;\n+        case stos: new_code = Bytecodes::_fast_sgetfield; break;\n+        case ltos: new_code = Bytecodes::_fast_lgetfield; break;\n+        case ftos: new_code = Bytecodes::_fast_fgetfield; break;\n+        case dtos: new_code = Bytecodes::_fast_dgetfield; break;\n+        default:\n+          ShouldNotReachHere();\n+          break;\n+        }\n+      } else {\n+        new_code = Bytecodes::_nofast_getfield;\n+      }\n+      break;\n+    }\n+    case Bytecodes::_putfield: {\n+      uint rfe_index = bcs.get_index_u2();\n+      bool is_resolved = cp->is_resolved(rfe_index, opcode);\n+      if (is_resolved) {\n+        assert(!CDSConfig::is_dumping_preimage_static_archive(), \"preimage should not have resolved field references\");\n+        ResolvedFieldEntry* rfe = cp->resolved_field_entry_at(bcs.get_index_u2());\n+        switch(rfe->tos_state()) {\n+        case btos: new_code = Bytecodes::_fast_bputfield; break;\n+        case ztos: new_code = Bytecodes::_fast_zputfield; break;\n+        case atos: new_code = Bytecodes::_fast_aputfield; break;\n+        case itos: new_code = Bytecodes::_fast_iputfield; break;\n+        case ctos: new_code = Bytecodes::_fast_cputfield; break;\n+        case stos: new_code = Bytecodes::_fast_sputfield; break;\n+        case ltos: new_code = Bytecodes::_fast_lputfield; break;\n+        case ftos: new_code = Bytecodes::_fast_fputfield; break;\n+        case dtos: new_code = Bytecodes::_fast_dputfield; break;\n+        default:\n+          ShouldNotReachHere();\n+          break;\n+        }\n+      } else {\n+        new_code = Bytecodes::_nofast_putfield;\n+      }\n+      break;\n+    }\n+    case Bytecodes::_aload_0:\n+      \/\/ Revert _fast_Xaccess_0 or _aload_0 to _nofast_aload_0\n+      new_code = Bytecodes::_nofast_aload_0;\n+      break;\n+    case Bytecodes::_iload:\n+      if (!bcs.is_wide()) {\n+        new_code = Bytecodes::_nofast_iload;\n+      }\n+      break;\n+    default:\n+      break;\n+    }\n+    if (opcode != new_code) {\n+      *bcs.bcp() = new_code;\n+      if (lsh.is_enabled()) {\n+        lsh.print_cr(\"%d:%s -> %s\", bcs.bci(), Bytecodes::name(opcode), Bytecodes::name(new_code));\n+      }\n+    }\n+  }\n+}\n+\n+\/\/ [1] Rewrite all bytecodes as needed, so that the ConstMethod* will not be modified\n+\/\/     at run time by RewriteBytecodes\/RewriteFrequentPairs\n+\/\/ [2] Assign a fingerprint, so one doesn't need to be assigned at run-time.\n+void AOTMetaspace::rewrite_bytecodes_and_calculate_fingerprints(Thread* thread, InstanceKlass* ik) {\n+  for (int i = 0; i < ik->methods()->length(); i++) {\n+    methodHandle m(thread, ik->methods()->at(i));\n+    if (ik->can_be_verified_at_dumptime() && ik->is_linked()) {\n+      rewrite_bytecodes(m);\n+    }\n+    Fingerprinter fp(m);\n+    \/\/ The side effect of this call sets method's fingerprint field.\n+    fp.fingerprint();\n+  }\n+}\n+\n+class VM_PopulateDumpSharedSpace : public VM_Operation {\n+private:\n+  ArchiveMappedHeapInfo _mapped_heap_info;\n+  ArchiveStreamedHeapInfo _streamed_heap_info;\n+  FileMapInfo* _map_info;\n+  StaticArchiveBuilder& _builder;\n+\n+  void dump_java_heap_objects();\n+  void dump_shared_symbol_table(GrowableArray<Symbol*>* symbols) {\n+    log_info(aot)(\"Dumping symbol table ...\");\n+    SymbolTable::write_to_archive(symbols);\n+  }\n+  char* dump_early_read_only_tables();\n+  char* dump_read_only_tables(AOTClassLocationConfig*& cl_config);\n+\n+public:\n+\n+  VM_PopulateDumpSharedSpace(StaticArchiveBuilder& b, FileMapInfo* map_info) :\n+    VM_Operation(), _mapped_heap_info(), _streamed_heap_info(), _map_info(map_info), _builder(b) {}\n+\n+  bool skip_operation() const { return false; }\n+\n+  VMOp_Type type() const { return VMOp_PopulateDumpSharedSpace; }\n+  ArchiveMappedHeapInfo* mapped_heap_info()  { return &_mapped_heap_info; }\n+  ArchiveStreamedHeapInfo* streamed_heap_info()  { return &_streamed_heap_info; }\n+  void doit();   \/\/ outline because gdb sucks\n+  bool allow_nested_vm_operations() const { return true; }\n+}; \/\/ class VM_PopulateDumpSharedSpace\n+\n+class StaticArchiveBuilder : public ArchiveBuilder {\n+public:\n+  StaticArchiveBuilder() : ArchiveBuilder() {}\n+\n+  virtual void iterate_roots(MetaspaceClosure* it) {\n+    AOTArtifactFinder::all_cached_classes_do(it);\n+    SystemDictionaryShared::dumptime_classes_do(it);\n+    Universe::metaspace_pointers_do(it);\n+    vmSymbols::metaspace_pointers_do(it);\n+    TrainingData::iterate_roots(it);\n+\n+    \/\/ The above code should find all the symbols that are referenced by the\n+    \/\/ archived classes. We just need to add the extra symbols which\n+    \/\/ may not be used by any of the archived classes -- these are usually\n+    \/\/ symbols that we anticipate to be used at run time, so we can store\n+    \/\/ them in the RO region, to be shared across multiple processes.\n+    if (_extra_symbols != nullptr) {\n+      for (int i = 0; i < _extra_symbols->length(); i++) {\n+        it->push(_extra_symbols->adr_at(i));\n+      }\n+    }\n+\n+    for (int i = 0; i < _pending_method_handle_intrinsics->length(); i++) {\n+      it->push(_pending_method_handle_intrinsics->adr_at(i));\n+    }\n+  }\n+};\n+\n+char* VM_PopulateDumpSharedSpace::dump_early_read_only_tables() {\n+  ArchiveBuilder::OtherROAllocMark mark;\n+\n+  CDS_JAVA_HEAP_ONLY(Modules::dump_archived_module_info());\n+\n+  DumpRegion* ro_region = ArchiveBuilder::current()->ro_region();\n+  char* start = ro_region->top();\n+  WriteClosure wc(ro_region);\n+  AOTMetaspace::early_serialize(&wc);\n+  return start;\n+}\n+\n+char* VM_PopulateDumpSharedSpace::dump_read_only_tables(AOTClassLocationConfig*& cl_config) {\n+  ArchiveBuilder::OtherROAllocMark mark;\n+\n+  SystemDictionaryShared::write_to_archive();\n+  cl_config = AOTClassLocationConfig::dumptime()->write_to_archive();\n+  AOTClassLinker::write_to_archive();\n+  if (CDSConfig::is_dumping_preimage_static_archive()) {\n+    FinalImageRecipes::record_recipes();\n+  }\n+\n+  TrainingData::dump_training_data();\n+\n+  AOTMetaspace::write_method_handle_intrinsics();\n+\n+  \/\/ Write lambform lines into archive\n+  LambdaFormInvokers::dump_static_archive_invokers();\n+\n+  if (CDSConfig::is_dumping_adapters()) {\n+    AdapterHandlerLibrary::dump_aot_adapter_table();\n+  }\n+\n+  \/\/ Write the other data to the output array.\n+  DumpRegion* ro_region = ArchiveBuilder::current()->ro_region();\n+  char* start = ro_region->top();\n+  WriteClosure wc(ro_region);\n+  AOTMetaspace::serialize(&wc);\n+\n+  return start;\n+}\n+\n+void VM_PopulateDumpSharedSpace::doit() {\n+  CDSConfig::set_is_at_aot_safepoint(true);\n+\n+  if (!CDSConfig::is_dumping_final_static_archive()) {\n+    guarantee(!CDSConfig::is_using_archive(), \"We should not be using an archive when we dump\");\n+  }\n+\n+  DEBUG_ONLY(SystemDictionaryShared::NoClassLoadingMark nclm);\n+\n+  _pending_method_handle_intrinsics = new (mtClassShared) GrowableArray<Method*>(256, mtClassShared);\n+  if (CDSConfig::is_dumping_method_handles()) {\n+    \/\/ When dumping AOT-linked classes, some classes may have direct references to a method handle\n+    \/\/ intrinsic. The easiest thing is to save all of them into the AOT cache.\n+    SystemDictionary::get_all_method_handle_intrinsics(_pending_method_handle_intrinsics);\n+  }\n+\n+  AOTClassLocationConfig::dumptime_check_nonempty_dirs();\n+\n+  NOT_PRODUCT(SystemDictionary::verify();)\n+\n+  \/\/ Block concurrent class unloading from changing the _dumptime_table\n+  MutexLocker ml(DumpTimeTable_lock, Mutex::_no_safepoint_check_flag);\n+\n+  _builder.gather_source_objs();\n+  _builder.reserve_buffer();\n+\n+  CppVtables::dumptime_init(&_builder);\n+\n+  _builder.sort_metadata_objs();\n+  _builder.dump_rw_metadata();\n+  _builder.dump_ro_metadata();\n+  _builder.relocate_metaspaceobj_embedded_pointers();\n+\n+  log_info(aot)(\"Make classes shareable\");\n+  _builder.make_klasses_shareable();\n+  AOTMetaspace::make_method_handle_intrinsics_shareable();\n+\n+  dump_java_heap_objects();\n+  dump_shared_symbol_table(_builder.symbols());\n+\n+  char* early_serialized_data = dump_early_read_only_tables();\n+  AOTClassLocationConfig* cl_config;\n+  char* serialized_data = dump_read_only_tables(cl_config);\n+\n+  if (CDSConfig::is_dumping_lambdas_in_legacy_mode()) {\n+    log_info(aot)(\"Adjust lambda proxy class dictionary\");\n+    LambdaProxyClassDictionary::adjust_dumptime_table();\n+  }\n+\n+  log_info(cds)(\"Make training data shareable\");\n+  _builder.make_training_data_shareable();\n+\n+  \/\/ The vtable clones contain addresses of the current process.\n+  \/\/ We don't want to write these addresses into the archive.\n+  CppVtables::zero_archived_vtables();\n+\n+  \/\/ Write the archive file\n+  _map_info->populate_header(AOTMetaspace::core_region_alignment());\n+  _map_info->set_early_serialized_data(early_serialized_data);\n+  _map_info->set_serialized_data(serialized_data);\n+  _map_info->set_cloned_vtables(CppVtables::vtables_serialized_base());\n+  _map_info->header()->set_class_location_config(cl_config);\n+\n+  HeapShared::delete_tables_with_raw_oops();\n+  CDSConfig::set_is_at_aot_safepoint(false);\n+}\n+\n+class CollectClassesForLinking : public KlassClosure {\n+  GrowableArray<OopHandle> _mirrors;\n+\n+public:\n+   CollectClassesForLinking() : _mirrors() {\n+     \/\/ ClassLoaderDataGraph::loaded_classes_do_keepalive() requires ClassLoaderDataGraph_lock.\n+     \/\/ We cannot link the classes while holding this lock (or else we may run into deadlock).\n+     \/\/ Therefore, we need to first collect all the classes, keeping them alive by\n+     \/\/ holding onto their java_mirrors in global OopHandles. We then link the classes after\n+     \/\/ releasing the lock.\n+     MutexLocker lock(ClassLoaderDataGraph_lock);\n+     ClassLoaderDataGraph::loaded_classes_do_keepalive(this);\n+   }\n+\n+  ~CollectClassesForLinking() {\n+    for (int i = 0; i < _mirrors.length(); i++) {\n+      _mirrors.at(i).release(Universe::vm_global());\n+    }\n+  }\n+\n+  void do_cld(ClassLoaderData* cld) {\n+    assert(cld->is_alive(), \"must be\");\n+  }\n+\n+  void do_klass(Klass* k) {\n+    if (k->is_instance_klass()) {\n+      _mirrors.append(OopHandle(Universe::vm_global(), k->java_mirror()));\n+    }\n+  }\n+\n+  const GrowableArray<OopHandle>* mirrors() const { return &_mirrors; }\n+};\n+\n+\/\/ Check if we can eagerly link this class at dump time, so we can avoid the\n+\/\/ runtime linking overhead (especially verification)\n+bool AOTMetaspace::may_be_eagerly_linked(InstanceKlass* ik) {\n+  if (!ik->can_be_verified_at_dumptime()) {\n+    \/\/ For old classes, try to leave them in the unlinked state, so\n+    \/\/ we can still store them in the archive. They must be\n+    \/\/ linked\/verified at runtime.\n+    return false;\n+  }\n+  if (CDSConfig::is_dumping_dynamic_archive() && ik->defined_by_other_loaders()) {\n+    \/\/ Linking of unregistered classes at this stage may cause more\n+    \/\/ classes to be resolved, resulting in calls to ClassLoader.loadClass()\n+    \/\/ that may not be expected by custom class loaders.\n+    \/\/\n+    \/\/ It's OK to do this for the built-in loaders as we know they can\n+    \/\/ tolerate this.\n+    return false;\n+  }\n+  return true;\n+}\n+\n+void AOTMetaspace::link_all_loaded_classes(JavaThread* current) {\n+  while (true) {\n+    ResourceMark rm(current);\n+    CollectClassesForLinking collect_classes;\n+    bool has_linked = false;\n+    const GrowableArray<OopHandle>* mirrors = collect_classes.mirrors();\n+    for (int i = 0; i < mirrors->length(); i++) {\n+      OopHandle mirror = mirrors->at(i);\n+      InstanceKlass* ik = java_lang_Class::as_InstanceKlass(mirror.resolve());\n+      if (may_be_eagerly_linked(ik)) {\n+        has_linked |= try_link_class(current, ik);\n+      }\n+    }\n+\n+    if (!has_linked) {\n+      break;\n+    }\n+    \/\/ Class linking includes verification which may load more classes.\n+    \/\/ Keep scanning until we have linked no more classes.\n+  }\n+}\n+\n+void AOTMetaspace::link_shared_classes(TRAPS) {\n+  AOTClassLinker::initialize();\n+  AOTClassInitializer::init_test_class(CHECK);\n+\n+  if (CDSConfig::is_dumping_final_static_archive()) {\n+    \/\/ - Load and link all classes used in the training run.\n+    \/\/ - Initialize @AOTSafeClassInitializer classes that were\n+    \/\/   initialized in the training run.\n+    \/\/ - Perform per-class optimization such as AOT-resolution of\n+    \/\/   constant pool entries that were resolved during the training run.\n+    FinalImageRecipes::apply_recipes(CHECK);\n+\n+    \/\/ Because the AOT assembly phase does not run the same exact code as in the\n+    \/\/ training run (e.g., we use different lambda form invoker classes;\n+    \/\/ generated lambda form classes are not recorded in FinalImageRecipes),\n+    \/\/ the recipes do not cover all classes that have been loaded so far. As\n+    \/\/ a result, we might have some unlinked classes at this point. Since we\n+    \/\/ require cached classes to be linked, all such classes will be linked\n+    \/\/ by the following step.\n+  }\n+\n+  link_all_loaded_classes(THREAD);\n+\n+  \/\/ Eargerly resolve all string constants in constant pools\n+  {\n+    ResourceMark rm(THREAD);\n+    CollectClassesForLinking collect_classes;\n+    const GrowableArray<OopHandle>* mirrors = collect_classes.mirrors();\n+    for (int i = 0; i < mirrors->length(); i++) {\n+      OopHandle mirror = mirrors->at(i);\n+      InstanceKlass* ik = java_lang_Class::as_InstanceKlass(mirror.resolve());\n+      AOTConstantPoolResolver::preresolve_string_cp_entries(ik, CHECK);\n+    }\n+  }\n+}\n+\n+void AOTMetaspace::dump_static_archive(TRAPS) {\n+  CDSConfig::DumperThreadMark dumper_thread_mark(THREAD);\n+  ResourceMark rm(THREAD);\n+  HandleMark hm(THREAD);\n+\n+ if (CDSConfig::is_dumping_final_static_archive() && AOTPrintTrainingInfo) {\n+   tty->print_cr(\"==================== archived_training_data ** before dumping ====================\");\n+   TrainingData::print_archived_training_data_on(tty);\n+ }\n+\n+  StaticArchiveBuilder builder;\n+  dump_static_archive_impl(builder, THREAD);\n+  if (HAS_PENDING_EXCEPTION) {\n+    if (PENDING_EXCEPTION->is_a(vmClasses::OutOfMemoryError_klass())) {\n+      aot_log_error(aot)(\"Out of memory. Please run with a larger Java heap, current MaxHeapSize = \"\n+                     \"%zuM\", MaxHeapSize\/M);\n+      AOTMetaspace::writing_error();\n+    } else {\n+      oop message = java_lang_Throwable::message(PENDING_EXCEPTION);\n+      aot_log_error(aot)(\"%s: %s\", PENDING_EXCEPTION->klass()->external_name(),\n+                         message == nullptr ? \"(null)\" : java_lang_String::as_utf8_string(message));\n+      AOTMetaspace::writing_error(err_msg(\"Unexpected exception, use -Xlog:aot%s,exceptions=trace for detail\",\n+                                             CDSConfig::new_aot_flags_used() ? \"\" : \",cds\"));\n+    }\n+  }\n+\n+  if (CDSConfig::new_aot_flags_used()) {\n+    if (CDSConfig::is_dumping_preimage_static_archive()) {\n+      \/\/ We are in the JVM that runs the training run. Continue execution,\n+      \/\/ so that it can finish all clean-up and return the correct exit\n+      \/\/ code to the OS.\n+    } else {\n+      \/\/ The JLI launcher only recognizes the \"old\" -Xshare:dump flag.\n+      \/\/ When the new -XX:AOTMode=create flag is used, we can't return\n+      \/\/ to the JLI launcher, as the launcher will fail when trying to\n+      \/\/ run the main class, which is not what we want.\n+      struct stat st;\n+      if (os::stat(AOTCache, &st) != 0) {\n+        tty->print_cr(\"AOTCache creation failed: %s\", AOTCache);\n+      } else {\n+        tty->print_cr(\"AOTCache creation is complete: %s \" INT64_FORMAT \" bytes\", AOTCache, (int64_t)(st.st_size));\n+      }\n+      vm_direct_exit(0);\n+    }\n+  }\n+}\n+\n+#if INCLUDE_CDS_JAVA_HEAP && defined(_LP64)\n+void AOTMetaspace::adjust_heap_sizes_for_dumping() {\n+  if (!CDSConfig::is_dumping_heap() || UseCompressedOops) {\n+    return;\n+  }\n+  \/\/ CDS heap dumping requires all string oops to have an offset\n+  \/\/ from the heap bottom that can be encoded in 32-bit.\n+  julong max_heap_size = (julong)(4 * G);\n+\n+  if (MinHeapSize > max_heap_size) {\n+    log_debug(aot)(\"Setting MinHeapSize to 4G for CDS dumping, original size = %zuM\", MinHeapSize\/M);\n+    FLAG_SET_ERGO(MinHeapSize, max_heap_size);\n+  }\n+  if (InitialHeapSize > max_heap_size) {\n+    log_debug(aot)(\"Setting InitialHeapSize to 4G for CDS dumping, original size = %zuM\", InitialHeapSize\/M);\n+    FLAG_SET_ERGO(InitialHeapSize, max_heap_size);\n+  }\n+  if (MaxHeapSize > max_heap_size) {\n+    log_debug(aot)(\"Setting MaxHeapSize to 4G for CDS dumping, original size = %zuM\", MaxHeapSize\/M);\n+    FLAG_SET_ERGO(MaxHeapSize, max_heap_size);\n+  }\n+}\n+#endif \/\/ INCLUDE_CDS_JAVA_HEAP && _LP64\n+\n+void AOTMetaspace::get_default_classlist(char* default_classlist, const size_t buf_size) {\n+  const char* filesep = os::file_separator();\n+  jio_snprintf(default_classlist, buf_size, \"%s%slib%sclasslist\",\n+               Arguments::get_java_home(), filesep, filesep);\n+}\n+\n+void AOTMetaspace::load_classes(TRAPS) {\n+  char default_classlist[JVM_MAXPATHLEN];\n+  const char* classlist_path;\n+\n+  get_default_classlist(default_classlist, JVM_MAXPATHLEN);\n+  if (SharedClassListFile == nullptr) {\n+    classlist_path = default_classlist;\n+  } else {\n+    classlist_path = SharedClassListFile;\n+  }\n+\n+  aot_log_info(aot)(\"Loading classes to share ...\");\n+  ClassListParser::parse_classlist(classlist_path,\n+                                   ClassListParser::_parse_all, CHECK);\n+  if (ExtraSharedClassListFile) {\n+    ClassListParser::parse_classlist(ExtraSharedClassListFile,\n+                                     ClassListParser::_parse_all, CHECK);\n+  }\n+  if (classlist_path != default_classlist) {\n+    struct stat statbuf;\n+    if (os::stat(default_classlist, &statbuf) == 0) {\n+      \/\/ File exists, let's use it.\n+      ClassListParser::parse_classlist(default_classlist,\n+                                       ClassListParser::_parse_lambda_forms_invokers_only, CHECK);\n+    }\n+  }\n+\n+  \/\/ Some classes are used at CDS runtime but are not yet loaded at this point.\n+  \/\/ We can perform dummmy calls to these classes at dumptime to ensure they\n+  \/\/ are archived.\n+  exercise_runtime_cds_code(CHECK);\n+\n+  aot_log_info(aot)(\"Loading classes to share: done.\");\n+}\n+\n+void AOTMetaspace::exercise_runtime_cds_code(TRAPS) {\n+  \/\/ Exercise the manifest processing code\n+  const char* dummy = \"Manifest-Version: 1.0\\n\";\n+  CDSProtectionDomain::create_jar_manifest(dummy, strlen(dummy), CHECK);\n+\n+  \/\/ Exercise FileSystem and URL code\n+  CDSProtectionDomain::to_file_URL(\"dummy.jar\", Handle(), CHECK);\n+}\n+\n+bool AOTMetaspace::preimage_static_archive_dumped() {\n+  assert(CDSConfig::is_dumping_preimage_static_archive(), \"Required\");\n+  return AtomicAccess::load_acquire(&_preimage_static_archive_dumped) == 1;\n+}\n+\n+void AOTMetaspace::dump_static_archive_impl(StaticArchiveBuilder& builder, TRAPS) {\n+  if (CDSConfig::is_dumping_preimage_static_archive()) {\n+    \/\/ When dumping to the AOT configuration file ensure this function is only executed once.\n+    \/\/ Multiple invocations may happen via JCmd, during VM exit or other means (in the future)\n+    \/\/ from different threads and possibly concurrently.\n+    if (AtomicAccess::cmpxchg(&_preimage_static_archive_dumped, 0, 1) != 0) {\n+      return;\n+    }\n+  }\n+\n+  if (CDSConfig::is_dumping_classic_static_archive()) {\n+    \/\/ We are running with -Xshare:dump\n+    load_classes(CHECK);\n+\n+    if (SharedArchiveConfigFile) {\n+      log_info(aot)(\"Reading extra data from %s ...\", SharedArchiveConfigFile);\n+      read_extra_data(THREAD, SharedArchiveConfigFile);\n+      log_info(aot)(\"Reading extra data: done.\");\n+    }\n+  }\n+\n+  if (CDSConfig::is_dumping_preimage_static_archive()) {\n+    log_info(aot)(\"Reading lambda form invokers from JDK default classlist ...\");\n+    char default_classlist[JVM_MAXPATHLEN];\n+    get_default_classlist(default_classlist, JVM_MAXPATHLEN);\n+    struct stat statbuf;\n+    if (os::stat(default_classlist, &statbuf) == 0) {\n+      ClassListParser::parse_classlist(default_classlist,\n+                                       ClassListParser::_parse_lambda_forms_invokers_only, CHECK);\n+    }\n+  }\n+\n+#if INCLUDE_CDS_JAVA_HEAP\n+  if (CDSConfig::is_dumping_heap()) {\n+    assert(CDSConfig::allow_only_single_java_thread(), \"Required\");\n+    if (!HeapShared::is_archived_boot_layer_available(THREAD)) {\n+      report_loading_error(\"archivedBootLayer not available, disabling full module graph\");\n+      CDSConfig::stop_dumping_full_module_graph();\n+    }\n+    \/\/ Do this before link_shared_classes(), as the following line may load new classes.\n+    HeapShared::init_for_dumping(CHECK);\n+  }\n+#endif\n+\n+  if (CDSConfig::is_dumping_final_static_archive()) {\n+    if (ExtraSharedClassListFile) {\n+      log_info(aot)(\"Loading extra classes from %s ...\", ExtraSharedClassListFile);\n+      ClassListParser::parse_classlist(ExtraSharedClassListFile,\n+                                       ClassListParser::_parse_all, CHECK);\n+    }\n+  }\n+\n+  \/\/ Rewrite and link classes\n+  log_info(aot)(\"Rewriting and linking classes ...\");\n+\n+  \/\/ Link any classes which got missed. This would happen if we have loaded classes that\n+  \/\/ were not explicitly specified in the classlist. E.g., if an interface implemented by class K\n+  \/\/ fails verification, all other interfaces that were not specified in the classlist but\n+  \/\/ are implemented by K are not verified.\n+  link_shared_classes(CHECK);\n+  log_info(aot)(\"Rewriting and linking classes: done\");\n+  TrainingData::init_dumptime_table(CHECK); \/\/ captures TrainingDataSetLocker\n+\n+  if (CDSConfig::is_dumping_regenerated_lambdaform_invokers()) {\n+    LambdaFormInvokers::regenerate_holder_classes(CHECK);\n+  }\n+\n+#if INCLUDE_CDS_JAVA_HEAP\n+  if (CDSConfig::is_dumping_heap()) {\n+    HeapShared::init_heap_writer();\n+    if (CDSConfig::is_dumping_full_module_graph()) {\n+      ClassLoaderDataShared::ensure_module_entry_tables_exist();\n+      HeapShared::reset_archived_object_states(CHECK);\n+    }\n+\n+    AOTReferenceObjSupport::initialize(CHECK);\n+    AOTReferenceObjSupport::stabilize_cached_reference_objects(CHECK);\n+\n+    if (CDSConfig::is_initing_classes_at_dump_time()) {\n+      \/\/ java.lang.Class::reflectionFactory cannot be archived yet. We set this field\n+      \/\/ to null, and it will be initialized again at runtime.\n+      log_debug(aot)(\"Resetting Class::reflectionFactory\");\n+      TempNewSymbol method_name = SymbolTable::new_symbol(\"resetArchivedStates\");\n+      Symbol* method_sig = vmSymbols::void_method_signature();\n+      JavaValue result(T_VOID);\n+      JavaCalls::call_static(&result, vmClasses::Class_klass(),\n+                             method_name, method_sig, CHECK);\n+\n+      \/\/ Perhaps there is a way to avoid hard-coding these names here.\n+      \/\/ See discussion in JDK-8342481.\n+    }\n+\n+    if (HeapShared::is_writing_mapping_mode()) {\n+      \/\/ Do this at the very end, when no Java code will be executed. Otherwise\n+      \/\/ some new strings may be added to the intern table.\n+      StringTable::allocate_shared_strings_array(CHECK);\n+    }\n+  } else {\n+    log_info(aot)(\"Not dumping heap, reset CDSConfig::_is_using_optimized_module_handling\");\n+    CDSConfig::stop_using_optimized_module_handling();\n+  }\n+#endif\n+\n+  if (!CDSConfig::is_dumping_preimage_static_archive()) {\n+    if (CDSConfig::is_dumping_final_static_archive()) {\n+      FileMapInfo::free_current_info(); \/\/ FIXME: should not free current info\n+    }\n+    open_output_mapinfo();\n+  }\n+\n+  VM_PopulateDumpSharedSpace op(builder, _output_mapinfo);\n+  VMThread::execute(&op);\n+\n+  if (AOTCodeCache::is_on_for_dump() && CDSConfig::is_dumping_final_static_archive()) {\n+    CDSConfig::enable_dumping_aot_code();\n+    {\n+      builder.start_ac_region();\n+      \/\/ Write the contents to AOT code region and close AOTCodeCache before packing the region\n+      AOTCodeCache::close();\n+      builder.end_ac_region();\n+    }\n+    CDSConfig::disable_dumping_aot_code();\n+  }\n+\n+  bool status = write_static_archive(&builder, _output_mapinfo, op.mapped_heap_info(), op.streamed_heap_info());\n+  assert(!_output_mapinfo->is_open(), \"Must be closed already\");\n+  _output_mapinfo = nullptr;\n+  if (status && CDSConfig::is_dumping_preimage_static_archive()) {\n+    tty->print_cr(\"%s AOTConfiguration recorded: %s\",\n+                  CDSConfig::has_temp_aot_config_file() ? \"Temporary\" : \"\", AOTConfiguration);\n+    if (CDSConfig::is_single_command_training()) {\n+      fork_and_dump_final_static_archive(CHECK);\n+    }\n+  }\n+\n+  if (!status) {\n+    THROW_MSG(vmSymbols::java_io_IOException(), \"Encountered error while dumping\");\n+  }\n+}\n+\n+bool AOTMetaspace::write_static_archive(ArchiveBuilder* builder,\n+                                        FileMapInfo* map_info,\n+                                        ArchiveMappedHeapInfo* mapped_heap_info,\n+                                        ArchiveStreamedHeapInfo* streamed_heap_info) {\n+  \/\/ relocate the data so that it can be mapped to AOTMetaspace::requested_base_address()\n+  \/\/ without runtime relocation.\n+  builder->relocate_to_requested();\n+  if (!map_info->is_open()) {\n+    return false;\n+  }\n+  map_info->prepare_for_writing();\n+  builder->write_archive(map_info, mapped_heap_info, streamed_heap_info);\n+  return true;\n+}\n+\n+static void print_java_launcher(outputStream* st) {\n+  st->print(\"%s%sbin%sjava\", Arguments::get_java_home(), os::file_separator(), os::file_separator());\n+}\n+\n+static void append_args(GrowableArray<Handle>* args, const char* arg, TRAPS) {\n+  Handle string = java_lang_String::create_from_str(arg, CHECK);\n+  args->append(string);\n+}\n+\n+\/\/ Pass all options in Arguments::jvm_args_array() to a child JVM process\n+\/\/ using the JAVA_TOOL_OPTIONS environment variable.\n+static int exec_jvm_with_java_tool_options(const char* java_launcher_path, TRAPS) {\n+  ResourceMark rm(THREAD);\n+  HandleMark hm(THREAD);\n+  GrowableArray<Handle> args;\n+\n+  const char* cp = Arguments::get_appclasspath();\n+  if (cp != nullptr && strlen(cp) > 0 && strcmp(cp, \".\") != 0) {\n+    \/\/ We cannot use \"-cp\", because \"-cp\" is only interpreted by the java launcher,\n+    \/\/ and is not interpreter by arguments.cpp when it loads args from JAVA_TOOL_OPTIONS\n+    stringStream ss;\n+    ss.print(\"-Djava.class.path=\");\n+    ss.print_raw(cp);\n+    append_args(&args, ss.freeze(), CHECK_0);\n+    \/\/ CDS$ProcessLauncher::execWithJavaToolOptions() must unset CLASSPATH, which has\n+    \/\/ a higher priority than -Djava.class.path=\n+  }\n+\n+  \/\/ Pass all arguments. These include those from JAVA_TOOL_OPTIONS and _JAVA_OPTIONS.\n+  for (int i = 0; i < Arguments::num_jvm_args(); i++) {\n+    const char* arg = Arguments::jvm_args_array()[i];\n+    if (strstr(arg, \"-XX:AOTCacheOutput=\") == arg || \/\/ arg starts with ...\n+        strstr(arg, \"-XX:AOTConfiguration=\") == arg ||\n+        strstr(arg, \"-XX:AOTMode=\") == arg) {\n+      \/\/ Filter these out. They wiill be set below.\n+    } else {\n+      append_args(&args, arg, CHECK_0);\n+    }\n+  }\n+\n+  \/\/ Note: because we are running in AOTMode=record, JDK_AOT_VM_OPTIONS have not been\n+  \/\/ parsed, so they are not in Arguments::jvm_args_array. If JDK_AOT_VM_OPTIONS is in\n+  \/\/ the environment, it will be inherited and parsed by the child JVM process\n+  \/\/ in Arguments::parse_java_tool_options_environment_variable().\n+  precond(strcmp(AOTMode, \"record\") == 0);\n+\n+  \/\/ We don't pass Arguments::jvm_flags_array(), as those will be added by\n+  \/\/ the child process when it loads .hotspotrc\n+\n+  {\n+    \/\/ If AOTCacheOutput contains %p, it should have been already substituted with the\n+    \/\/ pid of the training process.\n+    stringStream ss;\n+    ss.print(\"-XX:AOTCacheOutput=\");\n+    ss.print_raw(AOTCacheOutput);\n+    append_args(&args, ss.freeze(), CHECK_0);\n+  }\n+  {\n+    \/\/ If AOTCacheConfiguration contains %p, it should have been already substituted with the\n+    \/\/ pid of the training process.\n+    \/\/ If AOTCacheConfiguration was not explicitly specified, it should have been assigned a\n+    \/\/ temporary file name.\n+    stringStream ss;\n+    ss.print(\"-XX:AOTConfiguration=\");\n+    ss.print_raw(AOTConfiguration);\n+    append_args(&args, ss.freeze(), CHECK_0);\n+  }\n+\n+  append_args(&args, \"-XX:AOTMode=create\", CHECK_0);\n+\n+  Symbol* klass_name = SymbolTable::new_symbol(\"jdk\/internal\/misc\/CDS$ProcessLauncher\");\n+  Klass* k = SystemDictionary::resolve_or_fail(klass_name, true, CHECK_0);\n+  Symbol* methodName = SymbolTable::new_symbol(\"execWithJavaToolOptions\");\n+  Symbol* methodSignature = SymbolTable::new_symbol(\"(Ljava\/lang\/String;[Ljava\/lang\/String;)I\");\n+\n+  Handle launcher = java_lang_String::create_from_str(java_launcher_path, CHECK_0);\n+  objArrayOop array = oopFactory::new_objArray(vmClasses::String_klass(), args.length(), CHECK_0);\n+  for (int i = 0; i < args.length(); i++) {\n+    array->obj_at_put(i, args.at(i)());\n+  }\n+  objArrayHandle launcher_args(THREAD, array);\n+\n+  \/\/ The following call will pass all options inside the JAVA_TOOL_OPTIONS env variable to\n+  \/\/ the child process. It will also clear the _JAVA_OPTIONS and CLASSPATH env variables for\n+  \/\/ the child process.\n+  \/\/\n+  \/\/ Note: the env variables are set only for the child process. They are not changed\n+  \/\/ for the current process. See java.lang.ProcessBuilder::environment().\n+  JavaValue result(T_OBJECT);\n+  JavaCallArguments javacall_args(2);\n+  javacall_args.push_oop(launcher);\n+  javacall_args.push_oop(launcher_args);\n+  JavaCalls::call_static(&result,\n+                          InstanceKlass::cast(k),\n+                          methodName,\n+                          methodSignature,\n+                          &javacall_args,\n+                          CHECK_0);\n+  return result.get_jint();\n+}\n+\n+void AOTMetaspace::fork_and_dump_final_static_archive(TRAPS) {\n+  assert(CDSConfig::is_dumping_preimage_static_archive(), \"sanity\");\n+\n+  ResourceMark rm;\n+  stringStream ss;\n+  print_java_launcher(&ss);\n+  const char* cmd = ss.freeze();\n+  tty->print_cr(\"Launching child process %s to assemble AOT cache %s using configuration %s\", cmd, AOTCacheOutput, AOTConfiguration);\n+  int status = exec_jvm_with_java_tool_options(cmd, CHECK);\n+  if (status != 0) {\n+    log_error(aot)(\"Child process failed; status = %d\", status);\n+    \/\/ We leave the temp config file for debugging\n+  } else if (CDSConfig::has_temp_aot_config_file()) {\n+    const char* tmp_config = AOTConfiguration;\n+    \/\/ On Windows, need WRITE permission to remove the file.\n+    WINDOWS_ONLY(chmod(tmp_config, _S_IREAD | _S_IWRITE));\n+    status = remove(tmp_config);\n+    if (status != 0) {\n+      log_error(aot)(\"Failed to remove temporary AOT configuration file %s\", tmp_config);\n+    } else {\n+      tty->print_cr(\"Removed temporary AOT configuration file %s\", tmp_config);\n+    }\n+  }\n+}\n+\n+\/\/ Returns true if the class's status has changed.\n+bool AOTMetaspace::try_link_class(JavaThread* current, InstanceKlass* ik) {\n+  ExceptionMark em(current);\n+  JavaThread* THREAD = current; \/\/ For exception macros.\n+  assert(CDSConfig::is_dumping_archive(), \"sanity\");\n+\n+  if (ik->in_aot_cache() && !CDSConfig::is_dumping_final_static_archive()) {\n+    assert(CDSConfig::is_dumping_dynamic_archive(), \"must be\");\n+    return false;\n+  }\n+\n+  if (ik->is_loaded() && !ik->is_linked() && ik->can_be_verified_at_dumptime() &&\n+      !SystemDictionaryShared::has_class_failed_verification(ik)) {\n+    bool saved = BytecodeVerificationLocal;\n+    if (ik->defined_by_other_loaders() && ik->class_loader() == nullptr) {\n+      \/\/ The verification decision is based on BytecodeVerificationRemote\n+      \/\/ for non-system classes. Since we are using the null classloader\n+      \/\/ to load non-system classes for customized class loaders during dumping,\n+      \/\/ we need to temporarily change BytecodeVerificationLocal to be the same as\n+      \/\/ BytecodeVerificationRemote. Note this can cause the parent system\n+      \/\/ classes also being verified. The extra overhead is acceptable during\n+      \/\/ dumping.\n+      BytecodeVerificationLocal = BytecodeVerificationRemote;\n+    }\n+    ik->link_class(THREAD);\n+    if (HAS_PENDING_EXCEPTION) {\n+      ResourceMark rm(THREAD);\n+      oop message = java_lang_Throwable::message(current->pending_exception());\n+      aot_log_warning(aot)(\"Preload Warning: Verification failed for %s because a %s was thrown: %s\",\n+                            ik->external_name(),\n+                            current->pending_exception()->klass()->external_name(),\n+                            message == nullptr ? \"(no message)\" : java_lang_String::as_utf8_string(message));\n+      CLEAR_PENDING_EXCEPTION;\n+      SystemDictionaryShared::set_class_has_failed_verification(ik);\n+    } else {\n+      assert(!SystemDictionaryShared::has_class_failed_verification(ik), \"sanity\");\n+      ik->compute_has_loops_flag_for_methods();\n+    }\n+    BytecodeVerificationLocal = saved;\n+    return true;\n+  } else {\n+    return false;\n+  }\n+}\n+\n+void VM_PopulateDumpSharedSpace::dump_java_heap_objects() {\n+  if (CDSConfig::is_dumping_heap()) {\n+    HeapShared::write_heap(&_mapped_heap_info, &_streamed_heap_info);\n+  } else {\n+    CDSConfig::log_reasons_for_not_dumping_heap();\n+  }\n+}\n+\n+void AOTMetaspace::set_aot_metaspace_range(void* base, void *static_top, void* top) {\n+  assert(base <= static_top && static_top <= top, \"must be\");\n+  _aot_metaspace_static_top = static_top;\n+  MetaspaceObj::set_aot_metaspace_range(base, top);\n+}\n+\n+bool AOTMetaspace::in_aot_cache_dynamic_region(void* p) {\n+  if ((p < MetaspaceObj::aot_metaspace_top()) &&\n+      (p >= _aot_metaspace_static_top)) {\n+    return true;\n+  } else {\n+    return false;\n+  }\n+}\n+\n+bool AOTMetaspace::in_aot_cache_static_region(void* p) {\n+  if (in_aot_cache(p) && !in_aot_cache_dynamic_region(p)) {\n+    return true;\n+  } else {\n+    return false;\n+  }\n+}\n+\n+\/\/ This function is called when the JVM is unable to load the specified archive(s) due to one\n+\/\/ of the following conditions.\n+\/\/ - There's an error that indicates that the archive(s) files were corrupt or otherwise damaged.\n+\/\/ - When -XX:+RequireSharedSpaces is specified, AND the JVM cannot load the archive(s) due\n+\/\/   to version or classpath mismatch.\n+void AOTMetaspace::unrecoverable_loading_error(const char* message) {\n+  report_loading_error(\"%s\", message);\n+\n+  if (CDSConfig::is_dumping_final_static_archive()) {\n+    vm_exit_during_initialization(\"Must be a valid AOT configuration generated by the current JVM\", AOTConfiguration);\n+  } else if (CDSConfig::new_aot_flags_used()) {\n+    vm_exit_during_initialization(\"Unable to use AOT cache.\", nullptr);\n+  } else {\n+    vm_exit_during_initialization(\"Unable to use shared archive. Unrecoverable archive loading error (run with -Xlog:aot,cds for details)\", message);\n+  }\n+}\n+\n+void AOTMetaspace::report_loading_error(const char* format, ...) {\n+  \/\/ When using AOT cache, errors messages are always printed on the error channel.\n+  LogStream ls_aot(LogLevel::Error, LogTagSetMapping<LOG_TAGS(aot)>::tagset());\n+\n+  \/\/ If we are loading load the default CDS archive, it may fail due to incompatible VM options.\n+  \/\/ Print at the info level to avoid excessive verbosity.\n+  \/\/ However, if the user has specified a CDS archive (or AOT cache), they would be interested in\n+  \/\/ knowing that the loading fails, so we print at the error level.\n+  LogLevelType level = (!CDSConfig::is_using_archive() || CDSConfig::is_using_only_default_archive()) ?\n+                        LogLevel::Info : LogLevel::Error;\n+  LogStream ls_cds(level, LogTagSetMapping<LOG_TAGS(cds)>::tagset());\n+\n+  LogStream& ls = CDSConfig::new_aot_flags_used() ? ls_aot : ls_cds;\n+  if (!ls.is_enabled()) {\n+    return;\n+  }\n+\n+  va_list ap;\n+  va_start(ap, format);\n+\n+  static bool printed_error = false;\n+  if (!printed_error) { \/\/ No need for locks. Loading error checks happen only in main thread.\n+    ls.print_cr(\"An error has occurred while processing the %s. Run with -Xlog:%s for details.\",\n+                CDSConfig::type_of_archive_being_loaded(), CDSConfig::new_aot_flags_used() ? \"aot\" : \"aot,cds\");\n+    printed_error = true;\n+  }\n+  ls.vprint_cr(format, ap);\n+\n+  va_end(ap);\n+}\n+\n+\/\/ This function is called when the JVM is unable to write the specified CDS archive due to an\n+\/\/ unrecoverable error.\n+void AOTMetaspace::unrecoverable_writing_error(const char* message) {\n+  writing_error(message);\n+  vm_direct_exit(1);\n+}\n+\n+\/\/ This function is called when the JVM is unable to write the specified CDS archive due to a\n+\/\/ an error. The error will be propagated\n+void AOTMetaspace::writing_error(const char* message) {\n+  aot_log_error(aot)(\"An error has occurred while writing the shared archive file.\");\n+  if (message != nullptr) {\n+    aot_log_error(aot)(\"%s\", message);\n+  }\n+}\n+\n+void AOTMetaspace::initialize_runtime_shared_and_meta_spaces() {\n+  assert(CDSConfig::is_using_archive(), \"Must be called when UseSharedSpaces is enabled\");\n+  MapArchiveResult result = MAP_ARCHIVE_OTHER_FAILURE;\n+\n+  FileMapInfo* static_mapinfo = open_static_archive();\n+  FileMapInfo* dynamic_mapinfo = nullptr;\n+\n+  if (static_mapinfo != nullptr) {\n+    aot_log_info(aot)(\"Core region alignment: %zu\", static_mapinfo->core_region_alignment());\n+    dynamic_mapinfo = open_dynamic_archive();\n+\n+    aot_log_info(aot)(\"ArchiveRelocationMode: %d\", ArchiveRelocationMode);\n+\n+    \/\/ First try to map at the requested address\n+    result = map_archives(static_mapinfo, dynamic_mapinfo, true);\n+    if (result == MAP_ARCHIVE_MMAP_FAILURE) {\n+      \/\/ Mapping has failed (probably due to ASLR). Let's map at an address chosen\n+      \/\/ by the OS.\n+      aot_log_info(aot)(\"Try to map archive(s) at an alternative address\");\n+      result = map_archives(static_mapinfo, dynamic_mapinfo, false);\n+    }\n+  }\n+\n+  if (result == MAP_ARCHIVE_SUCCESS) {\n+    bool dynamic_mapped = (dynamic_mapinfo != nullptr && dynamic_mapinfo->is_mapped());\n+    char* cds_base = static_mapinfo->mapped_base();\n+    char* cds_end =  dynamic_mapped ? dynamic_mapinfo->mapped_end() : static_mapinfo->mapped_end();\n+    \/\/ Register CDS memory region with LSan.\n+    LSAN_REGISTER_ROOT_REGION(cds_base, cds_end - cds_base);\n+    set_aot_metaspace_range(cds_base, static_mapinfo->mapped_end(), cds_end);\n+    _relocation_delta = static_mapinfo->relocation_delta();\n+    _requested_base_address = static_mapinfo->requested_base_address();\n+    if (dynamic_mapped) {\n+      \/\/ turn AutoCreateSharedArchive off if successfully mapped\n+      AutoCreateSharedArchive = false;\n+    }\n+  } else {\n+    set_aot_metaspace_range(nullptr, nullptr, nullptr);\n+    if (CDSConfig::is_dumping_dynamic_archive()) {\n+      aot_log_warning(aot)(\"-XX:ArchiveClassesAtExit is unsupported when base CDS archive is not loaded. Run with -Xlog:cds for more info.\");\n+    }\n+    UseSharedSpaces = false;\n+    \/\/ The base archive cannot be mapped. We cannot dump the dynamic shared archive.\n+    AutoCreateSharedArchive = false;\n+    CDSConfig::disable_dumping_dynamic_archive();\n+    if (PrintSharedArchiveAndExit) {\n+      AOTMetaspace::unrecoverable_loading_error(\"Unable to use shared archive.\");\n+    } else {\n+      if (RequireSharedSpaces) {\n+        AOTMetaspace::unrecoverable_loading_error(\"Unable to map shared spaces\");\n+      } else {\n+        report_loading_error(\"Unable to map shared spaces\");\n+      }\n+    }\n+  }\n+\n+  \/\/ If mapping failed and -XShare:on, the vm should exit\n+  bool has_failed = false;\n+  if (static_mapinfo != nullptr && !static_mapinfo->is_mapped()) {\n+    has_failed = true;\n+    delete static_mapinfo;\n+  }\n+  if (dynamic_mapinfo != nullptr && !dynamic_mapinfo->is_mapped()) {\n+    has_failed = true;\n+    delete dynamic_mapinfo;\n+  }\n+  if (RequireSharedSpaces && has_failed) {\n+      AOTMetaspace::unrecoverable_loading_error(\"Unable to map shared spaces\");\n+  }\n+}\n+\n+FileMapInfo* AOTMetaspace::open_static_archive() {\n+  const char* static_archive = CDSConfig::input_static_archive_path();\n+  assert(static_archive != nullptr, \"sanity\");\n+  FileMapInfo* mapinfo = new FileMapInfo(static_archive, true);\n+  if (!mapinfo->open_as_input()) {\n+    delete(mapinfo);\n+    log_info(cds)(\"Opening of static archive %s failed\", static_archive);\n+    return nullptr;\n+  }\n+  return mapinfo;\n+}\n+\n+FileMapInfo* AOTMetaspace::open_dynamic_archive() {\n+  if (CDSConfig::is_dumping_dynamic_archive()) {\n+    return nullptr;\n+  }\n+  const char* dynamic_archive = CDSConfig::input_dynamic_archive_path();\n+  if (dynamic_archive == nullptr) {\n+    return nullptr;\n+  }\n+\n+  FileMapInfo* mapinfo = new FileMapInfo(dynamic_archive, false);\n+  if (!mapinfo->open_as_input()) {\n+    delete(mapinfo);\n+    if (RequireSharedSpaces) {\n+      AOTMetaspace::unrecoverable_loading_error(\"Failed to initialize dynamic archive\");\n+    }\n+    return nullptr;\n+  }\n+  return mapinfo;\n+}\n+\n+\/\/ use_requested_addr:\n+\/\/  true  = map at FileMapHeader::_requested_base_address\n+\/\/  false = map at an alternative address picked by OS.\n+MapArchiveResult AOTMetaspace::map_archives(FileMapInfo* static_mapinfo, FileMapInfo* dynamic_mapinfo,\n+                                            bool use_requested_addr) {\n+  if (use_requested_addr && static_mapinfo->requested_base_address() == nullptr) {\n+    aot_log_info(aot)(\"Archive(s) were created with -XX:SharedBaseAddress=0. Always map at os-selected address.\");\n+    return MAP_ARCHIVE_MMAP_FAILURE;\n+  }\n+\n+  PRODUCT_ONLY(if (ArchiveRelocationMode == 1 && use_requested_addr) {\n+      \/\/ For product build only -- this is for benchmarking the cost of doing relocation.\n+      \/\/ For debug builds, the check is done below, after reserving the space, for better test coverage\n+      \/\/ (see comment below).\n+      aot_log_info(aot)(\"ArchiveRelocationMode == 1: always map archive(s) at an alternative address\");\n+      return MAP_ARCHIVE_MMAP_FAILURE;\n+    });\n+\n+  if (ArchiveRelocationMode == 2 && !use_requested_addr) {\n+    aot_log_info(aot)(\"ArchiveRelocationMode == 2: never map archive(s) at an alternative address\");\n+    return MAP_ARCHIVE_MMAP_FAILURE;\n+  };\n+\n+  if (dynamic_mapinfo != nullptr) {\n+    \/\/ Ensure that the OS won't be able to allocate new memory spaces between the two\n+    \/\/ archives, or else it would mess up the simple comparison in MetaspaceObj::in_aot_cache().\n+    assert(static_mapinfo->mapping_end_offset() == dynamic_mapinfo->mapping_base_offset(), \"no gap\");\n+  }\n+\n+  ReservedSpace total_space_rs, archive_space_rs, class_space_rs;\n+  MapArchiveResult result = MAP_ARCHIVE_OTHER_FAILURE;\n+  size_t prot_zone_size = 0;\n+  char* mapped_base_address = reserve_address_space_for_archives(static_mapinfo,\n+                                                                 dynamic_mapinfo,\n+                                                                 use_requested_addr,\n+                                                                 total_space_rs,\n+                                                                 archive_space_rs,\n+                                                                 class_space_rs);\n+  if (mapped_base_address == nullptr) {\n+    result = MAP_ARCHIVE_MMAP_FAILURE;\n+    aot_log_debug(aot)(\"Failed to reserve spaces (use_requested_addr=%u)\", (unsigned)use_requested_addr);\n+  } else {\n+\n+    if (Metaspace::using_class_space()) {\n+      prot_zone_size = protection_zone_size();\n+    }\n+\n+#ifdef ASSERT\n+    \/\/ Some sanity checks after reserving address spaces for archives\n+    \/\/  and class space.\n+    assert(archive_space_rs.is_reserved(), \"Sanity\");\n+    if (Metaspace::using_class_space()) {\n+      assert(archive_space_rs.base() == mapped_base_address &&\n+          archive_space_rs.size() > protection_zone_size(),\n+          \"Archive space must lead and include the protection zone\");\n+      \/\/ Class space must closely follow the archive space. Both spaces\n+      \/\/  must be aligned correctly.\n+      assert(class_space_rs.is_reserved() && class_space_rs.size() > 0,\n+             \"A class space should have been reserved\");\n+      assert(class_space_rs.base() >= archive_space_rs.end(),\n+             \"class space should follow the cds archive space\");\n+      assert(is_aligned(archive_space_rs.base(),\n+                        core_region_alignment()),\n+             \"Archive space misaligned\");\n+      assert(is_aligned(class_space_rs.base(),\n+                        Metaspace::reserve_alignment()),\n+             \"class space misaligned\");\n+    }\n+#endif \/\/ ASSERT\n+\n+    aot_log_info(aot)(\"Reserved archive_space_rs [\" INTPTR_FORMAT \" - \" INTPTR_FORMAT \"] (%zu) bytes%s\",\n+                   p2i(archive_space_rs.base()), p2i(archive_space_rs.end()), archive_space_rs.size(),\n+                   (prot_zone_size > 0 ? \" (includes protection zone)\" : \"\"));\n+    aot_log_info(aot)(\"Reserved class_space_rs   [\" INTPTR_FORMAT \" - \" INTPTR_FORMAT \"] (%zu) bytes\",\n+                   p2i(class_space_rs.base()), p2i(class_space_rs.end()), class_space_rs.size());\n+\n+    if (AOTMetaspace::use_windows_memory_mapping()) {\n+      \/\/ We have now reserved address space for the archives, and will map in\n+      \/\/  the archive files into this space.\n+      \/\/\n+      \/\/ Special handling for Windows: on Windows we cannot map a file view\n+      \/\/  into an existing memory mapping. So, we unmap the address range we\n+      \/\/  just reserved again, which will make it available for mapping the\n+      \/\/  archives.\n+      \/\/ Reserving this range has not been for naught however since it makes\n+      \/\/  us reasonably sure the address range is available.\n+      \/\/\n+      \/\/ But still it may fail, since between unmapping the range and mapping\n+      \/\/  in the archive someone else may grab the address space. Therefore\n+      \/\/  there is a fallback in FileMap::map_region() where we just read in\n+      \/\/  the archive files sequentially instead of mapping it in. We couple\n+      \/\/  this with use_requested_addr, since we're going to patch all the\n+      \/\/  pointers anyway so there's no benefit to mmap.\n+      if (use_requested_addr) {\n+        assert(!total_space_rs.is_reserved(), \"Should not be reserved for Windows\");\n+        aot_log_info(aot)(\"Windows mmap workaround: releasing archive space.\");\n+        MemoryReserver::release(archive_space_rs);\n+        \/\/ Mark as not reserved\n+        archive_space_rs = {};\n+        \/\/ The protection zone is part of the archive:\n+        \/\/ See comment above, the Windows way of loading CDS is to mmap the individual\n+        \/\/ parts of the archive into the address region we just vacated. The protection\n+        \/\/ zone will not be mapped (and, in fact, does not exist as physical region in\n+        \/\/ the archive). Therefore, after removing the archive space above, we must\n+        \/\/ re-reserve the protection zone part lest something else gets mapped into that\n+        \/\/ area later.\n+        if (prot_zone_size > 0) {\n+          assert(prot_zone_size >= os::vm_allocation_granularity(), \"must be\"); \/\/ not just page size!\n+          char* p = os::attempt_reserve_memory_at(mapped_base_address, prot_zone_size,\n+                                                  mtClassShared);\n+          assert(p == mapped_base_address || p == nullptr, \"must be\");\n+          if (p == nullptr) {\n+            aot_log_debug(aot)(\"Failed to re-reserve protection zone\");\n+            return MAP_ARCHIVE_MMAP_FAILURE;\n+          }\n+        }\n+      }\n+    }\n+\n+    if (prot_zone_size > 0) {\n+      os::commit_memory(mapped_base_address, prot_zone_size, false); \/\/ will later be protected\n+      \/\/ Before mapping the core regions into the newly established address space, we mark\n+      \/\/ start and the end of the future protection zone with canaries. That way we easily\n+      \/\/ catch mapping errors (accidentally mapping data into the future protection zone).\n+      *(mapped_base_address) = 'P';\n+      *(mapped_base_address + prot_zone_size - 1) = 'P';\n+    }\n+\n+    MapArchiveResult static_result = map_archive(static_mapinfo, mapped_base_address, archive_space_rs);\n+    MapArchiveResult dynamic_result = (static_result == MAP_ARCHIVE_SUCCESS) ?\n+                                     map_archive(dynamic_mapinfo, mapped_base_address, archive_space_rs) : MAP_ARCHIVE_OTHER_FAILURE;\n+\n+    DEBUG_ONLY(if (ArchiveRelocationMode == 1 && use_requested_addr) {\n+      \/\/ This is for simulating mmap failures at the requested address. In\n+      \/\/  debug builds, we do it here (after all archives have possibly been\n+      \/\/  mapped), so we can thoroughly test the code for failure handling\n+      \/\/  (releasing all allocated resource, etc).\n+      aot_log_info(aot)(\"ArchiveRelocationMode == 1: always map archive(s) at an alternative address\");\n+      if (static_result == MAP_ARCHIVE_SUCCESS) {\n+        static_result = MAP_ARCHIVE_MMAP_FAILURE;\n+      }\n+      if (dynamic_result == MAP_ARCHIVE_SUCCESS) {\n+        dynamic_result = MAP_ARCHIVE_MMAP_FAILURE;\n+      }\n+    });\n+\n+    if (static_result == MAP_ARCHIVE_SUCCESS) {\n+      if (dynamic_result == MAP_ARCHIVE_SUCCESS) {\n+        result = MAP_ARCHIVE_SUCCESS;\n+      } else if (dynamic_result == MAP_ARCHIVE_OTHER_FAILURE) {\n+        assert(dynamic_mapinfo != nullptr && !dynamic_mapinfo->is_mapped(), \"must have failed\");\n+        \/\/ No need to retry mapping the dynamic archive again, as it will never succeed\n+        \/\/ (bad file, etc) -- just keep the base archive.\n+        log_warning(cds, dynamic)(\"Unable to use shared archive. The top archive failed to load: %s\",\n+                                  dynamic_mapinfo->full_path());\n+        result = MAP_ARCHIVE_SUCCESS;\n+        \/\/ TODO, we can give the unused space for the dynamic archive to class_space_rs, but there's no\n+        \/\/ easy API to do that right now.\n+      } else {\n+        result = MAP_ARCHIVE_MMAP_FAILURE;\n+      }\n+    } else if (static_result == MAP_ARCHIVE_OTHER_FAILURE) {\n+      result = MAP_ARCHIVE_OTHER_FAILURE;\n+    } else {\n+      result = MAP_ARCHIVE_MMAP_FAILURE;\n+    }\n+  }\n+\n+  if (result == MAP_ARCHIVE_SUCCESS) {\n+    SharedBaseAddress = (size_t)mapped_base_address;\n+#ifdef _LP64\n+    if (Metaspace::using_class_space()) {\n+      assert(prot_zone_size > 0 &&\n+             *(mapped_base_address) == 'P' &&\n+             *(mapped_base_address + prot_zone_size - 1) == 'P',\n+             \"Protection zone was overwritten?\");\n+      \/\/ Set up ccs in metaspace.\n+      Metaspace::initialize_class_space(class_space_rs);\n+\n+      \/\/ Set up compressed Klass pointer encoding: the encoding range must\n+      \/\/  cover both archive and class space.\n+      const address klass_range_start = (address)mapped_base_address;\n+      const size_t klass_range_size = (address)class_space_rs.end() - klass_range_start;\n+      if (INCLUDE_CDS_JAVA_HEAP || UseCompactObjectHeaders) {\n+        \/\/ The CDS archive may contain narrow Klass IDs that were precomputed at archive generation time:\n+        \/\/ - every archived java object header (only if INCLUDE_CDS_JAVA_HEAP)\n+        \/\/ - every archived Klass' prototype   (only if +UseCompactObjectHeaders)\n+        \/\/\n+        \/\/ In order for those IDs to still be valid, we need to dictate base and shift: base should be the\n+        \/\/ mapping start (including protection zone), shift should be the shift used at archive generation time.\n+        CompressedKlassPointers::initialize_for_given_encoding(\n+          klass_range_start, klass_range_size,\n+          klass_range_start, ArchiveBuilder::precomputed_narrow_klass_shift() \/\/ precomputed encoding, see ArchiveBuilder\n+        );\n+        assert(CompressedKlassPointers::base() == klass_range_start, \"must be\");\n+      } else {\n+        \/\/ Let JVM freely choose encoding base and shift\n+        CompressedKlassPointers::initialize(klass_range_start, klass_range_size);\n+        assert(CompressedKlassPointers::base() == nullptr ||\n+               CompressedKlassPointers::base() == klass_range_start, \"must be\");\n+      }\n+      \/\/ Establish protection zone, but only if we need one\n+      if (CompressedKlassPointers::base() == klass_range_start) {\n+        CompressedKlassPointers::establish_protection_zone(klass_range_start, prot_zone_size);\n+      }\n+\n+      if (static_mapinfo->can_use_heap_region()) {\n+        if (static_mapinfo->object_streaming_mode()) {\n+          HeapShared::initialize_loading_mode(HeapArchiveMode::_streaming);\n+        } else {\n+          \/\/ map_or_load_heap_region() compares the current narrow oop and klass encodings\n+          \/\/ with the archived ones, so it must be done after all encodings are determined.\n+          static_mapinfo->map_or_load_heap_region();\n+          HeapShared::initialize_loading_mode(HeapArchiveMode::_mapping);\n+        }\n+      } else {\n+        FileMapRegion* r = static_mapinfo->region_at(AOTMetaspace::hp);\n+        if (r->used() > 0) {\n+          if (static_mapinfo->object_streaming_mode()) {\n+            AOTMetaspace::report_loading_error(\"Cannot use CDS heap data.\");\n+          } else {\n+            if (!UseCompressedOops && !AOTMappedHeapLoader::can_map()) {\n+              AOTMetaspace::report_loading_error(\"Cannot use CDS heap data. Selected GC not compatible -XX:-UseCompressedOops\");\n+            } else {\n+              AOTMetaspace::report_loading_error(\"Cannot use CDS heap data. UseEpsilonGC, UseG1GC, UseSerialGC, UseParallelGC, or UseShenandoahGC are required.\");\n+            }\n+          }\n+        }\n+      }\n+    }\n+#endif \/\/ _LP64\n+    log_info(aot)(\"initial optimized module handling: %s\", CDSConfig::is_using_optimized_module_handling() ? \"enabled\" : \"disabled\");\n+    log_info(aot)(\"initial full module graph: %s\", CDSConfig::is_using_full_module_graph() ? \"enabled\" : \"disabled\");\n+  } else {\n+    unmap_archive(static_mapinfo);\n+    unmap_archive(dynamic_mapinfo);\n+    release_reserved_spaces(total_space_rs, archive_space_rs, class_space_rs);\n+  }\n+\n+  return result;\n+}\n+\n+\n+\/\/ This will reserve two address spaces suitable to house Klass structures, one\n+\/\/  for the cds archives (static archive and optionally dynamic archive) and\n+\/\/  optionally one move for ccs.\n+\/\/\n+\/\/ Since both spaces must fall within the compressed class pointer encoding\n+\/\/  range, they are allocated close to each other.\n+\/\/\n+\/\/ Space for archives will be reserved first, followed by a potential gap,\n+\/\/  followed by the space for ccs:\n+\/\/\n+\/\/ +-- Base address             A        B                     End\n+\/\/ |                            |        |                      |\n+\/\/ v                            v        v                      v\n+\/\/ +-------------+--------------+        +----------------------+\n+\/\/ | static arc  | [dyn. arch]  | [gap]  | compr. class space   |\n+\/\/ +-------------+--------------+        +----------------------+\n+\/\/\n+\/\/ (The gap may result from different alignment requirements between metaspace\n+\/\/  and CDS)\n+\/\/\n+\/\/ If UseCompressedClassPointers is disabled, only one address space will be\n+\/\/  reserved:\n+\/\/\n+\/\/ +-- Base address             End\n+\/\/ |                            |\n+\/\/ v                            v\n+\/\/ +-------------+--------------+\n+\/\/ | static arc  | [dyn. arch]  |\n+\/\/ +-------------+--------------+\n+\/\/\n+\/\/ Base address: If use_archive_base_addr address is true, the Base address is\n+\/\/  determined by the address stored in the static archive. If\n+\/\/  use_archive_base_addr address is false, this base address is determined\n+\/\/  by the platform.\n+\/\/\n+\/\/ If UseCompressedClassPointers=1, the range encompassing both spaces will be\n+\/\/  suitable to en\/decode narrow Klass pointers: the base will be valid for\n+\/\/  encoding, the range [Base, End) and not surpass the max. range for that encoding.\n+\/\/\n+\/\/ Return:\n+\/\/\n+\/\/ - On success:\n+\/\/    - total_space_rs will be reserved as whole for archive_space_rs and\n+\/\/      class_space_rs if UseCompressedClassPointers is true.\n+\/\/      On Windows, try reserve archive_space_rs and class_space_rs\n+\/\/      separately first if use_archive_base_addr is true.\n+\/\/    - archive_space_rs will be reserved and large enough to host static and\n+\/\/      if needed dynamic archive: [Base, A).\n+\/\/      archive_space_rs.base and size will be aligned to CDS reserve\n+\/\/      granularity.\n+\/\/    - class_space_rs: If UseCompressedClassPointers=1, class_space_rs will\n+\/\/      be reserved. Its start address will be aligned to metaspace reserve\n+\/\/      alignment, which may differ from CDS alignment. It will follow the cds\n+\/\/      archive space, close enough such that narrow class pointer encoding\n+\/\/      covers both spaces.\n+\/\/      If UseCompressedClassPointers=0, class_space_rs remains unreserved.\n+\/\/ - On error: null is returned and the spaces remain unreserved.\n+char* AOTMetaspace::reserve_address_space_for_archives(FileMapInfo* static_mapinfo,\n+                                                       FileMapInfo* dynamic_mapinfo,\n+                                                       bool use_archive_base_addr,\n+                                                       ReservedSpace& total_space_rs,\n+                                                       ReservedSpace& archive_space_rs,\n+                                                       ReservedSpace& class_space_rs) {\n+\n+  address const base_address = (address) (use_archive_base_addr ? static_mapinfo->requested_base_address() : nullptr);\n+  const size_t archive_space_alignment = core_region_alignment();\n+\n+  \/\/ Size and requested location of the archive_space_rs (for both static and dynamic archives)\n+  size_t archive_end_offset  = (dynamic_mapinfo == nullptr) ? static_mapinfo->mapping_end_offset() : dynamic_mapinfo->mapping_end_offset();\n+  size_t archive_space_size = align_up(archive_end_offset, archive_space_alignment);\n+\n+  if (!Metaspace::using_class_space()) {\n+    \/\/ Get the simple case out of the way first:\n+    \/\/ no compressed class space, simple allocation.\n+\n+    \/\/ When running without class space, requested archive base should be aligned to cds core alignment.\n+    assert(is_aligned(base_address, archive_space_alignment),\n+             \"Archive base address unaligned: \" PTR_FORMAT \", needs alignment: %zu.\",\n+             p2i(base_address), archive_space_alignment);\n+\n+    archive_space_rs = MemoryReserver::reserve((char*)base_address,\n+                                               archive_space_size,\n+                                               archive_space_alignment,\n+                                               os::vm_page_size(),\n+                                               mtNone);\n+    if (archive_space_rs.is_reserved()) {\n+      assert(base_address == nullptr ||\n+             (address)archive_space_rs.base() == base_address, \"Sanity\");\n+      \/\/ Register archive space with NMT.\n+      MemTracker::record_virtual_memory_tag(archive_space_rs, mtClassShared);\n+      return archive_space_rs.base();\n+    }\n+    return nullptr;\n+  }\n+\n+#ifdef _LP64\n+\n+  \/\/ Complex case: two spaces adjacent to each other, both to be addressable\n+  \/\/  with narrow class pointers.\n+  \/\/ We reserve the whole range spanning both spaces, then split that range up.\n+\n+  const size_t class_space_alignment = Metaspace::reserve_alignment();\n+\n+  \/\/ When running with class space, requested archive base must satisfy both cds core alignment\n+  \/\/ and class space alignment.\n+  const size_t base_address_alignment = MAX2(class_space_alignment, archive_space_alignment);\n+  assert(is_aligned(base_address, base_address_alignment),\n+           \"Archive base address unaligned: \" PTR_FORMAT \", needs alignment: %zu.\",\n+           p2i(base_address), base_address_alignment);\n+\n+  size_t class_space_size = CompressedClassSpaceSize;\n+  assert(CompressedClassSpaceSize > 0 &&\n+         is_aligned(CompressedClassSpaceSize, class_space_alignment),\n+         \"CompressedClassSpaceSize malformed: %zu\", CompressedClassSpaceSize);\n+\n+  const size_t ccs_begin_offset = align_up(archive_space_size, class_space_alignment);\n+  const size_t gap_size = ccs_begin_offset - archive_space_size;\n+\n+  \/\/ Reduce class space size if it would not fit into the maximum possible Klass encoding range. That\n+  \/\/ range is defined by the narrowKlass size.\n+  const size_t max_encoding_range_size = CompressedKlassPointers::max_klass_range_size();\n+  guarantee(archive_space_size < max_encoding_range_size - class_space_alignment, \"Archive too large\");\n+  if ((archive_space_size + gap_size + class_space_size) > max_encoding_range_size) {\n+    class_space_size = align_down(max_encoding_range_size - archive_space_size - gap_size, class_space_alignment);\n+    log_info(metaspace)(\"CDS initialization: reducing class space size from %zu to %zu\",\n+        CompressedClassSpaceSize, class_space_size);\n+    FLAG_SET_ERGO(CompressedClassSpaceSize, class_space_size);\n+  }\n+\n+  const size_t total_range_size =\n+      archive_space_size + gap_size + class_space_size;\n+\n+  \/\/ Test that class space base address plus shift can be decoded by aarch64, when restored.\n+  const int precomputed_narrow_klass_shift = ArchiveBuilder::precomputed_narrow_klass_shift();\n+  if (!CompressedKlassPointers::check_klass_decode_mode(base_address, precomputed_narrow_klass_shift,\n+                                                        total_range_size)) {\n+    aot_log_info(aot)(\"CDS initialization: Cannot use SharedBaseAddress \" PTR_FORMAT \" with precomputed shift %d.\",\n+                  p2i(base_address), precomputed_narrow_klass_shift);\n+    use_archive_base_addr = false;\n+  }\n+\n+  assert(total_range_size > ccs_begin_offset, \"must be\");\n+  if (use_windows_memory_mapping() && use_archive_base_addr) {\n+    if (base_address != nullptr) {\n+      \/\/ On Windows, we cannot safely split a reserved memory space into two (see JDK-8255917).\n+      \/\/ Hence, we optimistically reserve archive space and class space side-by-side. We only\n+      \/\/ do this for use_archive_base_addr=true since for use_archive_base_addr=false case\n+      \/\/ caller will not split the combined space for mapping, instead read the archive data\n+      \/\/ via sequential file IO.\n+      address ccs_base = base_address + archive_space_size + gap_size;\n+      archive_space_rs = MemoryReserver::reserve((char*)base_address,\n+                                                 archive_space_size,\n+                                                 archive_space_alignment,\n+                                                 os::vm_page_size(),\n+                                                 mtNone);\n+      class_space_rs   = MemoryReserver::reserve((char*)ccs_base,\n+                                                 class_space_size,\n+                                                 class_space_alignment,\n+                                                 os::vm_page_size(),\n+                                                 mtNone);\n+    }\n+    if (!archive_space_rs.is_reserved() || !class_space_rs.is_reserved()) {\n+      release_reserved_spaces(total_space_rs, archive_space_rs, class_space_rs);\n+      return nullptr;\n+    }\n+    MemTracker::record_virtual_memory_tag(archive_space_rs, mtClassShared);\n+    MemTracker::record_virtual_memory_tag(class_space_rs, mtClass);\n+  } else {\n+    if (use_archive_base_addr && base_address != nullptr) {\n+      total_space_rs = MemoryReserver::reserve((char*) base_address,\n+                                               total_range_size,\n+                                               base_address_alignment,\n+                                               os::vm_page_size(),\n+                                               mtNone);\n+    } else {\n+      \/\/ We did not manage to reserve at the preferred address, or were instructed to relocate. In that\n+      \/\/ case we reserve wherever possible, but the start address needs to be encodable as narrow Klass\n+      \/\/ encoding base since the archived heap objects contain narrow Klass IDs pre-calculated toward the start\n+      \/\/ of the shared Metaspace. That prevents us from using zero-based encoding and therefore we won't\n+      \/\/ try allocating in low-address regions.\n+      total_space_rs = Metaspace::reserve_address_space_for_compressed_classes(total_range_size, false \/* optimize_for_zero_base *\/);\n+    }\n+\n+    if (!total_space_rs.is_reserved()) {\n+      return nullptr;\n+    }\n+\n+    \/\/ Paranoid checks:\n+    assert(!use_archive_base_addr || (address)total_space_rs.base() == base_address,\n+           \"Sanity (\" PTR_FORMAT \" vs \" PTR_FORMAT \")\", p2i(base_address), p2i(total_space_rs.base()));\n+    assert(is_aligned(total_space_rs.base(), base_address_alignment), \"Sanity\");\n+    assert(total_space_rs.size() == total_range_size, \"Sanity\");\n+\n+    \/\/ Now split up the space into ccs and cds archive. For simplicity, just leave\n+    \/\/  the gap reserved at the end of the archive space. Do not do real splitting.\n+    archive_space_rs = total_space_rs.first_part(ccs_begin_offset,\n+                                                 (size_t)archive_space_alignment);\n+    class_space_rs = total_space_rs.last_part(ccs_begin_offset);\n+    MemTracker::record_virtual_memory_split_reserved(total_space_rs.base(), total_space_rs.size(),\n+                                                     ccs_begin_offset, mtClassShared, mtClass);\n+  }\n+  assert(is_aligned(archive_space_rs.base(), archive_space_alignment), \"Sanity\");\n+  assert(is_aligned(archive_space_rs.size(), archive_space_alignment), \"Sanity\");\n+  assert(is_aligned(class_space_rs.base(), class_space_alignment), \"Sanity\");\n+  assert(is_aligned(class_space_rs.size(), class_space_alignment), \"Sanity\");\n+\n+\n+  return archive_space_rs.base();\n+\n+#else\n+  ShouldNotReachHere();\n+  return nullptr;\n+#endif\n+\n+}\n+\n+void AOTMetaspace::release_reserved_spaces(ReservedSpace& total_space_rs,\n+                                           ReservedSpace& archive_space_rs,\n+                                           ReservedSpace& class_space_rs) {\n+  if (total_space_rs.is_reserved()) {\n+    aot_log_debug(aot)(\"Released shared space (archive + class) \" INTPTR_FORMAT, p2i(total_space_rs.base()));\n+    MemoryReserver::release(total_space_rs);\n+    total_space_rs = {};\n+  } else {\n+    if (archive_space_rs.is_reserved()) {\n+      aot_log_debug(aot)(\"Released shared space (archive) \" INTPTR_FORMAT, p2i(archive_space_rs.base()));\n+      MemoryReserver::release(archive_space_rs);\n+      archive_space_rs = {};\n+    }\n+    if (class_space_rs.is_reserved()) {\n+      aot_log_debug(aot)(\"Released shared space (classes) \" INTPTR_FORMAT, p2i(class_space_rs.base()));\n+      MemoryReserver::release(class_space_rs);\n+      class_space_rs = {};\n+    }\n+  }\n+}\n+\n+static int archive_regions[]     = { AOTMetaspace::rw, AOTMetaspace::ro };\n+static int archive_regions_count = 2;\n+\n+MapArchiveResult AOTMetaspace::map_archive(FileMapInfo* mapinfo, char* mapped_base_address, ReservedSpace rs) {\n+  assert(CDSConfig::is_using_archive(), \"must be runtime\");\n+  if (mapinfo == nullptr) {\n+    return MAP_ARCHIVE_SUCCESS; \/\/ The dynamic archive has not been specified. No error has happened -- trivially succeeded.\n+  }\n+\n+  mapinfo->set_is_mapped(false);\n+  if (mapinfo->core_region_alignment() != (size_t)core_region_alignment()) {\n+    report_loading_error(\"Unable to map CDS archive -- core_region_alignment() expected: %zu\"\n+                         \" actual: %zu\", mapinfo->core_region_alignment(), core_region_alignment());\n+    return MAP_ARCHIVE_OTHER_FAILURE;\n+  }\n+\n+  MapArchiveResult result =\n+    mapinfo->map_regions(archive_regions, archive_regions_count, mapped_base_address, rs);\n+\n+  if (result != MAP_ARCHIVE_SUCCESS) {\n+    unmap_archive(mapinfo);\n+    return result;\n+  }\n+\n+  if (!mapinfo->validate_class_location()) {\n+    unmap_archive(mapinfo);\n+    return MAP_ARCHIVE_OTHER_FAILURE;\n+  }\n+\n+  if (mapinfo->is_static()) {\n+    \/\/ Currently, only static archive uses early serialized data.\n+    char* buffer = mapinfo->early_serialized_data();\n+    intptr_t* array = (intptr_t*)buffer;\n+    ReadClosure rc(&array, (intptr_t)mapped_base_address);\n+    early_serialize(&rc);\n+  }\n+\n+  if (!mapinfo->validate_aot_class_linking()) {\n+    unmap_archive(mapinfo);\n+    return MAP_ARCHIVE_OTHER_FAILURE;\n+  }\n+\n+  mapinfo->set_is_mapped(true);\n+  return MAP_ARCHIVE_SUCCESS;\n+}\n+\n+void AOTMetaspace::unmap_archive(FileMapInfo* mapinfo) {\n+  assert(CDSConfig::is_using_archive(), \"must be runtime\");\n+  if (mapinfo != nullptr) {\n+    mapinfo->unmap_regions(archive_regions, archive_regions_count);\n+    mapinfo->unmap_region(AOTMetaspace::bm);\n+    mapinfo->set_is_mapped(false);\n+  }\n+}\n+\n+\/\/ For -XX:PrintSharedArchiveAndExit\n+class CountSharedSymbols : public SymbolClosure {\n+ private:\n+   size_t _count;\n+ public:\n+   CountSharedSymbols() : _count(0) {}\n+  void do_symbol(Symbol** sym) {\n+    _count++;\n+  }\n+  size_t total() { return _count; }\n+\n+};\n+\n+\/\/ Read the miscellaneous data from the shared file, and\n+\/\/ serialize it out to its various destinations.\n+\n+void AOTMetaspace::initialize_shared_spaces() {\n+  FileMapInfo *static_mapinfo = FileMapInfo::current_info();\n+  FileMapInfo *dynamic_mapinfo = FileMapInfo::dynamic_info();\n+\n+  \/\/ Verify various attributes of the archive, plus initialize the\n+  \/\/ shared string\/symbol tables.\n+  char* buffer = static_mapinfo->serialized_data();\n+  intptr_t* array = (intptr_t*)buffer;\n+  ReadClosure rc(&array, (intptr_t)SharedBaseAddress);\n+  serialize(&rc);\n+\n+  \/\/ Finish initializing the heap dump mode used in the archive\n+  \/\/ Heap initialization can be done only after vtables are initialized by ReadClosure.\n+  HeapShared::finalize_initialization(static_mapinfo);\n+  Universe::load_archived_object_instances();\n+\n+  AOTCodeCache::initialize();\n+\n+  if (dynamic_mapinfo != nullptr) {\n+    intptr_t* buffer = (intptr_t*)dynamic_mapinfo->serialized_data();\n+    ReadClosure rc(&buffer, (intptr_t)SharedBaseAddress);\n+    DynamicArchive::serialize(&rc);\n+    DynamicArchive::setup_array_klasses();\n+  }\n+\n+  LogStreamHandle(Info, aot) lsh;\n+  if (lsh.is_enabled()) {\n+    lsh.print(\"Using AOT-linked classes: %s (static archive: %s aot-linked classes\",\n+              BOOL_TO_STR(CDSConfig::is_using_aot_linked_classes()),\n+              static_mapinfo->header()->has_aot_linked_classes() ? \"has\" : \"no\");\n+    if (dynamic_mapinfo != nullptr) {\n+      lsh.print(\", dynamic archive: %s aot-linked classes\",\n+                dynamic_mapinfo->header()->has_aot_linked_classes() ? \"has\" : \"no\");\n+    }\n+    lsh.print_cr(\")\");\n+  }\n+\n+  \/\/ Set up LambdaFormInvokers::_lambdaform_lines for dynamic dump\n+  if (CDSConfig::is_dumping_dynamic_archive()) {\n+    \/\/ Read stored LF format lines stored in static archive\n+    LambdaFormInvokers::read_static_archive_invokers();\n+  }\n+\n+  if (PrintSharedArchiveAndExit) {\n+    \/\/ Print archive names\n+    if (dynamic_mapinfo != nullptr) {\n+      tty->print_cr(\"\\n\\nBase archive name: %s\", CDSConfig::input_static_archive_path());\n+      tty->print_cr(\"Base archive version %d\", static_mapinfo->version());\n+    } else {\n+      tty->print_cr(\"Static archive name: %s\", static_mapinfo->full_path());\n+      tty->print_cr(\"Static archive version %d\", static_mapinfo->version());\n+    }\n+\n+    SystemDictionaryShared::print_shared_archive(tty);\n+    if (dynamic_mapinfo != nullptr) {\n+      tty->print_cr(\"\\n\\nDynamic archive name: %s\", dynamic_mapinfo->full_path());\n+      tty->print_cr(\"Dynamic archive version %d\", dynamic_mapinfo->version());\n+      SystemDictionaryShared::print_shared_archive(tty, false\/*dynamic*\/);\n+    }\n+\n+    TrainingData::print_archived_training_data_on(tty);\n+\n+    AOTCodeCache::print_on(tty);\n+\n+    \/\/ collect shared symbols and strings\n+    CountSharedSymbols cl;\n+    SymbolTable::shared_symbols_do(&cl);\n+    tty->print_cr(\"Number of shared symbols: %zu\", cl.total());\n+    if (HeapShared::is_loading_mapping_mode()) {\n+      tty->print_cr(\"Number of shared strings: %zu\", StringTable::shared_entry_count());\n+    }\n+    tty->print_cr(\"VM version: %s\\r\\n\", static_mapinfo->vm_version());\n+    if (FileMapInfo::current_info() == nullptr || _archive_loading_failed) {\n+      tty->print_cr(\"archive is invalid\");\n+      vm_exit(1);\n+    } else {\n+      tty->print_cr(\"archive is valid\");\n+      vm_exit(0);\n+    }\n+  }\n+}\n+\n+\/\/ JVM\/TI RedefineClasses() support:\n+bool AOTMetaspace::remap_shared_readonly_as_readwrite() {\n+  assert(SafepointSynchronize::is_at_safepoint(), \"must be at safepoint\");\n+\n+  if (CDSConfig::is_using_archive()) {\n+    \/\/ remap the shared readonly space to shared readwrite, private\n+    FileMapInfo* mapinfo = FileMapInfo::current_info();\n+    if (!mapinfo->remap_shared_readonly_as_readwrite()) {\n+      return false;\n+    }\n+    if (FileMapInfo::dynamic_info() != nullptr) {\n+      mapinfo = FileMapInfo::dynamic_info();\n+      if (!mapinfo->remap_shared_readonly_as_readwrite()) {\n+        return false;\n+      }\n+    }\n+    _remapped_readwrite = true;\n+  }\n+  return true;\n+}\n+\n+void AOTMetaspace::print_on(outputStream* st) {\n+  if (CDSConfig::is_using_archive()) {\n+    st->print(\"CDS archive(s) mapped at: \");\n+    address base = (address)MetaspaceObj::aot_metaspace_base();\n+    address static_top = (address)_aot_metaspace_static_top;\n+    address top = (address)MetaspaceObj::aot_metaspace_top();\n+    st->print(\"[\" PTR_FORMAT \"-\" PTR_FORMAT \"-\" PTR_FORMAT \"), \", p2i(base), p2i(static_top), p2i(top));\n+    st->print(\"size %zu, \", top - base);\n+    st->print(\"SharedBaseAddress: \" PTR_FORMAT \", ArchiveRelocationMode: %d.\", SharedBaseAddress, ArchiveRelocationMode);\n+  } else {\n+    st->print(\"CDS archive(s) not mapped\");\n+  }\n+  st->cr();\n+}\n","filename":"src\/hotspot\/share\/cds\/aotMetaspace.cpp","additions":2256,"deletions":0,"binary":false,"changes":2256,"status":"added"},{"patch":"@@ -0,0 +1,1214 @@\n+\/*\n+ * Copyright (c) 2018, 2025, Oracle and\/or its affiliates. All rights reserved.\n+ * DO NOT ALTER OR REMOVE COPYRIGHT NOTICES OR THIS FILE HEADER.\n+ *\n+ * This code is free software; you can redistribute it and\/or modify it\n+ * under the terms of the GNU General Public License version 2 only, as\n+ * published by the Free Software Foundation.\n+ *\n+ * This code is distributed in the hope that it will be useful, but WITHOUT\n+ * ANY WARRANTY; without even the implied warranty of MERCHANTABILITY or\n+ * FITNESS FOR A PARTICULAR PURPOSE.  See the GNU General Public License\n+ * version 2 for more details (a copy is included in the LICENSE file that\n+ * accompanied this code).\n+ *\n+ * You should have received a copy of the GNU General Public License version\n+ * 2 along with this work; if not, write to the Free Software Foundation,\n+ * Inc., 51 Franklin St, Fifth Floor, Boston, MA 02110-1301 USA.\n+ *\n+ * Please contact Oracle, 500 Oracle Parkway, Redwood Shores, CA 94065 USA\n+ * or visit www.oracle.com if you need additional information or have any\n+ * questions.\n+ *\n+ *\/\n+\n+#include \"cds\/aotMetaspace.hpp\"\n+#include \"cds\/aotStreamedHeapLoader.hpp\"\n+#include \"cds\/aotThread.hpp\"\n+#include \"cds\/cdsConfig.hpp\"\n+#include \"cds\/filemap.hpp\"\n+#include \"cds\/heapShared.inline.hpp\"\n+#include \"classfile\/classLoaderDataShared.hpp\"\n+#include \"classfile\/javaClasses.inline.hpp\"\n+#include \"classfile\/stringTable.hpp\"\n+#include \"classfile\/vmClasses.hpp\"\n+#include \"gc\/shared\/collectedHeap.inline.hpp\"\n+#include \"gc\/shared\/oopStorage.inline.hpp\"\n+#include \"gc\/shared\/oopStorageSet.inline.hpp\"\n+#include \"logging\/log.hpp\"\n+#include \"memory\/iterator.inline.hpp\"\n+#include \"memory\/oopFactory.hpp\"\n+#include \"oops\/access.inline.hpp\"\n+#include \"oops\/objArrayOop.inline.hpp\"\n+#include \"oops\/oop.inline.hpp\"\n+#include \"runtime\/globals.hpp\"\n+#include \"runtime\/globals_extension.hpp\"\n+#include \"runtime\/handles.inline.hpp\"\n+#include \"runtime\/java.hpp\"\n+#include \"runtime\/mutex.hpp\"\n+#include \"runtime\/thread.hpp\"\n+#include \"utilities\/bitMap.inline.hpp\"\n+#include \"utilities\/exceptions.hpp\"\n+#include \"utilities\/globalDefinitions.hpp\"\n+#include \"utilities\/stack.inline.hpp\"\n+#include \"utilities\/ticks.hpp\"\n+\n+#include <type_traits>\n+\n+#if INCLUDE_CDS_JAVA_HEAP\n+\n+FileMapRegion* AOTStreamedHeapLoader::_heap_region;\n+FileMapRegion* AOTStreamedHeapLoader::_bitmap_region;\n+int* AOTStreamedHeapLoader::_roots_archive;\n+OopHandle AOTStreamedHeapLoader::_roots;\n+BitMapView AOTStreamedHeapLoader::_oopmap;\n+bool AOTStreamedHeapLoader::_is_in_use;\n+int AOTStreamedHeapLoader::_previous_batch_last_object_index;\n+int AOTStreamedHeapLoader::_current_batch_last_object_index;\n+int AOTStreamedHeapLoader::_current_root_index;\n+size_t AOTStreamedHeapLoader::_allocated_words;\n+bool AOTStreamedHeapLoader::_allow_gc;\n+bool AOTStreamedHeapLoader::_objects_are_handles;\n+size_t AOTStreamedHeapLoader::_num_archived_objects;\n+int AOTStreamedHeapLoader::_num_roots;\n+size_t AOTStreamedHeapLoader::_heap_region_used;\n+bool AOTStreamedHeapLoader::_loading_all_objects;\n+\n+size_t* AOTStreamedHeapLoader::_object_index_to_buffer_offset_table;\n+void** AOTStreamedHeapLoader::_object_index_to_heap_object_table;\n+int* AOTStreamedHeapLoader::_root_highest_object_index_table;\n+\n+bool AOTStreamedHeapLoader::_waiting_for_iterator;\n+bool AOTStreamedHeapLoader::_swapping_root_format;\n+\n+static uint64_t _early_materialization_time_ns = 0;\n+static uint64_t _late_materialization_time_ns = 0;\n+static uint64_t _final_materialization_time_ns = 0;\n+static uint64_t _cleanup_materialization_time_ns = 0;\n+static volatile uint64_t _accumulated_lazy_materialization_time_ns = 0;\n+static Ticks _materialization_start_ticks;\n+\n+int AOTStreamedHeapLoader::object_index_for_root_index(int root_index) {\n+  return _roots_archive[root_index];\n+}\n+\n+int AOTStreamedHeapLoader::highest_object_index_for_root_index(int root_index) {\n+  return _root_highest_object_index_table[root_index];\n+}\n+\n+size_t AOTStreamedHeapLoader::buffer_offset_for_object_index(int object_index) {\n+  return _object_index_to_buffer_offset_table[object_index];\n+}\n+\n+oopDesc* AOTStreamedHeapLoader::archive_object_for_object_index(int object_index) {\n+  size_t buffer_offset = buffer_offset_for_object_index(object_index);\n+  address bottom = (address)_heap_region->mapped_base();\n+  return (oopDesc*)(bottom + buffer_offset);\n+}\n+\n+size_t AOTStreamedHeapLoader::buffer_offset_for_archive_object(oopDesc* archive_object) {\n+  address bottom = (address)_heap_region->mapped_base();\n+  return size_t(archive_object) - size_t(bottom);\n+}\n+\n+template <bool use_coops>\n+BitMap::idx_t AOTStreamedHeapLoader::obj_bit_idx_for_buffer_offset(size_t buffer_offset) {\n+  if constexpr (use_coops) {\n+    return BitMap::idx_t(buffer_offset \/ sizeof(narrowOop));\n+  } else {\n+    return BitMap::idx_t(buffer_offset \/ sizeof(HeapWord));\n+  }\n+}\n+\n+oop AOTStreamedHeapLoader::heap_object_for_object_index(int object_index) {\n+  assert(object_index >= 0 && object_index <= (int)_num_archived_objects,\n+         \"Heap object reference out of index: %d\", object_index);\n+\n+  if (_objects_are_handles) {\n+    oop* handle = (oop*)_object_index_to_heap_object_table[object_index];\n+    if (handle == nullptr) {\n+      return nullptr;\n+    }\n+    return NativeAccess<>::oop_load(handle);\n+  } else {\n+    return cast_to_oop(_object_index_to_heap_object_table[object_index]);\n+  }\n+}\n+\n+void AOTStreamedHeapLoader::set_heap_object_for_object_index(int object_index, oop heap_object) {\n+  assert(heap_object_for_object_index(object_index) == nullptr, \"Should only set once with this API\");\n+  if (_objects_are_handles) {\n+    oop* handle = Universe::vm_global()->allocate();\n+    NativeAccess<>::oop_store(handle, heap_object);\n+    _object_index_to_heap_object_table[object_index] = (void*)handle;\n+  } else {\n+    _object_index_to_heap_object_table[object_index] = cast_from_oop<void*>(heap_object);\n+  }\n+}\n+\n+int AOTStreamedHeapLoader::archived_string_value_object_index(oopDesc* archive_object) {\n+    assert(archive_object->klass() == vmClasses::String_klass(), \"Must be an archived string\");\n+    address archive_string_value_addr = (address)archive_object + java_lang_String::value_offset();\n+    return UseCompressedOops ? *(int*)archive_string_value_addr : (int)*(int64_t*)archive_string_value_addr;\n+}\n+\n+static int archive_array_length(oopDesc* archive_array) {\n+  return *(int*)(address(archive_array) + arrayOopDesc::length_offset_in_bytes());\n+}\n+\n+static size_t archive_object_size(oopDesc* archive_object) {\n+  Klass* klass = archive_object->klass();\n+  int lh = klass->layout_helper();\n+\n+  if (Klass::layout_helper_is_instance(lh)) {\n+    \/\/ Instance\n+    if (Klass::layout_helper_needs_slow_path(lh)) {\n+      return ((size_t*)(archive_object))[-1];\n+    } else {\n+      size_t size = (size_t)Klass::layout_helper_size_in_bytes(lh) >> LogHeapWordSize;\n+      if (UseCompactObjectHeaders && archive_object->mark().is_expanded() && klass->expand_for_hash(archive_object, archive_object->mark())) {\n+        size = align_object_size(size + 1);\n+      }\n+      return size;\n+    }\n+  } else if (Klass::layout_helper_is_array(lh)) {\n+    \/\/ Array\n+    size_t size_in_bytes;\n+    size_t array_length = (size_t)archive_array_length(archive_object);\n+    size_in_bytes = array_length << Klass::layout_helper_log2_element_size(lh);\n+    size_in_bytes += (size_t)Klass::layout_helper_header_size(lh);\n+\n+    size_t size = align_up(size_in_bytes, (size_t)MinObjAlignmentInBytes) \/ HeapWordSize;\n+    if (UseCompactObjectHeaders && archive_object->mark().is_expanded() && klass->expand_for_hash(archive_object, archive_object->mark())) {\n+      size = align_object_size(size + 1);\n+    }\n+    return size;\n+  } else {\n+    \/\/ Other\n+    return ((size_t*)(archive_object))[-1];\n+  }\n+}\n+\n+oop AOTStreamedHeapLoader::allocate_object(oopDesc* archive_object, markWord mark, size_t size, TRAPS) {\n+  assert(!archive_object->is_stackChunk(), \"no such objects are archived\");\n+\n+  oop heap_object;\n+\n+  Klass* klass = archive_object->klass();\n+  assert(!(UseCompactObjectHeaders && mark.is_hashed_not_expanded()), \"Must not be hashed\/not-expanded\");\n+  if (klass->is_mirror_instance_klass()) {\n+    size_t base_size = size;\n+    assert(!(UseCompactObjectHeaders && mark.is_not_hashed_expanded()), \"should not happen\");\n+    heap_object = Universe::heap()->class_allocate(klass, size, base_size, CHECK_NULL);\n+  } else if (klass->is_instance_klass()) {\n+    heap_object = Universe::heap()->obj_allocate(klass, size, CHECK_NULL);\n+  } else {\n+    assert(klass->is_array_klass(), \"must be\");\n+    int length = archive_array_length(archive_object);\n+    bool do_zero = klass->is_objArray_klass();\n+    heap_object = Universe::heap()->array_allocate(klass, size, length, do_zero, CHECK_NULL);\n+  }\n+\n+  heap_object->set_mark(mark);\n+\n+  return heap_object;\n+}\n+\n+void AOTStreamedHeapLoader::install_root(int root_index, oop heap_object) {\n+  objArrayOop roots = objArrayOop(_roots.resolve());\n+  OrderAccess::release(); \/\/ Once the store below publishes an object, it can be concurrently picked up by another thread without using the lock\n+  roots->obj_at_put(root_index, heap_object);\n+}\n+\n+void AOTStreamedHeapLoader::TracingObjectLoader::wait_for_iterator() {\n+  if (JavaThread::current()->is_active_Java_thread()) {\n+    \/\/ When the main thread has bootstrapped past the point of allowing safepoints,\n+    \/\/ we can and indeed have to use safepoint checking waiting.\n+    AOTHeapLoading_lock->wait();\n+  } else {\n+    \/\/ If we have no bootstrapped the main thread far enough, then we cannot and\n+    \/\/ indeed also don't need to perform safepoint checking waiting.\n+    AOTHeapLoading_lock->wait_without_safepoint_check();\n+  }\n+}\n+\n+\/\/ Link object after copying in-place\n+template <typename LinkerT>\n+class AOTStreamedHeapLoader::InPlaceLinkingOopClosure : public BasicOopIterateClosure {\n+private:\n+  oop _obj;\n+  LinkerT _linker;\n+\n+public:\n+  InPlaceLinkingOopClosure(oop obj, LinkerT linker)\n+    : _obj(obj),\n+      _linker(linker) {\n+  }\n+\n+  virtual void do_oop(oop* p) { do_oop_work(p, (int)*(intptr_t*)p); }\n+  virtual void do_oop(narrowOop* p) { do_oop_work(p, *(int*)p); }\n+\n+  template <typename T>\n+  void do_oop_work(T* p, int object_index) {\n+    int p_offset = pointer_delta_as_int((address)p, cast_from_oop<address>(_obj));\n+    oop pointee = _linker(p_offset, object_index);\n+    if (pointee != nullptr) {\n+      _obj->obj_field_put_access<IS_DEST_UNINITIALIZED>((int)p_offset, pointee);\n+    }\n+  }\n+};\n+\n+template <bool use_coops, typename LinkerT>\n+void AOTStreamedHeapLoader::copy_payload_carefully(oopDesc* archive_object,\n+                                                   oop heap_object,\n+                                                   BitMap::idx_t header_bit,\n+                                                   BitMap::idx_t start_bit,\n+                                                   BitMap::idx_t end_bit,\n+                                                   LinkerT linker) {\n+  using RawElementT = std::conditional_t<use_coops, int32_t, int64_t>;\n+  using OopElementT = std::conditional_t<use_coops, narrowOop, oop>;\n+\n+  BitMap::idx_t unfinished_bit = start_bit;\n+  BitMap::idx_t next_reference_bit = _oopmap.find_first_set_bit(unfinished_bit, end_bit);\n+\n+  \/\/ Fill in heap object bytes\n+  while (unfinished_bit < end_bit) {\n+    assert(unfinished_bit >= start_bit && unfinished_bit < end_bit, \"out of bounds copying\");\n+\n+    \/\/ This is the address of the pointee inside the input stream\n+    size_t payload_offset = unfinished_bit - header_bit;\n+    RawElementT* archive_payload_addr = ((RawElementT*)archive_object) + payload_offset;\n+    RawElementT* heap_payload_addr = cast_from_oop<RawElementT*>(heap_object) + payload_offset;\n+\n+    assert(heap_payload_addr >= cast_from_oop<RawElementT*>(heap_object) &&\n+           (HeapWord*)heap_payload_addr < cast_from_oop<HeapWord*>(heap_object) + heap_object->size(),\n+           \"Out of bounds copying\");\n+\n+    if (next_reference_bit > unfinished_bit) {\n+      \/\/ Primitive bytes available\n+      size_t primitive_elements = next_reference_bit - unfinished_bit;\n+      size_t primitive_bytes = primitive_elements * sizeof(RawElementT);\n+      ::memcpy(heap_payload_addr, archive_payload_addr, primitive_bytes);\n+\n+      unfinished_bit = next_reference_bit;\n+    } else {\n+      \/\/ Encountered reference\n+      RawElementT* archive_p = (RawElementT*)archive_payload_addr;\n+      OopElementT* heap_p = (OopElementT*)heap_payload_addr;\n+      int pointee_object_index = (int)*archive_p;\n+      int heap_p_offset = pointer_delta_as_int((address)heap_p, cast_from_oop<address>(heap_object));\n+\n+      \/\/ The object index is retrieved from the archive, not the heap object. This is\n+      \/\/ important after GC is enabled. Concurrent GC threads may scan references in the\n+      \/\/ heap for various reasons after this point. Therefore, it is not okay to first copy\n+      \/\/ the object index from a reference location in the archived object payload to a\n+      \/\/ corresponding location in the heap object payload, and then fix it up afterwards to\n+      \/\/ refer to a heap object. This is why this code iterates carefully over object references\n+      \/\/ in the archived object, linking them one by one, without clobbering the reference\n+      \/\/ locations in the heap objects with anything other than transitions from null to the\n+      \/\/ intended linked object.\n+      oop obj = linker(heap_p_offset, pointee_object_index);\n+      if (obj != nullptr) {\n+        heap_object->obj_field_put(heap_p_offset, obj);\n+      }\n+\n+      unfinished_bit++;\n+      next_reference_bit = _oopmap.find_first_set_bit(unfinished_bit, end_bit);\n+    }\n+  }\n+}\n+\n+template <bool use_coops, typename LinkerT>\n+void AOTStreamedHeapLoader::copy_object_impl(oopDesc* archive_object,\n+                                             oop heap_object,\n+                                             size_t size,\n+                                             LinkerT linker) {\n+  if (!_allow_gc) {\n+    \/\/ Without concurrent GC running, we can copy incorrect object references\n+    \/\/ and metadata references into the heap object and then fix them up in-place.\n+    size_t offset = 1;\n+    size_t payload_size = size - offset;\n+    HeapWord* archive_start = ((HeapWord*)archive_object);\n+    HeapWord* heap_start = cast_from_oop<HeapWord*>(heap_object);\n+\n+    Copy::disjoint_words(archive_start + offset, heap_start + offset, payload_size);\n+\n+    if (UseCompactObjectHeaders) {\n+      \/\/ The copying might have missed the first 4 bytes of payload\/arraylength, copy that also.\n+      *(reinterpret_cast<jint*>(heap_start) + 1) = *(reinterpret_cast<jint*>(archive_start) + 1);\n+    }\n+\n+    \/\/ In-place linking fixes up object indices from references of the heap object,\n+    \/\/ and patches them up to refer to objects. This can be done because we just copied\n+    \/\/ the payload of the object from the archive to the heap object, including the\n+    \/\/ reference object indices. However, this is only okay to do before the GC can run.\n+    \/\/ A concurrent GC thread might racingly read the object payload after GC is enabled.\n+    InPlaceLinkingOopClosure cl(heap_object, linker);\n+    heap_object->oop_iterate(&cl);\n+    HeapShared::remap_loaded_metadata(heap_object);\n+    return;\n+  }\n+\n+  \/\/ When a concurrent GC may be running, we take care not to copy incorrect oops,\n+  \/\/ narrowOops or Metadata* into the heap objects. Transitions go from 0 to the\n+  \/\/ intended runtime linked values only.\n+  size_t word_scale = use_coops ? 2 : 1;\n+  using RawElementT = std::conditional_t<use_coops, int32_t, int64_t>;\n+\n+  \/\/ Skip the markWord; it is set at allocation time\n+  size_t header_size = (UseCompactObjectHeaders && use_coops) ? 1 : word_scale;\n+\n+  size_t buffer_offset = buffer_offset_for_archive_object(archive_object);\n+  const BitMap::idx_t header_bit = obj_bit_idx_for_buffer_offset<use_coops>(buffer_offset);\n+  const BitMap::idx_t start_bit = header_bit + header_size;\n+  const BitMap::idx_t end_bit = header_bit + size * word_scale;\n+\n+  BitMap::idx_t curr_bit = start_bit;\n+\n+  if (UseCompactObjectHeaders && !use_coops) {\n+    \/\/ Copy first 4 primitive bytes.\n+    jint* archive_start = reinterpret_cast<jint*>(archive_object);\n+    HeapWord* heap_start = cast_from_oop<HeapWord*>(heap_object);\n+    *(reinterpret_cast<jint*>(heap_start) + 1) = *(archive_start + 1);\n+  }\n+\n+  \/\/ We are a bit paranoid about GC or other safepointing operations observing\n+  \/\/ shady metadata fields from the archive that do not point at real metadata.\n+  \/\/ We deal with this by explicitly reading the requested address from the\n+  \/\/ archive and fixing it to real Metadata before writing it into the heap object.\n+  HeapShared::do_metadata_offsets(heap_object, [&](int metadata_offset) {\n+    BitMap::idx_t metadata_field_idx = header_bit + (size_t)metadata_offset \/ sizeof(RawElementT);\n+    BitMap::idx_t skip = word_scale;\n+    assert(metadata_field_idx >= start_bit && metadata_field_idx + skip <= end_bit,\n+           \"Metadata field out of bounds\");\n+\n+    \/\/ Copy payload before metadata field\n+    copy_payload_carefully<use_coops>(archive_object,\n+                                      heap_object,\n+                                      header_bit,\n+                                      curr_bit,\n+                                      metadata_field_idx,\n+                                      linker);\n+\n+    \/\/ Copy metadata field\n+    Metadata* const archive_metadata = *(Metadata**)(uintptr_t(archive_object) + (size_t)metadata_offset);\n+    Metadata* const runtime_metadata = archive_metadata != nullptr\n+        ? (Metadata*)(address(archive_metadata) + AOTMetaspace::relocation_delta())\n+        : nullptr;\n+    assert(runtime_metadata == nullptr || AOTMetaspace::in_aot_cache(runtime_metadata), \"Invalid metadata pointer\");\n+    DEBUG_ONLY(Metadata* const previous_metadata = heap_object->metadata_field(metadata_offset);)\n+    assert(previous_metadata == nullptr || previous_metadata == runtime_metadata, \"Should not observe transient values\");\n+    heap_object->metadata_field_put(metadata_offset, runtime_metadata);\n+    curr_bit = metadata_field_idx + skip;\n+  });\n+\n+  \/\/ Copy trailing metadata after the last metadata word. This is usually doing\n+  \/\/ all the copying.\n+  copy_payload_carefully<use_coops>(archive_object,\n+                                    heap_object,\n+                                    header_bit,\n+                                    curr_bit,\n+                                    end_bit,\n+                                    linker);\n+}\n+\n+void AOTStreamedHeapLoader::copy_object_eager_linking(oopDesc* archive_object, oop heap_object, size_t size) {\n+  auto linker = [&](int p_offset, int pointee_object_index) {\n+    oop obj = AOTStreamedHeapLoader::heap_object_for_object_index(pointee_object_index);\n+    assert(pointee_object_index == 0 || obj != nullptr, \"Eager object loading should only encounter already allocated links\");\n+    return obj;\n+  };\n+  if (UseCompressedOops) {\n+    copy_object_impl<true>(archive_object, heap_object, size, linker);\n+  } else {\n+    copy_object_impl<false>(archive_object, heap_object, size, linker);\n+  }\n+}\n+\n+void AOTStreamedHeapLoader::TracingObjectLoader::copy_object_lazy_linking(int object_index,\n+                                                                          oopDesc* archive_object,\n+                                                                          oop heap_object,\n+                                                                          size_t size,\n+                                                                          Stack<AOTHeapTraversalEntry, mtClassShared>& dfs_stack) {\n+  auto linker = [&](int p_offset, int pointee_object_index) {\n+    dfs_stack.push({pointee_object_index, object_index, p_offset});\n+\n+    \/\/ The tracing linker is a bit lazy and mutates the reference fields in its traversal.\n+    \/\/ Returning null means don't link now.\n+    return oop(nullptr);\n+  };\n+  if (UseCompressedOops) {\n+    copy_object_impl<true>(archive_object, heap_object, size, linker);\n+  } else {\n+    copy_object_impl<false>(archive_object, heap_object, size, linker);\n+  }\n+}\n+\n+oop AOTStreamedHeapLoader::TracingObjectLoader::materialize_object_inner(int object_index, Stack<AOTHeapTraversalEntry, mtClassShared>& dfs_stack, TRAPS) {\n+  \/\/ Allocate object\n+  oopDesc* archive_object = archive_object_for_object_index(object_index);\n+  size_t size = archive_object_size(archive_object);\n+  markWord mark = archive_object->mark();\n+\n+  \/\/ The markWord is marked if the object is a String and it should be interned,\n+  \/\/ make sure to unmark it before allocating memory for the object.\n+  bool string_intern = mark.is_marked();\n+  mark = mark.set_unmarked();\n+\n+  oop heap_object;\n+\n+  if (string_intern) {\n+    int value_object_index = archived_string_value_object_index(archive_object);\n+\n+    \/\/ Materialize the value object.\n+    (void)materialize_object(value_object_index, dfs_stack, CHECK_NULL);\n+\n+    \/\/ Allocate and link the string.\n+    heap_object = allocate_object(archive_object, mark, size, CHECK_NULL);\n+    copy_object_eager_linking(archive_object, heap_object, size);\n+\n+    assert(java_lang_String::value(heap_object) == heap_object_for_object_index(value_object_index), \"Linker should have linked this correctly\");\n+\n+    \/\/ Replace the string with interned string\n+    heap_object = StringTable::intern(heap_object, CHECK_NULL);\n+  } else {\n+    heap_object = allocate_object(archive_object, mark, size, CHECK_NULL);\n+\n+    \/\/ Fill in object contents\n+    copy_object_lazy_linking(object_index, archive_object, heap_object, size, dfs_stack);\n+  }\n+\n+  \/\/ Install forwarding\n+  set_heap_object_for_object_index(object_index, heap_object);\n+\n+  return heap_object;\n+}\n+\n+oop AOTStreamedHeapLoader::TracingObjectLoader::materialize_object(int object_index, Stack<AOTHeapTraversalEntry, mtClassShared>& dfs_stack, TRAPS) {\n+  if (object_index <= _previous_batch_last_object_index) {\n+    \/\/ The transitive closure of this object has been materialized; no need to do anything\n+    return heap_object_for_object_index(object_index);\n+  }\n+\n+  if (object_index <= _current_batch_last_object_index) {\n+    \/\/ The AOTThread is currently materializing this object and its transitive closure; only need to wait for it to complete\n+    _waiting_for_iterator = true;\n+    while (object_index > _previous_batch_last_object_index) {\n+      wait_for_iterator();\n+    }\n+    _waiting_for_iterator = false;\n+\n+    \/\/ Notify the AOT thread if it is waiting for tracing to finish\n+    AOTHeapLoading_lock->notify_all();\n+    return heap_object_for_object_index(object_index);;\n+  }\n+\n+  oop heap_object = heap_object_for_object_index(object_index);\n+  if (heap_object != nullptr) {\n+    \/\/ Already materialized by mutator\n+    return heap_object;\n+  }\n+\n+  return materialize_object_inner(object_index, dfs_stack, THREAD);\n+}\n+\n+void AOTStreamedHeapLoader::TracingObjectLoader::drain_stack(Stack<AOTHeapTraversalEntry, mtClassShared>& dfs_stack, TRAPS) {\n+  while (!dfs_stack.is_empty()) {\n+    AOTHeapTraversalEntry entry = dfs_stack.pop();\n+    int pointee_object_index = entry._pointee_object_index;\n+    oop pointee_heap_object = materialize_object(pointee_object_index, dfs_stack, CHECK);\n+    oop heap_object = heap_object_for_object_index(entry._base_object_index);\n+    if (_allow_gc) {\n+      heap_object->obj_field_put(entry._heap_field_offset_bytes, pointee_heap_object);\n+    } else {\n+      heap_object->obj_field_put_access<IS_DEST_UNINITIALIZED>(entry._heap_field_offset_bytes, pointee_heap_object);\n+    }\n+  }\n+}\n+\n+oop AOTStreamedHeapLoader::TracingObjectLoader::materialize_object_transitive(int object_index, Stack<AOTHeapTraversalEntry, mtClassShared>& dfs_stack, TRAPS) {\n+  assert_locked_or_safepoint(AOTHeapLoading_lock);\n+  while (_waiting_for_iterator) {\n+    wait_for_iterator();\n+  }\n+\n+  auto handlized_materialize_object = [&](TRAPS) {\n+    oop obj = materialize_object(object_index, dfs_stack, CHECK_(Handle()));\n+    return Handle(THREAD, obj);\n+  };\n+\n+  Handle result = handlized_materialize_object(CHECK_NULL);\n+  drain_stack(dfs_stack, CHECK_NULL);\n+\n+  return result();\n+}\n+\n+oop AOTStreamedHeapLoader::TracingObjectLoader::materialize_root(int root_index, Stack<AOTHeapTraversalEntry, mtClassShared>& dfs_stack, TRAPS) {\n+  int root_object_index = object_index_for_root_index(root_index);\n+  oop root = materialize_object_transitive(root_object_index, dfs_stack, CHECK_NULL);\n+  install_root(root_index, root);\n+\n+  return root;\n+}\n+\n+int oop_handle_cmp(const void* left, const void* right) {\n+  oop* left_handle = *(oop**)left;\n+  oop* right_handle = *(oop**)right;\n+\n+  if (right_handle > left_handle) {\n+    return -1;\n+  } else if (left_handle > right_handle) {\n+    return 1;\n+  }\n+\n+  return 0;\n+}\n+\n+\/\/ The range is inclusive\n+void AOTStreamedHeapLoader::IterativeObjectLoader::initialize_range(int first_object_index, int last_object_index, TRAPS) {\n+  for (int i = first_object_index; i <= last_object_index; ++i) {\n+    oopDesc* archive_object = archive_object_for_object_index(i);\n+    markWord mark = archive_object->mark();\n+    bool string_intern = mark.is_marked();\n+    if (string_intern) {\n+      int value_object_index = archived_string_value_object_index(archive_object);\n+      if (value_object_index == i + 1) {\n+        \/\/ Interned strings are eagerly materialized in the allocation phase, so there is\n+        \/\/ nothing else to do for interned strings here for the string nor its value array.\n+        i++;\n+      }\n+      continue;\n+    }\n+    size_t size = archive_object_size(archive_object);\n+    oop heap_object = heap_object_for_object_index(i);\n+    copy_object_eager_linking(archive_object, heap_object, size);\n+  }\n+}\n+\n+\/\/ The range is inclusive\n+size_t AOTStreamedHeapLoader::IterativeObjectLoader::materialize_range(int first_object_index, int last_object_index, TRAPS) {\n+  GrowableArrayCHeap<int, mtClassShared> lazy_object_indices(0);\n+  size_t materialized_words = 0;\n+\n+  for (int i = first_object_index; i <= last_object_index; ++i) {\n+    oopDesc* archive_object = archive_object_for_object_index(i);\n+    markWord mark = archive_object->mark();\n+\n+    \/\/ The markWord is marked if the object is a String and it should be interned,\n+    \/\/ make sure to unmark it before allocating memory for the object.\n+    bool string_intern = mark.is_marked();\n+    mark = mark.set_unmarked();\n+\n+    size_t size = archive_object_size(archive_object);\n+    materialized_words += size;\n+\n+    oop heap_object = heap_object_for_object_index(i);\n+    if (heap_object != nullptr) {\n+      \/\/ Lazy loading has already initialized the object; we must not mutate it\n+      lazy_object_indices.append(i);\n+      continue;\n+    }\n+\n+    if (!string_intern) {\n+     \/\/ The normal case; no lazy loading have loaded the object yet\n+      heap_object = allocate_object(archive_object, mark, size, CHECK_0);\n+      set_heap_object_for_object_index(i, heap_object);\n+      continue;\n+    }\n+\n+    \/\/ Eagerly materialize interned strings to ensure that objects earlier than the string\n+    \/\/ in a batch get linked to the intended interned string, and not a copy.\n+    int value_object_index = archived_string_value_object_index(archive_object);\n+\n+    bool is_normal_interned_string = value_object_index == i + 1;\n+\n+    if (value_object_index < first_object_index) {\n+      \/\/ If materialized in a previous batch, the value should already be allocated and initialized.\n+      assert(heap_object_for_object_index(value_object_index) != nullptr, \"should be materialized\");\n+    } else {\n+      \/\/ Materialize the value object.\n+      oopDesc* archive_value_object = archive_object_for_object_index(value_object_index);\n+      markWord value_mark = archive_value_object->mark();\n+      size_t value_size = archive_object_size(archive_value_object);\n+      oop value_heap_object;\n+\n+      if (is_normal_interned_string) {\n+        \/\/ The common case: the value is next to the string. This happens when only the interned\n+        \/\/ string points to its value character array.\n+        assert(value_object_index <= last_object_index, \"Must be within this batch: %d <= %d\", value_object_index, last_object_index);\n+        value_heap_object = allocate_object(archive_value_object, value_mark, value_size, CHECK_0);\n+        set_heap_object_for_object_index(value_object_index, value_heap_object);\n+        materialized_words += value_size;\n+      } else {\n+        \/\/ In the uncommon case, multiple strings point to the value of an interned string.\n+        \/\/ The string can then be earlier in the batch.\n+        assert(value_object_index < i, \"surprising index\");\n+        value_heap_object = heap_object_for_object_index(value_object_index);\n+      }\n+\n+      copy_object_eager_linking(archive_value_object, value_heap_object, value_size);\n+    }\n+    \/\/ Allocate and link the string.\n+    heap_object = allocate_object(archive_object, mark, size, CHECK_0);\n+    copy_object_eager_linking(archive_object, heap_object, size);\n+\n+    assert(java_lang_String::value(heap_object) == heap_object_for_object_index(value_object_index), \"Linker should have linked this correctly\");\n+\n+    \/\/ Replace the string with interned string\n+    heap_object = StringTable::intern(heap_object, CHECK_0);\n+    set_heap_object_for_object_index(i, heap_object);\n+\n+    if (is_normal_interned_string) {\n+      \/\/ Skip over the string value, already materialized\n+      i++;\n+    }\n+  }\n+\n+  if (lazy_object_indices.is_empty()) {\n+    \/\/ Normal case; no sprinkled lazy objects in the root subgraph\n+    initialize_range(first_object_index, last_object_index, CHECK_0);\n+  } else {\n+    \/\/ The user lazy initialized some objects that are already initialized; we have to initialize around them\n+    \/\/ to make sure they are not mutated.\n+    int previous_object_index = first_object_index - 1; \/\/ Exclusive start of initialization slice\n+    for (int i = 0; i < lazy_object_indices.length(); ++i) {\n+      int lazy_object_index = lazy_object_indices.at(i);\n+      int slice_start_object_index = previous_object_index;\n+      int slice_end_object_index = lazy_object_index;\n+\n+      if (slice_end_object_index - slice_start_object_index > 1) { \/\/ Both markers are exclusive\n+        initialize_range(slice_start_object_index + 1, slice_end_object_index - 1, CHECK_0);\n+      }\n+      previous_object_index = lazy_object_index;\n+    }\n+    \/\/ Process tail range\n+    if (last_object_index - previous_object_index > 0) {\n+      initialize_range(previous_object_index + 1, last_object_index, CHECK_0);\n+    }\n+  }\n+\n+  return materialized_words;\n+}\n+\n+bool AOTStreamedHeapLoader::IterativeObjectLoader::has_more() {\n+  return _current_root_index < _num_roots;\n+}\n+\n+void AOTStreamedHeapLoader::IterativeObjectLoader::materialize_next_batch(TRAPS) {\n+  assert(has_more(), \"only materialize if there is something to materialize\");\n+\n+  int min_batch_objects = 128;\n+  int from_root_index = _current_root_index;\n+  int max_to_root_index = _num_roots - 1;\n+  int until_root_index = from_root_index;\n+  int highest_object_index;\n+\n+  \/\/ Expand the batch size from one root, to N roots until we cross 128 objects in total\n+  for (;;) {\n+    highest_object_index = highest_object_index_for_root_index(until_root_index);\n+    if (highest_object_index - _previous_batch_last_object_index >= min_batch_objects) {\n+      break;\n+    }\n+    if (until_root_index == max_to_root_index) {\n+      break;\n+    }\n+    until_root_index++;\n+  }\n+\n+  oop root = nullptr;\n+\n+  \/\/ Materialize objects of necessary, representing the transitive closure of the root\n+  if (highest_object_index > _previous_batch_last_object_index) {\n+    while (_swapping_root_format) {\n+      \/\/ When the roots are being upgraded to use handles, it is not safe to racingly\n+      \/\/ iterate over the object; we must wait. Setting the current batch last object index\n+      \/\/ to something other than the previous batch last object index indicates to the\n+      \/\/ root swapping that there is current iteration ongoing.\n+      AOTHeapLoading_lock->wait();\n+    }\n+    int first_object_index = _previous_batch_last_object_index + 1;\n+    _current_batch_last_object_index = highest_object_index;\n+    size_t allocated_words;\n+    {\n+      MutexUnlocker ml(AOTHeapLoading_lock, Mutex::_safepoint_check_flag);\n+      allocated_words = materialize_range(first_object_index, highest_object_index, CHECK);\n+    }\n+    _allocated_words += allocated_words;\n+    _previous_batch_last_object_index = _current_batch_last_object_index;\n+    if (_waiting_for_iterator) {\n+      \/\/ If tracer is waiting, let it know at the next point of unlocking that the root\n+      \/\/ set it waited for has been processed now.\n+      AOTHeapLoading_lock->notify_all();\n+    }\n+  }\n+\n+  \/\/ Install the root\n+  for (int i = from_root_index; i <= until_root_index; ++i) {\n+    int root_object_index = object_index_for_root_index(i);\n+    root = heap_object_for_object_index(root_object_index);\n+    install_root(i, root);\n+    ++_current_root_index;\n+  }\n+}\n+\n+bool AOTStreamedHeapLoader::materialize_early(TRAPS) {\n+  Ticks start = Ticks::now();\n+\n+  \/\/ Only help with early materialization from the AOT thread if the heap archive can be allocated\n+  \/\/ without the need for a GC. Otherwise, do lazy loading until GC is enabled later in the bootstrapping.\n+  size_t bootstrap_max_memory = Universe::heap()->bootstrap_max_memory();\n+  size_t bootstrap_min_memory = MAX2(_heap_region_used, 2 * M);\n+\n+  size_t before_gc_materialize_budget_bytes = (bootstrap_max_memory > bootstrap_min_memory) ? bootstrap_max_memory - bootstrap_min_memory : 0;\n+  size_t before_gc_materialize_budget_words = before_gc_materialize_budget_bytes \/ HeapWordSize;\n+\n+  log_info(aot, heap)(\"Max bootstrapping memory: %zuM, min bootstrapping memory: %zuM, selected budget: %zuM\",\n+                      bootstrap_max_memory \/ M, bootstrap_min_memory \/ M, before_gc_materialize_budget_bytes \/ M);\n+\n+  while (IterativeObjectLoader::has_more()) {\n+    if (_allow_gc || _allocated_words > before_gc_materialize_budget_words) {\n+      log_info(aot, heap)(\"Early object materialization interrupted at root %d\", _current_root_index);\n+      break;\n+    }\n+\n+    IterativeObjectLoader::materialize_next_batch(CHECK_false);\n+  }\n+\n+  _early_materialization_time_ns = (Ticks::now() - start).nanoseconds();\n+\n+  bool finished_before_gc_allowed = !_allow_gc && !IterativeObjectLoader::has_more();\n+\n+  return finished_before_gc_allowed;\n+}\n+\n+void AOTStreamedHeapLoader::materialize_late(TRAPS) {\n+  Ticks start = Ticks::now();\n+\n+  \/\/ Continue materializing with GC allowed\n+\n+  while (IterativeObjectLoader::has_more()) {\n+    IterativeObjectLoader::materialize_next_batch(CHECK);\n+  }\n+\n+  _late_materialization_time_ns = (Ticks::now() - start).nanoseconds();\n+}\n+\n+void AOTStreamedHeapLoader::cleanup() {\n+  \/\/ First ensure there is no concurrent tracing going on\n+  while (_waiting_for_iterator) {\n+    AOTHeapLoading_lock->wait();\n+  }\n+\n+  Ticks start = Ticks::now();\n+\n+  \/\/ Remove OopStorage roots\n+  if (_objects_are_handles) {\n+    size_t num_handles = _num_archived_objects;\n+    \/\/ Skip the null entry\n+    oop** handles = ((oop**)_object_index_to_heap_object_table) + 1;\n+    \/\/ Sort the handles so that oop storage can release them faster\n+    qsort(handles, num_handles, sizeof(oop*), (int (*)(const void*, const void*))oop_handle_cmp);\n+    size_t num_null_handles = 0;\n+    for (size_t handles_remaining = num_handles; handles_remaining != 0; --handles_remaining) {\n+      oop* handle = handles[handles_remaining - 1];\n+      if (handle == nullptr) {\n+        num_null_handles = handles_remaining;\n+        break;\n+      }\n+      NativeAccess<>::oop_store(handle, nullptr);\n+    }\n+    Universe::vm_global()->release(&handles[num_null_handles], num_handles - num_null_handles);\n+  }\n+\n+  FREE_C_HEAP_ARRAY(void*, _object_index_to_heap_object_table);\n+\n+  \/\/ Unmap regions\n+  FileMapInfo::current_info()->unmap_region(AOTMetaspace::hp);\n+  FileMapInfo::current_info()->unmap_region(AOTMetaspace::bm);\n+\n+  _cleanup_materialization_time_ns = (Ticks::now() - start).nanoseconds();\n+\n+  log_statistics();\n+}\n+\n+void AOTStreamedHeapLoader::log_statistics() {\n+  uint64_t total_duration_us = (Ticks::now() - _materialization_start_ticks).microseconds();\n+  const bool is_async = _loading_all_objects && !AOTEagerlyLoadObjects;\n+  const char* const async_or_sync = is_async ? \"async\" : \"sync\";\n+  log_info(aot, heap)(\"start to finish materialization time: \" UINT64_FORMAT \"us\",\n+                      total_duration_us);\n+  log_info(aot, heap)(\"early object materialization time (%s): \" UINT64_FORMAT \"us\",\n+                      async_or_sync, _early_materialization_time_ns \/ 1000);\n+  log_info(aot, heap)(\"late object materialization time (%s): \" UINT64_FORMAT \"us\",\n+                      async_or_sync, _late_materialization_time_ns \/ 1000);\n+  log_info(aot, heap)(\"object materialization cleanup time (%s): \" UINT64_FORMAT \"us\",\n+                      async_or_sync, _cleanup_materialization_time_ns \/ 1000);\n+  log_info(aot, heap)(\"final object materialization time stall (sync): \" UINT64_FORMAT \"us\",\n+                      _final_materialization_time_ns \/ 1000);\n+  log_info(aot, heap)(\"bootstrapping lazy materialization time (sync): \" UINT64_FORMAT \"us\",\n+                      _accumulated_lazy_materialization_time_ns \/ 1000);\n+\n+  uint64_t sync_time = _final_materialization_time_ns + _accumulated_lazy_materialization_time_ns;\n+  uint64_t async_time = _early_materialization_time_ns + _late_materialization_time_ns + _cleanup_materialization_time_ns;\n+\n+  if (!is_async) {\n+    sync_time += async_time;\n+    async_time = 0;\n+  }\n+\n+  log_info(aot, heap)(\"sync materialization time: \" UINT64_FORMAT \"us\",\n+                      sync_time \/ 1000);\n+\n+  log_info(aot, heap)(\"async materialization time: \" UINT64_FORMAT \"us\",\n+                      async_time \/ 1000);\n+\n+  uint64_t iterative_time = (uint64_t)(is_async ? async_time : sync_time);\n+  uint64_t materialized_bytes = _allocated_words * HeapWordSize;\n+  log_info(aot, heap)(\"%s materialized \" UINT64_FORMAT \"K (\" UINT64_FORMAT \"M\/s)\", async_or_sync,\n+                      materialized_bytes \/ 1024, uint64_t(materialized_bytes * UCONST64(1'000'000'000) \/ M \/ iterative_time));\n+}\n+\n+void AOTStreamedHeapLoader::materialize_objects() {\n+  \/\/ We cannot handle any exception when materializing roots. Exits the VM.\n+  EXCEPTION_MARK\n+\n+  \/\/ Objects are laid out in DFS order; DFS traverse the roots by linearly walking all objects\n+  HandleMark hm(THREAD);\n+\n+  \/\/ Early materialization with a budget before GC is allowed\n+  MutexLocker ml(AOTHeapLoading_lock, Mutex::_safepoint_check_flag);\n+\n+  materialize_early(CHECK);\n+  await_gc_enabled();\n+  materialize_late(CHECK);\n+  \/\/ Notify materialization is done\n+  AOTHeapLoading_lock->notify_all();\n+  cleanup();\n+}\n+\n+void AOTStreamedHeapLoader::switch_object_index_to_handle(int object_index) {\n+  oop heap_object = cast_to_oop(_object_index_to_heap_object_table[object_index]);\n+  if (heap_object == nullptr) {\n+    return;\n+  }\n+\n+  oop* handle = Universe::vm_global()->allocate();\n+  NativeAccess<>::oop_store(handle, heap_object);\n+  _object_index_to_heap_object_table[object_index] = handle;\n+}\n+\n+void AOTStreamedHeapLoader::enable_gc() {\n+  if (AOTEagerlyLoadObjects && !IterativeObjectLoader::has_more()) {\n+    \/\/ Everything was loaded eagerly at early startup\n+    return;\n+  }\n+\n+  \/\/ We cannot handle any exception when materializing roots. Exits the VM.\n+  EXCEPTION_MARK\n+\n+  MutexLocker ml(AOTHeapLoading_lock, Mutex::_safepoint_check_flag);\n+\n+  \/\/ First wait until no tracing is active\n+  while (_waiting_for_iterator) {\n+    AOTHeapLoading_lock->wait();\n+  }\n+\n+  \/\/ Lock further tracing from starting\n+  _waiting_for_iterator = true;\n+\n+  \/\/ Record iterator progress\n+  int num_handles = (int)_num_archived_objects;\n+\n+  \/\/ Lock further iteration from starting\n+  _swapping_root_format = true;\n+\n+  \/\/ Then wait for the iterator to stop\n+  while (_previous_batch_last_object_index != _current_batch_last_object_index) {\n+    AOTHeapLoading_lock->wait();\n+  }\n+\n+  if (IterativeObjectLoader::has_more()) {\n+    \/\/ If there is more to be materialized, we have to upgrade the object index\n+    \/\/ to object mapping to use handles. If there isn't more to materialize, the\n+    \/\/ handle will no longer e used; they are only used to materialize objects.\n+\n+    for (int i = 1; i <= num_handles; ++i) {\n+      \/\/ Upgrade the roots to use handles\n+      switch_object_index_to_handle(i);\n+    }\n+\n+    \/\/ From now on, accessing the object table must be done through a handle.\n+    _objects_are_handles = true;\n+  }\n+\n+  \/\/ Unlock tracing\n+  _waiting_for_iterator = false;\n+\n+  \/\/ Unlock iteration\n+  _swapping_root_format = false;\n+\n+  _allow_gc = true;\n+\n+  AOTHeapLoading_lock->notify_all();\n+\n+  if (AOTEagerlyLoadObjects && IterativeObjectLoader::has_more()) {\n+    materialize_late(CHECK);\n+    cleanup();\n+  }\n+}\n+\n+void AOTStreamedHeapLoader::materialize_thread_object() {\n+  AOTThread::materialize_thread_object();\n+}\n+\n+void AOTStreamedHeapLoader::finish_materialize_objects() {\n+  Ticks start = Ticks::now();\n+\n+  if (_loading_all_objects) {\n+    MutexLocker ml(AOTHeapLoading_lock, Mutex::_safepoint_check_flag);\n+    \/\/ Wait for the AOT thread to finish\n+    while (IterativeObjectLoader::has_more()) {\n+      AOTHeapLoading_lock->wait();\n+    }\n+  } else {\n+    assert(!AOTEagerlyLoadObjects, \"sanity\");\n+    assert(_current_root_index == 0, \"sanity\");\n+    \/\/ Without the full module graph we have done only lazy tracing materialization.\n+    \/\/ Ensure all roots are processed here by triggering root loading on every root.\n+    for (int i = 0; i < _num_roots; ++i) {\n+      get_root(i);\n+    }\n+    cleanup();\n+  }\n+\n+  _final_materialization_time_ns = (Ticks::now() - start).nanoseconds();\n+}\n+\n+void account_lazy_materialization_time_ns(uint64_t time, const char* description, int index) {\n+  AtomicAccess::add(&_accumulated_lazy_materialization_time_ns, time);\n+  log_debug(aot, heap)(\"Lazy materialization of %s: %d end (\" UINT64_FORMAT \" us of \" UINT64_FORMAT \" us)\", description, index, time \/ 1000, _accumulated_lazy_materialization_time_ns \/ 1000);\n+}\n+\n+\/\/ Initialize an empty array of AOT heap roots; materialize them lazily\n+void AOTStreamedHeapLoader::initialize() {\n+  EXCEPTION_MARK\n+\n+  _materialization_start_ticks = Ticks::now();\n+\n+  FileMapInfo::current_info()->map_bitmap_region();\n+\n+  _heap_region = FileMapInfo::current_info()->region_at(AOTMetaspace::hp);\n+  _bitmap_region = FileMapInfo::current_info()->region_at(AOTMetaspace::bm);\n+\n+  assert(_heap_region->used() > 0, \"empty heap archive?\");\n+\n+  _is_in_use = true;\n+\n+  \/\/ archived roots are at this offset in the stream.\n+  size_t roots_offset = FileMapInfo::current_info()->streamed_heap()->roots_offset();\n+  size_t forwarding_offset = FileMapInfo::current_info()->streamed_heap()->forwarding_offset();\n+  size_t root_highest_object_index_table_offset = FileMapInfo::current_info()->streamed_heap()->root_highest_object_index_table_offset();\n+  _num_archived_objects = FileMapInfo::current_info()->streamed_heap()->num_archived_objects();\n+\n+  \/\/ The first int is the length of the array\n+  _roots_archive = ((int*)(((address)_heap_region->mapped_base()) + roots_offset)) + 1;\n+  _num_roots = _roots_archive[-1];\n+  _heap_region_used = _heap_region->used();\n+\n+  \/\/ We can't retire a TLAB until the filler klass is set; set it to the archived object klass.\n+  CollectedHeap::set_filler_object_klass(vmClasses::Object_klass());\n+\n+  objArrayOop roots = oopFactory::new_objectArray(_num_roots, CHECK);\n+  _roots = OopHandle(Universe::vm_global(), roots);\n+\n+  _object_index_to_buffer_offset_table = (size_t*)(((address)_heap_region->mapped_base()) + forwarding_offset);\n+  \/\/ We allocate the first entry for \"null\"\n+  _object_index_to_heap_object_table = NEW_C_HEAP_ARRAY(void*, _num_archived_objects + 1, mtClassShared);\n+  Copy::zero_to_bytes(_object_index_to_heap_object_table, (_num_archived_objects + 1) * sizeof(void*));\n+\n+  _root_highest_object_index_table = (int*)(((address)_heap_region->mapped_base()) + root_highest_object_index_table_offset);\n+\n+  address start = (address)(_bitmap_region->mapped_base()) + _heap_region->oopmap_offset();\n+  _oopmap = BitMapView((BitMap::bm_word_t*)start, _heap_region->oopmap_size_in_bits());\n+\n+\n+  if (FLAG_IS_DEFAULT(AOTEagerlyLoadObjects)) {\n+    \/\/ Concurrency will not help much if there are no extra cores available.\n+    FLAG_SET_ERGO(AOTEagerlyLoadObjects, os::initial_active_processor_count() <= 1);\n+  }\n+\n+  \/\/ If the full module graph is not available or the JVMTI class file load hook is on, we\n+  \/\/ will prune the object graph to not include cached objects in subgraphs that are not intended\n+  \/\/ to be loaded.\n+  _loading_all_objects = CDSConfig::is_using_full_module_graph() && !JvmtiExport::should_post_class_file_load_hook();\n+  if (!_loading_all_objects) {\n+    \/\/ When not using FMG, fall back to tracing materialization\n+    FLAG_SET_ERGO(AOTEagerlyLoadObjects, false);\n+    return;\n+  }\n+\n+  if (AOTEagerlyLoadObjects) {\n+    \/\/ Objects are laid out in DFS order; DFS traverse the roots by linearly walking all objects\n+    HandleMark hm(THREAD);\n+\n+    \/\/ Early materialization with a budget before GC is allowed\n+    MutexLocker ml(AOTHeapLoading_lock, Mutex::_safepoint_check_flag);\n+\n+    bool finished_before_gc_allowed = materialize_early(CHECK);\n+    if (finished_before_gc_allowed) {\n+      cleanup();\n+    }\n+  } else {\n+    AOTThread::initialize();\n+  }\n+}\n+\n+oop AOTStreamedHeapLoader::materialize_root(int root_index) {\n+  Ticks start = Ticks::now();\n+  \/\/ We cannot handle any exception when materializing a root. Exits the VM.\n+  EXCEPTION_MARK\n+  Stack<AOTHeapTraversalEntry, mtClassShared> dfs_stack;\n+  HandleMark hm(THREAD);\n+\n+  oop result;\n+  {\n+    MutexLocker ml(AOTHeapLoading_lock, Mutex::_safepoint_check_flag);\n+\n+    oop root = objArrayOop(_roots.resolve())->obj_at(root_index);\n+\n+    if (root != nullptr) {\n+      \/\/ The root has already been materialized\n+      result = root;\n+    } else {\n+      \/\/ The root has not been materialized, start tracing materialization\n+      result = TracingObjectLoader::materialize_root(root_index, dfs_stack, CHECK_NULL);\n+    }\n+  }\n+\n+  uint64_t duration = (Ticks::now() - start).nanoseconds();\n+\n+  account_lazy_materialization_time_ns(duration, \"root\", root_index);\n+\n+  return result;\n+}\n+\n+oop AOTStreamedHeapLoader::get_root(int index) {\n+  oop result = objArrayOop(_roots.resolve())->obj_at(index);\n+  if (result == nullptr) {\n+    \/\/ Materialize root\n+    result = materialize_root(index);\n+  }\n+  if (result == _roots.resolve()) {\n+    \/\/ A self-reference to the roots array acts as a sentinel object for null,\n+    \/\/ indicating that the root has been cleared.\n+    result = nullptr;\n+  }\n+  \/\/ Acquire the root transitive object payload\n+  OrderAccess::acquire();\n+  return result;\n+}\n+\n+void AOTStreamedHeapLoader::clear_root(int index) {\n+  \/\/ Self-reference to the roots array acts as a sentinel object for null,\n+  \/\/ indicating that the root has been cleared.\n+  objArrayOop(_roots.resolve())->obj_at_put(index, _roots.resolve());\n+}\n+\n+void AOTStreamedHeapLoader::await_gc_enabled() {\n+  while (!_allow_gc) {\n+    AOTHeapLoading_lock->wait();\n+  }\n+}\n+\n+void AOTStreamedHeapLoader::finish_initialization(FileMapInfo* static_mapinfo) {\n+  static_mapinfo->stream_heap_region();\n+}\n+\n+AOTMapLogger::OopDataIterator* AOTStreamedHeapLoader::oop_iterator(FileMapInfo* info, address buffer_start, address buffer_end) {\n+  class StreamedLoaderOopIterator : public AOTMapLogger::OopDataIterator {\n+  private:\n+    int _current;\n+    int _next;\n+\n+    address _buffer_start;\n+\n+    int _num_archived_objects;\n+\n+  public:\n+    StreamedLoaderOopIterator(address buffer_start,\n+                              int num_archived_objects)\n+      : _current(0),\n+        _next(1),\n+        _buffer_start(buffer_start),\n+        _num_archived_objects(num_archived_objects) {\n+    }\n+\n+    AOTMapLogger::OopData capture(int dfs_index) {\n+      size_t buffered_offset = buffer_offset_for_object_index(dfs_index);\n+      address buffered_addr = _buffer_start + buffered_offset;\n+      oopDesc* raw_oop = (oopDesc*)buffered_addr;\n+      size_t size = archive_object_size(raw_oop);\n+\n+      intptr_t target_location = (intptr_t)buffered_offset;\n+      uint32_t narrow_location = checked_cast<uint32_t>(dfs_index);\n+      Klass* klass = raw_oop->klass();\n+\n+      address requested_addr = (address)buffered_offset;\n+\n+      return { buffered_addr,\n+               requested_addr,\n+               target_location,\n+               narrow_location,\n+               raw_oop,\n+               klass,\n+               size,\n+               false };\n+    }\n+\n+    bool has_next() override {\n+      return _next <= _num_archived_objects;\n+    }\n+\n+    AOTMapLogger::OopData next() override {\n+      _current = _next;\n+      AOTMapLogger::OopData result = capture(_current);\n+      _next = _current + 1;\n+      return result;\n+    }\n+\n+    AOTMapLogger::OopData obj_at(narrowOop* addr) override {\n+      int dfs_index = (int)(*addr);\n+      if (dfs_index == 0) {\n+        return null_data();\n+      } else {\n+        return capture(dfs_index);\n+      }\n+    }\n+\n+    AOTMapLogger::OopData obj_at(oop* addr) override {\n+      int dfs_index = (int)cast_from_oop<uintptr_t>(*addr);\n+      if (dfs_index == 0) {\n+        return null_data();\n+      } else {\n+        return capture(dfs_index);\n+      }\n+    }\n+\n+    GrowableArrayCHeap<AOTMapLogger::OopData, mtClass>* roots() override {\n+      GrowableArrayCHeap<AOTMapLogger::OopData, mtClass>* result = new GrowableArrayCHeap<AOTMapLogger::OopData, mtClass>();\n+\n+      for (int i = 0; i < _num_roots; ++i) {\n+        int object_index = object_index_for_root_index(i);\n+        result->append(capture(object_index));\n+      }\n+\n+      return result;\n+    }\n+  };\n+\n+  assert(_is_in_use, \"printing before initializing?\");\n+\n+  return new StreamedLoaderOopIterator(buffer_start, (int)info->streamed_heap()->num_archived_objects());\n+}\n+\n+#endif \/\/ INCLUDE_CDS_JAVA_HEAP\n","filename":"src\/hotspot\/share\/cds\/aotStreamedHeapLoader.cpp","additions":1214,"deletions":0,"binary":false,"changes":1214,"status":"added"},{"patch":"@@ -0,0 +1,628 @@\n+\/*\n+ * Copyright (c) 2025, Oracle and\/or its affiliates. All rights reserved.\n+ * DO NOT ALTER OR REMOVE COPYRIGHT NOTICES OR THIS FILE HEADER.\n+ *\n+ * This code is free software; you can redistribute it and\/or modify it\n+ * under the terms of the GNU General Public License version 2 only, as\n+ * published by the Free Software Foundation.\n+ *\n+ * This code is distributed in the hope that it will be useful, but WITHOUT\n+ * ANY WARRANTY; without even the implied warranty of MERCHANTABILITY or\n+ * FITNESS FOR A PARTICULAR PURPOSE.  See the GNU General Public License\n+ * version 2 for more details (a copy is included in the LICENSE file that\n+ * accompanied this code).\n+ *\n+ * You should have received a copy of the GNU General Public License version\n+ * 2 along with this work; if not, write to the Free Software Foundation,\n+ * Inc., 51 Franklin St, Fifth Floor, Boston, MA 02110-1301 USA.\n+ *\n+ * Please contact Oracle, 500 Oracle Parkway, Redwood Shores, CA 94065 USA\n+ * or visit www.oracle.com if you need additional information or have any\n+ * questions.\n+ *\n+ *\/\n+\n+#include \"cds\/aotReferenceObjSupport.hpp\"\n+#include \"cds\/aotStreamedHeapWriter.hpp\"\n+#include \"cds\/cdsConfig.hpp\"\n+#include \"cds\/filemap.hpp\"\n+#include \"cds\/heapShared.inline.hpp\"\n+#include \"cds\/regeneratedClasses.hpp\"\n+#include \"classfile\/modules.hpp\"\n+#include \"classfile\/stringTable.hpp\"\n+#include \"classfile\/systemDictionary.hpp\"\n+#include \"gc\/shared\/collectedHeap.hpp\"\n+#include \"memory\/iterator.inline.hpp\"\n+#include \"memory\/oopFactory.hpp\"\n+#include \"memory\/universe.hpp\"\n+#include \"oops\/compressedOops.hpp\"\n+#include \"oops\/objArrayOop.inline.hpp\"\n+#include \"oops\/oop.inline.hpp\"\n+#include \"oops\/oopHandle.inline.hpp\"\n+#include \"oops\/typeArrayKlass.hpp\"\n+#include \"oops\/typeArrayOop.hpp\"\n+#include \"runtime\/java.hpp\"\n+#include \"runtime\/mutexLocker.hpp\"\n+#include \"utilities\/bitMap.inline.hpp\"\n+#include \"utilities\/stack.inline.hpp\"\n+\n+#if INCLUDE_CDS_JAVA_HEAP\n+\n+GrowableArrayCHeap<u1, mtClassShared>* AOTStreamedHeapWriter::_buffer = nullptr;\n+\n+\/\/ The following are offsets from buffer_bottom()\n+size_t AOTStreamedHeapWriter::_buffer_used;\n+size_t AOTStreamedHeapWriter::_roots_offset;\n+size_t AOTStreamedHeapWriter::_forwarding_offset;\n+size_t AOTStreamedHeapWriter::_root_highest_object_index_table_offset;\n+\n+GrowableArrayCHeap<oop, mtClassShared>* AOTStreamedHeapWriter::_source_objs;\n+\n+AOTStreamedHeapWriter::BufferOffsetToSourceObjectTable* AOTStreamedHeapWriter::_buffer_offset_to_source_obj_table;\n+AOTStreamedHeapWriter::SourceObjectToDFSOrderTable* AOTStreamedHeapWriter::_dfs_order_table;\n+\n+int* AOTStreamedHeapWriter::_roots_highest_dfs;\n+size_t* AOTStreamedHeapWriter::_dfs_to_archive_object_table;\n+\n+static const int max_table_capacity = 0x3fffffff;\n+\n+void AOTStreamedHeapWriter::init() {\n+  if (CDSConfig::is_dumping_heap()) {\n+    _buffer_offset_to_source_obj_table = new (mtClassShared) BufferOffsetToSourceObjectTable(8, max_table_capacity);\n+\n+    int initial_source_objs_capacity = 10000;\n+    _source_objs = new GrowableArrayCHeap<oop, mtClassShared>(initial_source_objs_capacity);\n+  }\n+}\n+\n+void AOTStreamedHeapWriter::delete_tables_with_raw_oops() {\n+  delete _source_objs;\n+  _source_objs = nullptr;\n+\n+  delete _dfs_order_table;\n+  _dfs_order_table = nullptr;\n+}\n+\n+void AOTStreamedHeapWriter::add_source_obj(oop src_obj) {\n+  _source_objs->append(src_obj);\n+}\n+\n+class FollowOopIterateClosure: public BasicOopIterateClosure {\n+  Stack<oop, mtClassShared>* _dfs_stack;\n+  oop _src_obj;\n+  bool _is_java_lang_ref;\n+\n+public:\n+  FollowOopIterateClosure(Stack<oop, mtClassShared>* dfs_stack, oop src_obj, bool is_java_lang_ref) :\n+    _dfs_stack(dfs_stack),\n+    _src_obj(src_obj),\n+    _is_java_lang_ref(is_java_lang_ref) {}\n+\n+  void do_oop(narrowOop *p) { do_oop_work(p); }\n+  void do_oop(      oop *p) { do_oop_work(p); }\n+\n+private:\n+  template <class T> void do_oop_work(T *p) {\n+    size_t field_offset = pointer_delta(p, _src_obj, sizeof(char));\n+    oop obj = HeapShared::maybe_remap_referent(_is_java_lang_ref, field_offset, HeapAccess<>::oop_load(p));\n+    if (obj != nullptr) {\n+      _dfs_stack->push(obj);\n+    }\n+  }\n+};\n+\n+int AOTStreamedHeapWriter::cmp_dfs_order(oop* o1, oop* o2) {\n+  int* o1_dfs = _dfs_order_table->get(*o1);\n+  int* o2_dfs = _dfs_order_table->get(*o2);\n+  return *o1_dfs - *o2_dfs;\n+}\n+\n+void AOTStreamedHeapWriter::order_source_objs(GrowableArrayCHeap<oop, mtClassShared>* roots) {\n+  Stack<oop, mtClassShared> dfs_stack;\n+  _dfs_order_table = new (mtClassShared) SourceObjectToDFSOrderTable(8, max_table_capacity);\n+  _roots_highest_dfs = NEW_C_HEAP_ARRAY(int, (size_t)roots->length(), mtClassShared);\n+  _dfs_to_archive_object_table = NEW_C_HEAP_ARRAY(size_t, (size_t)_source_objs->length() + 1, mtClassShared);\n+\n+  for (int i = 0; i < _source_objs->length(); ++i) {\n+    oop obj = _source_objs->at(i);\n+    _dfs_order_table->put(cast_from_oop<void*>(obj), -1);\n+    _dfs_order_table->maybe_grow();\n+  }\n+\n+  int dfs_order = 0;\n+\n+  for (int i = 0; i < roots->length(); ++i) {\n+    oop root = roots->at(i);\n+\n+    if (root == nullptr) {\n+      log_info(aot, heap)(\"null root at %d\", i);\n+      continue;\n+    }\n+\n+    dfs_stack.push(root);\n+\n+    while (!dfs_stack.is_empty()) {\n+      oop obj = dfs_stack.pop();\n+      assert(obj != nullptr, \"null root\");\n+      int* dfs_number = _dfs_order_table->get(cast_from_oop<void*>(obj));\n+      if (*dfs_number != -1) {\n+        \/\/ Already visited in the traversal\n+        continue;\n+      }\n+      _dfs_order_table->put(cast_from_oop<void*>(obj), ++dfs_order);\n+      _dfs_order_table->maybe_grow();\n+\n+      FollowOopIterateClosure cl(&dfs_stack, obj, AOTReferenceObjSupport::check_if_ref_obj(obj));\n+      obj->oop_iterate(&cl);\n+    }\n+\n+    _roots_highest_dfs[i] = dfs_order;\n+  }\n+\n+  _source_objs->sort(cmp_dfs_order);\n+}\n+\n+void AOTStreamedHeapWriter::write(GrowableArrayCHeap<oop, mtClassShared>* roots,\n+                                  ArchiveStreamedHeapInfo* heap_info) {\n+  assert(CDSConfig::is_dumping_heap(), \"sanity\");\n+  allocate_buffer();\n+  order_source_objs(roots);\n+  copy_source_objs_to_buffer(roots);\n+  map_embedded_oops(heap_info);\n+  populate_archive_heap_info(heap_info);\n+}\n+\n+void AOTStreamedHeapWriter::allocate_buffer() {\n+  int initial_buffer_size = 100000;\n+  _buffer = new GrowableArrayCHeap<u1, mtClassShared>(initial_buffer_size);\n+  _buffer_used = 0;\n+  ensure_buffer_space(1); \/\/ so that buffer_bottom() works\n+}\n+\n+void AOTStreamedHeapWriter::ensure_buffer_space(size_t min_bytes) {\n+  \/\/ We usually have very small heaps. If we get a huge one it's probably caused by a bug.\n+  guarantee(min_bytes <= max_jint, \"we dont support archiving more than 2G of objects\");\n+  _buffer->at_grow(to_array_index(min_bytes));\n+}\n+\n+void AOTStreamedHeapWriter::copy_roots_to_buffer(GrowableArrayCHeap<oop, mtClassShared>* roots) {\n+  int length = roots->length();\n+  size_t byte_size = align_up(sizeof(int) + sizeof(int) * (size_t)length, (size_t)HeapWordSize);\n+\n+  size_t new_used = _buffer_used + byte_size;\n+  ensure_buffer_space(new_used);\n+\n+  int* mem = offset_to_buffered_address<int*>(_buffer_used);\n+  memset(mem, 0, byte_size);\n+  *mem = length;\n+\n+  for (int i = 0; i < length; i++) {\n+    \/\/ Do not use arrayOop->obj_at_put(i, o) as arrayOop is outside of the real heap!\n+    oop o = roots->at(i);\n+    int dfs_index = o == nullptr ? 0 : *_dfs_order_table->get(cast_from_oop<void*>(o));\n+    mem[i + 1] = dfs_index;\n+  }\n+  log_info(aot, heap)(\"archived obj roots[%d] = %zu bytes, mem = %p\", length, byte_size, mem);\n+\n+  _roots_offset = _buffer_used;\n+  _buffer_used = new_used;\n+}\n+\n+template <typename T>\n+void AOTStreamedHeapWriter::write(T value) {\n+  size_t new_used = _buffer_used + sizeof(T);\n+  ensure_buffer_space(new_used);\n+  T* mem = offset_to_buffered_address<T*>(_buffer_used);\n+  *mem = value;\n+  _buffer_used = new_used;\n+}\n+\n+void AOTStreamedHeapWriter::copy_forwarding_to_buffer() {\n+  _forwarding_offset = _buffer_used;\n+\n+  write<size_t>(0); \/\/ The first entry is the null entry\n+\n+  \/\/ Write a mapping from object index to buffer offset\n+  for (int i = 1; i <= _source_objs->length(); i++) {\n+    size_t buffer_offset = _dfs_to_archive_object_table[i];\n+    write(buffer_offset);\n+  }\n+}\n+\n+void AOTStreamedHeapWriter::copy_roots_max_dfs_to_buffer(int roots_length) {\n+  _root_highest_object_index_table_offset = _buffer_used;\n+\n+  for (int i = 0; i < roots_length; ++i) {\n+    int highest_dfs = _roots_highest_dfs[i];\n+    write(highest_dfs);\n+  }\n+\n+  if ((roots_length % 2) != 0) {\n+    write(-1); \/\/ Align up to a 64 bit word\n+  }\n+}\n+\n+static bool is_interned_string(oop obj) {\n+  if (!java_lang_String::is_instance(obj)) {\n+    return false;\n+  }\n+\n+  ResourceMark rm;\n+  int len;\n+  jchar* name = java_lang_String::as_unicode_string_or_null(obj, len);\n+  if (name == nullptr) {\n+    fatal(\"Insufficient memory for dumping\");\n+  }\n+  return StringTable::lookup(name, len) == obj;\n+}\n+\n+static BitMap::idx_t bit_idx_for_buffer_offset(size_t buffer_offset) {\n+  if (UseCompressedOops) {\n+    return BitMap::idx_t(buffer_offset \/ sizeof(narrowOop));\n+  } else {\n+    return BitMap::idx_t(buffer_offset \/ sizeof(HeapWord));\n+  }\n+}\n+\n+bool AOTStreamedHeapWriter::is_dumped_interned_string(oop obj) {\n+  return is_interned_string(obj) && HeapShared::get_cached_oop_info(obj) != nullptr;\n+}\n+\n+void AOTStreamedHeapWriter::copy_source_objs_to_buffer(GrowableArrayCHeap<oop, mtClassShared>* roots) {\n+  for (int i = 0; i < _source_objs->length(); i++) {\n+    oop src_obj = _source_objs->at(i);\n+    HeapShared::CachedOopInfo* info = HeapShared::get_cached_oop_info(src_obj);\n+    assert(info != nullptr, \"must be\");\n+    size_t buffer_offset = copy_one_source_obj_to_buffer(src_obj);\n+    info->set_buffer_offset(buffer_offset);\n+\n+    OopHandle handle(Universe::vm_global(), src_obj);\n+    _buffer_offset_to_source_obj_table->put_when_absent(buffer_offset, handle);\n+    _buffer_offset_to_source_obj_table->maybe_grow();\n+\n+    int dfs_order = i + 1;\n+    _dfs_to_archive_object_table[dfs_order] = buffer_offset;\n+  }\n+\n+  copy_roots_to_buffer(roots);\n+  copy_forwarding_to_buffer();\n+  copy_roots_max_dfs_to_buffer(roots->length());\n+\n+  log_info(aot)(\"Size of heap region = %zu bytes, %d objects, %d roots\",\n+                _buffer_used, _source_objs->length() + 1, roots->length());\n+}\n+\n+template <typename T>\n+void update_buffered_object_field(address buffered_obj, int field_offset, T value) {\n+  T* field_addr = cast_to_oop(buffered_obj)->field_addr<T>(field_offset);\n+  *field_addr = value;\n+}\n+\n+static bool needs_explicit_size(oop src_obj) {\n+  Klass* klass = src_obj->klass();\n+  int lh = klass->layout_helper();\n+\n+  \/\/ Simple instances or arrays don't need explicit size\n+  if (Klass::layout_helper_is_instance(lh)) {\n+    return Klass::layout_helper_needs_slow_path(lh);\n+  }\n+\n+  return !Klass::layout_helper_is_array(lh);\n+}\n+\n+size_t AOTStreamedHeapWriter::copy_one_source_obj_to_buffer(oop src_obj) {\n+  size_t old_size = src_obj->size();\n+  size_t size = src_obj->copy_size_cds(old_size, src_obj->mark());\n+  if (needs_explicit_size(src_obj)) {\n+    \/\/ Explicitly write object size for more complex objects, to avoid having to\n+    \/\/ pretend the buffer objects are objects when loading the objects, in order\n+    \/\/ to read the size. Most of the time, the layout helper of the class is enough.\n+    write<size_t>(size);\n+  }\n+  size_t byte_size = size * HeapWordSize;\n+  assert(byte_size > 0, \"no zero-size objects\");\n+\n+  size_t new_used = _buffer_used + byte_size;\n+  assert(new_used > _buffer_used, \"no wrap around\");\n+\n+  ensure_buffer_space(new_used);\n+\n+  if (is_interned_string(src_obj)) {\n+    java_lang_String::hash_code(src_obj);                   \/\/ Sets the hash code field(s)\n+    java_lang_String::set_deduplication_forbidden(src_obj); \/\/ Allows faster interning at runtime\n+    assert(java_lang_String::hash_is_set(src_obj), \"hash must be set\");\n+  }\n+\n+  address from = cast_from_oop<address>(src_obj);\n+  address to = offset_to_buffered_address<address>(_buffer_used);\n+  assert(is_object_aligned(_buffer_used), \"sanity\");\n+  assert(is_object_aligned(byte_size), \"sanity\");\n+  memcpy(to, from, MIN2(size, old_size) * HeapWordSize);\n+\n+  if (java_lang_Module::is_instance(src_obj)) {\n+    \/\/ These native pointers will be restored explicitly at run time.\n+    Modules::check_archived_module_oop(src_obj);\n+    update_buffered_object_field<ModuleEntry*>(to, java_lang_Module::module_entry_offset(), nullptr);\n+  } else if (java_lang_ClassLoader::is_instance(src_obj)) {\n+#ifdef ASSERT\n+    \/\/ We only archive these loaders\n+    if (src_obj != SystemDictionary::java_platform_loader() &&\n+        src_obj != SystemDictionary::java_system_loader()) {\n+      assert(src_obj->klass()->name()->equals(\"jdk\/internal\/loader\/ClassLoaders$BootClassLoader\"), \"must be\");\n+    }\n+#endif\n+    update_buffered_object_field<ClassLoaderData*>(to, java_lang_ClassLoader::loader_data_offset(), nullptr);\n+  }\n+\n+  size_t buffered_obj_offset = _buffer_used;\n+  _buffer_used = new_used;\n+\n+  return buffered_obj_offset;\n+}\n+\n+\/\/ Oop mapping\n+\n+inline void AOTStreamedHeapWriter::store_oop_in_buffer(oop* buffered_addr, int dfs_index) {\n+  *(ssize_t*)buffered_addr = dfs_index;\n+}\n+\n+inline void AOTStreamedHeapWriter::store_oop_in_buffer(narrowOop* buffered_addr, int dfs_index) {\n+  *(int32_t*)buffered_addr = (int32_t)dfs_index;\n+}\n+\n+template <typename T> void AOTStreamedHeapWriter::mark_oop_pointer(T* buffered_addr, CHeapBitMap* oopmap) {\n+  \/\/ Mark the pointer in the oopmap\n+  size_t buffered_offset = buffered_address_to_offset((address)buffered_addr);\n+  BitMap::idx_t idx = bit_idx_for_buffer_offset(buffered_offset);\n+  oopmap->set_bit(idx);\n+}\n+\n+template <typename T> void AOTStreamedHeapWriter::map_oop_field_in_buffer(oop obj, T* field_addr_in_buffer, CHeapBitMap* oopmap) {\n+  if (obj == nullptr) {\n+    store_oop_in_buffer(field_addr_in_buffer, 0);\n+  } else {\n+    int dfs_index = *_dfs_order_table->get(obj);\n+    store_oop_in_buffer(field_addr_in_buffer, dfs_index);\n+  }\n+\n+  mark_oop_pointer<T>(field_addr_in_buffer, oopmap);\n+}\n+\n+void AOTStreamedHeapWriter::update_header_for_buffered_addr(address buffered_addr, oop src_obj,  Klass* src_klass) {\n+  assert(UseCompressedClassPointers, \"Archived heap only supported for compressed klasses\");\n+  narrowKlass nk = ArchiveBuilder::current()->get_requested_narrow_klass(src_klass);\n+\n+  markWord mw = markWord::prototype();\n+  oopDesc* fake_oop = (oopDesc*)buffered_addr;\n+\n+  \/\/ We need to retain the identity_hash, because it may have been used by some hashtables\n+  \/\/ in the shared heap. This also has the side effect of pre-initializing the\n+  \/\/ identity_hash for all shared objects, so they are less likely to be written\n+  \/\/ into during run time, increasing the potential of memory sharing.\n+  if (src_obj != nullptr) {\n+    if (UseCompactObjectHeaders) {\n+      mw = mw.copy_hashctrl_from(src_obj->mark());\n+      if (mw.is_hashed_not_expanded()) {\n+        mw = fake_oop->initialize_hash_if_necessary(src_obj, src_klass, mw);\n+      } else if (mw.is_not_hashed_expanded()) {\n+        \/\/ If a scratch mirror class has not been hashed until now, then reset its\n+        \/\/ hash bits to initial state.\n+        mw = mw.set_not_hashed_not_expanded();\n+      }\n+    } else {\n+      intptr_t src_hash = src_obj->identity_hash();\n+      mw = mw.copy_set_hash(src_hash);\n+    }\n+  }\n+\n+  if (is_interned_string(src_obj)) {\n+    \/\/ Mark the mark word of interned string so the loader knows to link these to\n+    \/\/ the string table at runtime.\n+    mw = mw.set_marked();\n+  }\n+\n+  if (UseCompactObjectHeaders) {\n+    fake_oop->set_mark(mw.set_narrow_klass(nk));\n+  } else {\n+    fake_oop->set_mark(mw);\n+    fake_oop->set_narrow_klass(nk);\n+  }\n+}\n+\n+class AOTStreamedHeapWriter::EmbeddedOopMapper: public BasicOopIterateClosure {\n+  oop _src_obj;\n+  address _buffered_obj;\n+  CHeapBitMap* _oopmap;\n+  bool _is_java_lang_ref;\n+\n+public:\n+  EmbeddedOopMapper(oop src_obj, address buffered_obj, CHeapBitMap* oopmap)\n+    : _src_obj(src_obj),\n+      _buffered_obj(buffered_obj),\n+      _oopmap(oopmap),\n+      _is_java_lang_ref(AOTReferenceObjSupport::check_if_ref_obj(src_obj)) {}\n+\n+  void do_oop(narrowOop *p) { EmbeddedOopMapper::do_oop_work(p); }\n+  void do_oop(      oop *p) { EmbeddedOopMapper::do_oop_work(p); }\n+\n+private:\n+  template <typename T>\n+  void do_oop_work(T *p) {\n+    size_t field_offset = pointer_delta(p, _src_obj, sizeof(char));\n+    oop obj = HeapShared::maybe_remap_referent(_is_java_lang_ref, field_offset, HeapAccess<>::oop_load(p));\n+    AOTStreamedHeapWriter::map_oop_field_in_buffer<T>(obj, (T*)(_buffered_obj + field_offset), _oopmap);\n+  }\n+};\n+\n+static void log_bitmap_usage(const char* which, BitMap* bitmap, size_t total_bits) {\n+  \/\/ The whole heap is covered by total_bits, but there are only non-zero bits within [start ... end).\n+  size_t start = bitmap->find_first_set_bit(0);\n+  size_t end = bitmap->size();\n+  log_info(aot)(\"%s = %7zu ... %7zu (%3zu%% ... %3zu%% = %3zu%%)\", which,\n+                start, end,\n+                start * 100 \/ total_bits,\n+                end * 100 \/ total_bits,\n+                (end - start) * 100 \/ total_bits);\n+}\n+\n+\/\/ Update all oop fields embedded in the buffered objects\n+void AOTStreamedHeapWriter::map_embedded_oops(ArchiveStreamedHeapInfo* heap_info) {\n+  size_t oopmap_unit = (UseCompressedOops ? sizeof(narrowOop) : sizeof(oop));\n+  size_t heap_region_byte_size = _buffer_used;\n+  heap_info->oopmap()->resize(heap_region_byte_size \/ oopmap_unit);\n+\n+  for (int i = 0; i < _source_objs->length(); i++) {\n+    oop src_obj = _source_objs->at(i);\n+    HeapShared::CachedOopInfo* info = HeapShared::get_cached_oop_info(src_obj);\n+    assert(info != nullptr, \"must be\");\n+    address buffered_obj = offset_to_buffered_address<address>(info->buffer_offset());\n+\n+    update_header_for_buffered_addr(buffered_obj, src_obj, src_obj->klass());\n+\n+    EmbeddedOopMapper mapper(src_obj, buffered_obj, heap_info->oopmap());\n+    src_obj->oop_iterate(&mapper);\n+    HeapShared::remap_dumped_metadata(src_obj, buffered_obj);\n+  };\n+\n+  size_t total_bytes = (size_t)_buffer->length();\n+  log_bitmap_usage(\"oopmap\", heap_info->oopmap(), total_bytes \/ oopmap_unit);\n+}\n+\n+size_t AOTStreamedHeapWriter::source_obj_to_buffered_offset(oop src_obj) {\n+  HeapShared::CachedOopInfo* p = HeapShared::get_cached_oop_info(src_obj);\n+  return p->buffer_offset();\n+}\n+\n+address AOTStreamedHeapWriter::source_obj_to_buffered_addr(oop src_obj) {\n+  return offset_to_buffered_address<address>(source_obj_to_buffered_offset(src_obj));\n+}\n+\n+oop AOTStreamedHeapWriter::buffered_offset_to_source_obj(size_t buffered_offset) {\n+  OopHandle* oh = _buffer_offset_to_source_obj_table->get(buffered_offset);\n+  if (oh != nullptr) {\n+    return oh->resolve();\n+  } else {\n+    return nullptr;\n+  }\n+}\n+\n+oop AOTStreamedHeapWriter::buffered_addr_to_source_obj(address buffered_addr) {\n+  return buffered_offset_to_source_obj(buffered_address_to_offset(buffered_addr));\n+}\n+\n+void AOTStreamedHeapWriter::populate_archive_heap_info(ArchiveStreamedHeapInfo* info) {\n+  assert(!info->is_used(), \"only set once\");\n+\n+  size_t heap_region_byte_size = _buffer_used;\n+  assert(heap_region_byte_size > 0, \"must archived at least one object!\");\n+\n+  info->set_buffer_region(MemRegion(offset_to_buffered_address<HeapWord*>(0),\n+                                    offset_to_buffered_address<HeapWord*>(_buffer_used)));\n+  info->set_roots_offset(_roots_offset);\n+  info->set_num_roots((size_t)HeapShared::pending_roots()->length());\n+  info->set_forwarding_offset(_forwarding_offset);\n+  info->set_root_highest_object_index_table_offset(_root_highest_object_index_table_offset);\n+  info->set_num_archived_objects((size_t)_source_objs->length());\n+}\n+\n+AOTMapLogger::OopDataIterator* AOTStreamedHeapWriter::oop_iterator(ArchiveStreamedHeapInfo* heap_info) {\n+  class StreamedWriterOopIterator : public AOTMapLogger::OopDataIterator {\n+  private:\n+    int _current;\n+    int _next;\n+\n+    address _buffer_start;\n+\n+    int _num_archived_objects;\n+    int _num_archived_roots;\n+    int* _roots;\n+\n+  public:\n+    StreamedWriterOopIterator(address buffer_start,\n+                              int num_archived_objects,\n+                              int num_archived_roots,\n+                              int* roots)\n+      : _current(0),\n+        _next(1),\n+        _buffer_start(buffer_start),\n+        _num_archived_objects(num_archived_objects),\n+        _num_archived_roots(num_archived_roots),\n+        _roots(roots) {\n+    }\n+\n+    AOTMapLogger::OopData capture(int dfs_index) {\n+      size_t buffered_offset = _dfs_to_archive_object_table[dfs_index];\n+      address buffered_addr = _buffer_start + buffered_offset;\n+      oop src_obj = AOTStreamedHeapWriter::buffered_offset_to_source_obj(buffered_offset);\n+      assert(src_obj != nullptr, \"why is this null?\");\n+      oopDesc* raw_oop = (oopDesc*)buffered_addr;\n+      Klass* klass = src_obj->klass();\n+      size_t size = src_obj->size();\n+      size = src_obj->copy_size_cds(size, src_obj->mark());\n+\n+      intptr_t target_location = (intptr_t)buffered_offset;\n+      uint32_t narrow_location = checked_cast<uint32_t>(dfs_index);\n+\n+      address requested_addr = (address)buffered_offset;\n+\n+      return { buffered_addr,\n+               requested_addr,\n+               target_location,\n+               narrow_location,\n+               raw_oop,\n+               klass,\n+               size,\n+               false };\n+    }\n+\n+    bool has_next() override {\n+      return _next <= _num_archived_objects;\n+    }\n+\n+    AOTMapLogger::OopData next() override {\n+      _current = _next;\n+      AOTMapLogger::OopData result = capture(_current);\n+      _next = _current + 1;\n+      return result;\n+    }\n+\n+    AOTMapLogger::OopData obj_at(narrowOop* addr) override {\n+      int dfs_index = (int)(*addr);\n+      if (dfs_index == 0) {\n+        return null_data();\n+      } else {\n+        return capture(dfs_index);\n+      }\n+    }\n+\n+    AOTMapLogger::OopData obj_at(oop* addr) override {\n+      int dfs_index = (int)cast_from_oop<uintptr_t>(*addr);\n+      if (dfs_index == 0) {\n+        return null_data();\n+      } else {\n+        return capture(dfs_index);\n+      }\n+    }\n+\n+    GrowableArrayCHeap<AOTMapLogger::OopData, mtClass>* roots() override {\n+      GrowableArrayCHeap<AOTMapLogger::OopData, mtClass>* result = new GrowableArrayCHeap<AOTMapLogger::OopData, mtClass>();\n+\n+      for (int i = 0; i < _num_archived_roots; ++i) {\n+        int object_index = _roots[i];\n+        result->append(capture(object_index));\n+      }\n+\n+      return result;\n+    }\n+  };\n+\n+  MemRegion r = heap_info->buffer_region();\n+  address buffer_start = address(r.start());\n+\n+  size_t roots_offset = heap_info->roots_offset();\n+  int* roots = ((int*)(buffer_start + roots_offset)) + 1;\n+\n+  return new StreamedWriterOopIterator(buffer_start, (int)heap_info->num_archived_objects(), (int)heap_info->num_roots(), roots);\n+}\n+\n+#endif \/\/ INCLUDE_CDS_JAVA_HEAP\n","filename":"src\/hotspot\/share\/cds\/aotStreamedHeapWriter.cpp","additions":628,"deletions":0,"binary":false,"changes":628,"status":"added"},{"patch":"@@ -29,0 +29,4 @@\n+#include \"cds\/aotMappedHeapLoader.hpp\"\n+#include \"cds\/aotMappedHeapWriter.hpp\"\n+#include \"cds\/aotMetaspace.hpp\"\n+#include \"cds\/aotOopChecker.hpp\"\n@@ -30,0 +34,2 @@\n+#include \"cds\/aotStreamedHeapLoader.hpp\"\n+#include \"cds\/aotStreamedHeapWriter.hpp\"\n@@ -31,2 +37,1 @@\n-#include \"cds\/archiveHeapLoader.hpp\"\n-#include \"cds\/archiveHeapWriter.hpp\"\n+#include \"cds\/cds_globals.hpp\"\n@@ -37,2 +42,1 @@\n-#include \"cds\/heapShared.hpp\"\n-#include \"cds\/metaspaceShared.hpp\"\n+#include \"cds\/heapShared.inline.hpp\"\n@@ -61,0 +65,1 @@\n+#include \"oops\/oopHandle.inline.hpp\"\n@@ -65,0 +70,1 @@\n+#include \"runtime\/globals_extension.hpp\"\n@@ -92,1 +98,51 @@\n-DumpedInternedStrings *HeapShared::_dumped_interned_strings = nullptr;\n+\/\/ Anything that goes in the header must be thoroughly purged from uninitialized memory\n+\/\/ as it will be written to disk. Therefore, the constructors memset the memory to 0.\n+\/\/ This is not the prettiest thing, but we need to know every byte is initialized,\n+\/\/ including potential padding between fields.\n+\n+ArchiveMappedHeapHeader::ArchiveMappedHeapHeader(size_t ptrmap_start_pos,\n+                                                 size_t oopmap_start_pos,\n+                                                 HeapRootSegments root_segments) {\n+  memset((char*)this, 0, sizeof(*this));\n+  _ptrmap_start_pos = ptrmap_start_pos;\n+  _oopmap_start_pos = oopmap_start_pos;\n+  _root_segments = root_segments;\n+}\n+\n+ArchiveMappedHeapHeader::ArchiveMappedHeapHeader() {\n+  memset((char*)this, 0, sizeof(*this));\n+}\n+\n+ArchiveMappedHeapHeader ArchiveMappedHeapInfo::create_header() {\n+  return ArchiveMappedHeapHeader{_ptrmap_start_pos,\n+                                 _oopmap_start_pos,\n+                                 _root_segments};\n+}\n+\n+ArchiveStreamedHeapHeader::ArchiveStreamedHeapHeader(size_t forwarding_offset,\n+                                                     size_t roots_offset,\n+                                                     size_t num_roots,\n+                                                     size_t root_highest_object_index_table_offset,\n+                                                     size_t num_archived_objects) {\n+  memset((char*)this, 0, sizeof(*this));\n+  _forwarding_offset = forwarding_offset;\n+  _roots_offset = roots_offset;\n+  _num_roots = num_roots;\n+  _root_highest_object_index_table_offset = root_highest_object_index_table_offset;\n+  _num_archived_objects = num_archived_objects;\n+}\n+\n+ArchiveStreamedHeapHeader::ArchiveStreamedHeapHeader() {\n+  memset((char*)this, 0, sizeof(*this));\n+}\n+\n+ArchiveStreamedHeapHeader ArchiveStreamedHeapInfo::create_header() {\n+  return ArchiveStreamedHeapHeader{_forwarding_offset,\n+                                   _roots_offset,\n+                                   _num_roots,\n+                                   _root_highest_object_index_table_offset,\n+                                   _num_archived_objects};\n+}\n+\n+HeapArchiveMode HeapShared::_heap_load_mode = HeapArchiveMode::_uninitialized;\n+HeapArchiveMode HeapShared::_heap_write_mode = HeapArchiveMode::_uninitialized;\n@@ -143,2 +199,0 @@\n-GrowableArrayCHeap<OopHandle, mtClassShared>* HeapShared::_root_segments = nullptr;\n-int HeapShared::_root_segment_max_size_elems;\n@@ -162,0 +216,4 @@\n+oop HeapShared::CachedOopInfo::orig_referrer() const {\n+  return _orig_referrer.resolve();\n+}\n+\n@@ -163,0 +221,2 @@\n+  assert(SafepointSynchronize::is_at_safepoint() ||\n+         JavaThread::current()->is_in_no_safepoint_scope(), \"sanity\");\n@@ -168,0 +228,17 @@\n+unsigned int HeapShared::oop_handle_hash_raw(const OopHandle& oh) {\n+  return oop_hash(oh.resolve());\n+}\n+\n+unsigned int HeapShared::oop_handle_hash(const OopHandle& oh) {\n+  oop o = oh.resolve();\n+  if (o == nullptr) {\n+    return 0;\n+  } else {\n+    return o->identity_hash();\n+  }\n+}\n+\n+bool HeapShared::oop_handle_equals(const OopHandle& a, const OopHandle& b) {\n+  return a.resolve() == b.resolve();\n+}\n+\n@@ -186,1 +263,1 @@\n-    klass = klass->java_super();\n+    klass = klass->super();\n@@ -217,3 +294,10 @@\n-bool HeapShared::has_been_archived(oop obj) {\n-  assert(CDSConfig::is_dumping_heap(), \"dump-time only\");\n-  return archived_object_cache()->get(obj) != nullptr;\n+bool HeapShared::is_archived_heap_in_use() {\n+  if (HeapShared::is_loading()) {\n+    if (HeapShared::is_loading_streaming_mode()) {\n+      return AOTStreamedHeapLoader::is_in_use();\n+    } else {\n+      return AOTMappedHeapLoader::is_in_use();\n+    }\n+  }\n+\n+  return false;\n@@ -222,4 +306,13 @@\n-int HeapShared::append_root(oop obj) {\n-  assert(CDSConfig::is_dumping_heap(), \"dump-time only\");\n-  if (obj != nullptr) {\n-    assert(has_been_archived(obj), \"must be\");\n+bool HeapShared::can_use_archived_heap() {\n+  FileMapInfo* static_mapinfo = FileMapInfo::current_info();\n+  if (static_mapinfo == nullptr) {\n+    return false;\n+  }\n+  if (!static_mapinfo->has_heap_region()) {\n+    return false;\n+  }\n+  if (!static_mapinfo->object_streaming_mode() &&\n+      !Universe::heap()->can_load_archived_objects() &&\n+      !UseG1GC) {\n+    \/\/ Incompatible object format\n+    return false;\n@@ -227,3 +320,1 @@\n-  \/\/ No GC should happen since we aren't scanning _pending_roots.\n-  assert(Thread::current() == (Thread*)VMThread::vm_thread(), \"should be in vm thread\");\n-  return _pending_roots->append(obj);\n+  return true;\n@@ -233,3 +324,3 @@\n-objArrayOop HeapShared::root_segment(int segment_idx) {\n-  if (CDSConfig::is_dumping_heap()) {\n-    assert(Thread::current() == (Thread*)VMThread::vm_thread(), \"should be in vm thread\");\n+bool HeapShared::is_too_large_to_archive(size_t size) {\n+  if (HeapShared::is_writing_streaming_mode()) {\n+    return false;\n@@ -237,1 +328,1 @@\n-    assert(CDSConfig::is_using_archive(), \"must be\");\n+    return AOTMappedHeapWriter::is_too_large_to_archive(size);\n@@ -239,0 +330,1 @@\n+}\n@@ -240,3 +332,6 @@\n-  objArrayOop segment = (objArrayOop)_root_segments->at(segment_idx).resolve();\n-  assert(segment != nullptr, \"should have been initialized\");\n-  return segment;\n+bool HeapShared::is_too_large_to_archive(oop obj) {\n+  if (HeapShared::is_writing_streaming_mode()) {\n+    return false;\n+  } else {\n+    return AOTMappedHeapWriter::is_too_large_to_archive(obj);\n+  }\n@@ -245,2 +340,4 @@\n-void HeapShared::get_segment_indexes(int idx, int& seg_idx, int& int_idx) {\n-  assert(_root_segment_max_size_elems > 0, \"sanity\");\n+bool HeapShared::is_string_too_large_to_archive(oop string) {\n+  typeArrayOop value = java_lang_String::value_no_keepalive(string);\n+  return is_too_large_to_archive(value);\n+}\n@@ -248,7 +345,96 @@\n-  \/\/ Try to avoid divisions for the common case.\n-  if (idx < _root_segment_max_size_elems) {\n-    seg_idx = 0;\n-    int_idx = idx;\n-  } else {\n-    seg_idx = idx \/ _root_segment_max_size_elems;\n-    int_idx = idx % _root_segment_max_size_elems;\n+void HeapShared::initialize_loading_mode(HeapArchiveMode mode) {\n+  assert(_heap_load_mode == HeapArchiveMode::_uninitialized, \"already set?\");\n+  assert(mode != HeapArchiveMode::_uninitialized, \"sanity\");\n+  _heap_load_mode = mode;\n+};\n+\n+void HeapShared::initialize_writing_mode() {\n+  assert(!FLAG_IS_ERGO(AOTStreamableObjects), \"Should not have been ergonomically set yet\");\n+\n+  if (!CDSConfig::is_dumping_archive()) {\n+    \/\/ We use FLAG_IS_CMDLINE below because we are specifically looking to warn\n+    \/\/ a user that explicitly sets the flag on the command line for a JVM that is\n+    \/\/ not dumping an archive.\n+    if (FLAG_IS_CMDLINE(AOTStreamableObjects)) {\n+      log_warning(cds)(\"-XX:%cAOTStreamableObjects was specified, \"\n+                       \"AOTStreamableObjects is only used for writing \"\n+                       \"the AOT cache.\",\n+                       AOTStreamableObjects ? '+' : '-');\n+    }\n+  }\n+\n+  \/\/ The below checks use !FLAG_IS_DEFAULT instead of FLAG_IS_CMDLINE\n+  \/\/ because the one step AOT cache creation transfers the AOTStreamableObjects\n+  \/\/ flag value from the training JVM to the assembly JVM using an environment\n+  \/\/ variable that sets the flag as ERGO in the assembly JVM.\n+  if (FLAG_IS_DEFAULT(AOTStreamableObjects)) {\n+    \/\/ By default, the value of AOTStreamableObjects should match !UseCompressedOops.\n+    FLAG_SET_DEFAULT(AOTStreamableObjects, !UseCompressedOops);\n+  } else if (!AOTStreamableObjects && UseZGC) {\n+    \/\/ Never write mapped heap with ZGC\n+    if (CDSConfig::is_dumping_archive()) {\n+      log_warning(cds)(\"Heap archiving without streaming not supported for -XX:+UseZGC\");\n+    }\n+    FLAG_SET_ERGO(AOTStreamableObjects, true);\n+  }\n+\n+  if (CDSConfig::is_dumping_archive()) {\n+    \/\/ Select default mode\n+    assert(_heap_write_mode == HeapArchiveMode::_uninitialized, \"already initialized?\");\n+    _heap_write_mode = AOTStreamableObjects ? HeapArchiveMode::_streaming : HeapArchiveMode::_mapping;\n+  }\n+}\n+\n+void HeapShared::initialize_streaming() {\n+  assert(is_loading_streaming_mode(), \"shouldn't call this\");\n+  if (can_use_archived_heap()) {\n+    AOTStreamedHeapLoader::initialize();\n+  }\n+}\n+\n+void HeapShared::enable_gc() {\n+  if (AOTStreamedHeapLoader::is_in_use()) {\n+    AOTStreamedHeapLoader::enable_gc();\n+  }\n+}\n+\n+void HeapShared::materialize_thread_object() {\n+  if (AOTStreamedHeapLoader::is_in_use()) {\n+    AOTStreamedHeapLoader::materialize_thread_object();\n+  }\n+}\n+\n+void HeapShared::add_to_dumped_interned_strings(oop string) {\n+  assert(HeapShared::is_writing_mapping_mode(), \"Only used by this mode\");\n+  AOTMappedHeapWriter::add_to_dumped_interned_strings(string);\n+}\n+\n+void HeapShared::finalize_initialization(FileMapInfo* static_mapinfo) {\n+  if (HeapShared::is_loading()) {\n+    if (HeapShared::is_loading_streaming_mode()) {\n+      \/\/ Heap initialization can be done only after vtables are initialized by ReadClosure.\n+      AOTStreamedHeapLoader::finish_initialization(static_mapinfo);\n+    } else {\n+      \/\/ Finish up archived heap initialization. These must be\n+      \/\/ done after ReadClosure.\n+      AOTMappedHeapLoader::finish_initialization(static_mapinfo);\n+    }\n+  }\n+}\n+\n+HeapShared::CachedOopInfo* HeapShared::get_cached_oop_info(oop obj) {\n+  OopHandle oh(Universe::vm_global(), obj);\n+  CachedOopInfo* result = _archived_object_cache->get(oh);\n+  oh.release(Universe::vm_global());\n+  return result;\n+}\n+\n+bool HeapShared::has_been_archived(oop obj) {\n+  assert(CDSConfig::is_dumping_heap(), \"dump-time only\");\n+  return get_cached_oop_info(obj) != nullptr;\n+}\n+\n+int HeapShared::append_root(oop obj) {\n+  assert(CDSConfig::is_dumping_heap(), \"dump-time only\");\n+  if (obj != nullptr) {\n+    assert(has_been_archived(obj), \"must be\");\n@@ -256,0 +442,2 @@\n+  \/\/ No GC should happen since we aren't scanning _pending_roots.\n+  assert(Thread::current() == (Thread*)VMThread::vm_thread(), \"should be in vm thread\");\n@@ -257,2 +445,1 @@\n-  assert(idx == seg_idx * _root_segment_max_size_elems + int_idx,\n-         \"sanity: %d index maps to %d segment and %d internal\", idx, seg_idx, int_idx);\n+  return _pending_roots->append(obj);\n@@ -261,1 +448,0 @@\n-\/\/ Returns an objArray that contains all the roots of the archived objects\n@@ -265,4 +451,10 @@\n-  assert(!_root_segments->is_empty(), \"must have loaded shared heap\");\n-  int seg_idx, int_idx;\n-  get_segment_indexes(index, seg_idx, int_idx);\n-  oop result = root_segment(seg_idx)->obj_at(int_idx);\n+  assert(is_archived_heap_in_use(), \"getting roots into heap that is not used\");\n+\n+  oop result;\n+  if (HeapShared::is_loading_streaming_mode()) {\n+    result = AOTStreamedHeapLoader::get_root(index);\n+  } else {\n+    assert(HeapShared::is_loading_mapping_mode(), \"must be\");\n+    result = AOTMappedHeapLoader::get_root(index);\n+  }\n+\n@@ -272,0 +464,1 @@\n+\n@@ -275,0 +468,6 @@\n+void HeapShared::finish_materialize_objects() {\n+  if (AOTStreamedHeapLoader::is_in_use()) {\n+    AOTStreamedHeapLoader::finish_materialize_objects();\n+  }\n+}\n+\n@@ -278,3 +477,1 @@\n-  if (ArchiveHeapLoader::is_in_use()) {\n-    int seg_idx, int_idx;\n-    get_segment_indexes(index, seg_idx, int_idx);\n+  if (is_archived_heap_in_use()) {\n@@ -282,2 +479,7 @@\n-      oop old = root_segment(seg_idx)->obj_at(int_idx);\n-      log_debug(aot, heap)(\"Clearing root %d: was \" PTR_FORMAT, index, p2i(old));\n+      log_debug(aot, heap)(\"Clearing root %d: was %zu\", index, p2i(get_root(index, false \/* clear *\/)));\n+    }\n+    if (HeapShared::is_loading_streaming_mode()) {\n+      AOTStreamedHeapLoader::clear_root(index);\n+    } else {\n+      assert(HeapShared::is_loading_mapping_mode(), \"must be\");\n+      AOTMappedHeapLoader::clear_root(index);\n@@ -285,1 +487,0 @@\n-    root_segment(seg_idx)->obj_at_put(int_idx, nullptr);\n@@ -297,1 +498,1 @@\n-  if (ArchiveHeapWriter::is_too_large_to_archive(obj->size())) {\n+  if (is_too_large_to_archive(obj)) {\n@@ -302,0 +503,7 @@\n+  }\n+\n+  AOTOopChecker::check(obj); \/\/ Make sure contents of this oop are safe.\n+  count_allocation(obj->size());\n+\n+  if (HeapShared::is_writing_streaming_mode()) {\n+    AOTStreamedHeapWriter::add_source_obj(obj);\n@@ -303,28 +511,2 @@\n-    count_allocation(obj->size());\n-    ArchiveHeapWriter::add_source_obj(obj);\n-    CachedOopInfo info = make_cached_oop_info(obj, referrer);\n-    archived_object_cache()->put_when_absent(obj, info);\n-    archived_object_cache()->maybe_grow();\n-    mark_native_pointers(obj);\n-\n-    Klass* k = obj->klass();\n-    if (k->is_instance_klass()) {\n-      \/\/ Whenever we see a non-array Java object of type X, we mark X to be aot-initialized.\n-      \/\/ This ensures that during the production run, whenever Java code sees a cached object\n-      \/\/ of type X, we know that X is already initialized. (see TODO comment below ...)\n-\n-      if (InstanceKlass::cast(k)->is_enum_subclass()\n-          \/\/ We can't rerun <clinit> of enum classes (see cdsEnumKlass.cpp) so\n-          \/\/ we must store them as AOT-initialized.\n-          || (subgraph_info == _dump_time_special_subgraph))\n-          \/\/ TODO: we do this only for the special subgraph for now. Extending this to\n-          \/\/ other subgraphs would require more refactoring of the core library (such as\n-          \/\/ move some initialization logic into runtimeSetup()).\n-          \/\/\n-          \/\/ For the other subgraphs, we have a weaker mechanism to ensure that\n-          \/\/ all classes in a subgraph are initialized before the subgraph is programmatically\n-          \/\/ returned from jdk.internal.misc.CDS::initializeFromArchive().\n-          \/\/ See HeapShared::initialize_from_archived_subgraph().\n-      {\n-        AOTArtifactFinder::add_aot_inited_class(InstanceKlass::cast(k));\n-      }\n+    AOTMappedHeapWriter::add_source_obj(obj);\n+  }\n@@ -332,13 +514,37 @@\n-      if (java_lang_Class::is_instance(obj)) {\n-        Klass* mirror_k = java_lang_Class::as_Klass(obj);\n-        if (mirror_k != nullptr) {\n-          AOTArtifactFinder::add_cached_class(mirror_k);\n-        }\n-      } else if (java_lang_invoke_ResolvedMethodName::is_instance(obj)) {\n-        Method* m = java_lang_invoke_ResolvedMethodName::vmtarget(obj);\n-        if (m != nullptr) {\n-          if (RegeneratedClasses::has_been_regenerated(m)) {\n-            m = RegeneratedClasses::get_regenerated_object(m);\n-          }\n-          InstanceKlass* method_holder = m->method_holder();\n-          AOTArtifactFinder::add_cached_class(method_holder);\n+  OopHandle oh(Universe::vm_global(), obj);\n+  CachedOopInfo info = make_cached_oop_info(obj, referrer);\n+  archived_object_cache()->put_when_absent(oh, info);\n+  archived_object_cache()->maybe_grow();\n+\n+  Klass* k = obj->klass();\n+  if (k->is_instance_klass()) {\n+    \/\/ Whenever we see a non-array Java object of type X, we mark X to be aot-initialized.\n+    \/\/ This ensures that during the production run, whenever Java code sees a cached object\n+    \/\/ of type X, we know that X is already initialized. (see TODO comment below ...)\n+\n+    if (InstanceKlass::cast(k)->is_enum_subclass()\n+        \/\/ We can't rerun <clinit> of enum classes (see cdsEnumKlass.cpp) so\n+        \/\/ we must store them as AOT-initialized.\n+        || (subgraph_info == _dump_time_special_subgraph))\n+        \/\/ TODO: we do this only for the special subgraph for now. Extending this to\n+        \/\/ other subgraphs would require more refactoring of the core library (such as\n+        \/\/ move some initialization logic into runtimeSetup()).\n+        \/\/\n+        \/\/ For the other subgraphs, we have a weaker mechanism to ensure that\n+        \/\/ all classes in a subgraph are initialized before the subgraph is programmatically\n+        \/\/ returned from jdk.internal.misc.CDS::initializeFromArchive().\n+        \/\/ See HeapShared::initialize_from_archived_subgraph().\n+    {\n+      AOTArtifactFinder::add_aot_inited_class(InstanceKlass::cast(k));\n+    }\n+\n+    if (java_lang_Class::is_instance(obj)) {\n+      Klass* mirror_k = java_lang_Class::as_Klass(obj);\n+      if (mirror_k != nullptr) {\n+        AOTArtifactFinder::add_cached_class(mirror_k);\n+      }\n+    } else if (java_lang_invoke_ResolvedMethodName::is_instance(obj)) {\n+      Method* m = java_lang_invoke_ResolvedMethodName::vmtarget(obj);\n+      if (m != nullptr) {\n+        if (RegeneratedClasses::has_been_regenerated(m)) {\n+          m = RegeneratedClasses::get_regenerated_object(m);\n@@ -346,0 +552,2 @@\n+        InstanceKlass* method_holder = m->method_holder();\n+        AOTArtifactFinder::add_cached_class(method_holder);\n@@ -348,0 +556,1 @@\n+  }\n@@ -349,13 +558,12 @@\n-    if (log_is_enabled(Debug, aot, heap)) {\n-      ResourceMark rm;\n-      LogTarget(Debug, aot, heap) log;\n-      LogStream out(log);\n-      out.print(\"Archived heap object \" PTR_FORMAT \" : %s \",\n-                p2i(obj), obj->klass()->external_name());\n-      if (java_lang_Class::is_instance(obj)) {\n-        Klass* k = java_lang_Class::as_Klass(obj);\n-        if (k != nullptr) {\n-          out.print(\"%s\", k->external_name());\n-        } else {\n-          out.print(\"primitive\");\n-        }\n+  if (log_is_enabled(Debug, aot, heap)) {\n+    ResourceMark rm;\n+    LogTarget(Debug, aot, heap) log;\n+    LogStream out(log);\n+    out.print(\"Archived heap object \" PTR_FORMAT \" : %s \",\n+              p2i(obj), obj->klass()->external_name());\n+    if (java_lang_Class::is_instance(obj)) {\n+      Klass* k = java_lang_Class::as_Klass(obj);\n+      if (k != nullptr) {\n+        out.print(\"%s\", k->external_name());\n+      } else {\n+        out.print(\"primitive\");\n@@ -363,3 +571,1 @@\n-      out.cr();\n-\n-    return true;\n+    out.cr();\n@@ -368,0 +574,2 @@\n+\n+  return true;\n@@ -370,1 +578,1 @@\n-class MetaspaceObjToOopHandleTable: public ResourceHashtable<MetaspaceObj*, OopHandle,\n+class MetaspaceObjToOopHandleTable: public HashTable<MetaspaceObj*, OopHandle,\n@@ -410,3 +618,3 @@\n-void HeapShared::init_dumping() {\n-  _scratch_objects_table = new (mtClass)MetaspaceObjToOopHandleTable();\n-  _pending_roots = new GrowableArrayCHeap<oop, mtClassShared>(500);\n+ void HeapShared::init_dumping() {\n+   _scratch_objects_table = new (mtClass)MetaspaceObjToOopHandleTable();\n+   _pending_roots = new GrowableArrayCHeap<oop, mtClassShared>(500);\n@@ -426,3 +634,2 @@\n-\/\/ return the \"scratch\" version that represents the same type T.\n-\/\/ Note that if java_mirror will be returned if it's already a\n-\/\/ scratch mirror.\n+\/\/ return the \"scratch\" version that represents the same type T. Note\n+\/\/ that java_mirror will be returned if the mirror is already a scratch mirror.\n@@ -508,3 +715,0 @@\n-  if (AOTClassInitializer::is_runtime_setup_required(ik)) {\n-    ik->set_is_runtime_setup_required();\n-  }\n@@ -591,1 +795,1 @@\n-static void copy_java_mirror_hashcode(oop orig_mirror, oop scratch_m) {\n+void HeapShared::copy_java_mirror(oop orig_mirror, oop scratch_m) {\n@@ -595,0 +799,1 @@\n+  assert(!UseCompactObjectHeaders || !orig_mirror->mark().is_not_hashed_expanded(), \"must not be not-hashed-expanded\");\n@@ -606,1 +811,1 @@\n-        scratch_m->initialize_hash_if_necessary(orig_mirror, orig_klass, mark);\n+        scratch_m->set_mark(scratch_m->initialize_hash_if_necessary(orig_mirror, orig_klass, mark));\n@@ -610,1 +815,1 @@\n-        assert(offset >= 8, \"hash offset must not be in header\");\n+        assert(offset >= 4, \"hash offset must not be in header\");\n@@ -622,0 +827,5 @@\n+\n+  if (CDSConfig::is_dumping_aot_linked_classes()) {\n+    java_lang_Class::set_module(scratch_m, java_lang_Class::module(orig_mirror));\n+    java_lang_Class::set_protection_domain(scratch_m, java_lang_Class::protection_domain(orig_mirror));\n+  }\n@@ -628,1 +838,1 @@\n-    if (rr != nullptr && !ArchiveHeapWriter::is_too_large_to_archive(rr)) {\n+    if (rr != nullptr && !HeapShared::is_too_large_to_archive(rr)) {\n@@ -636,0 +846,1 @@\n+  assert(HeapShared::is_writing_mapping_mode(), \"should not reach here\");\n@@ -648,10 +859,2 @@\n-void HeapShared::mark_native_pointers(oop orig_obj) {\n-  if (java_lang_Class::is_instance(orig_obj)) {\n-    ArchiveHeapWriter::mark_native_pointer(orig_obj, java_lang_Class::klass_offset());\n-    ArchiveHeapWriter::mark_native_pointer(orig_obj, java_lang_Class::array_klass_offset());\n-  } else if (java_lang_invoke_ResolvedMethodName::is_instance(orig_obj)) {\n-    ArchiveHeapWriter::mark_native_pointer(orig_obj, java_lang_invoke_ResolvedMethodName::vmtarget_offset());\n-  }\n-}\n-\n-  CachedOopInfo* info = archived_object_cache()->get(src_obj);\n+  OopHandle oh(&src_obj);\n+  CachedOopInfo* info = archived_object_cache()->get(oh);\n@@ -665,1 +868,2 @@\n-  CachedOopInfo* info = archived_object_cache()->get(src_obj);\n+  OopHandle oh(&src_obj);\n+  CachedOopInfo* info = archived_object_cache()->get(oh);\n@@ -683,1 +887,1 @@\n-    if (UseCompressedOops || UseG1GC) {\n+    if (HeapShared::is_writing_mapping_mode() && (UseG1GC || UseCompressedOops)) {\n@@ -699,1 +903,3 @@\n-  archive_strings();\n+  if (is_writing_mapping_mode()) {\n+    archive_strings();\n+  }\n@@ -703,1 +909,1 @@\n-void HeapShared::write_heap(ArchiveHeapInfo *heap_info) {\n+void HeapShared::write_heap(ArchiveMappedHeapInfo* mapped_heap_info, ArchiveStreamedHeapInfo* streamed_heap_info) {\n@@ -710,2 +916,7 @@\n-  StringTable::write_shared_table();\n-  ArchiveHeapWriter::write(_pending_roots, heap_info);\n+  if (HeapShared::is_writing_mapping_mode()) {\n+    StringTable::write_shared_table();\n+    AOTMappedHeapWriter::write(_pending_roots, mapped_heap_info);\n+  } else {\n+    assert(HeapShared::is_writing_streaming_mode(), \"are there more modes?\");\n+    AOTStreamedHeapWriter::write(_pending_roots, streamed_heap_info);\n+  }\n@@ -720,1 +931,1 @@\n-    copy_java_mirror_hashcode(orig_mirror, m);\n+    copy_java_mirror(orig_mirror, m);\n@@ -895,1 +1106,1 @@\n-  MetaspaceShared::unrecoverable_writing_error();\n+  AOTMetaspace::unrecoverable_writing_error();\n@@ -1034,1 +1245,1 @@\n-  CompactHashtableWriter writer(d_table->_count, &stats);\n+  CompactHashtableWriter writer(d_table->number_of_entries(), &stats);\n@@ -1052,13 +1263,0 @@\n-void HeapShared::add_root_segment(objArrayOop segment_oop) {\n-  assert(segment_oop != nullptr, \"must be\");\n-  assert(ArchiveHeapLoader::is_in_use(), \"must be\");\n-  if (_root_segments == nullptr) {\n-    _root_segments = new GrowableArrayCHeap<OopHandle, mtClassShared>(10);\n-  }\n-  _root_segments->push(OopHandle(Universe::vm_global(), segment_oop));\n-}\n-\n-void HeapShared::init_root_segment_sizes(int max_size_elems) {\n-  _root_segment_max_size_elems = max_size_elems;\n-}\n-\n@@ -1085,4 +1283,4 @@\n-    VM_Verify verify_op;\n-    VMThread::execute(&verify_op);\n-\n-    if (VerifyArchivedFields > 1 && is_init_completed()) {\n+    if (VerifyArchivedFields == 1) {\n+      VM_Verify verify_op;\n+      VMThread::execute(&verify_op);\n+    } else if (VerifyArchivedFields == 2 && is_init_completed()) {\n@@ -1114,1 +1312,1 @@\n-  if (!ArchiveHeapLoader::is_in_use()) {\n+  if (!is_archived_heap_in_use()) {\n@@ -1173,1 +1371,1 @@\n-  if (!ArchiveHeapLoader::is_in_use()) {\n+  if (!is_archived_heap_in_use()) {\n@@ -1205,1 +1403,1 @@\n-  if (!ArchiveHeapLoader::is_in_use()) {\n+  if (!is_archived_heap_in_use()) {\n@@ -1242,1 +1440,1 @@\n-  if (!k->is_shared()) {\n+  if (!k->in_aot_cache()) {\n@@ -1296,1 +1494,1 @@\n-        if (!klass->is_shared()) {\n+        if (!klass->in_aot_cache()) {\n@@ -1341,3 +1539,0 @@\n-  \/\/ Load the subgraph entry fields from the record and store them back to\n-  \/\/ the corresponding fields within the mirror.\n-  oop m = k->java_mirror();\n@@ -1351,0 +1546,2 @@\n+      \/\/ Load the subgraph entry fields from the record and store them back to\n+      \/\/ the corresponding fields within the mirror.\n@@ -1352,0 +1549,1 @@\n+      oop m = k->java_mirror();\n@@ -1430,1 +1628,1 @@\n-    if (!CompressedOops::is_null(obj)) {\n+    if (obj != nullptr) {\n@@ -1475,1 +1673,1 @@\n-  return CachedOopInfo(referrer, points_to_oops_checker.result());\n+  return CachedOopInfo(OopHandle(Universe::vm_global(), referrer), points_to_oops_checker.result());\n@@ -1479,1 +1677,1 @@\n-  if (ArchiveHeapLoader::is_in_use()) {\n+  if (is_archived_heap_in_use()) {\n@@ -1531,1 +1729,1 @@\n-    MetaspaceShared::unrecoverable_writing_error();\n+    AOTMetaspace::unrecoverable_writing_error();\n@@ -1584,1 +1782,1 @@\n-      MetaspaceShared::unrecoverable_writing_error();\n+      AOTMetaspace::unrecoverable_writing_error();\n@@ -1614,1 +1812,1 @@\n-        MetaspaceShared::unrecoverable_writing_error();\n+        AOTMetaspace::unrecoverable_writing_error();\n@@ -1631,2 +1829,2 @@\n-    \/\/ The enum klasses are archived with aot-initialized mirror.\n-    \/\/ See AOTClassInitializer::can_archive_initialized_mirror().\n+    \/\/ The classes of all archived enum instances have been marked as aot-init,\n+    \/\/ so there's nothing else to be done in the production run.\n@@ -1634,0 +1832,2 @@\n+    \/\/ This is legacy support for enum classes before JEP 483 -- we cannot rerun\n+    \/\/ the enum's <clinit> in the production run, so special handling is needed.\n@@ -1722,2 +1922,2 @@\n-    oop obj = RawAccess<>::oop_load(p);\n-    if (!CompressedOops::is_null(obj)) {\n+    oop obj = HeapAccess<>::oop_load(p);\n+    if (obj != nullptr) {\n@@ -1792,1 +1992,2 @@\n-          name != vmSymbols::java_lang_NullPointerException()) {\n+          name != vmSymbols::java_lang_NullPointerException() &&\n+          name != vmSymbols::jdk_internal_vm_PreemptedException()) {\n@@ -1802,3 +2003,3 @@\n-int HeapShared::_num_new_walked_objs;\n-int HeapShared::_num_new_archived_objs;\n-int HeapShared::_num_old_recorded_klasses;\n+size_t HeapShared::_num_new_walked_objs;\n+size_t HeapShared::_num_new_archived_objs;\n+size_t HeapShared::_num_old_recorded_klasses;\n@@ -1806,5 +2007,5 @@\n-int HeapShared::_num_total_subgraph_recordings = 0;\n-int HeapShared::_num_total_walked_objs = 0;\n-int HeapShared::_num_total_archived_objs = 0;\n-int HeapShared::_num_total_recorded_klasses = 0;\n-int HeapShared::_num_total_verifications = 0;\n+size_t HeapShared::_num_total_subgraph_recordings = 0;\n+size_t HeapShared::_num_total_walked_objs = 0;\n+size_t HeapShared::_num_total_archived_objs = 0;\n+size_t HeapShared::_num_total_recorded_klasses = 0;\n+size_t HeapShared::_num_total_verifications = 0;\n@@ -1833,1 +2034,1 @@\n-  int num_new_recorded_klasses = get_subgraph_info(k)->num_subgraph_object_klasses() -\n+  size_t num_new_recorded_klasses = get_subgraph_info(k)->num_subgraph_object_klasses() -\n@@ -1836,1 +2037,1 @@\n-                      \"walked %d objs, archived %d new objs, recorded %d classes\",\n+                      \"walked %zu objs, archived %zu new objs, recorded %zu classes\",\n@@ -2023,1 +2224,1 @@\n-  if (k != nullptr && ArchiveHeapLoader::is_in_use()) {\n+  if (k != nullptr && is_archived_heap_in_use()) {\n@@ -2044,1 +2245,0 @@\n-    _dumped_interned_strings = new (mtClass)DumpedInternedStrings(INITIAL_TABLE_SIZE, MAX_TABLE_SIZE);\n@@ -2049,0 +2249,8 @@\n+void HeapShared::init_heap_writer() {\n+  if (HeapShared::is_writing_streaming_mode()) {\n+    AOTStreamedHeapWriter::init();\n+  } else {\n+    AOTMappedHeapWriter::init();\n+  }\n+}\n+\n@@ -2084,1 +2292,1 @@\n-  log_info(aot, heap)(\"Archived subgraph records = %d\",\n+  log_info(aot, heap)(\"Archived subgraph records = %zu\",\n@@ -2086,3 +2294,3 @@\n-  log_info(aot, heap)(\"  Walked %d objects\", _num_total_walked_objs);\n-  log_info(aot, heap)(\"  Archived %d objects\", _num_total_archived_objs);\n-  log_info(aot, heap)(\"  Recorded %d klasses\", _num_total_recorded_klasses);\n+  log_info(aot, heap)(\"  Walked %zu objects\", _num_total_walked_objs);\n+  log_info(aot, heap)(\"  Archived %zu objects\", _num_total_archived_objs);\n+  log_info(aot, heap)(\"  Recorded %zu klasses\", _num_total_recorded_klasses);\n@@ -2095,1 +2303,1 @@\n-  log_info(aot, heap)(\"  Verified %d references\", _num_total_verifications);\n+  log_info(aot, heap)(\"  Verified %zu references\", _num_total_verifications);\n@@ -2099,12 +2307,5 @@\n-\/\/ Keep track of the contents of the archived interned string table. This table\n-\/\/ is used only by CDSHeapVerifier.\n-void HeapShared::add_to_dumped_interned_strings(oop string) {\n-  assert_at_safepoint(); \/\/ DumpedInternedStrings uses raw oops\n-  assert(!ArchiveHeapWriter::is_string_too_large_to_archive(string), \"must be\");\n-  bool created;\n-  _dumped_interned_strings->put_if_absent(string, true, &created);\n-  if (created) {\n-    \/\/ Prevent string deduplication from changing the value field to\n-    \/\/ something not in the archive.\n-    java_lang_String::set_deduplication_forbidden(string);\n-    _dumped_interned_strings->maybe_grow();\n+bool HeapShared::is_dumped_interned_string(oop o) {\n+  if (is_writing_mapping_mode()) {\n+    return AOTMappedHeapWriter::is_dumped_interned_string(o);\n+  } else {\n+    return AOTStreamedHeapWriter::is_dumped_interned_string(o);\n@@ -2114,2 +2315,12 @@\n-bool HeapShared::is_dumped_interned_string(oop o) {\n-  return _dumped_interned_strings->get(o) != nullptr;\n+\/\/ These tables should be used only within the CDS safepoint, so\n+\/\/ delete them before we exit the safepoint. Otherwise the table will\n+\/\/ contain bad oops after a GC.\n+void HeapShared::delete_tables_with_raw_oops() {\n+  assert(_seen_objects_table == nullptr, \"should have been deleted\");\n+\n+  if (is_writing_mapping_mode()) {\n+    AOTMappedHeapWriter::delete_tables_with_raw_oops();\n+  } else {\n+    assert(is_writing_streaming_mode(), \"what other mode?\");\n+    AOTStreamedHeapWriter::delete_tables_with_raw_oops();\n+  }\n@@ -2135,2 +2346,2 @@\n-  int _num_total_oops;\n-  int _num_null_oops;\n+  size_t _num_total_oops;\n+  size_t _num_null_oops;\n@@ -2162,2 +2373,2 @@\n-  int num_total_oops() const { return _num_total_oops; }\n-  int num_null_oops()  const { return _num_null_oops; }\n+  size_t num_total_oops() const { return _num_total_oops; }\n+  size_t num_null_oops()  const { return _num_null_oops; }\n@@ -2212,0 +2423,29 @@\n+bool HeapShared::is_metadata_field(oop src_obj, int offset) {\n+  bool result = false;\n+  do_metadata_offsets(src_obj, [&](int metadata_offset) {\n+    if (metadata_offset == offset) {\n+      result = true;\n+    }\n+  });\n+  return result;\n+}\n+\n+void HeapShared::remap_dumped_metadata(oop src_obj, address archived_object) {\n+  do_metadata_offsets(src_obj, [&](int offset) {\n+    Metadata** buffered_field_addr = (Metadata**)(archived_object + offset);\n+    Metadata* native_ptr = *buffered_field_addr;\n+\n+    if (native_ptr == nullptr) {\n+      return;\n+    }\n+\n+    if (RegeneratedClasses::has_been_regenerated(native_ptr)) {\n+      native_ptr = RegeneratedClasses::get_regenerated_object(native_ptr);\n+    }\n+\n+    address buffered_native_ptr = ArchiveBuilder::current()->get_buffered_addr((address)native_ptr);\n+    address requested_native_ptr = ArchiveBuilder::current()->to_requested(buffered_native_ptr);\n+    *buffered_field_addr = (Metadata*)requested_native_ptr;\n+  });\n+}\n+\n","filename":"src\/hotspot\/share\/cds\/heapShared.cpp","additions":449,"deletions":209,"binary":false,"changes":658,"status":"modified"},{"patch":"@@ -85,0 +85,2 @@\n+  ciField* get_non_static_field_by_offset(int field_offset);\n+\n@@ -207,0 +209,1 @@\n+  BasicType get_field_type_by_offset(int field_offset, bool is_static);\n@@ -259,0 +262,1 @@\n+    assert(is_interface(), \"must be\");\n","filename":"src\/hotspot\/share\/ci\/ciInstanceKlass.hpp","additions":4,"deletions":0,"binary":false,"changes":4,"status":"modified"},{"patch":"@@ -113,1 +113,1 @@\n-    assert(is_in_encoding_range || k->is_interface() || k->is_abstract(), \"sanity\");\n+    assert(is_in_encoding_range, \"sanity\");\n","filename":"src\/hotspot\/share\/ci\/ciKlass.hpp","additions":1,"deletions":1,"binary":false,"changes":2,"status":"modified"},{"patch":"@@ -50,0 +50,1 @@\n+#include \"oops\/bsmAttribute.inline.hpp\"\n@@ -80,1 +81,1 @@\n-#include \"utilities\/formatBuffer.hpp\"\n+#include \"utilities\/formatBuffer.hpp\"\n@@ -84,0 +85,1 @@\n+#include \"utilities\/hashTable.hpp\"\n@@ -86,1 +88,0 @@\n-#include \"utilities\/resourceHash.hpp\"\n@@ -785,1 +786,1 @@\n-using NameSigHashtable = ResourceHashtable<NameSigHash, int,\n+using NameSigHashtable = HashTable<NameSigHash, int,\n@@ -852,1 +853,1 @@\n-    ResourceHashtable<Symbol*, int>* interface_names = new ResourceHashtable<Symbol*, int>();\n+    HashTable<Symbol*, int>* interface_names = new HashTable<Symbol*, int>();\n@@ -944,0 +945,2 @@\n+    _jdk_internal_vm_annotation_AOTSafeClassInitializer,\n+    _method_AOTRuntimeSetup,\n@@ -979,0 +982,2 @@\n+\n+  bool has_aot_runtime_setup() const { return has_annotation(_method_AOTRuntimeSetup); }\n@@ -1899,0 +1904,10 @@\n+    case VM_SYMBOL_ENUM_NAME(jdk_internal_vm_annotation_AOTSafeClassInitializer_signature): {\n+      if (_location != _in_class)   break;  \/\/ only allow for classes\n+      if (!privileged)              break;  \/\/ only allow in privileged code\n+      return _jdk_internal_vm_annotation_AOTSafeClassInitializer;\n+    }\n+    case VM_SYMBOL_ENUM_NAME(jdk_internal_vm_annotation_AOTRuntimeSetup_signature): {\n+      if (_location != _in_method)  break;  \/\/ only allow for methods\n+      if (!privileged)              break;  \/\/ only allow in privileged code\n+      return _method_AOTRuntimeSetup;\n+    }\n@@ -1978,0 +1993,3 @@\n+  if (has_annotation(_jdk_internal_vm_annotation_AOTSafeClassInitializer)) {\n+    ik->set_has_aot_safe_initializer();\n+  }\n@@ -2008,1 +2026,1 @@\n-  typedef ResourceHashtable<LocalVariableTableElement, LocalVariableTableElement*,\n+  typedef HashTable<LocalVariableTableElement, LocalVariableTableElement*,\n@@ -2431,1 +2449,1 @@\n-      if (!vmClasses::Parameter_klass_loaded())\n+      if (!vmClasses::reflect_Parameter_klass_is_loaded())\n@@ -2664,0 +2682,7 @@\n+  if (parsed_annotations.has_aot_runtime_setup()) {\n+    if (name != vmSymbols::runtimeSetup() || signature != vmSymbols::void_method_signature() ||\n+        !access_flags.is_private() || !access_flags.is_static()) {\n+      classfile_parse_error(\"@AOTRuntimeSetup method must be declared private static void runtimeSetup() for class %s\", CHECK_NULL);\n+    }\n+    _has_aot_runtime_setup_method = true;\n+  }\n@@ -2996,1 +3021,3 @@\n-    verify_legal_class_modifiers(flags, CHECK_0);\n+\n+    Symbol* inner_name_symbol = inner_name_index == 0 ? nullptr : cp->symbol_at(inner_name_index);\n+    verify_legal_class_modifiers(flags, inner_name_symbol, inner_name_index == 0, CHECK_0);\n@@ -3275,0 +3302,1 @@\n+  const int cp_size = cp->length();\n@@ -3276,1 +3304,1 @@\n-  const u1* const current_start = cfs->current();\n+  const u1* const current_before_parsing = cfs->current();\n@@ -3285,1 +3313,1 @@\n-  const int attribute_array_length = cfs->get_u2_fast();\n+  const int num_bootstrap_methods = cfs->get_u2_fast();\n@@ -3287,1 +3315,1 @@\n-  guarantee_property(_max_bootstrap_specifier_index < attribute_array_length,\n+  guarantee_property(_max_bootstrap_specifier_index < num_bootstrap_methods,\n@@ -3291,0 +3319,1 @@\n+  const u4 bootstrap_methods_u2_len = (attribute_byte_length - sizeof(u2)) \/ sizeof(u2);\n@@ -3292,14 +3321,1 @@\n-  \/\/ The attribute contains a counted array of counted tuples of shorts,\n-  \/\/ represending bootstrap specifiers:\n-  \/\/    length*{bootstrap_method_index, argument_count*{argument_index}}\n-  const unsigned int operand_count = (attribute_byte_length - (unsigned)sizeof(u2)) \/ (unsigned)sizeof(u2);\n-  \/\/ operand_count = number of shorts in attr, except for leading length\n-\n-  \/\/ The attribute is copied into a short[] array.\n-  \/\/ The array begins with a series of short[2] pairs, one for each tuple.\n-  const int index_size = (attribute_array_length * 2);\n-\n-  Array<u2>* const operands =\n-    MetadataFactory::new_array<u2>(_loader_data, index_size + operand_count, CHECK);\n-\n-  \/\/ Eagerly assign operands so they will be deallocated with the constant\n+  \/\/ Eagerly assign the arrays so that they will be deallocated with the constant\n@@ -3307,1 +3323,5 @@\n-  cp->set_operands(operands);\n+  BSMAttributeEntries::InsertionIterator iter =\n+    cp->bsm_entries().start_extension(num_bootstrap_methods,\n+                                      bootstrap_methods_u2_len,\n+                                      _loader_data,\n+                                      CHECK);\n@@ -3309,8 +3329,1 @@\n-  int operand_fill_index = index_size;\n-  const int cp_size = cp->length();\n-\n-  for (int n = 0; n < attribute_array_length; n++) {\n-    \/\/ Store a 32-bit offset into the header of the operand array.\n-    ConstantPool::operand_offset_at_put(operands, n, operand_fill_index);\n-\n-    \/\/ Read a bootstrap specifier.\n+  for (int i = 0; i < num_bootstrap_methods; i++) {\n@@ -3318,2 +3331,2 @@\n-    const u2 bootstrap_method_index = cfs->get_u2_fast();\n-    const u2 argument_count = cfs->get_u2_fast();\n+    u2 bootstrap_method_ref = cfs->get_u2_fast();\n+    u2 num_bootstrap_arguments = cfs->get_u2_fast();\n@@ -3321,15 +3334,13 @@\n-      valid_cp_range(bootstrap_method_index, cp_size) &&\n-      cp->tag_at(bootstrap_method_index).is_method_handle(),\n-      \"bootstrap_method_index %u has bad constant type in class file %s\",\n-      bootstrap_method_index,\n-      CHECK);\n-\n-    guarantee_property((operand_fill_index + 1 + argument_count) < operands->length(),\n-      \"Invalid BootstrapMethods num_bootstrap_methods or num_bootstrap_arguments value in class file %s\",\n-      CHECK);\n-\n-    operands->at_put(operand_fill_index++, bootstrap_method_index);\n-    operands->at_put(operand_fill_index++, argument_count);\n-\n-    cfs->guarantee_more(sizeof(u2) * argument_count, CHECK);  \/\/ argv[argc]\n-    for (int j = 0; j < argument_count; j++) {\n+       valid_cp_range(bootstrap_method_ref, cp_size) &&\n+       cp->tag_at(bootstrap_method_ref).is_method_handle(),\n+       \"bootstrap_method_index %u has bad constant type in class file %s\",\n+       bootstrap_method_ref,\n+       CHECK);\n+    cfs->guarantee_more(sizeof(u2) * num_bootstrap_arguments, CHECK); \/\/ argv[argc]\n+\n+    BSMAttributeEntry* entry = iter.reserve_new_entry(bootstrap_method_ref, num_bootstrap_arguments);\n+    guarantee_property(entry != nullptr,\n+                       \"Invalid BootstrapMethods num_bootstrap_methods.\"\n+                       \" The total amount of space reserved for the BootstrapMethod attribute was not sufficient\", CHECK);\n+\n+    for (int argi = 0; argi < num_bootstrap_arguments; argi++) {\n@@ -3343,1 +3354,1 @@\n-      operands->at_put(operand_fill_index++, argument_index);\n+      entry->set_argument(argi, argument_index);\n@@ -3346,1 +3357,2 @@\n-  guarantee_property(current_start + attribute_byte_length == cfs->current(),\n+  cp->bsm_entries().end_extension(iter, _loader_data, CHECK);\n+  guarantee_property(current_before_parsing + attribute_byte_length == cfs->current(),\n@@ -3782,4 +3794,4 @@\n-const InstanceKlass* ClassFileParser::parse_super_class(ConstantPool* const cp,\n-                                                        const int super_class_index,\n-                                                        const bool need_verify,\n-                                                        TRAPS) {\n+void ClassFileParser::check_super_class(ConstantPool* const cp,\n+                                        const int super_class_index,\n+                                        const bool need_verify,\n+                                        TRAPS) {\n@@ -3787,1 +3799,0 @@\n-  const InstanceKlass* super_klass = nullptr;\n@@ -3793,1 +3804,1 @@\n-                       CHECK_NULL);\n+                       CHECK);\n@@ -3798,1 +3809,2 @@\n-                       CHECK_NULL);\n+                       CHECK);\n+\n@@ -3801,10 +3813,2 @@\n-    bool is_array = false;\n-    if (cp->tag_at(super_class_index).is_klass()) {\n-      super_klass = InstanceKlass::cast(cp->resolved_klass_at(super_class_index));\n-      if (need_verify)\n-        is_array = super_klass->is_array_klass();\n-    } else if (need_verify) {\n-      is_array = (cp->klass_name_at(super_class_index)->char_at(0) == JVM_SIGNATURE_ARRAY);\n-    }\n-      guarantee_property(!is_array,\n-                        \"Bad superclass name in class file %s\", CHECK_NULL);\n+      guarantee_property(cp->klass_name_at(super_class_index)->char_at(0) != JVM_SIGNATURE_ARRAY,\n+                        \"Bad superclass name in class file %s\", CHECK);\n@@ -3814,1 +3818,0 @@\n-  return super_klass;\n@@ -3936,1 +3939,1 @@\n-  const InstanceKlass* const super = ik->java_super();\n+  const InstanceKlass* const super = ik->super();\n@@ -3965,1 +3968,1 @@\n-  if (vmClasses::Cloneable_klass_loaded()) {\n+  if (vmClasses::Cloneable_klass_is_loaded()) {\n@@ -3981,0 +3984,9 @@\n+\n+  \/\/ Propagate the AOT runtimeSetup method discovery\n+  if (_has_aot_runtime_setup_method) {\n+    ik->set_is_runtime_setup_required();\n+    if (log_is_enabled(Info, aot, init)) {\n+      ResourceMark rm;\n+      log_info(aot, init)(\"Found @AOTRuntimeSetup class %s\", ik->external_name());\n+    }\n+  }\n@@ -4062,1 +4074,1 @@\n-  const Klass* const super = this_klass->super();\n+  const InstanceKlass* const super = this_klass->super();\n@@ -4065,3 +4077,1 @@\n-    const InstanceKlass* super_ik = InstanceKlass::cast(super);\n-\n-      classfile_icce_error(\"class %s cannot inherit from final class %s\", super_ik, THREAD);\n+      classfile_icce_error(\"class %s cannot inherit from final class %s\", super, THREAD);\n@@ -4072,1 +4082,1 @@\n-    if (super_ik->is_sealed()) {\n+    if (super->is_sealed()) {\n@@ -4075,1 +4085,1 @@\n-      if (!super_ik->has_as_permitted_subclass(this_klass, ss)) {\n+      if (!super->has_as_permitted_subclass(this_klass, ss)) {\n@@ -4082,1 +4092,1 @@\n-      Reflection::verify_class_access(this_klass, InstanceKlass::cast(super), false);\n+      Reflection::verify_class_access(this_klass, super, false);\n@@ -4086,1 +4096,1 @@\n-                                                      InstanceKlass::cast(super),\n+                                                      super,\n@@ -4182,1 +4192,1 @@\n-      const InstanceKlass* k = this_klass->java_super();\n+      const InstanceKlass* k = this_klass->super();\n@@ -4188,1 +4198,1 @@\n-          super_m = InstanceKlass::cast(k)->lookup_method(name, signature);\n+          super_m = k->lookup_method(name, signature);\n@@ -4214,1 +4224,1 @@\n-          k = super_m->method_holder()->java_super();\n+          k = super_m->method_holder()->super();\n@@ -4218,1 +4228,1 @@\n-        k = k->java_super();\n+        k = k->super();\n@@ -4253,1 +4263,2 @@\n-void ClassFileParser::verify_legal_class_modifiers(jint flags, TRAPS) const {\n+\/\/ Verify the class modifiers for the current class, or an inner class if inner_name is non-null.\n+void ClassFileParser::verify_legal_class_modifiers(jint flags, Symbol* inner_name, bool is_anonymous_inner_class, TRAPS) const {\n@@ -4283,6 +4294,24 @@\n-    Exceptions::fthrow(\n-      THREAD_AND_LOCATION,\n-      vmSymbols::java_lang_ClassFormatError(),\n-      \"Illegal class modifiers in class %s: 0x%X\",\n-      _class_name->as_C_string(), flags\n-    );\n+    if (inner_name == nullptr && !is_anonymous_inner_class) {\n+      Exceptions::fthrow(\n+        THREAD_AND_LOCATION,\n+        vmSymbols::java_lang_ClassFormatError(),\n+        \"Illegal class modifiers in class %s: 0x%X\",\n+        _class_name->as_C_string(), flags\n+      );\n+    } else {\n+      if (is_anonymous_inner_class) {\n+        Exceptions::fthrow(\n+          THREAD_AND_LOCATION,\n+          vmSymbols::java_lang_ClassFormatError(),\n+          \"Illegal class modifiers in anonymous inner class of class %s: 0x%X\",\n+          _class_name->as_C_string(), flags\n+        );\n+      } else {\n+        Exceptions::fthrow(\n+          THREAD_AND_LOCATION,\n+          vmSymbols::java_lang_ClassFormatError(),\n+          \"Illegal class modifiers in inner class %s of class %s: 0x%X\",\n+          inner_name->as_C_string(), _class_name->as_C_string(), flags\n+        );\n+      }\n+    }\n@@ -4638,0 +4667,2 @@\n+        signature++;\n+        length--;\n@@ -4639,2 +4670,2 @@\n-        const char* const p = skip_over_field_name(signature + 1, true, --length);\n-\n+        const char* const p = skip_over_field_name(signature, true, length);\n+        assert(p == nullptr || p > signature, \"must parse one character at least\");\n@@ -4642,1 +4673,3 @@\n-        if (p && (p - signature) > 1 && p[0] == JVM_SIGNATURE_ENDCLASS) {\n+        if (p != nullptr                             && \/\/ Parse of field name succeeded.\n+            p - signature < static_cast<int>(length) && \/\/ There is at least one character left to parse.\n+            p[0] == JVM_SIGNATURE_ENDCLASS) {\n@@ -5124,1 +5157,1 @@\n-  if (_parsed_annotations->has_any_annotations()) {\n+  if (_parsed_annotations->has_any_annotations())\n@@ -5126,1 +5159,0 @@\n-  }\n@@ -5229,1 +5261,1 @@\n-      if (ik->java_super() != nullptr) {\n+      if (ik->super() != nullptr) {\n@@ -5232,1 +5264,1 @@\n-                   ik->java_super()->external_name());\n+                   ik->super()->external_name());\n@@ -5333,0 +5365,1 @@\n+  _has_aot_runtime_setup_method(false),\n@@ -5483,0 +5516,1 @@\n+    guarantee_property((u4)cp_size < 0xffff, \"Overflow in constant pool size for hidden class %s\", CHECK);\n@@ -5513,1 +5547,1 @@\n-  verify_legal_class_modifiers(flags, CHECK);\n+  verify_legal_class_modifiers(flags, nullptr, false, CHECK);\n@@ -5603,4 +5637,4 @@\n-  _super_klass = parse_super_class(cp,\n-                                   _super_class_index,\n-                                   _need_verify,\n-                                   CHECK);\n+  check_super_class(cp,\n+                    _super_class_index,\n+                    _need_verify,\n+                    CHECK);\n@@ -5673,2 +5707,2 @@\n-    Atomic::cmpxchg(&counter, (size_t)0, Arguments::default_SharedBaseAddress()); \/\/ initialize it\n-    size_t new_id = Atomic::add(&counter, (size_t)1);\n+    AtomicAccess::cmpxchg(&counter, (size_t)0, Arguments::default_SharedBaseAddress()); \/\/ initialize it\n+    size_t new_id = AtomicAccess::add(&counter, (size_t)1);\n@@ -5709,0 +5743,2 @@\n+    precond(_super_class_index == 0);\n+    precond(_super_klass == nullptr);\n@@ -5712,3 +5748,3 @@\n-  }\n-  \/\/ We check super class after class file is parsed and format is checked\n-  if (_super_class_index > 0 && nullptr == _super_klass) {\n+  } else {\n+    \/\/ Set _super_klass after class file is parsed and format is checked\n+    assert(_super_class_index > 0, \"any class other than Object must have a super class\");\n@@ -5725,0 +5761,1 @@\n+      \/\/ fast path to avoid lookup\n@@ -5850,9 +5887,0 @@\n-\/\/ Returns true if the future Klass will need to be addressable with a narrow Klass ID.\n-bool ClassFileParser::klass_needs_narrow_id() const {\n-  \/\/ Classes that are never instantiated need no narrow Klass Id, since the\n-  \/\/ only point of having a narrow id is to put it into an object header. Keeping\n-  \/\/ never instantiated classes out of class space lessens the class space pressure.\n-  \/\/ For more details, see JDK-8338526.\n-  return !is_interface() && !is_abstract();\n-}\n-\n","filename":"src\/hotspot\/share\/classfile\/classFileParser.cpp","additions":145,"deletions":117,"binary":false,"changes":262,"status":"modified"},{"patch":"@@ -196,0 +196,1 @@\n+  bool _has_aot_runtime_setup_method;\n@@ -245,4 +246,4 @@\n-  const InstanceKlass* parse_super_class(ConstantPool* const cp,\n-                                         const int super_class_index,\n-                                         const bool need_verify,\n-                                         TRAPS);\n+  void check_super_class(ConstantPool* const cp,\n+                         const int super_class_index,\n+                         const bool need_verify,\n+                         TRAPS);\n@@ -435,1 +436,2 @@\n-  void verify_legal_class_modifiers(jint flags, TRAPS) const;\n+  void verify_legal_class_modifiers(jint flags, Symbol* inner_name,\n+                                    bool is_anonymous_inner_class, TRAPS) const;\n@@ -518,5 +520,0 @@\n-  bool is_abstract() const { return _access_flags.is_abstract(); }\n-\n-  \/\/ Returns true if the Klass to be generated will need to be addressable\n-  \/\/ with a narrow Klass ID.\n-  bool klass_needs_narrow_id() const;\n","filename":"src\/hotspot\/share\/classfile\/classFileParser.hpp","additions":7,"deletions":10,"binary":false,"changes":17,"status":"modified"},{"patch":"@@ -31,1 +31,1 @@\n-#include \"oops\/instanceMirrorKlass.hpp\"\n+#include \"oops\/instanceMirrorKlass.hpp\"\n@@ -336,1 +336,1 @@\n-    ik = ik->super() == nullptr ? nullptr : InstanceKlass::cast(ik->super());\n+    ik = ik->super() == nullptr ? nullptr : ik->super();\n@@ -494,1 +494,1 @@\n-          ik = ik->java_super();\n+          ik = ik->super();\n","filename":"src\/hotspot\/share\/classfile\/fieldLayoutBuilder.cpp","additions":3,"deletions":3,"binary":false,"changes":6,"status":"modified"},{"patch":"@@ -25,0 +25,1 @@\n+#include \"cds\/aotMetaspace.hpp\"\n@@ -27,3 +28,1 @@\n-#include \"cds\/archiveHeapLoader.hpp\"\n-#include \"cds\/heapShared.hpp\"\n-#include \"cds\/metaspaceShared.hpp\"\n+#include \"cds\/heapShared.inline.hpp\"\n@@ -58,1 +57,0 @@\n-#include \"oops\/klass.hpp\"\n@@ -63,2 +61,1 @@\n-#include \"oops\/oopCast.inline.hpp\"\n-#include \"oops\/symbol.hpp\"\n+#include \"oops\/oopCast.inline.hpp\"\n@@ -67,0 +64,1 @@\n+#include \"oops\/symbol.hpp\"\n@@ -83,1 +81,1 @@\n-#include \"runtime\/reflectionUtils.hpp\"\n+#include \"runtime\/reflection.hpp\"\n@@ -208,1 +206,1 @@\n-  uint8_t value = Atomic::load(addr);\n+  uint8_t value = AtomicAccess::load(addr);\n@@ -212,1 +210,1 @@\n-    value = Atomic::cmpxchg(addr, old_value, value);\n+    value = AtomicAccess::cmpxchg(addr, old_value, value);\n@@ -873,0 +871,1 @@\n+int java_lang_Class::_raw_access_flags_offset;\n@@ -942,1 +941,1 @@\n-  if (!k->is_shared()) {\n+  if (!k->in_aot_cache()) {\n@@ -980,2 +979,2 @@\n-  if (k->is_shared() && k->has_archived_mirror_index()) {\n-    if (ArchiveHeapLoader::is_in_use()) {\n+  if (k->in_aot_cache() && k->has_archived_mirror_index()) {\n+    if (HeapShared::is_archived_heap_in_use()) {\n@@ -993,1 +992,1 @@\n-void java_lang_Class::initialize_mirror_fields(Klass* k,\n+void java_lang_Class::initialize_mirror_fields(InstanceKlass* ik,\n@@ -1008,1 +1007,1 @@\n-  InstanceKlass::cast(k)->do_local_static_fields(&initialize_static_field, mirror, CHECK);\n+  ik->do_local_static_fields(&initialize_static_field, mirror, CHECK);\n@@ -1016,0 +1015,9 @@\n+  if (CDSConfig::is_using_aot_linked_classes()) {\n+    oop archived_module = java_lang_Class::module(mirror());\n+    if (archived_module != nullptr) {\n+      precond(module() == nullptr || module() == archived_module);\n+      precond(AOTMetaspace::in_aot_cache_static_region((void*)k));\n+      return;\n+    }\n+  }\n+\n@@ -1022,0 +1030,4 @@\n+    \/\/ With AOT-linked classes, java.base should have been defined before the\n+    \/\/ VM loads any classes.\n+    precond(!CDSConfig::is_using_aot_linked_classes());\n+\n@@ -1055,3 +1067,6 @@\n-  GrowableArray<Klass*>* mirror_list =\n-    new (mtClass) GrowableArray<Klass*>(40, mtClass);\n-  set_fixup_mirror_list(mirror_list);\n+  if (!CDSConfig::is_using_aot_linked_classes()) {\n+    \/\/ fixup_mirror_list() is not used when we have preloaded classes. See\n+    \/\/ Universe::fixup_mirrors().\n+    GrowableArray<Klass*>* mirror_list =\n+      new (mtClass) GrowableArray<Klass*>(40, mtClass);\n+    set_fixup_mirror_list(mirror_list);\n@@ -1059,3 +1074,6 @@\n-  GrowableArray<Klass*>* module_list =\n-    new (mtModule) GrowableArray<Klass*>(500, mtModule);\n-  set_fixup_module_field_list(module_list);\n+    \/\/ With AOT-linked classes, java.base module is defined before any class\n+    \/\/ is loaded, so there's no need for fixup_module_field_list().\n+    GrowableArray<Klass*>* module_list =\n+      new (mtModule) GrowableArray<Klass*>(500, mtModule);\n+    set_fixup_module_field_list(module_list);\n+  }\n@@ -1076,0 +1094,4 @@\n+  \/\/ Set the raw access_flags, this is used by reflection instead of modifier flags.\n+  \/\/ The Java code for array classes gets the access flags from the element type.\n+  assert(!k->is_array_klass() || k->access_flags().as_unsigned_short() == 0, \"access flags are not set for arrays\");\n+  set_raw_access_flags(mirror(), k->access_flags().as_unsigned_short());\n@@ -1110,2 +1132,1 @@\n-\n-    initialize_mirror_fields(k, mirror, protection_domain, classData, THREAD);\n+    initialize_mirror_fields(InstanceKlass::cast(k), mirror, protection_domain, classData, THREAD);\n@@ -1131,1 +1152,1 @@\n-  if (vmClasses::Class_klass_loaded()) {\n+  if (vmClasses::Class_klass_is_loaded()) {\n@@ -1158,0 +1179,1 @@\n+    assert(!CDSConfig::is_using_aot_linked_classes(), \"should not come here\");\n@@ -1203,1 +1225,1 @@\n-  if (!vmClasses::Class_klass_loaded()) {\n+  if (!vmClasses::Class_klass_is_loaded() && !CDSConfig::is_using_aot_linked_classes()) {\n@@ -1222,4 +1244,1 @@\n-\n-    \/\/ create the init_lock\n-    typeArrayOop r = oopFactory::new_typeArray(T_INT, 0, CHECK_(false));\n-    set_init_lock(mirror(), r);\n+    assert(init_lock(mirror()) != nullptr, \"allocated during AOT assembly\");\n@@ -1317,5 +1336,0 @@\n-void java_lang_Class::set_reflection_data(oop java_class, oop reflection_data) {\n-  assert(_reflectionData_offset != 0, \"must be set\");\n-  java_class->obj_field_put(_reflectionData_offset, reflection_data);\n-}\n-\n@@ -1381,0 +1395,2 @@\n+  set_raw_access_flags(java_class, JVM_ACC_ABSTRACT | JVM_ACC_FINAL | JVM_ACC_PUBLIC);\n+\n@@ -1462,1 +1478,0 @@\n-\n@@ -1522,0 +1537,1 @@\n+  macro(_raw_access_flags_offset,    k, \"classFileAccessFlags\",      char_signature,    false); \\\n@@ -1567,0 +1583,5 @@\n+void java_lang_Class::set_raw_access_flags(oop the_class_mirror, u2 value) {\n+  assert(_raw_access_flags_offset != 0, \"offsets should have been initialized\");\n+  the_class_mirror->char_field_put(_raw_access_flags_offset, value);\n+}\n+\n@@ -1872,2 +1893,3 @@\n-         JavaThread::current()->thread_state() == _thread_in_vm,\n-         \"Java Thread is not running in vm\");\n+         JavaThread::current()->thread_state() == _thread_in_vm ||\n+         JavaThread::current() == java_lang_Thread::thread(java_thread),\n+         \"unsafe call to java_lang_Thread::get_thread_status()?\");\n@@ -1885,14 +1907,10 @@\n-oop java_lang_Thread::async_get_stack_trace(oop java_thread, TRAPS) {\n-  ThreadsListHandle tlh(JavaThread::current());\n-  JavaThread* thread;\n-  bool is_virtual = java_lang_VirtualThread::is_instance(java_thread);\n-  if (is_virtual) {\n-    oop carrier_thread = java_lang_VirtualThread::carrier_thread(java_thread);\n-    if (carrier_thread == nullptr) {\n-      return nullptr;\n-    }\n-    thread = java_lang_Thread::thread(carrier_thread);\n-  } else {\n-    thread = java_lang_Thread::thread(java_thread);\n-  }\n-  if (thread == nullptr) {\n+\/\/ Obtain stack trace for platform or mounted virtual thread.\n+\/\/ If jthread is a virtual thread and it has been unmounted (or remounted to different carrier) the method returns null.\n+\/\/ The caller (java.lang.VirtualThread) handles returned nulls via retry.\n+oop java_lang_Thread::async_get_stack_trace(jobject jthread, TRAPS) {\n+  ThreadsListHandle tlh(THREAD);\n+  JavaThread* java_thread = nullptr;\n+  oop thread_oop;\n+\n+  bool has_java_thread = tlh.cv_internal_thread_to_JavaThread(jthread, &java_thread, &thread_oop);\n+  if (!has_java_thread) {\n@@ -1904,1 +1922,1 @@\n-    const Handle _java_thread;\n+    const Handle _thread_h;\n@@ -1910,2 +1928,2 @@\n-    GetStackTraceHandshakeClosure(Handle java_thread) :\n-        HandshakeClosure(\"GetStackTraceHandshakeClosure\"), _java_thread(java_thread), _depth(0), _retry_handshake(false),\n+    GetStackTraceHandshakeClosure(Handle thread_h) :\n+        HandshakeClosure(\"GetStackTraceHandshakeClosure\"), _thread_h(thread_h), _depth(0), _retry_handshake(false),\n@@ -1933,1 +1951,1 @@\n-      JavaThread* thread = JavaThread::cast(th);\n+      JavaThread* java_thread = JavaThread::cast(th);\n@@ -1935,1 +1953,1 @@\n-      if (!thread->has_last_Java_frame()) {\n+      if (!java_thread->has_last_Java_frame()) {\n@@ -1940,5 +1958,6 @@\n-      if (java_lang_VirtualThread::is_instance(_java_thread())) {\n-        \/\/ if (thread->vthread() != _java_thread()) \/\/ We might be inside a System.executeOnCarrierThread\n-        const ContinuationEntry* ce = thread->vthread_continuation();\n-        if (ce == nullptr || ce->cont_oop(thread) != java_lang_VirtualThread::continuation(_java_thread())) {\n-          return; \/\/ not mounted\n+      if (java_lang_VirtualThread::is_instance(_thread_h())) {\n+        \/\/ Ensure _thread_h is still mounted to java_thread.\n+        const ContinuationEntry* ce = java_thread->vthread_continuation();\n+        if (ce == nullptr || ce->cont_oop(java_thread) != java_lang_VirtualThread::continuation(_thread_h())) {\n+          \/\/ Target thread has been unmounted.\n+          return;\n@@ -1947,1 +1966,1 @@\n-        carrier = (thread->vthread_continuation() != nullptr);\n+        carrier = (java_thread->vthread_continuation() != nullptr);\n@@ -1959,1 +1978,1 @@\n-      for (vframeStream vfst(thread, false, false, carrier); \/\/ we don't process frames as we don't care about oops\n+      for (vframeStream vfst(java_thread, false, false, carrier); \/\/ we don't process frames as we don't care about oops\n@@ -1980,1 +1999,1 @@\n-  GetStackTraceHandshakeClosure gsthc(Handle(THREAD, java_thread));\n+  GetStackTraceHandshakeClosure gsthc(Handle(THREAD, thread_oop));\n@@ -1982,1 +2001,1 @@\n-   Handshake::execute(&gsthc, &tlh, thread);\n+   Handshake::execute(&gsthc, &tlh, java_thread);\n@@ -2084,0 +2103,1 @@\n+int java_lang_VirtualThread::_interruptible_wait_offset;\n@@ -2095,0 +2115,1 @@\n+  macro(_interruptible_wait_offset,        k, \"interruptibleWait\",  bool_signature,              false); \\\n@@ -2128,1 +2149,1 @@\n-  int res = Atomic::cmpxchg(addr, old_state, new_state);\n+  int res = AtomicAccess::cmpxchg(addr, old_state, new_state);\n@@ -2146,1 +2167,1 @@\n-  jboolean vthread_on_list = Atomic::load(addr);\n+  jboolean vthread_on_list = AtomicAccess::load(addr);\n@@ -2148,1 +2169,1 @@\n-    vthread_on_list = Atomic::cmpxchg(addr, (jboolean)JNI_FALSE, (jboolean)JNI_TRUE);\n+    vthread_on_list = AtomicAccess::cmpxchg(addr, (jboolean)JNI_FALSE, (jboolean)JNI_TRUE);\n@@ -2164,0 +2185,4 @@\n+void java_lang_VirtualThread::set_interruptible_wait(oop vthread, jboolean value) {\n+  vthread->bool_field_put_volatile(_interruptible_wait_offset, value);\n+}\n+\n@@ -2571,1 +2596,2 @@\n-\/\/ Print stack trace element to resource allocated buffer\n+\/\/ Print stack trace element to the specified output stream.\n+\/\/ The output is formatted into a stringStream and written to the outputStream in one step.\n@@ -2575,0 +2601,1 @@\n+  stringStream ss;\n@@ -2576,2 +2603,1 @@\n-  \/\/ Get strings and string lengths\n-  InstanceKlass* holder = InstanceKlass::cast(java_lang_Class::as_Klass(mirror()));\n+  InstanceKlass* holder = java_lang_Class::as_InstanceKlass(mirror());\n@@ -2579,3 +2605,1 @@\n-  int buf_len = (int)strlen(klass_name);\n-\n-  buf_len += (int)strlen(method_name);\n+  ss.print(\"\\tat %s.%s(\", klass_name, method_name);\n@@ -2584,8 +2608,1 @@\n-  char* source_file_name = nullptr;\n-  Symbol* source = Backtrace::get_source_file_name(holder, version);\n-  if (source != nullptr) {\n-    source_file_name = source->as_C_string();\n-    buf_len += (int)strlen(source_file_name);\n-  }\n-\n-  char *module_name = nullptr, *module_version = nullptr;\n+  \/\/ Print module information\n@@ -2594,2 +2611,1 @@\n-    module_name = module->name()->as_C_string();\n-    buf_len += (int)strlen(module_name);\n+    char* module_name = module->name()->as_C_string();\n@@ -2597,2 +2613,4 @@\n-      module_version = module->version()->as_C_string();\n-      buf_len += (int)strlen(module_version);\n+      char* module_version = module->version()->as_C_string();\n+      ss.print(\"%s@%s\/\", module_name, module_version);\n+    } else {\n+      ss.print(\"%s\/\", module_name);\n@@ -2602,14 +2620,4 @@\n-  \/\/ Allocate temporary buffer with extra space for formatting and line number\n-  const size_t buf_size = buf_len + 64;\n-  char* buf = NEW_RESOURCE_ARRAY(char, buf_size);\n-\n-  \/\/ Print stack trace line in buffer\n-  size_t buf_off = os::snprintf_checked(buf, buf_size, \"\\tat %s.%s(\", klass_name, method_name);\n-\n-  \/\/ Print module information\n-  if (module_name != nullptr) {\n-    if (module_version != nullptr) {\n-      buf_off += os::snprintf_checked(buf + buf_off, buf_size - buf_off, \"%s@%s\/\", module_name, module_version);\n-    } else {\n-      buf_off += os::snprintf_checked(buf + buf_off, buf_size - buf_off, \"%s\/\", module_name);\n-    }\n+  char* source_file_name = nullptr;\n+  Symbol* source = Backtrace::get_source_file_name(holder, version);\n+  if (source != nullptr) {\n+    source_file_name = source->as_C_string();\n@@ -2621,1 +2629,1 @@\n-    strcat(buf, \"Redefined)\");\n+    ss.print(\"Redefined)\");\n@@ -2625,1 +2633,1 @@\n-      strcat(buf, \"Native Method)\");\n+      ss.print(\"Native Method)\");\n@@ -2629,1 +2637,1 @@\n-        buf_off += os::snprintf_checked(buf + buf_off, buf_size - buf_off, \"%s:%d)\", source_file_name, line_number);\n+        ss.print(\"%s:%d)\", source_file_name, line_number);\n@@ -2632,1 +2640,1 @@\n-        buf_off += os::snprintf_checked(buf + buf_off, buf_size - buf_off, \"%s)\", source_file_name);\n+        ss.print(\"%s)\", source_file_name);\n@@ -2635,1 +2643,1 @@\n-        buf_off += os::snprintf_checked(buf + buf_off, buf_size - buf_off, \"Unknown Source)\");\n+        ss.print(\"Unknown Source)\");\n@@ -2639,1 +2647,1 @@\n-        os::snprintf_checked(buf + buf_off, buf_size - buf_off, \"(nmethod \" INTPTR_FORMAT \")\", (intptr_t)nm);\n+        ss.print(\"(nmethod \" INTPTR_FORMAT \")\", p2i(nm));\n@@ -2644,1 +2652,2 @@\n-  st->print_cr(\"%s\", buf);\n+  ss.cr();\n+  st->print_raw(ss.freeze(), ss.size());\n@@ -2973,1 +2982,1 @@\n-    InstanceKlass* holder = InstanceKlass::cast(java_lang_Class::as_Klass(bte._mirror()));\n+    InstanceKlass* holder = java_lang_Class::as_InstanceKlass(bte._mirror());\n@@ -3059,1 +3068,1 @@\n-  InstanceKlass* holder = InstanceKlass::cast(java_lang_Class::as_Klass(bte._mirror()));\n+  InstanceKlass* holder = java_lang_Class::as_InstanceKlass(bte._mirror());\n@@ -3445,1 +3454,1 @@\n-  Klass* klass = vmClasses::reflect_Method_klass();\n+  InstanceKlass* klass = vmClasses::reflect_Method_klass();\n@@ -3448,2 +3457,2 @@\n-  assert(InstanceKlass::cast(klass)->is_initialized(), \"must be initialized\");\n-  return InstanceKlass::cast(klass)->allocate_instance_handle(THREAD);\n+  assert(klass->is_initialized(), \"must be initialized\");\n+  return klass->allocate_instance_handle(THREAD);\n@@ -3746,4 +3755,1 @@\n-int reflect_ConstantPool::_oop_offset;\n-\n-#define CONSTANTPOOL_FIELDS_DO(macro) \\\n-  macro(_oop_offset, k, \"constantPoolOop\", object_signature, false)\n+int reflect_ConstantPool::_vmholder_offset;\n@@ -3753,2 +3759,2 @@\n-  \/\/ The field is called ConstantPool* in the sun.reflect.ConstantPool class.\n-  CONSTANTPOOL_FIELDS_DO(FIELD_COMPUTE_OFFSET);\n+  \/\/ The field is injected and called Object vmholder in the jdk.internal.reflect.ConstantPool class.\n+  CONSTANTPOOL_INJECTED_FIELDS(INJECTED_FIELD_COMPUTE_OFFSET);\n@@ -3759,1 +3765,1 @@\n-  CONSTANTPOOL_FIELDS_DO(FIELD_SERIALIZE_OFFSET);\n+  CONSTANTPOOL_INJECTED_FIELDS(INJECTED_FIELD_SERIALIZE_OFFSET);\n@@ -3912,0 +3918,1 @@\n+  assert(_vmholder_offset != 0, \"Uninitialized vmholder\");\n@@ -3914,1 +3921,1 @@\n-  reflect->obj_field_put(_oop_offset, mirror);\n+  reflect->obj_field_put(_vmholder_offset, mirror);\n@@ -3918,4 +3925,3 @@\n-\n-  oop mirror = reflect->obj_field(_oop_offset);\n-  Klass* k = java_lang_Class::as_Klass(mirror);\n-  assert(k->is_instance_klass(), \"Must be\");\n+  assert(_vmholder_offset != 0, \"Uninitialized vmholder\");\n+  oop mirror = reflect->obj_field(_vmholder_offset);\n+  InstanceKlass* ik = java_lang_Class::as_InstanceKlass(mirror);\n@@ -3928,1 +3934,1 @@\n-  return InstanceKlass::cast(k)->constants();\n+  return ik->constants();\n@@ -4764,1 +4770,1 @@\n-  return Atomic::load_acquire(loader->field_addr<ClassLoaderData*>(_loader_data_offset));\n+  return AtomicAccess::load_acquire(loader->field_addr<ClassLoaderData*>(_loader_data_offset));\n@@ -4776,1 +4782,1 @@\n-  Atomic::release_store(loader->field_addr<ClassLoaderData*>(_loader_data_offset), new_data);\n+  AtomicAccess::release_store(loader->field_addr<ClassLoaderData*>(_loader_data_offset), new_data);\n@@ -5535,1 +5541,1 @@\n-  InstanceKlass* ik = InstanceKlass::cast(klass());\n+  InstanceKlass* ik = klass();\n@@ -5561,1 +5567,0 @@\n-  FilteredFieldsMap::initialize();  \/\/ must be done after computing offsets.\n","filename":"src\/hotspot\/share\/classfile\/javaClasses.cpp","additions":133,"deletions":128,"binary":false,"changes":261,"status":"modified"},{"patch":"@@ -237,0 +237,1 @@\n+  friend class HeapShared;\n@@ -261,0 +262,1 @@\n+  static int _raw_access_flags_offset;\n@@ -271,1 +273,1 @@\n-  static void initialize_mirror_fields(Klass* k, Handle mirror, Handle protection_domain,\n+  static void initialize_mirror_fields(InstanceKlass* ik, Handle mirror, Handle protection_domain,\n@@ -274,0 +276,6 @@\n+\n+  static void set_modifiers(oop java_class, u2 value);\n+  static void set_raw_access_flags(oop java_class, u2 value);\n+  static void set_is_primitive(oop java_class);\n+  static void release_set_array_klass(oop java_class, Klass* klass);\n+\n@@ -295,1 +303,1 @@\n-  \/\/ Conversion\n+  \/\/ Conversion -- java_class must not be null. The return value is null only if java_class is a primitive type.\n@@ -297,0 +305,2 @@\n+  static InstanceKlass* as_InstanceKlass(oop java_class);\n+\n@@ -306,1 +316,0 @@\n-  static void set_is_primitive(oop java_class);\n@@ -309,2 +318,1 @@\n-  \/\/ JVM_NewArray support\n-  static void release_set_array_klass(oop java_class, Klass* klass);\n+\n@@ -330,1 +338,0 @@\n-  static void set_reflection_data(oop java_class, oop reflection_data);\n@@ -343,1 +350,0 @@\n-  static void set_modifiers(oop java_class, u2 value);\n@@ -462,1 +468,1 @@\n-  static oop async_get_stack_trace(oop java_thread, TRAPS);\n+  static oop async_get_stack_trace(jobject jthread, TRAPS);\n@@ -559,0 +565,1 @@\n+  static int _interruptible_wait_offset;\n@@ -611,0 +618,1 @@\n+  static void set_interruptible_wait(oop vthread, jboolean value);\n@@ -933,0 +941,3 @@\n+#define CONSTANTPOOL_INJECTED_FIELDS(macro)                             \\\n+  macro(reflect_ConstantPool, vmholder, object_signature, false)\n+\n@@ -937,2 +948,3 @@\n-  \/\/ offsets at run-time.\n-  static int _oop_offset;\n+  \/\/ offsets at run-time. This field is the oop offset for the\n+  \/\/ actual constant pool, previously called constantPoolOop.\n+  static int _vmholder_offset;\n@@ -950,1 +962,0 @@\n-  static int oop_offset() { CHECK_INIT(_oop_offset); }\n@@ -1894,1 +1905,1 @@\n-  const bool           may_be_java;\n+  const bool may_be_java;\n@@ -1897,2 +1908,2 @@\n-  Klass* klass() const      { return vmClasses::klass_at(klass_id); }\n-  Symbol* name() const      { return lookup_symbol(name_index); }\n+  InstanceKlass* klass() const { return vmClasses::klass_at(klass_id); }\n+  Symbol* name() const { return lookup_symbol(name_index); }\n","filename":"src\/hotspot\/share\/classfile\/javaClasses.hpp","additions":25,"deletions":14,"binary":false,"changes":39,"status":"modified"},{"patch":"@@ -83,1 +83,1 @@\n-  return (Atomic::load(flags_addr(java_string)) & flag_mask) != 0;\n+  return (AtomicAccess::load(flags_addr(java_string)) & flag_mask) != 0;\n@@ -294,0 +294,6 @@\n+inline InstanceKlass* java_lang_Class::as_InstanceKlass(oop java_class) {\n+  Klass* k = as_Klass(java_class);\n+  assert(k == nullptr || k->is_instance_klass(), \"type check\");\n+  return static_cast<InstanceKlass*>(k);\n+}\n+\n","filename":"src\/hotspot\/share\/classfile\/javaClasses.inline.hpp","additions":7,"deletions":1,"binary":false,"changes":8,"status":"modified"},{"patch":"@@ -70,0 +70,6 @@\n+  \/\/ G1 prefers to use conditional card marking to avoid overwriting cards that\n+  \/\/ have already been found to contain a to-collection set reference. This reduces\n+  \/\/ refinement effort.\n+  if (FLAG_IS_DEFAULT(UseCondCardMark)) {\n+    FLAG_SET_ERGO(UseCondCardMark, true);\n+  }\n@@ -243,2 +249,1 @@\n-  uint max_parallel_refinement_threads = G1ConcRefinementThreads + G1DirtyCardQueueSet::num_par_ids();\n-  if (max_parallel_refinement_threads > UINT_MAX \/ divisor) {\n+  if (G1ConcRefinementThreads > UINT_MAX \/ divisor) {\n","filename":"src\/hotspot\/share\/gc\/g1\/g1Arguments.cpp","additions":7,"deletions":2,"binary":false,"changes":9,"status":"modified"},{"patch":"@@ -41,1 +41,0 @@\n-#include \"gc\/g1\/g1DirtyCardQueue.hpp\"\n@@ -63,1 +62,0 @@\n-#include \"gc\/g1\/g1RedirtyCardsQueue.hpp\"\n@@ -67,0 +65,1 @@\n+#include \"gc\/g1\/g1ReviseYoungLengthTask.hpp\"\n@@ -68,1 +67,0 @@\n-#include \"gc\/g1\/g1RootProcessor.hpp\"\n@@ -77,0 +75,1 @@\n+#include \"gc\/shared\/barrierSetNMethod.hpp\"\n@@ -107,1 +106,1 @@\n-#include \"runtime\/atomic.hpp\"\n+#include \"runtime\/atomicAccess.hpp\"\n@@ -113,0 +112,1 @@\n+#include \"runtime\/threads.hpp\"\n@@ -148,1 +148,1 @@\n-uint G1CollectedHeap::get_chunks_per_region() {\n+uint G1CollectedHeap::get_chunks_per_region_for_scan() {\n@@ -158,0 +158,12 @@\n+uint G1CollectedHeap::get_chunks_per_region_for_merge() {\n+  uint log_region_size = G1HeapRegion::LogOfHRGrainBytes;\n+  \/\/ Limit the expected input values to current known possible values of the\n+  \/\/ (log) region size. Adjust as necessary after testing if changing the permissible\n+  \/\/ values for region size.\n+  assert(log_region_size >= 20 && log_region_size <= 29,\n+         \"expected value in [20,29], but got %u\", log_region_size);\n+\n+  uint half_log_region_size = (log_region_size + 1) \/ 2;\n+  return 1 << (half_log_region_size - 9);\n+}\n+\n@@ -176,4 +188,7 @@\n-    \/\/ Currently, only attempts to allocate GC alloc regions set\n-    \/\/ do_expand to true. So, we should only reach here during a\n-    \/\/ safepoint.\n-    assert(SafepointSynchronize::is_at_safepoint(), \"invariant\");\n+    \/\/ There are two situations where do_expand is set to true:\n+    \/\/  - for mutator regions during initialization\n+    \/\/  - for GC alloc regions during a safepoint\n+    \/\/ Make sure we only reach here before initialization is complete\n+    \/\/ or during a safepoint.\n+    assert(!is_init_completed() ||\n+           SafepointSynchronize::is_at_safepoint() , \"invariant\");\n@@ -341,0 +356,8 @@\n+size_t G1CollectedHeap::allocation_used_bytes(size_t allocation_word_size) {\n+  if (is_humongous(allocation_word_size)) {\n+    return humongous_obj_size_in_regions(allocation_word_size) * G1HeapRegion::GrainBytes;\n+  } else {\n+    return allocation_word_size * HeapWordSize;\n+  }\n+}\n+\n@@ -394,1 +417,6 @@\n-  return attempt_allocation(min_size, requested_size, actual_size);\n+  \/\/ Do not allow a GC because we are allocating a new TLAB to avoid an issue\n+  \/\/ with UseGCOverheadLimit: although this GC would return null if the overhead\n+  \/\/ limit would be exceeded, but it would likely free at least some space.\n+  \/\/ So the subsequent outside-TLAB allocation could be successful anyway and\n+  \/\/ the indication that the overhead limit had been exceeded swallowed.\n+  return attempt_allocation(min_size, requested_size, actual_size, false \/* allow_gc *\/);\n@@ -397,3 +425,1 @@\n-HeapWord*\n-G1CollectedHeap::mem_allocate(size_t word_size,\n-                              bool*  gc_overhead_limit_was_exceeded) {\n+HeapWord* G1CollectedHeap::mem_allocate(size_t word_size) {\n@@ -406,1 +432,1 @@\n-  return attempt_allocation(word_size, word_size, &dummy);\n+  return attempt_allocation(word_size, word_size, &dummy, true \/* allow_gc *\/);\n@@ -409,1 +435,1 @@\n-HeapWord* G1CollectedHeap::attempt_allocation_slow(uint node_index, size_t word_size) {\n+HeapWord* G1CollectedHeap::attempt_allocation_slow(uint node_index, size_t word_size, bool allow_gc) {\n@@ -436,0 +462,2 @@\n+      } else if (!allow_gc) {\n+        return nullptr;\n@@ -453,0 +481,7 @@\n+    \/\/ Has the gc overhead limit been reached in the meantime? If so, this mutator\n+    \/\/ should receive null even when unsuccessfully scheduling a collection as well\n+    \/\/ for global consistency.\n+    if (gc_overhead_limit_exceeded()) {\n+      return nullptr;\n+    }\n+\n@@ -454,1 +489,1 @@\n-    \/\/ another thread beat us to it). In this case immeditealy retry the allocation\n+    \/\/ another thread beat us to it). In this case immediately retry the allocation\n@@ -599,1 +634,2 @@\n-                                                     size_t* actual_word_size) {\n+                                                     size_t* actual_word_size,\n+                                                     bool allow_gc) {\n@@ -611,1 +647,1 @@\n-    result = attempt_allocation_slow(node_index, desired_word_size);\n+    result = attempt_allocation_slow(node_index, desired_word_size, allow_gc);\n@@ -617,1 +653,0 @@\n-    dirty_young_block(result, *actual_word_size);\n@@ -625,0 +660,6 @@\n+\/\/ Helper for [try_]collect().\n+static G1GCCounters collection_counters(G1CollectedHeap* g1h) {\n+  MutexLocker ml(Heap_lock);\n+  return G1GCCounters(g1h);\n+}\n+\n@@ -650,1 +691,1 @@\n-    collect(GCCause::_g1_humongous_allocation);\n+    try_collect(word_size, GCCause::_g1_humongous_allocation, collection_counters(this));\n@@ -660,0 +701,2 @@\n+    \/\/ The amount of bytes the humongous object will actually take.\n+    size_t humongous_byte_size = G1HeapRegion::align_up_to_region_byte_size(word_size * HeapWordSize);\n@@ -664,1 +707,0 @@\n-      size_t size_in_regions = humongous_obj_size_in_regions(word_size);\n@@ -671,1 +713,1 @@\n-          add_allocated_humongous_bytes_since_last_gc(size_in_regions * G1HeapRegion::GrainBytes);\n+          add_allocated_humongous_bytes_since_last_gc(humongous_byte_size);\n@@ -685,2 +727,1 @@\n-        size_t size_in_regions = humongous_obj_size_in_regions(word_size);\n-          record_collection_pause_humongous_allocation(size_in_regions * G1HeapRegion::GrainBytes);\n+          record_collection_pause_humongous_allocation(humongous_byte_size);\n@@ -695,0 +736,7 @@\n+    \/\/ Has the gc overhead limit been reached in the meantime? If so, this mutator\n+    \/\/ should receive null even when unsuccessfully scheduling a collection as well\n+    \/\/ for global consistency.\n+    if (gc_overhead_limit_exceeded()) {\n+      return nullptr;\n+    }\n+\n@@ -725,1 +773,3 @@\n-    if (result != nullptr && policy()->need_to_start_conc_mark(\"STW humongous allocation\")) {\n+    if (result != nullptr &&\n+        \/\/ We just allocated the humongous object, so the given allocation size is 0.\n+        policy()->need_to_start_conc_mark(\"STW humongous allocation\", 0 \/* allocation_word_size *\/)) {\n@@ -775,1 +825,1 @@\n-  abandon_collection_set(collection_set());\n+  abandon_collection_set();\n@@ -802,0 +852,1 @@\n+  finish_codecache_marking_cycle();\n@@ -811,5 +862,21 @@\n-  \/\/ Discard all remembered set updates and reset refinement statistics.\n-  G1BarrierSet::dirty_card_queue_set().abandon_logs_and_stats();\n-  assert(G1BarrierSet::dirty_card_queue_set().num_cards() == 0,\n-         \"DCQS should be empty\");\n-  concurrent_refine()->get_and_reset_refinement_stats();\n+  G1ConcurrentRefineSweepState& sweep_state = concurrent_refine()->sweep_state();\n+  if (sweep_state.is_in_progress()) {\n+\n+    if (!sweep_state.are_java_threads_synched()) {\n+      \/\/ Synchronize Java threads with global card table that has already been swapped.\n+      class SwapThreadCardTableClosure : public ThreadClosure {\n+      public:\n+\n+        virtual void do_thread(Thread* t) {\n+          G1BarrierSet* bs = G1BarrierSet::g1_barrier_set();\n+          bs->update_card_table_base(t);\n+        }\n+      } cl;\n+      Threads::java_threads_do(&cl);\n+    }\n+\n+    \/\/ Record any available refinement statistics.\n+    policy()->record_refinement_stats(sweep_state.stats());\n+    sweep_state.complete_work(false \/* concurrent *\/, false \/* print_log *\/);\n+  }\n+  sweep_state.reset_stats();\n@@ -827,0 +894,1 @@\n+  _verifier->verify_card_tables_clean(true \/* both_card_tables *\/);\n@@ -846,3 +914,3 @@\n-void G1CollectedHeap::do_full_collection(bool clear_all_soft_refs,\n-                                         bool do_maximal_compaction,\n-                                         size_t allocation_word_size) {\n+void G1CollectedHeap::do_full_collection(size_t allocation_word_size,\n+                                         bool clear_all_soft_refs,\n+                                         bool do_maximal_compaction) {\n@@ -851,3 +919,0 @@\n-  const bool do_clear_all_soft_refs = clear_all_soft_refs ||\n-      soft_ref_policy()->should_clear_all_soft_refs();\n-\n@@ -856,1 +921,1 @@\n-  G1FullCollector collector(this, do_clear_all_soft_refs, do_maximal_compaction, gc_mark.tracer());\n+  G1FullCollector collector(this, clear_all_soft_refs, do_maximal_compaction, gc_mark.tracer());\n@@ -868,3 +933,3 @@\n-  do_full_collection(clear_all_soft_refs,\n-                     false \/* do_maximal_compaction *\/,\n-                     size_t(0) \/* allocation_word_size *\/);\n+  do_full_collection(size_t(0) \/* allocation_word_size *\/,\n+                     clear_all_soft_refs,\n+                     false \/* do_maximal_compaction *\/);\n@@ -876,3 +941,3 @@\n-  do_full_collection(true  \/* clear_all_soft_refs *\/,\n-                     false \/* do_maximal_compaction *\/,\n-                     size_t(0) \/* allocation_word_size *\/);\n+  do_full_collection(size_t(0) \/* allocation_word_size *\/,\n+                     true  \/* clear_all_soft_refs *\/,\n+                     false \/* do_maximal_compaction *\/);\n@@ -916,0 +981,27 @@\n+void G1CollectedHeap::update_gc_overhead_counter() {\n+  assert(SafepointSynchronize::is_at_safepoint(), \"precondition\");\n+\n+  if (!UseGCOverheadLimit) {\n+    return;\n+  }\n+\n+  bool gc_time_over_limit = (_policy->analytics()->long_term_gc_time_ratio() * 100) >= GCTimeLimit;\n+  double free_space_percent = percent_of(num_available_regions() * G1HeapRegion::GrainBytes, max_capacity());\n+  bool free_space_below_limit = free_space_percent < GCHeapFreeLimit;\n+\n+  log_debug(gc)(\"GC Overhead Limit: GC Time %f Free Space %f Counter %zu\",\n+                (_policy->analytics()->long_term_gc_time_ratio() * 100),\n+                free_space_percent,\n+                _gc_overhead_counter);\n+\n+  if (gc_time_over_limit && free_space_below_limit) {\n+    _gc_overhead_counter++;\n+  } else {\n+    _gc_overhead_counter = 0;\n+  }\n+}\n+\n+bool G1CollectedHeap::gc_overhead_limit_exceeded() {\n+  return _gc_overhead_counter >= GCOverheadLimitThreshold;\n+}\n+\n@@ -920,7 +1012,16 @@\n-  \/\/ Let's attempt the allocation first.\n-  HeapWord* result =\n-    attempt_allocation_at_safepoint(word_size,\n-                                    expect_null_mutator_alloc_region);\n-  if (result != nullptr) {\n-    return result;\n-  }\n+  \/\/ Skip allocation if GC overhead limit has been exceeded to let the mutator run\n+  \/\/ into an OOME. It can either exit \"gracefully\" or try to free up memory asap.\n+  \/\/ For the latter situation, keep running GCs. If the mutator frees up enough\n+  \/\/ memory quickly enough, the overhead(s) will go below the threshold(s) again\n+  \/\/ and the VM may continue running.\n+  \/\/ If we did not continue garbage collections, the (gc overhead) limit may decrease\n+  \/\/ enough by itself to not count as exceeding the limit any more, in the worst\n+  \/\/ case bouncing back-and-forth all the time.\n+  if (!gc_overhead_limit_exceeded()) {\n+    \/\/ Let's attempt the allocation first.\n+    HeapWord* result =\n+      attempt_allocation_at_safepoint(word_size,\n+                                      expect_null_mutator_alloc_region);\n+    if (result != nullptr) {\n+      return result;\n+    }\n@@ -928,7 +1029,8 @@\n-  \/\/ In a G1 heap, we're supposed to keep allocation from failing by\n-  \/\/ incremental pauses.  Therefore, at least for now, we'll favor\n-  \/\/ expansion over collection.  (This might change in the future if we can\n-  \/\/ do something smarter than full collection to satisfy a failed alloc.)\n-  result = expand_and_allocate(word_size);\n-  if (result != nullptr) {\n-    return result;\n+    \/\/ In a G1 heap, we're supposed to keep allocation from failing by\n+    \/\/ incremental pauses.  Therefore, at least for now, we'll favor\n+    \/\/ expansion over collection.  (This might change in the future if we can\n+    \/\/ do something smarter than full collection to satisfy a failed alloc.)\n+    result = expand_and_allocate(word_size);\n+    if (result != nullptr) {\n+      return result;\n+    }\n@@ -947,3 +1049,3 @@\n-    do_full_collection(maximal_compaction \/* clear_all_soft_refs *\/,\n-                       maximal_compaction \/* do_maximal_compaction *\/,\n-                       word_size \/* allocation_word_size *\/);\n+    do_full_collection(word_size \/* allocation_word_size *\/,\n+                       maximal_compaction \/* clear_all_soft_refs *\/,\n+                       maximal_compaction \/* do_maximal_compaction *\/);\n@@ -958,0 +1060,4 @@\n+  \/\/ Update GC overhead limits after the initial garbage collection leading to this\n+  \/\/ allocation attempt.\n+  update_gc_overhead_counter();\n+\n@@ -962,1 +1068,1 @@\n-                                     false, \/* maximum_collection *\/\n+                                     false, \/* maximal_compaction *\/\n@@ -972,1 +1078,1 @@\n-                                            true, \/* maximum_collection *\/\n+                                            true, \/* maximal_compaction *\/\n@@ -982,1 +1088,1 @@\n-                                            false, \/* maximum_collection *\/\n+                                            false, \/* maximal_compaction *\/\n@@ -989,2 +1095,3 @@\n-  assert(!soft_ref_policy()->should_clear_all_soft_refs(),\n-         \"Flag should have been handled and cleared prior to this point\");\n+  if (gc_overhead_limit_exceeded()) {\n+    log_info(gc)(\"GC Overhead Limit exceeded too often (%zu).\", GCOverheadLimitThreshold);\n+  }\n@@ -1137,1 +1244,1 @@\n-                FreeList_lock->owned_by_self() || OldSets_lock->owned_by_self(),\n+                G1FreeList_lock->owned_by_self() || G1OldSets_lock->owned_by_self(),\n@@ -1160,1 +1267,1 @@\n-                OldSets_lock->owned_by_self(),\n+                G1OldSets_lock->owned_by_self(),\n@@ -1173,0 +1280,1 @@\n+  _gc_overhead_counter(0),\n@@ -1176,0 +1284,1 @@\n+  _revise_young_length_task(nullptr),\n@@ -1177,1 +1286,5 @@\n-  _card_table(nullptr),\n+  _refinement_epoch(0),\n+  _last_synchronized_start(0),\n+  _last_refinement_epoch_start(0),\n+  _yield_duration_in_refinement_epoch(0),\n+  _last_safepoint_refinement_epoch(0),\n@@ -1208,1 +1321,1 @@\n-  _young_regions_cset_group(card_set_config(), &_card_set_freelist_pool, 1u \/* group_id *\/),\n+  _young_regions_cset_group(card_set_config(), &_card_set_freelist_pool, G1CSetCandidateGroup::YoungRegionId),\n@@ -1297,1 +1410,1 @@\n-  _cr = G1ConcurrentRefine::create(policy(), &ecode);\n+  _cr = G1ConcurrentRefine::create(this, &ecode);\n@@ -1353,6 +1466,2 @@\n-  G1CardTable* ct = new G1CardTable(_reserved);\n-  G1BarrierSet* bs = new G1BarrierSet(ct);\n-  bs->initialize();\n-  assert(bs->is_a(BarrierSet::G1BarrierSet), \"sanity\");\n-  BarrierSet::set_barrier_set(bs);\n-  _card_table = ct;\n+  G1CardTable* card_table = new G1CardTable(_reserved);\n+  G1CardTable* refinement_table = new G1CardTable(_reserved);\n@@ -1360,5 +1469,2 @@\n-  {\n-    G1SATBMarkQueueSet& satbqs = bs->satb_mark_queue_set();\n-    satbqs.set_process_completed_buffers_threshold(G1SATBProcessCompletedThreshold);\n-    satbqs.set_buffer_enqueue_threshold_percentage(G1SATBBufferEnqueueingThresholdPercent);\n-  }\n+  G1BarrierSet* bs = new G1BarrierSet(card_table, refinement_table);\n+  assert(bs->is_a(BarrierSet::G1BarrierSet), \"sanity\");\n@@ -1399,0 +1505,5 @@\n+  G1RegionToSpaceMapper* refinement_cards_storage =\n+    create_aux_memory_mapper(\"Refinement Card Table\",\n+                             G1CardTable::compute_size(heap_rs.size() \/ HeapWordSize),\n+                             G1CardTable::heap_map_factor());\n+\n@@ -1403,2 +1514,11 @@\n-  _hrm.initialize(heap_storage, bitmap_storage, bot_storage, cardtable_storage);\n-  _card_table->initialize(cardtable_storage);\n+  _hrm.initialize(heap_storage, bitmap_storage, bot_storage, cardtable_storage, refinement_cards_storage);\n+  card_table->initialize(cardtable_storage);\n+  refinement_table->initialize(refinement_cards_storage);\n+\n+  BarrierSet::set_barrier_set(bs);\n+\n+  {\n+    G1SATBMarkQueueSet& satbqs = bs->satb_mark_queue_set();\n+    satbqs.set_process_completed_buffers_threshold(G1SATBProcessCompletedThreshold);\n+    satbqs.set_buffer_enqueue_threshold_percentage(G1SATBBufferEnqueueingThresholdPercent);\n+  }\n@@ -1416,1 +1536,1 @@\n-  _rem_set = new G1RemSet(this, _card_table);\n+  _rem_set = new G1RemSet(this);\n@@ -1475,0 +1595,5 @@\n+  if (policy()->use_adaptive_young_list_length()) {\n+    _revise_young_length_task = new G1ReviseYoungLengthTask(\"Revise Young Length List Task\");\n+    _service_thread->register_task(_revise_young_length_task);\n+  }\n+\n@@ -1496,0 +1621,2 @@\n+  start_new_collection_set();\n+\n@@ -1501,0 +1628,1 @@\n+  CPUTimeCounters::create_counter(CPUTimeGroups::CPUTimeType::gc_conc_refine_control);\n@@ -1525,0 +1653,2 @@\n+\n+  _last_synchronized_start = os::elapsed_counter();\n@@ -1528,0 +1658,11 @@\n+  jlong now = os::elapsed_counter();\n+  jlong synchronize_duration = now - _last_synchronized_start;\n+\n+  if (_last_safepoint_refinement_epoch == _refinement_epoch) {\n+    _yield_duration_in_refinement_epoch += synchronize_duration;\n+  } else {\n+    _last_refinement_epoch_start = now;\n+    _last_safepoint_refinement_epoch = _refinement_epoch;\n+    _yield_duration_in_refinement_epoch = 0;\n+  }\n+\n@@ -1531,0 +1672,10 @@\n+void G1CollectedHeap::set_last_refinement_epoch_start(jlong epoch_start, jlong last_yield_duration) {\n+  _last_refinement_epoch_start = epoch_start;\n+  guarantee(_yield_duration_in_refinement_epoch >= last_yield_duration, \"should be\");\n+  _yield_duration_in_refinement_epoch -= last_yield_duration;\n+}\n+\n+jlong G1CollectedHeap::yield_duration_in_refinement_epoch() {\n+  return _yield_duration_in_refinement_epoch;\n+}\n+\n@@ -1702,7 +1853,1 @@\n-\/\/ Helper for collect().\n-static G1GCCounters collection_counters(G1CollectedHeap* g1h) {\n-  MutexLocker ml(Heap_lock);\n-  return G1GCCounters(g1h);\n-}\n-\n-  try_collect(cause, collection_counters(this));\n+  try_collect(0 \/* allocation_word_size *\/, cause, collection_counters(this));\n@@ -1735,1 +1880,62 @@\n-bool G1CollectedHeap::try_collect_concurrently(GCCause::Cause cause,\n+bool G1CollectedHeap::wait_full_mark_finished(GCCause::Cause cause,\n+                                              uint old_marking_started_before,\n+                                              uint old_marking_started_after,\n+                                              uint old_marking_completed_after) {\n+  \/\/ Request is finished if a full collection (concurrent or stw)\n+  \/\/ was started after this request and has completed, e.g.\n+  \/\/ started_before < completed_after.\n+  if (gc_counter_less_than(old_marking_started_before,\n+                           old_marking_completed_after)) {\n+    LOG_COLLECT_CONCURRENTLY_COMPLETE(cause, true);\n+    return true;\n+  }\n+\n+  if (old_marking_started_after != old_marking_completed_after) {\n+    \/\/ If there is an in-progress cycle (possibly started by us), then\n+    \/\/ wait for that cycle to complete, e.g.\n+    \/\/ while completed_now < started_after.\n+    LOG_COLLECT_CONCURRENTLY(cause, \"wait\");\n+    MonitorLocker ml(G1OldGCCount_lock);\n+    while (gc_counter_less_than(_old_marking_cycles_completed,\n+                                old_marking_started_after)) {\n+      ml.wait();\n+    }\n+    \/\/ Request is finished if the collection we just waited for was\n+    \/\/ started after this request.\n+    if (old_marking_started_before != old_marking_started_after) {\n+      LOG_COLLECT_CONCURRENTLY(cause, \"complete after wait\");\n+      return true;\n+    }\n+  }\n+  return false;\n+}\n+\n+\/\/ After calling wait_full_mark_finished(), this method determines whether we\n+\/\/ previously failed for ordinary reasons (concurrent cycle in progress, whitebox\n+\/\/ has control). Returns if this has been such an ordinary reason.\n+static bool should_retry_vm_op(GCCause::Cause cause,\n+                               VM_G1TryInitiateConcMark* op) {\n+  if (op->cycle_already_in_progress()) {\n+    \/\/ If VMOp failed because a cycle was already in progress, it\n+    \/\/ is now complete.  But it didn't finish this user-requested\n+    \/\/ GC, so try again.\n+    LOG_COLLECT_CONCURRENTLY(cause, \"retry after in-progress\");\n+    return true;\n+  } else if (op->whitebox_attached()) {\n+    \/\/ If WhiteBox wants control, wait for notification of a state\n+    \/\/ change in the controller, then try again.  Don't wait for\n+    \/\/ release of control, since collections may complete while in\n+    \/\/ control.  Note: This won't recognize a STW full collection\n+    \/\/ while waiting; we can't wait on multiple monitors.\n+    LOG_COLLECT_CONCURRENTLY(cause, \"whitebox control stall\");\n+    MonitorLocker ml(ConcurrentGCBreakpoints::monitor());\n+    if (ConcurrentGCBreakpoints::is_controlled()) {\n+      ml.wait();\n+    }\n+    return true;\n+  }\n+  return false;\n+}\n+\n+bool G1CollectedHeap::try_collect_concurrently(size_t allocation_word_size,\n+                                               GCCause::Cause cause,\n@@ -1746,1 +1952,1 @@\n-    VM_G1TryInitiateConcMark op(gc_counter, cause);\n+    VM_G1TryInitiateConcMark op(allocation_word_size, gc_counter, cause);\n@@ -1756,2 +1962,2 @@\n-    \/\/ we're terminating, then we're done.\n-    if (op.terminating()) {\n+    \/\/ we're shutting down, then we're done.\n+    if (op.is_shutting_down()) {\n@@ -1795,0 +2001,35 @@\n+    } else if (GCCause::is_codecache_requested_gc(cause)) {\n+      assert(allocation_word_size == 0, \"must be\");\n+      \/\/ For a CodeCache requested GC, before marking, progress is ensured as the\n+      \/\/ following Remark pause unloads code (and signals the requester such).\n+      \/\/ Otherwise we must ensure that it is restarted.\n+      \/\/\n+      \/\/ For a CodeCache requested GC, a successful GC operation means that\n+      \/\/ (1) marking is in progress. I.e. the VMOp started the marking or a\n+      \/\/     Remark pause is pending from a different VM op; we will potentially\n+      \/\/     abort a mixed phase if needed.\n+      \/\/ (2) a new cycle was started (by this thread or some other), or\n+      \/\/ (3) a Full GC was performed.\n+      \/\/\n+      \/\/ Cases (2) and (3) are detected together by a change to\n+      \/\/ _old_marking_cycles_started.\n+      \/\/\n+      \/\/ Compared to other \"automatic\" GCs (see below), we do not consider being\n+      \/\/ in whitebox as sufficient too because we might be anywhere within that\n+      \/\/ cycle and we need to make progress.\n+      if (op.mark_in_progress() ||\n+          (old_marking_started_before != old_marking_started_after)) {\n+        LOG_COLLECT_CONCURRENTLY_COMPLETE(cause, true);\n+        return true;\n+      }\n+\n+      if (wait_full_mark_finished(cause,\n+                                  old_marking_started_before,\n+                                  old_marking_started_after,\n+                                  old_marking_completed_after)) {\n+        return true;\n+      }\n+\n+      if (should_retry_vm_op(cause, &op)) {\n+        continue;\n+      }\n@@ -1796,0 +2037,4 @@\n+      assert(cause == GCCause::_g1_humongous_allocation ||\n+             cause == GCCause::_g1_periodic_collection,\n+             \"Unsupported cause %s\", GCCause::to_string(cause));\n+\n@@ -1807,5 +2052,0 @@\n-      \/\/\n-      \/\/ Note that (1) does not imply (4).  If we're still in the mixed\n-      \/\/ phase of an earlier concurrent collection, the request to make the\n-      \/\/ collection a concurrent start won't be honored.  If we don't check for\n-      \/\/ both conditions we'll spin doing back-to-back collections.\n@@ -1835,6 +2075,4 @@\n-      \/\/ Request is finished if a full collection (concurrent or stw)\n-      \/\/ was started after this request and has completed, e.g.\n-      \/\/ started_before < completed_after.\n-      if (gc_counter_less_than(old_marking_started_before,\n-                               old_marking_completed_after)) {\n-        LOG_COLLECT_CONCURRENTLY_COMPLETE(cause, true);\n+      if (wait_full_mark_finished(cause,\n+                                  old_marking_started_before,\n+                                  old_marking_started_after,\n+                                  old_marking_completed_after)) {\n@@ -1844,18 +2082,0 @@\n-      if (old_marking_started_after != old_marking_completed_after) {\n-        \/\/ If there is an in-progress cycle (possibly started by us), then\n-        \/\/ wait for that cycle to complete, e.g.\n-        \/\/ while completed_now < started_after.\n-        LOG_COLLECT_CONCURRENTLY(cause, \"wait\");\n-        MonitorLocker ml(G1OldGCCount_lock);\n-        while (gc_counter_less_than(_old_marking_cycles_completed,\n-                                    old_marking_started_after)) {\n-          ml.wait();\n-        }\n-        \/\/ Request is finished if the collection we just waited for was\n-        \/\/ started after this request.\n-        if (old_marking_started_before != old_marking_started_after) {\n-          LOG_COLLECT_CONCURRENTLY(cause, \"complete after wait\");\n-          return true;\n-        }\n-      }\n-\n@@ -1868,17 +2088,1 @@\n-      if (op.cycle_already_in_progress()) {\n-        \/\/ If VMOp failed because a cycle was already in progress, it\n-        \/\/ is now complete.  But it didn't finish this user-requested\n-        \/\/ GC, so try again.\n-        LOG_COLLECT_CONCURRENTLY(cause, \"retry after in-progress\");\n-        continue;\n-      } else if (op.whitebox_attached()) {\n-        \/\/ If WhiteBox wants control, wait for notification of a state\n-        \/\/ change in the controller, then try again.  Don't wait for\n-        \/\/ release of control, since collections may complete while in\n-        \/\/ control.  Note: This won't recognize a STW full collection\n-        \/\/ while waiting; we can't wait on multiple monitors.\n-        LOG_COLLECT_CONCURRENTLY(cause, \"whitebox control stall\");\n-        MonitorLocker ml(ConcurrentGCBreakpoints::monitor());\n-        if (ConcurrentGCBreakpoints::is_controlled()) {\n-          ml.wait();\n-        }\n+      if (should_retry_vm_op(cause, &op)) {\n@@ -1896,1 +2100,2 @@\n-bool G1CollectedHeap::try_collect(GCCause::Cause cause,\n+bool G1CollectedHeap::try_collect(size_t allocation_word_size,\n+                                  GCCause::Cause cause,\n@@ -1899,1 +2104,2 @@\n-    return try_collect_concurrently(cause,\n+    return try_collect_concurrently(allocation_word_size,\n+                                    cause,\n@@ -1905,0 +2111,1 @@\n+    assert(allocation_word_size == 0, \"must be\");\n@@ -1913,0 +2120,3 @@\n+    \/\/ The only path to get here is because of a periodic collection using a Full GC\n+    \/\/ or WhiteBox full gc.\n+    assert(allocation_word_size == 0, \"must be\");\n@@ -1929,1 +2139,1 @@\n-    do_collection_pause_at_safepoint();\n+    do_collection_pause_at_safepoint(0 \/* allocation_word_size *\/);\n@@ -2066,1 +2276,1 @@\n-size_t G1CollectedHeap::tlab_capacity(Thread* ignored) const {\n+size_t G1CollectedHeap::tlab_capacity() const {\n@@ -2070,1 +2280,1 @@\n-size_t G1CollectedHeap::tlab_used(Thread* ignored) const {\n+size_t G1CollectedHeap::tlab_used() const {\n@@ -2080,1 +2290,1 @@\n-size_t G1CollectedHeap::unsafe_max_tlab_alloc(Thread* ignored) const {\n+size_t G1CollectedHeap::unsafe_max_tlab_alloc() const {\n@@ -2143,0 +2353,4 @@\n+static void print_region_type(outputStream* st, const char* type, uint count, bool last = false) {\n+  st->print(\"%u %s (%zuM)%s\", count, type, count * G1HeapRegion::GrainBytes \/ M, last ? \"\\n\" : \", \");\n+}\n+\n@@ -2144,1 +2358,2 @@\n-  size_t heap_used = Heap_lock->owned_by_self() ? used() : used_unlocked();\n+  size_t heap_used = (Thread::current_or_null_safe() != nullptr &&\n+                      Heap_lock->owned_by_self()) ? used() : used_unlocked();\n@@ -2154,8 +2369,7 @@\n-  st->print(\"region size %zuK, \", G1HeapRegion::GrainBytes \/ K);\n-  uint young_regions = young_regions_count();\n-  st->print(\"%u young (%zuK), \", young_regions,\n-            (size_t) young_regions * G1HeapRegion::GrainBytes \/ K);\n-  uint survivor_regions = survivor_regions_count();\n-  st->print(\"%u survivors (%zuK)\", survivor_regions,\n-            (size_t) survivor_regions * G1HeapRegion::GrainBytes \/ K);\n-  st->cr();\n+  st->print(\"region size %zuM, \", G1HeapRegion::GrainBytes \/ M);\n+  print_region_type(st, \"eden\", eden_regions_count());\n+  print_region_type(st, \"survivor\", survivor_regions_count());\n+  print_region_type(st, \"old\", old_regions_count());\n+  print_region_type(st, \"humongous\", humongous_regions_count());\n+  print_region_type(st, \"free\", num_free_regions(), true \/* last *\/);\n+\n@@ -2285,0 +2499,1 @@\n+  _refinement_epoch++;\n@@ -2332,1 +2547,1 @@\n-  MutexLocker x(CGC_lock, Mutex::_no_safepoint_check_flag);\n+  MutexLocker x(G1CGC_lock, Mutex::_no_safepoint_check_flag);\n@@ -2340,1 +2555,1 @@\n-  CGC_lock->notify();\n+  G1CGC_lock->notify();\n@@ -2352,1 +2567,1 @@\n-void G1CollectedHeap::verify_region_attr_remset_is_tracked() {\n+void G1CollectedHeap::verify_region_attr_is_remset_tracked() {\n@@ -2357,4 +2572,6 @@\n-      bool const remset_is_tracked = g1h->region_attr(r->bottom()).remset_is_tracked();\n-      assert(r->rem_set()->is_tracked() == remset_is_tracked,\n-             \"Region %u remset tracking status (%s) different to region attribute (%s)\",\n-             r->hrm_index(), BOOL_TO_STR(r->rem_set()->is_tracked()), BOOL_TO_STR(remset_is_tracked));\n+      G1HeapRegionAttr attr = g1h->region_attr(r->bottom());\n+      bool const is_remset_tracked = attr.is_remset_tracked();\n+      assert((r->rem_set()->is_tracked() == is_remset_tracked) ||\n+             (attr.is_new_survivor() && is_remset_tracked),\n+             \"Region %u (%s) remset tracking status (%s) different to region attribute (%s)\",\n+             r->hrm_index(), r->get_type_str(), BOOL_TO_STR(r->rem_set()->is_tracked()), BOOL_TO_STR(is_remset_tracked));\n@@ -2389,1 +2606,1 @@\n-  collection_set()->start_incremental_building();\n+  collection_set()->start();\n@@ -2411,1 +2628,0 @@\n-  _verifier->verify_dirty_young_regions();\n@@ -2433,0 +2649,1 @@\n+  _verifier->verify_free_regions_card_tables_clean();\n@@ -2437,7 +2654,0 @@\n-void G1CollectedHeap::do_collection_pause_at_safepoint(size_t allocation_word_size) {\n-  assert_at_safepoint_on_vm_thread();\n-  guarantee(!is_stw_gc_active(), \"collection is not reentrant\");\n-\n-  do_collection_pause_at_safepoint_helper(allocation_word_size);\n-}\n-\n@@ -2505,1 +2715,4 @@\n-void G1CollectedHeap::do_collection_pause_at_safepoint_helper(size_t allocation_word_size) {\n+void G1CollectedHeap::do_collection_pause_at_safepoint(size_t allocation_word_size) {\n+  assert_at_safepoint_on_vm_thread();\n+  assert(!is_stw_gc_active(), \"collection is not reentrant\");\n+\n@@ -2542,2 +2755,1 @@\n-  uint num_workers = workers()->active_workers();\n-  G1ParallelCleaningTask unlink_task(num_workers, class_unloading_occurred);\n+  G1ParallelCleaningTask unlink_task(class_unloading_occurred);\n@@ -2677,0 +2889,5 @@\n+  if (VerifyDuringGC) {\n+    \/\/ Card and refinement table must be clear for freed regions.\n+    card_table()->verify_region(MemRegion(hr->bottom(), hr->end()), G1CardTable::clean_card_val(), true);\n+    refinement_table()->verify_region(MemRegion(hr->bottom(), hr->end()), G1CardTable::clean_card_val(), true);\n+  }\n@@ -2694,1 +2911,1 @@\n-    MutexLocker x(OldSets_lock, Mutex::_no_safepoint_check_flag);\n+    MutexLocker x(G1OldSets_lock, Mutex::_no_safepoint_check_flag);\n@@ -2704,1 +2921,1 @@\n-    MutexLocker x(FreeList_lock, Mutex::_no_safepoint_check_flag);\n+    MutexLocker x(G1FreeList_lock, Mutex::_no_safepoint_check_flag);\n@@ -2737,1 +2954,1 @@\n-void G1CollectedHeap::abandon_collection_set(G1CollectionSet* collection_set) {\n+void G1CollectedHeap::abandon_collection_set() {\n@@ -2741,2 +2958,1 @@\n-  collection_set->clear();\n-  collection_set->stop_incremental_building();\n+  collection_set()->abandon();\n@@ -2745,2 +2961,7 @@\n-bool G1CollectedHeap::is_old_gc_alloc_region(G1HeapRegion* hr) {\n-  return _allocator->is_retained_old_region(hr);\n+size_t G1CollectedHeap::non_young_occupancy_after_allocation(size_t allocation_word_size) {\n+  const size_t cur_occupancy = (old_regions_count() + humongous_regions_count()) * G1HeapRegion::GrainBytes -\n+                               _allocator->free_bytes_in_retained_old_region();\n+  \/\/ Humongous allocations will always be assigned to non-young heap, so consider\n+  \/\/ that allocation in the result as well. Otherwise the allocation will always\n+  \/\/ be in young gen, so there is no need to account it here.\n+  return cur_occupancy + (is_humongous(allocation_word_size) ? allocation_used_bytes(allocation_word_size) : 0);\n@@ -2749,4 +2970,2 @@\n-void G1CollectedHeap::set_region_short_lived_locked(G1HeapRegion* hr) {\n-  _eden.add(hr);\n-  _policy->set_region_eden(hr);\n-  young_regions_cset_group()->add(hr);\n+bool G1CollectedHeap::is_old_gc_alloc_region(G1HeapRegion* hr) {\n+  return _allocator->is_retained_old_region(hr);\n@@ -2900,1 +3119,1 @@\n-                                                false \/* do_expand *\/,\n+                                                policy()->should_expand_on_mutator_allocation() \/* do_expand *\/,\n@@ -2903,1 +3122,5 @@\n-      set_region_short_lived_locked(new_alloc_region);\n+      new_alloc_region->set_eden();\n+      _eden.add(new_alloc_region);\n+      _policy->set_region_eden(new_alloc_region);\n+\n+      collection_set()->add_eden_region(new_alloc_region);\n@@ -2905,1 +3128,0 @@\n-      _policy->remset_tracker()->update_at_allocate(new_alloc_region);\n@@ -2917,1 +3139,0 @@\n-  collection_set()->add_eden_region(alloc_region);\n@@ -2939,1 +3160,1 @@\n-  assert(FreeList_lock->owned_by_self(), \"pre-condition\");\n+  assert(G1FreeList_lock->owned_by_self(), \"pre-condition\");\n@@ -2961,0 +3182,6 @@\n+      \/\/ The remembered set\/group cardset for this region will be installed at the\n+      \/\/ end of GC. Cannot do that right now because we still need the current young\n+      \/\/ gen cardset group.\n+      \/\/ However, register with the attribute table to collect remembered set entries\n+      \/\/ immediately as it is the only source for determining the need for remembered\n+      \/\/ set tracking during GC.\n@@ -2962,2 +3189,0 @@\n-      \/\/ Install the group cardset.\n-      young_regions_cset_group()->add(new_alloc_region);\n@@ -2966,0 +3191,4 @@\n+      \/\/ Update remembered set\/cardset.\n+      _policy->remset_tracker()->update_at_allocate(new_alloc_region);\n+      \/\/ Synchronize with region attribute table.\n+      update_region_attr(new_alloc_region);\n@@ -2967,2 +3196,0 @@\n-    _policy->remset_tracker()->update_at_allocate(new_alloc_region);\n-    register_region_with_region_attr(new_alloc_region);\n@@ -3029,0 +3256,2 @@\n+  BarrierSetNMethod* bs_nm = BarrierSet::barrier_set()->barrier_set_nmethod();\n+  bs_nm->disarm(nm);\n@@ -3105,6 +3334,0 @@\n-\n-void G1CollectedHeap::prepare_group_cardsets_for_scan() {\n-  young_regions_cardset()->reset_table_scanner_for_groups();\n-\n-  collection_set()->prepare_groups_for_scan();\n-}\n","filename":"src\/hotspot\/share\/gc\/g1\/g1CollectedHeap.cpp","additions":425,"deletions":202,"binary":false,"changes":627,"status":"modified"},{"patch":"@@ -231,2 +231,0 @@\n-\n-  G1CollectedHeap::finish_codecache_marking_cycle();\n@@ -253,3 +251,1 @@\n-  _heap->young_regions_cset_group()->clear();\n-\n-  _heap->policy()->record_full_collection_end();\n+  _heap->policy()->record_full_collection_end(allocation_word_size);\n","filename":"src\/hotspot\/share\/gc\/g1\/g1FullCollector.cpp","additions":1,"deletions":5,"binary":false,"changes":6,"status":"modified"},{"patch":"@@ -152,0 +152,4 @@\n+    if (VerifyDuringGC) {\n+      \/\/ Satisfy some asserts in free_..._region\n+      hr->clear_both_card_tables();\n+    }\n","filename":"src\/hotspot\/share\/gc\/g1\/g1FullGCCompactTask.cpp","additions":4,"deletions":0,"binary":false,"changes":4,"status":"modified"},{"patch":"@@ -45,1 +45,1 @@\n-#include \"runtime\/atomic.hpp\"\n+#include \"runtime\/atomicAccess.hpp\"\n@@ -60,1 +60,0 @@\n-                                           G1RedirtyCardsQueueSet* rdcqs,\n@@ -67,2 +66,1 @@\n-    _rdc_local_qset(rdcqs),\n-    _ct(g1h->card_table()),\n+    _ct(g1h->refinement_table()),\n@@ -75,1 +73,2 @@\n-    _last_enqueued_card(SIZE_MAX),\n+    _num_cards_marked_dirty(0),\n+    _num_cards_marked_to_cset(0),\n@@ -91,1 +90,1 @@\n-    _evac_failure_enqueued_cards(0)\n+    _num_cards_from_evac_failure(0)\n@@ -115,2 +114,1 @@\n-size_t G1ParScanThreadState::flush_stats(size_t* surviving_young_words, uint num_workers, BufferNodeList* rdc_buffers) {\n-  *rdc_buffers = _rdc_local_qset.flush();\n+size_t G1ParScanThreadState::flush_stats(size_t* surviving_young_words, uint num_workers) {\n@@ -150,2 +148,10 @@\n-size_t G1ParScanThreadState::evac_failure_enqueued_cards() const {\n-  return _evac_failure_enqueued_cards;\n+size_t G1ParScanThreadState::num_cards_pending() const {\n+  return _num_cards_marked_dirty + _num_cards_from_evac_failure;\n+}\n+\n+size_t G1ParScanThreadState::num_cards_marked() const {\n+  return num_cards_pending() + _num_cards_marked_to_cset;\n+}\n+\n+size_t G1ParScanThreadState::num_cards_from_evac_failure() const {\n+  return _num_cards_from_evac_failure;\n@@ -233,1 +239,1 @@\n-  G1SkipCardEnqueueSetter x(&_scanner, dest_attr.is_new_survivor());\n+  G1SkipCardMarkSetter x(&_scanner, dest_attr.is_new_survivor());\n@@ -253,1 +259,1 @@\n-  assert(_scanner.skip_card_enqueue_set(), \"must be\");\n+  assert(_scanner.skip_card_mark_set(), \"must be\");\n@@ -454,1 +460,1 @@\n-    assert(_scanner.skip_card_enqueue_set(), \"must be\");\n+    assert(_scanner.skip_card_mark_set(), \"must be\");\n@@ -552,1 +558,1 @@\n-      G1SkipCardEnqueueSetter x(&_scanner, dest_attr.is_young());\n+      G1SkipCardMarkSetter x(&_scanner, dest_attr.is_young());\n@@ -575,1 +581,1 @@\n-      new G1ParScanThreadState(_g1h, rdcqs(),\n+      new G1ParScanThreadState(_g1h,\n@@ -601,2 +607,5 @@\n-    size_t copied_bytes = pss->flush_stats(_surviving_young_words_total, _num_workers, &_rdc_buffers[worker_id]) * HeapWordSize;\n-    size_t evac_fail_enqueued_cards = pss->evac_failure_enqueued_cards();\n+    size_t copied_bytes = pss->flush_stats(_surviving_young_words_total, _num_workers) * HeapWordSize;\n+    size_t pending_cards = pss->num_cards_pending();\n+    size_t to_young_gen_cards = pss->num_cards_marked() - pss->num_cards_pending();\n+    size_t evac_failure_cards = pss->num_cards_from_evac_failure();\n+    size_t marked_cards = pss->num_cards_marked();\n@@ -607,1 +616,4 @@\n-    p->record_or_add_thread_work_item(G1GCPhaseTimes::MergePSS, worker_id, evac_fail_enqueued_cards, G1GCPhaseTimes::MergePSSEvacFailExtra);\n+    p->record_or_add_thread_work_item(G1GCPhaseTimes::MergePSS, worker_id, pending_cards, G1GCPhaseTimes::MergePSSPendingCards);\n+    p->record_or_add_thread_work_item(G1GCPhaseTimes::MergePSS, worker_id, to_young_gen_cards, G1GCPhaseTimes::MergePSSToYoungGenCards);\n+    p->record_or_add_thread_work_item(G1GCPhaseTimes::MergePSS, worker_id, evac_failure_cards, G1GCPhaseTimes::MergePSSEvacFail);\n+    p->record_or_add_thread_work_item(G1GCPhaseTimes::MergePSS, worker_id, marked_cards, G1GCPhaseTimes::MergePSSMarked);\n@@ -613,4 +625,0 @@\n-  G1DirtyCardQueueSet& dcq = G1BarrierSet::dirty_card_queue_set();\n-  dcq.merge_bufferlists(rdcqs());\n-  rdcqs()->verify_empty();\n-\n@@ -658,1 +666,1 @@\n-      G1SkipCardEnqueueSetter x(&_scanner, false \/* skip_card_enqueue *\/);\n+      G1SkipCardMarkSetter x(&_scanner, false \/* skip_card_mark *\/);\n@@ -715,2 +723,0 @@\n-    _rdcqs(G1BarrierSet::dirty_card_queue_set().allocator()),\n-    _rdc_buffers(NEW_C_HEAP_ARRAY(BufferNodeList, num_workers, mtGC)),\n@@ -725,1 +731,0 @@\n-    _rdc_buffers[i] = BufferNodeList();\n@@ -734,1 +739,0 @@\n-  FREE_C_HEAP_ARRAY(BufferNodeList, _rdc_buffers);\n","filename":"src\/hotspot\/share\/gc\/g1\/g1ParScanThreadState.cpp","additions":31,"deletions":27,"binary":false,"changes":58,"status":"modified"},{"patch":"@@ -98,5 +98,0 @@\n-\/\/ The alignment used for spaces in young gen and old gen\n-static size_t default_space_alignment() {\n-  return 64 * K * HeapWordSize;\n-}\n-\n@@ -106,1 +101,1 @@\n-  SpaceAlignment = default_space_alignment();\n+  SpaceAlignment = ParallelScavengeHeap::default_space_alignment();\n@@ -118,0 +113,6 @@\n+  if (!UseLargePages) {\n+    ParallelScavengeHeap::set_desired_page_size(os::vm_page_size());\n+    return;\n+  }\n+\n+  \/\/ If using large-page, need to update SpaceAlignment so that spaces are page-size aligned.\n@@ -120,0 +121,6 @@\n+  ParallelScavengeHeap::set_desired_page_size(page_sz);\n+\n+  if (page_sz == os::vm_page_size()) {\n+    log_warning(gc, heap)(\"MinHeapSize (%zu) must be large enough for 4 * page-size; Disabling UseLargePages for heap\", MinHeapSize);\n+    return;\n+  }\n@@ -121,3 +128,2 @@\n-  \/\/ Can a page size be something else than a power of two?\n-  assert(is_power_of_2((intptr_t)page_sz), \"must be a power of 2\");\n-  size_t new_alignment = align_up(page_sz, SpaceAlignment);\n+  \/\/ Space is largepage-aligned.\n+  size_t new_alignment = page_sz;\n","filename":"src\/hotspot\/share\/gc\/parallel\/parallelArguments.cpp","additions":15,"deletions":9,"binary":false,"changes":24,"status":"modified"},{"patch":"@@ -36,0 +36,1 @@\n+#include \"gc\/shared\/barrierSetNMethod.hpp\"\n@@ -52,0 +53,1 @@\n+#include \"runtime\/globals_extension.hpp\"\n@@ -53,0 +55,1 @@\n+#include \"runtime\/init.hpp\"\n@@ -62,1 +65,2 @@\n-PSGCAdaptivePolicyCounters* ParallelScavengeHeap::_gc_policy_counters = nullptr;\n+GCPolicyCounters* ParallelScavengeHeap::_gc_policy_counters = nullptr;\n+size_t ParallelScavengeHeap::_desired_page_size = 0;\n@@ -67,1 +71,7 @@\n-  ReservedHeapSpace heap_rs = Universe::reserve_heap(reserved_heap_size, HeapAlignment);\n+  assert(_desired_page_size != 0, \"Should be initialized\");\n+  ReservedHeapSpace heap_rs = Universe::reserve_heap(reserved_heap_size, HeapAlignment, _desired_page_size);\n+  \/\/ Adjust SpaceAlignment based on actually used large page size.\n+  if (UseLargePages) {\n+    SpaceAlignment = MAX2(heap_rs.page_size(), default_space_alignment());\n+  }\n+  assert(is_aligned(SpaceAlignment, heap_rs.page_size()), \"inv\");\n@@ -81,1 +91,0 @@\n-  barrier_set->initialize();\n@@ -104,11 +113,2 @@\n-  const size_t eden_capacity = _young_gen->eden_space()->capacity_in_bytes();\n-  const size_t old_capacity = _old_gen->capacity_in_bytes();\n-  const size_t initial_promo_size = MIN2(eden_capacity, old_capacity);\n-  _size_policy =\n-    new PSAdaptiveSizePolicy(eden_capacity,\n-                             initial_promo_size,\n-                             young_gen()->to_space()->capacity_in_bytes(),\n-                             SpaceAlignment,\n-                             max_gc_pause_sec,\n-                             GCTimeRatio\n-                             );\n+  _size_policy = new PSAdaptiveSizePolicy(SpaceAlignment,\n+                                          max_gc_pause_sec);\n@@ -120,2 +120,1 @@\n-  _gc_policy_counters =\n-    new PSGCAdaptivePolicyCounters(\"ParScav:MSC\", 2, 2, _size_policy);\n+  _gc_policy_counters = new GCPolicyCounters(\"ParScav:MSC\", 2, 2);\n@@ -170,11 +169,0 @@\n-void ParallelScavengeHeap::safepoint_synchronize_begin() {\n-  if (UseStringDeduplication) {\n-    SuspendibleThreadSet::synchronize();\n-  }\n-}\n-\n-void ParallelScavengeHeap::safepoint_synchronize_end() {\n-  if (UseStringDeduplication) {\n-    SuspendibleThreadSet::desynchronize();\n-  }\n-}\n@@ -204,0 +192,15 @@\n+void ParallelScavengeHeap::gc_epilogue(bool full) {\n+  if (_is_heap_almost_full) {\n+    \/\/ Reset emergency state if eden is empty after a young\/full gc\n+    if (_young_gen->eden_space()->is_empty()) {\n+      log_debug(gc)(\"Leaving memory constrained state; back to normal\");\n+      _is_heap_almost_full = false;\n+    }\n+  } else {\n+    if (full && !_young_gen->eden_space()->is_empty()) {\n+      log_debug(gc)(\"Non-empty young-gen after full-gc; in memory constrained state\");\n+      _is_heap_almost_full = true;\n+    }\n+  }\n+}\n+\n@@ -273,2 +276,1 @@\n-HeapWord* ParallelScavengeHeap::mem_allocate(size_t size,\n-                                             bool* gc_overhead_limit_was_exceeded) {\n+HeapWord* ParallelScavengeHeap::mem_allocate(size_t size) {\n@@ -280,1 +282,1 @@\n-  return mem_allocate_work(size, is_tlab, gc_overhead_limit_was_exceeded);\n+  return mem_allocate_work(size, is_tlab);\n@@ -283,3 +285,30 @@\n-HeapWord* ParallelScavengeHeap::mem_allocate_work(size_t size,\n-                                                  bool is_tlab,\n-                                                  bool* gc_overhead_limit_was_exceeded) {\n+HeapWord* ParallelScavengeHeap::mem_allocate_cas_noexpand(size_t size, bool is_tlab) {\n+  \/\/ Try young-gen first.\n+  HeapWord* result = young_gen()->allocate(size);\n+  if (result != nullptr) {\n+    return result;\n+  }\n+\n+  \/\/ Try allocating from the old gen for non-TLAB and large allocations.\n+  if (!is_tlab) {\n+    if (!should_alloc_in_eden(size)) {\n+      result = old_gen()->cas_allocate_noexpand(size);\n+      if (result != nullptr) {\n+        return result;\n+      }\n+    }\n+  }\n+\n+  \/\/ In extreme cases, try allocating in from space also.\n+  if (_is_heap_almost_full) {\n+    result = young_gen()->from_space()->cas_allocate(size);\n+    if (result != nullptr) {\n+      return result;\n+    }\n+    if (!is_tlab) {\n+      result = old_gen()->cas_allocate_noexpand(size);\n+      if (result != nullptr) {\n+        return result;\n+      }\n+    }\n+  }\n@@ -287,4 +316,2 @@\n-  \/\/ In general gc_overhead_limit_was_exceeded should be false so\n-  \/\/ set it so here and reset it to true only if the gc time\n-  \/\/ limit is being exceeded as checked below.\n-  *gc_overhead_limit_was_exceeded = false;\n+  return nullptr;\n+}\n@@ -292,1 +319,6 @@\n-  HeapWord* result = young_gen()->allocate(size);\n+HeapWord* ParallelScavengeHeap::mem_allocate_work(size_t size, bool is_tlab) {\n+  for (uint loop_count = 0; \/* empty *\/; ++loop_count) {\n+    HeapWord* result = mem_allocate_cas_noexpand(size, is_tlab);\n+    if (result != nullptr) {\n+      return result;\n+    }\n@@ -294,15 +326,3 @@\n-  uint loop_count = 0;\n-  uint gc_count = 0;\n-\n-  while (result == nullptr) {\n-    \/\/ We don't want to have multiple collections for a single filled generation.\n-    \/\/ To prevent this, each thread tracks the total_collections() value, and if\n-    \/\/ the count has changed, does not do a new collection.\n-    \/\/\n-    \/\/ The collection count must be read only while holding the heap lock. VM\n-    \/\/ operations also hold the heap lock during collections. There is a lock\n-    \/\/ contention case where thread A blocks waiting on the Heap_lock, while\n-    \/\/ thread B is holding it doing a collection. When thread A gets the lock,\n-    \/\/ the collection count has already changed. To prevent duplicate collections,\n-    \/\/ The policy MUST attempt allocations during the same period it reads the\n-    \/\/ total_collections() value!\n+    \/\/ Read total_collections() under the lock so that multiple\n+    \/\/ allocation-failures result in one GC.\n+    uint gc_count;\n@@ -311,2 +331,3 @@\n-      gc_count = total_collections();\n-      result = young_gen()->allocate(size);\n+      \/\/ Re-try after acquiring the lock, because a GC might have occurred\n+      \/\/ while waiting for this lock.\n+      result = mem_allocate_cas_noexpand(size, is_tlab);\n@@ -318,3 +339,3 @@\n-      \/\/ If certain conditions hold, try allocating from the old gen.\n-      if (!is_tlab) {\n-        result = mem_allocate_old_gen(size);\n+      if (!is_init_completed()) {\n+        \/\/ Can't do GC; try heap expansion to satisfy the request.\n+        result = expand_heap_and_allocate(size, is_tlab);\n@@ -325,0 +346,2 @@\n+\n+      gc_count = total_collections();\n@@ -327,1 +350,0 @@\n-    assert(result == nullptr, \"inv\");\n@@ -332,3 +354,0 @@\n-      \/\/ Did the VM operation execute? If so, return the result directly.\n-      \/\/ This prevents us from looping until time out on requests that can\n-      \/\/ not be satisfied.\n@@ -337,25 +356,0 @@\n-\n-        \/\/ Exit the loop if the gc time limit has been exceeded.\n-        \/\/ The allocation must have failed above (\"result\" guarding\n-        \/\/ this path is null) and the most recent collection has exceeded the\n-        \/\/ gc overhead limit (although enough may have been collected to\n-        \/\/ satisfy the allocation).  Exit the loop so that an out-of-memory\n-        \/\/ will be thrown (return a null ignoring the contents of\n-        \/\/ op.result()),\n-        \/\/ but clear gc_overhead_limit_exceeded so that the next collection\n-        \/\/ starts with a clean slate (i.e., forgets about previous overhead\n-        \/\/ excesses).  Fill op.result() with a filler object so that the\n-        \/\/ heap remains parsable.\n-        const bool limit_exceeded = size_policy()->gc_overhead_limit_exceeded();\n-        const bool softrefs_clear = soft_ref_policy()->all_soft_refs_clear();\n-\n-        if (limit_exceeded && softrefs_clear) {\n-          *gc_overhead_limit_was_exceeded = true;\n-          size_policy()->set_gc_overhead_limit_exceeded(false);\n-          log_trace(gc)(\"ParallelScavengeHeap::mem_allocate: return null because gc_overhead_limit_exceeded is set\");\n-          if (op.result() != nullptr) {\n-            CollectedHeap::fill_with_object(op.result(), size);\n-          }\n-          return nullptr;\n-        }\n-\n@@ -366,4 +360,7 @@\n-    \/\/ The policy object will prevent us from looping forever. If the\n-    \/\/ time spent in gc crosses a threshold, we will bail out.\n-    loop_count++;\n-    if ((result == nullptr) && (QueuedAllocationWarningCount > 0) &&\n+    \/\/ Was the gc-overhead reached inside the safepoint? If so, this mutator\n+    \/\/ should return null as well for global consistency.\n+    if (_gc_overhead_counter >= GCOverheadLimitThreshold) {\n+      return nullptr;\n+    }\n+\n+    if ((QueuedAllocationWarningCount > 0) &&\n@@ -371,2 +368,1 @@\n-      log_warning(gc)(\"ParallelScavengeHeap::mem_allocate retries %d times\", loop_count);\n-      log_warning(gc)(\"\\tsize=%zu\", size);\n+      log_warning(gc)(\"ParallelScavengeHeap::mem_allocate retries %d times, size=%zu\", loop_count, size);\n@@ -375,2 +371,0 @@\n-\n-  return result;\n@@ -379,5 +373,7 @@\n-HeapWord* ParallelScavengeHeap::allocate_old_gen_and_record(size_t size) {\n-  assert_locked_or_safepoint(Heap_lock);\n-  HeapWord* res = old_gen()->allocate(size);\n-  if (res != nullptr) {\n-    _size_policy->tenured_allocation(size * HeapWordSize);\n+void ParallelScavengeHeap::do_full_collection(bool clear_all_soft_refs) {\n+  \/\/ No need for max-compaction in this context.\n+  const bool should_do_max_compaction = false;\n+  if (UseCompactObjectHeaders) {\n+    PSParallelCompactNew::invoke(clear_all_soft_refs, should_do_max_compaction);\n+  } else {\n+    PSParallelCompact::invoke(clear_all_soft_refs, should_do_max_compaction);\n@@ -385,1 +381,0 @@\n-  return res;\n@@ -388,4 +383,7 @@\n-HeapWord* ParallelScavengeHeap::mem_allocate_old_gen(size_t size) {\n-  if (!should_alloc_in_eden(size)) {\n-    \/\/ Size is too big for eden.\n-    return allocate_old_gen_and_record(size);\n+bool ParallelScavengeHeap::should_attempt_young_gc() const {\n+  const bool ShouldRunYoungGC = true;\n+  const bool ShouldRunFullGC = false;\n+\n+  if (!_young_gen->to_space()->is_empty()) {\n+    log_debug(gc, ergo)(\"To-space is not empty; run full-gc instead.\");\n+    return ShouldRunFullGC;\n@@ -394,1 +392,38 @@\n-  return nullptr;\n+  \/\/ Check if the predicted promoted bytes will overflow free space in old-gen.\n+  PSAdaptiveSizePolicy* policy = _size_policy;\n+\n+  size_t avg_promoted = (size_t) policy->padded_average_promoted_in_bytes();\n+  size_t promotion_estimate = MIN2(avg_promoted, _young_gen->used_in_bytes());\n+  \/\/ Total free size after possible old gen expansion\n+  size_t free_in_old_gen_with_expansion = _old_gen->max_gen_size() - _old_gen->used_in_bytes();\n+\n+  log_trace(gc, ergo)(\"average_promoted %zu; padded_average_promoted %zu\",\n+              (size_t) policy->average_promoted_in_bytes(),\n+              (size_t) policy->padded_average_promoted_in_bytes());\n+\n+  if (promotion_estimate >= free_in_old_gen_with_expansion) {\n+    log_debug(gc, ergo)(\"Run full-gc; predicted promotion size >= max free space in old-gen: %zu >= %zu\",\n+      promotion_estimate, free_in_old_gen_with_expansion);\n+    return ShouldRunFullGC;\n+  }\n+\n+  if (UseAdaptiveSizePolicy) {\n+    \/\/ Also checking OS has enough free memory to commit and expand old-gen.\n+    \/\/ Otherwise, the recorded gc-pause-time might be inflated to include time\n+    \/\/ of OS preparing free memory, resulting in inaccurate young-gen resizing.\n+    assert(_old_gen->committed().byte_size() >= _old_gen->used_in_bytes(), \"inv\");\n+    \/\/ Use uint64_t instead of size_t for 32bit compatibility.\n+    uint64_t free_mem_in_os;\n+    if (os::free_memory(free_mem_in_os)) {\n+      size_t actual_free = (size_t)MIN2(_old_gen->committed().byte_size() - _old_gen->used_in_bytes() + free_mem_in_os,\n+                                        (uint64_t)SIZE_MAX);\n+      if (promotion_estimate > actual_free) {\n+        log_debug(gc, ergo)(\"Run full-gc; predicted promotion size > free space in old-gen and OS: %zu > %zu\",\n+          promotion_estimate, actual_free);\n+        return ShouldRunFullGC;\n+      }\n+    }\n+  }\n+\n+  \/\/ No particular reasons to run full-gc, so young-gc.\n+  return ShouldRunYoungGC;\n@@ -397,5 +432,28 @@\n-void ParallelScavengeHeap::do_full_collection(bool clear_all_soft_refs) {\n-  if (UseCompactObjectHeaders) {\n-    PSParallelCompactNew::invoke(clear_all_soft_refs, false \/* serial *\/);\n-  } else {\n-    PSParallelCompact::invoke(clear_all_soft_refs);\n+static bool check_gc_heap_free_limit(size_t free_bytes, size_t capacity_bytes) {\n+  return (free_bytes * 100 \/ capacity_bytes) < GCHeapFreeLimit;\n+}\n+\n+bool ParallelScavengeHeap::check_gc_overhead_limit() {\n+  assert(SafepointSynchronize::is_at_safepoint(), \"precondition\");\n+\n+  if (UseGCOverheadLimit) {\n+    \/\/ The goal here is to return null prematurely so that apps can exit\n+    \/\/ gracefully when GC takes the most time.\n+    bool little_mutator_time = _size_policy->mutator_time_percent() * 100 < (100 - GCTimeLimit);\n+    bool little_free_space = check_gc_heap_free_limit(_young_gen->free_in_bytes(), _young_gen->capacity_in_bytes())\n+                          && check_gc_heap_free_limit(  _old_gen->free_in_bytes(),   _old_gen->capacity_in_bytes());\n+\n+    log_debug(gc)(\"GC Overhead Limit: GC Time %f Free Space Young %f Old %f Counter %zu\",\n+                  (100 - _size_policy->mutator_time_percent()),\n+                  percent_of(_young_gen->free_in_bytes(), _young_gen->capacity_in_bytes()),\n+                  percent_of(_old_gen->free_in_bytes(), _young_gen->capacity_in_bytes()),\n+                  _gc_overhead_counter);\n+\n+    if (little_mutator_time && little_free_space) {\n+      _gc_overhead_counter++;\n+      if (_gc_overhead_counter >= GCOverheadLimitThreshold) {\n+        return true;\n+      }\n+    } else {\n+      _gc_overhead_counter = 0;\n+    }\n@@ -403,0 +461,1 @@\n+  return false;\n@@ -406,1 +465,12 @@\n-  HeapWord* result = nullptr;\n+#ifdef ASSERT\n+  assert(Heap_lock->is_locked(), \"precondition\");\n+  if (is_init_completed()) {\n+    assert(SafepointSynchronize::is_at_safepoint(), \"precondition\");\n+    assert(Thread::current()->is_VM_thread(), \"precondition\");\n+  } else {\n+    assert(Thread::current()->is_Java_thread(), \"precondition\");\n+    assert(Heap_lock->owned_by_self(), \"precondition\");\n+  }\n+#endif\n+\n+  HeapWord* result = young_gen()->expand_and_allocate(size);\n@@ -408,1 +478,0 @@\n-  result = young_gen()->allocate(size);\n@@ -412,0 +481,1 @@\n+\n@@ -420,3 +490,3 @@\n-  \/\/ If young-gen can handle this allocation, attempt young-gc firstly.\n-  bool should_run_young_gc = is_tlab || should_alloc_in_eden(size);\n-  collect_at_safepoint(!should_run_young_gc);\n+  if (!_is_heap_almost_full) {\n+    \/\/ If young-gen can handle this allocation, attempt young-gc firstly, as young-gc is usually cheaper.\n+    bool should_run_young_gc = is_tlab || should_alloc_in_eden(size);\n@@ -424,3 +494,9 @@\n-  result = expand_heap_and_allocate(size, is_tlab);\n-  if (result != nullptr) {\n-    return result;\n+    collect_at_safepoint(!should_run_young_gc);\n+\n+    \/\/ If gc-overhead is reached, we will skip allocation.\n+    if (!check_gc_overhead_limit()) {\n+      result = expand_heap_and_allocate(size, is_tlab);\n+      if (result != nullptr) {\n+        return result;\n+      }\n+    }\n@@ -429,5 +505,1 @@\n-  \/\/ If we reach this point, we're really out of memory. Try every trick\n-  \/\/ we can to reclaim memory. Force collection of soft references. Force\n-  \/\/ a complete compaction of the heap. Any additional methods for finding\n-  \/\/ free memory should be here, especially if they are expensive. If this\n-  \/\/ attempt fails, an OOM exception will be thrown.\n+  \/\/ Last resort GC; clear soft refs and do max-compaction before throwing OOM.\n@@ -435,4 +507,1 @@\n-    \/\/ Make sure the heap is fully compacted\n-    uintx old_interval = HeapMaximumCompactionInterval;\n-    HeapMaximumCompactionInterval = 0;\n-\n+    const bool should_do_max_compaction = true;\n@@ -440,1 +509,1 @@\n-      PSParallelCompactNew::invoke(clear_all_soft_refs, false \/* serial *\/);\n+      PSParallelCompactNew::invoke(clear_all_soft_refs, should_do_max_compaction);\n@@ -442,1 +511,1 @@\n-      PSParallelCompact::invoke(clear_all_soft_refs);\n+      PSParallelCompact::invoke(clear_all_soft_refs, should_do_max_compaction);\n@@ -444,8 +513,0 @@\n-\n-    \/\/ Restore\n-    HeapMaximumCompactionInterval = old_interval;\n-  }\n-\n-  result = expand_heap_and_allocate(size, is_tlab);\n-  if (result != nullptr) {\n-    return result;\n@@ -455,2 +516,3 @@\n-  if (UseCompactObjectHeaders) {\n-    PSParallelCompactNew::invoke(true \/* clear_soft_refs *\/, true \/* serial *\/);\n+  if (check_gc_overhead_limit()) {\n+    log_info(gc)(\"GC Overhead Limit exceeded too often (%zu).\", GCOverheadLimitThreshold);\n+    return nullptr;\n@@ -460,9 +522,1 @@\n-  if (result != nullptr) {\n-    return result;\n-  }\n-\n-  \/\/ What else?  We might try synchronous finalization later.  If the total\n-  \/\/ space available is large enough for the allocation, then a more\n-  \/\/ complete compaction phase than we've tried so far might be\n-  \/\/ appropriate.\n-  return nullptr;\n+  return result;\n@@ -471,1 +525,0 @@\n-\n@@ -477,2 +530,2 @@\n-size_t ParallelScavengeHeap::tlab_capacity(Thread* thr) const {\n-  return young_gen()->eden_space()->tlab_capacity(thr);\n+size_t ParallelScavengeHeap::tlab_capacity() const {\n+  return young_gen()->eden_space()->tlab_capacity();\n@@ -481,2 +534,2 @@\n-size_t ParallelScavengeHeap::tlab_used(Thread* thr) const {\n-  return young_gen()->eden_space()->tlab_used(thr);\n+size_t ParallelScavengeHeap::tlab_used() const {\n+  return young_gen()->eden_space()->tlab_used();\n@@ -485,2 +538,2 @@\n-size_t ParallelScavengeHeap::unsafe_max_tlab_alloc(Thread* thr) const {\n-  return young_gen()->eden_space()->unsafe_max_tlab_alloc(thr);\n+size_t ParallelScavengeHeap::unsafe_max_tlab_alloc() const {\n+  return young_gen()->eden_space()->unsafe_max_tlab_alloc();\n@@ -490,3 +543,1 @@\n-  bool dummy;\n-                                       true \/* is_tlab *\/,\n-                                       &dummy);\n+                                       true \/* is_tlab *\/);\n@@ -530,6 +581,1 @@\n-bool ParallelScavengeHeap::must_clear_all_soft_refs() {\n-  return _gc_cause == GCCause::_metadata_GC_clear_soft_refs ||\n-         _gc_cause == GCCause::_wb_full_gc;\n-}\n-\n-void ParallelScavengeHeap::collect_at_safepoint(bool full) {\n+void ParallelScavengeHeap::collect_at_safepoint(bool is_full) {\n@@ -537,1 +583,1 @@\n-  bool clear_soft_refs = must_clear_all_soft_refs();\n+  bool clear_soft_refs = GCCause::should_clear_all_soft_refs(_gc_cause);\n@@ -539,3 +585,3 @@\n-  if (!full) {\n-    bool success = PSScavenge::invoke(clear_soft_refs);\n-    if (success) {\n+  if (!is_full && should_attempt_young_gc()) {\n+    bool young_gc_success = PSScavenge::invoke(clear_soft_refs);\n+    if (young_gc_success) {\n@@ -544,1 +590,1 @@\n-    \/\/ Upgrade to Full-GC if young-gc fails\n+    log_debug(gc, heap)(\"Upgrade to Full-GC since Young-gc failed.\");\n@@ -546,0 +592,2 @@\n+\n+  const bool should_do_max_compaction = false;\n@@ -547,1 +595,1 @@\n-    PSParallelCompactNew::invoke(clear_soft_refs, false \/* serial *\/);\n+    PSParallelCompactNew::invoke(clear_soft_refs, should_do_max_compaction);\n@@ -549,1 +597,1 @@\n-    PSParallelCompact::invoke(clear_soft_refs);\n+    PSParallelCompact::invoke(clear_soft_refs, should_do_max_compaction);\n@@ -576,1 +624,1 @@\n-    block_index = Atomic::fetch_then_add(&_claimed_index, 1u);\n+    block_index = AtomicAccess::fetch_then_add(&_claimed_index, 1u);\n@@ -705,1 +753,0 @@\n-  AdaptiveSizePolicyOutput::print();\n@@ -764,4 +811,2 @@\n-  \/\/ Why do we need the total_collections()-filter below?\n-  if (total_collections() > 0) {\n-    log_debug(gc, verify)(\"Tenured\");\n-    old_gen()->verify();\n+  log_debug(gc, verify)(\"Tenured\");\n+  old_gen()->verify();\n@@ -769,2 +814,2 @@\n-    log_debug(gc, verify)(\"Eden\");\n-    young_gen()->verify();\n+  log_debug(gc, verify)(\"Eden\");\n+  young_gen()->verify();\n@@ -772,3 +817,2 @@\n-    log_debug(gc, verify)(\"CardTable\");\n-    card_table()->verify_all_young_refs_imprecise();\n-  }\n+  log_debug(gc, verify)(\"CardTable\");\n+  card_table()->verify_all_young_refs_imprecise();\n@@ -806,4 +850,80 @@\n-void ParallelScavengeHeap::resize_young_gen(size_t eden_size,\n-                                            size_t survivor_size) {\n-  \/\/ Delegate the resize to the generation.\n-  _young_gen->resize(eden_size, survivor_size);\n+static size_t calculate_free_from_free_ratio_flag(size_t live, uintx free_percent) {\n+  assert(free_percent != 100, \"precondition\");\n+  \/\/ We want to calculate how much free memory there can be based on the\n+  \/\/ live size.\n+  \/\/   percent * (free + live) = free\n+  \/\/ =>\n+  \/\/   free = (live * percent) \/ (1 - percent)\n+\n+  const double percent = free_percent \/ 100.0;\n+  return live * percent \/ (1.0 - percent);\n+}\n+\n+size_t ParallelScavengeHeap::calculate_desired_old_gen_capacity(size_t old_gen_live_size) {\n+  \/\/ If min free percent is 100%, the old-gen should always be in its max capacity\n+  if (MinHeapFreeRatio == 100) {\n+    return _old_gen->max_gen_size();\n+  }\n+\n+  \/\/ Using recorded data to calculate the new capacity of old-gen to avoid\n+  \/\/ excessive expansion but also keep footprint low\n+\n+  size_t promoted_estimate = _size_policy->padded_average_promoted_in_bytes();\n+  \/\/ Should have at least this free room for the next young-gc promotion.\n+  size_t free_size = promoted_estimate;\n+\n+  size_t largest_live_size = MAX2((size_t)_size_policy->peak_old_gen_used_estimate(), old_gen_live_size);\n+  free_size += largest_live_size - old_gen_live_size;\n+\n+  \/\/ Respect free percent\n+  if (MinHeapFreeRatio != 0) {\n+    size_t min_free = calculate_free_from_free_ratio_flag(old_gen_live_size, MinHeapFreeRatio);\n+    free_size = MAX2(free_size, min_free);\n+  }\n+\n+  if (MaxHeapFreeRatio != 100) {\n+    size_t max_free = calculate_free_from_free_ratio_flag(old_gen_live_size, MaxHeapFreeRatio);\n+    free_size = MIN2(max_free, free_size);\n+  }\n+\n+  return old_gen_live_size + free_size;\n+}\n+\n+void ParallelScavengeHeap::resize_old_gen_after_full_gc() {\n+  size_t current_capacity = _old_gen->capacity_in_bytes();\n+  size_t desired_capacity = calculate_desired_old_gen_capacity(old_gen()->used_in_bytes());\n+\n+  \/\/ If MinHeapFreeRatio is at its default value; shrink cautiously. Otherwise, users expect prompt shrinking.\n+  if (FLAG_IS_DEFAULT(MinHeapFreeRatio)) {\n+    if (desired_capacity < current_capacity) {\n+      \/\/ Shrinking\n+      if (total_full_collections() < AdaptiveSizePolicyReadyThreshold) {\n+        \/\/ No enough data for shrinking\n+        return;\n+      }\n+    }\n+  }\n+\n+  _old_gen->resize(desired_capacity);\n+}\n+\n+void ParallelScavengeHeap::resize_after_young_gc(bool is_survivor_overflowing) {\n+  _young_gen->resize_after_young_gc(is_survivor_overflowing);\n+\n+  \/\/ Consider if should shrink old-gen\n+  if (!is_survivor_overflowing) {\n+    \/\/ Upper bound for a single step shrink\n+    size_t max_shrink_bytes = SpaceAlignment;\n+    size_t shrink_bytes = _size_policy->compute_old_gen_shrink_bytes(old_gen()->free_in_bytes(), max_shrink_bytes);\n+    if (shrink_bytes != 0) {\n+      if (MinHeapFreeRatio != 0) {\n+        size_t new_capacity = old_gen()->capacity_in_bytes() - shrink_bytes;\n+        size_t new_free_size = old_gen()->free_in_bytes() - shrink_bytes;\n+        if ((double)new_free_size \/ new_capacity * 100 < MinHeapFreeRatio) {\n+          \/\/ Would violate MinHeapFreeRatio\n+          return;\n+        }\n+      }\n+      old_gen()->shrink(shrink_bytes);\n+    }\n+  }\n@@ -812,3 +932,8 @@\n-void ParallelScavengeHeap::resize_old_gen(size_t desired_free_space) {\n-  \/\/ Delegate the resize to the generation.\n-  _old_gen->resize(desired_free_space);\n+void ParallelScavengeHeap::resize_after_full_gc() {\n+  resize_old_gen_after_full_gc();\n+  \/\/ We don't resize young-gen after full-gc because:\n+  \/\/ 1. eden-size directly affects young-gc frequency (GCTimeRatio), and we\n+  \/\/ don't have enough info to determine its desired size.\n+  \/\/ 2. eden can contain live objs after a full-gc, which is unsafe for\n+  \/\/ resizing. We will perform expansion on allocation if needed, in\n+  \/\/ satisfy_failed_allocation().\n@@ -829,0 +954,2 @@\n+  BarrierSetNMethod* bs_nm = BarrierSet::barrier_set()->barrier_set_nmethod();\n+  bs_nm->disarm(nm);\n","filename":"src\/hotspot\/share\/gc\/parallel\/parallelScavengeHeap.cpp","additions":317,"deletions":190,"binary":false,"changes":507,"status":"modified"},{"patch":"@@ -27,1 +27,0 @@\n-#include \"gc\/parallel\/parMarkBitMap.inline.hpp\"\n@@ -29,0 +28,1 @@\n+#include \"gc\/parallel\/parMarkBitMap.inline.hpp\"\n","filename":"src\/hotspot\/share\/gc\/parallel\/psCompactionManagerNew.cpp","additions":1,"deletions":1,"binary":false,"changes":2,"status":"modified"},{"patch":"@@ -32,1 +32,0 @@\n-#include \"gc\/shared\/partialArrayTaskStats.hpp\"\n@@ -34,0 +33,1 @@\n+#include \"gc\/shared\/partialArrayTaskStats.hpp\"\n","filename":"src\/hotspot\/share\/gc\/parallel\/psCompactionManagerNew.hpp","additions":1,"deletions":1,"binary":false,"changes":2,"status":"modified"},{"patch":"@@ -31,0 +31,1 @@\n+#include \"code\/nmethod.hpp\"\n@@ -59,0 +60,1 @@\n+#include \"gc\/shared\/parallelCleaning.hpp\"\n@@ -64,1 +66,0 @@\n-#include \"gc\/shared\/strongRootsScope.hpp\"\n@@ -84,1 +85,1 @@\n-#include \"runtime\/atomic.hpp\"\n+#include \"runtime\/atomicAccess.hpp\"\n@@ -300,23 +301,0 @@\n-void\n-ParallelCompactData::summarize_dense_prefix(HeapWord* beg, HeapWord* end)\n-{\n-  assert(is_region_aligned(beg), \"not RegionSize aligned\");\n-  assert(is_region_aligned(end), \"not RegionSize aligned\");\n-\n-  size_t cur_region = addr_to_region_idx(beg);\n-  const size_t end_region = addr_to_region_idx(end);\n-  HeapWord* addr = beg;\n-  while (cur_region < end_region) {\n-    _region_data[cur_region].set_destination(addr);\n-    _region_data[cur_region].set_destination_count(0);\n-    _region_data[cur_region].set_source_region(cur_region);\n-\n-    \/\/ Update live_obj_size so the region appears completely full.\n-    size_t live_size = RegionSize - _region_data[cur_region].partial_obj_size();\n-    _region_data[cur_region].set_live_obj_size(live_size);\n-\n-    ++cur_region;\n-    addr += RegionSize;\n-  }\n-}\n-\n@@ -662,1 +640,0 @@\n-  \/\/ Increment the invocation count\n@@ -689,0 +666,3 @@\n+  \/\/ Need to clear claim bits for the next full-gc (marking and adjust-pointers).\n+  ClassLoaderDataGraph::clear_claimed_marks();\n+\n@@ -735,10 +715,0 @@\n-  {\n-    \/\/ Delete metaspaces for unloaded class loaders and clean up loader_data graph\n-    GCTraceTime(Debug, gc, phases) t(\"Purge Class Loader Data\", gc_timer());\n-    ClassLoaderDataGraph::purge(true \/* at_safepoint *\/);\n-    DEBUG_ONLY(MetaspaceUtils::verify();)\n-  }\n-\n-  \/\/ Need to clear claim bits for the next mark.\n-  ClassLoaderDataGraph::clear_claimed_marks();\n-\n@@ -837,1 +807,2 @@\n-bool PSParallelCompact::check_maximum_compaction(size_t total_live_words,\n+bool PSParallelCompact::check_maximum_compaction(bool should_do_max_compaction,\n+                                                 size_t total_live_words,\n@@ -847,8 +818,2 @@\n-  \/\/ Check if all live objs are larger than old-gen.\n-  const bool is_old_gen_overflowing = (total_live_words > old_space->capacity_in_words());\n-\n-  \/\/ JVM flags\n-  const uint total_invocations = heap->total_full_collections();\n-  assert(total_invocations >= _maximum_compaction_gc_num, \"sanity\");\n-  const size_t gcs_since_max = total_invocations - _maximum_compaction_gc_num;\n-  const bool is_interval_ended = gcs_since_max > HeapMaximumCompactionInterval;\n+  \/\/ Check if all live objs are too much for old-gen.\n+  const bool is_old_gen_too_full = (total_live_words >= old_space->capacity_in_words());\n@@ -860,6 +825,4 @@\n-  if (is_max_on_system_gc || is_old_gen_overflowing || is_interval_ended || is_region_full) {\n-    _maximum_compaction_gc_num = total_invocations;\n-    return true;\n-  }\n-\n-  return false;\n+  return should_do_max_compaction\n+      || is_max_on_system_gc\n+      || is_old_gen_too_full\n+      || is_region_full;\n@@ -868,1 +831,1 @@\n-void PSParallelCompact::summary_phase()\n+void PSParallelCompact::summary_phase(bool should_do_max_compaction)\n@@ -891,4 +854,13 @@\n-    bool maximum_compaction = check_maximum_compaction(total_live_words,\n-                                                       old_space,\n-                                                       full_region_prefix_end);\n-    HeapWord* dense_prefix_end = maximum_compaction\n+    should_do_max_compaction = check_maximum_compaction(should_do_max_compaction,\n+                                                        total_live_words,\n+                                                        old_space,\n+                                                        full_region_prefix_end);\n+    {\n+      GCTraceTime(Info, gc, phases) tm(\"Summary Phase: expand\", &_gc_timer);\n+      \/\/ Try to expand old-gen in order to fit all live objs and waste.\n+      size_t target_capacity_bytes = total_live_words * HeapWordSize\n+                                   + old_space->capacity_in_bytes() * (MarkSweepDeadRatio \/ 100);\n+      ParallelScavengeHeap::heap()->old_gen()->try_expand_till_size(target_capacity_bytes);\n+    }\n+\n+    HeapWord* dense_prefix_end = should_do_max_compaction\n@@ -903,1 +875,0 @@\n-      _summary_data.summarize_dense_prefix(old_space->bottom(), dense_prefix_end);\n@@ -964,13 +935,1 @@\n-\/\/ This method should contain all heap-specific policy for invoking a full\n-\/\/ collection.  invoke_no_policy() will only attempt to compact the heap; it\n-\/\/ will do nothing further.  If we need to bail out for policy reasons, scavenge\n-\/\/ before full gc, or any other specialized behavior, it needs to be added here.\n-\/\/\n-\/\/ Note that this method should only be called from the vm_thread while at a\n-\/\/ safepoint.\n-\/\/\n-\/\/ Note that the all_soft_refs_clear flag in the soft ref policy\n-\/\/ may be true because this method can be called without intervening\n-\/\/ activity.  For example when the heap space is tight and full measure\n-\/\/ are being taken to free space.\n-bool PSParallelCompact::invoke(bool clear_all_soft_refs) {\n+bool PSParallelCompact::invoke(bool clear_all_soft_refs, bool should_do_max_compaction) {\n@@ -980,0 +939,1 @@\n+  assert(ref_processor() != nullptr, \"Sanity\");\n@@ -984,13 +944,0 @@\n-  ParallelScavengeHeap* heap = ParallelScavengeHeap::heap();\n-  clear_all_soft_refs = clear_all_soft_refs\n-                     || heap->soft_ref_policy()->should_clear_all_soft_refs();\n-\n-  return PSParallelCompact::invoke_no_policy(clear_all_soft_refs);\n-}\n-\n-\/\/ This method contains no policy. You should probably\n-\/\/ be calling invoke() instead.\n-bool PSParallelCompact::invoke_no_policy(bool clear_all_soft_refs) {\n-  assert(SafepointSynchronize::is_at_safepoint(), \"must be at a safepoint\");\n-  assert(ref_processor() != nullptr, \"Sanity\");\n-\n@@ -1004,1 +951,0 @@\n-  PSYoungGen* young_gen = heap->young_gen();\n@@ -1008,5 +954,0 @@\n-  \/\/ The scope of casr should end after code that can change\n-  \/\/ SoftRefPolicy::_should_clear_all_soft_refs.\n-  ClearedAllSoftRefs casr(clear_all_soft_refs,\n-                          heap->soft_ref_policy());\n-\n@@ -1047,4 +988,0 @@\n-    ClassUnloadingContext ctx(1 \/* num_nmethod_unlink_workers *\/,\n-                              false \/* unregister_nmethods_during_purge *\/,\n-                              false \/* lock_nmethod_free_separately *\/);\n-\n@@ -1053,1 +990,1 @@\n-    summary_phase();\n+    summary_phase(should_do_max_compaction);\n@@ -1078,63 +1015,1 @@\n-    \/\/ Let the size policy know we're done\n-    size_policy->major_collection_end(old_gen->used_in_bytes(), gc_cause);\n-\n-    if (UseAdaptiveSizePolicy) {\n-      log_debug(gc, ergo)(\"AdaptiveSizeStart: collection: %d \", heap->total_collections());\n-      log_trace(gc, ergo)(\"old_gen_capacity: %zu young_gen_capacity: %zu\",\n-                          old_gen->capacity_in_bytes(), young_gen->capacity_in_bytes());\n-\n-      \/\/ Don't check if the size_policy is ready here.  Let\n-      \/\/ the size_policy check that internally.\n-      if (UseAdaptiveGenerationSizePolicyAtMajorCollection &&\n-          AdaptiveSizePolicy::should_update_promo_stats(gc_cause)) {\n-        \/\/ Swap the survivor spaces if from_space is empty. The\n-        \/\/ resize_young_gen() called below is normally used after\n-        \/\/ a successful young GC and swapping of survivor spaces;\n-        \/\/ otherwise, it will fail to resize the young gen with\n-        \/\/ the current implementation.\n-        if (young_gen->from_space()->is_empty()) {\n-          young_gen->from_space()->clear(SpaceDecorator::Mangle);\n-          young_gen->swap_spaces();\n-        }\n-\n-        \/\/ Calculate optimal free space amounts\n-        assert(young_gen->max_gen_size() >\n-          young_gen->from_space()->capacity_in_bytes() +\n-          young_gen->to_space()->capacity_in_bytes(),\n-          \"Sizes of space in young gen are out-of-bounds\");\n-\n-        size_t young_live = young_gen->used_in_bytes();\n-        size_t eden_live = young_gen->eden_space()->used_in_bytes();\n-        size_t old_live = old_gen->used_in_bytes();\n-        size_t cur_eden = young_gen->eden_space()->capacity_in_bytes();\n-        size_t max_old_gen_size = old_gen->max_gen_size();\n-        size_t max_eden_size = young_gen->max_gen_size() -\n-          young_gen->from_space()->capacity_in_bytes() -\n-          young_gen->to_space()->capacity_in_bytes();\n-\n-        \/\/ Used for diagnostics\n-        size_policy->clear_generation_free_space_flags();\n-\n-        size_policy->compute_generations_free_space(young_live,\n-                                                    eden_live,\n-                                                    old_live,\n-                                                    cur_eden,\n-                                                    max_old_gen_size,\n-                                                    max_eden_size,\n-                                                    true \/* full gc*\/);\n-\n-        size_policy->check_gc_overhead_limit(eden_live,\n-                                             max_old_gen_size,\n-                                             max_eden_size,\n-                                             true \/* full gc*\/,\n-                                             gc_cause,\n-                                             heap->soft_ref_policy());\n-\n-        size_policy->decay_supplemental_growth(true \/* full gc*\/);\n-\n-        heap->resize_old_gen(\n-          size_policy->calculated_old_free_size_in_bytes());\n-\n-        heap->resize_young_gen(size_policy->calculated_eden_size_in_bytes(),\n-                               size_policy->calculated_survivor_size_in_bytes());\n-      }\n+    size_policy->major_collection_end();\n@@ -1142,2 +1017,1 @@\n-      log_debug(gc, ergo)(\"AdaptiveSizeStop: collection: %d \", heap->total_collections());\n-    }\n+    size_policy->sample_old_gen_used_bytes(MAX2(pre_gc_values.old_gen_used(), old_gen->used_in_bytes()));\n@@ -1145,5 +1019,2 @@\n-    if (UsePerfData) {\n-      PSGCAdaptivePolicyCounters* const counters = heap->gc_policy_counters();\n-      counters->update_counters();\n-      counters->update_old_capacity(old_gen->capacity_in_bytes());\n-      counters->update_young_capacity(young_gen->capacity_in_bytes());\n+    if (UseAdaptiveSizePolicy) {\n+      heap->resize_after_full_gc();\n@@ -1168,0 +1039,2 @@\n+\n+    size_policy->record_gc_pause_end_instant();\n@@ -1170,0 +1043,2 @@\n+  heap->gc_epilogue(true);\n+\n@@ -1177,2 +1052,0 @@\n-  AdaptiveSizePolicyOutput::print(size_policy, heap->total_collections());\n-\n@@ -1188,2 +1061,1 @@\n-private:\n-  uint _worker_id;\n+  ParCompactionManager* _cm;\n@@ -1192,1 +1064,1 @@\n-  PCAddThreadRootsMarkingTaskClosure(uint worker_id) : _worker_id(worker_id) { }\n+  PCAddThreadRootsMarkingTaskClosure(ParCompactionManager* cm) : _cm(cm) { }\n@@ -1194,2 +1066,0 @@\n-    assert(ParallelScavengeHeap::heap()->is_stw_gc_active(), \"called outside gc\");\n-\n@@ -1198,5 +1068,1 @@\n-    ParCompactionManager* cm = ParCompactionManager::gc_thread_compaction_manager(_worker_id);\n-\n-    MarkingNMethodClosure mark_and_push_in_blobs(&cm->_mark_and_push_closure,\n-                                                 !NMethodToOopClosure::FixRelocations,\n-                                                 true \/* keepalive nmethods *\/);\n+    MarkingNMethodClosure mark_and_push_in_blobs(&_cm->_mark_and_push_closure);\n@@ -1204,1 +1070,1 @@\n-    thread->oops_do(&cm->_mark_and_push_closure, &mark_and_push_in_blobs);\n+    thread->oops_do(&_cm->_mark_and_push_closure, &mark_and_push_in_blobs);\n@@ -1207,1 +1073,1 @@\n-    cm->follow_marking_stacks();\n+    _cm->follow_marking_stacks();\n@@ -1227,1 +1093,2 @@\n-  StrongRootsScope _strong_roots_scope; \/\/ needed for Threads::possibly_parallel_threads_do\n+  NMethodMarkingScope _nmethod_marking_scope;\n+  ThreadsClaimTokenScope _threads_claim_token_scope;\n@@ -1235,1 +1102,2 @@\n-      _strong_roots_scope(active_workers),\n+      _nmethod_marking_scope(),\n+      _threads_claim_token_scope(),\n@@ -1251,1 +1119,1 @@\n-      PCAddThreadRootsMarkingTaskClosure closure(worker_id);\n+      PCAddThreadRootsMarkingTaskClosure closure(cm);\n@@ -1296,0 +1164,34 @@\n+class PSParallelCleaningTask : public WorkerTask {\n+  bool                    _unloading_occurred;\n+  CodeCacheUnloadingTask  _code_cache_task;\n+  \/\/ Prune dead klasses from subklass\/sibling\/implementor lists.\n+  KlassCleaningTask       _klass_cleaning_task;\n+\n+public:\n+  PSParallelCleaningTask(bool unloading_occurred) :\n+    WorkerTask(\"PS Parallel Cleaning\"),\n+    _unloading_occurred(unloading_occurred),\n+    _code_cache_task(unloading_occurred),\n+    _klass_cleaning_task() {}\n+\n+  void work(uint worker_id) {\n+#if INCLUDE_JVMCI\n+    if (EnableJVMCI && worker_id == 0) {\n+      \/\/ Serial work; only first worker.\n+      \/\/ Clean JVMCI metadata handles.\n+      JVMCI::do_unloading(_unloading_occurred);\n+    }\n+#endif\n+\n+    \/\/ Do first pass of code cache cleaning.\n+    _code_cache_task.work(worker_id);\n+\n+    \/\/ Clean all klasses that were not unloaded.\n+    \/\/ The weak metadata in klass doesn't need to be\n+    \/\/ processed if there was no unloading.\n+    if (_unloading_occurred) {\n+      _klass_cleaning_task.work();\n+    }\n+  }\n+};\n+\n@@ -1344,1 +1246,3 @@\n-    ClassUnloadingContext* ctx = ClassUnloadingContext::context();\n+    ClassUnloadingContext ctx(active_gc_threads \/* num_nmethod_unlink_workers *\/,\n+                              false \/* unregister_nmethods_during_purge *\/,\n+                              false \/* lock_nmethod_free_separately *\/);\n@@ -1346,1 +1250,0 @@\n-    bool unloading_occurred;\n@@ -1351,1 +1254,1 @@\n-      unloading_occurred = SystemDictionary::do_unloading(&_gc_timer);\n+      bool unloading_occurred = SystemDictionary::do_unloading(&_gc_timer);\n@@ -1353,2 +1256,2 @@\n-      \/\/ Unload nmethods.\n-      CodeCache::do_unloading(unloading_occurred);\n+      PSParallelCleaningTask task{unloading_occurred};\n+      ParallelScavengeHeap::heap()->workers().run_task(&task);\n@@ -1360,1 +1263,1 @@\n-      ctx->purge_nmethods();\n+      ctx.purge_nmethods();\n@@ -1368,1 +1271,7 @@\n-      ctx->free_nmethods();\n+      ctx.free_nmethods();\n+    }\n+    {\n+      \/\/ Delete metaspaces for unloaded class loaders and clean up loader_data graph\n+      GCTraceTime(Debug, gc, phases) t(\"Purge Class Loader Data\", gc_timer());\n+      ClassLoaderDataGraph::purge(true \/* at_safepoint *\/);\n+      DEBUG_ONLY(MetaspaceUtils::verify();)\n@@ -1370,6 +1279,0 @@\n-\n-    \/\/ Prune dead klasses from subklass\/sibling\/implementor lists.\n-    Klass::clean_weak_klass_links(unloading_occurred);\n-\n-    \/\/ Clean JVMCI metadata handles.\n-    JVMCI_ONLY(JVMCI::do_unloading(unloading_occurred));\n@@ -1401,1 +1304,1 @@\n-    uint counter = Atomic::fetch_then_add(claim_counter, num_regions_per_stripe);\n+    uint counter = AtomicAccess::fetch_then_add(claim_counter, num_regions_per_stripe);\n@@ -1466,1 +1369,1 @@\n-  SubTasksDone                               _sub_tasks;\n+  ThreadsClaimTokenScope                     _threads_claim_token_scope;\n@@ -1470,0 +1373,1 @@\n+  volatile bool                              _code_cache_claimed;\n@@ -1472,5 +1376,4 @@\n-  enum PSAdjustSubTask {\n-    PSAdjustSubTask_code_cache,\n-\n-    PSAdjustSubTask_num_elements\n-  };\n+  bool try_claim_code_cache_task() {\n+    return AtomicAccess::load(&_code_cache_claimed) == false\n+        && AtomicAccess::cmpxchg(&_code_cache_claimed, false, true) == false;\n+  }\n@@ -1481,1 +1384,1 @@\n-    _sub_tasks(PSAdjustSubTask_num_elements),\n+    _threads_claim_token_scope(),\n@@ -1483,1 +1386,3 @@\n-    _nworkers(nworkers) {\n+    _oop_storage_iter(),\n+    _nworkers(nworkers),\n+    _code_cache_claimed(false) {\n@@ -1486,7 +1391,0 @@\n-    if (nworkers > 1) {\n-      Threads::change_thread_claim_token();\n-    }\n-  }\n-\n-  ~PSAdjustTask() {\n-    Threads::assert_all_threads_claimed();\n@@ -1496,3 +1394,4 @@\n-    ParCompactionManager* cm = ParCompactionManager::gc_thread_compaction_manager(worker_id);\n-    cm->preserved_marks()->adjust_during_full_gc();\n-      \/\/ adjust pointers in all spaces\n+      \/\/ Pointers in heap.\n+      ParCompactionManager* cm = ParCompactionManager::gc_thread_compaction_manager(worker_id);\n+      cm->preserved_marks()->adjust_during_full_gc();\n+\n@@ -1502,0 +1401,1 @@\n+\n@@ -1503,5 +1403,1 @@\n-      ResourceMark rm;\n-      Threads::possibly_parallel_oops_do(_nworkers > 1, &pc_adjust_pointer_closure, nullptr);\n-    }\n-    _oop_storage_iter.oops_do(&pc_adjust_pointer_closure);\n-    {\n+      \/\/ All (strong and weak) CLDs.\n@@ -1511,0 +1407,1 @@\n+\n@@ -1512,0 +1409,13 @@\n+      \/\/ Threads stack frames. No need to visit on-stack nmethods, because all\n+      \/\/ nmethods are visited in one go via CodeCache::nmethods_do.\n+      ResourceMark rm;\n+      Threads::possibly_parallel_oops_do(_nworkers > 1, &pc_adjust_pointer_closure, nullptr);\n+      if (try_claim_code_cache_task()) {\n+        NMethodToOopClosure adjust_code(&pc_adjust_pointer_closure, NMethodToOopClosure::FixRelocations);\n+        CodeCache::nmethods_do(&adjust_code);\n+      }\n+    }\n+\n+    {\n+      \/\/ VM internal strong and weak roots.\n+      _oop_storage_iter.oops_do(&pc_adjust_pointer_closure);\n@@ -1515,5 +1425,0 @@\n-    if (_sub_tasks.try_claim_task(PSAdjustSubTask_code_cache)) {\n-      NMethodToOopClosure adjust_code(&pc_adjust_pointer_closure, NMethodToOopClosure::FixRelocations);\n-      CodeCache::nmethods_do(&adjust_code);\n-    }\n-    _sub_tasks.all_tasks_claimed();\n@@ -1593,0 +1498,1 @@\n+          \/\/ Empty space\n@@ -1597,1 +1503,0 @@\n-\n@@ -1641,5 +1546,4 @@\n-  HeapWord* old_dense_prefix_addr = dense_prefix(SpaceId(old_space_id));\n-  RegionData* old_region = _summary_data.region(_summary_data.addr_to_region_idx(old_dense_prefix_addr));\n-  HeapWord* bump_ptr = old_region->partial_obj_size() != 0\n-                       ? old_dense_prefix_addr + old_region->partial_obj_size()\n-                       : old_dense_prefix_addr;\n+  HeapWord* const old_dense_prefix_addr = dense_prefix(SpaceId(old_space_id));\n+  \/\/ The destination addr for the first live obj after dense-prefix.\n+  HeapWord* bump_ptr = old_dense_prefix_addr\n+                     + _summary_data.addr_to_region_ptr(old_dense_prefix_addr)->partial_obj_size();\n@@ -1650,1 +1554,2 @@\n-    HeapWord* dense_prefix_addr = dense_prefix(SpaceId(id));\n+    \/\/ Only verify objs after dense-prefix, because those before dense-prefix are not moved (forwarded).\n+    HeapWord* cur_addr = dense_prefix(SpaceId(id));\n@@ -1652,1 +1557,0 @@\n-    HeapWord* cur_addr = dense_prefix_addr;\n@@ -1734,1 +1638,1 @@\n-  for (unsigned int id = to_space_id; id + 1 > old_space_id; --id) {\n+  for (unsigned int id = last_space_id - 1; id + 1 > old_space_id; --id) {\n@@ -1792,1 +1696,0 @@\n-  uint _num_workers;\n@@ -1798,1 +1701,0 @@\n-      _num_workers(active_workers),\n@@ -1803,1 +1705,1 @@\n-    {\n+    if (worker_id == 0) {\n@@ -1805,2 +1707,2 @@\n-      PSParallelCompact::fill_dead_objs_in_dense_prefix(worker_id, _num_workers);\n-      log_trace(gc, phases)(\"Fill dense prefix by worker %u: %.3f ms\", worker_id, (Ticks::now() - start).seconds() * 1000);\n+      PSParallelCompact::fill_dead_objs_in_dense_prefix();\n+      log_trace(gc, phases)(\"Fill dense prefix by worker 0: %.3f ms\", (Ticks::now() - start).seconds() * 1000);\n@@ -1819,0 +1721,1 @@\n+      \/\/ The preceding live obj.\n@@ -1820,2 +1723,2 @@\n-      HeapWord* after_obj = obj_start + cast_to_oop(obj_start)->size();\n-      assert(after_obj == start, \"precondition\");\n+      HeapWord* obj_end = obj_start + cast_to_oop(obj_start)->size();\n+      assert(obj_end == start, \"precondition\");\n@@ -1835,1 +1738,1 @@\n-void PSParallelCompact::fill_dead_objs_in_dense_prefix(uint worker_id, uint num_workers) {\n+void PSParallelCompact::fill_dead_objs_in_dense_prefix() {\n@@ -1841,12 +1744,1 @@\n-  if (bottom == prefix_end) {\n-    return;\n-  }\n-\n-  size_t bottom_region = _summary_data.addr_to_region_idx(bottom);\n-  size_t prefix_end_region = _summary_data.addr_to_region_idx(prefix_end);\n-\n-  size_t start_region;\n-  size_t end_region;\n-  split_regions_for_worker(bottom_region, prefix_end_region,\n-                           worker_id, num_workers,\n-                           &start_region, &end_region);\n+  const size_t region_size = ParallelCompactData::RegionSize;\n@@ -1854,3 +1746,3 @@\n-  if (start_region == end_region) {\n-    return;\n-  }\n+  \/\/ Fill dead space in [start_addr, end_addr)\n+  HeapWord* const start_addr = bottom;\n+  HeapWord* const end_addr   = prefix_end;\n@@ -1858,2 +1750,11 @@\n-  HeapWord* const start_addr = _summary_data.region_to_addr(start_region);\n-  HeapWord* const end_addr = _summary_data.region_to_addr(end_region);\n+  for (HeapWord* cur_addr = start_addr; cur_addr < end_addr; \/* empty *\/) {\n+    RegionData* cur_region_ptr = _summary_data.addr_to_region_ptr(cur_addr);\n+    if (cur_region_ptr->data_size() == region_size) {\n+      \/\/ Full; no dead space. Next region.\n+      if (_summary_data.is_region_aligned(cur_addr)) {\n+        cur_addr += region_size;\n+      } else {\n+        cur_addr = _summary_data.region_align_up(cur_addr);\n+      }\n+      continue;\n+    }\n@@ -1861,10 +1762,4 @@\n-  \/\/ Skip live partial obj (if any) from previous region.\n-  HeapWord* cur_addr;\n-  RegionData* start_region_ptr = _summary_data.region(start_region);\n-  if (start_region_ptr->partial_obj_size() != 0) {\n-    HeapWord* partial_obj_start = start_region_ptr->partial_obj_addr();\n-    assert(bitmap->is_marked(partial_obj_start), \"inv\");\n-    cur_addr = partial_obj_start + cast_to_oop(partial_obj_start)->size();\n-  } else {\n-    cur_addr = start_addr;\n-  }\n+    \/\/ Fill dead space inside cur_region.\n+    if (_summary_data.is_region_aligned(cur_addr)) {\n+      cur_addr += cur_region_ptr->partial_obj_size();\n+    }\n@@ -1872,7 +1767,7 @@\n-  \/\/ end_addr is inclusive to handle regions starting with dead space.\n-  while (cur_addr <= end_addr) {\n-    \/\/ Use prefix_end to handle trailing obj in each worker region-chunk.\n-    HeapWord* live_start = bitmap->find_obj_beg(cur_addr, prefix_end);\n-    if (cur_addr != live_start) {\n-      \/\/ Only worker 0 handles proceeding dead space.\n-      if (cur_addr != start_addr || worker_id == 0) {\n+    HeapWord* region_end_addr = _summary_data.region_align_up(cur_addr + 1);\n+    assert(region_end_addr <= end_addr, \"inv\");\n+    while (cur_addr < region_end_addr) {\n+      \/\/ Use end_addr to allow filler-obj to cross region boundary.\n+      HeapWord* live_start = bitmap->find_obj_beg(cur_addr, end_addr);\n+      if (cur_addr != live_start) {\n+        \/\/ Found dead space [cur_addr, live_start).\n@@ -1881,0 +1776,6 @@\n+      if (live_start >= region_end_addr) {\n+        cur_addr = live_start;\n+        break;\n+      }\n+      assert(bitmap->is_marked(live_start), \"inv\");\n+      cur_addr = live_start + cast_to_oop(live_start)->size();\n@@ -1882,5 +1783,0 @@\n-    if (live_start >= end_addr) {\n-      break;\n-    }\n-    assert(bitmap->is_marked(live_start), \"inv\");\n-    cur_addr = live_start + cast_to_oop(live_start)->size();\n@@ -1919,7 +1815,31 @@\n-  HeapWord* cur_addr = bottom;\n-  while (cur_addr < dense_prefix_end) {\n-    oop obj = cast_to_oop(cur_addr);\n-    oopDesc::verify(obj);\n-    if (!mark_bitmap()->is_marked(cur_addr)) {\n-      Klass* k = cast_to_oop(cur_addr)->klass();\n-      assert(k == Universe::fillerArrayKlass() || k == vmClasses::FillerObject_klass(), \"inv\");\n+\n+  const size_t region_size = ParallelCompactData::RegionSize;\n+\n+  for (HeapWord* cur_addr = bottom; cur_addr < dense_prefix_end; \/* empty *\/) {\n+    RegionData* cur_region_ptr = _summary_data.addr_to_region_ptr(cur_addr);\n+    if (cur_region_ptr->data_size() == region_size) {\n+      \/\/ Full; no dead space. Next region.\n+      if (_summary_data.is_region_aligned(cur_addr)) {\n+        cur_addr += region_size;\n+      } else {\n+        cur_addr = _summary_data.region_align_up(cur_addr);\n+      }\n+      continue;\n+    }\n+\n+    \/\/ This region contains filler objs.\n+    if (_summary_data.is_region_aligned(cur_addr)) {\n+      cur_addr += cur_region_ptr->partial_obj_size();\n+    }\n+\n+    HeapWord* region_end_addr = _summary_data.region_align_up(cur_addr + 1);\n+    assert(region_end_addr <= dense_prefix_end, \"inv\");\n+\n+    while (cur_addr < region_end_addr) {\n+      oop obj = cast_to_oop(cur_addr);\n+      oopDesc::verify(obj);\n+      if (!mark_bitmap()->is_marked(cur_addr)) {\n+        Klass* k = cast_to_oop(cur_addr)->klass();\n+        assert(k == Universe::fillerArrayKlass() || k == vmClasses::FillerObject_klass(), \"inv\");\n+      }\n+      cur_addr += obj->size();\n@@ -1927,1 +1847,0 @@\n-    cur_addr += obj->size();\n","filename":"src\/hotspot\/share\/gc\/parallel\/psParallelCompact.cpp","additions":206,"deletions":287,"binary":false,"changes":493,"status":"modified"},{"patch":"@@ -59,1 +59,0 @@\n-#include \"gc\/shared\/strongRootsScope.hpp\"\n@@ -433,1 +432,1 @@\n-bool PSParallelCompactNew::check_maximum_compaction() {\n+bool PSParallelCompactNew::check_maximum_compaction(bool should_do_max_compaction) {\n@@ -441,12 +440,2 @@\n-  \/\/ JVM flags\n-  const uint total_invocations = heap->total_full_collections();\n-  assert(total_invocations >= _maximum_compaction_gc_num, \"sanity\");\n-  const size_t gcs_since_max = total_invocations - _maximum_compaction_gc_num;\n-  const bool is_interval_ended = gcs_since_max > HeapMaximumCompactionInterval;\n-\n-  if (is_max_on_system_gc || is_interval_ended) {\n-    _maximum_compaction_gc_num = total_invocations;\n-    return true;\n-  }\n-\n-  return false;\n+  return should_do_max_compaction\n+      || is_max_on_system_gc;\n@@ -482,1 +471,1 @@\n-bool PSParallelCompactNew::invoke(bool clear_all_soft_refs, bool serial) {\n+bool PSParallelCompactNew::invoke(bool clear_all_soft_refs, bool should_do_max_compaction) {\n@@ -490,13 +479,0 @@\n-  ParallelScavengeHeap* heap = ParallelScavengeHeap::heap();\n-  clear_all_soft_refs = clear_all_soft_refs\n-                     || heap->soft_ref_policy()->should_clear_all_soft_refs();\n-\n-  return PSParallelCompactNew::invoke_no_policy(clear_all_soft_refs, serial);\n-}\n-\n-\/\/ This method contains no policy. You should probably\n-\/\/ be calling invoke() instead.\n-bool PSParallelCompactNew::invoke_no_policy(bool clear_all_soft_refs, bool serial) {\n-  assert(SafepointSynchronize::is_at_safepoint(), \"must be at a safepoint\");\n-  assert(ref_processor() != nullptr, \"Sanity\");\n-\n@@ -514,5 +490,0 @@\n-  \/\/ The scope of casr should end after code that can change\n-  \/\/ SoftRefPolicy::_should_clear_all_soft_refs.\n-  ClearedAllSoftRefs casr(clear_all_soft_refs,\n-                          heap->soft_ref_policy());\n-\n@@ -532,1 +503,1 @@\n-    if (serial || check_maximum_compaction()) {\n+    if (check_maximum_compaction(should_do_max_compaction)) {\n@@ -592,1 +563,1 @@\n-    size_policy->major_collection_end(old_gen->used_in_bytes(), gc_cause);\n+    size_policy->major_collection_end();\n@@ -594,63 +565,1 @@\n-    if (UseAdaptiveSizePolicy) {\n-      log_debug(gc, ergo)(\"AdaptiveSizeStart: collection: %d \", heap->total_collections());\n-      log_trace(gc, ergo)(\"old_gen_capacity: %zu young_gen_capacity: %zu\",\n-                          old_gen->capacity_in_bytes(), young_gen->capacity_in_bytes());\n-\n-      \/\/ Don't check if the size_policy is ready here.  Let\n-      \/\/ the size_policy check that internally.\n-      if (UseAdaptiveGenerationSizePolicyAtMajorCollection &&\n-          AdaptiveSizePolicy::should_update_promo_stats(gc_cause)) {\n-        \/\/ Swap the survivor spaces if from_space is empty. The\n-        \/\/ resize_young_gen() called below is normally used after\n-        \/\/ a successful young GC and swapping of survivor spaces;\n-        \/\/ otherwise, it will fail to resize the young gen with\n-        \/\/ the current implementation.\n-        if (young_gen->from_space()->is_empty()) {\n-          young_gen->from_space()->clear(SpaceDecorator::Mangle);\n-          young_gen->swap_spaces();\n-        }\n-\n-        \/\/ Calculate optimal free space amounts\n-        assert(young_gen->max_gen_size() >\n-          young_gen->from_space()->capacity_in_bytes() +\n-          young_gen->to_space()->capacity_in_bytes(),\n-          \"Sizes of space in young gen are out-of-bounds\");\n-\n-        size_t young_live = young_gen->used_in_bytes();\n-        size_t eden_live = young_gen->eden_space()->used_in_bytes();\n-        size_t old_live = old_gen->used_in_bytes();\n-        size_t cur_eden = young_gen->eden_space()->capacity_in_bytes();\n-        size_t max_old_gen_size = old_gen->max_gen_size();\n-        size_t max_eden_size = young_gen->max_gen_size() -\n-          young_gen->from_space()->capacity_in_bytes() -\n-          young_gen->to_space()->capacity_in_bytes();\n-\n-        \/\/ Used for diagnostics\n-        size_policy->clear_generation_free_space_flags();\n-\n-        size_policy->compute_generations_free_space(young_live,\n-                                                    eden_live,\n-                                                    old_live,\n-                                                    cur_eden,\n-                                                    max_old_gen_size,\n-                                                    max_eden_size,\n-                                                    true \/* full gc*\/);\n-\n-        size_policy->check_gc_overhead_limit(eden_live,\n-                                             max_old_gen_size,\n-                                             max_eden_size,\n-                                             true \/* full gc*\/,\n-                                             gc_cause,\n-                                             heap->soft_ref_policy());\n-\n-        size_policy->decay_supplemental_growth(true \/* full gc*\/);\n-\n-        heap->resize_old_gen(\n-          size_policy->calculated_old_free_size_in_bytes());\n-\n-        heap->resize_young_gen(size_policy->calculated_eden_size_in_bytes(),\n-                               size_policy->calculated_survivor_size_in_bytes());\n-      }\n-\n-      log_debug(gc, ergo)(\"AdaptiveSizeStop: collection: %d \", heap->total_collections());\n-    }\n+    size_policy->sample_old_gen_used_bytes(MAX2(pre_gc_values.old_gen_used(), old_gen->used_in_bytes()));\n@@ -658,5 +567,2 @@\n-    if (UsePerfData) {\n-      PSGCAdaptivePolicyCounters* const counters = ParallelScavengeHeap::gc_policy_counters();\n-      counters->update_counters();\n-      counters->update_old_capacity(old_gen->capacity_in_bytes());\n-      counters->update_young_capacity(young_gen->capacity_in_bytes());\n+    if (UseAdaptiveSizePolicy) {\n+      heap->resize_after_full_gc();\n@@ -681,0 +587,2 @@\n+\n+    size_policy->record_gc_pause_end_instant();\n@@ -683,0 +591,2 @@\n+  heap->gc_epilogue(true);\n+\n@@ -690,2 +600,0 @@\n-  AdaptiveSizePolicyOutput::print(size_policy, heap->total_collections());\n-\n@@ -712,3 +620,1 @@\n-    MarkingNMethodClosure mark_and_push_in_blobs(&cm->_mark_and_push_closure,\n-                                                 !NMethodToOopClosure::FixRelocations,\n-                                                 true \/* keepalive nmethods *\/);\n+    MarkingNMethodClosure mark_and_push_in_blobs(&cm->_mark_and_push_closure);\n@@ -739,1 +645,2 @@\n-  StrongRootsScope _strong_roots_scope; \/\/ needed for Threads::possibly_parallel_threads_do\n+  NMethodMarkingScope _nmethod_marking_scope;\n+  ThreadsClaimTokenScope _threads_claim_token_scope;\n@@ -747,1 +654,2 @@\n-      _strong_roots_scope(active_workers),\n+      _nmethod_marking_scope(),\n+      _threads_claim_token_scope(),\n","filename":"src\/hotspot\/share\/gc\/parallel\/psParallelCompactNew.cpp","additions":18,"deletions":110,"binary":false,"changes":128,"status":"modified"},{"patch":"@@ -221,1 +221,1 @@\n-    return !Atomic::cmpxchg(&_claimed, false, true);\n+    return !AtomicAccess::cmpxchg(&_claimed, false, true);\n@@ -279,1 +279,1 @@\n-  static bool check_maximum_compaction();\n+  static bool check_maximum_compaction(bool should_do_max_compaction);\n@@ -295,2 +295,1 @@\n-  static bool invoke(bool maximum_heap_compaction, bool serial);\n-  static bool invoke_no_policy(bool maximum_heap_compaction, bool serial);\n+  static bool invoke(bool maximum_heap_compaction, bool should_do_max_compaction);\n","filename":"src\/hotspot\/share\/gc\/parallel\/psParallelCompactNew.hpp","additions":3,"deletions":4,"binary":false,"changes":7,"status":"modified"},{"patch":"@@ -2,1 +2,1 @@\n- * Copyright (c) 2002, 2024, Oracle and\/or its affiliates. All rights reserved.\n+ * Copyright (c) 2002, 2025, Oracle and\/or its affiliates. All rights reserved.\n@@ -34,1 +34,1 @@\n-#include \"gc\/parallel\/psScavenge.inline.hpp\"\n+#include \"gc\/parallel\/psScavenge.hpp\"\n@@ -53,4 +53,0 @@\n-inline void PSPromotionManager::push_depth(ScannerTask task) {\n-  claimed_stack_depth()->push(task);\n-}\n-\n@@ -64,2 +60,2 @@\n-    Prefetch::write(obj->mark_addr(), 0);\n-    push_depth(ScannerTask(p));\n+    Prefetch::write(obj->base_addr(), oopDesc::mark_offset_in_bytes());\n+    claimed_stack_depth()->push(ScannerTask(p));\n@@ -142,1 +138,2 @@\n-  assert(should_scavenge(&o), \"Sanity\");\n+  assert(PSScavenge::is_obj_in_young(o), \"precondition\");\n+  assert(!PSScavenge::is_obj_in_to_space(o), \"precondition\");\n@@ -156,0 +153,74 @@\n+inline HeapWord* PSPromotionManager::allocate_in_young_gen(Klass* klass,\n+                                                           size_t obj_size,\n+                                                           uint age) {\n+  HeapWord* result = _young_lab.allocate(obj_size);\n+  if (result != nullptr) {\n+    return result;\n+  }\n+  if (_young_gen_is_full) {\n+    return nullptr;\n+  }\n+  \/\/ Do we allocate directly, or flush and refill?\n+  if (obj_size > (YoungPLABSize \/ 2)) {\n+    \/\/ Allocate this object directly\n+    result = young_space()->cas_allocate(obj_size);\n+    promotion_trace_event(cast_to_oop(result), klass, obj_size, age, false, nullptr);\n+  } else {\n+    \/\/ Flush and fill\n+    _young_lab.flush();\n+\n+    HeapWord* lab_base = young_space()->cas_allocate(YoungPLABSize);\n+    if (lab_base != nullptr) {\n+      _young_lab.initialize(MemRegion(lab_base, YoungPLABSize));\n+      \/\/ Try the young lab allocation again.\n+      result = _young_lab.allocate(obj_size);\n+      promotion_trace_event(cast_to_oop(result), klass, obj_size, age, false, &_young_lab);\n+    } else {\n+      _young_gen_is_full = true;\n+    }\n+  }\n+  if (result == nullptr && !_young_gen_is_full && !_young_gen_has_alloc_failure) {\n+    _young_gen_has_alloc_failure = true;\n+  }\n+  return result;\n+}\n+\n+inline HeapWord* PSPromotionManager::allocate_in_old_gen(Klass* klass,\n+                                                         size_t obj_size,\n+                                                         uint age) {\n+#ifndef PRODUCT\n+  if (ParallelScavengeHeap::heap()->promotion_should_fail()) {\n+    return nullptr;\n+  }\n+#endif  \/\/ #ifndef PRODUCT\n+\n+  HeapWord* result = _old_lab.allocate(obj_size);\n+  if (result != nullptr) {\n+    return result;\n+  }\n+  if (_old_gen_is_full) {\n+    return nullptr;\n+  }\n+  \/\/ Do we allocate directly, or flush and refill?\n+  if (obj_size > (OldPLABSize \/ 2)) {\n+    \/\/ Allocate this object directly\n+    result = old_gen()->allocate(obj_size);\n+    promotion_trace_event(cast_to_oop(result), klass, obj_size, age, true, nullptr);\n+  } else {\n+    \/\/ Flush and fill\n+    _old_lab.flush();\n+\n+    HeapWord* lab_base = old_gen()->allocate(OldPLABSize);\n+    if (lab_base != nullptr) {\n+      _old_lab.initialize(MemRegion(lab_base, OldPLABSize));\n+      \/\/ Try the old lab allocation again.\n+      result = _old_lab.allocate(obj_size);\n+      promotion_trace_event(cast_to_oop(result), klass, obj_size, age, true, &_old_lab);\n+    }\n+  }\n+  if (result == nullptr) {\n+    _old_gen_is_full = true;\n+  }\n+  return result;\n+}\n+\n@@ -164,3 +235,1 @@\n-  assert(should_scavenge(&o), \"Sanity\");\n-\n-  oop new_obj = nullptr;\n+  HeapWord* new_obj_addr = nullptr;\n@@ -190,22 +259,1 @@\n-      new_obj = cast_to_oop(_young_lab.allocate(new_obj_size));\n-      if (new_obj == nullptr && !_young_gen_is_full) {\n-        \/\/ Do we allocate directly, or flush and refill?\n-        if (new_obj_size > (YoungPLABSize \/ 2)) {\n-          \/\/ Allocate this object directly\n-          new_obj = cast_to_oop(young_space()->cas_allocate(new_obj_size));\n-          promotion_trace_event(new_obj, klass, new_obj_size, age, false, nullptr);\n-        } else {\n-          \/\/ Flush and fill\n-          _young_lab.flush();\n-\n-          HeapWord* lab_base = young_space()->cas_allocate(YoungPLABSize);\n-          if (lab_base != nullptr) {\n-            _young_lab.initialize(MemRegion(lab_base, YoungPLABSize));\n-            \/\/ Try the young lab allocation again.\n-            new_obj = cast_to_oop(_young_lab.allocate(new_obj_size));\n-            promotion_trace_event(new_obj, klass, new_obj_size, age, false, &_young_lab);\n-          } else {\n-            _young_gen_is_full = true;\n-          }\n-        }\n-      }\n+      new_obj_addr = allocate_in_young_gen(klass, new_obj_size, age);\n@@ -216,3 +264,3 @@\n-  if (new_obj == nullptr) {\n-#ifndef PRODUCT\n-    if (ParallelScavengeHeap::heap()->promotion_should_fail()) {\n+  if (new_obj_addr == nullptr) {\n+    new_obj_addr = allocate_in_old_gen(klass, new_obj_size, age);\n+    if (new_obj_addr == nullptr) {\n@@ -221,36 +269,0 @@\n-#endif  \/\/ #ifndef PRODUCT\n-\n-    new_obj = cast_to_oop(_old_lab.allocate(new_obj_size));\n-\n-    if (new_obj == nullptr) {\n-      if (!_old_gen_is_full) {\n-        \/\/ Do we allocate directly, or flush and refill?\n-        if (new_obj_size > (OldPLABSize \/ 2)) {\n-          \/\/ Allocate this object directly\n-          new_obj = cast_to_oop(old_gen()->allocate(new_obj_size));\n-          promotion_trace_event(new_obj, klass, new_obj_size, age, true, nullptr);\n-        } else {\n-          \/\/ Flush and fill\n-          _old_lab.flush();\n-\n-          HeapWord* lab_base = old_gen()->allocate(OldPLABSize);\n-          if(lab_base != nullptr) {\n-            _old_lab.initialize(MemRegion(lab_base, OldPLABSize));\n-            \/\/ Try the old lab allocation again.\n-            new_obj = cast_to_oop(_old_lab.allocate(new_obj_size));\n-            promotion_trace_event(new_obj, klass, new_obj_size, age, true, &_old_lab);\n-          }\n-        }\n-      }\n-\n-      \/\/ This is the promotion failed test, and code handling.\n-      \/\/ The code belongs here for two reasons. It is slightly\n-      \/\/ different than the code below, and cannot share the\n-      \/\/ CAS testing code. Keeping the code here also minimizes\n-      \/\/ the impact on the common case fast path code.\n-\n-      if (new_obj == nullptr) {\n-        _old_gen_is_full = true;\n-        return oop_promotion_failed(o, test_mark);\n-      }\n-    }\n@@ -260,1 +272,1 @@\n-  assert(new_obj != nullptr, \"allocation should have succeeded\");\n+  assert(new_obj_addr != nullptr, \"allocation should have succeeded\");\n@@ -263,1 +275,1 @@\n-  Copy::aligned_disjoint_words(cast_from_oop<HeapWord*>(o), cast_from_oop<HeapWord*>(new_obj), new_obj_size);\n+  Copy::aligned_disjoint_words(cast_from_oop<HeapWord*>(o), new_obj_addr, new_obj_size);\n@@ -270,1 +282,1 @@\n-  oop forwardee = o->forward_to_atomic(new_obj, test_mark, memory_order_relaxed);\n+  oop forwardee = o->forward_to_atomic(cast_to_oop(new_obj_addr), test_mark, memory_order_relaxed);\n@@ -273,0 +285,1 @@\n+    oop new_obj = cast_to_oop(new_obj_addr);\n@@ -290,1 +303,1 @@\n-    \/\/ So, the is->objArray() test would be very infrequent.\n+    \/\/ So, the objArray test would be very infrequent.\n@@ -292,1 +305,1 @@\n-        new_obj->is_objArray() &&\n+        klass->is_objArray_klass() &&\n@@ -299,2 +312,1 @@\n-      if (StringDedup::is_enabled() &&\n-          java_lang_String::is_instance(new_obj) &&\n+      if (StringDedup::is_enabled_string(klass) &&\n@@ -308,1 +320,0 @@\n-\n@@ -313,1 +324,1 @@\n-      _old_lab.unallocate_object(cast_from_oop<HeapWord*>(new_obj), new_obj_size);\n+      _old_lab.unallocate_object(new_obj_addr, new_obj_size);\n@@ -315,1 +326,1 @@\n-      _young_lab.unallocate_object(cast_from_oop<HeapWord*>(new_obj), new_obj_size);\n+      _young_lab.unallocate_object(new_obj_addr, new_obj_size);\n@@ -325,1 +336,0 @@\n-  assert(should_scavenge(p, true), \"revisiting object?\");\n","filename":"src\/hotspot\/share\/gc\/parallel\/psPromotionManager.inline.hpp","additions":94,"deletions":84,"binary":false,"changes":178,"status":"modified"},{"patch":"@@ -37,1 +37,1 @@\n-#include \"gc\/parallel\/psScavenge.inline.hpp\"\n+#include \"gc\/parallel\/psScavenge.hpp\"\n@@ -55,1 +55,0 @@\n-#include \"gc\/shared\/strongRootsScope.hpp\"\n@@ -103,1 +102,1 @@\n-        MarkingNMethodClosure code_closure(&roots_to_old_closure, NMethodToOopClosure::FixRelocations, false \/* keepalive nmethods *\/);\n+        NMethodToOopClosure code_closure(&roots_to_old_closure, NMethodToOopClosure::FixRelocations);\n@@ -131,1 +130,1 @@\n-      pm->drain_stacks_depth(true);\n+      pm->drain_stacks(true);\n@@ -152,2 +151,0 @@\n-protected:\n-  MutableSpace* _to_space;\n@@ -158,3 +155,0 @@\n-    ParallelScavengeHeap* heap = ParallelScavengeHeap::heap();\n-    _to_space = heap->young_gen()->to_space();\n-\n@@ -225,1 +219,1 @@\n-  uint _worker_id;\n+  PSPromotionManager* _pm;\n@@ -227,1 +221,1 @@\n-  PSThreadRootsTaskClosure(uint worker_id) : _worker_id(worker_id) { }\n+  PSThreadRootsTaskClosure(PSPromotionManager* pm) : _pm(pm) {}\n@@ -231,3 +225,1 @@\n-    PSPromotionManager* pm = PSPromotionManager::gc_thread_promotion_manager(_worker_id);\n-    PSScavengeRootsClosure roots_closure(pm);\n-    MarkingNMethodClosure roots_in_nmethods(&roots_closure, NMethodToOopClosure::FixRelocations, false \/* keepalive nmethods *\/);\n+    PSScavengeRootsClosure roots_closure(_pm);\n@@ -235,1 +227,2 @@\n-    thread->oops_do(&roots_closure, &roots_in_nmethods);\n+    \/\/ No need to visit nmethods, because they are handled by ScavengableNMethods.\n+    thread->oops_do(&roots_closure, nullptr);\n@@ -238,1 +231,1 @@\n-    pm->drain_stacks(false);\n+    _pm->drain_stacks(false);\n@@ -243,1 +236,1 @@\n-  StrongRootsScope _strong_roots_scope; \/\/ needed for Threads::possibly_parallel_threads_do\n+  ThreadsClaimTokenScope _threads_claim_token_scope; \/\/ needed for Threads::possibly_parallel_threads_do\n@@ -256,1 +249,1 @@\n-    _strong_roots_scope(active_workers),\n+    _threads_claim_token_scope(),\n@@ -272,0 +265,1 @@\n+    PSPromotionManager* pm = PSPromotionManager::gc_thread_promotion_manager(worker_id);\n@@ -277,1 +271,0 @@\n-        PSPromotionManager* pm = PSPromotionManager::gc_thread_promotion_manager(worker_id);\n@@ -297,2 +290,2 @@\n-    PSThreadRootsTaskClosure closure(worker_id);\n-    Threads::possibly_parallel_threads_do(_active_workers > 1 \/* is_par *\/, &closure);\n+    PSThreadRootsTaskClosure thread_closure(pm);\n+    Threads::possibly_parallel_threads_do(_active_workers > 1 \/* is_par *\/, &thread_closure);\n@@ -302,3 +295,3 @@\n-      PSPromotionManager* pm = PSPromotionManager::gc_thread_promotion_manager(worker_id);\n-      PSScavengeRootsClosure closure(pm);\n-      _oop_storage_strong_par_state.oops_do(&closure);\n+      PSScavengeRootsClosure root_closure(pm);\n+      _oop_storage_strong_par_state.oops_do(&root_closure);\n+\n@@ -324,5 +317,0 @@\n-  \/\/ Check for potential problems.\n-  if (!should_attempt_scavenge()) {\n-    return false;\n-  }\n-\n@@ -346,2 +334,1 @@\n-  assert(young_gen->to_space()->is_empty(),\n-         \"Attempt to scavenge with live objects in to_space\");\n+  assert(young_gen->to_space()->is_empty(), \"precondition\");\n@@ -351,4 +338,2 @@\n-  if (AdaptiveSizePolicy::should_update_eden_stats(gc_cause)) {\n-    \/\/ Gather the feedback data for eden occupancy.\n-    young_gen->eden_space()->accumulate_statistics();\n-  }\n+  \/\/ Gather the feedback data for eden occupancy.\n+  young_gen->eden_space()->accumulate_statistics();\n@@ -427,1 +412,1 @@\n-      WeakProcessor::weak_oops_do(&ParallelScavengeHeap::heap()->workers(), &_is_alive_closure, &root_closure, 1);\n+      WeakProcessor::weak_oops_do(&heap->workers(), &_is_alive_closure, &root_closure, 1);\n@@ -439,4 +424,3 @@\n-    \/\/ Let the size policy know we're done.  Note that we count promotion\n-    \/\/ failure cleanup time as part of the collection (otherwise, we're\n-    \/\/ implicitly saying it's mutator time).\n-    size_policy->minor_collection_end(gc_cause);\n+    \/\/ This is an underestimate, since it excludes time on auto-resizing. The\n+    \/\/ most expensive part in auto-resizing is commit\/uncommit OS API calls.\n+    size_policy->minor_collection_end(young_gen->eden_space()->capacity_in_bytes());\n@@ -451,0 +435,1 @@\n+      assert(old_gen->used_in_bytes() >= pre_gc_values.old_gen_used(), \"inv\");\n@@ -453,0 +438,1 @@\n+      size_policy->sample_old_gen_used_bytes(old_gen->used_in_bytes());\n@@ -454,21 +440,2 @@\n-      \/\/ A successful scavenge should restart the GC time limit count which is\n-      \/\/ for full GC's.\n-      size_policy->reset_gc_overhead_limit_count();\n-        \/\/ Calculate the new survivor size and tenuring threshold\n-\n-        log_debug(gc, ergo)(\"AdaptiveSizeStart:  collection: %d \", heap->total_collections());\n-        log_trace(gc, ergo)(\"old_gen_capacity: %zu young_gen_capacity: %zu\",\n-                            old_gen->capacity_in_bytes(), young_gen->capacity_in_bytes());\n-\n-        if (UsePerfData) {\n-          PSGCAdaptivePolicyCounters* counters = heap->gc_policy_counters();\n-          counters->update_old_eden_size(\n-            size_policy->calculated_eden_size_in_bytes());\n-          counters->update_old_promo_size(\n-            size_policy->calculated_promo_size_in_bytes());\n-          counters->update_old_capacity(old_gen->capacity_in_bytes());\n-          counters->update_young_capacity(young_gen->capacity_in_bytes());\n-          counters->update_survived(survived);\n-          counters->update_promoted(promoted);\n-          counters->update_survivor_overflowed(_survivor_overflow);\n-        }\n+        _tenuring_threshold = size_policy->compute_tenuring_threshold(_survivor_overflow,\n+                                                                      _tenuring_threshold);\n@@ -477,1 +444,1 @@\n-        size_t max_young_size = young_gen->max_gen_size();\n+        log_debug(gc, age)(\"New threshold %u (max threshold %u)\", _tenuring_threshold, MaxTenuringThreshold);\n@@ -479,8 +446,3 @@\n-        \/\/ Deciding a free ratio in the young generation is tricky, so if\n-        \/\/ MinHeapFreeRatio or MaxHeapFreeRatio are in use (implicating\n-        \/\/ that the old generation size may have been limited because of them) we\n-        \/\/ should then limit our young generation size using NewRatio to have it\n-        \/\/ follow the old generation size.\n-        if (MinHeapFreeRatio != 0 || MaxHeapFreeRatio != 100) {\n-          max_young_size = MIN2(old_gen->capacity_in_bytes() \/ NewRatio,\n-                                young_gen->max_gen_size());\n+        if (young_gen->is_from_to_layout()) {\n+          size_policy->print_stats(_survivor_overflow);\n+          heap->resize_after_young_gc(_survivor_overflow);\n@@ -489,14 +451,3 @@\n-        size_t survivor_limit =\n-          size_policy->max_survivor_size(max_young_size);\n-        _tenuring_threshold =\n-          size_policy->compute_survivor_space_size_and_threshold(_survivor_overflow,\n-                                                                 _tenuring_threshold,\n-                                                                 survivor_limit);\n-\n-        log_debug(gc, age)(\"Desired survivor size %zu bytes, new threshold %u (max threshold %u)\",\n-                           size_policy->calculated_survivor_size_in_bytes(),\n-                           _tenuring_threshold, MaxTenuringThreshold);\n-\n-          PSGCAdaptivePolicyCounters* counters = heap->gc_policy_counters();\n-          counters->update_tenuring_threshold(_tenuring_threshold);\n-          counters->update_survivor_size_counters();\n+          GCPolicyCounters* counters = ParallelScavengeHeap::gc_policy_counters();\n+          counters->tenuring_threshold()->set_value(_tenuring_threshold);\n+          counters->desired_survivor_size()->set_value(young_gen->from_space()->capacity_in_bytes());\n@@ -506,36 +457,6 @@\n-        \/\/ Do call at minor collections?\n-        \/\/ Don't check if the size_policy is ready at this\n-        \/\/ level.  Let the size_policy check that internally.\n-        if (UseAdaptiveGenerationSizePolicyAtMinorCollection &&\n-            AdaptiveSizePolicy::should_update_eden_stats(gc_cause)) {\n-          \/\/ Calculate optimal free space amounts\n-          assert(young_gen->max_gen_size() >\n-                 young_gen->from_space()->capacity_in_bytes() +\n-                 young_gen->to_space()->capacity_in_bytes(),\n-                 \"Sizes of space in young gen are out-of-bounds\");\n-\n-          size_t young_live = young_gen->used_in_bytes();\n-          size_t eden_live = young_gen->eden_space()->used_in_bytes();\n-          size_t cur_eden = young_gen->eden_space()->capacity_in_bytes();\n-          size_t max_old_gen_size = old_gen->max_gen_size();\n-          size_t max_eden_size = max_young_size -\n-                                 young_gen->from_space()->capacity_in_bytes() -\n-                                 young_gen->to_space()->capacity_in_bytes();\n-\n-          \/\/ Used for diagnostics\n-          size_policy->clear_generation_free_space_flags();\n-\n-          size_policy->compute_eden_space_size(young_live,\n-                                               eden_live,\n-                                               cur_eden,\n-                                               max_eden_size,\n-                                               false \/* not full gc*\/);\n-\n-          size_policy->check_gc_overhead_limit(eden_live,\n-                                               max_old_gen_size,\n-                                               max_eden_size,\n-                                               false \/* not full gc*\/,\n-                                               gc_cause,\n-                                               heap->soft_ref_policy());\n-\n-          size_policy->decay_supplemental_growth(false \/* not full gc*\/);\n+        {\n+          \/\/ In case the counter overflows\n+          uint num_minor_gcs = heap->total_collections() > heap->total_full_collections()\n+                                 ? heap->total_collections() - heap->total_full_collections()\n+                                 : 1;\n+          size_policy->decay_supplemental_growth(num_minor_gcs);\n@@ -543,13 +464,0 @@\n-        \/\/ Resize the young generation at every collection\n-        \/\/ even if new sizes have not been calculated.  This is\n-        \/\/ to allow resizes that may have been inhibited by the\n-        \/\/ relative location of the \"to\" and \"from\" spaces.\n-\n-        \/\/ Resizing the old gen at young collections can cause increases\n-        \/\/ that don't feed back to the generation sizing policy until\n-        \/\/ a full collection.  Don't resize the old gen here.\n-\n-        heap->resize_young_gen(size_policy->calculated_eden_size_in_bytes(),\n-                               size_policy->calculated_survivor_size_in_bytes());\n-\n-        log_debug(gc, ergo)(\"AdaptiveSizeStop: collection: %d \", heap->total_collections());\n@@ -564,2 +472,0 @@\n-      heap->gc_policy_counters()->update_counters();\n-\n@@ -569,0 +475,2 @@\n+\n+      heap->gc_epilogue(false);\n@@ -575,0 +483,2 @@\n+    size_policy->record_gc_pause_end_instant();\n+\n@@ -593,2 +503,0 @@\n-  AdaptiveSizePolicyOutput::print(size_policy, heap->total_collections());\n-\n@@ -609,28 +517,0 @@\n-bool PSScavenge::should_attempt_scavenge() {\n-  ParallelScavengeHeap* heap = ParallelScavengeHeap::heap();\n-\n-  PSYoungGen* young_gen = heap->young_gen();\n-  PSOldGen* old_gen = heap->old_gen();\n-\n-  if (!young_gen->to_space()->is_empty()) {\n-    \/\/ To-space is not empty; should run full-gc instead.\n-    return false;\n-  }\n-\n-  \/\/ Test to see if the scavenge will likely fail.\n-  PSAdaptiveSizePolicy* policy = heap->size_policy();\n-\n-  size_t avg_promoted = (size_t) policy->padded_average_promoted_in_bytes();\n-  size_t promotion_estimate = MIN2(avg_promoted, young_gen->used_in_bytes());\n-  \/\/ Total free size after possible old gen expansion\n-  size_t free_in_old_gen = old_gen->max_gen_size() - old_gen->used_in_bytes();\n-  bool result = promotion_estimate < free_in_old_gen;\n-\n-  log_trace(ergo)(\"%s scavenge: average_promoted %zu padded_average_promoted %zu free in old gen %zu\",\n-                result ? \"Do\" : \"Skip\", (size_t) policy->average_promoted_in_bytes(),\n-                (size_t) policy->padded_average_promoted_in_bytes(),\n-                free_in_old_gen);\n-\n-  return result;\n-}\n-\n@@ -663,1 +543,1 @@\n-  assert(old_gen->reserved().end() <= young_gen->eden_space()->bottom(),\n+  assert(old_gen->reserved().end() == young_gen->reserved().start(),\n@@ -665,1 +545,1 @@\n-  set_young_generation_boundary(young_gen->eden_space()->bottom());\n+  set_young_generation_boundary(young_gen->reserved().start());\n","filename":"src\/hotspot\/share\/gc\/parallel\/psScavenge.cpp","additions":47,"deletions":167,"binary":false,"changes":214,"status":"modified"},{"patch":"@@ -25,0 +25,1 @@\n+#include \"classfile\/classLoaderDataGraph.hpp\"\n@@ -41,0 +42,1 @@\n+#include \"gc\/shared\/oopStorageSet.inline.hpp\"\n@@ -43,0 +45,1 @@\n+#include \"gc\/shared\/scavengableNMethods.hpp\"\n@@ -45,1 +48,0 @@\n-#include \"gc\/shared\/strongRootsScope.hpp\"\n@@ -94,1 +96,6 @@\n-    ClassLoaderData* _scanned_cld;\n+  public:\n+    \/\/ Records whether this CLD contains oops pointing into young-gen after scavenging.\n+    bool _has_oops_into_young_gen;\n+\n+    CLDOopClosure(DefNewGeneration* g) : OffHeapScanClosure(g),\n+      _has_oops_into_young_gen(false) {}\n@@ -96,2 +103,1 @@\n-    template <typename T>\n-    void do_oop_work(T* p) {\n+    void do_oop(oop* p) {\n@@ -101,3 +107,2 @@\n-        assert(_scanned_cld != nullptr, \"inv\");\n-        if (is_in_young_gen(new_obj) && !_scanned_cld->has_modified_oops()) {\n-          _scanned_cld->record_modified_oops();\n+        if (!_has_oops_into_young_gen && is_in_young_gen(new_obj)) {\n+          _has_oops_into_young_gen = true;\n@@ -108,10 +113,0 @@\n-  public:\n-    CLDOopClosure(DefNewGeneration* g) : OffHeapScanClosure(g),\n-      _scanned_cld(nullptr) {}\n-\n-    void set_scanned_cld(ClassLoaderData* cld) {\n-      assert(cld == nullptr || _scanned_cld == nullptr, \"Must be\");\n-      _scanned_cld = cld;\n-    }\n-\n-    void do_oop(oop* p)       { do_oop_work(p); }\n@@ -121,1 +116,1 @@\n-  CLDOopClosure _oop_closure;\n+  DefNewGeneration* _g;\n@@ -123,1 +118,1 @@\n-  CLDScanClosure(DefNewGeneration* g) : _oop_closure(g) {}\n+  CLDScanClosure(DefNewGeneration* g) : _g(g) {}\n@@ -128,1 +123,3 @@\n-    if (cld->has_modified_oops()) {\n+    if (!cld->has_modified_oops()) {\n+      return;\n+    }\n@@ -130,3 +127,1 @@\n-      \/\/ Tell the closure which CLD is being scanned so that it can be dirtied\n-      \/\/ if oops are left pointing into the young gen.\n-      _oop_closure.set_scanned_cld(cld);\n+    CLDOopClosure oop_closure{_g};\n@@ -134,2 +129,2 @@\n-      \/\/ Clean the cld since we're going to scavenge all the metadata.\n-      cld->oops_do(&_oop_closure, ClassLoaderData::_claim_none, \/*clear_modified_oops*\/true);\n+    \/\/ Clean the cld since we're going to scavenge all the metadata.\n+    cld->oops_do(&oop_closure, ClassLoaderData::_claim_none, \/*clear_modified_oops*\/true);\n@@ -137,1 +132,2 @@\n-      _oop_closure.set_scanned_cld(nullptr);\n+    if (oop_closure._has_oops_into_young_gen) {\n+      cld->record_modified_oops();\n@@ -232,6 +228,0 @@\n-  MemRegion cmr((HeapWord*)_virtual_space.low(),\n-                (HeapWord*)_virtual_space.high());\n-  SerialHeap* gch = SerialHeap::heap();\n-\n-  gch->rem_set()->resize_covered_region(cmr);\n-\n@@ -242,0 +232,2 @@\n+  init_spaces();\n+\n@@ -263,1 +255,0 @@\n-  compute_space_boundaries(0, SpaceDecorator::Clear, SpaceDecorator::Mangle);\n@@ -267,1 +258,0 @@\n-  _pretenure_size_threshold_words = PretenureSizeThreshold >> LogHeapWordSize;\n@@ -276,9 +266,4 @@\n-void DefNewGeneration::compute_space_boundaries(uintx minimum_eden_size,\n-                                                bool clear_space,\n-                                                bool mangle_space) {\n-  \/\/ If the spaces are being cleared (only done at heap initialization\n-  \/\/ currently), the survivor spaces need not be empty.\n-  \/\/ Otherwise, no care is taken for used areas in the survivor spaces\n-  \/\/ so check.\n-  assert(clear_space || (to()->is_empty() && from()->is_empty()),\n-    \"Initialization of the survivor spaces assumes these are empty\");\n+void DefNewGeneration::init_spaces() {\n+  \/\/ Using layout: from, to, eden, so only from can be non-empty.\n+  assert(eden()->is_empty(), \"precondition\");\n+  assert(to()->is_empty(), \"precondition\");\n@@ -286,11 +271,2 @@\n-  \/\/ Compute sizes\n-  uintx size = _virtual_space.committed_size();\n-  uintx survivor_size = compute_survivor_size(size, SpaceAlignment);\n-  uintx eden_size = size - (2*survivor_size);\n-  if (eden_size > max_eden_size()) {\n-    \/\/ Need to reduce eden_size to satisfy the max constraint. The delta needs\n-    \/\/ to be 2*SpaceAlignment aligned so that both survivors are properly\n-    \/\/ aligned.\n-    uintx eden_delta = align_up(eden_size - max_eden_size(), 2*SpaceAlignment);\n-    eden_size     -= eden_delta;\n-    survivor_size += eden_delta\/2;\n+  if (!from()->is_empty()) {\n+    assert((char*) from()->bottom() == _virtual_space.low(), \"inv\");\n@@ -298,12 +274,7 @@\n-  assert(eden_size > 0 && survivor_size <= eden_size, \"just checking\");\n-  if (eden_size < minimum_eden_size) {\n-    \/\/ May happen due to 64Kb rounding, if so adjust eden size back up\n-    minimum_eden_size = align_up(minimum_eden_size, SpaceAlignment);\n-    uintx maximum_survivor_size = (size - minimum_eden_size) \/ 2;\n-    uintx unaligned_survivor_size =\n-      align_down(maximum_survivor_size, SpaceAlignment);\n-    survivor_size = MAX2(unaligned_survivor_size, SpaceAlignment);\n-    eden_size = size - (2*survivor_size);\n-    assert(eden_size > 0 && survivor_size <= eden_size, \"just checking\");\n-    assert(eden_size >= minimum_eden_size, \"just checking\");\n-  }\n+  \/\/ Compute sizes\n+  size_t size = _virtual_space.committed_size();\n+  size_t survivor_size = compute_survivor_size(size, SpaceAlignment);\n+  assert(survivor_size >= from()->used(), \"inv\");\n+  assert(size > 2 * survivor_size, \"inv\");\n+  size_t eden_size = size - (2 * survivor_size);\n+  assert(eden_size > 0 && survivor_size <= eden_size, \"just checking\");\n@@ -312,4 +283,5 @@\n-  char *eden_start = _virtual_space.low();\n-  char *from_start = eden_start + eden_size;\n-  char *to_start   = from_start + survivor_size;\n-  char *to_end     = to_start   + survivor_size;\n+  \/\/ layout: from, to, eden\n+  char* from_start = _virtual_space.low();\n+  char* to_start = from_start + survivor_size;\n+  char* eden_start = to_start + survivor_size;\n+  char* eden_end = eden_start + eden_size;\n@@ -317,2 +289,1 @@\n-  assert(to_end == _virtual_space.high(), \"just checking\");\n-  assert(is_aligned(eden_start, SpaceAlignment), \"checking alignment\");\n+  assert(eden_end == _virtual_space.high(), \"just checking\");\n@@ -321,0 +292,2 @@\n+  assert(is_aligned(eden_start, SpaceAlignment), \"checking alignment\");\n+  assert(is_aligned(eden_end, SpaceAlignment), \"checking alignment\");\n@@ -322,7 +295,2 @@\n-  MemRegion edenMR((HeapWord*)eden_start, (HeapWord*)from_start);\n-  MemRegion toMR  ((HeapWord*)to_start, (HeapWord*)to_end);\n-\n-  \/\/ A minimum eden size implies that there is a part of eden that\n-  \/\/ is being used and that affects the initialization of any\n-  \/\/ newly formed eden.\n-  bool live_in_eden = minimum_eden_size > 0;\n+  MemRegion toMR  ((HeapWord*)to_start, (HeapWord*)eden_start);\n+  MemRegion edenMR((HeapWord*)eden_start, (HeapWord*)eden_end);\n@@ -332,12 +300,11 @@\n-  eden()->initialize(edenMR,\n-                     clear_space && !live_in_eden,\n-                     SpaceDecorator::Mangle);\n-  \/\/ If clear_space and live_in_eden, we will not have cleared any\n-  \/\/ portion of eden above its top. This can cause newly\n-  \/\/ expanded space not to be mangled if using ZapUnusedHeapArea.\n-  \/\/ We explicitly do such mangling here.\n-  if (ZapUnusedHeapArea && clear_space && live_in_eden && mangle_space) {\n-    eden()->mangle_unused_area();\n-  }\n-  from()->initialize(fromMR, clear_space, mangle_space);\n-  to()->initialize(toMR, clear_space, mangle_space);\n+  from()->initialize(fromMR, from()->is_empty());\n+  to()->initialize(toMR, true);\n+  eden()->initialize(edenMR, true);\n+\n+  post_resize();\n+}\n+\n+void DefNewGeneration::post_resize() {\n+  MemRegion cmr((HeapWord*)_virtual_space.low(),\n+                (HeapWord*)_virtual_space.high());\n+  SerialHeap::heap()->rem_set()->resize_covered_region(cmr);\n@@ -359,1 +326,3 @@\n-  HeapWord* prev_high = (HeapWord*) _virtual_space.high();\n+  assert(bytes != 0, \"precondition\");\n+  assert(is_aligned(bytes, SpaceAlignment), \"precondition\");\n+\n@@ -361,7 +330,2 @@\n-  if (success && ZapUnusedHeapArea) {\n-    \/\/ Mangle newly committed space immediately because it\n-    \/\/ can be done here more simply that after the new\n-    \/\/ spaces have been computed.\n-    HeapWord* new_high = (HeapWord*) _virtual_space.high();\n-    MemRegion mangle_region(prev_high, new_high);\n-    SpaceMangler::mangle_region(mangle_region);\n+  if (!success) {\n+    log_info(gc)(\"Failed to expand young-gen by %zu bytes\", bytes);\n@@ -373,0 +337,11 @@\n+void DefNewGeneration::expand_eden_by(size_t delta_bytes) {\n+  if (!expand(delta_bytes)) {\n+    return;\n+  }\n+\n+  MemRegion eden_mr{eden()->bottom(), (HeapWord*)_virtual_space.high()};\n+  eden()->initialize(eden_mr, eden()->is_empty());\n+\n+  post_resize();\n+}\n+\n@@ -405,12 +380,2 @@\n-void DefNewGeneration::compute_new_size() {\n-  \/\/ This is called after a GC that includes the old generation, so from-space\n-  \/\/ will normally be empty.\n-  \/\/ Note that we check both spaces, since if scavenge failed they revert roles.\n-  \/\/ If not we bail out (otherwise we would have to relocate the objects).\n-  if (!from()->is_empty() || !to()->is_empty()) {\n-    return;\n-  }\n-\n-  SerialHeap* gch = SerialHeap::heap();\n-\n-  size_t old_size = gch->old_gen()->capacity();\n+size_t DefNewGeneration::calculate_desired_young_gen_bytes() const {\n+  size_t old_size = SerialHeap::heap()->old_gen()->capacity();\n@@ -437,8 +402,32 @@\n-  assert(desired_new_size <= max_new_size, \"just checking\");\n-\n-  bool changed = false;\n-  if (desired_new_size > new_size_before) {\n-    size_t change = desired_new_size - new_size_before;\n-    assert(change % alignment == 0, \"just checking\");\n-    if (expand(change)) {\n-       changed = true;\n+  if (!from()->is_empty()) {\n+    \/\/ Mininum constraint to hold all live objs inside from-space.\n+    size_t min_survivor_size = align_up(from()->used(), alignment);\n+\n+    \/\/ SurvivorRatio := eden_size \/ survivor_size\n+    \/\/ young-gen-size = eden_size                     + 2 * survivor_size\n+    \/\/                = SurvivorRatio * survivor_size + 2 * survivor_size\n+    \/\/                = (SurvivorRatio + 2) * survivor_size\n+    size_t min_young_gen_size = min_survivor_size * (SurvivorRatio + 2);\n+\n+    desired_new_size = MAX2(min_young_gen_size, desired_new_size);\n+  }\n+  assert(is_aligned(desired_new_size, alignment), \"postcondition\");\n+\n+  return desired_new_size;\n+}\n+\n+void DefNewGeneration::resize_inner() {\n+  assert(eden()->is_empty(), \"precondition\");\n+  assert(to()->is_empty(), \"precondition\");\n+\n+  size_t current_young_gen_size_bytes = _virtual_space.committed_size();\n+  size_t desired_young_gen_size_bytes = calculate_desired_young_gen_bytes();\n+  if (current_young_gen_size_bytes == desired_young_gen_size_bytes) {\n+    return;\n+  }\n+\n+  \/\/ Commit\/uncommit\n+  if (desired_young_gen_size_bytes > current_young_gen_size_bytes) {\n+    size_t delta_bytes = desired_young_gen_size_bytes - current_young_gen_size_bytes;\n+    if (!expand(delta_bytes)) {\n+      return;\n@@ -446,31 +435,43 @@\n-    \/\/ If the heap failed to expand to the desired size,\n-    \/\/ \"changed\" will be false.  If the expansion failed\n-    \/\/ (and at this point it was expected to succeed),\n-    \/\/ ignore the failure (leaving \"changed\" as false).\n-  }\n-  if (desired_new_size < new_size_before && eden()->is_empty()) {\n-    \/\/ bail out of shrinking if objects in eden\n-    size_t change = new_size_before - desired_new_size;\n-    assert(change % alignment == 0, \"just checking\");\n-    _virtual_space.shrink_by(change);\n-    changed = true;\n-  }\n-  if (changed) {\n-    \/\/ The spaces have already been mangled at this point but\n-    \/\/ may not have been cleared (set top = bottom) and should be.\n-    \/\/ Mangling was done when the heap was being expanded.\n-    compute_space_boundaries(eden()->used(),\n-                             SpaceDecorator::Clear,\n-                             SpaceDecorator::DontMangle);\n-    MemRegion cmr((HeapWord*)_virtual_space.low(),\n-                  (HeapWord*)_virtual_space.high());\n-    gch->rem_set()->resize_covered_region(cmr);\n-\n-    log_debug(gc, ergo, heap)(\n-        \"New generation size %zuK->%zuK [eden=%zuK,survivor=%zuK]\",\n-        new_size_before\/K, _virtual_space.committed_size()\/K,\n-        eden()->capacity()\/K, from()->capacity()\/K);\n-    log_trace(gc, ergo, heap)(\n-        \"  [allowed %zuK extra for %d threads]\",\n-          thread_increase_size\/K, threads_count);\n-      }\n+  } else {\n+    size_t delta_bytes = current_young_gen_size_bytes - desired_young_gen_size_bytes;\n+    _virtual_space.shrink_by(delta_bytes);\n+  }\n+\n+  assert(desired_young_gen_size_bytes == _virtual_space.committed_size(), \"inv\");\n+\n+  init_spaces();\n+\n+  log_debug(gc, ergo, heap)(\"New generation size %zuK->%zuK [eden=%zuK,survivor=%zuK]\",\n+    current_young_gen_size_bytes\/K, _virtual_space.committed_size()\/K,\n+    eden()->capacity()\/K, from()->capacity()\/K);\n+}\n+\n+void DefNewGeneration::resize_after_young_gc() {\n+  \/\/ Called only after successful young-gc.\n+  assert(eden()->is_empty(), \"precondition\");\n+  assert(to()->is_empty(), \"precondition\");\n+\n+  if ((char*)to()->bottom() == _virtual_space.low()) {\n+    \/\/ layout: to, from, eden; can't resize.\n+    return;\n+  }\n+\n+  assert((char*)from()->bottom() == _virtual_space.low(), \"inv\");\n+  resize_inner();\n+}\n+\n+void DefNewGeneration::resize_after_full_gc() {\n+  if (eden()->is_empty() && from()->is_empty() && to()->is_empty()) {\n+    resize_inner();\n+    return;\n+  }\n+\n+  \/\/ Usually the young-gen is empty after full-gc.\n+  \/\/ This is the extreme case; expand young-gen to its max size.\n+  if (_virtual_space.uncommitted_size() == 0) {\n+    \/\/ Already at its max size.\n+    return;\n+  }\n+\n+  \/\/ Keep from\/to and expand eden.\n+  expand_eden_by(_virtual_space.uncommitted_size());\n@@ -491,1 +492,0 @@\n-\n@@ -497,1 +497,0 @@\n-\n@@ -505,1 +504,2 @@\n-  return reserved_bytes - compute_survivor_size(reserved_bytes, SpaceAlignment);\n+  const size_t min_survivor_bytes = SpaceAlignment;\n+  return reserved_bytes - min_survivor_bytes;\n@@ -597,1 +597,0 @@\n-  to()->clear(SpaceDecorator::Mangle);\n@@ -607,3 +606,5 @@\n-    StrongRootsScope srs(0);\n-    RootScanClosure root_cl{this};\n-    CLDScanClosure cld_cl{this};\n+    RootScanClosure oop_closure{this};\n+    CLDScanClosure cld_closure{this};\n+\n+    NMethodToOopClosure nmethod_closure(&oop_closure,\n+                                        NMethodToOopClosure::FixRelocations);\n@@ -611,3 +612,5 @@\n-    MarkingNMethodClosure code_cl(&root_cl,\n-                                  NMethodToOopClosure::FixRelocations,\n-                                  false \/* keepalive_nmethods *\/);\n+    \/\/ Starting tracing from roots, there are 4 kinds of roots in young-gc.\n+    \/\/\n+    \/\/ 1. old-to-young pointers; processing them before relocating other kinds\n+    \/\/ of roots.\n+    _old_gen->scan_old_to_young_refs();\n@@ -615,6 +618,3 @@\n-    HeapWord* saved_top_in_old_gen = _old_gen->space()->top();\n-    heap->process_roots(SerialHeap::SO_ScavengeCodeCache,\n-                        &root_cl,\n-                        &cld_cl,\n-                        &cld_cl,\n-                        &code_cl);\n+    \/\/ 2. CLD; visit all (strong+weak) clds with the same closure, because we\n+    \/\/ don't perform class unloading during young-gc.\n+    ClassLoaderDataGraph::cld_do(&cld_closure);\n@@ -622,1 +622,8 @@\n-    _old_gen->scan_old_to_young_refs(saved_top_in_old_gen);\n+    \/\/ 3. Threads stack frames and nmethods.\n+    \/\/ Only nmethods that contain pointers into-young need to be processed\n+    \/\/ during young-gc, and they are tracked in ScavengableNMethods\n+    Threads::oops_do(&oop_closure, nullptr);\n+    ScavengableNMethods::nmethods_do(&nmethod_closure);\n+\n+    \/\/ 4. VM internal roots.\n+    OopStorageSet::strong_oops_do(&oop_closure);\n@@ -804,1 +811,1 @@\n-void DefNewGeneration::gc_epilogue(bool full) {\n+void DefNewGeneration::gc_epilogue() {\n@@ -837,7 +844,11 @@\n-HeapWord* DefNewGeneration::allocate(size_t word_size) {\n-  \/\/ This is the slow-path allocation for the DefNewGeneration.\n-  \/\/ Most allocations are fast-path in compiled code.\n-  \/\/ We try to allocate from the eden.  If that works, we are happy.\n-  \/\/ Note that since DefNewGeneration supports lock-free allocation, we\n-  \/\/ have to use it here, as well.\n-  HeapWord* result = eden()->par_allocate(word_size);\n+HeapWord* DefNewGeneration::expand_and_allocate(size_t word_size) {\n+  assert(Heap_lock->is_locked(), \"precondition\");\n+\n+  size_t eden_free_bytes = eden()->free();\n+  size_t requested_bytes = word_size * HeapWordSize;\n+  if (eden_free_bytes < requested_bytes) {\n+    size_t expand_bytes = requested_bytes - eden_free_bytes;\n+    expand_eden_by(align_up(expand_bytes, SpaceAlignment));\n+  }\n+\n+  HeapWord* result = eden()->allocate(word_size);\n","filename":"src\/hotspot\/share\/gc\/serial\/defNewGeneration.cpp","additions":186,"deletions":175,"binary":false,"changes":361,"status":"modified"},{"patch":"@@ -51,1 +51,1 @@\n-#include \"gc\/shared\/modRefBarrierSet.hpp\"\n+#include \"gc\/shared\/oopStorageSet.inline.hpp\"\n@@ -56,1 +56,0 @@\n-#include \"gc\/shared\/strongRootsScope.hpp\"\n@@ -69,0 +68,1 @@\n+#include \"runtime\/threads.hpp\"\n@@ -498,1 +498,1 @@\n-    StrongRootsScope srs(0);\n+    GCTraceTime(Debug, gc, phases) tm_m(\"Marking From Roots\", gc_timer());\n@@ -500,7 +500,17 @@\n-    CLDClosure* weak_cld_closure = ClassUnloading ? nullptr : &follow_cld_closure;\n-    MarkingNMethodClosure mark_code_closure(&follow_root_closure, !NMethodToOopClosure::FixRelocations, true);\n-    gch->process_roots(SerialHeap::SO_None,\n-                       &follow_root_closure,\n-                       &follow_cld_closure,\n-                       weak_cld_closure,\n-                       &mark_code_closure);\n+    \/\/ Start tracing from roots, there are 3 kinds of roots in full-gc.\n+    \/\/\n+    \/\/ 1. CLD. This method internally takes care of whether class loading is\n+    \/\/ enabled or not, applying the closure to both strong and weak or only\n+    \/\/ strong CLDs.\n+    ClassLoaderDataGraph::always_strong_cld_do(&follow_cld_closure);\n+\n+    {\n+      \/\/ 2. Threads stack frames and active nmethods in them.\n+      NMethodMarkingScope nmethod_marking_scope;\n+      MarkingNMethodClosure mark_code_closure(&follow_root_closure);\n+\n+      Threads::oops_do(&follow_root_closure, &mark_code_closure);\n+    }\n+\n+    \/\/ 3. VM internal roots.\n+    OopStorageSet::strong_oops_do(&follow_root_closure);\n@@ -711,0 +721,10 @@\n+  \/\/ Usually, all class unloading work occurs at the end of phase 1, but Serial\n+  \/\/ full-gc accesses dead-objs' klass to find out the start of next live-obj\n+  \/\/ during phase 2. This requires klasses of dead-objs to be kept loaded.\n+  \/\/ Therefore, we declare ClassUnloadingContext at the same level as\n+  \/\/ full-gc phases, and purge dead classes (invoking\n+  \/\/ ClassLoaderDataGraph::purge) after all phases of full-gc.\n+  ClassUnloadingContext ctx(1 \/* num_nmethod_unlink_workers *\/,\n+                            false \/* unregister_nmethods_during_purge *\/,\n+                            false \/* lock_nmethod_free_separately *\/);\n+\n@@ -736,6 +756,12 @@\n-    NMethodToOopClosure code_closure(&adjust_pointer_closure, NMethodToOopClosure::FixRelocations);\n-    gch->process_roots(SerialHeap::SO_AllCodeCache,\n-                       &adjust_pointer_closure,\n-                       &adjust_cld_closure,\n-                       &adjust_cld_closure,\n-                       &code_closure);\n+    \/\/ Remap strong and weak roots in adjust phase.\n+    \/\/ 1. All (strong and weak) CLDs.\n+    ClassLoaderDataGraph::cld_do(&adjust_cld_closure);\n+\n+    \/\/ 2. Threads stack frames. No need to visit on-stack nmethods, because all\n+    \/\/ nmethods are visited in one go via CodeCache::nmethods_do.\n+    Threads::oops_do(&adjust_pointer_closure, nullptr);\n+    NMethodToOopClosure nmethod_cl(&adjust_pointer_closure, NMethodToOopClosure::FixRelocations);\n+    CodeCache::nmethods_do(&nmethod_cl);\n+\n+    \/\/ 3. VM internal roots\n+    OopStorageSet::strong_oops_do(&adjust_pointer_closure);\n@@ -743,0 +769,1 @@\n+    \/\/ 4. VM internal weak roots\n@@ -756,0 +783,7 @@\n+  \/\/ Delete metaspaces for unloaded class loaders and clean up CLDG.\n+  ClassLoaderDataGraph::purge(true \/* at_safepoint *\/);\n+  DEBUG_ONLY(MetaspaceUtils::verify();)\n+\n+  \/\/ Need to clear claim bits for the next full-gc (specifically phase 1 and 3).\n+  ClassLoaderDataGraph::clear_claimed_marks();\n+\n","filename":"src\/hotspot\/share\/gc\/serial\/serialFullGC.cpp","additions":50,"deletions":16,"binary":false,"changes":66,"status":"modified"},{"patch":"@@ -37,0 +37,1 @@\n+#include \"gc\/shared\/barrierSetNMethod.hpp\"\n@@ -58,1 +59,0 @@\n-#include \"gc\/shared\/strongRootsScope.hpp\"\n@@ -69,1 +69,1 @@\n-#include \"runtime\/handles.hpp\"\n+#include \"runtime\/init.hpp\"\n@@ -148,12 +148,0 @@\n-void SerialHeap::safepoint_synchronize_begin() {\n-  if (UseStringDeduplication) {\n-    SuspendibleThreadSet::synchronize();\n-  }\n-}\n-\n-void SerialHeap::safepoint_synchronize_end() {\n-  if (UseStringDeduplication) {\n-    SuspendibleThreadSet::desynchronize();\n-  }\n-}\n-\n@@ -198,1 +186,0 @@\n-  bs->initialize();\n@@ -284,11 +271,5 @@\n-\/\/ Return true if any of the following is true:\n-\/\/ . the allocation won't fit into the current young gen heap\n-\/\/ . heap memory is tight\n-bool SerialHeap::should_try_older_generation_allocation(size_t word_size) const {\n-  size_t young_capacity = _young_gen->capacity_before_gc();\n-  return    (word_size > heap_word_size(young_capacity))\n-         || _is_heap_almost_full;\n-}\n-\n-  HeapWord* result = nullptr;\n-  if (_old_gen->should_allocate(size, is_tlab)) {\n+  assert(Heap_lock->is_locked(), \"precondition\");\n+\n+  HeapWord* result = _young_gen->expand_and_allocate(size);\n+\n+  if (result == nullptr && !is_tlab) {\n@@ -298,6 +279,1 @@\n-  if (result == nullptr) {\n-    if (_young_gen->should_allocate(size, is_tlab)) {\n-      \/\/ Young-gen is not expanded.\n-      result = _young_gen->allocate(size);\n-    }\n-  }\n+\n@@ -308,9 +284,10 @@\n-HeapWord* SerialHeap::mem_allocate_work(size_t size, bool is_tlab) {\n-  HeapWord* result = nullptr;\n-\n-  \/\/ Loop until the allocation is satisfied, or unsatisfied after GC.\n-  for (uint try_count = 1; \/* return or throw *\/; try_count += 1) {\n-    \/\/ First allocation attempt is lock-free.\n-    DefNewGeneration *young = _young_gen;\n-    if (young->should_allocate(size, is_tlab)) {\n-      result = young->par_allocate(size);\n+HeapWord* SerialHeap::mem_allocate_cas_noexpand(size_t size, bool is_tlab) {\n+  HeapWord* result = _young_gen->par_allocate(size);\n+  if (result != nullptr) {\n+    return result;\n+  }\n+  \/\/ Try old-gen allocation for non-TLAB.\n+  if (!is_tlab) {\n+    \/\/ If it's too large for young-gen or heap is too full.\n+    if (size > heap_word_size(_young_gen->capacity_before_gc()) || _is_heap_almost_full) {\n+      result = _old_gen->par_allocate(size);\n@@ -318,1 +295,0 @@\n-        assert(is_in_reserved(result), \"result not in heap\");\n@@ -322,0 +298,13 @@\n+  }\n+\n+  return nullptr;\n+}\n+\n+HeapWord* SerialHeap::mem_allocate_work(size_t size, bool is_tlab) {\n+  HeapWord* result = nullptr;\n+\n+  for (uint try_count = 1; \/* break *\/; try_count++) {\n+    result = mem_allocate_cas_noexpand(size, is_tlab);\n+    if (result != nullptr) {\n+      break;\n+    }\n@@ -325,5 +314,3 @@\n-      log_trace(gc, alloc)(\"SerialHeap::mem_allocate_work: attempting locked slow path allocation\");\n-      \/\/ Note that only large objects get a shot at being\n-      \/\/ allocated in later generations.\n-      bool first_only = !should_try_older_generation_allocation(size);\n-      result = attempt_allocation(size, is_tlab, first_only);\n+      \/\/ Re-try after acquiring the lock, because a GC might have occurred\n+      \/\/ while waiting for this lock.\n+      result = mem_allocate_cas_noexpand(size, is_tlab);\n@@ -332,2 +319,9 @@\n-        assert(is_in_reserved(result), \"result not in heap\");\n-        return result;\n+        break;\n+      }\n+\n+      if (!is_init_completed()) {\n+        \/\/ Can't do GC; try heap expansion to satisfy the request.\n+        result = expand_heap_and_allocate(size, is_tlab);\n+        if (result != nullptr) {\n+          return result;\n+        }\n@@ -336,1 +330,0 @@\n-      \/\/ Read the gc count while the heap lock is held.\n@@ -344,4 +337,1 @@\n-\n-      assert(result == nullptr || is_in_reserved(result),\n-             \"result not in heap\");\n-      return result;\n+      break;\n@@ -357,18 +347,2 @@\n-}\n-\n-HeapWord* SerialHeap::attempt_allocation(size_t size,\n-                                         bool is_tlab,\n-                                         bool first_only) {\n-  HeapWord* res = nullptr;\n-\n-  if (_young_gen->should_allocate(size, is_tlab)) {\n-    res = _young_gen->allocate(size);\n-    if (res != nullptr || first_only) {\n-      return res;\n-    }\n-  }\n-\n-  if (_old_gen->should_allocate(size, is_tlab)) {\n-    res = _old_gen->allocate(size);\n-  }\n-  return res;\n+  assert(result == nullptr || is_in_reserved(result), \"postcondition\");\n+  return result;\n@@ -378,2 +352,1 @@\n-HeapWord* SerialHeap::mem_allocate(size_t size,\n-                                   bool* gc_overhead_limit_was_exceeded) {\n+HeapWord* SerialHeap::mem_allocate(size_t size) {\n@@ -384,5 +357,0 @@\n-bool SerialHeap::must_clear_all_soft_refs() {\n-  return _gc_cause == GCCause::_metadata_GC_clear_soft_refs ||\n-         _gc_cause == GCCause::_wb_full_gc;\n-}\n-\n@@ -428,0 +396,1 @@\n+    _young_gen->resize_after_young_gc();\n@@ -434,2 +403,0 @@\n-  _young_gen->compute_new_size();\n-\n@@ -450,0 +417,2 @@\n+  BarrierSetNMethod* bs_nm = BarrierSet::barrier_set()->barrier_set_nmethod();\n+  bs_nm->disarm(nm);\n@@ -474,1 +443,1 @@\n-  bool should_run_young_gc = _young_gen->should_allocate(size, is_tlab);\n+  bool should_run_young_gc = is_tlab || size <= _young_gen->eden()->capacity();\n@@ -477,6 +446,1 @@\n-  result = attempt_allocation(size, is_tlab, false \/*first_only*\/);\n-  if (result != nullptr) {\n-    return result;\n-  }\n-\n-  \/\/ OK, collection failed, try expansion.\n+  \/\/ Just finished a GC, try to satisfy this allocation, using expansion if needed.\n@@ -499,4 +463,0 @@\n-  result = attempt_allocation(size, is_tlab, false \/* first_only *\/);\n-  if (result != nullptr) {\n-    return result;\n-  }\n@@ -516,32 +476,0 @@\n-void SerialHeap::process_roots(ScanningOption so,\n-                               OopClosure* strong_roots,\n-                               CLDClosure* strong_cld_closure,\n-                               CLDClosure* weak_cld_closure,\n-                               NMethodToOopClosure* code_roots) {\n-  \/\/ General roots.\n-  assert(code_roots != nullptr, \"code root closure should always be set\");\n-\n-  ClassLoaderDataGraph::roots_cld_do(strong_cld_closure, weak_cld_closure);\n-\n-  \/\/ Only process code roots from thread stacks if we aren't visiting the entire CodeCache anyway\n-  NMethodToOopClosure* roots_from_code_p = (so & SO_AllCodeCache) ? nullptr : code_roots;\n-\n-  Threads::oops_do(strong_roots, roots_from_code_p);\n-\n-  OopStorageSet::strong_oops_do(strong_roots);\n-\n-  if (so & SO_ScavengeCodeCache) {\n-    assert(code_roots != nullptr, \"must supply closure for code cache\");\n-\n-    \/\/ We only visit parts of the CodeCache when scavenging.\n-    ScavengableNMethods::nmethods_do(code_roots);\n-  }\n-  if (so & SO_AllCodeCache) {\n-    assert(code_roots != nullptr, \"must supply closure for code cache\");\n-\n-    \/\/ CMSCollector uses this to do intermediate-strength collections.\n-    \/\/ We scan the entire code cache, since CodeCache::do_unloading is not called.\n-    CodeCache::nmethods_do(code_roots);\n-  }\n-}\n-\n@@ -580,1 +508,1 @@\n-  bool clear_soft_refs = must_clear_all_soft_refs();\n+  bool clear_soft_refs = GCCause::should_clear_all_soft_refs(_gc_cause);\n@@ -638,3 +566,0 @@\n-  ClassUnloadingContext ctx(1 \/* num_nmethod_unlink_workers *\/,\n-                            false \/* unregister_nmethods_during_purge *\/,\n-                            false \/* lock_nmethod_free_separately *\/);\n@@ -663,8 +588,1 @@\n-  _young_gen->compute_new_size();\n-\n-  \/\/ Delete metaspaces for unloaded class loaders and clean up loader_data graph\n-  ClassLoaderDataGraph::purge(\/*at_safepoint*\/true);\n-  DEBUG_ONLY(MetaspaceUtils::verify();)\n-\n-  \/\/ Need to clear claim bits for the next mark.\n-  ClassLoaderDataGraph::clear_claimed_marks();\n+  _young_gen->resize_after_full_gc();\n@@ -740,1 +658,1 @@\n-size_t SerialHeap::tlab_capacity(Thread* thr) const {\n+size_t SerialHeap::tlab_capacity() const {\n@@ -745,1 +663,1 @@\n-size_t SerialHeap::tlab_used(Thread* thr) const {\n+size_t SerialHeap::tlab_used() const {\n@@ -749,1 +667,1 @@\n-size_t SerialHeap::unsafe_max_tlab_alloc(Thread* thr) const {\n+size_t SerialHeap::unsafe_max_tlab_alloc() const {\n@@ -855,1 +773,1 @@\n-  _young_gen->gc_epilogue(full);\n+  _young_gen->gc_epilogue();\n","filename":"src\/hotspot\/share\/gc\/serial\/serialHeap.cpp","additions":58,"deletions":140,"binary":false,"changes":198,"status":"modified"},{"patch":"@@ -280,2 +280,2 @@\n-void TenuredGeneration::scan_old_to_young_refs(HeapWord* saved_top_in_old_gen) {\n-  _rs->scan_old_to_young_refs(this, saved_top_in_old_gen);\n+void TenuredGeneration::scan_old_to_young_refs() {\n+  _rs->scan_old_to_young_refs(this, space()->top());\n@@ -317,1 +317,1 @@\n-  _the_space->initialize(MemRegion(bottom, end), SpaceDecorator::Clear, SpaceDecorator::Mangle);\n+  _the_space->initialize(MemRegion(bottom, end), SpaceDecorator::Clear);\n","filename":"src\/hotspot\/share\/gc\/serial\/tenuredGeneration.cpp","additions":3,"deletions":3,"binary":false,"changes":6,"status":"modified"},{"patch":"@@ -118,1 +118,1 @@\n-  _preserve.Insert(OptoReg::as_OptoReg(vm_reg));\n+  _preserve.insert(OptoReg::as_OptoReg(vm_reg));\n@@ -127,1 +127,1 @@\n-    _preserve.Remove(OptoReg::as_OptoReg(vm_reg));\n+    _preserve.remove(OptoReg::as_OptoReg(vm_reg));\n@@ -1178,1 +1178,1 @@\n-      new_live.OR(live[succ_id]);\n+      new_live.or_with(live[succ_id]);\n@@ -1189,1 +1189,1 @@\n-          regs->OR(new_live);\n+          regs->or_with(new_live);\n@@ -1197,1 +1197,1 @@\n-        new_live.Remove(first);\n+        new_live.remove(first);\n@@ -1200,1 +1200,1 @@\n-        new_live.Remove(second);\n+        new_live.remove(second);\n@@ -1209,1 +1209,1 @@\n-          new_live.Insert(first);\n+          new_live.insert(first);\n@@ -1212,1 +1212,1 @@\n-          new_live.Insert(second);\n+          new_live.insert(second);\n@@ -1220,1 +1220,1 @@\n-          regs->OR(new_live);\n+          regs->or_with(new_live);\n@@ -1226,2 +1226,2 @@\n-    new_live.SUBTRACT(old_live);\n-    if (new_live.is_NotEmpty()) {\n+    new_live.subtract(old_live);\n+    if (!new_live.is_empty()) {\n@@ -1229,1 +1229,1 @@\n-      old_live.OR(new_live);\n+      old_live.or_with(new_live);\n","filename":"src\/hotspot\/share\/gc\/shared\/c2\/barrierSetC2.cpp","additions":12,"deletions":12,"binary":false,"changes":24,"status":"modified"},{"patch":"@@ -30,1 +30,0 @@\n-#include \"gc\/shared\/softRefPolicy.hpp\"\n@@ -39,0 +38,1 @@\n+#include \"services\/cpuTimeUsage.hpp\"\n@@ -92,0 +92,1 @@\n+  friend class CPUTimeUsage::GC;\n@@ -98,0 +99,2 @@\n+  static bool _is_shutting_down;\n+\n@@ -105,2 +108,0 @@\n-  SoftRefPolicy _soft_ref_policy;\n-\n@@ -163,2 +164,1 @@\n-  virtual HeapWord* mem_allocate(size_t size,\n-                                 bool* gc_overhead_limit_was_exceeded) = 0;\n+  virtual HeapWord* mem_allocate(size_t size) = 0;\n@@ -173,1 +173,0 @@\n-  DEBUG_ONLY(static void fill_args_check(HeapWord* start, size_t words);)\n@@ -215,0 +214,1 @@\n+ public:\n@@ -218,2 +218,0 @@\n- public:\n-\n@@ -251,1 +249,3 @@\n-  void before_exit();\n+  static bool is_shutting_down();\n+\n+  void initiate_shutdown();\n@@ -308,3 +308,0 @@\n-  static void fill_with_object(MemRegion region, bool zap = true) {\n-    fill_with_object(region.start(), region.word_size(), zap);\n-  }\n@@ -343,1 +340,1 @@\n-  virtual size_t tlab_capacity(Thread *thr) const = 0;\n+  virtual size_t tlab_capacity() const = 0;\n@@ -345,2 +342,2 @@\n-  \/\/ The amount of used space for thread-local allocation buffers for the given thread.\n-  virtual size_t tlab_used(Thread *thr) const = 0;\n+  \/\/ The amount of space used for thread-local allocation buffers.\n+  virtual size_t tlab_used() const = 0;\n@@ -353,1 +350,1 @@\n-  virtual size_t unsafe_max_tlab_alloc(Thread *thr) const = 0;\n+  virtual size_t unsafe_max_tlab_alloc() const = 0;\n@@ -355,3 +352,1 @@\n-  \/\/ Perform a collection of the heap; intended for use in implementing\n-  \/\/ \"System.gc\".  This probably implies as full a collection as the\n-  \/\/ \"CollectedHeap\" supports.\n+  \/\/ Perform a collection of the heap of a type depending on the given cause.\n@@ -397,3 +392,0 @@\n-  \/\/ Return the SoftRefPolicy for the heap;\n-  SoftRefPolicy* soft_ref_policy() { return &_soft_ref_policy; }\n-\n@@ -432,2 +424,0 @@\n-  void log_gc_cpu_time() const;\n-\n@@ -466,2 +456,0 @@\n-  double elapsed_gc_cpu_time() const;\n-\n@@ -509,0 +497,1 @@\n+  virtual size_t bootstrap_max_memory() const;\n","filename":"src\/hotspot\/share\/gc\/shared\/collectedHeap.hpp","additions":15,"deletions":26,"binary":false,"changes":41,"status":"modified"},{"patch":"@@ -28,0 +28,1 @@\n+\n@@ -86,1 +87,1 @@\n-    HeapWord* prev = Atomic::cmpxchg(&_biased_bases[from_block_idx], UNUSED_BASE, to);\n+    HeapWord* prev = AtomicAccess::cmpxchg(&_biased_bases[from_block_idx], UNUSED_BASE, to);\n@@ -140,1 +141,1 @@\n-  NOT_PRODUCT(Atomic::inc(&_num_forwardings);)\n+  NOT_PRODUCT(AtomicAccess::inc(&_num_forwardings);)\n@@ -299,1 +300,1 @@\n-    FallbackTable* prev = Atomic::cmpxchg(&_fallback_table, static_cast<FallbackTable*>(nullptr), fallback_table);\n+    FallbackTable* prev = AtomicAccess::cmpxchg(&_fallback_table, static_cast<FallbackTable*>(nullptr), fallback_table);\n@@ -324,1 +325,1 @@\n-  NOT_PRODUCT(Atomic::inc(&_num_fallback_forwardings);)\n+  NOT_PRODUCT(AtomicAccess::inc(&_num_fallback_forwardings);)\n","filename":"src\/hotspot\/share\/gc\/shared\/fullGCForwarding.inline.hpp","additions":5,"deletions":4,"binary":false,"changes":9,"status":"modified"},{"patch":"@@ -51,1 +51,0 @@\n-  bool                _overhead_limit_exceeded;\n@@ -74,1 +73,0 @@\n-      _overhead_limit_exceeded(false),\n@@ -122,1 +120,1 @@\n-  const char* message = _overhead_limit_exceeded ? \"GC overhead limit exceeded\" : \"Java heap space\";\n+  const char* message = \"Java heap space\";\n@@ -136,4 +134,1 @@\n-    oop exception = _overhead_limit_exceeded ?\n-        Universe::out_of_memory_error_gc_overhead_limit() :\n-        Universe::out_of_memory_error_java_heap();\n-    THROW_OOP_(exception, true);\n+    THROW_OOP_(Universe::out_of_memory_error_java_heap(), true);\n@@ -241,1 +236,1 @@\n-  HeapWord* mem = Universe::heap()->mem_allocate(_word_size, &allocation._overhead_limit_exceeded);\n+  HeapWord* mem = Universe::heap()->mem_allocate(_word_size);\n","filename":"src\/hotspot\/share\/gc\/shared\/memAllocator.cpp","additions":3,"deletions":8,"binary":false,"changes":11,"status":"modified"},{"patch":"@@ -39,0 +39,1 @@\n+#include \"utilities\/powerOfTwo.hpp\"\n@@ -41,1 +42,1 @@\n-#if !(defined AARCH64 || defined AMD64 || defined IA32 || defined PPC64 || defined RISCV64)\n+#if !(defined AARCH64 || defined AMD64 || defined PPC64 || defined RISCV64)\n@@ -195,0 +196,7 @@\n+\n+  \/\/ Gen shen does not support any ShenandoahGCHeuristics value except for the default \"adaptive\"\n+  if ((strcmp(ShenandoahGCMode, \"generational\") == 0)\n+      && strcmp(ShenandoahGCHeuristics, \"adaptive\") != 0) {\n+    log_warning(gc)(\"Ignoring -XX:ShenandoahGCHeuristics input: %s, because generational shenandoah only\"\n+      \" supports adaptive heuristics\", ShenandoahGCHeuristics);\n+  }\n@@ -198,1 +206,1 @@\n-  size_t align = ShenandoahMaxRegionSize;\n+  size_t align = next_power_of_2(ShenandoahMaxRegionSize);\n","filename":"src\/hotspot\/share\/gc\/shenandoah\/shenandoahArguments.cpp","additions":10,"deletions":2,"binary":false,"changes":12,"status":"modified"},{"patch":"@@ -71,0 +71,1 @@\n+  ShenandoahGC(ShenandoahHeap::heap()->global_generation()),\n@@ -107,2 +108,3 @@\n-  ShenandoahMetricsSnapshot metrics;\n-  metrics.snap_before();\n+  ShenandoahHeap* const heap = ShenandoahHeap::heap();\n+\n+  ShenandoahMetricsSnapshot metrics(heap->free_set());\n@@ -113,2 +115,0 @@\n-  ShenandoahHeap* const heap = ShenandoahHeap::heap();\n-\n@@ -119,3 +119,1 @@\n-  metrics.snap_after();\n-\n-  if (metrics.is_good_progress(heap->global_generation())) {\n+  if (metrics.is_good_progress()) {\n@@ -130,1 +128,1 @@\n-  heap->global_generation()->heuristics()->record_success_full();\n+  _generation->heuristics()->record_success_full();\n@@ -147,1 +145,1 @@\n-    heap->verifier()->verify_before_fullgc();\n+    heap->verifier()->verify_before_fullgc(_generation);\n@@ -200,1 +198,1 @@\n-    ShenandoahReferenceProcessor* rp = heap->global_generation()->ref_processor();\n+    ShenandoahReferenceProcessor* rp = _generation->ref_processor();\n@@ -242,1 +240,0 @@\n-  ShenandoahGenerationalHeap::TransferResult result;\n@@ -258,1 +255,1 @@\n-    result = phase5_epilog();\n+    phase5_epilog();\n@@ -262,7 +259,0 @@\n-  if (heap->mode()->is_generational()) {\n-    LogTarget(Info, gc, ergo) lt;\n-    if (lt.is_enabled()) {\n-      LogStream ls(lt);\n-      result.print_on(\"Full GC\", &ls);\n-    }\n-  }\n@@ -283,1 +273,1 @@\n-    heap->verifier()->verify_after_fullgc();\n+    heap->verifier()->verify_after_fullgc(_generation);\n@@ -302,1 +292,1 @@\n-  heap->global_generation()->reset_mark_bitmap<true, true>();\n+  _generation->reset_mark_bitmap<true, true>();\n@@ -304,1 +294,1 @@\n-  assert(!heap->global_generation()->is_mark_complete(), \"sanity\");\n+  assert(!_generation->is_mark_complete(), \"sanity\");\n@@ -306,1 +296,1 @@\n-  heap->set_unload_classes(heap->global_generation()->heuristics()->can_unload_classes());\n+  heap->set_unload_classes(_generation->heuristics()->can_unload_classes());\n@@ -308,1 +298,1 @@\n-  ShenandoahReferenceProcessor* rp = heap->global_generation()->ref_processor();\n+  ShenandoahReferenceProcessor* rp = _generation->ref_processor();\n@@ -312,1 +302,1 @@\n-  ShenandoahSTWMark mark(heap->global_generation(), true \/*full_gc*\/);\n+  ShenandoahSTWMark mark(_generation, true \/*full_gc*\/);\n@@ -314,1 +304,1 @@\n-  heap->parallel_cleaning(true \/* full_gc *\/);\n+  heap->parallel_cleaning(_generation, true \/* full_gc *\/);\n@@ -360,1 +350,2 @@\n-  void do_object(oop p) {\n+  void do_object(oop p) override {\n+    shenandoah_assert_mark_complete(cast_from_oop<HeapWord*>(p));\n@@ -362,2 +353,3 @@\n-    assert(_heap->gc_generation()->complete_marking_context()->is_marked(p), \"must be marked\");\n-    assert(!_heap->gc_generation()->complete_marking_context()->allocated_after_mark_start(p), \"must be truly marked\");\n+    assert(_heap->global_generation()->is_mark_complete(), \"marking must be finished\");\n+    assert(_heap->marking_context()->is_marked(p), \"must be marked\");\n+    assert(!_heap->marking_context()->allocated_after_mark_start(p), \"must be truly marked\");\n@@ -536,5 +528,1 @@\n-private:\n-  ShenandoahHeap* const _heap;\n-\n-  ShenandoahEnsureHeapActiveClosure() : _heap(ShenandoahHeap::heap()) {}\n-  void heap_region_do(ShenandoahHeapRegion* r) {\n+  void heap_region_do(ShenandoahHeapRegion* r) override {\n@@ -773,1 +761,0 @@\n-  ShenandoahHeap* const _heap;\n@@ -791,2 +778,1 @@\n-    _heap(ShenandoahHeap::heap()),\n-    _ctx(ShenandoahHeap::heap()->gc_generation()->complete_marking_context()) {}\n+    _ctx(ShenandoahHeap::heap()->global_generation()->complete_marking_context()) {}\n@@ -802,1 +788,0 @@\n-  ShenandoahHeap* const _heap;\n@@ -806,5 +791,3 @@\n-  ShenandoahAdjustPointersObjectClosure() :\n-    _heap(ShenandoahHeap::heap()) {\n-  }\n-  void do_object(oop p) {\n-    assert(_heap->gc_generation()->complete_marking_context()->is_marked(p), \"must be marked\");\n+  void do_object(oop p) override {\n+    assert(ShenandoahHeap::heap()->global_generation()->is_mark_complete(), \"marking must be complete\");\n+    assert(ShenandoahHeap::heap()->marking_context()->is_marked(p), \"must be marked\");\n@@ -826,1 +809,1 @@\n-  void work(uint worker_id) {\n+  void work(uint worker_id) override {\n@@ -852,1 +835,1 @@\n-  void work(uint worker_id) {\n+  void work(uint worker_id) override {\n@@ -886,2 +869,1 @@\n-  ShenandoahHeap* const _heap;\n-  uint            const _worker_id;\n+  uint const _worker_id;\n@@ -890,2 +872,2 @@\n-  ShenandoahCompactObjectsClosure(uint worker_id) :\n-    _heap(ShenandoahHeap::heap()), _worker_id(worker_id) {}\n+  explicit ShenandoahCompactObjectsClosure(uint worker_id) :\n+    _worker_id(worker_id) {}\n@@ -893,2 +875,3 @@\n-  void do_object(oop p) {\n-    assert(_heap->gc_generation()->complete_marking_context()->is_marked(p), \"must be marked\");\n+  void do_object(oop p) override {\n+    assert(ShenandoahHeap::heap()->global_generation()->is_mark_complete(), \"marking must be finished\");\n+    assert(ShenandoahHeap::heap()->marking_context()->is_marked(p), \"must be marked\");\n@@ -922,1 +905,1 @@\n-  void work(uint worker_id) {\n+  void work(uint worker_id) override {\n@@ -959,1 +942,1 @@\n-  void heap_region_do(ShenandoahHeapRegion* r) {\n+  void heap_region_do(ShenandoahHeapRegion* r) override {\n@@ -968,1 +951,1 @@\n-      _heap->gc_generation()->complete_marking_context()->reset_top_at_mark_start(r);\n+      _heap->marking_context()->reset_top_at_mark_start(r);\n@@ -1004,17 +987,0 @@\n-\n-  void update_generation_usage() {\n-    if (_is_generational) {\n-      _heap->old_generation()->establish_usage(_old_regions, _old_usage, _old_humongous_waste);\n-      _heap->young_generation()->establish_usage(_young_regions, _young_usage, _young_humongous_waste);\n-    } else {\n-      assert(_old_regions == 0, \"Old regions only expected in generational mode\");\n-      assert(_old_usage == 0, \"Old usage only expected in generational mode\");\n-      assert(_old_humongous_waste == 0, \"Old humongous waste only expected in generational mode\");\n-    }\n-\n-    \/\/ In generational mode, global usage should be the sum of young and old. This is also true\n-    \/\/ for non-generational modes except that there are no old regions.\n-    _heap->global_generation()->establish_usage(_old_regions + _young_regions,\n-                                                _old_usage + _young_usage,\n-                                                _old_humongous_waste + _young_humongous_waste);\n-  }\n@@ -1093,1 +1059,1 @@\n-\/\/ cannot be iterated over using oop->size(). The only way to safely iterate over those is using\n+\/\/ cannot be iterated over using oop->size()). The only way to safely iterate over those is using\n@@ -1105,1 +1071,1 @@\n-  void work(uint worker_id) {\n+  void work(uint worker_id) override {\n@@ -1109,1 +1075,2 @@\n-    ShenandoahMarkingContext* const ctx = heap->gc_generation()->complete_marking_context();\n+    ShenandoahMarkingContext* const ctx = heap->marking_context();\n+    assert(heap->global_generation()->is_mark_complete(), \"Marking must be complete\");\n@@ -1139,1 +1106,1 @@\n-ShenandoahGenerationalHeap::TransferResult ShenandoahFullGC::phase5_epilog() {\n+void ShenandoahFullGC::phase5_epilog() {\n@@ -1142,1 +1109,0 @@\n-  ShenandoahGenerationalHeap::TransferResult result;\n@@ -1157,6 +1123,0 @@\n-    post_compact.update_generation_usage();\n-\n-    if (heap->mode()->is_generational()) {\n-      ShenandoahGenerationalFullGC::balance_generations_after_gc(heap);\n-    }\n-\n@@ -1177,1 +1137,1 @@\n-    heap->global_generation()->set_mark_incomplete();\n+    _generation->set_mark_incomplete();\n@@ -1179,1 +1139,1 @@\n-    heap->clear_cancelled_gc(true \/* clear oom handler *\/);\n+    heap->clear_cancelled_gc();\n@@ -1185,3 +1145,0 @@\n-  \/\/ We defer generation resizing actions until after cset regions have been recycled.  We do this even following an\n-  \/\/ abbreviated cycle.\n-    result = ShenandoahGenerationalFullGC::balance_generations_after_rebuilding_free_set();\n@@ -1191,1 +1148,0 @@\n-  return result;\n","filename":"src\/hotspot\/share\/gc\/shenandoah\/shenandoahFullGC.cpp","additions":44,"deletions":88,"binary":false,"changes":132,"status":"modified"},{"patch":"@@ -44,1 +44,1 @@\n-  assert(generation->used_including_humongous_waste() <= generation->used_regions_size(),\n+  assert(generation->used() <= generation->used_regions_size(),\n@@ -56,2 +56,1 @@\n-  heap->set_gc_generation(heap->global_generation());\n-  heap->set_active_generation();\n+  heap->set_active_generation(heap->global_generation());\n@@ -87,1 +86,1 @@\n-  old->set_live_bytes_after_last_mark(old->used_including_humongous_waste());\n+  old->set_live_bytes_after_last_mark(old->used());\n@@ -108,27 +107,0 @@\n-void ShenandoahGenerationalFullGC::balance_generations_after_gc(ShenandoahHeap* heap) {\n-  ShenandoahGenerationalHeap* gen_heap = ShenandoahGenerationalHeap::cast(heap);\n-  ShenandoahOldGeneration* const old_gen = gen_heap->old_generation();\n-\n-  size_t old_usage = old_gen->used_regions_size();\n-  size_t old_capacity = old_gen->max_capacity();\n-\n-  assert(old_usage % ShenandoahHeapRegion::region_size_bytes() == 0, \"Old usage must align with region size\");\n-  assert(old_capacity % ShenandoahHeapRegion::region_size_bytes() == 0, \"Old capacity must align with region size\");\n-\n-  if (old_capacity > old_usage) {\n-    size_t excess_old_regions = (old_capacity - old_usage) \/ ShenandoahHeapRegion::region_size_bytes();\n-    gen_heap->generation_sizer()->transfer_to_young(excess_old_regions);\n-  } else if (old_capacity < old_usage) {\n-    size_t old_regions_deficit = (old_usage - old_capacity) \/ ShenandoahHeapRegion::region_size_bytes();\n-    gen_heap->generation_sizer()->force_transfer_to_old(old_regions_deficit);\n-  }\n-\n-  log_info(gc, ergo)(\"FullGC done: young usage: \" PROPERFMT \", old usage: \" PROPERFMT,\n-               PROPERFMTARGS(gen_heap->young_generation()->used()),\n-               PROPERFMTARGS(old_gen->used()));\n-}\n-\n-ShenandoahGenerationalHeap::TransferResult ShenandoahGenerationalFullGC::balance_generations_after_rebuilding_free_set() {\n-  return ShenandoahGenerationalHeap::heap()->balance_generations();\n-}\n-\n@@ -196,1 +168,0 @@\n-        _tenuring_threshold(0),\n@@ -215,2 +186,0 @@\n-\n-  _tenuring_threshold = _heap->age_census()->tenuring_threshold();\n@@ -283,1 +252,1 @@\n-      (from_region_age + object_age >= _tenuring_threshold)) {\n+      _heap->age_census()->is_tenurable(from_region_age + object_age)) {\n","filename":"src\/hotspot\/share\/gc\/shenandoah\/shenandoahGenerationalFullGC.cpp","additions":4,"deletions":35,"binary":false,"changes":39,"status":"modified"},{"patch":"@@ -30,0 +30,1 @@\n+#include \"gc\/shenandoah\/shenandoahGeneration.hpp\"\n@@ -76,1 +77,1 @@\n-size_t ShenandoahGenerationalHeap::unsafe_max_tlab_alloc(Thread *thread) const {\n+size_t ShenandoahGenerationalHeap::unsafe_max_tlab_alloc() const {\n@@ -83,1 +84,0 @@\n-  _evac_tracker(new ShenandoahEvacuationTracker()),\n@@ -103,12 +103,0 @@\n-void ShenandoahGenerationalHeap::print_tracing_info() const {\n-  ShenandoahHeap::print_tracing_info();\n-\n-  LogTarget(Info, gc, stats) lt;\n-  if (lt.is_enabled()) {\n-    LogStream ls(lt);\n-    ls.cr();\n-    ls.cr();\n-    evac_tracker()->print_global_on(&ls);\n-  }\n-}\n-\n@@ -123,3 +111,4 @@\n-  _generation_sizer.heap_size_changed(max_capacity());\n-  size_t initial_capacity_young = _generation_sizer.max_young_size();\n-  size_t max_capacity_young = _generation_sizer.max_young_size();\n+  size_t region_count = num_regions();\n+  size_t max_young_regions = MAX2((region_count * ShenandoahMaxYoungPercentage) \/ 100, (size_t) 1U);\n+  size_t initial_capacity_young = max_young_regions * ShenandoahHeapRegion::region_size_bytes();\n+  size_t max_capacity_young = initial_capacity_young;\n@@ -129,2 +118,2 @@\n-  _young_generation = new ShenandoahYoungGeneration(max_workers(), max_capacity_young);\n-  _old_generation = new ShenandoahOldGeneration(max_workers(), max_capacity_old);\n+  _young_generation = new ShenandoahYoungGeneration(max_workers());\n+  _old_generation = new ShenandoahOldGeneration(max_workers());\n@@ -135,0 +124,6 @@\n+void ShenandoahGenerationalHeap::post_initialize_heuristics() {\n+  ShenandoahHeap::post_initialize_heuristics();\n+  _young_generation->post_initialize(this);\n+  _old_generation->post_initialize(this);\n+}\n+\n@@ -194,1 +189,1 @@\n-void ShenandoahGenerationalHeap::evacuate_collection_set(bool concurrent) {\n+void ShenandoahGenerationalHeap::evacuate_collection_set(ShenandoahGeneration* generation, bool concurrent) {\n@@ -196,1 +191,1 @@\n-  ShenandoahGenerationalEvacuationTask task(this, &regions, concurrent, false \/* only promote regions *\/);\n+  ShenandoahGenerationalEvacuationTask task(this, generation, &regions, concurrent, false \/* only promote regions *\/);\n@@ -200,1 +195,1 @@\n-void ShenandoahGenerationalHeap::promote_regions_in_place(bool concurrent) {\n+void ShenandoahGenerationalHeap::promote_regions_in_place(ShenandoahGeneration* generation, bool concurrent) {\n@@ -202,1 +197,1 @@\n-  ShenandoahGenerationalEvacuationTask task(this, &regions, concurrent, true \/* only promote regions *\/);\n+  ShenandoahGenerationalEvacuationTask task(this, generation, &regions, concurrent, true \/* only promote regions *\/);\n@@ -232,1 +227,1 @@\n-    } else if (r->age() + mark.age() >= age_census()->tenuring_threshold()) {\n+    } else if (age_census()->is_tenurable(r->age() + mark.age())) {\n@@ -290,0 +285,1 @@\n+              \/\/ Shrinking the desired PLAB size may allow us to eke out a small PLAB while staying beneath evacuation reserve.\n@@ -347,0 +343,4 @@\n+  if (ShenandoahEvacTracking) {\n+    evac_tracker()->begin_evacuation(thread, size * HeapWordSize, from_region->affiliation(), target_gen);\n+  }\n+\n@@ -348,1 +348,0 @@\n-  NOT_PRODUCT(evac_tracker()->begin_evacuation(thread, size * HeapWordSize));\n@@ -370,2 +369,4 @@\n-    \/\/ Record that the evacuation succeeded\n-    NOT_PRODUCT(evac_tracker()->end_evacuation(thread, size * HeapWordSize));\n+    if (ShenandoahEvacTracking) {\n+      \/\/ Record that the evacuation succeeded\n+      evac_tracker()->end_evacuation(thread, size * HeapWordSize, from_region->affiliation(), target_gen);\n+    }\n@@ -379,5 +380,0 @@\n-      \/\/ We record this census only when simulating pre-adaptive tenuring behavior, or\n-      \/\/ when we have been asked to record the census at evacuation rather than at mark\n-      if (ShenandoahGenerationalCensusAtEvac || !ShenandoahGenerationalAdaptiveTenuring) {\n-        evac_tracker()->record_age(thread, size * HeapWordSize, ShenandoahHeap::get_object_age(copy_val));\n-      }\n@@ -460,2 +456,1 @@\n-  \/\/ New object should fit the PLAB size\n-\n+\n@@ -475,1 +470,1 @@\n-  size_t future_size = MIN2(cur_size * 2, plab_max_size());\n+  const size_t future_size = MIN2(cur_size * 2, plab_max_size());\n@@ -479,2 +474,2 @@\n-          \", card_size: %zu, cur_size: %zu, max: %zu\",\n-         future_size, (size_t) CardTable::card_size_in_words(), cur_size, plab_max_size());\n+          \", card_size: %u, cur_size: %zu, max: %zu\",\n+         future_size, CardTable::card_size_in_words(), cur_size, plab_max_size());\n@@ -486,1 +481,1 @@\n-  log_debug(gc, free)(\"Set new PLAB size: %zu\", future_size);\n+  log_debug(gc, plab)(\"Set next PLAB refill size: %zu bytes\", future_size * HeapWordSize);\n@@ -488,0 +483,1 @@\n+\n@@ -491,1 +487,1 @@\n-    log_debug(gc, free)(\"Current PLAB size (%zu) is too small for %zu\", cur_size, size);\n+    log_debug(gc, plab)(\"Current PLAB size (%zu) is too small for %zu\", cur_size * HeapWordSize, size * HeapWordSize);\n@@ -577,0 +573,1 @@\n+    log_debug(gc, plab)(\"Retire PLAB, unexpend unpromoted: %zu\", not_promoted * HeapWordSize);\n@@ -588,2 +585,2 @@\n-    log_debug(gc)(\"retire_plab() is registering remnant of size %zu at \" PTR_FORMAT,\n-                  plab->waste() - original_waste, p2i(top));\n+    log_debug(gc, plab)(\"retire_plab() is registering remnant of size %zu at \" PTR_FORMAT,\n+                        (plab->waste() - original_waste) * HeapWordSize, p2i(top));\n@@ -600,29 +597,0 @@\n-ShenandoahGenerationalHeap::TransferResult ShenandoahGenerationalHeap::balance_generations() {\n-  shenandoah_assert_heaplocked_or_safepoint();\n-\n-  ShenandoahOldGeneration* old_gen = old_generation();\n-  const ssize_t old_region_balance = old_gen->get_region_balance();\n-  old_gen->set_region_balance(0);\n-\n-  if (old_region_balance > 0) {\n-    const auto old_region_surplus = checked_cast<size_t>(old_region_balance);\n-    const bool success = generation_sizer()->transfer_to_young(old_region_surplus);\n-    return TransferResult {\n-      success, old_region_surplus, \"young\"\n-    };\n-  }\n-\n-  if (old_region_balance < 0) {\n-    const auto old_region_deficit = checked_cast<size_t>(-old_region_balance);\n-    const bool success = generation_sizer()->transfer_to_old(old_region_deficit);\n-    if (!success) {\n-      old_gen->handle_failed_transfer();\n-    }\n-    return TransferResult {\n-      success, old_region_deficit, \"old\"\n-    };\n-  }\n-\n-  return TransferResult {true, 0, \"none\"};\n-}\n-\n@@ -632,1 +600,4 @@\n-\/\/ xfer_limit is the maximum we're able to transfer from young to old.\n+\/\/\n+\/\/ xfer_limit is the maximum we're able to transfer from young to old based on either:\n+\/\/  1. an assumption that we will be able to replenish memory \"borrowed\" from young at the end of collection, or\n+\/\/  2. there is sufficient excess in the allocation runway during GC idle cycles\n@@ -660,3 +631,3 @@\n-  const double max_old_reserve = (ShenandoahOldEvacRatioPercent == 100)?\n-                                 bound_on_old_reserve: MIN2(double(young_reserve * ShenandoahOldEvacRatioPercent) \/ double(100 - ShenandoahOldEvacRatioPercent),\n-                                                            bound_on_old_reserve);\n+  const double max_old_reserve = ((ShenandoahOldEvacRatioPercent == 100)? bound_on_old_reserve:\n+                                  MIN2(double(young_reserve * ShenandoahOldEvacRatioPercent)\n+                                       \/ double(100 - ShenandoahOldEvacRatioPercent), bound_on_old_reserve));\n@@ -671,1 +642,2 @@\n-    const double max_evac_need = (double(old_generation()->unprocessed_collection_candidates_live_memory()) * ShenandoahOldEvacWaste);\n+    const double max_evac_need =\n+      (double(old_generation()->unprocessed_collection_candidates_live_memory()) * ShenandoahOldEvacWaste);\n@@ -674,1 +646,2 @@\n-    const double old_fragmented_available = double(old_available - old_generation()->free_unaffiliated_regions() * region_size_bytes);\n+    const double old_fragmented_available =\n+      double(old_available - old_generation()->free_unaffiliated_regions() * region_size_bytes);\n@@ -721,0 +694,1 @@\n+  ShenandoahHeapLocker locker(lock());\n@@ -781,0 +755,2 @@\n+  \/\/ For update refs, _generation will be young or global. Mixed collections use the young generation.\n+  ShenandoahGeneration* _generation;\n@@ -786,2 +762,3 @@\n-  explicit ShenandoahGenerationalUpdateHeapRefsTask(ShenandoahRegionIterator* regions,\n-                                                    ShenandoahRegionChunkIterator* work_chunks) :\n+  ShenandoahGenerationalUpdateHeapRefsTask(ShenandoahGeneration* generation,\n+                                           ShenandoahRegionIterator* regions,\n+                                           ShenandoahRegionChunkIterator* work_chunks) :\n@@ -789,0 +766,1 @@\n+          _generation(generation),\n@@ -793,1 +771,1 @@\n-    bool old_bitmap_stable = _heap->old_generation()->is_mark_complete();\n+    const bool old_bitmap_stable = _heap->old_generation()->is_mark_complete();\n@@ -797,1 +775,1 @@\n-  void work(uint worker_id) {\n+  void work(uint worker_id) override {\n@@ -827,4 +805,2 @@\n-    \/\/ We update references for global, old, and young collections.\n-    ShenandoahGeneration* const gc_generation = _heap->gc_generation();\n-    shenandoah_assert_generations_reconciled();\n-    assert(gc_generation->is_mark_complete(), \"Expected complete marking\");\n+    \/\/ We update references for global, mixed, and young collections.\n+    assert(_generation->is_mark_complete(), \"Expected complete marking\");\n@@ -838,1 +814,0 @@\n-      bool region_progress = false;\n@@ -842,2 +817,1 @@\n-          region_progress = true;\n-          if (gc_generation->is_global()) {\n+          if (_generation->is_global()) {\n@@ -847,1 +821,0 @@\n-            region_progress = true;\n@@ -850,1 +823,0 @@\n-          \/\/ Don't bother to report pacing progress in this case.\n@@ -868,4 +840,0 @@\n-      if (region_progress && ShenandoahPacing) {\n-        _heap->pacer()->report_update_refs(pointer_delta(update_watermark, r->bottom()));\n-      }\n-\n@@ -879,1 +847,1 @@\n-    if (!gc_generation->is_global()) {\n+    if (_generation->is_young()) {\n@@ -927,4 +895,0 @@\n-\n-        if (ShenandoahPacing) {\n-          _heap->pacer()->report_update_refs(pointer_delta(end_of_range, start_of_range));\n-        }\n@@ -997,1 +961,1 @@\n-void ShenandoahGenerationalHeap::update_heap_references(bool concurrent) {\n+void ShenandoahGenerationalHeap::update_heap_references(ShenandoahGeneration* generation, bool concurrent) {\n@@ -1002,1 +966,1 @@\n-    ShenandoahGenerationalUpdateHeapRefsTask<true> task(&_update_refs_iterator, &work_list);\n+    ShenandoahGenerationalUpdateHeapRefsTask<true> task(generation, &_update_refs_iterator, &work_list);\n@@ -1005,1 +969,1 @@\n-    ShenandoahGenerationalUpdateHeapRefsTask<false> task(&_update_refs_iterator, &work_list);\n+    ShenandoahGenerationalUpdateHeapRefsTask<false> task(generation, &_update_refs_iterator, &work_list);\n@@ -1080,1 +1044,1 @@\n-  ShenandoahUpdateRegionAges ages(active_generation()->complete_marking_context());\n+  ShenandoahUpdateRegionAges ages(marking_context());\n@@ -1087,15 +1051,0 @@\n-  if (is_concurrent_old_mark_in_progress()) {\n-    \/\/ This is still necessary for degenerated cycles because the degeneration point may occur\n-    \/\/ after final mark of the young generation. See ShenandoahConcurrentGC::op_final_update_refs for\n-    \/\/ a more detailed explanation.\n-    old_generation()->transfer_pointers_from_satb();\n-  }\n-\n-  \/\/ We defer generation resizing actions until after cset regions have been recycled.\n-  TransferResult result = balance_generations();\n-  LogTarget(Info, gc, ergo) lt;\n-  if (lt.is_enabled()) {\n-    LogStream ls(lt);\n-    result.print_on(\"Degenerated GC\", &ls);\n-  }\n-\n@@ -1123,14 +1072,1 @@\n-\n-  TransferResult result;\n-  {\n-    ShenandoahHeapLocker locker(lock());\n-\n-    result = balance_generations();\n-    reset_generation_reserves();\n-  }\n-\n-  LogTarget(Info, gc, ergo) lt;\n-  if (lt.is_enabled()) {\n-    LogStream ls(lt);\n-    result.print_on(\"Concurrent GC\", &ls);\n-  }\n+  reset_generation_reserves();\n","filename":"src\/hotspot\/share\/gc\/shenandoah\/shenandoahGenerationalHeap.cpp","additions":67,"deletions":131,"binary":false,"changes":198,"status":"modified"},{"patch":"@@ -28,1 +28,1 @@\n-#include \"cds\/archiveHeapWriter.hpp\"\n+#include \"cds\/aotMappedHeapWriter.hpp\"\n@@ -32,0 +32,1 @@\n+#include \"gc\/shared\/gc_globals.hpp\"\n@@ -35,1 +36,0 @@\n-#include \"gc\/shared\/gc_globals.hpp\"\n@@ -66,1 +66,0 @@\n-#include \"gc\/shenandoah\/shenandoahPacer.inline.hpp\"\n@@ -90,1 +89,1 @@\n-#include \"runtime\/atomic.hpp\"\n+#include \"runtime\/atomicAccess.hpp\"\n@@ -205,1 +204,1 @@\n-  _soft_max_size = SoftMaxHeapSize;\n+  _soft_max_size = clamp(SoftMaxHeapSize, min_capacity(), max_capacity());\n@@ -252,0 +251,7 @@\n+  \/\/ If ShenandoahCardBarrier is enabled but it's not generational mode\n+  \/\/ it means we're under passive mode and we have to initialize old gen\n+  \/\/ for the purpose of having card table.\n+  if (ShenandoahCardBarrier && !(mode()->is_generational())) {\n+    _old_generation = new ShenandoahOldGeneration(max_workers());\n+  }\n+\n@@ -408,1 +414,0 @@\n-    _free_set = new ShenandoahFreeSet(this, _num_regions);\n@@ -423,0 +428,1 @@\n+    _free_set = new ShenandoahFreeSet(this, _num_regions);\n@@ -424,1 +430,1 @@\n-    size_t young_cset_regions, old_cset_regions;\n+    post_initialize_heuristics();\n@@ -427,1 +433,1 @@\n-    size_t first_old, last_old, num_old;\n+    size_t young_cset_regions, old_cset_regions, first_old, last_old, num_old;\n@@ -429,0 +435,8 @@\n+    if (mode()->is_generational()) {\n+      ShenandoahGenerationalHeap* gen_heap = ShenandoahGenerationalHeap::heap();\n+      \/\/ We cannot call\n+      \/\/  gen_heap->young_generation()->heuristics()->bytes_of_allocation_runway_before_gc_trigger(young_cset_regions)\n+      \/\/ until after the heap is fully initialized.  So we make up a safe value here.\n+      size_t allocation_runway = InitialHeapSize \/ 2;\n+      gen_heap->compute_old_generation_balance(allocation_runway, old_cset_regions);\n+    }\n@@ -473,5 +487,0 @@\n-  if (ShenandoahPacing) {\n-    _pacer = new ShenandoahPacer(this);\n-    _pacer->setup_for_idle();\n-  }\n-\n@@ -527,1 +536,1 @@\n-  _global_generation = new ShenandoahGlobalGeneration(mode()->is_generational(), max_workers(), max_capacity());\n+  _global_generation = new ShenandoahGlobalGeneration(mode()->is_generational(), max_workers());\n@@ -531,0 +540,4 @@\n+void ShenandoahHeap::post_initialize_heuristics() {\n+  _global_generation->post_initialize(this);\n+}\n+\n@@ -538,1 +551,0 @@\n-  _gc_generation(nullptr),\n@@ -561,1 +573,0 @@\n-  _pacer(nullptr),\n@@ -577,1 +588,2 @@\n-  _collection_set(nullptr)\n+  _collection_set(nullptr),\n+  _evac_tracker(new ShenandoahEvacuationTracker())\n@@ -589,10 +601,25 @@\n-  st->print_cr(\"Shenandoah Heap\");\n-  st->print_cr(\" %zu%s max, %zu%s soft max, %zu%s committed, %zu%s used\",\n-               byte_size_in_proper_unit(max_capacity()), proper_unit_for_byte_size(max_capacity()),\n-               byte_size_in_proper_unit(soft_max_capacity()), proper_unit_for_byte_size(soft_max_capacity()),\n-               byte_size_in_proper_unit(committed()),    proper_unit_for_byte_size(committed()),\n-               byte_size_in_proper_unit(used()),         proper_unit_for_byte_size(used()));\n-  st->print_cr(\" %zu x %zu %s regions\",\n-               num_regions(),\n-               byte_size_in_proper_unit(ShenandoahHeapRegion::region_size_bytes()),\n-               proper_unit_for_byte_size(ShenandoahHeapRegion::region_size_bytes()));\n+  const bool is_generational = mode()->is_generational();\n+  const char* front_spacing = \"\";\n+  if (is_generational) {\n+    st->print_cr(\"Generational Shenandoah Heap\");\n+    st->print_cr(\" Young:\");\n+    st->print_cr(\"  \" PROPERFMT \" max, \" PROPERFMT \" used\", PROPERFMTARGS(young_generation()->max_capacity()), PROPERFMTARGS(young_generation()->used()));\n+    st->print_cr(\" Old:\");\n+    st->print_cr(\"  \" PROPERFMT \" max, \" PROPERFMT \" used\", PROPERFMTARGS(old_generation()->max_capacity()), PROPERFMTARGS(old_generation()->used()));\n+    st->print_cr(\" Entire heap:\");\n+    st->print_cr(\"  \" PROPERFMT \" soft max, \" PROPERFMT \" committed\",\n+                PROPERFMTARGS(soft_max_capacity()), PROPERFMTARGS(committed()));\n+    front_spacing = \" \";\n+  } else {\n+    st->print_cr(\"Shenandoah Heap\");\n+    st->print_cr(\"  \" PROPERFMT \" max, \" PROPERFMT \" soft max, \" PROPERFMT \" committed, \" PROPERFMT \" used\",\n+      PROPERFMTARGS(max_capacity()),\n+      PROPERFMTARGS(soft_max_capacity()),\n+      PROPERFMTARGS(committed()),\n+      PROPERFMTARGS(used())\n+    );\n+  }\n+  st->print_cr(\"%s %zu x \" PROPERFMT \" regions\",\n+          front_spacing,\n+          num_regions(),\n+          PROPERFMTARGS(ShenandoahHeapRegion::region_size_bytes()));\n@@ -602,1 +629,1 @@\n-  if (!mode()->is_generational()) {\n+  if (!is_generational) {\n@@ -661,0 +688,2 @@\n+  check_soft_max_changed();\n+\n@@ -692,1 +721,1 @@\n-  return Atomic::load(&_committed);\n+  return AtomicAccess::load(&_committed);\n@@ -705,82 +734,0 @@\n-\/\/ For tracking usage based on allocations, it should be the case that:\n-\/\/ * The sum of regions::used == heap::used\n-\/\/ * The sum of a generation's regions::used == generation::used\n-\/\/ * The sum of a generation's humongous regions::free == generation::humongous_waste\n-\/\/ These invariants are checked by the verifier on GC safepoints.\n-\/\/\n-\/\/ Additional notes:\n-\/\/ * When a mutator's allocation request causes a region to be retired, the\n-\/\/   free memory left in that region is considered waste. It does not contribute\n-\/\/   to the usage, but it _does_ contribute to allocation rate.\n-\/\/ * The bottom of a PLAB must be aligned on card size. In some cases this will\n-\/\/   require padding in front of the PLAB (a filler object). Because this padding\n-\/\/   is included in the region's used memory we include the padding in the usage\n-\/\/   accounting as waste.\n-\/\/ * Mutator allocations are used to compute an allocation rate. They are also\n-\/\/   sent to the Pacer for those purposes.\n-\/\/ * There are three sources of waste:\n-\/\/  1. The padding used to align a PLAB on card size\n-\/\/  2. Region's free is less than minimum TLAB size and is retired\n-\/\/  3. The unused portion of memory in the last region of a humongous object\n-void ShenandoahHeap::increase_used(const ShenandoahAllocRequest& req) {\n-  size_t actual_bytes = req.actual_size() * HeapWordSize;\n-  size_t wasted_bytes = req.waste() * HeapWordSize;\n-  ShenandoahGeneration* generation = generation_for(req.affiliation());\n-\n-  if (req.is_gc_alloc()) {\n-    assert(wasted_bytes == 0 || req.type() == ShenandoahAllocRequest::_alloc_plab, \"Only PLABs have waste\");\n-    increase_used(generation, actual_bytes + wasted_bytes);\n-  } else {\n-    assert(req.is_mutator_alloc(), \"Expected mutator alloc here\");\n-    \/\/ padding and actual size both count towards allocation counter\n-    generation->increase_allocated(actual_bytes + wasted_bytes);\n-\n-    \/\/ only actual size counts toward usage for mutator allocations\n-    increase_used(generation, actual_bytes);\n-\n-    \/\/ notify pacer of both actual size and waste\n-    notify_mutator_alloc_words(req.actual_size(), req.waste());\n-\n-    if (wasted_bytes > 0 && ShenandoahHeapRegion::requires_humongous(req.actual_size())) {\n-      increase_humongous_waste(generation,wasted_bytes);\n-    }\n-  }\n-}\n-\n-void ShenandoahHeap::increase_humongous_waste(ShenandoahGeneration* generation, size_t bytes) {\n-  generation->increase_humongous_waste(bytes);\n-  if (!generation->is_global()) {\n-    global_generation()->increase_humongous_waste(bytes);\n-  }\n-}\n-\n-void ShenandoahHeap::decrease_humongous_waste(ShenandoahGeneration* generation, size_t bytes) {\n-  generation->decrease_humongous_waste(bytes);\n-  if (!generation->is_global()) {\n-    global_generation()->decrease_humongous_waste(bytes);\n-  }\n-}\n-\n-void ShenandoahHeap::increase_used(ShenandoahGeneration* generation, size_t bytes) {\n-  generation->increase_used(bytes);\n-  if (!generation->is_global()) {\n-    global_generation()->increase_used(bytes);\n-  }\n-}\n-\n-void ShenandoahHeap::decrease_used(ShenandoahGeneration* generation, size_t bytes) {\n-  generation->decrease_used(bytes);\n-  if (!generation->is_global()) {\n-    global_generation()->decrease_used(bytes);\n-  }\n-}\n-\n-void ShenandoahHeap::notify_mutator_alloc_words(size_t words, size_t waste) {\n-  if (ShenandoahPacing) {\n-    control_thread()->pacing_notify_alloc(words);\n-    if (waste > 0) {\n-      pacer()->claim_for_alloc<true>(waste);\n-    }\n-  }\n-}\n-\n@@ -796,1 +743,1 @@\n-  size_t v = Atomic::load(&_soft_max_size);\n+  size_t v = AtomicAccess::load(&_soft_max_size);\n@@ -807,1 +754,1 @@\n-  Atomic::store(&_soft_max_size, v);\n+  AtomicAccess::store(&_soft_max_size, v);\n@@ -859,1 +806,1 @@\n-  size_t new_soft_max = Atomic::load(&SoftMaxHeapSize);\n+  size_t new_soft_max = AtomicAccess::load(&SoftMaxHeapSize);\n@@ -968,1 +915,0 @@\n-  intptr_t pacer_epoch = 0;\n@@ -973,4 +919,0 @@\n-    if (ShenandoahPacing) {\n-      pacer()->pace_for_alloc(req.size());\n-      pacer_epoch = pacer()->epoch();\n-    }\n@@ -1040,4 +982,0 @@\n-  \/\/ This is called regardless of the outcome of the allocation to account\n-  \/\/ for any waste created by retiring regions with this request.\n-  increase_used(req);\n-\n@@ -1051,9 +989,0 @@\n-\n-    if (req.is_mutator_alloc()) {\n-      \/\/ If we requested more than we were granted, give the rest back to pacer.\n-      \/\/ This only matters if we are in the same pacing epoch: do not try to unpace\n-      \/\/ over the budget for the other phase.\n-      if (ShenandoahPacing && (pacer_epoch > 0) && (requested > actual)) {\n-        pacer()->unpace_for_alloc(pacer_epoch, requested - actual);\n-      }\n-    }\n@@ -1115,2 +1044,1 @@\n-HeapWord* ShenandoahHeap::mem_allocate(size_t size,\n-                                        bool*  gc_overhead_limit_was_exceeded) {\n+HeapWord* ShenandoahHeap::mem_allocate(size_t size) {\n@@ -1209,4 +1137,0 @@\n-      if (ShenandoahPacing) {\n-        _sh->pacer()->report_evac(r->used() >> LogHeapWordSize);\n-      }\n-\n@@ -1241,0 +1165,5 @@\n+\n+      \/\/ Re-enable promotions for the next evacuation phase.\n+      ShenandoahThreadLocalData::enable_plab_promotions(thread);\n+\n+      \/\/ Reset the fill size for next evacuation phase.\n@@ -1278,1 +1207,2 @@\n-void ShenandoahHeap::evacuate_collection_set(bool concurrent) {\n+void ShenandoahHeap::evacuate_collection_set(ShenandoahGeneration* generation, bool concurrent) {\n+  assert(generation->is_global(), \"Only global generation expected here\");\n@@ -1285,1 +1215,1 @@\n-    \/\/ Java threads take this lock while they are being attached and added to the list of thread.\n+    \/\/ Java threads take this lock while they are being attached and added to the list of threads.\n@@ -1398,0 +1328,4 @@\n+  if (ShenandoahEvacTracking) {\n+    evac_tracker()->begin_evacuation(thread, size * HeapWordSize, from_region->affiliation(), target_gen);\n+  }\n+\n@@ -1409,0 +1343,3 @@\n+    if (ShenandoahEvacTracking) {\n+      evac_tracker()->end_evacuation(thread, size * HeapWordSize, from_region->affiliation(), target_gen);\n+    }\n@@ -1462,0 +1399,21 @@\n+void ShenandoahHeap::process_gc_stats() const {\n+  \/\/ Commit worker statistics to cycle data\n+  phase_timings()->flush_par_workers_to_cycle();\n+\n+  \/\/ Print GC stats for current cycle\n+  LogTarget(Info, gc, stats) lt;\n+  if (lt.is_enabled()) {\n+    ResourceMark rm;\n+    LogStream ls(lt);\n+    phase_timings()->print_cycle_on(&ls);\n+    if (ShenandoahEvacTracking) {\n+      ShenandoahCycleStats  evac_stats = evac_tracker()->flush_cycle_to_global();\n+      evac_tracker()->print_evacuations_on(&ls, &evac_stats.workers,\n+                                               &evac_stats.mutators);\n+    }\n+  }\n+\n+  \/\/ Commit statistics to globals\n+  phase_timings()->flush_cycle_to_global();\n+}\n+\n@@ -1559,1 +1517,1 @@\n-size_t ShenandoahHeap::unsafe_max_tlab_alloc(Thread *thread) const {\n+size_t ShenandoahHeap::unsafe_max_tlab_alloc() const {\n@@ -1576,2 +1534,2 @@\n-  \/\/ concurrent cycle in the prologue of the heap inspect\/dump operation. This is how\n-  \/\/ other concurrent collectors in the JVM handle this scenario as well.\n+  \/\/ concurrent cycle in the prologue of the heap inspect\/dump operation (see VM_HeapDumper::doit_prologue).\n+  \/\/ This is how other concurrent collectors in the JVM handle this scenario as well.\n@@ -1587,1 +1545,4 @@\n-  \/\/assert(false, \"Shouldn't need to do full collections\");\n+  \/\/ This method is only called by `CollectedHeap::collect_as_vm_thread`, which we have\n+  \/\/ overridden to do nothing. See the comment there for an explanation of how heap inspections\n+  \/\/ work for Shenandoah.\n+  ShouldNotReachHere();\n@@ -1638,0 +1599,6 @@\n+    if (ShenandoahEvacTracking) {\n+      evac_tracker()->print_global_on(&ls);\n+      ls.cr();\n+      ls.cr();\n+    }\n+\n@@ -1650,6 +1617,1 @@\n-void ShenandoahHeap::set_gc_generation(ShenandoahGeneration* generation) {\n-  shenandoah_assert_control_or_vm_thread_at_safepoint();\n-  _gc_generation = generation;\n-}\n-\n-void ShenandoahHeap::set_active_generation() {\n+void ShenandoahHeap::set_active_generation(ShenandoahGeneration* generation) {\n@@ -1659,2 +1621,1 @@\n-  assert(_gc_generation != nullptr, \"Will set _active_generation to nullptr\");\n-  _active_generation = _gc_generation;\n+  _active_generation = generation;\n@@ -1669,1 +1630,0 @@\n-  assert(_gc_generation == nullptr, \"Over-writing _gc_generation\");\n@@ -1672,1 +1632,0 @@\n-  set_gc_generation(generation);\n@@ -1679,1 +1638,0 @@\n-  assert(_gc_generation != nullptr, \"_gc_generation wasn't set\");\n@@ -1688,1 +1646,0 @@\n-  set_gc_generation(nullptr);\n@@ -1695,1 +1652,1 @@\n-      verifier()->verify_generic(vo);\n+      verifier()->verify_generic(active_generation(), vo);\n@@ -1702,1 +1659,1 @@\n-size_t ShenandoahHeap::tlab_capacity(Thread *thr) const {\n+size_t ShenandoahHeap::tlab_capacity() const {\n@@ -1789,4 +1746,7 @@\n-\n-  if (!_aux_bitmap_region_special && !os::commit_memory((char*)_aux_bitmap_region.start(), _aux_bitmap_region.byte_size(), false)) {\n-    log_warning(gc)(\"Could not commit native memory for auxiliary marking bitmap for heap iteration\");\n-    return false;\n+  if (!_aux_bitmap_region_special) {\n+    bool success = os::commit_memory((char *) _aux_bitmap_region.start(), _aux_bitmap_region.byte_size(), false);\n+    if (!success) {\n+      log_warning(gc)(\"Auxiliary marking bitmap commit failed: \" PTR_FORMAT \" (%zu bytes)\",\n+                      p2i(_aux_bitmap_region.start()), _aux_bitmap_region.byte_size());\n+      return false;\n+    }\n@@ -1794,1 +1754,0 @@\n-  \/\/ Reset bitmap\n@@ -1810,2 +1769,7 @@\n-  if (!_aux_bitmap_region_special && !os::uncommit_memory((char*)_aux_bitmap_region.start(), _aux_bitmap_region.byte_size())) {\n-    log_warning(gc)(\"Could not uncommit native memory for auxiliary marking bitmap for heap iteration\");\n+  if (!_aux_bitmap_region_special) {\n+    bool success = os::uncommit_memory((char*)_aux_bitmap_region.start(), _aux_bitmap_region.byte_size());\n+    if (!success) {\n+      log_warning(gc)(\"Auxiliary marking bitmap uncommit failed: \" PTR_FORMAT \" (%zu bytes)\",\n+                      p2i(_aux_bitmap_region.start()), _aux_bitmap_region.byte_size());\n+      assert(false, \"Auxiliary marking bitmap uncommit should always succeed\");\n+    }\n@@ -1990,2 +1954,2 @@\n-    while (Atomic::load(&_index) < max) {\n-      size_t cur = Atomic::fetch_then_add(&_index, stride, memory_order_relaxed);\n+    while (AtomicAccess::load(&_index) < max) {\n+      size_t cur = AtomicAccess::fetch_then_add(&_index, stride, memory_order_relaxed);\n@@ -2048,1 +2012,1 @@\n-void ShenandoahHeap::stw_weak_refs(bool full_gc) {\n+void ShenandoahHeap::stw_weak_refs(ShenandoahGeneration* generation, bool full_gc) {\n@@ -2054,2 +2018,1 @@\n-  shenandoah_assert_generations_reconciled();\n-  gc_generation()->ref_processor()->process_references(phase, workers(), false \/* concurrent *\/);\n+  generation->ref_processor()->process_references(phase, workers(), false \/* concurrent *\/);\n@@ -2075,0 +2038,4 @@\n+    \/\/ If we are only marking old, we do not need to process young pointers\n+    ShenandoahBarrierSet::satb_mark_queue_set().set_filter_out_young(\n+      is_concurrent_old_mark_in_progress() && !is_concurrent_young_mark_in_progress()\n+    );\n@@ -2172,1 +2139,1 @@\n-size_t ShenandoahHeap::tlab_used(Thread* thread) const {\n+size_t ShenandoahHeap::tlab_used() const {\n@@ -2246,2 +2213,1 @@\n-      uint num_workers = _workers->active_workers();\n-      ShenandoahClassUnloadingTask unlink_task(phase, num_workers, unloading_occurred);\n+      ShenandoahClassUnloadingTask unlink_task(phase, unloading_occurred);\n@@ -2297,1 +2263,1 @@\n-void ShenandoahHeap::parallel_cleaning(bool full_gc) {\n+void ShenandoahHeap::parallel_cleaning(ShenandoahGeneration* generation, bool full_gc) {\n@@ -2303,1 +2269,1 @@\n-  stw_weak_refs(full_gc);\n+  stw_weak_refs(generation, full_gc);\n@@ -2327,0 +2293,11 @@\n+  \/\/ It is important to force_alloc_rate_sample() before the associated generation's bytes_allocated has been reset.\n+  \/\/ Note that there is no lock to prevent additional alloations between sampling bytes_allocated_since_gc_start() and\n+  \/\/ reset_bytes_allocated_since_gc_start().  If additional allocations happen, they will be ignored in the average\n+  \/\/ allocation rate computations.  This effect is considered to be be negligible.\n+\n+  \/\/ unaccounted_bytes is the bytes not accounted for by our forced sample.  If the sample interval is too short,\n+  \/\/ the \"forced sample\" will not happen, and any recently allocated bytes are \"unaccounted for\".  We pretend these\n+  \/\/ bytes are allocated after the start of subsequent gc.\n+  size_t unaccounted_bytes;\n+  ShenandoahFreeSet* _free_set = free_set();\n+  size_t bytes_allocated = _free_set->get_bytes_allocated_since_gc_start();\n@@ -2328,2 +2305,4 @@\n-    young_generation()->reset_bytes_allocated_since_gc_start();\n-    old_generation()->reset_bytes_allocated_since_gc_start();\n+    unaccounted_bytes = young_generation()->heuristics()->force_alloc_rate_sample(bytes_allocated);\n+  } else {\n+    \/\/ Single-gen Shenandoah uses global heuristics.\n+    unaccounted_bytes = heuristics()->force_alloc_rate_sample(bytes_allocated);\n@@ -2331,2 +2310,2 @@\n-\n-  global_generation()->reset_bytes_allocated_since_gc_start();\n+  ShenandoahHeapLocker locker(lock());\n+  _free_set->reset_bytes_allocated_since_gc_start(unaccounted_bytes);\n@@ -2393,1 +2372,5 @@\n-void ShenandoahHeap::assert_pinned_region_status() {\n+void ShenandoahHeap::assert_pinned_region_status() const {\n+  assert_pinned_region_status(global_generation());\n+}\n+\n+void ShenandoahHeap::assert_pinned_region_status(ShenandoahGeneration* generation) const {\n@@ -2396,2 +2379,1 @@\n-    shenandoah_assert_generations_reconciled();\n-    if (gc_generation()->contains(r)) {\n+    if (generation->contains(r)) {\n@@ -2494,3 +2476,0 @@\n-        if (ShenandoahPacing) {\n-          _heap->pacer()->report_update_refs(pointer_delta(update_watermark, r->bottom()));\n-        }\n@@ -2506,1 +2485,2 @@\n-void ShenandoahHeap::update_heap_references(bool concurrent) {\n+void ShenandoahHeap::update_heap_references(ShenandoahGeneration* generation, bool concurrent) {\n+  assert(generation->is_global(), \"Should only get global generation here\");\n@@ -2606,1 +2586,1 @@\n-bool ShenandoahHeap::commit_bitmap_slice(ShenandoahHeapRegion* r) {\n+void ShenandoahHeap::commit_bitmap_slice(ShenandoahHeapRegion* r) {\n@@ -2608,5 +2588,1 @@\n-\n-  \/\/ Bitmaps in special regions do not need commits\n-  if (_bitmap_region_special) {\n-    return true;\n-  }\n+  assert(!is_bitmap_region_special(), \"Not for special memory\");\n@@ -2617,1 +2593,1 @@\n-    return true;\n+    return;\n@@ -2626,3 +2602,1 @@\n-  if (!os::commit_memory(start, len, false)) {\n-    return false;\n-  }\n+  os::commit_memory_or_exit(start, len, false, \"Unable to commit bitmap slice\");\n@@ -2633,2 +2607,0 @@\n-\n-  return true;\n@@ -2637,1 +2609,1 @@\n-bool ShenandoahHeap::uncommit_bitmap_slice(ShenandoahHeapRegion *r) {\n+void ShenandoahHeap::uncommit_bitmap_slice(ShenandoahHeapRegion *r) {\n@@ -2639,5 +2611,1 @@\n-\n-  \/\/ Bitmaps in special regions do not need uncommits\n-  if (_bitmap_region_special) {\n-    return true;\n-  }\n+  assert(!is_bitmap_region_special(), \"Not for special memory\");\n@@ -2648,1 +2616,1 @@\n-    return true;\n+    return;\n@@ -2655,2 +2623,6 @@\n-  if (!os::uncommit_memory((char*)_bitmap_region.start() + off, len)) {\n-    return false;\n+\n+  char* addr = (char*) _bitmap_region.start() + off;\n+  bool success = os::uncommit_memory(addr, len);\n+  if (!success) {\n+    log_warning(gc)(\"Bitmap slice uncommit failed: \" PTR_FORMAT \" (%zu bytes)\", p2i(addr), len);\n+    assert(false, \"Bitmap slice uncommit should always succeed\");\n@@ -2658,1 +2630,0 @@\n-  return true;\n@@ -2725,0 +2696,3 @@\n+  assert(_initial_size <= ShenandoahHeap::heap()->max_capacity(), \"sanity\");\n+  assert(used() <= ShenandoahHeap::heap()->max_capacity(), \"sanity\");\n+  assert(committed() <= ShenandoahHeap::heap()->max_capacity(), \"sanity\");\n@@ -2801,33 +2775,6 @@\n-  \/\/ CDS wants a continuous memory range to load a bunch of objects.\n-  \/\/ This effectively bypasses normal allocation paths, and requires\n-  \/\/ a bit of massaging to unbreak GC invariants.\n-\n-  ShenandoahAllocRequest req = ShenandoahAllocRequest::for_shared(size);\n-\n-  \/\/ Easy case: a single regular region, no further adjustments needed.\n-  if (!ShenandoahHeapRegion::requires_humongous(size)) {\n-    return allocate_memory(req);\n-  }\n-\n-  \/\/ Hard case: the requested size would cause a humongous allocation.\n-  \/\/ We need to make sure it looks like regular allocation to the rest of GC.\n-\n-  \/\/ CDS code would guarantee no objects straddle multiple regions, as long as\n-  \/\/ regions are as large as MIN_GC_REGION_ALIGNMENT. It is impractical at this\n-  \/\/ point to deal with case when Shenandoah runs with smaller regions.\n-  \/\/ TODO: This check can be dropped once MIN_GC_REGION_ALIGNMENT agrees more with Shenandoah.\n-  if (ShenandoahHeapRegion::region_size_bytes() < ArchiveHeapWriter::MIN_GC_REGION_ALIGNMENT) {\n-    return nullptr;\n-  }\n-\n-  HeapWord* mem = allocate_memory(req);\n-  size_t start_idx = heap_region_index_containing(mem);\n-  size_t num_regions = ShenandoahHeapRegion::required_regions(size * HeapWordSize);\n-\n-  \/\/ Flip humongous -> regular.\n-  {\n-    ShenandoahHeapLocker locker(lock(), false);\n-    for (size_t c = start_idx; c < start_idx + num_regions; c++) {\n-      get_region(c)->make_regular_bypass();\n-    }\n-  }\n+  \/\/ CDS wants a raw continuous memory range to load a bunch of objects itself.\n+  \/\/ This is an unusual request, since all requested regions should be regular, not humongous.\n+  \/\/\n+  \/\/ CDS would guarantee no objects straddle multiple regions, as long as regions are as large\n+  \/\/ as MIN_GC_REGION_ALIGNMENT.\n+  guarantee(ShenandoahHeapRegion::region_size_bytes() >= AOTMappedHeapWriter::MIN_GC_REGION_ALIGNMENT, \"Must be\");\n@@ -2835,1 +2782,2 @@\n-  return mem;\n+  ShenandoahAllocRequest req = ShenandoahAllocRequest::for_cds(size);\n+  return allocate_memory(req);\n@@ -2862,11 +2810,19 @@\n-  \/\/ Region bounds are good.\n-  ShenandoahHeapRegion* begin_reg = heap_region_containing(start);\n-  ShenandoahHeapRegion* end_reg = heap_region_containing(end);\n-  assert(begin_reg->is_regular(), \"Must be\");\n-  assert(end_reg->is_regular(), \"Must be\");\n-  assert(begin_reg->bottom() == start,\n-         \"Must agree: archive-space-start: \" PTR_FORMAT \", begin-region-bottom: \" PTR_FORMAT,\n-         p2i(start), p2i(begin_reg->bottom()));\n-  assert(end_reg->top() == end,\n-         \"Must agree: archive-space-end: \" PTR_FORMAT \", end-region-top: \" PTR_FORMAT,\n-         p2i(end), p2i(end_reg->top()));\n+  \/\/ All regions in contiguous space have good state.\n+  size_t begin_reg_idx = heap_region_index_containing(start);\n+  size_t end_reg_idx   = heap_region_index_containing(end);\n+\n+  for (size_t idx = begin_reg_idx; idx <= end_reg_idx; idx++) {\n+    ShenandoahHeapRegion* r = get_region(idx);\n+    assert(r->is_regular(), \"Must be regular\");\n+    assert(r->is_young(), \"Must be young\");\n+    assert(idx == end_reg_idx || r->top() == r->end(),\n+           \"All regions except the last one should be full: \" PTR_FORMAT \" \" PTR_FORMAT,\n+           p2i(r->top()), p2i(r->end()));\n+    assert(idx != begin_reg_idx || r->bottom() == start,\n+           \"Archive space start should be at the bottom of first region: \" PTR_FORMAT \" \" PTR_FORMAT,\n+           p2i(r->bottom()), p2i(start));\n+    assert(idx != end_reg_idx || r->top() == end,\n+           \"Archive space end should be at the top of last region: \" PTR_FORMAT \" \" PTR_FORMAT,\n+           p2i(r->top()), p2i(end));\n+  }\n+\n","filename":"src\/hotspot\/share\/gc\/shenandoah\/shenandoahHeap.cpp","additions":209,"deletions":253,"binary":false,"changes":462,"status":"modified"},{"patch":"@@ -28,1 +28,0 @@\n-#include \"gc\/z\/zAllocator.inline.hpp\"\n@@ -36,0 +35,2 @@\n+#include \"gc\/z\/zNUMA.inline.hpp\"\n+#include \"gc\/z\/zObjectAllocator.hpp\"\n@@ -37,1 +38,1 @@\n-#include \"gc\/z\/zPageAge.hpp\"\n+#include \"gc\/z\/zPageAge.inline.hpp\"\n@@ -46,0 +47,1 @@\n+#include \"gc\/z\/zValue.inline.hpp\"\n@@ -49,1 +51,1 @@\n-#include \"runtime\/atomic.hpp\"\n+#include \"runtime\/atomicAccess.hpp\"\n@@ -65,1 +67,1 @@\n-  return Atomic::load(&_needs_attention) != 0;\n+  return AtomicAccess::load(&_needs_attention) != 0;\n@@ -69,1 +71,1 @@\n-  const int needs_attention = Atomic::add(&_needs_attention, 1);\n+  const int needs_attention = AtomicAccess::add(&_needs_attention, 1);\n@@ -74,1 +76,1 @@\n-  const int needs_attention = Atomic::sub(&_needs_attention, 1);\n+  const int needs_attention = AtomicAccess::sub(&_needs_attention, 1);\n@@ -84,1 +86,1 @@\n-  Atomic::store(&_is_active, false);\n+  AtomicAccess::store(&_is_active, false);\n@@ -89,1 +91,1 @@\n-  return Atomic::load(&_is_active);\n+  return AtomicAccess::load(&_is_active);\n@@ -307,0 +309,25 @@\n+ZRelocationTargets::ZRelocationTargets()\n+  : _targets() {}\n+\n+ZPage* ZRelocationTargets::get(uint32_t partition_id, ZPageAge age) {\n+  return _targets.get(partition_id)[untype(age) - 1];\n+}\n+\n+void ZRelocationTargets::set(uint32_t partition_id, ZPageAge age, ZPage* page) {\n+  _targets.get(partition_id)[untype(age) - 1] = page;\n+}\n+\n+template <typename Function>\n+void ZRelocationTargets::apply_and_clear_targets(Function function) {\n+  ZPerNUMAIterator<TargetArray> iter(&_targets);\n+  for (TargetArray* targets; iter.next(&targets);) {\n+    for (size_t i = 0; i < ZNumRelocationAges; i++) {\n+      \/\/ Apply function\n+      function((*targets)[i]);\n+\n+      \/\/ Clear target\n+      (*targets)[i] = nullptr;\n+    }\n+  }\n+}\n+\n@@ -309,1 +336,5 @@\n-    _queue() {}\n+    _queue(),\n+    _iters(),\n+    _small_targets(),\n+    _medium_targets(),\n+    _shared_medium_targets() {}\n@@ -329,0 +360,1 @@\n+  const ZPageAge to_age = forwarding->to_age();\n@@ -330,3 +362,1 @@\n-  ZAllocatorForRelocation* allocator = ZAllocator::relocation(forwarding->to_age());\n-\n-  const zaddress to_addr = allocator->alloc_object(size);\n+  const zaddress to_addr = ZHeap::heap()->alloc_object_for_relocation(size, to_age);\n@@ -349,1 +379,1 @@\n-    allocator->undo_alloc_object(to_addr, size);\n+    ZHeap::heap()->undo_alloc_object_for_relocation(to_addr, size);\n@@ -391,1 +421,1 @@\n-static ZPage* alloc_page(ZAllocatorForRelocation* allocator, ZPageType type, size_t size) {\n+static ZPage* alloc_page(ZForwarding* forwarding) {\n@@ -398,0 +428,5 @@\n+  const ZPageType type = forwarding->type();\n+  const size_t size = forwarding->size();\n+  const ZPageAge age = forwarding->to_age();\n+  const uint32_t preferred_partition = forwarding->partition_id();\n+\n@@ -402,1 +437,1 @@\n-  return allocator->alloc_page_for_relocation(type, size, flags);\n+  return ZHeap::heap()->alloc_page(type, size, flags, age, preferred_partition);\n@@ -432,2 +467,1 @@\n-    ZAllocatorForRelocation* const allocator = ZAllocator::relocation(forwarding->to_age());\n-    ZPage* const page = alloc_page(allocator, forwarding->type(), forwarding->size());\n+    ZPage* const page = alloc_page(forwarding);\n@@ -435,1 +469,1 @@\n-      Atomic::inc(&_in_place_count);\n+      AtomicAccess::inc(&_in_place_count);\n@@ -446,1 +480,1 @@\n-  void share_target_page(ZPage* page) {\n+  void share_target_page(ZPage* page, uint32_t partition_id) {\n@@ -471,5 +505,5 @@\n-  ZGeneration* const _generation;\n-  ZConditionLock     _lock;\n-  ZPage*             _shared[ZAllocator::_relocation_allocators];\n-  bool               _in_place;\n-  volatile size_t    _in_place_count;\n+  ZGeneration* const  _generation;\n+  ZConditionLock      _lock;\n+  ZRelocationTargets* _shared_targets;\n+  bool                _in_place;\n+  volatile size_t     _in_place_count;\n@@ -478,1 +512,1 @@\n-  ZRelocateMediumAllocator(ZGeneration* generation)\n+  ZRelocateMediumAllocator(ZGeneration* generation, ZRelocationTargets* shared_targets)\n@@ -481,1 +515,1 @@\n-      _shared(),\n+      _shared_targets(shared_targets),\n@@ -486,3 +520,3 @@\n-    for (uint i = 0; i < ZAllocator::_relocation_allocators; ++i) {\n-      if (_shared[i] != nullptr) {\n-        retire_target_page(_generation, _shared[i]);\n+    _shared_targets->apply_and_clear_targets([&](ZPage* page) {\n+      if (page != nullptr) {\n+        retire_target_page(_generation, page);\n@@ -490,9 +524,1 @@\n-    }\n-  }\n-\n-  ZPage* shared(ZPageAge age) {\n-    return _shared[untype(age - 1)];\n-  }\n-\n-  void set_shared(ZPageAge age, ZPage* page) {\n-    _shared[untype(age - 1)] = page;\n+    });\n@@ -514,4 +540,4 @@\n-    if (shared(to_age) == target) {\n-      ZAllocatorForRelocation* const allocator = ZAllocator::relocation(forwarding->to_age());\n-      ZPage* const to_page = alloc_page(allocator, forwarding->type(), forwarding->size());\n-      set_shared(to_age, to_page);\n+    const uint32_t partition_id = forwarding->partition_id();\n+    if (_shared_targets->get(partition_id, to_age) == target) {\n+      ZPage* const to_page = alloc_page(forwarding);\n+      _shared_targets->set(partition_id, to_age, to_page);\n@@ -519,1 +545,1 @@\n-        Atomic::inc(&_in_place_count);\n+        AtomicAccess::inc(&_in_place_count);\n@@ -529,1 +555,1 @@\n-    return shared(to_age);\n+    return _shared_targets->get(partition_id, to_age);\n@@ -532,1 +558,1 @@\n-  void share_target_page(ZPage* page) {\n+  void share_target_page(ZPage* page, uint32_t partition_id) {\n@@ -537,1 +563,1 @@\n-    assert(shared(age) == nullptr, \"Invalid state\");\n+    assert(_shared_targets->get(partition_id, age) == nullptr, \"Invalid state\");\n@@ -540,1 +566,1 @@\n-    set_shared(age, page);\n+    _shared_targets->set(partition_id, age, page);\n@@ -568,1 +594,1 @@\n-  ZPage*              _target[ZAllocator::_relocation_allocators];\n+  ZRelocationTargets* _targets;\n@@ -574,9 +600,0 @@\n-\n-  ZPage* target(ZPageAge age) {\n-    return _target[untype(age - 1)];\n-  }\n-\n-  void set_target(ZPageAge age, ZPage* page) {\n-    _target[untype(age - 1)] = page;\n-  }\n-\n@@ -596,1 +613,1 @@\n-  zaddress try_relocate_object_inner(zaddress from_addr, size_t old_size) {\n+  zaddress try_relocate_object_inner(zaddress from_addr, uint32_t partition_id, size_t old_size) {\n@@ -598,1 +615,2 @@\n-    ZPage* const to_page = target(_forwarding->to_age());\n+\n+    ZPage* const to_page = _targets->get(partition_id, _forwarding->to_age());\n@@ -737,1 +755,1 @@\n-    const zpointer ptr = Atomic::load(p);\n+    const zpointer ptr = AtomicAccess::load(p);\n@@ -818,1 +836,1 @@\n-  bool try_relocate_object(zaddress from_addr) {\n+  bool try_relocate_object(zaddress from_addr, uint32_t partition_id) {\n@@ -820,1 +838,1 @@\n-    const zaddress to_addr = try_relocate_object_inner(from_addr, size);\n+    const zaddress to_addr = try_relocate_object_inner(from_addr, partition_id, size);\n@@ -901,7 +919,12 @@\n-    while (!try_relocate_object(addr)) {\n-      \/\/ Allocate a new target page, or if that fails, use the page being\n-      \/\/ relocated as the new target, which will cause it to be relocated\n-      \/\/ in-place.\n-      const ZPageAge to_age = _forwarding->to_age();\n-      ZPage* to_page = _allocator->alloc_and_retire_target_page(_forwarding, target(to_age));\n-      set_target(to_age, to_page);\n+    const ZPageAge to_age = _forwarding->to_age();\n+    const uint32_t partition_id = _forwarding->partition_id();\n+\n+    while (!try_relocate_object(addr, partition_id)) {\n+      \/\/ Failed to relocate object, try to allocate a new target page,\n+      \/\/ or if that fails, use the page being relocated as the new target,\n+      \/\/ which will cause it to be relocated in-place.\n+      ZPage* const target_page = _targets->get(partition_id, to_age);\n+      ZPage* to_page = _allocator->alloc_and_retire_target_page(_forwarding, target_page);\n+      _targets->set(partition_id, to_age, to_page);\n+\n+      \/\/ We got a new page, retry relocation\n@@ -916,1 +939,1 @@\n-      set_target(to_age, to_page);\n+      _targets->set(partition_id, to_age, to_page);\n@@ -921,1 +944,1 @@\n-  ZRelocateWork(Allocator* allocator, ZGeneration* generation)\n+  ZRelocateWork(Allocator* allocator, ZRelocationTargets* targets, ZGeneration* generation)\n@@ -924,1 +947,1 @@\n-      _target(),\n+      _targets(targets),\n@@ -930,3 +953,4 @@\n-    for (uint i = 0; i < ZAllocator::_relocation_allocators; ++i) {\n-      _allocator->free_target_page(_target[i]);\n-    }\n+    _targets->apply_and_clear_targets([&](ZPage* page) {\n+        _allocator->free_target_page(page);\n+    });\n+\n@@ -1025,2 +1049,3 @@\n-      ZPage* const target_page = target(_forwarding->to_age());\n-      _allocator->share_target_page(target_page);\n+      const uint32_t target_partition = _forwarding->partition_id();\n+      ZPage* const target_page = _targets->get(target_partition, _forwarding->to_age());\n+      _allocator->share_target_page(target_page, target_partition);\n@@ -1070,5 +1095,9 @@\n-  ZRelocationSetParallelIterator _iter;\n-  ZGeneration* const             _generation;\n-  ZRelocateQueue* const          _queue;\n-  ZRelocateSmallAllocator        _small_allocator;\n-  ZRelocateMediumAllocator       _medium_allocator;\n+  ZGeneration* const                        _generation;\n+  ZRelocateQueue* const                     _queue;\n+  ZPerNUMA<ZRelocationSetParallelIterator>* _iters;\n+  ZPerWorker<ZRelocationTargets>*           _small_targets;\n+  ZPerWorker<ZRelocationTargets>*           _medium_targets;\n+  ZRelocateSmallAllocator                   _small_allocator;\n+  ZRelocateMediumAllocator                  _medium_allocator;\n+  const size_t                              _total_forwardings;\n+  volatile size_t                           _numa_local_forwardings;\n@@ -1077,1 +1106,6 @@\n-  ZRelocateTask(ZRelocationSet* relocation_set, ZRelocateQueue* queue)\n+  ZRelocateTask(ZRelocationSet* relocation_set,\n+                ZRelocateQueue* queue,\n+                ZPerNUMA<ZRelocationSetParallelIterator>* iters,\n+                ZPerWorker<ZRelocationTargets>* small_targets,\n+                ZPerWorker<ZRelocationTargets>* medium_targets,\n+                ZRelocationTargets* shared_medium_targets)\n@@ -1079,1 +1113,0 @@\n-      _iter(relocation_set),\n@@ -1082,0 +1115,3 @@\n+      _iters(iters),\n+      _small_targets(small_targets),\n+      _medium_targets(medium_targets),\n@@ -1083,1 +1119,15 @@\n-      _medium_allocator(_generation) {}\n+      _medium_allocator(_generation, shared_medium_targets),\n+      _total_forwardings(relocation_set->nforwardings()),\n+      _numa_local_forwardings(0) {\n+\n+    for (uint32_t i = 0; i < ZNUMA::count(); i++) {\n+      ZRelocationSetParallelIterator* const iter = _iters->addr(i);\n+\n+      \/\/ Destruct the iterator from the previous GC-cycle, which is a temporary\n+      \/\/ iterator if this is the first GC-cycle.\n+      iter->~ZRelocationSetParallelIterator();\n+\n+      \/\/ In-place construct the iterator with the current relocation set\n+      ::new (iter) ZRelocationSetParallelIterator(relocation_set);\n+    }\n+  }\n@@ -1090,0 +1140,5 @@\n+\n+    if (ZNUMA::is_enabled()) {\n+      log_debug(gc, reloc, numa)(\"Forwardings relocated NUMA-locally: %zu \/ %zu (%.0f%%)\",\n+                                 _numa_local_forwardings, _total_forwardings, percent_of(_numa_local_forwardings, _total_forwardings));\n+    }\n@@ -1093,2 +1148,4 @@\n-    ZRelocateWork<ZRelocateSmallAllocator> small(&_small_allocator, _generation);\n-    ZRelocateWork<ZRelocateMediumAllocator> medium(&_medium_allocator, _generation);\n+    ZRelocateWork<ZRelocateSmallAllocator> small(&_small_allocator, _small_targets->addr(), _generation);\n+    ZRelocateWork<ZRelocateMediumAllocator> medium(&_medium_allocator, _medium_targets->addr(), _generation);\n+    const uint32_t num_nodes = ZNUMA::count();\n+    uint32_t numa_local_forwardings_worker = 0;\n@@ -1120,0 +1177,4 @@\n+    const auto check_numa_local = [&](ZForwarding* forwarding, uint32_t numa_id) {\n+      return forwarding->partition_id() == numa_id;\n+    };\n+\n@@ -1122,0 +1183,14 @@\n+      const uint32_t start_node = ZNUMA::id();\n+      uint32_t current_node = start_node;\n+\n+      for (uint32_t i = 0; i < num_nodes; i++) {\n+        if (_iters->get(current_node).next_if(&forwarding, check_numa_local, current_node)) {\n+          claim_and_do_forwarding(forwarding);\n+\n+          if (current_node == start_node) {\n+            \/\/ Track if this forwarding was relocated on the local NUMA node\n+            numa_local_forwardings_worker++;\n+          }\n+\n+          return true;\n+        }\n@@ -1123,3 +1198,2 @@\n-      if (_iter.next(&forwarding)) {\n-        claim_and_do_forwarding(forwarding);\n-        return true;\n+        \/\/ Check next node.\n+        current_node = (current_node + 1) % num_nodes;\n@@ -1151,0 +1225,4 @@\n+    if (ZNUMA::is_enabled()) {\n+      AtomicAccess::add(&_numa_local_forwardings, numa_local_forwardings_worker, memory_order_relaxed);\n+    }\n+\n@@ -1160,1 +1238,1 @@\n-  const zpointer ptr = Atomic::load(p);\n+  const zpointer ptr = AtomicAccess::load(p);\n@@ -1231,1 +1309,1 @@\n-    ZRelocateTask relocate_task(relocation_set, &_queue);\n+    ZRelocateTask relocate_task(relocation_set, &_queue, &_iters, &_small_targets, &_medium_targets, &_shared_medium_targets);\n@@ -1260,1 +1338,1 @@\n-    : ZTask(\"ZPromotePagesTask\"),\n+    : ZTask(\"ZFlipAgePagesTask\"),\n@@ -1275,10 +1353,0 @@\n-      if (promotion) {\n-        \/\/ Before promoting an object (and before relocate start), we must ensure that all\n-        \/\/ contained zpointers are store good. The marking code ensures that for non-null\n-        \/\/ pointers, but null pointers are ignored. This code ensures that even null pointers\n-        \/\/ are made store good, for the promoted objects.\n-        prev_page->object_iterate([&](oop obj) {\n-          ZIterator::basic_oop_iterate_safe(obj, ZBarrier::promote_barrier_on_young_oop_field);\n-        });\n-      }\n-\n@@ -1298,1 +1366,1 @@\n-        \/\/ Defer promoted page registration times the lock is taken\n+        \/\/ Defer promoted page registration\n@@ -1309,0 +1377,26 @@\n+class ZPromoteBarrierTask : public ZTask {\n+private:\n+  ZArrayParallelIterator<ZPage*> _iter;\n+\n+public:\n+  ZPromoteBarrierTask(const ZArray<ZPage*>* pages)\n+    : ZTask(\"ZPromoteBarrierTask\"),\n+      _iter(pages) {}\n+\n+  virtual void work() {\n+    SuspendibleThreadSetJoiner sts_joiner;\n+\n+    for (ZPage* page; _iter.next(&page);) {\n+      \/\/ When promoting an object (and before relocate start), we must ensure that all\n+      \/\/ contained zpointers are store good. The marking code ensures that for non-null\n+      \/\/ pointers, but null pointers are ignored. This code ensures that even null pointers\n+      \/\/ are made store good, for the promoted objects.\n+      page->object_iterate([&](oop obj) {\n+        ZIterator::basic_oop_iterate_safe(obj, ZBarrier::promote_barrier_on_young_oop_field);\n+      });\n+\n+      SuspendibleThreadSet::yield();\n+    }\n+  }\n+};\n+\n@@ -1314,0 +1408,5 @@\n+void ZRelocate::barrier_flip_promoted_pages(const ZArray<ZPage*>* pages) {\n+  ZPromoteBarrierTask promote_barrier_task(pages);\n+  workers()->run(&promote_barrier_task);\n+}\n+\n","filename":"src\/hotspot\/share\/gc\/z\/zRelocate.cpp","additions":203,"deletions":104,"binary":false,"changes":307,"status":"modified"},{"patch":"@@ -25,0 +25,1 @@\n+#include \"jfr\/leakprofiler\/checkpoint\/eventEmitter.hpp\"\n@@ -26,0 +27,1 @@\n+#include \"jfr\/leakprofiler\/sampling\/objectSampler.hpp\"\n@@ -28,2 +30,0 @@\n-#include \"jfr\/leakprofiler\/checkpoint\/eventEmitter.hpp\"\n-#include \"jfr\/leakprofiler\/sampling\/objectSampler.hpp\"\n","filename":"src\/hotspot\/share\/jfr\/leakprofiler\/leakProfiler.cpp","additions":2,"deletions":2,"binary":false,"changes":4,"status":"modified"},{"patch":"@@ -59,1 +59,1 @@\n-#include \"runtime\/atomic.hpp\"\n+#include \"runtime\/atomicAccess.hpp\"\n@@ -1011,1 +1011,1 @@\n-  LinkResolver::resolve_field(fd, link_info, Bytecodes::java_code(code), false, CHECK_NULL);\n+  LinkResolver::resolve_field(fd, link_info, Bytecodes::java_code(code), ClassInitMode::dont_init, CHECK_NULL);\n@@ -2015,1 +2015,1 @@\n-    Klass* k = iklass->local_interfaces()->at(index);\n+    InstanceKlass* k = iklass->local_interfaces()->at(index);\n@@ -2280,1 +2280,1 @@\n-  JVMCIObjectArray array = JVMCIENV->new_FieldInfo_array(fields->length(), JVMCIENV);\n+  JVMCIObjectArray array = JVMCIENV->new_FieldInfo_array(fields->length(), JVMCI_CHECK_NULL);\n@@ -2987,0 +2987,1 @@\n+\/\/ Checks that `index` denotes a non-injected field in `klass`\n@@ -2993,1 +2994,8 @@\n-  if (index < 0 || index > iklass->total_fields_count()) {\n+  if (index < 0 || index >= iklass->java_fields_count()) {\n+    if (index >= 0 && index < iklass->total_fields_count()) {\n+      fieldDescriptor fd(iklass, index);\n+      if (fd.is_injected()) {\n+        JVMCI_THROW_MSG_NULL(IllegalArgumentException,\n+            err_msg(\"Cannot get Field for injected %s.%s\", klass->external_name(), fd.name()->as_C_string()));\n+      }\n+    }\n@@ -3003,1 +3011,1 @@\n-  InstanceKlass* iklass = check_field(klass, index, JVMCIENV);\n+  InstanceKlass* iklass = check_field(klass, index, JVMCI_CHECK_NULL);\n@@ -3011,1 +3019,1 @@\n-                                              JavaThread* THREAD, JVMCIEnv* JVMCIENV) {\n+                                              JavaThread* THREAD, JVMCI_TRAPS) {\n@@ -3089,1 +3097,1 @@\n-  InstanceKlass* holder = check_field(InstanceKlass::cast(UNPACK_PAIR(Klass, klass)), index, JVMCIENV);\n+  InstanceKlass* holder = check_field(InstanceKlass::cast(UNPACK_PAIR(Klass, klass)), index, JVMCI_CHECK_NULL);\n","filename":"src\/hotspot\/share\/jvmci\/jvmciCompilerToVM.cpp","additions":16,"deletions":8,"binary":false,"changes":24,"status":"modified"},{"patch":"@@ -54,1 +54,1 @@\n-#include \"utilities\/resourceHash.hpp\"\n+#include \"utilities\/hashTable.hpp\"\n@@ -147,0 +147,1 @@\n+address CompilerToVM::Data::dsinh;\n@@ -154,0 +155,2 @@\n+address CompilerToVM::Data::crc_table_addr;\n+\n@@ -290,0 +293,1 @@\n+  SET_TRIGFUNC_OR_NULL(dsinh);\n@@ -292,0 +296,1 @@\n+  SET_TRIGFUNC_OR_NULL(crc_table_addr);\n@@ -435,2 +440,2 @@\n-  ResourceHashtable<jlong, JVMCIObject> longs;\n-  ResourceHashtable<const char*, JVMCIObject,\n+  HashTable<jlong, JVMCIObject> longs;\n+  HashTable<const char*, JVMCIObject,\n","filename":"src\/hotspot\/share\/jvmci\/jvmciCompilerToVMInit.cpp","additions":8,"deletions":3,"binary":false,"changes":11,"status":"modified"},{"patch":"@@ -44,0 +44,1 @@\n+  LOG_TAG(asan) \\\n@@ -153,0 +154,1 @@\n+  LOG_TAG(package) \\\n","filename":"src\/hotspot\/share\/logging\/logTag.hpp","additions":2,"deletions":0,"binary":false,"changes":2,"status":"modified"},{"patch":"@@ -25,1 +25,1 @@\n-#include \"cds\/archiveHeapLoader.hpp\"\n+#include \"cds\/aotMetaspace.hpp\"\n@@ -28,2 +28,1 @@\n-#include \"cds\/heapShared.hpp\"\n-#include \"cds\/metaspaceShared.hpp\"\n+#include \"cds\/heapShared.inline.hpp\"\n@@ -32,0 +31,1 @@\n+#include \"classfile\/classLoaderDataShared.hpp\"\n@@ -72,1 +72,1 @@\n-#include \"runtime\/atomic.hpp\"\n+#include \"runtime\/atomicAccess.hpp\"\n@@ -83,0 +83,1 @@\n+#include \"services\/cpuTimeUsage.hpp\"\n@@ -88,0 +89,1 @@\n+#include \"utilities\/globalDefinitions.hpp\"\n@@ -237,0 +239,1 @@\n+static BuiltinException _preempted_exception;\n@@ -257,0 +260,1 @@\n+oop Universe::preempted_exception_instance()      { return _preempted_exception.instance(); }\n@@ -316,0 +320,1 @@\n+  _preempted_exception.store_in_cds();\n@@ -319,1 +324,1 @@\n-  if (ArchiveHeapLoader::is_in_use()) {\n+  if (HeapShared::is_archived_heap_in_use()) {\n@@ -335,0 +340,1 @@\n+    _preempted_exception.load_from_cds();\n@@ -354,0 +360,1 @@\n+  _preempted_exception.serialize(f);\n@@ -503,1 +510,1 @@\n-  \/\/ for Object_klass_loaded in objArrayKlassKlass::allocate_objArray_klass_impl.\n+  \/\/ for Object_klass_is_loaded in ObjArrayKlass::allocate_objArray_klass.\n@@ -552,11 +559,8 @@\n-    if (CDSConfig::is_using_archive() &&\n-        ArchiveHeapLoader::is_in_use() &&\n-        _basic_type_mirrors[T_INT].resolve() != nullptr) {\n-      assert(ArchiveHeapLoader::can_use(), \"Sanity\");\n-\n-      \/\/ check that all basic type mirrors are mapped also\n-      for (int i = T_BOOLEAN; i < T_VOID+1; i++) {\n-        if (!is_reference_type((BasicType)i)) {\n-          oop m = _basic_type_mirrors[i].resolve();\n-          assert(m != nullptr, \"archived mirrors should not be null\");\n-        }\n+  if (CDSConfig::is_using_archive() &&\n+      HeapShared::is_archived_heap_in_use() &&\n+      _basic_type_mirrors[T_INT].resolve() != nullptr) {\n+    \/\/ check that all basic type mirrors are mapped also\n+    for (int i = T_BOOLEAN; i < T_VOID+1; i++) {\n+      if (!is_reference_type((BasicType)i)) {\n+        oop m = _basic_type_mirrors[i].resolve();\n+        assert(m != nullptr, \"archived mirrors should not be null\");\n@@ -564,2 +568,3 @@\n-    } else\n-      \/\/ _basic_type_mirrors[T_INT], etc, are null if archived heap is not mapped.\n+    }\n+  } else\n+    \/\/ _basic_type_mirrors[T_INT], etc, are null if not using an archived heap\n@@ -567,8 +572,6 @@\n-    {\n-      for (int i = T_BOOLEAN; i < T_VOID+1; i++) {\n-        BasicType bt = (BasicType)i;\n-        if (!is_reference_type(bt)) {\n-          oop m = java_lang_Class::create_basic_type_mirror(type2name(bt), bt, false, CHECK);\n-          _basic_type_mirrors[i] = OopHandle(vm_global(), m);\n-        }\n-        CDS_JAVA_HEAP_ONLY(_archived_basic_type_mirror_indices[i] = -1);\n+  {\n+    for (int i = T_BOOLEAN; i < T_VOID+1; i++) {\n+      BasicType bt = (BasicType)i;\n+      if (!is_reference_type(bt)) {\n+        oop m = java_lang_Class::create_basic_type_mirror(type2name(bt), bt, false, CHECK);\n+        _basic_type_mirrors[i] = OopHandle(vm_global(), m);\n@@ -576,0 +579,1 @@\n+      CDS_JAVA_HEAP_ONLY(_archived_basic_type_mirror_indices[i] = -1);\n@@ -577,3 +581,4 @@\n-    if (CDSConfig::is_dumping_heap()) {\n-      HeapShared::init_scratch_objects_for_basic_type_mirrors(CHECK);\n-    }\n+  }\n+  if (CDSConfig::is_dumping_heap()) {\n+    HeapShared::init_scratch_objects_for_basic_type_mirrors(CHECK);\n+  }\n@@ -583,0 +588,5 @@\n+  if (CDSConfig::is_using_aot_linked_classes()) {\n+    \/\/ All mirrors of preloaded classes are already restored. No need to fix up.\n+    return;\n+  }\n+\n@@ -587,1 +597,1 @@\n-  assert(vmClasses::Class_klass_loaded(), \"java.lang.Class should be loaded\");\n+  assert(vmClasses::Class_klass_is_loaded(), \"java.lang.Class should be loaded\");\n@@ -738,1 +748,1 @@\n-    next = (int)Atomic::add(&_preallocated_out_of_memory_error_avail_count, -1);\n+    next = (int)AtomicAccess::add(&_preallocated_out_of_memory_error_avail_count, -1);\n@@ -878,1 +888,1 @@\n-  MetaspaceShared::adjust_heap_sizes_for_dumping();\n+  AOTMetaspace::adjust_heap_sizes_for_dumping();\n@@ -900,1 +910,14 @@\n-  ClassLoaderData::init_null_class_loader_data();\n+  \/\/ Add main_thread to threads list to finish barrier setup with\n+  \/\/ on_thread_attach.  Should be before starting to build Java objects in\n+  \/\/ the AOT heap loader, which invokes barriers.\n+  {\n+    JavaThread* main_thread = JavaThread::current();\n+    MutexLocker mu(Threads_lock);\n+    Threads::add(main_thread);\n+  }\n+\n+  HeapShared::initialize_writing_mode();\n+\n+  \/\/ Create the string table before the AOT object archive is loaded,\n+  \/\/ as it might need to access the string table.\n+  StringTable::create_table();\n@@ -906,1 +929,10 @@\n-    MetaspaceShared::initialize_shared_spaces();\n+    AOTMetaspace::initialize_shared_spaces();\n+  }\n+#endif\n+\n+  ClassLoaderData::init_null_class_loader_data();\n+\n+#if INCLUDE_CDS\n+#if INCLUDE_CDS_JAVA_HEAP\n+  if (CDSConfig::is_using_full_module_graph()) {\n+    ClassLoaderDataShared::restore_archived_entries_for_null_class_loader_data();\n@@ -908,0 +940,1 @@\n+#endif \/\/ INCLUDE_CDS_JAVA_HEAP\n@@ -914,1 +947,0 @@\n-  StringTable::create_table();\n@@ -941,1 +973,1 @@\n-ReservedHeapSpace Universe::reserve_heap(size_t heap_size, size_t alignment) {\n+ReservedHeapSpace Universe::reserve_heap(size_t heap_size, size_t alignment, size_t desired_page_size) {\n@@ -946,0 +978,1 @@\n+  assert(is_aligned(heap_size, alignment), \"precondition\");\n@@ -947,1 +980,1 @@\n-  size_t total_reserved = align_up(heap_size, alignment);\n+  size_t total_reserved = heap_size;\n@@ -951,3 +984,7 @@\n-  size_t page_size = os::vm_page_size();\n-  if (UseLargePages && is_aligned(alignment, os::large_page_size())) {\n-    page_size = os::large_page_size();\n+  size_t page_size;\n+  if (desired_page_size == 0) {\n+    if (UseLargePages) {\n+      page_size = os::large_page_size();\n+    } else {\n+      page_size = os::vm_page_size();\n+    }\n@@ -957,1 +994,3 @@\n-    assert(!UseLargePages || UseParallelGC , \"Wrong alignment to use large pages\");\n+    assert(UseParallelGC , \"only Parallel\");\n+    \/\/ Use caller provided value.\n+    page_size = desired_page_size;\n@@ -959,1 +998,1 @@\n-\n+  assert(is_aligned(heap_size, page_size), \"inv\");\n@@ -1118,0 +1157,1 @@\n+  _preempted_exception.init_if_empty(vmSymbols::jdk_internal_vm_PreemptedException(), CHECK_false);\n@@ -1151,0 +1191,1 @@\n+\n@@ -1152,1 +1193,1 @@\n-  MetaspaceShared::post_initialize(CHECK_false);\n+  AOTMetaspace::post_initialize(CHECK_false);\n@@ -1293,0 +1334,63 @@\n+static void log_cpu_time() {\n+  LogTarget(Info, cpu) cpuLog;\n+  if (!cpuLog.is_enabled()) {\n+    return;\n+  }\n+\n+  const double process_cpu_time = os::elapsed_process_cpu_time();\n+  if (process_cpu_time == 0 || process_cpu_time == -1) {\n+    \/\/ 0 can happen e.g. for short running processes with\n+    \/\/ low CPU utilization\n+    return;\n+  }\n+\n+  const double gc_threads_cpu_time = (double) CPUTimeUsage::GC::gc_threads() \/ NANOSECS_PER_SEC;\n+  const double gc_vm_thread_cpu_time = (double) CPUTimeUsage::GC::vm_thread() \/ NANOSECS_PER_SEC;\n+  const double gc_string_dedup_cpu_time = (double) CPUTimeUsage::GC::stringdedup() \/ NANOSECS_PER_SEC;\n+  const double gc_cpu_time = (double) gc_threads_cpu_time + gc_vm_thread_cpu_time + gc_string_dedup_cpu_time;\n+\n+  const double elasped_time = os::elapsedTime();\n+  const bool has_error = CPUTimeUsage::Error::has_error();\n+\n+  if (gc_cpu_time < process_cpu_time) {\n+    cpuLog.print(\"=== CPU time Statistics =============================================================\");\n+    if (has_error) {\n+      cpuLog.print(\"WARNING: CPU time sampling reported errors, numbers may be unreliable\");\n+    }\n+    cpuLog.print(\"                                                                            CPUs\");\n+    cpuLog.print(\"                                                               s       %%  utilized\");\n+    cpuLog.print(\"   Process\");\n+    cpuLog.print(\"     Total                        %30.4f  %6.2f  %8.1f\", process_cpu_time, 100.0, process_cpu_time \/ elasped_time);\n+    cpuLog.print(\"     Garbage Collection           %30.4f  %6.2f  %8.1f\", gc_cpu_time, percent_of(gc_cpu_time, process_cpu_time), gc_cpu_time \/ elasped_time);\n+    cpuLog.print(\"       GC Threads                 %30.4f  %6.2f  %8.1f\", gc_threads_cpu_time, percent_of(gc_threads_cpu_time, process_cpu_time), gc_threads_cpu_time \/ elasped_time);\n+    cpuLog.print(\"       VM Thread                  %30.4f  %6.2f  %8.1f\", gc_vm_thread_cpu_time, percent_of(gc_vm_thread_cpu_time, process_cpu_time), gc_vm_thread_cpu_time \/ elasped_time);\n+\n+    if (UseStringDeduplication) {\n+      cpuLog.print(\"       String Deduplication       %30.4f  %6.2f  %8.1f\", gc_string_dedup_cpu_time, percent_of(gc_string_dedup_cpu_time, process_cpu_time), gc_string_dedup_cpu_time \/ elasped_time);\n+    }\n+    cpuLog.print(\"=====================================================================================\");\n+  }\n+}\n+\n+void Universe::before_exit() {\n+  \/\/ Tell the GC that it is time to shutdown and to block requests for new GC pauses.\n+  heap()->initiate_shutdown();\n+\n+  \/\/ Log CPU time statistics before stopping the GC threads.\n+  log_cpu_time();\n+\n+  \/\/ Stop the GC threads.\n+  heap()->stop();\n+\n+  \/\/ Print GC\/heap related information.\n+  Log(gc, exit) log;\n+  if (log.is_info()) {\n+    LogStream ls_info(log.info());\n+    Universe::print_on(&ls_info);\n+    if (log.is_trace()) {\n+      LogStream ls_trace(log.trace());\n+      MutexLocker mcld(ClassLoaderDataGraph_lock);\n+      ClassLoaderDataGraph::print_on(&ls_trace);\n+    }\n+  }\n+}\n","filename":"src\/hotspot\/share\/memory\/universe.cpp","additions":148,"deletions":44,"binary":false,"changes":192,"status":"modified"},{"patch":"@@ -25,0 +25,1 @@\n+#include \"cds\/aotMetaspace.hpp\"\n@@ -26,1 +27,0 @@\n-#include \"cds\/metaspaceShared.hpp\"\n@@ -44,4 +44,0 @@\n-void* ArrayKlass::operator new(size_t size, ClassLoaderData* loader_data, size_t word_size, TRAPS) throw() {\n-  return Metaspace::allocate(loader_data, word_size, MetaspaceObj::ClassType, true, THREAD);\n-}\n-\n@@ -190,10 +186,0 @@\n-objArrayOop ArrayKlass::allocate_arrayArray(int n, int length, TRAPS) {\n-  check_array_allocation_length(length, arrayOopDesc::max_array_length(T_ARRAY), CHECK_NULL);\n-  size_t size = objArrayOopDesc::object_size(length);\n-  ArrayKlass* ak = array_klass(n + dimension(), CHECK_NULL);\n-  objArrayOop o = (objArrayOop)Universe::heap()->array_allocate(ak, size, length,\n-                                                                \/* do_zero *\/ true, CHECK_NULL);\n-  \/\/ initialization to null not necessary, area already cleared\n-  return o;\n-}\n-\n@@ -262,1 +248,1 @@\n-    if (MetaspaceShared::is_shared_dynamic((void*)k)) {\n+    if (AOTMetaspace::in_aot_cache_dynamic_region((void*)k)) {\n@@ -264,1 +250,1 @@\n-    } else if (MetaspaceShared::is_shared_static((void*)k)) {\n+    } else if (AOTMetaspace::in_aot_cache_static_region((void*)k)) {\n","filename":"src\/hotspot\/share\/oops\/arrayKlass.cpp","additions":3,"deletions":17,"binary":false,"changes":20,"status":"modified"},{"patch":"@@ -2,1 +2,1 @@\n- * Copyright (c) 1997, 2024, Oracle and\/or its affiliates. All rights reserved.\n+ * Copyright (c) 1997, 2025, Oracle and\/or its affiliates. All rights reserved.\n@@ -52,2 +52,0 @@\n-  void* operator new(size_t size, ClassLoaderData* loader_data, size_t word_size, TRAPS) throw();\n-\n@@ -90,1 +88,0 @@\n-  objArrayOop allocate_arrayArray(int n, int length, TRAPS);\n","filename":"src\/hotspot\/share\/oops\/arrayKlass.hpp","additions":1,"deletions":4,"binary":false,"changes":5,"status":"modified"},{"patch":"@@ -47,2 +47,0 @@\n-#ifdef _LP64\n-\n@@ -70,0 +68,1 @@\n+#ifdef _LP64\n@@ -72,0 +71,4 @@\n+#else\n+    _narrow_klass_pointer_bits = 32;\n+    _max_shift = 0;\n+#endif\n@@ -79,4 +82,4 @@\n-  os::snprintf(tmp, sizeof(tmp), \"klass range: \" RANGE2FMT \",\"\n-      \" base \" PTR_FORMAT \", shift %d, lowest\/highest valid narrowKlass %u\/%u\",\n-      RANGE2FMTARGS(_klass_range_start, _klass_range_end),\n-      p2i(_base), _shift, _lowest_valid_narrow_klass_id, _highest_valid_narrow_klass_id);\n+  os::snprintf_checked(tmp, sizeof(tmp), \"klass range: \" RANGE2FMT \",\"\n+                       \" base \" PTR_FORMAT \", shift %d, lowest\/highest valid narrowKlass %u\/%u\",\n+                       RANGE2FMTARGS(_klass_range_start, _klass_range_end),\n+                       p2i(_base), _shift, _lowest_valid_narrow_klass_id, _highest_valid_narrow_klass_id);\n@@ -98,0 +101,4 @@\n+  \/\/ We should need a class space if address space is larger than what narrowKlass can address\n+  const bool should_need_class_space = (BytesPerWord * BitsPerByte) > narrow_klass_pointer_bits();\n+  ASSERT_HERE(should_need_class_space == needs_class_space());\n+\n@@ -110,1 +117,3 @@\n-  const address encoding_end = (address)(p2u(_base) + (uintptr_t)nth_bit(narrow_klass_pointer_bits() + _shift));\n+  const address encoding_end = (address)\n+      LP64_ONLY((p2u(_base) + (uintptr_t)nth_bit(narrow_klass_pointer_bits() + _shift)))\n+      NOT_LP64(max_klass_range_size());\n@@ -155,1 +164,2 @@\n-  \/\/ is disallowed. Note that both Metaspace and CDS prvent allocation at the first address for this reason.\n+  \/\/ is disallowed. If the encoding range starts at the klass range start, both Metaspace and CDS establish an\n+  \/\/ mprotected zone for this reason (see establish_protection_zone).\n@@ -257,0 +267,1 @@\n+#ifdef _LP64\n@@ -268,0 +279,1 @@\n+\n@@ -288,0 +300,8 @@\n+#else\n+    \/\/ 32-bit \"compressed class pointer\" mode\n+    _base = nullptr;\n+    _shift = 0;\n+    \/\/ as our \"protection zone\", we just assume the lowest protected parts of\n+    \/\/ the user address space.\n+    _protection_zone_size = os::vm_min_address();\n+#endif \/\/ LP64\n@@ -292,1 +312,1 @@\n-  \/\/ Initialize klass decode mode and check compability with decode instructions\n+  \/\/ Initialize JIT-specific decoding settings\n@@ -306,3 +326,2 @@\n-#ifdef ASSERT\n-  sanity_check_after_initialization();\n-#endif\n+\n+  DEBUG_ONLY(sanity_check_after_initialization();)\n@@ -359,1 +378,0 @@\n-#endif \/\/ _LP64\n","filename":"src\/hotspot\/share\/oops\/compressedKlass.cpp","additions":31,"deletions":13,"binary":false,"changes":44,"status":"modified"},{"patch":"@@ -127,1 +127,1 @@\n-  \/\/ Start and end of the Klass Range.\n+  \/\/ Start and end of the Klass Range. Start includes the protection zone if one exists.\n@@ -146,0 +146,1 @@\n+\n@@ -199,0 +200,6 @@\n+  \/\/ On 64-bit, we need the class space to confine Klass structures to the encoding range, which is determined\n+  \/\/ by bit size of narrowKlass IDs and the shift. On 32-bit, we support compressed class pointer only\n+  \/\/ \"pro-forma\": narrowKlass have the same size as addresses (32 bits), and therefore the encoding range is\n+  \/\/ equal to the address space size. Here, we don't need a class space.\n+  static constexpr bool needs_class_space() { return LP64_ONLY(true) NOT_LP64(false); }\n+\n@@ -207,0 +214,1 @@\n+  \/\/ Note: CDS with +UCCP for 32-bit currently unsupported.\n@@ -237,2 +245,2 @@\n-  static bool is_null(Klass* v)      { return v == nullptr; }\n-  static bool is_null(narrowKlass v) { return v == 0; }\n+  static bool is_null(const Klass* v)  { return v == nullptr; }\n+  static bool is_null(narrowKlass v)   { return v == 0; }\n@@ -246,3 +254,3 @@\n-  static inline narrowKlass encode_not_null_without_asserts(Klass* k, address narrow_base, int shift);\n-  static inline narrowKlass encode_not_null(Klass* v);\n-  static inline narrowKlass encode(Klass* v);\n+  static inline narrowKlass encode_not_null_without_asserts(const Klass* k, address narrow_base, int shift);\n+  static inline narrowKlass encode_not_null(const Klass* v);\n+  static inline narrowKlass encode(const Klass* v);\n@@ -257,0 +265,3 @@\n+  \/\/ Given a narrow Klass ID, returns true if it appears to be valid\n+  inline static bool is_valid_narrow_klass_id(narrowKlass nk);\n+\n","filename":"src\/hotspot\/share\/oops\/compressedKlass.hpp","additions":17,"deletions":6,"binary":false,"changes":23,"status":"modified"},{"patch":"@@ -26,0 +26,1 @@\n+#include \"cds\/aotMetaspace.hpp\"\n@@ -31,1 +32,0 @@\n-#include \"cds\/metaspaceShared.hpp\"\n@@ -53,1 +53,1 @@\n-#include \"logging\/log.hpp\"\n+#include \"logging\/log.hpp\"\n@@ -64,1 +64,1 @@\n-#include \"oops\/fieldStreams.inline.hpp\"\n+#include \"oops\/fieldStreams.inline.hpp\"\n@@ -81,0 +81,1 @@\n+#include \"runtime\/atomicAccess.hpp\"\n@@ -82,1 +83,0 @@\n-#include \"runtime\/atomic.hpp\"\n@@ -162,1 +162,1 @@\n-  if (vmClasses::ClassLoader_klass_loaded()) {\n+  if (vmClasses::ClassLoader_klass_is_loaded()) {\n@@ -458,5 +458,0 @@\n-void* InstanceKlass::operator new(size_t size, ClassLoaderData* loader_data, size_t word_size,\n-                                  bool use_class_space, TRAPS) throw() {\n-  return Metaspace::allocate(loader_data, word_size, ClassType, use_class_space, THREAD);\n-}\n-\n@@ -475,1 +470,0 @@\n-  const bool use_class_space = parser.klass_needs_narrow_id();\n@@ -480,1 +474,1 @@\n-    ik = new (loader_data, size, use_class_space, THREAD) InstanceRefKlass(parser);\n+    ik = new (loader_data, size, THREAD) InstanceRefKlass(parser);\n@@ -483,1 +477,1 @@\n-    ik = new (loader_data, size, use_class_space, THREAD) InstanceMirrorKlass(parser);\n+    ik = new (loader_data, size, THREAD) InstanceMirrorKlass(parser);\n@@ -486,1 +480,1 @@\n-    ik = new (loader_data, size, use_class_space, THREAD) InstanceStackChunkKlass(parser);\n+    ik = new (loader_data, size, THREAD) InstanceStackChunkKlass(parser);\n@@ -489,1 +483,1 @@\n-    ik = new (loader_data, size, use_class_space, THREAD) InstanceClassLoaderKlass(parser);\n+    ik = new (loader_data, size, THREAD) InstanceClassLoaderKlass(parser);\n@@ -492,1 +486,1 @@\n-    ik = new (loader_data, size, use_class_space, THREAD) InstanceKlass(parser);\n+    ik = new (loader_data, size, THREAD) InstanceKlass(parser);\n@@ -495,1 +489,1 @@\n-  if (ik != nullptr && UseCompressedClassPointers && use_class_space) {\n+  if (ik != nullptr && UseCompressedClassPointers) {\n@@ -565,1 +559,1 @@\n-      !methods->is_shared()) {\n+      !methods->in_aot_cache()) {\n@@ -579,1 +573,1 @@\n-                                          const Klass* super_klass,\n+                                          const InstanceKlass* super_klass,\n@@ -588,2 +582,2 @@\n-                    InstanceKlass::cast(super_klass)->transitive_interfaces();\n-    if (ti != sti && ti != nullptr && !ti->is_shared()) {\n+                    super_klass->transitive_interfaces();\n+    if (ti != sti && ti != nullptr && !ti->in_aot_cache()) {\n@@ -596,1 +590,1 @@\n-      local_interfaces != nullptr && !local_interfaces->is_shared()) {\n+      local_interfaces != nullptr && !local_interfaces->in_aot_cache()) {\n@@ -603,1 +597,1 @@\n-  if (record_components != nullptr && !record_components->is_shared()) {\n+  if (record_components != nullptr && !record_components->in_aot_cache()) {\n@@ -647,1 +641,1 @@\n-      !method_ordering()->is_shared()) {\n+      !method_ordering()->in_aot_cache()) {\n@@ -655,1 +649,1 @@\n-      !default_methods()->is_shared()) {\n+      !default_methods()->in_aot_cache()) {\n@@ -663,1 +657,1 @@\n-      !default_vtable_indices()->is_shared()) {\n+      !default_vtable_indices()->in_aot_cache()) {\n@@ -676,1 +670,1 @@\n-      !secondary_supers()->is_shared()) {\n+      !secondary_supers()->in_aot_cache()) {\n@@ -685,1 +679,1 @@\n-  if (fieldinfo_stream() != nullptr && !fieldinfo_stream()->is_shared()) {\n+  if (fieldinfo_stream() != nullptr && !fieldinfo_stream()->in_aot_cache()) {\n@@ -690,1 +684,1 @@\n-  if (fieldinfo_search_table() != nullptr && !fieldinfo_search_table()->is_shared()) {\n+  if (fieldinfo_search_table() != nullptr && !fieldinfo_search_table()->in_aot_cache()) {\n@@ -695,1 +689,1 @@\n-  if (fields_status() != nullptr && !fields_status()->is_shared()) {\n+  if (fields_status() != nullptr && !fields_status()->in_aot_cache()) {\n@@ -704,1 +698,1 @@\n-    if (!constants()->is_shared()) {\n+    if (!constants()->in_aot_cache()) {\n@@ -715,1 +709,1 @@\n-      !inner_classes()->is_shared()) {\n+      !inner_classes()->in_aot_cache()) {\n@@ -722,1 +716,1 @@\n-      !nest_members()->is_shared()) {\n+      !nest_members()->in_aot_cache()) {\n@@ -729,1 +723,1 @@\n-      !permitted_subclasses()->is_shared()) {\n+      !permitted_subclasses()->in_aot_cache()) {\n@@ -735,1 +729,1 @@\n-  if (annotations() != nullptr && !annotations()->is_shared()) {\n+  if (annotations() != nullptr && !annotations()->in_aot_cache()) {\n@@ -752,1 +746,1 @@\n-         java_super() == vmClasses::Record_klass();\n+         super() == vmClasses::Record_klass();\n@@ -767,1 +761,1 @@\n-  InstanceKlass* s = java_super();\n+  InstanceKlass* s = super();\n@@ -769,1 +763,1 @@\n-          (s != nullptr && s->java_super() == vmClasses::Enum_klass()));\n+          (s != nullptr && s->super() == vmClasses::Enum_klass()));\n@@ -803,1 +797,1 @@\n-\/\/ Set the initialization lock to null so the object can be GC'ed.  Any racing\n+\/\/ Set the initialization lock to null so the object can be GC'ed. Any racing\n@@ -806,1 +800,2 @@\n-\/\/ the lock and return.\n+\/\/ the lock and return. For preempted vthreads we keep the oop protected\n+\/\/ in the ObjectMonitor (see ObjectMonitor::set_object_strong()).\n@@ -814,0 +809,25 @@\n+class PreemptableInitCall {\n+  JavaThread* _thread;\n+  bool _previous;\n+  DEBUG_ONLY(InstanceKlass* _previous_klass;)\n+ public:\n+  PreemptableInitCall(JavaThread* thread, InstanceKlass* ik) : _thread(thread) {\n+    _previous = thread->at_preemptable_init();\n+    _thread->set_at_preemptable_init(true);\n+    DEBUG_ONLY(_previous_klass = _thread->preempt_init_klass();)\n+    DEBUG_ONLY(_thread->set_preempt_init_klass(ik));\n+  }\n+  ~PreemptableInitCall() {\n+    _thread->set_at_preemptable_init(_previous);\n+    DEBUG_ONLY(_thread->set_preempt_init_klass(_previous_klass));\n+  }\n+};\n+\n+void InstanceKlass::initialize_preemptable(TRAPS) {\n+  if (this->should_be_initialized()) {\n+    PreemptableInitCall pic(THREAD, this);\n+    initialize_impl(THREAD);\n+  } else {\n+    assert(is_initialized(), \"sanity check\");\n+  }\n+}\n@@ -833,1 +853,1 @@\n-  InstanceKlass* s = java_super();\n+  InstanceKlass* s = super();\n@@ -946,1 +966,1 @@\n-  Klass* super_klass = super();\n+  InstanceKlass* super_klass = super();\n@@ -961,2 +981,1 @@\n-    InstanceKlass* ik_super = InstanceKlass::cast(super_klass);\n-    ik_super->link_class_impl(CHECK_false);\n+    super_klass->link_class_impl(CHECK_false);\n@@ -991,1 +1010,6 @@\n-    ObjectLocker ol(h_init_lock, jt);\n+    ObjectLocker ol(h_init_lock, CHECK_PREEMPTABLE_false);\n+    \/\/ Don't allow preemption if we link\/initialize classes below,\n+    \/\/ since that would release this monitor while we are in the\n+    \/\/ middle of linking this class.\n+    NoPreemptMark npm(THREAD);\n+\n@@ -999,1 +1023,1 @@\n-        if (is_shared()) {\n+        if (in_aot_cache()) {\n@@ -1018,1 +1042,1 @@\n-      } else if (is_shared()) {\n+      } else if (in_aot_cache()) {\n@@ -1036,1 +1060,1 @@\n-      if (is_shared() && verified_at_dump_time() &&\n+      if (in_aot_cache() && verified_at_dump_time() &&\n@@ -1078,1 +1102,1 @@\n-    assert(is_shared(), \"rewriting an unshared class?\");\n+    assert(in_aot_cache(), \"rewriting an unshared class?\");\n@@ -1120,1 +1144,1 @@\n-using InitializationErrorTable = ResourceHashtable<const InstanceKlass*, OopHandle, 107, AnyObj::C_HEAP, mtClass>;\n+using InitializationErrorTable = HashTable<const InstanceKlass*, OopHandle, 107, AnyObj::C_HEAP, mtClass>;\n@@ -1185,0 +1209,11 @@\n+class ThreadWaitingForClassInit : public StackObj {\n+  JavaThread* _thread;\n+ public:\n+  ThreadWaitingForClassInit(JavaThread* thread, InstanceKlass* ik) : _thread(thread) {\n+    _thread->set_class_to_be_initialized(ik);\n+  }\n+  ~ThreadWaitingForClassInit() {\n+    _thread->set_class_to_be_initialized(nullptr);\n+  }\n+};\n+\n@@ -1204,1 +1239,1 @@\n-    ObjectLocker ol(h_init_lock, jt);\n+    ObjectLocker ol(h_init_lock, CHECK_PREEMPTABLE);\n@@ -1217,3 +1252,2 @@\n-      jt->set_class_to_be_initialized(this);\n-      ol.wait_uninterruptibly(jt);\n-      jt->set_class_to_be_initialized(nullptr);\n+      ThreadWaitingForClassInit twcl(THREAD, this);\n+      ol.wait_uninterruptibly(CHECK_PREEMPTABLE);\n@@ -1277,0 +1311,4 @@\n+  \/\/ Block preemption once we are the initializer thread. Unmounting now\n+  \/\/ would complicate the reentrant case (identity is platform thread).\n+  NoPreemptMark npm(THREAD);\n+\n@@ -1418,1 +1456,1 @@\n-    InstanceKlass* ikls = Atomic::load_acquire(ik);\n+    InstanceKlass* ikls = AtomicAccess::load_acquire(ik);\n@@ -1434,1 +1472,1 @@\n-    Atomic::release_store(addr, ik);\n+    AtomicAccess::release_store(addr, ik);\n@@ -1470,1 +1508,1 @@\n-  InstanceKlass* super_ik = ik->java_super();\n+  InstanceKlass* super_ik = ik->super();\n@@ -1566,9 +1604,0 @@\n-objArrayOop InstanceKlass::allocate_objArray(int n, int length, TRAPS) {\n-  check_array_allocation_length(length, arrayOopDesc::max_array_length(T_OBJECT), CHECK_NULL);\n-  size_t size = objArrayOopDesc::object_size(length);\n-  ArrayKlass* ak = array_klass(n, CHECK_NULL);\n-  objArrayOop o = (objArrayOop)Universe::heap()->array_allocate(ak, size, length,\n-                                                                \/* do_zero *\/ true, CHECK_NULL);\n-  return o;\n-}\n-\n@@ -1690,1 +1719,1 @@\n-    assert(is_shared(), \"must be\");\n+    assert(in_aot_cache(), \"must be\");\n@@ -1764,1 +1793,1 @@\n-  OopMapCache* oop_map_cache = Atomic::load_acquire(&_oop_map_cache);\n+  OopMapCache* oop_map_cache = AtomicAccess::load_acquire(&_oop_map_cache);\n@@ -1768,1 +1797,1 @@\n-    OopMapCache* other = Atomic::cmpxchg(&_oop_map_cache, (OopMapCache*)nullptr, oop_map_cache);\n+    OopMapCache* other = AtomicAccess::cmpxchg(&_oop_map_cache, (OopMapCache*)nullptr, oop_map_cache);\n@@ -1809,1 +1838,1 @@\n-    Klass* intf1 = local_interfaces()->at(i);\n+    InstanceKlass* intf1 = local_interfaces()->at(i);\n@@ -1812,1 +1841,1 @@\n-    if (InstanceKlass::cast(intf1)->find_local_field(name, sig, fd)) {\n+    if (intf1->find_local_field(name, sig, fd)) {\n@@ -1817,1 +1846,1 @@\n-    Klass* intf2 = InstanceKlass::cast(intf1)->find_interface_field(name, sig, fd);\n+    Klass* intf2 = intf1->find_interface_field(name, sig, fd);\n@@ -1836,2 +1865,2 @@\n-  { Klass* supr = super();\n-    if (supr != nullptr) return InstanceKlass::cast(supr)->find_field(name, sig, fd);\n+  { InstanceKlass* supr = super();\n+    if (supr != nullptr) return supr->find_field(name, sig, fd);\n@@ -1856,2 +1885,2 @@\n-  { Klass* supr = super();\n-    if (supr != nullptr) return InstanceKlass::cast(supr)->find_field(name, sig, is_static, fd);\n+  { InstanceKlass* supr = super();\n+    if (supr != nullptr) return supr->find_field(name, sig, is_static, fd);\n@@ -1876,1 +1905,1 @@\n-  Klass* klass = const_cast<InstanceKlass*>(this);\n+  const InstanceKlass* klass = this;\n@@ -1878,1 +1907,1 @@\n-    if (InstanceKlass::cast(klass)->find_local_field_from_offset(offset, is_static, fd)) {\n+    if (klass->find_local_field_from_offset(offset, is_static, fd)) {\n@@ -1924,1 +1953,1 @@\n-  InstanceKlass* super = superklass();\n+  InstanceKlass* super = this->super();\n@@ -1941,1 +1970,1 @@\n-  InstanceKlass* super = superklass();\n+  InstanceKlass* super = this->super();\n@@ -2236,1 +2265,1 @@\n-  const Klass* klass = this;\n+  const InstanceKlass* klass = this;\n@@ -2238,5 +2267,5 @@\n-    Method* const method = InstanceKlass::cast(klass)->find_method_impl(name,\n-                                                                        signature,\n-                                                                        overpass_local_mode,\n-                                                                        StaticLookupMode::find,\n-                                                                        private_mode);\n+    Method* const method = klass->find_method_impl(name,\n+                                                   signature,\n+                                                   overpass_local_mode,\n+                                                   StaticLookupMode::find,\n+                                                   private_mode);\n@@ -2256,1 +2285,1 @@\n-  const Klass* klass = this;\n+  const InstanceKlass* klass = this;\n@@ -2258,1 +2287,1 @@\n-    if (InstanceKlass::cast(klass)->has_been_redefined()) {\n+    if (klass->has_been_redefined()) {\n@@ -2335,1 +2364,1 @@\n-    if (ik->is_shared()) buf[i++] = 'S';\n+    if (ik->in_aot_cache()) buf[i++] = 'S';\n@@ -2395,1 +2424,1 @@\n-  Atomic::release_store(&jmeths[idnum + 1], new_id);\n+  AtomicAccess::release_store(&jmeths[idnum + 1], new_id);\n@@ -2410,1 +2439,1 @@\n-  return Atomic::load_acquire(&_methods_jmethod_ids);\n+  return AtomicAccess::load_acquire(&_methods_jmethod_ids);\n@@ -2414,1 +2443,1 @@\n-  Atomic::release_store(&_methods_jmethod_ids, jmeths);\n+  AtomicAccess::release_store(&_methods_jmethod_ids, jmeths);\n@@ -2453,1 +2482,1 @@\n-  jmethodID id = Atomic::load_acquire(&jmeths[idnum + 1]);\n+  jmethodID id = AtomicAccess::load_acquire(&jmeths[idnum + 1]);\n@@ -2502,1 +2531,1 @@\n-    jmethodID id = Atomic::load_acquire(&jmeths[idnum + 1]);\n+    jmethodID id = AtomicAccess::load_acquire(&jmeths[idnum + 1]);\n@@ -2506,1 +2535,1 @@\n-      Atomic::release_store(&jmeths[idnum + 1], id);\n+      AtomicAccess::release_store(&jmeths[idnum + 1], id);\n@@ -2559,1 +2588,1 @@\n-      InstanceKlass* impl = Atomic::load_acquire(iklass);\n+      InstanceKlass* impl = AtomicAccess::load_acquire(iklass);\n@@ -2562,1 +2591,1 @@\n-        if (Atomic::cmpxchg(iklass, impl, (InstanceKlass*)nullptr) == impl) {\n+        if (AtomicAccess::cmpxchg(iklass, impl, (InstanceKlass*)nullptr) == impl) {\n@@ -2768,1 +2797,1 @@\n-             MetaspaceShared::is_in_shared_metaspace(_package_entry)) {\n+             AOTMetaspace::in_aot_cache(_package_entry)) {\n@@ -2844,6 +2873,1 @@\n-\/\/ Check if a class or any of its supertypes has a version older than 50.\n-\/\/ CDS will not perform verification of old classes during dump time because\n-\/\/ without changing the old verifier, the verification constraint cannot be\n-\/\/ retrieved during dump time.\n-\/\/ Verification of archived old classes will be performed during run time.\n-  if (MetaspaceShared::is_in_shared_metaspace(this)) {\n+  if (CDSConfig::is_dumping_dynamic_archive() && AOTMetaspace::in_aot_cache(this)) {\n@@ -2855,1 +2879,8 @@\n-  if (major_version() < 50 \/*JAVA_6_VERSION*\/) {\n+\n+  if (CDSConfig::is_preserving_verification_constraints()) {\n+    return true;\n+  }\n+\n+  if (CDSConfig::is_old_class_for_verifier(this)) {\n+    \/\/ The old verifier does not save verification constraints, so at run time\n+    \/\/ SystemDictionaryShared::check_verification_constraints() will not work for this class.\n@@ -2858,1 +2889,1 @@\n-  if (java_super() != nullptr && !java_super()->can_be_verified_at_dumptime()) {\n+  if (super() != nullptr && !super()->can_be_verified_at_dumptime()) {\n@@ -3086,1 +3117,1 @@\n-  if (!is_shared()) {\n+  if (!in_aot_cache()) {\n@@ -3090,1 +3121,1 @@\n-  if (is_shared() && _package_entry != nullptr) {\n+  if (in_aot_cache() && _package_entry != nullptr) {\n@@ -3093,1 +3124,1 @@\n-      assert(MetaspaceShared::is_in_shared_metaspace(_package_entry), \"must be\");\n+      assert(AOTMetaspace::in_aot_cache(_package_entry), \"must be\");\n@@ -3975,2 +4006,2 @@\n-    assert(this->is_shared(), \"must be\");\n-    if (MetaspaceShared::is_shared_dynamic((void*)this)) {\n+    assert(this->in_aot_cache(), \"must be\");\n+    if (AOTMetaspace::in_aot_cache_dynamic_region((void*)this)) {\n@@ -3990,1 +4021,1 @@\n-                       p2i(this),  p2i(superklass()));\n+                       p2i(this),  p2i(super()));\n@@ -3998,1 +4029,1 @@\n-                           p2i(InstanceKlass::cast(local_interfaces()->at(i))));\n+                           p2i(local_interfaces()->at(i)));\n@@ -4211,1 +4242,0 @@\n-\n@@ -4216,1 +4246,1 @@\n-JNIid::JNIid(Klass* holder, int offset, JNIid* next) {\n+JNIid::JNIid(InstanceKlass* holder, int offset, JNIid* next) {\n@@ -4223,1 +4253,0 @@\n-\n@@ -4241,2 +4270,1 @@\n-\n-void JNIid::verify(Klass* holder) {\n+void JNIid::verify(InstanceKlass* holder) {\n@@ -4245,1 +4273,1 @@\n-  end_field_offset = first_field_offset + (InstanceKlass::cast(holder)->static_field_size() * wordSize);\n+  end_field_offset = first_field_offset + (holder->static_field_size() * wordSize);\n@@ -4262,1 +4290,1 @@\n-  bool good_state = is_shared() ? (_init_state <= state)\n+  bool good_state = in_aot_cache() ? (_init_state <= state)\n@@ -4267,1 +4295,1 @@\n-  Atomic::release_store(&_init_state, state);\n+  AtomicAccess::release_store(&_init_state, state);\n@@ -4363,1 +4391,1 @@\n-      if (pvcp->is_shared()) {\n+      if (pvcp->in_aot_cache()) {\n@@ -4475,1 +4503,1 @@\n-  if (cp_ref->is_shared()) {\n+  if (cp_ref->in_aot_cache()) {\n@@ -4558,1 +4586,1 @@\n-    _current = _current->superklass(); \/\/ backtrack; no more sibling subclasses left\n+    _current = _current->java_super(); \/\/ backtrack; no more sibling subclasses left\n","filename":"src\/hotspot\/share\/oops\/instanceKlass.cpp","additions":152,"deletions":124,"binary":false,"changes":276,"status":"modified"},{"patch":"@@ -146,2 +146,0 @@\n-  void* operator new(size_t size, ClassLoaderData* loader_data, size_t word_size, bool use_class_space, TRAPS) throw();\n-\n@@ -511,1 +509,1 @@\n-  JavaThread* init_thread()  { return Atomic::load(&_init_thread); }\n+  JavaThread* init_thread()  { return AtomicAccess::load(&_init_thread); }\n@@ -525,1 +523,1 @@\n-  ClassState  init_state() const           { return Atomic::load_acquire(&_init_state); }\n+  ClassState  init_state() const           { return AtomicAccess::load_acquire(&_init_state); }\n@@ -545,0 +543,1 @@\n+  void initialize_preemptable(TRAPS);\n@@ -817,1 +816,0 @@\n-  objArrayOop allocate_objArray(int n, int length, TRAPS);\n@@ -915,0 +913,6 @@\n+  \/\/ This hides Klass::super(). The _super of an InstanceKlass is\n+  \/\/ always an InstanceKlass (or nullptr)\n+  InstanceKlass* super() const {\n+    return (Klass::super() == nullptr) ? nullptr : InstanceKlass::cast(Klass::super());\n+  }\n+\n@@ -916,1 +920,1 @@\n-    return (super() == nullptr) ? nullptr : cast(super());\n+    return InstanceKlass::super();\n@@ -994,1 +998,1 @@\n-                                    const Klass* super_klass,\n+                                    const InstanceKlass* super_klass,\n@@ -1070,1 +1074,1 @@\n-    Atomic::store(&_init_thread, thread);\n+    AtomicAccess::store(&_init_thread, thread);\n@@ -1211,1 +1215,1 @@\n-  Klass*             _holder;\n+  InstanceKlass*     _holder;\n@@ -1220,1 +1224,1 @@\n-  Klass* holder() const           { return _holder; }\n+  InstanceKlass* holder() const   { return _holder; }\n@@ -1224,1 +1228,1 @@\n-  JNIid(Klass* holder, int offset, JNIid* next);\n+  JNIid(InstanceKlass* holder, int offset, JNIid* next);\n@@ -1238,1 +1242,1 @@\n-  void verify(Klass* holder);\n+  void verify(InstanceKlass* holder);\n","filename":"src\/hotspot\/share\/oops\/instanceKlass.hpp","additions":16,"deletions":12,"binary":false,"changes":28,"status":"modified"},{"patch":"@@ -44,1 +44,0 @@\n-  friend class VMStructs;\n@@ -55,0 +54,3 @@\n+  template <class OopClosureType>\n+  inline void do_metadata(oop obj, OopClosureType* closure);\n+\n","filename":"src\/hotspot\/share\/oops\/instanceMirrorKlass.hpp","additions":3,"deletions":1,"binary":false,"changes":4,"status":"modified"},{"patch":"@@ -167,4 +167,0 @@\n-  if (LockingMode != LM_LIGHTWEIGHT) {\n-    return;\n-  }\n-\n@@ -202,1 +198,2 @@\n-  const RegisterMap* get_map(const SmallRegisterMap* map, intptr_t* sp) { return map->copy_to_RegisterMap(&_map, sp); }\n+  template <typename SmallRegisterMapT>\n+  const RegisterMap* get_map(const SmallRegisterMapT map, intptr_t* sp) { return map->copy_to_RegisterMap(&_map, sp); }\n","filename":"src\/hotspot\/share\/oops\/instanceStackChunkKlass.cpp","additions":2,"deletions":5,"binary":false,"changes":7,"status":"modified"},{"patch":"@@ -103,1 +103,0 @@\n-  friend class VMStructs;\n","filename":"src\/hotspot\/share\/oops\/instanceStackChunkKlass.hpp","additions":0,"deletions":1,"binary":false,"changes":1,"status":"modified"},{"patch":"@@ -25,2 +25,1 @@\n-#include \"cds\/archiveHeapLoader.hpp\"\n-#include \"cds\/heapShared.hpp\"\n+#include \"cds\/heapShared.inline.hpp\"\n@@ -53,1 +52,1 @@\n-#include \"runtime\/atomic.hpp\"\n+#include \"runtime\/atomicAccess.hpp\"\n@@ -256,0 +255,4 @@\n+void Klass::initialize_preemptable(TRAPS) {\n+  ShouldNotReachHere();\n+}\n+\n@@ -283,6 +286,3 @@\n-    \/\/ We therfore seed the mark word with the narrow Klass ID.\n-    \/\/ Note that only those Klass that can be instantiated have a narrow Klass ID.\n-    \/\/ For those who don't, we leave the klass bits empty and assert if someone\n-    \/\/ tries to use those.\n-    const narrowKlass nk = CompressedKlassPointers::is_encodable(kls) ?\n-        CompressedKlassPointers::encode(const_cast<Klass*>(kls)) : 0;\n+    \/\/ We therefore seed the mark word with the narrow Klass ID.\n+    precond(CompressedKlassPointers::is_encodable(kls));\n+    const narrowKlass nk = CompressedKlassPointers::encode(const_cast<Klass*>(kls));\n@@ -295,0 +295,4 @@\n+void* Klass::operator new(size_t size, ClassLoaderData* loader_data, size_t word_size, TRAPS) throw() {\n+  return Metaspace::allocate(loader_data, word_size, MetaspaceObj::ClassType, THREAD);\n+}\n+\n@@ -306,1 +310,1 @@\n-  CDS_ONLY(_shared_class_flags = 0;)\n+  CDS_ONLY(_aot_class_flags = 0;)\n@@ -615,6 +619,0 @@\n-\/\/ superklass links\n-InstanceKlass* Klass::superklass() const {\n-  assert(super() == nullptr || super()->is_instance_klass(), \"must be instance klass\");\n-  return _super == nullptr ? nullptr : InstanceKlass::cast(_super);\n-}\n-\n@@ -623,2 +621,1 @@\n-\/\/ The log parameter is for clean_weak_klass_links to report unlinked classes.\n-Klass* Klass::subklass(bool log) const {\n+Klass* Klass::subklass() const {\n@@ -627,1 +624,1 @@\n-  for (Klass* chain = Atomic::load_acquire(&_subklass);\n+  for (Klass* chain = AtomicAccess::load_acquire(&_subklass);\n@@ -631,1 +628,1 @@\n-       chain = Atomic::load(&chain->_next_sibling))\n+       chain = AtomicAccess::load(&chain->_next_sibling))\n@@ -635,5 +632,0 @@\n-    } else if (log) {\n-      if (log_is_enabled(Trace, class, unload)) {\n-        ResourceMark rm;\n-        log_trace(class, unload)(\"unlinking class (subclass): %s\", chain->external_name());\n-      }\n@@ -648,1 +640,1 @@\n-  for (Klass* chain = Atomic::load(&_next_sibling);\n+  for (Klass* chain = AtomicAccess::load(&_next_sibling);\n@@ -650,1 +642,1 @@\n-       chain = Atomic::load(&chain->_next_sibling)) {\n+       chain = AtomicAccess::load(&chain->_next_sibling)) {\n@@ -667,1 +659,1 @@\n-  Atomic::release_store(&_subklass, s);\n+  AtomicAccess::release_store(&_subklass, s);\n@@ -675,1 +667,1 @@\n-  Atomic::store(&_next_sibling, s);\n+  AtomicAccess::store(&_next_sibling, s);\n@@ -683,2 +675,2 @@\n-  \/\/ add ourselves to superklass' subklass list\n-  InstanceKlass* super = superklass();\n+  \/\/ add ourselves to super' subklass list\n+  InstanceKlass* super = java_super();\n@@ -687,1 +679,1 @@\n-          && (super->superklass() == nullptr || !is_interface())),\n+          && (super->java_super() == nullptr || !is_interface())),\n@@ -694,1 +686,1 @@\n-    Klass* prev_first_subklass = Atomic::load_acquire(&_super->_subklass);\n+    Klass* prev_first_subklass = AtomicAccess::load_acquire(&_super->_subklass);\n@@ -696,1 +688,1 @@\n-      \/\/ set our sibling to be the superklass' previous first subklass\n+      \/\/ set our sibling to be the super' previous first subklass\n@@ -703,1 +695,1 @@\n-    if (Atomic::cmpxchg(&super->_subklass, prev_first_subklass, this) == prev_first_subklass) {\n+    if (AtomicAccess::cmpxchg(&super->_subklass, prev_first_subklass, this) == prev_first_subklass) {\n@@ -710,1 +702,2 @@\n-void Klass::clean_subklass() {\n+\/\/ The log parameter is for clean_weak_klass_links to report unlinked classes.\n+Klass* Klass::clean_subklass(bool log) {\n@@ -713,1 +706,1 @@\n-    Klass* subklass = Atomic::load_acquire(&_subklass);\n+    Klass* subklass = AtomicAccess::load_acquire(&_subklass);\n@@ -715,1 +708,5 @@\n-      return;\n+      return subklass;\n+    }\n+    if (log && log_is_enabled(Trace, class, unload)) {\n+      ResourceMark rm;\n+      log_trace(class, unload)(\"unlinking class (subclass): %s\", subklass->external_name());\n@@ -718,1 +715,1 @@\n-    Atomic::cmpxchg(&_subklass, subklass, subklass->next_sibling());\n+    AtomicAccess::cmpxchg(&_subklass, subklass, subklass->next_sibling(log));\n@@ -737,2 +734,1 @@\n-    Klass* sub = current->subklass(true);\n-    current->clean_subklass();\n+    Klass* sub = current->clean_subklass(true);\n@@ -815,1 +811,1 @@\n-  set_is_shared();\n+  set_in_aot_cache();\n@@ -868,1 +864,1 @@\n-  assert(is_shared(), \"must be set\");\n+  assert(in_aot_cache(), \"must be set\");\n@@ -883,4 +879,3 @@\n-\n-    \/\/ Add to class loader list first before creating the mirror\n-    \/\/ (same order as class file parsing)\n-    loader_data->add_class(this);\n+  \/\/ Add to class loader list first before creating the mirror\n+  \/\/ (same order as class file parsing)\n+  loader_data->add_class(this);\n@@ -908,1 +903,1 @@\n-    if (ArchiveHeapLoader::is_in_use()) {\n+    if (HeapShared::is_archived_heap_in_use()) {\n@@ -1070,1 +1065,1 @@\n-  if (UseCompressedClassPointers && needs_narrow_id()) {\n+  if (UseCompressedClassPointers) {\n","filename":"src\/hotspot\/share\/oops\/klass.cpp","additions":44,"deletions":49,"binary":false,"changes":93,"status":"modified"},{"patch":"@@ -177,3 +177,3 @@\n-  u2     _shared_class_flags;\n-  enum CDSSharedClassFlags {\n-    _is_shared_class                       = 1 << 0,  \/\/ shadows MetaspaceObj::is_shared\n+  u2 _aot_class_flags;\n+  enum  {\n+    _in_aot_cache                          = 1 << 0,\n@@ -184,8 +184,7 @@\n-    \/\/ This class was not loaded from a classfile in the module image\n-    \/\/ or classpath.\n-    _is_generated_shared_class             = 1 << 5,\n-    \/\/ archived mirror already initialized by AOT-cache assembly: no further need to call <clinit>\n-    _has_aot_initialized_mirror            = 1 << 6,\n-    \/\/ If this class has been aot-inititalized, do we need to call its runtimeSetup()\n-    \/\/ method during the production run?\n-    _is_runtime_setup_required             = 1 << 7,\n+    _is_aot_generated_class                = 1 << 5, \/\/ this class was not loaded from a classfile in the module image\n+                                                     \/\/ or classpath, but was generated during AOT cache assembly.\n+    _has_aot_initialized_mirror            = 1 << 6, \/\/ archived mirror already initialized by AOT cache assembly.\n+                                                     \/\/ no further need to call <clinit>\n+    _has_aot_safe_initializer              = 1 << 7, \/\/ has @AOTSafeClassInitializer annotation\n+    _is_runtime_setup_required             = 1 << 8, \/\/ has a runtimeSetup method to be called when\n+                                                     \/\/ this class is loaded from AOT cache\n@@ -211,0 +210,2 @@\n+  void* operator new(size_t size, ClassLoaderData* loader_data, size_t word_size, TRAPS) throw();\n+\n@@ -223,1 +224,3 @@\n-  \/\/ If this is not what your code expects, you're probably looking for Klass::java_super().\n+  \/\/ If this is not what your code expects, you're probably looking for:\n+  \/\/ - Klass::java_super() - if you have a Klass*\n+  \/\/ - InstanceKlass::super() - if you have an InstanceKlass* ik, ik->super() returns InstanceKlass*.\n@@ -300,1 +303,1 @@\n-  Klass* subklass(bool log = false) const;\n+  Klass* subklass() const;\n@@ -303,1 +306,0 @@\n-  InstanceKlass* superklass() const;\n@@ -330,1 +332,1 @@\n-    CDS_ONLY(_shared_class_flags |= _archived_lambda_proxy_is_available;)\n+    CDS_ONLY(_aot_class_flags |= _archived_lambda_proxy_is_available;)\n@@ -333,1 +335,1 @@\n-    CDS_ONLY(_shared_class_flags &= (u2)(~_archived_lambda_proxy_is_available);)\n+    CDS_ONLY(_aot_class_flags &= (u2)(~_archived_lambda_proxy_is_available);)\n@@ -336,1 +338,1 @@\n-    CDS_ONLY(return (_shared_class_flags & _archived_lambda_proxy_is_available) != 0;)\n+    CDS_ONLY(return (_aot_class_flags & _archived_lambda_proxy_is_available) != 0;)\n@@ -341,1 +343,1 @@\n-    CDS_ONLY(_shared_class_flags |= _has_value_based_class_annotation;)\n+    CDS_ONLY(_aot_class_flags |= _has_value_based_class_annotation;)\n@@ -344,1 +346,1 @@\n-    CDS_ONLY(_shared_class_flags &= (u2)(~_has_value_based_class_annotation);)\n+    CDS_ONLY(_aot_class_flags &= (u2)(~_has_value_based_class_annotation);)\n@@ -347,1 +349,1 @@\n-    CDS_ONLY(return (_shared_class_flags & _has_value_based_class_annotation) != 0;)\n+    CDS_ONLY(return (_aot_class_flags & _has_value_based_class_annotation) != 0;)\n@@ -352,1 +354,1 @@\n-    CDS_ONLY(_shared_class_flags |= _verified_at_dump_time;)\n+    CDS_ONLY(_aot_class_flags |= _verified_at_dump_time;)\n@@ -355,1 +357,1 @@\n-    CDS_ONLY(return (_shared_class_flags & _verified_at_dump_time) != 0;)\n+    CDS_ONLY(return (_aot_class_flags & _verified_at_dump_time) != 0;)\n@@ -360,1 +362,1 @@\n-    CDS_ONLY(_shared_class_flags |= _has_archived_enum_objs;)\n+    CDS_ONLY(_aot_class_flags |= _has_archived_enum_objs;)\n@@ -363,1 +365,1 @@\n-    CDS_ONLY(return (_shared_class_flags & _has_archived_enum_objs) != 0;)\n+    CDS_ONLY(return (_aot_class_flags & _has_archived_enum_objs) != 0;)\n@@ -367,2 +369,2 @@\n-  void set_is_generated_shared_class() {\n-    CDS_ONLY(_shared_class_flags |= _is_generated_shared_class;)\n+  void set_is_aot_generated_class() {\n+    CDS_ONLY(_aot_class_flags |= _is_aot_generated_class;)\n@@ -370,2 +372,2 @@\n-  bool is_generated_shared_class() const {\n-    CDS_ONLY(return (_shared_class_flags & _is_generated_shared_class) != 0;)\n+  bool is_aot_generated_class() const {\n+    CDS_ONLY(return (_aot_class_flags & _is_aot_generated_class) != 0;)\n@@ -376,1 +378,1 @@\n-    CDS_ONLY(_shared_class_flags |= _has_aot_initialized_mirror;)\n+    CDS_ONLY(_aot_class_flags |= _has_aot_initialized_mirror;)\n@@ -379,1 +381,10 @@\n-    CDS_ONLY(return (_shared_class_flags & _has_aot_initialized_mirror) != 0;)\n+    CDS_ONLY(return (_aot_class_flags & _has_aot_initialized_mirror) != 0;)\n+    NOT_CDS(return false;)\n+  }\n+\n+  \/\/ Indicates presence of @AOTSafeClassInitializer. Also see AOTClassInitializer for more details.\n+  void set_has_aot_safe_initializer() {\n+    CDS_ONLY(_aot_class_flags |= _has_aot_safe_initializer;)\n+  }\n+  bool has_aot_safe_initializer() const {\n+    CDS_ONLY(return (_aot_class_flags & _has_aot_safe_initializer) != 0;)\n@@ -383,0 +394,1 @@\n+  \/\/ Indicates @AOTRuntimeSetup private static void runtimeSetup() presence.\n@@ -384,2 +396,1 @@\n-    assert(has_aot_initialized_mirror(), \"sanity\");\n-    CDS_ONLY(_shared_class_flags |= _is_runtime_setup_required;)\n+    CDS_ONLY(_aot_class_flags |= _is_runtime_setup_required;)\n@@ -388,1 +399,1 @@\n-    CDS_ONLY(return (_shared_class_flags & _is_runtime_setup_required) != 0;)\n+    CDS_ONLY(return (_aot_class_flags & _is_runtime_setup_required) != 0;)\n@@ -392,2 +403,2 @@\n-  bool is_shared() const                { \/\/ shadows MetaspaceObj::is_shared)()\n-    CDS_ONLY(return (_shared_class_flags & _is_shared_class) != 0;)\n+  bool in_aot_cache() const                { \/\/ shadows MetaspaceObj::in_aot_cache)()\n+    CDS_ONLY(return (_aot_class_flags & _in_aot_cache) != 0;)\n@@ -397,2 +408,2 @@\n-  void set_is_shared() {\n-    CDS_ONLY(_shared_class_flags |= _is_shared_class;)\n+  void set_in_aot_cache() {\n+    CDS_ONLY(_aot_class_flags |= _in_aot_cache;)\n@@ -405,0 +416,1 @@\n+  void     set_next_sibling(Klass* s);\n@@ -407,1 +419,0 @@\n-  void     set_next_sibling(Klass* s);\n@@ -572,0 +583,1 @@\n+  virtual void initialize_preemptable(TRAPS);\n@@ -624,1 +636,1 @@\n-    assert(is_shared(), \"use this for shared classes only\");\n+    assert(in_aot_cache(), \"use this for shared classes only\");\n@@ -736,1 +748,1 @@\n-  void clean_subklass();\n+  Klass* clean_subklass(bool log = false);\n@@ -790,3 +802,0 @@\n-  \/\/ Returns true if this Klass needs to be addressable via narrow Klass ID.\n-  inline bool needs_narrow_id() const;\n-\n@@ -794,0 +803,1 @@\n+\n","filename":"src\/hotspot\/share\/oops\/klass.hpp","additions":53,"deletions":43,"binary":false,"changes":96,"status":"modified"},{"patch":"@@ -2,1 +2,1 @@\n- * Copyright (c) 2005, 2024, Oracle and\/or its affiliates. All rights reserved.\n+ * Copyright (c) 2005, 2025, Oracle and\/or its affiliates. All rights reserved.\n@@ -178,10 +178,0 @@\n-\/\/ Returns true if this Klass needs to be addressable via narrow Klass ID.\n-inline bool Klass::needs_narrow_id() const {\n-  \/\/ Classes that are never instantiated need no narrow Klass Id, since the\n-  \/\/ only point of having a narrow id is to put it into an object header. Keeping\n-  \/\/ never instantiated classes out of class space lessens the class space pressure.\n-  \/\/ For more details, see JDK-8338526.\n-  \/\/ Note: don't call this function before access flags are initialized.\n-  return !is_abstract() && !is_interface();\n-}\n-\n","filename":"src\/hotspot\/share\/oops\/klass.inline.hpp","additions":1,"deletions":11,"binary":false,"changes":12,"status":"modified"},{"patch":"@@ -40,12 +40,4 @@\n-  if (has_monitor()) {\n-    \/\/ Has an inflated monitor. Must be checked before has_locker().\n-    ObjectMonitor* monitor = this->monitor();\n-    return monitor->header();\n-  }\n-  if (has_locker()) {  \/\/ has a stack lock\n-    BasicLock* locker = this->locker();\n-    return locker->displaced_header();\n-  }\n-  \/\/ This should never happen:\n-  fatal(\"bad header=\" INTPTR_FORMAT, value());\n-  return markWord(value());\n+  \/\/ Make sure we have an inflated monitor.\n+  guarantee(has_monitor(), \"bad header=\" INTPTR_FORMAT, value());\n+  ObjectMonitor* monitor = this->monitor();\n+  return monitor->header();\n@@ -56,13 +48,4 @@\n-  if (has_monitor()) {\n-    \/\/ Has an inflated monitor. Must be checked before has_locker().\n-    ObjectMonitor* monitor = this->monitor();\n-    monitor->set_header(m);\n-    return;\n-  }\n-  if (has_locker()) {  \/\/ has a stack lock\n-    BasicLock* locker = this->locker();\n-    locker->set_displaced_header(m);\n-    return;\n-  }\n-  \/\/ This should never happen:\n-  fatal(\"bad header=\" INTPTR_FORMAT, value());\n+  \/\/ Make sure we have an inflated monitor.\n+  guarantee(has_monitor(), \"bad header=\" INTPTR_FORMAT, value());\n+  ObjectMonitor* monitor = this->monitor();\n+  monitor->set_header(m);\n","filename":"src\/hotspot\/share\/oops\/markWord.cpp","additions":8,"deletions":25,"binary":false,"changes":33,"status":"modified"},{"patch":"@@ -2,1 +2,1 @@\n- * Copyright (c) 1997, 2024, Oracle and\/or its affiliates. All rights reserved.\n+ * Copyright (c) 1997, 2025, Oracle and\/or its affiliates. All rights reserved.\n@@ -28,0 +28,1 @@\n+#include \"cppstdlib\/type_traits.hpp\"\n@@ -33,2 +34,0 @@\n-#include <type_traits>\n-\n@@ -242,8 +241,0 @@\n-  bool has_locker() const {\n-    assert(LockingMode == LM_LEGACY, \"should only be called with legacy stack locking\");\n-    return (value() & lock_mask_in_place) == locked_value;\n-  }\n-  BasicLock* locker() const {\n-    assert(has_locker(), \"check\");\n-    return (BasicLock*) value();\n-  }\n@@ -252,1 +243,0 @@\n-    assert(LockingMode == LM_LIGHTWEIGHT, \"should only be called with new lightweight locking\");\n@@ -265,1 +255,1 @@\n-    assert(!UseObjectMonitorTable, \"Lightweight locking with OM table does not use markWord for monitors\");\n+    assert(!UseObjectMonitorTable, \"Locking with OM table does not use markWord for monitors\");\n@@ -271,5 +261,1 @@\n-    if (LockingMode == LM_LIGHTWEIGHT) {\n-      return !UseObjectMonitorTable && lockbits == monitor_value;\n-    }\n-    \/\/ monitor (0b10) | stack-locked (0b00)?\n-    return (lockbits & unlocked_value) == 0;\n+    return !UseObjectMonitorTable && lockbits == monitor_value;\n@@ -296,1 +282,1 @@\n-    assert(!UseObjectMonitorTable, \"Lightweight locking with OM table does not use markWord for monitors\");\n+    assert(!UseObjectMonitorTable, \"Locking with OM table does not use markWord for monitors\");\n@@ -367,0 +353,4 @@\n+  inline markWord set_not_hashed_not_expanded() const {\n+    assert(UseCompactObjectHeaders, \"only with compact i-hash\");\n+    return markWord(value() & ~(hashctrl_mask_in_place | hashctrl_expanded_mask_in_place));\n+  }\n@@ -414,1 +404,0 @@\n-    NOT_LP64(assert(LockingMode != LM_LEGACY, \"incorrect with LM_LEGACY on 32 bit\");)\n@@ -420,1 +409,0 @@\n-    NOT_LP64(assert(LockingMode != LM_LEGACY, \"incorrect with LM_LEGACY on 32 bit\");)\n@@ -425,1 +413,0 @@\n-    NOT_LP64(assert(LockingMode != LM_LEGACY, \"incorrect with LM_LEGACY on 32 bit\");)\n","filename":"src\/hotspot\/share\/oops\/markWord.hpp","additions":9,"deletions":22,"binary":false,"changes":31,"status":"modified"},{"patch":"@@ -28,1 +28,0 @@\n-#include \"oops\/compressedOops.inline.hpp\"\n@@ -31,0 +30,2 @@\n+#include \"oops\/compressedOops.inline.hpp\"\n+\n","filename":"src\/hotspot\/share\/oops\/markWord.inline.hpp","additions":2,"deletions":1,"binary":false,"changes":3,"status":"modified"},{"patch":"@@ -56,0 +56,20 @@\n+Symbol* ObjArrayKlass::create_element_klass_array_name(JavaThread* current, Klass* element_klass) {\n+  ResourceMark rm(current);\n+  char* name_str = element_klass->name()->as_C_string();\n+  int len = element_klass->name()->utf8_length();\n+  char* new_str = NEW_RESOURCE_ARRAY_IN_THREAD(current, char, len + 4);\n+  int idx = 0;\n+  new_str[idx++] = JVM_SIGNATURE_ARRAY;\n+  if (element_klass->is_instance_klass()) { \/\/ it could be an array or simple type\n+    new_str[idx++] = JVM_SIGNATURE_CLASS;\n+  }\n+  memcpy(&new_str[idx], name_str, len * sizeof(char));\n+  idx += len;\n+  if (element_klass->is_instance_klass()) {\n+    new_str[idx++] = JVM_SIGNATURE_ENDCLASS;\n+  }\n+  new_str[idx] = '\\0';\n+  return SymbolTable::new_symbol(new_str);\n+}\n+\n+\n@@ -61,1 +81,1 @@\n-  if (!Universe::is_bootstrapping() || vmClasses::Object_klass_loaded()) {\n+  if (!Universe::is_bootstrapping() || vmClasses::Object_klass_is_loaded()) {\n@@ -82,19 +102,1 @@\n-  Symbol* name = nullptr;\n-  {\n-    ResourceMark rm(THREAD);\n-    char *name_str = element_klass->name()->as_C_string();\n-    int len = element_klass->name()->utf8_length();\n-    char *new_str = NEW_RESOURCE_ARRAY(char, len + 4);\n-    int idx = 0;\n-    new_str[idx++] = JVM_SIGNATURE_ARRAY;\n-    if (element_klass->is_instance_klass()) { \/\/ it could be an array or simple type\n-      new_str[idx++] = JVM_SIGNATURE_CLASS;\n-    }\n-    memcpy(&new_str[idx], name_str, len * sizeof(char));\n-    idx += len;\n-    if (element_klass->is_instance_klass()) {\n-      new_str[idx++] = JVM_SIGNATURE_ENDCLASS;\n-    }\n-    new_str[idx++] = '\\0';\n-    name = SymbolTable::new_symbol(new_str);\n-  }\n+  Symbol* name = create_element_klass_array_name(THREAD, element_klass);\n","filename":"src\/hotspot\/share\/oops\/objArrayKlass.cpp","additions":22,"deletions":20,"binary":false,"changes":42,"status":"modified"},{"patch":"@@ -55,0 +55,5 @@\n+\n+ protected:\n+  \/\/ Create array_name for element klass\n+  static Symbol* create_element_klass_array_name(JavaThread* current, Klass* element_klass);\n+\n@@ -62,1 +67,3 @@\n-  Klass** element_klass_addr()      { return &_element_klass; }\n+\n+  \/\/ Compiler\/Interpreter offset\n+  static ByteSize element_klass_offset() { return byte_offset_of(ObjArrayKlass, _element_klass); }\n@@ -71,3 +78,0 @@\n-  \/\/ Compiler\/Interpreter offset\n-  static ByteSize element_klass_offset() { return byte_offset_of(ObjArrayKlass, _element_klass); }\n-\n","filename":"src\/hotspot\/share\/oops\/objArrayKlass.hpp","additions":8,"deletions":4,"binary":false,"changes":12,"status":"modified"},{"patch":"@@ -35,0 +35,1 @@\n+#ifdef _LP64\n@@ -49,0 +50,9 @@\n+#else\n+  assert(_klass_mode == Undefined, \"ObjLayout initialized twice\");\n+  assert(!UseCompactObjectHeaders, \"COH unsupported on 32-bit\");\n+  \/\/ We support +-UseCompressedClassPointers on 32-bit, but the layout\n+  \/\/ is exactly the same as it was with uncompressed klass pointers\n+  _klass_mode = UseCompressedClassPointers ? Compressed : Uncompressed;\n+  _oop_base_offset_in_bytes = sizeof(markWord) + sizeof(Klass*);\n+  _oop_has_klass_gap = false;\n+#endif\n","filename":"src\/hotspot\/share\/oops\/objLayout.cpp","additions":10,"deletions":0,"binary":false,"changes":10,"status":"modified"},{"patch":"@@ -40,1 +40,0 @@\n-#include \"runtime\/lightweightSynchronizer.hpp\"\n@@ -87,1 +86,4 @@\n-  if (java_lang_String::is_instance(obj)) {\n+  \/\/ We can't use java_lang_String::is_instance since that has klass assertions enabled.\n+  \/\/ If the klass is garbage we want to just fail the check and continue printing, as\n+  \/\/ opposed to aborting the VM entirely.\n+  if (obj != nullptr && obj->klass_without_asserts() == vmClasses::String_klass()) {\n@@ -91,1 +93,10 @@\n-    klass()->oop_print_value_on(obj, st);\n+    Klass* k = klass_without_asserts();\n+    if (k == nullptr) {\n+      st->print(\"null klass\");\n+    } else if (!Metaspace::contains(k)) {\n+      st->print(\"klass not in Metaspace\");\n+    } else if (!k->is_klass()) {\n+      st->print(\"klass not a Klass\");\n+    } else {\n+      k->oop_print_value_on(obj, st);\n+    }\n@@ -114,16 +125,2 @@\n-bool oopDesc::is_oop(oop obj, bool ignore_mark_word) {\n-  if (!Universe::heap()->is_oop(obj)) {\n-    return false;\n-  }\n-\n-  \/\/ Header verification: the mark is typically non-zero. If we're\n-  \/\/ at a safepoint, it must not be zero, except when using the new lightweight locking.\n-  \/\/ Outside of a safepoint, the header could be changing (for example,\n-  \/\/ another thread could be inflating a lock on this object).\n-  if (ignore_mark_word) {\n-    return true;\n-  }\n-  if (obj->mark().value() != 0) {\n-    return true;\n-  }\n-  return LockingMode == LM_LIGHTWEIGHT || !SafepointSynchronize::is_at_safepoint();\n+bool oopDesc::is_oop(oop obj) {\n+  return Universe::heap()->is_oop(obj);\n@@ -132,1 +129,1 @@\n-void oopDesc::initialize_hash_if_necessary(oop obj, Klass* k, markWord m) {\n+markWord oopDesc::initialize_hash_if_necessary(oop obj, Klass* k, markWord m) {\n@@ -143,1 +140,1 @@\n-  assert(static_cast<uint32_t>(LightweightSynchronizer::get_hash(m, cast_to_oop(this), k)) == hash,\n+  assert(static_cast<uint32_t>(ObjectSynchronizer::get_hash(m, cast_to_oop(this), k)) == hash,\n@@ -146,1 +143,1 @@\n-  set_mark(m);\n+  return m;\n@@ -150,2 +147,2 @@\n-bool oopDesc::is_oop_or_null(oop obj, bool ignore_mark_word) {\n-  return obj == nullptr ? true : is_oop(obj, ignore_mark_word);\n+bool oopDesc::is_oop_or_null(oop obj) {\n+  return obj == nullptr ? true : is_oop(obj);\n@@ -199,1 +196,1 @@\n-address oopDesc::address_field_acquire(int offset) const              { return Atomic::load_acquire(field_addr<address>(offset)); }\n+address oopDesc::address_field_acquire(int offset) const              { return AtomicAccess::load_acquire(field_addr<address>(offset)); }\n@@ -202,1 +199,1 @@\n-void oopDesc::release_address_field_put(int offset, address value)    { Atomic::release_store(field_addr<address>(offset), value); }\n+void oopDesc::release_address_field_put(int offset, address value)    { AtomicAccess::release_store(field_addr<address>(offset), value); }\n@@ -207,2 +204,2 @@\n-Metadata* oopDesc::metadata_field_acquire(int offset) const           { return Atomic::load_acquire(field_addr<Metadata*>(offset)); }\n-void oopDesc::release_metadata_field_put(int offset, Metadata* value) { Atomic::release_store(field_addr<Metadata*>(offset), value); }\n+Metadata* oopDesc::metadata_field_acquire(int offset) const           { return AtomicAccess::load_acquire(field_addr<Metadata*>(offset)); }\n+void oopDesc::release_metadata_field_put(int offset, Metadata* value) { AtomicAccess::release_store(field_addr<Metadata*>(offset), value); }\n@@ -210,2 +207,2 @@\n-jbyte oopDesc::byte_field_acquire(int offset) const                   { return Atomic::load_acquire(field_addr<jbyte>(offset)); }\n-void oopDesc::release_byte_field_put(int offset, jbyte value)         { Atomic::release_store(field_addr<jbyte>(offset), value); }\n+jbyte oopDesc::byte_field_acquire(int offset) const                   { return AtomicAccess::load_acquire(field_addr<jbyte>(offset)); }\n+void oopDesc::release_byte_field_put(int offset, jbyte value)         { AtomicAccess::release_store(field_addr<jbyte>(offset), value); }\n@@ -213,2 +210,2 @@\n-jchar oopDesc::char_field_acquire(int offset) const                   { return Atomic::load_acquire(field_addr<jchar>(offset)); }\n-void oopDesc::release_char_field_put(int offset, jchar value)         { Atomic::release_store(field_addr<jchar>(offset), value); }\n+jchar oopDesc::char_field_acquire(int offset) const                   { return AtomicAccess::load_acquire(field_addr<jchar>(offset)); }\n+void oopDesc::release_char_field_put(int offset, jchar value)         { AtomicAccess::release_store(field_addr<jchar>(offset), value); }\n@@ -216,2 +213,2 @@\n-jboolean oopDesc::bool_field_acquire(int offset) const                { return Atomic::load_acquire(field_addr<jboolean>(offset)); }\n-void oopDesc::release_bool_field_put(int offset, jboolean value)      { Atomic::release_store(field_addr<jboolean>(offset), jboolean(value & 1)); }\n+jboolean oopDesc::bool_field_acquire(int offset) const                { return AtomicAccess::load_acquire(field_addr<jboolean>(offset)); }\n+void oopDesc::release_bool_field_put(int offset, jboolean value)      { AtomicAccess::release_store(field_addr<jboolean>(offset), jboolean(value & 1)); }\n@@ -219,2 +216,2 @@\n-jint oopDesc::int_field_acquire(int offset) const                     { return Atomic::load_acquire(field_addr<jint>(offset)); }\n-void oopDesc::release_int_field_put(int offset, jint value)           { Atomic::release_store(field_addr<jint>(offset), value); }\n+jint oopDesc::int_field_acquire(int offset) const                     { return AtomicAccess::load_acquire(field_addr<jint>(offset)); }\n+void oopDesc::release_int_field_put(int offset, jint value)           { AtomicAccess::release_store(field_addr<jint>(offset), value); }\n@@ -222,2 +219,2 @@\n-jshort oopDesc::short_field_acquire(int offset) const                 { return Atomic::load_acquire(field_addr<jshort>(offset)); }\n-void oopDesc::release_short_field_put(int offset, jshort value)       { Atomic::release_store(field_addr<jshort>(offset), value); }\n+jshort oopDesc::short_field_acquire(int offset) const                 { return AtomicAccess::load_acquire(field_addr<jshort>(offset)); }\n+void oopDesc::release_short_field_put(int offset, jshort value)       { AtomicAccess::release_store(field_addr<jshort>(offset), value); }\n@@ -225,2 +222,2 @@\n-jlong oopDesc::long_field_acquire(int offset) const                   { return Atomic::load_acquire(field_addr<jlong>(offset)); }\n-void oopDesc::release_long_field_put(int offset, jlong value)         { Atomic::release_store(field_addr<jlong>(offset), value); }\n+jlong oopDesc::long_field_acquire(int offset) const                   { return AtomicAccess::load_acquire(field_addr<jlong>(offset)); }\n+void oopDesc::release_long_field_put(int offset, jlong value)         { AtomicAccess::release_store(field_addr<jlong>(offset), value); }\n@@ -228,2 +225,2 @@\n-jfloat oopDesc::float_field_acquire(int offset) const                 { return Atomic::load_acquire(field_addr<jfloat>(offset)); }\n-void oopDesc::release_float_field_put(int offset, jfloat value)       { Atomic::release_store(field_addr<jfloat>(offset), value); }\n+jfloat oopDesc::float_field_acquire(int offset) const                 { return AtomicAccess::load_acquire(field_addr<jfloat>(offset)); }\n+void oopDesc::release_float_field_put(int offset, jfloat value)       { AtomicAccess::release_store(field_addr<jfloat>(offset), value); }\n@@ -231,2 +228,2 @@\n-jdouble oopDesc::double_field_acquire(int offset) const               { return Atomic::load_acquire(field_addr<jdouble>(offset)); }\n-void oopDesc::release_double_field_put(int offset, jdouble value)     { Atomic::release_store(field_addr<jdouble>(offset), value); }\n+jdouble oopDesc::double_field_acquire(int offset) const               { return AtomicAccess::load_acquire(field_addr<jdouble>(offset)); }\n+void oopDesc::release_double_field_put(int offset, jdouble value)     { AtomicAccess::release_store(field_addr<jdouble>(offset), value); }\n","filename":"src\/hotspot\/share\/oops\/oop.cpp","additions":41,"deletions":44,"binary":false,"changes":85,"status":"modified"},{"patch":"@@ -28,0 +28,1 @@\n+#include \"cppstdlib\/type_traits.hpp\"\n@@ -30,1 +31,1 @@\n-#include \"oops\/compressedKlass.hpp\"\n+#include \"oops\/compressedKlass.hpp\"\n@@ -35,1 +36,1 @@\n-#include \"runtime\/atomic.hpp\"\n+#include \"runtime\/atomicAccess.hpp\"\n@@ -38,1 +39,0 @@\n-#include <type_traits>\n@@ -67,0 +67,3 @@\n+  inline void* base_addr();\n+  inline const void* base_addr() const;\n+\n@@ -69,1 +72,0 @@\n-  inline markWord* mark_addr() const;\n@@ -94,0 +96,1 @@\n+  inline narrowKlass narrow_klass() const;\n@@ -284,2 +287,2 @@\n-  static bool is_oop(oop obj, bool ignore_mark_word = false);\n-  static bool is_oop_or_null(oop obj, bool ignore_mark_word = false);\n+  static bool is_oop(oop obj);\n+  static bool is_oop_or_null(oop obj);\n@@ -343,1 +346,1 @@\n-  void initialize_hash_if_necessary(oop obj, Klass* k, markWord m);\n+  markWord initialize_hash_if_necessary(oop obj, Klass* k, markWord m);\n","filename":"src\/hotspot\/share\/oops\/oop.hpp","additions":10,"deletions":7,"binary":false,"changes":17,"status":"modified"},{"patch":"@@ -30,1 +30,1 @@\n-#include \"memory\/universe.hpp\"\n+#include \"memory\/universe.hpp\"\n@@ -37,1 +37,1 @@\n-#include \"oops\/objLayout.inline.hpp\"\n+#include \"oops\/objLayout.inline.hpp\"\n@@ -40,1 +40,1 @@\n-#include \"runtime\/atomic.hpp\"\n+#include \"runtime\/atomicAccess.hpp\"\n@@ -44,1 +44,1 @@\n-#include \"utilities\/macros.hpp\"\n+#include \"utilities\/macros.hpp\"\n@@ -50,0 +50,3 @@\n+void* oopDesc::base_addr() { return this; }\n+const void* oopDesc::base_addr() const { return this; }\n+\n@@ -51,1 +54,1 @@\n-  return Atomic::load(&_mark);\n+  return AtomicAccess::load(&_mark);\n@@ -55,5 +58,1 @@\n-  return Atomic::load_acquire(&_mark);\n-}\n-\n-markWord* oopDesc::mark_addr() const {\n-  return (markWord*) &_mark;\n+  return AtomicAccess::load_acquire(&_mark);\n@@ -64,1 +63,1 @@\n-    Atomic::store(reinterpret_cast<uint32_t volatile*>(&_mark), m.value32());\n+    AtomicAccess::store(reinterpret_cast<uint32_t volatile*>(&_mark), m.value32());\n@@ -66,1 +65,1 @@\n-    Atomic::store(&_mark, m);\n+    AtomicAccess::store(&_mark, m);\n@@ -71,1 +70,1 @@\n-  Atomic::store(&_mark, m);\n+  AtomicAccess::store(&_mark, m);\n@@ -84,1 +83,1 @@\n-    Atomic::release_store((uint32_t*)(((char*)mem) + mark_offset_in_bytes()), m.value32());\n+    AtomicAccess::release_store((uint32_t*)(((char*)mem) + mark_offset_in_bytes()), m.value32());\n@@ -86,1 +85,1 @@\n-    Atomic::release_store((markWord*)(((char*)mem) + mark_offset_in_bytes()), m);\n+    AtomicAccess::release_store((markWord*)(((char*)mem) + mark_offset_in_bytes()), m);\n@@ -92,1 +91,1 @@\n-    Atomic::release_store(reinterpret_cast<uint32_t volatile*>(&_mark), m.value32());\n+    AtomicAccess::release_store(reinterpret_cast<uint32_t volatile*>(&_mark), m.value32());\n@@ -94,1 +93,1 @@\n-    Atomic::release_store(&_mark, m);\n+    AtomicAccess::release_store(&_mark, m);\n@@ -99,1 +98,1 @@\n-  return Atomic::cmpxchg(&_mark, old_mark, new_mark);\n+  return AtomicAccess::cmpxchg(&_mark, old_mark, new_mark);\n@@ -103,1 +102,1 @@\n-  return Atomic::cmpxchg(&_mark, old_mark, new_mark, order);\n+  return AtomicAccess::cmpxchg(&_mark, old_mark, new_mark, order);\n@@ -151,1 +150,1 @@\n-      narrowKlass narrow_klass = Atomic::load_acquire(&_metadata._compressed_klass);\n+      narrowKlass narrow_klass = AtomicAccess::load_acquire(&_metadata._compressed_klass);\n@@ -155,1 +154,1 @@\n-      return Atomic::load_acquire(&_metadata._klass);\n+      return AtomicAccess::load_acquire(&_metadata._klass);\n@@ -170,0 +169,11 @@\n+narrowKlass oopDesc::narrow_klass() const {\n+  switch (ObjLayout::klass_mode()) {\n+    case ObjLayout::Compact:\n+      return mark().narrow_klass();\n+    case ObjLayout::Compressed:\n+      return _metadata._compressed_klass;\n+    default:\n+      ShouldNotReachHere();\n+  }\n+}\n+\n@@ -185,1 +195,1 @@\n-    Atomic::release_store((narrowKlass*)raw_mem,\n+    AtomicAccess::release_store((narrowKlass*)raw_mem,\n@@ -188,1 +198,1 @@\n-    Atomic::release_store((Klass**)raw_mem, k);\n+    AtomicAccess::release_store((Klass**)raw_mem, k);\n@@ -228,0 +238,3 @@\n+    if (mark.is_not_hashed_expanded()) {\n+      assert(klass->expand_for_hash(cast_to_oop(this), mark), \"must be?\");\n+    }\n@@ -339,2 +352,2 @@\n-inline jint oopDesc::int_field_relaxed(int offset) const            { return Atomic::load(field_addr<jint>(offset)); }\n-inline void oopDesc::int_field_put_relaxed(int offset, jint value)  { Atomic::store(field_addr<jint>(offset), value); }\n+inline jint oopDesc::int_field_relaxed(int offset) const            { return AtomicAccess::load(field_addr<jint>(offset)); }\n+inline void oopDesc::int_field_put_relaxed(int offset, jint value)  { AtomicAccess::store(field_addr<jint>(offset), value); }\n@@ -398,1 +411,1 @@\n-      fwd_mark = fwd_mark.set_hashed_expanded();\n+      fwd_mark = fwd_mark.set_hashed_not_expanded();\n@@ -567,1 +580,2 @@\n-  assert(!m.is_forwarded(), \"must not be forwarded header\");\n+  \/\/ ZGC temporarily marks the header in ZObjArrayAllocator::initialize().\n+  assert(UseZGC || !m.is_forwarded(), \"must not be forwarded header\");\n@@ -569,1 +583,1 @@\n-    initialize_hash_if_necessary(obj, m.klass(), m);\n+    set_mark(initialize_hash_if_necessary(obj, m.klass(), m));\n","filename":"src\/hotspot\/share\/oops\/oop.inline.hpp","additions":42,"deletions":28,"binary":false,"changes":70,"status":"modified"},{"patch":"@@ -2,1 +2,1 @@\n- * Copyright (c) 1997, 2021, Oracle and\/or its affiliates. All rights reserved.\n+ * Copyright (c) 1997, 2025, Oracle and\/or its affiliates. All rights reserved.\n@@ -28,0 +28,1 @@\n+#include \"cppstdlib\/type_traits.hpp\"\n@@ -30,1 +31,0 @@\n-#include <type_traits>\n","filename":"src\/hotspot\/share\/oops\/typeArrayOop.hpp","additions":2,"deletions":2,"binary":false,"changes":4,"status":"modified"},{"patch":"@@ -2,1 +2,1 @@\n- * Copyright (c) 1997, 2020, Oracle and\/or its affiliates. All rights reserved.\n+ * Copyright (c) 1997, 2025, Oracle and\/or its affiliates. All rights reserved.\n@@ -31,1 +31,1 @@\n-#include \"oops\/oop.inline.hpp\"\n+#include \"oops\/oop.inline.hpp\"\n@@ -163,1 +163,1 @@\n-  return Atomic::load_acquire(byte_at_addr(which));\n+  return AtomicAccess::load_acquire(byte_at_addr(which));\n@@ -166,1 +166,1 @@\n-  Atomic::release_store(byte_at_addr(which), contents);\n+  AtomicAccess::release_store(byte_at_addr(which), contents);\n","filename":"src\/hotspot\/share\/oops\/typeArrayOop.inline.hpp","additions":4,"deletions":4,"binary":false,"changes":8,"status":"modified"},{"patch":"@@ -90,0 +90,1 @@\n+#include \"utilities\/hashTable.hpp\"\n@@ -91,1 +92,0 @@\n-#include \"utilities\/resourceHash.hpp\"\n@@ -653,1 +653,0 @@\n-      _has_method_handle_invokes(false),\n@@ -696,0 +695,1 @@\n+      _FIRST_STACK_mask(comp_arena()),\n@@ -697,0 +697,1 @@\n+      _regmask_arena(mtCompiler, Arena::Tag::tag_regmask),\n@@ -926,1 +927,0 @@\n-      _has_method_handle_invokes(false),\n@@ -957,0 +957,1 @@\n+      _FIRST_STACK_mask(comp_arena()),\n@@ -958,0 +959,1 @@\n+      _regmask_arena(mtCompiler, Arena::Tag::tag_regmask),\n@@ -2105,0 +2107,6 @@\n+      if (should_stress_inlining()) {\n+        \/\/ randomly add repeated inline attempt if stress-inlining\n+        cg->call_node()->set_generator(cg);\n+        C->igvn_worklist()->push(cg->call_node());\n+        continue;\n+      }\n@@ -2152,1 +2160,1 @@\n-    igvn.reset_from_gvn(initial_gvn());\n+    igvn.reset();\n@@ -2313,2 +2321,1 @@\n-  \/\/ Initialize IterGVN with types and values from parse-time GVN\n-  PhaseIterGVN igvn(initial_gvn());\n+  PhaseIterGVN igvn;\n@@ -2383,1 +2390,1 @@\n-    igvn.reset_from_gvn(initial_gvn());\n+    igvn.reset();\n@@ -2790,1 +2797,1 @@\n-static uint eval_operand(Node* n, ResourceHashtable<Node*,uint>& eval_map) {\n+static uint eval_operand(Node* n, HashTable<Node*,uint>& eval_map) {\n@@ -2798,1 +2805,1 @@\n-                          ResourceHashtable<Node*,uint>& eval_map) {\n+                          HashTable<Node*,uint>& eval_map) {\n@@ -2822,1 +2829,1 @@\n-  ResourceHashtable<Node*,uint> eval_map;\n+  HashTable<Node*,uint> eval_map;\n@@ -3306,0 +3313,19 @@\n+  case Op_CallLeafPure: {\n+    \/\/ If the pure call is not supported, then lower to a CallLeaf.\n+    if (!Matcher::match_rule_supported(Op_CallLeafPure)) {\n+      CallNode* call = n->as_Call();\n+      CallNode* new_call = new CallLeafNode(call->tf(), call->entry_point(),\n+                                            call->_name, TypeRawPtr::BOTTOM);\n+      new_call->init_req(TypeFunc::Control, call->in(TypeFunc::Control));\n+      new_call->init_req(TypeFunc::I_O, C->top());\n+      new_call->init_req(TypeFunc::Memory, C->top());\n+      new_call->init_req(TypeFunc::ReturnAdr, C->top());\n+      new_call->init_req(TypeFunc::FramePtr, C->top());\n+      for (unsigned int i = TypeFunc::Parms; i < call->tf()->domain()->cnt(); i++) {\n+        new_call->init_req(i, call->in(i));\n+      }\n+      n->subsume_by(new_call, this);\n+    }\n+    frc.inc_call_count();\n+    break;\n+  }\n@@ -3604,1 +3630,4 @@\n-          new_in2 = ConNode::make(t->make_narrowklass());\n+          ciKlass* klass = t->is_klassptr()->exact_klass();\n+          if (klass->is_in_encoding_range()) {\n+            new_in2 = ConNode::make(t->make_narrowklass());\n+          }\n@@ -3641,1 +3670,7 @@\n-        n->subsume_by(ConNode::make(t->make_narrowklass()), this);\n+        ciKlass* klass = t->is_klassptr()->exact_klass();\n+        if (klass->is_in_encoding_range()) {\n+          n->subsume_by(ConNode::make(t->make_narrowklass()), this);\n+        } else {\n+          assert(false, \"unencodable klass in ConP -> EncodeP\");\n+          C->record_failure(\"unencodable klass in ConP -> EncodeP\");\n+        }\n@@ -5201,1 +5236,1 @@\n-  return PrintPhaseLevel > 0 && directive()->PhasePrintLevelOption >= level &&\n+  return PrintPhaseLevel >= 0 && directive()->PhasePrintLevelOption >= level &&\n@@ -5346,0 +5381,155 @@\n+\n+#ifndef PRODUCT\n+\/\/ Collects all the control inputs from nodes on the worklist and from their data dependencies\n+static void find_candidate_control_inputs(Unique_Node_List& worklist, Unique_Node_List& candidates) {\n+  \/\/ Follow non-control edges until we reach CFG nodes\n+  for (uint i = 0; i < worklist.size(); i++) {\n+    const Node* n = worklist.at(i);\n+    for (uint j = 0; j < n->req(); j++) {\n+      Node* in = n->in(j);\n+      if (in == nullptr || in->is_Root()) {\n+        continue;\n+      }\n+      if (in->is_CFG()) {\n+        if (in->is_Call()) {\n+          \/\/ The return value of a call is only available if the call did not result in an exception\n+          Node* control_proj_use = in->as_Call()->proj_out(TypeFunc::Control)->unique_out();\n+          if (control_proj_use->is_Catch()) {\n+            Node* fall_through = control_proj_use->as_Catch()->proj_out(CatchProjNode::fall_through_index);\n+            candidates.push(fall_through);\n+            continue;\n+          }\n+        }\n+\n+        if (in->is_Multi()) {\n+          \/\/ We got here by following data inputs so we should only have one control use\n+          \/\/ (no IfNode, etc)\n+          assert(!n->is_MultiBranch(), \"unexpected node type: %s\", n->Name());\n+          candidates.push(in->as_Multi()->proj_out(TypeFunc::Control));\n+        } else {\n+          candidates.push(in);\n+        }\n+      } else {\n+        worklist.push(in);\n+      }\n+    }\n+  }\n+}\n+\n+\/\/ Returns the candidate node that is a descendant to all the other candidates\n+static Node* pick_control(Unique_Node_List& candidates) {\n+  Unique_Node_List worklist;\n+  worklist.copy(candidates);\n+\n+  \/\/ Traverse backwards through the CFG\n+  for (uint i = 0; i < worklist.size(); i++) {\n+    const Node* n = worklist.at(i);\n+    if (n->is_Root()) {\n+      continue;\n+    }\n+    for (uint j = 0; j < n->req(); j++) {\n+      \/\/ Skip backedge of loops to avoid cycles\n+      if (n->is_Loop() && j == LoopNode::LoopBackControl) {\n+        continue;\n+      }\n+\n+      Node* pred = n->in(j);\n+      if (pred != nullptr && pred != n && pred->is_CFG()) {\n+        worklist.push(pred);\n+        \/\/ if pred is an ancestor of n, then pred is an ancestor to at least one candidate\n+        candidates.remove(pred);\n+      }\n+    }\n+  }\n+\n+  assert(candidates.size() == 1, \"unexpected control flow\");\n+  return candidates.at(0);\n+}\n+\n+\/\/ Initialize a parameter input for a debug print call, using a placeholder for jlong and jdouble\n+static void debug_print_init_parm(Node* call, Node* parm, Node* half, int* pos) {\n+  call->init_req((*pos)++, parm);\n+  const BasicType bt = parm->bottom_type()->basic_type();\n+  if (bt == T_LONG || bt == T_DOUBLE) {\n+    call->init_req((*pos)++, half);\n+  }\n+}\n+\n+Node* Compile::make_debug_print_call(const char* str, address call_addr, PhaseGVN* gvn,\n+                              Node* parm0, Node* parm1,\n+                              Node* parm2, Node* parm3,\n+                              Node* parm4, Node* parm5,\n+                              Node* parm6) const {\n+  Node* str_node = gvn->transform(new ConPNode(TypeRawPtr::make(((address) str))));\n+  const TypeFunc* type = OptoRuntime::debug_print_Type(parm0, parm1, parm2, parm3, parm4, parm5, parm6);\n+  Node* call = new CallLeafNode(type, call_addr, \"debug_print\", TypeRawPtr::BOTTOM);\n+\n+  \/\/ find the most suitable control input\n+  Unique_Node_List worklist, candidates;\n+  if (parm0 != nullptr) { worklist.push(parm0);\n+  if (parm1 != nullptr) { worklist.push(parm1);\n+  if (parm2 != nullptr) { worklist.push(parm2);\n+  if (parm3 != nullptr) { worklist.push(parm3);\n+  if (parm4 != nullptr) { worklist.push(parm4);\n+  if (parm5 != nullptr) { worklist.push(parm5);\n+  if (parm6 != nullptr) { worklist.push(parm6);\n+  \/* close each nested if ===> *\/  } } } } } } }\n+  find_candidate_control_inputs(worklist, candidates);\n+  Node* control = nullptr;\n+  if (candidates.size() == 0) {\n+    control = C->start()->proj_out(TypeFunc::Control);\n+  } else {\n+    control = pick_control(candidates);\n+  }\n+\n+  \/\/ find all the previous users of the control we picked\n+  GrowableArray<Node*> users_of_control;\n+  for (DUIterator_Fast kmax, i = control->fast_outs(kmax); i < kmax; i++) {\n+    Node* use = control->fast_out(i);\n+    if (use->is_CFG() && use != control) {\n+      users_of_control.push(use);\n+    }\n+  }\n+\n+  \/\/ we do not actually care about IO and memory as it uses neither\n+  call->init_req(TypeFunc::Control,   control);\n+  call->init_req(TypeFunc::I_O,       top());\n+  call->init_req(TypeFunc::Memory,    top());\n+  call->init_req(TypeFunc::FramePtr,  C->start()->proj_out(TypeFunc::FramePtr));\n+  call->init_req(TypeFunc::ReturnAdr, top());\n+\n+  int pos = TypeFunc::Parms;\n+  call->init_req(pos++, str_node);\n+  if (parm0 != nullptr) { debug_print_init_parm(call, parm0, top(), &pos);\n+  if (parm1 != nullptr) { debug_print_init_parm(call, parm1, top(), &pos);\n+  if (parm2 != nullptr) { debug_print_init_parm(call, parm2, top(), &pos);\n+  if (parm3 != nullptr) { debug_print_init_parm(call, parm3, top(), &pos);\n+  if (parm4 != nullptr) { debug_print_init_parm(call, parm4, top(), &pos);\n+  if (parm5 != nullptr) { debug_print_init_parm(call, parm5, top(), &pos);\n+  if (parm6 != nullptr) { debug_print_init_parm(call, parm6, top(), &pos);\n+  \/* close each nested if ===> *\/  } } } } } } }\n+  assert(call->in(call->req()-1) != nullptr, \"must initialize all parms\");\n+\n+  call = gvn->transform(call);\n+  Node* call_control_proj = gvn->transform(new ProjNode(call, TypeFunc::Control));\n+\n+  \/\/ rewire previous users to have the new call as control instead\n+  PhaseIterGVN* igvn = gvn->is_IterGVN();\n+  for (int i = 0; i < users_of_control.length(); i++) {\n+    Node* use = users_of_control.at(i);\n+    for (uint j = 0; j < use->req(); j++) {\n+      if (use->in(j) == control) {\n+        if (igvn != nullptr) {\n+          igvn->replace_input_of(use, j, call_control_proj);\n+        } else {\n+          gvn->hash_delete(use);\n+          use->set_req(j, call_control_proj);\n+          gvn->hash_insert(use);\n+        }\n+      }\n+    }\n+  }\n+\n+  return call;\n+}\n+#endif \/\/ !PRODUCT\n","filename":"src\/hotspot\/share\/opto\/compile.cpp","additions":203,"deletions":13,"binary":false,"changes":216,"status":"modified"},{"patch":"@@ -100,4 +100,5 @@\n-  const bool is_virtual_or_interface = (bytecode == Bytecodes::_invokevirtual) ||\n-                                       (bytecode == Bytecodes::_invokeinterface) ||\n-                                       (orig_callee->intrinsic_id() == vmIntrinsics::_linkToVirtual) ||\n-                                       (orig_callee->intrinsic_id() == vmIntrinsics::_linkToInterface);\n+  const bool is_virtual = (bytecode == Bytecodes::_invokevirtual) || (orig_callee->intrinsic_id() == vmIntrinsics::_linkToVirtual);\n+  const bool is_interface = (bytecode == Bytecodes::_invokeinterface) || (orig_callee->intrinsic_id() == vmIntrinsics::_linkToInterface);\n+  const bool is_virtual_or_interface = is_virtual || is_interface;\n+\n+  const bool check_access = !orig_callee->is_method_handle_intrinsic(); \/\/ method handle intrinsics don't perform access checks\n@@ -242,1 +243,2 @@\n-                                                   speculative_receiver_type);\n+                                                   speculative_receiver_type,\n+                                                   check_access);\n@@ -259,0 +261,1 @@\n+        assert(check_access, \"required\");\n@@ -260,1 +263,1 @@\n-                                                      profile.receiver(0));\n+                                                 profile.receiver(0));\n@@ -271,0 +274,1 @@\n+            assert(check_access, \"required\");\n@@ -272,1 +276,1 @@\n-                                                               profile.receiver(1));\n+                                                          profile.receiver(1));\n@@ -337,3 +341,12 @@\n-    if (call_does_dispatch && bytecode == Bytecodes::_invokeinterface) {\n-      ciInstanceKlass* declared_interface =\n-          caller->get_declared_method_holder_at_bci(bci)->as_instance_klass();\n+    if (call_does_dispatch && is_interface) {\n+      ciInstanceKlass* declared_interface = nullptr;\n+      if (orig_callee->intrinsic_id() == vmIntrinsics::_linkToInterface) {\n+        \/\/ MemberName doesn't keep information about resolved interface class (REFC) once\n+        \/\/ resolution is over, but resolved method holder (DECC) can be used as a\n+        \/\/ conservative approximation.\n+        declared_interface = callee->holder();\n+      } else {\n+        assert(!orig_callee->is_method_handle_intrinsic(), \"not allowed\");\n+        declared_interface = caller->get_declared_method_holder_at_bci(bci)->as_instance_klass();\n+      }\n+      assert(declared_interface->is_interface(), \"required\");\n@@ -346,1 +359,1 @@\n-            callee->find_monomorphic_target(caller->holder(), declared_interface, singleton);\n+            callee->find_monomorphic_target(caller->holder(), declared_interface, singleton, check_access);\n@@ -369,1 +382,1 @@\n-    } \/\/ call_does_dispatch && bytecode == Bytecodes::_invokeinterface\n+    } \/\/ call_does_dispatch && is_interface\n","filename":"src\/hotspot\/share\/opto\/doCall.cpp","additions":25,"deletions":12,"binary":false,"changes":37,"status":"modified"},{"patch":"@@ -117,0 +117,1 @@\n+  NOT_PRODUCT(if (C->should_print_igv(\/* Any level *\/ 1)) C->igv_printer()->set_congraph(congraph);)\n@@ -122,0 +123,1 @@\n+  NOT_PRODUCT(if (C->should_print_igv(\/* Any level *\/ 1)) C->igv_printer()->set_congraph(nullptr);)\n@@ -129,0 +131,2 @@\n+\n+  C->print_method(PHASE_AFTER_EA, 2);\n@@ -204,1 +208,1 @@\n-        break;\n+        \/\/ If MemBarStoreStore has a precedent edge add it to the worklist (like MemBarRelease)\n@@ -284,0 +288,2 @@\n+  _compile->print_method(PHASE_EA_AFTER_INITIAL_CONGRAPH, 4);\n+\n@@ -294,0 +300,2 @@\n+  _compile->print_method(PHASE_EA_AFTER_COMPLETE_CONGRAPH, 4);\n+\n@@ -315,0 +323,1 @@\n+    _compile->print_method(PHASE_EA_ADJUST_SCALAR_REPLACEABLE_ITER, 6, n);\n@@ -353,0 +362,1 @@\n+  _compile->print_method(PHASE_EA_AFTER_PROPAGATE_NSR, 4);\n@@ -390,0 +400,2 @@\n+  _compile->print_method(PHASE_EA_AFTER_GRAPH_OPTIMIZATION, 4);\n+\n@@ -401,1 +413,0 @@\n-    C->print_method(PHASE_AFTER_EA, 2);\n@@ -416,0 +427,2 @@\n+  _compile->print_method(PHASE_EA_AFTER_SPLIT_UNIQUE_TYPES, 4);\n+\n@@ -457,0 +470,2 @@\n+  _compile->print_method(PHASE_EA_AFTER_REDUCE_PHI_ON_SAFEPOINTS, 4);\n+\n@@ -866,1 +881,1 @@\n-void ConnectionGraph::reduce_phi_on_castpp_field_load(Node* curr_castpp, GrowableArray<Node *>  &alloc_worklist, GrowableArray<Node *>  &memnode_worklist) {\n+void ConnectionGraph::reduce_phi_on_castpp_field_load(Node* curr_castpp, GrowableArray<Node*> &alloc_worklist) {\n@@ -1282,1 +1297,1 @@\n-void ConnectionGraph::reduce_phi(PhiNode* ophi, GrowableArray<Node *>  &alloc_worklist, GrowableArray<Node *>  &memnode_worklist) {\n+void ConnectionGraph::reduce_phi(PhiNode* ophi, GrowableArray<Node*> &alloc_worklist) {\n@@ -1299,2 +1314,1 @@\n-    } else if (use->is_SafePoint()) {\n-      \/\/ processed later\n+      \/\/ Safepoints to be processed later; other users aren't expected here\n@@ -1306,0 +1320,2 @@\n+  _compile->print_method(PHASE_EA_BEFORE_PHI_REDUCTION, 5, ophi);\n+\n@@ -1310,1 +1326,2 @@\n-    reduce_phi_on_castpp_field_load(castpps.at(i), alloc_worklist, memnode_worklist);\n+    reduce_phi_on_castpp_field_load(castpps.at(i), alloc_worklist);\n+    _compile->print_method(PHASE_EA_AFTER_PHI_CASTPP_REDUCTION, 6, castpps.at(i));\n@@ -1318,0 +1335,1 @@\n+      _compile->print_method(PHASE_EA_AFTER_PHI_ADDP_REDUCTION, 6, use);\n@@ -1320,0 +1338,1 @@\n+      _compile->print_method(PHASE_EA_AFTER_PHI_CMP_REDUCTION, 6, use);\n@@ -2421,0 +2440,1 @@\n+      _compile->print_method(PHASE_EA_COMPLETE_CONNECTION_GRAPH_ITER, 5);\n@@ -2494,1 +2514,2 @@\n-                                               GrowableArray<JavaObjectNode*>& non_escaped_allocs_worklist) {\n+                                               GrowableArray<JavaObjectNode*>& non_escaped_allocs_worklist,\n+                                               bool print_method) {\n@@ -2554,0 +2575,3 @@\n+      if (print_method) {\n+        _compile->print_method(PHASE_EA_CONNECTION_GRAPH_PROPAGATE_ITER, 6, e->ideal_node());\n+      }\n@@ -3132,0 +3156,8 @@\n+        } else if (use->is_LocalVar()) {\n+          Node* phi = use->ideal_node();\n+          if (phi->Opcode() == Op_Phi && reducible_merges.member(phi) && !can_reduce_phi(phi->as_Phi())) {\n+            set_not_scalar_replaceable(jobj NOT_PRODUCT(COMMA \"is merged in a non-reducible phi\"));\n+            reducible_merges.yank(phi);\n+            found_nsr_alloc = true;\n+            break;\n+          }\n@@ -3133,0 +3165,1 @@\n+        _compile->print_method(PHASE_EA_PROPAGATE_NSR_ITER, 5, jobj->ideal_node());\n@@ -3155,1 +3188,1 @@\n-  find_non_escaped_objects(ptnodes_worklist, non_escaped_allocs_worklist);\n+  find_non_escaped_objects(ptnodes_worklist, non_escaped_allocs_worklist, \/*print_method=*\/ false);\n@@ -4148,0 +4181,5 @@\n+        } else if (C->get_alias_index(result->adr_type()) != alias_idx) {\n+          assert(C->get_general_index(alias_idx) == C->get_alias_index(result->adr_type()), \"should be projection for the same field\/array element\");\n+          result = get_map(result->_idx);\n+          assert(result != nullptr, \"new projection should have been allocated\");\n+          break;\n@@ -4444,0 +4482,16 @@\n+        \/\/ Add a new NarrowMem projection for each existing NarrowMem projection with new adr type\n+        InitializeNode* init = alloc->as_Allocate()->initialization();\n+        assert(init != nullptr, \"can't find Initialization node for this Allocate node\");\n+        auto process_narrow_proj = [&](NarrowMemProjNode* proj) {\n+          const TypePtr* adr_type = proj->adr_type();\n+          const TypePtr* new_adr_type = tinst->add_offset(adr_type->offset());\n+          if (adr_type != new_adr_type && !init->already_has_narrow_mem_proj_with_adr_type(new_adr_type)) {\n+            DEBUG_ONLY( uint alias_idx = _compile->get_alias_index(new_adr_type); )\n+            assert(_compile->get_general_index(alias_idx) == _compile->get_alias_index(adr_type), \"new adr type should be narrowed down from existing adr type\");\n+            NarrowMemProjNode* new_proj = new NarrowMemProjNode(init, new_adr_type);\n+            igvn->set_type(new_proj, new_proj->bottom_type());\n+            record_for_optimizer(new_proj);\n+            set_map(proj, new_proj); \/\/ record it so ConnectionGraph::find_inst_mem() can find it\n+          }\n+        };\n+        init->for_each_narrow_mem_proj_with_new_uses(process_narrow_proj);\n@@ -4515,1 +4569,1 @@\n-        reduce_phi(n->as_Phi(), alloc_worklist, memnode_worklist);\n+        reduce_phi(n->as_Phi(), alloc_worklist);\n@@ -4695,0 +4749,2 @@\n+  _compile->print_method(PHASE_EA_AFTER_SPLIT_UNIQUE_TYPES_1, 5);\n+\n@@ -4707,5 +4763,7 @@\n-    } else if (n->is_MemBar()) { \/\/ Initialize, MemBar nodes\n-      \/\/ we don't need to do anything, but the users must be pushed\n-      n = n->as_MemBar()->proj_out_or_null(TypeFunc::Memory);\n-      if (n == nullptr) {\n-        continue;\n+    } else if (n->is_MemBar()) { \/\/ MemBar nodes\n+      if (!n->is_Initialize()) { \/\/ memory projections for Initialize pushed below (so we get to all their uses)\n+        \/\/ we don't need to do anything, but the users must be pushed\n+        n = n->as_MemBar()->proj_out_or_null(TypeFunc::Memory);\n+        if (n == nullptr) {\n+          continue;\n+        }\n@@ -4728,0 +4786,2 @@\n+    } else if (n->is_Proj()) {\n+      assert(n->in(0)->is_Initialize(), \"we only push memory projections for Initialize\");\n@@ -4771,0 +4831,5 @@\n+      } else if (use->is_Proj()) {\n+        assert(n->is_Initialize(), \"We only push projections of Initialize\");\n+        if (use->as_Proj()->_con == TypeFunc::Memory) { \/\/ Ignore precedent edge\n+          memnode_worklist.append_if_missing(use);\n+        }\n@@ -4822,1 +4887,1 @@\n-        const Type *at = igvn->type(mem->in(MemNode::Address));\n+        const Type* at = igvn->type(mem->in(MemNode::Address));\n@@ -4886,0 +4951,2 @@\n+  _compile->print_method(PHASE_EA_AFTER_SPLIT_UNIQUE_TYPES_3, 5);\n+\n@@ -4942,1 +5009,1 @@\n-             n->is_AddP() || n->is_Phi(), \"unknown node used for set_map()\");\n+             n->is_AddP() || n->is_Phi() || n->is_NarrowMemProj(), \"unknown node used for set_map()\");\n@@ -4954,0 +5021,1 @@\n+  _compile->print_method(PHASE_EA_AFTER_SPLIT_UNIQUE_TYPES_4, 5);\n@@ -4976,0 +5044,4 @@\n+const char* PointsToNode::esc_name() const {\n+  return esc_names[(int)escape_state()];\n+}\n+\n@@ -5070,1 +5142,1 @@\n-  tty->print_cr(\"No escape = %d, Arg escape = %d, Global escape = %d\", Atomic::load(&_no_escape_counter), Atomic::load(&_arg_escape_counter), Atomic::load(&_global_escape_counter));\n+  tty->print_cr(\"No escape = %d, Arg escape = %d, Global escape = %d\", AtomicAccess::load(&_no_escape_counter), AtomicAccess::load(&_arg_escape_counter), AtomicAccess::load(&_global_escape_counter));\n@@ -5081,1 +5153,1 @@\n-        Atomic::inc(&ConnectionGraph::_no_escape_counter);\n+        AtomicAccess::inc(&ConnectionGraph::_no_escape_counter);\n@@ -5083,1 +5155,1 @@\n-        Atomic::inc(&ConnectionGraph::_arg_escape_counter);\n+        AtomicAccess::inc(&ConnectionGraph::_arg_escape_counter);\n@@ -5085,1 +5157,1 @@\n-        Atomic::inc(&ConnectionGraph::_global_escape_counter);\n+        AtomicAccess::inc(&ConnectionGraph::_global_escape_counter);\n","filename":"src\/hotspot\/share\/opto\/escape.cpp","additions":93,"deletions":21,"binary":false,"changes":114,"status":"modified"},{"patch":"@@ -1883,8 +1883,14 @@\n-  Node* memory = reset_memory();\n-  Node* m = narrow_mem == nullptr ? memory : narrow_mem;\n-  call->init_req( TypeFunc::Control,   control()  );\n-  call->init_req( TypeFunc::I_O,       top()      ); \/\/ does no i\/o\n-  call->init_req( TypeFunc::Memory,    m          ); \/\/ may gc ptrs\n-  call->init_req( TypeFunc::FramePtr,  frameptr() );\n-  call->init_req( TypeFunc::ReturnAdr, top()      );\n-  return memory;\n+  call->init_req(TypeFunc::Control, control());\n+  call->init_req(TypeFunc::I_O, top()); \/\/ does no i\/o\n+  call->init_req(TypeFunc::ReturnAdr, top());\n+  if (call->is_CallLeafPure()) {\n+    call->init_req(TypeFunc::Memory, top());\n+    call->init_req(TypeFunc::FramePtr, top());\n+    return nullptr;\n+  } else {\n+    Node* memory = reset_memory();\n+    Node* m = narrow_mem == nullptr ? memory : narrow_mem;\n+    call->init_req(TypeFunc::Memory, m); \/\/ may gc ptrs\n+    call->init_req(TypeFunc::FramePtr, frameptr());\n+    return memory;\n+  }\n@@ -1908,0 +1914,5 @@\n+  if (call->is_CallLeafPure()) {\n+    \/\/ Pure function have only control (for now) and data output, in particular\n+    \/\/ they don't touch the memory, so we don't want a memory proj that is set after.\n+    return;\n+  }\n@@ -2301,7 +2312,9 @@\n-        assert(data->is_ReceiverTypeData(), \"bad profile data type\");\n-        ciReceiverTypeData* call = (ciReceiverTypeData*)data->as_ReceiverTypeData();\n-        uint i = 0;\n-        for (; i < call->row_limit(); i++) {\n-          ciKlass* receiver = call->receiver(i);\n-          if (receiver != nullptr) {\n-            break;\n+        if (TypeProfileCasts) {\n+          assert(data->is_ReceiverTypeData(), \"bad profile data type\");\n+          ciReceiverTypeData* call = (ciReceiverTypeData*)data->as_ReceiverTypeData();\n+          uint i = 0;\n+          for (; i < call->row_limit(); i++) {\n+            ciKlass* receiver = call->receiver(i);\n+            if (receiver != nullptr) {\n+              break;\n+            }\n@@ -2309,0 +2322,1 @@\n+          ptr_kind = (i == call->row_limit()) ? ProfileAlwaysNull : ProfileMaybeNull;\n@@ -2310,1 +2324,0 @@\n-        ptr_kind = (i == call->row_limit()) ? ProfileAlwaysNull : ProfileMaybeNull;\n@@ -2494,0 +2507,2 @@\n+  } else if (flags & RC_PURE) {\n+    call = new CallLeafPureNode(call_type, call_addr, call_name, adr_type);\n@@ -3629,4 +3644,7 @@\n-    \/\/ Add an edge in the MergeMem for the header fields so an access\n-    \/\/ to one of those has correct memory state\n-    set_memory(minit_out, C->get_alias_index(oop_type->add_offset(oopDesc::mark_offset_in_bytes())));\n-    set_memory(minit_out, C->get_alias_index(oop_type->add_offset(Type::klass_offset())));\n+    int mark_idx = C->get_alias_index(oop_type->add_offset(oopDesc::mark_offset_in_bytes()));\n+    \/\/ Add an edge in the MergeMem for the header fields so an access to one of those has correct memory state.\n+    \/\/ Use one NarrowMemProjNode per slice to properly record the adr type of each slice. The Initialize node will have\n+    \/\/ multiple projections as a result.\n+    set_memory(_gvn.transform(new NarrowMemProjNode(init, C->get_adr_type(mark_idx))), mark_idx);\n+    int klass_idx = C->get_alias_index(oop_type->add_offset(Type::klass_offset()));\n+    set_memory(_gvn.transform(new NarrowMemProjNode(init, C->get_adr_type(klass_idx))), klass_idx);\n@@ -3636,1 +3654,1 @@\n-      hook_memory_on_init(*this, elemidx, minit_in, minit_out);\n+      hook_memory_on_init(*this, elemidx, minit_in, _gvn.transform(new NarrowMemProjNode(init, C->get_adr_type(elemidx))));\n@@ -3645,1 +3663,1 @@\n-        hook_memory_on_init(*this, fieldidx, minit_in, minit_out);\n+        hook_memory_on_init(*this, fieldidx, minit_in, _gvn.transform(new NarrowMemProjNode(init, C->get_adr_type(fieldidx))));\n@@ -4053,0 +4071,5 @@\n+  if (ShortRunningLongLoop) {\n+    \/\/ Will narrow the limit down with a cast node. Predicates added later may depend on the cast so should be last when\n+    \/\/ walking up from the loop.\n+    add_parse_predicate(Deoptimization::Reason_short_running_long_loop, nargs);\n+  }\n@@ -4059,1 +4082,3 @@\n-  add_parse_predicate(Deoptimization::Reason_auto_vectorization_check, nargs);\n+  if (UseAutoVectorizationPredicate) {\n+    add_parse_predicate(Deoptimization::Reason_auto_vectorization_check, nargs);\n+  }\n","filename":"src\/hotspot\/share\/opto\/graphKit.cpp","additions":48,"deletions":23,"binary":false,"changes":71,"status":"modified"},{"patch":"@@ -248,0 +248,1 @@\n+  case vmIntrinsics::_dsinh:\n@@ -522,2 +523,1 @@\n-  case vmIntrinsics::_getSuperclass:\n-  case vmIntrinsics::_getClassAccessFlags:      return inline_native_Class_query(intrinsic_id());\n+  case vmIntrinsics::_getSuperclass:            return inline_native_Class_query(intrinsic_id());\n@@ -942,1 +942,5 @@\n-void LibraryCallKit::generate_string_range_check(Node* array, Node* offset, Node* count, bool char_count) {\n+void LibraryCallKit::generate_string_range_check(Node* array,\n+                                                 Node* offset,\n+                                                 Node* count,\n+                                                 bool char_count,\n+                                                 bool halt_on_oob) {\n@@ -960,4 +964,11 @@\n-    PreserveJVMState pjvms(this);\n-    set_control(_gvn.transform(bailout));\n-    uncommon_trap(Deoptimization::Reason_intrinsic,\n-                  Deoptimization::Action_maybe_recompile);\n+    if (halt_on_oob) {\n+      bailout = _gvn.transform(bailout)->as_Region();\n+      Node* frame = _gvn.transform(new ParmNode(C->start(), TypeFunc::FramePtr));\n+      Node* halt = _gvn.transform(new HaltNode(bailout, frame, \"unexpected guard failure in intrinsic\"));\n+      C->root()->add_req(halt);\n+    } else {\n+      PreserveJVMState pjvms(this);\n+      set_control(_gvn.transform(bailout));\n+      uncommon_trap(Deoptimization::Reason_intrinsic,\n+                    Deoptimization::Action_maybe_recompile);\n+    }\n@@ -1121,0 +1132,1 @@\n+\/\/ int java.lang.StringCoding#countPositives0(byte[] ba, int off, int len)\n@@ -1132,6 +1144,6 @@\n-  ba = must_be_not_null(ba, true);\n-\n-  \/\/ Range checks\n-  generate_string_range_check(ba, offset, len, false);\n-  if (stopped()) {\n-    return true;\n+  if (VerifyIntrinsicChecks) {\n+    ba = must_be_not_null(ba, true);\n+    generate_string_range_check(ba, offset, len, false, true);\n+    if (stopped()) {\n+      return true;\n+    }\n@@ -1139,0 +1151,1 @@\n+\n@@ -1803,1 +1816,1 @@\n-  Node* trig = make_runtime_call(RC_LEAF, call_type, funcAddr, funcName,\n+  Node* trig = make_runtime_call(RC_LEAF | RC_PURE, call_type, funcAddr, funcName,\n@@ -1890,0 +1903,3 @@\n+  case vmIntrinsics::_dsinh:\n+    return StubRoutines::dsinh() != nullptr ?\n+      runtime_math(OptoRuntime::Math_D_D_Type(), StubRoutines::dsinh(), \"dsinh\") : false;\n@@ -3275,1 +3291,1 @@\n-  lease_result_value->init_req(_true_path, null()); \/\/ if the lease was returned, return 0.\n+  lease_result_value->init_req(_true_path, _gvn.longcon(0)); \/\/ if the lease was returned, return 0L.\n@@ -3625,2 +3641,2 @@\n- *   Atomic::store(&tl->_contextual_tid, java_lang_Thread::tid(thread));\n- *   Atomic::store(&tl->_contextual_thread_excluded, is_excluded);\n+ *   AtomicAccess::store(&tl->_contextual_tid, java_lang_Thread::tid(thread));\n+ *   AtomicAccess::store(&tl->_contextual_thread_excluded, is_excluded);\n@@ -3629,1 +3645,1 @@\n- *     Atomic::store(&tl->_vthread_epoch, vthread_epoch);\n+ *     AtomicAccess::store(&tl->_vthread_epoch, vthread_epoch);\n@@ -3631,1 +3647,1 @@\n- *   Atomic::release_store(&tl->_vthread, true);\n+ *   AtomicAccess::release_store(&tl->_vthread, true);\n@@ -3634,1 +3650,1 @@\n- * Atomic::release_store(&tl->_vthread, false);\n+ * AtomicAccess::release_store(&tl->_vthread, false);\n@@ -4010,4 +4026,0 @@\n-  case vmIntrinsics::_getClassAccessFlags:\n-    prim_return_value = intcon(JVM_ACC_ABSTRACT | JVM_ACC_FINAL | JVM_ACC_PUBLIC);\n-    return_type = TypeInt::CHAR;\n-    break;\n@@ -4109,5 +4121,0 @@\n-  case vmIntrinsics::_getClassAccessFlags:\n-    p = basic_plus_adr(kls, in_bytes(Klass::access_flags_offset()));\n-    query_value = make_load(nullptr, p, TypeInt::CHAR, T_CHAR, MemNode::unordered);\n-    break;\n-\n@@ -4916,4 +4923,3 @@\n-      if (LockingMode == LM_LIGHTWEIGHT) {\n-        Node *monitor_val   = _gvn.MakeConX(markWord::monitor_value);\n-        Node *chk_monitor   = _gvn.transform(new CmpXNode(lmasked_header, monitor_val));\n-        Node *test_monitor  = _gvn.transform(new BoolNode(chk_monitor, BoolTest::eq));\n+      Node *monitor_val   = _gvn.MakeConX(markWord::monitor_value);\n+      Node *chk_monitor   = _gvn.transform(new CmpXNode(lmasked_header, monitor_val));\n+      Node *test_monitor  = _gvn.transform(new BoolNode(chk_monitor, BoolTest::eq));\n@@ -4921,8 +4927,1 @@\n-        generate_slow_guard(test_monitor, slow_region);\n-      } else {\n-        Node *unlocked_val      = _gvn.MakeConX(markWord::unlocked_value);\n-        Node *chk_unlocked      = _gvn.transform(new CmpXNode(lmasked_header, unlocked_val));\n-        Node *test_not_unlocked = _gvn.transform(new BoolNode(chk_unlocked, BoolTest::ne));\n-\n-        generate_slow_guard(test_not_unlocked, slow_region);\n-      }\n+      generate_slow_guard(test_monitor, slow_region);\n@@ -5694,1 +5693,1 @@\n-    C->gvn_replace_by(init->proj_out(TypeFunc::Memory), alloc_mem);\n+    init->replace_mem_projs_by(alloc_mem, C);\n@@ -5745,2 +5744,14 @@\n-    set_memory(init->proj_out_or_null(TypeFunc::Memory), Compile::AliasIdxRaw);\n-    set_memory(init->proj_out_or_null(TypeFunc::Memory), elemidx);\n+    \/\/ Need to properly move every memory projection for the Initialize\n+#ifdef ASSERT\n+    int mark_idx = C->get_alias_index(ary_type->add_offset(oopDesc::mark_offset_in_bytes()));\n+    int klass_idx = C->get_alias_index(ary_type->add_offset(Type::klass_offset()));\n+#endif\n+    auto move_proj = [&](ProjNode* proj) {\n+      int alias_idx = C->get_alias_index(proj->adr_type());\n+      assert(alias_idx == Compile::AliasIdxRaw ||\n+             alias_idx == elemidx ||\n+             alias_idx == mark_idx ||\n+             alias_idx == klass_idx, \"should be raw memory or array element type\");\n+      set_memory(proj, alias_idx);\n+    };\n+    init->for_each_proj(move_proj, TypeFunc::Memory);\n@@ -6283,0 +6294,3 @@\n+\/\/ int sun.nio.cs.ISO_8859_1.Encoder#encodeISOArray0(byte[] sa, int sp, byte[] da, int dp, int len)\n+\/\/ int java.lang.StringCoding#encodeISOArray0(byte[] sa, int sp, byte[] da, int dp, int len)\n+\/\/ int java.lang.StringCoding#encodeAsciiArray0(char[] sa, int sp, byte[] da, int dp, int len)\n@@ -6293,2 +6307,8 @@\n-  src = must_be_not_null(src, true);\n-  dst = must_be_not_null(dst, true);\n+  \/\/ Cast source & target arrays to not-null\n+  if (VerifyIntrinsicChecks) {\n+    src = must_be_not_null(src, true);\n+    dst = must_be_not_null(dst, true);\n+    if (stopped()) {\n+      return true;\n+    }\n+  }\n@@ -6311,0 +6331,9 @@\n+  \/\/ Check source & target bounds\n+  if (VerifyIntrinsicChecks) {\n+    generate_string_range_check(src, src_offset, length, src_elem == T_BYTE, true);\n+    generate_string_range_check(dst, dst_offset, length, false, true);\n+    if (stopped()) {\n+      return true;\n+    }\n+  }\n+\n@@ -7405,1 +7434,1 @@\n-  ciKlass* klass_AESCrypt = tinst->instance_klass()->find_klass(ciSymbol::make(\"com\/sun\/crypto\/provider\/AESCrypt\"));\n+  ciKlass* klass_AESCrypt = tinst->instance_klass()->find_klass(ciSymbol::make(\"com\/sun\/crypto\/provider\/AES_Crypt\"));\n@@ -7491,1 +7520,1 @@\n-  ciKlass* klass_AESCrypt = tinst->instance_klass()->find_klass(ciSymbol::make(\"com\/sun\/crypto\/provider\/AESCrypt\"));\n+  ciKlass* klass_AESCrypt = tinst->instance_klass()->find_klass(ciSymbol::make(\"com\/sun\/crypto\/provider\/AES_Crypt\"));\n@@ -7561,1 +7590,1 @@\n-  ciKlass* klass_AESCrypt = tinst->instance_klass()->find_klass(ciSymbol::make(\"com\/sun\/crypto\/provider\/AESCrypt\"));\n+  ciKlass* klass_AESCrypt = tinst->instance_klass()->find_klass(ciSymbol::make(\"com\/sun\/crypto\/provider\/AES_Crypt\"));\n@@ -7601,1 +7630,1 @@\n-  assert (objSessionK != nullptr, \"wrong version of com.sun.crypto.provider.AESCrypt\");\n+  assert (objSessionK != nullptr, \"wrong version of com.sun.crypto.provider.AES_Crypt\");\n@@ -7609,1 +7638,1 @@\n-  assert (objAESCryptKey != nullptr, \"wrong version of com.sun.crypto.provider.AESCrypt\");\n+  assert (objAESCryptKey != nullptr, \"wrong version of com.sun.crypto.provider.AES_Crypt\");\n@@ -7644,1 +7673,1 @@\n-  ciKlass* klass_AESCrypt = tinst->instance_klass()->find_klass(ciSymbol::make(\"com\/sun\/crypto\/provider\/AESCrypt\"));\n+  ciKlass* klass_AESCrypt = tinst->instance_klass()->find_klass(ciSymbol::make(\"com\/sun\/crypto\/provider\/AES_Crypt\"));\n@@ -7707,1 +7736,1 @@\n-  ciKlass* klass_AESCrypt = tinst->instance_klass()->find_klass(ciSymbol::make(\"com\/sun\/crypto\/provider\/AESCrypt\"));\n+  ciKlass* klass_AESCrypt = tinst->instance_klass()->find_klass(ciSymbol::make(\"com\/sun\/crypto\/provider\/AES_Crypt\"));\n@@ -7767,1 +7796,1 @@\n-  ciKlass* klass_AESCrypt = tinst->instance_klass()->find_klass(ciSymbol::make(\"com\/sun\/crypto\/provider\/AESCrypt\"));\n+  ciKlass* klass_AESCrypt = tinst->instance_klass()->find_klass(ciSymbol::make(\"com\/sun\/crypto\/provider\/AES_Crypt\"));\n@@ -8740,1 +8769,1 @@\n-  ciKlass* klass_AESCrypt = tinst->instance_klass()->find_klass(ciSymbol::make(\"com\/sun\/crypto\/provider\/AESCrypt\"));\n+  ciKlass* klass_AESCrypt = tinst->instance_klass()->find_klass(ciSymbol::make(\"com\/sun\/crypto\/provider\/AES_Crypt\"));\n@@ -8794,1 +8823,1 @@\n-  ciKlass* klass_AESCrypt = tinst->instance_klass()->find_klass(ciSymbol::make(\"com\/sun\/crypto\/provider\/AESCrypt\"));\n+  ciKlass* klass_AESCrypt = tinst->instance_klass()->find_klass(ciSymbol::make(\"com\/sun\/crypto\/provider\/AES_Crypt\"));\n","filename":"src\/hotspot\/share\/opto\/library_call.cpp","additions":85,"deletions":56,"binary":false,"changes":141,"status":"modified"},{"patch":"@@ -150,1 +150,1 @@\n-    Atomic::inc(&PhaseMacroExpand::_GC_barriers_removed_counter);\n+    AtomicAccess::inc(&PhaseMacroExpand::_GC_barriers_removed_counter);\n@@ -504,0 +504,5 @@\n+    } else if (mem->is_top()) {\n+      \/\/ The slice is on a dead path. Returning nullptr would lead to elimination\n+      \/\/ bailout, but we want to prevent that. Just forwarding the top is also legal,\n+      \/\/ and IGVN can just clean things up, and remove whatever receives top.\n+      return mem;\n@@ -598,0 +603,5 @@\n+          if (n->is_Mem() && n->as_Mem()->is_mismatched_access()) {\n+            DEBUG_ONLY(disq_node = n);\n+            NOT_PRODUCT(fail_eliminate = \"Mismatched access\");\n+            can_eliminate = false;\n+          }\n@@ -735,0 +745,35 @@\n+#ifdef ASSERT\n+  \/\/ Verify if a value can be written into a field.\n+  void verify_type_compatability(const Type* value_type, const Type* field_type) {\n+    BasicType value_bt = value_type->basic_type();\n+    BasicType field_bt = field_type->basic_type();\n+\n+    \/\/ Primitive types must match.\n+    if (is_java_primitive(value_bt) && value_bt == field_bt) { return; }\n+\n+    \/\/ I have been struggling to make a similar assert for non-primitive\n+    \/\/ types. I we can add one in the future. For now, I just let them\n+    \/\/ pass without checks.\n+    \/\/ In particular, I was struggling with a value that came from a call,\n+    \/\/ and had only a non-null check CastPP. There was also a checkcast\n+    \/\/ in the graph to verify the interface, but the corresponding\n+    \/\/ CheckCastPP result was not updated in the stack slot, and so\n+    \/\/ we ended up using the CastPP. That means that the field knows\n+    \/\/ that it should get an oop from an interface, but the value lost\n+    \/\/ that information, and so it is not a subtype.\n+    \/\/ There may be other issues, feel free to investigate further!\n+    if (!is_java_primitive(value_bt)) { return; }\n+\n+    tty->print_cr(\"value not compatible for field: %s vs %s\",\n+                  type2name(value_bt),\n+                  type2name(field_bt));\n+    tty->print(\"value_type: \");\n+    value_type->dump();\n+    tty->cr();\n+    tty->print(\"field_type: \");\n+    field_type->dump();\n+    tty->cr();\n+    assert(false, \"value_type does not fit field_type\");\n+  }\n+#endif\n+\n@@ -851,0 +896,1 @@\n+    DEBUG_ONLY(verify_type_compatability(field_val->bottom_type(), field_type);)\n@@ -999,1 +1045,0 @@\n-        assert(init->outcnt() <= 2, \"only a control and memory projection expected\");\n@@ -1009,3 +1054,1 @@\n-        Node *mem_proj = init->proj_out_or_null(TypeFunc::Memory);\n-        if (mem_proj != nullptr) {\n-          Node *mem = init->in(TypeFunc::Memory);\n+        Node* mem = init->in(TypeFunc::Memory);\n@@ -1013,0 +1056,1 @@\n+        if (init->number_of_projs(TypeFunc::Memory) > 0) {\n@@ -1014,1 +1058,1 @@\n-            assert(mem->in(TypeFunc::Memory) == _callprojs.fallthrough_memproj, \"allocation memory projection\");\n+            assert(mem->as_MergeMem()->memory_at(Compile::AliasIdxRaw) == _callprojs.fallthrough_memproj, \"allocation memory projection\");\n@@ -1018,2 +1062,3 @@\n-#endif\n-          _igvn.replace_node(mem_proj, mem);\n+#endif\n+        init->replace_mem_projs_by(mem, &_igvn);\n+        assert(init->outcnt() == 0, \"should only have had a control and some memory projections, and we removed them\");\n@@ -1607,1 +1652,10 @@\n-      MemBarNode* mb = MemBarNode::make(C, Op_MemBarStoreStore, Compile::AliasIdxBot);\n+      \/\/ What we want is to prevent the compiler and the CPU from re-ordering the stores that initialize this object\n+      \/\/ with subsequent stores to any slice. As a consequence, this MemBar should capture the entire memory state at\n+      \/\/ this point in the IR and produce a new memory state that should cover all slices. However, the Initialize node\n+      \/\/ only captures\/produces a partial memory state making it complicated to insert such a MemBar. Because\n+      \/\/ re-ordering by the compiler can't happen by construction (a later Store that publishes the just allocated\n+      \/\/ object reference is indirectly control dependent on the Initialize node), preventing reordering by the CPU is\n+      \/\/ sufficient. For that a MemBar on the raw memory slice is good enough.\n+      \/\/ If init is null, this allocation does have an InitializeNode but this logic can't locate it (see comment in\n+      \/\/ PhaseMacroExpand::initialize_object()).\n+      MemBarNode* mb = MemBarNode::make(C, Op_MemBarStoreStore, Compile::AliasIdxRaw);\n@@ -1623,2 +1677,2 @@\n-      Node* init_mem = init->proj_out_or_null(TypeFunc::Memory);\n-      MemBarNode* mb = MemBarNode::make(C, Op_MemBarStoreStore, Compile::AliasIdxBot);\n+      \/\/ See comment above that explains why a raw memory MemBar is good enough.\n+      MemBarNode* mb = MemBarNode::make(C, Op_MemBarStoreStore, Compile::AliasIdxRaw);\n@@ -1630,2 +1684,11 @@\n-      Node* mem = new ProjNode(init, TypeFunc::Memory);\n-      transform_later(mem);\n+      Node* old_raw_mem_proj = nullptr;\n+      auto find_raw_mem = [&](ProjNode* proj) {\n+        if (C->get_alias_index(proj->adr_type()) == Compile::AliasIdxRaw) {\n+          assert(old_raw_mem_proj == nullptr, \"only one expected\");\n+          old_raw_mem_proj = proj;\n+        }\n+      };\n+      init->for_each_proj(find_raw_mem, TypeFunc::Memory);\n+      assert(old_raw_mem_proj != nullptr, \"should have found raw mem Proj\");\n+      Node* raw_mem_proj = new ProjNode(init, TypeFunc::Memory);\n+      transform_later(raw_mem_proj);\n@@ -1635,1 +1698,1 @@\n-      mb->init_req(TypeFunc::Memory, mem);\n+      mb->init_req(TypeFunc::Memory, raw_mem_proj);\n@@ -1640,1 +1703,1 @@\n-      mem = new ProjNode(mb, TypeFunc::Memory);\n+      Node* mem = new ProjNode(mb, TypeFunc::Memory);\n@@ -1649,3 +1712,1 @@\n-      if (init_mem != nullptr) {\n-        _igvn.replace_node(init_mem, mem);\n-      }\n+      _igvn.replace_node(old_raw_mem_proj, mem);\n@@ -1856,2 +1917,1 @@\n-      \/\/ Address is aligned to execute prefetch to the beginning of cache line size\n-      \/\/ (it is important when BIS instruction is used on SPARC as prefetch).\n+      \/\/ Address is aligned to execute prefetch to the beginning of cache line size.\n@@ -2394,1 +2454,1 @@\n-          Atomic::inc(&PhaseMacroExpand::_monitor_objects_removed_counter);\n+          AtomicAccess::inc(&PhaseMacroExpand::_monitor_objects_removed_counter);\n@@ -2419,1 +2479,1 @@\n-          Atomic::inc(&PhaseMacroExpand::_objs_scalar_replaced_counter);\n+          AtomicAccess::inc(&PhaseMacroExpand::_objs_scalar_replaced_counter);\n@@ -2459,1 +2519,1 @@\n-    Atomic::add(&PhaseMacroExpand::_memory_barriers_removed_counter, membar_before - membar_after);\n+    AtomicAccess::add(&PhaseMacroExpand::_memory_barriers_removed_counter, membar_before - membar_after);\n@@ -2599,5 +2659,2 @@\n-        bool is_drem = n->Opcode() == Op_ModD;\n-        CallNode* call = new CallLeafNode(mod_macro->tf(),\n-                                          is_drem ? CAST_FROM_FN_PTR(address, SharedRuntime::drem)\n-                                                  : CAST_FROM_FN_PTR(address, SharedRuntime::frem),\n-                                          is_drem ? \"drem\" : \"frem\", TypeRawPtr::BOTTOM);\n+        CallNode* call = new CallLeafPureNode(mod_macro->tf(), mod_macro->entry_point(),\n+                                              mod_macro->_name, TypeRawPtr::BOTTOM);\n@@ -2606,4 +2663,4 @@\n-        call->init_req(TypeFunc::I_O, mod_macro->in(TypeFunc::I_O));\n-        call->init_req(TypeFunc::Memory, mod_macro->in(TypeFunc::Memory));\n-        call->init_req(TypeFunc::ReturnAdr, mod_macro->in(TypeFunc::ReturnAdr));\n-        call->init_req(TypeFunc::FramePtr, mod_macro->in(TypeFunc::FramePtr));\n+        call->init_req(TypeFunc::I_O, C->top());\n+        call->init_req(TypeFunc::Memory, C->top());\n+        call->init_req(TypeFunc::ReturnAdr, C->top());\n+        call->init_req(TypeFunc::FramePtr, C->top());\n@@ -2687,4 +2744,4 @@\n-  tty->print(\"Objects scalar replaced = %d, \", Atomic::load(&_objs_scalar_replaced_counter));\n-  tty->print(\"Monitor objects removed = %d, \", Atomic::load(&_monitor_objects_removed_counter));\n-  tty->print(\"GC barriers removed = %d, \", Atomic::load(&_GC_barriers_removed_counter));\n-  tty->print_cr(\"Memory barriers removed = %d\", Atomic::load(&_memory_barriers_removed_counter));\n+  tty->print(\"Objects scalar replaced = %d, \", AtomicAccess::load(&_objs_scalar_replaced_counter));\n+  tty->print(\"Monitor objects removed = %d, \", AtomicAccess::load(&_monitor_objects_removed_counter));\n+  tty->print(\"GC barriers removed = %d, \", AtomicAccess::load(&_GC_barriers_removed_counter));\n+  tty->print_cr(\"Memory barriers removed = %d\", AtomicAccess::load(&_memory_barriers_removed_counter));\n","filename":"src\/hotspot\/share\/opto\/macro.cpp","additions":93,"deletions":36,"binary":false,"changes":129,"status":"modified"},{"patch":"@@ -94,1 +94,1 @@\n-  dump_adr_type(this, _adr_type, st);\n+  dump_adr_type(_adr_type, st);\n@@ -111,1 +111,1 @@\n-void MemNode::dump_adr_type(const Node* mem, const TypePtr* adr_type, outputStream *st) {\n+void MemNode::dump_adr_type(const TypePtr* adr_type, outputStream* st) {\n@@ -1984,1 +1984,0 @@\n-    \/\/ (Folds up the 2nd indirection in Reflection.getClassAccessFlags(aClassConstant).)\n@@ -1990,1 +1989,0 @@\n-    \/\/ (Folds up the 2nd indirection in Reflection.getClassAccessFlags(aClassConstant).)\n@@ -2017,5 +2015,1 @@\n-  \/\/ If we are loading from a freshly-allocated object, produce a zero,\n-  \/\/ if the load is provably beyond the header of the object.\n-  \/\/ (Also allow a variable load from a fresh array to produce zero.)\n-  const TypeOopPtr* tinst = tp->isa_oopptr();\n-  bool is_instance = (tinst != nullptr) && tinst->is_known_instance_field();\n+  \/\/ If load can see a previous constant store, use that.\n@@ -2232,7 +2226,10 @@\n-  bool is_vect = (_type->isa_vect() != nullptr);\n-  if (is_instance && !is_vect) {\n-    \/\/ If we have an instance type and our memory input is the\n-    \/\/ programs's initial memory state, there is no matching store,\n-    \/\/ so just return a zero of the appropriate type -\n-    \/\/ except if it is vectorized - then we have no zero constant.\n-    Node *mem = in(MemNode::Memory);\n+  \/\/ If we are loading from a freshly-allocated object\/array, produce a zero.\n+  \/\/ Things to check:\n+  \/\/   1. Load is beyond the header: headers are not guaranteed to be zero\n+  \/\/   2. Load is not vectorized: vectors have no zero constant\n+  \/\/   3. Load has no matching store, i.e. the input is the initial memory state\n+  const TypeOopPtr* tinst = tp->isa_oopptr();\n+  bool is_not_header = (tinst != nullptr) && tinst->is_known_instance_field();\n+  bool is_not_vect = (_type->isa_vect() == nullptr);\n+  if (is_not_header && is_not_vect) {\n+    Node* mem = in(MemNode::Memory);\n@@ -4234,4 +4231,1 @@\n-  if (outcnt() != 2) {\n-    assert(Opcode() == Op_Initialize, \"Only seen when there are no use of init memory\");\n-    assert(outcnt() == 1, \"Only control then\");\n-  }\n+  assert(outcnt() > 0 && (outcnt() <= 2 || Opcode() == Op_Initialize), \"Only one or two out edges allowed\");\n@@ -4245,3 +4239,0 @@\n-  if (proj_out_or_null(TypeFunc::Memory) != nullptr) {\n-    igvn->replace_node(proj_out(TypeFunc::Memory), in(TypeFunc::Memory));\n-  }\n@@ -4251,0 +4242,7 @@\n+  if (is_Initialize()) {\n+    as_Initialize()->replace_mem_projs_by(in(TypeFunc::Memory), igvn);\n+  } else {\n+    if (proj_out_or_null(TypeFunc::Memory) != nullptr) {\n+      igvn->replace_node(proj_out(TypeFunc::Memory), in(TypeFunc::Memory));\n+    }\n+  }\n@@ -4333,1 +4331,1 @@\n-    return new MachProjNode(this,proj->_con,RegMask::Empty,MachProjNode::unmatched_proj);\n+    return new MachProjNode(this, proj->_con, RegMask::EMPTY, MachProjNode::unmatched_proj);\n@@ -4580,1 +4578,1 @@\n-  return RegMask::Empty;\n+  return RegMask::EMPTY;\n@@ -5453,0 +5451,42 @@\n+void InitializeNode::replace_mem_projs_by(Node* mem, Compile* C) {\n+  auto replace_proj = [&](ProjNode* proj) {\n+    C->gvn_replace_by(proj, mem);\n+    return CONTINUE;\n+  };\n+  apply_to_projs(replace_proj, TypeFunc::Memory);\n+}\n+\n+void InitializeNode::replace_mem_projs_by(Node* mem, PhaseIterGVN* igvn) {\n+  DUIterator_Fast imax, i = fast_outs(imax);\n+  auto replace_proj = [&](ProjNode* proj) {\n+    igvn->replace_node(proj, mem);\n+    --i; --imax;\n+    return CONTINUE;\n+  };\n+  apply_to_projs(imax, i, replace_proj, TypeFunc::Memory);\n+}\n+\n+bool InitializeNode::already_has_narrow_mem_proj_with_adr_type(const TypePtr* adr_type) const {\n+  auto find_proj = [&](ProjNode* proj) {\n+    if (proj->adr_type() == adr_type) {\n+      return BREAK_AND_RETURN_CURRENT_PROJ;\n+    }\n+    return CONTINUE;\n+  };\n+  DUIterator_Fast imax, i = fast_outs(imax);\n+  return apply_to_narrow_mem_projs_any_iterator(UsesIteratorFast(imax, i, this), find_proj) != nullptr;\n+}\n+\n+MachProjNode* InitializeNode::mem_mach_proj() const {\n+  auto find_proj = [](ProjNode* proj) {\n+    if (proj->is_MachProj()) {\n+      return BREAK_AND_RETURN_CURRENT_PROJ;\n+    }\n+    return CONTINUE;\n+  };\n+  ProjNode* proj = apply_to_projs(find_proj, TypeFunc::Memory);\n+  if (proj == nullptr) {\n+    return nullptr;\n+  }\n+  return proj->as_MachProj();\n+}\n@@ -5792,1 +5832,1 @@\n-  return RegMask::Empty;\n+  return RegMask::EMPTY;\n@@ -5875,0 +5915,1 @@\n+           || n->is_NarrowMemProj()\n","filename":"src\/hotspot\/share\/opto\/memnode.cpp","additions":67,"deletions":26,"binary":false,"changes":93,"status":"modified"},{"patch":"@@ -1152,0 +1152,7 @@\n+\n+  \/\/ Capture receiver info for compiled lambda forms.\n+  if (method()->is_compiled_lambda_form()) {\n+    ciInstance* recv_info = _caller->compute_receiver_info(method());\n+    jvms->set_receiver_info(recv_info);\n+  }\n+\n","filename":"src\/hotspot\/share\/opto\/parse1.cpp","additions":7,"deletions":0,"binary":false,"changes":7,"status":"modified"},{"patch":"@@ -1100,1 +1100,1 @@\n-  CallNode* mod = type == BasicType::T_DOUBLE ? static_cast<CallNode*>(new ModDNode(C, a, b)) : new ModFNode(C, a, b);\n+  CallLeafPureNode* mod = type == BasicType::T_DOUBLE ? static_cast<CallLeafPureNode*>(new ModDNode(C, a, b)) : new ModFNode(C, a, b);\n@@ -1102,3 +1102,3 @@\n-  Node* prev_mem = set_predefined_input_for_runtime_call(mod);\n-  mod = _gvn.transform(mod)->as_Call();\n-  set_predefined_output_for_runtime_call(mod, prev_mem, TypeRawPtr::BOTTOM);\n+  set_predefined_input_for_runtime_call(mod);\n+  mod = _gvn.transform(mod)->as_CallLeafPure();\n+  set_predefined_output_for_runtime_call(mod);\n@@ -2785,0 +2785,1 @@\n+    printer->set_parse(this);\n@@ -2787,0 +2788,1 @@\n+    printer->set_parse(nullptr);\n","filename":"src\/hotspot\/share\/opto\/parse2.cpp","additions":6,"deletions":4,"binary":false,"changes":10,"status":"modified"},{"patch":"@@ -64,1 +64,1 @@\n-#include \"runtime\/atomic.hpp\"\n+#include \"runtime\/atomicAccess.hpp\"\n@@ -860,2 +860,3 @@\n-  fields = TypeTuple::fields(0);\n-  const TypeTuple *range = TypeTuple::make(TypeFunc::Parms, fields);\n+  fields = TypeTuple::fields(1);\n+  fields[TypeFunc::Parms] = TypeInstPtr::BOTTOM;\n+  const TypeTuple *range = TypeTuple::make(TypeFunc::Parms + 1, fields);\n@@ -1783,0 +1784,56 @@\n+#ifndef PRODUCT\n+static void debug_print_convert_type(const Type** fields, int* argp, Node *parm) {\n+  const BasicType bt = parm->bottom_type()->basic_type();\n+  fields[(*argp)++] = Type::get_const_basic_type(bt);\n+  if (bt == T_LONG || bt == T_DOUBLE) {\n+    fields[(*argp)++] = Type::HALF;\n+  }\n+}\n+\n+static void update_arg_cnt(const Node* parm, int* arg_cnt) {\n+  (*arg_cnt)++;\n+  const BasicType bt = parm->bottom_type()->basic_type();\n+  if (bt == T_LONG || bt == T_DOUBLE) {\n+    (*arg_cnt)++;\n+  }\n+}\n+\n+const TypeFunc* OptoRuntime::debug_print_Type(Node* parm0, Node* parm1,\n+                                        Node* parm2, Node* parm3,\n+                                        Node* parm4, Node* parm5,\n+                                        Node* parm6) {\n+  int argcnt = 1;\n+  if (parm0 != nullptr) { update_arg_cnt(parm0, &argcnt);\n+  if (parm1 != nullptr) { update_arg_cnt(parm1, &argcnt);\n+  if (parm2 != nullptr) { update_arg_cnt(parm2, &argcnt);\n+  if (parm3 != nullptr) { update_arg_cnt(parm3, &argcnt);\n+  if (parm4 != nullptr) { update_arg_cnt(parm4, &argcnt);\n+  if (parm5 != nullptr) { update_arg_cnt(parm5, &argcnt);\n+  if (parm6 != nullptr) { update_arg_cnt(parm6, &argcnt);\n+  \/* close each nested if ===> *\/  } } } } } } }\n+\n+  \/\/ create input type (domain)\n+  const Type** fields = TypeTuple::fields(argcnt);\n+  int argp = TypeFunc::Parms;\n+  fields[argp++] = TypePtr::NOTNULL;    \/\/ static string pointer\n+\n+  if (parm0 != nullptr) { debug_print_convert_type(fields, &argp, parm0);\n+  if (parm1 != nullptr) { debug_print_convert_type(fields, &argp, parm1);\n+  if (parm2 != nullptr) { debug_print_convert_type(fields, &argp, parm2);\n+  if (parm3 != nullptr) { debug_print_convert_type(fields, &argp, parm3);\n+  if (parm4 != nullptr) { debug_print_convert_type(fields, &argp, parm4);\n+  if (parm5 != nullptr) { debug_print_convert_type(fields, &argp, parm5);\n+  if (parm6 != nullptr) { debug_print_convert_type(fields, &argp, parm6);\n+  \/* close each nested if ===> *\/  } } } } } } }\n+\n+  assert(argp == TypeFunc::Parms+argcnt, \"correct decoding\");\n+  const TypeTuple* domain = TypeTuple::make(TypeFunc::Parms+argcnt, fields);\n+\n+  \/\/ no result type needed\n+  fields = TypeTuple::fields(1);\n+  fields[TypeFunc::Parms+0] = nullptr; \/\/ void\n+  const TypeTuple* range = TypeTuple::make(TypeFunc::Parms, fields);\n+  return TypeFunc::make(domain, range);\n+}\n+#endif \/\/ PRODUCT\n+\n@@ -1916,3 +1973,0 @@\n-\n-    \/\/ Check if the exception PC is a MethodHandle call site.\n-    current->set_is_method_handle_return(nm->is_method_handle_return(pc));\n@@ -2209,1 +2263,1 @@\n-  } while (Atomic::cmpxchg(&_named_counters, head, c) != head);\n+  } while (AtomicAccess::cmpxchg(&_named_counters, head, c) != head);\n","filename":"src\/hotspot\/share\/opto\/runtime.cpp","additions":61,"deletions":7,"binary":false,"changes":68,"status":"modified"},{"patch":"@@ -697,0 +697,5 @@\n+\n+  const Type* joined = r0->join(r1);\n+  if (joined == Type::TOP) {\n+    return TypeInt::CC_NE;\n+  }\n@@ -801,0 +806,6 @@\n+\n+  const Type* joined = r0->join(r1);\n+  if (joined == Type::TOP) {\n+    return TypeInt::CC_NE;\n+  }\n+\n@@ -942,0 +953,6 @@\n+\n+  const Type* joined = r0->join(r1);\n+  if (joined == Type::TOP) {\n+    return TypeInt::CC_NE;\n+  }\n+\n@@ -996,0 +1013,5 @@\n+  const Type* joined = r0->join(r1);\n+  if (joined == Type::TOP) {\n+    return TypeInt::CC_NE;\n+  }\n+\n@@ -1371,0 +1393,4 @@\n+  if( CC == TypeInt::CC_NE ) {\n+    if( _test == ne ) return TypeInt::ONE;\n+    if( _test == eq ) return TypeInt::ZERO;\n+  }\n@@ -1375,0 +1401,15 @@\n+BoolTest::mask BoolTest::unsigned_mask(BoolTest::mask btm) {\n+  switch(btm) {\n+    case eq:\n+    case ne:\n+      return btm;\n+    case lt:\n+    case le:\n+    case gt:\n+    case ge:\n+      return mask(btm | unsigned_compare);\n+    default:\n+      ShouldNotReachHere();\n+  }\n+}\n+\n@@ -2072,3 +2113,9 @@\n-Node* InvolutionNode::Identity(PhaseGVN* phase) {\n-  \/\/ Op ( Op x ) => x\n-  if (in(1)->Opcode() == Opcode()) {\n+Node* ReverseINode::Identity(PhaseGVN* phase) {\n+  if (in(1)->Opcode() == Op_ReverseI) {\n+    return in(1)->in(1);\n+  }\n+  return this;\n+}\n+\n+Node* ReverseLNode::Identity(PhaseGVN* phase) {\n+  if (in(1)->Opcode() == Op_ReverseL) {\n","filename":"src\/hotspot\/share\/opto\/subnode.cpp","additions":50,"deletions":3,"binary":false,"changes":53,"status":"modified"},{"patch":"@@ -48,0 +48,2 @@\n+#include \"utilities\/debug.hpp\"\n+#include \"utilities\/ostream.hpp\"\n@@ -2982,5 +2984,2 @@\n-  if( _ptr == Null ) st->print(\"null\");\n-  else st->print(\"%s *\", ptr_msg[_ptr]);\n-  if( _offset == OffsetTop ) st->print(\"+top\");\n-  else if( _offset == OffsetBot ) st->print(\"+bot\");\n-  else if( _offset ) st->print(\"+%d\", _offset);\n+  st->print(\"ptr:%s\", ptr_msg[_ptr]);\n+  dump_offset(st);\n@@ -2991,0 +2990,10 @@\n+void TypePtr::dump_offset(outputStream* st) const {\n+  if (_offset == OffsetBot) {\n+    st->print(\"+bot\");\n+  } else if (_offset == OffsetTop) {\n+    st->print(\"+top\");\n+  } else {\n+    st->print(\"+%d\", _offset);\n+  }\n+}\n+\n@@ -3162,4 +3171,4 @@\n-void TypeRawPtr::dump2( Dict &d, uint depth, outputStream *st ) const {\n-  if( _ptr == Constant )\n-    st->print(INTPTR_FORMAT, p2i(_bits));\n-  else\n+void TypeRawPtr::dump2(Dict& d, uint depth, outputStream* st) const {\n+  if (_ptr == Constant) {\n+    st->print(\"rawptr:Constant:\" INTPTR_FORMAT, p2i(_bits));\n+  } else {\n@@ -3167,0 +3176,1 @@\n+  }\n@@ -3481,1 +3491,1 @@\n-          ciField* field = nullptr;\n+          BasicType basic_elem_type = T_ILLEGAL;\n@@ -3484,1 +3494,1 @@\n-            field = k->get_field_by_offset(_offset, true);\n+            basic_elem_type = k->get_field_type_by_offset(_offset, true);\n@@ -3486,2 +3496,1 @@\n-          if (field != nullptr) {\n-            BasicType basic_elem_type = field->layout_type();\n+          if (basic_elem_type != T_ILLEGAL) {\n@@ -3495,3 +3504,2 @@\n-          ciField* field = ik->get_field_by_offset(_offset, false);\n-          if (field != nullptr) {\n-            BasicType basic_elem_type = field->layout_type();\n+          BasicType basic_elem_type = ik->get_field_type_by_offset(_offset, false);\n+          if (basic_elem_type != T_ILLEGAL) {\n@@ -3803,1 +3811,1 @@\n-void TypeOopPtr::dump2( Dict &d, uint depth, outputStream *st ) const {\n+void TypeOopPtr::dump2(Dict& d, uint depth, outputStream* st) const {\n@@ -3805,13 +3813,8 @@\n-  if( _klass_is_exact ) st->print(\":exact\");\n-  if( const_oop() ) st->print(INTPTR_FORMAT, p2i(const_oop()));\n-  switch( _offset ) {\n-  case OffsetTop: st->print(\"+top\"); break;\n-  case OffsetBot: st->print(\"+any\"); break;\n-  case         0: break;\n-  default:        st->print(\"+%d\",_offset); break;\n-  }\n-  if (_instance_id == InstanceTop)\n-    st->print(\",iid=top\");\n-  else if (_instance_id != InstanceBot)\n-    st->print(\",iid=%d\",_instance_id);\n-\n+  if (_klass_is_exact) {\n+    st->print(\":exact\");\n+  }\n+  if (const_oop() != nullptr) {\n+    st->print(\":\" INTPTR_FORMAT, p2i(const_oop()));\n+  }\n+  dump_offset(st);\n+  dump_instance_id(st);\n@@ -3821,0 +3824,10 @@\n+\n+void TypeOopPtr::dump_instance_id(outputStream* st) const {\n+  if (_instance_id == InstanceTop) {\n+    st->print(\",iid=top\");\n+  } else if (_instance_id == InstanceBot) {\n+    st->print(\",iid=bot\");\n+  } else {\n+    st->print(\",iid=%d\", _instance_id);\n+  }\n+}\n@@ -4458,0 +4471,1 @@\n+  st->print(\"instptr:\");\n@@ -4461,28 +4475,3 @@\n-  switch( _ptr ) {\n-  case Constant:\n-    if (WizardMode || Verbose) {\n-      ResourceMark rm;\n-      stringStream ss;\n-\n-      st->print(\" \");\n-      const_oop()->print_oop(&ss);\n-      \/\/ 'const_oop->print_oop()' may emit newlines('\\n') into ss.\n-      \/\/ suppress newlines from it so -XX:+Verbose -XX:+PrintIdeal dumps one-liner for each node.\n-      char* buf = ss.as_string(\/* c_heap= *\/false);\n-      StringUtils::replace_no_expand(buf, \"\\n\", \"\");\n-      st->print_raw(buf);\n-    }\n-  case BotPTR:\n-    if (!WizardMode && !Verbose) {\n-      if( _klass_is_exact ) st->print(\":exact\");\n-      break;\n-    }\n-  case TopPTR:\n-  case AnyNull:\n-  case NotNull:\n-    st->print(\":%s\", ptr_msg[_ptr]);\n-    if( _klass_is_exact ) st->print(\":exact\");\n-    break;\n-  default:\n-    break;\n-  }\n+  if (_ptr == Constant && (WizardMode || Verbose)) {\n+    ResourceMark rm;\n+    stringStream ss;\n@@ -4490,4 +4479,7 @@\n-  if( _offset ) {               \/\/ Dump offset, if any\n-    if( _offset == OffsetBot )      st->print(\"+any\");\n-    else if( _offset == OffsetTop ) st->print(\"+unknown\");\n-    else st->print(\"+%d\", _offset);\n+    st->print(\" \");\n+    const_oop()->print_oop(&ss);\n+    \/\/ 'const_oop->print_oop()' may emit newlines('\\n') into ss.\n+    \/\/ suppress newlines from it so -XX:+Verbose -XX:+PrintIdeal dumps one-liner for each node.\n+    char* buf = ss.as_string(\/* c_heap= *\/false);\n+    StringUtils::replace_no_expand(buf, \"\\n\", \"\");\n+    st->print_raw(buf);\n@@ -4496,5 +4488,4 @@\n-  st->print(\" *\");\n-  if (_instance_id == InstanceTop)\n-    st->print(\",iid=top\");\n-  else if (_instance_id != InstanceBot)\n-    st->print(\",iid=%d\",_instance_id);\n+  st->print(\":%s\", ptr_msg[_ptr]);\n+  if (_klass_is_exact) {\n+    st->print(\":exact\");\n+  }\n@@ -4502,0 +4493,2 @@\n+  dump_offset(st);\n+  dump_instance_id(st);\n@@ -5094,1 +5087,2 @@\n-  _ary->dump2(d,depth,st);\n+  st->print(\"aryptr:\");\n+  _ary->dump2(d, depth, st);\n@@ -5097,2 +5091,1 @@\n-  switch( _ptr ) {\n-  case Constant:\n+  if (_ptr == Constant) {\n@@ -5100,14 +5093,5 @@\n-    break;\n-  case BotPTR:\n-    if (!WizardMode && !Verbose) {\n-      if( _klass_is_exact ) st->print(\":exact\");\n-      break;\n-    }\n-  case TopPTR:\n-  case AnyNull:\n-  case NotNull:\n-    st->print(\":%s\", ptr_msg[_ptr]);\n-    if( _klass_is_exact ) st->print(\":exact\");\n-    break;\n-  default:\n-    break;\n+  }\n+\n+  st->print(\":%s\", ptr_msg[_ptr]);\n+  if (_klass_is_exact) {\n+    st->print(\":exact\");\n@@ -5131,5 +5115,1 @@\n-  st->print(\" *\");\n-  if (_instance_id == InstanceTop)\n-    st->print(\",iid=top\");\n-  else if (_instance_id != InstanceBot)\n-    st->print(\",iid=%d\",_instance_id);\n+  dump_instance_id(st);\n@@ -5495,6 +5475,2 @@\n-  if( metadata() ) st->print(INTPTR_FORMAT, p2i(metadata()));\n-  switch( _offset ) {\n-  case OffsetTop: st->print(\"+top\"); break;\n-  case OffsetBot: st->print(\"+any\"); break;\n-  case         0: break;\n-  default:        st->print(\"+%d\",_offset); break;\n+  if (metadata() != nullptr) {\n+    st->print(\":\" INTPTR_FORMAT, p2i(metadata()));\n@@ -5502,0 +5478,1 @@\n+  dump_offset(st);\n@@ -5649,38 +5626,0 @@\n-\/\/------------------------------dump2------------------------------------------\n-\/\/ Dump Klass Type\n-#ifndef PRODUCT\n-void TypeKlassPtr::dump2(Dict & d, uint depth, outputStream *st) const {\n-  switch(_ptr) {\n-  case Constant:\n-    st->print(\"precise \");\n-  case NotNull:\n-    {\n-      const char *name = klass()->name()->as_utf8();\n-      if (name) {\n-        st->print(\"%s: \" INTPTR_FORMAT, name, p2i(klass()));\n-      } else {\n-        ShouldNotReachHere();\n-      }\n-      _interfaces->dump(st);\n-    }\n-  case BotPTR:\n-    if (!WizardMode && !Verbose && _ptr != Constant) break;\n-  case TopPTR:\n-  case AnyNull:\n-    st->print(\":%s\", ptr_msg[_ptr]);\n-    if (_ptr == Constant) st->print(\":exact\");\n-    break;\n-  default:\n-    break;\n-  }\n-\n-  if (_offset) {               \/\/ Dump offset, if any\n-    if (_offset == OffsetBot)      { st->print(\"+any\"); }\n-    else if (_offset == OffsetTop) { st->print(\"+unknown\"); }\n-    else                            { st->print(\"+%d\", _offset); }\n-  }\n-\n-  st->print(\" *\");\n-}\n-#endif\n-\n@@ -6041,0 +5980,9 @@\n+#ifndef PRODUCT\n+void TypeInstKlassPtr::dump2(Dict& d, uint depth, outputStream* st) const {\n+  st->print(\"instklassptr:\");\n+  klass()->print_name_on(st);\n+  _interfaces->dump(st);\n+  st->print(\":%s\", ptr_msg[_ptr]);\n+  dump_offset(st);\n+}\n+#endif \/\/ PRODUCT\n@@ -6512,28 +6460,5 @@\n-  switch( _ptr ) {\n-  case Constant:\n-    st->print(\"precise \");\n-  case NotNull:\n-    {\n-      st->print(\"[\");\n-      _elem->dump2(d, depth, st);\n-      _interfaces->dump(st);\n-      st->print(\": \");\n-    }\n-  case BotPTR:\n-    if( !WizardMode && !Verbose && _ptr != Constant ) break;\n-  case TopPTR:\n-  case AnyNull:\n-    st->print(\":%s\", ptr_msg[_ptr]);\n-    if( _ptr == Constant ) st->print(\":exact\");\n-    break;\n-  default:\n-    break;\n-  }\n-\n-  if( _offset ) {               \/\/ Dump offset, if any\n-    if( _offset == OffsetBot )      { st->print(\"+any\"); }\n-    else if( _offset == OffsetTop ) { st->print(\"+unknown\"); }\n-    else                            { st->print(\"+%d\", _offset); }\n-  }\n-\n-  st->print(\" *\");\n+  st->print(\"aryklassptr:[\");\n+  _elem->dump2(d, depth, st);\n+  _interfaces->dump(st);\n+  st->print(\":%s\", ptr_msg[_ptr]);\n+  dump_offset(st);\n","filename":"src\/hotspot\/share\/opto\/type.cpp","additions":86,"deletions":161,"binary":false,"changes":247,"status":"modified"},{"patch":"@@ -1190,3 +1190,0 @@\n-#ifndef PRODUCT\n-  void dump_speculative(outputStream *st) const;\n-#endif\n@@ -1197,0 +1194,1 @@\n+\n@@ -1198,1 +1196,3 @@\n-  void dump_inline_depth(outputStream *st) const;\n+  void dump_speculative(outputStream* st) const;\n+  void dump_inline_depth(outputStream* st) const;\n+  void dump_offset(outputStream* st) const;\n@@ -1378,0 +1378,4 @@\n+#ifndef PRODUCT\n+  void dump_instance_id(outputStream* st) const;\n+#endif \/\/ PRODUCT\n+\n@@ -1846,3 +1850,0 @@\n-#ifndef PRODUCT\n-  virtual void dump2( Dict &d, uint depth, outputStream *st ) const; \/\/ Specialized per-Type dumping\n-#endif\n@@ -1928,0 +1929,5 @@\n+\n+#ifndef PRODUCT\n+  virtual void dump2(Dict& d, uint depth, outputStream* st) const;\n+#endif \/\/ PRODUCT\n+\n","filename":"src\/hotspot\/share\/opto\/type.hpp","additions":13,"deletions":7,"binary":false,"changes":20,"status":"modified"},{"patch":"@@ -34,0 +34,2 @@\n+#include \"cppstdlib\/limits.hpp\"\n+#include \"gc\/shared\/gc_globals.hpp\"\n@@ -77,2 +79,0 @@\n-#include <limits>\n-\n@@ -320,0 +320,4 @@\n+#define ENABLE_FINAL_FIELD_MUTATION \"enable.final.field.mutation\"\n+#define ENABLE_FINAL_FIELD_MUTATION_LEN 27\n+#define ILLEGAL_FINAL_FIELD_MUTATION \"illegal.final.field.mutation\"\n+#define ILLEGAL_FINAL_FIELD_MUTATION_LEN 28\n@@ -346,1 +350,3 @@\n-        matches_property_suffix(property_suffix, ILLEGAL_NATIVE_ACCESS, ILLEGAL_NATIVE_ACCESS_LEN)) {\n+        matches_property_suffix(property_suffix, ILLEGAL_NATIVE_ACCESS, ILLEGAL_NATIVE_ACCESS_LEN) ||\n+        matches_property_suffix(property_suffix, ENABLE_FINAL_FIELD_MUTATION, ENABLE_FINAL_FIELD_MUTATION_LEN) ||\n+        matches_property_suffix(property_suffix, ILLEGAL_FINAL_FIELD_MUTATION, ILLEGAL_FINAL_FIELD_MUTATION_LEN)) {\n@@ -533,1 +539,1 @@\n-  { \"UseCompressedClassPointers\",   JDK_Version::jdk(25),  JDK_Version::jdk(26), JDK_Version::undefined() },\n+  { \"UseCompressedClassPointers\",   JDK_Version::jdk(25),  JDK_Version::jdk(27), JDK_Version::undefined() },\n@@ -538,0 +544,4 @@\n+  { \"MaxRAM\",                       JDK_Version::jdk(26),  JDK_Version::jdk(27), JDK_Version::jdk(28) },\n+  { \"AggressiveHeap\",               JDK_Version::jdk(26),  JDK_Version::jdk(27), JDK_Version::jdk(28) },\n+  { \"NeverActAsServerClassMachine\", JDK_Version::jdk(26),  JDK_Version::jdk(27), JDK_Version::jdk(28) },\n+  { \"AlwaysActAsServerClassMachine\", JDK_Version::jdk(26),  JDK_Version::jdk(27), JDK_Version::jdk(28) },\n@@ -547,2 +557,2 @@\n-  { \"ZGenerational\",                JDK_Version::jdk(23), JDK_Version::jdk(24), JDK_Version::undefined() },\n-  { \"ZMarkStackSpaceLimit\",         JDK_Version::undefined(), JDK_Version::jdk(25), JDK_Version::undefined() },\n+  { \"G1UpdateBufferSize\",           JDK_Version::undefined(), JDK_Version::jdk(26), JDK_Version::jdk(27) },\n+  { \"ShenandoahPacing\",             JDK_Version::jdk(25), JDK_Version::jdk(26), JDK_Version::jdk(27) },\n@@ -553,0 +563,20 @@\n+  { \"AdaptiveSizeMajorGCDecayTimeScale\",                JDK_Version::undefined(), JDK_Version::jdk(26), JDK_Version::jdk(27) },\n+  { \"AdaptiveSizePolicyInitializingSteps\",              JDK_Version::undefined(), JDK_Version::jdk(26), JDK_Version::jdk(27) },\n+  { \"AdaptiveSizePolicyOutputInterval\",                 JDK_Version::undefined(), JDK_Version::jdk(26), JDK_Version::jdk(27) },\n+  { \"AdaptiveSizeThroughPutPolicy\",                     JDK_Version::undefined(), JDK_Version::jdk(26), JDK_Version::jdk(27) },\n+  { \"AdaptiveTimeWeight\",                               JDK_Version::undefined(), JDK_Version::jdk(26), JDK_Version::jdk(27) },\n+  { \"PausePadding\",                                     JDK_Version::undefined(), JDK_Version::jdk(26), JDK_Version::jdk(27) },\n+  { \"SurvivorPadding\",                                  JDK_Version::undefined(), JDK_Version::jdk(26), JDK_Version::jdk(27) },\n+  { \"TenuredGenerationSizeIncrement\",                   JDK_Version::undefined(), JDK_Version::jdk(26), JDK_Version::jdk(27) },\n+  { \"TenuredGenerationSizeSupplement\",                  JDK_Version::undefined(), JDK_Version::jdk(26), JDK_Version::jdk(27) },\n+  { \"TenuredGenerationSizeSupplementDecay\",             JDK_Version::undefined(), JDK_Version::jdk(26), JDK_Version::jdk(27) },\n+  { \"UseAdaptiveGenerationSizePolicyAtMajorCollection\", JDK_Version::undefined(), JDK_Version::jdk(26), JDK_Version::jdk(27) },\n+  { \"UseAdaptiveGenerationSizePolicyAtMinorCollection\", JDK_Version::undefined(), JDK_Version::jdk(26), JDK_Version::jdk(27) },\n+  { \"UseAdaptiveSizeDecayMajorGCCost\",                  JDK_Version::undefined(), JDK_Version::jdk(26), JDK_Version::jdk(27) },\n+  { \"UseAdaptiveSizePolicyFootprintGoal\",               JDK_Version::undefined(), JDK_Version::jdk(26), JDK_Version::jdk(27) },\n+  { \"UseAdaptiveSizePolicyWithSystemGC\",                JDK_Version::undefined(), JDK_Version::jdk(26), JDK_Version::jdk(27) },\n+  { \"UsePSAdaptiveSurvivorSizePolicy\",                  JDK_Version::undefined(), JDK_Version::jdk(26), JDK_Version::jdk(27) },\n+\n+  { \"PretenureSizeThreshold\",       JDK_Version::undefined(), JDK_Version::jdk(26), JDK_Version::jdk(27) },\n+  { \"HeapMaximumCompactionInterval\",JDK_Version::undefined(), JDK_Version::jdk(26), JDK_Version::jdk(27) },\n+\n@@ -1455,0 +1485,1 @@\n+  assert(is_power_of_2(_conservative_max_heap_alignment), \"Expected to be a power-of-2\");\n@@ -1473,13 +1504,10 @@\n-  size_t max_allocatable;\n-  size_t result = limit;\n-  if (os::has_allocatable_memory_limit(&max_allocatable)) {\n-    \/\/ The AggressiveHeap check is a temporary workaround to avoid calling\n-    \/\/ GCarguments::heap_virtual_to_physical_ratio() before a GC has been\n-    \/\/ selected. This works because AggressiveHeap implies UseParallelGC\n-    \/\/ where we know the ratio will be 1. Once the AggressiveHeap option is\n-    \/\/ removed, this can be cleaned up.\n-    size_t heap_virtual_to_physical_ratio = (AggressiveHeap ? 1 : GCConfig::arguments()->heap_virtual_to_physical_ratio());\n-    size_t fraction = MaxVirtMemFraction * heap_virtual_to_physical_ratio;\n-    result = MIN2(result, max_allocatable \/ fraction);\n-  }\n-  return result;\n+  \/\/ The AggressiveHeap check is a temporary workaround to avoid calling\n+  \/\/ GCarguments::heap_virtual_to_physical_ratio() before a GC has been\n+  \/\/ selected. This works because AggressiveHeap implies UseParallelGC\n+  \/\/ where we know the ratio will be 1. Once the AggressiveHeap option is\n+  \/\/ removed, this can be cleaned up.\n+  size_t heap_virtual_to_physical_ratio = (AggressiveHeap ? 1 : GCConfig::arguments()->heap_virtual_to_physical_ratio());\n+  size_t fraction = MaxVirtMemFraction * heap_virtual_to_physical_ratio;\n+  size_t max_allocatable = os::commit_memory_limit();\n+\n+  return MIN2(limit, max_allocatable \/ fraction);\n@@ -1491,0 +1519,4 @@\n+static size_t clamp_by_size_t_max(uint64_t value) {\n+  return (size_t)MIN2(value, (uint64_t)std::numeric_limits<size_t>::max());\n+}\n+\n@@ -1492,16 +1524,10 @@\n-  julong phys_mem;\n-\n-  \/\/ If the user specified one of these options, they\n-  \/\/ want specific memory sizing so do not limit memory\n-  \/\/ based on compressed oops addressability.\n-  \/\/ Also, memory limits will be calculated based on\n-  \/\/ available os physical memory, not our MaxRAM limit,\n-  \/\/ unless MaxRAM is also specified.\n-  bool override_coop_limit = (!FLAG_IS_DEFAULT(MaxRAMPercentage) ||\n-                           !FLAG_IS_DEFAULT(MinRAMPercentage) ||\n-                           !FLAG_IS_DEFAULT(InitialRAMPercentage) ||\n-                           !FLAG_IS_DEFAULT(MaxRAM));\n-  if (override_coop_limit) {\n-    if (FLAG_IS_DEFAULT(MaxRAM)) {\n-      phys_mem = os::physical_memory();\n-      FLAG_SET_ERGO(MaxRAM, (uint64_t)phys_mem);\n+  \/\/ Check if the user has configured any limit on the amount of RAM we may use.\n+  bool has_ram_limit = !FLAG_IS_DEFAULT(MaxRAMPercentage) ||\n+                       !FLAG_IS_DEFAULT(MinRAMPercentage) ||\n+                       !FLAG_IS_DEFAULT(InitialRAMPercentage) ||\n+                       !FLAG_IS_DEFAULT(MaxRAM);\n+\n+  if (FLAG_IS_DEFAULT(MaxRAM)) {\n+    if (CompilerConfig::should_set_client_emulation_mode_flags()) {\n+      \/\/ Limit the available memory if client emulation mode is enabled.\n+      FLAG_SET_ERGO(MaxRAM, 1ULL*G);\n@@ -1509,1 +1535,2 @@\n-      phys_mem = (julong)MaxRAM;\n+      \/\/ Use the available physical memory on the system.\n+      FLAG_SET_ERGO(MaxRAM, os::physical_memory());\n@@ -1511,3 +1538,0 @@\n-  } else {\n-    phys_mem = FLAG_IS_DEFAULT(MaxRAM) ? MIN2(os::physical_memory(), (julong)MaxRAM)\n-                                       : (julong)MaxRAM;\n@@ -1516,3 +1540,3 @@\n-  \/\/ If the maximum heap size has not been set with -Xmx,\n-  \/\/ then set it as fraction of the size of physical memory,\n-  \/\/ respecting the maximum and minimum sizes of the heap.\n+  \/\/ If the maximum heap size has not been set with -Xmx, then set it as\n+  \/\/ fraction of the size of physical memory, respecting the maximum and\n+  \/\/ minimum sizes of the heap.\n@@ -1520,2 +1544,6 @@\n-    julong reasonable_max = (julong)(((double)phys_mem * MaxRAMPercentage) \/ 100);\n-    const julong reasonable_min = (julong)(((double)phys_mem * MinRAMPercentage) \/ 100);\n+    uint64_t min_memory = (uint64_t)(((double)MaxRAM * MinRAMPercentage) \/ 100);\n+    uint64_t max_memory = (uint64_t)(((double)MaxRAM * MaxRAMPercentage) \/ 100);\n+\n+    const size_t reasonable_min = clamp_by_size_t_max(min_memory);\n+    size_t reasonable_max = clamp_by_size_t_max(max_memory);\n+\n@@ -1528,1 +1556,1 @@\n-      reasonable_max = MAX2(reasonable_max, (julong)MaxHeapSize);\n+      reasonable_max = MAX2(reasonable_max, MaxHeapSize);\n@@ -1533,1 +1561,1 @@\n-      reasonable_max = MIN2(reasonable_max, (julong)ErgoHeapSizeLimit);\n+      reasonable_max = MIN2(reasonable_max, ErgoHeapSizeLimit);\n@@ -1543,1 +1571,1 @@\n-      reasonable_max = MAX2(reasonable_max, (julong)InitialHeapSize);\n+      reasonable_max = MAX2(reasonable_max, InitialHeapSize);\n@@ -1545,1 +1573,1 @@\n-      reasonable_max = MAX2(reasonable_max, (julong)MinHeapSize);\n+      reasonable_max = MAX2(reasonable_max, MinHeapSize);\n@@ -1554,2 +1582,2 @@\n-          log_debug(gc, heap, coops)(\"HeapBaseMinAddress must be at least %zu\"\n-                                     \" (%zuG) which is greater than value given %zu\",\n+          log_debug(gc, heap, coops)(\"HeapBaseMinAddress must be at least %zu \"\n+                                     \"(%zuG) which is greater than value given %zu\",\n@@ -1563,0 +1591,1 @@\n+\n@@ -1564,2 +1593,2 @@\n-      \/\/ Limit the heap size to the maximum possible when using compressed oops\n-      julong max_coop_heap = (julong)max_heap_for_compressed_oops();\n+      size_t heap_end = HeapBaseMinAddress + MaxHeapSize;\n+      size_t max_coop_heap = max_heap_for_compressed_oops();\n@@ -1567,3 +1596,4 @@\n-      if (HeapBaseMinAddress + MaxHeapSize < max_coop_heap) {\n-        \/\/ Heap should be above HeapBaseMinAddress to get zero based compressed oops\n-        \/\/ but it should be not less than default MaxHeapSize.\n+      \/\/ Limit the heap size to the maximum possible when using compressed oops\n+      if (heap_end < max_coop_heap) {\n+        \/\/ Heap should be above HeapBaseMinAddress to get zero based compressed\n+        \/\/ oops but it should be not less than default MaxHeapSize.\n@@ -1573,4 +1603,3 @@\n-      \/\/ If user specified flags prioritizing os physical\n-      \/\/ memory limits, then disable compressed oops if\n-      \/\/ limits exceed max_coop_heap and UseCompressedOops\n-      \/\/ was not specified.\n+      \/\/ If the user has configured any limit on the amount of RAM we may use,\n+      \/\/ then disable compressed oops if the calculated max exceeds max_coop_heap\n+      \/\/ and UseCompressedOops was not specified.\n@@ -1578,5 +1607,5 @@\n-        if (FLAG_IS_ERGO(UseCompressedOops) && override_coop_limit) {\n-          aot_log_info(aot)(\"UseCompressedOops and UseCompressedClassPointers have been disabled due to\"\n-            \" max heap %zu > compressed oop heap %zu. \"\n-            \"Please check the setting of MaxRAMPercentage %5.2f.\"\n-            ,(size_t)reasonable_max, (size_t)max_coop_heap, MaxRAMPercentage);\n+        if (FLAG_IS_ERGO(UseCompressedOops) && has_ram_limit) {\n+          aot_log_info(aot)(\"UseCompressedOops disabled due to \"\n+                            \"max heap %zu > compressed oop heap %zu. \"\n+                            \"Please check the setting of MaxRAMPercentage %5.2f.\",\n+                            reasonable_max, max_coop_heap, MaxRAMPercentage);\n@@ -1585,1 +1614,1 @@\n-          reasonable_max = MIN2(reasonable_max, max_coop_heap);\n+          reasonable_max = max_coop_heap;\n@@ -1591,2 +1620,2 @@\n-    log_trace(gc, heap)(\"  Maximum heap size %zu\", (size_t) reasonable_max);\n-    FLAG_SET_ERGO(MaxHeapSize, (size_t)reasonable_max);\n+    log_trace(gc, heap)(\"  Maximum heap size %zu\", reasonable_max);\n+    FLAG_SET_ERGO(MaxHeapSize, reasonable_max);\n@@ -1598,4 +1627,2 @@\n-    julong reasonable_minimum = (julong)(OldSize + NewSize);\n-\n-    reasonable_minimum = MIN2(reasonable_minimum, (julong)MaxHeapSize);\n-\n+    size_t reasonable_minimum = clamp_by_size_t_max((uint64_t)OldSize + (uint64_t)NewSize);\n+    reasonable_minimum = MIN2(reasonable_minimum, MaxHeapSize);\n@@ -1605,1 +1632,2 @@\n-      julong reasonable_initial = (julong)(((double)phys_mem * InitialRAMPercentage) \/ 100);\n+      uint64_t initial_memory = (uint64_t)(((double)MaxRAM * InitialRAMPercentage) \/ 100);\n+      size_t reasonable_initial = clamp_by_size_t_max(initial_memory);\n@@ -1608,2 +1636,2 @@\n-      reasonable_initial = MAX3(reasonable_initial, reasonable_minimum, (julong)MinHeapSize);\n-      reasonable_initial = MIN2(reasonable_initial, (julong)MaxHeapSize);\n+      reasonable_initial = MAX3(reasonable_initial, reasonable_minimum, MinHeapSize);\n+      reasonable_initial = MIN2(reasonable_initial, MaxHeapSize);\n@@ -1614,0 +1642,1 @@\n+\n@@ -1617,1 +1646,1 @@\n-      FLAG_SET_ERGO(MinHeapSize, MIN2((size_t)reasonable_minimum, InitialHeapSize));\n+      FLAG_SET_ERGO(MinHeapSize, MIN2(reasonable_minimum, InitialHeapSize));\n@@ -1634,1 +1663,2 @@\n-  julong total_memory = os::physical_memory();\n+  physical_memory_size_type phys_mem = os::physical_memory();\n+  julong total_memory = static_cast<julong>(phys_mem);\n@@ -1789,0 +1819,1 @@\n+static unsigned int enable_final_field_mutation = 0;\n@@ -1840,18 +1871,0 @@\n-  if (UseObjectMonitorTable && LockingMode != LM_LIGHTWEIGHT) {\n-    \/\/ ObjectMonitorTable requires lightweight locking.\n-    FLAG_SET_CMDLINE(UseObjectMonitorTable, false);\n-    warning(\"UseObjectMonitorTable requires LM_LIGHTWEIGHT\");\n-  }\n-\n-#if !defined(X86) && !defined(AARCH64) && !defined(PPC64) && !defined(RISCV64) && !defined(S390)\n-  if (LockingMode == LM_MONITOR) {\n-    jio_fprintf(defaultStream::error_stream(),\n-                \"LockingMode == 0 (LM_MONITOR) is not fully implemented on this architecture\\n\");\n-    return false;\n-  }\n-#endif\n-  if (VerifyHeavyMonitors && LockingMode != LM_MONITOR) {\n-    jio_fprintf(defaultStream::error_stream(),\n-                \"-XX:+VerifyHeavyMonitors requires LockingMode == 0 (LM_MONITOR)\\n\");\n-    return false;\n-  }\n@@ -2271,0 +2284,13 @@\n+    } else if (match_option(option, \"--enable-final-field-mutation=\", &tail)) {\n+      if (!create_numbered_module_property(\"jdk.module.enable.final.field.mutation\", tail, enable_final_field_mutation++)) {\n+        return JNI_ENOMEM;\n+      }\n+    } else if (match_option(option, \"--illegal-final-field-mutation=\", &tail)) {\n+      if (strcmp(tail, \"allow\") == 0 || strcmp(tail, \"warn\") == 0 || strcmp(tail, \"debug\") == 0 || strcmp(tail, \"deny\") == 0) {\n+        PropertyList_unique_add(&_system_properties, \"jdk.module.illegal.final.field.mutation\", tail,\n+                                AddProperty, WriteableProperty, InternalProperty);\n+      } else {\n+        jio_fprintf(defaultStream::error_stream(),\n+                    \"Value specified to --illegal-final-field-mutation not recognized: '%s'\\n\", tail);\n+        return JNI_ERR;\n+      }\n@@ -2400,1 +2426,1 @@\n-      int maxf = (int)(strtod(tail, &err) * 100);\n+      double dmaxf = strtod(tail, &err);\n@@ -2403,1 +2429,1 @@\n-                    \"Bad max heap free percentage size: %s\\n\",\n+                    \"Bad max heap free ratio: %s\\n\",\n@@ -2406,4 +2432,16 @@\n-      } else {\n-        if (FLAG_SET_CMDLINE(MaxHeapFreeRatio, maxf) != JVMFlag::SUCCESS) {\n-            return JNI_EINVAL;\n-        }\n+      }\n+      if (dmaxf < 0.0 || dmaxf > 1.0) {\n+        jio_fprintf(defaultStream::error_stream(),\n+                    \"-Xmaxf value (%s) is outside the allowed range [ 0.0 ... 1.0 ]\\n\",\n+                    option->optionString);\n+        return JNI_EINVAL;\n+      }\n+      const uintx umaxf = (uintx)(dmaxf * 100);\n+      if (MinHeapFreeRatio > umaxf) {\n+        jio_fprintf(defaultStream::error_stream(),\n+                    \"-Xmaxf value (%s) must be greater than or equal to the implicit -Xminf value (%.2f)\\n\",\n+                    tail, MinHeapFreeRatio \/ 100.0f);\n+        return JNI_EINVAL;\n+      }\n+      if (FLAG_SET_CMDLINE(MaxHeapFreeRatio, umaxf) != JVMFlag::SUCCESS) {\n+        return JNI_EINVAL;\n@@ -2414,1 +2452,1 @@\n-      int minf = (int)(strtod(tail, &err) * 100);\n+      double dminf = strtod(tail, &err);\n@@ -2417,1 +2455,1 @@\n-                    \"Bad min heap free percentage size: %s\\n\",\n+                    \"Bad min heap free ratio: %s\\n\",\n@@ -2420,4 +2458,16 @@\n-      } else {\n-        if (FLAG_SET_CMDLINE(MinHeapFreeRatio, minf) != JVMFlag::SUCCESS) {\n-          return JNI_EINVAL;\n-        }\n+      }\n+      if (dminf < 0.0 || dminf > 1.0) {\n+        jio_fprintf(defaultStream::error_stream(),\n+                    \"-Xminf value (%s) is outside the allowed range [ 0.0 ... 1.0 ]\\n\",\n+                    tail);\n+        return JNI_EINVAL;\n+      }\n+      const uintx uminf = (uintx)(dminf * 100);\n+      if (MaxHeapFreeRatio < uminf) {\n+        jio_fprintf(defaultStream::error_stream(),\n+                    \"-Xminf value (%s) must be less than or equal to the implicit -Xmaxf value (%.2f)\\n\",\n+                    tail, MaxHeapFreeRatio \/ 100.0f);\n+        return JNI_EINVAL;\n+      }\n+      if (FLAG_SET_CMDLINE(MinHeapFreeRatio, uminf) != JVMFlag::SUCCESS) {\n+        return JNI_EINVAL;\n@@ -2437,0 +2487,3 @@\n+      if (match_option(option, \"-Xmaxjitcodesize\", &tail)) {\n+        warning(\"Option -Xmaxjitcodesize was deprecated in JDK 26 and will likely be removed in a future release.\");\n+      }\n","filename":"src\/hotspot\/share\/runtime\/arguments.cpp","additions":159,"deletions":106,"binary":false,"changes":265,"status":"modified"},{"patch":"@@ -36,1 +36,1 @@\n-#include \"runtime\/atomic.hpp\"\n+#include \"runtime\/atomicAccess.hpp\"\n@@ -44,1 +44,0 @@\n-#include \"runtime\/lightweightSynchronizer.hpp\"\n@@ -47,1 +46,0 @@\n-#include \"runtime\/objectMonitor.hpp\"\n@@ -55,1 +53,1 @@\n-#include \"runtime\/synchronizer.inline.hpp\"\n+#include \"runtime\/synchronizer.hpp\"\n@@ -58,0 +56,1 @@\n+#include \"runtime\/timerTrace.hpp\"\n@@ -62,0 +61,2 @@\n+#include \"utilities\/concurrentHashTable.inline.hpp\"\n+#include \"utilities\/concurrentHashTableTasks.inline.hpp\"\n@@ -64,0 +65,1 @@\n+#include \"utilities\/fastHash.hpp\"\n@@ -66,1 +68,0 @@\n-#include \"utilities\/fastHash.hpp\"\n@@ -75,1 +76,1 @@\n-    head = Atomic::load(&_head);\n+    head = AtomicAccess::load(&_head);\n@@ -77,1 +78,1 @@\n-  } while (Atomic::cmpxchg(&_head, head, m) != head);\n+  } while (AtomicAccess::cmpxchg(&_head, head, m) != head);\n@@ -79,1 +80,1 @@\n-  size_t count = Atomic::add(&_count, 1u, memory_order_relaxed);\n+  size_t count = AtomicAccess::add(&_count, 1u, memory_order_relaxed);\n@@ -82,1 +83,1 @@\n-    old_max = Atomic::load(&_max);\n+    old_max = AtomicAccess::load(&_max);\n@@ -86,1 +87,1 @@\n-  } while (Atomic::cmpxchg(&_max, old_max, count, memory_order_relaxed) != old_max);\n+  } while (AtomicAccess::cmpxchg(&_max, old_max, count, memory_order_relaxed) != old_max);\n@@ -90,1 +91,1 @@\n-  return Atomic::load(&_count);\n+  return AtomicAccess::load(&_count);\n@@ -94,1 +95,1 @@\n-  return Atomic::load(&_max);\n+  return AtomicAccess::load(&_max);\n@@ -115,1 +116,1 @@\n-  ObjectMonitor* m = Atomic::load_acquire(&_head);\n+  ObjectMonitor* m = AtomicAccess::load_acquire(&_head);\n@@ -136,1 +137,1 @@\n-        if (prev == nullptr && Atomic::load(&_head) != m) {\n+        if (prev == nullptr && AtomicAccess::load(&_head) != m) {\n@@ -148,1 +149,1 @@\n-        ObjectMonitor* prev_head = Atomic::cmpxchg(&_head, m, next);\n+        ObjectMonitor* prev_head = AtomicAccess::cmpxchg(&_head, m, next);\n@@ -160,1 +161,1 @@\n-        assert(Atomic::load(&_head) != m, \"Sanity\");\n+        assert(AtomicAccess::load(&_head) != m, \"Sanity\");\n@@ -185,1 +186,1 @@\n-    ObjectMonitor* m = Atomic::load_acquire(&_head);\n+    ObjectMonitor* m = AtomicAccess::load_acquire(&_head);\n@@ -193,1 +194,1 @@\n-  Atomic::sub(&_count, unlinked_count);\n+  AtomicAccess::sub(&_count, unlinked_count);\n@@ -198,1 +199,1 @@\n-  return Iterator(Atomic::load_acquire(&_head));\n+  return Iterator(AtomicAccess::load_acquire(&_head));\n@@ -286,3 +287,1 @@\n-  if (LockingMode == LM_LIGHTWEIGHT) {\n-    LightweightSynchronizer::initialize();\n-  }\n+  ObjectSynchronizer::create_om_table();\n@@ -347,12 +346,4 @@\n-  if (LockingMode == LM_LIGHTWEIGHT) {\n-    if (mark.is_fast_locked() && current->lock_stack().contains(cast_to_oop(obj))) {\n-      \/\/ Degenerate notify\n-      \/\/ fast-locked by caller so by definition the implied waitset is empty.\n-      return true;\n-    }\n-  } else if (LockingMode == LM_LEGACY) {\n-    if (mark.has_locker() && current->is_lock_owned((address)mark.locker())) {\n-      \/\/ Degenerate notify\n-      \/\/ stack-locked by caller so by definition the implied waitset is empty.\n-      return true;\n-    }\n+  if (mark.is_fast_locked() && current->lock_stack().contains(cast_to_oop(obj))) {\n+    \/\/ Degenerate notify\n+    \/\/ fast-locked by caller so by definition the implied waitset is empty.\n+    return true;\n@@ -363,1 +354,1 @@\n-    if (LockingMode == LM_LIGHTWEIGHT && mon == nullptr) {\n+    if (mon == nullptr) {\n@@ -386,73 +377,0 @@\n-static bool useHeavyMonitors() {\n-#if defined(X86) || defined(AARCH64) || defined(PPC64) || defined(RISCV64) || defined(S390)\n-  return LockingMode == LM_MONITOR;\n-#else\n-  return false;\n-#endif\n-}\n-\n-\/\/ The LockNode emitted directly at the synchronization site would have\n-\/\/ been too big if it were to have included support for the cases of inflated\n-\/\/ recursive enter and exit, so they go here instead.\n-\/\/ Note that we can't safely call AsyncPrintJavaStack() from within\n-\/\/ quick_enter() as our thread state remains _in_Java.\n-\n-bool ObjectSynchronizer::quick_enter_legacy(oop obj, BasicLock* lock, JavaThread* current) {\n-  assert(current->thread_state() == _thread_in_Java, \"invariant\");\n-\n-  if (useHeavyMonitors()) {\n-    return false;  \/\/ Slow path\n-  }\n-\n-  assert(LockingMode == LM_LEGACY, \"legacy mode below\");\n-\n-  const markWord mark = obj->mark();\n-\n-  if (mark.has_monitor()) {\n-\n-    ObjectMonitor* const m = read_monitor(mark);\n-    \/\/ An async deflation or GC can race us before we manage to make\n-    \/\/ the ObjectMonitor busy by setting the owner below. If we detect\n-    \/\/ that race we just bail out to the slow-path here.\n-    if (m->object_peek() == nullptr) {\n-      return false;\n-    }\n-\n-    \/\/ Lock contention and Transactional Lock Elision (TLE) diagnostics\n-    \/\/ and observability\n-    \/\/ Case: light contention possibly amenable to TLE\n-    \/\/ Case: TLE inimical operations such as nested\/recursive synchronization\n-\n-    if (m->has_owner(current)) {\n-      m->increment_recursions(current);\n-      current->inc_held_monitor_count();\n-      return true;\n-    }\n-\n-    \/\/ This Java Monitor is inflated so obj's header will never be\n-    \/\/ displaced to this thread's BasicLock. Make the displaced header\n-    \/\/ non-null so this BasicLock is not seen as recursive nor as\n-    \/\/ being locked. We do this unconditionally so that this thread's\n-    \/\/ BasicLock cannot be mis-interpreted by any stack walkers. For\n-    \/\/ performance reasons, stack walkers generally first check for\n-    \/\/ stack-locking in the object's header, the second check is for\n-    \/\/ recursive stack-locking in the displaced header in the BasicLock,\n-    \/\/ and last are the inflated Java Monitor (ObjectMonitor) checks.\n-    lock->set_displaced_header(markWord::unused_mark());\n-\n-    if (!m->has_owner() && m->try_set_owner(current)) {\n-      assert(m->recursions() == 0, \"invariant\");\n-      current->inc_held_monitor_count();\n-      return true;\n-    }\n-  }\n-\n-  \/\/ Note that we could inflate in quick_enter.\n-  \/\/ This is likely a useful optimization\n-  \/\/ Critically, in quick_enter() we must not:\n-  \/\/ -- block indefinitely, or\n-  \/\/ -- reach a safepoint\n-\n-  return false;        \/\/ revert to slow-path\n-}\n-\n@@ -509,148 +427,0 @@\n-\/\/ -----------------------------------------------------------------------------\n-\/\/ Monitor Enter\/Exit\n-\n-void ObjectSynchronizer::enter_for(Handle obj, BasicLock* lock, JavaThread* locking_thread) {\n-  \/\/ When called with locking_thread != Thread::current() some mechanism must synchronize\n-  \/\/ the locking_thread with respect to the current thread. Currently only used when\n-  \/\/ deoptimizing and re-locking locks. See Deoptimization::relock_objects\n-  assert(locking_thread == Thread::current() || locking_thread->is_obj_deopt_suspend(), \"must be\");\n-\n-  if (LockingMode == LM_LIGHTWEIGHT) {\n-    return LightweightSynchronizer::enter_for(obj, lock, locking_thread);\n-  }\n-\n-  if (!enter_fast_impl(obj, lock, locking_thread)) {\n-    \/\/ Inflated ObjectMonitor::enter_for is required\n-\n-    \/\/ An async deflation can race after the inflate_for() call and before\n-    \/\/ enter_for() can make the ObjectMonitor busy. enter_for() returns false\n-    \/\/ if we have lost the race to async deflation and we simply try again.\n-    while (true) {\n-      ObjectMonitor* monitor = inflate_for(locking_thread, obj(), inflate_cause_monitor_enter);\n-      if (monitor->enter_for(locking_thread)) {\n-        return;\n-      }\n-      assert(monitor->is_being_async_deflated(), \"must be\");\n-    }\n-  }\n-}\n-\n-void ObjectSynchronizer::enter_legacy(Handle obj, BasicLock* lock, JavaThread* current) {\n-  if (!enter_fast_impl(obj, lock, current)) {\n-    \/\/ Inflated ObjectMonitor::enter is required\n-\n-    \/\/ An async deflation can race after the inflate() call and before\n-    \/\/ enter() can make the ObjectMonitor busy. enter() returns false if\n-    \/\/ we have lost the race to async deflation and we simply try again.\n-    while (true) {\n-      ObjectMonitor* monitor = inflate(current, obj(), inflate_cause_monitor_enter);\n-      if (monitor->enter(current)) {\n-        return;\n-      }\n-    }\n-  }\n-}\n-\n-\/\/ The interpreter and compiler assembly code tries to lock using the fast path\n-\/\/ of this algorithm. Make sure to update that code if the following function is\n-\/\/ changed. The implementation is extremely sensitive to race condition. Be careful.\n-bool ObjectSynchronizer::enter_fast_impl(Handle obj, BasicLock* lock, JavaThread* locking_thread) {\n-  assert(LockingMode != LM_LIGHTWEIGHT, \"Use LightweightSynchronizer\");\n-\n-  if (obj->klass()->is_value_based()) {\n-    handle_sync_on_value_based_class(obj, locking_thread);\n-  }\n-\n-  locking_thread->inc_held_monitor_count();\n-\n-  if (!useHeavyMonitors()) {\n-    if (LockingMode == LM_LEGACY) {\n-      markWord mark = obj->mark();\n-      if (mark.is_unlocked()) {\n-        \/\/ Anticipate successful CAS -- the ST of the displaced mark must\n-        \/\/ be visible <= the ST performed by the CAS.\n-        lock->set_displaced_header(mark);\n-        if (mark == obj()->cas_set_mark(markWord::from_pointer(lock), mark)) {\n-          return true;\n-        }\n-      } else if (mark.has_locker() &&\n-                 locking_thread->is_lock_owned((address) mark.locker())) {\n-        assert(lock != mark.locker(), \"must not re-lock the same lock\");\n-        assert(lock != (BasicLock*) obj->mark().value(), \"don't relock with same BasicLock\");\n-        lock->set_displaced_header(markWord::from_pointer(nullptr));\n-        return true;\n-      }\n-\n-      \/\/ The object header will never be displaced to this lock,\n-      \/\/ so it does not matter what the value is, except that it\n-      \/\/ must be non-zero to avoid looking like a re-entrant lock,\n-      \/\/ and must not look locked either.\n-      lock->set_displaced_header(markWord::unused_mark());\n-\n-      \/\/ Failed to fast lock.\n-      return false;\n-    }\n-  } else if (VerifyHeavyMonitors) {\n-    guarantee((obj->mark().value() & markWord::lock_mask_in_place) != markWord::locked_value, \"must not be lightweight\/stack-locked\");\n-  }\n-\n-  return false;\n-}\n-\n-void ObjectSynchronizer::exit_legacy(oop object, BasicLock* lock, JavaThread* current) {\n-  assert(LockingMode != LM_LIGHTWEIGHT, \"Use LightweightSynchronizer\");\n-\n-  if (!useHeavyMonitors()) {\n-    markWord mark = object->mark();\n-    if (LockingMode == LM_LEGACY) {\n-      markWord dhw = lock->displaced_header();\n-      if (dhw.value() == 0) {\n-        \/\/ If the displaced header is null, then this exit matches up with\n-        \/\/ a recursive enter. No real work to do here except for diagnostics.\n-#ifndef PRODUCT\n-        if (mark != markWord::INFLATING()) {\n-          \/\/ Only do diagnostics if we are not racing an inflation. Simply\n-          \/\/ exiting a recursive enter of a Java Monitor that is being\n-          \/\/ inflated is safe; see the has_monitor() comment below.\n-          assert(!mark.is_unlocked(), \"invariant\");\n-          assert(!mark.has_locker() ||\n-                 current->is_lock_owned((address)mark.locker()), \"invariant\");\n-          if (mark.has_monitor()) {\n-            \/\/ The BasicLock's displaced_header is marked as a recursive\n-            \/\/ enter and we have an inflated Java Monitor (ObjectMonitor).\n-            \/\/ This is a special case where the Java Monitor was inflated\n-            \/\/ after this thread entered the stack-lock recursively. When a\n-            \/\/ Java Monitor is inflated, we cannot safely walk the Java\n-            \/\/ Monitor owner's stack and update the BasicLocks because a\n-            \/\/ Java Monitor can be asynchronously inflated by a thread that\n-            \/\/ does not own the Java Monitor.\n-            ObjectMonitor* m = read_monitor(mark);\n-            assert(m->object()->mark() == mark, \"invariant\");\n-            assert(m->is_entered(current), \"invariant\");\n-          }\n-        }\n-#endif\n-        return;\n-      }\n-\n-      if (mark == markWord::from_pointer(lock)) {\n-        \/\/ If the object is stack-locked by the current thread, try to\n-        \/\/ swing the displaced header from the BasicLock back to the mark.\n-        assert(dhw.is_neutral(), \"invariant\");\n-        if (object->cas_set_mark(dhw, mark) == mark) {\n-          return;\n-        }\n-      }\n-    }\n-  } else if (VerifyHeavyMonitors) {\n-    guarantee((object->mark().value() & markWord::lock_mask_in_place) != markWord::locked_value, \"must not be lightweight\/stack-locked\");\n-  }\n-\n-  \/\/ We have to take the slow-path of possible inflation and then exit.\n-  \/\/ The ObjectMonitor* can't be async deflated until ownership is\n-  \/\/ dropped inside exit() and the ObjectMonitor* must be !is_busy().\n-  ObjectMonitor* monitor = inflate(current, object, inflate_cause_vm_internal);\n-  assert(!monitor->has_anonymous_owner(), \"must not be\");\n-  monitor->exit(current);\n-}\n-\n@@ -675,12 +445,2 @@\n-    ObjectMonitor* monitor;\n-    bool entered;\n-    if (LockingMode == LM_LIGHTWEIGHT) {\n-      BasicLock lock;\n-      entered = LightweightSynchronizer::inflate_and_enter(obj(), &lock, inflate_cause_jni_enter, current, current) != nullptr;\n-    } else {\n-      monitor = inflate(current, obj(), inflate_cause_jni_enter);\n-      entered = monitor->enter(current);\n-    }\n-\n-    if (entered) {\n-      current->inc_held_monitor_count(1, true);\n+    BasicLock lock;\n+    if (ObjectSynchronizer::inflate_and_enter(obj(), &lock, inflate_cause_jni_enter, current, current) != nullptr) {\n@@ -698,7 +458,1 @@\n-  if (LockingMode == LM_LIGHTWEIGHT) {\n-    monitor = LightweightSynchronizer::inflate_locked_or_imse(obj, inflate_cause_jni_exit, CHECK);\n-  } else {\n-    \/\/ The ObjectMonitor* can't be async deflated until ownership is\n-    \/\/ dropped inside exit() and the ObjectMonitor* must be !is_busy().\n-    monitor = inflate(current, obj, inflate_cause_jni_exit);\n-  }\n+  monitor = ObjectSynchronizer::inflate_locked_or_imse(obj, inflate_cause_jni_exit, CHECK);\n@@ -710,1 +464,0 @@\n-    current->dec_held_monitor_count(1, true);\n@@ -717,2 +470,4 @@\n-ObjectLocker::ObjectLocker(Handle obj, JavaThread* thread) : _npm(thread) {\n-  _thread = thread;\n+ObjectLocker::ObjectLocker(Handle obj, TRAPS) : _thread(THREAD), _obj(obj),\n+  _npm(_thread, _thread->at_preemptable_init() \/* ignore_mark *\/), _skip_exit(false) {\n+  assert(!_thread->preempting(), \"\");\n+\n@@ -720,1 +475,0 @@\n-  _obj = obj;\n@@ -724,0 +478,14 @@\n+\n+    if (_thread->preempting()) {\n+      \/\/ If preemption was cancelled we acquired the monitor after freezing\n+      \/\/ the frames. Redoing the vm call laterin thaw will require us to\n+      \/\/ release it since the call should look like the original one. We\n+      \/\/ do it in ~ObjectLocker to reduce the window of time we hold the\n+      \/\/ monitor since we can't do anything useful with it now, and would\n+      \/\/ otherwise just force other vthreads to preempt in case they try\n+      \/\/ to acquire this monitor.\n+      _skip_exit = !_thread->preemption_cancelled();\n+      ObjectSynchronizer::read_monitor(_thread, _obj())->set_object_strong();\n+      _thread->set_pending_preempted_exception();\n+\n+    }\n@@ -728,1 +496,1 @@\n-  if (_obj() != nullptr) {\n+  if (_obj() != nullptr && !_skip_exit) {\n@@ -733,0 +501,8 @@\n+void ObjectLocker::wait_uninterruptibly(TRAPS) {\n+  ObjectSynchronizer::waitUninterruptibly(_obj, 0, _thread);\n+  if (_thread->preempting()) {\n+    _skip_exit = true;\n+    ObjectSynchronizer::read_monitor(_thread, _obj())->set_object_strong();\n+    _thread->set_pending_preempted_exception();\n+  }\n+}\n@@ -745,8 +521,1 @@\n-  if (LockingMode == LM_LIGHTWEIGHT) {\n-    monitor = LightweightSynchronizer::inflate_locked_or_imse(obj(), inflate_cause_wait, CHECK_0);\n-  } else {\n-    \/\/ The ObjectMonitor* can't be async deflated because the _waiters\n-    \/\/ field is incremented before ownership is dropped and decremented\n-    \/\/ after ownership is regained.\n-    monitor = inflate(current, obj(), inflate_cause_wait);\n-  }\n+  monitor = ObjectSynchronizer::inflate_locked_or_imse(obj(), inflate_cause_wait, CHECK_0);\n@@ -766,3 +535,1 @@\n-  if (millis < 0) {\n-    THROW_MSG(vmSymbols::java_lang_IllegalArgumentException(), \"timeout value is negative\");\n-  }\n+  assert(millis >= 0, \"timeout value is negative\");\n@@ -771,5 +538,1 @@\n-  if (LockingMode == LM_LIGHTWEIGHT) {\n-    monitor = LightweightSynchronizer::inflate_locked_or_imse(obj(), inflate_cause_wait, CHECK);\n-  } else {\n-    monitor = inflate(THREAD, obj(), inflate_cause_wait);\n-  }\n+  monitor = ObjectSynchronizer::inflate_locked_or_imse(obj(), inflate_cause_wait, CHECK);\n@@ -784,19 +547,3 @@\n-  if (LockingMode == LM_LIGHTWEIGHT) {\n-    if ((mark.is_fast_locked() && current->lock_stack().contains(obj()))) {\n-      \/\/ Not inflated so there can't be any waiters to notify.\n-      return;\n-    }\n-  } else if (LockingMode == LM_LEGACY) {\n-    if (mark.has_locker() && current->is_lock_owned((address)mark.locker())) {\n-      \/\/ Not inflated so there can't be any waiters to notify.\n-      return;\n-    }\n-  }\n-\n-  ObjectMonitor* monitor;\n-  if (LockingMode == LM_LIGHTWEIGHT) {\n-    monitor = LightweightSynchronizer::inflate_locked_or_imse(obj(), inflate_cause_notify, CHECK);\n-  } else {\n-    \/\/ The ObjectMonitor* can't be async deflated until ownership is\n-    \/\/ dropped by the calling thread.\n-    monitor = inflate(current, obj(), inflate_cause_notify);\n+  if ((mark.is_fast_locked() && current->lock_stack().contains(obj()))) {\n+    \/\/ Not inflated so there can't be any waiters to notify.\n+    return;\n@@ -804,0 +551,1 @@\n+  ObjectMonitor* monitor = ObjectSynchronizer::inflate_locked_or_imse(obj(), inflate_cause_notify, CHECK);\n@@ -812,10 +560,3 @@\n-  if (LockingMode == LM_LIGHTWEIGHT) {\n-    if ((mark.is_fast_locked() && current->lock_stack().contains(obj()))) {\n-      \/\/ Not inflated so there can't be any waiters to notify.\n-      return;\n-    }\n-  } else if (LockingMode == LM_LEGACY) {\n-    if (mark.has_locker() && current->is_lock_owned((address)mark.locker())) {\n-      \/\/ Not inflated so there can't be any waiters to notify.\n-      return;\n-    }\n+  if ((mark.is_fast_locked() && current->lock_stack().contains(obj()))) {\n+    \/\/ Not inflated so there can't be any waiters to notify.\n+    return;\n@@ -824,8 +565,1 @@\n-  ObjectMonitor* monitor;\n-  if (LockingMode == LM_LIGHTWEIGHT) {\n-    monitor = LightweightSynchronizer::inflate_locked_or_imse(obj(), inflate_cause_notify, CHECK);\n-  } else {\n-    \/\/ The ObjectMonitor* can't be async deflated until ownership is\n-    \/\/ dropped by the calling thread.\n-    monitor = inflate(current, obj(), inflate_cause_notify);\n-  }\n+  ObjectMonitor* monitor = ObjectSynchronizer::inflate_locked_or_imse(obj(), inflate_cause_notify, CHECK);\n@@ -851,61 +585,0 @@\n-static markWord read_stable_mark(oop obj) {\n-  markWord mark = obj->mark_acquire();\n-  if (!mark.is_being_inflated() || LockingMode == LM_LIGHTWEIGHT) {\n-    \/\/ New lightweight locking does not use the markWord::INFLATING() protocol.\n-    return mark;       \/\/ normal fast-path return\n-  }\n-\n-  int its = 0;\n-  for (;;) {\n-    markWord mark = obj->mark_acquire();\n-    if (!mark.is_being_inflated()) {\n-      return mark;    \/\/ normal fast-path return\n-    }\n-\n-    \/\/ The object is being inflated by some other thread.\n-    \/\/ The caller of read_stable_mark() must wait for inflation to complete.\n-    \/\/ Avoid live-lock.\n-\n-    ++its;\n-    if (its > 10000 || !os::is_MP()) {\n-      if (its & 1) {\n-        os::naked_yield();\n-      } else {\n-        \/\/ Note that the following code attenuates the livelock problem but is not\n-        \/\/ a complete remedy.  A more complete solution would require that the inflating\n-        \/\/ thread hold the associated inflation lock.  The following code simply restricts\n-        \/\/ the number of spinners to at most one.  We'll have N-2 threads blocked\n-        \/\/ on the inflationlock, 1 thread holding the inflation lock and using\n-        \/\/ a yield\/park strategy, and 1 thread in the midst of inflation.\n-        \/\/ A more refined approach would be to change the encoding of INFLATING\n-        \/\/ to allow encapsulation of a native thread pointer.  Threads waiting for\n-        \/\/ inflation to complete would use CAS to push themselves onto a singly linked\n-        \/\/ list rooted at the markword.  Once enqueued, they'd loop, checking a per-thread flag\n-        \/\/ and calling park().  When inflation was complete the thread that accomplished inflation\n-        \/\/ would detach the list and set the markword to inflated with a single CAS and\n-        \/\/ then for each thread on the list, set the flag and unpark() the thread.\n-\n-        \/\/ Index into the lock array based on the current object address.\n-        static_assert(is_power_of_2(inflation_lock_count()), \"must be\");\n-        size_t ix = (cast_from_oop<intptr_t>(obj) >> 5) & (inflation_lock_count() - 1);\n-        int YieldThenBlock = 0;\n-        assert(ix < inflation_lock_count(), \"invariant\");\n-        inflation_lock(ix)->lock();\n-        while (obj->mark_acquire() == markWord::INFLATING()) {\n-          \/\/ Beware: naked_yield() is advisory and has almost no effect on some platforms\n-          \/\/ so we periodically call current->_ParkEvent->park(1).\n-          \/\/ We use a mixed spin\/yield\/block mechanism.\n-          if ((YieldThenBlock++) >= 16) {\n-            Thread::current()->_ParkEvent->park(1);\n-          } else {\n-            os::naked_yield();\n-          }\n-        }\n-        inflation_lock(ix)->unlock();\n-      }\n-    } else {\n-      SpinPause();       \/\/ SMP-polite spinning\n-    }\n-  }\n-}\n-\n@@ -979,5 +652,6 @@\n-static intptr_t install_hash_code(Thread* current, oop obj) {\n-  assert(UseObjectMonitorTable && LockingMode == LM_LIGHTWEIGHT, \"must be\");\n-\n-  markWord mark = obj->mark_acquire();\n-  for (;;) {\n+intptr_t ObjectSynchronizer::FastHashCode(Thread* current, oop obj) {\n+  while (true) {\n+    ObjectMonitor* monitor = nullptr;\n+    markWord temp, test;\n+    intptr_t hash;\n+    markWord mark = obj->mark_acquire();\n@@ -986,1 +660,1 @@\n-        return LightweightSynchronizer::get_hash(mark, obj);\n+        return get_hash(mark, obj);\n@@ -988,1 +662,1 @@\n-      intptr_t hash = ObjectSynchronizer::get_next_hash(current, obj);  \/\/ get a new hash\n+      intptr_t hash = get_next_hash(current, obj);  \/\/ get a new hash\n@@ -1001,36 +675,5 @@\n-      mark = old_mark;\n-    } else {\n-      intptr_t hash = mark.hash();\n-      if (hash != 0) {\n-        return hash;\n-      }\n-\n-      hash = ObjectSynchronizer::get_next_hash(current, obj);\n-      const markWord old_mark = mark;\n-      const markWord new_mark = old_mark.copy_set_hash(hash);\n-\n-      mark = obj->cas_set_mark(new_mark, old_mark);\n-      if (old_mark == mark) {\n-        return hash;\n-      }\n-    }\n-  }\n-}\n-\n-intptr_t ObjectSynchronizer::FastHashCode(Thread* current, oop obj) {\n-  if (UseObjectMonitorTable) {\n-    \/\/ Since the monitor isn't in the object header, the hash can simply be\n-    \/\/ installed in the object header.\n-    return install_hash_code(current, obj);\n-  }\n-\n-  while (true) {\n-    ObjectMonitor* monitor = nullptr;\n-    markWord temp, test;\n-    intptr_t hash;\n-    markWord mark = read_stable_mark(obj);\n-    if (VerifyHeavyMonitors) {\n-      assert(LockingMode == LM_MONITOR, \"+VerifyHeavyMonitors requires LockingMode == 0 (LM_MONITOR)\");\n-      guarantee((obj->mark().value() & markWord::lock_mask_in_place) != markWord::locked_value, \"must not be lightweight\/stack-locked\");\n-    }\n-    if (mark.is_unlocked() || (LockingMode == LM_LIGHTWEIGHT && mark.is_fast_locked())) {\n+      \/\/ CAS failed, retry.\n+      continue;\n+    } else if (UseObjectMonitorTable || !mark.has_monitor()) {\n+      \/\/ If UseObjectMonitorTable is set the hash can simply be installed in the\n+      \/\/ object header, since the monitor isn't in the object header.\n@@ -1048,4 +691,3 @@\n-      if (LockingMode == LM_LIGHTWEIGHT) {\n-        \/\/ CAS failed, retry\n-        continue;\n-      }\n+      \/\/ CAS failed, retry\n+      continue;\n+\n@@ -1056,1 +698,2 @@\n-    } else if (mark.has_monitor()) {\n+    } else {\n+      assert(!mark.is_unlocked() && !mark.is_fast_locked(), \"invariant\");\n@@ -1083,19 +726,0 @@\n-    } else if (LockingMode == LM_LEGACY && mark.has_locker()\n-               && current->is_Java_thread()\n-               && JavaThread::cast(current)->is_lock_owned((address)mark.locker())) {\n-      \/\/ This is a stack-lock owned by the calling thread so fetch the\n-      \/\/ displaced markWord from the BasicLock on the stack.\n-      temp = mark.displaced_mark_helper();\n-      assert(temp.is_neutral(), \"invariant: header=\" INTPTR_FORMAT, temp.value());\n-      hash = temp.hash();\n-      if (hash != 0) {                  \/\/ if it has a hash, just return it\n-        return hash;\n-      }\n-      \/\/ WARNING:\n-      \/\/ The displaced header in the BasicLock on a thread's stack\n-      \/\/ is strictly immutable. It CANNOT be changed in ANY cases.\n-      \/\/ So we have to inflate the stack-lock into an ObjectMonitor\n-      \/\/ even if the current thread owns the lock. The BasicLock on\n-      \/\/ a thread's stack can be asynchronously read by other threads\n-      \/\/ during an inflate() call so any change to that stack memory\n-      \/\/ may not propagate to other threads correctly.\n@@ -1104,3 +728,0 @@\n-    \/\/ Inflate the monitor to set the hash.\n-\n-    \/\/ There's no need to inflate if the mark has already got a monitor.\n@@ -1110,1 +731,3 @@\n-    monitor = mark.has_monitor() ? mark.monitor() : inflate(current, obj, inflate_cause_hash_code);\n+    assert(mark.has_monitor(), \"must be\");\n+    monitor = mark.monitor();\n+\n@@ -1119,1 +742,1 @@\n-      uintptr_t v = Atomic::cmpxchg(monitor->metadata_addr(), mark.value(), temp.value());\n+      uintptr_t v = AtomicAccess::cmpxchg(monitor->metadata_addr(), mark.value(), temp.value());\n@@ -1145,0 +768,19 @@\n+\n+uint32_t ObjectSynchronizer::get_hash(markWord mark, oop obj, Klass* klass) {\n+  assert(UseCompactObjectHeaders, \"Only with compact i-hash\");\n+  \/\/assert(mark.is_neutral() | mark.is_fast_locked(), \"only from neutral or fast-locked mark: \" INTPTR_FORMAT, mark.value());\n+  assert(mark.is_hashed(), \"only from hashed or copied object\");\n+  if (mark.is_hashed_expanded()) {\n+    return obj->int_field(klass->hash_offset_in_bytes(obj, mark));\n+  } else {\n+    assert(mark.is_hashed_not_expanded(), \"must be hashed\");\n+    assert(hashCode == 6 || hashCode == 2, \"must have idempotent hashCode\");\n+    \/\/ Already marked as hashed, but not yet copied. Recompute hash and return it.\n+    return ObjectSynchronizer::get_next_hash(nullptr, obj); \/\/ recompute hash\n+  }\n+}\n+\n+uint32_t ObjectSynchronizer::get_hash(markWord mark, oop obj) {\n+  return get_hash(mark, obj, mark.klass());\n+}\n+\n@@ -1150,6 +792,1 @@\n-  markWord mark = read_stable_mark(obj);\n-\n-  if (LockingMode == LM_LEGACY && mark.has_locker()) {\n-    \/\/ stack-locked case, header points into owner's stack\n-    return current->is_lock_owned((address)mark.locker());\n-  }\n+  markWord mark = obj->mark_acquire();\n@@ -1157,1 +794,1 @@\n-  if (LockingMode == LM_LIGHTWEIGHT && mark.is_fast_locked()) {\n+  if (mark.is_fast_locked()) {\n@@ -1162,1 +799,1 @@\n-  while (LockingMode == LM_LIGHTWEIGHT && mark.has_monitor()) {\n+  while (mark.has_monitor()) {\n@@ -1176,7 +813,0 @@\n-  if (LockingMode != LM_LIGHTWEIGHT && mark.has_monitor()) {\n-    \/\/ Inflated monitor so header points to ObjectMonitor (tagged pointer).\n-    \/\/ The first stage of async deflation does not affect any field\n-    \/\/ used by this comparison so the ObjectMonitor* is usable here.\n-    ObjectMonitor* monitor = read_monitor(mark);\n-    return monitor->is_entered(current) != 0;\n-  }\n@@ -1190,7 +820,1 @@\n-  markWord mark = read_stable_mark(obj);\n-\n-  if (LockingMode == LM_LEGACY && mark.has_locker()) {\n-    \/\/ stack-locked so header points into owner's stack.\n-    \/\/ owning_thread_from_monitor_owner() may also return null here:\n-    return Threads::owning_thread_from_stacklock(t_list, (address) mark.locker());\n-  }\n+  markWord mark = obj->mark_acquire();\n@@ -1198,1 +822,1 @@\n-  if (LockingMode == LM_LIGHTWEIGHT && mark.is_fast_locked()) {\n+  if (mark.is_fast_locked()) {\n@@ -1204,1 +828,1 @@\n-  while (LockingMode == LM_LIGHTWEIGHT && mark.has_monitor()) {\n+  while (mark.has_monitor()) {\n@@ -1218,10 +842,0 @@\n-  if (LockingMode != LM_LIGHTWEIGHT && mark.has_monitor()) {\n-    \/\/ Inflated monitor so header points to ObjectMonitor (tagged pointer).\n-    \/\/ The first stage of async deflation does not affect any field\n-    \/\/ used by this comparison so the ObjectMonitor* is usable here.\n-    ObjectMonitor* monitor = read_monitor(mark);\n-    assert(monitor != nullptr, \"monitor should be non-null\");\n-    \/\/ owning_thread_from_monitor() may also return null here:\n-    return Threads::owning_thread_from_monitor(t_list, monitor);\n-  }\n-\n@@ -1352,1 +966,1 @@\n-  Atomic::sub(&_in_use_list_ceiling, AvgMonitorsPerThreadEstimate);\n+  AtomicAccess::sub(&_in_use_list_ceiling, AvgMonitorsPerThreadEstimate);\n@@ -1356,1 +970,1 @@\n-  Atomic::add(&_in_use_list_ceiling, AvgMonitorsPerThreadEstimate);\n+  AtomicAccess::add(&_in_use_list_ceiling, AvgMonitorsPerThreadEstimate);\n@@ -1450,13 +1064,7 @@\n-static void post_monitor_inflate_event(EventJavaMonitorInflate* event,\n-                                       const oop obj,\n-                                       ObjectSynchronizer::InflateCause cause) {\n-  assert(event != nullptr, \"invariant\");\n-  const Klass* monitor_klass = obj->klass();\n-  if (ObjectMonitor::is_jfr_excluded(monitor_klass)) {\n-    return;\n-  }\n-  event->set_monitorClass(monitor_klass);\n-  event->set_address((uintptr_t)(void*)obj);\n-  event->set_cause((u1)cause);\n-  event->commit();\n-}\n+\/\/ Walk the in-use list and deflate (at most MonitorDeflationMax) idle\n+\/\/ ObjectMonitors. Returns the number of deflated ObjectMonitors.\n+\/\/\n+size_t ObjectSynchronizer::deflate_monitor_list(ObjectMonitorDeflationSafepointer* safepointer) {\n+  MonitorList::Iterator iter = _in_use_list.iterator();\n+  size_t deflated_count = 0;\n+  Thread* current = Thread::current();\n@@ -1464,12 +1072,8 @@\n-\/\/ Fast path code shared by multiple functions\n-void ObjectSynchronizer::inflate_helper(oop obj) {\n-  assert(LockingMode != LM_LIGHTWEIGHT, \"only inflate through enter\");\n-  markWord mark = obj->mark_acquire();\n-  if (mark.has_monitor()) {\n-    ObjectMonitor* monitor = read_monitor(mark);\n-    markWord dmw = monitor->header();\n-    assert(dmw.is_neutral(), \"sanity check: header=\" INTPTR_FORMAT, dmw.value());\n-    return;\n-  }\n-  (void)inflate(Thread::current(), obj, inflate_cause_vm_internal);\n-}\n+  while (iter.has_next()) {\n+    if (deflated_count >= (size_t)MonitorDeflationMax) {\n+      break;\n+    }\n+    ObjectMonitor* mid = iter.next();\n+    if (mid->deflate_monitor(current)) {\n+      deflated_count++;\n+    }\n@@ -1477,5 +1081,3 @@\n-ObjectMonitor* ObjectSynchronizer::inflate(Thread* current, oop obj, const InflateCause cause) {\n-  assert(current == Thread::current(), \"must be\");\n-  assert(LockingMode != LM_LIGHTWEIGHT, \"only inflate through enter\");\n-  return inflate_impl(current->is_Java_thread() ? JavaThread::cast(current) : nullptr, obj, cause);\n-}\n+    \/\/ Must check for a safepoint\/handshake and honor it.\n+    safepointer->block_for_safepoint(\"deflation\", \"deflated_count\", deflated_count);\n+  }\n@@ -1483,4 +1085,1 @@\n-ObjectMonitor* ObjectSynchronizer::inflate_for(JavaThread* thread, oop obj, const InflateCause cause) {\n-  assert(thread == Thread::current() || thread->is_obj_deopt_suspend(), \"must be\");\n-  assert(LockingMode != LM_LIGHTWEIGHT, \"LM_LIGHTWEIGHT cannot use inflate_for\");\n-  return inflate_impl(thread, obj, cause);\n+  return deflated_count;\n@@ -1489,212 +1088,3 @@\n-ObjectMonitor* ObjectSynchronizer::inflate_impl(JavaThread* locking_thread, oop object, const InflateCause cause) {\n-  \/\/ The JavaThread* locking_thread requires that the locking_thread == Thread::current() or\n-  \/\/ is suspended throughout the call by some other mechanism.\n-  \/\/ The thread might be nullptr when called from a non JavaThread. (As may still be\n-  \/\/ the case from FastHashCode). However it is only important for correctness that the\n-  \/\/ thread is set when called from ObjectSynchronizer::enter from the owning thread,\n-  \/\/ ObjectSynchronizer::enter_for from any thread, or ObjectSynchronizer::exit.\n-  assert(LockingMode != LM_LIGHTWEIGHT, \"LM_LIGHTWEIGHT cannot use inflate_impl\");\n-  EventJavaMonitorInflate event;\n-\n-  for (;;) {\n-    const markWord mark = object->mark_acquire();\n-\n-    \/\/ The mark can be in one of the following states:\n-    \/\/ *  inflated     - If the ObjectMonitor owner is anonymous and the\n-    \/\/                   locking_thread owns the object lock, then we\n-    \/\/                   make the locking_thread the ObjectMonitor owner.\n-    \/\/ *  stack-locked - Coerce it to inflated from stack-locked.\n-    \/\/ *  INFLATING    - Busy wait for conversion from stack-locked to\n-    \/\/                   inflated.\n-    \/\/ *  unlocked     - Aggressively inflate the object.\n-\n-    \/\/ CASE: inflated\n-    if (mark.has_monitor()) {\n-      ObjectMonitor* inf = mark.monitor();\n-      markWord dmw = inf->header();\n-      assert(dmw.is_neutral(), \"invariant: header=\" INTPTR_FORMAT, dmw.value());\n-      if (inf->has_anonymous_owner() && locking_thread != nullptr) {\n-        assert(LockingMode == LM_LEGACY, \"invariant\");\n-        if (locking_thread->is_lock_owned((address)inf->stack_locker())) {\n-          inf->set_stack_locker(nullptr);\n-          inf->set_owner_from_anonymous(locking_thread);\n-        }\n-      }\n-      return inf;\n-    }\n-\n-    \/\/ CASE: inflation in progress - inflating over a stack-lock.\n-    \/\/ Some other thread is converting from stack-locked to inflated.\n-    \/\/ Only that thread can complete inflation -- other threads must wait.\n-    \/\/ The INFLATING value is transient.\n-    \/\/ Currently, we spin\/yield\/park and poll the markword, waiting for inflation to finish.\n-    \/\/ We could always eliminate polling by parking the thread on some auxiliary list.\n-    if (mark == markWord::INFLATING()) {\n-      read_stable_mark(object);\n-      continue;\n-    }\n-\n-    \/\/ CASE: stack-locked\n-    \/\/ Could be stack-locked either by current or by some other thread.\n-    \/\/\n-    \/\/ Note that we allocate the ObjectMonitor speculatively, _before_ attempting\n-    \/\/ to install INFLATING into the mark word.  We originally installed INFLATING,\n-    \/\/ allocated the ObjectMonitor, and then finally STed the address of the\n-    \/\/ ObjectMonitor into the mark.  This was correct, but artificially lengthened\n-    \/\/ the interval in which INFLATING appeared in the mark, thus increasing\n-    \/\/ the odds of inflation contention. If we lose the race to set INFLATING,\n-    \/\/ then we just delete the ObjectMonitor and loop around again.\n-    \/\/\n-    LogStreamHandle(Trace, monitorinflation) lsh;\n-    if (LockingMode == LM_LEGACY && mark.has_locker()) {\n-      ObjectMonitor* m = new ObjectMonitor(object);\n-      \/\/ Optimistically prepare the ObjectMonitor - anticipate successful CAS\n-      \/\/ We do this before the CAS in order to minimize the length of time\n-      \/\/ in which INFLATING appears in the mark.\n-\n-      markWord cmp = object->cas_set_mark(markWord::INFLATING(), mark);\n-      if (cmp != mark) {\n-        delete m;\n-        continue;       \/\/ Interference -- just retry\n-      }\n-\n-      \/\/ We've successfully installed INFLATING (0) into the mark-word.\n-      \/\/ This is the only case where 0 will appear in a mark-word.\n-      \/\/ Only the singular thread that successfully swings the mark-word\n-      \/\/ to 0 can perform (or more precisely, complete) inflation.\n-      \/\/\n-      \/\/ Why do we CAS a 0 into the mark-word instead of just CASing the\n-      \/\/ mark-word from the stack-locked value directly to the new inflated state?\n-      \/\/ Consider what happens when a thread unlocks a stack-locked object.\n-      \/\/ It attempts to use CAS to swing the displaced header value from the\n-      \/\/ on-stack BasicLock back into the object header.  Recall also that the\n-      \/\/ header value (hash code, etc) can reside in (a) the object header, or\n-      \/\/ (b) a displaced header associated with the stack-lock, or (c) a displaced\n-      \/\/ header in an ObjectMonitor.  The inflate() routine must copy the header\n-      \/\/ value from the BasicLock on the owner's stack to the ObjectMonitor, all\n-      \/\/ the while preserving the hashCode stability invariants.  If the owner\n-      \/\/ decides to release the lock while the value is 0, the unlock will fail\n-      \/\/ and control will eventually pass from slow_exit() to inflate.  The owner\n-      \/\/ will then spin, waiting for the 0 value to disappear.   Put another way,\n-      \/\/ the 0 causes the owner to stall if the owner happens to try to\n-      \/\/ drop the lock (restoring the header from the BasicLock to the object)\n-      \/\/ while inflation is in-progress.  This protocol avoids races that might\n-      \/\/ would otherwise permit hashCode values to change or \"flicker\" for an object.\n-      \/\/ Critically, while object->mark is 0 mark.displaced_mark_helper() is stable.\n-      \/\/ 0 serves as a \"BUSY\" inflate-in-progress indicator.\n-\n-\n-      \/\/ fetch the displaced mark from the owner's stack.\n-      \/\/ The owner can't die or unwind past the lock while our INFLATING\n-      \/\/ object is in the mark.  Furthermore the owner can't complete\n-      \/\/ an unlock on the object, either.\n-      markWord dmw = mark.displaced_mark_helper();\n-      \/\/ Catch if the object's header is not neutral (not locked and\n-      \/\/ not marked is what we care about here).\n-      assert(dmw.is_neutral(), \"invariant: header=\" INTPTR_FORMAT, dmw.value());\n-\n-      \/\/ Setup monitor fields to proper values -- prepare the monitor\n-      m->set_header(dmw);\n-\n-      \/\/ Note that a thread can inflate an object\n-      \/\/ that it has stack-locked -- as might happen in wait() -- directly\n-      \/\/ with CAS.  That is, we can avoid the xchg-nullptr .... ST idiom.\n-      if (locking_thread != nullptr && locking_thread->is_lock_owned((address)mark.locker())) {\n-        m->set_owner(locking_thread);\n-      } else {\n-        \/\/ Use ANONYMOUS_OWNER to indicate that the owner is the BasicLock on the stack,\n-        \/\/ and set the stack locker field in the monitor.\n-        m->set_stack_locker(mark.locker());\n-        m->set_anonymous_owner();\n-      }\n-      \/\/ TODO-FIXME: assert BasicLock->dhw != 0.\n-\n-      \/\/ Must preserve store ordering. The monitor state must\n-      \/\/ be stable at the time of publishing the monitor address.\n-      guarantee(object->mark() == markWord::INFLATING(), \"invariant\");\n-      \/\/ Release semantics so that above set_object() is seen first.\n-      object->release_set_mark(markWord::encode(m));\n-\n-      \/\/ Once ObjectMonitor is configured and the object is associated\n-      \/\/ with the ObjectMonitor, it is safe to allow async deflation:\n-      _in_use_list.add(m);\n-\n-      if (log_is_enabled(Trace, monitorinflation)) {\n-        ResourceMark rm;\n-        lsh.print_cr(\"inflate(has_locker): object=\" INTPTR_FORMAT \", mark=\"\n-                     INTPTR_FORMAT \", type='%s'\", p2i(object),\n-                     object->mark().value(), object->klass()->external_name());\n-      }\n-      if (event.should_commit()) {\n-        post_monitor_inflate_event(&event, object, cause);\n-      }\n-      return m;\n-    }\n-\n-    \/\/ CASE: unlocked\n-    \/\/ TODO-FIXME: for entry we currently inflate and then try to CAS _owner.\n-    \/\/ If we know we're inflating for entry it's better to inflate by swinging a\n-    \/\/ pre-locked ObjectMonitor pointer into the object header.   A successful\n-    \/\/ CAS inflates the object *and* confers ownership to the inflating thread.\n-    \/\/ In the current implementation we use a 2-step mechanism where we CAS()\n-    \/\/ to inflate and then CAS() again to try to swing _owner from null to current.\n-    \/\/ An inflateTry() method that we could call from enter() would be useful.\n-\n-    assert(mark.is_unlocked(), \"invariant: header=\" INTPTR_FORMAT, mark.value());\n-    ObjectMonitor* m = new ObjectMonitor(object);\n-    \/\/ prepare m for installation - set monitor to initial state\n-    m->set_header(mark);\n-\n-    if (object->cas_set_mark(markWord::encode(m), mark) != mark) {\n-      delete m;\n-      m = nullptr;\n-      continue;\n-      \/\/ interference - the markword changed - just retry.\n-      \/\/ The state-transitions are one-way, so there's no chance of\n-      \/\/ live-lock -- \"Inflated\" is an absorbing state.\n-    }\n-\n-    \/\/ Once the ObjectMonitor is configured and object is associated\n-    \/\/ with the ObjectMonitor, it is safe to allow async deflation:\n-    _in_use_list.add(m);\n-\n-    if (log_is_enabled(Trace, monitorinflation)) {\n-      ResourceMark rm;\n-      lsh.print_cr(\"inflate(unlocked): object=\" INTPTR_FORMAT \", mark=\"\n-                   INTPTR_FORMAT \", type='%s'\", p2i(object),\n-                   object->mark().value(), object->klass()->external_name());\n-    }\n-    if (event.should_commit()) {\n-      post_monitor_inflate_event(&event, object, cause);\n-    }\n-    return m;\n-  }\n-}\n-\n-\/\/ Walk the in-use list and deflate (at most MonitorDeflationMax) idle\n-\/\/ ObjectMonitors. Returns the number of deflated ObjectMonitors.\n-\/\/\n-size_t ObjectSynchronizer::deflate_monitor_list(ObjectMonitorDeflationSafepointer* safepointer) {\n-  MonitorList::Iterator iter = _in_use_list.iterator();\n-  size_t deflated_count = 0;\n-  Thread* current = Thread::current();\n-\n-  while (iter.has_next()) {\n-    if (deflated_count >= (size_t)MonitorDeflationMax) {\n-      break;\n-    }\n-    ObjectMonitor* mid = iter.next();\n-    if (mid->deflate_monitor(current)) {\n-      deflated_count++;\n-    }\n-\n-    \/\/ Must check for a safepoint\/handshake and honor it.\n-    safepointer->block_for_safepoint(\"deflation\", \"deflated_count\", deflated_count);\n-  }\n-\n-  return deflated_count;\n-}\n-\n-class DeflationHandshakeClosure : public HandshakeClosure {\n- public:\n-  DeflationHandshakeClosure() : HandshakeClosure(\"DeflationHandshakeClosure\") {}\n+class DeflationHandshakeClosure : public HandshakeClosure {\n+ public:\n+  DeflationHandshakeClosure() : HandshakeClosure(\"DeflationHandshakeClosure\") {}\n@@ -1859,1 +1249,1 @@\n-        assert(!LightweightSynchronizer::contains_monitor(current, monitor), \"Should have been removed\");\n+        assert(!ObjectSynchronizer::contains_monitor(current, monitor), \"Should have been removed\");\n@@ -1911,2 +1301,1 @@\n-    intx rec = mid->complete_exit(_thread);\n-    _thread->dec_held_monitor_count(rec + 1);\n+    mid->complete_exit(_thread);\n@@ -1938,3 +1327,0 @@\n-  assert(current->held_monitor_count() == 0, \"Should not be possible\");\n-  \/\/ All monitors (including entered via JNI) have been unlocked above, so we need to clear jni count.\n-  current->clear_jni_monitor_count();\n@@ -1949,1 +1335,0 @@\n-    case inflate_cause_hash_code:      return \"Monitor Hash Code\";\n@@ -2136,0 +1521,1227 @@\n+\n+static uintx objhash(oop obj) {\n+  if (UseCompactObjectHeaders) {\n+    uintx hash = ObjectSynchronizer::get_hash(obj->mark(), obj);\n+    assert(hash != 0, \"should have a hash\");\n+    return hash;\n+  } else {\n+    uintx hash = obj->mark().hash();\n+    assert(hash != 0, \"should have a hash\");\n+    return hash;\n+  }\n+}\n+\n+\/\/ -----------------------------------------------------------------------------\n+\/\/ ConcurrentHashTable storing links from objects to ObjectMonitors\n+class ObjectMonitorTable : AllStatic {\n+  struct Config {\n+    using Value = ObjectMonitor*;\n+    static uintx get_hash(Value const& value, bool* is_dead) {\n+      return (uintx)value->hash();\n+    }\n+    static void* allocate_node(void* context, size_t size, Value const& value) {\n+      ObjectMonitorTable::inc_items_count();\n+      return AllocateHeap(size, mtObjectMonitor);\n+    };\n+    static void free_node(void* context, void* memory, Value const& value) {\n+      ObjectMonitorTable::dec_items_count();\n+      FreeHeap(memory);\n+    }\n+  };\n+  using ConcurrentTable = ConcurrentHashTable<Config, mtObjectMonitor>;\n+\n+  static ConcurrentTable* _table;\n+  static volatile size_t _items_count;\n+  static size_t _table_size;\n+  static volatile bool _resize;\n+\n+  class Lookup : public StackObj {\n+    oop _obj;\n+\n+   public:\n+    explicit Lookup(oop obj) : _obj(obj) {}\n+\n+    uintx get_hash() const {\n+      return objhash(_obj);\n+    }\n+\n+    bool equals(ObjectMonitor** value) {\n+      assert(*value != nullptr, \"must be\");\n+      return (*value)->object_refers_to(_obj);\n+    }\n+\n+    bool is_dead(ObjectMonitor** value) {\n+      assert(*value != nullptr, \"must be\");\n+      return false;\n+    }\n+  };\n+\n+  class LookupMonitor : public StackObj {\n+    ObjectMonitor* _monitor;\n+\n+   public:\n+    explicit LookupMonitor(ObjectMonitor* monitor) : _monitor(monitor) {}\n+\n+    uintx get_hash() const {\n+      return _monitor->hash();\n+    }\n+\n+    bool equals(ObjectMonitor** value) {\n+      return (*value) == _monitor;\n+    }\n+\n+    bool is_dead(ObjectMonitor** value) {\n+      assert(*value != nullptr, \"must be\");\n+      return (*value)->object_is_dead();\n+    }\n+  };\n+\n+  static void inc_items_count() {\n+    AtomicAccess::inc(&_items_count, memory_order_relaxed);\n+  }\n+\n+  static void dec_items_count() {\n+    AtomicAccess::dec(&_items_count, memory_order_relaxed);\n+  }\n+\n+  static double get_load_factor() {\n+    size_t count = AtomicAccess::load(&_items_count);\n+    return (double)count \/ (double)_table_size;\n+  }\n+\n+  static size_t table_size(Thread* current = Thread::current()) {\n+    return ((size_t)1) << _table->get_size_log2(current);\n+  }\n+\n+  static size_t max_log_size() {\n+    \/\/ TODO[OMTable]: Evaluate the max size.\n+    \/\/ TODO[OMTable]: Need to fix init order to use Universe::heap()->max_capacity();\n+    \/\/                Using MaxHeapSize directly this early may be wrong, and there\n+    \/\/                are definitely rounding errors (alignment).\n+    const size_t max_capacity = MaxHeapSize;\n+    const size_t min_object_size = CollectedHeap::min_dummy_object_size() * HeapWordSize;\n+    const size_t max_objects = max_capacity \/ MAX2(MinObjAlignmentInBytes, checked_cast<int>(min_object_size));\n+    const size_t log_max_objects = log2i_graceful(max_objects);\n+\n+    return MAX2(MIN2<size_t>(SIZE_BIG_LOG2, log_max_objects), min_log_size());\n+  }\n+\n+  static size_t min_log_size() {\n+    \/\/ ~= log(AvgMonitorsPerThreadEstimate default)\n+    return 10;\n+  }\n+\n+  template<typename V>\n+  static size_t clamp_log_size(V log_size) {\n+    return MAX2(MIN2(log_size, checked_cast<V>(max_log_size())), checked_cast<V>(min_log_size()));\n+  }\n+\n+  static size_t initial_log_size() {\n+    const size_t estimate = log2i(MAX2(os::processor_count(), 1)) + log2i(MAX2(AvgMonitorsPerThreadEstimate, size_t(1)));\n+    return clamp_log_size(estimate);\n+  }\n+\n+  static size_t grow_hint () {\n+    return ConcurrentTable::DEFAULT_GROW_HINT;\n+  }\n+\n+ public:\n+  static void create() {\n+    _table = new ConcurrentTable(initial_log_size(), max_log_size(), grow_hint());\n+    _items_count = 0;\n+    _table_size = table_size();\n+    _resize = false;\n+  }\n+\n+  static void verify_monitor_get_result(oop obj, ObjectMonitor* monitor) {\n+#ifdef ASSERT\n+    if (SafepointSynchronize::is_at_safepoint()) {\n+      bool has_monitor = obj->mark().has_monitor();\n+      assert(has_monitor == (monitor != nullptr),\n+          \"Inconsistency between markWord and ObjectMonitorTable has_monitor: %s monitor: \" PTR_FORMAT,\n+          BOOL_TO_STR(has_monitor), p2i(monitor));\n+    }\n+#endif\n+  }\n+\n+  static ObjectMonitor* monitor_get(Thread* current, oop obj) {\n+    ObjectMonitor* result = nullptr;\n+    Lookup lookup_f(obj);\n+    auto found_f = [&](ObjectMonitor** found) {\n+      assert((*found)->object_peek() == obj, \"must be\");\n+      assert(objhash(obj) == (uintx)(*found)->hash(), \"hash must match\");\n+      result = *found;\n+    };\n+    _table->get(current, lookup_f, found_f);\n+    verify_monitor_get_result(obj, result);\n+    return result;\n+  }\n+\n+  static void try_notify_grow() {\n+    if (!_table->is_max_size_reached() && !AtomicAccess::load(&_resize)) {\n+      AtomicAccess::store(&_resize, true);\n+      if (Service_lock->try_lock()) {\n+        Service_lock->notify();\n+        Service_lock->unlock();\n+      }\n+    }\n+  }\n+\n+  static bool should_shrink() {\n+    \/\/ Not implemented;\n+    return false;\n+  }\n+\n+  static constexpr double GROW_LOAD_FACTOR = 0.75;\n+\n+  static bool should_grow() {\n+    return get_load_factor() > GROW_LOAD_FACTOR && !_table->is_max_size_reached();\n+  }\n+\n+  static bool should_resize() {\n+    return should_grow() || should_shrink() || AtomicAccess::load(&_resize);\n+  }\n+\n+  template<typename Task, typename... Args>\n+  static bool run_task(JavaThread* current, Task& task, const char* task_name, Args&... args) {\n+    if (task.prepare(current)) {\n+      log_trace(monitortable)(\"Started to %s\", task_name);\n+      TraceTime timer(task_name, TRACETIME_LOG(Debug, monitortable, perf));\n+      while (task.do_task(current, args...)) {\n+        task.pause(current);\n+        {\n+          ThreadBlockInVM tbivm(current);\n+        }\n+        task.cont(current);\n+      }\n+      task.done(current);\n+      return true;\n+    }\n+    return false;\n+  }\n+\n+  static bool grow(JavaThread* current) {\n+    ConcurrentTable::GrowTask grow_task(_table);\n+    if (run_task(current, grow_task, \"Grow\")) {\n+      _table_size = table_size(current);\n+      log_info(monitortable)(\"Grown to size: %zu\", _table_size);\n+      return true;\n+    }\n+    return false;\n+  }\n+\n+  static bool clean(JavaThread* current) {\n+    ConcurrentTable::BulkDeleteTask clean_task(_table);\n+    auto is_dead = [&](ObjectMonitor** monitor) {\n+      return (*monitor)->object_is_dead();\n+    };\n+    auto do_nothing = [&](ObjectMonitor** monitor) {};\n+    NativeHeapTrimmer::SuspendMark sm(\"ObjectMonitorTable\");\n+    return run_task(current, clean_task, \"Clean\", is_dead, do_nothing);\n+  }\n+\n+  static bool resize(JavaThread* current) {\n+    LogTarget(Info, monitortable) lt;\n+    bool success = false;\n+\n+    if (should_grow()) {\n+      lt.print(\"Start growing with load factor %f\", get_load_factor());\n+      success = grow(current);\n+    } else {\n+      if (!_table->is_max_size_reached() && AtomicAccess::load(&_resize)) {\n+        lt.print(\"WARNING: Getting resize hints with load factor %f\", get_load_factor());\n+      }\n+      lt.print(\"Start cleaning with load factor %f\", get_load_factor());\n+      success = clean(current);\n+    }\n+\n+    AtomicAccess::store(&_resize, false);\n+\n+    return success;\n+  }\n+\n+  static ObjectMonitor* monitor_put_get(Thread* current, ObjectMonitor* monitor, oop obj) {\n+    \/\/ Enter the monitor into the concurrent hashtable.\n+    ObjectMonitor* result = monitor;\n+    Lookup lookup_f(obj);\n+    auto found_f = [&](ObjectMonitor** found) {\n+      assert((*found)->object_peek() == obj, \"must be\");\n+      result = *found;\n+    };\n+    bool grow;\n+    _table->insert_get(current, lookup_f, monitor, found_f, &grow);\n+    verify_monitor_get_result(obj, result);\n+    if (grow) {\n+      try_notify_grow();\n+    }\n+    return result;\n+  }\n+\n+  static bool remove_monitor_entry(Thread* current, ObjectMonitor* monitor) {\n+    LookupMonitor lookup_f(monitor);\n+    return _table->remove(current, lookup_f);\n+  }\n+\n+  static bool contains_monitor(Thread* current, ObjectMonitor* monitor) {\n+    LookupMonitor lookup_f(monitor);\n+    bool result = false;\n+    auto found_f = [&](ObjectMonitor** found) {\n+      result = true;\n+    };\n+    _table->get(current, lookup_f, found_f);\n+    return result;\n+  }\n+\n+  static void print_on(outputStream* st) {\n+    auto printer = [&] (ObjectMonitor** entry) {\n+       ObjectMonitor* om = *entry;\n+       oop obj = om->object_peek();\n+       st->print(\"monitor=\" PTR_FORMAT \", \", p2i(om));\n+       st->print(\"object=\" PTR_FORMAT, p2i(obj));\n+       assert(objhash(obj) == (uintx)om->hash(), \"hash must match\");\n+       st->cr();\n+       return true;\n+    };\n+    if (SafepointSynchronize::is_at_safepoint()) {\n+      _table->do_safepoint_scan(printer);\n+    } else {\n+      _table->do_scan(Thread::current(), printer);\n+    }\n+  }\n+};\n+\n+ObjectMonitorTable::ConcurrentTable* ObjectMonitorTable::_table = nullptr;\n+volatile size_t ObjectMonitorTable::_items_count = 0;\n+size_t ObjectMonitorTable::_table_size = 0;\n+volatile bool ObjectMonitorTable::_resize = false;\n+\n+ObjectMonitor* ObjectSynchronizer::get_or_insert_monitor_from_table(oop object, JavaThread* current, bool* inserted) {\n+  ObjectMonitor* monitor = get_monitor_from_table(current, object);\n+  if (monitor != nullptr) {\n+    *inserted = false;\n+    return monitor;\n+  }\n+\n+  ObjectMonitor* alloced_monitor = new ObjectMonitor(object);\n+  alloced_monitor->set_anonymous_owner();\n+\n+  \/\/ Try insert monitor\n+  monitor = add_monitor(current, alloced_monitor, object);\n+\n+  *inserted = alloced_monitor == monitor;\n+  if (!*inserted) {\n+    delete alloced_monitor;\n+  }\n+\n+  return monitor;\n+}\n+\n+static void log_inflate(Thread* current, oop object, ObjectSynchronizer::InflateCause cause) {\n+  if (log_is_enabled(Trace, monitorinflation)) {\n+    ResourceMark rm(current);\n+    log_trace(monitorinflation)(\"inflate: object=\" INTPTR_FORMAT \", mark=\"\n+                                INTPTR_FORMAT \", type='%s' cause=%s\", p2i(object),\n+                                object->mark().value(), object->klass()->external_name(),\n+                                ObjectSynchronizer::inflate_cause_name(cause));\n+  }\n+}\n+\n+static void post_monitor_inflate_event(EventJavaMonitorInflate* event,\n+                                       const oop obj,\n+                                       ObjectSynchronizer::InflateCause cause) {\n+  assert(event != nullptr, \"invariant\");\n+  const Klass* monitor_klass = obj->klass();\n+  if (ObjectMonitor::is_jfr_excluded(monitor_klass)) {\n+    return;\n+  }\n+  event->set_monitorClass(monitor_klass);\n+  event->set_address((uintptr_t)(void*)obj);\n+  event->set_cause((u1)cause);\n+  event->commit();\n+}\n+\n+ObjectMonitor* ObjectSynchronizer::get_or_insert_monitor(oop object, JavaThread* current, ObjectSynchronizer::InflateCause cause) {\n+  assert(UseObjectMonitorTable, \"must be\");\n+\n+  EventJavaMonitorInflate event;\n+\n+  bool inserted;\n+  ObjectMonitor* monitor = get_or_insert_monitor_from_table(object, current, &inserted);\n+\n+  if (inserted) {\n+    log_inflate(current, object, cause);\n+    if (event.should_commit()) {\n+      post_monitor_inflate_event(&event, object, cause);\n+    }\n+\n+    \/\/ The monitor has an anonymous owner so it is safe from async deflation.\n+    ObjectSynchronizer::_in_use_list.add(monitor);\n+  }\n+\n+  return monitor;\n+}\n+\n+\/\/ Add the hashcode to the monitor to match the object and put it in the hashtable.\n+ObjectMonitor* ObjectSynchronizer::add_monitor(JavaThread* current, ObjectMonitor* monitor, oop obj) {\n+  assert(UseObjectMonitorTable, \"must be\");\n+  assert(obj == monitor->object(), \"must be\");\n+\n+  intptr_t hash = objhash(obj);\n+  assert(hash != 0, \"must be set when claiming the object monitor\");\n+  monitor->set_hash(hash);\n+\n+  return ObjectMonitorTable::monitor_put_get(current, monitor, obj);\n+}\n+\n+bool ObjectSynchronizer::remove_monitor(Thread* current, ObjectMonitor* monitor, oop obj) {\n+  assert(UseObjectMonitorTable, \"must be\");\n+  assert(monitor->object_peek() == obj, \"must be, cleared objects are removed by is_dead\");\n+\n+  return ObjectMonitorTable::remove_monitor_entry(current, monitor);\n+}\n+\n+void ObjectSynchronizer::deflate_mark_word(oop obj) {\n+  assert(UseObjectMonitorTable, \"must be\");\n+\n+  markWord mark = obj->mark_acquire();\n+  assert(!mark.has_no_hash(), \"obj with inflated monitor must have had a hash\");\n+\n+  while (mark.has_monitor()) {\n+    const markWord new_mark = mark.clear_lock_bits().set_unlocked();\n+    mark = obj->cas_set_mark(new_mark, mark);\n+  }\n+}\n+\n+void ObjectSynchronizer::create_om_table() {\n+  if (!UseObjectMonitorTable) {\n+    return;\n+  }\n+  ObjectMonitorTable::create();\n+}\n+\n+bool ObjectSynchronizer::needs_resize() {\n+  if (!UseObjectMonitorTable) {\n+    return false;\n+  }\n+  return ObjectMonitorTable::should_resize();\n+}\n+\n+bool ObjectSynchronizer::resize_table(JavaThread* current) {\n+  if (!UseObjectMonitorTable) {\n+    return true;\n+  }\n+  return ObjectMonitorTable::resize(current);\n+}\n+\n+class ObjectSynchronizer::LockStackInflateContendedLocks : private OopClosure {\n+ private:\n+  oop _contended_oops[LockStack::CAPACITY];\n+  int _length;\n+\n+  void do_oop(oop* o) final {\n+    oop obj = *o;\n+    if (obj->mark_acquire().has_monitor()) {\n+      if (_length > 0 && _contended_oops[_length - 1] == obj) {\n+        \/\/ Recursive\n+        return;\n+      }\n+      _contended_oops[_length++] = obj;\n+    }\n+  }\n+\n+  void do_oop(narrowOop* o) final {\n+    ShouldNotReachHere();\n+  }\n+\n+ public:\n+  LockStackInflateContendedLocks() :\n+    _contended_oops(),\n+    _length(0) {};\n+\n+  void inflate(JavaThread* current) {\n+    assert(current == JavaThread::current(), \"must be\");\n+    current->lock_stack().oops_do(this);\n+    for (int i = 0; i < _length; i++) {\n+      ObjectSynchronizer::\n+        inflate_fast_locked_object(_contended_oops[i], ObjectSynchronizer::inflate_cause_vm_internal, current, current);\n+    }\n+  }\n+};\n+\n+void ObjectSynchronizer::ensure_lock_stack_space(JavaThread* current) {\n+  assert(current == JavaThread::current(), \"must be\");\n+  LockStack& lock_stack = current->lock_stack();\n+\n+  \/\/ Make room on lock_stack\n+  if (lock_stack.is_full()) {\n+    \/\/ Inflate contended objects\n+    LockStackInflateContendedLocks().inflate(current);\n+    if (lock_stack.is_full()) {\n+      \/\/ Inflate the oldest object\n+      inflate_fast_locked_object(lock_stack.bottom(), ObjectSynchronizer::inflate_cause_vm_internal, current, current);\n+    }\n+  }\n+}\n+\n+class ObjectSynchronizer::CacheSetter : StackObj {\n+  JavaThread* const _thread;\n+  BasicLock* const _lock;\n+  ObjectMonitor* _monitor;\n+\n+  NONCOPYABLE(CacheSetter);\n+\n+ public:\n+  CacheSetter(JavaThread* thread, BasicLock* lock) :\n+    _thread(thread),\n+    _lock(lock),\n+    _monitor(nullptr) {}\n+\n+  ~CacheSetter() {\n+    \/\/ Only use the cache if using the table.\n+    if (UseObjectMonitorTable) {\n+      if (_monitor != nullptr) {\n+        \/\/ If the monitor is already in the BasicLock cache then it is most\n+        \/\/ likely in the thread cache, do not set it again to avoid reordering.\n+        if (_monitor != _lock->object_monitor_cache()) {\n+          _thread->om_set_monitor_cache(_monitor);\n+          _lock->set_object_monitor_cache(_monitor);\n+        }\n+      } else {\n+        _lock->clear_object_monitor_cache();\n+      }\n+    }\n+  }\n+\n+  void set_monitor(ObjectMonitor* monitor) {\n+    assert(_monitor == nullptr, \"only set once\");\n+    _monitor = monitor;\n+  }\n+\n+};\n+\n+\/\/ Reads first from the BasicLock cache then from the OMCache in the current thread.\n+\/\/ C2 fast-path may have put the monitor in the cache in the BasicLock.\n+inline static ObjectMonitor* read_caches(JavaThread* current, BasicLock* lock, oop object) {\n+  ObjectMonitor* monitor = lock->object_monitor_cache();\n+  if (monitor == nullptr) {\n+    monitor = current->om_get_from_monitor_cache(object);\n+  }\n+  return monitor;\n+}\n+\n+class ObjectSynchronizer::VerifyThreadState {\n+  bool _no_safepoint;\n+\n+ public:\n+  VerifyThreadState(JavaThread* locking_thread, JavaThread* current) : _no_safepoint(locking_thread != current) {\n+    assert(current == Thread::current(), \"must be\");\n+    assert(locking_thread == current || locking_thread->is_obj_deopt_suspend(), \"locking_thread may not run concurrently\");\n+    if (_no_safepoint) {\n+      DEBUG_ONLY(JavaThread::current()->inc_no_safepoint_count();)\n+    }\n+  }\n+  ~VerifyThreadState() {\n+    if (_no_safepoint){\n+      DEBUG_ONLY(JavaThread::current()->dec_no_safepoint_count();)\n+    }\n+  }\n+};\n+\n+inline bool ObjectSynchronizer::fast_lock_try_enter(oop obj, LockStack& lock_stack, JavaThread* current) {\n+  markWord mark = obj->mark();\n+  while (mark.is_unlocked()) {\n+    ensure_lock_stack_space(current);\n+    assert(!lock_stack.is_full(), \"must have made room on the lock stack\");\n+    assert(!lock_stack.contains(obj), \"thread must not already hold the lock\");\n+    \/\/ Try to swing into 'fast-locked' state.\n+    markWord locked_mark = mark.set_fast_locked();\n+    markWord old_mark = mark;\n+    mark = obj->cas_set_mark(locked_mark, old_mark);\n+    if (old_mark == mark) {\n+      \/\/ Successfully fast-locked, push object to lock-stack and return.\n+      lock_stack.push(obj);\n+      return true;\n+    }\n+  }\n+  return false;\n+}\n+\n+bool ObjectSynchronizer::fast_lock_spin_enter(oop obj, LockStack& lock_stack, JavaThread* current, bool observed_deflation) {\n+  assert(UseObjectMonitorTable, \"must be\");\n+  \/\/ Will spin with exponential backoff with an accumulative O(2^spin_limit) spins.\n+  const int log_spin_limit = os::is_MP() ? FastLockingSpins : 1;\n+  const int log_min_safepoint_check_interval = 10;\n+\n+  markWord mark = obj->mark();\n+  const auto should_spin = [&]() {\n+    if (!mark.has_monitor()) {\n+      \/\/ Spin while not inflated.\n+      return true;\n+    } else if (observed_deflation) {\n+      \/\/ Spin while monitor is being deflated.\n+      ObjectMonitor* monitor = ObjectSynchronizer::read_monitor(current, obj, mark);\n+      return monitor == nullptr || monitor->is_being_async_deflated();\n+    }\n+    \/\/ Else stop spinning.\n+    return false;\n+  };\n+  \/\/ Always attempt to lock once even when safepoint synchronizing.\n+  bool should_process = false;\n+  for (int i = 0; should_spin() && !should_process && i < log_spin_limit; i++) {\n+    \/\/ Spin with exponential backoff.\n+    const int total_spin_count = 1 << i;\n+    const int inner_spin_count = MIN2(1 << log_min_safepoint_check_interval, total_spin_count);\n+    const int outer_spin_count = total_spin_count \/ inner_spin_count;\n+    for (int outer = 0; outer < outer_spin_count; outer++) {\n+      should_process = SafepointMechanism::should_process(current);\n+      if (should_process) {\n+        \/\/ Stop spinning for safepoint.\n+        break;\n+      }\n+      for (int inner = 1; inner < inner_spin_count; inner++) {\n+        SpinPause();\n+      }\n+    }\n+\n+    if (fast_lock_try_enter(obj, lock_stack, current)) return true;\n+  }\n+  return false;\n+}\n+\n+void ObjectSynchronizer::enter_for(Handle obj, BasicLock* lock, JavaThread* locking_thread) {\n+  \/\/ When called with locking_thread != Thread::current() some mechanism must synchronize\n+  \/\/ the locking_thread with respect to the current thread. Currently only used when\n+  \/\/ deoptimizing and re-locking locks. See Deoptimization::relock_objects\n+  assert(locking_thread == Thread::current() || locking_thread->is_obj_deopt_suspend(), \"must be\");\n+\n+  assert(!UseObjectMonitorTable || lock->object_monitor_cache() == nullptr, \"must be cleared\");\n+  JavaThread* current = JavaThread::current();\n+  VerifyThreadState vts(locking_thread, current);\n+\n+  if (obj->klass()->is_value_based()) {\n+    ObjectSynchronizer::handle_sync_on_value_based_class(obj, locking_thread);\n+  }\n+\n+  LockStack& lock_stack = locking_thread->lock_stack();\n+\n+  ObjectMonitor* monitor = nullptr;\n+  if (lock_stack.contains(obj())) {\n+    monitor = inflate_fast_locked_object(obj(), ObjectSynchronizer::inflate_cause_monitor_enter, locking_thread, current);\n+    bool entered = monitor->enter_for(locking_thread);\n+    assert(entered, \"recursive ObjectMonitor::enter_for must succeed\");\n+  } else {\n+    do {\n+      \/\/ It is assumed that enter_for must enter on an object without contention.\n+      monitor = inflate_and_enter(obj(), lock, ObjectSynchronizer::inflate_cause_monitor_enter, locking_thread, current);\n+      \/\/ But there may still be a race with deflation.\n+    } while (monitor == nullptr);\n+  }\n+\n+  assert(monitor != nullptr, \"ObjectSynchronizer::enter_for must succeed\");\n+  assert(!UseObjectMonitorTable || lock->object_monitor_cache() == nullptr, \"unused. already cleared\");\n+}\n+\n+void ObjectSynchronizer::enter(Handle obj, BasicLock* lock, JavaThread* current) {\n+  assert(current == JavaThread::current(), \"must be\");\n+\n+  if (obj->klass()->is_value_based()) {\n+    ObjectSynchronizer::handle_sync_on_value_based_class(obj, current);\n+  }\n+\n+  CacheSetter cache_setter(current, lock);\n+\n+  \/\/ Used when deflation is observed. Progress here requires progress\n+  \/\/ from the deflator. After observing that the deflator is not\n+  \/\/ making progress (after two yields), switch to sleeping.\n+  SpinYield spin_yield(0, 2);\n+  bool observed_deflation = false;\n+\n+  LockStack& lock_stack = current->lock_stack();\n+\n+  if (!lock_stack.is_full() && lock_stack.try_recursive_enter(obj())) {\n+    \/\/ Recursively fast locked\n+    return;\n+  }\n+\n+  if (lock_stack.contains(obj())) {\n+    ObjectMonitor* monitor = inflate_fast_locked_object(obj(), ObjectSynchronizer::inflate_cause_monitor_enter, current, current);\n+    bool entered = monitor->enter(current);\n+    assert(entered, \"recursive ObjectMonitor::enter must succeed\");\n+    cache_setter.set_monitor(monitor);\n+    return;\n+  }\n+\n+  while (true) {\n+    \/\/ Fast-locking does not use the 'lock' argument.\n+    \/\/ Fast-lock spinning to avoid inflating for short critical sections.\n+    \/\/ The goal is to only inflate when the extra cost of using ObjectMonitors\n+    \/\/ is worth it.\n+    \/\/ If deflation has been observed we also spin while deflation is ongoing.\n+    if (fast_lock_try_enter(obj(), lock_stack, current)) {\n+      return;\n+    } else if (UseObjectMonitorTable && fast_lock_spin_enter(obj(), lock_stack, current, observed_deflation)) {\n+      return;\n+    }\n+\n+    if (observed_deflation) {\n+      spin_yield.wait();\n+    }\n+\n+    ObjectMonitor* monitor = inflate_and_enter(obj(), lock, ObjectSynchronizer::inflate_cause_monitor_enter, current, current);\n+    if (monitor != nullptr) {\n+      cache_setter.set_monitor(monitor);\n+      return;\n+    }\n+\n+    \/\/ If inflate_and_enter returns nullptr it is because a deflated monitor\n+    \/\/ was encountered. Fallback to fast locking. The deflater is responsible\n+    \/\/ for clearing out the monitor and transitioning the markWord back to\n+    \/\/ fast locking.\n+    observed_deflation = true;\n+  }\n+}\n+\n+void ObjectSynchronizer::exit(oop object, BasicLock* lock, JavaThread* current) {\n+  assert(current == Thread::current(), \"must be\");\n+\n+  markWord mark = object->mark();\n+  assert(!mark.is_unlocked(), \"must be\");\n+\n+  LockStack& lock_stack = current->lock_stack();\n+  if (mark.is_fast_locked()) {\n+    if (lock_stack.try_recursive_exit(object)) {\n+      \/\/ This is a recursive exit which succeeded\n+      return;\n+    }\n+    if (lock_stack.is_recursive(object)) {\n+      \/\/ Must inflate recursive locks if try_recursive_exit fails\n+      \/\/ This happens for un-structured unlocks, could potentially\n+      \/\/ fix try_recursive_exit to handle these.\n+      inflate_fast_locked_object(object, ObjectSynchronizer::inflate_cause_vm_internal, current, current);\n+    }\n+  }\n+\n+  while (mark.is_fast_locked()) {\n+    markWord unlocked_mark = mark.set_unlocked();\n+    markWord old_mark = mark;\n+    mark = object->cas_set_mark(unlocked_mark, old_mark);\n+    if (old_mark == mark) {\n+      \/\/ CAS successful, remove from lock_stack\n+      size_t recursion = lock_stack.remove(object) - 1;\n+      assert(recursion == 0, \"Should not have unlocked here\");\n+      return;\n+    }\n+  }\n+\n+  assert(mark.has_monitor(), \"must be\");\n+  \/\/ The monitor exists\n+  ObjectMonitor* monitor;\n+  if (UseObjectMonitorTable) {\n+    monitor = read_caches(current, lock, object);\n+    if (monitor == nullptr) {\n+      monitor = get_monitor_from_table(current, object);\n+    }\n+  } else {\n+    monitor = ObjectSynchronizer::read_monitor(mark);\n+  }\n+  if (monitor->has_anonymous_owner()) {\n+    assert(current->lock_stack().contains(object), \"current must have object on its lock stack\");\n+    monitor->set_owner_from_anonymous(current);\n+    monitor->set_recursions(current->lock_stack().remove(object) - 1);\n+  }\n+\n+  monitor->exit(current);\n+}\n+\n+\/\/ ObjectSynchronizer::inflate_locked_or_imse is used to get an\n+\/\/ inflated ObjectMonitor* from contexts which require that, such as\n+\/\/ notify\/wait and jni_exit. Fast locking keeps the invariant that it\n+\/\/ only inflates if it is already locked by the current thread or the current\n+\/\/ thread is in the process of entering. To maintain this invariant we need to\n+\/\/ throw a java.lang.IllegalMonitorStateException before inflating if the\n+\/\/ current thread is not the owner.\n+ObjectMonitor* ObjectSynchronizer::inflate_locked_or_imse(oop obj, ObjectSynchronizer::InflateCause cause, TRAPS) {\n+  JavaThread* current = THREAD;\n+\n+  for (;;) {\n+    markWord mark = obj->mark_acquire();\n+    if (mark.is_unlocked()) {\n+      \/\/ No lock, IMSE.\n+      THROW_MSG_(vmSymbols::java_lang_IllegalMonitorStateException(),\n+                 \"current thread is not owner\", nullptr);\n+    }\n+\n+    if (mark.is_fast_locked()) {\n+      if (!current->lock_stack().contains(obj)) {\n+        \/\/ Fast locked by other thread, IMSE.\n+        THROW_MSG_(vmSymbols::java_lang_IllegalMonitorStateException(),\n+                   \"current thread is not owner\", nullptr);\n+      } else {\n+        \/\/ Current thread owns the lock, must inflate\n+        return inflate_fast_locked_object(obj, cause, current, current);\n+      }\n+    }\n+\n+    assert(mark.has_monitor(), \"must be\");\n+    ObjectMonitor* monitor = ObjectSynchronizer::read_monitor(current, obj, mark);\n+    if (monitor != nullptr) {\n+      if (monitor->has_anonymous_owner()) {\n+        LockStack& lock_stack = current->lock_stack();\n+        if (lock_stack.contains(obj)) {\n+          \/\/ Current thread owns the lock but someone else inflated it.\n+          \/\/ Fix owner and pop lock stack.\n+          monitor->set_owner_from_anonymous(current);\n+          monitor->set_recursions(lock_stack.remove(obj) - 1);\n+        } else {\n+          \/\/ Fast locked (and inflated) by other thread, or deflation in progress, IMSE.\n+          THROW_MSG_(vmSymbols::java_lang_IllegalMonitorStateException(),\n+                     \"current thread is not owner\", nullptr);\n+        }\n+      }\n+      return monitor;\n+    }\n+  }\n+}\n+\n+ObjectMonitor* ObjectSynchronizer::inflate_into_object_header(oop object, ObjectSynchronizer::InflateCause cause, JavaThread* locking_thread, Thread* current) {\n+\n+  \/\/ The JavaThread* locking parameter requires that the locking_thread == JavaThread::current,\n+  \/\/ or is suspended throughout the call by some other mechanism.\n+  \/\/ Even with fast locking the thread might be nullptr when called from a non\n+  \/\/ JavaThread. (As may still be the case from FastHashCode). However it is only\n+  \/\/ important for the correctness of the fast locking algorithm that the thread\n+  \/\/ is set when called from ObjectSynchronizer::enter from the owning thread,\n+  \/\/ ObjectSynchronizer::enter_for from any thread, or ObjectSynchronizer::exit.\n+  EventJavaMonitorInflate event;\n+\n+  for (;;) {\n+    const markWord mark = object->mark_acquire();\n+\n+    \/\/ The mark can be in one of the following states:\n+    \/\/ *  inflated     - Just return if using stack-locking.\n+    \/\/                   If using fast-locking and the ObjectMonitor owner\n+    \/\/                   is anonymous and the locking_thread owns the\n+    \/\/                   object lock, then we make the locking_thread\n+    \/\/                   the ObjectMonitor owner and remove the lock from\n+    \/\/                   the locking_thread's lock stack.\n+    \/\/ *  fast-locked  - Coerce it to inflated from fast-locked.\n+    \/\/ *  unlocked     - Aggressively inflate the object.\n+\n+    \/\/ CASE: inflated\n+    if (mark.has_monitor()) {\n+      ObjectMonitor* inf = mark.monitor();\n+      markWord dmw = inf->header();\n+      assert(dmw.is_neutral(), \"invariant: header=\" INTPTR_FORMAT, dmw.value());\n+      if (inf->has_anonymous_owner() &&\n+          locking_thread != nullptr && locking_thread->lock_stack().contains(object)) {\n+        inf->set_owner_from_anonymous(locking_thread);\n+        size_t removed = locking_thread->lock_stack().remove(object);\n+        inf->set_recursions(removed - 1);\n+      }\n+      return inf;\n+    }\n+\n+    \/\/ CASE: fast-locked\n+    \/\/ Could be fast-locked either by the locking_thread or by some other thread.\n+    \/\/\n+    \/\/ Note that we allocate the ObjectMonitor speculatively, _before_\n+    \/\/ attempting to set the object's mark to the new ObjectMonitor. If\n+    \/\/ the locking_thread owns the monitor, then we set the ObjectMonitor's\n+    \/\/ owner to the locking_thread. Otherwise, we set the ObjectMonitor's owner\n+    \/\/ to anonymous. If we lose the race to set the object's mark to the\n+    \/\/ new ObjectMonitor, then we just delete it and loop around again.\n+    \/\/\n+    if (mark.is_fast_locked()) {\n+      ObjectMonitor* monitor = new ObjectMonitor(object);\n+      monitor->set_header(mark.set_unlocked());\n+      bool own = locking_thread != nullptr && locking_thread->lock_stack().contains(object);\n+      if (own) {\n+        \/\/ Owned by locking_thread.\n+        monitor->set_owner(locking_thread);\n+      } else {\n+        \/\/ Owned by somebody else.\n+        monitor->set_anonymous_owner();\n+      }\n+      markWord monitor_mark = markWord::encode(monitor);\n+      markWord old_mark = object->cas_set_mark(monitor_mark, mark);\n+      if (old_mark == mark) {\n+        \/\/ Success! Return inflated monitor.\n+        if (own) {\n+          size_t removed = locking_thread->lock_stack().remove(object);\n+          monitor->set_recursions(removed - 1);\n+        }\n+        \/\/ Once the ObjectMonitor is configured and object is associated\n+        \/\/ with the ObjectMonitor, it is safe to allow async deflation:\n+        ObjectSynchronizer::_in_use_list.add(monitor);\n+\n+        log_inflate(current, object, cause);\n+        if (event.should_commit()) {\n+          post_monitor_inflate_event(&event, object, cause);\n+        }\n+        return monitor;\n+      } else {\n+        delete monitor;\n+        continue;  \/\/ Interference -- just retry\n+      }\n+    }\n+\n+    \/\/ CASE: unlocked\n+    \/\/ TODO-FIXME: for entry we currently inflate and then try to CAS _owner.\n+    \/\/ If we know we're inflating for entry it's better to inflate by swinging a\n+    \/\/ pre-locked ObjectMonitor pointer into the object header.   A successful\n+    \/\/ CAS inflates the object *and* confers ownership to the inflating thread.\n+    \/\/ In the current implementation we use a 2-step mechanism where we CAS()\n+    \/\/ to inflate and then CAS() again to try to swing _owner from null to current.\n+    \/\/ An inflateTry() method that we could call from enter() would be useful.\n+\n+    assert(mark.is_unlocked(), \"invariant: header=\" INTPTR_FORMAT, mark.value());\n+    ObjectMonitor* m = new ObjectMonitor(object);\n+    \/\/ prepare m for installation - set monitor to initial state\n+    m->set_header(mark);\n+\n+    if (object->cas_set_mark(markWord::encode(m), mark) != mark) {\n+      delete m;\n+      m = nullptr;\n+      continue;\n+      \/\/ interference - the markword changed - just retry.\n+      \/\/ The state-transitions are one-way, so there's no chance of\n+      \/\/ live-lock -- \"Inflated\" is an absorbing state.\n+    }\n+\n+    \/\/ Once the ObjectMonitor is configured and object is associated\n+    \/\/ with the ObjectMonitor, it is safe to allow async deflation:\n+    ObjectSynchronizer::_in_use_list.add(m);\n+\n+    log_inflate(current, object, cause);\n+    if (event.should_commit()) {\n+      post_monitor_inflate_event(&event, object, cause);\n+    }\n+    return m;\n+  }\n+}\n+\n+ObjectMonitor* ObjectSynchronizer::inflate_fast_locked_object(oop object, ObjectSynchronizer::InflateCause cause, JavaThread* locking_thread, JavaThread* current) {\n+  VerifyThreadState vts(locking_thread, current);\n+  assert(locking_thread->lock_stack().contains(object), \"locking_thread must have object on its lock stack\");\n+\n+  ObjectMonitor* monitor;\n+\n+  if (!UseObjectMonitorTable) {\n+    return inflate_into_object_header(object, cause, locking_thread, current);\n+  }\n+\n+  \/\/ Inflating requires a hash code\n+  ObjectSynchronizer::FastHashCode(current, object);\n+\n+  markWord mark = object->mark_acquire();\n+  assert(!mark.is_unlocked(), \"Cannot be unlocked\");\n+\n+  for (;;) {\n+    \/\/ Fetch the monitor from the table\n+    monitor = get_or_insert_monitor(object, current, cause);\n+\n+    \/\/ ObjectMonitors are always inserted as anonymously owned, this thread is\n+    \/\/ the current holder of the monitor. So unless the entry is stale and\n+    \/\/ contains a deflating monitor it must be anonymously owned.\n+    if (monitor->has_anonymous_owner()) {\n+      \/\/ The monitor must be anonymously owned if it was added\n+      assert(monitor == get_monitor_from_table(current, object), \"The monitor must be found\");\n+      \/\/ New fresh monitor\n+      break;\n+    }\n+\n+    \/\/ If the monitor was not anonymously owned then we got a deflating monitor\n+    \/\/ from the table. We need to let the deflator make progress and remove this\n+    \/\/ entry before we are allowed to add a new one.\n+    os::naked_yield();\n+    assert(monitor->is_being_async_deflated(), \"Should be the reason\");\n+  }\n+\n+  \/\/ Set the mark word; loop to handle concurrent updates to other parts of the mark word\n+  while (mark.is_fast_locked()) {\n+    mark = object->cas_set_mark(mark.set_has_monitor(), mark);\n+  }\n+\n+  \/\/ Indicate that the monitor now has a known owner\n+  monitor->set_owner_from_anonymous(locking_thread);\n+\n+  \/\/ Remove the entry from the thread's lock stack\n+  monitor->set_recursions(locking_thread->lock_stack().remove(object) - 1);\n+\n+  if (locking_thread == current) {\n+    \/\/ Only change the thread local state of the current thread.\n+    locking_thread->om_set_monitor_cache(monitor);\n+  }\n+\n+  return monitor;\n+}\n+\n+ObjectMonitor* ObjectSynchronizer::inflate_and_enter(oop object, BasicLock* lock, ObjectSynchronizer::InflateCause cause, JavaThread* locking_thread, JavaThread* current) {\n+  VerifyThreadState vts(locking_thread, current);\n+\n+  \/\/ Note: In some paths (deoptimization) the 'current' thread inflates and\n+  \/\/ enters the lock on behalf of the 'locking_thread' thread.\n+\n+  ObjectMonitor* monitor = nullptr;\n+\n+  if (!UseObjectMonitorTable) {\n+    \/\/ Do the old inflate and enter.\n+    monitor = inflate_into_object_header(object, cause, locking_thread, current);\n+\n+    bool entered;\n+    if (locking_thread == current) {\n+      entered = monitor->enter(locking_thread);\n+    } else {\n+      entered = monitor->enter_for(locking_thread);\n+    }\n+\n+    \/\/ enter returns false for deflation found.\n+    return entered ? monitor : nullptr;\n+  }\n+\n+  NoSafepointVerifier nsv;\n+\n+  \/\/ Try to get the monitor from the thread-local cache.\n+  \/\/ There's no need to use the cache if we are locking\n+  \/\/ on behalf of another thread.\n+  if (current == locking_thread) {\n+    monitor = read_caches(current, lock, object);\n+  }\n+\n+  \/\/ Get or create the monitor\n+  if (monitor == nullptr) {\n+    \/\/ Lightweight monitors require that hash codes are installed first\n+    ObjectSynchronizer::FastHashCode(locking_thread, object);\n+    monitor = get_or_insert_monitor(object, current, cause);\n+  }\n+\n+  if (monitor->try_enter(locking_thread)) {\n+    return monitor;\n+  }\n+\n+  \/\/ Holds is_being_async_deflated() stable throughout this function.\n+  ObjectMonitorContentionMark contention_mark(monitor);\n+\n+  \/\/\/ First handle the case where the monitor from the table is deflated\n+  if (monitor->is_being_async_deflated()) {\n+    \/\/ The MonitorDeflation thread is deflating the monitor. The locking thread\n+    \/\/ must spin until further progress has been made.\n+\n+    \/\/ Clear the BasicLock cache as it may contain this monitor.\n+    lock->clear_object_monitor_cache();\n+\n+    const markWord mark = object->mark_acquire();\n+\n+    if (mark.has_monitor()) {\n+      \/\/ Waiting on the deflation thread to remove the deflated monitor from the table.\n+      os::naked_yield();\n+\n+    } else if (mark.is_fast_locked()) {\n+      \/\/ Some other thread managed to fast-lock the lock, or this is a\n+      \/\/ recursive lock from the same thread; yield for the deflation\n+      \/\/ thread to remove the deflated monitor from the table.\n+      os::naked_yield();\n+\n+    } else {\n+      assert(mark.is_unlocked(), \"Implied\");\n+      \/\/ Retry immediately\n+    }\n+\n+    \/\/ Retry\n+    return nullptr;\n+  }\n+\n+  for (;;) {\n+    const markWord mark = object->mark_acquire();\n+    \/\/ The mark can be in one of the following states:\n+    \/\/ *  inflated     - If the ObjectMonitor owner is anonymous\n+    \/\/                   and the locking_thread owns the object\n+    \/\/                   lock, then we make the locking_thread\n+    \/\/                   the ObjectMonitor owner and remove the\n+    \/\/                   lock from the locking_thread's lock stack.\n+    \/\/ *  fast-locked  - Coerce it to inflated from fast-locked.\n+    \/\/ *  neutral      - Inflate the object. Successful CAS is locked\n+\n+    \/\/ CASE: inflated\n+    if (mark.has_monitor()) {\n+      LockStack& lock_stack = locking_thread->lock_stack();\n+      if (monitor->has_anonymous_owner() && lock_stack.contains(object)) {\n+        \/\/ The lock is fast-locked by the locking thread,\n+        \/\/ convert it to a held monitor with a known owner.\n+        monitor->set_owner_from_anonymous(locking_thread);\n+        monitor->set_recursions(lock_stack.remove(object) - 1);\n+      }\n+\n+      break; \/\/ Success\n+    }\n+\n+    \/\/ CASE: fast-locked\n+    \/\/ Could be fast-locked either by locking_thread or by some other thread.\n+    \/\/\n+    if (mark.is_fast_locked()) {\n+      markWord old_mark = object->cas_set_mark(mark.set_has_monitor(), mark);\n+      if (old_mark != mark) {\n+        \/\/ CAS failed\n+        continue;\n+      }\n+\n+      \/\/ Success! Return inflated monitor.\n+      LockStack& lock_stack = locking_thread->lock_stack();\n+      if (lock_stack.contains(object)) {\n+        \/\/ The lock is fast-locked by the locking thread,\n+        \/\/ convert it to a held monitor with a known owner.\n+        monitor->set_owner_from_anonymous(locking_thread);\n+        monitor->set_recursions(lock_stack.remove(object) - 1);\n+      }\n+\n+      break; \/\/ Success\n+    }\n+\n+    \/\/ CASE: neutral (unlocked)\n+\n+    \/\/ Catch if the object's header is not neutral (not locked and\n+    \/\/ not marked is what we care about here).\n+    assert(mark.is_neutral(), \"invariant: header=\" INTPTR_FORMAT, mark.value());\n+    markWord old_mark = object->cas_set_mark(mark.set_has_monitor(), mark);\n+    if (old_mark != mark) {\n+      \/\/ CAS failed\n+      continue;\n+    }\n+\n+    \/\/ Transitioned from unlocked to monitor means locking_thread owns the lock.\n+    monitor->set_owner_from_anonymous(locking_thread);\n+\n+    return monitor;\n+  }\n+\n+  if (current == locking_thread) {\n+    \/\/ One round of spinning\n+    if (monitor->spin_enter(locking_thread)) {\n+      return monitor;\n+    }\n+\n+    \/\/ Monitor is contended, take the time before entering to fix the lock stack.\n+    LockStackInflateContendedLocks().inflate(current);\n+  }\n+\n+  \/\/ enter can block for safepoints; clear the unhandled object oop\n+  PauseNoSafepointVerifier pnsv(&nsv);\n+  object = nullptr;\n+\n+  if (current == locking_thread) {\n+    monitor->enter_with_contention_mark(locking_thread, contention_mark);\n+  } else {\n+    monitor->enter_for_with_contention_mark(locking_thread, contention_mark);\n+  }\n+\n+  return monitor;\n+}\n+\n+void ObjectSynchronizer::deflate_monitor(Thread* current, oop obj, ObjectMonitor* monitor) {\n+  if (obj != nullptr) {\n+    deflate_mark_word(obj);\n+  }\n+  bool removed = remove_monitor(current, monitor, obj);\n+  if (obj != nullptr) {\n+    assert(removed, \"Should have removed the entry if obj was alive\");\n+  }\n+}\n+\n+ObjectMonitor* ObjectSynchronizer::get_monitor_from_table(Thread* current, oop obj) {\n+  assert(UseObjectMonitorTable, \"must be\");\n+  return ObjectMonitorTable::monitor_get(current, obj);\n+}\n+\n+bool ObjectSynchronizer::contains_monitor(Thread* current, ObjectMonitor* monitor) {\n+  assert(UseObjectMonitorTable, \"must be\");\n+  return ObjectMonitorTable::contains_monitor(current, monitor);\n+}\n+\n+ObjectMonitor* ObjectSynchronizer::read_monitor(markWord mark) {\n+  return mark.monitor();\n+}\n+\n+ObjectMonitor* ObjectSynchronizer::read_monitor(Thread* current, oop obj) {\n+  return ObjectSynchronizer::read_monitor(current, obj, obj->mark());\n+}\n+\n+ObjectMonitor* ObjectSynchronizer::read_monitor(Thread* current, oop obj, markWord mark) {\n+  if (!UseObjectMonitorTable) {\n+    return read_monitor(mark);\n+  } else {\n+    return ObjectSynchronizer::get_monitor_from_table(current, obj);\n+  }\n+}\n+\n+bool ObjectSynchronizer::quick_enter_internal(oop obj, BasicLock* lock, JavaThread* current) {\n+  assert(current->thread_state() == _thread_in_Java, \"must be\");\n+  assert(obj != nullptr, \"must be\");\n+  NoSafepointVerifier nsv;\n+\n+  LockStack& lock_stack = current->lock_stack();\n+  if (lock_stack.is_full()) {\n+    \/\/ Always go into runtime if the lock stack is full.\n+    return false;\n+  }\n+\n+  const markWord mark = obj->mark();\n+\n+#ifndef _LP64\n+  \/\/ Only for 32bit which has limited support for fast locking outside the runtime.\n+  if (lock_stack.try_recursive_enter(obj)) {\n+    \/\/ Recursive lock successful.\n+    return true;\n+  }\n+\n+  if (mark.is_unlocked()) {\n+    markWord locked_mark = mark.set_fast_locked();\n+    if (obj->cas_set_mark(locked_mark, mark) == mark) {\n+      \/\/ Successfully fast-locked, push object to lock-stack and return.\n+      lock_stack.push(obj);\n+      return true;\n+    }\n+  }\n+#endif\n+\n+  if (mark.has_monitor()) {\n+    ObjectMonitor* monitor;\n+    if (UseObjectMonitorTable) {\n+      monitor = read_caches(current, lock, obj);\n+    } else {\n+      monitor = ObjectSynchronizer::read_monitor(mark);\n+    }\n+\n+    if (monitor == nullptr) {\n+      \/\/ Take the slow-path on a cache miss.\n+      return false;\n+    }\n+\n+    if (UseObjectMonitorTable) {\n+      \/\/ Set the monitor regardless of success.\n+      \/\/ Either we successfully lock on the monitor, or we retry with the\n+      \/\/ monitor in the slow path. If the monitor gets deflated, it will be\n+      \/\/ cleared, either by the CacheSetter if we fast lock in enter or in\n+      \/\/ inflate_and_enter when we see that the monitor is deflated.\n+      lock->set_object_monitor_cache(monitor);\n+    }\n+\n+    if (monitor->spin_enter(current)) {\n+      return true;\n+    }\n+  }\n+\n+  \/\/ Slow-path.\n+  return false;\n+}\n+\n+bool ObjectSynchronizer::quick_enter(oop obj, BasicLock* lock, JavaThread* current) {\n+  assert(current->thread_state() == _thread_in_Java, \"invariant\");\n+  NoSafepointVerifier nsv;\n+  if (obj == nullptr) return false;       \/\/ Need to throw NPE\n+\n+  if (obj->klass()->is_value_based()) {\n+    return false;\n+  }\n+\n+  return ObjectSynchronizer::quick_enter_internal(obj, lock, current);\n+}\n","filename":"src\/hotspot\/share\/runtime\/synchronizer.cpp","additions":1367,"deletions":755,"binary":false,"changes":2122,"status":"modified"},{"patch":"@@ -2,1 +2,1 @@\n- * Copyright (c) 1998, 2024, Oracle and\/or its affiliates. All rights reserved.\n+ * Copyright (c) 1998, 2025, Oracle and\/or its affiliates. All rights reserved.\n@@ -33,1 +33,1 @@\n-#include \"utilities\/resourceHash.hpp\"\n+#include \"utilities\/hashTable.hpp\"\n@@ -76,1 +76,1 @@\n- public:\n+public:\n@@ -82,4 +82,3 @@\n-    inflate_cause_hash_code = 4,\n-    inflate_cause_jni_enter = 5,\n-    inflate_cause_jni_exit = 6,\n-    inflate_cause_nof = 7 \/\/ Number of causes\n+    inflate_cause_jni_enter = 4,\n+    inflate_cause_jni_exit = 5,\n+    inflate_cause_nof = 6 \/\/ Number of causes\n@@ -98,2 +97,2 @@\n-  static inline void enter(Handle obj, BasicLock* lock, JavaThread* current);\n-  static inline void exit(oop obj, BasicLock* lock, JavaThread* current);\n+  static void enter(Handle obj, BasicLock* lock, JavaThread* current);\n+  static void exit(oop obj, BasicLock* lock, JavaThread* current);\n@@ -107,8 +106,0 @@\n-private:\n-  \/\/ Shared implementation for enter and enter_for. Performs all but\n-  \/\/ inflated monitor enter.\n-  static bool enter_fast_impl(Handle obj, BasicLock* lock, JavaThread* locking_thread);\n-  static bool quick_enter_legacy(oop obj, BasicLock* Lock, JavaThread* current);\n-  static void enter_legacy(Handle obj, BasicLock* Lock, JavaThread* current);\n-  static void exit_legacy(oop obj, BasicLock* lock, JavaThread* current);\n-public:\n@@ -127,1 +118,1 @@\n-  static inline bool quick_enter(oop obj, BasicLock* Lock, JavaThread* current);\n+  static bool quick_enter(oop obj, BasicLock* Lock, JavaThread* current);\n@@ -134,11 +125,0 @@\n-  \/\/ Inflate light weight monitor to heavy weight monitor\n-  static ObjectMonitor* inflate(Thread* current, oop obj, const InflateCause cause);\n-  \/\/ Used to inflate a monitor as if it was done from the thread JavaThread.\n-  static ObjectMonitor* inflate_for(JavaThread* thread, oop obj, const InflateCause cause);\n-\n-private:\n-  \/\/ Shared implementation between the different LockingMode.\n-  static ObjectMonitor* inflate_impl(JavaThread* locking_thread, oop obj, const InflateCause cause);\n-\n-  \/\/ This version is only for internal use\n-  static void inflate_helper(oop obj);\n@@ -148,2 +128,3 @@\n-  inline static ObjectMonitor* read_monitor(markWord mark);\n-  inline static ObjectMonitor* read_monitor(Thread* current, oop obj, markWord mark);\n+  static ObjectMonitor* read_monitor(markWord mark);\n+  static ObjectMonitor* read_monitor(Thread* current, oop obj);\n+  static ObjectMonitor* read_monitor(Thread* current, oop obj, markWord mark);\n@@ -155,0 +136,5 @@\n+  \/\/ NOTE: May not cause monitor inflation\n+  static uint32_t get_hash(markWord mark, oop obj);\n+  \/\/ For CDS path.\n+  static uint32_t get_hash(markWord mark, oop obj, Klass* klass);\n+\n@@ -219,1 +205,0 @@\n-  friend class LightweightSynchronizer;\n@@ -233,0 +218,38 @@\n+\n+  static ObjectMonitor* get_or_insert_monitor_from_table(oop object, JavaThread* current, bool* inserted);\n+  static ObjectMonitor* get_or_insert_monitor(oop object, JavaThread* current, ObjectSynchronizer::InflateCause cause);\n+\n+  static ObjectMonitor* add_monitor(JavaThread* current, ObjectMonitor* monitor, oop obj);\n+  static bool remove_monitor(Thread* current, ObjectMonitor* monitor, oop obj);\n+\n+  static void deflate_mark_word(oop object);\n+\n+  static void ensure_lock_stack_space(JavaThread* current);\n+\n+  class CacheSetter;\n+  class LockStackInflateContendedLocks;\n+  class VerifyThreadState;\n+\n+  static void create_om_table();\n+\n+ public:\n+  static bool needs_resize();\n+  static bool resize_table(JavaThread* current);\n+\n+ private:\n+  static inline bool fast_lock_try_enter(oop obj, LockStack& lock_stack, JavaThread* current);\n+  static bool fast_lock_spin_enter(oop obj, LockStack& lock_stack, JavaThread* current, bool observed_deflation);\n+\n+ public:\n+  static ObjectMonitor* inflate_into_object_header(oop object, ObjectSynchronizer::InflateCause cause, JavaThread* locking_thread, Thread* current);\n+  static ObjectMonitor* inflate_locked_or_imse(oop object, ObjectSynchronizer::InflateCause cause, TRAPS);\n+  static ObjectMonitor* inflate_fast_locked_object(oop object, ObjectSynchronizer::InflateCause cause, JavaThread* locking_thread, JavaThread* current);\n+  static ObjectMonitor* inflate_and_enter(oop object, BasicLock* lock, ObjectSynchronizer::InflateCause cause, JavaThread* locking_thread, JavaThread* current);\n+\n+  static void deflate_monitor(Thread* current, oop obj, ObjectMonitor* monitor);\n+\n+  static ObjectMonitor* get_monitor_from_table(Thread* current, oop obj);\n+\n+  static bool contains_monitor(Thread* current, ObjectMonitor* monitor);\n+\n+  static bool quick_enter_internal(oop obj, BasicLock* Lock, JavaThread* current);\n@@ -249,0 +272,1 @@\n+  bool    _skip_exit;\n@@ -250,1 +274,1 @@\n-  ObjectLocker(Handle obj, JavaThread* current);\n+  ObjectLocker(Handle obj, TRAPS);\n@@ -254,2 +278,1 @@\n-  void wait(TRAPS)  { ObjectSynchronizer::wait(_obj, 0, CHECK); } \/\/ wait forever\n-  void wait_uninterruptibly(TRAPS)  { ObjectSynchronizer::waitUninterruptibly(_obj, 0, CHECK); } \/\/ wait forever\n+  void wait_uninterruptibly(TRAPS);\n","filename":"src\/hotspot\/share\/runtime\/synchronizer.hpp","additions":58,"deletions":35,"binary":false,"changes":93,"status":"modified"},{"patch":"@@ -1,1 +1,0 @@\n-\n@@ -49,1 +48,0 @@\n-#include \"memory\/allocation.hpp\"\n@@ -59,1 +57,1 @@\n-#include \"oops\/constMethod.hpp\"\n+#include \"oops\/bsmAttribute.hpp\"\n@@ -61,0 +59,1 @@\n+#include \"oops\/constMethod.hpp\"\n@@ -63,3 +62,0 @@\n-#include \"oops\/instanceClassLoaderKlass.hpp\"\n-#include \"oops\/instanceMirrorKlass.hpp\"\n-#include \"oops\/instanceStackChunkKlass.hpp\"\n@@ -89,1 +85,0 @@\n-#include \"runtime\/java.hpp\"\n@@ -102,1 +97,1 @@\n-#include \"runtime\/vmStructs.hpp\"\n+#include \"runtime\/vmStructs.hpp\"\n@@ -175,0 +170,3 @@\n+  nonstatic_field(BSMAttributeEntries,         _offsets,                                      Array<u4>*)                            \\\n+  nonstatic_field(BSMAttributeEntries,         _bootstrap_methods,                            Array<u2>*)                            \\\n+  nonstatic_field(ConstantPool,                _bsm_entries,                                  BSMAttributeEntries)                   \\\n@@ -178,1 +176,0 @@\n-  nonstatic_field(ConstantPool,                _operands,                                     Array<u2>*)                            \\\n@@ -325,1 +322,1 @@\n-  nonstatic_field(JNIid,                       _holder,                                       Klass*)                                \\\n+  nonstatic_field(JNIid,                       _holder,                                       InstanceKlass*)                        \\\n@@ -352,2 +349,2 @@\n-     static_field(MetaspaceObj,                _shared_metaspace_base,                        void*)                                 \\\n-     static_field(MetaspaceObj,                _shared_metaspace_top,                         void*)                                 \\\n+     static_field(MetaspaceObj,                _aot_metaspace_base,                           void*)                                 \\\n+     static_field(MetaspaceObj,                _aot_metaspace_top,                            void*)                                 \\\n@@ -360,1 +357,0 @@\n-     static_field(ThreadLocalAllocBuffer,      _reserve_for_allocation_prefetch,              int)                                   \\\n@@ -545,2 +541,1 @@\n-  nonstatic_field(nmethod,                     _deopt_handler_offset,                         int)                                   \\\n-  nonstatic_field(nmethod,                     _deopt_mh_handler_offset,                      int)                                   \\\n+  nonstatic_field(nmethod,                     _deopt_handler_entry_offset,                   int)                                   \\\n@@ -549,0 +544,1 @@\n+  nonstatic_field(nmethod,                     _immutable_data_ref_count_offset,              int)                                   \\\n@@ -618,1 +614,0 @@\n-  volatile_nonstatic_field(JavaThread,         _is_method_handle_return,                      int)                                   \\\n@@ -628,0 +623,1 @@\n+  nonstatic_field(JavaThread,                  _cont_entry,                                   ContinuationEntry*)                    \\\n@@ -671,0 +667,8 @@\n+  \/******************************************************************************************\/                                       \\\n+  \/* CI (NOTE: these CI fields are retained in VMStructs for the benefit of external tools, *\/                                       \\\n+  \/* to ease their migration to a future alternative.)                                      *\/                                       \\\n+  \/******************************************************************************************\/                                       \\\n+                                                                                                                                     \\\n+  nonstatic_field(CompilerThread,              _env,                                          ciEnv*)                                \\\n+  nonstatic_field(ciEnv,                       _task,                                         CompileTask*)                          \\\n+                                                                                                                                     \\\n@@ -678,2 +682,0 @@\n-  volatile_nonstatic_field(ObjectMonitor,      _stack_locker,                                 BasicLock*)                            \\\n-  volatile_nonstatic_field(BasicLock,          _metadata,                                     uintptr_t)                             \\\n@@ -738,0 +740,1 @@\n+  unchecked_nonstatic_field(Array<u4>,                 _data,                                 sizeof(u4))                            \\\n@@ -802,1 +805,2 @@\n-  volatile_nonstatic_field(Mutex,              _owner,                                        Thread*)\n+  volatile_nonstatic_field(Mutex,              _owner,                                        Thread*)                               \\\n+  static_field(ContinuationEntry,              _return_pc,                                    address)\n@@ -968,0 +972,1 @@\n+  declare_toplevel_type(BSMAttributeEntries)                              \\\n@@ -1154,0 +1159,6 @@\n+  \/*********************\/                                                 \\\n+  \/* CI *\/                                                                \\\n+  \/*********************\/                                                 \\\n+                                                                          \\\n+  declare_toplevel_type(ciEnv)                                            \\\n+                                                                          \\\n@@ -1270,0 +1281,1 @@\n+  declare_toplevel_type(ContinuationEntry)                                \\\n@@ -1570,0 +1582,1 @@\n+  declare_constant(Deoptimization::Reason_short_running_long_loop)        \\\n@@ -1577,1 +1590,1 @@\n-  NOT_ZERO(JVMCI_ONLY(declare_constant(Deoptimization::Reason_aliasing)))                       \\\n+  declare_constant(Deoptimization::Reason_not_compiled_exception_handler) \\\n@@ -1579,1 +1592,0 @@\n-  NOT_ZERO(JVMCI_ONLY(declare_constant(Deoptimization::Reason_not_compiled_exception_handler))) \\\n@@ -1645,8 +1657,0 @@\n-  \/**********************************************\/                        \\\n-  \/* LockingMode enum (globalDefinitions.hpp) *\/                          \\\n-  \/**********************************************\/                        \\\n-                                                                          \\\n-  declare_constant(LM_MONITOR)                                            \\\n-  declare_constant(LM_LEGACY)                                             \\\n-  declare_constant(LM_LIGHTWEIGHT)                                        \\\n-                                                                          \\\n@@ -1717,1 +1721,0 @@\n-  declare_constant(PcDesc::PCDESC_is_method_handle_invoke)                \\\n","filename":"src\/hotspot\/share\/runtime\/vmStructs.cpp","additions":33,"deletions":30,"binary":false,"changes":63,"status":"modified"},{"patch":"@@ -2,1 +2,1 @@\n- * Copyright (c) 2001, 2024, Oracle and\/or its affiliates. All rights reserved.\n+ * Copyright (c) 2001, 2025, Oracle and\/or its affiliates. All rights reserved.\n@@ -166,6 +166,0 @@\n-  public BasicLock locker() {\n-    if (Assert.ASSERTS_ENABLED) {\n-      Assert.that(hasLocker(), \"check\");\n-    }\n-    return new BasicLock(valueAsAddress());\n-  }\n","filename":"src\/jdk.hotspot.agent\/share\/classes\/sun\/jvm\/hotspot\/oops\/Mark.java","additions":1,"deletions":7,"binary":false,"changes":8,"status":"modified"},{"patch":"@@ -29,1 +29,1 @@\n-#include \"runtime\/atomic.hpp\"\n+#include \"runtime\/atomicAccess.hpp\"\n@@ -109,1 +109,1 @@\n-    ol.wait(THREAD);\n+    ol.wait_uninterruptibly(THREAD);\n","filename":"test\/hotspot\/gtest\/oops\/test_markWord.cpp","additions":2,"deletions":2,"binary":false,"changes":4,"status":"modified"},{"patch":"@@ -148,0 +148,10 @@\n+    \/\/ Signed comparison: I\/L\n+    \/\/    I for I\n+    private int cmoveIEQforI(int a, int b, int c, int d) {\n+        return (a == b) ? c : d;\n+    }\n+\n+    private int cmoveINEforI(int a, int b, int c, int d) {\n+        return (a != b) ? c : d;\n+    }\n+\n@@ -152,0 +162,21 @@\n+    private int cmoveIGEforI(int a, int b, int c, int d) {\n+        return (a >= b) ? c : d;\n+    }\n+\n+    private int cmoveILTforI(int a, int b, int c, int d) {\n+        return (a < b) ? c : d;\n+    }\n+\n+    private int cmoveILEforI(int a, int b, int c, int d) {\n+        return (a <= b) ? c : d;\n+    }\n+\n+    \/\/    I for L\n+    private long cmoveIEQforL(int a, int b, long c, long d) {\n+        return (a == b) ? c : d;\n+    }\n+\n+    private long cmoveINEforL(int a, int b, long c, long d) {\n+        return (a != b) ? c : d;\n+    }\n+\n@@ -156,0 +187,21 @@\n+    private long cmoveIGEforL(int a, int b, long c, long d) {\n+        return (a >= b) ? c : d;\n+    }\n+\n+    private long cmoveILTforL(int a, int b, long c, long d) {\n+        return (a < b) ? c : d;\n+    }\n+\n+    private long cmoveILEforL(int a, int b, long c, long d) {\n+        return (a <= b) ? c : d;\n+    }\n+\n+    \/\/    I for F\n+    private float cmoveIEQforF(int a, int b, float c, float d) {\n+        return (a == b) ? c : d;\n+    }\n+\n+    private float cmoveINEforF(int a, int b, float c, float d) {\n+        return (a != b) ? c : d;\n+    }\n+\n@@ -160,0 +212,21 @@\n+    private float cmoveIGEforF(int a, int b, float c, float d) {\n+        return (a >= b) ? c : d;\n+    }\n+\n+    private float cmoveILTforF(int a, int b, float c, float d) {\n+        return (a < b) ? c : d;\n+    }\n+\n+    private float cmoveILEforF(int a, int b, float c, float d) {\n+        return (a <= b) ? c : d;\n+    }\n+\n+    \/\/    I for D\n+    private double cmoveIEQforD(int a, int b, double c, double d) {\n+        return (a == b) ? c : d;\n+    }\n+\n+    private double cmoveINEforD(int a, int b, double c, double d) {\n+        return (a != b) ? c : d;\n+    }\n+\n@@ -164,0 +237,21 @@\n+    private double cmoveIGEforD(int a, int b, double c, double d) {\n+        return (a >= b) ? c : d;\n+    }\n+\n+    private double cmoveILTforD(int a, int b, double c, double d) {\n+        return (a < b) ? c : d;\n+    }\n+\n+    private double cmoveILEforD(int a, int b, double c, double d) {\n+        return (a <= b) ? c : d;\n+    }\n+\n+    \/\/    L for I\n+    private int cmoveLEQforI(long a, long b, int c, int d) {\n+        return (a == b) ? c : d;\n+    }\n+\n+    private int cmoveLNEforI(long a, long b, int c, int d) {\n+        return (a != b) ? c : d;\n+    }\n+\n@@ -168,0 +262,21 @@\n+    private int cmoveLGEforI(long a, long b, int c, int d) {\n+        return (a >= b) ? c : d;\n+    }\n+\n+    private int cmoveLLTforI(long a, long b, int c, int d) {\n+        return (a < b) ? c : d;\n+    }\n+\n+    private int cmoveLLEforI(long a, long b, int c, int d) {\n+        return (a <= b) ? c : d;\n+    }\n+\n+    \/\/    L for L\n+    private long cmoveLEQforL(long a, long b, long c, long d) {\n+        return (a == b) ? c : d;\n+    }\n+\n+    private long cmoveLNEforL(long a, long b, long c, long d) {\n+        return (a != b) ? c : d;\n+    }\n+\n@@ -172,0 +287,21 @@\n+    private long cmoveLGEforL(long a, long b, long c, long d) {\n+        return (a >= b) ? c : d;\n+    }\n+\n+    private long cmoveLLTforL(long a, long b, long c, long d) {\n+        return (a < b) ? c : d;\n+    }\n+\n+    private long cmoveLLEforL(long a, long b, long c, long d) {\n+        return (a <= b) ? c : d;\n+    }\n+\n+    \/\/    L for F\n+    private float cmoveLEQforF(long a, long b, float c, float d) {\n+        return (a == b) ? c : d;\n+    }\n+\n+    private float cmoveLNEforF(long a, long b, float c, float d) {\n+        return (a != b) ? c : d;\n+    }\n+\n@@ -176,0 +312,21 @@\n+    private float cmoveLGEforF(long a, long b, float c, float d) {\n+        return (a >= b) ? c : d;\n+    }\n+\n+    private float cmoveLLTforF(long a, long b, float c, float d) {\n+        return (a < b) ? c : d;\n+    }\n+\n+    private float cmoveLLEforF(long a, long b, float c, float d) {\n+        return (a <= b) ? c : d;\n+    }\n+\n+    \/\/    L for D\n+    private double cmoveLEQforD(long a, long b, double c, double d) {\n+        return (a == b) ? c : d;\n+    }\n+\n+    private double cmoveLNEforD(long a, long b, double c, double d) {\n+        return (a != b) ? c : d;\n+    }\n+\n@@ -180,0 +337,214 @@\n+    private double cmoveLGEforD(long a, long b, double c, double d) {\n+        return (a >= b) ? c : d;\n+    }\n+\n+    private double cmoveLLTforD(long a, long b, double c, double d) {\n+        return (a < b) ? c : d;\n+    }\n+\n+    private double cmoveLLEforD(long a, long b, double c, double d) {\n+        return (a <= b) ? c : d;\n+    }\n+\n+    \/\/ Unsigned comparison: I\/L\n+    \/\/    I for I\n+    private int cmoveUIEQforI(int a, int b, int c, int d) {\n+        return Integer.compareUnsigned(a, b) == 0 ? c : d;\n+    }\n+\n+    private int cmoveUINEforI(int a, int b, int c, int d) {\n+        return Integer.compareUnsigned(a, b) != 0 ? c : d;\n+    }\n+\n+    private int cmoveUIGTforI(int a, int b, int c, int d) {\n+        return Integer.compareUnsigned(a, b) > 0 ? c : d;\n+    }\n+\n+    private int cmoveUIGEforI(int a, int b, int c, int d) {\n+        return Integer.compareUnsigned(a, b) >= 0 ? c : d;\n+    }\n+\n+    private int cmoveUILTforI(int a, int b, int c, int d) {\n+        return Integer.compareUnsigned(a, b) < 0 ? c : d;\n+    }\n+\n+    private int cmoveUILEforI(int a, int b, int c, int d) {\n+        return Integer.compareUnsigned(a, b) <= 0 ? c : d;\n+    }\n+\n+    \/\/    I for L\n+    private long cmoveUIEQforL(int a, int b, long c, long d) {\n+        return Integer.compareUnsigned(a, b) == 0 ? c : d;\n+    }\n+\n+    private long cmoveUINEforL(int a, int b, long c, long d) {\n+        return Integer.compareUnsigned(a, b) != 0 ? c : d;\n+    }\n+\n+    private long cmoveUIGTforL(int a, int b, long c, long d) {\n+        return Integer.compareUnsigned(a, b) > 0 ? c : d;\n+    }\n+\n+    private long cmoveUIGEforL(int a, int b, long c, long d) {\n+        return Integer.compareUnsigned(a, b) >= 0 ? c : d;\n+    }\n+\n+    private long cmoveUILTforL(int a, int b, long c, long d) {\n+        return Integer.compareUnsigned(a, b) < 0 ? c : d;\n+    }\n+\n+    private long cmoveUILEforL(int a, int b, long c, long d) {\n+        return Integer.compareUnsigned(a, b) <= 0 ? c : d;\n+    }\n+\n+    \/\/    I for F\n+    private float cmoveUIEQforF(int a, int b, float c, float d) {\n+        return Integer.compareUnsigned(a, b) == 0 ? c : d;\n+    }\n+\n+    private float cmoveUINEforF(int a, int b, float c, float d) {\n+        return Integer.compareUnsigned(a, b) != 0 ? c : d;\n+    }\n+\n+    private float cmoveUIGTforF(int a, int b, float c, float d) {\n+        return Integer.compareUnsigned(a, b) > 0 ? c : d;\n+    }\n+\n+    private float cmoveUIGEforF(int a, int b, float c, float d) {\n+        return Integer.compareUnsigned(a, b) >= 0 ? c : d;\n+    }\n+\n+    private float cmoveUILTforF(int a, int b, float c, float d) {\n+        return Integer.compareUnsigned(a, b) < 0 ? c : d;\n+    }\n+\n+    private float cmoveUILEforF(int a, int b, float c, float d) {\n+        return Integer.compareUnsigned(a, b) <= 0 ? c : d;\n+    }\n+\n+    \/\/    I for D\n+    private double cmoveUIEQforD(int a, int b, double c, double d) {\n+        return Integer.compareUnsigned(a, b) == 0 ? c : d;\n+    }\n+\n+    private double cmoveUINEforD(int a, int b, double c, double d) {\n+        return Integer.compareUnsigned(a, b) != 0 ? c : d;\n+    }\n+\n+    private double cmoveUIGTforD(int a, int b, double c, double d) {\n+        return Integer.compareUnsigned(a, b) > 0 ? c : d;\n+    }\n+\n+    private double cmoveUIGEforD(int a, int b, double c, double d) {\n+        return Integer.compareUnsigned(a, b) >= 0 ? c : d;\n+    }\n+\n+    private double cmoveUILTforD(int a, int b, double c, double d) {\n+        return Integer.compareUnsigned(a, b) < 0 ? c : d;\n+    }\n+\n+    private double cmoveUILEforD(int a, int b, double c, double d) {\n+        return Integer.compareUnsigned(a, b) <= 0 ? c : d;\n+    }\n+\n+    \/\/    L for I\n+    private int cmoveULEQforI(long a, long b, int c, int d) {\n+        return Long.compareUnsigned(a, b) == 0 ? c : d;\n+    }\n+\n+    private int cmoveULNEforI(long a, long b, int c, int d) {\n+        return Long.compareUnsigned(a, b) != 0 ? c : d;\n+    }\n+\n+    private int cmoveULGTforI(long a, long b, int c, int d) {\n+        return Long.compareUnsigned(a, b) > 0 ? c : d;\n+    }\n+\n+    private int cmoveULGEforI(long a, long b, int c, int d) {\n+        return Long.compareUnsigned(a, b) >= 0 ? c : d;\n+    }\n+\n+    private int cmoveULLTforI(long a, long b, int c, int d) {\n+        return Long.compareUnsigned(a, b) < 0 ? c : d;\n+    }\n+\n+    private int cmoveULLEforI(long a, long b, int c, int d) {\n+        return Long.compareUnsigned(a, b) <= 0 ? c : d;\n+    }\n+\n+    \/\/    L for L\n+    private long cmoveULEQforL(long a, long b, long c, long d) {\n+        return Long.compareUnsigned(a, b) == 0 ? c : d;\n+    }\n+\n+    private long cmoveULNEforL(long a, long b, long c, long d) {\n+        return Long.compareUnsigned(a, b) != 0 ? c : d;\n+    }\n+\n+    private long cmoveULGTforL(long a, long b, long c, long d) {\n+        return Long.compareUnsigned(a, b) > 0 ? c : d;\n+    }\n+\n+    private long cmoveULGEforL(long a, long b, long c, long d) {\n+        return Long.compareUnsigned(a, b) >= 0 ? c : d;\n+    }\n+\n+    private long cmoveULLTforL(long a, long b, long c, long d) {\n+        return Long.compareUnsigned(a, b) < 0 ? c : d;\n+    }\n+\n+    private long cmoveULLEforL(long a, long b, long c, long d) {\n+        return Long.compareUnsigned(a, b) <= 0 ? c : d;\n+    }\n+\n+    \/\/    L for F\n+    private float cmoveULEQforF(long a, long b, float c, float d) {\n+        return Long.compareUnsigned(a, b) == 0 ? c : d;\n+    }\n+\n+    private float cmoveULNEforF(long a, long b, float c, float d) {\n+        return Long.compareUnsigned(a, b) != 0 ? c : d;\n+    }\n+\n+    private float cmoveULGTforF(long a, long b, float c, float d) {\n+        return Long.compareUnsigned(a, b) > 0 ? c : d;\n+    }\n+\n+    private float cmoveULGEforF(long a, long b, float c, float d) {\n+        return Long.compareUnsigned(a, b) >= 0 ? c : d;\n+    }\n+\n+    private float cmoveULLTforF(long a, long b, float c, float d) {\n+        return Long.compareUnsigned(a, b) < 0 ? c : d;\n+    }\n+\n+    private float cmoveULLEforF(long a, long b, float c, float d) {\n+        return Long.compareUnsigned(a, b) <= 0 ? c : d;\n+    }\n+\n+    \/\/    L for D\n+    private double cmoveULEQforD(long a, long b, double c, double d) {\n+        return Long.compareUnsigned(a, b) == 0 ? c : d;\n+    }\n+\n+    private double cmoveULNEforD(long a, long b, double c, double d) {\n+        return Long.compareUnsigned(a, b) != 0 ? c : d;\n+    }\n+\n+    private double cmoveULGTforD(long a, long b, double c, double d) {\n+        return Long.compareUnsigned(a, b) > 0 ? c : d;\n+    }\n+\n+    private double cmoveULGEforD(long a, long b, double c, double d) {\n+        return Long.compareUnsigned(a, b) >= 0 ? c : d;\n+    }\n+\n+    private double cmoveULLTforD(long a, long b, double c, double d) {\n+        return Long.compareUnsigned(a, b) < 0 ? c : d;\n+    }\n+\n+    private double cmoveULLEforD(long a, long b, double c, double d) {\n+        return Long.compareUnsigned(a, b) <= 0 ? c : d;\n+    }\n+\n+    \/\/ Float comparison\n@@ -588,0 +959,2 @@\n+    \/\/ Signed comparison: I\/L\n+    \/\/     I fo I\n@@ -590,1 +963,1 @@\n-    private static void testCMoveIGTforI(int[] a, int[] b, int[] c, int[] d, int[] r, int[] r2) {\n+    private static void testCMoveIEQforI(int[] a, int[] b, int[] c, int[] d, int[] r, int[] r2) {\n@@ -595,1 +968,1 @@\n-            r[i] = (a[i] > b[i]) ? cc : dd;\n+            r[i] = (a[i] == b[i]) ? cc : dd;\n@@ -601,1 +974,1 @@\n-    private static void testCMoveIGTforL(int[] a, int[] b, long[] c, long[] d, long[] r, long[] r2) {\n+    private static void testCMoveINEforI(int[] a, int[] b, int[] c, int[] d, int[] r, int[] r2) {\n@@ -603,2 +976,2 @@\n-            long cc = c[i];\n-            long dd = d[i];\n+            int cc = c[i];\n+            int dd = d[i];\n@@ -606,1 +979,1 @@\n-            r[i] = (a[i] > b[i]) ? cc : dd;\n+            r[i] = (a[i] != b[i]) ? cc : dd;\n@@ -611,7 +984,2 @@\n-    @IR(counts = {IRNode.LOAD_VECTOR_I,     IRNode.VECTOR_SIZE + \"min(max_int, max_float)\", \">0\",\n-                  IRNode.LOAD_VECTOR_F,     IRNode.VECTOR_SIZE + \"min(max_int, max_float)\", \">0\",\n-                  IRNode.VECTOR_MASK_CMP_I, IRNode.VECTOR_SIZE + \"min(max_int, max_float)\", \">0\",\n-                  IRNode.VECTOR_BLEND_F,    IRNode.VECTOR_SIZE + \"min(max_int, max_float)\", \">0\",\n-                  IRNode.STORE_VECTOR, \">0\"},\n-        applyIfCPUFeatureOr = {\"avx\", \"true\", \"asimd\", \"true\"})\n-    private static void testCMoveIGTforF(int[] a, int[] b, float[] c, float[] d, float[] r, float[] r2) {\n+    @IR(failOn = {IRNode.STORE_VECTOR})\n+    private static void testCMoveIGTforI(int[] a, int[] b, int[] c, int[] d, int[] r, int[] r2) {\n@@ -619,2 +987,2 @@\n-            float cc = c[i];\n-            float dd = d[i];\n+            int cc = c[i];\n+            int dd = d[i];\n@@ -628,1 +996,1 @@\n-    private static void testCMoveIGTforD(int[] a, int[] b, double[] c, double[] d, double[] r, double[] r2) {\n+    private static void testCMoveIGEforI(int[] a, int[] b, int[] c, int[] d, int[] r, int[] r2) {\n@@ -630,2 +998,2 @@\n-            double cc = c[i];\n-            double dd = d[i];\n+            int cc = c[i];\n+            int dd = d[i];\n@@ -633,1 +1001,1 @@\n-            r[i] = (a[i] > b[i]) ? cc : dd;\n+            r[i] = (a[i] >= b[i]) ? cc : dd;\n@@ -639,1 +1007,1 @@\n-    private static void testCMoveLGTforI(long[] a, long[] b, int[] c, int[] d, int[] r, int[] r2) {\n+    private static void testCMoveILTforI(int[] a, int[] b, int[] c, int[] d, int[] r, int[] r2) {\n@@ -644,1 +1012,1 @@\n-            r[i] = (a[i] > b[i]) ? cc : dd;\n+            r[i] = (a[i] < b[i]) ? cc : dd;\n@@ -650,1 +1018,1 @@\n-    private static void testCMoveLGTforL(long[] a, long[] b, long[] c, long[] d, long[] r, long[] r2) {\n+    private static void testCMoveILEforI(int[] a, int[] b, int[] c, int[] d, int[] r, int[] r2) {\n@@ -652,2 +1020,2 @@\n-            long cc = c[i];\n-            long dd = d[i];\n+            int cc = c[i];\n+            int dd = d[i];\n@@ -655,1 +1023,1 @@\n-            r[i] = (a[i] > b[i]) ? cc : dd;\n+            r[i] = (a[i] <= b[i]) ? cc : dd;\n@@ -659,0 +1027,1 @@\n+    \/\/     I fo L\n@@ -661,1 +1030,1 @@\n-    private static void testCMoveLGTforF(long[] a, long[] b, float[] c, float[] d, float[] r, float[] r2) {\n+    private static void testCMoveIEQforL(int[] a, int[] b, long[] c, long[] d, long[] r, long[] r2) {\n@@ -663,2 +1032,2 @@\n-            float cc = c[i];\n-            float dd = d[i];\n+            long cc = c[i];\n+            long dd = d[i];\n@@ -666,1 +1035,1 @@\n-            r[i] = (a[i] > b[i]) ? cc : dd;\n+            r[i] = (a[i] == b[i]) ? cc : dd;\n@@ -671,8 +1040,2 @@\n-    @IR(counts = {IRNode.LOAD_VECTOR_L,     IRNode.VECTOR_SIZE + \"min(max_long, max_double)\", \">0\",\n-                  IRNode.LOAD_VECTOR_D,     IRNode.VECTOR_SIZE + \"min(max_long, max_double)\", \">0\",\n-                  IRNode.VECTOR_MASK_CMP_L, IRNode.VECTOR_SIZE + \"min(max_long, max_double)\", \">0\",\n-                  IRNode.VECTOR_BLEND_D,    IRNode.VECTOR_SIZE + \"min(max_long, max_double)\", \">0\",\n-                  IRNode.STORE_VECTOR, \">0\"},\n-        applyIfCPUFeatureOr = {\"avx2\", \"true\", \"asimd\", \"true\"})\n-    \/\/ Requires avx2, else L is restricted to 16 byte, and D has 32. That leads to a vector elements mismatch of 2 to 4.\n-    private static void testCMoveLGTforD(long[] a, long[] b, double[] c, double[] d, double[] r, double[] r2) {\n+    @IR(failOn = {IRNode.STORE_VECTOR})\n+    private static void testCMoveINEforL(int[] a, int[] b, long[] c, long[] d, long[] r, long[] r2) {\n@@ -680,2 +1043,2 @@\n-            double cc = c[i];\n-            double dd = d[i];\n+            long cc = c[i];\n+            long dd = d[i];\n@@ -683,1 +1046,1 @@\n-            r[i] = (a[i] > b[i]) ? cc : dd;\n+            r[i] = (a[i] != b[i]) ? cc : dd;\n@@ -689,1 +1052,1 @@\n-    private static void testCMoveFGTforI(float[] a, float[] b, int[] c, int[] d, int[] r, int[] r2) {\n+    private static void testCMoveIGTforL(int[] a, int[] b, long[] c, long[] d, long[] r, long[] r2) {\n@@ -691,2 +1054,2 @@\n-            int cc = c[i];\n-            int dd = d[i];\n+            long cc = c[i];\n+            long dd = d[i];\n@@ -700,1 +1063,1 @@\n-    private static void testCMoveFGTforL(float[] a, float[] b, long[] c, long[] d, long[] r, long[] r2) {\n+    private static void testCMoveIGEforL(int[] a, int[] b, long[] c, long[] d, long[] r, long[] r2) {\n@@ -705,1 +1068,1 @@\n-            r[i] = (a[i] > b[i]) ? cc : dd;\n+            r[i] = (a[i] >= b[i]) ? cc : dd;\n@@ -710,6 +1073,2 @@\n-    @IR(counts = {IRNode.LOAD_VECTOR_F, \">0\",\n-                  IRNode.VECTOR_MASK_CMP_F, \">0\",\n-                  IRNode.VECTOR_BLEND_F, \">0\",\n-                  IRNode.STORE_VECTOR, \">0\"},\n-        applyIfCPUFeatureOr = {\"avx\", \"true\", \"asimd\", \"true\"})\n-    private static void testCMoveFGTforF(float[] a, float[] b, float[] c, float[] d, float[] r, float[] r2) {\n+    @IR(failOn = {IRNode.STORE_VECTOR})\n+    private static void testCMoveILTforL(int[] a, int[] b, long[] c, long[] d, long[] r, long[] r2) {\n@@ -717,2 +1076,2 @@\n-            float cc = c[i];\n-            float dd = d[i];\n+            long cc = c[i];\n+            long dd = d[i];\n@@ -720,1 +1079,1 @@\n-            r[i] = (a[i] > b[i]) ? cc : dd;\n+            r[i] = (a[i] < b[i]) ? cc : dd;\n@@ -726,1 +1085,1 @@\n-    private static void testCMoveFGTforD(float[] a, float[] b, double[] c, double[] d, double[] r, double[] r2) {\n+    private static void testCMoveILEforL(int[] a, int[] b, long[] c, long[] d, long[] r, long[] r2) {\n@@ -728,2 +1087,133 @@\n-            double cc = c[i];\n-            double dd = d[i];\n+            long cc = c[i];\n+            long dd = d[i];\n+            r2[i] = cc + dd;\n+            r[i] = (a[i] <= b[i]) ? cc : dd;\n+        }\n+    }\n+\n+    \/\/     I fo F\n+    @Test\n+    @IR(counts = {IRNode.LOAD_VECTOR_I,     IRNode.VECTOR_SIZE + \"min(max_int, max_float)\", \">0\",\n+                  IRNode.LOAD_VECTOR_F,     IRNode.VECTOR_SIZE + \"min(max_int, max_float)\", \">0\",\n+                  IRNode.VECTOR_MASK_CMP_I, IRNode.VECTOR_SIZE + \"min(max_int, max_float)\", \">0\",\n+                  IRNode.VECTOR_BLEND_F,    IRNode.VECTOR_SIZE + \"min(max_int, max_float)\", \">0\",\n+                  IRNode.STORE_VECTOR, \">0\"},\n+        applyIfCPUFeatureOr = {\"avx\", \"true\", \"asimd\", \"true\"})\n+    private static void testCMoveIEQforF(int[] a, int[] b, float[] c, float[] d, float[] r, float[] r2) {\n+        for (int i = 0; i < a.length; i++) {\n+            float cc = c[i];\n+            float dd = d[i];\n+            r2[i] = cc + dd;\n+            r[i] = (a[i] == b[i]) ? cc : dd;\n+        }\n+    }\n+\n+    @Test\n+    @IR(counts = {IRNode.LOAD_VECTOR_I,     IRNode.VECTOR_SIZE + \"min(max_int, max_float)\", \">0\",\n+                  IRNode.LOAD_VECTOR_F,     IRNode.VECTOR_SIZE + \"min(max_int, max_float)\", \">0\",\n+                  IRNode.VECTOR_MASK_CMP_I, IRNode.VECTOR_SIZE + \"min(max_int, max_float)\", \">0\",\n+                  IRNode.VECTOR_BLEND_F,    IRNode.VECTOR_SIZE + \"min(max_int, max_float)\", \">0\",\n+                  IRNode.STORE_VECTOR, \">0\"},\n+        applyIfCPUFeatureOr = {\"avx\", \"true\", \"asimd\", \"true\"})\n+    private static void testCMoveINEforF(int[] a, int[] b, float[] c, float[] d, float[] r, float[] r2) {\n+        for (int i = 0; i < a.length; i++) {\n+            float cc = c[i];\n+            float dd = d[i];\n+            r2[i] = cc + dd;\n+            r[i] = (a[i] != b[i]) ? cc : dd;\n+        }\n+    }\n+\n+    @Test\n+    @IR(counts = {IRNode.LOAD_VECTOR_I,     IRNode.VECTOR_SIZE + \"min(max_int, max_float)\", \">0\",\n+                  IRNode.LOAD_VECTOR_F,     IRNode.VECTOR_SIZE + \"min(max_int, max_float)\", \">0\",\n+                  IRNode.VECTOR_MASK_CMP_I, IRNode.VECTOR_SIZE + \"min(max_int, max_float)\", \">0\",\n+                  IRNode.VECTOR_BLEND_F,    IRNode.VECTOR_SIZE + \"min(max_int, max_float)\", \">0\",\n+                  IRNode.STORE_VECTOR, \">0\"},\n+        applyIfCPUFeatureOr = {\"avx\", \"true\", \"asimd\", \"true\"})\n+    private static void testCMoveIGTforF(int[] a, int[] b, float[] c, float[] d, float[] r, float[] r2) {\n+        for (int i = 0; i < a.length; i++) {\n+            float cc = c[i];\n+            float dd = d[i];\n+            r2[i] = cc + dd;\n+            r[i] = (a[i] > b[i]) ? cc : dd;\n+        }\n+    }\n+\n+    @Test\n+    @IR(counts = {IRNode.LOAD_VECTOR_I,     IRNode.VECTOR_SIZE + \"min(max_int, max_float)\", \">0\",\n+                  IRNode.LOAD_VECTOR_F,     IRNode.VECTOR_SIZE + \"min(max_int, max_float)\", \">0\",\n+                  IRNode.VECTOR_MASK_CMP_I, IRNode.VECTOR_SIZE + \"min(max_int, max_float)\", \">0\",\n+                  IRNode.VECTOR_BLEND_F,    IRNode.VECTOR_SIZE + \"min(max_int, max_float)\", \">0\",\n+                  IRNode.STORE_VECTOR, \">0\"},\n+        applyIfCPUFeatureOr = {\"avx\", \"true\", \"asimd\", \"true\"})\n+    private static void testCMoveIGEforF(int[] a, int[] b, float[] c, float[] d, float[] r, float[] r2) {\n+        for (int i = 0; i < a.length; i++) {\n+            float cc = c[i];\n+            float dd = d[i];\n+            r2[i] = cc + dd;\n+            r[i] = (a[i] >= b[i]) ? cc : dd;\n+        }\n+    }\n+\n+    @Test\n+    @IR(counts = {IRNode.LOAD_VECTOR_I,     IRNode.VECTOR_SIZE + \"min(max_int, max_float)\", \">0\",\n+                  IRNode.LOAD_VECTOR_F,     IRNode.VECTOR_SIZE + \"min(max_int, max_float)\", \">0\",\n+                  IRNode.VECTOR_MASK_CMP_I, IRNode.VECTOR_SIZE + \"min(max_int, max_float)\", \">0\",\n+                  IRNode.VECTOR_BLEND_F,    IRNode.VECTOR_SIZE + \"min(max_int, max_float)\", \">0\",\n+                  IRNode.STORE_VECTOR, \">0\"},\n+        applyIfCPUFeatureOr = {\"avx\", \"true\", \"asimd\", \"true\"})\n+    private static void testCMoveILTforF(int[] a, int[] b, float[] c, float[] d, float[] r, float[] r2) {\n+        for (int i = 0; i < a.length; i++) {\n+            float cc = c[i];\n+            float dd = d[i];\n+            r2[i] = cc + dd;\n+            r[i] = (a[i] < b[i]) ? cc : dd;\n+        }\n+    }\n+\n+    @Test\n+    @IR(counts = {IRNode.LOAD_VECTOR_I,     IRNode.VECTOR_SIZE + \"min(max_int, max_float)\", \">0\",\n+                  IRNode.LOAD_VECTOR_F,     IRNode.VECTOR_SIZE + \"min(max_int, max_float)\", \">0\",\n+                  IRNode.VECTOR_MASK_CMP_I, IRNode.VECTOR_SIZE + \"min(max_int, max_float)\", \">0\",\n+                  IRNode.VECTOR_BLEND_F,    IRNode.VECTOR_SIZE + \"min(max_int, max_float)\", \">0\",\n+                  IRNode.STORE_VECTOR, \">0\"},\n+        applyIfCPUFeatureOr = {\"avx\", \"true\", \"asimd\", \"true\"})\n+    private static void testCMoveILEforF(int[] a, int[] b, float[] c, float[] d, float[] r, float[] r2) {\n+        for (int i = 0; i < a.length; i++) {\n+            float cc = c[i];\n+            float dd = d[i];\n+            r2[i] = cc + dd;\n+            r[i] = (a[i] <= b[i]) ? cc : dd;\n+        }\n+    }\n+\n+    \/\/     I fo D\n+    @Test\n+    @IR(failOn = {IRNode.STORE_VECTOR})\n+    private static void testCMoveIEQforD(int[] a, int[] b, double[] c, double[] d, double[] r, double[] r2) {\n+        for (int i = 0; i < a.length; i++) {\n+            double cc = c[i];\n+            double dd = d[i];\n+            r2[i] = cc + dd;\n+            r[i] = (a[i] == b[i]) ? cc : dd;\n+        }\n+    }\n+\n+    @Test\n+    @IR(failOn = {IRNode.STORE_VECTOR})\n+    private static void testCMoveINEforD(int[] a, int[] b, double[] c, double[] d, double[] r, double[] r2) {\n+        for (int i = 0; i < a.length; i++) {\n+            double cc = c[i];\n+            double dd = d[i];\n+            r2[i] = cc + dd;\n+            r[i] = (a[i] != b[i]) ? cc : dd;\n+        }\n+    }\n+\n+    @Test\n+    @IR(failOn = {IRNode.STORE_VECTOR})\n+    private static void testCMoveIGTforD(int[] a, int[] b, double[] c, double[] d, double[] r, double[] r2) {\n+        for (int i = 0; i < a.length; i++) {\n+            double cc = c[i];\n+            double dd = d[i];\n@@ -737,1 +1227,90 @@\n-    private static void testCMoveDGTforI(double[] a, double[] b, int[] c, int[] d, int[] r, int[] r2) {\n+    private static void testCMoveIGEforD(int[] a, int[] b, double[] c, double[] d, double[] r, double[] r2) {\n+        for (int i = 0; i < a.length; i++) {\n+            double cc = c[i];\n+            double dd = d[i];\n+            r2[i] = cc + dd;\n+            r[i] = (a[i] >= b[i]) ? cc : dd;\n+        }\n+    }\n+\n+    @Test\n+    @IR(failOn = {IRNode.STORE_VECTOR})\n+    private static void testCMoveILTforD(int[] a, int[] b, double[] c, double[] d, double[] r, double[] r2) {\n+        for (int i = 0; i < a.length; i++) {\n+            double cc = c[i];\n+            double dd = d[i];\n+            r2[i] = cc + dd;\n+            r[i] = (a[i] < b[i]) ? cc : dd;\n+        }\n+    }\n+\n+    @Test\n+    @IR(failOn = {IRNode.STORE_VECTOR})\n+    private static void testCMoveILEforD(int[] a, int[] b, double[] c, double[] d, double[] r, double[] r2) {\n+        for (int i = 0; i < a.length; i++) {\n+            double cc = c[i];\n+            double dd = d[i];\n+            r2[i] = cc + dd;\n+            r[i] = (a[i] <= b[i]) ? cc : dd;\n+        }\n+    }\n+\n+    \/\/     L fo I\n+    @Test\n+    @IR(failOn = {IRNode.STORE_VECTOR})\n+    private static void testCMoveLEQforI(long[] a, long[] b, int[] c, int[] d, int[] r, int[] r2) {\n+        for (int i = 0; i < a.length; i++) {\n+            int cc = c[i];\n+            int dd = d[i];\n+            r2[i] = cc + dd;\n+            r[i] = (a[i] == b[i]) ? cc : dd;\n+        }\n+    }\n+\n+    @Test\n+    @IR(failOn = {IRNode.STORE_VECTOR})\n+    private static void testCMoveLNEforI(long[] a, long[] b, int[] c, int[] d, int[] r, int[] r2) {\n+        for (int i = 0; i < a.length; i++) {\n+            int cc = c[i];\n+            int dd = d[i];\n+            r2[i] = cc + dd;\n+            r[i] = (a[i] != b[i]) ? cc : dd;\n+        }\n+    }\n+\n+    @Test\n+    @IR(failOn = {IRNode.STORE_VECTOR})\n+    private static void testCMoveLGTforI(long[] a, long[] b, int[] c, int[] d, int[] r, int[] r2) {\n+        for (int i = 0; i < a.length; i++) {\n+            int cc = c[i];\n+            int dd = d[i];\n+            r2[i] = cc + dd;\n+            r[i] = (a[i] > b[i]) ? cc : dd;\n+        }\n+    }\n+\n+    @Test\n+    @IR(failOn = {IRNode.STORE_VECTOR})\n+    private static void testCMoveLGEforI(long[] a, long[] b, int[] c, int[] d, int[] r, int[] r2) {\n+        for (int i = 0; i < a.length; i++) {\n+            int cc = c[i];\n+            int dd = d[i];\n+            r2[i] = cc + dd;\n+            r[i] = (a[i] >= b[i]) ? cc : dd;\n+        }\n+    }\n+\n+    @Test\n+    @IR(failOn = {IRNode.STORE_VECTOR})\n+    private static void testCMoveLLTforI(long[] a, long[] b, int[] c, int[] d, int[] r, int[] r2) {\n+        for (int i = 0; i < a.length; i++) {\n+            int cc = c[i];\n+            int dd = d[i];\n+            r2[i] = cc + dd;\n+            r[i] = (a[i] < b[i]) ? cc : dd;\n+        }\n+    }\n+\n+    @Test\n+    @IR(failOn = {IRNode.STORE_VECTOR})\n+    private static void testCMoveLLEforI(long[] a, long[] b, int[] c, int[] d, int[] r, int[] r2) {\n@@ -742,0 +1321,34 @@\n+            r[i] = (a[i] <= b[i]) ? cc : dd;\n+        }\n+    }\n+\n+    \/\/     L fo L\n+    @Test\n+    @IR(failOn = {IRNode.STORE_VECTOR})\n+    private static void testCMoveLEQforL(long[] a, long[] b, long[] c, long[] d, long[] r, long[] r2) {\n+        for (int i = 0; i < a.length; i++) {\n+            long cc = c[i];\n+            long dd = d[i];\n+            r2[i] = cc + dd;\n+            r[i] = (a[i] == b[i]) ? cc : dd;\n+        }\n+    }\n+\n+    @Test\n+    @IR(failOn = {IRNode.STORE_VECTOR})\n+    private static void testCMoveLNEforL(long[] a, long[] b, long[] c, long[] d, long[] r, long[] r2) {\n+        for (int i = 0; i < a.length; i++) {\n+            long cc = c[i];\n+            long dd = d[i];\n+            r2[i] = cc + dd;\n+            r[i] = (a[i] != b[i]) ? cc : dd;\n+        }\n+    }\n+\n+    @Test\n+    @IR(failOn = {IRNode.STORE_VECTOR})\n+    private static void testCMoveLGTforL(long[] a, long[] b, long[] c, long[] d, long[] r, long[] r2) {\n+        for (int i = 0; i < a.length; i++) {\n+            long cc = c[i];\n+            long dd = d[i];\n+            r2[i] = cc + dd;\n@@ -744,1 +1357,1572 @@\n-    }\n+    }\n+\n+    @Test\n+    @IR(failOn = {IRNode.STORE_VECTOR})\n+    private static void testCMoveLGEforL(long[] a, long[] b, long[] c, long[] d, long[] r, long[] r2) {\n+        for (int i = 0; i < a.length; i++) {\n+            long cc = c[i];\n+            long dd = d[i];\n+            r2[i] = cc + dd;\n+            r[i] = (a[i] >= b[i]) ? cc : dd;\n+        }\n+    }\n+\n+    @Test\n+    @IR(failOn = {IRNode.STORE_VECTOR})\n+    private static void testCMoveLLTforL(long[] a, long[] b, long[] c, long[] d, long[] r, long[] r2) {\n+        for (int i = 0; i < a.length; i++) {\n+            long cc = c[i];\n+            long dd = d[i];\n+            r2[i] = cc + dd;\n+            r[i] = (a[i] < b[i]) ? cc : dd;\n+        }\n+    }\n+\n+    @Test\n+    @IR(failOn = {IRNode.STORE_VECTOR})\n+    private static void testCMoveLLEforL(long[] a, long[] b, long[] c, long[] d, long[] r, long[] r2) {\n+        for (int i = 0; i < a.length; i++) {\n+            long cc = c[i];\n+            long dd = d[i];\n+            r2[i] = cc + dd;\n+            r[i] = (a[i] <= b[i]) ? cc : dd;\n+        }\n+    }\n+\n+    \/\/     L fo F\n+    @Test\n+    @IR(failOn = {IRNode.STORE_VECTOR})\n+    private static void testCMoveLEQforF(long[] a, long[] b, float[] c, float[] d, float[] r, float[] r2) {\n+        for (int i = 0; i < a.length; i++) {\n+            float cc = c[i];\n+            float dd = d[i];\n+            r2[i] = cc + dd;\n+            r[i] = (a[i] == b[i]) ? cc : dd;\n+        }\n+    }\n+\n+    @Test\n+    @IR(failOn = {IRNode.STORE_VECTOR})\n+    private static void testCMoveLNEforF(long[] a, long[] b, float[] c, float[] d, float[] r, float[] r2) {\n+        for (int i = 0; i < a.length; i++) {\n+            float cc = c[i];\n+            float dd = d[i];\n+            r2[i] = cc + dd;\n+            r[i] = (a[i] != b[i]) ? cc : dd;\n+        }\n+    }\n+\n+    @Test\n+    @IR(failOn = {IRNode.STORE_VECTOR})\n+    private static void testCMoveLGTforF(long[] a, long[] b, float[] c, float[] d, float[] r, float[] r2) {\n+        for (int i = 0; i < a.length; i++) {\n+            float cc = c[i];\n+            float dd = d[i];\n+            r2[i] = cc + dd;\n+            r[i] = (a[i] > b[i]) ? cc : dd;\n+        }\n+    }\n+\n+    @Test\n+    @IR(failOn = {IRNode.STORE_VECTOR})\n+    private static void testCMoveLGEforF(long[] a, long[] b, float[] c, float[] d, float[] r, float[] r2) {\n+        for (int i = 0; i < a.length; i++) {\n+            float cc = c[i];\n+            float dd = d[i];\n+            r2[i] = cc + dd;\n+            r[i] = (a[i] >= b[i]) ? cc : dd;\n+        }\n+    }\n+\n+    @Test\n+    @IR(failOn = {IRNode.STORE_VECTOR})\n+    private static void testCMoveLLTforF(long[] a, long[] b, float[] c, float[] d, float[] r, float[] r2) {\n+        for (int i = 0; i < a.length; i++) {\n+            float cc = c[i];\n+            float dd = d[i];\n+            r2[i] = cc + dd;\n+            r[i] = (a[i] < b[i]) ? cc : dd;\n+        }\n+    }\n+\n+    @Test\n+    @IR(failOn = {IRNode.STORE_VECTOR})\n+    private static void testCMoveLLEforF(long[] a, long[] b, float[] c, float[] d, float[] r, float[] r2) {\n+        for (int i = 0; i < a.length; i++) {\n+            float cc = c[i];\n+            float dd = d[i];\n+            r2[i] = cc + dd;\n+            r[i] = (a[i] <= b[i]) ? cc : dd;\n+        }\n+    }\n+\n+    \/\/     L fo D\n+    @Test\n+    @IR(counts = {IRNode.LOAD_VECTOR_L,     IRNode.VECTOR_SIZE + \"min(max_long, max_double)\", \">0\",\n+                  IRNode.LOAD_VECTOR_D,     IRNode.VECTOR_SIZE + \"min(max_long, max_double)\", \">0\",\n+                  IRNode.VECTOR_MASK_CMP_L, IRNode.VECTOR_SIZE + \"min(max_long, max_double)\", \">0\",\n+                  IRNode.VECTOR_BLEND_D,    IRNode.VECTOR_SIZE + \"min(max_long, max_double)\", \">0\",\n+                  IRNode.STORE_VECTOR, \">0\"},\n+        applyIfCPUFeatureOr = {\"avx2\", \"true\", \"asimd\", \"true\"})\n+    \/\/ Requires avx2, else L is restricted to 16 byte, and D has 32. That leads to a vector elements mismatch of 2 to 4.\n+    private static void testCMoveLEQforD(long[] a, long[] b, double[] c, double[] d, double[] r, double[] r2) {\n+        for (int i = 0; i < a.length; i++) {\n+            double cc = c[i];\n+            double dd = d[i];\n+            r2[i] = cc + dd;\n+            r[i] = (a[i] == b[i]) ? cc : dd;\n+        }\n+    }\n+\n+    @Test\n+    @IR(counts = {IRNode.LOAD_VECTOR_L,     IRNode.VECTOR_SIZE + \"min(max_long, max_double)\", \">0\",\n+                  IRNode.LOAD_VECTOR_D,     IRNode.VECTOR_SIZE + \"min(max_long, max_double)\", \">0\",\n+                  IRNode.VECTOR_MASK_CMP_L, IRNode.VECTOR_SIZE + \"min(max_long, max_double)\", \">0\",\n+                  IRNode.VECTOR_BLEND_D,    IRNode.VECTOR_SIZE + \"min(max_long, max_double)\", \">0\",\n+                  IRNode.STORE_VECTOR, \">0\"},\n+        applyIfCPUFeatureOr = {\"avx2\", \"true\", \"asimd\", \"true\"})\n+    \/\/ Requires avx2, else L is restricted to 16 byte, and D has 32. That leads to a vector elements mismatch of 2 to 4.\n+    private static void testCMoveLNEforD(long[] a, long[] b, double[] c, double[] d, double[] r, double[] r2) {\n+        for (int i = 0; i < a.length; i++) {\n+            double cc = c[i];\n+            double dd = d[i];\n+            r2[i] = cc + dd;\n+            r[i] = (a[i] != b[i]) ? cc : dd;\n+        }\n+    }\n+\n+    @Test\n+    @IR(counts = {IRNode.LOAD_VECTOR_L,     IRNode.VECTOR_SIZE + \"min(max_long, max_double)\", \">0\",\n+                  IRNode.LOAD_VECTOR_D,     IRNode.VECTOR_SIZE + \"min(max_long, max_double)\", \">0\",\n+                  IRNode.VECTOR_MASK_CMP_L, IRNode.VECTOR_SIZE + \"min(max_long, max_double)\", \">0\",\n+                  IRNode.VECTOR_BLEND_D,    IRNode.VECTOR_SIZE + \"min(max_long, max_double)\", \">0\",\n+                  IRNode.STORE_VECTOR, \">0\"},\n+        applyIfCPUFeatureOr = {\"avx2\", \"true\", \"asimd\", \"true\"})\n+    \/\/ Requires avx2, else L is restricted to 16 byte, and D has 32. That leads to a vector elements mismatch of 2 to 4.\n+    private static void testCMoveLGTforD(long[] a, long[] b, double[] c, double[] d, double[] r, double[] r2) {\n+        for (int i = 0; i < a.length; i++) {\n+            double cc = c[i];\n+            double dd = d[i];\n+            r2[i] = cc + dd;\n+            r[i] = (a[i] > b[i]) ? cc : dd;\n+        }\n+    }\n+\n+    @Test\n+    @IR(counts = {IRNode.LOAD_VECTOR_L,     IRNode.VECTOR_SIZE + \"min(max_long, max_double)\", \">0\",\n+                  IRNode.LOAD_VECTOR_D,     IRNode.VECTOR_SIZE + \"min(max_long, max_double)\", \">0\",\n+                  IRNode.VECTOR_MASK_CMP_L, IRNode.VECTOR_SIZE + \"min(max_long, max_double)\", \">0\",\n+                  IRNode.VECTOR_BLEND_D,    IRNode.VECTOR_SIZE + \"min(max_long, max_double)\", \">0\",\n+                  IRNode.STORE_VECTOR, \">0\"},\n+        applyIfCPUFeatureOr = {\"avx2\", \"true\", \"asimd\", \"true\"})\n+    \/\/ Requires avx2, else L is restricted to 16 byte, and D has 32. That leads to a vector elements mismatch of 2 to 4.\n+    private static void testCMoveLGEforD(long[] a, long[] b, double[] c, double[] d, double[] r, double[] r2) {\n+        for (int i = 0; i < a.length; i++) {\n+            double cc = c[i];\n+            double dd = d[i];\n+            r2[i] = cc + dd;\n+            r[i] = (a[i] >= b[i]) ? cc : dd;\n+        }\n+    }\n+\n+    @Test\n+    @IR(counts = {IRNode.LOAD_VECTOR_L,     IRNode.VECTOR_SIZE + \"min(max_long, max_double)\", \">0\",\n+                  IRNode.LOAD_VECTOR_D,     IRNode.VECTOR_SIZE + \"min(max_long, max_double)\", \">0\",\n+                  IRNode.VECTOR_MASK_CMP_L, IRNode.VECTOR_SIZE + \"min(max_long, max_double)\", \">0\",\n+                  IRNode.VECTOR_BLEND_D,    IRNode.VECTOR_SIZE + \"min(max_long, max_double)\", \">0\",\n+                  IRNode.STORE_VECTOR, \">0\"},\n+        applyIfCPUFeatureOr = {\"avx2\", \"true\", \"asimd\", \"true\"})\n+    \/\/ Requires avx2, else L is restricted to 16 byte, and D has 32. That leads to a vector elements mismatch of 2 to 4.\n+    private static void testCMoveLLTforD(long[] a, long[] b, double[] c, double[] d, double[] r, double[] r2) {\n+        for (int i = 0; i < a.length; i++) {\n+            double cc = c[i];\n+            double dd = d[i];\n+            r2[i] = cc + dd;\n+            r[i] = (a[i] < b[i]) ? cc : dd;\n+        }\n+    }\n+\n+    @Test\n+    @IR(counts = {IRNode.LOAD_VECTOR_L,     IRNode.VECTOR_SIZE + \"min(max_long, max_double)\", \">0\",\n+                  IRNode.LOAD_VECTOR_D,     IRNode.VECTOR_SIZE + \"min(max_long, max_double)\", \">0\",\n+                  IRNode.VECTOR_MASK_CMP_L, IRNode.VECTOR_SIZE + \"min(max_long, max_double)\", \">0\",\n+                  IRNode.VECTOR_BLEND_D,    IRNode.VECTOR_SIZE + \"min(max_long, max_double)\", \">0\",\n+                  IRNode.STORE_VECTOR, \">0\"},\n+        applyIfCPUFeatureOr = {\"avx2\", \"true\", \"asimd\", \"true\"})\n+    \/\/ Requires avx2, else L is restricted to 16 byte, and D has 32. That leads to a vector elements mismatch of 2 to 4.\n+    private static void testCMoveLLEforD(long[] a, long[] b, double[] c, double[] d, double[] r, double[] r2) {\n+        for (int i = 0; i < a.length; i++) {\n+            double cc = c[i];\n+            double dd = d[i];\n+            r2[i] = cc + dd;\n+            r[i] = (a[i] <= b[i]) ? cc : dd;\n+        }\n+    }\n+\n+    \/\/ Unsigned comparison: I\/L\n+    \/\/     I fo I\n+    @Test\n+    @IR(failOn = {IRNode.STORE_VECTOR})\n+    private static void testCMoveUIEQforI(int[] a, int[] b, int[] c, int[] d, int[] r, int[] r2) {\n+        for (int i = 0; i < a.length; i++) {\n+            int cc = c[i];\n+            int dd = d[i];\n+            r2[i] = cc + dd;\n+            r[i] = Integer.compareUnsigned(a[i], b[i]) == 0 ? cc : dd;\n+        }\n+    }\n+\n+    @Test\n+    @IR(failOn = {IRNode.STORE_VECTOR})\n+    private static void testCMoveUINEforI(int[] a, int[] b, int[] c, int[] d, int[] r, int[] r2) {\n+        for (int i = 0; i < a.length; i++) {\n+            int cc = c[i];\n+            int dd = d[i];\n+            r2[i] = cc + dd;\n+            r[i] = Integer.compareUnsigned(a[i], b[i]) != 0 ? cc : dd;\n+        }\n+    }\n+\n+    @Test\n+    @IR(failOn = {IRNode.STORE_VECTOR})\n+    private static void testCMoveUIGTforI(int[] a, int[] b, int[] c, int[] d, int[] r, int[] r2) {\n+        for (int i = 0; i < a.length; i++) {\n+            int cc = c[i];\n+            int dd = d[i];\n+            r2[i] = cc + dd;\n+            r[i] = Integer.compareUnsigned(a[i], b[i]) > 0 ? cc : dd;\n+        }\n+    }\n+\n+    @Test\n+    @IR(failOn = {IRNode.STORE_VECTOR})\n+    private static void testCMoveUIGEforI(int[] a, int[] b, int[] c, int[] d, int[] r, int[] r2) {\n+        for (int i = 0; i < a.length; i++) {\n+            int cc = c[i];\n+            int dd = d[i];\n+            r2[i] = cc + dd;\n+            r[i] = Integer.compareUnsigned(a[i], b[i]) >= 0 ? cc : dd;\n+        }\n+    }\n+\n+    @Test\n+    @IR(failOn = {IRNode.STORE_VECTOR})\n+    private static void testCMoveUILTforI(int[] a, int[] b, int[] c, int[] d, int[] r, int[] r2) {\n+        for (int i = 0; i < a.length; i++) {\n+            int cc = c[i];\n+            int dd = d[i];\n+            r2[i] = cc + dd;\n+            r[i] = Integer.compareUnsigned(a[i], b[i]) < 0 ? cc : dd;\n+        }\n+    }\n+\n+    @Test\n+    @IR(failOn = {IRNode.STORE_VECTOR})\n+    private static void testCMoveUILEforI(int[] a, int[] b, int[] c, int[] d, int[] r, int[] r2) {\n+        for (int i = 0; i < a.length; i++) {\n+            int cc = c[i];\n+            int dd = d[i];\n+            r2[i] = cc + dd;\n+            r[i] = Integer.compareUnsigned(a[i], b[i]) <= 0 ? cc : dd;\n+        }\n+    }\n+\n+    \/\/     I fo L\n+    @Test\n+    @IR(failOn = {IRNode.STORE_VECTOR})\n+    private static void testCMoveUIEQforL(int[] a, int[] b, long[] c, long[] d, long[] r, long[] r2) {\n+        for (int i = 0; i < a.length; i++) {\n+            long cc = c[i];\n+            long dd = d[i];\n+            r2[i] = cc + dd;\n+            r[i] = Integer.compareUnsigned(a[i], b[i]) == 0 ? cc : dd;\n+        }\n+    }\n+\n+    @Test\n+    @IR(failOn = {IRNode.STORE_VECTOR})\n+    private static void testCMoveUINEforL(int[] a, int[] b, long[] c, long[] d, long[] r, long[] r2) {\n+        for (int i = 0; i < a.length; i++) {\n+            long cc = c[i];\n+            long dd = d[i];\n+            r2[i] = cc + dd;\n+            r[i] = Integer.compareUnsigned(a[i], b[i]) != 0 ? cc : dd;\n+        }\n+    }\n+\n+    @Test\n+    @IR(failOn = {IRNode.STORE_VECTOR})\n+    private static void testCMoveUIGTforL(int[] a, int[] b, long[] c, long[] d, long[] r, long[] r2) {\n+        for (int i = 0; i < a.length; i++) {\n+            long cc = c[i];\n+            long dd = d[i];\n+            r2[i] = cc + dd;\n+            r[i] = Integer.compareUnsigned(a[i], b[i]) > 0 ? cc : dd;\n+        }\n+    }\n+\n+    @Test\n+    @IR(failOn = {IRNode.STORE_VECTOR})\n+    private static void testCMoveUIGEforL(int[] a, int[] b, long[] c, long[] d, long[] r, long[] r2) {\n+        for (int i = 0; i < a.length; i++) {\n+            long cc = c[i];\n+            long dd = d[i];\n+            r2[i] = cc + dd;\n+            r[i] = Integer.compareUnsigned(a[i], b[i]) >= 0 ? cc : dd;\n+        }\n+    }\n+\n+    @Test\n+    @IR(failOn = {IRNode.STORE_VECTOR})\n+    private static void testCMoveUILTforL(int[] a, int[] b, long[] c, long[] d, long[] r, long[] r2) {\n+        for (int i = 0; i < a.length; i++) {\n+            long cc = c[i];\n+            long dd = d[i];\n+            r2[i] = cc + dd;\n+            r[i] = Integer.compareUnsigned(a[i], b[i]) < 0 ? cc : dd;\n+        }\n+    }\n+\n+    @Test\n+    @IR(failOn = {IRNode.STORE_VECTOR})\n+    private static void testCMoveUILEforL(int[] a, int[] b, long[] c, long[] d, long[] r, long[] r2) {\n+        for (int i = 0; i < a.length; i++) {\n+            long cc = c[i];\n+            long dd = d[i];\n+            r2[i] = cc + dd;\n+            r[i] = Integer.compareUnsigned(a[i], b[i]) <= 0 ? cc : dd;\n+        }\n+    }\n+\n+    \/\/     I fo F\n+    @Test\n+    @IR(counts = {IRNode.LOAD_VECTOR_I,     IRNode.VECTOR_SIZE + \"min(max_int, max_float)\", \">0\",\n+                  IRNode.LOAD_VECTOR_F,     IRNode.VECTOR_SIZE + \"min(max_int, max_float)\", \">0\",\n+                  IRNode.VECTOR_MASK_CMP_I, IRNode.VECTOR_SIZE + \"min(max_int, max_float)\", \">0\",\n+                  IRNode.VECTOR_BLEND_F,    IRNode.VECTOR_SIZE + \"min(max_int, max_float)\", \">0\",\n+                  IRNode.STORE_VECTOR, \">0\"},\n+        applyIfCPUFeatureOr = {\"avx\", \"true\", \"asimd\", \"true\"})\n+    private static void testCMoveUIEQforF(int[] a, int[] b, float[] c, float[] d, float[] r, float[] r2) {\n+        for (int i = 0; i < a.length; i++) {\n+            float cc = c[i];\n+            float dd = d[i];\n+            r2[i] = cc + dd;\n+            r[i] = Integer.compareUnsigned(a[i], b[i]) == 0 ? cc : dd;\n+        }\n+    }\n+\n+    @Test\n+    @IR(counts = {IRNode.LOAD_VECTOR_I,     IRNode.VECTOR_SIZE + \"min(max_int, max_float)\", \">0\",\n+                  IRNode.LOAD_VECTOR_F,     IRNode.VECTOR_SIZE + \"min(max_int, max_float)\", \">0\",\n+                  IRNode.VECTOR_MASK_CMP_I, IRNode.VECTOR_SIZE + \"min(max_int, max_float)\", \">0\",\n+                  IRNode.VECTOR_BLEND_F,    IRNode.VECTOR_SIZE + \"min(max_int, max_float)\", \">0\",\n+                  IRNode.STORE_VECTOR, \">0\"},\n+        applyIfCPUFeatureOr = {\"avx\", \"true\", \"asimd\", \"true\"})\n+    private static void testCMoveUINEforF(int[] a, int[] b, float[] c, float[] d, float[] r, float[] r2) {\n+        for (int i = 0; i < a.length; i++) {\n+            float cc = c[i];\n+            float dd = d[i];\n+            r2[i] = cc + dd;\n+            r[i] = Integer.compareUnsigned(a[i], b[i]) != 0 ? cc : dd;\n+        }\n+    }\n+\n+    @Test\n+    @IR(counts = {IRNode.LOAD_VECTOR_I,     IRNode.VECTOR_SIZE + \"min(max_int, max_float)\", \">0\",\n+                  IRNode.LOAD_VECTOR_F,     IRNode.VECTOR_SIZE + \"min(max_int, max_float)\", \">0\",\n+                  IRNode.VECTOR_MASK_CMP_I, IRNode.VECTOR_SIZE + \"min(max_int, max_float)\", \">0\",\n+                  IRNode.VECTOR_BLEND_F,    IRNode.VECTOR_SIZE + \"min(max_int, max_float)\", \">0\",\n+                  IRNode.STORE_VECTOR, \">0\"},\n+        applyIfCPUFeatureOr = {\"avx\", \"true\", \"asimd\", \"true\"})\n+    private static void testCMoveUIGTforF(int[] a, int[] b, float[] c, float[] d, float[] r, float[] r2) {\n+        for (int i = 0; i < a.length; i++) {\n+            float cc = c[i];\n+            float dd = d[i];\n+            r2[i] = cc + dd;\n+            r[i] = Integer.compareUnsigned(a[i], b[i]) > 0 ? cc : dd;\n+        }\n+    }\n+\n+    @Test\n+    @IR(counts = {IRNode.LOAD_VECTOR_I,     IRNode.VECTOR_SIZE + \"min(max_int, max_float)\", \">0\",\n+                  IRNode.LOAD_VECTOR_F,     IRNode.VECTOR_SIZE + \"min(max_int, max_float)\", \">0\",\n+                  IRNode.VECTOR_MASK_CMP_I, IRNode.VECTOR_SIZE + \"min(max_int, max_float)\", \">0\",\n+                  IRNode.VECTOR_BLEND_F,    IRNode.VECTOR_SIZE + \"min(max_int, max_float)\", \">0\",\n+                  IRNode.STORE_VECTOR, \">0\"},\n+        applyIfCPUFeatureOr = {\"avx\", \"true\", \"asimd\", \"true\"})\n+    private static void testCMoveUIGEforF(int[] a, int[] b, float[] c, float[] d, float[] r, float[] r2) {\n+        for (int i = 0; i < a.length; i++) {\n+            float cc = c[i];\n+            float dd = d[i];\n+            r2[i] = cc + dd;\n+            r[i] = Integer.compareUnsigned(a[i], b[i]) >= 0 ? cc : dd;\n+        }\n+    }\n+\n+    @Test\n+    @IR(counts = {IRNode.LOAD_VECTOR_I,     IRNode.VECTOR_SIZE + \"min(max_int, max_float)\", \">0\",\n+                  IRNode.LOAD_VECTOR_F,     IRNode.VECTOR_SIZE + \"min(max_int, max_float)\", \">0\",\n+                  IRNode.VECTOR_MASK_CMP_I, IRNode.VECTOR_SIZE + \"min(max_int, max_float)\", \">0\",\n+                  IRNode.VECTOR_BLEND_F,    IRNode.VECTOR_SIZE + \"min(max_int, max_float)\", \">0\",\n+                  IRNode.STORE_VECTOR, \">0\"},\n+        applyIfCPUFeatureOr = {\"avx\", \"true\", \"asimd\", \"true\"})\n+    private static void testCMoveUILTforF(int[] a, int[] b, float[] c, float[] d, float[] r, float[] r2) {\n+        for (int i = 0; i < a.length; i++) {\n+            float cc = c[i];\n+            float dd = d[i];\n+            r2[i] = cc + dd;\n+            r[i] = Integer.compareUnsigned(a[i], b[i]) < 0 ? cc : dd;\n+        }\n+    }\n+\n+    @Test\n+    @IR(counts = {IRNode.LOAD_VECTOR_I,     IRNode.VECTOR_SIZE + \"min(max_int, max_float)\", \">0\",\n+                  IRNode.LOAD_VECTOR_F,     IRNode.VECTOR_SIZE + \"min(max_int, max_float)\", \">0\",\n+                  IRNode.VECTOR_MASK_CMP_I, IRNode.VECTOR_SIZE + \"min(max_int, max_float)\", \">0\",\n+                  IRNode.VECTOR_BLEND_F,    IRNode.VECTOR_SIZE + \"min(max_int, max_float)\", \">0\",\n+                  IRNode.STORE_VECTOR, \">0\"},\n+        applyIfCPUFeatureOr = {\"avx\", \"true\", \"asimd\", \"true\"})\n+    private static void testCMoveUILEforF(int[] a, int[] b, float[] c, float[] d, float[] r, float[] r2) {\n+        for (int i = 0; i < a.length; i++) {\n+            float cc = c[i];\n+            float dd = d[i];\n+            r2[i] = cc + dd;\n+            r[i] = Integer.compareUnsigned(a[i], b[i]) <= 0 ? cc : dd;\n+        }\n+    }\n+\n+    \/\/     I fo D\n+    @Test\n+    @IR(failOn = {IRNode.STORE_VECTOR})\n+    private static void testCMoveUIEQforD(int[] a, int[] b, double[] c, double[] d, double[] r, double[] r2) {\n+        for (int i = 0; i < a.length; i++) {\n+            double cc = c[i];\n+            double dd = d[i];\n+            r2[i] = cc + dd;\n+            r[i] = Integer.compareUnsigned(a[i], b[i]) == 0 ? cc : dd;\n+        }\n+    }\n+\n+    @Test\n+    @IR(failOn = {IRNode.STORE_VECTOR})\n+    private static void testCMoveUINEforD(int[] a, int[] b, double[] c, double[] d, double[] r, double[] r2) {\n+        for (int i = 0; i < a.length; i++) {\n+            double cc = c[i];\n+            double dd = d[i];\n+            r2[i] = cc + dd;\n+            r[i] = Integer.compareUnsigned(a[i], b[i]) != 0 ? cc : dd;\n+        }\n+    }\n+\n+    @Test\n+    @IR(failOn = {IRNode.STORE_VECTOR})\n+    private static void testCMoveUIGTforD(int[] a, int[] b, double[] c, double[] d, double[] r, double[] r2) {\n+        for (int i = 0; i < a.length; i++) {\n+            double cc = c[i];\n+            double dd = d[i];\n+            r2[i] = cc + dd;\n+            r[i] = Integer.compareUnsigned(a[i], b[i]) > 0 ? cc : dd;\n+        }\n+    }\n+\n+    @Test\n+    @IR(failOn = {IRNode.STORE_VECTOR})\n+    private static void testCMoveUIGEforD(int[] a, int[] b, double[] c, double[] d, double[] r, double[] r2) {\n+        for (int i = 0; i < a.length; i++) {\n+            double cc = c[i];\n+            double dd = d[i];\n+            r2[i] = cc + dd;\n+            r[i] = Integer.compareUnsigned(a[i], b[i]) >= 0 ? cc : dd;\n+        }\n+    }\n+\n+    @Test\n+    @IR(failOn = {IRNode.STORE_VECTOR})\n+    private static void testCMoveUILTforD(int[] a, int[] b, double[] c, double[] d, double[] r, double[] r2) {\n+        for (int i = 0; i < a.length; i++) {\n+            double cc = c[i];\n+            double dd = d[i];\n+            r2[i] = cc + dd;\n+            r[i] = Integer.compareUnsigned(a[i], b[i]) < 0 ? cc : dd;\n+        }\n+    }\n+\n+    @Test\n+    @IR(failOn = {IRNode.STORE_VECTOR})\n+    private static void testCMoveUILEforD(int[] a, int[] b, double[] c, double[] d, double[] r, double[] r2) {\n+        for (int i = 0; i < a.length; i++) {\n+            double cc = c[i];\n+            double dd = d[i];\n+            r2[i] = cc + dd;\n+            r[i] = Integer.compareUnsigned(a[i], b[i]) <= 0 ? cc : dd;\n+        }\n+    }\n+\n+    \/\/     L fo I\n+    @Test\n+    @IR(failOn = {IRNode.STORE_VECTOR})\n+    private static void testCMoveULEQforI(long[] a, long[] b, int[] c, int[] d, int[] r, int[] r2) {\n+        for (int i = 0; i < a.length; i++) {\n+            int cc = c[i];\n+            int dd = d[i];\n+            r2[i] = cc + dd;\n+            r[i] = Long.compareUnsigned(a[i], b[i]) == 0 ? cc : dd;\n+        }\n+    }\n+\n+    @Test\n+    @IR(failOn = {IRNode.STORE_VECTOR})\n+    private static void testCMoveULNEforI(long[] a, long[] b, int[] c, int[] d, int[] r, int[] r2) {\n+        for (int i = 0; i < a.length; i++) {\n+            int cc = c[i];\n+            int dd = d[i];\n+            r2[i] = cc + dd;\n+            r[i] = Long.compareUnsigned(a[i], b[i]) != 0 ? cc : dd;\n+        }\n+    }\n+\n+    @Test\n+    @IR(failOn = {IRNode.STORE_VECTOR})\n+    private static void testCMoveULGTforI(long[] a, long[] b, int[] c, int[] d, int[] r, int[] r2) {\n+        for (int i = 0; i < a.length; i++) {\n+            int cc = c[i];\n+            int dd = d[i];\n+            r2[i] = cc + dd;\n+            r[i] = Long.compareUnsigned(a[i], b[i]) > 0 ? cc : dd;\n+        }\n+    }\n+\n+    @Test\n+    @IR(failOn = {IRNode.STORE_VECTOR})\n+    private static void testCMoveULGEforI(long[] a, long[] b, int[] c, int[] d, int[] r, int[] r2) {\n+        for (int i = 0; i < a.length; i++) {\n+            int cc = c[i];\n+            int dd = d[i];\n+            r2[i] = cc + dd;\n+            r[i] = Long.compareUnsigned(a[i], b[i]) >= 0 ? cc : dd;\n+        }\n+    }\n+\n+    @Test\n+    @IR(failOn = {IRNode.STORE_VECTOR})\n+    private static void testCMoveULLTforI(long[] a, long[] b, int[] c, int[] d, int[] r, int[] r2) {\n+        for (int i = 0; i < a.length; i++) {\n+            int cc = c[i];\n+            int dd = d[i];\n+            r2[i] = cc + dd;\n+            r[i] = Long.compareUnsigned(a[i], b[i]) < 0 ? cc : dd;\n+        }\n+    }\n+\n+    @Test\n+    @IR(failOn = {IRNode.STORE_VECTOR})\n+    private static void testCMoveULLEforI(long[] a, long[] b, int[] c, int[] d, int[] r, int[] r2) {\n+        for (int i = 0; i < a.length; i++) {\n+            int cc = c[i];\n+            int dd = d[i];\n+            r2[i] = cc + dd;\n+            r[i] = Long.compareUnsigned(a[i], b[i]) <= 0 ? cc : dd;\n+        }\n+    }\n+\n+    \/\/     L fo L\n+    @Test\n+    @IR(failOn = {IRNode.STORE_VECTOR})\n+    private static void testCMoveULEQforL(long[] a, long[] b, long[] c, long[] d, long[] r, long[] r2) {\n+        for (int i = 0; i < a.length; i++) {\n+            long cc = c[i];\n+            long dd = d[i];\n+            r2[i] = cc + dd;\n+            r[i] = Long.compareUnsigned(a[i], b[i]) == 0 ? cc : dd;\n+        }\n+    }\n+\n+    @Test\n+    @IR(failOn = {IRNode.STORE_VECTOR})\n+    private static void testCMoveULNEforL(long[] a, long[] b, long[] c, long[] d, long[] r, long[] r2) {\n+        for (int i = 0; i < a.length; i++) {\n+            long cc = c[i];\n+            long dd = d[i];\n+            r2[i] = cc + dd;\n+            r[i] = Long.compareUnsigned(a[i], b[i]) != 0 ? cc : dd;\n+        }\n+    }\n+\n+    @Test\n+    @IR(failOn = {IRNode.STORE_VECTOR})\n+    private static void testCMoveULGTforL(long[] a, long[] b, long[] c, long[] d, long[] r, long[] r2) {\n+        for (int i = 0; i < a.length; i++) {\n+            long cc = c[i];\n+            long dd = d[i];\n+            r2[i] = cc + dd;\n+            r[i] = Long.compareUnsigned(a[i], b[i]) > 0 ? cc : dd;\n+        }\n+    }\n+\n+    @Test\n+    @IR(failOn = {IRNode.STORE_VECTOR})\n+    private static void testCMoveULGEforL(long[] a, long[] b, long[] c, long[] d, long[] r, long[] r2) {\n+        for (int i = 0; i < a.length; i++) {\n+            long cc = c[i];\n+            long dd = d[i];\n+            r2[i] = cc + dd;\n+            r[i] = Long.compareUnsigned(a[i], b[i]) >= 0 ? cc : dd;\n+        }\n+    }\n+\n+    @Test\n+    @IR(failOn = {IRNode.STORE_VECTOR})\n+    private static void testCMoveULLTforL(long[] a, long[] b, long[] c, long[] d, long[] r, long[] r2) {\n+        for (int i = 0; i < a.length; i++) {\n+            long cc = c[i];\n+            long dd = d[i];\n+            r2[i] = cc + dd;\n+            r[i] = Long.compareUnsigned(a[i], b[i]) < 0 ? cc : dd;\n+        }\n+    }\n+\n+    @Test\n+    @IR(failOn = {IRNode.STORE_VECTOR})\n+    private static void testCMoveULLEforL(long[] a, long[] b, long[] c, long[] d, long[] r, long[] r2) {\n+        for (int i = 0; i < a.length; i++) {\n+            long cc = c[i];\n+            long dd = d[i];\n+            r2[i] = cc + dd;\n+            r[i] = Long.compareUnsigned(a[i], b[i]) <= 0 ? cc : dd;\n+        }\n+    }\n+\n+    \/\/     L fo F\n+    @Test\n+    @IR(failOn = {IRNode.STORE_VECTOR})\n+    private static void testCMoveULEQforF(long[] a, long[] b, float[] c, float[] d, float[] r, float[] r2) {\n+        for (int i = 0; i < a.length; i++) {\n+            float cc = c[i];\n+            float dd = d[i];\n+            r2[i] = cc + dd;\n+            r[i] = Long.compareUnsigned(a[i], b[i]) == 0 ? cc : dd;\n+        }\n+    }\n+\n+    @Test\n+    @IR(failOn = {IRNode.STORE_VECTOR})\n+    private static void testCMoveULNEforF(long[] a, long[] b, float[] c, float[] d, float[] r, float[] r2) {\n+        for (int i = 0; i < a.length; i++) {\n+            float cc = c[i];\n+            float dd = d[i];\n+            r2[i] = cc + dd;\n+            r[i] = Long.compareUnsigned(a[i], b[i]) != 0 ? cc : dd;\n+        }\n+    }\n+\n+    @Test\n+    @IR(failOn = {IRNode.STORE_VECTOR})\n+    private static void testCMoveULGTforF(long[] a, long[] b, float[] c, float[] d, float[] r, float[] r2) {\n+        for (int i = 0; i < a.length; i++) {\n+            float cc = c[i];\n+            float dd = d[i];\n+            r2[i] = cc + dd;\n+            r[i] = Long.compareUnsigned(a[i], b[i]) > 0 ? cc : dd;\n+        }\n+    }\n+\n+    @Test\n+    @IR(failOn = {IRNode.STORE_VECTOR})\n+    private static void testCMoveULGEforF(long[] a, long[] b, float[] c, float[] d, float[] r, float[] r2) {\n+        for (int i = 0; i < a.length; i++) {\n+            float cc = c[i];\n+            float dd = d[i];\n+            r2[i] = cc + dd;\n+            r[i] = Long.compareUnsigned(a[i], b[i]) >= 0 ? cc : dd;\n+        }\n+    }\n+\n+    @Test\n+    @IR(failOn = {IRNode.STORE_VECTOR})\n+    private static void testCMoveULLTforF(long[] a, long[] b, float[] c, float[] d, float[] r, float[] r2) {\n+        for (int i = 0; i < a.length; i++) {\n+            float cc = c[i];\n+            float dd = d[i];\n+            r2[i] = cc + dd;\n+            r[i] = Long.compareUnsigned(a[i], b[i]) < 0 ? cc : dd;\n+        }\n+    }\n+\n+    @Test\n+    @IR(failOn = {IRNode.STORE_VECTOR})\n+    private static void testCMoveULLEforF(long[] a, long[] b, float[] c, float[] d, float[] r, float[] r2) {\n+        for (int i = 0; i < a.length; i++) {\n+            float cc = c[i];\n+            float dd = d[i];\n+            r2[i] = cc + dd;\n+            r[i] = Long.compareUnsigned(a[i], b[i]) <= 0 ? cc : dd;\n+        }\n+    }\n+\n+    \/\/     L fo D\n+    @Test\n+    @IR(counts = {IRNode.LOAD_VECTOR_L,     IRNode.VECTOR_SIZE + \"min(max_long, max_double)\", \">0\",\n+                  IRNode.LOAD_VECTOR_D,     IRNode.VECTOR_SIZE + \"min(max_long, max_double)\", \">0\",\n+                  IRNode.VECTOR_MASK_CMP_L, IRNode.VECTOR_SIZE + \"min(max_long, max_double)\", \">0\",\n+                  IRNode.VECTOR_BLEND_D,    IRNode.VECTOR_SIZE + \"min(max_long, max_double)\", \">0\",\n+                  IRNode.STORE_VECTOR, \">0\"},\n+        applyIfCPUFeatureOr = {\"avx2\", \"true\", \"asimd\", \"true\"})\n+    \/\/ Requires avx2, else L is restricted to 16 byte, and D has 32. That leads to a vector elements mismatch of 2 to 4.\n+    private static void testCMoveULEQforD(long[] a, long[] b, double[] c, double[] d, double[] r, double[] r2) {\n+        for (int i = 0; i < a.length; i++) {\n+            double cc = c[i];\n+            double dd = d[i];\n+            r2[i] = cc + dd;\n+            r[i] = Long.compareUnsigned(a[i], b[i]) == 0 ? cc : dd;\n+        }\n+    }\n+\n+    @Test\n+    @IR(counts = {IRNode.LOAD_VECTOR_L,     IRNode.VECTOR_SIZE + \"min(max_long, max_double)\", \">0\",\n+                  IRNode.LOAD_VECTOR_D,     IRNode.VECTOR_SIZE + \"min(max_long, max_double)\", \">0\",\n+                  IRNode.VECTOR_MASK_CMP_L, IRNode.VECTOR_SIZE + \"min(max_long, max_double)\", \">0\",\n+                  IRNode.VECTOR_BLEND_D,    IRNode.VECTOR_SIZE + \"min(max_long, max_double)\", \">0\",\n+                  IRNode.STORE_VECTOR, \">0\"},\n+        applyIfCPUFeatureOr = {\"avx2\", \"true\", \"asimd\", \"true\"})\n+    \/\/ Requires avx2, else L is restricted to 16 byte, and D has 32. That leads to a vector elements mismatch of 2 to 4.\n+    private static void testCMoveULNEforD(long[] a, long[] b, double[] c, double[] d, double[] r, double[] r2) {\n+        for (int i = 0; i < a.length; i++) {\n+            double cc = c[i];\n+            double dd = d[i];\n+            r2[i] = cc + dd;\n+            r[i] = Long.compareUnsigned(a[i], b[i]) != 0 ? cc : dd;\n+        }\n+    }\n+\n+    @Test\n+    @IR(counts = {IRNode.LOAD_VECTOR_L,     IRNode.VECTOR_SIZE + \"min(max_long, max_double)\", \">0\",\n+                  IRNode.LOAD_VECTOR_D,     IRNode.VECTOR_SIZE + \"min(max_long, max_double)\", \">0\",\n+                  IRNode.VECTOR_MASK_CMP_L, IRNode.VECTOR_SIZE + \"min(max_long, max_double)\", \">0\",\n+                  IRNode.VECTOR_BLEND_D,    IRNode.VECTOR_SIZE + \"min(max_long, max_double)\", \">0\",\n+                  IRNode.STORE_VECTOR, \">0\"},\n+        applyIfCPUFeatureOr = {\"avx2\", \"true\", \"asimd\", \"true\"})\n+    \/\/ Requires avx2, else L is restricted to 16 byte, and D has 32. That leads to a vector elements mismatch of 2 to 4.\n+    private static void testCMoveULGTforD(long[] a, long[] b, double[] c, double[] d, double[] r, double[] r2) {\n+        for (int i = 0; i < a.length; i++) {\n+            double cc = c[i];\n+            double dd = d[i];\n+            r2[i] = cc + dd;\n+            r[i] = Long.compareUnsigned(a[i], b[i]) > 0 ? cc : dd;\n+        }\n+    }\n+\n+    @Test\n+    @IR(counts = {IRNode.LOAD_VECTOR_L,     IRNode.VECTOR_SIZE + \"min(max_long, max_double)\", \">0\",\n+                  IRNode.LOAD_VECTOR_D,     IRNode.VECTOR_SIZE + \"min(max_long, max_double)\", \">0\",\n+                  IRNode.VECTOR_MASK_CMP_L, IRNode.VECTOR_SIZE + \"min(max_long, max_double)\", \">0\",\n+                  IRNode.VECTOR_BLEND_D,    IRNode.VECTOR_SIZE + \"min(max_long, max_double)\", \">0\",\n+                  IRNode.STORE_VECTOR, \">0\"},\n+        applyIfCPUFeatureOr = {\"avx2\", \"true\", \"asimd\", \"true\"})\n+    \/\/ Requires avx2, else L is restricted to 16 byte, and D has 32. That leads to a vector elements mismatch of 2 to 4.\n+    private static void testCMoveULGEforD(long[] a, long[] b, double[] c, double[] d, double[] r, double[] r2) {\n+        for (int i = 0; i < a.length; i++) {\n+            double cc = c[i];\n+            double dd = d[i];\n+            r2[i] = cc + dd;\n+            r[i] = Long.compareUnsigned(a[i], b[i]) >= 0 ? cc : dd;\n+        }\n+    }\n+\n+    @Test\n+    @IR(counts = {IRNode.LOAD_VECTOR_L,     IRNode.VECTOR_SIZE + \"min(max_long, max_double)\", \">0\",\n+                  IRNode.LOAD_VECTOR_D,     IRNode.VECTOR_SIZE + \"min(max_long, max_double)\", \">0\",\n+                  IRNode.VECTOR_MASK_CMP_L, IRNode.VECTOR_SIZE + \"min(max_long, max_double)\", \">0\",\n+                  IRNode.VECTOR_BLEND_D,    IRNode.VECTOR_SIZE + \"min(max_long, max_double)\", \">0\",\n+                  IRNode.STORE_VECTOR, \">0\"},\n+        applyIfCPUFeatureOr = {\"avx2\", \"true\", \"asimd\", \"true\"})\n+    \/\/ Requires avx2, else L is restricted to 16 byte, and D has 32. That leads to a vector elements mismatch of 2 to 4.\n+    private static void testCMoveULLTforD(long[] a, long[] b, double[] c, double[] d, double[] r, double[] r2) {\n+        for (int i = 0; i < a.length; i++) {\n+            double cc = c[i];\n+            double dd = d[i];\n+            r2[i] = cc + dd;\n+            r[i] = Long.compareUnsigned(a[i], b[i]) < 0 ? cc : dd;\n+        }\n+    }\n+\n+    @Test\n+    @IR(counts = {IRNode.LOAD_VECTOR_L,     IRNode.VECTOR_SIZE + \"min(max_long, max_double)\", \">0\",\n+                  IRNode.LOAD_VECTOR_D,     IRNode.VECTOR_SIZE + \"min(max_long, max_double)\", \">0\",\n+                  IRNode.VECTOR_MASK_CMP_L, IRNode.VECTOR_SIZE + \"min(max_long, max_double)\", \">0\",\n+                  IRNode.VECTOR_BLEND_D,    IRNode.VECTOR_SIZE + \"min(max_long, max_double)\", \">0\",\n+                  IRNode.STORE_VECTOR, \">0\"},\n+        applyIfCPUFeatureOr = {\"avx2\", \"true\", \"asimd\", \"true\"})\n+    \/\/ Requires avx2, else L is restricted to 16 byte, and D has 32. That leads to a vector elements mismatch of 2 to 4.\n+    private static void testCMoveULLEforD(long[] a, long[] b, double[] c, double[] d, double[] r, double[] r2) {\n+        for (int i = 0; i < a.length; i++) {\n+            double cc = c[i];\n+            double dd = d[i];\n+            r2[i] = cc + dd;\n+            r[i] = Long.compareUnsigned(a[i], b[i]) <= 0 ? cc : dd;\n+        }\n+    }\n+\n+    @Test\n+    @IR(failOn = {IRNode.STORE_VECTOR})\n+    private static void testCMoveFGTforI(float[] a, float[] b, int[] c, int[] d, int[] r, int[] r2) {\n+        for (int i = 0; i < a.length; i++) {\n+            int cc = c[i];\n+            int dd = d[i];\n+            r2[i] = cc + dd;\n+            r[i] = (a[i] > b[i]) ? cc : dd;\n+        }\n+    }\n+\n+    @Test\n+    @IR(failOn = {IRNode.STORE_VECTOR})\n+    private static void testCMoveFGTforL(float[] a, float[] b, long[] c, long[] d, long[] r, long[] r2) {\n+        for (int i = 0; i < a.length; i++) {\n+            long cc = c[i];\n+            long dd = d[i];\n+            r2[i] = cc + dd;\n+            r[i] = (a[i] > b[i]) ? cc : dd;\n+        }\n+    }\n+\n+    @Test\n+    @IR(counts = {IRNode.LOAD_VECTOR_F, \">0\",\n+                  IRNode.VECTOR_MASK_CMP_F, \">0\",\n+                  IRNode.VECTOR_BLEND_F, \">0\",\n+                  IRNode.STORE_VECTOR, \">0\"},\n+        applyIfCPUFeatureOr = {\"avx\", \"true\", \"asimd\", \"true\"})\n+    private static void testCMoveFGTforF(float[] a, float[] b, float[] c, float[] d, float[] r, float[] r2) {\n+        for (int i = 0; i < a.length; i++) {\n+            float cc = c[i];\n+            float dd = d[i];\n+            r2[i] = cc + dd;\n+            r[i] = (a[i] > b[i]) ? cc : dd;\n+        }\n+    }\n+\n+    @Test\n+    @IR(failOn = {IRNode.STORE_VECTOR})\n+    private static void testCMoveFGTforD(float[] a, float[] b, double[] c, double[] d, double[] r, double[] r2) {\n+        for (int i = 0; i < a.length; i++) {\n+            double cc = c[i];\n+            double dd = d[i];\n+            r2[i] = cc + dd;\n+            r[i] = (a[i] > b[i]) ? cc : dd;\n+        }\n+    }\n+\n+    @Test\n+    @IR(failOn = {IRNode.STORE_VECTOR})\n+    private static void testCMoveDGTforI(double[] a, double[] b, int[] c, int[] d, int[] r, int[] r2) {\n+        for (int i = 0; i < a.length; i++) {\n+            int cc = c[i];\n+            int dd = d[i];\n+            r2[i] = cc + dd;\n+            r[i] = (a[i] > b[i]) ? cc : dd;\n+        }\n+    }\n+\n+    @Test\n+    @IR(failOn = {IRNode.STORE_VECTOR})\n+    private static void testCMoveDGTforL(double[] a, double[] b, long[] c, long[] d, long[] r, long[] r2) {\n+        for (int i = 0; i < a.length; i++) {\n+            long cc = c[i];\n+            long dd = d[i];\n+            r2[i] = cc + dd;\n+            r[i] = (a[i] > b[i]) ? cc : dd;\n+        }\n+    }\n+\n+    @Test\n+    @IR(failOn = {IRNode.STORE_VECTOR})\n+    private static void testCMoveDGTforF(double[] a, double[] b, float[] c, float[] d, float[] r, float[] r2) {\n+        for (int i = 0; i < a.length; i++) {\n+            float cc = c[i];\n+            float dd = d[i];\n+            r2[i] = cc + dd;\n+            r[i] = (a[i] > b[i]) ? cc : dd;\n+        }\n+    }\n+\n+    @Test\n+    @IR(counts = {IRNode.LOAD_VECTOR_D, \">0\",\n+                  IRNode.VECTOR_MASK_CMP_D, \">0\",\n+                  IRNode.VECTOR_BLEND_D, \">0\",\n+                  IRNode.STORE_VECTOR, \">0\"},\n+        applyIfCPUFeatureOr = {\"avx\", \"true\", \"asimd\", \"true\"})\n+    private static void testCMoveDGTforD(double[] a, double[] b, double[] c, double[] d, double[] r, double[] r2) {\n+        for (int i = 0; i < a.length; i++) {\n+            double cc = c[i];\n+            double dd = d[i];\n+            r2[i] = cc + dd;\n+            r[i] = (a[i] > b[i]) ? cc : dd;\n+        }\n+    }\n+\n+    \/\/ Use some constants in the comparison\n+    @Test\n+    @IR(counts = {IRNode.LOAD_VECTOR_F, \">0\",\n+                  IRNode.VECTOR_MASK_CMP_F, \">0\",\n+                  IRNode.VECTOR_BLEND_F, \">0\",\n+                  IRNode.STORE_VECTOR, \">0\"},\n+        applyIfCPUFeatureOr = {\"avx\", \"true\", \"asimd\", \"true\"})\n+    private static void testCMoveFGTforFCmpCon1(float a, float[] b, float[] c, float[] d, float[] r, float[] r2) {\n+        for (int i = 0; i < b.length; i++) {\n+            float cc = c[i];\n+            float dd = d[i];\n+            r2[i] = cc + dd;\n+            r[i] = (a > b[i]) ? cc : dd;\n+        }\n+    }\n+\n+    @Test\n+    @IR(counts = {IRNode.LOAD_VECTOR_F, \">0\",\n+                  IRNode.VECTOR_MASK_CMP_F, \">0\",\n+                  IRNode.VECTOR_BLEND_F, \">0\",\n+                  IRNode.STORE_VECTOR, \">0\"},\n+        applyIfCPUFeatureOr = {\"avx\", \"true\", \"asimd\", \"true\"})\n+    private static void testCMoveFGTforFCmpCon2(float[] a, float b, float[] c, float[] d, float[] r, float[] r2) {\n+        for (int i = 0; i < a.length; i++) {\n+            float cc = c[i];\n+            float dd = d[i];\n+            r2[i] = cc + dd;\n+            r[i] = (a[i] > b) ? cc : dd;\n+        }\n+    }\n+\n+    \/\/ A case that is currently not supported and is not expected to vectorize\n+    @Test\n+    @IR(failOn = {IRNode.STORE_VECTOR})\n+    private static void testCMoveVDUnsupported() {\n+        double[] doublec = new double[SIZE];\n+        int seed = 1001;\n+        for (int i = 0; i < doublec.length; i++) {\n+            doublec[i] = (i % 2 == 0) ? seed + i : seed - i;\n+        }\n+    }\n+\n+    @Warmup(0)\n+    @Run(test = {\"testCMoveVFGT\", \"testCMoveVFLT\",\"testCMoveVDLE\", \"testCMoveVDGE\", \"testCMoveVFEQ\", \"testCMoveVDNE\",\n+                 \"testCMoveVFGTSwap\", \"testCMoveVFLTSwap\",\"testCMoveVDLESwap\", \"testCMoveVDGESwap\",\n+                 \"testCMoveFGTforFConst\", \"testCMoveFGEforFConst\", \"testCMoveFLTforFConst\",\n+                 \"testCMoveFLEforFConst\", \"testCMoveFEQforFConst\", \"testCMoveFNEQforFConst\",\n+                 \"testCMoveDGTforDConst\", \"testCMoveDGEforDConst\", \"testCMoveDLTforDConst\",\n+                 \"testCMoveDLEforDConst\", \"testCMoveDEQforDConst\", \"testCMoveDNEQforDConst\",\n+                 \"testCMoveFLTforFConstH2\", \"testCMoveFLEforFConstH2\",\n+                 \"testCMoveFYYforFConstH2\", \"testCMoveFXXforFConstH2\",\n+                 \"testCMoveDLTforDConstH2\", \"testCMoveDLEforDConstH2\",\n+                 \"testCMoveDYYforDConstH2\", \"testCMoveDXXforDConstH2\"})\n+    private void testCMove_runner() {\n+        float[] floata = new float[SIZE];\n+        float[] floatb = new float[SIZE];\n+        float[] floatc = new float[SIZE];\n+        double[] doublea = new double[SIZE];\n+        double[] doubleb = new double[SIZE];\n+        double[] doublec = new double[SIZE];\n+\n+        init(floata);\n+        init(floatb);\n+        init(doublea);\n+        init(doubleb);\n+\n+        testCMoveVFGT(floata, floatb, floatc);\n+        testCMoveVDLE(doublea, doubleb, doublec);\n+        for (int i = 0; i < SIZE; i++) {\n+            Asserts.assertEquals(floatc[i], cmoveFloatGT(floata[i], floatb[i]));\n+            Asserts.assertEquals(doublec[i], cmoveDoubleLE(doublea[i], doubleb[i]));\n+        }\n+\n+        testCMoveVFLT(floata, floatb, floatc);\n+        testCMoveVDGE(doublea, doubleb, doublec);\n+        for (int i = 0; i < SIZE; i++) {\n+            Asserts.assertEquals(floatc[i], cmoveFloatLT(floata[i], floatb[i]));\n+            Asserts.assertEquals(doublec[i], cmoveDoubleGE(doublea[i], doubleb[i]));\n+        }\n+\n+        \/\/ Ensure we frequently have equals\n+        for (int i = 0; i < SIZE; i++) {\n+            if (i % 3 == 0) {\n+                floatb[i] = floata[i];\n+                doubleb[i] = doublea[i];\n+            }\n+        }\n+\n+        testCMoveVFEQ(floata, floatb, floatc);\n+        testCMoveVDNE(doublea, doubleb, doublec);\n+        for (int i = 0; i < SIZE; i++) {\n+            Asserts.assertEquals(floatc[i], cmoveFloatEQ(floata[i], floatb[i]));\n+            Asserts.assertEquals(doublec[i], cmoveDoubleNE(doublea[i], doubleb[i]));\n+        }\n+\n+        testCMoveVFGTSwap(floata, floatb, floatc);\n+        testCMoveVDLESwap(doublea, doubleb, doublec);\n+        for (int i = 0; i < SIZE; i++) {\n+            Asserts.assertEquals(floatc[i], cmoveFloatGTSwap(floata[i], floatb[i]));\n+            Asserts.assertEquals(doublec[i], cmoveDoubleLESwap(doublea[i], doubleb[i]));\n+        }\n+\n+        testCMoveVFLTSwap(floata, floatb, floatc);\n+        testCMoveVDGESwap(doublea, doubleb, doublec);\n+        for (int i = 0; i < SIZE; i++) {\n+            Asserts.assertEquals(floatc[i], cmoveFloatLTSwap(floata[i], floatb[i]));\n+            Asserts.assertEquals(doublec[i], cmoveDoubleGESwap(doublea[i], doubleb[i]));\n+        }\n+\n+        \/\/ Extensions: compare 2 values, and pick from 2 consts\n+        testCMoveFGTforFConst(floata, floatb, floatc);\n+        testCMoveDGTforDConst(doublea, doubleb, doublec);\n+        for (int i = 0; i < SIZE; i++) {\n+            Asserts.assertEquals(floatc[i], cmoveFGTforFConst(floata[i], floatb[i]));\n+            Asserts.assertEquals(doublec[i], cmoveDGTforDConst(doublea[i], doubleb[i]));\n+        }\n+\n+        testCMoveFGEforFConst(floata, floatb, floatc);\n+        testCMoveDGEforDConst(doublea, doubleb, doublec);\n+        for (int i = 0; i < SIZE; i++) {\n+            Asserts.assertEquals(floatc[i], cmoveFGEforFConst(floata[i], floatb[i]));\n+            Asserts.assertEquals(doublec[i], cmoveDGEforDConst(doublea[i], doubleb[i]));\n+        }\n+\n+        testCMoveFLTforFConst(floata, floatb, floatc);\n+        testCMoveDLTforDConst(doublea, doubleb, doublec);\n+        for (int i = 0; i < SIZE; i++) {\n+            Asserts.assertEquals(floatc[i], cmoveFLTforFConst(floata[i], floatb[i]));\n+            Asserts.assertEquals(doublec[i], cmoveDLTforDConst(doublea[i], doubleb[i]));\n+        }\n+\n+        testCMoveFLEforFConst(floata, floatb, floatc);\n+        testCMoveDLEforDConst(doublea, doubleb, doublec);\n+        for (int i = 0; i < SIZE; i++) {\n+            Asserts.assertEquals(floatc[i], cmoveFLEforFConst(floata[i], floatb[i]));\n+            Asserts.assertEquals(doublec[i], cmoveDLEforDConst(doublea[i], doubleb[i]));\n+        }\n+\n+        testCMoveFEQforFConst(floata, floatb, floatc);\n+        testCMoveDEQforDConst(doublea, doubleb, doublec);\n+        for (int i = 0; i < SIZE; i++) {\n+            Asserts.assertEquals(floatc[i], cmoveFEQforFConst(floata[i], floatb[i]));\n+            Asserts.assertEquals(doublec[i], cmoveDEQforDConst(doublea[i], doubleb[i]));\n+        }\n+\n+        testCMoveFNEQforFConst(floata, floatb, floatc);\n+        testCMoveDNEQforDConst(doublea, doubleb, doublec);\n+        for (int i = 0; i < SIZE; i++) {\n+            Asserts.assertEquals(floatc[i], cmoveFNEQforFConst(floata[i], floatb[i]));\n+            Asserts.assertEquals(doublec[i], cmoveDNEQforDConst(doublea[i], doubleb[i]));\n+        }\n+\n+        \/\/ Hand-unrolled (H2) examples:\n+        testCMoveFLTforFConstH2(floata, floatb, floatc);\n+        testCMoveDLTforDConstH2(doublea, doubleb, doublec);\n+        for (int i = 0; i < SIZE; i++) {\n+            Asserts.assertEquals(floatc[i], cmoveFLTforFConst(floata[i], floatb[i]));\n+            Asserts.assertEquals(doublec[i], cmoveDLTforDConst(doublea[i], doubleb[i]));\n+        }\n+\n+        testCMoveFLEforFConstH2(floata, floatb, floatc);\n+        testCMoveDLEforDConstH2(doublea, doubleb, doublec);\n+        for (int i = 0; i < SIZE; i++) {\n+            Asserts.assertEquals(floatc[i], cmoveFLEforFConst(floata[i], floatb[i]));\n+            Asserts.assertEquals(doublec[i], cmoveDLEforDConst(doublea[i], doubleb[i]));\n+        }\n+\n+        testCMoveFYYforFConstH2(floata, floatb, floatc);\n+        testCMoveDYYforDConstH2(doublea, doubleb, doublec);\n+        for (int i = 0; i < SIZE; i+=2) {\n+            Asserts.assertEquals(floatc[i+0], cmoveFLEforFConst(floata[i+0], floatb[i+0]));\n+            Asserts.assertEquals(doublec[i+0], cmoveDLEforDConst(doublea[i+0], doubleb[i+0]));\n+            Asserts.assertEquals(floatc[i+1], cmoveFLTforFConst(floata[i+1], floatb[i+1]));\n+            Asserts.assertEquals(doublec[i+1], cmoveDLTforDConst(doublea[i+1], doubleb[i+1]));\n+        }\n+\n+        testCMoveFXXforFConstH2(floata, floatb, floatc);\n+        testCMoveDXXforDConstH2(doublea, doubleb, doublec);\n+        for (int i = 0; i < SIZE; i+=2) {\n+            Asserts.assertEquals(floatc[i+0], cmoveFLTforFConst(floata[i+0], floatb[i+0]));\n+            Asserts.assertEquals(doublec[i+0], cmoveDLTforDConst(doublea[i+0], doubleb[i+0]));\n+            Asserts.assertEquals(floatc[i+1], cmoveFLEforFConst(floata[i+1], floatb[i+1]));\n+            Asserts.assertEquals(doublec[i+1], cmoveDLEforDConst(doublea[i+1], doubleb[i+1]));\n+        }\n+    }\n+\n+    @Warmup(0)\n+    @Run(test = {\/\/ Signed\n+                 \/\/     I for I\n+                 \"testCMoveIEQforI\",\n+                 \"testCMoveINEforI\",\n+                 \"testCMoveIGTforI\",\n+                 \"testCMoveIGEforI\",\n+                 \"testCMoveILTforI\",\n+                 \"testCMoveILEforI\",\n+                 \/\/     I for L\n+                 \"testCMoveIEQforL\",\n+                 \"testCMoveINEforL\",\n+                 \"testCMoveIGTforL\",\n+                 \"testCMoveIGEforL\",\n+                 \"testCMoveILTforL\",\n+                 \"testCMoveILEforL\",\n+                 \/\/     I for F\n+                 \"testCMoveIEQforF\",\n+                 \"testCMoveINEforF\",\n+                 \"testCMoveIGTforF\",\n+                 \"testCMoveIGEforF\",\n+                 \"testCMoveILTforF\",\n+                 \"testCMoveILEforF\",\n+                 \/\/     I for D\n+                 \"testCMoveIEQforD\",\n+                 \"testCMoveINEforD\",\n+                 \"testCMoveIGTforD\",\n+                 \"testCMoveIGEforD\",\n+                 \"testCMoveILTforD\",\n+                 \"testCMoveILEforD\",\n+                 \/\/     L for I\n+                 \"testCMoveLEQforI\",\n+                 \"testCMoveLNEforI\",\n+                 \"testCMoveLGTforI\",\n+                 \"testCMoveLGEforI\",\n+                 \"testCMoveLLTforI\",\n+                 \"testCMoveLLEforI\",\n+                 \/\/     L for L\n+                 \"testCMoveLEQforL\",\n+                 \"testCMoveLNEforL\",\n+                 \"testCMoveLGTforL\",\n+                 \"testCMoveLGEforL\",\n+                 \"testCMoveLLTforL\",\n+                 \"testCMoveLLEforL\",\n+                 \/\/     L for F\n+                 \"testCMoveLEQforF\",\n+                 \"testCMoveLNEforF\",\n+                 \"testCMoveLGTforF\",\n+                 \"testCMoveLGEforF\",\n+                 \"testCMoveLLTforF\",\n+                 \"testCMoveLLEforF\",\n+                 \/\/     L for D\n+                 \"testCMoveLEQforD\",\n+                 \"testCMoveLNEforD\",\n+                 \"testCMoveLGTforD\",\n+                 \"testCMoveLGEforD\",\n+                 \"testCMoveLLTforD\",\n+                 \"testCMoveLLEforD\",\n+                 \/\/ Unsigned\n+                 \/\/     I for I\n+                 \"testCMoveUIEQforI\",\n+                 \"testCMoveUINEforI\",\n+                 \"testCMoveUIGTforI\",\n+                 \"testCMoveUIGEforI\",\n+                 \"testCMoveUILTforI\",\n+                 \"testCMoveUILEforI\",\n+                 \/\/     I for L\n+                 \"testCMoveUIEQforL\",\n+                 \"testCMoveUINEforL\",\n+                 \"testCMoveUIGTforL\",\n+                 \"testCMoveUIGEforL\",\n+                 \"testCMoveUILTforL\",\n+                 \"testCMoveUILEforL\",\n+                 \/\/     I for F\n+                 \"testCMoveUIEQforF\",\n+                 \"testCMoveUINEforF\",\n+                 \"testCMoveUIGTforF\",\n+                 \"testCMoveUIGEforF\",\n+                 \"testCMoveUILTforF\",\n+                 \"testCMoveUILEforF\",\n+                 \/\/     I for D\n+                 \"testCMoveUIEQforD\",\n+                 \"testCMoveUINEforD\",\n+                 \"testCMoveUIGTforD\",\n+                 \"testCMoveUIGEforD\",\n+                 \"testCMoveUILTforD\",\n+                 \"testCMoveUILEforD\",\n+                 \/\/     L for I\n+                 \"testCMoveULEQforI\",\n+                 \"testCMoveULNEforI\",\n+                 \"testCMoveULGTforI\",\n+                 \"testCMoveULGEforI\",\n+                 \"testCMoveULLTforI\",\n+                 \"testCMoveULLEforI\",\n+                 \/\/     L for L\n+                 \"testCMoveULEQforL\",\n+                 \"testCMoveULNEforL\",\n+                 \"testCMoveULGTforL\",\n+                 \"testCMoveULGEforL\",\n+                 \"testCMoveULLTforL\",\n+                 \"testCMoveULLEforL\",\n+                 \/\/     L for F\n+                 \"testCMoveULEQforF\",\n+                 \"testCMoveULNEforF\",\n+                 \"testCMoveULGTforF\",\n+                 \"testCMoveULGEforF\",\n+                 \"testCMoveULLTforF\",\n+                 \"testCMoveULLEforF\",\n+                 \/\/     L for D\n+                 \"testCMoveULEQforD\",\n+                 \"testCMoveULNEforD\",\n+                 \"testCMoveULGTforD\",\n+                 \"testCMoveULGEforD\",\n+                 \"testCMoveULLTforD\",\n+                 \"testCMoveULLEforD\",\n+                 \/\/ Float\n+                 \"testCMoveFGTforI\",\n+                 \"testCMoveFGTforL\",\n+                 \"testCMoveFGTforF\",\n+                 \"testCMoveFGTforD\",\n+                 \"testCMoveDGTforI\",\n+                 \"testCMoveDGTforL\",\n+                 \"testCMoveDGTforF\",\n+                 \"testCMoveDGTforD\",\n+                 \"testCMoveFGTforFCmpCon1\",\n+                 \"testCMoveFGTforFCmpCon2\"})\n+    private void testCMove_runner_two() {\n+        int[] aI = new int[SIZE];\n+        int[] bI = new int[SIZE];\n+        int[] cI = new int[SIZE];\n+        int[] dI = new int[SIZE];\n+        int[] rI = new int[SIZE];\n+        long[] aL = new long[SIZE];\n+        long[] bL = new long[SIZE];\n+        long[] cL = new long[SIZE];\n+        long[] dL = new long[SIZE];\n+        long[] rL = new long[SIZE];\n+        float[] aF = new float[SIZE];\n+        float[] bF = new float[SIZE];\n+        float[] cF = new float[SIZE];\n+        float[] dF = new float[SIZE];\n+        float[] rF = new float[SIZE];\n+        double[] aD = new double[SIZE];\n+        double[] bD = new double[SIZE];\n+        double[] cD = new double[SIZE];\n+        double[] dD = new double[SIZE];\n+        double[] rD = new double[SIZE];\n+\n+        init(aI);\n+        init(bI);\n+        init(cI);\n+        init(dI);\n+        init(aL);\n+        init(bL);\n+        init(cL);\n+        init(dL);\n+        init(aF);\n+        init(bF);\n+        init(cF);\n+        init(dF);\n+        init(aD);\n+        init(bD);\n+        init(cD);\n+        init(dD);\n+\n+        \/\/ Signed\n+        \/\/     I for I\n+        testCMoveIEQforI(aI, bI, cI, dI, rI, rI);\n+        for (int i = 0; i < SIZE; i++) {\n+            Asserts.assertEquals(rI[i], cmoveIEQforI(aI[i], bI[i], cI[i], dI[i]));\n+        }\n+\n+        testCMoveINEforI(aI, bI, cI, dI, rI, rI);\n+        for (int i = 0; i < SIZE; i++) {\n+            Asserts.assertEquals(rI[i], cmoveINEforI(aI[i], bI[i], cI[i], dI[i]));\n+        }\n+\n+        testCMoveIGTforI(aI, bI, cI, dI, rI, rI);\n+        for (int i = 0; i < SIZE; i++) {\n+            Asserts.assertEquals(rI[i], cmoveIGTforI(aI[i], bI[i], cI[i], dI[i]));\n+        }\n+\n+        testCMoveIGEforI(aI, bI, cI, dI, rI, rI);\n+        for (int i = 0; i < SIZE; i++) {\n+            Asserts.assertEquals(rI[i], cmoveIGEforI(aI[i], bI[i], cI[i], dI[i]));\n+        }\n+\n+        testCMoveILTforI(aI, bI, cI, dI, rI, rI);\n+        for (int i = 0; i < SIZE; i++) {\n+            Asserts.assertEquals(rI[i], cmoveILTforI(aI[i], bI[i], cI[i], dI[i]));\n+        }\n+\n+        testCMoveILEforI(aI, bI, cI, dI, rI, rI);\n+        for (int i = 0; i < SIZE; i++) {\n+            Asserts.assertEquals(rI[i], cmoveILEforI(aI[i], bI[i], cI[i], dI[i]));\n+        }\n+\n+        \/\/     I for L\n+        testCMoveIEQforL(aI, bI, cL, dL, rL, rL);\n+        for (int i = 0; i < SIZE; i++) {\n+            Asserts.assertEquals(rL[i], cmoveIEQforL(aI[i], bI[i], cL[i], dL[i]));\n+        }\n+\n+        testCMoveINEforL(aI, bI, cL, dL, rL, rL);\n+        for (int i = 0; i < SIZE; i++) {\n+            Asserts.assertEquals(rL[i], cmoveINEforL(aI[i], bI[i], cL[i], dL[i]));\n+        }\n+\n+        testCMoveIGTforL(aI, bI, cL, dL, rL, rL);\n+        for (int i = 0; i < SIZE; i++) {\n+            Asserts.assertEquals(rL[i], cmoveIGTforL(aI[i], bI[i], cL[i], dL[i]));\n+        }\n+\n+        testCMoveIGEforL(aI, bI, cL, dL, rL, rL);\n+        for (int i = 0; i < SIZE; i++) {\n+            Asserts.assertEquals(rL[i], cmoveIGEforL(aI[i], bI[i], cL[i], dL[i]));\n+        }\n+\n+        testCMoveILTforL(aI, bI, cL, dL, rL, rL);\n+        for (int i = 0; i < SIZE; i++) {\n+            Asserts.assertEquals(rL[i], cmoveILTforL(aI[i], bI[i], cL[i], dL[i]));\n+        }\n+\n+        testCMoveILEforL(aI, bI, cL, dL, rL, rL);\n+        for (int i = 0; i < SIZE; i++) {\n+            Asserts.assertEquals(rL[i], cmoveILEforL(aI[i], bI[i], cL[i], dL[i]));\n+        }\n+\n+        \/\/     I for F\n+        testCMoveIEQforF(aI, bI, cF, dF, rF, rF);\n+        for (int i = 0; i < SIZE; i++) {\n+            Asserts.assertEquals(rF[i], cmoveIEQforF(aI[i], bI[i], cF[i], dF[i]));\n+        }\n+\n+        testCMoveINEforF(aI, bI, cF, dF, rF, rF);\n+        for (int i = 0; i < SIZE; i++) {\n+            Asserts.assertEquals(rF[i], cmoveINEforF(aI[i], bI[i], cF[i], dF[i]));\n+        }\n+\n+        testCMoveIGTforF(aI, bI, cF, dF, rF, rF);\n+        for (int i = 0; i < SIZE; i++) {\n+            Asserts.assertEquals(rF[i], cmoveIGTforF(aI[i], bI[i], cF[i], dF[i]));\n+        }\n+\n+        testCMoveIGEforF(aI, bI, cF, dF, rF, rF);\n+        for (int i = 0; i < SIZE; i++) {\n+            Asserts.assertEquals(rF[i], cmoveIGEforF(aI[i], bI[i], cF[i], dF[i]));\n+        }\n+\n+        testCMoveILTforF(aI, bI, cF, dF, rF, rF);\n+        for (int i = 0; i < SIZE; i++) {\n+            Asserts.assertEquals(rF[i], cmoveILTforF(aI[i], bI[i], cF[i], dF[i]));\n+        }\n+\n+        testCMoveILEforF(aI, bI, cF, dF, rF, rF);\n+        for (int i = 0; i < SIZE; i++) {\n+            Asserts.assertEquals(rF[i], cmoveILEforF(aI[i], bI[i], cF[i], dF[i]));\n+        }\n+\n+        \/\/     I for D\n+        testCMoveIEQforD(aI, bI, cD, dD, rD, rD);\n+        for (int i = 0; i < SIZE; i++) {\n+            Asserts.assertEquals(rD[i], cmoveIEQforD(aI[i], bI[i], cD[i], dD[i]));\n+        }\n+\n+        testCMoveINEforD(aI, bI, cD, dD, rD, rD);\n+        for (int i = 0; i < SIZE; i++) {\n+            Asserts.assertEquals(rD[i], cmoveINEforD(aI[i], bI[i], cD[i], dD[i]));\n+        }\n+\n+        testCMoveIGTforD(aI, bI, cD, dD, rD, rD);\n+        for (int i = 0; i < SIZE; i++) {\n+            Asserts.assertEquals(rD[i], cmoveIGTforD(aI[i], bI[i], cD[i], dD[i]));\n+        }\n+\n+        testCMoveIGEforD(aI, bI, cD, dD, rD, rD);\n+        for (int i = 0; i < SIZE; i++) {\n+            Asserts.assertEquals(rD[i], cmoveIGEforD(aI[i], bI[i], cD[i], dD[i]));\n+        }\n+\n+        testCMoveILTforD(aI, bI, cD, dD, rD, rD);\n+        for (int i = 0; i < SIZE; i++) {\n+            Asserts.assertEquals(rD[i], cmoveILTforD(aI[i], bI[i], cD[i], dD[i]));\n+        }\n+\n+        testCMoveILEforD(aI, bI, cD, dD, rD, rD);\n+        for (int i = 0; i < SIZE; i++) {\n+            Asserts.assertEquals(rD[i], cmoveILEforD(aI[i], bI[i], cD[i], dD[i]));\n+        }\n+\n+        \/\/     L for I\n+        testCMoveLEQforI(aL, bL, cI, dI, rI, rI);\n+        for (int i = 0; i < SIZE; i++) {\n+            Asserts.assertEquals(rI[i], cmoveLEQforI(aL[i], bL[i], cI[i], dI[i]));\n+        }\n+\n+        testCMoveLNEforI(aL, bL, cI, dI, rI, rI);\n+        for (int i = 0; i < SIZE; i++) {\n+            Asserts.assertEquals(rI[i], cmoveLNEforI(aL[i], bL[i], cI[i], dI[i]));\n+        }\n+\n+        testCMoveLGTforI(aL, bL, cI, dI, rI, rI);\n+        for (int i = 0; i < SIZE; i++) {\n+            Asserts.assertEquals(rI[i], cmoveLGTforI(aL[i], bL[i], cI[i], dI[i]));\n+        }\n+\n+        testCMoveLGEforI(aL, bL, cI, dI, rI, rI);\n+        for (int i = 0; i < SIZE; i++) {\n+            Asserts.assertEquals(rI[i], cmoveLGEforI(aL[i], bL[i], cI[i], dI[i]));\n+        }\n+\n+        testCMoveLLTforI(aL, bL, cI, dI, rI, rI);\n+        for (int i = 0; i < SIZE; i++) {\n+            Asserts.assertEquals(rI[i], cmoveLLTforI(aL[i], bL[i], cI[i], dI[i]));\n+        }\n+\n+        testCMoveLLEforI(aL, bL, cI, dI, rI, rI);\n+        for (int i = 0; i < SIZE; i++) {\n+            Asserts.assertEquals(rI[i], cmoveLLEforI(aL[i], bL[i], cI[i], dI[i]));\n+        }\n+\n+        \/\/     L for L\n+        testCMoveLEQforL(aL, bL, cL, dL, rL, rL);\n+        for (int i = 0; i < SIZE; i++) {\n+            Asserts.assertEquals(rL[i], cmoveLEQforL(aL[i], bL[i], cL[i], dL[i]));\n+        }\n+\n+        testCMoveLNEforL(aL, bL, cL, dL, rL, rL);\n+        for (int i = 0; i < SIZE; i++) {\n+            Asserts.assertEquals(rL[i], cmoveLNEforL(aL[i], bL[i], cL[i], dL[i]));\n+        }\n+\n+        testCMoveLGTforL(aL, bL, cL, dL, rL, rL);\n+        for (int i = 0; i < SIZE; i++) {\n+            Asserts.assertEquals(rL[i], cmoveLGTforL(aL[i], bL[i], cL[i], dL[i]));\n+        }\n+\n+        testCMoveLGEforL(aL, bL, cL, dL, rL, rL);\n+        for (int i = 0; i < SIZE; i++) {\n+            Asserts.assertEquals(rL[i], cmoveLGEforL(aL[i], bL[i], cL[i], dL[i]));\n+        }\n+\n+        testCMoveLLTforL(aL, bL, cL, dL, rL, rL);\n+        for (int i = 0; i < SIZE; i++) {\n+            Asserts.assertEquals(rL[i], cmoveLLTforL(aL[i], bL[i], cL[i], dL[i]));\n+        }\n+\n+        testCMoveLLEforL(aL, bL, cL, dL, rL, rL);\n+        for (int i = 0; i < SIZE; i++) {\n+            Asserts.assertEquals(rL[i], cmoveLLEforL(aL[i], bL[i], cL[i], dL[i]));\n+        }\n+\n+        \/\/     L for F\n+        testCMoveLEQforF(aL, bL, cF, dF, rF, rF);\n+        for (int i = 0; i < SIZE; i++) {\n+            Asserts.assertEquals(rF[i], cmoveLEQforF(aL[i], bL[i], cF[i], dF[i]));\n+        }\n+\n+        testCMoveLNEforF(aL, bL, cF, dF, rF, rF);\n+        for (int i = 0; i < SIZE; i++) {\n+            Asserts.assertEquals(rF[i], cmoveLNEforF(aL[i], bL[i], cF[i], dF[i]));\n+        }\n+\n+        testCMoveLGTforF(aL, bL, cF, dF, rF, rF);\n+        for (int i = 0; i < SIZE; i++) {\n+            Asserts.assertEquals(rF[i], cmoveLGTforF(aL[i], bL[i], cF[i], dF[i]));\n+        }\n+\n+        testCMoveLGEforF(aL, bL, cF, dF, rF, rF);\n+        for (int i = 0; i < SIZE; i++) {\n+            Asserts.assertEquals(rF[i], cmoveLGEforF(aL[i], bL[i], cF[i], dF[i]));\n+        }\n+\n+        testCMoveLLTforF(aL, bL, cF, dF, rF, rF);\n+        for (int i = 0; i < SIZE; i++) {\n+            Asserts.assertEquals(rF[i], cmoveLLTforF(aL[i], bL[i], cF[i], dF[i]));\n+        }\n+\n+        testCMoveLLEforF(aL, bL, cF, dF, rF, rF);\n+        for (int i = 0; i < SIZE; i++) {\n+            Asserts.assertEquals(rF[i], cmoveLLEforF(aL[i], bL[i], cF[i], dF[i]));\n+        }\n+\n+        \/\/     L for D\n+        testCMoveLEQforD(aL, bL, cD, dD, rD, rD);\n+        for (int i = 0; i < SIZE; i++) {\n+            Asserts.assertEquals(rD[i], cmoveLEQforD(aL[i], bL[i], cD[i], dD[i]));\n+        }\n+\n+        testCMoveLNEforD(aL, bL, cD, dD, rD, rD);\n+        for (int i = 0; i < SIZE; i++) {\n+            Asserts.assertEquals(rD[i], cmoveLNEforD(aL[i], bL[i], cD[i], dD[i]));\n+        }\n+\n+        testCMoveLGTforD(aL, bL, cD, dD, rD, rD);\n+        for (int i = 0; i < SIZE; i++) {\n+            Asserts.assertEquals(rD[i], cmoveLGTforD(aL[i], bL[i], cD[i], dD[i]));\n+        }\n+\n+        testCMoveLGEforD(aL, bL, cD, dD, rD, rD);\n+        for (int i = 0; i < SIZE; i++) {\n+            Asserts.assertEquals(rD[i], cmoveLGEforD(aL[i], bL[i], cD[i], dD[i]));\n+        }\n+\n+        testCMoveLLTforD(aL, bL, cD, dD, rD, rD);\n+        for (int i = 0; i < SIZE; i++) {\n+            Asserts.assertEquals(rD[i], cmoveLLTforD(aL[i], bL[i], cD[i], dD[i]));\n+        }\n+\n+        testCMoveLLEforD(aL, bL, cD, dD, rD, rD);\n+        for (int i = 0; i < SIZE; i++) {\n+            Asserts.assertEquals(rD[i], cmoveLLEforD(aL[i], bL[i], cD[i], dD[i]));\n+        }\n+\n+        \/\/ Unsigned\n+        \/\/     I for I\n+        testCMoveUIEQforI(aI, bI, cI, dI, rI, rI);\n+        for (int i = 0; i < SIZE; i++) {\n+            Asserts.assertEquals(rI[i], cmoveUIEQforI(aI[i], bI[i], cI[i], dI[i]));\n+        }\n+\n+        testCMoveUINEforI(aI, bI, cI, dI, rI, rI);\n+        for (int i = 0; i < SIZE; i++) {\n+            Asserts.assertEquals(rI[i], cmoveUINEforI(aI[i], bI[i], cI[i], dI[i]));\n+        }\n+\n+        testCMoveUIGTforI(aI, bI, cI, dI, rI, rI);\n+        for (int i = 0; i < SIZE; i++) {\n+            Asserts.assertEquals(rI[i], cmoveUIGTforI(aI[i], bI[i], cI[i], dI[i]));\n+        }\n+\n+        testCMoveUIGEforI(aI, bI, cI, dI, rI, rI);\n+        for (int i = 0; i < SIZE; i++) {\n+            Asserts.assertEquals(rI[i], cmoveUIGEforI(aI[i], bI[i], cI[i], dI[i]));\n+        }\n+\n+        testCMoveUILTforI(aI, bI, cI, dI, rI, rI);\n+        for (int i = 0; i < SIZE; i++) {\n+            Asserts.assertEquals(rI[i], cmoveUILTforI(aI[i], bI[i], cI[i], dI[i]));\n+        }\n+\n+        testCMoveUILEforI(aI, bI, cI, dI, rI, rI);\n+        for (int i = 0; i < SIZE; i++) {\n+            Asserts.assertEquals(rI[i], cmoveUILEforI(aI[i], bI[i], cI[i], dI[i]));\n+        }\n+\n+        \/\/     I for L\n+        testCMoveUIEQforL(aI, bI, cL, dL, rL, rL);\n+        for (int i = 0; i < SIZE; i++) {\n+            Asserts.assertEquals(rL[i], cmoveUIEQforL(aI[i], bI[i], cL[i], dL[i]));\n+        }\n+\n+        testCMoveUINEforL(aI, bI, cL, dL, rL, rL);\n+        for (int i = 0; i < SIZE; i++) {\n+            Asserts.assertEquals(rL[i], cmoveUINEforL(aI[i], bI[i], cL[i], dL[i]));\n+        }\n+\n+        testCMoveUIGTforL(aI, bI, cL, dL, rL, rL);\n+        for (int i = 0; i < SIZE; i++) {\n+            Asserts.assertEquals(rL[i], cmoveUIGTforL(aI[i], bI[i], cL[i], dL[i]));\n+        }\n+\n+        testCMoveUIGEforL(aI, bI, cL, dL, rL, rL);\n+        for (int i = 0; i < SIZE; i++) {\n+            Asserts.assertEquals(rL[i], cmoveUIGEforL(aI[i], bI[i], cL[i], dL[i]));\n+        }\n+\n+        testCMoveUILTforL(aI, bI, cL, dL, rL, rL);\n+        for (int i = 0; i < SIZE; i++) {\n+            Asserts.assertEquals(rL[i], cmoveUILTforL(aI[i], bI[i], cL[i], dL[i]));\n+        }\n+\n+        testCMoveUILEforL(aI, bI, cL, dL, rL, rL);\n+        for (int i = 0; i < SIZE; i++) {\n+            Asserts.assertEquals(rL[i], cmoveUILEforL(aI[i], bI[i], cL[i], dL[i]));\n+        }\n+\n+        \/\/     I for F\n+        testCMoveUIEQforF(aI, bI, cF, dF, rF, rF);\n+        for (int i = 0; i < SIZE; i++) {\n+            Asserts.assertEquals(rF[i], cmoveUIEQforF(aI[i], bI[i], cF[i], dF[i]));\n+        }\n@@ -746,8 +2930,3 @@\n-    @Test\n-    @IR(failOn = {IRNode.STORE_VECTOR})\n-    private static void testCMoveDGTforL(double[] a, double[] b, long[] c, long[] d, long[] r, long[] r2) {\n-        for (int i = 0; i < a.length; i++) {\n-            long cc = c[i];\n-            long dd = d[i];\n-            r2[i] = cc + dd;\n-            r[i] = (a[i] > b[i]) ? cc : dd;\n+        testCMoveUINEforF(aI, bI, cF, dF, rF, rF);\n+        for (int i = 0; i < SIZE; i++) {\n+            Asserts.assertEquals(rF[i], cmoveUINEforF(aI[i], bI[i], cF[i], dF[i]));\n@@ -755,9 +2934,3 @@\n-    }\n-    @Test\n-    @IR(failOn = {IRNode.STORE_VECTOR})\n-    private static void testCMoveDGTforF(double[] a, double[] b, float[] c, float[] d, float[] r, float[] r2) {\n-        for (int i = 0; i < a.length; i++) {\n-            float cc = c[i];\n-            float dd = d[i];\n-            r2[i] = cc + dd;\n-            r[i] = (a[i] > b[i]) ? cc : dd;\n+        testCMoveUIGTforF(aI, bI, cF, dF, rF, rF);\n+        for (int i = 0; i < SIZE; i++) {\n+            Asserts.assertEquals(rF[i], cmoveUIGTforF(aI[i], bI[i], cF[i], dF[i]));\n@@ -766,13 +2939,3 @@\n-    }\n-    @Test\n-    @IR(counts = {IRNode.LOAD_VECTOR_D, \">0\",\n-                  IRNode.VECTOR_MASK_CMP_D, \">0\",\n-                  IRNode.VECTOR_BLEND_D, \">0\",\n-                  IRNode.STORE_VECTOR, \">0\"},\n-        applyIfCPUFeatureOr = {\"avx\", \"true\", \"asimd\", \"true\"})\n-    private static void testCMoveDGTforD(double[] a, double[] b, double[] c, double[] d, double[] r, double[] r2) {\n-        for (int i = 0; i < a.length; i++) {\n-            double cc = c[i];\n-            double dd = d[i];\n-            r2[i] = cc + dd;\n-            r[i] = (a[i] > b[i]) ? cc : dd;\n+        testCMoveUIGEforF(aI, bI, cF, dF, rF, rF);\n+        for (int i = 0; i < SIZE; i++) {\n+            Asserts.assertEquals(rF[i], cmoveUIGEforF(aI[i], bI[i], cF[i], dF[i]));\n@@ -781,14 +2944,3 @@\n-    }\n-    \/\/ Use some constants in the comparison\n-    @Test\n-    @IR(counts = {IRNode.LOAD_VECTOR_F, \">0\",\n-                  IRNode.VECTOR_MASK_CMP_F, \">0\",\n-                  IRNode.VECTOR_BLEND_F, \">0\",\n-                  IRNode.STORE_VECTOR, \">0\"},\n-        applyIfCPUFeatureOr = {\"avx\", \"true\", \"asimd\", \"true\"})\n-    private static void testCMoveFGTforFCmpCon1(float a, float[] b, float[] c, float[] d, float[] r, float[] r2) {\n-        for (int i = 0; i < b.length; i++) {\n-            float cc = c[i];\n-            float dd = d[i];\n-            r2[i] = cc + dd;\n-            r[i] = (a > b[i]) ? cc : dd;\n+        testCMoveUILTforF(aI, bI, cF, dF, rF, rF);\n+        for (int i = 0; i < SIZE; i++) {\n+            Asserts.assertEquals(rF[i], cmoveUILTforF(aI[i], bI[i], cF[i], dF[i]));\n@@ -797,13 +2949,3 @@\n-    }\n-    @Test\n-    @IR(counts = {IRNode.LOAD_VECTOR_F, \">0\",\n-                  IRNode.VECTOR_MASK_CMP_F, \">0\",\n-                  IRNode.VECTOR_BLEND_F, \">0\",\n-                  IRNode.STORE_VECTOR, \">0\"},\n-        applyIfCPUFeatureOr = {\"avx\", \"true\", \"asimd\", \"true\"})\n-    private static void testCMoveFGTforFCmpCon2(float[] a, float b, float[] c, float[] d, float[] r, float[] r2) {\n-        for (int i = 0; i < a.length; i++) {\n-            float cc = c[i];\n-            float dd = d[i];\n-            r2[i] = cc + dd;\n-            r[i] = (a[i] > b) ? cc : dd;\n+        testCMoveUILEforF(aI, bI, cF, dF, rF, rF);\n+        for (int i = 0; i < SIZE; i++) {\n+            Asserts.assertEquals(rF[i], cmoveUILEforF(aI[i], bI[i], cF[i], dF[i]));\n@@ -812,9 +2954,4 @@\n-    }\n-    \/\/ A case that is currently not supported and is not expected to vectorize\n-    @Test\n-    @IR(failOn = {IRNode.STORE_VECTOR})\n-    private static void testCMoveVDUnsupported() {\n-        double[] doublec = new double[SIZE];\n-        int seed = 1001;\n-        for (int i = 0; i < doublec.length; i++) {\n-            doublec[i] = (i % 2 == 0) ? seed + i : seed - i;\n+        \/\/     I for D\n+        testCMoveUIEQforD(aI, bI, cD, dD, rD, rD);\n+        for (int i = 0; i < SIZE; i++) {\n+            Asserts.assertEquals(rD[i], cmoveUIEQforD(aI[i], bI[i], cD[i], dD[i]));\n@@ -823,19 +2960,4 @@\n-    }\n-    @Warmup(0)\n-    @Run(test = {\"testCMoveVFGT\", \"testCMoveVFLT\",\"testCMoveVDLE\", \"testCMoveVDGE\", \"testCMoveVFEQ\", \"testCMoveVDNE\",\n-                 \"testCMoveVFGTSwap\", \"testCMoveVFLTSwap\",\"testCMoveVDLESwap\", \"testCMoveVDGESwap\",\n-                 \"testCMoveFGTforFConst\", \"testCMoveFGEforFConst\", \"testCMoveFLTforFConst\",\n-                 \"testCMoveFLEforFConst\", \"testCMoveFEQforFConst\", \"testCMoveFNEQforFConst\",\n-                 \"testCMoveDGTforDConst\", \"testCMoveDGEforDConst\", \"testCMoveDLTforDConst\",\n-                 \"testCMoveDLEforDConst\", \"testCMoveDEQforDConst\", \"testCMoveDNEQforDConst\",\n-                 \"testCMoveFLTforFConstH2\", \"testCMoveFLEforFConstH2\",\n-                 \"testCMoveFYYforFConstH2\", \"testCMoveFXXforFConstH2\",\n-                 \"testCMoveDLTforDConstH2\", \"testCMoveDLEforDConstH2\",\n-                 \"testCMoveDYYforDConstH2\", \"testCMoveDXXforDConstH2\"})\n-    private void testCMove_runner() {\n-        float[] floata = new float[SIZE];\n-        float[] floatb = new float[SIZE];\n-        float[] floatc = new float[SIZE];\n-        double[] doublea = new double[SIZE];\n-        double[] doubleb = new double[SIZE];\n-        double[] doublec = new double[SIZE];\n+        testCMoveUINEforD(aI, bI, cD, dD, rD, rD);\n+        for (int i = 0; i < SIZE; i++) {\n+            Asserts.assertEquals(rD[i], cmoveUINEforD(aI[i], bI[i], cD[i], dD[i]));\n+        }\n@@ -844,4 +2966,4 @@\n-        init(floata);\n-        init(floatb);\n-        init(doublea);\n-        init(doubleb);\n+        testCMoveUIGTforD(aI, bI, cD, dD, rD, rD);\n+        for (int i = 0; i < SIZE; i++) {\n+            Asserts.assertEquals(rD[i], cmoveUIGTforD(aI[i], bI[i], cD[i], dD[i]));\n+        }\n@@ -849,2 +2971,1 @@\n-        testCMoveVFGT(floata, floatb, floatc);\n-        testCMoveVDLE(doublea, doubleb, doublec);\n+        testCMoveUIGEforD(aI, bI, cD, dD, rD, rD);\n@@ -852,2 +2973,1 @@\n-            Asserts.assertEquals(floatc[i], cmoveFloatGT(floata[i], floatb[i]));\n-            Asserts.assertEquals(doublec[i], cmoveDoubleLE(doublea[i], doubleb[i]));\n+            Asserts.assertEquals(rD[i], cmoveUIGEforD(aI[i], bI[i], cD[i], dD[i]));\n@@ -856,2 +2976,1 @@\n-        testCMoveVFLT(floata, floatb, floatc);\n-        testCMoveVDGE(doublea, doubleb, doublec);\n+        testCMoveUILTforD(aI, bI, cD, dD, rD, rD);\n@@ -859,2 +2978,1 @@\n-            Asserts.assertEquals(floatc[i], cmoveFloatLT(floata[i], floatb[i]));\n-            Asserts.assertEquals(doublec[i], cmoveDoubleGE(doublea[i], doubleb[i]));\n+            Asserts.assertEquals(rD[i], cmoveUILTforD(aI[i], bI[i], cD[i], dD[i]));\n@@ -863,1 +2981,1 @@\n-        \/\/ Ensure we frequently have equals\n+        testCMoveUILEforD(aI, bI, cD, dD, rD, rD);\n@@ -865,4 +2983,1 @@\n-            if (i % 3 == 0) {\n-                floatb[i] = floata[i];\n-                doubleb[i] = doublea[i];\n-            }\n+            Asserts.assertEquals(rD[i], cmoveUILEforD(aI[i], bI[i], cD[i], dD[i]));\n@@ -871,2 +2986,2 @@\n-        testCMoveVFEQ(floata, floatb, floatc);\n-        testCMoveVDNE(doublea, doubleb, doublec);\n+        \/\/     L for I\n+        testCMoveULEQforI(aL, bL, cI, dI, rI, rI);\n@@ -874,2 +2989,1 @@\n-            Asserts.assertEquals(floatc[i], cmoveFloatEQ(floata[i], floatb[i]));\n-            Asserts.assertEquals(doublec[i], cmoveDoubleNE(doublea[i], doubleb[i]));\n+            Asserts.assertEquals(rI[i], cmoveULEQforI(aL[i], bL[i], cI[i], dI[i]));\n@@ -878,2 +2992,1 @@\n-        testCMoveVFGTSwap(floata, floatb, floatc);\n-        testCMoveVDLESwap(doublea, doubleb, doublec);\n+        testCMoveULNEforI(aL, bL, cI, dI, rI, rI);\n@@ -881,2 +2994,1 @@\n-            Asserts.assertEquals(floatc[i], cmoveFloatGTSwap(floata[i], floatb[i]));\n-            Asserts.assertEquals(doublec[i], cmoveDoubleLESwap(doublea[i], doubleb[i]));\n+            Asserts.assertEquals(rI[i], cmoveULNEforI(aL[i], bL[i], cI[i], dI[i]));\n@@ -885,2 +2997,1 @@\n-        testCMoveVFLTSwap(floata, floatb, floatc);\n-        testCMoveVDGESwap(doublea, doubleb, doublec);\n+        testCMoveULGTforI(aL, bL, cI, dI, rI, rI);\n@@ -888,2 +2999,1 @@\n-            Asserts.assertEquals(floatc[i], cmoveFloatLTSwap(floata[i], floatb[i]));\n-            Asserts.assertEquals(doublec[i], cmoveDoubleGESwap(doublea[i], doubleb[i]));\n+            Asserts.assertEquals(rI[i], cmoveULGTforI(aL[i], bL[i], cI[i], dI[i]));\n@@ -892,3 +3002,1 @@\n-        \/\/ Extensions: compare 2 values, and pick from 2 consts\n-        testCMoveFGTforFConst(floata, floatb, floatc);\n-        testCMoveDGTforDConst(doublea, doubleb, doublec);\n+        testCMoveULGEforI(aL, bL, cI, dI, rI, rI);\n@@ -896,2 +3004,1 @@\n-            Asserts.assertEquals(floatc[i], cmoveFGTforFConst(floata[i], floatb[i]));\n-            Asserts.assertEquals(doublec[i], cmoveDGTforDConst(doublea[i], doubleb[i]));\n+            Asserts.assertEquals(rI[i], cmoveULGEforI(aL[i], bL[i], cI[i], dI[i]));\n@@ -900,2 +3007,1 @@\n-        testCMoveFGEforFConst(floata, floatb, floatc);\n-        testCMoveDGEforDConst(doublea, doubleb, doublec);\n+        testCMoveULLTforI(aL, bL, cI, dI, rI, rI);\n@@ -903,2 +3009,1 @@\n-            Asserts.assertEquals(floatc[i], cmoveFGEforFConst(floata[i], floatb[i]));\n-            Asserts.assertEquals(doublec[i], cmoveDGEforDConst(doublea[i], doubleb[i]));\n+            Asserts.assertEquals(rI[i], cmoveULLTforI(aL[i], bL[i], cI[i], dI[i]));\n@@ -907,2 +3012,1 @@\n-        testCMoveFLTforFConst(floata, floatb, floatc);\n-        testCMoveDLTforDConst(doublea, doubleb, doublec);\n+        testCMoveULLEforI(aL, bL, cI, dI, rI, rI);\n@@ -910,2 +3014,1 @@\n-            Asserts.assertEquals(floatc[i], cmoveFLTforFConst(floata[i], floatb[i]));\n-            Asserts.assertEquals(doublec[i], cmoveDLTforDConst(doublea[i], doubleb[i]));\n+            Asserts.assertEquals(rI[i], cmoveULLEforI(aL[i], bL[i], cI[i], dI[i]));\n@@ -914,2 +3017,2 @@\n-        testCMoveFLEforFConst(floata, floatb, floatc);\n-        testCMoveDLEforDConst(doublea, doubleb, doublec);\n+        \/\/     L for L\n+        testCMoveULEQforL(aL, bL, cL, dL, rL, rL);\n@@ -917,2 +3020,1 @@\n-            Asserts.assertEquals(floatc[i], cmoveFLEforFConst(floata[i], floatb[i]));\n-            Asserts.assertEquals(doublec[i], cmoveDLEforDConst(doublea[i], doubleb[i]));\n+            Asserts.assertEquals(rL[i], cmoveULEQforL(aL[i], bL[i], cL[i], dL[i]));\n@@ -921,2 +3023,1 @@\n-        testCMoveFEQforFConst(floata, floatb, floatc);\n-        testCMoveDEQforDConst(doublea, doubleb, doublec);\n+        testCMoveULNEforL(aL, bL, cL, dL, rL, rL);\n@@ -924,2 +3025,1 @@\n-            Asserts.assertEquals(floatc[i], cmoveFEQforFConst(floata[i], floatb[i]));\n-            Asserts.assertEquals(doublec[i], cmoveDEQforDConst(doublea[i], doubleb[i]));\n+            Asserts.assertEquals(rL[i], cmoveULNEforL(aL[i], bL[i], cL[i], dL[i]));\n@@ -928,2 +3028,1 @@\n-        testCMoveFNEQforFConst(floata, floatb, floatc);\n-        testCMoveDNEQforDConst(doublea, doubleb, doublec);\n+        testCMoveULGTforL(aL, bL, cL, dL, rL, rL);\n@@ -931,2 +3030,1 @@\n-            Asserts.assertEquals(floatc[i], cmoveFNEQforFConst(floata[i], floatb[i]));\n-            Asserts.assertEquals(doublec[i], cmoveDNEQforDConst(doublea[i], doubleb[i]));\n+            Asserts.assertEquals(rL[i], cmoveULGTforL(aL[i], bL[i], cL[i], dL[i]));\n@@ -935,3 +3033,1 @@\n-        \/\/ Hand-unrolled (H2) examples:\n-        testCMoveFLTforFConstH2(floata, floatb, floatc);\n-        testCMoveDLTforDConstH2(doublea, doubleb, doublec);\n+        testCMoveULGEforL(aL, bL, cL, dL, rL, rL);\n@@ -939,2 +3035,1 @@\n-            Asserts.assertEquals(floatc[i], cmoveFLTforFConst(floata[i], floatb[i]));\n-            Asserts.assertEquals(doublec[i], cmoveDLTforDConst(doublea[i], doubleb[i]));\n+            Asserts.assertEquals(rL[i], cmoveULGEforL(aL[i], bL[i], cL[i], dL[i]));\n@@ -943,2 +3038,1 @@\n-        testCMoveFLEforFConstH2(floata, floatb, floatc);\n-        testCMoveDLEforDConstH2(doublea, doubleb, doublec);\n+        testCMoveULLTforL(aL, bL, cL, dL, rL, rL);\n@@ -946,2 +3040,1 @@\n-            Asserts.assertEquals(floatc[i], cmoveFLEforFConst(floata[i], floatb[i]));\n-            Asserts.assertEquals(doublec[i], cmoveDLEforDConst(doublea[i], doubleb[i]));\n+            Asserts.assertEquals(rL[i], cmoveULLTforL(aL[i], bL[i], cL[i], dL[i]));\n@@ -950,7 +3043,3 @@\n-        testCMoveFYYforFConstH2(floata, floatb, floatc);\n-        testCMoveDYYforDConstH2(doublea, doubleb, doublec);\n-        for (int i = 0; i < SIZE; i+=2) {\n-            Asserts.assertEquals(floatc[i+0], cmoveFLEforFConst(floata[i+0], floatb[i+0]));\n-            Asserts.assertEquals(doublec[i+0], cmoveDLEforDConst(doublea[i+0], doubleb[i+0]));\n-            Asserts.assertEquals(floatc[i+1], cmoveFLTforFConst(floata[i+1], floatb[i+1]));\n-            Asserts.assertEquals(doublec[i+1], cmoveDLTforDConst(doublea[i+1], doubleb[i+1]));\n+        testCMoveULLEforL(aL, bL, cL, dL, rL, rL);\n+        for (int i = 0; i < SIZE; i++) {\n+            Asserts.assertEquals(rL[i], cmoveULLEforL(aL[i], bL[i], cL[i], dL[i]));\n@@ -959,7 +3048,4 @@\n-        testCMoveFXXforFConstH2(floata, floatb, floatc);\n-        testCMoveDXXforDConstH2(doublea, doubleb, doublec);\n-        for (int i = 0; i < SIZE; i+=2) {\n-            Asserts.assertEquals(floatc[i+0], cmoveFLTforFConst(floata[i+0], floatb[i+0]));\n-            Asserts.assertEquals(doublec[i+0], cmoveDLTforDConst(doublea[i+0], doubleb[i+0]));\n-            Asserts.assertEquals(floatc[i+1], cmoveFLEforFConst(floata[i+1], floatb[i+1]));\n-            Asserts.assertEquals(doublec[i+1], cmoveDLEforDConst(doublea[i+1], doubleb[i+1]));\n+        \/\/     L for F\n+        testCMoveULEQforF(aL, bL, cF, dF, rF, rF);\n+        for (int i = 0; i < SIZE; i++) {\n+            Asserts.assertEquals(rF[i], cmoveULEQforF(aL[i], bL[i], cF[i], dF[i]));\n@@ -967,41 +3053,4 @@\n-    }\n-    @Warmup(0)\n-    @Run(test = {\"testCMoveIGTforI\",\n-                 \"testCMoveIGTforL\",\n-                 \"testCMoveIGTforF\",\n-                 \"testCMoveIGTforD\",\n-                 \"testCMoveLGTforI\",\n-                 \"testCMoveLGTforL\",\n-                 \"testCMoveLGTforF\",\n-                 \"testCMoveLGTforD\",\n-                 \"testCMoveFGTforI\",\n-                 \"testCMoveFGTforL\",\n-                 \"testCMoveFGTforF\",\n-                 \"testCMoveFGTforD\",\n-                 \"testCMoveDGTforI\",\n-                 \"testCMoveDGTforL\",\n-                 \"testCMoveDGTforF\",\n-                 \"testCMoveDGTforD\",\n-                 \"testCMoveFGTforFCmpCon1\",\n-                 \"testCMoveFGTforFCmpCon2\"})\n-    private void testCMove_runner_two() {\n-        int[] aI = new int[SIZE];\n-        int[] bI = new int[SIZE];\n-        int[] cI = new int[SIZE];\n-        int[] dI = new int[SIZE];\n-        int[] rI = new int[SIZE];\n-        long[] aL = new long[SIZE];\n-        long[] bL = new long[SIZE];\n-        long[] cL = new long[SIZE];\n-        long[] dL = new long[SIZE];\n-        long[] rL = new long[SIZE];\n-        float[] aF = new float[SIZE];\n-        float[] bF = new float[SIZE];\n-        float[] cF = new float[SIZE];\n-        float[] dF = new float[SIZE];\n-        float[] rF = new float[SIZE];\n-        double[] aD = new double[SIZE];\n-        double[] bD = new double[SIZE];\n-        double[] cD = new double[SIZE];\n-        double[] dD = new double[SIZE];\n-        double[] rD = new double[SIZE];\n+        testCMoveULNEforF(aL, bL, cF, dF, rF, rF);\n+        for (int i = 0; i < SIZE; i++) {\n+            Asserts.assertEquals(rF[i], cmoveULNEforF(aL[i], bL[i], cF[i], dF[i]));\n+        }\n@@ -1010,16 +3059,4 @@\n-        init(aI);\n-        init(bI);\n-        init(cI);\n-        init(dI);\n-        init(aL);\n-        init(bL);\n-        init(cL);\n-        init(dL);\n-        init(aF);\n-        init(bF);\n-        init(cF);\n-        init(dF);\n-        init(aD);\n-        init(bD);\n-        init(cD);\n-        init(dD);\n+        testCMoveULGTforF(aL, bL, cF, dF, rF, rF);\n+        for (int i = 0; i < SIZE; i++) {\n+            Asserts.assertEquals(rF[i], cmoveULGTforF(aL[i], bL[i], cF[i], dF[i]));\n+        }\n@@ -1027,1 +3064,1 @@\n-        testCMoveIGTforI(aI, bI, cI, dI, rI, rI);\n+        testCMoveULGEforF(aL, bL, cF, dF, rF, rF);\n@@ -1029,1 +3066,1 @@\n-            Asserts.assertEquals(rI[i], cmoveIGTforI(aI[i], bI[i], cI[i], dI[i]));\n+            Asserts.assertEquals(rF[i], cmoveULGEforF(aL[i], bL[i], cF[i], dF[i]));\n@@ -1032,1 +3069,1 @@\n-        testCMoveIGTforL(aI, bI, cL, dL, rL, rL);\n+        testCMoveULLTforF(aL, bL, cF, dF, rF, rF);\n@@ -1034,1 +3071,1 @@\n-            Asserts.assertEquals(rL[i], cmoveIGTforL(aI[i], bI[i], cL[i], dL[i]));\n+            Asserts.assertEquals(rF[i], cmoveULLTforF(aL[i], bL[i], cF[i], dF[i]));\n@@ -1037,1 +3074,1 @@\n-        testCMoveIGTforF(aI, bI, cF, dF, rF, rF);\n+        testCMoveULLEforF(aL, bL, cF, dF, rF, rF);\n@@ -1039,1 +3076,1 @@\n-            Asserts.assertEquals(rF[i], cmoveIGTforF(aI[i], bI[i], cF[i], dF[i]));\n+            Asserts.assertEquals(rF[i], cmoveULLEforF(aL[i], bL[i], cF[i], dF[i]));\n@@ -1042,1 +3079,2 @@\n-        testCMoveIGTforD(aI, bI, cD, dD, rD, rD);\n+        \/\/     L for D\n+        testCMoveULEQforD(aL, bL, cD, dD, rD, rD);\n@@ -1044,1 +3082,1 @@\n-            Asserts.assertEquals(rD[i], cmoveIGTforD(aI[i], bI[i], cD[i], dD[i]));\n+            Asserts.assertEquals(rD[i], cmoveULEQforD(aL[i], bL[i], cD[i], dD[i]));\n@@ -1047,1 +3085,1 @@\n-        testCMoveLGTforI(aL, bL, cI, dI, rI, rI);\n+        testCMoveULNEforD(aL, bL, cD, dD, rD, rD);\n@@ -1049,1 +3087,1 @@\n-            Asserts.assertEquals(rI[i], cmoveLGTforI(aL[i], bL[i], cI[i], dI[i]));\n+            Asserts.assertEquals(rD[i], cmoveULNEforD(aL[i], bL[i], cD[i], dD[i]));\n@@ -1052,1 +3090,1 @@\n-        testCMoveLGTforL(aL, bL, cL, dL, rL, rL);\n+        testCMoveULGTforD(aL, bL, cD, dD, rD, rD);\n@@ -1054,1 +3092,1 @@\n-            Asserts.assertEquals(rL[i], cmoveLGTforL(aL[i], bL[i], cL[i], dL[i]));\n+            Asserts.assertEquals(rD[i], cmoveULGTforD(aL[i], bL[i], cD[i], dD[i]));\n@@ -1057,1 +3095,1 @@\n-        testCMoveLGTforF(aL, bL, cF, dF, rF, rF);\n+        testCMoveULGEforD(aL, bL, cD, dD, rD, rD);\n@@ -1059,1 +3097,1 @@\n-            Asserts.assertEquals(rF[i], cmoveLGTforF(aL[i], bL[i], cF[i], dF[i]));\n+            Asserts.assertEquals(rD[i], cmoveULGEforD(aL[i], bL[i], cD[i], dD[i]));\n@@ -1062,1 +3100,1 @@\n-        testCMoveLGTforD(aL, bL, cD, dD, rD, rD);\n+        testCMoveULLTforD(aL, bL, cD, dD, rD, rD);\n@@ -1064,1 +3102,6 @@\n-            Asserts.assertEquals(rD[i], cmoveLGTforD(aL[i], bL[i], cD[i], dD[i]));\n+            Asserts.assertEquals(rD[i], cmoveULLTforD(aL[i], bL[i], cD[i], dD[i]));\n+        }\n+\n+        testCMoveULLEforD(aL, bL, cD, dD, rD, rD);\n+        for (int i = 0; i < SIZE; i++) {\n+            Asserts.assertEquals(rD[i], cmoveULLEforD(aL[i], bL[i], cD[i], dD[i]));\n@@ -1067,0 +3110,1 @@\n+        \/\/ Float\n","filename":"test\/hotspot\/jtreg\/compiler\/c2\/irTests\/TestVectorConditionalMove.java","additions":2338,"deletions":294,"binary":false,"changes":2632,"status":"modified"},{"patch":"@@ -34,0 +34,1 @@\n+import java.util.List;\n@@ -53,13 +54,18 @@\n-        \/\/ Cross-product: +-AlignVector and +-UseCompactObjectHeaders\n-        TestFramework.runWithFlags(\"--add-modules\", \"java.base\", \"--add-exports\", \"java.base\/jdk.internal.misc=ALL-UNNAMED\",\n-                                   \"-XX:-UseCompactObjectHeaders\",\n-                                   \"-XX:-AlignVector\");\n-        TestFramework.runWithFlags(\"--add-modules\", \"java.base\", \"--add-exports\", \"java.base\/jdk.internal.misc=ALL-UNNAMED\",\n-                                   \"-XX:-UseCompactObjectHeaders\",\n-                                   \"-XX:+AlignVector\");\n-        TestFramework.runWithFlags(\"--add-modules\", \"java.base\", \"--add-exports\", \"java.base\/jdk.internal.misc=ALL-UNNAMED\",\n-                                   \"-XX:+UseCompactObjectHeaders\",\n-                                   \"-XX:-AlignVector\");\n-        TestFramework.runWithFlags(\"--add-modules\", \"java.base\", \"--add-exports\", \"java.base\/jdk.internal.misc=ALL-UNNAMED\",\n-                                   \"-XX:+UseCompactObjectHeaders\",\n-                                   \"-XX:+AlignVector\");\n+        TestFramework framework = new TestFramework();\n+        framework.addFlags(\"--add-modules\", \"java.base\", \"--add-exports\", \"java.base\/jdk.internal.misc=ALL-UNNAMED\",\n+                           \"-XX:+UnlockExperimentalVMOptions\");\n+\n+        \/\/ Cross-product:\n+        \/\/   +-AlignVector\n+        \/\/   +-UseCompactObjectHeaders\n+        \/\/   +-UseAutoVectorizationSpeculativeAliasingChecks\n+        int idx = 0;\n+        for (String av : List.of(\"-XX:-AlignVector\", \"-XX:+AlignVector\")) {\n+            for (String coh : List.of(\"-XX:-UseCompactObjectHeaders\", \"-XX:+UseCompactObjectHeaders\")) {\n+                for (String sac : List.of(\"-XX:-UseAutoVectorizationSpeculativeAliasingChecks\", \"-XX:+UseAutoVectorizationSpeculativeAliasingChecks\")) {\n+                    framework.addScenarios(new Scenario(idx++, av, coh, sac));\n+                }\n+            }\n+        }\n+\n+        framework.start();\n@@ -129,1 +135,1 @@\n-            int val = offset > 0 ? verifyByteArray[(i-offset) % 8] : verifyByteArray[i-offset];\n+            int val = offset >=1 ? verifyByteArray[(i-offset) % 8] : verifyByteArray[i-offset];\n@@ -400,1 +406,8 @@\n-    @IR(failOn = { IRNode.LOAD_VECTOR_L, IRNode.STORE_VECTOR })\n+    @IR(failOn = { IRNode.LOAD_VECTOR_L, IRNode.STORE_VECTOR },\n+        applyIf = {\"UseAutoVectorizationSpeculativeAliasingChecks\", \"false\"})\n+    @IR(counts = { IRNode.LOAD_VECTOR_L, \">=1\", IRNode.STORE_VECTOR, \">=1\", \".*multiversion.*\", \">=1\"},\n+        phase = CompilePhase.PRINT_IDEAL,\n+        applyIfCPUFeatureOr = {\"sse2\", \"true\", \"asimd\", \"true\", \"rvv\", \"true\"},\n+        applyIfAnd = {\"UseAutoVectorizationSpeculativeAliasingChecks\", \"true\", \"AlignVector\", \"false\"},\n+        applyIfPlatform = {\"64-bit\", \"true\"})\n+    \/\/ We have unknown aliasing. At runtime \"dest == src\", so the AutoVectorization Predicate fails, and recompiles with Multiversioning.\n@@ -408,1 +421,8 @@\n-    @IR(failOn = { IRNode.LOAD_VECTOR_L, IRNode.STORE_VECTOR })\n+    @IR(failOn = { IRNode.LOAD_VECTOR_L, IRNode.STORE_VECTOR },\n+        applyIf = {\"UseAutoVectorizationSpeculativeAliasingChecks\", \"false\"})\n+    @IR(counts = { IRNode.LOAD_VECTOR_L, \">=1\", IRNode.STORE_VECTOR, \">=1\", \".*multiversion.*\", \">=1\"},\n+        phase = CompilePhase.PRINT_IDEAL,\n+        applyIfCPUFeatureOr = {\"sse2\", \"true\", \"asimd\", \"true\", \"rvv\", \"true\"},\n+        applyIfAnd = {\"UseAutoVectorizationSpeculativeAliasingChecks\", \"true\", \"AlignVector\", \"false\"},\n+        applyIfPlatform = {\"64-bit\", \"true\"})\n+    \/\/ We have unknown aliasing. At runtime \"dest == src\", so the AutoVectorization Predicate fails, and recompiles with Multiversioning.\n@@ -422,1 +442,8 @@\n-    @IR(failOn = { IRNode.LOAD_VECTOR_L, IRNode.STORE_VECTOR })\n+    @IR(failOn = { IRNode.LOAD_VECTOR_L, IRNode.STORE_VECTOR },\n+        applyIf = {\"UseAutoVectorizationSpeculativeAliasingChecks\", \"false\"})\n+    @IR(counts = { IRNode.LOAD_VECTOR_L, \">=1\", IRNode.STORE_VECTOR, \">=1\", \".*multiversion.*\", \">=1\"},\n+        phase = CompilePhase.PRINT_IDEAL,\n+        applyIfCPUFeatureOr = {\"sse2\", \"true\", \"asimd\", \"true\", \"rvv\", \"true\"},\n+        applyIfAnd = {\"UseAutoVectorizationSpeculativeAliasingChecks\", \"true\", \"AlignVector\", \"false\"},\n+        applyIfPlatform = {\"64-bit\", \"true\"})\n+    \/\/ We have unknown aliasing. At runtime \"dest == src\", so the AutoVectorization Predicate fails, and recompiles with Multiversioning.\n@@ -430,1 +457,8 @@\n-    @IR(failOn = { IRNode.LOAD_VECTOR_L, IRNode.STORE_VECTOR })\n+    @IR(failOn = { IRNode.LOAD_VECTOR_L, IRNode.STORE_VECTOR },\n+        applyIf = {\"UseAutoVectorizationSpeculativeAliasingChecks\", \"false\"})\n+    @IR(counts = { IRNode.LOAD_VECTOR_L, \">=1\", IRNode.STORE_VECTOR, \">=1\", \".*multiversion.*\", \">=1\"},\n+        phase = CompilePhase.PRINT_IDEAL,\n+        applyIfCPUFeatureOr = {\"sse2\", \"true\", \"asimd\", \"true\", \"rvv\", \"true\"},\n+        applyIfAnd = {\"UseAutoVectorizationSpeculativeAliasingChecks\", \"true\", \"AlignVector\", \"false\"},\n+        applyIfPlatform = {\"64-bit\", \"true\"})\n+    \/\/ We have unknown aliasing. At runtime \"dest == src\", so the AutoVectorization Predicate fails, and recompiles with Multiversioning.\n@@ -445,1 +479,8 @@\n-    @IR(failOn = { IRNode.LOAD_VECTOR_L, IRNode.STORE_VECTOR })\n+    @IR(failOn = { IRNode.LOAD_VECTOR_L, IRNode.STORE_VECTOR },\n+        applyIf = {\"UseAutoVectorizationSpeculativeAliasingChecks\", \"false\"})\n+    @IR(counts = { IRNode.LOAD_VECTOR_L, \">=1\", IRNode.STORE_VECTOR, \">=1\", \".*multiversion.*\", \">=1\"},\n+        phase = CompilePhase.PRINT_IDEAL,\n+        applyIfCPUFeatureOr = {\"sse2\", \"true\", \"asimd\", \"true\", \"rvv\", \"true\"},\n+        applyIfAnd = {\"UseAutoVectorizationSpeculativeAliasingChecks\", \"true\", \"AlignVector\", \"false\"},\n+        applyIfPlatform = {\"64-bit\", \"true\"})\n+    \/\/ We have unknown aliasing. At runtime \"dest == src\", so the AutoVectorization Predicate fails, and recompiles with Multiversioning.\n@@ -453,1 +494,8 @@\n-    @IR(failOn = { IRNode.LOAD_VECTOR_L, IRNode.STORE_VECTOR })\n+    @IR(failOn = { IRNode.LOAD_VECTOR_L, IRNode.STORE_VECTOR },\n+        applyIf = {\"UseAutoVectorizationSpeculativeAliasingChecks\", \"false\"})\n+    @IR(counts = { IRNode.LOAD_VECTOR_L, \">=1\", IRNode.STORE_VECTOR, \">=1\", \".*multiversion.*\", \">=1\"},\n+        phase = CompilePhase.PRINT_IDEAL,\n+        applyIfCPUFeatureOr = {\"sse2\", \"true\", \"asimd\", \"true\", \"rvv\", \"true\"},\n+        applyIfAnd = {\"UseAutoVectorizationSpeculativeAliasingChecks\", \"true\", \"AlignVector\", \"false\"},\n+        applyIfPlatform = {\"64-bit\", \"true\"})\n+    \/\/ We have unknown aliasing. At runtime \"dest == src\", so the AutoVectorization Predicate fails, and recompiles with Multiversioning.\n","filename":"test\/hotspot\/jtreg\/compiler\/c2\/irTests\/TestVectorizationMismatchedAccess.java","additions":68,"deletions":20,"binary":false,"changes":88,"status":"modified"},{"patch":"@@ -1014,1 +1014,1 @@\n-        applyIfCPUFeatureOr = {\"avx2\", \"true\", \"rvv\", \"true\"})\n+        applyIfCPUFeature = {\"avx2\", \"true\"})\n@@ -1016,0 +1016,8 @@\n+    @IR(counts = {IRNode.LOAD_VECTOR_I, IRNode.VECTOR_SIZE + \"min(max_int, max_long)\", \"> 0\",\n+                  IRNode.LOAD_VECTOR_L, IRNode.VECTOR_SIZE + \"min(max_int, max_long)\", \"> 0\",\n+                  IRNode.ADD_VI, IRNode.VECTOR_SIZE + \"min(max_int, max_long)\", \"> 0\",\n+                  IRNode.ADD_VL, IRNode.VECTOR_SIZE + \"min(max_int, max_long)\", \"> 0\",\n+                  IRNode.STORE_VECTOR, \"> 0\"},\n+        applyIfPlatform = {\"riscv64\", \"true\"},\n+        applyIfCPUFeature = {\"rvv\", \"true\"},\n+        applyIf = {\"MaxVectorSize\", \">=32\"})\n@@ -1085,1 +1093,1 @@\n-        applyIfCPUFeatureOr = {\"avx2\", \"true\", \"rvv\", \"true\"})\n+        applyIfCPUFeature = {\"avx2\", \"true\"})\n@@ -1087,0 +1095,8 @@\n+    @IR(counts = {IRNode.LOAD_VECTOR_I, IRNode.VECTOR_SIZE + \"min(max_int, max_long)\", \"> 0\",\n+                  IRNode.LOAD_VECTOR_L, IRNode.VECTOR_SIZE + \"min(max_int, max_long)\", \"> 0\",\n+                  IRNode.ADD_VI, IRNode.VECTOR_SIZE + \"min(max_int, max_long)\", \"> 0\",\n+                  IRNode.ADD_VL, IRNode.VECTOR_SIZE + \"min(max_int, max_long)\", \"> 0\",\n+                  IRNode.STORE_VECTOR, \"> 0\"},\n+        applyIfPlatform = {\"riscv64\", \"true\"},\n+        applyIfCPUFeature = {\"rvv\", \"true\"},\n+        applyIf = {\"MaxVectorSize\", \">=32\"})\n","filename":"test\/hotspot\/jtreg\/compiler\/loopopts\/superword\/TestAlignVector.java","additions":18,"deletions":2,"binary":false,"changes":20,"status":"modified"},{"patch":"@@ -40,4 +40,8 @@\n- * @run driver compiler.loopopts.superword.TestSplitPacks nCOH_nAV\n- * @run driver compiler.loopopts.superword.TestSplitPacks nCOH_yAV\n- * @run driver compiler.loopopts.superword.TestSplitPacks yCOH_nAV\n- * @run driver compiler.loopopts.superword.TestSplitPacks yCOH_yAV\n+ * @run driver compiler.loopopts.superword.TestSplitPacks nCOH_nAV_ySAC\n+ * @run driver compiler.loopopts.superword.TestSplitPacks nCOH_yAV_ySAC\n+ * @run driver compiler.loopopts.superword.TestSplitPacks yCOH_nAV_ySAC\n+ * @run driver compiler.loopopts.superword.TestSplitPacks yCOH_yAV_ySAC\n+ * @run driver compiler.loopopts.superword.TestSplitPacks nCOH_nAV_nSAC\n+ * @run driver compiler.loopopts.superword.TestSplitPacks nCOH_yAV_nSAC\n+ * @run driver compiler.loopopts.superword.TestSplitPacks yCOH_nAV_nSAC\n+ * @run driver compiler.loopopts.superword.TestSplitPacks yCOH_yAV_nSAC\n@@ -79,4 +83,8 @@\n-            case \"nCOH_nAV\" -> { framework.addFlags(\"-XX:-UseCompactObjectHeaders\", \"-XX:-AlignVector\"); }\n-            case \"nCOH_yAV\" -> { framework.addFlags(\"-XX:-UseCompactObjectHeaders\", \"-XX:+AlignVector\"); }\n-            case \"yCOH_nAV\" -> { framework.addFlags(\"-XX:+UseCompactObjectHeaders\", \"-XX:-AlignVector\"); }\n-            case \"yCOH_yAV\" -> { framework.addFlags(\"-XX:+UseCompactObjectHeaders\", \"-XX:+AlignVector\"); }\n+            case \"nCOH_nAV_ySAC\" -> { framework.addFlags(\"-XX:+UnlockExperimentalVMOptions\", \"-XX:-UseCompactObjectHeaders\", \"-XX:-AlignVector\", \"-XX:+UseAutoVectorizationSpeculativeAliasingChecks\"); }\n+            case \"nCOH_yAV_ySAC\" -> { framework.addFlags(\"-XX:+UnlockExperimentalVMOptions\", \"-XX:-UseCompactObjectHeaders\", \"-XX:+AlignVector\", \"-XX:+UseAutoVectorizationSpeculativeAliasingChecks\"); }\n+            case \"yCOH_nAV_ySAC\" -> { framework.addFlags(\"-XX:+UnlockExperimentalVMOptions\", \"-XX:+UseCompactObjectHeaders\", \"-XX:-AlignVector\", \"-XX:+UseAutoVectorizationSpeculativeAliasingChecks\"); }\n+            case \"yCOH_yAV_ySAC\" -> { framework.addFlags(\"-XX:+UnlockExperimentalVMOptions\", \"-XX:+UseCompactObjectHeaders\", \"-XX:+AlignVector\", \"-XX:+UseAutoVectorizationSpeculativeAliasingChecks\"); }\n+            case \"nCOH_nAV_nSAC\" -> { framework.addFlags(\"-XX:+UnlockExperimentalVMOptions\", \"-XX:-UseCompactObjectHeaders\", \"-XX:-AlignVector\", \"-XX:-UseAutoVectorizationSpeculativeAliasingChecks\"); }\n+            case \"nCOH_yAV_nSAC\" -> { framework.addFlags(\"-XX:+UnlockExperimentalVMOptions\", \"-XX:-UseCompactObjectHeaders\", \"-XX:+AlignVector\", \"-XX:-UseAutoVectorizationSpeculativeAliasingChecks\"); }\n+            case \"yCOH_nAV_nSAC\" -> { framework.addFlags(\"-XX:+UnlockExperimentalVMOptions\", \"-XX:+UseCompactObjectHeaders\", \"-XX:-AlignVector\", \"-XX:-UseAutoVectorizationSpeculativeAliasingChecks\"); }\n+            case \"yCOH_yAV_nSAC\" -> { framework.addFlags(\"-XX:+UnlockExperimentalVMOptions\", \"-XX:+UseCompactObjectHeaders\", \"-XX:+AlignVector\", \"-XX:-UseAutoVectorizationSpeculativeAliasingChecks\"); }\n@@ -117,0 +125,7 @@\n+        tests.put(\"test4a_alias\",() -> { short[] x = aS.clone(); return test4a_alias(x, x); });\n+        tests.put(\"test4b_alias\",() -> { short[] x = aS.clone(); return test4b_alias(x, x); });\n+        tests.put(\"test4c_alias\",() -> { short[] x = aS.clone(); return test4c_alias(x, x); });\n+        tests.put(\"test4d_alias\",() -> { short[] x = aS.clone(); return test4d_alias(x, x); });\n+        tests.put(\"test4e_alias\",() -> { short[] x = aS.clone(); return test4e_alias(x, x); });\n+        tests.put(\"test4f_alias\",() -> { short[] x = aS.clone(); return test4f_alias(x, x); });\n+        tests.put(\"test4g_alias\",() -> { short[] x = aS.clone(); return test4g_alias(x, x); });\n@@ -148,0 +163,7 @@\n+                 \"test4a_alias\",\n+                 \"test4b_alias\",\n+                 \"test4c_alias\",\n+                 \"test4d_alias\",\n+                 \"test4e_alias\",\n+                 \"test4f_alias\",\n+                 \"test4g_alias\",\n@@ -596,1 +618,4 @@\n-                  IRNode.STORE_VECTOR, \"> 0\"},\n+                  IRNode.STORE_VECTOR, \"> 0\",\n+                  \".*multiversion.*\", \"= 0\"},\n+        phase = CompilePhase.PRINT_IDEAL,\n+        applyIf = {\"UseAutoVectorizationSpeculativeAliasingChecks\", \"false\"},\n@@ -600,0 +625,8 @@\n+    @IR(counts = {IRNode.LOAD_VECTOR_S, \"> 0\",\n+                  IRNode.STORE_VECTOR, \"> 0\",\n+                  \".*multiversion.*\", \"= 0\"},\n+        phase = CompilePhase.PRINT_IDEAL,\n+        applyIfAnd = {\"UseAutoVectorizationSpeculativeAliasingChecks\", \"true\", \"AlignVector\", \"false\"},\n+        applyIfPlatform = {\"64-bit\", \"true\"},\n+        applyIfCPUFeatureOr = {\"sse4.1\", \"true\"})\n+    \/\/ Speculative aliasing check -> full vectorization.\n@@ -609,2 +642,4 @@\n-                  IRNode.STORE_VECTOR, \"> 0\"},\n-        applyIf = {\"AlignVector\", \"false\"},\n+                  IRNode.STORE_VECTOR, \"> 0\",\n+                  \".*multiversion.*\", \"= 0\"},\n+        phase = CompilePhase.PRINT_IDEAL,\n+        applyIfAnd = {\"AlignVector\", \"false\", \"UseAutoVectorizationSpeculativeAliasingChecks\", \"false\"},\n@@ -614,0 +649,8 @@\n+    @IR(counts = {IRNode.LOAD_VECTOR_S, \"> 0\",\n+                  IRNode.STORE_VECTOR, \"> 0\",\n+                  \".*multiversion.*\", \"= 0\"},\n+        phase = CompilePhase.PRINT_IDEAL,\n+        applyIfAnd = {\"AlignVector\", \"false\", \"UseAutoVectorizationSpeculativeAliasingChecks\", \"true\"},\n+        applyIfPlatform = {\"64-bit\", \"true\"},\n+        applyIfCPUFeatureOr = {\"sse4.1\", \"true\"})\n+    \/\/ Speculative aliasing check -> full vectorization.\n@@ -623,2 +666,4 @@\n-                  IRNode.STORE_VECTOR, \"> 0\"},\n-        applyIf = {\"MaxVectorSize\", \">=8\"},\n+                  IRNode.STORE_VECTOR, \"> 0\",\n+                  \".*multiversion.*\", \"= 0\"},\n+        phase = CompilePhase.PRINT_IDEAL,\n+        applyIfAnd = {\"MaxVectorSize\", \">=8\", \"UseAutoVectorizationSpeculativeAliasingChecks\", \"false\"},\n@@ -628,0 +673,8 @@\n+    @IR(counts = {IRNode.LOAD_VECTOR_S, \"> 0\",\n+                  IRNode.STORE_VECTOR, \"> 0\",\n+                  \".*multiversion.*\", \"= 0\"},\n+        phase = CompilePhase.PRINT_IDEAL,\n+        applyIfAnd = {\"MaxVectorSize\", \">=8\", \"UseAutoVectorizationSpeculativeAliasingChecks\", \"true\"},\n+        applyIfPlatform = {\"64-bit\", \"true\"},\n+        applyIfCPUFeatureOr = {\"sse4.1\", \"true\", \"asimd\", \"true\", \"rvv\", \"true\"})\n+    \/\/ Speculative aliasing check -> full vectorization.\n@@ -637,2 +690,4 @@\n-                  IRNode.STORE_VECTOR, \"> 0\"},\n-        applyIfAnd = {\"MaxVectorSize\", \">=8\", \"AlignVector\", \"false\"},\n+                  IRNode.STORE_VECTOR, \"> 0\",\n+                  \".*multiversion.*\", \"= 0\"},\n+        phase = CompilePhase.PRINT_IDEAL,\n+        applyIfAnd = {\"MaxVectorSize\", \">=8\", \"AlignVector\", \"false\", \"UseAutoVectorizationSpeculativeAliasingChecks\", \"false\"},\n@@ -642,0 +697,8 @@\n+    @IR(counts = {IRNode.LOAD_VECTOR_S, \"> 0\",\n+                  IRNode.STORE_VECTOR, \"> 0\",\n+                  \".*multiversion.*\", \"= 0\"},\n+        phase = CompilePhase.PRINT_IDEAL,\n+        applyIfAnd = {\"MaxVectorSize\", \">=8\", \"AlignVector\", \"false\", \"UseAutoVectorizationSpeculativeAliasingChecks\", \"true\"},\n+        applyIfPlatform = {\"64-bit\", \"true\"},\n+        applyIfCPUFeatureOr = {\"sse4.1\", \"true\", \"asimd\", \"true\", \"rvv\", \"true\"})\n+    \/\/ Speculative aliasing check -> full vectorization.\n@@ -651,2 +714,4 @@\n-                  IRNode.STORE_VECTOR, \"> 0\"},\n-        applyIfAnd = {\"MaxVectorSize\", \">=8\", \"AlignVector\", \"false\"},\n+                  IRNode.STORE_VECTOR, \"> 0\",\n+                  \".*multiversion.*\", \"= 0\"},\n+        phase = CompilePhase.PRINT_IDEAL,\n+        applyIfAnd = {\"MaxVectorSize\", \">=8\", \"AlignVector\", \"false\", \"UseAutoVectorizationSpeculativeAliasingChecks\", \"false\"},\n@@ -656,0 +721,8 @@\n+    @IR(counts = {IRNode.LOAD_VECTOR_S, \"> 0\",\n+                  IRNode.STORE_VECTOR, \"> 0\",\n+                  \".*multiversion.*\", \"= 0\"},\n+        phase = CompilePhase.PRINT_IDEAL,\n+        applyIfAnd = {\"MaxVectorSize\", \">=8\", \"AlignVector\", \"false\", \"UseAutoVectorizationSpeculativeAliasingChecks\", \"true\"},\n+        applyIfPlatform = {\"64-bit\", \"true\"},\n+        applyIfCPUFeatureOr = {\"sse4.1\", \"true\", \"asimd\", \"true\", \"rvv\", \"true\"})\n+    \/\/ Speculative aliasing check -> full vectorization.\n@@ -665,2 +738,4 @@\n-                  IRNode.STORE_VECTOR, \"> 0\"},\n-        applyIfAnd = {\"MaxVectorSize\", \">=8\", \"AlignVector\", \"false\"},\n+                  IRNode.STORE_VECTOR, \"> 0\",\n+                  \".*multiversion.*\", \"= 0\"},\n+        phase = CompilePhase.PRINT_IDEAL,\n+        applyIfAnd = {\"MaxVectorSize\", \">=8\", \"AlignVector\", \"false\", \"UseAutoVectorizationSpeculativeAliasingChecks\", \"false\"},\n@@ -670,0 +745,8 @@\n+    @IR(counts = {IRNode.LOAD_VECTOR_S, \"> 0\",\n+                  IRNode.STORE_VECTOR, \"> 0\",\n+                  \".*multiversion.*\", \"= 0\"},\n+        phase = CompilePhase.PRINT_IDEAL,\n+        applyIfAnd = {\"MaxVectorSize\", \">=8\", \"AlignVector\", \"false\", \"UseAutoVectorizationSpeculativeAliasingChecks\", \"true\"},\n+        applyIfPlatform = {\"64-bit\", \"true\"},\n+        applyIfCPUFeatureOr = {\"sse4.1\", \"true\", \"asimd\", \"true\", \"rvv\", \"true\"})\n+    \/\/ Speculative aliasing check -> full vectorization.\n@@ -679,2 +762,4 @@\n-                  IRNode.STORE_VECTOR, \"> 0\"},\n-        applyIf = {\"MaxVectorSize\", \">=32\"},\n+                  IRNode.STORE_VECTOR, \"> 0\",\n+                  \".*multiversion.*\", \"= 0\"},\n+        phase = CompilePhase.PRINT_IDEAL,\n+        applyIfAnd = {\"MaxVectorSize\", \">=32\", \"UseAutoVectorizationSpeculativeAliasingChecks\", \"false\"},\n@@ -684,0 +769,8 @@\n+    @IR(counts = {IRNode.LOAD_VECTOR_S, \"> 0\",\n+                  IRNode.STORE_VECTOR, \"> 0\",\n+                  \".*multiversion.*\", \"= 0\"},\n+        phase = CompilePhase.PRINT_IDEAL,\n+        applyIfAnd = {\"MaxVectorSize\", \">=32\", \"UseAutoVectorizationSpeculativeAliasingChecks\", \"true\"},\n+        applyIfPlatform = {\"64-bit\", \"true\"},\n+        applyIfCPUFeatureOr = {\"sse4.1\", \"true\", \"asimd\", \"true\", \"rvv\", \"true\"})\n+    \/\/ Speculative aliasing check -> full vectorization.\n@@ -691,0 +784,175 @@\n+    @Test\n+    @IR(counts = {IRNode.LOAD_VECTOR_S, IRNode.VECTOR_SIZE_2, \"> 0\",\n+                  IRNode.STORE_VECTOR, \"> 0\",\n+                  \".*multiversion.*\", \"= 0\"},\n+        phase = CompilePhase.PRINT_IDEAL,\n+        applyIf = {\"UseAutoVectorizationSpeculativeAliasingChecks\", \"false\"},\n+        applyIfPlatform = {\"64-bit\", \"true\"},\n+        applyIfCPUFeatureOr = {\"sse4.1\", \"true\"})\n+    \/\/ Cyclic dependency with distance 2 -> split into 2-packs\n+    @IR(counts = {IRNode.LOAD_VECTOR_S, \"> 0\",\n+                  IRNode.LOAD_VECTOR_S, IRNode.VECTOR_SIZE_2, \"> 0\",\n+                  IRNode.STORE_VECTOR, \"> 0\",\n+                  \".*multiversion.*\", \"> 0\"},\n+        phase = CompilePhase.PRINT_IDEAL,\n+        applyIfAnd = {\"UseAutoVectorizationSpeculativeAliasingChecks\", \"true\", \"AlignVector\", \"false\"},\n+        applyIfPlatform = {\"64-bit\", \"true\"},\n+        applyIfCPUFeatureOr = {\"sse4.1\", \"true\"})\n+    \/\/ Speculative aliasing check with multiversioning -> full vectorization & split packs.\n+    static Object[] test4a_alias(short[] a, short[] b) {\n+        for (int i = 0; i < RANGE-64; i++) {\n+          b[i+2] = a[i+0];\n+        }\n+        return new Object[]{ a, b };\n+    }\n+\n+    @Test\n+    @IR(counts = {IRNode.LOAD_VECTOR_S, IRNode.VECTOR_SIZE_2, \"> 0\",\n+                  IRNode.STORE_VECTOR, \"> 0\",\n+                  \".*multiversion.*\", \"= 0\"},\n+        phase = CompilePhase.PRINT_IDEAL,\n+        applyIfAnd = {\"AlignVector\", \"false\", \"UseAutoVectorizationSpeculativeAliasingChecks\", \"false\"},\n+        applyIfPlatform = {\"64-bit\", \"true\"},\n+        applyIfCPUFeatureOr = {\"sse4.1\", \"true\"})\n+    \/\/ Cyclic dependency with distance 3 -> split into 2-packs\n+    @IR(counts = {IRNode.LOAD_VECTOR_S, \"> 0\",\n+                  IRNode.LOAD_VECTOR_S, IRNode.VECTOR_SIZE_2, \"> 0\",\n+                  IRNode.STORE_VECTOR, \"> 0\",\n+                  \".*multiversion.*\", \"> 0\"},\n+        phase = CompilePhase.PRINT_IDEAL,\n+        applyIfAnd = {\"AlignVector\", \"false\", \"UseAutoVectorizationSpeculativeAliasingChecks\", \"true\"},\n+        applyIfPlatform = {\"64-bit\", \"true\"},\n+        applyIfCPUFeatureOr = {\"sse4.1\", \"true\"})\n+    \/\/ Speculative aliasing check with multiversioning -> full vectorization & split packs.\n+    static Object[] test4b_alias(short[] a, short[] b) {\n+        for (int i = 0; i < RANGE-64; i++) {\n+          b[i+3] = a[i+0];\n+        }\n+        return new Object[]{ a, b };\n+    }\n+\n+    @Test\n+    @IR(counts = {IRNode.LOAD_VECTOR_S, IRNode.VECTOR_SIZE_4, \"> 0\",\n+                  IRNode.STORE_VECTOR, \"> 0\",\n+                  \".*multiversion.*\", \"= 0\"},\n+        phase = CompilePhase.PRINT_IDEAL,\n+        applyIfAnd = {\"MaxVectorSize\", \">=8\", \"UseAutoVectorizationSpeculativeAliasingChecks\", \"false\"},\n+        applyIfPlatform = {\"64-bit\", \"true\"},\n+        applyIfCPUFeatureOr = {\"sse4.1\", \"true\", \"asimd\", \"true\", \"rvv\", \"true\"})\n+    \/\/ Cyclic dependency with distance 4 -> split into 4-packs\n+    @IR(counts = {IRNode.LOAD_VECTOR_S, \"> 0\",\n+                  IRNode.LOAD_VECTOR_S, IRNode.VECTOR_SIZE_4, \"> 0\",\n+                  IRNode.STORE_VECTOR, \"> 0\",\n+                  \".*multiversion.*\", \"> 0\"},\n+        phase = CompilePhase.PRINT_IDEAL,\n+        applyIfAnd = {\"MaxVectorSize\", \">=8\", \"UseAutoVectorizationSpeculativeAliasingChecks\", \"true\"},\n+        applyIfPlatform = {\"64-bit\", \"true\"},\n+        applyIfCPUFeatureOr = {\"sse4.1\", \"true\", \"asimd\", \"true\", \"rvv\", \"true\"})\n+    \/\/ Speculative aliasing check with multiversioning -> full vectorization & split packs.\n+    static Object[] test4c_alias(short[] a, short[] b) {\n+        for (int i = 0; i < RANGE-64; i++) {\n+          b[i+4] = a[i+0];\n+        }\n+        return new Object[]{ a, b };\n+    }\n+\n+    @Test\n+    @IR(counts = {IRNode.LOAD_VECTOR_S, IRNode.VECTOR_SIZE_4, \"> 0\",\n+                  IRNode.STORE_VECTOR, \"> 0\",\n+                  \".*multiversion.*\", \"= 0\"},\n+        phase = CompilePhase.PRINT_IDEAL,\n+        applyIfAnd = {\"MaxVectorSize\", \">=8\", \"AlignVector\", \"false\", \"UseAutoVectorizationSpeculativeAliasingChecks\", \"false\"},\n+        applyIfPlatform = {\"64-bit\", \"true\"},\n+        applyIfCPUFeatureOr = {\"sse4.1\", \"true\", \"asimd\", \"true\", \"rvv\", \"true\"})\n+    \/\/ Cyclic dependency with distance 5 -> split into 4-packs\n+    @IR(counts = {IRNode.LOAD_VECTOR_S, \"> 0\",\n+                  IRNode.LOAD_VECTOR_S, IRNode.VECTOR_SIZE_4, \"> 0\",\n+                  IRNode.STORE_VECTOR, \"> 0\",\n+                  \".*multiversion.*\", \"> 0\"},\n+        phase = CompilePhase.PRINT_IDEAL,\n+        applyIfAnd = {\"MaxVectorSize\", \">=8\", \"AlignVector\", \"false\", \"UseAutoVectorizationSpeculativeAliasingChecks\", \"true\"},\n+        applyIfPlatform = {\"64-bit\", \"true\"},\n+        applyIfCPUFeatureOr = {\"sse4.1\", \"true\", \"asimd\", \"true\", \"rvv\", \"true\"})\n+    \/\/ Speculative aliasing check with multiversioning -> full vectorization & split packs.\n+    static Object[] test4d_alias(short[] a, short[] b) {\n+        for (int i = 0; i < RANGE-64; i++) {\n+          b[i+5] = a[i+0];\n+        }\n+        return new Object[]{ a, b };\n+    }\n+\n+    @Test\n+    @IR(counts = {IRNode.LOAD_VECTOR_S, IRNode.VECTOR_SIZE_4, \"> 0\",\n+                  IRNode.STORE_VECTOR, \"> 0\",\n+                  \".*multiversion.*\", \"= 0\"},\n+        phase = CompilePhase.PRINT_IDEAL,\n+        applyIfAnd = {\"MaxVectorSize\", \">=8\", \"AlignVector\", \"false\", \"UseAutoVectorizationSpeculativeAliasingChecks\", \"false\"},\n+        applyIfPlatform = {\"64-bit\", \"true\"},\n+        applyIfCPUFeatureOr = {\"sse4.1\", \"true\", \"asimd\", \"true\", \"rvv\", \"true\"})\n+    \/\/ Cyclic dependency with distance 6 -> split into 4-packs\n+    @IR(counts = {IRNode.LOAD_VECTOR_S, \"> 0\",\n+                  IRNode.LOAD_VECTOR_S, IRNode.VECTOR_SIZE_4, \"> 0\",\n+                  IRNode.STORE_VECTOR, \"> 0\",\n+                  \".*multiversion.*\", \"> 0\"},\n+        phase = CompilePhase.PRINT_IDEAL,\n+        applyIfAnd = {\"MaxVectorSize\", \">=8\", \"AlignVector\", \"false\", \"UseAutoVectorizationSpeculativeAliasingChecks\", \"true\"},\n+        applyIfPlatform = {\"64-bit\", \"true\"},\n+        applyIfCPUFeatureOr = {\"sse4.1\", \"true\", \"asimd\", \"true\", \"rvv\", \"true\"})\n+    \/\/ Speculative aliasing check with multiversioning -> full vectorization & split packs.\n+    static Object[] test4e_alias(short[] a, short[] b) {\n+        for (int i = 0; i < RANGE-64; i++) {\n+          b[i+6] = a[i+0];\n+        }\n+        return new Object[]{ a, b };\n+    }\n+\n+    @Test\n+    @IR(counts = {IRNode.LOAD_VECTOR_S, IRNode.VECTOR_SIZE_4, \"> 0\",\n+                  IRNode.STORE_VECTOR, \"> 0\",\n+                  \".*multiversion.*\", \"= 0\"},\n+        phase = CompilePhase.PRINT_IDEAL,\n+        applyIfAnd = {\"MaxVectorSize\", \">=8\", \"AlignVector\", \"false\", \"UseAutoVectorizationSpeculativeAliasingChecks\", \"false\"},\n+        applyIfPlatform = {\"64-bit\", \"true\"},\n+        applyIfCPUFeatureOr = {\"sse4.1\", \"true\", \"asimd\", \"true\", \"rvv\", \"true\"})\n+    \/\/ Cyclic dependency with distance 7 -> split into 4-packs\n+    @IR(counts = {IRNode.LOAD_VECTOR_S, \"> 0\",\n+                  IRNode.LOAD_VECTOR_S, IRNode.VECTOR_SIZE_4, \"> 0\",\n+                  IRNode.STORE_VECTOR, \"> 0\",\n+                  \".*multiversion.*\", \"> 0\"},\n+        phase = CompilePhase.PRINT_IDEAL,\n+        applyIfAnd = {\"MaxVectorSize\", \">=8\", \"AlignVector\", \"false\", \"UseAutoVectorizationSpeculativeAliasingChecks\", \"true\"},\n+        applyIfPlatform = {\"64-bit\", \"true\"},\n+        applyIfCPUFeatureOr = {\"sse4.1\", \"true\", \"asimd\", \"true\", \"rvv\", \"true\"})\n+    \/\/ Speculative aliasing check with multiversioning -> full vectorization & split packs.\n+    static Object[] test4f_alias(short[] a, short[] b) {\n+        for (int i = 0; i < RANGE-64; i++) {\n+          b[i+7] = a[i+0];\n+        }\n+        return new Object[]{ a, b };\n+    }\n+\n+    @Test\n+    @IR(counts = {IRNode.LOAD_VECTOR_S, IRNode.VECTOR_SIZE_8, \"> 0\",\n+                  IRNode.STORE_VECTOR, \"> 0\",\n+                  \".*multiversion.*\", \"= 0\"},\n+        phase = CompilePhase.PRINT_IDEAL,\n+        applyIfAnd = {\"MaxVectorSize\", \">=32\", \"UseAutoVectorizationSpeculativeAliasingChecks\", \"false\"},\n+        applyIfPlatform = {\"64-bit\", \"true\"},\n+        applyIfCPUFeatureOr = {\"sse4.1\", \"true\", \"asimd\", \"true\", \"rvv\", \"true\"})\n+    \/\/ Cyclic dependency with distance 8 -> split into 8-packs\n+    @IR(counts = {IRNode.LOAD_VECTOR_S, \"> 0\",\n+                  IRNode.LOAD_VECTOR_S, IRNode.VECTOR_SIZE_8, \"> 0\",\n+                  IRNode.STORE_VECTOR, \"> 0\",\n+                  \".*multiversion.*\", \"> 0\"},\n+        phase = CompilePhase.PRINT_IDEAL,\n+        applyIfAnd = {\"MaxVectorSize\", \">=32\", \"UseAutoVectorizationSpeculativeAliasingChecks\", \"true\"},\n+        applyIfPlatform = {\"64-bit\", \"true\"},\n+        applyIfCPUFeatureOr = {\"avx2\", \"true\", \"asimd\", \"true\", \"rvv\", \"true\"})\n+    \/\/ Speculative aliasing check with multiversioning -> full vectorization & split packs.\n+    static Object[] test4g_alias(short[] a, short[] b) {\n+        for (int i = 0; i < RANGE-64; i++) {\n+          b[i+8] = a[i+0];\n+        }\n+        return new Object[]{ a, b };\n+    }\n+\n","filename":"test\/hotspot\/jtreg\/compiler\/loopopts\/superword\/TestSplitPacks.java","additions":289,"deletions":21,"binary":false,"changes":310,"status":"modified"},{"patch":"@@ -67,1 +67,13 @@\n-        applyIfCPUFeatureOr = {\"avx2\", \"true\", \"rvv\", \"true\"})\n+        applyIfCPUFeature = {\"avx2\", \"true\"})\n+    @IR(counts = {IRNode.LOAD_VECTOR_I,   IRNode.VECTOR_SIZE + \"min(max_int, max_long)\", \"> 0\",\n+                  IRNode.VECTOR_CAST_I2L, IRNode.VECTOR_SIZE + \"min(max_int, max_long)\", \"> 0\",\n+                  IRNode.OR_REDUCTION_V,                                                 \"> 0\",},\n+        applyIfAnd = {\"AlignVector\", \"false\", \"MaxVectorSize\", \">=32\"},\n+        applyIfPlatform = {\"riscv64\", \"true\"},\n+        applyIfCPUFeature = {\"rvv\", \"true\"})\n+    @IR(counts = {IRNode.LOAD_VECTOR_I,   IRNode.VECTOR_SIZE + \"min(max_int, max_long)\", \"> 0\",\n+                  IRNode.VECTOR_CAST_I2L, IRNode.VECTOR_SIZE + \"min(max_int, max_long)\", \"> 0\",\n+                  IRNode.OR_REDUCTION_V,                                                 \"> 0\",},\n+        applyIfAnd = {\"UseCompactObjectHeaders\", \"false\", \"MaxVectorSize\", \">=32\"},\n+        applyIfPlatform = {\"riscv64\", \"true\"},\n+        applyIfCPUFeature = {\"rvv\", \"true\"})\n","filename":"test\/hotspot\/jtreg\/compiler\/loopopts\/superword\/TestUnorderedReductionPartialVectorization.java","additions":13,"deletions":1,"binary":false,"changes":14,"status":"modified"},{"patch":"@@ -2,1 +2,1 @@\n- * Copyright (c) 2022, 2024, Oracle and\/or its affiliates. All rights reserved.\n+ * Copyright (c) 2022, 2025, Oracle and\/or its affiliates. All rights reserved.\n@@ -30,4 +30,1 @@\n- * @run driver compiler.vectorization.TestFloatConversionsVector nCOH_nAV\n- * @run driver compiler.vectorization.TestFloatConversionsVector nCOH_yAV\n- * @run driver compiler.vectorization.TestFloatConversionsVector yCOH_nAV\n- * @run driver compiler.vectorization.TestFloatConversionsVector yCOH_yAV\n+ * @run driver compiler.vectorization.TestFloatConversionsVector\n@@ -38,0 +35,2 @@\n+import java.util.Set;\n+\n@@ -52,7 +51,2 @@\n-        switch (args[0]) {\n-            case \"nCOH_nAV\" -> { framework.addFlags(\"-XX:-UseCompactObjectHeaders\", \"-XX:-AlignVector\"); }\n-            case \"nCOH_yAV\" -> { framework.addFlags(\"-XX:-UseCompactObjectHeaders\", \"-XX:+AlignVector\"); }\n-            case \"yCOH_nAV\" -> { framework.addFlags(\"-XX:+UseCompactObjectHeaders\", \"-XX:-AlignVector\"); }\n-            case \"yCOH_yAV\" -> { framework.addFlags(\"-XX:+UseCompactObjectHeaders\", \"-XX:+AlignVector\"); }\n-            default -> { throw new RuntimeException(\"Test argument not recognized: \" + args[0]); }\n-        };\n+        framework.addCrossProductScenarios(Set.of(\"-XX:-UseCompactObjectHeaders\", \"-XX:+UseCompactObjectHeaders\"),\n+                                           Set.of(\"-XX:-AlignVector\", \"-XX:+AlignVector\"));\n@@ -81,0 +75,3 @@\n+    @IR(counts = {IRNode.VECTOR_CAST_F2HF, IRNode.VECTOR_SIZE_2, \"> 0\"},\n+        applyIfOr = {\"UseCompactObjectHeaders\", \"false\", \"AlignVector\", \"false\"},\n+        applyIfCPUFeature = {\"asimd\", \"true\"})\n@@ -82,3 +79,3 @@\n-        for (int i = 0; i < finp.length; i+= 4) {\n-            sout[i+0] = Float.floatToFloat16(finp[i+0]);\n-            sout[i+1] = Float.floatToFloat16(finp[i+1]);\n+        for (int i = 0; i < finp.length; i += 4) {\n+            sout[i] = Float.floatToFloat16(finp[i]);\n+            sout[i + 1] = Float.floatToFloat16(finp[i + 1]);\n@@ -121,1 +118,1 @@\n-        for (int i = 0; i < ARRLEN; i++) {\n+        for (int i = 0; i < ARRLEN; i += 4) {\n@@ -123,0 +120,1 @@\n+            Asserts.assertEquals(Float.floatToFloat16(finp[i + 1]), sout[i + 1]);\n@@ -143,1 +141,13 @@\n-    @Run(test = {\"test_float16_float\", \"test_float16_float_strided\"}, mode = RunMode.STANDALONE)\n+    @Test\n+    @IR(counts = {IRNode.VECTOR_CAST_HF2F, IRNode.VECTOR_SIZE_2, \"> 0\"},\n+        applyIfOr = {\"UseCompactObjectHeaders\", \"false\", \"AlignVector\", \"false\"},\n+        applyIfCPUFeature = {\"asimd\", \"true\"})\n+    public void test_float16_float_short_vector(float[] fout, short[] sinp) {\n+        for (int i = 0; i < sinp.length; i += 4) {\n+            fout[i] = Float.float16ToFloat(sinp[i]);\n+            fout[i + 1] = Float.float16ToFloat(sinp[i + 1]);\n+        }\n+    }\n+\n+    @Run(test = {\"test_float16_float\", \"test_float16_float_strided\",\n+                 \"test_float16_float_short_vector\"}, mode = RunMode.STANDALONE)\n@@ -169,0 +179,10 @@\n+\n+        for (int i = 0; i < ITERS; i++) {\n+            test_float16_float_short_vector(fout, sinp);\n+        }\n+\n+        \/\/ Verifying the result\n+        for (int i = 0; i < ARRLEN; i += 4) {\n+            Asserts.assertEquals(Float.float16ToFloat(sinp[i]), fout[i]);\n+            Asserts.assertEquals(Float.float16ToFloat(sinp[i + 1]), fout[i + 1]);\n+        }\n","filename":"test\/hotspot\/jtreg\/compiler\/vectorization\/TestFloatConversionsVector.java","additions":37,"deletions":17,"binary":false,"changes":54,"status":"modified"},{"patch":"@@ -26,0 +26,1 @@\n+ * @key randomness\n@@ -40,0 +41,1 @@\n+import java.util.Random;\n@@ -43,0 +45,1 @@\n+import jdk.test.lib.Utils;\n@@ -76,0 +79,1 @@\n+        Random rand = Utils.getRandomInstance();\n@@ -82,2 +86,3 @@\n-            if (i%39 == 0) {\n-                int x = 0x7f800000 + ((i\/39) << 13);\n+            if (i%3 == 0) {\n+                int shift = rand.nextInt(13+1);\n+                int x = 0x7f800000 + ((i\/39) << shift);\n@@ -125,1 +130,2 @@\n-        String msg = \"floatToFloat16 wrong result: idx: \" + idx + \", \\t\" + f +\n+        String msg = \"floatToFloat16 wrong result: idx: \" + idx +\n+                     \", \\t\" + f + \", hex: \" + Integer.toHexString(Float.floatToRawIntBits(f)) +\n@@ -158,1 +164,1 @@\n-            if (i%39 == 0) {\n+            if (i%3 == 0) {\n","filename":"test\/hotspot\/jtreg\/compiler\/vectorization\/TestFloatConversionsVectorNaN.java","additions":10,"deletions":4,"binary":false,"changes":14,"status":"modified"},{"patch":"@@ -3,1 +3,1 @@\n- * Copyright (c) 2024, Oracle and\/or its affiliates. All rights reserved.\n+ * Copyright (c) 2024, 2025, Oracle and\/or its affiliates. All rights reserved.\n@@ -247,1 +247,1 @@\n-    @IR(applyIfCPUFeatureOr = {\"sve\", \"true\", \"avx2\", \"true\", \"rvv\", \"true\"},\n+    @IR(applyIfCPUFeature = {\"rvv\", \"true\"},\n@@ -250,0 +250,3 @@\n+    @IR(applyIfCPUFeatureOr = {\"asimd\", \"true\", \"avx\", \"true\"},\n+        applyIf = {\"MaxVectorSize\", \">=16\"},\n+        counts = {IRNode.VECTOR_CAST_S2D, IRNode.VECTOR_SIZE + \"min(max_short, max_double)\", \">0\"})\n@@ -286,2 +289,6 @@\n-    @IR(applyIfCPUFeatureOr = {\"asimd\", \"true\", \"avx\", \"true\", \"rvv\", \"true\"},\n-        counts = {IRNode.VECTOR_CAST_F2I, IRNode.VECTOR_SIZE + \"min(max_float, max_int)\", \">0\"})\n+    @IR(applyIfCPUFeatureOr = {\"asimd\", \"true\", \"avx\", \"true\", \"avx10_2\", \"true\", \"rvv\", \"true\"},\n+        counts = {IRNode.VECTOR_CAST_F2I, IRNode.VECTOR_SIZE + \"min(max_float, max_int)\", \"> 0\"})\n+    @IR(counts = {IRNode.X86_VCAST_F2X, \"> 0\"},\n+        applyIfCPUFeatureAnd = {\"avx\", \"true\", \"avx10_2\", \"false\"})\n+    @IR(counts = {IRNode.X86_VCAST_F2X_AVX10, \"> 0\"},\n+        applyIfCPUFeature = {\"avx10_2\", \"true\"})\n@@ -297,2 +304,6 @@\n-    @IR(applyIfCPUFeatureOr = {\"asimd\", \"true\", \"avx512dq\", \"true\", \"rvv\", \"true\"},\n-        counts = {IRNode.VECTOR_CAST_F2L, IRNode.VECTOR_SIZE + \"min(max_float, max_long)\", \">0\"})\n+    @IR(applyIfCPUFeatureOr = {\"asimd\", \"true\", \"avx512dq\", \"true\", \"avx10_2\", \"true\", \"rvv\", \"true\"},\n+        counts = {IRNode.VECTOR_CAST_F2L, IRNode.VECTOR_SIZE + \"min(max_float, max_long)\", \"> 0\"})\n+    @IR(counts = {IRNode.X86_VCAST_F2X, \"> 0\"},\n+        applyIfCPUFeatureAnd = {\"avx512dq\", \"true\", \"avx10_2\", \"false\"})\n+    @IR(counts = {IRNode.X86_VCAST_F2X_AVX10, \"> 0\"},\n+        applyIfCPUFeature = {\"avx10_2\", \"true\"})\n@@ -308,2 +319,6 @@\n-    @IR(applyIfCPUFeatureOr = {\"sve\", \"true\", \"avx\", \"true\", \"rvv\", \"true\"},\n-        counts = {IRNode.VECTOR_CAST_D2I, IRNode.VECTOR_SIZE + \"min(max_double, max_int)\", \">0\"})\n+    @IR(applyIfCPUFeatureOr = {\"sve\", \"true\", \"avx\", \"true\", \"avx10_2\", \"true\", \"rvv\", \"true\"},\n+        counts = {IRNode.VECTOR_CAST_D2I, IRNode.VECTOR_SIZE + \"min(max_double, max_int)\", \"> 0\"})\n+    @IR(counts = {IRNode.X86_VCAST_D2X, \"> 0\"},\n+        applyIfCPUFeatureAnd = {\"avx\", \"true\", \"avx10_2\", \"false\"})\n+    @IR(counts = {IRNode.X86_VCAST_D2X_AVX10, \"> 0\"},\n+        applyIfCPUFeature = {\"avx10_2\", \"true\"})\n@@ -319,2 +334,6 @@\n-    @IR(applyIfCPUFeatureOr = {\"asimd\", \"true\", \"avx512dq\", \"true\", \"rvv\", \"true\"},\n-        counts = {IRNode.VECTOR_CAST_D2L, IRNode.VECTOR_SIZE + \"min(max_double, max_long)\", \">0\"})\n+    @IR(applyIfCPUFeatureOr = {\"asimd\", \"true\", \"avx512dq\", \"true\", \"avx10_2\", \"true\", \"rvv\", \"true\"},\n+        counts = {IRNode.VECTOR_CAST_D2L, IRNode.VECTOR_SIZE + \"min(max_double, max_long)\", \"> 0\"})\n+    @IR(counts = {IRNode.X86_VCAST_D2X, \"> 0\"},\n+        applyIfCPUFeatureAnd = {\"avx512dq\", \"true\", \"avx10_2\", \"false\"})\n+    @IR(counts = {IRNode.X86_VCAST_D2X_AVX10, \"> 0\"},\n+        applyIfCPUFeature = {\"avx10_2\", \"true\"})\n@@ -331,2 +350,9 @@\n-    @IR(applyIfCPUFeatureOr = {\"asimd\", \"true\", \"avx2\", \"true\", \"rvv\", \"true\"},\n-        counts = {IRNode.VECTOR_CAST_F2S, IRNode.VECTOR_SIZE + \"min(max_float, max_short)\", \">0\"})\n+    @IR(applyIfCPUFeatureOr = {\"asimd\", \"true\", \"avx2\", \"true\", \"avx10_2\", \"true\", \"rvv\", \"true\"},\n+        applyIf = {\"AlignVector\", \"false\"},\n+        counts = {IRNode.VECTOR_CAST_F2S, IRNode.VECTOR_SIZE + \"min(max_float, max_short)\", \"> 0\"})\n+    @IR(counts = {IRNode.X86_VCAST_F2X, \"> 0\"},\n+        applyIf = {\"AlignVector\", \"false\"},\n+        applyIfCPUFeatureAnd = {\"avx2\", \"true\", \"avx10_2\", \"false\"})\n+    @IR(counts = {IRNode.X86_VCAST_F2X_AVX10, \"> 0\"},\n+        applyIf = {\"AlignVector\", \"false\"},\n+        applyIfCPUFeature = {\"avx10_2\", \"true\"})\n@@ -342,2 +368,9 @@\n-    @IR(applyIfCPUFeatureOr = {\"asimd\", \"true\", \"avx2\", \"true\", \"rvv\", \"true\"},\n-        counts = {IRNode.VECTOR_CAST_F2S, IRNode.VECTOR_SIZE + \"min(max_float, max_char)\", \">0\"})\n+    @IR(applyIfCPUFeatureOr = {\"asimd\", \"true\", \"avx2\", \"true\", \"avx10_2\", \"true\", \"rvv\", \"true\"},\n+        applyIf = {\"AlignVector\", \"false\"},\n+        counts = {IRNode.VECTOR_CAST_F2S, IRNode.VECTOR_SIZE + \"min(max_float, max_char)\", \"> 0\"})\n+    @IR(counts = {IRNode.X86_VCAST_F2X, \"> 0\"},\n+        applyIf = {\"AlignVector\", \"false\"},\n+        applyIfCPUFeatureAnd = {\"avx2\", \"true\", \"avx10_2\", \"false\"})\n+    @IR(counts = {IRNode.X86_VCAST_F2X_AVX10, \"> 0\"},\n+        applyIf = {\"AlignVector\", \"false\"},\n+        applyIfCPUFeature = {\"avx10_2\", \"true\"})\n@@ -353,1 +386,1 @@\n-    @IR(applyIfCPUFeatureOr = {\"sve\", \"true\", \"avx\", \"true\", \"rvv\", \"true\"},\n+    @IR(applyIfCPUFeature = {\"rvv\", \"true\"},\n@@ -355,1 +388,10 @@\n-        counts = {IRNode.VECTOR_CAST_D2S, IRNode.VECTOR_SIZE + \"min(max_double, max_short)\", \">0\"})\n+        counts = {IRNode.VECTOR_CAST_D2S, IRNode.VECTOR_SIZE + \"min(max_double, max_short)\", \"> 0\"})\n+    @IR(applyIfCPUFeatureOr = {\"sve\", \"true\", \"avx\", \"true\", \"avx10_2\", \"true\"},\n+        applyIf = {\"MaxVectorSize\", \">=16\"},\n+        counts = {IRNode.VECTOR_CAST_D2S, IRNode.VECTOR_SIZE + \"min(max_double, max_short)\", \"> 0\"})\n+    @IR(counts = {IRNode.X86_VCAST_D2X, \"> 0\"},\n+        applyIf = {\"MaxVectorSize\", \">=16\"},\n+        applyIfCPUFeatureAnd = {\"avx\", \"true\", \"avx10_2\", \"false\"})\n+    @IR(counts = {IRNode.X86_VCAST_D2X_AVX10, \"> 0\"},\n+        applyIf = {\"MaxVectorSize\", \">=16\"},\n+        applyIfCPUFeature = {\"avx10_2\", \"true\"})\n@@ -365,3 +407,12 @@\n-    @IR(applyIfCPUFeatureOr = {\"sve\", \"true\", \"avx\", \"true\", \"rvv\", \"true\"},\n-        applyIf = {\"MaxVectorSize\", \">=32\"},\n-        counts = {IRNode.VECTOR_CAST_D2S, IRNode.VECTOR_SIZE + \"min(max_double, max_char)\", \">0\"})\n+    @IR(applyIfCPUFeature = {\"rvv\", \"true\"},\n+        applyIf = {\"MaxVectorSize\", \">= 32\"},\n+        counts = {IRNode.VECTOR_CAST_D2S, IRNode.VECTOR_SIZE + \"min(max_double, max_char)\", \"> 0\"})\n+    @IR(applyIfCPUFeatureOr = {\"sve\", \"true\", \"avx\", \"true\", \"avx10_2\", \"true\"},\n+        applyIf = {\"MaxVectorSize\", \">= 16\"},\n+        counts = {IRNode.VECTOR_CAST_D2S, IRNode.VECTOR_SIZE + \"min(max_double, max_char)\", \"> 0\"})\n+    @IR(counts = {IRNode.X86_VCAST_D2X, \"> 0\"},\n+        applyIf = {\"MaxVectorSize\", \">=16\"},\n+        applyIfCPUFeatureAnd = {\"avx\", \"true\", \"avx10_2\", \"false\"})\n+    @IR(counts = {IRNode.X86_VCAST_D2X_AVX10, \"> 0\"},\n+        applyIf = {\"MaxVectorSize\", \">=16\"},\n+        applyIfCPUFeature = {\"avx10_2\", \"true\"})\n","filename":"test\/hotspot\/jtreg\/compiler\/vectorization\/runner\/ArrayTypeConvertTest.java","additions":70,"deletions":19,"binary":false,"changes":89,"status":"modified"},{"patch":"@@ -32,1 +32,1 @@\n- * @run main\/othervm -Xbootclasspath\/a:. -XX:+UnlockDiagnosticVMOptions\n+ * @run main\/othervm\/timeout=480 -Xbootclasspath\/a:. -XX:+UnlockDiagnosticVMOptions\n","filename":"test\/hotspot\/jtreg\/runtime\/cds\/DeterministicDump.java","additions":1,"deletions":1,"binary":false,"changes":2,"status":"modified"},{"patch":"@@ -126,18 +126,17 @@\n-        if (!dumpWithParallel && execWithParallel) {\n-            \/\/ We dumped with G1, so we have an archived heap. At exec time, try to load them into\n-            \/\/ a small ParallelGC heap that may be too small.\n-            System.out.println(\"2. Exec with \" + execGC);\n-            out = TestCommon.exec(helloJar,\n-                                    execGC,\n-                                    small1,\n-                                    small2,\n-                                    \"-Xmx4m\",\n-                                    coops,\n-                                    \"-Xlog:cds\",\n-                                    \"Hello\");\n-            if (out.getExitValue() == 0) {\n-                out.shouldContain(HELLO);\n-                out.shouldNotContain(errMsg);\n-            } else {\n-                out.shouldNotHaveFatalError();\n-            }\n+        \/\/ Regardless of which GC dumped the heap, there will be an object archive, either\n+        \/\/ created with mapping if dumped with G1, or streaming if dumped with parallel GC.\n+        \/\/ At exec time, try to load them into a small ParallelGC heap that may be too small.\n+        System.out.println(\"2. Exec with \" + execGC);\n+        out = TestCommon.exec(helloJar,\n+                              execGC,\n+                              small1,\n+                              small2,\n+                              \"-Xmx4m\",\n+                              coops,\n+                              \"-Xlog:cds\",\n+                              \"Hello\");\n+        if (out.getExitValue() == 0) {\n+            out.shouldContain(HELLO);\n+            out.shouldNotContain(errMsg);\n+        } else {\n+            out.shouldNotHaveFatalError();\n","filename":"test\/hotspot\/jtreg\/runtime\/cds\/appcds\/TestParallelGCWithCDS.java","additions":17,"deletions":18,"binary":false,"changes":35,"status":"modified"},{"patch":"@@ -49,1 +49,1 @@\n- * @run main\/othervm -XX:TLABSize=2k jdk.jfr.event.oldobject.TestObjectDescription\n+ * @run main\/othervm\/timeout=960 -XX:TLABSize=2k jdk.jfr.event.oldobject.TestObjectDescription\n","filename":"test\/jdk\/jdk\/jfr\/event\/oldobject\/TestObjectDescription.java","additions":1,"deletions":1,"binary":false,"changes":2,"status":"modified"},{"patch":"@@ -124,0 +124,1 @@\n+        map.put(\"vm.cds.nocoops.archive.available\", this::vmCDSNocoopsArchiveAvailable);\n@@ -128,0 +129,2 @@\n+        map.put(\"vm.cds.write.mapped.java.heap\", this::vmCDSCanWriteMappedArchivedJavaHeap);\n+        map.put(\"vm.cds.write.streamed.java.heap\", this::vmCDSCanWriteStreamedArchivedJavaHeap);\n@@ -151,0 +154,1 @@\n+        vmOptFinalIntxFlags(map);\n@@ -384,0 +388,1 @@\n+        vmOptFinalFlag(map, \"UseAdaptiveSizePolicy\");\n@@ -391,0 +396,20 @@\n+    \/**\n+     * Selected final flag of type intx.\n+     *\n+     * @param map - property-value pairs\n+     * @param flagName - flag name\n+     *\/\n+    private void vmOptFinalIntxFlag(SafeMap map, String flagName) {\n+        map.put(\"vm.opt.final.\" + flagName,\n+                () -> String.valueOf(WB.getIntxVMFlag(flagName)));\n+    }\n+\n+    \/**\n+     * Selected sets of final flags of type intx.\n+     *\n+     * @param map - property-value pairs\n+     *\/\n+    protected void vmOptFinalIntxFlags(SafeMap map) {\n+        vmOptFinalIntxFlag(map, \"MaxVectorSize\");\n+    }\n+\n@@ -426,1 +451,6 @@\n-        return \"\" + WB.isCDSIncluded();\n+        boolean noJvmtiAdded = allFlags()\n+                .filter(s -> s.startsWith(\"-agentpath\"))\n+                .findAny()\n+                .isEmpty();\n+\n+        return \"\" + (noJvmtiAdded && WB.isCDSIncluded());\n@@ -439,0 +469,10 @@\n+    \/**\n+     * Check for CDS no compressed oops archive existence.\n+     *\n+     * @return true if CDS archive classes_nocoops.jsa exists in the JDK to be tested.\n+     *\/\n+    protected String vmCDSNocoopsArchiveAvailable() {\n+        Path archive = Paths.get(System.getProperty(\"java.home\"), \"lib\", \"server\", \"classes_nocoops.jsa\");\n+        return \"\" + (\"true\".equals(vmCDS()) && Files.exists(archive));\n+    }\n+\n@@ -451,1 +491,1 @@\n-     *         if -XX:-UseCompressedClassPointers is specified,\n+     *         if -XX:-UseCompressedClassPointers is specified.\n@@ -454,2 +494,19 @@\n-        return \"\" + (\"true\".equals(vmCDS()) && WB.canWriteJavaHeapArchive()\n-                     && isCDSRuntimeOptionsCompatible());\n+        return \"\" + (\"true\".equals(vmCDS()) && WB.canWriteJavaHeapArchive());\n+    }\n+\n+    \/**\n+     * @return true if it's possible for \"java -Xshare:dump\" to write Java heap objects\n+     *         with the current set of jtreg VM options. For example, false will be returned\n+     *         if -XX:-UseCompressedClassPointers is specified.\n+     *\/\n+    protected String vmCDSCanWriteMappedArchivedJavaHeap() {\n+        return \"\" + (\"true\".equals(vmCDS()) && WB.canWriteMappedJavaHeapArchive());\n+    }\n+\n+    \/**\n+     * @return true if it's possible for \"java -Xshare:dump\" to write Java heap objects\n+     *         with the current set of jtreg VM options. For example, false will be returned\n+     *         if -XX:-UseCompressedClassPointers is specified.\n+     *\/\n+    protected String vmCDSCanWriteStreamedArchivedJavaHeap() {\n+        return \"\" + (\"true\".equals(vmCDS()) && WB.canWriteStreamedJavaHeapArchive());\n@@ -480,25 +537,0 @@\n-    \/**\n-     * @return true if the VM options specified via the \"test.cds.runtime.options\"\n-     * property is compatible with writing Java heap objects into the CDS archive\n-     *\/\n-    protected boolean isCDSRuntimeOptionsCompatible() {\n-        String jtropts = System.getProperty(\"test.cds.runtime.options\");\n-        if (jtropts == null) {\n-            return true;\n-        }\n-        String CCP_DISABLED = \"-XX:-UseCompressedClassPointers\";\n-        String G1GC_ENABLED = \"-XX:+UseG1GC\";\n-        String PARALLELGC_ENABLED = \"-XX:+UseParallelGC\";\n-        String SERIALGC_ENABLED = \"-XX:+UseSerialGC\";\n-        for (String opt : jtropts.split(\",\")) {\n-            if (opt.equals(CCP_DISABLED)) {\n-                return false;\n-            }\n-            if (opt.startsWith(GC_PREFIX) && opt.endsWith(GC_SUFFIX) &&\n-                !opt.equals(G1GC_ENABLED) && !opt.equals(PARALLELGC_ENABLED) && !opt.equals(SERIALGC_ENABLED)) {\n-                return false;\n-            }\n-        }\n-        return true;\n-    }\n-\n","filename":"test\/jtreg-ext\/requires\/VMProps.java","additions":61,"deletions":29,"binary":false,"changes":90,"status":"modified"}]}