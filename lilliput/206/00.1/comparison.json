{"files":[{"patch":"@@ -5909,0 +5909,3 @@\n+opclass memory_noindex(indirect,\n+                       indOffI1, indOffL1,indOffI2, indOffL2, indOffI4, indOffL4, indOffI8, indOffL8,\n+                       indirectN, indOffIN, indOffLN, indirectX2P, indOffX2P);\n@@ -6843,1 +6846,1 @@\n-instruct loadNKlassCompactHeaders(iRegNNoSp dst, memory4 mem)\n+instruct loadNKlassCompactHeaders(iRegNNoSp dst, memory_noindex mem)\n@@ -6851,1 +6854,1 @@\n-    \"lsrw  $dst, $dst, markWord::klass_shift_at_offset\"\n+    \"lsrw  $dst, $dst, markWord::klass_shift\"\n@@ -6854,4 +6857,5 @@\n-    \/\/ inlined aarch64_enc_ldrw\n-    loadStore(masm, &MacroAssembler::ldrw, $dst$$Register, $mem->opcode(),\n-              as_Register($mem$$base), $mem$$index, $mem$$scale, $mem$$disp, 4);\n-    __ lsrw($dst$$Register, $dst$$Register, markWord::klass_shift_at_offset);\n+    assert($mem$$index$$Register == noreg, \"must not have indexed address\");\n+    \/\/ The incoming address is pointing into obj-start + klass_offset_in_bytes. We need to extract\n+    \/\/ obj-start, so that we can load from the object's mark-word instead.\n+    __ ldrw($dst$$Register, Address($mem$$base$$Register, $mem$$disp - Type::klass_offset()));\n+    __ lsrw($dst$$Register, $dst$$Register, markWord::klass_shift);\n","filename":"src\/hotspot\/cpu\/aarch64\/aarch64.ad","additions":10,"deletions":6,"binary":false,"changes":16,"status":"modified"},{"patch":"@@ -4914,2 +4914,2 @@\n-  ldr(dst, Address(src, oopDesc::mark_offset_in_bytes()));\n-  lsr(dst, dst, markWord::klass_shift);\n+  ldrw(dst, Address(src, oopDesc::mark_offset_in_bytes()));\n+  lsrw(dst, dst, markWord::klass_shift);\n@@ -5737,0 +5737,1 @@\n+  Register cnt2 = tmp2;  \/\/ cnt2 only used in array length compare\n@@ -5739,1 +5740,0 @@\n-  int klass_offset  = arrayOopDesc::klass_offset_in_bytes();\n@@ -5743,10 +5743,0 @@\n-  \/\/ When the length offset is not aligned to 8 bytes,\n-  \/\/ then we align it down. This is valid because the new\n-  \/\/ offset will always be the klass which is the same\n-  \/\/ for type arrays.\n-  int start_offset = align_down(length_offset, BytesPerWord);\n-  int extra_length = base_offset - start_offset;\n-  assert(start_offset == length_offset || start_offset == klass_offset,\n-         \"start offset must be 8-byte-aligned or be the klass offset\");\n-  assert(base_offset != start_offset, \"must include the length field\");\n-  extra_length = extra_length \/ elem_size; \/\/ We count in elements, not bytes.\n@@ -5786,4 +5776,5 @@\n-    \/\/ Increase loop counter by diff between base- and actual start-offset.\n-    addw(cnt1, cnt1, extra_length);\n-    lea(a1, Address(a1, start_offset));\n-    lea(a2, Address(a2, start_offset));\n+    ldrw(cnt2, Address(a2, length_offset));\n+    eorw(tmp5, cnt1, cnt2);\n+    cbnzw(tmp5, DONE);\n+    lea(a1, Address(a1, base_offset));\n+    lea(a2, Address(a2, base_offset));\n@@ -5852,3 +5843,1 @@\n-    \/\/ Increase loop counter by diff between base- and actual start-offset.\n-    addw(cnt1, cnt1, extra_length);\n-\n+    ldrw(cnt2, Address(a2, length_offset));\n@@ -5859,1 +5848,1 @@\n-    ldr(tmp3, Address(pre(a1, start_offset)));\n+    ldr(tmp3, Address(pre(a1, base_offset)));\n@@ -5862,1 +5851,1 @@\n-    ldr(tmp4, Address(pre(a2, start_offset)));\n+    ldr(tmp4, Address(pre(a2, base_offset)));\n@@ -5864,0 +5853,2 @@\n+    cmp(cnt2, cnt1);\n+    br(NE, DONE);\n@@ -5895,1 +5886,3 @@\n-    ldr(tmp4, Address(pre(a2, start_offset)));\n+    ldr(tmp4, Address(pre(a2, base_offset)));\n+    cmp(cnt2, cnt1);\n+    br(NE, DONE);\n@@ -5916,0 +5909,3 @@\n+    cmp(cnt2, cnt1);\n+    br(NE, DONE);\n+    cbz(cnt1, SAME);\n@@ -5917,2 +5913,2 @@\n-    ldr(tmp3, Address(a1, start_offset));\n-    ldr(tmp4, Address(a2, start_offset));\n+    ldr(tmp3, Address(a1, base_offset));\n+    ldr(tmp4, Address(a2, base_offset));\n","filename":"src\/hotspot\/cpu\/aarch64\/macroAssembler_aarch64.cpp","additions":21,"deletions":25,"binary":false,"changes":46,"status":"modified"},{"patch":"@@ -3662,1 +3662,0 @@\n-    assert(is_aligned(header_size, BytesPerLong), \"oop header size must be 8-byte-aligned\");\n","filename":"src\/hotspot\/cpu\/aarch64\/templateTable_aarch64.cpp","additions":0,"deletions":1,"binary":false,"changes":1,"status":"modified"},{"patch":"@@ -4705,2 +4705,3 @@\n-    __ lwu(as_Register($dst$$reg), Address(as_Register($mem$$base), $mem$$disp));\n-    __ srli(as_Register($dst$$reg), as_Register($dst$$reg), (unsigned) markWord::klass_shift_at_offset);\n+    Unimplemented();\n+    \/\/ __ lwu(as_Register($dst$$reg), Address(as_Register($mem$$base), $mem$$disp));\n+    \/\/ __ srli(as_Register($dst$$reg), as_Register($dst$$reg), (unsigned) markWord::klass_shift_at_offset);\n","filename":"src\/hotspot\/cpu\/riscv\/riscv.ad","additions":3,"deletions":2,"binary":false,"changes":5,"status":"modified"},{"patch":"@@ -1,1 +1,1 @@\n-\/*\n+ \/*\n@@ -35,1 +35,1 @@\n-         CompressedKlassPointers::narrow_klass_pointer_bits() == 22, \"Rethink if we ever use different nKlass bit sizes\");\n+         CompressedKlassPointers::narrow_klass_pointer_bits() == 19, \"Rethink if we ever use different nKlass bit sizes\");\n","filename":"src\/hotspot\/cpu\/x86\/compressedKlass_x86.cpp","additions":2,"deletions":2,"binary":false,"changes":4,"status":"modified"},{"patch":"@@ -5149,2 +5149,2 @@\n-  movq(dst, Address(src, oopDesc::mark_offset_in_bytes()));\n-  shrq(dst, markWord::klass_shift);\n+  movl(dst, Address(src, oopDesc::mark_offset_in_bytes()));\n+  shrl(dst, markWord::klass_shift);\n","filename":"src\/hotspot\/cpu\/x86\/macroAssembler_x86.cpp","additions":2,"deletions":2,"binary":false,"changes":4,"status":"modified"},{"patch":"@@ -60,0 +60,5 @@\n+  if (UseCompactObjectHeaders) {\n+    \/\/ Don't generate anything else and always take the slow-path for now.\n+    return;\n+  }\n+\n","filename":"src\/hotspot\/cpu\/x86\/sharedRuntime_x86.cpp","additions":5,"deletions":0,"binary":false,"changes":5,"status":"modified"},{"patch":"@@ -3581,2 +3581,1 @@\n-      assert(is_aligned(oopDesc::base_offset_in_bytes(), BytesPerLong), \"oop base offset must be 8-byte-aligned\");\n-      __ decrement(rdx, oopDesc::base_offset_in_bytes());\n+      __ decrement(rdx, align_up(oopDesc::base_offset_in_bytes(), BytesPerLong));\n","filename":"src\/hotspot\/cpu\/x86\/templateTable_x86.cpp","additions":1,"deletions":2,"binary":false,"changes":3,"status":"modified"},{"patch":"@@ -7166,1 +7166,1 @@\n-    \"shrl    $dst, markWord::klass_shift_at_offset\"\n+    \"shrl    $dst, markWord::klass_shift\"\n@@ -7169,0 +7169,4 @@\n+    \/\/ The incoming address is pointing into obj-start + Type::klass_offset(). We need to extract\n+    \/\/ obj-start, so that we can load from the object's mark-word instead.\n+    Register d = $dst$$Register;\n+    Address  s = ($mem$$Address).plus_disp(-Type::klass_offset());\n@@ -7170,5 +7174,4 @@\n-      __ eshrl($dst$$Register, $mem$$Address, markWord::klass_shift_at_offset, false);\n-    }\n-    else {\n-      __ movl($dst$$Register, $mem$$Address);\n-      __ shrl($dst$$Register, markWord::klass_shift_at_offset);\n+      __ eshrl(d, s, markWord::klass_shift, false);\n+    } else {\n+      __ movl(d, s);\n+      __ shrl(d, markWord::klass_shift);\n","filename":"src\/hotspot\/cpu\/x86\/x86.ad","additions":9,"deletions":6,"binary":false,"changes":15,"status":"modified"},{"patch":"@@ -163,1 +163,3 @@\n-  return is_too_large_to_archive(o->size());\n+  size_t size = o->size();\n+  size = o->copy_size_cds(size, o->mark());\n+  return is_too_large_to_archive(size);\n@@ -250,1 +252,2 @@\n-    return p->size();\n+    size_t size = p->size();\n+    return p->copy_size_cds(size, p->mark());\n@@ -535,1 +538,3 @@\n-  size_t byte_size = src_obj->size() * HeapWordSize;\n+  size_t old_size = src_obj->size();\n+  size_t new_size = src_obj->copy_size_cds(old_size, src_obj->mark());\n+  size_t byte_size = new_size * HeapWordSize;\n@@ -556,1 +561,1 @@\n-  memcpy(to, from, byte_size);\n+  memcpy(to, from, MIN2(new_size, old_size) * HeapWordSize);\n@@ -732,0 +737,1 @@\n+    assert(fake_oop->mark().narrow_klass() != 0, \"must not be null\");\n@@ -744,1 +750,9 @@\n-      fake_oop->set_mark(markWord::prototype().set_narrow_klass(nk).copy_set_hash(src_hash));\n+      markWord m = markWord::prototype().set_narrow_klass(nk);\n+      m = m.copy_hashctrl_from(src_obj->mark());\n+      fake_oop->set_mark(m);\n+      if (m.is_hashed_not_expanded()) {\n+        fake_oop->set_mark(fake_oop->initialize_hash_if_necessary(src_obj, src_klass, m));\n+      } else if (m.is_not_hashed_expanded()) {\n+        fake_oop->set_mark(m.set_not_hashed_not_expanded());\n+      }\n+      assert(!fake_oop->mark().is_not_hashed_expanded() && !fake_oop->mark().is_hashed_not_expanded(), \"must not be not-hashed-moved and not be hashed-not-moved\");\n@@ -747,0 +761,2 @@\n+      DEBUG_ONLY(intptr_t archived_hash = fake_oop->identity_hash());\n+      assert(src_hash == archived_hash, \"Different hash codes: original \" INTPTR_FORMAT \", archived \" INTPTR_FORMAT, src_hash, archived_hash);\n@@ -749,3 +765,0 @@\n-\n-    DEBUG_ONLY(intptr_t archived_hash = fake_oop->identity_hash());\n-    assert(src_hash == archived_hash, \"Different hash codes: original \" INTPTR_FORMAT \", archived \" INTPTR_FORMAT, src_hash, archived_hash);\n","filename":"src\/hotspot\/share\/cds\/aotMappedHeapWriter.cpp","additions":21,"deletions":8,"binary":false,"changes":29,"status":"modified"},{"patch":"@@ -1950,2 +1950,3 @@\n-  \/\/ Reduce class space size if it would not fit into the Klass encoding range\n-  constexpr size_t max_encoding_range_size = 4 * G;\n+  \/\/ Reduce class space size if it would not fit into the maximum possible Klass encoding range. That\n+  \/\/ range is defined by the narrowKlass size.\n+  const size_t max_encoding_range_size = CompressedKlassPointers::max_klass_range_size();\n","filename":"src\/hotspot\/share\/cds\/aotMetaspace.cpp","additions":3,"deletions":2,"binary":false,"changes":5,"status":"modified"},{"patch":"@@ -168,1 +168,5 @@\n-      return (size_t)Klass::layout_helper_size_in_bytes(lh) >> LogHeapWordSize;\n+      size_t size = (size_t)Klass::layout_helper_size_in_bytes(lh) >> LogHeapWordSize;\n+      if (UseCompactObjectHeaders && archive_object->mark().is_expanded() && klass->expand_for_hash(archive_object, archive_object->mark())) {\n+        size = align_object_size(size + 1);\n+      }\n+      return size;\n@@ -177,1 +181,5 @@\n-    return align_up(size_in_bytes, (size_t)MinObjAlignmentInBytes) \/ HeapWordSize;\n+    size_t size = align_up(size_in_bytes, (size_t)MinObjAlignmentInBytes) \/ HeapWordSize;\n+    if (UseCompactObjectHeaders && archive_object->mark().is_expanded() && klass->expand_for_hash(archive_object, archive_object->mark())) {\n+      size = align_object_size(size + 1);\n+    }\n+    return size;\n@@ -190,0 +198,1 @@\n+  assert(!(UseCompactObjectHeaders && mark.is_hashed_not_expanded()), \"Must not be hashed\/not-expanded\");\n@@ -191,1 +200,3 @@\n-    heap_object = Universe::heap()->class_allocate(klass, size, CHECK_NULL);\n+    size_t base_size = size;\n+    assert(!(UseCompactObjectHeaders && mark.is_not_hashed_expanded()), \"should not happen\");\n+    heap_object = Universe::heap()->class_allocate(klass, size, base_size, CHECK_NULL);\n@@ -318,3 +329,6 @@\n-    size_t payload_size = size - 1;\n-    HeapWord* archive_start = ((HeapWord*)archive_object) + 1;\n-    HeapWord* heap_start = cast_from_oop<HeapWord*>(heap_object) + 1;\n+    size_t offset = 1;\n+    size_t payload_size = size - offset;\n+    HeapWord* archive_start = ((HeapWord*)archive_object);\n+    HeapWord* heap_start = cast_from_oop<HeapWord*>(heap_object);\n+\n+    Copy::disjoint_words(archive_start + offset, heap_start + offset, payload_size);\n@@ -322,1 +336,4 @@\n-    Copy::disjoint_words(archive_start, heap_start, payload_size);\n+    if (UseCompactObjectHeaders) {\n+      \/\/ The copying might have missed the first 4 bytes of payload\/arraylength, copy that also.\n+      *(reinterpret_cast<jint*>(heap_start) + 1) = *(reinterpret_cast<jint*>(archive_start) + 1);\n+    }\n@@ -342,1 +359,1 @@\n-  size_t header_size = word_scale;\n+  size_t header_size = (UseCompactObjectHeaders && use_coops) ? 1 : word_scale;\n@@ -351,0 +368,7 @@\n+  if (UseCompactObjectHeaders && !use_coops) {\n+    \/\/ Copy first 4 primitive bytes.\n+    jint* archive_start = reinterpret_cast<jint*>(archive_object);\n+    HeapWord* heap_start = cast_from_oop<HeapWord*>(heap_object);\n+    *(reinterpret_cast<jint*>(heap_start) + 1) = *(archive_start + 1);\n+  }\n+\n","filename":"src\/hotspot\/share\/cds\/aotStreamedHeapLoader.cpp","additions":32,"deletions":8,"binary":false,"changes":40,"status":"modified"},{"patch":"@@ -314,0 +314,2 @@\n+  size_t old_size = src_obj->size();\n+  size_t size = src_obj->copy_size_cds(old_size, src_obj->mark());\n@@ -318,1 +320,1 @@\n-    write<size_t>(src_obj->size());\n+    write<size_t>(size);\n@@ -320,1 +322,1 @@\n-  size_t byte_size = src_obj->size() * HeapWordSize;\n+  size_t byte_size = size * HeapWordSize;\n@@ -338,1 +340,1 @@\n-  memcpy(to, from, byte_size);\n+  memcpy(to, from, MIN2(size, old_size) * HeapWordSize);\n@@ -401,2 +403,13 @@\n-    intptr_t src_hash = src_obj->identity_hash();\n-    mw = mw.copy_set_hash(src_hash);\n+    if (UseCompactObjectHeaders) {\n+      mw = mw.copy_hashctrl_from(src_obj->mark());\n+      if (mw.is_hashed_not_expanded()) {\n+        mw = fake_oop->initialize_hash_if_necessary(src_obj, src_klass, mw);\n+      } else if (mw.is_not_hashed_expanded()) {\n+        \/\/ If a scratch mirror class has not been hashed until now, then reset its\n+        \/\/ hash bits to initial state.\n+        mw = mw.set_not_hashed_not_expanded();\n+      }\n+    } else {\n+      intptr_t src_hash = src_obj->identity_hash();\n+      mw = mw.copy_set_hash(src_hash);\n+    }\n@@ -548,0 +561,1 @@\n+      size = src_obj->copy_size_cds(size, src_obj->mark());\n","filename":"src\/hotspot\/share\/cds\/aotStreamedHeapWriter.cpp","additions":19,"deletions":5,"binary":false,"changes":24,"status":"modified"},{"patch":"@@ -627,1 +627,1 @@\n-      oop m = java_lang_Class::create_basic_type_mirror(type2name(bt), bt, CHECK);\n+      oop m = java_lang_Class::create_basic_type_mirror(type2name(bt), bt, true, CHECK);\n@@ -798,0 +798,2 @@\n+  assert(!UseCompactObjectHeaders || scratch_m->mark().is_not_hashed_expanded(), \"scratch mirror must have not-hashed-expanded state\");\n+  assert(!UseCompactObjectHeaders || !orig_mirror->mark().is_not_hashed_expanded(), \"must not be not-hashed-expanded\");\n@@ -799,0 +801,1 @@\n+    intptr_t orig_mark = orig_mirror->mark().value();\n@@ -801,2 +804,17 @@\n-      narrowKlass nk = CompressedKlassPointers::encode(orig_mirror->klass());\n-      scratch_m->set_mark(markWord::prototype().set_narrow_klass(nk).copy_set_hash(src_hash));\n+      \/\/ We leave the cases not_hashed\/not_hashed_expanded as they are.\n+      assert(orig_mirror->mark().is_hashed_not_expanded() || orig_mirror->mark().is_hashed_expanded(), \"must be hashed\");\n+      Klass* orig_klass = orig_mirror->klass();\n+      narrowKlass nk = CompressedKlassPointers::encode(orig_klass);\n+      markWord mark = markWord::prototype().set_narrow_klass(nk);\n+      mark = mark.copy_hashctrl_from(orig_mirror->mark());\n+      if (mark.is_hashed_not_expanded()) {\n+        scratch_m->set_mark(scratch_m->initialize_hash_if_necessary(orig_mirror, orig_klass, mark));\n+      } else {\n+        assert(mark.is_hashed_expanded(), \"must be hashed & moved\");\n+        int offset = orig_klass->hash_offset_in_bytes(orig_mirror, mark);\n+        assert(offset >= 4, \"hash offset must not be in header\");\n+        scratch_m->int_field_put(offset, (jint) src_hash);\n+        scratch_m->set_mark(mark);\n+      }\n+      assert(scratch_m->mark().is_hashed_expanded(), \"must be hashed & moved\");\n+      assert(scratch_m->mark().is_not_hashed_expanded() || scratch_m->mark().is_hashed_expanded(), \"must be not hashed and expanded\");\n@@ -805,0 +823,2 @@\n+      DEBUG_ONLY(intptr_t archived_hash = scratch_m->identity_hash());\n+      assert(src_hash == archived_hash, \"Different hash codes: original \" INTPTR_FORMAT \", archived \" INTPTR_FORMAT, src_hash, archived_hash);\n@@ -807,3 +827,0 @@\n-\n-    DEBUG_ONLY(intptr_t archived_hash = scratch_m->identity_hash());\n-    assert(src_hash == archived_hash, \"Different hash codes: original \" INTPTR_FORMAT \", archived \" INTPTR_FORMAT, src_hash, archived_hash);\n","filename":"src\/hotspot\/share\/cds\/heapShared.cpp","additions":23,"deletions":6,"binary":false,"changes":29,"status":"modified"},{"patch":"@@ -302,0 +302,4 @@\n+  int hash_offset_in_bytes() const {\n+    return get_instanceKlass()->hash_offset_in_bytes(nullptr, markWord(0));\n+  }\n+\n","filename":"src\/hotspot\/share\/ci\/ciInstanceKlass.hpp","additions":4,"deletions":0,"binary":false,"changes":4,"status":"modified"},{"patch":"@@ -75,0 +75,3 @@\n+  bool is_mirror_instance_klass() { return get_Klass()->is_mirror_instance_klass(); }\n+  bool is_reference_instance_klass() { return get_Klass()->is_reference_instance_klass(); }\n+\n","filename":"src\/hotspot\/share\/ci\/ciKlass.hpp","additions":3,"deletions":0,"binary":false,"changes":3,"status":"modified"},{"patch":"@@ -4932,0 +4932,4 @@\n+int ClassFileParser::hash_offset() const {\n+  return _field_info->_hash_offset;\n+}\n+\n","filename":"src\/hotspot\/share\/classfile\/classFileParser.cpp","additions":4,"deletions":0,"binary":false,"changes":4,"status":"modified"},{"patch":"@@ -75,0 +75,1 @@\n+  int _hash_offset;\n@@ -510,0 +511,1 @@\n+  int hash_offset() const;\n","filename":"src\/hotspot\/share\/classfile\/classFileParser.hpp","additions":2,"deletions":0,"binary":false,"changes":2,"status":"modified"},{"patch":"@@ -213,0 +213,17 @@\n+\/\/ Finds a slot for the identity hash-code.\n+\/\/ Same basic algorithm as above add() method, but simplified\n+\/\/ and does not actually insert the field.\n+int FieldLayout::find_hash_offset() {\n+  LayoutRawBlock* start = this->_start;\n+  LayoutRawBlock* last = last_block();\n+  LayoutRawBlock* cursor = start;\n+  while (cursor != last) {\n+    assert(cursor != nullptr, \"Sanity check\");\n+    if (cursor->kind() == LayoutRawBlock::EMPTY && cursor->fit(4, 1)) {\n+      break;\n+    }\n+    cursor = cursor->next_block();\n+  }\n+  return cursor->offset();\n+}\n+\n@@ -700,0 +717,3 @@\n+  if (UseCompactObjectHeaders) {\n+    _info->_hash_offset   = _layout->find_hash_offset();\n+  }\n","filename":"src\/hotspot\/share\/classfile\/fieldLayoutBuilder.cpp","additions":20,"deletions":0,"binary":false,"changes":20,"status":"modified"},{"patch":"@@ -1085,1 +1085,1 @@\n-  oop mirror_oop = InstanceMirrorKlass::cast(vmClasses::Class_klass())->allocate_instance(k, CHECK);\n+  oop mirror_oop = InstanceMirrorKlass::cast(vmClasses::Class_klass())->allocate_instance(k, is_scratch, CHECK);\n@@ -1382,1 +1382,1 @@\n-oop java_lang_Class::create_basic_type_mirror(const char* basic_type_name, BasicType type, TRAPS) {\n+oop java_lang_Class::create_basic_type_mirror(const char* basic_type_name, BasicType type, bool is_scratch, TRAPS) {\n@@ -1384,1 +1384,1 @@\n-  oop java_class = InstanceMirrorKlass::cast(vmClasses::Class_klass())->allocate_instance(nullptr, CHECK_NULL);\n+  oop java_class = InstanceMirrorKlass::cast(vmClasses::Class_klass())->allocate_instance(nullptr, is_scratch, CHECK_NULL);\n","filename":"src\/hotspot\/share\/classfile\/javaClasses.cpp","additions":3,"deletions":3,"binary":false,"changes":6,"status":"modified"},{"patch":"@@ -292,1 +292,1 @@\n-  static oop  create_basic_type_mirror(const char* basic_type_name, BasicType type, TRAPS);\n+  static oop  create_basic_type_mirror(const char* basic_type_name, BasicType type, bool is_scratch, TRAPS);\n","filename":"src\/hotspot\/share\/classfile\/javaClasses.hpp","additions":1,"deletions":1,"binary":false,"changes":2,"status":"modified"},{"patch":"@@ -288,1 +288,1 @@\n-  assert(java_lang_Class::is_instance(java_class), \"must be a Class object\");\n+  \/\/assert(java_lang_Class::is_instance(java_class), \"must be a Class object\");\n","filename":"src\/hotspot\/share\/classfile\/javaClasses.inline.hpp","additions":1,"deletions":1,"binary":false,"changes":2,"status":"modified"},{"patch":"@@ -36,1 +36,0 @@\n-#include \"gc\/shared\/fullGCForwarding.hpp\"\n@@ -254,2 +253,0 @@\n-\n-  FullGCForwarding::initialize_flags(heap_reserved_size_bytes());\n","filename":"src\/hotspot\/share\/gc\/g1\/g1Arguments.cpp","additions":0,"deletions":3,"binary":false,"changes":3,"status":"modified"},{"patch":"@@ -78,1 +78,1 @@\n-#include \"gc\/shared\/fullGCForwarding.hpp\"\n+#include \"gc\/shared\/fullGCForwarding.inline.hpp\"\n","filename":"src\/hotspot\/share\/gc\/g1\/g1CollectedHeap.cpp","additions":1,"deletions":1,"binary":false,"changes":2,"status":"modified"},{"patch":"@@ -214,0 +214,2 @@\n+  FullGCForwarding::begin();\n+\n@@ -226,0 +228,2 @@\n+  FullGCForwarding::end();\n+\n","filename":"src\/hotspot\/share\/gc\/g1\/g1FullCollector.cpp","additions":4,"deletions":0,"binary":false,"changes":4,"status":"modified"},{"patch":"@@ -62,0 +62,1 @@\n+  assert(obj_addr != destination, \"only copy actually-moving objects\");\n@@ -66,0 +67,1 @@\n+  cast_to_oop(destination)->initialize_hash_if_necessary(obj);\n","filename":"src\/hotspot\/share\/gc\/g1\/g1FullGCCompactTask.cpp","additions":2,"deletions":0,"binary":false,"changes":2,"status":"modified"},{"patch":"@@ -484,1 +484,2 @@\n-  const size_t word_sz = old->size_given_klass(klass);\n+  const size_t old_size = old->size_given_mark_and_klass(old_mark, klass);\n+  const size_t word_sz = old->copy_size(old_size, old_mark);\n@@ -522,1 +523,1 @@\n-  Copy::aligned_disjoint_words(cast_from_oop<HeapWord*>(old), obj_ptr, word_sz);\n+  Copy::aligned_disjoint_words(cast_from_oop<HeapWord*>(old), obj_ptr, old_size);\n@@ -539,0 +540,2 @@\n+    obj->initialize_hash_if_necessary(old);\n+\n","filename":"src\/hotspot\/share\/gc\/g1\/g1ParScanThreadState.cpp","additions":5,"deletions":2,"binary":false,"changes":7,"status":"modified"},{"patch":"@@ -29,1 +29,0 @@\n-#include \"gc\/shared\/fullGCForwarding.hpp\"\n@@ -97,2 +96,0 @@\n-\n-  FullGCForwarding::initialize_flags(heap_reserved_size_bytes());\n","filename":"src\/hotspot\/share\/gc\/parallel\/parallelArguments.cpp","additions":0,"deletions":3,"binary":false,"changes":3,"status":"modified"},{"patch":"@@ -32,0 +32,1 @@\n+#include \"gc\/parallel\/psParallelCompactNew.inline.hpp\"\n@@ -121,2 +122,8 @@\n-  if (!PSParallelCompact::initialize_aux_data()) {\n-    return JNI_ENOMEM;\n+  if (UseCompactObjectHeaders) {\n+    if (!PSParallelCompactNew::initialize_aux_data()) {\n+      return JNI_ENOMEM;\n+    }\n+  } else {\n+    if (!PSParallelCompact::initialize_aux_data()) {\n+      return JNI_ENOMEM;\n+    }\n@@ -174,1 +181,5 @@\n-  PSParallelCompact::post_initialize();\n+  if (UseCompactObjectHeaders) {\n+    PSParallelCompactNew::post_initialize();\n+  } else {\n+    PSParallelCompact::post_initialize();\n+  }\n@@ -365,1 +376,5 @@\n-  PSParallelCompact::invoke(clear_all_soft_refs, should_do_max_compaction);\n+  if (UseCompactObjectHeaders) {\n+    PSParallelCompactNew::invoke(clear_all_soft_refs, should_do_max_compaction);\n+  } else {\n+    PSParallelCompact::invoke(clear_all_soft_refs, should_do_max_compaction);\n+  }\n@@ -493,1 +508,5 @@\n-    PSParallelCompact::invoke(clear_all_soft_refs, should_do_max_compaction);\n+    if (UseCompactObjectHeaders) {\n+      PSParallelCompactNew::invoke(clear_all_soft_refs, should_do_max_compaction);\n+    } else {\n+      PSParallelCompact::invoke(clear_all_soft_refs, should_do_max_compaction);\n+    }\n@@ -503,1 +522,0 @@\n-\n@@ -576,1 +594,5 @@\n-  PSParallelCompact::invoke(clear_soft_refs, should_do_max_compaction);\n+  if (UseCompactObjectHeaders) {\n+    PSParallelCompactNew::invoke(clear_soft_refs, should_do_max_compaction);\n+  } else {\n+    PSParallelCompact::invoke(clear_soft_refs, should_do_max_compaction);\n+  }\n@@ -719,1 +741,5 @@\n-  PSParallelCompact::print_on(st);\n+  if (UseCompactObjectHeaders) {\n+    PSParallelCompactNew::print_on(st);\n+  } else {\n+    PSParallelCompact::print_on(st);\n+  }\n@@ -727,1 +753,5 @@\n-  log_debug(gc, heap, exit)(\"Accumulated old generation GC time %3.7f secs\", PSParallelCompact::accumulated_time()->seconds());\n+  if (UseCompactObjectHeaders) {\n+    log_debug(gc, heap, exit)(\"Accumulated old generation GC time %3.7f secs\", PSParallelCompactNew::accumulated_time()->seconds());\n+  } else {\n+    log_debug(gc, heap, exit)(\"Accumulated old generation GC time %3.7f secs\", PSParallelCompact::accumulated_time()->seconds());\n+  }\n","filename":"src\/hotspot\/share\/gc\/parallel\/parallelScavengeHeap.cpp","additions":39,"deletions":9,"binary":false,"changes":48,"status":"modified"},{"patch":"@@ -0,0 +1,167 @@\n+\/*\n+ * Copyright (c) 2005, 2025, Oracle and\/or its affiliates. All rights reserved.\n+ * Copyright Amazon.com Inc. or its affiliates. All Rights Reserved.\n+ * DO NOT ALTER OR REMOVE COPYRIGHT NOTICES OR THIS FILE HEADER.\n+ *\n+ * This code is free software; you can redistribute it and\/or modify it\n+ * under the terms of the GNU General Public License version 2 only, as\n+ * published by the Free Software Foundation.\n+ *\n+ * This code is distributed in the hope that it will be useful, but WITHOUT\n+ * ANY WARRANTY; without even the implied warranty of MERCHANTABILITY or\n+ * FITNESS FOR A PARTICULAR PURPOSE.  See the GNU General Public License\n+ * version 2 for more details (a copy is included in the LICENSE file that\n+ * accompanied this code).\n+ *\n+ * You should have received a copy of the GNU General Public License version\n+ * 2 along with this work; if not, write to the Free Software Foundation,\n+ * Inc., 51 Franklin St, Fifth Floor, Boston, MA 02110-1301 USA.\n+ *\n+ * Please contact Oracle, 500 Oracle Parkway, Redwood Shores, CA 94065 USA\n+ * or visit www.oracle.com if you need additional information or have any\n+ * questions.\n+ *\n+ *\/\n+\n+#include \"gc\/parallel\/objectStartArray.hpp\"\n+#include \"gc\/parallel\/parallelScavengeHeap.hpp\"\n+#include \"gc\/parallel\/parMarkBitMap.inline.hpp\"\n+#include \"gc\/parallel\/psCompactionManagerNew.inline.hpp\"\n+#include \"gc\/parallel\/psOldGen.hpp\"\n+#include \"gc\/parallel\/psParallelCompactNew.inline.hpp\"\n+#include \"gc\/shared\/partialArraySplitter.inline.hpp\"\n+#include \"gc\/shared\/partialArrayState.hpp\"\n+#include \"gc\/shared\/preservedMarks.inline.hpp\"\n+#include \"memory\/iterator.inline.hpp\"\n+\n+PSOldGen*                  ParCompactionManagerNew::_old_gen = nullptr;\n+ParCompactionManagerNew**  ParCompactionManagerNew::_manager_array = nullptr;\n+\n+ParCompactionManagerNew::PSMarkTasksQueueSet*  ParCompactionManagerNew::_marking_stacks = nullptr;\n+PartialArrayStateManager* ParCompactionManagerNew::_partial_array_state_manager = nullptr;\n+\n+ObjectStartArray*    ParCompactionManagerNew::_start_array = nullptr;\n+ParMarkBitMap*       ParCompactionManagerNew::_mark_bitmap = nullptr;\n+\n+PreservedMarksSet* ParCompactionManagerNew::_preserved_marks_set = nullptr;\n+\n+ParCompactionManagerNew::ParCompactionManagerNew(PreservedMarks* preserved_marks,\n+                                           ReferenceProcessor* ref_processor,\n+                                           uint parallel_gc_threads)\n+  :_partial_array_splitter(_partial_array_state_manager, parallel_gc_threads, ObjArrayMarkingStride),\n+   _mark_and_push_closure(this, ref_processor) {\n+\n+  _old_gen = ParallelScavengeHeap::old_gen();\n+  _start_array = old_gen()->start_array();\n+\n+  _preserved_marks = preserved_marks;\n+}\n+\n+void ParCompactionManagerNew::initialize(ParMarkBitMap* mbm) {\n+  assert(ParallelScavengeHeap::heap() != nullptr, \"Needed for initialization\");\n+  assert(PSParallelCompactNew::ref_processor() != nullptr, \"precondition\");\n+  assert(ParallelScavengeHeap::heap()->workers().max_workers() != 0, \"Not initialized?\");\n+\n+  _mark_bitmap = mbm;\n+\n+  uint parallel_gc_threads = ParallelScavengeHeap::heap()->workers().max_workers();\n+\n+  assert(_manager_array == nullptr, \"Attempt to initialize twice\");\n+  _manager_array = NEW_C_HEAP_ARRAY(ParCompactionManagerNew*, parallel_gc_threads, mtGC);\n+\n+  assert(_partial_array_state_manager == nullptr, \"Attempt to initialize twice\");\n+  _partial_array_state_manager\n+    = new PartialArrayStateManager(parallel_gc_threads);\n+  _marking_stacks = new PSMarkTasksQueueSet(parallel_gc_threads);\n+\n+  _preserved_marks_set = new PreservedMarksSet(true);\n+  _preserved_marks_set->init(parallel_gc_threads);\n+\n+  \/\/ Create and register the ParCompactionManagerNew(s) for the worker threads.\n+  for(uint i=0; i<parallel_gc_threads; i++) {\n+    _manager_array[i] = new ParCompactionManagerNew(_preserved_marks_set->get(i),\n+                                                 PSParallelCompactNew::ref_processor(),\n+                                                 parallel_gc_threads);\n+    marking_stacks()->register_queue(i, _manager_array[i]->marking_stack());\n+  }\n+}\n+\n+void ParCompactionManagerNew::flush_all_string_dedup_requests() {\n+  uint parallel_gc_threads = ParallelScavengeHeap::heap()->workers().max_workers();\n+  for (uint i=0; i<parallel_gc_threads; i++) {\n+    _manager_array[i]->flush_string_dedup_requests();\n+  }\n+}\n+\n+ParCompactionManagerNew*\n+ParCompactionManagerNew::gc_thread_compaction_manager(uint index) {\n+  assert(index < ParallelGCThreads, \"index out of range\");\n+  assert(_manager_array != nullptr, \"Sanity\");\n+  return _manager_array[index];\n+}\n+\n+void ParCompactionManagerNew::push_objArray(oop obj) {\n+  assert(obj->is_objArray(), \"precondition\");\n+  _mark_and_push_closure.do_klass(obj->klass());\n+\n+  objArrayOop obj_array = objArrayOop(obj);\n+  size_t array_length = obj_array->length();\n+  size_t initial_chunk_size =\n+    _partial_array_splitter.start(&_marking_stack, obj_array, nullptr, array_length);\n+  follow_array(obj_array, 0, initial_chunk_size);\n+}\n+\n+void ParCompactionManagerNew::process_array_chunk(PartialArrayState* state, bool stolen) {\n+  \/\/ Access before release by claim().\n+  oop obj = state->source();\n+  PartialArraySplitter::Claim claim =\n+    _partial_array_splitter.claim(state, &_marking_stack, stolen);\n+  follow_array(objArrayOop(obj), claim._start, claim._end);\n+}\n+\n+void ParCompactionManagerNew::follow_marking_stacks() {\n+  ScannerTask task;\n+  do {\n+    \/\/ First, try to move tasks from the overflow stack into the shared buffer, so\n+    \/\/ that other threads can steal. Otherwise process the overflow stack first.\n+    while (marking_stack()->pop_overflow(task)) {\n+      if (!marking_stack()->try_push_to_taskqueue(task)) {\n+        follow_contents(task, false);\n+      }\n+    }\n+    while (marking_stack()->pop_local(task)) {\n+      follow_contents(task, false);\n+    }\n+  } while (!marking_stack_empty());\n+\n+  assert(marking_stack_empty(), \"Sanity\");\n+}\n+\n+#if TASKQUEUE_STATS\n+void ParCompactionManagerNew::print_and_reset_taskqueue_stats() {\n+  marking_stacks()->print_and_reset_taskqueue_stats(\"Marking Stacks\");\n+\n+  auto get_pa_stats = [&](uint i) {\n+    return _manager_array[i]->partial_array_task_stats();\n+  };\n+  PartialArrayTaskStats::log_set(ParallelGCThreads, get_pa_stats,\n+                                 \"Partial Array Task Stats\");\n+  uint parallel_gc_threads = ParallelScavengeHeap::heap()->workers().max_workers();\n+  for (uint i = 0; i < parallel_gc_threads; ++i) {\n+    get_pa_stats(i)->reset();\n+  }\n+}\n+\n+PartialArrayTaskStats* ParCompactionManagerNew::partial_array_task_stats() {\n+  return _partial_array_splitter.stats();\n+}\n+#endif \/\/ TASKQUEUE_STATS\n+\n+#ifdef ASSERT\n+void ParCompactionManagerNew::verify_all_marking_stack_empty() {\n+  uint parallel_gc_threads = ParallelGCThreads;\n+  for (uint i = 0; i < parallel_gc_threads; i++) {\n+    assert(_manager_array[i]->marking_stack_empty(), \"Marking stack should be empty\");\n+  }\n+}\n+#endif\n","filename":"src\/hotspot\/share\/gc\/parallel\/psCompactionManagerNew.cpp","additions":167,"deletions":0,"binary":false,"changes":167,"status":"added"},{"patch":"@@ -0,0 +1,165 @@\n+\/*\n+ * Copyright (c) 2005, 2024, Oracle and\/or its affiliates. All rights reserved.\n+ * Copyright Amazon.com Inc. or its affiliates. All Rights Reserved.\n+ * DO NOT ALTER OR REMOVE COPYRIGHT NOTICES OR THIS FILE HEADER.\n+ *\n+ * This code is free software; you can redistribute it and\/or modify it\n+ * under the terms of the GNU General Public License version 2 only, as\n+ * published by the Free Software Foundation.\n+ *\n+ * This code is distributed in the hope that it will be useful, but WITHOUT\n+ * ANY WARRANTY; without even the implied warranty of MERCHANTABILITY or\n+ * FITNESS FOR A PARTICULAR PURPOSE.  See the GNU General Public License\n+ * version 2 for more details (a copy is included in the LICENSE file that\n+ * accompanied this code).\n+ *\n+ * You should have received a copy of the GNU General Public License version\n+ * 2 along with this work; if not, write to the Free Software Foundation,\n+ * Inc., 51 Franklin St, Fifth Floor, Boston, MA 02110-1301 USA.\n+ *\n+ * Please contact Oracle, 500 Oracle Parkway, Redwood Shores, CA 94065 USA\n+ * or visit www.oracle.com if you need additional information or have any\n+ * questions.\n+ *\n+ *\/\n+\n+#ifndef SHARE_GC_PARALLEL_PSCOMPACTIONMANAGERNEW_HPP\n+#define SHARE_GC_PARALLEL_PSCOMPACTIONMANAGERNEW_HPP\n+\n+#include \"classfile\/classLoaderData.hpp\"\n+#include \"gc\/parallel\/psParallelCompactNew.hpp\"\n+#include \"gc\/shared\/partialArraySplitter.hpp\"\n+#include \"gc\/shared\/partialArrayState.hpp\"\n+#include \"gc\/shared\/partialArrayTaskStats.hpp\"\n+#include \"gc\/shared\/preservedMarks.hpp\"\n+#include \"gc\/shared\/stringdedup\/stringDedup.hpp\"\n+#include \"gc\/shared\/taskqueue.hpp\"\n+#include \"gc\/shared\/taskTerminator.hpp\"\n+#include \"memory\/allocation.hpp\"\n+#include \"utilities\/stack.hpp\"\n+\n+class MutableSpace;\n+class PSOldGen;\n+class ParCompactionManagerNew;\n+class ObjectStartArray;\n+class ParMarkBitMap;\n+\n+class PCMarkAndPushClosureNew: public ClaimMetadataVisitingOopIterateClosure {\n+  ParCompactionManagerNew* _compaction_manager;\n+\n+  template <typename T> void do_oop_work(T* p);\n+public:\n+  PCMarkAndPushClosureNew(ParCompactionManagerNew* cm, ReferenceProcessor* rp) :\n+    ClaimMetadataVisitingOopIterateClosure(ClassLoaderData::_claim_stw_fullgc_mark, rp),\n+    _compaction_manager(cm) { }\n+\n+  void do_oop(oop* p) final               { do_oop_work(p); }\n+  void do_oop(narrowOop* p) final         { do_oop_work(p); }\n+};\n+\n+class ParCompactionManagerNew : public CHeapObj<mtGC> {\n+  friend class MarkFromRootsTaskNew;\n+  friend class ParallelCompactRefProcProxyTaskNew;\n+  friend class ParallelScavengeRefProcProxyTask;\n+  friend class ParMarkBitMap;\n+  friend class PSParallelCompactNew;\n+  friend class PCAddThreadRootsMarkingTaskClosureNew;\n+\n+ private:\n+  typedef OverflowTaskQueue<ScannerTask, mtGC>           PSMarkTaskQueue;\n+  typedef GenericTaskQueueSet<PSMarkTaskQueue, mtGC>     PSMarkTasksQueueSet;\n+\n+  static ParCompactionManagerNew** _manager_array;\n+  static PSMarkTasksQueueSet*   _marking_stacks;\n+  static ObjectStartArray*      _start_array;\n+  static PSOldGen*              _old_gen;\n+\n+  static PartialArrayStateManager*  _partial_array_state_manager;\n+  PartialArraySplitter              _partial_array_splitter;\n+\n+  PSMarkTaskQueue               _marking_stack;\n+\n+  PCMarkAndPushClosureNew _mark_and_push_closure;\n+\n+  static PreservedMarksSet* _preserved_marks_set;\n+  PreservedMarks* _preserved_marks;\n+\n+  static ParMarkBitMap* _mark_bitmap;\n+\n+  StringDedup::Requests _string_dedup_requests;\n+\n+  static PSOldGen* old_gen()             { return _old_gen; }\n+  static ObjectStartArray* start_array() { return _start_array; }\n+  static PSMarkTasksQueueSet* marking_stacks()  { return _marking_stacks; }\n+\n+  static void initialize(ParMarkBitMap* mbm);\n+\n+  ParCompactionManagerNew(PreservedMarks* preserved_marks,\n+                       ReferenceProcessor* ref_processor,\n+                       uint parallel_gc_threads);\n+\n+  inline PSMarkTaskQueue*  marking_stack() { return &_marking_stack; }\n+  inline void push(PartialArrayState* stat);\n+  void push_objArray(oop obj);\n+\n+#if TASKQUEUE_STATS\n+  static void print_and_reset_taskqueue_stats();\n+  PartialArrayTaskStats* partial_array_task_stats();\n+#endif \/\/ TASKQUEUE_STATS\n+\n+public:\n+  void flush_string_dedup_requests() {\n+    _string_dedup_requests.flush();\n+  }\n+\n+  static void flush_all_string_dedup_requests();\n+\n+  \/\/ Get the compaction manager when doing evacuation work from the VM thread.\n+  \/\/ Simply use the first compaction manager here.\n+  static ParCompactionManagerNew* get_vmthread_cm() { return _manager_array[0]; }\n+\n+  PreservedMarks* preserved_marks() const {\n+    return _preserved_marks;\n+  }\n+\n+  static ParMarkBitMap* mark_bitmap() { return _mark_bitmap; }\n+\n+  \/\/ Save for later processing.  Must not fail.\n+  inline void push(oop obj);\n+\n+  \/\/ Check mark and maybe push on marking stack.\n+  template <typename T> inline void mark_and_push(T* p);\n+\n+  \/\/ Access function for compaction managers\n+  static ParCompactionManagerNew* gc_thread_compaction_manager(uint index);\n+\n+  static bool steal(uint queue_num, ScannerTask& t);\n+\n+  \/\/ Process tasks remaining on marking stack\n+  void follow_marking_stacks();\n+  inline bool marking_stack_empty() const;\n+\n+  inline void follow_contents(const ScannerTask& task, bool stolen);\n+  inline void follow_array(objArrayOop array, size_t start, size_t end);\n+  void process_array_chunk(PartialArrayState* state, bool stolen);\n+\n+  class FollowStackClosure: public VoidClosure {\n+   private:\n+    ParCompactionManagerNew* _compaction_manager;\n+    TaskTerminator* _terminator;\n+    uint _worker_id;\n+   public:\n+    FollowStackClosure(ParCompactionManagerNew* cm, TaskTerminator* terminator, uint worker_id)\n+      : _compaction_manager(cm), _terminator(terminator), _worker_id(worker_id) { }\n+    void do_void() final;\n+  };\n+\n+  \/\/ Called after marking.\n+  static void verify_all_marking_stack_empty() NOT_DEBUG_RETURN;\n+};\n+\n+bool ParCompactionManagerNew::marking_stack_empty() const {\n+  return _marking_stack.is_empty();\n+}\n+\n+#endif \/\/ SHARE_GC_PARALLEL_PSCOMPACTIONMANAGERNEW_HPP\n","filename":"src\/hotspot\/share\/gc\/parallel\/psCompactionManagerNew.hpp","additions":165,"deletions":0,"binary":false,"changes":165,"status":"added"},{"patch":"@@ -530,1 +530,1 @@\n-class PCAdjustPointerClosure: public BasicOopIterateClosure {\n+class PCAdjustPointerClosureNew: public BasicOopIterateClosure {\n@@ -541,1 +541,1 @@\n-static PCAdjustPointerClosure pc_adjust_pointer_closure;\n+static PCAdjustPointerClosureNew pc_adjust_pointer_closure;\n@@ -997,0 +997,2 @@\n+    FullGCForwarding::begin();\n+\n@@ -1003,0 +1005,2 @@\n+    FullGCForwarding::end();\n+\n","filename":"src\/hotspot\/share\/gc\/parallel\/psParallelCompact.cpp","additions":6,"deletions":2,"binary":false,"changes":8,"status":"modified"},{"patch":"@@ -0,0 +1,1054 @@\n+\/*\n+ * Copyright (c) 2005, 2025, Oracle and\/or its affiliates. All rights reserved.\n+ * Copyright Amazon.com Inc. or its affiliates. All Rights Reserved.\n+ * DO NOT ALTER OR REMOVE COPYRIGHT NOTICES OR THIS FILE HEADER.\n+ *\n+ * This code is free software; you can redistribute it and\/or modify it\n+ * under the terms of the GNU General Public License version 2 only, as\n+ * published by the Free Software Foundation.\n+ *\n+ * This code is distributed in the hope that it will be useful, but WITHOUT\n+ * ANY WARRANTY; without even the implied warranty of MERCHANTABILITY or\n+ * FITNESS FOR A PARTICULAR PURPOSE.  See the GNU General Public License\n+ * version 2 for more details (a copy is included in the LICENSE file that\n+ * accompanied this code).\n+ *\n+ * You should have received a copy of the GNU General Public License version\n+ * 2 along with this work; if not, write to the Free Software Foundation,\n+ * Inc., 51 Franklin St, Fifth Floor, Boston, MA 02110-1301 USA.\n+ *\n+ * Please contact Oracle, 500 Oracle Parkway, Redwood Shores, CA 94065 USA\n+ * or visit www.oracle.com if you need additional information or have any\n+ * questions.\n+ *\n+ *\/\n+\n+#include \"classfile\/classLoaderDataGraph.hpp\"\n+#include \"classfile\/symbolTable.hpp\"\n+#include \"classfile\/systemDictionary.hpp\"\n+#include \"code\/codeCache.hpp\"\n+#include \"compiler\/oopMap.hpp\"\n+#include \"gc\/parallel\/objectStartArray.inline.hpp\"\n+#include \"gc\/parallel\/parallelArguments.hpp\"\n+#include \"gc\/parallel\/parallelScavengeHeap.inline.hpp\"\n+#include \"gc\/parallel\/parMarkBitMap.inline.hpp\"\n+#include \"gc\/parallel\/psAdaptiveSizePolicy.hpp\"\n+#include \"gc\/parallel\/psCompactionManagerNew.inline.hpp\"\n+#include \"gc\/parallel\/psOldGen.hpp\"\n+#include \"gc\/parallel\/psParallelCompactNew.inline.hpp\"\n+#include \"gc\/parallel\/psPromotionManager.inline.hpp\"\n+#include \"gc\/parallel\/psScavenge.hpp\"\n+#include \"gc\/parallel\/psYoungGen.hpp\"\n+#include \"gc\/shared\/classUnloadingContext.hpp\"\n+#include \"gc\/shared\/fullGCForwarding.inline.hpp\"\n+#include \"gc\/shared\/gcCause.hpp\"\n+#include \"gc\/shared\/gcHeapSummary.hpp\"\n+#include \"gc\/shared\/gcId.hpp\"\n+#include \"gc\/shared\/gcLocker.hpp\"\n+#include \"gc\/shared\/gcTimer.hpp\"\n+#include \"gc\/shared\/gcTrace.hpp\"\n+#include \"gc\/shared\/gcTraceTime.inline.hpp\"\n+#include \"gc\/shared\/gcVMOperations.hpp\"\n+#include \"gc\/shared\/isGCActiveMark.hpp\"\n+#include \"gc\/shared\/oopStorageSetParState.inline.hpp\"\n+#include \"gc\/shared\/preservedMarks.inline.hpp\"\n+#include \"gc\/shared\/referencePolicy.hpp\"\n+#include \"gc\/shared\/referenceProcessor.hpp\"\n+#include \"gc\/shared\/referenceProcessorPhaseTimes.hpp\"\n+#include \"gc\/shared\/spaceDecorator.hpp\"\n+#include \"gc\/shared\/taskTerminator.hpp\"\n+#include \"gc\/shared\/weakProcessor.inline.hpp\"\n+#include \"gc\/shared\/workerPolicy.hpp\"\n+#include \"gc\/shared\/workerThread.hpp\"\n+#include \"gc\/shared\/workerUtils.hpp\"\n+#include \"logging\/log.hpp\"\n+#include \"memory\/iterator.inline.hpp\"\n+#include \"memory\/memoryReserver.hpp\"\n+#include \"memory\/metaspaceUtils.hpp\"\n+#include \"memory\/resourceArea.hpp\"\n+#include \"memory\/universe.hpp\"\n+#include \"nmt\/memTracker.hpp\"\n+#include \"oops\/methodData.hpp\"\n+#include \"runtime\/java.hpp\"\n+#include \"runtime\/safepoint.hpp\"\n+#include \"runtime\/threads.hpp\"\n+#include \"runtime\/vmThread.hpp\"\n+#include \"services\/memoryService.hpp\"\n+#include \"utilities\/align.hpp\"\n+#include \"utilities\/debug.hpp\"\n+#include \"utilities\/events.hpp\"\n+#include \"utilities\/formatBuffer.hpp\"\n+#include \"utilities\/macros.hpp\"\n+#if INCLUDE_JVMCI\n+#include \"jvmci\/jvmci.hpp\"\n+#endif\n+\n+SpaceInfoNew PSParallelCompactNew::_space_info[PSParallelCompactNew::last_space_id];\n+\n+size_t PSParallelCompactNew::_num_regions;\n+PCRegionData* PSParallelCompactNew::_region_data_array;\n+size_t PSParallelCompactNew::_num_regions_serial;\n+PCRegionData* PSParallelCompactNew::_region_data_array_serial;\n+PCRegionData** PSParallelCompactNew::_per_worker_region_data;\n+bool PSParallelCompactNew::_serial = false;\n+\n+SpanSubjectToDiscoveryClosure PSParallelCompactNew::_span_based_discoverer;\n+ReferenceProcessor* PSParallelCompactNew::_ref_processor = nullptr;\n+\n+void PSParallelCompactNew::print_on(outputStream* st) {\n+  _mark_bitmap.print_on(st);\n+}\n+\n+STWGCTimer          PSParallelCompactNew::_gc_timer;\n+ParallelOldTracer   PSParallelCompactNew::_gc_tracer;\n+elapsedTimer        PSParallelCompactNew::_accumulated_time;\n+unsigned int        PSParallelCompactNew::_maximum_compaction_gc_num = 0;\n+CollectorCounters*  PSParallelCompactNew::_counters = nullptr;\n+ParMarkBitMap       PSParallelCompactNew::_mark_bitmap;\n+\n+PSParallelCompactNew::IsAliveClosure PSParallelCompactNew::_is_alive_closure;\n+\n+class PCAdjustPointerClosure: public BasicOopIterateClosure {\n+  template <typename T>\n+  void do_oop_work(T* p) { PSParallelCompactNew::adjust_pointer(p); }\n+\n+public:\n+  void do_oop(oop* p) final          { do_oop_work(p); }\n+  void do_oop(narrowOop* p) final    { do_oop_work(p); }\n+\n+  ReferenceIterationMode reference_iteration_mode() final { return DO_FIELDS; }\n+};\n+\n+static PCAdjustPointerClosure pc_adjust_pointer_closure;\n+\n+class IsAliveClosure: public BoolObjectClosure {\n+public:\n+  bool do_object_b(oop p) final;\n+};\n+\n+\n+bool PSParallelCompactNew::IsAliveClosure::do_object_b(oop p) { return mark_bitmap()->is_marked(p); }\n+\n+void PSParallelCompactNew::post_initialize() {\n+  ParallelScavengeHeap* heap = ParallelScavengeHeap::heap();\n+  _span_based_discoverer.set_span(heap->reserved_region());\n+  _ref_processor =\n+    new ReferenceProcessor(&_span_based_discoverer,\n+                           ParallelGCThreads,   \/\/ mt processing degree\n+                           ParallelGCThreads,   \/\/ mt discovery degree\n+                           false,               \/\/ concurrent_discovery\n+                           &_is_alive_closure); \/\/ non-header is alive closure\n+\n+  _counters = new CollectorCounters(\"Parallel full collection pauses\", 1);\n+\n+  \/\/ Initialize static fields in ParCompactionManager.\n+  ParCompactionManagerNew::initialize(mark_bitmap());\n+}\n+\n+bool PSParallelCompactNew::initialize_aux_data() {\n+  ParallelScavengeHeap* heap = ParallelScavengeHeap::heap();\n+  MemRegion mr = heap->reserved_region();\n+  assert(mr.byte_size() != 0, \"heap should be reserved\");\n+\n+  initialize_space_info();\n+\n+  if (!_mark_bitmap.initialize(mr)) {\n+    vm_shutdown_during_initialization(\n+      err_msg(\"Unable to allocate %zuKB bitmaps for parallel \"\n+      \"garbage collection for the requested %zuKB heap.\",\n+      _mark_bitmap.reserved_byte_size()\/K, mr.byte_size()\/K));\n+    return false;\n+  }\n+\n+  return true;\n+}\n+\n+void PSParallelCompactNew::initialize_space_info()\n+{\n+  memset(&_space_info, 0, sizeof(_space_info));\n+\n+  PSYoungGen* young_gen = ParallelScavengeHeap::young_gen();\n+\n+  _space_info[old_space_id].set_space(ParallelScavengeHeap::old_gen()->object_space());\n+  _space_info[eden_space_id].set_space(young_gen->eden_space());\n+  _space_info[from_space_id].set_space(young_gen->from_space());\n+  _space_info[to_space_id].set_space(young_gen->to_space());\n+\n+  _space_info[old_space_id].set_start_array(ParallelScavengeHeap::old_gen()->start_array());\n+}\n+\n+void\n+PSParallelCompactNew::clear_data_covering_space(SpaceId id)\n+{\n+  \/\/ At this point, top is the value before GC, new_top() is the value that will\n+  \/\/ be set at the end of GC.  The marking bitmap is cleared to top; nothing\n+  \/\/ should be marked above top.\n+  MutableSpace* const space = _space_info[id].space();\n+  HeapWord* const bot = space->bottom();\n+  HeapWord* const top = space->top();\n+\n+  _mark_bitmap.clear_range(bot, top);\n+}\n+\n+void PSParallelCompactNew::pre_compact()\n+{\n+  \/\/ Update the from & to space pointers in space_info, since they are swapped\n+  \/\/ at each young gen gc.  Do the update unconditionally (even though a\n+  \/\/ promotion failure does not swap spaces) because an unknown number of young\n+  \/\/ collections will have swapped the spaces an unknown number of times.\n+  GCTraceTime(Debug, gc, phases) tm(\"Pre Compact\", &_gc_timer);\n+  ParallelScavengeHeap* heap = ParallelScavengeHeap::heap();\n+  _space_info[from_space_id].set_space(ParallelScavengeHeap::young_gen()->from_space());\n+  _space_info[to_space_id].set_space(ParallelScavengeHeap::young_gen()->to_space());\n+\n+  \/\/ Increment the invocation count\n+  heap->increment_total_collections(true);\n+\n+  CodeCache::on_gc_marking_cycle_start();\n+\n+  heap->print_before_gc();\n+  heap->trace_heap_before_gc(&_gc_tracer);\n+\n+  \/\/ Fill in TLABs\n+  heap->ensure_parsability(true);  \/\/ retire TLABs\n+\n+  if (VerifyBeforeGC && heap->total_collections() >= VerifyGCStartAt) {\n+    Universe::verify(\"Before GC\");\n+  }\n+\n+  DEBUG_ONLY(mark_bitmap()->verify_clear();)\n+}\n+\n+void PSParallelCompactNew::post_compact()\n+{\n+  GCTraceTime(Info, gc, phases) tm(\"Post Compact\", &_gc_timer);\n+\n+  CodeCache::on_gc_marking_cycle_finish();\n+  CodeCache::arm_all_nmethods();\n+\n+  for (unsigned int id = old_space_id; id < last_space_id; ++id) {\n+    \/\/ Clear the marking bitmap, summary data and split info.\n+    clear_data_covering_space(SpaceId(id));\n+  }\n+\n+  {\n+    PCRegionData* last_live[last_space_id];\n+    for (uint i = old_space_id; i < last_space_id; ++i) {\n+      last_live[i] = nullptr;\n+    }\n+\n+    \/\/ Figure out last region in each space that has live data.\n+    uint space_id = old_space_id;\n+    MutableSpace* space = _space_info[space_id].space();\n+    size_t num_regions = get_num_regions();\n+    PCRegionData* region_data_array = get_region_data_array();\n+    last_live[space_id] = &region_data_array[0];\n+    for (size_t idx = 0; idx < num_regions; idx++) {\n+      PCRegionData* rd = region_data_array + idx;\n+      if(!space->contains(rd->bottom())) {\n+        ++space_id;\n+        assert(space_id < last_space_id, \"invariant\");\n+        space = _space_info[space_id].space();\n+        log_develop_trace(gc, compaction)(\"Last live for space: %u: %zu\", space_id, idx);\n+        last_live[space_id] = rd;\n+      }\n+      assert(space->contains(rd->bottom()), \"next space should contain next region\");\n+      log_develop_trace(gc, compaction)(\"post-compact region: idx: %zu, bottom: \" PTR_FORMAT \", new_top: \" PTR_FORMAT \", end: \" PTR_FORMAT, rd->idx(), p2i(rd->bottom()), p2i(rd->new_top()), p2i(rd->end()));\n+      if (rd->new_top() > rd->bottom()) {\n+        last_live[space_id] = rd;\n+        log_develop_trace(gc, compaction)(\"Bump last live for space: %u\", space_id);\n+      }\n+    }\n+\n+    for (uint i = old_space_id; i < last_space_id; ++i) {\n+      PCRegionData* rd = last_live[i];\n+        log_develop_trace(gc, compaction)(\n+                \"Last live region in space: %u, compaction region, \" PTR_FORMAT \", #%zu: [\" PTR_FORMAT \", \" PTR_FORMAT \"), new_top: \" PTR_FORMAT,\n+                i, p2i(rd), rd->idx(),\n+                p2i(rd->bottom()), p2i(rd->end()), p2i(rd->new_top()));\n+    }\n+\n+    \/\/ Fill all gaps and update the space boundaries.\n+    space_id = old_space_id;\n+    space = _space_info[space_id].space();\n+    size_t total_live = 0;\n+    size_t total_waste = 0;\n+    for (size_t idx = 0; idx < num_regions; idx++) {\n+      PCRegionData* rd = &region_data_array[idx];\n+      PCRegionData* last_live_in_space = last_live[space_id];\n+      assert(last_live_in_space != nullptr, \"last live must not be null\");\n+      if (rd != last_live_in_space) {\n+        if (rd->new_top() < rd->end()) {\n+          ObjectStartArray* sa = start_array(SpaceId(space_id));\n+          if (sa != nullptr) {\n+            sa->update_for_block(rd->new_top(), rd->end());\n+          }\n+          ParallelScavengeHeap::heap()->fill_with_dummy_object(rd->new_top(), rd->end(), false);\n+        }\n+        size_t live = pointer_delta(rd->new_top(), rd->bottom());\n+        size_t waste = pointer_delta(rd->end(), rd->new_top());\n+        total_live += live;\n+        total_waste += waste;\n+        log_develop_trace(gc, compaction)(\n+                \"Live compaction region, #%zu: [\" PTR_FORMAT \", \" PTR_FORMAT \"), new_top: \" PTR_FORMAT \", live: %zu, waste: %zu\",\n+                rd->idx(),\n+                p2i(rd->bottom()), p2i(rd->end()), p2i(rd->new_top()), live, waste);\n+      } else {\n+        \/\/ Update top of space.\n+        space->set_top(rd->new_top());\n+        size_t live = pointer_delta(rd->new_top(), rd->bottom());\n+        total_live += live;\n+        log_develop_trace(gc, compaction)(\n+                \"Live compaction region, #%zu: [\" PTR_FORMAT \", \" PTR_FORMAT \"), new_top: \" PTR_FORMAT \", live: %zu, waste: %zu\",\n+                rd->idx(),\n+                p2i(rd->bottom()), p2i(rd->end()), p2i(rd->new_top()), live, size_t(0));\n+\n+        \/\/ Fast-Forward to next space.\n+        for (; idx < num_regions - 1; idx++) {\n+          rd = &region_data_array[idx + 1];\n+          if (!space->contains(rd->bottom())) {\n+            space_id++;\n+            assert(space_id < last_space_id, \"must be\");\n+            space = _space_info[space_id].space();\n+            assert(space->contains(rd->bottom()), \"space must contain region\");\n+            break;\n+          }\n+        }\n+      }\n+    }\n+    log_develop_debug(gc, compaction)(\"total live: %zu, total waste: %zu, ratio: %f\", total_live, total_waste, ((float)total_waste)\/((float)(total_live + total_waste)));\n+  }\n+  {\n+    FREE_C_HEAP_ARRAY(PCRegionData*, _per_worker_region_data);\n+    FREE_C_HEAP_ARRAY(PCRegionData, _region_data_array);\n+    FREE_C_HEAP_ARRAY(PCRegionData, _region_data_array_serial);\n+  }\n+#ifdef ASSERT\n+  {\n+    mark_bitmap()->verify_clear();\n+  }\n+#endif\n+\n+  ParCompactionManagerNew::flush_all_string_dedup_requests();\n+\n+  MutableSpace* const eden_space = _space_info[eden_space_id].space();\n+  MutableSpace* const from_space = _space_info[from_space_id].space();\n+  MutableSpace* const to_space   = _space_info[to_space_id].space();\n+\n+  ParallelScavengeHeap* heap = ParallelScavengeHeap::heap();\n+  bool eden_empty = eden_space->is_empty();\n+\n+  \/\/ Update heap occupancy information which is used as input to the soft ref\n+  \/\/ clearing policy at the next gc.\n+  Universe::heap()->update_capacity_and_used_at_gc();\n+\n+  bool young_gen_empty = eden_empty && from_space->is_empty() &&\n+    to_space->is_empty();\n+\n+  PSCardTable* ct = heap->card_table();\n+  MemRegion old_mr = ParallelScavengeHeap::old_gen()->committed();\n+  if (young_gen_empty) {\n+    ct->clear_MemRegion(old_mr);\n+  } else {\n+    ct->dirty_MemRegion(old_mr);\n+  }\n+\n+  {\n+    \/\/ Delete metaspaces for unloaded class loaders and clean up loader_data graph\n+    GCTraceTime(Debug, gc, phases) t(\"Purge Class Loader Data\", gc_timer());\n+    ClassLoaderDataGraph::purge(true \/* at_safepoint *\/);\n+    DEBUG_ONLY(MetaspaceUtils::verify();)\n+  }\n+\n+  \/\/ Need to clear claim bits for the next mark.\n+  ClassLoaderDataGraph::clear_claimed_marks();\n+\n+  heap->prune_scavengable_nmethods();\n+\n+#if COMPILER2_OR_JVMCI\n+  DerivedPointerTable::update_pointers();\n+#endif\n+\n+  \/\/ Signal that we have completed a visit to all live objects.\n+  Universe::heap()->record_whole_heap_examined_timestamp();\n+}\n+\n+void PSParallelCompactNew::setup_regions_parallel() {\n+  static const size_t REGION_SIZE_WORDS = (SpaceAlignment \/ HeapWordSize);\n+  size_t num_regions = 0;\n+  for (uint i = old_space_id; i < last_space_id; ++i) {\n+    MutableSpace* const space = _space_info[i].space();\n+    size_t const space_size_words = space->capacity_in_words();\n+    num_regions += align_up(space_size_words, REGION_SIZE_WORDS) \/ REGION_SIZE_WORDS;\n+  }\n+  _region_data_array = NEW_C_HEAP_ARRAY(PCRegionData, num_regions, mtGC);\n+\n+  size_t region_idx = 0;\n+  for (uint i = old_space_id; i < last_space_id; ++i) {\n+    const MutableSpace* space = _space_info[i].space();\n+    HeapWord* addr = space->bottom();\n+    HeapWord* sp_end = space->end();\n+    HeapWord* sp_top = space->top();\n+    while (addr < sp_end) {\n+      HeapWord* end = MIN2(align_up(addr + REGION_SIZE_WORDS, REGION_SIZE_WORDS), space->end());\n+      if (addr < sp_top) {\n+        HeapWord* prev_obj_start = _mark_bitmap.find_obj_beg_reverse(addr, end);\n+        if (prev_obj_start < end) {\n+          HeapWord* prev_obj_end = prev_obj_start + cast_to_oop(prev_obj_start)->size();\n+          if (end < prev_obj_end) {\n+            \/\/ Object crosses region boundary, adjust end to be after object's last word.\n+            end = prev_obj_end;\n+          }\n+        }\n+      }\n+      assert(region_idx < num_regions, \"must not exceed number of regions: region_idx: %zu, num_regions: %zu\", region_idx, num_regions);\n+      HeapWord* top;\n+      if (sp_top < addr) {\n+        top = addr;\n+      } else if (sp_top >= end) {\n+        top = end;\n+      } else {\n+        top = sp_top;\n+      }\n+      assert(ParallelScavengeHeap::heap()->is_in_reserved(addr), \"addr must be in heap: \" PTR_FORMAT, p2i(addr));\n+      new (_region_data_array + region_idx) PCRegionData(region_idx, addr, top, end);\n+      addr = end;\n+      region_idx++;\n+    }\n+  }\n+  _num_regions = region_idx;\n+  log_info(gc)(\"Number of regions: %zu\", _num_regions);\n+}\n+\n+void PSParallelCompactNew::setup_regions_serial() {\n+  _num_regions_serial = last_space_id;\n+  _region_data_array_serial = NEW_C_HEAP_ARRAY(PCRegionData, _num_regions_serial, mtGC);\n+  new (_region_data_array_serial + old_space_id)  PCRegionData(old_space_id, space(old_space_id)->bottom(), space(old_space_id)->top(), space(old_space_id)->end());\n+  new (_region_data_array_serial + eden_space_id) PCRegionData(eden_space_id, space(eden_space_id)->bottom(), space(eden_space_id)->top(), space(eden_space_id)->end());\n+  new (_region_data_array_serial + from_space_id) PCRegionData(from_space_id, space(from_space_id)->bottom(), space(from_space_id)->top(), space(from_space_id)->end());\n+  new (_region_data_array_serial + to_space_id)   PCRegionData(to_space_id, space(to_space_id)->bottom(), space(to_space_id)->top(), space(to_space_id)->end());\n+}\n+\n+bool PSParallelCompactNew::check_maximum_compaction(bool should_do_max_compaction) {\n+\n+  ParallelScavengeHeap* heap = ParallelScavengeHeap::heap();\n+\n+  \/\/ Check System.GC\n+  bool is_max_on_system_gc = UseMaximumCompactionOnSystemGC\n+                          && GCCause::is_user_requested_gc(heap->gc_cause());\n+\n+  return should_do_max_compaction\n+      || is_max_on_system_gc;\n+}\n+\n+void PSParallelCompactNew::summary_phase() {\n+  GCTraceTime(Info, gc, phases) tm(\"Summary Phase\", &_gc_timer);\n+\n+  setup_regions_serial();\n+  setup_regions_parallel();\n+\n+#ifndef PRODUCT\n+  for (size_t idx = 0; idx < _num_regions; idx++) {\n+    PCRegionData* rd = &_region_data_array[idx];\n+    log_develop_trace(gc, compaction)(\"Compaction region #%zu: [\" PTR_FORMAT \", \" PTR_FORMAT \")\", rd->idx(), p2i(\n+            rd->bottom()), p2i(rd->end()));\n+  }\n+#endif\n+}\n+\n+\/\/ This method should contain all heap-specific policy for invoking a full\n+\/\/ collection.  invoke_no_policy() will only attempt to compact the heap; it\n+\/\/ will do nothing further.  If we need to bail out for policy reasons, scavenge\n+\/\/ before full gc, or any other specialized behavior, it needs to be added here.\n+\/\/\n+\/\/ Note that this method should only be called from the vm_thread while at a\n+\/\/ safepoint.\n+\/\/\n+\/\/ Note that the all_soft_refs_clear flag in the soft ref policy\n+\/\/ may be true because this method can be called without intervening\n+\/\/ activity.  For example when the heap space is tight and full measure\n+\/\/ are being taken to free space.\n+bool PSParallelCompactNew::invoke(bool clear_all_soft_refs, bool should_do_max_compaction) {\n+  assert(SafepointSynchronize::is_at_safepoint(), \"should be at safepoint\");\n+  assert(Thread::current() == (Thread*)VMThread::vm_thread(),\n+         \"should be in vm thread\");\n+\n+  SvcGCMarker sgcm(SvcGCMarker::FULL);\n+  IsSTWGCActiveMark mark;\n+\n+  ParallelScavengeHeap* heap = ParallelScavengeHeap::heap();\n+\n+  GCIdMark gc_id_mark;\n+  _gc_timer.register_gc_start();\n+  _gc_tracer.report_gc_start(heap->gc_cause(), _gc_timer.gc_start());\n+\n+  GCCause::Cause gc_cause = heap->gc_cause();\n+  PSYoungGen* young_gen = ParallelScavengeHeap::young_gen();\n+  PSOldGen* old_gen = ParallelScavengeHeap::old_gen();\n+  PSAdaptiveSizePolicy* size_policy = heap->size_policy();\n+\n+  \/\/ Make sure data structures are sane, make the heap parsable, and do other\n+  \/\/ miscellaneous bookkeeping.\n+  pre_compact();\n+\n+  const PreGenGCValues pre_gc_values = heap->get_pre_gc_values();\n+\n+  {\n+    const uint active_workers =\n+      WorkerPolicy::calc_active_workers(ParallelScavengeHeap::heap()->workers().max_workers(),\n+                                        ParallelScavengeHeap::heap()->workers().active_workers(),\n+                                        Threads::number_of_non_daemon_threads());\n+    ParallelScavengeHeap::heap()->workers().set_active_workers(active_workers);\n+\n+    if (check_maximum_compaction(should_do_max_compaction)) {\n+      \/\/ Serial compaction executes the forwarding and compaction phases serially,\n+      \/\/ thus achieving perfect compaction.\n+      \/\/ Marking and ajust-references would still be executed in parallel threads.\n+      _serial = true;\n+    } else {\n+      _serial = false;\n+    }\n+\n+    GCTraceCPUTime tcpu(&_gc_tracer);\n+    GCTraceTime(Info, gc) tm(\"Pause Full\", nullptr, gc_cause, true);\n+\n+    heap->pre_full_gc_dump(&_gc_timer);\n+\n+    TraceCollectorStats tcs(counters());\n+    TraceMemoryManagerStats tms(heap->old_gc_manager(), gc_cause, \"end of major GC\");\n+\n+    if (log_is_enabled(Debug, gc, heap, exit)) {\n+      accumulated_time()->start();\n+    }\n+\n+    \/\/ Let the size policy know we're starting\n+    size_policy->major_collection_begin();\n+\n+#if COMPILER2_OR_JVMCI\n+    DerivedPointerTable::clear();\n+#endif\n+\n+    ref_processor()->start_discovery(clear_all_soft_refs);\n+\n+    ClassUnloadingContext ctx(1 \/* num_nmethod_unlink_workers *\/,\n+                              false \/* unregister_nmethods_during_purge *\/,\n+                              false \/* lock_nmethod_free_separately *\/);\n+\n+    marking_phase(&_gc_tracer);\n+\n+    summary_phase();\n+\n+#if COMPILER2_OR_JVMCI\n+    assert(DerivedPointerTable::is_active(), \"Sanity\");\n+    DerivedPointerTable::set_active(false);\n+#endif\n+\n+    FullGCForwarding::begin();\n+\n+    forward_to_new_addr();\n+\n+    adjust_pointers();\n+\n+    compact();\n+\n+    FullGCForwarding::end();\n+\n+    ParCompactionManagerNew::_preserved_marks_set->restore(&ParallelScavengeHeap::heap()->workers());\n+\n+    \/\/ Reset the mark bitmap, summary data, and do other bookkeeping.  Must be\n+    \/\/ done before resizing.\n+    post_compact();\n+\n+    \/\/ Let the size policy know we're done\n+    size_policy->major_collection_end();\n+\n+    size_policy->sample_old_gen_used_bytes(MAX2(pre_gc_values.old_gen_used(), old_gen->used_in_bytes()));\n+\n+    if (UseAdaptiveSizePolicy) {\n+      heap->resize_after_full_gc();\n+    }\n+\n+    heap->resize_all_tlabs();\n+\n+    \/\/ Resize the metaspace capacity after a collection\n+    MetaspaceGC::compute_new_size();\n+\n+    if (log_is_enabled(Debug, gc, heap, exit)) {\n+      accumulated_time()->stop();\n+    }\n+\n+    heap->print_heap_change(pre_gc_values);\n+\n+    \/\/ Track memory usage and detect low memory\n+    MemoryService::track_memory_usage();\n+    heap->update_counters();\n+\n+    heap->post_full_gc_dump(&_gc_timer);\n+\n+    size_policy->record_gc_pause_end_instant();\n+  }\n+\n+  heap->gc_epilogue(true);\n+\n+  if (VerifyAfterGC && heap->total_collections() >= VerifyGCStartAt) {\n+    Universe::verify(\"After GC\");\n+  }\n+\n+  heap->print_after_gc();\n+  heap->trace_heap_after_gc(&_gc_tracer);\n+\n+  _gc_timer.register_gc_end();\n+\n+  _gc_tracer.report_gc_end(_gc_timer.gc_end(), _gc_timer.time_partitions());\n+\n+  return true;\n+}\n+\n+class PCAddThreadRootsMarkingTaskClosureNew : public ThreadClosure {\n+private:\n+  uint _worker_id;\n+\n+public:\n+  explicit PCAddThreadRootsMarkingTaskClosureNew(uint worker_id) : _worker_id(worker_id) { }\n+  void do_thread(Thread* thread) final {\n+    assert(ParallelScavengeHeap::heap()->is_stw_gc_active(), \"called outside gc\");\n+\n+    ResourceMark rm;\n+\n+    ParCompactionManagerNew* cm = ParCompactionManagerNew::gc_thread_compaction_manager(_worker_id);\n+\n+    MarkingNMethodClosure mark_and_push_in_blobs(&cm->_mark_and_push_closure);\n+\n+    thread->oops_do(&cm->_mark_and_push_closure, &mark_and_push_in_blobs);\n+\n+    \/\/ Do the real work\n+    cm->follow_marking_stacks();\n+  }\n+};\n+\n+void steal_marking_work_new(TaskTerminator& terminator, uint worker_id) {\n+  assert(ParallelScavengeHeap::heap()->is_stw_gc_active(), \"called outside gc\");\n+\n+  ParCompactionManagerNew* cm =\n+    ParCompactionManagerNew::gc_thread_compaction_manager(worker_id);\n+\n+  do {\n+    ScannerTask task;\n+    if (ParCompactionManagerNew::steal(worker_id, task)) {\n+      cm->follow_contents(task, true);\n+    }\n+    cm->follow_marking_stacks();\n+  } while (!terminator.offer_termination());\n+}\n+\n+class MarkFromRootsTaskNew : public WorkerTask {\n+  NMethodMarkingScope _nmethod_marking_scope;\n+  ThreadsClaimTokenScope _threads_claim_token_scope;\n+  OopStorageSetStrongParState<false \/* concurrent *\/, false \/* is_const *\/> _oop_storage_set_par_state;\n+  TaskTerminator _terminator;\n+  uint _active_workers;\n+\n+public:\n+  explicit MarkFromRootsTaskNew(uint active_workers) :\n+      WorkerTask(\"MarkFromRootsTaskNew\"),\n+      _nmethod_marking_scope(),\n+      _threads_claim_token_scope(),\n+      _terminator(active_workers, ParCompactionManagerNew::marking_stacks()),\n+      _active_workers(active_workers) {}\n+\n+  void work(uint worker_id) final {\n+    ParCompactionManagerNew* cm = ParCompactionManagerNew::gc_thread_compaction_manager(worker_id);\n+    {\n+      CLDToOopClosure cld_closure(&cm->_mark_and_push_closure, ClassLoaderData::_claim_stw_fullgc_mark);\n+      ClassLoaderDataGraph::always_strong_cld_do(&cld_closure);\n+\n+      \/\/ Do the real work\n+      cm->follow_marking_stacks();\n+    }\n+\n+    {\n+      PCAddThreadRootsMarkingTaskClosureNew closure(worker_id);\n+      Threads::possibly_parallel_threads_do(_active_workers > 1 \/* is_par *\/, &closure);\n+    }\n+\n+    \/\/ Mark from OopStorages\n+    {\n+      _oop_storage_set_par_state.oops_do(&cm->_mark_and_push_closure);\n+      \/\/ Do the real work\n+      cm->follow_marking_stacks();\n+    }\n+\n+    if (_active_workers > 1) {\n+      steal_marking_work_new(_terminator, worker_id);\n+    }\n+  }\n+};\n+\n+class ParallelCompactRefProcProxyTaskNew : public RefProcProxyTask {\n+  TaskTerminator _terminator;\n+\n+public:\n+  explicit ParallelCompactRefProcProxyTaskNew(uint max_workers)\n+    : RefProcProxyTask(\"ParallelCompactRefProcProxyTaskNew\", max_workers),\n+      _terminator(_max_workers, ParCompactionManagerNew::marking_stacks()) {}\n+\n+  void work(uint worker_id) final {\n+    assert(worker_id < _max_workers, \"sanity\");\n+    ParCompactionManagerNew* cm = (_tm == RefProcThreadModel::Single) ? ParCompactionManagerNew::get_vmthread_cm() : ParCompactionManagerNew::gc_thread_compaction_manager(worker_id);\n+    BarrierEnqueueDiscoveredFieldClosure enqueue;\n+    ParCompactionManagerNew::FollowStackClosure complete_gc(cm, (_tm == RefProcThreadModel::Single) ? nullptr : &_terminator, worker_id);\n+    _rp_task->rp_work(worker_id, PSParallelCompactNew::is_alive_closure(), &cm->_mark_and_push_closure, &enqueue, &complete_gc);\n+  }\n+\n+  void prepare_run_task_hook() final {\n+    _terminator.reset_for_reuse(_queue_count);\n+  }\n+};\n+\n+void PSParallelCompactNew::marking_phase(ParallelOldTracer *gc_tracer) {\n+  \/\/ Recursively traverse all live objects and mark them\n+  GCTraceTime(Info, gc, phases) tm(\"Marking Phase\", &_gc_timer);\n+\n+  uint active_gc_threads = ParallelScavengeHeap::heap()->workers().active_workers();\n+\n+  ClassLoaderDataGraph::verify_claimed_marks_cleared(ClassLoaderData::_claim_stw_fullgc_mark);\n+  {\n+    GCTraceTime(Debug, gc, phases) pm_tm(\"Par Mark\", &_gc_timer);\n+\n+    MarkFromRootsTaskNew task(active_gc_threads);\n+    ParallelScavengeHeap::heap()->workers().run_task(&task);\n+  }\n+\n+  \/\/ Process reference objects found during marking\n+  {\n+    GCTraceTime(Debug, gc, phases) rp_tm(\"Reference Processing\", &_gc_timer);\n+\n+    ReferenceProcessorStats stats;\n+    ReferenceProcessorPhaseTimes pt(&_gc_timer, ref_processor()->max_num_queues());\n+\n+    ParallelCompactRefProcProxyTaskNew task(ref_processor()->max_num_queues());\n+    stats = ref_processor()->process_discovered_references(task, &ParallelScavengeHeap::heap()->workers(), pt);\n+\n+    gc_tracer->report_gc_reference_stats(stats);\n+    pt.print_all_references();\n+  }\n+\n+  \/\/ This is the point where the entire marking should have completed.\n+  ParCompactionManagerNew::verify_all_marking_stack_empty();\n+\n+  {\n+    GCTraceTime(Debug, gc, phases) wp_tm(\"Weak Processing\", &_gc_timer);\n+    WeakProcessor::weak_oops_do(&ParallelScavengeHeap::heap()->workers(),\n+                                is_alive_closure(),\n+                                &do_nothing_cl,\n+                                1);\n+  }\n+\n+  {\n+    GCTraceTime(Debug, gc, phases) tm_m(\"Class Unloading\", &_gc_timer);\n+\n+    ClassUnloadingContext* ctx = ClassUnloadingContext::context();\n+\n+    bool unloading_occurred;\n+    {\n+      CodeCache::UnlinkingScope scope(is_alive_closure());\n+\n+      \/\/ Follow system dictionary roots and unload classes.\n+      unloading_occurred = SystemDictionary::do_unloading(&_gc_timer);\n+\n+      \/\/ Unload nmethods.\n+      CodeCache::do_unloading(unloading_occurred);\n+    }\n+\n+    {\n+      GCTraceTime(Debug, gc, phases) t(\"Purge Unlinked NMethods\", gc_timer());\n+      \/\/ Release unloaded nmethod's memory.\n+      ctx->purge_nmethods();\n+    }\n+    {\n+      GCTraceTime(Debug, gc, phases) ur(\"Unregister NMethods\", &_gc_timer);\n+      ParallelScavengeHeap::heap()->prune_unlinked_nmethods();\n+    }\n+    {\n+      GCTraceTime(Debug, gc, phases) t(\"Free Code Blobs\", gc_timer());\n+      ctx->free_nmethods();\n+    }\n+\n+    \/\/ Prune dead klasses from subklass\/sibling\/implementor lists.\n+    Klass::clean_weak_klass_links(unloading_occurred);\n+\n+    \/\/ Clean JVMCI metadata handles.\n+    JVMCI_ONLY(JVMCI::do_unloading(unloading_occurred));\n+  }\n+\n+  {\n+    GCTraceTime(Debug, gc, phases) roc_tm(\"Report Object Count\", &_gc_timer);\n+    _gc_tracer.report_object_count_after_gc(is_alive_closure(), &ParallelScavengeHeap::heap()->workers());\n+  }\n+#if TASKQUEUE_STATS\n+  ParCompactionManagerNew::print_and_reset_taskqueue_stats();\n+#endif\n+}\n+\n+void PSParallelCompactNew::adjust_pointers_in_spaces(uint worker_id) {\n+  auto start_time = Ticks::now();\n+  for (size_t i = 0; i < _num_regions; i++) {\n+    PCRegionData* region = &_region_data_array[i];\n+    if (!region->claim()) {\n+      continue;\n+    }\n+    log_trace(gc, compaction)(\"Adjusting pointers in region: %zu (worker_id: %u)\", region->idx(), worker_id);\n+    HeapWord* end = region->top();\n+    HeapWord* current = _mark_bitmap.find_obj_beg(region->bottom(), end);\n+    while (current < end) {\n+      assert(_mark_bitmap.is_marked(current), \"must be marked\");\n+      oop obj = cast_to_oop(current);\n+      size_t size = obj->size();\n+      obj->oop_iterate(&pc_adjust_pointer_closure);\n+      current = _mark_bitmap.find_obj_beg(current + size, end);\n+    }\n+  }\n+  log_trace(gc, phases)(\"adjust_pointers_in_spaces worker %u: %.3f ms\", worker_id, (Ticks::now() - start_time).seconds() * 1000);\n+}\n+\n+class PSAdjustTaskNew final : public WorkerTask {\n+  SubTasksDone                               _sub_tasks;\n+  WeakProcessor::Task                        _weak_proc_task;\n+  OopStorageSetStrongParState<false, false>  _oop_storage_iter;\n+  uint                                       _nworkers;\n+\n+  enum PSAdjustSubTask {\n+    PSAdjustSubTask_code_cache,\n+\n+    PSAdjustSubTask_num_elements\n+  };\n+\n+public:\n+  explicit PSAdjustTaskNew(uint nworkers) :\n+    WorkerTask(\"PSAdjust task\"),\n+    _sub_tasks(PSAdjustSubTask_num_elements),\n+    _weak_proc_task(nworkers),\n+    _nworkers(nworkers) {\n+\n+    ClassLoaderDataGraph::verify_claimed_marks_cleared(ClassLoaderData::_claim_stw_fullgc_adjust);\n+    if (nworkers > 1) {\n+      Threads::change_thread_claim_token();\n+    }\n+  }\n+\n+  ~PSAdjustTaskNew() {\n+    Threads::assert_all_threads_claimed();\n+  }\n+\n+  void work(uint worker_id) final {\n+    ParCompactionManagerNew* cm = ParCompactionManagerNew::gc_thread_compaction_manager(worker_id);\n+    cm->preserved_marks()->adjust_during_full_gc();\n+    {\n+      \/\/ adjust pointers in all spaces\n+      PSParallelCompactNew::adjust_pointers_in_spaces(worker_id);\n+    }\n+    {\n+      ResourceMark rm;\n+      Threads::possibly_parallel_oops_do(_nworkers > 1, &pc_adjust_pointer_closure, nullptr);\n+    }\n+    _oop_storage_iter.oops_do(&pc_adjust_pointer_closure);\n+    {\n+      CLDToOopClosure cld_closure(&pc_adjust_pointer_closure, ClassLoaderData::_claim_stw_fullgc_adjust);\n+      ClassLoaderDataGraph::cld_do(&cld_closure);\n+    }\n+    {\n+      AlwaysTrueClosure always_alive;\n+      _weak_proc_task.work(worker_id, &always_alive, &pc_adjust_pointer_closure);\n+    }\n+    if (_sub_tasks.try_claim_task(PSAdjustSubTask_code_cache)) {\n+      NMethodToOopClosure adjust_code(&pc_adjust_pointer_closure, NMethodToOopClosure::FixRelocations);\n+      CodeCache::nmethods_do(&adjust_code);\n+    }\n+    _sub_tasks.all_tasks_claimed();\n+  }\n+};\n+\n+void PSParallelCompactNew::adjust_pointers() {\n+  \/\/ Adjust the pointers to reflect the new locations\n+  GCTraceTime(Info, gc, phases) tm(\"Adjust Pointers\", &_gc_timer);\n+  uint num_workers = ParallelScavengeHeap::heap()->workers().active_workers();\n+  PSAdjustTaskNew task(num_workers);\n+  ParallelScavengeHeap::heap()->workers().run_task(&task);\n+}\n+\n+void PSParallelCompactNew::forward_to_new_addr() {\n+  GCTraceTime(Info, gc, phases) tm(\"Forward\", &_gc_timer);\n+  uint num_workers = get_num_workers();\n+  _per_worker_region_data = NEW_C_HEAP_ARRAY(PCRegionData*, num_workers, mtGC);\n+  for (uint i = 0; i < num_workers; i++) {\n+    _per_worker_region_data[i] = nullptr;\n+  }\n+\n+  class ForwardState {\n+    uint _worker_id;\n+    PCRegionData* _compaction_region;\n+    HeapWord* _compaction_point;\n+\n+    void ensure_compaction_point() {\n+      if (_compaction_point == nullptr) {\n+        assert(_compaction_region == nullptr, \"invariant\");\n+        _compaction_region = _per_worker_region_data[_worker_id];\n+        assert(_compaction_region != nullptr, \"invariant\");\n+        _compaction_point = _compaction_region->bottom();\n+      }\n+    }\n+  public:\n+    explicit ForwardState(uint worker_id) :\n+            _worker_id(worker_id),\n+            _compaction_region(nullptr),\n+            _compaction_point(nullptr) {\n+    }\n+\n+    size_t available() const {\n+      return pointer_delta(_compaction_region->end(), _compaction_point);\n+    }\n+\n+    void forward_objs_in_region(ParCompactionManagerNew* cm, PCRegionData* region) {\n+      ensure_compaction_point();\n+      HeapWord* end = region->end();\n+      HeapWord* current = _mark_bitmap.find_obj_beg(region->bottom(), end);\n+      while (current < end) {\n+        assert(_mark_bitmap.is_marked(current), \"must be marked\");\n+        oop obj = cast_to_oop(current);\n+        assert(region->contains(obj), \"object must not cross region boundary: obj: \" PTR_FORMAT \", obj_end: \" PTR_FORMAT \", region start: \" PTR_FORMAT \", region end: \" PTR_FORMAT, p2i(obj), p2i(cast_from_oop<HeapWord*>(obj) + obj->size()), p2i(region->bottom()), p2i(region->end()));\n+        size_t old_size = obj->size();\n+        size_t new_size = obj->copy_size(old_size, obj->mark());\n+        size_t size = (current == _compaction_point) ? old_size : new_size;\n+        while (size > available()) {\n+          _compaction_region->set_new_top(_compaction_point);\n+          _compaction_region = _compaction_region->local_next();\n+          assert(_compaction_region != nullptr, \"must find a compaction region\");\n+          _compaction_point = _compaction_region->bottom();\n+          size = (current == _compaction_point) ? old_size : new_size;\n+        }\n+        \/\/log_develop_trace(gc, compaction)(\"Forwarding obj: \" PTR_FORMAT \", to: \" PTR_FORMAT, p2i(obj), p2i(_compaction_point));\n+        if (current != _compaction_point) {\n+          cm->preserved_marks()->push_if_necessary(obj, obj->mark());\n+          FullGCForwarding::forward_to(obj, cast_to_oop(_compaction_point));\n+        }\n+        _compaction_point += size;\n+        assert(_compaction_point <= _compaction_region->end(), \"object must fit in region\");\n+        current += old_size;\n+        assert(current <= end, \"object must not cross region boundary\");\n+        current = _mark_bitmap.find_obj_beg(current, end);\n+      }\n+    }\n+    void finish() {\n+      if (_compaction_region != nullptr) {\n+        _compaction_region->set_new_top(_compaction_point);\n+      }\n+    }\n+  };\n+\n+  struct ForwardTask final : public WorkerTask {\n+    ForwardTask() : WorkerTask(\"PSForward task\") {}\n+\n+    void work(uint worker_id) override {\n+      ParCompactionManagerNew* cm = ParCompactionManagerNew::gc_thread_compaction_manager(worker_id);\n+      ForwardState state(worker_id);\n+      PCRegionData** last_link = &_per_worker_region_data[worker_id];\n+      size_t idx = worker_id;\n+      uint num_workers = get_num_workers();\n+      size_t num_regions = get_num_regions();\n+      PCRegionData* region_data_array = get_region_data_array();\n+      while (idx < num_regions) {\n+        PCRegionData* region = region_data_array + idx;\n+        *last_link = region;\n+        last_link = region->local_next_addr();\n+        state.forward_objs_in_region(cm, region);\n+        idx += num_workers;\n+      }\n+      state.finish();\n+    }\n+  } task;\n+\n+  uint par_workers = ParallelScavengeHeap::heap()->workers().active_workers();\n+  ParallelScavengeHeap::heap()->workers().set_active_workers(num_workers);\n+  ParallelScavengeHeap::heap()->workers().run_task(&task);\n+  ParallelScavengeHeap::heap()->workers().set_active_workers(par_workers);\n+\n+#ifndef PRODUCT\n+  for (uint wid = 0; wid < num_workers; wid++) {\n+    for (PCRegionData* rd = _per_worker_region_data[wid]; rd != nullptr; rd = rd->local_next()) {\n+      log_develop_trace(gc, compaction)(\"Per worker compaction region, worker: %d, #%zu: [\" PTR_FORMAT \", \" PTR_FORMAT \"), new_top: \" PTR_FORMAT, wid, rd->idx(),\n+                                        p2i(rd->bottom()), p2i(rd->end()), p2i(rd->new_top()));\n+    }\n+  }\n+#endif\n+}\n+\n+void PSParallelCompactNew::compact() {\n+  GCTraceTime(Info, gc, phases) tm(\"Compaction Phase\", &_gc_timer);\n+  class CompactTask final : public WorkerTask {\n+    static void compact_region(PCRegionData* region) {\n+      HeapWord* bottom = region->bottom();\n+      HeapWord* end = region->top();\n+      if (bottom == end) {\n+        return;\n+      }\n+      HeapWord* current = _mark_bitmap.find_obj_beg(bottom, end);\n+      while (current < end) {\n+        oop obj = cast_to_oop(current);\n+        size_t size = obj->size();\n+        if (FullGCForwarding::is_forwarded(obj)) {\n+          oop fwd = FullGCForwarding::forwardee(obj);\n+          auto* dst = cast_from_oop<HeapWord*>(fwd);\n+          ObjectStartArray* sa = start_array(space_id(dst));\n+          if (sa != nullptr) {\n+            assert(dst != current, \"expect moving object\");\n+            size_t new_words = obj->copy_size(size, obj->mark());\n+            sa->update_for_block(dst, dst + new_words);\n+          }\n+\n+          Copy::aligned_conjoint_words(current, dst, size);\n+          fwd->init_mark();\n+          fwd->initialize_hash_if_necessary(obj);\n+        } else {\n+          \/\/ The start_array must be updated even if the object is not moving.\n+          ObjectStartArray* sa = start_array(space_id(current));\n+          if (sa != nullptr) {\n+            sa->update_for_block(current, current + size);\n+          }\n+        }\n+        current = _mark_bitmap.find_obj_beg(current + size, end);\n+      }\n+    }\n+  public:\n+    explicit CompactTask() : WorkerTask(\"PSCompact task\") {}\n+    void work(uint worker_id) override {\n+      PCRegionData* region = _per_worker_region_data[worker_id];\n+      while (region != nullptr) {\n+        log_trace(gc)(\"Compact worker: %u, compacting region: %zu\", worker_id, region->idx());\n+        compact_region(region);\n+        region = region->local_next();\n+      }\n+    }\n+  } task;\n+\n+  uint num_workers = get_num_workers();\n+  uint par_workers = ParallelScavengeHeap::heap()->workers().active_workers();\n+  ParallelScavengeHeap::heap()->workers().set_active_workers(num_workers);\n+  ParallelScavengeHeap::heap()->workers().run_task(&task);\n+  ParallelScavengeHeap::heap()->workers().set_active_workers(par_workers);\n+}\n+\n+\/\/ Return the SpaceId for the space containing addr.  If addr is not in the\n+\/\/ heap, last_space_id is returned.  In debug mode it expects the address to be\n+\/\/ in the heap and asserts such.\n+PSParallelCompactNew::SpaceId PSParallelCompactNew::space_id(HeapWord* addr) {\n+  assert(ParallelScavengeHeap::heap()->is_in_reserved(addr), \"addr not in the heap\");\n+\n+  for (unsigned int id = old_space_id; id < last_space_id; ++id) {\n+    if (_space_info[id].space()->contains(addr)) {\n+      return SpaceId(id);\n+    }\n+  }\n+\n+  assert(false, \"no space contains the addr\");\n+  return last_space_id;\n+}\n","filename":"src\/hotspot\/share\/gc\/parallel\/psParallelCompactNew.cpp","additions":1054,"deletions":0,"binary":false,"changes":1054,"status":"added"},{"patch":"@@ -0,0 +1,337 @@\n+\/*\n+ * Copyright (c) 2005, 2024, Oracle and\/or its affiliates. All rights reserved.\n+ * Copyright Amazon.com Inc. or its affiliates. All Rights Reserved.\n+ * * DO NOT ALTER OR REMOVE COPYRIGHT NOTICES OR THIS FILE HEADER.\n+ *\n+ * This code is free software; you can redistribute it and\/or modify it\n+ * under the terms of the GNU General Public License version 2 only, as\n+ * published by the Free Software Foundation.\n+ *\n+ * This code is distributed in the hope that it will be useful, but WITHOUT\n+ * ANY WARRANTY; without even the implied warranty of MERCHANTABILITY or\n+ * FITNESS FOR A PARTICULAR PURPOSE.  See the GNU General Public License\n+ * version 2 for more details (a copy is included in the LICENSE file that\n+ * accompanied this code).\n+ *\n+ * You should have received a copy of the GNU General Public License version\n+ * 2 along with this work; if not, write to the Free Software Foundation,\n+ * Inc., 51 Franklin St, Fifth Floor, Boston, MA 02110-1301 USA.\n+ *\n+ * Please contact Oracle, 500 Oracle Parkway, Redwood Shores, CA 94065 USA\n+ * or visit www.oracle.com if you need additional information or have any\n+ * questions.\n+ *\n+ *\/\n+\n+#ifndef SHARE_GC_PARALLEL_PSPARALLELCOMPACTNEW_HPP\n+#define SHARE_GC_PARALLEL_PSPARALLELCOMPACTNEW_HPP\n+\n+#include \"gc\/parallel\/mutableSpace.hpp\"\n+#include \"gc\/parallel\/objectStartArray.hpp\"\n+#include \"gc\/parallel\/parallelScavengeHeap.hpp\"\n+#include \"gc\/parallel\/parMarkBitMap.hpp\"\n+#include \"gc\/shared\/collectedHeap.hpp\"\n+#include \"gc\/shared\/collectorCounters.hpp\"\n+#include \"gc\/shared\/referenceProcessor.hpp\"\n+#include \"gc\/shared\/taskTerminator.hpp\"\n+#include \"oops\/oop.hpp\"\n+#include \"runtime\/atomic.hpp\"\n+#include \"runtime\/orderAccess.hpp\"\n+\n+class ParallelScavengeHeap;\n+class PSAdaptiveSizePolicy;\n+class PSYoungGen;\n+class PSOldGen;\n+class ParCompactionManagerNew;\n+class PSParallelCompactNew;\n+class ParallelOldTracer;\n+class STWGCTimer;\n+\n+class SpaceInfoNew\n+{\n+public:\n+  MutableSpace* space() const { return _space; }\n+\n+  \/\/ The start array for the (generation containing the) space, or null if there\n+  \/\/ is no start array.\n+  ObjectStartArray* start_array() const { return _start_array; }\n+\n+  void set_space(MutableSpace* s)           { _space = s; }\n+  void set_start_array(ObjectStartArray* s) { _start_array = s; }\n+\n+private:\n+  MutableSpace*     _space;\n+  ObjectStartArray* _start_array;\n+};\n+\n+\/\/ The Parallel compaction collector is a stop-the-world garbage collector that\n+\/\/ does parts of the collection using parallel threads.  The collection includes\n+\/\/ the tenured generation and the young generation.\n+\/\/\n+\/\/ A collection consists of the following phases.\n+\/\/\n+\/\/      - marking phase\n+\/\/      - summary phase (single-threaded)\n+\/\/      - forward (to new address) phase\n+\/\/      - adjust pointers phase\n+\/\/      - compacting phase\n+\/\/      - clean up phase\n+\/\/\n+\/\/ Roughly speaking these phases correspond, respectively, to\n+\/\/\n+\/\/      - mark all the live objects\n+\/\/      - set-up temporary regions to enable parallelism in following phases\n+\/\/      - calculate the destination of each object at the end of the collection\n+\/\/      - adjust pointers to reflect new destination of objects\n+\/\/      - move the objects to their destination\n+\/\/      - update some references and reinitialize some variables\n+\/\/\n+\/\/ A space that is being collected is divided into regions and with each region\n+\/\/ is associated an object of type PCRegionData. Regions are targeted to be of\n+\/\/ a mostly uniform size, but if an object would cross a region boundary, then\n+\/\/ the boundary is adjusted to be after the end of that object.\n+\/\/\n+\/\/ The marking phase does a complete marking of all live objects in the heap.\n+\/\/ The marking phase uses multiple GC threads and marking is done in a bit\n+\/\/ array of type ParMarkBitMap.  The marking of the bit map is done atomically.\n+\/\/\n+\/\/ The summary phase sets up the regions, such that region covers roughly\n+\/\/ uniform memory regions (currently same size as SpaceAlignment). However,\n+\/\/ if that would result in an object crossing a region boundary, then\n+\/\/ the upper bounds is adjusted such that the region ends after that object.\n+\/\/ This way we can ensure that a GC worker thread can fully 'own' a region\n+\/\/ during the forwarding, adjustment and compaction phases, without worrying\n+\/\/ about other threads messing with parts of the object. The summary phase\n+\/\/ also sets up an alternative set of regions, where each region covers\n+\/\/ a single space. This is used for a serial compaction mode which achieves\n+\/\/ maximum compaction at the expense of parallelism during the forwarding\n+\/\/ compaction phases.\n+\/\/\n+\/\/ The forwarding phase calculates the new address of each live\n+\/\/ object and records old-addr-to-new-addr association. It does this using\n+\/\/ multiple GC threads. Each thread 'claims' a source region and appends it to a\n+\/\/ local work-list. The region is also set as the current compaction region\n+\/\/ for that thread. All live objects in the region are then visited and its\n+\/\/ new location calculated by tracking the compaction point in the compaction\n+\/\/ region. Once the source region is exhausted, the next source region is\n+\/\/ claimed from the global pool and appended to the end of the local work-list.\n+\/\/ Once the compaction region is exhausted, the top of the old compaction region\n+\/\/ is recorded, and the next compaction region is fetched from the front of the\n+\/\/ local work-list (which is guaranteed to already have finished processing, or\n+\/\/ is the same as the source region). This way, each worker forms a local\n+\/\/ list of regions in which the worker can compact as if it were a serial\n+\/\/ compaction.\n+\/\/\n+\/\/ The adjust pointers phase remaps all pointers to reflect the new address of each\n+\/\/ object. Again, this uses multiple GC worker threads. Each thread claims\n+\/\/ a region, processes all references in all live objects of that region. Then\n+\/\/ the thread proceeds to claim the next region from the global pool, until\n+\/\/ all regions have been processed.\n+\/\/\n+\/\/ The compaction phase moves objects to their new location. Again, this uses\n+\/\/ multiple GC worker threads. Each worker processes the local work-list that\n+\/\/ has been set-up during the forwarding phase and processes it from bottom\n+\/\/ to top, copying each live object to its new location (which is guaranteed\n+\/\/ to be lower in that threads parts of the heap, and thus would never overwrite\n+\/\/ other objects).\n+\/\/\n+\/\/ This algorithm will usually leave gaps of non-fillable memory at the end\n+\/\/ of regions, and potentially whole empty regions at the end of compaction.\n+\/\/ The post-compaction phase fills those gaps with filler objects to ensure\n+\/\/ that the heap remains parsable.\n+\/\/\n+\/\/ In some situations, this inefficiency of leaving gaps can lead to a\n+\/\/ situation where after a full GC, it is still not possible to satisfy an\n+\/\/ allocation, even though there should be enough memory available. When\n+\/\/ that happens, the collector switches to a serial mode, where we only\n+\/\/ have 4 regions which correspond exaxtly to the 4 spaces, and the forwarding\n+\/\/ and compaction phases are executed using only a single thread. This\n+\/\/ achieves maximum compaction. This serial mode is also invoked when\n+\/\/ System.gc() is called *and* UseMaximumCompactionOnSystemGC is set to\n+\/\/ true (which is the default), or when the number of full GCs exceeds\n+\/\/ the HeapMaximumCompactionInterval.\n+\/\/\n+\/\/ Possible improvements to the algorithm include:\n+\/\/ - Identify and ignore a 'dense prefix'. This requires that we collect\n+\/\/   liveness data during marking, or that we scan the prefix object-by-object\n+\/\/   during the summary phase.\n+\/\/ - When an object does not fit into a remaining gap of a region, and the\n+\/\/   object is rather large, we could attempt to forward\/compact subsequent\n+\/\/   objects 'around' that large object in an attempt to minimize the\n+\/\/   resulting gap. This could be achieved by reconfiguring the regions\n+\/\/   to exclude the large object.\n+\/\/ - Instead of finding out *after* the whole compaction that an allocation\n+\/\/   can still not be satisfied, and then re-running the whole compaction\n+\/\/   serially, we could determine that after the forwarding phase, and\n+\/\/   re-do only forwarding serially, thus avoiding running marking,\n+\/\/   adjusting references and compaction twice.\n+class PCRegionData \/*: public CHeapObj<mtGC> *\/ {\n+  \/\/ A region index\n+  size_t const _idx;\n+\n+  \/\/ The start of the region\n+  HeapWord* const _bottom;\n+  \/\/ The top of the region. (first word after last live object in containing space)\n+  HeapWord* const _top;\n+  \/\/ The end of the region (first word after last word of the region)\n+  HeapWord* const _end;\n+\n+  \/\/ The next compaction address\n+  HeapWord* _new_top;\n+\n+  \/\/ Points to the next region in the GC-worker-local work-list\n+  PCRegionData* _local_next;\n+\n+  \/\/ Parallel workers claiming protocol, used during adjust-references phase.\n+  volatile bool _claimed;\n+\n+public:\n+\n+  PCRegionData(size_t idx, HeapWord* bottom, HeapWord* top, HeapWord* end) :\n+    _idx(idx), _bottom(bottom), _top(top), _end(end), _new_top(bottom),\n+          _local_next(nullptr), _claimed(false) {}\n+\n+  size_t idx() const { return _idx; };\n+\n+  HeapWord* bottom() const { return _bottom; }\n+  HeapWord* top() const { return _top; }\n+  HeapWord* end()   const { return _end;   }\n+\n+  PCRegionData*  local_next() const { return _local_next; }\n+  PCRegionData** local_next_addr() { return &_local_next; }\n+\n+  HeapWord* new_top() const {\n+    return _new_top;\n+  }\n+  void set_new_top(HeapWord* new_top) {\n+    _new_top = new_top;\n+  }\n+\n+  bool contains(oop obj) {\n+    auto* obj_start = cast_from_oop<HeapWord*>(obj);\n+    HeapWord* obj_end = obj_start + obj->size();\n+    return _bottom <= obj_start && obj_start < _end && _bottom < obj_end && obj_end <= _end;\n+  }\n+\n+  bool claim() {\n+    bool claimed =  _claimed;\n+    if (claimed) {\n+      return false;\n+    }\n+    return !AtomicAccess::cmpxchg(&_claimed, false, true);\n+  }\n+};\n+\n+class PSParallelCompactNew : AllStatic {\n+public:\n+  typedef enum {\n+    old_space_id, eden_space_id,\n+    from_space_id, to_space_id, last_space_id\n+  } SpaceId;\n+\n+public:\n+  class IsAliveClosure: public BoolObjectClosure {\n+  public:\n+    bool do_object_b(oop p) final;\n+  };\n+\n+private:\n+  static STWGCTimer           _gc_timer;\n+  static ParallelOldTracer    _gc_tracer;\n+  static elapsedTimer         _accumulated_time;\n+  static unsigned int         _maximum_compaction_gc_num;\n+  static CollectorCounters*   _counters;\n+  static ParMarkBitMap        _mark_bitmap;\n+  static IsAliveClosure       _is_alive_closure;\n+  static SpaceInfoNew         _space_info[last_space_id];\n+\n+  \/\/ The head of the global region data list.\n+  static size_t               _num_regions;\n+  static PCRegionData*        _region_data_array;\n+  static PCRegionData**       _per_worker_region_data;\n+\n+  static size_t               _num_regions_serial;\n+  static PCRegionData*        _region_data_array_serial;\n+  static bool                 _serial;\n+\n+  \/\/ Reference processing (used in ...follow_contents)\n+  static SpanSubjectToDiscoveryClosure  _span_based_discoverer;\n+  static ReferenceProcessor*  _ref_processor;\n+\n+  static uint get_num_workers() { return _serial ? 1 : ParallelScavengeHeap::heap()->workers().active_workers(); }\n+  static size_t get_num_regions() { return _serial ? _num_regions_serial : _num_regions; }\n+  static PCRegionData* get_region_data_array() { return _serial ? _region_data_array_serial : _region_data_array; }\n+\n+public:\n+  static ParallelOldTracer* gc_tracer() { return &_gc_tracer; }\n+\n+private:\n+\n+  static void initialize_space_info();\n+\n+  \/\/ Clear the marking bitmap and summary data that cover the specified space.\n+  static void clear_data_covering_space(SpaceId id);\n+\n+  static void pre_compact();\n+\n+  static void post_compact();\n+\n+  static bool check_maximum_compaction(bool should_do_max_compaction);\n+\n+  \/\/ Mark live objects\n+  static void marking_phase(ParallelOldTracer *gc_tracer);\n+\n+  static void summary_phase();\n+  static void setup_regions_parallel();\n+  static void setup_regions_serial();\n+\n+  static void adjust_pointers();\n+  static void forward_to_new_addr();\n+\n+  \/\/ Move objects to new locations.\n+  static void compact();\n+\n+public:\n+  static bool invoke(bool maximum_heap_compaction, bool should_do_max_compaction);\n+\n+  static void adjust_pointers_in_spaces(uint worker_id);\n+\n+  static void post_initialize();\n+  \/\/ Perform initialization for PSParallelCompactNew that requires\n+  \/\/ allocations.  This should be called during the VM initialization\n+  \/\/ at a pointer where it would be appropriate to return a JNI_ENOMEM\n+  \/\/ in the event of a failure.\n+  static bool initialize_aux_data();\n+\n+  \/\/ Closure accessors\n+  static BoolObjectClosure* is_alive_closure()     { return &_is_alive_closure; }\n+\n+  \/\/ Public accessors\n+  static elapsedTimer* accumulated_time() { return &_accumulated_time; }\n+\n+  static CollectorCounters* counters()    { return _counters; }\n+\n+  static inline bool is_marked(oop obj);\n+\n+  template <class T> static inline void adjust_pointer(T* p);\n+\n+  \/\/ Convenience wrappers for per-space data kept in _space_info.\n+  static inline MutableSpace*     space(SpaceId space_id);\n+  static inline ObjectStartArray* start_array(SpaceId space_id);\n+\n+  static ParMarkBitMap* mark_bitmap() { return &_mark_bitmap; }\n+\n+  \/\/ Reference Processing\n+  static ReferenceProcessor* ref_processor() { return _ref_processor; }\n+\n+  static STWGCTimer* gc_timer() { return &_gc_timer; }\n+\n+  \/\/ Return the SpaceId for the given address.\n+  static SpaceId space_id(HeapWord* addr);\n+\n+  static void print_on(outputStream* st);\n+};\n+\n+void steal_marking_work_new(TaskTerminator& terminator, uint worker_id);\n+\n+#endif \/\/ SHARE_GC_PARALLEL_PSPARALLELCOMPACTNEW_HPP\n","filename":"src\/hotspot\/share\/gc\/parallel\/psParallelCompactNew.hpp","additions":337,"deletions":0,"binary":false,"changes":337,"status":"added"},{"patch":"@@ -249,1 +249,2 @@\n-  size_t new_obj_size = o->size_given_klass(klass);\n+  size_t old_obj_size = o->size_given_mark_and_klass(test_mark, klass);\n+  size_t new_obj_size = o->copy_size(old_obj_size, test_mark);\n@@ -295,0 +296,2 @@\n+    new_obj->initialize_hash_if_necessary(o);\n+\n","filename":"src\/hotspot\/share\/gc\/parallel\/psPromotionManager.inline.hpp","additions":4,"deletions":1,"binary":false,"changes":5,"status":"modified"},{"patch":"@@ -33,0 +33,1 @@\n+#include \"gc\/parallel\/psCompactionManagerNew.hpp\"\n@@ -200,1 +201,1 @@\n-      _terminator(max_workers, ParCompactionManager::marking_stacks()) {}\n+      _terminator(max_workers, UseCompactObjectHeaders ? ParCompactionManagerNew::marking_stacks() : ParCompactionManager::marking_stacks()) {}\n","filename":"src\/hotspot\/share\/gc\/parallel\/psScavenge.cpp","additions":2,"deletions":1,"binary":false,"changes":3,"status":"modified"},{"patch":"@@ -698,7 +698,1 @@\n-      if (obj->is_self_forwarded()) {\n-        obj->unset_self_forwarded();\n-      } else if (obj->is_forwarded()) {\n-        \/\/ To restore the klass-bits in the header.\n-        \/\/ Needed for object iteration to work properly.\n-        obj->set_mark(obj->forwardee()->prototype_mark());\n-      }\n+      obj->reset_forwarded();\n@@ -735,1 +729,3 @@\n-  size_t s = old->size();\n+  size_t old_size = old->size();\n+  size_t s = old->copy_size(old_size, old->mark());\n+\n@@ -760,1 +756,1 @@\n-  Copy::aligned_disjoint_words(cast_from_oop<HeapWord*>(old), cast_from_oop<HeapWord*>(obj), s);\n+  Copy::aligned_disjoint_words(cast_from_oop<HeapWord*>(old), cast_from_oop<HeapWord*>(obj), old_size);\n@@ -770,0 +766,2 @@\n+  obj->initialize_hash_if_necessary(old);\n+\n","filename":"src\/hotspot\/share\/gc\/serial\/defNewGeneration.cpp","additions":7,"deletions":9,"binary":false,"changes":16,"status":"modified"},{"patch":"@@ -193,1 +193,2 @@\n-  HeapWord* alloc(size_t words) {\n+  HeapWord* alloc(size_t old_size, size_t new_size, HeapWord* old_obj) {\n+    size_t words = (old_obj == _spaces[_index]._compaction_top) ? old_size : new_size;\n@@ -209,0 +210,1 @@\n+      words = (old_obj == _spaces[_index]._compaction_top) ? old_size : new_size;\n@@ -230,1 +232,1 @@\n-  static void forward_obj(oop obj, HeapWord* new_addr) {\n+  static void forward_obj(oop obj, HeapWord* new_addr, bool after_first_dead) {\n@@ -236,2 +238,7 @@\n-      \/\/ This obj will stay in-place. Fix the markword.\n-      obj->init_mark();\n+      if (!after_first_dead) {\n+        \/\/ This obj will stay in-place and we'll not see it during relocation.\n+        \/\/ Fix the markword.\n+        obj->init_mark();\n+      } else {\n+        FullGCForwarding::forward_to(obj, cast_to_oop(new_addr));\n+      }\n@@ -260,2 +267,0 @@\n-    assert(addr != new_addr, \"inv\");\n-    prefetch_write_copy(new_addr);\n@@ -264,1 +269,4 @@\n-    Copy::aligned_conjoint_words(addr, new_addr, obj_size);\n+    if (addr != new_addr) {\n+      prefetch_write_copy(new_addr);\n+      Copy::aligned_conjoint_words(addr, new_addr, obj_size);\n+    }\n@@ -266,0 +274,3 @@\n+    if (addr != new_addr) {\n+      new_obj->initialize_hash_if_necessary(obj);\n+    }\n@@ -301,0 +312,1 @@\n+        size_t new_size = obj->copy_size(obj_size, obj->mark());\n@@ -302,2 +314,3 @@\n-          HeapWord* new_addr = alloc(obj_size);\n-          forward_obj(obj, new_addr);\n+          HeapWord* new_addr = alloc(obj_size, new_size, cur_addr);\n+          forward_obj(obj, new_addr, record_first_dead_done);\n+          assert(obj->size() == obj_size, \"size must not change after forwarding\");\n@@ -310,1 +323,2 @@\n-            alloc(pointer_delta(next_live_addr, cur_addr));\n+            size_t size = pointer_delta(next_live_addr, cur_addr);\n+            alloc(size, size, cur_addr);\n@@ -606,1 +620,1 @@\n-  obj->set_mark(obj->prototype_mark().set_marked());\n+  obj->set_mark(mark.set_marked());\n@@ -719,0 +733,2 @@\n+  FullGCForwarding::begin();\n+\n@@ -776,0 +792,2 @@\n+  FullGCForwarding::end();\n+\n","filename":"src\/hotspot\/share\/gc\/serial\/serialFullGC.cpp","additions":29,"deletions":11,"binary":false,"changes":40,"status":"modified"},{"patch":"@@ -43,1 +43,1 @@\n-#include \"gc\/shared\/fullGCForwarding.hpp\"\n+#include \"gc\/shared\/fullGCForwarding.inline.hpp\"\n","filename":"src\/hotspot\/share\/gc\/serial\/serialHeap.cpp","additions":1,"deletions":1,"binary":false,"changes":2,"status":"modified"},{"patch":"@@ -390,1 +390,1 @@\n-  assert(obj_size == obj->size(), \"bad obj_size passed in\");\n+  assert(obj_size == obj->size() || UseCompactObjectHeaders, \"bad obj_size passed in\");\n","filename":"src\/hotspot\/share\/gc\/serial\/tenuredGeneration.cpp","additions":1,"deletions":1,"binary":false,"changes":2,"status":"modified"},{"patch":"@@ -707,1 +707,2 @@\n-  \/\/ 8  - 32-bit VM or 64-bit VM, compact headers\n+  \/\/ 4  - compact headers\n+  \/\/ 8  - 32-bit VM\n@@ -712,1 +713,0 @@\n-    assert(!UseCompactObjectHeaders, \"\");\n@@ -717,2 +717,6 @@\n-      \/\/ Include klass to copy by 8 bytes words.\n-      base_off = instanceOopDesc::klass_offset_in_bytes();\n+      if (UseCompactObjectHeaders) {\n+        base_off = 0; \/* FIXME *\/\n+      } else {\n+        \/\/ Include klass to copy by 8 bytes words.\n+        base_off = instanceOopDesc::klass_offset_in_bytes();\n+      }\n@@ -720,1 +724,1 @@\n-    assert(base_off % BytesPerLong == 0, \"expect 8 bytes alignment\");\n+    assert(base_off % BytesPerLong == 0 || UseCompactObjectHeaders, \"expect 8 bytes alignment\");\n","filename":"src\/hotspot\/share\/gc\/shared\/c2\/barrierSetC2.cpp","additions":9,"deletions":5,"binary":false,"changes":14,"status":"modified"},{"patch":"@@ -292,1 +292,1 @@\n-  oop class_allocate(Klass* klass, size_t size, TRAPS);\n+  oop class_allocate(Klass* klass, size_t size, size_t base_size, TRAPS);\n","filename":"src\/hotspot\/share\/gc\/shared\/collectedHeap.hpp","additions":1,"deletions":1,"binary":false,"changes":2,"status":"modified"},{"patch":"@@ -22,1 +22,0 @@\n- *\n@@ -25,2 +24,2 @@\n-#ifndef GC_SHARED_FULLGCFORWARDING_INLINE_HPP\n-#define GC_SHARED_FULLGCFORWARDING_INLINE_HPP\n+#ifndef SHARE_GC_SHARED_FULLGCFORWARDING_INLINE_HPP\n+#define SHARE_GC_SHARED_FULLGCFORWARDING_INLINE_HPP\n@@ -30,0 +29,3 @@\n+#include \"logging\/log.hpp\"\n+#include \"nmt\/memTag.hpp\"\n+#include \"oops\/markWord.hpp\"\n@@ -31,1 +33,82 @@\n-#include \"utilities\/globalDefinitions.hpp\"\n+#include \"utilities\/concurrentHashTable.inline.hpp\"\n+#include \"utilities\/fastHash.hpp\"\n+#include \"utilities\/macros.hpp\"\n+#include \"utilities\/powerOfTwo.hpp\"\n+\n+\/\/ We cannot use 0, because that may already be a valid base address in zero-based heaps.\n+\/\/ 0x1 is safe because heap base addresses must be aligned by much larger alignment\n+template <int BITS>\n+HeapWord* const FullGCForwardingImpl<BITS>::UNUSED_BASE = reinterpret_cast<HeapWord*>(0x1);\n+\n+template <int BITS>\n+HeapWord* FullGCForwardingImpl<BITS>::_heap_start = nullptr;\n+template <int BITS>\n+size_t FullGCForwardingImpl<BITS>::_heap_start_region_bias = 0;\n+template <int BITS>\n+size_t FullGCForwardingImpl<BITS>::_num_regions = 0;\n+template <int BITS>\n+uintptr_t FullGCForwardingImpl<BITS>::_region_mask = 0;\n+template <int BITS>\n+HeapWord** FullGCForwardingImpl<BITS>::_biased_bases = nullptr;\n+template <int BITS>\n+HeapWord** FullGCForwardingImpl<BITS>::_bases_table = nullptr;\n+template <int BITS>\n+size_t FullGCForwardingImpl<BITS>::_fallback_table_log2_start_size = 9; \/\/ 512 entries.\n+template <int BITS>\n+FallbackTable* FullGCForwardingImpl<BITS>::_fallback_table = nullptr;\n+#ifndef PRODUCT\n+template <int BITS>\n+volatile uint64_t FullGCForwardingImpl<BITS>::_num_forwardings = 0;\n+template <int BITS>\n+volatile uint64_t FullGCForwardingImpl<BITS>::_num_fallback_forwardings = 0;\n+#endif\n+\n+template <int BITS>\n+bool FullGCForwardingImpl<BITS>::is_forwarded(oop obj) {\n+  return obj->is_forwarded();\n+}\n+\n+template <int BITS>\n+size_t FullGCForwardingImpl<BITS>::biased_region_index_containing(HeapWord* addr) {\n+  return reinterpret_cast<uintptr_t>(addr) >> BLOCK_SIZE_BYTES_SHIFT;\n+}\n+\n+template <int BITS>\n+bool FullGCForwardingImpl<BITS>::is_fallback(uintptr_t encoded) {\n+  return (encoded & OFFSET_MASK) == FALLBACK_PATTERN_IN_PLACE;\n+}\n+\n+template <int BITS>\n+uintptr_t FullGCForwardingImpl<BITS>::encode_forwarding(HeapWord* from, HeapWord* to) {\n+  size_t from_block_idx = biased_region_index_containing(from);\n+\n+  HeapWord* to_region_base = _biased_bases[from_block_idx];\n+  if (to_region_base == UNUSED_BASE) {\n+    HeapWord* prev = AtomicAccess::cmpxchg(&_biased_bases[from_block_idx], UNUSED_BASE, to);\n+    if (prev == UNUSED_BASE) {\n+      to_region_base = to;\n+    } else {\n+      to_region_base = prev;\n+    }\n+    \/\/ _biased_bases[from_block_idx] = to_region_base = to;\n+  }\n+  \/\/ Avoid pointer_delta() on purpose: using an unsigned subtraction,\n+  \/\/ we get an underflow when to < to_region_base, which means\n+  \/\/ we can use a single comparison instead of:\n+  \/\/ if (to_region_base > to || (to - to_region_base) > MAX_OFFSET) { .. }\n+  size_t offset = static_cast<size_t>(to - to_region_base);\n+  if (offset > MAX_OFFSET) {\n+    offset = FALLBACK_PATTERN;\n+  }\n+  uintptr_t encoded = (offset << OFFSET_BITS_SHIFT) | markWord::marked_value;\n+\n+  assert(is_fallback(encoded) || to == decode_forwarding(from, encoded), \"must be reversible: \" PTR_FORMAT \" -> \" PTR_FORMAT \", reversed: \" PTR_FORMAT \", encoded: \" INTPTR_FORMAT \", to_region_base: \" PTR_FORMAT \", from_block_idx: %lu\", p2i(from), p2i(to), p2i(decode_forwarding(from, encoded)), encoded, p2i(to_region_base), from_block_idx);\n+  assert((encoded & ~AVAILABLE_BITS_MASK) == 0, \"must encode to available bits\");\n+  return encoded;\n+}\n+\n+template <int BITS>\n+HeapWord* FullGCForwardingImpl<BITS>::decode_forwarding(HeapWord* from, uintptr_t encoded) {\n+  assert(!is_fallback(encoded), \"must not be fallback-forwarded, encoded: \" INTPTR_FORMAT \", OFFSET_MASK: \" INTPTR_FORMAT \", FALLBACK_PATTERN_IN_PLACE: \" INTPTR_FORMAT, encoded, OFFSET_MASK, FALLBACK_PATTERN_IN_PLACE);\n+  assert((encoded & ~AVAILABLE_BITS_MASK) == 0, \"must decode from available bits, encoded: \" INTPTR_FORMAT, encoded);\n+  uintptr_t offset = (encoded >> OFFSET_BITS_SHIFT);\n@@ -33,1 +116,31 @@\n-void FullGCForwarding::forward_to(oop from, oop to) {\n+  size_t from_idx = biased_region_index_containing(from);\n+  HeapWord* base = _biased_bases[from_idx];\n+  assert(base != UNUSED_BASE, \"must not be unused base: encoded: \" INTPTR_FORMAT, encoded);\n+  HeapWord* decoded = base + offset;\n+  assert(decoded >= _heap_start,\n+         \"Address must be above heap start. encoded: \" INTPTR_FORMAT \", base: \" PTR_FORMAT,\n+          encoded, p2i(base));\n+\n+  return decoded;\n+}\n+\n+template <int BITS>\n+void FullGCForwardingImpl<BITS>::forward_to_impl(oop from, oop to) {\n+  assert(_bases_table != nullptr, \"call begin() before forwarding\");\n+\n+  markWord from_header = from->mark();\n+  HeapWord* from_hw = cast_from_oop<HeapWord*>(from);\n+  HeapWord* to_hw   = cast_from_oop<HeapWord*>(to);\n+  uintptr_t encoded = encode_forwarding(from_hw, to_hw);\n+  markWord new_header = markWord((from_header.value() & ~OFFSET_MASK) | encoded);\n+  from->set_mark(new_header);\n+\n+  if (is_fallback(encoded)) {\n+    fallback_forward_to(from_hw, to_hw);\n+  }\n+  NOT_PRODUCT(AtomicAccess::inc(&_num_forwardings);)\n+}\n+\n+template <int BITS>\n+void FullGCForwardingImpl<BITS>::forward_to(oop obj, oop fwd) {\n+  assert(fwd != nullptr, \"no null forwarding\");\n@@ -35,6 +148,3 @@\n-  uintptr_t encoded = pointer_delta(cast_from_oop<HeapWord*>(to), _heap_base) << Shift;\n-  assert(encoded <= static_cast<uintptr_t>(right_n_bits(_num_low_bits)), \"encoded forwardee must fit\");\n-  uintptr_t mark = from->mark().value();\n-  mark &= ~right_n_bits(_num_low_bits);\n-  mark |= (encoded | markWord::marked_value);\n-  from->set_mark(markWord(mark));\n+  assert(_bases_table != nullptr, \"expect sliding forwarding initialized\");\n+  forward_to_impl(obj, fwd);\n+  \/\/ assert(forwardee(obj) == fwd, \"must be forwarded to correct forwardee, obj: \" PTR_FORMAT \", forwardee(obj): \" PTR_FORMAT \", fwd: \" PTR_FORMAT \", mark: \" INTPTR_FORMAT \", num-regions: %lu, base: \" PTR_FORMAT \", OFFSET_MASK: \" INTPTR_FORMAT \", encoded: \" PTR_FORMAT \", biased-base: \" PTR_FORMAT \", heap-start: \" PTR_FORMAT, p2i(obj), p2i(forwardee(obj)), p2i(fwd), obj->mark().value(), _num_regions, p2i(_bases_table[0]), OFFSET_MASK, encode_forwarding(cast_from_oop<HeapWord*>(obj), cast_from_oop<HeapWord*>(fwd)), p2i(_biased_bases[biased_region_index_containing(cast_from_oop<HeapWord*>(obj))]), p2i(_heap_start));\n@@ -42,1 +152,1 @@\n-  from->forward_to(to);\n+  obj->forward_to(fwd);\n@@ -46,1 +156,17 @@\n-oop FullGCForwarding::forwardee(oop from) {\n+template <int BITS>\n+oop FullGCForwardingImpl<BITS>::forwardee_impl(oop from) {\n+  assert(_bases_table != nullptr, \"call begin() before asking for forwarding\");\n+\n+  markWord header = from->mark();\n+  HeapWord* from_hw = cast_from_oop<HeapWord*>(from);\n+  if (is_fallback(header.value())) {\n+    HeapWord* to = fallback_forwardee(from_hw);\n+    return cast_to_oop(to);\n+  }\n+  uintptr_t encoded = header.value() & OFFSET_MASK;\n+  HeapWord* to = decode_forwarding(from_hw, encoded);\n+  return cast_to_oop(to);\n+}\n+\n+template <int BITS>\n+oop FullGCForwardingImpl<BITS>::forwardee(oop obj) {\n@@ -48,3 +174,2 @@\n-  uintptr_t mark = from->mark().value();\n-  HeapWord* decoded = _heap_base + ((mark & right_n_bits(_num_low_bits)) >> Shift);\n-  return cast_to_oop(decoded);\n+  assert(_bases_table != nullptr, \"expect sliding forwarding initialized\");\n+  return forwardee_impl(obj);\n@@ -52,1 +177,152 @@\n-  return from->forwardee();\n+  return obj->forwardee();\n+#endif\n+}\n+\n+static uintx hash(HeapWord* const& addr) {\n+  uint64_t val = reinterpret_cast<uint64_t>(addr);\n+  uint32_t hash = FastHash::get_hash32(static_cast<uint32_t>(val), static_cast<uint32_t>(val >> 32));\n+  return hash;\n+}\n+\n+struct ForwardingEntry {\n+  HeapWord* _from;\n+  HeapWord* _to;\n+  ForwardingEntry(HeapWord* from, HeapWord* to) : _from(from), _to(to) {}\n+};\n+\n+struct FallbackTableConfig {\n+  using Value = ForwardingEntry;\n+  static uintx get_hash(Value const& entry, bool* is_dead) {\n+    return hash(entry._from);\n+  }\n+  static void* allocate_node(void* context, size_t size, Value const& value) {\n+    return AllocateHeap(size, mtGC);\n+  }\n+  static void free_node(void* context, void* memory, Value const& value) {\n+    FreeHeap(memory);\n+  }\n+};\n+\n+class FallbackTable : public ConcurrentHashTable<FallbackTableConfig, mtGC> {\n+public:\n+  explicit FallbackTable(size_t log2size) : ConcurrentHashTable(log2size) {}\n+};\n+\n+class FallbackTableLookup : public StackObj {\n+  ForwardingEntry const _entry;\n+public:\n+  explicit FallbackTableLookup(HeapWord* from) : _entry(from, nullptr) {}\n+  uintx get_hash() const {\n+    return hash(_entry._from);\n+  }\n+  bool equals(const ForwardingEntry* value) const {\n+    return _entry._from == value->_from;\n+  }\n+  static bool is_dead(ForwardingEntry* value) { return false; }\n+};\n+\n+template <int BITS>\n+void FullGCForwardingImpl<BITS>::initialize(MemRegion heap) {\n+#ifdef _LP64\n+  _heap_start = heap.start();\n+\n+  size_t rounded_heap_size = MAX2(round_up_power_of_2(heap.byte_size()) \/ BytesPerWord, BLOCK_SIZE_WORDS);\n+\n+  _num_regions = rounded_heap_size \/ BLOCK_SIZE_WORDS;\n+\n+  _heap_start_region_bias = reinterpret_cast<uintptr_t>(_heap_start) >> BLOCK_SIZE_BYTES_SHIFT;\n+  _region_mask = ~((static_cast<uintptr_t>(1) << BLOCK_SIZE_BYTES_SHIFT) - 1);\n+\n+  assert(_bases_table == nullptr, \"should not be initialized yet\");\n+  assert(_fallback_table == nullptr, \"should not be initialized yet\");\n+#endif\n+}\n+\n+template <int BITS>\n+void FullGCForwardingImpl<BITS>::begin() {\n+#ifdef _LP64\n+  assert(_bases_table == nullptr, \"should not be initialized yet\");\n+  assert(_fallback_table == nullptr, \"should not be initialized yet\");\n+\n+  _fallback_table = nullptr;\n+\n+#ifndef PRODUCT\n+  _num_forwardings = 0;\n+  _num_fallback_forwardings = 0;\n+#endif\n+\n+  size_t max = _num_regions;\n+  _bases_table = NEW_C_HEAP_ARRAY(HeapWord*, max, mtGC);\n+  HeapWord** biased_start = _bases_table - _heap_start_region_bias;\n+  _biased_bases = biased_start;\n+  if (max == 1) {\n+    \/\/ Optimize the case when the block-size >= heap-size.\n+    \/\/ In this case we can use the heap-start as block-start,\n+    \/\/ and don't risk that competing GC threads set a higher\n+    \/\/ address as block-start, which would lead to unnecessary\n+    \/\/ fallback-usage.\n+    _bases_table[0] = _heap_start;\n+  } else {\n+    for (size_t i = 0; i < max; i++) {\n+      _bases_table[i] = UNUSED_BASE;\n+    }\n+  }\n+#endif\n+}\n+\n+template <int BITS>\n+void FullGCForwardingImpl<BITS>::end() {\n+#ifndef PRODUCT\n+  size_t fallback_table_size = _fallback_table != nullptr ? _fallback_table->get_mem_size(Thread::current()) : 0;\n+  log_info(gc)(\"Total forwardings: \" UINT64_FORMAT \", fallback forwardings: \" UINT64_FORMAT\n+                \", ratio: %f, memory used by fallback table: %zu%s, memory used by bases table: %zu%s\",\n+               _num_forwardings, _num_fallback_forwardings, static_cast<float>(_num_forwardings) \/ static_cast<float>(_num_fallback_forwardings),\n+               byte_size_in_proper_unit(fallback_table_size),\n+               proper_unit_for_byte_size(fallback_table_size),\n+               byte_size_in_proper_unit(sizeof(HeapWord*) * _num_regions),\n+               proper_unit_for_byte_size(sizeof(HeapWord*) * _num_regions));\n+#endif\n+#ifdef _LP64\n+  assert(_bases_table != nullptr, \"should be initialized\");\n+  FREE_C_HEAP_ARRAY(HeapWord*, _bases_table);\n+  _bases_table = nullptr;\n+  if (_fallback_table != nullptr) {\n+    delete _fallback_table;\n+    _fallback_table = nullptr;\n+  }\n+#endif\n+}\n+\n+template <int BITS>\n+void FullGCForwardingImpl<BITS>::maybe_init_fallback_table() {\n+  if (_fallback_table == nullptr) {\n+    FallbackTable* fallback_table = new FallbackTable(_fallback_table_log2_start_size);\n+    FallbackTable* prev = AtomicAccess::cmpxchg(&_fallback_table, static_cast<FallbackTable*>(nullptr), fallback_table);\n+    if (prev != nullptr) {\n+      \/\/ Another thread won, discard our table.\n+      delete fallback_table;\n+    }\n+  }\n+}\n+\n+template <int BITS>\n+void FullGCForwardingImpl<BITS>::fallback_forward_to(HeapWord* from, HeapWord* to) {\n+  assert(to != nullptr, \"no null forwarding\");\n+  maybe_init_fallback_table();\n+  assert(_fallback_table != nullptr, \"should be initialized\");\n+  FallbackTableLookup lookup_f(from);\n+  ForwardingEntry entry(from, to);\n+  auto found_f = [&](ForwardingEntry* found) {\n+    \/\/ If dupe has been found, override it with new value.\n+    \/\/ This is also called when new entry is succussfully inserted.\n+    if (found->_to != to) {\n+      found->_to = to;\n+    }\n+  };\n+  Thread* current_thread = Thread::current();\n+  bool grow;\n+  bool added = _fallback_table->insert_get(current_thread, lookup_f, entry, found_f, &grow);\n+  NOT_PRODUCT(AtomicAccess::inc(&_num_fallback_forwardings);)\n+#ifdef ASSERT\n+  assert(fallback_forwardee(from) != nullptr, \"must have entered forwarding\");\n+  assert(fallback_forwardee(from) == to, \"forwarding must be correct, added: %s, from: \" PTR_FORMAT \", to: \" PTR_FORMAT \", fwd: \" PTR_FORMAT, BOOL_TO_STR(added), p2i(from), p2i(to), p2i(fallback_forwardee(from)));\n@@ -54,0 +330,4 @@\n+  if (grow) {\n+    _fallback_table->grow(current_thread);\n+    log_debug(gc)(\"grow fallback table to size: %zu bytes\", _fallback_table->get_mem_size(current_thread));\n+  }\n@@ -56,2 +336,12 @@\n-bool FullGCForwarding::is_forwarded(oop obj) {\n-  return obj->mark().is_forwarded();\n+template <int BITS>\n+HeapWord* FullGCForwardingImpl<BITS>::fallback_forwardee(HeapWord* from) {\n+  assert(_fallback_table != nullptr, \"fallback table must be present\");\n+  HeapWord* result;\n+  FallbackTableLookup lookup_f(from);\n+  auto found_f = [&](const ForwardingEntry* found) {\n+    result = found->_to;\n+  };\n+  bool found = _fallback_table->get(Thread::current(), lookup_f, found_f);\n+  assert(found, \"something must have been found\");\n+  assert(result != nullptr, \"must have found forwarding\");\n+  return result;\n@@ -60,1 +350,1 @@\n-#endif \/\/ GC_SHARED_FULLGCFORWARDING_INLINE_HPP\n+#endif \/\/ SHARE_GC_SHARED_FULLGCFORWARDING_INLINE_HPP\n","filename":"src\/hotspot\/share\/gc\/shared\/fullGCForwarding.inline.hpp","additions":310,"deletions":20,"binary":false,"changes":330,"status":"modified"},{"patch":"@@ -444,1 +444,1 @@\n-  assert(_word_size > 0, \"oop_size must be positive.\");\n+  assert(_base_size > 0, \"oop_size must be positive.\");\n@@ -446,1 +446,1 @@\n-  java_lang_Class::set_oop_size(mem, _word_size);\n+  java_lang_Class::set_oop_size(mem, _base_size);\n","filename":"src\/hotspot\/share\/gc\/shared\/memAllocator.cpp","additions":2,"deletions":2,"binary":false,"changes":4,"status":"modified"},{"patch":"@@ -27,1 +27,0 @@\n-#include \"gc\/shared\/fullGCForwarding.hpp\"\n@@ -204,2 +203,0 @@\n-\n-  FullGCForwarding::initialize_flags(MaxHeapSize);\n","filename":"src\/hotspot\/share\/gc\/shenandoah\/shenandoahArguments.cpp","additions":0,"deletions":3,"binary":false,"changes":3,"status":"modified"},{"patch":"@@ -245,0 +245,2 @@\n+    FullGCForwarding::begin();\n+\n@@ -254,0 +256,2 @@\n+\n+    FullGCForwarding::end();\n@@ -353,1 +357,3 @@\n-    size_t obj_size = p->size();\n+    size_t old_size = p->size();\n+    size_t new_size = p->copy_size(old_size, p->mark());\n+    size_t obj_size = _compact_point == cast_from_oop<HeapWord*>(p) ? old_size : new_size;\n@@ -371,0 +377,1 @@\n+      obj_size = _compact_point == cast_from_oop<HeapWord*>(p) ? old_size : new_size;\n@@ -881,0 +888,1 @@\n+      new_obj->initialize_hash_if_necessary(p);\n","filename":"src\/hotspot\/share\/gc\/shenandoah\/shenandoahFullGC.cpp","additions":9,"deletions":1,"binary":false,"changes":10,"status":"modified"},{"patch":"@@ -245,1 +245,2 @@\n-  size_t obj_size = p->size();\n+  size_t old_size = p->size();\n+  size_t new_size = p->copy_size(old_size, p->mark());\n@@ -252,1 +253,1 @@\n-    if ((_old_to_region != nullptr) && (_old_compact_point + obj_size > _old_to_region->end())) {\n+    if ((_old_to_region != nullptr) && (_old_compact_point + new_size > _old_to_region->end())) {\n@@ -274,0 +275,1 @@\n+    size_t obj_size = _old_compact_point == cast_from_oop<HeapWord*>(p) ? old_size : new_size;\n@@ -298,0 +300,1 @@\n+      obj_size = _old_compact_point == cast_from_oop<HeapWord*>(p) ? old_size : new_size;\n@@ -321,0 +324,1 @@\n+    size_t obj_size = _young_compact_point == cast_from_oop<HeapWord*>(p) ? old_size : new_size;\n@@ -344,0 +348,1 @@\n+      obj_size = _young_compact_point == cast_from_oop<HeapWord*>(p) ? old_size : new_size;\n","filename":"src\/hotspot\/share\/gc\/shenandoah\/shenandoahGenerationalFullGC.cpp","additions":7,"deletions":2,"binary":false,"changes":9,"status":"modified"},{"patch":"@@ -245,1 +245,8 @@\n-  size_t size = ShenandoahForwarding::size(p);\n+\n+  markWord mark = p->mark();\n+  if (ShenandoahForwarding::is_forwarded(mark)) {\n+    return ShenandoahForwarding::get_forwardee(p);\n+  }\n+  size_t old_size = ShenandoahForwarding::size(p);\n+  size_t size = p->copy_size(old_size, mark);\n+\n@@ -341,1 +348,1 @@\n-  Copy::aligned_disjoint_words(cast_from_oop<HeapWord*>(p), copy, size);\n+  Copy::aligned_disjoint_words(cast_from_oop<HeapWord*>(p), copy, old_size);\n@@ -359,0 +366,1 @@\n+    copy_val->initialize_hash_if_necessary(p);\n","filename":"src\/hotspot\/share\/gc\/shenandoah\/shenandoahGenerationalHeap.cpp","additions":10,"deletions":2,"binary":false,"changes":12,"status":"modified"},{"patch":"@@ -31,1 +31,1 @@\n-#include \"gc\/shared\/fullGCForwarding.hpp\"\n+#include \"gc\/shared\/fullGCForwarding.inline.hpp\"\n@@ -1293,1 +1293,7 @@\n-  size_t size = ShenandoahForwarding::size(p);\n+\n+  markWord mark = p->mark();\n+  if (ShenandoahForwarding::is_forwarded(mark)) {\n+    return ShenandoahForwarding::get_forwardee(p);\n+  }\n+  size_t old_size = ShenandoahForwarding::size(p);\n+  size_t size = p->copy_size(old_size, mark);\n@@ -1327,1 +1333,1 @@\n-  Copy::aligned_disjoint_words(cast_from_oop<HeapWord*>(p), copy, size);\n+  Copy::aligned_disjoint_words(cast_from_oop<HeapWord*>(p), copy, old_size);\n@@ -1334,0 +1340,1 @@\n+    copy_val->initialize_hash_if_necessary(p);\n","filename":"src\/hotspot\/share\/gc\/shenandoah\/shenandoahHeap.cpp","additions":10,"deletions":3,"binary":false,"changes":13,"status":"modified"},{"patch":"@@ -358,1 +358,2 @@\n-  const size_t size = ZUtils::object_size(from_addr);\n+  const size_t old_size = ZUtils::object_size(from_addr);\n+  const size_t size = ZUtils::copy_size(from_addr, old_size);\n@@ -367,0 +368,1 @@\n+  assert(to_addr != from_addr, \"addresses must be different\");\n@@ -369,1 +371,2 @@\n-  ZUtils::object_copy_disjoint(from_addr, to_addr, size);\n+  ZUtils::object_copy_disjoint(from_addr, to_addr, old_size);\n+  ZUtils::initialize_hash_if_necessary(to_addr, from_addr);\n@@ -610,1 +613,1 @@\n-  zaddress try_relocate_object_inner(zaddress from_addr, uint32_t partition_id) {\n+  zaddress try_relocate_object_inner(zaddress from_addr, uint32_t partition_id, size_t old_size) {\n@@ -613,1 +616,0 @@\n-    const size_t size = ZUtils::object_size(from_addr);\n@@ -615,0 +617,4 @@\n+    zoffset_end from_offset = to_zoffset_end(ZAddress::offset(from_addr));\n+    zoffset_end top = to_page != nullptr ? to_page->top() : to_zoffset_end(0);\n+    const size_t new_size = ZUtils::copy_size(from_addr, old_size);\n+    const size_t size = top == from_offset ? old_size : new_size;\n@@ -632,0 +638,4 @@\n+    if (old_size != new_size && ((top == from_offset) != (allocated_addr == from_addr))) {\n+      _allocator->undo_alloc_object(to_page, allocated_addr, size);\n+      return zaddress::null;\n+    }\n@@ -635,2 +645,2 @@\n-    if (_forwarding->in_place_relocation() && allocated_addr + size > from_addr) {\n-      ZUtils::object_copy_conjoint(from_addr, allocated_addr, size);\n+    if (_forwarding->in_place_relocation() && allocated_addr + old_size > from_addr) {\n+      ZUtils::object_copy_conjoint(from_addr, allocated_addr, old_size);\n@@ -638,1 +648,4 @@\n-      ZUtils::object_copy_disjoint(from_addr, allocated_addr, size);\n+      ZUtils::object_copy_disjoint(from_addr, allocated_addr, old_size);\n+    }\n+    if (from_addr != allocated_addr) {\n+      ZUtils::initialize_hash_if_necessary(allocated_addr, from_addr);\n@@ -652,1 +665,1 @@\n-  void update_remset_old_to_old(zaddress from_addr, zaddress to_addr) const {\n+  void update_remset_old_to_old(zaddress from_addr, zaddress to_addr, size_t size) const {\n@@ -674,3 +687,2 @@\n-    \/\/ Read the size from the to-object, since the from-object\n-    \/\/ could have been overwritten during in-place relocation.\n-    const size_t size = ZUtils::object_size(to_addr);\n+    assert(size <= ZUtils::object_size(to_addr), \"old size must be <= new size\");\n+    assert(size > 0, \"size must be set\");\n@@ -801,1 +813,1 @@\n-  void update_remset_for_fields(zaddress from_addr, zaddress to_addr) const {\n+  void update_remset_for_fields(zaddress from_addr, zaddress to_addr, size_t size) const {\n@@ -809,1 +821,1 @@\n-      update_remset_old_to_old(from_addr, to_addr);\n+      update_remset_old_to_old(from_addr, to_addr, size);\n@@ -825,1 +837,2 @@\n-    const zaddress to_addr = try_relocate_object_inner(from_addr, partition_id);\n+    size_t size = ZUtils::object_size(from_addr);\n+    const zaddress to_addr = try_relocate_object_inner(from_addr, partition_id, size);\n@@ -831,1 +844,1 @@\n-    update_remset_for_fields(from_addr, to_addr);\n+    update_remset_for_fields(from_addr, to_addr, size);\n","filename":"src\/hotspot\/share\/gc\/z\/zRelocate.cpp","additions":28,"deletions":15,"binary":false,"changes":43,"status":"modified"},{"patch":"@@ -37,3 +37,6 @@\n-  if (UseShenandoahGC) {\n-    \/\/ Leak Profiler uses mark words in the ways that might interfere\n-    \/\/ with concurrent GC uses of them. This affects Shenandoah.\n+  if (UseCompactObjectHeaders || UseShenandoahGC) {\n+    \/\/ 1. With a 32-bit mark word in Lilliput2, we don't have enough unused\n+    \/\/    bits to store edge index information in the mark word\n+    \/\/ 2. Even without compressed object headers, with Shenandoah, we don't\n+    \/\/    have enough free bits in the mark word because of needing that\n+    \/\/    space for forwarding pointers in the evacuation & update refs phase.\n","filename":"src\/hotspot\/share\/jfr\/leakprofiler\/leakProfiler.cpp","additions":6,"deletions":3,"binary":false,"changes":9,"status":"modified"},{"patch":"@@ -455,1 +455,1 @@\n-  if (offset == oopDesc::klass_offset_in_bytes()) {\n+  if (offset == 1 \/*oopDesc::klass_offset_in_bytes()*\/) {\n","filename":"src\/hotspot\/share\/jvmci\/jvmciCompilerToVM.cpp","additions":1,"deletions":1,"binary":false,"changes":2,"status":"modified"},{"patch":"@@ -165,1 +165,1 @@\n-  oopDesc_klass_offset_in_bytes = oopDesc::klass_offset_in_bytes();\n+  oopDesc_klass_offset_in_bytes = 1; \/\/oopDesc::klass_offset_in_bytes();\n","filename":"src\/hotspot\/share\/jvmci\/jvmciCompilerToVMInit.cpp","additions":1,"deletions":1,"binary":false,"changes":2,"status":"modified"},{"patch":"@@ -99,0 +99,1 @@\n+  NOT_PRODUCT(LOG_TAG(ihash)) \\\n","filename":"src\/hotspot\/share\/logging\/logTag.hpp","additions":1,"deletions":0,"binary":false,"changes":1,"status":"modified"},{"patch":"@@ -576,1 +576,1 @@\n-        oop m = java_lang_Class::create_basic_type_mirror(type2name(bt), bt, CHECK);\n+        oop m = java_lang_Class::create_basic_type_mirror(type2name(bt), bt, false, CHECK);\n","filename":"src\/hotspot\/share\/memory\/universe.cpp","additions":1,"deletions":1,"binary":false,"changes":2,"status":"modified"},{"patch":"@@ -289,0 +289,8 @@\n+\n+int ArrayKlass::hash_offset_in_bytes(oop obj, markWord m) const {\n+  assert(UseCompactObjectHeaders, \"only with compact i-hash\");\n+  arrayOop ary = arrayOop(obj);\n+  BasicType type = element_type();\n+  int length = LP64_ONLY(m.array_length()) NOT_LP64(ary->length());\n+  return ary->base_offset_in_bytes(type) + (length << log2_element_size());\n+}\n","filename":"src\/hotspot\/share\/oops\/arrayKlass.cpp","additions":8,"deletions":0,"binary":false,"changes":8,"status":"modified"},{"patch":"@@ -118,0 +118,2 @@\n+  int hash_offset_in_bytes(oop obj, markWord m) const;\n+\n","filename":"src\/hotspot\/share\/oops\/arrayKlass.hpp","additions":2,"deletions":0,"binary":false,"changes":2,"status":"modified"},{"patch":"@@ -47,5 +47,6 @@\n-#ifdef _LP64\n-  const size_t encoding_allows = nth_bit(narrow_klass_pointer_bits() + max_shift());\n-  constexpr size_t cap = 4 * G;\n-  return MIN2(encoding_allows, cap);\n-#else\n+ #ifdef _LP64\n+   const size_t encoding_allows = nth_bit(narrow_klass_pointer_bits() + max_shift());\n+   assert(!UseCompactObjectHeaders || max_klass_range_size_coh == encoding_allows, \"Sanity\");\n+   constexpr size_t cap = 4 * G;\n+   return MIN2(encoding_allows, cap);\n+ #else\n@@ -88,0 +89,4 @@\n+  \/\/ There is no technical reason preventing us from using other klass pointer bit lengths,\n+  \/\/ but it should be a deliberate choice\n+  ASSERT_HERE(_narrow_klass_pointer_bits == 32 || _narrow_klass_pointer_bits == 19);\n+\n@@ -241,1 +246,6 @@\n-    \/\/ In compact object header mode, with 22-bit narrowKlass, we don't attempt for\n+    \/\/ This handles the case that we - experimentally - reduce the number of\n+    \/\/ class pointer bits further, such that (shift + num bits) < 32.\n+    assert(len <= (size_t)nth_bit(narrow_klass_pointer_bits() + max_shift()),\n+           \"klass range size exceeds encoding, len: %zu, narrow_klass_pointer_bits: %d, max_shift: %d\", len, narrow_klass_pointer_bits(), max_shift());\n+\n+    \/\/ In compact object header mode, with 19-bit narrowKlass, we don't attempt for\n","filename":"src\/hotspot\/share\/oops\/compressedKlass.cpp","additions":16,"deletions":6,"binary":false,"changes":22,"status":"modified"},{"patch":"@@ -111,1 +111,1 @@\n-  static constexpr int narrow_klass_pointer_bits_coh = 22;\n+  static constexpr int narrow_klass_pointer_bits_coh = 19;\n@@ -191,2 +191,7 @@\n-  \/\/ Returns the maximum allowed klass range size. It is calculated from the length of the encoding range\n-  \/\/ resulting from the current encoding settings (base, shift), capped to a certain max. value.\n+  \/\/ Returns the maximum encoding range, given the current geometry (narrow klass bit size and shift)\n+  static size_t max_encoding_range_size() { return nth_bit(narrow_klass_pointer_bits() + max_shift()); }\n+\n+  \/\/ For use before pre-initialization\n+  static constexpr size_t max_klass_range_size_coh = nth_bit(narrow_klass_pointer_bits_coh + max_shift_coh);\n+\n+  \/\/ Returns the maximum allowed klass range size.\n","filename":"src\/hotspot\/share\/oops\/compressedKlass.hpp","additions":8,"deletions":3,"binary":false,"changes":11,"status":"modified"},{"patch":"@@ -542,1 +542,2 @@\n-  _init_thread(nullptr)\n+  _init_thread(nullptr),\n+  _hash_offset(parser.hash_offset())\n@@ -3856,1 +3857,1 @@\n-  st->print_cr(BULLET\"---- fields (total size %zu words):\", oop_size(obj));\n+  st->print_cr(BULLET\"---- fields (total size %zu words):\", oop_size(obj, obj->mark()));\n","filename":"src\/hotspot\/share\/oops\/instanceKlass.cpp","additions":3,"deletions":2,"binary":false,"changes":5,"status":"modified"},{"patch":"@@ -239,0 +239,2 @@\n+  int             _hash_offset;             \/\/ Offset of hidden field for i-hash\n+\n@@ -888,1 +890,1 @@\n-  size_t oop_size(oop obj)  const             { return size_helper(); }\n+  size_t oop_size(oop obj, markWord mark) const { return size_helper(); }\n@@ -955,0 +957,9 @@\n+  virtual int hash_offset_in_bytes(oop obj, markWord m) const {\n+    assert(UseCompactObjectHeaders, \"only with compact i-hash\");\n+    return _hash_offset;\n+  }\n+  static int hash_offset_offset_in_bytes() {\n+    assert(UseCompactObjectHeaders, \"only with compact i-hash\");\n+    return (int)offset_of(InstanceKlass, _hash_offset);\n+  }\n+\n","filename":"src\/hotspot\/share\/oops\/instanceKlass.hpp","additions":12,"deletions":1,"binary":false,"changes":13,"status":"modified"},{"patch":"@@ -70,1 +70,2 @@\n-  virtual size_t oop_size(oop obj) const;\n+  size_t oop_size(oop obj, markWord mark) const;\n+  int hash_offset_in_bytes(oop obj, markWord m) const;\n@@ -94,1 +95,1 @@\n-  instanceOop allocate_instance(Klass* k, TRAPS);\n+  instanceOop allocate_instance(Klass* k, bool extend, TRAPS);\n","filename":"src\/hotspot\/share\/oops\/instanceMirrorKlass.hpp","additions":3,"deletions":2,"binary":false,"changes":5,"status":"modified"},{"patch":"@@ -66,1 +66,1 @@\n-size_t InstanceStackChunkKlass::oop_size(oop obj) const {\n+size_t InstanceStackChunkKlass::oop_size(oop obj, markWord mark) const {\n","filename":"src\/hotspot\/share\/oops\/instanceStackChunkKlass.cpp","additions":1,"deletions":1,"binary":false,"changes":2,"status":"modified"},{"patch":"@@ -130,1 +130,1 @@\n-  virtual size_t oop_size(oop obj) const override;\n+  size_t oop_size(oop obj, markWord mark) const override;\n","filename":"src\/hotspot\/share\/oops\/instanceStackChunkKlass.hpp","additions":1,"deletions":1,"binary":false,"changes":2,"status":"modified"},{"patch":"@@ -56,0 +56,1 @@\n+#include \"utilities\/numberSeq.hpp\"\n@@ -1357,0 +1358,13 @@\n+\n+static int expanded = 0;\n+static int not_expanded = 0;\n+static NumberSeq seq = NumberSeq();\n+\n+bool Klass::expand_for_hash(oop obj, markWord m) const {\n+  assert(UseCompactObjectHeaders, \"only with compact i-hash\");\n+  {\n+    ResourceMark rm;\n+    assert((size_t)hash_offset_in_bytes(obj,m ) <= (obj->base_size_given_klass(m, this) * HeapWordSize), \"hash offset must be eq or lt base size: hash offset: %d, base size: %zu, class-name: %s\", hash_offset_in_bytes(obj, m), obj->base_size_given_klass(m, this) * HeapWordSize, external_name());\n+  }\n+  return obj->base_size_given_klass(m, this) * HeapWordSize - hash_offset_in_bytes(obj, m) < (int)sizeof(uint32_t);\n+}\n","filename":"src\/hotspot\/share\/oops\/klass.cpp","additions":14,"deletions":0,"binary":false,"changes":14,"status":"modified"},{"patch":"@@ -653,1 +653,2 @@\n-  virtual size_t oop_size(oop obj) const = 0;\n+  virtual size_t oop_size(oop obj, markWord mark) const = 0;\n+  size_t oop_size(oop obj) const;\n@@ -800,0 +801,6 @@\n+\n+  virtual int hash_offset_in_bytes(oop obj, markWord m) const = 0;\n+\n+  static int kind_offset_in_bytes() { return (int)offset_of(Klass, _kind); }\n+\n+  bool expand_for_hash(oop obj, markWord m) const;\n","filename":"src\/hotspot\/share\/oops\/klass.hpp","additions":8,"deletions":1,"binary":false,"changes":9,"status":"modified"},{"patch":"@@ -178,0 +178,4 @@\n+inline size_t Klass::oop_size(oop obj) const {\n+  return oop_size(obj, obj->mark());\n+}\n+\n","filename":"src\/hotspot\/share\/oops\/klass.inline.hpp","additions":4,"deletions":0,"binary":false,"changes":4,"status":"modified"},{"patch":"@@ -32,3 +32,4 @@\n-STATIC_ASSERT(markWord::klass_shift + markWord::klass_bits == 64);\n-\/\/ The hash (preceding klass bits) shall be a direct neighbor but not interleave\n-STATIC_ASSERT(markWord::klass_shift == markWord::hash_bits + markWord::hash_shift);\n+STATIC_ASSERT(markWord::klass_shift + markWord::klass_bits == 32);\n+\/\/ The hashctrl bits (preceding klass bits) shall be a direct neighbor but not interleave\n+\/\/ klass_shift=13, hashctrl_shift=11, hashctrl_bits=2, so 13 == 2 + 11\n+STATIC_ASSERT(markWord::klass_shift == markWord::hashctrl_bits + markWord::hashctrl_shift);\n@@ -76,0 +77,3 @@\n+      } else if (UseCompactObjectHeaders) {\n+        st->print(\" hash is-hashed=%s is-copied=%s\", BOOL_TO_STR(is_hashed_not_expanded()), BOOL_TO_STR(\n+                is_hashed_expanded()));\n","filename":"src\/hotspot\/share\/oops\/markWord.cpp","additions":7,"deletions":3,"binary":false,"changes":10,"status":"modified"},{"patch":"@@ -48,1 +48,3 @@\n-\/\/  klass:22  hash:31 -->| unused_gap:4  age:4  self-fwd:1  lock:2 (normal object)\n+\/\/  unused:32 klass:19 hashctrl:2 -->| unused_gap:4  age:4  self-fwd:1  lock:2 (normal object)\n+\/\/\n+\/\/  Note: klass occupies bits 13-31 (19 bits), hashctrl occupies bits 11-12 (2 bits)\n@@ -55,0 +57,13 @@\n+\/\/  - With +UseCompactObjectHeaders:\n+\/\/    hashctrl bits indicate if object has been hashed:\n+\/\/    00 - never hashed\n+\/\/    01 - hashed, but not expanded by GC: will recompute hash\n+\/\/    10 - not hashed, but expanded; special state used only by CDS to deal with scratch classes\n+\/\/    11 - hashed and expanded by GC, and hashcode has been installed in hidden field\n+\/\/\n+\/\/    When identityHashCode() is called, the transitions work as follows:\n+\/\/    00 - set the hashctrl bits to 01, and compute the identity hash\n+\/\/    01 - recompute idendity hash. When GC encounters 01 when moving an object, it will allocate an extra word, if\n+\/\/         necessary, for the object copy, and install 11.\n+\/\/    11 - read hashcode from field\n+\/\/\n@@ -107,0 +122,1 @@\n+  uint32_t value32() const { return (uint32_t)_value; }\n@@ -115,0 +131,1 @@\n+  static const int hashctrl_bits                  = 2;\n@@ -120,0 +137,1 @@\n+  static const int hashctrl_shift                 = age_shift + age_bits + unused_gap_bits;\n@@ -129,0 +147,4 @@\n+  static const uintptr_t hashctrl_mask            = right_n_bits(hashctrl_bits);\n+  static const uintptr_t hashctrl_mask_in_place   = hashctrl_mask << hashctrl_shift;\n+  static const uintptr_t hashctrl_hashed_mask_in_place    = ((uintptr_t)1) << hashctrl_shift;\n+  static const uintptr_t hashctrl_expanded_mask_in_place  = ((uintptr_t)2) << hashctrl_shift;\n@@ -132,7 +154,17 @@\n-  \/\/ We store the (narrow) Klass* in the bits 43 to 64.\n-\n-  \/\/ These are for bit-precise extraction of the narrow Klass* from the 64-bit Markword\n-  static constexpr int klass_offset_in_bytes      = 4;\n-  static constexpr int klass_shift                = hash_shift + hash_bits;\n-  static constexpr int klass_shift_at_offset      = klass_shift - klass_offset_in_bytes * BitsPerByte;\n-  static constexpr int klass_bits                 = 22;\n+  \/\/ With UseCompactObjectHeaders: We store the (narrow) Klass* in bits 13-31 (19 bits total).\n+  \/\/ Without UseCompactObjectHeaders: Klass* is stored separately in object header, not in markword.\n+\n+  \/\/ These are for bit-precise extraction of the narrow Klass* from the markword (UseCompactObjectHeaders only)\n+  \/\/\n+  \/\/ Bit position summary for UseCompactObjectHeaders:\n+  \/\/ Bits  0- 1: lock (2 bits)\n+  \/\/ Bit   2   : self-fwd (1 bit)\n+  \/\/ Bits  3- 6: age (4 bits)\n+  \/\/ Bits  7-10: unused_gap (4 bits)\n+  \/\/ Bits 11-12: hashctrl (2 bits) - hash control state\n+  \/\/ Bits 13-31: klass (19 bits) - narrow klass pointer\n+  \/\/ Bits 32-63: unused (32 bits)\n+  \/\/\n+  \/\/ Without UseCompactObjectHeaders, klass is stored separately in object header\n+  static constexpr int klass_shift                = hashctrl_shift + hashctrl_bits;\n+  static constexpr int klass_bits                 = 19;\n@@ -148,0 +180,1 @@\n+  static const uintptr_t forward_expanded_value   = 0b111;\n@@ -166,1 +199,1 @@\n-    return (mask_bits(value(), lock_mask_in_place) == marked_value);\n+    return (value() & (self_fwd_mask_in_place | lock_mask_in_place)) > monitor_value;\n@@ -176,0 +209,8 @@\n+  markWord set_forward_expanded() {\n+    assert((value() & (lock_mask_in_place | self_fwd_mask_in_place)) == marked_value, \"must be normal-forwarded here\");\n+    return markWord(value() | forward_expanded_value);\n+  }\n+  bool is_forward_expanded() {\n+    return (value() & (lock_mask_in_place | self_fwd_mask_in_place)) == forward_expanded_value;\n+  }\n+\n@@ -191,1 +232,1 @@\n-    return (!is_unlocked() || !has_no_hash());\n+    return UseCompactObjectHeaders ? !is_unlocked() : (!is_unlocked() || !has_no_hash());\n@@ -225,0 +266,1 @@\n+    assert(!UseCompactObjectHeaders, \"Do not use with compact i-hash\");\n@@ -250,1 +292,1 @@\n-  markWord clear_lock_bits() const { return markWord(value() & ~lock_mask_in_place); }\n+  markWord clear_lock_bits() const { return markWord(value() & ~(lock_mask_in_place | self_fwd_mask_in_place)); }\n@@ -265,0 +307,1 @@\n+    assert(!UseCompactObjectHeaders, \"only without compact i-hash\");\n@@ -269,1 +312,61 @@\n-    return hash() == no_hash;\n+    if (UseCompactObjectHeaders) {\n+      return !is_hashed();\n+    } else {\n+      return hash() == no_hash;\n+    }\n+  }\n+\n+  inline bool is_hashed_not_expanded() const {\n+    assert(UseCompactObjectHeaders, \"only with compact i-hash\");\n+    return (value() & hashctrl_mask_in_place) == hashctrl_hashed_mask_in_place;\n+  }\n+  inline markWord set_hashed_not_expanded() const {\n+    assert(UseCompactObjectHeaders, \"only with compact i-hash\");\n+    return markWord((value() & ~hashctrl_mask_in_place) | hashctrl_hashed_mask_in_place);\n+  }\n+\n+  inline bool is_hashed_expanded() const {\n+    assert(UseCompactObjectHeaders, \"only with compact i-hash\");\n+    return (value() & hashctrl_mask_in_place) == (hashctrl_hashed_mask_in_place | hashctrl_expanded_mask_in_place);\n+  }\n+  inline markWord set_hashed_expanded() const {\n+    assert(UseCompactObjectHeaders, \"only with compact i-hash\");\n+    return markWord((value() & ~hashctrl_mask_in_place) | (hashctrl_hashed_mask_in_place | hashctrl_expanded_mask_in_place));\n+  }\n+\n+  \/\/ This is a special hashctrl state (11) that is only used\n+  \/\/ during CDS archive dumping. There we allocate 'scratch mirrors' for\n+  \/\/ each real mirror klass. We allocate those scratch mirrors\n+  \/\/ in a pre-extended form, but without being hashed. When the\n+  \/\/ real mirror gets hashed, then we turn the scratch mirror into\n+  \/\/ hashed_moved state, otherwise we leave it in that special state\n+  \/\/ which indicates that the archived copy will be allocated in the\n+  \/\/ unhashed form.\n+  inline bool is_not_hashed_expanded() const {\n+    assert(UseCompactObjectHeaders, \"only with compact i-hash\");\n+    return (value() & hashctrl_mask_in_place) == hashctrl_expanded_mask_in_place;\n+  }\n+  inline markWord set_not_hashed_expanded() const {\n+    assert(UseCompactObjectHeaders, \"only with compact i-hash\");\n+    return markWord((value() & ~hashctrl_mask_in_place) | hashctrl_expanded_mask_in_place);\n+  }\n+  inline markWord set_not_hashed_not_expanded() const {\n+    assert(UseCompactObjectHeaders, \"only with compact i-hash\");\n+    return markWord(value() & ~(hashctrl_mask_in_place | hashctrl_expanded_mask_in_place));\n+  }\n+  \/\/ Return true when object is either hashed_moved or not_hashed_moved.\n+  inline bool is_expanded() const {\n+    assert(UseCompactObjectHeaders, \"only with compact i-hash\");\n+    return (value() & hashctrl_expanded_mask_in_place) != 0;\n+  }\n+  inline bool is_hashed() const {\n+    assert(UseCompactObjectHeaders, \"only with compact i-hash\");\n+    return (value() & hashctrl_hashed_mask_in_place) != 0;\n+  }\n+\n+  inline markWord copy_hashctrl_from(markWord m) const {\n+    if (UseCompactObjectHeaders) {\n+      return markWord((value() & ~hashctrl_mask_in_place) | (m.value() & hashctrl_mask_in_place));\n+    } else {\n+      return markWord(value());\n+    }\n@@ -278,0 +381,4 @@\n+#ifdef _LP64\n+  inline int array_length() { return checked_cast<int>(value() >> 32); }\n+#endif\n+\n@@ -280,1 +387,5 @@\n-    return markWord( no_hash_in_place | no_lock_in_place );\n+    if (UseCompactObjectHeaders) {\n+      return markWord(no_lock_in_place);\n+    } else {\n+      return markWord(no_hash_in_place | no_lock_in_place);\n+    }\n@@ -293,1 +404,2 @@\n-    return mask_bits(value(), self_fwd_mask_in_place) != 0;\n+    \/\/ Match 100, 101, 110 but not 111.\n+    return mask_bits(value() + 1, (lock_mask_in_place | self_fwd_mask_in_place)) > 4;\n","filename":"src\/hotspot\/share\/oops\/markWord.hpp","additions":126,"deletions":14,"binary":false,"changes":140,"status":"modified"},{"patch":"@@ -35,1 +35,1 @@\n-  return narrowKlass(value() >> klass_shift);\n+  return narrowKlass(value32() >> klass_shift);\n","filename":"src\/hotspot\/share\/oops\/markWord.inline.hpp","additions":1,"deletions":1,"binary":false,"changes":2,"status":"modified"},{"patch":"@@ -146,1 +146,1 @@\n-size_t ObjArrayKlass::oop_size(oop obj) const {\n+size_t ObjArrayKlass::oop_size(oop obj, markWord mark) const {\n@@ -151,1 +151,2 @@\n-  return objArrayOop(obj)->object_size();\n+  int length = LP64_ONLY(UseCompactObjectHeaders ? mark.array_length() :) objArrayOop(obj)->length();\n+  return objArrayOop(obj)->object_size(length);\n","filename":"src\/hotspot\/share\/oops\/objArrayKlass.cpp","additions":3,"deletions":2,"binary":false,"changes":5,"status":"modified"},{"patch":"@@ -83,1 +83,1 @@\n-  size_t oop_size(oop obj) const;\n+  size_t oop_size(oop obj, markWord) const;\n","filename":"src\/hotspot\/share\/oops\/objArrayKlass.hpp","additions":1,"deletions":1,"binary":false,"changes":2,"status":"modified"},{"patch":"@@ -39,2 +39,2 @@\n-    _oop_base_offset_in_bytes = sizeof(markWord);\n-    _oop_has_klass_gap = false;\n+    _oop_base_offset_in_bytes = sizeof(uint32_t);\n+    _oop_has_klass_gap = true;\n","filename":"src\/hotspot\/share\/oops\/objLayout.cpp","additions":2,"deletions":2,"binary":false,"changes":4,"status":"modified"},{"patch":"@@ -129,0 +129,17 @@\n+markWord oopDesc::initialize_hash_if_necessary(oop obj, Klass* k, markWord m) {\n+  assert(UseCompactObjectHeaders, \"only with compact object headers\");\n+  assert(!m.has_displaced_mark_helper(), \"must not be displaced header\");\n+  assert(m.is_hashed_not_expanded(), \"must be hashed but not moved\");\n+  assert(!m.is_hashed_expanded(), \"must not be moved: \" INTPTR_FORMAT, m.value());\n+  uint32_t hash = static_cast<uint32_t>(ObjectSynchronizer::get_next_hash(nullptr, obj));\n+  int offset = k->hash_offset_in_bytes(cast_to_oop(this), m);\n+  assert(offset >= 4, \"hash offset must not be in header\");\n+  log_develop_trace(gc)(\"Initializing hash for \" PTR_FORMAT \", old: \" PTR_FORMAT \", hash: %d, offset: %d, is_mirror: %s\", p2i(this), p2i(obj), hash, offset, BOOL_TO_STR(k->is_mirror_instance_klass()));\n+  int_field_put(offset, (jint) hash);\n+  m = m.set_hashed_expanded();\n+  assert(static_cast<uint32_t>(ObjectSynchronizer::get_hash(m, cast_to_oop(this), k)) == hash,\n+         \"hash must remain the same\");\n+  assert(m.narrow_klass() != 0, \"must not be null\");\n+  return m;\n+}\n+\n","filename":"src\/hotspot\/share\/oops\/oop.cpp","additions":17,"deletions":0,"binary":false,"changes":17,"status":"modified"},{"patch":"@@ -75,0 +75,1 @@\n+  inline void set_mark_full(markWord m);\n@@ -117,0 +118,22 @@\n+  \/\/ Returns the size that a copy of this object requires, in machine words.\n+  \/\/ It can be 1 word larger than its current size to accomodate\n+  \/\/ an additional 4-byte-field for the identity hash-code.\n+  \/\/\n+  \/\/ size: the current size of this object, we're passing this here for performance\n+  \/\/       reasons, because all callers compute this anyway, and we want to avoid\n+  \/\/       recomputing it.\n+  \/\/ mark: the mark-word of this object. Some callers (e.g. G1ParScanThreadState::do_copy_to_survivor_space())\n+  \/\/       need to use a known markWord because of racing GC threads that can change\n+  \/\/       the markWord at any time.\n+  inline size_t copy_size(size_t size, markWord mark) const;\n+  \/\/ Special version to deal with scratch classes in CDS. There we allocate\n+  \/\/ temporary scratch classes (which are skeleton versions of InstanceMirrorKlass,\n+  \/\/ which represent java.lang.Class objects in the CDS archive). At that point, we\n+  \/\/ don't know whether or not the final archived version will be hashed or expanded,\n+  \/\/ and therefore we allocate them in the special state not-hashed-but-expanded.\n+  \/\/ When creating the final copy of those objects, we either populate the hidden hash\n+  \/\/ field and make the object 'expanded', or we turn it back to 'not-hashed'\n+  \/\/ and reduce the object's size. We do this by providing a separate method for CDS\n+  \/\/ so that we don't affect GC performance.\n+  inline size_t copy_size_cds(size_t size, markWord mark) const;\n+\n@@ -119,1 +142,2 @@\n-  inline size_t size_given_klass(Klass* klass);\n+  inline size_t base_size_given_klass(markWord m, const Klass* klass);\n+  inline size_t size_given_mark_and_klass(markWord mrk, const Klass* kls);\n@@ -287,0 +311,1 @@\n+  inline void reset_forwarded();\n@@ -317,0 +342,6 @@\n+  \/\/ Initialize identity hash code in hash word of object copy from original object.\n+  \/\/ Returns true if the object has been expanded, false otherwise.\n+  inline void initialize_hash_if_necessary(oop obj);\n+  \/\/ For CDS only.\n+  markWord initialize_hash_if_necessary(oop obj, Klass* k, markWord m);\n+\n@@ -333,10 +364,2 @@\n-#ifdef _LP64\n-    if (UseCompactObjectHeaders) {\n-      \/\/ NOTE: The only places where this is used with compact headers are the C2\n-      \/\/ compiler and JVMCI.\n-      return mark_offset_in_bytes() + markWord::klass_offset_in_bytes;\n-    } else\n-#endif\n-    {\n-      return (int)offset_of(oopDesc, _metadata._klass);\n-    }\n+    assert(!UseCompactObjectHeaders, \"don't use this with compact headers\");\n+    return (int)offset_of(oopDesc, _metadata._klass);\n@@ -346,1 +369,5 @@\n-    return klass_offset_in_bytes() + sizeof(narrowKlass);\n+    if (UseCompactObjectHeaders) {\n+      return base_offset_in_bytes();\n+    } else {\n+      return klass_offset_in_bytes() + sizeof(narrowKlass);\n+    }\n","filename":"src\/hotspot\/share\/oops\/oop.hpp","additions":39,"deletions":12,"binary":false,"changes":51,"status":"modified"},{"patch":"@@ -62,0 +62,8 @@\n+  if (UseCompactObjectHeaders) {\n+    AtomicAccess::store(reinterpret_cast<uint32_t volatile*>(&_mark), m.value32());\n+  } else {\n+    AtomicAccess::store(&_mark, m);\n+  }\n+}\n+\n+void oopDesc::set_mark_full(markWord m) {\n@@ -66,1 +74,5 @@\n-  *(markWord*)(((char*)mem) + mark_offset_in_bytes()) = m;\n+  if (UseCompactObjectHeaders) {\n+    *(uint32_t*)(((char*)mem) + mark_offset_in_bytes()) = m.value32();\n+  } else {\n+    *(markWord*)(((char*)mem) + mark_offset_in_bytes()) = m;\n+  }\n@@ -70,1 +82,5 @@\n-  AtomicAccess::release_store((markWord*)(((char*)mem) + mark_offset_in_bytes()), m);\n+  if (UseCompactObjectHeaders) {\n+    AtomicAccess::release_store((uint32_t*)(((char*)mem) + mark_offset_in_bytes()), m.value32());\n+  } else {\n+    AtomicAccess::release_store((markWord*)(((char*)mem) + mark_offset_in_bytes()), m);\n+  }\n@@ -74,1 +90,5 @@\n-  AtomicAccess::release_store(&_mark, m);\n+  if (UseCompactObjectHeaders) {\n+    AtomicAccess::release_store(reinterpret_cast<uint32_t volatile*>(&_mark), m.value32());\n+  } else {\n+    AtomicAccess::release_store(&_mark, m);\n+  }\n@@ -94,1 +114,7 @@\n-  set_mark(prototype_mark());\n+  if (UseCompactObjectHeaders) {\n+    markWord m = prototype_mark().copy_hashctrl_from(mark());\n+    assert(m.is_neutral(), \"must be neutral\");\n+    set_mark(m);\n+  } else {\n+    set_mark(prototype_mark());\n+  }\n@@ -185,0 +211,41 @@\n+size_t oopDesc::size_given_mark_and_klass(markWord mrk, const Klass* kls) {\n+  size_t sz = base_size_given_klass(mrk, kls);\n+  if (UseCompactObjectHeaders) {\n+    assert(!mrk.has_displaced_mark_helper(), \"must not be displaced\");\n+    if (mrk.is_expanded() && kls->expand_for_hash(cast_to_oop(this), mrk)) {\n+      sz = align_object_size(sz + 1);\n+    }\n+  }\n+  return sz;\n+}\n+\n+size_t oopDesc::copy_size(size_t size, markWord mark) const {\n+  if (UseCompactObjectHeaders) {\n+    assert(!mark.has_displaced_mark_helper(), \"must not be displaced\");\n+    Klass* klass = mark.klass();\n+    if (mark.is_hashed_not_expanded() && klass->expand_for_hash(cast_to_oop(this), mark)) {\n+      size = align_object_size(size + 1);\n+    }\n+  }\n+  assert(is_object_aligned(size), \"Oop size is not properly aligned: %zu\", size);\n+  return size;\n+}\n+\n+size_t oopDesc::copy_size_cds(size_t size, markWord mark) const {\n+  if (UseCompactObjectHeaders) {\n+    assert(!mark.has_displaced_mark_helper(), \"must not be displaced\");\n+    Klass* klass = mark.klass();\n+    if (mark.is_not_hashed_expanded()) {\n+      assert(klass->expand_for_hash(cast_to_oop(this), mark), \"must be?\");\n+    }\n+    if (mark.is_hashed_not_expanded() && klass->expand_for_hash(cast_to_oop(this), mark)) {\n+      size = align_object_size(size + 1);\n+    }\n+    if (mark.is_not_hashed_expanded() && klass->expand_for_hash(cast_to_oop(this), mark)) {\n+      size = align_object_size(size - ObjectAlignmentInBytes \/ HeapWordSize);\n+    }\n+  }\n+  assert(is_object_aligned(size), \"Oop size is not properly aligned: %zu\", size);\n+  return size;\n+}\n+\n@@ -186,1 +253,1 @@\n-  return size_given_klass(klass());\n+  return size_given_mark_and_klass(mark(), klass());\n@@ -189,1 +256,1 @@\n-size_t oopDesc::size_given_klass(Klass* klass)  {\n+size_t oopDesc::base_size_given_klass(markWord mrk, const Klass* klass)  {\n@@ -208,1 +275,1 @@\n-      s = klass->oop_size(this);\n+      s = klass->oop_size(this, mrk);\n@@ -217,1 +284,8 @@\n-      size_t array_length = (size_t) ((arrayOop)this)->length();\n+      size_t array_length;\n+#ifdef _LP64\n+      if (UseCompactObjectHeaders) {\n+        array_length = (size_t) mrk.array_length();\n+      } else\n+#endif\n+        array_length = (size_t)((arrayOop)this)->length();\n+\n@@ -225,2 +299,6 @@\n-\n-      assert(s == klass->oop_size(this), \"wrong array object size\");\n+      if (s != klass->oop_size(this, mrk)) {\n+        tty->print_cr(\"length: %zu\", array_length);\n+        tty->print_cr(\"log element size: %d\", Klass::layout_helper_log2_element_size(lh));\n+        tty->print_cr(\"is_objArray: %s\", BOOL_TO_STR(klass->is_objArray_klass()));\n+      }\n+      assert(s == klass->oop_size(this, mrk), \"wrong array object size, s: %zu, oop_size: %zu\", s, klass->oop_size(this, mrk));\n@@ -229,1 +307,1 @@\n-      s = klass->oop_size(this);\n+      s = klass->oop_size(this, mrk);\n@@ -312,0 +390,3 @@\n+  if (UseCompactObjectHeaders && p->mark().is_expanded() && !mark().is_expanded()) {\n+    m = m.set_forward_expanded();\n+  }\n@@ -313,1 +394,1 @@\n-  set_mark(m);\n+  set_mark_full(m);\n@@ -320,0 +401,16 @@\n+void oopDesc::reset_forwarded() {\n+  markWord m = mark();\n+  if (m.is_self_forwarded()) {\n+    unset_self_forwarded();\n+  } else if (m.is_forwarded()) {\n+    \/\/ Restore Klass* and hash-bits in the header,\n+    \/\/ for correct iteration.\n+    markWord fwd_mark = forwardee()->mark();\n+    if (m.is_forward_expanded()) {\n+      \/\/ Un-expand original object.\n+      fwd_mark = fwd_mark.set_hashed_not_expanded();\n+    }\n+    set_mark_full(fwd_mark);\n+  }\n+}\n+\n@@ -398,1 +495,1 @@\n-  size_t size = size_given_klass(k);\n+  size_t size = size_given_mark_and_klass(mark(), k);\n@@ -406,1 +503,1 @@\n-  size_t size = size_given_klass(k);\n+  size_t size = size_given_mark_and_klass(mark(), k);\n@@ -430,5 +527,7 @@\n-  markWord mrk = mark();\n-  if (mrk.is_unlocked() && !mrk.has_no_hash()) {\n-    return mrk.hash();\n-  } else if (mrk.is_marked()) {\n-    return mrk.hash();\n+  if (UseCompactObjectHeaders) {\n+    markWord mrk = mark();\n+    if (mrk.is_hashed_expanded()) {\n+      Klass* klass = mrk.klass();\n+      return int_field(klass->hash_offset_in_bytes(cast_to_oop(this), mrk));\n+    }\n+    \/\/ Fall-through to slow-case.\n@@ -436,1 +535,7 @@\n-    return slow_identity_hash();\n+    markWord mrk = mark();\n+    if (mrk.is_unlocked() && !mrk.has_no_hash()) {\n+      return mrk.hash();\n+    } else if (mrk.is_marked()) {\n+      return mrk.hash();\n+    }\n+    \/\/ Fall-through to slow-case.\n@@ -438,0 +543,1 @@\n+  return slow_identity_hash();\n@@ -445,1 +551,1 @@\n-  return mrk.is_unlocked() && mrk.has_no_hash();\n+  return (UseCompactObjectHeaders || mrk.is_unlocked()) && mrk.has_no_hash();\n@@ -468,0 +574,14 @@\n+inline void oopDesc::initialize_hash_if_necessary(oop obj) {\n+  if (!UseCompactObjectHeaders) {\n+    return;\n+  }\n+  markWord m = mark();\n+  assert(!m.has_displaced_mark_helper(), \"must not be displaced header\");\n+  \/\/ ZGC temporarily marks the header in ZObjArrayAllocator::initialize().\n+  assert(UseZGC || !m.is_forwarded(), \"must not be forwarded header\");\n+  if (m.is_hashed_not_expanded()) {\n+    set_mark(initialize_hash_if_necessary(obj, m.klass(), m));\n+  }\n+}\n+\n+\n","filename":"src\/hotspot\/share\/oops\/oop.inline.hpp","additions":141,"deletions":21,"binary":false,"changes":162,"status":"modified"},{"patch":"@@ -136,0 +136,1 @@\n+  inline size_t object_size(const TypeArrayKlass* tk, int length) const;\n","filename":"src\/hotspot\/share\/oops\/typeArrayOop.hpp","additions":1,"deletions":0,"binary":false,"changes":1,"status":"modified"},{"patch":"@@ -34,0 +34,4 @@\n+size_t typeArrayOopDesc::object_size(const TypeArrayKlass* tk, int length) const {\n+  return object_size(tk->layout_helper(), length);\n+}\n+\n@@ -35,1 +39,1 @@\n-  return object_size(tk->layout_helper(), length());\n+  return object_size(tk, length());\n","filename":"src\/hotspot\/share\/oops\/typeArrayOop.inline.hpp","additions":5,"deletions":1,"binary":false,"changes":6,"status":"modified"},{"patch":"@@ -1385,1 +1385,1 @@\n-      } else if( offset == oopDesc::klass_offset_in_bytes() ) {\n+      } else if( offset == Type::klass_offset() ) {\n@@ -1558,1 +1558,1 @@\n-          (offset == oopDesc::klass_offset_in_bytes() && tj->base() == Type::AryPtr) ||\n+          (offset == Type::klass_offset() && tj->base() == Type::AryPtr) ||\n","filename":"src\/hotspot\/share\/opto\/compile.cpp","additions":2,"deletions":2,"binary":false,"changes":4,"status":"modified"},{"patch":"@@ -960,1 +960,1 @@\n-    Node* p = basic_plus_adr( ex_node, ex_node, oopDesc::klass_offset_in_bytes());\n+    Node* p = basic_plus_adr( ex_node, ex_node, Type::klass_offset());\n@@ -978,2 +978,2 @@\n-        Node* p = basic_plus_adr(ex_in, ex_in, oopDesc::klass_offset_in_bytes());\n-        Node* k = _gvn.transform(LoadKlassNode::make(_gvn, immutable_memory(), p, TypeInstPtr::KLASS, TypeInstKlassPtr::OBJECT));\n+        Node* p = basic_plus_adr(ex_in, ex_in, Type::klass_offset());\n+        Node* k = _gvn.transform( LoadKlassNode::make(_gvn, immutable_memory(), p, TypeInstPtr::KLASS, TypeInstKlassPtr::OBJECT));\n","filename":"src\/hotspot\/share\/opto\/doCall.cpp","additions":3,"deletions":3,"binary":false,"changes":6,"status":"modified"},{"patch":"@@ -3476,1 +3476,1 @@\n-  } else if (offset != oopDesc::klass_offset_in_bytes()) {\n+  } else if (offset != Type::klass_offset()) {\n@@ -4480,1 +4480,1 @@\n-      _compile->get_alias_index(tinst->add_offset(oopDesc::klass_offset_in_bytes()));\n+      _compile->get_alias_index(tinst->add_offset(Type::klass_offset()));\n","filename":"src\/hotspot\/share\/opto\/escape.cpp","additions":2,"deletions":2,"binary":false,"changes":4,"status":"modified"},{"patch":"@@ -1217,1 +1217,1 @@\n-  Node* k_adr = basic_plus_adr(obj, oopDesc::klass_offset_in_bytes());\n+  Node* k_adr = basic_plus_adr(obj, Type::klass_offset());\n@@ -3649,1 +3649,1 @@\n-    int klass_idx = C->get_alias_index(oop_type->add_offset(oopDesc::klass_offset_in_bytes()));\n+    int klass_idx = C->get_alias_index(oop_type->add_offset(Type::klass_offset()));\n","filename":"src\/hotspot\/share\/opto\/graphKit.cpp","additions":2,"deletions":2,"binary":false,"changes":4,"status":"modified"},{"patch":"@@ -4732,1 +4732,1 @@\n-  enum { _slow_path = 1, _fast_path, _null_path, PATH_LIMIT };\n+  enum { _slow_path = 1, _null_path, _fast_path, _fast_path2, PATH_LIMIT };\n@@ -4780,37 +4780,178 @@\n-  \/\/ Get the header out of the object, use LoadMarkNode when available\n-  Node* header_addr = basic_plus_adr(obj, oopDesc::mark_offset_in_bytes());\n-  \/\/ The control of the load must be null. Otherwise, the load can move before\n-  \/\/ the null check after castPP removal.\n-  Node* no_ctrl = nullptr;\n-  Node* header = make_load(no_ctrl, header_addr, TypeX_X, TypeX_X->basic_type(), MemNode::unordered);\n-\n-  if (!UseObjectMonitorTable) {\n-    \/\/ Test the header to see if it is safe to read w.r.t. locking.\n-    Node *lock_mask      = _gvn.MakeConX(markWord::lock_mask_in_place);\n-    Node *lmasked_header = _gvn.transform(new AndXNode(header, lock_mask));\n-    Node *monitor_val   = _gvn.MakeConX(markWord::monitor_value);\n-    Node *chk_monitor   = _gvn.transform(new CmpXNode(lmasked_header, monitor_val));\n-    Node *test_monitor  = _gvn.transform(new BoolNode(chk_monitor, BoolTest::eq));\n-\n-    generate_slow_guard(test_monitor, slow_region);\n-  }\n-\n-  \/\/ Get the hash value and check to see that it has been properly assigned.\n-  \/\/ We depend on hash_mask being at most 32 bits and avoid the use of\n-  \/\/ hash_mask_in_place because it could be larger than 32 bits in a 64-bit\n-  \/\/ vm: see markWord.hpp.\n-  Node *hash_mask      = _gvn.intcon(markWord::hash_mask);\n-  Node *hash_shift     = _gvn.intcon(markWord::hash_shift);\n-  Node *hshifted_header= _gvn.transform(new URShiftXNode(header, hash_shift));\n-  \/\/ This hack lets the hash bits live anywhere in the mark object now, as long\n-  \/\/ as the shift drops the relevant bits into the low 32 bits.  Note that\n-  \/\/ Java spec says that HashCode is an int so there's no point in capturing\n-  \/\/ an 'X'-sized hashcode (32 in 32-bit build or 64 in 64-bit build).\n-  hshifted_header      = ConvX2I(hshifted_header);\n-  Node *hash_val       = _gvn.transform(new AndINode(hshifted_header, hash_mask));\n-\n-  Node *no_hash_val    = _gvn.intcon(markWord::no_hash);\n-  Node *chk_assigned   = _gvn.transform(new CmpINode( hash_val, no_hash_val));\n-  Node *test_assigned  = _gvn.transform(new BoolNode( chk_assigned, BoolTest::eq));\n-\n-  generate_slow_guard(test_assigned, slow_region);\n+  if (UseCompactObjectHeaders) {\n+    \/\/ Get the header out of the object.\n+    Node* header_addr = basic_plus_adr(obj, oopDesc::mark_offset_in_bytes());\n+    \/\/ The control of the load must be null. Otherwise, the load can move before\n+    \/\/ the null check after castPP removal.\n+    Node* no_ctrl = nullptr;\n+    Node* header = make_load(no_ctrl, header_addr, TypeX_X, TypeX_X->basic_type(), MemNode::unordered);\n+\n+    \/\/ Test the header to see if the object is in hashed or copied state.\n+    Node* hashctrl_mask  = _gvn.MakeConX(markWord::hashctrl_mask_in_place);\n+    Node* masked_header  = _gvn.transform(new AndXNode(header, hashctrl_mask));\n+\n+    \/\/ Take slow-path when the object has not been hashed.\n+    Node* not_hashed_val = _gvn.MakeConX(0);\n+    Node* chk_hashed     = _gvn.transform(new CmpXNode(masked_header, not_hashed_val));\n+    Node* test_hashed    = _gvn.transform(new BoolNode(chk_hashed, BoolTest::eq));\n+\n+    generate_slow_guard(test_hashed, slow_region);\n+\n+    \/\/ Test whether the object is hashed or hashed&copied.\n+    Node* hashed_copied = _gvn.MakeConX(markWord::hashctrl_expanded_mask_in_place | markWord::hashctrl_hashed_mask_in_place);\n+    Node* chk_copied    = _gvn.transform(new CmpXNode(masked_header, hashed_copied));\n+    \/\/ If true, then object has been hashed&copied, otherwise it's only hashed.\n+    Node* test_copied   = _gvn.transform(new BoolNode(chk_copied, BoolTest::eq));\n+    IfNode* if_copied   = create_and_map_if(control(), test_copied, PROB_FAIR, COUNT_UNKNOWN);\n+    Node* if_true = _gvn.transform(new IfTrueNode(if_copied));\n+    Node* if_false = _gvn.transform(new IfFalseNode(if_copied));\n+\n+    \/\/ Hashed&Copied path: read hash-code out of the object.\n+    set_control(if_true);\n+    \/\/ result_val->del_req(_fast_path2);\n+    \/\/ result_reg->del_req(_fast_path2);\n+    \/\/ result_io->del_req(_fast_path2);\n+    \/\/ result_mem->del_req(_fast_path2);\n+\n+    Node* obj_klass = load_object_klass(obj);\n+    Node* hash_addr;\n+    const TypeKlassPtr* klass_t = _gvn.type(obj_klass)->isa_klassptr();\n+    bool load_offset_runtime = true;\n+\n+    if (klass_t != nullptr) {\n+      if (klass_t->klass_is_exact()  && klass_t->isa_instklassptr()) {\n+        ciInstanceKlass* ciKlass = reinterpret_cast<ciInstanceKlass*>(klass_t->is_instklassptr()->exact_klass());\n+        if (!ciKlass->is_mirror_instance_klass() && !ciKlass->is_reference_instance_klass()) {\n+          \/\/ We know the InstanceKlass, load hash_offset from there at compile-time.\n+          int hash_offset = ciKlass->hash_offset_in_bytes();\n+          hash_addr = basic_plus_adr(obj, hash_offset);\n+          Node* loaded_hash = make_load(control(), hash_addr, TypeInt::INT, T_INT, MemNode::unordered);\n+          result_val->init_req(_fast_path2, loaded_hash);\n+          result_reg->init_req(_fast_path2, control());\n+          load_offset_runtime = false;\n+        }\n+      }\n+    }\n+\n+    \/\/tty->print_cr(\"Load hash-offset at runtime: %s\", BOOL_TO_STR(load_offset_runtime));\n+\n+    if (load_offset_runtime) {\n+      \/\/ We don't know if it is an array or an exact type, figure it out at run-time.\n+      \/\/ If not an ordinary instance, then we need to take slow-path.\n+      Node* kind_addr = basic_plus_adr(obj_klass, Klass::kind_offset_in_bytes());\n+      Node* kind = make_load(control(), kind_addr, TypeInt::INT, T_INT, MemNode::unordered);\n+      Node* instance_val = _gvn.intcon(Klass::InstanceKlassKind);\n+      Node* chk_inst     = _gvn.transform(new CmpINode(kind, instance_val));\n+      Node* test_inst    = _gvn.transform(new BoolNode(chk_inst, BoolTest::ne));\n+      generate_slow_guard(test_inst, slow_region);\n+\n+      \/\/ Otherwise it's an instance and we can read the hash_offset from the InstanceKlass.\n+      Node* hash_offset_addr = basic_plus_adr(obj_klass, InstanceKlass::hash_offset_offset_in_bytes());\n+      Node* hash_offset = make_load(control(), hash_offset_addr, TypeInt::INT, T_INT, MemNode::unordered);\n+      \/\/ hash_offset->dump();\n+      Node* hash_addr = basic_plus_adr(obj, ConvI2X(hash_offset));\n+      Compile::current()->set_has_unsafe_access(true);\n+      Node* loaded_hash = make_load(control(), hash_addr, TypeInt::INT, T_INT, MemNode::unordered);\n+      result_val->init_req(_fast_path2, loaded_hash);\n+      result_reg->init_req(_fast_path2, control());\n+    }\n+\n+    \/\/ Hashed-only path: recompute hash-code from object address.\n+    set_control(if_false);\n+    if (hashCode == 6) {\n+      \/\/ Our constants.\n+      Node* M = _gvn.intcon(0x337954D5);\n+      Node* A = _gvn.intcon(0xAAAAAAAA);\n+      \/\/ Split object address into lo and hi 32 bits.\n+      Node* obj_addr = _gvn.transform(new CastP2XNode(nullptr, obj));\n+      Node* x = _gvn.transform(new ConvL2INode(obj_addr));\n+      Node* upper_addr = _gvn.transform(new URShiftLNode(obj_addr, _gvn.intcon(32)));\n+      Node* y = _gvn.transform(new ConvL2INode(upper_addr));\n+\n+      Node* H0 = _gvn.transform(new XorINode(x, y));\n+      Node* L0 = _gvn.transform(new XorINode(x, A));\n+\n+      \/\/ Full multiplication of two 32 bit values L0 and M into a hi\/lo result in two 32 bit values V0 and U0.\n+      Node* L0_64 = _gvn.transform(new ConvI2LNode(L0));\n+      L0_64 = _gvn.transform(new AndLNode(L0_64, _gvn.longcon(0xFFFFFFFF)));\n+      Node* M_64 = _gvn.transform(new ConvI2LNode(M));\n+      \/\/ M_64 = _gvn.transform(new AndLNode(M_64, _gvn.longcon(0xFFFFFFFF)));\n+      Node* prod64 = _gvn.transform(new MulLNode(L0_64, M_64));\n+      Node* V0 = _gvn.transform(new ConvL2INode(prod64));\n+      Node* prod_upper = _gvn.transform(new URShiftLNode(prod64, _gvn.intcon(32)));\n+      Node* U0 = _gvn.transform(new ConvL2INode(prod_upper));\n+\n+      Node* Q0 = _gvn.transform(new MulINode(H0, M));\n+      Node* L1 = _gvn.transform(new XorINode(Q0, U0));\n+\n+      \/\/ Full multiplication of two 32 bit values L1 and M into a hi\/lo result in two 32 bit values V1 and U1.\n+      Node* L1_64 = _gvn.transform(new ConvI2LNode(L1));\n+      L1_64 = _gvn.transform(new AndLNode(L1_64, _gvn.longcon(0xFFFFFFFF)));\n+      prod64 = _gvn.transform(new MulLNode(L1_64, M_64));\n+      Node* V1 = _gvn.transform(new ConvL2INode(prod64));\n+      prod_upper = _gvn.transform(new URShiftLNode(prod64, _gvn.intcon(32)));\n+      Node* U1 = _gvn.transform(new ConvL2INode(prod_upper));\n+\n+      Node* P1 = _gvn.transform(new XorINode(V0, M));\n+\n+      \/\/ Right rotate P1 by distance L1.\n+      Node* distance = _gvn.transform(new AndINode(L1, _gvn.intcon(32 - 1)));\n+      Node* inverse_distance = _gvn.transform(new SubINode(_gvn.intcon(32), distance));\n+      Node* ror_part1 = _gvn.transform(new URShiftINode(P1, distance));\n+      Node* ror_part2 = _gvn.transform(new LShiftINode(P1, inverse_distance));\n+      Node* Q1 = _gvn.transform(new OrINode(ror_part1, ror_part2));\n+\n+      Node* L2 = _gvn.transform(new XorINode(Q1, U1));\n+      Node* hash = _gvn.transform(new XorINode(V1, L2));\n+      Node* hash_truncated = _gvn.transform(new AndINode(hash, _gvn.intcon(markWord::hash_mask)));\n+\n+      result_val->init_req(_fast_path, hash_truncated);\n+    } else if (hashCode == 2) {\n+      result_val->init_req(_fast_path, _gvn.intcon(1));\n+    }\n+  } else {\n+    \/\/ Get the header out of the object, use LoadMarkNode when available\n+    Node* header_addr = basic_plus_adr(obj, oopDesc::mark_offset_in_bytes());\n+    \/\/ The control of the load must be null. Otherwise, the load can move before\n+    \/\/ the null check after castPP removal.\n+    Node* no_ctrl = nullptr;\n+    Node* header = make_load(no_ctrl, header_addr, TypeX_X, TypeX_X->basic_type(), MemNode::unordered);\n+\n+    if (!UseObjectMonitorTable) {\n+      \/\/ Test the header to see if it is safe to read w.r.t. locking.\n+      Node *lock_mask      = _gvn.MakeConX(markWord::lock_mask_in_place);\n+      Node *lmasked_header = _gvn.transform(new AndXNode(header, lock_mask));\n+      Node *monitor_val   = _gvn.MakeConX(markWord::monitor_value);\n+      Node *chk_monitor   = _gvn.transform(new CmpXNode(lmasked_header, monitor_val));\n+      Node *test_monitor  = _gvn.transform(new BoolNode(chk_monitor, BoolTest::eq));\n+\n+      generate_slow_guard(test_monitor, slow_region);\n+    }\n+\n+    \/\/ Get the hash value and check to see that it has been properly assigned.\n+    \/\/ We depend on hash_mask being at most 32 bits and avoid the use of\n+    \/\/ hash_mask_in_place because it could be larger than 32 bits in a 64-bit\n+    \/\/ vm: see markWord.hpp.\n+    Node *hash_mask      = _gvn.intcon(markWord::hash_mask);\n+    Node *hash_shift     = _gvn.intcon(markWord::hash_shift);\n+    Node *hshifted_header= _gvn.transform(new URShiftXNode(header, hash_shift));\n+    \/\/ This hack lets the hash bits live anywhere in the mark object now, as long\n+    \/\/ as the shift drops the relevant bits into the low 32 bits.  Note that\n+    \/\/ Java spec says that HashCode is an int so there's no point in capturing\n+    \/\/ an 'X'-sized hashcode (32 in 32-bit build or 64 in 64-bit build).\n+    hshifted_header      = ConvX2I(hshifted_header);\n+    Node *hash_val       = _gvn.transform(new AndINode(hshifted_header, hash_mask));\n+\n+    Node *no_hash_val    = _gvn.intcon(markWord::no_hash);\n+    Node *chk_assigned   = _gvn.transform(new CmpINode( hash_val, no_hash_val));\n+    Node *test_assigned  = _gvn.transform(new BoolNode( chk_assigned, BoolTest::eq));\n+\n+    generate_slow_guard(test_assigned, slow_region);\n+\n+    result_val->init_req(_fast_path, hash_val);\n+\n+    \/\/ _fast_path2 is not used here.\n+    result_val->del_req(_fast_path2);\n+    result_reg->del_req(_fast_path2);\n+    result_io->del_req(_fast_path2);\n+    result_mem->del_req(_fast_path2);\n+  }\n@@ -4823,1 +4964,0 @@\n-  result_val->init_req(_fast_path, hash_val);\n@@ -4828,0 +4968,5 @@\n+  if (UseCompactObjectHeaders) {\n+    result_io->init_req(_fast_path2, i_o());\n+    result_mem->init_req(_fast_path2, init_mem);\n+  }\n+\n@@ -4829,0 +4974,1 @@\n+  assert(slow_region != nullptr, \"must have slow_region\");\n@@ -5601,1 +5747,1 @@\n-    int klass_idx = C->get_alias_index(ary_type->add_offset(oopDesc::klass_offset_in_bytes()));\n+    int klass_idx = C->get_alias_index(ary_type->add_offset(Type::klass_offset()));\n","filename":"src\/hotspot\/share\/opto\/library_call.cpp","additions":186,"deletions":40,"binary":false,"changes":226,"status":"modified"},{"patch":"@@ -1763,1 +1763,1 @@\n-    rawmem = make_store(control, rawmem, object, oopDesc::klass_offset_in_bytes(), klass_node, T_METADATA);\n+    rawmem = make_store(control, rawmem, object, Type::klass_offset(), klass_node, T_METADATA);\n@@ -2389,1 +2389,1 @@\n-      Node* k_adr = basic_plus_adr(obj_or_subklass, oopDesc::klass_offset_in_bytes());\n+      Node* k_adr = basic_plus_adr(obj_or_subklass, Type::klass_offset());\n","filename":"src\/hotspot\/share\/opto\/macro.cpp","additions":2,"deletions":2,"binary":false,"changes":4,"status":"modified"},{"patch":"@@ -265,1 +265,1 @@\n-          adr_check->offset() == oopDesc::klass_offset_in_bytes() ||\n+          adr_check->offset() == Type::klass_offset() ||\n@@ -953,1 +953,1 @@\n-           adr_type->offset() == oopDesc::klass_offset_in_bytes()),\n+           adr_type->offset() == Type::klass_offset()),\n@@ -2461,1 +2461,1 @@\n-    if (offset == oopDesc::klass_offset_in_bytes()) {\n+    if (offset == Type::klass_offset()) {\n@@ -2469,1 +2469,1 @@\n-      tary->offset() == oopDesc::klass_offset_in_bytes()) {\n+      tary->offset() == Type::klass_offset()) {\n@@ -2535,1 +2535,1 @@\n-  if (offset == oopDesc::klass_offset_in_bytes()) {\n+  if (offset == Type::klass_offset()) {\n","filename":"src\/hotspot\/share\/opto\/memnode.cpp","additions":5,"deletions":5,"binary":false,"changes":10,"status":"modified"},{"patch":"@@ -2129,1 +2129,1 @@\n-  Node* klass_addr = basic_plus_adr( receiver, receiver, oopDesc::klass_offset_in_bytes() );\n+  Node* klass_addr = basic_plus_adr( receiver, receiver, Type::klass_offset() );\n","filename":"src\/hotspot\/share\/opto\/parse1.cpp","additions":1,"deletions":1,"binary":false,"changes":2,"status":"modified"},{"patch":"@@ -1713,1 +1713,1 @@\n-  if (obj == nullptr || off != oopDesc::klass_offset_in_bytes()) \/\/ loading oopDesc::_klass?\n+  if (obj == nullptr || off != Type::klass_offset()) \/\/ loading oopDesc::_klass?\n","filename":"src\/hotspot\/share\/opto\/parse2.cpp","additions":1,"deletions":1,"binary":false,"changes":2,"status":"modified"},{"patch":"@@ -427,1 +427,1 @@\n-    const size_t size = TypeArrayKlass::cast(array_type)->oop_size(result);\n+    const size_t size = TypeArrayKlass::cast(array_type)->oop_size(result, result->mark());\n","filename":"src\/hotspot\/share\/opto\/runtime.cpp","additions":1,"deletions":1,"binary":false,"changes":2,"status":"modified"},{"patch":"@@ -1203,1 +1203,1 @@\n-  if (con2 == oopDesc::klass_offset_in_bytes()) {\n+  if (con2 == Type::klass_offset()) {\n","filename":"src\/hotspot\/share\/opto\/subnode.cpp","additions":1,"deletions":1,"binary":false,"changes":2,"status":"modified"},{"patch":"@@ -580,1 +580,1 @@\n-                                           false, nullptr, oopDesc::klass_offset_in_bytes());\n+                                           false, nullptr, Type::klass_offset());\n@@ -3463,1 +3463,1 @@\n-    if (_offset == oopDesc::klass_offset_in_bytes()) {\n+    if (_offset == Type::klass_offset()) {\n","filename":"src\/hotspot\/share\/opto\/type.cpp","additions":2,"deletions":2,"binary":false,"changes":4,"status":"modified"},{"patch":"@@ -203,0 +203,11 @@\n+  \/\/ This is used as a marker to identify narrow Klass* loads, which\n+  \/\/ are really extracted from the mark-word, but we still want to\n+  \/\/ distinguish it.\n+  static int klass_offset() {\n+    if (UseCompactObjectHeaders) {\n+      return 1;\n+    } else {\n+      return oopDesc::klass_offset_in_bytes();\n+    }\n+  }\n+\n@@ -1626,1 +1637,1 @@\n-        _offset != arrayOopDesc::klass_offset_in_bytes()) {\n+        _offset != Type::klass_offset()) {\n","filename":"src\/hotspot\/share\/opto\/type.hpp","additions":12,"deletions":1,"binary":false,"changes":13,"status":"modified"},{"patch":"@@ -3821,0 +3821,6 @@\n+  if (UseCompactObjectHeaders && FLAG_IS_DEFAULT(hashCode)) {\n+    hashCode = 6;\n+  }\n+  if (UseCompactObjectHeaders && FLAG_IS_DEFAULT(CompressedClassSpaceSize)) {\n+    FLAG_SET_DEFAULT(CompressedClassSpaceSize, CompressedKlassPointers::max_klass_range_size_coh);\n+  }\n","filename":"src\/hotspot\/share\/runtime\/arguments.cpp","additions":6,"deletions":0,"binary":false,"changes":6,"status":"modified"},{"patch":"@@ -65,0 +65,1 @@\n+#include \"utilities\/fastHash.hpp\"\n@@ -601,1 +602,1 @@\n-static intptr_t get_next_hash(Thread* current, oop obj) {\n+intptr_t ObjectSynchronizer::get_next_hash(Thread* current, oop obj) {\n@@ -620,1 +621,1 @@\n-  } else {\n+  } else if (hashCode == 5) {\n@@ -633,0 +634,10 @@\n+  } else {\n+    assert(UseCompactObjectHeaders, \"Only with compact i-hash\");\n+#ifdef _LP64\n+    uint64_t val = cast_from_oop<uint64_t>(obj);\n+    uint32_t hash = FastHash::get_hash32((uint32_t)val, (uint32_t)(val >> 32));\n+#else\n+    uint32_t val = cast_from_oop<uint32_t>(obj);\n+    uint32_t hash = FastHash::get_hash32(val, UCONST64(0xAAAAAAAA));\n+#endif\n+    value= static_cast<intptr_t>(hash);\n@@ -636,2 +647,2 @@\n-  if (value == 0) value = 0xBAD;\n-  assert(value != markWord::no_hash, \"invariant\");\n+  if (hashCode != 6 && value == 0) value = 0xBAD;\n+  assert(value != markWord::no_hash || hashCode == 6, \"invariant\");\n@@ -647,3 +658,22 @@\n-    \/\/ If UseObjectMonitorTable is set the hash can simply be installed in the\n-    \/\/ object header, since the monitor isn't in the object header.\n-    if (UseObjectMonitorTable || !mark.has_monitor()) {\n+    if (UseCompactObjectHeaders) {\n+      if (mark.is_hashed()) {\n+        return get_hash(mark, obj);\n+      }\n+      intptr_t hash = get_next_hash(current, obj);  \/\/ get a new hash\n+      markWord new_mark;\n+      if (mark.is_not_hashed_expanded()) {\n+        new_mark = mark.set_hashed_expanded();\n+        int offset = mark.klass()->hash_offset_in_bytes(obj, mark);\n+        obj->int_field_put(offset, (jint) hash);\n+      } else {\n+        new_mark = mark.set_hashed_not_expanded();\n+      }\n+      markWord old_mark = obj->cas_set_mark(new_mark, mark);\n+      if (old_mark == mark) {\n+        return hash;\n+      }\n+      \/\/ CAS failed, retry.\n+      continue;\n+    } else if (UseObjectMonitorTable || !mark.has_monitor()) {\n+      \/\/ If UseObjectMonitorTable is set the hash can simply be installed in the\n+      \/\/ object header, since the monitor isn't in the object header.\n@@ -738,0 +768,19 @@\n+\n+uint32_t ObjectSynchronizer::get_hash(markWord mark, oop obj, Klass* klass) {\n+  assert(UseCompactObjectHeaders, \"Only with compact i-hash\");\n+  \/\/assert(mark.is_neutral() | mark.is_fast_locked(), \"only from neutral or fast-locked mark: \" INTPTR_FORMAT, mark.value());\n+  assert(mark.is_hashed(), \"only from hashed or copied object\");\n+  if (mark.is_hashed_expanded()) {\n+    return obj->int_field(klass->hash_offset_in_bytes(obj, mark));\n+  } else {\n+    assert(mark.is_hashed_not_expanded(), \"must be hashed\");\n+    assert(hashCode == 6 || hashCode == 2, \"must have idempotent hashCode\");\n+    \/\/ Already marked as hashed, but not yet copied. Recompute hash and return it.\n+    return ObjectSynchronizer::get_next_hash(nullptr, obj); \/\/ recompute hash\n+  }\n+}\n+\n+uint32_t ObjectSynchronizer::get_hash(markWord mark, oop obj) {\n+  return get_hash(mark, obj, mark.klass());\n+}\n+\n@@ -1473,0 +1522,12 @@\n+static uintx objhash(oop obj) {\n+  if (UseCompactObjectHeaders) {\n+    uintx hash = ObjectSynchronizer::get_hash(obj->mark(), obj);\n+    assert(hash != 0, \"should have a hash\");\n+    return hash;\n+  } else {\n+    uintx hash = obj->mark().hash();\n+    assert(hash != 0, \"should have a hash\");\n+    return hash;\n+  }\n+}\n+\n@@ -1504,3 +1565,1 @@\n-      uintx hash = _obj->mark().hash();\n-      assert(hash != 0, \"should have a hash\");\n-      return hash;\n+      return objhash(_obj);\n@@ -1613,0 +1672,1 @@\n+      assert(objhash(obj) == (uintx)(*found)->hash(), \"hash must match\");\n@@ -1741,1 +1801,1 @@\n-       assert(obj->mark().hash() == om->hash(), \"hash must match\");\n+       assert(objhash(obj) == (uintx)om->hash(), \"hash must match\");\n@@ -1829,1 +1889,1 @@\n-  intptr_t hash = obj->mark().hash();\n+  intptr_t hash = objhash(obj);\n","filename":"src\/hotspot\/share\/runtime\/synchronizer.cpp","additions":72,"deletions":12,"binary":false,"changes":84,"status":"modified"},{"patch":"@@ -136,0 +136,5 @@\n+  \/\/ NOTE: May not cause monitor inflation\n+  static uint32_t get_hash(markWord mark, oop obj);\n+  \/\/ For CDS path.\n+  static uint32_t get_hash(markWord mark, oop obj, Klass* klass);\n+\n@@ -196,0 +201,2 @@\n+  static intptr_t get_next_hash(Thread* current, oop obj);\n+\n","filename":"src\/hotspot\/share\/runtime\/synchronizer.hpp","additions":7,"deletions":0,"binary":false,"changes":7,"status":"modified"},{"patch":"@@ -209,0 +209,1 @@\n+  nonstatic_field(InstanceKlass,               _hash_offset,                                  int)                                    \\\n@@ -1825,0 +1826,1 @@\n+  declare_constant(markWord::hashctrl_bits)                               \\\n@@ -1829,0 +1831,1 @@\n+  declare_constant(markWord::hashctrl_shift)                              \\\n@@ -1837,0 +1840,4 @@\n+  declare_constant(markWord::hashctrl_mask)                               \\\n+  declare_constant(markWord::hashctrl_mask_in_place)                      \\\n+  declare_constant(markWord::hashctrl_hashed_mask_in_place)               \\\n+  declare_constant(markWord::hashctrl_expanded_mask_in_place)             \\\n","filename":"src\/hotspot\/share\/runtime\/vmStructs.cpp","additions":7,"deletions":0,"binary":false,"changes":7,"status":"modified"},{"patch":"@@ -54,0 +54,1 @@\n+    hashCtrlBits        = db.lookupLongConstant(\"markWord::hashctrl_bits\").longValue();\n@@ -57,0 +58,1 @@\n+    hashCtrlShift       = db.lookupLongConstant(\"markWord::hashctrl_shift\").longValue();\n@@ -66,0 +68,4 @@\n+    hashCtrlMask        = db.lookupLongConstant(\"markWord::hashctrl_mask\").longValue();\n+    hashCtrlMaskInPlace = db.lookupLongConstant(\"markWord::hashctrl_mask_in_place\").longValue();\n+    hashCtrlHashedMaskInPlace =   db.lookupLongConstant(\"markWord::hashctrl_hashed_mask_in_place\").longValue();\n+    hashCtrlExpandedMaskInPlace = db.lookupLongConstant(\"markWord::hashctrl_expanded_mask_in_place\").longValue();\n@@ -84,0 +90,1 @@\n+  private static long hashCtrlBits;\n@@ -88,0 +95,1 @@\n+  private static long hashCtrlShift;\n@@ -96,0 +104,4 @@\n+  private static long hashCtrlMask;\n+  private static long hashCtrlMaskInPlace;\n+  private static long hashCtrlHashedMaskInPlace;\n+  private static long hashCtrlExpandedMaskInPlace;\n@@ -189,1 +201,6 @@\n-    return Bits.maskBitsLong(value() >> hashShift, hashMask);\n+    if (VM.getVM().isCompactObjectHeadersEnabled()) {\n+      System.exit(-23);\n+      throw new RuntimeException(\"Compact I-Hash not yet implemented\");\n+    } else {\n+      return Bits.maskBitsLong(value() >> hashShift, hashMask);\n+    }\n@@ -196,0 +213,5 @@\n+  public boolean isExpanded() {\n+    assert(VM.getVM().isCompactObjectHeadersEnabled());\n+    return Bits.maskBitsLong(value(), hashCtrlExpandedMaskInPlace) != 0;\n+  }\n+\n","filename":"src\/jdk.hotspot.agent\/share\/classes\/sun\/jvm\/hotspot\/oops\/Mark.java","additions":23,"deletions":1,"binary":false,"changes":24,"status":"modified"},{"patch":"@@ -94,1 +94,5 @@\n-  assert_test_pattern(h_obj, \"is_unlocked hash=0x\");\n+  if (UseCompactObjectHeaders) {\n+    assert_test_pattern(h_obj, \"is_unlocked hash is-hashed=true is-copied=false\");\n+  } else {\n+    assert_test_pattern(h_obj, \"is_unlocked hash=0x\");\n+  }\n","filename":"test\/hotspot\/gtest\/oops\/test_markWord.cpp","additions":5,"deletions":1,"binary":false,"changes":6,"status":"modified"},{"patch":"@@ -782,1 +782,0 @@\n-        applyIfOr = {\"UseCompactObjectHeaders\", \"false\", \"AlignVector\", \"false\"},\n@@ -788,4 +787,0 @@\n-            \/\/ With AlignVector, we need 8-byte alignment of vector loads\/stores.\n-            \/\/ UseCompactObjectHeaders=false                        UseCompactObjectHeaders=true\n-            \/\/ adr = base + 16 + 8*i      ->  always                adr = base + 12 + 8*i      ->  never\n-            \/\/ -> vectorize                                         -> no vectorization\n@@ -800,1 +795,0 @@\n-        applyIfOr = {\"UseCompactObjectHeaders\", \"false\", \"AlignVector\", \"false\"},\n@@ -806,4 +800,0 @@\n-            \/\/ With AlignVector, we need 8-byte alignment of vector loads\/stores.\n-            \/\/ UseCompactObjectHeaders=false                        UseCompactObjectHeaders=true\n-            \/\/ adr = base + 16 + 8*i      ->  always                adr = base + 12 + 8*i      ->  never\n-            \/\/ -> vectorize                                         -> no vectorization\n","filename":"test\/hotspot\/jtreg\/compiler\/c2\/irTests\/TestVectorConditionalMove.java","additions":0,"deletions":10,"binary":false,"changes":10,"status":"modified"},{"patch":"@@ -174,1 +174,0 @@\n-        applyIfOr = {\"UseCompactObjectHeaders\", \"false\", \"AlignVector\", \"false\"},\n@@ -182,5 +181,0 @@\n-            \/\/ With AlignVector, we need 8-byte alignment of vector loads\/stores.\n-            \/\/ UseCompactObjectHeaders=false                  UseCompactObjectHeaders=true\n-            \/\/ B_adr = base + 16 + 8*i  ->  always            B_adr = base + 12 + 8*i  ->  never\n-            \/\/ L_adr = base + 16 + 8*i  ->  always            L_adr = base + 16 + 8*i  ->  always\n-            \/\/ -> vectorize                                   -> no vectorization\n@@ -192,1 +186,0 @@\n-        applyIfOr = {\"UseCompactObjectHeaders\", \"false\", \"AlignVector\", \"false\"},\n@@ -199,5 +192,0 @@\n-            \/\/ With AlignVector, we need 8-byte alignment of vector loads\/stores.\n-            \/\/ UseCompactObjectHeaders=false                  UseCompactObjectHeaders=true\n-            \/\/ B_adr = base + 16 + 8*i  ->  always            B_adr = base + 12 + 8*i  ->  never\n-            \/\/ L_adr = base + 16 + 8*i  ->  always            L_adr = base + 16 + 8*i  ->  always\n-            \/\/ -> vectorize                                   -> no vectorization\n@@ -214,5 +202,0 @@\n-            \/\/ With AlignVector, we need 8-byte alignment of vector loads\/stores.\n-            \/\/ UseCompactObjectHeaders=false                  UseCompactObjectHeaders=true\n-            \/\/ B_adr = base + 64 + 8*i  ->  always            B_adr = base + 64 + 8*i  ->  always\n-            \/\/ L_adr = base + 16 + 8*i  ->  always            L_adr = base + 16 + 8*i  ->  always\n-            \/\/ -> vectorize                                   -> vectorize\n@@ -231,5 +214,0 @@\n-            \/\/ With AlignVector, we need 8-byte alignment of vector loads\/stores.\n-            \/\/ UseCompactObjectHeaders=false                  UseCompactObjectHeaders=true\n-            \/\/ B_adr = base + 64 + 8*i  ->  always            B_adr = base + 64 + 8*i  ->  always\n-            \/\/ L_adr = base + 16 + 8*i  ->  always            L_adr = base + 16 + 8*i  ->  always\n-            \/\/ -> vectorize                                   -> vectorize\n@@ -249,1 +227,0 @@\n-        applyIfOr = {\"UseCompactObjectHeaders\", \"false\", \"AlignVector\", \"false\"},\n@@ -257,5 +234,0 @@\n-            \/\/ With AlignVector, we need 8-byte alignment of vector loads\/stores.\n-            \/\/ UseCompactObjectHeaders=false                      UseCompactObjectHeaders=true\n-            \/\/ B_adr = base + 16 + 8*(i-1)  ->  always            B_adr = base + 12 + 8*(i-1)  ->  never\n-            \/\/ L_adr = base + 16 + 8*i      ->  always            L_adr = base + 16 + 8*i      ->  always\n-            \/\/ -> vectorize                                       -> no vectorization\n@@ -267,1 +239,0 @@\n-        applyIfOr = {\"UseCompactObjectHeaders\", \"false\", \"AlignVector\", \"false\"},\n@@ -274,5 +245,0 @@\n-            \/\/ With AlignVector, we need 8-byte alignment of vector loads\/stores.\n-            \/\/ UseCompactObjectHeaders=false                      UseCompactObjectHeaders=true\n-            \/\/ B_adr = base + 16 + 8*(i-1)  ->  always            B_adr = base + 12 + 8*(i-1)  ->  never\n-            \/\/ L_adr = base + 16 + 8*i      ->  always            L_adr = base + 16 + 8*i      ->  always\n-            \/\/ -> vectorize                                       -> no vectorization\n@@ -290,1 +256,0 @@\n-        applyIfOr = {\"UseCompactObjectHeaders\", \"false\", \"AlignVector\", \"false\"},\n@@ -298,5 +263,0 @@\n-            \/\/ With AlignVector, we need 8-byte alignment of vector loads\/stores.\n-            \/\/ UseCompactObjectHeaders=false                      UseCompactObjectHeaders=true\n-            \/\/ B_adr = base + 16 + 8*(i+1)  ->  always            B_adr = base + 12 + 8*(i+1)  ->  never\n-            \/\/ L_adr = base + 16 + 8*i      ->  always            L_adr = base + 16 + 8*i      ->  always\n-            \/\/ -> vectorize                                       -> no vectorization\n@@ -308,1 +268,0 @@\n-        applyIfOr = {\"UseCompactObjectHeaders\", \"false\", \"AlignVector\", \"false\"},\n@@ -315,5 +274,0 @@\n-            \/\/ With AlignVector, we need 8-byte alignment of vector loads\/stores.\n-            \/\/ UseCompactObjectHeaders=false                      UseCompactObjectHeaders=true\n-            \/\/ B_adr = base + 16 + 8*(i+1)  ->  always            B_adr = base + 12 + 8*(i+1)  ->  never\n-            \/\/ L_adr = base + 16 + 8*i      ->  always            L_adr = base + 16 + 8*i      ->  always\n-            \/\/ -> vectorize                                       -> no vectorization\n@@ -365,1 +319,0 @@\n-        applyIfOr = {\"UseCompactObjectHeaders\", \"false\", \"AlignVector\", \"false\"},\n@@ -373,5 +326,0 @@\n-            \/\/ With AlignVector, we need 8-byte alignment of vector loads\/stores.\n-            \/\/ UseCompactObjectHeaders=false                      UseCompactObjectHeaders=true\n-            \/\/ B_adr = base + 16 + 8*(i+x)  ->  always            B_adr = base + 12 + 8*(i+x)  ->  never\n-            \/\/ L_adr = base + 16 + 8*i      ->  always            L_adr = base + 16 + 8*i      ->  always\n-            \/\/ -> vectorize                                       -> no vectorization\n@@ -383,1 +331,0 @@\n-        applyIfOr = {\"UseCompactObjectHeaders\", \"false\", \"AlignVector\", \"false\"},\n@@ -390,5 +337,0 @@\n-            \/\/ With AlignVector, we need 8-byte alignment of vector loads\/stores.\n-            \/\/ UseCompactObjectHeaders=false                      UseCompactObjectHeaders=true\n-            \/\/ B_adr = base + 16 + 8*(i+x)  ->  always            B_adr = base + 12 + 8*(i+x)  ->  never\n-            \/\/ L_adr = base + 16 + 8*i      ->  always            L_adr = base + 16 + 8*i      ->  always\n-            \/\/ -> vectorize                                       -> no vectorization\n@@ -407,1 +349,0 @@\n-        applyIfOr = {\"UseCompactObjectHeaders\", \"false\", \"AlignVector\", \"false\"},\n@@ -415,5 +356,0 @@\n-            \/\/ With AlignVector, we need 8-byte alignment of vector loads\/stores.\n-            \/\/ UseCompactObjectHeaders=false                    UseCompactObjectHeaders=true\n-            \/\/ src_adr = base + 16 + 8*i  ->  always            src_adr = base + 12 + 8*i  ->  never\n-            \/\/ dst_adr = base + 16 + 8*i  ->  always            dst_adr = base + 12 + 8*i  ->  never\n-            \/\/ -> vectorize                                     -> no vectorization\n@@ -425,1 +361,0 @@\n-        applyIfOr = {\"UseCompactObjectHeaders\", \"false\", \"AlignVector\", \"false\"},\n@@ -432,5 +367,0 @@\n-            \/\/ With AlignVector, we need 8-byte alignment of vector loads\/stores.\n-            \/\/ UseCompactObjectHeaders=false                    UseCompactObjectHeaders=true\n-            \/\/ src_adr = base + 16 + 8*i  ->  always            src_adr = base + 12 + 8*i  ->  never\n-            \/\/ dst_adr = base + 16 + 8*i  ->  always            dst_adr = base + 12 + 8*i  ->  never\n-            \/\/ -> vectorize                                     -> no vectorization\n@@ -448,1 +378,0 @@\n-        applyIfOr = {\"UseCompactObjectHeaders\", \"false\", \"AlignVector\", \"false\"},\n@@ -456,5 +385,0 @@\n-            \/\/ With AlignVector, we need 8-byte alignment of vector loads\/stores.\n-            \/\/ UseCompactObjectHeaders=false                        UseCompactObjectHeaders=true\n-            \/\/ src_adr = base + 16 + 8*i      ->  always            src_adr = base + 12 + 8*i      ->  never\n-            \/\/ dst_adr = base + 16 + 8*(i-1)  ->  always            dst_adr = base + 12 + 8*(i-1)  ->  never\n-            \/\/ -> vectorize                                         -> no vectorization\n@@ -466,1 +390,0 @@\n-        applyIfOr = {\"UseCompactObjectHeaders\", \"false\", \"AlignVector\", \"false\"},\n@@ -473,5 +396,0 @@\n-            \/\/ With AlignVector, we need 8-byte alignment of vector loads\/stores.\n-            \/\/ UseCompactObjectHeaders=false                        UseCompactObjectHeaders=true\n-            \/\/ src_adr = base + 16 + 8*i      ->  always            src_adr = base + 12 + 8*i      ->  never\n-            \/\/ dst_adr = base + 16 + 8*(i-1)  ->  always            dst_adr = base + 12 + 8*(i-1)  ->  never\n-            \/\/ -> vectorize                                         -> no vectorization\n","filename":"test\/hotspot\/jtreg\/compiler\/c2\/irTests\/TestVectorizationMismatchedAccess.java","additions":0,"deletions":82,"binary":false,"changes":82,"status":"modified"},{"patch":"@@ -140,2 +140,1 @@\n-        tests.put(\"test1a\",      () -> { return test1a(aB.clone(), bB.clone(), mB); });\n-        tests.put(\"test1b\",      () -> { return test1b(aB.clone(), bB.clone(), mB); });\n+        tests.put(\"test1\",       () -> { return test1(aB.clone(), bB.clone(), mB); });\n@@ -156,1 +155,0 @@\n-        tests.put(\"test10e\",     () -> { return test10e(aS.clone(), bS.clone(), mS); });\n@@ -226,2 +224,1 @@\n-                 \"test1a\",\n-                 \"test1b\",\n+                 \"test1\",\n@@ -240,1 +237,0 @@\n-                 \"test10e\",\n@@ -431,3 +427,0 @@\n-        applyIfOr = {\"UseCompactObjectHeaders\", \"false\", \"AlignVector\", \"false\"},\n-        \/\/ UNSAFE.ARRAY_BYTE_BASE_OFFSET = 16, but with compact object headers UNSAFE.ARRAY_BYTE_BASE_OFFSET=12.\n-        \/\/ If AlignVector=true, we need the offset to be 8-byte aligned, else the vectors are filtered out.\n@@ -436,1 +429,1 @@\n-    static Object[] test1a(byte[] a, byte[] b, byte mask) {\n+    static Object[] test1(byte[] a, byte[] b, byte mask) {\n@@ -450,23 +443,0 @@\n-    @Test\n-    @IR(counts = {IRNode.LOAD_VECTOR_B, \"> 0\",\n-                  IRNode.AND_VB, \"> 0\",\n-                  IRNode.STORE_VECTOR, \"> 0\"},\n-        applyIfOr = {\"UseCompactObjectHeaders\", \"true\", \"AlignVector\", \"false\"},\n-        \/\/ UNSAFE.ARRAY_BYTE_BASE_OFFSET = 16, but with compact object headers UNSAFE.ARRAY_BYTE_BASE_OFFSET=12.\n-        \/\/ If AlignVector=true, we need the offset to be 8-byte aligned, else the vectors are filtered out.\n-        applyIfPlatform = {\"64-bit\", \"true\"},\n-        applyIfCPUFeatureOr = {\"avx2\", \"true\", \"asimd\", \"true\", \"rvv\", \"true\"})\n-    static Object[] test1b(byte[] a, byte[] b, byte mask) {\n-        for (int i = 4; i < RANGE-8; i+=8) {\n-            b[i+0] = (byte)(a[i+0] & mask); \/\/ adr = base + UNSAFE.ARRAY_BYTE_BASE_OFFSET + 4 + iter*8\n-            b[i+1] = (byte)(a[i+1] & mask);\n-            b[i+2] = (byte)(a[i+2] & mask);\n-            b[i+3] = (byte)(a[i+3] & mask);\n-            b[i+4] = (byte)(a[i+4] & mask);\n-            b[i+5] = (byte)(a[i+5] & mask);\n-            b[i+6] = (byte)(a[i+6] & mask);\n-            b[i+7] = (byte)(a[i+7] & mask);\n-        }\n-        return new Object[]{ a, b };\n-    }\n-\n@@ -764,3 +734,1 @@\n-        applyIfAnd = {\"MaxVectorSize\", \">=16\", \"UseCompactObjectHeaders\", \"false\"},\n-        \/\/ UNSAFE.ARRAY_BYTE_BASE_OFFSET = 16, but with compact object headers UNSAFE.ARRAY_BYTE_BASE_OFFSET=12.\n-        \/\/ If AlignVector=true, we need the offset to be 8-byte aligned, else the vectors are filtered out.\n+        applyIf = {\"MaxVectorSize\", \">=16\"},\n@@ -780,20 +748,0 @@\n-    @Test\n-    @IR(counts = {IRNode.LOAD_VECTOR_S, IRNode.VECTOR_SIZE_4, \"> 0\",\n-                  IRNode.AND_VS,        IRNode.VECTOR_SIZE_4, \"> 0\",\n-                  IRNode.STORE_VECTOR, \"> 0\"},\n-        applyIfAnd = {\"MaxVectorSize\", \">=16\", \"UseCompactObjectHeaders\", \"true\"},\n-        \/\/ UNSAFE.ARRAY_BYTE_BASE_OFFSET = 16, but with compact object headers UNSAFE.ARRAY_BYTE_BASE_OFFSET=12.\n-        \/\/ If AlignVector=true, we need the offset to be 8-byte aligned, else the vectors are filtered out.\n-        applyIfPlatform = {\"64-bit\", \"true\"},\n-        applyIfCPUFeatureOr = {\"avx2\", \"true\", \"asimd\", \"true\", \"rvv\", \"true\"})\n-    static Object[] test10e(short[] a, short[] b, short mask) {\n-        for (int i = 11; i < RANGE-16; i+=8) {\n-            \/\/ adr = base + UNSAFE.ARRAY_SHORT_BASE_OFFSET + 2*(3 + 11) + iter*16\n-            b[i+0+3] = (short)(a[i+0+3] & mask);\n-            b[i+1+3] = (short)(a[i+1+3] & mask);\n-            b[i+2+3] = (short)(a[i+2+3] & mask);\n-            b[i+3+3] = (short)(a[i+3+3] & mask);\n-        }\n-        return new Object[]{ a, b };\n-    }\n-\n@@ -1090,1 +1038,0 @@\n-        applyIfOr = {\"UseCompactObjectHeaders\", \"false\", \"AlignVector\", \"false\"},\n@@ -1095,13 +1042,0 @@\n-            \/\/ adr = base + UNSAFE.ARRAY_BYTE_BASE_OFFSET + 1*iter\n-            \/\/              = 16 (or 12 if UseCompactObjectHeaders=true)\n-            \/\/ adr = base + UNSAFE.ARRAY_INT_BASE_OFFSET  + 4*iter\n-            \/\/              = 16 (or 12 if UseCompactObjectHeaders=true)\n-            \/\/ For AlignVector, all adr must be 8-byte aligned. Let's see for which iteration this can hold:\n-            \/\/ If UseCompactObjectHeaders=false:\n-            \/\/   a: 0, 8, 16, 24, 32, ...\n-            \/\/   b: 0, 2,  4,  6,  8, ...\n-            \/\/   -> Ok, aligns every 8th iteration.\n-            \/\/ If UseCompactObjectHeaders=true:\n-            \/\/   a: 4, 12, 20, 28, 36, ...\n-            \/\/   b: 1,  3,  5,  7,  9, ...\n-            \/\/   -> we can never align both vectors!\n@@ -1120,1 +1054,0 @@\n-        applyIfOr = {\"UseCompactObjectHeaders\", \"false\", \"AlignVector\", \"false\"},\n@@ -1125,13 +1058,0 @@\n-            \/\/ adr = base + UNSAFE.ARRAY_BYTE_BASE_OFFSET + 4*iter\n-            \/\/              = 16 (or 12 if UseCompactObjectHeaders=true)\n-            \/\/ adr = base + UNSAFE.ARRAY_SHORT_BASE_OFFSET + 2*iter\n-            \/\/              = 16 (or 12 if UseCompactObjectHeaders=true)\n-            \/\/ For AlignVector, all adr must be 8-byte aligned. Let's see for which iteration this can hold:\n-            \/\/ If UseCompactObjectHeaders=false:\n-            \/\/   a: iter % 2 == 0\n-            \/\/   b: iter % 4 == 0\n-            \/\/   -> Ok, aligns every 4th iteration.\n-            \/\/ If UseCompactObjectHeaders=true:\n-            \/\/   a: iter % 2 = 1\n-            \/\/   b: iter % 4 = 2\n-            \/\/   -> we can never align both vectors!\n@@ -1154,1 +1074,0 @@\n-        applyIfOr = {\"UseCompactObjectHeaders\", \"false\", \"AlignVector\", \"false\"},\n@@ -1159,12 +1078,0 @@\n-            \/\/ adr = base + UNSAFE.ARRAY_BYTE_BASE_OFFSET + 1*iter\n-            \/\/              = 16 (or 12 if UseCompactObjectHeaders=true)\n-            \/\/ adr = base + UNSAFE.ARRAY_SHORT_BASE_OFFSET + 2*iter\n-            \/\/              = 16 (or 12 if UseCompactObjectHeaders=true)\n-            \/\/ adr = base + UNSAFE.ARRAY_INT_BASE_OFFSET + 4*iter\n-            \/\/              = 16 (or 12 if UseCompactObjectHeaders=true)\n-            \/\/ adr = base + UNSAFE.ARRAY_LONG_BASE_OFFSET + 8*iter\n-            \/\/              = 16 (always)\n-            \/\/ If AlignVector and UseCompactObjectHeaders, and we want all adr 8-byte aligned:\n-            \/\/   a: iter % 8 = 4\n-            \/\/   c: iter % 2 = 1\n-            \/\/   -> can never align both vectors!\n@@ -1210,1 +1117,0 @@\n-        applyIfOr = {\"UseCompactObjectHeaders\", \"false\", \"AlignVector\", \"false\"},\n@@ -1215,8 +1121,0 @@\n-            \/\/ adr = base + UNSAFE.ARRAY_INT_BASE_OFFSET + 4 + 4*iter\n-            \/\/              = 16 (or 12 if UseCompactObjectHeaders=true)\n-            \/\/ adr = base + UNSAFE.ARRAY_BYTE_BASE_OFFSET + 1 + 1*iter\n-            \/\/              = 16 (or 12 if UseCompactObjectHeaders=true)\n-            \/\/ If AlignVector and UseCompactObjectHeaders, and we want all adr 8-byte aligned:\n-            \/\/   a: iter % 2 = 0\n-            \/\/   b: iter % 8 = 3\n-            \/\/   -> can never align both vectors!\n@@ -1235,1 +1133,0 @@\n-        applyIfOr = {\"UseCompactObjectHeaders\", \"false\", \"AlignVector\", \"false\"},\n@@ -1240,8 +1137,0 @@\n-            \/\/ adr = base + UNSAFE.ARRAY_INT_BASE_OFFSET + 4 + 4*iter\n-            \/\/              = 16 (or 12 if UseCompactObjectHeaders=true)\n-            \/\/ adr = base + UNSAFE.ARRAY_SHORT_BASE_OFFSET + 2 + 2*iter\n-            \/\/              = 16 (or 12 if UseCompactObjectHeaders=true)\n-            \/\/ If AlignVector and UseCompactObjectHeaders, and we want all adr 8-byte aligned:\n-            \/\/   a: iter % 2 = 0\n-            \/\/   b: iter % 4 = 1\n-            \/\/   -> can never align both vectors!\n@@ -1264,1 +1153,0 @@\n-        applyIfOr = {\"UseCompactObjectHeaders\", \"false\", \"AlignVector\", \"false\"},\n@@ -1269,12 +1157,0 @@\n-            \/\/ adr = base + UNSAFE.ARRAY_BYTE_BASE_OFFSET + 1 + 1*iter\n-            \/\/              = 16 (or 12 if UseCompactObjectHeaders=true)\n-            \/\/ adr = base + UNSAFE.ARRAY_SHORT_BASE_OFFSET + 2 + 2*iter\n-            \/\/              = 16 (or 12 if UseCompactObjectHeaders=true)\n-            \/\/ adr = base + UNSAFE.ARRAY_INT_BASE_OFFSET + 4 + 4*iter\n-            \/\/              = 16 (or 12 if UseCompactObjectHeaders=true)\n-            \/\/ adr = base + UNSAFE.ARRAY_LONG_BASE_OFFSET + 8 + 8*iter\n-            \/\/              = 16 (always)\n-            \/\/ If AlignVector and UseCompactObjectHeaders, and we want all adr 8-byte aligned:\n-            \/\/   a: iter % 8 = 3\n-            \/\/   c: iter % 2 = 0\n-            \/\/   -> can never align both vectors!\n","filename":"test\/hotspot\/jtreg\/compiler\/loopopts\/superword\/TestAlignVector.java","additions":4,"deletions":128,"binary":false,"changes":132,"status":"modified"},{"patch":"@@ -303,9 +303,1 @@\n-        applyIfAnd = {\"MaxVectorSize\", \">=32\", \"AlignVector\", \"false\"},\n-        applyIfPlatform = {\"64-bit\", \"true\"},\n-        applyIfCPUFeatureOr = {\"sse4.1\", \"true\", \"asimd\", \"true\", \"rvv\", \"true\"})\n-    @IR(counts = {IRNode.LOAD_VECTOR_I, IRNode.VECTOR_SIZE_2, \"> 0\",\n-                  IRNode.LOAD_VECTOR_I, IRNode.VECTOR_SIZE_4, \"> 0\",\n-                  IRNode.AND_VI,        IRNode.VECTOR_SIZE_2, \"> 0\",\n-                  IRNode.AND_VI,        IRNode.VECTOR_SIZE_4, \"> 0\",\n-                  IRNode.STORE_VECTOR, \"> 0\"},\n-        applyIfAnd = {\"MaxVectorSize\", \">=32\", \"UseCompactObjectHeaders\", \"false\"},\n+        applyIf = {\"MaxVectorSize\", \">=32\"},\n@@ -336,4 +328,0 @@\n-            \/\/ With AlignVector, we need 8-byte alignment of vector loads\/stores.\n-            \/\/ UseCompactObjectHeaders=false                 UseCompactObjectHeaders=true\n-            \/\/ adr = base + 16 + 32*i  ->  always            adr = base + 12 + 32*i  ->  never\n-            \/\/ -> vectorize                                  -> no vectorization\n@@ -350,9 +338,1 @@\n-        applyIfAnd = {\"MaxVectorSize\", \">=32\", \"AlignVector\", \"false\"},\n-        applyIfPlatform = {\"64-bit\", \"true\"},\n-        applyIfCPUFeatureOr = {\"sse4.1\", \"true\", \"asimd\", \"true\", \"rvv\", \"true\"})\n-    @IR(counts = {IRNode.LOAD_VECTOR_I, IRNode.VECTOR_SIZE_2, \"> 0\",\n-                  IRNode.LOAD_VECTOR_I, IRNode.VECTOR_SIZE_4, \"> 0\",\n-                  IRNode.ADD_VI,        IRNode.VECTOR_SIZE_4, \"> 0\",\n-                  IRNode.MUL_VI,        IRNode.VECTOR_SIZE_2, \"> 0\",\n-                  IRNode.STORE_VECTOR, \"> 0\"},\n-        applyIfAnd = {\"MaxVectorSize\", \">=32\", \"UseCompactObjectHeaders\", \"false\"},\n+        applyIf = {\"MaxVectorSize\", \">=32\"},\n@@ -371,4 +351,0 @@\n-            \/\/ With AlignVector, we need 8-byte alignment of vector loads\/stores.\n-            \/\/ UseCompactObjectHeaders=false                 UseCompactObjectHeaders=true\n-            \/\/ adr = base + 16 + 32*i  ->  always            adr = base + 12 + 32*i  ->  never\n-            \/\/ -> vectorize                                  -> no vectorization\n@@ -385,9 +361,1 @@\n-        applyIfAnd = {\"MaxVectorSize\", \">=32\", \"AlignVector\", \"false\"},\n-        applyIfPlatform = {\"64-bit\", \"true\"},\n-        applyIfCPUFeatureOr = {\"sse4.1\", \"true\", \"asimd\", \"true\", \"rvv\", \"true\"})\n-    @IR(counts = {IRNode.LOAD_VECTOR_I, IRNode.VECTOR_SIZE_2, \"> 0\",\n-                  IRNode.LOAD_VECTOR_I, IRNode.VECTOR_SIZE_4, \"> 0\",\n-                  IRNode.ADD_VI,        IRNode.VECTOR_SIZE_2, \"> 0\",\n-                  IRNode.MUL_VI,        IRNode.VECTOR_SIZE_4, \"> 0\",\n-                  IRNode.STORE_VECTOR, \"> 0\"},\n-        applyIfAnd = {\"MaxVectorSize\", \">=32\", \"UseCompactObjectHeaders\", \"false\"},\n+        applyIf = {\"MaxVectorSize\", \">=32\"},\n@@ -406,4 +374,0 @@\n-            \/\/ With AlignVector, we need 8-byte alignment of vector loads\/stores.\n-            \/\/ UseCompactObjectHeaders=false                 UseCompactObjectHeaders=true\n-            \/\/ adr = base + 16 + 32*i  ->  always            adr = base + 12 + 32*i  ->  never\n-            \/\/ -> vectorize                                  -> no vectorization\n@@ -420,9 +384,1 @@\n-        applyIfAnd = {\"MaxVectorSize\", \">=32\", \"AlignVector\", \"false\"},\n-        applyIfPlatform = {\"64-bit\", \"true\"},\n-        applyIfCPUFeatureOr = {\"avx2\", \"true\", \"asimd\", \"true\", \"rvv\", \"true\"})\n-    @IR(counts = {IRNode.LOAD_VECTOR_I, IRNode.VECTOR_SIZE_2, \"> 0\",\n-                  IRNode.LOAD_VECTOR_I, IRNode.VECTOR_SIZE_4, \"> 0\",\n-                  IRNode.ADD_VI,        IRNode.VECTOR_SIZE_2, \"> 0\",\n-                  IRNode.MUL_VI,        IRNode.VECTOR_SIZE_4, \"> 0\",\n-                  IRNode.STORE_VECTOR, \"> 0\"},\n-        applyIfAnd = {\"MaxVectorSize\", \">=32\", \"UseCompactObjectHeaders\", \"false\"},\n+        applyIf = {\"MaxVectorSize\", \">=32\"},\n@@ -441,4 +397,0 @@\n-            \/\/ With AlignVector, we need 8-byte alignment of vector loads\/stores.\n-            \/\/ UseCompactObjectHeaders=false                 UseCompactObjectHeaders=true\n-            \/\/ adr = base + 16 + 32*i  ->  always            adr = base + 12 + 32*i  ->  never\n-            \/\/ -> vectorize                                  -> no vectorization\n@@ -455,9 +407,1 @@\n-        applyIfAnd = {\"MaxVectorSize\", \">=32\", \"AlignVector\", \"false\"},\n-        applyIfPlatform = {\"64-bit\", \"true\"},\n-        applyIfCPUFeatureOr = {\"avx2\", \"true\", \"asimd\", \"true\", \"rvv\", \"true\"})\n-    @IR(counts = {IRNode.LOAD_VECTOR_I, IRNode.VECTOR_SIZE_2, \"> 0\",\n-                  IRNode.LOAD_VECTOR_I, IRNode.VECTOR_SIZE_4, \"> 0\",\n-                  IRNode.ADD_VI,        IRNode.VECTOR_SIZE_4, \"> 0\",\n-                  IRNode.MUL_VI,        IRNode.VECTOR_SIZE_2, \"> 0\",\n-                  IRNode.STORE_VECTOR, \"> 0\"},\n-        applyIfAnd = {\"MaxVectorSize\", \">=32\", \"UseCompactObjectHeaders\", \"false\"},\n+        applyIf = {\"MaxVectorSize\", \">=32\"},\n@@ -476,4 +420,0 @@\n-            \/\/ With AlignVector, we need 8-byte alignment of vector loads\/stores.\n-            \/\/ UseCompactObjectHeaders=false                 UseCompactObjectHeaders=true\n-            \/\/ adr = base + 16 + 32*i  ->  always            adr = base + 12 + 32*i  ->  never\n-            \/\/ -> vectorize                                  -> no vectorization\n@@ -490,9 +430,1 @@\n-        applyIfAnd = {\"MaxVectorSize\", \">=32\", \"AlignVector\", \"false\"},\n-        applyIfPlatform = {\"64-bit\", \"true\"},\n-        applyIfCPUFeatureOr = {\"sse4.1\", \"true\", \"asimd\", \"true\", \"rvv\", \"true\"})\n-    @IR(counts = {IRNode.LOAD_VECTOR_I, IRNode.VECTOR_SIZE_2, \"> 0\",\n-                  IRNode.LOAD_VECTOR_I, IRNode.VECTOR_SIZE_4, \"> 0\",\n-                  IRNode.AND_VI,        IRNode.VECTOR_SIZE_2, \"> 0\",\n-                  IRNode.AND_VI,        IRNode.VECTOR_SIZE_4, \"> 0\",\n-                  IRNode.STORE_VECTOR, \"> 0\"},\n-        applyIfAnd = {\"MaxVectorSize\", \">=32\", \"UseCompactObjectHeaders\", \"false\"},\n+        applyIf = {\"MaxVectorSize\", \">=32\"},\n@@ -525,4 +457,0 @@\n-            \/\/ With AlignVector, we need 8-byte alignment of vector loads\/stores.\n-            \/\/ UseCompactObjectHeaders=false                 UseCompactObjectHeaders=true\n-            \/\/ adr = base + 16 + 32*i  ->  always            adr = base + 12 + 32*i  ->  never\n-            \/\/ -> vectorize                                  -> no vectorization\n@@ -539,9 +467,1 @@\n-        applyIfAnd = {\"MaxVectorSize\", \">=32\", \"AlignVector\", \"false\"},\n-        applyIfPlatform = {\"64-bit\", \"true\"},\n-        applyIfCPUFeatureOr = {\"sse4.1\", \"true\", \"asimd\", \"true\", \"rvv\", \"true\"})\n-    @IR(counts = {IRNode.LOAD_VECTOR_I, IRNode.VECTOR_SIZE_2, \"> 0\",\n-                  IRNode.LOAD_VECTOR_I, IRNode.VECTOR_SIZE_4, \"> 0\",\n-                  IRNode.AND_VI,        IRNode.VECTOR_SIZE_2, \"> 0\",\n-                  IRNode.AND_VI,        IRNode.VECTOR_SIZE_4, \"> 0\",\n-                  IRNode.STORE_VECTOR, \"> 0\"},\n-        applyIfAnd = {\"MaxVectorSize\", \">=32\", \"UseCompactObjectHeaders\", \"false\"},\n+        applyIf = {\"MaxVectorSize\", \">=32\"},\n@@ -574,4 +494,0 @@\n-            \/\/ With AlignVector, we need 8-byte alignment of vector loads\/stores.\n-            \/\/ UseCompactObjectHeaders=false                 UseCompactObjectHeaders=true\n-            \/\/ adr = base + 16 + 32*i  ->  always            adr = base + 12 + 32*i  ->  never\n-            \/\/ -> vectorize                                  -> no vectorization\n@@ -588,9 +504,1 @@\n-        applyIfAnd = {\"MaxVectorSize\", \">=32\", \"AlignVector\", \"false\"},\n-        applyIfPlatform = {\"64-bit\", \"true\"},\n-        applyIfCPUFeatureOr = {\"sse4.1\", \"true\", \"asimd\", \"true\", \"rvv\", \"true\"})\n-    @IR(counts = {IRNode.LOAD_VECTOR_I, IRNode.VECTOR_SIZE_2, \"> 0\",\n-                  IRNode.LOAD_VECTOR_I, IRNode.VECTOR_SIZE_4, \"> 0\",\n-                  IRNode.AND_VI,        IRNode.VECTOR_SIZE_2, \"> 0\",\n-                  IRNode.AND_VI,        IRNode.VECTOR_SIZE_4, \"> 0\",\n-                  IRNode.STORE_VECTOR, \"> 0\"},\n-        applyIfAnd = {\"MaxVectorSize\", \">=32\", \"UseCompactObjectHeaders\", \"false\"},\n+        applyIf = {\"MaxVectorSize\", \">=32\"},\n@@ -623,4 +531,0 @@\n-            \/\/ With AlignVector, we need 8-byte alignment of vector loads\/stores.\n-            \/\/ UseCompactObjectHeaders=false                 UseCompactObjectHeaders=true\n-            \/\/ adr = base + 16 + 32*i  ->  always            adr = base + 12 + 32*i  ->  never\n-            \/\/ -> vectorize                                  -> no vectorization\n@@ -637,9 +541,1 @@\n-        applyIfAnd = {\"MaxVectorSize\", \">=32\", \"AlignVector\", \"false\"},\n-        applyIfPlatform = {\"64-bit\", \"true\"},\n-        applyIfCPUFeatureOr = {\"sse4.1\", \"true\", \"asimd\", \"true\", \"rvv\", \"true\"})\n-    @IR(counts = {IRNode.LOAD_VECTOR_I, IRNode.VECTOR_SIZE_2, \"> 0\",\n-                  IRNode.LOAD_VECTOR_I, IRNode.VECTOR_SIZE_4, \"> 0\",\n-                  IRNode.AND_VI,        IRNode.VECTOR_SIZE_2, \"> 0\",\n-                  IRNode.AND_VI,        IRNode.VECTOR_SIZE_4, \"> 0\",\n-                  IRNode.STORE_VECTOR, \"> 0\"},\n-        applyIfAnd = {\"MaxVectorSize\", \">=32\", \"UseCompactObjectHeaders\", \"false\"},\n+        applyIf = {\"MaxVectorSize\", \">=32\"},\n@@ -672,4 +568,0 @@\n-            \/\/ With AlignVector, we need 8-byte alignment of vector loads\/stores.\n-            \/\/ UseCompactObjectHeaders=false                 UseCompactObjectHeaders=true\n-            \/\/ adr = base + 16 + 32*i  ->  always            adr = base + 12 + 32*i  ->  never\n-            \/\/ -> vectorize                                  -> no vectorization\n@@ -683,6 +575,1 @@\n-        applyIfAnd = {\"MaxVectorSize\", \">=32\", \"AlignVector\", \"false\"},\n-        applyIfPlatform = {\"64-bit\", \"true\"},\n-        applyIfCPUFeatureOr = {\"sse4.1\", \"true\", \"asimd\", \"true\", \"rvv\", \"true\"})\n-    @IR(counts = {IRNode.LOAD_VECTOR_S, IRNode.VECTOR_SIZE_4, \"> 0\",\n-                  IRNode.STORE_VECTOR, \"> 0\"},\n-        applyIfAnd = {\"MaxVectorSize\", \">=32\", \"UseCompactObjectHeaders\", \"false\"},\n+        applyIf = {\"MaxVectorSize\", \">=32\"},\n@@ -725,5 +612,0 @@\n-\n-            \/\/ With AlignVector, we need 8-byte alignment of vector loads\/stores.\n-            \/\/ UseCompactObjectHeaders=false                 UseCompactObjectHeaders=true\n-            \/\/ adr = base + 16 + 8 + 32*i  ->  always        adr = base + 12 + 8 + 32*i  ->  never\n-            \/\/ -> vectorize                                  -> no vectorization\n@@ -1129,9 +1011,1 @@\n-        applyIfAnd = {\"MaxVectorSize\", \">=32\", \"AlignVector\", \"false\"},\n-        applyIfPlatform = {\"64-bit\", \"true\"},\n-        applyIfCPUFeatureOr = {\"sse4.1\", \"true\", \"asimd\", \"true\", \"rvv\", \"true\"})\n-    @IR(counts = {IRNode.LOAD_VECTOR_I,   IRNode.VECTOR_SIZE_4, \"> 0\",\n-                  IRNode.MUL_VI,          IRNode.VECTOR_SIZE_4, \"> 0\",\n-                  IRNode.AND_VI,          IRNode.VECTOR_SIZE_4, \"> 0\",\n-                  IRNode.ADD_VI,          IRNode.VECTOR_SIZE_4, \"> 0\", \/\/ reduction moved out of loop\n-                  IRNode.ADD_REDUCTION_V,                       \"> 0\"},\n-        applyIfAnd = {\"MaxVectorSize\", \">=32\", \"UseCompactObjectHeaders\", \"false\"},\n+        applyIf = {\"MaxVectorSize\", \">=32\"},\n@@ -1153,4 +1027,0 @@\n-            \/\/ With AlignVector, we need 8-byte alignment of vector loads\/stores.\n-            \/\/ UseCompactObjectHeaders=false                 UseCompactObjectHeaders=true\n-            \/\/ adr = base + 16 + 32*i  ->  always            adr = base + 12 + 32*i  ->  never\n-            \/\/ -> vectorize                                  -> no vectorization\n","filename":"test\/hotspot\/jtreg\/compiler\/loopopts\/superword\/TestSplitPacks.java","additions":11,"deletions":141,"binary":false,"changes":152,"status":"modified"},{"patch":"@@ -67,1 +67,0 @@\n-        applyIfOr = {\"AlignVector\", \"false\", \"UseCompactObjectHeaders\", \"false\"},\n@@ -107,5 +106,0 @@\n-\n-            \/\/ With AlignVector, we need 8-byte alignment of vector loads\/stores.\n-            \/\/ UseCompactObjectHeaders=false                 UseCompactObjectHeaders=true\n-            \/\/ adr = base + 16 + 8*i  ->  always             adr = base + 12 + 8*i  ->  never\n-            \/\/ -> vectorize                                  -> no vectorization\n","filename":"test\/hotspot\/jtreg\/compiler\/loopopts\/superword\/TestUnorderedReductionPartialVectorization.java","additions":0,"deletions":6,"binary":false,"changes":6,"status":"modified"},{"patch":"@@ -59,1 +59,0 @@\n-        applyIfOr = {\"UseCompactObjectHeaders\", \"false\", \"AlignVector\", \"false\"},\n@@ -65,5 +64,0 @@\n-            \/\/ With AlignVector, we need 8-byte alignment of vector loads\/stores.\n-            \/\/ UseCompactObjectHeaders=false                 UseCompactObjectHeaders=true\n-            \/\/ F_adr = base + 16 + 4*i   ->  i % 2 = 0       F_adr = base + 12 + 4*i   ->  i % 2 = 1\n-            \/\/ S_adr = base + 16 + 2*i   ->  i % 4 = 0       S_adr = base + 12 + 2*i   ->  i % 4 = 2\n-            \/\/ -> vectorize                                  -> no vectorization\n@@ -132,1 +126,0 @@\n-        applyIfOr = {\"UseCompactObjectHeaders\", \"false\", \"AlignVector\", \"false\"},\n@@ -138,5 +131,0 @@\n-            \/\/ With AlignVector, we need 8-byte alignment of vector loads\/stores.\n-            \/\/ UseCompactObjectHeaders=false                 UseCompactObjectHeaders=true\n-            \/\/ F_adr = base + 16 + 4*i   ->  i % 2 = 0       F_adr = base + 12 + 4*i   ->  i % 2 = 1\n-            \/\/ S_adr = base + 16 + 2*i   ->  i % 4 = 0       S_adr = base + 12 + 2*i   ->  i % 4 = 2\n-            \/\/ -> vectorize                                  -> no vectorization\n","filename":"test\/hotspot\/jtreg\/compiler\/vectorization\/TestFloatConversionsVector.java","additions":0,"deletions":12,"binary":false,"changes":12,"status":"modified"},{"patch":"@@ -71,1 +71,0 @@\n-        applyIfOr = {\"UseCompactObjectHeaders\", \"false\", \"AlignVector\", \"false\"})\n@@ -75,5 +74,0 @@\n-            \/\/ With AlignVector, we need 8-byte alignment of vector loads\/stores.\n-            \/\/ UseCompactObjectHeaders=false                 UseCompactObjectHeaders=true\n-            \/\/ F_adr = base + 16 + 4*i   ->  i % 2 = 0       F_adr = base + 12 + 4*i   ->  i % 2 = 1\n-            \/\/ S_adr = base + 16 + 2*i   ->  i % 4 = 0       S_adr = base + 12 + 2*i   ->  i % 4 = 2\n-            \/\/ -> vectorize                                  -> no vectorization\n@@ -155,2 +149,1 @@\n-    @IR(counts = {IRNode.VECTOR_CAST_HF2F, IRNode.VECTOR_SIZE + \"min(max_float, max_short)\", \"> 0\"},\n-        applyIfOr = {\"UseCompactObjectHeaders\", \"false\", \"AlignVector\", \"false\"})\n+    @IR(counts = {IRNode.VECTOR_CAST_HF2F, IRNode.VECTOR_SIZE + \"min(max_float, max_short)\", \"> 0\"})\n@@ -160,5 +153,0 @@\n-            \/\/ With AlignVector, we need 8-byte alignment of vector loads\/stores.\n-            \/\/ UseCompactObjectHeaders=false                 UseCompactObjectHeaders=true\n-            \/\/ F_adr = base + 16 + 4*i   ->  i % 2 = 0       F_adr = base + 12 + 4*i   ->  i % 2 = 1\n-            \/\/ S_adr = base + 16 + 2*i   ->  i % 4 = 0       S_adr = base + 12 + 2*i   ->  i % 4 = 2\n-            \/\/ -> vectorize                                  -> no vectorization\n","filename":"test\/hotspot\/jtreg\/compiler\/vectorization\/TestFloatConversionsVectorNaN.java","additions":1,"deletions":13,"binary":false,"changes":14,"status":"modified"},{"patch":"@@ -64,1 +64,1 @@\n-\/\/ This means it affects the alignment constraints.\n+\/\/ It should, however, not affect the alignment constraints.\n@@ -237,1 +237,0 @@\n-        applyIfOr = {\"AlignVector\", \"false\", \"UseCompactObjectHeaders\", \"false\"},\n@@ -243,7 +242,0 @@\n-            \/\/ AlignVector=true requires that all vector load\/store are 8-byte aligned.\n-            \/\/ F_adr = base + UNSAFE.ARRAY_FLOAT_BASE_OFFSET + 4*i\n-            \/\/                = 16 (UseCompactObjectHeaders=false)    -> i % 2 = 0\n-            \/\/                = 12 (UseCompactObjectHeaders=true )    -> i % 2 = 1\n-            \/\/ S_adr = base + UNSAFE.ARRAY_SHORT_BASE_OFFSET + 2*i\n-            \/\/                = 16 (UseCompactObjectHeaders=false)    -> i % 4 = 0  -> can align both\n-            \/\/                = 12 (UseCompactObjectHeaders=true )    -> i % 4 = 2  -> cannot align both\n@@ -359,1 +351,1 @@\n-        applyIfOr = {\"AlignVector\", \"false\", \"UseCompactObjectHeaders\", \"false\"},\n+        applyIf = {\"AlignVector\", \"false\"},\n@@ -362,1 +354,1 @@\n-        applyIfOr = {\"AlignVector\", \"false\", \"UseCompactObjectHeaders\", \"false\"},\n+        applyIf = {\"AlignVector\", \"false\"},\n@@ -365,1 +357,1 @@\n-        applyIfOr = {\"AlignVector\", \"false\", \"UseCompactObjectHeaders\", \"false\"},\n+        applyIf = {\"AlignVector\", \"false\"},\n@@ -371,7 +363,0 @@\n-            \/\/ AlignVector=true requires that all vector load\/store are 8-byte aligned.\n-            \/\/ F_adr = base + UNSAFE.ARRAY_FLOAT_BASE_OFFSET + 4*i\n-            \/\/                = 16 (UseCompactObjectHeaders=false)    -> i % 2 = 0\n-            \/\/                = 12 (UseCompactObjectHeaders=true )    -> i % 2 = 1\n-            \/\/ S_adr = base + UNSAFE.ARRAY_SHORT_BASE_OFFSET + 2*i\n-            \/\/                = 16 (UseCompactObjectHeaders=false)    -> i % 4 = 0  -> can align both\n-            \/\/                = 12 (UseCompactObjectHeaders=true )    -> i % 4 = 2  -> cannot align both\n@@ -384,1 +369,1 @@\n-        applyIfOr = {\"AlignVector\", \"false\", \"UseCompactObjectHeaders\", \"false\"},\n+        applyIf = {\"AlignVector\", \"false\"},\n@@ -387,1 +372,1 @@\n-        applyIfOr = {\"AlignVector\", \"false\", \"UseCompactObjectHeaders\", \"false\"},\n+        applyIf = {\"AlignVector\", \"false\"},\n@@ -390,1 +375,1 @@\n-        applyIfOr = {\"AlignVector\", \"false\", \"UseCompactObjectHeaders\", \"false\"},\n+        applyIf = {\"AlignVector\", \"false\"},\n@@ -396,7 +381,0 @@\n-            \/\/ AlignVector=true requires that all vector load\/store are 8-byte aligned.\n-            \/\/ F_adr = base + UNSAFE.ARRAY_FLOAT_BASE_OFFSET + 4*i\n-            \/\/                = 16 (UseCompactObjectHeaders=false)    -> i % 2 = 0\n-            \/\/                = 12 (UseCompactObjectHeaders=true )    -> i % 2 = 1\n-            \/\/ S_adr = base + UNSAFE.ARRAY_SHORT_BASE_OFFSET + 2*i\n-            \/\/                = 16 (UseCompactObjectHeaders=false)    -> i % 4 = 0  -> can align both\n-            \/\/                = 12 (UseCompactObjectHeaders=true )    -> i % 4 = 2  -> cannot align both\n","filename":"test\/hotspot\/jtreg\/compiler\/vectorization\/runner\/ArrayTypeConvertTest.java","additions":7,"deletions":29,"binary":false,"changes":36,"status":"modified"},{"patch":"@@ -77,0 +77,2 @@\n+            baseArgs.add(\"-XX:+UnlockExperimentalVMOptions\");\n+            baseArgs.add(\"-XX:-UseCompactObjectHeaders\");\n","filename":"test\/hotspot\/jtreg\/runtime\/cds\/DeterministicDump.java","additions":2,"deletions":0,"binary":false,"changes":2,"status":"modified"},{"patch":"@@ -30,0 +30,1 @@\n+ * @requires vm.opt.final.UseCompactObjectHeaders == false\n@@ -51,0 +52,1 @@\n+ * @requires vm.opt.final.UseCompactObjectHeaders == false\n","filename":"test\/hotspot\/jtreg\/runtime\/cds\/appcds\/TestParallelGCWithCDS.java","additions":2,"deletions":0,"binary":false,"changes":2,"status":"modified"},{"patch":"@@ -43,0 +43,1 @@\n+ * @comment Marked as flagless until JDK-8344015 is fixed\n@@ -45,2 +46,1 @@\n- * @requires vm.flagless\n- * @comment Marked as flagless until JDK-8344015 is fixed\n+ * @requires vm.opt.final.UseCompactObjectHeaders == false\n","filename":"test\/jdk\/jdk\/jfr\/event\/oldobject\/TestObjectDescription.java","additions":2,"deletions":2,"binary":false,"changes":4,"status":"modified"},{"patch":"@@ -389,0 +389,1 @@\n+        vmOptFinalFlag(map, \"UseCompactObjectHeaders\");\n","filename":"test\/jtreg-ext\/requires\/VMProps.java","additions":1,"deletions":0,"binary":false,"changes":1,"status":"modified"}]}