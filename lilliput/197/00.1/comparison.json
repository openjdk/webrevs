{"files":[{"patch":"@@ -5773,0 +5773,3 @@\n+opclass memory_noindex(indirect,\n+                       indOffI1, indOffL1,indOffI2, indOffL2, indOffI4, indOffL4, indOffI8, indOffL8,\n+                       indirectN, indOffIN, indOffLN, indirectX2P, indOffX2P);\n@@ -6710,1 +6713,1 @@\n-instruct loadNKlassCompactHeaders(iRegNNoSp dst, memory4 mem)\n+instruct loadNKlassCompactHeaders(iRegNNoSp dst, memory_noindex mem)\n@@ -6718,1 +6721,1 @@\n-    \"lsrw  $dst, $dst, markWord::klass_shift_at_offset\"\n+    \"lsrw  $dst, $dst, markWord::klass_shift\"\n@@ -6721,4 +6724,5 @@\n-    \/\/ inlined aarch64_enc_ldrw\n-    loadStore(masm, &MacroAssembler::ldrw, $dst$$Register, $mem->opcode(),\n-              as_Register($mem$$base), $mem$$index, $mem$$scale, $mem$$disp, 4);\n-    __ lsrw($dst$$Register, $dst$$Register, markWord::klass_shift_at_offset);\n+    assert($mem$$index$$Register == noreg, \"must not have indexed address\");\n+    \/\/ The incoming address is pointing into obj-start + klass_offset_in_bytes. We need to extract\n+    \/\/ obj-start, so that we can load from the object's mark-word instead.\n+    __ ldrw($dst$$Register, Address($mem$$base$$Register, $mem$$disp - Type::klass_offset()));\n+    __ lsrw($dst$$Register, $dst$$Register, markWord::klass_shift);\n","filename":"src\/hotspot\/cpu\/aarch64\/aarch64.ad","additions":10,"deletions":6,"binary":false,"changes":16,"status":"modified"},{"patch":"@@ -4736,2 +4736,3 @@\n-    __ lwu(as_Register($dst$$reg), Address(as_Register($mem$$base), $mem$$disp));\n-    __ srli(as_Register($dst$$reg), as_Register($dst$$reg), (unsigned) markWord::klass_shift_at_offset);\n+    Unimplemented();\n+    \/\/ __ lwu(as_Register($dst$$reg), Address(as_Register($mem$$base), $mem$$disp));\n+    \/\/ __ srli(as_Register($dst$$reg), as_Register($dst$$reg), (unsigned) markWord::klass_shift_at_offset);\n","filename":"src\/hotspot\/cpu\/riscv\/riscv.ad","additions":3,"deletions":2,"binary":false,"changes":5,"status":"modified"},{"patch":"@@ -6081,2 +6081,2 @@\n-  movq(dst, Address(src, oopDesc::mark_offset_in_bytes()));\n-  shrq(dst, markWord::klass_shift);\n+  movl(dst, Address(src, oopDesc::mark_offset_in_bytes()));\n+  shrl(dst, markWord::klass_shift);\n","filename":"src\/hotspot\/cpu\/x86\/macroAssembler_x86.cpp","additions":2,"deletions":2,"binary":false,"changes":4,"status":"modified"},{"patch":"@@ -3602,2 +3602,1 @@\n-      assert(is_aligned(oopDesc::base_offset_in_bytes(), BytesPerLong), \"oop base offset must be 8-byte-aligned\");\n-      __ decrement(rdx, oopDesc::base_offset_in_bytes());\n+      __ decrement(rdx, align_up(oopDesc::base_offset_in_bytes(), BytesPerLong));\n","filename":"src\/hotspot\/cpu\/x86\/templateTable_x86.cpp","additions":1,"deletions":2,"binary":false,"changes":3,"status":"modified"},{"patch":"@@ -4387,1 +4387,1 @@\n-    \"shrl    $dst, markWord::klass_shift_at_offset\"\n+    \"shrl    $dst, markWord::klass_shift\"\n@@ -4390,0 +4390,4 @@\n+    \/\/ The incoming address is pointing into obj-start + Type::klass_offset(). We need to extract\n+    \/\/ obj-start, so that we can load from the object's mark-word instead.\n+    Register d = $dst$$Register;\n+    Address  s = ($mem$$Address).plus_disp(-Type::klass_offset());\n@@ -4391,5 +4395,4 @@\n-      __ eshrl($dst$$Register, $mem$$Address, markWord::klass_shift_at_offset, false);\n-    }\n-    else {\n-      __ movl($dst$$Register, $mem$$Address);\n-      __ shrl($dst$$Register, markWord::klass_shift_at_offset);\n+      __ eshrl(d, s, markWord::klass_shift, false);\n+    } else {\n+      __ movl(d, s);\n+      __ shrl(d, markWord::klass_shift);\n","filename":"src\/hotspot\/cpu\/x86\/x86_64.ad","additions":9,"deletions":6,"binary":false,"changes":15,"status":"modified"},{"patch":"@@ -1250,1 +1250,3 @@\n-        byte_size = source_oop->size() * BytesPerWord;\n+        size_t old_size = source_oop->size();\n+        size_t new_size = source_oop->copy_size_cds(old_size, source_oop->mark());\n+        byte_size = new_size * BytesPerWord;\n","filename":"src\/hotspot\/share\/cds\/archiveBuilder.cpp","additions":3,"deletions":1,"binary":false,"changes":4,"status":"modified"},{"patch":"@@ -414,1 +414,1 @@\n-      oop m = java_lang_Class::create_basic_type_mirror(type2name(bt), bt, CHECK);\n+      oop m = java_lang_Class::create_basic_type_mirror(type2name(bt), bt, true, CHECK);\n@@ -582,0 +582,1 @@\n+  assert(!UseCompactObjectHeaders || scratch_m->mark().is_not_hashed_expanded(), \"scratch mirror must have not-hashed-expanded state\");\n@@ -583,0 +584,1 @@\n+    intptr_t orig_mark = orig_mirror->mark().value();\n@@ -585,2 +587,17 @@\n-      narrowKlass nk = CompressedKlassPointers::encode(orig_mirror->klass());\n-      scratch_m->set_mark(markWord::prototype().set_narrow_klass(nk).copy_set_hash(src_hash));\n+      \/\/ We leave the cases not_hashed\/not_hashed_expanded as they are.\n+      assert(orig_mirror->mark().is_hashed_not_expanded() || orig_mirror->mark().is_hashed_expanded(), \"must be hashed\");\n+      Klass* orig_klass = orig_mirror->klass();\n+      narrowKlass nk = CompressedKlassPointers::encode(orig_klass);\n+      markWord mark = markWord::prototype().set_narrow_klass(nk);\n+      mark = mark.copy_hashctrl_from(orig_mirror->mark());\n+      if (mark.is_hashed_not_expanded()) {\n+        scratch_m->initialize_hash_if_necessary(orig_mirror, orig_klass, mark);\n+      } else {\n+        assert(mark.is_hashed_expanded(), \"must be hashed & moved\");\n+        int offset = orig_klass->hash_offset_in_bytes(orig_mirror, mark);\n+        assert(offset >= 8, \"hash offset must not be in header\");\n+        scratch_m->int_field_put(offset, (jint) src_hash);\n+        scratch_m->set_mark(mark);\n+      }\n+      assert(scratch_m->mark().is_hashed_expanded(), \"must be hashed & moved\");\n+      assert(scratch_m->mark().is_not_hashed_expanded() || scratch_m->mark().is_hashed_expanded(), \"must be not hashed and expanded\");\n@@ -589,0 +606,2 @@\n+      DEBUG_ONLY(intptr_t archived_hash = scratch_m->identity_hash());\n+      assert(src_hash == archived_hash, \"Different hash codes: original \" INTPTR_FORMAT \", archived \" INTPTR_FORMAT, src_hash, archived_hash);\n@@ -591,3 +610,0 @@\n-\n-    DEBUG_ONLY(intptr_t archived_hash = scratch_m->identity_hash());\n-    assert(src_hash == archived_hash, \"Different hash codes: original \" INTPTR_FORMAT \", archived \" INTPTR_FORMAT, src_hash, archived_hash);\n","filename":"src\/hotspot\/share\/cds\/heapShared.cpp","additions":22,"deletions":6,"binary":false,"changes":28,"status":"modified"},{"patch":"@@ -295,0 +295,4 @@\n+  int hash_offset_in_bytes() const {\n+    return get_instanceKlass()->hash_offset_in_bytes(nullptr, markWord(0));\n+  }\n+\n","filename":"src\/hotspot\/share\/ci\/ciInstanceKlass.hpp","additions":4,"deletions":0,"binary":false,"changes":4,"status":"modified"},{"patch":"@@ -75,0 +75,3 @@\n+  bool is_mirror_instance_klass() { return get_Klass()->is_mirror_instance_klass(); }\n+  bool is_reference_instance_klass() { return get_Klass()->is_reference_instance_klass(); }\n+\n","filename":"src\/hotspot\/share\/ci\/ciKlass.hpp","additions":3,"deletions":0,"binary":false,"changes":3,"status":"modified"},{"patch":"@@ -4894,0 +4894,4 @@\n+int ClassFileParser::hash_offset() const {\n+  return _field_info->_hash_offset;\n+}\n+\n","filename":"src\/hotspot\/share\/classfile\/classFileParser.cpp","additions":4,"deletions":0,"binary":false,"changes":4,"status":"modified"},{"patch":"@@ -1059,1 +1059,1 @@\n-  oop mirror_oop = InstanceMirrorKlass::cast(vmClasses::Class_klass())->allocate_instance(k, CHECK);\n+  oop mirror_oop = InstanceMirrorKlass::cast(vmClasses::Class_klass())->allocate_instance(k, is_scratch, CHECK);\n@@ -1360,1 +1360,1 @@\n-oop java_lang_Class::create_basic_type_mirror(const char* basic_type_name, BasicType type, TRAPS) {\n+oop java_lang_Class::create_basic_type_mirror(const char* basic_type_name, BasicType type, bool is_scratch, TRAPS) {\n@@ -1362,1 +1362,1 @@\n-  oop java_class = InstanceMirrorKlass::cast(vmClasses::Class_klass())->allocate_instance(nullptr, CHECK_NULL);\n+  oop java_class = InstanceMirrorKlass::cast(vmClasses::Class_klass())->allocate_instance(nullptr, is_scratch, CHECK_NULL);\n","filename":"src\/hotspot\/share\/classfile\/javaClasses.cpp","additions":3,"deletions":3,"binary":false,"changes":6,"status":"modified"},{"patch":"@@ -284,1 +284,1 @@\n-  static oop  create_basic_type_mirror(const char* basic_type_name, BasicType type, TRAPS);\n+  static oop  create_basic_type_mirror(const char* basic_type_name, BasicType type, bool is_scratch, TRAPS);\n","filename":"src\/hotspot\/share\/classfile\/javaClasses.hpp","additions":1,"deletions":1,"binary":false,"changes":2,"status":"modified"},{"patch":"@@ -288,1 +288,1 @@\n-  assert(java_lang_Class::is_instance(java_class), \"must be a Class object\");\n+  \/\/assert(java_lang_Class::is_instance(java_class), \"must be a Class object\");\n","filename":"src\/hotspot\/share\/classfile\/javaClasses.inline.hpp","additions":1,"deletions":1,"binary":false,"changes":2,"status":"modified"},{"patch":"@@ -214,0 +214,2 @@\n+  FullGCForwarding::begin();\n+\n@@ -226,0 +228,2 @@\n+  FullGCForwarding::end();\n+\n","filename":"src\/hotspot\/share\/gc\/g1\/g1FullCollector.cpp","additions":4,"deletions":0,"binary":false,"changes":4,"status":"modified"},{"patch":"@@ -445,1 +445,2 @@\n-  const size_t word_sz = old->size_given_klass(klass);\n+  const size_t old_size = old->size_given_mark_and_klass(old_mark, klass);\n+  const size_t word_sz = old->copy_size(old_size, old_mark);\n@@ -483,1 +484,1 @@\n-  Copy::aligned_disjoint_words(cast_from_oop<HeapWord*>(old), obj_ptr, word_sz);\n+  Copy::aligned_disjoint_words(cast_from_oop<HeapWord*>(old), obj_ptr, old_size);\n@@ -500,0 +501,2 @@\n+    obj->initialize_hash_if_necessary(old);\n+\n","filename":"src\/hotspot\/share\/gc\/g1\/g1ParScanThreadState.cpp","additions":5,"deletions":2,"binary":false,"changes":7,"status":"modified"},{"patch":"@@ -32,0 +32,1 @@\n+#include \"gc\/parallel\/psParallelCompactNew.inline.hpp\"\n@@ -123,2 +124,8 @@\n-  if (!PSParallelCompact::initialize_aux_data()) {\n-    return JNI_ENOMEM;\n+  if (UseCompactObjectHeaders) {\n+    if (!PSParallelCompactNew::initialize_aux_data()) {\n+      return JNI_ENOMEM;\n+    }\n+  } else {\n+    if (!PSParallelCompact::initialize_aux_data()) {\n+      return JNI_ENOMEM;\n+    }\n@@ -187,1 +194,5 @@\n-  PSParallelCompact::post_initialize();\n+  if (UseCompactObjectHeaders) {\n+    PSParallelCompactNew::post_initialize();\n+  } else {\n+    PSParallelCompact::post_initialize();\n+  }\n@@ -394,1 +405,5 @@\n-  PSParallelCompact::invoke(clear_all_soft_refs);\n+  if (UseCompactObjectHeaders) {\n+    PSParallelCompactNew::invoke(clear_all_soft_refs, false \/* serial *\/);\n+  } else {\n+    PSParallelCompact::invoke(clear_all_soft_refs);\n+  }\n@@ -432,1 +447,5 @@\n-    PSParallelCompact::invoke(clear_all_soft_refs);\n+    if (UseCompactObjectHeaders) {\n+      PSParallelCompactNew::invoke(clear_all_soft_refs, false \/* serial *\/);\n+    } else {\n+      PSParallelCompact::invoke(clear_all_soft_refs);\n+    }\n@@ -443,0 +462,9 @@\n+  if (UseCompactObjectHeaders) {\n+    PSParallelCompactNew::invoke(true \/* clear_soft_refs *\/, true \/* serial *\/);\n+  }\n+\n+  result = expand_heap_and_allocate(size, is_tlab);\n+  if (result != nullptr) {\n+    return result;\n+  }\n+\n@@ -538,1 +566,5 @@\n-  PSParallelCompact::invoke(clear_soft_refs);\n+  if (UseCompactObjectHeaders) {\n+    PSParallelCompactNew::invoke(clear_soft_refs, false \/* serial *\/);\n+  } else {\n+    PSParallelCompact::invoke(clear_soft_refs);\n+  }\n@@ -679,1 +711,5 @@\n-  PSParallelCompact::print_on_error(st);\n+  if (UseCompactObjectHeaders) {\n+    PSParallelCompactNew::print_on_error(st);\n+  } else {\n+    PSParallelCompact::print_on_error(st);\n+  }\n@@ -689,1 +725,5 @@\n-  log_debug(gc, heap, exit)(\"Accumulated old generation GC time %3.7f secs\", PSParallelCompact::accumulated_time()->seconds());\n+  if (UseCompactObjectHeaders) {\n+    log_debug(gc, heap, exit)(\"Accumulated old generation GC time %3.7f secs\", PSParallelCompactNew::accumulated_time()->seconds());\n+  } else {\n+    log_debug(gc, heap, exit)(\"Accumulated old generation GC time %3.7f secs\", PSParallelCompact::accumulated_time()->seconds());\n+  }\n","filename":"src\/hotspot\/share\/gc\/parallel\/parallelScavengeHeap.cpp","additions":48,"deletions":8,"binary":false,"changes":56,"status":"modified"},{"patch":"@@ -551,1 +551,1 @@\n-class PCAdjustPointerClosure: public BasicOopIterateClosure {\n+class PCAdjustPointerClosureNew: public BasicOopIterateClosure {\n@@ -562,1 +562,1 @@\n-static PCAdjustPointerClosure pc_adjust_pointer_closure;\n+static PCAdjustPointerClosureNew pc_adjust_pointer_closure;\n@@ -1059,0 +1059,2 @@\n+    FullGCForwarding::begin();\n+\n@@ -1065,0 +1067,2 @@\n+    FullGCForwarding::end();\n+\n","filename":"src\/hotspot\/share\/gc\/parallel\/psParallelCompact.cpp","additions":6,"deletions":2,"binary":false,"changes":8,"status":"modified"},{"patch":"@@ -0,0 +1,1147 @@\n+\/*\n+ * Copyright (c) 2005, 2025, Oracle and\/or its affiliates. All rights reserved.\n+ * Copyright Amazon.com Inc. or its affiliates. All Rights Reserved.\n+ * DO NOT ALTER OR REMOVE COPYRIGHT NOTICES OR THIS FILE HEADER.\n+ *\n+ * This code is free software; you can redistribute it and\/or modify it\n+ * under the terms of the GNU General Public License version 2 only, as\n+ * published by the Free Software Foundation.\n+ *\n+ * This code is distributed in the hope that it will be useful, but WITHOUT\n+ * ANY WARRANTY; without even the implied warranty of MERCHANTABILITY or\n+ * FITNESS FOR A PARTICULAR PURPOSE.  See the GNU General Public License\n+ * version 2 for more details (a copy is included in the LICENSE file that\n+ * accompanied this code).\n+ *\n+ * You should have received a copy of the GNU General Public License version\n+ * 2 along with this work; if not, write to the Free Software Foundation,\n+ * Inc., 51 Franklin St, Fifth Floor, Boston, MA 02110-1301 USA.\n+ *\n+ * Please contact Oracle, 500 Oracle Parkway, Redwood Shores, CA 94065 USA\n+ * or visit www.oracle.com if you need additional information or have any\n+ * questions.\n+ *\n+ *\/\n+\n+#include \"classfile\/classLoaderDataGraph.hpp\"\n+#include \"classfile\/symbolTable.hpp\"\n+#include \"classfile\/systemDictionary.hpp\"\n+#include \"code\/codeCache.hpp\"\n+#include \"compiler\/oopMap.hpp\"\n+#include \"gc\/parallel\/objectStartArray.inline.hpp\"\n+#include \"gc\/parallel\/parallelArguments.hpp\"\n+#include \"gc\/parallel\/parallelScavengeHeap.inline.hpp\"\n+#include \"gc\/parallel\/parMarkBitMap.inline.hpp\"\n+#include \"gc\/parallel\/psAdaptiveSizePolicy.hpp\"\n+#include \"gc\/parallel\/psCompactionManagerNew.inline.hpp\"\n+#include \"gc\/parallel\/psOldGen.hpp\"\n+#include \"gc\/parallel\/psParallelCompactNew.inline.hpp\"\n+#include \"gc\/parallel\/psPromotionManager.inline.hpp\"\n+#include \"gc\/parallel\/psScavenge.hpp\"\n+#include \"gc\/parallel\/psYoungGen.hpp\"\n+#include \"gc\/shared\/classUnloadingContext.hpp\"\n+#include \"gc\/shared\/fullGCForwarding.inline.hpp\"\n+#include \"gc\/shared\/gcCause.hpp\"\n+#include \"gc\/shared\/gcHeapSummary.hpp\"\n+#include \"gc\/shared\/gcId.hpp\"\n+#include \"gc\/shared\/gcLocker.hpp\"\n+#include \"gc\/shared\/gcTimer.hpp\"\n+#include \"gc\/shared\/gcTrace.hpp\"\n+#include \"gc\/shared\/gcTraceTime.inline.hpp\"\n+#include \"gc\/shared\/gcVMOperations.hpp\"\n+#include \"gc\/shared\/isGCActiveMark.hpp\"\n+#include \"gc\/shared\/oopStorageSetParState.inline.hpp\"\n+#include \"gc\/shared\/preservedMarks.inline.hpp\"\n+#include \"gc\/shared\/referencePolicy.hpp\"\n+#include \"gc\/shared\/referenceProcessor.hpp\"\n+#include \"gc\/shared\/referenceProcessorPhaseTimes.hpp\"\n+#include \"gc\/shared\/spaceDecorator.hpp\"\n+#include \"gc\/shared\/strongRootsScope.hpp\"\n+#include \"gc\/shared\/taskTerminator.hpp\"\n+#include \"gc\/shared\/weakProcessor.inline.hpp\"\n+#include \"gc\/shared\/workerPolicy.hpp\"\n+#include \"gc\/shared\/workerThread.hpp\"\n+#include \"gc\/shared\/workerUtils.hpp\"\n+#include \"logging\/log.hpp\"\n+#include \"memory\/iterator.inline.hpp\"\n+#include \"memory\/memoryReserver.hpp\"\n+#include \"memory\/metaspaceUtils.hpp\"\n+#include \"memory\/resourceArea.hpp\"\n+#include \"memory\/universe.hpp\"\n+#include \"nmt\/memTracker.hpp\"\n+#include \"oops\/methodData.hpp\"\n+#include \"runtime\/java.hpp\"\n+#include \"runtime\/safepoint.hpp\"\n+#include \"runtime\/threads.hpp\"\n+#include \"runtime\/vmThread.hpp\"\n+#include \"services\/memoryService.hpp\"\n+#include \"utilities\/align.hpp\"\n+#include \"utilities\/debug.hpp\"\n+#include \"utilities\/events.hpp\"\n+#include \"utilities\/formatBuffer.hpp\"\n+#include \"utilities\/macros.hpp\"\n+#if INCLUDE_JVMCI\n+#include \"jvmci\/jvmci.hpp\"\n+#endif\n+\n+SpaceInfoNew PSParallelCompactNew::_space_info[PSParallelCompactNew::last_space_id];\n+\n+size_t PSParallelCompactNew::_num_regions;\n+PCRegionData* PSParallelCompactNew::_region_data_array;\n+size_t PSParallelCompactNew::_num_regions_serial;\n+PCRegionData* PSParallelCompactNew::_region_data_array_serial;\n+PCRegionData** PSParallelCompactNew::_per_worker_region_data;\n+bool PSParallelCompactNew::_serial = false;\n+\n+SpanSubjectToDiscoveryClosure PSParallelCompactNew::_span_based_discoverer;\n+ReferenceProcessor* PSParallelCompactNew::_ref_processor = nullptr;\n+\n+void PSParallelCompactNew::print_on_error(outputStream* st) {\n+  _mark_bitmap.print_on_error(st);\n+}\n+\n+STWGCTimer          PSParallelCompactNew::_gc_timer;\n+ParallelOldTracer   PSParallelCompactNew::_gc_tracer;\n+elapsedTimer        PSParallelCompactNew::_accumulated_time;\n+unsigned int        PSParallelCompactNew::_maximum_compaction_gc_num = 0;\n+CollectorCounters*  PSParallelCompactNew::_counters = nullptr;\n+ParMarkBitMap       PSParallelCompactNew::_mark_bitmap;\n+\n+PSParallelCompactNew::IsAliveClosure PSParallelCompactNew::_is_alive_closure;\n+\n+class PCAdjustPointerClosure: public BasicOopIterateClosure {\n+  template <typename T>\n+  void do_oop_work(T* p) { PSParallelCompactNew::adjust_pointer(p); }\n+\n+public:\n+  void do_oop(oop* p) final          { do_oop_work(p); }\n+  void do_oop(narrowOop* p) final    { do_oop_work(p); }\n+\n+  ReferenceIterationMode reference_iteration_mode() final { return DO_FIELDS; }\n+};\n+\n+static PCAdjustPointerClosure pc_adjust_pointer_closure;\n+\n+class IsAliveClosure: public BoolObjectClosure {\n+public:\n+  bool do_object_b(oop p) final;\n+};\n+\n+\n+bool PSParallelCompactNew::IsAliveClosure::do_object_b(oop p) { return mark_bitmap()->is_marked(p); }\n+\n+void PSParallelCompactNew::post_initialize() {\n+  ParallelScavengeHeap* heap = ParallelScavengeHeap::heap();\n+  _span_based_discoverer.set_span(heap->reserved_region());\n+  _ref_processor =\n+    new ReferenceProcessor(&_span_based_discoverer,\n+                           ParallelGCThreads,   \/\/ mt processing degree\n+                           ParallelGCThreads,   \/\/ mt discovery degree\n+                           false,               \/\/ concurrent_discovery\n+                           &_is_alive_closure); \/\/ non-header is alive closure\n+\n+  _counters = new CollectorCounters(\"Parallel full collection pauses\", 1);\n+\n+  \/\/ Initialize static fields in ParCompactionManager.\n+  ParCompactionManagerNew::initialize(mark_bitmap());\n+}\n+\n+bool PSParallelCompactNew::initialize_aux_data() {\n+  ParallelScavengeHeap* heap = ParallelScavengeHeap::heap();\n+  MemRegion mr = heap->reserved_region();\n+  assert(mr.byte_size() != 0, \"heap should be reserved\");\n+\n+  initialize_space_info();\n+\n+  if (!_mark_bitmap.initialize(mr)) {\n+    vm_shutdown_during_initialization(\n+      err_msg(\"Unable to allocate %zuKB bitmaps for parallel \"\n+      \"garbage collection for the requested %zuKB heap.\",\n+      _mark_bitmap.reserved_byte_size()\/K, mr.byte_size()\/K));\n+    return false;\n+  }\n+\n+  return true;\n+}\n+\n+void PSParallelCompactNew::initialize_space_info()\n+{\n+  memset(&_space_info, 0, sizeof(_space_info));\n+\n+  PSYoungGen* young_gen = ParallelScavengeHeap::young_gen();\n+\n+  _space_info[old_space_id].set_space(ParallelScavengeHeap::old_gen()->object_space());\n+  _space_info[eden_space_id].set_space(young_gen->eden_space());\n+  _space_info[from_space_id].set_space(young_gen->from_space());\n+  _space_info[to_space_id].set_space(young_gen->to_space());\n+\n+  _space_info[old_space_id].set_start_array(ParallelScavengeHeap::old_gen()->start_array());\n+}\n+\n+void\n+PSParallelCompactNew::clear_data_covering_space(SpaceId id)\n+{\n+  \/\/ At this point, top is the value before GC, new_top() is the value that will\n+  \/\/ be set at the end of GC.  The marking bitmap is cleared to top; nothing\n+  \/\/ should be marked above top.\n+  MutableSpace* const space = _space_info[id].space();\n+  HeapWord* const bot = space->bottom();\n+  HeapWord* const top = space->top();\n+\n+  _mark_bitmap.clear_range(bot, top);\n+}\n+\n+void PSParallelCompactNew::pre_compact()\n+{\n+  \/\/ Update the from & to space pointers in space_info, since they are swapped\n+  \/\/ at each young gen gc.  Do the update unconditionally (even though a\n+  \/\/ promotion failure does not swap spaces) because an unknown number of young\n+  \/\/ collections will have swapped the spaces an unknown number of times.\n+  GCTraceTime(Debug, gc, phases) tm(\"Pre Compact\", &_gc_timer);\n+  ParallelScavengeHeap* heap = ParallelScavengeHeap::heap();\n+  _space_info[from_space_id].set_space(ParallelScavengeHeap::young_gen()->from_space());\n+  _space_info[to_space_id].set_space(ParallelScavengeHeap::young_gen()->to_space());\n+\n+  \/\/ Increment the invocation count\n+  heap->increment_total_collections(true);\n+\n+  CodeCache::on_gc_marking_cycle_start();\n+\n+  heap->print_heap_before_gc();\n+  heap->trace_heap_before_gc(&_gc_tracer);\n+\n+  \/\/ Fill in TLABs\n+  heap->ensure_parsability(true);  \/\/ retire TLABs\n+\n+  if (VerifyBeforeGC && heap->total_collections() >= VerifyGCStartAt) {\n+    Universe::verify(\"Before GC\");\n+  }\n+\n+  DEBUG_ONLY(mark_bitmap()->verify_clear();)\n+}\n+\n+void PSParallelCompactNew::post_compact()\n+{\n+  GCTraceTime(Info, gc, phases) tm(\"Post Compact\", &_gc_timer);\n+\n+  CodeCache::on_gc_marking_cycle_finish();\n+  CodeCache::arm_all_nmethods();\n+\n+  for (unsigned int id = old_space_id; id < last_space_id; ++id) {\n+    \/\/ Clear the marking bitmap, summary data and split info.\n+    clear_data_covering_space(SpaceId(id));\n+  }\n+\n+  {\n+    PCRegionData* last_live[last_space_id];\n+    for (uint i = old_space_id; i < last_space_id; ++i) {\n+      last_live[i] = nullptr;\n+    }\n+\n+    \/\/ Figure out last region in each space that has live data.\n+    uint space_id = old_space_id;\n+    MutableSpace* space = _space_info[space_id].space();\n+    size_t num_regions = get_num_regions();\n+    PCRegionData* region_data_array = get_region_data_array();\n+    last_live[space_id] = &region_data_array[0];\n+    for (size_t idx = 0; idx < num_regions; idx++) {\n+      PCRegionData* rd = region_data_array + idx;\n+      if(!space->contains(rd->bottom())) {\n+        ++space_id;\n+        assert(space_id < last_space_id, \"invariant\");\n+        space = _space_info[space_id].space();\n+        log_develop_trace(gc, compaction)(\"Last live for space: %u: %zu\", space_id, idx);\n+        last_live[space_id] = rd;\n+      }\n+      assert(space->contains(rd->bottom()), \"next space should contain next region\");\n+      log_develop_trace(gc, compaction)(\"post-compact region: idx: %zu, bottom: \" PTR_FORMAT \", new_top: \" PTR_FORMAT \", end: \" PTR_FORMAT, rd->idx(), p2i(rd->bottom()), p2i(rd->new_top()), p2i(rd->end()));\n+      if (rd->new_top() > rd->bottom()) {\n+        last_live[space_id] = rd;\n+        log_develop_trace(gc, compaction)(\"Bump last live for space: %u\", space_id);\n+      }\n+    }\n+\n+    for (uint i = old_space_id; i < last_space_id; ++i) {\n+      PCRegionData* rd = last_live[i];\n+        log_develop_trace(gc, compaction)(\n+                \"Last live region in space: %u, compaction region, \" PTR_FORMAT \", #%zu: [\" PTR_FORMAT \", \" PTR_FORMAT \"), new_top: \" PTR_FORMAT,\n+                i, p2i(rd), rd->idx(),\n+                p2i(rd->bottom()), p2i(rd->end()), p2i(rd->new_top()));\n+    }\n+\n+    \/\/ Fill all gaps and update the space boundaries.\n+    space_id = old_space_id;\n+    space = _space_info[space_id].space();\n+    size_t total_live = 0;\n+    size_t total_waste = 0;\n+    for (size_t idx = 0; idx < num_regions; idx++) {\n+      PCRegionData* rd = &region_data_array[idx];\n+      PCRegionData* last_live_in_space = last_live[space_id];\n+      assert(last_live_in_space != nullptr, \"last live must not be null\");\n+      if (rd != last_live_in_space) {\n+        if (rd->new_top() < rd->end()) {\n+          ObjectStartArray* sa = start_array(SpaceId(space_id));\n+          if (sa != nullptr) {\n+            sa->update_for_block(rd->new_top(), rd->end());\n+          }\n+          ParallelScavengeHeap::heap()->fill_with_dummy_object(rd->new_top(), rd->end(), false);\n+        }\n+        size_t live = pointer_delta(rd->new_top(), rd->bottom());\n+        size_t waste = pointer_delta(rd->end(), rd->new_top());\n+        total_live += live;\n+        total_waste += waste;\n+        log_develop_trace(gc, compaction)(\n+                \"Live compaction region, #%zu: [\" PTR_FORMAT \", \" PTR_FORMAT \"), new_top: \" PTR_FORMAT \", live: %zu, waste: %zu\",\n+                rd->idx(),\n+                p2i(rd->bottom()), p2i(rd->end()), p2i(rd->new_top()), live, waste);\n+      } else {\n+        \/\/ Update top of space.\n+        space->set_top(rd->new_top());\n+        size_t live = pointer_delta(rd->new_top(), rd->bottom());\n+        total_live += live;\n+        log_develop_trace(gc, compaction)(\n+                \"Live compaction region, #%zu: [\" PTR_FORMAT \", \" PTR_FORMAT \"), new_top: \" PTR_FORMAT \", live: %zu, waste: %zu\",\n+                rd->idx(),\n+                p2i(rd->bottom()), p2i(rd->end()), p2i(rd->new_top()), live, size_t(0));\n+\n+        \/\/ Fast-Forward to next space.\n+        for (; idx < num_regions - 1; idx++) {\n+          rd = &region_data_array[idx + 1];\n+          if (!space->contains(rd->bottom())) {\n+            space_id++;\n+            assert(space_id < last_space_id, \"must be\");\n+            space = _space_info[space_id].space();\n+            assert(space->contains(rd->bottom()), \"space must contain region\");\n+            break;\n+          }\n+        }\n+      }\n+    }\n+    log_develop_debug(gc, compaction)(\"total live: %zu, total waste: %zu, ratio: %f\", total_live, total_waste, ((float)total_waste)\/((float)(total_live + total_waste)));\n+  }\n+  {\n+    FREE_C_HEAP_ARRAY(PCRegionData*, _per_worker_region_data);\n+    FREE_C_HEAP_ARRAY(PCRegionData, _region_data_array);\n+    FREE_C_HEAP_ARRAY(PCRegionData, _region_data_array_serial);\n+  }\n+#ifdef ASSERT\n+  {\n+    mark_bitmap()->verify_clear();\n+  }\n+#endif\n+\n+  ParCompactionManagerNew::flush_all_string_dedup_requests();\n+\n+  MutableSpace* const eden_space = _space_info[eden_space_id].space();\n+  MutableSpace* const from_space = _space_info[from_space_id].space();\n+  MutableSpace* const to_space   = _space_info[to_space_id].space();\n+\n+  ParallelScavengeHeap* heap = ParallelScavengeHeap::heap();\n+  bool eden_empty = eden_space->is_empty();\n+\n+  \/\/ Update heap occupancy information which is used as input to the soft ref\n+  \/\/ clearing policy at the next gc.\n+  Universe::heap()->update_capacity_and_used_at_gc();\n+\n+  bool young_gen_empty = eden_empty && from_space->is_empty() &&\n+    to_space->is_empty();\n+\n+  PSCardTable* ct = heap->card_table();\n+  MemRegion old_mr = ParallelScavengeHeap::old_gen()->committed();\n+  if (young_gen_empty) {\n+    ct->clear_MemRegion(old_mr);\n+  } else {\n+    ct->dirty_MemRegion(old_mr);\n+  }\n+\n+  {\n+    \/\/ Delete metaspaces for unloaded class loaders and clean up loader_data graph\n+    GCTraceTime(Debug, gc, phases) t(\"Purge Class Loader Data\", gc_timer());\n+    ClassLoaderDataGraph::purge(true \/* at_safepoint *\/);\n+    DEBUG_ONLY(MetaspaceUtils::verify();)\n+  }\n+\n+  \/\/ Need to clear claim bits for the next mark.\n+  ClassLoaderDataGraph::clear_claimed_marks();\n+\n+  heap->prune_scavengable_nmethods();\n+\n+#if COMPILER2_OR_JVMCI\n+  DerivedPointerTable::update_pointers();\n+#endif\n+\n+  \/\/ Signal that we have completed a visit to all live objects.\n+  Universe::heap()->record_whole_heap_examined_timestamp();\n+}\n+\n+void PSParallelCompactNew::setup_regions_parallel() {\n+  static const size_t REGION_SIZE_WORDS = (SpaceAlignment \/ HeapWordSize);\n+  size_t num_regions = 0;\n+  for (uint i = old_space_id; i < last_space_id; ++i) {\n+    MutableSpace* const space = _space_info[i].space();\n+    size_t const space_size_words = space->capacity_in_words();\n+    num_regions += align_up(space_size_words, REGION_SIZE_WORDS) \/ REGION_SIZE_WORDS;\n+  }\n+  _region_data_array = NEW_C_HEAP_ARRAY(PCRegionData, num_regions, mtGC);\n+\n+  size_t region_idx = 0;\n+  for (uint i = old_space_id; i < last_space_id; ++i) {\n+    const MutableSpace* space = _space_info[i].space();\n+    HeapWord* addr = space->bottom();\n+    HeapWord* sp_end = space->end();\n+    HeapWord* sp_top = space->top();\n+    while (addr < sp_end) {\n+      HeapWord* end = MIN2(align_up(addr + REGION_SIZE_WORDS, REGION_SIZE_WORDS), space->end());\n+      if (addr < sp_top) {\n+        HeapWord* prev_obj_start = _mark_bitmap.find_obj_beg_reverse(addr, end);\n+        if (prev_obj_start < end) {\n+          HeapWord* prev_obj_end = prev_obj_start + cast_to_oop(prev_obj_start)->size();\n+          if (end < prev_obj_end) {\n+            \/\/ Object crosses region boundary, adjust end to be after object's last word.\n+            end = prev_obj_end;\n+          }\n+        }\n+      }\n+      assert(region_idx < num_regions, \"must not exceed number of regions: region_idx: %zu, num_regions: %zu\", region_idx, num_regions);\n+      HeapWord* top;\n+      if (sp_top < addr) {\n+        top = addr;\n+      } else if (sp_top >= end) {\n+        top = end;\n+      } else {\n+        top = sp_top;\n+      }\n+      assert(ParallelScavengeHeap::heap()->is_in_reserved(addr), \"addr must be in heap: \" PTR_FORMAT, p2i(addr));\n+      new (_region_data_array + region_idx) PCRegionData(region_idx, addr, top, end);\n+      addr = end;\n+      region_idx++;\n+    }\n+  }\n+  _num_regions = region_idx;\n+  log_info(gc)(\"Number of regions: %zu\", _num_regions);\n+}\n+\n+void PSParallelCompactNew::setup_regions_serial() {\n+  _num_regions_serial = last_space_id;\n+  _region_data_array_serial = NEW_C_HEAP_ARRAY(PCRegionData, _num_regions_serial, mtGC);\n+  new (_region_data_array_serial + old_space_id)  PCRegionData(old_space_id, space(old_space_id)->bottom(), space(old_space_id)->top(), space(old_space_id)->end());\n+  new (_region_data_array_serial + eden_space_id) PCRegionData(eden_space_id, space(eden_space_id)->bottom(), space(eden_space_id)->top(), space(eden_space_id)->end());\n+  new (_region_data_array_serial + from_space_id) PCRegionData(from_space_id, space(from_space_id)->bottom(), space(from_space_id)->top(), space(from_space_id)->end());\n+  new (_region_data_array_serial + to_space_id)   PCRegionData(to_space_id, space(to_space_id)->bottom(), space(to_space_id)->top(), space(to_space_id)->end());\n+}\n+\n+bool PSParallelCompactNew::check_maximum_compaction() {\n+\n+  ParallelScavengeHeap* heap = ParallelScavengeHeap::heap();\n+\n+  \/\/ Check System.GC\n+  bool is_max_on_system_gc = UseMaximumCompactionOnSystemGC\n+                          && GCCause::is_user_requested_gc(heap->gc_cause());\n+\n+  \/\/ JVM flags\n+  const uint total_invocations = heap->total_full_collections();\n+  assert(total_invocations >= _maximum_compaction_gc_num, \"sanity\");\n+  const size_t gcs_since_max = total_invocations - _maximum_compaction_gc_num;\n+  const bool is_interval_ended = gcs_since_max > HeapMaximumCompactionInterval;\n+\n+  if (is_max_on_system_gc || is_interval_ended) {\n+    _maximum_compaction_gc_num = total_invocations;\n+    return true;\n+  }\n+\n+  return false;\n+}\n+\n+void PSParallelCompactNew::summary_phase() {\n+  GCTraceTime(Info, gc, phases) tm(\"Summary Phase\", &_gc_timer);\n+\n+  setup_regions_serial();\n+  setup_regions_parallel();\n+\n+#ifndef PRODUCT\n+  for (size_t idx = 0; idx < _num_regions; idx++) {\n+    PCRegionData* rd = &_region_data_array[idx];\n+    log_develop_trace(gc, compaction)(\"Compaction region #%zu: [\" PTR_FORMAT \", \" PTR_FORMAT \")\", rd->idx(), p2i(\n+            rd->bottom()), p2i(rd->end()));\n+  }\n+#endif\n+}\n+\n+\/\/ This method should contain all heap-specific policy for invoking a full\n+\/\/ collection.  invoke_no_policy() will only attempt to compact the heap; it\n+\/\/ will do nothing further.  If we need to bail out for policy reasons, scavenge\n+\/\/ before full gc, or any other specialized behavior, it needs to be added here.\n+\/\/\n+\/\/ Note that this method should only be called from the vm_thread while at a\n+\/\/ safepoint.\n+\/\/\n+\/\/ Note that the all_soft_refs_clear flag in the soft ref policy\n+\/\/ may be true because this method can be called without intervening\n+\/\/ activity.  For example when the heap space is tight and full measure\n+\/\/ are being taken to free space.\n+bool PSParallelCompactNew::invoke(bool clear_all_soft_refs, bool serial) {\n+  assert(SafepointSynchronize::is_at_safepoint(), \"should be at safepoint\");\n+  assert(Thread::current() == (Thread*)VMThread::vm_thread(),\n+         \"should be in vm thread\");\n+\n+  SvcGCMarker sgcm(SvcGCMarker::FULL);\n+  IsSTWGCActiveMark mark;\n+\n+  ParallelScavengeHeap* heap = ParallelScavengeHeap::heap();\n+  clear_all_soft_refs = clear_all_soft_refs\n+                     || heap->soft_ref_policy()->should_clear_all_soft_refs();\n+\n+  return PSParallelCompactNew::invoke_no_policy(clear_all_soft_refs, serial);\n+}\n+\n+\/\/ This method contains no policy. You should probably\n+\/\/ be calling invoke() instead.\n+bool PSParallelCompactNew::invoke_no_policy(bool clear_all_soft_refs, bool serial) {\n+  assert(SafepointSynchronize::is_at_safepoint(), \"must be at a safepoint\");\n+  assert(ref_processor() != nullptr, \"Sanity\");\n+\n+  ParallelScavengeHeap* heap = ParallelScavengeHeap::heap();\n+\n+  GCIdMark gc_id_mark;\n+  _gc_timer.register_gc_start();\n+  _gc_tracer.report_gc_start(heap->gc_cause(), _gc_timer.gc_start());\n+\n+  GCCause::Cause gc_cause = heap->gc_cause();\n+  PSYoungGen* young_gen = ParallelScavengeHeap::young_gen();\n+  PSOldGen* old_gen = ParallelScavengeHeap::old_gen();\n+  PSAdaptiveSizePolicy* size_policy = heap->size_policy();\n+\n+  \/\/ The scope of casr should end after code that can change\n+  \/\/ SoftRefPolicy::_should_clear_all_soft_refs.\n+  ClearedAllSoftRefs casr(clear_all_soft_refs,\n+                          heap->soft_ref_policy());\n+\n+  \/\/ Make sure data structures are sane, make the heap parsable, and do other\n+  \/\/ miscellaneous bookkeeping.\n+  pre_compact();\n+\n+  const PreGenGCValues pre_gc_values = heap->get_pre_gc_values();\n+\n+  {\n+    const uint active_workers =\n+      WorkerPolicy::calc_active_workers(ParallelScavengeHeap::heap()->workers().max_workers(),\n+                                        ParallelScavengeHeap::heap()->workers().active_workers(),\n+                                        Threads::number_of_non_daemon_threads());\n+    ParallelScavengeHeap::heap()->workers().set_active_workers(active_workers);\n+\n+    if (serial || check_maximum_compaction()) {\n+      \/\/ Serial compaction executes the forwarding and compaction phases serially,\n+      \/\/ thus achieving perfect compaction.\n+      \/\/ Marking and ajust-references would still be executed in parallel threads.\n+      _serial = true;\n+    } else {\n+      _serial = false;\n+    }\n+\n+    GCTraceCPUTime tcpu(&_gc_tracer);\n+    GCTraceTime(Info, gc) tm(\"Pause Full\", nullptr, gc_cause, true);\n+\n+    heap->pre_full_gc_dump(&_gc_timer);\n+\n+    TraceCollectorStats tcs(counters());\n+    TraceMemoryManagerStats tms(heap->old_gc_manager(), gc_cause, \"end of major GC\");\n+\n+    if (log_is_enabled(Debug, gc, heap, exit)) {\n+      accumulated_time()->start();\n+    }\n+\n+    \/\/ Let the size policy know we're starting\n+    size_policy->major_collection_begin();\n+\n+#if COMPILER2_OR_JVMCI\n+    DerivedPointerTable::clear();\n+#endif\n+\n+    ref_processor()->start_discovery(clear_all_soft_refs);\n+\n+    ClassUnloadingContext ctx(1 \/* num_nmethod_unlink_workers *\/,\n+                              false \/* unregister_nmethods_during_purge *\/,\n+                              false \/* lock_nmethod_free_separately *\/);\n+\n+    marking_phase(&_gc_tracer);\n+\n+    summary_phase();\n+\n+#if COMPILER2_OR_JVMCI\n+    assert(DerivedPointerTable::is_active(), \"Sanity\");\n+    DerivedPointerTable::set_active(false);\n+#endif\n+\n+    FullGCForwarding::begin();\n+\n+    forward_to_new_addr();\n+\n+    adjust_pointers();\n+\n+    compact();\n+\n+    FullGCForwarding::end();\n+\n+    ParCompactionManagerNew::_preserved_marks_set->restore(&ParallelScavengeHeap::heap()->workers());\n+\n+    \/\/ Reset the mark bitmap, summary data, and do other bookkeeping.  Must be\n+    \/\/ done before resizing.\n+    post_compact();\n+\n+    \/\/ Let the size policy know we're done\n+    size_policy->major_collection_end(old_gen->used_in_bytes(), gc_cause);\n+\n+    if (UseAdaptiveSizePolicy) {\n+      log_debug(gc, ergo)(\"AdaptiveSizeStart: collection: %d \", heap->total_collections());\n+      log_trace(gc, ergo)(\"old_gen_capacity: %zu young_gen_capacity: %zu\",\n+                          old_gen->capacity_in_bytes(), young_gen->capacity_in_bytes());\n+\n+      \/\/ Don't check if the size_policy is ready here.  Let\n+      \/\/ the size_policy check that internally.\n+      if (UseAdaptiveGenerationSizePolicyAtMajorCollection &&\n+          AdaptiveSizePolicy::should_update_promo_stats(gc_cause)) {\n+        \/\/ Swap the survivor spaces if from_space is empty. The\n+        \/\/ resize_young_gen() called below is normally used after\n+        \/\/ a successful young GC and swapping of survivor spaces;\n+        \/\/ otherwise, it will fail to resize the young gen with\n+        \/\/ the current implementation.\n+        if (young_gen->from_space()->is_empty()) {\n+          young_gen->from_space()->clear(SpaceDecorator::Mangle);\n+          young_gen->swap_spaces();\n+        }\n+\n+        \/\/ Calculate optimal free space amounts\n+        assert(young_gen->max_gen_size() >\n+          young_gen->from_space()->capacity_in_bytes() +\n+          young_gen->to_space()->capacity_in_bytes(),\n+          \"Sizes of space in young gen are out-of-bounds\");\n+\n+        size_t young_live = young_gen->used_in_bytes();\n+        size_t eden_live = young_gen->eden_space()->used_in_bytes();\n+        size_t old_live = old_gen->used_in_bytes();\n+        size_t cur_eden = young_gen->eden_space()->capacity_in_bytes();\n+        size_t max_old_gen_size = old_gen->max_gen_size();\n+        size_t max_eden_size = young_gen->max_gen_size() -\n+          young_gen->from_space()->capacity_in_bytes() -\n+          young_gen->to_space()->capacity_in_bytes();\n+\n+        \/\/ Used for diagnostics\n+        size_policy->clear_generation_free_space_flags();\n+\n+        size_policy->compute_generations_free_space(young_live,\n+                                                    eden_live,\n+                                                    old_live,\n+                                                    cur_eden,\n+                                                    max_old_gen_size,\n+                                                    max_eden_size,\n+                                                    true \/* full gc*\/);\n+\n+        size_policy->check_gc_overhead_limit(eden_live,\n+                                             max_old_gen_size,\n+                                             max_eden_size,\n+                                             true \/* full gc*\/,\n+                                             gc_cause,\n+                                             heap->soft_ref_policy());\n+\n+        size_policy->decay_supplemental_growth(true \/* full gc*\/);\n+\n+        heap->resize_old_gen(\n+          size_policy->calculated_old_free_size_in_bytes());\n+\n+        heap->resize_young_gen(size_policy->calculated_eden_size_in_bytes(),\n+                               size_policy->calculated_survivor_size_in_bytes());\n+      }\n+\n+      log_debug(gc, ergo)(\"AdaptiveSizeStop: collection: %d \", heap->total_collections());\n+    }\n+\n+    if (UsePerfData) {\n+      PSGCAdaptivePolicyCounters* const counters = ParallelScavengeHeap::gc_policy_counters();\n+      counters->update_counters();\n+      counters->update_old_capacity(old_gen->capacity_in_bytes());\n+      counters->update_young_capacity(young_gen->capacity_in_bytes());\n+    }\n+\n+    heap->resize_all_tlabs();\n+\n+    \/\/ Resize the metaspace capacity after a collection\n+    MetaspaceGC::compute_new_size();\n+\n+    if (log_is_enabled(Debug, gc, heap, exit)) {\n+      accumulated_time()->stop();\n+    }\n+\n+    heap->print_heap_change(pre_gc_values);\n+\n+    \/\/ Track memory usage and detect low memory\n+    MemoryService::track_memory_usage();\n+    heap->update_counters();\n+\n+    heap->post_full_gc_dump(&_gc_timer);\n+  }\n+\n+  if (VerifyAfterGC && heap->total_collections() >= VerifyGCStartAt) {\n+    Universe::verify(\"After GC\");\n+  }\n+\n+  heap->print_heap_after_gc();\n+  heap->trace_heap_after_gc(&_gc_tracer);\n+\n+  AdaptiveSizePolicyOutput::print(size_policy, heap->total_collections());\n+\n+  _gc_timer.register_gc_end();\n+\n+  _gc_tracer.report_gc_end(_gc_timer.gc_end(), _gc_timer.time_partitions());\n+\n+  return true;\n+}\n+\n+class PCAddThreadRootsMarkingTaskClosureNew : public ThreadClosure {\n+private:\n+  uint _worker_id;\n+\n+public:\n+  explicit PCAddThreadRootsMarkingTaskClosureNew(uint worker_id) : _worker_id(worker_id) { }\n+  void do_thread(Thread* thread) final {\n+    assert(ParallelScavengeHeap::heap()->is_stw_gc_active(), \"called outside gc\");\n+\n+    ResourceMark rm;\n+\n+    ParCompactionManagerNew* cm = ParCompactionManagerNew::gc_thread_compaction_manager(_worker_id);\n+\n+    MarkingNMethodClosure mark_and_push_in_blobs(&cm->_mark_and_push_closure,\n+                                                 !NMethodToOopClosure::FixRelocations,\n+                                                 true \/* keepalive nmethods *\/);\n+\n+    thread->oops_do(&cm->_mark_and_push_closure, &mark_and_push_in_blobs);\n+\n+    \/\/ Do the real work\n+    cm->follow_marking_stacks();\n+  }\n+};\n+\n+void steal_marking_work_new(TaskTerminator& terminator, uint worker_id) {\n+  assert(ParallelScavengeHeap::heap()->is_stw_gc_active(), \"called outside gc\");\n+\n+  ParCompactionManagerNew* cm =\n+    ParCompactionManagerNew::gc_thread_compaction_manager(worker_id);\n+\n+  do {\n+    ScannerTask task;\n+    if (ParCompactionManagerNew::steal(worker_id, task)) {\n+      cm->follow_contents(task, true);\n+    }\n+    cm->follow_marking_stacks();\n+  } while (!terminator.offer_termination());\n+}\n+\n+class MarkFromRootsTaskNew : public WorkerTask {\n+  StrongRootsScope _strong_roots_scope; \/\/ needed for Threads::possibly_parallel_threads_do\n+  OopStorageSetStrongParState<false \/* concurrent *\/, false \/* is_const *\/> _oop_storage_set_par_state;\n+  TaskTerminator _terminator;\n+  uint _active_workers;\n+\n+public:\n+  explicit MarkFromRootsTaskNew(uint active_workers) :\n+      WorkerTask(\"MarkFromRootsTaskNew\"),\n+      _strong_roots_scope(active_workers),\n+      _terminator(active_workers, ParCompactionManagerNew::marking_stacks()),\n+      _active_workers(active_workers) {}\n+\n+  void work(uint worker_id) final {\n+    ParCompactionManagerNew* cm = ParCompactionManagerNew::gc_thread_compaction_manager(worker_id);\n+    {\n+      CLDToOopClosure cld_closure(&cm->_mark_and_push_closure, ClassLoaderData::_claim_stw_fullgc_mark);\n+      ClassLoaderDataGraph::always_strong_cld_do(&cld_closure);\n+\n+      \/\/ Do the real work\n+      cm->follow_marking_stacks();\n+    }\n+\n+    {\n+      PCAddThreadRootsMarkingTaskClosureNew closure(worker_id);\n+      Threads::possibly_parallel_threads_do(_active_workers > 1 \/* is_par *\/, &closure);\n+    }\n+\n+    \/\/ Mark from OopStorages\n+    {\n+      _oop_storage_set_par_state.oops_do(&cm->_mark_and_push_closure);\n+      \/\/ Do the real work\n+      cm->follow_marking_stacks();\n+    }\n+\n+    if (_active_workers > 1) {\n+      steal_marking_work_new(_terminator, worker_id);\n+    }\n+  }\n+};\n+\n+class ParallelCompactRefProcProxyTaskNew : public RefProcProxyTask {\n+  TaskTerminator _terminator;\n+\n+public:\n+  explicit ParallelCompactRefProcProxyTaskNew(uint max_workers)\n+    : RefProcProxyTask(\"ParallelCompactRefProcProxyTaskNew\", max_workers),\n+      _terminator(_max_workers, ParCompactionManagerNew::marking_stacks()) {}\n+\n+  void work(uint worker_id) final {\n+    assert(worker_id < _max_workers, \"sanity\");\n+    ParCompactionManagerNew* cm = (_tm == RefProcThreadModel::Single) ? ParCompactionManagerNew::get_vmthread_cm() : ParCompactionManagerNew::gc_thread_compaction_manager(worker_id);\n+    BarrierEnqueueDiscoveredFieldClosure enqueue;\n+    ParCompactionManagerNew::FollowStackClosure complete_gc(cm, (_tm == RefProcThreadModel::Single) ? nullptr : &_terminator, worker_id);\n+    _rp_task->rp_work(worker_id, PSParallelCompactNew::is_alive_closure(), &cm->_mark_and_push_closure, &enqueue, &complete_gc);\n+  }\n+\n+  void prepare_run_task_hook() final {\n+    _terminator.reset_for_reuse(_queue_count);\n+  }\n+};\n+\n+void PSParallelCompactNew::marking_phase(ParallelOldTracer *gc_tracer) {\n+  \/\/ Recursively traverse all live objects and mark them\n+  GCTraceTime(Info, gc, phases) tm(\"Marking Phase\", &_gc_timer);\n+\n+  uint active_gc_threads = ParallelScavengeHeap::heap()->workers().active_workers();\n+\n+  ClassLoaderDataGraph::verify_claimed_marks_cleared(ClassLoaderData::_claim_stw_fullgc_mark);\n+  {\n+    GCTraceTime(Debug, gc, phases) pm_tm(\"Par Mark\", &_gc_timer);\n+\n+    MarkFromRootsTaskNew task(active_gc_threads);\n+    ParallelScavengeHeap::heap()->workers().run_task(&task);\n+  }\n+\n+  \/\/ Process reference objects found during marking\n+  {\n+    GCTraceTime(Debug, gc, phases) rp_tm(\"Reference Processing\", &_gc_timer);\n+\n+    ReferenceProcessorStats stats;\n+    ReferenceProcessorPhaseTimes pt(&_gc_timer, ref_processor()->max_num_queues());\n+\n+    ref_processor()->set_active_mt_degree(active_gc_threads);\n+    ParallelCompactRefProcProxyTaskNew task(ref_processor()->max_num_queues());\n+    stats = ref_processor()->process_discovered_references(task, pt);\n+\n+    gc_tracer->report_gc_reference_stats(stats);\n+    pt.print_all_references();\n+  }\n+\n+  \/\/ This is the point where the entire marking should have completed.\n+  ParCompactionManagerNew::verify_all_marking_stack_empty();\n+\n+  {\n+    GCTraceTime(Debug, gc, phases) wp_tm(\"Weak Processing\", &_gc_timer);\n+    WeakProcessor::weak_oops_do(&ParallelScavengeHeap::heap()->workers(),\n+                                is_alive_closure(),\n+                                &do_nothing_cl,\n+                                1);\n+  }\n+\n+  {\n+    GCTraceTime(Debug, gc, phases) tm_m(\"Class Unloading\", &_gc_timer);\n+\n+    ClassUnloadingContext* ctx = ClassUnloadingContext::context();\n+\n+    bool unloading_occurred;\n+    {\n+      CodeCache::UnlinkingScope scope(is_alive_closure());\n+\n+      \/\/ Follow system dictionary roots and unload classes.\n+      unloading_occurred = SystemDictionary::do_unloading(&_gc_timer);\n+\n+      \/\/ Unload nmethods.\n+      CodeCache::do_unloading(unloading_occurred);\n+    }\n+\n+    {\n+      GCTraceTime(Debug, gc, phases) t(\"Purge Unlinked NMethods\", gc_timer());\n+      \/\/ Release unloaded nmethod's memory.\n+      ctx->purge_nmethods();\n+    }\n+    {\n+      GCTraceTime(Debug, gc, phases) ur(\"Unregister NMethods\", &_gc_timer);\n+      ParallelScavengeHeap::heap()->prune_unlinked_nmethods();\n+    }\n+    {\n+      GCTraceTime(Debug, gc, phases) t(\"Free Code Blobs\", gc_timer());\n+      ctx->free_nmethods();\n+    }\n+\n+    \/\/ Prune dead klasses from subklass\/sibling\/implementor lists.\n+    Klass::clean_weak_klass_links(unloading_occurred);\n+\n+    \/\/ Clean JVMCI metadata handles.\n+    JVMCI_ONLY(JVMCI::do_unloading(unloading_occurred));\n+  }\n+\n+  {\n+    GCTraceTime(Debug, gc, phases) roc_tm(\"Report Object Count\", &_gc_timer);\n+    _gc_tracer.report_object_count_after_gc(is_alive_closure(), &ParallelScavengeHeap::heap()->workers());\n+  }\n+#if TASKQUEUE_STATS\n+  ParCompactionManagerNew::print_and_reset_taskqueue_stats();\n+#endif\n+}\n+\n+void PSParallelCompactNew::adjust_pointers_in_spaces(uint worker_id) {\n+  auto start_time = Ticks::now();\n+  for (size_t i = 0; i < _num_regions; i++) {\n+    PCRegionData* region = &_region_data_array[i];\n+    if (!region->claim()) {\n+      continue;\n+    }\n+    log_trace(gc, compaction)(\"Adjusting pointers in region: %zu (worker_id: %u)\", region->idx(), worker_id);\n+    HeapWord* end = region->top();\n+    HeapWord* current = _mark_bitmap.find_obj_beg(region->bottom(), end);\n+    while (current < end) {\n+      assert(_mark_bitmap.is_marked(current), \"must be marked\");\n+      oop obj = cast_to_oop(current);\n+      size_t size = obj->size();\n+      obj->oop_iterate(&pc_adjust_pointer_closure);\n+      current = _mark_bitmap.find_obj_beg(current + size, end);\n+    }\n+  }\n+  log_trace(gc, phases)(\"adjust_pointers_in_spaces worker %u: %.3f ms\", worker_id, (Ticks::now() - start_time).seconds() * 1000);\n+}\n+\n+class PSAdjustTaskNew final : public WorkerTask {\n+  SubTasksDone                               _sub_tasks;\n+  WeakProcessor::Task                        _weak_proc_task;\n+  OopStorageSetStrongParState<false, false>  _oop_storage_iter;\n+  uint                                       _nworkers;\n+\n+  enum PSAdjustSubTask {\n+    PSAdjustSubTask_code_cache,\n+\n+    PSAdjustSubTask_num_elements\n+  };\n+\n+public:\n+  explicit PSAdjustTaskNew(uint nworkers) :\n+    WorkerTask(\"PSAdjust task\"),\n+    _sub_tasks(PSAdjustSubTask_num_elements),\n+    _weak_proc_task(nworkers),\n+    _nworkers(nworkers) {\n+\n+    ClassLoaderDataGraph::verify_claimed_marks_cleared(ClassLoaderData::_claim_stw_fullgc_adjust);\n+    if (nworkers > 1) {\n+      Threads::change_thread_claim_token();\n+    }\n+  }\n+\n+  ~PSAdjustTaskNew() {\n+    Threads::assert_all_threads_claimed();\n+  }\n+\n+  void work(uint worker_id) final {\n+    ParCompactionManagerNew* cm = ParCompactionManagerNew::gc_thread_compaction_manager(worker_id);\n+    cm->preserved_marks()->adjust_during_full_gc();\n+    {\n+      \/\/ adjust pointers in all spaces\n+      PSParallelCompactNew::adjust_pointers_in_spaces(worker_id);\n+    }\n+    {\n+      ResourceMark rm;\n+      Threads::possibly_parallel_oops_do(_nworkers > 1, &pc_adjust_pointer_closure, nullptr);\n+    }\n+    _oop_storage_iter.oops_do(&pc_adjust_pointer_closure);\n+    {\n+      CLDToOopClosure cld_closure(&pc_adjust_pointer_closure, ClassLoaderData::_claim_stw_fullgc_adjust);\n+      ClassLoaderDataGraph::cld_do(&cld_closure);\n+    }\n+    {\n+      AlwaysTrueClosure always_alive;\n+      _weak_proc_task.work(worker_id, &always_alive, &pc_adjust_pointer_closure);\n+    }\n+    if (_sub_tasks.try_claim_task(PSAdjustSubTask_code_cache)) {\n+      NMethodToOopClosure adjust_code(&pc_adjust_pointer_closure, NMethodToOopClosure::FixRelocations);\n+      CodeCache::nmethods_do(&adjust_code);\n+    }\n+    _sub_tasks.all_tasks_claimed();\n+  }\n+};\n+\n+void PSParallelCompactNew::adjust_pointers() {\n+  \/\/ Adjust the pointers to reflect the new locations\n+  GCTraceTime(Info, gc, phases) tm(\"Adjust Pointers\", &_gc_timer);\n+  uint num_workers = ParallelScavengeHeap::heap()->workers().active_workers();\n+  PSAdjustTaskNew task(num_workers);\n+  ParallelScavengeHeap::heap()->workers().run_task(&task);\n+}\n+\n+void PSParallelCompactNew::forward_to_new_addr() {\n+  GCTraceTime(Info, gc, phases) tm(\"Forward\", &_gc_timer);\n+  uint num_workers = get_num_workers();\n+  _per_worker_region_data = NEW_C_HEAP_ARRAY(PCRegionData*, num_workers, mtGC);\n+  for (uint i = 0; i < num_workers; i++) {\n+    _per_worker_region_data[i] = nullptr;\n+  }\n+\n+  class ForwardState {\n+    uint _worker_id;\n+    PCRegionData* _compaction_region;\n+    HeapWord* _compaction_point;\n+\n+    void ensure_compaction_point() {\n+      if (_compaction_point == nullptr) {\n+        assert(_compaction_region == nullptr, \"invariant\");\n+        _compaction_region = _per_worker_region_data[_worker_id];\n+        assert(_compaction_region != nullptr, \"invariant\");\n+        _compaction_point = _compaction_region->bottom();\n+      }\n+    }\n+  public:\n+    explicit ForwardState(uint worker_id) :\n+            _worker_id(worker_id),\n+            _compaction_region(nullptr),\n+            _compaction_point(nullptr) {\n+    }\n+\n+    size_t available() const {\n+      return pointer_delta(_compaction_region->end(), _compaction_point);\n+    }\n+\n+    void forward_objs_in_region(ParCompactionManagerNew* cm, PCRegionData* region) {\n+      ensure_compaction_point();\n+      HeapWord* end = region->end();\n+      HeapWord* current = _mark_bitmap.find_obj_beg(region->bottom(), end);\n+      while (current < end) {\n+        assert(_mark_bitmap.is_marked(current), \"must be marked\");\n+        oop obj = cast_to_oop(current);\n+        assert(region->contains(obj), \"object must not cross region boundary: obj: \" PTR_FORMAT \", obj_end: \" PTR_FORMAT \", region start: \" PTR_FORMAT \", region end: \" PTR_FORMAT, p2i(obj), p2i(cast_from_oop<HeapWord*>(obj) + obj->size()), p2i(region->bottom()), p2i(region->end()));\n+        size_t old_size = obj->size();\n+        size_t new_size = obj->copy_size(old_size, obj->mark());\n+        size_t size = (current == _compaction_point) ? old_size : new_size;\n+        while (size > available()) {\n+          _compaction_region->set_new_top(_compaction_point);\n+          _compaction_region = _compaction_region->local_next();\n+          assert(_compaction_region != nullptr, \"must find a compaction region\");\n+          _compaction_point = _compaction_region->bottom();\n+          size = (current == _compaction_point) ? old_size : new_size;\n+        }\n+        \/\/log_develop_trace(gc, compaction)(\"Forwarding obj: \" PTR_FORMAT \", to: \" PTR_FORMAT, p2i(obj), p2i(_compaction_point));\n+        if (current != _compaction_point) {\n+          cm->preserved_marks()->push_if_necessary(obj, obj->mark());\n+          FullGCForwarding::forward_to(obj, cast_to_oop(_compaction_point));\n+        }\n+        _compaction_point += size;\n+        assert(_compaction_point <= _compaction_region->end(), \"object must fit in region\");\n+        current += old_size;\n+        assert(current <= end, \"object must not cross region boundary\");\n+        current = _mark_bitmap.find_obj_beg(current, end);\n+      }\n+    }\n+    void finish() {\n+      if (_compaction_region != nullptr) {\n+        _compaction_region->set_new_top(_compaction_point);\n+      }\n+    }\n+  };\n+\n+  struct ForwardTask final : public WorkerTask {\n+    ForwardTask() : WorkerTask(\"PSForward task\") {}\n+\n+    void work(uint worker_id) override {\n+      ParCompactionManagerNew* cm = ParCompactionManagerNew::gc_thread_compaction_manager(worker_id);\n+      ForwardState state(worker_id);\n+      PCRegionData** last_link = &_per_worker_region_data[worker_id];\n+      size_t idx = worker_id;\n+      uint num_workers = get_num_workers();\n+      size_t num_regions = get_num_regions();\n+      PCRegionData* region_data_array = get_region_data_array();\n+      while (idx < num_regions) {\n+        PCRegionData* region = region_data_array + idx;\n+        *last_link = region;\n+        last_link = region->local_next_addr();\n+        state.forward_objs_in_region(cm, region);\n+        idx += num_workers;\n+      }\n+      state.finish();\n+    }\n+  } task;\n+\n+  uint par_workers = ParallelScavengeHeap::heap()->workers().active_workers();\n+  ParallelScavengeHeap::heap()->workers().set_active_workers(num_workers);\n+  ParallelScavengeHeap::heap()->workers().run_task(&task);\n+  ParallelScavengeHeap::heap()->workers().set_active_workers(par_workers);\n+\n+#ifndef PRODUCT\n+  for (uint wid = 0; wid < num_workers; wid++) {\n+    for (PCRegionData* rd = _per_worker_region_data[wid]; rd != nullptr; rd = rd->local_next()) {\n+      log_develop_trace(gc, compaction)(\"Per worker compaction region, worker: %d, #%zu: [\" PTR_FORMAT \", \" PTR_FORMAT \"), new_top: \" PTR_FORMAT, wid, rd->idx(),\n+                                        p2i(rd->bottom()), p2i(rd->end()), p2i(rd->new_top()));\n+    }\n+  }\n+#endif\n+}\n+\n+void PSParallelCompactNew::compact() {\n+  GCTraceTime(Info, gc, phases) tm(\"Compaction Phase\", &_gc_timer);\n+  class CompactTask final : public WorkerTask {\n+    static void compact_region(PCRegionData* region) {\n+      HeapWord* bottom = region->bottom();\n+      HeapWord* end = region->top();\n+      if (bottom == end) {\n+        return;\n+      }\n+      HeapWord* current = _mark_bitmap.find_obj_beg(bottom, end);\n+      while (current < end) {\n+        oop obj = cast_to_oop(current);\n+        size_t size = obj->size();\n+        if (FullGCForwarding::is_forwarded(obj)) {\n+          oop fwd = FullGCForwarding::forwardee(obj);\n+          auto* dst = cast_from_oop<HeapWord*>(fwd);\n+          ObjectStartArray* sa = start_array(space_id(dst));\n+          if (sa != nullptr) {\n+            assert(dst != current, \"expect moving object\");\n+            size_t new_words = obj->copy_size(size, obj->mark());\n+            sa->update_for_block(dst, dst + new_words);\n+          }\n+\n+          Copy::aligned_conjoint_words(current, dst, size);\n+          fwd->init_mark();\n+          fwd->initialize_hash_if_necessary(obj);\n+        } else {\n+          \/\/ The start_array must be updated even if the object is not moving.\n+          ObjectStartArray* sa = start_array(space_id(current));\n+          if (sa != nullptr) {\n+            sa->update_for_block(current, current + size);\n+          }\n+        }\n+        current = _mark_bitmap.find_obj_beg(current + size, end);\n+      }\n+    }\n+  public:\n+    explicit CompactTask() : WorkerTask(\"PSCompact task\") {}\n+    void work(uint worker_id) override {\n+      PCRegionData* region = _per_worker_region_data[worker_id];\n+      while (region != nullptr) {\n+        log_trace(gc)(\"Compact worker: %u, compacting region: %zu\", worker_id, region->idx());\n+        compact_region(region);\n+        region = region->local_next();\n+      }\n+    }\n+  } task;\n+\n+  uint num_workers = get_num_workers();\n+  uint par_workers = ParallelScavengeHeap::heap()->workers().active_workers();\n+  ParallelScavengeHeap::heap()->workers().set_active_workers(num_workers);\n+  ParallelScavengeHeap::heap()->workers().run_task(&task);\n+  ParallelScavengeHeap::heap()->workers().set_active_workers(par_workers);\n+}\n+\n+\/\/ Return the SpaceId for the space containing addr.  If addr is not in the\n+\/\/ heap, last_space_id is returned.  In debug mode it expects the address to be\n+\/\/ in the heap and asserts such.\n+PSParallelCompactNew::SpaceId PSParallelCompactNew::space_id(HeapWord* addr) {\n+  assert(ParallelScavengeHeap::heap()->is_in_reserved(addr), \"addr not in the heap\");\n+\n+  for (unsigned int id = old_space_id; id < last_space_id; ++id) {\n+    if (_space_info[id].space()->contains(addr)) {\n+      return SpaceId(id);\n+    }\n+  }\n+\n+  assert(false, \"no space contains the addr\");\n+  return last_space_id;\n+}\n","filename":"src\/hotspot\/share\/gc\/parallel\/psParallelCompactNew.cpp","additions":1147,"deletions":0,"binary":false,"changes":1147,"status":"added"},{"patch":"@@ -691,7 +691,1 @@\n-      if (obj->is_self_forwarded()) {\n-        obj->unset_self_forwarded();\n-      } else if (obj->is_forwarded()) {\n-        \/\/ To restore the klass-bits in the header.\n-        \/\/ Needed for object iteration to work properly.\n-        obj->set_mark(obj->forwardee()->prototype_mark());\n-      }\n+      obj->reset_forwarded();\n@@ -728,1 +722,3 @@\n-  size_t s = old->size();\n+  size_t old_size = old->size();\n+  size_t s = old->copy_size(old_size, old->mark());\n+\n@@ -753,1 +749,1 @@\n-  Copy::aligned_disjoint_words(cast_from_oop<HeapWord*>(old), cast_from_oop<HeapWord*>(obj), s);\n+  Copy::aligned_disjoint_words(cast_from_oop<HeapWord*>(old), cast_from_oop<HeapWord*>(obj), old_size);\n@@ -763,0 +759,2 @@\n+  obj->initialize_hash_if_necessary(old);\n+\n","filename":"src\/hotspot\/share\/gc\/serial\/defNewGeneration.cpp","additions":7,"deletions":9,"binary":false,"changes":16,"status":"modified"},{"patch":"@@ -390,1 +390,1 @@\n-  assert(obj_size == obj->size(), \"bad obj_size passed in\");\n+  assert(obj_size == obj->size() || UseCompactObjectHeaders, \"bad obj_size passed in\");\n","filename":"src\/hotspot\/share\/gc\/serial\/tenuredGeneration.cpp","additions":1,"deletions":1,"binary":false,"changes":2,"status":"modified"},{"patch":"@@ -707,1 +707,2 @@\n-  \/\/ 8  - 32-bit VM or 64-bit VM, compact headers\n+  \/\/ 4  - compact headers\n+  \/\/ 8  - 32-bit VM\n@@ -712,1 +713,0 @@\n-    assert(!UseCompactObjectHeaders, \"\");\n@@ -717,2 +717,6 @@\n-      \/\/ Include klass to copy by 8 bytes words.\n-      base_off = instanceOopDesc::klass_offset_in_bytes();\n+      if (UseCompactObjectHeaders) {\n+        base_off = 0; \/* FIXME *\/\n+      } else {\n+        \/\/ Include klass to copy by 8 bytes words.\n+        base_off = instanceOopDesc::klass_offset_in_bytes();\n+      }\n@@ -720,1 +724,1 @@\n-    assert(base_off % BytesPerLong == 0, \"expect 8 bytes alignment\");\n+    assert(base_off % BytesPerLong == 0 || UseCompactObjectHeaders, \"expect 8 bytes alignment\");\n","filename":"src\/hotspot\/share\/gc\/shared\/c2\/barrierSetC2.cpp","additions":9,"deletions":5,"binary":false,"changes":14,"status":"modified"},{"patch":"@@ -27,1 +27,0 @@\n-#include \"gc\/shared\/fullGCForwarding.hpp\"\n@@ -195,2 +194,0 @@\n-\n-  FullGCForwarding::initialize_flags(MaxHeapSize);\n","filename":"src\/hotspot\/share\/gc\/shenandoah\/shenandoahArguments.cpp","additions":0,"deletions":3,"binary":false,"changes":3,"status":"modified"},{"patch":"@@ -248,0 +248,2 @@\n+    FullGCForwarding::begin();\n+\n@@ -257,0 +259,2 @@\n+\n+    FullGCForwarding::end();\n@@ -354,1 +358,3 @@\n-    size_t obj_size = p->size();\n+    size_t old_size = p->size();\n+    size_t new_size = p->copy_size(old_size, p->mark());\n+    size_t obj_size = _compact_point == cast_from_oop<HeapWord*>(p) ? old_size : new_size;\n@@ -372,0 +378,1 @@\n+      obj_size = _compact_point == cast_from_oop<HeapWord*>(p) ? old_size : new_size;\n@@ -891,0 +898,1 @@\n+      new_obj->initialize_hash_if_necessary(p);\n","filename":"src\/hotspot\/share\/gc\/shenandoah\/shenandoahFullGC.cpp","additions":9,"deletions":1,"binary":false,"changes":10,"status":"modified"},{"patch":"@@ -281,1 +281,2 @@\n-  size_t obj_size = p->size();\n+  size_t old_size = p->size();\n+  size_t new_size = p->copy_size(old_size, p->mark());\n@@ -288,1 +289,1 @@\n-    if ((_old_to_region != nullptr) && (_old_compact_point + obj_size > _old_to_region->end())) {\n+    if ((_old_to_region != nullptr) && (_old_compact_point + new_size > _old_to_region->end())) {\n@@ -310,0 +311,1 @@\n+    size_t obj_size = _old_compact_point == cast_from_oop<HeapWord*>(p) ? old_size : new_size;\n@@ -334,0 +336,1 @@\n+      obj_size = _old_compact_point == cast_from_oop<HeapWord*>(p) ? old_size : new_size;\n@@ -357,0 +360,1 @@\n+    size_t obj_size = _young_compact_point == cast_from_oop<HeapWord*>(p) ? old_size : new_size;\n@@ -380,0 +384,1 @@\n+      obj_size = _young_compact_point == cast_from_oop<HeapWord*>(p) ? old_size : new_size;\n","filename":"src\/hotspot\/share\/gc\/shenandoah\/shenandoahGenerationalFullGC.cpp","additions":7,"deletions":2,"binary":false,"changes":9,"status":"modified"},{"patch":"@@ -1365,1 +1365,7 @@\n-  size_t size = ShenandoahForwarding::size(p);\n+\n+  markWord mark = p->mark();\n+  if (ShenandoahForwarding::is_forwarded(mark)) {\n+    return ShenandoahForwarding::get_forwardee(p);\n+  }\n+  size_t old_size = ShenandoahForwarding::size(p);\n+  size_t size = p->copy_size(old_size, mark);\n@@ -1395,1 +1401,1 @@\n-  Copy::aligned_disjoint_words(cast_from_oop<HeapWord*>(p), copy, size);\n+  Copy::aligned_disjoint_words(cast_from_oop<HeapWord*>(p), copy, old_size);\n@@ -1402,0 +1408,1 @@\n+    copy_val->initialize_hash_if_necessary(p);\n","filename":"src\/hotspot\/share\/gc\/shenandoah\/shenandoahHeap.cpp","additions":9,"deletions":2,"binary":false,"changes":11,"status":"modified"},{"patch":"@@ -326,1 +326,2 @@\n-  const size_t size = ZUtils::object_size(from_addr);\n+  const size_t old_size = ZUtils::object_size(from_addr);\n+  const size_t size = ZUtils::copy_size(from_addr, old_size);\n@@ -336,0 +337,1 @@\n+  assert(to_addr != from_addr, \"addresses must be different\");\n@@ -338,1 +340,2 @@\n-  ZUtils::object_copy_disjoint(from_addr, to_addr, size);\n+  ZUtils::object_copy_disjoint(from_addr, to_addr, old_size);\n+  ZUtils::initialize_hash_if_necessary(to_addr, from_addr);\n@@ -590,1 +593,1 @@\n-  zaddress try_relocate_object_inner(zaddress from_addr) {\n+  zaddress try_relocate_object_inner(zaddress from_addr, size_t old_size) {\n@@ -592,2 +595,4 @@\n-\n-    const size_t size = ZUtils::object_size(from_addr);\n+    zoffset_end from_offset = to_zoffset_end(ZAddress::offset(from_addr));\n+    zoffset_end top = to_page != nullptr ? to_page->top() : to_zoffset_end(0);\n+    const size_t new_size = ZUtils::copy_size(from_addr, old_size);\n+    const size_t size = top == from_offset ? old_size : new_size;\n@@ -612,0 +617,4 @@\n+    if (old_size != new_size && ((top == from_offset) != (allocated_addr == from_addr))) {\n+      _allocator->undo_alloc_object(to_page, allocated_addr, size);\n+      return zaddress::null;\n+    }\n@@ -615,2 +624,2 @@\n-    if (_forwarding->in_place_relocation() && allocated_addr + size > from_addr) {\n-      ZUtils::object_copy_conjoint(from_addr, allocated_addr, size);\n+    if (_forwarding->in_place_relocation() && allocated_addr + old_size > from_addr) {\n+      ZUtils::object_copy_conjoint(from_addr, allocated_addr, old_size);\n@@ -618,1 +627,4 @@\n-      ZUtils::object_copy_disjoint(from_addr, allocated_addr, size);\n+      ZUtils::object_copy_disjoint(from_addr, allocated_addr, old_size);\n+    }\n+    if (from_addr != allocated_addr) {\n+      ZUtils::initialize_hash_if_necessary(allocated_addr, from_addr);\n@@ -632,1 +644,1 @@\n-  void update_remset_old_to_old(zaddress from_addr, zaddress to_addr) const {\n+  void update_remset_old_to_old(zaddress from_addr, zaddress to_addr, size_t size) const {\n@@ -654,4 +666,2 @@\n-\n-    \/\/ Read the size from the to-object, since the from-object\n-    \/\/ could have been overwritten during in-place relocation.\n-    const size_t size = ZUtils::object_size(to_addr);\n+    assert(size <= ZUtils::object_size(to_addr), \"old size must be <= new size\");\n+    assert(size > 0, \"size must be set\");\n@@ -782,1 +792,1 @@\n-  void update_remset_for_fields(zaddress from_addr, zaddress to_addr) const {\n+  void update_remset_for_fields(zaddress from_addr, zaddress to_addr, size_t size) const {\n@@ -790,1 +800,1 @@\n-      update_remset_old_to_old(from_addr, to_addr);\n+      update_remset_old_to_old(from_addr, to_addr, size);\n@@ -799,1 +809,2 @@\n-    const zaddress to_addr = try_relocate_object_inner(from_addr);\n+    size_t size = ZUtils::object_size(from_addr);\n+    const zaddress to_addr = try_relocate_object_inner(from_addr, size);\n@@ -805,1 +816,1 @@\n-    update_remset_for_fields(from_addr, to_addr);\n+    update_remset_for_fields(from_addr, to_addr, size);\n","filename":"src\/hotspot\/share\/gc\/z\/zRelocate.cpp","additions":28,"deletions":17,"binary":false,"changes":45,"status":"modified"},{"patch":"@@ -453,1 +453,1 @@\n-  if (offset == oopDesc::klass_offset_in_bytes()) {\n+  if (offset == 1 \/*oopDesc::klass_offset_in_bytes()*\/) {\n","filename":"src\/hotspot\/share\/jvmci\/jvmciCompilerToVM.cpp","additions":1,"deletions":1,"binary":false,"changes":2,"status":"modified"},{"patch":"@@ -151,1 +151,1 @@\n-  oopDesc_klass_offset_in_bytes = oopDesc::klass_offset_in_bytes();\n+  oopDesc_klass_offset_in_bytes = 1; \/\/oopDesc::klass_offset_in_bytes();\n","filename":"src\/hotspot\/share\/jvmci\/jvmciCompilerToVMInit.cpp","additions":1,"deletions":1,"binary":false,"changes":2,"status":"modified"},{"patch":"@@ -98,0 +98,1 @@\n+  NOT_PRODUCT(LOG_TAG(ihash)) \\\n","filename":"src\/hotspot\/share\/logging\/logTag.hpp","additions":1,"deletions":0,"binary":false,"changes":1,"status":"modified"},{"patch":"@@ -568,1 +568,1 @@\n-          oop m = java_lang_Class::create_basic_type_mirror(type2name(bt), bt, CHECK);\n+          oop m = java_lang_Class::create_basic_type_mirror(type2name(bt), bt, false, CHECK);\n","filename":"src\/hotspot\/share\/memory\/universe.cpp","additions":1,"deletions":1,"binary":false,"changes":2,"status":"modified"},{"patch":"@@ -79,0 +79,4 @@\n+  \/\/ There is no technical reason preventing us from using other klass pointer bit lengths,\n+  \/\/ but it should be a deliberate choice\n+  ASSERT_HERE(_narrow_klass_pointer_bits == 32 || _narrow_klass_pointer_bits == 19);\n+\n@@ -221,1 +225,6 @@\n-    \/\/ In compact object header mode, with 22-bit narrowKlass, we don't attempt for\n+    \/\/ This handles the case that we - experimentally - reduce the number of\n+    \/\/ class pointer bits further, such that (shift + num bits) < 32.\n+    assert(len <= (size_t)nth_bit(narrow_klass_pointer_bits() + max_shift()),\n+           \"klass range size exceeds encoding, len: %zu, narrow_klass_pointer_bits: %d, max_shift: %d\", len, narrow_klass_pointer_bits(), max_shift());\n+\n+    \/\/ In compact object header mode, with 19-bit narrowKlass, we don't attempt for\n","filename":"src\/hotspot\/share\/oops\/compressedKlass.cpp","additions":10,"deletions":1,"binary":false,"changes":11,"status":"modified"},{"patch":"@@ -111,1 +111,1 @@\n-  static constexpr int narrow_klass_pointer_bits_coh = 22;\n+  static constexpr int narrow_klass_pointer_bits_coh = 19;\n","filename":"src\/hotspot\/share\/oops\/compressedKlass.hpp","additions":1,"deletions":1,"binary":false,"changes":2,"status":"modified"},{"patch":"@@ -549,1 +549,2 @@\n-  _init_thread(nullptr)\n+  _init_thread(nullptr),\n+  _hash_offset(parser.hash_offset())\n@@ -3809,1 +3810,1 @@\n-  st->print_cr(BULLET\"---- fields (total size %zu words):\", oop_size(obj));\n+  st->print_cr(BULLET\"---- fields (total size %zu words):\", oop_size(obj, obj->mark()));\n","filename":"src\/hotspot\/share\/oops\/instanceKlass.cpp","additions":3,"deletions":2,"binary":false,"changes":5,"status":"modified"},{"patch":"@@ -57,0 +57,1 @@\n+#include \"utilities\/numberSeq.hpp\"\n@@ -1339,0 +1340,13 @@\n+\n+static int expanded = 0;\n+static int not_expanded = 0;\n+static NumberSeq seq = NumberSeq();\n+\n+bool Klass::expand_for_hash(oop obj, markWord m) const {\n+  assert(UseCompactObjectHeaders, \"only with compact i-hash\");\n+  {\n+    ResourceMark rm;\n+    assert((size_t)hash_offset_in_bytes(obj,m ) <= (obj->base_size_given_klass(m, this) * HeapWordSize), \"hash offset must be eq or lt base size: hash offset: %d, base size: %zu, class-name: %s\", hash_offset_in_bytes(obj, m), obj->base_size_given_klass(m, this) * HeapWordSize, external_name());\n+  }\n+  return obj->base_size_given_klass(m, this) * HeapWordSize - hash_offset_in_bytes(obj, m) < (int)sizeof(uint32_t);\n+}\n","filename":"src\/hotspot\/share\/oops\/klass.cpp","additions":14,"deletions":0,"binary":false,"changes":14,"status":"modified"},{"patch":"@@ -641,1 +641,2 @@\n-  virtual size_t oop_size(oop obj) const = 0;\n+  virtual size_t oop_size(oop obj, markWord mark) const = 0;\n+  size_t oop_size(oop obj) const;\n@@ -791,0 +792,4 @@\n+  virtual int hash_offset_in_bytes(oop obj, markWord m) const = 0;\n+  static int kind_offset_in_bytes() { return (int)offset_of(Klass, _kind); }\n+\n+  bool expand_for_hash(oop obj, markWord m) const;\n","filename":"src\/hotspot\/share\/oops\/klass.hpp","additions":6,"deletions":1,"binary":false,"changes":7,"status":"modified"},{"patch":"@@ -144,1 +144,1 @@\n-size_t ObjArrayKlass::oop_size(oop obj) const {\n+size_t ObjArrayKlass::oop_size(oop obj, markWord mark) const {\n@@ -149,1 +149,2 @@\n-  return objArrayOop(obj)->object_size();\n+  int length = LP64_ONLY(UseCompactObjectHeaders ? mark.array_length() :) objArrayOop(obj)->length();\n+  return objArrayOop(obj)->object_size(length);\n","filename":"src\/hotspot\/share\/oops\/objArrayKlass.cpp","additions":3,"deletions":2,"binary":false,"changes":5,"status":"modified"},{"patch":"@@ -176,1 +176,1 @@\n-size_t TypeArrayKlass::oop_size(oop obj) const {\n+size_t TypeArrayKlass::oop_size(oop obj, markWord mark) const {\n@@ -180,1 +180,2 @@\n-  return t->object_size(this);\n+  int length = LP64_ONLY(UseCompactObjectHeaders ? mark.array_length() :) t->length();\n+  return t->object_size(this, length);\n","filename":"src\/hotspot\/share\/oops\/typeArrayKlass.cpp","additions":3,"deletions":2,"binary":false,"changes":5,"status":"modified"},{"patch":"@@ -66,1 +66,1 @@\n-  size_t oop_size(oop obj) const;\n+  size_t oop_size(oop obj, markWord mark) const;\n","filename":"src\/hotspot\/share\/oops\/typeArrayKlass.hpp","additions":1,"deletions":1,"binary":false,"changes":2,"status":"modified"},{"patch":"@@ -1372,1 +1372,1 @@\n-      } else if( offset == oopDesc::klass_offset_in_bytes() ) {\n+      } else if( offset == Type::klass_offset() ) {\n@@ -1545,1 +1545,1 @@\n-          (offset == oopDesc::klass_offset_in_bytes() && tj->base() == Type::AryPtr) ||\n+          (offset == Type::klass_offset() && tj->base() == Type::AryPtr) ||\n","filename":"src\/hotspot\/share\/opto\/compile.cpp","additions":2,"deletions":2,"binary":false,"changes":4,"status":"modified"},{"patch":"@@ -947,1 +947,1 @@\n-    Node* p = basic_plus_adr( ex_node, ex_node, oopDesc::klass_offset_in_bytes());\n+    Node* p = basic_plus_adr( ex_node, ex_node, Type::klass_offset());\n@@ -965,2 +965,2 @@\n-        Node* p = basic_plus_adr(ex_in, ex_in, oopDesc::klass_offset_in_bytes());\n-        Node* k = _gvn.transform(LoadKlassNode::make(_gvn, immutable_memory(), p, TypeInstPtr::KLASS, TypeInstKlassPtr::OBJECT));\n+        Node* p = basic_plus_adr(ex_in, ex_in, Type::klass_offset());\n+        Node* k = _gvn.transform( LoadKlassNode::make(_gvn, immutable_memory(), p, TypeInstPtr::KLASS, TypeInstKlassPtr::OBJECT));\n","filename":"src\/hotspot\/share\/opto\/doCall.cpp","additions":3,"deletions":3,"binary":false,"changes":6,"status":"modified"},{"patch":"@@ -3438,1 +3438,1 @@\n-  } else if (offset != oopDesc::klass_offset_in_bytes()) {\n+  } else if (offset != Type::klass_offset()) {\n@@ -4437,1 +4437,1 @@\n-      _compile->get_alias_index(tinst->add_offset(oopDesc::klass_offset_in_bytes()));\n+      _compile->get_alias_index(tinst->add_offset(Type::klass_offset()));\n","filename":"src\/hotspot\/share\/opto\/escape.cpp","additions":2,"deletions":2,"binary":false,"changes":4,"status":"modified"},{"patch":"@@ -1202,1 +1202,1 @@\n-  Node* k_adr = basic_plus_adr(obj, oopDesc::klass_offset_in_bytes());\n+  Node* k_adr = basic_plus_adr(obj, Type::klass_offset());\n@@ -3621,1 +3621,1 @@\n-    set_memory(minit_out, C->get_alias_index(oop_type->add_offset(oopDesc::klass_offset_in_bytes())));\n+    set_memory(minit_out, C->get_alias_index(oop_type->add_offset(Type::klass_offset())));\n","filename":"src\/hotspot\/share\/opto\/graphKit.cpp","additions":2,"deletions":2,"binary":false,"changes":4,"status":"modified"},{"patch":"@@ -4668,1 +4668,1 @@\n-  enum { _slow_path = 1, _fast_path, _null_path, PATH_LIMIT };\n+  enum { _slow_path = 1, _null_path, _fast_path, _fast_path2, PATH_LIMIT };\n@@ -4716,6 +4716,34 @@\n-  \/\/ Get the header out of the object, use LoadMarkNode when available\n-  Node* header_addr = basic_plus_adr(obj, oopDesc::mark_offset_in_bytes());\n-  \/\/ The control of the load must be null. Otherwise, the load can move before\n-  \/\/ the null check after castPP removal.\n-  Node* no_ctrl = nullptr;\n-  Node* header = make_load(no_ctrl, header_addr, TypeX_X, TypeX_X->basic_type(), MemNode::unordered);\n+  if (UseCompactObjectHeaders) {\n+    \/\/ Get the header out of the object.\n+    Node* header_addr = basic_plus_adr(obj, oopDesc::mark_offset_in_bytes());\n+    \/\/ The control of the load must be null. Otherwise, the load can move before\n+    \/\/ the null check after castPP removal.\n+    Node* no_ctrl = nullptr;\n+    Node* header = make_load(no_ctrl, header_addr, TypeX_X, TypeX_X->basic_type(), MemNode::unordered);\n+\n+    \/\/ Test the header to see if the object is in hashed or copied state.\n+    Node* hashctrl_mask  = _gvn.MakeConX(markWord::hashctrl_mask_in_place);\n+    Node* masked_header  = _gvn.transform(new AndXNode(header, hashctrl_mask));\n+\n+    \/\/ Take slow-path when the object has not been hashed.\n+    Node* not_hashed_val = _gvn.MakeConX(0);\n+    Node* chk_hashed     = _gvn.transform(new CmpXNode(masked_header, not_hashed_val));\n+    Node* test_hashed    = _gvn.transform(new BoolNode(chk_hashed, BoolTest::eq));\n+\n+    generate_slow_guard(test_hashed, slow_region);\n+\n+    \/\/ Test whether the object is hashed or hashed&copied.\n+    Node* hashed_copied = _gvn.MakeConX(markWord::hashctrl_expanded_mask_in_place | markWord::hashctrl_hashed_mask_in_place);\n+    Node* chk_copied    = _gvn.transform(new CmpXNode(masked_header, hashed_copied));\n+    \/\/ If true, then object has been hashed&copied, otherwise it's only hashed.\n+    Node* test_copied   = _gvn.transform(new BoolNode(chk_copied, BoolTest::eq));\n+    IfNode* if_copied   = create_and_map_if(control(), test_copied, PROB_FAIR, COUNT_UNKNOWN);\n+    Node* if_true = _gvn.transform(new IfTrueNode(if_copied));\n+    Node* if_false = _gvn.transform(new IfFalseNode(if_copied));\n+\n+    \/\/ Hashed&Copied path: read hash-code out of the object.\n+    set_control(if_true);\n+    \/\/ result_val->del_req(_fast_path2);\n+    \/\/ result_reg->del_req(_fast_path2);\n+    \/\/ result_io->del_req(_fast_path2);\n+    \/\/ result_mem->del_req(_fast_path2);\n@@ -4723,8 +4751,19 @@\n-  if (!UseObjectMonitorTable) {\n-    \/\/ Test the header to see if it is safe to read w.r.t. locking.\n-    Node *lock_mask      = _gvn.MakeConX(markWord::lock_mask_in_place);\n-    Node *lmasked_header = _gvn.transform(new AndXNode(header, lock_mask));\n-    if (LockingMode == LM_LIGHTWEIGHT) {\n-      Node *monitor_val   = _gvn.MakeConX(markWord::monitor_value);\n-      Node *chk_monitor   = _gvn.transform(new CmpXNode(lmasked_header, monitor_val));\n-      Node *test_monitor  = _gvn.transform(new BoolNode(chk_monitor, BoolTest::eq));\n+    Node* obj_klass = load_object_klass(obj);\n+    Node* hash_addr;\n+    const TypeKlassPtr* klass_t = _gvn.type(obj_klass)->isa_klassptr();\n+    bool load_offset_runtime = true;\n+\n+    if (klass_t != nullptr) {\n+      if (klass_t->klass_is_exact()  && klass_t->isa_instklassptr()) {\n+        ciInstanceKlass* ciKlass = reinterpret_cast<ciInstanceKlass*>(klass_t->is_instklassptr()->exact_klass());\n+        if (!ciKlass->is_mirror_instance_klass() && !ciKlass->is_reference_instance_klass()) {\n+          \/\/ We know the InstanceKlass, load hash_offset from there at compile-time.\n+          int hash_offset = ciKlass->hash_offset_in_bytes();\n+          hash_addr = basic_plus_adr(obj, hash_offset);\n+          Node* loaded_hash = make_load(control(), hash_addr, TypeInt::INT, T_INT, MemNode::unordered);\n+          result_val->init_req(_fast_path2, loaded_hash);\n+          result_reg->init_req(_fast_path2, control());\n+          load_offset_runtime = false;\n+        }\n+      }\n+    }\n@@ -4732,5 +4771,22 @@\n-      generate_slow_guard(test_monitor, slow_region);\n-    } else {\n-      Node *unlocked_val      = _gvn.MakeConX(markWord::unlocked_value);\n-      Node *chk_unlocked      = _gvn.transform(new CmpXNode(lmasked_header, unlocked_val));\n-      Node *test_not_unlocked = _gvn.transform(new BoolNode(chk_unlocked, BoolTest::ne));\n+    \/\/tty->print_cr(\"Load hash-offset at runtime: %s\", BOOL_TO_STR(load_offset_runtime));\n+\n+    if (load_offset_runtime) {\n+      \/\/ We don't know if it is an array or an exact type, figure it out at run-time.\n+      \/\/ If not an ordinary instance, then we need to take slow-path.\n+      Node* kind_addr = basic_plus_adr(obj_klass, Klass::kind_offset_in_bytes());\n+      Node* kind = make_load(control(), kind_addr, TypeInt::INT, T_INT, MemNode::unordered);\n+      Node* instance_val = _gvn.intcon(Klass::InstanceKlassKind);\n+      Node* chk_inst     = _gvn.transform(new CmpINode(kind, instance_val));\n+      Node* test_inst    = _gvn.transform(new BoolNode(chk_inst, BoolTest::ne));\n+      generate_slow_guard(test_inst, slow_region);\n+\n+      \/\/ Otherwise it's an instance and we can read the hash_offset from the InstanceKlass.\n+      Node* hash_offset_addr = basic_plus_adr(obj_klass, InstanceKlass::hash_offset_offset_in_bytes());\n+      Node* hash_offset = make_load(control(), hash_offset_addr, TypeInt::INT, T_INT, MemNode::unordered);\n+      \/\/ hash_offset->dump();\n+      Node* hash_addr = basic_plus_adr(obj, ConvI2X(hash_offset));\n+      Compile::current()->set_has_unsafe_access(true);\n+      Node* loaded_hash = make_load(control(), hash_addr, TypeInt::INT, T_INT, MemNode::unordered);\n+      result_val->init_req(_fast_path2, loaded_hash);\n+      result_reg->init_req(_fast_path2, control());\n+    }\n@@ -4738,1 +4794,78 @@\n-      generate_slow_guard(test_not_unlocked, slow_region);\n+    \/\/ Hashed-only path: recompute hash-code from object address.\n+    set_control(if_false);\n+    \/\/ Our constants.\n+    Node* M = _gvn.intcon(0x337954D5);\n+    Node* A = _gvn.intcon(0xAAAAAAAA);\n+    \/\/ Split object address into lo and hi 32 bits.\n+    Node* obj_addr = _gvn.transform(new CastP2XNode(nullptr, obj));\n+    Node* x = _gvn.transform(new ConvL2INode(obj_addr));\n+    Node* upper_addr = _gvn.transform(new URShiftLNode(obj_addr, _gvn.intcon(32)));\n+    Node* y = _gvn.transform(new ConvL2INode(upper_addr));\n+\n+    Node* H0 = _gvn.transform(new XorINode(x, y));\n+    Node* L0 = _gvn.transform(new XorINode(x, A));\n+\n+    \/\/ Full multiplication of two 32 bit values L0 and M into a hi\/lo result in two 32 bit values V0 and U0.\n+    Node* L0_64 = _gvn.transform(new ConvI2LNode(L0));\n+    L0_64 = _gvn.transform(new AndLNode(L0_64, _gvn.longcon(0xFFFFFFFF)));\n+    Node* M_64 = _gvn.transform(new ConvI2LNode(M));\n+    \/\/ M_64 = _gvn.transform(new AndLNode(M_64, _gvn.longcon(0xFFFFFFFF)));\n+    Node* prod64 = _gvn.transform(new MulLNode(L0_64, M_64));\n+    Node* V0 = _gvn.transform(new ConvL2INode(prod64));\n+    Node* prod_upper = _gvn.transform(new URShiftLNode(prod64, _gvn.intcon(32)));\n+    Node* U0 = _gvn.transform(new ConvL2INode(prod_upper));\n+\n+    Node* Q0 = _gvn.transform(new MulINode(H0, M));\n+    Node* L1 = _gvn.transform(new XorINode(Q0, U0));\n+\n+    \/\/ Full multiplication of two 32 bit values L1 and M into a hi\/lo result in two 32 bit values V1 and U1.\n+    Node* L1_64 = _gvn.transform(new ConvI2LNode(L1));\n+    L1_64 = _gvn.transform(new AndLNode(L1_64, _gvn.longcon(0xFFFFFFFF)));\n+    prod64 = _gvn.transform(new MulLNode(L1_64, M_64));\n+    Node* V1 = _gvn.transform(new ConvL2INode(prod64));\n+    prod_upper = _gvn.transform(new URShiftLNode(prod64, _gvn.intcon(32)));\n+    Node* U1 = _gvn.transform(new ConvL2INode(prod_upper));\n+\n+    Node* P1 = _gvn.transform(new XorINode(V0, M));\n+\n+    \/\/ Right rotate P1 by distance L1.\n+    Node* distance = _gvn.transform(new AndINode(L1, _gvn.intcon(32 - 1)));\n+    Node* inverse_distance = _gvn.transform(new SubINode(_gvn.intcon(32), distance));\n+    Node* ror_part1 = _gvn.transform(new URShiftINode(P1, distance));\n+    Node* ror_part2 = _gvn.transform(new LShiftINode(P1, inverse_distance));\n+    Node* Q1 = _gvn.transform(new OrINode(ror_part1, ror_part2));\n+\n+    Node* L2 = _gvn.transform(new XorINode(Q1, U1));\n+    Node* hash = _gvn.transform(new XorINode(V1, L2));\n+    Node* hash_truncated = _gvn.transform(new AndINode(hash, _gvn.intcon(markWord::hash_mask)));\n+\n+    \/\/ TODO: We could generate a fast case here under the following conditions:\n+    \/\/ - The hashctrl is set to hash_is_copied (see markWord::hash_is_copied())\n+    \/\/ - The type of the object is known\n+    \/\/ Then we can load the identity hashcode from the int field at Klass::hash_offset_in_bytes() of the object.\n+    result_val->init_req(_fast_path, hash_truncated);\n+  } else {\n+    \/\/ Get the header out of the object, use LoadMarkNode when available\n+    Node* header_addr = basic_plus_adr(obj, oopDesc::mark_offset_in_bytes());\n+    \/\/ The control of the load must be null. Otherwise, the load can move before\n+    \/\/ the null check after castPP removal.\n+    Node* no_ctrl = nullptr;\n+    Node* header = make_load(no_ctrl, header_addr, TypeX_X, TypeX_X->basic_type(), MemNode::unordered);\n+\n+    if (!UseObjectMonitorTable) {\n+      \/\/ Test the header to see if it is safe to read w.r.t. locking.\n+      Node *lock_mask      = _gvn.MakeConX(markWord::lock_mask_in_place);\n+      Node *lmasked_header = _gvn.transform(new AndXNode(header, lock_mask));\n+      if (LockingMode == LM_LIGHTWEIGHT) {\n+        Node *monitor_val   = _gvn.MakeConX(markWord::monitor_value);\n+        Node *chk_monitor   = _gvn.transform(new CmpXNode(lmasked_header, monitor_val));\n+        Node *test_monitor  = _gvn.transform(new BoolNode(chk_monitor, BoolTest::eq));\n+\n+        generate_slow_guard(test_monitor, slow_region);\n+      } else {\n+        Node *unlocked_val      = _gvn.MakeConX(markWord::unlocked_value);\n+        Node *chk_unlocked      = _gvn.transform(new CmpXNode(lmasked_header, unlocked_val));\n+        Node *test_not_unlocked = _gvn.transform(new BoolNode(chk_unlocked, BoolTest::ne));\n+\n+        generate_slow_guard(test_not_unlocked, slow_region);\n+      }\n@@ -4740,14 +4873,13 @@\n-  }\n-  \/\/ Get the hash value and check to see that it has been properly assigned.\n-  \/\/ We depend on hash_mask being at most 32 bits and avoid the use of\n-  \/\/ hash_mask_in_place because it could be larger than 32 bits in a 64-bit\n-  \/\/ vm: see markWord.hpp.\n-  Node *hash_mask      = _gvn.intcon(markWord::hash_mask);\n-  Node *hash_shift     = _gvn.intcon(markWord::hash_shift);\n-  Node *hshifted_header= _gvn.transform(new URShiftXNode(header, hash_shift));\n-  \/\/ This hack lets the hash bits live anywhere in the mark object now, as long\n-  \/\/ as the shift drops the relevant bits into the low 32 bits.  Note that\n-  \/\/ Java spec says that HashCode is an int so there's no point in capturing\n-  \/\/ an 'X'-sized hashcode (32 in 32-bit build or 64 in 64-bit build).\n-  hshifted_header      = ConvX2I(hshifted_header);\n-  Node *hash_val       = _gvn.transform(new AndINode(hshifted_header, hash_mask));\n+    \/\/ Get the hash value and check to see that it has been properly assigned.\n+    \/\/ We depend on hash_mask being at most 32 bits and avoid the use of\n+    \/\/ hash_mask_in_place because it could be larger than 32 bits in a 64-bit\n+    \/\/ vm: see markWord.hpp.\n+    Node *hash_mask      = _gvn.intcon(markWord::hash_mask);\n+    Node *hash_shift     = _gvn.intcon(markWord::hash_shift);\n+    Node *hshifted_header= _gvn.transform(new URShiftXNode(header, hash_shift));\n+    \/\/ This hack lets the hash bits live anywhere in the mark object now, as long\n+    \/\/ as the shift drops the relevant bits into the low 32 bits.  Note that\n+    \/\/ Java spec says that HashCode is an int so there's no point in capturing\n+    \/\/ an 'X'-sized hashcode (32 in 32-bit build or 64 in 64-bit build).\n+    hshifted_header      = ConvX2I(hshifted_header);\n+    Node *hash_val       = _gvn.transform(new AndINode(hshifted_header, hash_mask));\n@@ -4756,3 +4888,3 @@\n-  Node *no_hash_val    = _gvn.intcon(markWord::no_hash);\n-  Node *chk_assigned   = _gvn.transform(new CmpINode( hash_val, no_hash_val));\n-  Node *test_assigned  = _gvn.transform(new BoolNode( chk_assigned, BoolTest::eq));\n+    Node *no_hash_val    = _gvn.intcon(markWord::no_hash);\n+    Node *chk_assigned   = _gvn.transform(new CmpINode( hash_val, no_hash_val));\n+    Node *test_assigned  = _gvn.transform(new BoolNode( chk_assigned, BoolTest::eq));\n@@ -4760,1 +4892,10 @@\n-  generate_slow_guard(test_assigned, slow_region);\n+    generate_slow_guard(test_assigned, slow_region);\n+\n+    result_val->init_req(_fast_path, hash_val);\n+\n+    \/\/ _fast_path2 is not used here.\n+    result_val->del_req(_fast_path2);\n+    result_reg->del_req(_fast_path2);\n+    result_io->del_req(_fast_path2);\n+    result_mem->del_req(_fast_path2);\n+  }\n@@ -4767,1 +4908,0 @@\n-  result_val->init_req(_fast_path, hash_val);\n@@ -4772,0 +4912,5 @@\n+  if (UseCompactObjectHeaders) {\n+    result_io->init_req(_fast_path2, i_o());\n+    result_mem->init_req(_fast_path2, init_mem);\n+  }\n+\n@@ -4773,0 +4918,1 @@\n+  assert(slow_region != nullptr, \"must have slow_region\");\n","filename":"src\/hotspot\/share\/opto\/library_call.cpp","additions":186,"deletions":40,"binary":false,"changes":226,"status":"modified"},{"patch":"@@ -1713,1 +1713,1 @@\n-    rawmem = make_store(control, rawmem, object, oopDesc::klass_offset_in_bytes(), klass_node, T_METADATA);\n+    rawmem = make_store(control, rawmem, object, Type::klass_offset(), klass_node, T_METADATA);\n@@ -2340,1 +2340,1 @@\n-      Node* k_adr = basic_plus_adr(obj_or_subklass, oopDesc::klass_offset_in_bytes());\n+      Node* k_adr = basic_plus_adr(obj_or_subklass, Type::klass_offset());\n","filename":"src\/hotspot\/share\/opto\/macro.cpp","additions":2,"deletions":2,"binary":false,"changes":4,"status":"modified"},{"patch":"@@ -265,1 +265,1 @@\n-          adr_check->offset() == oopDesc::klass_offset_in_bytes() ||\n+          adr_check->offset() == Type::klass_offset() ||\n@@ -946,1 +946,1 @@\n-           adr_type->offset() == oopDesc::klass_offset_in_bytes()),\n+           adr_type->offset() == Type::klass_offset()),\n@@ -2459,1 +2459,1 @@\n-    if (offset == oopDesc::klass_offset_in_bytes()) {\n+    if (offset == Type::klass_offset()) {\n@@ -2467,1 +2467,1 @@\n-      tary->offset() == oopDesc::klass_offset_in_bytes()) {\n+      tary->offset() == Type::klass_offset()) {\n@@ -2533,1 +2533,1 @@\n-  if (offset == oopDesc::klass_offset_in_bytes()) {\n+  if (offset == Type::klass_offset()) {\n","filename":"src\/hotspot\/share\/opto\/memnode.cpp","additions":5,"deletions":5,"binary":false,"changes":10,"status":"modified"},{"patch":"@@ -2122,1 +2122,1 @@\n-  Node* klass_addr = basic_plus_adr( receiver, receiver, oopDesc::klass_offset_in_bytes() );\n+  Node* klass_addr = basic_plus_adr( receiver, receiver, Type::klass_offset() );\n","filename":"src\/hotspot\/share\/opto\/parse1.cpp","additions":1,"deletions":1,"binary":false,"changes":2,"status":"modified"},{"patch":"@@ -1713,1 +1713,1 @@\n-  if (obj == nullptr || off != oopDesc::klass_offset_in_bytes()) \/\/ loading oopDesc::_klass?\n+  if (obj == nullptr || off != Type::klass_offset()) \/\/ loading oopDesc::_klass?\n","filename":"src\/hotspot\/share\/opto\/parse2.cpp","additions":1,"deletions":1,"binary":false,"changes":2,"status":"modified"},{"patch":"@@ -156,1 +156,1 @@\n-  int klass_offset = oopDesc::klass_offset_in_bytes();\n+  int klass_offset = Type::klass_offset();\n","filename":"src\/hotspot\/share\/opto\/parseHelper.cpp","additions":1,"deletions":1,"binary":false,"changes":2,"status":"modified"},{"patch":"@@ -430,1 +430,1 @@\n-    const size_t size = TypeArrayKlass::cast(array_type)->oop_size(result);\n+    const size_t size = TypeArrayKlass::cast(array_type)->oop_size(result, result->mark());\n","filename":"src\/hotspot\/share\/opto\/runtime.cpp","additions":1,"deletions":1,"binary":false,"changes":2,"status":"modified"},{"patch":"@@ -1186,1 +1186,1 @@\n-  if (con2 == oopDesc::klass_offset_in_bytes()) {\n+  if (con2 == Type::klass_offset()) {\n","filename":"src\/hotspot\/share\/opto\/subnode.cpp","additions":1,"deletions":1,"binary":false,"changes":2,"status":"modified"},{"patch":"@@ -96,1 +96,1 @@\n-    if (con == oopDesc::klass_offset_in_bytes() && obj != nullptr) {\n+    if (con == Type::klass_offset() && obj != nullptr) {\n@@ -220,1 +220,1 @@\n-    Node* adr = phase->transform(new AddPNode(obj_or_subklass, obj_or_subklass, phase->MakeConX(oopDesc::klass_offset_in_bytes())));\n+    Node* adr = phase->transform(new AddPNode(obj_or_subklass, obj_or_subklass, phase->MakeConX(Type::klass_offset())));\n@@ -246,1 +246,1 @@\n-#endif\n+#endif\n","filename":"src\/hotspot\/share\/opto\/subtypenode.cpp","additions":3,"deletions":3,"binary":false,"changes":6,"status":"modified"},{"patch":"@@ -575,1 +575,1 @@\n-                                           false, nullptr, oopDesc::klass_offset_in_bytes());\n+                                           false, nullptr, Type::klass_offset());\n@@ -3710,1 +3710,1 @@\n-    if (_offset == oopDesc::klass_offset_in_bytes()) {\n+    if (_offset == Type::klass_offset()) {\n","filename":"src\/hotspot\/share\/opto\/type.cpp","additions":2,"deletions":2,"binary":false,"changes":4,"status":"modified"},{"patch":"@@ -198,0 +198,11 @@\n+  \/\/ This is used as a marker to identify narrow Klass* loads, which\n+  \/\/ are really extracted from the mark-word, but we still want to\n+  \/\/ distinguish it.\n+  static int klass_offset() {\n+    if (UseCompactObjectHeaders) {\n+      return 1;\n+    } else {\n+      return oopDesc::klass_offset_in_bytes();\n+    }\n+  }\n+\n@@ -1436,1 +1447,1 @@\n-        _offset != arrayOopDesc::klass_offset_in_bytes()) {\n+        _offset != Type::klass_offset()) {\n","filename":"src\/hotspot\/share\/opto\/type.hpp","additions":12,"deletions":1,"binary":false,"changes":13,"status":"modified"},{"patch":"@@ -3692,0 +3692,3 @@\n+  if (UseCompactObjectHeaders && FLAG_IS_DEFAULT(hashCode)) {\n+    hashCode = 6;\n+  }\n","filename":"src\/hotspot\/share\/runtime\/arguments.cpp","additions":3,"deletions":0,"binary":false,"changes":3,"status":"modified"},{"patch":"@@ -1394,1 +1394,1 @@\n-  product(size_t, CompressedClassSpaceSize, 1*G,                            \\\n+  product(size_t, CompressedClassSpaceSize, 128*M,                          \\\n","filename":"src\/hotspot\/share\/runtime\/globals.hpp","additions":1,"deletions":1,"binary":false,"changes":2,"status":"modified"},{"patch":"@@ -51,0 +51,12 @@\n+static uintx objhash(oop obj) {\n+  if (UseCompactObjectHeaders) {\n+    uintx hash = LightweightSynchronizer::get_hash(obj->mark(), obj);\n+    assert(hash != 0, \"should have a hash\");\n+    return hash;\n+  } else {\n+    uintx hash = obj->mark().hash();\n+    assert(hash != 0, \"should have a hash\");\n+    return hash;\n+  }\n+}\n+\n@@ -81,3 +93,1 @@\n-      uintx hash = _obj->mark().hash();\n-      assert(hash != 0, \"should have a hash\");\n-      return hash;\n+      return objhash(_obj);\n@@ -285,0 +295,1 @@\n+      assert(objhash(obj) == (uintx)(*found)->hash(), \"hash must match\");\n@@ -317,1 +328,1 @@\n-       assert(obj->mark().hash() == om->hash(), \"hash must match\");\n+       assert(objhash(obj) == (uintx)om->hash(), \"hash must match\");\n@@ -407,1 +418,1 @@\n-  intptr_t hash = obj->mark().hash();\n+  intptr_t hash = objhash(obj);\n@@ -1212,0 +1223,18 @@\n+\n+uint32_t LightweightSynchronizer::get_hash(markWord mark, oop obj, Klass* klass) {\n+  assert(UseCompactObjectHeaders, \"Only with compact i-hash\");\n+  \/\/assert(mark.is_neutral() | mark.is_fast_locked(), \"only from neutral or fast-locked mark: \" INTPTR_FORMAT, mark.value());\n+  assert(mark.is_hashed(), \"only from hashed or copied object\");\n+  if (mark.is_hashed_expanded()) {\n+    return obj->int_field(klass->hash_offset_in_bytes(obj, mark));\n+  } else {\n+    assert(mark.is_hashed_not_expanded(), \"must be hashed\");\n+    assert(hashCode == 6 || hashCode == 2, \"must have idempotent hashCode\");\n+    \/\/ Already marked as hashed, but not yet copied. Recompute hash and return it.\n+    return ObjectSynchronizer::get_next_hash(nullptr, obj); \/\/ recompute hash\n+  }\n+}\n+\n+uint32_t LightweightSynchronizer::get_hash(markWord mark, oop obj) {\n+  return get_hash(mark, obj, mark.klass());\n+}\n","filename":"src\/hotspot\/share\/runtime\/lightweightSynchronizer.cpp","additions":34,"deletions":5,"binary":false,"changes":39,"status":"modified"},{"patch":"@@ -66,0 +66,1 @@\n+#include \"utilities\/fastHash.hpp\"\n@@ -931,1 +932,1 @@\n-static intptr_t get_next_hash(Thread* current, oop obj) {\n+intptr_t ObjectSynchronizer::get_next_hash(Thread* current, oop obj) {\n@@ -950,1 +951,1 @@\n-  } else {\n+  } else if (hashCode == 5) {\n@@ -963,0 +964,10 @@\n+  } else {\n+    assert(UseCompactObjectHeaders, \"Only with compact i-hash\");\n+#ifdef _LP64\n+    uint64_t val = cast_from_oop<uint64_t>(obj);\n+    uint32_t hash = FastHash::get_hash32((uint32_t)val, (uint32_t)(val >> 32));\n+#else\n+    uint32_t val = cast_from_oop<uint32_t>(obj);\n+    uint32_t hash = FastHash::get_hash32(val, UCONST64(0xAAAAAAAA));\n+#endif\n+    value= static_cast<intptr_t>(hash);\n@@ -966,2 +977,2 @@\n-  if (value == 0) value = 0xBAD;\n-  assert(value != markWord::no_hash, \"invariant\");\n+  if (hashCode != 6 && value == 0) value = 0xBAD;\n+  assert(value != markWord::no_hash || hashCode == 6, \"invariant\");\n@@ -976,4 +987,23 @@\n-    intptr_t hash = mark.hash();\n-    if (hash != 0) {\n-      return hash;\n-    }\n+    if (UseCompactObjectHeaders) {\n+      if (mark.is_hashed()) {\n+        return LightweightSynchronizer::get_hash(mark, obj);\n+      }\n+      intptr_t hash = ObjectSynchronizer::get_next_hash(current, obj);  \/\/ get a new hash\n+      markWord new_mark;\n+      if (mark.is_not_hashed_expanded()) {\n+        new_mark = mark.set_hashed_expanded();\n+        int offset = mark.klass()->hash_offset_in_bytes(obj, mark);\n+        obj->int_field_put(offset, (jint) hash);\n+      } else {\n+        new_mark = mark.set_hashed_not_expanded();\n+      }\n+      markWord old_mark = obj->cas_set_mark(new_mark, mark);\n+      if (old_mark == mark) {\n+        return hash;\n+      }\n+      mark = old_mark;\n+    } else {\n+      intptr_t hash = mark.hash();\n+      if (hash != 0) {\n+        return hash;\n+      }\n@@ -981,3 +1011,3 @@\n-    hash = get_next_hash(current, obj);\n-    const markWord old_mark = mark;\n-    const markWord new_mark = old_mark.copy_set_hash(hash);\n+      hash = ObjectSynchronizer::get_next_hash(current, obj);\n+      const markWord old_mark = mark;\n+      const markWord new_mark = old_mark.copy_set_hash(hash);\n@@ -985,3 +1015,4 @@\n-    mark = obj->cas_set_mark(new_mark, old_mark);\n-    if (old_mark == mark) {\n-      return hash;\n+      mark = obj->cas_set_mark(new_mark, old_mark);\n+      if (old_mark == mark) {\n+        return hash;\n+      }\n","filename":"src\/hotspot\/share\/runtime\/synchronizer.cpp","additions":45,"deletions":14,"binary":false,"changes":59,"status":"modified"},{"patch":"@@ -215,0 +215,2 @@\n+  static intptr_t get_next_hash(Thread* current, oop obj);\n+\n","filename":"src\/hotspot\/share\/runtime\/synchronizer.hpp","additions":2,"deletions":0,"binary":false,"changes":2,"status":"modified"},{"patch":"@@ -210,0 +210,1 @@\n+  nonstatic_field(InstanceKlass,               _hash_offset,                                  int)                                    \\\n@@ -1819,0 +1820,1 @@\n+  declare_constant(markWord::hashctrl_bits)                               \\\n@@ -1823,0 +1825,1 @@\n+  declare_constant(markWord::hashctrl_shift)                              \\\n@@ -1831,0 +1834,4 @@\n+  declare_constant(markWord::hashctrl_mask)                               \\\n+  declare_constant(markWord::hashctrl_mask_in_place)                      \\\n+  declare_constant(markWord::hashctrl_hashed_mask_in_place)               \\\n+  declare_constant(markWord::hashctrl_expanded_mask_in_place)             \\\n","filename":"src\/hotspot\/share\/runtime\/vmStructs.cpp","additions":7,"deletions":0,"binary":false,"changes":7,"status":"modified"},{"patch":"@@ -87,0 +87,1 @@\n+    hashOffset           = new CIntField(type.getCIntegerField(\"_hash_offset\"), 0);\n@@ -153,0 +154,1 @@\n+  private static CIntField hashOffset;\n@@ -243,1 +245,9 @@\n-    return getSizeHelper() * VM.getVM().getAddressSize();\n+    long baseSize = getSizeHelper() * VM.getVM().getAddressSize();\n+    if (VM.getVM().isCompactObjectHeadersEnabled()) {\n+      Mark mark = object.getMark();\n+      if (mark.isExpanded() && (getHashOffset() + 4 \/* size of hash field *\/) > baseSize) {\n+        \/\/ Needs extra word for identity hash-code.\n+        return baseSize + VM.getVM().getBytesPerWord();\n+      }\n+    }\n+    return baseSize;\n@@ -377,0 +387,1 @@\n+  public long      getHashOffset()          { return                hashOffset.getValue(this); }\n","filename":"src\/jdk.hotspot.agent\/share\/classes\/sun\/jvm\/hotspot\/oops\/InstanceKlass.java","additions":12,"deletions":1,"binary":false,"changes":13,"status":"modified"},{"patch":"@@ -79,1 +79,0 @@\n-        applyIfOr = {\"UseCompactObjectHeaders\", \"false\", \"AlignVector\", \"false\"},\n@@ -87,5 +86,0 @@\n-            \/\/ With AlignVector, we need 8-byte alignment of vector loads\/stores.\n-            \/\/ UseCompactObjectHeaders=false                  UseCompactObjectHeaders=true\n-            \/\/ I_adr = base + 16 + 4*i  ->  i % 2 = 0         B_adr = base + 12 + 4*i  ->  i % 2 = 1\n-            \/\/ N_adr = base      + 4*i  ->  i % 2 = 0         N_adr = base      + 4*i  ->  i % 2 = 0\n-            \/\/ -> vectorize                                   -> no vectorization\n","filename":"test\/hotspot\/jtreg\/compiler\/c2\/TestCastX2NotProcessedIGVN.java","additions":0,"deletions":6,"binary":false,"changes":6,"status":"modified"},{"patch":"@@ -168,1 +168,0 @@\n-        applyIfOr = {\"UseCompactObjectHeaders\", \"false\", \"AlignVector\", \"false\"},\n@@ -176,5 +175,0 @@\n-            \/\/ With AlignVector, we need 8-byte alignment of vector loads\/stores.\n-            \/\/ UseCompactObjectHeaders=false                  UseCompactObjectHeaders=true\n-            \/\/ B_adr = base + 16 + 8*i  ->  always            B_adr = base + 12 + 8*i  ->  never\n-            \/\/ L_adr = base + 16 + 8*i  ->  always            L_adr = base + 16 + 8*i  ->  always\n-            \/\/ -> vectorize                                   -> no vectorization\n@@ -186,1 +180,0 @@\n-        applyIfOr = {\"UseCompactObjectHeaders\", \"false\", \"AlignVector\", \"false\"},\n@@ -193,5 +186,0 @@\n-            \/\/ With AlignVector, we need 8-byte alignment of vector loads\/stores.\n-            \/\/ UseCompactObjectHeaders=false                  UseCompactObjectHeaders=true\n-            \/\/ B_adr = base + 16 + 8*i  ->  always            B_adr = base + 12 + 8*i  ->  never\n-            \/\/ L_adr = base + 16 + 8*i  ->  always            L_adr = base + 16 + 8*i  ->  always\n-            \/\/ -> vectorize                                   -> no vectorization\n@@ -208,5 +196,0 @@\n-            \/\/ With AlignVector, we need 8-byte alignment of vector loads\/stores.\n-            \/\/ UseCompactObjectHeaders=false                  UseCompactObjectHeaders=true\n-            \/\/ B_adr = base + 64 + 8*i  ->  always            B_adr = base + 64 + 8*i  ->  always\n-            \/\/ L_adr = base + 16 + 8*i  ->  always            L_adr = base + 16 + 8*i  ->  always\n-            \/\/ -> vectorize                                   -> vectorize\n@@ -225,5 +208,0 @@\n-            \/\/ With AlignVector, we need 8-byte alignment of vector loads\/stores.\n-            \/\/ UseCompactObjectHeaders=false                  UseCompactObjectHeaders=true\n-            \/\/ B_adr = base + 64 + 8*i  ->  always            B_adr = base + 64 + 8*i  ->  always\n-            \/\/ L_adr = base + 16 + 8*i  ->  always            L_adr = base + 16 + 8*i  ->  always\n-            \/\/ -> vectorize                                   -> vectorize\n@@ -243,1 +221,0 @@\n-        applyIfOr = {\"UseCompactObjectHeaders\", \"false\", \"AlignVector\", \"false\"},\n@@ -251,5 +228,0 @@\n-            \/\/ With AlignVector, we need 8-byte alignment of vector loads\/stores.\n-            \/\/ UseCompactObjectHeaders=false                      UseCompactObjectHeaders=true\n-            \/\/ B_adr = base + 16 + 8*(i-1)  ->  always            B_adr = base + 12 + 8*(i-1)  ->  never\n-            \/\/ L_adr = base + 16 + 8*i      ->  always            L_adr = base + 16 + 8*i      ->  always\n-            \/\/ -> vectorize                                       -> no vectorization\n@@ -261,1 +233,0 @@\n-        applyIfOr = {\"UseCompactObjectHeaders\", \"false\", \"AlignVector\", \"false\"},\n@@ -268,5 +239,0 @@\n-            \/\/ With AlignVector, we need 8-byte alignment of vector loads\/stores.\n-            \/\/ UseCompactObjectHeaders=false                      UseCompactObjectHeaders=true\n-            \/\/ B_adr = base + 16 + 8*(i-1)  ->  always            B_adr = base + 12 + 8*(i-1)  ->  never\n-            \/\/ L_adr = base + 16 + 8*i      ->  always            L_adr = base + 16 + 8*i      ->  always\n-            \/\/ -> vectorize                                       -> no vectorization\n@@ -284,1 +250,0 @@\n-        applyIfOr = {\"UseCompactObjectHeaders\", \"false\", \"AlignVector\", \"false\"},\n@@ -292,5 +257,0 @@\n-            \/\/ With AlignVector, we need 8-byte alignment of vector loads\/stores.\n-            \/\/ UseCompactObjectHeaders=false                      UseCompactObjectHeaders=true\n-            \/\/ B_adr = base + 16 + 8*(i+1)  ->  always            B_adr = base + 12 + 8*(i+1)  ->  never\n-            \/\/ L_adr = base + 16 + 8*i      ->  always            L_adr = base + 16 + 8*i      ->  always\n-            \/\/ -> vectorize                                       -> no vectorization\n@@ -302,1 +262,0 @@\n-        applyIfOr = {\"UseCompactObjectHeaders\", \"false\", \"AlignVector\", \"false\"},\n@@ -309,5 +268,0 @@\n-            \/\/ With AlignVector, we need 8-byte alignment of vector loads\/stores.\n-            \/\/ UseCompactObjectHeaders=false                      UseCompactObjectHeaders=true\n-            \/\/ B_adr = base + 16 + 8*(i+1)  ->  always            B_adr = base + 12 + 8*(i+1)  ->  never\n-            \/\/ L_adr = base + 16 + 8*i      ->  always            L_adr = base + 16 + 8*i      ->  always\n-            \/\/ -> vectorize                                       -> no vectorization\n@@ -359,1 +313,0 @@\n-        applyIfOr = {\"UseCompactObjectHeaders\", \"false\", \"AlignVector\", \"false\"},\n@@ -367,5 +320,0 @@\n-            \/\/ With AlignVector, we need 8-byte alignment of vector loads\/stores.\n-            \/\/ UseCompactObjectHeaders=false                      UseCompactObjectHeaders=true\n-            \/\/ B_adr = base + 16 + 8*(i+x)  ->  always            B_adr = base + 12 + 8*(i+x)  ->  never\n-            \/\/ L_adr = base + 16 + 8*i      ->  always            L_adr = base + 16 + 8*i      ->  always\n-            \/\/ -> vectorize                                       -> no vectorization\n@@ -377,1 +325,0 @@\n-        applyIfOr = {\"UseCompactObjectHeaders\", \"false\", \"AlignVector\", \"false\"},\n@@ -384,5 +331,0 @@\n-            \/\/ With AlignVector, we need 8-byte alignment of vector loads\/stores.\n-            \/\/ UseCompactObjectHeaders=false                      UseCompactObjectHeaders=true\n-            \/\/ B_adr = base + 16 + 8*(i+x)  ->  always            B_adr = base + 12 + 8*(i+x)  ->  never\n-            \/\/ L_adr = base + 16 + 8*i      ->  always            L_adr = base + 16 + 8*i      ->  always\n-            \/\/ -> vectorize                                       -> no vectorization\n@@ -401,1 +343,0 @@\n-        applyIfOr = {\"UseCompactObjectHeaders\", \"false\", \"AlignVector\", \"false\"},\n@@ -409,5 +350,0 @@\n-            \/\/ With AlignVector, we need 8-byte alignment of vector loads\/stores.\n-            \/\/ UseCompactObjectHeaders=false                    UseCompactObjectHeaders=true\n-            \/\/ src_adr = base + 16 + 8*i  ->  always            src_adr = base + 12 + 8*i  ->  never\n-            \/\/ dst_adr = base + 16 + 8*i  ->  always            dst_adr = base + 12 + 8*i  ->  never\n-            \/\/ -> vectorize                                     -> no vectorization\n@@ -419,1 +355,0 @@\n-        applyIfOr = {\"UseCompactObjectHeaders\", \"false\", \"AlignVector\", \"false\"},\n@@ -426,5 +361,0 @@\n-            \/\/ With AlignVector, we need 8-byte alignment of vector loads\/stores.\n-            \/\/ UseCompactObjectHeaders=false                    UseCompactObjectHeaders=true\n-            \/\/ src_adr = base + 16 + 8*i  ->  always            src_adr = base + 12 + 8*i  ->  never\n-            \/\/ dst_adr = base + 16 + 8*i  ->  always            dst_adr = base + 12 + 8*i  ->  never\n-            \/\/ -> vectorize                                     -> no vectorization\n@@ -442,1 +372,0 @@\n-        applyIfOr = {\"UseCompactObjectHeaders\", \"false\", \"AlignVector\", \"false\"},\n@@ -450,5 +379,0 @@\n-            \/\/ With AlignVector, we need 8-byte alignment of vector loads\/stores.\n-            \/\/ UseCompactObjectHeaders=false                        UseCompactObjectHeaders=true\n-            \/\/ src_adr = base + 16 + 8*i      ->  always            src_adr = base + 12 + 8*i      ->  never\n-            \/\/ dst_adr = base + 16 + 8*(i-1)  ->  always            dst_adr = base + 12 + 8*(i-1)  ->  never\n-            \/\/ -> vectorize                                         -> no vectorization\n@@ -460,1 +384,0 @@\n-        applyIfOr = {\"UseCompactObjectHeaders\", \"false\", \"AlignVector\", \"false\"},\n@@ -467,5 +390,0 @@\n-            \/\/ With AlignVector, we need 8-byte alignment of vector loads\/stores.\n-            \/\/ UseCompactObjectHeaders=false                        UseCompactObjectHeaders=true\n-            \/\/ src_adr = base + 16 + 8*i      ->  always            src_adr = base + 12 + 8*i      ->  never\n-            \/\/ dst_adr = base + 16 + 8*(i-1)  ->  always            dst_adr = base + 12 + 8*(i-1)  ->  never\n-            \/\/ -> vectorize                                         -> no vectorization\n","filename":"test\/hotspot\/jtreg\/compiler\/c2\/irTests\/TestVectorizationMismatchedAccess.java","additions":0,"deletions":82,"binary":false,"changes":82,"status":"modified"},{"patch":"@@ -66,1 +66,0 @@\n-        applyIfOr = { \"UseCompactObjectHeaders\", \"false\", \"AlignVector\", \"false\" },\n@@ -75,4 +74,0 @@\n-            \/\/ For UseCompactObjectHeaders and AlignVector, we must 8-byte align all vector loads\/stores.\n-            \/\/ But the long-stores to the byte-array are never aligned:\n-            \/\/ adr = base + UNSAFE.ARRAY_BYTE_BASE_OFFSET + 8*iter\n-            \/\/              = 16 (or 12 if UseCompactObjectHeaders=true)\n","filename":"test\/hotspot\/jtreg\/compiler\/c2\/irTests\/TestVectorizationNotRun.java","additions":0,"deletions":5,"binary":false,"changes":5,"status":"modified"},{"patch":"@@ -140,2 +140,1 @@\n-        tests.put(\"test1a\",      () -> { return test1a(aB.clone(), bB.clone(), mB); });\n-        tests.put(\"test1b\",      () -> { return test1b(aB.clone(), bB.clone(), mB); });\n+        tests.put(\"test1\",       () -> { return test1(aB.clone(), bB.clone(), mB); });\n@@ -156,1 +155,0 @@\n-        tests.put(\"test10e\",     () -> { return test10e(aS.clone(), bS.clone(), mS); });\n@@ -226,2 +224,1 @@\n-                 \"test1a\",\n-                 \"test1b\",\n+                 \"test1\",\n@@ -240,1 +237,0 @@\n-                 \"test10e\",\n@@ -431,3 +427,0 @@\n-        applyIfOr = {\"UseCompactObjectHeaders\", \"false\", \"AlignVector\", \"false\"},\n-        \/\/ UNSAFE.ARRAY_BYTE_BASE_OFFSET = 16, but with compact object headers UNSAFE.ARRAY_BYTE_BASE_OFFSET=12.\n-        \/\/ If AlignVector=true, we need the offset to be 8-byte aligned, else the vectors are filtered out.\n@@ -435,1 +428,1 @@\n-    static Object[] test1a(byte[] a, byte[] b, byte mask) {\n+    static Object[] test1(byte[] a, byte[] b, byte mask) {\n@@ -450,23 +443,0 @@\n-    @Test\n-    @IR(counts = {IRNode.LOAD_VECTOR_B, \"> 0\",\n-                  IRNode.AND_VB, \"> 0\",\n-                  IRNode.STORE_VECTOR, \"> 0\"},\n-        applyIfOr = {\"UseCompactObjectHeaders\", \"true\", \"AlignVector\", \"false\"},\n-        \/\/ UNSAFE.ARRAY_BYTE_BASE_OFFSET = 16, but with compact object headers UNSAFE.ARRAY_BYTE_BASE_OFFSET=12.\n-        \/\/ If AlignVector=true, we need the offset to be 8-byte aligned, else the vectors are filtered out.\n-        applyIfPlatform = {\"64-bit\", \"true\"},\n-        applyIfCPUFeatureOr = {\"avx2\", \"true\", \"asimd\", \"true\", \"rvv\", \"true\"})\n-    static Object[] test1b(byte[] a, byte[] b, byte mask) {\n-        for (int i = 4; i < RANGE-8; i+=8) {\n-            b[i+0] = (byte)(a[i+0] & mask); \/\/ adr = base + UNSAFE.ARRAY_BYTE_BASE_OFFSET + 4 + iter*8\n-            b[i+1] = (byte)(a[i+1] & mask);\n-            b[i+2] = (byte)(a[i+2] & mask);\n-            b[i+3] = (byte)(a[i+3] & mask);\n-            b[i+4] = (byte)(a[i+4] & mask);\n-            b[i+5] = (byte)(a[i+5] & mask);\n-            b[i+6] = (byte)(a[i+6] & mask);\n-            b[i+7] = (byte)(a[i+7] & mask);\n-        }\n-        return new Object[]{ a, b };\n-    }\n-\n@@ -764,3 +734,1 @@\n-        applyIfAnd = {\"MaxVectorSize\", \">=16\", \"UseCompactObjectHeaders\", \"false\"},\n-        \/\/ UNSAFE.ARRAY_BYTE_BASE_OFFSET = 16, but with compact object headers UNSAFE.ARRAY_BYTE_BASE_OFFSET=12.\n-        \/\/ If AlignVector=true, we need the offset to be 8-byte aligned, else the vectors are filtered out.\n+        applyIf = {\"MaxVectorSize\", \">=16\"},\n@@ -780,20 +748,0 @@\n-    @Test\n-    @IR(counts = {IRNode.LOAD_VECTOR_S, IRNode.VECTOR_SIZE_4, \"> 0\",\n-                  IRNode.AND_VS,        IRNode.VECTOR_SIZE_4, \"> 0\",\n-                  IRNode.STORE_VECTOR, \"> 0\"},\n-        applyIfAnd = {\"MaxVectorSize\", \">=16\", \"UseCompactObjectHeaders\", \"true\"},\n-        \/\/ UNSAFE.ARRAY_BYTE_BASE_OFFSET = 16, but with compact object headers UNSAFE.ARRAY_BYTE_BASE_OFFSET=12.\n-        \/\/ If AlignVector=true, we need the offset to be 8-byte aligned, else the vectors are filtered out.\n-        applyIfPlatform = {\"64-bit\", \"true\"},\n-        applyIfCPUFeatureOr = {\"avx2\", \"true\", \"asimd\", \"true\", \"rvv\", \"true\"})\n-    static Object[] test10e(short[] a, short[] b, short mask) {\n-        for (int i = 11; i < RANGE-16; i+=8) {\n-            \/\/ adr = base + UNSAFE.ARRAY_SHORT_BASE_OFFSET + 2*(3 + 11) + iter*16\n-            b[i+0+3] = (short)(a[i+0+3] & mask);\n-            b[i+1+3] = (short)(a[i+1+3] & mask);\n-            b[i+2+3] = (short)(a[i+2+3] & mask);\n-            b[i+3+3] = (short)(a[i+3+3] & mask);\n-        }\n-        return new Object[]{ a, b };\n-    }\n-\n@@ -1082,1 +1030,0 @@\n-        applyIfOr = {\"UseCompactObjectHeaders\", \"false\", \"AlignVector\", \"false\"},\n@@ -1087,13 +1034,0 @@\n-            \/\/ adr = base + UNSAFE.ARRAY_BYTE_BASE_OFFSET + 1*iter\n-            \/\/              = 16 (or 12 if UseCompactObjectHeaders=true)\n-            \/\/ adr = base + UNSAFE.ARRAY_INT_BASE_OFFSET  + 4*iter\n-            \/\/              = 16 (or 12 if UseCompactObjectHeaders=true)\n-            \/\/ For AlignVector, all adr must be 8-byte aligned. Let's see for which iteration this can hold:\n-            \/\/ If UseCompactObjectHeaders=false:\n-            \/\/   a: 0, 8, 16, 24, 32, ...\n-            \/\/   b: 0, 2,  4,  6,  8, ...\n-            \/\/   -> Ok, aligns every 8th iteration.\n-            \/\/ If UseCompactObjectHeaders=true:\n-            \/\/   a: 4, 12, 20, 28, 36, ...\n-            \/\/   b: 1,  3,  5,  7,  9, ...\n-            \/\/   -> we can never align both vectors!\n@@ -1112,1 +1046,0 @@\n-        applyIfOr = {\"UseCompactObjectHeaders\", \"false\", \"AlignVector\", \"false\"},\n@@ -1117,13 +1050,0 @@\n-            \/\/ adr = base + UNSAFE.ARRAY_BYTE_BASE_OFFSET + 4*iter\n-            \/\/              = 16 (or 12 if UseCompactObjectHeaders=true)\n-            \/\/ adr = base + UNSAFE.ARRAY_SHORT_BASE_OFFSET + 2*iter\n-            \/\/              = 16 (or 12 if UseCompactObjectHeaders=true)\n-            \/\/ For AlignVector, all adr must be 8-byte aligned. Let's see for which iteration this can hold:\n-            \/\/ If UseCompactObjectHeaders=false:\n-            \/\/   a: iter % 2 == 0\n-            \/\/   b: iter % 4 == 0\n-            \/\/   -> Ok, aligns every 4th iteration.\n-            \/\/ If UseCompactObjectHeaders=true:\n-            \/\/   a: iter % 2 = 1\n-            \/\/   b: iter % 4 = 2\n-            \/\/   -> we can never align both vectors!\n@@ -1146,1 +1066,0 @@\n-        applyIfOr = {\"UseCompactObjectHeaders\", \"false\", \"AlignVector\", \"false\"},\n@@ -1151,12 +1070,0 @@\n-            \/\/ adr = base + UNSAFE.ARRAY_BYTE_BASE_OFFSET + 1*iter\n-            \/\/              = 16 (or 12 if UseCompactObjectHeaders=true)\n-            \/\/ adr = base + UNSAFE.ARRAY_SHORT_BASE_OFFSET + 2*iter\n-            \/\/              = 16 (or 12 if UseCompactObjectHeaders=true)\n-            \/\/ adr = base + UNSAFE.ARRAY_INT_BASE_OFFSET + 4*iter\n-            \/\/              = 16 (or 12 if UseCompactObjectHeaders=true)\n-            \/\/ adr = base + UNSAFE.ARRAY_LONG_BASE_OFFSET + 8*iter\n-            \/\/              = 16 (always)\n-            \/\/ If AlignVector and UseCompactObjectHeaders, and we want all adr 8-byte aligned:\n-            \/\/   a: iter % 8 = 4\n-            \/\/   c: iter % 2 = 1\n-            \/\/   -> can never align both vectors!\n@@ -1194,1 +1101,0 @@\n-        applyIfOr = {\"UseCompactObjectHeaders\", \"false\", \"AlignVector\", \"false\"},\n@@ -1199,8 +1105,0 @@\n-            \/\/ adr = base + UNSAFE.ARRAY_INT_BASE_OFFSET + 4 + 4*iter\n-            \/\/              = 16 (or 12 if UseCompactObjectHeaders=true)\n-            \/\/ adr = base + UNSAFE.ARRAY_BYTE_BASE_OFFSET + 1 + 1*iter\n-            \/\/              = 16 (or 12 if UseCompactObjectHeaders=true)\n-            \/\/ If AlignVector and UseCompactObjectHeaders, and we want all adr 8-byte aligned:\n-            \/\/   a: iter % 2 = 0\n-            \/\/   b: iter % 8 = 3\n-            \/\/   -> can never align both vectors!\n@@ -1219,1 +1117,0 @@\n-        applyIfOr = {\"UseCompactObjectHeaders\", \"false\", \"AlignVector\", \"false\"},\n@@ -1224,8 +1121,0 @@\n-            \/\/ adr = base + UNSAFE.ARRAY_INT_BASE_OFFSET + 4 + 4*iter\n-            \/\/              = 16 (or 12 if UseCompactObjectHeaders=true)\n-            \/\/ adr = base + UNSAFE.ARRAY_SHORT_BASE_OFFSET + 2 + 2*iter\n-            \/\/              = 16 (or 12 if UseCompactObjectHeaders=true)\n-            \/\/ If AlignVector and UseCompactObjectHeaders, and we want all adr 8-byte aligned:\n-            \/\/   a: iter % 2 = 0\n-            \/\/   b: iter % 4 = 1\n-            \/\/   -> can never align both vectors!\n@@ -1248,1 +1137,0 @@\n-        applyIfOr = {\"UseCompactObjectHeaders\", \"false\", \"AlignVector\", \"false\"},\n@@ -1253,12 +1141,0 @@\n-            \/\/ adr = base + UNSAFE.ARRAY_BYTE_BASE_OFFSET + 1 + 1*iter\n-            \/\/              = 16 (or 12 if UseCompactObjectHeaders=true)\n-            \/\/ adr = base + UNSAFE.ARRAY_SHORT_BASE_OFFSET + 2 + 2*iter\n-            \/\/              = 16 (or 12 if UseCompactObjectHeaders=true)\n-            \/\/ adr = base + UNSAFE.ARRAY_INT_BASE_OFFSET + 4 + 4*iter\n-            \/\/              = 16 (or 12 if UseCompactObjectHeaders=true)\n-            \/\/ adr = base + UNSAFE.ARRAY_LONG_BASE_OFFSET + 8 + 8*iter\n-            \/\/              = 16 (always)\n-            \/\/ If AlignVector and UseCompactObjectHeaders, and we want all adr 8-byte aligned:\n-            \/\/   a: iter % 8 = 3\n-            \/\/   c: iter % 2 = 0\n-            \/\/   -> can never align both vectors!\n","filename":"test\/hotspot\/jtreg\/compiler\/loopopts\/superword\/TestAlignVector.java","additions":4,"deletions":128,"binary":false,"changes":132,"status":"modified"},{"patch":"@@ -133,1 +133,0 @@\n-        applyIfOr = {\"UseCompactObjectHeaders\", \"false\", \"AlignVector\", \"false\"},\n@@ -143,4 +142,0 @@\n-            \/\/ With AlignVector, we need 8-byte alignment of vector loads\/stores.\n-            \/\/ UseCompactObjectHeaders=false                 UseCompactObjectHeaders=true\n-            \/\/ adr = base + 16 + 8*i   ->  always            adr = base + 12 + 8*i   ->  never\n-            \/\/ -> vectorize                                  -> no vectorization\n@@ -163,1 +158,0 @@\n-        applyIfOr = {\"UseCompactObjectHeaders\", \"false\", \"AlignVector\", \"false\"},\n@@ -173,4 +167,0 @@\n-            \/\/ With AlignVector, we need 8-byte alignment of vector loads\/stores.\n-            \/\/ UseCompactObjectHeaders=false                 UseCompactObjectHeaders=true\n-            \/\/ adr = base + 16 + 8*i   ->  always            adr = base + 12 + 8*i   ->  never\n-            \/\/ -> vectorize                                  -> no vectorization\n@@ -192,1 +182,0 @@\n-        applyIfOr = {\"UseCompactObjectHeaders\", \"false\", \"AlignVector\", \"false\"},\n@@ -202,4 +191,0 @@\n-            \/\/ With AlignVector, we need 8-byte alignment of vector loads\/stores.\n-            \/\/ UseCompactObjectHeaders=false                 UseCompactObjectHeaders=true\n-            \/\/ adr = base + 16 + 8*i   ->  always            adr = base + 12 + 8*i   ->  never\n-            \/\/ -> vectorize                                  -> no vectorization\n@@ -222,1 +207,0 @@\n-        applyIfOr = {\"UseCompactObjectHeaders\", \"false\", \"AlignVector\", \"false\"},\n@@ -234,4 +218,0 @@\n-            \/\/ With AlignVector, we need 8-byte alignment of vector loads\/stores.\n-            \/\/ UseCompactObjectHeaders=false                 UseCompactObjectHeaders=true\n-            \/\/ adr = base + 16 + 8*i   ->  always            adr = base + 12 + 8*i   ->  never\n-            \/\/ -> vectorize                                  -> no vectorization\n@@ -304,1 +284,0 @@\n-        applyIfOr = {\"UseCompactObjectHeaders\", \"false\", \"AlignVector\", \"false\"},\n@@ -323,4 +302,0 @@\n-            \/\/ With AlignVector, we need 8-byte alignment of vector loads\/stores.\n-            \/\/ UseCompactObjectHeaders=false                 UseCompactObjectHeaders=true\n-            \/\/ adr = base + 16 + 8*i   ->  always            adr = base + 12 + 8*i   ->  never\n-            \/\/ -> vectorize                                  -> no vectorization\n","filename":"test\/hotspot\/jtreg\/compiler\/loopopts\/superword\/TestIndependentPacksWithCyclicDependency.java","additions":0,"deletions":25,"binary":false,"changes":25,"status":"modified"},{"patch":"@@ -281,9 +281,1 @@\n-        applyIfAnd = {\"MaxVectorSize\", \">=32\", \"AlignVector\", \"false\"},\n-        applyIfPlatform = {\"64-bit\", \"true\"},\n-        applyIfCPUFeatureOr = {\"sse4.1\", \"true\", \"asimd\", \"true\", \"rvv\", \"true\"})\n-    @IR(counts = {IRNode.LOAD_VECTOR_I, IRNode.VECTOR_SIZE_2, \"> 0\",\n-                  IRNode.LOAD_VECTOR_I, IRNode.VECTOR_SIZE_4, \"> 0\",\n-                  IRNode.AND_VI,        IRNode.VECTOR_SIZE_2, \"> 0\",\n-                  IRNode.AND_VI,        IRNode.VECTOR_SIZE_4, \"> 0\",\n-                  IRNode.STORE_VECTOR, \"> 0\"},\n-        applyIfAnd = {\"MaxVectorSize\", \">=32\", \"UseCompactObjectHeaders\", \"false\"},\n+        applyIf = {\"MaxVectorSize\", \">=32\"},\n@@ -314,4 +306,0 @@\n-            \/\/ With AlignVector, we need 8-byte alignment of vector loads\/stores.\n-            \/\/ UseCompactObjectHeaders=false                 UseCompactObjectHeaders=true\n-            \/\/ adr = base + 16 + 32*i  ->  always            adr = base + 12 + 32*i  ->  never\n-            \/\/ -> vectorize                                  -> no vectorization\n@@ -328,9 +316,1 @@\n-        applyIfAnd = {\"MaxVectorSize\", \">=32\", \"AlignVector\", \"false\"},\n-        applyIfPlatform = {\"64-bit\", \"true\"},\n-        applyIfCPUFeatureOr = {\"sse4.1\", \"true\", \"asimd\", \"true\", \"rvv\", \"true\"})\n-    @IR(counts = {IRNode.LOAD_VECTOR_I, IRNode.VECTOR_SIZE_2, \"> 0\",\n-                  IRNode.LOAD_VECTOR_I, IRNode.VECTOR_SIZE_4, \"> 0\",\n-                  IRNode.ADD_VI,        IRNode.VECTOR_SIZE_4, \"> 0\",\n-                  IRNode.MUL_VI,        IRNode.VECTOR_SIZE_2, \"> 0\",\n-                  IRNode.STORE_VECTOR, \"> 0\"},\n-        applyIfAnd = {\"MaxVectorSize\", \">=32\", \"UseCompactObjectHeaders\", \"false\"},\n+        applyIf = {\"MaxVectorSize\", \">=32\"},\n@@ -349,4 +329,0 @@\n-            \/\/ With AlignVector, we need 8-byte alignment of vector loads\/stores.\n-            \/\/ UseCompactObjectHeaders=false                 UseCompactObjectHeaders=true\n-            \/\/ adr = base + 16 + 32*i  ->  always            adr = base + 12 + 32*i  ->  never\n-            \/\/ -> vectorize                                  -> no vectorization\n@@ -363,9 +339,1 @@\n-        applyIfAnd = {\"MaxVectorSize\", \">=32\", \"AlignVector\", \"false\"},\n-        applyIfPlatform = {\"64-bit\", \"true\"},\n-        applyIfCPUFeatureOr = {\"sse4.1\", \"true\", \"asimd\", \"true\", \"rvv\", \"true\"})\n-    @IR(counts = {IRNode.LOAD_VECTOR_I, IRNode.VECTOR_SIZE_2, \"> 0\",\n-                  IRNode.LOAD_VECTOR_I, IRNode.VECTOR_SIZE_4, \"> 0\",\n-                  IRNode.ADD_VI,        IRNode.VECTOR_SIZE_2, \"> 0\",\n-                  IRNode.MUL_VI,        IRNode.VECTOR_SIZE_4, \"> 0\",\n-                  IRNode.STORE_VECTOR, \"> 0\"},\n-        applyIfAnd = {\"MaxVectorSize\", \">=32\", \"UseCompactObjectHeaders\", \"false\"},\n+        applyIf = {\"MaxVectorSize\", \">=32\"},\n@@ -384,4 +352,0 @@\n-            \/\/ With AlignVector, we need 8-byte alignment of vector loads\/stores.\n-            \/\/ UseCompactObjectHeaders=false                 UseCompactObjectHeaders=true\n-            \/\/ adr = base + 16 + 32*i  ->  always            adr = base + 12 + 32*i  ->  never\n-            \/\/ -> vectorize                                  -> no vectorization\n@@ -398,9 +362,1 @@\n-        applyIfAnd = {\"MaxVectorSize\", \">=32\", \"AlignVector\", \"false\"},\n-        applyIfPlatform = {\"64-bit\", \"true\"},\n-        applyIfCPUFeatureOr = {\"avx2\", \"true\", \"asimd\", \"true\", \"rvv\", \"true\"})\n-    @IR(counts = {IRNode.LOAD_VECTOR_I, IRNode.VECTOR_SIZE_2, \"> 0\",\n-                  IRNode.LOAD_VECTOR_I, IRNode.VECTOR_SIZE_4, \"> 0\",\n-                  IRNode.ADD_VI,        IRNode.VECTOR_SIZE_2, \"> 0\",\n-                  IRNode.MUL_VI,        IRNode.VECTOR_SIZE_4, \"> 0\",\n-                  IRNode.STORE_VECTOR, \"> 0\"},\n-        applyIfAnd = {\"MaxVectorSize\", \">=32\", \"UseCompactObjectHeaders\", \"false\"},\n+        applyIf = {\"MaxVectorSize\", \">=32\"},\n@@ -419,4 +375,0 @@\n-            \/\/ With AlignVector, we need 8-byte alignment of vector loads\/stores.\n-            \/\/ UseCompactObjectHeaders=false                 UseCompactObjectHeaders=true\n-            \/\/ adr = base + 16 + 32*i  ->  always            adr = base + 12 + 32*i  ->  never\n-            \/\/ -> vectorize                                  -> no vectorization\n@@ -433,9 +385,1 @@\n-        applyIfAnd = {\"MaxVectorSize\", \">=32\", \"AlignVector\", \"false\"},\n-        applyIfPlatform = {\"64-bit\", \"true\"},\n-        applyIfCPUFeatureOr = {\"avx2\", \"true\", \"asimd\", \"true\", \"rvv\", \"true\"})\n-    @IR(counts = {IRNode.LOAD_VECTOR_I, IRNode.VECTOR_SIZE_2, \"> 0\",\n-                  IRNode.LOAD_VECTOR_I, IRNode.VECTOR_SIZE_4, \"> 0\",\n-                  IRNode.ADD_VI,        IRNode.VECTOR_SIZE_4, \"> 0\",\n-                  IRNode.MUL_VI,        IRNode.VECTOR_SIZE_2, \"> 0\",\n-                  IRNode.STORE_VECTOR, \"> 0\"},\n-        applyIfAnd = {\"MaxVectorSize\", \">=32\", \"UseCompactObjectHeaders\", \"false\"},\n+        applyIf = {\"MaxVectorSize\", \">=32\"},\n@@ -454,4 +398,0 @@\n-            \/\/ With AlignVector, we need 8-byte alignment of vector loads\/stores.\n-            \/\/ UseCompactObjectHeaders=false                 UseCompactObjectHeaders=true\n-            \/\/ adr = base + 16 + 32*i  ->  always            adr = base + 12 + 32*i  ->  never\n-            \/\/ -> vectorize                                  -> no vectorization\n@@ -468,9 +408,1 @@\n-        applyIfAnd = {\"MaxVectorSize\", \">=32\", \"AlignVector\", \"false\"},\n-        applyIfPlatform = {\"64-bit\", \"true\"},\n-        applyIfCPUFeatureOr = {\"sse4.1\", \"true\", \"asimd\", \"true\", \"rvv\", \"true\"})\n-    @IR(counts = {IRNode.LOAD_VECTOR_I, IRNode.VECTOR_SIZE_2, \"> 0\",\n-                  IRNode.LOAD_VECTOR_I, IRNode.VECTOR_SIZE_4, \"> 0\",\n-                  IRNode.AND_VI,        IRNode.VECTOR_SIZE_2, \"> 0\",\n-                  IRNode.AND_VI,        IRNode.VECTOR_SIZE_4, \"> 0\",\n-                  IRNode.STORE_VECTOR, \"> 0\"},\n-        applyIfAnd = {\"MaxVectorSize\", \">=32\", \"UseCompactObjectHeaders\", \"false\"},\n+        applyIf = {\"MaxVectorSize\", \">=32\"},\n@@ -503,4 +435,0 @@\n-            \/\/ With AlignVector, we need 8-byte alignment of vector loads\/stores.\n-            \/\/ UseCompactObjectHeaders=false                 UseCompactObjectHeaders=true\n-            \/\/ adr = base + 16 + 32*i  ->  always            adr = base + 12 + 32*i  ->  never\n-            \/\/ -> vectorize                                  -> no vectorization\n@@ -517,9 +445,1 @@\n-        applyIfAnd = {\"MaxVectorSize\", \">=32\", \"AlignVector\", \"false\"},\n-        applyIfPlatform = {\"64-bit\", \"true\"},\n-        applyIfCPUFeatureOr = {\"sse4.1\", \"true\", \"asimd\", \"true\", \"rvv\", \"true\"})\n-    @IR(counts = {IRNode.LOAD_VECTOR_I, IRNode.VECTOR_SIZE_2, \"> 0\",\n-                  IRNode.LOAD_VECTOR_I, IRNode.VECTOR_SIZE_4, \"> 0\",\n-                  IRNode.AND_VI,        IRNode.VECTOR_SIZE_2, \"> 0\",\n-                  IRNode.AND_VI,        IRNode.VECTOR_SIZE_4, \"> 0\",\n-                  IRNode.STORE_VECTOR, \"> 0\"},\n-        applyIfAnd = {\"MaxVectorSize\", \">=32\", \"UseCompactObjectHeaders\", \"false\"},\n+        applyIf = {\"MaxVectorSize\", \">=32\"},\n@@ -552,4 +472,0 @@\n-            \/\/ With AlignVector, we need 8-byte alignment of vector loads\/stores.\n-            \/\/ UseCompactObjectHeaders=false                 UseCompactObjectHeaders=true\n-            \/\/ adr = base + 16 + 32*i  ->  always            adr = base + 12 + 32*i  ->  never\n-            \/\/ -> vectorize                                  -> no vectorization\n@@ -566,9 +482,1 @@\n-        applyIfAnd = {\"MaxVectorSize\", \">=32\", \"AlignVector\", \"false\"},\n-        applyIfPlatform = {\"64-bit\", \"true\"},\n-        applyIfCPUFeatureOr = {\"sse4.1\", \"true\", \"asimd\", \"true\", \"rvv\", \"true\"})\n-    @IR(counts = {IRNode.LOAD_VECTOR_I, IRNode.VECTOR_SIZE_2, \"> 0\",\n-                  IRNode.LOAD_VECTOR_I, IRNode.VECTOR_SIZE_4, \"> 0\",\n-                  IRNode.AND_VI,        IRNode.VECTOR_SIZE_2, \"> 0\",\n-                  IRNode.AND_VI,        IRNode.VECTOR_SIZE_4, \"> 0\",\n-                  IRNode.STORE_VECTOR, \"> 0\"},\n-        applyIfAnd = {\"MaxVectorSize\", \">=32\", \"UseCompactObjectHeaders\", \"false\"},\n+        applyIf = {\"MaxVectorSize\", \">=32\"},\n@@ -601,4 +509,0 @@\n-            \/\/ With AlignVector, we need 8-byte alignment of vector loads\/stores.\n-            \/\/ UseCompactObjectHeaders=false                 UseCompactObjectHeaders=true\n-            \/\/ adr = base + 16 + 32*i  ->  always            adr = base + 12 + 32*i  ->  never\n-            \/\/ -> vectorize                                  -> no vectorization\n@@ -615,9 +519,1 @@\n-        applyIfAnd = {\"MaxVectorSize\", \">=32\", \"AlignVector\", \"false\"},\n-        applyIfPlatform = {\"64-bit\", \"true\"},\n-        applyIfCPUFeatureOr = {\"sse4.1\", \"true\", \"asimd\", \"true\", \"rvv\", \"true\"})\n-    @IR(counts = {IRNode.LOAD_VECTOR_I, IRNode.VECTOR_SIZE_2, \"> 0\",\n-                  IRNode.LOAD_VECTOR_I, IRNode.VECTOR_SIZE_4, \"> 0\",\n-                  IRNode.AND_VI,        IRNode.VECTOR_SIZE_2, \"> 0\",\n-                  IRNode.AND_VI,        IRNode.VECTOR_SIZE_4, \"> 0\",\n-                  IRNode.STORE_VECTOR, \"> 0\"},\n-        applyIfAnd = {\"MaxVectorSize\", \">=32\", \"UseCompactObjectHeaders\", \"false\"},\n+        applyIf = {\"MaxVectorSize\", \">=32\"},\n@@ -650,4 +546,0 @@\n-            \/\/ With AlignVector, we need 8-byte alignment of vector loads\/stores.\n-            \/\/ UseCompactObjectHeaders=false                 UseCompactObjectHeaders=true\n-            \/\/ adr = base + 16 + 32*i  ->  always            adr = base + 12 + 32*i  ->  never\n-            \/\/ -> vectorize                                  -> no vectorization\n@@ -661,6 +553,1 @@\n-        applyIfAnd = {\"MaxVectorSize\", \">=32\", \"AlignVector\", \"false\"},\n-        applyIfPlatform = {\"64-bit\", \"true\"},\n-        applyIfCPUFeatureOr = {\"sse4.1\", \"true\", \"asimd\", \"true\", \"rvv\", \"true\"})\n-    @IR(counts = {IRNode.LOAD_VECTOR_S, IRNode.VECTOR_SIZE_4, \"> 0\",\n-                  IRNode.STORE_VECTOR, \"> 0\"},\n-        applyIfAnd = {\"MaxVectorSize\", \">=32\", \"UseCompactObjectHeaders\", \"false\"},\n+        applyIf = {\"MaxVectorSize\", \">=32\"},\n@@ -703,5 +590,0 @@\n-\n-            \/\/ With AlignVector, we need 8-byte alignment of vector loads\/stores.\n-            \/\/ UseCompactObjectHeaders=false                 UseCompactObjectHeaders=true\n-            \/\/ adr = base + 16 + 8 + 32*i  ->  always        adr = base + 12 + 8 + 32*i  ->  never\n-            \/\/ -> vectorize                                  -> no vectorization\n@@ -861,9 +743,1 @@\n-        applyIfAnd = {\"MaxVectorSize\", \">=32\", \"AlignVector\", \"false\"},\n-        applyIfPlatform = {\"64-bit\", \"true\"},\n-        applyIfCPUFeatureOr = {\"sse4.1\", \"true\", \"asimd\", \"true\", \"rvv\", \"true\"})\n-    @IR(counts = {IRNode.LOAD_VECTOR_I,   IRNode.VECTOR_SIZE_4, \"> 0\",\n-                  IRNode.MUL_VI,          IRNode.VECTOR_SIZE_4, \"> 0\",\n-                  IRNode.AND_VI,          IRNode.VECTOR_SIZE_4, \"> 0\",\n-                  IRNode.ADD_VI,          IRNode.VECTOR_SIZE_4, \"> 0\", \/\/ reduction moved out of loop\n-                  IRNode.ADD_REDUCTION_V,                       \"> 0\"},\n-        applyIfAnd = {\"MaxVectorSize\", \">=32\", \"UseCompactObjectHeaders\", \"false\"},\n+        applyIf = {\"MaxVectorSize\", \">=32\"},\n@@ -885,4 +759,0 @@\n-            \/\/ With AlignVector, we need 8-byte alignment of vector loads\/stores.\n-            \/\/ UseCompactObjectHeaders=false                 UseCompactObjectHeaders=true\n-            \/\/ adr = base + 16 + 32*i  ->  always            adr = base + 12 + 32*i  ->  never\n-            \/\/ -> vectorize                                  -> no vectorization\n","filename":"test\/hotspot\/jtreg\/compiler\/loopopts\/superword\/TestSplitPacks.java","additions":11,"deletions":141,"binary":false,"changes":152,"status":"modified"},{"patch":"@@ -67,1 +67,0 @@\n-        applyIfOr = {\"AlignVector\", \"false\", \"UseCompactObjectHeaders\", \"false\"},\n@@ -95,5 +94,0 @@\n-\n-            \/\/ With AlignVector, we need 8-byte alignment of vector loads\/stores.\n-            \/\/ UseCompactObjectHeaders=false                 UseCompactObjectHeaders=true\n-            \/\/ adr = base + 16 + 8*i  ->  always             adr = base + 12 + 8*i  ->  never\n-            \/\/ -> vectorize                                  -> no vectorization\n","filename":"test\/hotspot\/jtreg\/compiler\/loopopts\/superword\/TestUnorderedReductionPartialVectorization.java","additions":0,"deletions":6,"binary":false,"changes":6,"status":"modified"},{"patch":"@@ -64,1 +64,1 @@\n-\/\/ This means it affects the alignment constraints.\n+\/\/ It should, however, not affect the alignment constraints.\n@@ -237,1 +237,0 @@\n-        applyIfOr = {\"AlignVector\", \"false\", \"UseCompactObjectHeaders\", \"false\"},\n@@ -243,7 +242,0 @@\n-            \/\/ AlignVector=true requires that all vector load\/store are 8-byte aligned.\n-            \/\/ F_adr = base + UNSAFE.ARRAY_FLOAT_BASE_OFFSET + 4*i\n-            \/\/                = 16 (UseCompactObjectHeaders=false)    -> i % 2 = 0\n-            \/\/                = 12 (UseCompactObjectHeaders=true )    -> i % 2 = 1\n-            \/\/ S_adr = base + UNSAFE.ARRAY_SHORT_BASE_OFFSET + 2*i\n-            \/\/                = 16 (UseCompactObjectHeaders=false)    -> i % 4 = 0  -> can align both\n-            \/\/                = 12 (UseCompactObjectHeaders=true )    -> i % 4 = 2  -> cannot align both\n@@ -340,1 +332,0 @@\n-        applyIfOr = {\"AlignVector\", \"false\", \"UseCompactObjectHeaders\", \"false\"},\n@@ -346,7 +337,0 @@\n-            \/\/ AlignVector=true requires that all vector load\/store are 8-byte aligned.\n-            \/\/ F_adr = base + UNSAFE.ARRAY_FLOAT_BASE_OFFSET + 4*i\n-            \/\/                = 16 (UseCompactObjectHeaders=false)    -> i % 2 = 0\n-            \/\/                = 12 (UseCompactObjectHeaders=true )    -> i % 2 = 1\n-            \/\/ S_adr = base + UNSAFE.ARRAY_SHORT_BASE_OFFSET + 2*i\n-            \/\/                = 16 (UseCompactObjectHeaders=false)    -> i % 4 = 0  -> can align both\n-            \/\/                = 12 (UseCompactObjectHeaders=true )    -> i % 4 = 2  -> cannot align both\n@@ -359,1 +343,0 @@\n-        applyIfOr = {\"AlignVector\", \"false\", \"UseCompactObjectHeaders\", \"false\"},\n@@ -365,7 +348,0 @@\n-            \/\/ AlignVector=true requires that all vector load\/store are 8-byte aligned.\n-            \/\/ F_adr = base + UNSAFE.ARRAY_FLOAT_BASE_OFFSET + 4*i\n-            \/\/                = 16 (UseCompactObjectHeaders=false)    -> i % 2 = 0\n-            \/\/                = 12 (UseCompactObjectHeaders=true )    -> i % 2 = 1\n-            \/\/ S_adr = base + UNSAFE.ARRAY_SHORT_BASE_OFFSET + 2*i\n-            \/\/                = 16 (UseCompactObjectHeaders=false)    -> i % 4 = 0  -> can align both\n-            \/\/                = 12 (UseCompactObjectHeaders=true )    -> i % 4 = 2  -> cannot align both\n","filename":"test\/hotspot\/jtreg\/compiler\/vectorization\/runner\/ArrayTypeConvertTest.java","additions":1,"deletions":25,"binary":false,"changes":26,"status":"modified"},{"patch":"@@ -210,1 +210,0 @@\n-        applyIfOr = { \"UseCompactObjectHeaders\", \"false\", \"AlignVector\", \"false\"},\n@@ -220,17 +219,0 @@\n-            \/\/ We have a mix of int and short loads\/stores.\n-            \/\/ With UseCompactObjectHeaders and AlignVector,\n-            \/\/ we must 8-byte align all vector loads\/stores.\n-            \/\/\n-            \/\/ int:\n-            \/\/ adr = base + UNSAFE.ARRAY_INT_BASE_OFFSET + 4*iter\n-            \/\/              = 16 (or 12 if UseCompactObjectHeaders=true)\n-            \/\/ If UseCompactObjectHeaders=false: iter % 2 = 0\n-            \/\/ If UseCompactObjectHeaders=true:  iter % 2 = 1\n-            \/\/\n-            \/\/ byte:\n-            \/\/ adr = base + UNSAFE.ARRAY_BYTE_BASE_OFFSET + 1*iter\n-            \/\/              = 16 (or 12 if UseCompactObjectHeaders=true)\n-            \/\/ If UseCompactObjectHeaders=false: iter % 8 = 0\n-            \/\/ If UseCompactObjectHeaders=true:  iter % 8 = 4\n-            \/\/\n-            \/\/ -> we cannot align both if UseCompactObjectHeaders=true.\n@@ -243,1 +225,0 @@\n-        applyIfOr = { \"UseCompactObjectHeaders\", \"false\", \"AlignVector\", \"false\"},\n@@ -255,17 +236,0 @@\n-            \/\/ We have a mix of int and short loads\/stores.\n-            \/\/ With UseCompactObjectHeaders and AlignVector,\n-            \/\/ we must 8-byte align all vector loads\/stores.\n-            \/\/\n-            \/\/ int:\n-            \/\/ adr = base + UNSAFE.ARRAY_INT_BASE_OFFSET + 4*iter\n-            \/\/              = 16 (or 12 if UseCompactObjectHeaders=true)\n-            \/\/ If UseCompactObjectHeaders=false: iter % 2 = 0\n-            \/\/ If UseCompactObjectHeaders=true:  iter % 2 = 1\n-            \/\/\n-            \/\/ byte:\n-            \/\/ adr = base + UNSAFE.ARRAY_BYTE_BASE_OFFSET + 1*iter\n-            \/\/              = 16 (or 12 if UseCompactObjectHeaders=true)\n-            \/\/ If UseCompactObjectHeaders=false: iter % 8 = 0\n-            \/\/ If UseCompactObjectHeaders=true:  iter % 8 = 4\n-            \/\/\n-            \/\/ -> we cannot align both if UseCompactObjectHeaders=true.\n@@ -293,1 +257,0 @@\n-        applyIfOr = { \"UseCompactObjectHeaders\", \"false\", \"AlignVector\", \"false\"},\n@@ -303,17 +266,0 @@\n-            \/\/ We have a mix of int and short loads\/stores.\n-            \/\/ With UseCompactObjectHeaders and AlignVector,\n-            \/\/ we must 8-byte align all vector loads\/stores.\n-            \/\/\n-            \/\/ int:\n-            \/\/ adr = base + UNSAFE.ARRAY_INT_BASE_OFFSET + 4*iter\n-            \/\/              = 16 (or 12 if UseCompactObjectHeaders=true)\n-            \/\/ If UseCompactObjectHeaders=false: iter % 2 = 0\n-            \/\/ If UseCompactObjectHeaders=true:  iter % 2 = 1\n-            \/\/\n-            \/\/ byte:\n-            \/\/ adr = base + UNSAFE.ARRAY_BYTE_BASE_OFFSET + 1*iter\n-            \/\/              = 16 (or 12 if UseCompactObjectHeaders=true)\n-            \/\/ If UseCompactObjectHeaders=false: iter % 8 = 0\n-            \/\/ If UseCompactObjectHeaders=true:  iter % 8 = 4\n-            \/\/\n-            \/\/ -> we cannot align both if UseCompactObjectHeaders=true.\n@@ -326,1 +272,0 @@\n-        applyIfOr = { \"UseCompactObjectHeaders\", \"false\", \"AlignVector\", \"false\"},\n@@ -336,17 +281,0 @@\n-            \/\/ We have a mix of int and short loads\/stores.\n-            \/\/ With UseCompactObjectHeaders and AlignVector,\n-            \/\/ we must 8-byte align all vector loads\/stores.\n-            \/\/\n-            \/\/ int:\n-            \/\/ adr = base + UNSAFE.ARRAY_INT_BASE_OFFSET + 4*iter\n-            \/\/              = 16 (or 12 if UseCompactObjectHeaders=true)\n-            \/\/ If UseCompactObjectHeaders=false: iter % 2 = 0\n-            \/\/ If UseCompactObjectHeaders=true:  iter % 2 = 1\n-            \/\/\n-            \/\/ byte:\n-            \/\/ adr = base + UNSAFE.ARRAY_BYTE_BASE_OFFSET + 1*iter\n-            \/\/              = 16 (or 12 if UseCompactObjectHeaders=true)\n-            \/\/ If UseCompactObjectHeaders=false: iter % 8 = 0\n-            \/\/ If UseCompactObjectHeaders=true:  iter % 8 = 4\n-            \/\/\n-            \/\/ -> we cannot align both if UseCompactObjectHeaders=true.\n@@ -359,1 +287,0 @@\n-        applyIfOr = { \"UseCompactObjectHeaders\", \"false\", \"AlignVector\", \"false\"},\n@@ -369,1 +296,0 @@\n-            \/\/ same argument as in multipleOpsWith2DifferentTypesAndInvariant.\n@@ -376,1 +302,0 @@\n-        applyIfOr = { \"UseCompactObjectHeaders\", \"false\", \"AlignVector\", \"false\"},\n@@ -387,17 +312,0 @@\n-            \/\/ We have a mix of int and short loads\/stores.\n-            \/\/ With UseCompactObjectHeaders and AlignVector,\n-            \/\/ we must 8-byte align all vector loads\/stores.\n-            \/\/\n-            \/\/ int:\n-            \/\/ adr = base + UNSAFE.ARRAY_INT_BASE_OFFSET + 4*iter\n-            \/\/              = 16 (or 12 if UseCompactObjectHeaders=true)\n-            \/\/ If UseCompactObjectHeaders=false: iter % 2 = 0\n-            \/\/ If UseCompactObjectHeaders=true:  iter % 2 = 1\n-            \/\/\n-            \/\/ byte:\n-            \/\/ adr = base + UNSAFE.ARRAY_BYTE_BASE_OFFSET + 1*iter\n-            \/\/              = 16 (or 12 if UseCompactObjectHeaders=true)\n-            \/\/ If UseCompactObjectHeaders=false: iter % 8 = 0\n-            \/\/ If UseCompactObjectHeaders=true:  iter % 8 = 4\n-            \/\/\n-            \/\/ -> we cannot align both if UseCompactObjectHeaders=true.\n@@ -450,1 +358,0 @@\n-        applyIfOr = { \"UseCompactObjectHeaders\", \"false\", \"AlignVector\", \"false\"},\n@@ -457,9 +364,0 @@\n-            \/\/ Hand-unrolling can mess with alignment!\n-            \/\/\n-            \/\/ With UseCompactObjectHeaders and AlignVector,\n-            \/\/ we must 8-byte align all vector loads\/stores.\n-            \/\/\n-            \/\/ adr = base + UNSAFE.ARRAY_INT_BASE_OFFSET + 8*iter\n-            \/\/              = 16 (or 12 if UseCompactObjectHeaders=true)\n-            \/\/ If UseCompactObjectHeaders=false: 16 divisible by 8 -> vectorize\n-            \/\/ If UseCompactObjectHeaders=true:  12 not divisibly by 8 -> not vectorize\n","filename":"test\/hotspot\/jtreg\/compiler\/vectorization\/runner\/LoopCombinedOpTest.java","additions":0,"deletions":102,"binary":false,"changes":102,"status":"modified"},{"patch":"@@ -30,0 +30,1 @@\n+ * @requires vm.opt.final.UseCompactObjectHeaders == false\n@@ -51,0 +52,1 @@\n+ * @requires vm.opt.final.UseCompactObjectHeaders == false\n","filename":"test\/hotspot\/jtreg\/runtime\/cds\/appcds\/TestParallelGCWithCDS.java","additions":2,"deletions":0,"binary":false,"changes":2,"status":"modified"}]}