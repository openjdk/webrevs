{"files":[{"patch":"@@ -133,1 +133,1 @@\n-# Param2 - _nocoops, _coh, _nocoops_coh, or empty\n+# Param2 - _nocoops, or empty\n@@ -189,8 +189,0 @@\n-    ifeq ($(BUILD_CDS_ARCHIVE_COH), true)\n-      $(foreach v, $(JVM_VARIANTS), \\\n-        $(eval $(call CreateCDSArchive,$v,_coh)) \\\n-      )\n-      $(foreach v, $(JVM_VARIANTS), \\\n-        $(eval $(call CreateCDSArchive,$v,_nocoops_coh)) \\\n-      )\n-    endif\n","filename":"make\/Images.gmk","additions":1,"deletions":9,"binary":false,"changes":10,"status":"modified"},{"patch":"@@ -263,1 +263,0 @@\n-JDKOPT_ENABLE_DISABLE_CDS_ARCHIVE_COH\n","filename":"make\/autoconf\/configure.ac","additions":0,"deletions":1,"binary":false,"changes":1,"status":"modified"},{"patch":"@@ -371,1 +371,0 @@\n-BUILD_CDS_ARCHIVE_COH := @BUILD_CDS_ARCHIVE_COH@\n","filename":"make\/autoconf\/spec.gmk.template","additions":0,"deletions":1,"binary":false,"changes":1,"status":"modified"},{"patch":"@@ -121,1 +121,9 @@\n-void CompressedKlassPointers::initialize(address addr, size_t len) {\n+bool CompressedKlassPointers::pd_initialize(address addr, size_t len) {\n+\n+  if (tiny_classpointer_mode()) {\n+    \/\/ In tiny-classpointer mode, we do what all other platforms do.\n+    return false;\n+  }\n+\n+  \/\/ Aarch64 uses an own initialization logic that avoids zero-base shifted mode\n+  \/\/ (_base=0 _shift>0), instead preferring non-zero-based mode with shift=0\n@@ -125,1 +133,0 @@\n-  \/\/ Shift is always 0 on aarch64.\n@@ -128,1 +135,0 @@\n-  \/\/ On aarch64, we don't bother with zero-based encoding (base=0 shift>0).\n@@ -133,0 +139,9 @@\n+\n+#ifdef ASSERT\n+  _klass_range_start = addr;\n+  _klass_range_end = addr + len;\n+  calc_lowest_highest_narrow_klass_id();\n+  sanity_check_after_initialization();\n+#endif\n+\n+  return true;\n","filename":"src\/hotspot\/cpu\/aarch64\/compressedKlass_aarch64.cpp","additions":18,"deletions":3,"binary":false,"changes":21,"status":"modified"},{"patch":"@@ -4761,3 +4761,0 @@\n-  assert(LogKlassAlignmentInBytes == CompressedKlassPointers::shift()\n-         || 0 == CompressedKlassPointers::shift(), \"decode alg wrong\");\n-\n@@ -4789,1 +4786,1 @@\n-      lsr(dst, src, LogKlassAlignmentInBytes);\n+      lsr(dst, src, CompressedKlassPointers::shift());\n@@ -4798,1 +4795,1 @@\n-      lsr(dst, dst, LogKlassAlignmentInBytes);\n+      lsr(dst, dst, CompressedKlassPointers::shift());\n@@ -4806,1 +4803,1 @@\n-      ubfx(dst, src, LogKlassAlignmentInBytes, 32);\n+      ubfx(dst, src, CompressedKlassPointers::shift(), 32);\n@@ -4828,1 +4825,1 @@\n-      lsl(dst, src, LogKlassAlignmentInBytes);\n+      lsl(dst, src, CompressedKlassPointers::shift());\n@@ -4836,1 +4833,1 @@\n-      lsl(dst, src, LogKlassAlignmentInBytes);\n+      lsl(dst, src, CompressedKlassPointers::shift());\n@@ -4851,1 +4848,1 @@\n-      lsl(dst, dst, LogKlassAlignmentInBytes);\n+      lsl(dst, dst, CompressedKlassPointers::shift());\n","filename":"src\/hotspot\/cpu\/aarch64\/macroAssembler_aarch64.cpp","additions":6,"deletions":9,"binary":false,"changes":15,"status":"modified"},{"patch":"@@ -49,0 +49,2 @@\n+\n+bool CompressedKlassPointers::pd_initialize(address addr, size_t len) { return false; }\n","filename":"src\/hotspot\/cpu\/ppc\/compressedKlass_ppc.cpp","additions":2,"deletions":0,"binary":false,"changes":2,"status":"modified"},{"patch":"@@ -174,1 +174,1 @@\n-\n+\/\/ Todo UseCompactObjectHeaders\n","filename":"src\/hotspot\/cpu\/riscv\/c1_MacroAssembler_riscv.cpp","additions":1,"deletions":1,"binary":false,"changes":2,"status":"modified"},{"patch":"@@ -59,1 +59,1 @@\n-  \/\/ Failing that, optimize for case (3) - a base with only bits set between [33-44)\n+  \/\/ Failing that, optimize for case (3) - a base with only bits set between [32-44)\n@@ -61,1 +61,1 @@\n-    const uintptr_t from = nth_bit(32 + (optimize_for_zero_base ? LogKlassAlignmentInBytes : 0));\n+    const uintptr_t from = nth_bit(32);\n@@ -77,0 +77,2 @@\n+\n+bool CompressedKlassPointers::pd_initialize(address addr, size_t len) { return false; }\n","filename":"src\/hotspot\/cpu\/riscv\/compressedKlass_riscv.cpp","additions":4,"deletions":2,"binary":false,"changes":6,"status":"modified"},{"patch":"@@ -2291,2 +2291,1 @@\n-      assert(LogKlassAlignmentInBytes == CompressedKlassPointers::shift(), \"decode alg wrong\");\n-      slli(dst, src, LogKlassAlignmentInBytes);\n+      slli(dst, src, CompressedKlassPointers::shift());\n@@ -2308,1 +2307,0 @@\n-    assert(LogKlassAlignmentInBytes == CompressedKlassPointers::shift(), \"decode alg wrong\");\n@@ -2310,1 +2308,1 @@\n-    shadd(dst, src, xbase, t0, LogKlassAlignmentInBytes);\n+    shadd(dst, src, xbase, t0, CompressedKlassPointers::shift());\n@@ -2326,2 +2324,1 @@\n-      assert(LogKlassAlignmentInBytes == CompressedKlassPointers::shift(), \"decode alg wrong\");\n-      srli(dst, src, LogKlassAlignmentInBytes);\n+      srli(dst, src, CompressedKlassPointers::shift());\n@@ -2349,2 +2346,1 @@\n-    assert(LogKlassAlignmentInBytes == CompressedKlassPointers::shift(), \"decode alg wrong\");\n-    srli(dst, dst, LogKlassAlignmentInBytes);\n+    srli(dst, dst, CompressedKlassPointers::shift());\n","filename":"src\/hotspot\/cpu\/riscv\/macroAssembler_riscv.cpp","additions":4,"deletions":8,"binary":false,"changes":12,"status":"modified"},{"patch":"@@ -53,0 +53,2 @@\n+\n+bool CompressedKlassPointers::pd_initialize(address addr, size_t len) { return false; }\n","filename":"src\/hotspot\/cpu\/s390\/compressedKlass_s390.cpp","additions":2,"deletions":0,"binary":false,"changes":2,"status":"modified"},{"patch":"@@ -3512,1 +3512,1 @@\n-  z_tmll(current, KlassAlignmentInBytes-1); \/\/ Check alignment.\n+  z_tmll(current, CompressedKlassPointers::klass_alignment_in_bytes() - 1); \/\/ Check alignment.\n@@ -3526,1 +3526,0 @@\n-    assert (LogKlassAlignmentInBytes == shift, \"decode alg wrong\");\n@@ -3656,1 +3655,1 @@\n-  z_tmll(dst, KlassAlignmentInBytes-1); \/\/ Check alignment.\n+  z_tmll(dst, CompressedKlassPointers::klass_alignment_in_bytes() - 1); \/\/ Check alignment.\n@@ -3703,1 +3702,1 @@\n-  z_tmll(dst, KlassAlignmentInBytes-1); \/\/ Check alignment.\n+  z_tmll(dst, CompressedKlassPointers::klass_alignment_in_bytes() - 1); \/\/ Check alignment.\n@@ -3772,1 +3771,5 @@\n-    assert((shift == 0) || (shift == LogKlassAlignmentInBytes), \"cKlass encoder detected bad shift\");\n+    if (CompressedKlassPointers::tiny_classpointer_mode()) {\n+      assert(shift >= 3, \"cKlass encoder detected bad shift\");\n+    } else {\n+      assert((shift == 0) || (shift == 3), \"cKlass encoder detected bad shift\");\n+    }\n","filename":"src\/hotspot\/cpu\/s390\/macroAssembler_s390.cpp","additions":8,"deletions":5,"binary":false,"changes":13,"status":"modified"},{"patch":"@@ -49,0 +49,2 @@\n+bool CompressedKlassPointers::pd_initialize(address addr, size_t len) { return false; }\n+\n","filename":"src\/hotspot\/cpu\/x86\/compressedKlass_x86.cpp","additions":2,"deletions":0,"binary":false,"changes":2,"status":"modified"},{"patch":"@@ -5346,0 +5346,1 @@\n+  BLOCK_COMMENT(\"load_klass\");\n@@ -5376,0 +5377,1 @@\n+  BLOCK_COMMENT(\"cmp_klass 1\");\n@@ -5390,0 +5392,1 @@\n+  BLOCK_COMMENT(\"cmp_klass 2\");\n@@ -5620,2 +5623,1 @@\n-    assert (LogKlassAlignmentInBytes == CompressedKlassPointers::shift(), \"decode alg wrong\");\n-    shrq(r, LogKlassAlignmentInBytes);\n+    shrq(r, CompressedKlassPointers::shift());\n@@ -5634,2 +5636,1 @@\n-    assert (LogKlassAlignmentInBytes == CompressedKlassPointers::shift(), \"decode alg wrong\");\n-    shrq(dst, LogKlassAlignmentInBytes);\n+    shrq(dst, CompressedKlassPointers::shift());\n@@ -5647,2 +5648,1 @@\n-    assert(LogKlassAlignmentInBytes == CompressedKlassPointers::shift(), \"decode alg wrong\");\n-    shlq(r, LogKlassAlignmentInBytes);\n+    shlq(r, CompressedKlassPointers::shift());\n@@ -5670,9 +5670,12 @@\n-    if (CompressedKlassPointers::base() != nullptr) {\n-      mov64(dst, (int64_t)CompressedKlassPointers::base());\n-    } else {\n-      xorq(dst, dst);\n-    }\n-    if (CompressedKlassPointers::shift() != 0) {\n-      assert(LogKlassAlignmentInBytes == CompressedKlassPointers::shift(), \"decode alg wrong\");\n-      assert(LogKlassAlignmentInBytes == Address::times_8, \"klass not aligned on 64bits?\");\n-      leaq(dst, Address(dst, src, Address::times_8, 0));\n+    if (CompressedKlassPointers::shift() <= Address::times_8) {\n+      if (CompressedKlassPointers::base() != nullptr) {\n+        mov64(dst, (int64_t)CompressedKlassPointers::base());\n+      } else {\n+        xorq(dst, dst);\n+      }\n+      if (CompressedKlassPointers::shift() != 0) {\n+        assert(CompressedKlassPointers::shift() == Address::times_8, \"klass not aligned on 64bits?\");\n+        leaq(dst, Address(dst, src, Address::times_8, 0));\n+      } else {\n+        addq(dst, src);\n+      }\n@@ -5680,0 +5683,7 @@\n+      if (CompressedKlassPointers::base() != nullptr) {\n+        const uint64_t base_right_shifted =\n+            (uint64_t)CompressedKlassPointers::base() >> CompressedKlassPointers::shift();\n+        mov64(dst, base_right_shifted);\n+      } else {\n+        xorq(dst, dst);\n+      }\n@@ -5681,0 +5691,1 @@\n+      shlq(dst, CompressedKlassPointers::shift());\n","filename":"src\/hotspot\/cpu\/x86\/macroAssembler_x86.cpp","additions":26,"deletions":15,"binary":false,"changes":41,"status":"modified"},{"patch":"@@ -96,1 +96,1 @@\n-    return (LogKlassAlignmentInBytes <= 3);\n+    return (CompressedKlassPointers::shift() <= 3);\n","filename":"src\/hotspot\/cpu\/x86\/matcher_x86.hpp","additions":1,"deletions":1,"binary":false,"changes":2,"status":"modified"},{"patch":"@@ -80,0 +80,1 @@\n+    STATIC_ASSERT(markWord::hash_mask_compact < nth_bit(32));\n","filename":"src\/hotspot\/cpu\/x86\/sharedRuntime_x86.cpp","additions":1,"deletions":0,"binary":false,"changes":1,"status":"modified"},{"patch":"@@ -215,2 +215,4 @@\n-    \/\/ See RunTimeClassInfo::get_for()\n-    _estimated_metaspaceobj_bytes += align_up(BytesPerWord, SharedSpaceObjectAlignment);\n+    \/\/ See RunTimeClassInfo::get_for(): make sure we have enough space for both maximum\n+    \/\/ Klass alignment as well as the RuntimeInfo* pointer we will embed in front of a Klass.\n+    _estimated_metaspaceobj_bytes += align_up(BytesPerWord, CompressedKlassPointers::klass_alignment_in_bytes()) +\n+        align_up(sizeof(void*), SharedSpaceObjectAlignment);\n@@ -629,1 +631,1 @@\n-    \/\/ Save a pointer immediate in front of an InstanceKlass, so\n+    \/\/ Allocate space for a pointer directly in front of the future InstanceKlass, so\n@@ -638,0 +640,12 @@\n+    \/\/ Allocate space for the future InstanceKlass with proper alignment\n+    const size_t alignment =\n+#ifdef _LP64\n+      UseCompressedClassPointers ?\n+        nth_bit(ArchiveBuilder::precomputed_narrow_klass_shift()) :\n+        SharedSpaceObjectAlignment;\n+#else\n+    SharedSpaceObjectAlignment;\n+#endif\n+    dest = dump_region->allocate(bytes, alignment);\n+  } else {\n+    dest = dump_region->allocate(bytes);\n@@ -639,1 +653,0 @@\n-  dest = dump_region->allocate(bytes);\n@@ -670,0 +683,2 @@\n+\n+  DEBUG_ONLY(_alloc_stats.verify((int)dump_region->used(), src_info->read_only()));\n@@ -734,2 +749,2 @@\n-      const int narrow_klass_shift = precomputed_narrow_klass_shift;\n-      narrowKlass nk = CompressedKlassPointers::encode_not_null(requested_k, narrow_klass_base, narrow_klass_shift);\n+      const int narrow_klass_shift = precomputed_narrow_klass_shift();\n+      narrowKlass nk = CompressedKlassPointers::encode_raw(requested_k, narrow_klass_base, narrow_klass_shift);\n@@ -840,0 +855,5 @@\n+  const int narrow_klass_shift = ArchiveBuilder::precomputed_narrow_klass_shift();\n+#ifdef ASSERT\n+  const size_t klass_alignment = MAX2(SharedSpaceObjectAlignment, (size_t)nth_bit(narrow_klass_shift));\n+  assert(is_aligned(k, klass_alignment), \"Klass \" PTR_FORMAT \" misaligned.\", p2i(k));\n+#endif\n@@ -841,2 +861,3 @@\n-  const int narrow_klass_shift = precomputed_narrow_klass_shift;\n-  return CompressedKlassPointers::encode_not_null(requested_k, narrow_klass_base, narrow_klass_shift);\n+  \/\/ Note: use the \"raw\" version of encode that takes explicit narrow klass base and shift. Don't use any\n+  \/\/ of the variants that do sanity checks, nor any of those that use the current - dump - JVM's encoding setting.\n+  return CompressedKlassPointers::encode_raw(requested_k, narrow_klass_base, narrow_klass_shift);\n@@ -1372,0 +1393,14 @@\n+#ifdef _LP64\n+int ArchiveBuilder::precomputed_narrow_klass_shift() {\n+  \/\/ Legacy Mode:\n+  \/\/    We use 32 bits for narrowKlass, which should cover the full 4G Klass range. Shift can be 0.\n+  \/\/ CompactObjectHeader Mode:\n+  \/\/    narrowKlass is much smaller, and we use the highest possible shift value to later get the maximum\n+  \/\/    Klass encoding range.\n+  \/\/\n+  \/\/ Note that all of this may change in the future, if we decide to correct the pre-calculated\n+  \/\/ narrow Klass IDs at archive load time.\n+  assert(UseCompressedClassPointers, \"Only needed for compressed class pointers\");\n+  return CompressedKlassPointers::tiny_classpointer_mode() ?  CompressedKlassPointers::max_shift() : 0;\n+}\n+#endif \/\/ _LP64\n","filename":"src\/hotspot\/share\/cds\/archiveBuilder.cpp","additions":43,"deletions":8,"binary":false,"changes":51,"status":"modified"},{"patch":"@@ -30,0 +30,1 @@\n+#include \"memory\/metaspace.hpp\"\n@@ -46,3 +47,3 @@\n-\/\/ Metaspace::allocate() requires that all blocks must be aligned with KlassAlignmentInBytes.\n-\/\/ We enforce the same alignment rule in blocks allocated from the shared space.\n-const int SharedSpaceObjectAlignment = KlassAlignmentInBytes;\n+\/\/ The minimum alignment for non-Klass objects inside the CDS archive. Klass objects need\n+\/\/ to follow CompressedKlassPointers::klass_alignment_in_bytes().\n+constexpr size_t SharedSpaceObjectAlignment = Metaspace::min_allocation_alignment_bytes;\n@@ -93,14 +94,0 @@\n-public:\n-  \/\/ The archive contains pre-computed narrow Klass IDs in two places:\n-  \/\/ - in the header of archived java objects (only if the archive contains java heap portions)\n-  \/\/ - within the prototype markword of archived Klass structures.\n-  \/\/ These narrow Klass ids have been computed at dump time with the following scheme:\n-  \/\/ 1) the encoding base must be the mapping start address.\n-  \/\/ 2) shift must be large enough to result in an encoding range that covers the runtime Klass range.\n-  \/\/    That Klass range is defined by CDS archive size and runtime class space size. Luckily, the maximum\n-  \/\/    size can be predicted: archive size is assumed to be <1G, class space size capped at 3G, and at\n-  \/\/    runtime we put both regions adjacent to each other. Therefore, runtime Klass range size < 4G.\n-  \/\/    Since nKlass itself is 32 bit, our encoding range len is 4G, and since we set the base directly\n-  \/\/    at mapping start, these 4G are enough. Therefore, we don't need to shift at all (shift=0).\n-  static constexpr int precomputed_narrow_klass_shift = 0;\n-\n@@ -457,0 +444,19 @@\n+\n+#ifdef _LP64\n+  \/\/ Archived heap object headers (and soon, with Lilliput, markword prototypes) carry pre-computed\n+  \/\/ narrow Klass ids calculated with the following scheme:\n+  \/\/ 1) the encoding base must be the mapping start address.\n+  \/\/ 2) shift must be large enough to result in an encoding range that covers the runtime Klass range.\n+  \/\/    That Klass range is defined by CDS archive size and runtime class space size. Luckily, the maximum\n+  \/\/    size can be predicted: archive size is assumed to be <1G, class space size capped at 3G, and at\n+  \/\/    runtime we put both regions adjacent to each other. Therefore, runtime Klass range size < 4G.\n+  \/\/ The value of this precomputed shift depends on the class pointer mode at dump time.\n+  \/\/ Legacy Mode:\n+  \/\/    Since nKlass itself is 32 bit, our encoding range len is 4G, and since we set the base directly\n+  \/\/    at mapping start, these 4G are enough. Therefore, we don't need to shift at all (shift=0).\n+  \/\/ TinyClassPointer Mode:\n+  \/\/    To cover the 4G, we need the highest possible shift value. That may change in the future, if\n+  \/\/    we decide to correct the pre-calculated narrow Klass IDs at load time.\n+  static int precomputed_narrow_klass_shift();\n+#endif \/\/ _LP64\n+\n","filename":"src\/hotspot\/share\/cds\/archiveBuilder.hpp","additions":23,"deletions":17,"binary":false,"changes":40,"status":"modified"},{"patch":"@@ -209,3 +209,4 @@\n-\n-char* DumpRegion::allocate(size_t num_bytes) {\n-  char* p = (char*)align_up(_top, (size_t)SharedSpaceObjectAlignment);\n+char* DumpRegion::allocate(size_t num_bytes, size_t alignment) {\n+  \/\/ Always align to at least minimum alignment\n+  alignment = MAX2(SharedSpaceObjectAlignment, alignment);\n+  char* p = (char*)align_up(_top, alignment);\n@@ -319,1 +320,1 @@\n-  assert(tag == old_tag, \"old tag doesn't match\");\n+  assert(tag == old_tag, \"tag doesn't match (%d, expected %d)\", old_tag, tag);\n","filename":"src\/hotspot\/share\/cds\/archiveUtils.cpp","additions":5,"deletions":4,"binary":false,"changes":9,"status":"modified"},{"patch":"@@ -146,1 +146,2 @@\n-      _max_delta(max_delta), _is_packed(false) {}\n+      _max_delta(max_delta), _is_packed(false),\n+      _rs(NULL), _vs(NULL) {}\n@@ -149,1 +150,1 @@\n-  char* allocate(size_t num_bytes);\n+  char* allocate(size_t num_bytes, size_t alignment = 0);\n","filename":"src\/hotspot\/share\/cds\/archiveUtils.hpp","additions":3,"deletions":2,"binary":false,"changes":5,"status":"modified"},{"patch":"@@ -73,14 +73,7 @@\n-    stringStream tmp;\n-    tmp.print(\"%s%sclasses\", jvm_path, os::file_separator());\n-#ifdef _LP64\n-    if (!UseCompressedOops) {\n-      tmp.print_raw(\"_nocoops\");\n-    }\n-    if (UseCompactObjectHeaders) {\n-      \/\/ Note that generation of xxx_coh.jsa variants require\n-      \/\/ --enable-cds-archive-coh at build time\n-      tmp.print_raw(\"_coh\");\n-    }\n-#endif\n-    tmp.print_raw(\".jsa\");\n-    _default_archive_path = os::strdup(tmp.base());\n+    size_t jvm_path_len = strlen(jvm_path);\n+    size_t file_sep_len = strlen(os::file_separator());\n+    const size_t len = jvm_path_len + file_sep_len + 20;\n+    _default_archive_path = NEW_C_HEAP_ARRAY(char, len, mtArguments);\n+    jio_snprintf(_default_archive_path, len,\n+                LP64_ONLY(!UseCompressedOops ? \"%s%sclasses_nocoops.jsa\":) \"%s%sclasses.jsa\",\n+                jvm_path, os::file_separator());\n","filename":"src\/hotspot\/share\/cds\/cdsConfig.cpp","additions":7,"deletions":14,"binary":false,"changes":21,"status":"modified"},{"patch":"@@ -110,0 +110,13 @@\n+\n+#ifdef ASSERT\n+void DumpAllocStats::verify(int expected_byte_size, bool read_only) const {\n+  int bytes = 0;\n+  const int what = (int)(read_only ? RO : RW);\n+  for (int type = 0; type < int(_number_of_types); type ++) {\n+    bytes += _bytes[what][type];\n+  }\n+  assert(bytes == expected_byte_size, \"counter mismatch (%s: %d vs %d)\",\n+         (read_only ? \"RO\" : \"RW\"), bytes, expected_byte_size);\n+}\n+#endif \/\/ ASSERT\n+\n","filename":"src\/hotspot\/share\/cds\/dumpAllocStats.cpp","additions":13,"deletions":0,"binary":false,"changes":13,"status":"modified"},{"patch":"@@ -111,0 +111,3 @@\n+\n+  DEBUG_ONLY(void verify(int expected_byte_size, bool read_only) const);\n+\n","filename":"src\/hotspot\/share\/cds\/dumpAllocStats.hpp","additions":3,"deletions":0,"binary":false,"changes":3,"status":"modified"},{"patch":"@@ -58,0 +58,1 @@\n+#include \"oops\/compressedKlass.hpp\"\n@@ -215,0 +216,8 @@\n+  if (UseCompressedClassPointers) {\n+#ifdef _LP64\n+    _narrow_klass_pointer_bits = CompressedKlassPointers::narrow_klass_pointer_bits();\n+    _narrow_klass_shift = ArchiveBuilder::precomputed_narrow_klass_shift();\n+#endif\n+  } else {\n+    _narrow_klass_pointer_bits = _narrow_klass_shift = -1;\n+  }\n@@ -279,0 +288,2 @@\n+  st->print_cr(\"- narrow_klass_pointer_bits:      %d\", _narrow_klass_pointer_bits);\n+  st->print_cr(\"- narrow_klass_shift:             %d\", _narrow_klass_shift);\n@@ -2008,1 +2019,2 @@\n-  const int archive_narrow_klass_shift = ArchiveBuilder::precomputed_narrow_klass_shift;\n+  const int archive_narrow_klass_pointer_bits = header()->narrow_klass_pointer_bits();\n+  const int archive_narrow_klass_shift = header()->narrow_klass_shift();\n@@ -2012,2 +2024,2 @@\n-  log_info(cds)(\"    narrow_klass_base at mapping start address, narrow_klass_shift = %d\",\n-                archive_narrow_klass_shift);\n+  log_info(cds)(\"    narrow_klass_base at mapping start address, narrow_klass_pointer_bits = %d, narrow_klass_shift = %d\",\n+                archive_narrow_klass_pointer_bits, archive_narrow_klass_shift);\n@@ -2018,2 +2030,2 @@\n-  log_info(cds)(\"    narrow_klass_base = \" PTR_FORMAT \", narrow_klass_shift = %d\",\n-                p2i(CompressedKlassPointers::base()), CompressedKlassPointers::shift());\n+  log_info(cds)(\"    narrow_klass_base = \" PTR_FORMAT \", arrow_klass_pointer_bits = %d, narrow_klass_shift = %d\",\n+                p2i(CompressedKlassPointers::base()), CompressedKlassPointers::narrow_klass_pointer_bits(), CompressedKlassPointers::shift());\n@@ -2028,4 +2040,29 @@\n-  assert(archive_narrow_klass_base == CompressedKlassPointers::base(), \"Unexpected encoding base encountered \"\n-         \"(\" PTR_FORMAT \", expected \" PTR_FORMAT \")\", p2i(CompressedKlassPointers::base()), p2i(archive_narrow_klass_base));\n-  assert(archive_narrow_klass_shift == CompressedKlassPointers::shift(), \"Unexpected encoding shift encountered \"\n-         \"(%d, expected %d)\", CompressedKlassPointers::shift(), archive_narrow_klass_shift);\n+  int err = 0;\n+  if ( archive_narrow_klass_base != CompressedKlassPointers::base() ||\n+       (err = 1, archive_narrow_klass_pointer_bits != CompressedKlassPointers::narrow_klass_pointer_bits()) ||\n+       (err = 2, archive_narrow_klass_shift != CompressedKlassPointers::shift()) ) {\n+    stringStream ss;\n+    switch (err) {\n+    case 0:\n+      ss.print(\"Unexpected encoding base encountered (\" PTR_FORMAT \", expected \" PTR_FORMAT \")\",\n+               p2i(CompressedKlassPointers::base()), p2i(archive_narrow_klass_base));\n+      break;\n+    case 1:\n+      ss.print(\"Unexpected narrow Klass bit length encountered (%d, expected %d)\",\n+               CompressedKlassPointers::narrow_klass_pointer_bits(), archive_narrow_klass_pointer_bits);\n+      break;\n+    case 2:\n+      ss.print(\"Unexpected narrow Klass shift encountered (%d, expected %d)\",\n+               CompressedKlassPointers::shift(), archive_narrow_klass_shift);\n+      break;\n+    default:\n+      ShouldNotReachHere();\n+    };\n+    LogTarget(Info, cds) lt;\n+    if (lt.is_enabled()) {\n+      LogStream ls(lt);\n+      ls.print_raw(ss.base());\n+      header()->print(&ls);\n+    }\n+    assert(false, \"%s\", ss.base());\n+  }\n","filename":"src\/hotspot\/share\/cds\/filemap.cpp","additions":46,"deletions":9,"binary":false,"changes":55,"status":"modified"},{"patch":"@@ -196,0 +196,2 @@\n+  int     _narrow_klass_pointer_bits;             \/\/ save number of bits in narrowKlass\n+  int     _narrow_klass_shift;                    \/\/ save shift width used to pre-compute narrowKlass IDs in archived heap objects\n@@ -273,0 +275,2 @@\n+  int narrow_klass_pointer_bits()          const { return _narrow_klass_pointer_bits; }\n+  int narrow_klass_shift()                 const { return _narrow_klass_shift; }\n","filename":"src\/hotspot\/share\/cds\/filemap.hpp","additions":4,"deletions":0,"binary":false,"changes":4,"status":"modified"},{"patch":"@@ -89,0 +89,1 @@\n+#include \"utilities\/macros.hpp\"\n@@ -1169,1 +1170,1 @@\n-            const int precomputed_narrow_klass_shift = ArchiveBuilder::precomputed_narrow_klass_shift;\n+            const int precomputed_narrow_klass_shift = ArchiveBuilder::precomputed_narrow_klass_shift();\n@@ -1234,1 +1235,1 @@\n-\/\/  encoding, the range [Base, End) not surpass KlassEncodingMetaspaceMax.\n+\/\/  encoding, the range [Base, End) and not surpass the max. range for that encoding.\n","filename":"src\/hotspot\/share\/cds\/metaspaceShared.cpp","additions":3,"deletions":2,"binary":false,"changes":5,"status":"modified"},{"patch":"@@ -62,0 +62,1 @@\n+#include \"oops\/compressedKlass.inline.hpp\"\n@@ -424,0 +425,7 @@\n+\n+#ifdef ASSERT\n+    if (UseCompressedClassPointers && k != nullptr) {\n+      CompressedKlassPointers::check_valid_klass(k);\n+    }\n+#endif\n+\n@@ -1333,1 +1341,5 @@\n-    assert(check_alignment(record->_klass), \"Address not aligned\");\n+#ifdef _LP64\n+    if (UseCompressedClassPointers) {\n+      DEBUG_ONLY(CompressedKlassPointers::check_valid_klass(record->_klass);)\n+    }\n+#endif\n","filename":"src\/hotspot\/share\/classfile\/systemDictionaryShared.cpp","additions":13,"deletions":1,"binary":false,"changes":14,"status":"modified"},{"patch":"@@ -661,0 +661,1 @@\n+    assert(!UseCompactObjectHeaders, \"\");\n","filename":"src\/hotspot\/share\/gc\/shared\/c2\/barrierSetC2.cpp","additions":1,"deletions":0,"binary":false,"changes":1,"status":"modified"},{"patch":"@@ -456,1 +456,0 @@\n-  declare_constant(LogKlassAlignmentInBytes)                              \\\n","filename":"src\/hotspot\/share\/jvmci\/vmStructs_jvmci.cpp","additions":0,"deletions":1,"binary":false,"changes":1,"status":"modified"},{"patch":"@@ -33,0 +33,1 @@\n+#include \"memory\/metaspace\/metablock.hpp\"\n@@ -35,0 +36,2 @@\n+#include \"memory\/metaspace\/metaspaceCommon.hpp\"\n+#include \"memory\/metaspace\/metaspaceContext.hpp\"\n@@ -39,0 +42,1 @@\n+#include \"oops\/klass.hpp\"\n@@ -43,0 +47,1 @@\n+using metaspace::MetaBlock;\n@@ -44,0 +49,1 @@\n+using metaspace::MetaspaceContext;\n@@ -52,0 +58,10 @@\n+    ClassLoaderMetaspace(lock, space_type,\n+                         MetaspaceContext::context_nonclass(),\n+                         MetaspaceContext::context_class(),\n+                         CompressedKlassPointers::klass_alignment_in_words())\n+{}\n+\n+ClassLoaderMetaspace::ClassLoaderMetaspace(Mutex* lock, Metaspace::MetaspaceType space_type,\n+                                           MetaspaceContext* non_class_context,\n+                                           MetaspaceContext* class_context,\n+                                           size_t klass_alignment_words) :\n@@ -57,3 +73,0 @@\n-  ChunkManager* const non_class_cm =\n-          ChunkManager::chunkmanager_nonclass();\n-\n@@ -62,1 +75,1 @@\n-      non_class_cm,\n+      non_class_context,\n@@ -64,2 +77,2 @@\n-      RunningCounters::used_nonclass_counter(),\n-      \"non-class sm\");\n+      Metaspace::min_allocation_alignment_words,\n+      \"non-class arena\");\n@@ -68,3 +81,1 @@\n-  if (Metaspace::using_class_space()) {\n-    ChunkManager* const class_cm =\n-            ChunkManager::chunkmanager_class();\n+  if (class_context != nullptr) {\n@@ -72,1 +83,1 @@\n-        class_cm,\n+        class_context,\n@@ -74,2 +85,2 @@\n-        RunningCounters::used_class_counter(),\n-        \"class sm\");\n+        klass_alignment_words,\n+        \"class arena\");\n@@ -92,0 +103,1 @@\n+  word_size = align_up(word_size, Metaspace::min_allocation_word_size);\n@@ -93,2 +105,5 @@\n-  if (Metaspace::is_class_space_allocation(mdType)) {\n-    return class_space_arena()->allocate(word_size);\n+  MetaBlock result, wastage;\n+  const bool is_class = have_class_space_arena() && mdType == Metaspace::ClassType;\n+  if (is_class) {\n+    assert(word_size >= (sizeof(Klass)\/BytesPerWord), \"weird size for klass: %zu\", word_size);\n+    result = class_space_arena()->allocate(word_size, wastage);\n@@ -96,1 +111,11 @@\n-    return non_class_space_arena()->allocate(word_size);\n+    result = non_class_space_arena()->allocate(word_size, wastage);\n+  }\n+  if (wastage.is_nonempty()) {\n+    non_class_space_arena()->deallocate(wastage);\n+  }\n+#ifdef ASSERT\n+  if (result.is_nonempty()) {\n+    const bool in_class_arena = class_space_arena() != nullptr ? class_space_arena()->contains(result) : false;\n+    const bool in_nonclass_arena = non_class_space_arena()->contains(result);\n+    assert((is_class && in_class_arena) || (!is_class && in_class_arena != in_nonclass_arena),\n+           \"block from neither arena \" METABLOCKFORMAT \"?\", METABLOCKFORMATARGS(result));\n@@ -98,0 +123,2 @@\n+#endif\n+  return result.base();\n@@ -134,5 +161,12 @@\n-  MutexLocker fcl(lock(), Mutex::_no_safepoint_check_flag);\n-  if (Metaspace::using_class_space() && is_class) {\n-    class_space_arena()->deallocate(ptr, word_size);\n-  } else {\n-    non_class_space_arena()->deallocate(ptr, word_size);\n+  NOT_LP64(word_size = align_down(word_size, Metaspace::min_allocation_word_size);)\n+  MetaBlock bl(ptr, word_size);\n+  if (word_size >= Metaspace::min_allocation_word_size) {\n+    MutexLocker fcl(lock(), Mutex::_no_safepoint_check_flag);\n+    if (have_class_space_arena() && is_class) {\n+      assert(class_space_arena()->contains(bl),\n+             \"Not from class arena \" METABLOCKFORMAT \"?\", METABLOCKFORMATARGS(bl));\n+      class_space_arena()->deallocate(MetaBlock(ptr, word_size));\n+    } else {\n+      non_class_space_arena()->deallocate(MetaBlock(ptr, word_size));\n+    }\n+    DEBUG_ONLY(InternalStats::inc_num_deallocs();)\n@@ -140,1 +174,0 @@\n-  DEBUG_ONLY(InternalStats::inc_num_deallocs();)\n@@ -182,1 +215,1 @@\n-    if (Metaspace::using_class_space()) {\n+    if (have_class_space_arena()) {\n","filename":"src\/hotspot\/share\/memory\/classLoaderMetaspace.cpp","additions":55,"deletions":22,"binary":false,"changes":77,"status":"modified"},{"patch":"@@ -36,0 +36,1 @@\n+  class ClmsTester;\n@@ -37,0 +38,1 @@\n+  class MetaspaceContext;\n@@ -60,0 +62,1 @@\n+  friend class metaspace::ClmsTester; \/\/ for gtests\n@@ -78,1 +81,6 @@\n-public:\n+  bool have_class_space_arena() const { return _class_space_arena != nullptr; }\n+\n+  ClassLoaderMetaspace(Mutex* lock, Metaspace::MetaspaceType space_type,\n+                       metaspace::MetaspaceContext* non_class_context,\n+                       metaspace::MetaspaceContext* class_context,\n+                       size_t klass_alignment_words);\n@@ -80,0 +88,1 @@\n+public:\n","filename":"src\/hotspot\/share\/memory\/classLoaderMetaspace.hpp","additions":10,"deletions":1,"binary":false,"changes":11,"status":"modified"},{"patch":"@@ -541,0 +541,2 @@\n+const void* Metaspace::_class_space_start = nullptr;\n+const void* Metaspace::_class_space_end = nullptr;\n@@ -573,0 +575,2 @@\n+  _class_space_start = rs.base();\n+  _class_space_end = rs.end();\n@@ -777,0 +781,1 @@\n+    \/\/ In CDS=off mode, we give the JVM some leeway to choose a favorable base\/shift combination.\n@@ -845,0 +850,9 @@\n+#ifdef ASSERT\n+    if (using_class_space() && mdtype == ClassType) {\n+      assert(is_in_class_space(result) &&\n+             is_aligned(result, CompressedKlassPointers::klass_alignment_in_bytes()), \"Sanity\");\n+    } else {\n+      assert((is_in_class_space(result) || is_in_nonclass_metaspace(result)) &&\n+             is_aligned(result, Metaspace::min_allocation_alignment_bytes), \"Sanity\");\n+    }\n+#endif\n@@ -847,1 +861,0 @@\n-\n@@ -979,5 +992,4 @@\n-bool Metaspace::contains(const void* ptr) {\n-  if (MetaspaceShared::is_in_shared_metaspace(ptr)) {\n-    return true;\n-  }\n-  return contains_non_shared(ptr);\n+\/\/ Returns true if pointer points into one of the metaspace regions, or\n+\/\/ into the class space.\n+bool Metaspace::is_in_shared_metaspace(const void* ptr) {\n+  return MetaspaceShared::is_in_shared_metaspace(ptr);\n@@ -986,5 +998,2 @@\n-bool Metaspace::contains_non_shared(const void* ptr) {\n-  if (using_class_space() && VirtualSpaceList::vslist_class()->contains((MetaWord*)ptr)) {\n-     return true;\n-  }\n-\n+\/\/ Returns true if pointer points into one of the non-class-space metaspace regions.\n+bool Metaspace::is_in_nonclass_metaspace(const void* ptr) {\n","filename":"src\/hotspot\/share\/memory\/metaspace.cpp","additions":20,"deletions":11,"binary":false,"changes":31,"status":"modified"},{"patch":"@@ -29,0 +29,1 @@\n+#include \"memory\/virtualspace.hpp\"\n@@ -38,1 +39,0 @@\n-class ReservedSpace;\n@@ -67,1 +67,3 @@\n-  static bool _initialized;\n+  \/\/ For quick pointer testing: extent of class space; nullptr if no class space.\n+  static const void* _class_space_start;\n+  static const void* _class_space_end;\n@@ -108,0 +110,11 @@\n+  \/\/ Minimum allocation alignment, in bytes. All MetaData shall be aligned correclty\n+  \/\/ to be able to hold 64-bit data types. Unlike malloc, we don't care for larger\n+  \/\/ data types.\n+  static constexpr size_t min_allocation_alignment_bytes = sizeof(uint64_t);\n+\n+  \/\/ Minimum allocation alignment, in words, Metaspace observes.\n+  static constexpr size_t min_allocation_alignment_words = min_allocation_alignment_bytes \/ BytesPerWord;\n+\n+  \/\/ Every allocation will get rounded up to the minimum word size.\n+  static constexpr size_t min_allocation_word_size = min_allocation_alignment_words;\n+\n@@ -116,2 +129,24 @@\n-  static bool contains(const void* ptr);\n-  static bool contains_non_shared(const void* ptr);\n+  static bool contains(const void* ptr) {\n+    return is_in_shared_metaspace(ptr) || \/\/ in cds\n+           is_in_class_space(ptr) ||      \/\/ in class space\n+           is_in_nonclass_metaspace(ptr); \/\/ in one of the non-class regions?\n+  }\n+\n+  \/\/ kept for now for backward compat reasons, but lets test if callers really need this\n+  static bool contains_non_shared(const void* ptr) {\n+    return is_in_class_space(ptr) ||      \/\/ in class space\n+           is_in_nonclass_metaspace(ptr); \/\/ in one of the non-class regions?\n+  }\n+\n+  \/\/ Returns true if pointer points into one of the metaspace regions, or\n+  \/\/ into the class space.\n+  static bool is_in_shared_metaspace(const void* ptr);\n+\n+  \/\/ Returns true if pointer points into one of the non-class-space metaspace regions.\n+  static bool is_in_nonclass_metaspace(const void* ptr);\n+\n+  \/\/ Returns true if ptr points into class space, false if it doesn't or if\n+  \/\/ there is no class space.\n+  static inline bool is_in_class_space(const void* ptr) {\n+    return ptr < _class_space_end && ptr >= _class_space_start;\n+  }\n","filename":"src\/hotspot\/share\/memory\/metaspace.hpp","additions":39,"deletions":4,"binary":false,"changes":43,"status":"modified"},{"patch":"@@ -31,0 +31,1 @@\n+#include \"memory\/metaspace\/metablock.hpp\"\n@@ -39,2 +40,1 @@\n-\/\/ (only a few words). It is used to manage deallocated blocks - see\n-\/\/ class FreeBlocks.\n+\/\/ (only a few words). It is used to manage deallocated small blocks.\n@@ -146,1 +146,4 @@\n-  void add_block(MetaWord* p, size_t word_size) {\n+  void add_block(MetaBlock mb) {\n+    assert(!mb.is_empty(), \"Don't add empty blocks\");\n+    const size_t word_size = mb.word_size();\n+    MetaWord* const p = mb.base();\n@@ -158,2 +161,2 @@\n-  \/\/ Block may be larger. Real block size is returned in *p_real_word_size.\n-  MetaWord* remove_block(size_t word_size, size_t* p_real_word_size) {\n+  \/\/ Block may be larger.\n+  MetaBlock remove_block(size_t word_size) {\n@@ -162,0 +165,1 @@\n+    MetaBlock result;\n@@ -172,5 +176,1 @@\n-      *p_real_word_size = real_word_size;\n-      return (MetaWord*)b;\n-    } else {\n-      *p_real_word_size = 0;\n-      return nullptr;\n+      result = MetaBlock((MetaWord*)b, real_word_size);\n@@ -178,0 +178,1 @@\n+    return result;\n@@ -194,0 +195,1 @@\n+      Block* b_last = nullptr; \/\/ catch simple circularities\n@@ -196,0 +198,1 @@\n+        assert(b != b_last, \"Circle\");\n@@ -197,0 +200,1 @@\n+        b_last = b;\n@@ -198,0 +202,1 @@\n+      if (UseNewCode)printf(\"\\n\");\n","filename":"src\/hotspot\/share\/memory\/metaspace\/binList.hpp","additions":15,"deletions":10,"binary":false,"changes":25,"status":"modified"},{"patch":"@@ -185,2 +185,2 @@\n-void BlockTree::zap_range(MetaWord* p, size_t word_size) {\n-  memset(p, 0xF3, word_size * sizeof(MetaWord));\n+void BlockTree::zap_block(MetaBlock bl) {\n+  memset(bl.base(), 0xF3, bl.word_size() * sizeof(MetaWord));\n@@ -229,0 +229,6 @@\n+      \/\/ Handle simple circularities\n+      if (n == n->_right || n == n->_left || n == n->_next) {\n+        st->print_cr(\"@\" PTR_FORMAT \": circularity detected.\", p2i(n));\n+        return; \/\/ stop printing\n+      }\n+\n","filename":"src\/hotspot\/share\/memory\/metaspace\/blockTree.cpp","additions":8,"deletions":2,"binary":false,"changes":10,"status":"modified"},{"patch":"@@ -32,0 +32,1 @@\n+#include \"memory\/metaspace\/metablock.hpp\"\n@@ -38,1 +39,1 @@\n-\/\/  manage small to medium free memory blocks (see class FreeBlocks).\n+\/\/  manage medium to large free memory blocks.\n@@ -83,2 +84,1 @@\n-    \/\/  The space for that is there (these nodes are only used to manage larger blocks,\n-    \/\/  see FreeBlocks::MaxSmallBlocksWordSize).\n+    \/\/  The space for that is there (these nodes are only used to manage larger blocks).\n@@ -338,1 +338,1 @@\n-  void zap_range(MetaWord* p, size_t word_size);\n+  void zap_block(MetaBlock block);\n@@ -348,2 +348,3 @@\n-  void add_block(MetaWord* p, size_t word_size) {\n-    DEBUG_ONLY(zap_range(p, word_size));\n+  void add_block(MetaBlock block) {\n+    DEBUG_ONLY(zap_block(block);)\n+    const size_t word_size = block.word_size();\n@@ -351,1 +352,1 @@\n-    Node* n = new(p) Node(word_size);\n+    Node* n = new(block.base()) Node(word_size);\n@@ -361,3 +362,2 @@\n-  \/\/  larger than that size. Upon return, *p_real_word_size contains the actual\n-  \/\/  block size.\n-  MetaWord* remove_block(size_t word_size, size_t* p_real_word_size) {\n+  \/\/  larger than that size.\n+  MetaBlock remove_block(size_t word_size) {\n@@ -366,0 +366,1 @@\n+    MetaBlock result;\n@@ -382,2 +383,1 @@\n-      MetaWord* p = (MetaWord*)n;\n-      *p_real_word_size = n->_word_size;\n+      result = MetaBlock((MetaWord*)n, n->_word_size);\n@@ -387,2 +387,1 @@\n-      DEBUG_ONLY(zap_range(p, n->_word_size));\n-      return p;\n+      DEBUG_ONLY(zap_block(result);)\n@@ -390,1 +389,1 @@\n-    return nullptr;\n+    return result;\n","filename":"src\/hotspot\/share\/memory\/metaspace\/blockTree.hpp","additions":14,"deletions":15,"binary":false,"changes":29,"status":"modified"},{"patch":"@@ -33,3 +33,3 @@\n-void FreeBlocks::add_block(MetaWord* p, size_t word_size) {\n-  if (word_size > MaxSmallBlocksWordSize) {\n-    _tree.add_block(p, word_size);\n+void FreeBlocks::add_block(MetaBlock bl) {\n+  if (bl.word_size() > _small_blocks.MaxWordSize) {\n+    _tree.add_block(bl);\n@@ -37,1 +37,1 @@\n-    _small_blocks.add_block(p, word_size);\n+    _small_blocks.add_block(bl);\n@@ -41,1 +41,1 @@\n-MetaWord* FreeBlocks::remove_block(size_t requested_word_size) {\n+MetaBlock FreeBlocks::remove_block(size_t requested_word_size) {\n@@ -43,3 +43,3 @@\n-  MetaWord* p = nullptr;\n-  if (requested_word_size > MaxSmallBlocksWordSize) {\n-    p = _tree.remove_block(requested_word_size, &real_size);\n+  MetaBlock bl;\n+  if (requested_word_size > _small_blocks.MaxWordSize) {\n+    bl = _tree.remove_block(requested_word_size);\n@@ -47,1 +47,1 @@\n-    p = _small_blocks.remove_block(requested_word_size, &real_size);\n+    bl = _small_blocks.remove_block(requested_word_size);\n@@ -49,9 +49,1 @@\n-  if (p != nullptr) {\n-    \/\/ Blocks which are larger than a certain threshold are split and\n-    \/\/  the remainder is handed back to the manager.\n-    const size_t waste = real_size - requested_word_size;\n-    if (waste >= MinWordSize) {\n-      add_block(p + requested_word_size, waste);\n-    }\n-  }\n-  return p;\n+  return bl;\n@@ -61,1 +53,0 @@\n-\n","filename":"src\/hotspot\/share\/memory\/metaspace\/freeBlocks.cpp","additions":10,"deletions":19,"binary":false,"changes":29,"status":"modified"},{"patch":"@@ -76,4 +76,0 @@\n-  \/\/ Cutoff point: blocks larger than this size are kept in the\n-  \/\/ tree, blocks smaller than or equal to this size in the bin list.\n-  const size_t MaxSmallBlocksWordSize = BinList32::MaxWordSize;\n-\n@@ -86,1 +82,1 @@\n-  void add_block(MetaWord* p, size_t word_size);\n+  void add_block(MetaBlock bl);\n@@ -88,2 +84,2 @@\n-  \/\/ Retrieve a block of at least requested_word_size.\n-  MetaWord* remove_block(size_t requested_word_size);\n+  \/\/ Retrieve a block of at least requested_word_size. May be larger.\n+  MetaBlock remove_block(size_t requested_word_size);\n","filename":"src\/hotspot\/share\/memory\/metaspace\/freeBlocks.hpp","additions":3,"deletions":7,"binary":false,"changes":10,"status":"modified"},{"patch":"@@ -0,0 +1,76 @@\n+\/*\n+ * Copyright (c) 2023 Red Hat, Inc. All rights reserved.\n+ * Copyright (c) 2023, Oracle and\/or its affiliates. All rights reserved.\n+ * DO NOT ALTER OR REMOVE COPYRIGHT NOTICES OR THIS FILE HEADER.\n+ *\n+ * This code is free software; you can redistribute it and\/or modify it\n+ * under the terms of the GNU General Public License version 2 only, as\n+ * published by the Free Software Foundation.\n+ *\n+ * This code is distributed in the hope that it will be useful, but WITHOUT\n+ * ANY WARRANTY; without even the implied warranty of MERCHANTABILITY or\n+ * FITNESS FOR A PARTICULAR PURPOSE.  See the GNU General Public License\n+ * version 2 for more details (a copy is included in the LICENSE file that\n+ * accompanied this code).\n+ *\n+ * You should have received a copy of the GNU General Public License version\n+ * 2 along with this work; if not, write to the Free Software Foundation,\n+ * Inc., 51 Franklin St, Fifth Floor, Boston, MA 02110-1301 USA.\n+ *\n+ * Please contact Oracle, 500 Oracle Parkway, Redwood Shores, CA 94065 USA\n+ * or visit www.oracle.com if you need additional information or have any\n+ * questions.\n+ *\n+ *\/\n+\n+#ifndef SHARE_MEMORY_METASPACE_METABLOCK_HPP\n+#define SHARE_MEMORY_METASPACE_METABLOCK_HPP\n+\n+#include \"utilities\/globalDefinitions.hpp\"\n+\n+class outputStream;\n+\n+namespace metaspace {\n+\n+\/\/ Tiny structure to be passed by value\n+class MetaBlock {\n+\n+  MetaWord* _base;\n+  size_t _word_size;\n+\n+public:\n+\n+  MetaBlock(MetaWord* p, size_t word_size) :\n+    _base(word_size == 0 ? nullptr : p), _word_size(word_size) {}\n+  MetaBlock() : MetaBlock(nullptr, 0) {}\n+\n+  MetaWord* base() const { return _base; }\n+  const MetaWord* end() const { return _base + _word_size; }\n+  size_t word_size() const { return _word_size; }\n+  bool is_empty() const { return _base == nullptr; }\n+  bool is_nonempty() const { return _base != nullptr; }\n+  void reset() { _base = nullptr; _word_size = 0; }\n+\n+  bool operator==(const MetaBlock& rhs) const {\n+    return base() == rhs.base() &&\n+           word_size() == rhs.word_size();\n+  }\n+\n+  \/\/ Split off tail block.\n+  inline MetaBlock split_off_tail(size_t tailsize);\n+\n+  DEBUG_ONLY(inline void verify() const;)\n+\n+  \/\/ Convenience functions\n+  inline bool is_aligned_base(size_t alignment_words) const;\n+  inline bool is_aligned_size(size_t alignment_words) const;\n+\n+  void print_on(outputStream* st) const;\n+};\n+\n+#define METABLOCKFORMAT                 \"block (@\" PTR_FORMAT \" word size \" SIZE_FORMAT \")\"\n+#define METABLOCKFORMATARGS(__block__)  p2i((__block__).base()), (__block__).word_size()\n+\n+} \/\/ namespace metaspace\n+\n+#endif \/\/ SHARE_MEMORY_METASPACE_METABLOCK_HPP\n","filename":"src\/hotspot\/share\/memory\/metaspace\/metablock.hpp","additions":76,"deletions":0,"binary":false,"changes":76,"status":"added"},{"patch":"@@ -0,0 +1,87 @@\n+\/*\n+ * Copyright (c) 2023 Red Hat, Inc. All rights reserved.\n+ * Copyright (c) 2023, Oracle and\/or its affiliates. All rights reserved.\n+ * DO NOT ALTER OR REMOVE COPYRIGHT NOTICES OR THIS FILE HEADER.\n+ *\n+ * This code is free software; you can redistribute it and\/or modify it\n+ * under the terms of the GNU General Public License version 2 only, as\n+ * published by the Free Software Foundation.\n+ *\n+ * This code is distributed in the hope that it will be useful, but WITHOUT\n+ * ANY WARRANTY; without even the implied warranty of MERCHANTABILITY or\n+ * FITNESS FOR A PARTICULAR PURPOSE.  See the GNU General Public License\n+ * version 2 for more details (a copy is included in the LICENSE file that\n+ * accompanied this code).\n+ *\n+ * You should have received a copy of the GNU General Public License version\n+ * 2 along with this work; if not, write to the Free Software Foundation,\n+ * Inc., 51 Franklin St, Fifth Floor, Boston, MA 02110-1301 USA.\n+ *\n+ * Please contact Oracle, 500 Oracle Parkway, Redwood Shores, CA 94065 USA\n+ * or visit www.oracle.com if you need additional information or have any\n+ * questions.\n+ *\n+ *\/\n+\n+#ifndef SHARE_MEMORY_METASPACE_METABLOCK_INLINE_HPP\n+#define SHARE_MEMORY_METASPACE_METABLOCK_INLINE_HPP\n+\n+#include \"memory\/metaspace\/metablock.hpp\"\n+#include \"utilities\/globalDefinitions.hpp\"\n+#include \"utilities\/align.hpp\"\n+#include \"utilities\/ostream.hpp\"\n+#include \"utilities\/debug.hpp\"\n+\n+class outputStream;\n+\n+namespace metaspace {\n+\n+inline MetaBlock MetaBlock::split_off_tail(size_t tailsize) {\n+  if (is_empty() || tailsize == 0) {\n+    return MetaBlock();\n+  }\n+  assert(tailsize <= _word_size, \"invalid split point for block \"\n+         METABLOCKFORMAT \": %zu\", METABLOCKFORMATARGS(*this), tailsize);\n+  const size_t new_size = _word_size - tailsize;\n+  MetaBlock tail(_base + new_size, tailsize);\n+  _word_size = new_size;\n+  if (_word_size == 0) {\n+    _base = nullptr;\n+  }\n+  return tail;\n+}\n+\n+inline void MetaBlock::print_on(outputStream* st) const {\n+  st->print(METABLOCKFORMAT, METABLOCKFORMATARGS(*this));\n+}\n+\n+\/\/ Convenience functions\n+inline bool MetaBlock::is_aligned_base(size_t alignment_words) const {\n+  return is_aligned(_base, alignment_words * BytesPerWord);\n+}\n+\n+inline bool MetaBlock::is_aligned_size(size_t alignment_words) const {\n+  return is_aligned(_word_size, alignment_words);\n+}\n+\n+\/\/ some convenience asserts\n+#define assert_block_base_aligned(block, alignment_words) \\\n+  assert(block.is_aligned_base(alignment_words), \"Block wrong base alignment \" METABLOCKFORMAT, METABLOCKFORMATARGS(block));\n+\n+#define assert_block_size_aligned(block, alignment_words) \\\n+  assert(block.is_aligned_size(alignment_words), \"Block wrong size alignment \" METABLOCKFORMAT, METABLOCKFORMATARGS(block));\n+\n+#define assert_block_larger_or_equal(block, x) \\\n+  assert(block.word_size() >= x, \"Block too small \" METABLOCKFORMAT, METABLOCKFORMATARGS(block));\n+\n+#ifdef ASSERT\n+inline void MetaBlock::verify() const {\n+  assert( (_base == nullptr && _word_size == 0) ||\n+          (_base != nullptr && _word_size > 0),\n+          \"block invalid \" METABLOCKFORMAT, METABLOCKFORMATARGS(*this));\n+}\n+#endif\n+\n+} \/\/ namespace metaspace\n+\n+#endif \/\/ SHARE_MEMORY_METASPACE_METABLOCK_INLINE_HPP\n","filename":"src\/hotspot\/share\/memory\/metaspace\/metablock.inline.hpp","additions":87,"deletions":0,"binary":false,"changes":87,"status":"added"},{"patch":"@@ -33,0 +33,1 @@\n+#include \"memory\/metaspace\/metablock.inline.hpp\"\n@@ -37,0 +38,1 @@\n+#include \"memory\/metaspace\/metaspaceContext.hpp\"\n@@ -59,3 +61,4 @@\n-\/\/ Given a chunk, add its remaining free committed space to the free block list.\n-void MetaspaceArena::salvage_chunk(Metachunk* c) {\n-  size_t remaining_words = c->free_below_committed_words();\n+\/\/ Given a chunk, return the committed remainder of this chunk.\n+MetaBlock MetaspaceArena::salvage_chunk(Metachunk* c) {\n+  MetaBlock result;\n+  const size_t remaining_words = c->free_below_committed_words();\n@@ -68,1 +71,0 @@\n-    _total_used_words_counter->increment_by(remaining_words);\n@@ -70,1 +72,1 @@\n-    add_allocation_to_fbl(ptr, remaining_words);\n+    result = MetaBlock(ptr, remaining_words);\n@@ -77,0 +79,1 @@\n+  return result;\n@@ -100,5 +103,4 @@\n-void MetaspaceArena::add_allocation_to_fbl(MetaWord* p, size_t word_size) {\n-  assert(p != nullptr, \"p is null\");\n-  assert_is_aligned_metaspace_pointer(p);\n-  assert(word_size > 0, \"zero sized\");\n-\n+void MetaspaceArena::add_allocation_to_fbl(MetaBlock bl) {\n+  assert(bl.is_nonempty(), \"Sanity\");\n+  assert_block_base_aligned(bl, allocation_alignment_words());\n+  assert_block_size_aligned(bl, Metaspace::min_allocation_alignment_words);\n@@ -108,1 +110,1 @@\n-  _fbl->add_block(p, word_size);\n+  _fbl->add_block(bl);\n@@ -111,4 +113,6 @@\n-MetaspaceArena::MetaspaceArena(ChunkManager* chunk_manager, const ArenaGrowthPolicy* growth_policy,\n-                               SizeAtomicCounter* total_used_words_counter,\n-                               const char* name) :\n-  _chunk_manager(chunk_manager),\n+MetaspaceArena::MetaspaceArena(MetaspaceContext* context,\n+               const ArenaGrowthPolicy* growth_policy,\n+               size_t allocation_alignment_words,\n+               const char* name) :\n+  _allocation_alignment_words(allocation_alignment_words),\n+  _chunk_manager(context->cm()),\n@@ -118,1 +122,1 @@\n-  _total_used_words_counter(total_used_words_counter),\n+  _total_used_words_counter(context->used_words_counter()),\n@@ -124,1 +128,7 @@\n-  UL(debug, \": born.\");\n+  \/\/ Check arena allocation alignment\n+  assert(is_power_of_2(_allocation_alignment_words) &&\n+         _allocation_alignment_words >= Metaspace::min_allocation_alignment_words &&\n+         _allocation_alignment_words <= chunklevel::MIN_CHUNK_WORD_SIZE,\n+         \"Invalid alignment: %zu\", _allocation_alignment_words);\n+\n+  UL(debug, \"born.\");\n@@ -153,1 +163,1 @@\n-  UL2(info, \"returned %d chunks, total capacity \" SIZE_FORMAT \" words.\",\n+  UL2(debug, \"returned %d chunks, total capacity \" SIZE_FORMAT \" words.\",\n@@ -218,1 +228,1 @@\n-MetaWord* MetaspaceArena::allocate(size_t requested_word_size) {\n+MetaBlock MetaspaceArena::allocate(size_t requested_word_size, MetaBlock& wastage) {\n@@ -221,1 +231,0 @@\n-  MetaWord* p = nullptr;\n@@ -224,0 +233,3 @@\n+  MetaBlock result;\n+  bool taken_from_fbl = false;\n+\n@@ -226,2 +238,8 @@\n-    p = _fbl->remove_block(aligned_word_size);\n-    if (p != nullptr) {\n+    result = _fbl->remove_block(aligned_word_size);\n+    if (result.is_nonempty()) {\n+      assert_block_larger_or_equal(result, aligned_word_size);\n+      assert_block_base_aligned(result, allocation_alignment_words());\n+      assert_block_size_aligned(result, Metaspace::min_allocation_alignment_words);\n+      \/\/ Split off wastage\n+      wastage = result.split_off_tail(result.word_size() - aligned_word_size);\n+      \/\/ Stats, logging\n@@ -229,3 +247,2 @@\n-      UL2(trace, \"returning \" PTR_FORMAT \" - taken from fbl (now: %d, \" SIZE_FORMAT \").\",\n-          p2i(p), _fbl->count(), _fbl->total_size());\n-      assert_is_aligned_metaspace_pointer(p);\n+      UL2(trace, \"returning \" METABLOCKFORMAT \" with wastage \" METABLOCKFORMAT \" - taken from fbl (now: %d, \" SIZE_FORMAT \").\",\n+          METABLOCKFORMATARGS(result), METABLOCKFORMATARGS(wastage), _fbl->count(), _fbl->total_size());\n@@ -233,3 +250,2 @@\n-      \/\/ therefore we have no need to adjust any usage counters (see epilogue of allocate_inner())\n-      \/\/ and can just return here.\n-      return p;\n+      \/\/ therefore we don't need to adjust any usage counters (see epilogue of allocate_inner()).\n+      taken_from_fbl = true;\n@@ -239,2 +255,15 @@\n-  \/\/ Primary allocation\n-  p = allocate_inner(aligned_word_size);\n+  if (result.is_empty()) {\n+    \/\/ Free-block allocation failed; we allocate from the arena.\n+    \/\/ These allocations are fenced.\n+    size_t plus_fence = 0;\n+  #ifdef ASSERT\n+    static constexpr size_t fence_word_size = sizeof(Fence) \/ BytesPerWord;\n+    STATIC_ASSERT(is_aligned(fence_word_size, Metaspace::min_allocation_alignment_words));\n+    if (Settings::use_allocation_guard() &&\n+        aligned_word_size <= Metaspace::max_allocation_word_size() - fence_word_size) {\n+      plus_fence = fence_word_size;\n+    }\n+  #endif\n+\n+    \/\/ Allocate from arena proper\n+    result = allocate_inner(aligned_word_size + plus_fence, wastage);\n@@ -242,12 +271,5 @@\n-#ifdef ASSERT\n-  \/\/ Fence allocation\n-  if (p != nullptr && Settings::use_allocation_guard()) {\n-    STATIC_ASSERT(is_aligned(sizeof(Fence), BytesPerWord));\n-    MetaWord* guard = allocate_inner(sizeof(Fence) \/ BytesPerWord);\n-    if (guard != nullptr) {\n-      \/\/ Ignore allocation errors for the fence to keep coding simple. If this\n-      \/\/ happens (e.g. because right at this time we hit the Metaspace GC threshold)\n-      \/\/ we miss adding this one fence. Not a big deal. Note that his would\n-      \/\/ be pretty rare. Chances are much higher the primary allocation above\n-      \/\/ would have already failed).\n-      Fence* f = new(guard) Fence(_first_fence);\n+  #ifdef ASSERT\n+    if (result.is_nonempty() && plus_fence > 0) {\n+      assert(result.word_size() == aligned_word_size + plus_fence, \"Sanity\");\n+      MetaBlock fenceblock = result.split_off_tail(fence_word_size);\n+      Fence* f = new(fenceblock.base()) Fence(_first_fence);\n@@ -256,0 +278,18 @@\n+  #endif\n+  } \/\/ End: allocate from arena proper\n+\n+  \/\/ Logging\n+  if (result.is_nonempty()) {\n+    LogTarget(Trace, metaspace) lt;\n+    if (lt.is_enabled()) {\n+      LogStream ls(lt);\n+      ls.print(LOGFMT \": returning \" METABLOCKFORMAT \" taken from %s, \", LOGFMT_ARGS,\n+               METABLOCKFORMATARGS(result), (taken_from_fbl ? \"fbl\" : \"arena\"));\n+      if (wastage.is_empty()) {\n+        ls.print(\"no wastage\");\n+      } else {\n+        ls.print(\"wastage \" METABLOCKFORMAT, METABLOCKFORMATARGS(wastage));\n+      }\n+    }\n+  } else {\n+    UL(info, \"allocation failed, returned null.\");\n@@ -257,0 +297,16 @@\n+\n+  \/\/ Final sanity checks\n+#ifdef ASSERT\n+    result.verify();\n+    wastage.verify();\n+    if (result.is_nonempty()) {\n+      assert(result.word_size() == aligned_word_size &&\n+             is_aligned(result.base(), _allocation_alignment_words * BytesPerWord),\n+             \"result bad or unaligned: \" METABLOCKFORMAT \".\", METABLOCKFORMATARGS(result));\n+    }\n+    if (wastage.is_nonempty()) {\n+      assert(wastage.is_empty() ||\n+             (wastage.is_aligned_base(Metaspace::min_allocation_alignment_words) &&\n+              wastage.is_aligned_size(Metaspace::min_allocation_alignment_words)),\n+             \"Misaligned wastage: \" METABLOCKFORMAT\".\", METABLOCKFORMATARGS(wastage));\n+    }\n@@ -259,1 +315,1 @@\n-  return p;\n+  return result;\n@@ -263,2 +319,1 @@\n-MetaWord* MetaspaceArena::allocate_inner(size_t word_size) {\n-  assert_is_aligned(word_size, metaspace::AllocationAlignmentWordSize);\n+MetaBlock MetaspaceArena::allocate_inner(size_t word_size, MetaBlock& wastage) {\n@@ -266,1 +321,1 @@\n-  MetaWord* p = nullptr;\n+  MetaBlock result;\n@@ -269,0 +324,1 @@\n+  size_t alignment_gap_size = 0;\n@@ -271,1 +327,0 @@\n-\n@@ -274,0 +329,4 @@\n+    const MetaWord* const chunk_top = current_chunk()->top();\n+    alignment_gap_size = align_up(chunk_top, _allocation_alignment_words * BytesPerWord) - chunk_top;\n+    const size_t word_size_plus_alignment = word_size + alignment_gap_size;\n+\n@@ -276,2 +335,2 @@\n-    if (current_chunk()->free_words() < word_size) {\n-      if (!attempt_enlarge_current_chunk(word_size)) {\n+    if (current_chunk()->free_words() < word_size_plus_alignment) {\n+      if (!attempt_enlarge_current_chunk(word_size_plus_alignment)) {\n@@ -289,2 +348,2 @@\n-      if (!current_chunk()->ensure_committed_additional(word_size)) {\n-        UL2(info, \"commit failure (requested size: \" SIZE_FORMAT \")\", word_size);\n+      if (!current_chunk()->ensure_committed_additional(word_size_plus_alignment)) {\n+        UL2(info, \"commit failure (requested size: \" SIZE_FORMAT \")\", word_size_plus_alignment);\n@@ -297,2 +356,8 @@\n-      p = current_chunk()->allocate(word_size);\n-      assert(p != nullptr, \"Allocation from chunk failed.\");\n+      MetaWord* const p_gap = current_chunk()->allocate(word_size_plus_alignment);\n+      assert(p_gap != nullptr, \"Allocation from chunk failed.\");\n+      MetaWord* const p_user_allocation = p_gap + alignment_gap_size;\n+      result = MetaBlock(p_user_allocation, word_size);\n+      if (alignment_gap_size > 0) {\n+        NOT_LP64(assert(alignment_gap_size >= AllocationAlignmentWordSize, \"Sanity\"));\n+        wastage = MetaBlock(p_gap, alignment_gap_size);\n+      }\n@@ -302,1 +367,1 @@\n-  if (p == nullptr) {\n+  if (result.is_empty()) {\n@@ -316,1 +381,1 @@\n-        salvage_chunk(current_chunk());\n+        wastage = salvage_chunk(current_chunk());\n@@ -322,2 +387,4 @@\n-      \/\/ Now, allocate from that chunk. That should work.\n-      p = current_chunk()->allocate(word_size);\n+      \/\/ Now, allocate from that chunk. That should work. Note that the resulting allocation\n+      \/\/ is guaranteed to be aligned to arena alignment, since arena alignment cannot be larger\n+      \/\/ than smallest chunk size, and chunk starts are aligned by their size (buddy allocation).\n+      MetaWord* const p = current_chunk()->allocate(word_size);\n@@ -325,0 +392,1 @@\n+      result = MetaBlock(p, word_size);\n@@ -330,1 +398,1 @@\n-  if (p == nullptr) {\n+  if (result.is_empty()) {\n@@ -334,1 +402,1 @@\n-    _total_used_words_counter->increment_by(word_size);\n+    _total_used_words_counter->increment_by(word_size + wastage.word_size());\n@@ -339,3 +407,1 @@\n-  if (p == nullptr) {\n-    UL(info, \"allocation failed, returned null.\");\n-  } else {\n+  if (result.is_nonempty()) {\n@@ -344,1 +410,0 @@\n-    UL2(trace, \"returning \" PTR_FORMAT \".\", p2i(p));\n@@ -347,1 +412,12 @@\n-  assert_is_aligned_metaspace_pointer(p);\n+#ifdef ASSERT\n+  if (wastage.is_nonempty()) {\n+    \/\/ Wastage from arena allocations only occurs if either or both are true:\n+    \/\/ - it is too small to hold the requested allocation words\n+    \/\/ - it is misaligned\n+    assert(!wastage.is_aligned_base(allocation_alignment_words()) ||\n+           wastage.word_size() < word_size,\n+           \"Unexpected wastage: \" METABLOCKFORMAT \", arena alignment: %zu, allocation word size: %zu\",\n+           METABLOCKFORMATARGS(wastage), allocation_alignment_words(), word_size);\n+    wastage.verify();\n+  }\n+#endif \/\/ ASSERT\n@@ -349,1 +425,1 @@\n-  return p;\n+  return result;\n@@ -354,19 +430,12 @@\n-void MetaspaceArena::deallocate(MetaWord* p, size_t word_size) {\n-  \/\/ At this point a current chunk must exist since we only deallocate if we did allocate before.\n-  assert(current_chunk() != nullptr, \"stray deallocation?\");\n-  assert(is_valid_area(p, word_size),\n-         \"Pointer range not part of this Arena and cannot be deallocated: (\" PTR_FORMAT \"..\" PTR_FORMAT \").\",\n-         p2i(p), p2i(p + word_size));\n-\n-  UL2(trace, \"deallocating \" PTR_FORMAT \", word size: \" SIZE_FORMAT \".\",\n-      p2i(p), word_size);\n-\n-  \/\/ Only blocks that had been allocated via MetaspaceArena::allocate(size) must be handed in\n-  \/\/ to MetaspaceArena::deallocate(), and only with the same size that had been original used for allocation.\n-  \/\/ Therefore the pointer must be aligned correctly, and size can be alignment-adjusted (the latter\n-  \/\/ only matters on 32-bit):\n-  assert_is_aligned_metaspace_pointer(p);\n-  size_t raw_word_size = get_raw_word_size_for_requested_word_size(word_size);\n-\n-  add_allocation_to_fbl(p, raw_word_size);\n-\n+void MetaspaceArena::deallocate(MetaBlock block) {\n+  DEBUG_ONLY(block.verify();)\n+  \/\/ This only matters on 32-bit:\n+  \/\/ Since we always align up allocations from arena, we align up here, too.\n+#ifndef _LP64\n+  MetaBlock raw_block(block.base(), get_raw_word_size_for_requested_word_size(block.word_size()));\n+  add_allocation_to_fbl(raw_block);\n+#else\n+  add_allocation_to_fbl(block);\n+#endif\n+  UL2(trace, \"added to fbl: \" METABLOCKFORMAT \", (now: %d, \" SIZE_FORMAT \").\",\n+      METABLOCKFORMATARGS(block), _fbl->count(), _fbl->total_size());\n@@ -442,4 +511,5 @@\n-\/\/ Returns true if the area indicated by pointer and size have actually been allocated\n-\/\/ from this arena.\n-bool MetaspaceArena::is_valid_area(MetaWord* p, size_t word_size) const {\n-  assert(p != nullptr && word_size > 0, \"Sanity\");\n+\/\/ Returns true if the given block is contained in this arena\n+\/\/ Returns true if the given block is contained in this arena\n+bool MetaspaceArena::contains(MetaBlock bl) const {\n+  DEBUG_ONLY(bl.verify();)\n+  assert(bl.is_nonempty(), \"Sanity\");\n@@ -448,3 +518,3 @@\n-    assert(c->is_valid_committed_pointer(p) ==\n-           c->is_valid_committed_pointer(p + word_size - 1), \"range intersects\");\n-    found = c->is_valid_committed_pointer(p);\n+    assert(c->is_valid_committed_pointer(bl.base()) ==\n+           c->is_valid_committed_pointer(bl.end() - 1), \"range intersects\");\n+    found = c->is_valid_committed_pointer(bl.base());\n","filename":"src\/hotspot\/share\/memory\/metaspace\/metaspaceArena.cpp","additions":161,"deletions":91,"binary":false,"changes":252,"status":"modified"},{"patch":"@@ -32,0 +32,1 @@\n+#include \"memory\/metaspace\/metablock.hpp\"\n@@ -40,0 +41,2 @@\n+struct ArenaStats;\n+class MetaspaceContext;\n@@ -41,1 +44,0 @@\n-class Metachunk;\n@@ -43,0 +45,1 @@\n+class Metachunk;\n@@ -44,1 +47,0 @@\n-struct ArenaStats;\n@@ -77,0 +79,1 @@\n+  friend class MetaspaceArenaTestFriend;\n@@ -81,0 +84,2 @@\n+  const size_t _allocation_alignment_words;\n+\n@@ -129,1 +134,1 @@\n-  void add_allocation_to_fbl(MetaWord* p, size_t word_size);\n+  void add_allocation_to_fbl(MetaBlock bl);\n@@ -131,2 +136,2 @@\n-  \/\/ Given a chunk, add its remaining free committed space to the free block list.\n-  void salvage_chunk(Metachunk* c);\n+  \/\/ Given a chunk, return the committed remainder of this chunk.\n+  MetaBlock salvage_chunk(Metachunk* c);\n@@ -147,4 +152,0 @@\n-  \/\/ Returns true if the area indicated by pointer and size have actually been allocated\n-  \/\/ from this arena.\n-  DEBUG_ONLY(bool is_valid_area(MetaWord* p, size_t word_size) const;)\n-\n@@ -152,1 +153,1 @@\n-  MetaWord* allocate_inner(size_t word_size);\n+  MetaBlock allocate_inner(size_t word_size, MetaBlock& wastage);\n@@ -156,2 +157,3 @@\n-  MetaspaceArena(ChunkManager* chunk_manager, const ArenaGrowthPolicy* growth_policy,\n-                 SizeAtomicCounter* total_used_words_counter,\n+  MetaspaceArena(MetaspaceContext* context,\n+                 const ArenaGrowthPolicy* growth_policy,\n+                 size_t allocation_alignment_words,\n@@ -162,0 +164,3 @@\n+  size_t allocation_alignment_words() const { return _allocation_alignment_words; }\n+  size_t allocation_alignment_bytes() const { return allocation_alignment_words() * BytesPerWord; }\n+\n@@ -163,6 +168,5 @@\n-  \/\/ 1) Attempt to allocate from the dictionary of deallocated blocks.\n-  \/\/ 2) Attempt to allocate from the current chunk.\n-  \/\/ 3) Attempt to enlarge the current chunk in place if it is too small.\n-  \/\/ 4) Attempt to get a new chunk and allocate from that chunk.\n-  \/\/ At any point, if we hit a commit limit, we return null.\n-  MetaWord* allocate(size_t word_size);\n+  \/\/ On success, returns non-empty block of the specified word size, and\n+  \/\/ possibly a wastage block that is the result of alignment operations.\n+  \/\/ On failure, returns an empty block. Failure may happen if we hit a\n+  \/\/ commit limit.\n+  MetaBlock allocate(size_t word_size, MetaBlock& wastage);\n@@ -172,1 +176,1 @@\n-  void deallocate(MetaWord* p, size_t word_size);\n+  void deallocate(MetaBlock bl);\n@@ -186,0 +190,2 @@\n+  \/\/ Returns true if the given block is contained in this arena\n+  DEBUG_ONLY(bool contains(MetaBlock bl) const;)\n","filename":"src\/hotspot\/share\/memory\/metaspace\/metaspaceArena.hpp","additions":25,"deletions":19,"binary":false,"changes":44,"status":"modified"},{"patch":"@@ -45,2 +45,1 @@\n-\/\/ Klass* structures need to be aligned to KlassAlignmentInBytes, but since that is\n-\/\/ 64-bit, we don't need special handling for allocating Klass*.\n+\/\/ Klass* structures need to be aligned to Klass* alignment,\n@@ -51,1 +50,0 @@\n-STATIC_ASSERT(AllocationAlignmentByteSize == (size_t)KlassAlignmentInBytes);\n","filename":"src\/hotspot\/share\/memory\/metaspace\/metaspaceCommon.hpp","additions":1,"deletions":3,"binary":false,"changes":4,"status":"modified"},{"patch":"@@ -78,0 +78,12 @@\n+size_t MetaspaceContext::used_words() const {\n+  return _used_words_counter.get();\n+}\n+\n+size_t MetaspaceContext::committed_words() const {\n+  return _vslist->committed_words();\n+}\n+\n+size_t MetaspaceContext::reserved_words() const {\n+  return _vslist->reserved_words();\n+}\n+\n","filename":"src\/hotspot\/share\/memory\/metaspace\/metaspaceContext.cpp","additions":12,"deletions":0,"binary":false,"changes":12,"status":"modified"},{"patch":"@@ -30,0 +30,1 @@\n+#include \"memory\/metaspace\/counters.hpp\"\n@@ -64,0 +65,1 @@\n+  SizeAtomicCounter _used_words_counter;\n@@ -81,2 +83,3 @@\n-  VirtualSpaceList* vslist() { return _vslist; }\n-  ChunkManager* cm() { return _cm; }\n+  VirtualSpaceList* vslist()                    { return _vslist; }\n+  ChunkManager* cm()                            { return _cm; }\n+  SizeAtomicCounter* used_words_counter()       { return &_used_words_counter; }\n@@ -106,0 +109,3 @@\n+  size_t used_words() const;\n+  size_t committed_words() const;\n+  size_t reserved_words() const;\n","filename":"src\/hotspot\/share\/memory\/metaspace\/metaspaceContext.hpp","additions":8,"deletions":2,"binary":false,"changes":10,"status":"modified"},{"patch":"@@ -42,0 +42,1 @@\n+#include \"oops\/compressedKlass.hpp\"\n@@ -121,0 +122,3 @@\n+#ifdef _LP64\n+  CompressedKlassPointers::print_mode(out);\n+#endif\n@@ -328,1 +332,1 @@\n-  out->print(\"(percentages refer to total committed size \");\n+  out->print(\" (percentages refer to total committed size \");\n","filename":"src\/hotspot\/share\/memory\/metaspace\/metaspaceReporter.cpp","additions":5,"deletions":1,"binary":false,"changes":6,"status":"modified"},{"patch":"@@ -189,1 +189,0 @@\n-\n@@ -196,3 +195,0 @@\n-  \/\/ Deallocated allocations still count as used\n-  assert(total_used >= _free_blocks_word_size,\n-         \"Sanity\");\n","filename":"src\/hotspot\/share\/memory\/metaspace\/metaspaceStatistics.cpp","additions":0,"deletions":4,"binary":false,"changes":4,"status":"modified"},{"patch":"@@ -28,1 +28,1 @@\n-#include \"memory\/metaspace\/counters.hpp\"\n+#include \"memory\/metaspace\/metaspaceContext.hpp\"\n@@ -34,3 +34,0 @@\n-SizeAtomicCounter RunningCounters::_used_class_counter;\n-SizeAtomicCounter RunningCounters::_used_nonclass_counter;\n-\n@@ -73,1 +70,2 @@\n-  return _used_class_counter.get();\n+  const MetaspaceContext* context = MetaspaceContext::context_class();\n+  return context != nullptr ? context->used_words() : 0;\n@@ -77,1 +75,1 @@\n-  return _used_nonclass_counter.get();\n+  return MetaspaceContext::context_nonclass()->used_words();\n","filename":"src\/hotspot\/share\/memory\/metaspace\/runningCounters.cpp","additions":4,"deletions":6,"binary":false,"changes":10,"status":"modified"},{"patch":"@@ -30,1 +30,0 @@\n-#include \"memory\/metaspace\/counters.hpp\"\n@@ -35,6 +34,1 @@\n-class RunningCounters : public AllStatic {\n-\n-  static SizeAtomicCounter _used_class_counter;\n-  static SizeAtomicCounter _used_nonclass_counter;\n-\n-public:\n+struct RunningCounters : public AllStatic {\n@@ -68,4 +62,0 @@\n-  \/\/ Direct access to the counters.\n-  static SizeAtomicCounter* used_nonclass_counter()     { return &_used_nonclass_counter; }\n-  static SizeAtomicCounter* used_class_counter()        { return &_used_class_counter; }\n-\n","filename":"src\/hotspot\/share\/memory\/metaspace\/runningCounters.hpp","additions":1,"deletions":11,"binary":false,"changes":12,"status":"modified"},{"patch":"@@ -57,1 +57,6 @@\n-  return _arena->allocate(word_size);\n+  MetaBlock result, wastage;\n+  result = _arena->allocate(word_size, wastage);\n+  if (wastage.is_nonempty()) {\n+    _arena->deallocate(wastage);\n+  }\n+  return result.base();\n@@ -62,1 +67,1 @@\n-  return _arena->deallocate(p, word_size);\n+  _arena->deallocate(MetaBlock(p, word_size));\n@@ -73,1 +78,0 @@\n-  _used_words_counter(),\n@@ -106,1 +110,1 @@\n-    arena = new MetaspaceArena(_context->cm(), growth_policy, &_used_words_counter, _name);\n+    arena = new MetaspaceArena(_context, growth_policy, Metaspace::min_allocation_alignment_words, _name);\n@@ -127,0 +131,13 @@\n+size_t MetaspaceTestContext::used_words() const {\n+  return _context->used_words_counter()->get();\n+}\n+\n+size_t MetaspaceTestContext::committed_words() const {\n+  assert(_commit_limiter.committed_words() == _context->committed_words(), \"Sanity\");\n+  return _context->committed_words();\n+}\n+\n+size_t MetaspaceTestContext::reserved_words() const {\n+  return _context->reserved_words();\n+}\n+\n","filename":"src\/hotspot\/share\/memory\/metaspace\/testHelpers.cpp","additions":21,"deletions":4,"binary":false,"changes":25,"status":"modified"},{"patch":"@@ -78,1 +78,0 @@\n-  SizeAtomicCounter _used_words_counter;\n@@ -101,0 +100,1 @@\n+  MetaspaceContext* context() const           { return _context; }\n@@ -107,3 +107,3 @@\n-  \/\/ Convenience function to retrieve total committed\/used words\n-  size_t used_words() const       { return _used_words_counter.get(); }\n-  size_t committed_words() const  { return _commit_limiter.committed_words(); }\n+  size_t used_words() const;\n+  size_t committed_words() const;\n+  size_t reserved_words() const;\n","filename":"src\/hotspot\/share\/memory\/metaspace\/testHelpers.hpp","additions":4,"deletions":4,"binary":false,"changes":8,"status":"modified"},{"patch":"@@ -28,1 +28,2 @@\n-#include \"oops\/compressedKlass.hpp\"\n+#include \"oops\/klass.hpp\"\n+#include \"oops\/compressedKlass.inline.hpp\"\n@@ -30,0 +31,1 @@\n+#include \"runtime\/java.hpp\"\n@@ -35,3 +37,22 @@\n-address CompressedKlassPointers::_base = nullptr;\n-int CompressedKlassPointers::_shift = 0;\n-size_t CompressedKlassPointers::_range = 0;\n+int CompressedKlassPointers::_tiny_cp = -1;\n+int CompressedKlassPointers::_narrow_klass_pointer_bits = -1;\n+int CompressedKlassPointers::_max_shift = -1;\n+#ifdef ASSERT\n+address CompressedKlassPointers::_klass_range_start = (address)-1;\n+address CompressedKlassPointers::_klass_range_end = (address)-1;\n+narrowKlass CompressedKlassPointers::_lowest_valid_narrow_klass_id = (narrowKlass)-1;\n+narrowKlass CompressedKlassPointers::_highest_valid_narrow_klass_id = (narrowKlass)-1;\n+#endif\n+\n+address CompressedKlassPointers::_base = (address)-1;\n+int CompressedKlassPointers::_shift = -1;\n+size_t CompressedKlassPointers::_range = (size_t)-1;\n+\n+\/\/ The maximum allowed length of the Klass range (the address range engulfing\n+\/\/ CDS + class space) must not exceed 32-bit.\n+\/\/ There is a theoretical limit of: must not exceed the size of a fully-shifted\n+\/\/ narrow Klass pointer, which would be 32 + 3 = 35 bits in legacy mode;\n+\/\/ however, keeping this size below 32-bit allows us to use decoding techniques\n+\/\/ like 16-bit moves into the third quadrant on some architectures, and keeps\n+\/\/ the code less complex. 32-bit have always been enough for CDS+class space.\n+static constexpr size_t max_klass_range_size = 4 * G;\n@@ -41,4 +62,10 @@\n-#ifdef ASSERT\n-void CompressedKlassPointers::assert_is_valid_encoding(address addr, size_t len, address base, int shift) {\n-  assert(base + nth_bit(32 + shift) >= addr + len, \"Encoding (base=\" PTR_FORMAT \", shift=%d) does not \"\n-         \"fully cover the class range \" PTR_FORMAT \"-\" PTR_FORMAT, p2i(base), shift, p2i(addr), p2i(addr + len));\n+void CompressedKlassPointers::pre_initialize() {\n+  if (UseCompactObjectHeaders) {\n+    _tiny_cp = 1;\n+    _narrow_klass_pointer_bits = narrow_klass_pointer_bits_tinycp;\n+    _max_shift = max_shift_tinycp;\n+  } else {\n+    _tiny_cp = 0;\n+    _narrow_klass_pointer_bits = narrow_klass_pointer_bits_legacy;\n+    _max_shift = max_shift_legacy;\n+  }\n@@ -46,0 +73,60 @@\n+\n+#ifdef ASSERT\n+void CompressedKlassPointers::sanity_check_after_initialization() {\n+  \/\/ In expectation of an assert, prepare condensed info to be printed with the assert.\n+  char tmp[256];\n+  os::snprintf(tmp, sizeof(tmp), PTR_FORMAT \" \" PTR_FORMAT \" \" PTR_FORMAT \" %d \" SIZE_FORMAT \" %u %u\",\n+      p2i(_klass_range_start), p2i(_klass_range_end), p2i(_base), _shift, _range,\n+      _lowest_valid_narrow_klass_id, _highest_valid_narrow_klass_id);\n+#define ASSERT_HERE(cond) assert(cond, \" (%s)\", tmp);\n+#define ASSERT_HERE_2(cond, msg) assert(cond, msg \" (%s)\", tmp);\n+\n+  \/\/ There is no technical reason preventing us from using other klass pointer bit lengths,\n+  \/\/ but it should be a deliberate choice\n+  ASSERT_HERE(_narrow_klass_pointer_bits == 32 || _narrow_klass_pointer_bits == 22);\n+\n+  \/\/ All values must be inited\n+  ASSERT_HERE(_max_shift != -1);\n+  ASSERT_HERE(_klass_range_start != (address)-1);\n+  ASSERT_HERE(_klass_range_end != (address)-1);\n+  ASSERT_HERE(_lowest_valid_narrow_klass_id != (narrowKlass)-1);\n+  ASSERT_HERE(_base != (address)-1);\n+  ASSERT_HERE(_shift != -1);\n+  ASSERT_HERE(_range != (size_t)-1);\n+\n+  const size_t klab = klass_alignment_in_bytes();\n+  ASSERT_HERE(klab >= sizeof(uint64_t) && klab <= K);\n+\n+  \/\/ Check that Klass range is fully engulfed in the encoding range\n+  ASSERT_HERE(_klass_range_end > _klass_range_start);\n+\n+  const address encoding_end = _base + nth_bit(narrow_klass_pointer_bits() + _shift);\n+  ASSERT_HERE_2(_klass_range_start >= _base && _klass_range_end <= encoding_end,\n+                \"Resulting encoding range does not fully cover the class range\");\n+\n+  \/\/ Check that Klass range is aligned to Klass alignment. That should never be an issue since we mmap the\n+  \/\/ relevant regions and klass alignment - tied to smallest metachunk size of 1K - will always be smaller\n+  \/\/ than smallest page size of 4K.\n+  ASSERT_HERE_2(is_aligned(_klass_range_start, klab) && is_aligned(_klass_range_end, klab),\n+                \"Klass range must start at a properly aligned address\");\n+\n+  \/\/ Check that lowest and highest possible narrowKlass values make sense\n+  ASSERT_HERE_2(_lowest_valid_narrow_klass_id > 0, \"Null is not a valid narrowKlass\");\n+  ASSERT_HERE(_highest_valid_narrow_klass_id > _lowest_valid_narrow_klass_id);\n+\n+  Klass* k1 = decode_raw(_lowest_valid_narrow_klass_id, _base, _shift);\n+  ASSERT_HERE_2((address)k1 == _klass_range_start + klab, \"Not lowest\");\n+  narrowKlass nk1 = encode_raw(k1, _base, _shift);\n+  ASSERT_HERE_2(nk1 == _lowest_valid_narrow_klass_id, \"not reversible\");\n+\n+  Klass* k2 = decode_raw(_highest_valid_narrow_klass_id, _base, _shift);\n+  \/\/ _highest_valid_narrow_klass_id must be decoded to the highest theoretically possible\n+  \/\/ valid Klass* position in range, if we assume minimal Klass size\n+  ASSERT_HERE((address)k2 < _klass_range_end);\n+  ASSERT_HERE_2(align_up(((address)k2 + sizeof(Klass)), klab) >= _klass_range_end, \"Not highest\");\n+  narrowKlass nk2 = encode_raw(k2, _base, _shift);\n+  ASSERT_HERE_2(nk2 == _highest_valid_narrow_klass_id, \"not reversible\");\n+\n+#ifdef AARCH64\n+  \/\/ On aarch64, we never expect a shift value > 0 in legacy mode\n+  ASSERT_HERE_2(tiny_classpointer_mode() || _shift == 0, \"Shift > 0 in legacy mode?\");\n@@ -47,0 +134,15 @@\n+#undef ASSERT_HERE\n+#undef ASSERT_HERE_2\n+}\n+\n+void CompressedKlassPointers::calc_lowest_highest_narrow_klass_id() {\n+  \/\/ Given a Klass range, calculate lowest and highest narrowKlass.\n+  const size_t klab = klass_alignment_in_bytes();\n+  \/\/ Note that 0 is not a valid narrowKlass, and Metaspace prevents us for that reason from allocating at\n+  \/\/ the very start of class space. So the very first valid Klass position is start-of-range + klab.\n+  _lowest_valid_narrow_klass_id =\n+      (narrowKlass) (((uintptr_t)(_klass_range_start - _base) + klab) >> _shift);\n+  address highest_possible_klass = align_down(_klass_range_end - sizeof(Klass), klab);\n+  _highest_valid_narrow_klass_id = (narrowKlass) ((uintptr_t)(highest_possible_klass - _base) >> _shift);\n+}\n+#endif \/\/ ASSERT\n@@ -54,2 +156,7 @@\n-  const int narrow_klasspointer_bits = sizeof(narrowKlass) * 8;\n-  const size_t encoding_range_size = nth_bit(narrow_klasspointer_bits + requested_shift);\n+  if (len > max_klass_range_size) {\n+    \/\/ Class space size is limited to 3G. This can theoretically happen if the CDS archive\n+    \/\/ is larger than 1G and class space size is set to the maximum possible 3G.\n+    vm_exit_during_initialization(\"Sum of CDS archive size and class space size exceed 4 GB\");\n+  }\n+\n+  const size_t encoding_range_size = nth_bit(narrow_klass_pointer_bits() + requested_shift);\n@@ -61,1 +168,0 @@\n-  assert(encoding_range_end >= end, \"Encoding does not cover the full Klass range\");\n@@ -67,1 +173,8 @@\n-  DEBUG_ONLY(assert_is_valid_encoding(addr, len, _base, _shift);)\n+#ifdef ASSERT\n+  _klass_range_start = addr;\n+  _klass_range_end = addr + len;\n+  calc_lowest_highest_narrow_klass_id();\n+  sanity_check_after_initialization();\n+#endif\n+\n+  DEBUG_ONLY(sanity_check_after_initialization();)\n@@ -76,1 +189,5 @@\n-  return reserve_address_space_X(0, nth_bit(32), size, Metaspace::reserve_alignment(), aslr);\n+  if (tiny_classpointer_mode()) {\n+    return nullptr;\n+  }\n+  const size_t unscaled_max = nth_bit(narrow_klass_pointer_bits());\n+  return reserve_address_space_X(0, unscaled_max, size, Metaspace::reserve_alignment(), aslr);\n@@ -80,1 +197,6 @@\n-  return reserve_address_space_X(nth_bit(32), nth_bit(32 + LogKlassAlignmentInBytes), size, Metaspace::reserve_alignment(), aslr);\n+  if (tiny_classpointer_mode()) {\n+    return nullptr;\n+  }\n+  const size_t unscaled_max = nth_bit(narrow_klass_pointer_bits());\n+  const size_t zerobased_max = nth_bit(narrow_klass_pointer_bits() + max_shift());\n+  return reserve_address_space_X(unscaled_max, zerobased_max, size, Metaspace::reserve_alignment(), aslr);\n@@ -87,2 +209,0 @@\n-#if !defined(AARCH64) || defined(ZERO)\n-\/\/ On aarch64 we have an own version; all other platforms use the default version\n@@ -90,8 +210,0 @@\n-  \/\/ The default version of this code tries, in order of preference:\n-  \/\/ -unscaled    (base=0 shift=0)\n-  \/\/ -zero-based  (base=0 shift>0)\n-  \/\/ -nonzero-base (base>0 shift=0)\n-  \/\/ Note that base>0 shift>0 should never be needed, since the klass range will\n-  \/\/ never exceed 4GB.\n-  constexpr uintptr_t unscaled_max = nth_bit(32);\n-  assert(len <= unscaled_max, \"Klass range larger than 32 bits?\");\n@@ -99,1 +211,38 @@\n-  constexpr uintptr_t zerobased_max = nth_bit(32 + LogKlassAlignmentInBytes);\n+  if (len > max_klass_range_size) {\n+    \/\/ Class space size is limited to 3G. This can theoretically happen if the CDS archive\n+    \/\/ is larger than 1G and class space size is set to the maximum possible 3G.\n+    vm_exit_during_initialization(\"Sum of CDS archive size and class space size exceed 4 GB\");\n+  }\n+\n+  \/\/ Give CPU a shot at a specialized init sequence\n+#ifndef ZERO\n+  if (pd_initialize(addr, len)) {\n+    return;\n+  }\n+#endif\n+\n+  if (tiny_classpointer_mode()) {\n+\n+    \/\/ This handles the case that we - experimentally - reduce the number of\n+    \/\/ class pointer bits further, such that (shift + num bits) < 32.\n+    assert(len <= (size_t)nth_bit(narrow_klass_pointer_bits() + max_shift()),\n+           \"klass range size exceeds encoding\");\n+\n+    \/\/ In tiny classpointer mode, we don't attempt for zero-based mode.\n+    \/\/ Instead, we set the base to the start of the klass range and then try\n+    \/\/ for the smallest shift possible that still covers the whole range.\n+    \/\/ The reason is that we want to avoid, if possible, shifts larger than\n+    \/\/ a cacheline size.\n+    _base = addr;\n+    _range = len;\n+\n+    if (TinyClassPointerShift != 0) {\n+      _shift = TinyClassPointerShift;\n+    } else {\n+      constexpr int log_cacheline = 6;\n+      int s = max_shift();\n+      while (s > log_cacheline && ((size_t)nth_bit(narrow_klass_pointer_bits() + s - 1) > len)) {\n+        s--;\n+      }\n+      _shift = s;\n+    }\n@@ -101,4 +250,0 @@\n-  address const end = addr + len;\n-  if (end <= (address)unscaled_max) {\n-    _base = nullptr;\n-    _shift = 0;\n@@ -106,1 +251,12 @@\n-    if (end <= (address)zerobased_max) {\n+\n+    \/\/ In legacy mode, we try, in order of preference:\n+    \/\/ -unscaled    (base=0 shift=0)\n+    \/\/ -zero-based  (base=0 shift>0)\n+    \/\/ -nonzero-base (base>0 shift=0)\n+    \/\/ Note that base>0 shift>0 should never be needed, since the klass range will\n+    \/\/ never exceed 4GB.\n+    const uintptr_t unscaled_max = nth_bit(narrow_klass_pointer_bits());\n+    const uintptr_t zerobased_max = nth_bit(narrow_klass_pointer_bits() + max_shift());\n+\n+    address const end = addr + len;\n+    if (end <= (address)unscaled_max) {\n@@ -108,3 +264,0 @@\n-      _shift = LogKlassAlignmentInBytes;\n-    } else {\n-      _base = addr;\n@@ -112,0 +265,8 @@\n+    } else {\n+      if (end <= (address)zerobased_max) {\n+        _base = nullptr;\n+        _shift = max_shift();\n+      } else {\n+        _base = addr;\n+        _shift = 0;\n+      }\n@@ -113,0 +274,2 @@\n+    _range = end - _base;\n+\n@@ -114,1 +277,0 @@\n-  _range = end - _base;\n@@ -116,1 +278,6 @@\n-  DEBUG_ONLY(assert_is_valid_encoding(addr, len, _base, _shift);)\n+#ifdef ASSERT\n+  _klass_range_start = addr;\n+  _klass_range_end = addr + len;\n+  calc_lowest_highest_narrow_klass_id();\n+  sanity_check_after_initialization();\n+#endif\n@@ -118,1 +285,0 @@\n-#endif \/\/ !AARCH64 || ZERO\n@@ -121,0 +287,8 @@\n+  st->print_cr(\"UseCompressedClassPointers %d, UseCompactObjectHeaders %d, \"\n+               \"narrow klass pointer bits %d, max shift %d\",\n+               UseCompressedClassPointers, UseCompactObjectHeaders,\n+               _narrow_klass_pointer_bits, _max_shift);\n+  if (_base == (address)-1) {\n+    st->print_cr(\"Narrow klass encoding not initialized\");\n+    return;\n+  }\n@@ -124,0 +298,6 @@\n+#ifdef ASSERT\n+  st->print_cr(\"Klass range: [\" PTR_FORMAT \",\" PTR_FORMAT \")\",\n+               p2i(_klass_range_start), p2i(_klass_range_end));\n+  st->print_cr(\"Lowest valid nklass id: %u Highest valid nklass id: %u\",\n+               _lowest_valid_narrow_klass_id, _highest_valid_narrow_klass_id);\n+#endif\n","filename":"src\/hotspot\/share\/oops\/compressedKlass.cpp","additions":216,"deletions":36,"binary":false,"changes":252,"status":"modified"},{"patch":"@@ -37,8 +37,0 @@\n-const int LogKlassAlignmentInBytes = 3;\n-const int KlassAlignmentInBytes    = 1 << LogKlassAlignmentInBytes;\n-\n-\/\/ Maximal size of compressed class space. Above this limit compression is not possible.\n-\/\/ Also upper bound for placement of zero based class space. (Class space is further limited\n-\/\/ to be < 3G, see arguments.cpp.)\n-const  uint64_t KlassEncodingMetaspaceMax = (uint64_t(max_juint) + 1) << LogKlassAlignmentInBytes;\n-\n@@ -49,0 +41,19 @@\n+  \/\/ Tiny-class-pointer mode\n+  static int _tiny_cp; \/\/ -1, 0=true, 1=false\n+\n+  \/\/ We use a different narrow Klass pointer geometry depending on\n+  \/\/ whether we run in standard mode or in compact-object-header-mode (Lilliput):\n+  \/\/ In Lilliput, we use smaller-than-32-bit class pointers (\"tiny classpointer mode\")\n+\n+  \/\/ Narrow klass pointer bits for an unshifted narrow Klass pointer.\n+  static constexpr int narrow_klass_pointer_bits_legacy = 32;\n+  static constexpr int narrow_klass_pointer_bits_tinycp = 22;\n+\n+  static int _narrow_klass_pointer_bits;\n+\n+  \/\/ The maximum shift we can use for standard mode and for TinyCP mode\n+  static constexpr int max_shift_legacy = 3;\n+  static constexpr int max_shift_tinycp = 10;\n+\n+  static int _max_shift;\n+\n@@ -65,1 +76,20 @@\n-  DEBUG_ONLY(static void assert_is_valid_encoding(address addr, size_t len, address base, int shift);)\n+  \/\/ Returns the highest address expressable with an unshifted narrow Klass pointer\n+  inline static uintptr_t highest_unscaled_address();\n+\n+  static bool pd_initialize(address addr, size_t len);\n+\n+#ifdef ASSERT\n+  \/\/ For sanity checks: Klass range\n+  static address _klass_range_start;\n+  static address _klass_range_end;\n+  \/\/ For sanity checks: lowest, highest valid narrow klass ids != null\n+  static narrowKlass _lowest_valid_narrow_klass_id;\n+  static narrowKlass _highest_valid_narrow_klass_id;\n+  static void calc_lowest_highest_narrow_klass_id();\n+  static void sanity_check_after_initialization();\n+#endif \/\/ ASSERT\n+\n+  template <typename T>\n+  static inline void check_init(T var) {\n+    assert(var != (T)-1, \"Not yet initialized\");\n+  }\n@@ -69,0 +99,31 @@\n+  \/\/ Initialization sequence:\n+  \/\/ 1) Parse arguments. The following arguments take a role:\n+  \/\/      - UseCompressedClassPointers\n+  \/\/      - UseCompactObjectHeaders\n+  \/\/      - Xshare on off dump\n+  \/\/      - CompressedClassSpaceSize\n+  \/\/ 2) call pre_initialize(): depending on UseCompactObjectHeaders, defines the limits of narrow Klass pointer\n+  \/\/    geometry (how many bits, the max. possible shift)\n+  \/\/ 3) .. from here on, narrow_klass_pointer_bits() and max_shift() can be used\n+  \/\/ 4) call reserve_address_space_for_compressed_classes() either from CDS initialization or, if CDS is off,\n+  \/\/    from metaspace initialization. Reserves space for class space + CDS, attempts to reserve such that\n+  \/\/    we later can use a \"good\" encoding scheme. Reservation is highly CPU-specific.\n+  \/\/ 5) Initialize the narrow Klass encoding scheme by determining encoding base and shift:\n+  \/\/   5a) if CDS=on: Calls initialize_for_given_encoding() with the reservation base from step (4) and the\n+  \/\/       CDS-intrinsic setting for shift; here, we don't have any freedom to deviate from the base.\n+  \/\/   5b) if CDS=off: Calls initialize() - here, we have more freedom and, if we want, can choose an encoding\n+  \/\/       base that differs from the reservation base from step (4). That allows us, e.g., to later use\n+  \/\/       zero-based encoding.\n+  \/\/ 6) ... from now on, we can use base() and shift().\n+\n+  \/\/ Called right after argument parsing; defines narrow klass pointer geometry limits\n+  static void pre_initialize();\n+\n+  static bool tiny_classpointer_mode()   { check_init(_tiny_cp); return (_tiny_cp == 1); }\n+\n+  \/\/ The number of bits a narrow Klass pointer has;\n+  static int narrow_klass_pointer_bits() { check_init(_narrow_klass_pointer_bits); return _narrow_klass_pointer_bits; }\n+\n+  \/\/ The maximum possible shift; the actual shift employed later can be smaller (see initialize())\n+  static int max_shift()                 { check_init(_max_shift); return _max_shift; }\n+\n@@ -88,3 +149,10 @@\n-  static address  base()               { return  _base; }\n-  static size_t   range()              { return  _range; }\n-  static int      shift()              { return  _shift; }\n+  \/\/ Can only be used after initialization\n+  static address  base()             { check_init(_base); return  _base; }\n+  static size_t   range()            { check_init(_range); return  _range; }\n+  static int      shift()            { check_init(_shift); return  _shift; }\n+\n+  \/\/ Returns the alignment a Klass* is guaranteed to have.\n+  \/\/ Note: *Not* the same as 1 << shift ! Klass are always guaranteed to be at least 64-bit aligned,\n+  \/\/ so this will return 8 even if shift is 0.\n+  static int klass_alignment_in_bytes() { return nth_bit(MAX2(3, _shift)); }\n+  static int klass_alignment_in_words() { return klass_alignment_in_bytes() \/ BytesPerWord; }\n@@ -95,1 +163,1 @@\n-  static inline Klass* decode_raw(narrowKlass v, address base, int shift);\n+  static inline Klass* decode_raw(narrowKlass v, address narrow_base, int narrow_shift);\n@@ -98,1 +166,0 @@\n-  static inline Klass* decode_not_null(narrowKlass v, address base, int shift);\n@@ -100,3 +167,13 @@\n-  static inline narrowKlass encode_not_null(Klass* v);\n-  static inline narrowKlass encode_not_null(Klass* v, address base, int shift);\n-  static inline narrowKlass encode(Klass* v);\n+\n+  static inline narrowKlass encode_raw(Klass* k, address narrow_base, int narrow_shift);\n+  static inline narrowKlass encode_not_null(Klass* k);\n+  static inline narrowKlass encode(Klass* k);\n+\n+#ifdef ASSERT\n+  \/\/ Given a Klass* k and an encoding (base, shift), check that k can be encoded\n+  inline static void check_valid_klass(const Klass* k, address narrow_base, int narrow_shift);\n+  \/\/ Given a Klass* k, check that k can be encoded with the current encoding\n+  inline static void check_valid_klass(const Klass* k);\n+  \/\/ Given a narrow Klass ID, check that it is valid according to current encoding\n+  inline static void check_valid_narrow_klass_id(narrowKlass nk);\n+#endif\n","filename":"src\/hotspot\/share\/oops\/compressedKlass.hpp","additions":94,"deletions":17,"binary":false,"changes":111,"status":"modified"},{"patch":"@@ -35,4 +35,0 @@\n-static inline bool check_alignment(Klass* v) {\n-  return (intptr_t)v % KlassAlignmentInBytes == 0;\n-}\n-\n@@ -43,2 +39,2 @@\n-inline Klass* CompressedKlassPointers::decode_raw(narrowKlass v, address narrow_base, int shift) {\n-  return (Klass*)((uintptr_t)narrow_base +((uintptr_t)v << shift));\n+inline Klass* CompressedKlassPointers::decode_raw(narrowKlass v, address narrow_base, int narrow_shift) {\n+  return (Klass*)((uintptr_t)narrow_base +((uintptr_t)v << narrow_shift));\n@@ -48,1 +44,8 @@\n-  return decode_not_null(v, base(), shift());\n+  assert(!is_null(v), \"narrow klass value is null\");\n+  DEBUG_ONLY(check_valid_narrow_klass_id(v);)\n+  Klass* k = decode_raw(v);\n+  DEBUG_ONLY(check_valid_klass(k);)\n+  return k;\n+}\n+inline Klass* CompressedKlassPointers::decode(narrowKlass v) {\n+  return is_null(v) ? nullptr : decode_not_null(v);\n@@ -51,5 +54,2 @@\n-inline Klass* CompressedKlassPointers::decode_not_null(narrowKlass v, address narrow_base, int shift) {\n-  assert(!is_null(v), \"narrow klass value can never be zero\");\n-  Klass* result = decode_raw(v, narrow_base, shift);\n-  assert(check_alignment(result), \"address not aligned: \" PTR_FORMAT, p2i(result));\n-  return result;\n+inline narrowKlass CompressedKlassPointers::encode_raw(Klass* k, address narrow_base, int narrow_shift) {\n+  return (narrowKlass)(pointer_delta(k, narrow_base, 1) >> narrow_shift);\n@@ -58,2 +58,7 @@\n-inline Klass* CompressedKlassPointers::decode(narrowKlass v) {\n-  return is_null(v) ? nullptr : decode_not_null(v);\n+inline narrowKlass CompressedKlassPointers::encode_not_null(Klass* k) {\n+  assert(!is_null(k), \"klass value can never be zero\");\n+  DEBUG_ONLY(check_valid_klass(k);)\n+  narrowKlass nk = encode_raw(k, base(), shift());\n+  DEBUG_ONLY(check_valid_narrow_klass_id(nk);)\n+  assert(decode_raw(nk, base(), shift()) == k, \"reversibility\");\n+  return nk;\n@@ -62,2 +67,2 @@\n-inline narrowKlass CompressedKlassPointers::encode_not_null(Klass* v) {\n-  return encode_not_null(v, base(), shift());\n+inline narrowKlass CompressedKlassPointers::encode(Klass* k) {\n+  return is_null(k) ? (narrowKlass)0 : encode_not_null(k);\n@@ -66,9 +71,9 @@\n-inline narrowKlass CompressedKlassPointers::encode_not_null(Klass* v, address narrow_base, int shift) {\n-  assert(!is_null(v), \"klass value can never be zero\");\n-  assert(check_alignment(v), \"Address not aligned\");\n-  uint64_t pd = (uint64_t)(pointer_delta(v, narrow_base, 1));\n-  assert(KlassEncodingMetaspaceMax > pd, \"change encoding max if new encoding\");\n-  uint64_t result = pd >> shift;\n-  assert((result & CONST64(0xffffffff00000000)) == 0, \"narrow klass pointer overflow\");\n-  assert(decode_not_null((narrowKlass)result, narrow_base, shift) == v, \"reversibility\");\n-  return (narrowKlass)result;\n+#ifdef ASSERT\n+inline void CompressedKlassPointers::check_valid_klass(const Klass* k, address narrow_base, int narrow_shift) {\n+  const int log_alignment = MAX2(3, narrow_shift); \/\/ always at least 64-bit aligned\n+  assert(is_aligned(k, nth_bit(log_alignment)), \"Klass (\" PTR_FORMAT \") not properly aligned to %zu\",\n+         p2i(k), nth_bit(narrow_shift));\n+  const address encoding_end = narrow_base + nth_bit(narrow_klass_pointer_bits() + narrow_shift);\n+  assert((address)k >= narrow_base && (address)k < encoding_end,\n+         \"Klass (\" PTR_FORMAT \") falls outside of the valid encoding range [\" PTR_FORMAT \"-\" PTR_FORMAT \")\",\n+         p2i(k), p2i(narrow_base), p2i(encoding_end));\n@@ -77,2 +82,17 @@\n-inline narrowKlass CompressedKlassPointers::encode(Klass* v) {\n-  return is_null(v) ? (narrowKlass)0 : encode_not_null(v);\n+inline void CompressedKlassPointers::check_valid_klass(const Klass* k) {\n+  assert(UseCompressedClassPointers, \"Only call for +UseCCP\");\n+  check_valid_klass(k, base(), shift());\n+  \/\/ Also assert that k falls into what we know is the valid Klass range. This is usually smaller\n+  \/\/ than the encoding range (e.g. encoding range covers 4G, but we only have 1G class space and a\n+  \/\/ tiny bit of CDS => 1.1G)\n+  const address klassrange_end = base() + range();\n+  assert((address)k < klassrange_end,\n+      \"Klass (\" PTR_FORMAT \") falls outside of the valid klass range [\" PTR_FORMAT \"-\" PTR_FORMAT \")\",\n+      p2i(k), p2i(base()), p2i(klassrange_end));\n+}\n+inline void CompressedKlassPointers::check_valid_narrow_klass_id(narrowKlass nk) {\n+  assert(UseCompressedClassPointers, \"Only call for +UseCCP\");\n+  const uint64_t nk_mask = ~right_n_bits(narrow_klass_pointer_bits());\n+  assert(((uint64_t)nk & nk_mask) == 0, \"narrow klass id bit spillover (%u)\", nk);\n+  assert(nk >= _lowest_valid_narrow_klass_id &&\n+         nk <= _highest_valid_narrow_klass_id, \"narrowKlass ID out of range (%u)\", nk);\n@@ -80,0 +100,1 @@\n+#endif \/\/ ASSERT\n","filename":"src\/hotspot\/share\/oops\/compressedKlass.inline.hpp","additions":48,"deletions":27,"binary":false,"changes":75,"status":"modified"},{"patch":"@@ -115,1 +115,1 @@\n-  static const int hash_bits_compact              = max_hash_bits > 25 ? 25 : max_hash_bits;\n+  static const int hash_bits_compact              = hash_bits;\n@@ -118,4 +118,0 @@\n-#ifdef _LP64\n-  \/\/ Used only with compact headers.\n-  static const int klass_bits                     = 32;\n-#endif\n@@ -127,5 +123,1 @@\n-  static const int hash_shift_compact             = age_shift + age_bits;\n-#ifdef _LP64\n-  \/\/ Used only with compact headers.\n-  static const int klass_shift                    = hash_shift_compact + hash_bits_compact;\n-#endif\n+  static const int hash_shift_compact             = 11;\n@@ -143,0 +135,1 @@\n+\n@@ -144,2 +137,17 @@\n-  static const uintptr_t klass_mask               = right_n_bits(klass_bits);\n-  static const uintptr_t klass_mask_in_place      = klass_mask << klass_shift;\n+  \/\/ Used only with compact headers:\n+  \/\/ We store nKlass in the upper 22 bits of the markword. When extracting, we need to read the upper\n+  \/\/ 32 bits and rightshift by the lower 10 foreign bits.\n+\n+  \/\/ These are for loading the nKlass with a 32-bit load and subsequent masking of the lower\n+  \/\/ shadow bits\n+  static constexpr int klass_load_shift           = 32;\n+  static constexpr int klass_load_bits            = 32;\n+  static constexpr int klass_shadow_bits          = 10;\n+  static constexpr uintptr_t klass_shadow_mask    = right_n_bits(klass_shadow_bits);\n+  static constexpr uintptr_t klass_shadow_mask_inplace  = klass_shadow_mask << klass_load_shift;\n+\n+  \/\/ These are for bit-precise extraction of the nKlass from the 64-bit Markword\n+  static constexpr int klass_shift                = 42;\n+  static constexpr int klass_bits                 = 22;\n+  static constexpr uintptr_t klass_mask           = right_n_bits(klass_bits);\n+  static constexpr uintptr_t klass_mask_in_place  = klass_mask << klass_shift;\n","filename":"src\/hotspot\/share\/oops\/markWord.hpp","additions":20,"deletions":12,"binary":false,"changes":32,"status":"modified"},{"patch":"@@ -54,0 +54,5 @@\n+  \/\/ Make sure we don't need to zero any upper bits\n+  STATIC_ASSERT(klass_shift + klass_bits == 64);\n+  \/\/ Also, the hash (preceding nKlass) shall be a direct neighbor but\n+  \/\/ not interleave\n+  STATIC_ASSERT(klass_shift == hash_bits_compact + hash_shift_compact);\n@@ -59,1 +64,9 @@\n-  return markWord((value() & ~klass_mask_in_place) | ((uintptr_t) nklass << klass_shift));\n+  markWord mw((value() & ~klass_mask_in_place) | ((uintptr_t) nklass << klass_shift));\n+#ifdef ASSERT\n+  \/\/ Sanity:\n+  \/\/ Shadow bits and real nKlass bits must not intersect, but both should be contained\n+  \/\/ in the klass load mask\n+  STATIC_ASSERT((klass_shadow_mask_inplace & klass_mask_in_place) == 0);\n+  STATIC_ASSERT((klass_load_shift + klass_shadow_bits) == klass_shift);\n+#endif\n+  return mw;\n","filename":"src\/hotspot\/share\/oops\/markWord.inline.hpp","additions":14,"deletions":1,"binary":false,"changes":15,"status":"modified"},{"patch":"@@ -343,2 +343,3 @@\n-      STATIC_ASSERT(markWord::klass_shift % 8 == 0);\n-      return mark_offset_in_bytes() + markWord::klass_shift \/ 8;\n+      constexpr int load_shift = markWord::klass_load_shift;\n+      STATIC_ASSERT(load_shift % 8 == 0);\n+      return mark_offset_in_bytes() + load_shift \/ 8;\n","filename":"src\/hotspot\/share\/oops\/oop.hpp","additions":3,"deletions":2,"binary":false,"changes":5,"status":"modified"},{"patch":"@@ -271,2 +271,2 @@\n-        if ((UseCompressedOops || UseCompressedClassPointers) &&\n-            (CompressedOops::shift() == 0 || CompressedKlassPointers::shift() == 0)) {\n+        if ((UseCompressedOops && CompressedOops::shift() == 0) ||\n+            (UseCompressedClassPointers && CompressedKlassPointers::shift() == 0)) {\n","filename":"src\/hotspot\/share\/opto\/lcm.cpp","additions":2,"deletions":2,"binary":false,"changes":4,"status":"modified"},{"patch":"@@ -394,1 +394,1 @@\n-  if (t->isa_narrowklass() && CompressedKlassPointers::shift() == 0) {\n+  if (t->isa_narrowklass() && UseCompressedClassPointers && CompressedKlassPointers::shift() == 0) {\n","filename":"src\/hotspot\/share\/opto\/machnode.cpp","additions":1,"deletions":1,"binary":false,"changes":2,"status":"modified"},{"patch":"@@ -1417,7 +1417,0 @@\n-void Arguments::set_use_compressed_klass_ptrs() {\n-#ifdef _LP64\n-  assert(!UseCompressedClassPointers || CompressedClassSpaceSize <= KlassEncodingMetaspaceMax,\n-         \"CompressedClassSpaceSize is too large for UseCompressedClassPointers\");\n-#endif \/\/ _LP64\n-}\n-\n@@ -1442,1 +1435,0 @@\n-  set_use_compressed_klass_ptrs();\n@@ -3676,0 +3668,4 @@\n+  if (UseCompressedClassPointers) {\n+    CompressedKlassPointers::pre_initialize();\n+  }\n+\n","filename":"src\/hotspot\/share\/runtime\/arguments.cpp","additions":4,"deletions":8,"binary":false,"changes":12,"status":"modified"},{"patch":"@@ -264,1 +264,0 @@\n-  static void set_use_compressed_klass_ptrs();\n","filename":"src\/hotspot\/share\/runtime\/arguments.hpp","additions":0,"deletions":1,"binary":false,"changes":1,"status":"modified"},{"patch":"@@ -136,0 +136,2 @@\n+  develop(int, TinyClassPointerShift, 0, \"\")                                \\\n+                                                                            \\\n","filename":"src\/hotspot\/share\/runtime\/globals.hpp","additions":2,"deletions":0,"binary":false,"changes":2,"status":"modified"},{"patch":"@@ -2018,2 +2018,0 @@\n-  declare_constant(LogKlassAlignmentInBytes)                              \\\n-                                                                          \\\n","filename":"src\/hotspot\/share\/runtime\/vmStructs.cpp","additions":0,"deletions":2,"binary":false,"changes":2,"status":"modified"},{"patch":"@@ -40,0 +40,1 @@\n+  int _num_arenas_created;\n@@ -42,2 +43,5 @@\n-    metaspace::MetaspaceTestContext(\"gtest-metaspace-context\", commit_limit, reserve_limit)\n-  {}\n+    metaspace::MetaspaceTestContext(\"gtest-metaspace-context\", commit_limit, reserve_limit),\n+    _num_arenas_created(0) {}\n+\n+  int num_arenas_created() const { return _num_arenas_created; }\n+  void inc_num_arenas_created() { _num_arenas_created ++; }\n","filename":"test\/hotspot\/gtest\/metaspace\/metaspaceGtestContexts.hpp","additions":6,"deletions":2,"binary":false,"changes":8,"status":"modified"},{"patch":"@@ -29,0 +29,1 @@\n+#include \"memory\/metaspace\/metablock.hpp\"\n@@ -35,0 +36,1 @@\n+using metaspace::MetaBlock;\n@@ -46,0 +48,13 @@\n+template <int num_lists>\n+struct TestedBinList : public BinListImpl<num_lists> {\n+  typedef BinListImpl<num_lists> ListType;\n+  void add_block(MetaWord* p, size_t word_size) {\n+    ListType::add_block(MetaBlock(p, word_size));\n+  }\n+  MetaWord* remove_block(size_t requested_size, size_t* real_size) {\n+    MetaBlock result = ListType::remove_block(requested_size);\n+    (*real_size) = result.word_size();\n+    return result.base();\n+  }\n+};\n+\n@@ -209,3 +224,3 @@\n-TEST_VM(metaspace, BinList_basic_1)     { BinListBasicTest< BinListImpl<1> >::basic_test(); }\n-TEST_VM(metaspace, BinList_basic_8)     { BinListBasicTest< BinListImpl<8> >::basic_test(); }\n-TEST_VM(metaspace, BinList_basic_32)    { BinListBasicTest<BinList32>::basic_test(); }\n+TEST_VM(metaspace, BinList_basic_1)     { BinListBasicTest< TestedBinList<1> >::basic_test(); }\n+TEST_VM(metaspace, BinList_basic_8)     { BinListBasicTest< TestedBinList<8> >::basic_test(); }\n+TEST_VM(metaspace, BinList_basic_32)    { BinListBasicTest< TestedBinList<32> >::basic_test(); }\n@@ -213,3 +228,3 @@\n-TEST_VM(metaspace, BinList_basic_2_1)     { BinListBasicTest< BinListImpl<1> >::basic_test_2(); }\n-TEST_VM(metaspace, BinList_basic_2_8)     { BinListBasicTest< BinListImpl<8> >::basic_test_2(); }\n-TEST_VM(metaspace, BinList_basic_2_32)    { BinListBasicTest<BinList32>::basic_test_2(); }\n+TEST_VM(metaspace, BinList_basic_2_1)     { BinListBasicTest< TestedBinList<1> >::basic_test_2(); }\n+TEST_VM(metaspace, BinList_basic_2_8)     { BinListBasicTest< TestedBinList<8> >::basic_test_2(); }\n+TEST_VM(metaspace, BinList_basic_2_32)    { BinListBasicTest< TestedBinList<32> >::basic_test_2(); }\n@@ -217,3 +232,3 @@\n-TEST_VM(metaspace, BinList_basic_rand_1)     { BinListBasicTest< BinListImpl<1> >::random_test(); }\n-TEST_VM(metaspace, BinList_basic_rand_8)     { BinListBasicTest< BinListImpl<8> >::random_test(); }\n-TEST_VM(metaspace, BinList_basic_rand_32)    { BinListBasicTest<BinList32>::random_test(); }\n+TEST_VM(metaspace, BinList_basic_rand_1)     { BinListBasicTest< TestedBinList<1> >::random_test(); }\n+TEST_VM(metaspace, BinList_basic_rand_8)     { BinListBasicTest< TestedBinList<8> >::random_test(); }\n+TEST_VM(metaspace, BinList_basic_rand_32)    { BinListBasicTest< TestedBinList<32> >::random_test(); }\n","filename":"test\/hotspot\/gtest\/metaspace\/test_binlist.cpp","additions":24,"deletions":9,"binary":false,"changes":33,"status":"modified"},{"patch":"@@ -29,0 +29,1 @@\n+#include \"memory\/metaspace\/metablock.hpp\"\n@@ -35,0 +36,12 @@\n+using metaspace::MetaBlock;\n+\n+struct TestedBlockTree : public BlockTree {\n+  void add_block(MetaWord* p, size_t word_size) {\n+    BlockTree::add_block(MetaBlock(p, word_size));\n+  }\n+  MetaWord* remove_block(size_t requested_size, size_t* real_size) {\n+    MetaBlock result = BlockTree::remove_block(requested_size);\n+    (*real_size) = result.word_size();\n+    return result.base();\n+  }\n+};\n@@ -38,1 +51,1 @@\n-static void create_nodes(const size_t sizes[], FeederBuffer& fb, BlockTree& bt) {\n+static void create_nodes(const size_t sizes[], FeederBuffer& fb, TestedBlockTree& bt) {\n@@ -58,1 +71,1 @@\n-  BlockTree bt;\n+  TestedBlockTree bt;\n@@ -115,1 +128,1 @@\n-  BlockTree bt;\n+  TestedBlockTree bt;\n@@ -158,1 +171,1 @@\n-  BlockTree bt;\n+  TestedBlockTree bt;\n@@ -173,1 +186,1 @@\n-  BlockTree bt;\n+  TestedBlockTree bt;\n@@ -207,1 +220,1 @@\n-  BlockTree bt;\n+  TestedBlockTree bt;\n@@ -225,1 +238,1 @@\n-  BlockTree bt;\n+  TestedBlockTree bt;\n@@ -252,1 +265,1 @@\n-  BlockTree _bt[2];\n+  TestedBlockTree _bt[2];\n@@ -359,1 +372,1 @@\n-      BlockTree* bt = _bt + which;\n+      TestedBlockTree* bt = _bt + which;\n","filename":"test\/hotspot\/gtest\/metaspace\/test_blocktree.cpp","additions":22,"deletions":9,"binary":false,"changes":31,"status":"modified"},{"patch":"@@ -0,0 +1,419 @@\n+\/*\n+ * Copyright (c) 2023, Oracle and\/or its affiliates. All rights reserved.\n+ * Copyright (c) 2023 Red Hat, Inc. All rights reserved.\n+ * DO NOT ALTER OR REMOVE COPYRIGHT NOTICES OR THIS FILE HEADER.\n+ *\n+ * This code is free software; you can redistribute it and\/or modify it\n+ * under the terms of the GNU General Public License version 2 only, as\n+ * published by the Free Software Foundation.\n+ *\n+ * This code is distributed in the hope that it will be useful, but WITHOUT\n+ * ANY WARRANTY; without even the implied warranty of MERCHANTABILITY or\n+ * FITNESS FOR A PARTICULAR PURPOSE.  See the GNU General Public License\n+ * version 2 for more details (a copy is included in the LICENSE file that\n+ * accompanied this code).\n+ *\n+ * You should have received a copy of the GNU General Public License version\n+ * 2 along with this work; if not, write to the Free Software Foundation,\n+ * Inc., 51 Franklin St, Fifth Floor, Boston, MA 02110-1301 USA.\n+ *\n+ * Please contact Oracle, 500 Oracle Parkway, Redwood Shores, CA 94065 USA\n+ * or visit www.oracle.com if you need additional information or have any\n+ * questions.\n+ *\n+ *\/\n+\n+#include \"precompiled.hpp\"\n+#include \"memory\/classLoaderMetaspace.hpp\"\n+#include \"memory\/metaspace\/freeBlocks.hpp\"\n+#include \"memory\/metaspace\/metablock.inline.hpp\"\n+#include \"memory\/metaspace\/metaspaceArena.hpp\"\n+#include \"memory\/metaspace\/metaspaceSettings.hpp\"\n+#include \"memory\/metaspace\/metaspaceStatistics.hpp\"\n+#include \"memory\/metaspace.hpp\"\n+#include \"oops\/klass.hpp\"\n+#include \"runtime\/mutex.hpp\"\n+#include \"utilities\/debug.hpp\"\n+#include \"utilities\/align.hpp\"\n+#include \"utilities\/globalDefinitions.hpp\"\n+\n+#ifdef _LP64\n+\n+#define LOG_PLEASE\n+#include \"metaspaceGtestCommon.hpp\"\n+#include \"metaspaceGtestContexts.hpp\"\n+#include \"metaspaceGtestRangeHelpers.hpp\"\n+#include \"metaspaceGtestSparseArray.hpp\"\n+\n+#define HANDLE_FAILURE \\\n+  if (testing::Test::HasFailure()) { \\\n+    return; \\\n+  }\n+\n+namespace metaspace {\n+\n+class ClmsTester {\n+\n+  Mutex _lock;\n+  MetaspaceContext* _class_context;\n+  MetaspaceContext* _nonclass_context;\n+  ClassLoaderMetaspace* _clms;\n+  const size_t _klass_arena_alignment_words;\n+  unsigned _num_allocations;\n+\n+  struct Deltas {\n+    int num_chunks_delta;\n+    ssize_t used_words_delta;\n+    int num_freeblocks_delta;\n+    ssize_t freeblocks_words_delta;\n+  };\n+\n+  Deltas calc_deltas(const ArenaStats& before, const ArenaStats& after) {\n+    Deltas d;\n+    d.num_chunks_delta = after.totals()._num - before.totals()._num;\n+    d.used_words_delta = after.totals()._used_words - before.totals()._used_words;\n+    d.num_freeblocks_delta = (int)after._free_blocks_num - (int)before._free_blocks_num;\n+    d.freeblocks_words_delta = after._free_blocks_word_size - before._free_blocks_word_size;\n+    return d;\n+  }\n+\n+public:\n+\n+  ClmsTester(size_t klass_alignment_words, Metaspace::MetaspaceType space_type,\n+             MetaspaceContext* class_context, MetaspaceContext* nonclass_context)\n+  : _lock(Monitor::nosafepoint, \"CLMSTest_lock\"),\n+    _class_context(class_context), _nonclass_context(nonclass_context),\n+    _clms(nullptr), _klass_arena_alignment_words(klass_alignment_words), _num_allocations(0) {\n+    _clms = new ClassLoaderMetaspace(&_lock, space_type, nonclass_context, class_context, klass_alignment_words);\n+  }\n+\n+  ~ClmsTester() {\n+    delete _clms;\n+    EXPECT_EQ(_class_context->used_words(), (size_t)0);\n+    EXPECT_EQ(_nonclass_context->used_words(), (size_t)0);\n+  }\n+\n+  MetaBlock allocate_and_check(size_t word_size, bool is_class) {\n+\n+    \/\/ take stats before allocation\n+    ClmsStats stats_before;\n+    _clms->add_to_statistics(&stats_before);\n+\n+    \/\/ allocate\n+    MetaWord* p = _clms->allocate(word_size, is_class ? Metaspace::ClassType : Metaspace::NonClassType);\n+    _num_allocations ++;\n+\n+    \/\/ take stats after allocation\n+    ClmsStats stats_after;\n+    _clms->add_to_statistics(&stats_after);\n+\n+    \/\/ for less verbose testing:\n+    const ArenaStats& ca_before = stats_before._arena_stats_class;\n+    const ArenaStats& ca_after = stats_after._arena_stats_class;\n+    const ArenaStats& nca_before = stats_before._arena_stats_nonclass;\n+    const ArenaStats& nca_after = stats_after._arena_stats_nonclass;\n+\n+    \/\/ deltas\n+    const Deltas d_ca = calc_deltas(ca_before, ca_after);\n+    const Deltas d_nca = calc_deltas(nca_before, nca_after);\n+\n+#define EXPECT_FREEBLOCKS_UNCHANGED(arena_prefix) \\\n+    EXPECT_EQ(d_##arena_prefix.num_freeblocks_delta, 0);  \\\n+    EXPECT_EQ(d_##arena_prefix.freeblocks_words_delta, (ssize_t)0);\n+\n+#define EXPECT_ARENA_UNCHANGED(arena_prefix) \\\n+    EXPECT_EQ(d_##arena_prefix.num_chunks_delta, 0);  \\\n+    EXPECT_EQ(d_##arena_prefix.used_words_delta, (ssize_t)0);\n+\n+    if (p != nullptr) {\n+\n+      MetaBlock bl(p, word_size);\n+\n+      if (is_class) {\n+\n+        EXPECT_TRUE(bl.is_aligned_base(_klass_arena_alignment_words));\n+\n+        if (_num_allocations == 1) {\n+          \/\/ first allocation: nonclass arena unchanged, class arena grows by 1 chunk and wordsize,\n+          \/\/ class arena freeblocks unchanged\n+          EXPECT_ARENA_UNCHANGED(nca);\n+          EXPECT_FREEBLOCKS_UNCHANGED(nca);\n+          EXPECT_EQ(d_ca.num_chunks_delta, 1);\n+          EXPECT_EQ((size_t)d_ca.used_words_delta, word_size);\n+          EXPECT_FREEBLOCKS_UNCHANGED(ca);\n+          return bl;\n+        }\n+\n+        \/\/ Had this been taken from class arena freeblocks?\n+        if (d_ca.num_freeblocks_delta == -1) {\n+          \/\/ the class arena freeblocks should have gone down, and the non-class arena freeblocks may have gone\n+          \/\/ up in case the block was larger than required\n+          const size_t wordsize_block_taken = (size_t)(-d_ca.freeblocks_words_delta);\n+          EXPECT_GE(wordsize_block_taken, word_size); \/\/ the block we took must be at least allocation size\n+          const size_t expected_freeblock_remainder = wordsize_block_taken - word_size;\n+          if (expected_freeblock_remainder > 0) {\n+            \/\/ the remainder, if it existed, should have been added to nonclass freeblocks\n+            EXPECT_EQ(d_nca.num_freeblocks_delta, 1);\n+            EXPECT_EQ((size_t)d_nca.freeblocks_words_delta, expected_freeblock_remainder);\n+          }\n+          \/\/ finally, nothing should have happened in the arenas proper.\n+          EXPECT_ARENA_UNCHANGED(ca);\n+          EXPECT_ARENA_UNCHANGED(nca);\n+          return bl;\n+        }\n+\n+        \/\/ block was taken from class arena proper\n+\n+        \/\/ We expect allocation waste due to alignment, should have been added to the freeblocks\n+        \/\/ of nonclass arena. Allocation waste can be 0. If no chunk turnover happened, it must be\n+        \/\/ smaller than klass alignment, otherwise it can get as large as a commit granule.\n+        const size_t max_expected_allocation_waste =\n+            d_ca.num_chunks_delta == 0 ? (_klass_arena_alignment_words - 1) : Settings::commit_granule_words();\n+        EXPECT_GE(d_ca.num_chunks_delta, 0);\n+        EXPECT_LE(d_ca.num_chunks_delta, 1);\n+        EXPECT_GE((size_t)d_ca.used_words_delta, word_size);\n+        EXPECT_LE((size_t)d_ca.used_words_delta, word_size + max_expected_allocation_waste);\n+        EXPECT_FREEBLOCKS_UNCHANGED(ca);\n+        EXPECT_ARENA_UNCHANGED(nca);\n+        if (max_expected_allocation_waste > 0) {\n+          EXPECT_GE(d_nca.num_freeblocks_delta, 0);\n+          EXPECT_LE(d_nca.num_freeblocks_delta, 1);\n+          EXPECT_GE(d_nca.freeblocks_words_delta, 0);\n+          EXPECT_LE((size_t)d_nca.freeblocks_words_delta, max_expected_allocation_waste);\n+        } else {\n+          EXPECT_FREEBLOCKS_UNCHANGED(nca);\n+        }\n+        return bl;\n+\n+      } \/\/ end: is_class\n+\n+      else\n+\n+      {\n+        \/\/ Nonclass arena allocation.\n+        \/\/ Allocation waste can happen:\n+        \/\/ - if we allocate from nonclass freeblocks, the block remainder\n+        \/\/ - if we allocate from arena proper, by chunk turnover\n+\n+        if (d_nca.freeblocks_words_delta < 0) {\n+          \/\/ We allocated a block from the nonclass arena freeblocks.\n+          const size_t wordsize_block_taken = (size_t)(-d_nca.freeblocks_words_delta);\n+          EXPECT_EQ(wordsize_block_taken, word_size);\n+          \/\/ The number of blocks may or may not have decreased (depending on whether there\n+          \/\/ was a wastage block)\n+          EXPECT_GE(d_nca.num_chunks_delta, -1);\n+          EXPECT_LE(d_nca.num_chunks_delta, 0);\n+          EXPECT_ARENA_UNCHANGED(nca);\n+          EXPECT_ARENA_UNCHANGED(ca);\n+          EXPECT_FREEBLOCKS_UNCHANGED(ca);\n+          return bl;\n+        }\n+\n+        \/\/ We don't expect alignment waste. Only wastage happens at chunk turnover.\n+        const size_t max_expected_allocation_waste =\n+            d_nca.num_chunks_delta == 0 ? 0 : Settings::commit_granule_words();\n+        EXPECT_ARENA_UNCHANGED(ca);\n+        EXPECT_FREEBLOCKS_UNCHANGED(ca);\n+        EXPECT_GE(d_nca.num_chunks_delta, 0);\n+        EXPECT_LE(d_nca.num_chunks_delta, 1);\n+        EXPECT_GE((size_t)d_nca.used_words_delta, word_size);\n+        EXPECT_LE((size_t)d_nca.used_words_delta, word_size + max_expected_allocation_waste);\n+        if (max_expected_allocation_waste == 0) {\n+          EXPECT_FREEBLOCKS_UNCHANGED(nca);\n+        }\n+      }\n+      return bl;\n+\n+    } \/\/ end: allocation successful\n+\n+    \/\/ allocation failed.\n+    EXPECT_ARENA_UNCHANGED(ca);\n+    EXPECT_FREEBLOCKS_UNCHANGED(ca);\n+    EXPECT_ARENA_UNCHANGED(nca);\n+    EXPECT_FREEBLOCKS_UNCHANGED(nca);\n+\n+    return MetaBlock();\n+  }\n+\n+  MetaBlock allocate_expect_success(size_t word_size, bool is_class) {\n+    MetaBlock bl = allocate_and_check(word_size, is_class);\n+    EXPECT_TRUE(bl.is_nonempty());\n+    return bl;\n+  }\n+\n+  MetaBlock allocate_expect_failure(size_t word_size, bool is_class) {\n+    MetaBlock bl = allocate_and_check(word_size, is_class);\n+    EXPECT_TRUE(bl.is_empty());\n+    return bl;\n+  }\n+\n+  void deallocate_and_check(MetaBlock bl, bool is_class) {\n+\n+    \/\/ take stats before deallocation\n+    ClmsStats stats_before;\n+    _clms->add_to_statistics(&stats_before);\n+\n+    \/\/ allocate\n+    _clms->deallocate(bl.base(), bl.word_size(), is_class);\n+\n+    \/\/ take stats after deallocation\n+    ClmsStats stats_after;\n+    _clms->add_to_statistics(&stats_after);\n+\n+    \/\/ for less verbose testing:\n+    const ArenaStats& ca_before = stats_before._arena_stats_class;\n+    const ArenaStats& ca_after = stats_after._arena_stats_class;\n+    const ArenaStats& nca_before = stats_before._arena_stats_nonclass;\n+    const ArenaStats& nca_after = stats_after._arena_stats_nonclass;\n+\n+    \/\/ deltas\n+    \/\/ deltas\n+    const Deltas d_ca = calc_deltas(ca_before, ca_after);\n+    const Deltas d_nca = calc_deltas(nca_before, nca_after);\n+\n+    EXPECT_ARENA_UNCHANGED(ca);\n+    EXPECT_ARENA_UNCHANGED(nca);\n+    if (is_class) {\n+      EXPECT_EQ(d_ca.num_freeblocks_delta, 1);\n+      EXPECT_EQ((size_t)d_ca.freeblocks_words_delta, bl.word_size());\n+      EXPECT_FREEBLOCKS_UNCHANGED(nca);\n+    } else {\n+      EXPECT_EQ(d_nca.num_freeblocks_delta, 1);\n+      EXPECT_EQ((size_t)d_nca.freeblocks_words_delta, bl.word_size());\n+      EXPECT_FREEBLOCKS_UNCHANGED(ca);\n+    }\n+  }\n+};\n+\n+static constexpr size_t klass_size = sizeof(Klass) \/ BytesPerWord;\n+\n+static void basic_test(size_t klass_arena_alignment) {\n+  if (Settings::use_allocation_guard()) {\n+    return;\n+  }\n+  MetaspaceGtestContext class_context, nonclass_context;\n+  {\n+    ClmsTester tester(klass_arena_alignment, Metaspace::StandardMetaspaceType, class_context.context(), nonclass_context.context());\n+\n+    MetaBlock bl1 = tester.allocate_expect_success(klass_size, true);\n+    HANDLE_FAILURE;\n+\n+    MetaBlock bl2 = tester.allocate_expect_success(klass_size, true);\n+    HANDLE_FAILURE;\n+\n+    tester.deallocate_and_check(bl1, true);\n+    HANDLE_FAILURE;\n+\n+    MetaBlock bl3 = tester.allocate_expect_success(klass_size, true);\n+    HANDLE_FAILURE;\n+    EXPECT_EQ(bl3, bl1); \/\/ should have gotten the same block back from freelist\n+\n+    MetaBlock bl4 = tester.allocate_expect_success(Metaspace::min_allocation_word_size, false);\n+    HANDLE_FAILURE;\n+\n+    MetaBlock bl5 = tester.allocate_expect_success(K, false);\n+    HANDLE_FAILURE;\n+\n+    tester.deallocate_and_check(bl5, false);\n+    HANDLE_FAILURE;\n+\n+    MetaBlock bl6 = tester.allocate_expect_success(K, false);\n+    HANDLE_FAILURE;\n+\n+    EXPECT_EQ(bl5, bl6); \/\/ should have gotten the same block back from freelist\n+  }\n+  EXPECT_EQ(class_context.used_words(), (size_t)0);\n+  EXPECT_EQ(nonclass_context.used_words(), (size_t)0);\n+  \/\/ we should have used exactly one commit granule (64K), not more, for each context\n+  EXPECT_EQ(class_context.committed_words(), Settings::commit_granule_words());\n+  EXPECT_EQ(nonclass_context.committed_words(), Settings::commit_granule_words());\n+}\n+\n+#define TEST_BASIC_N(n)               \\\n+TEST_VM(metaspace, CLMS_basics_##n) { \\\n+  basic_test(n);            \\\n+}\n+\n+TEST_BASIC_N(1)\n+TEST_BASIC_N(4)\n+TEST_BASIC_N(16)\n+TEST_BASIC_N(32)\n+TEST_BASIC_N(128)\n+\n+static void test_random(size_t klass_arena_alignment) {\n+  if (Settings::use_allocation_guard()) {\n+    return;\n+  }\n+\n+  MetaspaceGtestContext class_context, nonclass_context;\n+  constexpr int max_allocations = 1024;\n+  const SizeRange nonclass_alloc_range(Metaspace::min_allocation_alignment_words, 1024);\n+  const SizeRange class_alloc_range(klass_size, 1024);\n+  const IntRange one_out_of_ten(0, 10);\n+  for (int runs = 9; runs >= 0; runs--) {\n+    {\n+      ClmsTester tester(64, Metaspace::StandardMetaspaceType, class_context.context(), nonclass_context.context());\n+      struct LifeBlock {\n+        MetaBlock bl;\n+        bool is_class;\n+      };\n+      LifeBlock life_allocations[max_allocations];\n+      for (int i = 0; i < max_allocations; i++) {\n+        life_allocations[i].bl.reset();\n+      }\n+\n+      unsigned num_class_allocs = 0, num_nonclass_allocs = 0, num_class_deallocs = 0, num_nonclass_deallocs = 0;\n+      for (int i = 0; i < 5000; i ++) {\n+        const int slot = IntRange(0, max_allocations).random_value();\n+        if (life_allocations[slot].bl.is_empty()) {\n+          const bool is_class = one_out_of_ten.random_value() == 0;\n+          const size_t word_size =\n+              is_class ? class_alloc_range.random_value() : nonclass_alloc_range.random_value();\n+          MetaBlock bl = tester.allocate_expect_success(word_size, is_class);\n+          HANDLE_FAILURE;\n+          life_allocations[slot].bl = bl;\n+          life_allocations[slot].is_class = is_class;\n+          if (is_class) {\n+            num_class_allocs ++;\n+          } else {\n+            num_nonclass_allocs ++;\n+          }\n+        } else {\n+          tester.deallocate_and_check(life_allocations[slot].bl, life_allocations[slot].is_class);\n+          HANDLE_FAILURE;\n+          life_allocations[slot].bl.reset();\n+          if (life_allocations[slot].is_class) {\n+            num_class_deallocs ++;\n+          } else {\n+            num_nonclass_deallocs ++;\n+          }\n+        }\n+      }\n+      LOG(\"num class allocs: %u, num nonclass allocs: %u, num class deallocs: %u, num nonclass deallocs: %u\",\n+          num_class_allocs, num_nonclass_allocs, num_class_deallocs, num_nonclass_deallocs);\n+    }\n+    EXPECT_EQ(class_context.used_words(), (size_t)0);\n+    EXPECT_EQ(nonclass_context.used_words(), (size_t)0);\n+    constexpr float fragmentation_factor = 3.0f;\n+    const size_t max_expected_nonclass_committed = max_allocations * nonclass_alloc_range.highest() * fragmentation_factor;\n+    const size_t max_expected_class_committed = max_allocations * class_alloc_range.highest() * fragmentation_factor;\n+    \/\/ we should have used exactly one commit granule (64K), not more, for each context\n+    EXPECT_LT(class_context.committed_words(), max_expected_class_committed);\n+    EXPECT_LT(nonclass_context.committed_words(), max_expected_nonclass_committed);\n+  }\n+}\n+\n+#define TEST_RANDOM_N(n)               \\\n+TEST_VM(metaspace, CLMS_random_##n) {  \\\n+  test_random(n);                      \\\n+}\n+\n+TEST_RANDOM_N(1)\n+TEST_RANDOM_N(4)\n+TEST_RANDOM_N(16)\n+TEST_RANDOM_N(32)\n+TEST_RANDOM_N(128)\n+\n+} \/\/ namespace metaspace\n+\n+#endif \/\/ _LP64\n","filename":"test\/hotspot\/gtest\/metaspace\/test_clms.cpp","additions":419,"deletions":0,"binary":false,"changes":419,"status":"added"},{"patch":"@@ -29,0 +29,2 @@\n+#include \"memory\/metaspace\/metablock.hpp\"\n+\n@@ -33,0 +35,1 @@\n+using metaspace::MetaBlock;\n@@ -46,166 +49,0 @@\n-class FreeBlocksTest {\n-\n-  FeederBuffer _fb;\n-  FreeBlocks _freeblocks;\n-\n-  \/\/ random generator for block feeding\n-  RandSizeGenerator _rgen_feeding;\n-\n-  \/\/ random generator for allocations (and, hence, deallocations)\n-  RandSizeGenerator _rgen_allocations;\n-\n-  SizeCounter _allocated_words;\n-\n-  struct allocation_t {\n-    allocation_t* next;\n-    size_t word_size;\n-    MetaWord* p;\n-  };\n-\n-  \/\/ Array of the same size as the pool max capacity; holds the allocated elements.\n-  allocation_t* _allocations;\n-\n-  int _num_allocs;\n-  int _num_deallocs;\n-  int _num_feeds;\n-\n-  bool feed_some() {\n-    size_t word_size = _rgen_feeding.get();\n-    MetaWord* p = _fb.get(word_size);\n-    if (p != nullptr) {\n-      _freeblocks.add_block(p, word_size);\n-      return true;\n-    }\n-    return false;\n-  }\n-\n-  bool deallocate_top() {\n-\n-    allocation_t* a = _allocations;\n-    if (a != nullptr) {\n-      _allocations = a->next;\n-      check_marked_range(a->p, a->word_size);\n-      _freeblocks.add_block(a->p, a->word_size);\n-      delete a;\n-      DEBUG_ONLY(_freeblocks.verify();)\n-      return true;\n-    }\n-    return false;\n-  }\n-\n-  void deallocate_all() {\n-    while (deallocate_top());\n-  }\n-\n-  bool allocate() {\n-\n-    size_t word_size = MAX2(_rgen_allocations.get(), _freeblocks.MinWordSize);\n-    MetaWord* p = _freeblocks.remove_block(word_size);\n-    if (p != nullptr) {\n-      _allocated_words.increment_by(word_size);\n-      allocation_t* a = new allocation_t;\n-      a->p = p; a->word_size = word_size;\n-      a->next = _allocations;\n-      _allocations = a;\n-      DEBUG_ONLY(_freeblocks.verify();)\n-      mark_range(p, word_size);\n-      return true;\n-    }\n-    return false;\n-  }\n-\n-  void test_all_marked_ranges() {\n-    for (allocation_t* a = _allocations; a != nullptr; a = a->next) {\n-      check_marked_range(a->p, a->word_size);\n-    }\n-  }\n-\n-  void test_loop() {\n-    \/\/ We loop and in each iteration execute one of three operations:\n-    \/\/ - allocation from fbl\n-    \/\/ - deallocation to fbl of a previously allocated block\n-    \/\/ - feeding a new larger block into the fbl (mimicks chunk retiring)\n-    \/\/ When we have fed all large blocks into the fbl (feedbuffer empty), we\n-    \/\/  switch to draining the fbl completely (only allocs)\n-    bool forcefeed = false;\n-    bool draining = false;\n-    bool stop = false;\n-    int iter = 25000; \/\/ safety stop\n-    while (!stop && iter > 0) {\n-      iter --;\n-      int surprise = (int)os::random() % 10;\n-      if (!draining && (surprise >= 7 || forcefeed)) {\n-        forcefeed = false;\n-        if (feed_some()) {\n-          _num_feeds++;\n-        } else {\n-          \/\/ We fed all input memory into the fbl. Now lets proceed until the fbl is drained.\n-          draining = true;\n-        }\n-      } else if (!draining && surprise < 1) {\n-        deallocate_top();\n-        _num_deallocs++;\n-      } else {\n-        if (allocate()) {\n-          _num_allocs++;\n-        } else {\n-          if (draining) {\n-            stop = _freeblocks.total_size() < 512;\n-          } else {\n-            forcefeed = true;\n-          }\n-        }\n-      }\n-      if ((iter % 1000) == 0) {\n-        DEBUG_ONLY(_freeblocks.verify();)\n-        test_all_marked_ranges();\n-        LOG(\"a %d (\" SIZE_FORMAT \"), d %d, f %d\", _num_allocs, _allocated_words.get(), _num_deallocs, _num_feeds);\n-#ifdef LOG_PLEASE\n-        _freeblocks.print(tty, true);\n-        tty->cr();\n-#endif\n-      }\n-    }\n-\n-    \/\/ Drain\n-\n-  }\n-\n-public:\n-\n-  FreeBlocksTest(size_t avg_alloc_size) :\n-    _fb(512 * K), _freeblocks(),\n-    _rgen_feeding(128, 4096),\n-    _rgen_allocations(avg_alloc_size \/ 4, avg_alloc_size * 2, 0.01f, avg_alloc_size \/ 3, avg_alloc_size * 30),\n-    _allocations(nullptr),\n-    _num_allocs(0),\n-    _num_deallocs(0),\n-    _num_feeds(0)\n-  {\n-    CHECK_CONTENT(_freeblocks, 0, 0);\n-    \/\/ some initial feeding\n-    _freeblocks.add_block(_fb.get(1024), 1024);\n-    CHECK_CONTENT(_freeblocks, 1, 1024);\n-  }\n-\n-  ~FreeBlocksTest() {\n-    deallocate_all();\n-  }\n-\n-  static void test_small_allocations() {\n-    FreeBlocksTest test(10);\n-    test.test_loop();\n-  }\n-\n-  static void test_medium_allocations() {\n-    FreeBlocksTest test(30);\n-    test.test_loop();\n-  }\n-\n-  static void test_large_allocations() {\n-    FreeBlocksTest test(150);\n-    test.test_loop();\n-  }\n-\n-};\n-\n@@ -218,1 +55,2 @@\n-  fbl.add_block(tmp, 1024);\n+  MetaBlock bl(tmp, 1024);\n+  fbl.add_block(bl);\n@@ -223,2 +61,2 @@\n-  MetaWord* p = fbl.remove_block(1024);\n-  EXPECT_EQ(p, tmp);\n+  MetaBlock bl2 = fbl.remove_block(1024);\n+  ASSERT_EQ(bl, bl2);\n@@ -229,13 +67,0 @@\n-\n-TEST_VM(metaspace, freeblocks_small) {\n-  FreeBlocksTest::test_small_allocations();\n-}\n-\n-TEST_VM(metaspace, freeblocks_medium) {\n-  FreeBlocksTest::test_medium_allocations();\n-}\n-\n-TEST_VM(metaspace, freeblocks_large) {\n-  FreeBlocksTest::test_large_allocations();\n-}\n-\n","filename":"test\/hotspot\/gtest\/metaspace\/test_freeblocks.cpp","additions":7,"deletions":182,"binary":false,"changes":189,"status":"modified"},{"patch":"@@ -0,0 +1,97 @@\n+\/*\n+ * Copyright (c) 2020, Oracle and\/or its affiliates. All rights reserved.\n+ * Copyright (c) 2020 SAP SE. All rights reserved.\n+ * DO NOT ALTER OR REMOVE COPYRIGHT NOTICES OR THIS FILE HEADER.\n+ *\n+ * This code is free software; you can redistribute it and\/or modify it\n+ * under the terms of the GNU General Public License version 2 only, as\n+ * published by the Free Software Foundation.\n+ *\n+ * This code is distributed in the hope that it will be useful, but WITHOUT\n+ * ANY WARRANTY; without even the implied warranty of MERCHANTABILITY or\n+ * FITNESS FOR A PARTICULAR PURPOSE.  See the GNU General Public License\n+ * version 2 for more details (a copy is included in the LICENSE file that\n+ * accompanied this code).\n+ *\n+ * You should have received a copy of the GNU General Public License version\n+ * 2 along with this work; if not, write to the Free Software Foundation,\n+ * Inc., 51 Franklin St, Fifth Floor, Boston, MA 02110-1301 USA.\n+ *\n+ * Please contact Oracle, 500 Oracle Parkway, Redwood Shores, CA 94065 USA\n+ * or visit www.oracle.com if you need additional information or have any\n+ * questions.\n+ *\n+ *\/\n+\n+#include \"precompiled.hpp\"\n+#include \"memory\/metaspace\/metablock.inline.hpp\"\n+\/\/#define LOG_PLEASE\n+#include \"metaspaceGtestCommon.hpp\"\n+\n+using metaspace::MetaBlock;\n+\n+\n+#define CHECK_BLOCK_EMPTY(block) { \\\n+  EXPECT_TRUE(block.is_empty()); \\\n+  DEBUG_ONLY(block.verify()); \\\n+}\n+\n+#define CHECK_BLOCK(block, expected_base, expected_size) { \\\n+    EXPECT_EQ(block.base(), (MetaWord*)expected_base); \\\n+    EXPECT_EQ((size_t)expected_size, block.word_size()); \\\n+    EXPECT_EQ(block.end(), expected_base + expected_size); \\\n+    DEBUG_ONLY(block.verify()); \\\n+}\n+\n+static constexpr uintptr_t large_pointer = NOT_LP64(0x99999990) LP64_ONLY(0x9999999999999990ULL);\n+\n+TEST(metaspace, MetaBlock_1) {\n+  MetaBlock bl;\n+  CHECK_BLOCK_EMPTY(bl);\n+}\n+\n+TEST(metaspace, MetaBlock_2) {\n+  MetaWord* const p = (MetaWord*)large_pointer;\n+  constexpr size_t s = G;\n+  MetaBlock bl(p, s);\n+  CHECK_BLOCK(bl, p, s);\n+}\n+\n+TEST(metaspace, MetaBlock_3) {\n+  MetaWord* const p = (MetaWord*)large_pointer;\n+  MetaBlock bl(p, 0);\n+  CHECK_BLOCK_EMPTY(bl);\n+}\n+\n+TEST_VM(metaspace, MetaBlock_4) {\n+  MetaWord* const p = (MetaWord*)large_pointer;\n+  MetaBlock bl(p, G);\n+  CHECK_BLOCK(bl, p, G);\n+\n+  MetaBlock bl_copy = bl, bl2;\n+\n+  bl2 = bl.split_off_tail(M);\n+  CHECK_BLOCK(bl, p, G - M);\n+  CHECK_BLOCK(bl2, p + G - M, M);\n+\n+  bl = bl_copy;\n+\n+bl.print_on(tty);\n+bl2.print_on(tty);\n+  bl2 = bl.split_off_tail(G);\n+  bl.print_on(tty);\n+  bl2.print_on(tty);\n+\n+  ASSERT_EQ(bl2, bl_copy);\n+  ASSERT_TRUE(bl.is_empty());\n+\n+  bl = bl_copy;\n+\n+  bl2 = bl.split_off_tail(0);\n+  ASSERT_EQ(bl, bl_copy);\n+  ASSERT_TRUE(bl2.is_empty());\n+\n+  MetaBlock empty;\n+  bl = empty.split_off_tail(0);\n+  ASSERT_TRUE(bl.is_empty());\n+}\n","filename":"test\/hotspot\/gtest\/metaspace\/test_metablock.cpp","additions":97,"deletions":0,"binary":false,"changes":97,"status":"added"},{"patch":"@@ -31,1 +31,1 @@\n-TEST_VM(MetaspaceUtils, reserved) {\n+TEST_VM(metaspace, MetaspaceUtils_reserved) {\n@@ -40,1 +40,1 @@\n-TEST_VM(MetaspaceUtils, reserved_compressed_class_pointers) {\n+TEST_VM(metaspace, MetaspaceUtils_reserved_compressed_class_pointers) {\n@@ -52,1 +52,1 @@\n-TEST_VM(MetaspaceUtils, committed) {\n+TEST_VM(metaspace, MetaspaceUtils_committed) {\n@@ -64,1 +64,1 @@\n-TEST_VM(MetaspaceUtils, committed_compressed_class_pointers) {\n+TEST_VM(metaspace, MetaspaceUtils_committed_compressed_class_pointers) {\n@@ -76,1 +76,1 @@\n-TEST_VM(MetaspaceUtils, non_compressed_class_pointers) {\n+TEST_VM(metaspace, MetaspaceUtils_non_compressed_class_pointers) {\n@@ -102,1 +102,1 @@\n-TEST_VM(MetaspaceUtils, get_statistics) {\n+TEST_VM(MetaspaceUtils, MetaspaceUtils_get_statistics) {\n","filename":"test\/hotspot\/gtest\/metaspace\/test_metaspaceUtils.cpp","additions":6,"deletions":6,"binary":false,"changes":12,"status":"modified"},{"patch":"@@ -4,0 +4,1 @@\n+ * Copyright (c) 2023 Red Hat, Inc. All rights reserved.\n@@ -31,0 +32,2 @@\n+#include \"memory\/metaspace\/freeBlocks.hpp\"\n+#include \"memory\/metaspace\/metablock.inline.hpp\"\n@@ -33,0 +36,1 @@\n+#include \"memory\/metaspace\/metachunkList.hpp\"\n@@ -36,0 +40,1 @@\n+#include \"memory\/metaspace.hpp\"\n@@ -39,1 +44,1 @@\n-\/\/#define LOG_PLEASE\n+#define LOG_PLEASE\n@@ -44,9 +49,14 @@\n-using metaspace::AllocationAlignmentByteSize;\n-using metaspace::ArenaGrowthPolicy;\n-using metaspace::CommitLimiter;\n-using metaspace::InternalStats;\n-using metaspace::MemRangeCounter;\n-using metaspace::MetaspaceArena;\n-using metaspace::SizeAtomicCounter;\n-using metaspace::Settings;\n-using metaspace::ArenaStats;\n+#define HANDLE_FAILURE \\\n+  if (testing::Test::HasFailure()) { \\\n+    return; \\\n+  }\n+\n+namespace metaspace {\n+\n+class MetaspaceArenaTestFriend {\n+  const MetaspaceArena* const _arena;\n+public:\n+  MetaspaceArenaTestFriend(const MetaspaceArena* arena) : _arena(arena) {}\n+  const MetachunkList& chunks() const { return _arena->_chunks; }\n+  const FreeBlocks* fbl() const { return _arena->_fbl; }\n+};\n@@ -57,0 +67,1 @@\n+  const ArenaGrowthPolicy* const _growth_policy;\n@@ -58,2 +69,0 @@\n-  const ArenaGrowthPolicy* _growth_policy;\n-  SizeAtomicCounter _used_words_counter;\n@@ -62,6 +71,0 @@\n-  void initialize(const ArenaGrowthPolicy* growth_policy, const char* name = \"gtest-MetaspaceArena\") {\n-    _growth_policy = growth_policy;\n-    _arena = new MetaspaceArena(&_context.cm(), _growth_policy, &_used_words_counter, name);\n-    DEBUG_ONLY(_arena->verify());\n-  }\n-\n@@ -70,9 +73,0 @@\n-  \/\/ Create a helper; growth policy for arena is determined by the given spacetype|class tupel\n-  MetaspaceArenaTestHelper(MetaspaceGtestContext& helper,\n-                            Metaspace::MetaspaceType space_type, bool is_class,\n-                            const char* name = \"gtest-MetaspaceArena\") :\n-    _context(helper)\n-  {\n-    initialize(ArenaGrowthPolicy::policy_for_space_type(space_type, is_class), name);\n-  }\n-\n@@ -81,2 +75,2 @@\n-                           const char* name = \"gtest-MetaspaceArena\") :\n-    _context(helper)\n+                           size_t allocation_alignment_words = Metaspace::min_allocation_alignment_words) :\n+    _context(helper), _growth_policy(growth_policy), _arena(nullptr)\n@@ -84,1 +78,3 @@\n-    initialize(growth_policy, name);\n+    _arena = new MetaspaceArena(_context.context(), _growth_policy, allocation_alignment_words, \"gtest-MetaspaceArena\");\n+    DEBUG_ONLY(_arena->verify());\n+    _context.inc_num_arenas_created();\n@@ -87,0 +83,8 @@\n+\n+  \/\/ Create a helper; growth policy for arena is determined by the given spacetype|class tupel\n+  MetaspaceArenaTestHelper(MetaspaceGtestContext& helper,\n+                           Metaspace::MetaspaceType space_type, bool is_class,\n+                           size_t allocation_alignment_words = Metaspace::min_allocation_alignment_words) :\n+    MetaspaceArenaTestHelper(helper, ArenaGrowthPolicy::policy_for_space_type(space_type, is_class), allocation_alignment_words)\n+  {}\n+\n@@ -91,1 +95,0 @@\n-  const CommitLimiter& limiter() const { return _context.commit_limiter(); }\n@@ -93,1 +96,0 @@\n-  SizeAtomicCounter& used_words_counter() { return _used_words_counter; }\n@@ -100,2 +102,2 @@\n-      size_t used_words_before = _used_words_counter.get();\n-      size_t committed_words_before = limiter().committed_words();\n+      size_t used_words_before = _context.used_words();\n+      size_t committed_words_before = _context.committed_words();\n@@ -105,3 +107,8 @@\n-      size_t used_words_after = _used_words_counter.get();\n-      size_t committed_words_after = limiter().committed_words();\n-      ASSERT_0(used_words_after);\n+      size_t used_words_after = _context.used_words();\n+      size_t committed_words_after = _context.committed_words();\n+      assert(_context.num_arenas_created() >= 1, \"Sanity\");\n+      if (_context.num_arenas_created() == 1) {\n+        ASSERT_0(used_words_after);\n+      } else {\n+        ASSERT_LE(used_words_after, used_words_before);\n+      }\n@@ -113,7 +120,25 @@\n-    _arena->usage_numbers(p_used, p_committed, p_capacity);\n-    if (p_used != nullptr) {\n-      if (p_committed != nullptr) {\n-        ASSERT_GE(*p_committed, *p_used);\n-      }\n-      \/\/ Since we own the used words counter, it should reflect our usage number 1:1\n-      ASSERT_EQ(_used_words_counter.get(), *p_used);\n+    size_t arena_used = 0, arena_committed = 0, arena_reserved = 0;\n+    _arena->usage_numbers(&arena_used, &arena_committed, &arena_reserved);\n+    EXPECT_GE(arena_committed, arena_used);\n+    EXPECT_GE(arena_reserved, arena_committed);\n+\n+    size_t context_used = _context.used_words();\n+    size_t context_committed = _context.committed_words();\n+    size_t context_reserved = _context.reserved_words();\n+    EXPECT_GE(context_committed, context_used);\n+    EXPECT_GE(context_reserved, context_committed);\n+\n+    \/\/ If only one arena uses the context, usage numbers must match.\n+    if (_context.num_arenas_created() == 1) {\n+      EXPECT_EQ(context_used, arena_used);\n+    } else {\n+      assert(_context.num_arenas_created() > 1, \"Sanity\");\n+      EXPECT_GE(context_used, arena_used);\n+    }\n+\n+    \/\/ commit, reserve numbers don't have to match since free chunks may exist\n+    EXPECT_GE(context_committed, arena_committed);\n+    EXPECT_GE(context_reserved, arena_reserved);\n+\n+    if (p_used) {\n+      *p_used = arena_used;\n@@ -121,2 +146,5 @@\n-    if (p_committed != nullptr && p_capacity != nullptr) {\n-      ASSERT_GE(*p_capacity, *p_committed);\n+    if (p_committed) {\n+      *p_committed = arena_committed;\n+    }\n+    if (p_capacity) {\n+      *p_capacity = arena_reserved;\n@@ -145,1 +173,0 @@\n-  \/\/ Allocate; it may or may not work; return value in *p_return_value\n@@ -147,0 +174,11 @@\n+    MetaBlock result, wastage;\n+    allocate_from_arena_with_tests(word_size, result, wastage);\n+    if (wastage.is_nonempty()) {\n+      _arena->deallocate(wastage);\n+      wastage.reset();\n+    }\n+    (*p_return_value) = result.base();\n+  }\n+\n+  \/\/ Allocate; it may or may not work; return value in *p_return_value\n+  void allocate_from_arena_with_tests(size_t word_size, MetaBlock& result, MetaBlock& wastage) {\n@@ -152,1 +190,1 @@\n-    size_t possible_expansion = limiter().possible_expansion_words();\n+    size_t possible_expansion = _context.commit_limiter().possible_expansion_words();\n@@ -154,1 +192,1 @@\n-    MetaWord* p = _arena->allocate(word_size);\n+    result = _arena->allocate(word_size, wastage);\n@@ -161,1 +199,1 @@\n-    if (p == nullptr) {\n+    if (result.is_empty()) {\n@@ -169,1 +207,2 @@\n-      ASSERT_TRUE(is_aligned(p, AllocationAlignmentByteSize));\n+      ASSERT_TRUE(result.is_aligned_base(_arena->allocation_alignment_words()));\n+\n@@ -178,2 +217,0 @@\n-\n-    *p_return_value = p;\n@@ -192,1 +229,1 @@\n-    _arena->deallocate(p, word_size);\n+    _arena->deallocate(MetaBlock(p, word_size));\n@@ -212,0 +249,4 @@\n+  MetaspaceArenaTestFriend internal_access() const {\n+    return MetaspaceArenaTestFriend (_arena);\n+  }\n+\n@@ -214,1 +255,1 @@\n-    return get_arena_statistics().totals()._num;\n+    return internal_access().chunks().count();\n@@ -411,0 +452,1 @@\n+    ASSERT_FALSE(HasFailure());\n@@ -414,0 +456,1 @@\n+    ASSERT_FALSE(HasFailure());\n@@ -420,0 +463,1 @@\n+    ASSERT_FALSE(HasFailure());\n@@ -425,0 +469,1 @@\n+    ASSERT_FALSE(HasFailure());\n@@ -428,0 +473,1 @@\n+    ASSERT_FALSE(HasFailure());\n@@ -470,0 +516,1 @@\n+    HANDLE_FAILURE\n@@ -518,1 +565,1 @@\n-  MetaspaceArenaTestHelper smhelper(context, type, is_class, \"Grower\");\n+  MetaspaceArenaTestHelper smhelper(context, type, is_class);\n@@ -520,1 +567,1 @@\n-  MetaspaceArenaTestHelper smhelper_harrasser(context, Metaspace::ReflectionMetaspaceType, true, \"Harasser\");\n+  MetaspaceArenaTestHelper smhelper_harrasser(context, Metaspace::ReflectionMetaspaceType, true);\n@@ -583,0 +630,1 @@\n+    HANDLE_FAILURE\n@@ -589,0 +637,1 @@\n+    HANDLE_FAILURE\n@@ -630,0 +679,4 @@\n+  \/\/ No FBL should exist, we did not deallocate\n+  ASSERT_EQ(smhelper.internal_access().fbl(), (FreeBlocks*)nullptr);\n+  ASSERT_EQ(smhelper_harrasser.internal_access().fbl(), (FreeBlocks*)nullptr);\n+\n@@ -733,1 +786,2 @@\n-  for (size_t blocksize = Metaspace::max_allocation_word_size(); blocksize >= 1; blocksize \/= 2) {\n+  for (size_t blocksize = Metaspace::max_allocation_word_size();\n+       blocksize >= Metaspace::min_allocation_word_size; blocksize \/= 2) {\n@@ -745,0 +799,1 @@\n+      HANDLE_FAILURE\n@@ -754,0 +809,1 @@\n+      HANDLE_FAILURE\n@@ -772,0 +828,121 @@\n+\n+static void test_random_aligned_allocation(size_t arena_alignment_words, SizeRange range) {\n+  if (Settings::use_allocation_guard()) {\n+    return;\n+  }\n+\n+  \/\/ We let the arena use 4K chunks, unless the alloc size is larger.\n+  chunklevel_t level = CHUNK_LEVEL_4K;\n+  const ArenaGrowthPolicy policy (&level, 1);\n+  const size_t chunk_word_size = word_size_for_level(level);\n+\n+  size_t expected_used = 0;\n+\n+  MetaspaceGtestContext context;\n+  MetaspaceArenaTestHelper helper(context, &policy, arena_alignment_words);\n+\n+  size_t last_alloc_size = 0;\n+  unsigned num_allocations = 0;\n+\n+  const size_t max_used = MIN2(MAX2(chunk_word_size * 10, (range.highest() * 100)),\n+                               LP64_ONLY(64) NOT_LP64(16) * M); \/\/ word size!\n+  while (expected_used < max_used) {\n+\n+    const int chunks_before = helper.get_number_of_chunks();\n+\n+    MetaBlock result, wastage;\n+    size_t alloc_words = range.random_value();\n+    NOT_LP64(alloc_words = align_up(alloc_words, Metaspace::min_allocation_alignment_words));\n+    helper.allocate_from_arena_with_tests(alloc_words, result, wastage);\n+\n+    ASSERT_TRUE(result.is_nonempty());\n+    ASSERT_TRUE(result.is_aligned_base(arena_alignment_words));\n+    ASSERT_EQ(result.word_size(), alloc_words);\n+\n+    expected_used += alloc_words + wastage.word_size();\n+    const int chunks_now = helper.get_number_of_chunks();\n+    ASSERT_GE(chunks_now, chunks_before);\n+    ASSERT_LE(chunks_now, chunks_before + 1);\n+\n+    \/\/ Estimate wastage:\n+    \/\/ Guessing at wastage is somewhat simple since we don't expect to ever use the fbl (we\n+    \/\/ don't deallocate). Therefore, wastage can only be caused by alignment gap or by\n+    \/\/ salvaging an old chunk before a new chunk is added.\n+    const bool expect_alignment_gap = !is_aligned(last_alloc_size, arena_alignment_words);\n+    const bool new_chunk_added = chunks_now > chunks_before;\n+\n+    if (num_allocations == 0) {\n+      \/\/ expect no wastage if its the first allocation in the arena\n+      ASSERT_TRUE(wastage.is_empty());\n+    } else {\n+      if (expect_alignment_gap) {\n+        \/\/ expect wastage if the alignment requires it\n+        ASSERT_TRUE(wastage.is_nonempty());\n+      }\n+    }\n+\n+    if (wastage.is_nonempty()) {\n+      \/\/ If we have wastage, we expect it to be either too small or unaligned. That would not be true\n+      \/\/ for wastage from the fbl, which could have any size; however, in this test we don't deallocate,\n+      \/\/ so we don't expect wastage from the fbl.\n+      if (wastage.is_aligned_base(arena_alignment_words)) {\n+        ASSERT_LT(wastage.word_size(), alloc_words);\n+      }\n+      if (new_chunk_added) {\n+        \/\/ chunk turnover: no more wastage than size of a commit granule, since we salvage the\n+        \/\/ committed remainder of the old chunk.\n+        ASSERT_LT(wastage.word_size(), Settings::commit_granule_words());\n+      } else {\n+        \/\/ No chunk turnover: no more wastage than what alignment requires.\n+        ASSERT_LT(wastage.word_size(), arena_alignment_words);\n+      }\n+    }\n+\n+    \/\/ Check stats too\n+    size_t used, committed, reserved;\n+    helper.usage_numbers_with_test(&used, &committed, &reserved);\n+    ASSERT_EQ(used, expected_used);\n+\n+    \/\/ No FBL should exist, we did not deallocate\n+    ASSERT_EQ(helper.internal_access().fbl(), (FreeBlocks*)nullptr);\n+\n+    HANDLE_FAILURE\n+\n+    last_alloc_size = alloc_words;\n+    num_allocations ++;\n+  }\n+  LOG(\"allocs: %u\", num_allocations);\n+}\n+\n+#define TEST_ARENA_WITH_ALIGNMENT_SMALL_RANGE(al)                              \\\n+TEST_VM(metaspace, MetaspaceArena_test_random_small_aligned_allocation_##al) { \\\n+  static const SizeRange range(Metaspace::min_allocation_word_size, 128);      \\\n+  test_random_aligned_allocation(al, range);                                   \\\n+}\n+\n+#ifdef _LP64\n+TEST_ARENA_WITH_ALIGNMENT_SMALL_RANGE(1);\n+#endif\n+TEST_ARENA_WITH_ALIGNMENT_SMALL_RANGE(2);\n+TEST_ARENA_WITH_ALIGNMENT_SMALL_RANGE(8);\n+TEST_ARENA_WITH_ALIGNMENT_SMALL_RANGE(32);\n+TEST_ARENA_WITH_ALIGNMENT_SMALL_RANGE(128);\n+TEST_ARENA_WITH_ALIGNMENT_SMALL_RANGE(MIN_CHUNK_WORD_SIZE);\n+\n+#define TEST_ARENA_WITH_ALIGNMENT_LARGE_RANGE(al)                              \\\n+TEST_VM(metaspace, MetaspaceArena_test_random_large_aligned_allocation_##al) { \\\n+  static const SizeRange range(Metaspace::max_allocation_word_size() \/ 2,      \\\n+                                   Metaspace::max_allocation_word_size());     \\\n+  test_random_aligned_allocation(al, range);                                   \\\n+}\n+\n+#ifdef _LP64\n+TEST_ARENA_WITH_ALIGNMENT_LARGE_RANGE(1);\n+#endif\n+TEST_ARENA_WITH_ALIGNMENT_LARGE_RANGE(2);\n+TEST_ARENA_WITH_ALIGNMENT_LARGE_RANGE(8);\n+TEST_ARENA_WITH_ALIGNMENT_LARGE_RANGE(32);\n+TEST_ARENA_WITH_ALIGNMENT_LARGE_RANGE(128);\n+TEST_ARENA_WITH_ALIGNMENT_LARGE_RANGE(MIN_CHUNK_WORD_SIZE);\n+\n+} \/\/ namespace metaspace\n","filename":"test\/hotspot\/gtest\/metaspace\/test_metaspacearena.cpp","additions":235,"deletions":58,"binary":false,"changes":293,"status":"modified"},{"patch":"@@ -29,0 +29,1 @@\n+#include \"memory\/metaspace\/metablock.hpp\"\n@@ -31,0 +32,1 @@\n+#include \"memory\/metaspace\/metaspaceContext.hpp\"\n@@ -46,0 +48,1 @@\n+using metaspace::MetaBlock;\n@@ -47,0 +50,1 @@\n+using metaspace::MetaspaceContext;\n@@ -129,8 +133,6 @@\n-  MetaspaceArenaTestBed(ChunkManager* cm, const ArenaGrowthPolicy* alloc_sequence,\n-                        SizeAtomicCounter* used_words_counter, SizeRange allocation_range) :\n-    _arena(nullptr),\n-    _allocation_range(allocation_range),\n-    _size_of_last_failed_allocation(0),\n-    _allocations(nullptr),\n-    _alloc_count(),\n-    _dealloc_count()\n+  MetaspaceArenaTestBed(MetaspaceContext* context, const ArenaGrowthPolicy* growth_policy,\n+                        size_t allocation_alignment_words, SizeRange allocation_range)\n+    : _arena(nullptr)\n+    , _allocation_range(allocation_range)\n+    , _size_of_last_failed_allocation(0)\n+    , _allocations(nullptr)\n@@ -138,1 +140,1 @@\n-    _arena = new MetaspaceArena(cm, alloc_sequence, used_words_counter, \"gtest-MetaspaceArenaTestBed-sm\");\n+    _arena = new MetaspaceArena(context, growth_policy, Metaspace::min_allocation_alignment_words, \"gtest-MetaspaceArenaTestBed-sm\");\n@@ -168,3 +170,10 @@\n-    MetaWord* p = _arena->allocate(word_size);\n-    if (p != nullptr) {\n-      EXPECT_TRUE(is_aligned(p, AllocationAlignmentByteSize));\n+    MetaBlock wastage;\n+    MetaBlock bl = _arena->allocate(word_size, wastage);\n+    \/\/ We only expect wastage if either alignment was not met or the chunk remainder\n+    \/\/ was not large enough.\n+    if (wastage.is_nonempty()) {\n+      _arena->deallocate(wastage);\n+      wastage.reset();\n+    }\n+    if (bl.is_nonempty()) {\n+      EXPECT_TRUE(is_aligned(bl.base(), AllocationAlignmentByteSize));\n@@ -174,1 +183,1 @@\n-      a->p = p;\n+      a->p = bl.base();\n@@ -198,1 +207,1 @@\n-      _arena->deallocate(a->p, a->word_size);\n+      _arena->deallocate(MetaBlock(a->p, a->word_size));\n@@ -223,2 +232,2 @@\n-    MetaspaceArenaTestBed* bed = new MetaspaceArenaTestBed(&_context.cm(), growth_policy,\n-                                                       &_used_words_counter, allocation_range);\n+    MetaspaceArenaTestBed* bed = new MetaspaceArenaTestBed(_context.context(), growth_policy,\n+        Metaspace::min_allocation_alignment_words, allocation_range);\n","filename":"test\/hotspot\/gtest\/metaspace\/test_metaspacearena_stress.cpp","additions":25,"deletions":16,"binary":false,"changes":41,"status":"modified"},{"patch":"@@ -978,1 +978,0 @@\n-\n","filename":"test\/hotspot\/gtest\/runtime\/test_os.cpp","additions":0,"deletions":1,"binary":false,"changes":1,"status":"modified"},{"patch":"@@ -43,1 +43,1 @@\n-\/* @test id=balanced-with-guards\n+\/* @test id=MetaspaceGuardAllocations-on\n@@ -53,1 +53,1 @@\n-\/* @test id=balanced-no-ccs\n+\/* @test id=UseCompressedClassPointers-off\n@@ -62,0 +62,11 @@\n+\n+\/* @test id=UseCompactObjectHeaders\n+ * @summary Run metaspace-related gtests with tiny classpointers\n+ * @library \/test\/lib\n+ * @modules java.base\/jdk.internal.misc\n+ *          java.xml\n+ * @requires vm.bits == 64\n+ * @requires vm.flagless\n+ * @requires vm.debug\n+ * @run main\/native GTestWrapper --gtest_filter=metaspace* -XX:+UnlockExperimentalVMOptions -XX:+UseCompactObjectHeaders\n+ *\/\n","filename":"test\/hotspot\/jtreg\/gtest\/MetaspaceGtests.java","additions":13,"deletions":2,"binary":false,"changes":15,"status":"modified"},{"patch":"@@ -1,40 +0,0 @@\n-\/*\n- * Copyright (C) 2021 THL A29 Limited, a Tencent company. All rights reserved.\n- * DO NOT ALTER OR REMOVE COPYRIGHT NOTICES OR THIS FILE HEADER.\n- *\n- * This code is free software; you can redistribute it and\/or modify it\n- * under the terms of the GNU General Public License version 2 only, as\n- * published by the Free Software Foundation.\n- *\n- * This code is distributed in the hope that it will be useful, but WITHOUT\n- * ANY WARRANTY; without even the implied warranty of MERCHANTABILITY or\n- * FITNESS FOR A PARTICULAR PURPOSE.  See the GNU General Public License\n- * version 2 for more details (a copy is included in the LICENSE file that\n- * accompanied this code).\n- *\n- * You should have received a copy of the GNU General Public License version\n- * 2 along with this work; if not, write to the Free Software Foundation,\n- * Inc., 51 Franklin St, Fifth Floor, Boston, MA 02110-1301 USA.\n- *\n- * Please contact Oracle, 500 Oracle Parkway, Redwood Shores, CA 94065 USA\n- * or visit www.oracle.com if you need additional information or have any\n- * questions.\n- *\n- *\/\n-\n-\/*\n- * Note: This runs the metaspace utils related parts of gtest in configurations which\n- *  are not tested explicitly in the standard gtests.\n- *\n- *\/\n-\n-\/* @test\n- * @bug 8264008\n- * @summary Run metaspace utils related gtests with compressed class pointers off\n- * @requires vm.bits == 64\n- * @library \/test\/lib\n- * @modules java.base\/jdk.internal.misc\n- *          java.xml\n- * @requires vm.flagless\n- * @run main\/native GTestWrapper --gtest_filter=MetaspaceUtils* -XX:-UseCompressedClassPointers\n- *\/\n","filename":"test\/hotspot\/jtreg\/gtest\/MetaspaceUtilsGtests.java","additions":0,"deletions":40,"binary":false,"changes":40,"status":"deleted"},{"patch":"@@ -58,0 +58,1 @@\n+                \"-XX:+UnlockExperimentalVMOptions\", \"-XX:-UseCompactObjectHeaders\",\n","filename":"test\/hotspot\/jtreg\/runtime\/CompressedOops\/CompressedCPUSpecificClassSpaceReservation.java","additions":1,"deletions":0,"binary":false,"changes":1,"status":"modified"},{"patch":"@@ -45,1 +45,1 @@\n-    private static void test(long forceAddress, long classSpaceSize, long expectedEncodingBase, int expectedEncodingShift) throws IOException {\n+    private static void test(long forceAddress, boolean COH, long classSpaceSize, long expectedEncodingBase, int expectedEncodingShift) throws IOException {\n@@ -52,0 +52,2 @@\n+                \"-XX:+UnlockExperimentalVMOptions\",\n+                \"-XX:\" + (COH ? \"+\" : \"-\") + \"UseCompactObjectHeaders\",\n@@ -63,1 +65,2 @@\n-            throw new SkippedException(\"Skipping because we cannot force ccs to \" + forceAddressString);\n+            System.out.println(\"Skipping because we cannot force ccs to \" + forceAddressString);\n+            return;\n@@ -76,1 +79,1 @@\n-        test(4 * G - 128 * M, 128 * M, 0, 0);\n+        test(4 * G - 128 * M, false, 128 * M, 0, 0);\n@@ -78,0 +81,19 @@\n+        \/\/ Test ccs nestling right at the end of the 32G range\n+        \/\/ Expecting:\n+        \/\/ - non-aarch64: base=0, shift=3\n+        \/\/ - aarch64: base to start of class range, shift 0\n+        if (Platform.isAArch64()) {\n+            long forceAddress = 0x70000000; \/\/ make sure we have a valid EOR immediate\n+            test(forceAddress, false, G, forceAddress, 0);\n+        } else {\n+            test(32 * G - 128 * M, false, 128 * M, 0, 3);\n+        }\n+\n+        \/\/ Test ccs starting *below* 4G, but extending upwards beyond 4G. All platforms except aarch64 should pick\n+        \/\/ zero based encoding.\n+        if (Platform.isAArch64()) {\n+            long forceAddress = 0xc0000000; \/\/ make sure we have a valid EOR immediate\n+            test(forceAddress, false, G + (128 * M), forceAddress, 0);\n+        } else {\n+            test(4 * G - 128 * M, false, 2 * 128 * M, 0, 3);\n+        }\n@@ -80,0 +102,19 @@\n+        \/\/ Compact Object Header Mode with tiny classpointers\n+        \/\/ On all platforms we expect the VM to chose the smallest possible shift value needed to cover the encoding range\n+        long forceAddress = 30 * G;\n+\n+        long ccsSize = 128 * M;\n+        int expectedShift = 6;\n+        test(forceAddress, true, ccsSize, forceAddress, expectedShift);\n+\n+        ccsSize = 512 * M;\n+        expectedShift = 8;\n+        test(forceAddress, true, ccsSize, forceAddress, expectedShift);\n+\n+        ccsSize = G;\n+        expectedShift = 9;\n+        test(forceAddress, true, ccsSize, forceAddress, expectedShift);\n+\n+        ccsSize = 3 * G;\n+        expectedShift = 10;\n+        test(forceAddress, true, ccsSize, forceAddress, expectedShift);\n","filename":"test\/hotspot\/jtreg\/runtime\/CompressedOops\/CompressedClassPointersEncodingScheme.java","additions":44,"deletions":3,"binary":false,"changes":47,"status":"modified"}]}