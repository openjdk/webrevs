{"files":[{"patch":"@@ -2,1 +2,1 @@\n-# Copyright (c) 2022, Oracle and\/or its affiliates. All rights reserved.\n+# Copyright (c) 2022, 2023, Oracle and\/or its affiliates. All rights reserved.\n","filename":".github\/workflows\/build-cross-compile.yml","additions":1,"deletions":1,"binary":false,"changes":2,"status":"modified"},{"patch":"@@ -4,1 +4,1 @@\n-version=22\n+version=23\n","filename":".jcheck\/conf","additions":1,"deletions":1,"binary":false,"changes":2,"status":"modified"},{"patch":"@@ -1514,1 +1514,0 @@\n-  assert(VM_Version::supports_cx8(), \"wrong machine\");\n@@ -2682,0 +2681,1 @@\n+    assert_different_registers(obj, tmp, rscratch1, rscratch2, mdo_addr.base(), mdo_addr.index());\n@@ -2683,0 +2683,2 @@\n+  } else {\n+    assert_different_registers(obj, rscratch1, rscratch2, mdo_addr.base(), mdo_addr.index());\n@@ -2739,2 +2741,2 @@\n-          \/\/ There is a chance that the checks above (re-reading profiling\n-          \/\/ data from memory) fail if another thread has just set the\n+          \/\/ There is a chance that the checks above\n+          \/\/ fail if another thread has just set the\n@@ -2743,0 +2745,1 @@\n+          __ eor(tmp, tmp, rscratch2); \/\/ get back original value before XOR\n@@ -2767,0 +2770,4 @@\n+#ifdef ASSERT\n+        __ andr(tmp, tmp, TypeEntries::type_mask);\n+        __ verify_klass_ptr(tmp);\n+#endif\n@@ -2798,0 +2805,4 @@\n+#ifdef ASSERT\n+        __ andr(tmp, tmp, TypeEntries::type_mask);\n+        __ verify_klass_ptr(tmp);\n+#endif\n","filename":"src\/hotspot\/cpu\/aarch64\/c1_LIRAssembler_aarch64.cpp","additions":14,"deletions":3,"binary":false,"changes":17,"status":"modified"},{"patch":"@@ -1343,1 +1343,1 @@\n-                                           FloatRegister src, BasicType src_bt) {\n+                                           FloatRegister src, BasicType src_bt, bool is_unsigned) {\n@@ -1347,2 +1347,1 @@\n-      assert(dst_vlen_in_bytes == 8 || dst_vlen_in_bytes == 16, \"unsupported\");\n-      sxtl(dst, T8H, src, T8B);\n+      _xshll(is_unsigned, dst, T8H, src, T8B, 0);\n@@ -1352,2 +1351,2 @@\n-      sxtl(dst, T8H, src, T8B);\n-      sxtl(dst, T4S, dst, T4H);\n+      _xshll(is_unsigned, dst, T8H, src, T8B, 0);\n+      _xshll(is_unsigned, dst, T4S, dst, T4H, 0);\n@@ -1358,1 +1357,1 @@\n-    sxtl(dst, T4S, src, T4H);\n+    _xshll(is_unsigned, dst, T4S, src, T4H, 0);\n@@ -1362,1 +1361,1 @@\n-    sxtl(dst, T2D, src, T2S);\n+    _xshll(is_unsigned, dst, T2D, src, T2S, 0);\n@@ -1396,1 +1395,2 @@\n-                                          FloatRegister src, SIMD_RegVariant src_size) {\n+                                          FloatRegister src, SIMD_RegVariant src_size,\n+                                          bool is_unsigned) {\n@@ -1398,0 +1398,1 @@\n+\n@@ -1401,1 +1402,1 @@\n-      sve_sunpklo(dst, H, src);\n+      _sve_xunpk(is_unsigned, \/* is_high *\/ false, dst, H, src);\n@@ -1404,2 +1405,2 @@\n-      sve_sunpklo(dst, H, src);\n-      sve_sunpklo(dst, S, dst);\n+      _sve_xunpk(is_unsigned, \/* is_high *\/ false, dst, H, src);\n+      _sve_xunpk(is_unsigned, \/* is_high *\/ false, dst, S, dst);\n@@ -1408,3 +1409,3 @@\n-      sve_sunpklo(dst, H, src);\n-      sve_sunpklo(dst, S, dst);\n-      sve_sunpklo(dst, D, dst);\n+      _sve_xunpk(is_unsigned, \/* is_high *\/ false, dst, H, src);\n+      _sve_xunpk(is_unsigned, \/* is_high *\/ false, dst, S, dst);\n+      _sve_xunpk(is_unsigned, \/* is_high *\/ false, dst, D, dst);\n@@ -1417,1 +1418,1 @@\n-      sve_sunpklo(dst, S, src);\n+      _sve_xunpk(is_unsigned, \/* is_high *\/ false, dst, S, src);\n@@ -1419,2 +1420,2 @@\n-      sve_sunpklo(dst, S, src);\n-      sve_sunpklo(dst, D, dst);\n+      _sve_xunpk(is_unsigned, \/* is_high *\/ false, dst, S, src);\n+      _sve_xunpk(is_unsigned, \/* is_high *\/ false, dst, D, dst);\n@@ -1423,1 +1424,1 @@\n-    sve_sunpklo(dst, D, src);\n+    _sve_xunpk(is_unsigned, \/* is_high *\/ false, dst, D, src);\n","filename":"src\/hotspot\/cpu\/aarch64\/c2_MacroAssembler_aarch64.cpp","additions":19,"deletions":18,"binary":false,"changes":37,"status":"modified"},{"patch":"@@ -97,1 +97,1 @@\n-                          FloatRegister src, BasicType src_bt);\n+                          FloatRegister src, BasicType src_bt, bool is_unsigned = false);\n@@ -103,1 +103,1 @@\n-                         FloatRegister src, SIMD_RegVariant src_size);\n+                         FloatRegister src, SIMD_RegVariant src_size, bool is_unsigned = false);\n","filename":"src\/hotspot\/cpu\/aarch64\/c2_MacroAssembler_aarch64.hpp","additions":2,"deletions":2,"binary":false,"changes":4,"status":"modified"},{"patch":"@@ -4453,0 +4453,17 @@\n+void MacroAssembler::restore_cpu_control_state_after_jni(Register tmp1, Register tmp2) {\n+  if (RestoreMXCSROnJNICalls) {\n+    Label OK;\n+    get_fpcr(tmp1);\n+    mov(tmp2, tmp1);\n+    \/\/ Set FPCR to the state we need. We do want Round to Nearest. We\n+    \/\/ don't want non-IEEE rounding modes or floating-point traps.\n+    bfi(tmp1, zr, 22, 4); \/\/ Clear DN, FZ, and Rmode\n+    bfi(tmp1, zr, 8, 5);  \/\/ Clear exception-control bits (8-12)\n+    bfi(tmp1, zr, 0, 2);  \/\/ Clear AH:FIZ\n+    eor(tmp2, tmp1, tmp2);\n+    cbz(tmp2, OK);        \/\/ Only reset FPCR if it's wrong\n+    set_fpcr(tmp1);\n+    bind(OK);\n+  }\n+}\n+\n@@ -5699,1 +5716,1 @@\n-\/\/     return zero (0) if copy fails, otherwise 'len'.\n+\/\/     return index of non-latin1 character if copy fails, otherwise 'len'.\n@@ -5916,0 +5933,3 @@\n+\/\/ Intrinsic for java.lang.StringUTF16.compress(char[] src, int srcOff, byte[] dst, int dstOff, int len)\n+\/\/ Return the array length if every element in array can be encoded,\n+\/\/ otherwise, the index of first non-latin1 (> 0xff) character.\n@@ -5922,3 +5942,0 @@\n-  \/\/ Adjust result: res == len ? len : 0\n-  cmp(len, res);\n-  csel(res, res, zr, EQ);\n","filename":"src\/hotspot\/cpu\/aarch64\/macroAssembler_aarch64.cpp","additions":21,"deletions":4,"binary":false,"changes":25,"status":"modified"},{"patch":"@@ -1052,2 +1052,2 @@\n-  \/\/ only if +VerifyFPU\n-  void verify_FPU(int stack_depth, const char* s = \"illegal FPU state\");\n+  \/\/ Restore cpu control state after JNI call\n+  void restore_cpu_control_state_after_jni(Register tmp1, Register tmp2);\n","filename":"src\/hotspot\/cpu\/aarch64\/macroAssembler_aarch64.hpp","additions":2,"deletions":2,"binary":false,"changes":4,"status":"modified"},{"patch":"@@ -1388,1 +1388,0 @@\n-    assert(VM_Version::supports_cx8(), \"wrong machine\");\n","filename":"src\/hotspot\/cpu\/arm\/c1_LIRAssembler_arm.cpp","additions":0,"deletions":1,"binary":false,"changes":1,"status":"modified"},{"patch":"@@ -347,6 +347,1 @@\n-  RuntimeAddress target(CAST_FROM_FN_PTR(address, SharedRuntime::fixup_callers_callsite));\n-  __ relocate(target.rspec(), [&] {\n-    int32_t offset;\n-    __ la_patchable(t0, target, offset);\n-    __ jalr(x1, t0, offset);\n-  });\n+  __ rt_call(CAST_FROM_FN_PTR(address, SharedRuntime::fixup_callers_callsite));\n@@ -1625,1 +1620,1 @@\n-      __ la_patchable(t0, target, offset);\n+      __ la(t0, target.target(), offset);\n@@ -1849,1 +1844,1 @@\n-      __ la_patchable(t0, target, offset);\n+      __ la(t0, target.target(), offset);\n@@ -1982,6 +1977,1 @@\n-    RuntimeAddress target(CAST_FROM_FN_PTR(address, JavaThread::check_special_condition_for_native_trans));\n-    __ relocate(target.rspec(), [&] {\n-      int32_t offset;\n-      __ la_patchable(t0, target, offset);\n-      __ jalr(x1, t0, offset);\n-    });\n+    __ rt_call(CAST_FROM_FN_PTR(address, JavaThread::check_special_condition_for_native_trans));\n@@ -2159,6 +2149,1 @@\n-    RuntimeAddress target(CAST_FROM_FN_PTR(address, Deoptimization::uncommon_trap));\n-    __ relocate(target.rspec(), [&] {\n-      int32_t offset;\n-      __ la_patchable(t0, target, offset);\n-      __ jalr(x1, t0, offset);\n-    });\n+    __ rt_call(CAST_FROM_FN_PTR(address, Deoptimization::uncommon_trap));\n@@ -2256,6 +2241,1 @@\n-  RuntimeAddress target(CAST_FROM_FN_PTR(address, Deoptimization::fetch_unroll_info));\n-  __ relocate(target.rspec(), [&] {\n-    int32_t offset;\n-    __ la_patchable(t0, target, offset);\n-    __ jalr(x1, t0, offset);\n-  });\n+  __ rt_call(CAST_FROM_FN_PTR(address, Deoptimization::fetch_unroll_info));\n@@ -2403,6 +2383,1 @@\n-  target = RuntimeAddress(CAST_FROM_FN_PTR(address, Deoptimization::unpack_frames));\n-  __ relocate(target.rspec(), [&] {\n-    int32_t offset;\n-    __ la_patchable(t0, target, offset);\n-    __ jalr(x1, t0, offset);\n-  });\n+  __ rt_call(CAST_FROM_FN_PTR(address, Deoptimization::unpack_frames));\n@@ -2498,6 +2473,1 @@\n-  RuntimeAddress target(CAST_FROM_FN_PTR(address, Deoptimization::uncommon_trap));\n-  __ relocate(target.rspec(), [&] {\n-    int32_t offset;\n-    __ la_patchable(t0, target, offset);\n-    __ jalr(x1, t0, offset);\n-  });\n+  __ rt_call(CAST_FROM_FN_PTR(address, Deoptimization::uncommon_trap));\n@@ -2625,6 +2595,1 @@\n-  target = RuntimeAddress(CAST_FROM_FN_PTR(address, Deoptimization::unpack_frames));\n-  __ relocate(target.rspec(), [&] {\n-    int32_t offset;\n-    __ la_patchable(t0, target, offset);\n-    __ jalr(x1, t0, offset);\n-  });\n+  __ rt_call(CAST_FROM_FN_PTR(address, Deoptimization::unpack_frames));\n@@ -2699,6 +2664,1 @@\n-  RuntimeAddress target(call_ptr);\n-  __ relocate(target.rspec(), [&] {\n-    int32_t offset;\n-    __ la_patchable(t0, target, offset);\n-    __ jalr(x1, t0, offset);\n-  });\n+  __ rt_call(call_ptr);\n@@ -2812,6 +2772,1 @@\n-    RuntimeAddress target(destination);\n-    __ relocate(target.rspec(), [&] {\n-      int32_t offset;\n-      __ la_patchable(t0, target, offset);\n-      __ jalr(x1, t0, offset);\n-    });\n+    __ rt_call(destination);\n@@ -2946,7 +2901,1 @@\n-  RuntimeAddress target(CAST_FROM_FN_PTR(address, OptoRuntime::handle_exception_C));\n-  __ relocate(target.rspec(), [&] {\n-    int32_t offset;\n-    __ la_patchable(t0, target, offset);\n-    __ jalr(x1, t0, offset);\n-  });\n-\n+  __ rt_call(CAST_FROM_FN_PTR(address, OptoRuntime::handle_exception_C));\n","filename":"src\/hotspot\/cpu\/riscv\/sharedRuntime_riscv.cpp","additions":12,"deletions":63,"binary":false,"changes":75,"status":"modified"},{"patch":"@@ -1932,1 +1932,1 @@\n-  if (LP64_ONLY(false &&) op->code() == lir_cas_long && VM_Version::supports_cx8()) {\n+  if (LP64_ONLY(false &&) op->code() == lir_cas_long) {\n@@ -3636,2 +3636,13 @@\n-  if (tmp != obj) {\n-    __ mov(tmp, obj);\n+#ifdef ASSERT\n+  if (obj == tmp) {\n+#ifdef _LP64\n+    assert_different_registers(obj, rscratch1, mdo_addr.base(), mdo_addr.index());\n+#else\n+    assert_different_registers(obj, mdo_addr.base(), mdo_addr.index());\n+#endif\n+  } else {\n+#ifdef _LP64\n+    assert_different_registers(obj, tmp, rscratch1, mdo_addr.base(), mdo_addr.index());\n+#else\n+    assert_different_registers(obj, tmp, mdo_addr.base(), mdo_addr.index());\n+#endif\n@@ -3639,0 +3650,1 @@\n+#endif\n@@ -3640,1 +3652,1 @@\n-    __ testptr(tmp, tmp);\n+    __ testptr(obj, obj);\n@@ -3643,0 +3655,8 @@\n+      __ testptr(mdo_addr, TypeEntries::null_seen);\n+#ifndef ASSERT\n+      __ jccb(Assembler::notZero, next); \/\/ already set\n+#else\n+      __ jcc(Assembler::notZero, next); \/\/ already set\n+#endif\n+      \/\/ atomic update to prevent overwriting Klass* with 0\n+      __ lock();\n@@ -3653,1 +3673,1 @@\n-    __ testptr(tmp, tmp);\n+    __ testptr(obj, obj);\n@@ -3665,1 +3685,1 @@\n-      __ load_klass(tmp, tmp, tmp_load_klass);\n+      __ load_klass(tmp, obj, tmp_load_klass);\n@@ -3680,1 +3700,1 @@\n-          __ load_klass(tmp, tmp, tmp_load_klass);\n+          __ load_klass(tmp, obj, tmp_load_klass);\n@@ -3682,1 +3702,3 @@\n-\n+#ifdef _LP64\n+        __ mov(rscratch1, tmp); \/\/ save original value before XOR\n+#endif\n@@ -3693,4 +3715,3 @@\n-          __ cmpptr(mdo_addr, 0);\n-          __ jccb(Assembler::equal, none);\n-          __ cmpptr(mdo_addr, TypeEntries::null_seen);\n-          __ jccb(Assembler::equal, none);\n+          __ testptr(mdo_addr, TypeEntries::type_mask);\n+          __ jccb(Assembler::zero, none);\n+#ifdef _LP64\n@@ -3700,0 +3721,1 @@\n+          __ mov(tmp, rscratch1); \/\/ get back original value before XOR\n@@ -3703,0 +3725,1 @@\n+#endif\n@@ -3708,2 +3731,1 @@\n-        __ movptr(tmp, mdo_addr);\n-        __ testptr(tmp, TypeEntries::type_unknown);\n+        __ testptr(mdo_addr, TypeEntries::type_unknown);\n@@ -3722,0 +3744,4 @@\n+#ifdef ASSERT\n+        __ andptr(tmp, TypeEntries::type_klass_mask);\n+        __ verify_klass_ptr(tmp);\n+#endif\n@@ -3736,4 +3762,2 @@\n-          __ cmpptr(mdo_addr, 0);\n-          __ jcc(Assembler::equal, ok);\n-          __ cmpptr(mdo_addr, TypeEntries::null_seen);\n-          __ jcc(Assembler::equal, ok);\n+          __ testptr(mdo_addr, TypeEntries::type_mask);\n+          __ jcc(Assembler::zero, ok);\n@@ -3755,0 +3779,4 @@\n+#ifdef ASSERT\n+        __ andptr(tmp, TypeEntries::type_klass_mask);\n+        __ verify_klass_ptr(tmp);\n+#endif\n@@ -3759,2 +3787,1 @@\n-        __ movptr(tmp, mdo_addr);\n-        __ testptr(tmp, TypeEntries::type_unknown);\n+        __ testptr(mdo_addr, TypeEntries::type_unknown);\n@@ -3766,2 +3793,1 @@\n-\n-    __ bind(next);\n+  __ bind(next);\n","filename":"src\/hotspot\/cpu\/x86\/c1_LIRAssembler_x86.cpp","additions":48,"deletions":22,"binary":false,"changes":70,"status":"modified"},{"patch":"@@ -1097,0 +1097,24 @@\n+  \/* Note on 'non-obvious' assembly sequence:\n+   *\n+   * While there are vminps\/vmaxps instructions, there are two important differences between hardware\n+   * and Java on how they handle floats:\n+   *  a. -0.0 and +0.0 are considered equal (vminps\/vmaxps will return second parameter when inputs are equal)\n+   *  b. NaN is not necesarily propagated (vminps\/vmaxps will return second parameter when either input is NaN)\n+   *\n+   * It is still more efficient to use vminps\/vmaxps, but with some pre\/post-processing:\n+   *  a. -0.0\/+0.0: Bias negative (positive) numbers to second parameter before vminps (vmaxps)\n+   *                (only useful when signs differ, noop otherwise)\n+   *  b. NaN: Check if it was the first parameter that had the NaN (with vcmp[UNORD_Q])\n+\n+   *  Following pseudo code describes the algorithm for max[FD] (Min algorithm is on similar lines):\n+   *   btmp = (b < +0.0) ? a : b\n+   *   atmp = (b < +0.0) ? b : a\n+   *   Tmp  = Max_Float(atmp , btmp)\n+   *   Res  = (atmp == NaN) ? atmp : Tmp\n+   *\/\n+\n+  void (MacroAssembler::*vblend)(XMMRegister, XMMRegister, XMMRegister, XMMRegister, int, bool, XMMRegister);\n+  void (MacroAssembler::*vmaxmin)(XMMRegister, XMMRegister, XMMRegister, int);\n+  void (MacroAssembler::*vcmp)(XMMRegister, XMMRegister, XMMRegister, int, int);\n+  XMMRegister mask;\n+\n@@ -1098,5 +1122,4 @@\n-    vblendvps(atmp, a, b, a, vlen_enc);\n-    vblendvps(btmp, b, a, a, vlen_enc);\n-    vminps(tmp, atmp, btmp, vlen_enc);\n-    vcmpps(btmp, atmp, atmp, Assembler::UNORD_Q, vlen_enc);\n-    vblendvps(dst, tmp, atmp, btmp, vlen_enc);\n+    mask = a;\n+    vblend = &MacroAssembler::vblendvps;\n+    vmaxmin = &MacroAssembler::vminps;\n+    vcmp = &MacroAssembler::vcmpps;\n@@ -1104,5 +1127,4 @@\n-    vblendvps(btmp, b, a, b, vlen_enc);\n-    vblendvps(atmp, a, b, b, vlen_enc);\n-    vmaxps(tmp, atmp, btmp, vlen_enc);\n-    vcmpps(btmp, atmp, atmp, Assembler::UNORD_Q, vlen_enc);\n-    vblendvps(dst, tmp, atmp, btmp, vlen_enc);\n+    mask = b;\n+    vblend = &MacroAssembler::vblendvps;\n+    vmaxmin = &MacroAssembler::vmaxps;\n+    vcmp = &MacroAssembler::vcmpps;\n@@ -1110,5 +1132,4 @@\n-    vblendvpd(atmp, a, b, a, vlen_enc);\n-    vblendvpd(btmp, b, a, a, vlen_enc);\n-    vminpd(tmp, atmp, btmp, vlen_enc);\n-    vcmppd(btmp, atmp, atmp, Assembler::UNORD_Q, vlen_enc);\n-    vblendvpd(dst, tmp, atmp, btmp, vlen_enc);\n+    mask = a;\n+    vblend = &MacroAssembler::vblendvpd;\n+    vmaxmin = &MacroAssembler::vminpd;\n+    vcmp = &MacroAssembler::vcmppd;\n@@ -1117,5 +1138,4 @@\n-    vblendvpd(btmp, b, a, b, vlen_enc);\n-    vblendvpd(atmp, a, b, b, vlen_enc);\n-    vmaxpd(tmp, atmp, btmp, vlen_enc);\n-    vcmppd(btmp, atmp, atmp, Assembler::UNORD_Q, vlen_enc);\n-    vblendvpd(dst, tmp, atmp, btmp, vlen_enc);\n+    mask = b;\n+    vblend = &MacroAssembler::vblendvpd;\n+    vmaxmin = &MacroAssembler::vmaxpd;\n+    vcmp = &MacroAssembler::vcmppd;\n@@ -1123,0 +1143,26 @@\n+\n+  \/\/ Make sure EnableX86ECoreOpts isn't disabled on register overlaps\n+  XMMRegister maxmin, scratch;\n+  if (dst == btmp) {\n+    maxmin = btmp;\n+    scratch = tmp;\n+  } else {\n+    maxmin = tmp;\n+    scratch = btmp;\n+  }\n+\n+  bool precompute_mask = EnableX86ECoreOpts && UseAVX>1;\n+  if (precompute_mask && !is_double_word) {\n+    vpsrad(tmp, mask, 32, vlen_enc);\n+    mask = tmp;\n+  } else if (precompute_mask && is_double_word) {\n+    vpxor(tmp, tmp, tmp, vlen_enc);\n+    vpcmpgtq(tmp, tmp, mask, vlen_enc);\n+    mask = tmp;\n+  }\n+\n+  (this->*vblend)(atmp, a, b, mask, vlen_enc, !precompute_mask, btmp);\n+  (this->*vblend)(btmp, b, a, mask, vlen_enc, !precompute_mask, tmp);\n+  (this->*vmaxmin)(maxmin, atmp, btmp, vlen_enc);\n+  (this->*vcmp)(scratch, atmp, atmp, Assembler::UNORD_Q, vlen_enc);\n+  (this->*vblend)(dst, maxmin, atmp, scratch, vlen_enc, false, scratch);\n@@ -5321,1 +5367,1 @@\n-    vblendvpd(dst, one, dst, src, vec_enc);\n+    vblendvpd(dst, one, dst, src, vec_enc, true, xtmp1);\n@@ -5324,1 +5370,1 @@\n-    vblendvpd(dst, dst, src, xtmp1, vec_enc);\n+    vblendvpd(dst, dst, src, xtmp1, vec_enc, false, xtmp1);\n@@ -5329,1 +5375,1 @@\n-    vblendvps(dst, one, dst, src, vec_enc);\n+    vblendvps(dst, one, dst, src, vec_enc, true, xtmp1);\n@@ -5332,1 +5378,1 @@\n-    vblendvps(dst, dst, src, xtmp1, vec_enc);\n+    vblendvps(dst, dst, src, xtmp1, vec_enc, false, xtmp1);\n","filename":"src\/hotspot\/cpu\/x86\/c2_MacroAssembler_x86.cpp","additions":70,"deletions":24,"binary":false,"changes":94,"status":"modified"},{"patch":"@@ -3569,0 +3569,50 @@\n+\/\/ Vector float blend\n+\/\/ vblendvps(XMMRegister dst, XMMRegister nds, XMMRegister src, XMMRegister mask, int vector_len, bool compute_mask = true, XMMRegister scratch = xnoreg)\n+void MacroAssembler::vblendvps(XMMRegister dst, XMMRegister src1, XMMRegister src2, XMMRegister mask, int vector_len, bool compute_mask, XMMRegister scratch) {\n+  \/\/ WARN: Allow dst == (src1|src2), mask == scratch\n+  bool blend_emulation = EnableX86ECoreOpts && UseAVX > 1;\n+  bool scratch_available = scratch != xnoreg && scratch != src1 && scratch != src2 && scratch != dst;\n+  bool dst_available = dst != mask && (dst != src1 || dst != src2);\n+  if (blend_emulation && scratch_available && dst_available) {\n+    if (compute_mask) {\n+      vpsrad(scratch, mask, 32, vector_len);\n+      mask = scratch;\n+    }\n+    if (dst == src1) {\n+      vpandn(dst,     mask, src1, vector_len); \/\/ if mask == 0, src1\n+      vpand (scratch, mask, src2, vector_len); \/\/ if mask == 1, src2\n+    } else {\n+      vpand (dst,     mask, src2, vector_len); \/\/ if mask == 1, src2\n+      vpandn(scratch, mask, src1, vector_len); \/\/ if mask == 0, src1\n+    }\n+    vpor(dst, dst, scratch, vector_len);\n+  } else {\n+    Assembler::vblendvps(dst, src1, src2, mask, vector_len);\n+  }\n+}\n+\n+\/\/ vblendvpd(XMMRegister dst, XMMRegister nds, XMMRegister src, XMMRegister mask, int vector_len, bool compute_mask = true, XMMRegister scratch = xnoreg)\n+void MacroAssembler::vblendvpd(XMMRegister dst, XMMRegister src1, XMMRegister src2, XMMRegister mask, int vector_len, bool compute_mask, XMMRegister scratch) {\n+  \/\/ WARN: Allow dst == (src1|src2), mask == scratch\n+  bool blend_emulation = EnableX86ECoreOpts && UseAVX > 1;\n+  bool scratch_available = scratch != xnoreg && scratch != src1 && scratch != src2 && scratch != dst && (!compute_mask || scratch != mask);\n+  bool dst_available = dst != mask && (dst != src1 || dst != src2);\n+  if (blend_emulation && scratch_available && dst_available) {\n+    if (compute_mask) {\n+      vpxor(scratch, scratch, scratch, vector_len);\n+      vpcmpgtq(scratch, scratch, mask, vector_len);\n+      mask = scratch;\n+    }\n+    if (dst == src1) {\n+      vpandn(dst,     mask, src1, vector_len); \/\/ if mask == 0, src\n+      vpand (scratch, mask, src2, vector_len); \/\/ if mask == 1, src2\n+    } else {\n+      vpand (dst,     mask, src2, vector_len); \/\/ if mask == 1, src2\n+      vpandn(scratch, mask, src1, vector_len); \/\/ if mask == 0, src\n+    }\n+    vpor(dst, dst, scratch, vector_len);\n+  } else {\n+    Assembler::vblendvpd(dst, src1, src2, mask, vector_len);\n+  }\n+}\n+\n@@ -8638,1 +8688,3 @@\n-\/\/   ..\\jdk\\src\\java.base\\share\\classes\\java\\lang\\StringUTF16.java\n+\/\/ Intrinsic for java.lang.StringUTF16.compress(char[] src, int srcOff, byte[] dst, int dstOff, int len)\n+\/\/ Return the array length if every element in array can be encoded,\n+\/\/ otherwise, the index of first non-latin1 (> 0xff) character.\n@@ -8640,1 +8692,1 @@\n-\/\/   private static int compress(char[] src, int srcOff, byte[] dst, int dstOff, int len) {\n+\/\/   public static int compress(char[] src, int srcOff, byte[] dst, int dstOff, int len) {\n@@ -8642,3 +8694,3 @@\n-\/\/       int c = src[srcOff++];\n-\/\/       if (c >>> 8 != 0) {\n-\/\/         return 0;\n+\/\/       char c = src[srcOff];\n+\/\/       if (c > 0xff) {\n+\/\/           return i;  \/\/ return index of non-latin1 char\n@@ -8646,1 +8698,3 @@\n-\/\/       dst[dstOff++] = (byte)c;\n+\/\/       dst[dstOff] = (byte)c;\n+\/\/       srcOff++;\n+\/\/       dstOff++;\n@@ -8654,1 +8708,1 @@\n-  Label copy_chars_loop, return_length, return_zero, done;\n+  Label copy_chars_loop, done, reset_sp, copy_tail;\n@@ -8669,1 +8723,1 @@\n-  push(len);\n+  movl(result, len);\n@@ -8675,1 +8729,1 @@\n-    Label copy_32_loop, copy_loop_tail, below_threshold;\n+    Label copy_32_loop, copy_loop_tail, below_threshold, reset_for_copy_tail;\n@@ -8680,1 +8734,1 @@\n-    \/\/ if length of the string is less than 16, handle it in an old fashioned way\n+    \/\/ if length of the string is less than 32, handle it the old fashioned way\n@@ -8686,2 +8740,2 @@\n-    movl(result, 0x00FF);\n-    evpbroadcastw(tmp2Reg, result, Assembler::AVX_512bit);\n+    movl(tmp5, 0x00FF);\n+    evpbroadcastw(tmp2Reg, tmp5, Assembler::AVX_512bit);\n@@ -8690,1 +8744,1 @@\n-    jcc(Assembler::zero, post_alignment);\n+    jccb(Assembler::zero, post_alignment);\n@@ -8699,1 +8753,1 @@\n-    jcc(Assembler::zero, post_alignment);\n+    jccb(Assembler::zero, post_alignment);\n@@ -8702,4 +8756,5 @@\n-    movl(result, 0xFFFFFFFF);\n-    shlxl(result, result, tmp5);\n-    notl(result);\n-    kmovdl(mask2, result);\n+    movl(len, 0xFFFFFFFF);\n+    shlxl(len, len, tmp5);\n+    notl(len);\n+    kmovdl(mask2, len);\n+    movl(len, result);\n@@ -8710,1 +8765,1 @@\n-    jcc(Assembler::carryClear, return_zero);\n+    jcc(Assembler::carryClear, copy_tail);\n@@ -8725,1 +8780,1 @@\n-    jcc(Assembler::zero, copy_loop_tail);\n+    jccb(Assembler::zero, copy_loop_tail);\n@@ -8735,1 +8790,1 @@\n-    jcc(Assembler::carryClear, return_zero);\n+    jccb(Assembler::carryClear, reset_for_copy_tail);\n@@ -8741,1 +8796,1 @@\n-    jcc(Assembler::notZero, copy_32_loop);\n+    jccb(Assembler::notZero, copy_32_loop);\n@@ -8746,1 +8801,1 @@\n-    jcc(Assembler::zero, return_length);\n+    jcc(Assembler::zero, done);\n@@ -8751,3 +8806,3 @@\n-    movl(result, 0xFFFFFFFF);\n-    shlxl(result, result, len);\n-    notl(result);\n+    movl(tmp5, 0xFFFFFFFF);\n+    shlxl(tmp5, tmp5, len);\n+    notl(tmp5);\n@@ -8755,1 +8810,1 @@\n-    kmovdl(mask2, result);\n+    kmovdl(mask2, tmp5);\n@@ -8760,1 +8815,1 @@\n-    jcc(Assembler::carryClear, return_zero);\n+    jcc(Assembler::carryClear, copy_tail);\n@@ -8763,1 +8818,7 @@\n-    jmp(return_length);\n+    jmp(done);\n+\n+    bind(reset_for_copy_tail);\n+    lea(src, Address(src, tmp5, Address::times_2));\n+    lea(dst, Address(dst, tmp5, Address::times_1));\n+    subptr(len, tmp5);\n+    jmp(copy_chars_loop);\n@@ -8769,1 +8830,1 @@\n-    Label copy_32_loop, copy_16, copy_tail;\n+    Label copy_32_loop, copy_16, copy_tail_sse, reset_for_copy_tail;\n@@ -8771,1 +8832,3 @@\n-    movl(result, len);\n+    \/\/ vectored compression\n+    testl(len, 0xfffffff8);\n+    jcc(Assembler::zero, copy_tail);\n@@ -8774,0 +8837,2 @@\n+    movdl(tmp1Reg, tmp5);\n+    pshufd(tmp1Reg, tmp1Reg, 0);   \/\/ store Unicode mask in tmp1Reg\n@@ -8775,5 +8840,2 @@\n-    \/\/ vectored compression\n-    andl(len, 0xfffffff0);    \/\/ vector count (in chars)\n-    andl(result, 0x0000000f);    \/\/ tail count (in chars)\n-    testl(len, len);\n-    jcc(Assembler::zero, copy_16);\n+    andl(len, 0xfffffff0);\n+    jccb(Assembler::zero, copy_16);\n@@ -8782,2 +8844,0 @@\n-    movdl(tmp1Reg, tmp5);\n-    pshufd(tmp1Reg, tmp1Reg, 0);   \/\/ store Unicode mask in tmp1Reg\n@@ -8796,1 +8856,1 @@\n-    jcc(Assembler::notZero, return_zero);\n+    jccb(Assembler::notZero, reset_for_copy_tail);\n@@ -8800,1 +8860,1 @@\n-    jcc(Assembler::notZero, copy_32_loop);\n+    jccb(Assembler::notZero, copy_32_loop);\n@@ -8804,5 +8864,3 @@\n-    movl(len, result);\n-    andl(len, 0xfffffff8);    \/\/ vector count (in chars)\n-    andl(result, 0x00000007);    \/\/ tail count (in chars)\n-    testl(len, len);\n-    jccb(Assembler::zero, copy_tail);\n+    \/\/ len = 0\n+    testl(result, 0x00000008);     \/\/ check if there's a block of 8 chars to compress\n+    jccb(Assembler::zero, copy_tail_sse);\n@@ -8810,2 +8868,0 @@\n-    movdl(tmp1Reg, tmp5);\n-    pshufd(tmp1Reg, tmp1Reg, 0);   \/\/ store Unicode mask in tmp1Reg\n@@ -8816,1 +8872,1 @@\n-    jccb(Assembler::notZero, return_zero);\n+    jccb(Assembler::notZero, reset_for_copy_tail);\n@@ -8821,0 +8877,1 @@\n+    jmpb(copy_tail_sse);\n@@ -8822,1 +8879,9 @@\n-    bind(copy_tail);\n+    bind(reset_for_copy_tail);\n+    movl(tmp5, result);\n+    andl(tmp5, 0x0000000f);\n+    lea(src, Address(src, tmp5, Address::times_2));\n+    lea(dst, Address(dst, tmp5, Address::times_1));\n+    subptr(len, tmp5);\n+    jmpb(copy_chars_loop);\n+\n+    bind(copy_tail_sse);\n@@ -8824,0 +8889,1 @@\n+    andl(len, 0x00000007);    \/\/ tail count (in chars)\n@@ -8826,0 +8892,1 @@\n+  bind(copy_tail);\n@@ -8827,1 +8894,1 @@\n-  jccb(Assembler::zero, return_length);\n+  jccb(Assembler::zero, done);\n@@ -8833,4 +8900,4 @@\n-  load_unsigned_short(result, Address(src, len, Address::times_2));\n-  testl(result, 0xff00);      \/\/ check if Unicode char\n-  jccb(Assembler::notZero, return_zero);\n-  movb(Address(dst, len, Address::times_1), result);  \/\/ ASCII char; compress to 1 byte\n+  load_unsigned_short(tmp5, Address(src, len, Address::times_2));\n+  testl(tmp5, 0xff00);      \/\/ check if Unicode char\n+  jccb(Assembler::notZero, reset_sp);\n+  movb(Address(dst, len, Address::times_1), tmp5);  \/\/ ASCII char; compress to 1 byte\n@@ -8838,6 +8905,1 @@\n-  jcc(Assembler::notZero, copy_chars_loop);\n-\n-  \/\/ if compression succeeded, return length\n-  bind(return_length);\n-  pop(result);\n-  jmpb(done);\n+  jccb(Assembler::notZero, copy_chars_loop);\n@@ -8845,4 +8907,3 @@\n-  \/\/ if compression failed, return 0\n-  bind(return_zero);\n-  xorl(result, result);\n-  addptr(rsp, wordSize);\n+  \/\/ add len then return (len will be zero if compress succeeded, otherwise negative)\n+  bind(reset_sp);\n+  addl(result, len);\n","filename":"src\/hotspot\/cpu\/x86\/macroAssembler_x86.cpp","additions":125,"deletions":64,"binary":false,"changes":189,"status":"modified"},{"patch":"@@ -908,0 +908,1 @@\n+  void testptr(Address src, int32_t imm32) {  LP64_ONLY(testq(src, imm32)) NOT_LP64(testl(src, imm32)); }\n@@ -1143,0 +1144,4 @@\n+  \/\/ Vector float blend\n+  void vblendvps(XMMRegister dst, XMMRegister nds, XMMRegister src, XMMRegister mask, int vector_len, bool compute_mask = true, XMMRegister scratch = xnoreg);\n+  void vblendvpd(XMMRegister dst, XMMRegister nds, XMMRegister src, XMMRegister mask, int vector_len, bool compute_mask = true, XMMRegister scratch = xnoreg);\n+\n","filename":"src\/hotspot\/cpu\/x86\/macroAssembler_x86.hpp","additions":5,"deletions":0,"binary":false,"changes":5,"status":"modified"},{"patch":"@@ -4497,8 +4497,0 @@\n-\n-\/\/ Following pseudo code describes the algorithm for max[FD]:\n-\/\/ Min algorithm is on similar lines\n-\/\/  btmp = (b < +0.0) ? a : b\n-\/\/  atmp = (b < +0.0) ? b : a\n-\/\/  Tmp  = Max_Float(atmp , btmp)\n-\/\/  Res  = (atmp == NaN) ? atmp : Tmp\n-\n@@ -4510,15 +4502,4 @@\n-  format %{\n-     \"vblendvps        $btmp,$b,$a,$b           \\n\\t\"\n-     \"vblendvps        $atmp,$a,$b,$b           \\n\\t\"\n-     \"vmaxss           $tmp,$atmp,$btmp         \\n\\t\"\n-     \"vcmpps.unordered $btmp,$atmp,$atmp        \\n\\t\"\n-     \"vblendvps        $dst,$tmp,$atmp,$btmp    \\n\\t\"\n-  %}\n-  ins_encode %{\n-    int vector_len = Assembler::AVX_128bit;\n-    __ vblendvps($btmp$$XMMRegister, $b$$XMMRegister, $a$$XMMRegister, $b$$XMMRegister, vector_len);\n-    __ vblendvps($atmp$$XMMRegister, $a$$XMMRegister, $b$$XMMRegister, $b$$XMMRegister, vector_len);\n-    __ vmaxss($tmp$$XMMRegister, $atmp$$XMMRegister, $btmp$$XMMRegister);\n-    __ vcmpps($btmp$$XMMRegister, $atmp$$XMMRegister, $atmp$$XMMRegister, Assembler::_false, vector_len);\n-    __ vblendvps($dst$$XMMRegister, $tmp$$XMMRegister, $atmp$$XMMRegister, $btmp$$XMMRegister, vector_len);\n- %}\n+  format %{ \"maxF $dst, $a, $b \\t! using tmp, atmp and btmp as TEMP\" %}\n+  ins_encode %{\n+    __ vminmax_fp(Op_MaxV, T_FLOAT, $dst$$XMMRegister, $a$$XMMRegister, $b$$XMMRegister, $tmp$$XMMRegister, $atmp$$XMMRegister, $btmp$$XMMRegister, Assembler::AVX_128bit);\n+  %}\n@@ -4546,7 +4527,1 @@\n-  format %{\n-     \"vblendvpd        $btmp,$b,$a,$b            \\n\\t\"\n-     \"vblendvpd        $atmp,$a,$b,$b            \\n\\t\"\n-     \"vmaxsd           $tmp,$atmp,$btmp          \\n\\t\"\n-     \"vcmppd.unordered $btmp,$atmp,$atmp         \\n\\t\"\n-     \"vblendvpd        $dst,$tmp,$atmp,$btmp     \\n\\t\"\n-  %}\n+  format %{ \"maxD $dst, $a, $b \\t! using tmp, atmp and btmp as TEMP\" %}\n@@ -4554,6 +4529,1 @@\n-    int vector_len = Assembler::AVX_128bit;\n-    __ vblendvpd($btmp$$XMMRegister, $b$$XMMRegister, $a$$XMMRegister, $b$$XMMRegister, vector_len);\n-    __ vblendvpd($atmp$$XMMRegister, $a$$XMMRegister, $b$$XMMRegister, $b$$XMMRegister, vector_len);\n-    __ vmaxsd($tmp$$XMMRegister, $atmp$$XMMRegister, $btmp$$XMMRegister);\n-    __ vcmppd($btmp$$XMMRegister, $atmp$$XMMRegister, $atmp$$XMMRegister, Assembler::_false, vector_len);\n-    __ vblendvpd($dst$$XMMRegister, $tmp$$XMMRegister, $atmp$$XMMRegister, $btmp$$XMMRegister, vector_len);\n+    __ vminmax_fp(Op_MaxV, T_DOUBLE, $dst$$XMMRegister, $a$$XMMRegister, $b$$XMMRegister, $tmp$$XMMRegister, $atmp$$XMMRegister, $btmp$$XMMRegister, Assembler::AVX_128bit);\n@@ -4582,7 +4552,1 @@\n-  format %{\n-     \"vblendvps        $atmp,$a,$b,$a             \\n\\t\"\n-     \"vblendvps        $btmp,$b,$a,$a             \\n\\t\"\n-     \"vminss           $tmp,$atmp,$btmp           \\n\\t\"\n-     \"vcmpps.unordered $btmp,$atmp,$atmp          \\n\\t\"\n-     \"vblendvps        $dst,$tmp,$atmp,$btmp      \\n\\t\"\n-  %}\n+  format %{ \"minF $dst, $a, $b \\t! using tmp, atmp and btmp as TEMP\" %}\n@@ -4590,6 +4554,1 @@\n-    int vector_len = Assembler::AVX_128bit;\n-    __ vblendvps($atmp$$XMMRegister, $a$$XMMRegister, $b$$XMMRegister, $a$$XMMRegister, vector_len);\n-    __ vblendvps($btmp$$XMMRegister, $b$$XMMRegister, $a$$XMMRegister, $a$$XMMRegister, vector_len);\n-    __ vminss($tmp$$XMMRegister, $atmp$$XMMRegister, $btmp$$XMMRegister);\n-    __ vcmpps($btmp$$XMMRegister, $atmp$$XMMRegister, $atmp$$XMMRegister, Assembler::_false, vector_len);\n-    __ vblendvps($dst$$XMMRegister, $tmp$$XMMRegister, $atmp$$XMMRegister, $btmp$$XMMRegister, vector_len);\n+    __ vminmax_fp(Op_MinV, T_FLOAT, $dst$$XMMRegister, $a$$XMMRegister, $b$$XMMRegister, $tmp$$XMMRegister, $atmp$$XMMRegister, $btmp$$XMMRegister, Assembler::AVX_128bit);\n@@ -4618,7 +4577,1 @@\n-  format %{\n-     \"vblendvpd        $atmp,$a,$b,$a           \\n\\t\"\n-     \"vblendvpd        $btmp,$b,$a,$a           \\n\\t\"\n-     \"vminsd           $tmp,$atmp,$btmp         \\n\\t\"\n-     \"vcmppd.unordered $btmp,$atmp,$atmp        \\n\\t\"\n-     \"vblendvpd        $dst,$tmp,$atmp,$btmp    \\n\\t\"\n-  %}\n+    format %{ \"minD $dst, $a, $b \\t! using tmp, atmp and btmp as TEMP\" %}\n@@ -4626,6 +4579,1 @@\n-    int vector_len = Assembler::AVX_128bit;\n-    __ vblendvpd($atmp$$XMMRegister, $a$$XMMRegister, $b$$XMMRegister, $a$$XMMRegister, vector_len);\n-    __ vblendvpd($btmp$$XMMRegister, $b$$XMMRegister, $a$$XMMRegister, $a$$XMMRegister, vector_len);\n-    __ vminsd($tmp$$XMMRegister, $atmp$$XMMRegister, $btmp$$XMMRegister);\n-    __ vcmppd($btmp$$XMMRegister, $atmp$$XMMRegister, $atmp$$XMMRegister, Assembler::_false, vector_len);\n-    __ vblendvpd($dst$$XMMRegister, $tmp$$XMMRegister, $atmp$$XMMRegister, $btmp$$XMMRegister, vector_len);\n+    __ vminmax_fp(Op_MinV, T_DOUBLE, $dst$$XMMRegister, $a$$XMMRegister, $b$$XMMRegister, $tmp$$XMMRegister, $atmp$$XMMRegister, $btmp$$XMMRegister, Assembler::AVX_128bit);\n@@ -7193,1 +7141,1 @@\n-  predicate(VM_Version::supports_cx8() && n->as_LoadStore()->barrier_data() == 0);\n+  predicate(n->as_LoadStore()->barrier_data() == 0);\n@@ -7216,1 +7164,0 @@\n-  predicate(VM_Version::supports_cx8());\n@@ -7377,1 +7324,0 @@\n-  predicate(VM_Version::supports_cx8());\n@@ -7411,1 +7357,1 @@\n-  predicate(VM_Version::supports_cx8() && n->as_LoadStore()->barrier_data() == 0);\n+  predicate(n->as_LoadStore()->barrier_data() == 0);\n","filename":"src\/hotspot\/cpu\/x86\/x86_64.ad","additions":12,"deletions":66,"binary":false,"changes":78,"status":"modified"},{"patch":"@@ -406,2 +406,14 @@\n-    if (s->kind() == ValueStack::EmptyExceptionState) {\n-      assert(s->stack_size() == 0 && s->locals_size() == 0 && (s->locks_size() == 0 || s->locks_size() == 1), \"state must be empty\");\n+    if (s->kind() == ValueStack::EmptyExceptionState ||\n+        s->kind() == ValueStack::CallerEmptyExceptionState)\n+    {\n+#ifdef ASSERT\n+      int index;\n+      Value value;\n+      for_each_stack_value(s, index, value) {\n+        fatal(\"state must be empty\");\n+      }\n+      for_each_local_value(s, index, value) {\n+        fatal(\"state must be empty\");\n+      }\n+#endif\n+      assert(s->locks_size() == 0 || s->locks_size() == 1, \"state must be empty\");\n","filename":"src\/hotspot\/share\/c1\/c1_LIRGenerator.cpp","additions":14,"deletions":2,"binary":false,"changes":16,"status":"modified"},{"patch":"@@ -165,2 +165,2 @@\n-    const char* default_base_archive_name = Arguments::get_default_shared_archive_path();\n-    const char* current_base_archive_name = Arguments::GetSharedArchivePath();\n+    const char* default_base_archive_name = CDSConfig::default_archive_path();\n+    const char* current_base_archive_name = CDSConfig::static_archive_path();\n@@ -202,1 +202,1 @@\n-    copy_base_archive_name(Arguments::GetSharedArchivePath());\n+    copy_base_archive_name(CDSConfig::static_archive_path());\n@@ -402,15 +402,7 @@\n-  } else if ((has_timestamp() && _timestamp != st.st_mtime) ||\n-             _filesize != st.st_size) {\n-    ok = false;\n-    if (PrintSharedArchiveAndExit) {\n-      log_warning(cds)(_timestamp != st.st_mtime ?\n-                                 \"Timestamp mismatch\" :\n-                                 \"File size mismatch\");\n-    } else {\n-      const char* bad_jar_msg = \"A jar file is not the one used while building the shared archive file:\";\n-      log_warning(cds)(\"%s %s\", bad_jar_msg, name);\n-      if (!log_is_enabled(Info, cds)) {\n-        log_warning(cds)(\"%s %s\", bad_jar_msg, name);\n-      }\n-      if (_timestamp != st.st_mtime) {\n-        log_warning(cds)(\"%s timestamp has changed.\", name);\n+  } else {\n+    bool size_differs = _filesize != st.st_size;\n+    bool time_differs = has_timestamp() && _timestamp != st.st_mtime;\n+    if (time_differs || size_differs) {\n+      ok = false;\n+      if (PrintSharedArchiveAndExit) {\n+        log_warning(cds)(time_differs ? \"Timestamp mismatch\" : \"File size mismatch\");\n@@ -418,1 +410,11 @@\n-        log_warning(cds)(\"%s size has changed.\", name);\n+        const char* bad_file_msg = \"This file is not the one used while building the shared archive file:\";\n+        log_warning(cds)(\"%s %s\", bad_file_msg, name);\n+        if (!log_is_enabled(Info, cds)) {\n+          log_warning(cds)(\"%s %s\", bad_file_msg, name);\n+        }\n+        if (time_differs) {\n+          log_warning(cds)(\"%s timestamp has changed.\", name);\n+        }\n+        if (size_differs) {\n+          log_warning(cds)(\"%s size has changed.\", name);\n+        }\n@@ -921,1 +923,1 @@\n-  int num_paths = Arguments::num_archives(rp);\n+  int num_paths = CDSConfig::num_archives(rp);\n@@ -1253,1 +1255,1 @@\n-    *base_archive_name = Arguments::get_default_shared_archive_path();\n+    *base_archive_name = CDSConfig::default_archive_path();\n@@ -2278,1 +2280,1 @@\n-        ArchiveClassesAtExit = Arguments::GetSharedDynamicArchivePath();\n+        ArchiveClassesAtExit = CDSConfig::dynamic_archive_path();\n","filename":"src\/hotspot\/share\/cds\/filemap.cpp","additions":24,"deletions":22,"binary":false,"changes":46,"status":"modified"},{"patch":"@@ -144,5 +144,10 @@\n-#ifdef _LP64\n-  return CompressedKlassPointers::is_valid_base((address)shared_base);\n-#else\n-  return true;\n-#endif\n+  \/\/ We check user input for SharedBaseAddress at dump time. We must weed out values\n+  \/\/ we already know to be invalid later.\n+\n+  \/\/ At CDS runtime, \"shared_base\" will be the (attempted) mapping start. It will also\n+  \/\/ be the encoding base, since the the headers of archived base objects (and with Lilliput,\n+  \/\/ the prototype mark words) carry pre-computed narrow Klass IDs that refer to the mapping\n+  \/\/ start as base.\n+  \/\/\n+  \/\/ Therefore, \"shared_base\" must be later usable as encoding base.\n+  return AARCH64_ONLY(is_aligned(shared_base, 4 * G)) NOT_AARCH64(true);\n@@ -537,1 +542,1 @@\n-  const char* static_archive = Arguments::GetSharedArchivePath();\n+  const char* static_archive = CDSConfig::static_archive_path();\n@@ -649,2 +654,1 @@\n-  Arguments::check_unsupported_dumping_properties();\n-\n+  CDSConfig::check_unsupported_dumping_properties();\n@@ -781,1 +785,0 @@\n-    StringTable::allocate_shared_strings_array(CHECK);\n@@ -791,0 +794,4 @@\n+\n+    \/\/ Do this at the very end, when no Java code will be executed. Otherwise\n+    \/\/ some new strings may be added to the intern table.\n+    StringTable::allocate_shared_strings_array(CHECK);\n@@ -982,2 +989,2 @@\n-  const char* static_archive = Arguments::GetSharedArchivePath();\n-  assert(static_archive != nullptr, \"SharedArchivePath is nullptr\");\n+  const char* static_archive = CDSConfig::static_archive_path();\n+  assert(static_archive != nullptr, \"sanity\");\n@@ -996,1 +1003,1 @@\n-  const char* dynamic_archive = Arguments::GetSharedDynamicArchivePath();\n+  const char* dynamic_archive = CDSConfig::dynamic_archive_path();\n@@ -1260,6 +1267,0 @@\n-#ifdef _LP64\n-    if (Metaspace::using_class_space()) {\n-      assert(CompressedKlassPointers::is_valid_base(base_address),\n-             \"Archive base address invalid: \" PTR_FORMAT \".\", p2i(base_address));\n-    }\n-#endif\n@@ -1335,2 +1336,2 @@\n-      \/\/ case we reserve whereever possible, but the start address needs to be encodable as narrow Klass\n-      \/\/ encoding base since the archived heap objects contain nKlass IDs precalculated toward the start\n+      \/\/ case we reserve wherever possible, but the start address needs to be encodable as narrow Klass\n+      \/\/ encoding base since the archived heap objects contain nKlass IDs pre-calculated toward the start\n@@ -1339,1 +1340,1 @@\n-      total_space_rs = Metaspace::reserve_address_space_for_compressed_classes(total_range_size, false \/* try_in_low_address_ranges *\/);\n+      total_space_rs = Metaspace::reserve_address_space_for_compressed_classes(total_range_size, false \/* optimize_for_zero_base *\/);\n@@ -1351,1 +1352,0 @@\n-    assert(CompressedKlassPointers::is_valid_base((address)total_space_rs.base()), \"Sanity\");\n@@ -1496,1 +1496,1 @@\n-      tty->print_cr(\"\\n\\nBase archive name: %s\", Arguments::GetSharedArchivePath());\n+      tty->print_cr(\"\\n\\nBase archive name: %s\", CDSConfig::static_archive_path());\n","filename":"src\/hotspot\/share\/cds\/metaspaceShared.cpp","additions":23,"deletions":23,"binary":false,"changes":46,"status":"modified"},{"patch":"@@ -28,1 +28,1 @@\n-#include \"classfile\/stringTable.hpp\"\n+#include \"classfile\/systemDictionary.hpp\"\n@@ -73,1 +73,1 @@\n-#include \"gc\/g1\/g1YoungGCEvacFailureInjector.hpp\"\n+#include \"gc\/g1\/g1YoungGCAllocationFailureInjector.hpp\"\n@@ -77,0 +77,1 @@\n+#include \"gc\/shared\/classUnloadingContext.hpp\"\n@@ -81,1 +82,0 @@\n-#include \"gc\/shared\/gcLocker.inline.hpp\"\n@@ -84,1 +84,0 @@\n-#include \"gc\/shared\/generationSpec.hpp\"\n@@ -108,0 +107,1 @@\n+#include \"runtime\/cpuTimeCounters.hpp\"\n@@ -416,4 +416,3 @@\n-  \/\/ We will loop until a) we manage to successfully perform the\n-  \/\/ allocation or b) we successfully schedule a collection which\n-  \/\/ fails to perform the allocation. b) is the only case when we'll\n-  \/\/ return null.\n+  \/\/ We will loop until a) we manage to successfully perform the allocation or b)\n+  \/\/ successfully schedule a collection which fails to perform the allocation.\n+  \/\/ Case b) is the only case when we'll return null.\n@@ -421,2 +420,1 @@\n-  for (uint try_count = 1, gclocker_retry_count = 0; \/* we'll return *\/; try_count += 1) {\n-    bool should_try_gc;\n+  for (uint try_count = 1; \/* we'll return *\/; try_count++) {\n@@ -435,16 +433,0 @@\n-      \/\/ If the GCLocker is active and we are bound for a GC, try expanding young gen.\n-      \/\/ This is different to when only GCLocker::needs_gc() is set: try to avoid\n-      \/\/ waiting because the GCLocker is active to not wait too long.\n-      if (GCLocker::is_active_and_needs_gc() && policy()->can_expand_young_list()) {\n-        \/\/ No need for an ergo message here, can_expand_young_list() does this when\n-        \/\/ it returns true.\n-        result = _allocator->attempt_allocation_force(word_size);\n-        if (result != nullptr) {\n-          return result;\n-        }\n-      }\n-\n-      \/\/ Only try a GC if the GCLocker does not signal the need for a GC. Wait until\n-      \/\/ the GCLocker initiated GC has been performed and then retry. This includes\n-      \/\/ the case when the GC Locker is not active but has not been performed.\n-      should_try_gc = !GCLocker::needs_gc();\n@@ -455,32 +437,6 @@\n-    if (should_try_gc) {\n-      bool succeeded;\n-      result = do_collection_pause(word_size, gc_count_before, &succeeded, GCCause::_g1_inc_collection_pause);\n-      if (result != nullptr) {\n-        assert(succeeded, \"only way to get back a non-null result\");\n-        log_trace(gc, alloc)(\"%s: Successfully scheduled collection returning \" PTR_FORMAT,\n-                             Thread::current()->name(), p2i(result));\n-        return result;\n-      }\n-\n-      if (succeeded) {\n-        \/\/ We successfully scheduled a collection which failed to allocate. No\n-        \/\/ point in trying to allocate further. We'll just return null.\n-        log_trace(gc, alloc)(\"%s: Successfully scheduled collection failing to allocate \"\n-                             SIZE_FORMAT \" words\", Thread::current()->name(), word_size);\n-        return nullptr;\n-      }\n-      log_trace(gc, alloc)(\"%s: Unsuccessfully scheduled collection allocating \" SIZE_FORMAT \" words\",\n-                           Thread::current()->name(), word_size);\n-    } else {\n-      \/\/ Failed to schedule a collection.\n-      if (gclocker_retry_count > GCLockerRetryAllocationCount) {\n-        log_warning(gc, alloc)(\"%s: Retried waiting for GCLocker too often allocating \"\n-                               SIZE_FORMAT \" words\", Thread::current()->name(), word_size);\n-        return nullptr;\n-      }\n-      log_trace(gc, alloc)(\"%s: Stall until clear\", Thread::current()->name());\n-      \/\/ The GCLocker is either active or the GCLocker initiated\n-      \/\/ GC has not yet been performed. Stall until it is and\n-      \/\/ then retry the allocation.\n-      GCLocker::stall_until_clear();\n-      gclocker_retry_count += 1;\n+    bool succeeded;\n+    result = do_collection_pause(word_size, gc_count_before, &succeeded, GCCause::_g1_inc_collection_pause);\n+    if (succeeded) {\n+      log_trace(gc, alloc)(\"%s: Successfully scheduled collection returning \" PTR_FORMAT,\n+                           Thread::current()->name(), p2i(result));\n+      return result;\n@@ -489,7 +445,8 @@\n-    \/\/ We can reach here if we were unsuccessful in scheduling a\n-    \/\/ collection (because another thread beat us to it) or if we were\n-    \/\/ stalled due to the GC locker. In either can we should retry the\n-    \/\/ allocation attempt in case another thread successfully\n-    \/\/ performed a collection and reclaimed enough space. We do the\n-    \/\/ first attempt (without holding the Heap_lock) here and the\n-    \/\/ follow-on attempt will be at the start of the next loop\n+    log_trace(gc, alloc)(\"%s: Unsuccessfully scheduled collection allocating \" SIZE_FORMAT \" words\",\n+                         Thread::current()->name(), word_size);\n+\n+    \/\/ We can reach here if we were unsuccessful in scheduling a collection (because\n+    \/\/ another thread beat us to it). In this case immeditealy retry the allocation\n+    \/\/ attempt because another thread successfully performed a collection and possibly\n+    \/\/ reclaimed enough space. The first attempt (without holding the Heap_lock) is\n+    \/\/ here and the follow-on attempt will be at the start of the next loop\n@@ -678,4 +635,3 @@\n-  \/\/ We will loop until a) we manage to successfully perform the\n-  \/\/ allocation or b) we successfully schedule a collection which\n-  \/\/ fails to perform the allocation. b) is the only case when we'll\n-  \/\/ return null.\n+  \/\/ We will loop until a) we manage to successfully perform the allocation or b)\n+  \/\/ successfully schedule a collection which fails to perform the allocation.\n+  \/\/ Case b) is the only case when we'll return null.\n@@ -683,2 +639,1 @@\n-  for (uint try_count = 1, gclocker_retry_count = 0; \/* we'll return *\/; try_count += 1) {\n-    bool should_try_gc;\n+  for (uint try_count = 1; \/* we'll return *\/; try_count++) {\n@@ -702,4 +657,0 @@\n-      \/\/ Only try a GC if the GCLocker does not signal the need for a GC. Wait until\n-      \/\/ the GCLocker initiated GC has been performed and then retry. This includes\n-      \/\/ the case when the GC Locker is not active but has not been performed.\n-      should_try_gc = !GCLocker::needs_gc();\n@@ -710,3 +661,5 @@\n-    if (should_try_gc) {\n-      bool succeeded;\n-      result = do_collection_pause(word_size, gc_count_before, &succeeded, GCCause::_g1_humongous_allocation);\n+    bool succeeded;\n+    result = do_collection_pause(word_size, gc_count_before, &succeeded, GCCause::_g1_humongous_allocation);\n+    if (succeeded) {\n+      log_trace(gc, alloc)(\"%s: Successfully scheduled collection returning \" PTR_FORMAT,\n+                           Thread::current()->name(), p2i(result));\n@@ -714,3 +667,0 @@\n-        assert(succeeded, \"only way to get back a non-null result\");\n-        log_trace(gc, alloc)(\"%s: Successfully scheduled collection returning \" PTR_FORMAT,\n-                             Thread::current()->name(), p2i(result));\n@@ -720,24 +670,1 @@\n-        return result;\n-      }\n-\n-      if (succeeded) {\n-        \/\/ We successfully scheduled a collection which failed to allocate. No\n-        \/\/ point in trying to allocate further. We'll just return null.\n-        log_trace(gc, alloc)(\"%s: Successfully scheduled collection failing to allocate \"\n-                             SIZE_FORMAT \" words\", Thread::current()->name(), word_size);\n-        return nullptr;\n-      log_trace(gc, alloc)(\"%s: Unsuccessfully scheduled collection allocating \" SIZE_FORMAT \"\",\n-                           Thread::current()->name(), word_size);\n-    } else {\n-      \/\/ Failed to schedule a collection.\n-      if (gclocker_retry_count > GCLockerRetryAllocationCount) {\n-        log_warning(gc, alloc)(\"%s: Retried waiting for GCLocker too often allocating \"\n-                               SIZE_FORMAT \" words\", Thread::current()->name(), word_size);\n-        return nullptr;\n-      }\n-      log_trace(gc, alloc)(\"%s: Stall until clear\", Thread::current()->name());\n-      \/\/ The GCLocker is either active or the GCLocker initiated\n-      \/\/ GC has not yet been performed. Stall until it is and\n-      \/\/ then retry the allocation.\n-      GCLocker::stall_until_clear();\n-      gclocker_retry_count += 1;\n+      return result;\n@@ -747,0 +674,2 @@\n+    log_trace(gc, alloc)(\"%s: Unsuccessfully scheduled collection allocating \" SIZE_FORMAT \"\",\n+                         Thread::current()->name(), word_size);\n@@ -748,5 +677,2 @@\n-    \/\/ We can reach here if we were unsuccessful in scheduling a\n-    \/\/ collection (because another thread beat us to it) or if we were\n-    \/\/ stalled due to the GC locker. In either can we should retry the\n-    \/\/ allocation attempt in case another thread successfully\n-    \/\/ performed a collection and reclaimed enough space.\n+    \/\/ We can reach here if we were unsuccessful in scheduling a collection (because\n+    \/\/ another thread beat us to it).\n@@ -759,1 +685,1 @@\n-      log_warning(gc, alloc)(\"%s: Retried allocation %u times for \" SIZE_FORMAT \" words\",\n+      log_warning(gc, alloc)(\"%s: Retried allocation %u times for %zu words\",\n@@ -852,4 +778,0 @@\n-  \/\/ Delete metaspaces for unloaded class loaders and clean up loader_data graph\n-  ClassLoaderDataGraph::purge(\/*at_safepoint*\/true);\n-  DEBUG_ONLY(MetaspaceUtils::verify();)\n-\n@@ -913,5 +835,0 @@\n-  if (GCLocker::check_active_before_gc()) {\n-    \/\/ Full GC was not completed.\n-    return false;\n-  }\n-\n@@ -1231,1 +1148,1 @@\n-  _evac_failure_injector(),\n+  _allocation_failure_injector(),\n@@ -1274,3 +1191,5 @@\n-  \/\/ Override the default _filler_array_max_size so that no humongous filler\n-  \/\/ objects are created.\n-  _filler_array_max_size = _humongous_object_threshold_in_words;\n+  \/\/ Since filler arrays are never referenced, we can make them region sized.\n+  \/\/ This simplifies filling up the region in case we have some potentially\n+  \/\/ unreferenced (by Java code, but still in use by native code) pinned objects\n+  \/\/ in there.\n+  _filler_array_max_size = HeapRegion::GrainWords;\n@@ -1517,1 +1436,6 @@\n-  evac_failure_injector()->reset();\n+  allocation_failure_injector()->reset();\n+\n+  CPUTimeCounters::create_counter(CPUTimeGroups::CPUTimeType::gc_parallel_workers);\n+  CPUTimeCounters::create_counter(CPUTimeGroups::CPUTimeType::gc_conc_mark);\n+  CPUTimeCounters::create_counter(CPUTimeGroups::CPUTimeType::gc_conc_refine);\n+  CPUTimeCounters::create_counter(CPUTimeGroups::CPUTimeType::gc_service);\n@@ -1911,6 +1835,0 @@\n-    if (GCLocker::is_active_and_needs_gc()) {\n-      \/\/ If GCLocker is active, wait until clear before retrying.\n-      LOG_COLLECT_CONCURRENTLY(cause, \"gc-locker stall\");\n-      GCLocker::stall_until_clear();\n-    }\n-\n@@ -1942,5 +1860,0 @@\n-\n-    if (GCLocker::is_active_and_needs_gc()) {\n-      \/\/ If GCLocker is active, wait until clear before retrying.\n-      GCLocker::stall_until_clear();\n-    }\n@@ -1956,5 +1869,0 @@\n-  } else if (GCLocker::should_discard(cause, counters_before.total_collections())) {\n-    \/\/ Indicate failure to be consistent with VMOp failure due to\n-    \/\/ another collection slipping in after our gc_count but before\n-    \/\/ our request is processed.\n-    return false;\n@@ -2186,8 +2094,0 @@\n-void G1CollectedHeap::pin_object(JavaThread* thread, oop obj) {\n-  GCLocker::lock_critical(thread);\n-}\n-\n-void G1CollectedHeap::unpin_object(JavaThread* thread, oop obj) {\n-  GCLocker::unlock_critical(thread);\n-}\n-\n@@ -2336,0 +2236,2 @@\n+\n+  update_parallel_gc_threads_cpu_time();\n@@ -2420,0 +2322,20 @@\n+void G1CollectedHeap::update_parallel_gc_threads_cpu_time() {\n+  assert(Thread::current()->is_VM_thread(),\n+         \"Must be called from VM thread to avoid races\");\n+  if (!UsePerfData || !os::is_thread_cpu_time_supported()) {\n+    return;\n+  }\n+\n+  \/\/ Ensure ThreadTotalCPUTimeClosure destructor is called before publishing gc\n+  \/\/ time.\n+  {\n+    ThreadTotalCPUTimeClosure tttc(CPUTimeGroups::CPUTimeType::gc_parallel_workers);\n+    \/\/ Currently parallel worker threads never terminate (JDK-8081682), so it is\n+    \/\/ safe for VMThread to read their CPU times. However, if JDK-8087340 is\n+    \/\/ resolved so they terminate, we should rethink if it is still safe.\n+    workers()->threads_do(&tttc);\n+  }\n+\n+  CPUTimeCounters::publish_gc_total_cpu_time();\n+}\n+\n@@ -2496,4 +2418,0 @@\n-  if (GCLocker::check_active_before_gc()) {\n-    return false;\n-  }\n-\n@@ -2601,0 +2519,27 @@\n+void G1CollectedHeap::unload_classes_and_code(const char* description, BoolObjectClosure* is_alive, GCTimer* timer) {\n+  GCTraceTime(Debug, gc, phases) debug(description, timer);\n+\n+  ClassUnloadingContext ctx(workers()->active_workers(),\n+                            false \/* lock_codeblob_free_separately *\/);\n+  {\n+    CodeCache::UnlinkingScope scope(is_alive);\n+    bool unloading_occurred = SystemDictionary::do_unloading(timer);\n+    GCTraceTime(Debug, gc, phases) t(\"G1 Complete Cleaning\", timer);\n+    complete_cleaning(unloading_occurred);\n+  }\n+  {\n+    GCTraceTime(Debug, gc, phases) t(\"Purge Unlinked NMethods\", timer);\n+    ctx.purge_nmethods();\n+  }\n+  {\n+    GCTraceTime(Debug, gc, phases) t(\"Free Code Blobs\", timer);\n+    ctx.free_code_blobs();\n+  }\n+  {\n+    GCTraceTime(Debug, gc, phases) t(\"Purge Class Loader Data\", timer);\n+    ClassLoaderDataGraph::purge(true \/* at_safepoint *\/);\n+    DEBUG_ONLY(MetaspaceUtils::verify();)\n+  }\n+}\n+\n+\n@@ -2654,0 +2599,2 @@\n+  assert(!hr->has_pinned_objects(),\n+         \"must not free a region which contains pinned objects\");\n@@ -2771,1 +2718,1 @@\n-   if (hr->is_humongous()) {\n+  if (hr->is_humongous()) {\n@@ -3061,3 +3008,0 @@\n-    \/\/ Reset the G1EvacuationFailureALot counters and flags\n-    evac_failure_injector()->reset();\n-\n","filename":"src\/hotspot\/share\/gc\/g1\/g1CollectedHeap.cpp","additions":101,"deletions":157,"binary":false,"changes":258,"status":"modified"},{"patch":"@@ -27,3 +27,0 @@\n-#include \"classfile\/systemDictionary.hpp\"\n-#include \"code\/codeCache.hpp\"\n-#include \"compiler\/oopMap.hpp\"\n@@ -44,0 +41,1 @@\n+#include \"gc\/shared\/classUnloadingContext.hpp\"\n@@ -175,0 +173,1 @@\n+    hr->prepare_for_full_gc();\n@@ -264,3 +263,3 @@\n-  } else if (hr->is_humongous()) {\n-    \/\/ Humongous objects will never be moved in the \"main\" compaction phase, but\n-    \/\/ afterwards in a special phase if needed.\n+  } else if (hr->is_humongous() || hr->has_pinned_objects()) {\n+    \/\/ Humongous objects or pinned regions will never be moved in the \"main\"\n+    \/\/ compaction phase, but non-pinned regions might afterwards in a special phase.\n@@ -326,8 +325,1 @@\n-    GCTraceTime(Debug, gc, phases) debug(\"Phase 1: Class Unloading and Cleanup\", scope()->timer());\n-    {\n-      CodeCache::UnlinkingScope unloading_scope(&_is_alive);\n-      \/\/ Unload classes and purge the SystemDictionary.\n-      bool unloading_occurred = SystemDictionary::do_unloading(scope()->timer());\n-      _heap->complete_cleaning(unloading_occurred);\n-    }\n-    CodeCache::flush_unlinked_nmethods();\n+    _heap->unload_classes_and_code(\"Phase 1: Class Unloading and Cleanup\", &_is_alive, scope()->timer());\n@@ -471,2 +463,7 @@\n-      uint num_regions = humongous_cp->forward_humongous<ALT_FWD>(hr);\n-      region_index += num_regions; \/\/ Skip over the continues humongous regions.\n+      size_t obj_size = cast_to_oop(hr->bottom())->size();\n+      uint num_regions = (uint)G1CollectedHeap::humongous_obj_size_in_regions(obj_size);\n+      \/\/ Even during last-ditch compaction we should not move pinned humongous objects.\n+      if (!hr->has_pinned_objects()) {\n+        humongous_cp->forward_humongous<ALT_FWD>(hr);\n+      }\n+      region_index += num_regions; \/\/ Advance over all humongous regions.\n@@ -475,0 +472,1 @@\n+      assert(!hr->has_pinned_objects(), \"pinned regions should not be compaction targets\");\n","filename":"src\/hotspot\/share\/gc\/g1\/g1FullCollector.cpp","additions":14,"deletions":16,"binary":false,"changes":30,"status":"modified"},{"patch":"@@ -74,0 +74,1 @@\n+  assert(!hr->has_pinned_objects(), \"Should be no region with pinned objects in compaction queue\");\n","filename":"src\/hotspot\/share\/gc\/g1\/g1FullGCCompactTask.cpp","additions":1,"deletions":0,"binary":false,"changes":1,"status":"modified"},{"patch":"@@ -154,1 +154,1 @@\n-uint G1FullGCCompactionPoint::forward_humongous(HeapRegion* hr) {\n+void G1FullGCCompactionPoint::forward_humongous(HeapRegion* hr) {\n@@ -162,1 +162,1 @@\n-    return num_regions;\n+    return;\n@@ -170,1 +170,1 @@\n-    return num_regions;\n+    return;\n@@ -186,1 +186,1 @@\n-  return num_regions;\n+  return;\n@@ -189,2 +189,2 @@\n-template uint G1FullGCCompactionPoint::forward_humongous<true>(HeapRegion* hr);\n-template uint G1FullGCCompactionPoint::forward_humongous<false>(HeapRegion* hr);\n+template void G1FullGCCompactionPoint::forward_humongous<true>(HeapRegion* hr);\n+template void G1FullGCCompactionPoint::forward_humongous<false>(HeapRegion* hr);\n","filename":"src\/hotspot\/share\/gc\/g1\/g1FullGCCompactionPoint.cpp","additions":6,"deletions":6,"binary":false,"changes":12,"status":"modified"},{"patch":"@@ -60,1 +60,1 @@\n-  uint forward_humongous(HeapRegion* hr);\n+  void forward_humongous(HeapRegion* hr);\n","filename":"src\/hotspot\/share\/gc\/g1\/g1FullGCCompactionPoint.hpp","additions":1,"deletions":1,"binary":false,"changes":2,"status":"modified"},{"patch":"@@ -46,1 +46,1 @@\n-  if (hr->is_humongous()) {\n+  if (hr->is_humongous() || hr->has_pinned_objects()) {\n@@ -77,0 +77,5 @@\n+static bool has_pinned_objects(HeapRegion* hr) {\n+  return hr->has_pinned_objects() ||\n+      (hr->is_humongous() && hr->humongous_start_region()->has_pinned_objects());\n+}\n+\n@@ -81,10 +86,15 @@\n-  } else {\n-    assert(hr->containing_set() == nullptr, \"already cleared by PrepareRegionsClosure\");\n-    if (hr->is_humongous()) {\n-      oop obj = cast_to_oop(hr->humongous_start_region()->bottom());\n-      bool is_empty = !_collector->mark_bitmap()->is_marked(obj);\n-      if (is_empty) {\n-        free_empty_humongous_region(hr);\n-      } else {\n-        _collector->set_has_humongous();\n-      }\n+    return false;\n+  }\n+\n+  assert(hr->containing_set() == nullptr, \"already cleared by PrepareRegionsClosure\");\n+  if (has_pinned_objects(hr)) {\n+    \/\/ First check regions with pinned objects: they need to be skipped regardless\n+    \/\/ of region type and never be considered for reclamation.\n+    assert(_collector->is_skip_compacting(hr->hrm_index()), \"pinned region %u must be skip_compacting\", hr->hrm_index());\n+    log_trace(gc, phases)(\"Phase 2: skip compaction region index: %u (%s), has pinned objects\",\n+                          hr->hrm_index(), hr->get_short_type_str());\n+  } else if (hr->is_humongous()) {\n+    oop obj = cast_to_oop(hr->humongous_start_region()->bottom());\n+    bool is_empty = !_collector->mark_bitmap()->is_marked(obj);\n+    if (is_empty) {\n+      free_empty_humongous_region(hr);\n@@ -92,2 +102,5 @@\n-      assert(MarkSweepDeadRatio > 0,\n-             \"only skip compaction for other regions when MarkSweepDeadRatio > 0\");\n+      _collector->set_has_humongous();\n+    }\n+  } else {\n+    assert(MarkSweepDeadRatio > 0,\n+           \"only skip compaction for other regions when MarkSweepDeadRatio > 0\");\n@@ -95,3 +108,3 @@\n-      \/\/ Too many live objects in the region; skip compacting it.\n-      _collector->update_from_compacting_to_skip_compacting(hr->hrm_index());\n-      log_trace(gc, phases)(\"Phase 2: skip compaction region index: %u, live words: \" SIZE_FORMAT,\n+    \/\/ Too many live objects in the region; skip compacting it.\n+    _collector->update_from_compacting_to_skip_compacting(hr->hrm_index());\n+    log_trace(gc, phases)(\"Phase 2: skip compaction region index: %u, live words: \" SIZE_FORMAT,\n@@ -99,1 +112,0 @@\n-    }\n","filename":"src\/hotspot\/share\/gc\/g1\/g1FullGCPrepareTask.inline.hpp","additions":29,"deletions":17,"binary":false,"changes":46,"status":"modified"},{"patch":"@@ -35,1 +35,1 @@\n-#include \"gc\/g1\/g1YoungGCEvacFailureInjector.inline.hpp\"\n+#include \"gc\/g1\/g1YoungGCAllocationFailureInjector.inline.hpp\"\n@@ -88,1 +88,1 @@\n-    EVAC_FAILURE_INJECTOR_ONLY(_evac_failure_inject_counter(0) COMMA)\n+    ALLOCATION_FAILURE_INJECTOR_ONLY(_allocation_failure_inject_counter(0) COMMA)\n@@ -430,3 +430,3 @@\n-#if EVAC_FAILURE_INJECTOR\n-bool G1ParScanThreadState::inject_evacuation_failure(uint region_idx) {\n-  return _g1h->evac_failure_injector()->evacuation_should_fail(_evac_failure_inject_counter, region_idx);\n+#if ALLOCATION_FAILURE_INJECTOR\n+bool G1ParScanThreadState::inject_allocation_failure(uint region_idx) {\n+  return _g1h->allocation_failure_injector()->allocation_should_fail(_allocation_failure_inject_counter, region_idx);\n@@ -470,0 +470,5 @@\n+  \/\/ JNI only allows pinning of typeArrays, so we only need to keep those in place.\n+  if (region_attr.is_pinned() && klass->is_typeArray_klass()) {\n+    return handle_evacuation_failure_par(old, old_mark, word_sz, true \/* cause_pinned *\/);\n+  }\n+\n@@ -484,1 +489,1 @@\n-      return handle_evacuation_failure_par(old, old_mark, word_sz);\n+      return handle_evacuation_failure_par(old, old_mark, word_sz, false \/* cause_pinned *\/);\n@@ -492,1 +497,1 @@\n-  if (inject_evacuation_failure(from_region->hrm_index())) {\n+  if (inject_allocation_failure(from_region->hrm_index())) {\n@@ -496,1 +501,1 @@\n-    return handle_evacuation_failure_par(old, old_mark, word_sz);\n+    return handle_evacuation_failure_par(old, old_mark, word_sz, false \/* cause_pinned *\/);\n@@ -633,1 +638,1 @@\n-oop G1ParScanThreadState::handle_evacuation_failure_par(oop old, markWord m, size_t word_sz) {\n+oop G1ParScanThreadState::handle_evacuation_failure_par(oop old, markWord m, size_t word_sz, bool cause_pinned) {\n@@ -641,1 +646,1 @@\n-    if (_evac_failure_regions->record(r->hrm_index())) {\n+    if (_evac_failure_regions->record(_worker_id, r->hrm_index(), cause_pinned)) {\n","filename":"src\/hotspot\/share\/gc\/g1\/g1ParScanThreadState.cpp","additions":15,"deletions":10,"binary":false,"changes":25,"status":"modified"},{"patch":"@@ -31,1 +31,1 @@\n-#include \"gc\/g1\/g1YoungGCEvacFailureInjector.hpp\"\n+#include \"gc\/g1\/g1YoungGCAllocationFailureInjector.hpp\"\n@@ -107,1 +107,1 @@\n-  EVAC_FAILURE_INJECTOR_ONLY(size_t _evac_failure_inject_counter;)\n+  ALLOCATION_FAILURE_INJECTOR_ONLY(size_t _allocation_failure_inject_counter;)\n@@ -123,1 +123,1 @@\n-  bool inject_evacuation_failure(uint region_idx) EVAC_FAILURE_INJECTOR_RETURN_( return false; );\n+  bool inject_allocation_failure(uint region_idx) ALLOCATION_FAILURE_INJECTOR_RETURN_( return false; );\n@@ -234,1 +234,1 @@\n-  oop handle_evacuation_failure_par(oop obj, markWord m, size_t word_sz);\n+  oop handle_evacuation_failure_par(oop obj, markWord m, size_t word_sz, bool cause_pinned);\n","filename":"src\/hotspot\/share\/gc\/g1\/g1ParScanThreadState.hpp","additions":4,"deletions":4,"binary":false,"changes":8,"status":"modified"},{"patch":"@@ -95,1 +95,1 @@\n-  guarantee(ct->is_card_aligned(reserved_mr.start()), \"generation must be card aligned\");\n+  guarantee(CardTable::is_card_aligned(reserved_mr.start()), \"generation must be card aligned\");\n@@ -98,1 +98,1 @@\n-  guarantee(ct->is_card_aligned(reserved_mr.end()), \"generation must be card aligned\");\n+  guarantee(CardTable::is_card_aligned(reserved_mr.end()), \"generation must be card aligned\");\n@@ -136,2 +136,2 @@\n-  assert((block_word_size % (ObjectStartArray::card_size())) == 0,\n-         \"Block size not a multiple of start_array block\");\n+  assert((block_word_size % BOTConstants::card_size_in_words()) == 0,\n+         \"To ensure fast object_start calls\");\n@@ -144,4 +144,0 @@\n-  if (!start_array()->object_starts_in_range(begin, end)) {\n-    return;\n-  }\n-\n@@ -289,1 +285,1 @@\n-    _start_array.allocate_block(cur);\n+    _start_array.update_for_block(cur, cur + word_size);\n@@ -393,1 +389,1 @@\n- public:\n+public:\n@@ -402,1 +398,0 @@\n-    guarantee(_start_array->is_block_allocated(cast_from_oop<HeapWord*>(obj)), \"ObjectStartArray missing block allocation\");\n","filename":"src\/hotspot\/share\/gc\/parallel\/psOldGen.cpp","additions":6,"deletions":11,"binary":false,"changes":17,"status":"modified"},{"patch":"@@ -43,1 +43,0 @@\n-#include \"gc\/shared\/generationSpec.hpp\"\n@@ -563,1 +562,1 @@\n-  size_t min_new_size = initial_size();\n+  size_t min_new_size = NewSize;\n","filename":"src\/hotspot\/share\/gc\/serial\/defNewGeneration.cpp","additions":1,"deletions":2,"binary":false,"changes":3,"status":"modified"},{"patch":"@@ -41,0 +41,1 @@\n+#include \"gc\/shared\/classUnloadingContext.hpp\"\n@@ -208,0 +209,2 @@\n+    ClassUnloadingContext* ctx = ClassUnloadingContext::context();\n+\n@@ -219,2 +222,9 @@\n-    \/\/ Release unloaded nmethod's memory.\n-    CodeCache::flush_unlinked_nmethods();\n+    {\n+      GCTraceTime(Debug, gc, phases) t(\"Purge Unlinked NMethods\", gc_timer());\n+      \/\/ Release unloaded nmethod's memory.\n+      ctx->purge_nmethods();\n+    }\n+    {\n+      GCTraceTime(Debug, gc, phases) t(\"Free Code Blobs\", gc_timer());\n+      ctx->free_code_blobs();\n+    }\n","filename":"src\/hotspot\/share\/gc\/serial\/genMarkSweep.cpp","additions":12,"deletions":2,"binary":false,"changes":14,"status":"modified"},{"patch":"@@ -168,1 +168,3 @@\n-  static inline size_t filler_array_min_size();\n+  static size_t filler_array_min_size();\n+\n+protected:\n","filename":"src\/hotspot\/share\/gc\/shared\/collectedHeap.hpp","additions":3,"deletions":1,"binary":false,"changes":4,"status":"modified"},{"patch":"@@ -37,0 +37,1 @@\n+#include \"gc\/serial\/tenuredGeneration.hpp\"\n@@ -38,0 +39,1 @@\n+#include \"gc\/shared\/classUnloadingContext.hpp\"\n@@ -50,1 +52,0 @@\n-#include \"gc\/shared\/generationSpec.hpp\"\n@@ -89,8 +90,0 @@\n-  _young_gen_spec(new GenerationSpec(young,\n-                                     NewSize,\n-                                     MaxNewSize,\n-                                     GenAlignment)),\n-  _old_gen_spec(new GenerationSpec(old,\n-                                   OldSize,\n-                                   MaxOldSize,\n-                                   GenAlignment)),\n@@ -119,2 +112,2 @@\n-  ReservedSpace young_rs = heap_rs.first_part(_young_gen_spec->max_size());\n-  ReservedSpace old_rs = heap_rs.last_part(_young_gen_spec->max_size());\n+  ReservedSpace young_rs = heap_rs.first_part(MaxNewSize);\n+  ReservedSpace old_rs = heap_rs.last_part(MaxNewSize);\n@@ -129,2 +122,2 @@\n-  _young_gen = _young_gen_spec->init(young_rs, rem_set());\n-  _old_gen = _old_gen_spec->init(old_rs, rem_set());\n+  _young_gen = new DefNewGeneration(young_rs, NewSize, MinNewSize, MaxNewSize);\n+  _old_gen = new TenuredGeneration(old_rs, OldSize, MinOldSize, MaxOldSize, rem_set());\n@@ -149,2 +142,2 @@\n-  size_t total_reserved = _young_gen_spec->max_size() + _old_gen_spec->max_size();\n-  if (total_reserved < _young_gen_spec->max_size()) {\n+  size_t total_reserved = MaxNewSize + MaxOldSize;\n+  if (total_reserved < MaxNewSize) {\n@@ -205,8 +198,0 @@\n-GenerationSpec* GenCollectedHeap::young_gen_spec() const {\n-  return _young_gen_spec;\n-}\n-\n-GenerationSpec* GenCollectedHeap::old_gen_spec() const {\n-  return _old_gen_spec;\n-}\n-\n@@ -544,0 +529,3 @@\n+    ClassUnloadingContext ctx(1 \/* num_nmethod_unlink_workers *\/,\n+                              false \/* lock_codeblob_free_separately *\/);\n+\n@@ -559,1 +547,1 @@\n-    ClassLoaderDataGraph::purge(\/*at_safepoint*\/true);\n+    ClassLoaderDataGraph::purge(true \/* at_safepoint *\/);\n","filename":"src\/hotspot\/share\/gc\/shared\/genCollectedHeap.cpp","additions":12,"deletions":24,"binary":false,"changes":36,"status":"modified"},{"patch":"@@ -30,0 +30,1 @@\n+#include \"gc\/shared\/classUnloadingContext.hpp\"\n@@ -1028,1 +1029,1 @@\n-      ShenandoahSuspendibleThreadSetJoiner stsj(ShenandoahSuspendibleWorkers);\n+      ShenandoahSuspendibleThreadSetJoiner stsj;\n@@ -1192,7 +1193,3 @@\n-  if (ShenandoahElasticTLAB) {\n-    \/\/ With Elastic TLABs, return the max allowed size, and let the allocation path\n-    \/\/ figure out the safe size for current allocation.\n-    return ShenandoahHeapRegion::max_tlab_size_bytes();\n-  } else {\n-    return MIN2(_free_set->unsafe_peek_free(), ShenandoahHeapRegion::max_tlab_size_bytes());\n-  }\n+  \/\/ Return the max allowed size, and let the allocation path\n+  \/\/ figure out the safe size for current allocation.\n+  return ShenandoahHeapRegion::max_tlab_size_bytes();\n@@ -1830,0 +1827,3 @@\n+  ClassUnloadingContext ctx(_workers->active_workers(),\n+                            false \/* lock_codeblob_free_separately *\/);\n+\n@@ -1847,1 +1847,1 @@\n-    CodeCache::flush_unlinked_nmethods();\n+    ClassUnloadingContext::context()->purge_and_free_nmethods();\n@@ -1854,1 +1854,1 @@\n-    ClassLoaderDataGraph::purge(\/*at_safepoint*\/true);\n+    ClassLoaderDataGraph::purge(true \/* at_safepoint *\/);\n@@ -2024,6 +2024,3 @@\n-    if (UseDynamicNumberOfGCThreads) {\n-      assert(nworkers <= ParallelGCThreads, \"Cannot use more than it has\");\n-    } else {\n-      \/\/ Use ParallelGCThreads inside safepoints\n-      assert(nworkers == ParallelGCThreads, \"Use ParallelGCThreads within safepoints\");\n-    }\n+    \/\/ Use ParallelGCThreads inside safepoints\n+    assert(nworkers == ParallelGCThreads, \"Use ParallelGCThreads (%u) within safepoint, not %u\",\n+           ParallelGCThreads, nworkers);\n@@ -2031,6 +2028,3 @@\n-    if (UseDynamicNumberOfGCThreads) {\n-      assert(nworkers <= ConcGCThreads, \"Cannot use more than it has\");\n-    } else {\n-      \/\/ Use ConcGCThreads outside safepoints\n-      assert(nworkers == ConcGCThreads, \"Use ConcGCThreads outside safepoints\");\n-    }\n+    \/\/ Use ConcGCThreads outside safepoints\n+    assert(nworkers == ConcGCThreads, \"Use ConcGCThreads (%u) outside safepoints, %u\",\n+           ConcGCThreads, nworkers);\n@@ -2062,1 +2056,1 @@\n-      ShenandoahSuspendibleThreadSetJoiner stsj(ShenandoahSuspendibleWorkers);\n+      ShenandoahSuspendibleThreadSetJoiner stsj;\n@@ -2244,3 +2238,1 @@\n-  if (ShenandoahSuspendibleWorkers) {\n-    SuspendibleThreadSet::synchronize();\n-  }\n+  SuspendibleThreadSet::synchronize();\n@@ -2250,3 +2242,1 @@\n-  if (ShenandoahSuspendibleWorkers) {\n-    SuspendibleThreadSet::desynchronize();\n-  }\n+  SuspendibleThreadSet::desynchronize();\n","filename":"src\/hotspot\/share\/gc\/shenandoah\/shenandoahHeap.cpp","additions":19,"deletions":29,"binary":false,"changes":48,"status":"modified"},{"patch":"@@ -259,1 +259,1 @@\n-  if (sts_active && ShenandoahSuspendibleWorkers && !cancelled_gc()) {\n+  if (sts_active && !cancelled_gc()) {\n","filename":"src\/hotspot\/share\/gc\/shenandoah\/shenandoahHeap.inline.hpp","additions":1,"deletions":1,"binary":false,"changes":2,"status":"modified"},{"patch":"@@ -45,0 +45,1 @@\n+#include \"utilities\/debug.hpp\"\n@@ -229,0 +230,1 @@\n+  AARCH64_ONLY(fatal(\"Should use ZLoadBarrierStubC2Aarch64::create\"));\n@@ -278,0 +280,1 @@\n+  AARCH64_ONLY(fatal(\"Should use ZStoreBarrierStubC2Aarch64::create\"));\n","filename":"src\/hotspot\/share\/gc\/z\/c2\/zBarrierSetC2.cpp","additions":3,"deletions":0,"binary":false,"changes":3,"status":"modified"},{"patch":"@@ -71,0 +71,1 @@\n+#include \"runtime\/cpuTimeCounters.hpp\"\n@@ -789,0 +790,3 @@\n+  \/\/ Initialize CPUTimeCounters object, which must be done before creation of the heap.\n+  CPUTimeCounters::initialize();\n+\n","filename":"src\/hotspot\/share\/memory\/universe.cpp","additions":4,"deletions":0,"binary":false,"changes":4,"status":"modified"},{"patch":"@@ -64,4 +64,0 @@\n-oop Klass::java_mirror_no_keepalive() const {\n-  return _java_mirror.peek();\n-}\n-\n","filename":"src\/hotspot\/share\/oops\/klass.cpp","additions":0,"deletions":4,"binary":false,"changes":4,"status":"modified"},{"patch":"@@ -64,0 +64,4 @@\n+inline oop Klass::java_mirror_no_keepalive() const {\n+  return _java_mirror.peek();\n+}\n+\n","filename":"src\/hotspot\/share\/oops\/klass.inline.hpp","additions":4,"deletions":0,"binary":false,"changes":4,"status":"modified"},{"patch":"@@ -593,1 +593,5 @@\n-    xtty->print(\"%s\", ss.as_string()); \/\/ print to tty would use xml escape encoding\n+  }\n+\n+  tty->print(\"%s\", ss.as_string());\n+\n+  if (xtty != nullptr) {\n@@ -595,2 +599,0 @@\n-  } else {\n-    tty->print(\"%s\", ss.as_string());\n@@ -5148,1 +5150,1 @@\n-  if ((_directive->ideal_phase_mask() & CompilerPhaseTypeHelper::to_bitmask(cpt)) != 0) {\n+  if (_directive->should_print_phase(cpt)) {\n","filename":"src\/hotspot\/share\/opto\/compile.cpp","additions":6,"deletions":4,"binary":false,"changes":10,"status":"modified"},{"patch":"@@ -73,1 +73,0 @@\n-\n@@ -80,0 +79,23 @@\n+\/\/ Note that scoped accesses (cf. scopedMemoryAccess.cpp) can install\n+\/\/ an async handshake on the entry to an Unsafe method. When that happens,\n+\/\/ it is expected that we are not allowed to touch the underlying memory\n+\/\/ that might have gotten unmapped. Therefore, we check at the entry\n+\/\/ to unsafe functions, if we have such async exception conditions,\n+\/\/ and return immediately if that is the case.\n+\/\/\n+\/\/ We can't have safepoints in this code.\n+\/\/ It would be problematic if an async exception handshake were installed later on\n+\/\/ during another safepoint in the function, but before the memory access happens,\n+\/\/ as the memory will be freed after the handshake is installed. We must notice\n+\/\/ the installed handshake and return early before doing the memory access to prevent\n+\/\/ accesses to freed memory.\n+\/\/\n+\/\/ Note also that we MUST do a scoped memory access in the VM (or Java) thread\n+\/\/ state. Since we rely on a handshake to check for threads that are accessing\n+\/\/ scoped memory, and we need the handshaking thread to wait until we get to a\n+\/\/ safepoint, in order to make sure we are not in the middle of accessing memory\n+\/\/ that is about to be freed. (i.e. there can be no UNSAFE_LEAF_SCOPED)\n+#define UNSAFE_ENTRY_SCOPED(result_type, header) \\\n+  JVM_ENTRY(static result_type, header) \\\n+  if (thread->has_async_exception_condition()) {return (result_type)0;}\n+\n@@ -282,1 +304,1 @@\n-UNSAFE_ENTRY(java_type, Unsafe_Get##Type(JNIEnv *env, jobject unsafe, jobject obj, jlong offset)) { \\\n+UNSAFE_ENTRY_SCOPED(java_type, Unsafe_Get##Type(JNIEnv *env, jobject unsafe, jobject obj, jlong offset)) { \\\n@@ -286,1 +308,1 @@\n-UNSAFE_ENTRY(void, Unsafe_Put##Type(JNIEnv *env, jobject unsafe, jobject obj, jlong offset, java_type x)) { \\\n+UNSAFE_ENTRY_SCOPED(void, Unsafe_Put##Type(JNIEnv *env, jobject unsafe, jobject obj, jlong offset, java_type x)) { \\\n@@ -305,1 +327,1 @@\n-UNSAFE_ENTRY(java_type, Unsafe_Get##Type##Volatile(JNIEnv *env, jobject unsafe, jobject obj, jlong offset)) { \\\n+UNSAFE_ENTRY_SCOPED(java_type, Unsafe_Get##Type##Volatile(JNIEnv *env, jobject unsafe, jobject obj, jlong offset)) { \\\n@@ -309,1 +331,1 @@\n-UNSAFE_ENTRY(void, Unsafe_Put##Type##Volatile(JNIEnv *env, jobject unsafe, jobject obj, jlong offset, java_type x)) { \\\n+UNSAFE_ENTRY_SCOPED(void, Unsafe_Put##Type##Volatile(JNIEnv *env, jobject unsafe, jobject obj, jlong offset, java_type x)) { \\\n@@ -365,1 +387,1 @@\n-UNSAFE_ENTRY(void, Unsafe_SetMemory0(JNIEnv *env, jobject unsafe, jobject obj, jlong offset, jlong size, jbyte value)) {\n+UNSAFE_ENTRY_SCOPED(void, Unsafe_SetMemory0(JNIEnv *env, jobject unsafe, jobject obj, jlong offset, jlong size, jbyte value)) {\n@@ -374,1 +396,1 @@\n-UNSAFE_ENTRY(void, Unsafe_CopyMemory0(JNIEnv *env, jobject unsafe, jobject srcObj, jlong srcOffset, jobject dstObj, jlong dstOffset, jlong size)) {\n+UNSAFE_ENTRY_SCOPED(void, Unsafe_CopyMemory0(JNIEnv *env, jobject unsafe, jobject srcObj, jlong srcOffset, jobject dstObj, jlong dstOffset, jlong size)) {\n@@ -393,5 +415,1 @@\n-\/\/ This function is a leaf since if the source and destination are both in native memory\n-\/\/ the copy may potentially be very large, and we don't want to disable GC if we can avoid it.\n-\/\/ If either source or destination (or both) are on the heap, the function will enter VM using\n-\/\/ JVM_ENTRY_FROM_LEAF\n-UNSAFE_LEAF(void, Unsafe_CopySwapMemory0(JNIEnv *env, jobject unsafe, jobject srcObj, jlong srcOffset, jobject dstObj, jlong dstOffset, jlong size, jlong elemSize)) {\n+UNSAFE_ENTRY_SCOPED(void, Unsafe_CopySwapMemory0(JNIEnv *env, jobject unsafe, jobject srcObj, jlong srcOffset, jobject dstObj, jlong dstOffset, jlong size, jlong elemSize)) {\n@@ -401,16 +419,2 @@\n-  if (srcObj == nullptr && dstObj == nullptr) {\n-    \/\/ Both src & dst are in native memory\n-    address src = (address)srcOffset;\n-    address dst = (address)dstOffset;\n-\n-    {\n-      JavaThread* thread = JavaThread::thread_from_jni_environment(env);\n-      GuardUnsafeAccess guard(thread);\n-      Copy::conjoint_swap(src, dst, sz, esz);\n-    }\n-  } else {\n-    \/\/ At least one of src\/dst are on heap, transition to VM to access raw pointers\n-\n-    JVM_ENTRY_FROM_LEAF(env, void, Unsafe_CopySwapMemory0) {\n-      oop srcp = JNIHandles::resolve(srcObj);\n-      oop dstp = JNIHandles::resolve(dstObj);\n+  oop srcp = JNIHandles::resolve(srcObj);\n+  oop dstp = JNIHandles::resolve(dstObj);\n@@ -418,2 +422,2 @@\n-      address src = (address)index_oop_from_field_offset_long(srcp, srcOffset);\n-      address dst = (address)index_oop_from_field_offset_long(dstp, dstOffset);\n+  address src = (address)index_oop_from_field_offset_long(srcp, srcOffset);\n+  address dst = (address)index_oop_from_field_offset_long(dstp, dstOffset);\n@@ -421,5 +425,3 @@\n-      {\n-        GuardUnsafeAccess guard(thread);\n-        Copy::conjoint_swap(src, dst, sz, esz);\n-      }\n-    } JVM_END\n+  {\n+    GuardUnsafeAccess guard(thread);\n+    Copy::conjoint_swap(src, dst, sz, esz);\n@@ -721,1 +723,1 @@\n-UNSAFE_ENTRY(jint, Unsafe_CompareAndExchangeInt(JNIEnv *env, jobject unsafe, jobject obj, jlong offset, jint e, jint x)) {\n+UNSAFE_ENTRY_SCOPED(jint, Unsafe_CompareAndExchangeInt(JNIEnv *env, jobject unsafe, jobject obj, jlong offset, jint e, jint x)) {\n@@ -727,1 +729,1 @@\n-UNSAFE_ENTRY(jlong, Unsafe_CompareAndExchangeLong(JNIEnv *env, jobject unsafe, jobject obj, jlong offset, jlong e, jlong x)) {\n+UNSAFE_ENTRY_SCOPED(jlong, Unsafe_CompareAndExchangeLong(JNIEnv *env, jobject unsafe, jobject obj, jlong offset, jlong e, jlong x)) {\n@@ -742,1 +744,1 @@\n-UNSAFE_ENTRY(jboolean, Unsafe_CompareAndSetInt(JNIEnv *env, jobject unsafe, jobject obj, jlong offset, jint e, jint x)) {\n+UNSAFE_ENTRY_SCOPED(jboolean, Unsafe_CompareAndSetInt(JNIEnv *env, jobject unsafe, jobject obj, jlong offset, jint e, jint x)) {\n@@ -748,1 +750,1 @@\n-UNSAFE_ENTRY(jboolean, Unsafe_CompareAndSetLong(JNIEnv *env, jobject unsafe, jobject obj, jlong offset, jlong e, jlong x)) {\n+UNSAFE_ENTRY_SCOPED(jboolean, Unsafe_CompareAndSetLong(JNIEnv *env, jobject unsafe, jobject obj, jlong offset, jlong e, jlong x)) {\n","filename":"src\/hotspot\/share\/prims\/unsafe.cpp","additions":41,"deletions":39,"binary":false,"changes":80,"status":"modified"},{"patch":"@@ -104,4 +104,0 @@\n-char*  Arguments::_default_shared_archive_path  = nullptr;\n-char*  Arguments::SharedArchivePath             = nullptr;\n-char*  Arguments::SharedDynamicArchivePath      = nullptr;\n-\n@@ -512,0 +508,1 @@\n+  { \"RegisterFinalizersAtInit\",     JDK_Version::jdk(22), JDK_Version::jdk(23), JDK_Version::jdk(24) },\n@@ -1265,13 +1262,1 @@\n-#if INCLUDE_CDS\n-  if (is_internal_module_property(key)) {\n-    MetaspaceShared::disable_optimized_module_handling();\n-    log_info(cds)(\"optimized module handling: disabled due to incompatible property: %s=%s\", key, value);\n-  }\n-  if (strcmp(key, \"jdk.module.showModuleResolution\") == 0 ||\n-      strcmp(key, \"jdk.module.validation\") == 0 ||\n-      strcmp(key, \"java.system.class.loader\") == 0) {\n-    CDSConfig::disable_loading_full_module_graph();\n-    CDSConfig::disable_dumping_full_module_graph();\n-    log_info(cds)(\"full module graph: disabled due to incompatible property: %s=%s\", key, value);\n-  }\n-#endif\n+  CDSConfig::check_system_property(key, value);\n@@ -1331,54 +1316,0 @@\n-#if INCLUDE_CDS\n-const char* unsupported_properties[] = { \"jdk.module.limitmods\",\n-                                         \"jdk.module.upgrade.path\",\n-                                         \"jdk.module.patch.0\" };\n-const char* unsupported_options[] = { \"--limit-modules\",\n-                                      \"--upgrade-module-path\",\n-                                      \"--patch-module\"\n-                                    };\n-void Arguments::check_unsupported_dumping_properties() {\n-  assert(CDSConfig::is_dumping_archive(),\n-         \"this function is only used with CDS dump time\");\n-  assert(ARRAY_SIZE(unsupported_properties) == ARRAY_SIZE(unsupported_options), \"must be\");\n-  \/\/ If a vm option is found in the unsupported_options array, vm will exit with an error message.\n-  SystemProperty* sp = system_properties();\n-  while (sp != nullptr) {\n-    for (uint i = 0; i < ARRAY_SIZE(unsupported_properties); i++) {\n-      if (strcmp(sp->key(), unsupported_properties[i]) == 0) {\n-        vm_exit_during_initialization(\n-          \"Cannot use the following option when dumping the shared archive\", unsupported_options[i]);\n-      }\n-    }\n-    sp = sp->next();\n-  }\n-\n-  \/\/ Check for an exploded module build in use with -Xshare:dump.\n-  if (!has_jimage()) {\n-    vm_exit_during_initialization(\"Dumping the shared archive is not supported with an exploded module build\");\n-  }\n-}\n-\n-bool Arguments::check_unsupported_cds_runtime_properties() {\n-  assert(UseSharedSpaces, \"this function is only used with -Xshare:{on,auto}\");\n-  assert(ARRAY_SIZE(unsupported_properties) == ARRAY_SIZE(unsupported_options), \"must be\");\n-  if (ArchiveClassesAtExit != nullptr) {\n-    \/\/ dynamic dumping, just return false for now.\n-    \/\/ check_unsupported_dumping_properties() will be called later to check the same set of\n-    \/\/ properties, and will exit the VM with the correct error message if the unsupported properties\n-    \/\/ are used.\n-    return false;\n-  }\n-  for (uint i = 0; i < ARRAY_SIZE(unsupported_properties); i++) {\n-    if (get_property(unsupported_properties[i]) != nullptr) {\n-      if (RequireSharedSpaces) {\n-        warning(\"CDS is disabled when the %s option is specified.\", unsupported_options[i]);\n-      } else {\n-        log_info(cds)(\"CDS is disabled when the %s option is specified.\", unsupported_options[i]);\n-      }\n-      return true;\n-    }\n-  }\n-  return false;\n-}\n-#endif\n-\n@@ -1434,1 +1365,1 @@\n-static void no_shared_spaces(const char* message) {\n+void Arguments::no_shared_spaces(const char* message) {\n@@ -3039,25 +2970,1 @@\n-#if INCLUDE_CDS\n-  if (CDSConfig::is_dumping_static_archive()) {\n-    if (!mode_flag_cmd_line) {\n-      \/\/ By default, -Xshare:dump runs in interpreter-only mode, which is required for deterministic archive.\n-      \/\/\n-      \/\/ If your classlist is large and you don't care about deterministic dumping, you can use\n-      \/\/ -Xshare:dump -Xmixed to improve dumping speed.\n-      set_mode_flags(_int);\n-    } else if (_mode == _comp) {\n-      \/\/ -Xcomp may use excessive CPU for the test tiers. Also, -Xshare:dump runs a small and fixed set of\n-      \/\/ Java code, so there's not much benefit in running -Xcomp.\n-      log_info(cds)(\"reduced -Xcomp to -Xmixed for static dumping\");\n-      set_mode_flags(_mixed);\n-    }\n-\n-    \/\/ String deduplication may cause CDS to iterate the strings in different order from one\n-    \/\/ run to another which resulting in non-determinstic CDS archives.\n-    \/\/ Disable UseStringDeduplication while dumping CDS archive.\n-    UseStringDeduplication = false;\n-  }\n-\n-  \/\/ RecordDynamicDumpInfo is not compatible with ArchiveClassesAtExit\n-  if (ArchiveClassesAtExit != nullptr && RecordDynamicDumpInfo) {\n-    jio_fprintf(defaultStream::output_stream(),\n-                \"-XX:+RecordDynamicDumpInfo cannot be used with -XX:ArchiveClassesAtExit.\\n\");\n+  if (!CDSConfig::check_vm_args_consistency(patch_mod_javabase, mode_flag_cmd_line)) {\n@@ -3067,33 +2974,0 @@\n-  if (ArchiveClassesAtExit == nullptr && !RecordDynamicDumpInfo) {\n-    CDSConfig::disable_dumping_dynamic_archive();\n-  } else {\n-    CDSConfig::enable_dumping_dynamic_archive();\n-  }\n-\n-  if (AutoCreateSharedArchive) {\n-    if (SharedArchiveFile == nullptr) {\n-      log_warning(cds)(\"-XX:+AutoCreateSharedArchive requires -XX:SharedArchiveFile\");\n-      return JNI_ERR;\n-    }\n-    if (ArchiveClassesAtExit != nullptr) {\n-      log_warning(cds)(\"-XX:+AutoCreateSharedArchive does not work with ArchiveClassesAtExit\");\n-      return JNI_ERR;\n-    }\n-  }\n-\n-  if (UseSharedSpaces && patch_mod_javabase) {\n-    no_shared_spaces(\"CDS is disabled when \" JAVA_BASE_NAME \" module is patched.\");\n-  }\n-  if (UseSharedSpaces && check_unsupported_cds_runtime_properties()) {\n-    UseSharedSpaces = false;\n-  }\n-\n-  if (CDSConfig::is_dumping_archive()) {\n-    \/\/ Always verify non-system classes during CDS dump\n-    if (!BytecodeVerificationRemote) {\n-      BytecodeVerificationRemote = true;\n-      log_info(cds)(\"All non-system classes will be verified (-Xverify:remote) during CDS dump time.\");\n-    }\n-  }\n-#endif\n-\n@@ -3389,181 +3263,0 @@\n-void Arguments::set_shared_spaces_flags_and_archive_paths() {\n-  if (CDSConfig::is_dumping_static_archive()) {\n-    if (RequireSharedSpaces) {\n-      warning(\"Cannot dump shared archive while using shared archive\");\n-    }\n-    UseSharedSpaces = false;\n-  }\n-#if INCLUDE_CDS\n-  \/\/ Initialize shared archive paths which could include both base and dynamic archive paths\n-  \/\/ This must be after set_ergonomics_flags() called so flag UseCompressedOops is set properly.\n-  \/\/\n-  \/\/ UseSharedSpaces may be disabled if -XX:SharedArchiveFile is invalid.\n-  if (CDSConfig::is_dumping_static_archive() || UseSharedSpaces) {\n-    init_shared_archive_paths();\n-  }\n-#endif  \/\/ INCLUDE_CDS\n-}\n-\n-#if INCLUDE_CDS\n-\/\/ Sharing support\n-\/\/ Construct the path to the archive\n-char* Arguments::get_default_shared_archive_path() {\n-  if (_default_shared_archive_path == nullptr) {\n-    char jvm_path[JVM_MAXPATHLEN];\n-    os::jvm_path(jvm_path, sizeof(jvm_path));\n-    char *end = strrchr(jvm_path, *os::file_separator());\n-    if (end != nullptr) *end = '\\0';\n-    size_t jvm_path_len = strlen(jvm_path);\n-    size_t file_sep_len = strlen(os::file_separator());\n-    const size_t len = jvm_path_len + file_sep_len + 20;\n-    _default_shared_archive_path = NEW_C_HEAP_ARRAY(char, len, mtArguments);\n-    jio_snprintf(_default_shared_archive_path, len,\n-                LP64_ONLY(!UseCompressedOops ? \"%s%sclasses_nocoops.jsa\":) \"%s%sclasses.jsa\",\n-                jvm_path, os::file_separator());\n-  }\n-  return _default_shared_archive_path;\n-}\n-\n-int Arguments::num_archives(const char* archive_path) {\n-  if (archive_path == nullptr) {\n-    return 0;\n-  }\n-  int npaths = 1;\n-  char* p = (char*)archive_path;\n-  while (*p != '\\0') {\n-    if (*p == os::path_separator()[0]) {\n-      npaths++;\n-    }\n-    p++;\n-  }\n-  return npaths;\n-}\n-\n-void Arguments::extract_shared_archive_paths(const char* archive_path,\n-                                         char** base_archive_path,\n-                                         char** top_archive_path) {\n-  char* begin_ptr = (char*)archive_path;\n-  char* end_ptr = strchr((char*)archive_path, os::path_separator()[0]);\n-  if (end_ptr == nullptr || end_ptr == begin_ptr) {\n-    vm_exit_during_initialization(\"Base archive was not specified\", archive_path);\n-  }\n-  size_t len = end_ptr - begin_ptr;\n-  char* cur_path = NEW_C_HEAP_ARRAY(char, len + 1, mtInternal);\n-  strncpy(cur_path, begin_ptr, len);\n-  cur_path[len] = '\\0';\n-  *base_archive_path = cur_path;\n-\n-  begin_ptr = ++end_ptr;\n-  if (*begin_ptr == '\\0') {\n-    vm_exit_during_initialization(\"Top archive was not specified\", archive_path);\n-  }\n-  end_ptr = strchr(begin_ptr, '\\0');\n-  assert(end_ptr != nullptr, \"sanity\");\n-  len = end_ptr - begin_ptr;\n-  cur_path = NEW_C_HEAP_ARRAY(char, len + 1, mtInternal);\n-  strncpy(cur_path, begin_ptr, len + 1);\n-  *top_archive_path = cur_path;\n-}\n-\n-void Arguments::init_shared_archive_paths() {\n-  if (ArchiveClassesAtExit != nullptr) {\n-    assert(!RecordDynamicDumpInfo, \"already checked\");\n-    if (CDSConfig::is_dumping_static_archive()) {\n-      vm_exit_during_initialization(\"-XX:ArchiveClassesAtExit cannot be used with -Xshare:dump\");\n-    }\n-    check_unsupported_dumping_properties();\n-\n-    if (os::same_files(get_default_shared_archive_path(), ArchiveClassesAtExit)) {\n-      vm_exit_during_initialization(\n-        \"Cannot specify the default CDS archive for -XX:ArchiveClassesAtExit\", get_default_shared_archive_path());\n-    }\n-  }\n-\n-  if (SharedArchiveFile == nullptr) {\n-    SharedArchivePath = get_default_shared_archive_path();\n-  } else {\n-    int archives = num_archives(SharedArchiveFile);\n-    assert(archives > 0, \"must be\");\n-\n-    if (CDSConfig::is_dumping_archive() && archives > 1) {\n-      vm_exit_during_initialization(\n-        \"Cannot have more than 1 archive file specified in -XX:SharedArchiveFile during CDS dumping\");\n-    }\n-\n-    if (CDSConfig::is_dumping_static_archive()) {\n-      assert(archives == 1, \"must be\");\n-      \/\/ Static dump is simple: only one archive is allowed in SharedArchiveFile. This file\n-      \/\/ will be overwritten no matter regardless of its contents\n-      SharedArchivePath = os::strdup_check_oom(SharedArchiveFile, mtArguments);\n-    } else {\n-      \/\/ SharedArchiveFile may specify one or two files. In case (c), the path for base.jsa\n-      \/\/ is read from top.jsa\n-      \/\/    (a) 1 file:  -XX:SharedArchiveFile=base.jsa\n-      \/\/    (b) 2 files: -XX:SharedArchiveFile=base.jsa:top.jsa\n-      \/\/    (c) 2 files: -XX:SharedArchiveFile=top.jsa\n-      \/\/\n-      \/\/ However, if either RecordDynamicDumpInfo or ArchiveClassesAtExit is used, we do not\n-      \/\/ allow cases (b) and (c). Case (b) is already checked above.\n-\n-      if (archives > 2) {\n-        vm_exit_during_initialization(\n-          \"Cannot have more than 2 archive files specified in the -XX:SharedArchiveFile option\");\n-      }\n-      if (archives == 1) {\n-        char* base_archive_path = nullptr;\n-        bool success =\n-          FileMapInfo::get_base_archive_name_from_header(SharedArchiveFile, &base_archive_path);\n-        if (!success) {\n-          \/\/ If +AutoCreateSharedArchive and the specified shared archive does not exist,\n-          \/\/ regenerate the dynamic archive base on default archive.\n-          if (AutoCreateSharedArchive && !os::file_exists(SharedArchiveFile)) {\n-            CDSConfig::enable_dumping_dynamic_archive();\n-            ArchiveClassesAtExit = const_cast<char *>(SharedArchiveFile);\n-            SharedArchivePath = get_default_shared_archive_path();\n-            SharedArchiveFile = nullptr;\n-          } else {\n-            if (AutoCreateSharedArchive) {\n-              warning(\"-XX:+AutoCreateSharedArchive is unsupported when base CDS archive is not loaded. Run with -Xlog:cds for more info.\");\n-              AutoCreateSharedArchive = false;\n-            }\n-            no_shared_spaces(\"invalid archive\");\n-          }\n-        } else if (base_archive_path == nullptr) {\n-          \/\/ User has specified a single archive, which is a static archive.\n-          SharedArchivePath = const_cast<char *>(SharedArchiveFile);\n-        } else {\n-          \/\/ User has specified a single archive, which is a dynamic archive.\n-          SharedDynamicArchivePath = const_cast<char *>(SharedArchiveFile);\n-          SharedArchivePath = base_archive_path; \/\/ has been c-heap allocated.\n-        }\n-      } else {\n-        extract_shared_archive_paths((const char*)SharedArchiveFile,\n-                                      &SharedArchivePath, &SharedDynamicArchivePath);\n-        if (SharedArchivePath == nullptr) {\n-          assert(SharedDynamicArchivePath == nullptr, \"must be\");\n-          no_shared_spaces(\"invalid archive\");\n-        }\n-      }\n-\n-      if (SharedDynamicArchivePath != nullptr) {\n-        \/\/ Check for case (c)\n-        if (RecordDynamicDumpInfo) {\n-          vm_exit_during_initialization(\"-XX:+RecordDynamicDumpInfo is unsupported when a dynamic CDS archive is specified in -XX:SharedArchiveFile\",\n-                                        SharedArchiveFile);\n-        }\n-        if (ArchiveClassesAtExit != nullptr) {\n-          vm_exit_during_initialization(\"-XX:ArchiveClassesAtExit is unsupported when a dynamic CDS archive is specified in -XX:SharedArchiveFile\",\n-                                        SharedArchiveFile);\n-        }\n-      }\n-\n-      if (ArchiveClassesAtExit != nullptr && os::same_files(SharedArchiveFile, ArchiveClassesAtExit)) {\n-          vm_exit_during_initialization(\n-            \"Cannot have the same archive file specified for -XX:SharedArchiveFile and -XX:ArchiveClassesAtExit\",\n-            SharedArchiveFile);\n-      }\n-    }\n-  }\n-}\n-#endif \/\/ INCLUDE_CDS\n-\n@@ -4014,1 +3707,1 @@\n-  set_shared_spaces_flags_and_archive_paths();\n+  CDSConfig::initialize();\n","filename":"src\/hotspot\/share\/runtime\/arguments.cpp","additions":5,"deletions":312,"binary":false,"changes":317,"status":"modified"},{"patch":"@@ -302,0 +302,3 @@\n+  product(size_t, InlineCacheBufferSize, 10*K, EXPERIMENTAL,                \\\n+          \"InlineCacheBuffer size\")                                         \\\n+                                                                            \\\n@@ -674,2 +677,2 @@\n-          \"Register finalizable objects at end of Object.<init> or \"        \\\n-          \"after allocation\")                                               \\\n+          \"(Deprecated) Register finalizable objects at end of \"            \\\n+          \"Object.<init> or after allocation\")                              \\\n@@ -740,0 +743,4 @@\n+  product(intx, MonitorUnlinkBatch, 500, DIAGNOSTIC,                        \\\n+          \"The maximum number of monitors to unlink in one batch. \")        \\\n+          range(1, max_jint)                                                \\\n+                                                                            \\\n@@ -842,0 +849,7 @@\n+  product(intx, UserThreadWaitAttemptsAtExit, 30,                           \\\n+          \"The number of times to wait for user threads to stop executing \" \\\n+          \"native code during JVM exit. Each wait lasts 10 milliseconds. \"  \\\n+          \"The maximum number of waits is 1000, to wait at most 10 \"        \\\n+          \"seconds.\")                                                       \\\n+          range(0, 1000)                                                    \\\n+                                                                            \\\n@@ -1991,0 +2005,7 @@\n+                                                                            \\\n+  develop(bool, SimulateFullAddressSpace, false,                            \\\n+          \"Simulates a very populated, fragmented address space; no \"       \\\n+          \"targeted reservations will succeed.\")                            \\\n+                                                                            \\\n+  product(bool, ProfileExceptionHandlers, true,                             \\\n+          \"Profile exception handlers\")                                     \\\n","filename":"src\/hotspot\/share\/runtime\/globals.hpp","additions":23,"deletions":2,"binary":false,"changes":25,"status":"modified"},{"patch":"@@ -368,1 +368,0 @@\n-  void      release_object() { _object.release(_oop_storage); }\n","filename":"src\/hotspot\/share\/runtime\/objectMonitor.hpp","additions":0,"deletions":1,"binary":false,"changes":1,"status":"modified"},{"patch":"@@ -66,0 +66,2 @@\n+class ObjectMonitorDeflationLogging;\n+\n@@ -87,5 +89,16 @@\n-\/\/ Walk the in-use list and unlink (at most MonitorDeflationMax) deflated\n-\/\/ ObjectMonitors. Returns the number of unlinked ObjectMonitors.\n-size_t MonitorList::unlink_deflated(Thread* current, LogStream* ls,\n-                                    elapsedTimer* timer_p,\n-                                    GrowableArray<ObjectMonitor*>* unlinked_list) {\n+class ObjectMonitorDeflationSafepointer : public StackObj {\n+  JavaThread* const                    _current;\n+  ObjectMonitorDeflationLogging* const _log;\n+\n+public:\n+  ObjectMonitorDeflationSafepointer(JavaThread* current, ObjectMonitorDeflationLogging* log)\n+    : _current(current), _log(log) {}\n+\n+  void block_for_safepoint(const char* op_name, const char* count_name, size_t counter);\n+};\n+\n+\/\/ Walk the in-use list and unlink deflated ObjectMonitors.\n+\/\/ Returns the number of unlinked ObjectMonitors.\n+size_t MonitorList::unlink_deflated(size_t deflated_count,\n+                                    GrowableArray<ObjectMonitor*>* unlinked_list,\n+                                    ObjectMonitorDeflationSafepointer* safepointer) {\n@@ -94,3 +107,2 @@\n-  ObjectMonitor* head = Atomic::load_acquire(&_head);\n-  ObjectMonitor* m = head;\n-  \/\/ The in-use list head can be null during the final audit.\n+  ObjectMonitor* m = Atomic::load_acquire(&_head);\n+\n@@ -99,1 +111,3 @@\n-      \/\/ Find next live ObjectMonitor.\n+      \/\/ Find next live ObjectMonitor. Batch up the unlinkable monitors, so we can\n+      \/\/ modify the list once per batch. The batch starts at \"m\".\n+      size_t unlinked_batch = 0;\n@@ -101,0 +115,4 @@\n+      \/\/ Look for at most MonitorUnlinkBatch monitors, or the number of\n+      \/\/ deflated and not unlinked monitors, whatever comes first.\n+      assert(deflated_count >= unlinked_count, \"Sanity: underflow\");\n+      size_t unlinked_batch_limit = MIN2<size_t>(deflated_count - unlinked_count, MonitorUnlinkBatch);\n@@ -103,1 +121,1 @@\n-        unlinked_count++;\n+        unlinked_batch++;\n@@ -106,2 +124,8 @@\n-        if (unlinked_count >= (size_t)MonitorDeflationMax) {\n-          \/\/ Reached the max so bail out on the gathering loop.\n+        if (unlinked_batch >= unlinked_batch_limit) {\n+          \/\/ Reached the max batch, so bail out of the gathering loop.\n+          break;\n+        }\n+        if (prev == nullptr && Atomic::load(&_head) != m) {\n+          \/\/ Current batch used to be at head, but it is not at head anymore.\n+          \/\/ Bail out and figure out where we currently are. This avoids long\n+          \/\/ walks searching for new prev during unlink under heavy list inserts.\n@@ -111,0 +135,2 @@\n+\n+      \/\/ Unlink the found batch.\n@@ -112,3 +138,5 @@\n-        ObjectMonitor* prev_head = Atomic::cmpxchg(&_head, head, next);\n-        if (prev_head != head) {\n-          \/\/ Find new prev ObjectMonitor that just got inserted.\n+        \/\/ The current batch is the first batch, so there is a chance that it starts at head.\n+        \/\/ Optimistically assume no inserts happened, and try to unlink the entire batch from the head.\n+        ObjectMonitor* prev_head = Atomic::cmpxchg(&_head, m, next);\n+        if (prev_head != m) {\n+          \/\/ Something must have updated the head. Figure out the actual prev for this batch.\n@@ -118,0 +146,1 @@\n+          assert(prev != nullptr, \"Should have found the prev for the current batch\");\n@@ -121,0 +150,3 @@\n+        \/\/ The current batch is preceded by another batch. This guarantees the current batch\n+        \/\/ does not start at head. Unlink the entire current batch without updating the head.\n+        assert(Atomic::load(&_head) != m, \"Sanity\");\n@@ -123,2 +155,5 @@\n-      if (unlinked_count >= (size_t)MonitorDeflationMax) {\n-        \/\/ Reached the max so bail out on the searching loop.\n+\n+      unlinked_count += unlinked_batch;\n+      if (unlinked_count >= deflated_count) {\n+        \/\/ Reached the max so bail out of the searching loop.\n+        \/\/ There should be no more deflated monitors left.\n@@ -133,5 +168,13 @@\n-    if (current->is_Java_thread()) {\n-      \/\/ A JavaThread must check for a safepoint\/handshake and honor it.\n-      ObjectSynchronizer::chk_for_block_req(JavaThread::cast(current), \"unlinking\",\n-                                            \"unlinked_count\", unlinked_count,\n-                                            ls, timer_p);\n+    \/\/ Must check for a safepoint\/handshake and honor it.\n+    safepointer->block_for_safepoint(\"unlinking\", \"unlinked_count\", unlinked_count);\n+  }\n+\n+#ifdef ASSERT\n+  \/\/ Invariant: the code above should unlink all deflated monitors.\n+  \/\/ The code that runs after this unlinking does not expect deflated monitors.\n+  \/\/ Notably, attempting to deflate the already deflated monitor would break.\n+  {\n+    ObjectMonitor* m = Atomic::load_acquire(&_head);\n+    while (m != nullptr) {\n+      assert(!m->is_being_async_deflated(), \"All deflated monitors should be unlinked\");\n+      m = m->next_om();\n@@ -140,0 +183,2 @@\n+#endif\n+\n@@ -602,7 +647,1 @@\n-  if (LockingMode == LM_LIGHTWEIGHT && monitor->is_owner_anonymous()) {\n-    \/\/ It must be owned by us. Pop lock object from lock stack.\n-    LockStack& lock_stack = current->lock_stack();\n-    oop popped = lock_stack.pop();\n-    assert(popped == object, \"must be owned by this thread\");\n-    monitor->set_owner_from_anonymous(current);\n-  }\n+  assert(!monitor->is_owner_anonymous(), \"must not be\");\n@@ -1086,1 +1125,0 @@\n-      assert(monitor->object_peek() != nullptr, \"Owned monitors should not have a dead object\");\n@@ -1508,29 +1546,0 @@\n-void ObjectSynchronizer::chk_for_block_req(JavaThread* current, const char* op_name,\n-                                           const char* cnt_name, size_t cnt,\n-                                           LogStream* ls, elapsedTimer* timer_p) {\n-  if (!SafepointMechanism::should_process(current)) {\n-    return;\n-  }\n-\n-  \/\/ A safepoint\/handshake has started.\n-  if (ls != nullptr) {\n-    timer_p->stop();\n-    ls->print_cr(\"pausing %s: %s=\" SIZE_FORMAT \", in_use_list stats: ceiling=\"\n-                 SIZE_FORMAT \", count=\" SIZE_FORMAT \", max=\" SIZE_FORMAT,\n-                 op_name, cnt_name, cnt, in_use_list_ceiling(),\n-                 _in_use_list.count(), _in_use_list.max());\n-  }\n-\n-  {\n-    \/\/ Honor block request.\n-    ThreadBlockInVM tbivm(current);\n-  }\n-\n-  if (ls != nullptr) {\n-    ls->print_cr(\"resuming %s: in_use_list stats: ceiling=\" SIZE_FORMAT\n-                 \", count=\" SIZE_FORMAT \", max=\" SIZE_FORMAT, op_name,\n-                 in_use_list_ceiling(), _in_use_list.count(), _in_use_list.max());\n-    timer_p->start();\n-  }\n-}\n-\n@@ -1540,2 +1549,1 @@\n-size_t ObjectSynchronizer::deflate_monitor_list(Thread* current, LogStream* ls,\n-                                                elapsedTimer* timer_p) {\n+size_t ObjectSynchronizer::deflate_monitor_list(ObjectMonitorDeflationSafepointer* safepointer) {\n@@ -1554,5 +1562,2 @@\n-    if (current->is_Java_thread()) {\n-      \/\/ A JavaThread must check for a safepoint\/handshake and honor it.\n-      chk_for_block_req(JavaThread::cast(current), \"deflation\", \"deflated_count\",\n-                        deflated_count, ls, timer_p);\n-    }\n+    \/\/ Must check for a safepoint\/handshake and honor it.\n+    safepointer->block_for_safepoint(\"deflation\", \"deflated_count\", deflated_count);\n@@ -1584,1 +1589,2 @@\n-static size_t delete_monitors(GrowableArray<ObjectMonitor*>* delete_list) {\n+static size_t delete_monitors(GrowableArray<ObjectMonitor*>* delete_list,\n+                              ObjectMonitorDeflationSafepointer* safepointer) {\n@@ -1586,1 +1592,1 @@\n-  size_t count = 0;\n+  size_t deleted_count = 0;\n@@ -1589,1 +1595,3 @@\n-    count++;\n+    deleted_count++;\n+    \/\/ A JavaThread must check for a safepoint\/handshake and honor it.\n+    safepointer->block_for_safepoint(\"deletion\", \"deleted_count\", deleted_count);\n@@ -1591,1 +1599,1 @@\n-  return count;\n+  return deleted_count;\n@@ -1594,8 +1602,36 @@\n-\/\/ This function is called by the MonitorDeflationThread to deflate\n-\/\/ ObjectMonitors.\n-size_t ObjectSynchronizer::deflate_idle_monitors() {\n-  Thread* current = Thread::current();\n-  if (current->is_Java_thread()) {\n-    \/\/ The async deflation request has been processed.\n-    _last_async_deflation_time_ns = os::javaTimeNanos();\n-    set_is_async_deflation_requested(false);\n+class ObjectMonitorDeflationLogging: public StackObj {\n+  LogStreamHandle(Debug, monitorinflation) _debug;\n+  LogStreamHandle(Info, monitorinflation)  _info;\n+  LogStream*                               _stream;\n+  elapsedTimer                             _timer;\n+\n+  size_t ceiling() const { return ObjectSynchronizer::in_use_list_ceiling(); }\n+  size_t count() const   { return ObjectSynchronizer::_in_use_list.count(); }\n+  size_t max() const     { return ObjectSynchronizer::_in_use_list.max(); }\n+\n+public:\n+  ObjectMonitorDeflationLogging()\n+    : _debug(), _info(), _stream(nullptr) {\n+    if (_debug.is_enabled()) {\n+      _stream = &_debug;\n+    } else if (_info.is_enabled()) {\n+      _stream = &_info;\n+    }\n+  }\n+\n+  void begin() {\n+    if (_stream != nullptr) {\n+      _stream->print_cr(\"begin deflating: in_use_list stats: ceiling=\" SIZE_FORMAT \", count=\" SIZE_FORMAT \", max=\" SIZE_FORMAT,\n+                        ceiling(), count(), max());\n+      _timer.start();\n+    }\n+  }\n+\n+  void before_handshake(size_t unlinked_count) {\n+    if (_stream != nullptr) {\n+      _timer.stop();\n+      _stream->print_cr(\"before handshaking: unlinked_count=\" SIZE_FORMAT\n+                        \", in_use_list stats: ceiling=\" SIZE_FORMAT \", count=\"\n+                        SIZE_FORMAT \", max=\" SIZE_FORMAT,\n+                        unlinked_count, ceiling(), count(), max());\n+    }\n@@ -1604,7 +1640,28 @@\n-  LogStreamHandle(Debug, monitorinflation) lsh_debug;\n-  LogStreamHandle(Info, monitorinflation) lsh_info;\n-  LogStream* ls = nullptr;\n-  if (log_is_enabled(Debug, monitorinflation)) {\n-    ls = &lsh_debug;\n-  } else if (log_is_enabled(Info, monitorinflation)) {\n-    ls = &lsh_info;\n+  void after_handshake() {\n+    if (_stream != nullptr) {\n+      _stream->print_cr(\"after handshaking: in_use_list stats: ceiling=\"\n+                        SIZE_FORMAT \", count=\" SIZE_FORMAT \", max=\" SIZE_FORMAT,\n+                        ceiling(), count(), max());\n+      _timer.start();\n+    }\n+  }\n+\n+  void end(size_t deflated_count, size_t unlinked_count) {\n+    if (_stream != nullptr) {\n+      _timer.stop();\n+      if (deflated_count != 0 || unlinked_count != 0 || _debug.is_enabled()) {\n+        _stream->print_cr(\"deflated_count=\" SIZE_FORMAT \", {unlinked,deleted}_count=\" SIZE_FORMAT \" monitors in %3.7f secs\",\n+                          deflated_count, unlinked_count, _timer.seconds());\n+      }\n+      _stream->print_cr(\"end deflating: in_use_list stats: ceiling=\" SIZE_FORMAT \", count=\" SIZE_FORMAT \", max=\" SIZE_FORMAT,\n+                        ceiling(), count(), max());\n+    }\n+  }\n+\n+  void before_block_for_safepoint(const char* op_name, const char* cnt_name, size_t cnt) {\n+    if (_stream != nullptr) {\n+      _timer.stop();\n+      _stream->print_cr(\"pausing %s: %s=\" SIZE_FORMAT \", in_use_list stats: ceiling=\"\n+                        SIZE_FORMAT \", count=\" SIZE_FORMAT \", max=\" SIZE_FORMAT,\n+                        op_name, cnt_name, cnt, ceiling(), count(), max());\n+    }\n@@ -1613,5 +1670,7 @@\n-  elapsedTimer timer;\n-  if (ls != nullptr) {\n-    ls->print_cr(\"begin deflating: in_use_list stats: ceiling=\" SIZE_FORMAT \", count=\" SIZE_FORMAT \", max=\" SIZE_FORMAT,\n-                 in_use_list_ceiling(), _in_use_list.count(), _in_use_list.max());\n-    timer.start();\n+  void after_block_for_safepoint(const char* op_name) {\n+    if (_stream != nullptr) {\n+      _stream->print_cr(\"resuming %s: in_use_list stats: ceiling=\" SIZE_FORMAT\n+                        \", count=\" SIZE_FORMAT \", max=\" SIZE_FORMAT, op_name,\n+                        ceiling(), count(), max());\n+      _timer.start();\n+    }\n@@ -1619,0 +1678,32 @@\n+};\n+\n+void ObjectMonitorDeflationSafepointer::block_for_safepoint(const char* op_name, const char* count_name, size_t counter) {\n+  if (!SafepointMechanism::should_process(_current)) {\n+    return;\n+  }\n+\n+  \/\/ A safepoint\/handshake has started.\n+  _log->before_block_for_safepoint(op_name, count_name, counter);\n+\n+  {\n+    \/\/ Honor block request.\n+    ThreadBlockInVM tbivm(_current);\n+  }\n+\n+  _log->after_block_for_safepoint(op_name);\n+}\n+\n+\/\/ This function is called by the MonitorDeflationThread to deflate\n+\/\/ ObjectMonitors.\n+size_t ObjectSynchronizer::deflate_idle_monitors() {\n+  JavaThread* current = JavaThread::current();\n+  assert(current->is_monitor_deflation_thread(), \"The only monitor deflater\");\n+\n+  \/\/ The async deflation request has been processed.\n+  _last_async_deflation_time_ns = os::javaTimeNanos();\n+  set_is_async_deflation_requested(false);\n+\n+  ObjectMonitorDeflationLogging log;\n+  ObjectMonitorDeflationSafepointer safepointer(current, &log);\n+\n+  log.begin();\n@@ -1621,1 +1712,3 @@\n-  size_t deflated_count = deflate_monitor_list(current, ls, &timer);\n+  size_t deflated_count = deflate_monitor_list(&safepointer);\n+\n+  \/\/ Unlink the deflated ObjectMonitors from the in-use list.\n@@ -1625,4 +1718,1 @@\n-    \/\/ There are ObjectMonitors that have been deflated.\n-\n-    \/\/ Unlink deflated ObjectMonitors from the in-use list.\n-    ResourceMark rm;\n+    ResourceMark rm(current);\n@@ -1630,10 +1720,1 @@\n-    unlinked_count = _in_use_list.unlink_deflated(current, ls, &timer, &delete_list);\n-    if (current->is_monitor_deflation_thread()) {\n-      if (ls != nullptr) {\n-        timer.stop();\n-        ls->print_cr(\"before handshaking: unlinked_count=\" SIZE_FORMAT\n-                     \", in_use_list stats: ceiling=\" SIZE_FORMAT \", count=\"\n-                     SIZE_FORMAT \", max=\" SIZE_FORMAT,\n-                     unlinked_count, in_use_list_ceiling(),\n-                     _in_use_list.count(), _in_use_list.max());\n-      }\n+    unlinked_count = _in_use_list.unlink_deflated(deflated_count, &delete_list, &safepointer);\n@@ -1641,21 +1722,13 @@\n-      \/\/ A JavaThread needs to handshake in order to safely free the\n-      \/\/ ObjectMonitors that were deflated in this cycle.\n-      HandshakeForDeflation hfd_hc;\n-      Handshake::execute(&hfd_hc);\n-      \/\/ Also, we sync and desync GC threads around the handshake, so that they can\n-      \/\/ safely read the mark-word and look-through to the object-monitor, without\n-      \/\/ being afraid that the object-monitor is going away.\n-      VM_RendezvousGCThreads sync_gc;\n-      VMThread::execute(&sync_gc);\n-\n-      if (ls != nullptr) {\n-        ls->print_cr(\"after handshaking: in_use_list stats: ceiling=\"\n-                     SIZE_FORMAT \", count=\" SIZE_FORMAT \", max=\" SIZE_FORMAT,\n-                     in_use_list_ceiling(), _in_use_list.count(), _in_use_list.max());\n-        timer.start();\n-      }\n-    } else {\n-      \/\/ This is not a monitor deflation thread.\n-      \/\/ No handshake or rendezvous is needed when we are already at safepoint.\n-      assert_at_safepoint();\n-    }\n+    log.before_handshake(unlinked_count);\n+\n+    \/\/ A JavaThread needs to handshake in order to safely free the\n+    \/\/ ObjectMonitors that were deflated in this cycle.\n+    HandshakeForDeflation hfd_hc;\n+    Handshake::execute(&hfd_hc);\n+    \/\/ Also, we sync and desync GC threads around the handshake, so that they can\n+    \/\/ safely read the mark-word and look-through to the object-monitor, without\n+    \/\/ being afraid that the object-monitor is going away.\n+    VM_RendezvousGCThreads sync_gc;\n+    VMThread::execute(&sync_gc);\n+\n+    log.after_handshake();\n@@ -1665,24 +1738,3 @@\n-    if (current->is_Java_thread()) {\n-      if (ls != NULL) {\n-        timer.stop();\n-        ls->print_cr(\"before setting blocked: unlinked_count=\" SIZE_FORMAT\n-                     \", in_use_list stats: ceiling=\" SIZE_FORMAT \", count=\"\n-                     SIZE_FORMAT \", max=\" SIZE_FORMAT,\n-                     unlinked_count, in_use_list_ceiling(),\n-                     _in_use_list.count(), _in_use_list.max());\n-      }\n-      \/\/ Mark the calling JavaThread blocked (safepoint safe) while we free\n-      \/\/ the ObjectMonitors so we don't delay safepoints whilst doing that.\n-      ThreadBlockInVM tbivm(JavaThread::cast(current));\n-      if (ls != NULL) {\n-        ls->print_cr(\"after setting blocked: in_use_list stats: ceiling=\"\n-                     SIZE_FORMAT \", count=\" SIZE_FORMAT \", max=\" SIZE_FORMAT,\n-                     in_use_list_ceiling(), _in_use_list.count(), _in_use_list.max());\n-        timer.start();\n-      }\n-      deleted_count = delete_monitors(&delete_list);\n-      \/\/ ThreadBlockInVM is destroyed here\n-    } else {\n-      \/\/ A non-JavaThread can just free the ObjectMonitors:\n-      deleted_count = delete_monitors(&delete_list);\n-    }\n+\n+    \/\/ Delete the unlinked ObjectMonitors.\n+    deleted_count = delete_monitors(&delete_list, &safepointer);\n@@ -1692,9 +1744,1 @@\n-  if (ls != nullptr) {\n-    timer.stop();\n-    if (deflated_count != 0 || unlinked_count != 0 || log_is_enabled(Debug, monitorinflation)) {\n-      ls->print_cr(\"deflated_count=\" SIZE_FORMAT \", {unlinked,deleted}_count=\" SIZE_FORMAT \" monitors in %3.7f secs\",\n-                   deflated_count, unlinked_count, timer.seconds());\n-    }\n-    ls->print_cr(\"end deflating: in_use_list stats: ceiling=\" SIZE_FORMAT \", count=\" SIZE_FORMAT \", max=\" SIZE_FORMAT,\n-                 in_use_list_ceiling(), _in_use_list.count(), _in_use_list.max());\n-  }\n+  log.end(deflated_count, unlinked_count);\n","filename":"src\/hotspot\/share\/runtime\/synchronizer.cpp","additions":203,"deletions":159,"binary":false,"changes":362,"status":"modified"},{"patch":"@@ -1121,0 +1121,1 @@\n+  declare_unsigned_integer_type(volatile uint)                            \\\n","filename":"src\/hotspot\/share\/runtime\/vmStructs.cpp","additions":1,"deletions":0,"binary":false,"changes":1,"status":"modified"}]}