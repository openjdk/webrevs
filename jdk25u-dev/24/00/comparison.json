{"files":[{"patch":"@@ -0,0 +1,649 @@\n+\/*\n+ * Copyright (c) 1997, 2025, Oracle and\/or its affiliates. All rights reserved.\n+ * Copyright (c) 2012, 2025 SAP SE. All rights reserved.\n+ * DO NOT ALTER OR REMOVE COPYRIGHT NOTICES OR THIS FILE HEADER.\n+ *\n+ * This code is free software; you can redistribute it and\/or modify it\n+ * under the terms of the GNU General Public License version 2 only, as\n+ * published by the Free Software Foundation.\n+ *\n+ * This code is distributed in the hope that it will be useful, but WITHOUT\n+ * ANY WARRANTY; without even the implied warranty of MERCHANTABILITY or\n+ * FITNESS FOR A PARTICULAR PURPOSE.  See the GNU General Public License\n+ * version 2 for more details (a copy is included in the LICENSE file that\n+ * accompanied this code).\n+ *\n+ * You should have received a copy of the GNU General Public License version\n+ * 2 along with this work; if not, write to the Free Software Foundation,\n+ * Inc., 51 Franklin St, Fifth Floor, Boston, MA 02110-1301 USA.\n+ *\n+ * Please contact Oracle, 500 Oracle Parkway, Redwood Shores, CA 94065 USA\n+ * or visit www.oracle.com if you need additional information or have any\n+ * questions.\n+ *\n+ *\/\n+\n+#ifndef CPU_PPC_ATOMICACCESS_PPC_HPP\n+#define CPU_PPC_ATOMICACCESS_PPC_HPP\n+\n+#ifndef PPC64\n+#error \"Atomic currently only implemented for PPC64\"\n+#endif\n+\n+#include \"orderAccess_ppc.hpp\"\n+#include \"utilities\/debug.hpp\"\n+\n+\/\/ Implementation of class AtomicAccess\n+\n+\/\/\n+\/\/ machine barrier instructions:\n+\/\/\n+\/\/ - sync            two-way memory barrier, aka fence\n+\/\/ - lwsync          orders  Store|Store,\n+\/\/                            Load|Store,\n+\/\/                            Load|Load,\n+\/\/                   but not Store|Load\n+\/\/ - eieio           orders memory accesses for device memory (only)\n+\/\/ - isync           invalidates speculatively executed instructions\n+\/\/                   From the POWER ISA 2.06 documentation:\n+\/\/                    \"[...] an isync instruction prevents the execution of\n+\/\/                   instructions following the isync until instructions\n+\/\/                   preceding the isync have completed, [...]\"\n+\/\/                   From IBM's AIX assembler reference:\n+\/\/                    \"The isync [...] instructions causes the processor to\n+\/\/                   refetch any instructions that might have been fetched\n+\/\/                   prior to the isync instruction. The instruction isync\n+\/\/                   causes the processor to wait for all previous instructions\n+\/\/                   to complete. Then any instructions already fetched are\n+\/\/                   discarded and instruction processing continues in the\n+\/\/                   environment established by the previous instructions.\"\n+\/\/\n+\/\/ semantic barrier instructions:\n+\/\/ (as defined in orderAccess.hpp)\n+\/\/\n+\/\/ - release         orders Store|Store,       (maps to lwsync)\n+\/\/                           Load|Store\n+\/\/ - acquire         orders  Load|Store,       (maps to lwsync)\n+\/\/                           Load|Load\n+\/\/ - fence           orders Store|Store,       (maps to sync)\n+\/\/                           Load|Store,\n+\/\/                           Load|Load,\n+\/\/                          Store|Load\n+\/\/\n+\n+inline void pre_membar(atomic_memory_order order) {\n+  switch (order) {\n+    case memory_order_relaxed:\n+    case memory_order_acquire: break;\n+    case memory_order_release:\n+    case memory_order_acq_rel: __asm__ __volatile__ (\"lwsync\" : : : \"memory\"); break;\n+    default \/*conservative*\/ : __asm__ __volatile__ (\"sync\"   : : : \"memory\"); break;\n+  }\n+}\n+\n+inline void post_membar(atomic_memory_order order) {\n+  switch (order) {\n+    case memory_order_relaxed:\n+    case memory_order_release: break;\n+    case memory_order_acquire:\n+    case memory_order_acq_rel: __asm__ __volatile__ (\"isync\"  : : : \"memory\"); break;\n+    default \/*conservative*\/ : __asm__ __volatile__ (\"sync\"   : : : \"memory\"); break;\n+  }\n+}\n+\n+\n+\n+template<size_t byte_size>\n+struct AtomicAccess::PlatformAdd {\n+  template<typename D, typename I>\n+  D add_then_fetch(D volatile* dest, I add_value, atomic_memory_order order) const;\n+\n+  template<typename D, typename I>\n+  D fetch_then_add(D volatile* dest, I add_value, atomic_memory_order order) const {\n+    return add_then_fetch(dest, add_value, order) - add_value;\n+  }\n+};\n+\n+template<>\n+template<typename D, typename I>\n+inline D AtomicAccess::PlatformAdd<4>::add_then_fetch(D volatile* dest, I add_value,\n+                                                      atomic_memory_order order) const {\n+  STATIC_ASSERT(4 == sizeof(I));\n+  STATIC_ASSERT(4 == sizeof(D));\n+\n+  D result;\n+\n+  pre_membar(order);\n+\n+  __asm__ __volatile__ (\n+    \"1: lwarx   %[result], 0, %[dest]                 \\n\"\n+    \"   add     %[result], %[result], %[add_value]    \\n\"\n+    \"   stwcx.  %[result], 0, %[dest]                 \\n\"\n+    \"   bne-    1b                                    \\n\"\n+    : [result]     \"=&r\"  (result)\n+    : [add_value]  \"r\"    (add_value),\n+      [dest]       \"b\"    (dest)\n+    : \"cc\", \"memory\" );\n+\n+  post_membar(order);\n+\n+  return result;\n+}\n+\n+\n+template<>\n+template<typename D, typename I>\n+inline D AtomicAccess::PlatformAdd<8>::add_then_fetch(D volatile* dest, I add_value,\n+                                                      atomic_memory_order order) const {\n+  STATIC_ASSERT(8 == sizeof(I));\n+  STATIC_ASSERT(8 == sizeof(D));\n+\n+  D result;\n+\n+  pre_membar(order);\n+\n+  __asm__ __volatile__ (\n+    \"1: ldarx   %[result], 0, %[dest]                 \\n\"\n+    \"   add     %[result], %[result], %[add_value]    \\n\"\n+    \"   stdcx.  %[result], 0, %[dest]                 \\n\"\n+    \"   bne-    1b                                    \\n\"\n+    : [result]     \"=&r\"  (result)\n+    : [add_value]  \"r\"    (add_value),\n+      [dest]       \"b\"    (dest)\n+    : \"cc\", \"memory\" );\n+\n+  post_membar(order);\n+\n+  return result;\n+}\n+\n+template<>\n+template<typename T>\n+inline T AtomicAccess::PlatformXchg<4>::operator()(T volatile* dest,\n+                                                   T exchange_value,\n+                                                   atomic_memory_order order) const {\n+  STATIC_ASSERT(4 == sizeof(T));\n+  \/\/ Note that xchg doesn't necessarily do an acquire\n+  \/\/ (see synchronizer.cpp).\n+\n+  T old_value;\n+\n+  pre_membar(order);\n+\n+  __asm__ __volatile__ (\n+    \/* atomic loop *\/\n+    \"1:                                                 \\n\"\n+    \"   lwarx   %[old_value], 0, %[dest]                \\n\"\n+    \"   stwcx.  %[exchange_value], 0, %[dest]           \\n\"\n+    \"   bne-    1b                                      \\n\"\n+    \/* exit *\/\n+    \"2:                                                 \\n\"\n+    \/* out *\/\n+    : [old_value]       \"=&r\"   (old_value)\n+    \/* in *\/\n+    : [dest]            \"b\"     (dest),\n+      [exchange_value]  \"r\"     (exchange_value)\n+    \/* clobber *\/\n+    : \"cc\",\n+      \"memory\"\n+    );\n+\n+  post_membar(order);\n+\n+  return old_value;\n+}\n+\n+template<>\n+template<typename T>\n+inline T AtomicAccess::PlatformXchg<8>::operator()(T volatile* dest,\n+                                                   T exchange_value,\n+                                                   atomic_memory_order order) const {\n+  STATIC_ASSERT(8 == sizeof(T));\n+  \/\/ Note that xchg doesn't necessarily do an acquire\n+  \/\/ (see synchronizer.cpp).\n+\n+  T old_value;\n+\n+  pre_membar(order);\n+\n+  __asm__ __volatile__ (\n+    \/* atomic loop *\/\n+    \"1:                                                 \\n\"\n+    \"   ldarx   %[old_value], 0, %[dest]                \\n\"\n+    \"   stdcx.  %[exchange_value], 0, %[dest]           \\n\"\n+    \"   bne-    1b                                      \\n\"\n+    \/* exit *\/\n+    \"2:                                                 \\n\"\n+    \/* out *\/\n+    : [old_value]       \"=&r\"   (old_value)\n+    \/* in *\/\n+    : [dest]            \"b\"     (dest),\n+      [exchange_value]  \"r\"     (exchange_value)\n+    \/* clobber *\/\n+    : \"cc\",\n+      \"memory\"\n+    );\n+\n+  post_membar(order);\n+\n+  return old_value;\n+}\n+\n+template<>\n+template<typename T>\n+inline T AtomicAccess::PlatformCmpxchg<1>::operator()(T volatile* dest,\n+                                                      T compare_value,\n+                                                      T exchange_value,\n+                                                      atomic_memory_order order) const {\n+  STATIC_ASSERT(1 == sizeof(T));\n+\n+  \/\/ Note that cmpxchg guarantees a two-way memory barrier across\n+  \/\/ the cmpxchg, so it's really a 'fence_cmpxchg_fence' if not\n+  \/\/ specified otherwise (see atomicAccess.hpp).\n+\n+  const unsigned int masked_compare_val = (unsigned int)(unsigned char)compare_value;\n+\n+  unsigned int old_value;\n+\n+  pre_membar(order);\n+\n+  __asm__ __volatile__ (\n+    \/* simple guard *\/\n+    \"   lbz     %[old_value], 0(%[dest])                  \\n\"\n+    \"   cmpw    %[masked_compare_val], %[old_value]       \\n\"\n+    \"   bne-    2f                                        \\n\"\n+    \/* atomic loop *\/\n+    \"1:                                                   \\n\"\n+    \"   lbarx   %[old_value], 0, %[dest]                  \\n\"\n+    \"   cmpw    %[masked_compare_val], %[old_value]       \\n\"\n+    \"   bne-    2f                                        \\n\"\n+    \"   stbcx.  %[exchange_value], 0, %[dest]             \\n\"\n+    \"   bne-    1b                                        \\n\"\n+    \/* exit *\/\n+    \"2:                                                   \\n\"\n+    \/* out *\/\n+    : [old_value]       \"=&r\"   (old_value)\n+    \/* in *\/\n+    : [dest]                   \"b\"     (dest),\n+      [masked_compare_val]     \"r\"     (masked_compare_val),\n+      [exchange_value]         \"r\"     (exchange_value)\n+    \/* clobber *\/\n+    : \"cc\",\n+      \"memory\"\n+    );\n+\n+  post_membar(order);\n+\n+  return PrimitiveConversions::cast<T>((unsigned char)old_value);\n+}\n+\n+template<>\n+template<typename T>\n+inline T AtomicAccess::PlatformCmpxchg<4>::operator()(T volatile* dest,\n+                                                      T compare_value,\n+                                                      T exchange_value,\n+                                                      atomic_memory_order order) const {\n+  STATIC_ASSERT(4 == sizeof(T));\n+\n+  \/\/ Note that cmpxchg guarantees a two-way memory barrier across\n+  \/\/ the cmpxchg, so it's really a 'fence_cmpxchg_fence' if not\n+  \/\/ specified otherwise (see atomicAccess.hpp).\n+\n+  T old_value;\n+\n+  pre_membar(order);\n+\n+  __asm__ __volatile__ (\n+    \/* simple guard *\/\n+    \"   lwz     %[old_value], 0(%[dest])                \\n\"\n+    \"   cmpw    %[compare_value], %[old_value]          \\n\"\n+    \"   bne-    2f                                      \\n\"\n+    \/* atomic loop *\/\n+    \"1:                                                 \\n\"\n+    \"   lwarx   %[old_value], 0, %[dest]                \\n\"\n+    \"   cmpw    %[compare_value], %[old_value]          \\n\"\n+    \"   bne-    2f                                      \\n\"\n+    \"   stwcx.  %[exchange_value], 0, %[dest]           \\n\"\n+    \"   bne-    1b                                      \\n\"\n+    \/* exit *\/\n+    \"2:                                                 \\n\"\n+    \/* out *\/\n+    : [old_value]       \"=&r\"   (old_value)\n+    \/* in *\/\n+    : [dest]            \"b\"     (dest),\n+      [compare_value]   \"r\"     (compare_value),\n+      [exchange_value]  \"r\"     (exchange_value)\n+    \/* clobber *\/\n+    : \"cc\",\n+      \"memory\"\n+    );\n+\n+  post_membar(order);\n+\n+  return old_value;\n+}\n+\n+template<>\n+template<typename T>\n+inline T AtomicAccess::PlatformCmpxchg<8>::operator()(T volatile* dest,\n+                                                      T compare_value,\n+                                                      T exchange_value,\n+                                                      atomic_memory_order order) const {\n+  STATIC_ASSERT(8 == sizeof(T));\n+\n+  \/\/ Note that cmpxchg guarantees a two-way memory barrier across\n+  \/\/ the cmpxchg, so it's really a 'fence_cmpxchg_fence' if not\n+  \/\/ specified otherwise (see atomicAccess.hpp).\n+\n+  T old_value;\n+\n+  pre_membar(order);\n+\n+  __asm__ __volatile__ (\n+    \/* simple guard *\/\n+    \"   ld      %[old_value], 0(%[dest])                \\n\"\n+    \"   cmpd    %[compare_value], %[old_value]          \\n\"\n+    \"   bne-    2f                                      \\n\"\n+    \/* atomic loop *\/\n+    \"1:                                                 \\n\"\n+    \"   ldarx   %[old_value], 0, %[dest]                \\n\"\n+    \"   cmpd    %[compare_value], %[old_value]          \\n\"\n+    \"   bne-    2f                                      \\n\"\n+    \"   stdcx.  %[exchange_value], 0, %[dest]           \\n\"\n+    \"   bne-    1b                                      \\n\"\n+    \/* exit *\/\n+    \"2:                                                 \\n\"\n+    \/* out *\/\n+    : [old_value]       \"=&r\"   (old_value)\n+    \/* in *\/\n+    : [dest]            \"b\"     (dest),\n+      [compare_value]   \"r\"     (compare_value),\n+      [exchange_value]  \"r\"     (exchange_value)\n+    \/* clobber *\/\n+    : \"cc\",\n+      \"memory\"\n+    );\n+\n+  post_membar(order);\n+\n+  return old_value;\n+}\n+\n+template<size_t byte_size>\n+struct AtomicAccess::PlatformOrderedLoad<byte_size, X_ACQUIRE>\n+{\n+  template <typename T>\n+  T operator()(const volatile T* p) const {\n+    T t = AtomicAccess::load(p);\n+    \/\/ Use twi-isync for load_acquire (faster than lwsync).\n+    __asm__ __volatile__ (\"twi 0,%0,0\\n isync\\n\" : : \"r\" (t) : \"memory\");\n+    return t;\n+  }\n+};\n+\n+template<>\n+class AtomicAccess::PlatformBitops<4, true> {\n+public:\n+  template<typename T>\n+  T fetch_then_and(T volatile* dest, T bits, atomic_memory_order order) const {\n+    STATIC_ASSERT(4 == sizeof(T));\n+    T old_value, result;\n+\n+    pre_membar(order);\n+\n+    __asm__ __volatile__ (\n+      \"1: lwarx   %[old_value], 0, %[dest]            \\n\"\n+      \"   and     %[result], %[old_value], %[bits]    \\n\"\n+      \"   stwcx.  %[result], 0, %[dest]               \\n\"\n+      \"   bne-    1b                                  \\n\"\n+      : [old_value]  \"=&r\"  (old_value),\n+        [result]     \"=&r\"  (result)\n+      : [dest]       \"b\"    (dest),\n+        [bits]       \"r\"    (bits)\n+      : \"cc\", \"memory\" );\n+\n+    post_membar(order);\n+    return old_value;\n+  }\n+\n+  template<typename T>\n+  T fetch_then_or(T volatile* dest, T bits, atomic_memory_order order) const {\n+    STATIC_ASSERT(4 == sizeof(T));\n+    T old_value, result;\n+\n+    pre_membar(order);\n+\n+    __asm__ __volatile__ (\n+      \"1: lwarx   %[old_value], 0, %[dest]            \\n\"\n+      \"   or      %[result], %[old_value], %[bits]    \\n\"\n+      \"   stwcx.  %[result], 0, %[dest]               \\n\"\n+      \"   bne-    1b                                  \\n\"\n+      : [old_value]  \"=&r\"  (old_value),\n+        [result]     \"=&r\"  (result)\n+      : [dest]       \"b\"    (dest),\n+        [bits]       \"r\"    (bits)\n+      : \"cc\", \"memory\" );\n+\n+    post_membar(order);\n+    return old_value;\n+  }\n+\n+  template<typename T>\n+  T fetch_then_xor(T volatile* dest, T bits, atomic_memory_order order) const {\n+    STATIC_ASSERT(4 == sizeof(T));\n+    T old_value, result;\n+\n+    pre_membar(order);\n+\n+    __asm__ __volatile__ (\n+      \"1: lwarx   %[old_value], 0, %[dest]            \\n\"\n+      \"   xor     %[result], %[old_value], %[bits]    \\n\"\n+      \"   stwcx.  %[result], 0, %[dest]               \\n\"\n+      \"   bne-    1b                                  \\n\"\n+      : [old_value]  \"=&r\"  (old_value),\n+        [result]     \"=&r\"  (result)\n+      : [dest]       \"b\"    (dest),\n+        [bits]       \"r\"    (bits)\n+      : \"cc\", \"memory\" );\n+\n+    post_membar(order);\n+    return old_value;\n+  }\n+\n+  template<typename T>\n+  T and_then_fetch(T volatile* dest, T bits, atomic_memory_order order) const {\n+    STATIC_ASSERT(4 == sizeof(T));\n+    T result;\n+\n+    pre_membar(order);\n+\n+    __asm__ __volatile__ (\n+      \"1: lwarx   %[result], 0, %[dest]            \\n\"\n+      \"   and     %[result], %[result], %[bits]    \\n\"\n+      \"   stwcx.  %[result], 0, %[dest]            \\n\"\n+      \"   bne-    1b                               \\n\"\n+      : [result]  \"=&r\"  (result)\n+      : [dest]    \"b\"    (dest),\n+        [bits]    \"r\"    (bits)\n+      : \"cc\", \"memory\" );\n+\n+    post_membar(order);\n+    return result;\n+  }\n+\n+  template<typename T>\n+  T or_then_fetch(T volatile* dest, T bits, atomic_memory_order order) const {\n+    STATIC_ASSERT(4 == sizeof(T));\n+    T result;\n+\n+    pre_membar(order);\n+\n+    __asm__ __volatile__ (\n+      \"1: lwarx   %[result], 0, %[dest]            \\n\"\n+      \"   or      %[result], %[result], %[bits]    \\n\"\n+      \"   stwcx.  %[result], 0, %[dest]            \\n\"\n+      \"   bne-    1b                               \\n\"\n+      : [result]  \"=&r\"  (result)\n+      : [dest]    \"b\"    (dest),\n+        [bits]    \"r\"    (bits)\n+      : \"cc\", \"memory\" );\n+\n+    post_membar(order);\n+    return result;\n+  }\n+\n+  template<typename T>\n+  T xor_then_fetch(T volatile* dest, T bits, atomic_memory_order order) const {\n+    STATIC_ASSERT(4 == sizeof(T));\n+    T result;\n+\n+    pre_membar(order);\n+\n+    __asm__ __volatile__ (\n+      \"1: lwarx   %[result], 0, %[dest]            \\n\"\n+      \"   xor     %[result], %[result], %[bits]    \\n\"\n+      \"   stwcx.  %[result], 0, %[dest]            \\n\"\n+      \"   bne-    1b                               \\n\"\n+      : [result]  \"=&r\"  (result)\n+      : [dest]    \"b\"    (dest),\n+        [bits]    \"r\"    (bits)\n+      : \"cc\", \"memory\" );\n+\n+    post_membar(order);\n+    return result;\n+  }\n+};\n+\n+template<>\n+class AtomicAccess::PlatformBitops<8, true> {\n+public:\n+  template<typename T>\n+  T fetch_then_and(T volatile* dest, T bits, atomic_memory_order order) const {\n+    STATIC_ASSERT(8 == sizeof(T));\n+    T old_value, result;\n+\n+    pre_membar(order);\n+\n+    __asm__ __volatile__ (\n+      \"1: ldarx   %[old_value], 0, %[dest]            \\n\"\n+      \"   and     %[result], %[old_value], %[bits]    \\n\"\n+      \"   stdcx.  %[result], 0, %[dest]               \\n\"\n+      \"   bne-    1b                                  \\n\"\n+      : [old_value]  \"=&r\"  (old_value),\n+        [result]     \"=&r\"  (result)\n+      : [dest]       \"b\"    (dest),\n+        [bits]       \"r\"    (bits)\n+      : \"cc\", \"memory\" );\n+\n+    post_membar(order);\n+    return old_value;\n+  }\n+\n+  template<typename T>\n+  T fetch_then_or(T volatile* dest, T bits, atomic_memory_order order) const {\n+    STATIC_ASSERT(8 == sizeof(T));\n+    T old_value, result;\n+\n+    pre_membar(order);\n+\n+    __asm__ __volatile__ (\n+      \"1: ldarx   %[old_value], 0, %[dest]            \\n\"\n+      \"   or      %[result], %[old_value], %[bits]    \\n\"\n+      \"   stdcx.  %[result], 0, %[dest]               \\n\"\n+      \"   bne-    1b                                  \\n\"\n+      : [old_value]  \"=&r\"  (old_value),\n+        [result]     \"=&r\"  (result)\n+      : [dest]       \"b\"    (dest),\n+        [bits]       \"r\"    (bits)\n+      : \"cc\", \"memory\" );\n+\n+    post_membar(order);\n+    return old_value;\n+  }\n+\n+  template<typename T>\n+  T fetch_then_xor(T volatile* dest, T bits, atomic_memory_order order) const {\n+    STATIC_ASSERT(8 == sizeof(T));\n+    T old_value, result;\n+\n+    pre_membar(order);\n+\n+    __asm__ __volatile__ (\n+      \"1: ldarx   %[old_value], 0, %[dest]            \\n\"\n+      \"   xor     %[result], %[old_value], %[bits]    \\n\"\n+      \"   stdcx.  %[result], 0, %[dest]               \\n\"\n+      \"   bne-    1b                                  \\n\"\n+      : [old_value]  \"=&r\"  (old_value),\n+        [result]     \"=&r\"  (result)\n+      : [dest]       \"b\"    (dest),\n+        [bits]       \"r\"    (bits)\n+      : \"cc\", \"memory\" );\n+\n+    post_membar(order);\n+    return old_value;\n+  }\n+\n+  template<typename T>\n+  T and_then_fetch(T volatile* dest, T bits, atomic_memory_order order) const {\n+    STATIC_ASSERT(8 == sizeof(T));\n+    T result;\n+\n+    pre_membar(order);\n+\n+    __asm__ __volatile__ (\n+      \"1: ldarx   %[result], 0, %[dest]            \\n\"\n+      \"   and     %[result], %[result], %[bits]    \\n\"\n+      \"   stdcx.  %[result], 0, %[dest]            \\n\"\n+      \"   bne-    1b                               \\n\"\n+      : [result]     \"=&r\"  (result)\n+      : [dest]       \"b\"    (dest),\n+        [bits]       \"r\"    (bits)\n+      : \"cc\", \"memory\" );\n+\n+    post_membar(order);\n+    return result;\n+  }\n+\n+  template<typename T>\n+  T or_then_fetch(T volatile* dest, T bits, atomic_memory_order order) const {\n+    STATIC_ASSERT(8 == sizeof(T));\n+    T result;\n+\n+    pre_membar(order);\n+\n+    __asm__ __volatile__ (\n+      \"1: ldarx   %[result], 0, %[dest]            \\n\"\n+      \"   or      %[result], %[result], %[bits]    \\n\"\n+      \"   stdcx.  %[result], 0, %[dest]            \\n\"\n+      \"   bne-    1b                               \\n\"\n+      : [result]     \"=&r\"  (result)\n+      : [dest]       \"b\"    (dest),\n+        [bits]       \"r\"    (bits)\n+      : \"cc\", \"memory\" );\n+\n+    post_membar(order);\n+    return result;\n+  }\n+\n+  template<typename T>\n+  T xor_then_fetch(T volatile* dest, T bits, atomic_memory_order order) const {\n+    STATIC_ASSERT(8 == sizeof(T));\n+    T result;\n+\n+    pre_membar(order);\n+\n+    __asm__ __volatile__ (\n+      \"1: ldarx   %[result], 0, %[dest]            \\n\"\n+      \"   xor     %[result], %[result], %[bits]    \\n\"\n+      \"   stdcx.  %[result], 0, %[dest]            \\n\"\n+      \"   bne-    1b                               \\n\"\n+      : [result]     \"=&r\"  (result)\n+      : [dest]       \"b\"    (dest),\n+        [bits]       \"r\"    (bits)\n+      : \"cc\", \"memory\" );\n+\n+    post_membar(order);\n+    return result;\n+  }\n+};\n+#endif \/\/ CPU_PPC_ATOMICACCESS_PPC_HPP\n","filename":"src\/hotspot\/cpu\/ppc\/atomicAccess_ppc.hpp","additions":649,"deletions":0,"binary":false,"changes":649,"status":"added"},{"patch":"@@ -2,2 +2,2 @@\n- * Copyright (c) 1997, 2020, Oracle and\/or its affiliates. All rights reserved.\n- * Copyright (c) 2012, 2014 SAP SE. All rights reserved.\n+ * Copyright (c) 1997, 2025, Oracle and\/or its affiliates. All rights reserved.\n+ * Copyright (c) 2012, 2025 SAP SE. All rights reserved.\n@@ -26,4 +26,2 @@\n-#ifndef OS_CPU_LINUX_PPC_ORDERACCESS_LINUX_PPC_HPP\n-#define OS_CPU_LINUX_PPC_ORDERACCESS_LINUX_PPC_HPP\n-\n-\/\/ Included in orderAccess.hpp header file.\n+#ifndef CPU_PPC_ORDERACCESS_PPC_HPP\n+#define CPU_PPC_ORDERACCESS_PPC_HPP\n@@ -88,1 +86,1 @@\n-#endif \/\/ OS_CPU_LINUX_PPC_ORDERACCESS_LINUX_PPC_HPP\n+#endif \/\/ CPU_PPC_ORDERACCESS_PPC_HPP\n","filename":"src\/hotspot\/cpu\/ppc\/orderAccess_ppc.hpp","additions":5,"deletions":7,"binary":false,"changes":12,"previous_filename":"src\/hotspot\/os_cpu\/linux_ppc\/orderAccess_linux_ppc.hpp","status":"copied"},{"patch":"@@ -2,2 +2,2 @@\n- * Copyright (c) 1997, 2023, Oracle and\/or its affiliates. All rights reserved.\n- * Copyright (c) 2012, 2019 SAP SE. All rights reserved.\n+ * Copyright (c) 1997, 2025, Oracle and\/or its affiliates. All rights reserved.\n+ * Copyright (c) 2012, 2025 SAP SE. All rights reserved.\n@@ -26,392 +26,2 @@\n-#ifndef OS_CPU_AIX_PPC_ATOMIC_AIX_PPC_HPP\n-#define OS_CPU_AIX_PPC_ATOMIC_AIX_PPC_HPP\n-\n-#ifndef PPC64\n-#error \"Atomic currently only implemented for PPC64\"\n-#endif\n-\n-#include \"orderAccess_aix_ppc.hpp\"\n-#include \"utilities\/debug.hpp\"\n-\n-\/\/ Implementation of class atomic\n-\n-\/\/\n-\/\/ machine barrier instructions:\n-\/\/\n-\/\/ - sync            two-way memory barrier, aka fence\n-\/\/ - lwsync          orders  Store|Store,\n-\/\/                            Load|Store,\n-\/\/                            Load|Load,\n-\/\/                   but not Store|Load\n-\/\/ - eieio           orders memory accesses for device memory (only)\n-\/\/ - isync           invalidates speculatively executed instructions\n-\/\/                   From the POWER ISA 2.06 documentation:\n-\/\/                    \"[...] an isync instruction prevents the execution of\n-\/\/                   instructions following the isync until instructions\n-\/\/                   preceding the isync have completed, [...]\"\n-\/\/                   From IBM's AIX assembler reference:\n-\/\/                    \"The isync [...] instructions causes the processor to\n-\/\/                   refetch any instructions that might have been fetched\n-\/\/                   prior to the isync instruction. The instruction isync\n-\/\/                   causes the processor to wait for all previous instructions\n-\/\/                   to complete. Then any instructions already fetched are\n-\/\/                   discarded and instruction processing continues in the\n-\/\/                   environment established by the previous instructions.\"\n-\/\/\n-\/\/ semantic barrier instructions:\n-\/\/ (as defined in orderAccess.hpp)\n-\/\/\n-\/\/ - release         orders Store|Store,       (maps to lwsync)\n-\/\/                           Load|Store\n-\/\/ - acquire         orders  Load|Store,       (maps to lwsync)\n-\/\/                           Load|Load\n-\/\/ - fence           orders Store|Store,       (maps to sync)\n-\/\/                           Load|Store,\n-\/\/                           Load|Load,\n-\/\/                          Store|Load\n-\/\/\n-\n-inline void pre_membar(atomic_memory_order order) {\n-  switch (order) {\n-    case memory_order_relaxed:\n-    case memory_order_acquire: break;\n-    case memory_order_release:\n-    case memory_order_acq_rel: __asm__ __volatile__ (\"lwsync\" : : : \"memory\"); break;\n-    default \/*conservative*\/ : __asm__ __volatile__ (\"sync\"   : : : \"memory\"); break;\n-  }\n-}\n-\n-inline void post_membar(atomic_memory_order order) {\n-  switch (order) {\n-    case memory_order_relaxed:\n-    case memory_order_release: break;\n-    case memory_order_acquire:\n-    case memory_order_acq_rel: __asm__ __volatile__ (\"isync\"  : : : \"memory\"); break;\n-    default \/*conservative*\/ : __asm__ __volatile__ (\"sync\"   : : : \"memory\"); break;\n-  }\n-}\n-\n-\n-template<size_t byte_size>\n-struct Atomic::PlatformAdd {\n-  template<typename D, typename I>\n-  D add_then_fetch(D volatile* dest, I add_value, atomic_memory_order order) const;\n-\n-  template<typename D, typename I>\n-  D fetch_then_add(D volatile* dest, I add_value, atomic_memory_order order) const {\n-    return add_then_fetch(dest, add_value, order) - add_value;\n-  }\n-};\n-\n-template<>\n-template<typename D, typename I>\n-inline D Atomic::PlatformAdd<4>::add_then_fetch(D volatile* dest, I add_value,\n-                                               atomic_memory_order order) const {\n-  STATIC_ASSERT(4 == sizeof(I));\n-  STATIC_ASSERT(4 == sizeof(D));\n-\n-  D result;\n-\n-  pre_membar(order);\n-\n-  __asm__ __volatile__ (\n-    \"1: lwarx   %0,  0, %2    \\n\"\n-    \"   add     %0, %0, %1    \\n\"\n-    \"   stwcx.  %0,  0, %2    \\n\"\n-    \"   bne-    1b            \\n\"\n-    : \/*%0*\/\"=&r\" (result)\n-    : \/*%1*\/\"r\" (add_value), \/*%2*\/\"r\" (dest)\n-    : \"cc\", \"memory\" );\n-\n-  post_membar(order);\n-\n-  return result;\n-}\n-\n-\n-template<>\n-template<typename D, typename I>\n-inline D Atomic::PlatformAdd<8>::add_then_fetch(D volatile* dest, I add_value,\n-                                               atomic_memory_order order) const {\n-  STATIC_ASSERT(8 == sizeof(I));\n-  STATIC_ASSERT(8 == sizeof(D));\n-\n-  D result;\n-\n-  pre_membar(order);\n-\n-  __asm__ __volatile__ (\n-    \"1: ldarx   %0,  0, %2    \\n\"\n-    \"   add     %0, %0, %1    \\n\"\n-    \"   stdcx.  %0,  0, %2    \\n\"\n-    \"   bne-    1b            \\n\"\n-    : \/*%0*\/\"=&r\" (result)\n-    : \/*%1*\/\"r\" (add_value), \/*%2*\/\"r\" (dest)\n-    : \"cc\", \"memory\" );\n-\n-  post_membar(order);\n-\n-  return result;\n-}\n-\n-template<>\n-template<typename T>\n-inline T Atomic::PlatformXchg<4>::operator()(T volatile* dest,\n-                                             T exchange_value,\n-                                             atomic_memory_order order) const {\n-  \/\/ Note that xchg doesn't necessarily do an acquire\n-  \/\/ (see synchronizer.cpp).\n-\n-  T old_value;\n-  const uint64_t zero = 0;\n-\n-  pre_membar(order);\n-\n-  __asm__ __volatile__ (\n-    \/* atomic loop *\/\n-    \"1:                                                 \\n\"\n-    \"   lwarx   %[old_value], %[dest], %[zero]          \\n\"\n-    \"   stwcx.  %[exchange_value], %[dest], %[zero]     \\n\"\n-    \"   bne-    1b                                      \\n\"\n-    \/* exit *\/\n-    \"2:                                                 \\n\"\n-    \/* out *\/\n-    : [old_value]       \"=&r\"   (old_value),\n-                        \"=m\"    (*dest)\n-    \/* in *\/\n-    : [dest]            \"b\"     (dest),\n-      [zero]            \"r\"     (zero),\n-      [exchange_value]  \"r\"     (exchange_value),\n-                        \"m\"     (*dest)\n-    \/* clobber *\/\n-    : \"cc\",\n-      \"memory\"\n-    );\n-\n-  post_membar(order);\n-\n-  return old_value;\n-}\n-\n-template<>\n-template<typename T>\n-inline T Atomic::PlatformXchg<8>::operator()(T volatile* dest,\n-                                             T exchange_value,\n-                                             atomic_memory_order order) const {\n-  STATIC_ASSERT(8 == sizeof(T));\n-  \/\/ Note that xchg doesn't necessarily do an acquire\n-  \/\/ (see synchronizer.cpp).\n-\n-  T old_value;\n-  const uint64_t zero = 0;\n-\n-  pre_membar(order);\n-\n-  __asm__ __volatile__ (\n-    \/* atomic loop *\/\n-    \"1:                                                 \\n\"\n-    \"   ldarx   %[old_value], %[dest], %[zero]          \\n\"\n-    \"   stdcx.  %[exchange_value], %[dest], %[zero]     \\n\"\n-    \"   bne-    1b                                      \\n\"\n-    \/* exit *\/\n-    \"2:                                                 \\n\"\n-    \/* out *\/\n-    : [old_value]       \"=&r\"   (old_value),\n-                        \"=m\"    (*dest)\n-    \/* in *\/\n-    : [dest]            \"b\"     (dest),\n-      [zero]            \"r\"     (zero),\n-      [exchange_value]  \"r\"     (exchange_value),\n-                        \"m\"     (*dest)\n-    \/* clobber *\/\n-    : \"cc\",\n-      \"memory\"\n-    );\n-\n-  post_membar(order);\n-\n-  return old_value;\n-}\n-\n-template<>\n-template<typename T>\n-inline T Atomic::PlatformCmpxchg<1>::operator()(T volatile* dest,\n-                                                T compare_value,\n-                                                T exchange_value,\n-                                                atomic_memory_order order) const {\n-  STATIC_ASSERT(1 == sizeof(T));\n-\n-  \/\/ Note that cmpxchg guarantees a two-way memory barrier across\n-  \/\/ the cmpxchg, so it's really a 'fence_cmpxchg_fence' if not\n-  \/\/ specified otherwise (see atomic.hpp).\n-\n-  \/\/ Using 32 bit internally.\n-  volatile int *dest_base = (volatile int*)((uintptr_t)dest & ~3);\n-\n-#ifdef VM_LITTLE_ENDIAN\n-  const unsigned int shift_amount        = ((uintptr_t)dest & 3) * 8;\n-#else\n-  const unsigned int shift_amount        = ((~(uintptr_t)dest) & 3) * 8;\n-#endif\n-  const unsigned int masked_compare_val  = ((unsigned int)(unsigned char)compare_value),\n-                     masked_exchange_val = ((unsigned int)(unsigned char)exchange_value),\n-                     xor_value           = (masked_compare_val ^ masked_exchange_val) << shift_amount;\n-\n-  unsigned int old_value, value32;\n-\n-  pre_membar(order);\n-\n-  __asm__ __volatile__ (\n-    \/* simple guard *\/\n-    \"   lbz     %[old_value], 0(%[dest])                  \\n\"\n-    \"   cmpw    %[masked_compare_val], %[old_value]       \\n\"\n-    \"   bne-    2f                                        \\n\"\n-    \/* atomic loop *\/\n-    \"1:                                                   \\n\"\n-    \"   lwarx   %[value32], 0, %[dest_base]               \\n\"\n-    \/* extract byte and compare *\/\n-    \"   srd     %[old_value], %[value32], %[shift_amount] \\n\"\n-    \"   clrldi  %[old_value], %[old_value], 56            \\n\"\n-    \"   cmpw    %[masked_compare_val], %[old_value]       \\n\"\n-    \"   bne-    2f                                        \\n\"\n-    \/* replace byte and try to store *\/\n-    \"   xor     %[value32], %[xor_value], %[value32]      \\n\"\n-    \"   stwcx.  %[value32], 0, %[dest_base]               \\n\"\n-    \"   bne-    1b                                        \\n\"\n-    \/* exit *\/\n-    \"2:                                                   \\n\"\n-    \/* out *\/\n-    : [old_value]           \"=&r\"   (old_value),\n-      [value32]             \"=&r\"   (value32),\n-                            \"=m\"    (*dest),\n-                            \"=m\"    (*dest_base)\n-    \/* in *\/\n-    : [dest]                \"b\"     (dest),\n-      [dest_base]           \"b\"     (dest_base),\n-      [shift_amount]        \"r\"     (shift_amount),\n-      [masked_compare_val]  \"r\"     (masked_compare_val),\n-      [xor_value]           \"r\"     (xor_value),\n-                            \"m\"     (*dest),\n-                            \"m\"     (*dest_base)\n-    \/* clobber *\/\n-    : \"cc\",\n-      \"memory\"\n-    );\n-\n-  post_membar(order);\n-\n-  return PrimitiveConversions::cast<T>((unsigned char)old_value);\n-}\n-\n-template<>\n-template<typename T>\n-inline T Atomic::PlatformCmpxchg<4>::operator()(T volatile* dest,\n-                                                T compare_value,\n-                                                T exchange_value,\n-                                                atomic_memory_order order) const {\n-  STATIC_ASSERT(4 == sizeof(T));\n-\n-  \/\/ Note that cmpxchg guarantees a two-way memory barrier across\n-  \/\/ the cmpxchg, so it's really a 'fence_cmpxchg_fence' if not\n-  \/\/ specified otherwise (see atomic.hpp).\n-\n-  T old_value;\n-  const uint64_t zero = 0;\n-\n-  pre_membar(order);\n-\n-  __asm__ __volatile__ (\n-    \/* simple guard *\/\n-    \"   lwz     %[old_value], 0(%[dest])                \\n\"\n-    \"   cmpw    %[compare_value], %[old_value]          \\n\"\n-    \"   bne-    2f                                      \\n\"\n-    \/* atomic loop *\/\n-    \"1:                                                 \\n\"\n-    \"   lwarx   %[old_value], %[dest], %[zero]          \\n\"\n-    \"   cmpw    %[compare_value], %[old_value]          \\n\"\n-    \"   bne-    2f                                      \\n\"\n-    \"   stwcx.  %[exchange_value], %[dest], %[zero]     \\n\"\n-    \"   bne-    1b                                      \\n\"\n-    \/* exit *\/\n-    \"2:                                                 \\n\"\n-    \/* out *\/\n-    : [old_value]       \"=&r\"   (old_value),\n-                        \"=m\"    (*dest)\n-    \/* in *\/\n-    : [dest]            \"b\"     (dest),\n-      [zero]            \"r\"     (zero),\n-      [compare_value]   \"r\"     (compare_value),\n-      [exchange_value]  \"r\"     (exchange_value),\n-                        \"m\"     (*dest)\n-    \/* clobber *\/\n-    : \"cc\",\n-      \"memory\"\n-    );\n-\n-  post_membar(order);\n-\n-  return old_value;\n-}\n-\n-template<>\n-template<typename T>\n-inline T Atomic::PlatformCmpxchg<8>::operator()(T volatile* dest,\n-                                                T compare_value,\n-                                                T exchange_value,\n-                                                atomic_memory_order order) const {\n-  STATIC_ASSERT(8 == sizeof(T));\n-\n-  \/\/ Note that cmpxchg guarantees a two-way memory barrier across\n-  \/\/ the cmpxchg, so it's really a 'fence_cmpxchg_fence' if not\n-  \/\/ specified otherwise (see atomic.hpp).\n-\n-  T old_value;\n-  const uint64_t zero = 0;\n-\n-  pre_membar(order);\n-\n-  __asm__ __volatile__ (\n-    \/* simple guard *\/\n-    \"   ld      %[old_value], 0(%[dest])                \\n\"\n-    \"   cmpd    %[compare_value], %[old_value]          \\n\"\n-    \"   bne-    2f                                      \\n\"\n-    \/* atomic loop *\/\n-    \"1:                                                 \\n\"\n-    \"   ldarx   %[old_value], %[dest], %[zero]          \\n\"\n-    \"   cmpd    %[compare_value], %[old_value]          \\n\"\n-    \"   bne-    2f                                      \\n\"\n-    \"   stdcx.  %[exchange_value], %[dest], %[zero]     \\n\"\n-    \"   bne-    1b                                      \\n\"\n-    \/* exit *\/\n-    \"2:                                                 \\n\"\n-    \/* out *\/\n-    : [old_value]       \"=&r\"   (old_value),\n-                        \"=m\"    (*dest)\n-    \/* in *\/\n-    : [dest]            \"b\"     (dest),\n-      [zero]            \"r\"     (zero),\n-      [compare_value]   \"r\"     (compare_value),\n-      [exchange_value]  \"r\"     (exchange_value),\n-                        \"m\"     (*dest)\n-    \/* clobber *\/\n-    : \"cc\",\n-      \"memory\"\n-    );\n-\n-  post_membar(order);\n-\n-  return old_value;\n-}\n-\n-template<size_t byte_size>\n-struct Atomic::PlatformOrderedLoad<byte_size, X_ACQUIRE> {\n-  template <typename T>\n-  T operator()(const volatile T* p) const {\n-    T t = Atomic::load(p);\n-    \/\/ Use twi-isync for load_acquire (faster than lwsync).\n-    __asm__ __volatile__ (\"twi 0,%0,0\\n isync\\n\" : : \"r\" (t) : \"memory\");\n-    return t;\n-  }\n-};\n-\n-#endif \/\/ OS_CPU_AIX_PPC_ATOMIC_AIX_PPC_HPP\n+\/\/ Including inline assembler functions that are shared between multiple PPC64 platforms.\n+#include \"atomicAccess_ppc.hpp\"\n","filename":"src\/hotspot\/os_cpu\/aix_ppc\/atomic_aix_ppc.hpp","additions":4,"deletions":394,"binary":false,"changes":398,"status":"modified"},{"patch":"@@ -2,2 +2,2 @@\n- * Copyright (c) 1997, 2020, Oracle and\/or its affiliates. All rights reserved.\n- * Copyright (c) 2012, 2019 SAP SE. All rights reserved.\n+ * Copyright (c) 1997, 2025, Oracle and\/or its affiliates. All rights reserved.\n+ * Copyright (c) 2012, 2025 SAP SE. All rights reserved.\n@@ -26,59 +26,2 @@\n-#ifndef OS_CPU_AIX_PPC_ORDERACCESS_AIX_PPC_HPP\n-#define OS_CPU_AIX_PPC_ORDERACCESS_AIX_PPC_HPP\n-\n-\/\/ Included in orderAccess.hpp header file.\n-\n-\/\/ Compiler version last used for testing: xlc 12\n-\/\/ Please update this information when this file changes\n-\n-\/\/ Implementation of class OrderAccess.\n-\n-\/\/\n-\/\/ Machine barrier instructions:\n-\/\/\n-\/\/ - sync            Two-way memory barrier, aka fence.\n-\/\/ - lwsync          orders  Store|Store,\n-\/\/                            Load|Store,\n-\/\/                            Load|Load,\n-\/\/                   but not Store|Load\n-\/\/ - eieio           orders  Store|Store\n-\/\/ - isync           Invalidates speculatively executed instructions,\n-\/\/                   but isync may complete before storage accesses\n-\/\/                   associated with instructions preceding isync have\n-\/\/                   been performed.\n-\/\/\n-\/\/ Semantic barrier instructions:\n-\/\/ (as defined in orderAccess.hpp)\n-\/\/\n-\/\/ - release         orders Store|Store,       (maps to lwsync)\n-\/\/                           Load|Store\n-\/\/ - acquire         orders  Load|Store,       (maps to lwsync)\n-\/\/                           Load|Load\n-\/\/ - fence           orders Store|Store,       (maps to sync)\n-\/\/                           Load|Store,\n-\/\/                           Load|Load,\n-\/\/                          Store|Load\n-\/\/\n-\n-#define inlasm_sync()     __asm__ __volatile__ (\"sync\"   : : : \"memory\");\n-#define inlasm_lwsync()   __asm__ __volatile__ (\"lwsync\" : : : \"memory\");\n-#define inlasm_eieio()    __asm__ __volatile__ (\"eieio\"  : : : \"memory\");\n-#define inlasm_isync()    __asm__ __volatile__ (\"isync\"  : : : \"memory\");\n-\n-inline void OrderAccess::loadload()   { inlasm_lwsync(); }\n-inline void OrderAccess::storestore() { inlasm_lwsync(); }\n-inline void OrderAccess::loadstore()  { inlasm_lwsync(); }\n-inline void OrderAccess::storeload()  { inlasm_sync();   }\n-\n-inline void OrderAccess::acquire()    { inlasm_lwsync(); }\n-inline void OrderAccess::release()    { inlasm_lwsync(); }\n-inline void OrderAccess::fence()      { inlasm_sync();   }\n-inline void OrderAccess::cross_modify_fence_impl()\n-                                      { inlasm_isync();  }\n-\n-#undef inlasm_sync\n-#undef inlasm_lwsync\n-#undef inlasm_eieio\n-#undef inlasm_isync\n-\n-#endif \/\/ OS_CPU_AIX_PPC_ORDERACCESS_AIX_PPC_HPP\n+\/\/ Including inline assembler functions that are shared between multiple PPC64 platforms.\n+#include \"orderAccess_ppc.hpp\"\n","filename":"src\/hotspot\/os_cpu\/aix_ppc\/orderAccess_aix_ppc.hpp","additions":4,"deletions":61,"binary":false,"changes":65,"status":"modified"},{"patch":"@@ -2,2 +2,2 @@\n- * Copyright (c) 1997, 2023, Oracle and\/or its affiliates. All rights reserved.\n- * Copyright (c) 2012, 2019 SAP SE. All rights reserved.\n+ * Copyright (c) 1997, 2025, Oracle and\/or its affiliates. All rights reserved.\n+ * Copyright (c) 2012, 2025 SAP SE. All rights reserved.\n@@ -26,370 +26,2 @@\n-#ifndef OS_CPU_LINUX_PPC_ATOMIC_LINUX_PPC_HPP\n-#define OS_CPU_LINUX_PPC_ATOMIC_LINUX_PPC_HPP\n-\n-#ifndef PPC64\n-#error \"Atomic currently only implemented for PPC64\"\n-#endif\n-\n-#include \"orderAccess_linux_ppc.hpp\"\n-#include \"utilities\/debug.hpp\"\n-\n-\/\/ Implementation of class atomic\n-\n-\/\/\n-\/\/ machine barrier instructions:\n-\/\/\n-\/\/ - sync            two-way memory barrier, aka fence\n-\/\/ - lwsync          orders  Store|Store,\n-\/\/                            Load|Store,\n-\/\/                            Load|Load,\n-\/\/                   but not Store|Load\n-\/\/ - eieio           orders memory accesses for device memory (only)\n-\/\/ - isync           invalidates speculatively executed instructions\n-\/\/                   From the POWER ISA 2.06 documentation:\n-\/\/                    \"[...] an isync instruction prevents the execution of\n-\/\/                   instructions following the isync until instructions\n-\/\/                   preceding the isync have completed, [...]\"\n-\/\/                   From IBM's AIX assembler reference:\n-\/\/                    \"The isync [...] instructions causes the processor to\n-\/\/                   refetch any instructions that might have been fetched\n-\/\/                   prior to the isync instruction. The instruction isync\n-\/\/                   causes the processor to wait for all previous instructions\n-\/\/                   to complete. Then any instructions already fetched are\n-\/\/                   discarded and instruction processing continues in the\n-\/\/                   environment established by the previous instructions.\"\n-\/\/\n-\/\/ semantic barrier instructions:\n-\/\/ (as defined in orderAccess.hpp)\n-\/\/\n-\/\/ - release         orders Store|Store,       (maps to lwsync)\n-\/\/                           Load|Store\n-\/\/ - acquire         orders  Load|Store,       (maps to lwsync)\n-\/\/                           Load|Load\n-\/\/ - fence           orders Store|Store,       (maps to sync)\n-\/\/                           Load|Store,\n-\/\/                           Load|Load,\n-\/\/                          Store|Load\n-\/\/\n-\n-inline void pre_membar(atomic_memory_order order) {\n-  switch (order) {\n-    case memory_order_relaxed:\n-    case memory_order_acquire: break;\n-    case memory_order_release:\n-    case memory_order_acq_rel: __asm__ __volatile__ (\"lwsync\" : : : \"memory\"); break;\n-    default \/*conservative*\/ : __asm__ __volatile__ (\"sync\"   : : : \"memory\"); break;\n-  }\n-}\n-\n-inline void post_membar(atomic_memory_order order) {\n-  switch (order) {\n-    case memory_order_relaxed:\n-    case memory_order_release: break;\n-    case memory_order_acquire:\n-    case memory_order_acq_rel: __asm__ __volatile__ (\"isync\"  : : : \"memory\"); break;\n-    default \/*conservative*\/ : __asm__ __volatile__ (\"sync\"   : : : \"memory\"); break;\n-  }\n-}\n-\n-\n-template<size_t byte_size>\n-struct Atomic::PlatformAdd {\n-  template<typename D, typename I>\n-  D add_then_fetch(D volatile* dest, I add_value, atomic_memory_order order) const;\n-\n-  template<typename D, typename I>\n-  D fetch_then_add(D volatile* dest, I add_value, atomic_memory_order order) const {\n-    return add_then_fetch(dest, add_value, order) - add_value;\n-  }\n-};\n-\n-template<>\n-template<typename D, typename I>\n-inline D Atomic::PlatformAdd<4>::add_then_fetch(D volatile* dest, I add_value,\n-                                                atomic_memory_order order) const {\n-  STATIC_ASSERT(4 == sizeof(I));\n-  STATIC_ASSERT(4 == sizeof(D));\n-\n-  D result;\n-\n-  pre_membar(order);\n-\n-  __asm__ __volatile__ (\n-    \"1: lwarx   %0,  0, %2    \\n\"\n-    \"   add     %0, %0, %1    \\n\"\n-    \"   stwcx.  %0,  0, %2    \\n\"\n-    \"   bne-    1b            \\n\"\n-    : \/*%0*\/\"=&r\" (result)\n-    : \/*%1*\/\"r\" (add_value), \/*%2*\/\"r\" (dest)\n-    : \"cc\", \"memory\" );\n-\n-  post_membar(order);\n-\n-  return result;\n-}\n-\n-\n-template<>\n-template<typename D, typename I>\n-inline D Atomic::PlatformAdd<8>::add_then_fetch(D volatile* dest, I add_value,\n-                                                atomic_memory_order order) const {\n-  STATIC_ASSERT(8 == sizeof(I));\n-  STATIC_ASSERT(8 == sizeof(D));\n-\n-  D result;\n-\n-  pre_membar(order);\n-\n-  __asm__ __volatile__ (\n-    \"1: ldarx   %0,  0, %2    \\n\"\n-    \"   add     %0, %0, %1    \\n\"\n-    \"   stdcx.  %0,  0, %2    \\n\"\n-    \"   bne-    1b            \\n\"\n-    : \/*%0*\/\"=&r\" (result)\n-    : \/*%1*\/\"r\" (add_value), \/*%2*\/\"r\" (dest)\n-    : \"cc\", \"memory\" );\n-\n-  post_membar(order);\n-\n-  return result;\n-}\n-\n-template<>\n-template<typename T>\n-inline T Atomic::PlatformXchg<4>::operator()(T volatile* dest,\n-                                             T exchange_value,\n-                                             atomic_memory_order order) const {\n-  \/\/ Note that xchg doesn't necessarily do an acquire\n-  \/\/ (see synchronizer.cpp).\n-\n-  T old_value;\n-  const uint64_t zero = 0;\n-\n-  pre_membar(order);\n-\n-  __asm__ __volatile__ (\n-    \/* atomic loop *\/\n-    \"1:                                                 \\n\"\n-    \"   lwarx   %[old_value], %[dest], %[zero]          \\n\"\n-    \"   stwcx.  %[exchange_value], %[dest], %[zero]     \\n\"\n-    \"   bne-    1b                                      \\n\"\n-    \/* exit *\/\n-    \"2:                                                 \\n\"\n-    \/* out *\/\n-    : [old_value]       \"=&r\"   (old_value),\n-                        \"=m\"    (*dest)\n-    \/* in *\/\n-    : [dest]            \"b\"     (dest),\n-      [zero]            \"r\"     (zero),\n-      [exchange_value]  \"r\"     (exchange_value),\n-                        \"m\"     (*dest)\n-    \/* clobber *\/\n-    : \"cc\",\n-      \"memory\"\n-    );\n-\n-  post_membar(order);\n-\n-  return old_value;\n-}\n-\n-template<>\n-template<typename T>\n-inline T Atomic::PlatformXchg<8>::operator()(T volatile* dest,\n-                                             T exchange_value,\n-                                             atomic_memory_order order) const {\n-  STATIC_ASSERT(8 == sizeof(T));\n-  \/\/ Note that xchg doesn't necessarily do an acquire\n-  \/\/ (see synchronizer.cpp).\n-\n-  T old_value;\n-  const uint64_t zero = 0;\n-\n-  pre_membar(order);\n-\n-  __asm__ __volatile__ (\n-    \/* atomic loop *\/\n-    \"1:                                                 \\n\"\n-    \"   ldarx   %[old_value], %[dest], %[zero]          \\n\"\n-    \"   stdcx.  %[exchange_value], %[dest], %[zero]     \\n\"\n-    \"   bne-    1b                                      \\n\"\n-    \/* exit *\/\n-    \"2:                                                 \\n\"\n-    \/* out *\/\n-    : [old_value]       \"=&r\"   (old_value),\n-                        \"=m\"    (*dest)\n-    \/* in *\/\n-    : [dest]            \"b\"     (dest),\n-      [zero]            \"r\"     (zero),\n-      [exchange_value]  \"r\"     (exchange_value),\n-                        \"m\"     (*dest)\n-    \/* clobber *\/\n-    : \"cc\",\n-      \"memory\"\n-    );\n-\n-  post_membar(order);\n-\n-  return old_value;\n-}\n-\n-template<>\n-template<typename T>\n-inline T Atomic::PlatformCmpxchg<1>::operator()(T volatile* dest,\n-                                                T compare_value,\n-                                                T exchange_value,\n-                                                atomic_memory_order order) const {\n-  STATIC_ASSERT(1 == sizeof(T));\n-\n-  \/\/ Note that cmpxchg guarantees a two-way memory barrier across\n-  \/\/ the cmpxchg, so it's really a 'fence_cmpxchg_fence' if not\n-  \/\/ specified otherwise (see atomic.hpp).\n-\n-  \/\/ Using 32 bit internally.\n-  unsigned int old_value, loaded_value;\n-  pre_membar(order);\n-\n-  __asm__ __volatile__ (\n-    \/* atomic loop *\/\n-    \"1:                                                   \\n\"\n-    \"   lbarx   %[old_value], 0, %[dest]                  \\n\"\n-    \/* extract byte and compare *\/\n-    \"   cmpw    %[compare_value], %[old_value]            \\n\"\n-    \"   bne-    2f                                        \\n\"\n-    \/* replace byte and try to store *\/\n-    \"   stbcx.  %[exchange_value], 0, %[dest]             \\n\"\n-    \"   bne-    1b                                        \\n\"\n-    \/* exit *\/\n-    \"2:                                                   \\n\"\n-    \/* out *\/\n-    : [old_value]           \"=&r\"   (old_value),\n-      [loaded_value]        \"=&r\"   (loaded_value),\n-                            \"=m\"    (*dest)\n-    \/* in *\/\n-    : [dest]            \"b\"     (dest),\n-      [compare_value]   \"r\"     (compare_value),\n-      [exchange_value]  \"r\"     (exchange_value),\n-                        \"m\"     (*dest)\n-    \/* clobber *\/\n-    : \"cc\",\n-      \"memory\"\n-    );\n-\n-  post_membar(order);\n-\n-  return PrimitiveConversions::cast<T>((unsigned char)old_value);\n-}\n-\n-template<>\n-template<typename T>\n-inline T Atomic::PlatformCmpxchg<4>::operator()(T volatile* dest,\n-                                                T compare_value,\n-                                                T exchange_value,\n-                                                atomic_memory_order order) const {\n-  STATIC_ASSERT(4 == sizeof(T));\n-\n-  \/\/ Note that cmpxchg guarantees a two-way memory barrier across\n-  \/\/ the cmpxchg, so it's really a 'fence_cmpxchg_fence' if not\n-  \/\/ specified otherwise (see atomic.hpp).\n-\n-  T old_value;\n-  const uint64_t zero = 0;\n-\n-  pre_membar(order);\n-\n-  __asm__ __volatile__ (\n-    \/* simple guard *\/\n-    \"   lwz     %[old_value], 0(%[dest])                \\n\"\n-    \"   cmpw    %[compare_value], %[old_value]          \\n\"\n-    \"   bne-    2f                                      \\n\"\n-    \/* atomic loop *\/\n-    \"1:                                                 \\n\"\n-    \"   lwarx   %[old_value], %[dest], %[zero]          \\n\"\n-    \"   cmpw    %[compare_value], %[old_value]          \\n\"\n-    \"   bne-    2f                                      \\n\"\n-    \"   stwcx.  %[exchange_value], %[dest], %[zero]     \\n\"\n-    \"   bne-    1b                                      \\n\"\n-    \/* exit *\/\n-    \"2:                                                 \\n\"\n-    \/* out *\/\n-    : [old_value]       \"=&r\"   (old_value),\n-                        \"=m\"    (*dest)\n-    \/* in *\/\n-    : [dest]            \"b\"     (dest),\n-      [zero]            \"r\"     (zero),\n-      [compare_value]   \"r\"     (compare_value),\n-      [exchange_value]  \"r\"     (exchange_value),\n-                        \"m\"     (*dest)\n-    \/* clobber *\/\n-    : \"cc\",\n-      \"memory\"\n-    );\n-\n-  post_membar(order);\n-\n-  return old_value;\n-}\n-\n-template<>\n-template<typename T>\n-inline T Atomic::PlatformCmpxchg<8>::operator()(T volatile* dest,\n-                                                T compare_value,\n-                                                T exchange_value,\n-                                                atomic_memory_order order) const {\n-  STATIC_ASSERT(8 == sizeof(T));\n-\n-  \/\/ Note that cmpxchg guarantees a two-way memory barrier across\n-  \/\/ the cmpxchg, so it's really a 'fence_cmpxchg_fence' if not\n-  \/\/ specified otherwise (see atomic.hpp).\n-\n-  T old_value;\n-  const uint64_t zero = 0;\n-\n-  pre_membar(order);\n-\n-  __asm__ __volatile__ (\n-    \/* simple guard *\/\n-    \"   ld      %[old_value], 0(%[dest])                \\n\"\n-    \"   cmpd    %[compare_value], %[old_value]          \\n\"\n-    \"   bne-    2f                                      \\n\"\n-    \/* atomic loop *\/\n-    \"1:                                                 \\n\"\n-    \"   ldarx   %[old_value], %[dest], %[zero]          \\n\"\n-    \"   cmpd    %[compare_value], %[old_value]          \\n\"\n-    \"   bne-    2f                                      \\n\"\n-    \"   stdcx.  %[exchange_value], %[dest], %[zero]     \\n\"\n-    \"   bne-    1b                                      \\n\"\n-    \/* exit *\/\n-    \"2:                                                 \\n\"\n-    \/* out *\/\n-    : [old_value]       \"=&r\"   (old_value),\n-                        \"=m\"    (*dest)\n-    \/* in *\/\n-    : [dest]            \"b\"     (dest),\n-      [zero]            \"r\"     (zero),\n-      [compare_value]   \"r\"     (compare_value),\n-      [exchange_value]  \"r\"     (exchange_value),\n-                        \"m\"     (*dest)\n-    \/* clobber *\/\n-    : \"cc\",\n-      \"memory\"\n-    );\n-\n-  post_membar(order);\n-\n-  return old_value;\n-}\n-\n-template<size_t byte_size>\n-struct Atomic::PlatformOrderedLoad<byte_size, X_ACQUIRE>\n-{\n-  template <typename T>\n-  T operator()(const volatile T* p) const {\n-    T t = Atomic::load(p);\n-    \/\/ Use twi-isync for load_acquire (faster than lwsync).\n-    __asm__ __volatile__ (\"twi 0,%0,0\\n isync\\n\" : : \"r\" (t) : \"memory\");\n-    return t;\n-  }\n-};\n-\n-#endif \/\/ OS_CPU_LINUX_PPC_ATOMIC_LINUX_PPC_HPP\n+\/\/ Including inline assembler functions that are shared between multiple PPC64 platforms.\n+#include \"atomicAccess_ppc.hpp\"\n","filename":"src\/hotspot\/os_cpu\/linux_ppc\/atomic_linux_ppc.hpp","additions":4,"deletions":372,"binary":false,"changes":376,"status":"modified"},{"patch":"@@ -2,2 +2,2 @@\n- * Copyright (c) 1997, 2020, Oracle and\/or its affiliates. All rights reserved.\n- * Copyright (c) 2012, 2014 SAP SE. All rights reserved.\n+ * Copyright (c) 1997, 2025, Oracle and\/or its affiliates. All rights reserved.\n+ * Copyright (c) 2012, 2025 SAP SE. All rights reserved.\n@@ -26,63 +26,2 @@\n-#ifndef OS_CPU_LINUX_PPC_ORDERACCESS_LINUX_PPC_HPP\n-#define OS_CPU_LINUX_PPC_ORDERACCESS_LINUX_PPC_HPP\n-\n-\/\/ Included in orderAccess.hpp header file.\n-\n-#ifndef PPC64\n-#error \"OrderAccess currently only implemented for PPC64\"\n-#endif\n-\n-\/\/ Compiler version last used for testing: gcc 4.1.2\n-\/\/ Please update this information when this file changes\n-\n-\/\/ Implementation of class OrderAccess.\n-\n-\/\/\n-\/\/ Machine barrier instructions:\n-\/\/\n-\/\/ - sync            Two-way memory barrier, aka fence.\n-\/\/ - lwsync          orders  Store|Store,\n-\/\/                            Load|Store,\n-\/\/                            Load|Load,\n-\/\/                   but not Store|Load\n-\/\/ - eieio           orders  Store|Store\n-\/\/ - isync           Invalidates speculatively executed instructions,\n-\/\/                   but isync may complete before storage accesses\n-\/\/                   associated with instructions preceding isync have\n-\/\/                   been performed.\n-\/\/\n-\/\/ Semantic barrier instructions:\n-\/\/ (as defined in orderAccess.hpp)\n-\/\/\n-\/\/ - release         orders Store|Store,       (maps to lwsync)\n-\/\/                           Load|Store\n-\/\/ - acquire         orders  Load|Store,       (maps to lwsync)\n-\/\/                           Load|Load\n-\/\/ - fence           orders Store|Store,       (maps to sync)\n-\/\/                           Load|Store,\n-\/\/                           Load|Load,\n-\/\/                          Store|Load\n-\/\/\n-\n-#define inlasm_sync()     __asm__ __volatile__ (\"sync\"   : : : \"memory\");\n-#define inlasm_lwsync()   __asm__ __volatile__ (\"lwsync\" : : : \"memory\");\n-#define inlasm_eieio()    __asm__ __volatile__ (\"eieio\"  : : : \"memory\");\n-#define inlasm_isync()    __asm__ __volatile__ (\"isync\"  : : : \"memory\");\n-\n-inline void   OrderAccess::loadload()   { inlasm_lwsync(); }\n-inline void   OrderAccess::storestore() { inlasm_lwsync(); }\n-inline void   OrderAccess::loadstore()  { inlasm_lwsync(); }\n-inline void   OrderAccess::storeload()  { inlasm_sync();   }\n-\n-inline void   OrderAccess::acquire()    { inlasm_lwsync(); }\n-inline void   OrderAccess::release()    { inlasm_lwsync(); }\n-inline void   OrderAccess::fence()      { inlasm_sync();   }\n-inline void   OrderAccess::cross_modify_fence_impl()\n-                                        { inlasm_isync();  }\n-\n-#undef inlasm_sync\n-#undef inlasm_lwsync\n-#undef inlasm_eieio\n-#undef inlasm_isync\n-\n-#endif \/\/ OS_CPU_LINUX_PPC_ORDERACCESS_LINUX_PPC_HPP\n+\/\/ Including inline assembler functions that are shared between multiple PPC64 platforms.\n+#include \"orderAccess_ppc.hpp\"\n","filename":"src\/hotspot\/os_cpu\/linux_ppc\/orderAccess_linux_ppc.hpp","additions":4,"deletions":65,"binary":false,"changes":69,"status":"modified"}]}