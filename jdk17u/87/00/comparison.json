{"files":[{"patch":"@@ -1316,0 +1316,6 @@\n+\n+  void ghash_modmul (FloatRegister result,\n+                     FloatRegister result_lo, FloatRegister result_hi, FloatRegister b,\n+                     FloatRegister a, FloatRegister vzr, FloatRegister a1_xor_a0, FloatRegister p,\n+                     FloatRegister t1, FloatRegister t2, FloatRegister t3);\n+  void ghash_load_wide(int index, Register data, FloatRegister result, FloatRegister state);\n@@ -1321,0 +1327,20 @@\n+  void ghash_multiply(FloatRegister result_lo, FloatRegister result_hi,\n+                      FloatRegister a, FloatRegister b, FloatRegister a1_xor_a0,\n+                      FloatRegister tmp1, FloatRegister tmp2, FloatRegister tmp3);\n+  void ghash_multiply_wide(int index,\n+                           FloatRegister result_lo, FloatRegister result_hi,\n+                           FloatRegister a, FloatRegister b, FloatRegister a1_xor_a0,\n+                           FloatRegister tmp1, FloatRegister tmp2, FloatRegister tmp3);\n+  void ghash_reduce(FloatRegister result, FloatRegister lo, FloatRegister hi,\n+                    FloatRegister p, FloatRegister z, FloatRegister t1);\n+  void ghash_reduce_wide(int index, FloatRegister result, FloatRegister lo, FloatRegister hi,\n+                    FloatRegister p, FloatRegister z, FloatRegister t1);\n+  void ghash_processBlocks_wide(address p, Register state, Register subkeyH,\n+                                Register data, Register blocks, int unrolls);\n+\n+\n+  void aesenc_loadkeys(Register key, Register keylen);\n+  void aesecb_encrypt(Register from, Register to, Register keylen,\n+                      FloatRegister data = v0, int unrolls = 1);\n+  void aesecb_decrypt(Register from, Register to, Register key, Register keylen);\n+  void aes_round(FloatRegister input, FloatRegister subkey);\n","filename":"src\/hotspot\/cpu\/aarch64\/macroAssembler_aarch64.hpp","additions":26,"deletions":0,"binary":false,"changes":26,"status":"modified"},{"patch":"@@ -0,0 +1,680 @@\n+\/*\n+ * Copyright (c) 2003, 2021, Oracle and\/or its affiliates. All rights reserved.\n+ * Copyright (c) 2014, 2021, Red Hat Inc. All rights reserved.\n+ * DO NOT ALTER OR REMOVE COPYRIGHT NOTICES OR THIS FILE HEADER.\n+ *\n+ * This code is free software; you can redistribute it and\/or modify it\n+ * under the terms of the GNU General Public License version 2 only, as\n+ * published by the Free Software Foundation.\n+ *\n+ * This code is distributed in the hope that it will be useful, but WITHOUT\n+ * ANY WARRANTY; without even the implied warranty of MERCHANTABILITY or\n+ * FITNESS FOR A PARTICULAR PURPOSE.  See the GNU General Public License\n+ * version 2 for more details (a copy is included in the LICENSE file that\n+ * accompanied this code).\n+ *\n+ * You should have received a copy of the GNU General Public License version\n+ * 2 along with this work; if not, write to the Free Software Foundation,\n+ * Inc., 51 Franklin St, Fifth Floor, Boston, MA 02110-1301 USA.\n+ *\n+ * Please contact Oracle, 500 Oracle Parkway, Redwood Shores, CA 94065 USA\n+ * or visit www.oracle.com if you need additional information or have any\n+ * questions.\n+ *\n+ *\/\n+\n+#include \"precompiled.hpp\"\n+\n+#include \"asm\/assembler.hpp\"\n+#include \"asm\/assembler.inline.hpp\"\n+#include \"macroAssembler_aarch64.hpp\"\n+#include \"memory\/resourceArea.hpp\"\n+#include \"runtime\/stubRoutines.hpp\"\n+\n+void MacroAssembler::aesecb_decrypt(Register from, Register to, Register key, Register keylen) {\n+  Label L_doLast;\n+\n+  ld1(v0, T16B, from); \/\/ get 16 bytes of input\n+\n+  ld1(v5, T16B, post(key, 16));\n+  rev32(v5, T16B, v5);\n+\n+  ld1(v1, v2, v3, v4, T16B, post(key, 64));\n+  rev32(v1, T16B, v1);\n+  rev32(v2, T16B, v2);\n+  rev32(v3, T16B, v3);\n+  rev32(v4, T16B, v4);\n+  aesd(v0, v1);\n+  aesimc(v0, v0);\n+  aesd(v0, v2);\n+  aesimc(v0, v0);\n+  aesd(v0, v3);\n+  aesimc(v0, v0);\n+  aesd(v0, v4);\n+  aesimc(v0, v0);\n+\n+  ld1(v1, v2, v3, v4, T16B, post(key, 64));\n+  rev32(v1, T16B, v1);\n+  rev32(v2, T16B, v2);\n+  rev32(v3, T16B, v3);\n+  rev32(v4, T16B, v4);\n+  aesd(v0, v1);\n+  aesimc(v0, v0);\n+  aesd(v0, v2);\n+  aesimc(v0, v0);\n+  aesd(v0, v3);\n+  aesimc(v0, v0);\n+  aesd(v0, v4);\n+  aesimc(v0, v0);\n+\n+  ld1(v1, v2, T16B, post(key, 32));\n+  rev32(v1, T16B, v1);\n+  rev32(v2, T16B, v2);\n+\n+  cmpw(keylen, 44);\n+  br(Assembler::EQ, L_doLast);\n+\n+  aesd(v0, v1);\n+  aesimc(v0, v0);\n+  aesd(v0, v2);\n+  aesimc(v0, v0);\n+\n+  ld1(v1, v2, T16B, post(key, 32));\n+  rev32(v1, T16B, v1);\n+  rev32(v2, T16B, v2);\n+\n+  cmpw(keylen, 52);\n+  br(Assembler::EQ, L_doLast);\n+\n+  aesd(v0, v1);\n+  aesimc(v0, v0);\n+  aesd(v0, v2);\n+  aesimc(v0, v0);\n+\n+  ld1(v1, v2, T16B, post(key, 32));\n+  rev32(v1, T16B, v1);\n+  rev32(v2, T16B, v2);\n+\n+  bind(L_doLast);\n+\n+  aesd(v0, v1);\n+  aesimc(v0, v0);\n+  aesd(v0, v2);\n+\n+  eor(v0, T16B, v0, v5);\n+\n+  st1(v0, T16B, to);\n+\n+  \/\/ Preserve the address of the start of the key\n+  sub(key, key, keylen, LSL, exact_log2(sizeof (jint)));\n+}\n+\n+\/\/ Load expanded key into v17..v31\n+void MacroAssembler::aesenc_loadkeys(Register key, Register keylen) {\n+  Label L_loadkeys_44, L_loadkeys_52;\n+  cmpw(keylen, 52);\n+  br(Assembler::LO, L_loadkeys_44);\n+  br(Assembler::EQ, L_loadkeys_52);\n+\n+  ld1(v17, v18,  T16B,  post(key, 32));\n+  rev32(v17,  T16B, v17);\n+  rev32(v18,  T16B, v18);\n+  bind(L_loadkeys_52);\n+  ld1(v19, v20,  T16B,  post(key, 32));\n+  rev32(v19,  T16B, v19);\n+  rev32(v20,  T16B, v20);\n+  bind(L_loadkeys_44);\n+  ld1(v21, v22, v23, v24,  T16B,  post(key, 64));\n+  rev32(v21,  T16B, v21);\n+  rev32(v22,  T16B, v22);\n+  rev32(v23,  T16B, v23);\n+  rev32(v24,  T16B, v24);\n+  ld1(v25, v26, v27, v28,  T16B,  post(key, 64));\n+  rev32(v25,  T16B, v25);\n+  rev32(v26,  T16B, v26);\n+  rev32(v27,  T16B, v27);\n+  rev32(v28,  T16B, v28);\n+  ld1(v29, v30, v31,  T16B, post(key, 48));\n+  rev32(v29,  T16B, v29);\n+  rev32(v30,  T16B, v30);\n+  rev32(v31,  T16B, v31);\n+\n+  \/\/ Preserve the address of the start of the key\n+  sub(key, key, keylen, LSL, exact_log2(sizeof (jint)));\n+}\n+\n+\/\/ NeoverseTM N1Software Optimization Guide:\n+\/\/ Adjacent AESE\/AESMC instruction pairs and adjacent AESD\/AESIMC\n+\/\/ instruction pairs will exhibit the performance characteristics\n+\/\/ described in Section 4.6.\n+void MacroAssembler::aes_round(FloatRegister input, FloatRegister subkey) {\n+  aese(input, subkey); aesmc(input, input);\n+}\n+\n+\/\/ KernelGenerator\n+\/\/\n+\/\/ The abstract base class of an unrolled function generator.\n+\/\/ Subclasses override generate(), length(), and next() to generate\n+\/\/ unrolled and interleaved functions.\n+\/\/\n+\/\/ The core idea is that a subclass defines a method which generates\n+\/\/ the base case of a function and a method to generate a clone of it,\n+\/\/ shifted to a different set of registers. KernelGenerator will then\n+\/\/ generate several interleaved copies of the function, with each one\n+\/\/ using a different set of registers.\n+\n+\/\/ The subclass must implement three methods: length(), which is the\n+\/\/ number of instruction bundles in the intrinsic, generate(int n)\n+\/\/ which emits the nth instruction bundle in the intrinsic, and next()\n+\/\/ which takes an instance of the generator and returns a version of it,\n+\/\/ shifted to a new set of registers.\n+\n+class KernelGenerator: public MacroAssembler {\n+protected:\n+  const int _unrolls;\n+public:\n+  KernelGenerator(Assembler *as, int unrolls)\n+    : MacroAssembler(as->code()), _unrolls(unrolls) { }\n+  virtual void generate(int index) = 0;\n+  virtual int length() = 0;\n+  virtual KernelGenerator *next() = 0;\n+  int unrolls() { return _unrolls; }\n+  void unroll();\n+};\n+\n+void KernelGenerator::unroll() {\n+  ResourceMark rm;\n+  KernelGenerator **generators\n+    = NEW_RESOURCE_ARRAY(KernelGenerator *, unrolls());\n+\n+  generators[0] = this;\n+  for (int i = 1; i < unrolls(); i++) {\n+    generators[i] = generators[i-1]->next();\n+  }\n+\n+  for (int j = 0; j < length(); j++) {\n+    for (int i = 0; i < unrolls(); i++) {\n+      generators[i]->generate(j);\n+    }\n+  }\n+}\n+\n+\/\/ An unrolled and interleaved generator for AES encryption.\n+class AESKernelGenerator: public KernelGenerator {\n+  Register _from, _to;\n+  const Register _keylen;\n+  FloatRegister _data;\n+  const FloatRegister _subkeys;\n+  bool _once;\n+  Label _rounds_44, _rounds_52;\n+\n+public:\n+  AESKernelGenerator(Assembler *as, int unrolls,\n+                     Register from, Register to, Register keylen, FloatRegister data,\n+                     FloatRegister subkeys, bool once = true)\n+    : KernelGenerator(as, unrolls),\n+      _from(from), _to(to), _keylen(keylen), _data(data),\n+      _subkeys(subkeys), _once(once) {\n+  }\n+\n+  virtual void generate(int index) {\n+    switch (index) {\n+    case  0:\n+      if (_from != noreg) {\n+        ld1(_data, T16B, _from); \/\/ get 16 bytes of input\n+      }\n+      break;\n+    case  1:\n+      if (_once) {\n+        cmpw(_keylen, 52);\n+        br(Assembler::LO, _rounds_44);\n+        br(Assembler::EQ, _rounds_52);\n+      }\n+      break;\n+    case  2:  aes_round(_data, _subkeys +  0);  break;\n+    case  3:  aes_round(_data, _subkeys +  1);  break;\n+    case  4:\n+      if (_once)  bind(_rounds_52);\n+      break;\n+    case  5:  aes_round(_data, _subkeys +  2);  break;\n+    case  6:  aes_round(_data, _subkeys +  3);  break;\n+    case  7:\n+      if (_once)  bind(_rounds_44);\n+      break;\n+    case  8:  aes_round(_data, _subkeys +  4);  break;\n+    case  9:  aes_round(_data, _subkeys +  5);  break;\n+    case 10:  aes_round(_data, _subkeys +  6);  break;\n+    case 11:  aes_round(_data, _subkeys +  7);  break;\n+    case 12:  aes_round(_data, _subkeys +  8);  break;\n+    case 13:  aes_round(_data, _subkeys +  9);  break;\n+    case 14:  aes_round(_data, _subkeys + 10);  break;\n+    case 15:  aes_round(_data, _subkeys + 11);  break;\n+    case 16:  aes_round(_data, _subkeys + 12);  break;\n+    case 17:  aese(_data, _subkeys + 13);  break;\n+    case 18:  eor(_data, T16B, _data, _subkeys + 14);  break;\n+    case 19:\n+      if (_to != noreg) {\n+        st1(_data, T16B, _to);\n+      }\n+      break;\n+    default: ShouldNotReachHere();\n+    }\n+  }\n+\n+  virtual KernelGenerator *next() {\n+    return new AESKernelGenerator(this, _unrolls,\n+                                  _from, _to, _keylen,\n+                                  _data + 1, _subkeys, \/*once*\/false);\n+  }\n+\n+  virtual int length() { return 20; }\n+};\n+\n+\/\/ Uses expanded key in v17..v31\n+\/\/ Returns encrypted values in inputs.\n+\/\/ If to != noreg, store value at to; likewise from\n+\/\/ Preserves key, keylen\n+\/\/ Increments from, to\n+\/\/ Input data in v0, v1, ...\n+\/\/ unrolls controls the number of times to unroll the generated function\n+void MacroAssembler::aesecb_encrypt(Register from, Register to, Register keylen,\n+                                    FloatRegister data, int unrolls) {\n+  AESKernelGenerator(this, unrolls, from, to, keylen, data, v17) .unroll();\n+}\n+\n+\/\/ ghash_multiply and ghash_reduce are the non-unrolled versions of\n+\/\/ the GHASH function generators.\n+void MacroAssembler::ghash_multiply(FloatRegister result_lo, FloatRegister result_hi,\n+                                     FloatRegister a, FloatRegister b, FloatRegister a1_xor_a0,\n+                                     FloatRegister tmp1, FloatRegister tmp2, FloatRegister tmp3) {\n+  \/\/ Karatsuba multiplication performs a 128*128 -> 256-bit\n+  \/\/ multiplication in three 128-bit multiplications and a few\n+  \/\/ additions.\n+  \/\/\n+  \/\/ (C1:C0) = A1*B1, (D1:D0) = A0*B0, (E1:E0) = (A0+A1)(B0+B1)\n+  \/\/ (A1:A0)(B1:B0) = C1:(C0+C1+D1+E1):(D1+C0+D0+E0):D0\n+  \/\/\n+  \/\/ Inputs:\n+  \/\/\n+  \/\/ A0 in a.d[0]     (subkey)\n+  \/\/ A1 in a.d[1]\n+  \/\/ (A1+A0) in a1_xor_a0.d[0]\n+  \/\/\n+  \/\/ B0 in b.d[0]     (state)\n+  \/\/ B1 in b.d[1]\n+\n+  ext(tmp1, T16B, b, b, 0x08);\n+  pmull2(result_hi, T1Q, b, a, T2D);  \/\/ A1*B1\n+  eor(tmp1, T16B, tmp1, b);           \/\/ (B1+B0)\n+  pmull(result_lo,  T1Q, b, a, T1D);  \/\/ A0*B0\n+  pmull(tmp2, T1Q, tmp1, a1_xor_a0, T1D); \/\/ (A1+A0)(B1+B0)\n+\n+  ext(tmp1, T16B, result_lo, result_hi, 0x08);\n+  eor(tmp3, T16B, result_hi, result_lo); \/\/ A1*B1+A0*B0\n+  eor(tmp2, T16B, tmp2, tmp1);\n+  eor(tmp2, T16B, tmp2, tmp3);\n+\n+  \/\/ Register pair <result_hi:result_lo> holds the result of carry-less multiplication\n+  ins(result_hi, D, tmp2, 0, 1);\n+  ins(result_lo, D, tmp2, 1, 0);\n+}\n+\n+void MacroAssembler::ghash_reduce(FloatRegister result, FloatRegister lo, FloatRegister hi,\n+                  FloatRegister p, FloatRegister vzr, FloatRegister t1) {\n+  const FloatRegister t0 = result;\n+\n+  \/\/ The GCM field polynomial f is z^128 + p(z), where p =\n+  \/\/ z^7+z^2+z+1.\n+  \/\/\n+  \/\/    z^128 === -p(z)  (mod (z^128 + p(z)))\n+  \/\/\n+  \/\/ so, given that the product we're reducing is\n+  \/\/    a == lo + hi * z^128\n+  \/\/ substituting,\n+  \/\/      === lo - hi * p(z)  (mod (z^128 + p(z)))\n+  \/\/\n+  \/\/ we reduce by multiplying hi by p(z) and subtracting the result\n+  \/\/ from (i.e. XORing it with) lo.  Because p has no nonzero high\n+  \/\/ bits we can do this with two 64-bit multiplications, lo*p and\n+  \/\/ hi*p.\n+\n+  pmull2(t0, T1Q, hi, p, T2D);\n+  ext(t1, T16B, t0, vzr, 8);\n+  eor(hi, T16B, hi, t1);\n+  ext(t1, T16B, vzr, t0, 8);\n+  eor(lo, T16B, lo, t1);\n+  pmull(t0, T1Q, hi, p, T1D);\n+  eor(result, T16B, lo, t0);\n+}\n+\n+class GHASHMultiplyGenerator: public KernelGenerator {\n+  FloatRegister _result_lo, _result_hi, _b,\n+    _a, _vzr, _a1_xor_a0, _p,\n+    _tmp1, _tmp2, _tmp3;\n+\n+public:\n+  GHASHMultiplyGenerator(Assembler *as, int unrolls,\n+                         \/* offsetted registers *\/\n+                         FloatRegister result_lo, FloatRegister result_hi,\n+                         FloatRegister b,\n+                         \/* non-offsetted (shared) registers *\/\n+                         FloatRegister a, FloatRegister a1_xor_a0, FloatRegister p, FloatRegister vzr,\n+                         \/* offseted (temp) registers *\/\n+                         FloatRegister tmp1, FloatRegister tmp2, FloatRegister tmp3)\n+    : KernelGenerator(as, unrolls),\n+      _result_lo(result_lo), _result_hi(result_hi), _b(b),\n+      _a(a), _vzr(vzr), _a1_xor_a0(a1_xor_a0), _p(p),\n+      _tmp1(tmp1), _tmp2(tmp2), _tmp3(tmp3) { }\n+\n+  int register_stride = 7;\n+\n+  virtual void generate(int index) {\n+    \/\/ Karatsuba multiplication performs a 128*128 -> 256-bit\n+    \/\/ multiplication in three 128-bit multiplications and a few\n+    \/\/ additions.\n+    \/\/\n+    \/\/ (C1:C0) = A1*B1, (D1:D0) = A0*B0, (E1:E0) = (A0+A1)(B0+B1)\n+    \/\/ (A1:A0)(B1:B0) = C1:(C0+C1+D1+E1):(D1+C0+D0+E0):D0\n+    \/\/\n+    \/\/ Inputs:\n+    \/\/\n+    \/\/ A0 in a.d[0]     (subkey)\n+    \/\/ A1 in a.d[1]\n+    \/\/ (A1+A0) in a1_xor_a0.d[0]\n+    \/\/\n+    \/\/ B0 in b.d[0]     (state)\n+    \/\/ B1 in b.d[1]\n+\n+    switch (index) {\n+      case  0:  ext(_tmp1, T16B, _b, _b, 0x08);  break;\n+      case  1:  pmull2(_result_hi, T1Q, _b, _a, T2D);  \/\/ A1*B1\n+        break;\n+      case  2:  eor(_tmp1, T16B, _tmp1, _b);           \/\/ (B1+B0)\n+        break;\n+      case  3:  pmull(_result_lo,  T1Q, _b, _a, T1D);  \/\/ A0*B0\n+        break;\n+      case  4:  pmull(_tmp2, T1Q, _tmp1, _a1_xor_a0, T1D); \/\/ (A1+A0)(B1+B0)\n+        break;\n+\n+      case  5:  ext(_tmp1, T16B, _result_lo, _result_hi, 0x08);  break;\n+      case  6:  eor(_tmp3, T16B, _result_hi, _result_lo); \/\/ A1*B1+A0*B0\n+        break;\n+      case  7:  eor(_tmp2, T16B, _tmp2, _tmp1);  break;\n+      case  8:  eor(_tmp2, T16B, _tmp2, _tmp3);  break;\n+\n+        \/\/ Register pair <_result_hi:_result_lo> holds the _result of carry-less multiplication\n+      case  9:  ins(_result_hi, D, _tmp2, 0, 1);  break;\n+      case 10:  ins(_result_lo, D, _tmp2, 1, 0);  break;\n+      default: ShouldNotReachHere();\n+    }\n+  }\n+\n+  virtual KernelGenerator *next() {\n+    GHASHMultiplyGenerator *result = new GHASHMultiplyGenerator(*this);\n+    result->_result_lo += register_stride;\n+    result->_result_hi += register_stride;\n+    result->_b += register_stride;\n+    result->_tmp1 += register_stride;\n+    result->_tmp2 += register_stride;\n+    result->_tmp3 += register_stride;\n+    return result;\n+  }\n+\n+  virtual int length() { return 11; }\n+};\n+\n+\/\/ Reduce the 128-bit product in hi:lo by the GCM field polynomial.\n+\/\/ The FloatRegister argument called data is optional: if it is a\n+\/\/ valid register, we interleave LD1 instructions with the\n+\/\/ reduction. This is to reduce latency next time around the loop.\n+class GHASHReduceGenerator: public KernelGenerator {\n+  FloatRegister _result, _lo, _hi, _p, _vzr, _data, _t1;\n+  int _once;\n+public:\n+  GHASHReduceGenerator(Assembler *as, int unrolls,\n+                       \/* offsetted registers *\/\n+                       FloatRegister result, FloatRegister lo, FloatRegister hi,\n+                       \/* non-offsetted (shared) registers *\/\n+                       FloatRegister p, FloatRegister vzr, FloatRegister data,\n+                       \/* offseted (temp) registers *\/\n+                       FloatRegister t1)\n+    : KernelGenerator(as, unrolls),\n+      _result(result), _lo(lo), _hi(hi),\n+      _p(p), _vzr(vzr), _data(data), _t1(t1), _once(true) { }\n+\n+  int register_stride = 7;\n+\n+  virtual void generate(int index) {\n+    const FloatRegister t0 = _result;\n+\n+    switch (index) {\n+      \/\/ The GCM field polynomial f is z^128 + p(z), where p =\n+      \/\/ z^7+z^2+z+1.\n+      \/\/\n+      \/\/    z^128 === -p(z)  (mod (z^128 + p(z)))\n+      \/\/\n+      \/\/ so, given that the product we're reducing is\n+      \/\/    a == lo + hi * z^128\n+      \/\/ substituting,\n+      \/\/      === lo - hi * p(z)  (mod (z^128 + p(z)))\n+      \/\/\n+      \/\/ we reduce by multiplying hi by p(z) and subtracting the _result\n+      \/\/ from (i.e. XORing it with) lo.  Because p has no nonzero high\n+      \/\/ bits we can do this with two 64-bit multiplications, lo*p and\n+      \/\/ hi*p.\n+\n+      case  0:  pmull2(t0, T1Q, _hi, _p, T2D);  break;\n+      case  1:  ext(_t1, T16B, t0, _vzr, 8);  break;\n+      case  2:  eor(_hi, T16B, _hi, _t1);  break;\n+      case  3:  ext(_t1, T16B, _vzr, t0, 8);  break;\n+      case  4:  eor(_lo, T16B, _lo, _t1);  break;\n+      case  5:  pmull(t0, T1Q, _hi, _p, T1D);  break;\n+      case  6:  eor(_result, T16B, _lo, t0);  break;\n+      default: ShouldNotReachHere();\n+    }\n+\n+    \/\/ Sprinkle load instructions into the generated instructions\n+    if (_data->is_valid() && _once) {\n+      assert(length() >= unrolls(), \"not enough room for inteleaved loads\");\n+      if (index < unrolls()) {\n+        ld1((_data + index*register_stride), T16B, post(r2, 0x10));\n+      }\n+    }\n+  }\n+\n+  virtual KernelGenerator *next() {\n+    GHASHReduceGenerator *result = new GHASHReduceGenerator(*this);\n+    result->_result += register_stride;\n+    result->_hi += register_stride;\n+    result->_lo += register_stride;\n+    result->_t1 += register_stride;\n+    result->_once = false;\n+    return result;\n+  }\n+\n+ int length() { return 7; }\n+};\n+\n+\/\/ Perform a GHASH multiply\/reduce on a single FloatRegister.\n+void MacroAssembler::ghash_modmul(FloatRegister result,\n+                                  FloatRegister result_lo, FloatRegister result_hi, FloatRegister b,\n+                                  FloatRegister a, FloatRegister vzr, FloatRegister a1_xor_a0, FloatRegister p,\n+                                  FloatRegister t1, FloatRegister t2, FloatRegister t3) {\n+  ghash_multiply(result_lo, result_hi, a, b, a1_xor_a0, t1, t2, t3);\n+  ghash_reduce(result, result_lo, result_hi, p, vzr, t1);\n+}\n+\n+\/\/ Interleaved GHASH processing.\n+\/\/\n+\/\/ Clobbers all vector registers.\n+\/\/\n+void MacroAssembler::ghash_processBlocks_wide(address field_polynomial, Register state,\n+                                              Register subkeyH,\n+                                              Register data, Register blocks, int unrolls) {\n+  int register_stride = 7;\n+\n+  \/\/ Bafflingly, GCM uses little-endian for the byte order, but\n+  \/\/ big-endian for the bit order.  For example, the polynomial 1 is\n+  \/\/ represented as the 16-byte string 80 00 00 00 | 12 bytes of 00.\n+  \/\/\n+  \/\/ So, we must either reverse the bytes in each word and do\n+  \/\/ everything big-endian or reverse the bits in each byte and do\n+  \/\/ it little-endian.  On AArch64 it's more idiomatic to reverse\n+  \/\/ the bits in each byte (we have an instruction, RBIT, to do\n+  \/\/ that) and keep the data in little-endian bit order throught the\n+  \/\/ calculation, bit-reversing the inputs and outputs.\n+\n+  assert(unrolls * register_stride < 32, \"out of registers\");\n+\n+  FloatRegister a1_xor_a0 = v28;\n+  FloatRegister Hprime = v29;\n+  FloatRegister vzr = v30;\n+  FloatRegister p = v31;\n+  eor(vzr, T16B, vzr, vzr); \/\/ zero register\n+\n+  ldrq(p, field_polynomial);    \/\/ The field polynomial\n+\n+  ldrq(v0, Address(state));\n+  ldrq(Hprime, Address(subkeyH));\n+\n+  rev64(v0, T16B, v0);          \/\/ Bit-reverse words in state and subkeyH\n+  rbit(v0, T16B, v0);\n+  rev64(Hprime, T16B, Hprime);\n+  rbit(Hprime, T16B, Hprime);\n+\n+  \/\/ Powers of H -> Hprime\n+\n+  Label already_calculated, done;\n+  {\n+    \/\/ The first time around we'll have to calculate H**2, H**3, etc.\n+    \/\/ Look at the largest power of H in the subkeyH array to see if\n+    \/\/ it's already been calculated.\n+    ldp(rscratch1, rscratch2, Address(subkeyH, 16 * (unrolls - 1)));\n+    orr(rscratch1, rscratch1, rscratch2);\n+    cbnz(rscratch1, already_calculated);\n+\n+    orr(v6, T16B, Hprime, Hprime);  \/\/ Start with H in v6 and Hprime\n+    for (int i = 1; i < unrolls; i++) {\n+      ext(a1_xor_a0, T16B, Hprime, Hprime, 0x08); \/\/ long-swap subkeyH into a1_xor_a0\n+      eor(a1_xor_a0, T16B, a1_xor_a0, Hprime);    \/\/ xor subkeyH into subkeyL (Karatsuba: (A1+A0))\n+      ghash_modmul(\/*result*\/v6, \/*result_lo*\/v5, \/*result_hi*\/v4, \/*b*\/v6,\n+                   Hprime, vzr, a1_xor_a0, p,\n+                   \/*temps*\/v1, v3, v2);\n+      rev64(v1, T16B, v6);\n+      rbit(v1, T16B, v1);\n+      strq(v1, Address(subkeyH, 16 * i));\n+    }\n+    b(done);\n+  }\n+  {\n+    bind(already_calculated);\n+\n+    \/\/ Load the largest power of H we need into v6.\n+    ldrq(v6, Address(subkeyH, 16 * (unrolls - 1)));\n+    rev64(v6, T16B, v6);\n+    rbit(v6, T16B, v6);\n+  }\n+  bind(done);\n+\n+  orr(Hprime, T16B, v6, v6);     \/\/ Move H ** unrolls into Hprime\n+\n+  \/\/ Hprime contains (H ** 1, H ** 2, ... H ** unrolls)\n+  \/\/ v0 contains the initial state. Clear the others.\n+  for (int i = 1; i < unrolls; i++) {\n+    int ofs = register_stride * i;\n+    eor(ofs+v0, T16B, ofs+v0, ofs+v0); \/\/ zero each state register\n+  }\n+\n+  ext(a1_xor_a0, T16B, Hprime, Hprime, 0x08); \/\/ long-swap subkeyH into a1_xor_a0\n+  eor(a1_xor_a0, T16B, a1_xor_a0, Hprime);    \/\/ xor subkeyH into subkeyL (Karatsuba: (A1+A0))\n+\n+  \/\/ Load #unrolls blocks of data\n+  for (int ofs = 0; ofs < unrolls * register_stride; ofs += register_stride) {\n+    ld1(v2+ofs, T16B, post(data, 0x10));\n+  }\n+\n+  \/\/ Register assignments, replicated across 4 clones, v0 ... v23\n+  \/\/\n+  \/\/ v0: input \/ output: current state, result of multiply\/reduce\n+  \/\/ v1: temp\n+  \/\/ v2: input: one block of data (the ciphertext)\n+  \/\/     also used as a temp once the data has been consumed\n+  \/\/ v3: temp\n+  \/\/ v4: output: high part of product\n+  \/\/ v5: output: low part ...\n+  \/\/ v6: unused\n+  \/\/\n+  \/\/ Not replicated:\n+  \/\/\n+  \/\/ v28: High part of H xor low part of H'\n+  \/\/ v29: H' (hash subkey)\n+  \/\/ v30: zero\n+  \/\/ v31: Reduction polynomial of the Galois field\n+\n+  \/\/ Inner loop.\n+  \/\/ Do the whole load\/add\/multiply\/reduce over all our data except\n+  \/\/ the last few rows.\n+  {\n+    Label L_ghash_loop;\n+    bind(L_ghash_loop);\n+\n+    \/\/ Prefetching doesn't help here. In fact, on Neoverse N1 it's worse.\n+    \/\/ prfm(Address(data, 128), PLDL1KEEP);\n+\n+    \/\/ Xor data into current state\n+    for (int ofs = 0; ofs < unrolls * register_stride; ofs += register_stride) {\n+      rbit((v2+ofs), T16B, (v2+ofs));\n+      eor((v2+ofs), T16B, v0+ofs, (v2+ofs));   \/\/ bit-swapped data ^ bit-swapped state\n+    }\n+\n+    \/\/ Generate fully-unrolled multiply-reduce in two stages.\n+\n+    GHASHMultiplyGenerator(this, unrolls,\n+                           \/*result_lo*\/v5, \/*result_hi*\/v4, \/*data*\/v2,\n+                           Hprime, a1_xor_a0, p, vzr,\n+                           \/*temps*\/v1, v3, \/* reuse b*\/v2) .unroll();\n+\n+    \/\/ NB: GHASHReduceGenerator also loads the next #unrolls blocks of\n+    \/\/ data into v0, v0+ofs, the current state.\n+    GHASHReduceGenerator (this, unrolls,\n+                          \/*result*\/v0, \/*lo*\/v5, \/*hi*\/v4, p, vzr,\n+                          \/*data*\/v2, \/*temp*\/v3) .unroll();\n+\n+    sub(blocks, blocks, unrolls);\n+    cmp(blocks, (unsigned char)(unrolls * 2));\n+    br(GE, L_ghash_loop);\n+  }\n+\n+  \/\/ Merge the #unrolls states.  Note that the data for the next\n+  \/\/ iteration has already been loaded into v4, v4+ofs, etc...\n+\n+  \/\/ First, we multiply\/reduce each clone by the appropriate power of H.\n+  for (int i = 0; i < unrolls; i++) {\n+    int ofs = register_stride * i;\n+    ldrq(Hprime, Address(subkeyH, 16 * (unrolls - i - 1)));\n+\n+    rbit(v2+ofs, T16B, v2+ofs);\n+    eor(v2+ofs, T16B, ofs+v0, v2+ofs);   \/\/ bit-swapped data ^ bit-swapped state\n+\n+    rev64(Hprime, T16B, Hprime);\n+    rbit(Hprime, T16B, Hprime);\n+    ext(a1_xor_a0, T16B, Hprime, Hprime, 0x08); \/\/ long-swap subkeyH into a1_xor_a0\n+    eor(a1_xor_a0, T16B, a1_xor_a0, Hprime);    \/\/ xor subkeyH into subkeyL (Karatsuba: (A1+A0))\n+    ghash_modmul(\/*result*\/v0+ofs, \/*result_lo*\/v5+ofs, \/*result_hi*\/v4+ofs, \/*b*\/v2+ofs,\n+                 Hprime, vzr, a1_xor_a0, p,\n+                 \/*temps*\/v1+ofs, v3+ofs, \/* reuse b*\/v2+ofs);\n+  }\n+\n+  \/\/ Then we sum the results.\n+  for (int i = 0; i < unrolls - 1; i++) {\n+    int ofs = register_stride * i;\n+    eor(v0, T16B, v0, v0 + register_stride + ofs);\n+  }\n+\n+  sub(blocks, blocks, (unsigned char)unrolls);\n+\n+  \/\/ And finally bit-reverse the state back to big endian.\n+  rev64(v0, T16B, v0);\n+  rbit(v0, T16B, v0);\n+  st1(v0, T16B, state);\n+}\n","filename":"src\/hotspot\/cpu\/aarch64\/macroAssembler_aarch64_aes.cpp","additions":680,"deletions":0,"binary":false,"changes":680,"status":"added"},{"patch":"@@ -2563,2 +2563,0 @@\n-    Label L_doLast;\n-\n@@ -2575,69 +2573,2 @@\n-    __ ld1(v0, __ T16B, from); \/\/ get 16 bytes of input\n-\n-    __ ld1(v1, v2, v3, v4, __ T16B, __ post(key, 64));\n-    __ rev32(v1, __ T16B, v1);\n-    __ rev32(v2, __ T16B, v2);\n-    __ rev32(v3, __ T16B, v3);\n-    __ rev32(v4, __ T16B, v4);\n-    __ aese(v0, v1);\n-    __ aesmc(v0, v0);\n-    __ aese(v0, v2);\n-    __ aesmc(v0, v0);\n-    __ aese(v0, v3);\n-    __ aesmc(v0, v0);\n-    __ aese(v0, v4);\n-    __ aesmc(v0, v0);\n-\n-    __ ld1(v1, v2, v3, v4, __ T16B, __ post(key, 64));\n-    __ rev32(v1, __ T16B, v1);\n-    __ rev32(v2, __ T16B, v2);\n-    __ rev32(v3, __ T16B, v3);\n-    __ rev32(v4, __ T16B, v4);\n-    __ aese(v0, v1);\n-    __ aesmc(v0, v0);\n-    __ aese(v0, v2);\n-    __ aesmc(v0, v0);\n-    __ aese(v0, v3);\n-    __ aesmc(v0, v0);\n-    __ aese(v0, v4);\n-    __ aesmc(v0, v0);\n-\n-    __ ld1(v1, v2, __ T16B, __ post(key, 32));\n-    __ rev32(v1, __ T16B, v1);\n-    __ rev32(v2, __ T16B, v2);\n-\n-    __ cmpw(keylen, 44);\n-    __ br(Assembler::EQ, L_doLast);\n-\n-    __ aese(v0, v1);\n-    __ aesmc(v0, v0);\n-    __ aese(v0, v2);\n-    __ aesmc(v0, v0);\n-\n-    __ ld1(v1, v2, __ T16B, __ post(key, 32));\n-    __ rev32(v1, __ T16B, v1);\n-    __ rev32(v2, __ T16B, v2);\n-\n-    __ cmpw(keylen, 52);\n-    __ br(Assembler::EQ, L_doLast);\n-\n-    __ aese(v0, v1);\n-    __ aesmc(v0, v0);\n-    __ aese(v0, v2);\n-    __ aesmc(v0, v0);\n-\n-    __ ld1(v1, v2, __ T16B, __ post(key, 32));\n-    __ rev32(v1, __ T16B, v1);\n-    __ rev32(v2, __ T16B, v2);\n-\n-    __ BIND(L_doLast);\n-\n-    __ aese(v0, v1);\n-    __ aesmc(v0, v0);\n-    __ aese(v0, v2);\n-\n-    __ ld1(v1, __ T16B, key);\n-    __ rev32(v1, __ T16B, v1);\n-    __ eor(v0, __ T16B, v0, v1);\n-\n-    __ st1(v0, __ T16B, to);\n+    __ aesenc_loadkeys(key, keylen);\n+    __ aesecb_encrypt(from, to, keylen);\n@@ -2676,70 +2607,1 @@\n-    __ ld1(v0, __ T16B, from); \/\/ get 16 bytes of input\n-\n-    __ ld1(v5, __ T16B, __ post(key, 16));\n-    __ rev32(v5, __ T16B, v5);\n-\n-    __ ld1(v1, v2, v3, v4, __ T16B, __ post(key, 64));\n-    __ rev32(v1, __ T16B, v1);\n-    __ rev32(v2, __ T16B, v2);\n-    __ rev32(v3, __ T16B, v3);\n-    __ rev32(v4, __ T16B, v4);\n-    __ aesd(v0, v1);\n-    __ aesimc(v0, v0);\n-    __ aesd(v0, v2);\n-    __ aesimc(v0, v0);\n-    __ aesd(v0, v3);\n-    __ aesimc(v0, v0);\n-    __ aesd(v0, v4);\n-    __ aesimc(v0, v0);\n-\n-    __ ld1(v1, v2, v3, v4, __ T16B, __ post(key, 64));\n-    __ rev32(v1, __ T16B, v1);\n-    __ rev32(v2, __ T16B, v2);\n-    __ rev32(v3, __ T16B, v3);\n-    __ rev32(v4, __ T16B, v4);\n-    __ aesd(v0, v1);\n-    __ aesimc(v0, v0);\n-    __ aesd(v0, v2);\n-    __ aesimc(v0, v0);\n-    __ aesd(v0, v3);\n-    __ aesimc(v0, v0);\n-    __ aesd(v0, v4);\n-    __ aesimc(v0, v0);\n-\n-    __ ld1(v1, v2, __ T16B, __ post(key, 32));\n-    __ rev32(v1, __ T16B, v1);\n-    __ rev32(v2, __ T16B, v2);\n-\n-    __ cmpw(keylen, 44);\n-    __ br(Assembler::EQ, L_doLast);\n-\n-    __ aesd(v0, v1);\n-    __ aesimc(v0, v0);\n-    __ aesd(v0, v2);\n-    __ aesimc(v0, v0);\n-\n-    __ ld1(v1, v2, __ T16B, __ post(key, 32));\n-    __ rev32(v1, __ T16B, v1);\n-    __ rev32(v2, __ T16B, v2);\n-\n-    __ cmpw(keylen, 52);\n-    __ br(Assembler::EQ, L_doLast);\n-\n-    __ aesd(v0, v1);\n-    __ aesimc(v0, v0);\n-    __ aesd(v0, v2);\n-    __ aesimc(v0, v0);\n-\n-    __ ld1(v1, v2, __ T16B, __ post(key, 32));\n-    __ rev32(v1, __ T16B, v1);\n-    __ rev32(v2, __ T16B, v2);\n-\n-    __ BIND(L_doLast);\n-\n-    __ aesd(v0, v1);\n-    __ aesimc(v0, v0);\n-    __ aesd(v0, v2);\n-\n-    __ eor(v0, __ T16B, v0, v5);\n-\n-    __ st1(v0, __ T16B, to);\n+    __ aesecb_decrypt(from, to, key, keylen);\n@@ -2967,0 +2829,379 @@\n+  \/\/ CTR AES crypt.\n+  \/\/ Arguments:\n+  \/\/\n+  \/\/ Inputs:\n+  \/\/   c_rarg0   - source byte array address\n+  \/\/   c_rarg1   - destination byte array address\n+  \/\/   c_rarg2   - K (key) in little endian int array\n+  \/\/   c_rarg3   - counter vector byte array address\n+  \/\/   c_rarg4   - input length\n+  \/\/   c_rarg5   - saved encryptedCounter start\n+  \/\/   c_rarg6   - saved used length\n+  \/\/\n+  \/\/ Output:\n+  \/\/   r0       - input length\n+  \/\/\n+  address generate_counterMode_AESCrypt() {\n+    const Register in = c_rarg0;\n+    const Register out = c_rarg1;\n+    const Register key = c_rarg2;\n+    const Register counter = c_rarg3;\n+    const Register saved_len = c_rarg4, len = r10;\n+    const Register saved_encrypted_ctr = c_rarg5;\n+    const Register used_ptr = c_rarg6, used = r12;\n+\n+    const Register offset = r7;\n+    const Register keylen = r11;\n+\n+    const unsigned char block_size = 16;\n+    const int bulk_width = 4;\n+    \/\/ NB: bulk_width can be 4 or 8. 8 gives slightly faster\n+    \/\/ performance with larger data sizes, but it also means that the\n+    \/\/ fast path isn't used until you have at least 8 blocks, and up\n+    \/\/ to 127 bytes of data will be executed on the slow path. For\n+    \/\/ that reason, and also so as not to blow away too much icache, 4\n+    \/\/ blocks seems like a sensible compromise.\n+\n+    \/\/ Algorithm:\n+    \/\/\n+    \/\/    if (len == 0) {\n+    \/\/        goto DONE;\n+    \/\/    }\n+    \/\/    int result = len;\n+    \/\/    do {\n+    \/\/        if (used >= blockSize) {\n+    \/\/            if (len >= bulk_width * blockSize) {\n+    \/\/                CTR_large_block();\n+    \/\/                if (len == 0)\n+    \/\/                    goto DONE;\n+    \/\/            }\n+    \/\/            for (;;) {\n+    \/\/                16ByteVector v0 = counter;\n+    \/\/                embeddedCipher.encryptBlock(v0, 0, encryptedCounter, 0);\n+    \/\/                used = 0;\n+    \/\/                if (len < blockSize)\n+    \/\/                    break;    \/* goto NEXT *\/\n+    \/\/                16ByteVector v1 = load16Bytes(in, offset);\n+    \/\/                v1 = v1 ^ encryptedCounter;\n+    \/\/                store16Bytes(out, offset);\n+    \/\/                used = blockSize;\n+    \/\/                offset += blockSize;\n+    \/\/                len -= blockSize;\n+    \/\/                if (len == 0)\n+    \/\/                    goto DONE;\n+    \/\/            }\n+    \/\/        }\n+    \/\/      NEXT:\n+    \/\/        out[outOff++] = (byte)(in[inOff++] ^ encryptedCounter[used++]);\n+    \/\/        len--;\n+    \/\/    } while (len != 0);\n+    \/\/  DONE:\n+    \/\/    return result;\n+    \/\/\n+    \/\/ CTR_large_block()\n+    \/\/    Wide bulk encryption of whole blocks.\n+\n+    __ align(CodeEntryAlignment);\n+    StubCodeMark mark(this, \"StubRoutines\", \"counterMode_AESCrypt\");\n+    const address start = __ pc();\n+    __ enter();\n+\n+    Label DONE, CTR_large_block, large_block_return;\n+    __ ldrw(used, Address(used_ptr));\n+    __ cbzw(saved_len, DONE);\n+\n+    __ mov(len, saved_len);\n+    __ mov(offset, 0);\n+\n+    \/\/ Compute #rounds for AES based on the length of the key array\n+    __ ldrw(keylen, Address(key, arrayOopDesc::length_offset_in_bytes() - arrayOopDesc::base_offset_in_bytes(T_INT)));\n+\n+    __ aesenc_loadkeys(key, keylen);\n+\n+    {\n+      Label L_CTR_loop, NEXT;\n+\n+      __ bind(L_CTR_loop);\n+\n+      __ cmp(used, block_size);\n+      __ br(__ LO, NEXT);\n+\n+      \/\/ Maybe we have a lot of data\n+      __ subsw(rscratch1, len, bulk_width * block_size);\n+      __ br(__ HS, CTR_large_block);\n+      __ BIND(large_block_return);\n+      __ cbzw(len, DONE);\n+\n+      \/\/ Setup the counter\n+      __ movi(v4, __ T4S, 0);\n+      __ movi(v5, __ T4S, 1);\n+      __ ins(v4, __ S, v5, 3, 3); \/\/ v4 contains { 0, 0, 0, 1 }\n+\n+      __ ld1(v0, __ T16B, counter); \/\/ Load the counter into v0\n+      __ rev32(v16, __ T16B, v0);\n+      __ addv(v16, __ T4S, v16, v4);\n+      __ rev32(v16, __ T16B, v16);\n+      __ st1(v16, __ T16B, counter); \/\/ Save the incremented counter back\n+\n+      {\n+        \/\/ We have fewer than bulk_width blocks of data left. Encrypt\n+        \/\/ them one by one until there is less than a full block\n+        \/\/ remaining, being careful to save both the encrypted counter\n+        \/\/ and the counter.\n+\n+        Label inner_loop;\n+        __ bind(inner_loop);\n+        \/\/ Counter to encrypt is in v0\n+        __ aesecb_encrypt(noreg, noreg, keylen);\n+        __ st1(v0, __ T16B, saved_encrypted_ctr);\n+\n+        \/\/ Do we have a remaining full block?\n+\n+        __ mov(used, 0);\n+        __ cmp(len, block_size);\n+        __ br(__ LO, NEXT);\n+\n+        \/\/ Yes, we have a full block\n+        __ ldrq(v1, Address(in, offset));\n+        __ eor(v1, __ T16B, v1, v0);\n+        __ strq(v1, Address(out, offset));\n+        __ mov(used, block_size);\n+        __ add(offset, offset, block_size);\n+\n+        __ subw(len, len, block_size);\n+        __ cbzw(len, DONE);\n+\n+        \/\/ Increment the counter, store it back\n+        __ orr(v0, __ T16B, v16, v16);\n+        __ rev32(v16, __ T16B, v16);\n+        __ addv(v16, __ T4S, v16, v4);\n+        __ rev32(v16, __ T16B, v16);\n+        __ st1(v16, __ T16B, counter); \/\/ Save the incremented counter back\n+\n+        __ b(inner_loop);\n+      }\n+\n+      __ BIND(NEXT);\n+\n+      \/\/ Encrypt a single byte, and loop.\n+      \/\/ We expect this to be a rare event.\n+      __ ldrb(rscratch1, Address(in, offset));\n+      __ ldrb(rscratch2, Address(saved_encrypted_ctr, used));\n+      __ eor(rscratch1, rscratch1, rscratch2);\n+      __ strb(rscratch1, Address(out, offset));\n+      __ add(offset, offset, 1);\n+      __ add(used, used, 1);\n+      __ subw(len, len,1);\n+      __ cbnzw(len, L_CTR_loop);\n+    }\n+\n+    __ bind(DONE);\n+    __ strw(used, Address(used_ptr));\n+    __ mov(r0, saved_len);\n+\n+    __ leave(); \/\/ required for proper stackwalking of RuntimeStub frame\n+    __ ret(lr);\n+\n+    \/\/ Bulk encryption\n+\n+    __ BIND (CTR_large_block);\n+    assert(bulk_width == 4 || bulk_width == 8, \"must be\");\n+\n+    if (bulk_width == 8) {\n+      __ sub(sp, sp, 4 * 16);\n+      __ st1(v12, v13, v14, v15, __ T16B, Address(sp));\n+    }\n+    __ sub(sp, sp, 4 * 16);\n+    __ st1(v8, v9, v10, v11, __ T16B, Address(sp));\n+    RegSet saved_regs = (RegSet::of(in, out, offset)\n+                         + RegSet::of(saved_encrypted_ctr, used_ptr, len));\n+    __ push(saved_regs, sp);\n+    __ andr(len, len, -16 * bulk_width);  \/\/ 8\/4 encryptions, 16 bytes per encryption\n+    __ add(in, in, offset);\n+    __ add(out, out, offset);\n+\n+    \/\/ Keys should already be loaded into the correct registers\n+\n+    __ ld1(v0, __ T16B, counter); \/\/ v0 contains the first counter\n+    __ rev32(v16, __ T16B, v0); \/\/ v16 contains byte-reversed counter\n+\n+    \/\/ AES\/CTR loop\n+    {\n+      Label L_CTR_loop;\n+      __ BIND(L_CTR_loop);\n+\n+      \/\/ Setup the counters\n+      __ movi(v8, __ T4S, 0);\n+      __ movi(v9, __ T4S, 1);\n+      __ ins(v8, __ S, v9, 3, 3); \/\/ v8 contains { 0, 0, 0, 1 }\n+\n+      for (FloatRegister f = v0; f < v0 + bulk_width; f++) {\n+        __ rev32(f, __ T16B, v16);\n+        __ addv(v16, __ T4S, v16, v8);\n+      }\n+\n+      __ ld1(v8, v9, v10, v11, __ T16B, __ post(in, 4 * 16));\n+\n+      \/\/ Encrypt the counters\n+      __ aesecb_encrypt(noreg, noreg, keylen, v0, bulk_width);\n+\n+      if (bulk_width == 8) {\n+        __ ld1(v12, v13, v14, v15, __ T16B, __ post(in, 4 * 16));\n+      }\n+\n+      \/\/ XOR the encrypted counters with the inputs\n+      for (int i = 0; i < bulk_width; i++) {\n+        __ eor(v0 + i, __ T16B, v0 + i, v8 + i);\n+      }\n+\n+      \/\/ Write the encrypted data\n+      __ st1(v0, v1, v2, v3, __ T16B, __ post(out, 4 * 16));\n+      if (bulk_width == 8) {\n+        __ st1(v4, v5, v6, v7, __ T16B, __ post(out, 4 * 16));\n+      }\n+\n+      __ subw(len, len, 16 * bulk_width);\n+      __ cbnzw(len, L_CTR_loop);\n+    }\n+\n+    \/\/ Save the counter back where it goes\n+    __ rev32(v16, __ T16B, v16);\n+    __ st1(v16, __ T16B, counter);\n+\n+    __ pop(saved_regs, sp);\n+\n+    __ ld1(v8, v9, v10, v11, __ T16B, __ post(sp, 4 * 16));\n+    if (bulk_width == 8) {\n+      __ ld1(v12, v13, v14, v15, __ T16B, __ post(sp, 4 * 16));\n+    }\n+\n+    __ andr(rscratch1, len, -16 * bulk_width);\n+    __ sub(len, len, rscratch1);\n+    __ add(offset, offset, rscratch1);\n+    __ mov(used, 16);\n+    __ strw(used, Address(used_ptr));\n+    __ b(large_block_return);\n+\n+    return start;\n+  }\n+\n+  \/\/ Vector AES Galois Counter Mode implementation. Parameters:\n+  \/\/\n+  \/\/ in = c_rarg0\n+  \/\/ len = c_rarg1\n+  \/\/ ct = c_rarg2 - ciphertext that ghash will read (in for encrypt, out for decrypt)\n+  \/\/ out = c_rarg3\n+  \/\/ key = c_rarg4\n+  \/\/ state = c_rarg5 - GHASH.state\n+  \/\/ subkeyHtbl = c_rarg6 - powers of H\n+  \/\/ counter = c_rarg7 - pointer to 16 bytes of CTR\n+  \/\/ return - number of processed bytes\n+  address generate_galoisCounterMode_AESCrypt() {\n+    address ghash_polynomial = __ pc();\n+    __ emit_int64(0x87);  \/\/ The low-order bits of the field\n+                          \/\/ polynomial (i.e. p = z^7+z^2+z+1)\n+                          \/\/ repeated in the low and high parts of a\n+                          \/\/ 128-bit vector\n+    __ emit_int64(0x87);\n+\n+    __ align(CodeEntryAlignment);\n+     StubCodeMark mark(this, \"StubRoutines\", \"galoisCounterMode_AESCrypt\");\n+    address start = __ pc();\n+    const Register in = c_rarg0;\n+    const Register len = c_rarg1;\n+    const Register ct = c_rarg2;\n+    const Register out = c_rarg3;\n+    \/\/ and updated with the incremented counter in the end\n+\n+    const Register key = c_rarg4;\n+    const Register state = c_rarg5;\n+\n+    const Register subkeyHtbl = c_rarg6;\n+\n+    const Register counter = c_rarg7;\n+\n+    const Register keylen = r10;\n+    __ enter();\n+    \/\/ Save state before entering routine\n+    __ sub(sp, sp, 4 * 16);\n+    __ st1(v12, v13, v14, v15, __ T16B, Address(sp));\n+    __ sub(sp, sp, 4 * 16);\n+    __ st1(v8, v9, v10, v11, __ T16B, Address(sp));\n+\n+    \/\/ __ andr(len, len, -512);\n+    __ andr(len, len, -16 * 8);  \/\/ 8 encryptions, 16 bytes per encryption\n+    __ str(len, __ pre(sp, -2 * wordSize));\n+\n+    Label DONE;\n+    __ cbz(len, DONE);\n+\n+    \/\/ Compute #rounds for AES based on the length of the key array\n+    __ ldrw(keylen, Address(key, arrayOopDesc::length_offset_in_bytes() - arrayOopDesc::base_offset_in_bytes(T_INT)));\n+\n+    __ aesenc_loadkeys(key, keylen);\n+    __ ld1(v0, __ T16B, counter); \/\/ v0 contains the first counter\n+    __ rev32(v16, __ T16B, v0); \/\/ v16 contains byte-reversed counter\n+\n+    \/\/ AES\/CTR loop\n+    {\n+      Label L_CTR_loop;\n+      __ BIND(L_CTR_loop);\n+\n+      \/\/ Setup the counters\n+      __ movi(v8, __ T4S, 0);\n+      __ movi(v9, __ T4S, 1);\n+      __ ins(v8, __ S, v9, 3, 3); \/\/ v8 contains { 0, 0, 0, 1 }\n+      for (FloatRegister f = v0; f < v8; f++) {\n+        __ rev32(f, __ T16B, v16);\n+        __ addv(v16, __ T4S, v16, v8);\n+      }\n+\n+      __ ld1(v8, v9, v10, v11, __ T16B, __ post(in, 4 * 16));\n+\n+      \/\/ Encrypt the counters\n+      __ aesecb_encrypt(noreg, noreg, keylen, v0, \/*unrolls*\/8);\n+\n+      __ ld1(v12, v13, v14, v15, __ T16B, __ post(in, 4 * 16));\n+\n+      \/\/ XOR the encrypted counters with the inputs\n+      for (int i = 0; i < 8; i++) {\n+        __ eor(v0 + i, __ T16B, v0 + i, v8 + i);\n+      }\n+      __ st1(v0, v1, v2, v3, __ T16B, __ post(out, 4 * 16));\n+      __ st1(v4, v5, v6, v7, __ T16B, __ post(out, 4 * 16));\n+\n+      __ subw(len, len, 16 * 8);\n+      __ cbnzw(len, L_CTR_loop);\n+    }\n+\n+    __ rev32(v16, __ T16B, v16);\n+    __ st1(v16, __ T16B, counter);\n+\n+    __ ldr(len, Address(sp));\n+    __ lsr(len, len, exact_log2(16));  \/\/ We want the count of blocks\n+\n+    \/\/ GHASH\/CTR loop\n+    __ ghash_processBlocks_wide(ghash_polynomial, state, subkeyHtbl, ct,\n+                                len, \/*unrolls*\/4);\n+\n+#ifdef ASSERT\n+    { Label L;\n+      __ cmp(len, (unsigned char)0);\n+      __ br(Assembler::EQ, L);\n+      __ stop(\"stubGenerator: abort\");\n+      __ bind(L);\n+  }\n+#endif\n+\n+  __ bind(DONE);\n+    \/\/ Return the number of bytes processed\n+    __ ldr(r0, __ post(sp, 2 * wordSize));\n+\n+    __ ld1(v8, v9, v10, v11, __ T16B, __ post(sp, 4 * 16));\n+    __ ld1(v12, v13, v14, v15, __ T16B, __ post(sp, 4 * 16));\n+\n+    __ leave(); \/\/ required for proper stackwalking of RuntimeStub frame\n+    __ ret(lr);\n+     return start;\n+  }\n+\n@@ -4230,63 +4471,0 @@\n-  void ghash_multiply(FloatRegister result_lo, FloatRegister result_hi,\n-                      FloatRegister a, FloatRegister b, FloatRegister a1_xor_a0,\n-                      FloatRegister tmp1, FloatRegister tmp2, FloatRegister tmp3, FloatRegister tmp4) {\n-    \/\/ Karatsuba multiplication performs a 128*128 -> 256-bit\n-    \/\/ multiplication in three 128-bit multiplications and a few\n-    \/\/ additions.\n-    \/\/\n-    \/\/ (C1:C0) = A1*B1, (D1:D0) = A0*B0, (E1:E0) = (A0+A1)(B0+B1)\n-    \/\/ (A1:A0)(B1:B0) = C1:(C0+C1+D1+E1):(D1+C0+D0+E0):D0\n-    \/\/\n-    \/\/ Inputs:\n-    \/\/\n-    \/\/ A0 in a.d[0]     (subkey)\n-    \/\/ A1 in a.d[1]\n-    \/\/ (A1+A0) in a1_xor_a0.d[0]\n-    \/\/\n-    \/\/ B0 in b.d[0]     (state)\n-    \/\/ B1 in b.d[1]\n-\n-    __ ext(tmp1, __ T16B, b, b, 0x08);\n-    __ pmull2(result_hi, __ T1Q, b, a, __ T2D);  \/\/ A1*B1\n-    __ eor(tmp1, __ T16B, tmp1, b);            \/\/ (B1+B0)\n-    __ pmull(result_lo,  __ T1Q, b, a, __ T1D);  \/\/ A0*B0\n-    __ pmull(tmp2, __ T1Q, tmp1, a1_xor_a0, __ T1D); \/\/ (A1+A0)(B1+B0)\n-\n-    __ ext(tmp4, __ T16B, result_lo, result_hi, 0x08);\n-    __ eor(tmp3, __ T16B, result_hi, result_lo); \/\/ A1*B1+A0*B0\n-    __ eor(tmp2, __ T16B, tmp2, tmp4);\n-    __ eor(tmp2, __ T16B, tmp2, tmp3);\n-\n-    \/\/ Register pair <result_hi:result_lo> holds the result of carry-less multiplication\n-    __ ins(result_hi, __ D, tmp2, 0, 1);\n-    __ ins(result_lo, __ D, tmp2, 1, 0);\n-  }\n-\n-  void ghash_reduce(FloatRegister result, FloatRegister lo, FloatRegister hi,\n-                    FloatRegister p, FloatRegister z, FloatRegister t1) {\n-    const FloatRegister t0 = result;\n-\n-    \/\/ The GCM field polynomial f is z^128 + p(z), where p =\n-    \/\/ z^7+z^2+z+1.\n-    \/\/\n-    \/\/    z^128 === -p(z)  (mod (z^128 + p(z)))\n-    \/\/\n-    \/\/ so, given that the product we're reducing is\n-    \/\/    a == lo + hi * z^128\n-    \/\/ substituting,\n-    \/\/      === lo - hi * p(z)  (mod (z^128 + p(z)))\n-    \/\/\n-    \/\/ we reduce by multiplying hi by p(z) and subtracting the result\n-    \/\/ from (i.e. XORing it with) lo.  Because p has no nonzero high\n-    \/\/ bits we can do this with two 64-bit multiplications, lo*p and\n-    \/\/ hi*p.\n-\n-    __ pmull2(t0, __ T1Q, hi, p, __ T2D);\n-    __ ext(t1, __ T16B, t0, z, 8);\n-    __ eor(hi, __ T16B, hi, t1);\n-    __ ext(t1, __ T16B, z, t0, 8);\n-    __ eor(lo, __ T16B, lo, t1);\n-    __ pmull(t0, __ T1Q, hi, p, __ T1D);\n-    __ eor(result, __ T16B, lo, t0);\n-  }\n-\n@@ -5390,0 +5568,2 @@\n+    __ ldrq(v24, p);    \/\/ The field polynomial\n+\n@@ -5398,4 +5578,2 @@\n-    __ ldrq(v26, p);\n-\n-    __ ext(v16, __ T16B, v1, v1, 0x08); \/\/ long-swap subkeyH into v1\n-    __ eor(v16, __ T16B, v16, v1);      \/\/ xor subkeyH into subkeyL (Karatsuba: (A1+A0))\n+    __ ext(v4, __ T16B, v1, v1, 0x08); \/\/ long-swap subkeyH into v1\n+    __ eor(v4, __ T16B, v4, v1);       \/\/ xor subkeyH into subkeyL (Karatsuba: (A1+A0))\n@@ -5413,3 +5591,3 @@\n-      ghash_multiply(\/*result_lo*\/v5, \/*result_hi*\/v7,\n-                     \/*a*\/v1, \/*b*\/v2, \/*a1_xor_a0*\/v16,\n-                     \/*temps*\/v6, v20, v18, v21);\n+      __ ghash_multiply(\/*result_lo*\/v5, \/*result_hi*\/v7,\n+                        \/*a*\/v1, \/*b*\/v2, \/*a1_xor_a0*\/v4,\n+                        \/*temps*\/v6, v3, \/*reuse\/clobber b*\/v2);\n@@ -5417,1 +5595,1 @@\n-      ghash_reduce(v0, v5, v7, v26, vzr, v20);\n+      __ ghash_reduce(\/*result*\/v0, \/*lo*\/v5, \/*hi*\/v7, \/*p*\/v24, vzr, \/*temp*\/v3);\n@@ -5424,2 +5602,52 @@\n-    __ rev64(v1, __ T16B, v0);\n-    __ rbit(v1, __ T16B, v1);\n+    __ rev64(v0, __ T16B, v0);\n+    __ rbit(v0, __ T16B, v0);\n+\n+    __ st1(v0, __ T16B, state);\n+    __ ret(lr);\n+\n+    return start;\n+  }\n+\n+  address generate_ghash_processBlocks_wide() {\n+    address small = generate_ghash_processBlocks();\n+\n+    StubCodeMark mark(this, \"StubRoutines\", \"ghash_processBlocks_wide\");\n+    __ align(wordSize * 2);\n+    address p = __ pc();\n+    __ emit_int64(0x87);  \/\/ The low-order bits of the field\n+                          \/\/ polynomial (i.e. p = z^7+z^2+z+1)\n+                          \/\/ repeated in the low and high parts of a\n+                          \/\/ 128-bit vector\n+    __ emit_int64(0x87);\n+\n+    __ align(CodeEntryAlignment);\n+    address start = __ pc();\n+\n+    Register state   = c_rarg0;\n+    Register subkeyH = c_rarg1;\n+    Register data    = c_rarg2;\n+    Register blocks  = c_rarg3;\n+\n+    const int unroll = 4;\n+\n+    __ cmp(blocks, (unsigned char)(unroll * 2));\n+    __ br(__ LT, small);\n+\n+    if (unroll > 1) {\n+    \/\/ Save state before entering routine\n+      __ sub(sp, sp, 4 * 16);\n+      __ st1(v12, v13, v14, v15, __ T16B, Address(sp));\n+      __ sub(sp, sp, 4 * 16);\n+      __ st1(v8, v9, v10, v11, __ T16B, Address(sp));\n+    }\n+\n+    __ ghash_processBlocks_wide(p, state, subkeyH, data, blocks, unroll);\n+\n+    if (unroll > 1) {\n+      \/\/ And restore state\n+      __ ld1(v8, v9, v10, v11, __ T16B, __ post(sp, 4 * 16));\n+      __ ld1(v12, v13, v14, v15, __ T16B, __ post(sp, 4 * 16));\n+    }\n+\n+    __ cmp(blocks, (unsigned char)0);\n+    __ br(__ GT, small);\n@@ -5427,1 +5655,0 @@\n-    __ st1(v1, __ T16B, state);\n@@ -7114,1 +7341,2 @@\n-      StubRoutines::_ghash_processBlocks = generate_ghash_processBlocks();\n+      \/\/ StubRoutines::_ghash_processBlocks = generate_ghash_processBlocks();\n+      StubRoutines::_ghash_processBlocks = generate_ghash_processBlocks_wide();\n@@ -7131,0 +7359,1 @@\n+      StubRoutines::_counterMode_AESCrypt = generate_counterMode_AESCrypt();\n","filename":"src\/hotspot\/cpu\/aarch64\/stubGenerator_aarch64.cpp","additions":445,"deletions":216,"binary":false,"changes":661,"status":"modified"},{"patch":"@@ -39,1 +39,1 @@\n-  code_size2 = 28000           \/\/ simply increase if too small (assembler will crash if too small)\n+  code_size2 = 38000           \/\/ simply increase if too small (assembler will crash if too small)\n","filename":"src\/hotspot\/cpu\/aarch64\/stubRoutines_aarch64.hpp","additions":1,"deletions":1,"binary":false,"changes":2,"status":"modified"},{"patch":"@@ -240,0 +240,3 @@\n+    if (FLAG_IS_DEFAULT(UseAESCTRIntrinsics)) {\n+      FLAG_SET_DEFAULT(UseAESCTRIntrinsics, true);\n+    }\n@@ -249,0 +252,4 @@\n+    if (UseAESCTRIntrinsics) {\n+      warning(\"AES\/CTR intrinsics are not available on this CPU\");\n+      FLAG_SET_DEFAULT(UseAESCTRIntrinsics, false);\n+    }\n@@ -251,4 +258,0 @@\n-  if (UseAESCTRIntrinsics) {\n-    warning(\"AES\/CTR intrinsics are not available on this CPU\");\n-    FLAG_SET_DEFAULT(UseAESCTRIntrinsics, false);\n-  }\n","filename":"src\/hotspot\/cpu\/aarch64\/vm_version_aarch64.cpp","additions":7,"deletions":4,"binary":false,"changes":11,"status":"modified"},{"patch":"@@ -63,0 +63,3 @@\n+  \/\/ All Apple-darwin Arm processors have AES.\n+  _features |= CPU_AES;\n+\n@@ -91,0 +94,1 @@\n+\n","filename":"src\/hotspot\/os_cpu\/bsd_aarch64\/vm_version_bsd_aarch64.cpp","additions":4,"deletions":0,"binary":false,"changes":4,"status":"modified"}]}