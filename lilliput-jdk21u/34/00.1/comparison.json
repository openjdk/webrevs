{"files":[{"patch":"@@ -2,1 +2,1 @@\n-project=jdk-updates\n+project=lilliput\n@@ -25,1 +25,1 @@\n-reviewers=1\n+committers=1\n","filename":".jcheck\/conf","additions":2,"deletions":2,"binary":false,"changes":4,"status":"modified"},{"patch":"@@ -2,1 +2,1 @@\n- * Copyright (c) 1997, 2023, Oracle and\/or its affiliates. All rights reserved.\n+ * Copyright (c) 1997, 2024, Oracle and\/or its affiliates. All rights reserved.\n@@ -26,2 +26,0 @@\n-#include <sys\/types.h>\n-\n@@ -57,0 +55,1 @@\n+#include \"utilities\/globalDefinitions.hpp\"\n@@ -68,0 +67,2 @@\n+#include <sys\/types.h>\n+\n@@ -4329,0 +4330,19 @@\n+\/\/ Loads the obj's Klass* into dst.\n+\/\/ Preserves all registers (incl src, rscratch1 and rscratch2).\n+void MacroAssembler::load_nklass_compact(Register dst, Register src) {\n+  assert(UseCompactObjectHeaders, \"expects UseCompactObjectHeaders\");\n+\n+  Label fast;\n+\n+  \/\/ Check if we can take the (common) fast path, if obj is unlocked.\n+  ldr(dst, Address(src, oopDesc::mark_offset_in_bytes()));\n+  tbz(dst, exact_log2(markWord::monitor_value), fast);\n+\n+  \/\/ Fetch displaced header\n+  ldr(dst, Address(dst, OM_OFFSET_NO_MONITOR_VALUE_TAG(header)));\n+\n+  \/\/ Fast-path: shift to get narrowKlass.\n+  bind(fast);\n+  lsr(dst, dst, markWord::klass_shift);\n+}\n+\n@@ -4330,1 +4350,4 @@\n-  if (UseCompressedClassPointers) {\n+  if (UseCompactObjectHeaders) {\n+    load_nklass_compact(dst, src);\n+    decode_klass_not_null(dst);\n+  } else if (UseCompressedClassPointers) {\n@@ -4369,0 +4392,1 @@\n+  assert_different_registers(oop, trial_klass, tmp);\n@@ -4370,1 +4394,5 @@\n-    ldrw(tmp, Address(oop, oopDesc::klass_offset_in_bytes()));\n+    if (UseCompactObjectHeaders) {\n+      load_nklass_compact(tmp, oop);\n+    } else {\n+      ldrw(tmp, Address(oop, oopDesc::klass_offset_in_bytes()));\n+    }\n@@ -4387,0 +4415,16 @@\n+void MacroAssembler::cmp_klass(Register src, Register dst, Register tmp1, Register tmp2) {\n+  if (UseCompactObjectHeaders) {\n+    load_nklass_compact(tmp1, src);\n+    load_nklass_compact(tmp2, dst);\n+    cmpw(tmp1, tmp2);\n+  } else if (UseCompressedClassPointers) {\n+    ldrw(tmp1, Address(src, oopDesc::klass_offset_in_bytes()));\n+    ldrw(tmp2, Address(dst, oopDesc::klass_offset_in_bytes()));\n+    cmpw(tmp1, tmp2);\n+  } else {\n+    ldr(tmp1, Address(src, oopDesc::klass_offset_in_bytes()));\n+    ldr(tmp2, Address(dst, oopDesc::klass_offset_in_bytes()));\n+    cmp(tmp1, tmp2);\n+  }\n+}\n+\n@@ -4390,0 +4434,1 @@\n+  assert(!UseCompactObjectHeaders, \"not with compact headers\");\n@@ -4399,0 +4444,1 @@\n+  assert(!UseCompactObjectHeaders, \"not with compact headers\");\n@@ -6229,2 +6275,0 @@\n-\/\/ Branches to slow upon failure to lock the object, with ZF cleared.\n-\/\/ Falls through upon success with ZF set.\n@@ -6233,3 +6277,3 @@\n-\/\/  - hdr: the header, already loaded from obj, will be destroyed\n-\/\/  - t1, t2: temporary registers, will be destroyed\n-void MacroAssembler::lightweight_lock(Register obj, Register hdr, Register t1, Register t2, Label& slow) {\n+\/\/  - t1, t2, t3: temporary registers, will be destroyed\n+\/\/  - slow: branched to if locking fails, absolute offset may larger than 32KB (imm14 encoding).\n+void MacroAssembler::lightweight_lock(Register obj, Register t1, Register t2, Register t3, Label& slow) {\n@@ -6237,15 +6281,24 @@\n-  assert_different_registers(obj, hdr, t1, t2, rscratch1);\n-\n-  \/\/ Check if we would have space on lock-stack for the object.\n-  ldrw(t1, Address(rthread, JavaThread::lock_stack_top_offset()));\n-  cmpw(t1, (unsigned)LockStack::end_offset() - 1);\n-  br(Assembler::GT, slow);\n-\n-  \/\/ Load (object->mark() | 1) into hdr\n-  orr(hdr, hdr, markWord::unlocked_value);\n-  \/\/ Clear lock-bits, into t2\n-  eor(t2, hdr, markWord::unlocked_value);\n-  \/\/ Try to swing header from unlocked to locked\n-  \/\/ Clobbers rscratch1 when UseLSE is false\n-  cmpxchg(\/*addr*\/ obj, \/*expected*\/ hdr, \/*new*\/ t2, Assembler::xword,\n-          \/*acquire*\/ true, \/*release*\/ true, \/*weak*\/ false, t1);\n+  assert_different_registers(obj, t1, t2, t3, rscratch1);\n+\n+  Label push;\n+  const Register top = t1;\n+  const Register mark = t2;\n+  const Register t = t3;\n+\n+  \/\/ Preload the markWord. It is important that this is the first\n+  \/\/ instruction emitted as it is part of C1's null check semantics.\n+  ldr(mark, Address(obj, oopDesc::mark_offset_in_bytes()));\n+\n+  \/\/ Check if the lock-stack is full.\n+  ldrw(top, Address(rthread, JavaThread::lock_stack_top_offset()));\n+  cmpw(top, (unsigned)LockStack::end_offset());\n+  br(Assembler::GE, slow);\n+\n+  \/\/ Check for recursion.\n+  subw(t, top, oopSize);\n+  ldr(t, Address(rthread, t));\n+  cmp(obj, t);\n+  br(Assembler::EQ, push);\n+\n+  \/\/ Check header for monitor (0b10).\n+  tst(mark, markWord::monitor_value);\n@@ -6254,5 +6307,13 @@\n-  \/\/ After successful lock, push object on lock-stack\n-  ldrw(t1, Address(rthread, JavaThread::lock_stack_top_offset()));\n-  str(obj, Address(rthread, t1));\n-  addw(t1, t1, oopSize);\n-  strw(t1, Address(rthread, JavaThread::lock_stack_top_offset()));\n+  \/\/ Try to lock. Transition lock bits 0b01 => 0b00\n+  assert(oopDesc::mark_offset_in_bytes() == 0, \"required to avoid lea\");\n+  orr(mark, mark, markWord::unlocked_value);\n+  eor(t, mark, markWord::unlocked_value);\n+  cmpxchg(\/*addr*\/ obj, \/*expected*\/ mark, \/*new*\/ t, Assembler::xword,\n+          \/*acquire*\/ true, \/*release*\/ false, \/*weak*\/ false, noreg);\n+  br(Assembler::NE, slow);\n+\n+  bind(push);\n+  \/\/ After successful lock, push object on lock-stack.\n+  str(obj, Address(rthread, top));\n+  addw(top, top, oopSize);\n+  strw(top, Address(rthread, JavaThread::lock_stack_top_offset()));\n@@ -6262,2 +6323,0 @@\n-\/\/ Branches to slow upon failure, with ZF cleared.\n-\/\/ Falls through upon success, with ZF set.\n@@ -6266,3 +6325,3 @@\n-\/\/ - hdr: the (pre-loaded) header of the object\n-\/\/ - t1, t2: temporary registers\n-void MacroAssembler::lightweight_unlock(Register obj, Register hdr, Register t1, Register t2, Label& slow) {\n+\/\/ - t1, t2, t3: temporary registers\n+\/\/ - slow: branched to if unlocking fails, absolute offset may larger than 32KB (imm14 encoding).\n+void MacroAssembler::lightweight_unlock(Register obj, Register t1, Register t2, Register t3, Label& slow) {\n@@ -6270,1 +6329,2 @@\n-  assert_different_registers(obj, hdr, t1, t2, rscratch1);\n+  \/\/ cmpxchg clobbers rscratch1.\n+  assert_different_registers(obj, t1, t2, t3, rscratch1);\n@@ -6274,4 +6334,0 @@\n-    \/\/ The following checks rely on the fact that LockStack is only ever modified by\n-    \/\/ its owning thread, even if the lock got inflated concurrently; removal of LockStack\n-    \/\/ entries after inflation will happen delayed in that case.\n-\n@@ -6282,1 +6338,1 @@\n-    br(Assembler::GT, stack_ok);\n+    br(Assembler::GE, stack_ok);\n@@ -6286,18 +6342,0 @@\n-  {\n-    \/\/ Check if the top of the lock-stack matches the unlocked object.\n-    Label tos_ok;\n-    subw(t1, t1, oopSize);\n-    ldr(t1, Address(rthread, t1));\n-    cmpoop(t1, obj);\n-    br(Assembler::EQ, tos_ok);\n-    STOP(\"Top of lock-stack does not match the unlocked object\");\n-    bind(tos_ok);\n-  }\n-  {\n-    \/\/ Check that hdr is fast-locked.\n-    Label hdr_ok;\n-    tst(hdr, markWord::lock_mask_in_place);\n-    br(Assembler::EQ, hdr_ok);\n-    STOP(\"Header is not fast-locked\");\n-    bind(hdr_ok);\n-  }\n@@ -6306,2 +6344,4 @@\n-  \/\/ Load the new header (unlocked) into t1\n-  orr(t1, hdr, markWord::unlocked_value);\n+  Label unlocked, push_and_slow;\n+  const Register top = t1;\n+  const Register mark = t2;\n+  const Register t = t3;\n@@ -6309,4 +6349,5 @@\n-  \/\/ Try to swing header from locked to unlocked\n-  \/\/ Clobbers rscratch1 when UseLSE is false\n-  cmpxchg(obj, hdr, t1, Assembler::xword,\n-          \/*acquire*\/ true, \/*release*\/ true, \/*weak*\/ false, t2);\n+  \/\/ Check if obj is top of lock-stack.\n+  ldrw(top, Address(rthread, JavaThread::lock_stack_top_offset()));\n+  subw(top, top, oopSize);\n+  ldr(t, Address(rthread, top));\n+  cmp(obj, t);\n@@ -6315,3 +6356,14 @@\n-  \/\/ After successful unlock, pop object from lock-stack\n-  ldrw(t1, Address(rthread, JavaThread::lock_stack_top_offset()));\n-  subw(t1, t1, oopSize);\n+  \/\/ Pop lock-stack.\n+  DEBUG_ONLY(str(zr, Address(rthread, top));)\n+  strw(top, Address(rthread, JavaThread::lock_stack_top_offset()));\n+\n+  \/\/ Check if recursive.\n+  subw(t, top, oopSize);\n+  ldr(t, Address(rthread, t));\n+  cmp(obj, t);\n+  br(Assembler::EQ, unlocked);\n+\n+  \/\/ Not recursive. Check header for monitor (0b10).\n+  ldr(mark, Address(obj, oopDesc::mark_offset_in_bytes()));\n+  tbnz(mark, log2i_exact(markWord::monitor_value), push_and_slow);\n+\n@@ -6319,1 +6371,5 @@\n-  str(zr, Address(rthread, t1));\n+  \/\/ Check header not unlocked (0b01).\n+  Label not_unlocked;\n+  tbz(mark, log2i_exact(markWord::unlocked_value), not_unlocked);\n+  stop(\"lightweight_unlock already unlocked\");\n+  bind(not_unlocked);\n@@ -6321,1 +6377,16 @@\n-  strw(t1, Address(rthread, JavaThread::lock_stack_top_offset()));\n+\n+  \/\/ Try to unlock. Transition lock bits 0b00 => 0b01\n+  assert(oopDesc::mark_offset_in_bytes() == 0, \"required to avoid lea\");\n+  orr(t, mark, markWord::unlocked_value);\n+  cmpxchg(obj, mark, t, Assembler::xword,\n+          \/*acquire*\/ false, \/*release*\/ true, \/*weak*\/ false, noreg);\n+  br(Assembler::EQ, unlocked);\n+\n+  bind(push_and_slow);\n+  \/\/ Restore lock-stack and handle the unlock in runtime.\n+  DEBUG_ONLY(str(obj, Address(rthread, top));)\n+  addw(top, top, oopSize);\n+  strw(top, Address(rthread, JavaThread::lock_stack_top_offset()));\n+  b(slow);\n+\n+  bind(unlocked);\n","filename":"src\/hotspot\/cpu\/aarch64\/macroAssembler_aarch64.cpp","additions":141,"deletions":70,"binary":false,"changes":211,"status":"modified"},{"patch":"@@ -2,1 +2,1 @@\n- * Copyright (c) 2003, 2023, Oracle and\/or its affiliates. All rights reserved.\n+ * Copyright (c) 2003, 2024, Oracle and\/or its affiliates. All rights reserved.\n@@ -1819,1 +1819,0 @@\n-      __ ldr(swap_reg, Address(obj_reg, oopDesc::mark_offset_in_bytes()));\n@@ -1959,2 +1958,0 @@\n-      __ ldr(old_hdr, Address(obj_reg, oopDesc::mark_offset_in_bytes()));\n-      __ tbnz(old_hdr, exact_log2(markWord::monitor_value), slow_path_unlock);\n","filename":"src\/hotspot\/cpu\/aarch64\/sharedRuntime_aarch64.cpp","additions":1,"deletions":4,"binary":false,"changes":5,"status":"modified"},{"patch":"@@ -2,1 +2,1 @@\n- * Copyright (c) 1997, 2023, Oracle and\/or its affiliates. All rights reserved.\n+ * Copyright (c) 1997, 2024, Oracle and\/or its affiliates. All rights reserved.\n@@ -167,0 +167,1 @@\n+  constexpr static bool supports_recursive_lightweight_locking() { return true; }\n","filename":"src\/hotspot\/cpu\/aarch64\/vm_version_aarch64.hpp","additions":2,"deletions":1,"binary":false,"changes":3,"status":"modified"},{"patch":"@@ -301,0 +301,4 @@\n+void LoadKlassStub::emit_code(LIR_Assembler* ce) {\n+  \/\/ Currently not needed.\n+  Unimplemented();\n+}\n","filename":"src\/hotspot\/cpu\/ppc\/c1_CodeStubs_ppc.cpp","additions":4,"deletions":0,"binary":false,"changes":4,"status":"modified"},{"patch":"@@ -54,7 +54,0 @@\n-#if defined(COMPILER2) && (defined(AIX) || defined(LINUX))\n-\/\/ Include Transactional Memory lock eliding optimization\n-#define INCLUDE_RTM_OPT 1\n-#else\n-#define INCLUDE_RTM_OPT 0\n-#endif\n-\n","filename":"src\/hotspot\/cpu\/ppc\/globalDefinitions_ppc.hpp","additions":0,"deletions":7,"binary":false,"changes":7,"status":"modified"},{"patch":"@@ -12149,0 +12149,1 @@\n+  predicate(LockingMode != LM_LIGHTWEIGHT);\n@@ -12151,1 +12152,0 @@\n-  predicate(!Compile::current()->use_rtm());\n@@ -12164,20 +12164,1 @@\n-\/\/ Separate version for TM. Use bound register for box to enable USE_KILL.\n-instruct cmpFastLock_tm(flagsReg crx, iRegPdst oop, rarg2RegP box, iRegPdst tmp1, iRegPdst tmp2, iRegPdst tmp3) %{\n-  match(Set crx (FastLock oop box));\n-  effect(TEMP tmp1, TEMP tmp2, TEMP tmp3, USE_KILL box);\n-  predicate(Compile::current()->use_rtm());\n-\n-  format %{ \"FASTLOCK  $oop, $box, $tmp1, $tmp2, $tmp3 (TM)\" %}\n-  ins_encode %{\n-    __ compiler_fast_lock_object($crx$$CondRegister, $oop$$Register, $box$$Register,\n-                                 $tmp1$$Register, $tmp2$$Register, $tmp3$$Register,\n-                                 _rtm_counters, _stack_rtm_counters,\n-                                 ((Method*)(ra_->C->method()->constant_encoding()))->method_data(),\n-                                 \/*RTM*\/ true, ra_->C->profile_rtm());\n-    \/\/ If locking was successful, crx should indicate 'EQ'.\n-    \/\/ The compiler generates a branch to the runtime call to\n-    \/\/ _complete_monitor_locking_Java for the case where crx is 'NE'.\n-  %}\n-  ins_pipe(pipe_class_compare);\n-%}\n-\n+  predicate(LockingMode != LM_LIGHTWEIGHT);\n@@ -12187,1 +12168,0 @@\n-  predicate(!Compile::current()->use_rtm());\n@@ -12192,2 +12172,1 @@\n-                                   $tmp1$$Register, $tmp2$$Register, $tmp3$$Register,\n-                                   false);\n+                                   $tmp1$$Register, $tmp2$$Register, $tmp3$$Register);\n@@ -12201,1 +12180,18 @@\n-instruct cmpFastUnlock_tm(flagsReg crx, iRegPdst oop, iRegPdst box, iRegPdst tmp1, iRegPdst tmp2, iRegPdst tmp3) %{\n+instruct cmpFastLockLightweight(flagsRegCR0 crx, iRegPdst oop, iRegPdst box, iRegPdst tmp1, iRegPdst tmp2) %{\n+  predicate(LockingMode == LM_LIGHTWEIGHT);\n+  match(Set crx (FastLock oop box));\n+  effect(TEMP tmp1, TEMP tmp2);\n+\n+  format %{ \"FASTLOCK  $oop, $box, $tmp1, $tmp2\" %}\n+  ins_encode %{\n+    __ fast_lock_lightweight($crx$$CondRegister, $oop$$Register, $box$$Register,\n+                             $tmp1$$Register, $tmp2$$Register, \/*tmp3*\/ R0);\n+    \/\/ If locking was successful, crx should indicate 'EQ'.\n+    \/\/ The compiler generates a branch to the runtime call to\n+    \/\/ _complete_monitor_locking_Java for the case where crx is 'NE'.\n+  %}\n+  ins_pipe(pipe_class_compare);\n+%}\n+\n+instruct cmpFastUnlockLightweight(flagsRegCR0 crx, iRegPdst oop, iRegPdst box, iRegPdst tmp1, iRegPdst tmp2, iRegPdst tmp3) %{\n+  predicate(LockingMode == LM_LIGHTWEIGHT);\n@@ -12204,2 +12200,1 @@\n-  predicate(Compile::current()->use_rtm());\n-  format %{ \"FASTUNLOCK  $oop, $box, $tmp1, $tmp2 (TM)\" %}\n+  format %{ \"FASTUNLOCK  $oop, $box, $tmp1, $tmp2\" %}\n@@ -12208,3 +12203,2 @@\n-    __ compiler_fast_unlock_object($crx$$CondRegister, $oop$$Register, $box$$Register,\n-                                   $tmp1$$Register, $tmp2$$Register, $tmp3$$Register,\n-                                   \/*RTM*\/ true);\n+    __ fast_unlock_lightweight($crx$$CondRegister, $oop$$Register, $box$$Register,\n+                               $tmp1$$Register, $tmp2$$Register, $tmp3$$Register);\n","filename":"src\/hotspot\/cpu\/ppc\/ppc.ad","additions":24,"deletions":30,"binary":false,"changes":54,"status":"modified"},{"patch":"@@ -2280,7 +2280,0 @@\n-  if (UseRTMLocking) {\n-    \/\/ Abort RTM transaction before calling JNI\n-    \/\/ because critical section can be large and\n-    \/\/ abort anyway. Also nmethod can be deoptimized.\n-    __ tabort_();\n-  }\n-\n@@ -2473,2 +2466,7 @@\n-    \/\/ fast_lock kills r_temp_1, r_temp_2, r_temp_3.\n-    __ compiler_fast_lock_object(CCR0, r_oop, r_box, r_temp_1, r_temp_2, r_temp_3);\n+    if (LockingMode == LM_LIGHTWEIGHT) {\n+      \/\/ fast_lock kills r_temp_1, r_temp_2, r_temp_3.\n+      __ compiler_fast_lock_lightweight_object(CCR0, r_oop, r_temp_1, r_temp_2, r_temp_3);\n+    } else {\n+      \/\/ fast_lock kills r_temp_1, r_temp_2, r_temp_3.\n+      __ compiler_fast_lock_object(CCR0, r_oop, r_box, r_temp_1, r_temp_2, r_temp_3);\n+    }\n@@ -2684,1 +2682,5 @@\n-    __ compiler_fast_unlock_object(CCR0, r_oop, r_box, r_temp_1, r_temp_2, r_temp_3);\n+    if (LockingMode == LM_LIGHTWEIGHT) {\n+      __ compiler_fast_unlock_lightweight_object(CCR0, r_oop, r_temp_1, r_temp_2, r_temp_3);\n+    } else {\n+      __ compiler_fast_unlock_object(CCR0, r_oop, r_box, r_temp_1, r_temp_2, r_temp_3);\n+    }\n@@ -3168,5 +3170,0 @@\n-  if (UseRTMLocking) {\n-    \/\/ Abort RTM transaction before possible nmethod deoptimization.\n-    __ tabort_();\n-  }\n-\n@@ -3323,7 +3320,0 @@\n-  if (UseRTMLocking) {\n-    \/\/ Abort RTM transaction before calling runtime\n-    \/\/ because critical section can be large and so\n-    \/\/ will abort anyway. Also nmethod can be deoptimized.\n-    __ tabort_();\n-  }\n-\n","filename":"src\/hotspot\/cpu\/ppc\/sharedRuntime_ppc.cpp","additions":12,"deletions":22,"binary":false,"changes":34,"status":"modified"},{"patch":"@@ -50,0 +50,1 @@\n+#include \"utilities\/globalDefinitions.hpp\"\n@@ -4706,2 +4707,0 @@\n-\/\/ Branches to slow upon failure to lock the object.\n-\/\/ Falls through upon success.\n@@ -4710,3 +4709,3 @@\n-\/\/  - hdr: the header, already loaded from obj, will be destroyed\n-\/\/  - tmp1, tmp2: temporary registers, will be destroyed\n-void MacroAssembler::lightweight_lock(Register obj, Register hdr, Register tmp1, Register tmp2, Label& slow) {\n+\/\/  - tmp1, tmp2, tmp3: temporary registers, will be destroyed\n+\/\/  - slow: branched to if locking fails\n+void MacroAssembler::lightweight_lock(Register obj, Register tmp1, Register tmp2, Register tmp3, Label& slow) {\n@@ -4714,23 +4713,39 @@\n-  assert_different_registers(obj, hdr, tmp1, tmp2, t0);\n-\n-  \/\/ Check if we would have space on lock-stack for the object.\n-  lwu(tmp1, Address(xthread, JavaThread::lock_stack_top_offset()));\n-  mv(tmp2, (unsigned)LockStack::end_offset());\n-  bge(tmp1, tmp2, slow, \/* is_far *\/ true);\n-\n-  \/\/ Load (object->mark() | 1) into hdr\n-  ori(hdr, hdr, markWord::unlocked_value);\n-  \/\/ Clear lock-bits, into tmp2\n-  xori(tmp2, hdr, markWord::unlocked_value);\n-\n-  \/\/ Try to swing header from unlocked to locked\n-  Label success;\n-  cmpxchgptr(hdr, tmp2, obj, tmp1, success, &slow);\n-  bind(success);\n-\n-  \/\/ After successful lock, push object on lock-stack\n-  lwu(tmp1, Address(xthread, JavaThread::lock_stack_top_offset()));\n-  add(tmp2, xthread, tmp1);\n-  sd(obj, Address(tmp2, 0));\n-  addw(tmp1, tmp1, oopSize);\n-  sw(tmp1, Address(xthread, JavaThread::lock_stack_top_offset()));\n+  assert_different_registers(obj, tmp1, tmp2, tmp3, t0);\n+\n+  Label push;\n+  const Register top = tmp1;\n+  const Register mark = tmp2;\n+  const Register t = tmp3;\n+\n+  \/\/ Preload the markWord. It is important that this is the first\n+  \/\/ instruction emitted as it is part of C1's null check semantics.\n+  ld(mark, Address(obj, oopDesc::mark_offset_in_bytes()));\n+\n+  \/\/ Check if the lock-stack is full.\n+  lwu(top, Address(xthread, JavaThread::lock_stack_top_offset()));\n+  mv(t, (unsigned)LockStack::end_offset());\n+  bge(top, t, slow, \/* is_far *\/ true);\n+\n+  \/\/ Check for recursion.\n+  add(t, xthread, top);\n+  ld(t, Address(t, -oopSize));\n+  beq(obj, t, push);\n+\n+  \/\/ Check header for monitor (0b10).\n+  test_bit(t, mark, exact_log2(markWord::monitor_value));\n+  bnez(t, slow, \/* is_far *\/ true);\n+\n+  \/\/ Try to lock. Transition lock-bits 0b01 => 0b00\n+  assert(oopDesc::mark_offset_in_bytes() == 0, \"required to avoid a la\");\n+  ori(mark, mark, markWord::unlocked_value);\n+  xori(t, mark, markWord::unlocked_value);\n+  cmpxchg(\/*addr*\/ obj, \/*expected*\/ mark, \/*new*\/ t, Assembler::int64,\n+          \/*acquire*\/ Assembler::aq, \/*release*\/ Assembler::relaxed, \/*result*\/ t);\n+  bne(mark, t, slow, \/* is_far *\/ true);\n+\n+  bind(push);\n+  \/\/ After successful lock, push object on lock-stack.\n+  add(t, xthread, top);\n+  sd(obj, Address(t));\n+  addw(top, top, oopSize);\n+  sw(top, Address(xthread, JavaThread::lock_stack_top_offset()));\n@@ -4740,2 +4755,0 @@\n-\/\/ Branches to slow upon failure.\n-\/\/ Falls through upon success.\n@@ -4744,3 +4757,3 @@\n-\/\/ - hdr: the (pre-loaded) header of the object\n-\/\/ - tmp1, tmp2: temporary registers\n-void MacroAssembler::lightweight_unlock(Register obj, Register hdr, Register tmp1, Register tmp2, Label& slow) {\n+\/\/ - tmp1, tmp2, tmp3: temporary registers\n+\/\/ - slow: branched to if unlocking fails\n+void MacroAssembler::lightweight_unlock(Register obj, Register tmp1, Register tmp2, Register tmp3, Label& slow) {\n@@ -4748,1 +4761,1 @@\n-  assert_different_registers(obj, hdr, tmp1, tmp2, t0);\n+  assert_different_registers(obj, tmp1, tmp2, tmp3, t0);\n@@ -4752,4 +4765,0 @@\n-    \/\/ The following checks rely on the fact that LockStack is only ever modified by\n-    \/\/ its owning thread, even if the lock got inflated concurrently; removal of LockStack\n-    \/\/ entries after inflation will happen delayed in that case.\n-\n@@ -4760,1 +4769,1 @@\n-    bgt(tmp1, tmp2, stack_ok);\n+    bge(tmp1, tmp2, stack_ok);\n@@ -4764,18 +4773,0 @@\n-  {\n-    \/\/ Check if the top of the lock-stack matches the unlocked object.\n-    Label tos_ok;\n-    subw(tmp1, tmp1, oopSize);\n-    add(tmp1, xthread, tmp1);\n-    ld(tmp1, Address(tmp1, 0));\n-    beq(tmp1, obj, tos_ok);\n-    STOP(\"Top of lock-stack does not match the unlocked object\");\n-    bind(tos_ok);\n-  }\n-  {\n-    \/\/ Check that hdr is fast-locked.\n-   Label hdr_ok;\n-    andi(tmp1, hdr, markWord::lock_mask_in_place);\n-    beqz(tmp1, hdr_ok);\n-    STOP(\"Header is not fast-locked\");\n-    bind(hdr_ok);\n-  }\n@@ -4784,2 +4775,26 @@\n-  \/\/ Load the new header (unlocked) into tmp1\n-  ori(tmp1, hdr, markWord::unlocked_value);\n+  Label unlocked, push_and_slow;\n+  const Register top = tmp1;\n+  const Register mark = tmp2;\n+  const Register t = tmp3;\n+\n+  \/\/ Check if obj is top of lock-stack.\n+  lwu(top, Address(xthread, JavaThread::lock_stack_top_offset()));\n+  subw(top, top, oopSize);\n+  add(t, xthread, top);\n+  ld(t, Address(t));\n+  bne(obj, t, slow, \/* is_far *\/ true);\n+\n+  \/\/ Pop lock-stack.\n+  DEBUG_ONLY(add(t, xthread, top);)\n+  DEBUG_ONLY(sd(zr, Address(t));)\n+  sw(top, Address(xthread, JavaThread::lock_stack_top_offset()));\n+\n+  \/\/ Check if recursive.\n+  add(t, xthread, top);\n+  ld(t, Address(t, -oopSize));\n+  beq(obj, t, unlocked);\n+\n+  \/\/ Not recursive. Check header for monitor (0b10).\n+  ld(mark, Address(obj, oopDesc::mark_offset_in_bytes()));\n+  test_bit(t, mark, exact_log2(markWord::monitor_value));\n+  bnez(t, push_and_slow);\n@@ -4787,10 +4802,6 @@\n-  \/\/ Try to swing header from locked to unlocked\n-  Label success;\n-  cmpxchgptr(hdr, tmp1, obj, tmp2, success, &slow);\n-  bind(success);\n-\n-  \/\/ After successful unlock, pop object from lock-stack\n-  lwu(tmp1, Address(xthread, JavaThread::lock_stack_top_offset()));\n-  subw(tmp1, tmp1, oopSize);\n-  add(tmp2, xthread, tmp1);\n-  sd(zr, Address(tmp2, 0));\n+  \/\/ Check header not unlocked (0b01).\n+  Label not_unlocked;\n+  test_bit(t, mark, exact_log2(markWord::unlocked_value));\n+  beqz(t, not_unlocked);\n+  stop(\"lightweight_unlock already unlocked\");\n+  bind(not_unlocked);\n@@ -4799,1 +4810,17 @@\n-  sw(tmp1, Address(xthread, JavaThread::lock_stack_top_offset()));\n+\n+  \/\/ Try to unlock. Transition lock bits 0b00 => 0b01\n+  assert(oopDesc::mark_offset_in_bytes() == 0, \"required to avoid lea\");\n+  ori(t, mark, markWord::unlocked_value);\n+  cmpxchg(\/*addr*\/ obj, \/*expected*\/ mark, \/*new*\/ t, Assembler::int64,\n+          \/*acquire*\/ Assembler::relaxed, \/*release*\/ Assembler::rl, \/*result*\/ t);\n+  beq(mark, t, unlocked);\n+\n+  bind(push_and_slow);\n+  \/\/ Restore lock-stack and handle the unlock in runtime.\n+  DEBUG_ONLY(add(t, xthread, top);)\n+  DEBUG_ONLY(sd(obj, Address(t));)\n+  addw(top, top, oopSize);\n+  sw(top, Address(xthread, JavaThread::lock_stack_top_offset()));\n+  j(slow);\n+\n+  bind(unlocked);\n","filename":"src\/hotspot\/cpu\/riscv\/macroAssembler_riscv.cpp","additions":97,"deletions":70,"binary":false,"changes":167,"status":"modified"},{"patch":"@@ -1455,2 +1455,2 @@\n-  void lightweight_lock(Register obj, Register hdr, Register tmp1, Register tmp2, Label& slow);\n-  void lightweight_unlock(Register obj, Register hdr, Register tmp1, Register tmp2, Label& slow);\n+  void lightweight_lock(Register obj, Register tmp1, Register tmp2, Register tmp3, Label& slow);\n+  void lightweight_unlock(Register obj, Register tmp1, Register tmp2, Register tmp3, Label& slow);\n","filename":"src\/hotspot\/cpu\/riscv\/macroAssembler_riscv.hpp","additions":2,"deletions":2,"binary":false,"changes":4,"status":"modified"},{"patch":"@@ -1707,2 +1707,1 @@\n-      assert(LockingMode == LM_LIGHTWEIGHT, \"\");\n-      __ ld(swap_reg, Address(obj_reg, oopDesc::mark_offset_in_bytes()));\n+      assert(LockingMode == LM_LIGHTWEIGHT, \"must be\");\n@@ -1834,3 +1833,0 @@\n-      __ ld(old_hdr, Address(obj_reg, oopDesc::mark_offset_in_bytes()));\n-      __ test_bit(t0, old_hdr, exact_log2(markWord::monitor_value));\n-      __ bnez(t0, slow_path_unlock);\n","filename":"src\/hotspot\/cpu\/riscv\/sharedRuntime_riscv.cpp","additions":1,"deletions":5,"binary":false,"changes":6,"status":"modified"},{"patch":"@@ -212,0 +212,2 @@\n+  constexpr static bool supports_recursive_lightweight_locking() { return true; }\n+\n","filename":"src\/hotspot\/cpu\/riscv\/vm_version_riscv.hpp","additions":2,"deletions":0,"binary":false,"changes":2,"status":"modified"},{"patch":"@@ -256,0 +256,5 @@\n+void LoadKlassStub::emit_code(LIR_Assembler* ce) {\n+  \/\/ Currently not needed.\n+  Unimplemented();\n+}\n+\n","filename":"src\/hotspot\/cpu\/s390\/c1_CodeStubs_s390.cpp","additions":5,"deletions":0,"binary":false,"changes":5,"status":"modified"},{"patch":"@@ -2,1 +2,1 @@\n- * Copyright (c) 2003, 2023, Oracle and\/or its affiliates. All rights reserved.\n+ * Copyright (c) 2003, 2024, Oracle and\/or its affiliates. All rights reserved.\n@@ -1717,2 +1717,0 @@\n-      \/\/ Load object header\n-      __ movptr(swap_reg, Address(obj_reg, oopDesc::mark_offset_in_bytes()));\n@@ -1876,3 +1874,1 @@\n-      __ movptr(swap_reg, Address(obj_reg, oopDesc::mark_offset_in_bytes()));\n-      __ andptr(swap_reg, ~(int32_t)markWord::lock_mask_in_place);\n-      __ lightweight_unlock(obj_reg, swap_reg, lock_reg, slow_path_unlock);\n+      __ lightweight_unlock(obj_reg, swap_reg, thread, lock_reg, slow_path_unlock);\n","filename":"src\/hotspot\/cpu\/x86\/sharedRuntime_x86_32.cpp","additions":2,"deletions":6,"binary":false,"changes":8,"status":"modified"},{"patch":"@@ -2,1 +2,1 @@\n- * Copyright (c) 2003, 2023, Oracle and\/or its affiliates. All rights reserved.\n+ * Copyright (c) 2003, 2024, Oracle and\/or its affiliates. All rights reserved.\n@@ -2192,2 +2192,0 @@\n-      \/\/ Load object header\n-      __ movptr(swap_reg, Address(obj_reg, oopDesc::mark_offset_in_bytes()));\n@@ -2336,3 +2334,1 @@\n-      __ movptr(swap_reg, Address(obj_reg, oopDesc::mark_offset_in_bytes()));\n-      __ andptr(swap_reg, ~(int32_t)markWord::lock_mask_in_place);\n-      __ lightweight_unlock(obj_reg, swap_reg, lock_reg, slow_path_unlock);\n+      __ lightweight_unlock(obj_reg, swap_reg, r15_thread, lock_reg, slow_path_unlock);\n","filename":"src\/hotspot\/cpu\/x86\/sharedRuntime_x86_64.cpp","additions":2,"deletions":6,"binary":false,"changes":8,"status":"modified"},{"patch":"@@ -91,0 +91,1 @@\n+#include \"gc\/shared\/slidingForwarding.hpp\"\n@@ -1524,0 +1525,2 @@\n+  SlidingForwarding::initialize(heap_rs.region(), HeapRegion::GrainWords);\n+\n","filename":"src\/hotspot\/share\/gc\/g1\/g1CollectedHeap.cpp","additions":3,"deletions":0,"binary":false,"changes":3,"status":"modified"},{"patch":"@@ -43,0 +43,1 @@\n+#include \"gc\/shared\/slidingForwarding.hpp\"\n@@ -212,0 +213,2 @@\n+  SlidingForwarding::begin();\n+\n@@ -224,0 +227,2 @@\n+  SlidingForwarding::end();\n+\n@@ -392,1 +397,2 @@\n-void G1FullCollector::phase2c_prepare_serial_compaction() {\n+template <bool ALT_FWD>\n+void G1FullCollector::phase2c_prepare_serial_compaction_impl() {\n@@ -417,1 +423,1 @@\n-  G1SerialRePrepareClosure re_prepare(serial_cp, dense_prefix_top);\n+  G1SerialRePrepareClosure<ALT_FWD> re_prepare(serial_cp, dense_prefix_top);\n@@ -430,1 +436,10 @@\n-void G1FullCollector::phase2d_prepare_humongous_compaction() {\n+void G1FullCollector::phase2c_prepare_serial_compaction() {\n+  if (UseAltGCForwarding) {\n+    phase2c_prepare_serial_compaction_impl<true>();\n+  } else {\n+    phase2c_prepare_serial_compaction_impl<false>();\n+  }\n+}\n+\n+template <bool ALT_FWD>\n+void G1FullCollector::phase2d_prepare_humongous_compaction_impl() {\n@@ -448,1 +463,1 @@\n-      uint num_regions = humongous_cp->forward_humongous(hr);\n+      uint num_regions = humongous_cp->forward_humongous<ALT_FWD>(hr);\n@@ -459,0 +474,8 @@\n+void G1FullCollector::phase2d_prepare_humongous_compaction() {\n+  if (UseAltGCForwarding) {\n+    phase2d_prepare_humongous_compaction_impl<true>();\n+  } else {\n+    phase2d_prepare_humongous_compaction_impl<false>();\n+  }\n+}\n+\n","filename":"src\/hotspot\/share\/gc\/g1\/g1FullCollector.cpp","additions":27,"deletions":4,"binary":false,"changes":31,"status":"modified"},{"patch":"@@ -236,1 +236,2 @@\n-void MutableSpace::object_iterate(ObjectClosure* cl) {\n+template<bool COMPACT_HEADERS>\n+void MutableSpace::object_iterate_impl(ObjectClosure* cl) {\n@@ -245,3 +246,2 @@\n-    }\n-#ifdef ASSERT\n-    else {\n+      p += obj->size();\n+    } else {\n@@ -249,0 +249,8 @@\n+      if (COMPACT_HEADERS) {\n+        \/\/ It is safe to use the forwardee here. Parallel GC only uses\n+        \/\/ header-based forwarding during promotion. Full GC doesn't\n+        \/\/ use the object header for forwarding at all.\n+        p += obj->forwardee()->size();\n+      } else {\n+        p += obj->size();\n+      }\n@@ -250,2 +258,8 @@\n-#endif\n-    p += cast_to_oop(p)->size();\n+  }\n+}\n+\n+void MutableSpace::object_iterate(ObjectClosure* cl) {\n+  if (UseCompactObjectHeaders) {\n+    object_iterate_impl<true>(cl);\n+  } else {\n+    object_iterate_impl<false>(cl);\n","filename":"src\/hotspot\/share\/gc\/parallel\/mutableSpace.cpp","additions":20,"deletions":6,"binary":false,"changes":26,"status":"modified"},{"patch":"@@ -49,0 +49,1 @@\n+#include \"gc\/shared\/slidingForwarding.hpp\"\n@@ -91,0 +92,2 @@\n+  SlidingForwarding::begin();\n+\n@@ -109,0 +112,2 @@\n+  SlidingForwarding::end();\n+\n@@ -263,9 +268,21 @@\n-  CodeBlobToOopClosure code_closure(&adjust_pointer_closure, CodeBlobToOopClosure::FixRelocations);\n-  gch->process_roots(GenCollectedHeap::SO_AllCodeCache,\n-                     &adjust_pointer_closure,\n-                     &adjust_cld_closure,\n-                     &adjust_cld_closure,\n-                     &code_closure);\n-\n-  gch->gen_process_weak_roots(&adjust_pointer_closure);\n-\n+  if (UseAltGCForwarding) {\n+    AdjustPointerClosure<true> adjust_pointer_closure;\n+    CLDToOopClosure adjust_cld_closure(&adjust_pointer_closure, ClassLoaderData::_claim_stw_fullgc_adjust);\n+    CodeBlobToOopClosure code_closure(&adjust_pointer_closure, CodeBlobToOopClosure::FixRelocations);\n+    gch->process_roots(GenCollectedHeap::SO_AllCodeCache,\n+                       &adjust_pointer_closure,\n+                       &adjust_cld_closure,\n+                       &adjust_cld_closure,\n+                       &code_closure);\n+    gch->gen_process_weak_roots(&adjust_pointer_closure);\n+  } else {\n+    AdjustPointerClosure<false> adjust_pointer_closure;\n+    CLDToOopClosure adjust_cld_closure(&adjust_pointer_closure, ClassLoaderData::_claim_stw_fullgc_adjust);\n+    CodeBlobToOopClosure code_closure(&adjust_pointer_closure, CodeBlobToOopClosure::FixRelocations);\n+    gch->process_roots(GenCollectedHeap::SO_AllCodeCache,\n+                       &adjust_pointer_closure,\n+                       &adjust_cld_closure,\n+                       &adjust_cld_closure,\n+                       &code_closure);\n+    gch->gen_process_weak_roots(&adjust_pointer_closure);\n+  }\n","filename":"src\/hotspot\/share\/gc\/serial\/genMarkSweep.cpp","additions":26,"deletions":9,"binary":false,"changes":35,"status":"modified"},{"patch":"@@ -58,0 +58,1 @@\n+#include \"gc\/shared\/slidingForwarding.hpp\"\n@@ -136,0 +137,2 @@\n+  SlidingForwarding::initialize(_reserved, SpaceAlignment \/ HeapWordSize);\n+\n","filename":"src\/hotspot\/share\/gc\/shared\/genCollectedHeap.cpp","additions":3,"deletions":0,"binary":false,"changes":3,"status":"modified"},{"patch":"@@ -37,0 +37,1 @@\n+#include \"gc\/shared\/slidingForwarding.hpp\"\n@@ -407,0 +408,2 @@\n+  SlidingForwarding::initialize(_heap_region, ShenandoahHeapRegion::region_size_words());\n+\n","filename":"src\/hotspot\/share\/gc\/shenandoah\/shenandoahHeap.cpp","additions":3,"deletions":0,"binary":false,"changes":3,"status":"modified"},{"patch":"@@ -101,1 +101,1 @@\n-      if (is_instance_ref_klass(obj->klass())) {\n+      if (is_instance_ref_klass(obj->forward_safe_klass())) {\n@@ -128,1 +128,1 @@\n-    Klass* obj_klass = obj->klass_or_null();\n+    Klass* obj_klass = obj->forward_safe_klass();\n@@ -143,1 +143,1 @@\n-        check(ShenandoahAsserts::_safe_unknown, obj, (obj_addr + obj->size()) <= obj_reg->top(),\n+        check(ShenandoahAsserts::_safe_unknown, obj, (obj_addr + obj->forward_safe_size()) <= obj_reg->top(),\n@@ -147,1 +147,1 @@\n-        size_t humongous_end = humongous_start + (obj->size() >> ShenandoahHeapRegion::region_size_words_shift());\n+        size_t humongous_end = humongous_start + (obj->forward_safe_size() >> ShenandoahHeapRegion::region_size_words_shift());\n@@ -164,1 +164,1 @@\n-          Atomic::add(&_ld[obj_reg->index()], (uint) obj->size(), memory_order_relaxed);\n+          Atomic::add(&_ld[obj_reg->index()], (uint) obj->forward_safe_size(), memory_order_relaxed);\n@@ -205,1 +205,1 @@\n-      check(ShenandoahAsserts::_safe_oop, obj, (fwd_addr + fwd->size()) <= fwd_reg->top(),\n+      check(ShenandoahAsserts::_safe_oop, obj, (fwd_addr + fwd->forward_safe_size()) <= fwd_reg->top(),\n@@ -308,1 +308,2 @@\n-    obj->oop_iterate(this);\n+    Klass* klass = obj->forward_safe_klass();\n+    obj->oop_iterate_backwards(this, klass);\n@@ -588,1 +589,1 @@\n-    if (!is_instance_ref_klass(obj->klass())) {\n+    if (!is_instance_ref_klass(obj->forward_safe_klass())) {\n","filename":"src\/hotspot\/share\/gc\/shenandoah\/shenandoahVerifier.cpp","additions":9,"deletions":8,"binary":false,"changes":17,"status":"modified"},{"patch":"@@ -2389,1 +2389,1 @@\n-  return arrayOopDesc::header_size(type) * HeapWordSize;\n+  return arrayOopDesc::base_offset_in_bytes(type);\n","filename":"src\/hotspot\/share\/jvmci\/jvmciCompilerToVM.cpp","additions":1,"deletions":1,"binary":false,"changes":2,"status":"modified"},{"patch":"@@ -194,0 +194,10 @@\n+static markWord make_prototype(Klass* kls) {\n+  markWord prototype = markWord::prototype();\n+#ifdef _LP64\n+  if (UseCompactObjectHeaders) {\n+    prototype = prototype.set_klass(kls);\n+  }\n+#endif\n+  return prototype;\n+}\n+\n@@ -199,0 +209,1 @@\n+                           _prototype_header(make_prototype(this)),\n@@ -743,0 +754,4 @@\n+     if (UseCompactObjectHeaders) {\n+       st->print(BULLET\"prototype_header: \" INTPTR_FORMAT, _prototype_header.value());\n+       st->cr();\n+     }\n","filename":"src\/hotspot\/share\/oops\/klass.cpp","additions":15,"deletions":0,"binary":false,"changes":15,"status":"modified"},{"patch":"@@ -55,0 +55,5 @@\n+inline void Klass::set_prototype_header(markWord header) {\n+  assert(UseCompactObjectHeaders, \"only with compact headers\");\n+  _prototype_header = header;\n+}\n+\n","filename":"src\/hotspot\/share\/oops\/klass.inline.hpp","additions":5,"deletions":0,"binary":false,"changes":5,"status":"modified"},{"patch":"@@ -1692,0 +1692,4 @@\n+      if (UseCompactObjectHeaders) {\n+        if (flat->offset() == in_bytes(Klass::prototype_header_offset()))\n+          alias_type(idx)->set_rewritable(false);\n+      }\n","filename":"src\/hotspot\/share\/opto\/compile.cpp","additions":4,"deletions":0,"binary":false,"changes":4,"status":"modified"},{"patch":"@@ -1877,0 +1877,7 @@\n+  if (UseCompactObjectHeaders) {\n+    if (tkls->offset() == in_bytes(Klass::prototype_header_offset())) {\n+      \/\/ The field is Klass::_prototype_header.  Return its (constant) value.\n+      assert(this->Opcode() == Op_LoadX, \"must load a proper type from _prototype_header\");\n+      return TypeX::make(klass->prototype_header());\n+    }\n+  }\n@@ -2049,0 +2056,7 @@\n+      if (UseCompactObjectHeaders) {\n+        if (tkls->offset() == in_bytes(Klass::prototype_header_offset())) {\n+          \/\/ The field is Klass::_prototype_header. Return its (constant) value.\n+          assert(this->Opcode() == Op_LoadX, \"must load a proper type from _prototype_header\");\n+          return TypeX::make(klass->prototype_header());\n+        }\n+      }\n@@ -2139,1 +2153,1 @@\n-  if (alloc != nullptr) {\n+  if (!UseCompactObjectHeaders && alloc != nullptr) {\n","filename":"src\/hotspot\/share\/opto\/memnode.cpp","additions":15,"deletions":1,"binary":false,"changes":16,"status":"modified"},{"patch":"@@ -5171,1 +5171,2 @@\n-    int header_size = objArrayOopDesc::header_size() * wordSize;\n+    BasicType basic_elem_type = elem()->basic_type();\n+    int header_size = arrayOopDesc::base_offset_in_bytes(basic_elem_type);\n@@ -5176,1 +5177,0 @@\n-      BasicType basic_elem_type = elem()->basic_type();\n","filename":"src\/hotspot\/share\/opto\/type.cpp","additions":2,"deletions":2,"binary":false,"changes":4,"status":"modified"},{"patch":"@@ -85,0 +85,1 @@\n+#include \"runtime\/lockStack.hpp\"\n@@ -1834,0 +1835,8 @@\n+WB_ENTRY(jint, WB_getLockStackCapacity(JNIEnv* env))\n+  return (jint) LockStack::CAPACITY;\n+WB_END\n+\n+WB_ENTRY(jboolean, WB_supportsRecursiveLightweightLocking(JNIEnv* env))\n+  return (jboolean) VM_Version::supports_recursive_lightweight_locking();\n+WB_END\n+\n@@ -2753,0 +2762,2 @@\n+  {CC\"getLockStackCapacity\", CC\"()I\",                 (void*)&WB_getLockStackCapacity },\n+  {CC\"supportsRecursiveLightweightLocking\", CC\"()Z\",  (void*)&WB_supportsRecursiveLightweightLocking },\n","filename":"src\/hotspot\/share\/prims\/whitebox.cpp","additions":11,"deletions":0,"binary":false,"changes":11,"status":"modified"},{"patch":"@@ -133,0 +133,3 @@\n+  product(bool, UseCompactObjectHeaders, false, EXPERIMENTAL,               \\\n+          \"Use compact 64-bit object headers in 64-bit VM\")                 \\\n+                                                                            \\\n@@ -150,0 +153,1 @@\n+const bool UseCompactObjectHeaders = false;\n","filename":"src\/hotspot\/share\/runtime\/globals.hpp","additions":4,"deletions":0,"binary":false,"changes":4,"status":"modified"},{"patch":"@@ -2,1 +2,1 @@\n- * Copyright (c) 1998, 2023, Oracle and\/or its affiliates. All rights reserved.\n+ * Copyright (c) 1998, 2024, Oracle and\/or its affiliates. All rights reserved.\n@@ -42,0 +42,1 @@\n+#include \"runtime\/globals.hpp\"\n@@ -56,0 +57,1 @@\n+#include \"utilities\/globalDefinitions.hpp\"\n@@ -315,0 +317,62 @@\n+bool ObjectMonitor::enter_for(JavaThread* locking_thread) {\n+  \/\/ Used by ObjectSynchronizer::enter_for to enter for another thread.\n+  \/\/ The monitor is private to or already owned by locking_thread which must be suspended.\n+  \/\/ So this code may only contend with deflation.\n+  assert(locking_thread == Thread::current() || locking_thread->is_obj_deopt_suspend(), \"must be\");\n+\n+  \/\/ Block out deflation as soon as possible.\n+  add_to_contentions(1);\n+\n+  bool success = false;\n+  if (!is_being_async_deflated()) {\n+    void* prev_owner = try_set_owner_from(nullptr, locking_thread);\n+\n+    if (prev_owner == nullptr) {\n+      assert(_recursions == 0, \"invariant\");\n+      success = true;\n+    } else if (prev_owner == locking_thread) {\n+      _recursions++;\n+      success = true;\n+    } else if (prev_owner == DEFLATER_MARKER) {\n+      \/\/ Racing with deflation.\n+      prev_owner = try_set_owner_from(DEFLATER_MARKER, locking_thread);\n+      if (prev_owner == DEFLATER_MARKER) {\n+        \/\/ Cancelled deflation. Increment contentions as part of the deflation protocol.\n+        add_to_contentions(1);\n+        success = true;\n+      } else if (prev_owner == nullptr) {\n+        \/\/ At this point we cannot race with deflation as we have both incremented\n+        \/\/ contentions, seen contention > 0 and seen a DEFLATER_MARKER.\n+        \/\/ success will only be false if this races with something other than\n+        \/\/ deflation.\n+        prev_owner = try_set_owner_from(nullptr, locking_thread);\n+        success = prev_owner == nullptr;\n+      }\n+    } else if (LockingMode == LM_LEGACY && locking_thread->is_lock_owned((address)prev_owner)) {\n+      assert(_recursions == 0, \"must be\");\n+      _recursions = 1;\n+      set_owner_from_BasicLock(prev_owner, locking_thread);\n+      success = true;\n+    }\n+    assert(success, \"Failed to enter_for: locking_thread=\" INTPTR_FORMAT\n+           \", this=\" INTPTR_FORMAT \"{owner=\" INTPTR_FORMAT \"}, observed owner: \" INTPTR_FORMAT,\n+           p2i(locking_thread), p2i(this), p2i(owner_raw()), p2i(prev_owner));\n+  } else {\n+    \/\/ Async deflation is in progress and our contentions increment\n+    \/\/ above lost the race to async deflation. Undo the work and\n+    \/\/ force the caller to retry.\n+    const oop l_object = object();\n+    if (l_object != nullptr) {\n+      \/\/ Attempt to restore the header\/dmw to the object's header so that\n+      \/\/ we only retry once if the deflater thread happens to be slow.\n+      install_displaced_markword_in_object(l_object);\n+    }\n+  }\n+\n+  add_to_contentions(-1);\n+\n+  assert(!success || owner_raw() == locking_thread, \"must be\");\n+\n+  return success;\n+}\n+\n@@ -316,0 +380,1 @@\n+  assert(current == JavaThread::current(), \"must be\");\n","filename":"src\/hotspot\/share\/runtime\/objectMonitor.cpp","additions":66,"deletions":1,"binary":false,"changes":67,"status":"modified"},{"patch":"@@ -3260,6 +3260,11 @@\n-      \/\/ Inflate so the object's header no longer refers to the BasicLock.\n-      if (lock->displaced_header().is_unlocked()) {\n-        \/\/ The object is locked and the resulting ObjectMonitor* will also be\n-        \/\/ locked so it can't be async deflated until ownership is dropped.\n-        \/\/ See the big comment in basicLock.cpp: BasicLock::move_to().\n-        ObjectSynchronizer::inflate_helper(kptr2->obj());\n+      if (LockingMode == LM_LEGACY) {\n+        \/\/ Inflate so the object's header no longer refers to the BasicLock.\n+        if (lock->displaced_header().is_unlocked()) {\n+          \/\/ The object is locked and the resulting ObjectMonitor* will also be\n+          \/\/ locked so it can't be async deflated until ownership is dropped.\n+          \/\/ See the big comment in basicLock.cpp: BasicLock::move_to().\n+          ObjectSynchronizer::inflate_helper(kptr2->obj());\n+        }\n+        \/\/ Now the displaced header is free to move because the\n+        \/\/ object's header no longer refers to it.\n+        buf[i] = (intptr_t)lock->displaced_header().value();\n@@ -3267,3 +3272,6 @@\n-      \/\/ Now the displaced header is free to move because the\n-      \/\/ object's header no longer refers to it.\n-      buf[i++] = (intptr_t)lock->displaced_header().value();\n+#ifdef ASSERT\n+      else {\n+        buf[i] = badDispHeaderOSR;\n+      }\n+#endif\n+      i++;\n","filename":"src\/hotspot\/share\/runtime\/sharedRuntime.cpp","additions":17,"deletions":9,"binary":false,"changes":26,"status":"modified"},{"patch":"@@ -2,1 +2,1 @@\n- * Copyright (c) 1998, 2023, Oracle and\/or its affiliates. All rights reserved.\n+ * Copyright (c) 1998, 2024, Oracle and\/or its affiliates. All rights reserved.\n@@ -39,0 +39,1 @@\n+#include \"runtime\/globals.hpp\"\n@@ -63,0 +64,1 @@\n+#include \"utilities\/globalDefinitions.hpp\"\n@@ -387,0 +389,13 @@\n+  if (LockingMode == LM_LIGHTWEIGHT) {\n+    LockStack& lock_stack = current->lock_stack();\n+    if (lock_stack.is_full()) {\n+      \/\/ Always go into runtime if the lock stack is full.\n+      return false;\n+    }\n+    if (lock_stack.try_recursive_enter(obj)) {\n+      \/\/ Recursive lock successful.\n+      current->inc_held_monitor_count();\n+      return true;\n+    }\n+  }\n+\n@@ -440,2 +455,3 @@\n-void ObjectSynchronizer::handle_sync_on_value_based_class(Handle obj, JavaThread* current) {\n-  frame last_frame = current->last_frame();\n+void ObjectSynchronizer::handle_sync_on_value_based_class(Handle obj, JavaThread* locking_thread) {\n+  assert(locking_thread == Thread::current() || locking_thread->is_obj_deopt_suspend(), \"must be\");\n+  frame last_frame = locking_thread->last_frame();\n@@ -454,1 +470,1 @@\n-    ResourceMark rm(current);\n+    ResourceMark rm;\n@@ -456,1 +472,1 @@\n-    current->print_active_stack_on(&ss);\n+    locking_thread->print_active_stack_on(&ss);\n@@ -465,1 +481,1 @@\n-    ResourceMark rm(current);\n+    ResourceMark rm;\n@@ -469,1 +485,1 @@\n-    if (current->has_last_Java_frame()) {\n+    if (locking_thread->has_last_Java_frame()) {\n@@ -471,1 +487,1 @@\n-      current->print_active_stack_on(&info_stream);\n+      locking_thread->print_active_stack_on(&info_stream);\n@@ -498,0 +514,39 @@\n+\n+void ObjectSynchronizer::enter_for(Handle obj, BasicLock* lock, JavaThread* locking_thread) {\n+  \/\/ When called with locking_thread != Thread::current() some mechanism must synchronize\n+  \/\/ the locking_thread with respect to the current thread. Currently only used when\n+  \/\/ deoptimizing and re-locking locks. See Deoptimization::relock_objects\n+  assert(locking_thread == Thread::current() || locking_thread->is_obj_deopt_suspend(), \"must be\");\n+  if (!enter_fast_impl(obj, lock, locking_thread)) {\n+    \/\/ Inflated ObjectMonitor::enter_for is required\n+\n+    \/\/ An async deflation can race after the inflate_for() call and before\n+    \/\/ enter_for() can make the ObjectMonitor busy. enter_for() returns false\n+    \/\/ if we have lost the race to async deflation and we simply try again.\n+    while (true) {\n+      ObjectMonitor* monitor = inflate_for(locking_thread, obj(), inflate_cause_monitor_enter);\n+      if (monitor->enter_for(locking_thread)) {\n+        return;\n+      }\n+      assert(monitor->is_being_async_deflated(), \"must be\");\n+    }\n+  }\n+}\n+\n+void ObjectSynchronizer::enter(Handle obj, BasicLock* lock, JavaThread* current) {\n+  assert(current == Thread::current(), \"must be\");\n+  if (!enter_fast_impl(obj, lock, current)) {\n+    \/\/ Inflated ObjectMonitor::enter is required\n+\n+    \/\/ An async deflation can race after the inflate() call and before\n+    \/\/ enter() can make the ObjectMonitor busy. enter() returns false if\n+    \/\/ we have lost the race to async deflation and we simply try again.\n+    while (true) {\n+      ObjectMonitor* monitor = inflate(current, obj(), inflate_cause_monitor_enter);\n+      if (monitor->enter(current)) {\n+        return;\n+      }\n+    }\n+  }\n+}\n+\n@@ -501,0 +556,1 @@\n+bool ObjectSynchronizer::enter_fast_impl(Handle obj, BasicLock* lock, JavaThread* locking_thread) {\n@@ -502,2 +558,1 @@\n-void ObjectSynchronizer::enter(Handle obj, BasicLock* lock, JavaThread* current) {\n-    handle_sync_on_value_based_class(obj, current);\n+    handle_sync_on_value_based_class(obj, locking_thread);\n@@ -507,1 +562,1 @@\n-  current->inc_held_monitor_count();\n+  locking_thread->inc_held_monitor_count();\n@@ -512,13 +567,41 @@\n-      LockStack& lock_stack = current->lock_stack();\n-      if (lock_stack.can_push()) {\n-        markWord mark = obj()->mark_acquire();\n-        if (mark.is_neutral()) {\n-          assert(!lock_stack.contains(obj()), \"thread must not already hold the lock\");\n-          \/\/ Try to swing into 'fast-locked' state.\n-          markWord locked_mark = mark.set_fast_locked();\n-          markWord old_mark = obj()->cas_set_mark(locked_mark, mark);\n-          if (old_mark == mark) {\n-            \/\/ Successfully fast-locked, push object to lock-stack and return.\n-            lock_stack.push(obj());\n-            return;\n-          }\n+      LockStack& lock_stack = locking_thread->lock_stack();\n+      if (lock_stack.is_full()) {\n+        \/\/ We unconditionally make room on the lock stack by inflating\n+        \/\/ the least recently locked object on the lock stack.\n+\n+        \/\/ About the choice to inflate least recently locked object.\n+        \/\/ First we must chose to inflate a lock, either some lock on\n+        \/\/ the lock-stack or the lock that is currently being entered\n+        \/\/ (which may or may not be on the lock-stack).\n+        \/\/ Second the best lock to inflate is a lock which is entered\n+        \/\/ in a control flow where there are only a very few locks being\n+        \/\/ used, as the costly part of inflated locking is inflation,\n+        \/\/ not locking. But this property is entirely program dependent.\n+        \/\/ Third inflating the lock currently being entered on when it\n+        \/\/ is not present on the lock-stack will result in a still full\n+        \/\/ lock-stack. This creates a scenario where every deeper nested\n+        \/\/ monitorenter must call into the runtime.\n+        \/\/ The rational here is as follows:\n+        \/\/ Because we cannot (currently) figure out the second, and want\n+        \/\/ to avoid the third, we inflate a lock on the lock-stack.\n+        \/\/ The least recently locked lock is chosen as it is the lock\n+        \/\/ with the longest critical section.\n+\n+        log_info(monitorinflation)(\"LockStack capacity exceeded, inflating.\");\n+        ObjectMonitor* monitor = inflate_for(locking_thread, lock_stack.bottom(), inflate_cause_vm_internal);\n+        assert(monitor->owner() == Thread::current(), \"must be owner=\" PTR_FORMAT \" current=\" PTR_FORMAT \" mark=\" PTR_FORMAT,\n+               p2i(monitor->owner()), p2i(Thread::current()), monitor->object()->mark_acquire().value());\n+        assert(!lock_stack.is_full(), \"must have made room here\");\n+      }\n+\n+      markWord mark = obj()->mark_acquire();\n+      while (mark.is_neutral()) {\n+        \/\/ Retry until a lock state change has been observed. cas_set_mark() may collide with non lock bits modifications.\n+        \/\/ Try to swing into 'fast-locked' state.\n+        assert(!lock_stack.contains(obj()), \"thread must not already hold the lock\");\n+        const markWord locked_mark = mark.set_fast_locked();\n+        const markWord old_mark = obj()->cas_set_mark(locked_mark, mark);\n+        if (old_mark == mark) {\n+          \/\/ Successfully fast-locked, push object to lock-stack and return.\n+          lock_stack.push(obj());\n+          return true;\n@@ -526,0 +609,1 @@\n+        mark = old_mark;\n@@ -527,1 +611,8 @@\n-      \/\/ All other paths fall-through to inflate-enter.\n+\n+      if (mark.is_fast_locked() && lock_stack.try_recursive_enter(obj())) {\n+        \/\/ Recursive lock successful.\n+        return true;\n+      }\n+\n+      \/\/ Failed to fast lock.\n+      return false;\n@@ -535,1 +626,1 @@\n-          return;\n+          return true;\n@@ -537,2 +628,1 @@\n-        \/\/ Fall through to inflate() ...\n-                 current->is_lock_owned((address) mark.locker())) {\n+                 locking_thread->is_lock_owned((address) mark.locker())) {\n@@ -543,1 +633,1 @@\n-        return;\n+        return true;\n@@ -551,0 +641,3 @@\n+\n+      \/\/ Failed to fast lock.\n+      return false;\n@@ -556,9 +649,1 @@\n-  \/\/ An async deflation can race after the inflate() call and before\n-  \/\/ enter() can make the ObjectMonitor busy. enter() returns false if\n-  \/\/ we have lost the race to async deflation and we simply try again.\n-  while (true) {\n-    ObjectMonitor* monitor = inflate(current, obj(), inflate_cause_monitor_enter);\n-    if (monitor->enter(current)) {\n-      return;\n-    }\n-  }\n+  return false;\n@@ -574,16 +659,3 @@\n-      if (mark.is_fast_locked()) {\n-        markWord unlocked_mark = mark.set_unlocked();\n-        markWord old_mark = object->cas_set_mark(unlocked_mark, mark);\n-        if (old_mark != mark) {\n-          \/\/ Another thread won the CAS, it must have inflated the monitor.\n-          \/\/ It can only have installed an anonymously locked monitor at this point.\n-          \/\/ Fetch that monitor, set owner correctly to this thread, and\n-          \/\/ exit it (allowing waiting threads to enter).\n-          assert(old_mark.has_monitor(), \"must have monitor\");\n-          ObjectMonitor* monitor = old_mark.monitor();\n-          assert(monitor->is_owner_anonymous(), \"must be anonymous owner\");\n-          monitor->set_owner_from_anonymous(current);\n-          monitor->exit(current);\n-        }\n-        LockStack& lock_stack = current->lock_stack();\n-        lock_stack.remove(object);\n+      LockStack& lock_stack = current->lock_stack();\n+      if (mark.is_fast_locked() && lock_stack.try_recursive_exit(object)) {\n+        \/\/ Recursively unlocked.\n@@ -592,0 +664,18 @@\n+\n+      if (mark.is_fast_locked() && lock_stack.is_recursive(object)) {\n+        \/\/ This lock is recursive but is not at the top of the lock stack so we're\n+        \/\/ doing an unbalanced exit. We have to fall thru to inflation below and\n+        \/\/ let ObjectMonitor::exit() do the unlock.\n+      } else {\n+        while (mark.is_fast_locked()) {\n+          \/\/ Retry until a lock state change has been observed. cas_set_mark() may collide with non lock bits modifications.\n+          const markWord unlocked_mark = mark.set_unlocked();\n+          const markWord old_mark = object->cas_set_mark(unlocked_mark, mark);\n+          if (old_mark == mark) {\n+            size_t recursions = lock_stack.remove(object) - 1;\n+            assert(recursions == 0, \"must not be recursive here\");\n+            return;\n+          }\n+          mark = old_mark;\n+        }\n+      }\n@@ -640,7 +730,1 @@\n-  if (LockingMode == LM_LIGHTWEIGHT && monitor->is_owner_anonymous()) {\n-    \/\/ It must be owned by us. Pop lock object from lock stack.\n-    LockStack& lock_stack = current->lock_stack();\n-    oop popped = lock_stack.pop();\n-    assert(popped == object, \"must be owned by this thread\");\n-    monitor->set_owner_from_anonymous(current);\n-  }\n+  assert(!monitor->is_owner_anonymous(), \"must not be\");\n@@ -904,1 +988,1 @@\n-  value &= markWord::hash_mask;\n+  value &= UseCompactObjectHeaders ? markWord::hash_mask_compact : markWord::hash_mask;\n@@ -910,7 +994,0 @@\n-\/\/ Can be called from non JavaThreads (e.g., VMThread) for FastHashCode\n-\/\/ calculations as part of JVM\/TI tagging.\n-static bool is_lock_owned(Thread* thread, oop obj) {\n-  assert(LockingMode == LM_LIGHTWEIGHT, \"only call this with new lightweight locking enabled\");\n-  return thread->is_Java_thread() ? JavaThread::cast(thread)->lock_stack().contains(obj) : false;\n-}\n-\n@@ -928,1 +1005,1 @@\n-    if (mark.is_neutral()) {               \/\/ if this is a normal header\n+    if (mark.is_neutral() || (LockingMode == LM_LIGHTWEIGHT && mark.is_fast_locked())) {\n@@ -940,0 +1017,4 @@\n+      if (LockingMode == LM_LIGHTWEIGHT) {\n+        \/\/ CAS failed, retry\n+        continue;\n+      }\n@@ -971,7 +1052,0 @@\n-    } else if (LockingMode == LM_LIGHTWEIGHT && mark.is_fast_locked() && is_lock_owned(current, obj)) {\n-      \/\/ This is a fast-lock owned by the calling thread so use the\n-      \/\/ markWord from the object.\n-      hash = mark.hash();\n-      if (hash != 0) {                  \/\/ if it has a hash, just return it\n-        return hash;\n-      }\n@@ -1307,2 +1381,22 @@\n-ObjectMonitor* ObjectSynchronizer::inflate(Thread* current, oop object,\n-                                           const InflateCause cause) {\n+ObjectMonitor* ObjectSynchronizer::inflate(Thread* current, oop obj, const InflateCause cause) {\n+  assert(current == Thread::current(), \"must be\");\n+  if (LockingMode == LM_LIGHTWEIGHT && current->is_Java_thread()) {\n+    return inflate_impl(JavaThread::cast(current), obj, cause);\n+  }\n+  return inflate_impl(nullptr, obj, cause);\n+}\n+\n+ObjectMonitor* ObjectSynchronizer::inflate_for(JavaThread* thread, oop obj, const InflateCause cause) {\n+  assert(thread == Thread::current() || thread->is_obj_deopt_suspend(), \"must be\");\n+  return inflate_impl(thread, obj, cause);\n+}\n+\n+ObjectMonitor* ObjectSynchronizer::inflate_impl(JavaThread* inflating_thread, oop object, const InflateCause cause) {\n+  \/\/ The JavaThread* inflating_thread parameter is only used by LM_LIGHTWEIGHT and requires\n+  \/\/ that the inflating_thread == Thread::current() or is suspended throughout the call by\n+  \/\/ some other mechanism.\n+  \/\/ Even with LM_LIGHTWEIGHT the thread might be nullptr when called from a non\n+  \/\/ JavaThread. (As may still be the case from FastHashCode). However it is only\n+  \/\/ important for the correctness of the LM_LIGHTWEIGHT algorithm that the thread\n+  \/\/ is set when called from ObjectSynchronizer::enter from the owning thread,\n+  \/\/ ObjectSynchronizer::enter_for from any thread, or ObjectSynchronizer::exit.\n@@ -1317,4 +1411,4 @@\n-    \/\/                   is anonymous and the current thread owns the\n-    \/\/                   object lock, then we make the current thread the\n-    \/\/                   ObjectMonitor owner and remove the lock from the\n-    \/\/                   current thread's lock stack.\n+    \/\/                   is anonymous and the inflating_thread owns the\n+    \/\/                   object lock, then we make the inflating_thread\n+    \/\/                   the ObjectMonitor owner and remove the lock from\n+    \/\/                   the inflating_thread's lock stack.\n@@ -1332,3 +1426,5 @@\n-      if (LockingMode == LM_LIGHTWEIGHT && inf->is_owner_anonymous() && is_lock_owned(current, object)) {\n-        inf->set_owner_from_anonymous(current);\n-        JavaThread::cast(current)->lock_stack().remove(object);\n+      if (LockingMode == LM_LIGHTWEIGHT && inf->is_owner_anonymous() &&\n+          inflating_thread != nullptr && inflating_thread->lock_stack().contains(object)) {\n+        inf->set_owner_from_anonymous(inflating_thread);\n+        size_t removed = inflating_thread->lock_stack().remove(object);\n+        inf->set_recursions(removed - 1);\n@@ -1354,1 +1450,1 @@\n-    \/\/ Could be fast-locked either by current or by some other thread.\n+    \/\/ Could be fast-locked either by the inflating_thread or by some other thread.\n@@ -1358,2 +1454,2 @@\n-    \/\/ this thread owns the monitor, then we set the ObjectMonitor's\n-    \/\/ owner to this thread. Otherwise, we set the ObjectMonitor's owner\n+    \/\/ the inflating_thread owns the monitor, then we set the ObjectMonitor's\n+    \/\/ owner to the inflating_thread. Otherwise, we set the ObjectMonitor's owner\n@@ -1367,1 +1463,1 @@\n-      bool own = is_lock_owned(current, object);\n+      bool own = inflating_thread != nullptr && inflating_thread->lock_stack().contains(object);\n@@ -1369,2 +1465,2 @@\n-        \/\/ Owned by us.\n-        monitor->set_owner_from(nullptr, current);\n+        \/\/ Owned by inflating_thread.\n+        monitor->set_owner_from(nullptr, inflating_thread);\n@@ -1380,1 +1476,2 @@\n-          JavaThread::cast(current)->lock_stack().remove(object);\n+          size_t removed = inflating_thread->lock_stack().remove(object);\n+          monitor->set_recursions(removed - 1);\n@@ -1390,1 +1487,1 @@\n-          ResourceMark rm(current);\n+          ResourceMark rm;\n@@ -1489,1 +1586,1 @@\n-        ResourceMark rm(current);\n+        ResourceMark rm;\n@@ -1533,1 +1630,1 @@\n-      ResourceMark rm(current);\n+      ResourceMark rm;\n","filename":"src\/hotspot\/share\/runtime\/synchronizer.cpp","additions":193,"deletions":96,"binary":false,"changes":289,"status":"modified"},{"patch":"@@ -2,1 +2,1 @@\n- * Copyright (c) 1998, 2023, Oracle and\/or its affiliates. All rights reserved.\n+ * Copyright (c) 1998, 2024, Oracle and\/or its affiliates. All rights reserved.\n@@ -95,0 +95,10 @@\n+  \/\/ Used to enter a monitor for another thread. This requires that the\n+  \/\/ locking_thread is suspended, and that entering on a potential\n+  \/\/ inflated monitor may only contend with deflation. That is the obj being\n+  \/\/ locked on is either already locked by the locking_thread or cannot\n+  \/\/ escape the locking_thread.\n+  static void enter_for(Handle obj, BasicLock* lock, JavaThread* locking_thread);\n+private:\n+  \/\/ Shared implementation for enter and enter_for. Performs all but\n+  \/\/ inflated monitor enter.\n+  static bool enter_fast_impl(Handle obj, BasicLock* lock, JavaThread* locking_thread);\n@@ -96,0 +106,1 @@\n+public:\n@@ -111,0 +122,8 @@\n+  \/\/ Used to inflate a monitor as if it was done from the thread JavaThread.\n+  static ObjectMonitor* inflate_for(JavaThread* thread, oop obj, const InflateCause cause);\n+\n+private:\n+  \/\/ Shared implementation between the different LockingMode.\n+  static ObjectMonitor* inflate_impl(JavaThread* thread, oop obj, const InflateCause cause);\n+\n+public:\n@@ -191,1 +210,1 @@\n-  static void handle_sync_on_value_based_class(Handle obj, JavaThread* current);\n+  static void handle_sync_on_value_based_class(Handle obj, JavaThread* locking_thread);\n","filename":"src\/hotspot\/share\/runtime\/synchronizer.hpp","additions":21,"deletions":2,"binary":false,"changes":23,"status":"modified"},{"patch":"@@ -2584,0 +2584,1 @@\n+  declare_constant(markWord::hash_bits_compact)                           \\\n@@ -2588,0 +2589,2 @@\n+  declare_constant(markWord::hash_shift_compact)                          \\\n+  LP64_ONLY(declare_constant(markWord::klass_shift))                      \\\n@@ -2595,0 +2598,2 @@\n+  declare_constant(markWord::hash_mask_compact)                           \\\n+  declare_constant(markWord::hash_mask_compact_in_place)                  \\\n","filename":"src\/hotspot\/share\/runtime\/vmStructs.cpp","additions":5,"deletions":0,"binary":false,"changes":5,"status":"modified"},{"patch":"@@ -58,1 +58,1 @@\n-compiler\/rtm\/locking\/TestRTMDeoptOnLowAbortRatio.java 8183263,8307907 generic-x64,generic-i586,aix-ppc64\n+compiler\/rtm\/locking\/TestRTMDeoptOnLowAbortRatio.java 8183263 generic-x64,generic-i586\n@@ -60,1 +60,1 @@\n-compiler\/rtm\/locking\/TestRTMLockingThreshold.java 8183263,8307907 generic-x64,generic-i586,aix-ppc64\n+compiler\/rtm\/locking\/TestRTMLockingThreshold.java 8183263 generic-x64,generic-i586\n@@ -63,2 +63,2 @@\n-compiler\/rtm\/locking\/TestUseRTMXendForLockBusy.java 8183263,8307907 generic-x64,generic-i586,aix-ppc64\n-compiler\/rtm\/print\/TestPrintPreciseRTMLockingStatistics.java 8183263,8307907 generic-x64,generic-i586,aix-ppc64\n+compiler\/rtm\/locking\/TestUseRTMXendForLockBusy.java 8183263 generic-x64,generic-i586\n+compiler\/rtm\/print\/TestPrintPreciseRTMLockingStatistics.java 8183263 generic-x64,generic-i586\n","filename":"test\/hotspot\/jtreg\/ProblemList.txt","additions":4,"deletions":4,"binary":false,"changes":8,"status":"modified"},{"patch":"@@ -2,1 +2,1 @@\n-# Copyright (c) 2013, 2023, Oracle and\/or its affiliates. All rights reserved.\n+# Copyright (c) 2013, 2024, Oracle and\/or its affiliates. All rights reserved.\n@@ -145,0 +145,1 @@\n+  gtest\/LockStackGtests.java \\\n","filename":"test\/hotspot\/jtreg\/TEST.groups","additions":2,"deletions":1,"binary":false,"changes":3,"status":"modified"},{"patch":"@@ -35,1 +35,1 @@\n- * @run main\/timeout=240 gc.g1.plab.TestPLABPromotion\n+ * @run main\/othervm\/timeout=240 -Xbootclasspath\/a:. -XX:+UnlockDiagnosticVMOptions -XX:+WhiteBoxAPI gc.g1.plab.TestPLABPromotion\n@@ -48,0 +48,1 @@\n+import jdk.test.lib.Platform;\n@@ -50,0 +51,1 @@\n+import jdk.test.whitebox.WhiteBox;\n@@ -56,0 +58,2 @@\n+    private static final boolean COMPACT_HEADERS = Platform.is64bit() && WhiteBox.getWhiteBox().getBooleanVMFlag(\"UseCompactObjectHeaders\");\n+\n@@ -75,1 +79,1 @@\n-    private static final int OBJECT_SIZE_HIGH = 3250;\n+    private static final int OBJECT_SIZE_HIGH = COMPACT_HEADERS ? 3266 : 3250;\n","filename":"test\/hotspot\/jtreg\/gc\/g1\/plab\/TestPLABPromotion.java","additions":6,"deletions":2,"binary":false,"changes":8,"status":"modified"},{"patch":"@@ -123,0 +123,38 @@\n+ * @bug 8324881\n+ * @comment Regression test for using the wrong thread when logging during re-locking from deoptimization.\n+ *\n+ * @comment DiagnoseSyncOnValueBasedClasses=2 will cause logging when locking on \\@ValueBased objects.\n+ * @run driver EATests\n+ *                 -XX:+UnlockDiagnosticVMOptions\n+ *                 -Xms256m -Xmx256m\n+ *                 -Xbootclasspath\/a:.\n+ *                 -XX:CompileCommand=dontinline,*::dontinline_*\n+ *                 -XX:+WhiteBoxAPI\n+ *                 -Xbatch\n+ *                 -XX:+DoEscapeAnalysis -XX:+EliminateAllocations -XX:+EliminateLocks -XX:+EliminateNestedLocks\n+ *                 -XX:+UnlockExperimentalVMOptions -XX:LockingMode=1\n+ *                 -XX:DiagnoseSyncOnValueBasedClasses=2\n+ *\n+ * @comment Re-lock may inflate monitors when re-locking, which cause monitorinflation trace logging.\n+ * @run driver EATests\n+ *                 -XX:+UnlockDiagnosticVMOptions\n+ *                 -Xms256m -Xmx256m\n+ *                 -Xbootclasspath\/a:.\n+ *                 -XX:CompileCommand=dontinline,*::dontinline_*\n+ *                 -XX:+WhiteBoxAPI\n+ *                 -Xbatch\n+ *                 -XX:+DoEscapeAnalysis -XX:+EliminateAllocations -XX:+EliminateLocks -XX:+EliminateNestedLocks\n+ *                 -XX:+UnlockExperimentalVMOptions -XX:LockingMode=2\n+ *                 -Xlog:monitorinflation=trace:file=monitorinflation.log\n+ *\n+ * @comment Re-lock may race with deflation.\n+ * @run driver EATests\n+ *                 -XX:+UnlockDiagnosticVMOptions\n+ *                 -Xms256m -Xmx256m\n+ *                 -Xbootclasspath\/a:.\n+ *                 -XX:CompileCommand=dontinline,*::dontinline_*\n+ *                 -XX:+WhiteBoxAPI\n+ *                 -Xbatch\n+ *                 -XX:+DoEscapeAnalysis -XX:+EliminateAllocations -XX:+EliminateLocks -XX:+EliminateNestedLocks\n+ *                 -XX:+UnlockExperimentalVMOptions -XX:LockingMode=0\n+ *                 -XX:GuaranteedAsyncDeflationInterval=1000\n@@ -124,0 +162,1 @@\n+\n@@ -256,0 +295,1 @@\n+        new EARelockingNestedInflated_03Target()                                            .run();\n@@ -262,0 +302,1 @@\n+        new EARelockingValueBasedTarget()                                                   .run();\n@@ -375,0 +416,1 @@\n+        new EARelockingNestedInflated_03()                                            .run(this);\n@@ -381,0 +423,1 @@\n+        new EARelockingValueBased()                                                   .run(this);\n@@ -1925,0 +1968,88 @@\n+\/**\n+ * Like {@link EARelockingNestedInflated_02} with the difference that the\n+ * inflation of the lock happens because of contention.\n+ *\/\n+class EARelockingNestedInflated_03 extends EATestCaseBaseDebugger {\n+\n+    public void runTestCase() throws Exception {\n+        BreakpointEvent bpe = resumeTo(TARGET_TESTCASE_BASE_NAME, \"dontinline_brkpt\", \"()V\");\n+        printStack(bpe.thread());\n+        @SuppressWarnings(\"unused\")\n+        ObjectReference o = getLocalRef(bpe.thread().frame(2), XYVAL_NAME, \"l1\");\n+    }\n+}\n+\n+class EARelockingNestedInflated_03Target extends EATestCaseBaseTarget {\n+\n+    public XYVal lockInflatedByContention;\n+    public boolean doLockNow;\n+    public EATestCaseBaseTarget testCase;\n+\n+    @Override\n+    public void setUp() {\n+        super.setUp();\n+        testMethodDepth = 2;\n+        lockInflatedByContention = new XYVal(1, 1);\n+        testCase = this;\n+    }\n+\n+    @Override\n+    public void warmupDone() {\n+        super.warmupDone();\n+        \/\/ Use new lock. lockInflatedByContention might have been inflated because of recursion.\n+        lockInflatedByContention = new XYVal(1, 1);\n+        \/\/ Start thread that tries to enter lockInflatedByContention while the main thread owns it -> inflation\n+        TestScaffold.newThread(() -> {\n+            while (true) {\n+                synchronized (testCase) {\n+                    try {\n+                        if (doLockNow) {\n+                            doLockNow = false; \/\/ reset for main thread\n+                            testCase.notify();\n+                            break;\n+                        }\n+                        testCase.wait();\n+                    } catch (InterruptedException e) { \/* ignored *\/ }\n+                }\n+            }\n+            synchronized (lockInflatedByContention) { \/\/ will block and trigger inflation\n+                msg(Thread.currentThread().getName() + \": acquired lockInflatedByContention\");\n+            }\n+            }, testCaseName + \": Lock Contender (test thread)\").start();\n+    }\n+\n+    public void dontinline_testMethod() {\n+        @SuppressWarnings(\"unused\")\n+        XYVal xy = new XYVal(1, 1);            \/\/ scalar replaced\n+        XYVal l1 = lockInflatedByContention;   \/\/ read by debugger\n+        synchronized (l1) {\n+            testMethod_inlined(l1);\n+        }\n+    }\n+\n+    public void testMethod_inlined(XYVal l2) {\n+        synchronized (l2) {                 \/\/ eliminated nested locking\n+            dontinline_notifyOtherThread();\n+            dontinline_brkpt();\n+        }\n+    }\n+\n+    public void dontinline_notifyOtherThread() {\n+        if (!warmupDone) {\n+            return;\n+        }\n+        synchronized (testCase) {\n+            doLockNow = true;\n+            testCase.notify();\n+            \/\/ wait for other thread to reset doLockNow again\n+            while (doLockNow) {\n+                try {\n+                    testCase.wait();\n+                } catch (InterruptedException e) { \/* ignored *\/ }\n+            }\n+        }\n+    }\n+}\n+\n+\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\n+\n@@ -2140,0 +2271,26 @@\n+\n+\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\n+\n+\/**\n+ * Test relocking eliminated @ValueBased object.\n+ *\/\n+class EARelockingValueBased extends EATestCaseBaseDebugger {\n+\n+    public void runTestCase() throws Exception {\n+        BreakpointEvent bpe = resumeTo(TARGET_TESTCASE_BASE_NAME, \"dontinline_brkpt\", \"()V\");\n+        printStack(bpe.thread());\n+        @SuppressWarnings(\"unused\")\n+        ObjectReference o = getLocalRef(bpe.thread().frame(1), Integer.class.getName(), \"l1\");\n+    }\n+}\n+\n+class EARelockingValueBasedTarget extends EATestCaseBaseTarget {\n+\n+    public void dontinline_testMethod() {\n+        Integer l1 = new Integer(255);\n+        synchronized (l1) {\n+            dontinline_brkpt();\n+        }\n+    }\n+}\n+\n","filename":"test\/jdk\/com\/sun\/jdi\/EATests.java","additions":157,"deletions":0,"binary":false,"changes":157,"status":"modified"}]}