{"files":[{"patch":"@@ -2,1 +2,1 @@\n-project=jdk-updates\n+project=lilliput\n@@ -26,1 +26,1 @@\n-reviewers=1\n+committers=1\n","filename":".jcheck\/conf","additions":2,"deletions":2,"binary":false,"changes":4,"status":"modified"},{"patch":"@@ -263,0 +263,1 @@\n+JDKOPT_ENABLE_DISABLE_CDS_ARCHIVE_COH\n","filename":"make\/autoconf\/configure.ac","additions":1,"deletions":0,"binary":false,"changes":1,"status":"modified"},{"patch":"@@ -687,0 +687,27 @@\n+################################################################################\n+#\n+# Enable or disable the default CDS archive generation for Compact Object Headers\n+#\n+AC_DEFUN([JDKOPT_ENABLE_DISABLE_CDS_ARCHIVE_COH],\n+[\n+  UTIL_ARG_ENABLE(NAME: cds-archive-coh, DEFAULT: auto, RESULT: BUILD_CDS_ARCHIVE_COH,\n+      DESC: [enable generation of default CDS archives for compact object headers (requires --enable-cds-archive)],\n+      DEFAULT_DESC: [auto],\n+      CHECKING_MSG: [if default CDS archives for compact object headers should be generated],\n+      CHECK_AVAILABLE: [\n+        AC_MSG_CHECKING([if CDS archive with compact object headers is available])\n+        if test \"x$BUILD_CDS_ARCHIVE\" = \"xfalse\"; then\n+          AC_MSG_RESULT([no (CDS default archive generation is disabled)])\n+          AVAILABLE=false\n+        elif test \"x$OPENJDK_TARGET_CPU\" != \"xx86_64\" &&\n+             test \"x$OPENJDK_TARGET_CPU\" != \"xaarch64\"; then\n+          AC_MSG_RESULT([no (compact object headers not supported for this platform)])\n+          AVAILABLE=false\n+        else\n+          AC_MSG_RESULT([yes])\n+          AVAILABLE=true\n+        fi\n+      ])\n+  AC_SUBST(BUILD_CDS_ARCHIVE_COH)\n+])\n+\n","filename":"make\/autoconf\/jdk-options.m4","additions":27,"deletions":0,"binary":false,"changes":27,"status":"modified"},{"patch":"@@ -7149,1 +7149,1 @@\n-  predicate(!needs_acquiring_load(n));\n+  predicate(!needs_acquiring_load(n) && !UseCompactObjectHeaders);\n@@ -7159,0 +7159,14 @@\n+instruct loadNKlassCompactHeaders(iRegNNoSp dst, memory4 mem, rFlagsReg cr)\n+%{\n+  match(Set dst (LoadNKlass mem));\n+  effect(KILL cr);\n+  predicate(!needs_acquiring_load(n) && UseCompactObjectHeaders);\n+\n+  ins_cost(4 * INSN_COST);\n+  format %{ \"ldrw  $dst, $mem\\t# compressed class ptr\" %}\n+  ins_encode %{\n+    __ load_nklass_compact($dst$$Register, $mem$$base$$Register, $mem$$index$$Register, $mem$$scale, $mem$$disp);\n+  %}\n+  ins_pipe(pipe_slow);\n+%}\n+\n@@ -16440,0 +16454,1 @@\n+  predicate(LockingMode != LM_LIGHTWEIGHT);\n@@ -16443,3 +16458,1 @@\n-  \/\/ TODO\n-  \/\/ identify correct cost\n-  format %{ \"fastlock $object,$box\\t! kills $tmp,$tmp2\" %}\n+  format %{ \"fastlock $object,$box\\t! kills $tmp,$tmp2,$tmp3\" %}\n@@ -16457,0 +16470,1 @@\n+  predicate(LockingMode != LM_LIGHTWEIGHT);\n@@ -16470,0 +16484,31 @@\n+instruct cmpFastLockLightweight(rFlagsReg cr, iRegP object, iRegP box, iRegPNoSp tmp, iRegPNoSp tmp2)\n+%{\n+  predicate(LockingMode == LM_LIGHTWEIGHT);\n+  match(Set cr (FastLock object box));\n+  effect(TEMP tmp, TEMP tmp2);\n+\n+  ins_cost(5 * INSN_COST);\n+  format %{ \"fastlock $object,$box\\t! kills $tmp,$tmp2\" %}\n+\n+  ins_encode %{\n+    __ fast_lock_lightweight($object$$Register, $box$$Register, $tmp$$Register, $tmp2$$Register);\n+  %}\n+\n+  ins_pipe(pipe_serial);\n+%}\n+\n+instruct cmpFastUnlockLightweight(rFlagsReg cr, iRegP object, iRegP box, iRegPNoSp tmp, iRegPNoSp tmp2)\n+%{\n+  predicate(LockingMode == LM_LIGHTWEIGHT);\n+  match(Set cr (FastUnlock object box));\n+  effect(TEMP tmp, TEMP tmp2);\n+\n+  ins_cost(5 * INSN_COST);\n+  format %{ \"fastunlock $object,$box\\t! kills $tmp, $tmp2\" %}\n+\n+  ins_encode %{\n+    __ fast_unlock_lightweight($object$$Register, $box$$Register, $tmp$$Register, $tmp2$$Register);\n+  %}\n+\n+  ins_pipe(pipe_serial);\n+%}\n","filename":"src\/hotspot\/cpu\/aarch64\/aarch64.ad","additions":49,"deletions":4,"binary":false,"changes":53,"status":"modified"},{"patch":"@@ -2,1 +2,1 @@\n- * Copyright (c) 1997, 2023, Oracle and\/or its affiliates. All rights reserved.\n+ * Copyright (c) 1997, 2024, Oracle and\/or its affiliates. All rights reserved.\n@@ -26,2 +26,0 @@\n-#include <sys\/types.h>\n-\n@@ -57,0 +55,1 @@\n+#include \"utilities\/globalDefinitions.hpp\"\n@@ -68,0 +67,2 @@\n+#include <sys\/types.h>\n+\n@@ -4433,0 +4434,19 @@\n+\/\/ Loads the obj's Klass* into dst.\n+\/\/ Preserves all registers (incl src, rscratch1 and rscratch2).\n+void MacroAssembler::load_nklass_compact(Register dst, Register src) {\n+  assert(UseCompactObjectHeaders, \"expects UseCompactObjectHeaders\");\n+\n+  Label fast;\n+\n+  \/\/ Check if we can take the (common) fast path, if obj is unlocked.\n+  ldr(dst, Address(src, oopDesc::mark_offset_in_bytes()));\n+  tbz(dst, exact_log2(markWord::monitor_value), fast);\n+\n+  \/\/ Fetch displaced header\n+  ldr(dst, Address(dst, OM_OFFSET_NO_MONITOR_VALUE_TAG(header)));\n+\n+  \/\/ Fast-path: shift to get narrowKlass.\n+  bind(fast);\n+  lsr(dst, dst, markWord::klass_shift);\n+}\n+\n@@ -4434,1 +4454,4 @@\n-  if (UseCompressedClassPointers) {\n+  if (UseCompactObjectHeaders) {\n+    load_nklass_compact(dst, src);\n+    decode_klass_not_null(dst);\n+  } else if (UseCompressedClassPointers) {\n@@ -4473,0 +4496,1 @@\n+  assert_different_registers(oop, trial_klass, tmp);\n@@ -4474,1 +4498,5 @@\n-    ldrw(tmp, Address(oop, oopDesc::klass_offset_in_bytes()));\n+    if (UseCompactObjectHeaders) {\n+      load_nklass_compact(tmp, oop);\n+    } else {\n+      ldrw(tmp, Address(oop, oopDesc::klass_offset_in_bytes()));\n+    }\n@@ -4491,0 +4519,16 @@\n+void MacroAssembler::cmp_klass(Register src, Register dst, Register tmp1, Register tmp2) {\n+  if (UseCompactObjectHeaders) {\n+    load_nklass_compact(tmp1, src);\n+    load_nklass_compact(tmp2, dst);\n+    cmpw(tmp1, tmp2);\n+  } else if (UseCompressedClassPointers) {\n+    ldrw(tmp1, Address(src, oopDesc::klass_offset_in_bytes()));\n+    ldrw(tmp2, Address(dst, oopDesc::klass_offset_in_bytes()));\n+    cmpw(tmp1, tmp2);\n+  } else {\n+    ldr(tmp1, Address(src, oopDesc::klass_offset_in_bytes()));\n+    ldr(tmp2, Address(dst, oopDesc::klass_offset_in_bytes()));\n+    cmp(tmp1, tmp2);\n+  }\n+}\n+\n@@ -4494,0 +4538,1 @@\n+  assert(!UseCompactObjectHeaders, \"not with compact headers\");\n@@ -4503,0 +4548,1 @@\n+  assert(!UseCompactObjectHeaders, \"not with compact headers\");\n@@ -6325,2 +6371,0 @@\n-\/\/ Branches to slow upon failure to lock the object, with ZF cleared.\n-\/\/ Falls through upon success with ZF set.\n@@ -6329,3 +6373,3 @@\n-\/\/  - hdr: the header, already loaded from obj, will be destroyed\n-\/\/  - t1, t2: temporary registers, will be destroyed\n-void MacroAssembler::lightweight_lock(Register obj, Register hdr, Register t1, Register t2, Label& slow) {\n+\/\/  - t1, t2, t3: temporary registers, will be destroyed\n+\/\/  - slow: branched to if locking fails, absolute offset may larger than 32KB (imm14 encoding).\n+void MacroAssembler::lightweight_lock(Register obj, Register t1, Register t2, Register t3, Label& slow) {\n@@ -6333,15 +6377,24 @@\n-  assert_different_registers(obj, hdr, t1, t2, rscratch1);\n-\n-  \/\/ Check if we would have space on lock-stack for the object.\n-  ldrw(t1, Address(rthread, JavaThread::lock_stack_top_offset()));\n-  cmpw(t1, (unsigned)LockStack::end_offset() - 1);\n-  br(Assembler::GT, slow);\n-\n-  \/\/ Load (object->mark() | 1) into hdr\n-  orr(hdr, hdr, markWord::unlocked_value);\n-  \/\/ Clear lock-bits, into t2\n-  eor(t2, hdr, markWord::unlocked_value);\n-  \/\/ Try to swing header from unlocked to locked\n-  \/\/ Clobbers rscratch1 when UseLSE is false\n-  cmpxchg(\/*addr*\/ obj, \/*expected*\/ hdr, \/*new*\/ t2, Assembler::xword,\n-          \/*acquire*\/ true, \/*release*\/ true, \/*weak*\/ false, t1);\n+  assert_different_registers(obj, t1, t2, t3, rscratch1);\n+\n+  Label push;\n+  const Register top = t1;\n+  const Register mark = t2;\n+  const Register t = t3;\n+\n+  \/\/ Preload the markWord. It is important that this is the first\n+  \/\/ instruction emitted as it is part of C1's null check semantics.\n+  ldr(mark, Address(obj, oopDesc::mark_offset_in_bytes()));\n+\n+  \/\/ Check if the lock-stack is full.\n+  ldrw(top, Address(rthread, JavaThread::lock_stack_top_offset()));\n+  cmpw(top, (unsigned)LockStack::end_offset());\n+  br(Assembler::GE, slow);\n+\n+  \/\/ Check for recursion.\n+  subw(t, top, oopSize);\n+  ldr(t, Address(rthread, t));\n+  cmp(obj, t);\n+  br(Assembler::EQ, push);\n+\n+  \/\/ Check header for monitor (0b10).\n+  tst(mark, markWord::monitor_value);\n@@ -6350,5 +6403,13 @@\n-  \/\/ After successful lock, push object on lock-stack\n-  ldrw(t1, Address(rthread, JavaThread::lock_stack_top_offset()));\n-  str(obj, Address(rthread, t1));\n-  addw(t1, t1, oopSize);\n-  strw(t1, Address(rthread, JavaThread::lock_stack_top_offset()));\n+  \/\/ Try to lock. Transition lock bits 0b01 => 0b00\n+  assert(oopDesc::mark_offset_in_bytes() == 0, \"required to avoid lea\");\n+  orr(mark, mark, markWord::unlocked_value);\n+  eor(t, mark, markWord::unlocked_value);\n+  cmpxchg(\/*addr*\/ obj, \/*expected*\/ mark, \/*new*\/ t, Assembler::xword,\n+          \/*acquire*\/ true, \/*release*\/ false, \/*weak*\/ false, noreg);\n+  br(Assembler::NE, slow);\n+\n+  bind(push);\n+  \/\/ After successful lock, push object on lock-stack.\n+  str(obj, Address(rthread, top));\n+  addw(top, top, oopSize);\n+  strw(top, Address(rthread, JavaThread::lock_stack_top_offset()));\n@@ -6358,2 +6419,0 @@\n-\/\/ Branches to slow upon failure, with ZF cleared.\n-\/\/ Falls through upon success, with ZF set.\n@@ -6362,3 +6421,3 @@\n-\/\/ - hdr: the (pre-loaded) header of the object\n-\/\/ - t1, t2: temporary registers\n-void MacroAssembler::lightweight_unlock(Register obj, Register hdr, Register t1, Register t2, Label& slow) {\n+\/\/ - t1, t2, t3: temporary registers\n+\/\/ - slow: branched to if unlocking fails, absolute offset may larger than 32KB (imm14 encoding).\n+void MacroAssembler::lightweight_unlock(Register obj, Register t1, Register t2, Register t3, Label& slow) {\n@@ -6366,1 +6425,2 @@\n-  assert_different_registers(obj, hdr, t1, t2, rscratch1);\n+  \/\/ cmpxchg clobbers rscratch1.\n+  assert_different_registers(obj, t1, t2, t3, rscratch1);\n@@ -6370,4 +6430,0 @@\n-    \/\/ The following checks rely on the fact that LockStack is only ever modified by\n-    \/\/ its owning thread, even if the lock got inflated concurrently; removal of LockStack\n-    \/\/ entries after inflation will happen delayed in that case.\n-\n@@ -6378,1 +6434,1 @@\n-    br(Assembler::GT, stack_ok);\n+    br(Assembler::GE, stack_ok);\n@@ -6382,18 +6438,0 @@\n-  {\n-    \/\/ Check if the top of the lock-stack matches the unlocked object.\n-    Label tos_ok;\n-    subw(t1, t1, oopSize);\n-    ldr(t1, Address(rthread, t1));\n-    cmpoop(t1, obj);\n-    br(Assembler::EQ, tos_ok);\n-    STOP(\"Top of lock-stack does not match the unlocked object\");\n-    bind(tos_ok);\n-  }\n-  {\n-    \/\/ Check that hdr is fast-locked.\n-    Label hdr_ok;\n-    tst(hdr, markWord::lock_mask_in_place);\n-    br(Assembler::EQ, hdr_ok);\n-    STOP(\"Header is not fast-locked\");\n-    bind(hdr_ok);\n-  }\n@@ -6402,2 +6440,4 @@\n-  \/\/ Load the new header (unlocked) into t1\n-  orr(t1, hdr, markWord::unlocked_value);\n+  Label unlocked, push_and_slow;\n+  const Register top = t1;\n+  const Register mark = t2;\n+  const Register t = t3;\n@@ -6405,4 +6445,5 @@\n-  \/\/ Try to swing header from locked to unlocked\n-  \/\/ Clobbers rscratch1 when UseLSE is false\n-  cmpxchg(obj, hdr, t1, Assembler::xword,\n-          \/*acquire*\/ true, \/*release*\/ true, \/*weak*\/ false, t2);\n+  \/\/ Check if obj is top of lock-stack.\n+  ldrw(top, Address(rthread, JavaThread::lock_stack_top_offset()));\n+  subw(top, top, oopSize);\n+  ldr(t, Address(rthread, top));\n+  cmp(obj, t);\n@@ -6411,3 +6452,14 @@\n-  \/\/ After successful unlock, pop object from lock-stack\n-  ldrw(t1, Address(rthread, JavaThread::lock_stack_top_offset()));\n-  subw(t1, t1, oopSize);\n+  \/\/ Pop lock-stack.\n+  DEBUG_ONLY(str(zr, Address(rthread, top));)\n+  strw(top, Address(rthread, JavaThread::lock_stack_top_offset()));\n+\n+  \/\/ Check if recursive.\n+  subw(t, top, oopSize);\n+  ldr(t, Address(rthread, t));\n+  cmp(obj, t);\n+  br(Assembler::EQ, unlocked);\n+\n+  \/\/ Not recursive. Check header for monitor (0b10).\n+  ldr(mark, Address(obj, oopDesc::mark_offset_in_bytes()));\n+  tbnz(mark, log2i_exact(markWord::monitor_value), push_and_slow);\n+\n@@ -6415,1 +6467,5 @@\n-  str(zr, Address(rthread, t1));\n+  \/\/ Check header not unlocked (0b01).\n+  Label not_unlocked;\n+  tbz(mark, log2i_exact(markWord::unlocked_value), not_unlocked);\n+  stop(\"lightweight_unlock already unlocked\");\n+  bind(not_unlocked);\n@@ -6417,1 +6473,16 @@\n-  strw(t1, Address(rthread, JavaThread::lock_stack_top_offset()));\n+\n+  \/\/ Try to unlock. Transition lock bits 0b00 => 0b01\n+  assert(oopDesc::mark_offset_in_bytes() == 0, \"required to avoid lea\");\n+  orr(t, mark, markWord::unlocked_value);\n+  cmpxchg(obj, mark, t, Assembler::xword,\n+          \/*acquire*\/ false, \/*release*\/ true, \/*weak*\/ false, noreg);\n+  br(Assembler::EQ, unlocked);\n+\n+  bind(push_and_slow);\n+  \/\/ Restore lock-stack and handle the unlock in runtime.\n+  DEBUG_ONLY(str(obj, Address(rthread, top));)\n+  addw(top, top, oopSize);\n+  strw(top, Address(rthread, JavaThread::lock_stack_top_offset()));\n+  b(slow);\n+\n+  bind(unlocked);\n","filename":"src\/hotspot\/cpu\/aarch64\/macroAssembler_aarch64.cpp","additions":141,"deletions":70,"binary":false,"changes":211,"status":"modified"},{"patch":"@@ -2,1 +2,1 @@\n- * Copyright (c) 1997, 2023, Oracle and\/or its affiliates. All rights reserved.\n+ * Copyright (c) 1997, 2024, Oracle and\/or its affiliates. All rights reserved.\n@@ -852,0 +852,1 @@\n+  void load_nklass_compact(Register dst, Register src);\n@@ -855,0 +856,1 @@\n+  void cmp_klass(Register src, Register dst, Register tmp1, Register tmp2);\n@@ -1594,2 +1596,2 @@\n-  void lightweight_lock(Register obj, Register hdr, Register t1, Register t2, Label& slow);\n-  void lightweight_unlock(Register obj, Register hdr, Register t1, Register t2, Label& slow);\n+  void lightweight_lock(Register obj, Register t1, Register t2, Register t3, Label& slow);\n+  void lightweight_unlock(Register obj, Register t1, Register t2, Register t3, Label& slow);\n","filename":"src\/hotspot\/cpu\/aarch64\/macroAssembler_aarch64.hpp","additions":5,"deletions":3,"binary":false,"changes":8,"status":"modified"},{"patch":"@@ -2,1 +2,1 @@\n- * Copyright (c) 2003, 2023, Oracle and\/or its affiliates. All rights reserved.\n+ * Copyright (c) 2003, 2024, Oracle and\/or its affiliates. All rights reserved.\n@@ -1820,1 +1820,0 @@\n-      __ ldr(swap_reg, Address(obj_reg, oopDesc::mark_offset_in_bytes()));\n@@ -1960,2 +1959,0 @@\n-      __ ldr(old_hdr, Address(obj_reg, oopDesc::mark_offset_in_bytes()));\n-      __ tbnz(old_hdr, exact_log2(markWord::monitor_value), slow_path_unlock);\n","filename":"src\/hotspot\/cpu\/aarch64\/sharedRuntime_aarch64.cpp","additions":1,"deletions":4,"binary":false,"changes":5,"status":"modified"},{"patch":"@@ -2,1 +2,1 @@\n- * Copyright (c) 1997, 2023, Oracle and\/or its affiliates. All rights reserved.\n+ * Copyright (c) 1997, 2024, Oracle and\/or its affiliates. All rights reserved.\n@@ -171,0 +171,1 @@\n+  constexpr static bool supports_recursive_lightweight_locking() { return true; }\n","filename":"src\/hotspot\/cpu\/aarch64\/vm_version_aarch64.hpp","additions":2,"deletions":1,"binary":false,"changes":3,"status":"modified"},{"patch":"@@ -301,0 +301,4 @@\n+void LoadKlassStub::emit_code(LIR_Assembler* ce) {\n+  \/\/ Currently not needed.\n+  Unimplemented();\n+}\n","filename":"src\/hotspot\/cpu\/ppc\/c1_CodeStubs_ppc.cpp","additions":4,"deletions":0,"binary":false,"changes":4,"status":"modified"},{"patch":"@@ -54,7 +54,0 @@\n-#if defined(COMPILER2) && (defined(AIX) || defined(LINUX))\n-\/\/ Include Transactional Memory lock eliding optimization\n-#define INCLUDE_RTM_OPT 1\n-#else\n-#define INCLUDE_RTM_OPT 0\n-#endif\n-\n","filename":"src\/hotspot\/cpu\/ppc\/globalDefinitions_ppc.hpp","additions":0,"deletions":7,"binary":false,"changes":7,"status":"modified"},{"patch":"@@ -12150,0 +12150,1 @@\n+  predicate(LockingMode != LM_LIGHTWEIGHT);\n@@ -12152,1 +12153,0 @@\n-  predicate(!Compile::current()->use_rtm());\n@@ -12165,20 +12165,1 @@\n-\/\/ Separate version for TM. Use bound register for box to enable USE_KILL.\n-instruct cmpFastLock_tm(flagsReg crx, iRegPdst oop, rarg2RegP box, iRegPdst tmp1, iRegPdst tmp2, iRegPdst tmp3) %{\n-  match(Set crx (FastLock oop box));\n-  effect(TEMP tmp1, TEMP tmp2, TEMP tmp3, USE_KILL box);\n-  predicate(Compile::current()->use_rtm());\n-\n-  format %{ \"FASTLOCK  $oop, $box, $tmp1, $tmp2, $tmp3 (TM)\" %}\n-  ins_encode %{\n-    __ compiler_fast_lock_object($crx$$CondRegister, $oop$$Register, $box$$Register,\n-                                 $tmp1$$Register, $tmp2$$Register, $tmp3$$Register,\n-                                 _rtm_counters, _stack_rtm_counters,\n-                                 ((Method*)(ra_->C->method()->constant_encoding()))->method_data(),\n-                                 \/*RTM*\/ true, ra_->C->profile_rtm());\n-    \/\/ If locking was successful, crx should indicate 'EQ'.\n-    \/\/ The compiler generates a branch to the runtime call to\n-    \/\/ _complete_monitor_locking_Java for the case where crx is 'NE'.\n-  %}\n-  ins_pipe(pipe_class_compare);\n-%}\n-\n+  predicate(LockingMode != LM_LIGHTWEIGHT);\n@@ -12188,1 +12169,0 @@\n-  predicate(!Compile::current()->use_rtm());\n@@ -12193,2 +12173,1 @@\n-                                   $tmp1$$Register, $tmp2$$Register, $tmp3$$Register,\n-                                   false);\n+                                   $tmp1$$Register, $tmp2$$Register, $tmp3$$Register);\n@@ -12202,1 +12181,18 @@\n-instruct cmpFastUnlock_tm(flagsReg crx, iRegPdst oop, iRegPdst box, iRegPdst tmp1, iRegPdst tmp2, iRegPdst tmp3) %{\n+instruct cmpFastLockLightweight(flagsRegCR0 crx, iRegPdst oop, iRegPdst box, iRegPdst tmp1, iRegPdst tmp2) %{\n+  predicate(LockingMode == LM_LIGHTWEIGHT);\n+  match(Set crx (FastLock oop box));\n+  effect(TEMP tmp1, TEMP tmp2);\n+\n+  format %{ \"FASTLOCK  $oop, $box, $tmp1, $tmp2\" %}\n+  ins_encode %{\n+    __ fast_lock_lightweight($crx$$CondRegister, $oop$$Register, $box$$Register,\n+                             $tmp1$$Register, $tmp2$$Register, \/*tmp3*\/ R0);\n+    \/\/ If locking was successful, crx should indicate 'EQ'.\n+    \/\/ The compiler generates a branch to the runtime call to\n+    \/\/ _complete_monitor_locking_Java for the case where crx is 'NE'.\n+  %}\n+  ins_pipe(pipe_class_compare);\n+%}\n+\n+instruct cmpFastUnlockLightweight(flagsRegCR0 crx, iRegPdst oop, iRegPdst box, iRegPdst tmp1, iRegPdst tmp2, iRegPdst tmp3) %{\n+  predicate(LockingMode == LM_LIGHTWEIGHT);\n@@ -12205,2 +12201,1 @@\n-  predicate(Compile::current()->use_rtm());\n-  format %{ \"FASTUNLOCK  $oop, $box, $tmp1, $tmp2 (TM)\" %}\n+  format %{ \"FASTUNLOCK  $oop, $box, $tmp1, $tmp2\" %}\n@@ -12209,3 +12204,2 @@\n-    __ compiler_fast_unlock_object($crx$$CondRegister, $oop$$Register, $box$$Register,\n-                                   $tmp1$$Register, $tmp2$$Register, $tmp3$$Register,\n-                                   \/*RTM*\/ true);\n+    __ fast_unlock_lightweight($crx$$CondRegister, $oop$$Register, $box$$Register,\n+                               $tmp1$$Register, $tmp2$$Register, $tmp3$$Register);\n","filename":"src\/hotspot\/cpu\/ppc\/ppc.ad","additions":24,"deletions":30,"binary":false,"changes":54,"status":"modified"},{"patch":"@@ -2280,7 +2280,0 @@\n-  if (UseRTMLocking) {\n-    \/\/ Abort RTM transaction before calling JNI\n-    \/\/ because critical section can be large and\n-    \/\/ abort anyway. Also nmethod can be deoptimized.\n-    __ tabort_();\n-  }\n-\n@@ -2473,2 +2466,7 @@\n-    \/\/ fast_lock kills r_temp_1, r_temp_2, r_temp_3.\n-    __ compiler_fast_lock_object(CCR0, r_oop, r_box, r_temp_1, r_temp_2, r_temp_3);\n+    if (LockingMode == LM_LIGHTWEIGHT) {\n+      \/\/ fast_lock kills r_temp_1, r_temp_2, r_temp_3.\n+      __ compiler_fast_lock_lightweight_object(CCR0, r_oop, r_temp_1, r_temp_2, r_temp_3);\n+    } else {\n+      \/\/ fast_lock kills r_temp_1, r_temp_2, r_temp_3.\n+      __ compiler_fast_lock_object(CCR0, r_oop, r_box, r_temp_1, r_temp_2, r_temp_3);\n+    }\n@@ -2684,1 +2682,5 @@\n-    __ compiler_fast_unlock_object(CCR0, r_oop, r_box, r_temp_1, r_temp_2, r_temp_3);\n+    if (LockingMode == LM_LIGHTWEIGHT) {\n+      __ compiler_fast_unlock_lightweight_object(CCR0, r_oop, r_temp_1, r_temp_2, r_temp_3);\n+    } else {\n+      __ compiler_fast_unlock_object(CCR0, r_oop, r_box, r_temp_1, r_temp_2, r_temp_3);\n+    }\n@@ -3168,5 +3170,0 @@\n-  if (UseRTMLocking) {\n-    \/\/ Abort RTM transaction before possible nmethod deoptimization.\n-    __ tabort_();\n-  }\n-\n@@ -3323,7 +3320,0 @@\n-  if (UseRTMLocking) {\n-    \/\/ Abort RTM transaction before calling runtime\n-    \/\/ because critical section can be large and so\n-    \/\/ will abort anyway. Also nmethod can be deoptimized.\n-    __ tabort_();\n-  }\n-\n","filename":"src\/hotspot\/cpu\/ppc\/sharedRuntime_ppc.cpp","additions":12,"deletions":22,"binary":false,"changes":34,"status":"modified"},{"patch":"@@ -50,0 +50,1 @@\n+#include \"utilities\/globalDefinitions.hpp\"\n@@ -4706,2 +4707,0 @@\n-\/\/ Branches to slow upon failure to lock the object.\n-\/\/ Falls through upon success.\n@@ -4710,3 +4709,3 @@\n-\/\/  - hdr: the header, already loaded from obj, will be destroyed\n-\/\/  - tmp1, tmp2: temporary registers, will be destroyed\n-void MacroAssembler::lightweight_lock(Register obj, Register hdr, Register tmp1, Register tmp2, Label& slow) {\n+\/\/  - tmp1, tmp2, tmp3: temporary registers, will be destroyed\n+\/\/  - slow: branched to if locking fails\n+void MacroAssembler::lightweight_lock(Register obj, Register tmp1, Register tmp2, Register tmp3, Label& slow) {\n@@ -4714,23 +4713,39 @@\n-  assert_different_registers(obj, hdr, tmp1, tmp2, t0);\n-\n-  \/\/ Check if we would have space on lock-stack for the object.\n-  lwu(tmp1, Address(xthread, JavaThread::lock_stack_top_offset()));\n-  mv(tmp2, (unsigned)LockStack::end_offset());\n-  bge(tmp1, tmp2, slow, \/* is_far *\/ true);\n-\n-  \/\/ Load (object->mark() | 1) into hdr\n-  ori(hdr, hdr, markWord::unlocked_value);\n-  \/\/ Clear lock-bits, into tmp2\n-  xori(tmp2, hdr, markWord::unlocked_value);\n-\n-  \/\/ Try to swing header from unlocked to locked\n-  Label success;\n-  cmpxchgptr(hdr, tmp2, obj, tmp1, success, &slow);\n-  bind(success);\n-\n-  \/\/ After successful lock, push object on lock-stack\n-  lwu(tmp1, Address(xthread, JavaThread::lock_stack_top_offset()));\n-  add(tmp2, xthread, tmp1);\n-  sd(obj, Address(tmp2, 0));\n-  addw(tmp1, tmp1, oopSize);\n-  sw(tmp1, Address(xthread, JavaThread::lock_stack_top_offset()));\n+  assert_different_registers(obj, tmp1, tmp2, tmp3, t0);\n+\n+  Label push;\n+  const Register top = tmp1;\n+  const Register mark = tmp2;\n+  const Register t = tmp3;\n+\n+  \/\/ Preload the markWord. It is important that this is the first\n+  \/\/ instruction emitted as it is part of C1's null check semantics.\n+  ld(mark, Address(obj, oopDesc::mark_offset_in_bytes()));\n+\n+  \/\/ Check if the lock-stack is full.\n+  lwu(top, Address(xthread, JavaThread::lock_stack_top_offset()));\n+  mv(t, (unsigned)LockStack::end_offset());\n+  bge(top, t, slow, \/* is_far *\/ true);\n+\n+  \/\/ Check for recursion.\n+  add(t, xthread, top);\n+  ld(t, Address(t, -oopSize));\n+  beq(obj, t, push);\n+\n+  \/\/ Check header for monitor (0b10).\n+  test_bit(t, mark, exact_log2(markWord::monitor_value));\n+  bnez(t, slow, \/* is_far *\/ true);\n+\n+  \/\/ Try to lock. Transition lock-bits 0b01 => 0b00\n+  assert(oopDesc::mark_offset_in_bytes() == 0, \"required to avoid a la\");\n+  ori(mark, mark, markWord::unlocked_value);\n+  xori(t, mark, markWord::unlocked_value);\n+  cmpxchg(\/*addr*\/ obj, \/*expected*\/ mark, \/*new*\/ t, Assembler::int64,\n+          \/*acquire*\/ Assembler::aq, \/*release*\/ Assembler::relaxed, \/*result*\/ t);\n+  bne(mark, t, slow, \/* is_far *\/ true);\n+\n+  bind(push);\n+  \/\/ After successful lock, push object on lock-stack.\n+  add(t, xthread, top);\n+  sd(obj, Address(t));\n+  addw(top, top, oopSize);\n+  sw(top, Address(xthread, JavaThread::lock_stack_top_offset()));\n@@ -4740,2 +4755,0 @@\n-\/\/ Branches to slow upon failure.\n-\/\/ Falls through upon success.\n@@ -4744,3 +4757,3 @@\n-\/\/ - hdr: the (pre-loaded) header of the object\n-\/\/ - tmp1, tmp2: temporary registers\n-void MacroAssembler::lightweight_unlock(Register obj, Register hdr, Register tmp1, Register tmp2, Label& slow) {\n+\/\/ - tmp1, tmp2, tmp3: temporary registers\n+\/\/ - slow: branched to if unlocking fails\n+void MacroAssembler::lightweight_unlock(Register obj, Register tmp1, Register tmp2, Register tmp3, Label& slow) {\n@@ -4748,1 +4761,1 @@\n-  assert_different_registers(obj, hdr, tmp1, tmp2, t0);\n+  assert_different_registers(obj, tmp1, tmp2, tmp3, t0);\n@@ -4752,4 +4765,0 @@\n-    \/\/ The following checks rely on the fact that LockStack is only ever modified by\n-    \/\/ its owning thread, even if the lock got inflated concurrently; removal of LockStack\n-    \/\/ entries after inflation will happen delayed in that case.\n-\n@@ -4760,1 +4769,1 @@\n-    bgt(tmp1, tmp2, stack_ok);\n+    bge(tmp1, tmp2, stack_ok);\n@@ -4764,18 +4773,0 @@\n-  {\n-    \/\/ Check if the top of the lock-stack matches the unlocked object.\n-    Label tos_ok;\n-    subw(tmp1, tmp1, oopSize);\n-    add(tmp1, xthread, tmp1);\n-    ld(tmp1, Address(tmp1, 0));\n-    beq(tmp1, obj, tos_ok);\n-    STOP(\"Top of lock-stack does not match the unlocked object\");\n-    bind(tos_ok);\n-  }\n-  {\n-    \/\/ Check that hdr is fast-locked.\n-   Label hdr_ok;\n-    andi(tmp1, hdr, markWord::lock_mask_in_place);\n-    beqz(tmp1, hdr_ok);\n-    STOP(\"Header is not fast-locked\");\n-    bind(hdr_ok);\n-  }\n@@ -4784,2 +4775,26 @@\n-  \/\/ Load the new header (unlocked) into tmp1\n-  ori(tmp1, hdr, markWord::unlocked_value);\n+  Label unlocked, push_and_slow;\n+  const Register top = tmp1;\n+  const Register mark = tmp2;\n+  const Register t = tmp3;\n+\n+  \/\/ Check if obj is top of lock-stack.\n+  lwu(top, Address(xthread, JavaThread::lock_stack_top_offset()));\n+  subw(top, top, oopSize);\n+  add(t, xthread, top);\n+  ld(t, Address(t));\n+  bne(obj, t, slow, \/* is_far *\/ true);\n+\n+  \/\/ Pop lock-stack.\n+  DEBUG_ONLY(add(t, xthread, top);)\n+  DEBUG_ONLY(sd(zr, Address(t));)\n+  sw(top, Address(xthread, JavaThread::lock_stack_top_offset()));\n+\n+  \/\/ Check if recursive.\n+  add(t, xthread, top);\n+  ld(t, Address(t, -oopSize));\n+  beq(obj, t, unlocked);\n+\n+  \/\/ Not recursive. Check header for monitor (0b10).\n+  ld(mark, Address(obj, oopDesc::mark_offset_in_bytes()));\n+  test_bit(t, mark, exact_log2(markWord::monitor_value));\n+  bnez(t, push_and_slow);\n@@ -4787,10 +4802,6 @@\n-  \/\/ Try to swing header from locked to unlocked\n-  Label success;\n-  cmpxchgptr(hdr, tmp1, obj, tmp2, success, &slow);\n-  bind(success);\n-\n-  \/\/ After successful unlock, pop object from lock-stack\n-  lwu(tmp1, Address(xthread, JavaThread::lock_stack_top_offset()));\n-  subw(tmp1, tmp1, oopSize);\n-  add(tmp2, xthread, tmp1);\n-  sd(zr, Address(tmp2, 0));\n+  \/\/ Check header not unlocked (0b01).\n+  Label not_unlocked;\n+  test_bit(t, mark, exact_log2(markWord::unlocked_value));\n+  beqz(t, not_unlocked);\n+  stop(\"lightweight_unlock already unlocked\");\n+  bind(not_unlocked);\n@@ -4799,1 +4810,17 @@\n-  sw(tmp1, Address(xthread, JavaThread::lock_stack_top_offset()));\n+\n+  \/\/ Try to unlock. Transition lock bits 0b00 => 0b01\n+  assert(oopDesc::mark_offset_in_bytes() == 0, \"required to avoid lea\");\n+  ori(t, mark, markWord::unlocked_value);\n+  cmpxchg(\/*addr*\/ obj, \/*expected*\/ mark, \/*new*\/ t, Assembler::int64,\n+          \/*acquire*\/ Assembler::relaxed, \/*release*\/ Assembler::rl, \/*result*\/ t);\n+  beq(mark, t, unlocked);\n+\n+  bind(push_and_slow);\n+  \/\/ Restore lock-stack and handle the unlock in runtime.\n+  DEBUG_ONLY(add(t, xthread, top);)\n+  DEBUG_ONLY(sd(obj, Address(t));)\n+  addw(top, top, oopSize);\n+  sw(top, Address(xthread, JavaThread::lock_stack_top_offset()));\n+  j(slow);\n+\n+  bind(unlocked);\n","filename":"src\/hotspot\/cpu\/riscv\/macroAssembler_riscv.cpp","additions":97,"deletions":70,"binary":false,"changes":167,"status":"modified"},{"patch":"@@ -1455,2 +1455,2 @@\n-  void lightweight_lock(Register obj, Register hdr, Register tmp1, Register tmp2, Label& slow);\n-  void lightweight_unlock(Register obj, Register hdr, Register tmp1, Register tmp2, Label& slow);\n+  void lightweight_lock(Register obj, Register tmp1, Register tmp2, Register tmp3, Label& slow);\n+  void lightweight_unlock(Register obj, Register tmp1, Register tmp2, Register tmp3, Label& slow);\n","filename":"src\/hotspot\/cpu\/riscv\/macroAssembler_riscv.hpp","additions":2,"deletions":2,"binary":false,"changes":4,"status":"modified"},{"patch":"@@ -10236,0 +10236,1 @@\n+  predicate(LockingMode != LM_LIGHTWEIGHT);\n@@ -10239,1 +10240,1 @@\n-  ins_cost(LOAD_COST * 2 + STORE_COST * 3 + ALU_COST * 6 + BRANCH_COST * 3);\n+  ins_cost(10 * DEFAULT_COST);\n@@ -10252,0 +10253,1 @@\n+  predicate(LockingMode != LM_LIGHTWEIGHT);\n@@ -10255,1 +10257,1 @@\n-  ins_cost(LOAD_COST * 2 + STORE_COST + ALU_COST * 2 + BRANCH_COST * 4);\n+  ins_cost(10 * DEFAULT_COST);\n@@ -10265,0 +10267,32 @@\n+instruct cmpFastLockLightweight(rFlagsReg cr, iRegP object, iRegP_R10 box, iRegPNoSp tmp1, iRegPNoSp tmp2)\n+%{\n+  predicate(LockingMode == LM_LIGHTWEIGHT);\n+  match(Set cr (FastLock object box));\n+  effect(TEMP tmp1, TEMP tmp2, USE_KILL box);\n+\n+  ins_cost(10 * DEFAULT_COST);\n+  format %{ \"fastlock $object,$box\\t! kills $box,$tmp1,$tmp2 #@cmpFastLockLightweight\" %}\n+\n+  ins_encode %{\n+    __ fast_lock_lightweight($object$$Register, $box$$Register, $tmp1$$Register, $tmp2$$Register);\n+  %}\n+\n+  ins_pipe(pipe_serial);\n+%}\n+\n+instruct cmpFastUnlockLightweight(rFlagsReg cr, iRegP object, iRegP_R10 box, iRegPNoSp tmp1, iRegPNoSp tmp2)\n+%{\n+  predicate(LockingMode == LM_LIGHTWEIGHT);\n+  match(Set cr (FastUnlock object box));\n+  effect(TEMP tmp1, TEMP tmp2, USE_KILL box);\n+\n+  ins_cost(10 * DEFAULT_COST);\n+  format %{ \"fastunlock $object,$box\\t! kills $box,$tmp1,$tmp2, #@cmpFastUnlockLightweight\" %}\n+\n+  ins_encode %{\n+    __ fast_unlock_lightweight($object$$Register, $box$$Register, $tmp1$$Register, $tmp2$$Register);\n+  %}\n+\n+  ins_pipe(pipe_serial);\n+%}\n+\n","filename":"src\/hotspot\/cpu\/riscv\/riscv.ad","additions":36,"deletions":2,"binary":false,"changes":38,"status":"modified"},{"patch":"@@ -1707,2 +1707,1 @@\n-      assert(LockingMode == LM_LIGHTWEIGHT, \"\");\n-      __ ld(swap_reg, Address(obj_reg, oopDesc::mark_offset_in_bytes()));\n+      assert(LockingMode == LM_LIGHTWEIGHT, \"must be\");\n@@ -1834,3 +1833,0 @@\n-      __ ld(old_hdr, Address(obj_reg, oopDesc::mark_offset_in_bytes()));\n-      __ test_bit(t0, old_hdr, exact_log2(markWord::monitor_value));\n-      __ bnez(t0, slow_path_unlock);\n","filename":"src\/hotspot\/cpu\/riscv\/sharedRuntime_riscv.cpp","additions":1,"deletions":5,"binary":false,"changes":6,"status":"modified"},{"patch":"@@ -212,0 +212,2 @@\n+  constexpr static bool supports_recursive_lightweight_locking() { return true; }\n+\n","filename":"src\/hotspot\/cpu\/riscv\/vm_version_riscv.hpp","additions":2,"deletions":0,"binary":false,"changes":2,"status":"modified"},{"patch":"@@ -256,0 +256,5 @@\n+void LoadKlassStub::emit_code(LIR_Assembler* ce) {\n+  \/\/ Currently not needed.\n+  Unimplemented();\n+}\n+\n","filename":"src\/hotspot\/cpu\/s390\/c1_CodeStubs_s390.cpp","additions":5,"deletions":0,"binary":false,"changes":5,"status":"modified"},{"patch":"@@ -2,1 +2,1 @@\n- * Copyright (c) 1997, 2023, Oracle and\/or its affiliates. All rights reserved.\n+ * Copyright (c) 1997, 2024, Oracle and\/or its affiliates. All rights reserved.\n@@ -5254,0 +5254,17 @@\n+#ifdef _LP64\n+void MacroAssembler::load_nklass_compact(Register dst, Register src) {\n+  assert(UseCompactObjectHeaders, \"expect compact object headers\");\n+\n+  Label fast;\n+  movq(dst, Address(src, oopDesc::mark_offset_in_bytes()));\n+  testb(dst, markWord::monitor_value);\n+  jccb(Assembler::zero, fast);\n+\n+  \/\/ Fetch displaced header\n+  movq(dst, Address(dst, OM_OFFSET_NO_MONITOR_VALUE_TAG(header)));\n+\n+  bind(fast);\n+  shrq(dst, markWord::klass_shift);\n+}\n+#endif\n+\n@@ -5258,1 +5275,4 @@\n-  if (UseCompressedClassPointers) {\n+  if (UseCompactObjectHeaders) {\n+    load_nklass_compact(dst, src);\n+    decode_klass_not_null(dst, tmp);\n+  } else if (UseCompressedClassPointers) {\n@@ -5263,0 +5283,1 @@\n+  {\n@@ -5264,0 +5285,1 @@\n+  }\n@@ -5267,0 +5289,1 @@\n+  assert(!UseCompactObjectHeaders, \"not with compact headers\");\n@@ -5278,0 +5301,33 @@\n+void MacroAssembler::cmp_klass(Register klass, Register obj, Register tmp) {\n+#ifdef _LP64\n+  if (UseCompactObjectHeaders) {\n+    load_nklass_compact(tmp, obj);\n+    cmpl(klass, tmp);\n+  } else if (UseCompressedClassPointers) {\n+    cmpl(klass, Address(obj, oopDesc::klass_offset_in_bytes()));\n+  } else\n+#endif\n+  {\n+    cmpptr(klass, Address(obj, oopDesc::klass_offset_in_bytes()));\n+  }\n+}\n+\n+void MacroAssembler::cmp_klass(Register src, Register dst, Register tmp1, Register tmp2) {\n+#ifdef _LP64\n+  if (UseCompactObjectHeaders) {\n+    assert(tmp2 != noreg, \"need tmp2\");\n+    assert_different_registers(src, dst, tmp1, tmp2);\n+    load_nklass_compact(tmp1, src);\n+    load_nklass_compact(tmp2, dst);\n+    cmpl(tmp1, tmp2);\n+  } else if (UseCompressedClassPointers) {\n+    movl(tmp1, Address(src, oopDesc::klass_offset_in_bytes()));\n+    cmpl(tmp1, Address(dst, oopDesc::klass_offset_in_bytes()));\n+  } else\n+#endif\n+  {\n+    movptr(tmp1, Address(src, oopDesc::klass_offset_in_bytes()));\n+    cmpptr(tmp1, Address(dst, oopDesc::klass_offset_in_bytes()));\n+  }\n+}\n+\n@@ -5325,0 +5381,1 @@\n+  assert(!UseCompactObjectHeaders, \"Don't use with compact headers\");\n@@ -9818,2 +9875,0 @@\n-\/\/ Branches to slow upon failure to lock the object, with ZF cleared.\n-\/\/ Falls through upon success with unspecified ZF.\n@@ -9822,1 +9877,1 @@\n-\/\/ hdr: the (pre-loaded) header of the object, must be rax\n+\/\/ reg_rax: rax\n@@ -9825,19 +9880,31 @@\n-void MacroAssembler::lightweight_lock(Register obj, Register hdr, Register thread, Register tmp, Label& slow) {\n-  assert(hdr == rax, \"header must be in rax for cmpxchg\");\n-  assert_different_registers(obj, hdr, thread, tmp);\n-\n-  \/\/ First we need to check if the lock-stack has room for pushing the object reference.\n-  \/\/ Note: we subtract 1 from the end-offset so that we can do a 'greater' comparison, instead\n-  \/\/ of 'greaterEqual' below, which readily clears the ZF. This makes C2 code a little simpler and\n-  \/\/ avoids one branch.\n-  cmpl(Address(thread, JavaThread::lock_stack_top_offset()), LockStack::end_offset() - 1);\n-  jcc(Assembler::greater, slow);\n-\n-  \/\/ Now we attempt to take the fast-lock.\n-  \/\/ Clear lock_mask bits (locked state).\n-  andptr(hdr, ~(int32_t)markWord::lock_mask_in_place);\n-  movptr(tmp, hdr);\n-  \/\/ Set unlocked_value bit.\n-  orptr(hdr, markWord::unlocked_value);\n-  lock();\n-  cmpxchgptr(tmp, Address(obj, oopDesc::mark_offset_in_bytes()));\n+void MacroAssembler::lightweight_lock(Register obj, Register reg_rax, Register thread, Register tmp, Label& slow) {\n+  assert(reg_rax == rax, \"\");\n+  assert_different_registers(obj, reg_rax, thread, tmp);\n+\n+  Label push;\n+  const Register top = tmp;\n+\n+  \/\/ Preload the markWord. It is important that this is the first\n+  \/\/ instruction emitted as it is part of C1's null check semantics.\n+  movptr(reg_rax, Address(obj, oopDesc::mark_offset_in_bytes()));\n+\n+  \/\/ Load top.\n+  movl(top, Address(thread, JavaThread::lock_stack_top_offset()));\n+\n+  \/\/ Check if the lock-stack is full.\n+  cmpl(top, LockStack::end_offset());\n+  jcc(Assembler::greaterEqual, slow);\n+\n+  \/\/ Check for recursion.\n+  cmpptr(obj, Address(thread, top, Address::times_1, -oopSize));\n+  jcc(Assembler::equal, push);\n+\n+  \/\/ Check header for monitor (0b10).\n+  testptr(reg_rax, markWord::monitor_value);\n+  jcc(Assembler::notZero, slow);\n+\n+  \/\/ Try to lock. Transition lock bits 0b01 => 0b00\n+  movptr(tmp, reg_rax);\n+  andptr(tmp, ~(int32_t)markWord::unlocked_value);\n+  orptr(reg_rax, markWord::unlocked_value);\n+  lock(); cmpxchgptr(tmp, Address(obj, oopDesc::mark_offset_in_bytes()));\n@@ -9846,5 +9913,8 @@\n-  \/\/ If successful, push object to lock-stack.\n-  movl(tmp, Address(thread, JavaThread::lock_stack_top_offset()));\n-  movptr(Address(thread, tmp), obj);\n-  incrementl(tmp, oopSize);\n-  movl(Address(thread, JavaThread::lock_stack_top_offset()), tmp);\n+  \/\/ Restore top, CAS clobbers register.\n+  movl(top, Address(thread, JavaThread::lock_stack_top_offset()));\n+\n+  bind(push);\n+  \/\/ After successful lock, push object on lock-stack.\n+  movptr(Address(thread, top), obj);\n+  incrementl(top, oopSize);\n+  movl(Address(thread, JavaThread::lock_stack_top_offset()), top);\n@@ -9854,2 +9924,0 @@\n-\/\/ Branches to slow upon failure, with ZF cleared.\n-\/\/ Falls through upon success, with unspecified ZF.\n@@ -9858,1 +9926,2 @@\n-\/\/ hdr: the (pre-loaded) header of the object, must be rax\n+\/\/ reg_rax: rax\n+\/\/ thread: the thread\n@@ -9860,9 +9929,14 @@\n-void MacroAssembler::lightweight_unlock(Register obj, Register hdr, Register tmp, Label& slow) {\n-  assert(hdr == rax, \"header must be in rax for cmpxchg\");\n-  assert_different_registers(obj, hdr, tmp);\n-\n-  \/\/ Mark-word must be lock_mask now, try to swing it back to unlocked_value.\n-  movptr(tmp, hdr); \/\/ The expected old value\n-  orptr(tmp, markWord::unlocked_value);\n-  lock();\n-  cmpxchgptr(tmp, Address(obj, oopDesc::mark_offset_in_bytes()));\n+\/\/\n+\/\/ x86_32 Note: reg_rax and thread may alias each other due to limited register\n+\/\/              availiability.\n+void MacroAssembler::lightweight_unlock(Register obj, Register reg_rax, Register thread, Register tmp, Label& slow) {\n+  assert(reg_rax == rax, \"\");\n+  assert_different_registers(obj, reg_rax, tmp);\n+  LP64_ONLY(assert_different_registers(obj, reg_rax, thread, tmp);)\n+\n+  Label unlocked, push_and_slow;\n+  const Register top = tmp;\n+\n+  \/\/ Check if obj is top of lock-stack.\n+  movl(top, Address(thread, JavaThread::lock_stack_top_offset()));\n+  cmpptr(obj, Address(thread, top, Address::times_1, -oopSize));\n@@ -9870,7 +9944,3 @@\n-  \/\/ Pop the lock object from the lock-stack.\n-#ifdef _LP64\n-  const Register thread = r15_thread;\n-#else\n-  const Register thread = rax;\n-  get_thread(thread);\n-#endif\n+\n+  \/\/ Pop lock-stack.\n+  DEBUG_ONLY(movptr(Address(thread, top, Address::times_1, -oopSize), 0);)\n@@ -9878,0 +9948,10 @@\n+\n+  \/\/ Check if recursive.\n+  cmpptr(obj, Address(thread, top, Address::times_1, -2 * oopSize));\n+  jcc(Assembler::equal, unlocked);\n+\n+  \/\/ Not recursive. Check header for monitor (0b10).\n+  movptr(reg_rax, Address(obj, oopDesc::mark_offset_in_bytes()));\n+  testptr(reg_rax, markWord::monitor_value);\n+  jcc(Assembler::notZero, push_and_slow);\n+\n@@ -9879,2 +9959,6 @@\n-  movl(tmp, Address(thread, JavaThread::lock_stack_top_offset()));\n-  movptr(Address(thread, tmp), 0);\n+  \/\/ Check header not unlocked (0b01).\n+  Label not_unlocked;\n+  testptr(reg_rax, markWord::unlocked_value);\n+  jcc(Assembler::zero, not_unlocked);\n+  stop(\"lightweight_unlock already unlocked\");\n+  bind(not_unlocked);\n@@ -9882,0 +9966,21 @@\n+\n+  \/\/ Try to unlock. Transition lock bits 0b00 => 0b01\n+  movptr(tmp, reg_rax);\n+  orptr(tmp, markWord::unlocked_value);\n+  lock(); cmpxchgptr(tmp, Address(obj, oopDesc::mark_offset_in_bytes()));\n+  jcc(Assembler::equal, unlocked);\n+\n+  bind(push_and_slow);\n+  \/\/ Restore lock-stack and handle the unlock in runtime.\n+  if (thread == reg_rax) {\n+    \/\/ On x86_32 we may lose the thread.\n+    get_thread(thread);\n+  }\n+#ifdef ASSERT\n+  movl(top, Address(thread, JavaThread::lock_stack_top_offset()));\n+  movptr(Address(thread, top), obj);\n+#endif\n+  addl(Address(thread, JavaThread::lock_stack_top_offset()), oopSize);\n+  jmp(slow);\n+\n+  bind(unlocked);\n","filename":"src\/hotspot\/cpu\/x86\/macroAssembler_x86.cpp","additions":155,"deletions":50,"binary":false,"changes":205,"status":"modified"},{"patch":"@@ -2,1 +2,1 @@\n- * Copyright (c) 1997, 2023, Oracle and\/or its affiliates. All rights reserved.\n+ * Copyright (c) 1997, 2024, Oracle and\/or its affiliates. All rights reserved.\n@@ -366,0 +366,3 @@\n+#ifdef _LP64\n+  void load_nklass_compact(Register dst, Register src);\n+#endif\n@@ -369,0 +372,8 @@\n+  \/\/ Compares the Klass pointer of an object to a given Klass (which might be narrow,\n+  \/\/ depending on UseCompressedClassPointers).\n+  void cmp_klass(Register klass, Register dst, Register tmp);\n+\n+  \/\/ Compares the Klass pointer of two objects o1 and o2. Result is in the condition flags.\n+  \/\/ Uses tmp1 and tmp2 as temporary registers.\n+  void cmp_klass(Register src, Register dst, Register tmp1, Register tmp2);\n+\n@@ -2026,2 +2037,2 @@\n-  void lightweight_lock(Register obj, Register hdr, Register thread, Register tmp, Label& slow);\n-  void lightweight_unlock(Register obj, Register hdr, Register tmp, Label& slow);\n+  void lightweight_lock(Register obj, Register reg_rax, Register thread, Register tmp, Label& slow);\n+  void lightweight_unlock(Register obj, Register reg_rax, Register thread, Register tmp, Label& slow);\n","filename":"src\/hotspot\/cpu\/x86\/macroAssembler_x86.hpp","additions":14,"deletions":3,"binary":false,"changes":17,"status":"modified"},{"patch":"@@ -2,1 +2,1 @@\n- * Copyright (c) 2003, 2023, Oracle and\/or its affiliates. All rights reserved.\n+ * Copyright (c) 2003, 2024, Oracle and\/or its affiliates. All rights reserved.\n@@ -1717,2 +1717,0 @@\n-      \/\/ Load object header\n-      __ movptr(swap_reg, Address(obj_reg, oopDesc::mark_offset_in_bytes()));\n@@ -1876,3 +1874,1 @@\n-      __ movptr(swap_reg, Address(obj_reg, oopDesc::mark_offset_in_bytes()));\n-      __ andptr(swap_reg, ~(int32_t)markWord::lock_mask_in_place);\n-      __ lightweight_unlock(obj_reg, swap_reg, lock_reg, slow_path_unlock);\n+      __ lightweight_unlock(obj_reg, swap_reg, thread, lock_reg, slow_path_unlock);\n","filename":"src\/hotspot\/cpu\/x86\/sharedRuntime_x86_32.cpp","additions":2,"deletions":6,"binary":false,"changes":8,"status":"modified"},{"patch":"@@ -2,1 +2,1 @@\n- * Copyright (c) 2003, 2023, Oracle and\/or its affiliates. All rights reserved.\n+ * Copyright (c) 2003, 2024, Oracle and\/or its affiliates. All rights reserved.\n@@ -2192,2 +2192,0 @@\n-      \/\/ Load object header\n-      __ movptr(swap_reg, Address(obj_reg, oopDesc::mark_offset_in_bytes()));\n@@ -2336,3 +2334,1 @@\n-      __ movptr(swap_reg, Address(obj_reg, oopDesc::mark_offset_in_bytes()));\n-      __ andptr(swap_reg, ~(int32_t)markWord::lock_mask_in_place);\n-      __ lightweight_unlock(obj_reg, swap_reg, lock_reg, slow_path_unlock);\n+      __ lightweight_unlock(obj_reg, swap_reg, r15_thread, lock_reg, slow_path_unlock);\n","filename":"src\/hotspot\/cpu\/x86\/sharedRuntime_x86_64.cpp","additions":2,"deletions":6,"binary":false,"changes":8,"status":"modified"},{"patch":"@@ -2,1 +2,1 @@\n- * Copyright (c) 1997, 2023, Oracle and\/or its affiliates. All rights reserved.\n+ * Copyright (c) 1997, 2024, Oracle and\/or its affiliates. All rights reserved.\n@@ -769,0 +769,4 @@\n+  constexpr static bool supports_recursive_lightweight_locking() {\n+    return true;\n+  }\n+\n","filename":"src\/hotspot\/cpu\/x86\/vm_version_x86.hpp","additions":5,"deletions":1,"binary":false,"changes":6,"status":"modified"},{"patch":"@@ -13783,1 +13783,1 @@\n-  predicate(!Compile::current()->use_rtm());\n+  predicate(LockingMode != LM_LIGHTWEIGHT && !Compile::current()->use_rtm());\n@@ -13797,0 +13797,1 @@\n+  predicate(LockingMode != LM_LIGHTWEIGHT);\n@@ -13807,0 +13808,26 @@\n+instruct cmpFastLockLightweight(eFlagsReg cr, eRegP object, eBXRegP box, eAXRegI eax_reg, eRegP tmp, eRegP thread) %{\n+  predicate(LockingMode == LM_LIGHTWEIGHT);\n+  match(Set cr (FastLock object box));\n+  effect(TEMP eax_reg, TEMP tmp, USE_KILL box, TEMP thread);\n+  ins_cost(300);\n+  format %{ \"FASTLOCK $object,$box\\t! kills $box,$eax_reg,$tmp\" %}\n+  ins_encode %{\n+    __ get_thread($thread$$Register);\n+    __ fast_lock_lightweight($object$$Register, $box$$Register, $eax_reg$$Register, $tmp$$Register, $thread$$Register);\n+  %}\n+  ins_pipe(pipe_slow);\n+%}\n+\n+instruct cmpFastUnlockLightweight(eFlagsReg cr, eRegP object, eAXRegP eax_reg, eRegP tmp, eRegP thread) %{\n+  predicate(LockingMode == LM_LIGHTWEIGHT);\n+  match(Set cr (FastUnlock object eax_reg));\n+  effect(TEMP tmp, USE_KILL eax_reg, TEMP thread);\n+  ins_cost(300);\n+  format %{ \"FASTUNLOCK $object,$eax_reg\\t! kills $eax_reg,$tmp\" %}\n+  ins_encode %{\n+    __ get_thread($thread$$Register);\n+    __ fast_unlock_lightweight($object$$Register, $eax_reg$$Register, $tmp$$Register, $thread$$Register);\n+  %}\n+  ins_pipe(pipe_slow);\n+%}\n+\n","filename":"src\/hotspot\/cpu\/x86\/x86_32.ad","additions":28,"deletions":1,"binary":false,"changes":29,"status":"modified"},{"patch":"@@ -5317,0 +5317,1 @@\n+  predicate(!UseCompactObjectHeaders);\n@@ -5327,0 +5328,15 @@\n+instruct loadNKlassCompactHeaders(rRegN dst, memory mem, rFlagsReg cr)\n+%{\n+  predicate(UseCompactObjectHeaders);\n+  match(Set dst (LoadNKlass mem));\n+  effect(KILL cr);\n+  ins_cost(125); \/\/ XXX\n+  format %{ \"movl    $dst, $mem\\t# compressed klass ptr\" %}\n+  ins_encode %{\n+    Register index = $mem$$index != 4 ? $mem$$index$$Register : noreg;\n+    Address::ScaleFactor sf = (index != noreg) ? static_cast<Address::ScaleFactor>($mem$$scale) : Address::no_scale;\n+    __ load_nklass_compact_c2($dst$$Register, $mem$$base$$Register, index, sf, $mem$$disp);\n+  %}\n+  ins_pipe(pipe_slow); \/\/ XXX\n+%}\n+\n@@ -12645,0 +12661,1 @@\n+  predicate(!UseCompactObjectHeaders);\n@@ -13291,1 +13308,1 @@\n-  predicate(!Compile::current()->use_rtm());\n+  predicate(LockingMode != LM_LIGHTWEIGHT && !Compile::current()->use_rtm());\n@@ -13304,0 +13321,1 @@\n+  predicate(LockingMode != LM_LIGHTWEIGHT);\n@@ -13314,0 +13332,24 @@\n+instruct cmpFastLockLightweight(rFlagsReg cr, rRegP object, rbx_RegP box, rax_RegI rax_reg, rRegP tmp) %{\n+  predicate(LockingMode == LM_LIGHTWEIGHT);\n+  match(Set cr (FastLock object box));\n+  effect(TEMP rax_reg, TEMP tmp, USE_KILL box);\n+  ins_cost(300);\n+  format %{ \"fastlock $object,$box\\t! kills $box,$rax_reg,$tmp\" %}\n+  ins_encode %{\n+    __ fast_lock_lightweight($object$$Register, $box$$Register, $rax_reg$$Register, $tmp$$Register, r15_thread);\n+  %}\n+  ins_pipe(pipe_slow);\n+%}\n+\n+instruct cmpFastUnlockLightweight(rFlagsReg cr, rRegP object, rax_RegP rax_reg, rRegP tmp) %{\n+  predicate(LockingMode == LM_LIGHTWEIGHT);\n+  match(Set cr (FastUnlock object rax_reg));\n+  effect(TEMP tmp, USE_KILL rax_reg);\n+  ins_cost(300);\n+  format %{ \"fastunlock $object,$rax_reg\\t! kills $rax_reg,$tmp\" %}\n+  ins_encode %{\n+    __ fast_unlock_lightweight($object$$Register, $rax_reg$$Register, $tmp$$Register, r15_thread);\n+  %}\n+  ins_pipe(pipe_slow);\n+%}\n+\n","filename":"src\/hotspot\/cpu\/x86\/x86_64.ad","additions":43,"deletions":1,"binary":false,"changes":44,"status":"modified"},{"patch":"@@ -584,0 +584,18 @@\n+class LoadKlassStub: public CodeStub {\n+private:\n+  LIR_Opr          _result;\n+\n+public:\n+  LoadKlassStub(LIR_Opr result) :\n+    CodeStub(), _result(result) {};\n+\n+  virtual void emit_code(LIR_Assembler* e);\n+  virtual void visit(LIR_OpVisitState* visitor) {\n+    visitor->do_temp(_result);\n+    visitor->do_output(_result);\n+  }\n+#ifndef PRODUCT\n+  virtual void print_name(outputStream* out) const { out->print(\"LoadKlassStub\"); }\n+#endif \/\/ PRODUCT\n+};\n+\n","filename":"src\/hotspot\/share\/c1\/c1_CodeStubs.hpp","additions":18,"deletions":0,"binary":false,"changes":18,"status":"modified"},{"patch":"@@ -196,2 +196,6 @@\n-    oopDesc::set_mark(mem, markWord::prototype());\n-    oopDesc::release_set_klass(mem, k);\n+    if (UseCompactObjectHeaders) {\n+      oopDesc::release_set_mark(mem, k->prototype_header());\n+    } else {\n+      oopDesc::set_mark(mem, markWord::prototype());\n+      oopDesc::release_set_klass(mem, k);\n+    }\n@@ -263,2 +267,6 @@\n-  oopDesc::set_mark(mem, markWord::prototype());\n-  cast_to_oop(mem)->set_narrow_klass(nk);\n+  if (UseCompactObjectHeaders) {\n+    oopDesc::release_set_mark(mem, markWord::prototype().set_narrow_klass(nk));\n+  } else {\n+    oopDesc::set_mark(mem, markWord::prototype());\n+    cast_to_oop(mem)->set_narrow_klass(nk);\n+  }\n@@ -425,1 +433,5 @@\n-  fake_oop->set_narrow_klass(nk);\n+  if (UseCompactObjectHeaders) {\n+    fake_oop->set_mark(fake_oop->mark().set_narrow_klass(nk));\n+  } else {\n+    fake_oop->set_narrow_klass(nk);\n+  }\n@@ -431,1 +443,5 @@\n-    fake_oop->set_mark(markWord::prototype().copy_set_hash(src_hash));\n+    if (UseCompactObjectHeaders) {\n+      fake_oop->set_mark(markWord::prototype().set_narrow_klass(nk).copy_set_hash(src_hash));\n+    } else {\n+      fake_oop->set_mark(markWord::prototype().copy_set_hash(src_hash));\n+    }\n","filename":"src\/hotspot\/share\/cds\/archiveHeapWriter.cpp","additions":22,"deletions":6,"binary":false,"changes":28,"status":"modified"},{"patch":"@@ -91,0 +91,1 @@\n+#include \"gc\/shared\/slidingForwarding.hpp\"\n@@ -1524,0 +1525,2 @@\n+  SlidingForwarding::initialize(heap_rs.region(), HeapRegion::GrainWords);\n+\n","filename":"src\/hotspot\/share\/gc\/g1\/g1CollectedHeap.cpp","additions":3,"deletions":0,"binary":false,"changes":3,"status":"modified"},{"patch":"@@ -43,0 +43,1 @@\n+#include \"gc\/shared\/slidingForwarding.hpp\"\n@@ -212,0 +213,2 @@\n+  SlidingForwarding::begin();\n+\n@@ -224,0 +227,2 @@\n+  SlidingForwarding::end();\n+\n@@ -392,1 +397,2 @@\n-void G1FullCollector::phase2c_prepare_serial_compaction() {\n+template <bool ALT_FWD>\n+void G1FullCollector::phase2c_prepare_serial_compaction_impl() {\n@@ -417,1 +423,1 @@\n-  G1SerialRePrepareClosure re_prepare(serial_cp, dense_prefix_top);\n+  G1SerialRePrepareClosure<ALT_FWD> re_prepare(serial_cp, dense_prefix_top);\n@@ -430,1 +436,10 @@\n-void G1FullCollector::phase2d_prepare_humongous_compaction() {\n+void G1FullCollector::phase2c_prepare_serial_compaction() {\n+  if (UseAltGCForwarding) {\n+    phase2c_prepare_serial_compaction_impl<true>();\n+  } else {\n+    phase2c_prepare_serial_compaction_impl<false>();\n+  }\n+}\n+\n+template <bool ALT_FWD>\n+void G1FullCollector::phase2d_prepare_humongous_compaction_impl() {\n@@ -448,1 +463,1 @@\n-      uint num_regions = humongous_cp->forward_humongous(hr);\n+      uint num_regions = humongous_cp->forward_humongous<ALT_FWD>(hr);\n@@ -459,0 +474,8 @@\n+void G1FullCollector::phase2d_prepare_humongous_compaction() {\n+  if (UseAltGCForwarding) {\n+    phase2d_prepare_humongous_compaction_impl<true>();\n+  } else {\n+    phase2d_prepare_humongous_compaction_impl<false>();\n+  }\n+}\n+\n","filename":"src\/hotspot\/share\/gc\/g1\/g1FullCollector.cpp","additions":27,"deletions":4,"binary":false,"changes":31,"status":"modified"},{"patch":"@@ -243,5 +243,1 @@\n-    if (!obj->is_forwarded()) {\n-      cl->do_object(obj);\n-    }\n-#ifdef ASSERT\n-    else {\n+    if (obj->is_forwarded()) {\n@@ -249,2 +245,7 @@\n-#endif\n-    p += cast_to_oop(p)->size();\n+      \/\/ It is safe to use the forwardee here. Parallel GC only uses\n+      \/\/ header-based forwarding during promotion. Full GC doesn't\n+      \/\/ use the object header for forwarding at all.\n+      p += obj->forwardee()->size();\n+    } else {\n+      cl->do_object(obj);\n+      p += obj->size();\n","filename":"src\/hotspot\/share\/gc\/parallel\/mutableSpace.cpp","additions":8,"deletions":7,"binary":false,"changes":15,"status":"modified"},{"patch":"@@ -49,0 +49,1 @@\n+#include \"gc\/shared\/slidingForwarding.hpp\"\n@@ -91,0 +92,2 @@\n+  SlidingForwarding::begin();\n+\n@@ -109,0 +112,2 @@\n+  SlidingForwarding::end();\n+\n@@ -263,9 +268,21 @@\n-  CodeBlobToOopClosure code_closure(&adjust_pointer_closure, CodeBlobToOopClosure::FixRelocations);\n-  gch->process_roots(GenCollectedHeap::SO_AllCodeCache,\n-                     &adjust_pointer_closure,\n-                     &adjust_cld_closure,\n-                     &adjust_cld_closure,\n-                     &code_closure);\n-\n-  gch->gen_process_weak_roots(&adjust_pointer_closure);\n-\n+  if (UseAltGCForwarding) {\n+    AdjustPointerClosure<true> adjust_pointer_closure;\n+    CLDToOopClosure adjust_cld_closure(&adjust_pointer_closure, ClassLoaderData::_claim_stw_fullgc_adjust);\n+    CodeBlobToOopClosure code_closure(&adjust_pointer_closure, CodeBlobToOopClosure::FixRelocations);\n+    gch->process_roots(GenCollectedHeap::SO_AllCodeCache,\n+                       &adjust_pointer_closure,\n+                       &adjust_cld_closure,\n+                       &adjust_cld_closure,\n+                       &code_closure);\n+    gch->gen_process_weak_roots(&adjust_pointer_closure);\n+  } else {\n+    AdjustPointerClosure<false> adjust_pointer_closure;\n+    CLDToOopClosure adjust_cld_closure(&adjust_pointer_closure, ClassLoaderData::_claim_stw_fullgc_adjust);\n+    CodeBlobToOopClosure code_closure(&adjust_pointer_closure, CodeBlobToOopClosure::FixRelocations);\n+    gch->process_roots(GenCollectedHeap::SO_AllCodeCache,\n+                       &adjust_pointer_closure,\n+                       &adjust_cld_closure,\n+                       &adjust_cld_closure,\n+                       &code_closure);\n+    gch->gen_process_weak_roots(&adjust_pointer_closure);\n+  }\n","filename":"src\/hotspot\/share\/gc\/serial\/genMarkSweep.cpp","additions":26,"deletions":9,"binary":false,"changes":35,"status":"modified"},{"patch":"@@ -231,1 +231,3 @@\n-  if (!Metaspace::contains(object->klass_raw())) {\n+  \/\/ With compact headers, we can't safely access the class, due\n+  \/\/ to possibly forwarded objects.\n+  if (!UseCompactObjectHeaders && !Metaspace::contains(object->klass_raw())) {\n@@ -403,0 +405,7 @@\n+\/\/ Returns the header size in words aligned to the requirements of the\n+\/\/ array object type.\n+static int int_array_header_size() {\n+  size_t typesize_in_bytes = arrayOopDesc::header_size_in_bytes();\n+  return (int)align_up(typesize_in_bytes, HeapWordSize)\/HeapWordSize;\n+}\n+\n@@ -412,1 +421,1 @@\n-  size_t max_int_size = typeArrayOopDesc::header_size(T_INT) +\n+  size_t max_int_size = int_array_header_size() +\n@@ -419,1 +428,1 @@\n-  return align_object_offset(arrayOopDesc::header_size(T_INT)); \/\/ align to Long\n+  return align_object_offset(int_array_header_size()); \/\/ align to Long\n","filename":"src\/hotspot\/share\/gc\/shared\/collectedHeap.cpp","additions":12,"deletions":3,"binary":false,"changes":15,"status":"modified"},{"patch":"@@ -310,1 +310,1 @@\n-  static constexpr size_t min_dummy_object_size() {\n+  static size_t min_dummy_object_size() {\n","filename":"src\/hotspot\/share\/gc\/shared\/collectedHeap.hpp","additions":1,"deletions":1,"binary":false,"changes":2,"status":"modified"},{"patch":"@@ -58,0 +58,1 @@\n+#include \"gc\/shared\/slidingForwarding.hpp\"\n@@ -136,0 +137,2 @@\n+  SlidingForwarding::initialize(_reserved, SpaceAlignment \/ HeapWordSize);\n+\n","filename":"src\/hotspot\/share\/gc\/shared\/genCollectedHeap.cpp","additions":3,"deletions":0,"binary":false,"changes":3,"status":"modified"},{"patch":"@@ -380,1 +380,3 @@\n-  oopDesc::set_klass_gap(mem, 0);\n+  if (!UseCompactObjectHeaders) {\n+    oopDesc::set_klass_gap(mem, 0);\n+  }\n@@ -386,2 +388,0 @@\n-  \/\/ May be bootstrapping\n-  oopDesc::set_mark(mem, markWord::prototype());\n@@ -391,1 +391,6 @@\n-  oopDesc::release_set_klass(mem, _klass);\n+  if (UseCompactObjectHeaders) {\n+    oopDesc::release_set_mark(mem, _klass->prototype_header());\n+  } else {\n+    oopDesc::set_mark(mem, markWord::prototype());\n+    oopDesc::release_set_klass(mem, _klass);\n+  }\n","filename":"src\/hotspot\/share\/gc\/shared\/memAllocator.cpp","additions":9,"deletions":4,"binary":false,"changes":13,"status":"modified"},{"patch":"@@ -31,0 +31,1 @@\n+#include \"gc\/shared\/slidingForwarding.inline.hpp\"\n@@ -224,0 +225,2 @@\n+    SlidingForwarding::begin();\n+\n@@ -237,0 +240,1 @@\n+    SlidingForwarding::end();\n@@ -298,0 +302,1 @@\n+template <bool ALT_FWD>\n@@ -367,1 +372,1 @@\n-      p->forward_to(cast_to_oop(_compact_point));\n+      SlidingForwarding::forward_to<ALT_FWD>(p, cast_to_oop(_compact_point));\n@@ -399,0 +404,10 @@\n+    if (UseAltGCForwarding) {\n+      work_impl<true>(worker_id);\n+    } else {\n+      work_impl<false>(worker_id);\n+    }\n+  }\n+\n+private:\n+  template <bool ALT_FWD>\n+  void work_impl(uint worker_id) {\n@@ -414,1 +429,1 @@\n-    ShenandoahPrepareForCompactionObjectClosure cl(_preserved_marks->get(worker_id), empty_regions, from_region);\n+    ShenandoahPrepareForCompactionObjectClosure<ALT_FWD> cl(_preserved_marks->get(worker_id), empty_regions, from_region);\n@@ -440,1 +455,2 @@\n-void ShenandoahFullGC::calculate_target_humongous_objects() {\n+template <bool ALT_FWD>\n+void ShenandoahFullGC::calculate_target_humongous_objects_impl() {\n@@ -476,1 +492,1 @@\n-        old_obj->forward_to(cast_to_oop(heap->get_region(start)->bottom()));\n+        SlidingForwarding::forward_to<ALT_FWD>(old_obj, cast_to_oop(heap->get_region(start)->bottom()));\n@@ -488,0 +504,8 @@\n+void ShenandoahFullGC::calculate_target_humongous_objects() {\n+  if (UseAltGCForwarding) {\n+    calculate_target_humongous_objects_impl<true>();\n+  } else {\n+    calculate_target_humongous_objects_impl<false>();\n+  }\n+}\n+\n@@ -725,0 +749,1 @@\n+template <bool ALT_FWD>\n@@ -736,2 +761,2 @@\n-      if (obj->is_forwarded()) {\n-        oop forw = obj->forwardee();\n+      if (SlidingForwarding::is_forwarded(obj)) {\n+        oop forw = SlidingForwarding::forwardee<ALT_FWD>(obj);\n@@ -754,0 +779,1 @@\n+template <bool ALT_FWD>\n@@ -757,1 +783,1 @@\n-  ShenandoahAdjustPointersClosure _cl;\n+  ShenandoahAdjustPointersClosure<ALT_FWD> _cl;\n@@ -780,1 +806,3 @@\n-  void work(uint worker_id) {\n+private:\n+  template <bool ALT_FWD>\n+  void work_impl(uint worker_id) {\n@@ -782,1 +810,1 @@\n-    ShenandoahAdjustPointersObjectClosure obj_cl;\n+    ShenandoahAdjustPointersObjectClosure<ALT_FWD> obj_cl;\n@@ -791,0 +819,9 @@\n+\n+public:\n+  void work(uint worker_id) {\n+    if (UseAltGCForwarding) {\n+      work_impl<true>(worker_id);\n+    } else {\n+      work_impl<false>(worker_id);\n+    }\n+  }\n@@ -797,0 +834,1 @@\n+\n@@ -803,1 +841,3 @@\n-  void work(uint worker_id) {\n+private:\n+  template <bool ALT_FWD>\n+  void work_impl(uint worker_id) {\n@@ -805,1 +845,1 @@\n-    ShenandoahAdjustPointersClosure cl;\n+    ShenandoahAdjustPointersClosure<ALT_FWD> cl;\n@@ -809,0 +849,9 @@\n+\n+public:\n+  void work(uint worker_id) {\n+    if (UseAltGCForwarding) {\n+      work_impl<true>(worker_id);\n+    } else {\n+      work_impl<false>(worker_id);\n+    }\n+  }\n@@ -835,0 +884,1 @@\n+template <bool ALT_FWD>\n@@ -847,1 +897,1 @@\n-    if (p->is_forwarded()) {\n+    if (SlidingForwarding::is_forwarded(p)) {\n@@ -849,1 +899,1 @@\n-      HeapWord* compact_to = cast_from_oop<HeapWord*>(p->forwardee());\n+      HeapWord* compact_to = cast_from_oop<HeapWord*>(SlidingForwarding::forwardee<ALT_FWD>(p));\n@@ -872,1 +922,3 @@\n-  void work(uint worker_id) {\n+private:\n+  template <bool ALT_FWD>\n+  void work_impl(uint worker_id) {\n@@ -876,1 +928,1 @@\n-    ShenandoahCompactObjectsClosure cl(worker_id);\n+    ShenandoahCompactObjectsClosure<ALT_FWD> cl(worker_id);\n@@ -887,0 +939,9 @@\n+\n+public:\n+  void work(uint worker_id) {\n+    if (UseAltGCForwarding) {\n+      work_impl<true>(worker_id);\n+    } else {\n+      work_impl<false>(worker_id);\n+    }\n+  }\n@@ -942,1 +1003,2 @@\n-void ShenandoahFullGC::compact_humongous_objects() {\n+template <bool ALT_FWD>\n+void ShenandoahFullGC::compact_humongous_objects_impl() {\n@@ -955,1 +1017,1 @@\n-      if (!old_obj->is_forwarded()) {\n+      if (SlidingForwarding::is_not_forwarded(old_obj)) {\n@@ -964,1 +1026,1 @@\n-      size_t new_start = heap->heap_region_index_containing(old_obj->forwardee());\n+      size_t new_start = heap->heap_region_index_containing(SlidingForwarding::forwardee<ALT_FWD>(old_obj));\n@@ -1005,0 +1067,8 @@\n+void ShenandoahFullGC::compact_humongous_objects() {\n+  if (UseAltGCForwarding) {\n+    compact_humongous_objects_impl<true>();\n+  } else {\n+    compact_humongous_objects_impl<false>();\n+  }\n+}\n+\n","filename":"src\/hotspot\/share\/gc\/shenandoah\/shenandoahFullGC.cpp","additions":88,"deletions":18,"binary":false,"changes":106,"status":"modified"},{"patch":"@@ -37,0 +37,1 @@\n+#include \"gc\/shared\/slidingForwarding.hpp\"\n@@ -407,0 +408,2 @@\n+  SlidingForwarding::initialize(_heap_region, ShenandoahHeapRegion::region_size_words());\n+\n","filename":"src\/hotspot\/share\/gc\/shenandoah\/shenandoahHeap.cpp","additions":3,"deletions":0,"binary":false,"changes":3,"status":"modified"},{"patch":"@@ -101,1 +101,1 @@\n-      if (is_instance_ref_klass(obj->klass())) {\n+      if (is_instance_ref_klass(obj->forward_safe_klass())) {\n@@ -128,1 +128,1 @@\n-    Klass* obj_klass = obj->klass_or_null();\n+    Klass* obj_klass = obj->forward_safe_klass();\n@@ -143,1 +143,1 @@\n-        check(ShenandoahAsserts::_safe_unknown, obj, (obj_addr + obj->size()) <= obj_reg->top(),\n+        check(ShenandoahAsserts::_safe_unknown, obj, (obj_addr + obj->forward_safe_size()) <= obj_reg->top(),\n@@ -147,1 +147,1 @@\n-        size_t humongous_end = humongous_start + (obj->size() >> ShenandoahHeapRegion::region_size_words_shift());\n+        size_t humongous_end = humongous_start + (obj->forward_safe_size() >> ShenandoahHeapRegion::region_size_words_shift());\n@@ -164,1 +164,1 @@\n-          Atomic::add(&_ld[obj_reg->index()], (uint) obj->size(), memory_order_relaxed);\n+          Atomic::add(&_ld[obj_reg->index()], (uint) obj->forward_safe_size(), memory_order_relaxed);\n@@ -205,1 +205,1 @@\n-      check(ShenandoahAsserts::_safe_oop, obj, (fwd_addr + fwd->size()) <= fwd_reg->top(),\n+      check(ShenandoahAsserts::_safe_oop, obj, (fwd_addr + fwd->forward_safe_size()) <= fwd_reg->top(),\n@@ -308,1 +308,2 @@\n-    obj->oop_iterate(this);\n+    Klass* klass = obj->forward_safe_klass();\n+    obj->oop_iterate_backwards(this, klass);\n@@ -588,1 +589,1 @@\n-    if (!is_instance_ref_klass(obj->klass())) {\n+    if (!is_instance_ref_klass(obj->forward_safe_klass())) {\n","filename":"src\/hotspot\/share\/gc\/shenandoah\/shenandoahVerifier.cpp","additions":9,"deletions":8,"binary":false,"changes":17,"status":"modified"},{"patch":"@@ -2000,4 +2000,7 @@\n-              oopDesc::set_mark(result, markWord::prototype());\n-              oopDesc::set_klass_gap(result, 0);\n-              oopDesc::release_set_klass(result, ik);\n-\n+              if (UseCompactObjectHeaders) {\n+                oopDesc::release_set_mark(result, ik->prototype_header());\n+              } else {\n+                oopDesc::set_mark(result, markWord::prototype());\n+                oopDesc::set_klass_gap(result, 0);\n+                oopDesc::release_set_klass(result, ik);\n+              }\n","filename":"src\/hotspot\/share\/interpreter\/zero\/bytecodeInterpreter.cpp","additions":7,"deletions":4,"binary":false,"changes":11,"status":"modified"},{"patch":"@@ -2389,1 +2389,1 @@\n-  return arrayOopDesc::header_size(type) * HeapWordSize;\n+  return arrayOopDesc::base_offset_in_bytes(type);\n","filename":"src\/hotspot\/share\/jvmci\/jvmciCompilerToVM.cpp","additions":1,"deletions":1,"binary":false,"changes":2,"status":"modified"},{"patch":"@@ -322,2 +322,7 @@\n-  assert(oopDesc::klass_offset_in_bytes() < static_cast<intptr_t>(os::vm_page_size()),\n-         \"Klass offset is expected to be less than the page size\");\n+  if (UseCompactObjectHeaders) {\n+    assert(oopDesc::mark_offset_in_bytes() < static_cast<intptr_t>(os::vm_page_size()),\n+           \"Mark offset is expected to be less than the page size\");\n+  } else {\n+    assert(oopDesc::klass_offset_in_bytes() < static_cast<intptr_t>(os::vm_page_size()),\n+           \"Klass offset is expected to be less than the page size\");\n+  }\n","filename":"src\/hotspot\/share\/memory\/universe.cpp","additions":7,"deletions":2,"binary":false,"changes":9,"status":"modified"},{"patch":"@@ -194,0 +194,10 @@\n+static markWord make_prototype(Klass* kls) {\n+  markWord prototype = markWord::prototype();\n+#ifdef _LP64\n+  if (UseCompactObjectHeaders) {\n+    prototype = prototype.set_klass(kls);\n+  }\n+#endif\n+  return prototype;\n+}\n+\n@@ -199,0 +209,1 @@\n+                           _prototype_header(make_prototype(this)),\n@@ -743,0 +754,4 @@\n+     if (UseCompactObjectHeaders) {\n+       st->print(BULLET\"prototype_header: \" INTPTR_FORMAT, _prototype_header.value());\n+       st->cr();\n+     }\n","filename":"src\/hotspot\/share\/oops\/klass.cpp","additions":15,"deletions":0,"binary":false,"changes":15,"status":"modified"},{"patch":"@@ -55,0 +55,5 @@\n+inline void Klass::set_prototype_header(markWord header) {\n+  assert(UseCompactObjectHeaders, \"only with compact headers\");\n+  _prototype_header = header;\n+}\n+\n","filename":"src\/hotspot\/share\/oops\/klass.inline.hpp","additions":5,"deletions":0,"binary":false,"changes":5,"status":"modified"},{"patch":"@@ -159,1 +159,2 @@\n-  return UseCompressedClassPointers;\n+  \/\/ Except when using compact headers.\n+  return UseCompressedClassPointers && !UseCompactObjectHeaders;\n@@ -171,1 +172,3 @@\n-  if (UseCompressedClassPointers) {\n+  if (UseCompactObjectHeaders) {\n+    return obj->klass();\n+  } else if (UseCompressedClassPointers) {\n","filename":"src\/hotspot\/share\/oops\/oop.cpp","additions":5,"deletions":2,"binary":false,"changes":7,"status":"modified"},{"patch":"@@ -82,0 +82,5 @@\n+  inline markWord resolve_mark() const;\n+\n+  \/\/ Returns the prototype mark that should be used for this object.\n+  inline markWord prototype_mark() const;\n+\n@@ -100,1 +105,7 @@\n-  static constexpr int header_size() { return sizeof(oopDesc)\/HeapWordSize; }\n+  static int header_size() {\n+    if (UseCompactObjectHeaders) {\n+      return sizeof(markWord) \/ HeapWordSize;\n+    } else {\n+      return sizeof(oopDesc)\/HeapWordSize;\n+    }\n+  }\n@@ -112,0 +123,14 @@\n+  \/\/ The following set of methods is used to access the mark-word and related\n+  \/\/ properties when the object may be forwarded. Be careful where and when\n+  \/\/ using this method. It assumes that the forwardee is installed in\n+  \/\/ the header as a plain pointer (or self-forwarded). In particular,\n+  \/\/ those methods can not deal with the sliding-forwarding that is used\n+  \/\/ in Serial, G1 and Shenandoah full-GCs.\n+private:\n+  inline Klass*   forward_safe_klass_impl(markWord m) const;\n+public:\n+  inline Klass*   forward_safe_klass() const;\n+  inline Klass*   forward_safe_klass(markWord m) const;\n+  inline size_t   forward_safe_size();\n+  inline void     forward_safe_init_mark();\n+\n@@ -264,0 +289,1 @@\n+  inline void forward_to_self();\n@@ -270,0 +296,1 @@\n+  inline oop forward_to_self_atomic(markWord compare, atomic_memory_order order = memory_order_conservative);\n@@ -272,0 +299,1 @@\n+  inline oop forwardee(markWord header) const;\n@@ -315,1 +343,11 @@\n-  static int klass_offset_in_bytes()     { return (int)offset_of(oopDesc, _metadata._klass); }\n+  static int klass_offset_in_bytes()     {\n+#ifdef _LP64\n+    if (UseCompactObjectHeaders) {\n+      STATIC_ASSERT(markWord::klass_shift % 8 == 0);\n+      return mark_offset_in_bytes() + markWord::klass_shift \/ 8;\n+    } else\n+#endif\n+    {\n+      return (int)offset_of(oopDesc, _metadata._klass);\n+    }\n+  }\n@@ -318,0 +356,1 @@\n+    assert(!UseCompactObjectHeaders, \"don't use klass_offset_in_bytes() with compact headers\");\n@@ -321,0 +360,16 @@\n+  static int base_offset_in_bytes() {\n+#ifdef _LP64\n+    if (UseCompactObjectHeaders) {\n+      \/\/ With compact headers, the Klass* field is not used for the Klass*\n+      \/\/ and is used for the object fields instead.\n+      STATIC_ASSERT(sizeof(markWord) == 8);\n+      return sizeof(markWord);\n+    } else if (UseCompressedClassPointers) {\n+      return sizeof(markWord) + sizeof(narrowKlass);\n+    } else\n+#endif\n+    {\n+      return sizeof(oopDesc);\n+    }\n+  }\n+\n","filename":"src\/hotspot\/share\/oops\/oop.hpp","additions":57,"deletions":2,"binary":false,"changes":59,"status":"modified"},{"patch":"@@ -37,1 +37,1 @@\n-#include \"oops\/markWord.hpp\"\n+#include \"oops\/markWord.inline.hpp\"\n@@ -41,0 +41,1 @@\n+#include \"runtime\/safepoint.hpp\"\n@@ -69,4 +70,0 @@\n-void oopDesc::release_set_mark(HeapWord* mem, markWord m) {\n-  Atomic::release_store((markWord*)(((char*)mem) + mark_offset_in_bytes()), m);\n-}\n-\n@@ -77,0 +74,4 @@\n+void oopDesc::release_set_mark(HeapWord* mem, markWord m) {\n+  Atomic::release_store((markWord*)(((char*)mem) + mark_offset_in_bytes()), m);\n+}\n+\n@@ -85,0 +86,17 @@\n+markWord oopDesc::resolve_mark() const {\n+  assert(LockingMode != LM_LEGACY, \"Not safe with legacy stack-locking\");\n+  markWord m = mark();\n+  if (m.has_displaced_mark_helper()) {\n+    m = m.displaced_mark_helper();\n+  }\n+  return m;\n+}\n+\n+markWord oopDesc::prototype_mark() const {\n+  if (UseCompactObjectHeaders) {\n+    return klass()->prototype_header();\n+  } else {\n+    return markWord::prototype();\n+  }\n+}\n+\n@@ -86,1 +104,5 @@\n-  set_mark(markWord::prototype());\n+  if (UseCompactObjectHeaders) {\n+    set_mark(klass()->prototype_header());\n+  } else {\n+    set_mark(markWord::prototype());\n+  }\n@@ -90,1 +112,5 @@\n-  if (UseCompressedClassPointers) {\n+#ifdef _LP64\n+  if (UseCompactObjectHeaders) {\n+    markWord m = resolve_mark();\n+    return m.klass();\n+  } else if (UseCompressedClassPointers) {\n@@ -92,1 +118,3 @@\n-  } else {\n+  } else\n+#endif\n+  {\n@@ -98,1 +126,5 @@\n-  if (UseCompressedClassPointers) {\n+#ifdef _LP64\n+  if (UseCompactObjectHeaders) {\n+    markWord m = resolve_mark();\n+    return m.klass_or_null();\n+  } else if (UseCompressedClassPointers) {\n@@ -100,1 +132,3 @@\n-  } else {\n+  } else\n+#endif\n+  {\n@@ -106,4 +140,13 @@\n-  if (UseCompressedClassPointers) {\n-    narrowKlass nklass = Atomic::load_acquire(&_metadata._compressed_klass);\n-    return CompressedKlassPointers::decode(nklass);\n-  } else {\n+#ifdef _LP64\n+  if (UseCompactObjectHeaders) {\n+    markWord m = mark_acquire();\n+    if (m.has_displaced_mark_helper()) {\n+      m = m.displaced_mark_helper();\n+    }\n+    return m.klass_or_null();\n+  } else if (UseCompressedClassPointers) {\n+     narrowKlass nklass = Atomic::load_acquire(&_metadata._compressed_klass);\n+     return CompressedKlassPointers::decode(nklass);\n+  } else\n+#endif\n+  {\n@@ -115,1 +158,3 @@\n-  if (UseCompressedClassPointers) {\n+  if (UseCompactObjectHeaders) {\n+    return klass();\n+  } else if (UseCompressedClassPointers) {\n@@ -124,0 +169,1 @@\n+  assert(!UseCompactObjectHeaders, \"don't set Klass* with compact headers\");\n@@ -133,0 +179,1 @@\n+  assert(!UseCompactObjectHeaders, \"don't set Klass* with compact headers\");\n@@ -143,0 +190,1 @@\n+  assert(!UseCompactObjectHeaders, \"don't set Klass* gap with compact headers\");\n@@ -205,0 +253,47 @@\n+#ifdef _LP64\n+Klass* oopDesc::forward_safe_klass_impl(markWord m) const {\n+  assert(UseCompactObjectHeaders, \"Only get here with compact headers\");\n+  if (m.is_marked()) {\n+    oop fwd = forwardee(m);\n+    markWord m2 = fwd->mark();\n+    assert(!m2.is_marked() || m2.self_forwarded(), \"no double forwarding: this: \" PTR_FORMAT \" (\" INTPTR_FORMAT \"), fwd: \" PTR_FORMAT \" (\" INTPTR_FORMAT \")\", p2i(this), m.value(), p2i(fwd), m2.value());\n+    m = m2;\n+  }\n+  return m.actual_mark().klass();\n+}\n+#endif\n+\n+Klass* oopDesc::forward_safe_klass(markWord m) const {\n+#ifdef _LP64\n+  if (UseCompactObjectHeaders) {\n+    return forward_safe_klass_impl(m);\n+  } else\n+#endif\n+  {\n+    return klass();\n+  }\n+}\n+\n+Klass* oopDesc::forward_safe_klass() const {\n+#ifdef _LP64\n+  if (UseCompactObjectHeaders) {\n+    return forward_safe_klass_impl(mark());\n+  } else\n+#endif\n+  {\n+    return klass();\n+  }\n+}\n+\n+size_t oopDesc::forward_safe_size() {\n+  return size_given_klass(forward_safe_klass());\n+}\n+\n+void oopDesc::forward_safe_init_mark() {\n+  if (UseCompactObjectHeaders) {\n+    set_mark(forward_safe_klass()->prototype_header());\n+  } else {\n+    set_mark(markWord::prototype());\n+  }\n+}\n+\n@@ -275,0 +370,1 @@\n+  assert(p != cast_to_oop(this) || !UseAltGCForwarding, \"Must not be called with self-forwarding\");\n@@ -276,1 +372,1 @@\n-  assert(m.decode_pointer() == p, \"encoding must be reversible\");\n+  assert(forwardee(m) == p, \"encoding must be reversible\");\n@@ -280,0 +376,20 @@\n+void oopDesc::forward_to_self() {\n+#ifdef _LP64\n+  if (UseAltGCForwarding) {\n+    markWord m = mark();\n+    \/\/ If mark is displaced, we need to preserve the real header during GC.\n+    \/\/ It will be restored to the displaced header after GC.\n+    assert(SafepointSynchronize::is_at_safepoint(), \"we can only safely fetch the displaced header at safepoint\");\n+    if (m.has_displaced_mark_helper()) {\n+      m = m.displaced_mark_helper();\n+    }\n+    m = m.set_self_forwarded();\n+    assert(forwardee(m) == cast_to_oop(this), \"encoding must be reversible\");\n+    set_mark(m);\n+  } else\n+#endif\n+  {\n+    forward_to(oop(this));\n+  }\n+}\n+\n@@ -281,0 +397,1 @@\n+  assert(p != cast_to_oop(this) || !UseAltGCForwarding, \"Must not be called with self-forwarding\");\n@@ -287,1 +404,39 @@\n-    return cast_to_oop(old_mark.decode_pointer());\n+    return forwardee(old_mark);\n+  }\n+}\n+\n+oop oopDesc::forward_to_self_atomic(markWord compare, atomic_memory_order order) {\n+#ifdef _LP64\n+  if (UseAltGCForwarding) {\n+    markWord m = compare;\n+    \/\/ If mark is displaced, we need to preserve the real header during GC.\n+    \/\/ It will be restored to the displaced header after GC.\n+    assert(SafepointSynchronize::is_at_safepoint(), \"we can only safely fetch the displaced header at safepoint\");\n+    if (m.has_displaced_mark_helper()) {\n+      m = m.displaced_mark_helper();\n+    }\n+    m = m.set_self_forwarded();\n+    assert(forwardee(m) == cast_to_oop(this), \"encoding must be reversible\");\n+    markWord old_mark = cas_set_mark(m, compare, order);\n+    if (old_mark == compare) {\n+      return nullptr;\n+    } else {\n+      assert(old_mark.is_marked(), \"must be marked here\");\n+      return forwardee(old_mark);\n+    }\n+  } else\n+#endif\n+  {\n+    return forward_to_atomic(cast_to_oop(this), compare, order);\n+  }\n+}\n+\n+oop oopDesc::forwardee(markWord header) const {\n+  assert(header.is_marked(), \"only decode when actually forwarded\");\n+#ifdef _LP64\n+  if (header.self_forwarded()) {\n+    return cast_to_oop(this);\n+  } else\n+#endif\n+  {\n+    return cast_to_oop(header.decode_pointer());\n@@ -295,2 +450,1 @@\n-  assert(is_forwarded(), \"only decode when actually forwarded\");\n-  return cast_to_oop(mark().decode_pointer());\n+  return forwardee(mark());\n@@ -351,1 +505,2 @@\n-  assert(k == klass(), \"wrong klass\");\n+  \/\/ In this assert, we cannot safely access the Klass* with compact headers.\n+  assert(UseCompactObjectHeaders || k == klass(), \"wrong klass\");\n","filename":"src\/hotspot\/share\/oops\/oop.inline.hpp","additions":175,"deletions":20,"binary":false,"changes":195,"status":"modified"},{"patch":"@@ -1578,2 +1578,8 @@\n-  \/\/ For now only enable fast locking for non-array types\n-  mark_node = phase->MakeConX(markWord::prototype().value());\n+  if (UseCompactObjectHeaders) {\n+    Node* klass_node = in(AllocateNode::KlassNode);\n+    Node* proto_adr = phase->transform(new AddPNode(klass_node, klass_node, phase->MakeConX(in_bytes(Klass::prototype_header_offset()))));\n+    mark_node = LoadNode::make(*phase, control, mem, proto_adr, TypeRawPtr::BOTTOM, TypeX_X, TypeX_X->basic_type(), MemNode::unordered);\n+  } else {\n+    \/\/ For now only enable fast locking for non-array types\n+    mark_node = phase->MakeConX(markWord::prototype().value());\n+  }\n","filename":"src\/hotspot\/share\/opto\/callnode.cpp","additions":8,"deletions":2,"binary":false,"changes":10,"status":"modified"},{"patch":"@@ -1693,0 +1693,4 @@\n+      if (UseCompactObjectHeaders) {\n+        if (flat->offset() == in_bytes(Klass::prototype_header_offset()))\n+          alias_type(idx)->set_rewritable(false);\n+      }\n","filename":"src\/hotspot\/share\/opto\/compile.cpp","additions":4,"deletions":0,"binary":false,"changes":4,"status":"modified"},{"patch":"@@ -4543,2 +4543,2 @@\n-  Node *hash_mask      = _gvn.intcon(markWord::hash_mask);\n-  Node *hash_shift     = _gvn.intcon(markWord::hash_shift);\n+  Node *hash_mask      = _gvn.intcon(UseCompactObjectHeaders ? markWord::hash_mask_compact  : markWord::hash_mask);\n+  Node *hash_shift     = _gvn.intcon(UseCompactObjectHeaders ? markWord::hash_shift_compact : markWord::hash_shift);\n","filename":"src\/hotspot\/share\/opto\/library_call.cpp","additions":2,"deletions":2,"binary":false,"changes":4,"status":"modified"},{"patch":"@@ -1664,1 +1664,3 @@\n-  rawmem = make_store(control, rawmem, object, oopDesc::klass_offset_in_bytes(), klass_node, T_METADATA);\n+  if (!UseCompactObjectHeaders) {\n+    rawmem = make_store(control, rawmem, object, oopDesc::klass_offset_in_bytes(), klass_node, T_METADATA);\n+  }\n","filename":"src\/hotspot\/share\/opto\/macro.cpp","additions":3,"deletions":1,"binary":false,"changes":4,"status":"modified"},{"patch":"@@ -1888,0 +1888,7 @@\n+  if (UseCompactObjectHeaders) {\n+    if (tkls->offset() == in_bytes(Klass::prototype_header_offset())) {\n+      \/\/ The field is Klass::_prototype_header.  Return its (constant) value.\n+      assert(this->Opcode() == Op_LoadX, \"must load a proper type from _prototype_header\");\n+      return TypeX::make(klass->prototype_header());\n+    }\n+  }\n@@ -2060,0 +2067,7 @@\n+      if (UseCompactObjectHeaders) {\n+        if (tkls->offset() == in_bytes(Klass::prototype_header_offset())) {\n+          \/\/ The field is Klass::_prototype_header. Return its (constant) value.\n+          assert(this->Opcode() == Op_LoadX, \"must load a proper type from _prototype_header\");\n+          return TypeX::make(klass->prototype_header());\n+        }\n+      }\n@@ -2150,1 +2164,1 @@\n-  if (alloc != nullptr) {\n+  if (!UseCompactObjectHeaders && alloc != nullptr) {\n","filename":"src\/hotspot\/share\/opto\/memnode.cpp","additions":15,"deletions":1,"binary":false,"changes":16,"status":"modified"},{"patch":"@@ -323,3 +323,2 @@\n-    const size_t hs = arrayOopDesc::header_size(elem_type);\n-    \/\/ Align to next 8 bytes to avoid trashing arrays's length.\n-    const size_t aligned_hs = align_object_offset(hs);\n+    size_t hs_bytes = arrayOopDesc::base_offset_in_bytes(elem_type);\n+    assert(is_aligned(hs_bytes, BytesPerInt), \"must be 4 byte aligned\");\n@@ -327,2 +326,3 @@\n-    if (aligned_hs > hs) {\n-      Copy::zero_to_words(obj+hs, aligned_hs-hs);\n+    if (!is_aligned(hs_bytes, BytesPerLong)) {\n+      *reinterpret_cast<jint*>(reinterpret_cast<char*>(obj) + hs_bytes) = 0;\n+      hs_bytes += BytesPerInt;\n@@ -330,0 +330,1 @@\n+\n@@ -331,0 +332,2 @@\n+    assert(is_aligned(hs_bytes, BytesPerLong), \"must be 8-byte aligned\");\n+    const size_t aligned_hs = hs_bytes \/ BytesPerLong;\n","filename":"src\/hotspot\/share\/opto\/runtime.cpp","additions":8,"deletions":5,"binary":false,"changes":13,"status":"modified"},{"patch":"@@ -5175,1 +5175,2 @@\n-    int header_size = objArrayOopDesc::header_size() * wordSize;\n+    BasicType basic_elem_type = elem()->basic_type();\n+    int header_size = arrayOopDesc::base_offset_in_bytes(basic_elem_type);\n@@ -5180,1 +5181,0 @@\n-      BasicType basic_elem_type = elem()->basic_type();\n","filename":"src\/hotspot\/share\/opto\/type.cpp","additions":2,"deletions":2,"binary":false,"changes":4,"status":"modified"},{"patch":"@@ -71,1 +71,1 @@\n-  ( arrayOopDesc::header_size(T_DOUBLE) * HeapWordSize \\\n+  ( arrayOopDesc::base_offset_in_bytes(T_DOUBLE) \\\n","filename":"src\/hotspot\/share\/prims\/unsafe.cpp","additions":1,"deletions":1,"binary":false,"changes":2,"status":"modified"},{"patch":"@@ -85,0 +85,1 @@\n+#include \"runtime\/lockStack.hpp\"\n@@ -1834,0 +1835,8 @@\n+WB_ENTRY(jint, WB_getLockStackCapacity(JNIEnv* env))\n+  return (jint) LockStack::CAPACITY;\n+WB_END\n+\n+WB_ENTRY(jboolean, WB_supportsRecursiveLightweightLocking(JNIEnv* env))\n+  return (jboolean) VM_Version::supports_recursive_lightweight_locking();\n+WB_END\n+\n@@ -2753,0 +2762,2 @@\n+  {CC\"getLockStackCapacity\", CC\"()I\",                 (void*)&WB_getLockStackCapacity },\n+  {CC\"supportsRecursiveLightweightLocking\", CC\"()Z\",  (void*)&WB_supportsRecursiveLightweightLocking },\n","filename":"src\/hotspot\/share\/prims\/whitebox.cpp","additions":11,"deletions":0,"binary":false,"changes":11,"status":"modified"},{"patch":"@@ -393,0 +393,1 @@\n+  DEBUG_ONLY(GrowableArray<oop> lock_order{0};)\n@@ -402,0 +403,7 @@\n+#ifdef ASSERT\n+      if (LockingMode == LM_LIGHTWEIGHT && !realloc_failures) {\n+        for (MonitorInfo* mi : *monitors) {\n+          lock_order.push(mi->owner());\n+        }\n+      }\n+#endif \/\/ ASSERT\n@@ -433,0 +441,5 @@\n+#ifdef ASSERT\n+  if (LockingMode == LM_LIGHTWEIGHT && !realloc_failures) {\n+    deoptee_thread->lock_stack().verify_consistent_lock_order(lock_order, exec_mode != Deoptimization::Unpack_none);\n+  }\n+#endif \/\/ ASSERT\n@@ -1657,1 +1670,1 @@\n-        if (LockingMode == LM_LIGHTWEIGHT && exec_mode == Unpack_none) {\n+        if (LockingMode == LM_LIGHTWEIGHT) {\n@@ -1661,1 +1674,1 @@\n-          ObjectSynchronizer::enter(obj, nullptr, deoptee_thread);\n+          ObjectSynchronizer::enter_for(obj, nullptr, deoptee_thread);\n@@ -1663,1 +1676,1 @@\n-          ObjectMonitor* mon = ObjectSynchronizer::inflate(deoptee_thread, obj(), ObjectSynchronizer::inflate_cause_vm_internal);\n+          ObjectMonitor* mon = ObjectSynchronizer::inflate_for(deoptee_thread, obj(), ObjectSynchronizer::inflate_cause_vm_internal);\n@@ -1667,1 +1680,1 @@\n-          ObjectSynchronizer::enter(obj, lock, deoptee_thread);\n+          ObjectSynchronizer::enter_for(obj, lock, deoptee_thread);\n","filename":"src\/hotspot\/share\/runtime\/deoptimization.cpp","additions":17,"deletions":4,"binary":false,"changes":21,"status":"modified"},{"patch":"@@ -133,0 +133,3 @@\n+  product(bool, UseCompactObjectHeaders, false, EXPERIMENTAL,               \\\n+          \"Use compact 64-bit object headers in 64-bit VM\")                 \\\n+                                                                            \\\n@@ -150,0 +153,1 @@\n+const bool UseCompactObjectHeaders = false;\n","filename":"src\/hotspot\/share\/runtime\/globals.hpp","additions":4,"deletions":0,"binary":false,"changes":4,"status":"modified"},{"patch":"@@ -2,1 +2,1 @@\n- * Copyright (c) 1998, 2023, Oracle and\/or its affiliates. All rights reserved.\n+ * Copyright (c) 1998, 2024, Oracle and\/or its affiliates. All rights reserved.\n@@ -42,0 +42,1 @@\n+#include \"runtime\/globals.hpp\"\n@@ -56,0 +57,1 @@\n+#include \"utilities\/globalDefinitions.hpp\"\n@@ -315,0 +317,62 @@\n+bool ObjectMonitor::enter_for(JavaThread* locking_thread) {\n+  \/\/ Used by ObjectSynchronizer::enter_for to enter for another thread.\n+  \/\/ The monitor is private to or already owned by locking_thread which must be suspended.\n+  \/\/ So this code may only contend with deflation.\n+  assert(locking_thread == Thread::current() || locking_thread->is_obj_deopt_suspend(), \"must be\");\n+\n+  \/\/ Block out deflation as soon as possible.\n+  add_to_contentions(1);\n+\n+  bool success = false;\n+  if (!is_being_async_deflated()) {\n+    void* prev_owner = try_set_owner_from(nullptr, locking_thread);\n+\n+    if (prev_owner == nullptr) {\n+      assert(_recursions == 0, \"invariant\");\n+      success = true;\n+    } else if (prev_owner == locking_thread) {\n+      _recursions++;\n+      success = true;\n+    } else if (prev_owner == DEFLATER_MARKER) {\n+      \/\/ Racing with deflation.\n+      prev_owner = try_set_owner_from(DEFLATER_MARKER, locking_thread);\n+      if (prev_owner == DEFLATER_MARKER) {\n+        \/\/ Cancelled deflation. Increment contentions as part of the deflation protocol.\n+        add_to_contentions(1);\n+        success = true;\n+      } else if (prev_owner == nullptr) {\n+        \/\/ At this point we cannot race with deflation as we have both incremented\n+        \/\/ contentions, seen contention > 0 and seen a DEFLATER_MARKER.\n+        \/\/ success will only be false if this races with something other than\n+        \/\/ deflation.\n+        prev_owner = try_set_owner_from(nullptr, locking_thread);\n+        success = prev_owner == nullptr;\n+      }\n+    } else if (LockingMode == LM_LEGACY && locking_thread->is_lock_owned((address)prev_owner)) {\n+      assert(_recursions == 0, \"must be\");\n+      _recursions = 1;\n+      set_owner_from_BasicLock(prev_owner, locking_thread);\n+      success = true;\n+    }\n+    assert(success, \"Failed to enter_for: locking_thread=\" INTPTR_FORMAT\n+           \", this=\" INTPTR_FORMAT \"{owner=\" INTPTR_FORMAT \"}, observed owner: \" INTPTR_FORMAT,\n+           p2i(locking_thread), p2i(this), p2i(owner_raw()), p2i(prev_owner));\n+  } else {\n+    \/\/ Async deflation is in progress and our contentions increment\n+    \/\/ above lost the race to async deflation. Undo the work and\n+    \/\/ force the caller to retry.\n+    const oop l_object = object();\n+    if (l_object != nullptr) {\n+      \/\/ Attempt to restore the header\/dmw to the object's header so that\n+      \/\/ we only retry once if the deflater thread happens to be slow.\n+      install_displaced_markword_in_object(l_object);\n+    }\n+  }\n+\n+  add_to_contentions(-1);\n+\n+  assert(!success || owner_raw() == locking_thread, \"must be\");\n+\n+  return success;\n+}\n+\n@@ -316,0 +380,1 @@\n+  assert(current == JavaThread::current(), \"must be\");\n","filename":"src\/hotspot\/share\/runtime\/objectMonitor.cpp","additions":66,"deletions":1,"binary":false,"changes":67,"status":"modified"},{"patch":"@@ -3260,6 +3260,11 @@\n-      \/\/ Inflate so the object's header no longer refers to the BasicLock.\n-      if (lock->displaced_header().is_unlocked()) {\n-        \/\/ The object is locked and the resulting ObjectMonitor* will also be\n-        \/\/ locked so it can't be async deflated until ownership is dropped.\n-        \/\/ See the big comment in basicLock.cpp: BasicLock::move_to().\n-        ObjectSynchronizer::inflate_helper(kptr2->obj());\n+      if (LockingMode == LM_LEGACY) {\n+        \/\/ Inflate so the object's header no longer refers to the BasicLock.\n+        if (lock->displaced_header().is_unlocked()) {\n+          \/\/ The object is locked and the resulting ObjectMonitor* will also be\n+          \/\/ locked so it can't be async deflated until ownership is dropped.\n+          \/\/ See the big comment in basicLock.cpp: BasicLock::move_to().\n+          ObjectSynchronizer::inflate_helper(kptr2->obj());\n+        }\n+        \/\/ Now the displaced header is free to move because the\n+        \/\/ object's header no longer refers to it.\n+        buf[i] = (intptr_t)lock->displaced_header().value();\n@@ -3267,3 +3272,6 @@\n-      \/\/ Now the displaced header is free to move because the\n-      \/\/ object's header no longer refers to it.\n-      buf[i++] = (intptr_t)lock->displaced_header().value();\n+#ifdef ASSERT\n+      else {\n+        buf[i] = badDispHeaderOSR;\n+      }\n+#endif\n+      i++;\n","filename":"src\/hotspot\/share\/runtime\/sharedRuntime.cpp","additions":17,"deletions":9,"binary":false,"changes":26,"status":"modified"},{"patch":"@@ -2,1 +2,1 @@\n- * Copyright (c) 1998, 2023, Oracle and\/or its affiliates. All rights reserved.\n+ * Copyright (c) 1998, 2024, Oracle and\/or its affiliates. All rights reserved.\n@@ -39,0 +39,1 @@\n+#include \"runtime\/globals.hpp\"\n@@ -63,0 +64,1 @@\n+#include \"utilities\/globalDefinitions.hpp\"\n@@ -387,0 +389,13 @@\n+  if (LockingMode == LM_LIGHTWEIGHT) {\n+    LockStack& lock_stack = current->lock_stack();\n+    if (lock_stack.is_full()) {\n+      \/\/ Always go into runtime if the lock stack is full.\n+      return false;\n+    }\n+    if (lock_stack.try_recursive_enter(obj)) {\n+      \/\/ Recursive lock successful.\n+      current->inc_held_monitor_count();\n+      return true;\n+    }\n+  }\n+\n@@ -440,2 +455,3 @@\n-void ObjectSynchronizer::handle_sync_on_value_based_class(Handle obj, JavaThread* current) {\n-  frame last_frame = current->last_frame();\n+void ObjectSynchronizer::handle_sync_on_value_based_class(Handle obj, JavaThread* locking_thread) {\n+  assert(locking_thread == Thread::current() || locking_thread->is_obj_deopt_suspend(), \"must be\");\n+  frame last_frame = locking_thread->last_frame();\n@@ -454,1 +470,1 @@\n-    ResourceMark rm(current);\n+    ResourceMark rm;\n@@ -456,1 +472,1 @@\n-    current->print_active_stack_on(&ss);\n+    locking_thread->print_active_stack_on(&ss);\n@@ -465,1 +481,1 @@\n-    ResourceMark rm(current);\n+    ResourceMark rm;\n@@ -469,1 +485,1 @@\n-    if (current->has_last_Java_frame()) {\n+    if (locking_thread->has_last_Java_frame()) {\n@@ -471,1 +487,1 @@\n-      current->print_active_stack_on(&info_stream);\n+      locking_thread->print_active_stack_on(&info_stream);\n@@ -498,0 +514,39 @@\n+\n+void ObjectSynchronizer::enter_for(Handle obj, BasicLock* lock, JavaThread* locking_thread) {\n+  \/\/ When called with locking_thread != Thread::current() some mechanism must synchronize\n+  \/\/ the locking_thread with respect to the current thread. Currently only used when\n+  \/\/ deoptimizing and re-locking locks. See Deoptimization::relock_objects\n+  assert(locking_thread == Thread::current() || locking_thread->is_obj_deopt_suspend(), \"must be\");\n+  if (!enter_fast_impl(obj, lock, locking_thread)) {\n+    \/\/ Inflated ObjectMonitor::enter_for is required\n+\n+    \/\/ An async deflation can race after the inflate_for() call and before\n+    \/\/ enter_for() can make the ObjectMonitor busy. enter_for() returns false\n+    \/\/ if we have lost the race to async deflation and we simply try again.\n+    while (true) {\n+      ObjectMonitor* monitor = inflate_for(locking_thread, obj(), inflate_cause_monitor_enter);\n+      if (monitor->enter_for(locking_thread)) {\n+        return;\n+      }\n+      assert(monitor->is_being_async_deflated(), \"must be\");\n+    }\n+  }\n+}\n+\n+void ObjectSynchronizer::enter(Handle obj, BasicLock* lock, JavaThread* current) {\n+  assert(current == Thread::current(), \"must be\");\n+  if (!enter_fast_impl(obj, lock, current)) {\n+    \/\/ Inflated ObjectMonitor::enter is required\n+\n+    \/\/ An async deflation can race after the inflate() call and before\n+    \/\/ enter() can make the ObjectMonitor busy. enter() returns false if\n+    \/\/ we have lost the race to async deflation and we simply try again.\n+    while (true) {\n+      ObjectMonitor* monitor = inflate(current, obj(), inflate_cause_monitor_enter);\n+      if (monitor->enter(current)) {\n+        return;\n+      }\n+    }\n+  }\n+}\n+\n@@ -501,0 +556,1 @@\n+bool ObjectSynchronizer::enter_fast_impl(Handle obj, BasicLock* lock, JavaThread* locking_thread) {\n@@ -502,2 +558,1 @@\n-void ObjectSynchronizer::enter(Handle obj, BasicLock* lock, JavaThread* current) {\n-    handle_sync_on_value_based_class(obj, current);\n+    handle_sync_on_value_based_class(obj, locking_thread);\n@@ -507,1 +562,1 @@\n-  current->inc_held_monitor_count();\n+  locking_thread->inc_held_monitor_count();\n@@ -512,15 +567,41 @@\n-      LockStack& lock_stack = current->lock_stack();\n-      if (lock_stack.can_push()) {\n-        markWord mark = obj()->mark_acquire();\n-        while (mark.is_neutral()) {\n-          \/\/ Retry until a lock state change has been observed.  cas_set_mark() may collide with non lock bits modifications.\n-          \/\/ Try to swing into 'fast-locked' state.\n-          assert(!lock_stack.contains(obj()), \"thread must not already hold the lock\");\n-          const markWord locked_mark = mark.set_fast_locked();\n-          const markWord old_mark = obj()->cas_set_mark(locked_mark, mark);\n-          if (old_mark == mark) {\n-            \/\/ Successfully fast-locked, push object to lock-stack and return.\n-            lock_stack.push(obj());\n-            return;\n-          }\n-          mark = old_mark;\n+      LockStack& lock_stack = locking_thread->lock_stack();\n+      if (lock_stack.is_full()) {\n+        \/\/ We unconditionally make room on the lock stack by inflating\n+        \/\/ the least recently locked object on the lock stack.\n+\n+        \/\/ About the choice to inflate least recently locked object.\n+        \/\/ First we must chose to inflate a lock, either some lock on\n+        \/\/ the lock-stack or the lock that is currently being entered\n+        \/\/ (which may or may not be on the lock-stack).\n+        \/\/ Second the best lock to inflate is a lock which is entered\n+        \/\/ in a control flow where there are only a very few locks being\n+        \/\/ used, as the costly part of inflated locking is inflation,\n+        \/\/ not locking. But this property is entirely program dependent.\n+        \/\/ Third inflating the lock currently being entered on when it\n+        \/\/ is not present on the lock-stack will result in a still full\n+        \/\/ lock-stack. This creates a scenario where every deeper nested\n+        \/\/ monitorenter must call into the runtime.\n+        \/\/ The rational here is as follows:\n+        \/\/ Because we cannot (currently) figure out the second, and want\n+        \/\/ to avoid the third, we inflate a lock on the lock-stack.\n+        \/\/ The least recently locked lock is chosen as it is the lock\n+        \/\/ with the longest critical section.\n+\n+        log_info(monitorinflation)(\"LockStack capacity exceeded, inflating.\");\n+        ObjectMonitor* monitor = inflate_for(locking_thread, lock_stack.bottom(), inflate_cause_vm_internal);\n+        assert(monitor->owner() == Thread::current(), \"must be owner=\" PTR_FORMAT \" current=\" PTR_FORMAT \" mark=\" PTR_FORMAT,\n+               p2i(monitor->owner()), p2i(Thread::current()), monitor->object()->mark_acquire().value());\n+        assert(!lock_stack.is_full(), \"must have made room here\");\n+      }\n+\n+      markWord mark = obj()->mark_acquire();\n+      while (mark.is_neutral()) {\n+        \/\/ Retry until a lock state change has been observed. cas_set_mark() may collide with non lock bits modifications.\n+        \/\/ Try to swing into 'fast-locked' state.\n+        assert(!lock_stack.contains(obj()), \"thread must not already hold the lock\");\n+        const markWord locked_mark = mark.set_fast_locked();\n+        const markWord old_mark = obj()->cas_set_mark(locked_mark, mark);\n+        if (old_mark == mark) {\n+          \/\/ Successfully fast-locked, push object to lock-stack and return.\n+          lock_stack.push(obj());\n+          return true;\n@@ -528,0 +609,1 @@\n+        mark = old_mark;\n@@ -529,1 +611,8 @@\n-      \/\/ All other paths fall-through to inflate-enter.\n+\n+      if (mark.is_fast_locked() && lock_stack.try_recursive_enter(obj())) {\n+        \/\/ Recursive lock successful.\n+        return true;\n+      }\n+\n+      \/\/ Failed to fast lock.\n+      return false;\n@@ -537,1 +626,1 @@\n-          return;\n+          return true;\n@@ -539,2 +628,1 @@\n-        \/\/ Fall through to inflate() ...\n-                 current->is_lock_owned((address) mark.locker())) {\n+                 locking_thread->is_lock_owned((address) mark.locker())) {\n@@ -545,1 +633,1 @@\n-        return;\n+        return true;\n@@ -553,0 +641,3 @@\n+\n+      \/\/ Failed to fast lock.\n+      return false;\n@@ -558,9 +649,1 @@\n-  \/\/ An async deflation can race after the inflate() call and before\n-  \/\/ enter() can make the ObjectMonitor busy. enter() returns false if\n-  \/\/ we have lost the race to async deflation and we simply try again.\n-  while (true) {\n-    ObjectMonitor* monitor = inflate(current, obj(), inflate_cause_monitor_enter);\n-    if (monitor->enter(current)) {\n-      return;\n-    }\n-  }\n+  return false;\n@@ -576,7 +659,21 @@\n-      while (mark.is_fast_locked()) {\n-        \/\/ Retry until a lock state change has been observed.  cas_set_mark() may collide with non lock bits modifications.\n-        const markWord unlocked_mark = mark.set_unlocked();\n-        const markWord old_mark = object->cas_set_mark(unlocked_mark, mark);\n-        if (old_mark == mark) {\n-          current->lock_stack().remove(object);\n-          return;\n+      LockStack& lock_stack = current->lock_stack();\n+      if (mark.is_fast_locked() && lock_stack.try_recursive_exit(object)) {\n+        \/\/ Recursively unlocked.\n+        return;\n+      }\n+\n+      if (mark.is_fast_locked() && lock_stack.is_recursive(object)) {\n+        \/\/ This lock is recursive but is not at the top of the lock stack so we're\n+        \/\/ doing an unbalanced exit. We have to fall thru to inflation below and\n+        \/\/ let ObjectMonitor::exit() do the unlock.\n+      } else {\n+        while (mark.is_fast_locked()) {\n+          \/\/ Retry until a lock state change has been observed. cas_set_mark() may collide with non lock bits modifications.\n+          const markWord unlocked_mark = mark.set_unlocked();\n+          const markWord old_mark = object->cas_set_mark(unlocked_mark, mark);\n+          if (old_mark == mark) {\n+            size_t recursions = lock_stack.remove(object) - 1;\n+            assert(recursions == 0, \"must not be recursive here\");\n+            return;\n+          }\n+          mark = old_mark;\n@@ -584,1 +681,0 @@\n-        mark = old_mark;\n@@ -634,7 +730,1 @@\n-  if (LockingMode == LM_LIGHTWEIGHT && monitor->is_owner_anonymous()) {\n-    \/\/ It must be owned by us. Pop lock object from lock stack.\n-    LockStack& lock_stack = current->lock_stack();\n-    oop popped = lock_stack.pop();\n-    assert(popped == object, \"must be owned by this thread\");\n-    monitor->set_owner_from_anonymous(current);\n-  }\n+  assert(!monitor->is_owner_anonymous(), \"must not be\");\n@@ -908,1 +998,1 @@\n-  value &= markWord::hash_mask;\n+  value &= UseCompactObjectHeaders ? markWord::hash_mask_compact : markWord::hash_mask;\n@@ -1303,5 +1393,11 @@\n-\/\/ Can be called from non JavaThreads (e.g., VMThread) for FastHashCode\n-\/\/ calculations as part of JVM\/TI tagging.\n-static bool is_lock_owned(Thread* thread, oop obj) {\n-  assert(LockingMode == LM_LIGHTWEIGHT, \"only call this with new lightweight locking enabled\");\n-  return thread->is_Java_thread() ? JavaThread::cast(thread)->lock_stack().contains(obj) : false;\n+ObjectMonitor* ObjectSynchronizer::inflate(Thread* current, oop obj, const InflateCause cause) {\n+  assert(current == Thread::current(), \"must be\");\n+  if (LockingMode == LM_LIGHTWEIGHT && current->is_Java_thread()) {\n+    return inflate_impl(JavaThread::cast(current), obj, cause);\n+  }\n+  return inflate_impl(nullptr, obj, cause);\n+}\n+\n+ObjectMonitor* ObjectSynchronizer::inflate_for(JavaThread* thread, oop obj, const InflateCause cause) {\n+  assert(thread == Thread::current() || thread->is_obj_deopt_suspend(), \"must be\");\n+  return inflate_impl(thread, obj, cause);\n@@ -1310,2 +1406,9 @@\n-ObjectMonitor* ObjectSynchronizer::inflate(Thread* current, oop object,\n-                                           const InflateCause cause) {\n+ObjectMonitor* ObjectSynchronizer::inflate_impl(JavaThread* inflating_thread, oop object, const InflateCause cause) {\n+  \/\/ The JavaThread* inflating_thread parameter is only used by LM_LIGHTWEIGHT and requires\n+  \/\/ that the inflating_thread == Thread::current() or is suspended throughout the call by\n+  \/\/ some other mechanism.\n+  \/\/ Even with LM_LIGHTWEIGHT the thread might be nullptr when called from a non\n+  \/\/ JavaThread. (As may still be the case from FastHashCode). However it is only\n+  \/\/ important for the correctness of the LM_LIGHTWEIGHT algorithm that the thread\n+  \/\/ is set when called from ObjectSynchronizer::enter from the owning thread,\n+  \/\/ ObjectSynchronizer::enter_for from any thread, or ObjectSynchronizer::exit.\n@@ -1320,4 +1423,4 @@\n-    \/\/                   is anonymous and the current thread owns the\n-    \/\/                   object lock, then we make the current thread the\n-    \/\/                   ObjectMonitor owner and remove the lock from the\n-    \/\/                   current thread's lock stack.\n+    \/\/                   is anonymous and the inflating_thread owns the\n+    \/\/                   object lock, then we make the inflating_thread\n+    \/\/                   the ObjectMonitor owner and remove the lock from\n+    \/\/                   the inflating_thread's lock stack.\n@@ -1335,3 +1438,5 @@\n-      if (LockingMode == LM_LIGHTWEIGHT && inf->is_owner_anonymous() && is_lock_owned(current, object)) {\n-        inf->set_owner_from_anonymous(current);\n-        JavaThread::cast(current)->lock_stack().remove(object);\n+      if (LockingMode == LM_LIGHTWEIGHT && inf->is_owner_anonymous() &&\n+          inflating_thread != nullptr && inflating_thread->lock_stack().contains(object)) {\n+        inf->set_owner_from_anonymous(inflating_thread);\n+        size_t removed = inflating_thread->lock_stack().remove(object);\n+        inf->set_recursions(removed - 1);\n@@ -1357,1 +1462,1 @@\n-    \/\/ Could be fast-locked either by current or by some other thread.\n+    \/\/ Could be fast-locked either by the inflating_thread or by some other thread.\n@@ -1361,2 +1466,2 @@\n-    \/\/ this thread owns the monitor, then we set the ObjectMonitor's\n-    \/\/ owner to this thread. Otherwise, we set the ObjectMonitor's owner\n+    \/\/ the inflating_thread owns the monitor, then we set the ObjectMonitor's\n+    \/\/ owner to the inflating_thread. Otherwise, we set the ObjectMonitor's owner\n@@ -1370,1 +1475,1 @@\n-      bool own = is_lock_owned(current, object);\n+      bool own = inflating_thread != nullptr && inflating_thread->lock_stack().contains(object);\n@@ -1372,2 +1477,2 @@\n-        \/\/ Owned by us.\n-        monitor->set_owner_from(nullptr, current);\n+        \/\/ Owned by inflating_thread.\n+        monitor->set_owner_from(nullptr, inflating_thread);\n@@ -1383,1 +1488,2 @@\n-          JavaThread::cast(current)->lock_stack().remove(object);\n+          size_t removed = inflating_thread->lock_stack().remove(object);\n+          monitor->set_recursions(removed - 1);\n@@ -1393,1 +1499,1 @@\n-          ResourceMark rm(current);\n+          ResourceMark rm;\n@@ -1492,1 +1598,1 @@\n-        ResourceMark rm(current);\n+        ResourceMark rm;\n@@ -1536,1 +1642,1 @@\n-      ResourceMark rm(current);\n+      ResourceMark rm;\n","filename":"src\/hotspot\/share\/runtime\/synchronizer.cpp","additions":186,"deletions":80,"binary":false,"changes":266,"status":"modified"},{"patch":"@@ -2,1 +2,1 @@\n- * Copyright (c) 1998, 2023, Oracle and\/or its affiliates. All rights reserved.\n+ * Copyright (c) 1998, 2024, Oracle and\/or its affiliates. All rights reserved.\n@@ -95,0 +95,10 @@\n+  \/\/ Used to enter a monitor for another thread. This requires that the\n+  \/\/ locking_thread is suspended, and that entering on a potential\n+  \/\/ inflated monitor may only contend with deflation. That is the obj being\n+  \/\/ locked on is either already locked by the locking_thread or cannot\n+  \/\/ escape the locking_thread.\n+  static void enter_for(Handle obj, BasicLock* lock, JavaThread* locking_thread);\n+private:\n+  \/\/ Shared implementation for enter and enter_for. Performs all but\n+  \/\/ inflated monitor enter.\n+  static bool enter_fast_impl(Handle obj, BasicLock* lock, JavaThread* locking_thread);\n@@ -96,0 +106,1 @@\n+public:\n@@ -116,0 +127,8 @@\n+  \/\/ Used to inflate a monitor as if it was done from the thread JavaThread.\n+  static ObjectMonitor* inflate_for(JavaThread* thread, oop obj, const InflateCause cause);\n+\n+private:\n+  \/\/ Shared implementation between the different LockingMode.\n+  static ObjectMonitor* inflate_impl(JavaThread* thread, oop obj, const InflateCause cause);\n+\n+public:\n@@ -196,1 +215,1 @@\n-  static void handle_sync_on_value_based_class(Handle obj, JavaThread* current);\n+  static void handle_sync_on_value_based_class(Handle obj, JavaThread* locking_thread);\n","filename":"src\/hotspot\/share\/runtime\/synchronizer.hpp","additions":21,"deletions":2,"binary":false,"changes":23,"status":"modified"},{"patch":"@@ -2584,0 +2584,1 @@\n+  declare_constant(markWord::hash_bits_compact)                           \\\n@@ -2588,0 +2589,2 @@\n+  declare_constant(markWord::hash_shift_compact)                          \\\n+  LP64_ONLY(declare_constant(markWord::klass_shift))                      \\\n@@ -2595,0 +2598,2 @@\n+  declare_constant(markWord::hash_mask_compact)                           \\\n+  declare_constant(markWord::hash_mask_compact_in_place)                  \\\n","filename":"src\/hotspot\/share\/runtime\/vmStructs.cpp","additions":5,"deletions":0,"binary":false,"changes":5,"status":"modified"},{"patch":"@@ -1082,1 +1082,2 @@\n-\n+const intptr_t badDispHeaderDeopt = 0xDE0BD000;             \/\/ value to fill unused displaced header during deoptimization\n+const intptr_t badDispHeaderOSR   = 0xDEAD05A0;             \/\/ value to fill unused displaced header during OSR\n","filename":"src\/hotspot\/share\/utilities\/globalDefinitions.hpp","additions":2,"deletions":1,"binary":false,"changes":3,"status":"modified"},{"patch":"@@ -58,1 +58,1 @@\n-compiler\/rtm\/locking\/TestRTMDeoptOnLowAbortRatio.java 8183263,8307907 generic-x64,generic-i586,aix-ppc64\n+compiler\/rtm\/locking\/TestRTMDeoptOnLowAbortRatio.java 8183263 generic-x64,generic-i586\n@@ -60,1 +60,1 @@\n-compiler\/rtm\/locking\/TestRTMLockingThreshold.java 8183263,8307907 generic-x64,generic-i586,aix-ppc64\n+compiler\/rtm\/locking\/TestRTMLockingThreshold.java 8183263 generic-x64,generic-i586\n@@ -63,2 +63,2 @@\n-compiler\/rtm\/locking\/TestUseRTMXendForLockBusy.java 8183263,8307907 generic-x64,generic-i586,aix-ppc64\n-compiler\/rtm\/print\/TestPrintPreciseRTMLockingStatistics.java 8183263,8307907 generic-x64,generic-i586,aix-ppc64\n+compiler\/rtm\/locking\/TestUseRTMXendForLockBusy.java 8183263 generic-x64,generic-i586\n+compiler\/rtm\/print\/TestPrintPreciseRTMLockingStatistics.java 8183263 generic-x64,generic-i586\n","filename":"test\/hotspot\/jtreg\/ProblemList.txt","additions":4,"deletions":4,"binary":false,"changes":8,"status":"modified"},{"patch":"@@ -2,1 +2,1 @@\n-# Copyright (c) 2013, 2023, Oracle and\/or its affiliates. All rights reserved.\n+# Copyright (c) 2013, 2024, Oracle and\/or its affiliates. All rights reserved.\n@@ -152,0 +152,1 @@\n+  gtest\/LockStackGtests.java \\\n","filename":"test\/hotspot\/jtreg\/TEST.groups","additions":2,"deletions":1,"binary":false,"changes":3,"status":"modified"},{"patch":"@@ -155,1 +155,2 @@\n-        applyIfPlatform = {\"64-bit\", \"true\"})\n+        applyIfPlatform = {\"64-bit\", \"true\"}),\n+        applyIf = {\"UseCompactObjectHeaders\", \"false\"})\n@@ -167,1 +168,2 @@\n-        applyIfPlatform = {\"64-bit\", \"true\"})\n+        applyIfPlatform = {\"64-bit\", \"true\"}),\n+        applyIf = {\"UseCompactObjectHeaders\", \"false\"})\n@@ -177,1 +179,2 @@\n-        applyIfCPUFeatureOr = {\"sse2\", \"true\", \"asimd\", \"true\"})\n+        applyIfCPUFeatureOr = {\"sse2\", \"true\", \"asimd\", \"true\"}),\n+        applyIf = {\"UseCompactObjectHeaders\", \"false\"})\n@@ -188,1 +191,2 @@\n-        applyIfPlatform = {\"64-bit\", \"true\"})\n+        applyIfPlatform = {\"64-bit\", \"true\"}),\n+        applyIf = {\"UseCompactObjectHeaders\", \"false\"})\n@@ -208,1 +212,2 @@\n-        applyIfPlatform = {\"64-bit\", \"true\"})\n+        applyIfPlatform = {\"64-bit\", \"true\"}),\n+        applyIf = {\"UseCompactObjectHeaders\", \"false\"})\n@@ -220,1 +225,2 @@\n-        applyIfPlatform = {\"64-bit\", \"true\"})\n+        applyIfPlatform = {\"64-bit\", \"true\"}),\n+        applyIf = {\"UseCompactObjectHeaders\", \"false\"})\n@@ -237,1 +243,2 @@\n-        applyIfPlatform = {\"64-bit\", \"true\"})\n+        applyIfPlatform = {\"64-bit\", \"true\"}),\n+        applyIf = {\"UseCompactObjectHeaders\", \"false\"})\n@@ -249,1 +256,2 @@\n-        applyIfPlatform = {\"64-bit\", \"true\"})\n+        applyIfPlatform = {\"64-bit\", \"true\"}),\n+        applyIf = {\"UseCompactObjectHeaders\", \"false\"})\n@@ -296,1 +304,2 @@\n-        applyIfPlatform = {\"64-bit\", \"true\"})\n+        applyIfPlatform = {\"64-bit\", \"true\"}),\n+        applyIf = {\"UseCompactObjectHeaders\", \"false\"})\n@@ -308,1 +317,2 @@\n-        applyIfPlatform = {\"64-bit\", \"true\"})\n+        applyIfPlatform = {\"64-bit\", \"true\"}),\n+        applyIf = {\"UseCompactObjectHeaders\", \"false\"})\n@@ -326,1 +336,2 @@\n-        applyIfPlatform = {\"64-bit\", \"true\"})\n+        applyIfPlatform = {\"64-bit\", \"true\"}),\n+        applyIf = {\"UseCompactObjectHeaders\", \"false\"})\n@@ -338,1 +349,2 @@\n-        applyIfPlatform = {\"64-bit\", \"true\"})\n+        applyIfPlatform = {\"64-bit\", \"true\"}),\n+        applyIf = {\"UseCompactObjectHeaders\", \"false\"})\n","filename":"test\/hotspot\/jtreg\/compiler\/c2\/irTests\/TestVectorizationMismatchedAccess.java","additions":24,"deletions":12,"binary":false,"changes":36,"status":"modified"},{"patch":"@@ -270,1 +270,2 @@\n-        applyIfCPUFeatureOr = {\"sse4.1\", \"true\", \"asimd\", \"true\"})\n+        applyIfCPUFeatureOr = {\"sse4.1\", \"true\", \"asimd\", \"true\"},\n+        applyIf = {\"UseCompactObjectHeaders\", \"false\"})\n","filename":"test\/hotspot\/jtreg\/compiler\/loopopts\/superword\/TestIndependentPacksWithCyclicDependency.java","additions":2,"deletions":1,"binary":false,"changes":3,"status":"modified"},{"patch":"@@ -35,1 +35,1 @@\n- * @run main\/timeout=240 gc.g1.plab.TestPLABPromotion\n+ * @run main\/othervm\/timeout=240 -Xbootclasspath\/a:. -XX:+UnlockDiagnosticVMOptions -XX:+WhiteBoxAPI gc.g1.plab.TestPLABPromotion\n@@ -48,0 +48,1 @@\n+import jdk.test.lib.Platform;\n@@ -50,0 +51,1 @@\n+import jdk.test.whitebox.WhiteBox;\n@@ -56,0 +58,2 @@\n+    private static final boolean COMPACT_HEADERS = Platform.is64bit() && WhiteBox.getWhiteBox().getBooleanVMFlag(\"UseCompactObjectHeaders\");\n+\n@@ -75,1 +79,1 @@\n-    private static final int OBJECT_SIZE_HIGH = 3250;\n+    private static final int OBJECT_SIZE_HIGH = COMPACT_HEADERS ? 3266 : 3250;\n","filename":"test\/hotspot\/jtreg\/gc\/g1\/plab\/TestPLABPromotion.java","additions":6,"deletions":2,"binary":false,"changes":8,"status":"modified"},{"patch":"@@ -123,0 +123,38 @@\n+ * @bug 8324881\n+ * @comment Regression test for using the wrong thread when logging during re-locking from deoptimization.\n+ *\n+ * @comment DiagnoseSyncOnValueBasedClasses=2 will cause logging when locking on \\@ValueBased objects.\n+ * @run driver EATests\n+ *                 -XX:+UnlockDiagnosticVMOptions\n+ *                 -Xms256m -Xmx256m\n+ *                 -Xbootclasspath\/a:.\n+ *                 -XX:CompileCommand=dontinline,*::dontinline_*\n+ *                 -XX:+WhiteBoxAPI\n+ *                 -Xbatch\n+ *                 -XX:+DoEscapeAnalysis -XX:+EliminateAllocations -XX:+EliminateLocks -XX:+EliminateNestedLocks\n+ *                 -XX:+UnlockExperimentalVMOptions -XX:LockingMode=1\n+ *                 -XX:DiagnoseSyncOnValueBasedClasses=2\n+ *\n+ * @comment Re-lock may inflate monitors when re-locking, which cause monitorinflation trace logging.\n+ * @run driver EATests\n+ *                 -XX:+UnlockDiagnosticVMOptions\n+ *                 -Xms256m -Xmx256m\n+ *                 -Xbootclasspath\/a:.\n+ *                 -XX:CompileCommand=dontinline,*::dontinline_*\n+ *                 -XX:+WhiteBoxAPI\n+ *                 -Xbatch\n+ *                 -XX:+DoEscapeAnalysis -XX:+EliminateAllocations -XX:+EliminateLocks -XX:+EliminateNestedLocks\n+ *                 -XX:+UnlockExperimentalVMOptions -XX:LockingMode=2\n+ *                 -Xlog:monitorinflation=trace:file=monitorinflation.log\n+ *\n+ * @comment Re-lock may race with deflation.\n+ * @run driver EATests\n+ *                 -XX:+UnlockDiagnosticVMOptions\n+ *                 -Xms256m -Xmx256m\n+ *                 -Xbootclasspath\/a:.\n+ *                 -XX:CompileCommand=dontinline,*::dontinline_*\n+ *                 -XX:+WhiteBoxAPI\n+ *                 -Xbatch\n+ *                 -XX:+DoEscapeAnalysis -XX:+EliminateAllocations -XX:+EliminateLocks -XX:+EliminateNestedLocks\n+ *                 -XX:+UnlockExperimentalVMOptions -XX:LockingMode=0\n+ *                 -XX:GuaranteedAsyncDeflationInterval=1000\n@@ -124,0 +162,1 @@\n+\n@@ -256,0 +295,1 @@\n+        new EARelockingNestedInflated_03Target()                                            .run();\n@@ -262,0 +302,1 @@\n+        new EARelockingValueBasedTarget()                                                   .run();\n@@ -375,0 +416,1 @@\n+        new EARelockingNestedInflated_03()                                            .run(this);\n@@ -381,0 +423,1 @@\n+        new EARelockingValueBased()                                                   .run(this);\n@@ -1925,0 +1968,88 @@\n+\/**\n+ * Like {@link EARelockingNestedInflated_02} with the difference that the\n+ * inflation of the lock happens because of contention.\n+ *\/\n+class EARelockingNestedInflated_03 extends EATestCaseBaseDebugger {\n+\n+    public void runTestCase() throws Exception {\n+        BreakpointEvent bpe = resumeTo(TARGET_TESTCASE_BASE_NAME, \"dontinline_brkpt\", \"()V\");\n+        printStack(bpe.thread());\n+        @SuppressWarnings(\"unused\")\n+        ObjectReference o = getLocalRef(bpe.thread().frame(2), XYVAL_NAME, \"l1\");\n+    }\n+}\n+\n+class EARelockingNestedInflated_03Target extends EATestCaseBaseTarget {\n+\n+    public XYVal lockInflatedByContention;\n+    public boolean doLockNow;\n+    public EATestCaseBaseTarget testCase;\n+\n+    @Override\n+    public void setUp() {\n+        super.setUp();\n+        testMethodDepth = 2;\n+        lockInflatedByContention = new XYVal(1, 1);\n+        testCase = this;\n+    }\n+\n+    @Override\n+    public void warmupDone() {\n+        super.warmupDone();\n+        \/\/ Use new lock. lockInflatedByContention might have been inflated because of recursion.\n+        lockInflatedByContention = new XYVal(1, 1);\n+        \/\/ Start thread that tries to enter lockInflatedByContention while the main thread owns it -> inflation\n+        TestScaffold.newThread(() -> {\n+            while (true) {\n+                synchronized (testCase) {\n+                    try {\n+                        if (doLockNow) {\n+                            doLockNow = false; \/\/ reset for main thread\n+                            testCase.notify();\n+                            break;\n+                        }\n+                        testCase.wait();\n+                    } catch (InterruptedException e) { \/* ignored *\/ }\n+                }\n+            }\n+            synchronized (lockInflatedByContention) { \/\/ will block and trigger inflation\n+                msg(Thread.currentThread().getName() + \": acquired lockInflatedByContention\");\n+            }\n+            }, testCaseName + \": Lock Contender (test thread)\").start();\n+    }\n+\n+    public void dontinline_testMethod() {\n+        @SuppressWarnings(\"unused\")\n+        XYVal xy = new XYVal(1, 1);            \/\/ scalar replaced\n+        XYVal l1 = lockInflatedByContention;   \/\/ read by debugger\n+        synchronized (l1) {\n+            testMethod_inlined(l1);\n+        }\n+    }\n+\n+    public void testMethod_inlined(XYVal l2) {\n+        synchronized (l2) {                 \/\/ eliminated nested locking\n+            dontinline_notifyOtherThread();\n+            dontinline_brkpt();\n+        }\n+    }\n+\n+    public void dontinline_notifyOtherThread() {\n+        if (!warmupDone) {\n+            return;\n+        }\n+        synchronized (testCase) {\n+            doLockNow = true;\n+            testCase.notify();\n+            \/\/ wait for other thread to reset doLockNow again\n+            while (doLockNow) {\n+                try {\n+                    testCase.wait();\n+                } catch (InterruptedException e) { \/* ignored *\/ }\n+            }\n+        }\n+    }\n+}\n+\n+\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\n+\n@@ -2140,0 +2271,26 @@\n+\n+\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\n+\n+\/**\n+ * Test relocking eliminated @ValueBased object.\n+ *\/\n+class EARelockingValueBased extends EATestCaseBaseDebugger {\n+\n+    public void runTestCase() throws Exception {\n+        BreakpointEvent bpe = resumeTo(TARGET_TESTCASE_BASE_NAME, \"dontinline_brkpt\", \"()V\");\n+        printStack(bpe.thread());\n+        @SuppressWarnings(\"unused\")\n+        ObjectReference o = getLocalRef(bpe.thread().frame(1), Integer.class.getName(), \"l1\");\n+    }\n+}\n+\n+class EARelockingValueBasedTarget extends EATestCaseBaseTarget {\n+\n+    public void dontinline_testMethod() {\n+        Integer l1 = new Integer(255);\n+        synchronized (l1) {\n+            dontinline_brkpt();\n+        }\n+    }\n+}\n+\n","filename":"test\/jdk\/com\/sun\/jdi\/EATests.java","additions":157,"deletions":0,"binary":false,"changes":157,"status":"modified"}]}