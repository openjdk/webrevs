{"files":[{"patch":"@@ -4,1 +4,1 @@\n-version=21.0.3\n+version=21.0.5\n@@ -8,0 +8,1 @@\n+warning=issuestitle,binary\n","filename":".jcheck\/conf","additions":2,"deletions":1,"binary":false,"changes":3,"status":"modified"},{"patch":"@@ -317,1 +317,0 @@\n-BASIC_POST_CONFIG_OUTPUT\n@@ -323,0 +322,3 @@\n+\n+# All output is done. Do the post-config output management.\n+BASIC_POST_CONFIG_OUTPUT\n","filename":"make\/autoconf\/configure.ac","additions":3,"deletions":1,"binary":false,"changes":4,"status":"modified"},{"patch":"@@ -193,0 +193,11 @@\n+  # Setup default CDS alignment. On platforms where one build may run on machines with different\n+  # page sizes, the JVM choses a compatible alignment to fit all possible page sizes. This slightly\n+  # increases archive size.\n+  # The only platform having this problem at the moment is Linux on aarch64, which may encounter\n+  # three different page sizes: 4K, 64K, and if run on Mac m1 hardware, 16K.\n+  COMPATIBLE_CDS_ALIGNMENT_DEFAULT=false\n+  if test \"x$OPENJDK_TARGET_OS\" = \"xlinux\" && test \"x$OPENJDK_TARGET_CPU\" = \"xaarch64\"; then\n+    COMPATIBLE_CDS_ALIGNMENT_DEFAULT=true\n+  fi\n+  AC_SUBST(COMPATIBLE_CDS_ALIGNMENT_DEFAULT)\n+\n@@ -492,0 +503,5 @@\n+  UTIL_ARG_WITH(NAME: additional-ubsan-checks, TYPE: string,\n+      DEFAULT: [],\n+      DESC: [Customizes the ubsan checks],\n+      OPTIONAL: true)\n+\n@@ -494,1 +510,2 @@\n-  UBSAN_CHECKS=\"-fsanitize=undefined -fsanitize=float-divide-by-zero -fno-sanitize=shift-base\"\n+  UBSAN_CHECKS=\"-fsanitize=undefined -fsanitize=float-divide-by-zero -fno-sanitize=shift-base -fno-sanitize=alignment \\\n+      $ADDITIONAL_UBSAN_CHECKS\"\n@@ -703,1 +720,1 @@\n-  UTIL_ARG_ENABLE(NAME: compatible-cds-alignment, DEFAULT: false,\n+  UTIL_ARG_ENABLE(NAME: compatible-cds-alignment, DEFAULT: $COMPATIBLE_CDS_ALIGNMENT_DEFAULT,\n","filename":"make\/autoconf\/jdk-options.m4","additions":19,"deletions":2,"binary":false,"changes":21,"status":"modified"},{"patch":"@@ -697,0 +697,5 @@\n+\/\/ Class for all non_special pointer registers (excluding rfp)\n+reg_class no_special_no_rfp_ptr_reg %{\n+  return _NO_SPECIAL_NO_RFP_PTR_REG_mask;\n+%}\n+\n@@ -1128,0 +1133,1 @@\n+extern RegMask _NO_SPECIAL_NO_RFP_PTR_REG_mask;\n@@ -1216,0 +1222,1 @@\n+  RegMask _NO_SPECIAL_NO_RFP_PTR_REG_mask;\n@@ -1252,0 +1259,3 @@\n+\n+    _NO_SPECIAL_NO_RFP_PTR_REG_mask = _NO_SPECIAL_PTR_REG_mask;\n+    _NO_SPECIAL_NO_RFP_PTR_REG_mask.Remove(OptoReg::as_OptoReg(r29->as_VMReg()));\n@@ -1724,2 +1734,2 @@\n-    st->print(\"ldr zr, [lr]\\n\\t\");\n-    st->print(\"pacia  lr, rfp\\n\\t\");\n+    st->print(\"ldr  zr, [lr]\\n\\t\");\n+    st->print(\"paciaz\\n\\t\");\n@@ -1854,2 +1864,2 @@\n-    st->print(\"autia lr, rfp\\n\\t\");\n-    st->print(\"ldr zr, [lr]\\n\\t\");\n+    st->print(\"autiaz\\n\\t\");\n+    st->print(\"ldr  zr, [lr]\\n\\t\");\n@@ -4888,0 +4898,12 @@\n+\/\/ This operand is not allowed to use rfp even if\n+\/\/ rfp is not used to hold the frame pointer.\n+operand iRegPNoSpNoRfp()\n+%{\n+  constraint(ALLOC_IN_RC(no_special_no_rfp_ptr_reg));\n+  match(RegP);\n+  match(iRegPNoSp);\n+  op_cost(0);\n+  format %{ %}\n+  interface(REG_INTER);\n+%}\n+\n@@ -16610,1 +16632,3 @@\n-instruct TailCalljmpInd(iRegPNoSp jump_target, inline_cache_RegP method_ptr)\n+\/\/ Don't use rfp for 'jump_target' because a MachEpilogNode has already been\n+\/\/ emitted just above the TailCall which has reset rfp to the caller state.\n+instruct TailCalljmpInd(iRegPNoSpNoRfp jump_target, inline_cache_RegP method_ptr)\n@@ -16623,1 +16647,1 @@\n-instruct TailjmpInd(iRegPNoSp jump_target, iRegP_R0 ex_oop)\n+instruct TailjmpInd(iRegPNoSpNoRfp jump_target, iRegP_R0 ex_oop)\n","filename":"src\/hotspot\/cpu\/aarch64\/aarch64.ad","additions":30,"deletions":6,"binary":false,"changes":36,"status":"modified"},{"patch":"@@ -393,4 +393,4 @@\n-class Decoder : public RelocActions {\n-  virtual reloc_insn adrpMem() { return &Decoder::adrpMem_impl; }\n-  virtual reloc_insn adrpAdd() { return &Decoder::adrpAdd_impl; }\n-  virtual reloc_insn adrpMovk() { return &Decoder::adrpMovk_impl; }\n+class AArch64Decoder : public RelocActions {\n+  virtual reloc_insn adrpMem() { return &AArch64Decoder::adrpMem_impl; }\n+  virtual reloc_insn adrpAdd() { return &AArch64Decoder::adrpAdd_impl; }\n+  virtual reloc_insn adrpMovk() { return &AArch64Decoder::adrpMovk_impl; }\n@@ -399,1 +399,1 @@\n-  Decoder(address insn_addr, uint32_t insn) : RelocActions(insn_addr, insn) {}\n+  AArch64Decoder(address insn_addr, uint32_t insn) : RelocActions(insn_addr, insn) {}\n@@ -495,1 +495,1 @@\n-  Decoder decoder(insn_addr, insn);\n+  AArch64Decoder decoder(insn_addr, insn);\n@@ -1201,0 +1201,104 @@\n+\/\/ Look up the method for a megamorphic invokeinterface call in a single pass over itable:\n+\/\/ - check recv_klass (actual object class) is a subtype of resolved_klass from CompiledICHolder\n+\/\/ - find a holder_klass (class that implements the method) vtable offset and get the method from vtable by index\n+\/\/ The target method is determined by <holder_klass, itable_index>.\n+\/\/ The receiver klass is in recv_klass.\n+\/\/ On success, the result will be in method_result, and execution falls through.\n+\/\/ On failure, execution transfers to the given label.\n+void MacroAssembler::lookup_interface_method_stub(Register recv_klass,\n+                                                  Register holder_klass,\n+                                                  Register resolved_klass,\n+                                                  Register method_result,\n+                                                  Register temp_itbl_klass,\n+                                                  Register scan_temp,\n+                                                  int itable_index,\n+                                                  Label& L_no_such_interface) {\n+  \/\/ 'method_result' is only used as output register at the very end of this method.\n+  \/\/ Until then we can reuse it as 'holder_offset'.\n+  Register holder_offset = method_result;\n+  assert_different_registers(resolved_klass, recv_klass, holder_klass, temp_itbl_klass, scan_temp, holder_offset);\n+\n+  int vtable_start_offset = in_bytes(Klass::vtable_start_offset());\n+  int itable_offset_entry_size = itableOffsetEntry::size() * wordSize;\n+  int ioffset = in_bytes(itableOffsetEntry::interface_offset());\n+  int ooffset = in_bytes(itableOffsetEntry::offset_offset());\n+\n+  Label L_loop_search_resolved_entry, L_resolved_found, L_holder_found;\n+\n+  ldrw(scan_temp, Address(recv_klass, Klass::vtable_length_offset()));\n+  add(recv_klass, recv_klass, vtable_start_offset + ioffset);\n+  \/\/ itableOffsetEntry[] itable = recv_klass + Klass::vtable_start_offset() + sizeof(vtableEntry) * recv_klass->_vtable_len;\n+  \/\/ temp_itbl_klass = itable[0]._interface;\n+  int vtblEntrySize = vtableEntry::size_in_bytes();\n+  assert(vtblEntrySize == wordSize, \"ldr lsl shift amount must be 3\");\n+  ldr(temp_itbl_klass, Address(recv_klass, scan_temp, Address::lsl(exact_log2(vtblEntrySize))));\n+  mov(holder_offset, zr);\n+  \/\/ scan_temp = &(itable[0]._interface)\n+  lea(scan_temp, Address(recv_klass, scan_temp, Address::lsl(exact_log2(vtblEntrySize))));\n+\n+  \/\/ Initial checks:\n+  \/\/   - if (holder_klass != resolved_klass), go to \"scan for resolved\"\n+  \/\/   - if (itable[0] == holder_klass), shortcut to \"holder found\"\n+  \/\/   - if (itable[0] == 0), no such interface\n+  cmp(resolved_klass, holder_klass);\n+  br(Assembler::NE, L_loop_search_resolved_entry);\n+  cmp(holder_klass, temp_itbl_klass);\n+  br(Assembler::EQ, L_holder_found);\n+  cbz(temp_itbl_klass, L_no_such_interface);\n+\n+  \/\/ Loop: Look for holder_klass record in itable\n+  \/\/   do {\n+  \/\/     temp_itbl_klass = *(scan_temp += itable_offset_entry_size);\n+  \/\/     if (temp_itbl_klass == holder_klass) {\n+  \/\/       goto L_holder_found; \/\/ Found!\n+  \/\/     }\n+  \/\/   } while (temp_itbl_klass != 0);\n+  \/\/   goto L_no_such_interface \/\/ Not found.\n+  Label L_search_holder;\n+  bind(L_search_holder);\n+    ldr(temp_itbl_klass, Address(pre(scan_temp, itable_offset_entry_size)));\n+    cmp(holder_klass, temp_itbl_klass);\n+    br(Assembler::EQ, L_holder_found);\n+    cbnz(temp_itbl_klass, L_search_holder);\n+\n+  b(L_no_such_interface);\n+\n+  \/\/ Loop: Look for resolved_class record in itable\n+  \/\/   while (true) {\n+  \/\/     temp_itbl_klass = *(scan_temp += itable_offset_entry_size);\n+  \/\/     if (temp_itbl_klass == 0) {\n+  \/\/       goto L_no_such_interface;\n+  \/\/     }\n+  \/\/     if (temp_itbl_klass == resolved_klass) {\n+  \/\/        goto L_resolved_found;  \/\/ Found!\n+  \/\/     }\n+  \/\/     if (temp_itbl_klass == holder_klass) {\n+  \/\/        holder_offset = scan_temp;\n+  \/\/     }\n+  \/\/   }\n+  \/\/\n+  Label L_loop_search_resolved;\n+  bind(L_loop_search_resolved);\n+    ldr(temp_itbl_klass, Address(pre(scan_temp, itable_offset_entry_size)));\n+  bind(L_loop_search_resolved_entry);\n+    cbz(temp_itbl_klass, L_no_such_interface);\n+    cmp(resolved_klass, temp_itbl_klass);\n+    br(Assembler::EQ, L_resolved_found);\n+    cmp(holder_klass, temp_itbl_klass);\n+    br(Assembler::NE, L_loop_search_resolved);\n+    mov(holder_offset, scan_temp);\n+    b(L_loop_search_resolved);\n+\n+  \/\/ See if we already have a holder klass. If not, go and scan for it.\n+  bind(L_resolved_found);\n+  cbz(holder_offset, L_search_holder);\n+  mov(scan_temp, holder_offset);\n+\n+  \/\/ Finally, scan_temp contains holder_klass vtable offset\n+  bind(L_holder_found);\n+  ldrw(method_result, Address(scan_temp, ooffset - ioffset));\n+  add(recv_klass, recv_klass, itable_index * wordSize + in_bytes(itableMethodEntry::method_offset())\n+    - vtable_start_offset - ioffset); \/\/ substract offsets to restore the original value of recv_klass\n+  ldr(method_result, Address(recv_klass, method_result, Address::uxtw(0)));\n+}\n+\n@@ -6027,1 +6131,1 @@\n-\/\/ Uses the FP as the modifier.\n+\/\/ Uses value zero as the modifier.\n@@ -6032,4 +6136,1 @@\n-    \/\/ The standard convention for C code is to use paciasp, which uses SP as the modifier. This\n-    \/\/ works because in C code, FP and SP match on function entry. In the JDK, SP and FP may not\n-    \/\/ match, so instead explicitly use the FP.\n-    pacia(lr, rfp);\n+    paciaz();\n@@ -6041,2 +6142,1 @@\n-\/\/ Uses the FP from the start of the function as the modifier - which is stored at the address of\n-\/\/ the current FP.\n+\/\/ Uses value zero as the modifier.\n@@ -6044,1 +6144,1 @@\n-void MacroAssembler::protect_return_address(Register return_reg, Register temp_reg) {\n+void MacroAssembler::protect_return_address(Register return_reg) {\n@@ -6046,3 +6146,1 @@\n-    assert(PreserveFramePointer, \"PreserveFramePointer must be set for ROP protection\");\n-    ldr(temp_reg, Address(rfp));\n-    pacia(return_reg, temp_reg);\n+    paciza(return_reg);\n@@ -6054,0 +6152,1 @@\n+\/\/ Uses value zero as the modifier.\n@@ -6055,1 +6154,1 @@\n-void MacroAssembler::authenticate_return_address(Register return_reg) {\n+void MacroAssembler::authenticate_return_address() {\n@@ -6057,2 +6156,2 @@\n-    autia(return_reg, rfp);\n-    check_return_address(return_reg);\n+    autiaz();\n+    check_return_address();\n@@ -6064,2 +6163,1 @@\n-\/\/ Uses the FP from the start of the function as the modifier - which is stored at the address of\n-\/\/ the current FP.\n+\/\/ Uses value zero as the modifier.\n@@ -6067,1 +6165,1 @@\n-void MacroAssembler::authenticate_return_address(Register return_reg, Register temp_reg) {\n+void MacroAssembler::authenticate_return_address(Register return_reg) {\n@@ -6069,3 +6167,1 @@\n-    assert(PreserveFramePointer, \"PreserveFramePointer must be set for ROP protection\");\n-    ldr(temp_reg, Address(rfp));\n-    autia(return_reg, temp_reg);\n+    autiza(return_reg);\n","filename":"src\/hotspot\/cpu\/aarch64\/macroAssembler_aarch64.cpp","additions":122,"deletions":26,"binary":false,"changes":148,"status":"modified"},{"patch":"@@ -719,3 +719,3 @@\n-  void protect_return_address(Register return_reg, Register temp_reg);\n-  void authenticate_return_address(Register return_reg = lr);\n-  void authenticate_return_address(Register return_reg, Register temp_reg);\n+  void protect_return_address(Register return_reg);\n+  void authenticate_return_address();\n+  void authenticate_return_address(Register return_reg);\n@@ -947,0 +947,9 @@\n+  void lookup_interface_method_stub(Register recv_klass,\n+                                    Register holder_klass,\n+                                    Register resolved_klass,\n+                                    Register method_result,\n+                                    Register temp_reg,\n+                                    Register temp_reg2,\n+                                    int itable_index,\n+                                    Label& L_no_such_interface);\n+\n","filename":"src\/hotspot\/cpu\/aarch64\/macroAssembler_aarch64.hpp","additions":12,"deletions":3,"binary":false,"changes":15,"status":"modified"},{"patch":"@@ -313,1 +313,1 @@\n-  uint stk_args = 0; \/\/ inc by 2 each time\n+  uint stk_args = 0;\n@@ -325,0 +325,1 @@\n+        stk_args = align_up(stk_args, 2);\n@@ -326,1 +327,1 @@\n-        stk_args += 2;\n+        stk_args += 1;\n@@ -343,0 +344,1 @@\n+        stk_args = align_up(stk_args, 2);\n@@ -351,0 +353,1 @@\n+        stk_args = align_up(stk_args, 2);\n@@ -352,1 +355,1 @@\n-        stk_args += 2;\n+        stk_args += 1;\n@@ -360,0 +363,1 @@\n+        stk_args = align_up(stk_args, 2);\n@@ -370,1 +374,1 @@\n-  return align_up(stk_args, 2);\n+  return stk_args;\n@@ -393,1 +397,1 @@\n-  __ authenticate_return_address(c_rarg1, rscratch1);\n+  __ authenticate_return_address(c_rarg1);\n@@ -1170,0 +1174,1 @@\n+      __ authenticate_return_address(c_rarg1);\n@@ -2333,1 +2338,1 @@\n-  __ protect_return_address(r3, rscratch1);\n+  __ protect_return_address(r3);\n@@ -2439,3 +2444,1 @@\n-  __ ldp(rfp, lr, __ post(sp, 2 * wordSize));\n-  __ authenticate_return_address();\n-  \/\/ LR should now be the return address to the caller (3)\n+  __ ldp(rfp, zr, __ post(sp, 2 * wordSize));\n@@ -2656,3 +2659,1 @@\n-  __ ldp(rfp, lr, __ post(sp, 2 * wordSize));\n-  __ authenticate_return_address();\n-  \/\/ LR should now be the return address to the caller (3) frame\n+  __ ldp(rfp, zr, __ post(sp, 2 * wordSize));\n@@ -2804,1 +2805,1 @@\n-    __ protect_return_address(r20, rscratch1);\n+    __ protect_return_address(r20);\n@@ -2845,1 +2846,1 @@\n-    __ authenticate_return_address(r20, rscratch1);\n+    __ authenticate_return_address(r20);\n@@ -2860,1 +2861,1 @@\n-    __ protect_return_address(r20, rscratch1);\n+    __ protect_return_address(r20);\n","filename":"src\/hotspot\/cpu\/aarch64\/sharedRuntime_aarch64.cpp","additions":16,"deletions":15,"binary":false,"changes":31,"status":"modified"},{"patch":"@@ -113,1 +113,2 @@\n-    CPU_MODEL_AMPERE_1A = 0xac4  \/* CPU implementer is CPU_AMPERE *\/\n+    CPU_MODEL_AMPERE_1A = 0xac4, \/* CPU implementer is CPU_AMPERE *\/\n+    CPU_MODEL_AMPERE_1B = 0xac5  \/* AMPERE_1B core Implements ARMv8.7 with CSSC, MTE, SM3\/SM4 extensions *\/\n@@ -154,0 +155,4 @@\n+  static bool model_is(int cpu_model) {\n+    return _model == cpu_model || _model2 == cpu_model;\n+  }\n+\n","filename":"src\/hotspot\/cpu\/aarch64\/vm_version_aarch64.hpp","additions":6,"deletions":1,"binary":false,"changes":7,"status":"modified"},{"patch":"@@ -463,0 +463,3 @@\n+  if (ce->compilation()->bailed_out()) {\n+    return; \/\/ CodeCache is full\n+  }\n","filename":"src\/hotspot\/cpu\/ppc\/c1_CodeStubs_ppc.cpp","additions":3,"deletions":0,"binary":false,"changes":3,"status":"modified"},{"patch":"@@ -34,0 +34,6 @@\n+#ifdef AIX\n+const size_t pd_segfault_address = -1;\n+#else\n+const size_t pd_segfault_address = 1024;\n+#endif\n+\n","filename":"src\/hotspot\/cpu\/ppc\/globalDefinitions_ppc.hpp","additions":6,"deletions":0,"binary":false,"changes":6,"status":"modified"},{"patch":"@@ -2065,1 +2065,4 @@\n-  if (base == NULL) return 0; \/\/ CodeBuffer::expand failed\n+  if (base == nullptr) {\n+    ciEnv::current()->record_failure(\"CodeCache is full\");\n+    return 0;  \/\/ CodeBuffer::expand failed\n+  }\n@@ -2082,1 +2085,4 @@\n-  if (base == NULL) return 0; \/\/ CodeBuffer::expand failed\n+  if (base == nullptr) {\n+    ciEnv::current()->record_failure(\"CodeCache is full\");\n+    return 0;  \/\/ CodeBuffer::expand failed\n+  }\n@@ -2801,0 +2807,1 @@\n+    RelocationHolder r; \/\/ Initializes type to none.\n@@ -2803,1 +2810,1 @@\n-      AddressLiteral a = __ allocate_oop_address((jobject)val);\n+      AddressLiteral a = __ constant_oop_address((jobject)val);\n@@ -2805,1 +2812,1 @@\n-      __ relocate(a.rspec());\n+      r = a.rspec();\n@@ -2807,0 +2814,1 @@\n+      \/\/ Notify OOP recorder (don't need the relocation)\n@@ -2809,1 +2817,0 @@\n-      __ relocate(a.rspec());\n@@ -2819,0 +2826,1 @@\n+    __ relocate(r); \/\/ If set above.\n@@ -2832,0 +2840,1 @@\n+      RelocationHolder r; \/\/ Initializes type to none.\n@@ -2834,1 +2843,1 @@\n-        AddressLiteral a = __ allocate_oop_address((jobject)val);\n+        AddressLiteral a = __ constant_oop_address((jobject)val);\n@@ -2836,1 +2845,1 @@\n-        __ relocate(a.rspec());\n+        r = a.rspec();\n@@ -2838,0 +2847,1 @@\n+        \/\/ Notify OOP recorder (don't need the relocation)\n@@ -2840,1 +2850,0 @@\n-        __ relocate(a.rspec());\n@@ -2850,0 +2859,1 @@\n+      __ relocate(r); \/\/ If set above.\n@@ -3526,0 +3536,1 @@\n+    call->_arg_escape        = _arg_escape;\n","filename":"src\/hotspot\/cpu\/ppc\/ppc.ad","additions":19,"deletions":8,"binary":false,"changes":27,"status":"modified"},{"patch":"@@ -737,1 +737,1 @@\n-  return align_up(stk, 2);\n+  return stk;\n","filename":"src\/hotspot\/cpu\/ppc\/sharedRuntime_ppc.cpp","additions":1,"deletions":1,"binary":false,"changes":2,"status":"modified"},{"patch":"@@ -724,1 +724,1 @@\n-  if (is_simm32(offset)) {\n+  if (is_valid_32bit_offset(offset)) {\n","filename":"src\/hotspot\/cpu\/riscv\/macroAssembler_riscv.cpp","additions":1,"deletions":1,"binary":false,"changes":2,"status":"modified"},{"patch":"@@ -643,0 +643,8 @@\n+  \/\/ The signed 20-bit upper imm can materialize at most negative 0xF...F80000000, two G.\n+  \/\/ The following signed 12-bit imm can at max subtract 0x800, two K, from that previously loaded two G.\n+  bool is_valid_32bit_offset(int64_t x) {\n+    constexpr int64_t twoG = (2 * G);\n+    constexpr int64_t twoK = (2 * K);\n+    return x < (twoG - twoK) && x >= (-twoG - twoK);\n+  }\n+\n@@ -797,1 +805,1 @@\n-    if (is_simm32(distance)) {                                                                     \\\n+    if (is_valid_32bit_offset(distance)) {                                                         \\\n@@ -854,1 +862,1 @@\n-    if (is_simm32(distance)) {                                                                     \\\n+    if (is_valid_32bit_offset(distance)) {                                                         \\\n@@ -915,1 +923,1 @@\n-    if (is_simm32(distance)) {                                                                     \\\n+    if (is_valid_32bit_offset(distance)) {                                                         \\\n@@ -960,1 +968,1 @@\n-    if (is_simm32(distance)) {                                                                     \\\n+    if (is_valid_32bit_offset(distance)) {                                                         \\\n","filename":"src\/hotspot\/cpu\/riscv\/macroAssembler_riscv.hpp","additions":12,"deletions":4,"binary":false,"changes":16,"status":"modified"},{"patch":"@@ -652,0 +652,1 @@\n+\/\/ Class for all non-special integer registers\n@@ -656,0 +657,1 @@\n+\/\/ Class for all non-special long integer registers\n@@ -664,0 +666,1 @@\n+\/\/ Class for all non_special pointer registers\n@@ -668,0 +671,5 @@\n+\/\/ Class for all non_special pointer registers (excluding fp)\n+reg_class no_special_no_fp_ptr_reg %{\n+  return _NO_SPECIAL_NO_FP_PTR_REG_mask;\n+%}\n+\n@@ -1020,0 +1028,1 @@\n+extern RegMask _NO_SPECIAL_NO_FP_PTR_REG_mask;\n@@ -1082,0 +1091,1 @@\n+RegMask _NO_SPECIAL_NO_FP_PTR_REG_mask;\n@@ -1116,0 +1126,3 @@\n+\n+  _NO_SPECIAL_NO_FP_PTR_REG_mask = _NO_SPECIAL_PTR_REG_mask;\n+  _NO_SPECIAL_NO_FP_PTR_REG_mask.Remove(OptoReg::as_OptoReg(x8->as_VMReg()));\n@@ -3192,0 +3205,12 @@\n+\/\/ This operand is not allowed to use fp even if\n+\/\/ fp is not used to hold the frame pointer.\n+operand iRegPNoSpNoFp()\n+%{\n+  constraint(ALLOC_IN_RC(no_special_no_fp_ptr_reg));\n+  match(RegP);\n+  match(iRegPNoSp);\n+  op_cost(0);\n+  format %{ %}\n+  interface(REG_INTER);\n+%}\n+\n@@ -10278,1 +10303,3 @@\n-instruct TailCalljmpInd(iRegPNoSp jump_target, inline_cache_RegP method_oop)\n+\/\/ Don't use fp for 'jump_target' because a MachEpilogNode has already been\n+\/\/ emitted just above the TailCall which has reset fp to the caller state.\n+instruct TailCalljmpInd(iRegPNoSpNoFp jump_target, inline_cache_RegP method_oop)\n@@ -10291,1 +10318,1 @@\n-instruct TailjmpInd(iRegPNoSp jump_target, iRegP_R10 ex_oop)\n+instruct TailjmpInd(iRegPNoSpNoFp jump_target, iRegP_R10 ex_oop)\n","filename":"src\/hotspot\/cpu\/riscv\/riscv.ad","additions":29,"deletions":2,"binary":false,"changes":31,"status":"modified"},{"patch":"@@ -269,1 +269,1 @@\n-  uint stk_args = 0; \/\/ inc by 2 each time\n+  uint stk_args = 0;\n@@ -281,0 +281,1 @@\n+          stk_args = align_up(stk_args, 2);\n@@ -282,1 +283,1 @@\n-          stk_args += 2;\n+          stk_args += 1;\n@@ -298,0 +299,1 @@\n+          stk_args = align_up(stk_args, 2);\n@@ -306,0 +308,1 @@\n+          stk_args = align_up(stk_args, 2);\n@@ -307,1 +310,1 @@\n-          stk_args += 2;\n+          stk_args += 1;\n@@ -315,0 +318,1 @@\n+          stk_args = align_up(stk_args, 2);\n@@ -324,1 +328,1 @@\n-  return align_up(stk_args, 2);\n+  return stk_args;\n","filename":"src\/hotspot\/cpu\/riscv\/sharedRuntime_riscv.cpp","additions":8,"deletions":4,"binary":false,"changes":12,"status":"modified"},{"patch":"@@ -58,0 +58,4 @@\n+    void disable_feature() {\n+      _enabled = false;\n+      _value = -1;\n+    }\n@@ -66,7 +70,12 @@\n-  #define UPDATE_DEFAULT(flag)        \\\n-  void update_flag() {                \\\n-      assert(enabled(), \"Must be.\");  \\\n-      if (FLAG_IS_DEFAULT(flag)) {    \\\n-        FLAG_SET_DEFAULT(flag, true); \\\n-      }                               \\\n-  }                                   \\\n+  #define UPDATE_DEFAULT(flag)             \\\n+  void update_flag() {                     \\\n+      assert(enabled(), \"Must be.\");       \\\n+      if (FLAG_IS_DEFAULT(flag)) {         \\\n+        FLAG_SET_DEFAULT(flag, true);      \\\n+      } else {                             \\\n+        \/* Sync CPU features with flags *\/ \\\n+        if (!flag) {                       \\\n+          disable_feature();               \\\n+        }                                  \\\n+      }                                    \\\n+  }                                        \\\n@@ -74,2 +83,2 @@\n-  #define NO_UPDATE_DEFAULT           \\\n-  void update_flag() {}               \\\n+  #define NO_UPDATE_DEFAULT                \\\n+  void update_flag() {}                    \\\n","filename":"src\/hotspot\/cpu\/riscv\/vm_version_riscv.hpp","additions":18,"deletions":9,"binary":false,"changes":27,"status":"modified"},{"patch":"@@ -2,2 +2,2 @@\n- * Copyright (c) 2016, 2023, Oracle and\/or its affiliates. All rights reserved.\n- * Copyright (c) 2016, 2018 SAP SE. All rights reserved.\n+ * Copyright (c) 2016, 2024, Oracle and\/or its affiliates. All rights reserved.\n+ * Copyright (c) 2016, 2024 SAP SE. All rights reserved.\n@@ -436,0 +436,1 @@\n+  CHECK_BAILOUT();\n","filename":"src\/hotspot\/cpu\/s390\/c1_CodeStubs_s390.cpp","additions":3,"deletions":2,"binary":false,"changes":5,"status":"modified"},{"patch":"@@ -1340,0 +1340,4 @@\n+#ifdef _LP64\n+  \/\/ Needs full 64-bit immediate for later patching.\n+  mov64(rax, (intptr_t)Universe::non_oop_word());\n+#else\n@@ -1341,0 +1345,1 @@\n+#endif\n@@ -2038,4 +2043,4 @@\n-  emit_int8((int8_t)0x0f);\n-  emit_int8((int8_t)0x1f);\n-  emit_int8((int8_t)0x84);\n-  emit_int8((int8_t)0x00);\n+  emit_int8((uint8_t)0x0f);\n+  emit_int8((uint8_t)0x1f);\n+  emit_int8((uint8_t)0x84);\n+  emit_int8((uint8_t)0x00);\n@@ -2050,5 +2055,5 @@\n-    emit_int8((int8_t)0x26); \/\/ es:\n-    emit_int8((int8_t)0x2e); \/\/ cs:\n-    emit_int8((int8_t)0x64); \/\/ fs:\n-    emit_int8((int8_t)0x65); \/\/ gs:\n-    emit_int8((int8_t)0x90);\n+    emit_int8((uint8_t)0x26); \/\/ es:\n+    emit_int8((uint8_t)0x2e); \/\/ cs:\n+    emit_int8((uint8_t)0x64); \/\/ fs:\n+    emit_int8((uint8_t)0x65); \/\/ gs:\n+    emit_int8((uint8_t)0x90);\n@@ -2577,1 +2582,9 @@\n-  LP64_ONLY(mov64(dst, src)) NOT_LP64(movl(dst, src));\n+#ifdef _LP64\n+  if (is_simm32(src)) {\n+    movq(dst, checked_cast<int32_t>(src));\n+  } else {\n+    mov64(dst, src);\n+  }\n+#else\n+  movl(dst, src);\n+#endif\n","filename":"src\/hotspot\/cpu\/x86\/macroAssembler_x86.cpp","additions":23,"deletions":10,"binary":false,"changes":33,"status":"modified"},{"patch":"@@ -120,1 +120,1 @@\n-      int imm8 = target - (address) &disp[1];\n+      int imm8 = checked_cast<int>(target - (address) &disp[1]);\n@@ -123,1 +123,1 @@\n-      *disp = imm8;\n+      *disp = (char)imm8;\n@@ -126,1 +126,1 @@\n-      int imm32 = target - (address) &disp[1];\n+      int imm32 = checked_cast<int>(target - (address) &disp[1]);\n@@ -763,1 +763,1 @@\n-    if (src.is_constant()) addptr(dst, src.as_constant());\n+    if (src.is_constant()) addptr(dst, checked_cast<int>(src.as_constant()));\n","filename":"src\/hotspot\/cpu\/x86\/macroAssembler_x86.hpp","additions":4,"deletions":4,"binary":false,"changes":8,"status":"modified"},{"patch":"@@ -531,2 +531,1 @@\n-  \/\/ return value can be odd number of VMRegImpl stack slots make multiple of 2\n-  return align_up(stack, 2);\n+  return stack;\n","filename":"src\/hotspot\/cpu\/x86\/sharedRuntime_x86_32.cpp","additions":1,"deletions":2,"binary":false,"changes":3,"status":"modified"},{"patch":"@@ -500,1 +500,1 @@\n-  uint stk_args = 0; \/\/ inc by 2 each time\n+  uint stk_args = 0;\n@@ -512,0 +512,1 @@\n+        stk_args = align_up(stk_args, 2);\n@@ -513,1 +514,1 @@\n-        stk_args += 2;\n+        stk_args += 1;\n@@ -530,0 +531,1 @@\n+        stk_args = align_up(stk_args, 2);\n@@ -538,0 +540,1 @@\n+        stk_args = align_up(stk_args, 2);\n@@ -539,1 +542,1 @@\n-        stk_args += 2;\n+        stk_args += 1;\n@@ -547,0 +550,1 @@\n+        stk_args = align_up(stk_args, 2);\n@@ -557,1 +561,1 @@\n-  return align_up(stk_args, 2);\n+  return stk_args;\n","filename":"src\/hotspot\/cpu\/x86\/sharedRuntime_x86_64.cpp","additions":8,"deletions":4,"binary":false,"changes":12,"status":"modified"},{"patch":"@@ -636,1 +636,1 @@\n-  static intx L1_line_size();\n+  static uint L1_line_size();\n@@ -638,1 +638,1 @@\n-  static intx prefetch_data_size()  {\n+  static uint prefetch_data_size()  {\n","filename":"src\/hotspot\/cpu\/x86\/vm_version_x86.hpp","additions":2,"deletions":2,"binary":false,"changes":4,"status":"modified"},{"patch":"@@ -13710,0 +13710,2 @@\n+\/\/ Don't use ebp for 'jump_target' because a MachEpilogNode has already been\n+\/\/ emitted just above the TailCall which has reset ebp to the caller state.\n","filename":"src\/hotspot\/cpu\/x86\/x86_32.ad","additions":2,"deletions":0,"binary":false,"changes":2,"status":"modified"},{"patch":"@@ -13500,0 +13500,2 @@\n+\/\/ Don't use rbp for 'jump_target' because a MachEpilogNode has already been\n+\/\/ emitted just above the TailCall which has reset rbp to the caller state.\n","filename":"src\/hotspot\/cpu\/x86\/x86_64.ad","additions":2,"deletions":0,"binary":false,"changes":2,"status":"modified"},{"patch":"@@ -446,1 +446,1 @@\n-    _bytes_to_copy = masm->pc() - pc_start();\n+    _bytes_to_copy = pointer_delta_as_int(masm->pc(), pc_start());\n","filename":"src\/hotspot\/share\/c1\/c1_CodeStubs.hpp","additions":1,"deletions":1,"binary":false,"changes":2,"status":"modified"},{"patch":"@@ -440,4 +440,2 @@\n-  \/\/ in the shared heap. This also has the side effect of pre-initializing the\n-  \/\/ identity_hash for all shared objects, so they are less likely to be written\n-  \/\/ into during run time, increasing the potential of memory sharing.\n-  if (src_obj != nullptr) {\n+  \/\/ in the shared heap.\n+  if (src_obj != nullptr && !src_obj->fast_no_hash_check()) {\n","filename":"src\/hotspot\/share\/cds\/archiveHeapWriter.cpp","additions":2,"deletions":4,"binary":false,"changes":6,"status":"modified"},{"patch":"@@ -28,1 +28,1 @@\n-#include \"classfile\/stringTable.hpp\"\n+#include \"classfile\/systemDictionary.hpp\"\n@@ -77,0 +77,1 @@\n+#include \"gc\/shared\/classUnloadingContext.hpp\"\n@@ -859,4 +860,0 @@\n-  \/\/ Delete metaspaces for unloaded class loaders and clean up loader_data graph\n-  ClassLoaderDataGraph::purge(\/*at_safepoint*\/true);\n-  DEBUG_ONLY(MetaspaceUtils::verify();)\n-\n@@ -2495,1 +2492,1 @@\n-  guarantee(!is_gc_active(), \"collection is not reentrant\");\n+  guarantee(!is_stw_gc_active(), \"collection is not reentrant\");\n@@ -2563,1 +2560,1 @@\n-  IsGCActiveMark active_gc_mark;\n+  IsSTWGCActiveMark active_gc_mark;\n@@ -2602,0 +2599,59 @@\n+void G1CollectedHeap::unload_classes_and_code(const char* description, BoolObjectClosure* is_alive, GCTimer* timer) {\n+  GCTraceTime(Debug, gc, phases) debug(description, timer);\n+\n+  ClassUnloadingContext ctx(workers()->active_workers(),\n+                            false \/* unregister_nmethods_during_purge *\/,\n+                            false \/* lock_codeblob_free_separately *\/);\n+  {\n+    CodeCache::UnlinkingScope scope(is_alive);\n+    bool unloading_occurred = SystemDictionary::do_unloading(timer);\n+    GCTraceTime(Debug, gc, phases) t(\"G1 Complete Cleaning\", timer);\n+    complete_cleaning(unloading_occurred);\n+  }\n+  {\n+    GCTraceTime(Debug, gc, phases) t(\"Purge Unlinked NMethods\", timer);\n+    ctx.purge_nmethods();\n+  }\n+  {\n+    GCTraceTime(Debug, gc, phases) ur(\"Unregister NMethods\", timer);\n+    G1CollectedHeap::heap()->bulk_unregister_nmethods();\n+  }\n+  {\n+    GCTraceTime(Debug, gc, phases) t(\"Free Code Blobs\", timer);\n+    ctx.free_code_blobs();\n+  }\n+  {\n+    GCTraceTime(Debug, gc, phases) t(\"Purge Class Loader Data\", timer);\n+    ClassLoaderDataGraph::purge(true \/* at_safepoint *\/);\n+    DEBUG_ONLY(MetaspaceUtils::verify();)\n+  }\n+}\n+\n+class G1BulkUnregisterNMethodTask : public WorkerTask {\n+  HeapRegionClaimer _hrclaimer;\n+\n+  class UnregisterNMethodsHeapRegionClosure : public HeapRegionClosure {\n+  public:\n+\n+    bool do_heap_region(HeapRegion* hr) {\n+      hr->rem_set()->bulk_remove_code_roots();\n+      return false;\n+    }\n+  } _cl;\n+\n+public:\n+  G1BulkUnregisterNMethodTask(uint num_workers)\n+  : WorkerTask(\"G1 Remove Unlinked NMethods From Code Root Set Task\"),\n+    _hrclaimer(num_workers) { }\n+\n+  void work(uint worker_id) {\n+    G1CollectedHeap::heap()->heap_region_par_iterate_from_worker_offset(&_cl, &_hrclaimer, worker_id);\n+  }\n+};\n+\n+void G1CollectedHeap::bulk_unregister_nmethods() {\n+  uint num_workers = workers()->active_workers();\n+  G1BulkUnregisterNMethodTask t(num_workers);\n+  workers()->run_task(&t);\n+}\n+\n@@ -2767,1 +2823,1 @@\n-   if (hr->is_humongous()) {\n+  if (hr->is_humongous()) {\n@@ -3014,27 +3070,1 @@\n-      \/\/ HeapRegion::add_code_root_locked() avoids adding duplicate entries.\n-      hr->add_code_root_locked(_nm);\n-    }\n-  }\n-\n-  void do_oop(narrowOop* p) { ShouldNotReachHere(); }\n-};\n-\n-class UnregisterNMethodOopClosure: public OopClosure {\n-  G1CollectedHeap* _g1h;\n-  nmethod* _nm;\n-\n-public:\n-  UnregisterNMethodOopClosure(G1CollectedHeap* g1h, nmethod* nm) :\n-    _g1h(g1h), _nm(nm) {}\n-\n-  void do_oop(oop* p) {\n-    oop heap_oop = RawAccess<>::oop_load(p);\n-    if (!CompressedOops::is_null(heap_oop)) {\n-      oop obj = CompressedOops::decode_not_null(heap_oop);\n-      HeapRegion* hr = _g1h->heap_region_containing(obj);\n-      assert(!hr->is_continues_humongous(),\n-             \"trying to remove code root \" PTR_FORMAT \" in continuation of humongous region \" HR_FORMAT\n-             \" starting at \" HR_FORMAT,\n-             p2i(_nm), HR_FORMAT_PARAMS(hr), HR_FORMAT_PARAMS(hr->humongous_start_region()));\n-\n-      hr->remove_code_root(_nm);\n+      hr->add_code_root(_nm);\n@@ -3054,3 +3084,2 @@\n-  guarantee(nm != nullptr, \"sanity\");\n-  UnregisterNMethodOopClosure reg_cl(this, nm);\n-  nm->oops_do(&reg_cl, true);\n+  \/\/ We always unregister nmethods in bulk during code unloading only.\n+  ShouldNotReachHere();\n","filename":"src\/hotspot\/share\/gc\/g1\/g1CollectedHeap.cpp","additions":67,"deletions":38,"binary":false,"changes":105,"status":"modified"},{"patch":"@@ -27,3 +27,0 @@\n-#include \"classfile\/systemDictionary.hpp\"\n-#include \"code\/codeCache.hpp\"\n-#include \"compiler\/oopMap.hpp\"\n@@ -44,0 +41,1 @@\n+#include \"gc\/shared\/classUnloadingContext.hpp\"\n@@ -175,0 +173,1 @@\n+    hr->prepare_for_full_gc();\n@@ -326,5 +325,1 @@\n-    GCTraceTime(Debug, gc, phases) debug(\"Phase 1: Class Unloading and Cleanup\", scope()->timer());\n-    CodeCache::UnloadingScope unloading_scope(&_is_alive);\n-    \/\/ Unload classes and purge the SystemDictionary.\n-    bool purged_class = SystemDictionary::do_unloading(scope()->timer());\n-    _heap->complete_cleaning(purged_class);\n+    _heap->unload_classes_and_code(\"Phase 1: Class Unloading and Cleanup\", &_is_alive, scope()->timer());\n","filename":"src\/hotspot\/share\/gc\/g1\/g1FullCollector.cpp","additions":3,"deletions":8,"binary":false,"changes":11,"status":"modified"},{"patch":"@@ -123,0 +123,1 @@\n+      size_t pretouch_page_size = UseLargePages ? page_size : os::vm_page_size();\n@@ -124,1 +125,1 @@\n-                             page_size, pretouch_workers);\n+                             pretouch_page_size, pretouch_workers);\n@@ -127,1 +128,1 @@\n-                             page_size, pretouch_workers);\n+                             pretouch_page_size, pretouch_workers);\n@@ -235,2 +236,1 @@\n-template<bool COMPACT_HEADERS>\n-void MutableSpace::object_iterate_impl(ObjectClosure* cl) {\n+void MutableSpace::object_iterate(ObjectClosure* cl) {\n@@ -243,1 +243,7 @@\n-    if (!obj->is_forwarded()) {\n+    if (obj->is_forwarded()) {\n+      assert(obj->forwardee() != obj, \"must not be self-forwarded\");\n+      \/\/ It is safe to use the forwardee here. Parallel GC only uses\n+      \/\/ header-based forwarding during promotion. Full GC doesn't\n+      \/\/ use the object header for forwarding at all.\n+      p += obj->forwardee()->size();\n+    } else {\n@@ -246,10 +252,0 @@\n-    } else {\n-      assert(obj->forwardee() != obj, \"must not be self-forwarded\");\n-      if (COMPACT_HEADERS) {\n-        \/\/ It is safe to use the forwardee here. Parallel GC only uses\n-        \/\/ header-based forwarding during promotion. Full GC doesn't\n-        \/\/ use the object header for forwarding at all.\n-        p += obj->forwardee()->size();\n-      } else {\n-        p += obj->size();\n-      }\n@@ -260,8 +256,0 @@\n-void MutableSpace::object_iterate(ObjectClosure* cl) {\n-  if (UseCompactObjectHeaders) {\n-    object_iterate_impl<true>(cl);\n-  } else {\n-    object_iterate_impl<false>(cl);\n-  }\n-}\n-\n","filename":"src\/hotspot\/share\/gc\/parallel\/mutableSpace.cpp","additions":11,"deletions":23,"binary":false,"changes":34,"status":"modified"},{"patch":"@@ -38,0 +38,1 @@\n+#include \"gc\/shared\/classUnloadingContext.hpp\"\n@@ -203,6 +204,26 @@\n-    CodeCache::UnloadingScope scope(&is_alive);\n-    \/\/ Unload classes and purge the SystemDictionary.\n-    bool purged_class = SystemDictionary::do_unloading(gc_timer());\n-\n-    \/\/ Unload nmethods.\n-    CodeCache::do_unloading(purged_class);\n+    ClassUnloadingContext* ctx = ClassUnloadingContext::context();\n+\n+    bool unloading_occurred;\n+    {\n+      CodeCache::UnlinkingScope scope(&is_alive);\n+\n+      \/\/ Unload classes and purge the SystemDictionary.\n+      unloading_occurred = SystemDictionary::do_unloading(gc_timer());\n+\n+      \/\/ Unload nmethods.\n+      CodeCache::do_unloading(unloading_occurred);\n+    }\n+\n+    {\n+      GCTraceTime(Debug, gc, phases) t(\"Purge Unlinked NMethods\", gc_timer());\n+      \/\/ Release unloaded nmethod's memory.\n+      ctx->purge_nmethods();\n+    }\n+    {\n+      GCTraceTime(Debug, gc, phases) ur(\"Unregister NMethods\", gc_timer());\n+      gch->prune_unlinked_nmethods();\n+    }\n+    {\n+      GCTraceTime(Debug, gc, phases) t(\"Free Code Blobs\", gc_timer());\n+      ctx->free_code_blobs();\n+    }\n@@ -212,1 +233,1 @@\n-    Klass::clean_weak_klass_links(purged_class);\n+    Klass::clean_weak_klass_links(unloading_occurred);\n@@ -215,1 +236,1 @@\n-    JVMCI_ONLY(JVMCI::do_unloading(purged_class));\n+    JVMCI_ONLY(JVMCI::do_unloading(unloading_occurred));\n","filename":"src\/hotspot\/share\/gc\/serial\/genMarkSweep.cpp","additions":29,"deletions":8,"binary":false,"changes":37,"status":"modified"},{"patch":"@@ -246,1 +246,1 @@\n-  _is_gc_active(false),\n+  _is_stw_gc_active(false),\n","filename":"src\/hotspot\/share\/gc\/shared\/collectedHeap.cpp","additions":1,"deletions":1,"binary":false,"changes":2,"status":"modified"},{"patch":"@@ -97,2 +97,1 @@\n-  friend class IsGCActiveMark; \/\/ Block structured external access to _is_gc_active\n-  friend class DisableIsGCActiveMark; \/\/ Disable current IsGCActiveMark\n+  friend class IsSTWGCActiveMark; \/\/ Block structured external access to _is_stw_gc_active\n@@ -117,1 +116,1 @@\n-  bool _is_gc_active;\n+  bool _is_stw_gc_active;\n@@ -385,4 +384,2 @@\n-  \/\/ Returns \"true\" iff there is a stop-world GC in progress.  (I assume\n-  \/\/ that it should answer \"false\" for the concurrent part of a concurrent\n-  \/\/ collector -- dld).\n-  bool is_gc_active() const { return _is_gc_active; }\n+  \/\/ Returns \"true\" iff there is a stop-world GC in progress.\n+  bool is_stw_gc_active() const { return _is_stw_gc_active; }\n","filename":"src\/hotspot\/share\/gc\/shared\/collectedHeap.hpp","additions":4,"deletions":7,"binary":false,"changes":11,"status":"modified"},{"patch":"@@ -39,0 +39,1 @@\n+#include \"gc\/shared\/classUnloadingContext.hpp\"\n@@ -462,1 +463,1 @@\n-  guarantee(!is_gc_active(), \"collection is not reentrant\");\n+  guarantee(!is_stw_gc_active(), \"collection is not reentrant\");\n@@ -473,1 +474,1 @@\n-  AutoModifyRestore<bool> temporarily(_is_gc_active, true);\n+  AutoModifyRestore<bool> temporarily(_is_stw_gc_active, true);\n@@ -561,0 +562,4 @@\n+    ClassUnloadingContext ctx(1 \/* num_nmethod_unlink_workers *\/,\n+                              false \/* unregister_nmethods_during_purge *\/,\n+                              false \/* lock_codeblob_free_separately *\/);\n+\n@@ -576,1 +581,1 @@\n-    ClassLoaderDataGraph::purge(\/*at_safepoint*\/true);\n+    ClassLoaderDataGraph::purge(true \/* at_safepoint *\/);\n@@ -617,1 +622,5 @@\n-  ScavengableNMethods::prune_nmethods();\n+  ScavengableNMethods::prune_nmethods_not_into_young();\n+}\n+\n+void GenCollectedHeap::prune_unlinked_nmethods() {\n+  ScavengableNMethods::prune_unlinked_nmethods();\n","filename":"src\/hotspot\/share\/gc\/shared\/genCollectedHeap.cpp","additions":13,"deletions":4,"binary":false,"changes":17,"status":"modified"},{"patch":"@@ -148,1 +148,1 @@\n-  assert(!Universe::heap()->is_gc_active(), \"Allocation during gc not allowed\");\n+  assert(!Universe::heap()->is_stw_gc_active(), \"Allocation during GC pause not allowed\");\n@@ -317,0 +317,1 @@\n+  \/\/ ...and clear or zap just allocated TLAB, if needed.\n@@ -318,4 +319,1 @@\n-    \/\/ ..and clear it.\n-  } else {\n-    \/\/ ...and zap just allocated object.\n-#ifdef ASSERT\n+  } else if (ZapTLAB) {\n@@ -328,1 +326,0 @@\n-#endif \/\/ ASSERT\n","filename":"src\/hotspot\/share\/gc\/shared\/memAllocator.cpp","additions":3,"deletions":6,"binary":false,"changes":9,"status":"modified"},{"patch":"@@ -367,1 +367,1 @@\n-    \/\/ Object fits into current region, record new location:\n+    \/\/ Object fits into current region, record new location, if object does not move:\n@@ -370,2 +370,4 @@\n-    _preserved_marks->push_if_necessary(p, p->mark());\n-    SlidingForwarding::forward_to<ALT_FWD>(p, cast_to_oop(_compact_point));\n+    if (_compact_point != cast_from_oop<HeapWord*>(p)) {\n+      _preserved_marks->push_if_necessary(p, p->mark());\n+      SlidingForwarding::forward_to<ALT_FWD>(p, cast_to_oop(_compact_point));\n+    }\n@@ -898,0 +900,1 @@\n+      assert(compact_from != compact_to, \"Forwarded object should move\");\n","filename":"src\/hotspot\/share\/gc\/shenandoah\/shenandoahFullGC.cpp","additions":6,"deletions":3,"binary":false,"changes":9,"status":"modified"},{"patch":"@@ -30,0 +30,1 @@\n+#include \"gc\/shared\/classUnloadingContext.hpp\"\n@@ -473,0 +474,1 @@\n+  _gc_state_changed(false),\n@@ -781,0 +783,1 @@\n+  \/\/ ...and clear or zap just allocated TLAB, if needed.\n@@ -782,4 +785,1 @@\n-    \/\/ ..and clear it.\n-  } else {\n-    \/\/ ...and zap just allocated object.\n-#ifdef ASSERT\n+  } else if (ZapTLAB) {\n@@ -792,1 +792,0 @@\n-#endif \/\/ ASSERT\n@@ -889,1 +888,5 @@\n-  ShenandoahHeapLocker locker(lock());\n+  \/\/ If we are dealing with mutator allocation, then we may need to block for safepoint.\n+  \/\/ We cannot block for safepoint for GC allocations, because there is a high chance\n+  \/\/ we are already running at safepoint or from stack watermark machinery, and we cannot\n+  \/\/ block again.\n+  ShenandoahHeapLocker locker(lock(), req.is_mutator_alloc());\n@@ -1687,3 +1690,8 @@\n-void ShenandoahHeap::set_gc_state_all_threads(char state) {\n-  for (JavaThreadIteratorWithHandle jtiwh; JavaThread *t = jtiwh.next(); ) {\n-    ShenandoahThreadLocalData::set_gc_state(t, state);\n+void ShenandoahHeap::propagate_gc_state_to_java_threads() {\n+  assert(ShenandoahSafepoint::is_at_shenandoah_safepoint(), \"Must be at Shenandoah safepoint\");\n+  if (_gc_state_changed) {\n+    _gc_state_changed = false;\n+    char state = gc_state();\n+    for (JavaThreadIteratorWithHandle jtiwh; JavaThread *t = jtiwh.next(); ) {\n+      ShenandoahThreadLocalData::set_gc_state(t, state);\n+    }\n@@ -1693,2 +1701,2 @@\n-void ShenandoahHeap::set_gc_state_mask(uint mask, bool value) {\n-  assert(ShenandoahSafepoint::is_at_shenandoah_safepoint(), \"Should really be Shenandoah safepoint\");\n+void ShenandoahHeap::set_gc_state(uint mask, bool value) {\n+  assert(ShenandoahSafepoint::is_at_shenandoah_safepoint(), \"Must be at Shenandoah safepoint\");\n@@ -1696,1 +1704,1 @@\n-  set_gc_state_all_threads(_gc_state.raw_value());\n+  _gc_state_changed = true;\n@@ -1701,1 +1709,1 @@\n-  set_gc_state_mask(MARKING, in_progress);\n+  set_gc_state(MARKING, in_progress);\n@@ -1707,1 +1715,1 @@\n-  set_gc_state_mask(EVACUATION, in_progress);\n+  set_gc_state(EVACUATION, in_progress);\n@@ -1719,1 +1727,1 @@\n-  set_gc_state_mask(WEAK_ROOTS, cond);\n+  set_gc_state(WEAK_ROOTS, cond);\n@@ -1767,0 +1775,4 @@\n+  ClassUnloadingContext ctx(_workers->active_workers(),\n+                            true \/* unregister_nmethods_during_purge *\/,\n+                            false \/* lock_codeblob_free_separately *\/);\n+\n@@ -1773,8 +1785,12 @@\n-    CodeCache::UnloadingScope scope(is_alive.is_alive_closure());\n-    ShenandoahGCPhase gc_phase(phase);\n-    ShenandoahGCWorkerPhase worker_phase(phase);\n-    bool purged_class = SystemDictionary::do_unloading(gc_timer());\n-\n-    uint num_workers = _workers->active_workers();\n-    ShenandoahClassUnloadingTask unlink_task(phase, num_workers, purged_class);\n-    _workers->run_task(&unlink_task);\n+    {\n+      CodeCache::UnlinkingScope scope(is_alive.is_alive_closure());\n+      ShenandoahGCPhase gc_phase(phase);\n+      ShenandoahGCWorkerPhase worker_phase(phase);\n+      bool unloading_occurred = SystemDictionary::do_unloading(gc_timer());\n+\n+      uint num_workers = _workers->active_workers();\n+      ShenandoahClassUnloadingTask unlink_task(phase, num_workers, unloading_occurred);\n+      _workers->run_task(&unlink_task);\n+    }\n+    \/\/ Release unloaded nmethods's memory.\n+    ClassUnloadingContext::context()->purge_and_free_nmethods();\n@@ -1787,1 +1803,1 @@\n-    ClassLoaderDataGraph::purge(\/*at_safepoint*\/true);\n+    ClassLoaderDataGraph::purge(true \/* at_safepoint *\/);\n@@ -1838,1 +1854,1 @@\n-  set_gc_state_mask(HAS_FORWARDED, cond);\n+  set_gc_state(HAS_FORWARDED, cond);\n@@ -1877,1 +1893,1 @@\n-  set_gc_state_mask(UPDATEREFS, in_progress);\n+  set_gc_state(UPDATEREFS, in_progress);\n","filename":"src\/hotspot\/share\/gc\/shenandoah\/shenandoahHeap.cpp","additions":42,"deletions":26,"binary":false,"changes":68,"status":"modified"},{"patch":"@@ -624,0 +624,2 @@\n+  ShenandoahHeap::heap()->propagate_gc_state_to_java_threads();\n+\n","filename":"src\/hotspot\/share\/gc\/shenandoah\/shenandoahVerifier.cpp","additions":2,"deletions":0,"binary":false,"changes":2,"status":"modified"},{"patch":"@@ -2,1 +2,1 @@\n- * Copyright (c) 2002, 2023, Oracle and\/or its affiliates. All rights reserved.\n+ * Copyright (c) 2002, 2024, Oracle and\/or its affiliates. All rights reserved.\n@@ -1992,5 +1992,3 @@\n-              \/\/ Initialize object field block:\n-              \/\/   - if TLAB is pre-zeroed, we can skip this path\n-              \/\/   - in debug mode, ThreadLocalAllocBuffer::allocate mangles\n-              \/\/     this area, and we still need to initialize it\n-              if (DEBUG_ONLY(true ||) !ZeroTLAB) {\n+              \/\/ Initialize object field block.\n+              if (!ZeroTLAB) {\n+                \/\/ The TLAB was not pre-zeroed, we need to clear the memory here.\n","filename":"src\/hotspot\/share\/interpreter\/zero\/bytecodeInterpreter.cpp","additions":4,"deletions":6,"binary":false,"changes":10,"status":"modified"},{"patch":"@@ -55,0 +55,1 @@\n+#include \"runtime\/arguments.hpp\"\n@@ -588,0 +589,12 @@\n+#ifdef ASSERT\n+  const char* val = Arguments::PropertyList_get_value(Arguments::system_properties(), \"test.jvmci.lookupTypeException\");\n+  if (val != nullptr) {\n+    if (strstr(val, \"<trace>\") != nullptr) {\n+      tty->print_cr(\"CompilerToVM.lookupType: %s\", str);\n+    } else if (strstr(val, str) != nullptr) {\n+      THROW_MSG_0(vmSymbols::java_lang_Exception(),\n+                  err_msg(\"lookupTypeException: %s\", str));\n+    }\n+  }\n+#endif\n+\n","filename":"src\/hotspot\/share\/jvmci\/jvmciCompilerToVM.cpp","additions":13,"deletions":0,"binary":false,"changes":13,"status":"modified"},{"patch":"@@ -1299,2 +1299,2 @@\n-bool Universe::is_gc_active() {\n-  return heap()->is_gc_active();\n+bool Universe::is_stw_gc_active() {\n+  return heap()->is_stw_gc_active();\n","filename":"src\/hotspot\/share\/memory\/universe.cpp","additions":2,"deletions":2,"binary":false,"changes":4,"status":"modified"},{"patch":"@@ -64,4 +64,0 @@\n-oop Klass::java_mirror_no_keepalive() const {\n-  return _java_mirror.peek();\n-}\n-\n","filename":"src\/hotspot\/share\/oops\/klass.cpp","additions":0,"deletions":4,"binary":false,"changes":4,"status":"modified"},{"patch":"@@ -64,0 +64,4 @@\n+inline oop Klass::java_mirror_no_keepalive() const {\n+  return _java_mirror.peek();\n+}\n+\n","filename":"src\/hotspot\/share\/oops\/klass.inline.hpp","additions":4,"deletions":0,"binary":false,"changes":4,"status":"modified"},{"patch":"@@ -244,1 +244,1 @@\n-  return Universe::heap()->is_gc_active() && is_objArray() && is_forwarded() && (UseParallelGC || UseG1GC);\n+  return Universe::heap()->is_stw_gc_active() && is_objArray() && is_forwarded() && (UseParallelGC || UseG1GC);\n","filename":"src\/hotspot\/share\/oops\/oop.cpp","additions":1,"deletions":1,"binary":false,"changes":2,"status":"modified"},{"patch":"@@ -235,0 +235,2 @@\n+  jint int_field_relaxed(int offset) const;\n+  void int_field_put_relaxed(int offset, jint contents);\n","filename":"src\/hotspot\/share\/oops\/oop.hpp","additions":2,"deletions":0,"binary":false,"changes":2,"status":"modified"},{"patch":"@@ -336,0 +336,2 @@\n+inline jint oopDesc::int_field_relaxed(int offset) const            { return Atomic::load(field_addr<jint>(offset)); }\n+inline void oopDesc::int_field_put_relaxed(int offset, jint value)  { Atomic::store(field_addr<jint>(offset), value); }\n","filename":"src\/hotspot\/share\/oops\/oop.inline.hpp","additions":2,"deletions":0,"binary":false,"changes":2,"status":"modified"},{"patch":"@@ -1468,0 +1468,1 @@\n+                                                     uint depth,\n@@ -1471,0 +1472,1 @@\n+  _depth(depth),\n@@ -1916,0 +1918,16 @@\n+\/\/ Check that all locks\/unlocks associated with object come from balanced regions.\n+bool AbstractLockNode::is_balanced() {\n+  Node* obj = obj_node();\n+  for (uint j = 0; j < obj->outcnt(); j++) {\n+    Node* n = obj->raw_out(j);\n+    if (n->is_AbstractLock() &&\n+        n->as_AbstractLock()->obj_node()->eqv_uncast(obj)) {\n+      BoxLockNode* n_box = n->as_AbstractLock()->box_node()->as_BoxLock();\n+      if (n_box->is_unbalanced()) {\n+        return false;\n+      }\n+    }\n+  }\n+  return true;\n+}\n+\n@@ -1968,1 +1986,1 @@\n-    if (cgr != nullptr && cgr->not_global_escape(obj_node())) {\n+    if (cgr != nullptr && cgr->can_eliminate_lock(this)) {\n@@ -2022,0 +2040,2 @@\n+            tty->print(\"Obj: \");\n+            obj_node()->dump();\n@@ -2030,0 +2050,2 @@\n+              tty->print(\"Box %d: \", i);\n+              box_node()->dump();\n@@ -2131,0 +2153,1 @@\n+        box->set_nested();\n@@ -2164,1 +2187,1 @@\n-    if (cgr != nullptr && cgr->not_global_escape(obj_node())) {\n+    if (cgr != nullptr && cgr->can_eliminate_lock(this)) {\n","filename":"src\/hotspot\/share\/opto\/callnode.cpp","additions":25,"deletions":2,"binary":false,"changes":27,"status":"modified"},{"patch":"@@ -56,0 +56,1 @@\n+#include \"opto\/locknode.hpp\"\n@@ -625,1 +626,0 @@\n-                  _failure_reason(nullptr),\n@@ -922,1 +922,0 @@\n-    _failure_reason(nullptr),\n@@ -1943,1 +1942,1 @@\n-          if (Verbose) {\n+          if (PrintOpto && Verbose) {\n@@ -4335,1 +4334,1 @@\n-  if (_failure_reason == nullptr) {\n+  if (_failure_reason.get() == nullptr) {\n@@ -4337,1 +4336,1 @@\n-    _failure_reason = reason;\n+    _failure_reason.set(reason);\n@@ -4816,0 +4815,2 @@\n+    AbstractLockNode* alock = locks.at(0);\n+    BoxLockNode* box = alock->box_node()->as_BoxLock();\n@@ -4820,0 +4821,14 @@\n+      BoxLockNode* this_box = lock->box_node()->as_BoxLock();\n+      if (this_box != box) {\n+        \/\/ Locking regions (BoxLock) could be Unbalanced here:\n+        \/\/  - its coarsened locks were eliminated in earlier\n+        \/\/    macro nodes elimination followed by loop unroll\n+        \/\/  - it is OSR locking region (no Lock node)\n+        \/\/ Preserve Unbalanced status in such cases.\n+        if (!this_box->is_unbalanced()) {\n+          this_box->set_coarsened();\n+        }\n+        if (!box->is_unbalanced()) {\n+          box->set_coarsened();\n+        }\n+      }\n@@ -4897,0 +4912,32 @@\n+\/\/ Mark locking regions (identified by BoxLockNode) as unbalanced if\n+\/\/ locks coarsening optimization removed Lock\/Unlock nodes from them.\n+\/\/ Such regions become unbalanced because coarsening only removes part\n+\/\/ of Lock\/Unlock nodes in region. As result we can't execute other\n+\/\/ locks elimination optimizations which assume all code paths have\n+\/\/ corresponding pair of Lock\/Unlock nodes - they are balanced.\n+void Compile::mark_unbalanced_boxes() const {\n+  int count = coarsened_count();\n+  for (int i = 0; i < count; i++) {\n+    Node_List* locks_list = _coarsened_locks.at(i);\n+    uint size = locks_list->size();\n+    if (size > 0) {\n+      AbstractLockNode* alock = locks_list->at(0)->as_AbstractLock();\n+      BoxLockNode* box = alock->box_node()->as_BoxLock();\n+      if (alock->is_coarsened()) {\n+        \/\/ coarsened_locks_consistent(), which is called before this method, verifies\n+        \/\/ that the rest of Lock\/Unlock nodes on locks_list are also coarsened.\n+        assert(!box->is_eliminated(), \"regions with coarsened locks should not be marked as eliminated\");\n+        for (uint j = 1; j < size; j++) {\n+          assert(locks_list->at(j)->as_AbstractLock()->is_coarsened(), \"only coarsened locks are expected here\");\n+          BoxLockNode* this_box = locks_list->at(j)->as_AbstractLock()->box_node()->as_BoxLock();\n+          if (box != this_box) {\n+            assert(!this_box->is_eliminated(), \"regions with coarsened locks should not be marked as eliminated\");\n+            box->set_unbalanced();\n+            this_box->set_unbalanced();\n+          }\n+        }\n+      }\n+    }\n+  }\n+}\n+\n@@ -4916,1 +4963,10 @@\n-          assert(in_hash, \"node should be in igvn hash table\");\n+#ifdef ASSERT\n+          if (!in_hash) {\n+            tty->print_cr(\"current graph:\");\n+            n->dump_bfs(MaxNodeLimit, nullptr, \"S$\");\n+            tty->cr();\n+            tty->print_cr(\"erroneous node:\");\n+            n->dump();\n+            assert(false, \"node should be in igvn hash table\");\n+          }\n+#endif\n","filename":"src\/hotspot\/share\/opto\/compile.cpp","additions":62,"deletions":6,"binary":false,"changes":68,"status":"modified"},{"patch":"@@ -4275,1 +4275,1 @@\n-    \/\/ Bail out if length is negative.\n+    \/\/ Bail out if length is negative (i.e., if start > end).\n@@ -4281,0 +4281,4 @@\n+    \/\/ Bail out if start is larger than the original length\n+    Node* orig_tail = _gvn.transform(new SubINode(orig_length, start));\n+    generate_negative_guard(orig_tail, bailout, &orig_tail);\n+\n@@ -4290,2 +4294,1 @@\n-      \/\/ The answer is MinI(orig_length - start, length).\n-      Node* orig_tail = _gvn.transform(new SubINode(orig_length, start));\n+      \/\/ The answer is MinI(orig_tail, length).\n@@ -4339,1 +4342,1 @@\n-        ArrayCopyNode* ac = ArrayCopyNode::make(this, true, original, start, newcopy, intcon(0), moved, true, false,\n+        ArrayCopyNode* ac = ArrayCopyNode::make(this, true, original, start, newcopy, intcon(0), moved, true, true,\n@@ -4903,1 +4906,7 @@\n-  if (ReduceBulkZeroing) {\n+  if (ReduceBulkZeroing &&\n+      \/\/ If we are implementing an array clone without knowing its source type\n+      \/\/ (can happen when compiling the array-guarded branch of a reflective\n+      \/\/ Object.clone() invocation), initialize the array within the allocation.\n+      \/\/ This is needed because some GCs (e.g. ZGC) might fall back in this case\n+      \/\/ to a runtime clone call that assumes fully initialized source arrays.\n+      (!is_array || obj->get_ptr_type()->isa_aryptr() != nullptr)) {\n","filename":"src\/hotspot\/share\/opto\/library_call.cpp","additions":14,"deletions":5,"binary":false,"changes":19,"status":"modified"},{"patch":"@@ -2,1 +2,1 @@\n- * Copyright (c) 2005, 2023, Oracle and\/or its affiliates. All rights reserved.\n+ * Copyright (c) 2005, 2024, Oracle and\/or its affiliates. All rights reserved.\n@@ -728,1 +728,1 @@\n-                                                 first_ind, nfields);\n+                                                 first_ind, sfpt->jvms()->depth(), nfields);\n@@ -1907,2 +1907,3 @@\n-void PhaseMacroExpand::mark_eliminated_box(Node* oldbox, Node* obj) {\n-  if (oldbox->as_BoxLock()->is_eliminated()) {\n+void PhaseMacroExpand::mark_eliminated_box(Node* box, Node* obj) {\n+  BoxLockNode* oldbox = box->as_BoxLock();\n+  if (oldbox->is_eliminated()) {\n@@ -1911,0 +1912,1 @@\n+  assert(!oldbox->is_unbalanced(), \"this should not be called for unbalanced region\");\n@@ -1918,0 +1920,1 @@\n+    oldbox->set_local();      \/\/ This verifies correct state of BoxLock\n@@ -1919,1 +1922,1 @@\n-    oldbox->as_BoxLock()->set_eliminated(); \/\/ This changes box's hash value\n+    oldbox->set_eliminated(); \/\/ This changes box's hash value\n@@ -1946,0 +1949,1 @@\n+  newbox->set_local(); \/\/ This verifies correct state of BoxLock\n@@ -2001,0 +2005,3 @@\n+  if (!alock->is_balanced()) {\n+    return; \/\/ Can't do any more elimination for this locking region\n+  }\n@@ -2317,0 +2324,5 @@\n+  } else {\n+    \/\/ After coarsened locks are eliminated locking regions\n+    \/\/ become unbalanced. We should not execute any more\n+    \/\/ locks elimination optimizations on them.\n+    C->mark_unbalanced_boxes();\n","filename":"src\/hotspot\/share\/opto\/macro.cpp","additions":17,"deletions":5,"binary":false,"changes":22,"status":"modified"},{"patch":"@@ -848,2 +848,6 @@\n-bool LoadNode::cmp( const Node &n ) const\n-{ return !Type::cmp( _type, ((LoadNode&)n)._type ); }\n+bool LoadNode::cmp(const Node &n) const {\n+  LoadNode& load = (LoadNode &)n;\n+  return !Type::cmp(_type, load._type) &&\n+         _control_dependency == load._control_dependency &&\n+         _mo == load._mo;\n+}\n@@ -986,0 +990,8 @@\n+LoadNode* LoadNode::pin_array_access_node() const {\n+  const TypePtr* adr_type = this->adr_type();\n+  if (adr_type != nullptr && adr_type->isa_aryptr()) {\n+    return clone_pinned();\n+  }\n+  return nullptr;\n+}\n+\n@@ -1005,1 +1017,2 @@\n-    LoadNode* ld = clone()->as_Load();\n+    \/\/ load depends on the tests that validate the arraycopy\n+    LoadNode* ld = clone_pinned();\n@@ -1047,2 +1060,0 @@\n-    \/\/ load depends on the tests that validate the arraycopy\n-    ld->_control_dependency = UnknownControl;\n@@ -2482,0 +2493,6 @@\n+LoadNode* LoadNode::clone_pinned() const {\n+  LoadNode* ld = clone()->as_Load();\n+  ld->_control_dependency = UnknownControl;\n+  return ld;\n+}\n+\n@@ -2758,1 +2775,4 @@\n-    result = mem;\n+    \/\/ Ensure vector type is the same\n+    if (!is_StoreVector() || (mem->is_LoadVector() && as_StoreVector()->vect_type() == mem->as_LoadVector()->vect_type())) {\n+      result = mem;\n+    }\n@@ -2767,1 +2787,18 @@\n-    result = mem;\n+    if (!is_StoreVector()) {\n+      result = mem;\n+    } else {\n+      const StoreVectorNode* store_vector = as_StoreVector();\n+      const StoreVectorNode* mem_vector = mem->as_StoreVector();\n+      const Node* store_indices = store_vector->indices();\n+      const Node* mem_indices = mem_vector->indices();\n+      const Node* store_mask = store_vector->mask();\n+      const Node* mem_mask = mem_vector->mask();\n+      \/\/ Ensure types, indices, and masks match\n+      if (store_vector->vect_type() == mem_vector->vect_type() &&\n+          ((store_indices == nullptr) == (mem_indices == nullptr) &&\n+           (store_indices == nullptr || store_indices->eqv_uncast(mem_indices))) &&\n+          ((store_mask == nullptr) == (mem_mask == nullptr) &&\n+           (store_mask == nullptr || store_mask->eqv_uncast(mem_mask)))) {\n+        result = mem;\n+      }\n+    }\n@@ -3353,0 +3390,1 @@\n+          assert(!trailing_load_store(), \"load store node can't be eliminated\");\n","filename":"src\/hotspot\/share\/opto\/memnode.cpp","additions":45,"deletions":7,"binary":false,"changes":52,"status":"modified"},{"patch":"@@ -1569,4 +1569,1 @@\n-  \/\/ This needs authenticating, but to do that here requires the fp of the previous frame.\n-  \/\/ A better way of doing it would be authenticate in the caller by adding a\n-  \/\/ AuthPAuthNode and using it in GraphKit::gen_stub. For now, just strip it.\n-  AARCH64_PORT_ONLY(ret_pc = pauth_strip_pointer(ret_pc));\n+  AARCH64_PORT_ONLY(ret_pc = pauth_strip_verifiable(ret_pc));\n","filename":"src\/hotspot\/share\/opto\/runtime.cpp","additions":1,"deletions":4,"binary":false,"changes":5,"status":"modified"},{"patch":"@@ -4176,1 +4176,1 @@\n-    if (loaded->ptr() == TypePtr::TopPTR)        { return unloaded; }\n+    if (loaded->ptr() == TypePtr::TopPTR)        { return unloaded->with_speculative(speculative); }\n@@ -4178,1 +4178,1 @@\n-    else if (loaded->ptr() == TypePtr::BotPTR)   { return TypeInstPtr::BOTTOM; }\n+    else if (loaded->ptr() == TypePtr::BotPTR)   { return TypeInstPtr::BOTTOM->with_speculative(speculative); }\n@@ -4180,2 +4180,2 @@\n-      if (unloaded->ptr() == TypePtr::BotPTR)    { return TypeInstPtr::BOTTOM;  }\n-      else                                       { return TypeInstPtr::NOTNULL; }\n+      if (unloaded->ptr() == TypePtr::BotPTR)    { return TypeInstPtr::BOTTOM->with_speculative(speculative);  }\n+      else                                       { return TypeInstPtr::NOTNULL->with_speculative(speculative); }\n@@ -4183,1 +4183,1 @@\n-    else if (unloaded->ptr() == TypePtr::TopPTR) { return unloaded; }\n+    else if (unloaded->ptr() == TypePtr::TopPTR) { return unloaded->with_speculative(speculative); }\n@@ -4185,1 +4185,1 @@\n-    return unloaded->cast_to_ptr_type(TypePtr::AnyNull)->is_instptr();\n+    return unloaded->cast_to_ptr_type(TypePtr::AnyNull)->is_instptr()->with_speculative(speculative);\n@@ -4191,1 +4191,1 @@\n-    return TypeInstPtr::NOTNULL;\n+    return TypeInstPtr::NOTNULL->with_speculative(speculative);\n@@ -4193,1 +4193,1 @@\n-  return TypeInstPtr::BOTTOM;\n+  return TypeInstPtr::BOTTOM->with_speculative(speculative);\n@@ -4596,0 +4596,4 @@\n+const TypeInstPtr* TypeInstPtr::with_speculative(const TypePtr* speculative) const {\n+  return make(_ptr, klass(), _interfaces, klass_is_exact(), const_oop(), _offset, _instance_id, speculative, _inline_depth);\n+}\n+\n@@ -6461,3 +6465,1 @@\n-  int dummy;\n-  bool this_top_or_bottom = (this_one->base_element_type(dummy) == Type::TOP || this_one->base_element_type(dummy) == Type::BOTTOM);\n-  if (!this_one->is_loaded() || !other->is_loaded() || this_top_or_bottom) {\n+  if (!this_one->is_loaded() || !other->is_loaded()) {\n@@ -6469,0 +6471,7 @@\n+\n+  int dummy;\n+  bool this_top_or_bottom = (this_one->base_element_type(dummy) == Type::TOP || this_one->base_element_type(dummy) == Type::BOTTOM);\n+  if (this_top_or_bottom) {\n+    return true;\n+  }\n+\n","filename":"src\/hotspot\/share\/opto\/type.cpp","additions":20,"deletions":11,"binary":false,"changes":31,"status":"modified"},{"patch":"@@ -58,0 +58,1 @@\n+#include \"sanitizers\/ub.hpp\"\n@@ -134,7 +135,3 @@\n-  jlong byte_offset = field_offset_to_byte_offset(field_offset);\n-\n-  if (sizeof(char*) == sizeof(jint)) {   \/\/ (this constant folds!)\n-    return cast_from_oop<address>(p) + (jint) byte_offset;\n-  } else {\n-    return cast_from_oop<address>(p) +        byte_offset;\n-  }\n+  uintptr_t base_address = cast_from_oop<uintptr_t>(p);\n+  uintptr_t byte_offset  = (uintptr_t)field_offset_to_byte_offset(field_offset);\n+  return (void*)(base_address + byte_offset);\n@@ -226,0 +223,3 @@\n+  \/\/ we use this method at some places for writing to 0 e.g. to cause a crash;\n+  \/\/ ubsan does not know that this is the desired behavior\n+  ATTRIBUTE_NO_UBSAN\n","filename":"src\/hotspot\/share\/prims\/unsafe.cpp","additions":7,"deletions":7,"binary":false,"changes":14,"status":"modified"},{"patch":"@@ -1845,1 +1845,1 @@\n-  return ObjectSynchronizer::request_deflate_idle_monitors();\n+  return ObjectSynchronizer::request_deflate_idle_monitors_from_wb();\n","filename":"src\/hotspot\/share\/prims\/whitebox.cpp","additions":1,"deletions":1,"binary":false,"changes":2,"status":"modified"},{"patch":"@@ -550,1 +550,1 @@\n-    bool unused;\n+    bool unused = false;\n@@ -1760,1 +1760,1 @@\n-      array->element(i)->free_monitors(thread);\n+      array->element(i)->free_monitors();\n","filename":"src\/hotspot\/share\/runtime\/deoptimization.cpp","additions":2,"deletions":2,"binary":false,"changes":4,"status":"modified"},{"patch":"@@ -490,0 +490,3 @@\n+  develop(bool, ZapTLAB, trueInDebug,                                       \\\n+          \"Zap allocated TLABs\")                                            \\\n+                                                                            \\\n@@ -739,0 +742,4 @@\n+  product(intx, MonitorUnlinkBatch, 500, DIAGNOSTIC,                        \\\n+          \"The maximum number of monitors to unlink in one batch. \")        \\\n+          range(1, max_jint)                                                \\\n+                                                                            \\\n","filename":"src\/hotspot\/share\/runtime\/globals.hpp","additions":7,"deletions":0,"binary":false,"changes":7,"status":"modified"},{"patch":"@@ -577,10 +577,0 @@\n-  if (ObjectSynchronizer::is_final_audit() && owner_is_DEFLATER_MARKER()) {\n-    \/\/ The final audit can see an already deflated ObjectMonitor on the\n-    \/\/ in-use list because MonitorList::unlink_deflated() might have\n-    \/\/ blocked for the final safepoint before unlinking all the deflated\n-    \/\/ monitors.\n-    assert(contentions() < 0, \"must be negative: contentions=%d\", contentions());\n-    \/\/ Already returned 'true' when it was originally deflated.\n-    return false;\n-  }\n-\n","filename":"src\/hotspot\/share\/runtime\/objectMonitor.cpp","additions":0,"deletions":10,"binary":false,"changes":10,"status":"modified"},{"patch":"@@ -1440,1 +1440,1 @@\n-           callee_method->method_holder()->is_init_thread(current),\n+           callee_method->method_holder()->is_reentrant_initialization(current),\n@@ -1994,1 +1994,1 @@\n-  int comp_args_on_stack = java_calling_convention(sig_bt, regs_without_member_name, total_args_passed - 1);\n+  java_calling_convention(sig_bt, regs_without_member_name, total_args_passed - 1);\n@@ -1999,1 +1999,1 @@\n-    assert(a->value() == b->value(), \"register allocation mismatch: a=\" INTX_FORMAT \", b=\" INTX_FORMAT, a->value(), b->value());\n+    assert(a->value() == b->value(), \"register allocation mismatch: a= %d, b= %d\", a->value(), b->value());\n@@ -3092,1 +3092,1 @@\n-      int comp_args_on_stack = SharedRuntime::java_calling_convention(sig_bt, regs, total_args_passed);\n+      SharedRuntime::java_calling_convention(sig_bt, regs, total_args_passed);\n","filename":"src\/hotspot\/share\/runtime\/sharedRuntime.cpp","additions":4,"deletions":4,"binary":false,"changes":8,"status":"modified"},{"patch":"@@ -68,39 +68,0 @@\n-class ObjectMonitorsHashtable::PtrList :\n-  public LinkedListImpl<ObjectMonitor*,\n-                        AnyObj::C_HEAP, mtThread,\n-                        AllocFailStrategy::RETURN_NULL> {};\n-\n-class CleanupObjectMonitorsHashtable: StackObj {\n- public:\n-  bool do_entry(void*& key, ObjectMonitorsHashtable::PtrList*& list) {\n-    list->clear();  \/\/ clear the LinkListNodes\n-    delete list;    \/\/ then delete the LinkedList\n-    return true;\n-  }\n-};\n-\n-ObjectMonitorsHashtable::~ObjectMonitorsHashtable() {\n-  CleanupObjectMonitorsHashtable cleanup;\n-  _ptrs->unlink(&cleanup);  \/\/ cleanup the LinkedLists\n-  delete _ptrs;             \/\/ then delete the hash table\n-}\n-\n-void ObjectMonitorsHashtable::add_entry(void* key, ObjectMonitor* om) {\n-  ObjectMonitorsHashtable::PtrList* list = get_entry(key);\n-  if (list == nullptr) {\n-    \/\/ Create new list and add it to the hash table:\n-    list = new (mtThread) ObjectMonitorsHashtable::PtrList;\n-    add_entry(key, list);\n-  }\n-  list->add(om);  \/\/ Add the ObjectMonitor to the list.\n-  _om_count++;\n-}\n-\n-bool ObjectMonitorsHashtable::has_entry(void* key, ObjectMonitor* om) {\n-  ObjectMonitorsHashtable::PtrList* list = get_entry(key);\n-  if (list == nullptr || list->find(om) == nullptr) {\n-    return false;\n-  }\n-  return true;\n-}\n-\n@@ -128,2 +89,2 @@\n-\/\/ Walk the in-use list and unlink (at most MonitorDeflationMax) deflated\n-\/\/ ObjectMonitors. Returns the number of unlinked ObjectMonitors.\n+\/\/ Walk the in-use list and unlink deflated ObjectMonitors.\n+\/\/ Returns the number of unlinked ObjectMonitors.\n@@ -132,0 +93,1 @@\n+                                    size_t deflated_count,\n@@ -135,2 +97,2 @@\n-  ObjectMonitor* head = Atomic::load_acquire(&_head);\n-  ObjectMonitor* m = head;\n+  ObjectMonitor* m = Atomic::load_acquire(&_head);\n+\n@@ -140,1 +102,3 @@\n-      \/\/ Find next live ObjectMonitor.\n+      \/\/ Find next live ObjectMonitor. Batch up the unlinkable monitors, so we can\n+      \/\/ modify the list once per batch. The batch starts at \"m\".\n+      size_t unlinked_batch = 0;\n@@ -142,0 +106,4 @@\n+      \/\/ Look for at most MonitorUnlinkBatch monitors, or the number of\n+      \/\/ deflated and not unlinked monitors, whatever comes first.\n+      assert(deflated_count >= unlinked_count, \"Sanity: underflow\");\n+      size_t unlinked_batch_limit = MIN2<size_t>(deflated_count - unlinked_count, MonitorUnlinkBatch);\n@@ -144,1 +112,1 @@\n-        unlinked_count++;\n+        unlinked_batch++;\n@@ -147,2 +115,8 @@\n-        if (unlinked_count >= (size_t)MonitorDeflationMax) {\n-          \/\/ Reached the max so bail out on the gathering loop.\n+        if (unlinked_batch >= unlinked_batch_limit) {\n+          \/\/ Reached the max batch, so bail out of the gathering loop.\n+          break;\n+        }\n+        if (prev == nullptr && Atomic::load(&_head) != m) {\n+          \/\/ Current batch used to be at head, but it is not at head anymore.\n+          \/\/ Bail out and figure out where we currently are. This avoids long\n+          \/\/ walks searching for new prev during unlink under heavy list inserts.\n@@ -152,0 +126,2 @@\n+\n+      \/\/ Unlink the found batch.\n@@ -153,3 +129,5 @@\n-        ObjectMonitor* prev_head = Atomic::cmpxchg(&_head, head, next);\n-        if (prev_head != head) {\n-          \/\/ Find new prev ObjectMonitor that just got inserted.\n+        \/\/ The current batch is the first batch, so there is a chance that it starts at head.\n+        \/\/ Optimistically assume no inserts happened, and try to unlink the entire batch from the head.\n+        ObjectMonitor* prev_head = Atomic::cmpxchg(&_head, m, next);\n+        if (prev_head != m) {\n+          \/\/ Something must have updated the head. Figure out the actual prev for this batch.\n@@ -159,0 +137,1 @@\n+          assert(prev != nullptr, \"Should have found the prev for the current batch\");\n@@ -162,0 +141,3 @@\n+        \/\/ The current batch is preceded by another batch. This guarantees the current batch\n+        \/\/ does not start at head. Unlink the entire current batch without updating the head.\n+        assert(Atomic::load(&_head) != m, \"Sanity\");\n@@ -164,2 +146,5 @@\n-      if (unlinked_count >= (size_t)MonitorDeflationMax) {\n-        \/\/ Reached the max so bail out on the searching loop.\n+\n+      unlinked_count += unlinked_batch;\n+      if (unlinked_count >= deflated_count) {\n+        \/\/ Reached the max so bail out of the searching loop.\n+        \/\/ There should be no more deflated monitors left.\n@@ -181,0 +166,14 @@\n+\n+#ifdef ASSERT\n+  \/\/ Invariant: the code above should unlink all deflated monitors.\n+  \/\/ The code that runs after this unlinking does not expect deflated monitors.\n+  \/\/ Notably, attempting to deflate the already deflated monitor would break.\n+  {\n+    ObjectMonitor* m = Atomic::load_acquire(&_head);\n+    while (m != nullptr) {\n+      assert(!m->is_being_async_deflated(), \"All deflated monitors should be unlinked\");\n+      m = m->next_om();\n+    }\n+  }\n+#endif\n+\n@@ -818,0 +817,10 @@\n+void ObjectSynchronizer::waitUninterruptibly(Handle obj, jlong millis, TRAPS) {\n+  if (millis < 0) {\n+    THROW_MSG(vmSymbols::java_lang_IllegalArgumentException(), \"timeout value is negative\");\n+  }\n+  ObjectSynchronizer::inflate(THREAD,\n+                              obj(),\n+                              inflate_cause_wait)->wait(millis, false, THREAD);\n+}\n+\n+\n@@ -1053,1 +1062,3 @@\n-    } else if (LockingMode == LM_LEGACY && mark.has_locker() && current->is_lock_owned((address)mark.locker())) {\n+    } else if (LockingMode == LM_LEGACY && mark.has_locker()\n+               && current->is_Java_thread()\n+               && JavaThread::cast(current)->is_lock_owned((address)mark.locker())) {\n@@ -1176,6 +1187,3 @@\n-\/\/ Iterate ObjectMonitors where the owner == thread; this does NOT include\n-\/\/ ObjectMonitors where owner is set to a stack-lock address in thread.\n-\/\/\n-\/\/ This version of monitors_iterate() works with the in-use monitor list.\n-\/\/\n-void ObjectSynchronizer::monitors_iterate(MonitorClosure* closure, JavaThread* thread) {\n+\/\/ Iterate over all ObjectMonitors.\n+template <typename Function>\n+void ObjectSynchronizer::monitors_iterate(Function function) {\n@@ -1184,17 +1192,2 @@\n-    ObjectMonitor* mid = iter.next();\n-    if (mid->owner() != thread) {\n-      \/\/ Not owned by the target thread and intentionally skips when owner\n-      \/\/ is set to a stack-lock address in the target thread.\n-      continue;\n-    }\n-    if (!mid->is_being_async_deflated() && mid->object_peek() != nullptr) {\n-      \/\/ Only process with closure if the object is set.\n-\n-      \/\/ monitors_iterate() is only called at a safepoint or when the\n-      \/\/ target thread is suspended or when the target thread is\n-      \/\/ operating on itself. The current closures in use today are\n-      \/\/ only interested in an owned ObjectMonitor and ownership\n-      \/\/ cannot be dropped under the calling contexts so the\n-      \/\/ ObjectMonitor cannot be async deflated.\n-      closure->do_monitor(mid);\n-    }\n+    ObjectMonitor* monitor = iter.next();\n+    function(monitor);\n@@ -1204,21 +1197,15 @@\n-\/\/ This version of monitors_iterate() works with the specified linked list.\n-\/\/\n-void ObjectSynchronizer::monitors_iterate(MonitorClosure* closure,\n-                                          ObjectMonitorsHashtable::PtrList* list,\n-                                          JavaThread* thread) {\n-  typedef LinkedListIterator<ObjectMonitor*> ObjectMonitorIterator;\n-  ObjectMonitorIterator iter(list->head());\n-  while (!iter.is_empty()) {\n-    ObjectMonitor* mid = *iter.next();\n-    \/\/ Owner set to a stack-lock address in thread should never be seen here:\n-    assert(mid->owner() == thread, \"must be\");\n-    if (!mid->is_being_async_deflated() && mid->object_peek() != nullptr) {\n-      \/\/ Only process with closure if the object is set.\n-\n-      \/\/ monitors_iterate() is only called at a safepoint or when the\n-      \/\/ target thread is suspended or when the target thread is\n-      \/\/ operating on itself. The current closures in use today are\n-      \/\/ only interested in an owned ObjectMonitor and ownership\n-      \/\/ cannot be dropped under the calling contexts so the\n-      \/\/ ObjectMonitor cannot be async deflated.\n-      closure->do_monitor(mid);\n+\/\/ Iterate ObjectMonitors owned by any thread and where the owner `filter`\n+\/\/ returns true.\n+template <typename OwnerFilter>\n+void ObjectSynchronizer::owned_monitors_iterate_filtered(MonitorClosure* closure, OwnerFilter filter) {\n+  monitors_iterate([&](ObjectMonitor* monitor) {\n+    \/\/ This function is only called at a safepoint or when the\n+    \/\/ target thread is suspended or when the target thread is\n+    \/\/ operating on itself. The current closures in use today are\n+    \/\/ only interested in an owned ObjectMonitor and ownership\n+    \/\/ cannot be dropped under the calling contexts so the\n+    \/\/ ObjectMonitor cannot be async deflated.\n+    if (monitor->has_owner() && filter(monitor->owner_raw())) {\n+      assert(!monitor->is_being_async_deflated(), \"Owned monitors should not be deflating\");\n+\n+      closure->do_monitor(monitor);\n@@ -1226,1 +1213,14 @@\n-  }\n+  });\n+}\n+\n+\/\/ Iterate ObjectMonitors where the owner == thread; this does NOT include\n+\/\/ ObjectMonitors where owner is set to a stack-lock address in thread.\n+void ObjectSynchronizer::owned_monitors_iterate(MonitorClosure* closure, JavaThread* thread) {\n+  auto thread_filter = [&](void* owner) { return owner == thread; };\n+  return owned_monitors_iterate_filtered(closure, thread_filter);\n+}\n+\n+\/\/ Iterate ObjectMonitors owned by any thread.\n+void ObjectSynchronizer::owned_monitors_iterate(MonitorClosure* closure) {\n+  auto all_filter = [&](void* owner) { return true; };\n+  return owned_monitors_iterate_filtered(closure, all_filter);\n@@ -1333,1 +1333,7 @@\n-bool ObjectSynchronizer::request_deflate_idle_monitors() {\n+void ObjectSynchronizer::request_deflate_idle_monitors() {\n+  MonitorLocker ml(MonitorDeflation_lock, Mutex::_no_safepoint_check_flag);\n+  set_is_async_deflation_requested(true);\n+  ml.notify_all();\n+}\n+\n+bool ObjectSynchronizer::request_deflate_idle_monitors_from_wb() {\n@@ -1338,5 +1344,3 @@\n-  set_is_async_deflation_requested(true);\n-  {\n-    MonitorLocker ml(MonitorDeflation_lock, Mutex::_no_safepoint_check_flag);\n-    ml.notify_all();\n-  }\n+\n+  request_deflate_idle_monitors();\n+\n@@ -1682,9 +1686,1 @@\n-\/\/ If table != nullptr, we gather owned ObjectMonitors indexed by the\n-\/\/ owner in the table. Please note that ObjectMonitors where the owner\n-\/\/ is set to a stack-lock address are NOT associated with the JavaThread\n-\/\/ that holds that stack-lock. All of the current consumers of\n-\/\/ ObjectMonitorsHashtable info only care about JNI locked monitors and\n-\/\/ those do not have the owner set to a stack-lock address.\n-\/\/\n-                                                elapsedTimer* timer_p,\n-                                                ObjectMonitorsHashtable* table) {\n+                                                elapsedTimer* timer_p) {\n@@ -1702,12 +1698,0 @@\n-    } else if (table != nullptr) {\n-      \/\/ The caller is interested in the owned ObjectMonitors. This does\n-      \/\/ not include when owner is set to a stack-lock address in thread.\n-      \/\/ This also does not capture unowned ObjectMonitors that cannot be\n-      \/\/ deflated because of a waiter.\n-      void* key = mid->owner();\n-      \/\/ Since deflate_idle_monitors() and deflate_monitor_list() can be\n-      \/\/ called more than once, we have to make sure the entry has not\n-      \/\/ already been added.\n-      if (key != nullptr && !table->has_entry(key, mid)) {\n-        table->add_entry(key, mid);\n-      }\n@@ -1763,3 +1747,2 @@\n-\/\/ ObjectMonitors. It is also called via do_final_audit_and_print_stats()\n-\/\/ and VM_ThreadDump::doit() by the VMThread.\n-size_t ObjectSynchronizer::deflate_idle_monitors(ObjectMonitorsHashtable* table) {\n+\/\/ ObjectMonitors.\n+size_t ObjectSynchronizer::deflate_idle_monitors() {\n@@ -1790,1 +1773,1 @@\n-  size_t deflated_count = deflate_monitor_list(current, ls, &timer, table);\n+  size_t deflated_count = deflate_monitor_list(current, ls, &timer);\n@@ -1793,5 +1776,2 @@\n-  if (deflated_count > 0 || is_final_audit()) {\n-    \/\/ There are ObjectMonitors that have been deflated or this is the\n-    \/\/ final audit and all the remaining ObjectMonitors have been\n-    \/\/ deflated, BUT the MonitorDeflationThread blocked for the final\n-    \/\/ safepoint during unlinking.\n+  if (deflated_count > 0) {\n+    \/\/ There are ObjectMonitors that have been deflated.\n@@ -1802,1 +1782,1 @@\n-    unlinked_count = _in_use_list.unlink_deflated(current, ls, &timer, &delete_list);\n+    unlinked_count = _in_use_list.unlink_deflated(current, ls, &timer, deflated_count, &delete_list);\n@@ -1849,4 +1829,0 @@\n-    if (table != nullptr) {\n-      ls->print_cr(\"ObjectMonitorsHashtable: key_count=\" SIZE_FORMAT \", om_count=\" SIZE_FORMAT,\n-                   table->key_count(), table->om_count());\n-    }\n@@ -1905,1 +1881,1 @@\n-  ObjectSynchronizer::monitors_iterate(&rjmc, current);\n+  ObjectSynchronizer::owned_monitors_iterate(&rjmc, current);\n@@ -1959,6 +1935,0 @@\n-    \/\/ Do deflations in order to reduce the in-use monitor population\n-    \/\/ that is reported by ObjectSynchronizer::log_in_use_monitor_details()\n-    \/\/ which is called by ObjectSynchronizer::audit_and_print_stats().\n-    while (deflate_idle_monitors(\/* ObjectMonitorsHashtable is not needed here *\/ nullptr) > 0) {\n-      ; \/\/ empty\n-    }\n@@ -2013,1 +1983,1 @@\n-    log_in_use_monitor_details(ls);\n+    log_in_use_monitor_details(ls, !on_exit \/* log_all *\/);\n@@ -2059,3 +2029,1 @@\n-    \/\/ This should not happen, but if it does, it is not fatal.\n-    out->print_cr(\"WARNING: monitor=\" INTPTR_FORMAT \": in-use monitor is \"\n-                  \"deflated.\", p2i(n));\n+    \/\/ This could happen when monitor deflation blocks for a safepoint.\n@@ -2064,0 +2032,1 @@\n+\n@@ -2093,2 +2062,1 @@\n-void ObjectSynchronizer::log_in_use_monitor_details(outputStream* out) {\n-  stringStream ss;\n+void ObjectSynchronizer::log_in_use_monitor_details(outputStream* out, bool log_all) {\n@@ -2096,0 +2064,1 @@\n+    stringStream ss;\n@@ -2101,12 +2070,18 @@\n-    MonitorList::Iterator iter = _in_use_list.iterator();\n-    while (iter.has_next()) {\n-      ObjectMonitor* mid = iter.next();\n-      const oop obj = mid->object_peek();\n-      const markWord mark = mid->header();\n-      ResourceMark rm;\n-      out->print(INTPTR_FORMAT \"  %d%d%d  \" INTPTR_FORMAT \"  %s\", p2i(mid),\n-                 mid->is_busy(), mark.hash() != 0, mid->owner() != nullptr,\n-                 p2i(obj), obj == nullptr ? \"\" : obj->klass()->external_name());\n-      if (mid->is_busy()) {\n-        out->print(\" (%s)\", mid->is_busy_to_string(&ss));\n-        ss.reset();\n+\n+    auto is_interesting = [&](ObjectMonitor* monitor) {\n+      return log_all || monitor->has_owner() || monitor->is_busy();\n+    };\n+\n+    monitors_iterate([&](ObjectMonitor* monitor) {\n+      if (is_interesting(monitor)) {\n+        const oop obj = monitor->object_peek();\n+        const markWord mark = monitor->header();\n+        ResourceMark rm;\n+        out->print(INTPTR_FORMAT \"  %d%d%d  \" INTPTR_FORMAT \"  %s\", p2i(monitor),\n+                   monitor->is_busy(), mark.hash() != 0, monitor->owner() != nullptr,\n+                   p2i(obj), obj == nullptr ? \"\" : obj->klass()->external_name());\n+        if (monitor->is_busy()) {\n+          out->print(\" (%s)\", monitor->is_busy_to_string(&ss));\n+          ss.reset();\n+        }\n+        out->cr();\n@@ -2114,2 +2089,1 @@\n-      out->cr();\n-    }\n+    });\n","filename":"src\/hotspot\/share\/runtime\/synchronizer.cpp","additions":140,"deletions":166,"binary":false,"changes":306,"status":"modified"},{"patch":"@@ -39,49 +39,0 @@\n-\/\/ Hash table of void* to a list of ObjectMonitor* owned by the JavaThread.\n-\/\/ The JavaThread's owner key is either a JavaThread* or a stack lock\n-\/\/ address in the JavaThread so we use \"void*\".\n-\/\/\n-class ObjectMonitorsHashtable {\n- private:\n-  static unsigned int ptr_hash(void* const& s1) {\n-    \/\/ 2654435761 = 2^32 * Phi (golden ratio)\n-    return (unsigned int)(((uint32_t)(uintptr_t)s1) * 2654435761u);\n-  }\n-\n- public:\n-  class PtrList;\n-\n- private:\n-  \/\/ ResourceHashtable SIZE is specified at compile time so we\n-  \/\/ use 1031 which is the first prime after 1024.\n-  typedef ResourceHashtable<void*, PtrList*, 1031, AnyObj::C_HEAP, mtThread,\n-                            &ObjectMonitorsHashtable::ptr_hash> PtrTable;\n-  PtrTable* _ptrs;\n-  size_t _key_count;\n-  size_t _om_count;\n-\n- public:\n-  \/\/ ResourceHashtable is passed to various functions and populated in\n-  \/\/ different places so we allocate it using C_HEAP to make it immune\n-  \/\/ from any ResourceMarks that happen to be in the code paths.\n-  ObjectMonitorsHashtable() : _ptrs(new (mtThread) PtrTable), _key_count(0), _om_count(0) {}\n-\n-  ~ObjectMonitorsHashtable();\n-\n-  void add_entry(void* key, ObjectMonitor* om);\n-\n-  void add_entry(void* key, PtrList* list) {\n-    _ptrs->put(key, list);\n-    _key_count++;\n-  }\n-\n-  PtrList* get_entry(void* key) {\n-    PtrList** listpp = _ptrs->get(key);\n-    return (listpp == nullptr) ? nullptr : *listpp;\n-  }\n-\n-  bool has_entry(void* key, ObjectMonitor* om);\n-\n-  size_t key_count() { return _key_count; }\n-  size_t om_count() { return _om_count; }\n-};\n-\n@@ -99,0 +50,1 @@\n+                         size_t deflated_count,\n@@ -168,0 +120,5 @@\n+  \/\/ Special internal-use-only method for use by JVM infrastructure\n+  \/\/ that needs to wait() on a java-level object but that can't risk\n+  \/\/ throwing unexpected InterruptedExecutionExceptions.\n+  static void waitUninterruptibly(Handle obj, jlong Millis, TRAPS);\n+\n@@ -194,0 +151,9 @@\n+  \/\/ Iterate over all ObjectMonitors.\n+  template <typename Function>\n+  static void monitors_iterate(Function function);\n+\n+  \/\/ Iterate ObjectMonitors owned by any thread and where the owner `filter`\n+  \/\/ returns true.\n+  template <typename OwnerFilter>\n+  static void owned_monitors_iterate_filtered(MonitorClosure* closure, OwnerFilter filter);\n+\n@@ -195,8 +161,5 @@\n-  \/\/ ObjectMonitors where owner is set to a stack lock address in thread:\n-  \/\/\n-  \/\/ This version of monitors_iterate() works with the in-use monitor list.\n-  static void monitors_iterate(MonitorClosure* m, JavaThread* thread);\n-  \/\/ This version of monitors_iterate() works with the specified linked list.\n-  static void monitors_iterate(MonitorClosure* closure,\n-                               ObjectMonitorsHashtable::PtrList* list,\n-                               JavaThread* thread);\n+  \/\/ ObjectMonitors where owner is set to a stack lock address in thread.\n+  static void owned_monitors_iterate(MonitorClosure* m, JavaThread* thread);\n+\n+  \/\/ Iterate ObjectMonitors owned by any thread.\n+  static void owned_monitors_iterate(MonitorClosure* closure);\n@@ -207,3 +170,3 @@\n-  \/\/ GC: we currently use aggressive monitor deflation policy\n-  \/\/ Basically we try to deflate all monitors that are not busy.\n-  static size_t deflate_idle_monitors(ObjectMonitorsHashtable* table);\n+  \/\/ We currently use aggressive monitor deflation policy;\n+  \/\/ basically we try to deflate all monitors that are not busy.\n+  static size_t deflate_idle_monitors();\n@@ -215,2 +178,1 @@\n-  static size_t deflate_monitor_list(Thread* current, LogStream* ls, elapsedTimer* timer_p,\n-                                     ObjectMonitorsHashtable* table);\n+  static size_t deflate_monitor_list(Thread* current, LogStream* ls, elapsedTimer* timer_p);\n@@ -226,1 +188,2 @@\n-  static bool request_deflate_idle_monitors();  \/\/ for whitebox test support\n+  static void request_deflate_idle_monitors();\n+  static bool request_deflate_idle_monitors_from_wb();  \/\/ for whitebox test support\n@@ -236,1 +199,1 @@\n-  static void log_in_use_monitor_details(outputStream* out);\n+  static void log_in_use_monitor_details(outputStream* out, bool log_all);\n@@ -271,0 +234,1 @@\n+  void wait_uninterruptibly(TRAPS)  { ObjectSynchronizer::waitUninterruptibly(_obj, 0, CHECK); } \/\/ wait forever\n@@ -274,0 +238,7 @@\n+\/\/ Interface to visit monitors\n+class ObjectMonitorsView {\n+public:\n+  \/\/ Visit monitors that belong to the given thread\n+  virtual void visit(MonitorClosure* closure, JavaThread* thread) = 0;\n+};\n+\n","filename":"src\/hotspot\/share\/runtime\/synchronizer.hpp","additions":35,"deletions":64,"binary":false,"changes":99,"status":"modified"},{"patch":"@@ -242,0 +242,1 @@\n+  nonstatic_field(InstanceKlass,               _is_marked_dependent,                          bool)                                  \\\n@@ -788,1 +789,0 @@\n-  nonstatic_field(ciEnv,                       _failure_reason,                               const char*)                           \\\n@@ -2237,1 +2237,0 @@\n-  declare_constant(InstanceKlass::being_linked)                           \\\n","filename":"src\/hotspot\/share\/runtime\/vmStructs.cpp","additions":1,"deletions":2,"binary":false,"changes":3,"status":"modified"},{"patch":"@@ -523,1 +523,1 @@\n-T2 checked_cast(T1 thing) {\n+constexpr T2 checked_cast(T1 thing) {\n@@ -529,0 +529,8 @@\n+\/\/ pointer_delta_as_int is called to do pointer subtraction for nearby pointers that\n+\/\/ returns a non-negative int, usually used as a size of a code buffer range.\n+\/\/ This scales to sizeof(T).\n+template <typename T>\n+inline int pointer_delta_as_int(const volatile T* left, const volatile T* right) {\n+  return checked_cast<int>(pointer_delta(left, right, sizeof(T)));\n+}\n+\n@@ -670,1 +678,1 @@\n-  return denominator != 0 ? (double)numerator \/ denominator * 100.0 : 0.0;\n+  return denominator != 0 ? (double)numerator \/ (double)denominator * 100.0 : 0.0;\n","filename":"src\/hotspot\/share\/utilities\/globalDefinitions.hpp","additions":10,"deletions":2,"binary":false,"changes":12,"status":"modified"},{"patch":"@@ -76,0 +76,2 @@\n+compiler\/codecache\/CheckLargePages.java 8319795 linux-x64\n+\n@@ -127,1 +129,1 @@\n-serviceability\/dcmd\/gc\/RunFinalizationTest.java 8227120 linux-all,windows-x64,aix-ppc64\n+serviceability\/dcmd\/gc\/RunFinalizationTest.java 8227120 generic-all\n@@ -139,0 +141,2 @@\n+serviceability\/jvmti\/stress\/StackTrace\/NotSuspended\/GetStackTraceNotSuspendedStressTest.java 8315980 linux-all,windows-x64\n+\n","filename":"test\/hotspot\/jtreg\/ProblemList.txt","additions":5,"deletions":1,"binary":false,"changes":6,"status":"modified"},{"patch":"@@ -60,0 +60,7 @@\n+hotspot_runtime_non_cds_mode = \\\n+  runtime \\\n+  -runtime\/cds\/CheckSharingWithDefaultArchive.java \\\n+  -runtime\/cds\/appcds\/dynamicArchive\/DynamicSharedSymbols.java \\\n+  -runtime\/cds\/appcds\/dynamicArchive\/TestAutoCreateSharedArchive.java \\\n+  -runtime\/cds\/appcds\/jcmd\n+\n@@ -257,0 +264,1 @@\n+  applications\/ctw\/modules \\\n@@ -402,0 +410,1 @@\n+ -runtime\/Monitor\/ConcurrentDeflation.java \\\n","filename":"test\/hotspot\/jtreg\/TEST.groups","additions":9,"deletions":0,"binary":false,"changes":9,"status":"modified"},{"patch":"@@ -3,0 +3,1 @@\n+ * Copyright (c) 2024, Oracle and\/or its affiliates. All rights reserved.\n@@ -38,1 +39,0 @@\n- * @requires (os.simpleArch == \"x64\") | (os.simpleArch == \"aarch64\")\n@@ -154,0 +154,2 @@\n+        applyIfCPUFeatureOr = {\"sse2\", \"true\", \"asimd\", \"true\"},\n+        applyIfPlatform = {\"64-bit\", \"true\"}),\n@@ -155,1 +157,3 @@\n-    public static void testByteLong1(byte[] dest, long[] src) {\n+    \/\/ 32-bit: offsets are badly aligned (UNSAFE.ARRAY_BYTE_BASE_OFFSET is 4 byte aligned, but not 8 byte aligned).\n+    \/\/         might get fixed with JDK-8325155.\n+    public static void testByteLong1a(byte[] dest, long[] src) {\n@@ -161,1 +165,37 @@\n-    @Run(test = \"testByteLong1\")\n+    @Test\n+    @IR(counts = { IRNode.LOAD_VECTOR_L, \">=1\", IRNode.STORE_VECTOR, \">=1\" },\n+        applyIfCPUFeatureOr = {\"sse2\", \"true\", \"asimd\", \"true\"},\n+        applyIfPlatform = {\"64-bit\", \"true\"}),\n+        applyIf = {\"UseCompactObjectHeaders\", \"false\"})\n+    \/\/ 32-bit: address has ConvL2I for cast of long to address, not supported.\n+    public static void testByteLong1b(byte[] dest, long[] src) {\n+        for (int i = 0; i < src.length; i++) {\n+            UNSAFE.putLongUnaligned(dest, UNSAFE.ARRAY_BYTE_BASE_OFFSET + 8L * i, src[i]);\n+        }\n+    }\n+\n+    @Test\n+    @IR(counts = { IRNode.LOAD_VECTOR_L, \">=1\", IRNode.STORE_VECTOR, \">=1\" },\n+        applyIfCPUFeatureOr = {\"sse2\", \"true\", \"asimd\", \"true\"}),\n+        applyIf = {\"UseCompactObjectHeaders\", \"false\"})\n+    public static void testByteLong1c(byte[] dest, long[] src) {\n+        long base = 64; \/\/ make sure it is big enough and 8 byte aligned (required for 32-bit)\n+        for (int i = 0; i < src.length - 8; i++) {\n+            UNSAFE.putLongUnaligned(dest, base + 8 * i, src[i]);\n+        }\n+    }\n+\n+    @Test\n+    @IR(counts = { IRNode.LOAD_VECTOR_L, \">=1\", IRNode.STORE_VECTOR, \">=1\" },\n+        applyIfCPUFeatureOr = {\"sse2\", \"true\", \"asimd\", \"true\"},\n+        applyIfPlatform = {\"64-bit\", \"true\"}),\n+        applyIf = {\"UseCompactObjectHeaders\", \"false\"})\n+    \/\/ 32-bit: address has ConvL2I for cast of long to address, not supported.\n+    public static void testByteLong1d(byte[] dest, long[] src) {\n+        long base = 64; \/\/ make sure it is big enough and 8 byte aligned (required for 32-bit)\n+        for (int i = 0; i < src.length - 8; i++) {\n+            UNSAFE.putLongUnaligned(dest, base + 8L * i, src[i]);\n+        }\n+    }\n+\n+    @Run(test = {\"testByteLong1a\", \"testByteLong1b\", \"testByteLong1c\", \"testByteLong1d\"})\n@@ -163,1 +203,4 @@\n-        runAndVerify(() -> testByteLong1(byteArray, longArray), 0);\n+        runAndVerify(() -> testByteLong1a(byteArray, longArray), 0);\n+        runAndVerify(() -> testByteLong1b(byteArray, longArray), 0);\n+        testByteLong1c(byteArray, longArray);\n+        testByteLong1d(byteArray, longArray);\n@@ -168,0 +211,2 @@\n+        applyIfCPUFeatureOr = {\"sse2\", \"true\", \"asimd\", \"true\"},\n+        applyIfPlatform = {\"64-bit\", \"true\"}),\n@@ -169,1 +214,3 @@\n-    public static void testByteLong2(byte[] dest, long[] src) {\n+    \/\/ 32-bit: offsets are badly aligned (UNSAFE.ARRAY_BYTE_BASE_OFFSET is 4 byte aligned, but not 8 byte aligned).\n+    \/\/         might get fixed with JDK-8325155.\n+    public static void testByteLong2a(byte[] dest, long[] src) {\n@@ -175,1 +222,13 @@\n-    @Run(test = \"testByteLong2\")\n+    @Test\n+    @IR(counts = { IRNode.LOAD_VECTOR_L, \">=1\", IRNode.STORE_VECTOR, \">=1\" },\n+        applyIfCPUFeatureOr = {\"sse2\", \"true\", \"asimd\", \"true\"},\n+        applyIfPlatform = {\"64-bit\", \"true\"}),\n+        applyIf = {\"UseCompactObjectHeaders\", \"false\"})\n+    \/\/ 32-bit: address has ConvL2I for cast of long to address, not supported.\n+    public static void testByteLong2b(byte[] dest, long[] src) {\n+        for (int i = 1; i < src.length; i++) {\n+            UNSAFE.putLongUnaligned(dest, UNSAFE.ARRAY_BYTE_BASE_OFFSET + 8L * (i - 1), src[i]);\n+        }\n+    }\n+\n+    @Run(test = {\"testByteLong2a\", \"testByteLong2b\"})\n@@ -177,1 +236,2 @@\n-        runAndVerify(() -> testByteLong2(byteArray, longArray), -8);\n+        runAndVerify(() -> testByteLong2a(byteArray, longArray), -8);\n+        runAndVerify(() -> testByteLong2b(byteArray, longArray), -8);\n@@ -182,0 +242,2 @@\n+        applyIfCPUFeatureOr = {\"sse2\", \"true\", \"asimd\", \"true\"},\n+        applyIfPlatform = {\"64-bit\", \"true\"}),\n@@ -183,1 +245,3 @@\n-    public static void testByteLong3(byte[] dest, long[] src) {\n+    \/\/ 32-bit: offsets are badly aligned (UNSAFE.ARRAY_BYTE_BASE_OFFSET is 4 byte aligned, but not 8 byte aligned).\n+    \/\/         might get fixed with JDK-8325155.\n+    public static void testByteLong3a(byte[] dest, long[] src) {\n@@ -189,1 +253,13 @@\n-    @Run(test = \"testByteLong3\")\n+    @Test\n+    @IR(counts = { IRNode.LOAD_VECTOR_L, \">=1\", IRNode.STORE_VECTOR, \">=1\" },\n+        applyIfCPUFeatureOr = {\"sse2\", \"true\", \"asimd\", \"true\"},\n+        applyIfPlatform = {\"64-bit\", \"true\"}),\n+        applyIf = {\"UseCompactObjectHeaders\", \"false\"})\n+    \/\/ 32-bit: address has ConvL2I for cast of long to address, not supported.\n+    public static void testByteLong3b(byte[] dest, long[] src) {\n+        for (int i = 0; i < src.length - 1; i++) {\n+            UNSAFE.putLongUnaligned(dest, UNSAFE.ARRAY_BYTE_BASE_OFFSET + 8L * (i + 1), src[i]);\n+        }\n+    }\n+\n+    @Run(test = {\"testByteLong3a\", \"testByteLong3b\"})\n@@ -191,1 +267,2 @@\n-        runAndVerify(() -> testByteLong3(byteArray, longArray), 8);\n+        runAndVerify(() -> testByteLong3a(byteArray, longArray), 8);\n+        runAndVerify(() -> testByteLong3b(byteArray, longArray), 8);\n@@ -195,2 +272,6 @@\n-    @IR(counts = { IRNode.LOAD_VECTOR_L, \">=1\", IRNode.STORE_VECTOR, \">=1\" })\n-    public static void testByteLong4(byte[] dest, long[] src, int start, int stop) {\n+    @IR(counts = { IRNode.LOAD_VECTOR_L, \">=1\", IRNode.STORE_VECTOR, \">=1\" },\n+        applyIfCPUFeatureOr = {\"sse2\", \"true\", \"asimd\", \"true\"},\n+        applyIfPlatform = {\"64-bit\", \"true\"})\n+    \/\/ 32-bit: offsets are badly aligned (UNSAFE.ARRAY_BYTE_BASE_OFFSET is 4 byte aligned, but not 8 byte aligned).\n+    \/\/         might get fixed with JDK-8325155.\n+    public static void testByteLong4a(byte[] dest, long[] src, int start, int stop) {\n@@ -202,1 +283,12 @@\n-    @Run(test = \"testByteLong4\")\n+    @Test\n+    @IR(counts = { IRNode.LOAD_VECTOR_L, \">=1\", IRNode.STORE_VECTOR, \">=1\" },\n+        applyIfCPUFeatureOr = {\"sse2\", \"true\", \"asimd\", \"true\"},\n+        applyIfPlatform = {\"64-bit\", \"true\"})\n+    \/\/ 32-bit: address has ConvL2I for cast of long to address, not supported.\n+    public static void testByteLong4b(byte[] dest, long[] src, int start, int stop) {\n+        for (int i = start; i < stop; i++) {\n+            UNSAFE.putLongUnaligned(dest, 8L * i + baseOffset, src[i]);\n+        }\n+    }\n+\n+    @Run(test = {\"testByteLong4a\", \"testByteLong4b\"})\n@@ -205,1 +297,2 @@\n-        runAndVerify(() -> testByteLong4(byteArray, longArray, 0, size), 0);\n+        runAndVerify(() -> testByteLong4a(byteArray, longArray, 0, size), 0);\n+        runAndVerify(() -> testByteLong4b(byteArray, longArray, 0, size), 0);\n@@ -210,0 +303,2 @@\n+        applyIfCPUFeatureOr = {\"sse2\", \"true\", \"asimd\", \"true\"},\n+        applyIfPlatform = {\"64-bit\", \"true\"}),\n@@ -211,1 +306,3 @@\n-    public static void testByteLong5(byte[] dest, long[] src, int start, int stop) {\n+    \/\/ 32-bit: offsets are badly aligned (UNSAFE.ARRAY_BYTE_BASE_OFFSET is 4 byte aligned, but not 8 byte aligned).\n+    \/\/         might get fixed with JDK-8325155.\n+    public static void testByteLong5a(byte[] dest, long[] src, int start, int stop) {\n@@ -217,1 +314,13 @@\n-    @Run(test = \"testByteLong5\")\n+    @Test\n+    @IR(counts = { IRNode.LOAD_VECTOR_L, \">=1\", IRNode.STORE_VECTOR, \">=1\" },\n+        applyIfCPUFeatureOr = {\"sse2\", \"true\", \"asimd\", \"true\"},\n+        applyIfPlatform = {\"64-bit\", \"true\"}),\n+        applyIf = {\"UseCompactObjectHeaders\", \"false\"})\n+    \/\/ 32-bit: address has ConvL2I for cast of long to address, not supported.\n+    public static void testByteLong5b(byte[] dest, long[] src, int start, int stop) {\n+        for (int i = start; i < stop; i++) {\n+            UNSAFE.putLongUnaligned(dest, UNSAFE.ARRAY_BYTE_BASE_OFFSET + 8L * (i + baseOffset), src[i]);\n+        }\n+    }\n+\n+    @Run(test = {\"testByteLong5a\", \"testByteLong5b\"})\n@@ -220,1 +329,2 @@\n-        runAndVerify(() -> testByteLong5(byteArray, longArray, 0, size-1), 8);\n+        runAndVerify(() -> testByteLong5a(byteArray, longArray, 0, size-1), 8);\n+        runAndVerify(() -> testByteLong5b(byteArray, longArray, 0, size-1), 8);\n@@ -225,0 +335,2 @@\n+        applyIfCPUFeatureOr = {\"sse2\", \"true\", \"asimd\", \"true\"},\n+        applyIfPlatform = {\"64-bit\", \"true\"}),\n@@ -226,1 +338,3 @@\n-    public static void testByteByte1(byte[] dest, byte[] src) {\n+    \/\/ 32-bit: offsets are badly aligned (UNSAFE.ARRAY_BYTE_BASE_OFFSET is 4 byte aligned, but not 8 byte aligned).\n+    \/\/         might get fixed with JDK-8325155.\n+    public static void testByteByte1a(byte[] dest, byte[] src) {\n@@ -232,1 +346,13 @@\n-    @Run(test = \"testByteByte1\")\n+    @Test\n+    @IR(counts = { IRNode.LOAD_VECTOR_L, \">=1\", IRNode.STORE_VECTOR, \">=1\" },\n+        applyIfCPUFeatureOr = {\"sse2\", \"true\", \"asimd\", \"true\"},\n+        applyIfPlatform = {\"64-bit\", \"true\"}),\n+        applyIf = {\"UseCompactObjectHeaders\", \"false\"})\n+    \/\/ 32-bit: address has ConvL2I for cast of long to address, not supported.\n+    public static void testByteByte1b(byte[] dest, byte[] src) {\n+        for (int i = 0; i < src.length \/ 8; i++) {\n+            UNSAFE.putLongUnaligned(dest, UNSAFE.ARRAY_BYTE_BASE_OFFSET + 8L * i, UNSAFE.getLongUnaligned(src, UNSAFE.ARRAY_BYTE_BASE_OFFSET + 8L * i));\n+        }\n+    }\n+\n+    @Run(test = {\"testByteByte1a\", \"testByteByte1b\"})\n@@ -234,1 +360,2 @@\n-        runAndVerify2(() -> testByteByte1(byteArray, byteArray), 0);\n+        runAndVerify2(() -> testByteByte1a(byteArray, byteArray), 0);\n+        runAndVerify2(() -> testByteByte1b(byteArray, byteArray), 0);\n@@ -237,3 +364,7 @@\n-    \/\/ It would be legal to vectorize this one but it's not currently\n-    \/\/@IR(counts = { IRNode.LOAD_VECTOR_L, \">=1\", IRNode.STORE_VECTOR, \">=1\" })\n-    public static void testByteByte2(byte[] dest, byte[] src) {\n+    \/\/ It would be legal to vectorize this one but it's not currently\n+    \/\/@IR(counts = { IRNode.LOAD_VECTOR_L, \">=1\", IRNode.STORE_VECTOR, \">=1\" },\n+    \/\/    applyIfCPUFeatureOr = {\"sse2\", \"true\", \"asimd\", \"true\"},\n+    \/\/    applyIfPlatform = {\"64-bit\", \"true\"})\n+    \/\/ 32-bit: offsets are badly aligned (UNSAFE.ARRAY_BYTE_BASE_OFFSET is 4 byte aligned, but not 8 byte aligned).\n+    \/\/         might get fixed with JDK-8325155.\n+    public static void testByteByte2a(byte[] dest, byte[] src) {\n@@ -246,1 +377,13 @@\n-    @Run(test = \"testByteByte2\")\n+    @Test\n+    \/\/ It would be legal to vectorize this one but it's not currently\n+    \/\/@IR(counts = { IRNode.LOAD_VECTOR_L, \">=1\", IRNode.STORE_VECTOR, \">=1\" },\n+    \/\/    applyIfCPUFeatureOr = {\"sse2\", \"true\", \"asimd\", \"true\"},\n+    \/\/    applyIfPlatform = {\"64-bit\", \"true\"})\n+    \/\/ 32-bit: address has ConvL2I for cast of long to address, not supported.\n+    public static void testByteByte2b(byte[] dest, byte[] src) {\n+        for (int i = 1; i < src.length \/ 8; i++) {\n+            UNSAFE.putLongUnaligned(dest, UNSAFE.ARRAY_BYTE_BASE_OFFSET + 8L * (i - 1), UNSAFE.getLongUnaligned(src, UNSAFE.ARRAY_BYTE_BASE_OFFSET + 8L * i));\n+        }\n+    }\n+\n+    @Run(test = {\"testByteByte2a\", \"testByteByte2b\"})\n@@ -248,1 +391,2 @@\n-        runAndVerify2(() -> testByteByte2(byteArray, byteArray), -8);\n+        runAndVerify2(() -> testByteByte2a(byteArray, byteArray), -8);\n+        runAndVerify2(() -> testByteByte2b(byteArray, byteArray), -8);\n@@ -253,1 +397,1 @@\n-    public static void testByteByte3(byte[] dest, byte[] src) {\n+    public static void testByteByte3a(byte[] dest, byte[] src) {\n@@ -259,1 +403,9 @@\n-    @Run(test = \"testByteByte3\")\n+    @Test\n+    @IR(failOn = { IRNode.LOAD_VECTOR_L, IRNode.STORE_VECTOR })\n+    public static void testByteByte3b(byte[] dest, byte[] src) {\n+        for (int i = 0; i < src.length \/ 8 - 1; i++) {\n+            UNSAFE.putLongUnaligned(dest, UNSAFE.ARRAY_BYTE_BASE_OFFSET + 8L * (i + 1), UNSAFE.getLongUnaligned(src, UNSAFE.ARRAY_BYTE_BASE_OFFSET + 8L * i));\n+        }\n+    }\n+\n+    @Run(test = {\"testByteByte3a\", \"testByteByte3b\"})\n@@ -261,1 +413,2 @@\n-        runAndVerify2(() -> testByteByte3(byteArray, byteArray), 8);\n+        runAndVerify2(() -> testByteByte3a(byteArray, byteArray), 8);\n+        runAndVerify2(() -> testByteByte3b(byteArray, byteArray), 8);\n@@ -266,1 +419,1 @@\n-    public static void testByteByte4(byte[] dest, byte[] src, int start, int stop) {\n+    public static void testByteByte4a(byte[] dest, byte[] src, int start, int stop) {\n@@ -272,1 +425,9 @@\n-    @Run(test = \"testByteByte4\")\n+    @Test\n+    @IR(failOn = { IRNode.LOAD_VECTOR_L, IRNode.STORE_VECTOR })\n+    public static void testByteByte4b(byte[] dest, byte[] src, int start, int stop) {\n+        for (int i = start; i < stop; i++) {\n+            UNSAFE.putLongUnaligned(dest, 8L * i + baseOffset, UNSAFE.getLongUnaligned(src, UNSAFE.ARRAY_BYTE_BASE_OFFSET + 8L * i));\n+        }\n+    }\n+\n+    @Run(test = {\"testByteByte4a\", \"testByteByte4b\"})\n@@ -275,1 +436,2 @@\n-        runAndVerify2(() -> testByteByte4(byteArray, byteArray, 0, size), 0);\n+        runAndVerify2(() -> testByteByte4a(byteArray, byteArray, 0, size), 0);\n+        runAndVerify2(() -> testByteByte4b(byteArray, byteArray, 0, size), 0);\n@@ -280,1 +442,1 @@\n-    public static void testByteByte5(byte[] dest, byte[] src, int start, int stop) {\n+    public static void testByteByte5a(byte[] dest, byte[] src, int start, int stop) {\n@@ -286,1 +448,9 @@\n-    @Run(test = \"testByteByte5\")\n+    @Test\n+    @IR(failOn = { IRNode.LOAD_VECTOR_L, IRNode.STORE_VECTOR })\n+    public static void testByteByte5b(byte[] dest, byte[] src, int start, int stop) {\n+        for (int i = start; i < stop; i++) {\n+            UNSAFE.putLongUnaligned(dest, UNSAFE.ARRAY_BYTE_BASE_OFFSET + 8L * (i + baseOffset), UNSAFE.getLongUnaligned(src, UNSAFE.ARRAY_BYTE_BASE_OFFSET + 8L * i));\n+        }\n+    }\n+\n+    @Run(test = {\"testByteByte5a\", \"testByteByte5b\"})\n@@ -289,1 +459,2 @@\n-        runAndVerify2(() -> testByteByte5(byteArray, byteArray, 0, size-1), 8);\n+        runAndVerify2(() -> testByteByte5a(byteArray, byteArray, 0, size-1), 8);\n+        runAndVerify2(() -> testByteByte5b(byteArray, byteArray, 0, size-1), 8);\n@@ -293,2 +464,5 @@\n-    @IR(counts = { IRNode.LOAD_VECTOR_L, \">=1\", IRNode.STORE_VECTOR, \">=1\" })\n-    public static void testOffHeapLong1(long dest, long[] src) {\n+    @IR(counts = { IRNode.LOAD_VECTOR_L, \"=0\", IRNode.STORE_VECTOR, \"=0\" }) \/\/ temporary\n+    \/\/ @IR(counts = { IRNode.LOAD_VECTOR_L, \">=1\", IRNode.STORE_VECTOR, \">=1\" })\n+    \/\/ FAILS: adr is CastX2P(dest + 8 * (i + int_con))\n+    \/\/ See: JDK-8331576\n+    public static void testOffHeapLong1a(long dest, long[] src) {\n@@ -300,1 +474,12 @@\n-    @Run(test = \"testOffHeapLong1\")\n+    @Test\n+    @IR(counts = { IRNode.LOAD_VECTOR_L, \"=0\", IRNode.STORE_VECTOR, \"=0\" }) \/\/ temporary\n+    \/\/ @IR(counts = { IRNode.LOAD_VECTOR_L, \">=1\", IRNode.STORE_VECTOR, \">=1\" })\n+    \/\/ FAILS: adr is CastX2P(dest + 8L * (i + int_con))\n+    \/\/ See: JDK-8331576\n+    public static void testOffHeapLong1b(long dest, long[] src) {\n+        for (int i = 0; i < src.length; i++) {\n+            UNSAFE.putLongUnaligned(null, dest + 8L * i, src[i]);\n+        }\n+    }\n+\n+    @Run(test = {\"testOffHeapLong1a\", \"testOffHeapLong1b\"})\n@@ -302,1 +487,2 @@\n-        runAndVerify3(() -> testOffHeapLong1(baseOffHeap, longArray), 0);\n+        runAndVerify3(() -> testOffHeapLong1a(baseOffHeap, longArray), 0);\n+        runAndVerify3(() -> testOffHeapLong1b(baseOffHeap, longArray), 0);\n@@ -306,2 +492,5 @@\n-    @IR(counts = { IRNode.LOAD_VECTOR_L, \">=1\", IRNode.STORE_VECTOR, \">=1\" })\n-    public static void testOffHeapLong2(long dest, long[] src) {\n+    @IR(counts = { IRNode.LOAD_VECTOR_L, \"=0\", IRNode.STORE_VECTOR, \"=0\" }) \/\/ temporary\n+    \/\/ @IR(counts = { IRNode.LOAD_VECTOR_L, \">=1\", IRNode.STORE_VECTOR, \">=1\" })\n+    \/\/ FAILS: adr is CastX2P\n+    \/\/ See: JDK-8331576\n+    public static void testOffHeapLong2a(long dest, long[] src) {\n@@ -313,1 +502,12 @@\n-    @Run(test = \"testOffHeapLong2\")\n+    @Test\n+    @IR(counts = { IRNode.LOAD_VECTOR_L, \"=0\", IRNode.STORE_VECTOR, \"=0\" }) \/\/ temporary\n+    \/\/ @IR(counts = { IRNode.LOAD_VECTOR_L, \">=1\", IRNode.STORE_VECTOR, \">=1\" })\n+    \/\/ FAILS: adr is CastX2P\n+    \/\/ See: JDK-8331576\n+    public static void testOffHeapLong2b(long dest, long[] src) {\n+        for (int i = 1; i < src.length; i++) {\n+            UNSAFE.putLongUnaligned(null, dest + 8L * (i - 1), src[i]);\n+        }\n+    }\n+\n+    @Run(test = {\"testOffHeapLong2a\", \"testOffHeapLong2b\"})\n@@ -315,1 +515,2 @@\n-        runAndVerify3(() -> testOffHeapLong2(baseOffHeap, longArray), -8);\n+        runAndVerify3(() -> testOffHeapLong2a(baseOffHeap, longArray), -8);\n+        runAndVerify3(() -> testOffHeapLong2b(baseOffHeap, longArray), -8);\n@@ -319,2 +520,5 @@\n-    @IR(counts = { IRNode.LOAD_VECTOR_L, \">=1\", IRNode.STORE_VECTOR, \">=1\" })\n-    public static void testOffHeapLong3(long dest, long[] src) {\n+    @IR(counts = { IRNode.LOAD_VECTOR_L, \"=0\", IRNode.STORE_VECTOR, \"=0\" }) \/\/ temporary\n+    \/\/ @IR(counts = { IRNode.LOAD_VECTOR_L, \">=1\", IRNode.STORE_VECTOR, \">=1\" })\n+    \/\/ FAILS: adr is CastX2P\n+    \/\/ See: JDK-8331576\n+    public static void testOffHeapLong3a(long dest, long[] src) {\n@@ -326,1 +530,12 @@\n-    @Run(test = \"testOffHeapLong3\")\n+    @Test\n+    @IR(counts = { IRNode.LOAD_VECTOR_L, \"=0\", IRNode.STORE_VECTOR, \"=0\" }) \/\/ temporary\n+    \/\/ @IR(counts = { IRNode.LOAD_VECTOR_L, \">=1\", IRNode.STORE_VECTOR, \">=1\" })\n+    \/\/ FAILS: adr is CastX2P\n+    \/\/ See: JDK-8331576\n+    public static void testOffHeapLong3b(long dest, long[] src) {\n+        for (int i = 0; i < src.length - 1; i++) {\n+            UNSAFE.putLongUnaligned(null, dest + 8L * (i + 1), src[i]);\n+        }\n+    }\n+\n+    @Run(test = {\"testOffHeapLong3a\", \"testOffHeapLong3b\"})\n@@ -328,1 +543,2 @@\n-        runAndVerify3(() -> testOffHeapLong3(baseOffHeap, longArray), 8);\n+        runAndVerify3(() -> testOffHeapLong3a(baseOffHeap, longArray), 8);\n+        runAndVerify3(() -> testOffHeapLong3b(baseOffHeap, longArray), 8);\n@@ -332,2 +548,5 @@\n-    @IR(counts = { IRNode.LOAD_VECTOR_L, \">=1\", IRNode.STORE_VECTOR, \">=1\" })\n-    public static void testOffHeapLong4(long dest, long[] src, int start, int stop) {\n+    @IR(counts = { IRNode.LOAD_VECTOR_L, \"=0\", IRNode.STORE_VECTOR, \"=0\" }) \/\/ temporary\n+    \/\/ @IR(counts = { IRNode.LOAD_VECTOR_L, \">=1\", IRNode.STORE_VECTOR, \">=1\" })\n+    \/\/ FAILS: adr is CastX2P\n+    \/\/ See: JDK-8331576\n+    public static void testOffHeapLong4a(long dest, long[] src, int start, int stop) {\n@@ -339,1 +558,12 @@\n-    @Run(test = \"testOffHeapLong4\")\n+    @Test\n+    @IR(counts = { IRNode.LOAD_VECTOR_L, \"=0\", IRNode.STORE_VECTOR, \"=0\" }) \/\/ temporary\n+    \/\/ @IR(counts = { IRNode.LOAD_VECTOR_L, \">=1\", IRNode.STORE_VECTOR, \">=1\" })\n+    \/\/ FAILS: adr is CastX2P\n+    \/\/ See: JDK-8331576\n+    public static void testOffHeapLong4b(long dest, long[] src, int start, int stop) {\n+        for (int i = start; i < stop; i++) {\n+            UNSAFE.putLongUnaligned(null, dest + 8L * i + baseOffset, src[i]);\n+        }\n+    }\n+\n+    @Run(test = {\"testOffHeapLong4a\", \"testOffHeapLong4b\"})\n@@ -342,1 +572,2 @@\n-        runAndVerify3(() -> testOffHeapLong4(baseOffHeap, longArray, 0, size-1), 8);\n+        runAndVerify3(() -> testOffHeapLong4a(baseOffHeap, longArray, 0, size-1), 8);\n+        runAndVerify3(() -> testOffHeapLong4b(baseOffHeap, longArray, 0, size-1), 8);\n","filename":"test\/hotspot\/jtreg\/compiler\/c2\/irTests\/TestVectorizationMismatchedAccess.java","additions":281,"deletions":50,"binary":false,"changes":331,"status":"modified"},{"patch":"@@ -174,4 +174,4 @@\n-            unsafe.putInt(dataFa, unsafe.ARRAY_FLOAT_BASE_OFFSET + 4 * i + 0, dataIa[i+0] + 1);\n-            unsafe.putInt(dataFa, unsafe.ARRAY_FLOAT_BASE_OFFSET + 4 * i + 4, dataIa[i+1] + 1);\n-            dataIb[i+0] = 11 * unsafe.getInt(dataFb, unsafe.ARRAY_INT_BASE_OFFSET + 4 * i + 0);\n-            dataIb[i+1] = 11 * unsafe.getInt(dataFb, unsafe.ARRAY_INT_BASE_OFFSET + 4 * i + 4);\n+            unsafe.putInt(dataFa, unsafe.ARRAY_FLOAT_BASE_OFFSET + 4L * i + 0, dataIa[i+0] + 1);\n+            unsafe.putInt(dataFa, unsafe.ARRAY_FLOAT_BASE_OFFSET + 4L * i + 4, dataIa[i+1] + 1);\n+            dataIb[i+0] = 11 * unsafe.getInt(dataFb, unsafe.ARRAY_INT_BASE_OFFSET + 4L * i + 0);\n+            dataIb[i+1] = 11 * unsafe.getInt(dataFb, unsafe.ARRAY_INT_BASE_OFFSET + 4L * i + 4);\n@@ -249,4 +249,4 @@\n-            unsafe.putInt(dataFa, unsafe.ARRAY_FLOAT_BASE_OFFSET + 4 * i + 0, dataIa[i+0] + 1); \/\/ A\n-            dataIb[i+0] = 11 * unsafe.getInt(dataFb, unsafe.ARRAY_INT_BASE_OFFSET + 4 * i + 0); \/\/ X\n-            dataIb[i+1] = 11 * unsafe.getInt(dataFb, unsafe.ARRAY_INT_BASE_OFFSET + 4 * i + 4); \/\/ Y\n-            unsafe.putInt(dataFa, unsafe.ARRAY_FLOAT_BASE_OFFSET + 4 * i + 4, dataIa[i+1] + 1); \/\/ B\n+            unsafe.putInt(dataFa, unsafe.ARRAY_FLOAT_BASE_OFFSET + 4L * i + 0, dataIa[i+0] + 1); \/\/ A\n+            dataIb[i+0] = 11 * unsafe.getInt(dataFb, unsafe.ARRAY_INT_BASE_OFFSET + 4L * i + 0); \/\/ X\n+            dataIb[i+1] = 11 * unsafe.getInt(dataFb, unsafe.ARRAY_INT_BASE_OFFSET + 4L * i + 4); \/\/ Y\n+            unsafe.putInt(dataFa, unsafe.ARRAY_FLOAT_BASE_OFFSET + 4L * i + 4, dataIa[i+1] + 1); \/\/ B\n@@ -276,12 +276,12 @@\n-            int v00 = unsafe.getInt(dataIa, unsafe.ARRAY_INT_BASE_OFFSET + 4 * i + 0) + 3;\n-            int v01 = unsafe.getInt(dataIa, unsafe.ARRAY_INT_BASE_OFFSET + 4 * i + 4) + 3;\n-            unsafe.putInt(dataFa, unsafe.ARRAY_FLOAT_BASE_OFFSET + 4 * i + 0, v00);\n-            unsafe.putInt(dataFa, unsafe.ARRAY_FLOAT_BASE_OFFSET + 4 * i + 4, v01);\n-            int v10 = unsafe.getInt(dataFb, unsafe.ARRAY_FLOAT_BASE_OFFSET + 4 * i + 0) * 45;\n-            int v11 = unsafe.getInt(dataFb, unsafe.ARRAY_FLOAT_BASE_OFFSET + 4 * i + 4) * 45;\n-            unsafe.putInt(dataLa, unsafe.ARRAY_LONG_BASE_OFFSET + 4 * i + 0, v10);\n-            unsafe.putInt(dataLa, unsafe.ARRAY_LONG_BASE_OFFSET + 4 * i + 4, v11);\n-            float v20 = unsafe.getFloat(dataLb, unsafe.ARRAY_LONG_BASE_OFFSET + 4 * i + 0) + 0.55f;\n-            float v21 = unsafe.getFloat(dataLb, unsafe.ARRAY_LONG_BASE_OFFSET + 4 * i + 4) + 0.55f;\n-            unsafe.putFloat(dataIb, unsafe.ARRAY_INT_BASE_OFFSET + 4 * i + 0, v20);\n-            unsafe.putFloat(dataIb, unsafe.ARRAY_INT_BASE_OFFSET + 4 * i + 4, v21);\n+            int v00 = unsafe.getInt(dataIa, unsafe.ARRAY_INT_BASE_OFFSET + 4L * i + 0) + 3;\n+            int v01 = unsafe.getInt(dataIa, unsafe.ARRAY_INT_BASE_OFFSET + 4L * i + 4) + 3;\n+            unsafe.putInt(dataFa, unsafe.ARRAY_FLOAT_BASE_OFFSET + 4L * i + 0, v00);\n+            unsafe.putInt(dataFa, unsafe.ARRAY_FLOAT_BASE_OFFSET + 4L * i + 4, v01);\n+            int v10 = unsafe.getInt(dataFb, unsafe.ARRAY_FLOAT_BASE_OFFSET + 4L * i + 0) * 45;\n+            int v11 = unsafe.getInt(dataFb, unsafe.ARRAY_FLOAT_BASE_OFFSET + 4L * i + 4) * 45;\n+            unsafe.putInt(dataLa, unsafe.ARRAY_LONG_BASE_OFFSET + 4L * i + 0, v10);\n+            unsafe.putInt(dataLa, unsafe.ARRAY_LONG_BASE_OFFSET + 4L * i + 4, v11);\n+            float v20 = unsafe.getFloat(dataLb, unsafe.ARRAY_LONG_BASE_OFFSET + 4L * i + 0) + 0.55f;\n+            float v21 = unsafe.getFloat(dataLb, unsafe.ARRAY_LONG_BASE_OFFSET + 4L * i + 4) + 0.55f;\n+            unsafe.putFloat(dataIb, unsafe.ARRAY_INT_BASE_OFFSET + 4L * i + 0, v20);\n+            unsafe.putFloat(dataIb, unsafe.ARRAY_INT_BASE_OFFSET + 4L * i + 4, v21);\n@@ -308,12 +308,12 @@\n-            int v00 = unsafe.getInt(dataIa, unsafe.ARRAY_INT_BASE_OFFSET + 4 * i + 0) + 3;\n-            unsafe.putInt(dataFa, unsafe.ARRAY_FLOAT_BASE_OFFSET + 4 * i + 0, v00);\n-            int v10 = unsafe.getInt(dataFb, unsafe.ARRAY_FLOAT_BASE_OFFSET + 4 * i + 0) * 45;\n-            int v11 = unsafe.getInt(dataFb, unsafe.ARRAY_FLOAT_BASE_OFFSET + 4 * i + 4) * 45;\n-            unsafe.putInt(dataLa, unsafe.ARRAY_LONG_BASE_OFFSET + 4 * i + 0, v10);\n-            unsafe.putInt(dataLa, unsafe.ARRAY_LONG_BASE_OFFSET + 4 * i + 4, v11);\n-            float v20 = unsafe.getFloat(dataLb, unsafe.ARRAY_LONG_BASE_OFFSET + 4 * i + 0) + 0.55f;\n-            float v21 = unsafe.getFloat(dataLb, unsafe.ARRAY_LONG_BASE_OFFSET + 4 * i + 4) + 0.55f;\n-            unsafe.putFloat(dataIb, unsafe.ARRAY_INT_BASE_OFFSET + 4 * i + 0, v20);\n-            unsafe.putFloat(dataIb, unsafe.ARRAY_INT_BASE_OFFSET + 4 * i + 4, v21);\n-            int v01 = unsafe.getInt(dataIa, unsafe.ARRAY_INT_BASE_OFFSET + 4 * i + 4) + 3; \/\/ moved down\n-            unsafe.putInt(dataFa, unsafe.ARRAY_FLOAT_BASE_OFFSET + 4 * i + 4, v01);\n+            int v00 = unsafe.getInt(dataIa, unsafe.ARRAY_INT_BASE_OFFSET + 4L * i + 0) + 3;\n+            unsafe.putInt(dataFa, unsafe.ARRAY_FLOAT_BASE_OFFSET + 4L * i + 0, v00);\n+            int v10 = unsafe.getInt(dataFb, unsafe.ARRAY_FLOAT_BASE_OFFSET + 4L * i + 0) * 45;\n+            int v11 = unsafe.getInt(dataFb, unsafe.ARRAY_FLOAT_BASE_OFFSET + 4L * i + 4) * 45;\n+            unsafe.putInt(dataLa, unsafe.ARRAY_LONG_BASE_OFFSET + 4L * i + 0, v10);\n+            unsafe.putInt(dataLa, unsafe.ARRAY_LONG_BASE_OFFSET + 4L * i + 4, v11);\n+            float v20 = unsafe.getFloat(dataLb, unsafe.ARRAY_LONG_BASE_OFFSET + 4L * i + 0) + 0.55f;\n+            float v21 = unsafe.getFloat(dataLb, unsafe.ARRAY_LONG_BASE_OFFSET + 4L * i + 4) + 0.55f;\n+            unsafe.putFloat(dataIb, unsafe.ARRAY_INT_BASE_OFFSET + 4L * i + 0, v20);\n+            unsafe.putFloat(dataIb, unsafe.ARRAY_INT_BASE_OFFSET + 4L * i + 4, v21);\n+            int v01 = unsafe.getInt(dataIa, unsafe.ARRAY_INT_BASE_OFFSET + 4L * i + 4) + 3; \/\/ moved down\n+            unsafe.putInt(dataFa, unsafe.ARRAY_FLOAT_BASE_OFFSET + 4L * i + 4, v01);\n@@ -341,8 +341,8 @@\n-            int v00 = unsafe.getInt(dataIa, unsafe.ARRAY_INT_BASE_OFFSET + 4 * i + 0) + 3;\n-            unsafe.putInt(dataFa, unsafe.ARRAY_FLOAT_BASE_OFFSET + 4 * i + 0, v00);\n-            int v10 = unsafe.getInt(dataFb, unsafe.ARRAY_FLOAT_BASE_OFFSET + 4 * i + 0) * 45;\n-            int v11 = unsafe.getInt(dataFb, unsafe.ARRAY_FLOAT_BASE_OFFSET + 4 * i + 4) * 45;\n-            unsafe.putInt(dataLa, unsafe.ARRAY_LONG_BASE_OFFSET + 4 * i + 0, v10);\n-            unsafe.putInt(dataLa, unsafe.ARRAY_LONG_BASE_OFFSET + 4 * i + 4, v11);\n-            int v01 = unsafe.getInt(dataIa, unsafe.ARRAY_INT_BASE_OFFSET + 4 * i + 4) + 3;\n-            unsafe.putInt(dataFa, unsafe.ARRAY_FLOAT_BASE_OFFSET + 4 * i + 4, v01);\n+            int v00 = unsafe.getInt(dataIa, unsafe.ARRAY_INT_BASE_OFFSET + 4L * i + 0) + 3;\n+            unsafe.putInt(dataFa, unsafe.ARRAY_FLOAT_BASE_OFFSET + 4L * i + 0, v00);\n+            int v10 = unsafe.getInt(dataFb, unsafe.ARRAY_FLOAT_BASE_OFFSET + 4L * i + 0) * 45;\n+            int v11 = unsafe.getInt(dataFb, unsafe.ARRAY_FLOAT_BASE_OFFSET + 4L * i + 4) * 45;\n+            unsafe.putInt(dataLa, unsafe.ARRAY_LONG_BASE_OFFSET + 4L * i + 0, v10);\n+            unsafe.putInt(dataLa, unsafe.ARRAY_LONG_BASE_OFFSET + 4L * i + 4, v11);\n+            int v01 = unsafe.getInt(dataIa, unsafe.ARRAY_INT_BASE_OFFSET + 4L * i + 4) + 3;\n+            unsafe.putInt(dataFa, unsafe.ARRAY_FLOAT_BASE_OFFSET + 4L * i + 4, v01);\n@@ -350,4 +350,4 @@\n-            float v20 = unsafe.getFloat(dataLb, unsafe.ARRAY_LONG_BASE_OFFSET + 4 * i + 0) + 0.55f;\n-            float v21 = unsafe.getFloat(dataLb, unsafe.ARRAY_LONG_BASE_OFFSET + 4 * i + 4) + 0.55f;\n-            unsafe.putFloat(dataIb, unsafe.ARRAY_INT_BASE_OFFSET + 4 * i + 0, v20);\n-            unsafe.putFloat(dataIb, unsafe.ARRAY_INT_BASE_OFFSET + 4 * i + 4, v21);\n+            float v20 = unsafe.getFloat(dataLb, unsafe.ARRAY_LONG_BASE_OFFSET + 4L * i + 0) + 0.55f;\n+            float v21 = unsafe.getFloat(dataLb, unsafe.ARRAY_LONG_BASE_OFFSET + 4L * i + 4) + 0.55f;\n+            unsafe.putFloat(dataIb, unsafe.ARRAY_INT_BASE_OFFSET + 4L * i + 0, v20);\n+            unsafe.putFloat(dataIb, unsafe.ARRAY_INT_BASE_OFFSET + 4L * i + 4, v21);\n@@ -374,4 +374,4 @@\n-            float v20 = unsafe.getFloat(dataLb, unsafe.ARRAY_LONG_BASE_OFFSET + 4 * i + 0) + 0.55f;\n-            float v21 = unsafe.getFloat(dataLb, unsafe.ARRAY_LONG_BASE_OFFSET + 4 * i + 4) + 0.55f;\n-            unsafe.putFloat(dataIb, unsafe.ARRAY_INT_BASE_OFFSET + 4 * i + 0, v20);\n-            unsafe.putFloat(dataIb, unsafe.ARRAY_INT_BASE_OFFSET + 4 * i + 4, v21);\n+            float v20 = unsafe.getFloat(dataLb, unsafe.ARRAY_LONG_BASE_OFFSET + 4L * i + 0) + 0.55f;\n+            float v21 = unsafe.getFloat(dataLb, unsafe.ARRAY_LONG_BASE_OFFSET + 4L * i + 4) + 0.55f;\n+            unsafe.putFloat(dataIb, unsafe.ARRAY_INT_BASE_OFFSET + 4L * i + 0, v20);\n+            unsafe.putFloat(dataIb, unsafe.ARRAY_INT_BASE_OFFSET + 4L * i + 4, v21);\n@@ -379,8 +379,8 @@\n-            int v00 = unsafe.getInt(dataIa, unsafe.ARRAY_INT_BASE_OFFSET + 4 * i + 0) + 3;\n-            unsafe.putInt(dataFa, unsafe.ARRAY_FLOAT_BASE_OFFSET + 4 * i + 0, v00);\n-            int v10 = unsafe.getInt(dataFb, unsafe.ARRAY_FLOAT_BASE_OFFSET + 4 * i + 0) * 45;\n-            int v11 = unsafe.getInt(dataFb, unsafe.ARRAY_FLOAT_BASE_OFFSET + 4 * i + 4) * 45;\n-            unsafe.putInt(dataLa, unsafe.ARRAY_LONG_BASE_OFFSET + 4 * i + 0, v10);\n-            unsafe.putInt(dataLa, unsafe.ARRAY_LONG_BASE_OFFSET + 4 * i + 4, v11);\n-            int v01 = unsafe.getInt(dataIa, unsafe.ARRAY_INT_BASE_OFFSET + 4 * i + 4) + 3;\n-            unsafe.putInt(dataFa, unsafe.ARRAY_FLOAT_BASE_OFFSET + 4 * i + 4, v01);\n+            int v00 = unsafe.getInt(dataIa, unsafe.ARRAY_INT_BASE_OFFSET + 4L * i + 0) + 3;\n+            unsafe.putInt(dataFa, unsafe.ARRAY_FLOAT_BASE_OFFSET + 4L * i + 0, v00);\n+            int v10 = unsafe.getInt(dataFb, unsafe.ARRAY_FLOAT_BASE_OFFSET + 4L * i + 0) * 45;\n+            int v11 = unsafe.getInt(dataFb, unsafe.ARRAY_FLOAT_BASE_OFFSET + 4L * i + 4) * 45;\n+            unsafe.putInt(dataLa, unsafe.ARRAY_LONG_BASE_OFFSET + 4L * i + 0, v10);\n+            unsafe.putInt(dataLa, unsafe.ARRAY_LONG_BASE_OFFSET + 4L * i + 4, v11);\n+            int v01 = unsafe.getInt(dataIa, unsafe.ARRAY_INT_BASE_OFFSET + 4L * i + 4) + 3;\n+            unsafe.putInt(dataFa, unsafe.ARRAY_FLOAT_BASE_OFFSET + 4L * i + 4, v01);\n@@ -424,12 +424,12 @@\n-            int v00 = unsafe.getInt(dataIa, unsafe.ARRAY_INT_BASE_OFFSET + 4 * i + 0) + 3; \/\/ A\n-            unsafe.putInt(dataFa, unsafe.ARRAY_FLOAT_BASE_OFFSET + 4 * i + 0, v00);\n-            int v10 = unsafe.getInt(dataFb, unsafe.ARRAY_FLOAT_BASE_OFFSET + 4 * i + 0) * 45; \/\/ R: constant mismatch\n-            int v11 = unsafe.getInt(dataFb, unsafe.ARRAY_FLOAT_BASE_OFFSET + 4 * i + 4) + 43; \/\/ S\n-            unsafe.putInt(dataLa, unsafe.ARRAY_LONG_BASE_OFFSET + 4 * i + 0, v10);\n-            unsafe.putInt(dataLa, unsafe.ARRAY_LONG_BASE_OFFSET + 4 * i + 4, v11);\n-            float v20 = unsafe.getFloat(dataLb, unsafe.ARRAY_LONG_BASE_OFFSET + 4 * i + 0) + 0.55f; \/\/ U\n-            float v21 = unsafe.getFloat(dataLb, unsafe.ARRAY_LONG_BASE_OFFSET + 4 * i + 4) + 0.55f; \/\/ V\n-            unsafe.putFloat(dataIb, unsafe.ARRAY_INT_BASE_OFFSET + 4 * i + 0, v20);\n-            unsafe.putFloat(dataIb, unsafe.ARRAY_INT_BASE_OFFSET + 4 * i + 4, v21);\n-            int v01 = unsafe.getInt(dataIa, unsafe.ARRAY_INT_BASE_OFFSET + 4 * i + 4) + 3; \/\/ B: moved down\n-            unsafe.putInt(dataFa, unsafe.ARRAY_FLOAT_BASE_OFFSET + 4 * i + 4, v01);\n+            int v00 = unsafe.getInt(dataIa, unsafe.ARRAY_INT_BASE_OFFSET + 4L * i + 0) + 3; \/\/ A\n+            unsafe.putInt(dataFa, unsafe.ARRAY_FLOAT_BASE_OFFSET + 4L * i + 0, v00);\n+            int v10 = unsafe.getInt(dataFb, unsafe.ARRAY_FLOAT_BASE_OFFSET + 4L * i + 0) * 45; \/\/ R: constant mismatch\n+            int v11 = unsafe.getInt(dataFb, unsafe.ARRAY_FLOAT_BASE_OFFSET + 4L * i + 4) + 43; \/\/ S\n+            unsafe.putInt(dataLa, unsafe.ARRAY_LONG_BASE_OFFSET + 4L * i + 0, v10);\n+            unsafe.putInt(dataLa, unsafe.ARRAY_LONG_BASE_OFFSET + 4L * i + 4, v11);\n+            float v20 = unsafe.getFloat(dataLb, unsafe.ARRAY_LONG_BASE_OFFSET + 4L * i + 0) + 0.55f; \/\/ U\n+            float v21 = unsafe.getFloat(dataLb, unsafe.ARRAY_LONG_BASE_OFFSET + 4L * i + 4) + 0.55f; \/\/ V\n+            unsafe.putFloat(dataIb, unsafe.ARRAY_INT_BASE_OFFSET + 4L * i + 0, v20);\n+            unsafe.putFloat(dataIb, unsafe.ARRAY_INT_BASE_OFFSET + 4L * i + 4, v21);\n+            int v01 = unsafe.getInt(dataIa, unsafe.ARRAY_INT_BASE_OFFSET + 4L * i + 4) + 3; \/\/ B: moved down\n+            unsafe.putInt(dataFa, unsafe.ARRAY_FLOAT_BASE_OFFSET + 4L * i + 4, v01);\n@@ -464,2 +464,2 @@\n-            int datav = unsafe.getInt(data, unsafe.ARRAY_FLOAT_BASE_OFFSET + 4 * i);\n-            int goldv = unsafe.getInt(gold, unsafe.ARRAY_FLOAT_BASE_OFFSET + 4 * i);\n+            int datav = unsafe.getInt(data, unsafe.ARRAY_FLOAT_BASE_OFFSET + 4L * i);\n+            int goldv = unsafe.getInt(gold, unsafe.ARRAY_FLOAT_BASE_OFFSET + 4L * i);\n","filename":"test\/hotspot\/jtreg\/compiler\/loopopts\/superword\/TestIndependentPacksWithCyclicDependency.java","additions":70,"deletions":70,"binary":false,"changes":140,"status":"modified"},{"patch":"@@ -2,1 +2,1 @@\n- * Copyright (c) 2016, 2022, Oracle and\/or its affiliates. All rights reserved.\n+ * Copyright (c) 2016, 2024, Oracle and\/or its affiliates. All rights reserved.\n@@ -29,1 +29,1 @@\n- * @requires !vm.flightRecorder\n+ * @requires vm.flagless\n@@ -123,1 +123,1 @@\n-            OutputAnalyzer out = ProcessTools.executeTestJvm(options);\n+            OutputAnalyzer out = ProcessTools.executeTestJava(options);\n","filename":"test\/hotspot\/jtreg\/gc\/g1\/plab\/TestPLABPromotion.java","additions":3,"deletions":3,"binary":false,"changes":6,"status":"modified"},{"patch":"@@ -934,1 +934,1 @@\n-            inflatorThread = TestScaffold.newThread(() -> {\n+            inflatorThread = DebuggeeWrapper.newThread(() -> {\n","filename":"test\/jdk\/com\/sun\/jdi\/EATests.java","additions":1,"deletions":1,"binary":false,"changes":2,"status":"modified"}]}