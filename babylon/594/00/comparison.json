{"files":[{"patch":"@@ -339,206 +339,0 @@\n-    private interface SharedWithPad extends Buffer {\n-        void array(long index, float value);\n-        float array(long index);\n-        Schema<SharedWithPad> schema = Schema.of(SharedWithPad.class,\n-                arr -> arr.array(\"array\", 1088));   \/\/ The size is BLOCK_M * (TILE_K + 1) to mitigate memory bank conflicts\n-        static SharedWithPad create(Accelerator accelerator) {\n-            return schema.allocate(accelerator);\n-        }\n-        static SharedWithPad createLocal() {\n-            return schema.allocate(new Accelerator(MethodHandles.lookup(), Backend.FIRST));\n-        }\n-    }\n-\n-    private interface SharedWithPadB extends Buffer {\n-        void array(long index, float value);\n-        float array(long index);\n-        Schema<SharedWithPadB> schema = Schema.of(SharedWithPadB.class,\n-                arr -> arr.array(\"array\", 1040));    \/\/ The size is TILE_K * (BLOCk_M + 1) to mitigate memory bank conflicts\n-        static SharedWithPadB create(Accelerator accelerator) {\n-            return schema.allocate(accelerator);\n-        }\n-        static SharedWithPadB createLocal() {\n-            return schema.allocate(new Accelerator(MethodHandles.lookup(), Backend.FIRST));\n-        }\n-    }\n-\n-    private interface PrivateAcc extends Buffer {\n-        void array(long index, float value);\n-        float array(long index);\n-        Schema<PrivateAcc> schema = Schema.of(PrivateAcc.class,\n-                arr -> arr.array(\"array\", 16));\n-        static PrivateAcc create(Accelerator accelerator) {\n-            return schema.allocate(accelerator);\n-        }\n-        static PrivateAcc createPrivate() {\n-            return schema.allocate(new Accelerator(MethodHandles.lookup(), Backend.FIRST));\n-        }\n-    }\n-\n-    private interface PrivateReg extends Buffer {\n-        void array(long index, float value);\n-        float array(long index);\n-        Schema<PrivateReg> schema = Schema.of(PrivateReg.class,\n-                arr -> arr.array(\"array\", 4));\n-        static PrivateReg create(Accelerator accelerator) {\n-            return schema.allocate(accelerator);\n-        }\n-        static PrivateReg createPrivate() {\n-            return schema.allocate(new Accelerator(MethodHandles.lookup(), Backend.FIRST));\n-        }\n-    }\n-\n-    \/**\n-     * The following algorithm is a variant of the previous kernel to show register tiling and 2D thread-block. This algorithm\n-     * is portable across CUDA and OpenCL backend of the HAT compiler\/runtime. It can be tuned by changing the configuration\n-     * parameters at the beginning of the function. Some notes for auto-tuning:\n-     * - Global size must be multiples of local group, and cover (BLOCK_M x BLOCK_N) per group\n-     * - BLOCK_M = WG_M * REG_M, BLOCK_N = WG_N * REG_N\n-     *\n-     * <p>\n-     * Some possible values for tuning: WG_M {8 .. 32}, and WG_N {8 .. 32}, REG_M {1 .. 4}, REG_N {1 .. 4}, TILE_K {16 .. 128}.\n-     * Depending on the GPU used, some of the configuration values might be too high\/illegal.\n-     * <\/p>\n-     *\n-     * <p>\n-     * The goal of this example is to show how HAT could be used to express more complex algorithm and optimizations for GPUs,\n-     * and how HAT could table performance tuning.\n-     * <\/p>\n-     *\n-     * @param kc\n-     * @param matrixA\n-     * @param matrixB\n-     * @param matrixC\n-     * @param size\n-     *\/\n-    @CodeReflection\n-    public static void matrixMultiplyKernel2DRegisterTilingPortable(@RO KernelContext kc, @RO F32Array matrixA, @RO F32Array matrixB, @RW F32Array matrixC, int size) {\n-\n-        \/\/ Configuration for this kernel\n-        final int WG_M = 16;\n-        final int WG_N = 16;\n-        final int REG_M = 4;\n-        final int REG_N = 4;\n-        final int TILE_K = 16;\n-        final int BLOCK_M = WG_M * REG_M;\n-        final int BLOCK_N = WG_N * REG_N;\n-\n-        \/\/ We compute squares matrices for simplification. But the code is\n-        \/\/ prepared to compute any compatible matmul sizes\n-        final int M = size;\n-        final int N = size;\n-        final int K = size;\n-        final int lda = size;\n-        final int ldb = size;\n-        final int ldc = size;\n-\n-        final int lid_m = kc.lix;\n-        final int lid_n = kc.liy;\n-        final int gid_m = kc.bix;\n-        final int gid_n = kc.biy;\n-\n-        \/\/ starting index of the tile in the matrix\n-        final int blockRow = gid_m * BLOCK_M;\n-        final int blockCol = gid_n * BLOCK_N;\n-\n-        \/\/ Within the block, each thread computes REG_M x REG_N micro-tile\n-        \/\/ start at (blockRow + lid_m * REG_M) and (blockCol + lid_n * REG_N)\n-        final int cRowBase = blockRow + lid_m * REG_M;\n-        final int cColBase = blockCol + lid_n * REG_N;\n-\n-        \/\/ Accumulators in private memory. This is set to REG_M x REG_N values\n-        PrivateAcc acc = PrivateAcc.createPrivate();\n-        for (int i = 0; i < REG_M; i++) {\n-            for (int j = 0; j < REG_N; j++) {\n-                acc.array(i * REG_M + j, 0.0f);\n-            }\n-        }\n-\n-        \/\/ Shared memory to store tiles of matrixA and matrixB\n-        SharedWithPad sharedA = SharedWithPad.createLocal();\n-        SharedWithPadB sharedB = SharedWithPadB.createLocal();\n-\n-        \/\/ Padding scales to access local memory.\n-        \/\/ We add +1 to mitigate memory bank conflicts.\n-        final int padA = TILE_K + 1;\n-        final int padB = BLOCK_M + 1;\n-        for (int tileIndex = 0; tileIndex < K; tileIndex += TILE_K) {\n-            \/\/ Load A tile (Block_M x Tile_K)\n-            for (int i = 0; i < REG_M; i++) {\n-                final int aRow = cRowBase + i;\n-                for (int kk = lid_n; kk < TILE_K; kk += WG_N) {\n-                    final int aCol = tileIndex + kk;\n-                    float valA = 0.0f;\n-                    if (aRow < M && aCol < K) {\n-                        valA = matrixA.array(aRow * lda + aCol);\n-                    }\n-                    sharedA.array(((aRow - blockRow) * padA) + kk, valA);\n-                }\n-            }\n-\n-            \/\/ Load B tile (Tile_K x Block_N)\n-            for (int j = 0; j < REG_N; j++) {\n-                final int bCol = cColBase + j;\n-                for (int kk = lid_m; kk < TILE_K; kk += WG_M) {\n-                    final int bRow = tileIndex + kk;\n-                    float valB = 0.0f;\n-                    if (bRow < K && bCol < N) {\n-                        valB = matrixB.array(bRow * ldb + bCol);\n-                    }\n-                    sharedB.array((kk * padB) + (bCol - blockCol), valB);\n-                }\n-            }\n-            kc.barrier();\n-\n-            \/\/ Compute the tile acc += sharedA[:,k] * sharedB[k,:]\n-            for (int kk = 0; kk < TILE_K && (tileIndex + kk) < K; kk++) {\n-                \/\/ Load the A and B operands into registers to reuse for REG_M x REG_N accumulations\n-                PrivateReg aReg = PrivateReg.createPrivate();\n-                PrivateReg bReg = PrivateReg.createPrivate();\n-\n-                \/\/ Fetch a column kk of sharedA to private memory\n-                for (int i = 0; i < REG_M; i++) {\n-                    final int aRowL = (cRowBase + i) - blockRow;\n-                    float valPrivA = 0.0f;\n-                    if ((cRowBase + i) < M) {\n-                        valPrivA = sharedA.array(aRowL * padA + kk);\n-                    }\n-                    aReg.array(i, valPrivA);\n-                }\n-\n-                \/\/ Fetch a row kk of sharedB to private memory\n-                for (int j = 0; j < REG_N; j++) {\n-                    final int bColL = (cColBase + j) - blockCol;\n-                    float valPrivB = 0.0f;\n-                    if ((cColBase + j) < N) {\n-                        valPrivB = sharedB.array(kk * padB + bColL);\n-                    }\n-                    bReg.array(j, valPrivB);\n-                }\n-\n-                \/\/ FMA over the register tile\n-                for (int i = 0; i < REG_M; i++) {\n-                    final float a_ik = aReg.array(i);\n-                    for (int j = 0; j < REG_N; j++) {\n-                        float valRes = acc.array(i * REG_M + j);\n-                        valRes += a_ik * bReg.array(j);\n-                        acc.array(i * REG_M + j, valRes);\n-                    }\n-                }\n-            }\n-            kc.barrier();\n-        }\n-        \/\/ Write back the result of the register tile into matrixC\n-        for (int i = 0; i < REG_M; i++) {\n-            final int row = cRowBase + i;\n-            if (row < M) {\n-                for (int j = 0; j < REG_N; j++) {\n-                    final int col = cColBase + j;\n-                    if (col < N) {\n-                        matrixC.array(row * ldc + col, acc.array(i * REG_M + j));\n-                    }\n-                }\n-            }\n-        }\n-    }\n-\n@@ -639,8 +433,0 @@\n-    @CodeReflection\n-    public static void matrixMultiply2DRegisterTilingPortable(@RO ComputeContext cc, @RO F32Array matrixA, @RO F32Array matrixB, @RW  F32Array matrixC, int globalSize) {\n-        ComputeRange cudaRange = new ComputeRange(new GlobalMesh2D(256, 256), new LocalMesh2D(16, 16));\n-        cc.dispatchKernel(cudaRange,\n-                kc -> matrixMultiplyKernel2DRegisterTilingPortable(kc, matrixA, matrixB, matrixC, globalSize)\n-        );\n-    }\n-\n@@ -673,1 +459,0 @@\n-        _2DREGISTER_TILING_PORTABLE,\n@@ -695,1 +480,0 @@\n-                case \"2DRTPORTABLE\" -> Configuration._2DREGISTER_TILING_PORTABLE;\n@@ -741,2 +525,0 @@\n-                case _2DREGISTER_TILING_PORTABLE -> accelerator.compute(cc ->\n-                        matrixMultiply2DRegisterTilingPortable(cc, matrixA, matrixB, matrixC, size));\n","filename":"hat\/examples\/matmul\/src\/main\/java\/matmul\/Main.java","additions":0,"deletions":218,"binary":false,"changes":218,"status":"modified"},{"patch":"@@ -401,226 +401,0 @@\n-    private interface SharedWithPad extends Buffer {\n-        void array(long index, float value);\n-        float array(long index);\n-        Schema<SharedWithPad> schema = Schema.of(SharedWithPad.class,\n-                arr -> arr.array(\"array\", 1088));   \/\/ The size is BLOCK_M * (TILE_K + 1) to mitigate memory bank conflicts\n-        static SharedWithPad create(Accelerator accelerator) {\n-            return schema.allocate(accelerator);\n-        }\n-        static SharedWithPad createLocal() {\n-            return schema.allocate(new Accelerator(MethodHandles.lookup(), Backend.FIRST));\n-        }\n-    }\n-\n-    private interface SharedWithPadB extends Buffer {\n-        void array(long index, float value);\n-        float array(long index);\n-        Schema<SharedWithPadB> schema = Schema.of(SharedWithPadB.class,\n-                arr -> arr.array(\"array\", 1040));    \/\/ The size is TILE_K * (BLOCk_M + 1) to mitigate memory bank conflicts\n-        static SharedWithPadB create(Accelerator accelerator) {\n-            return schema.allocate(accelerator);\n-        }\n-        static SharedWithPadB createLocal() {\n-            return schema.allocate(new Accelerator(MethodHandles.lookup(), Backend.FIRST));\n-        }\n-    }\n-\n-    private interface PrivateAcc extends Buffer {\n-        void array(long index, float value);\n-        float array(long index);\n-        Schema<PrivateAcc> schema = Schema.of(PrivateAcc.class,\n-                arr -> arr.array(\"array\", 16));\n-        static PrivateAcc create(Accelerator accelerator) {\n-            return schema.allocate(accelerator);\n-        }\n-        static PrivateAcc createPrivate() {\n-            return schema.allocate(new Accelerator(MethodHandles.lookup(), Backend.FIRST));\n-        }\n-    }\n-\n-    private interface PrivateReg extends Buffer {\n-        void array(long index, float value);\n-        float array(long index);\n-        Schema<PrivateReg> schema = Schema.of(PrivateReg.class,\n-                arr -> arr.array(\"array\", 4));\n-        static PrivateReg create(Accelerator accelerator) {\n-            return schema.allocate(accelerator);\n-        }\n-        static PrivateReg createPrivate() {\n-            return schema.allocate(new Accelerator(MethodHandles.lookup(), Backend.FIRST));\n-        }\n-    }\n-\n-    \/\/ Ported from the HAT examples module.\n-    @CodeReflection\n-    public static void matrixMultiplyKernel2DRegisterTilingPortable(@RO KernelContext kc, @RO F32Array matrixA, @RO F32Array matrixB, @RW F32Array matrixC, int size) {\n-\n-        \/\/ Configuration for this kernel\n-        final int WG_M = 16;\n-        final int WG_N = 16;\n-        final int REG_M = 4;\n-        final int REG_N = 4;\n-        final int TILE_K = 16;\n-        final int BLOCK_M = WG_M * REG_M;\n-        final int BLOCK_N = WG_N * REG_N;\n-\n-        \/\/ We compute squares matrices for simplification. But the code is\n-        \/\/ prepared to compute any compatible matmul sizes\n-        final int M = size;\n-        final int N = size;\n-        final int K = size;\n-        final int lda = size;\n-        final int ldb = size;\n-        final int ldc = size;\n-\n-        final int lid_m = kc.lix;\n-        final int lid_n = kc.liy;\n-        final int gid_m = kc.bix;\n-        final int gid_n = kc.biy;\n-\n-        \/\/ starting index of the tile in the matrix\n-        final int blockRow = gid_m * BLOCK_M;\n-        final int blockCol = gid_n * BLOCK_N;\n-\n-        \/\/ Within the block, each thread computes REG_M x REG_N micro-tile\n-        \/\/ start at (blockRow + lid_m * REG_M) and (blockCol + lid_n * REG_N)\n-        final int cRowBase = blockRow + lid_m * REG_M;\n-        final int cColBase = blockCol + lid_n * REG_N;\n-\n-        \/\/ Accumulators in private memory. This is set to REG_M x REG_N values\n-        PrivateAcc acc = PrivateAcc.createPrivate();\n-        for (int i = 0; i < REG_M; i++) {\n-            for (int j = 0; j < REG_N; j++) {\n-                acc.array(i * REG_M + j, 0.0f);\n-            }\n-        }\n-\n-        \/\/ Shared memory to store tiles of matrixA and matrixB\n-        SharedWithPad sharedA = SharedWithPad.createLocal();\n-        SharedWithPadB sharedB = SharedWithPadB.createLocal();\n-\n-        \/\/ Padding scales to access local memory.\n-        \/\/ We add +1 to mitigate memory bank conflicts.\n-        final int padA = TILE_K + 1;\n-        final int padB = BLOCK_M + 1;\n-        for (int tileIndex = 0; tileIndex < K; tileIndex += TILE_K) {\n-            \/\/ Load A tile (Block_M x Tile_K)\n-            for (int i = 0; i < REG_M; i++) {\n-                final int aRow = cRowBase + i;\n-                for (int kk = lid_n; kk < TILE_K; kk += WG_N) {\n-                    final int aCol = tileIndex + kk;\n-                    float valA = 0.0f;\n-                    if (aRow < M && aCol < K) {\n-                        valA = matrixA.array(aRow * lda + aCol);\n-                    }\n-                    sharedA.array(((aRow - blockRow) * padA) + kk, valA);\n-                }\n-            }\n-\n-            \/\/ Load B tile (Tile_K x Block_N)\n-            for (int j = 0; j < REG_N; j++) {\n-                final int bCol = cColBase + j;\n-                for (int kk = lid_m; kk < TILE_K; kk += WG_M) {\n-                    final int bRow = tileIndex + kk;\n-                    float valB = 0.0f;\n-                    if (bRow < K && bCol < N) {\n-                        valB = matrixB.array(bRow * ldb + bCol);\n-                    }\n-                    sharedB.array((kk * padB) + (bCol - blockCol), valB);\n-                }\n-            }\n-            kc.barrier();\n-\n-            \/\/ Compute the tile acc += sharedA[:,k] * sharedB[k,:]\n-            for (int kk = 0; kk < TILE_K && (tileIndex + kk) < K; kk++) {\n-                \/\/ Load the A and B operands into registers to reuse for REG_M x REG_N accumulations\n-                PrivateReg aReg = PrivateReg.createPrivate();\n-                PrivateReg bReg = PrivateReg.createPrivate();\n-\n-                \/\/ Fetch a column kk of sharedA to private memory\n-                for (int i = 0; i < REG_M; i++) {\n-                    final int aRowL = (cRowBase + i) - blockRow;\n-                    float valPrivA = 0.0f;\n-                    if ((cRowBase + i) < M) {\n-                        valPrivA = sharedA.array(aRowL * padA + kk);\n-                    }\n-                    aReg.array(i, valPrivA);\n-                }\n-\n-                \/\/ Fetch a row kk of sharedB to private memory\n-                for (int j = 0; j < REG_N; j++) {\n-                    final int bColL = (cColBase + j) - blockCol;\n-                    float valPrivB = 0.0f;\n-                    if ((cColBase + j) < N) {\n-                        valPrivB = sharedB.array(kk * padB + bColL);\n-                    }\n-                    bReg.array(j, valPrivB);\n-                }\n-\n-                \/\/ FMA over the register tile\n-                for (int i = 0; i < REG_M; i++) {\n-                    final float a_ik = aReg.array(i);\n-                    for (int j = 0; j < REG_N; j++) {\n-                        float valRes = acc.array(i * REG_M + j);\n-                        valRes += a_ik * bReg.array(j);\n-                        acc.array(i * REG_M + j, valRes);\n-                    }\n-                }\n-            }\n-            kc.barrier();\n-        }\n-        \/\/ Write back the result of the register tile into matrixC\n-        for (int i = 0; i < REG_M; i++) {\n-            final int row = cRowBase + i;\n-            if (row < M) {\n-                for (int j = 0; j < REG_N; j++) {\n-                    final int col = cColBase + j;\n-                    if (col < N) {\n-                        matrixC.array(row * ldc + col, acc.array(i * REG_M + j));\n-                    }\n-                }\n-            }\n-        }\n-    }\n-\n-    @CodeReflection\n-    public static void matrixMultiply2DRegisterTilingPortable(@RO ComputeContext cc, @RO F32Array matrixA, @RO F32Array matrixB, @RW F32Array matrixC, int globalSize) {\n-        ComputeRange cudaRange = new ComputeRange(new GlobalMesh2D(256, 256), new LocalMesh2D(16, 16));\n-        cc.dispatchKernel(cudaRange,\n-                kc -> matrixMultiplyKernel2DRegisterTilingPortable(kc, matrixA, matrixB, matrixC, globalSize)\n-        );\n-    }\n-\n-    @HatTest\n-    public void testMatrixMultiply2DRegisterTiling() {\n-        var lookup = java.lang.invoke.MethodHandles.lookup();\n-        var accelerator = new Accelerator(lookup, Backend.FIRST);\n-\n-        final int size = SIZE;\n-        var matrixA = F32Array.create(accelerator, size * size);\n-        var matrixB = F32Array.create(accelerator, size * size);\n-\n-        \/\/ Matrix for the results\n-        var matrixC = F32Array.create(accelerator, size * size);\n-        var resultSeq = F32Array.create(accelerator, size * size);\n-\n-        \/\/ Initialize matrices (A and B have the same size)\n-        Random r = new Random(19);\n-\n-        for (int j = 0; j < matrixA.length(); j++) {\n-            matrixA.array(j, r.nextFloat());\n-            matrixB.array(j, r.nextFloat());\n-        }\n-\n-        accelerator.compute(cc ->\n-                TestMatMul.matrixMultiply2DRegisterTilingPortable(cc, matrixA, matrixB, matrixC, size));\n-\n-        \/\/ Run Seq for reference\n-        runSequential(matrixA, matrixB, resultSeq, size);\n-\n-        for (int j = 0; j < size; j++) {\n-            for (int i = 0; i < size; i++) {\n-                HatAsserts.assertEquals(resultSeq.array(i * size + j), matrixC.array(i * size + j), 0.01f);\n-            }\n-        }\n-    }\n-\n@@ -778,1 +552,1 @@\n-    public void testMatMul2DRegisterTilingV2() {\n+    public void testMatMul2DRegisterTiling() {\n","filename":"hat\/tests\/src\/main\/java\/oracle\/code\/hat\/TestMatMul.java","additions":1,"deletions":227,"binary":false,"changes":228,"status":"modified"}]}