{"files":[{"patch":"@@ -135,0 +135,2 @@\n+        Set<String> javaTypeVariables = javaTypeVariables(javaTypeConstraints);\n+\n@@ -136,0 +138,1 @@\n+        List<String> recordResultTypeVariables = new ArrayList<>();\n@@ -137,10 +140,4 @@\n-            w.write(\"public record \" + s.name() + \"Result\");\n-            if (!s.type_constraints().isEmpty()) {\n-                boolean first = true;\n-                w.write(\"<\");\n-                for (OpSchema.TypeConstraintParam typeConstraint : s.type_constraints()) {\n-                    if (!first) {\n-                        w.write(\", \");\n-                    }\n-                    w.write(typeConstraint.type_param_str());\n-                    first = false;\n+            for (OpSchema.TypeConstraintParam typeConstraint : s.type_constraints()) {\n+                if (s.outputs().stream().anyMatch(op -> typeConstraint.type_param_str().equals(op.type_str())) &&\n+                        javaTypeVariables.contains(typeConstraint.type_param_str())) {\n+                    recordResultTypeVariables.add(typeConstraint.type_param_str());\n@@ -148,1 +145,0 @@\n-                w.write(\">\");\n@@ -151,0 +147,4 @@\n+            w.write(\"public record \" + s.name() + \"Result\");\n+            if (!recordResultTypeVariables.isEmpty()) {\n+                w.write(recordResultTypeVariables.stream().collect(Collectors.joining(\", \", \"<\", \">\")));\n+            }\n@@ -174,2 +174,1 @@\n-            boolean first = true;\n-            w.write(\"<\");\n+            List<String> typeVariables = new ArrayList<>();\n@@ -177,2 +176,2 @@\n-                if (!first) {\n-                    w.write(\", \");\n+                if (javaTypeVariables.contains(typeConstraint.type_param_str())) {\n+                    typeVariables.add(typeConstraint.type_param_str());\n@@ -180,2 +179,0 @@\n-                w.write(typeConstraint.type_param_str());\n-                first = false;\n@@ -183,2 +180,5 @@\n-            w.write(\">\");\n-            w.write(\" \");\n+\n+            if (!typeVariables.isEmpty()) {\n+                w.write(typeVariables.stream().collect(Collectors.joining(\", \", \"<\", \">\")));\n+                w.write(\" \");\n+            }\n@@ -211,11 +211,2 @@\n-            if (!s.type_constraints().isEmpty()) {\n-                boolean first = true;\n-                w.write(\"<\");\n-                for (OpSchema.TypeConstraintParam typeConstraint : s.type_constraints()) {\n-                    if (!first) {\n-                        w.write(\", \");\n-                    }\n-                    w.write(typeConstraint.type_param_str());\n-                    first = false;\n-                }\n-                w.write(\">\");\n+            if (!recordResultTypeVariables.isEmpty()) {\n+                w.write(recordResultTypeVariables.stream().collect(Collectors.joining(\", \", \"<\", \">\")));\n@@ -321,1 +312,1 @@\n-            if (!s.type_constraints().isEmpty()) {\n+            if (!recordResultTypeVariables.isEmpty()) {\n@@ -355,0 +346,14 @@\n+    static Set<String> javaTypeVariables(Map<String, TypeElement.ExternalizedTypeElement> tcm) {\n+        return tcm.entrySet().stream()\n+                .filter(e -> usesTypeVariable(e.getKey(), e.getValue()))\n+                .map(e -> e.getKey())\n+                .collect(Collectors.toSet());\n+    }\n+\n+    static boolean usesTypeVariable(String typeVariable, TypeElement.ExternalizedTypeElement ete) {\n+        if (ete.arguments().isEmpty()) {\n+            return typeVariable.equals(ete.identifier());\n+        }\n+        return ete.arguments().stream().anyMatch(a -> usesTypeVariable(typeVariable, a));\n+    }\n+\n","filename":"cr-examples\/onnx\/opgen\/src\/main\/java\/oracle\/code\/onnx\/opgen\/OperatorGen.java","additions":36,"deletions":31,"binary":false,"changes":67,"status":"modified"},{"patch":"@@ -55,1 +55,1 @@\n-    public static <T1, T2, T3> List<Tensor<T3>> Adagrad(Tensor<T1> R, Tensor<Long> T, List<Tensor<T3>> inputs, Optional<Float> epsilon, Optional<Float> decay_factor, Optional<Float> norm_coefficient) {\n+    public static <T1, T3> List<Tensor<T3>> Adagrad(Tensor<T1> R, Tensor<Long> T, List<Tensor<T3>> inputs, Optional<Float> epsilon, Optional<Float> decay_factor, Optional<Float> norm_coefficient) {\n@@ -60,1 +60,1 @@\n-    public static <T1, T2, T3> List<Tensor<T3>> Adam(Tensor<T1> R, Tensor<Long> T, List<Tensor<T3>> inputs, Optional<Float> epsilon, Optional<Float> norm_coefficient_post, Optional<Float> norm_coefficient, Optional<Float> alpha, Optional<Float> beta) {\n+    public static <T1, T3> List<Tensor<T3>> Adam(Tensor<T1> R, Tensor<Long> T, List<Tensor<T3>> inputs, Optional<Float> epsilon, Optional<Float> norm_coefficient_post, Optional<Float> norm_coefficient, Optional<Float> alpha, Optional<Float> beta) {\n@@ -70,1 +70,1 @@\n-    public static <T1, T2> Tensor<T1> AffineGrid(Tensor<T1> theta, Tensor<Long> size, Optional<Long> align_corners) {\n+    public static <T1> Tensor<T1> AffineGrid(Tensor<T1> theta, Tensor<Long> size, Optional<Long> align_corners) {\n@@ -75,1 +75,1 @@\n-    public static <T, T1> Tensor<Boolean> And(Tensor<Boolean> A, Tensor<Boolean> B) {\n+    public static Tensor<Boolean> And(Tensor<Boolean> A, Tensor<Boolean> B) {\n@@ -120,2 +120,2 @@\n-    public record BatchNormalizationResult<T, T1, T2>(Tensor<T> Y, Tensor<T2> running_mean, Tensor<T2> running_var) { }\n-    public static <T, T1, T2> BatchNormalizationResult<T, T1, T2> BatchNormalization(Tensor<T> X, Tensor<T1> scale, Tensor<T1> B, Tensor<T2> input_mean, Tensor<T2> input_var, Optional<Float> epsilon, Optional<Long> training_mode, Optional<Float> momentum) {\n+    public record BatchNormalizationResult<T, T2>(Tensor<T> Y, Tensor<T2> running_mean, Tensor<T2> running_var) { }\n+    public static <T, T1, T2> BatchNormalizationResult<T, T2> BatchNormalization(Tensor<T> X, Tensor<T1> scale, Tensor<T1> B, Tensor<T2> input_mean, Tensor<T2> input_var, Optional<Float> epsilon, Optional<Long> training_mode, Optional<Float> momentum) {\n@@ -192,1 +192,1 @@\n-    public static <T> Tensor<Float> Celu(Tensor<Float> X, Optional<Float> alpha) {\n+    public static Tensor<Float> Celu(Tensor<Float> X, Optional<Float> alpha) {\n@@ -212,1 +212,1 @@\n-    public static <T, T1> Tensor<T> Compress(Tensor<T> input, Tensor<Boolean> condition, Optional<Long> axis) {\n+    public static <T> Tensor<T> Compress(Tensor<T> input, Tensor<Boolean> condition, Optional<Long> axis) {\n@@ -232,1 +232,1 @@\n-    public static <T1, T2> Tensor<T2> ConstantOfShape(Tensor<Long> input, Optional<byte[]> value) {\n+    public static <T2> Tensor<T2> ConstantOfShape(Tensor<Long> input, Optional<byte[]> value) {\n@@ -242,1 +242,1 @@\n-    public static <T1, T2, T3> Tensor<Integer> ConvInteger(Tensor<T1> x, Tensor<T2> w, Optional<Tensor<T1>> x_zero_point, Optional<Tensor<T2>> w_zero_point, Optional<long[]> pads, Optional<long[]> dilations, Optional<String> auto_pad, Optional<long[]> strides, Optional<Long> group, Optional<long[]> kernel_shape) {\n+    public static <T1, T2> Tensor<Integer> ConvInteger(Tensor<T1> x, Tensor<T2> w, Optional<Tensor<T1>> x_zero_point, Optional<Tensor<T2>> w_zero_point, Optional<long[]> pads, Optional<long[]> dilations, Optional<String> auto_pad, Optional<long[]> strides, Optional<Long> group, Optional<long[]> kernel_shape) {\n@@ -292,1 +292,1 @@\n-    public static <T1, T2> Tensor<T2> DictVectorizer(Map<?, ?> X, Optional<String[]> string_vocabulary, Optional<long[]> int64_vocabulary) {\n+    public static <T2> Tensor<T2> DictVectorizer(Map<?, ?> X, Optional<String[]> string_vocabulary, Optional<long[]> int64_vocabulary) {\n@@ -302,2 +302,2 @@\n-    public record DropoutResult<T, T1, T2>(Tensor<T> output, Tensor<Boolean> mask) { }\n-    public static <T, T1, T2> DropoutResult<T, T1, T2> Dropout(Tensor<T> data, Optional<Tensor<T1>> ratio, Optional<Tensor<Boolean>> training_mode, Optional<Long> seed) {\n+    public record DropoutResult<T>(Tensor<T> output, Tensor<Boolean> mask) { }\n+    public static <T, T1> DropoutResult<T> Dropout(Tensor<T> data, Optional<Tensor<T1>> ratio, Optional<Tensor<Boolean>> training_mode, Optional<Long> seed) {\n@@ -309,2 +309,2 @@\n-    public record DynamicQuantizeLinearResult<T1, T2>(Tensor<Byte> y, Tensor<Float> y_scale, Tensor<Byte> y_zero_point) { }\n-    public static <T1, T2> DynamicQuantizeLinearResult<T1, T2> DynamicQuantizeLinear(Tensor<Float> x) {\n+    public record DynamicQuantizeLinearResult(Tensor<Byte> y, Tensor<Float> y_scale, Tensor<Byte> y_zero_point) { }\n+    public static DynamicQuantizeLinearResult DynamicQuantizeLinear(Tensor<Float> x) {\n@@ -313,1 +313,1 @@\n-        return new DynamicQuantizeLinearResult<>((Tensor<Byte>)resultArray[0], (Tensor<Float>)resultArray[1], (Tensor<Byte>)resultArray[2]);\n+        return new DynamicQuantizeLinearResult((Tensor<Byte>)resultArray[0], (Tensor<Float>)resultArray[1], (Tensor<Byte>)resultArray[2]);\n@@ -326,1 +326,1 @@\n-    public static <T, T1> Tensor<Boolean> Equal(Tensor<T> A, Tensor<T> B) {\n+    public static <T> Tensor<Boolean> Equal(Tensor<T> A, Tensor<T> B) {\n@@ -366,2 +366,2 @@\n-    public record GRUResult<T, T1>(Tensor<T> Y, Tensor<T> Y_h) { }\n-    public static <T, T1> GRUResult<T, T1> GRU(Tensor<T> X, Tensor<T> W, Tensor<T> R, Optional<Tensor<T>> B, Optional<Tensor<Integer>> sequence_lens, Optional<Tensor<T>> initial_h, Optional<Long> layout, Optional<float[]> activation_alpha, Optional<Long> hidden_size, Optional<float[]> activation_beta, Optional<String[]> activations, Optional<Long> linear_before_reset, Optional<Float> clip, Optional<String> direction) {\n+    public record GRUResult<T>(Tensor<T> Y, Tensor<T> Y_h) { }\n+    public static <T> GRUResult<T> GRU(Tensor<T> X, Tensor<T> W, Tensor<T> R, Optional<Tensor<T>> B, Optional<Tensor<Integer>> sequence_lens, Optional<Tensor<T>> initial_h, Optional<Long> layout, Optional<float[]> activation_alpha, Optional<Long> hidden_size, Optional<float[]> activation_beta, Optional<String[]> activations, Optional<Long> linear_before_reset, Optional<Float> clip, Optional<String> direction) {\n@@ -418,1 +418,1 @@\n-    public static <T, T1> Tensor<Boolean> Greater(Tensor<T> A, Tensor<T> B) {\n+    public static <T> Tensor<Boolean> Greater(Tensor<T> A, Tensor<T> B) {\n@@ -423,1 +423,1 @@\n-    public static <T, T1> Tensor<Boolean> GreaterOrEqual(Tensor<T> A, Tensor<T> B) {\n+    public static <T> Tensor<Boolean> GreaterOrEqual(Tensor<T> A, Tensor<T> B) {\n@@ -468,1 +468,1 @@\n-    public static <T1, T2> Tensor<Byte> ImageDecoder(Tensor<Byte> encoded_stream, Optional<String> pixel_format) {\n+    public static Tensor<Byte> ImageDecoder(Tensor<Byte> encoded_stream, Optional<String> pixel_format) {\n@@ -483,1 +483,1 @@\n-    public static <T1, T2> Tensor<Boolean> IsInf(Tensor<T1> X, Optional<Long> detect_negative, Optional<Long> detect_positive) {\n+    public static <T1> Tensor<Boolean> IsInf(Tensor<T1> X, Optional<Long> detect_negative, Optional<Long> detect_positive) {\n@@ -488,1 +488,1 @@\n-    public static <T1, T2> Tensor<Boolean> IsNaN(Tensor<T1> X) {\n+    public static <T1> Tensor<Boolean> IsNaN(Tensor<T1> X) {\n@@ -498,2 +498,2 @@\n-    public record LSTMResult<T, T1>(Tensor<T> Y, Tensor<T> Y_h, Tensor<T> Y_c) { }\n-    public static <T, T1> LSTMResult<T, T1> LSTM(Tensor<T> X, Tensor<T> W, Tensor<T> R, Optional<Tensor<T>> B, Optional<Tensor<Integer>> sequence_lens, Optional<Tensor<T>> initial_h, Optional<Tensor<T>> initial_c, Optional<Tensor<T>> P, Optional<Long> layout, Optional<Long> input_forget, Optional<float[]> activation_alpha, Optional<Long> hidden_size, Optional<float[]> activation_beta, Optional<String[]> activations, Optional<Float> clip, Optional<String> direction) {\n+    public record LSTMResult<T>(Tensor<T> Y, Tensor<T> Y_h, Tensor<T> Y_c) { }\n+    public static <T> LSTMResult<T> LSTM(Tensor<T> X, Tensor<T> W, Tensor<T> R, Optional<Tensor<T>> B, Optional<Tensor<Integer>> sequence_lens, Optional<Tensor<T>> initial_h, Optional<Tensor<T>> initial_c, Optional<Tensor<T>> P, Optional<Long> layout, Optional<Long> input_forget, Optional<float[]> activation_alpha, Optional<Long> hidden_size, Optional<float[]> activation_beta, Optional<String[]> activations, Optional<Float> clip, Optional<String> direction) {\n@@ -522,1 +522,1 @@\n-    public static <T, T1> Tensor<Boolean> Less(Tensor<T> A, Tensor<T> B) {\n+    public static <T> Tensor<Boolean> Less(Tensor<T> A, Tensor<T> B) {\n@@ -527,1 +527,1 @@\n-    public static <T, T1> Tensor<Boolean> LessOrEqual(Tensor<T> A, Tensor<T> B) {\n+    public static <T> Tensor<Boolean> LessOrEqual(Tensor<T> A, Tensor<T> B) {\n@@ -532,2 +532,2 @@\n-    public record LinearClassifierResult<T1, T2>(Tensor<T2> Y, Tensor<Float> Z) { }\n-    public static <T1, T2> LinearClassifierResult<T1, T2> LinearClassifier(Tensor<T1> X, Optional<long[]> classlabels_ints, Optional<String> post_transform, float[] coefficients, Optional<Long> multi_class, Optional<float[]> intercepts, Optional<String[]> classlabels_strings) {\n+    public record LinearClassifierResult<T2>(Tensor<T2> Y, Tensor<Float> Z) { }\n+    public static <T1, T2> LinearClassifierResult<T2> LinearClassifier(Tensor<T1> X, Optional<long[]> classlabels_ints, Optional<String> post_transform, float[] coefficients, Optional<Long> multi_class, Optional<float[]> intercepts, Optional<String[]> classlabels_strings) {\n@@ -569,1 +569,1 @@\n-    public static <T1, T2, T3> Tensor<Integer> MatMulInteger(Tensor<T1> A, Tensor<T2> B, Optional<Tensor<T1>> a_zero_point, Optional<Tensor<T2>> b_zero_point) {\n+    public static <T1, T2> Tensor<Integer> MatMulInteger(Tensor<T1> A, Tensor<T2> B, Optional<Tensor<T1>> a_zero_point, Optional<Tensor<T2>> b_zero_point) {\n@@ -579,2 +579,2 @@\n-    public record MaxPoolResult<T, I>(Tensor<T> Y, Tensor<Long> Indices) { }\n-    public static <T, I> MaxPoolResult<T, I> MaxPool(Tensor<T> X, Optional<long[]> pads, Optional<long[]> dilations, Optional<String> auto_pad, Optional<Long> ceil_mode, Optional<Long> storage_order, Optional<long[]> strides, long[] kernel_shape) {\n+    public record MaxPoolResult<T>(Tensor<T> Y, Tensor<Long> Indices) { }\n+    public static <T> MaxPoolResult<T> MaxPool(Tensor<T> X, Optional<long[]> pads, Optional<long[]> dilations, Optional<String> auto_pad, Optional<Long> ceil_mode, Optional<Long> storage_order, Optional<long[]> strides, long[] kernel_shape) {\n@@ -591,1 +591,1 @@\n-    public static <T1, T2> Tensor<T1> MaxUnpool(Tensor<T1> X, Tensor<Long> I, Optional<Tensor<Long>> output_shape, Optional<long[]> pads, Optional<long[]> strides, long[] kernel_shape) {\n+    public static <T1> Tensor<T1> MaxUnpool(Tensor<T1> X, Tensor<Long> I, Optional<Tensor<Long>> output_shape, Optional<long[]> pads, Optional<long[]> strides, long[] kernel_shape) {\n@@ -626,1 +626,1 @@\n-    public static <T1, T2, T3> List<Tensor<T3>> Momentum(Tensor<T1> R, Tensor<Long> T, List<Tensor<T3>> inputs, String mode, float norm_coefficient, float alpha, float beta) {\n+    public static <T1, T3> List<Tensor<T3>> Momentum(Tensor<T1> R, Tensor<Long> T, List<Tensor<T3>> inputs, String mode, float norm_coefficient, float alpha, float beta) {\n@@ -666,1 +666,1 @@\n-    public static <T> Tensor<Boolean> Not(Tensor<Boolean> X) {\n+    public static Tensor<Boolean> Not(Tensor<Boolean> X) {\n@@ -691,1 +691,1 @@\n-    public static <O, B> Tensor<Boolean> OptionalHasElement(Optional<O> input) {\n+    public static <O> Tensor<Boolean> OptionalHasElement(Optional<O> input) {\n@@ -696,1 +696,1 @@\n-    public static <T, T1> Tensor<Boolean> Or(Tensor<Boolean> A, Tensor<Boolean> B) {\n+    public static Tensor<Boolean> Or(Tensor<Boolean> A, Tensor<Boolean> B) {\n@@ -716,1 +716,1 @@\n-    public static <T1, T2, T3, T4> Tensor<T3> QLinearConv(Tensor<T1> x, Tensor<Float> x_scale, Tensor<T1> x_zero_point, Tensor<T2> w, Tensor<Float> w_scale, Tensor<T2> w_zero_point, Tensor<Float> y_scale, Tensor<T3> y_zero_point, Optional<Tensor<Integer>> B, Optional<long[]> pads, Optional<long[]> dilations, Optional<String> auto_pad, Optional<long[]> strides, Optional<Long> group, Optional<long[]> kernel_shape) {\n+    public static <T1, T2, T3> Tensor<T3> QLinearConv(Tensor<T1> x, Tensor<Float> x_scale, Tensor<T1> x_zero_point, Tensor<T2> w, Tensor<Float> w_scale, Tensor<T2> w_zero_point, Tensor<Float> y_scale, Tensor<T3> y_zero_point, Optional<Tensor<Integer>> B, Optional<long[]> pads, Optional<long[]> dilations, Optional<String> auto_pad, Optional<long[]> strides, Optional<Long> group, Optional<long[]> kernel_shape) {\n@@ -731,2 +731,2 @@\n-    public record RNNResult<T, T1>(Tensor<T> Y, Tensor<T> Y_h) { }\n-    public static <T, T1> RNNResult<T, T1> RNN(Tensor<T> X, Tensor<T> W, Tensor<T> R, Optional<Tensor<T>> B, Optional<Tensor<Integer>> sequence_lens, Optional<Tensor<T>> initial_h, Optional<Long> layout, Optional<float[]> activation_alpha, Optional<Long> hidden_size, Optional<float[]> activation_beta, Optional<String[]> activations, Optional<Float> clip, Optional<String> direction) {\n+    public record RNNResult<T>(Tensor<T> Y, Tensor<T> Y_h) { }\n+    public static <T> RNNResult<T> RNN(Tensor<T> X, Tensor<T> W, Tensor<T> R, Optional<Tensor<T>> B, Optional<Tensor<Integer>> sequence_lens, Optional<Tensor<T>> initial_h, Optional<Long> layout, Optional<float[]> activation_alpha, Optional<Long> hidden_size, Optional<float[]> activation_beta, Optional<String[]> activations, Optional<Float> clip, Optional<String> direction) {\n@@ -818,1 +818,1 @@\n-    public static <T1, T2> Tensor<Boolean> RegexFullMatch(Tensor<String> X, Optional<String> pattern) {\n+    public static Tensor<Boolean> RegexFullMatch(Tensor<String> X, Optional<String> pattern) {\n@@ -843,1 +843,1 @@\n-    public static <T1, T2> Tensor<T1> RoiAlign(Tensor<T1> X, Tensor<T1> rois, Tensor<Long> batch_indices, Optional<String> mode, Optional<Long> output_width, Optional<Float> spatial_scale, Optional<String> coordinate_transformation_mode, Optional<Long> sampling_ratio, Optional<Long> output_height) {\n+    public static <T1> Tensor<T1> RoiAlign(Tensor<T1> X, Tensor<T1> rois, Tensor<Long> batch_indices, Optional<String> mode, Optional<Long> output_width, Optional<Float> spatial_scale, Optional<String> coordinate_transformation_mode, Optional<Long> sampling_ratio, Optional<Long> output_height) {\n@@ -858,2 +858,2 @@\n-    public record SVMClassifierResult<T1, T2>(Tensor<T2> Y, Tensor<Float> Z) { }\n-    public static <T1, T2> SVMClassifierResult<T1, T2> SVMClassifier(Tensor<T1> X, Optional<float[]> prob_b, Optional<float[]> kernel_params, Optional<String> kernel_type, Optional<long[]> classlabels_ints, Optional<String> post_transform, Optional<float[]> rho, Optional<float[]> coefficients, Optional<float[]> support_vectors, Optional<long[]> vectors_per_class, Optional<float[]> prob_a, Optional<String[]> classlabels_strings) {\n+    public record SVMClassifierResult<T2>(Tensor<T2> Y, Tensor<Float> Z) { }\n+    public static <T1, T2> SVMClassifierResult<T2> SVMClassifier(Tensor<T1> X, Optional<float[]> prob_b, Optional<float[]> kernel_params, Optional<String> kernel_type, Optional<long[]> classlabels_ints, Optional<String> post_transform, Optional<float[]> rho, Optional<float[]> coefficients, Optional<float[]> support_vectors, Optional<long[]> vectors_per_class, Optional<float[]> prob_a, Optional<String[]> classlabels_strings) {\n@@ -920,1 +920,1 @@\n-    public static <S, I> Tensor<Long> SequenceLength(List<Tensor<S>> input_sequence) {\n+    public static <S> Tensor<Long> SequenceLength(List<Tensor<S>> input_sequence) {\n@@ -925,1 +925,1 @@\n-    public static <T, T1> Tensor<Long> Shape(Tensor<T> data, Optional<Long> start, Optional<Long> end) {\n+    public static <T> Tensor<Long> Shape(Tensor<T> data, Optional<Long> start, Optional<Long> end) {\n@@ -955,1 +955,1 @@\n-    public static <T, T1> Tensor<Long> Size(Tensor<T> data) {\n+    public static <T> Tensor<Long> Size(Tensor<T> data) {\n@@ -970,2 +970,2 @@\n-    public record SoftmaxCrossEntropyLossResult<T, Tind>(Tensor<T> output, Tensor<T> log_prob) { }\n-    public static <T, Tind> SoftmaxCrossEntropyLossResult<T, Tind> SoftmaxCrossEntropyLoss(Tensor<T> scores, Tensor<Tind> labels, Optional<Tensor<T>> weights, Optional<Long> ignore_index, Optional<String> reduction) {\n+    public record SoftmaxCrossEntropyLossResult<T>(Tensor<T> output, Tensor<T> log_prob) { }\n+    public static <T, Tind> SoftmaxCrossEntropyLossResult<T> SoftmaxCrossEntropyLoss(Tensor<T> scores, Tensor<Tind> labels, Optional<Tensor<T>> weights, Optional<Long> ignore_index, Optional<String> reduction) {\n@@ -1012,1 +1012,1 @@\n-    public static <T> Tensor<String> StringConcat(Tensor<String> X, Tensor<String> Y) {\n+    public static Tensor<String> StringConcat(Tensor<String> X, Tensor<String> Y) {\n@@ -1022,2 +1022,2 @@\n-    public record StringSplitResult<T1, T2, T3>(Tensor<String> Y, Tensor<Long> Z) { }\n-    public static <T1, T2, T3> StringSplitResult<T1, T2, T3> StringSplit(Tensor<String> X, Optional<String> delimiter, Optional<Long> maxsplit) {\n+    public record StringSplitResult(Tensor<String> Y, Tensor<Long> Z) { }\n+    public static StringSplitResult StringSplit(Tensor<String> X, Optional<String> delimiter, Optional<Long> maxsplit) {\n@@ -1026,1 +1026,1 @@\n-        return new StringSplitResult<>((Tensor<String>)resultArray[0], (Tensor<Long>)resultArray[1]);\n+        return new StringSplitResult((Tensor<String>)resultArray[0], (Tensor<Long>)resultArray[1]);\n@@ -1049,1 +1049,1 @@\n-    public static <T, T1> Tensor<Float> TfIdfVectorizer(Tensor<T> X, long[] ngram_counts, long min_gram_length, Optional<String[]> pool_strings, String mode, long max_gram_length, long max_skip_count, Optional<long[]> pool_int64s, Optional<float[]> weights, long[] ngram_indexes) {\n+    public static <T> Tensor<Float> TfIdfVectorizer(Tensor<T> X, long[] ngram_counts, long min_gram_length, Optional<String[]> pool_strings, String mode, long max_gram_length, long max_skip_count, Optional<long[]> pool_int64s, Optional<float[]> weights, long[] ngram_indexes) {\n@@ -1059,1 +1059,1 @@\n-    public static <T, T1> Tensor<T> Tile(Tensor<T> input, Tensor<Long> repeats) {\n+    public static <T> Tensor<T> Tile(Tensor<T> input, Tensor<Long> repeats) {\n@@ -1064,2 +1064,2 @@\n-    public record TopKResult<T, I>(Tensor<T> Values, Tensor<Long> Indices) { }\n-    public static <T, I> TopKResult<T, I> TopK(Tensor<T> X, Tensor<Long> K, Optional<Long> largest, Optional<Long> sorted, Optional<Long> axis) {\n+    public record TopKResult<T>(Tensor<T> Values, Tensor<Long> Indices) { }\n+    public static <T> TopKResult<T> TopK(Tensor<T> X, Tensor<Long> K, Optional<Long> largest, Optional<Long> sorted, Optional<Long> axis) {\n@@ -1081,2 +1081,2 @@\n-    public record TreeEnsembleClassifierResult<T1, T2>(Tensor<T2> Y, Tensor<Float> Z) { }\n-    public static <T1, T2> TreeEnsembleClassifierResult<T1, T2> TreeEnsembleClassifier(Tensor<T1> X, Optional<long[]> classlabels_int64s, Optional<long[]> class_ids, Optional<float[]> nodes_hitrates, Optional<long[]> nodes_featureids, Optional<long[]> nodes_treeids, Optional<byte[]> class_weights_as_tensor, Optional<String> post_transform, Optional<String[]> nodes_modes, Optional<long[]> nodes_falsenodeids, Optional<String[]> classlabels_strings, Optional<long[]> nodes_truenodeids, Optional<long[]> nodes_nodeids, Optional<byte[]> nodes_hitrates_as_tensor, Optional<float[]> class_weights, Optional<byte[]> base_values_as_tensor, Optional<long[]> nodes_missing_value_tracks_true, Optional<long[]> class_nodeids, Optional<long[]> class_treeids, Optional<float[]> base_values, Optional<float[]> nodes_values, Optional<byte[]> nodes_values_as_tensor) {\n+    public record TreeEnsembleClassifierResult<T2>(Tensor<T2> Y, Tensor<Float> Z) { }\n+    public static <T1, T2> TreeEnsembleClassifierResult<T2> TreeEnsembleClassifier(Tensor<T1> X, Optional<long[]> classlabels_int64s, Optional<long[]> class_ids, Optional<float[]> nodes_hitrates, Optional<long[]> nodes_featureids, Optional<long[]> nodes_treeids, Optional<byte[]> class_weights_as_tensor, Optional<String> post_transform, Optional<String[]> nodes_modes, Optional<long[]> nodes_falsenodeids, Optional<String[]> classlabels_strings, Optional<long[]> nodes_truenodeids, Optional<long[]> nodes_nodeids, Optional<byte[]> nodes_hitrates_as_tensor, Optional<float[]> class_weights, Optional<byte[]> base_values_as_tensor, Optional<long[]> nodes_missing_value_tracks_true, Optional<long[]> class_nodeids, Optional<long[]> class_treeids, Optional<float[]> base_values, Optional<float[]> nodes_values, Optional<byte[]> nodes_values_as_tensor) {\n@@ -1115,1 +1115,1 @@\n-    public static <B, T> Tensor<T> Where(Tensor<Boolean> condition, Tensor<T> X, Tensor<T> Y) {\n+    public static <T> Tensor<T> Where(Tensor<Boolean> condition, Tensor<T> X, Tensor<T> Y) {\n@@ -1120,1 +1120,1 @@\n-    public static <T, T1> Tensor<Boolean> Xor(Tensor<Boolean> A, Tensor<Boolean> B) {\n+    public static Tensor<Boolean> Xor(Tensor<Boolean> A, Tensor<Boolean> B) {\n","filename":"cr-examples\/onnx\/src\/main\/java\/oracle\/code\/onnx\/OnnxOperators.java","additions":62,"deletions":62,"binary":false,"changes":124,"status":"modified"}]}