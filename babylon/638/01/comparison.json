{"files":[{"patch":"@@ -56,1 +56,11 @@\n-```\n\\ No newline at end of file\n+```\n+\n+### Lifting ONNX model from binary to Java source.\n+\n+OnnxLift is an experimental tool for lifting ONNX binary models to ONNX code reflection model, extraction of weights, and generation of Java model source.\n+\n+Running the OnnxLift:\n+```\n+JAVA_HOME=<path to the Babylon JDK home>\n+mvn package exec:java -Dexec.mainClass=oracle.code.onnx.lift.OnnxLift -Dexec.args=\"<model.onnx> <target folder>\"\n+```\n","filename":"cr-examples\/onnx\/README.md","additions":11,"deletions":1,"binary":false,"changes":12,"status":"modified"},{"patch":"@@ -100,1 +100,0 @@\n-                    <commandlineArgs>--add-modules jdk.incubator.code -classpath %classpath ${exec.mainClass}<\/commandlineArgs>\n","filename":"cr-examples\/onnx\/pom.xml","additions":0,"deletions":1,"binary":false,"changes":1,"status":"modified"},{"patch":"@@ -0,0 +1,719 @@\n+\/*\n+ * Copyright (c) 2025, Oracle and\/or its affiliates. All rights reserved.\n+ * DO NOT ALTER OR REMOVE COPYRIGHT NOTICES OR THIS FILE HEADER.\n+ *\n+ * This code is free software; you can redistribute it and\/or modify it\n+ * under the terms of the GNU General Public License version 2 only, as\n+ * published by the Free Software Foundation.\n+ *\n+ * This code is distributed in the hope that it will be useful, but WITHOUT\n+ * ANY WARRANTY; without even the implied warranty of MERCHANTABILITY or\n+ * FITNESS FOR A PARTICULAR PURPOSE.  See the GNU General Public License\n+ * version 2 for more details (a copy is included in the LICENSE file that\n+ * accompanied this code).\n+ *\n+ * You should have received a copy of the GNU General Public License version\n+ * 2 along with this work; if not, write to the Free Software Foundation,\n+ * Inc., 51 Franklin St, Fifth Floor, Boston, MA 02110-1301 USA.\n+ *\n+ * Please contact Oracle, 500 Oracle Parkway, Redwood Shores, CA 94065 USA\n+ * or visit www.oracle.com if you need additional information or have any\n+ * questions.\n+ *\/\n+\n+package oracle.code.onnx.lift;\n+\n+import java.io.FileOutputStream;\n+import java.io.IOException;\n+import java.io.RandomAccessFile;\n+import java.lang.foreign.Arena;\n+import java.lang.foreign.MemorySegment;\n+import java.lang.foreign.ValueLayout;\n+import java.lang.reflect.AccessFlag;\n+import java.lang.reflect.Field;\n+import java.lang.reflect.Method;\n+import java.nio.channels.FileChannel;\n+import java.nio.file.Files;\n+import java.nio.file.Path;\n+import java.util.ArrayList;\n+import java.util.HashMap;\n+import java.util.HashSet;\n+import java.util.HexFormat;\n+import java.util.Iterator;\n+import java.util.LinkedHashMap;\n+import java.util.List;\n+import java.util.Map;\n+import java.util.Optional;\n+import java.util.Set;\n+import java.util.function.Function;\n+import java.util.regex.Pattern;\n+import java.util.stream.Collectors;\n+import java.util.stream.IntStream;\n+import java.util.stream.LongStream;\n+import jdk.incubator.code.Block;\n+import jdk.incubator.code.CodeItem;\n+import jdk.incubator.code.Location;\n+import jdk.incubator.code.Op;\n+import jdk.incubator.code.TypeElement;\n+import jdk.incubator.code.Value;\n+import jdk.incubator.code.dialect.core.CoreOp;\n+import jdk.incubator.code.dialect.core.CoreType;\n+import jdk.incubator.code.dialect.core.FunctionType;\n+import jdk.incubator.code.dialect.java.JavaType;\n+import jdk.incubator.code.extern.ExternalizedOp;\n+import jdk.incubator.code.extern.OpFactory;\n+import jdk.incubator.code.extern.OpWriter;\n+import oracle.code.onnx.OnnxOperators;\n+import oracle.code.onnx.Tensor;\n+import oracle.code.onnx.ir.ExplicitOnnxOps;\n+import oracle.code.onnx.ir.OnnxOp;\n+import oracle.code.onnx.ir.OnnxOps;\n+import oracle.code.onnx.ir.OnnxType;\n+import oracle.code.onnx.ir.OpFactoryHelper;\n+import oracle.code.onnx.proto.OnnxModel;\n+\n+\n+\/**\n+ * Lifts ONNX model binary to ONNX code reflection model, extracts weights, and generates Java model source.\n+ *\/\n+public class OnnxLift {\n+\n+    record LiftedModelWrapper(CoreOp.FuncOp func, List<String> names, List<OnnxModel.TensorProto> weights) {\n+\n+        private Function<CodeItem, String> namer() {\n+            var defNamer = OpWriter.CodeItemNamerOption.defaultValue().namer();\n+            var namer = new HashMap<Value, Integer>();\n+            return ci -> ci instanceof Value v ? names.get(namer.computeIfAbsent(v, _ -> namer.size())) : defNamer.apply(ci);\n+        }\n+\n+        public String toText() {\n+            return OpWriter.toText(func, OpWriter.CodeItemNamerOption.of(namer()));\n+        }\n+\n+        public String toJava() {\n+            var out = new StringBuilder();\n+            var namer = namer();\n+            var entryBlock = func.bodies().getFirst().entryBlock();\n+            entryBlock.parameters().forEach(namer::apply);\n+            out.append(\"\"\"\n+                    import java.io.IOException;\n+                    import java.io.RandomAccessFile;\n+                    import java.lang.foreign.Arena;\n+                    import java.lang.foreign.MemorySegment;\n+                    import java.nio.channels.FileChannel;\n+                    import jdk.incubator.code.CodeReflection;\n+                    import oracle.code.onnx.Tensor;\n+\n+                    import static java.util.Optional.*;\n+                    import static oracle.code.onnx.OnnxOperators.*;\n+                    import static oracle.code.onnx.Tensor.ElementType.*;\n+\n+                    public class Model {\n+\n+                        final Arena arena = Arena.ofAuto();\n+\n+                        MemorySegment mmap(String pathname) {\n+                            try (var f = new RandomAccessFile(pathname, \"r\")) {\n+                                return f.getChannel().map(FileChannel.MapMode.READ_ONLY, 0, f.length(), arena);\n+                            } catch (IOException e) {\n+                                throw new RuntimeException(e);\n+                            }\n+                        }\n+\n+                        <T> Tensor<T> load(String path, Tensor.ElementType type, long... shape) {\n+                            return new Tensor<>(arena, mmap(path), type, shape);\n+                        }\n+\n+                    \"\"\");\n+            int pSize = entryBlock.parameters().size();\n+            int realParamsSize = pSize - weights().size();\n+            var weightParams = entryBlock.parameters().subList(realParamsSize, pSize);\n+            var wMap = weights.stream().collect(Collectors.toUnmodifiableMap(OnnxModel.TensorProto::name, Function.identity()));\n+            for (int i = 0; i < weightParams.size(); i++) {\n+                Block.Parameter wp = weightParams.get(i);\n+                OnnxModel.TensorProto w = wMap.get(namer.apply(wp));\n+                String name = toJavaName(w.name());\n+                long[] dims = OnnxLift.joinLongArray(w.dims());\n+                out.append(\"    final \" + toJavaType(wp.type()) + \" \" + name + \" = load(\\\"\"\n+                        + name + \"\\\", \"\n+                        + Tensor.ElementType.fromOnnxId(w.dataType()).name()\n+                        + (dims.length > 0 ? (\", \" + longJoin(dims)) : \"\") + \");\\n\");\n+            }\n+            out.append(\"\"\"\n+\n+                        @CodeReflection\n+                        public Object mainGraph(\n+                    \"\"\");\n+            for (int i = 0; i < realParamsSize; i++) {\n+                if (i > 0) {\n+                    out.append(\",\\n\");\n+                }\n+                var param = entryBlock.parameters().get(i);\n+                out.append(\"            \").append(toJavaType(param.type())).append(' ').append(toJavaName(namer.apply(param)));\n+            }\n+            out.append(\") {\\n\");\n+            for (var op : entryBlock.ops()) {\n+                if (!(op instanceof CoreOp.TupleLoadOp)) { \/\/ lazy tupple loads\n+                    out.append(\"        \");\n+                    if (!op.resultType().equals(JavaType.VOID)) {\n+                        out.append(toJavaType(op.resultType())).append(' ').append(toJavaName(namer.apply(op.result()))).append(\" = \");\n+                    }\n+\n+                    switch (op) {\n+                        case OnnxOp oo -> {\n+                            String opName = op.externalizeOpName();\n+                            out.append(opName.substring(opName.lastIndexOf('.') + 1)).append('(');\n+                            var schema = getSchema(oo);\n+                            var inputs = oo.onnxInputs();\n+                            boolean first = true;\n+                            for (var oi : schema.inputs()) {\n+                                if (first) {\n+                                    first = false;\n+                                } else {\n+                                    out.append(\", \");\n+                                }\n+                                out.append(toJava(namer, inputs.get(oi)));\n+                            }\n+                            var attrs = oo.onnxAttributes();\n+                            for (var oa : schema.attributes()) {\n+                                if (first) {\n+                                    first = false;\n+                                } else {\n+                                    out.append(\", \");\n+                                }\n+                                var a = attrs.get(oa.name());\n+                                if (a == null) {\n+                                    out.append(\"empty()\");\n+                                } else if (oa.isOptional()) {\n+                                    out.append(\"of(\").append(toString(a)).append(')');\n+                                } else {\n+                                    out.append(toString(a));\n+                                }\n+                            }\n+                            out.append(\");\\n\");\n+                        }\n+                        case CoreOp.TupleOp to -> {\n+                            out.append(\"List.of(\");\n+                            boolean first = true;\n+                            for (var te : to.operands()) {\n+                                if (first) {\n+                                    first = false;\n+                                } else {\n+                                    out.append(\", \");\n+                                }\n+                                out.append(toJava(namer, te));\n+                            }\n+                            out.append(\");\\n\");\n+                        }\n+                        case CoreOp.ReturnOp ro -> {\n+                            out.append(\"return \").append(toJava(namer, ro.operands().getFirst())).append(\";\\n\");\n+                        }\n+                        default -> throw new UnsupportedOperationException(op.toText());\n+                    }\n+                } else {\n+                    namer.apply(op.result());\n+                }\n+            }\n+            out.append(\"    }\\n}\\n\");\n+            return out.toString();\n+        }\n+\n+        private static String toString(Object o) {\n+            return switch (o) {\n+                case long[] la -> newArray(la);\n+                case float[] fa -> newArray(fa);\n+                case Long l -> l.toString() + \"L\";\n+                case Float f -> f.toString() + \"F\";\n+                case String s -> \"\\\"\" + s + \"\\\"\";\n+                case Tensor t when t.shape().length == 0 && t.elementType() == Tensor.ElementType.BOOL ->\n+                        \"Tensor.ofScalar(\" + t.data().get(ValueLayout.JAVA_BOOLEAN, 0) + \")\";\n+                case Tensor t when t.shape().length == 0 && t.elementType() == Tensor.ElementType.INT8 ->\n+                        \"Tensor.ofScalar((byte)\" + t.data().get(ValueLayout.JAVA_BYTE, 0) + \")\";\n+                case Tensor t -> \"Tensor.ofShape(\" + toString(t.shape()) + \", \"\n+                    + switch (t.elementType()) {\n+                        case FLOAT -> toString(t.data().toArray(ValueLayout.JAVA_FLOAT));\n+                        case INT64 -> toString(t.data().toArray(ValueLayout.JAVA_LONG));\n+                        default -> \"HexFormat.of().parseHex(\\\"\" + HexFormat.of().formatHex(t.data().toArray(ValueLayout.JAVA_BYTE))\n+                                                            + \"\\\"), \" + t.elementType().name();\n+                    } + \")\";\n+                default -> o.toString();\n+            };\n+        }\n+\n+        private static String newArray(long[] la) {\n+            for (long l : la) {\n+                if (l != 0l) {\n+                    return \"new long[] {\" + longJoin(la) + \"}\";\n+                }\n+            }\n+            return \"new long[\" + la.length + \"]\";\n+        }\n+\n+        private static String longJoin(long[] la) {\n+            return LongStream.of(la).mapToObj(d -> String.valueOf(d) + \"L\")\n+                                     .collect(Collectors.joining(\", \"));\n+        }\n+\n+        private static String newArray(float[] fa) {\n+            for (float f : fa) {\n+                if (f != 0f) {\n+                    return IntStream.range(0, fa.length).mapToObj(i -> String.valueOf(fa[i]) + \"F\")\n+                                    .collect(Collectors.joining(\", \",  \"new float[] {\", \"}\"));\n+                }\n+            }\n+            return \"new float[\" + fa.length + \"]\";\n+        }\n+\n+        private static String tupleAccessor(Value tuple, int componentIndex) {\n+            if (tuple instanceof Op.Result or && or.op() instanceof OnnxOp oo) {\n+                String mName = oo.externalizeOpName();\n+                mName = mName.substring(mName.lastIndexOf('.') + 1);\n+                for (Method m : OnnxOperators.class.getMethods()) {\n+                    if (m.getName().equals(mName)) {\n+                        return m.getReturnType().getRecordComponents()[componentIndex].getAccessor().getName() + \"()\";\n+                    }\n+                }\n+                throw new IllegalStateException(mName);\n+            }\n+            return \"get(\" + componentIndex + \")\"; \/\/ fallback to List\n+        }\n+\n+        private static String toJava(Function<CodeItem, String> namer, Object value) {\n+            return switch (value) {\n+                case Optional o when o.isEmpty() -> \"empty()\";\n+                case Optional o -> \"of(\" + toJava(namer, o.get()) + \")\";\n+                case List l -> \"List.of(\" + l.stream().map(le -> toJava(namer, le)).collect(Collectors.joining(\", \")) + \")\";\n+                case Op.Result or when or.op() instanceof CoreOp.TupleLoadOp tlo ->\n+                    toJavaName(namer.apply(tlo.operands().getFirst())) + '.' + tupleAccessor(tlo.operands().getFirst(), tlo.index());\n+                case Value v -> toJavaName(namer.apply(v));\n+                default -> throw new UnsupportedOperationException(value.toString());\n+            };\n+        }\n+\n+        private static OnnxOp.OnnxSchema getSchema(OnnxOp oo) {\n+            try {\n+                return (OnnxOp.OnnxSchema) oo.getClass().getDeclaredField(\"SCHEMA\").get(null);\n+            } catch (ReflectiveOperationException ex) {\n+                throw new RuntimeException(ex);\n+            }\n+        }\n+\n+        private static String toJavaType(TypeElement t) {\n+            return switch (t) {\n+                case OnnxType.TensorType tt ->\n+                    \"Tensor<\" + switch (tt.eType()) {\n+                        case OnnxType.Float32Type _ -> \"Float\";\n+                        case OnnxType.Int64Type _ -> \"Long\";\n+                        case OnnxType.Int32Type _ -> \"Integer\";\n+                        case OnnxType.UInt8Type _ -> \"Byte\";\n+                        case OnnxType.BoolType _ -> \"Boolean\";\n+                        default -> throw new UnsupportedOperationException(t.toString());\n+                    } + \">\";\n+                default -> \"var\";\n+            };\n+        }\n+    }\n+\n+    static OnnxType toOnnxType(OnnxModel.TypeProto tp) {\n+        if (tp.tensorType() instanceof OnnxModel.TypeProto.Tensor t) {\n+            return toTensorType(t.elemType());\n+        } else if (tp.optionalType() instanceof OnnxModel.TypeProto.Optional o) {\n+            return OnnxType.optional(toOnnxType(o.elemType()));\n+        } else if (tp.sequenceType()  instanceof OnnxModel.TypeProto.Sequence s) {\n+            return OnnxType.seq(toOnnxType(s.elemType()));\n+        } else if (tp.mapType() instanceof OnnxModel.TypeProto.Map m) {\n+            return OnnxType.map(toKeyType(m.keyType()), toOnnxType(m.valueType()));\n+        } else if (tp.sparseTensorType() instanceof OnnxModel.TypeProto.SparseTensor st) {\n+            throw new UnsupportedOperationException(\"Sparse tensors not supported yet.\");\n+        }\n+        throw new IllegalArgumentException(\"No type specified.\");\n+    }\n+\n+    static FunctionType toFunctionType(OnnxModel.GraphProto g) {\n+        var paramTypes = new ArrayList<TypeElement>();\n+        Set<String> dedup = new HashSet();\n+        for (OnnxModel.ValueInfoProto input : g.input()) {\n+            if (dedup.add(input.name())) {\n+                paramTypes.add(toOnnxType(input.type()));\n+            }\n+        }\n+\n+        for (OnnxModel.TensorProto init : g.initializer()) {\n+            if (dedup.add(init.name())) {\n+                paramTypes.add(toTensorType(init.dataType()));\n+            }\n+        }\n+        var returnType = g.output().size() == 1\n+                ? toOnnxType(g.output().getFirst().type())\n+                : CoreType.tupleType(g.output().stream().map(OnnxModel.ValueInfoProto::type).map(OnnxLift::toOnnxType).toList());\n+        return CoreType.functionType(returnType, paramTypes);\n+    }\n+\n+    static OnnxType toKeyType(int kt) {\n+        return switch (kt) {\n+            case 2 -> OnnxType.UINT8;\n+            case 3 -> OnnxType.INT8;\n+            case 4 -> OnnxType.UINT16;\n+            case 5 -> OnnxType.INT16;\n+            case 6 -> OnnxType.INT32;\n+            case 7 -> OnnxType.INT64;\n+            case 8 -> OnnxType.STRING;\n+            case 12 -> OnnxType.UINT32;\n+            case 13 -> OnnxType.UINT64;\n+            default -> throw new IllegalArgumentException(\"Invalid key type: \" + kt);\n+        };\n+    }\n+\n+    static OnnxType.TensorType toTensorType(int tt) {\n+        return switch (tt) {\n+            case 1 -> OnnxType.TENSOR_FLOAT32;\n+            case 2 -> OnnxType.TENSOR_UINT8;\n+            case 3 -> OnnxType.TENSOR_INT8;\n+            case 4 -> OnnxType.TENSOR_UINT16;\n+            case 5 -> OnnxType.TENSOR_INT16;\n+            case 6 -> OnnxType.TENSOR_INT32;\n+            case 7 -> OnnxType.TENSOR_INT64;\n+            case 8 -> OnnxType.TENSOR_STRING;\n+            case 9 -> OnnxType.TENSOR_BOOL;\n+            case 10 -> OnnxType.TENSOR_FLOAT16;\n+            case 11 -> OnnxType.TENSOR_FLOAT64;\n+            case 12 -> OnnxType.TENSOR_UINT32;\n+            case 13 -> OnnxType.TENSOR_UINT64;\n+            case 14 -> OnnxType.TENSOR_COMPLEX64;\n+            case 15 -> OnnxType.TENSOR_COMPLEX128;\n+            case 16 -> OnnxType.TENSOR_BFLOAT16;\n+            case 17 -> OnnxType.TENSOR_FLOAT8E4M3FN;\n+            case 18 -> OnnxType.TENSOR_FLOAT8E4M3FNUZ;\n+            case 19 -> OnnxType.TENSOR_FLOAT8E5M2;\n+            case 20 -> OnnxType.TENSOR_FLOAT8E5M2FNUZ;\n+            case 21 -> OnnxType.TENSOR_UINT4;\n+            case 22 -> OnnxType.TENSOR_INT4;\n+            case 23 -> OnnxType.TENSOR_FLOAT4E2M1;\n+            default -> OnnxType.tensor(null);\n+        };\n+    }\n+\n+    static final OpFactory ONNX_OP_FACTORY = OpFactoryHelper.OP_FACTORY.get(ExplicitOnnxOps.class).andThen(OpFactoryHelper.OP_FACTORY.get(OnnxOps.class));\n+\n+    static final Map<String, OnnxOp.OnnxSchema> ONNX_SCHEMA_REGISTRY = collectSchemas(ExplicitOnnxOps.class, OnnxOps.class);\n+\n+    static Map<String, OnnxOp.OnnxSchema> collectSchemas(Class<?>... cls) {\n+        Map<String, OnnxOp.OnnxSchema> reg = new HashMap<>();\n+        for (Class<?> c : cls) {\n+            for (Class<?> nm : c.getNestMembers()) {\n+                for (Field f : nm.getFields()) {\n+                    if (f.accessFlags().contains(AccessFlag.STATIC) && OnnxOp.OnnxSchema.class.isAssignableFrom(f.getType())) try {\n+                        OnnxOp.OnnxSchema sch = (OnnxOp.OnnxSchema)f.get(null);\n+                        reg.put(sch.name(), sch);\n+                    } catch (ReflectiveOperationException e) {\n+                        throw new RuntimeException(e);\n+                    }\n+                }\n+            }\n+        }\n+        return reg;\n+    }\n+\n+    static LiftedModelWrapper lift(OnnxModel.GraphProto g) {\n+        var valueMap = new LinkedHashMap<String, Value>();\n+        var func = CoreOp.FuncOp.func(g.name(), toFunctionType(g)).body(fb -> {\n+\n+            { \/\/ fill value map for parameters and initializers\n+                Iterator<Block.Parameter> params = fb.entryBlock().parameters().iterator();\n+                for (OnnxModel.ValueInfoProto input : g.input()) {\n+                    valueMap.put(input.name(), params.next());\n+                }\n+                for (OnnxModel.TensorProto init : g.initializer()) {\n+                    valueMap.computeIfAbsent(init.name(), _ -> params.next());\n+                }\n+            }\n+\n+            for (OnnxModel.NodeProto n : g.node()) {\n+                String opType = n.opType();\n+\n+                 \/\/ @@@ an old alias ? could not find the spec\n+                if (opType.equals(\"SimplifiedLayerNormalization\")) {\n+                    opType = \"LayerNormalization\";\n+                }\n+\n+                if (n.domain() != null && !n.domain().isEmpty() && !n.domain().equals(\"ai.onnx\")) {\n+                    opType = n.domain() + \".\" + opType;\n+                }\n+\n+                OnnxOp.OnnxSchema schema = ONNX_SCHEMA_REGISTRY.computeIfAbsent(opType, ot -> {throw new IllegalArgumentException(\"Unknown op type: \" + ot);});\n+                Map<String, Object> attributes = new LinkedHashMap<>();\n+                if (n.attribute() != null) {\n+                    for (OnnxModel.AttributeProto a : n.attribute()) {\n+                        attributes.put(a.name(), toAttributeValue(a));\n+                    }\n+                }\n+\n+                \/\/ map inputs\n+                List<Value> inputs = new ArrayList<>();\n+                if (n.input() != null) {\n+                    List<OnnxOp.OnnxParameter> optionalInputs = new ArrayList<>();\n+                    for (int i = 0; i < n.input().size(); i++) {\n+                        OnnxOp.OnnxParameter param = i < schema.inputs().size() ? schema.inputs().get(i) : schema.inputs().getLast();\n+                        Value v = valueMap.get(n.input().get(i));\n+                        if (v != null) {\n+                            switch (param.quantifier()) {\n+                                case REQUIRED -> {\n+                                    inputs.add(v);\n+                                }\n+                                case OPTIONAL -> {\n+                                    optionalInputs.add(param);\n+                                    inputs.add(v);\n+                                }\n+                                case VARIADIC -> {\n+                                    inputs.add(v);\n+                                }\n+                            }\n+                        }\n+                    }\n+                    if (!optionalInputs.isEmpty()) {\n+                        attributes.put(\"optional_inputs\", optionalInputs);\n+                    }\n+                }\n+\n+                \/\/ map outputs\n+                List<OnnxOp.OnnxParameter> optionalOutputs = new ArrayList<>();\n+                List<String> outputNames = new ArrayList<>();\n+                if (n.output() != null) {\n+                    for (int i = 0; i < n.output().size(); i++) {\n+                        OnnxOp.OnnxParameter param = schema.outputs().get(i);\n+                        if (!n.output().get(i).isEmpty()) {\n+                            outputNames.add(n.output().get(i));\n+                            if (param.quantifier() == OnnxOp.OnnxParameter.Quantifier.OPTIONAL) {\n+                                optionalOutputs.add(param);\n+                            }\n+                        }\n+                    }\n+                    if (!optionalOutputs.isEmpty()) {\n+                        attributes.put(\"optional_outputs\", optionalOutputs);\n+                    }\n+                }\n+\n+                \/\/ inline Constant op tensor attribute as value\n+                if (opType.equals(\"Constant\") && attributes.remove(OnnxOps.Constant.Attribute.value.name()) instanceof Tensor t) {\n+                    switch (t.shape().length) {\n+                        case 0 -> { \/\/ scalar\n+                            switch (t.elementType()) {\n+                                case FLOAT -> attributes.put(OnnxOps.Constant.Attribute.value_float.name(), t.data().get(ValueLayout.JAVA_FLOAT, 0));\n+                                case INT64 -> attributes.put(OnnxOps.Constant.Attribute.value_int.name(), t.data().get(ValueLayout.JAVA_LONG, 0));\n+                                default -> attributes.put(OnnxOps.Constant.Attribute.value.name(), t);\n+                            }\n+                        }\n+                        case 1 -> { \/\/ 1d tensor\n+                            switch (t.elementType()) {\n+                                case FLOAT -> attributes.put(OnnxOps.Constant.Attribute.value_floats.name(), t.data().toArray(ValueLayout.JAVA_FLOAT));\n+                                case INT64 -> attributes.put(OnnxOps.Constant.Attribute.value_ints.name(), t.data().toArray(ValueLayout.JAVA_LONG));\n+                                default -> attributes.put(OnnxOps.Constant.Attribute.value.name(), t);\n+                            }\n+                        }\n+                        default ->  attributes.put(OnnxOps.Constant.Attribute.value.name(), t);\n+                    }\n+                }\n+\n+                \/\/ get the op\n+                ExternalizedOp extOp = new ExternalizedOp(\n+                        opType,\n+                        Location.NO_LOCATION,\n+                        inputs,\n+                        List.of(),\n+                        new OnnxType.TensorType(null),\n+                        attributes,\n+                        List.of());\n+                OnnxOp rawOp = (OnnxOp)ONNX_OP_FACTORY.constructOpOrFail(extOp);\n+\n+                \/\/ patch the op return type\n+                TypeElement returnType = schema.outputs().size() == 1\n+                        ? inferTypeVariableType(rawOp.onnxOutputs().getFirst().type(), rawOp, n)\n+                        : CoreType.tupleType(rawOp.onnxOutputs().stream().map(o -> inferTypeVariableType(o.type(), rawOp, n)).toList());\n+                extOp = new ExternalizedOp(\n+                        extOp.name(),\n+                        Location.NO_LOCATION,\n+                        extOp.operands(),\n+                        extOp.successors(),\n+                        returnType,\n+                        extOp.attributes(),\n+                        extOp.bodyDefinitions());\n+                Op.Result res = fb.op((OnnxOp)ONNX_OP_FACTORY.constructOpOrFail(extOp));\n+\n+                \/\/ map outputs\n+                if (schema.outputs().size() == 1) {\n+                    valueMap.put(n.output().getFirst(), res);\n+                } else {\n+                    valueMap.put(n.name(), res);\n+                    for (int i = 0; i < outputNames.size(); i++) {\n+                        valueMap.put(outputNames.get(i), fb.op(CoreOp.tupleLoad(res, i)));\n+                    }\n+                }\n+            }\n+\n+            if (g.output().size() == 1) {\n+                fb.op(CoreOp.return_(valueMap.get(g.output().getFirst().name())));\n+            } else {\n+                Op.Result ret = fb.op(CoreOp.tuple(g.output().stream().map(OnnxModel.ValueInfoProto::name).map(valueMap::get).toList()));\n+                valueMap.put(g.name() + \"_return\", ret);\n+                fb.op(CoreOp.return_(ret));\n+            }\n+        });\n+        return new LiftedModelWrapper(func, List.of(valueMap.sequencedKeySet().toArray(String[]::new)), g.initializer());\n+    }\n+\n+    static OnnxType inferTypeVariableType(OnnxType type, OnnxOp op, OnnxModel.NodeProto n) {\n+        if (type instanceof OnnxType.TypeVariable tv) {\n+            if (tv.types().size() == 1) {\n+                return tv.types().getFirst();\n+            }\n+            \/\/ search for the same type variable across inputs\n+            for (var ie : op.onnxInputs().entrySet()) {\n+                if (ie.getKey().type().equals(tv)) {\n+                    if (ie.getValue() instanceof Value v && v.type() instanceof OnnxType ot) {\n+                        return ot;\n+                    } else if (ie.getValue() instanceof List l && !l.isEmpty() && l.getFirst() instanceof Value v && v.type() instanceof OnnxType ot) {\n+                        return ot;\n+                    }\n+                }\n+            }\n+\n+            \/\/ special cases\n+            return switch (op) {\n+                case OnnxOps.Cast c ->\n+                    toTensorType((int)c.to());\n+                case OnnxOps.ConstantOfShape _, OnnxOps.Constant _-> \/\/ get tensor type from tensor attribute\n+                    n.attribute() != null\n+                    && !n.attribute().isEmpty()\n+                    && n.attribute().getFirst().t() instanceof OnnxModel.TensorProto tp\n+                            ? toTensorType(tp.dataType())\n+                            : OnnxType.TENSOR_FLOAT32; \/\/ default\n+                default ->\n+                    throw new IllegalArgumentException(\"Could not infer op type for: \" + op.toText());\n+            };\n+        }\n+        return type;\n+    }\n+\n+    static Object toAttributeValue(OnnxModel.AttributeProto a) {\n+        return switch (a.type()) {\n+            case FLOAT -> a.f();\n+            case INT -> a.i();\n+            case STRING -> new String(a.s());\n+            case TENSOR -> toTensor(a.t());\n+\/\/    GRAPH = 5;\n+\/\/    SPARSE_TENSOR = 11;\n+\/\/    TYPE_PROTO = 13;\n+            case FLOATS -> joinFloatArray(a.floats());\n+            case INTS -> joinLongArray(a.ints());\n+            case STRINGS -> a.strings();\n+            case TENSORS -> a.tensors().stream().map(OnnxLift::toTensor).toArray(Tensor[]::new);\n+\/\/    GRAPHS = 10;\n+\/\/    SPARSE_TENSORS = 12;\n+\/\/    TYPE_PROTOS = 14;\n+            default -> throw new UnsupportedOperationException(\"Unsupported \" + a.type());\n+        };\n+    }\n+\n+    static Tensor toTensor(OnnxModel.TensorProto tensorProto) {\n+        \/\/ @@@ floatData, longData, stringData...\n+        \/\/ @@@ externalData\n+        \/\/ @@@ segments\n+        return Tensor.ofShape(joinLongArray(tensorProto.dims()), tensorProto.rawData(), Tensor.ElementType.fromOnnxId(tensorProto.dataType()));\n+    }\n+\n+    static float[] joinFloatArray(List<float[]> floats) {\n+        if (floats == null) return new float[0];\n+        float[] join = new float[floats.stream().mapToInt(f -> f.length).sum()];\n+        int i = 0;\n+        for (float[] f : floats) {\n+            System.arraycopy(f, 0, join, i, f.length);\n+            i += f.length;\n+        }\n+        return join;\n+    }\n+\n+    static long[] joinLongArray(List<long[]> longs) {\n+        if (longs == null) return new long[0];\n+        long[] join = new long[longs.stream().mapToInt(f -> f.length).sum()];\n+        int i = 0;\n+        for (long[] f : longs) {\n+            System.arraycopy(f, 0, join, i, f.length);\n+            i += f.length;\n+        }\n+        return join;\n+    }\n+\n+    static String toJavaName(String name) {\n+        name = Pattern.compile(\"[^\\\\p{Alnum}]+(\\\\p{Alnum})\").matcher(name).replaceAll(mr -> mr.group(1).toUpperCase());\n+        name = name.replace(\"Output\", \"\");\n+        return Character.toLowerCase(name.charAt(0)) + name.substring(1);\n+    }\n+\n+    static String colorModelToANSI(String codeModel) {\n+        return codeModel.replaceAll(\"(%\\\\d+)([; ])\", \"\\033[31m$1\\033[0m$2\")\n+                        .replaceAll(\" : ([A-Za-z0-9:.<>\\\\[\\\\]\\\", ]+)\", \" : \\033[32m$1\\033[0m\")\n+                        .replaceAll(\"\\\\)([A-Za-z0-9:.<>\\\\[\\\\]\\\", ]+) -> \\\\{\", \")\\033[32m$1\\033[0m -> {\")\n+                        .replaceAll(\"(^|    |= )([A-Za-z0-9.]+)\", \"$1\\033[34m$2\\033[0m\")\n+                        .replaceAll(\"(\\\\^block_[0-9_]+)([;: ])\", \"\\033[35m$1\\033[0m$2\");\n+    }\n+\n+    static void extractWeights(OnnxModel.ModelProto model, Path sourceFolder, Path targetFolder) throws IOException {\n+        targetFolder.toFile().mkdirs();\n+        for (OnnxModel.TensorProto i : model.graph().initializer()) {\n+            Path weightFile = targetFolder.resolve(toJavaName(i.name()));\n+            try (var weightStream = new FileOutputStream(weightFile.toFile())) {\n+                if (i.floatData() instanceof List<float[]> fd) {\n+                    for (var fa : fd) {\n+                        var data = MemorySegment.ofArray(fa).toArray(ValueLayout.JAVA_BYTE);\n+                        weightStream.write(data);\n+                    }\n+                } else if (i.int64Data() instanceof List<long[]> ld) {\n+                    for (var la : ld) {\n+                        var data = MemorySegment.ofArray(la).toArray(ValueLayout.JAVA_BYTE);\n+                        weightStream.write(data);\n+                    }\n+                } else if (i.int32Data() instanceof List<int[]> id) {\n+                    for (var ia : id) {\n+                        var data = MemorySegment.ofArray(ia).toArray(ValueLayout.JAVA_BYTE);\n+                        weightStream.write(data);\n+                    }\n+                } else if (i.rawData() instanceof byte[] bd) {\n+                    weightStream.write(bd);\n+                } else if (i.externalData() instanceof List<OnnxModel.StringStringEntryProto> ssep) {\n+                    var map = ssep.stream().collect(Collectors.toUnmodifiableMap(OnnxModel.StringStringEntryProto::key, OnnxModel.StringStringEntryProto::value));\n+                    Path dataFilePath = sourceFolder.resolve(map.get(\"location\"));\n+                    long offset = Long.parseLong(map.get(\"offset\"));\n+                    int length = Integer.parseInt(map.get(\"length\"));\n+                    try (var dataFile = new RandomAccessFile(dataFilePath.toString(), \"r\");Arena a = Arena.ofConfined()) {\n+                        var ms = dataFile.getChannel().map(FileChannel.MapMode.READ_ONLY, offset, length, a);\n+                        weightStream.write(ms.toArray(ValueLayout.JAVA_BYTE));\n+                    }\n+                } else {\n+                    throw new UnsupportedOperationException();\n+                }\n+            }\n+            IO.println(weightFile + \" extracted.\");\n+        }\n+    }\n+\n+    public static void main(String[] args) throws Exception {\n+        if (args.length != 2) {\n+            IO.println(\"Usage: OnnxLift <model.onnx> <target folder>\");\n+        } else {\n+            Path source = Path.of(args[0]);\n+            Path targetFolder = Path.of(args[1]);\n+            try (var in = new RandomAccessFile(source.toFile(), \"r\")) {\n+                OnnxModel.ModelProto protoModel = OnnxModel.readFrom(in.getChannel().map(FileChannel.MapMode.READ_ONLY, 0, in.length()));\n+                LiftedModelWrapper liftedModel = lift(protoModel.graph());\n+\n+                IO.println(colorModelToANSI(liftedModel.toText()));\n+\n+                Path java = targetFolder.resolve(\"Model.java\");\n+                Files.writeString(java, liftedModel.toJava());\n+                IO.println(java + \" generated.\");\n+\n+                extractWeights(protoModel, source.getParent(), targetFolder);\n+            }\n+        }\n+    }\n+}\n","filename":"cr-examples\/onnx\/src\/main\/java\/oracle\/code\/onnx\/lift\/OnnxLift.java","additions":719,"deletions":0,"binary":false,"changes":719,"status":"added"}]}