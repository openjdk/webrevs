{"files":[{"patch":"@@ -37,2 +37,2 @@\n-        <maven.compiler.source>24<\/maven.compiler.source>\n-        <maven.compiler.target>24<\/maven.compiler.target>\n+        <maven.compiler.source>25<\/maven.compiler.source>\n+        <maven.compiler.target>25<\/maven.compiler.target>\n","filename":"cr-examples\/onnx\/opgen\/pom.xml","additions":2,"deletions":2,"binary":false,"changes":4,"status":"modified"},{"patch":"@@ -666,10 +666,1 @@\n-            String typeLiteralString = switch (aType) {\n-                \/\/ @@@ sub-graphs have inputs and outputs\n-                default -> {\n-                    if (a.required()) {\n-                        yield aType.type().getSimpleName();\n-                    } else {\n-                        yield toBoxType(aType.type()).getSimpleName();\n-                    }\n-                }\n-            };\n+\n@@ -687,0 +678,3 @@\n+            \/\/ @@@ sub-graphs have inputs and outputs\n+            String typeLiteralString = toBoxType(aType.type()).getSimpleName();\n+\n","filename":"cr-examples\/onnx\/opgen\/src\/main\/java\/oracle\/code\/onnx\/opgen\/OpGen.java","additions":4,"deletions":10,"binary":false,"changes":14,"status":"modified"},{"patch":"@@ -374,2 +374,2 @@\n-            for (int i = 0; i < schema.outputs().size(); i++) {\n-                inputs.put(schema.outputs().get(i), inputArguments.get(i));\n+            for (int i = 0; i < schema.inputs().size(); i++) {\n+                inputs.put(schema.inputs().get(i), inputArguments.get(i));\n","filename":"cr-examples\/onnx\/src\/main\/java\/oracle\/code\/onnx\/ir\/OnnxOp.java","additions":2,"deletions":2,"binary":false,"changes":4,"status":"modified"},{"patch":"@@ -3584,1 +3584,1 @@\n-            long to = Attribute.to.access(long.class, onnxAttributes);\n+            long to = Attribute.to.access(Long.class, onnxAttributes);\n@@ -5056,1 +5056,1 @@\n-            long axis = Attribute.axis.access(long.class, onnxAttributes);\n+            long axis = Attribute.axis.access(Long.class, onnxAttributes);\n@@ -5203,1 +5203,1 @@\n-            long axis = Attribute.axis.access(long.class, onnxAttributes);\n+            long axis = Attribute.axis.access(Long.class, onnxAttributes);\n@@ -6991,1 +6991,1 @@\n-            long blocksize = Attribute.blocksize.access(long.class, onnxAttributes);\n+            long blocksize = Attribute.blocksize.access(Long.class, onnxAttributes);\n@@ -11218,1 +11218,1 @@\n-            long num_groups = Attribute.num_groups.access(long.class, onnxAttributes);\n+            long num_groups = Attribute.num_groups.access(Long.class, onnxAttributes);\n@@ -12925,1 +12925,1 @@\n-            long size = Attribute.size.access(long.class, onnxAttributes);\n+            long size = Attribute.size.access(Long.class, onnxAttributes);\n@@ -16712,1 +16712,1 @@\n-            float norm_coefficient = Attribute.norm_coefficient.access(float.class, onnxAttributes);\n+            float norm_coefficient = Attribute.norm_coefficient.access(Float.class, onnxAttributes);\n@@ -16717,1 +16717,1 @@\n-            float alpha = Attribute.alpha.access(float.class, onnxAttributes);\n+            float alpha = Attribute.alpha.access(Float.class, onnxAttributes);\n@@ -16722,1 +16722,1 @@\n-            float beta = Attribute.beta.access(float.class, onnxAttributes);\n+            float beta = Attribute.beta.access(Float.class, onnxAttributes);\n@@ -27019,1 +27019,1 @@\n-            long blocksize = Attribute.blocksize.access(long.class, onnxAttributes);\n+            long blocksize = Attribute.blocksize.access(Long.class, onnxAttributes);\n@@ -28610,1 +28610,1 @@\n-            long min_gram_length = Attribute.min_gram_length.access(long.class, onnxAttributes);\n+            long min_gram_length = Attribute.min_gram_length.access(Long.class, onnxAttributes);\n@@ -28625,1 +28625,1 @@\n-            long max_gram_length = Attribute.max_gram_length.access(long.class, onnxAttributes);\n+            long max_gram_length = Attribute.max_gram_length.access(Long.class, onnxAttributes);\n@@ -28630,1 +28630,1 @@\n-            long max_skip_count = Attribute.max_skip_count.access(long.class, onnxAttributes);\n+            long max_skip_count = Attribute.max_skip_count.access(Long.class, onnxAttributes);\n","filename":"cr-examples\/onnx\/src\/main\/java\/oracle\/code\/onnx\/ir\/OnnxOps.java","additions":13,"deletions":13,"binary":false,"changes":26,"status":"modified"},{"patch":"@@ -464,1 +464,1 @@\n-            args.add(eType.externalize());\n+            if (eType != null) args.add(eType.externalize());\n","filename":"cr-examples\/onnx\/src\/main\/java\/oracle\/code\/onnx\/ir\/OnnxType.java","additions":1,"deletions":1,"binary":false,"changes":2,"status":"modified"},{"patch":"@@ -71,1 +71,1 @@\n-            @f(20) Long type,\n+            @f(20) Integer type,\n","filename":"cr-examples\/onnx\/src\/main\/java\/oracle\/code\/onnx\/proto\/OnnxProtoModel.java","additions":1,"deletions":1,"binary":false,"changes":2,"status":"modified"},{"patch":"@@ -0,0 +1,329 @@\n+\/*\n+ * Copyright (c) 2025, Oracle and\/or its affiliates. All rights reserved.\n+ * DO NOT ALTER OR REMOVE COPYRIGHT NOTICES OR THIS FILE HEADER.\n+ *\n+ * This code is free software; you can redistribute it and\/or modify it\n+ * under the terms of the GNU General Public License version 2 only, as\n+ * published by the Free Software Foundation.\n+ *\n+ * This code is distributed in the hope that it will be useful, but WITHOUT\n+ * ANY WARRANTY; without even the implied warranty of MERCHANTABILITY or\n+ * FITNESS FOR A PARTICULAR PURPOSE.  See the GNU General Public License\n+ * version 2 for more details (a copy is included in the LICENSE file that\n+ * accompanied this code).\n+ *\n+ * You should have received a copy of the GNU General Public License version\n+ * 2 along with this work; if not, write to the Free Software Foundation,\n+ * Inc., 51 Franklin St, Fifth Floor, Boston, MA 02110-1301 USA.\n+ *\n+ * Please contact Oracle, 500 Oracle Parkway, Redwood Shores, CA 94065 USA\n+ * or visit www.oracle.com if you need additional information or have any\n+ * questions.\n+ *\/\n+\n+package oracle.code.onnx.proto;\n+\n+import java.io.InputStream;\n+import java.io.RandomAccessFile;\n+import java.lang.foreign.Arena;\n+import java.lang.foreign.ValueLayout;\n+import java.nio.ByteOrder;\n+import java.nio.channels.FileChannel;\n+import java.util.ArrayList;\n+import java.util.Arrays;\n+import java.util.HashMap;\n+import java.util.Iterator;\n+import java.util.LinkedHashMap;\n+import java.util.List;\n+import java.util.Map;\n+import java.util.SequencedSet;\n+import java.util.stream.Collectors;\n+import jdk.incubator.code.Block;\n+import jdk.incubator.code.Op;\n+import jdk.incubator.code.TypeElement;\n+import jdk.incubator.code.Value;\n+import jdk.incubator.code.op.CoreOp;\n+import jdk.incubator.code.op.ExternalizableOp;\n+import jdk.incubator.code.op.OpFactory;\n+import jdk.incubator.code.type.FunctionType;\n+import jdk.incubator.code.type.TupleType;\n+import jdk.incubator.code.writer.OpWriter;\n+import oracle.code.onnx.CNNTest;\n+import oracle.code.onnx.OnnxRuntime;\n+import oracle.code.onnx.Tensor;\n+import oracle.code.onnx.ir.OnnxOp;\n+import oracle.code.onnx.ir.OnnxOps;\n+import oracle.code.onnx.ir.OnnxType;\n+import org.junit.jupiter.api.Test;\n+\n+\n+public class OnnxProtoModelTest {\n+\n+    static OnnxType toOnnxType(OnnxProtoModel.TypeProto tp) {\n+        if (tp.tensorType() instanceof OnnxProtoModel.TypeProto.Tensor t) {\n+            return toTensorType(t.elemType());\n+        } else if (tp.optionalType() instanceof OnnxProtoModel.TypeProto.Optional o) {\n+            return OnnxType.optional(toOnnxType(o.elemType()));\n+        } else if (tp.sequenceType()  instanceof OnnxProtoModel.TypeProto.Sequence s) {\n+            return OnnxType.seq(toOnnxType(s.elemType()));\n+        } else if (tp.mapType() instanceof OnnxProtoModel.TypeProto.Map m) {\n+            return OnnxType.map(toKeyType(m.keyType()), toOnnxType(m.valueType()));\n+        } else if (tp.sparseTensorType() instanceof OnnxProtoModel.TypeProto.SparseTensor st) {\n+            throw new UnsupportedOperationException(\"Sparse tensors not supported yet.\"); \/\/ @@@\n+        }\n+        throw new IllegalArgumentException(\"No type specified.\");\n+    }\n+\n+    static FunctionType toFunctionType(OnnxProtoModel.GraphProto g) {\n+        var paramTypes = new ArrayList<TypeElement>();\n+        for (OnnxProtoModel.ValueInfoProto input : g.inputs()) {\n+            paramTypes.add(toOnnxType(input.type()));\n+        }\n+        for (OnnxProtoModel.TensorProto init : g.initializers()) {\n+            paramTypes.add(toTensorType(init.dataType()));\n+        }\n+        var returnType = g.outputs().size() == 1\n+                ? toOnnxType(g.outputs().getFirst().type())\n+                : TupleType.tupleType(g.outputs().stream().map(OnnxProtoModel.ValueInfoProto::type).map(OnnxProtoModelTest::toOnnxType).toList());\n+        return FunctionType.functionType(returnType, paramTypes);\n+    }\n+\n+    static OnnxType toKeyType(int kt) {\n+        return switch (kt) {\n+            case 2 -> OnnxType.UINT8;\n+            case 3 -> OnnxType.INT8;\n+            case 4 -> OnnxType.UINT16;\n+            case 5 -> OnnxType.INT16;\n+            case 6 -> OnnxType.INT32;\n+            case 7 -> OnnxType.INT64;\n+            case 8 -> OnnxType.STRING;\n+            case 12 -> OnnxType.UINT32;\n+            case 13 -> OnnxType.UINT64;\n+            default -> throw new IllegalArgumentException(\"Invalid key type: \" + kt);\n+        };\n+    }\n+\n+    static OnnxType.TensorType toTensorType(int tt) {\n+        return switch (tt) {\n+            case 1 -> OnnxType.TENSOR_FLOAT32;\n+            case 2 -> OnnxType.TENSOR_UINT8;\n+            case 3 -> OnnxType.TENSOR_INT8;\n+            case 4 -> OnnxType.TENSOR_UINT16;\n+            case 5 -> OnnxType.TENSOR_INT16;\n+            case 6 -> OnnxType.TENSOR_INT32;\n+            case 7 -> OnnxType.TENSOR_INT64;\n+            case 8 -> OnnxType.TENSOR_STRING;\n+            case 9 -> OnnxType.TENSOR_BOOL;\n+            case 10 -> OnnxType.TENSOR_FLOAT16;\n+            case 11 -> OnnxType.TENSOR_FLOAT64;\n+            case 12 -> OnnxType.TENSOR_UINT32;\n+            case 13 -> OnnxType.TENSOR_UINT64;\n+            case 14 -> OnnxType.TENSOR_COMPLEX64;\n+            case 15 -> OnnxType.TENSOR_COMPLEX128;\n+            case 16 -> OnnxType.TENSOR_BFLOAT16;\n+            case 17 -> OnnxType.TENSOR_FLOAT8E4M3FN;\n+            case 18 -> OnnxType.TENSOR_FLOAT8E4M3FNUZ;\n+            case 19 -> OnnxType.TENSOR_FLOAT8E5M2;\n+            case 20 -> OnnxType.TENSOR_FLOAT8E5M2FNUZ;\n+            case 21 -> OnnxType.TENSOR_UINT4;\n+            case 22 -> OnnxType.TENSOR_INT4;\n+            case 23 -> OnnxType.TENSOR_FLOAT4E2M1;\n+            default -> OnnxType.tensor(null);\n+        };\n+    }\n+\n+    static final OpFactory ONNX_FACTORY = OpFactory.OP_FACTORY.get(OnnxOps.class);\n+\n+\n+    record OpWithNames<T extends Op> (T op, List<String> names) {\n+        public String toText() {\n+            var defNamer = OpWriter.CodeItemNamerOption.defaultValue().namer();\n+            var namer = new HashMap<Value, Integer>();\n+            return OpWriter.toText(op, OpWriter.CodeItemNamerOption.of(ci -> ci instanceof Value v ? names.get(namer.computeIfAbsent(v, _ -> namer.size())) : defNamer.apply(ci)));\n+\n+        }\n+    }\n+\n+    static OpWithNames toFuncOp(OnnxProtoModel.GraphProto g) {\n+        var valueMap = new LinkedHashMap<String, Value>();\n+        var func = CoreOp.FuncOp.func(g.name(), toFunctionType(g)).body(fb -> {\n+\n+            { \/\/ fill value map for parameters and initializers\n+                Iterator<Block.Parameter> params = fb.entryBlock().parameters().iterator();\n+                for (OnnxProtoModel.ValueInfoProto input : g.inputs()) {\n+                    valueMap.put(input.name(), params.next());\n+                }\n+                for (OnnxProtoModel.TensorProto init : g.initializers()) {\n+                    valueMap.put(init.name(), params.next());\n+                }\n+            }\n+\n+            for (OnnxProtoModel.NodeProto n : g.nodes()) {\n+                \/\/ get the op\n+                ExternalizableOp.ExternalizedOp extOp = new ExternalizableOp.ExternalizedOp(\n+                        n.opType(),\n+                        n.inputs() == null ? List.of() : n.inputs().stream().map(valueMap::get).toList(),\n+                        List.of(),\n+                        new OnnxType.TensorType(null),\n+                        n.attributes() == null ? Map.of() : n.attributes().stream().collect(Collectors.toMap(OnnxProtoModel.Attribute::name, OnnxProtoModelTest::toAttributeValue)),\n+                        List.of());\n+                OnnxOp rawOp = (OnnxOp)ONNX_FACTORY.constructOpOrFail(extOp);\n+\n+                \/\/ patch the op return type\n+                TypeElement returnType = rawOp.onnxOutputs().size() == 1\n+                        ? inferTypeVariableType(rawOp.onnxOutputs().getFirst().type(), rawOp, n)\n+                        : TupleType.tupleType(rawOp.onnxOutputs().stream().map(o -> inferTypeVariableType(o.type(), rawOp, n)).toList());\n+                extOp = new ExternalizableOp.ExternalizedOp(\n+                        extOp.name(),\n+                        extOp.operands(),\n+                        extOp.successors(),\n+                        returnType,\n+                        extOp.attributes(),\n+                        extOp.bodyDefinitions());\n+                Op.Result res = fb.op((OnnxOp)ONNX_FACTORY.constructOpOrFail(extOp));\n+\n+                \/\/ map outputs\n+                if (rawOp.onnxOutputs().size() == 1) {\n+                    valueMap.put(n.outputs().getFirst(), res);\n+                } else {\n+                    valueMap.put(n.name(), res);\n+                    for (int i = 0; i < n.outputs().size(); i++) {\n+                        valueMap.put(n.outputs().get(i), fb.op(CoreOp.tupleLoad(res, i)));\n+                    }\n+                }\n+            }\n+\n+            if (g.outputs().size() == 1) {\n+                fb.op(CoreOp._return(valueMap.get(g.outputs().getFirst().name())));\n+            } else {\n+                Op.Result ret = fb.op(CoreOp.tuple(g.outputs().stream().map(OnnxProtoModel.ValueInfoProto::name).map(valueMap::get).toList()));\n+                valueMap.put(g.name() + \"_return\", ret);\n+                fb.op(CoreOp._return(ret));\n+            }\n+        });\n+\n+        return new OpWithNames(func, List.of(valueMap.sequencedKeySet().toArray(String[]::new)));\n+    }\n+\n+    static OnnxType inferTypeVariableType(OnnxType type, OnnxOp op, OnnxProtoModel.NodeProto n) {\n+        if (type instanceof OnnxType.TypeVariable tv) {\n+            if (tv.types().size() == 1) {\n+                return tv.types().getFirst();\n+            }\n+            \/\/ search for the same type variable across inputs\n+            for (var ie : op.onnxInputs().entrySet()) {\n+                if (ie.getKey().type().equals(tv)) {\n+                    if (ie.getValue() instanceof Value v && v.type() instanceof OnnxType ot) {\n+                        return ot;\n+                    } else if (ie.getValue() instanceof List l && !l.isEmpty() && l.getFirst() instanceof Value v && v.type() instanceof OnnxType ot) {\n+                        return ot;\n+                    }\n+                }\n+            }\n+\n+            \/\/ special cases\n+            return switch (op) {\n+                case OnnxOps.Cast c ->\n+                    toTensorType((int)c.to());\n+                case OnnxOps.ConstantOfShape cos -> \/\/ get tensor type from tensor attribute\n+                    n.attributes() != null\n+                    && !n.attributes().isEmpty()\n+                    && n.attributes().getFirst().t() instanceof OnnxProtoModel.TensorProto tp\n+                            ? toTensorType(tp.dataType())\n+                            : OnnxType.TENSOR_FLOAT32; \/\/ default\n+                default ->\n+                    throw new IllegalArgumentException(\"Could not infer op type for: \" + op.toText());\n+            };\n+        }\n+        return type;\n+    }\n+\n+    static Object toAttributeValue(OnnxProtoModel.Attribute a) {\n+        return switch (a.type()) {\n+            case 1 -> a.f();\n+            case 2 -> a.i();\n+            case 3 -> a.s();\n+            case 4 -> a.t().rawData(); \/\/ @@@ need to store all tensor info + data\n+\/\/    GRAPH = 5;\n+\/\/    SPARSE_TENSOR = 11;\n+\/\/    TYPE_PROTO = 13;\n+            case 6 -> joinFloatArray(a.floats());\n+            case 7 -> joinLongArray(a.ints());\n+            case 8 -> a.strings();\n+\/\/    TENSORS = 9;\n+\/\/    GRAPHS = 10;\n+\/\/    SPARSE_TENSORS = 12;\n+\/\/    TYPE_PROTOS = 14;\n+            default -> throw new UnsupportedOperationException(\"Unsupported \" + a.type());\n+        };\n+    }\n+\n+    static float[] joinFloatArray(List<float[]> floats) {\n+        float[] join = new float[floats.stream().mapToInt(f -> f.length).sum()];\n+        int i = 0;\n+        for (float[] f : floats) {\n+            System.arraycopy(f, 0, join, i, f.length);\n+            i += f.length;\n+        }\n+        return join;\n+    }\n+\n+    static long[] joinLongArray(List<long[]> floats) {\n+        long[] join = new long[floats.stream().mapToInt(f -> f.length).sum()];\n+        int i = 0;\n+        for (long[] f : floats) {\n+            System.arraycopy(f, 0, join, i, f.length);\n+            i += f.length;\n+        }\n+        return join;\n+    }\n+\n+    @Test\n+    public void cnnLiftTest() throws Exception {\n+        try (InputStream in = CNNTest.class.getResourceAsStream(\"lenet-torchscript.onnx\")) {\n+\n+            \/\/ parse onnx protobuf model\n+            OnnxProtoModel protoModel = OnnxProtoModel.readFrom(in.readAllBytes());\n+\n+\/\/            System.out.println(model.toText());\n+\n+            \/\/ lift the cnnFuncOp from Onnx protobuf model\n+            OpWithNames<CoreOp.FuncOp> cnnFuncOp = toFuncOp(protoModel.graph());\n+\n+            System.out.println(cnnFuncOp.toText());\n+\/\/            System.out.println(cnnFuncOp.op().toText());\n+\n+            \/\/ test the lifted model\n+            try (Arena a = Arena.ofConfined()) {\n+                List<Tensor> inputValues = new ArrayList<>();\n+\n+                \/\/ initializers are extracted from the proto model directly\n+                for (OnnxProtoModel.TensorProto init : protoModel.graph().initializers()) {\n+                    inputValues.add(Tensor.ofShape(a, joinLongArray(init.dims()), init.rawData(), Tensor.ElementType.fromOnnxId(init.dataType())));\n+                }\n+\n+                \/\/ fake image\n+                float[] image = new float[28 * 28];\n+                for (int i = 13; i < 28 * 28; i+=28) {\n+                    image[i] = 1f;\n+                }\n+                inputValues.add(Tensor.ofShape(a, new long[] {1, 1, 28, 28}, image));\n+\n+                \/\/ run\n+                List<Tensor> res = OnnxRuntime.getInstance().run(a, cnnFuncOp.op().body().entryBlock(), inputValues, inputValues.size() - 1);\n+\n+                System.out.println(Arrays.toString(res.getFirst().data().toArray(ValueLayout.JAVA_FLOAT)));\n+            }\n+        }\n+    }\n+\n+    public static void main(String[] args) throws Exception {\n+        for (var fName : args) {\n+            try (var in = new RandomAccessFile(fName, \"r\")) {\n+                OnnxProtoModel model = OnnxProtoModel.readFrom(in.getChannel().map(FileChannel.MapMode.READ_ONLY, 0, in.length()));\n+\/\/                System.out.println(model.toText());\n+                System.out.println(toFuncOp(model.graph()).toText());\n+            }\n+        }\n+    }\n+}\n","filename":"cr-examples\/onnx\/src\/test\/java\/oracle\/code\/onnx\/proto\/OnnxProtoModelTest.java","additions":329,"deletions":0,"binary":false,"changes":329,"status":"added"}]}