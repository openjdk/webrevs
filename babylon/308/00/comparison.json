{"files":[{"patch":"@@ -61,1 +61,1 @@\n-        INT(int.class),\n+        INT(long.class),\n@@ -72,1 +72,1 @@\n-        INTS(int[].class),\n+        INTS(long[].class),\n","filename":"cr-examples\/onnx\/opgen\/src\/main\/java\/oracle\/code\/onnx\/OpSchema.java","additions":2,"deletions":2,"binary":false,"changes":4,"status":"modified"},{"patch":"@@ -35,1 +35,1 @@\n-            Integer c) {\n+            Long c) {\n@@ -42,1 +42,1 @@\n-            int[] c) {\n+            long[] c) {\n","filename":"cr-examples\/onnx\/src\/main\/java\/oracle\/code\/onnx\/ExplicitOnnxOperators.java","additions":2,"deletions":2,"binary":false,"changes":4,"status":"modified"},{"patch":"@@ -70,1 +70,1 @@\n-    public static <T1, T2> Tensor<T1> AffineGrid(Tensor<T1> theta, Tensor<Long> size, Optional<Integer> align_corners) {\n+    public static <T1, T2> Tensor<T1> AffineGrid(Tensor<T1> theta, Tensor<Long> size, Optional<Long> align_corners) {\n@@ -80,1 +80,1 @@\n-    public static <T> Tensor<Long> ArgMax(Tensor<T> data, Optional<Integer> keepdims, Optional<Integer> select_last_index, Optional<Integer> axis) {\n+    public static <T> Tensor<Long> ArgMax(Tensor<T> data, Optional<Long> keepdims, Optional<Long> select_last_index, Optional<Long> axis) {\n@@ -85,1 +85,1 @@\n-    public static <T> Tensor<Long> ArgMin(Tensor<T> data, Optional<Integer> keepdims, Optional<Integer> select_last_index, Optional<Integer> axis) {\n+    public static <T> Tensor<Long> ArgMin(Tensor<T> data, Optional<Long> keepdims, Optional<Long> select_last_index, Optional<Long> axis) {\n@@ -115,1 +115,1 @@\n-    public static <T> Tensor<T> AveragePool(Tensor<T> X, Optional<int[]> pads, Optional<int[]> dilations, Optional<String> auto_pad, Optional<Integer> count_include_pad, Optional<Integer> ceil_mode, Optional<int[]> strides, int[] kernel_shape) {\n+    public static <T> Tensor<T> AveragePool(Tensor<T> X, Optional<long[]> pads, Optional<long[]> dilations, Optional<String> auto_pad, Optional<Long> count_include_pad, Optional<Long> ceil_mode, Optional<long[]> strides, long[] kernel_shape) {\n@@ -121,1 +121,1 @@\n-    public static <T, T1, T2> BatchNormalizationResult<T, T1, T2> BatchNormalization(Tensor<T> X, Tensor<T1> scale, Tensor<T1> B, Tensor<T2> input_mean, Tensor<T2> input_var, Optional<Float> epsilon, Optional<Integer> training_mode, Optional<Float> momentum) {\n+    public static <T, T1, T2> BatchNormalizationResult<T, T1, T2> BatchNormalization(Tensor<T> X, Tensor<T1> scale, Tensor<T1> B, Tensor<T2> input_mean, Tensor<T2> input_var, Optional<Float> epsilon, Optional<Long> training_mode, Optional<Float> momentum) {\n@@ -127,1 +127,1 @@\n-    public static <T1, T2> Tensor<T2> Bernoulli(Tensor<T1> input, Optional<Float> seed, Optional<Integer> dtype) {\n+    public static <T1, T2> Tensor<T2> Bernoulli(Tensor<T1> input, Optional<Float> seed, Optional<Long> dtype) {\n@@ -162,1 +162,1 @@\n-    public static <T1, T2> Tensor<T2> BlackmanWindow(Tensor<T1> size, Optional<Integer> periodic, Optional<Integer> output_datatype) {\n+    public static <T1, T2> Tensor<T2> BlackmanWindow(Tensor<T1> size, Optional<Long> periodic, Optional<Long> output_datatype) {\n@@ -167,1 +167,1 @@\n-    public static <T1, T2> Tensor<T2> Cast(Tensor<T1> input, Optional<Integer> saturate, int to) {\n+    public static <T1, T2> Tensor<T2> Cast(Tensor<T1> input, Optional<Long> saturate, long to) {\n@@ -172,1 +172,1 @@\n-    public static <T1, T2> Tensor<T2> CastLike(Tensor<T1> input, Tensor<T2> target_type, Optional<Integer> saturate) {\n+    public static <T1, T2> Tensor<T2> CastLike(Tensor<T1> input, Tensor<T2> target_type, Optional<Long> saturate) {\n@@ -177,1 +177,1 @@\n-    public static <T1, T2> Tensor<T2> CastMap(Map<Long, T1> X, Optional<String> map_form, Optional<String> cast_to, Optional<Integer> max_map) {\n+    public static <T1, T2> Tensor<T2> CastMap(Map<Long, T1> X, Optional<String> map_form, Optional<String> cast_to, Optional<Long> max_map) {\n@@ -182,1 +182,1 @@\n-    public static <T1, T2> Tensor<T2> CategoryMapper(Tensor<T1> X, Optional<int[]> cats_int64s, Optional<String[]> cats_strings, Optional<Integer> default_int64, Optional<String> default_string) {\n+    public static <T1, T2> Tensor<T2> CategoryMapper(Tensor<T1> X, Optional<long[]> cats_int64s, Optional<String[]> cats_strings, Optional<Long> default_int64, Optional<String> default_string) {\n@@ -197,1 +197,1 @@\n-    public static <T, Tind> Tensor<T> CenterCropPad(Tensor<T> input_data, Tensor<Tind> shape, Optional<int[]> axes) {\n+    public static <T, Tind> Tensor<T> CenterCropPad(Tensor<T> input_data, Tensor<Tind> shape, Optional<long[]> axes) {\n@@ -207,1 +207,1 @@\n-    public static <T> Tensor<T> Col2Im(Tensor<T> input, Tensor<Long> image_shape, Tensor<Long> block_shape, Optional<int[]> pads, Optional<int[]> dilations, Optional<int[]> strides) {\n+    public static <T> Tensor<T> Col2Im(Tensor<T> input, Tensor<Long> image_shape, Tensor<Long> block_shape, Optional<long[]> pads, Optional<long[]> dilations, Optional<long[]> strides) {\n@@ -212,1 +212,1 @@\n-    public static <T, T1> Tensor<T> Compress(Tensor<T> input, Tensor<Boolean> condition, Optional<Integer> axis) {\n+    public static <T, T1> Tensor<T> Compress(Tensor<T> input, Tensor<Boolean> condition, Optional<Long> axis) {\n@@ -217,1 +217,1 @@\n-    public static <T> Tensor<T> Concat(List<Tensor<T>> inputs, int axis) {\n+    public static <T> Tensor<T> Concat(List<Tensor<T>> inputs, long axis) {\n@@ -222,1 +222,1 @@\n-    public static <S, T> Tensor<T> ConcatFromSequence(List<Tensor<S>> input_sequence, int axis, Optional<Integer> new_axis) {\n+    public static <S, T> Tensor<T> ConcatFromSequence(List<Tensor<S>> input_sequence, long axis, Optional<Long> new_axis) {\n@@ -227,1 +227,1 @@\n-    public static <T> Tensor<T> Constant(Optional<Integer> value_int, Optional<float[]> value_floats, Optional<String[]> value_strings, Optional<Float> value_float, Optional<String> value_string, Optional<int[]> value_ints, Optional<byte[]> sparse_value, Optional<byte[]> value) {\n+    public static <T> Tensor<T> Constant(Optional<Long> value_int, Optional<float[]> value_floats, Optional<String[]> value_strings, Optional<Float> value_float, Optional<String> value_string, Optional<long[]> value_ints, Optional<byte[]> sparse_value, Optional<byte[]> value) {\n@@ -237,1 +237,1 @@\n-    public static <T> Tensor<T> Conv(Tensor<T> X, Tensor<T> W, Optional<Tensor<T>> B, Optional<int[]> pads, Optional<int[]> dilations, Optional<String> auto_pad, Optional<int[]> strides, Optional<Integer> group, Optional<int[]> kernel_shape) {\n+    public static <T> Tensor<T> Conv(Tensor<T> X, Tensor<T> W, Optional<Tensor<T>> B, Optional<long[]> pads, Optional<long[]> dilations, Optional<String> auto_pad, Optional<long[]> strides, Optional<Long> group, Optional<long[]> kernel_shape) {\n@@ -242,1 +242,1 @@\n-    public static <T1, T2, T3> Tensor<Integer> ConvInteger(Tensor<T1> x, Tensor<T2> w, Optional<Tensor<T1>> x_zero_point, Optional<Tensor<T2>> w_zero_point, Optional<int[]> pads, Optional<int[]> dilations, Optional<String> auto_pad, Optional<int[]> strides, Optional<Integer> group, Optional<int[]> kernel_shape) {\n+    public static <T1, T2, T3> Tensor<Integer> ConvInteger(Tensor<T1> x, Tensor<T2> w, Optional<Tensor<T1>> x_zero_point, Optional<Tensor<T2>> w_zero_point, Optional<long[]> pads, Optional<long[]> dilations, Optional<String> auto_pad, Optional<long[]> strides, Optional<Long> group, Optional<long[]> kernel_shape) {\n@@ -247,1 +247,1 @@\n-    public static <T> Tensor<T> ConvTranspose(Tensor<T> X, Tensor<T> W, Optional<Tensor<T>> B, Optional<int[]> output_shape, Optional<int[]> pads, Optional<int[]> dilations, Optional<String> auto_pad, Optional<int[]> strides, Optional<Integer> group, Optional<int[]> kernel_shape, Optional<int[]> output_padding) {\n+    public static <T> Tensor<T> ConvTranspose(Tensor<T> X, Tensor<T> W, Optional<Tensor<T>> B, Optional<long[]> output_shape, Optional<long[]> pads, Optional<long[]> dilations, Optional<String> auto_pad, Optional<long[]> strides, Optional<Long> group, Optional<long[]> kernel_shape, Optional<long[]> output_padding) {\n@@ -262,1 +262,1 @@\n-    public static <T, T2> Tensor<T> CumSum(Tensor<T> x, Tensor<T2> axis, Optional<Integer> exclusive, Optional<Integer> reverse) {\n+    public static <T, T2> Tensor<T> CumSum(Tensor<T> x, Tensor<T2> axis, Optional<Long> exclusive, Optional<Long> reverse) {\n@@ -267,1 +267,1 @@\n-    public static <T1, T2> Tensor<T1> DFT(Tensor<T1> input, Optional<Tensor<T2>> dft_length, Optional<Tensor<Long>> axis, Optional<Integer> inverse, Optional<Integer> onesided) {\n+    public static <T1, T2> Tensor<T1> DFT(Tensor<T1> input, Optional<Tensor<T2>> dft_length, Optional<Tensor<Long>> axis, Optional<Long> inverse, Optional<Long> onesided) {\n@@ -272,1 +272,1 @@\n-    public static <T> Tensor<T> DeformConv(Tensor<T> X, Tensor<T> W, Tensor<T> offset, Optional<Tensor<T>> B, Optional<Tensor<T>> mask, Optional<int[]> pads, Optional<int[]> dilations, Optional<int[]> strides, Optional<Integer> offset_group, Optional<Integer> group, Optional<int[]> kernel_shape) {\n+    public static <T> Tensor<T> DeformConv(Tensor<T> X, Tensor<T> W, Tensor<T> offset, Optional<Tensor<T>> B, Optional<Tensor<T>> mask, Optional<long[]> pads, Optional<long[]> dilations, Optional<long[]> strides, Optional<Long> offset_group, Optional<Long> group, Optional<long[]> kernel_shape) {\n@@ -277,1 +277,1 @@\n-    public static <T> Tensor<T> DepthToSpace(Tensor<T> input, Optional<String> mode, int blocksize) {\n+    public static <T> Tensor<T> DepthToSpace(Tensor<T> input, Optional<String> mode, long blocksize) {\n@@ -282,1 +282,1 @@\n-    public static <T1, T2> Tensor<T2> DequantizeLinear(Tensor<T1> x, Tensor<T2> x_scale, Optional<Tensor<T1>> x_zero_point, Optional<Integer> axis, Optional<Integer> block_size) {\n+    public static <T1, T2> Tensor<T2> DequantizeLinear(Tensor<T1> x, Tensor<T2> x_scale, Optional<Tensor<T1>> x_zero_point, Optional<Long> axis, Optional<Long> block_size) {\n@@ -292,1 +292,1 @@\n-    public static <T1, T2> Tensor<T2> DictVectorizer(Map<?, ?> X, Optional<String[]> string_vocabulary, Optional<int[]> int64_vocabulary) {\n+    public static <T1, T2> Tensor<T2> DictVectorizer(Map<?, ?> X, Optional<String[]> string_vocabulary, Optional<long[]> int64_vocabulary) {\n@@ -303,1 +303,1 @@\n-    public static <T, T1, T2> DropoutResult<T, T1, T2> Dropout(Tensor<T> data, Optional<Tensor<T1>> ratio, Optional<Tensor<Boolean>> training_mode, Optional<Integer> seed) {\n+    public static <T, T1, T2> DropoutResult<T, T1, T2> Dropout(Tensor<T> data, Optional<Tensor<T1>> ratio, Optional<Tensor<Boolean>> training_mode, Optional<Long> seed) {\n@@ -346,1 +346,1 @@\n-    public static <T1, T2> Tensor<T2> EyeLike(Tensor<T1> input, Optional<Integer> dtype, Optional<Integer> k) {\n+    public static <T1, T2> Tensor<T2> EyeLike(Tensor<T1> input, Optional<Long> dtype, Optional<Long> k) {\n@@ -351,1 +351,1 @@\n-    public static <T1> Tensor<Float> FeatureVectorizer(List<Tensor<T1>> X, Optional<int[]> inputdimensions) {\n+    public static <T1> Tensor<Float> FeatureVectorizer(List<Tensor<T1>> X, Optional<long[]> inputdimensions) {\n@@ -356,1 +356,1 @@\n-    public static <T> Tensor<T> Flatten(Tensor<T> input, Optional<Integer> axis) {\n+    public static <T> Tensor<T> Flatten(Tensor<T> input, Optional<Long> axis) {\n@@ -367,1 +367,1 @@\n-    public static <T, T1> GRUResult<T, T1> GRU(Tensor<T> X, Tensor<T> W, Tensor<T> R, Optional<Tensor<T>> B, Optional<Tensor<Integer>> sequence_lens, Optional<Tensor<T>> initial_h, Optional<Integer> layout, Optional<float[]> activation_alpha, Optional<Integer> hidden_size, Optional<float[]> activation_beta, Optional<String[]> activations, Optional<Integer> linear_before_reset, Optional<Float> clip, Optional<String> direction) {\n+    public static <T, T1> GRUResult<T, T1> GRU(Tensor<T> X, Tensor<T> W, Tensor<T> R, Optional<Tensor<T>> B, Optional<Tensor<Integer>> sequence_lens, Optional<Tensor<T>> initial_h, Optional<Long> layout, Optional<float[]> activation_alpha, Optional<Long> hidden_size, Optional<float[]> activation_beta, Optional<String[]> activations, Optional<Long> linear_before_reset, Optional<Float> clip, Optional<String> direction) {\n@@ -373,1 +373,1 @@\n-    public static <T, Tind> Tensor<T> Gather(Tensor<T> data, Tensor<Tind> indices, Optional<Integer> axis) {\n+    public static <T, Tind> Tensor<T> Gather(Tensor<T> data, Tensor<Tind> indices, Optional<Long> axis) {\n@@ -378,1 +378,1 @@\n-    public static <T, Tind> Tensor<T> GatherElements(Tensor<T> data, Tensor<Tind> indices, Optional<Integer> axis) {\n+    public static <T, Tind> Tensor<T> GatherElements(Tensor<T> data, Tensor<Tind> indices, Optional<Long> axis) {\n@@ -383,1 +383,1 @@\n-    public static <T> Tensor<T> GatherND(Tensor<T> data, Tensor<Long> indices, Optional<Integer> batch_dims) {\n+    public static <T> Tensor<T> GatherND(Tensor<T> data, Tensor<Long> indices, Optional<Long> batch_dims) {\n@@ -393,1 +393,1 @@\n-    public static <T> Tensor<T> Gemm(Tensor<T> A, Tensor<T> B, Optional<Tensor<T>> C, Optional<Float> alpha, Optional<Integer> transB, Optional<Float> beta, Optional<Integer> transA) {\n+    public static <T> Tensor<T> Gemm(Tensor<T> A, Tensor<T> B, Optional<Tensor<T>> C, Optional<Float> alpha, Optional<Long> transB, Optional<Float> beta, Optional<Long> transA) {\n@@ -403,1 +403,1 @@\n-    public static <T> Tensor<T> GlobalLpPool(Tensor<T> X, Optional<Integer> p) {\n+    public static <T> Tensor<T> GlobalLpPool(Tensor<T> X, Optional<Long> p) {\n@@ -428,1 +428,1 @@\n-    public static <T1, T2> Tensor<T1> GridSample(Tensor<T1> X, Tensor<T2> grid, Optional<String> mode, Optional<Integer> align_corners, Optional<String> padding_mode) {\n+    public static <T1, T2> Tensor<T1> GridSample(Tensor<T1> X, Tensor<T2> grid, Optional<String> mode, Optional<Long> align_corners, Optional<String> padding_mode) {\n@@ -433,1 +433,1 @@\n-    public static <T> Tensor<T> GroupNormalization(Tensor<T> X, Tensor<T> scale, Tensor<T> bias, Optional<Float> epsilon, Optional<Integer> stash_type, int num_groups) {\n+    public static <T> Tensor<T> GroupNormalization(Tensor<T> X, Tensor<T> scale, Tensor<T> bias, Optional<Float> epsilon, Optional<Long> stash_type, long num_groups) {\n@@ -438,1 +438,1 @@\n-    public static <T1, T2> Tensor<T2> HammingWindow(Tensor<T1> size, Optional<Integer> periodic, Optional<Integer> output_datatype) {\n+    public static <T1, T2> Tensor<T2> HammingWindow(Tensor<T1> size, Optional<Long> periodic, Optional<Long> output_datatype) {\n@@ -443,1 +443,1 @@\n-    public static <T1, T2> Tensor<T2> HannWindow(Tensor<T1> size, Optional<Integer> periodic, Optional<Integer> output_datatype) {\n+    public static <T1, T2> Tensor<T2> HannWindow(Tensor<T1> size, Optional<Long> periodic, Optional<Long> output_datatype) {\n@@ -458,1 +458,1 @@\n-    public static <T> Tensor<T> Hardmax(Tensor<T> input, Optional<Integer> axis) {\n+    public static <T> Tensor<T> Hardmax(Tensor<T> input, Optional<Long> axis) {\n@@ -473,1 +473,1 @@\n-    public static <T> Tensor<T> Imputer(Tensor<T> X, Optional<Integer> replaced_value_int64, Optional<Float> replaced_value_float, Optional<int[]> imputed_value_int64s, Optional<float[]> imputed_value_floats) {\n+    public static <T> Tensor<T> Imputer(Tensor<T> X, Optional<Long> replaced_value_int64, Optional<Float> replaced_value_float, Optional<long[]> imputed_value_int64s, Optional<float[]> imputed_value_floats) {\n@@ -483,1 +483,1 @@\n-    public static <T1, T2> Tensor<Boolean> IsInf(Tensor<T1> X, Optional<Integer> detect_negative, Optional<Integer> detect_positive) {\n+    public static <T1, T2> Tensor<Boolean> IsInf(Tensor<T1> X, Optional<Long> detect_negative, Optional<Long> detect_positive) {\n@@ -493,1 +493,1 @@\n-    public static <T> Tensor<T> LRN(Tensor<T> X, int size, Optional<Float> alpha, Optional<Float> bias, Optional<Float> beta) {\n+    public static <T> Tensor<T> LRN(Tensor<T> X, long size, Optional<Float> alpha, Optional<Float> bias, Optional<Float> beta) {\n@@ -499,1 +499,1 @@\n-    public static <T, T1> LSTMResult<T, T1> LSTM(Tensor<T> X, Tensor<T> W, Tensor<T> R, Optional<Tensor<T>> B, Optional<Tensor<Integer>> sequence_lens, Optional<Tensor<T>> initial_h, Optional<Tensor<T>> initial_c, Optional<Tensor<T>> P, Optional<Integer> layout, Optional<Integer> input_forget, Optional<float[]> activation_alpha, Optional<Integer> hidden_size, Optional<float[]> activation_beta, Optional<String[]> activations, Optional<Float> clip, Optional<String> direction) {\n+    public static <T, T1> LSTMResult<T, T1> LSTM(Tensor<T> X, Tensor<T> W, Tensor<T> R, Optional<Tensor<T>> B, Optional<Tensor<Integer>> sequence_lens, Optional<Tensor<T>> initial_h, Optional<Tensor<T>> initial_c, Optional<Tensor<T>> P, Optional<Long> layout, Optional<Long> input_forget, Optional<float[]> activation_alpha, Optional<Long> hidden_size, Optional<float[]> activation_beta, Optional<String[]> activations, Optional<Float> clip, Optional<String> direction) {\n@@ -505,1 +505,1 @@\n-    public static <T1, T2> Tensor<T2> LabelEncoder(Tensor<T1> X, Optional<String[]> values_strings, Optional<int[]> keys_int64s, Optional<byte[]> keys_tensor, Optional<String[]> keys_strings, Optional<Float> default_float, Optional<float[]> keys_floats, Optional<byte[]> default_tensor, Optional<Integer> default_int64, Optional<byte[]> values_tensor, Optional<int[]> values_int64s, Optional<String> default_string, Optional<float[]> values_floats) {\n+    public static <T1, T2> Tensor<T2> LabelEncoder(Tensor<T1> X, Optional<String[]> values_strings, Optional<long[]> keys_int64s, Optional<byte[]> keys_tensor, Optional<String[]> keys_strings, Optional<Float> default_float, Optional<float[]> keys_floats, Optional<byte[]> default_tensor, Optional<Long> default_int64, Optional<byte[]> values_tensor, Optional<long[]> values_int64s, Optional<String> default_string, Optional<float[]> values_floats) {\n@@ -511,1 +511,1 @@\n-    public static <T, U> LayerNormalizationResult<T, U> LayerNormalization(Tensor<T> X, Tensor<T> Scale, Optional<Tensor<T>> B, Optional<Float> epsilon, Optional<Integer> stash_type, Optional<Integer> axis) {\n+    public static <T, U> LayerNormalizationResult<T, U> LayerNormalization(Tensor<T> X, Tensor<T> Scale, Optional<Tensor<T>> B, Optional<Float> epsilon, Optional<Long> stash_type, Optional<Long> axis) {\n@@ -533,1 +533,1 @@\n-    public static <T1, T2> LinearClassifierResult<T1, T2> LinearClassifier(Tensor<T1> X, Optional<int[]> classlabels_ints, Optional<String> post_transform, float[] coefficients, Optional<Integer> multi_class, Optional<float[]> intercepts, Optional<String[]> classlabels_strings) {\n+    public static <T1, T2> LinearClassifierResult<T1, T2> LinearClassifier(Tensor<T1> X, Optional<long[]> classlabels_ints, Optional<String> post_transform, float[] coefficients, Optional<Long> multi_class, Optional<float[]> intercepts, Optional<String[]> classlabels_strings) {\n@@ -539,1 +539,1 @@\n-    public static <T> Tensor<Float> LinearRegressor(Tensor<T> X, Optional<String> post_transform, Optional<float[]> coefficients, Optional<Integer> targets, Optional<float[]> intercepts) {\n+    public static <T> Tensor<Float> LinearRegressor(Tensor<T> X, Optional<String> post_transform, Optional<float[]> coefficients, Optional<Long> targets, Optional<float[]> intercepts) {\n@@ -549,1 +549,1 @@\n-    public static <T> Tensor<T> LogSoftmax(Tensor<T> input, Optional<Integer> axis) {\n+    public static <T> Tensor<T> LogSoftmax(Tensor<T> input, Optional<Long> axis) {\n@@ -554,1 +554,1 @@\n-    public static <T> Tensor<T> LpNormalization(Tensor<T> input, Optional<Integer> p, Optional<Integer> axis) {\n+    public static <T> Tensor<T> LpNormalization(Tensor<T> input, Optional<Long> p, Optional<Long> axis) {\n@@ -559,1 +559,1 @@\n-    public static <T> Tensor<T> LpPool(Tensor<T> X, Optional<Integer> p, Optional<int[]> pads, Optional<int[]> dilations, Optional<String> auto_pad, Optional<Integer> ceil_mode, Optional<int[]> strides, int[] kernel_shape) {\n+    public static <T> Tensor<T> LpPool(Tensor<T> X, Optional<Long> p, Optional<long[]> pads, Optional<long[]> dilations, Optional<String> auto_pad, Optional<Long> ceil_mode, Optional<long[]> strides, long[] kernel_shape) {\n@@ -580,1 +580,1 @@\n-    public static <T, I> MaxPoolResult<T, I> MaxPool(Tensor<T> X, Optional<int[]> pads, Optional<int[]> dilations, Optional<String> auto_pad, Optional<Integer> ceil_mode, Optional<Integer> storage_order, Optional<int[]> strides, int[] kernel_shape) {\n+    public static <T, I> MaxPoolResult<T, I> MaxPool(Tensor<T> X, Optional<long[]> pads, Optional<long[]> dilations, Optional<String> auto_pad, Optional<Long> ceil_mode, Optional<Long> storage_order, Optional<long[]> strides, long[] kernel_shape) {\n@@ -586,1 +586,1 @@\n-    public static <T> Tensor<T> MaxRoiPool(Tensor<T> X, Tensor<T> rois, Optional<Float> spatial_scale, int[] pooled_shape) {\n+    public static <T> Tensor<T> MaxRoiPool(Tensor<T> X, Tensor<T> rois, Optional<Float> spatial_scale, long[] pooled_shape) {\n@@ -591,1 +591,1 @@\n-    public static <T1, T2> Tensor<T1> MaxUnpool(Tensor<T1> X, Tensor<Long> I, Optional<Tensor<Long>> output_shape, Optional<int[]> pads, Optional<int[]> strides, int[] kernel_shape) {\n+    public static <T1, T2> Tensor<T1> MaxUnpool(Tensor<T1> X, Tensor<Long> I, Optional<Tensor<Long>> output_shape, Optional<long[]> pads, Optional<long[]> strides, long[] kernel_shape) {\n@@ -601,1 +601,1 @@\n-    public static <T> Tensor<T> MeanVarianceNormalization(Tensor<T> X, Optional<int[]> axes) {\n+    public static <T> Tensor<T> MeanVarianceNormalization(Tensor<T> X, Optional<long[]> axes) {\n@@ -606,1 +606,1 @@\n-    public static <T1, T2, T3> Tensor<T3> MelWeightMatrix(Tensor<T1> num_mel_bins, Tensor<T1> dft_length, Tensor<T1> sample_rate, Tensor<T2> lower_edge_hertz, Tensor<T2> upper_edge_hertz, Optional<Integer> output_datatype) {\n+    public static <T1, T2, T3> Tensor<T3> MelWeightMatrix(Tensor<T1> num_mel_bins, Tensor<T1> dft_length, Tensor<T1> sample_rate, Tensor<T2> lower_edge_hertz, Tensor<T2> upper_edge_hertz, Optional<Long> output_datatype) {\n@@ -621,1 +621,1 @@\n-    public static <T> Tensor<T> Mod(Tensor<T> A, Tensor<T> B, Optional<Integer> fmod) {\n+    public static <T> Tensor<T> Mod(Tensor<T> A, Tensor<T> B, Optional<Long> fmod) {\n@@ -636,1 +636,1 @@\n-    public static <T1, T2> Tensor<T2> Multinomial(Tensor<T1> input, Optional<Float> seed, Optional<Integer> sample_size, Optional<Integer> dtype) {\n+    public static <T1, T2> Tensor<T2> Multinomial(Tensor<T1> input, Optional<Float> seed, Optional<Long> sample_size, Optional<Long> dtype) {\n@@ -646,1 +646,1 @@\n-    public static <T, Tind> Tensor<T> NegativeLogLikelihoodLoss(Tensor<T> input, Tensor<Tind> target, Optional<Tensor<T>> weight, Optional<Integer> ignore_index, Optional<String> reduction) {\n+    public static <T, Tind> Tensor<T> NegativeLogLikelihoodLoss(Tensor<T> input, Tensor<Tind> target, Optional<Tensor<T>> weight, Optional<Long> ignore_index, Optional<String> reduction) {\n@@ -651,1 +651,1 @@\n-    public static Tensor<Long> NonMaxSuppression(Tensor<Float> boxes, Tensor<Float> scores, Optional<Tensor<Long>> max_output_boxes_per_class, Optional<Tensor<Float>> iou_threshold, Optional<Tensor<Float>> score_threshold, Optional<Integer> center_point_box) {\n+    public static Tensor<Long> NonMaxSuppression(Tensor<Float> boxes, Tensor<Float> scores, Optional<Tensor<Long>> max_output_boxes_per_class, Optional<Tensor<Float>> iou_threshold, Optional<Tensor<Float>> score_threshold, Optional<Long> center_point_box) {\n@@ -671,1 +671,1 @@\n-    public static <T1, T2, T3> Tensor<T3> OneHot(Tensor<T1> indices, Tensor<T2> depth, Tensor<T3> values, Optional<Integer> axis) {\n+    public static <T1, T2, T3> Tensor<T3> OneHot(Tensor<T1> indices, Tensor<T2> depth, Tensor<T3> values, Optional<Long> axis) {\n@@ -676,1 +676,1 @@\n-    public static <T> Tensor<Float> OneHotEncoder(Tensor<T> X, Optional<String[]> cats_strings, Optional<int[]> cats_int64s, Optional<Integer> zeros) {\n+    public static <T> Tensor<Float> OneHotEncoder(Tensor<T> X, Optional<String[]> cats_strings, Optional<long[]> cats_int64s, Optional<Long> zeros) {\n@@ -716,1 +716,1 @@\n-    public static <T1, T2, T3, T4> Tensor<T3> QLinearConv(Tensor<T1> x, Tensor<Float> x_scale, Tensor<T1> x_zero_point, Tensor<T2> w, Tensor<Float> w_scale, Tensor<T2> w_zero_point, Tensor<Float> y_scale, Tensor<T3> y_zero_point, Optional<Tensor<Integer>> B, Optional<int[]> pads, Optional<int[]> dilations, Optional<String> auto_pad, Optional<int[]> strides, Optional<Integer> group, Optional<int[]> kernel_shape) {\n+    public static <T1, T2, T3, T4> Tensor<T3> QLinearConv(Tensor<T1> x, Tensor<Float> x_scale, Tensor<T1> x_zero_point, Tensor<T2> w, Tensor<Float> w_scale, Tensor<T2> w_zero_point, Tensor<Float> y_scale, Tensor<T3> y_zero_point, Optional<Tensor<Integer>> B, Optional<long[]> pads, Optional<long[]> dilations, Optional<String> auto_pad, Optional<long[]> strides, Optional<Long> group, Optional<long[]> kernel_shape) {\n@@ -726,1 +726,1 @@\n-    public static <T1, T2> Tensor<T2> QuantizeLinear(Tensor<T1> x, Tensor<T1> y_scale, Optional<Tensor<T2>> y_zero_point, Optional<Integer> output_dtype, Optional<Integer> saturate, Optional<Integer> axis, Optional<Integer> block_size) {\n+    public static <T1, T2> Tensor<T2> QuantizeLinear(Tensor<T1> x, Tensor<T1> y_scale, Optional<Tensor<T2>> y_zero_point, Optional<Long> output_dtype, Optional<Long> saturate, Optional<Long> axis, Optional<Long> block_size) {\n@@ -732,1 +732,1 @@\n-    public static <T, T1> RNNResult<T, T1> RNN(Tensor<T> X, Tensor<T> W, Tensor<T> R, Optional<Tensor<T>> B, Optional<Tensor<Integer>> sequence_lens, Optional<Tensor<T>> initial_h, Optional<Integer> layout, Optional<float[]> activation_alpha, Optional<Integer> hidden_size, Optional<float[]> activation_beta, Optional<String[]> activations, Optional<Float> clip, Optional<String> direction) {\n+    public static <T, T1> RNNResult<T, T1> RNN(Tensor<T> X, Tensor<T> W, Tensor<T> R, Optional<Tensor<T>> B, Optional<Tensor<Integer>> sequence_lens, Optional<Tensor<T>> initial_h, Optional<Long> layout, Optional<float[]> activation_alpha, Optional<Long> hidden_size, Optional<float[]> activation_beta, Optional<String[]> activations, Optional<Float> clip, Optional<String> direction) {\n@@ -738,1 +738,1 @@\n-    public static <T> Tensor<T> RandomNormal(int[] shape, Optional<Float> seed, Optional<Float> mean, Optional<Float> scale, Optional<Integer> dtype) {\n+    public static <T> Tensor<T> RandomNormal(long[] shape, Optional<Float> seed, Optional<Float> mean, Optional<Float> scale, Optional<Long> dtype) {\n@@ -743,1 +743,1 @@\n-    public static <T1, T2> Tensor<T2> RandomNormalLike(Tensor<T1> input, Optional<Float> seed, Optional<Float> mean, Optional<Float> scale, Optional<Integer> dtype) {\n+    public static <T1, T2> Tensor<T2> RandomNormalLike(Tensor<T1> input, Optional<Float> seed, Optional<Float> mean, Optional<Float> scale, Optional<Long> dtype) {\n@@ -748,1 +748,1 @@\n-    public static <T> Tensor<T> RandomUniform(Optional<Float> high, int[] shape, Optional<Float> seed, Optional<Float> low, Optional<Integer> dtype) {\n+    public static <T> Tensor<T> RandomUniform(Optional<Float> high, long[] shape, Optional<Float> seed, Optional<Float> low, Optional<Long> dtype) {\n@@ -753,1 +753,1 @@\n-    public static <T1, T2> Tensor<T2> RandomUniformLike(Tensor<T1> input, Optional<Float> high, Optional<Float> seed, Optional<Float> low, Optional<Integer> dtype) {\n+    public static <T1, T2> Tensor<T2> RandomUniformLike(Tensor<T1> input, Optional<Float> high, Optional<Float> seed, Optional<Float> low, Optional<Long> dtype) {\n@@ -768,1 +768,1 @@\n-    public static <T> Tensor<T> ReduceL1(Tensor<T> data, Optional<Tensor<Long>> axes, Optional<Integer> noop_with_empty_axes, Optional<Integer> keepdims) {\n+    public static <T> Tensor<T> ReduceL1(Tensor<T> data, Optional<Tensor<Long>> axes, Optional<Long> noop_with_empty_axes, Optional<Long> keepdims) {\n@@ -773,1 +773,1 @@\n-    public static <T> Tensor<T> ReduceL2(Tensor<T> data, Optional<Tensor<Long>> axes, Optional<Integer> noop_with_empty_axes, Optional<Integer> keepdims) {\n+    public static <T> Tensor<T> ReduceL2(Tensor<T> data, Optional<Tensor<Long>> axes, Optional<Long> noop_with_empty_axes, Optional<Long> keepdims) {\n@@ -778,1 +778,1 @@\n-    public static <T> Tensor<T> ReduceLogSum(Tensor<T> data, Optional<Tensor<Long>> axes, Optional<Integer> noop_with_empty_axes, Optional<Integer> keepdims) {\n+    public static <T> Tensor<T> ReduceLogSum(Tensor<T> data, Optional<Tensor<Long>> axes, Optional<Long> noop_with_empty_axes, Optional<Long> keepdims) {\n@@ -783,1 +783,1 @@\n-    public static <T> Tensor<T> ReduceLogSumExp(Tensor<T> data, Optional<Tensor<Long>> axes, Optional<Integer> noop_with_empty_axes, Optional<Integer> keepdims) {\n+    public static <T> Tensor<T> ReduceLogSumExp(Tensor<T> data, Optional<Tensor<Long>> axes, Optional<Long> noop_with_empty_axes, Optional<Long> keepdims) {\n@@ -788,1 +788,1 @@\n-    public static <T> Tensor<T> ReduceMax(Tensor<T> data, Optional<Tensor<Long>> axes, Optional<Integer> noop_with_empty_axes, Optional<Integer> keepdims) {\n+    public static <T> Tensor<T> ReduceMax(Tensor<T> data, Optional<Tensor<Long>> axes, Optional<Long> noop_with_empty_axes, Optional<Long> keepdims) {\n@@ -793,1 +793,1 @@\n-    public static <T> Tensor<T> ReduceMean(Tensor<T> data, Optional<Tensor<Long>> axes, Optional<Integer> noop_with_empty_axes, Optional<Integer> keepdims) {\n+    public static <T> Tensor<T> ReduceMean(Tensor<T> data, Optional<Tensor<Long>> axes, Optional<Long> noop_with_empty_axes, Optional<Long> keepdims) {\n@@ -798,1 +798,1 @@\n-    public static <T> Tensor<T> ReduceMin(Tensor<T> data, Optional<Tensor<Long>> axes, Optional<Integer> noop_with_empty_axes, Optional<Integer> keepdims) {\n+    public static <T> Tensor<T> ReduceMin(Tensor<T> data, Optional<Tensor<Long>> axes, Optional<Long> noop_with_empty_axes, Optional<Long> keepdims) {\n@@ -803,1 +803,1 @@\n-    public static <T> Tensor<T> ReduceProd(Tensor<T> data, Optional<Tensor<Long>> axes, Optional<Integer> noop_with_empty_axes, Optional<Integer> keepdims) {\n+    public static <T> Tensor<T> ReduceProd(Tensor<T> data, Optional<Tensor<Long>> axes, Optional<Long> noop_with_empty_axes, Optional<Long> keepdims) {\n@@ -808,1 +808,1 @@\n-    public static <T> Tensor<T> ReduceSum(Tensor<T> data, Optional<Tensor<Long>> axes, Optional<Integer> noop_with_empty_axes, Optional<Integer> keepdims) {\n+    public static <T> Tensor<T> ReduceSum(Tensor<T> data, Optional<Tensor<Long>> axes, Optional<Long> noop_with_empty_axes, Optional<Long> keepdims) {\n@@ -813,1 +813,1 @@\n-    public static <T> Tensor<T> ReduceSumSquare(Tensor<T> data, Optional<Tensor<Long>> axes, Optional<Integer> noop_with_empty_axes, Optional<Integer> keepdims) {\n+    public static <T> Tensor<T> ReduceSumSquare(Tensor<T> data, Optional<Tensor<Long>> axes, Optional<Long> noop_with_empty_axes, Optional<Long> keepdims) {\n@@ -828,1 +828,1 @@\n-    public static <T> Tensor<T> Reshape(Tensor<T> data, Tensor<Long> shape, Optional<Integer> allowzero) {\n+    public static <T> Tensor<T> Reshape(Tensor<T> data, Tensor<Long> shape, Optional<Long> allowzero) {\n@@ -833,1 +833,1 @@\n-    public static <T1, T2> Tensor<T1> Resize(Tensor<T1> X, Optional<Tensor<T2>> roi, Optional<Tensor<Float>> scales, Optional<Tensor<Long>> sizes, Optional<String> mode, Optional<Float> extrapolation_value, Optional<String> nearest_mode, Optional<Integer> antialias, Optional<Float> cubic_coeff_a, Optional<int[]> axes, Optional<String> coordinate_transformation_mode, Optional<String> keep_aspect_ratio_policy, Optional<Integer> exclude_outside) {\n+    public static <T1, T2> Tensor<T1> Resize(Tensor<T1> X, Optional<Tensor<T2>> roi, Optional<Tensor<Float>> scales, Optional<Tensor<Long>> sizes, Optional<String> mode, Optional<Float> extrapolation_value, Optional<String> nearest_mode, Optional<Long> antialias, Optional<Float> cubic_coeff_a, Optional<long[]> axes, Optional<String> coordinate_transformation_mode, Optional<String> keep_aspect_ratio_policy, Optional<Long> exclude_outside) {\n@@ -838,1 +838,1 @@\n-    public static <T> Tensor<T> ReverseSequence(Tensor<T> input, Tensor<Long> sequence_lens, Optional<Integer> time_axis, Optional<Integer> batch_axis) {\n+    public static <T> Tensor<T> ReverseSequence(Tensor<T> input, Tensor<Long> sequence_lens, Optional<Long> time_axis, Optional<Long> batch_axis) {\n@@ -843,1 +843,1 @@\n-    public static <T1, T2> Tensor<T1> RoiAlign(Tensor<T1> X, Tensor<T1> rois, Tensor<Long> batch_indices, Optional<String> mode, Optional<Integer> output_width, Optional<Float> spatial_scale, Optional<String> coordinate_transformation_mode, Optional<Integer> sampling_ratio, Optional<Integer> output_height) {\n+    public static <T1, T2> Tensor<T1> RoiAlign(Tensor<T1> X, Tensor<T1> rois, Tensor<Long> batch_indices, Optional<String> mode, Optional<Long> output_width, Optional<Float> spatial_scale, Optional<String> coordinate_transformation_mode, Optional<Long> sampling_ratio, Optional<Long> output_height) {\n@@ -853,1 +853,1 @@\n-    public static <T1, T2> Tensor<T1> STFT(Tensor<T1> signal, Tensor<T2> frame_step, Optional<Tensor<T1>> window, Optional<Tensor<T2>> frame_length, Optional<Integer> onesided) {\n+    public static <T1, T2> Tensor<T1> STFT(Tensor<T1> signal, Tensor<T2> frame_step, Optional<Tensor<T1>> window, Optional<Tensor<T2>> frame_length, Optional<Long> onesided) {\n@@ -859,1 +859,1 @@\n-    public static <T1, T2> SVMClassifierResult<T1, T2> SVMClassifier(Tensor<T1> X, Optional<float[]> prob_b, Optional<float[]> kernel_params, Optional<String> kernel_type, Optional<int[]> classlabels_ints, Optional<String> post_transform, Optional<float[]> rho, Optional<float[]> coefficients, Optional<float[]> support_vectors, Optional<int[]> vectors_per_class, Optional<float[]> prob_a, Optional<String[]> classlabels_strings) {\n+    public static <T1, T2> SVMClassifierResult<T1, T2> SVMClassifier(Tensor<T1> X, Optional<float[]> prob_b, Optional<float[]> kernel_params, Optional<String> kernel_type, Optional<long[]> classlabels_ints, Optional<String> post_transform, Optional<float[]> rho, Optional<float[]> coefficients, Optional<float[]> support_vectors, Optional<long[]> vectors_per_class, Optional<float[]> prob_a, Optional<String[]> classlabels_strings) {\n@@ -865,1 +865,1 @@\n-    public static <T> Tensor<Float> SVMRegressor(Tensor<T> X, Optional<String> kernel_type, Optional<float[]> kernel_params, Optional<Integer> n_supports, Optional<float[]> rho, Optional<String> post_transform, Optional<float[]> coefficients, Optional<float[]> support_vectors, Optional<Integer> one_class) {\n+    public static <T> Tensor<Float> SVMRegressor(Tensor<T> X, Optional<String> kernel_type, Optional<float[]> kernel_params, Optional<Long> n_supports, Optional<float[]> rho, Optional<String> post_transform, Optional<float[]> coefficients, Optional<float[]> support_vectors, Optional<Long> one_class) {\n@@ -875,1 +875,1 @@\n-    public static <T, Tind> Tensor<T> Scatter(Tensor<T> data, Tensor<Tind> indices, Tensor<T> updates, Optional<Integer> axis) {\n+    public static <T, Tind> Tensor<T> Scatter(Tensor<T> data, Tensor<Tind> indices, Tensor<T> updates, Optional<Long> axis) {\n@@ -880,1 +880,1 @@\n-    public static <T, Tind> Tensor<T> ScatterElements(Tensor<T> data, Tensor<Tind> indices, Tensor<T> updates, Optional<String> reduction, Optional<Integer> axis) {\n+    public static <T, Tind> Tensor<T> ScatterElements(Tensor<T> data, Tensor<Tind> indices, Tensor<T> updates, Optional<String> reduction, Optional<Long> axis) {\n@@ -905,1 +905,1 @@\n-    public static <S> List<Tensor<S>> SequenceEmpty(Optional<Integer> dtype) {\n+    public static <S> List<Tensor<S>> SequenceEmpty(Optional<Long> dtype) {\n@@ -925,1 +925,1 @@\n-    public static <T, T1> Tensor<Long> Shape(Tensor<T> data, Optional<Integer> start, Optional<Integer> end) {\n+    public static <T, T1> Tensor<Long> Shape(Tensor<T> data, Optional<Long> start, Optional<Long> end) {\n@@ -965,1 +965,1 @@\n-    public static <T> Tensor<T> Softmax(Tensor<T> input, Optional<Integer> axis) {\n+    public static <T> Tensor<T> Softmax(Tensor<T> input, Optional<Long> axis) {\n@@ -971,1 +971,1 @@\n-    public static <T, Tind> SoftmaxCrossEntropyLossResult<T, Tind> SoftmaxCrossEntropyLoss(Tensor<T> scores, Tensor<Tind> labels, Optional<Tensor<T>> weights, Optional<Integer> ignore_index, Optional<String> reduction) {\n+    public static <T, Tind> SoftmaxCrossEntropyLossResult<T, Tind> SoftmaxCrossEntropyLoss(Tensor<T> scores, Tensor<Tind> labels, Optional<Tensor<T>> weights, Optional<Long> ignore_index, Optional<String> reduction) {\n@@ -987,1 +987,1 @@\n-    public static <T> Tensor<T> SpaceToDepth(Tensor<T> input, int blocksize) {\n+    public static <T> Tensor<T> SpaceToDepth(Tensor<T> input, long blocksize) {\n@@ -992,1 +992,1 @@\n-    public static <T> List<Tensor<T>> Split(Tensor<T> input, Optional<Tensor<Long>> split, Optional<Integer> num_outputs, Optional<Integer> axis) {\n+    public static <T> List<Tensor<T>> Split(Tensor<T> input, Optional<Tensor<Long>> split, Optional<Long> num_outputs, Optional<Long> axis) {\n@@ -997,1 +997,1 @@\n-    public static <T, I, S> List<Tensor<S>> SplitToSequence(Tensor<T> input, Optional<Tensor<I>> split, Optional<Integer> keepdims, Optional<Integer> axis) {\n+    public static <T, I, S> List<Tensor<S>> SplitToSequence(Tensor<T> input, Optional<Tensor<I>> split, Optional<Long> keepdims, Optional<Long> axis) {\n@@ -1017,1 +1017,1 @@\n-    public static Tensor<String> StringNormalizer(Tensor<String> X, Optional<Integer> is_case_sensitive, Optional<String> locale, Optional<String[]> stopwords, Optional<String> case_change_action) {\n+    public static Tensor<String> StringNormalizer(Tensor<String> X, Optional<Long> is_case_sensitive, Optional<String> locale, Optional<String[]> stopwords, Optional<String> case_change_action) {\n@@ -1023,1 +1023,1 @@\n-    public static <T1, T2, T3> StringSplitResult<T1, T2, T3> StringSplit(Tensor<String> X, Optional<String> delimiter, Optional<Integer> maxsplit) {\n+    public static <T1, T2, T3> StringSplitResult<T1, T2, T3> StringSplit(Tensor<String> X, Optional<String> delimiter, Optional<Long> maxsplit) {\n@@ -1049,1 +1049,1 @@\n-    public static <T, T1> Tensor<Float> TfIdfVectorizer(Tensor<T> X, int[] ngram_counts, int min_gram_length, Optional<String[]> pool_strings, String mode, int max_gram_length, int max_skip_count, Optional<int[]> pool_int64s, Optional<float[]> weights, int[] ngram_indexes) {\n+    public static <T, T1> Tensor<Float> TfIdfVectorizer(Tensor<T> X, long[] ngram_counts, long min_gram_length, Optional<String[]> pool_strings, String mode, long max_gram_length, long max_skip_count, Optional<long[]> pool_int64s, Optional<float[]> weights, long[] ngram_indexes) {\n@@ -1065,1 +1065,1 @@\n-    public static <T, I> TopKResult<T, I> TopK(Tensor<T> X, Tensor<Long> K, Optional<Integer> largest, Optional<Integer> sorted, Optional<Integer> axis) {\n+    public static <T, I> TopKResult<T, I> TopK(Tensor<T> X, Tensor<Long> K, Optional<Long> largest, Optional<Long> sorted, Optional<Long> axis) {\n@@ -1071,1 +1071,1 @@\n-    public static <T> Tensor<T> Transpose(Tensor<T> data, Optional<int[]> perm) {\n+    public static <T> Tensor<T> Transpose(Tensor<T> data, Optional<long[]> perm) {\n@@ -1076,1 +1076,1 @@\n-    public static <T> Tensor<T> TreeEnsemble(Tensor<T> X, Optional<Integer> aggregate_function, Optional<byte[]> nodes_hitrates, int[] nodes_featureids, int[] nodes_falseleafs, Optional<Integer> post_transform, int[] nodes_trueleafs, byte[] nodes_modes, int[] nodes_falsenodeids, int[] nodes_truenodeids, byte[] leaf_weights, int[] leaf_targetids, int[] tree_roots, Optional<Integer> n_targets, Optional<int[]> nodes_missing_value_tracks_true, Optional<byte[]> membership_values, byte[] nodes_splits) {\n+    public static <T> Tensor<T> TreeEnsemble(Tensor<T> X, Optional<Long> aggregate_function, Optional<byte[]> nodes_hitrates, long[] nodes_featureids, long[] nodes_falseleafs, Optional<Long> post_transform, long[] nodes_trueleafs, byte[] nodes_modes, long[] nodes_falsenodeids, long[] nodes_truenodeids, byte[] leaf_weights, long[] leaf_targetids, long[] tree_roots, Optional<Long> n_targets, Optional<long[]> nodes_missing_value_tracks_true, Optional<byte[]> membership_values, byte[] nodes_splits) {\n@@ -1082,1 +1082,1 @@\n-    public static <T1, T2> TreeEnsembleClassifierResult<T1, T2> TreeEnsembleClassifier(Tensor<T1> X, Optional<int[]> classlabels_int64s, Optional<int[]> class_ids, Optional<float[]> nodes_hitrates, Optional<int[]> nodes_featureids, Optional<int[]> nodes_treeids, Optional<byte[]> class_weights_as_tensor, Optional<String> post_transform, Optional<String[]> nodes_modes, Optional<int[]> nodes_falsenodeids, Optional<String[]> classlabels_strings, Optional<int[]> nodes_truenodeids, Optional<int[]> nodes_nodeids, Optional<byte[]> nodes_hitrates_as_tensor, Optional<float[]> class_weights, Optional<byte[]> base_values_as_tensor, Optional<int[]> nodes_missing_value_tracks_true, Optional<int[]> class_nodeids, Optional<int[]> class_treeids, Optional<float[]> base_values, Optional<float[]> nodes_values, Optional<byte[]> nodes_values_as_tensor) {\n+    public static <T1, T2> TreeEnsembleClassifierResult<T1, T2> TreeEnsembleClassifier(Tensor<T1> X, Optional<long[]> classlabels_int64s, Optional<long[]> class_ids, Optional<float[]> nodes_hitrates, Optional<long[]> nodes_featureids, Optional<long[]> nodes_treeids, Optional<byte[]> class_weights_as_tensor, Optional<String> post_transform, Optional<String[]> nodes_modes, Optional<long[]> nodes_falsenodeids, Optional<String[]> classlabels_strings, Optional<long[]> nodes_truenodeids, Optional<long[]> nodes_nodeids, Optional<byte[]> nodes_hitrates_as_tensor, Optional<float[]> class_weights, Optional<byte[]> base_values_as_tensor, Optional<long[]> nodes_missing_value_tracks_true, Optional<long[]> class_nodeids, Optional<long[]> class_treeids, Optional<float[]> base_values, Optional<float[]> nodes_values, Optional<byte[]> nodes_values_as_tensor) {\n@@ -1088,1 +1088,1 @@\n-    public static <T> Tensor<Float> TreeEnsembleRegressor(Tensor<T> X, Optional<String> aggregate_function, Optional<float[]> nodes_hitrates, Optional<byte[]> target_weights_as_tensor, Optional<int[]> nodes_featureids, Optional<int[]> target_treeids, Optional<int[]> nodes_treeids, Optional<String> post_transform, Optional<String[]> nodes_modes, Optional<float[]> target_weights, Optional<int[]> nodes_falsenodeids, Optional<int[]> target_ids, Optional<int[]> nodes_truenodeids, Optional<int[]> target_nodeids, Optional<int[]> nodes_nodeids, Optional<byte[]> nodes_hitrates_as_tensor, Optional<byte[]> base_values_as_tensor, Optional<Integer> n_targets, Optional<int[]> nodes_missing_value_tracks_true, Optional<float[]> base_values, Optional<float[]> nodes_values, Optional<byte[]> nodes_values_as_tensor) {\n+    public static <T> Tensor<Float> TreeEnsembleRegressor(Tensor<T> X, Optional<String> aggregate_function, Optional<float[]> nodes_hitrates, Optional<byte[]> target_weights_as_tensor, Optional<long[]> nodes_featureids, Optional<long[]> target_treeids, Optional<long[]> nodes_treeids, Optional<String> post_transform, Optional<String[]> nodes_modes, Optional<float[]> target_weights, Optional<long[]> nodes_falsenodeids, Optional<long[]> target_ids, Optional<long[]> nodes_truenodeids, Optional<long[]> target_nodeids, Optional<long[]> nodes_nodeids, Optional<byte[]> nodes_hitrates_as_tensor, Optional<byte[]> base_values_as_tensor, Optional<Long> n_targets, Optional<long[]> nodes_missing_value_tracks_true, Optional<float[]> base_values, Optional<float[]> nodes_values, Optional<byte[]> nodes_values_as_tensor) {\n@@ -1093,1 +1093,1 @@\n-    public static <T> Tensor<T> Trilu(Tensor<T> input, Optional<Tensor<Long>> k, Optional<Integer> upper) {\n+    public static <T> Tensor<T> Trilu(Tensor<T> input, Optional<Tensor<Long>> k, Optional<Long> upper) {\n@@ -1099,1 +1099,1 @@\n-    public static <T> UniqueResult<T> Unique(Tensor<T> X, Optional<Integer> sorted, Optional<Integer> axis) {\n+    public static <T> UniqueResult<T> Unique(Tensor<T> X, Optional<Long> sorted, Optional<Long> axis) {\n@@ -1125,1 +1125,1 @@\n-    public static <T> List<Map<T, Float>> ZipMap(Tensor<Float> X, Optional<int[]> classlabels_int64s, Optional<String[]> classlabels_strings) {\n+    public static <T> List<Map<T, Float>> ZipMap(Tensor<Float> X, Optional<long[]> classlabels_int64s, Optional<String[]> classlabels_strings) {\n","filename":"cr-examples\/onnx\/src\/main\/java\/oracle\/code\/onnx\/OnnxOperators.java","additions":107,"deletions":107,"binary":false,"changes":214,"status":"modified"},{"patch":"@@ -2,1 +2,1 @@\n- * Copyright (c) 2024, Oracle and\/or its affiliates. All rights reserved.\n+ * Copyright (c) 2025, Oracle and\/or its affiliates. All rights reserved.\n@@ -26,1 +26,1 @@\n-package jdk.incubator.code.interpreter;\n+package oracle.code.onnx.compiler;\n@@ -395,1 +395,1 @@\n-   \/\/ long\n+    \/\/ long\n@@ -486,1 +486,0 @@\n-\n@@ -538,1 +537,0 @@\n-\n@@ -619,0 +617,1 @@\n+\n@@ -622,0 +621,1 @@\n+\n@@ -625,0 +625,1 @@\n+\n@@ -628,0 +629,1 @@\n+\n@@ -631,0 +633,1 @@\n+\n@@ -634,0 +637,1 @@\n+\n@@ -637,0 +641,1 @@\n+\n@@ -638,1 +643,1 @@\n-        return ((int)i & 1) == 1;\n+        return ((int) i & 1) == 1;\n@@ -645,0 +650,1 @@\n+\n@@ -648,0 +654,1 @@\n+\n@@ -651,0 +658,1 @@\n+\n@@ -654,0 +662,1 @@\n+\n@@ -657,0 +666,1 @@\n+\n@@ -660,0 +670,1 @@\n+\n@@ -663,0 +674,1 @@\n+\n@@ -664,1 +676,1 @@\n-        return ((int)i & 1) == 1;\n+        return ((int) i & 1) == 1;\n@@ -671,0 +683,1 @@\n+\n@@ -674,0 +687,1 @@\n+\n@@ -677,0 +691,1 @@\n+\n@@ -680,0 +695,1 @@\n+\n@@ -683,0 +699,1 @@\n+\n@@ -686,0 +703,1 @@\n+\n@@ -689,0 +707,1 @@\n+\n@@ -697,0 +716,1 @@\n+\n@@ -700,0 +720,1 @@\n+\n@@ -703,0 +724,1 @@\n+\n@@ -706,0 +728,1 @@\n+\n@@ -709,0 +732,1 @@\n+\n@@ -712,0 +736,1 @@\n+\n@@ -715,0 +740,1 @@\n+\n@@ -723,0 +749,1 @@\n+\n@@ -726,0 +753,1 @@\n+\n@@ -729,0 +757,1 @@\n+\n@@ -732,0 +761,1 @@\n+\n@@ -735,0 +765,1 @@\n+\n@@ -738,0 +769,1 @@\n+\n@@ -741,0 +773,1 @@\n+\n@@ -749,0 +782,1 @@\n+\n@@ -752,0 +786,1 @@\n+\n@@ -755,0 +790,1 @@\n+\n@@ -758,0 +794,1 @@\n+\n@@ -761,0 +798,1 @@\n+\n@@ -764,0 +802,1 @@\n+\n@@ -767,0 +806,1 @@\n+\n@@ -775,0 +815,1 @@\n+\n@@ -778,0 +819,1 @@\n+\n@@ -781,0 +823,1 @@\n+\n@@ -784,0 +827,1 @@\n+\n@@ -787,0 +831,1 @@\n+\n@@ -790,0 +835,1 @@\n+\n@@ -793,0 +839,1 @@\n+\n@@ -801,0 +848,1 @@\n+\n@@ -804,0 +852,1 @@\n+\n@@ -807,0 +856,1 @@\n+\n@@ -810,0 +860,1 @@\n+\n@@ -811,1 +862,1 @@\n-        return i ? (short)1 : 0;\n+        return i ? (short) 1 : 0;\n@@ -813,0 +864,1 @@\n+\n@@ -814,1 +866,1 @@\n-        return i ? (char)1 : 0;\n+        return i ? (char) 1 : 0;\n@@ -816,0 +868,1 @@\n+\n@@ -817,1 +870,1 @@\n-        return i ? (byte)1 : 0;\n+        return i ? (byte) 1 : 0;\n@@ -819,0 +872,1 @@\n+\n","filename":"cr-examples\/onnx\/src\/main\/java\/oracle\/code\/onnx\/compiler\/InvokableLeafOps.java","additions":64,"deletions":10,"binary":false,"changes":74,"previous_filename":"src\/jdk.incubator.code\/share\/classes\/jdk\/incubator\/code\/interpreter\/InvokableLeafOps.java","status":"copied"},{"patch":"@@ -0,0 +1,565 @@\n+\/*\n+ * Copyright (c) 2025, Oracle and\/or its affiliates. All rights reserved.\n+ * DO NOT ALTER OR REMOVE COPYRIGHT NOTICES OR THIS FILE HEADER.\n+ *\n+ * This code is free software; you can redistribute it and\/or modify it\n+ * under the terms of the GNU General Public License version 2 only, as\n+ * published by the Free Software Foundation.  Oracle designates this\n+ * particular file as subject to the \"Classpath\" exception as provided\n+ * by Oracle in the LICENSE file that accompanied this code.\n+ *\n+ * This code is distributed in the hope that it will be useful, but WITHOUT\n+ * ANY WARRANTY; without even the implied warranty of MERCHANTABILITY or\n+ * FITNESS FOR A PARTICULAR PURPOSE.  See the GNU General Public License\n+ * version 2 for more details (a copy is included in the LICENSE file that\n+ * accompanied this code).\n+ *\n+ * You should have received a copy of the GNU General Public License version\n+ * 2 along with this work; if not, write to the Free Software Foundation,\n+ * Inc., 51 Franklin St, Fifth Floor, Boston, MA 02110-1301 USA.\n+ *\n+ * Please contact Oracle, 500 Oracle Parkway, Redwood Shores, CA 94065 USA\n+ * or visit www.oracle.com if you need additional information or have any\n+ * questions.\n+ *\/\n+\n+package oracle.code.onnx.compiler;\n+\n+import jdk.incubator.code.*;\n+import jdk.incubator.code.op.CoreOp;\n+import jdk.incubator.code.type.*;\n+import oracle.code.onnx.OnnxOperators;\n+import oracle.code.onnx.ir.OnnxOp;\n+import oracle.code.onnx.ir.OnnxOps;\n+\n+import java.lang.invoke.*;\n+import java.lang.reflect.Array;\n+import java.util.*;\n+import java.util.stream.Collectors;\n+import java.util.stream.Stream;\n+\n+final class OnnxPartialEvaluator {\n+\n+    static final JavaType ONNX_OPERATORS_CLASS = JavaType.type(OnnxOperators.class);\n+\n+    \/\/ Map from ONNX operator invocation to evaluated attributes\n+    Map<CoreOp.InvokeOp, List<Object>> evaluatedAttributes;\n+\n+    \/\/ Operations that depend directly or indirectly on input parameters\n+    \/\/ The operations' results are not evaluated\n+    Set<Op> unevaluatedOperations;\n+\n+    public OnnxPartialEvaluator() {\n+        this.evaluatedAttributes = new HashMap<>();\n+        this.unevaluatedOperations = new HashSet<>();\n+    }\n+\n+    public <T extends Op & Op.Invokable>\n+    void evaluate(MethodHandles.Lookup l, T op) {\n+        Map<Value, Object> evaluatedValues = new HashMap<>();\n+        interpretEntryBlock(l, op.body().entryBlock(), new OpContext(), evaluatedValues);\n+\n+        evaluatedAttributes.forEach((invokeOp, objects) -> {\n+            System.out.println(invokeOp.invokeDescriptor().name() + \" -> \" + objects);\n+        });\n+    }\n+\n+\n+    @SuppressWarnings(\"serial\")\n+    public static final class InterpreterException extends RuntimeException {\n+        private InterpreterException(Throwable cause) {\n+            super(cause);\n+        }\n+    }\n+\n+    static InterpreterException interpreterException(Throwable cause) {\n+        return new InterpreterException(cause);\n+    }\n+\n+    record BlockContext(Block b, Map<Value, Object> evaluatedValues) {\n+    }\n+\n+    static final class OpContext {\n+        final Deque<BlockContext> stack = new ArrayDeque<>();\n+\n+        boolean isValueDefined(Value v) {\n+            \/\/ @@@ Only dominating values are accessible\n+            BlockContext bc = findContext(v);\n+            return bc != null;\n+        }\n+\n+        Object getValue(Value v) {\n+            \/\/ @@@ Only dominating values are accessible\n+            BlockContext bc = findContext(v);\n+            if (bc != null) {\n+                return bc.evaluatedValues.get(v);\n+            } else {\n+                throw interpreterException(new IllegalArgumentException(\"Undefined value: \" + v));\n+            }\n+        }\n+\n+        Object setValue(Value v, Object o) {\n+            BlockContext bc = findContext(v);\n+            if (bc != null) {\n+                throw interpreterException(new IllegalArgumentException(\"Value already defined: \" + v));\n+            }\n+            stack.peek().evaluatedValues.put(v, o);\n+            return o;\n+        }\n+\n+        BlockContext findContext(Value v) {\n+            Optional<BlockContext> ob = stack.stream().filter(b -> b.evaluatedValues.containsKey(v))\n+                    .findFirst();\n+            return ob.orElse(null);\n+        }\n+\n+        boolean contains(Block.Reference s) {\n+            Block sb = s.targetBlock();\n+            return stack.stream().anyMatch(bc -> bc.b.equals(sb));\n+        }\n+\n+        void successor(Block.Reference sb) {\n+            List<Object> sbValues = sb.arguments().stream().map(this::getValue).toList();\n+\n+            Block b = sb.targetBlock();\n+            Map<Value, Object> bValues = new HashMap<>();\n+            for (int i = 0; i < sbValues.size(); i++) {\n+                bValues.put(b.parameters().get(i), sbValues.get(i));\n+            }\n+\n+            if (contains(sb)) {\n+                \/\/ if block is already dominating pop back up from the back branch to the block\n+                \/\/ before the successor block\n+                while (!stack.peek().b.equals(sb.targetBlock())) {\n+                    stack.pop();\n+                }\n+                stack.pop();\n+            }\n+            stack.push(new BlockContext(b, bValues));\n+        }\n+\n+        void popTo(BlockContext bc) {\n+            while (!stack.peek().equals(bc)) {\n+                stack.pop();\n+            }\n+        }\n+    }\n+\n+    static final class VarBox\n+            implements CoreOp.Var<Object> {\n+        Object value;\n+\n+        public Object value() {\n+            return value;\n+        }\n+\n+        VarBox(Object value) {\n+            this.value = value;\n+        }\n+\n+        static final Object UINITIALIZED = new Object();\n+    }\n+\n+    record TupleRecord(List<Object> components) {\n+        Object getComponent(int index) {\n+            return components.get(index);\n+        }\n+\n+        TupleRecord with(int index, Object value) {\n+            List<Object> copy = new ArrayList<>(components);\n+            copy.set(index, value);\n+            return new TupleRecord(copy);\n+        }\n+    }\n+\n+    void interpretBody(MethodHandles.Lookup l, Body body,\n+                       OpContext oc,\n+                       List<Object> args) {\n+        List<Block.Parameter> parameters = body.entryBlock().parameters();\n+        if (parameters.size() != args.size()) {\n+            throw interpreterException(new IllegalArgumentException(\n+                    \"Incorrect number of arguments arguments\"));\n+        }\n+\n+        \/\/ Map symbolic parameters to runtime arguments\n+        Map<Value, Object> arguments = new HashMap<>();\n+        for (int i = 0; i < parameters.size(); i++) {\n+            arguments.put(parameters.get(i), args.get(i));\n+        }\n+\n+        interpretEntryBlock(l, body.entryBlock(), oc, arguments);\n+    }\n+\n+    void interpretEntryBlock(MethodHandles.Lookup l, Block entry,\n+                             OpContext oc,\n+                             Map<Value, Object> evaluatedValues) {\n+        assert entry.isEntryBlock();\n+\n+        \/\/ If the stack is not empty it means we are interpreting\n+        \/\/ an entry block with a parent body whose nearest ancestor body\n+        \/\/ is the current context block's parent body\n+        BlockContext yieldContext = oc.stack.peek();\n+        assert yieldContext == null ||\n+                yieldContext.b().parentBody() == entry.parentBody().parentOp().ancestorBody();\n+\n+        \/\/ Note that first block cannot have any successors so the queue will have at least one entry\n+        oc.stack.push(new BlockContext(entry, evaluatedValues));\n+        while (true) {\n+            BlockContext bc = oc.stack.peek();\n+\n+            \/\/ Execute all but the terminating operation\n+            int nops = bc.b.ops().size();\n+            try {\n+                for (int i = 0; i < nops - 1; i++) {\n+                    Op op = bc.b.ops().get(i);\n+                    assert !(op instanceof Op.Terminating) : op.opName();\n+\n+                    Object result = interpretOp(l, oc, op);\n+                    if (result != null) {\n+                        oc.setValue(op.result(), result);\n+                    }\n+                }\n+            } catch (InterpreterException e) {\n+                throw e;\n+            }\n+\n+            \/\/ Execute the terminating operation\n+            Op to = bc.b.terminatingOp();\n+            if (!to.operands().stream().allMatch(oc::isValueDefined)) {\n+                \/\/ Ignore operation if any value is undefined, meaning it is not part of the attribute value space\n+                unevaluatedOperations.add(to);\n+            }\n+\n+            if (to instanceof CoreOp.ConditionalBranchOp cb) {\n+                boolean p;\n+                Object bop = oc.getValue(cb.predicate());\n+                if (bop instanceof Boolean bp) {\n+                    p = bp;\n+                } else if (bop instanceof Integer ip) {\n+                    \/\/ @@@ This is required when lifting up from bytecode, since boolean values\n+                    \/\/ are erased to int values, abd the bytecode lifting implementation is not currently\n+                    \/\/ sophisticated enough to recover the type information\n+                    p = ip != 0;\n+                } else {\n+                    throw interpreterException(\n+                            new UnsupportedOperationException(\"Unsupported type input to operation: \" + cb));\n+                }\n+                Block.Reference sb = p ? cb.trueBranch() : cb.falseBranch();\n+                oc.successor(sb);\n+            } else if (to instanceof CoreOp.BranchOp b) {\n+                Block.Reference sb = b.branch();\n+\n+                oc.successor(sb);\n+            } else if (to instanceof CoreOp.ReturnOp ret) {\n+                \/\/ @@@ value should not be in scope\n+                \/\/ return rv == null ? null : oc.getValue(rv);\n+                return;\n+            } else {\n+                throw interpreterException(\n+                        new UnsupportedOperationException(\"Unsupported terminating operation: \" + to.opName()));\n+            }\n+        }\n+    }\n+\n+\n+    @SuppressWarnings(\"unchecked\")\n+    public static <E extends Throwable> void eraseAndThrow(Throwable e) throws E {\n+        throw (E) e;\n+    }\n+\n+    @SuppressWarnings({\"rawtypes\", \"unchecked\"})\n+    static Class<? extends OnnxOp> onnxOpClassFromName(String operatorName) {\n+        Class<? extends OnnxOp> opClass;\n+        try {\n+            return (Class) Class.forName(OnnxOps.class.getName() + \"$\" + operatorName);\n+        } catch (ClassNotFoundException e) {\n+            throw new InternalError(e);\n+        }\n+    }\n+\n+    static OnnxOp.OnnxSchema schemaFromOnnxOpClass(Class<? extends OnnxOp> opClass) {\n+        try {\n+            return (OnnxOp.OnnxSchema) opClass.getField(\"SCHEMA\").get(null);\n+        } catch (ReflectiveOperationException e) {\n+            throw new InternalError(e);\n+        }\n+    }\n+\n+    Object interpretOp(MethodHandles.Lookup l, OpContext oc, Op o) {\n+        \/\/ Invocation to ONNX operator\n+        \/\/ The input operands will be left unevaluated\n+        \/\/ The attribute operands will be evaluated\n+        \/\/ @@@ Clone attributes or disallow subsequent operation\n+        if (o instanceof CoreOp.InvokeOp io && io.invokeDescriptor().refType().equals(ONNX_OPERATORS_CLASS)) {\n+            String operatorName = io.invokeDescriptor().name();\n+\n+            Class<? extends OnnxOp> opClass = onnxOpClassFromName(operatorName);\n+            OnnxOp.OnnxSchema schema = schemaFromOnnxOpClass(opClass);\n+\n+            List<OnnxOp.OnnxParameter> inputs = schema.inputs();\n+            assert o.operands().subList(0, inputs.size()).stream().noneMatch(oc::isValueDefined);\n+            List<OnnxOp.OnnxAttribute> attributes = schema.attributes();\n+\n+            if (opClass == OnnxOps.Constant.class && o.operands().size() == 1) {\n+                \/\/ Specialized one argument invocations\n+                List<Object> attrs = new ArrayList<>();\n+                for (OnnxOp.OnnxAttribute attribute : attributes) {\n+                    if (JavaType.type(attribute.type()).equals(o.operands().getFirst().type())) {\n+                        attrs.add(Optional.of(oc.getValue(o.operands().getFirst())));\n+                    } else {\n+                        attrs.add(Optional.empty());\n+                    }\n+                }\n+                evaluatedAttributes.put(io, attrs);\n+            } else {\n+                for (int i = 0; i < attributes.size(); i++) {\n+                    assert oc.isValueDefined(o.operands().get(inputs.size() + i)) : operatorName;\n+                }\n+                List<Object> attrs = o.operands().subList(inputs.size(), inputs.size() + attributes.size()).stream()\n+                        .map(oc::getValue)\n+                        .toList();\n+                evaluatedAttributes.put(io, attrs);\n+            }\n+\n+            unevaluatedOperations.add(o);\n+            return null;\n+        } else if (!o.operands().stream().allMatch(oc::isValueDefined)) {\n+            \/\/ Ignore operation if any value is undefined, meaning it is not part of the attribute value space\n+            unevaluatedOperations.add(o);\n+            return null;\n+        }\n+\n+        switch (o) {\n+            case CoreOp.ConstantOp co -> {\n+                if (co.resultType().equals(JavaType.J_L_CLASS)) {\n+                    return resolveToClass(l, (JavaType) co.value());\n+                } else {\n+                    return co.value();\n+                }\n+            }\n+            case CoreOp.InvokeOp co -> {\n+                MethodType target = resolveToMethodType(l, o.opType());\n+                MethodHandles.Lookup il = switch (co.invokeKind()) {\n+                    case STATIC, INSTANCE -> l;\n+                    case SUPER -> l.in(target.parameterType(0));\n+                };\n+                MethodHandle mh = resolveToMethodHandle(il, co.invokeDescriptor(), co.invokeKind());\n+\n+                mh = mh.asType(target).asFixedArity();\n+                Object[] values = o.operands().stream().map(oc::getValue).toArray();\n+                return invoke(mh, values);\n+            }\n+            case CoreOp.NewOp no -> {\n+                Object[] values = o.operands().stream().map(oc::getValue).toArray();\n+                JavaType nType = (JavaType) no.constructorType().returnType();\n+                if (nType instanceof ArrayType at) {\n+                    if (values.length > at.dimensions()) {\n+                        throw interpreterException(new IllegalArgumentException(\"Bad constructor NewOp: \" + no));\n+                    }\n+                    int[] lengths = Stream.of(values).mapToInt(v -> (int) v).toArray();\n+                    for (int length : lengths) {\n+                        nType = ((ArrayType) nType).componentType();\n+                    }\n+                    return Array.newInstance(resolveToClass(l, nType), lengths);\n+                } else {\n+                    MethodHandle mh = constructorHandle(l, no.constructorType());\n+                    return invoke(mh, values);\n+                }\n+            }\n+            case CoreOp.VarOp vo -> {\n+                Object v = vo.isUninitialized()\n+                        ? VarBox.UINITIALIZED\n+                        : oc.getValue(o.operands().get(0));\n+                return new VarBox(v);\n+            }\n+            case CoreOp.VarAccessOp.VarLoadOp vlo -> {\n+                \/\/ Cast to CoreOp.Var, since the instance may have originated as an external instance\n+                \/\/ via a captured value map\n+                CoreOp.Var<?> vb = (CoreOp.Var<?>) oc.getValue(o.operands().get(0));\n+                Object value = vb.value();\n+                if (value == VarBox.UINITIALIZED) {\n+                    throw interpreterException(new IllegalStateException(\"Loading from uninitialized variable\"));\n+                }\n+                return value;\n+            }\n+            case CoreOp.VarAccessOp.VarStoreOp vso -> {\n+                VarBox vb = (VarBox) oc.getValue(o.operands().get(0));\n+                vb.value = oc.getValue(o.operands().get(1));\n+                return null;\n+            }\n+            case CoreOp.TupleOp to -> {\n+                List<Object> values = o.operands().stream().map(oc::getValue).toList();\n+                return new TupleRecord(values);\n+            }\n+            case CoreOp.TupleLoadOp tlo -> {\n+                TupleRecord tb = (TupleRecord) oc.getValue(o.operands().get(0));\n+                return tb.getComponent(tlo.index());\n+            }\n+            case CoreOp.TupleWithOp two -> {\n+                TupleRecord tb = (TupleRecord) oc.getValue(o.operands().get(0));\n+                return tb.with(two.index(), oc.getValue(o.operands().get(1)));\n+            }\n+            case CoreOp.FieldAccessOp.FieldLoadOp fo -> {\n+                if (fo.operands().isEmpty()) {\n+                    VarHandle vh = fieldStaticHandle(l, fo.fieldDescriptor());\n+                    return vh.get();\n+                } else {\n+                    Object v = oc.getValue(o.operands().get(0));\n+                    VarHandle vh = fieldHandle(l, fo.fieldDescriptor());\n+                    return vh.get(v);\n+                }\n+            }\n+            case CoreOp.FieldAccessOp.FieldStoreOp fo -> {\n+                if (fo.operands().size() == 1) {\n+                    Object v = oc.getValue(o.operands().get(0));\n+                    VarHandle vh = fieldStaticHandle(l, fo.fieldDescriptor());\n+                    vh.set(v);\n+                } else {\n+                    Object r = oc.getValue(o.operands().get(0));\n+                    Object v = oc.getValue(o.operands().get(1));\n+                    VarHandle vh = fieldHandle(l, fo.fieldDescriptor());\n+                    vh.set(r, v);\n+                }\n+                return null;\n+            }\n+            case CoreOp.InstanceOfOp io -> {\n+                Object v = oc.getValue(o.operands().get(0));\n+                return isInstance(l, io.type(), v);\n+            }\n+            case CoreOp.CastOp co -> {\n+                Object v = oc.getValue(o.operands().get(0));\n+                return cast(l, co.type(), v);\n+            }\n+            case CoreOp.ArrayLengthOp arrayLengthOp -> {\n+                Object a = oc.getValue(o.operands().get(0));\n+                return Array.getLength(a);\n+            }\n+            case CoreOp.ArrayAccessOp.ArrayLoadOp arrayLoadOp -> {\n+                Object a = oc.getValue(o.operands().get(0));\n+                Object index = oc.getValue(o.operands().get(1));\n+                return Array.get(a, (int) index);\n+            }\n+            case CoreOp.ArrayAccessOp.ArrayStoreOp arrayStoreOp -> {\n+                Object a = oc.getValue(o.operands().get(0));\n+                Object index = oc.getValue(o.operands().get(1));\n+                Object v = oc.getValue(o.operands().get(2));\n+                Array.set(a, (int) index, v);\n+                return null;\n+            }\n+            case CoreOp.ArithmeticOperation arithmeticOperation -> {\n+                MethodHandle mh = opHandle(l, o.opName(), o.opType());\n+                Object[] values = o.operands().stream().map(oc::getValue).toArray();\n+                return invoke(mh, values);\n+            }\n+            case CoreOp.TestOperation testOperation -> {\n+                MethodHandle mh = opHandle(l, o.opName(), o.opType());\n+                Object[] values = o.operands().stream().map(oc::getValue).toArray();\n+                return invoke(mh, values);\n+            }\n+            case CoreOp.ConvOp convOp -> {\n+                MethodHandle mh = opHandle(l, o.opName() + \"_\" + o.opType().returnType(), o.opType());\n+                Object[] values = o.operands().stream().map(oc::getValue).toArray();\n+                return invoke(mh, values);\n+            }\n+            case CoreOp.ConcatOp concatOp -> {\n+                return o.operands().stream()\n+                        .map(oc::getValue)\n+                        .map(String::valueOf)\n+                        .collect(Collectors.joining());\n+            }\n+            case null, default -> throw interpreterException(\n+                    new UnsupportedOperationException(\"Unsupported operation: \" + o.opName()));\n+        }\n+    }\n+\n+    static MethodHandle opHandle(MethodHandles.Lookup l, String opName, FunctionType ft) {\n+        MethodType mt = resolveToMethodType(l, ft).erase();\n+        try {\n+            return MethodHandles.lookup().findStatic(InvokableLeafOps.class, opName, mt);\n+        } catch (NoSuchMethodException | IllegalAccessException e) {\n+            throw interpreterException(e);\n+        }\n+    }\n+\n+    static MethodHandle constructorHandle(MethodHandles.Lookup l, FunctionType ft) {\n+        MethodType mt = resolveToMethodType(l, ft);\n+\n+        if (mt.returnType().isArray()) {\n+            if (mt.parameterCount() != 1 || mt.parameterType(0) != int.class) {\n+                throw interpreterException(new IllegalArgumentException(\"Bad constructor descriptor: \" + ft));\n+            }\n+            return MethodHandles.arrayConstructor(mt.returnType());\n+        } else {\n+            try {\n+                return l.findConstructor(mt.returnType(), mt.changeReturnType(void.class));\n+            } catch (NoSuchMethodException | IllegalAccessException e) {\n+                throw interpreterException(e);\n+            }\n+        }\n+    }\n+\n+    static VarHandle fieldStaticHandle(MethodHandles.Lookup l, FieldRef d) {\n+        return resolveToVarHandle(l, d);\n+    }\n+\n+    static VarHandle fieldHandle(MethodHandles.Lookup l, FieldRef d) {\n+        return resolveToVarHandle(l, d);\n+    }\n+\n+    static Object isInstance(MethodHandles.Lookup l, TypeElement d, Object v) {\n+        Class<?> c = resolveToClass(l, d);\n+        return c.isInstance(v);\n+    }\n+\n+    static Object cast(MethodHandles.Lookup l, TypeElement d, Object v) {\n+        Class<?> c = resolveToClass(l, d);\n+        return c.cast(v);\n+    }\n+\n+    static MethodHandle resolveToMethodHandle(MethodHandles.Lookup l, MethodRef d, CoreOp.InvokeOp.InvokeKind kind) {\n+        try {\n+            return d.resolveToHandle(l, kind);\n+        } catch (ReflectiveOperationException e) {\n+            throw interpreterException(e);\n+        }\n+    }\n+\n+    static VarHandle resolveToVarHandle(MethodHandles.Lookup l, FieldRef d) {\n+        try {\n+            return d.resolveToHandle(l);\n+        } catch (ReflectiveOperationException e) {\n+            throw interpreterException(e);\n+        }\n+    }\n+\n+    public static MethodType resolveToMethodType(MethodHandles.Lookup l, FunctionType ft) {\n+        try {\n+            return MethodRef.toNominalDescriptor(ft).resolveConstantDesc(l);\n+        } catch (ReflectiveOperationException e) {\n+            throw interpreterException(e);\n+        }\n+    }\n+\n+    public static Class<?> resolveToClass(MethodHandles.Lookup l, TypeElement d) {\n+        try {\n+            if (d instanceof JavaType jt) {\n+                return (Class<?>) jt.erasure().resolve(l);\n+            } else {\n+                throw new ReflectiveOperationException();\n+            }\n+        } catch (ReflectiveOperationException e) {\n+            throw interpreterException(e);\n+        }\n+    }\n+\n+    static Object invoke(MethodHandle m, Object... args) {\n+        try {\n+            return m.invokeWithArguments(args);\n+        } catch (RuntimeException | Error e) {\n+            throw e;\n+        } catch (Throwable e) {\n+            eraseAndThrow(e);\n+            throw new InternalError(\"should not reach here\");\n+        }\n+    }\n+}\n","filename":"cr-examples\/onnx\/src\/main\/java\/oracle\/code\/onnx\/compiler\/OnnxPartialEvaluator.java","additions":565,"deletions":0,"binary":false,"changes":565,"status":"added"},{"patch":"@@ -0,0 +1,229 @@\n+package oracle.code.onnx.compiler;\n+\n+import jdk.incubator.code.Op;\n+import jdk.incubator.code.TypeElement;\n+import jdk.incubator.code.Value;\n+import jdk.incubator.code.analysis.SSA;\n+import jdk.incubator.code.op.CoreOp;\n+import jdk.incubator.code.type.*;\n+import oracle.code.onnx.OnnxOperators;\n+import oracle.code.onnx.Tensor;\n+import oracle.code.onnx.ir.OnnxOp;\n+import oracle.code.onnx.ir.OnnxOps;\n+import oracle.code.onnx.ir.OnnxType;\n+\n+import java.lang.invoke.MethodHandles;\n+import java.lang.reflect.*;\n+import java.util.*;\n+import java.util.stream.Collectors;\n+import java.util.stream.Stream;\n+\n+\/\/ Transform the Java code model of an ONNX function to an ONNX code model\n+public class OnnxTransformer {\n+\n+    static final JavaType ONNX_OPERATORS_CLASS = JavaType.type(OnnxOperators.class);\n+\n+    private OnnxTransformer() {\n+    }\n+\n+    public static CoreOp.FuncOp transform(MethodHandles.Lookup l, CoreOp.FuncOp in) {\n+        OnnxPartialEvaluator pe = new OnnxPartialEvaluator();\n+        pe.evaluate(l, in);\n+\n+        FunctionType ft = FunctionType.functionType(\n+                type(in.invokableType().returnType()),\n+                in.invokableType().parameterTypes().stream().map(OnnxTransformer::type).toList()\n+        );\n+\n+        CoreOp.FuncOp onnxModel = CoreOp.func(in.funcName(), ft).body(b -> {\n+            b.transformBody(in.body(), b.parameters(), (bb, op) -> {\n+                if (!pe.unevaluatedOperations.contains(op)) {\n+                    return bb;\n+                }\n+                switch (op) {\n+                    \/\/ Transform invocation to ONNX operator to operation modeling the operator\n+                    case CoreOp.InvokeOp io when io.invokeDescriptor().refType().equals(ONNX_OPERATORS_CLASS) -> {\n+                        String operatorName = io.invokeDescriptor().name();\n+                        Class<? extends OnnxOp> opClass = onnxOpClassFromName(operatorName);\n+                        OnnxOp.OnnxSchema schema = schemaFromOnnxOpClass(opClass);\n+\n+                        List<Object> attributes = pe.evaluatedAttributes.get(io);\n+\n+                        Method opMethod = Stream.of(OnnxOps.class.getMethods())\n+                                .filter(m -> m.getName().equals(operatorName))\n+                                .findFirst().orElseThrow();\n+\n+                        List<Object> opArgs = new ArrayList<>();\n+\n+                        \/\/ @@@ Operator API currently requires all optional output parameters are required\n+                        if (schema.outputs().stream().anyMatch(p -> p.quantifier().isOptional())) {\n+                            opArgs.add(recordTypeToTupleType(l, (ClassType) op.resultType()));\n+                            Set<? extends OnnxOp.OnnxParameter> optionalOutputs = schema.outputs().stream()\n+                                    .filter(p -> p.quantifier().isOptional())\n+                                    .collect(Collectors.toSet());\n+                            opArgs.add(optionalOutputs);\n+                        } else {\n+                            opArgs.add(type(op.resultType()));\n+                        }\n+\n+                        for (int i = 0; i < schema.inputs().size(); i++) {\n+                            OnnxOp.OnnxParameter p = schema.inputs().get(i);\n+                            Value v = io.operands().get(i);\n+\n+                            switch (p.quantifier()) {\n+                                case REQUIRED -> {\n+                                    opArgs.add(bb.context().getValue(v));\n+                                }\n+                                case OPTIONAL -> {\n+                                    \/\/ Evaluation of expressions Optional.empty and Optional.of() with symbolic values\n+                                    if (v instanceof Op.Result r && r.op() instanceof CoreOp.InvokeOp optionalInvoke\n+                                            && optionalInvoke.invokeDescriptor().refType().equals(JavaType.type(Optional.class))) {\n+                                        switch (optionalInvoke.invokeDescriptor().name()) {\n+                                            case \"of\" -> {\n+                                                opArgs.add(Optional.of(bb.context().getValue(optionalInvoke.operands().getFirst())));\n+                                            }\n+                                            case \"empty\" -> {\n+                                                opArgs.add(Optional.empty());\n+                                            }\n+                                            default -> throw new UnsupportedOperationException();\n+                                        }\n+                                    } else {\n+                                        throw new UnsupportedOperationException();\n+                                    }\n+                                }\n+                                case VARIADIC -> {\n+                                    throw new UnsupportedOperationException();\n+                                }\n+                            }\n+                        }\n+                        opArgs.addAll(attributes);\n+\n+                        OnnxOp onnxOp;\n+                        try {\n+                            onnxOp = (OnnxOp) opMethod.invoke(null, opArgs.toArray());\n+                        } catch (ReflectiveOperationException | RuntimeException e) {\n+                            throw new RuntimeException(e);\n+                        }\n+                        Op.Result result = bb.op(onnxOp);\n+                        bb.context().mapValue(io.result(), result);\n+                    }\n+                    \/\/ Transform access to the result of an operator that is a record access\n+                    case CoreOp.InvokeOp io when\n+                            recordComponentAccessToTupleIndex(l, io.invokeDescriptor()) instanceof Integer index -> {\n+                        Op.Result result = bb.op(CoreOp.tupleLoad(bb.context().getValue(io.operands().getFirst()), index));\n+                        bb.context().mapValue(io.result(), result);\n+                    }\n+                    \/\/ Copy remaining operations, which may be removed later transformations\n+                    default -> bb.op(op);\n+                }\n+                return bb;\n+            });\n+        });\n+\n+        return SSA.transform(onnxModel).transform((b, op) -> {\n+            \/\/ Drop any non-terminating operation whose result is not used\n+            if (op instanceof Op.Terminating || !op.result().uses().isEmpty()) {\n+                b.op(op);\n+            }\n+            return b;\n+        });\n+    }\n+\n+    @SuppressWarnings({\"rawtypes\", \"unchecked\"})\n+    static Class<? extends OnnxOp> onnxOpClassFromName(String operatorName) {\n+        Class<? extends OnnxOp> opClass;\n+        try {\n+            return (Class) Class.forName(OnnxOps.class.getName() + \"$\" + operatorName);\n+        } catch (ClassNotFoundException e) {\n+            throw new InternalError(e);\n+        }\n+    }\n+\n+    static OnnxOp.OnnxSchema schemaFromOnnxOpClass(Class<? extends OnnxOp> opClass) {\n+        try {\n+            return (OnnxOp.OnnxSchema) opClass.getField(\"SCHEMA\").get(null);\n+        } catch (ReflectiveOperationException e) {\n+            throw new InternalError(e);\n+        }\n+    }\n+\n+    static TupleType recordTypeToTupleType(MethodHandles.Lookup l, ClassType recordType) {\n+        Class<?> recordClass;\n+        try {\n+            recordClass = (Class<?>) recordType.rawType().resolve(l);\n+        } catch (ReflectiveOperationException e) {\n+            throw new RuntimeException(e);\n+        }\n+        assert recordClass.isRecord();\n+\n+        List<TypeElement> tupleComponentTypes = new ArrayList<>();\n+        for (RecordComponent rc : recordClass.getRecordComponents()) {\n+            switch (rc.getGenericType()) {\n+                case ParameterizedType pt when pt.getRawType().equals(Tensor.class) -> {\n+                    Type elementType = pt.getActualTypeArguments()[0];\n+                    switch (elementType) {\n+                        case Class<?> _ -> {\n+                            tupleComponentTypes.add(type(JavaType.type(pt)));\n+                        }\n+                        case TypeVariable<?> tv -> {\n+                            \/\/ Resolve type variable\n+                            JavaType e = null;\n+                            for (int j = 0; j < recordClass.getTypeParameters().length; j++) {\n+                                if (recordClass.getTypeParameters()[j].getName().equals(tv.getName())) {\n+                                    e = recordType.typeArguments().get(j);\n+                                    break;\n+                                }\n+                            }\n+                            tupleComponentTypes.add(type(JavaType.parameterized(JavaType.type(Tensor.class), e)));\n+                        }\n+                        default -> throw new IllegalStateException(\"Unexpected value: \" + elementType);\n+                    }\n+                }\n+                default -> throw new IllegalStateException(\"Unexpected value: \" + rc.getGenericType());\n+            }\n+        }\n+\n+        return TupleType.tupleType(tupleComponentTypes);\n+    }\n+\n+    static Integer recordComponentAccessToTupleIndex(MethodHandles.Lookup l, MethodRef ref) {\n+        if (ref.refType() instanceof ClassType ct && ct.toClassName().startsWith(\"oracle.code.onnx.OnnxOperators$\")) {\n+            Class<?> refClass;\n+            try {\n+                refClass = (Class<?>) ct.resolve(l);\n+            } catch (ReflectiveOperationException e) {\n+                throw new RuntimeException(e);\n+            }\n+\n+            if (refClass.isRecord()) {\n+                RecordComponent[] recordComponents = refClass.getRecordComponents();\n+                for (int i = 0; i < recordComponents.length; i++) {\n+                    if (recordComponents[i].getName().equals(ref.name())) {\n+                        return i;\n+                    }\n+                }\n+                throw new InternalError();\n+            }\n+        }\n+        return null;\n+    }\n+\n+    static final TypeElement TENSOR_RAW_CLASS = JavaType.type(Tensor.class);\n+\n+    \/\/ @@@ Map of Java tensor types to ONNX tensor types\n+    \/\/ @@@ Shape??\n+    static OnnxType type(TypeElement type) {\n+        if (type instanceof ClassType ct && ct.rawType().equals(TENSOR_RAW_CLASS)) {\n+            JavaType elementType = ct.typeArguments().getFirst();\n+            if (elementType.equals(JavaType.J_L_INTEGER)) {\n+                return OnnxType.TENSOR_INT32;\n+            } else if (elementType.equals(JavaType.J_L_FLOAT)) {\n+                return OnnxType.TENSOR_FLOAT32;\n+            } else if (elementType.equals(JavaType.J_L_LONG)) {\n+                return OnnxType.TENSOR_INT64;\n+            }\n+        }\n+        throw new UnsupportedOperationException(\"Unknown type: \" + type);\n+    }\n+\n+}\n","filename":"cr-examples\/onnx\/src\/main\/java\/oracle\/code\/onnx\/compiler\/OnnxTransformer.java","additions":229,"deletions":0,"binary":false,"changes":229,"status":"added"},{"patch":"@@ -855,1 +855,1 @@\n-            align_corners(Integer.class, true, 0),\n+            align_corners(Long.class, true, 0),\n@@ -968,1 +968,1 @@\n-        AffineGrid(TypeElement resultType, Value theta, Value size, java.util.Optional<Integer> align_corners) {\n+        AffineGrid(TypeElement resultType, Value theta, Value size, java.util.Optional<Long> align_corners) {\n@@ -990,2 +990,2 @@\n-        public java.util.Optional<Integer> align_corners() {\n-            Integer align_corners = Attribute.align_corners.access(Integer.class, onnxAttributes);\n+        public java.util.Optional<Long> align_corners() {\n+            Long align_corners = Attribute.align_corners.access(Long.class, onnxAttributes);\n@@ -997,1 +997,1 @@\n-    public static AffineGrid AffineGrid(TypeElement resultType, Value theta, Value size, java.util.Optional<Integer> align_corners) {\n+    public static AffineGrid AffineGrid(TypeElement resultType, Value theta, Value size, java.util.Optional<Long> align_corners) {\n@@ -1126,3 +1126,3 @@\n-            keepdims(Integer.class, true, 1),\n-            select_last_index(Integer.class, true, 0),\n-            axis(Integer.class, true, 0),\n+            keepdims(Long.class, true, 1),\n+            select_last_index(Long.class, true, 0),\n+            axis(Long.class, true, 0),\n@@ -1239,1 +1239,1 @@\n-        ArgMax(TypeElement resultType, Value data, java.util.Optional<Integer> keepdims, java.util.Optional<Integer> select_last_index, java.util.Optional<Integer> axis) {\n+        ArgMax(TypeElement resultType, Value data, java.util.Optional<Long> keepdims, java.util.Optional<Long> select_last_index, java.util.Optional<Long> axis) {\n@@ -1257,2 +1257,2 @@\n-        public java.util.Optional<Integer> keepdims() {\n-            Integer keepdims = Attribute.keepdims.access(Integer.class, onnxAttributes);\n+        public java.util.Optional<Long> keepdims() {\n+            Long keepdims = Attribute.keepdims.access(Long.class, onnxAttributes);\n@@ -1262,2 +1262,2 @@\n-        public java.util.Optional<Integer> select_last_index() {\n-            Integer select_last_index = Attribute.select_last_index.access(Integer.class, onnxAttributes);\n+        public java.util.Optional<Long> select_last_index() {\n+            Long select_last_index = Attribute.select_last_index.access(Long.class, onnxAttributes);\n@@ -1267,2 +1267,2 @@\n-        public java.util.Optional<Integer> axis() {\n-            Integer axis = Attribute.axis.access(Integer.class, onnxAttributes);\n+        public java.util.Optional<Long> axis() {\n+            Long axis = Attribute.axis.access(Long.class, onnxAttributes);\n@@ -1274,1 +1274,1 @@\n-    public static ArgMax ArgMax(TypeElement resultType, Value data, java.util.Optional<Integer> keepdims, java.util.Optional<Integer> select_last_index, java.util.Optional<Integer> axis) {\n+    public static ArgMax ArgMax(TypeElement resultType, Value data, java.util.Optional<Long> keepdims, java.util.Optional<Long> select_last_index, java.util.Optional<Long> axis) {\n@@ -1283,3 +1283,3 @@\n-            keepdims(Integer.class, true, 1),\n-            select_last_index(Integer.class, true, 0),\n-            axis(Integer.class, true, 0),\n+            keepdims(Long.class, true, 1),\n+            select_last_index(Long.class, true, 0),\n+            axis(Long.class, true, 0),\n@@ -1396,1 +1396,1 @@\n-        ArgMin(TypeElement resultType, Value data, java.util.Optional<Integer> keepdims, java.util.Optional<Integer> select_last_index, java.util.Optional<Integer> axis) {\n+        ArgMin(TypeElement resultType, Value data, java.util.Optional<Long> keepdims, java.util.Optional<Long> select_last_index, java.util.Optional<Long> axis) {\n@@ -1414,2 +1414,2 @@\n-        public java.util.Optional<Integer> keepdims() {\n-            Integer keepdims = Attribute.keepdims.access(Integer.class, onnxAttributes);\n+        public java.util.Optional<Long> keepdims() {\n+            Long keepdims = Attribute.keepdims.access(Long.class, onnxAttributes);\n@@ -1419,2 +1419,2 @@\n-        public java.util.Optional<Integer> select_last_index() {\n-            Integer select_last_index = Attribute.select_last_index.access(Integer.class, onnxAttributes);\n+        public java.util.Optional<Long> select_last_index() {\n+            Long select_last_index = Attribute.select_last_index.access(Long.class, onnxAttributes);\n@@ -1424,2 +1424,2 @@\n-        public java.util.Optional<Integer> axis() {\n-            Integer axis = Attribute.axis.access(Integer.class, onnxAttributes);\n+        public java.util.Optional<Long> axis() {\n+            Long axis = Attribute.axis.access(Long.class, onnxAttributes);\n@@ -1431,1 +1431,1 @@\n-    public static ArgMin ArgMin(TypeElement resultType, Value data, java.util.Optional<Integer> keepdims, java.util.Optional<Integer> select_last_index, java.util.Optional<Integer> axis) {\n+    public static ArgMin ArgMin(TypeElement resultType, Value data, java.util.Optional<Long> keepdims, java.util.Optional<Long> select_last_index, java.util.Optional<Long> axis) {\n@@ -2015,2 +2015,2 @@\n-            pads(int[].class, true, null),\n-            dilations(int[].class, true, null),\n+            pads(long[].class, true, null),\n+            dilations(long[].class, true, null),\n@@ -2018,4 +2018,4 @@\n-            count_include_pad(Integer.class, true, 0),\n-            ceil_mode(Integer.class, true, 0),\n-            strides(int[].class, true, null),\n-            kernel_shape(int[].class, false, null),\n+            count_include_pad(Long.class, true, 0),\n+            ceil_mode(Long.class, true, 0),\n+            strides(long[].class, true, null),\n+            kernel_shape(long[].class, false, null),\n@@ -2132,1 +2132,1 @@\n-        AveragePool(TypeElement resultType, Value X, java.util.Optional<int[]> pads, java.util.Optional<int[]> dilations, java.util.Optional<String> auto_pad, java.util.Optional<Integer> count_include_pad, java.util.Optional<Integer> ceil_mode, java.util.Optional<int[]> strides, int[] kernel_shape) {\n+        AveragePool(TypeElement resultType, Value X, java.util.Optional<long[]> pads, java.util.Optional<long[]> dilations, java.util.Optional<String> auto_pad, java.util.Optional<Long> count_include_pad, java.util.Optional<Long> ceil_mode, java.util.Optional<long[]> strides, long[] kernel_shape) {\n@@ -2150,3 +2150,3 @@\n-        public java.util.Optional<int[]> pads() {\n-            int[] pads = Attribute.pads.access(int[].class, onnxAttributes);\n-            return java.util.Optional.ofNullable(pads).map(int[]::clone);\n+        public java.util.Optional<long[]> pads() {\n+            long[] pads = Attribute.pads.access(long[].class, onnxAttributes);\n+            return java.util.Optional.ofNullable(pads).map(long[]::clone);\n@@ -2155,3 +2155,3 @@\n-        public java.util.Optional<int[]> dilations() {\n-            int[] dilations = Attribute.dilations.access(int[].class, onnxAttributes);\n-            return java.util.Optional.ofNullable(dilations).map(int[]::clone);\n+        public java.util.Optional<long[]> dilations() {\n+            long[] dilations = Attribute.dilations.access(long[].class, onnxAttributes);\n+            return java.util.Optional.ofNullable(dilations).map(long[]::clone);\n@@ -2165,2 +2165,2 @@\n-        public java.util.Optional<Integer> count_include_pad() {\n-            Integer count_include_pad = Attribute.count_include_pad.access(Integer.class, onnxAttributes);\n+        public java.util.Optional<Long> count_include_pad() {\n+            Long count_include_pad = Attribute.count_include_pad.access(Long.class, onnxAttributes);\n@@ -2170,2 +2170,2 @@\n-        public java.util.Optional<Integer> ceil_mode() {\n-            Integer ceil_mode = Attribute.ceil_mode.access(Integer.class, onnxAttributes);\n+        public java.util.Optional<Long> ceil_mode() {\n+            Long ceil_mode = Attribute.ceil_mode.access(Long.class, onnxAttributes);\n@@ -2175,3 +2175,3 @@\n-        public java.util.Optional<int[]> strides() {\n-            int[] strides = Attribute.strides.access(int[].class, onnxAttributes);\n-            return java.util.Optional.ofNullable(strides).map(int[]::clone);\n+        public java.util.Optional<long[]> strides() {\n+            long[] strides = Attribute.strides.access(long[].class, onnxAttributes);\n+            return java.util.Optional.ofNullable(strides).map(long[]::clone);\n@@ -2180,2 +2180,2 @@\n-        public int[] kernel_shape() {\n-            int[] kernel_shape = Attribute.kernel_shape.access(int[].class, onnxAttributes);\n+        public long[] kernel_shape() {\n+            long[] kernel_shape = Attribute.kernel_shape.access(long[].class, onnxAttributes);\n@@ -2187,1 +2187,1 @@\n-    public static AveragePool AveragePool(TypeElement resultType, Value X, java.util.Optional<int[]> pads, java.util.Optional<int[]> dilations, java.util.Optional<String> auto_pad, java.util.Optional<Integer> count_include_pad, java.util.Optional<Integer> ceil_mode, java.util.Optional<int[]> strides, int[] kernel_shape) {\n+    public static AveragePool AveragePool(TypeElement resultType, Value X, java.util.Optional<long[]> pads, java.util.Optional<long[]> dilations, java.util.Optional<String> auto_pad, java.util.Optional<Long> count_include_pad, java.util.Optional<Long> ceil_mode, java.util.Optional<long[]> strides, long[] kernel_shape) {\n@@ -2197,1 +2197,1 @@\n-            training_mode(Integer.class, true, 0),\n+            training_mode(Long.class, true, 0),\n@@ -2317,1 +2317,1 @@\n-        BatchNormalization(TypeElement resultType, Set<OutputParameter> optionalOutputs, Value X, Value scale, Value B, Value input_mean, Value input_var, java.util.Optional<Float> epsilon, java.util.Optional<Integer> training_mode, java.util.Optional<Float> momentum) {\n+        BatchNormalization(TypeElement resultType, Set<OutputParameter> optionalOutputs, Value X, Value scale, Value B, Value input_mean, Value input_var, java.util.Optional<Float> epsilon, java.util.Optional<Long> training_mode, java.util.Optional<Float> momentum) {\n@@ -2356,2 +2356,2 @@\n-        public java.util.Optional<Integer> training_mode() {\n-            Integer training_mode = Attribute.training_mode.access(Integer.class, onnxAttributes);\n+        public java.util.Optional<Long> training_mode() {\n+            Long training_mode = Attribute.training_mode.access(Long.class, onnxAttributes);\n@@ -2368,1 +2368,1 @@\n-    public static BatchNormalization BatchNormalization(TypeElement resultType, Set<BatchNormalization.OutputParameter> optionalOutputs, Value X, Value scale, Value B, Value input_mean, Value input_var, java.util.Optional<Float> epsilon, java.util.Optional<Integer> training_mode, java.util.Optional<Float> momentum) {\n+    public static BatchNormalization BatchNormalization(TypeElement resultType, Set<BatchNormalization.OutputParameter> optionalOutputs, Value X, Value scale, Value B, Value input_mean, Value input_var, java.util.Optional<Float> epsilon, java.util.Optional<Long> training_mode, java.util.Optional<Float> momentum) {\n@@ -2378,1 +2378,1 @@\n-            dtype(Integer.class, true, null),\n+            dtype(Long.class, true, null),\n@@ -2490,1 +2490,1 @@\n-        Bernoulli(TypeElement resultType, Value input, java.util.Optional<Float> seed, java.util.Optional<Integer> dtype) {\n+        Bernoulli(TypeElement resultType, Value input, java.util.Optional<Float> seed, java.util.Optional<Long> dtype) {\n@@ -2513,2 +2513,2 @@\n-        public java.util.Optional<Integer> dtype() {\n-            Integer dtype = Attribute.dtype.access(Integer.class, onnxAttributes);\n+        public java.util.Optional<Long> dtype() {\n+            Long dtype = Attribute.dtype.access(Long.class, onnxAttributes);\n@@ -2520,1 +2520,1 @@\n-    public static Bernoulli Bernoulli(TypeElement resultType, Value input, java.util.Optional<Float> seed, java.util.Optional<Integer> dtype) {\n+    public static Bernoulli Bernoulli(TypeElement resultType, Value input, java.util.Optional<Float> seed, java.util.Optional<Long> dtype) {\n@@ -3295,2 +3295,2 @@\n-            periodic(Integer.class, true, 1),\n-            output_datatype(Integer.class, true, 1),\n+            periodic(Long.class, true, 1),\n+            output_datatype(Long.class, true, 1),\n@@ -3408,1 +3408,1 @@\n-        BlackmanWindow(TypeElement resultType, Value size, java.util.Optional<Integer> periodic, java.util.Optional<Integer> output_datatype) {\n+        BlackmanWindow(TypeElement resultType, Value size, java.util.Optional<Long> periodic, java.util.Optional<Long> output_datatype) {\n@@ -3426,2 +3426,2 @@\n-        public java.util.Optional<Integer> periodic() {\n-            Integer periodic = Attribute.periodic.access(Integer.class, onnxAttributes);\n+        public java.util.Optional<Long> periodic() {\n+            Long periodic = Attribute.periodic.access(Long.class, onnxAttributes);\n@@ -3431,2 +3431,2 @@\n-        public java.util.Optional<Integer> output_datatype() {\n-            Integer output_datatype = Attribute.output_datatype.access(Integer.class, onnxAttributes);\n+        public java.util.Optional<Long> output_datatype() {\n+            Long output_datatype = Attribute.output_datatype.access(Long.class, onnxAttributes);\n@@ -3438,1 +3438,1 @@\n-    public static BlackmanWindow BlackmanWindow(TypeElement resultType, Value size, java.util.Optional<Integer> periodic, java.util.Optional<Integer> output_datatype) {\n+    public static BlackmanWindow BlackmanWindow(TypeElement resultType, Value size, java.util.Optional<Long> periodic, java.util.Optional<Long> output_datatype) {\n@@ -3447,2 +3447,2 @@\n-            saturate(Integer.class, true, 1),\n-            to(Integer.class, false, null),\n+            saturate(Long.class, true, 1),\n+            to(Long.class, false, null),\n@@ -3560,1 +3560,1 @@\n-        Cast(TypeElement resultType, Value input, java.util.Optional<Integer> saturate, int to) {\n+        Cast(TypeElement resultType, Value input, java.util.Optional<Long> saturate, long to) {\n@@ -3578,2 +3578,2 @@\n-        public java.util.Optional<Integer> saturate() {\n-            Integer saturate = Attribute.saturate.access(Integer.class, onnxAttributes);\n+        public java.util.Optional<Long> saturate() {\n+            Long saturate = Attribute.saturate.access(Long.class, onnxAttributes);\n@@ -3583,2 +3583,2 @@\n-        public int to() {\n-            int to = Attribute.to.access(int.class, onnxAttributes);\n+        public long to() {\n+            long to = Attribute.to.access(long.class, onnxAttributes);\n@@ -3590,1 +3590,1 @@\n-    public static Cast Cast(TypeElement resultType, Value input, java.util.Optional<Integer> saturate, int to) {\n+    public static Cast Cast(TypeElement resultType, Value input, java.util.Optional<Long> saturate, long to) {\n@@ -3599,1 +3599,1 @@\n-            saturate(Integer.class, true, 1),\n+            saturate(Long.class, true, 1),\n@@ -3712,1 +3712,1 @@\n-        CastLike(TypeElement resultType, Value input, Value target_type, java.util.Optional<Integer> saturate) {\n+        CastLike(TypeElement resultType, Value input, Value target_type, java.util.Optional<Long> saturate) {\n@@ -3734,2 +3734,2 @@\n-        public java.util.Optional<Integer> saturate() {\n-            Integer saturate = Attribute.saturate.access(Integer.class, onnxAttributes);\n+        public java.util.Optional<Long> saturate() {\n+            Long saturate = Attribute.saturate.access(Long.class, onnxAttributes);\n@@ -3741,1 +3741,1 @@\n-    public static CastLike CastLike(TypeElement resultType, Value input, Value target_type, java.util.Optional<Integer> saturate) {\n+    public static CastLike CastLike(TypeElement resultType, Value input, Value target_type, java.util.Optional<Long> saturate) {\n@@ -3752,1 +3752,1 @@\n-            max_map(Integer.class, true, 1),\n+            max_map(Long.class, true, 1),\n@@ -3864,1 +3864,1 @@\n-        CastMap(TypeElement resultType, Value X, java.util.Optional<String> map_form, java.util.Optional<String> cast_to, java.util.Optional<Integer> max_map) {\n+        CastMap(TypeElement resultType, Value X, java.util.Optional<String> map_form, java.util.Optional<String> cast_to, java.util.Optional<Long> max_map) {\n@@ -3892,2 +3892,2 @@\n-        public java.util.Optional<Integer> max_map() {\n-            Integer max_map = Attribute.max_map.access(Integer.class, onnxAttributes);\n+        public java.util.Optional<Long> max_map() {\n+            Long max_map = Attribute.max_map.access(Long.class, onnxAttributes);\n@@ -3899,1 +3899,1 @@\n-    public static CastMap CastMap(TypeElement resultType, Value X, java.util.Optional<String> map_form, java.util.Optional<String> cast_to, java.util.Optional<Integer> max_map) {\n+    public static CastMap CastMap(TypeElement resultType, Value X, java.util.Optional<String> map_form, java.util.Optional<String> cast_to, java.util.Optional<Long> max_map) {\n@@ -3908,1 +3908,1 @@\n-            cats_int64s(int[].class, true, null),\n+            cats_int64s(long[].class, true, null),\n@@ -3910,1 +3910,1 @@\n-            default_int64(Integer.class, true, -1),\n+            default_int64(Long.class, true, -1),\n@@ -4023,1 +4023,1 @@\n-        CategoryMapper(TypeElement resultType, Value X, java.util.Optional<int[]> cats_int64s, java.util.Optional<String[]> cats_strings, java.util.Optional<Integer> default_int64, java.util.Optional<String> default_string) {\n+        CategoryMapper(TypeElement resultType, Value X, java.util.Optional<long[]> cats_int64s, java.util.Optional<String[]> cats_strings, java.util.Optional<Long> default_int64, java.util.Optional<String> default_string) {\n@@ -4041,3 +4041,3 @@\n-        public java.util.Optional<int[]> cats_int64s() {\n-            int[] cats_int64s = Attribute.cats_int64s.access(int[].class, onnxAttributes);\n-            return java.util.Optional.ofNullable(cats_int64s).map(int[]::clone);\n+        public java.util.Optional<long[]> cats_int64s() {\n+            long[] cats_int64s = Attribute.cats_int64s.access(long[].class, onnxAttributes);\n+            return java.util.Optional.ofNullable(cats_int64s).map(long[]::clone);\n@@ -4051,2 +4051,2 @@\n-        public java.util.Optional<Integer> default_int64() {\n-            Integer default_int64 = Attribute.default_int64.access(Integer.class, onnxAttributes);\n+        public java.util.Optional<Long> default_int64() {\n+            Long default_int64 = Attribute.default_int64.access(Long.class, onnxAttributes);\n@@ -4063,1 +4063,1 @@\n-    public static CategoryMapper CategoryMapper(TypeElement resultType, Value X, java.util.Optional<int[]> cats_int64s, java.util.Optional<String[]> cats_strings, java.util.Optional<Integer> default_int64, java.util.Optional<String> default_string) {\n+    public static CategoryMapper CategoryMapper(TypeElement resultType, Value X, java.util.Optional<long[]> cats_int64s, java.util.Optional<String[]> cats_strings, java.util.Optional<Long> default_int64, java.util.Optional<String> default_string) {\n@@ -4331,1 +4331,1 @@\n-            axes(int[].class, true, null),\n+            axes(long[].class, true, null),\n@@ -4444,1 +4444,1 @@\n-        CenterCropPad(TypeElement resultType, Value input_data, Value shape, java.util.Optional<int[]> axes) {\n+        CenterCropPad(TypeElement resultType, Value input_data, Value shape, java.util.Optional<long[]> axes) {\n@@ -4466,3 +4466,3 @@\n-        public java.util.Optional<int[]> axes() {\n-            int[] axes = Attribute.axes.access(int[].class, onnxAttributes);\n-            return java.util.Optional.ofNullable(axes).map(int[]::clone);\n+        public java.util.Optional<long[]> axes() {\n+            long[] axes = Attribute.axes.access(long[].class, onnxAttributes);\n+            return java.util.Optional.ofNullable(axes).map(long[]::clone);\n@@ -4473,1 +4473,1 @@\n-    public static CenterCropPad CenterCropPad(TypeElement resultType, Value input_data, Value shape, java.util.Optional<int[]> axes) {\n+    public static CenterCropPad CenterCropPad(TypeElement resultType, Value input_data, Value shape, java.util.Optional<long[]> axes) {\n@@ -4608,3 +4608,3 @@\n-            pads(int[].class, true, null),\n-            dilations(int[].class, true, null),\n-            strides(int[].class, true, null),\n+            pads(long[].class, true, null),\n+            dilations(long[].class, true, null),\n+            strides(long[].class, true, null),\n@@ -4723,1 +4723,1 @@\n-        Col2Im(TypeElement resultType, Value input, Value image_shape, Value block_shape, java.util.Optional<int[]> pads, java.util.Optional<int[]> dilations, java.util.Optional<int[]> strides) {\n+        Col2Im(TypeElement resultType, Value input, Value image_shape, Value block_shape, java.util.Optional<long[]> pads, java.util.Optional<long[]> dilations, java.util.Optional<long[]> strides) {\n@@ -4749,3 +4749,3 @@\n-        public java.util.Optional<int[]> pads() {\n-            int[] pads = Attribute.pads.access(int[].class, onnxAttributes);\n-            return java.util.Optional.ofNullable(pads).map(int[]::clone);\n+        public java.util.Optional<long[]> pads() {\n+            long[] pads = Attribute.pads.access(long[].class, onnxAttributes);\n+            return java.util.Optional.ofNullable(pads).map(long[]::clone);\n@@ -4754,3 +4754,3 @@\n-        public java.util.Optional<int[]> dilations() {\n-            int[] dilations = Attribute.dilations.access(int[].class, onnxAttributes);\n-            return java.util.Optional.ofNullable(dilations).map(int[]::clone);\n+        public java.util.Optional<long[]> dilations() {\n+            long[] dilations = Attribute.dilations.access(long[].class, onnxAttributes);\n+            return java.util.Optional.ofNullable(dilations).map(long[]::clone);\n@@ -4759,3 +4759,3 @@\n-        public java.util.Optional<int[]> strides() {\n-            int[] strides = Attribute.strides.access(int[].class, onnxAttributes);\n-            return java.util.Optional.ofNullable(strides).map(int[]::clone);\n+        public java.util.Optional<long[]> strides() {\n+            long[] strides = Attribute.strides.access(long[].class, onnxAttributes);\n+            return java.util.Optional.ofNullable(strides).map(long[]::clone);\n@@ -4766,1 +4766,1 @@\n-    public static Col2Im Col2Im(TypeElement resultType, Value input, Value image_shape, Value block_shape, java.util.Optional<int[]> pads, java.util.Optional<int[]> dilations, java.util.Optional<int[]> strides) {\n+    public static Col2Im Col2Im(TypeElement resultType, Value input, Value image_shape, Value block_shape, java.util.Optional<long[]> pads, java.util.Optional<long[]> dilations, java.util.Optional<long[]> strides) {\n@@ -4775,1 +4775,1 @@\n-            axis(Integer.class, true, null),\n+            axis(Long.class, true, null),\n@@ -4888,1 +4888,1 @@\n-        Compress(TypeElement resultType, Value input, Value condition, java.util.Optional<Integer> axis) {\n+        Compress(TypeElement resultType, Value input, Value condition, java.util.Optional<Long> axis) {\n@@ -4910,2 +4910,2 @@\n-        public java.util.Optional<Integer> axis() {\n-            Integer axis = Attribute.axis.access(Integer.class, onnxAttributes);\n+        public java.util.Optional<Long> axis() {\n+            Long axis = Attribute.axis.access(Long.class, onnxAttributes);\n@@ -4917,1 +4917,1 @@\n-    public static Compress Compress(TypeElement resultType, Value input, Value condition, java.util.Optional<Integer> axis) {\n+    public static Compress Compress(TypeElement resultType, Value input, Value condition, java.util.Optional<Long> axis) {\n@@ -4926,1 +4926,1 @@\n-            axis(Integer.class, false, null),\n+            axis(Long.class, false, null),\n@@ -5037,1 +5037,1 @@\n-        Concat(TypeElement resultType, List<Value> inputs, int axis) {\n+        Concat(TypeElement resultType, List<Value> inputs, long axis) {\n@@ -5055,2 +5055,2 @@\n-        public int axis() {\n-            int axis = Attribute.axis.access(int.class, onnxAttributes);\n+        public long axis() {\n+            long axis = Attribute.axis.access(long.class, onnxAttributes);\n@@ -5062,1 +5062,1 @@\n-    public static Concat Concat(TypeElement resultType, List<Value> inputs, int axis) {\n+    public static Concat Concat(TypeElement resultType, List<Value> inputs, long axis) {\n@@ -5071,2 +5071,2 @@\n-            axis(Integer.class, false, null),\n-            new_axis(Integer.class, true, 0),\n+            axis(Long.class, false, null),\n+            new_axis(Long.class, true, 0),\n@@ -5184,1 +5184,1 @@\n-        ConcatFromSequence(TypeElement resultType, Value input_sequence, int axis, java.util.Optional<Integer> new_axis) {\n+        ConcatFromSequence(TypeElement resultType, Value input_sequence, long axis, java.util.Optional<Long> new_axis) {\n@@ -5202,2 +5202,2 @@\n-        public int axis() {\n-            int axis = Attribute.axis.access(int.class, onnxAttributes);\n+        public long axis() {\n+            long axis = Attribute.axis.access(long.class, onnxAttributes);\n@@ -5207,2 +5207,2 @@\n-        public java.util.Optional<Integer> new_axis() {\n-            Integer new_axis = Attribute.new_axis.access(Integer.class, onnxAttributes);\n+        public java.util.Optional<Long> new_axis() {\n+            Long new_axis = Attribute.new_axis.access(Long.class, onnxAttributes);\n@@ -5214,1 +5214,1 @@\n-    public static ConcatFromSequence ConcatFromSequence(TypeElement resultType, Value input_sequence, int axis, java.util.Optional<Integer> new_axis) {\n+    public static ConcatFromSequence ConcatFromSequence(TypeElement resultType, Value input_sequence, long axis, java.util.Optional<Long> new_axis) {\n@@ -5223,1 +5223,1 @@\n-            value_int(Integer.class, true, null),\n+            value_int(Long.class, true, null),\n@@ -5228,1 +5228,1 @@\n-            value_ints(int[].class, true, null),\n+            value_ints(long[].class, true, null),\n@@ -5320,1 +5320,1 @@\n-        Constant(TypeElement resultType, java.util.Optional<Integer> value_int, java.util.Optional<float[]> value_floats, java.util.Optional<String[]> value_strings, java.util.Optional<Float> value_float, java.util.Optional<String> value_string, java.util.Optional<int[]> value_ints, java.util.Optional<byte[]> sparse_value, java.util.Optional<byte[]> value) {\n+        Constant(TypeElement resultType, java.util.Optional<Long> value_int, java.util.Optional<float[]> value_floats, java.util.Optional<String[]> value_strings, java.util.Optional<Float> value_float, java.util.Optional<String> value_string, java.util.Optional<long[]> value_ints, java.util.Optional<byte[]> sparse_value, java.util.Optional<byte[]> value) {\n@@ -5334,2 +5334,2 @@\n-        public java.util.Optional<Integer> value_int() {\n-            Integer value_int = Attribute.value_int.access(Integer.class, onnxAttributes);\n+        public java.util.Optional<Long> value_int() {\n+            Long value_int = Attribute.value_int.access(Long.class, onnxAttributes);\n@@ -5359,3 +5359,3 @@\n-        public java.util.Optional<int[]> value_ints() {\n-            int[] value_ints = Attribute.value_ints.access(int[].class, onnxAttributes);\n-            return java.util.Optional.ofNullable(value_ints).map(int[]::clone);\n+        public java.util.Optional<long[]> value_ints() {\n+            long[] value_ints = Attribute.value_ints.access(long[].class, onnxAttributes);\n+            return java.util.Optional.ofNullable(value_ints).map(long[]::clone);\n@@ -5376,1 +5376,1 @@\n-    public static Constant Constant(TypeElement resultType, java.util.Optional<Integer> value_int, java.util.Optional<float[]> value_floats, java.util.Optional<String[]> value_strings, java.util.Optional<Float> value_float, java.util.Optional<String> value_string, java.util.Optional<int[]> value_ints, java.util.Optional<byte[]> sparse_value, java.util.Optional<byte[]> value) {\n+    public static Constant Constant(TypeElement resultType, java.util.Optional<Long> value_int, java.util.Optional<float[]> value_floats, java.util.Optional<String[]> value_strings, java.util.Optional<Float> value_float, java.util.Optional<String> value_string, java.util.Optional<long[]> value_ints, java.util.Optional<byte[]> sparse_value, java.util.Optional<byte[]> value) {\n@@ -5531,2 +5531,2 @@\n-            pads(int[].class, true, null),\n-            dilations(int[].class, true, null),\n+            pads(long[].class, true, null),\n+            dilations(long[].class, true, null),\n@@ -5534,3 +5534,3 @@\n-            strides(int[].class, true, null),\n-            group(Integer.class, true, 1),\n-            kernel_shape(int[].class, true, null),\n+            strides(long[].class, true, null),\n+            group(Long.class, true, 1),\n+            kernel_shape(long[].class, true, null),\n@@ -5649,1 +5649,1 @@\n-        Conv(TypeElement resultType, Value X, Value W, java.util.Optional<Value> B, java.util.Optional<int[]> pads, java.util.Optional<int[]> dilations, java.util.Optional<String> auto_pad, java.util.Optional<int[]> strides, java.util.Optional<Integer> group, java.util.Optional<int[]> kernel_shape) {\n+        Conv(TypeElement resultType, Value X, Value W, java.util.Optional<Value> B, java.util.Optional<long[]> pads, java.util.Optional<long[]> dilations, java.util.Optional<String> auto_pad, java.util.Optional<long[]> strides, java.util.Optional<Long> group, java.util.Optional<long[]> kernel_shape) {\n@@ -5676,3 +5676,3 @@\n-        public java.util.Optional<int[]> pads() {\n-            int[] pads = Attribute.pads.access(int[].class, onnxAttributes);\n-            return java.util.Optional.ofNullable(pads).map(int[]::clone);\n+        public java.util.Optional<long[]> pads() {\n+            long[] pads = Attribute.pads.access(long[].class, onnxAttributes);\n+            return java.util.Optional.ofNullable(pads).map(long[]::clone);\n@@ -5681,3 +5681,3 @@\n-        public java.util.Optional<int[]> dilations() {\n-            int[] dilations = Attribute.dilations.access(int[].class, onnxAttributes);\n-            return java.util.Optional.ofNullable(dilations).map(int[]::clone);\n+        public java.util.Optional<long[]> dilations() {\n+            long[] dilations = Attribute.dilations.access(long[].class, onnxAttributes);\n+            return java.util.Optional.ofNullable(dilations).map(long[]::clone);\n@@ -5691,3 +5691,3 @@\n-        public java.util.Optional<int[]> strides() {\n-            int[] strides = Attribute.strides.access(int[].class, onnxAttributes);\n-            return java.util.Optional.ofNullable(strides).map(int[]::clone);\n+        public java.util.Optional<long[]> strides() {\n+            long[] strides = Attribute.strides.access(long[].class, onnxAttributes);\n+            return java.util.Optional.ofNullable(strides).map(long[]::clone);\n@@ -5696,2 +5696,2 @@\n-        public java.util.Optional<Integer> group() {\n-            Integer group = Attribute.group.access(Integer.class, onnxAttributes);\n+        public java.util.Optional<Long> group() {\n+            Long group = Attribute.group.access(Long.class, onnxAttributes);\n@@ -5701,3 +5701,3 @@\n-        public java.util.Optional<int[]> kernel_shape() {\n-            int[] kernel_shape = Attribute.kernel_shape.access(int[].class, onnxAttributes);\n-            return java.util.Optional.ofNullable(kernel_shape).map(int[]::clone);\n+        public java.util.Optional<long[]> kernel_shape() {\n+            long[] kernel_shape = Attribute.kernel_shape.access(long[].class, onnxAttributes);\n+            return java.util.Optional.ofNullable(kernel_shape).map(long[]::clone);\n@@ -5708,1 +5708,1 @@\n-    public static Conv Conv(TypeElement resultType, Value X, Value W, java.util.Optional<Value> B, java.util.Optional<int[]> pads, java.util.Optional<int[]> dilations, java.util.Optional<String> auto_pad, java.util.Optional<int[]> strides, java.util.Optional<Integer> group, java.util.Optional<int[]> kernel_shape) {\n+    public static Conv Conv(TypeElement resultType, Value X, Value W, java.util.Optional<Value> B, java.util.Optional<long[]> pads, java.util.Optional<long[]> dilations, java.util.Optional<String> auto_pad, java.util.Optional<long[]> strides, java.util.Optional<Long> group, java.util.Optional<long[]> kernel_shape) {\n@@ -5717,2 +5717,2 @@\n-            pads(int[].class, true, null),\n-            dilations(int[].class, true, null),\n+            pads(long[].class, true, null),\n+            dilations(long[].class, true, null),\n@@ -5720,3 +5720,3 @@\n-            strides(int[].class, true, null),\n-            group(Integer.class, true, 1),\n-            kernel_shape(int[].class, true, null),\n+            strides(long[].class, true, null),\n+            group(Long.class, true, 1),\n+            kernel_shape(long[].class, true, null),\n@@ -5838,1 +5838,1 @@\n-        ConvInteger(TypeElement resultType, Value x, Value w, java.util.Optional<Value> x_zero_point, java.util.Optional<Value> w_zero_point, java.util.Optional<int[]> pads, java.util.Optional<int[]> dilations, java.util.Optional<String> auto_pad, java.util.Optional<int[]> strides, java.util.Optional<Integer> group, java.util.Optional<int[]> kernel_shape) {\n+        ConvInteger(TypeElement resultType, Value x, Value w, java.util.Optional<Value> x_zero_point, java.util.Optional<Value> w_zero_point, java.util.Optional<long[]> pads, java.util.Optional<long[]> dilations, java.util.Optional<String> auto_pad, java.util.Optional<long[]> strides, java.util.Optional<Long> group, java.util.Optional<long[]> kernel_shape) {\n@@ -5870,3 +5870,3 @@\n-        public java.util.Optional<int[]> pads() {\n-            int[] pads = Attribute.pads.access(int[].class, onnxAttributes);\n-            return java.util.Optional.ofNullable(pads).map(int[]::clone);\n+        public java.util.Optional<long[]> pads() {\n+            long[] pads = Attribute.pads.access(long[].class, onnxAttributes);\n+            return java.util.Optional.ofNullable(pads).map(long[]::clone);\n@@ -5875,3 +5875,3 @@\n-        public java.util.Optional<int[]> dilations() {\n-            int[] dilations = Attribute.dilations.access(int[].class, onnxAttributes);\n-            return java.util.Optional.ofNullable(dilations).map(int[]::clone);\n+        public java.util.Optional<long[]> dilations() {\n+            long[] dilations = Attribute.dilations.access(long[].class, onnxAttributes);\n+            return java.util.Optional.ofNullable(dilations).map(long[]::clone);\n@@ -5885,3 +5885,3 @@\n-        public java.util.Optional<int[]> strides() {\n-            int[] strides = Attribute.strides.access(int[].class, onnxAttributes);\n-            return java.util.Optional.ofNullable(strides).map(int[]::clone);\n+        public java.util.Optional<long[]> strides() {\n+            long[] strides = Attribute.strides.access(long[].class, onnxAttributes);\n+            return java.util.Optional.ofNullable(strides).map(long[]::clone);\n@@ -5890,2 +5890,2 @@\n-        public java.util.Optional<Integer> group() {\n-            Integer group = Attribute.group.access(Integer.class, onnxAttributes);\n+        public java.util.Optional<Long> group() {\n+            Long group = Attribute.group.access(Long.class, onnxAttributes);\n@@ -5895,3 +5895,3 @@\n-        public java.util.Optional<int[]> kernel_shape() {\n-            int[] kernel_shape = Attribute.kernel_shape.access(int[].class, onnxAttributes);\n-            return java.util.Optional.ofNullable(kernel_shape).map(int[]::clone);\n+        public java.util.Optional<long[]> kernel_shape() {\n+            long[] kernel_shape = Attribute.kernel_shape.access(long[].class, onnxAttributes);\n+            return java.util.Optional.ofNullable(kernel_shape).map(long[]::clone);\n@@ -5902,1 +5902,1 @@\n-    public static ConvInteger ConvInteger(TypeElement resultType, Value x, Value w, java.util.Optional<Value> x_zero_point, java.util.Optional<Value> w_zero_point, java.util.Optional<int[]> pads, java.util.Optional<int[]> dilations, java.util.Optional<String> auto_pad, java.util.Optional<int[]> strides, java.util.Optional<Integer> group, java.util.Optional<int[]> kernel_shape) {\n+    public static ConvInteger ConvInteger(TypeElement resultType, Value x, Value w, java.util.Optional<Value> x_zero_point, java.util.Optional<Value> w_zero_point, java.util.Optional<long[]> pads, java.util.Optional<long[]> dilations, java.util.Optional<String> auto_pad, java.util.Optional<long[]> strides, java.util.Optional<Long> group, java.util.Optional<long[]> kernel_shape) {\n@@ -5911,3 +5911,3 @@\n-            output_shape(int[].class, true, null),\n-            pads(int[].class, true, null),\n-            dilations(int[].class, true, null),\n+            output_shape(long[].class, true, null),\n+            pads(long[].class, true, null),\n+            dilations(long[].class, true, null),\n@@ -5915,4 +5915,4 @@\n-            strides(int[].class, true, null),\n-            group(Integer.class, true, 1),\n-            kernel_shape(int[].class, true, null),\n-            output_padding(int[].class, true, null),\n+            strides(long[].class, true, null),\n+            group(Long.class, true, 1),\n+            kernel_shape(long[].class, true, null),\n+            output_padding(long[].class, true, null),\n@@ -6031,1 +6031,1 @@\n-        ConvTranspose(TypeElement resultType, Value X, Value W, java.util.Optional<Value> B, java.util.Optional<int[]> output_shape, java.util.Optional<int[]> pads, java.util.Optional<int[]> dilations, java.util.Optional<String> auto_pad, java.util.Optional<int[]> strides, java.util.Optional<Integer> group, java.util.Optional<int[]> kernel_shape, java.util.Optional<int[]> output_padding) {\n+        ConvTranspose(TypeElement resultType, Value X, Value W, java.util.Optional<Value> B, java.util.Optional<long[]> output_shape, java.util.Optional<long[]> pads, java.util.Optional<long[]> dilations, java.util.Optional<String> auto_pad, java.util.Optional<long[]> strides, java.util.Optional<Long> group, java.util.Optional<long[]> kernel_shape, java.util.Optional<long[]> output_padding) {\n@@ -6058,3 +6058,3 @@\n-        public java.util.Optional<int[]> output_shape() {\n-            int[] output_shape = Attribute.output_shape.access(int[].class, onnxAttributes);\n-            return java.util.Optional.ofNullable(output_shape).map(int[]::clone);\n+        public java.util.Optional<long[]> output_shape() {\n+            long[] output_shape = Attribute.output_shape.access(long[].class, onnxAttributes);\n+            return java.util.Optional.ofNullable(output_shape).map(long[]::clone);\n@@ -6063,3 +6063,3 @@\n-        public java.util.Optional<int[]> pads() {\n-            int[] pads = Attribute.pads.access(int[].class, onnxAttributes);\n-            return java.util.Optional.ofNullable(pads).map(int[]::clone);\n+        public java.util.Optional<long[]> pads() {\n+            long[] pads = Attribute.pads.access(long[].class, onnxAttributes);\n+            return java.util.Optional.ofNullable(pads).map(long[]::clone);\n@@ -6068,3 +6068,3 @@\n-        public java.util.Optional<int[]> dilations() {\n-            int[] dilations = Attribute.dilations.access(int[].class, onnxAttributes);\n-            return java.util.Optional.ofNullable(dilations).map(int[]::clone);\n+        public java.util.Optional<long[]> dilations() {\n+            long[] dilations = Attribute.dilations.access(long[].class, onnxAttributes);\n+            return java.util.Optional.ofNullable(dilations).map(long[]::clone);\n@@ -6078,3 +6078,3 @@\n-        public java.util.Optional<int[]> strides() {\n-            int[] strides = Attribute.strides.access(int[].class, onnxAttributes);\n-            return java.util.Optional.ofNullable(strides).map(int[]::clone);\n+        public java.util.Optional<long[]> strides() {\n+            long[] strides = Attribute.strides.access(long[].class, onnxAttributes);\n+            return java.util.Optional.ofNullable(strides).map(long[]::clone);\n@@ -6083,2 +6083,2 @@\n-        public java.util.Optional<Integer> group() {\n-            Integer group = Attribute.group.access(Integer.class, onnxAttributes);\n+        public java.util.Optional<Long> group() {\n+            Long group = Attribute.group.access(Long.class, onnxAttributes);\n@@ -6088,3 +6088,3 @@\n-        public java.util.Optional<int[]> kernel_shape() {\n-            int[] kernel_shape = Attribute.kernel_shape.access(int[].class, onnxAttributes);\n-            return java.util.Optional.ofNullable(kernel_shape).map(int[]::clone);\n+        public java.util.Optional<long[]> kernel_shape() {\n+            long[] kernel_shape = Attribute.kernel_shape.access(long[].class, onnxAttributes);\n+            return java.util.Optional.ofNullable(kernel_shape).map(long[]::clone);\n@@ -6093,3 +6093,3 @@\n-        public java.util.Optional<int[]> output_padding() {\n-            int[] output_padding = Attribute.output_padding.access(int[].class, onnxAttributes);\n-            return java.util.Optional.ofNullable(output_padding).map(int[]::clone);\n+        public java.util.Optional<long[]> output_padding() {\n+            long[] output_padding = Attribute.output_padding.access(long[].class, onnxAttributes);\n+            return java.util.Optional.ofNullable(output_padding).map(long[]::clone);\n@@ -6100,1 +6100,1 @@\n-    public static ConvTranspose ConvTranspose(TypeElement resultType, Value X, Value W, java.util.Optional<Value> B, java.util.Optional<int[]> output_shape, java.util.Optional<int[]> pads, java.util.Optional<int[]> dilations, java.util.Optional<String> auto_pad, java.util.Optional<int[]> strides, java.util.Optional<Integer> group, java.util.Optional<int[]> kernel_shape, java.util.Optional<int[]> output_padding) {\n+    public static ConvTranspose ConvTranspose(TypeElement resultType, Value X, Value W, java.util.Optional<Value> B, java.util.Optional<long[]> output_shape, java.util.Optional<long[]> pads, java.util.Optional<long[]> dilations, java.util.Optional<String> auto_pad, java.util.Optional<long[]> strides, java.util.Optional<Long> group, java.util.Optional<long[]> kernel_shape, java.util.Optional<long[]> output_padding) {\n@@ -6337,2 +6337,2 @@\n-            exclusive(Integer.class, true, 0),\n-            reverse(Integer.class, true, 0),\n+            exclusive(Long.class, true, 0),\n+            reverse(Long.class, true, 0),\n@@ -6451,1 +6451,1 @@\n-        CumSum(TypeElement resultType, Value x, Value axis, java.util.Optional<Integer> exclusive, java.util.Optional<Integer> reverse) {\n+        CumSum(TypeElement resultType, Value x, Value axis, java.util.Optional<Long> exclusive, java.util.Optional<Long> reverse) {\n@@ -6473,2 +6473,2 @@\n-        public java.util.Optional<Integer> exclusive() {\n-            Integer exclusive = Attribute.exclusive.access(Integer.class, onnxAttributes);\n+        public java.util.Optional<Long> exclusive() {\n+            Long exclusive = Attribute.exclusive.access(Long.class, onnxAttributes);\n@@ -6478,2 +6478,2 @@\n-        public java.util.Optional<Integer> reverse() {\n-            Integer reverse = Attribute.reverse.access(Integer.class, onnxAttributes);\n+        public java.util.Optional<Long> reverse() {\n+            Long reverse = Attribute.reverse.access(Long.class, onnxAttributes);\n@@ -6485,1 +6485,1 @@\n-    public static CumSum CumSum(TypeElement resultType, Value x, Value axis, java.util.Optional<Integer> exclusive, java.util.Optional<Integer> reverse) {\n+    public static CumSum CumSum(TypeElement resultType, Value x, Value axis, java.util.Optional<Long> exclusive, java.util.Optional<Long> reverse) {\n@@ -6494,2 +6494,2 @@\n-            inverse(Integer.class, true, 0),\n-            onesided(Integer.class, true, 0),\n+            inverse(Long.class, true, 0),\n+            onesided(Long.class, true, 0),\n@@ -6609,1 +6609,1 @@\n-        DFT(TypeElement resultType, Value input, java.util.Optional<Value> dft_length, java.util.Optional<Value> axis, java.util.Optional<Integer> inverse, java.util.Optional<Integer> onesided) {\n+        DFT(TypeElement resultType, Value input, java.util.Optional<Value> dft_length, java.util.Optional<Value> axis, java.util.Optional<Long> inverse, java.util.Optional<Long> onesided) {\n@@ -6637,2 +6637,2 @@\n-        public java.util.Optional<Integer> inverse() {\n-            Integer inverse = Attribute.inverse.access(Integer.class, onnxAttributes);\n+        public java.util.Optional<Long> inverse() {\n+            Long inverse = Attribute.inverse.access(Long.class, onnxAttributes);\n@@ -6642,2 +6642,2 @@\n-        public java.util.Optional<Integer> onesided() {\n-            Integer onesided = Attribute.onesided.access(Integer.class, onnxAttributes);\n+        public java.util.Optional<Long> onesided() {\n+            Long onesided = Attribute.onesided.access(Long.class, onnxAttributes);\n@@ -6649,1 +6649,1 @@\n-    public static DFT DFT(TypeElement resultType, Value input, java.util.Optional<Value> dft_length, java.util.Optional<Value> axis, java.util.Optional<Integer> inverse, java.util.Optional<Integer> onesided) {\n+    public static DFT DFT(TypeElement resultType, Value input, java.util.Optional<Value> dft_length, java.util.Optional<Value> axis, java.util.Optional<Long> inverse, java.util.Optional<Long> onesided) {\n@@ -6658,6 +6658,6 @@\n-            pads(int[].class, true, null),\n-            dilations(int[].class, true, null),\n-            strides(int[].class, true, null),\n-            offset_group(Integer.class, true, 1),\n-            group(Integer.class, true, 1),\n-            kernel_shape(int[].class, true, null),\n+            pads(long[].class, true, null),\n+            dilations(long[].class, true, null),\n+            strides(long[].class, true, null),\n+            offset_group(Long.class, true, 1),\n+            group(Long.class, true, 1),\n+            kernel_shape(long[].class, true, null),\n@@ -6778,1 +6778,1 @@\n-        DeformConv(TypeElement resultType, Value X, Value W, Value offset, java.util.Optional<Value> B, java.util.Optional<Value> mask, java.util.Optional<int[]> pads, java.util.Optional<int[]> dilations, java.util.Optional<int[]> strides, java.util.Optional<Integer> offset_group, java.util.Optional<Integer> group, java.util.Optional<int[]> kernel_shape) {\n+        DeformConv(TypeElement resultType, Value X, Value W, Value offset, java.util.Optional<Value> B, java.util.Optional<Value> mask, java.util.Optional<long[]> pads, java.util.Optional<long[]> dilations, java.util.Optional<long[]> strides, java.util.Optional<Long> offset_group, java.util.Optional<Long> group, java.util.Optional<long[]> kernel_shape) {\n@@ -6814,3 +6814,3 @@\n-        public java.util.Optional<int[]> pads() {\n-            int[] pads = Attribute.pads.access(int[].class, onnxAttributes);\n-            return java.util.Optional.ofNullable(pads).map(int[]::clone);\n+        public java.util.Optional<long[]> pads() {\n+            long[] pads = Attribute.pads.access(long[].class, onnxAttributes);\n+            return java.util.Optional.ofNullable(pads).map(long[]::clone);\n@@ -6819,3 +6819,3 @@\n-        public java.util.Optional<int[]> dilations() {\n-            int[] dilations = Attribute.dilations.access(int[].class, onnxAttributes);\n-            return java.util.Optional.ofNullable(dilations).map(int[]::clone);\n+        public java.util.Optional<long[]> dilations() {\n+            long[] dilations = Attribute.dilations.access(long[].class, onnxAttributes);\n+            return java.util.Optional.ofNullable(dilations).map(long[]::clone);\n@@ -6824,3 +6824,3 @@\n-        public java.util.Optional<int[]> strides() {\n-            int[] strides = Attribute.strides.access(int[].class, onnxAttributes);\n-            return java.util.Optional.ofNullable(strides).map(int[]::clone);\n+        public java.util.Optional<long[]> strides() {\n+            long[] strides = Attribute.strides.access(long[].class, onnxAttributes);\n+            return java.util.Optional.ofNullable(strides).map(long[]::clone);\n@@ -6829,2 +6829,2 @@\n-        public java.util.Optional<Integer> offset_group() {\n-            Integer offset_group = Attribute.offset_group.access(Integer.class, onnxAttributes);\n+        public java.util.Optional<Long> offset_group() {\n+            Long offset_group = Attribute.offset_group.access(Long.class, onnxAttributes);\n@@ -6834,2 +6834,2 @@\n-        public java.util.Optional<Integer> group() {\n-            Integer group = Attribute.group.access(Integer.class, onnxAttributes);\n+        public java.util.Optional<Long> group() {\n+            Long group = Attribute.group.access(Long.class, onnxAttributes);\n@@ -6839,3 +6839,3 @@\n-        public java.util.Optional<int[]> kernel_shape() {\n-            int[] kernel_shape = Attribute.kernel_shape.access(int[].class, onnxAttributes);\n-            return java.util.Optional.ofNullable(kernel_shape).map(int[]::clone);\n+        public java.util.Optional<long[]> kernel_shape() {\n+            long[] kernel_shape = Attribute.kernel_shape.access(long[].class, onnxAttributes);\n+            return java.util.Optional.ofNullable(kernel_shape).map(long[]::clone);\n@@ -6846,1 +6846,1 @@\n-    public static DeformConv DeformConv(TypeElement resultType, Value X, Value W, Value offset, java.util.Optional<Value> B, java.util.Optional<Value> mask, java.util.Optional<int[]> pads, java.util.Optional<int[]> dilations, java.util.Optional<int[]> strides, java.util.Optional<Integer> offset_group, java.util.Optional<Integer> group, java.util.Optional<int[]> kernel_shape) {\n+    public static DeformConv DeformConv(TypeElement resultType, Value X, Value W, Value offset, java.util.Optional<Value> B, java.util.Optional<Value> mask, java.util.Optional<long[]> pads, java.util.Optional<long[]> dilations, java.util.Optional<long[]> strides, java.util.Optional<Long> offset_group, java.util.Optional<Long> group, java.util.Optional<long[]> kernel_shape) {\n@@ -6856,1 +6856,1 @@\n-            blocksize(Integer.class, false, null),\n+            blocksize(Long.class, false, null),\n@@ -6967,1 +6967,1 @@\n-        DepthToSpace(TypeElement resultType, Value input, java.util.Optional<String> mode, int blocksize) {\n+        DepthToSpace(TypeElement resultType, Value input, java.util.Optional<String> mode, long blocksize) {\n@@ -6990,2 +6990,2 @@\n-        public int blocksize() {\n-            int blocksize = Attribute.blocksize.access(int.class, onnxAttributes);\n+        public long blocksize() {\n+            long blocksize = Attribute.blocksize.access(long.class, onnxAttributes);\n@@ -6997,1 +6997,1 @@\n-    public static DepthToSpace DepthToSpace(TypeElement resultType, Value input, java.util.Optional<String> mode, int blocksize) {\n+    public static DepthToSpace DepthToSpace(TypeElement resultType, Value input, java.util.Optional<String> mode, long blocksize) {\n@@ -7006,2 +7006,2 @@\n-            axis(Integer.class, true, 1),\n-            block_size(Integer.class, true, 0),\n+            axis(Long.class, true, 1),\n+            block_size(Long.class, true, 0),\n@@ -7121,1 +7121,1 @@\n-        DequantizeLinear(TypeElement resultType, Value x, Value x_scale, java.util.Optional<Value> x_zero_point, java.util.Optional<Integer> axis, java.util.Optional<Integer> block_size) {\n+        DequantizeLinear(TypeElement resultType, Value x, Value x_scale, java.util.Optional<Value> x_zero_point, java.util.Optional<Long> axis, java.util.Optional<Long> block_size) {\n@@ -7148,2 +7148,2 @@\n-        public java.util.Optional<Integer> axis() {\n-            Integer axis = Attribute.axis.access(Integer.class, onnxAttributes);\n+        public java.util.Optional<Long> axis() {\n+            Long axis = Attribute.axis.access(Long.class, onnxAttributes);\n@@ -7153,2 +7153,2 @@\n-        public java.util.Optional<Integer> block_size() {\n-            Integer block_size = Attribute.block_size.access(Integer.class, onnxAttributes);\n+        public java.util.Optional<Long> block_size() {\n+            Long block_size = Attribute.block_size.access(Long.class, onnxAttributes);\n@@ -7160,1 +7160,1 @@\n-    public static DequantizeLinear DequantizeLinear(TypeElement resultType, Value x, Value x_scale, java.util.Optional<Value> x_zero_point, java.util.Optional<Integer> axis, java.util.Optional<Integer> block_size) {\n+    public static DequantizeLinear DequantizeLinear(TypeElement resultType, Value x, Value x_scale, java.util.Optional<Value> x_zero_point, java.util.Optional<Long> axis, java.util.Optional<Long> block_size) {\n@@ -7284,1 +7284,1 @@\n-            int64_vocabulary(int[].class, true, null),\n+            int64_vocabulary(long[].class, true, null),\n@@ -7396,1 +7396,1 @@\n-        DictVectorizer(TypeElement resultType, Value X, java.util.Optional<String[]> string_vocabulary, java.util.Optional<int[]> int64_vocabulary) {\n+        DictVectorizer(TypeElement resultType, Value X, java.util.Optional<String[]> string_vocabulary, java.util.Optional<long[]> int64_vocabulary) {\n@@ -7419,3 +7419,3 @@\n-        public java.util.Optional<int[]> int64_vocabulary() {\n-            int[] int64_vocabulary = Attribute.int64_vocabulary.access(int[].class, onnxAttributes);\n-            return java.util.Optional.ofNullable(int64_vocabulary).map(int[]::clone);\n+        public java.util.Optional<long[]> int64_vocabulary() {\n+            long[] int64_vocabulary = Attribute.int64_vocabulary.access(long[].class, onnxAttributes);\n+            return java.util.Optional.ofNullable(int64_vocabulary).map(long[]::clone);\n@@ -7426,1 +7426,1 @@\n-    public static DictVectorizer DictVectorizer(TypeElement resultType, Value X, java.util.Optional<String[]> string_vocabulary, java.util.Optional<int[]> int64_vocabulary) {\n+    public static DictVectorizer DictVectorizer(TypeElement resultType, Value X, java.util.Optional<String[]> string_vocabulary, java.util.Optional<long[]> int64_vocabulary) {\n@@ -7554,1 +7554,1 @@\n-            seed(Integer.class, true, null),\n+            seed(Long.class, true, null),\n@@ -7670,1 +7670,1 @@\n-        Dropout(TypeElement resultType, Set<OutputParameter> optionalOutputs, Value data, java.util.Optional<Value> ratio, java.util.Optional<Value> training_mode, java.util.Optional<Integer> seed) {\n+        Dropout(TypeElement resultType, Set<OutputParameter> optionalOutputs, Value data, java.util.Optional<Value> ratio, java.util.Optional<Value> training_mode, java.util.Optional<Long> seed) {\n@@ -7698,2 +7698,2 @@\n-        public java.util.Optional<Integer> seed() {\n-            Integer seed = Attribute.seed.access(Integer.class, onnxAttributes);\n+        public java.util.Optional<Long> seed() {\n+            Long seed = Attribute.seed.access(Long.class, onnxAttributes);\n@@ -7705,1 +7705,1 @@\n-    public static Dropout Dropout(TypeElement resultType, Set<Dropout.OutputParameter> optionalOutputs, Value data, java.util.Optional<Value> ratio, java.util.Optional<Value> training_mode, java.util.Optional<Integer> seed) {\n+    public static Dropout Dropout(TypeElement resultType, Set<Dropout.OutputParameter> optionalOutputs, Value data, java.util.Optional<Value> ratio, java.util.Optional<Value> training_mode, java.util.Optional<Long> seed) {\n@@ -8588,2 +8588,2 @@\n-            dtype(Integer.class, true, null),\n-            k(Integer.class, true, 0),\n+            dtype(Long.class, true, null),\n+            k(Long.class, true, 0),\n@@ -8701,1 +8701,1 @@\n-        EyeLike(TypeElement resultType, Value input, java.util.Optional<Integer> dtype, java.util.Optional<Integer> k) {\n+        EyeLike(TypeElement resultType, Value input, java.util.Optional<Long> dtype, java.util.Optional<Long> k) {\n@@ -8719,2 +8719,2 @@\n-        public java.util.Optional<Integer> dtype() {\n-            Integer dtype = Attribute.dtype.access(Integer.class, onnxAttributes);\n+        public java.util.Optional<Long> dtype() {\n+            Long dtype = Attribute.dtype.access(Long.class, onnxAttributes);\n@@ -8724,2 +8724,2 @@\n-        public java.util.Optional<Integer> k() {\n-            Integer k = Attribute.k.access(Integer.class, onnxAttributes);\n+        public java.util.Optional<Long> k() {\n+            Long k = Attribute.k.access(Long.class, onnxAttributes);\n@@ -8731,1 +8731,1 @@\n-    public static EyeLike EyeLike(TypeElement resultType, Value input, java.util.Optional<Integer> dtype, java.util.Optional<Integer> k) {\n+    public static EyeLike EyeLike(TypeElement resultType, Value input, java.util.Optional<Long> dtype, java.util.Optional<Long> k) {\n@@ -8740,1 +8740,1 @@\n-            inputdimensions(int[].class, true, null),\n+            inputdimensions(long[].class, true, null),\n@@ -8851,1 +8851,1 @@\n-        FeatureVectorizer(TypeElement resultType, List<Value> X, java.util.Optional<int[]> inputdimensions) {\n+        FeatureVectorizer(TypeElement resultType, List<Value> X, java.util.Optional<long[]> inputdimensions) {\n@@ -8869,3 +8869,3 @@\n-        public java.util.Optional<int[]> inputdimensions() {\n-            int[] inputdimensions = Attribute.inputdimensions.access(int[].class, onnxAttributes);\n-            return java.util.Optional.ofNullable(inputdimensions).map(int[]::clone);\n+        public java.util.Optional<long[]> inputdimensions() {\n+            long[] inputdimensions = Attribute.inputdimensions.access(long[].class, onnxAttributes);\n+            return java.util.Optional.ofNullable(inputdimensions).map(long[]::clone);\n@@ -8876,1 +8876,1 @@\n-    public static FeatureVectorizer FeatureVectorizer(TypeElement resultType, List<Value> X, java.util.Optional<int[]> inputdimensions) {\n+    public static FeatureVectorizer FeatureVectorizer(TypeElement resultType, List<Value> X, java.util.Optional<long[]> inputdimensions) {\n@@ -8885,1 +8885,1 @@\n-            axis(Integer.class, true, 1),\n+            axis(Long.class, true, 1),\n@@ -8996,1 +8996,1 @@\n-        Flatten(TypeElement resultType, Value input, java.util.Optional<Integer> axis) {\n+        Flatten(TypeElement resultType, Value input, java.util.Optional<Long> axis) {\n@@ -9014,2 +9014,2 @@\n-        public java.util.Optional<Integer> axis() {\n-            Integer axis = Attribute.axis.access(Integer.class, onnxAttributes);\n+        public java.util.Optional<Long> axis() {\n+            Long axis = Attribute.axis.access(Long.class, onnxAttributes);\n@@ -9021,1 +9021,1 @@\n-    public static Flatten Flatten(TypeElement resultType, Value input, java.util.Optional<Integer> axis) {\n+    public static Flatten Flatten(TypeElement resultType, Value input, java.util.Optional<Long> axis) {\n@@ -9144,1 +9144,1 @@\n-            layout(Integer.class, true, 0),\n+            layout(Long.class, true, 0),\n@@ -9146,1 +9146,1 @@\n-            hidden_size(Integer.class, true, null),\n+            hidden_size(Long.class, true, null),\n@@ -9149,1 +9149,1 @@\n-            linear_before_reset(Integer.class, true, 0),\n+            linear_before_reset(Long.class, true, 0),\n@@ -9269,1 +9269,1 @@\n-        GRU(TypeElement resultType, Set<OutputParameter> optionalOutputs, Value X, Value W, Value R, java.util.Optional<Value> B, java.util.Optional<Value> sequence_lens, java.util.Optional<Value> initial_h, java.util.Optional<Integer> layout, java.util.Optional<float[]> activation_alpha, java.util.Optional<Integer> hidden_size, java.util.Optional<float[]> activation_beta, java.util.Optional<String[]> activations, java.util.Optional<Integer> linear_before_reset, java.util.Optional<Float> clip, java.util.Optional<String> direction) {\n+        GRU(TypeElement resultType, Set<OutputParameter> optionalOutputs, Value X, Value W, Value R, java.util.Optional<Value> B, java.util.Optional<Value> sequence_lens, java.util.Optional<Value> initial_h, java.util.Optional<Long> layout, java.util.Optional<float[]> activation_alpha, java.util.Optional<Long> hidden_size, java.util.Optional<float[]> activation_beta, java.util.Optional<String[]> activations, java.util.Optional<Long> linear_before_reset, java.util.Optional<Float> clip, java.util.Optional<String> direction) {\n@@ -9310,2 +9310,2 @@\n-        public java.util.Optional<Integer> layout() {\n-            Integer layout = Attribute.layout.access(Integer.class, onnxAttributes);\n+        public java.util.Optional<Long> layout() {\n+            Long layout = Attribute.layout.access(Long.class, onnxAttributes);\n@@ -9320,2 +9320,2 @@\n-        public java.util.Optional<Integer> hidden_size() {\n-            Integer hidden_size = Attribute.hidden_size.access(Integer.class, onnxAttributes);\n+        public java.util.Optional<Long> hidden_size() {\n+            Long hidden_size = Attribute.hidden_size.access(Long.class, onnxAttributes);\n@@ -9335,2 +9335,2 @@\n-        public java.util.Optional<Integer> linear_before_reset() {\n-            Integer linear_before_reset = Attribute.linear_before_reset.access(Integer.class, onnxAttributes);\n+        public java.util.Optional<Long> linear_before_reset() {\n+            Long linear_before_reset = Attribute.linear_before_reset.access(Long.class, onnxAttributes);\n@@ -9352,1 +9352,1 @@\n-    public static GRU GRU(TypeElement resultType, Set<GRU.OutputParameter> optionalOutputs, Value X, Value W, Value R, java.util.Optional<Value> B, java.util.Optional<Value> sequence_lens, java.util.Optional<Value> initial_h, java.util.Optional<Integer> layout, java.util.Optional<float[]> activation_alpha, java.util.Optional<Integer> hidden_size, java.util.Optional<float[]> activation_beta, java.util.Optional<String[]> activations, java.util.Optional<Integer> linear_before_reset, java.util.Optional<Float> clip, java.util.Optional<String> direction) {\n+    public static GRU GRU(TypeElement resultType, Set<GRU.OutputParameter> optionalOutputs, Value X, Value W, Value R, java.util.Optional<Value> B, java.util.Optional<Value> sequence_lens, java.util.Optional<Value> initial_h, java.util.Optional<Long> layout, java.util.Optional<float[]> activation_alpha, java.util.Optional<Long> hidden_size, java.util.Optional<float[]> activation_beta, java.util.Optional<String[]> activations, java.util.Optional<Long> linear_before_reset, java.util.Optional<Float> clip, java.util.Optional<String> direction) {\n@@ -9361,1 +9361,1 @@\n-            axis(Integer.class, true, 0),\n+            axis(Long.class, true, 0),\n@@ -9474,1 +9474,1 @@\n-        Gather(TypeElement resultType, Value data, Value indices, java.util.Optional<Integer> axis) {\n+        Gather(TypeElement resultType, Value data, Value indices, java.util.Optional<Long> axis) {\n@@ -9496,2 +9496,2 @@\n-        public java.util.Optional<Integer> axis() {\n-            Integer axis = Attribute.axis.access(Integer.class, onnxAttributes);\n+        public java.util.Optional<Long> axis() {\n+            Long axis = Attribute.axis.access(Long.class, onnxAttributes);\n@@ -9503,1 +9503,1 @@\n-    public static Gather Gather(TypeElement resultType, Value data, Value indices, java.util.Optional<Integer> axis) {\n+    public static Gather Gather(TypeElement resultType, Value data, Value indices, java.util.Optional<Long> axis) {\n@@ -9512,1 +9512,1 @@\n-            axis(Integer.class, true, 0),\n+            axis(Long.class, true, 0),\n@@ -9625,1 +9625,1 @@\n-        GatherElements(TypeElement resultType, Value data, Value indices, java.util.Optional<Integer> axis) {\n+        GatherElements(TypeElement resultType, Value data, Value indices, java.util.Optional<Long> axis) {\n@@ -9647,2 +9647,2 @@\n-        public java.util.Optional<Integer> axis() {\n-            Integer axis = Attribute.axis.access(Integer.class, onnxAttributes);\n+        public java.util.Optional<Long> axis() {\n+            Long axis = Attribute.axis.access(Long.class, onnxAttributes);\n@@ -9654,1 +9654,1 @@\n-    public static GatherElements GatherElements(TypeElement resultType, Value data, Value indices, java.util.Optional<Integer> axis) {\n+    public static GatherElements GatherElements(TypeElement resultType, Value data, Value indices, java.util.Optional<Long> axis) {\n@@ -9663,1 +9663,1 @@\n-            batch_dims(Integer.class, true, 0),\n+            batch_dims(Long.class, true, 0),\n@@ -9775,1 +9775,1 @@\n-        GatherND(TypeElement resultType, Value data, Value indices, java.util.Optional<Integer> batch_dims) {\n+        GatherND(TypeElement resultType, Value data, Value indices, java.util.Optional<Long> batch_dims) {\n@@ -9797,2 +9797,2 @@\n-        public java.util.Optional<Integer> batch_dims() {\n-            Integer batch_dims = Attribute.batch_dims.access(Integer.class, onnxAttributes);\n+        public java.util.Optional<Long> batch_dims() {\n+            Long batch_dims = Attribute.batch_dims.access(Long.class, onnxAttributes);\n@@ -9804,1 +9804,1 @@\n-    public static GatherND GatherND(TypeElement resultType, Value data, Value indices, java.util.Optional<Integer> batch_dims) {\n+    public static GatherND GatherND(TypeElement resultType, Value data, Value indices, java.util.Optional<Long> batch_dims) {\n@@ -9959,1 +9959,1 @@\n-            transB(Integer.class, true, 0),\n+            transB(Long.class, true, 0),\n@@ -9961,1 +9961,1 @@\n-            transA(Integer.class, true, 0),\n+            transA(Long.class, true, 0),\n@@ -10074,1 +10074,1 @@\n-        Gemm(TypeElement resultType, Value A, Value B, java.util.Optional<Value> C, java.util.Optional<Float> alpha, java.util.Optional<Integer> transB, java.util.Optional<Float> beta, java.util.Optional<Integer> transA) {\n+        Gemm(TypeElement resultType, Value A, Value B, java.util.Optional<Value> C, java.util.Optional<Float> alpha, java.util.Optional<Long> transB, java.util.Optional<Float> beta, java.util.Optional<Long> transA) {\n@@ -10106,2 +10106,2 @@\n-        public java.util.Optional<Integer> transB() {\n-            Integer transB = Attribute.transB.access(Integer.class, onnxAttributes);\n+        public java.util.Optional<Long> transB() {\n+            Long transB = Attribute.transB.access(Long.class, onnxAttributes);\n@@ -10116,2 +10116,2 @@\n-        public java.util.Optional<Integer> transA() {\n-            Integer transA = Attribute.transA.access(Integer.class, onnxAttributes);\n+        public java.util.Optional<Long> transA() {\n+            Long transA = Attribute.transA.access(Long.class, onnxAttributes);\n@@ -10123,1 +10123,1 @@\n-    public static Gemm Gemm(TypeElement resultType, Value A, Value B, java.util.Optional<Value> C, java.util.Optional<Float> alpha, java.util.Optional<Integer> transB, java.util.Optional<Float> beta, java.util.Optional<Integer> transA) {\n+    public static Gemm Gemm(TypeElement resultType, Value A, Value B, java.util.Optional<Value> C, java.util.Optional<Float> alpha, java.util.Optional<Long> transB, java.util.Optional<Float> beta, java.util.Optional<Long> transA) {\n@@ -10246,1 +10246,1 @@\n-            p(Integer.class, true, 2),\n+            p(Long.class, true, 2),\n@@ -10357,1 +10357,1 @@\n-        GlobalLpPool(TypeElement resultType, Value X, java.util.Optional<Integer> p) {\n+        GlobalLpPool(TypeElement resultType, Value X, java.util.Optional<Long> p) {\n@@ -10375,2 +10375,2 @@\n-        public java.util.Optional<Integer> p() {\n-            Integer p = Attribute.p.access(Integer.class, onnxAttributes);\n+        public java.util.Optional<Long> p() {\n+            Long p = Attribute.p.access(Long.class, onnxAttributes);\n@@ -10382,1 +10382,1 @@\n-    public static GlobalLpPool GlobalLpPool(TypeElement resultType, Value X, java.util.Optional<Integer> p) {\n+    public static GlobalLpPool GlobalLpPool(TypeElement resultType, Value X, java.util.Optional<Long> p) {\n@@ -10904,1 +10904,1 @@\n-            align_corners(Integer.class, true, 0),\n+            align_corners(Long.class, true, 0),\n@@ -11018,1 +11018,1 @@\n-        GridSample(TypeElement resultType, Value X, Value grid, java.util.Optional<String> mode, java.util.Optional<Integer> align_corners, java.util.Optional<String> padding_mode) {\n+        GridSample(TypeElement resultType, Value X, Value grid, java.util.Optional<String> mode, java.util.Optional<Long> align_corners, java.util.Optional<String> padding_mode) {\n@@ -11045,2 +11045,2 @@\n-        public java.util.Optional<Integer> align_corners() {\n-            Integer align_corners = Attribute.align_corners.access(Integer.class, onnxAttributes);\n+        public java.util.Optional<Long> align_corners() {\n+            Long align_corners = Attribute.align_corners.access(Long.class, onnxAttributes);\n@@ -11057,1 +11057,1 @@\n-    public static GridSample GridSample(TypeElement resultType, Value X, Value grid, java.util.Optional<String> mode, java.util.Optional<Integer> align_corners, java.util.Optional<String> padding_mode) {\n+    public static GridSample GridSample(TypeElement resultType, Value X, Value grid, java.util.Optional<String> mode, java.util.Optional<Long> align_corners, java.util.Optional<String> padding_mode) {\n@@ -11067,2 +11067,2 @@\n-            stash_type(Integer.class, true, 1),\n-            num_groups(Integer.class, false, null),\n+            stash_type(Long.class, true, 1),\n+            num_groups(Long.class, false, null),\n@@ -11181,1 +11181,1 @@\n-        GroupNormalization(TypeElement resultType, Value X, Value scale, Value bias, java.util.Optional<Float> epsilon, java.util.Optional<Integer> stash_type, int num_groups) {\n+        GroupNormalization(TypeElement resultType, Value X, Value scale, Value bias, java.util.Optional<Float> epsilon, java.util.Optional<Long> stash_type, long num_groups) {\n@@ -11212,2 +11212,2 @@\n-        public java.util.Optional<Integer> stash_type() {\n-            Integer stash_type = Attribute.stash_type.access(Integer.class, onnxAttributes);\n+        public java.util.Optional<Long> stash_type() {\n+            Long stash_type = Attribute.stash_type.access(Long.class, onnxAttributes);\n@@ -11217,2 +11217,2 @@\n-        public int num_groups() {\n-            int num_groups = Attribute.num_groups.access(int.class, onnxAttributes);\n+        public long num_groups() {\n+            long num_groups = Attribute.num_groups.access(long.class, onnxAttributes);\n@@ -11224,1 +11224,1 @@\n-    public static GroupNormalization GroupNormalization(TypeElement resultType, Value X, Value scale, Value bias, java.util.Optional<Float> epsilon, java.util.Optional<Integer> stash_type, int num_groups) {\n+    public static GroupNormalization GroupNormalization(TypeElement resultType, Value X, Value scale, Value bias, java.util.Optional<Float> epsilon, java.util.Optional<Long> stash_type, long num_groups) {\n@@ -11233,2 +11233,2 @@\n-            periodic(Integer.class, true, 1),\n-            output_datatype(Integer.class, true, 1),\n+            periodic(Long.class, true, 1),\n+            output_datatype(Long.class, true, 1),\n@@ -11346,1 +11346,1 @@\n-        HammingWindow(TypeElement resultType, Value size, java.util.Optional<Integer> periodic, java.util.Optional<Integer> output_datatype) {\n+        HammingWindow(TypeElement resultType, Value size, java.util.Optional<Long> periodic, java.util.Optional<Long> output_datatype) {\n@@ -11364,2 +11364,2 @@\n-        public java.util.Optional<Integer> periodic() {\n-            Integer periodic = Attribute.periodic.access(Integer.class, onnxAttributes);\n+        public java.util.Optional<Long> periodic() {\n+            Long periodic = Attribute.periodic.access(Long.class, onnxAttributes);\n@@ -11369,2 +11369,2 @@\n-        public java.util.Optional<Integer> output_datatype() {\n-            Integer output_datatype = Attribute.output_datatype.access(Integer.class, onnxAttributes);\n+        public java.util.Optional<Long> output_datatype() {\n+            Long output_datatype = Attribute.output_datatype.access(Long.class, onnxAttributes);\n@@ -11376,1 +11376,1 @@\n-    public static HammingWindow HammingWindow(TypeElement resultType, Value size, java.util.Optional<Integer> periodic, java.util.Optional<Integer> output_datatype) {\n+    public static HammingWindow HammingWindow(TypeElement resultType, Value size, java.util.Optional<Long> periodic, java.util.Optional<Long> output_datatype) {\n@@ -11385,2 +11385,2 @@\n-            periodic(Integer.class, true, 1),\n-            output_datatype(Integer.class, true, 1),\n+            periodic(Long.class, true, 1),\n+            output_datatype(Long.class, true, 1),\n@@ -11498,1 +11498,1 @@\n-        HannWindow(TypeElement resultType, Value size, java.util.Optional<Integer> periodic, java.util.Optional<Integer> output_datatype) {\n+        HannWindow(TypeElement resultType, Value size, java.util.Optional<Long> periodic, java.util.Optional<Long> output_datatype) {\n@@ -11516,2 +11516,2 @@\n-        public java.util.Optional<Integer> periodic() {\n-            Integer periodic = Attribute.periodic.access(Integer.class, onnxAttributes);\n+        public java.util.Optional<Long> periodic() {\n+            Long periodic = Attribute.periodic.access(Long.class, onnxAttributes);\n@@ -11521,2 +11521,2 @@\n-        public java.util.Optional<Integer> output_datatype() {\n-            Integer output_datatype = Attribute.output_datatype.access(Integer.class, onnxAttributes);\n+        public java.util.Optional<Long> output_datatype() {\n+            Long output_datatype = Attribute.output_datatype.access(Long.class, onnxAttributes);\n@@ -11528,1 +11528,1 @@\n-    public static HannWindow HannWindow(TypeElement resultType, Value size, java.util.Optional<Integer> periodic, java.util.Optional<Integer> output_datatype) {\n+    public static HannWindow HannWindow(TypeElement resultType, Value size, java.util.Optional<Long> periodic, java.util.Optional<Long> output_datatype) {\n@@ -11802,1 +11802,1 @@\n-            axis(Integer.class, true, -1),\n+            axis(Long.class, true, -1),\n@@ -11913,1 +11913,1 @@\n-        Hardmax(TypeElement resultType, Value input, java.util.Optional<Integer> axis) {\n+        Hardmax(TypeElement resultType, Value input, java.util.Optional<Long> axis) {\n@@ -11931,2 +11931,2 @@\n-        public java.util.Optional<Integer> axis() {\n-            Integer axis = Attribute.axis.access(Integer.class, onnxAttributes);\n+        public java.util.Optional<Long> axis() {\n+            Long axis = Attribute.axis.access(Long.class, onnxAttributes);\n@@ -11938,1 +11938,1 @@\n-    public static Hardmax Hardmax(TypeElement resultType, Value input, java.util.Optional<Integer> axis) {\n+    public static Hardmax Hardmax(TypeElement resultType, Value input, java.util.Optional<Long> axis) {\n@@ -12207,1 +12207,1 @@\n-            replaced_value_int64(Integer.class, true, 0),\n+            replaced_value_int64(Long.class, true, 0),\n@@ -12209,1 +12209,1 @@\n-            imputed_value_int64s(int[].class, true, null),\n+            imputed_value_int64s(long[].class, true, null),\n@@ -12321,1 +12321,1 @@\n-        Imputer(TypeElement resultType, Value X, java.util.Optional<Integer> replaced_value_int64, java.util.Optional<Float> replaced_value_float, java.util.Optional<int[]> imputed_value_int64s, java.util.Optional<float[]> imputed_value_floats) {\n+        Imputer(TypeElement resultType, Value X, java.util.Optional<Long> replaced_value_int64, java.util.Optional<Float> replaced_value_float, java.util.Optional<long[]> imputed_value_int64s, java.util.Optional<float[]> imputed_value_floats) {\n@@ -12339,2 +12339,2 @@\n-        public java.util.Optional<Integer> replaced_value_int64() {\n-            Integer replaced_value_int64 = Attribute.replaced_value_int64.access(Integer.class, onnxAttributes);\n+        public java.util.Optional<Long> replaced_value_int64() {\n+            Long replaced_value_int64 = Attribute.replaced_value_int64.access(Long.class, onnxAttributes);\n@@ -12349,3 +12349,3 @@\n-        public java.util.Optional<int[]> imputed_value_int64s() {\n-            int[] imputed_value_int64s = Attribute.imputed_value_int64s.access(int[].class, onnxAttributes);\n-            return java.util.Optional.ofNullable(imputed_value_int64s).map(int[]::clone);\n+        public java.util.Optional<long[]> imputed_value_int64s() {\n+            long[] imputed_value_int64s = Attribute.imputed_value_int64s.access(long[].class, onnxAttributes);\n+            return java.util.Optional.ofNullable(imputed_value_int64s).map(long[]::clone);\n@@ -12361,1 +12361,1 @@\n-    public static Imputer Imputer(TypeElement resultType, Value X, java.util.Optional<Integer> replaced_value_int64, java.util.Optional<Float> replaced_value_float, java.util.Optional<int[]> imputed_value_int64s, java.util.Optional<float[]> imputed_value_floats) {\n+    public static Imputer Imputer(TypeElement resultType, Value X, java.util.Optional<Long> replaced_value_int64, java.util.Optional<Float> replaced_value_float, java.util.Optional<long[]> imputed_value_int64s, java.util.Optional<float[]> imputed_value_floats) {\n@@ -12525,2 +12525,2 @@\n-            detect_negative(Integer.class, true, 1),\n-            detect_positive(Integer.class, true, 1),\n+            detect_negative(Long.class, true, 1),\n+            detect_positive(Long.class, true, 1),\n@@ -12638,1 +12638,1 @@\n-        IsInf(TypeElement resultType, Value X, java.util.Optional<Integer> detect_negative, java.util.Optional<Integer> detect_positive) {\n+        IsInf(TypeElement resultType, Value X, java.util.Optional<Long> detect_negative, java.util.Optional<Long> detect_positive) {\n@@ -12656,2 +12656,2 @@\n-        public java.util.Optional<Integer> detect_negative() {\n-            Integer detect_negative = Attribute.detect_negative.access(Integer.class, onnxAttributes);\n+        public java.util.Optional<Long> detect_negative() {\n+            Long detect_negative = Attribute.detect_negative.access(Long.class, onnxAttributes);\n@@ -12661,2 +12661,2 @@\n-        public java.util.Optional<Integer> detect_positive() {\n-            Integer detect_positive = Attribute.detect_positive.access(Integer.class, onnxAttributes);\n+        public java.util.Optional<Long> detect_positive() {\n+            Long detect_positive = Attribute.detect_positive.access(Long.class, onnxAttributes);\n@@ -12668,1 +12668,1 @@\n-    public static IsInf IsInf(TypeElement resultType, Value X, java.util.Optional<Integer> detect_negative, java.util.Optional<Integer> detect_positive) {\n+    public static IsInf IsInf(TypeElement resultType, Value X, java.util.Optional<Long> detect_negative, java.util.Optional<Long> detect_positive) {\n@@ -12792,1 +12792,1 @@\n-            size(Integer.class, false, null),\n+            size(Long.class, false, null),\n@@ -12906,1 +12906,1 @@\n-        LRN(TypeElement resultType, Value X, int size, java.util.Optional<Float> alpha, java.util.Optional<Float> bias, java.util.Optional<Float> beta) {\n+        LRN(TypeElement resultType, Value X, long size, java.util.Optional<Float> alpha, java.util.Optional<Float> bias, java.util.Optional<Float> beta) {\n@@ -12924,2 +12924,2 @@\n-        public int size() {\n-            int size = Attribute.size.access(int.class, onnxAttributes);\n+        public long size() {\n+            long size = Attribute.size.access(long.class, onnxAttributes);\n@@ -12946,1 +12946,1 @@\n-    public static LRN LRN(TypeElement resultType, Value X, int size, java.util.Optional<Float> alpha, java.util.Optional<Float> bias, java.util.Optional<Float> beta) {\n+    public static LRN LRN(TypeElement resultType, Value X, long size, java.util.Optional<Float> alpha, java.util.Optional<Float> bias, java.util.Optional<Float> beta) {\n@@ -12955,2 +12955,2 @@\n-            layout(Integer.class, true, 0),\n-            input_forget(Integer.class, true, 0),\n+            layout(Long.class, true, 0),\n+            input_forget(Long.class, true, 0),\n@@ -12958,1 +12958,1 @@\n-            hidden_size(Integer.class, true, null),\n+            hidden_size(Long.class, true, null),\n@@ -13083,1 +13083,1 @@\n-        LSTM(TypeElement resultType, Set<OutputParameter> optionalOutputs, Value X, Value W, Value R, java.util.Optional<Value> B, java.util.Optional<Value> sequence_lens, java.util.Optional<Value> initial_h, java.util.Optional<Value> initial_c, java.util.Optional<Value> P, java.util.Optional<Integer> layout, java.util.Optional<Integer> input_forget, java.util.Optional<float[]> activation_alpha, java.util.Optional<Integer> hidden_size, java.util.Optional<float[]> activation_beta, java.util.Optional<String[]> activations, java.util.Optional<Float> clip, java.util.Optional<String> direction) {\n+        LSTM(TypeElement resultType, Set<OutputParameter> optionalOutputs, Value X, Value W, Value R, java.util.Optional<Value> B, java.util.Optional<Value> sequence_lens, java.util.Optional<Value> initial_h, java.util.Optional<Value> initial_c, java.util.Optional<Value> P, java.util.Optional<Long> layout, java.util.Optional<Long> input_forget, java.util.Optional<float[]> activation_alpha, java.util.Optional<Long> hidden_size, java.util.Optional<float[]> activation_beta, java.util.Optional<String[]> activations, java.util.Optional<Float> clip, java.util.Optional<String> direction) {\n@@ -13134,2 +13134,2 @@\n-        public java.util.Optional<Integer> layout() {\n-            Integer layout = Attribute.layout.access(Integer.class, onnxAttributes);\n+        public java.util.Optional<Long> layout() {\n+            Long layout = Attribute.layout.access(Long.class, onnxAttributes);\n@@ -13139,2 +13139,2 @@\n-        public java.util.Optional<Integer> input_forget() {\n-            Integer input_forget = Attribute.input_forget.access(Integer.class, onnxAttributes);\n+        public java.util.Optional<Long> input_forget() {\n+            Long input_forget = Attribute.input_forget.access(Long.class, onnxAttributes);\n@@ -13149,2 +13149,2 @@\n-        public java.util.Optional<Integer> hidden_size() {\n-            Integer hidden_size = Attribute.hidden_size.access(Integer.class, onnxAttributes);\n+        public java.util.Optional<Long> hidden_size() {\n+            Long hidden_size = Attribute.hidden_size.access(Long.class, onnxAttributes);\n@@ -13176,1 +13176,1 @@\n-    public static LSTM LSTM(TypeElement resultType, Set<LSTM.OutputParameter> optionalOutputs, Value X, Value W, Value R, java.util.Optional<Value> B, java.util.Optional<Value> sequence_lens, java.util.Optional<Value> initial_h, java.util.Optional<Value> initial_c, java.util.Optional<Value> P, java.util.Optional<Integer> layout, java.util.Optional<Integer> input_forget, java.util.Optional<float[]> activation_alpha, java.util.Optional<Integer> hidden_size, java.util.Optional<float[]> activation_beta, java.util.Optional<String[]> activations, java.util.Optional<Float> clip, java.util.Optional<String> direction) {\n+    public static LSTM LSTM(TypeElement resultType, Set<LSTM.OutputParameter> optionalOutputs, Value X, Value W, Value R, java.util.Optional<Value> B, java.util.Optional<Value> sequence_lens, java.util.Optional<Value> initial_h, java.util.Optional<Value> initial_c, java.util.Optional<Value> P, java.util.Optional<Long> layout, java.util.Optional<Long> input_forget, java.util.Optional<float[]> activation_alpha, java.util.Optional<Long> hidden_size, java.util.Optional<float[]> activation_beta, java.util.Optional<String[]> activations, java.util.Optional<Float> clip, java.util.Optional<String> direction) {\n@@ -13186,1 +13186,1 @@\n-            keys_int64s(int[].class, true, null),\n+            keys_int64s(long[].class, true, null),\n@@ -13192,1 +13192,1 @@\n-            default_int64(Integer.class, true, -1),\n+            default_int64(Long.class, true, -1),\n@@ -13194,1 +13194,1 @@\n-            values_int64s(int[].class, true, null),\n+            values_int64s(long[].class, true, null),\n@@ -13308,1 +13308,1 @@\n-        LabelEncoder(TypeElement resultType, Value X, java.util.Optional<String[]> values_strings, java.util.Optional<int[]> keys_int64s, java.util.Optional<byte[]> keys_tensor, java.util.Optional<String[]> keys_strings, java.util.Optional<Float> default_float, java.util.Optional<float[]> keys_floats, java.util.Optional<byte[]> default_tensor, java.util.Optional<Integer> default_int64, java.util.Optional<byte[]> values_tensor, java.util.Optional<int[]> values_int64s, java.util.Optional<String> default_string, java.util.Optional<float[]> values_floats) {\n+        LabelEncoder(TypeElement resultType, Value X, java.util.Optional<String[]> values_strings, java.util.Optional<long[]> keys_int64s, java.util.Optional<byte[]> keys_tensor, java.util.Optional<String[]> keys_strings, java.util.Optional<Float> default_float, java.util.Optional<float[]> keys_floats, java.util.Optional<byte[]> default_tensor, java.util.Optional<Long> default_int64, java.util.Optional<byte[]> values_tensor, java.util.Optional<long[]> values_int64s, java.util.Optional<String> default_string, java.util.Optional<float[]> values_floats) {\n@@ -13331,3 +13331,3 @@\n-        public java.util.Optional<int[]> keys_int64s() {\n-            int[] keys_int64s = Attribute.keys_int64s.access(int[].class, onnxAttributes);\n-            return java.util.Optional.ofNullable(keys_int64s).map(int[]::clone);\n+        public java.util.Optional<long[]> keys_int64s() {\n+            long[] keys_int64s = Attribute.keys_int64s.access(long[].class, onnxAttributes);\n+            return java.util.Optional.ofNullable(keys_int64s).map(long[]::clone);\n@@ -13361,2 +13361,2 @@\n-        public java.util.Optional<Integer> default_int64() {\n-            Integer default_int64 = Attribute.default_int64.access(Integer.class, onnxAttributes);\n+        public java.util.Optional<Long> default_int64() {\n+            Long default_int64 = Attribute.default_int64.access(Long.class, onnxAttributes);\n@@ -13371,3 +13371,3 @@\n-        public java.util.Optional<int[]> values_int64s() {\n-            int[] values_int64s = Attribute.values_int64s.access(int[].class, onnxAttributes);\n-            return java.util.Optional.ofNullable(values_int64s).map(int[]::clone);\n+        public java.util.Optional<long[]> values_int64s() {\n+            long[] values_int64s = Attribute.values_int64s.access(long[].class, onnxAttributes);\n+            return java.util.Optional.ofNullable(values_int64s).map(long[]::clone);\n@@ -13388,1 +13388,1 @@\n-    public static LabelEncoder LabelEncoder(TypeElement resultType, Value X, java.util.Optional<String[]> values_strings, java.util.Optional<int[]> keys_int64s, java.util.Optional<byte[]> keys_tensor, java.util.Optional<String[]> keys_strings, java.util.Optional<Float> default_float, java.util.Optional<float[]> keys_floats, java.util.Optional<byte[]> default_tensor, java.util.Optional<Integer> default_int64, java.util.Optional<byte[]> values_tensor, java.util.Optional<int[]> values_int64s, java.util.Optional<String> default_string, java.util.Optional<float[]> values_floats) {\n+    public static LabelEncoder LabelEncoder(TypeElement resultType, Value X, java.util.Optional<String[]> values_strings, java.util.Optional<long[]> keys_int64s, java.util.Optional<byte[]> keys_tensor, java.util.Optional<String[]> keys_strings, java.util.Optional<Float> default_float, java.util.Optional<float[]> keys_floats, java.util.Optional<byte[]> default_tensor, java.util.Optional<Long> default_int64, java.util.Optional<byte[]> values_tensor, java.util.Optional<long[]> values_int64s, java.util.Optional<String> default_string, java.util.Optional<float[]> values_floats) {\n@@ -13398,2 +13398,2 @@\n-            stash_type(Integer.class, true, 1),\n-            axis(Integer.class, true, -1),\n+            stash_type(Long.class, true, 1),\n+            axis(Long.class, true, -1),\n@@ -13515,1 +13515,1 @@\n-        LayerNormalization(TypeElement resultType, Set<OutputParameter> optionalOutputs, Value X, Value Scale, java.util.Optional<Value> B, java.util.Optional<Float> epsilon, java.util.Optional<Integer> stash_type, java.util.Optional<Integer> axis) {\n+        LayerNormalization(TypeElement resultType, Set<OutputParameter> optionalOutputs, Value X, Value Scale, java.util.Optional<Value> B, java.util.Optional<Float> epsilon, java.util.Optional<Long> stash_type, java.util.Optional<Long> axis) {\n@@ -13547,2 +13547,2 @@\n-        public java.util.Optional<Integer> stash_type() {\n-            Integer stash_type = Attribute.stash_type.access(Integer.class, onnxAttributes);\n+        public java.util.Optional<Long> stash_type() {\n+            Long stash_type = Attribute.stash_type.access(Long.class, onnxAttributes);\n@@ -13552,2 +13552,2 @@\n-        public java.util.Optional<Integer> axis() {\n-            Integer axis = Attribute.axis.access(Integer.class, onnxAttributes);\n+        public java.util.Optional<Long> axis() {\n+            Long axis = Attribute.axis.access(Long.class, onnxAttributes);\n@@ -13559,1 +13559,1 @@\n-    public static LayerNormalization LayerNormalization(TypeElement resultType, Set<LayerNormalization.OutputParameter> optionalOutputs, Value X, Value Scale, java.util.Optional<Value> B, java.util.Optional<Float> epsilon, java.util.Optional<Integer> stash_type, java.util.Optional<Integer> axis) {\n+    public static LayerNormalization LayerNormalization(TypeElement resultType, Set<LayerNormalization.OutputParameter> optionalOutputs, Value X, Value Scale, java.util.Optional<Value> B, java.util.Optional<Float> epsilon, java.util.Optional<Long> stash_type, java.util.Optional<Long> axis) {\n@@ -13953,1 +13953,1 @@\n-            classlabels_ints(int[].class, true, null),\n+            classlabels_ints(long[].class, true, null),\n@@ -13956,1 +13956,1 @@\n-            multi_class(Integer.class, true, 0),\n+            multi_class(Long.class, true, 0),\n@@ -14071,1 +14071,1 @@\n-        LinearClassifier(TypeElement resultType, Value X, java.util.Optional<int[]> classlabels_ints, java.util.Optional<String> post_transform, float[] coefficients, java.util.Optional<Integer> multi_class, java.util.Optional<float[]> intercepts, java.util.Optional<String[]> classlabels_strings) {\n+        LinearClassifier(TypeElement resultType, Value X, java.util.Optional<long[]> classlabels_ints, java.util.Optional<String> post_transform, float[] coefficients, java.util.Optional<Long> multi_class, java.util.Optional<float[]> intercepts, java.util.Optional<String[]> classlabels_strings) {\n@@ -14089,3 +14089,3 @@\n-        public java.util.Optional<int[]> classlabels_ints() {\n-            int[] classlabels_ints = Attribute.classlabels_ints.access(int[].class, onnxAttributes);\n-            return java.util.Optional.ofNullable(classlabels_ints).map(int[]::clone);\n+        public java.util.Optional<long[]> classlabels_ints() {\n+            long[] classlabels_ints = Attribute.classlabels_ints.access(long[].class, onnxAttributes);\n+            return java.util.Optional.ofNullable(classlabels_ints).map(long[]::clone);\n@@ -14104,2 +14104,2 @@\n-        public java.util.Optional<Integer> multi_class() {\n-            Integer multi_class = Attribute.multi_class.access(Integer.class, onnxAttributes);\n+        public java.util.Optional<Long> multi_class() {\n+            Long multi_class = Attribute.multi_class.access(Long.class, onnxAttributes);\n@@ -14121,1 +14121,1 @@\n-    public static LinearClassifier LinearClassifier(TypeElement resultType, Value X, java.util.Optional<int[]> classlabels_ints, java.util.Optional<String> post_transform, float[] coefficients, java.util.Optional<Integer> multi_class, java.util.Optional<float[]> intercepts, java.util.Optional<String[]> classlabels_strings) {\n+    public static LinearClassifier LinearClassifier(TypeElement resultType, Value X, java.util.Optional<long[]> classlabels_ints, java.util.Optional<String> post_transform, float[] coefficients, java.util.Optional<Long> multi_class, java.util.Optional<float[]> intercepts, java.util.Optional<String[]> classlabels_strings) {\n@@ -14132,1 +14132,1 @@\n-            targets(Integer.class, true, 1),\n+            targets(Long.class, true, 1),\n@@ -14244,1 +14244,1 @@\n-        LinearRegressor(TypeElement resultType, Value X, java.util.Optional<String> post_transform, java.util.Optional<float[]> coefficients, java.util.Optional<Integer> targets, java.util.Optional<float[]> intercepts) {\n+        LinearRegressor(TypeElement resultType, Value X, java.util.Optional<String> post_transform, java.util.Optional<float[]> coefficients, java.util.Optional<Long> targets, java.util.Optional<float[]> intercepts) {\n@@ -14272,2 +14272,2 @@\n-        public java.util.Optional<Integer> targets() {\n-            Integer targets = Attribute.targets.access(Integer.class, onnxAttributes);\n+        public java.util.Optional<Long> targets() {\n+            Long targets = Attribute.targets.access(Long.class, onnxAttributes);\n@@ -14284,1 +14284,1 @@\n-    public static LinearRegressor LinearRegressor(TypeElement resultType, Value X, java.util.Optional<String> post_transform, java.util.Optional<float[]> coefficients, java.util.Optional<Integer> targets, java.util.Optional<float[]> intercepts) {\n+    public static LinearRegressor LinearRegressor(TypeElement resultType, Value X, java.util.Optional<String> post_transform, java.util.Optional<float[]> coefficients, java.util.Optional<Long> targets, java.util.Optional<float[]> intercepts) {\n@@ -14407,1 +14407,1 @@\n-            axis(Integer.class, true, -1),\n+            axis(Long.class, true, -1),\n@@ -14518,1 +14518,1 @@\n-        LogSoftmax(TypeElement resultType, Value input, java.util.Optional<Integer> axis) {\n+        LogSoftmax(TypeElement resultType, Value input, java.util.Optional<Long> axis) {\n@@ -14536,2 +14536,2 @@\n-        public java.util.Optional<Integer> axis() {\n-            Integer axis = Attribute.axis.access(Integer.class, onnxAttributes);\n+        public java.util.Optional<Long> axis() {\n+            Long axis = Attribute.axis.access(Long.class, onnxAttributes);\n@@ -14543,1 +14543,1 @@\n-    public static LogSoftmax LogSoftmax(TypeElement resultType, Value input, java.util.Optional<Integer> axis) {\n+    public static LogSoftmax LogSoftmax(TypeElement resultType, Value input, java.util.Optional<Long> axis) {\n@@ -14552,2 +14552,2 @@\n-            p(Integer.class, true, 2),\n-            axis(Integer.class, true, -1),\n+            p(Long.class, true, 2),\n+            axis(Long.class, true, -1),\n@@ -14664,1 +14664,1 @@\n-        LpNormalization(TypeElement resultType, Value input, java.util.Optional<Integer> p, java.util.Optional<Integer> axis) {\n+        LpNormalization(TypeElement resultType, Value input, java.util.Optional<Long> p, java.util.Optional<Long> axis) {\n@@ -14682,2 +14682,2 @@\n-        public java.util.Optional<Integer> p() {\n-            Integer p = Attribute.p.access(Integer.class, onnxAttributes);\n+        public java.util.Optional<Long> p() {\n+            Long p = Attribute.p.access(Long.class, onnxAttributes);\n@@ -14687,2 +14687,2 @@\n-        public java.util.Optional<Integer> axis() {\n-            Integer axis = Attribute.axis.access(Integer.class, onnxAttributes);\n+        public java.util.Optional<Long> axis() {\n+            Long axis = Attribute.axis.access(Long.class, onnxAttributes);\n@@ -14694,1 +14694,1 @@\n-    public static LpNormalization LpNormalization(TypeElement resultType, Value input, java.util.Optional<Integer> p, java.util.Optional<Integer> axis) {\n+    public static LpNormalization LpNormalization(TypeElement resultType, Value input, java.util.Optional<Long> p, java.util.Optional<Long> axis) {\n@@ -14703,3 +14703,3 @@\n-            p(Integer.class, true, 2),\n-            pads(int[].class, true, null),\n-            dilations(int[].class, true, null),\n+            p(Long.class, true, 2),\n+            pads(long[].class, true, null),\n+            dilations(long[].class, true, null),\n@@ -14707,3 +14707,3 @@\n-            ceil_mode(Integer.class, true, 0),\n-            strides(int[].class, true, null),\n-            kernel_shape(int[].class, false, null),\n+            ceil_mode(Long.class, true, 0),\n+            strides(long[].class, true, null),\n+            kernel_shape(long[].class, false, null),\n@@ -14820,1 +14820,1 @@\n-        LpPool(TypeElement resultType, Value X, java.util.Optional<Integer> p, java.util.Optional<int[]> pads, java.util.Optional<int[]> dilations, java.util.Optional<String> auto_pad, java.util.Optional<Integer> ceil_mode, java.util.Optional<int[]> strides, int[] kernel_shape) {\n+        LpPool(TypeElement resultType, Value X, java.util.Optional<Long> p, java.util.Optional<long[]> pads, java.util.Optional<long[]> dilations, java.util.Optional<String> auto_pad, java.util.Optional<Long> ceil_mode, java.util.Optional<long[]> strides, long[] kernel_shape) {\n@@ -14838,2 +14838,2 @@\n-        public java.util.Optional<Integer> p() {\n-            Integer p = Attribute.p.access(Integer.class, onnxAttributes);\n+        public java.util.Optional<Long> p() {\n+            Long p = Attribute.p.access(Long.class, onnxAttributes);\n@@ -14843,3 +14843,3 @@\n-        public java.util.Optional<int[]> pads() {\n-            int[] pads = Attribute.pads.access(int[].class, onnxAttributes);\n-            return java.util.Optional.ofNullable(pads).map(int[]::clone);\n+        public java.util.Optional<long[]> pads() {\n+            long[] pads = Attribute.pads.access(long[].class, onnxAttributes);\n+            return java.util.Optional.ofNullable(pads).map(long[]::clone);\n@@ -14848,3 +14848,3 @@\n-        public java.util.Optional<int[]> dilations() {\n-            int[] dilations = Attribute.dilations.access(int[].class, onnxAttributes);\n-            return java.util.Optional.ofNullable(dilations).map(int[]::clone);\n+        public java.util.Optional<long[]> dilations() {\n+            long[] dilations = Attribute.dilations.access(long[].class, onnxAttributes);\n+            return java.util.Optional.ofNullable(dilations).map(long[]::clone);\n@@ -14858,2 +14858,2 @@\n-        public java.util.Optional<Integer> ceil_mode() {\n-            Integer ceil_mode = Attribute.ceil_mode.access(Integer.class, onnxAttributes);\n+        public java.util.Optional<Long> ceil_mode() {\n+            Long ceil_mode = Attribute.ceil_mode.access(Long.class, onnxAttributes);\n@@ -14863,3 +14863,3 @@\n-        public java.util.Optional<int[]> strides() {\n-            int[] strides = Attribute.strides.access(int[].class, onnxAttributes);\n-            return java.util.Optional.ofNullable(strides).map(int[]::clone);\n+        public java.util.Optional<long[]> strides() {\n+            long[] strides = Attribute.strides.access(long[].class, onnxAttributes);\n+            return java.util.Optional.ofNullable(strides).map(long[]::clone);\n@@ -14868,2 +14868,2 @@\n-        public int[] kernel_shape() {\n-            int[] kernel_shape = Attribute.kernel_shape.access(int[].class, onnxAttributes);\n+        public long[] kernel_shape() {\n+            long[] kernel_shape = Attribute.kernel_shape.access(long[].class, onnxAttributes);\n@@ -14875,1 +14875,1 @@\n-    public static LpPool LpPool(TypeElement resultType, Value X, java.util.Optional<Integer> p, java.util.Optional<int[]> pads, java.util.Optional<int[]> dilations, java.util.Optional<String> auto_pad, java.util.Optional<Integer> ceil_mode, java.util.Optional<int[]> strides, int[] kernel_shape) {\n+    public static LpPool LpPool(TypeElement resultType, Value X, java.util.Optional<Long> p, java.util.Optional<long[]> pads, java.util.Optional<long[]> dilations, java.util.Optional<String> auto_pad, java.util.Optional<Long> ceil_mode, java.util.Optional<long[]> strides, long[] kernel_shape) {\n@@ -15250,2 +15250,2 @@\n-            pads(int[].class, true, null),\n-            dilations(int[].class, true, null),\n+            pads(long[].class, true, null),\n+            dilations(long[].class, true, null),\n@@ -15253,4 +15253,4 @@\n-            ceil_mode(Integer.class, true, 0),\n-            storage_order(Integer.class, true, 0),\n-            strides(int[].class, true, null),\n-            kernel_shape(int[].class, false, null),\n+            ceil_mode(Long.class, true, 0),\n+            storage_order(Long.class, true, 0),\n+            strides(long[].class, true, null),\n+            kernel_shape(long[].class, false, null),\n@@ -15369,1 +15369,1 @@\n-        MaxPool(TypeElement resultType, Set<OutputParameter> optionalOutputs, Value X, java.util.Optional<int[]> pads, java.util.Optional<int[]> dilations, java.util.Optional<String> auto_pad, java.util.Optional<Integer> ceil_mode, java.util.Optional<Integer> storage_order, java.util.Optional<int[]> strides, int[] kernel_shape) {\n+        MaxPool(TypeElement resultType, Set<OutputParameter> optionalOutputs, Value X, java.util.Optional<long[]> pads, java.util.Optional<long[]> dilations, java.util.Optional<String> auto_pad, java.util.Optional<Long> ceil_mode, java.util.Optional<Long> storage_order, java.util.Optional<long[]> strides, long[] kernel_shape) {\n@@ -15387,3 +15387,3 @@\n-        public java.util.Optional<int[]> pads() {\n-            int[] pads = Attribute.pads.access(int[].class, onnxAttributes);\n-            return java.util.Optional.ofNullable(pads).map(int[]::clone);\n+        public java.util.Optional<long[]> pads() {\n+            long[] pads = Attribute.pads.access(long[].class, onnxAttributes);\n+            return java.util.Optional.ofNullable(pads).map(long[]::clone);\n@@ -15392,3 +15392,3 @@\n-        public java.util.Optional<int[]> dilations() {\n-            int[] dilations = Attribute.dilations.access(int[].class, onnxAttributes);\n-            return java.util.Optional.ofNullable(dilations).map(int[]::clone);\n+        public java.util.Optional<long[]> dilations() {\n+            long[] dilations = Attribute.dilations.access(long[].class, onnxAttributes);\n+            return java.util.Optional.ofNullable(dilations).map(long[]::clone);\n@@ -15402,2 +15402,2 @@\n-        public java.util.Optional<Integer> ceil_mode() {\n-            Integer ceil_mode = Attribute.ceil_mode.access(Integer.class, onnxAttributes);\n+        public java.util.Optional<Long> ceil_mode() {\n+            Long ceil_mode = Attribute.ceil_mode.access(Long.class, onnxAttributes);\n@@ -15407,2 +15407,2 @@\n-        public java.util.Optional<Integer> storage_order() {\n-            Integer storage_order = Attribute.storage_order.access(Integer.class, onnxAttributes);\n+        public java.util.Optional<Long> storage_order() {\n+            Long storage_order = Attribute.storage_order.access(Long.class, onnxAttributes);\n@@ -15412,3 +15412,3 @@\n-        public java.util.Optional<int[]> strides() {\n-            int[] strides = Attribute.strides.access(int[].class, onnxAttributes);\n-            return java.util.Optional.ofNullable(strides).map(int[]::clone);\n+        public java.util.Optional<long[]> strides() {\n+            long[] strides = Attribute.strides.access(long[].class, onnxAttributes);\n+            return java.util.Optional.ofNullable(strides).map(long[]::clone);\n@@ -15417,2 +15417,2 @@\n-        public int[] kernel_shape() {\n-            int[] kernel_shape = Attribute.kernel_shape.access(int[].class, onnxAttributes);\n+        public long[] kernel_shape() {\n+            long[] kernel_shape = Attribute.kernel_shape.access(long[].class, onnxAttributes);\n@@ -15424,1 +15424,1 @@\n-    public static MaxPool MaxPool(TypeElement resultType, Set<MaxPool.OutputParameter> optionalOutputs, Value X, java.util.Optional<int[]> pads, java.util.Optional<int[]> dilations, java.util.Optional<String> auto_pad, java.util.Optional<Integer> ceil_mode, java.util.Optional<Integer> storage_order, java.util.Optional<int[]> strides, int[] kernel_shape) {\n+    public static MaxPool MaxPool(TypeElement resultType, Set<MaxPool.OutputParameter> optionalOutputs, Value X, java.util.Optional<long[]> pads, java.util.Optional<long[]> dilations, java.util.Optional<String> auto_pad, java.util.Optional<Long> ceil_mode, java.util.Optional<Long> storage_order, java.util.Optional<long[]> strides, long[] kernel_shape) {\n@@ -15434,1 +15434,1 @@\n-            pooled_shape(int[].class, false, null),\n+            pooled_shape(long[].class, false, null),\n@@ -15546,1 +15546,1 @@\n-        MaxRoiPool(TypeElement resultType, Value X, Value rois, java.util.Optional<Float> spatial_scale, int[] pooled_shape) {\n+        MaxRoiPool(TypeElement resultType, Value X, Value rois, java.util.Optional<Float> spatial_scale, long[] pooled_shape) {\n@@ -15573,2 +15573,2 @@\n-        public int[] pooled_shape() {\n-            int[] pooled_shape = Attribute.pooled_shape.access(int[].class, onnxAttributes);\n+        public long[] pooled_shape() {\n+            long[] pooled_shape = Attribute.pooled_shape.access(long[].class, onnxAttributes);\n@@ -15580,1 +15580,1 @@\n-    public static MaxRoiPool MaxRoiPool(TypeElement resultType, Value X, Value rois, java.util.Optional<Float> spatial_scale, int[] pooled_shape) {\n+    public static MaxRoiPool MaxRoiPool(TypeElement resultType, Value X, Value rois, java.util.Optional<Float> spatial_scale, long[] pooled_shape) {\n@@ -15589,3 +15589,3 @@\n-            pads(int[].class, true, null),\n-            strides(int[].class, true, null),\n-            kernel_shape(int[].class, false, null),\n+            pads(long[].class, true, null),\n+            strides(long[].class, true, null),\n+            kernel_shape(long[].class, false, null),\n@@ -15705,1 +15705,1 @@\n-        MaxUnpool(TypeElement resultType, Value X, Value I, java.util.Optional<Value> output_shape, java.util.Optional<int[]> pads, java.util.Optional<int[]> strides, int[] kernel_shape) {\n+        MaxUnpool(TypeElement resultType, Value X, Value I, java.util.Optional<Value> output_shape, java.util.Optional<long[]> pads, java.util.Optional<long[]> strides, long[] kernel_shape) {\n@@ -15732,3 +15732,3 @@\n-        public java.util.Optional<int[]> pads() {\n-            int[] pads = Attribute.pads.access(int[].class, onnxAttributes);\n-            return java.util.Optional.ofNullable(pads).map(int[]::clone);\n+        public java.util.Optional<long[]> pads() {\n+            long[] pads = Attribute.pads.access(long[].class, onnxAttributes);\n+            return java.util.Optional.ofNullable(pads).map(long[]::clone);\n@@ -15737,3 +15737,3 @@\n-        public java.util.Optional<int[]> strides() {\n-            int[] strides = Attribute.strides.access(int[].class, onnxAttributes);\n-            return java.util.Optional.ofNullable(strides).map(int[]::clone);\n+        public java.util.Optional<long[]> strides() {\n+            long[] strides = Attribute.strides.access(long[].class, onnxAttributes);\n+            return java.util.Optional.ofNullable(strides).map(long[]::clone);\n@@ -15742,2 +15742,2 @@\n-        public int[] kernel_shape() {\n-            int[] kernel_shape = Attribute.kernel_shape.access(int[].class, onnxAttributes);\n+        public long[] kernel_shape() {\n+            long[] kernel_shape = Attribute.kernel_shape.access(long[].class, onnxAttributes);\n@@ -15749,1 +15749,1 @@\n-    public static MaxUnpool MaxUnpool(TypeElement resultType, Value X, Value I, java.util.Optional<Value> output_shape, java.util.Optional<int[]> pads, java.util.Optional<int[]> strides, int[] kernel_shape) {\n+    public static MaxUnpool MaxUnpool(TypeElement resultType, Value X, Value I, java.util.Optional<Value> output_shape, java.util.Optional<long[]> pads, java.util.Optional<long[]> strides, long[] kernel_shape) {\n@@ -15872,1 +15872,1 @@\n-            axes(int[].class, true, null),\n+            axes(long[].class, true, null),\n@@ -15983,1 +15983,1 @@\n-        MeanVarianceNormalization(TypeElement resultType, Value X, java.util.Optional<int[]> axes) {\n+        MeanVarianceNormalization(TypeElement resultType, Value X, java.util.Optional<long[]> axes) {\n@@ -16001,3 +16001,3 @@\n-        public java.util.Optional<int[]> axes() {\n-            int[] axes = Attribute.axes.access(int[].class, onnxAttributes);\n-            return java.util.Optional.ofNullable(axes).map(int[]::clone);\n+        public java.util.Optional<long[]> axes() {\n+            long[] axes = Attribute.axes.access(long[].class, onnxAttributes);\n+            return java.util.Optional.ofNullable(axes).map(long[]::clone);\n@@ -16008,1 +16008,1 @@\n-    public static MeanVarianceNormalization MeanVarianceNormalization(TypeElement resultType, Value X, java.util.Optional<int[]> axes) {\n+    public static MeanVarianceNormalization MeanVarianceNormalization(TypeElement resultType, Value X, java.util.Optional<long[]> axes) {\n@@ -16017,1 +16017,1 @@\n-            output_datatype(Integer.class, true, 1),\n+            output_datatype(Long.class, true, 1),\n@@ -16134,1 +16134,1 @@\n-        MelWeightMatrix(TypeElement resultType, Value num_mel_bins, Value dft_length, Value sample_rate, Value lower_edge_hertz, Value upper_edge_hertz, java.util.Optional<Integer> output_datatype) {\n+        MelWeightMatrix(TypeElement resultType, Value num_mel_bins, Value dft_length, Value sample_rate, Value lower_edge_hertz, Value upper_edge_hertz, java.util.Optional<Long> output_datatype) {\n@@ -16168,2 +16168,2 @@\n-        public java.util.Optional<Integer> output_datatype() {\n-            Integer output_datatype = Attribute.output_datatype.access(Integer.class, onnxAttributes);\n+        public java.util.Optional<Long> output_datatype() {\n+            Long output_datatype = Attribute.output_datatype.access(Long.class, onnxAttributes);\n@@ -16175,1 +16175,1 @@\n-    public static MelWeightMatrix MelWeightMatrix(TypeElement resultType, Value num_mel_bins, Value dft_length, Value sample_rate, Value lower_edge_hertz, Value upper_edge_hertz, java.util.Optional<Integer> output_datatype) {\n+    public static MelWeightMatrix MelWeightMatrix(TypeElement resultType, Value num_mel_bins, Value dft_length, Value sample_rate, Value lower_edge_hertz, Value upper_edge_hertz, java.util.Optional<Long> output_datatype) {\n@@ -16412,1 +16412,1 @@\n-            fmod(Integer.class, true, 0),\n+            fmod(Long.class, true, 0),\n@@ -16524,1 +16524,1 @@\n-        Mod(TypeElement resultType, Value A, Value B, java.util.Optional<Integer> fmod) {\n+        Mod(TypeElement resultType, Value A, Value B, java.util.Optional<Long> fmod) {\n@@ -16546,2 +16546,2 @@\n-        public java.util.Optional<Integer> fmod() {\n-            Integer fmod = Attribute.fmod.access(Integer.class, onnxAttributes);\n+        public java.util.Optional<Long> fmod() {\n+            Long fmod = Attribute.fmod.access(Long.class, onnxAttributes);\n@@ -16553,1 +16553,1 @@\n-    public static Mod Mod(TypeElement resultType, Value A, Value B, java.util.Optional<Integer> fmod) {\n+    public static Mod Mod(TypeElement resultType, Value A, Value B, java.util.Optional<Long> fmod) {\n@@ -16857,2 +16857,2 @@\n-            sample_size(Integer.class, true, 1),\n-            dtype(Integer.class, true, 6),\n+            sample_size(Long.class, true, 1),\n+            dtype(Long.class, true, 6),\n@@ -16970,1 +16970,1 @@\n-        Multinomial(TypeElement resultType, Value input, java.util.Optional<Float> seed, java.util.Optional<Integer> sample_size, java.util.Optional<Integer> dtype) {\n+        Multinomial(TypeElement resultType, Value input, java.util.Optional<Float> seed, java.util.Optional<Long> sample_size, java.util.Optional<Long> dtype) {\n@@ -16993,2 +16993,2 @@\n-        public java.util.Optional<Integer> sample_size() {\n-            Integer sample_size = Attribute.sample_size.access(Integer.class, onnxAttributes);\n+        public java.util.Optional<Long> sample_size() {\n+            Long sample_size = Attribute.sample_size.access(Long.class, onnxAttributes);\n@@ -16998,2 +16998,2 @@\n-        public java.util.Optional<Integer> dtype() {\n-            Integer dtype = Attribute.dtype.access(Integer.class, onnxAttributes);\n+        public java.util.Optional<Long> dtype() {\n+            Long dtype = Attribute.dtype.access(Long.class, onnxAttributes);\n@@ -17005,1 +17005,1 @@\n-    public static Multinomial Multinomial(TypeElement resultType, Value input, java.util.Optional<Float> seed, java.util.Optional<Integer> sample_size, java.util.Optional<Integer> dtype) {\n+    public static Multinomial Multinomial(TypeElement resultType, Value input, java.util.Optional<Float> seed, java.util.Optional<Long> sample_size, java.util.Optional<Long> dtype) {\n@@ -17128,1 +17128,1 @@\n-            ignore_index(Integer.class, true, null),\n+            ignore_index(Long.class, true, null),\n@@ -17243,1 +17243,1 @@\n-        NegativeLogLikelihoodLoss(TypeElement resultType, Value input, Value target, java.util.Optional<Value> weight, java.util.Optional<Integer> ignore_index, java.util.Optional<String> reduction) {\n+        NegativeLogLikelihoodLoss(TypeElement resultType, Value input, Value target, java.util.Optional<Value> weight, java.util.Optional<Long> ignore_index, java.util.Optional<String> reduction) {\n@@ -17270,2 +17270,2 @@\n-        public java.util.Optional<Integer> ignore_index() {\n-            Integer ignore_index = Attribute.ignore_index.access(Integer.class, onnxAttributes);\n+        public java.util.Optional<Long> ignore_index() {\n+            Long ignore_index = Attribute.ignore_index.access(Long.class, onnxAttributes);\n@@ -17282,1 +17282,1 @@\n-    public static NegativeLogLikelihoodLoss NegativeLogLikelihoodLoss(TypeElement resultType, Value input, Value target, java.util.Optional<Value> weight, java.util.Optional<Integer> ignore_index, java.util.Optional<String> reduction) {\n+    public static NegativeLogLikelihoodLoss NegativeLogLikelihoodLoss(TypeElement resultType, Value input, Value target, java.util.Optional<Value> weight, java.util.Optional<Long> ignore_index, java.util.Optional<String> reduction) {\n@@ -17291,1 +17291,1 @@\n-            center_point_box(Integer.class, true, 0),\n+            center_point_box(Long.class, true, 0),\n@@ -17391,1 +17391,1 @@\n-        NonMaxSuppression(TypeElement resultType, Value boxes, Value scores, java.util.Optional<Value> max_output_boxes_per_class, java.util.Optional<Value> iou_threshold, java.util.Optional<Value> score_threshold, java.util.Optional<Integer> center_point_box) {\n+        NonMaxSuppression(TypeElement resultType, Value boxes, Value scores, java.util.Optional<Value> max_output_boxes_per_class, java.util.Optional<Value> iou_threshold, java.util.Optional<Value> score_threshold, java.util.Optional<Long> center_point_box) {\n@@ -17428,2 +17428,2 @@\n-        public java.util.Optional<Integer> center_point_box() {\n-            Integer center_point_box = Attribute.center_point_box.access(Integer.class, onnxAttributes);\n+        public java.util.Optional<Long> center_point_box() {\n+            Long center_point_box = Attribute.center_point_box.access(Long.class, onnxAttributes);\n@@ -17435,1 +17435,1 @@\n-    public static NonMaxSuppression NonMaxSuppression(TypeElement resultType, Value boxes, Value scores, java.util.Optional<Value> max_output_boxes_per_class, java.util.Optional<Value> iou_threshold, java.util.Optional<Value> score_threshold, java.util.Optional<Integer> center_point_box) {\n+    public static NonMaxSuppression NonMaxSuppression(TypeElement resultType, Value boxes, Value scores, java.util.Optional<Value> max_output_boxes_per_class, java.util.Optional<Value> iou_threshold, java.util.Optional<Value> score_threshold, java.util.Optional<Long> center_point_box) {\n@@ -17817,1 +17817,1 @@\n-            axis(Integer.class, true, -1),\n+            axis(Long.class, true, -1),\n@@ -17932,1 +17932,1 @@\n-        OneHot(TypeElement resultType, Value indices, Value depth, Value values, java.util.Optional<Integer> axis) {\n+        OneHot(TypeElement resultType, Value indices, Value depth, Value values, java.util.Optional<Long> axis) {\n@@ -17958,2 +17958,2 @@\n-        public java.util.Optional<Integer> axis() {\n-            Integer axis = Attribute.axis.access(Integer.class, onnxAttributes);\n+        public java.util.Optional<Long> axis() {\n+            Long axis = Attribute.axis.access(Long.class, onnxAttributes);\n@@ -17965,1 +17965,1 @@\n-    public static OneHot OneHot(TypeElement resultType, Value indices, Value depth, Value values, java.util.Optional<Integer> axis) {\n+    public static OneHot OneHot(TypeElement resultType, Value indices, Value depth, Value values, java.util.Optional<Long> axis) {\n@@ -17975,2 +17975,2 @@\n-            cats_int64s(int[].class, true, null),\n-            zeros(Integer.class, true, 1),\n+            cats_int64s(long[].class, true, null),\n+            zeros(Long.class, true, 1),\n@@ -18087,1 +18087,1 @@\n-        OneHotEncoder(TypeElement resultType, Value X, java.util.Optional<String[]> cats_strings, java.util.Optional<int[]> cats_int64s, java.util.Optional<Integer> zeros) {\n+        OneHotEncoder(TypeElement resultType, Value X, java.util.Optional<String[]> cats_strings, java.util.Optional<long[]> cats_int64s, java.util.Optional<Long> zeros) {\n@@ -18110,3 +18110,3 @@\n-        public java.util.Optional<int[]> cats_int64s() {\n-            int[] cats_int64s = Attribute.cats_int64s.access(int[].class, onnxAttributes);\n-            return java.util.Optional.ofNullable(cats_int64s).map(int[]::clone);\n+        public java.util.Optional<long[]> cats_int64s() {\n+            long[] cats_int64s = Attribute.cats_int64s.access(long[].class, onnxAttributes);\n+            return java.util.Optional.ofNullable(cats_int64s).map(long[]::clone);\n@@ -18115,2 +18115,2 @@\n-        public java.util.Optional<Integer> zeros() {\n-            Integer zeros = Attribute.zeros.access(Integer.class, onnxAttributes);\n+        public java.util.Optional<Long> zeros() {\n+            Long zeros = Attribute.zeros.access(Long.class, onnxAttributes);\n@@ -18122,1 +18122,1 @@\n-    public static OneHotEncoder OneHotEncoder(TypeElement resultType, Value X, java.util.Optional<String[]> cats_strings, java.util.Optional<int[]> cats_int64s, java.util.Optional<Integer> zeros) {\n+    public static OneHotEncoder OneHotEncoder(TypeElement resultType, Value X, java.util.Optional<String[]> cats_strings, java.util.Optional<long[]> cats_int64s, java.util.Optional<Long> zeros) {\n@@ -19031,2 +19031,2 @@\n-            pads(int[].class, true, null),\n-            dilations(int[].class, true, null),\n+            pads(long[].class, true, null),\n+            dilations(long[].class, true, null),\n@@ -19034,3 +19034,3 @@\n-            strides(int[].class, true, null),\n-            group(Integer.class, true, 1),\n-            kernel_shape(int[].class, true, null),\n+            strides(long[].class, true, null),\n+            group(Long.class, true, 1),\n+            kernel_shape(long[].class, true, null),\n@@ -19158,1 +19158,1 @@\n-        QLinearConv(TypeElement resultType, Value x, Value x_scale, Value x_zero_point, Value w, Value w_scale, Value w_zero_point, Value y_scale, Value y_zero_point, java.util.Optional<Value> B, java.util.Optional<int[]> pads, java.util.Optional<int[]> dilations, java.util.Optional<String> auto_pad, java.util.Optional<int[]> strides, java.util.Optional<Integer> group, java.util.Optional<int[]> kernel_shape) {\n+        QLinearConv(TypeElement resultType, Value x, Value x_scale, Value x_zero_point, Value w, Value w_scale, Value w_zero_point, Value y_scale, Value y_zero_point, java.util.Optional<Value> B, java.util.Optional<long[]> pads, java.util.Optional<long[]> dilations, java.util.Optional<String> auto_pad, java.util.Optional<long[]> strides, java.util.Optional<Long> group, java.util.Optional<long[]> kernel_shape) {\n@@ -19209,3 +19209,3 @@\n-        public java.util.Optional<int[]> pads() {\n-            int[] pads = Attribute.pads.access(int[].class, onnxAttributes);\n-            return java.util.Optional.ofNullable(pads).map(int[]::clone);\n+        public java.util.Optional<long[]> pads() {\n+            long[] pads = Attribute.pads.access(long[].class, onnxAttributes);\n+            return java.util.Optional.ofNullable(pads).map(long[]::clone);\n@@ -19214,3 +19214,3 @@\n-        public java.util.Optional<int[]> dilations() {\n-            int[] dilations = Attribute.dilations.access(int[].class, onnxAttributes);\n-            return java.util.Optional.ofNullable(dilations).map(int[]::clone);\n+        public java.util.Optional<long[]> dilations() {\n+            long[] dilations = Attribute.dilations.access(long[].class, onnxAttributes);\n+            return java.util.Optional.ofNullable(dilations).map(long[]::clone);\n@@ -19224,3 +19224,3 @@\n-        public java.util.Optional<int[]> strides() {\n-            int[] strides = Attribute.strides.access(int[].class, onnxAttributes);\n-            return java.util.Optional.ofNullable(strides).map(int[]::clone);\n+        public java.util.Optional<long[]> strides() {\n+            long[] strides = Attribute.strides.access(long[].class, onnxAttributes);\n+            return java.util.Optional.ofNullable(strides).map(long[]::clone);\n@@ -19229,2 +19229,2 @@\n-        public java.util.Optional<Integer> group() {\n-            Integer group = Attribute.group.access(Integer.class, onnxAttributes);\n+        public java.util.Optional<Long> group() {\n+            Long group = Attribute.group.access(Long.class, onnxAttributes);\n@@ -19234,3 +19234,3 @@\n-        public java.util.Optional<int[]> kernel_shape() {\n-            int[] kernel_shape = Attribute.kernel_shape.access(int[].class, onnxAttributes);\n-            return java.util.Optional.ofNullable(kernel_shape).map(int[]::clone);\n+        public java.util.Optional<long[]> kernel_shape() {\n+            long[] kernel_shape = Attribute.kernel_shape.access(long[].class, onnxAttributes);\n+            return java.util.Optional.ofNullable(kernel_shape).map(long[]::clone);\n@@ -19241,1 +19241,1 @@\n-    public static QLinearConv QLinearConv(TypeElement resultType, Value x, Value x_scale, Value x_zero_point, Value w, Value w_scale, Value w_zero_point, Value y_scale, Value y_zero_point, java.util.Optional<Value> B, java.util.Optional<int[]> pads, java.util.Optional<int[]> dilations, java.util.Optional<String> auto_pad, java.util.Optional<int[]> strides, java.util.Optional<Integer> group, java.util.Optional<int[]> kernel_shape) {\n+    public static QLinearConv QLinearConv(TypeElement resultType, Value x, Value x_scale, Value x_zero_point, Value w, Value w_scale, Value w_zero_point, Value y_scale, Value y_zero_point, java.util.Optional<Value> B, java.util.Optional<long[]> pads, java.util.Optional<long[]> dilations, java.util.Optional<String> auto_pad, java.util.Optional<long[]> strides, java.util.Optional<Long> group, java.util.Optional<long[]> kernel_shape) {\n@@ -19402,4 +19402,4 @@\n-            output_dtype(Integer.class, true, 0),\n-            saturate(Integer.class, true, 1),\n-            axis(Integer.class, true, 1),\n-            block_size(Integer.class, true, 0),\n+            output_dtype(Long.class, true, 0),\n+            saturate(Long.class, true, 1),\n+            axis(Long.class, true, 1),\n+            block_size(Long.class, true, 0),\n@@ -19519,1 +19519,1 @@\n-        QuantizeLinear(TypeElement resultType, Value x, Value y_scale, java.util.Optional<Value> y_zero_point, java.util.Optional<Integer> output_dtype, java.util.Optional<Integer> saturate, java.util.Optional<Integer> axis, java.util.Optional<Integer> block_size) {\n+        QuantizeLinear(TypeElement resultType, Value x, Value y_scale, java.util.Optional<Value> y_zero_point, java.util.Optional<Long> output_dtype, java.util.Optional<Long> saturate, java.util.Optional<Long> axis, java.util.Optional<Long> block_size) {\n@@ -19546,2 +19546,2 @@\n-        public java.util.Optional<Integer> output_dtype() {\n-            Integer output_dtype = Attribute.output_dtype.access(Integer.class, onnxAttributes);\n+        public java.util.Optional<Long> output_dtype() {\n+            Long output_dtype = Attribute.output_dtype.access(Long.class, onnxAttributes);\n@@ -19551,2 +19551,2 @@\n-        public java.util.Optional<Integer> saturate() {\n-            Integer saturate = Attribute.saturate.access(Integer.class, onnxAttributes);\n+        public java.util.Optional<Long> saturate() {\n+            Long saturate = Attribute.saturate.access(Long.class, onnxAttributes);\n@@ -19556,2 +19556,2 @@\n-        public java.util.Optional<Integer> axis() {\n-            Integer axis = Attribute.axis.access(Integer.class, onnxAttributes);\n+        public java.util.Optional<Long> axis() {\n+            Long axis = Attribute.axis.access(Long.class, onnxAttributes);\n@@ -19561,2 +19561,2 @@\n-        public java.util.Optional<Integer> block_size() {\n-            Integer block_size = Attribute.block_size.access(Integer.class, onnxAttributes);\n+        public java.util.Optional<Long> block_size() {\n+            Long block_size = Attribute.block_size.access(Long.class, onnxAttributes);\n@@ -19568,1 +19568,1 @@\n-    public static QuantizeLinear QuantizeLinear(TypeElement resultType, Value x, Value y_scale, java.util.Optional<Value> y_zero_point, java.util.Optional<Integer> output_dtype, java.util.Optional<Integer> saturate, java.util.Optional<Integer> axis, java.util.Optional<Integer> block_size) {\n+    public static QuantizeLinear QuantizeLinear(TypeElement resultType, Value x, Value y_scale, java.util.Optional<Value> y_zero_point, java.util.Optional<Long> output_dtype, java.util.Optional<Long> saturate, java.util.Optional<Long> axis, java.util.Optional<Long> block_size) {\n@@ -19577,1 +19577,1 @@\n-            layout(Integer.class, true, 0),\n+            layout(Long.class, true, 0),\n@@ -19579,1 +19579,1 @@\n-            hidden_size(Integer.class, true, null),\n+            hidden_size(Long.class, true, null),\n@@ -19701,1 +19701,1 @@\n-        RNN(TypeElement resultType, Set<OutputParameter> optionalOutputs, Value X, Value W, Value R, java.util.Optional<Value> B, java.util.Optional<Value> sequence_lens, java.util.Optional<Value> initial_h, java.util.Optional<Integer> layout, java.util.Optional<float[]> activation_alpha, java.util.Optional<Integer> hidden_size, java.util.Optional<float[]> activation_beta, java.util.Optional<String[]> activations, java.util.Optional<Float> clip, java.util.Optional<String> direction) {\n+        RNN(TypeElement resultType, Set<OutputParameter> optionalOutputs, Value X, Value W, Value R, java.util.Optional<Value> B, java.util.Optional<Value> sequence_lens, java.util.Optional<Value> initial_h, java.util.Optional<Long> layout, java.util.Optional<float[]> activation_alpha, java.util.Optional<Long> hidden_size, java.util.Optional<float[]> activation_beta, java.util.Optional<String[]> activations, java.util.Optional<Float> clip, java.util.Optional<String> direction) {\n@@ -19742,2 +19742,2 @@\n-        public java.util.Optional<Integer> layout() {\n-            Integer layout = Attribute.layout.access(Integer.class, onnxAttributes);\n+        public java.util.Optional<Long> layout() {\n+            Long layout = Attribute.layout.access(Long.class, onnxAttributes);\n@@ -19752,2 +19752,2 @@\n-        public java.util.Optional<Integer> hidden_size() {\n-            Integer hidden_size = Attribute.hidden_size.access(Integer.class, onnxAttributes);\n+        public java.util.Optional<Long> hidden_size() {\n+            Long hidden_size = Attribute.hidden_size.access(Long.class, onnxAttributes);\n@@ -19779,1 +19779,1 @@\n-    public static RNN RNN(TypeElement resultType, Set<RNN.OutputParameter> optionalOutputs, Value X, Value W, Value R, java.util.Optional<Value> B, java.util.Optional<Value> sequence_lens, java.util.Optional<Value> initial_h, java.util.Optional<Integer> layout, java.util.Optional<float[]> activation_alpha, java.util.Optional<Integer> hidden_size, java.util.Optional<float[]> activation_beta, java.util.Optional<String[]> activations, java.util.Optional<Float> clip, java.util.Optional<String> direction) {\n+    public static RNN RNN(TypeElement resultType, Set<RNN.OutputParameter> optionalOutputs, Value X, Value W, Value R, java.util.Optional<Value> B, java.util.Optional<Value> sequence_lens, java.util.Optional<Value> initial_h, java.util.Optional<Long> layout, java.util.Optional<float[]> activation_alpha, java.util.Optional<Long> hidden_size, java.util.Optional<float[]> activation_beta, java.util.Optional<String[]> activations, java.util.Optional<Float> clip, java.util.Optional<String> direction) {\n@@ -19788,1 +19788,1 @@\n-            shape(int[].class, false, null),\n+            shape(long[].class, false, null),\n@@ -19792,1 +19792,1 @@\n-            dtype(Integer.class, true, 1),\n+            dtype(Long.class, true, 1),\n@@ -19882,1 +19882,1 @@\n-        RandomNormal(TypeElement resultType, int[] shape, java.util.Optional<Float> seed, java.util.Optional<Float> mean, java.util.Optional<Float> scale, java.util.Optional<Integer> dtype) {\n+        RandomNormal(TypeElement resultType, long[] shape, java.util.Optional<Float> seed, java.util.Optional<Float> mean, java.util.Optional<Float> scale, java.util.Optional<Long> dtype) {\n@@ -19896,2 +19896,2 @@\n-        public int[] shape() {\n-            int[] shape = Attribute.shape.access(int[].class, onnxAttributes);\n+        public long[] shape() {\n+            long[] shape = Attribute.shape.access(long[].class, onnxAttributes);\n@@ -19916,2 +19916,2 @@\n-        public java.util.Optional<Integer> dtype() {\n-            Integer dtype = Attribute.dtype.access(Integer.class, onnxAttributes);\n+        public java.util.Optional<Long> dtype() {\n+            Long dtype = Attribute.dtype.access(Long.class, onnxAttributes);\n@@ -19923,1 +19923,1 @@\n-    public static RandomNormal RandomNormal(TypeElement resultType, int[] shape, java.util.Optional<Float> seed, java.util.Optional<Float> mean, java.util.Optional<Float> scale, java.util.Optional<Integer> dtype) {\n+    public static RandomNormal RandomNormal(TypeElement resultType, long[] shape, java.util.Optional<Float> seed, java.util.Optional<Float> mean, java.util.Optional<Float> scale, java.util.Optional<Long> dtype) {\n@@ -19935,1 +19935,1 @@\n-            dtype(Integer.class, true, null),\n+            dtype(Long.class, true, null),\n@@ -20047,1 +20047,1 @@\n-        RandomNormalLike(TypeElement resultType, Value input, java.util.Optional<Float> seed, java.util.Optional<Float> mean, java.util.Optional<Float> scale, java.util.Optional<Integer> dtype) {\n+        RandomNormalLike(TypeElement resultType, Value input, java.util.Optional<Float> seed, java.util.Optional<Float> mean, java.util.Optional<Float> scale, java.util.Optional<Long> dtype) {\n@@ -20080,2 +20080,2 @@\n-        public java.util.Optional<Integer> dtype() {\n-            Integer dtype = Attribute.dtype.access(Integer.class, onnxAttributes);\n+        public java.util.Optional<Long> dtype() {\n+            Long dtype = Attribute.dtype.access(Long.class, onnxAttributes);\n@@ -20087,1 +20087,1 @@\n-    public static RandomNormalLike RandomNormalLike(TypeElement resultType, Value input, java.util.Optional<Float> seed, java.util.Optional<Float> mean, java.util.Optional<Float> scale, java.util.Optional<Integer> dtype) {\n+    public static RandomNormalLike RandomNormalLike(TypeElement resultType, Value input, java.util.Optional<Float> seed, java.util.Optional<Float> mean, java.util.Optional<Float> scale, java.util.Optional<Long> dtype) {\n@@ -20097,1 +20097,1 @@\n-            shape(int[].class, false, null),\n+            shape(long[].class, false, null),\n@@ -20100,1 +20100,1 @@\n-            dtype(Integer.class, true, 1),\n+            dtype(Long.class, true, 1),\n@@ -20190,1 +20190,1 @@\n-        RandomUniform(TypeElement resultType, java.util.Optional<Float> high, int[] shape, java.util.Optional<Float> seed, java.util.Optional<Float> low, java.util.Optional<Integer> dtype) {\n+        RandomUniform(TypeElement resultType, java.util.Optional<Float> high, long[] shape, java.util.Optional<Float> seed, java.util.Optional<Float> low, java.util.Optional<Long> dtype) {\n@@ -20209,2 +20209,2 @@\n-        public int[] shape() {\n-            int[] shape = Attribute.shape.access(int[].class, onnxAttributes);\n+        public long[] shape() {\n+            long[] shape = Attribute.shape.access(long[].class, onnxAttributes);\n@@ -20224,2 +20224,2 @@\n-        public java.util.Optional<Integer> dtype() {\n-            Integer dtype = Attribute.dtype.access(Integer.class, onnxAttributes);\n+        public java.util.Optional<Long> dtype() {\n+            Long dtype = Attribute.dtype.access(Long.class, onnxAttributes);\n@@ -20231,1 +20231,1 @@\n-    public static RandomUniform RandomUniform(TypeElement resultType, java.util.Optional<Float> high, int[] shape, java.util.Optional<Float> seed, java.util.Optional<Float> low, java.util.Optional<Integer> dtype) {\n+    public static RandomUniform RandomUniform(TypeElement resultType, java.util.Optional<Float> high, long[] shape, java.util.Optional<Float> seed, java.util.Optional<Float> low, java.util.Optional<Long> dtype) {\n@@ -20243,1 +20243,1 @@\n-            dtype(Integer.class, true, null),\n+            dtype(Long.class, true, null),\n@@ -20355,1 +20355,1 @@\n-        RandomUniformLike(TypeElement resultType, Value input, java.util.Optional<Float> high, java.util.Optional<Float> seed, java.util.Optional<Float> low, java.util.Optional<Integer> dtype) {\n+        RandomUniformLike(TypeElement resultType, Value input, java.util.Optional<Float> high, java.util.Optional<Float> seed, java.util.Optional<Float> low, java.util.Optional<Long> dtype) {\n@@ -20388,2 +20388,2 @@\n-        public java.util.Optional<Integer> dtype() {\n-            Integer dtype = Attribute.dtype.access(Integer.class, onnxAttributes);\n+        public java.util.Optional<Long> dtype() {\n+            Long dtype = Attribute.dtype.access(Long.class, onnxAttributes);\n@@ -20395,1 +20395,1 @@\n-    public static RandomUniformLike RandomUniformLike(TypeElement resultType, Value input, java.util.Optional<Float> high, java.util.Optional<Float> seed, java.util.Optional<Float> low, java.util.Optional<Integer> dtype) {\n+    public static RandomUniformLike RandomUniformLike(TypeElement resultType, Value input, java.util.Optional<Float> high, java.util.Optional<Float> seed, java.util.Optional<Float> low, java.util.Optional<Long> dtype) {\n@@ -20642,2 +20642,2 @@\n-            noop_with_empty_axes(Integer.class, true, 0),\n-            keepdims(Integer.class, true, 1),\n+            noop_with_empty_axes(Long.class, true, 0),\n+            keepdims(Long.class, true, 1),\n@@ -20755,1 +20755,1 @@\n-        ReduceL1(TypeElement resultType, Value data, java.util.Optional<Value> axes, java.util.Optional<Integer> noop_with_empty_axes, java.util.Optional<Integer> keepdims) {\n+        ReduceL1(TypeElement resultType, Value data, java.util.Optional<Value> axes, java.util.Optional<Long> noop_with_empty_axes, java.util.Optional<Long> keepdims) {\n@@ -20778,2 +20778,2 @@\n-        public java.util.Optional<Integer> noop_with_empty_axes() {\n-            Integer noop_with_empty_axes = Attribute.noop_with_empty_axes.access(Integer.class, onnxAttributes);\n+        public java.util.Optional<Long> noop_with_empty_axes() {\n+            Long noop_with_empty_axes = Attribute.noop_with_empty_axes.access(Long.class, onnxAttributes);\n@@ -20783,2 +20783,2 @@\n-        public java.util.Optional<Integer> keepdims() {\n-            Integer keepdims = Attribute.keepdims.access(Integer.class, onnxAttributes);\n+        public java.util.Optional<Long> keepdims() {\n+            Long keepdims = Attribute.keepdims.access(Long.class, onnxAttributes);\n@@ -20790,1 +20790,1 @@\n-    public static ReduceL1 ReduceL1(TypeElement resultType, Value data, java.util.Optional<Value> axes, java.util.Optional<Integer> noop_with_empty_axes, java.util.Optional<Integer> keepdims) {\n+    public static ReduceL1 ReduceL1(TypeElement resultType, Value data, java.util.Optional<Value> axes, java.util.Optional<Long> noop_with_empty_axes, java.util.Optional<Long> keepdims) {\n@@ -20799,2 +20799,2 @@\n-            noop_with_empty_axes(Integer.class, true, 0),\n-            keepdims(Integer.class, true, 1),\n+            noop_with_empty_axes(Long.class, true, 0),\n+            keepdims(Long.class, true, 1),\n@@ -20912,1 +20912,1 @@\n-        ReduceL2(TypeElement resultType, Value data, java.util.Optional<Value> axes, java.util.Optional<Integer> noop_with_empty_axes, java.util.Optional<Integer> keepdims) {\n+        ReduceL2(TypeElement resultType, Value data, java.util.Optional<Value> axes, java.util.Optional<Long> noop_with_empty_axes, java.util.Optional<Long> keepdims) {\n@@ -20935,2 +20935,2 @@\n-        public java.util.Optional<Integer> noop_with_empty_axes() {\n-            Integer noop_with_empty_axes = Attribute.noop_with_empty_axes.access(Integer.class, onnxAttributes);\n+        public java.util.Optional<Long> noop_with_empty_axes() {\n+            Long noop_with_empty_axes = Attribute.noop_with_empty_axes.access(Long.class, onnxAttributes);\n@@ -20940,2 +20940,2 @@\n-        public java.util.Optional<Integer> keepdims() {\n-            Integer keepdims = Attribute.keepdims.access(Integer.class, onnxAttributes);\n+        public java.util.Optional<Long> keepdims() {\n+            Long keepdims = Attribute.keepdims.access(Long.class, onnxAttributes);\n@@ -20947,1 +20947,1 @@\n-    public static ReduceL2 ReduceL2(TypeElement resultType, Value data, java.util.Optional<Value> axes, java.util.Optional<Integer> noop_with_empty_axes, java.util.Optional<Integer> keepdims) {\n+    public static ReduceL2 ReduceL2(TypeElement resultType, Value data, java.util.Optional<Value> axes, java.util.Optional<Long> noop_with_empty_axes, java.util.Optional<Long> keepdims) {\n@@ -20956,2 +20956,2 @@\n-            noop_with_empty_axes(Integer.class, true, 0),\n-            keepdims(Integer.class, true, 1),\n+            noop_with_empty_axes(Long.class, true, 0),\n+            keepdims(Long.class, true, 1),\n@@ -21069,1 +21069,1 @@\n-        ReduceLogSum(TypeElement resultType, Value data, java.util.Optional<Value> axes, java.util.Optional<Integer> noop_with_empty_axes, java.util.Optional<Integer> keepdims) {\n+        ReduceLogSum(TypeElement resultType, Value data, java.util.Optional<Value> axes, java.util.Optional<Long> noop_with_empty_axes, java.util.Optional<Long> keepdims) {\n@@ -21092,2 +21092,2 @@\n-        public java.util.Optional<Integer> noop_with_empty_axes() {\n-            Integer noop_with_empty_axes = Attribute.noop_with_empty_axes.access(Integer.class, onnxAttributes);\n+        public java.util.Optional<Long> noop_with_empty_axes() {\n+            Long noop_with_empty_axes = Attribute.noop_with_empty_axes.access(Long.class, onnxAttributes);\n@@ -21097,2 +21097,2 @@\n-        public java.util.Optional<Integer> keepdims() {\n-            Integer keepdims = Attribute.keepdims.access(Integer.class, onnxAttributes);\n+        public java.util.Optional<Long> keepdims() {\n+            Long keepdims = Attribute.keepdims.access(Long.class, onnxAttributes);\n@@ -21104,1 +21104,1 @@\n-    public static ReduceLogSum ReduceLogSum(TypeElement resultType, Value data, java.util.Optional<Value> axes, java.util.Optional<Integer> noop_with_empty_axes, java.util.Optional<Integer> keepdims) {\n+    public static ReduceLogSum ReduceLogSum(TypeElement resultType, Value data, java.util.Optional<Value> axes, java.util.Optional<Long> noop_with_empty_axes, java.util.Optional<Long> keepdims) {\n@@ -21113,2 +21113,2 @@\n-            noop_with_empty_axes(Integer.class, true, 0),\n-            keepdims(Integer.class, true, 1),\n+            noop_with_empty_axes(Long.class, true, 0),\n+            keepdims(Long.class, true, 1),\n@@ -21226,1 +21226,1 @@\n-        ReduceLogSumExp(TypeElement resultType, Value data, java.util.Optional<Value> axes, java.util.Optional<Integer> noop_with_empty_axes, java.util.Optional<Integer> keepdims) {\n+        ReduceLogSumExp(TypeElement resultType, Value data, java.util.Optional<Value> axes, java.util.Optional<Long> noop_with_empty_axes, java.util.Optional<Long> keepdims) {\n@@ -21249,2 +21249,2 @@\n-        public java.util.Optional<Integer> noop_with_empty_axes() {\n-            Integer noop_with_empty_axes = Attribute.noop_with_empty_axes.access(Integer.class, onnxAttributes);\n+        public java.util.Optional<Long> noop_with_empty_axes() {\n+            Long noop_with_empty_axes = Attribute.noop_with_empty_axes.access(Long.class, onnxAttributes);\n@@ -21254,2 +21254,2 @@\n-        public java.util.Optional<Integer> keepdims() {\n-            Integer keepdims = Attribute.keepdims.access(Integer.class, onnxAttributes);\n+        public java.util.Optional<Long> keepdims() {\n+            Long keepdims = Attribute.keepdims.access(Long.class, onnxAttributes);\n@@ -21261,1 +21261,1 @@\n-    public static ReduceLogSumExp ReduceLogSumExp(TypeElement resultType, Value data, java.util.Optional<Value> axes, java.util.Optional<Integer> noop_with_empty_axes, java.util.Optional<Integer> keepdims) {\n+    public static ReduceLogSumExp ReduceLogSumExp(TypeElement resultType, Value data, java.util.Optional<Value> axes, java.util.Optional<Long> noop_with_empty_axes, java.util.Optional<Long> keepdims) {\n@@ -21270,2 +21270,2 @@\n-            noop_with_empty_axes(Integer.class, true, 0),\n-            keepdims(Integer.class, true, 1),\n+            noop_with_empty_axes(Long.class, true, 0),\n+            keepdims(Long.class, true, 1),\n@@ -21383,1 +21383,1 @@\n-        ReduceMax(TypeElement resultType, Value data, java.util.Optional<Value> axes, java.util.Optional<Integer> noop_with_empty_axes, java.util.Optional<Integer> keepdims) {\n+        ReduceMax(TypeElement resultType, Value data, java.util.Optional<Value> axes, java.util.Optional<Long> noop_with_empty_axes, java.util.Optional<Long> keepdims) {\n@@ -21406,2 +21406,2 @@\n-        public java.util.Optional<Integer> noop_with_empty_axes() {\n-            Integer noop_with_empty_axes = Attribute.noop_with_empty_axes.access(Integer.class, onnxAttributes);\n+        public java.util.Optional<Long> noop_with_empty_axes() {\n+            Long noop_with_empty_axes = Attribute.noop_with_empty_axes.access(Long.class, onnxAttributes);\n@@ -21411,2 +21411,2 @@\n-        public java.util.Optional<Integer> keepdims() {\n-            Integer keepdims = Attribute.keepdims.access(Integer.class, onnxAttributes);\n+        public java.util.Optional<Long> keepdims() {\n+            Long keepdims = Attribute.keepdims.access(Long.class, onnxAttributes);\n@@ -21418,1 +21418,1 @@\n-    public static ReduceMax ReduceMax(TypeElement resultType, Value data, java.util.Optional<Value> axes, java.util.Optional<Integer> noop_with_empty_axes, java.util.Optional<Integer> keepdims) {\n+    public static ReduceMax ReduceMax(TypeElement resultType, Value data, java.util.Optional<Value> axes, java.util.Optional<Long> noop_with_empty_axes, java.util.Optional<Long> keepdims) {\n@@ -21427,2 +21427,2 @@\n-            noop_with_empty_axes(Integer.class, true, 0),\n-            keepdims(Integer.class, true, 1),\n+            noop_with_empty_axes(Long.class, true, 0),\n+            keepdims(Long.class, true, 1),\n@@ -21540,1 +21540,1 @@\n-        ReduceMean(TypeElement resultType, Value data, java.util.Optional<Value> axes, java.util.Optional<Integer> noop_with_empty_axes, java.util.Optional<Integer> keepdims) {\n+        ReduceMean(TypeElement resultType, Value data, java.util.Optional<Value> axes, java.util.Optional<Long> noop_with_empty_axes, java.util.Optional<Long> keepdims) {\n@@ -21563,2 +21563,2 @@\n-        public java.util.Optional<Integer> noop_with_empty_axes() {\n-            Integer noop_with_empty_axes = Attribute.noop_with_empty_axes.access(Integer.class, onnxAttributes);\n+        public java.util.Optional<Long> noop_with_empty_axes() {\n+            Long noop_with_empty_axes = Attribute.noop_with_empty_axes.access(Long.class, onnxAttributes);\n@@ -21568,2 +21568,2 @@\n-        public java.util.Optional<Integer> keepdims() {\n-            Integer keepdims = Attribute.keepdims.access(Integer.class, onnxAttributes);\n+        public java.util.Optional<Long> keepdims() {\n+            Long keepdims = Attribute.keepdims.access(Long.class, onnxAttributes);\n@@ -21575,1 +21575,1 @@\n-    public static ReduceMean ReduceMean(TypeElement resultType, Value data, java.util.Optional<Value> axes, java.util.Optional<Integer> noop_with_empty_axes, java.util.Optional<Integer> keepdims) {\n+    public static ReduceMean ReduceMean(TypeElement resultType, Value data, java.util.Optional<Value> axes, java.util.Optional<Long> noop_with_empty_axes, java.util.Optional<Long> keepdims) {\n@@ -21584,2 +21584,2 @@\n-            noop_with_empty_axes(Integer.class, true, 0),\n-            keepdims(Integer.class, true, 1),\n+            noop_with_empty_axes(Long.class, true, 0),\n+            keepdims(Long.class, true, 1),\n@@ -21697,1 +21697,1 @@\n-        ReduceMin(TypeElement resultType, Value data, java.util.Optional<Value> axes, java.util.Optional<Integer> noop_with_empty_axes, java.util.Optional<Integer> keepdims) {\n+        ReduceMin(TypeElement resultType, Value data, java.util.Optional<Value> axes, java.util.Optional<Long> noop_with_empty_axes, java.util.Optional<Long> keepdims) {\n@@ -21720,2 +21720,2 @@\n-        public java.util.Optional<Integer> noop_with_empty_axes() {\n-            Integer noop_with_empty_axes = Attribute.noop_with_empty_axes.access(Integer.class, onnxAttributes);\n+        public java.util.Optional<Long> noop_with_empty_axes() {\n+            Long noop_with_empty_axes = Attribute.noop_with_empty_axes.access(Long.class, onnxAttributes);\n@@ -21725,2 +21725,2 @@\n-        public java.util.Optional<Integer> keepdims() {\n-            Integer keepdims = Attribute.keepdims.access(Integer.class, onnxAttributes);\n+        public java.util.Optional<Long> keepdims() {\n+            Long keepdims = Attribute.keepdims.access(Long.class, onnxAttributes);\n@@ -21732,1 +21732,1 @@\n-    public static ReduceMin ReduceMin(TypeElement resultType, Value data, java.util.Optional<Value> axes, java.util.Optional<Integer> noop_with_empty_axes, java.util.Optional<Integer> keepdims) {\n+    public static ReduceMin ReduceMin(TypeElement resultType, Value data, java.util.Optional<Value> axes, java.util.Optional<Long> noop_with_empty_axes, java.util.Optional<Long> keepdims) {\n@@ -21741,2 +21741,2 @@\n-            noop_with_empty_axes(Integer.class, true, 0),\n-            keepdims(Integer.class, true, 1),\n+            noop_with_empty_axes(Long.class, true, 0),\n+            keepdims(Long.class, true, 1),\n@@ -21854,1 +21854,1 @@\n-        ReduceProd(TypeElement resultType, Value data, java.util.Optional<Value> axes, java.util.Optional<Integer> noop_with_empty_axes, java.util.Optional<Integer> keepdims) {\n+        ReduceProd(TypeElement resultType, Value data, java.util.Optional<Value> axes, java.util.Optional<Long> noop_with_empty_axes, java.util.Optional<Long> keepdims) {\n@@ -21877,2 +21877,2 @@\n-        public java.util.Optional<Integer> noop_with_empty_axes() {\n-            Integer noop_with_empty_axes = Attribute.noop_with_empty_axes.access(Integer.class, onnxAttributes);\n+        public java.util.Optional<Long> noop_with_empty_axes() {\n+            Long noop_with_empty_axes = Attribute.noop_with_empty_axes.access(Long.class, onnxAttributes);\n@@ -21882,2 +21882,2 @@\n-        public java.util.Optional<Integer> keepdims() {\n-            Integer keepdims = Attribute.keepdims.access(Integer.class, onnxAttributes);\n+        public java.util.Optional<Long> keepdims() {\n+            Long keepdims = Attribute.keepdims.access(Long.class, onnxAttributes);\n@@ -21889,1 +21889,1 @@\n-    public static ReduceProd ReduceProd(TypeElement resultType, Value data, java.util.Optional<Value> axes, java.util.Optional<Integer> noop_with_empty_axes, java.util.Optional<Integer> keepdims) {\n+    public static ReduceProd ReduceProd(TypeElement resultType, Value data, java.util.Optional<Value> axes, java.util.Optional<Long> noop_with_empty_axes, java.util.Optional<Long> keepdims) {\n@@ -21898,2 +21898,2 @@\n-            noop_with_empty_axes(Integer.class, true, 0),\n-            keepdims(Integer.class, true, 1),\n+            noop_with_empty_axes(Long.class, true, 0),\n+            keepdims(Long.class, true, 1),\n@@ -22011,1 +22011,1 @@\n-        ReduceSum(TypeElement resultType, Value data, java.util.Optional<Value> axes, java.util.Optional<Integer> noop_with_empty_axes, java.util.Optional<Integer> keepdims) {\n+        ReduceSum(TypeElement resultType, Value data, java.util.Optional<Value> axes, java.util.Optional<Long> noop_with_empty_axes, java.util.Optional<Long> keepdims) {\n@@ -22034,2 +22034,2 @@\n-        public java.util.Optional<Integer> noop_with_empty_axes() {\n-            Integer noop_with_empty_axes = Attribute.noop_with_empty_axes.access(Integer.class, onnxAttributes);\n+        public java.util.Optional<Long> noop_with_empty_axes() {\n+            Long noop_with_empty_axes = Attribute.noop_with_empty_axes.access(Long.class, onnxAttributes);\n@@ -22039,2 +22039,2 @@\n-        public java.util.Optional<Integer> keepdims() {\n-            Integer keepdims = Attribute.keepdims.access(Integer.class, onnxAttributes);\n+        public java.util.Optional<Long> keepdims() {\n+            Long keepdims = Attribute.keepdims.access(Long.class, onnxAttributes);\n@@ -22046,1 +22046,1 @@\n-    public static ReduceSum ReduceSum(TypeElement resultType, Value data, java.util.Optional<Value> axes, java.util.Optional<Integer> noop_with_empty_axes, java.util.Optional<Integer> keepdims) {\n+    public static ReduceSum ReduceSum(TypeElement resultType, Value data, java.util.Optional<Value> axes, java.util.Optional<Long> noop_with_empty_axes, java.util.Optional<Long> keepdims) {\n@@ -22055,2 +22055,2 @@\n-            noop_with_empty_axes(Integer.class, true, 0),\n-            keepdims(Integer.class, true, 1),\n+            noop_with_empty_axes(Long.class, true, 0),\n+            keepdims(Long.class, true, 1),\n@@ -22168,1 +22168,1 @@\n-        ReduceSumSquare(TypeElement resultType, Value data, java.util.Optional<Value> axes, java.util.Optional<Integer> noop_with_empty_axes, java.util.Optional<Integer> keepdims) {\n+        ReduceSumSquare(TypeElement resultType, Value data, java.util.Optional<Value> axes, java.util.Optional<Long> noop_with_empty_axes, java.util.Optional<Long> keepdims) {\n@@ -22191,2 +22191,2 @@\n-        public java.util.Optional<Integer> noop_with_empty_axes() {\n-            Integer noop_with_empty_axes = Attribute.noop_with_empty_axes.access(Integer.class, onnxAttributes);\n+        public java.util.Optional<Long> noop_with_empty_axes() {\n+            Long noop_with_empty_axes = Attribute.noop_with_empty_axes.access(Long.class, onnxAttributes);\n@@ -22196,2 +22196,2 @@\n-        public java.util.Optional<Integer> keepdims() {\n-            Integer keepdims = Attribute.keepdims.access(Integer.class, onnxAttributes);\n+        public java.util.Optional<Long> keepdims() {\n+            Long keepdims = Attribute.keepdims.access(Long.class, onnxAttributes);\n@@ -22203,1 +22203,1 @@\n-    public static ReduceSumSquare ReduceSumSquare(TypeElement resultType, Value data, java.util.Optional<Value> axes, java.util.Optional<Integer> noop_with_empty_axes, java.util.Optional<Integer> keepdims) {\n+    public static ReduceSumSquare ReduceSumSquare(TypeElement resultType, Value data, java.util.Optional<Value> axes, java.util.Optional<Long> noop_with_empty_axes, java.util.Optional<Long> keepdims) {\n@@ -22472,1 +22472,1 @@\n-            allowzero(Integer.class, true, 0),\n+            allowzero(Long.class, true, 0),\n@@ -22584,1 +22584,1 @@\n-        Reshape(TypeElement resultType, Value data, Value shape, java.util.Optional<Integer> allowzero) {\n+        Reshape(TypeElement resultType, Value data, Value shape, java.util.Optional<Long> allowzero) {\n@@ -22606,2 +22606,2 @@\n-        public java.util.Optional<Integer> allowzero() {\n-            Integer allowzero = Attribute.allowzero.access(Integer.class, onnxAttributes);\n+        public java.util.Optional<Long> allowzero() {\n+            Long allowzero = Attribute.allowzero.access(Long.class, onnxAttributes);\n@@ -22613,1 +22613,1 @@\n-    public static Reshape Reshape(TypeElement resultType, Value data, Value shape, java.util.Optional<Integer> allowzero) {\n+    public static Reshape Reshape(TypeElement resultType, Value data, Value shape, java.util.Optional<Long> allowzero) {\n@@ -22625,1 +22625,1 @@\n-            antialias(Integer.class, true, 0),\n+            antialias(Long.class, true, 0),\n@@ -22627,1 +22627,1 @@\n-            axes(int[].class, true, null),\n+            axes(long[].class, true, null),\n@@ -22630,1 +22630,1 @@\n-            exclude_outside(Integer.class, true, 0),\n+            exclude_outside(Long.class, true, 0),\n@@ -22745,1 +22745,1 @@\n-        Resize(TypeElement resultType, Value X, java.util.Optional<Value> roi, java.util.Optional<Value> scales, java.util.Optional<Value> sizes, java.util.Optional<String> mode, java.util.Optional<Float> extrapolation_value, java.util.Optional<String> nearest_mode, java.util.Optional<Integer> antialias, java.util.Optional<Float> cubic_coeff_a, java.util.Optional<int[]> axes, java.util.Optional<String> coordinate_transformation_mode, java.util.Optional<String> keep_aspect_ratio_policy, java.util.Optional<Integer> exclude_outside) {\n+        Resize(TypeElement resultType, Value X, java.util.Optional<Value> roi, java.util.Optional<Value> scales, java.util.Optional<Value> sizes, java.util.Optional<String> mode, java.util.Optional<Float> extrapolation_value, java.util.Optional<String> nearest_mode, java.util.Optional<Long> antialias, java.util.Optional<Float> cubic_coeff_a, java.util.Optional<long[]> axes, java.util.Optional<String> coordinate_transformation_mode, java.util.Optional<String> keep_aspect_ratio_policy, java.util.Optional<Long> exclude_outside) {\n@@ -22793,2 +22793,2 @@\n-        public java.util.Optional<Integer> antialias() {\n-            Integer antialias = Attribute.antialias.access(Integer.class, onnxAttributes);\n+        public java.util.Optional<Long> antialias() {\n+            Long antialias = Attribute.antialias.access(Long.class, onnxAttributes);\n@@ -22803,3 +22803,3 @@\n-        public java.util.Optional<int[]> axes() {\n-            int[] axes = Attribute.axes.access(int[].class, onnxAttributes);\n-            return java.util.Optional.ofNullable(axes).map(int[]::clone);\n+        public java.util.Optional<long[]> axes() {\n+            long[] axes = Attribute.axes.access(long[].class, onnxAttributes);\n+            return java.util.Optional.ofNullable(axes).map(long[]::clone);\n@@ -22818,2 +22818,2 @@\n-        public java.util.Optional<Integer> exclude_outside() {\n-            Integer exclude_outside = Attribute.exclude_outside.access(Integer.class, onnxAttributes);\n+        public java.util.Optional<Long> exclude_outside() {\n+            Long exclude_outside = Attribute.exclude_outside.access(Long.class, onnxAttributes);\n@@ -22825,1 +22825,1 @@\n-    public static Resize Resize(TypeElement resultType, Value X, java.util.Optional<Value> roi, java.util.Optional<Value> scales, java.util.Optional<Value> sizes, java.util.Optional<String> mode, java.util.Optional<Float> extrapolation_value, java.util.Optional<String> nearest_mode, java.util.Optional<Integer> antialias, java.util.Optional<Float> cubic_coeff_a, java.util.Optional<int[]> axes, java.util.Optional<String> coordinate_transformation_mode, java.util.Optional<String> keep_aspect_ratio_policy, java.util.Optional<Integer> exclude_outside) {\n+    public static Resize Resize(TypeElement resultType, Value X, java.util.Optional<Value> roi, java.util.Optional<Value> scales, java.util.Optional<Value> sizes, java.util.Optional<String> mode, java.util.Optional<Float> extrapolation_value, java.util.Optional<String> nearest_mode, java.util.Optional<Long> antialias, java.util.Optional<Float> cubic_coeff_a, java.util.Optional<long[]> axes, java.util.Optional<String> coordinate_transformation_mode, java.util.Optional<String> keep_aspect_ratio_policy, java.util.Optional<Long> exclude_outside) {\n@@ -22834,2 +22834,2 @@\n-            time_axis(Integer.class, true, 0),\n-            batch_axis(Integer.class, true, 1),\n+            time_axis(Long.class, true, 0),\n+            batch_axis(Long.class, true, 1),\n@@ -22947,1 +22947,1 @@\n-        ReverseSequence(TypeElement resultType, Value input, Value sequence_lens, java.util.Optional<Integer> time_axis, java.util.Optional<Integer> batch_axis) {\n+        ReverseSequence(TypeElement resultType, Value input, Value sequence_lens, java.util.Optional<Long> time_axis, java.util.Optional<Long> batch_axis) {\n@@ -22969,2 +22969,2 @@\n-        public java.util.Optional<Integer> time_axis() {\n-            Integer time_axis = Attribute.time_axis.access(Integer.class, onnxAttributes);\n+        public java.util.Optional<Long> time_axis() {\n+            Long time_axis = Attribute.time_axis.access(Long.class, onnxAttributes);\n@@ -22974,2 +22974,2 @@\n-        public java.util.Optional<Integer> batch_axis() {\n-            Integer batch_axis = Attribute.batch_axis.access(Integer.class, onnxAttributes);\n+        public java.util.Optional<Long> batch_axis() {\n+            Long batch_axis = Attribute.batch_axis.access(Long.class, onnxAttributes);\n@@ -22981,1 +22981,1 @@\n-    public static ReverseSequence ReverseSequence(TypeElement resultType, Value input, Value sequence_lens, java.util.Optional<Integer> time_axis, java.util.Optional<Integer> batch_axis) {\n+    public static ReverseSequence ReverseSequence(TypeElement resultType, Value input, Value sequence_lens, java.util.Optional<Long> time_axis, java.util.Optional<Long> batch_axis) {\n@@ -22991,1 +22991,1 @@\n-            output_width(Integer.class, true, 1),\n+            output_width(Long.class, true, 1),\n@@ -22994,2 +22994,2 @@\n-            sampling_ratio(Integer.class, true, 0),\n-            output_height(Integer.class, true, 1),\n+            sampling_ratio(Long.class, true, 0),\n+            output_height(Long.class, true, 1),\n@@ -23109,1 +23109,1 @@\n-        RoiAlign(TypeElement resultType, Value X, Value rois, Value batch_indices, java.util.Optional<String> mode, java.util.Optional<Integer> output_width, java.util.Optional<Float> spatial_scale, java.util.Optional<String> coordinate_transformation_mode, java.util.Optional<Integer> sampling_ratio, java.util.Optional<Integer> output_height) {\n+        RoiAlign(TypeElement resultType, Value X, Value rois, Value batch_indices, java.util.Optional<String> mode, java.util.Optional<Long> output_width, java.util.Optional<Float> spatial_scale, java.util.Optional<String> coordinate_transformation_mode, java.util.Optional<Long> sampling_ratio, java.util.Optional<Long> output_height) {\n@@ -23140,2 +23140,2 @@\n-        public java.util.Optional<Integer> output_width() {\n-            Integer output_width = Attribute.output_width.access(Integer.class, onnxAttributes);\n+        public java.util.Optional<Long> output_width() {\n+            Long output_width = Attribute.output_width.access(Long.class, onnxAttributes);\n@@ -23155,2 +23155,2 @@\n-        public java.util.Optional<Integer> sampling_ratio() {\n-            Integer sampling_ratio = Attribute.sampling_ratio.access(Integer.class, onnxAttributes);\n+        public java.util.Optional<Long> sampling_ratio() {\n+            Long sampling_ratio = Attribute.sampling_ratio.access(Long.class, onnxAttributes);\n@@ -23160,2 +23160,2 @@\n-        public java.util.Optional<Integer> output_height() {\n-            Integer output_height = Attribute.output_height.access(Integer.class, onnxAttributes);\n+        public java.util.Optional<Long> output_height() {\n+            Long output_height = Attribute.output_height.access(Long.class, onnxAttributes);\n@@ -23167,1 +23167,1 @@\n-    public static RoiAlign RoiAlign(TypeElement resultType, Value X, Value rois, Value batch_indices, java.util.Optional<String> mode, java.util.Optional<Integer> output_width, java.util.Optional<Float> spatial_scale, java.util.Optional<String> coordinate_transformation_mode, java.util.Optional<Integer> sampling_ratio, java.util.Optional<Integer> output_height) {\n+    public static RoiAlign RoiAlign(TypeElement resultType, Value X, Value rois, Value batch_indices, java.util.Optional<String> mode, java.util.Optional<Long> output_width, java.util.Optional<Float> spatial_scale, java.util.Optional<String> coordinate_transformation_mode, java.util.Optional<Long> sampling_ratio, java.util.Optional<Long> output_height) {\n@@ -23290,1 +23290,1 @@\n-            onesided(Integer.class, true, 1),\n+            onesided(Long.class, true, 1),\n@@ -23405,1 +23405,1 @@\n-        STFT(TypeElement resultType, Value signal, Value frame_step, java.util.Optional<Value> window, java.util.Optional<Value> frame_length, java.util.Optional<Integer> onesided) {\n+        STFT(TypeElement resultType, Value signal, Value frame_step, java.util.Optional<Value> window, java.util.Optional<Value> frame_length, java.util.Optional<Long> onesided) {\n@@ -23437,2 +23437,2 @@\n-        public java.util.Optional<Integer> onesided() {\n-            Integer onesided = Attribute.onesided.access(Integer.class, onnxAttributes);\n+        public java.util.Optional<Long> onesided() {\n+            Long onesided = Attribute.onesided.access(Long.class, onnxAttributes);\n@@ -23444,1 +23444,1 @@\n-    public static STFT STFT(TypeElement resultType, Value signal, Value frame_step, java.util.Optional<Value> window, java.util.Optional<Value> frame_length, java.util.Optional<Integer> onesided) {\n+    public static STFT STFT(TypeElement resultType, Value signal, Value frame_step, java.util.Optional<Value> window, java.util.Optional<Value> frame_length, java.util.Optional<Long> onesided) {\n@@ -23456,1 +23456,1 @@\n-            classlabels_ints(int[].class, true, null),\n+            classlabels_ints(long[].class, true, null),\n@@ -23461,1 +23461,1 @@\n-            vectors_per_class(int[].class, true, null),\n+            vectors_per_class(long[].class, true, null),\n@@ -23576,1 +23576,1 @@\n-        SVMClassifier(TypeElement resultType, Value X, java.util.Optional<float[]> prob_b, java.util.Optional<float[]> kernel_params, java.util.Optional<String> kernel_type, java.util.Optional<int[]> classlabels_ints, java.util.Optional<String> post_transform, java.util.Optional<float[]> rho, java.util.Optional<float[]> coefficients, java.util.Optional<float[]> support_vectors, java.util.Optional<int[]> vectors_per_class, java.util.Optional<float[]> prob_a, java.util.Optional<String[]> classlabels_strings) {\n+        SVMClassifier(TypeElement resultType, Value X, java.util.Optional<float[]> prob_b, java.util.Optional<float[]> kernel_params, java.util.Optional<String> kernel_type, java.util.Optional<long[]> classlabels_ints, java.util.Optional<String> post_transform, java.util.Optional<float[]> rho, java.util.Optional<float[]> coefficients, java.util.Optional<float[]> support_vectors, java.util.Optional<long[]> vectors_per_class, java.util.Optional<float[]> prob_a, java.util.Optional<String[]> classlabels_strings) {\n@@ -23609,3 +23609,3 @@\n-        public java.util.Optional<int[]> classlabels_ints() {\n-            int[] classlabels_ints = Attribute.classlabels_ints.access(int[].class, onnxAttributes);\n-            return java.util.Optional.ofNullable(classlabels_ints).map(int[]::clone);\n+        public java.util.Optional<long[]> classlabels_ints() {\n+            long[] classlabels_ints = Attribute.classlabels_ints.access(long[].class, onnxAttributes);\n+            return java.util.Optional.ofNullable(classlabels_ints).map(long[]::clone);\n@@ -23634,3 +23634,3 @@\n-        public java.util.Optional<int[]> vectors_per_class() {\n-            int[] vectors_per_class = Attribute.vectors_per_class.access(int[].class, onnxAttributes);\n-            return java.util.Optional.ofNullable(vectors_per_class).map(int[]::clone);\n+        public java.util.Optional<long[]> vectors_per_class() {\n+            long[] vectors_per_class = Attribute.vectors_per_class.access(long[].class, onnxAttributes);\n+            return java.util.Optional.ofNullable(vectors_per_class).map(long[]::clone);\n@@ -23651,1 +23651,1 @@\n-    public static SVMClassifier SVMClassifier(TypeElement resultType, Value X, java.util.Optional<float[]> prob_b, java.util.Optional<float[]> kernel_params, java.util.Optional<String> kernel_type, java.util.Optional<int[]> classlabels_ints, java.util.Optional<String> post_transform, java.util.Optional<float[]> rho, java.util.Optional<float[]> coefficients, java.util.Optional<float[]> support_vectors, java.util.Optional<int[]> vectors_per_class, java.util.Optional<float[]> prob_a, java.util.Optional<String[]> classlabels_strings) {\n+    public static SVMClassifier SVMClassifier(TypeElement resultType, Value X, java.util.Optional<float[]> prob_b, java.util.Optional<float[]> kernel_params, java.util.Optional<String> kernel_type, java.util.Optional<long[]> classlabels_ints, java.util.Optional<String> post_transform, java.util.Optional<float[]> rho, java.util.Optional<float[]> coefficients, java.util.Optional<float[]> support_vectors, java.util.Optional<long[]> vectors_per_class, java.util.Optional<float[]> prob_a, java.util.Optional<String[]> classlabels_strings) {\n@@ -23662,1 +23662,1 @@\n-            n_supports(Integer.class, true, 0),\n+            n_supports(Long.class, true, 0),\n@@ -23667,1 +23667,1 @@\n-            one_class(Integer.class, true, 0),\n+            one_class(Long.class, true, 0),\n@@ -23778,1 +23778,1 @@\n-        SVMRegressor(TypeElement resultType, Value X, java.util.Optional<String> kernel_type, java.util.Optional<float[]> kernel_params, java.util.Optional<Integer> n_supports, java.util.Optional<float[]> rho, java.util.Optional<String> post_transform, java.util.Optional<float[]> coefficients, java.util.Optional<float[]> support_vectors, java.util.Optional<Integer> one_class) {\n+        SVMRegressor(TypeElement resultType, Value X, java.util.Optional<String> kernel_type, java.util.Optional<float[]> kernel_params, java.util.Optional<Long> n_supports, java.util.Optional<float[]> rho, java.util.Optional<String> post_transform, java.util.Optional<float[]> coefficients, java.util.Optional<float[]> support_vectors, java.util.Optional<Long> one_class) {\n@@ -23806,2 +23806,2 @@\n-        public java.util.Optional<Integer> n_supports() {\n-            Integer n_supports = Attribute.n_supports.access(Integer.class, onnxAttributes);\n+        public java.util.Optional<Long> n_supports() {\n+            Long n_supports = Attribute.n_supports.access(Long.class, onnxAttributes);\n@@ -23831,2 +23831,2 @@\n-        public java.util.Optional<Integer> one_class() {\n-            Integer one_class = Attribute.one_class.access(Integer.class, onnxAttributes);\n+        public java.util.Optional<Long> one_class() {\n+            Long one_class = Attribute.one_class.access(Long.class, onnxAttributes);\n@@ -23838,1 +23838,1 @@\n-    public static SVMRegressor SVMRegressor(TypeElement resultType, Value X, java.util.Optional<String> kernel_type, java.util.Optional<float[]> kernel_params, java.util.Optional<Integer> n_supports, java.util.Optional<float[]> rho, java.util.Optional<String> post_transform, java.util.Optional<float[]> coefficients, java.util.Optional<float[]> support_vectors, java.util.Optional<Integer> one_class) {\n+    public static SVMRegressor SVMRegressor(TypeElement resultType, Value X, java.util.Optional<String> kernel_type, java.util.Optional<float[]> kernel_params, java.util.Optional<Long> n_supports, java.util.Optional<float[]> rho, java.util.Optional<String> post_transform, java.util.Optional<float[]> coefficients, java.util.Optional<float[]> support_vectors, java.util.Optional<Long> one_class) {\n@@ -23998,1 +23998,1 @@\n-            axis(Integer.class, true, 0),\n+            axis(Long.class, true, 0),\n@@ -24112,1 +24112,1 @@\n-        Scatter(TypeElement resultType, Value data, Value indices, Value updates, java.util.Optional<Integer> axis) {\n+        Scatter(TypeElement resultType, Value data, Value indices, Value updates, java.util.Optional<Long> axis) {\n@@ -24138,2 +24138,2 @@\n-        public java.util.Optional<Integer> axis() {\n-            Integer axis = Attribute.axis.access(Integer.class, onnxAttributes);\n+        public java.util.Optional<Long> axis() {\n+            Long axis = Attribute.axis.access(Long.class, onnxAttributes);\n@@ -24145,1 +24145,1 @@\n-    public static Scatter Scatter(TypeElement resultType, Value data, Value indices, Value updates, java.util.Optional<Integer> axis) {\n+    public static Scatter Scatter(TypeElement resultType, Value data, Value indices, Value updates, java.util.Optional<Long> axis) {\n@@ -24155,1 +24155,1 @@\n-            axis(Integer.class, true, 0),\n+            axis(Long.class, true, 0),\n@@ -24269,1 +24269,1 @@\n-        ScatterElements(TypeElement resultType, Value data, Value indices, Value updates, java.util.Optional<String> reduction, java.util.Optional<Integer> axis) {\n+        ScatterElements(TypeElement resultType, Value data, Value indices, Value updates, java.util.Optional<String> reduction, java.util.Optional<Long> axis) {\n@@ -24300,2 +24300,2 @@\n-        public java.util.Optional<Integer> axis() {\n-            Integer axis = Attribute.axis.access(Integer.class, onnxAttributes);\n+        public java.util.Optional<Long> axis() {\n+            Long axis = Attribute.axis.access(Long.class, onnxAttributes);\n@@ -24307,1 +24307,1 @@\n-    public static ScatterElements ScatterElements(TypeElement resultType, Value data, Value indices, Value updates, java.util.Optional<String> reduction, java.util.Optional<Integer> axis) {\n+    public static ScatterElements ScatterElements(TypeElement resultType, Value data, Value indices, Value updates, java.util.Optional<String> reduction, java.util.Optional<Long> axis) {\n@@ -24858,1 +24858,1 @@\n-            dtype(Integer.class, true, null),\n+            dtype(Long.class, true, null),\n@@ -24948,1 +24948,1 @@\n-        SequenceEmpty(TypeElement resultType, java.util.Optional<Integer> dtype) {\n+        SequenceEmpty(TypeElement resultType, java.util.Optional<Long> dtype) {\n@@ -24962,2 +24962,2 @@\n-        public java.util.Optional<Integer> dtype() {\n-            Integer dtype = Attribute.dtype.access(Integer.class, onnxAttributes);\n+        public java.util.Optional<Long> dtype() {\n+            Long dtype = Attribute.dtype.access(Long.class, onnxAttributes);\n@@ -24969,1 +24969,1 @@\n-    public static SequenceEmpty SequenceEmpty(TypeElement resultType, java.util.Optional<Integer> dtype) {\n+    public static SequenceEmpty SequenceEmpty(TypeElement resultType, java.util.Optional<Long> dtype) {\n@@ -25341,2 +25341,2 @@\n-            start(Integer.class, true, 0),\n-            end(Integer.class, true, null),\n+            start(Long.class, true, 0),\n+            end(Long.class, true, null),\n@@ -25454,1 +25454,1 @@\n-        Shape(TypeElement resultType, Value data, java.util.Optional<Integer> start, java.util.Optional<Integer> end) {\n+        Shape(TypeElement resultType, Value data, java.util.Optional<Long> start, java.util.Optional<Long> end) {\n@@ -25472,2 +25472,2 @@\n-        public java.util.Optional<Integer> start() {\n-            Integer start = Attribute.start.access(Integer.class, onnxAttributes);\n+        public java.util.Optional<Long> start() {\n+            Long start = Attribute.start.access(Long.class, onnxAttributes);\n@@ -25477,2 +25477,2 @@\n-        public java.util.Optional<Integer> end() {\n-            Integer end = Attribute.end.access(Integer.class, onnxAttributes);\n+        public java.util.Optional<Long> end() {\n+            Long end = Attribute.end.access(Long.class, onnxAttributes);\n@@ -25484,1 +25484,1 @@\n-    public static Shape Shape(TypeElement resultType, Value data, java.util.Optional<Integer> start, java.util.Optional<Integer> end) {\n+    public static Shape Shape(TypeElement resultType, Value data, java.util.Optional<Long> start, java.util.Optional<Long> end) {\n@@ -26352,1 +26352,1 @@\n-            axis(Integer.class, true, -1),\n+            axis(Long.class, true, -1),\n@@ -26463,1 +26463,1 @@\n-        Softmax(TypeElement resultType, Value input, java.util.Optional<Integer> axis) {\n+        Softmax(TypeElement resultType, Value input, java.util.Optional<Long> axis) {\n@@ -26481,2 +26481,2 @@\n-        public java.util.Optional<Integer> axis() {\n-            Integer axis = Attribute.axis.access(Integer.class, onnxAttributes);\n+        public java.util.Optional<Long> axis() {\n+            Long axis = Attribute.axis.access(Long.class, onnxAttributes);\n@@ -26488,1 +26488,1 @@\n-    public static Softmax Softmax(TypeElement resultType, Value input, java.util.Optional<Integer> axis) {\n+    public static Softmax Softmax(TypeElement resultType, Value input, java.util.Optional<Long> axis) {\n@@ -26497,1 +26497,1 @@\n-            ignore_index(Integer.class, true, null),\n+            ignore_index(Long.class, true, null),\n@@ -26613,1 +26613,1 @@\n-        SoftmaxCrossEntropyLoss(TypeElement resultType, Set<OutputParameter> optionalOutputs, Value scores, Value labels, java.util.Optional<Value> weights, java.util.Optional<Integer> ignore_index, java.util.Optional<String> reduction) {\n+        SoftmaxCrossEntropyLoss(TypeElement resultType, Set<OutputParameter> optionalOutputs, Value scores, Value labels, java.util.Optional<Value> weights, java.util.Optional<Long> ignore_index, java.util.Optional<String> reduction) {\n@@ -26640,2 +26640,2 @@\n-        public java.util.Optional<Integer> ignore_index() {\n-            Integer ignore_index = Attribute.ignore_index.access(Integer.class, onnxAttributes);\n+        public java.util.Optional<Long> ignore_index() {\n+            Long ignore_index = Attribute.ignore_index.access(Long.class, onnxAttributes);\n@@ -26652,1 +26652,1 @@\n-    public static SoftmaxCrossEntropyLoss SoftmaxCrossEntropyLoss(TypeElement resultType, Set<SoftmaxCrossEntropyLoss.OutputParameter> optionalOutputs, Value scores, Value labels, java.util.Optional<Value> weights, java.util.Optional<Integer> ignore_index, java.util.Optional<String> reduction) {\n+    public static SoftmaxCrossEntropyLoss SoftmaxCrossEntropyLoss(TypeElement resultType, Set<SoftmaxCrossEntropyLoss.OutputParameter> optionalOutputs, Value scores, Value labels, java.util.Optional<Value> weights, java.util.Optional<Long> ignore_index, java.util.Optional<String> reduction) {\n@@ -26889,1 +26889,1 @@\n-            blocksize(Integer.class, false, null),\n+            blocksize(Long.class, false, null),\n@@ -27000,1 +27000,1 @@\n-        SpaceToDepth(TypeElement resultType, Value input, int blocksize) {\n+        SpaceToDepth(TypeElement resultType, Value input, long blocksize) {\n@@ -27018,2 +27018,2 @@\n-        public int blocksize() {\n-            int blocksize = Attribute.blocksize.access(int.class, onnxAttributes);\n+        public long blocksize() {\n+            long blocksize = Attribute.blocksize.access(long.class, onnxAttributes);\n@@ -27025,1 +27025,1 @@\n-    public static SpaceToDepth SpaceToDepth(TypeElement resultType, Value input, int blocksize) {\n+    public static SpaceToDepth SpaceToDepth(TypeElement resultType, Value input, long blocksize) {\n@@ -27034,2 +27034,2 @@\n-            num_outputs(Integer.class, true, null),\n-            axis(Integer.class, true, 0),\n+            num_outputs(Long.class, true, null),\n+            axis(Long.class, true, 0),\n@@ -27147,1 +27147,1 @@\n-        Split(TypeElement resultType, Value input, java.util.Optional<Value> split, java.util.Optional<Integer> num_outputs, java.util.Optional<Integer> axis) {\n+        Split(TypeElement resultType, Value input, java.util.Optional<Value> split, java.util.Optional<Long> num_outputs, java.util.Optional<Long> axis) {\n@@ -27170,2 +27170,2 @@\n-        public java.util.Optional<Integer> num_outputs() {\n-            Integer num_outputs = Attribute.num_outputs.access(Integer.class, onnxAttributes);\n+        public java.util.Optional<Long> num_outputs() {\n+            Long num_outputs = Attribute.num_outputs.access(Long.class, onnxAttributes);\n@@ -27175,2 +27175,2 @@\n-        public java.util.Optional<Integer> axis() {\n-            Integer axis = Attribute.axis.access(Integer.class, onnxAttributes);\n+        public java.util.Optional<Long> axis() {\n+            Long axis = Attribute.axis.access(Long.class, onnxAttributes);\n@@ -27182,1 +27182,1 @@\n-    public static Split Split(TypeElement resultType, Value input, java.util.Optional<Value> split, java.util.Optional<Integer> num_outputs, java.util.Optional<Integer> axis) {\n+    public static Split Split(TypeElement resultType, Value input, java.util.Optional<Value> split, java.util.Optional<Long> num_outputs, java.util.Optional<Long> axis) {\n@@ -27191,2 +27191,2 @@\n-            keepdims(Integer.class, true, 1),\n-            axis(Integer.class, true, 0),\n+            keepdims(Long.class, true, 1),\n+            axis(Long.class, true, 0),\n@@ -27306,1 +27306,1 @@\n-        SplitToSequence(TypeElement resultType, Value input, java.util.Optional<Value> split, java.util.Optional<Integer> keepdims, java.util.Optional<Integer> axis) {\n+        SplitToSequence(TypeElement resultType, Value input, java.util.Optional<Value> split, java.util.Optional<Long> keepdims, java.util.Optional<Long> axis) {\n@@ -27329,2 +27329,2 @@\n-        public java.util.Optional<Integer> keepdims() {\n-            Integer keepdims = Attribute.keepdims.access(Integer.class, onnxAttributes);\n+        public java.util.Optional<Long> keepdims() {\n+            Long keepdims = Attribute.keepdims.access(Long.class, onnxAttributes);\n@@ -27334,2 +27334,2 @@\n-        public java.util.Optional<Integer> axis() {\n-            Integer axis = Attribute.axis.access(Integer.class, onnxAttributes);\n+        public java.util.Optional<Long> axis() {\n+            Long axis = Attribute.axis.access(Long.class, onnxAttributes);\n@@ -27341,1 +27341,1 @@\n-    public static SplitToSequence SplitToSequence(TypeElement resultType, Value input, java.util.Optional<Value> split, java.util.Optional<Integer> keepdims, java.util.Optional<Integer> axis) {\n+    public static SplitToSequence SplitToSequence(TypeElement resultType, Value input, java.util.Optional<Value> split, java.util.Optional<Long> keepdims, java.util.Optional<Long> axis) {\n@@ -27703,1 +27703,1 @@\n-            is_case_sensitive(Integer.class, true, 0),\n+            is_case_sensitive(Long.class, true, 0),\n@@ -27802,1 +27802,1 @@\n-        StringNormalizer(TypeElement resultType, Value X, java.util.Optional<Integer> is_case_sensitive, java.util.Optional<String> locale, java.util.Optional<String[]> stopwords, java.util.Optional<String> case_change_action) {\n+        StringNormalizer(TypeElement resultType, Value X, java.util.Optional<Long> is_case_sensitive, java.util.Optional<String> locale, java.util.Optional<String[]> stopwords, java.util.Optional<String> case_change_action) {\n@@ -27820,2 +27820,2 @@\n-        public java.util.Optional<Integer> is_case_sensitive() {\n-            Integer is_case_sensitive = Attribute.is_case_sensitive.access(Integer.class, onnxAttributes);\n+        public java.util.Optional<Long> is_case_sensitive() {\n+            Long is_case_sensitive = Attribute.is_case_sensitive.access(Long.class, onnxAttributes);\n@@ -27842,1 +27842,1 @@\n-    public static StringNormalizer StringNormalizer(TypeElement resultType, Value X, java.util.Optional<Integer> is_case_sensitive, java.util.Optional<String> locale, java.util.Optional<String[]> stopwords, java.util.Optional<String> case_change_action) {\n+    public static StringNormalizer StringNormalizer(TypeElement resultType, Value X, java.util.Optional<Long> is_case_sensitive, java.util.Optional<String> locale, java.util.Optional<String[]> stopwords, java.util.Optional<String> case_change_action) {\n@@ -27852,1 +27852,1 @@\n-            maxsplit(Integer.class, true, null),\n+            maxsplit(Long.class, true, null),\n@@ -27966,1 +27966,1 @@\n-        StringSplit(TypeElement resultType, Value X, java.util.Optional<String> delimiter, java.util.Optional<Integer> maxsplit) {\n+        StringSplit(TypeElement resultType, Value X, java.util.Optional<String> delimiter, java.util.Optional<Long> maxsplit) {\n@@ -27989,2 +27989,2 @@\n-        public java.util.Optional<Integer> maxsplit() {\n-            Integer maxsplit = Attribute.maxsplit.access(Integer.class, onnxAttributes);\n+        public java.util.Optional<Long> maxsplit() {\n+            Long maxsplit = Attribute.maxsplit.access(Long.class, onnxAttributes);\n@@ -27996,1 +27996,1 @@\n-    public static StringSplit StringSplit(TypeElement resultType, Value X, java.util.Optional<String> delimiter, java.util.Optional<Integer> maxsplit) {\n+    public static StringSplit StringSplit(TypeElement resultType, Value X, java.util.Optional<String> delimiter, java.util.Optional<Long> maxsplit) {\n@@ -28466,2 +28466,2 @@\n-            ngram_counts(int[].class, false, null),\n-            min_gram_length(Integer.class, false, null),\n+            ngram_counts(long[].class, false, null),\n+            min_gram_length(Long.class, false, null),\n@@ -28470,3 +28470,3 @@\n-            max_gram_length(Integer.class, false, null),\n-            max_skip_count(Integer.class, false, null),\n-            pool_int64s(int[].class, true, null),\n+            max_gram_length(Long.class, false, null),\n+            max_skip_count(Long.class, false, null),\n+            pool_int64s(long[].class, true, null),\n@@ -28474,1 +28474,1 @@\n-            ngram_indexes(int[].class, false, null),\n+            ngram_indexes(long[].class, false, null),\n@@ -28586,1 +28586,1 @@\n-        TfIdfVectorizer(TypeElement resultType, Value X, int[] ngram_counts, int min_gram_length, java.util.Optional<String[]> pool_strings, String mode, int max_gram_length, int max_skip_count, java.util.Optional<int[]> pool_int64s, java.util.Optional<float[]> weights, int[] ngram_indexes) {\n+        TfIdfVectorizer(TypeElement resultType, Value X, long[] ngram_counts, long min_gram_length, java.util.Optional<String[]> pool_strings, String mode, long max_gram_length, long max_skip_count, java.util.Optional<long[]> pool_int64s, java.util.Optional<float[]> weights, long[] ngram_indexes) {\n@@ -28604,2 +28604,2 @@\n-        public int[] ngram_counts() {\n-            int[] ngram_counts = Attribute.ngram_counts.access(int[].class, onnxAttributes);\n+        public long[] ngram_counts() {\n+            long[] ngram_counts = Attribute.ngram_counts.access(long[].class, onnxAttributes);\n@@ -28609,2 +28609,2 @@\n-        public int min_gram_length() {\n-            int min_gram_length = Attribute.min_gram_length.access(int.class, onnxAttributes);\n+        public long min_gram_length() {\n+            long min_gram_length = Attribute.min_gram_length.access(long.class, onnxAttributes);\n@@ -28624,2 +28624,2 @@\n-        public int max_gram_length() {\n-            int max_gram_length = Attribute.max_gram_length.access(int.class, onnxAttributes);\n+        public long max_gram_length() {\n+            long max_gram_length = Attribute.max_gram_length.access(long.class, onnxAttributes);\n@@ -28629,2 +28629,2 @@\n-        public int max_skip_count() {\n-            int max_skip_count = Attribute.max_skip_count.access(int.class, onnxAttributes);\n+        public long max_skip_count() {\n+            long max_skip_count = Attribute.max_skip_count.access(long.class, onnxAttributes);\n@@ -28634,3 +28634,3 @@\n-        public java.util.Optional<int[]> pool_int64s() {\n-            int[] pool_int64s = Attribute.pool_int64s.access(int[].class, onnxAttributes);\n-            return java.util.Optional.ofNullable(pool_int64s).map(int[]::clone);\n+        public java.util.Optional<long[]> pool_int64s() {\n+            long[] pool_int64s = Attribute.pool_int64s.access(long[].class, onnxAttributes);\n+            return java.util.Optional.ofNullable(pool_int64s).map(long[]::clone);\n@@ -28644,2 +28644,2 @@\n-        public int[] ngram_indexes() {\n-            int[] ngram_indexes = Attribute.ngram_indexes.access(int[].class, onnxAttributes);\n+        public long[] ngram_indexes() {\n+            long[] ngram_indexes = Attribute.ngram_indexes.access(long[].class, onnxAttributes);\n@@ -28651,1 +28651,1 @@\n-    public static TfIdfVectorizer TfIdfVectorizer(TypeElement resultType, Value X, int[] ngram_counts, int min_gram_length, java.util.Optional<String[]> pool_strings, String mode, int max_gram_length, int max_skip_count, java.util.Optional<int[]> pool_int64s, java.util.Optional<float[]> weights, int[] ngram_indexes) {\n+    public static TfIdfVectorizer TfIdfVectorizer(TypeElement resultType, Value X, long[] ngram_counts, long min_gram_length, java.util.Optional<String[]> pool_strings, String mode, long max_gram_length, long max_skip_count, java.util.Optional<long[]> pool_int64s, java.util.Optional<float[]> weights, long[] ngram_indexes) {\n@@ -28925,3 +28925,3 @@\n-            largest(Integer.class, true, 1),\n-            sorted(Integer.class, true, 1),\n-            axis(Integer.class, true, -1),\n+            largest(Long.class, true, 1),\n+            sorted(Long.class, true, 1),\n+            axis(Long.class, true, -1),\n@@ -29041,1 +29041,1 @@\n-        TopK(TypeElement resultType, Value X, Value K, java.util.Optional<Integer> largest, java.util.Optional<Integer> sorted, java.util.Optional<Integer> axis) {\n+        TopK(TypeElement resultType, Value X, Value K, java.util.Optional<Long> largest, java.util.Optional<Long> sorted, java.util.Optional<Long> axis) {\n@@ -29063,2 +29063,2 @@\n-        public java.util.Optional<Integer> largest() {\n-            Integer largest = Attribute.largest.access(Integer.class, onnxAttributes);\n+        public java.util.Optional<Long> largest() {\n+            Long largest = Attribute.largest.access(Long.class, onnxAttributes);\n@@ -29068,2 +29068,2 @@\n-        public java.util.Optional<Integer> sorted() {\n-            Integer sorted = Attribute.sorted.access(Integer.class, onnxAttributes);\n+        public java.util.Optional<Long> sorted() {\n+            Long sorted = Attribute.sorted.access(Long.class, onnxAttributes);\n@@ -29073,2 +29073,2 @@\n-        public java.util.Optional<Integer> axis() {\n-            Integer axis = Attribute.axis.access(Integer.class, onnxAttributes);\n+        public java.util.Optional<Long> axis() {\n+            Long axis = Attribute.axis.access(Long.class, onnxAttributes);\n@@ -29080,1 +29080,1 @@\n-    public static TopK TopK(TypeElement resultType, Value X, Value K, java.util.Optional<Integer> largest, java.util.Optional<Integer> sorted, java.util.Optional<Integer> axis) {\n+    public static TopK TopK(TypeElement resultType, Value X, Value K, java.util.Optional<Long> largest, java.util.Optional<Long> sorted, java.util.Optional<Long> axis) {\n@@ -29089,1 +29089,1 @@\n-            perm(int[].class, true, null),\n+            perm(long[].class, true, null),\n@@ -29200,1 +29200,1 @@\n-        Transpose(TypeElement resultType, Value data, java.util.Optional<int[]> perm) {\n+        Transpose(TypeElement resultType, Value data, java.util.Optional<long[]> perm) {\n@@ -29218,3 +29218,3 @@\n-        public java.util.Optional<int[]> perm() {\n-            int[] perm = Attribute.perm.access(int[].class, onnxAttributes);\n-            return java.util.Optional.ofNullable(perm).map(int[]::clone);\n+        public java.util.Optional<long[]> perm() {\n+            long[] perm = Attribute.perm.access(long[].class, onnxAttributes);\n+            return java.util.Optional.ofNullable(perm).map(long[]::clone);\n@@ -29225,1 +29225,1 @@\n-    public static Transpose Transpose(TypeElement resultType, Value data, java.util.Optional<int[]> perm) {\n+    public static Transpose Transpose(TypeElement resultType, Value data, java.util.Optional<long[]> perm) {\n@@ -29234,1 +29234,1 @@\n-            aggregate_function(Integer.class, true, 1),\n+            aggregate_function(Long.class, true, 1),\n@@ -29236,4 +29236,4 @@\n-            nodes_featureids(int[].class, false, null),\n-            nodes_falseleafs(int[].class, false, null),\n-            post_transform(Integer.class, true, 0),\n-            nodes_trueleafs(int[].class, false, null),\n+            nodes_featureids(long[].class, false, null),\n+            nodes_falseleafs(long[].class, false, null),\n+            post_transform(Long.class, true, 0),\n+            nodes_trueleafs(long[].class, false, null),\n@@ -29241,2 +29241,2 @@\n-            nodes_falsenodeids(int[].class, false, null),\n-            nodes_truenodeids(int[].class, false, null),\n+            nodes_falsenodeids(long[].class, false, null),\n+            nodes_truenodeids(long[].class, false, null),\n@@ -29244,4 +29244,4 @@\n-            leaf_targetids(int[].class, false, null),\n-            tree_roots(int[].class, false, null),\n-            n_targets(Integer.class, true, null),\n-            nodes_missing_value_tracks_true(int[].class, true, null),\n+            leaf_targetids(long[].class, false, null),\n+            tree_roots(long[].class, false, null),\n+            n_targets(Long.class, true, null),\n+            nodes_missing_value_tracks_true(long[].class, true, null),\n@@ -29360,1 +29360,1 @@\n-        TreeEnsemble(TypeElement resultType, Value X, java.util.Optional<Integer> aggregate_function, java.util.Optional<byte[]> nodes_hitrates, int[] nodes_featureids, int[] nodes_falseleafs, java.util.Optional<Integer> post_transform, int[] nodes_trueleafs, byte[] nodes_modes, int[] nodes_falsenodeids, int[] nodes_truenodeids, byte[] leaf_weights, int[] leaf_targetids, int[] tree_roots, java.util.Optional<Integer> n_targets, java.util.Optional<int[]> nodes_missing_value_tracks_true, java.util.Optional<byte[]> membership_values, byte[] nodes_splits) {\n+        TreeEnsemble(TypeElement resultType, Value X, java.util.Optional<Long> aggregate_function, java.util.Optional<byte[]> nodes_hitrates, long[] nodes_featureids, long[] nodes_falseleafs, java.util.Optional<Long> post_transform, long[] nodes_trueleafs, byte[] nodes_modes, long[] nodes_falsenodeids, long[] nodes_truenodeids, byte[] leaf_weights, long[] leaf_targetids, long[] tree_roots, java.util.Optional<Long> n_targets, java.util.Optional<long[]> nodes_missing_value_tracks_true, java.util.Optional<byte[]> membership_values, byte[] nodes_splits) {\n@@ -29378,2 +29378,2 @@\n-        public java.util.Optional<Integer> aggregate_function() {\n-            Integer aggregate_function = Attribute.aggregate_function.access(Integer.class, onnxAttributes);\n+        public java.util.Optional<Long> aggregate_function() {\n+            Long aggregate_function = Attribute.aggregate_function.access(Long.class, onnxAttributes);\n@@ -29388,2 +29388,2 @@\n-        public int[] nodes_featureids() {\n-            int[] nodes_featureids = Attribute.nodes_featureids.access(int[].class, onnxAttributes);\n+        public long[] nodes_featureids() {\n+            long[] nodes_featureids = Attribute.nodes_featureids.access(long[].class, onnxAttributes);\n@@ -29393,2 +29393,2 @@\n-        public int[] nodes_falseleafs() {\n-            int[] nodes_falseleafs = Attribute.nodes_falseleafs.access(int[].class, onnxAttributes);\n+        public long[] nodes_falseleafs() {\n+            long[] nodes_falseleafs = Attribute.nodes_falseleafs.access(long[].class, onnxAttributes);\n@@ -29398,2 +29398,2 @@\n-        public java.util.Optional<Integer> post_transform() {\n-            Integer post_transform = Attribute.post_transform.access(Integer.class, onnxAttributes);\n+        public java.util.Optional<Long> post_transform() {\n+            Long post_transform = Attribute.post_transform.access(Long.class, onnxAttributes);\n@@ -29403,2 +29403,2 @@\n-        public int[] nodes_trueleafs() {\n-            int[] nodes_trueleafs = Attribute.nodes_trueleafs.access(int[].class, onnxAttributes);\n+        public long[] nodes_trueleafs() {\n+            long[] nodes_trueleafs = Attribute.nodes_trueleafs.access(long[].class, onnxAttributes);\n@@ -29413,2 +29413,2 @@\n-        public int[] nodes_falsenodeids() {\n-            int[] nodes_falsenodeids = Attribute.nodes_falsenodeids.access(int[].class, onnxAttributes);\n+        public long[] nodes_falsenodeids() {\n+            long[] nodes_falsenodeids = Attribute.nodes_falsenodeids.access(long[].class, onnxAttributes);\n@@ -29418,2 +29418,2 @@\n-        public int[] nodes_truenodeids() {\n-            int[] nodes_truenodeids = Attribute.nodes_truenodeids.access(int[].class, onnxAttributes);\n+        public long[] nodes_truenodeids() {\n+            long[] nodes_truenodeids = Attribute.nodes_truenodeids.access(long[].class, onnxAttributes);\n@@ -29428,2 +29428,2 @@\n-        public int[] leaf_targetids() {\n-            int[] leaf_targetids = Attribute.leaf_targetids.access(int[].class, onnxAttributes);\n+        public long[] leaf_targetids() {\n+            long[] leaf_targetids = Attribute.leaf_targetids.access(long[].class, onnxAttributes);\n@@ -29433,2 +29433,2 @@\n-        public int[] tree_roots() {\n-            int[] tree_roots = Attribute.tree_roots.access(int[].class, onnxAttributes);\n+        public long[] tree_roots() {\n+            long[] tree_roots = Attribute.tree_roots.access(long[].class, onnxAttributes);\n@@ -29438,2 +29438,2 @@\n-        public java.util.Optional<Integer> n_targets() {\n-            Integer n_targets = Attribute.n_targets.access(Integer.class, onnxAttributes);\n+        public java.util.Optional<Long> n_targets() {\n+            Long n_targets = Attribute.n_targets.access(Long.class, onnxAttributes);\n@@ -29443,3 +29443,3 @@\n-        public java.util.Optional<int[]> nodes_missing_value_tracks_true() {\n-            int[] nodes_missing_value_tracks_true = Attribute.nodes_missing_value_tracks_true.access(int[].class, onnxAttributes);\n-            return java.util.Optional.ofNullable(nodes_missing_value_tracks_true).map(int[]::clone);\n+        public java.util.Optional<long[]> nodes_missing_value_tracks_true() {\n+            long[] nodes_missing_value_tracks_true = Attribute.nodes_missing_value_tracks_true.access(long[].class, onnxAttributes);\n+            return java.util.Optional.ofNullable(nodes_missing_value_tracks_true).map(long[]::clone);\n@@ -29460,1 +29460,1 @@\n-    public static TreeEnsemble TreeEnsemble(TypeElement resultType, Value X, java.util.Optional<Integer> aggregate_function, java.util.Optional<byte[]> nodes_hitrates, int[] nodes_featureids, int[] nodes_falseleafs, java.util.Optional<Integer> post_transform, int[] nodes_trueleafs, byte[] nodes_modes, int[] nodes_falsenodeids, int[] nodes_truenodeids, byte[] leaf_weights, int[] leaf_targetids, int[] tree_roots, java.util.Optional<Integer> n_targets, java.util.Optional<int[]> nodes_missing_value_tracks_true, java.util.Optional<byte[]> membership_values, byte[] nodes_splits) {\n+    public static TreeEnsemble TreeEnsemble(TypeElement resultType, Value X, java.util.Optional<Long> aggregate_function, java.util.Optional<byte[]> nodes_hitrates, long[] nodes_featureids, long[] nodes_falseleafs, java.util.Optional<Long> post_transform, long[] nodes_trueleafs, byte[] nodes_modes, long[] nodes_falsenodeids, long[] nodes_truenodeids, byte[] leaf_weights, long[] leaf_targetids, long[] tree_roots, java.util.Optional<Long> n_targets, java.util.Optional<long[]> nodes_missing_value_tracks_true, java.util.Optional<byte[]> membership_values, byte[] nodes_splits) {\n@@ -29469,2 +29469,2 @@\n-            classlabels_int64s(int[].class, true, null),\n-            class_ids(int[].class, true, null),\n+            classlabels_int64s(long[].class, true, null),\n+            class_ids(long[].class, true, null),\n@@ -29472,2 +29472,2 @@\n-            nodes_featureids(int[].class, true, null),\n-            nodes_treeids(int[].class, true, null),\n+            nodes_featureids(long[].class, true, null),\n+            nodes_treeids(long[].class, true, null),\n@@ -29477,1 +29477,1 @@\n-            nodes_falsenodeids(int[].class, true, null),\n+            nodes_falsenodeids(long[].class, true, null),\n@@ -29479,2 +29479,2 @@\n-            nodes_truenodeids(int[].class, true, null),\n-            nodes_nodeids(int[].class, true, null),\n+            nodes_truenodeids(long[].class, true, null),\n+            nodes_nodeids(long[].class, true, null),\n@@ -29484,3 +29484,3 @@\n-            nodes_missing_value_tracks_true(int[].class, true, null),\n-            class_nodeids(int[].class, true, null),\n-            class_treeids(int[].class, true, null),\n+            nodes_missing_value_tracks_true(long[].class, true, null),\n+            class_nodeids(long[].class, true, null),\n+            class_treeids(long[].class, true, null),\n@@ -29602,1 +29602,1 @@\n-        TreeEnsembleClassifier(TypeElement resultType, Value X, java.util.Optional<int[]> classlabels_int64s, java.util.Optional<int[]> class_ids, java.util.Optional<float[]> nodes_hitrates, java.util.Optional<int[]> nodes_featureids, java.util.Optional<int[]> nodes_treeids, java.util.Optional<byte[]> class_weights_as_tensor, java.util.Optional<String> post_transform, java.util.Optional<String[]> nodes_modes, java.util.Optional<int[]> nodes_falsenodeids, java.util.Optional<String[]> classlabels_strings, java.util.Optional<int[]> nodes_truenodeids, java.util.Optional<int[]> nodes_nodeids, java.util.Optional<byte[]> nodes_hitrates_as_tensor, java.util.Optional<float[]> class_weights, java.util.Optional<byte[]> base_values_as_tensor, java.util.Optional<int[]> nodes_missing_value_tracks_true, java.util.Optional<int[]> class_nodeids, java.util.Optional<int[]> class_treeids, java.util.Optional<float[]> base_values, java.util.Optional<float[]> nodes_values, java.util.Optional<byte[]> nodes_values_as_tensor) {\n+        TreeEnsembleClassifier(TypeElement resultType, Value X, java.util.Optional<long[]> classlabels_int64s, java.util.Optional<long[]> class_ids, java.util.Optional<float[]> nodes_hitrates, java.util.Optional<long[]> nodes_featureids, java.util.Optional<long[]> nodes_treeids, java.util.Optional<byte[]> class_weights_as_tensor, java.util.Optional<String> post_transform, java.util.Optional<String[]> nodes_modes, java.util.Optional<long[]> nodes_falsenodeids, java.util.Optional<String[]> classlabels_strings, java.util.Optional<long[]> nodes_truenodeids, java.util.Optional<long[]> nodes_nodeids, java.util.Optional<byte[]> nodes_hitrates_as_tensor, java.util.Optional<float[]> class_weights, java.util.Optional<byte[]> base_values_as_tensor, java.util.Optional<long[]> nodes_missing_value_tracks_true, java.util.Optional<long[]> class_nodeids, java.util.Optional<long[]> class_treeids, java.util.Optional<float[]> base_values, java.util.Optional<float[]> nodes_values, java.util.Optional<byte[]> nodes_values_as_tensor) {\n@@ -29620,3 +29620,3 @@\n-        public java.util.Optional<int[]> classlabels_int64s() {\n-            int[] classlabels_int64s = Attribute.classlabels_int64s.access(int[].class, onnxAttributes);\n-            return java.util.Optional.ofNullable(classlabels_int64s).map(int[]::clone);\n+        public java.util.Optional<long[]> classlabels_int64s() {\n+            long[] classlabels_int64s = Attribute.classlabels_int64s.access(long[].class, onnxAttributes);\n+            return java.util.Optional.ofNullable(classlabels_int64s).map(long[]::clone);\n@@ -29625,3 +29625,3 @@\n-        public java.util.Optional<int[]> class_ids() {\n-            int[] class_ids = Attribute.class_ids.access(int[].class, onnxAttributes);\n-            return java.util.Optional.ofNullable(class_ids).map(int[]::clone);\n+        public java.util.Optional<long[]> class_ids() {\n+            long[] class_ids = Attribute.class_ids.access(long[].class, onnxAttributes);\n+            return java.util.Optional.ofNullable(class_ids).map(long[]::clone);\n@@ -29635,3 +29635,3 @@\n-        public java.util.Optional<int[]> nodes_featureids() {\n-            int[] nodes_featureids = Attribute.nodes_featureids.access(int[].class, onnxAttributes);\n-            return java.util.Optional.ofNullable(nodes_featureids).map(int[]::clone);\n+        public java.util.Optional<long[]> nodes_featureids() {\n+            long[] nodes_featureids = Attribute.nodes_featureids.access(long[].class, onnxAttributes);\n+            return java.util.Optional.ofNullable(nodes_featureids).map(long[]::clone);\n@@ -29640,3 +29640,3 @@\n-        public java.util.Optional<int[]> nodes_treeids() {\n-            int[] nodes_treeids = Attribute.nodes_treeids.access(int[].class, onnxAttributes);\n-            return java.util.Optional.ofNullable(nodes_treeids).map(int[]::clone);\n+        public java.util.Optional<long[]> nodes_treeids() {\n+            long[] nodes_treeids = Attribute.nodes_treeids.access(long[].class, onnxAttributes);\n+            return java.util.Optional.ofNullable(nodes_treeids).map(long[]::clone);\n@@ -29660,3 +29660,3 @@\n-        public java.util.Optional<int[]> nodes_falsenodeids() {\n-            int[] nodes_falsenodeids = Attribute.nodes_falsenodeids.access(int[].class, onnxAttributes);\n-            return java.util.Optional.ofNullable(nodes_falsenodeids).map(int[]::clone);\n+        public java.util.Optional<long[]> nodes_falsenodeids() {\n+            long[] nodes_falsenodeids = Attribute.nodes_falsenodeids.access(long[].class, onnxAttributes);\n+            return java.util.Optional.ofNullable(nodes_falsenodeids).map(long[]::clone);\n@@ -29670,3 +29670,3 @@\n-        public java.util.Optional<int[]> nodes_truenodeids() {\n-            int[] nodes_truenodeids = Attribute.nodes_truenodeids.access(int[].class, onnxAttributes);\n-            return java.util.Optional.ofNullable(nodes_truenodeids).map(int[]::clone);\n+        public java.util.Optional<long[]> nodes_truenodeids() {\n+            long[] nodes_truenodeids = Attribute.nodes_truenodeids.access(long[].class, onnxAttributes);\n+            return java.util.Optional.ofNullable(nodes_truenodeids).map(long[]::clone);\n@@ -29675,3 +29675,3 @@\n-        public java.util.Optional<int[]> nodes_nodeids() {\n-            int[] nodes_nodeids = Attribute.nodes_nodeids.access(int[].class, onnxAttributes);\n-            return java.util.Optional.ofNullable(nodes_nodeids).map(int[]::clone);\n+        public java.util.Optional<long[]> nodes_nodeids() {\n+            long[] nodes_nodeids = Attribute.nodes_nodeids.access(long[].class, onnxAttributes);\n+            return java.util.Optional.ofNullable(nodes_nodeids).map(long[]::clone);\n@@ -29695,3 +29695,3 @@\n-        public java.util.Optional<int[]> nodes_missing_value_tracks_true() {\n-            int[] nodes_missing_value_tracks_true = Attribute.nodes_missing_value_tracks_true.access(int[].class, onnxAttributes);\n-            return java.util.Optional.ofNullable(nodes_missing_value_tracks_true).map(int[]::clone);\n+        public java.util.Optional<long[]> nodes_missing_value_tracks_true() {\n+            long[] nodes_missing_value_tracks_true = Attribute.nodes_missing_value_tracks_true.access(long[].class, onnxAttributes);\n+            return java.util.Optional.ofNullable(nodes_missing_value_tracks_true).map(long[]::clone);\n@@ -29700,3 +29700,3 @@\n-        public java.util.Optional<int[]> class_nodeids() {\n-            int[] class_nodeids = Attribute.class_nodeids.access(int[].class, onnxAttributes);\n-            return java.util.Optional.ofNullable(class_nodeids).map(int[]::clone);\n+        public java.util.Optional<long[]> class_nodeids() {\n+            long[] class_nodeids = Attribute.class_nodeids.access(long[].class, onnxAttributes);\n+            return java.util.Optional.ofNullable(class_nodeids).map(long[]::clone);\n@@ -29705,3 +29705,3 @@\n-        public java.util.Optional<int[]> class_treeids() {\n-            int[] class_treeids = Attribute.class_treeids.access(int[].class, onnxAttributes);\n-            return java.util.Optional.ofNullable(class_treeids).map(int[]::clone);\n+        public java.util.Optional<long[]> class_treeids() {\n+            long[] class_treeids = Attribute.class_treeids.access(long[].class, onnxAttributes);\n+            return java.util.Optional.ofNullable(class_treeids).map(long[]::clone);\n@@ -29727,1 +29727,1 @@\n-    public static TreeEnsembleClassifier TreeEnsembleClassifier(TypeElement resultType, Value X, java.util.Optional<int[]> classlabels_int64s, java.util.Optional<int[]> class_ids, java.util.Optional<float[]> nodes_hitrates, java.util.Optional<int[]> nodes_featureids, java.util.Optional<int[]> nodes_treeids, java.util.Optional<byte[]> class_weights_as_tensor, java.util.Optional<String> post_transform, java.util.Optional<String[]> nodes_modes, java.util.Optional<int[]> nodes_falsenodeids, java.util.Optional<String[]> classlabels_strings, java.util.Optional<int[]> nodes_truenodeids, java.util.Optional<int[]> nodes_nodeids, java.util.Optional<byte[]> nodes_hitrates_as_tensor, java.util.Optional<float[]> class_weights, java.util.Optional<byte[]> base_values_as_tensor, java.util.Optional<int[]> nodes_missing_value_tracks_true, java.util.Optional<int[]> class_nodeids, java.util.Optional<int[]> class_treeids, java.util.Optional<float[]> base_values, java.util.Optional<float[]> nodes_values, java.util.Optional<byte[]> nodes_values_as_tensor) {\n+    public static TreeEnsembleClassifier TreeEnsembleClassifier(TypeElement resultType, Value X, java.util.Optional<long[]> classlabels_int64s, java.util.Optional<long[]> class_ids, java.util.Optional<float[]> nodes_hitrates, java.util.Optional<long[]> nodes_featureids, java.util.Optional<long[]> nodes_treeids, java.util.Optional<byte[]> class_weights_as_tensor, java.util.Optional<String> post_transform, java.util.Optional<String[]> nodes_modes, java.util.Optional<long[]> nodes_falsenodeids, java.util.Optional<String[]> classlabels_strings, java.util.Optional<long[]> nodes_truenodeids, java.util.Optional<long[]> nodes_nodeids, java.util.Optional<byte[]> nodes_hitrates_as_tensor, java.util.Optional<float[]> class_weights, java.util.Optional<byte[]> base_values_as_tensor, java.util.Optional<long[]> nodes_missing_value_tracks_true, java.util.Optional<long[]> class_nodeids, java.util.Optional<long[]> class_treeids, java.util.Optional<float[]> base_values, java.util.Optional<float[]> nodes_values, java.util.Optional<byte[]> nodes_values_as_tensor) {\n@@ -29739,3 +29739,3 @@\n-            nodes_featureids(int[].class, true, null),\n-            target_treeids(int[].class, true, null),\n-            nodes_treeids(int[].class, true, null),\n+            nodes_featureids(long[].class, true, null),\n+            target_treeids(long[].class, true, null),\n+            nodes_treeids(long[].class, true, null),\n@@ -29745,5 +29745,5 @@\n-            nodes_falsenodeids(int[].class, true, null),\n-            target_ids(int[].class, true, null),\n-            nodes_truenodeids(int[].class, true, null),\n-            target_nodeids(int[].class, true, null),\n-            nodes_nodeids(int[].class, true, null),\n+            nodes_falsenodeids(long[].class, true, null),\n+            target_ids(long[].class, true, null),\n+            nodes_truenodeids(long[].class, true, null),\n+            target_nodeids(long[].class, true, null),\n+            nodes_nodeids(long[].class, true, null),\n@@ -29752,2 +29752,2 @@\n-            n_targets(Integer.class, true, null),\n-            nodes_missing_value_tracks_true(int[].class, true, null),\n+            n_targets(Long.class, true, null),\n+            nodes_missing_value_tracks_true(long[].class, true, null),\n@@ -29867,1 +29867,1 @@\n-        TreeEnsembleRegressor(TypeElement resultType, Value X, java.util.Optional<String> aggregate_function, java.util.Optional<float[]> nodes_hitrates, java.util.Optional<byte[]> target_weights_as_tensor, java.util.Optional<int[]> nodes_featureids, java.util.Optional<int[]> target_treeids, java.util.Optional<int[]> nodes_treeids, java.util.Optional<String> post_transform, java.util.Optional<String[]> nodes_modes, java.util.Optional<float[]> target_weights, java.util.Optional<int[]> nodes_falsenodeids, java.util.Optional<int[]> target_ids, java.util.Optional<int[]> nodes_truenodeids, java.util.Optional<int[]> target_nodeids, java.util.Optional<int[]> nodes_nodeids, java.util.Optional<byte[]> nodes_hitrates_as_tensor, java.util.Optional<byte[]> base_values_as_tensor, java.util.Optional<Integer> n_targets, java.util.Optional<int[]> nodes_missing_value_tracks_true, java.util.Optional<float[]> base_values, java.util.Optional<float[]> nodes_values, java.util.Optional<byte[]> nodes_values_as_tensor) {\n+        TreeEnsembleRegressor(TypeElement resultType, Value X, java.util.Optional<String> aggregate_function, java.util.Optional<float[]> nodes_hitrates, java.util.Optional<byte[]> target_weights_as_tensor, java.util.Optional<long[]> nodes_featureids, java.util.Optional<long[]> target_treeids, java.util.Optional<long[]> nodes_treeids, java.util.Optional<String> post_transform, java.util.Optional<String[]> nodes_modes, java.util.Optional<float[]> target_weights, java.util.Optional<long[]> nodes_falsenodeids, java.util.Optional<long[]> target_ids, java.util.Optional<long[]> nodes_truenodeids, java.util.Optional<long[]> target_nodeids, java.util.Optional<long[]> nodes_nodeids, java.util.Optional<byte[]> nodes_hitrates_as_tensor, java.util.Optional<byte[]> base_values_as_tensor, java.util.Optional<Long> n_targets, java.util.Optional<long[]> nodes_missing_value_tracks_true, java.util.Optional<float[]> base_values, java.util.Optional<float[]> nodes_values, java.util.Optional<byte[]> nodes_values_as_tensor) {\n@@ -29900,3 +29900,3 @@\n-        public java.util.Optional<int[]> nodes_featureids() {\n-            int[] nodes_featureids = Attribute.nodes_featureids.access(int[].class, onnxAttributes);\n-            return java.util.Optional.ofNullable(nodes_featureids).map(int[]::clone);\n+        public java.util.Optional<long[]> nodes_featureids() {\n+            long[] nodes_featureids = Attribute.nodes_featureids.access(long[].class, onnxAttributes);\n+            return java.util.Optional.ofNullable(nodes_featureids).map(long[]::clone);\n@@ -29905,3 +29905,3 @@\n-        public java.util.Optional<int[]> target_treeids() {\n-            int[] target_treeids = Attribute.target_treeids.access(int[].class, onnxAttributes);\n-            return java.util.Optional.ofNullable(target_treeids).map(int[]::clone);\n+        public java.util.Optional<long[]> target_treeids() {\n+            long[] target_treeids = Attribute.target_treeids.access(long[].class, onnxAttributes);\n+            return java.util.Optional.ofNullable(target_treeids).map(long[]::clone);\n@@ -29910,3 +29910,3 @@\n-        public java.util.Optional<int[]> nodes_treeids() {\n-            int[] nodes_treeids = Attribute.nodes_treeids.access(int[].class, onnxAttributes);\n-            return java.util.Optional.ofNullable(nodes_treeids).map(int[]::clone);\n+        public java.util.Optional<long[]> nodes_treeids() {\n+            long[] nodes_treeids = Attribute.nodes_treeids.access(long[].class, onnxAttributes);\n+            return java.util.Optional.ofNullable(nodes_treeids).map(long[]::clone);\n@@ -29930,3 +29930,3 @@\n-        public java.util.Optional<int[]> nodes_falsenodeids() {\n-            int[] nodes_falsenodeids = Attribute.nodes_falsenodeids.access(int[].class, onnxAttributes);\n-            return java.util.Optional.ofNullable(nodes_falsenodeids).map(int[]::clone);\n+        public java.util.Optional<long[]> nodes_falsenodeids() {\n+            long[] nodes_falsenodeids = Attribute.nodes_falsenodeids.access(long[].class, onnxAttributes);\n+            return java.util.Optional.ofNullable(nodes_falsenodeids).map(long[]::clone);\n@@ -29935,3 +29935,3 @@\n-        public java.util.Optional<int[]> target_ids() {\n-            int[] target_ids = Attribute.target_ids.access(int[].class, onnxAttributes);\n-            return java.util.Optional.ofNullable(target_ids).map(int[]::clone);\n+        public java.util.Optional<long[]> target_ids() {\n+            long[] target_ids = Attribute.target_ids.access(long[].class, onnxAttributes);\n+            return java.util.Optional.ofNullable(target_ids).map(long[]::clone);\n@@ -29940,3 +29940,3 @@\n-        public java.util.Optional<int[]> nodes_truenodeids() {\n-            int[] nodes_truenodeids = Attribute.nodes_truenodeids.access(int[].class, onnxAttributes);\n-            return java.util.Optional.ofNullable(nodes_truenodeids).map(int[]::clone);\n+        public java.util.Optional<long[]> nodes_truenodeids() {\n+            long[] nodes_truenodeids = Attribute.nodes_truenodeids.access(long[].class, onnxAttributes);\n+            return java.util.Optional.ofNullable(nodes_truenodeids).map(long[]::clone);\n@@ -29945,3 +29945,3 @@\n-        public java.util.Optional<int[]> target_nodeids() {\n-            int[] target_nodeids = Attribute.target_nodeids.access(int[].class, onnxAttributes);\n-            return java.util.Optional.ofNullable(target_nodeids).map(int[]::clone);\n+        public java.util.Optional<long[]> target_nodeids() {\n+            long[] target_nodeids = Attribute.target_nodeids.access(long[].class, onnxAttributes);\n+            return java.util.Optional.ofNullable(target_nodeids).map(long[]::clone);\n@@ -29950,3 +29950,3 @@\n-        public java.util.Optional<int[]> nodes_nodeids() {\n-            int[] nodes_nodeids = Attribute.nodes_nodeids.access(int[].class, onnxAttributes);\n-            return java.util.Optional.ofNullable(nodes_nodeids).map(int[]::clone);\n+        public java.util.Optional<long[]> nodes_nodeids() {\n+            long[] nodes_nodeids = Attribute.nodes_nodeids.access(long[].class, onnxAttributes);\n+            return java.util.Optional.ofNullable(nodes_nodeids).map(long[]::clone);\n@@ -29965,2 +29965,2 @@\n-        public java.util.Optional<Integer> n_targets() {\n-            Integer n_targets = Attribute.n_targets.access(Integer.class, onnxAttributes);\n+        public java.util.Optional<Long> n_targets() {\n+            Long n_targets = Attribute.n_targets.access(Long.class, onnxAttributes);\n@@ -29970,3 +29970,3 @@\n-        public java.util.Optional<int[]> nodes_missing_value_tracks_true() {\n-            int[] nodes_missing_value_tracks_true = Attribute.nodes_missing_value_tracks_true.access(int[].class, onnxAttributes);\n-            return java.util.Optional.ofNullable(nodes_missing_value_tracks_true).map(int[]::clone);\n+        public java.util.Optional<long[]> nodes_missing_value_tracks_true() {\n+            long[] nodes_missing_value_tracks_true = Attribute.nodes_missing_value_tracks_true.access(long[].class, onnxAttributes);\n+            return java.util.Optional.ofNullable(nodes_missing_value_tracks_true).map(long[]::clone);\n@@ -29992,1 +29992,1 @@\n-    public static TreeEnsembleRegressor TreeEnsembleRegressor(TypeElement resultType, Value X, java.util.Optional<String> aggregate_function, java.util.Optional<float[]> nodes_hitrates, java.util.Optional<byte[]> target_weights_as_tensor, java.util.Optional<int[]> nodes_featureids, java.util.Optional<int[]> target_treeids, java.util.Optional<int[]> nodes_treeids, java.util.Optional<String> post_transform, java.util.Optional<String[]> nodes_modes, java.util.Optional<float[]> target_weights, java.util.Optional<int[]> nodes_falsenodeids, java.util.Optional<int[]> target_ids, java.util.Optional<int[]> nodes_truenodeids, java.util.Optional<int[]> target_nodeids, java.util.Optional<int[]> nodes_nodeids, java.util.Optional<byte[]> nodes_hitrates_as_tensor, java.util.Optional<byte[]> base_values_as_tensor, java.util.Optional<Integer> n_targets, java.util.Optional<int[]> nodes_missing_value_tracks_true, java.util.Optional<float[]> base_values, java.util.Optional<float[]> nodes_values, java.util.Optional<byte[]> nodes_values_as_tensor) {\n+    public static TreeEnsembleRegressor TreeEnsembleRegressor(TypeElement resultType, Value X, java.util.Optional<String> aggregate_function, java.util.Optional<float[]> nodes_hitrates, java.util.Optional<byte[]> target_weights_as_tensor, java.util.Optional<long[]> nodes_featureids, java.util.Optional<long[]> target_treeids, java.util.Optional<long[]> nodes_treeids, java.util.Optional<String> post_transform, java.util.Optional<String[]> nodes_modes, java.util.Optional<float[]> target_weights, java.util.Optional<long[]> nodes_falsenodeids, java.util.Optional<long[]> target_ids, java.util.Optional<long[]> nodes_truenodeids, java.util.Optional<long[]> target_nodeids, java.util.Optional<long[]> nodes_nodeids, java.util.Optional<byte[]> nodes_hitrates_as_tensor, java.util.Optional<byte[]> base_values_as_tensor, java.util.Optional<Long> n_targets, java.util.Optional<long[]> nodes_missing_value_tracks_true, java.util.Optional<float[]> base_values, java.util.Optional<float[]> nodes_values, java.util.Optional<byte[]> nodes_values_as_tensor) {\n@@ -30001,1 +30001,1 @@\n-            upper(Integer.class, true, 1),\n+            upper(Long.class, true, 1),\n@@ -30113,1 +30113,1 @@\n-        Trilu(TypeElement resultType, Value input, java.util.Optional<Value> k, java.util.Optional<Integer> upper) {\n+        Trilu(TypeElement resultType, Value input, java.util.Optional<Value> k, java.util.Optional<Long> upper) {\n@@ -30136,2 +30136,2 @@\n-        public java.util.Optional<Integer> upper() {\n-            Integer upper = Attribute.upper.access(Integer.class, onnxAttributes);\n+        public java.util.Optional<Long> upper() {\n+            Long upper = Attribute.upper.access(Long.class, onnxAttributes);\n@@ -30143,1 +30143,1 @@\n-    public static Trilu Trilu(TypeElement resultType, Value input, java.util.Optional<Value> k, java.util.Optional<Integer> upper) {\n+    public static Trilu Trilu(TypeElement resultType, Value input, java.util.Optional<Value> k, java.util.Optional<Long> upper) {\n@@ -30152,2 +30152,2 @@\n-            sorted(Integer.class, true, 1),\n-            axis(Integer.class, true, null),\n+            sorted(Long.class, true, 1),\n+            axis(Long.class, true, null),\n@@ -30267,1 +30267,1 @@\n-        Unique(TypeElement resultType, Set<OutputParameter> optionalOutputs, Value X, java.util.Optional<Integer> sorted, java.util.Optional<Integer> axis) {\n+        Unique(TypeElement resultType, Set<OutputParameter> optionalOutputs, Value X, java.util.Optional<Long> sorted, java.util.Optional<Long> axis) {\n@@ -30285,2 +30285,2 @@\n-        public java.util.Optional<Integer> sorted() {\n-            Integer sorted = Attribute.sorted.access(Integer.class, onnxAttributes);\n+        public java.util.Optional<Long> sorted() {\n+            Long sorted = Attribute.sorted.access(Long.class, onnxAttributes);\n@@ -30290,2 +30290,2 @@\n-        public java.util.Optional<Integer> axis() {\n-            Integer axis = Attribute.axis.access(Integer.class, onnxAttributes);\n+        public java.util.Optional<Long> axis() {\n+            Long axis = Attribute.axis.access(Long.class, onnxAttributes);\n@@ -30297,1 +30297,1 @@\n-    public static Unique Unique(TypeElement resultType, Set<Unique.OutputParameter> optionalOutputs, Value X, java.util.Optional<Integer> sorted, java.util.Optional<Integer> axis) {\n+    public static Unique Unique(TypeElement resultType, Set<Unique.OutputParameter> optionalOutputs, Value X, java.util.Optional<Long> sorted, java.util.Optional<Long> axis) {\n@@ -30820,1 +30820,1 @@\n-            classlabels_int64s(int[].class, true, null),\n+            classlabels_int64s(long[].class, true, null),\n@@ -30932,1 +30932,1 @@\n-        ZipMap(TypeElement resultType, Value X, java.util.Optional<int[]> classlabels_int64s, java.util.Optional<String[]> classlabels_strings) {\n+        ZipMap(TypeElement resultType, Value X, java.util.Optional<long[]> classlabels_int64s, java.util.Optional<String[]> classlabels_strings) {\n@@ -30950,3 +30950,3 @@\n-        public java.util.Optional<int[]> classlabels_int64s() {\n-            int[] classlabels_int64s = Attribute.classlabels_int64s.access(int[].class, onnxAttributes);\n-            return java.util.Optional.ofNullable(classlabels_int64s).map(int[]::clone);\n+        public java.util.Optional<long[]> classlabels_int64s() {\n+            long[] classlabels_int64s = Attribute.classlabels_int64s.access(long[].class, onnxAttributes);\n+            return java.util.Optional.ofNullable(classlabels_int64s).map(long[]::clone);\n@@ -30962,1 +30962,1 @@\n-    public static ZipMap ZipMap(TypeElement resultType, Value X, java.util.Optional<int[]> classlabels_int64s, java.util.Optional<String[]> classlabels_strings) {\n+    public static ZipMap ZipMap(TypeElement resultType, Value X, java.util.Optional<long[]> classlabels_int64s, java.util.Optional<String[]> classlabels_strings) {\n","filename":"cr-examples\/onnx\/src\/main\/java\/oracle\/code\/onnx\/ir\/OnnxOps.java","additions":1010,"deletions":1010,"binary":false,"changes":2020,"status":"modified"},{"patch":"@@ -31,0 +31,3 @@\n+import jdk.incubator.code.type.TupleType;\n+import jdk.incubator.code.writer.OpWriter;\n+import oracle.code.onnx.compiler.OnnxTransformer;\n@@ -34,0 +37,1 @@\n+import org.junit.jupiter.api.Assertions;\n@@ -35,0 +39,2 @@\n+import java.io.StringWriter;\n+import java.lang.invoke.MethodHandles;\n@@ -36,0 +42,1 @@\n+import java.util.Optional;\n@@ -37,0 +44,1 @@\n+import java.util.stream.Stream;\n@@ -52,17 +60,0 @@\n-    \/\/ (5, 5, NUM_CHANNELS, 32)\n-    private Tensor<Float> conv1Weights;\n-    \/\/ (32)\n-    private Tensor<Float> conv1Biases;\n-    \/\/ (5, 5, 32, 64)\n-    private Tensor<Float> conv2Weights;\n-    \/\/ (64)\n-    private Tensor<Float> conv2Biases;\n-    \/\/ (IMAGE_SIZE * IMAGE_SIZE * 4, 512)\n-    private Tensor<Float> fc1Weights;\n-    \/\/ (512)\n-    private Tensor<Float> fc1Biases;\n-    \/\/ (512, NUM_LABELS)\n-    private Tensor<Float> fc2Weights;\n-    \/\/ (NUM_LABELS)\n-    private Tensor<Float> fc2Biases;\n-\n@@ -70,2 +61,21 @@\n-    public Tensor<Float> cnn(Tensor<Float> inputImage) {\n-        var shape = Constant(new int[]{-1, IMAGE_SIZE, IMAGE_SIZE, NUM_CHANNELS});\n+    public static Tensor<Float> cnn(\n+            \/\/ Weights and biases\n+            \/\/ (5, 5, NUM_CHANNELS, 32)\n+            Tensor<Float> conv1Weights,\n+            \/\/ (32)\n+            Tensor<Float> conv1Biases,\n+            \/\/ (5, 5, 32, 64)\n+            Tensor<Float> conv2Weights,\n+            \/\/ (64)\n+            Tensor<Float> conv2Biases,\n+            \/\/ (IMAGE_SIZE * IMAGE_SIZE * 4, 512)\n+            Tensor<Float> fc1Weights,\n+            \/\/ (512)\n+            Tensor<Float> fc1Biases,\n+            \/\/ (512, NUM_LABELS)\n+            Tensor<Float> fc2Weights,\n+            \/\/ (NUM_LABELS)\n+            Tensor<Float> fc2Biases,\n+            \/\/ Inputs\n+            Tensor<Float> inputImage) {\n+        var shape = Constant(new long[]{-1, IMAGE_SIZE, IMAGE_SIZE, NUM_CHANNELS});\n@@ -81,1 +91,1 @@\n-                empty(), of(\"SAME_UPPER\"), of(new int[]{1, 1, 1, 1}),\n+                empty(), of(\"SAME_UPPER\"), of(new long[]{1, 1, 1, 1}),\n@@ -87,1 +97,1 @@\n-                empty(), empty(), of(new int[]{1, 2, 2, 1}), new int[]{1, 2, 2, 1});\n+                empty(), empty(), of(new long[]{1, 2, 2, 1}), new long[]{1, 2, 2, 1});\n@@ -91,1 +101,1 @@\n-                empty(), of(\"SAME_UPPER\"), of(new int[]{1, 1, 1, 1}),\n+                empty(), of(\"SAME_UPPER\"), of(new long[]{1, 1, 1, 1}),\n@@ -97,1 +107,1 @@\n-                empty(), empty(), of(new int[]{1, 2, 2, 1}), new int[]{1, 2, 2, 1});\n+                empty(), empty(), of(new long[]{1, 2, 2, 1}), new long[]{1, 2, 2, 1});\n@@ -100,1 +110,1 @@\n-        var flatShape = Constant(new int[]{0, 3136});\n+        var flatShape = Constant(new long[]{0, 3136});\n@@ -104,1 +114,1 @@\n-        var fc1 = Gemm(flatten, fc1Weights, of(fc1Biases), of(1f), of(1), of(1f), empty());\n+        var fc1 = Gemm(flatten, fc1Weights, of(fc1Biases), of(1f), of(1L), of(1f), empty());\n@@ -108,2 +118,2 @@\n-        var fc2 = Gemm(relu3, fc2Weights, of(fc2Biases), of(1f), of(1), of(1f), empty());\n-        var prediction = Softmax(fc2, of(1));\n+        var fc2 = Gemm(relu3, fc2Weights, of(fc2Biases), of(1f), of(1L), of(1f), empty());\n+        var prediction = Softmax(fc2, of(1L));\n@@ -119,1 +129,0 @@\n-                OnnxType.TENSOR_FLOAT32, \/\/ input arg\n@@ -128,0 +137,2 @@\n+                OnnxType.TENSOR_FLOAT32,\n+                \/\/ input\n@@ -132,2 +143,0 @@\n-            Block.Parameter inputImage = b.parameters().get(0);\n-\n@@ -135,8 +144,10 @@\n-            Block.Parameter conv1Weights = b.parameters().get(1);\n-            Block.Parameter conv1Biases = b.parameters().get(2);\n-            Block.Parameter conv2Weights = b.parameters().get(3);\n-            Block.Parameter conv2Biases = b.parameters().get(4);\n-            Block.Parameter fc1Weights = b.parameters().get(5);\n-            Block.Parameter fc1Biases = b.parameters().get(6);\n-            Block.Parameter fc2Weights = b.parameters().get(7);\n-            Block.Parameter fc2Biases = b.parameters().get(8);\n+            Block.Parameter conv1Weights = b.parameters().get(0);\n+            Block.Parameter conv1Biases = b.parameters().get(1);\n+            Block.Parameter conv2Weights = b.parameters().get(2);\n+            Block.Parameter conv2Biases = b.parameters().get(3);\n+            Block.Parameter fc1Weights = b.parameters().get(4);\n+            Block.Parameter fc1Biases = b.parameters().get(5);\n+            Block.Parameter fc2Weights = b.parameters().get(6);\n+            Block.Parameter fc2Biases = b.parameters().get(7);\n+\n+            Block.Parameter inputImage = b.parameters().get(8);\n@@ -150,1 +161,1 @@\n-                    of(new int[]{-1, IMAGE_SIZE, IMAGE_SIZE, NUM_CHANNELS}),\n+                    of(new long[]{-1, IMAGE_SIZE, IMAGE_SIZE, NUM_CHANNELS}),\n@@ -187,1 +198,1 @@\n-                    of(new int[]{1, 1, 1, 1}),\n+                    of(new long[]{1, 1, 1, 1}),\n@@ -195,1 +206,2 @@\n-            var pool1 = b.op(OnnxOps.MaxPool(relu1.type(), Set.of(),\n+            var pool1Result = b.op(OnnxOps.MaxPool(TupleType.tupleType(relu1.type(), OnnxType.TENSOR_INT64),\n+                    Set.of(OnnxOps.MaxPool.OutputParameter.Indices),\n@@ -202,2 +214,2 @@\n-                    of(new int[]{1, 2, 2, 1}),\n-                    new int[]{1, 2, 2, 1}));\n+                    of(new long[]{1, 2, 2, 1}),\n+                    new long[]{1, 2, 2, 1}));\n@@ -206,0 +218,1 @@\n+            var pool1 = b.op(CoreOp.tupleLoad(pool1Result, 0));\n@@ -213,1 +226,1 @@\n-                    of(new int[]{1, 1, 1, 1}),\n+                    of(new long[]{1, 1, 1, 1}),\n@@ -221,1 +234,2 @@\n-            var pool2 = b.op(OnnxOps.MaxPool(relu2.type(), Set.of(),\n+            var pool2Result = b.op(OnnxOps.MaxPool(TupleType.tupleType(relu2.type(), OnnxType.TENSOR_INT64),\n+                    Set.of(OnnxOps.MaxPool.OutputParameter.Indices),\n@@ -228,2 +242,2 @@\n-                    of(new int[]{1, 2, 2, 1}),\n-                    new int[]{1, 2, 2, 1}));\n+                    of(new long[]{1, 2, 2, 1}),\n+                    new long[]{1, 2, 2, 1}));\n@@ -238,1 +252,1 @@\n-                    of(new int[]{0, 3136}),\n+                    of(new long[]{0, 3136}),\n@@ -241,0 +255,1 @@\n+            var pool2 = b.op(CoreOp.tupleLoad(pool2Result, 0));\n@@ -252,1 +267,1 @@\n-                    of(1),\n+                    of(1L),\n@@ -264,1 +279,1 @@\n-                    of(1),\n+                    of(1L),\n@@ -269,1 +284,1 @@\n-                    of(1)));\n+                    of(1L)));\n@@ -276,11 +291,15 @@\n-    public void test() throws Exception {\n-        {\n-            Method cnn = CNNTest.class.getMethod(\"cnn\", Tensor.class);\n-            CoreOp.FuncOp funcOp = Op.ofMethod(cnn).get();\n-            System.out.println(funcOp.toText());\n-        }\n-\n-        {\n-            CoreOp.FuncOp funcOp = cnnModel();\n-            System.out.println(funcOp.toText());\n-        }\n+    public void test() {\n+        CoreOp.FuncOp f = getFuncOp(\"cnn\");\n+        CoreOp.FuncOp onnxModel = OnnxTransformer.transform(MethodHandles.lookup(), f);\n+        System.out.println(onnxModel.toText());\n+\n+        CoreOp.FuncOp expectedOnnxModel = cnnModel();\n+        System.out.println(expectedOnnxModel.toText());\n+\n+        Assertions.assertEquals(serialize(expectedOnnxModel), serialize(onnxModel));\n+    }\n+\n+    static String serialize(Op o) {\n+        StringWriter w = new StringWriter();\n+        OpWriter.writeTo(w, o, OpWriter.LocationOption.DROP_LOCATION);\n+        return w.toString();\n@@ -289,0 +308,8 @@\n+    static CoreOp.FuncOp getFuncOp(String name) {\n+        Optional<Method> om = Stream.of(CNNTest.class.getDeclaredMethods())\n+                .filter(m -> m.getName().equals(name))\n+                .findFirst();\n+\n+        Method m = om.get();\n+        return Op.ofMethod(m).get();\n+    }\n","filename":"cr-examples\/onnx\/src\/test\/java\/oracle\/code\/onnx\/CNNTest.java","additions":89,"deletions":62,"binary":false,"changes":151,"status":"modified"}]}