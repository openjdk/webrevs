{"files":[{"patch":"@@ -7,0 +7,1 @@\n+import java.lang.reflect.Method;\n@@ -10,3 +11,2 @@\n-import java.util.List;\n-import java.util.Locale;\n-import java.util.Map;\n+import java.util.*;\n+import java.util.function.Predicate;\n@@ -18,0 +18,1 @@\n+import jdk.incubator.code.type.ClassType;\n@@ -19,0 +20,1 @@\n+import jdk.incubator.code.type.MethodRef;\n@@ -52,1 +54,5 @@\n-    public static <T, U extends Quotable & Supplier<Tensor<T>>> Tensor<T> execute(U codeLambda) {\n+    @FunctionalInterface\n+    public interface OnnxFunction<T> extends Supplier<T>, Quotable {\n+    }\n+\n+    public static <T> Tensor<T> execute(MethodHandles.Lookup l, OnnxFunction<Tensor<T>> codeLambda) {\n@@ -55,12 +61,38 @@\n-        var capturedValues = lambda.capturedValues();\n-        var onnxFunc = OnnxTransformer.transform(MethodHandles.lookup(), CoreOp.func(\"onnxCode\", FunctionType.functionType(\n-                lambda.invokableType().returnType(),\n-                capturedValues.stream().map(Value::type).map(t -> t instanceof VarType vt ? vt.valueType() : t).toList()))\n-                .body(bb -> {\n-                    bb.context().mapValues(capturedValues, bb.parameters());\n-                    for (Op op : lambda.body().entryBlock().ops()) {\n-                        int i;\n-                        if (op instanceof CoreOp.VarAccessOp.VarLoadOp load && (i = capturedValues.indexOf(load.varOp().result())) >= 0) {\n-                            bb.context().mapValue(op.result(), bb.parameters().get(i)); \/\/ remap var load result to block param\n-                        } else {\n-                            bb.apply(op);\n+\n+        CoreOp.FuncOp onnxFunc;\n+        List<Tensor> arguments;\n+        \/\/ Shortcut for lambda expressions that call just one method\n+        if (singleMethodInvocation(lambda) instanceof\n+                SingleMethod(CoreOp.InvokeOp iop, Map<Value, Value> valueMapping)) {\n+            Method m;\n+            try {\n+                m = iop.invokeDescriptor().resolveToMethod(l, iop.invokeKind());\n+            } catch (ReflectiveOperationException e) {\n+                throw new RuntimeException(e);\n+            }\n+\n+            CoreOp.FuncOp f = Op.ofMethod(m).orElseThrow();\n+            onnxFunc = OnnxTransformer.transform(l, f);\n+\n+            arguments = iop.operands().stream().toList().stream()\n+                    .map(valueMapping::get)\n+                    .map(v -> quotable.capturedValues().get(v))\n+                    .map(val -> val instanceof CoreOp.Var<?> v ? v.value() : val)\n+                    .map(val -> (Tensor) val)\n+                    .toList();\n+        } else {\n+            var capturedValues = lambda.capturedValues();\n+            var functionType = FunctionType.functionType(lambda.invokableType().returnType(),\n+                    capturedValues.stream().map(Value::type)\n+                            .map(t -> t instanceof VarType vt ? vt.valueType() : t).toList());\n+            onnxFunc = OnnxTransformer.transform(l, CoreOp.func(\"onnxCode\", functionType)\n+                    .body(bb -> {\n+                        bb.context().mapValues(capturedValues, bb.parameters());\n+                        for (Op op : lambda.body().entryBlock().ops()) {\n+                            int i;\n+                            if (op instanceof CoreOp.VarAccessOp.VarLoadOp load &&\n+                                    (i = capturedValues.indexOf(load.varOp().result())) >= 0) {\n+                                bb.context().mapValue(op.result(), bb.parameters().get(i)); \/\/ remap var load result to block param\n+                            } else {\n+                                bb.apply(op);\n+                            }\n@@ -68,2 +100,8 @@\n-                    }\n-                }));\n+                    }));\n+\n+            arguments = quotable.capturedValues().values().stream()\n+                    .map(val -> val instanceof CoreOp.Var<?> v ? v.value() : val)\n+                    .map(val -> (Tensor) val)\n+                    .toList();\n+        }\n+\n@@ -71,1 +109,9 @@\n-            return session.run(quotable.capturedValues().values().stream().map(val -> (Tensor)(val instanceof CoreOp.Var v ? v.value() : val)).toList()).getFirst();\n+            return session.run(arguments).getFirst();\n+        }\n+    }\n+\n+    record SingleMethod(CoreOp.InvokeOp iop, Map<Value, Value> valueMapping) {}\n+    static SingleMethod singleMethodInvocation(CoreOp.LambdaOp lop) {\n+        \/\/ Single block\n+        if (lop.body().blocks().size() > 1) {\n+            return null;\n@@ -73,0 +119,83 @@\n+\n+        Map<Value, Value> valueMapping = new HashMap<>();\n+        CoreOp.InvokeOp methodRefInvokeOp = extractMethodInvoke(valueMapping, lop.body().entryBlock().ops());\n+        if (methodRefInvokeOp == null) {\n+            return null;\n+        }\n+\n+        return new SingleMethod(methodRefInvokeOp, valueMapping);\n+    }\n+\n+    static CoreOp.InvokeOp extractMethodInvoke(Map<Value, Value> valueMapping, List<Op> ops) {\n+        CoreOp.InvokeOp methodRefInvokeOp = null;\n+        for (Op op : ops) {\n+            switch (op) {\n+                case CoreOp.VarOp varOp -> {\n+                    if (isValueUsedWithOp(varOp.result(), o -> o instanceof CoreOp.VarAccessOp.VarStoreOp)) {\n+                        return null;\n+                    }\n+                }\n+                case CoreOp.VarAccessOp.VarLoadOp varLoadOp -> {\n+                    Value v = varLoadOp.varOp().result();\n+                    valueMapping.put(varLoadOp.result(), valueMapping.getOrDefault(v, v));\n+                }\n+                case CoreOp.InvokeOp iop when isBoxOrUnboxInvocation(iop) -> {\n+                    Value v = iop.operands().getFirst();\n+                    valueMapping.put(iop.result(), valueMapping.getOrDefault(v, v));\n+                }\n+                case CoreOp.InvokeOp iop -> {\n+                    if (methodRefInvokeOp != null) {\n+                        return null;\n+                    }\n+\n+                    for (Value o : iop.operands()) {\n+                        valueMapping.put(o, valueMapping.getOrDefault(o, o));\n+                    }\n+                    methodRefInvokeOp = iop;\n+                }\n+                case CoreOp.ReturnOp rop -> {\n+                    if (methodRefInvokeOp == null) {\n+                        return null;\n+                    }\n+                    Value r = rop.returnValue();\n+                    if (!(valueMapping.getOrDefault(r, r) instanceof Op.Result invokeResult)) {\n+                        return null;\n+                    }\n+                    if (invokeResult.op() != methodRefInvokeOp) {\n+                        return null;\n+                    }\n+                    assert methodRefInvokeOp.result().uses().size() == 1;\n+                }\n+                default -> {\n+                    return null;\n+                }\n+            }\n+        }\n+\n+        return methodRefInvokeOp;\n+    }\n+\n+    private static boolean isValueUsedWithOp(Value value, Predicate<Op> opPredicate) {\n+        for (Op.Result user : value.uses()) {\n+            if (opPredicate.test(user.op())) {\n+                return true;\n+            }\n+        }\n+        return false;\n+    }\n+\n+    \/\/ @@@ Move to functionality on JavaType(s)\n+    static final Set<String> UNBOX_NAMES = Set.of(\n+            \"byteValue\",\n+            \"shortValue\",\n+            \"charValue\",\n+            \"intValue\",\n+            \"longValue\",\n+            \"floatValue\",\n+            \"doubleValue\",\n+            \"booleanValue\");\n+\n+    private static boolean isBoxOrUnboxInvocation(CoreOp.InvokeOp iop) {\n+        MethodRef mr = iop.invokeDescriptor();\n+        return mr.refType() instanceof ClassType ct && ct.unbox().isPresent() &&\n+                (UNBOX_NAMES.contains(mr.name()) || mr.name().equals(\"valueOf\"));\n","filename":"cr-examples\/onnx\/src\/main\/java\/oracle\/code\/onnx\/OnnxRuntime.java","additions":148,"deletions":19,"binary":false,"changes":167,"status":"modified"},{"patch":"@@ -26,0 +26,2 @@\n+import jdk.incubator.code.CodeReflection;\n+\n@@ -32,0 +34,1 @@\n+import java.lang.invoke.MethodHandles;\n@@ -51,0 +54,1 @@\n+    @CodeReflection\n@@ -52,51 +56,49 @@\n-        return OnnxRuntime.execute(() -> {\n-            \/\/ Scaling to 0-1\n-            var scaledInput = Div(inputImage, Constant(255f));\n-\n-            \/\/ First conv layer\n-            var conv1Weights = Reshape(Constant(loadConstant(\"conv1-weight-float-le\")), Constant(new long[]{6, 1, 5, 5}), empty());\n-            var conv1Biases = Reshape(Constant(loadConstant(\"conv1-bias-float-le\")), Constant(new long[]{6}), empty());\n-            var conv1 = Conv(scaledInput, conv1Weights, of(conv1Biases), of(new long[4]),\n-                    of(new long[]{1,1}), empty(), of(new long[]{1, 1, 1, 1}),\n-                    of(1L), of(new long[]{5,5}));\n-            var relu1 = Relu(conv1);\n-\n-            \/\/ First pooling layer\n-            var pool1 = MaxPool(relu1, of(new long[4]), of(new long[]{1,1}), empty(),\n-                    of(0L), empty(), of(new long[]{2, 2}), new long[]{2, 2});\n-\n-            \/\/ Second conv layer\n-            var conv2Weights = Reshape(Constant(loadConstant(\"conv2-weight-float-le\")), Constant(new long[]{16, 6, 5, 5}), empty());\n-            var conv2Biases = Reshape(Constant(loadConstant(\"conv2-bias-float-le\")), Constant(new long[]{16}), empty());\n-            var conv2 = Conv(pool1.Y(), conv2Weights, of(conv2Biases), of(new long[4]),\n-                    of(new long[]{1,1}), empty(), of(new long[]{1, 1, 1, 1}),\n-                    of(1L), of(new long[]{5,5}));\n-            var relu2 = Relu(conv2);\n-\n-            \/\/ Second pooling layer\n-            var pool2 = MaxPool(relu2, of(new long[4]), of(new long[]{1,1}), empty(),\n-                    of(0L), empty(), of(new long[]{2, 2}), new long[]{2, 2});\n-\n-            \/\/ Flatten inputs\n-            var flatten = Flatten(pool2.Y(), of(1L));\n-\n-            \/\/ First fully connected layer\n-            var fc1Weights = Reshape(Constant(loadConstant(\"fc1-weight-float-le\")), Constant(new long[]{120, 256}), empty());\n-            var fc1Biases = Reshape(Constant(loadConstant(\"fc1-bias-float-le\")), Constant(new long[]{120}), empty());\n-            var fc1 = Gemm(flatten, fc1Weights, of(fc1Biases), of(1f), of(1L), of(1f), empty());\n-            var relu3 = Relu(fc1);\n-\n-            \/\/ Second fully connected layer\n-            var fc2Weights = Reshape(Constant(loadConstant(\"fc2-weight-float-le\")), Constant(new long[]{84, 120}), empty());\n-            var fc2Biases = Reshape(Constant(loadConstant(\"fc2-bias-float-le\")), Constant(new long[]{84}), empty());\n-            var fc2 = Gemm(relu3, fc2Weights, of(fc2Biases), of(1f), of(1L), of(1f), empty());\n-            var relu4 = Relu(fc2);\n-\n-            \/\/ Softmax layer\n-            var fc3Weights = Reshape(Constant(loadConstant(\"fc3-weight-float-le\")), Constant(new long[]{10, 84}), empty());\n-            var fc3Biases = Reshape(Constant(loadConstant(\"fc3-bias-float-le\")), Constant(new long[]{10}), empty());\n-            var fc3 = Gemm(relu4, fc3Weights, of(fc3Biases), of(1f), of(1L), of(1f), empty());\n-            var prediction = Softmax(fc3, of(1L));\n-\n-            return prediction;\n-        });\n+        \/\/ Scaling to 0-1\n+        var scaledInput = Div(inputImage, Constant(255f));\n+\n+        \/\/ First conv layer\n+        var conv1Weights = Reshape(Constant(loadConstant(\"conv1-weight-float-le\")), Constant(new long[]{6, 1, 5, 5}), empty());\n+        var conv1Biases = Reshape(Constant(loadConstant(\"conv1-bias-float-le\")), Constant(new long[]{6}), empty());\n+        var conv1 = Conv(scaledInput, conv1Weights, of(conv1Biases), of(new long[4]),\n+                of(new long[]{1,1}), empty(), of(new long[]{1, 1, 1, 1}),\n+                of(1L), of(new long[]{5,5}));\n+        var relu1 = Relu(conv1);\n+\n+        \/\/ First pooling layer\n+        var pool1 = MaxPool(relu1, of(new long[4]), of(new long[]{1,1}), empty(),\n+                of(0L), empty(), of(new long[]{2, 2}), new long[]{2, 2});\n+\n+        \/\/ Second conv layer\n+        var conv2Weights = Reshape(Constant(loadConstant(\"conv2-weight-float-le\")), Constant(new long[]{16, 6, 5, 5}), empty());\n+        var conv2Biases = Reshape(Constant(loadConstant(\"conv2-bias-float-le\")), Constant(new long[]{16}), empty());\n+        var conv2 = Conv(pool1.Y(), conv2Weights, of(conv2Biases), of(new long[4]),\n+                of(new long[]{1,1}), empty(), of(new long[]{1, 1, 1, 1}),\n+                of(1L), of(new long[]{5,5}));\n+        var relu2 = Relu(conv2);\n+\n+        \/\/ Second pooling layer\n+        var pool2 = MaxPool(relu2, of(new long[4]), of(new long[]{1,1}), empty(),\n+                of(0L), empty(), of(new long[]{2, 2}), new long[]{2, 2});\n+\n+        \/\/ Flatten inputs\n+        var flatten = Flatten(pool2.Y(), of(1L));\n+\n+        \/\/ First fully connected layer\n+        var fc1Weights = Reshape(Constant(loadConstant(\"fc1-weight-float-le\")), Constant(new long[]{120, 256}), empty());\n+        var fc1Biases = Reshape(Constant(loadConstant(\"fc1-bias-float-le\")), Constant(new long[]{120}), empty());\n+        var fc1 = Gemm(flatten, fc1Weights, of(fc1Biases), of(1f), of(1L), of(1f), empty());\n+        var relu3 = Relu(fc1);\n+\n+        \/\/ Second fully connected layer\n+        var fc2Weights = Reshape(Constant(loadConstant(\"fc2-weight-float-le\")), Constant(new long[]{84, 120}), empty());\n+        var fc2Biases = Reshape(Constant(loadConstant(\"fc2-bias-float-le\")), Constant(new long[]{84}), empty());\n+        var fc2 = Gemm(relu3, fc2Weights, of(fc2Biases), of(1f), of(1L), of(1f), empty());\n+        var relu4 = Relu(fc2);\n+\n+        \/\/ Softmax layer\n+        var fc3Weights = Reshape(Constant(loadConstant(\"fc3-weight-float-le\")), Constant(new long[]{10, 84}), empty());\n+        var fc3Biases = Reshape(Constant(loadConstant(\"fc3-bias-float-le\")), Constant(new long[]{10}), empty());\n+        var fc3 = Gemm(relu4, fc3Weights, of(fc3Biases), of(1f), of(1L), of(1f), empty());\n+        var prediction = Softmax(fc3, of(1L));\n+\n+        return prediction;\n@@ -147,1 +149,5 @@\n-                    var result = cnn(imageTensor).data().toArray(ValueLayout.JAVA_FLOAT);\n+\n+                    var prediction = OnnxRuntime.execute(MethodHandles.lookup(),\n+                            () -> cnn(imageTensor));\n+\n+                    var result = prediction.data().toArray(ValueLayout.JAVA_FLOAT);\n","filename":"cr-examples\/onnx\/src\/test\/java\/oracle\/code\/onnx\/MNISTDemo.java","additions":58,"deletions":52,"binary":false,"changes":110,"status":"modified"}]}