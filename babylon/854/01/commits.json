[{"commit":{"message":"[hat][example] code-formatted"},"files":[{"filename":"hat\/examples\/flashattention\/src\/main\/java\/flashattention\/Main.java"}],"sha":"9bdca80ab6ff54903a8388eef8f7bcff0f3eb1e5"},{"commit":{"message":"minor change"},"files":[{"filename":"hat\/examples\/flashattention\/src\/main\/java\/flashattention\/Main.java"}],"sha":"fabb593099002ca84fd75a11925917d07df8c65a"},{"commit":{"message":"merge with code-reflection branch"},"files":[{"filename":"hat\/core\/src\/main\/java\/hat\/device\/DeviceSchema.java"},{"filename":"hat\/examples\/flashattention\/src\/main\/java\/flashattention\/Main.java"},{"filename":"hat\/examples\/matmul\/src\/main\/java\/matmul\/Main.java"}],"sha":"947788860c162c8e01b373f38e979ca95a17631c"},{"commit":{"message":"Merge branch 'code-reflection' into hat\/examples\/flash-attention"},"files":[],"sha":"e3abe8ca2d6fa9171244bf5ce8f480a550fa1219"},{"commit":{"message":"[hat][example] flash-attention improved. Speedups calculations fixed"},"files":[{"filename":"hat\/examples\/flashattention\/src\/main\/java\/flashattention\/Main.java"}],"sha":"cc19a67ebc81ecd29d01df740a69d5a0ffcd99ec"},{"commit":{"message":"[hat][example] flash-attention functions renamed to avoid colisions with CUDA functions"},"files":[{"filename":"hat\/examples\/flashattention\/src\/main\/java\/flashattention\/Main.java"}],"sha":"7a3f4a3ff9343440ca6ce70e82d67fada9e992c6"},{"commit":{"message":"[hat][example] Simple flash-attention added"},"files":[{"filename":"hat\/core\/src\/main\/java\/hat\/buffer\/F32Array.java"},{"filename":"hat\/core\/src\/main\/java\/hat\/buffer\/KernelBufferContext.java"},{"filename":"hat\/examples\/flashattention\/pom.xml"},{"filename":"hat\/examples\/flashattention\/src\/main\/java\/flashattention\/Main.java"},{"filename":"hat\/examples\/pom.xml"},{"filename":"hat\/hat.java"},{"filename":"hat\/hat\/bld.java"},{"filename":"hat\/hat\/job.jar"}],"sha":"3b935ed200911cad0d1be97b33b95818f66be9d5"}]