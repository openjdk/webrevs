{"files":[{"patch":"@@ -34,0 +34,1 @@\n+import java.util.stream.IntStream;\n@@ -39,1 +40,5 @@\n-    @Reflect default void  schema(){array(length());}\n+    @Reflect\n+    default void schema() {\n+        array(length());\n+    }\n+\n@@ -41,0 +46,1 @@\n+\n@@ -42,0 +48,1 @@\n+\n@@ -48,1 +55,1 @@\n-    static F32Array create(CommonCarrier cc, int length){\n+    static F32Array create(CommonCarrier cc, int length) {\n@@ -57,2 +64,2 @@\n-    static F32Array createFrom(CommonCarrier cc, float[] arr){\n-        return create( cc, arr.length).copyFrom(arr);\n+    static F32Array createFrom(CommonCarrier cc, float[] arr) {\n+        return create(cc, arr.length).copyFrom(arr);\n","filename":"hat\/core\/src\/main\/java\/hat\/buffer\/F32Array.java","additions":11,"deletions":4,"binary":false,"changes":15,"status":"modified"},{"patch":"@@ -100,1 +100,0 @@\n-\n","filename":"hat\/core\/src\/main\/java\/hat\/buffer\/KernelBufferContext.java","additions":0,"deletions":1,"binary":false,"changes":1,"status":"modified"},{"patch":"@@ -57,1 +57,1 @@\n-    public DeviceSchema(MethodHandles.Lookup lookup, CoreOp.FuncOp funcOp,Class<T> klass) {\n+    public DeviceSchema(MethodHandles.Lookup lookup, CoreOp.FuncOp funcOp, Class<T> klass) {\n","filename":"hat\/core\/src\/main\/java\/hat\/device\/DeviceSchema.java","additions":1,"deletions":1,"binary":false,"changes":2,"status":"modified"},{"patch":"@@ -31,1 +31,1 @@\n-    <artifactId>hat-example-matmul<\/artifactId>\n+    <artifactId>hat-example-flash-attention<\/artifactId>\n","filename":"hat\/examples\/flashattention\/pom.xml","additions":1,"deletions":1,"binary":false,"changes":2,"previous_filename":"hat\/examples\/matmul\/pom.xml","status":"copied"},{"patch":"@@ -0,0 +1,603 @@\n+\/*\n+ * Copyright (c) 2026, Oracle and\/or its affiliates. All rights reserved.\n+ * DO NOT ALTER OR REMOVE COPYRIGHT NOTICES OR THIS FILE HEADER.\n+ *\n+ * This code is free software; you can redistribute it and\/or modify it\n+ * under the terms of the GNU General Public License version 2 only, as\n+ * published by the Free Software Foundation.  Oracle designates this\n+ * particular file as subject to the \"Classpath\" exception as provided\n+ * by Oracle in the LICENSE file that accompanied this code.\n+ *\n+ * This code is distributed in the hope that it will be useful, but WITHOUT\n+ * ANY WARRANTY; without even the implied warranty of MERCHANTABILITY or\n+ * FITNESS FOR A PARTICULAR PURPOSE.  See the GNU General Public License\n+ * version 2 for more details (a copy is included in the LICENSE file that\n+ * accompanied this code).\n+ *\n+ * You should have received a copy of the GNU General Public License version\n+ * 2 along with this work; if not, write to the Free Software Foundation,\n+ * Inc., 51 Franklin St, Fifth Floor, Boston, MA 02110-1301 USA.\n+ *\n+ * Please contact Oracle, 500 Oracle Parkway, Redwood Shores, CA 94065 USA\n+ * or visit www.oracle.com if you need additional information or have any\n+ * questions.\n+ *\/\n+package flashattention;\n+\n+import hat.Accelerator;\n+import hat.ComputeContext;\n+import hat.KernelContext;\n+import hat.backend.Backend;\n+import hat.buffer.F32Array;\n+import hat.device.DeviceSchema;\n+import hat.device.DeviceType;\n+import jdk.incubator.code.Reflect;\n+import optkl.ifacemapper.MappableIface.RW;\n+\n+import java.lang.invoke.MethodHandles;\n+import java.util.ArrayList;\n+import java.util.List;\n+import java.util.Random;\n+import java.util.stream.IntStream;\n+\n+import static hat.Accelerator.Compute;\n+import static hat.NDRange.Global1D;\n+import static hat.NDRange.Local1D;\n+import static hat.NDRange.NDRange1D;\n+import static optkl.ifacemapper.MappableIface.RO;\n+import static optkl.ifacemapper.MappableIface.WO;\n+\n+\/**\n+ * How to run?\n+ *\n+ * <p>\n+ * <code>\n+ * # Using the OpenCL Backend:\n+ * java -cp hat\/job.jar hat.java run ffi-opencl flashattention\n+ * <\/code>\n+ * <\/p>\n+ *\n+ * <p>\n+ * <code>\n+ * # Using the CUDA Backend:\n+ * java -cp hat\/job.jar hat.java run ffi-cuda flashattention\n+ * <\/code>\n+ * <\/p>\n+ *\n+ * <p>\n+ * This version of FlashAttention corresponds with a simplification of version 1 for the forward pass only from the\n+ * original article presented in this paper: {@url https:\/\/arxiv.org\/pdf\/2205.14135}.\n+ * <\/p>\n+ *\n+ * <p>\n+ * This version is used for demonstration purposes. It computes the self-attention on CPU with Java and on GPU with HAT.\n+ * Then, it implements a simplified version of flash-attention (single-head, in FP32, with no tensors).\n+ * This example demonstrates how to perform loop-tiling and private\/shared-memory to improve performance with a\n+ * well-known kernel in the LLM and NLP domains.\n+ * <\/p>\n+ *\/\n+public class Main {\n+\n+    public static final int ITERATIONS = 100;\n+\n+    \/**\n+     * Computes self-attention with HAT in a 1D parallel kernel. It fuses:\n+     * - Matmul: Q @ K^T with a scale factor\n+     * - Softmax\n+     * - Final matmul: attention ^ V\n+     * In a single kernel. But it does not apply the techniques for the self-attention using tiling and shared-memory.\n+     *\n+     * @param kernelContext\n+     * @param Q\n+     * @param K\n+     * @param V\n+     * @param attentionMatrix\n+     * @param O\n+     * @param N\n+     * @param d\n+     * @param softMaxScale\n+     *\/\n+    @Reflect\n+    public static void selfAttentionV2HAT(@RO KernelContext kernelContext,\n+                                          @RO F32Array Q, @RO F32Array K, @RO F32Array V,\n+                                          @WO F32Array attentionMatrix, @WO F32Array O,\n+                                          @RO final int N, @RO final int d, @RO final float softMaxScale) {\n+        int idx = kernelContext.gix;\n+        if (idx < N) {\n+            \/\/ Compute the attention scores: Q * K^T and scale it to sqrt(d) => softMaxScale\n+            for (int j = 0; j < N; j++) {\n+                float acc = 0.0f;\n+                for (int k = 0; k < d; k++) {\n+                    acc += Q.array(idx * d + k) * K.array(j * d + k);\n+                }\n+                \/\/ multiply by the scale factor\n+                acc *= softMaxScale;\n+                \/\/ store partial results in the temporary matrix for flash-attention\n+                attentionMatrix.array(idx * N + j,  acc);\n+            }\n+\n+            \/\/ SoftMax: apply softmax function to the attention score to normalize them\n+            float maxVal = Float.MIN_VALUE;\n+            \/\/ Compute max\n+            for (int j = 0; j < N; j++) {\n+                maxVal = Math.max(maxVal, attentionMatrix.array(idx * N + j));\n+            }\n+            \/\/ Compute exp()\n+            float sum = 0.0f;\n+            for (int j = 0; j < N; j++) {\n+                float p = (float) Math.exp(attentionMatrix.array(idx * N + j) - maxVal);\n+                attentionMatrix.array(idx * N + j, p);\n+                sum += p;\n+            }\n+\n+            \/\/ normalization by the sum compute in the prev. step\n+            for (int j = 0; j < N; j++) {\n+                float val = attentionMatrix.array(idx * N + j) \/ sum;\n+                attentionMatrix.array(idx * N + j, val);\n+            }\n+\n+            \/\/ Final matmul: O = attention * V\n+            for (int j = 0; j < d; j++) {\n+                float acc = 0.0f;\n+                for (int k = 0; k < N; k++) {\n+                    acc += attentionMatrix.array(idx * N + k) * V.array(k * d + j);\n+                }\n+                O.array(idx * d + j,  acc);\n+            }\n+        }\n+    }\n+\n+\n+    public static void selfAttentionV2(F32Array Q, F32Array K, F32Array V,\n+                                       F32Array attentionMatrix, F32Array O,\n+                                       final int N, final int d, final float softMaxScale) {\n+\n+        \/\/ Compute the attention scores: Q * K^T and scale it to sqrt(d) => softMaxScale\n+        for (int i = 0; i < N; i++) {\n+            for (int j = 0; j < N; j++) {\n+                float acc = 0.0f;\n+                for (int k = 0; k < d; k++) {\n+                    acc += Q.array(i * d + k) * K.array(j * d + k);\n+                }\n+                \/\/ multiply by the scale factor\n+                acc *= softMaxScale;\n+                \/\/ store partial results in the temporary matrix for flash-attention\n+                attentionMatrix.array(i * N + j,  acc);\n+            }\n+\n+            \/\/ SoftMax: apply softmax function to the attention score to normalize them\n+            float max = Float.MIN_VALUE;\n+            \/\/ Compute max\n+            for (int j = 0; j < N; j++) {\n+                max = Math.max(max, attentionMatrix.array(i * N + j));\n+            }\n+            float sum = 0.0f;\n+            \/\/ Compute exp()\n+            for (int j = 0; j < N; j++) {\n+                float p = (float) Math.exp(attentionMatrix.array(i * N + j) - max);\n+                attentionMatrix.array(i * N + j, p);\n+                sum += p;\n+            }\n+            \/\/ normalization by the sum compute in the prev. step\n+            for (int j = 0; j < N; j++) {\n+                float val = attentionMatrix.array(i * N + j) \/ sum;\n+                attentionMatrix.array(i * N + j, val);\n+            }\n+\n+            \/\/ Final matmul: O = attention * V\n+            for (int j = 0; j < d; j++) {\n+                float acc = 0.0f;\n+                for (int k = 0; k < N; k++) {\n+                    acc += attentionMatrix.array(i * N + k) * V.array(k * d + j);\n+                }\n+                O.array(i * d + j,  acc);\n+            }\n+        }\n+    }\n+\n+    \/**\n+     * Single-head scale-dot product attention.\n+     * @param Q\n+     * @param K\n+     * @param V\n+     * @param attentionMatrix\n+     * @param O\n+     * @param N\n+     * @param d\n+     * @param softMaxScale\n+     *\/\n+    public static void selfAttention(F32Array Q, F32Array K, F32Array V,\n+                                     F32Array attentionMatrix, F32Array O,\n+                                     final int N, final int d, final float softMaxScale) {\n+\n+        \/\/ Compute attention scores: Q @ K^T and scale it to (1\/sqrt(head_dim))\n+        \/\/ In this example, the parameter d already computes 1\/sqrt(head_dim)\n+        for (int i = 0; i < N; i++) {\n+            for (int j = 0; j < N; j++) {\n+                float acc = 0.0f;\n+                for (int k = 0; k < d; k++) {\n+                    acc += Q.array(i * d + k) * K.array(j * d + k);\n+                }\n+                acc *= softMaxScale;\n+                attentionMatrix.array(i * N + j,  acc);\n+            }\n+        }\n+\n+        \/\/ SoftMax: apply softmax function to the attention score to normalize them\n+        for (int i = 0; i < N; i++) {\n+            \/\/ Compute max\n+            float max = Float.MIN_VALUE;\n+            for (int j = 0; j < N; j++) {\n+                max = Math.max(max, attentionMatrix.array(i * N + j));\n+            }\n+\n+            \/\/ exp(attention[i][j] - max)\n+            \/\/ compute total sum\n+            float sum = 0.0f;\n+            for (int j = 0; j < N; j++) {\n+                float p = (float) Math.exp(attentionMatrix.array(i * N + j) - max);\n+                attentionMatrix.array(i * N + j, p);\n+                sum += p;\n+            }\n+\n+            \/\/ normalize:\n+            \/\/ attention[i][j] \/= sum\n+            for (int j = 0; j < N; j++) {\n+                float val = attentionMatrix.array(i * N + j) \/ sum;\n+                attentionMatrix.array(i * N + j, val);\n+            }\n+        }\n+\n+        \/\/ Final matmul: O = attention @ V\n+        for (int i = 0; i < N; i++) {\n+            for (int j = 0; j < d; j++) {\n+                float acc = 0.0f;\n+                for (int k = 0; k < N; k++) {\n+                    acc += attentionMatrix.array(i * N + k) * V.array(k * d + j);\n+                }\n+                O.array(i * d + j,  acc);\n+            }\n+        }\n+    }\n+\n+    @Reflect\n+    public static void selfAttentionCompute(@RO ComputeContext computeContext, @RO F32Array Q, @RO F32Array K, @RO F32Array V,\n+                                            @WO F32Array attentionMatrix, @WO F32Array O,\n+                                            final int N, final int d, final float softmaxScale) {\n+        var ndRange = NDRange1D.of(Global1D.of(N), Local1D.of(256));\n+        computeContext.dispatchKernel(ndRange, kernelContext -> selfAttentionV2HAT(kernelContext, Q, K, V, attentionMatrix, O, N, d, softmaxScale));\n+    }\n+\n+    \/\/ Express a float array in shared memory with HAT\n+    private interface SharedFloatArray extends DeviceType {\n+        void array(long index, float value);\n+\n+        float array(long index);\n+\n+        DeviceSchema<SharedFloatArray> schema = DeviceSchema.of(MethodHandles.lookup(), null, SharedFloatArray.class,\n+                arr -> arr\n+                        \/\/ final int sharedMemorySize = block_m * head_dim\n+                        \/\/                + block_n * head_dim\n+                        \/\/                + block_n * head_dim\n+                        \/\/                + block_m * block_n;\n+                        .withArray(\"array\", 7168));\n+\n+        static SharedFloatArray createLocal() { return null;}\n+    }\n+\n+    \/\/ Express an array of floats in private memory with HAT\n+    private interface PrivateFloatArray extends DeviceType {\n+        void array(long index, float value);\n+\n+        float array(long index);\n+\n+        DeviceSchema<PrivateFloatArray> schema = DeviceSchema.of(MethodHandles.lookup(), null, PrivateFloatArray.class,\n+                arr -> arr\n+                        \/\/ SIZE = HEAD_DIM (e.g., 64)\n+                        .withArray(\"array\", 64));\n+\n+        static PrivateFloatArray createPrivate() {return null;}\n+    }\n+\n+    @Reflect\n+    public static int ceilFunction(int N, int blockN) {\n+        return (N + blockN - 1) \/ blockN;\n+    }\n+\n+    \/**\n+     * Flash-attention: simplification of the original paper {@url https:\/\/arxiv.org\/abs\/2205.14135}\n+     * using a single-head. It computes the local max and sums using the m and l arrays.\n+     *\n+     * <p>This example is mainly used for illustration purposes, showcasing how to achieve\n+     * a naive version of flash-attention using tiling, private and shared memory on the GPU\n+     * with HAT.<\/p>\n+     *\n+     * @param kernelContext\n+     * @param Q\n+     * @param K\n+     * @param V\n+     * @param O\n+     * @param m\n+     * @param l\n+     * @param N\n+     * @param d\n+     * @param softmaxScale\n+     *\/\n+    @Reflect\n+    public static void flashAttention(@RO KernelContext kernelContext,\n+                                      @RO F32Array Q, @RO F32Array K, @RO F32Array V,\n+                                      @WO F32Array O, @RW F32Array m, @RW F32Array l,\n+                                      final int N, final int d, final float softmaxScale) {\n+        int bx = kernelContext.bix;\n+        int tid = kernelContext.lix;\n+\n+        \/\/ Parameters used\n+        final int headDim = 64;\n+        final int blockM = 32;\n+        final int blockN = 32;\n+\n+        int startIndex = bx * blockM;\n+\n+        \/\/ We use a unique space in shared memory to compute matrices\n+        \/\/ Q, K, V and the intermediate one (S).\n+        \/\/ The way we distinguish the matrices are by using different\n+        \/\/ indexes.\n+        SharedFloatArray sharedArray = SharedFloatArray.createLocal();\n+        int sQ_index = 0;\n+        int baseIndex = blockN * headDim;\n+        int sK_index = baseIndex;\n+        int sV_index = baseIndex * 2;\n+        int sS_index = baseIndex * 3;\n+\n+        \/\/ Load Q into shared memory (sQ_index)\n+        for (int k = 0; k < d; k++) {\n+            sharedArray.array((tid * d + k) + sQ_index,\n+                    Q.array((startIndex + (tid * d + k) * d + k)));\n+        }\n+        kernelContext.barrier();\n+\n+        int numBlocks = ceilFunction(N, blockN);\n+        for (int tileId = 0; tileId < numBlocks; tileId++) {\n+\n+            int kvTileRow = (tileId * blockN) + tid;\n+\n+            \/\/ Load the tiles K and V into shared memoru\n+            for (int k = 0; k < d; k++) {\n+                sharedArray.array((tid * d + k) + sK_index, K.array(kvTileRow * d + k));\n+                sharedArray.array((tid + d + k) + sV_index, V.array(kvTileRow * d + k));\n+            }\n+            kernelContext.barrier();\n+\n+            \/\/ m we accumulate the max values\n+            float m_prev = m.array(tileId * blockN + tid);\n+            \/\/ in l we accumulate the sum values\n+            float l_prev = l.array(tileId * blockN + tid);\n+            float m_block = Float.MIN_VALUE; \/\/ for calculating max\n+            float l_block = 0.0f; \/\/ for sum\n+\n+            \/\/ Compute attention scores: S = Qi @ Kj^T * scale\n+            \/\/ Then: rowmax(m_block)\n+            PrivateFloatArray privateFloatArray = PrivateFloatArray.createPrivate();\n+            for (int t = 0; t < blockN; t++) {\n+                float score = 0.0f;\n+                for (int k = 0; k < d; k++) {\n+                    score += sharedArray.array((tid * d + k) + sQ_index)\n+                           * sharedArray.array((t * d + k) + sK_index);\n+                }\n+                score *= softmaxScale;\n+                privateFloatArray.array((t) + sS_index, score);\n+                m_block = Math.max(m_block, score);\n+            }\n+\n+            \/\/ Compute local sum of Math.exp(p_i - m_block)\n+            for (int t = 0; t < blockN; t++) {\n+                float p = (float) Math.exp(privateFloatArray.array(t) - m_block);\n+                privateFloatArray.array(t, p);\n+                l_block += p;\n+            }\n+\n+            \/\/ Update m and l with the new values\n+            float m_new = Math.max(m_prev, m_block);\n+            float l_new = (float) (Math.exp(m_prev - m_new) * l_prev + Math.exp(m_block - m_new) * l_block);\n+\n+            \/\/ Update the Output (O)\n+            for (int k = 0; k < d; k++) {\n+                float pv = 0.0f;\n+                for (int t = 0; t < blockN; t++) {\n+                    \/\/ MMA: P @ V (V in shared memory)\n+                    pv += privateFloatArray.array(t) * sharedArray.array(t * d + k + sV_index);\n+                }\n+\n+                \/\/ compute the output value using the formula:\n+                \/\/ diag(l_new)^-1 (diag(l_prev)*exp(m_prev-m_new) * O(current) + exp(m_block - m_new) * pv\n+                int oldIndex = startIndex + (tileId * blockN + tid) * d + k;\n+                int oIndex = (bx * blockN + tid) * d + k;\n+                float value = O.array(oIndex);\n+                float outVal = (float) ((l_prev * Math.exp(m_prev - m_new) * value +\n+                               Math.exp(m_block - m_new) * pv) \/ l_new);\n+                \/\/ write output\n+                O.array(oIndex, outVal);\n+            }\n+\n+            \/\/ update m and l in global memory\n+            m.array(tileId * blockN + tid, m_new);\n+            l.array(tileId * blockN + tid, l_new);\n+\n+            kernelContext.barrier();\n+        }\n+    }\n+\n+    @Reflect\n+    public static void computeFlashAttention(@RO ComputeContext computeContext,\n+                                             @RO F32Array Q, @RO F32Array K, @RO F32Array V,\n+                                             @WO F32Array O, @RW F32Array m, @RW F32Array l,\n+                                             final int N, final int d, final float scale, final int blockSize) {\n+        var ndRange = NDRange1D.of(Global1D.of(N), Local1D.of(blockSize));\n+        computeContext.dispatchKernel(ndRange, kernelContext -> flashAttention(kernelContext, Q, K, V, O, m, l, N, d, scale));\n+    }\n+\n+    public static boolean checkResult(F32Array O_reference, F32Array O, final int matrixSize) {\n+        for (int i = 0; i < matrixSize; i++) {\n+            if (Math.abs(O_reference.array(i) - O.array(i)) > 0.1f) {\n+                IO.println(\"Iteration: #\" + i + \" \" + O_reference.array(i) + \" != \" + O.array(i));\n+                return false;\n+            }\n+        }\n+        return true;\n+    }\n+\n+    public static double computeAverage(List<Long> timers, int discard) {\n+        double sum = timers.stream().skip(discard).reduce(0L, Long::sum).doubleValue();\n+        int totalCountedValues = timers.size() - discard;\n+        return (sum \/ totalCountedValues);\n+    }\n+\n+    @Reflect\n+    static void main(String[] args) {\n+        IO.println(\"Example of Flash-Attention in HAT\");\n+\n+        var lookup = MethodHandles.lookup();\n+        var accelerator = new Accelerator(lookup, Backend.FIRST);\n+\n+        boolean verbose = false;\n+        if (args.length > 0) {\n+            verbose = args[0].equals(\"verbose\");\n+            IO.println(\"Verbose mode on? \" + verbose);\n+        }\n+\n+        \/\/ Configuration parameters\n+        final int sequenceLen = 512;   \/\/ represent the number of tokens (or words)\n+        final int headDim = 64;        \/\/ vector representation for a single token\n+        final int blockM = 32;         \/\/ tile size\n+        final int blockN = 32;         \/\/ tile size\n+        final float softmaxScale = (float) (1.0f\/Math.sqrt(headDim));\n+\n+        final int sharedMemorySize = blockM * headDim\n+                + blockN * headDim\n+                + blockN * headDim\n+                + blockM * blockN;\n+        IO.println(\"Shared Array Size: \" + sharedMemorySize);\n+\n+        \/\/ Inputs preparation\n+        final int matrixSize = sequenceLen * headDim;\n+        var Q = F32Array.create(accelerator, matrixSize);\n+        var K = F32Array.create(accelerator, matrixSize);\n+        var V = F32Array.create(accelerator, matrixSize);\n+        var m = F32Array.create(accelerator, matrixSize);\n+        var l = F32Array.create(accelerator, matrixSize);\n+        var O_flashAttention = F32Array.create(accelerator, matrixSize);\n+        var O_selfAttention = F32Array.create(accelerator, matrixSize);\n+        var O_java = F32Array.create(accelerator, matrixSize);\n+\n+        F32Array attentionMatrix = F32Array.create(accelerator, sequenceLen * sequenceLen);\n+\n+        \/\/ Initialize matrices with random values\n+        \/\/ In the real-world, this will be calculated from the input embeddings\n+        Random r = new Random(71);\n+        for (int i = 0; i < matrixSize; i++) {\n+            Q.array(i, r.nextFloat(1));\n+            K.array(i, r.nextFloat(1));\n+            V.array(i, r.nextFloat(1));\n+        }\n+\n+        IntStream.range(0, m.length()).forEach(k -> m.array(k, 0.0f));\n+        IntStream.range(0, l.length()).forEach(k -> l.array(k, 1.0f));\n+\n+        List<Long> timersSelfAttentionJava = new ArrayList<>();\n+        List<Long> timersSelfAttentionHAT = new ArrayList<>();\n+        List<Long> timersFlashAttentionHAT = new ArrayList<>();\n+\n+        \/\/ Run the CPU version with Java:\n+        for (int i = 0; i < ITERATIONS; i++) {\n+\n+            \/\/ reset attention\n+            IntStream.range(0, attentionMatrix.length()).forEach(k -> attentionMatrix.array(k, 0.0f));\n+            IntStream.range(0, O_java.length()).forEach(k -> O_java.array(k, 0.0f));\n+\n+            long start = System.nanoTime();\n+            selfAttentionV2(Q, K, V, attentionMatrix, O_java, sequenceLen, headDim, softmaxScale);\n+            long end = System.nanoTime();\n+            timersSelfAttentionJava.add((end-start));\n+\n+            if (verbose) {\n+                IO.println(\"Sequential elapsed time: \" + (end - start) + \" ns\");\n+            }\n+        }\n+\n+        \/\/ Run self-attention algorithm with HAT\n+        for (int i = 0; i < ITERATIONS; i++) {\n+            long start = System.nanoTime();\n+            accelerator.compute((@Reflect Compute)\n+                    cc -> Main.selfAttentionCompute(\n+                            cc,\n+                            Q,\n+                            K,\n+                            V,\n+                            attentionMatrix,\n+                            O_selfAttention,\n+                            sequenceLen,\n+                            headDim,\n+                            softmaxScale));\n+\n+            long end = System.nanoTime();\n+            timersSelfAttentionHAT.add((end - start));\n+            if (verbose) {\n+                IO.println(\"HAT-Self-Attention elapsed time: \" + (end - start) + \" ns\");\n+            }\n+        }\n+\n+        \/\/ Run flashAttention in HAT\n+        for (int i = 0; i < ITERATIONS; i++) {\n+            long start = System.nanoTime();\n+            accelerator.compute((@Reflect Compute)\n+                    cc -> Main.computeFlashAttention(\n+                            cc,\n+                            Q,\n+                            K,\n+                            V,\n+                            O_flashAttention,\n+                            m, l,\n+                            sequenceLen,\n+                            headDim,\n+                            softmaxScale,\n+                            blockM));\n+\n+            long end = System.nanoTime();\n+            timersFlashAttentionHAT.add((end - start));\n+            if (verbose) {\n+                IO.println(\"HAT-Flash-Attention elapsed time: \" + (end - start) + \" ns\");\n+            }\n+        }\n+\n+        \/\/ Check results\n+        boolean isHATSelfAttentionCorrect = checkResult(O_java, O_selfAttention, matrixSize);\n+        if (isHATSelfAttentionCorrect) {\n+            IO.println(\"HAT-Self-Attention Result is correct\");\n+        } else {\n+            IO.println(\"HAT-Self-Attention is wrong\");\n+        }\n+        boolean isFlashAttentionCorrect = checkResult(O_java, O_flashAttention, matrixSize);\n+        if (isFlashAttentionCorrect) {\n+            IO.println(\"HAT-Flash-Attention is correct\");\n+        } else {\n+            IO.println(\"HAT_Flash-Attention is wrong. Note: expected due to use of multiple Math.exp operations \" +\n+                    \"not present in the self-attention version.\");\n+        }\n+\n+        \/\/ Perf. Metrics\n+        final int skip = ITERATIONS\/2;\n+        double averageJavaTimer = computeAverage(timersSelfAttentionJava, skip);\n+        double averageSelfAttentionHAT = computeAverage(timersSelfAttentionHAT, skip);\n+        double averageFlashAttentionHAT = computeAverage(timersFlashAttentionHAT, skip);\n+\n+        IO.println(\"\\nAverage elapsed time:\");\n+        IO.println(\"Average Java Self-Attention: \" + averageJavaTimer);\n+        IO.println(\"Average HAT Self-Attention : \" + averageSelfAttentionHAT);\n+        IO.println(\"Average HAT Flash-Attention: \" + averageFlashAttentionHAT);\n+\n+        IO.println(\"\\nSpeedups:\");\n+        IO.println(\"Java \/ HAT-Self-Attention  = \" + (Math.ceil(averageJavaTimer\/ averageSelfAttentionHAT * 100) \/ 100) + \"x\");\n+        IO.println(\"Java \/ HAT-Flash-Attention = \" + (Math.ceil(averageJavaTimer\/ averageFlashAttentionHAT * 100) \/ 100) + \"x\");\n+        IO.println(\"HAT-Self-Attention \/ HAT-Flash-Attention = \" + (Math.ceil(averageSelfAttentionHAT\/ averageFlashAttentionHAT * 100) \/ 100) + \"x\");\n+    }\n+}\n","filename":"hat\/examples\/flashattention\/src\/main\/java\/flashattention\/Main.java","additions":603,"deletions":0,"binary":false,"changes":603,"status":"added"},{"patch":"@@ -125,1 +125,1 @@\n-        DeviceSchema<MyLocalArrayFixedSize> schema = DeviceSchema.of(MethodHandles.lookup(),null,MyLocalArrayFixedSize.class,\n+        DeviceSchema<MyLocalArrayFixedSize> schema = DeviceSchema.of(MethodHandles.lookup(),null, MyLocalArrayFixedSize.class,\n","filename":"hat\/examples\/matmul\/src\/main\/java\/matmul\/Main.java","additions":1,"deletions":1,"binary":false,"changes":2,"status":"modified"},{"patch":"@@ -52,0 +52,1 @@\n+        <module>flashattention<\/module>\n","filename":"hat\/examples\/pom.xml","additions":1,"deletions":0,"binary":false,"changes":1,"status":"modified"},{"patch":"@@ -148,0 +148,1 @@\n+        var example_flash_attention = hat.jar(\"example{s}-flashattention\", core);\n","filename":"hat\/hat.java","additions":1,"deletions":0,"binary":false,"changes":1,"status":"modified"},{"patch":"@@ -312,1 +312,1 @@\n-    Stream.of( \"blackscholes\", \"squares\", \"matmul\")\n+    Stream.of( \"blackscholes\", \"squares\", \"matmul\", \"flashattention\")\n","filename":"hat\/hat\/bld.java","additions":1,"deletions":1,"binary":false,"changes":2,"status":"modified"},{"filename":"hat\/hat\/job.jar","binary":true,"status":"modified"}]}