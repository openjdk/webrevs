{"files":[{"patch":"@@ -153,3 +153,0 @@\n-\n-\n-\n@@ -157,1 +154,0 @@\n-  \/\/std::cout << \"inside nvcc\" << std::endl;\n@@ -197,2 +193,0 @@\n-     \/\/ std::cout << \"inside compile\" << std::endl;\n-    \/\/ std::cout << \"cuda \" << cudaSource->text << std::endl;\n@@ -200,1 +194,0 @@\n-       \/\/ std::cout << \"ptx \" << ptx->text << std::endl;\n@@ -207,1 +200,0 @@\n-\n","filename":"hat\/backends\/ffi\/cuda\/src\/main\/native\/cpp\/cuda_backend.cpp","additions":0,"deletions":8,"binary":false,"changes":8,"status":"modified"},{"patch":"@@ -67,2 +67,0 @@\n-\n-\n@@ -73,3 +71,0 @@\n-\n-\n-\n@@ -110,1 +105,0 @@\n-\n@@ -129,1 +123,0 @@\n-\n@@ -141,0 +134,11 @@\n+\/\/ TODO: Improve heuristics to decide a better block size, if possible.\n+\/\/ The following is just a rough number to fit into a modern NVIDIA GPU.\n+int CudaBackend::CudaQueue::estimateThreadsPerBlock(int dimensions) {\n+    switch (dimensions) {\n+        case 1: return 256;\n+        case 2: return 16;\n+        case 3: return 16;\n+        default: return 1;\n+    }\n+}\n+\n@@ -144,5 +148,12 @@\n-    const int range = kernelContext->maxX;\n-    int rangediv1024 = range \/ 1024;\n-    int rangemod1024 = range % 1024;\n-    if (rangemod1024 > 0) {\n-        rangediv1024++;\n+    const int threadsPerBlock = estimateThreadsPerBlock(kernelContext->dimensions);\n+\n+    int blocksPerGridX = (kernelContext->maxX + threadsPerBlock - 1) \/ threadsPerBlock;\n+    int blocksPerGridY = 1;\n+    int blocksPerGridZ = 1;\n+    int threadsPerBlockX = threadsPerBlock;\n+    int threadsPerBlockY = 1;\n+    int threadsPerBlockZ = 1;\n+\n+    if (kernelContext->dimensions > 1) {\n+        blocksPerGridY = (kernelContext->maxY + threadsPerBlock - 1) \/ threadsPerBlock;\n+        threadsPerBlockY = threadsPerBlock;\n@@ -150,5 +161,14 @@\n-\/\/ std::cout << \"Running the kernel...\" << std::endl;\n-\/\/ std::cout << \"   Requested range   = \" << range << std::endl;\n-\/\/ std::cout << \"   Range mod 1024    = \" << rangemod1024 << std::endl;\n-\/\/ std::cout << \"   Actual range 1024 = \" << (rangediv1024 * 1024) << std::endl;\n-\/\/  auto status= static_cast<CUresult>(cudaStreamSynchronize(cudaBackend->cudaQueue.cuStream));\n+    if (kernelContext->dimensions > 2) {\n+        blocksPerGridZ = (kernelContext->maxZ + threadsPerBlock - 1) \/ threadsPerBlock;\n+        threadsPerBlockZ = threadsPerBlock;\n+    }\n+\n+    \/\/ Enable debug information with trace. Use HAT=TRACE\n+    if (backend->config->trace) {\n+        std::cout << \"Dispatching the CUDA kernel\" << std::endl;\n+        std::cout << \"   \\\\_ BlocksPerGrid  = [\" << blocksPerGridX << \",\" << blocksPerGridY << \",\" << blocksPerGridZ << \"]\" << std::endl;\n+        std::cout << \"   \\\\_ ThreadsPerBlock  [\" << threadsPerBlockX << \",\" << threadsPerBlockY << \",\" << threadsPerBlockZ << \"]\" << std::endl;\n+    }\n+\n+    \/\/  auto status= static_cast<CUresult>(cudaStreamSynchronize(cudaBackend->cudaQueue.cuStream));\n+    \/\/  cudaBackend->cudaQueue.wait();\n@@ -156,1 +176,0 @@\n-\/\/  cudaBackend->cudaQueue.wait();\n@@ -162,5 +181,7 @@\n-    const auto status = cuLaunchKernel(cudaKernel->function,\n-                                 rangediv1024, 1, 1,\n-                                 1024, 1, 1,\n-                                 0, cuStream,\n-                                 cudaKernel->argslist, nullptr);\n+    const auto status = cuLaunchKernel(cudaKernel->function, \/\/\n+                                 blocksPerGridX, blocksPerGridY, blocksPerGridZ, \/\/\n+                                 threadsPerBlockX, threadsPerBlockY, threadsPerBlockZ, \/\/\n+                                 0, \/\/\n+                                 cuStream, \/\/\n+                                 cudaKernel->argslist, \/\/\n+                                 nullptr);\n","filename":"hat\/backends\/ffi\/cuda\/src\/main\/native\/cpp\/cuda_backend_queue.cpp","additions":44,"deletions":23,"binary":false,"changes":67,"status":"modified"},{"patch":"@@ -116,0 +116,2 @@\n+        int estimateThreadsPerBlock(int dimensions);\n+\n@@ -119,1 +121,0 @@\n-\n","filename":"hat\/backends\/ffi\/cuda\/src\/main\/native\/include\/cuda_backend.h","additions":2,"deletions":1,"binary":false,"changes":3,"status":"modified"}]}