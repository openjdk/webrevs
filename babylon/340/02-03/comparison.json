{"files":[{"patch":"@@ -19,1 +19,1 @@\n-public record LambdaToFunc(CoreOp.FuncOp func, int[] operandsMapping) {\n+public record LambdaToFunc(OnnxTransformer.OnnxFuncOp func, int[] operandsMapping) {\n@@ -35,1 +35,1 @@\n-                CoreOp.FuncOp onnxFunc = OnnxTransformer.transform(l, f);\n+                OnnxTransformer.OnnxFuncOp onnxFunc = OnnxTransformer.transform(l, f);\n@@ -50,1 +50,1 @@\n-        CoreOp.FuncOp onnxFunc = OnnxTransformer.transform(l, CoreOp.func(\"onnxCode\", functionType)\n+        OnnxTransformer.OnnxFuncOp onnxFunc = OnnxTransformer.transform(l, CoreOp.func(\"onnxCode\", functionType)\n","filename":"cr-examples\/onnx\/src\/main\/java\/oracle\/code\/onnx\/LambdaToFunc.java","additions":3,"deletions":3,"binary":false,"changes":6,"status":"modified"},{"patch":"@@ -302,0 +302,1 @@\n+        int first = params.size() - initializers.size();\n@@ -303,2 +304,2 @@\n-                IntStream.range(0, initializers.size()).mapToObj(i -> tensorProto(indexer.getName(params.get(i)), initializers.get(i))).toList(),\n-                params.subList(initializers.size(), params.size()).stream().map(v ->\n+                IntStream.range(0, initializers.size()).mapToObj(i -> tensorProto(indexer.getName(params.get(i + first)), initializers.get(i))).toList(),\n+                params.stream().map(v ->\n","filename":"cr-examples\/onnx\/src\/main\/java\/oracle\/code\/onnx\/OnnxProtoBuilder.java","additions":3,"deletions":2,"binary":false,"changes":5,"status":"modified"},{"patch":"@@ -57,1 +57,0 @@\n-        private int in;\n@@ -59,1 +58,1 @@\n-        CachedSession computeIfAbsent(Class<?> lambdaClass, MethodHandles.Lookup l,  Quoted q, int initializers) {\n+        CachedSession computeIfAbsent(Class<?> lambdaClass, MethodHandles.Lookup l,  Quoted q) {\n@@ -63,1 +62,0 @@\n-                this.in = initializers;\n@@ -69,1 +67,0 @@\n-                this.in = 0;\n@@ -77,7 +74,1 @@\n-            var captured = q.capturedValues().sequencedValues().toArray();\n-            List<Tensor> initializers = IntStream.range(0, in)\n-                    .map(i -> mf.operandsMapping()[i])\n-                    .mapToObj(i -> captured[i])\n-                    .map(val -> val instanceof CoreOp.Var<?> v ? v.value() : val)\n-                    .map(val -> (Tensor) val)\n-                    .toList();\n+            List<Tensor> initializers = mf.func().initializers().stream().map(val -> (Tensor) val).toList();\n@@ -87,1 +78,1 @@\n-                    OnnxProtoBuilder.build(mf.func().body().entryBlock(), initializers)), mf.operandsMapping());\n+                    OnnxProtoBuilder.build(mf.func().func().body().entryBlock(), initializers)), mf.operandsMapping());\n@@ -94,1 +85,1 @@\n-        return execute(l, 0, codeLambda);\n+        return execute(Arena.ofAuto(), l, codeLambda);\n@@ -97,3 +88,0 @@\n-    public static <T> Tensor<T> execute(MethodHandles.Lookup l, int initializers, OnnxFunction<Tensor<T>> codeLambda) {\n-        return execute(Arena.ofAuto(), l, initializers, codeLambda);\n-    }\n@@ -101,1 +89,1 @@\n-    public static <T> Tensor<T> execute(Arena arena, MethodHandles.Lookup l, int initializers, OnnxFunction<Tensor<T>> codeLambda) {\n+    public static <T> Tensor<T> execute(Arena arena, MethodHandles.Lookup l, OnnxFunction<Tensor<T>> codeLambda) {\n@@ -104,1 +92,1 @@\n-        var model = SESSION_CACHE.computeIfAbsent(codeLambda.getClass(), l, q, initializers);\n+        var model = SESSION_CACHE.computeIfAbsent(codeLambda.getClass(), l, q);\n@@ -107,3 +95,2 @@\n-        var mapping = model.operandsMapping();\n-        List<Tensor> arguments = IntStream.range(initializers, mapping.length)\n-                .mapToObj(i -> captured[mapping[i]])\n+        List<Tensor> arguments = IntStream.of(model.operandsMapping())\n+                .mapToObj(i -> captured[i])\n","filename":"cr-examples\/onnx\/src\/main\/java\/oracle\/code\/onnx\/OnnxRuntime.java","additions":8,"deletions":21,"binary":false,"changes":29,"status":"modified"},{"patch":"@@ -38,0 +38,2 @@\n+import java.util.logging.Level;\n+import java.util.logging.Logger;\n@@ -40,1 +42,1 @@\n-import oracle.code.onnx.LambdaToFunc;\n+import oracle.code.onnx.Tensor;\n@@ -46,0 +48,1 @@\n+    static final TypeElement TENSOR_RAW_CLASS = JavaType.type(Tensor.class);\n@@ -54,0 +57,2 @@\n+    List<Object> initializers;\n+\n@@ -57,0 +62,1 @@\n+        this.initializers = new ArrayList<>();\n@@ -335,0 +341,8 @@\n+            unevaluatedOperations.add(o);\n+            return null;\n+        } else if (o instanceof CoreOp.FieldAccessOp.FieldLoadOp fo && fo.fieldDescriptor().type() instanceof ClassType ct && ct.rawType().equals(TENSOR_RAW_CLASS)) {\n+            try {\n+                initializers.add(fo.fieldDescriptor().resolveToHandle(l).get());\n+            } catch (ReflectiveOperationException ex) {\n+                throw interpreterException(ex);\n+            }\n","filename":"cr-examples\/onnx\/src\/main\/java\/oracle\/code\/onnx\/compiler\/OnnxPartialEvaluator.java","additions":15,"deletions":1,"binary":false,"changes":16,"status":"modified"},{"patch":"@@ -32,1 +32,3 @@\n-    public static CoreOp.FuncOp transform(MethodHandles.Lookup l, CoreOp.FuncOp in) {\n+    public record OnnxFuncOp(CoreOp.FuncOp func, List<Object> initializers) {}\n+\n+    public static OnnxFuncOp transform(MethodHandles.Lookup l, CoreOp.FuncOp in) {\n@@ -107,1 +109,1 @@\n-                                var params = ltf.func().body().entryBlock().parameters();\n+                                var params = ltf.func().func().body().entryBlock().parameters();\n@@ -113,1 +115,1 @@\n-                                ltf.func().body().entryBlock().ops().forEach(eb::apply);\n+                                ltf.func().func().body().entryBlock().ops().forEach(eb::apply);\n@@ -134,0 +136,3 @@\n+                    case CoreOp.FieldAccessOp.FieldLoadOp fo when fo.fieldDescriptor().type() instanceof ClassType ct && ct.rawType().equals(TENSOR_RAW_CLASS) -> {\n+                        bb.context().mapValue(fo.result(), b.parameter(type(fo.resultType())));\n+                    }\n@@ -141,1 +146,1 @@\n-        return SSA.transform(onnxModel).transform((b, op) -> {\n+        return new OnnxFuncOp(SSA.transform(onnxModel).transform((b, op) -> {\n@@ -147,1 +152,1 @@\n-        });\n+        }), pe.initializers);\n","filename":"cr-examples\/onnx\/src\/main\/java\/oracle\/code\/onnx\/compiler\/OnnxTransformer.java","additions":10,"deletions":5,"binary":false,"changes":15,"status":"modified"},{"patch":"@@ -325,2 +325,2 @@\n-            CoreOp.FuncOp onnxModel = OnnxTransformer.transform(MethodHandles.lookup(), f);\n-            System.out.println(onnxModel.toText());\n+            var onnxModel = OnnxTransformer.transform(MethodHandles.lookup(), f);\n+            System.out.println(onnxModel.func().toText());\n@@ -331,1 +331,1 @@\n-            Assertions.assertEquals(serialize(expectedOnnxModel), serialize(onnxModel));\n+            Assertions.assertEquals(serialize(expectedOnnxModel), serialize(onnxModel.func()));\n@@ -370,1 +370,1 @@\n-            test(arena, inputImage -> OnnxRuntime.execute(arena, MethodHandles.lookup(), 10, () ->\n+            test(arena, inputImage -> OnnxRuntime.execute(arena, MethodHandles.lookup(), () ->\n","filename":"cr-examples\/onnx\/src\/test\/java\/oracle\/code\/onnx\/CNNTest.java","additions":4,"deletions":4,"binary":false,"changes":8,"status":"modified"},{"patch":"@@ -158,10 +158,0 @@\n-    @Test\n-    public void testInitializer() throws Exception {\n-        var a = Tensor.ofFlat(1f, 2, 3, 4);\n-        var b = Tensor.ofFlat(5f, 6, 7, 8);\n-        var c = Tensor.ofFlat(6f, 8, 10, 12);\n-        assertEquals(c, OnnxRuntime.execute(MethodHandles.lookup(), 0, () -> add(a, b)));\n-        assertEquals(c, OnnxRuntime.execute(MethodHandles.lookup(), 1, () -> add(a, b)));\n-        assertEquals(c, OnnxRuntime.execute(MethodHandles.lookup(), 2, () -> add(a, b)));\n-    }\n-\n","filename":"cr-examples\/onnx\/src\/test\/java\/oracle\/code\/onnx\/SimpleTest.java","additions":0,"deletions":10,"binary":false,"changes":10,"status":"modified"},{"patch":"@@ -1,150 +0,0 @@\n-\/*\n- * Copyright (c) 2025, Oracle and\/or its affiliates. All rights reserved.\n- * DO NOT ALTER OR REMOVE COPYRIGHT NOTICES OR THIS FILE HEADER.\n- *\n- * This code is free software; you can redistribute it and\/or modify it\n- * under the terms of the GNU General Public License version 2 only, as\n- * published by the Free Software Foundation.\n- *\n- * This code is distributed in the hope that it will be useful, but WITHOUT\n- * ANY WARRANTY; without even the implied warranty of MERCHANTABILITY or\n- * FITNESS FOR A PARTICULAR PURPOSE.  See the GNU General Public License\n- * version 2 for more details (a copy is included in the LICENSE file that\n- * accompanied this code).\n- *\n- * You should have received a copy of the GNU General Public License version\n- * 2 along with this work; if not, write to the Free Software Foundation,\n- * Inc., 51 Franklin St, Fifth Floor, Boston, MA 02110-1301 USA.\n- *\n- * Please contact Oracle, 500 Oracle Parkway, Redwood Shores, CA 94065 USA\n- * or visit www.oracle.com if you need additional information or have any\n- * questions.\n- *\/\n-\n-package oracle.code.onnx.mnist;\n-\n-import jdk.incubator.code.CodeReflection;\n-\n-import java.io.*;\n-import java.lang.foreign.Arena;\n-import java.lang.foreign.MemorySegment;\n-import java.lang.foreign.ValueLayout;\n-import java.lang.invoke.MethodHandles;\n-import oracle.code.onnx.OnnxRuntime;\n-import oracle.code.onnx.Tensor;\n-\n-import static java.util.Optional.empty;\n-import static java.util.Optional.of;\n-import static oracle.code.onnx.OnnxOperators.*;\n-\n-public class MNISTDemo {\n-\n-    static final int IMAGE_SIZE = 28;\n-    static final long[] IMAGE_SHAPE = new long[]{1, 1, IMAGE_SIZE, IMAGE_SIZE};\n-\n-    @CodeReflection\n-    public static Tensor<Float> cnn(Tensor<Float> conv1Weights, Tensor<Float> conv1Biases,\n-                                    Tensor<Float> conv2Weights, Tensor<Float> conv2Biases,\n-                                    Tensor<Float> fc1Weights, Tensor<Float> fc1Biases,\n-                                    Tensor<Float> fc2Weights, Tensor<Float> fc2Biases,\n-                                    Tensor<Float> fc3Weights, Tensor<Float> fc3Biases,\n-                                    Tensor<Float> inputImage) {\n-        \/\/ Scaling to 0-1\n-        var scaledInput = Div(inputImage, Constant(255f));\n-\n-        \/\/ First conv layer\n-        var conv1 = Conv(scaledInput, conv1Weights, of(conv1Biases), of(new long[4]),\n-                of(new long[]{1,1}), empty(), of(new long[]{1, 1, 1, 1}),\n-                of(1L), of(new long[]{5,5}));\n-        var relu1 = Relu(conv1);\n-\n-        \/\/ First pooling layer\n-        var pool1 = MaxPool(relu1, of(new long[4]), of(new long[]{1,1}), empty(),\n-                of(0L), empty(), of(new long[]{2, 2}), new long[]{2, 2});\n-\n-        \/\/ Second conv layer\n-        var conv2 = Conv(pool1.Y(), conv2Weights, of(conv2Biases), of(new long[4]),\n-                of(new long[]{1,1}), empty(), of(new long[]{1, 1, 1, 1}),\n-                of(1L), of(new long[]{5,5}));\n-        var relu2 = Relu(conv2);\n-\n-        \/\/ Second pooling layer\n-        var pool2 = MaxPool(relu2, of(new long[4]), of(new long[]{1,1}), empty(),\n-                of(0L), empty(), of(new long[]{2, 2}), new long[]{2, 2});\n-\n-        \/\/ Flatten inputs\n-        var flatten = Flatten(pool2.Y(), of(1L));\n-\n-        \/\/ First fully connected layer\n-        var fc1 = Gemm(flatten, fc1Weights, of(fc1Biases), of(1f), of(1L), of(1f), empty());\n-        var relu3 = Relu(fc1);\n-\n-        \/\/ Second fully connected layer\n-        var fc2 = Gemm(relu3, fc2Weights, of(fc2Biases), of(1f), of(1L), of(1f), empty());\n-        var relu4 = Relu(fc2);\n-\n-        \/\/ Softmax layer\n-        var fc3 = Gemm(relu4, fc3Weights, of(fc3Biases), of(1f), of(1L), of(1f), empty());\n-        var prediction = Softmax(fc3, of(1L));\n-\n-        return prediction;\n-    }\n-\n-    private static Tensor<Float> initialize(String resource, long... shape) {\n-        try (var in = MNISTDemo.class.getResourceAsStream(resource)) {\n-            return Tensor.ofShape(shape, MemorySegment.ofArray(in.readAllBytes()).toArray(ValueLayout.JAVA_FLOAT_UNALIGNED));\n-        } catch (IOException e) {\n-            throw new UncheckedIOException(e);\n-        }\n-    }\n-\n-    private final Tensor<Float> conv1Weights, conv1Biases,\n-                                conv2Weights, conv2Biases,\n-                                fc1Weights, fc1Biases,\n-                                fc2Weights, fc2Biases,\n-                                fc3Weights, fc3Biases;\n-\n-    public MNISTDemo() {\n-        conv1Weights = initialize(\"conv1-weight-float-le\", 6, 1, 5, 5);\n-        conv1Biases = initialize(\"conv1-bias-float-le\", 6);\n-        conv2Weights = initialize(\"conv2-weight-float-le\", 16, 6, 5, 5);\n-        conv2Biases = initialize(\"conv2-bias-float-le\", 16);\n-        fc1Weights = initialize(\"fc1-weight-float-le\", 120, 256);\n-        fc1Biases = initialize(\"fc1-bias-float-le\", 120);\n-        fc2Weights = initialize(\"fc2-weight-float-le\", 84, 120);\n-        fc2Biases = initialize(\"fc2-bias-float-le\", 84);\n-        fc3Weights = initialize(\"fc3-weight-float-le\", 10, 84);\n-        fc3Biases = initialize(\"fc3-bias-float-le\", 10);\n-    }\n-\n-    public float[] classify(float[] imageData) {\n-\n-        return classify(conv1Weights, conv1Biases,\n-                        conv2Weights, conv2Biases,\n-                        fc1Weights, fc1Biases,\n-                        fc2Weights, fc2Biases,\n-                        fc3Weights, fc3Biases,\n-                        imageData);\n-    }\n-\n-    float[] classify(Tensor<Float> conv1Weights, Tensor<Float> conv1Biases,\n-                     Tensor<Float> conv2Weights, Tensor<Float> conv2Biases,\n-                     Tensor<Float> fc1Weights, Tensor<Float> fc1Biases,\n-                     Tensor<Float> fc2Weights, Tensor<Float> fc2Biases,\n-                     Tensor<Float> fc3Weights, Tensor<Float> fc3Biases,\n-                     float[] imageData) {\n-\n-        try (var arena = Arena.ofConfined()) {\n-\n-            Tensor<Float> imageTensor = Tensor.ofShape(arena, IMAGE_SHAPE, imageData);\n-\n-            return OnnxRuntime.execute(arena, MethodHandles.lookup(), 10,\n-                    () -> cnn(conv1Weights, conv1Biases,\n-                              conv2Weights, conv2Biases,\n-                              fc1Weights, fc1Biases,\n-                              fc2Weights, fc2Biases,\n-                              fc3Weights, fc3Biases,\n-                              imageTensor)).data().toArray(ValueLayout.JAVA_FLOAT);\n-        }\n-    }\n-}\n","filename":"cr-examples\/onnx\/src\/test\/java\/oracle\/code\/onnx\/mnist\/MNISTDemo.java","additions":0,"deletions":150,"binary":false,"changes":150,"status":"deleted"},{"patch":"@@ -35,1 +35,1 @@\n-import static oracle.code.onnx.mnist.MNISTDemo.IMAGE_SIZE;\n+import static oracle.code.onnx.mnist.MNISTModel.IMAGE_SIZE;\n@@ -49,1 +49,1 @@\n-        var mnist = new MNISTDemo();\n+        var mnist = new MNISTModel();\n","filename":"cr-examples\/onnx\/src\/test\/java\/oracle\/code\/onnx\/mnist\/MNISTDemoUI.java","additions":2,"deletions":2,"binary":false,"changes":4,"status":"modified"},{"patch":"@@ -0,0 +1,130 @@\n+\/*\n+ * Copyright (c) 2025, Oracle and\/or its affiliates. All rights reserved.\n+ * DO NOT ALTER OR REMOVE COPYRIGHT NOTICES OR THIS FILE HEADER.\n+ *\n+ * This code is free software; you can redistribute it and\/or modify it\n+ * under the terms of the GNU General Public License version 2 only, as\n+ * published by the Free Software Foundation.\n+ *\n+ * This code is distributed in the hope that it will be useful, but WITHOUT\n+ * ANY WARRANTY; without even the implied warranty of MERCHANTABILITY or\n+ * FITNESS FOR A PARTICULAR PURPOSE.  See the GNU General Public License\n+ * version 2 for more details (a copy is included in the LICENSE file that\n+ * accompanied this code).\n+ *\n+ * You should have received a copy of the GNU General Public License version\n+ * 2 along with this work; if not, write to the Free Software Foundation,\n+ * Inc., 51 Franklin St, Fifth Floor, Boston, MA 02110-1301 USA.\n+ *\n+ * Please contact Oracle, 500 Oracle Parkway, Redwood Shores, CA 94065 USA\n+ * or visit www.oracle.com if you need additional information or have any\n+ * questions.\n+ *\/\n+\n+package oracle.code.onnx.mnist;\n+\n+import jdk.incubator.code.CodeReflection;\n+\n+import java.io.*;\n+import java.lang.foreign.Arena;\n+import java.lang.foreign.MemorySegment;\n+import java.lang.foreign.ValueLayout;\n+import java.lang.invoke.MethodHandles;\n+import oracle.code.onnx.OnnxRuntime;\n+import oracle.code.onnx.Tensor;\n+\n+import static java.util.Optional.empty;\n+import static java.util.Optional.of;\n+import static oracle.code.onnx.OnnxOperators.*;\n+\n+public class MNISTModel {\n+\n+    static final int IMAGE_SIZE = 28;\n+    static final long[] IMAGE_SHAPE = new long[]{1, 1, IMAGE_SIZE, IMAGE_SIZE};\n+\n+    private static Tensor<Float> initialize(String resource, long... shape) {\n+        try (var in = MNISTModel.class.getResourceAsStream(resource)) {\n+            return Tensor.ofShape(shape, MemorySegment.ofArray(in.readAllBytes()).toArray(ValueLayout.JAVA_FLOAT_UNALIGNED));\n+        } catch (IOException e) {\n+            throw new UncheckedIOException(e);\n+        }\n+    }\n+\n+    static final Tensor<Float> conv1Weights;\n+    static final Tensor<Float> conv1Biases;\n+    static final Tensor<Float> conv2Weights;\n+    static final Tensor<Float> conv2Biases;\n+    static final Tensor<Float> fc1Weights;\n+    static final Tensor<Float> fc1Biases;\n+    static final Tensor<Float> fc2Weights;\n+    static final Tensor<Float> fc2Biases;\n+    static final Tensor<Float> fc3Weights;\n+    static final Tensor<Float> fc3Biases;\n+\n+    static {\n+        conv1Weights = initialize(\"conv1-weight-float-le\", 6, 1, 5, 5);\n+        conv1Biases = initialize(\"conv1-bias-float-le\", 6);\n+        conv2Weights = initialize(\"conv2-weight-float-le\", 16, 6, 5, 5);\n+        conv2Biases = initialize(\"conv2-bias-float-le\", 16);\n+        fc1Weights = initialize(\"fc1-weight-float-le\", 120, 256);\n+        fc1Biases = initialize(\"fc1-bias-float-le\", 120);\n+        fc2Weights = initialize(\"fc2-weight-float-le\", 84, 120);\n+        fc2Biases = initialize(\"fc2-bias-float-le\", 84);\n+        fc3Weights = initialize(\"fc3-weight-float-le\", 10, 84);\n+        fc3Biases = initialize(\"fc3-bias-float-le\", 10);\n+    }\n+\n+    @CodeReflection\n+    public static Tensor<Float> cnn(Tensor<Float> inputImage) {\n+        \/\/ Scaling to 0-1\n+        var scaledInput = Div(inputImage, Constant(255f));\n+\n+        \/\/ First conv layer\n+        var conv1 = Conv(scaledInput, conv1Weights, of(conv1Biases), of(new long[4]),\n+                of(new long[]{1,1}), empty(), of(new long[]{1, 1, 1, 1}),\n+                of(1L), of(new long[]{5,5}));\n+        var relu1 = Relu(conv1);\n+\n+        \/\/ First pooling layer\n+        var pool1 = MaxPool(relu1, of(new long[4]), of(new long[]{1,1}), empty(),\n+                of(0L), empty(), of(new long[]{2, 2}), new long[]{2, 2});\n+\n+        \/\/ Second conv layer\n+        var conv2 = Conv(pool1.Y(), conv2Weights, of(conv2Biases), of(new long[4]),\n+                of(new long[]{1,1}), empty(), of(new long[]{1, 1, 1, 1}),\n+                of(1L), of(new long[]{5,5}));\n+        var relu2 = Relu(conv2);\n+\n+        \/\/ Second pooling layer\n+        var pool2 = MaxPool(relu2, of(new long[4]), of(new long[]{1,1}), empty(),\n+                of(0L), empty(), of(new long[]{2, 2}), new long[]{2, 2});\n+\n+        \/\/ Flatten inputs\n+        var flatten = Flatten(pool2.Y(), of(1L));\n+\n+        \/\/ First fully connected layer\n+        var fc1 = Gemm(flatten, fc1Weights, of(fc1Biases), of(1f), of(1L), of(1f), empty());\n+        var relu3 = Relu(fc1);\n+\n+        \/\/ Second fully connected layer\n+        var fc2 = Gemm(relu3, fc2Weights, of(fc2Biases), of(1f), of(1L), of(1f), empty());\n+        var relu4 = Relu(fc2);\n+\n+        \/\/ Softmax layer\n+        var fc3 = Gemm(relu4, fc3Weights, of(fc3Biases), of(1f), of(1L), of(1f), empty());\n+        var prediction = Softmax(fc3, of(1L));\n+\n+        return prediction;\n+    }\n+\n+    public static float[] classify(float[] imageData) {\n+        try (Arena arena = Arena.ofConfined()) {\n+            var imageTensor = Tensor.ofShape(arena, IMAGE_SHAPE, imageData);\n+\n+            var predictionTensor = OnnxRuntime.execute(arena, MethodHandles.lookup(),\n+                    () -> cnn(imageTensor));\n+\n+            return predictionTensor.data().toArray(ValueLayout.JAVA_FLOAT);\n+        }\n+    }\n+}\n","filename":"cr-examples\/onnx\/src\/test\/java\/oracle\/code\/onnx\/mnist\/MNISTModel.java","additions":130,"deletions":0,"binary":false,"changes":130,"status":"added"}]}