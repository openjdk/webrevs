{"files":[{"patch":"@@ -120,0 +120,7 @@\n+                    return Float.valueOf(s);\n+                } else if (value instanceof Number n) {\n+                    return n.floatValue();\n+                }\n+            } else if (t.equals(Float16.FLOAT_16_TYPE)) {\n+                \/\/ represent as a float for now\n+                if (value instanceof String s) {\n","filename":"cr-examples\/triton\/src\/main\/java\/oracle\/code\/triton\/ArithMathOps.java","additions":7,"deletions":0,"binary":false,"changes":7,"status":"modified"},{"patch":"@@ -0,0 +1,559 @@\n+\/*\n+ * Copyright (c) 2024, Oracle and\/or its affiliates. All rights reserved.\n+ * DO NOT ALTER OR REMOVE COPYRIGHT NOTICES OR THIS FILE HEADER.\n+ *\n+ * This code is free software; you can redistribute it and\/or modify it\n+ * under the terms of the GNU General Public License version 2 only, as\n+ * published by the Free Software Foundation.\n+ *\n+ * This code is distributed in the hope that it will be useful, but WITHOUT\n+ * ANY WARRANTY; without even the implied warranty of MERCHANTABILITY or\n+ * FITNESS FOR A PARTICULAR PURPOSE.  See the GNU General Public License\n+ * version 2 for more details (a copy is included in the LICENSE file that\n+ * accompanied this code).\n+ *\n+ * You should have received a copy of the GNU General Public License version\n+ * 2 along with this work; if not, write to the Free Software Foundation,\n+ * Inc., 51 Franklin St, Fifth Floor, Boston, MA 02110-1301 USA.\n+ *\n+ * Please contact Oracle, 500 Oracle Parkway, Redwood Shores, CA 94065 USA\n+ * or visit www.oracle.com if you need additional information or have any\n+ * questions.\n+ *\/\n+\n+package oracle.code.triton;\n+\n+import org.junit.jupiter.api.Test;\n+import org.junit.jupiter.api.extension.ExtendWith;\n+\n+import java.lang.reflect.code.TypeElement;\n+import java.lang.reflect.code.type.JavaType;\n+import java.lang.runtime.CodeReflection;\n+import java.util.List;\n+\n+import static oracle.code.triton.Triton.*;\n+import static oracle.code.triton.Triton.CompareKind.*;\n+\n+@ExtendWith(TritonTestExtension.class)\n+public class TestMatrixFp16 {\n+\n+    @TritonCodeModel(\"\"\"\n+            module ()void -> {\n+                tt.func @\"cdiv_int_32_int\" (%0 : int)int -> {\n+                    %1 : int = arith.constant @\"32\";\n+                    %2 : int = arith.addi %0 %1;\n+                    %3 : int = arith.constant @\"1\";\n+                    %4 : int = arith.subi %2 %3;\n+                    %5 : int = arith.divsi %4 %1;\n+                    tt.return %5;\n+                };\n+                tt.func @\"cdiv_int_64_int\" (%6 : int)int -> {\n+                    %7 : int = arith.constant @\"64\";\n+                    %8 : int = arith.addi %6 %7;\n+                    %9 : int = arith.constant @\"1\";\n+                    %10 : int = arith.subi %8 %9;\n+                    %11 : int = arith.divsi %10 %7;\n+                    tt.return %11;\n+                };\n+                tt.func @sym_name=\"matmul_kernel_fp16_ptr<oracle.code.triton.Float16>_ptr<oracle.code.triton.Float16>_ptr<oracle.code.triton.Float16>_int_int_int_int_1_int_1_int_1_32_64_32_8_false_void\" (%12 : ptr<oracle.code.triton.Float16>, %13 : ptr<oracle.code.triton.Float16>, %14 : ptr<oracle.code.triton.Float16>, %15 : int, %16 : int, %17 : int, %18 : int, %19 : int, %20 : int)void -> {\n+                    %21 : int = arith.constant @value=\"1\";\n+                    %22 : int = arith.constant @value=\"1\";\n+                    %23 : int = arith.constant @value=\"1\";\n+                    %24 : int = arith.constant @value=\"32\";\n+                    %25 : int = arith.constant @value=\"64\";\n+                    %26 : int = arith.constant @value=\"32\";\n+                    %27 : int = arith.constant @value=\"8\";\n+                    %28 : int = tt.get_program_id @axis=\"0\";\n+                    %29 : int = tt.call %15 @callee=\"cdiv_int_32_int\";\n+                    %30 : int = tt.call %16 @callee=\"cdiv_int_64_int\";\n+                    %31 : int = arith.muli %27 %30;\n+                    %32 : int = arith.divsi %28 %31;\n+                    %33 : int = arith.muli %32 %27;\n+                    %34 : int = arith.subi %29 %33;\n+                    %35 : int = arith.minsi %34 %27;\n+                    %36 : int = arith.remsi %28 %35;\n+                    %37 : int = arith.addi %33 %36;\n+                    %38 : int = arith.remsi %28 %31;\n+                    %39 : int = arith.divsi %38 %35;\n+                    %40 : tensor<x32, int> = tt.make_range @start=\"0\" @end=\"32\";\n+                    %41 : int = arith.muli %37 %24;\n+                    %42 : tensor<x32, int> = tt.splat %41;\n+                    %43 : tensor<x32, int> = arith.addi %42 %40;\n+                    %44 : tensor<x32, int> = tt.splat %15;\n+                    %45 : tensor<x32, int> = arith.remsi %43 %44;\n+                    %46 : tensor<x64, int> = tt.make_range @start=\"0\" @end=\"64\";\n+                    %47 : int = arith.muli %39 %25;\n+                    %48 : tensor<x64, int> = tt.splat %47;\n+                    %49 : tensor<x64, int> = arith.addi %48 %46;\n+                    %50 : tensor<x64, int> = tt.splat %16;\n+                    %51 : tensor<x64, int> = arith.remsi %49 %50;\n+                    %52 : tensor<x32, int> = tt.make_range @start=\"0\" @end=\"32\";\n+                    %53 : tensor<x32, x1, int> = tt.expand_dims %45 @axis=\"1\";\n+                    %54 : tensor<x32, x1, int> = tt.splat %18;\n+                    %55 : tensor<x32, x1, int> = arith.muli %53 %54;\n+                    %56 : tensor<x1, x32, int> = tt.expand_dims %52 @axis=\"0\";\n+                    %57 : tensor<x1, x32, int> = tt.splat %21;\n+                    %58 : tensor<x1, x32, int> = arith.muli %56 %57;\n+                    %59 : tensor<x32, x32, int> = tt.broadcast %55;\n+                    %60 : tensor<x32, x32, int> = tt.broadcast %58;\n+                    %61 : tensor<x32, x32, int> = arith.addi %59 %60;\n+                    %62 : tensor<x32, x32, ptr<oracle.code.triton.Float16>> = tt.splat %12;\n+                    %63 : tensor<x32, x32, ptr<oracle.code.triton.Float16>> = tt.addptr %62 %61;\n+                    %64 : tensor<x32, x1, int> = tt.expand_dims %52 @axis=\"1\";\n+                    %65 : tensor<x32, x1, int> = tt.splat %19;\n+                    %66 : tensor<x32, x1, int> = arith.muli %64 %65;\n+                    %67 : tensor<x1, x64, int> = tt.expand_dims %51 @axis=\"0\";\n+                    %68 : tensor<x1, x64, int> = tt.splat %22;\n+                    %69 : tensor<x1, x64, int> = arith.muli %67 %68;\n+                    %70 : tensor<x32, x64, int> = tt.broadcast %66;\n+                    %71 : tensor<x32, x64, int> = tt.broadcast %69;\n+                    %72 : tensor<x32, x64, int> = arith.addi %70 %71;\n+                    %73 : tensor<x32, x64, ptr<oracle.code.triton.Float16>> = tt.splat %13;\n+                    %74 : tensor<x32, x64, ptr<oracle.code.triton.Float16>> = tt.addptr %73 %72;\n+                    %75 : tensor<x32, x64, float> = arith.constant @value=\"0.0\";\n+                    %76 : int = arith.constant @value=\"0\";\n+                    %77 : int = tt.call %17 @callee=\"cdiv_int_32_int\";\n+                    %78 : int = arith.constant @value=\"1\";\n+                    %79 : Tuple<tensor<x32, x64, float>, tensor<x32, x32, ptr<oracle.code.triton.Float16>>, tensor<x32, x64, ptr<oracle.code.triton.Float16>>> = scf.for %76 %77 %78 %75 %63 %74 (%80 : int, %81 : tensor<x32, x64, float>, %82 : tensor<x32, x32, ptr<oracle.code.triton.Float16>>, %83 : tensor<x32, x64, ptr<oracle.code.triton.Float16>>)Tuple<tensor<x32, x64, float>, tensor<x32, x32, ptr<oracle.code.triton.Float16>>, tensor<x32, x64, ptr<oracle.code.triton.Float16>>> -> {\n+                        %84 : tensor<x1, x32, int> = tt.expand_dims %52 @axis=\"0\";\n+                        %85 : int = arith.muli %80 %26;\n+                        %86 : int = arith.subi %17 %85;\n+                        %87 : tensor<x1, x32, int> = tt.splat %86;\n+                        %88 : tensor<x1, x32, boolean> = arith.cmpi %84 %87 @predicate=\"slt\";\n+                        %89 : tensor<x32, x32, boolean> = tt.broadcast %88;\n+                        %90 : tensor<x32, x32, oracle.code.triton.Float16> = arith.constant @value=\"0.0\";\n+                        %91 : tensor<x32, x32, oracle.code.triton.Float16> = tt.load %82 %89 %90;\n+                        %92 : tensor<x32, x1, int> = tt.expand_dims %52 @axis=\"1\";\n+                        %93 : int = arith.muli %80 %26;\n+                        %94 : int = arith.subi %17 %93;\n+                        %95 : tensor<x32, x1, int> = tt.splat %94;\n+                        %96 : tensor<x32, x1, boolean> = arith.cmpi %92 %95 @predicate=\"slt\";\n+                        %97 : tensor<x32, x64, boolean> = tt.broadcast %96;\n+                        %98 : tensor<x32, x64, oracle.code.triton.Float16> = arith.constant @value=\"0.0\";\n+                        %99 : tensor<x32, x64, oracle.code.triton.Float16> = tt.load %83 %97 %98;\n+                        %100 : tensor<x32, x64, float> = arith.constant @value=\"0.0\";\n+                        %101 : tensor<x32, x64, float> = tt.dot %91 %99 %100;\n+                        %102 : tensor<x32, x64, float> = arith.addf %81 %101;\n+                        %103 : int = arith.muli %26 %21;\n+                        %104 : tensor<x32, x32, int> = tt.splat %103;\n+                        %105 : tensor<x32, x32, ptr<oracle.code.triton.Float16>> = tt.addptr %82 %104;\n+                        %106 : int = arith.muli %26 %19;\n+                        %107 : tensor<x32, x64, int> = tt.splat %106;\n+                        %108 : tensor<x32, x64, ptr<oracle.code.triton.Float16>> = tt.addptr %83 %107;\n+                        scf.yield %102 %105 %108;\n+                    };\n+                    %109 : tensor<x32, x64, float> = tuple.load %79 @\"0\";\n+                    %110 : tensor<x32, x32, ptr<oracle.code.triton.Float16>> = tuple.load %79 @\"1\";\n+                    %111 : tensor<x32, x64, ptr<oracle.code.triton.Float16>> = tuple.load %79 @\"2\";\n+                    %112 : tensor<x32, x64, oracle.code.triton.Float16> = arith.truncf %109;\n+                    %113 : int = arith.muli %37 %24;\n+                    %114 : tensor<x32, int> = tt.splat %113;\n+                    %115 : tensor<x32, int> = arith.addi %114 %40;\n+                    %116 : int = arith.muli %39 %25;\n+                    %117 : tensor<x64, int> = tt.splat %116;\n+                    %118 : tensor<x64, int> = arith.addi %117 %46;\n+                    %119 : tensor<x32, x1, int> = tt.expand_dims %115 @axis=\"1\";\n+                    %120 : tensor<x32, x1, int> = tt.splat %20;\n+                    %121 : tensor<x32, x1, int> = arith.muli %120 %119;\n+                    %122 : tensor<x1, x64, int> = tt.expand_dims %118 @axis=\"0\";\n+                    %123 : tensor<x1, x64, int> = tt.splat %23;\n+                    %124 : tensor<x1, x64, int> = arith.muli %123 %122;\n+                    %125 : tensor<x32, x64, int> = tt.broadcast %121;\n+                    %126 : tensor<x32, x64, int> = tt.broadcast %124;\n+                    %127 : tensor<x32, x64, int> = arith.addi %125 %126;\n+                    %128 : tensor<x32, x64, ptr<oracle.code.triton.Float16>> = tt.splat %14;\n+                    %129 : tensor<x32, x64, ptr<oracle.code.triton.Float16>> = tt.addptr %128 %127;\n+                    %130 : tensor<x32, x1, int> = tt.expand_dims %115 @axis=\"1\";\n+                    %131 : tensor<x32, x1, int> = tt.splat %15;\n+                    %132 : tensor<x32, x1, boolean> = arith.cmpi %130 %131 @predicate=\"slt\";\n+                    %133 : tensor<x1, x64, int> = tt.expand_dims %118 @axis=\"0\";\n+                    %134 : tensor<x1, x64, int> = tt.splat %16;\n+                    %135 : tensor<x1, x64, boolean> = arith.cmpi %133 %134 @predicate=\"slt\";\n+                    %136 : tensor<x32, x64, boolean> = tt.broadcast %132;\n+                    %137 : tensor<x32, x64, boolean> = tt.broadcast %135;\n+                    %138 : tensor<x32, x64, boolean> = arith.andi %136 %137;\n+                    tt.store %129 %112 %138;\n+                    tt.return;\n+                };\n+                unreachable;\n+            };\n+            \"\"\")\n+    @CodeReflection\n+    static void matmul_kernel_fp16(\n+            \/\/ Pointers to matrices\n+            Ptr a_ptr, Ptr b_ptr, Ptr c_ptr,\n+            \/\/ Matrix dimensions\n+            int M, int N, int K,\n+            \/\/ The stride variables represent how much to increase the ptr by when moving by 1\n+            \/\/ element in a particular dimension. E.g. `stride_am` is how much to increase `a_ptr`\n+            \/\/ by to get the element one row down (A has M rows).\n+            int stride_am, @Constant int stride_ak,\n+            int stride_bk, @Constant int stride_bn,\n+            int stride_cm, @Constant int stride_cn,\n+            \/\/ Meta-parameters\n+            @Constant int BLOCK_SIZE_M, @Constant int BLOCK_SIZE_N, @Constant int BLOCK_SIZE_K,\n+            @Constant int GROUP_SIZE_M,\n+            @Constant boolean ACTIVATION) {\n+\n+        \/\/ \"\"\"Kernel for computing the matmul C = A x B.\n+        \/\/ A has shape (M, K), B has shape (K, N) and C has shape (M, N)\n+        \/\/ \"\"\"\n+        \/\/ -----------------------------------------------------------\n+        \/\/ Map program ids `pid` to the block of C it should compute.\n+        \/\/ This is done in a grouped ordering to promote L2 data reuse.\n+        \/\/ See above `L2 Cache Optimizations` section for details.\n+        var pid = programId(0);\n+        var num_pid_m = cdiv(M, BLOCK_SIZE_M);\n+        var num_pid_n = cdiv(N, BLOCK_SIZE_N);\n+        var num_pid_in_group = GROUP_SIZE_M * num_pid_n;\n+        var group_id = pid \/ num_pid_in_group;\n+        var first_pid_m = group_id * GROUP_SIZE_M;\n+        var group_size_m = Math.min(num_pid_m - first_pid_m, GROUP_SIZE_M);\n+        var pid_m = first_pid_m + (pid % group_size_m);\n+        var pid_n = (pid % num_pid_in_group) \/ group_size_m;\n+\n+        \/\/ ----------------------------------------------------------\n+        \/\/ Create pointers for the first blocks of A and B.\n+        \/\/ We will advance this pointer as we move in the K direction\n+        \/\/ and accumulate\n+        \/\/ `a_ptrs` is a block of [BLOCK_SIZE_M, BLOCK_SIZE_K] pointers\n+        \/\/ `b_ptrs` is a block of [BLOCK_SIZE_K, BLOCK_SIZE_N] pointers\n+        \/\/ See above `Pointer Arithmetics` section for details\n+        var offs_m = arange(0, BLOCK_SIZE_M);\n+        var offs_am = mod(add(pid_m * BLOCK_SIZE_M, offs_m), M);\n+        var offs_n = arange(0, BLOCK_SIZE_N);\n+        var offs_bn = mod(add(pid_n * BLOCK_SIZE_N, offs_n), N);\n+        var offs_k = arange(0, BLOCK_SIZE_K);\n+        var a_ptrs = add(a_ptr, add(\n+                mul(expand(offs_am, 1), stride_am),\n+                mul(expand(offs_k, 0), stride_ak)));\n+        var b_ptrs = add(b_ptr, add(\n+                        mul(expand(offs_k, 1), stride_bk),\n+                        mul(expand(offs_bn, 0), stride_bn)));\n+\n+        \/\/ -----------------------------------------------------------\n+        \/\/ Iterate to compute a block of the C matrix.\n+        \/\/ We accumulate into a `[BLOCK_SIZE_M, BLOCK_SIZE_N]` block\n+        \/\/ of fp32 values for higher accuracy.\n+        \/\/ `accumulator` will be converted back to fp16 after the loop.\n+        var accumulator = zeros(float.class, BLOCK_SIZE_M, BLOCK_SIZE_N);\n+        for (int k = 0; k < cdiv(K, BLOCK_SIZE_K); k++) {\n+            \/\/ Load the next block of A and B, generate a mask by checking the K dimension.\n+            \/\/ If it is out of bounds, set it to 0.\n+            var a = load(a_ptrs,\n+                    compare(expand(offs_k, 0), K - k * BLOCK_SIZE_K, LessThan), 0f);\n+            var b = load(b_ptrs,\n+                    compare(expand(offs_k, 1), K - k * BLOCK_SIZE_K, LessThan), 0f);\n+            \/\/ We accumulate along the K dimension.\n+            accumulator = add(accumulator, dot(a, b));\n+            \/\/ Advance the ptrs to the next K block.\n+            a_ptrs = add(a_ptrs, BLOCK_SIZE_K * stride_ak);\n+            b_ptrs = add(b_ptrs, BLOCK_SIZE_K * stride_bk);\n+        }\n+\n+        \/\/ You can fuse arbitrary activation functions here\n+        \/\/ while the accumulator is still in FP32!\n+\/\/        if (ACTIVATION) {\n+\/\/            \/\/ ...\n+\/\/        }\n+        var c = Triton.conv(Float16.class, accumulator);\n+\n+        \/\/ -----------------------------------------------------------\n+        \/\/ Write back the block of the output matrix C with masks.\n+        var offs_cm = add(pid_m * BLOCK_SIZE_M, offs_m);\n+        var offs_cn = add(pid_n * BLOCK_SIZE_N, offs_n);\n+        var c_ptrs = add(c_ptr, add(\n+                        mul(stride_cm, expand(offs_cm, 1)),\n+                        mul(stride_cn, expand(offs_cn, 0))));\n+        var c_mask = and(\n+                compare(expand(offs_cm, 1), M, LessThan),\n+                compare(expand(offs_cn, 0), N, LessThan));\n+        store(c_ptrs, c, c_mask);\n+    }\n+\n+    @TritonTestExtension.Kernel(\"matmul_kernel_fp16\")\n+    @Test\n+    public void test(TritonTestExtension.TritonTestData t) {\n+        List<TypeElement> argTypes = List.of(\n+                new PtrType(Float16.FLOAT_16_TYPE),\n+                new PtrType(Float16.FLOAT_16_TYPE),\n+                new PtrType(Float16.FLOAT_16_TYPE),\n+                JavaType.INT, JavaType.INT, JavaType.INT,\n+                JavaType.INT, new ConstantType(JavaType.INT, 1),\n+                JavaType.INT, new ConstantType(JavaType.INT, 1),\n+                JavaType.INT, new ConstantType(JavaType.INT, 1),\n+                new ConstantType(JavaType.INT, 32), new ConstantType(JavaType.INT, 64), new ConstantType(JavaType.INT, 32),\n+                new ConstantType(JavaType.INT, 8),\n+                new ConstantType(JavaType.INT, false));\n+\n+        t.test(argTypes);\n+    }\n+\n+}\n+\n+\/*\n+@triton.jit\n+def matmul_kernel(\n+        # Pointers to matrices\n+        a_ptr, b_ptr, c_ptr,\n+        # Matrix dimensions\n+        M, N, K,\n+        # The stride variables represent how much to increase the ptr by when moving by 1\n+        # element in a particular dimension. E.g. `stride_am` is how much to increase `a_ptr`\n+        # by to get the element one row down (A has M rows).\n+        stride_am, stride_ak,  #\n+        stride_bk, stride_bn,  #\n+        stride_cm, stride_cn,\n+        # Meta-parameters\n+        BLOCK_SIZE_M: tl.constexpr, BLOCK_SIZE_N: tl.constexpr, BLOCK_SIZE_K: tl.constexpr,  #\n+        GROUP_SIZE_M: tl.constexpr,  #\n+        ACTIVATION: tl.constexpr  #\n+):\n+    \"\"\"Kernel for computing the matmul C = A x B.\n+    A has shape (M, K), B has shape (K, N) and C has shape (M, N)\n+    \"\"\"\n+    # -----------------------------------------------------------\n+    # Map program ids `pid` to the block of C it should compute.\n+    # This is done in a grouped ordering to promote L2 data reuse.\n+    # See above `L2 Cache Optimizations` section for details.\n+    pid = tl.program_id(axis=0)\n+    num_pid_m = tl.cdiv(M, BLOCK_SIZE_M)\n+    num_pid_n = tl.cdiv(N, BLOCK_SIZE_N)\n+    num_pid_in_group = GROUP_SIZE_M * num_pid_n\n+    group_id = pid \/\/ num_pid_in_group\n+    first_pid_m = group_id * GROUP_SIZE_M\n+    group_size_m = min(num_pid_m - first_pid_m, GROUP_SIZE_M)\n+    pid_m = first_pid_m + (pid % group_size_m)\n+    pid_n = (pid % num_pid_in_group) \/\/ group_size_m\n+\n+    # ----------------------------------------------------------\n+    # Create pointers for the first blocks of A and B.\n+    # We will advance this pointer as we move in the K direction\n+    # and accumulate\n+    # `a_ptrs` is a block of [BLOCK_SIZE_M, BLOCK_SIZE_K] pointers\n+    # `b_ptrs` is a block of [BLOCK_SIZE_K, BLOCK_SIZE_N] pointers\n+    # See above `Pointer Arithmetics` section for details\n+    offs_am = (pid_m * BLOCK_SIZE_M + tl.arange(0, BLOCK_SIZE_M)) % M\n+    offs_bn = (pid_n * BLOCK_SIZE_N + tl.arange(0, BLOCK_SIZE_N)) % N\n+    offs_k = tl.arange(0, BLOCK_SIZE_K)\n+    a_ptrs = a_ptr + (offs_am[:, None] * stride_am + offs_k[None, :] * stride_ak)\n+    b_ptrs = b_ptr + (offs_k[:, None] * stride_bk + offs_bn[None, :] * stride_bn)\n+\n+    # -----------------------------------------------------------\n+    # Iterate to compute a block of the C matrix.\n+    # We accumulate into a `[BLOCK_SIZE_M, BLOCK_SIZE_N]` block\n+    # of fp32 values for higher accuracy.\n+    # `accumulator` will be converted back to fp16 after the loop.\n+    accumulator = tl.zeros((BLOCK_SIZE_M, BLOCK_SIZE_N), dtype=tl.float32)\n+    for k in range(0, tl.cdiv(K, BLOCK_SIZE_K)):\n+        # Load the next block of A and B, generate a mask by checking the K dimension.\n+        # If it is out of bounds, set it to 0.\n+        a = tl.load(a_ptrs, mask=offs_k[None, :] < K - k * BLOCK_SIZE_K, other=0.0)\n+        b = tl.load(b_ptrs, mask=offs_k[:, None] < K - k * BLOCK_SIZE_K, other=0.0)\n+        # We accumulate along the K dimension.\n+        accumulator += tl.dot(a, b)\n+        # Advance the ptrs to the next K block.\n+        a_ptrs += BLOCK_SIZE_K * stride_ak\n+        b_ptrs += BLOCK_SIZE_K * stride_bk\n+    # You can fuse arbitrary activation functions here\n+    # while the accumulator is still in FP32!\n+    if ACTIVATION == \"leaky_relu\":\n+        accumulator = leaky_relu(accumulator)\n+    c = accumulator.to(tl.float16)\n+\n+    # -----------------------------------------------------------\n+    # Write back the block of the output matrix C with masks.\n+    offs_cm = pid_m * BLOCK_SIZE_M + tl.arange(0, BLOCK_SIZE_M)\n+    offs_cn = pid_n * BLOCK_SIZE_N + tl.arange(0, BLOCK_SIZE_N)\n+    c_ptrs = c_ptr + stride_cm * offs_cm[:, None] + stride_cn * offs_cn[None, :]\n+    c_mask = (offs_cm[:, None] < M) & (offs_cn[None, :] < N)\n+    tl.store(c_ptrs, c, mask=c_mask)\n+\n+\n+# We can fuse `leaky_relu` by providing it as an `ACTIVATION` meta-parameter in `_matmul`.\n+@triton.jit\n+def leaky_relu(x):\n+    x = x + 1\n+    return tl.where(x >= 0, x, 0.01 * x)\n+*\/\n+\n+\/*\n+\n+ triton\/python\/triton\/tools\/compile.py \\\n+    --kernel-name matmul_kernel \\\n+    --signature \"*fp16,*fp16,*fp16,i32,i32,i32,i32,i32,i32,i32,i32,i32,32,64,32,8,0\" \\\n+    --grid=1024,1024,1024 \\\n+    03-matrix-multiplication.py\n+\n+BLOCK_SIZE_M = 32\n+BLOCK_SIZE_N = 64\n+BLOCK_SIZE_K = 32\n+GROUP_SIZE_M = 8\n+ACTIVATION = 0\n+\n+module {\n+  tt.func public @matmul_kernel_01234567891011(\n+            %arg0: !tt.ptr<f16, 1>, %arg1: !tt.ptr<f16, 1>, %arg2: !tt.ptr<f16, 1> ,\n+            %arg3: i32, %arg4: i32, %arg5: i32 ,\n+            %arg6: i32, %arg7: i32, %arg8: i32 ,\n+            %%arg9: i32, %arg10: i32, %arg11: i32 ) attributes {noinline = false} {\n+    %0 = tt.get_program_id x : i32\n+    %1 = tt.call @cdiv__i32__1cconstexpr_32_(%arg3) : (i32) -> i32\n+    %2 = tt.call @cdiv__i32__1cconstexpr_64_(%arg4) : (i32) -> i32\n+    %c8_i32 = arith.constant 8 : i32\n+    %3 = arith.muli %2, %c8_i32 : i32\n+    %4 = arith.divsi %0, %3 : i32\n+    %c8_i32_0 = arith.constant 8 : i32\n+    %5 = arith.muli %4, %c8_i32_0 : i32\n+    %6 = arith.subi %1, %5 : i32\n+    %7 = tt.call @minimum__i32__1cconstexpr_8_(%6) : (i32) -> i32\n+    %8 = arith.remsi %0, %7 : i32\n+    %9 = arith.addi %5, %8 : i32\n+    %10 = arith.remsi %0, %3 : i32\n+    %11 = arith.divsi %10, %7 : i32\n+    %c32_i32 = arith.constant 32 : i32\n+    %12 = arith.muli %9, %c32_i32 : i32\n+    %13 = tt.make_range {end = 32 : i32, start = 0 : i32} : tensor<32xi32>\n+    %14 = tt.splat %12 : (i32) -> tensor<32xi32>\n+    %15 = arith.addi %14, %13 : tensor<32xi32>\n+    %16 = tt.splat %arg3 : (i32) -> tensor<32xi32>\n+    %17 = arith.remsi %15, %16 : tensor<32xi32>\n+    %c64_i32 = arith.constant 64 : i32\n+    %18 = arith.muli %11, %c64_i32 : i32\n+    %19 = tt.make_range {end = 64 : i32, start = 0 : i32} : tensor<64xi32>\n+    %20 = tt.splat %18 : (i32) -> tensor<64xi32>\n+    %21 = arith.addi %20, %19 : tensor<64xi32>\n+    %22 = tt.splat %arg4 : (i32) -> tensor<64xi32>\n+    %23 = arith.remsi %21, %22 : tensor<64xi32>\n+    %24 = tt.make_range {end = 32 : i32, start = 0 : i32} : tensor<32xi32>\n+    %25 = tt.expand_dims %17 {axis = 1 : i32} : (tensor<32xi32>) -> tensor<32x1xi32>\n+    %26 = tt.splat %arg6 : (i32) -> tensor<32x1xi32>\n+    %27 = arith.muli %25, %26 : tensor<32x1xi32>\n+    %28 = tt.expand_dims %24 {axis = 0 : i32} : (tensor<32xi32>) -> tensor<1x32xi32>\n+    %29 = tt.splat %arg7 : (i32) -> tensor<1x32xi32>\n+    %30 = arith.muli %28, %29 : tensor<1x32xi32>\n+    %31 = tt.broadcast %27 : (tensor<32x1xi32>) -> tensor<32x32xi32>\n+    %32 = tt.broadcast %30 : (tensor<1x32xi32>) -> tensor<32x32xi32>\n+    %33 = arith.addi %31, %32 : tensor<32x32xi32>\n+    %34 = tt.splat %arg0 : (!tt.ptr<f16, 1>) -> tensor<32x32x!tt.ptr<f16, 1>>\n+    %35 = tt.addptr %34, %33 : tensor<32x32x!tt.ptr<f16, 1>>, tensor<32x32xi32>\n+    %36 = tt.expand_dims %24 {axis = 1 : i32} : (tensor<32xi32>) -> tensor<32x1xi32>\n+    %37 = tt.splat %arg8 : (i32) -> tensor<32x1xi32>\n+    %38 = arith.muli %36, %37 : tensor<32x1xi32>\n+    %39 = tt.expand_dims %23 {axis = 0 : i32} : (tensor<64xi32>) -> tensor<1x64xi32>\n+    %40 = tt.splat %arg9 : (i32) -> tensor<1x64xi32>\n+    %41 = arith.muli %39, %40 : tensor<1x64xi32>\n+    %42 = tt.broadcast %38 : (tensor<32x1xi32>) -> tensor<32x64xi32>\n+    %43 = tt.broadcast %41 : (tensor<1x64xi32>) -> tensor<32x64xi32>\n+    %44 = arith.addi %42, %43 : tensor<32x64xi32>\n+    %45 = tt.splat %arg1 : (!tt.ptr<f16, 1>) -> tensor<32x64x!tt.ptr<f16, 1>>\n+    %46 = tt.addptr %45, %44 : tensor<32x64x!tt.ptr<f16, 1>>, tensor<32x64xi32>\n+    %47 = tt.call @\"zeros____0cconstexpr_(constexpr_32_, constexpr_64_)__1cconstexpr_fp32_\"() : () -> tensor<32x64xf32>\n+    %48 = tt.call @cdiv__i32__1cconstexpr_32_(%arg5) : (i32) -> i32\n+    %c0_i32 = arith.constant 0 : i32\n+    %c1_i32 = arith.constant 1 : i32\n+    %49 = arith.bitcast %c0_i32 : i32 to i32\n+    %50 = arith.bitcast %48 : i32 to i32\n+    %51 = arith.bitcast %c1_i32 : i32 to i32\n+    %52 = llvm.mlir.undef : i32\n+    %53:3 = scf.for %arg12 = %49 to %50 step %51 iter_args(%arg13 = %47, %arg14 = %35, %arg15 = %46) -> (tensor<32x64xf32>, tensor<32x32x!tt.ptr<f16, 1>>, tensor<32x64x!tt.ptr<f16, 1>>)  : i32 {\n+      %83 = tt.expand_dims %24 {axis = 0 : i32} : (tensor<32xi32>) -> tensor<1x32xi32>\n+      %c32_i32_3 = arith.constant 32 : i32\n+      %84 = arith.muli %arg12, %c32_i32_3 : i32\n+      %85 = arith.subi %arg5, %84 : i32\n+      %86 = tt.splat %85 : (i32) -> tensor<1x32xi32>\n+      %87 = arith.cmpi slt, %83, %86 : tensor<1x32xi32>\n+      %cst = arith.constant 0.000000e+00 : f32\n+      %88 = tt.broadcast %87 : (tensor<1x32xi1>) -> tensor<32x32xi1>\n+      %cst_4 = arith.constant dense<0.000000e+00> : tensor<32x32xf32>\n+      %89 = arith.truncf %cst_4 : tensor<32x32xf32> to tensor<32x32xf16>\n+      %90 = tt.load %arg14, %88, %89 {cache = 1 : i32, evict = 1 : i32, isVolatile = false} : tensor<32x32xf16>\n+      %91 = tt.expand_dims %24 {axis = 1 : i32} : (tensor<32xi32>) -> tensor<32x1xi32>\n+      %c32_i32_5 = arith.constant 32 : i32\n+      %92 = arith.muli %arg12, %c32_i32_5 : i32\n+      %93 = arith.subi %arg5, %92 : i32\n+      %94 = tt.splat %93 : (i32) -> tensor<32x1xi32>\n+      %95 = arith.cmpi slt, %91, %94 : tensor<32x1xi32>\n+      %cst_6 = arith.constant 0.000000e+00 : f32\n+      %96 = tt.broadcast %95 : (tensor<32x1xi1>) -> tensor<32x64xi1>\n+      %cst_7 = arith.constant dense<0.000000e+00> : tensor<32x64xf32>\n+      %97 = arith.truncf %cst_7 : tensor<32x64xf32> to tensor<32x64xf16>\n+      %98 = tt.load %arg15, %96, %97 {cache = 1 : i32, evict = 1 : i32, isVolatile = false} : tensor<32x64xf16>\n+      %cst_8 = arith.constant 0.000000e+00 : f32\n+      %cst_9 = arith.constant dense<0.000000e+00> : tensor<32x64xf32>\n+      %99 = tt.dot %90, %98, %cst_9 {allowTF32 = true, maxNumImpreciseAcc = 0 : i32} : tensor<32x32xf16> * tensor<32x64xf16> -> tensor<32x64xf32>\n+      %100 = arith.addf %arg13, %99 : tensor<32x64xf32>\n+      %c32_i32_10 = arith.constant 32 : i32\n+      %101 = arith.muli %arg7, %c32_i32_10 : i32\n+      %102 = tt.splat %101 : (i32) -> tensor<32x32xi32>\n+      %103 = tt.addptr %arg14, %102 : tensor<32x32x!tt.ptr<f16, 1>>, tensor<32x32xi32>\n+      %c32_i32_11 = arith.constant 32 : i32\n+      %104 = arith.muli %arg8, %c32_i32_11 : i32\n+      %105 = tt.splat %104 : (i32) -> tensor<32x64xi32>\n+      %106 = tt.addptr %arg15, %105 : tensor<32x64x!tt.ptr<f16, 1>>, tensor<32x64xi32>\n+      scf.yield %100, %103, %106 : tensor<32x64xf32>, tensor<32x32x!tt.ptr<f16, 1>>, tensor<32x64x!tt.ptr<f16, 1>>\n+    }\n+    %54 = arith.truncf %53#0 : tensor<32x64xf32> to tensor<32x64xf16>\n+    %c32_i32_1 = arith.constant 32 : i32\n+    %55 = arith.muli %9, %c32_i32_1 : i32\n+    %56 = tt.make_range {end = 32 : i32, start = 0 : i32} : tensor<32xi32>\n+    %57 = tt.splat %55 : (i32) -> tensor<32xi32>\n+    %58 = arith.addi %57, %56 : tensor<32xi32>\n+    %c64_i32_2 = arith.constant 64 : i32\n+    %59 = arith.muli %11, %c64_i32_2 : i32\n+    %60 = tt.make_range {end = 64 : i32, start = 0 : i32} : tensor<64xi32>\n+    %61 = tt.splat %59 : (i32) -> tensor<64xi32>\n+    %62 = arith.addi %61, %60 : tensor<64xi32>\n+    %63 = tt.expand_dims %58 {axis = 1 : i32} : (tensor<32xi32>) -> tensor<32x1xi32>\n+    %64 = tt.splat %arg10 : (i32) -> tensor<32x1xi32>\n+    %65 = arith.muli %64, %63 : tensor<32x1xi32>\n+    %66 = tt.splat %arg2 : (!tt.ptr<f16, 1>) -> tensor<32x1x!tt.ptr<f16, 1>>\n+    %67 = tt.addptr %66, %65 : tensor<32x1x!tt.ptr<f16, 1>>, tensor<32x1xi32>\n+    %68 = tt.expand_dims %62 {axis = 0 : i32} : (tensor<64xi32>) -> tensor<1x64xi32>\n+    %69 = tt.splat %arg11 : (i32) -> tensor<1x64xi32>\n+    %70 = arith.muli %69, %68 : tensor<1x64xi32>\n+    %71 = tt.broadcast %67 : (tensor<32x1x!tt.ptr<f16, 1>>) -> tensor<32x64x!tt.ptr<f16, 1>>\n+    %72 = tt.broadcast %70 : (tensor<1x64xi32>) -> tensor<32x64xi32>\n+    %73 = tt.addptr %71, %72 : tensor<32x64x!tt.ptr<f16, 1>>, tensor<32x64xi32>\n+    %74 = tt.expand_dims %58 {axis = 1 : i32} : (tensor<32xi32>) -> tensor<32x1xi32>\n+    %75 = tt.splat %arg3 : (i32) -> tensor<32x1xi32>\n+    %76 = arith.cmpi slt, %74, %75 : tensor<32x1xi32>\n+    %77 = tt.expand_dims %62 {axis = 0 : i32} : (tensor<64xi32>) -> tensor<1x64xi32>\n+    %78 = tt.splat %arg4 : (i32) -> tensor<1x64xi32>\n+    %79 = arith.cmpi slt, %77, %78 : tensor<1x64xi32>\n+    %80 = tt.broadcast %76 : (tensor<32x1xi1>) -> tensor<32x64xi1>\n+    %81 = tt.broadcast %79 : (tensor<1x64xi1>) -> tensor<32x64xi1>\n+    %82 = arith.andi %80, %81 : tensor<32x64xi1>\n+    tt.store %73, %54, %82 {cache = 1 : i32, evict = 1 : i32} : tensor<32x64xf16>\n+    tt.return\n+  }\n+  tt.func private @cdiv__i32__1cconstexpr_32_(%arg0: i32 ) -> i32 attributes {noinline = false} {\n+    %c32_i32 = arith.constant 32 : i32\n+    %0 = arith.addi %arg0, %c32_i32 : i32\n+    %c1_i32 = arith.constant 1 : i32\n+    %1 = arith.subi %0, %c1_i32 : i32\n+    %c32_i32_0 = arith.constant 32 : i32\n+    %2 = arith.divsi %1, %c32_i32_0 : i32\n+    tt.return %2 : i32\n+  }\n+  tt.func private @cdiv__i32__1cconstexpr_64_(%arg0: i32 ) -> i32 attributes {noinline = false} {\n+    %c64_i32 = arith.constant 64 : i32\n+    %0 = arith.addi %arg0, %c64_i32 : i32\n+    %c1_i32 = arith.constant 1 : i32\n+    %1 = arith.subi %0, %c1_i32 : i32\n+    %c64_i32_0 = arith.constant 64 : i32\n+    %2 = arith.divsi %1, %c64_i32_0 : i32\n+    tt.return %2 : i32\n+  }\n+  tt.func private @minimum__i32__1cconstexpr_8_(%arg0: i32 ) -> i32 attributes {noinline = false} {\n+    %c8_i32 = arith.constant 8 : i32\n+    %0 = arith.minsi %arg0, %c8_i32 : i32\n+    tt.return %0 : i32\n+  }\n+  tt.func private @\"zeros____0cconstexpr_(constexpr_32_, constexpr_64_)__1cconstexpr_fp32_\"() -> tensor<32x64xf32> attributes {noinline = false} {\n+    %cst = arith.constant 0.000000e+00 : f32\n+    %cst_0 = arith.constant dense<0.000000e+00> : tensor<32x64xf32>\n+    tt.return %cst_0 : tensor<32x64xf32>\n+  }\n+}\n+ *\/\n","filename":"cr-examples\/triton\/src\/test\/java\/oracle\/code\/triton\/TestMatrixFp16.java","additions":559,"deletions":0,"binary":false,"changes":559,"status":"added"}]}