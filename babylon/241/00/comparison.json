{"files":[{"patch":"@@ -0,0 +1,50 @@\n+diff --git a\/python\/triton\/compiler\/compiler.py b\/python\/triton\/compiler\/compiler.py\n+index 3ec8ff32..bd59e1b6 100644\n+--- a\/python\/triton\/compiler\/compiler.py\n++++ b\/python\/triton\/compiler\/compiler.py\n+@@ -223,7 +223,7 @@ def filter_traceback(e: BaseException):\n+         e.__traceback__ = frames[0]\n+ \n+ \n+-def compile(src, target_mlir=None, target=None, options=None):\n++def compile(src, target=None, options=None):\n+     if target is None:\n+         target = driver.active.get_current_target()\n+     assert isinstance(target, GPUTarget), \"target must be of GPUTarget type\"\n+@@ -268,7 +268,7 @@ def compile(src, target_mlir=None, target=None, options=None):\n+     }\n+     # run compilation pipeline  and populate metadata\n+     stages = dict()\n+-    backend.add_stages(stages, options, target_mlir)\n++    backend.add_stages(stages, options)\n+     first_stage = list(stages.keys()).index(src.ext)\n+     # when the source is an IR file, don't apply the passes related to this stage. This makes it easier to write IR level tests.\n+     if ir_source:\n+diff --git a\/third_party\/intel\/backend\/compiler.py b\/third_party\/intel\/backend\/compiler.py\n+index 9b4261c7..8dad5654 100644\n+--- a\/third_party\/intel\/backend\/compiler.py\n++++ b\/third_party\/intel\/backend\/compiler.py\n+@@ -138,11 +138,7 @@ class XPUBackend(BaseBackend):\n+         intel.load_dialects(ctx)\n+ \n+     @staticmethod\n+-    def make_ttir(mod, metadata, opt, target_mlir):\n+-        if (target_mlir):\n+-            context = mod.context\n+-            mod = ir.parse_mlir_module(f\"{target_mlir}\", mod.context)\n+-            mod.context = context\n++    def make_ttir(mod, metadata, opt):\n+         pm = ir.pass_manager(mod.context)\n+         pm.enable_debug()\n+         passes.common.add_inliner(pm)\n+@@ -254,8 +250,8 @@ class XPUBackend(BaseBackend):\n+         metadata[\"name\"] = name\n+         return ret\n+ \n+-    def add_stages(self, stages, options, target_mlir):\n+-        stages[\"ttir\"] = lambda src, metadata: self.make_ttir(src, metadata, options, target_mlir)\n++    def add_stages(self, stages, options):\n++        stages[\"ttir\"] = lambda src, metadata: self.make_ttir(src, metadata, options)\n+         stages[\"ttgir\"] = lambda src, metadata: self.make_ttgir(src, metadata, options, self.properties)\n+         stages[\"llir\"] = lambda src, metadata: self.make_llir(src, metadata, options)\n+         stages[\"spv\"] = lambda src, metadata: self.make_spv(src, metadata)\n","filename":"cr-examples\/triton\/add-mlir-insertion.patch","additions":50,"deletions":0,"binary":false,"changes":50,"status":"added"},{"patch":"@@ -0,0 +1,871 @@\n+import java.io.File;\n+import java.io.FileInputStream;\n+import java.util.List;\n+import java.util.Map;\n+import java.util.HashMap;\n+import java.util.ArrayList;\n+import java.util.Deque;\n+import java.util.ArrayDeque;\n+import java.util.Set;\n+import java.util.HashSet;\n+import java.util.Iterator;\n+import java.util.Arrays;\n+import java.util.Queue;\n+import java.util.Optional;\n+import java.util.stream.Stream;\n+import java.util.function.Consumer;\n+import java.util.function.Function;\n+import java.util.function.Supplier;\n+import java.time.*;\n+import java.nio.file.Files;\n+import java.nio.file.Paths;\n+import java.io.IOException;\n+import java.io.BufferedReader;\n+import java.io.FileWriter;\n+import java.io.FileReader;\n+import java.lang.reflect.Method;\n+import java.lang.ref.Cleaner;\n+import java.lang.foreign.MemorySegment;\n+import java.lang.foreign.ValueLayout;\n+import java.lang.foreign.AddressLayout;\n+import java.lang.foreign.Arena;\n+import java.lang.foreign.MemorySegment.Scope;\n+import static java.lang.foreign.ValueLayout.*;\n+import jdk.incubator.vector.VectorSpecies;\n+import jdk.incubator.vector.FloatVector;\n+import static oneapi.levelzero.ze_api_h.*;\n+import oneapi.levelzero.ze_api_h;\n+import oneapi.levelzero.ze_context_desc_t;\n+import oneapi.levelzero.ze_kernel_desc_t;\n+import oneapi.levelzero.ze_command_queue_desc_t;\n+import oneapi.levelzero.ze_command_list_desc_t;\n+import oneapi.levelzero.ze_command_queue_group_properties_t;\n+import oneapi.levelzero.ze_event_pool_desc_t;\n+import oneapi.levelzero.ze_event_desc_t;\n+import oneapi.levelzero.ze_fence_desc_t;\n+import oneapi.levelzero.ze_module_desc_t;\n+import oneapi.levelzero.ze_group_count_t;\n+import oneapi.levelzero.ze_host_mem_alloc_desc_t;\n+import oneapi.levelzero.ze_device_mem_alloc_desc_t;\n+import oneapi.levelzero.ze_device_properties_t;\n+import oneapi.levelzero.ze_device_compute_properties_t;\n+import oneapi.levelzero.ze_driver_properties_t;\n+import oneapi.levelzero.ze_driver_extension_properties_t;\n+import org.json.JSONArray;\n+import org.json.JSONObject;\n+\n+import java.util.Random;\n+\n+public class LevelZero {\n+    public static final AddressLayout driver_handle_t = AddressLayout.ADDRESS;\n+    private final Arena arena;\n+    private final MemorySegment driverHandle;\n+    private final MemorySegment contextHandle;\n+    private final MemorySegment deviceHandle;\n+    private final MemorySegment queueHandle;\n+    private final MemorySegment eventPoolDescription;\n+    private final String homeDir = System.getProperty(\"user.home\");\n+    private final String cacheDir = homeDir + \"\/.triton\/cache\/\";\n+    private final String addKernelCache = \"7961f2e8b433c656051d8638d6a3bb65f43f6cb885525c05d611100dd905aa31\";\n+    private final String softmaxKernelCache = \"f0c32acd1173759227ef8e0e8d197c94493b90ebf8d1fc254399ffac6b527d6a\";\n+    private final String matmulKernelCache = \"07e17c2833c9c9efea8ccd782af1c3ee05dcac3efb2cb75f2f8a6eecffe381ef\";\n+    private final static VectorSpecies<Float> SPECIES = FloatVector.SPECIES_256;\n+    private Double timeElapsedForRun, timeElapsedForRerun;\n+\n+    static {\n+        System.loadLibrary(\"ze_loader\");\n+    }\n+\n+    static void debug(String format, Object... args) {\n+        System.out.printf(format + \"%n\", args);\n+    }\n+\n+    private static void check(int result) {\n+        if (result != ZE_RESULT_SUCCESS()) {\n+            throw new RuntimeException(String.format(\"Call failed: 0x%x (%d)\", result, result));\n+        }\n+    }\n+\n+    MemorySegment contextHandle() {\n+        return contextHandle;\n+    }\n+\n+    MemorySegment deviceHandle() {\n+        return deviceHandle;\n+    }\n+\n+    public LevelZero() {\n+        arena = Arena.ofShared();\n+\n+        \/\/ get driver\n+        check(zeInit(ZE_INIT_FLAG_GPU_ONLY()));\n+        MemorySegment driverCount = arena.allocate(Integer.BYTES);\n+        check(zeDriverGet(driverCount, MemorySegment.NULL));\n+        debug(\"driverCount = %d\", driverCount.get(JAVA_INT, 0));\n+        MemorySegment driverHandles = arena.allocate(driverCount.get(JAVA_INT, 0) * driver_handle_t.byteSize(), 8);\n+        check(zeDriverGet(driverCount, driverHandles));\n+        driverHandle = driverHandles.get(ADDRESS, 0);\n+\n+        \/\/ create context\n+        MemorySegment pContextDesc = arena.allocate(ze_context_desc_t.layout());\n+        ze_context_desc_t.stype(pContextDesc, ZE_STRUCTURE_TYPE_CONTEXT_DESC());\n+        MemorySegment pContextHandle = arena.allocate(ze_context_handle_t);\n+        check(zeContextCreate(driverHandle, pContextDesc, pContextHandle));\n+        contextHandle = pContextHandle.get(ADDRESS, 0);\n+\n+        \/\/ get device\n+        MemorySegment pDeviceCount = arena.allocate(Integer.BYTES);\n+        check(zeDeviceGet(driverHandle, pDeviceCount, MemorySegment.NULL));\n+        int deviceCount = pDeviceCount.get(JAVA_INT, 0);\n+        assert deviceCount > 0;\n+        debug(\"deviceCount = %d\", deviceCount);\n+        MemorySegment deviceHandles = arena.allocate(deviceCount * ze_device_handle_t.byteSize(), 8);\n+        check(zeDeviceGet(driverHandle, pDeviceCount, deviceHandles));\n+        for (int i = 0; i < deviceCount; i++) {\n+            debug(\"device #%d: %s\", i, deviceHandles.get(ze_device_handle_t, i * ze_device_handle_t.byteSize()));\n+        }\n+        deviceHandle = deviceHandles.get(ze_device_handle_t, 0 * ze_device_handle_t.byteSize());\n+        MemorySegment pDeviceProperties = arena.allocate(ze_device_properties_t.layout());\n+        ze_device_properties_t.stype(pDeviceProperties, ZE_STRUCTURE_TYPE_DEVICE_PROPERTIES());\n+        check(zeDeviceGetProperties(deviceHandle, pDeviceProperties));\n+        debug(\"deviceProperties:\\n\\ttype = %d\\n\\tvendorId = %d\\n\\tmaxMemAllocSize = %d\\n\\tdeviceId = %d\\n\\tcoreClockRate = %d\",\n+            ze_device_properties_t.type(pDeviceProperties),\n+            ze_device_properties_t.vendorId(pDeviceProperties),\n+            ze_device_properties_t.maxMemAllocSize(pDeviceProperties),\n+            ze_device_properties_t.deviceId(pDeviceProperties),\n+            ze_device_properties_t.coreClockRate(pDeviceProperties));\n+\n+        MemorySegment pDeviceComputeProperties = arena.allocate(ze_device_compute_properties_t.layout());\n+        ze_device_compute_properties_t.stype(pDeviceComputeProperties, ZE_STRUCTURE_TYPE_DEVICE_COMPUTE_PROPERTIES());\n+        check(zeDeviceGetComputeProperties(deviceHandle, pDeviceComputeProperties));\n+        debug(\"deviceProperties:\\n\\tshared = %d\\n\\tmaxTotalGroupSize = %d\",\n+            ze_device_compute_properties_t.maxSharedLocalMemory(pDeviceComputeProperties),\n+            ze_device_compute_properties_t.maxTotalGroupSize(pDeviceComputeProperties));\n+\n+        \/\/ create queue\n+        MemorySegment pNumQueueGroups = arena.allocate(JAVA_INT, 1);\n+        check(zeDeviceGetCommandQueueGroupProperties(deviceHandle, pNumQueueGroups, MemorySegment.NULL));\n+        debug(\"#Queue Groups: %d\", pNumQueueGroups.get(JAVA_INT, 0));\n+        MemorySegment pGroupProperties = arena.allocate(ze_command_queue_group_properties_t.layout(), pNumQueueGroups.get(JAVA_INT, 0));\n+        check(zeDeviceGetCommandQueueGroupProperties(deviceHandle, pNumQueueGroups, pGroupProperties));\n+\n+        MemorySegment pQueueDesc = arena.allocate(ze_command_queue_desc_t.layout());\n+        ze_command_queue_desc_t.stype(pQueueDesc, ZE_STRUCTURE_TYPE_COMMAND_QUEUE_DESC());\n+        ze_command_queue_desc_t.index(pQueueDesc, 0);\n+        ze_command_queue_desc_t.mode(pQueueDesc, ZE_COMMAND_QUEUE_MODE_SYNCHRONOUS());\n+        ze_command_queue_desc_t.ordinal(pQueueDesc, 0);\n+        MemorySegment pQueueHandle = arena.allocate(ze_command_queue_handle_t);\n+        check(zeCommandQueueCreate(contextHandle, deviceHandle, pQueueDesc, pQueueHandle));\n+        queueHandle = pQueueHandle.get(ADDRESS, 0);\n+\n+        eventPoolDescription = arena.allocate(ze_event_pool_desc_t.layout());\n+        ze_event_pool_desc_t.stype(eventPoolDescription, ZE_STRUCTURE_TYPE_EVENT_POOL_DESC());\n+        ze_event_pool_desc_t.count(eventPoolDescription, 20);\n+        ze_event_pool_desc_t.flags(eventPoolDescription, ZE_EVENT_POOL_FLAG_HOST_VISIBLE());\n+\n+        timeElapsedForRun = timeElapsedForRerun = 0.0;\n+    }\n+\n+    public void clear() {\n+        check(zeCommandQueueDestroy(queueHandle));\n+        check(zeContextDestroy(contextHandle));\n+    }\n+\n+    public void test(String testName) {\n+        Object[] args = {};\n+        Random rand = new Random();\n+        if (testName.equals(\"add\")) {\n+            String jsonFileName = cacheDir + addKernelCache + \"\/add_kernel.json\";\n+            String moduleName = cacheDir + addKernelCache + \"\/add_kernel.spv\";\n+\n+            int BLOCK_SIZE = 64;\n+            int elementSize = 4096;\n+            int gridSize = (elementSize + BLOCK_SIZE - 1) \/ BLOCK_SIZE;\n+\n+            JSONObject jsonObject = loadJson(jsonFileName);\n+            String kernelName = jsonObject.getString(\"name\");\n+            int threads_per_warp = jsonObject.getInt(\"threads_per_warp\");\n+            int num_warps = jsonObject.getInt(\"num_warps\");\n+            int shared = jsonObject.getInt(\"shared\");\n+\n+            float[] input1 = new float[elementSize];\n+            float[] input2 = new float[elementSize];\n+            float[] output = new float[elementSize];\n+            for (int i = 0; i < elementSize; i++) {\n+                input1[i] = rand.nextFloat();\n+                input2[i] = rand.nextFloat();\n+            }\n+            args = new Object[] {input1, input2, output, elementSize};\n+            run(kernelName, moduleName, args, threads_per_warp, num_warps, shared, gridSize);\n+\n+            float[] expected = Test.add(input1, input2, elementSize);\n+            Test.check(expected, output);\n+        } else if (testName.equals(\"softmax\")) {\n+            String jsonFileName = cacheDir + softmaxKernelCache + \"\/softmax_kernel.json\";\n+            String moduleName = cacheDir + softmaxKernelCache + \"\/softmax_kernel.spv\";\n+\n+            JSONObject jsonObject = loadJson(jsonFileName);\n+            String kernelName = jsonObject.getString(\"name\");\n+            int threads_per_warp = jsonObject.getInt(\"threads_per_warp\");\n+            int num_warps = jsonObject.getInt(\"num_warps\");\n+            int shared = jsonObject.getInt(\"shared\");\n+\n+            int elementSizeX = 4096, elementSizeY = 64;\n+            int gridSize = elementSizeX;\n+            float[] input = new float[elementSizeX * elementSizeY];\n+            float[] output = new float[elementSizeX * elementSizeY];\n+            byte[] sharedMemory = new byte[shared]; \/\/ use for storing temporary value of max element and sum of exp\n+            for (int i = 0; i < elementSizeX * elementSizeY; i++) {\n+                input[i] = rand.nextFloat();\n+            }\n+            args = new Object[] {output, input, elementSizeY, elementSizeY, elementSizeY, sharedMemory};\n+            run(kernelName, moduleName, args, threads_per_warp, num_warps, shared, gridSize);\n+\n+            float[] expected = Test.softmax(input, elementSizeX, elementSizeY);\n+            Test.check(expected, output);\n+        } else if (testName.equals(\"matmul\")) {\n+            String jsonFileName = cacheDir + matmulKernelCache + \"\/matmul_kernel.json\";\n+            String moduleName = cacheDir + matmulKernelCache + \"\/matmul_kernel.spv\";\n+\n+            JSONObject jsonObject = loadJson(jsonFileName);\n+            String kernelName = jsonObject.getString(\"name\");\n+            int threads_per_warp = jsonObject.getInt(\"threads_per_warp\");\n+            int num_warps = jsonObject.getInt(\"num_warps\");\n+            int shared = jsonObject.getInt(\"shared\");\n+\n+            int M = 1024, N = 1024, K = 1024;\n+            int BLOCK_SIZE_M = 32, BLOCK_SIZE_N = 64;\n+            int gridSize = ((M + BLOCK_SIZE_M - 1) \/ BLOCK_SIZE_M) * ((N + BLOCK_SIZE_N - 1) \/ BLOCK_SIZE_N);\n+            float[] a = new float[M * K];\n+            float[] b = new float[K * N];\n+            float[] c = new float[M * N];\n+            byte[] sharedMemory = new byte[shared];\n+\n+            for (int i = 0; i < M * K; i++) {\n+                a[i] = rand.nextFloat();\n+            }\n+            for (int i = 0; i < K * N; i++) {\n+                b[i] = rand.nextFloat();\n+            }\n+            args = new Object[] {a, b, c, M, N, K, K, N, N, sharedMemory};\n+            run(kernelName, moduleName, args, threads_per_warp, num_warps, shared, gridSize);\n+\n+            float[] expected = Test.matmul(a, b, M, N, K);\n+            Test.check(expected, c);\n+        } else {\n+            throw new RuntimeException(\"Unsupported test: \" + testName);\n+        }\n+    }\n+\n+    public void run(String kernelName, String fileName, Object[] args, int threads_per_warp, int num_warps, int shared, int gridSize) {\n+        debug(\"=========== run %s ===========\", kernelName);\n+        MemorySegment spirvBinary = loadModule(fileName);\n+        List<Arg> kernelArgs = collectArgs(args);\n+        int[] globalSizes = new int[] {gridSize * threads_per_warp * num_warps, 1, 1};\n+        int[] localSizes = new int[] {threads_per_warp * num_warps, 1, 1};\n+        KernelGeometry geometry = new KernelGeometry(globalSizes, localSizes);\n+        debug(\"geometry = %s\", geometry);\n+        MemorySegment commandListHandle = createCommandList(spirvBinary, kernelName, geometry, kernelArgs, shared, false);\n+        executeCommandList(commandListHandle);\n+        check(zeCommandQueueSynchronize(queueHandle, -1L));\n+        for (int i = 0; i < kernelArgs.size(); i++) {\n+            copyArgToHost(kernelArgs.get(i), contextHandle);\n+        }\n+\n+        for (int i = 0; i < kernelArgs.size(); i++) {\n+            Arg arg = kernelArgs.get(i);\n+            MemorySegment dataSegment = arg.dataSegment();\n+            if (dataSegment != null) {\n+                check(zeMemFree(contextHandle, dataSegment));\n+            }\n+        }\n+        check(zeCommandListDestroy(commandListHandle));\n+    }\n+\n+    public void runRefMatmul(String kernelName, String fileName, Object[] args, int size) {\n+        debug(\"=========== run %s ===========\", kernelName);\n+        MemorySegment spirvBinary = loadModule(fileName);\n+        List<Arg> kernelArgs = collectArgs(args);\n+        int[] globalSizes = new int[] {size, size, 1};\n+        int[] localSizes = new int[] {512, 1, 1};\n+        KernelGeometry geometry = new KernelGeometry(globalSizes, localSizes);\n+        debug(\"geometry = %s\", geometry);\n+        MemorySegment commandListHandle = createCommandList(spirvBinary, kernelName, geometry, kernelArgs, 0, true);\n+        executeCommandList(commandListHandle);\n+        check(zeCommandQueueSynchronize(queueHandle, -1L));\n+        for (int i = 0; i < kernelArgs.size(); i++) {\n+            copyArgToHost(kernelArgs.get(i), contextHandle);\n+        }\n+\n+        for (int i = 0; i < kernelArgs.size(); i++) {\n+            Arg arg = kernelArgs.get(i);\n+            MemorySegment dataSegment = arg.dataSegment();\n+            if (dataSegment != null) {\n+                check(zeMemFree(contextHandle, dataSegment));\n+            }\n+        }\n+        check(zeCommandListDestroy(commandListHandle));\n+    }\n+\n+    private List<Arg> collectArgs(Object[] values) {\n+        List<Arg> args = new ArrayList<>();\n+        for (int i = 0; i < values.length; i++) {\n+            args.add(Arg.createArg(this, \"arg\" + i, values[i]));\n+        }\n+        debug(\"args = %s\", args);\n+        return args;\n+    }\n+\n+    MemorySegment loadModule(String fileName) {\n+        byte[] data = readBytes(fileName);\n+        MemorySegment segment = arena.allocate(data.length);\n+        segment.copyFrom(MemorySegment.ofArray(data));\n+        return segment;\n+    }\n+\n+    byte[] readBytes(String filename) {\n+        File file = new File(filename);\n+        try (FileInputStream fis = new FileInputStream(file)) {\n+            byte[] data = new byte[(int) file.length()];\n+            fis.read(data);\n+            return data;\n+        } catch (IOException e) {\n+            throw new RuntimeException(e);\n+        }\n+    }\n+\n+    void provisionArg(Arg arg) {\n+        if (arg.cls() == byte[].class) {\n+            byte[] array = (byte[])arg.value();\n+            int segmentSize = array.length;\n+            arg.setDataSegment(allocateSharedSegment(segmentSize));\n+            arg.dataSegment().copyFrom(MemorySegment.ofArray(array));\n+            arg.setSize(8);\n+            arg.setNeedsCleanup(true);\n+        }\n+        else if (arg.cls() == short[].class) {\n+            short[] array = (short[])arg.value();\n+            int segmentSize = array.length * Short.BYTES;\n+            arg.setDataSegment(allocateSharedSegment(segmentSize));\n+            arg.dataSegment().copyFrom(MemorySegment.ofArray(array));\n+            arg.setSize(8);\n+            arg.setNeedsCleanup(true);\n+        }\n+        else if (arg.cls() == int[].class) {\n+            int[] array = (int[])arg.value();\n+            int segmentSize = array.length * Integer.BYTES;\n+            arg.setDataSegment(allocateSharedSegment(segmentSize));\n+            arg.dataSegment().copyFrom(MemorySegment.ofArray(array));\n+            arg.setSize(8);\n+            arg.setNeedsCleanup(true);\n+        }\n+        else if (arg.cls() == float[].class) {\n+            float[] array = (float[])arg.value();\n+            int segmentSize = array.length * Float.BYTES;\n+            arg.setDataSegment(allocateSharedSegment(segmentSize));\n+            arg.dataSegment().copyFrom(MemorySegment.ofArray(array));\n+            arg.setSize(8);\n+            arg.setNeedsCleanup(true);\n+        }\n+        else if (VectorSpecies.class.isAssignableFrom(arg.cls())) {\n+            arg.setSize(4);\n+        }\n+        else if (arg.cls() == Short.class) {\n+            arg.setSize(2);\n+        }\n+        else if (arg.cls() == Integer.class || arg.cls() == Float.class || arg.cls() == Boolean.class) {\n+            arg.setSize(4);\n+        }\n+        else if (arg.cls() == Long.class) {\n+            arg.setSize(8);\n+        }\n+        else if (arg.cls() == GPU.Index.class) {\n+            MemorySegment pBuffer = arena.allocate(ADDRESS);\n+            arg.setDataSegment(allocateSharedSegment(24));\n+            arg.setSize(24);\n+        }\n+        else throw new RuntimeException(\"unsupported type: \" + arg.cls());\n+    }\n+\n+    void copyArgToHost(Arg arg, MemorySegment contextHandle) {\n+        if (arg.cls() == short[].class) {\n+            short[] array = (short[])arg.value();\n+            MemorySegment arraySegment = MemorySegment.ofArray(array);\n+            arraySegment.copyFrom(arg.dataSegment());\n+        }\n+        else if (arg.cls() == int[].class) {\n+            int[] array = (int[])arg.value();\n+            MemorySegment arraySegment = MemorySegment.ofArray(array);\n+            arraySegment.copyFrom(arg.dataSegment());\n+        }\n+        else if (arg.cls() == float[].class) {\n+            float[] array = (float[])arg.value();\n+            MemorySegment arraySegment = MemorySegment.ofArray(array);\n+            arraySegment.copyFrom(arg.dataSegment());\n+        }\n+        \/\/ else nothing to do\n+    }\n+\n+    private MemorySegment createCommandList(MemorySegment spirvModule, String kernelName, KernelGeometry geometry, List<Arg> args, int shared, boolean suggested) {\n+        Arena arena = Arena.ofShared();\n+        MemorySegment pCommandListHandle = arena.allocate(ze_command_list_handle_t);\n+        MemorySegment commandListDesc = arena.allocate(ze_command_list_desc_t.layout());\n+        ze_command_list_desc_t.stype(eventPoolDescription, ZE_STRUCTURE_TYPE_COMMAND_LIST_DESC());\n+        ze_command_list_desc_t.commandQueueGroupOrdinal(commandListDesc, 0);\n+        MemorySegment moduleHandle = createModule(kernelName, spirvModule);\n+        check(zeCommandListCreate(contextHandle, deviceHandle, commandListDesc, pCommandListHandle));\n+        MemorySegment commandListHandle = pCommandListHandle.get(ADDRESS, 0);\n+        MemorySegment kernelHandle = createKernel(moduleHandle, kernelName, geometry, suggested);\n+        for (int i = 0; i < args.size(); i++) {\n+            Arg arg = args.get(i);\n+            setKernelArg(arg, i, commandListHandle, kernelHandle, (shared != 0) && (i == args.size() - 1));\n+        }\n+        MemorySegment groupCount = arena.allocate(ze_group_count_t.layout());\n+        ze_group_count_t.groupCountX(groupCount, (geometry.globalSizes()[0] + geometry.localSizes()[0] - 1) \/ geometry.localSizes()[0]);\n+        ze_group_count_t.groupCountY(groupCount, (geometry.globalSizes()[1] + geometry.localSizes()[1] - 1) \/ geometry.localSizes()[1]);\n+        ze_group_count_t.groupCountZ(groupCount, (geometry.globalSizes()[2] + geometry.localSizes()[2] - 1) \/ geometry.localSizes()[2]);\n+        MemorySegment pKernelWaitHandles = MemorySegment.NULL;\n+        check(zeCommandListAppendLaunchKernel(commandListHandle, kernelHandle, groupCount, MemorySegment.NULL, 0, pKernelWaitHandles));\n+        check(zeCommandListClose(commandListHandle));\n+        return commandListHandle;\n+    }\n+\n+    private MemorySegment executeCommandList(MemorySegment commandListHandle) {\n+        MemorySegment fenceDesc = arena.allocate(ze_fence_desc_t.layout());\n+        ze_module_desc_t.stype(fenceDesc, ZE_STRUCTURE_TYPE_FENCE_DESC());\n+        ze_fence_desc_t.flags(fenceDesc, ZE_FENCE_FLAG_SIGNALED());\n+        MemorySegment pFenceHandle = arena.allocate(ze_fence_handle_t);\n+        check(zeFenceCreate(queueHandle, fenceDesc, pFenceHandle));\n+        MemorySegment fenceHandle = pFenceHandle.get(ADDRESS, 0);\n+        MemorySegment pCommandListHandle = arena.allocate(ze_command_list_handle_t);\n+        pCommandListHandle.set(ADDRESS, 0, commandListHandle);\n+        Instant start = Instant.now();\n+        check(zeCommandQueueExecuteCommandLists(queueHandle, 1, pCommandListHandle, fenceHandle));\n+        check(zeCommandQueueSynchronize(queueHandle, -1L));\n+        Instant finish = Instant.now();\n+        Double timeElapsed = Duration.between(start, finish).toNanos() * 1e-6;\n+        timeElapsedForRun += timeElapsed;\n+        debug(\"time: %f %f\\n\", timeElapsed, timeElapsedForRun);\n+\n+        start = Instant.now();\n+        check(zeCommandQueueExecuteCommandLists(queueHandle, 1, pCommandListHandle, fenceHandle));\n+        check(zeCommandQueueSynchronize(queueHandle, -1L));\n+        finish = Instant.now();\n+        timeElapsed = Duration.between(start, finish).toNanos() * 1e-6;\n+        timeElapsedForRerun += timeElapsed;\n+        debug(\"time for rerun: %f %f\\n\", timeElapsed, timeElapsedForRerun);\n+        return fenceHandle;\n+    }\n+\n+    private MemorySegment createKernel(MemorySegment moduleHandle, String kernelNameString, KernelGeometry geometry, boolean suggested) {\n+        MemorySegment kernelDesc = arena.allocate(ze_kernel_desc_t.layout());\n+        MemorySegment kernelName = arena.allocateFrom(kernelNameString);\n+        ze_kernel_desc_t.stype(kernelDesc, ZE_STRUCTURE_TYPE_KERNEL_DESC());\n+        ze_kernel_desc_t.pKernelName(kernelDesc, kernelName);\n+        debug(\"name = %s\", kernelNameString);\n+        MemorySegment pKernelHandle = arena.allocate(ze_kernel_handle_t);\n+        check(zeKernelCreate(moduleHandle, kernelDesc, pKernelHandle));\n+        int[] globalSizes = geometry.globalSizes();\n+        int[] localSizes = geometry.localSizes();\n+        MemorySegment kernelHandle = pKernelHandle.get(ADDRESS, 0);\n+        if (suggested) {\n+            MemorySegment pGroupSizeX = arena.allocate(JAVA_INT, localSizes[0]);\n+            MemorySegment pGroupSizeY = arena.allocate(JAVA_INT, localSizes[1]);\n+            MemorySegment pGroupSizeZ = arena.allocate(JAVA_INT, localSizes[2]);\n+            check(zeKernelSuggestGroupSize(kernelHandle, globalSizes[0], globalSizes[1], globalSizes[2], pGroupSizeX, pGroupSizeY, pGroupSizeZ));\n+            geometry.localSizes()[0] = pGroupSizeX.get(JAVA_INT, 0);\n+            geometry.localSizes()[1] = pGroupSizeY.get(JAVA_INT, 0);\n+            geometry.localSizes()[2] = pGroupSizeZ.get(JAVA_INT, 0);\n+            debug(\"use suggested group size\", geometry.toString());\n+            check(zeKernelSetGroupSize(kernelHandle, pGroupSizeX.get(JAVA_INT, 0), pGroupSizeY.get(JAVA_INT, 0), pGroupSizeZ.get(JAVA_INT, 0)));\n+        } else {\n+            debug(\"use localSizes\", geometry.toString());\n+            check(zeKernelSetGroupSize(kernelHandle, localSizes[0], localSizes[1], localSizes[2]));\n+        }\n+        return kernelHandle;\n+    }\n+\n+    private void setKernelArg(Arg arg, int ordinal, MemorySegment commandListHandle, MemorySegment kernelHandle, boolean shared) {\n+        MemorySegment dataSegment = arg.dataSegment();\n+        Class<?> cls = arg.cls();\n+        debug(\"ordinal = %d, cls = %s, data = %s\", ordinal, cls.getSimpleName(), dataSegment);\n+        if (shared) { \/\/ shared memory\n+            check(zeKernelSetArgumentValue(kernelHandle, ordinal, dataSegment.byteSize(), dataSegment));\n+        }\n+        else if (cls == byte[].class || cls == short[].class || cls == int[].class || cls == float[].class || cls.getSimpleName().equals(\"NativeMemorySegmentImpl\")) {\n+            check(zeCommandListAppendMemoryPrefetch(commandListHandle, dataSegment, dataSegment.byteSize()));\n+            check(zeCommandListAppendMemAdvise(commandListHandle, deviceHandle, dataSegment, dataSegment.byteSize(), ZE_MEMORY_ADVICE_SET_PREFERRED_LOCATION()));\n+            MemorySegment pDataSegment = arena.allocateFrom(ADDRESS, dataSegment);\n+            check(zeKernelSetArgumentValue(kernelHandle, ordinal, ADDRESS.byteSize(), pDataSegment));\n+        }\n+        else if (cls == Short.class) {\n+            MemorySegment pArgValue = arena.allocateFrom(JAVA_SHORT, (short)arg.value());\n+            check(zeKernelSetArgumentValue(kernelHandle, ordinal, Short.BYTES, pArgValue));\n+        }\n+        else if (VectorSpecies.class.isAssignableFrom(cls)) {\n+            MemorySegment pArgValue = arena.allocateFrom(JAVA_INT, FloatVector.SPECIES_256.length());\n+            check(zeKernelSetArgumentValue(kernelHandle, ordinal, Integer.BYTES, pArgValue));\n+        }\n+        else if (cls == Integer.class || cls == Boolean.class) {\n+            MemorySegment pArgValue = arena.allocateFrom(JAVA_INT, (int)arg.value());\n+            check(zeKernelSetArgumentValue(kernelHandle, ordinal, Integer.BYTES, pArgValue));\n+        }\n+        else if (cls == Long.class) {\n+            MemorySegment pArgValue = arena.allocateFrom(JAVA_LONG, (long)arg.value());\n+            check(zeKernelSetArgumentValue(kernelHandle, ordinal, Long.BYTES, pArgValue));\n+        }\n+        else if (cls == Float.class) {\n+            MemorySegment pArgValue = arena.allocateFrom(JAVA_LONG, Float.floatToIntBits((float)arg.value()));\n+            check(zeKernelSetArgumentValue(kernelHandle, ordinal, Float.BYTES, pArgValue));\n+        }\n+        else if (cls == GPU.Index.class) {\n+            MemorySegment pDataSegment = arena.allocateFrom(ADDRESS, dataSegment);\n+            check(zeKernelSetArgumentValue(kernelHandle, ordinal, 24, pDataSegment));\n+        }\n+        else throw new RuntimeException(\"unsupported type: \" + cls);\n+    }\n+\n+    private MemorySegment createModule(String moduleName, MemorySegment spirvCode) {\n+        MemorySegment pModuleHandle = arena.allocate(ze_module_handle_t);\n+        MemorySegment moduleDesc = arena.allocate(ze_module_desc_t.layout());\n+        ze_module_desc_t.stype(moduleDesc, ZE_STRUCTURE_TYPE_MODULE_DESC());\n+        ze_module_desc_t.format(moduleDesc, ZE_MODULE_FORMAT_IL_SPIRV());\n+        ze_module_desc_t.pInputModule(moduleDesc, spirvCode);\n+        ze_module_desc_t.inputSize(moduleDesc, spirvCode.byteSize());\n+        ze_module_desc_t.pBuildFlags(moduleDesc, arena.allocateFrom(\"\"));\n+        MemorySegment buildLogHandle = arena.allocate(ze_module_build_log_handle_t);\n+        check(zeModuleCreate(contextHandle, deviceHandle, moduleDesc, pModuleHandle, buildLogHandle));\n+        MemorySegment moduleHandle = pModuleHandle.get(ADDRESS, 0);\n+        return moduleHandle;\n+    }\n+\n+    public MemorySegment allocateSharedSegment(long byteSize) {\n+        return allocateSharedSegment(contextHandle(), deviceHandle(), byteSize, Arena.global());\n+    }\n+\n+    public static MemorySegment allocateSharedSegment(MemorySegment contextHandle, MemorySegment deviceHandle, long byteSize, Arena arena) {\n+        MemorySegment pDeviceMemAllocDesc = arena.allocate(ze_device_mem_alloc_desc_t.layout());\n+        ze_device_mem_alloc_desc_t.stype(pDeviceMemAllocDesc, ZE_STRUCTURE_TYPE_DEVICE_MEM_ALLOC_DESC());\n+        ze_device_mem_alloc_desc_t.ordinal(pDeviceMemAllocDesc, 0);\n+        MemorySegment pHostMemAllocDesc = arena.allocate(ze_host_mem_alloc_desc_t.layout());\n+        ze_host_mem_alloc_desc_t.stype(pHostMemAllocDesc, ZE_STRUCTURE_TYPE_HOST_MEM_ALLOC_DESC());\n+        MemorySegment pBuffer = arena.allocate(ADDRESS);\n+        check(zeMemAllocShared(contextHandle, pDeviceMemAllocDesc, pHostMemAllocDesc, byteSize, 1, deviceHandle, pBuffer));\n+        long address = pBuffer.get(JAVA_LONG, 0);\n+        return MemorySegment.ofAddress(address).reinterpret(byteSize);\n+    }\n+\n+    private static record KernelGeometry(int[] globalSizes, int[] localSizes) {\n+        public KernelGeometry() {\n+            this(new int[3], new int[] {512, 1, 1});\n+        }\n+\n+        @Override\n+        public String toString() {\n+            return String.format(\"global: %s, local: %s\", Arrays.toString(globalSizes), Arrays.toString(localSizes));\n+        }\n+    }\n+\n+    public void benchAddKernel() {\n+        String jsonFileName = cacheDir + addKernelCache + \"\/add_kernel.json\";\n+        String moduleName = cacheDir + addKernelCache + \"\/add_kernel.spv\";\n+        JSONObject jsonObject = loadJson(jsonFileName);\n+        String kernelName = jsonObject.getString(\"name\");\n+        int threads_per_warp = jsonObject.getInt(\"threads_per_warp\");\n+        int num_warps = jsonObject.getInt(\"num_warps\");\n+        int shared = jsonObject.getInt(\"shared\");\n+        Random rand = new Random();\n+\n+        Writer writer = new Writer(\"benchmark\/vector_add_benchmark.txt\");\n+        writer.write(\"elementSize timeElapsed timeElapsedForRerun RTT gb\/s \\n\");\n+\n+        for (int elementSize = (1 << 12); elementSize <= (1 << 28); elementSize <<= 1) {\n+            int BLOCK_SIZE = 1024;\n+            int gridSize = (elementSize + BLOCK_SIZE - 1) \/ BLOCK_SIZE;\n+\n+            float[] input1 = new float[elementSize];\n+            float[] input2 = new float[elementSize];\n+            float[] output = new float[elementSize];\n+            for (int i = 0; i < elementSize; i++) {\n+                input1[i] = rand.nextFloat();\n+                input2[i] = rand.nextFloat();\n+            }\n+            Object[] args = new Object[] {input1, input2, output, elementSize};\n+\n+            \/\/ warmup\n+            run(kernelName, moduleName, args, threads_per_warp, num_warps, shared, gridSize);\n+            this.timeElapsedForRun = this.timeElapsedForRerun = (double) 0;\n+\n+            int nTimes = 10;\n+            Instant start = Instant.now();\n+            for (int i = 0; i < nTimes; ++i)\n+                run(kernelName, moduleName, args, threads_per_warp, num_warps, shared, gridSize);\n+            Instant finish = Instant.now();\n+            Double RTT = Duration.between(start, finish).toNanos() * 1e-6 \/ nTimes;\n+            Double timeElapsedForRun = this.timeElapsedForRun \/ nTimes;\n+            Double timeElapsedForRerun = this.timeElapsedForRerun \/ nTimes;\n+            writer.write(String.format(\"%d %.4f %.4f %.4f %.4f\\n\", elementSize, timeElapsedForRun, timeElapsedForRerun, RTT, (4f * 3f * elementSize \/ timeElapsedForRerun * 1e-6)));\n+        }\n+        writer.close();\n+    }\n+\n+    public void benchSoftmaxKernel() {\n+        String jsonFileName = cacheDir + softmaxKernelCache + \"\/softmax_kernel.json\";\n+        String moduleName = cacheDir + softmaxKernelCache + \"\/softmax_kernel.spv\";\n+        JSONObject jsonObject = loadJson(jsonFileName);\n+        String kernelName = jsonObject.getString(\"name\");\n+        int threads_per_warp = jsonObject.getInt(\"threads_per_warp\");\n+        int num_warps = jsonObject.getInt(\"num_warps\");\n+        int shared = jsonObject.getInt(\"shared\");\n+        Random rand = new Random();\n+\n+        Writer writer = new Writer(\"benchmark\/softmax_benchmark.txt\");\n+        writer.write(\"elementSizeX elementSizeY timeElapsed timeElapsedForRerun RTT gb\/s \\n\");\n+\n+        for (int i = 2; i < 50; i++) {\n+            int elementSizeX = 4096;\n+            int elementSizeY = 128 * i;\n+            int gridSize = elementSizeX;\n+            float[] input = new float[elementSizeX * elementSizeY];\n+            float[] output = new float[elementSizeX * elementSizeY];\n+            byte[] sharedMemory = new byte[shared];\n+            for (int j = 0; j < elementSizeX * elementSizeY; j++) {\n+                input[j] = rand.nextFloat();\n+            }\n+            Object[] args = new Object[] {output, input, elementSizeY, elementSizeY, elementSizeY, sharedMemory};\n+\n+            \/\/ warmup\n+            run(kernelName, moduleName, args, threads_per_warp, num_warps, shared, gridSize);\n+            this.timeElapsedForRun = this.timeElapsedForRerun = (double) 0;\n+\n+            int nTimes = 10;\n+            Instant start = Instant.now();\n+            for (int j = 0; j < nTimes; ++j)\n+                run(kernelName, moduleName, args, threads_per_warp, num_warps, shared, gridSize);\n+            Instant finish = Instant.now();\n+            Double RTT = Duration.between(start, finish).toNanos() * 1e-6 \/ nTimes;\n+            Double timeElapsedForRun = this.timeElapsedForRun \/ nTimes;\n+            Double timeElapsedForRerun = this.timeElapsedForRerun \/ nTimes;\n+            writer.write(String.format(\"%d %d %.4f %.4f %.4f %.4f\\n\", elementSizeX, elementSizeY, timeElapsedForRun, timeElapsedForRerun, RTT, (4 * 2 * 1e-9 * elementSizeX * elementSizeY \/ (timeElapsedForRerun * 1e-3))));\n+        }\n+        writer.close();\n+    }\n+\n+    public void benchMatmulKernel() {\n+        String jsonFileName = cacheDir + matmulKernelCache + \"\/matmul_kernel.json\";\n+        String moduleName = cacheDir + matmulKernelCache + \"\/matmul_kernel.spv\";\n+\n+        JSONObject jsonObject = loadJson(jsonFileName);\n+        String kernelName = jsonObject.getString(\"name\");\n+        int threads_per_warp = jsonObject.getInt(\"threads_per_warp\");\n+        int num_warps = jsonObject.getInt(\"num_warps\");\n+        int shared = jsonObject.getInt(\"shared\");\n+        Random rand = new Random();\n+        int BLOCK_SIZE_M = 128, BLOCK_SIZE_N = 64;\n+\n+\n+        Writer writer = new Writer(\"benchmark\/matmul_benchmark.txt\");\n+        writer.write(\"M N K timeElapsed timeElapsedForRerun RTT TFLOPS \\n\");\n+\n+        for (int i = 2; i <= 64; i++) {\n+            int M = 128 * i;\n+            int N = 128 * i;\n+            int K = 128 * i;\n+            int gridSize = ((M + BLOCK_SIZE_M - 1) \/ BLOCK_SIZE_M) * ((N + BLOCK_SIZE_N - 1) \/ BLOCK_SIZE_N);\n+\n+            float[] a = new float[M * K];\n+            float[] b = new float[K * N];\n+            float[] c = new float[M * N];\n+            byte[] sharedMemory = new byte[shared];\n+            for (int j = 0; j < M * K; j++) {\n+                a[j] = rand.nextFloat();\n+            }\n+            for (int j = 0; j < K * N; j++) {\n+                b[j] = rand.nextFloat();\n+            }\n+            Object[] args = new Object[] {a, b, c, M, N, K, K, N, N, sharedMemory};\n+\n+            \/\/ warmup\n+            run(kernelName, moduleName, args, threads_per_warp, num_warps, shared, gridSize);\n+            this.timeElapsedForRun = this.timeElapsedForRerun = (double) 0;\n+\n+            int nTimes = 10;\n+            Instant start = Instant.now();\n+            for (int j = 0; j < nTimes; ++j)\n+                run(kernelName, moduleName, args, threads_per_warp, num_warps, shared, gridSize);\n+            Instant finish = Instant.now();\n+            Double RTT = Duration.between(start, finish).toNanos() * 1e-6 \/ nTimes;\n+            Double timeElapsedForRun = this.timeElapsedForRun \/ nTimes;\n+            Double timeElapsedForRerun = this.timeElapsedForRerun \/ nTimes;\n+            writer.write(String.format(\"%d %d %d %.4f %.4f %.4f %.4f\\n\", M, N, K, timeElapsedForRun, timeElapsedForRerun, RTT, (2 * 1e-12 * M * N * K \/ (timeElapsedForRerun * 1e-3))));\n+        }\n+        writer.close();\n+    }\n+\n+    public static void main(String[] args) {\n+        LevelZero lz = new LevelZero();\n+        lz.test(\"add\");\n+        lz.test(\"softmax\");\n+        lz.test(\"matmul\");\n+        lz.benchAddKernel();\n+        lz.benchSoftmaxKernel();\n+        lz.benchMatmulKernel();\n+        lz.clear();\n+    }\n+\n+\n+    public static class Arg {\n+        private final String name;\n+        private final Object value;\n+        private final Class<?> cls;\n+        private int size;\n+        private boolean needsCleanup;\n+        private MemorySegment dataSegment;\n+\n+        public static Arg createArg(LevelZero lz, String name, Object value) {\n+            Arg arg = new Arg(name, value);\n+            lz.provisionArg(arg);\n+            return arg;\n+        }\n+\n+        private Arg(String name, Object value) {\n+            this.name = name;\n+            this.cls = value.getClass();\n+            this.value = value;\n+        }\n+\n+        public String name() {\n+            return name;\n+        }\n+\n+        public Object value() {\n+            return value;\n+        }\n+\n+        public Class<?> cls() {\n+            return cls;\n+        }\n+\n+        public void setSize(int size) {\n+            this.size = size;\n+        }\n+\n+        public int size() {\n+            return size;\n+        }\n+\n+        public void setDataSegment(MemorySegment segment) {\n+            dataSegment = segment;\n+        }\n+\n+        public MemorySegment dataSegment() {\n+            return dataSegment;\n+        }\n+\n+        public void setNeedsCleanup(boolean needsCleanup) {\n+            this.needsCleanup = needsCleanup;\n+        }\n+\n+        public boolean needsCleanup() {\n+            return needsCleanup;\n+        }\n+\n+        public String toString() {\n+            return String.format(\"name = %s, cls = %s\", name, cls);\n+        }\n+    }\n+\n+    private JSONObject loadJson(String fileName) {\n+        StringBuilder jsonString = new StringBuilder();\n+        try (BufferedReader br = new BufferedReader(new FileReader(fileName)))\n+        {\n+            String line;\n+            while ((line = br.readLine()) != null) {\n+                jsonString.append(line);\n+            }\n+        } catch (IOException e) {\n+            e.printStackTrace();\n+        }\n+        JSONObject jsonObject = new JSONObject(jsonString.toString());\n+        return jsonObject;\n+    }\n+\n+    private class Test {\n+        public static float[] add(float[] a, float[] b, int SIZE) {\n+            float[] output = new float[SIZE];\n+            for (int i = 0; i < SIZE; ++i)\n+                output[i] = a[i] + b[i];\n+            return output;\n+        }\n+        public static float[] softmax(float[] a, int X, int Y) {\n+            float[] output = new float[X * Y];\n+            for (int i = 0; i < X; ++i) {\n+                float max = Float.MIN_VALUE;\n+                for (int j = 0; j < Y; ++j) {\n+                    max = Math.max(max, a[i * Y + j]);\n+                }\n+                float sum = 0;\n+                for (int j = 0; j < Y; ++j) {\n+                    output[i * Y + j] = (float)Math.exp(a[i * Y + j] - max);\n+                    sum += output[i * Y + j];\n+                }\n+                for (int j = 0; j < Y; ++j) {\n+                    output[i * Y + j] \/= sum;\n+                }\n+            }\n+            return output;\n+        }\n+        public static float[] matmul(float[] a, float[] b, int M, int N, int K) {\n+            float[] output = new float[M * N];\n+            for (int i = 0; i < M; i++) {\n+                for (int j = 0; j < N; j++) {\n+                    float tmp = 0;\n+                    for (int k = 0; k < K; k++) {\n+                        tmp += a[i * K + k] * b[k * N + j];\n+                    }\n+                    output[i * N + j] = tmp;\n+                }\n+            }\n+            return output;\n+        }\n+        public static void check(float[] expected, float[] output) {\n+            for (int i = 0; i < expected.length; i++) {\n+                if (Math.abs(expected[i] - output[i]) > 1e-2) {\n+                    System.out.printf(\"Mismatch at %d: %f != %f%n\", i, expected[i], output[i]);\n+                    throw new RuntimeException(\"Mismatch\");\n+                }\n+            }\n+            System.out.println(\"Test passed\");\n+        }\n+    }\n+\n+    private class Writer {\n+        private final String fileName;\n+        private final FileWriter writer;\n+\n+        public Writer(String fileName) {\n+            this.fileName = fileName;\n+            try {\n+                writer = new FileWriter(fileName, false);\n+            } catch (IOException e) {\n+                throw new RuntimeException(e);\n+            }\n+        }\n+\n+        public void write(String line) {\n+            try {\n+                writer.write(line);\n+            } catch (IOException e) {\n+                throw new RuntimeException(e);\n+            }\n+        }\n+\n+        public void close() {\n+            try {\n+                writer.close();\n+            } catch (IOException e) {\n+                throw new RuntimeException(e);\n+            }\n+        }\n+    }\n+}\n","filename":"cr-examples\/triton\/spirv\/LevelZero.java","additions":871,"deletions":0,"binary":false,"changes":871,"status":"added"},{"patch":"@@ -0,0 +1,33 @@\n+import torch\n+import intel_extension_for_pytorch  # type: ignore # noqa: F401\n+\n+import triton\n+import triton.language as tl\n+import shutil\n+import os\n+\n+HOME = os.environ['HOME']\n+BABYLON_PATH = os.path.join(HOME, 'babylon')\n+\n+@triton.jit\n+def add_kernel():\n+   pass\n+\n+@triton.jit\n+def matmul_kernel():\n+   pass\n+\n+@triton.jit\n+def softmax_kernel():\n+   pass\n+\n+ADD_KERNEL_MLIR = f\"{BABYLON_PATH}\/cr-examples\/triton\/result\/add_kernel\"\n+MATMUL_MLIR = f\"{BABYLON_PATH}\/cr-examples\/triton\/result\/matmul_kernel\"\n+SOFTMAX_MLIR = f\"{BABYLON_PATH}\/cr-examples\/triton\/result\/softmax_kernel\"\n+\n+if os.path.isdir(f'{HOME}\/.triton\/cache'):\n+   shutil.rmtree(f'{HOME}\/.triton\/cache')\n+\n+triton.compile(triton.compiler.ASTSource(fn=add_kernel, signature={}, constants={}), target_mlir=ADD_KERNEL_MLIR)\n+triton.compile(triton.compiler.ASTSource(fn=softmax_kernel, signature={}, constants={}), target_mlir=SOFTMAX_MLIR, options={\"num_warps\":32})\n+triton.compile(triton.compiler.ASTSource(fn=matmul_kernel, signature={}, constants={}), target_mlir=MATMUL_MLIR, options={\"threads_per_warp\":16, \"num_warps\":64})\n\\ No newline at end of file\n","filename":"cr-examples\/triton\/spirv\/translate.py","additions":33,"deletions":0,"binary":false,"changes":33,"status":"added"},{"patch":"@@ -67,1 +67,1 @@\n-        public static final String ATTRIBUTE_CONSTANT_VALUE = NAME + \".value\";\n+        public static final String ATTRIBUTE_CONSTANT_VALUE = \"value\";\n@@ -163,1 +163,1 @@\n-            attrs.put(\"\", value);\n+            attrs.put(ATTRIBUTE_CONSTANT_VALUE, value);\n@@ -398,1 +398,1 @@\n-        public static final String ATTRIBUTE_CONSTANT_VALUE = NAME + \".compare\";\n+        public static final String ATTRIBUTE_CONSTANT_VALUE = \"predicate\";\n@@ -401,1 +401,10 @@\n-            slt\n+            eq,\n+            ne,\n+            slt,\n+            sle,\n+            sgt,\n+            sge,\n+            ult,\n+            ule,\n+            ugt,\n+            uge\n@@ -434,1 +443,8 @@\n-            super(NAME + nameSuffixFromType(a.type(), false), a.type(), List.of(a, b));\n+            TypeElement t;\n+            if (a.type() instanceof TensorType ot) {\n+                t = new TensorType(JavaType.BOOLEAN, ot.shape());\n+            }\n+            else {\n+                t = JavaType.BOOLEAN;\n+            }\n+            super(NAME + nameSuffixFromType(a.type(), false), t, List.of(a, b));\n@@ -442,1 +458,1 @@\n-            attrs.put(\"\", ck);\n+            attrs.put(ATTRIBUTE_CONSTANT_VALUE, Long.valueOf(ck.ordinal()));\n","filename":"cr-examples\/triton\/src\/main\/java\/oracle\/code\/triton\/ArithMathOps.java","additions":22,"deletions":6,"binary":false,"changes":28,"status":"modified"},{"patch":"@@ -0,0 +1,731 @@\n+package oracle.code.triton;\n+\n+import java.io.IOException;\n+import java.io.StringWriter;\n+import java.io.UncheckedIOException;\n+import java.io.Writer;\n+import java.lang.reflect.code.*;\n+import java.lang.reflect.code.op.ExternalizableOp;\n+import java.lang.reflect.code.type.JavaType;\n+import java.util.HashMap;\n+import java.util.LinkedHashMap;\n+import java.util.Map;\n+import java.util.List;\n+import java.util.Stack;\n+import java.util.regex.Pattern;\n+import java.util.function.Consumer;\n+import java.util.function.Function;\n+import java.util.stream.Collectors;\n+\n+\/**\n+ * A writer of code models to the textual form.\n+ * <p>\n+ * A code model in textual form may be parsed back into the runtime form by\n+ * parsing it.\n+ *\/\n+\/\/ @@@ We cannot link to OpParser since this code is copied into the\n+\/\/ jdk.compiler module\n+\n+public final class MLIRGenerator {\n+\n+    static final class GlobalValueBlockNaming implements Function<CodeItem, String> {\n+        final Map<CodeItem, String> gn;\n+        int valueOrdinal = 0;\n+        int blockOrdinal = 0;\n+\n+        GlobalValueBlockNaming() {\n+            this.gn = new HashMap<>();\n+        }\n+\n+        @Override\n+        public String apply(CodeItem codeItem) {\n+            return switch (codeItem) {\n+                case Block block -> gn.computeIfAbsent(block, _b -> \"block_\" + blockOrdinal++);\n+                case Value value -> gn.computeIfAbsent(value, _v -> String.valueOf(valueOrdinal++));\n+                default -> throw new IllegalStateException(\"Unexpected code item: \" + codeItem);\n+            };\n+        }\n+    }\n+\n+    \/**\n+     * A function that will get value from multiple result values\n+     *\n+     * @param codeItem the code item\n+     * @return the string representation of the code item\n+     *\/\n+    String applyWrapper(CodeItem codeItem) {\n+        String s = namer.apply(codeItem);\n+        return m.getOrDefault(s, s);\n+    }\n+\n+    \/**\n+     * A mapping function that add additional characters to the attribute\n+     *\/\n+    static final class AttributeMapper {\n+        static String toString(String name, Object value) {\n+            if (value == ExternalizableOp.NULL_ATTRIBUTE_VALUE)\n+                return \"null\";\n+            else if (name.equals(\"function_type\") || name.equals(\"arg_attrs\") || name.equals(\"value\"))\n+                return quote(value.toString());\n+            else if (name.equals(\"callee\"))\n+                return \"@\" + quote(value.toString());\n+            else if (name.equals(\"operandSegmentSizes\"))\n+                return value.toString();\n+            else if (value instanceof String)\n+                return \"\\\"\" + quote(value.toString()) + \"\\\"\";\n+            else if (value instanceof Integer)\n+                return quote(value.toString()) + \": i32\";\n+            else if (value instanceof Long)\n+                return quote(value.toString()) + \": i64\";\n+            else if (value instanceof Float)\n+                return quote(value.toString()) + \": f32\";\n+            else if (value instanceof Double)\n+                return quote(value.toString()) + \": f64\";\n+            else\n+                return quote(value.toString());\n+        }\n+    }\n+\n+    \/**\n+     * A mapping function that converts Java Triton types to Triton types.\n+     *\/\n+    static final class TypeConverter {\n+        static final Map<Pattern, String> typeReplacements = new LinkedHashMap<>();\n+\n+        static {\n+            typeReplacements.put(Pattern.compile(\"boolean\"), \"i1\");\n+            typeReplacements.put(Pattern.compile(\"int\"), \"i32\");\n+            typeReplacements.put(Pattern.compile(\"float\"), \"f32\");\n+            typeReplacements.put(Pattern.compile(\"oracle\\\\.code\\\\.triton\\\\.Float16\"), \"f16\");\n+            typeReplacements.put(Pattern.compile(\"ptr\"), \"!tt.ptr\");\n+            typeReplacements.put(Pattern.compile(\"x(\\\\d+)\"), \"$1\");\n+            typeReplacements.put(Pattern.compile(\"void\"), \"()\");\n+        }\n+\n+        \/**\n+         * Helper function to convert the type to MLIR type\n+         *\n+         * @param type    the type\n+         * @param m       the map of the start index to the end index\n+         * @param idxToOp the map of the index to the operation\n+         * @param op      the operation\n+         * @param start   the start index\n+         * @param end     the end index\n+         * @return the string represents MLIR type\n+         *\/\n+        private static String convertType(String type, Map<Integer, Integer> m, Map<Integer, String> idxToOp, String op,\n+                int start, int end) {\n+            StringBuilder sb = new StringBuilder();\n+            for (int i = start; i < end; ++i) {\n+                if (idxToOp.containsKey(i)) {\n+                    String nextOp = idxToOp.get(i);\n+                    if (nextOp.equals(\"ptr\")) {\n+                        sb.append(\"ptr<\");\n+                        sb.append(convertType(type, m, idxToOp, nextOp, i + 4, m.get(i + 3)));\n+                        sb.append(\", 1>\");\n+                        i = m.get(i + 3);\n+                    } else if (nextOp.equals(\"Tuple\")) {\n+                        sb.append(\"(\");\n+                        sb.append(convertType(type, m, idxToOp, nextOp, i + 6, m.get(i + 5)));\n+                        sb.append(\")\");\n+                        i = m.get(i + 5);\n+                    } else if (nextOp.equals(\"tensor\")) {\n+                        sb.append(\"tensor<\");\n+                        sb.append(convertType(type, m, idxToOp, nextOp, i + 7, m.get(i + 6)));\n+                        sb.append(\">\");\n+                        i = m.get(i + 6);\n+                    }\n+                } else {\n+                    if (op.equals(\"tensor\") && type.charAt(i) == ',') {\n+                        sb.append(\"x\");\n+                        i++; \/\/ skip a space\n+                    } else {\n+                        sb.append(type.charAt(i));\n+                    }\n+                }\n+            }\n+            return sb.toString();\n+        }\n+\n+        \/**\n+         * Maps Java types to MLIR types.\n+         *\n+         * @param type the Java type\n+         * @return the string represents MLIR type\n+         *\/\n+        public static String mapType(String type) {\n+            for (Map.Entry<Pattern, String> entry : typeReplacements.entrySet()) {\n+                type = entry.getKey().matcher(type).replaceAll(entry.getValue());\n+            }\n+            StringBuilder sb = new StringBuilder();\n+            Stack<Integer> s = new Stack<>();\n+            Map<Integer, Integer> m = new HashMap<>();\n+            Map<Integer, String> idxToOp = new HashMap<>();\n+\n+            for (int i = 0; i < type.length(); ++i) {\n+                if (i + 3 < type.length() && type.substring(i, i + 3).equals(\"ptr\")) {\n+                    s.push(i + 3);\n+                    idxToOp.put(i, \"ptr\");\n+                } else if (i + 5 < type.length() && type.substring(i, i + 5).equals(\"Tuple\")) {\n+                    s.push(i + 5);\n+                    idxToOp.put(i, \"Tuple\");\n+                } else if (i + 6 < type.length() && type.substring(i, i + 6).equals(\"tensor\")) {\n+                    s.push(i + 6);\n+                    idxToOp.put(i, \"tensor\");\n+                } else if (type.charAt(i) == '>') {\n+                    m.put(s.pop(), i);\n+                }\n+            }\n+\n+            return convertType(type, m, idxToOp, \"\", 0, type.length());\n+        }\n+    }\n+\n+    \/\/ Copied from com.sun.tools.javac.util.Convert\n+    static String quote(String s) {\n+        StringBuilder buf = new StringBuilder();\n+        for (int i = 0; i < s.length(); i++) {\n+            buf.append(quote(s.charAt(i)));\n+        }\n+        return buf.toString();\n+    }\n+\n+    \/**\n+     * Escapes a character if it has an escape sequence or is\n+     * non-printable ASCII. Leaves non-ASCII characters alone.\n+     *\/\n+    static String quote(char ch) {\n+        return switch (ch) {\n+            case '\\b' -> \"\\\\b\";\n+            case '\\f' -> \"\\\\f\";\n+            case '\\n' -> \"\\\\n\";\n+            case '\\r' -> \"\\\\r\";\n+            case '\\t' -> \"\\\\t\";\n+            case '\\'' -> \"\\\\'\";\n+            case '\\\"' -> \"\\\\\\\"\";\n+            case '\\\\' -> \"\\\\\\\\\";\n+            default -> (isPrintableAscii(ch))\n+                    ? String.valueOf(ch)\n+                    : String.format(\"\\\\u%04x\", (int) ch);\n+        };\n+    }\n+\n+    \/**\n+     * Is a character printable ASCII?\n+     *\/\n+    static boolean isPrintableAscii(char ch) {\n+        return ch >= ' ' && ch <= '~';\n+    }\n+\n+    static final class IndentWriter extends Writer {\n+        static final int INDENT = 2;\n+\n+        final Writer w;\n+        int indent;\n+        boolean writeIndent = true;\n+\n+        IndentWriter(Writer w) {\n+            this(w, 0);\n+        }\n+\n+        IndentWriter(Writer w, int indent) {\n+            this.w = w;\n+            this.indent = indent;\n+        }\n+\n+        @Override\n+        public void write(char[] cbuf, int off, int len) throws IOException {\n+            if (writeIndent) {\n+                w.write(\" \".repeat(indent));\n+                writeIndent = false;\n+            }\n+            w.write(cbuf, off, len);\n+            if (len > 0 && cbuf[off + len - 1] == '\\n') {\n+                writeIndent = true;\n+            }\n+        }\n+\n+        @Override\n+        public void flush() throws IOException {\n+            w.flush();\n+        }\n+\n+        @Override\n+        public void close() throws IOException {\n+            w.close();\n+        }\n+\n+        void in() {\n+            in(INDENT);\n+        }\n+\n+        void in(int i) {\n+            indent += i;\n+        }\n+\n+        void out() {\n+            out(INDENT);\n+        }\n+\n+        void out(int i) {\n+            indent -= i;\n+        }\n+    }\n+\n+    \/**\n+     * Computes global names for blocks and values in a code model.\n+     * <p>\n+     * The code model is traversed in the same order as if the model\n+     * was written. Therefore, the names in the returned map will the\n+     * same as the names that are written. This can be useful for debugging\n+     * and testing.\n+     *\n+     * @param root the code model\n+     * @return the map of computed names, modifiable\n+     *\/\n+    public static Function<CodeItem, String> computeGlobalNames(Op root) {\n+        MLIRGenerator w = new MLIRGenerator(Writer.nullWriter());\n+        w.writeOp(root);\n+        return w.namer();\n+    }\n+\n+    \/**\n+     * Writes a code model (an operation) to the character stream.\n+     * <p>\n+     * The character stream will be flushed after the model is writen.\n+     *\n+     * @param w  the character stream\n+     * @param op the code model\n+     *\/\n+    public static void writeTo(Writer w, Op op) {\n+        MLIRGenerator ow = new MLIRGenerator(w);\n+        ow.writeOp(op);\n+        try {\n+            w.flush();\n+        } catch (IOException e) {\n+            throw new UncheckedIOException(e);\n+        }\n+    }\n+\n+    \/**\n+     * Writes a code model (an operation) to the character stream.\n+     * <p>\n+     * The character stream will be flushed after the model is writen.\n+     *\n+     * @param w       the character stream\n+     * @param op      the code model\n+     * @param options the writer options\n+     *\/\n+    public static void writeTo(Writer w, Op op, Option... options) {\n+        MLIRGenerator ow = new MLIRGenerator(w, options);\n+        ow.writeOp(op);\n+        try {\n+            \/\/ @@@ Is this needed?\n+            w.flush();\n+        } catch (IOException e) {\n+            throw new UncheckedIOException(e);\n+        }\n+    }\n+\n+    \/**\n+     * Writes a code model (an operation) to a string.\n+     *\n+     * @param op the code model\n+     *\/\n+    public static String transform(Op op) {\n+        StringWriter w = new StringWriter();\n+        writeTo(w, op);\n+        return w.toString();\n+    }\n+\n+    \/**\n+     * Writes a code model (an operation) to a string.\n+     *\n+     * @param op      the code model\n+     * @param options the writer options\n+     *\/\n+    public static String toText(Op op, MLIRGenerator.Option... options) {\n+        StringWriter w = new StringWriter();\n+        writeTo(w, op, options);\n+        return w.toString();\n+    }\n+\n+    \/**\n+     * An option that affects the writing operations.\n+     *\/\n+    public sealed interface Option {\n+    }\n+\n+    \/**\n+     * An option describing the function to use for naming code items.\n+     *\/\n+    public sealed interface CodeItemNamerOption extends Option\n+            permits NamerOptionImpl {\n+\n+        static CodeItemNamerOption of(Function<CodeItem, String> named) {\n+            return new NamerOptionImpl(named);\n+        }\n+\n+        static CodeItemNamerOption defaultValue() {\n+            return of(new GlobalValueBlockNaming());\n+        }\n+\n+        Function<CodeItem, String> namer();\n+    }\n+\n+    private record NamerOptionImpl(Function<CodeItem, String> namer) implements CodeItemNamerOption {\n+    }\n+\n+    \/**\n+     * An option describing whether location information should be written or\n+     * dropped.\n+     *\/\n+    public enum LocationOption implements Option {\n+        \/** Writes location *\/\n+        WRITE_LOCATION,\n+        \/** Drops location *\/\n+        DROP_LOCATION;\n+\n+        public static LocationOption defaultValue() {\n+            return WRITE_LOCATION;\n+        }\n+    }\n+\n+    \/**\n+     * An option describing whether an operation's descendant code elements should\n+     * be written or dropped.\n+     *\/\n+    public enum OpDescendantsOption implements Option {\n+        \/** Writes descendants of an operation, if any *\/\n+        WRITE_DESCENDANTS,\n+        \/** Drops descendants of an operation, if any *\/\n+        DROP_DESCENDANTS;\n+\n+        public static OpDescendantsOption defaultValue() {\n+            return WRITE_DESCENDANTS;\n+        }\n+    }\n+\n+    \/**\n+     * An option describing whether an operation's result be written or dropped if\n+     * its type is void.\n+     *\/\n+    public enum VoidOpResultOption implements Option {\n+        \/** Writes void operation result *\/\n+        WRITE_VOID,\n+        \/** Drops void operation result *\/\n+        DROP_VOID;\n+\n+        public static VoidOpResultOption defaultValue() {\n+            return DROP_VOID;\n+        }\n+    }\n+\n+    final Function<CodeItem, String> namer;\n+    final IndentWriter w;\n+    final boolean dropLocation;\n+    final boolean dropOpDescendants;\n+    final boolean writeVoidOpResult;\n+    \/* A map store the original variable number to the var#number *\/\n+    final HashMap<String, String> m;\n+\n+    \/**\n+     * Creates a writer of code models (operations) to their textual form.\n+     *\n+     * @param w the character stream writer to write the textual form.\n+     *\/\n+    public MLIRGenerator(Writer w) {\n+        this.w = new IndentWriter(w);\n+        this.namer = new GlobalValueBlockNaming();\n+        this.dropLocation = false;\n+        this.dropOpDescendants = false;\n+        this.writeVoidOpResult = false;\n+        this.m = new HashMap<>();\n+    }\n+\n+    \/**\n+     * Creates a writer of code models (operations) to their textual form.\n+     *\n+     * @param w       the character stream writer to write the textual form.\n+     * @param options the writer options\n+     *\/\n+    public MLIRGenerator(Writer w, Option... options) {\n+        Function<CodeItem, String> namer = null;\n+        boolean dropLocation = false;\n+        boolean dropOpDescendants = false;\n+        boolean writeVoidOpResult = false;\n+        for (Option option : options) {\n+            switch (option) {\n+                case CodeItemNamerOption namerOption -> {\n+                    namer = namerOption.namer();\n+                }\n+                case LocationOption locationOption -> {\n+                    dropLocation = locationOption == LocationOption.DROP_LOCATION;\n+                }\n+                case OpDescendantsOption opDescendantsOption -> {\n+                    dropOpDescendants = opDescendantsOption == OpDescendantsOption.DROP_DESCENDANTS;\n+                }\n+                case VoidOpResultOption voidOpResultOption -> {\n+                    writeVoidOpResult = voidOpResultOption == VoidOpResultOption.WRITE_VOID;\n+                }\n+            }\n+        }\n+\n+        this.w = new IndentWriter(w);\n+        this.namer = (namer == null) ? new GlobalValueBlockNaming() : namer;\n+        this.dropLocation = dropLocation;\n+        this.dropOpDescendants = dropOpDescendants;\n+        this.writeVoidOpResult = writeVoidOpResult;\n+        this.m = new HashMap<>();\n+    }\n+\n+    \/**\n+     * {@return the function that names blocks and values.}\n+     *\/\n+    public Function<CodeItem, String> namer() {\n+        return namer;\n+    }\n+\n+    \/**\n+     * Add additional attributes to the operation\n+     * This can be removed after babylon support these function attributes\n+     *\n+     * @param op the operation\n+     * @param attributes the attributes of the operation\n+     * @return the updated attributes\n+     *\/\n+    Map<String, Object> addAditionalAttributes(Op op, Map<String, Object> attributes) {\n+        if (op.opName().equals(\"tt.func\")) {\n+            String retType = op.bodies().get(0).bodyType().returnType().toString();\n+            List<Block.Parameter> parameters = op.bodies().get(0).entryBlock().parameters();\n+            attributes = new HashMap<>(attributes);\n+            attributes.put(\"sym_visibility\", retType.equals(\"void\") ? \"public\" : \"private\");\n+            StringBuilder sb = new StringBuilder();\n+            sb.append(\"[\");\n+            sb.append(parameters.stream()\n+                .map(p -> \"{tt.divisibility = 16 : i32}\")\n+                .collect(Collectors.joining(\", \")));\n+            sb.append(\"]\");\n+            attributes.put(\"arg_attrs\", sb.toString());\n+            sb = new StringBuilder();\n+            sb.append(\"(\");\n+            sb.append(parameters.stream()\n+                .map(v -> TypeConverter.mapType(v.type().externalize().toString()))\n+                .collect(Collectors.joining(\", \")));\n+            sb.append(\") -> \");\n+            sb.append(TypeConverter.mapType(retType));\n+            attributes.put(\"function_type\", sb.toString());\n+        } else if (op.opName().equals(\"tt.load\")) {\n+            attributes = new HashMap<>(attributes);\n+            attributes.put(\"operandSegmentSizes\", \"array<i32: \" + (op.operands().size() < 3 ? \"1, 1, 0\" : \"1, 1, 1\") + \">\");\n+        } else if (op.opName().equals(\"arith.constant\")) {\n+            if (op.result().type() instanceof TensorType) {\n+                attributes = new HashMap<>(attributes);\n+                String val = attributes.get(\"value\")\n+                                       .toString()\n+                                       .replaceAll(\"-Infinity\", \"0xFF800000\")\n+                                       .replaceAll(\"Infinity\", \"0x7F800000\");\n+                attributes.put(\"value\", \"dense<\" + val + \">\");\n+            }\n+        }\n+        return attributes;\n+    }\n+\n+    \/**\n+     * Writes a code model, an operation, to the character stream.\n+     *\n+     * @param op the code model\n+     *\/\n+    public void writeOp(Op op) {\n+        \/\/ We use var#number instead of tuple.load\n+        if (op.opName().equals(\"tuple.load\")) {\n+            write(\"\/\/ \");\n+        }\n+        if (op.opName() == \"unreachable\") {\n+            return;\n+        }\n+        if (op.parent() != null) {\n+            Op.Result opr = op.result();\n+            if (writeVoidOpResult || !opr.type().equals(JavaType.VOID)) {\n+                String number = writeValueDeclaration(opr);\n+                if (op.opName().equals(\"scf.for\")) {\n+                    write(\":\" + String.valueOf(op.operands().size() - 3));\n+                } else if (op.opName().equals(\"tuple.load\")) {\n+                    Object value = op instanceof ExternalizableOp exop ? exop.attributes().values().toArray()[0] : 0;\n+                    m.put(number, namer.apply(op.operands().get(0)) + \"#\" + String.valueOf((int) value));\n+                }\n+                write(\" = \");\n+            }\n+        }\n+        write(\"\\\"\");\n+        if (op.opName().equals(\"module\"))\n+            write(\"builtin.\");\n+        write(op.opName());\n+        write(\"\\\"\");\n+\n+        write(\" \");\n+        write(\"(\");\n+        writeCommaSeparatedList(op.operands(), this::writeValueUse);\n+        write(\")\");\n+\n+        if (!op.successors().isEmpty()) {\n+            write(\" \");\n+            writeSpaceSeparatedList(op.successors(), this::writeSuccessor);\n+        }\n+\n+        if (!dropOpDescendants && !op.bodies().isEmpty()) {\n+            int nBodies = op.bodies().size();\n+            if (nBodies == 1) {\n+                write(\" \");\n+            } else {\n+                write(\"\\n\");\n+                w.in();\n+                w.in();\n+            }\n+            boolean first = true;\n+            for (Body body : op.bodies()) {\n+                if (!first) {\n+                    write(\"\\n\");\n+                }\n+                writeBody(body);\n+                first = false;\n+            }\n+            if (nBodies > 1) {\n+                w.out();\n+                w.out();\n+            }\n+        }\n+\n+        Map<String, Object> attributes = op instanceof ExternalizableOp exop ? exop.attributes() : Map.of();\n+        if (dropLocation && !attributes.isEmpty() &&\n+                attributes.containsKey(ExternalizableOp.ATTRIBUTE_LOCATION)) {\n+            attributes = new HashMap<>(attributes);\n+            attributes.remove(ExternalizableOp.ATTRIBUTE_LOCATION);\n+        }\n+        attributes = addAditionalAttributes(op, attributes);\n+        if (!attributes.isEmpty()) {\n+            write(\" \");\n+            write(\"{\");\n+            writeCommaSeparatedList(attributes.entrySet(), e -> writeAttribute(e.getKey(), e.getValue()));\n+            if (op.opName().equals(\"arith.constant\")) {\n+                \/\/ arith.constant verifier needs type information\n+                write(\":\");\n+                writeType(op.resultType());\n+            }\n+            write(\"}\");\n+        }\n+        write(\" : \");\n+        write(\"(\");\n+        writeCommaSeparatedList(op.operands(), this::writeValueType);\n+        write(\") -> \");\n+        writeType(op.resultType());\n+    }\n+\n+    void writeSuccessor(Block.Reference successor) {\n+        writeBlockName(successor.targetBlock());\n+        if (!successor.arguments().isEmpty()) {\n+            write(\"(\");\n+            writeCommaSeparatedList(successor.arguments(), this::writeValueUse);\n+            write(\")\");\n+        }\n+    }\n+\n+    void writeAttribute(String name, Object value) {\n+        if (!name.isEmpty()) {\n+            write(name);\n+            write(\"=\");\n+        }\n+        write(AttributeMapper.toString(name, value));\n+    }\n+\n+    \/\/ writeRegion\n+    void writeBody(Body body) {\n+        write(\"(\");\n+        Block eb = body.entryBlock();\n+        write(\"{\\n\");\n+        w.in();\n+        for (Block b : body.blocks()) {\n+            if (!b.isEntryBlock()) {\n+                write(\"\\n\");\n+            }\n+            writeBlock(b);\n+        }\n+        w.out();\n+        write(\"}\");\n+        write(\")\");\n+    }\n+\n+    void writeBlock(Block block) {\n+        writeBlockName(block);\n+        if (!block.parameters().isEmpty()) {\n+            write(\"(\");\n+            writeCommaSeparatedList(block.parameters(), this::writeValueDeclarationWithType);\n+            write(\")\");\n+        }\n+        write(\":\\n\");\n+        w.in();\n+        for (Op op : block.ops()) {\n+            writeOp(op);\n+            write(\"\\n\");\n+        }\n+        w.out();\n+    }\n+\n+    void writeBlockName(Block b) {\n+        write(\"^\");\n+        write(applyWrapper(b));\n+    }\n+\n+    void writeValueUse(Value v) {\n+        write(\"%\");\n+        write(applyWrapper(v));\n+    }\n+\n+    String writeValueDeclaration(Value v) {\n+        write(\"%\");\n+        String ret = namer.apply(v);\n+        write(ret);\n+        return ret;\n+    }\n+\n+    void writeValueDeclarationWithType(Value v) {\n+        writeValueDeclaration(v);\n+        write(\": \");\n+        writeType(v.type());\n+    }\n+\n+    void writeValueType(Value v) {\n+        writeType(v.type());\n+    }\n+\n+    <T> void writeSpaceSeparatedList(Iterable<T> l, Consumer<T> c) {\n+        writeSeparatedList(\" \", l, c);\n+    }\n+\n+    <T> void writeCommaSeparatedList(Iterable<T> l, Consumer<T> c) {\n+        writeSeparatedList(\", \", l, c);\n+    }\n+\n+    <T> void writeSeparatedList(String separator, Iterable<T> l, Consumer<T> c) {\n+        boolean first = true;\n+        for (T t : l) {\n+            if (!first) {\n+                write(separator);\n+            }\n+            c.accept(t);\n+            first = false;\n+        }\n+    }\n+\n+    void writeType(TypeElement te) {\n+        write(TypeConverter.mapType(te.externalize().toString()));\n+    }\n+\n+    void write(String s) {\n+        try {\n+            w.write(s);\n+        } catch (IOException e) {\n+            throw new UncheckedIOException(e);\n+        }\n+    }\n+}\n\\ No newline at end of file\n","filename":"cr-examples\/triton\/src\/main\/java\/oracle\/code\/triton\/MLIRGenerator.java","additions":731,"deletions":0,"binary":false,"changes":731,"status":"added"},{"patch":"@@ -48,0 +48,4 @@\n+    public static Tensor load(Tensor ptr, Tensor mask, @Constant float other) {\n+        throw new UnsupportedOperationException();\n+    }\n+\n","filename":"cr-examples\/triton\/src\/main\/java\/oracle\/code\/triton\/Triton.java","additions":4,"deletions":0,"binary":false,"changes":4,"status":"modified"},{"patch":"@@ -155,1 +155,1 @@\n-        public static final String ATTRIBUTE_FUNC_NAME = NAME + \".name\";\n+        public static final String ATTRIBUTE_FUNC_NAME = \"sym_name\";\n@@ -220,1 +220,1 @@\n-            m.put(\"\", funcName);\n+            m.put(ATTRIBUTE_FUNC_NAME, funcName);\n@@ -250,1 +250,1 @@\n-        public static final String ATTRIBUTE_FUNC_NAME = NAME + \".name\";\n+        public static final String ATTRIBUTE_FUNC_NAME = \"callee\";\n@@ -290,1 +290,1 @@\n-            m.put(\"\", funcName);\n+            m.put(ATTRIBUTE_FUNC_NAME, funcName);\n@@ -452,1 +452,1 @@\n-            m.put(\"\", axis);\n+            m.put(ATTRIBUTE_AXIS, axis);\n@@ -568,1 +568,1 @@\n-            m.put(\"\", axis);\n+            m.put(ATTRIBUTE_AXIS, axis);\n@@ -663,0 +663,4 @@\n+\n+        LoadOp(TypeElement tensorType, Value ptr, Value mask, Value other) {\n+            super(NAME, tensorType, List.of(ptr, mask, other));\n+        }\n@@ -730,2 +734,2 @@\n-        DotOp(TypeElement tensorType, Value a, Value b) {\n-            super(NAME, tensorType, List.of(a, b));\n+        DotOp(TypeElement tensorType, Value a, Value b, Value c) {\n+            super(NAME, tensorType, List.of(a, b, c));\n@@ -805,0 +809,4 @@\n+    public static LoadOp load(TypeElement tensorType, Value ptr, Value mask, Value other) {\n+        return new LoadOp(tensorType, ptr, mask, other);\n+    }\n+\n@@ -817,2 +825,2 @@\n-    public static DotOp dot(TypeElement tensorType, Value a, Value b) {\n-        return new DotOp(tensorType, a, b);\n+    public static DotOp dot(TypeElement tensorType, Value a, Value b, Value c) {\n+        return new DotOp(tensorType, a, b, c);\n","filename":"cr-examples\/triton\/src\/main\/java\/oracle\/code\/triton\/TritonOps.java","additions":18,"deletions":10,"binary":false,"changes":28,"status":"modified"},{"patch":"@@ -225,0 +225,1 @@\n+                    .filter(m -> m.isVarArgs() ? m.getParameterCount() <= op.operands().size() : m.getParameterCount() == op.operands().size())\n@@ -294,0 +295,10 @@\n+        \/\/                Tensor load(Tensor ptr, Tensor mask, ConstantType other) {\n+        public static TensorType load(TensorType ptr, TensorType mask, ConstantType other) {\n+            checkTensorShape(ptr, mask);\n+            if (ptr.eType() instanceof PtrType eptr) {\n+                return new TensorType(eptr.rType(), ptr.shape());\n+            }\n+\n+            throw new IllegalStateException();\n+        }\n+\n@@ -400,1 +411,6 @@\n-            return binary(a, b);\n+            TypeElement t = binary(a, b);\n+            if (t instanceof TensorType tt) {\n+                return new TensorType(JavaType.BOOLEAN, tt.shape());\n+            } else {\n+                return t;\n+            }\n@@ -845,0 +861,3 @@\n+                        .filter(m -> m.isVarArgs()\n+                        ? m.getParameterCount() \/ 2 - 1 <= op.operands().size()\n+                        : m.getParameterCount() \/ 2 - 1 == op.operands().size())\n@@ -914,0 +933,14 @@\n+        public Value load(TensorType rType, Op.Result r,\n+                          TensorType ptrType, Value ptr,\n+                          TensorType maskType, Value mask,\n+                          ConstantType otherType, Value other) {\n+            broadcastConversionRight(ptrType, maskType, mask);\n+            Value mb = block.op(ArithMathOps.constant(rType, (float)otherType.value()));\n+            block.context().mapValue(other, mb);\n+            return block.op(TritonOps.load(\n+                    rType,\n+                    block.context().getValue(ptr),\n+                    block.context().getValue(mask),\n+                    block.context().getValue(other)));\n+        }\n+\n@@ -1021,2 +1054,9 @@\n-\n-            return block.op(TritonOps.dot(rType, a, b));\n+            Object zero;\n+            try {\n+                JavaType zeroType = JavaType.DOUBLE;\n+                zero = MethodHandles.zero((Class<?>) zeroType.resolve(MethodHandles.lookup())).invoke();\n+            } catch (Throwable e) {\n+                throw new RuntimeException(e);\n+            }\n+            var c = block.op(ArithMathOps.constant(rType, zero));\n+            return block.op(TritonOps.dot(rType, a, b, c));\n@@ -1086,1 +1126,1 @@\n-            broadcastConversion(rType, aType, a, bType, b);\n+            broadcastConversionRight(aType, bType, b);\n","filename":"cr-examples\/triton\/src\/main\/java\/oracle\/code\/triton\/TritonTransformer.java","additions":44,"deletions":4,"binary":false,"changes":48,"status":"modified"},{"patch":"@@ -48,1 +48,1 @@\n-                    %11 : tensor<x64, int> = arith.cmpi %9 %10 @\"slt\";\n+                    %11 : tensor<x64, boolean> = arith.cmpi %9 %10 @\"slt\";\n@@ -117,1 +117,1 @@\n-                    %11 : tensor<x64, int> = arith.cmpi %9 %10 @\"slt\";\n+                    %11 : tensor<x64, boolean> = arith.cmpi %9 %10 @\"slt\";\n","filename":"cr-examples\/triton\/src\/test\/java\/oracle\/code\/triton\/TestAddKernel.java","additions":2,"deletions":2,"binary":false,"changes":4,"status":"modified"},{"patch":"@@ -121,2 +121,2 @@\n-                        %88 : tensor<x1, x32, int> = arith.cmpi %84 %87 @\"slt\";\n-                        %89 : tensor<x32, x32, int> = tt.broadcast %88;\n+                        %88 : tensor<x1, x32, boolean> = arith.cmpi %84 %87 @\"slt\";\n+                        %89 : tensor<x32, x32, boolean> = tt.broadcast %88;\n@@ -128,2 +128,2 @@\n-                        %95 : tensor<x32, x1, int> = arith.cmpi %91 %94 @\"slt\";\n-                        %96 : tensor<x32, x64, int> = tt.broadcast %95;\n+                        %95 : tensor<x32, x1, boolean> = arith.cmpi %91 %94 @\"slt\";\n+                        %96 : tensor<x32, x64, boolean> = tt.broadcast %95;\n@@ -131,9 +131,10 @@\n-                        %98 : tensor<x32, x64, float> = tt.dot %90 %97;\n-                        %99 : tensor<x32, x64, float> = arith.addf %81 %98;\n-                        %100 : int = arith.muli %26 %19;\n-                        %101 : tensor<x32, x32, int> = tt.splat %100;\n-                        %102 : tensor<x32, x32, ptr<float>> = tt.addptr %82 %101;\n-                        %103 : int = arith.muli %26 %20;\n-                        %104 : tensor<x32, x64, int> = tt.splat %103;\n-                        %105 : tensor<x32, x64, ptr<float>> = tt.addptr %83 %104;\n-                        scf.yield %99 %102 %105;\n+                        %98 : tensor<x32, x64, float> = arith.constant @\"0.0\";\n+                        %99 : tensor<x32, x64, float> = tt.dot %90 %97 %98;\n+                        %100 : tensor<x32, x64, float> = arith.addf %81 %99;\n+                        %101 : int = arith.muli %26 %19;\n+                        %102 : tensor<x32, x32, int> = tt.splat %101;\n+                        %103 : tensor<x32, x32, ptr<float>> = tt.addptr %82 %102;\n+                        %104 : int = arith.muli %26 %20;\n+                        %105 : tensor<x32, x64, int> = tt.splat %104;\n+                        %106 : tensor<x32, x64, ptr<float>> = tt.addptr %83 %105;\n+                        scf.yield %100 %103 %106;\n@@ -141,30 +142,30 @@\n-                    %106 : tensor<x32, x64, float> = tuple.load %79 @\"0\";\n-                    %107 : tensor<x32, x32, ptr<float>> = tuple.load %79 @\"1\";\n-                    %108 : tensor<x32, x64, ptr<float>> = tuple.load %79 @\"2\";\n-                    %109 : int = arith.muli %37 %24;\n-                    %110 : tensor<x32, int> = tt.splat %109;\n-                    %111 : tensor<x32, int> = arith.addi %110 %40;\n-                    %112 : int = arith.muli %39 %25;\n-                    %113 : tensor<x64, int> = tt.splat %112;\n-                    %114 : tensor<x64, int> = arith.addi %113 %46;\n-                    %115 : tensor<x32, x1, int> = tt.expand_dims %111 @\"1\";\n-                    %116 : tensor<x32, x1, int> = tt.splat %22;\n-                    %117 : tensor<x32, x1, int> = arith.muli %115 %116;\n-                    %118 : tensor<x1, x64, int> = tt.expand_dims %114 @\"0\";\n-                    %119 : tensor<x1, x64, int> = tt.splat %23;\n-                    %120 : tensor<x1, x64, int> = arith.muli %118 %119;\n-                    %121 : tensor<x32, x64, ptr<float>> = tt.splat %14;\n-                    %122 : tensor<x32, x64, int> = tt.broadcast %117;\n-                    %123 : tensor<x32, x64, int> = tt.broadcast %120;\n-                    %124 : tensor<x32, x64, int> = arith.addi %122 %123;\n-                    %125 : tensor<x32, x64, ptr<float>> = tt.addptr %121 %124;\n-                    %126 : tensor<x32, x1, int> = tt.expand_dims %111 @\"1\";\n-                    %127 : tensor<x32, x1, int> = tt.splat %15;\n-                    %128 : tensor<x32, x1, int> = arith.cmpi %126 %127 @\"slt\";\n-                    %129 : tensor<x1, x64, int> = tt.expand_dims %114 @\"0\";\n-                    %130 : tensor<x1, x64, int> = tt.splat %16;\n-                    %131 : tensor<x1, x64, int> = arith.cmpi %129 %130 @\"slt\";\n-                    %132 : tensor<x32, x64, int> = tt.broadcast %128;\n-                    %133 : tensor<x32, x64, int> = tt.broadcast %131;\n-                    %134 : tensor<x32, x64, int> = arith.andi %132 %133;\n-                    tt.store %125 %106 %134;\n+                    %107 : tensor<x32, x64, float> = tuple.load %79 @\"0\";\n+                    %108 : tensor<x32, x32, ptr<float>> = tuple.load %79 @\"1\";\n+                    %109 : tensor<x32, x64, ptr<float>> = tuple.load %79 @\"2\";\n+                    %110 : int = arith.muli %37 %24;\n+                    %111 : tensor<x32, int> = tt.splat %110;\n+                    %112 : tensor<x32, int> = arith.addi %111 %40;\n+                    %113 : int = arith.muli %39 %25;\n+                    %114 : tensor<x64, int> = tt.splat %113;\n+                    %115 : tensor<x64, int> = arith.addi %114 %46;\n+                    %116 : tensor<x32, x1, int> = tt.expand_dims %112 @\"1\";\n+                    %117 : tensor<x32, x1, int> = tt.splat %22;\n+                    %118 : tensor<x32, x1, int> = arith.muli %116 %117;\n+                    %119 : tensor<x1, x64, int> = tt.expand_dims %115 @\"0\";\n+                    %120 : tensor<x1, x64, int> = tt.splat %23;\n+                    %121 : tensor<x1, x64, int> = arith.muli %119 %120;\n+                    %122 : tensor<x32, x64, ptr<float>> = tt.splat %14;\n+                    %123 : tensor<x32, x64, int> = tt.broadcast %118;\n+                    %124 : tensor<x32, x64, int> = tt.broadcast %121;\n+                    %125 : tensor<x32, x64, int> = arith.addi %123 %124;\n+                    %126 : tensor<x32, x64, ptr<float>> = tt.addptr %122 %125;\n+                    %127 : tensor<x32, x1, int> = tt.expand_dims %112 @\"1\";\n+                    %128 : tensor<x32, x1, int> = tt.splat %15;\n+                    %129 : tensor<x32, x1, boolean> = arith.cmpi %127 %128 @\"slt\";\n+                    %130 : tensor<x1, x64, int> = tt.expand_dims %115 @\"0\";\n+                    %131 : tensor<x1, x64, int> = tt.splat %16;\n+                    %132 : tensor<x1, x64, boolean> = arith.cmpi %130 %131 @\"slt\";\n+                    %133 : tensor<x32, x64, boolean> = tt.broadcast %129;\n+                    %134 : tensor<x32, x64, boolean> = tt.broadcast %132;\n+                    %135 : tensor<x32, x64, boolean> = arith.andi %133 %134;\n+                    tt.store %126 %107 %135;\n@@ -254,1 +255,1 @@\n-            offs_k_m_0 = compare(offs_k_m_0,\n+            var offs_k_m_1 = compare(offs_k_m_0,\n@@ -257,4 +258,4 @@\n-            var a = load(a_ptrs, broadcast(offs_k_m_0, a_ptrs.type()));\n-            var offs_k_m_1 = expand(offs_k, 1);\n-            offs_k_m_1 = compare(offs_k_m_1,\n-                    broadcast(K - k * BLOCK_SIZE_K, offs_k_m_1.type()),\n+            var a = load(a_ptrs, broadcast(offs_k_m_1, a_ptrs.type()));\n+            var offs_k_m_2 = expand(offs_k, 1);\n+            var offs_k_m_3 = compare(offs_k_m_2,\n+                    broadcast(K - k * BLOCK_SIZE_K, offs_k_m_2.type()),\n@@ -262,1 +263,1 @@\n-            var b = load(b_ptrs, broadcast(offs_k_m_1, b_ptrs.type()));\n+            var b = load(b_ptrs, broadcast(offs_k_m_3, b_ptrs.type()));\n@@ -337,1 +338,4 @@\n-                tt.func @\"matmul_kernel_ptr<oracle.code.triton.Float16>_ptr<oracle.code.triton.Float16>_ptr<oracle.code.triton.Float16>_int_int_int_int_int_int_int_int_int_32_64_32_8_false_void\" (%12 : ptr<oracle.code.triton.Float16>, %13 : ptr<oracle.code.triton.Float16>, %14 : ptr<oracle.code.triton.Float16>, %15 : int, %16 : int, %17 : int, %18 : int, %19 : int, %20 : int, %21 : int, %22 : int, %23 : int)void -> {\n+                tt.func @\"matmul_kernel_ptr<float>_ptr<float>_ptr<float>_int_int_int_int_1_int_1_int_1_32_64_32_8_false_void\" (%12 : ptr<float>, %13 : ptr<float>, %14 : ptr<float>, %15 : int, %16 : int, %17 : int, %18 : int, %19 : int, %20 : int)void -> {\n+                    %21 : int = arith.constant @\"1\";\n+                    %22 : int = arith.constant @\"1\";\n+                    %23 : int = arith.constant @\"1\";\n@@ -371,1 +375,1 @@\n-                    %57 : tensor<x1, x32, int> = tt.splat %19;\n+                    %57 : tensor<x1, x32, int> = tt.splat %21;\n@@ -376,2 +380,2 @@\n-                    %62 : tensor<x32, x32, ptr<oracle.code.triton.Float16>> = tt.splat %12;\n-                    %63 : tensor<x32, x32, ptr<oracle.code.triton.Float16>> = tt.addptr %62 %61;\n+                    %62 : tensor<x32, x32, ptr<float>> = tt.splat %12;\n+                    %63 : tensor<x32, x32, ptr<float>> = tt.addptr %62 %61;\n@@ -379,1 +383,1 @@\n-                    %65 : tensor<x32, x1, int> = tt.splat %20;\n+                    %65 : tensor<x32, x1, int> = tt.splat %19;\n@@ -382,1 +386,1 @@\n-                    %68 : tensor<x1, x64, int> = tt.splat %21;\n+                    %68 : tensor<x1, x64, int> = tt.splat %22;\n@@ -387,2 +391,2 @@\n-                    %73 : tensor<x32, x64, ptr<oracle.code.triton.Float16>> = tt.splat %13;\n-                    %74 : tensor<x32, x64, ptr<oracle.code.triton.Float16>> = tt.addptr %73 %72;\n+                    %73 : tensor<x32, x64, ptr<float>> = tt.splat %13;\n+                    %74 : tensor<x32, x64, ptr<float>> = tt.addptr %73 %72;\n@@ -393,1 +397,1 @@\n-                    %79 : Tuple<tensor<x32, x64, float>, tensor<x32, x32, ptr<oracle.code.triton.Float16>>, tensor<x32, x64, ptr<oracle.code.triton.Float16>>> = scf.for %76 %77 %78 %75 %63 %74 (%80 : int, %81 : tensor<x32, x64, float>, %82 : tensor<x32, x32, ptr<oracle.code.triton.Float16>>, %83 : tensor<x32, x64, ptr<oracle.code.triton.Float16>>)Tuple<tensor<x32, x64, float>, tensor<x32, x32, ptr<oracle.code.triton.Float16>>, tensor<x32, x64, ptr<oracle.code.triton.Float16>>> -> {\n+                    %79 : Tuple<tensor<x32, x64, float>, tensor<x32, x32, ptr<float>>, tensor<x32, x64, ptr<float>>> = scf.for %76 %77 %78 %75 %63 %74 (%80 : int, %81 : tensor<x32, x64, float>, %82 : tensor<x32, x32, ptr<float>>, %83 : tensor<x32, x64, ptr<float>>)Tuple<tensor<x32, x64, float>, tensor<x32, x32, ptr<float>>, tensor<x32, x64, ptr<float>>> -> {\n@@ -398,19 +402,22 @@\n-                        %88 : tensor<x1, x32, int> = arith.cmpi %84 %87 @\"slt\";\n-                        %89 : tensor<x32, x32, int> = tt.broadcast %88;\n-                        %90 : tensor<x32, x32, oracle.code.triton.Float16> = tt.load %82 %89;\n-                        %91 : tensor<x32, x1, int> = tt.expand_dims %52 @\"1\";\n-                        %92 : int = arith.muli %80 %26;\n-                        %93 : int = arith.subi %17 %92;\n-                        %94 : tensor<x32, x1, int> = tt.splat %93;\n-                        %95 : tensor<x32, x1, int> = arith.cmpi %91 %94 @\"slt\";\n-                        %96 : tensor<x32, x64, int> = tt.broadcast %95;\n-                        %97 : tensor<x32, x64, oracle.code.triton.Float16> = tt.load %83 %96;\n-                        %98 : tensor<x32, x64, float> = tt.dot %90 %97;\n-                        %99 : tensor<x32, x64, float> = arith.addf %81 %98;\n-                        %100 : int = arith.muli %26 %19;\n-                        %101 : tensor<x32, x32, int> = tt.splat %100;\n-                        %102 : tensor<x32, x32, ptr<oracle.code.triton.Float16>> = tt.addptr %82 %101;\n-                        %103 : int = arith.muli %26 %20;\n-                        %104 : tensor<x32, x64, int> = tt.splat %103;\n-                        %105 : tensor<x32, x64, ptr<oracle.code.triton.Float16>> = tt.addptr %83 %104;\n-                        scf.yield %99 %102 %105;\n+                        %88 : tensor<x1, x32, boolean> = arith.cmpi %84 %87 @\"slt\";\n+                        %89 : tensor<x32, x32, boolean> = tt.broadcast %88;\n+                        %90 : tensor<x32, x32, float> = arith.constant @\"0.0\";\n+                        %91 : tensor<x32, x32, float> = tt.load %82 %89 %90;\n+                        %92 : tensor<x32, x1, int> = tt.expand_dims %52 @\"1\";\n+                        %93 : int = arith.muli %80 %26;\n+                        %94 : int = arith.subi %17 %93;\n+                        %95 : tensor<x32, x1, int> = tt.splat %94;\n+                        %96 : tensor<x32, x1, boolean> = arith.cmpi %92 %95 @\"slt\";\n+                        %97 : tensor<x32, x64, boolean> = tt.broadcast %96;\n+                        %98 : tensor<x32, x64, float> = arith.constant @\"0.0\";\n+                        %99 : tensor<x32, x64, float> = tt.load %83 %97 %98;\n+                        %100 : tensor<x32, x64, float> = arith.constant @\"0.0\";\n+                        %101 : tensor<x32, x64, float> = tt.dot %91 %99 %100;\n+                        %102 : tensor<x32, x64, float> = arith.addf %81 %101;\n+                        %103 : int = arith.muli %26 %21;\n+                        %104 : tensor<x32, x32, int> = tt.splat %103;\n+                        %105 : tensor<x32, x32, ptr<float>> = tt.addptr %82 %104;\n+                        %106 : int = arith.muli %26 %19;\n+                        %107 : tensor<x32, x64, int> = tt.splat %106;\n+                        %108 : tensor<x32, x64, ptr<float>> = tt.addptr %83 %107;\n+                        scf.yield %102 %105 %108;\n@@ -418,31 +425,30 @@\n-                    %106 : tensor<x32, x64, float> = tuple.load %79 @\"0\";\n-                    %107 : tensor<x32, x32, ptr<oracle.code.triton.Float16>> = tuple.load %79 @\"1\";\n-                    %108 : tensor<x32, x64, ptr<oracle.code.triton.Float16>> = tuple.load %79 @\"2\";\n-                    %109 : tensor<x32, x64, oracle.code.triton.Float16> = arith.truncf %106;\n-                    %110 : int = arith.muli %37 %24;\n-                    %111 : tensor<x32, int> = tt.splat %110;\n-                    %112 : tensor<x32, int> = arith.addi %111 %40;\n-                    %113 : int = arith.muli %39 %25;\n-                    %114 : tensor<x64, int> = tt.splat %113;\n-                    %115 : tensor<x64, int> = arith.addi %114 %46;\n-                    %116 : tensor<x32, x1, int> = tt.expand_dims %112 @\"1\";\n-                    %117 : tensor<x32, x1, int> = tt.splat %22;\n-                    %118 : tensor<x32, x1, int> = arith.muli %117 %116;\n-                    %119 : tensor<x1, x64, int> = tt.expand_dims %115 @\"0\";\n-                    %120 : tensor<x1, x64, int> = tt.splat %23;\n-                    %121 : tensor<x1, x64, int> = arith.muli %120 %119;\n-                    %122 : tensor<x32, x64, int> = tt.broadcast %118;\n-                    %123 : tensor<x32, x64, int> = tt.broadcast %121;\n-                    %124 : tensor<x32, x64, int> = arith.addi %122 %123;\n-                    %125 : tensor<x32, x64, ptr<oracle.code.triton.Float16>> = tt.splat %14;\n-                    %126 : tensor<x32, x64, ptr<oracle.code.triton.Float16>> = tt.addptr %125 %124;\n-                    %127 : tensor<x32, x1, int> = tt.expand_dims %112 @\"1\";\n-                    %128 : tensor<x32, x1, int> = tt.splat %15;\n-                    %129 : tensor<x32, x1, int> = arith.cmpi %127 %128 @\"slt\";\n-                    %130 : tensor<x1, x64, int> = tt.expand_dims %115 @\"0\";\n-                    %131 : tensor<x1, x64, int> = tt.splat %16;\n-                    %132 : tensor<x1, x64, int> = arith.cmpi %130 %131 @\"slt\";\n-                    %133 : tensor<x32, x64, int> = tt.broadcast %129;\n-                    %134 : tensor<x32, x64, int> = tt.broadcast %132;\n-                    %135 : tensor<x32, x64, int> = arith.andi %133 %134;\n-                    tt.store %126 %109 %135;\n+                    %109 : tensor<x32, x64, float> = tuple.load %79 @\"0\";\n+                    %110 : tensor<x32, x32, ptr<float>> = tuple.load %79 @\"1\";\n+                    %111 : tensor<x32, x64, ptr<float>> = tuple.load %79 @\"2\";\n+                    %112 : int = arith.muli %37 %24;\n+                    %113 : tensor<x32, int> = tt.splat %112;\n+                    %114 : tensor<x32, int> = arith.addi %113 %40;\n+                    %115 : int = arith.muli %39 %25;\n+                    %116 : tensor<x64, int> = tt.splat %115;\n+                    %117 : tensor<x64, int> = arith.addi %116 %46;\n+                    %118 : tensor<x32, x1, int> = tt.expand_dims %114 @\"1\";\n+                    %119 : tensor<x32, x1, int> = tt.splat %20;\n+                    %120 : tensor<x32, x1, int> = arith.muli %119 %118;\n+                    %121 : tensor<x1, x64, int> = tt.expand_dims %117 @\"0\";\n+                    %122 : tensor<x1, x64, int> = tt.splat %23;\n+                    %123 : tensor<x1, x64, int> = arith.muli %122 %121;\n+                    %124 : tensor<x32, x64, int> = tt.broadcast %120;\n+                    %125 : tensor<x32, x64, int> = tt.broadcast %123;\n+                    %126 : tensor<x32, x64, int> = arith.addi %124 %125;\n+                    %127 : tensor<x32, x64, ptr<float>> = tt.splat %14;\n+                    %128 : tensor<x32, x64, ptr<float>> = tt.addptr %127 %126;\n+                    %129 : tensor<x32, x1, int> = tt.expand_dims %114 @\"1\";\n+                    %130 : tensor<x32, x1, int> = tt.splat %15;\n+                    %131 : tensor<x32, x1, boolean> = arith.cmpi %129 %130 @\"slt\";\n+                    %132 : tensor<x1, x64, int> = tt.expand_dims %117 @\"0\";\n+                    %133 : tensor<x1, x64, int> = tt.splat %16;\n+                    %134 : tensor<x1, x64, boolean> = arith.cmpi %132 %133 @\"slt\";\n+                    %135 : tensor<x32, x64, boolean> = tt.broadcast %131;\n+                    %136 : tensor<x32, x64, boolean> = tt.broadcast %134;\n+                    %137 : tensor<x32, x64, boolean> = arith.andi %135 %136;\n+                    tt.store %128 %109 %137;\n@@ -463,3 +469,3 @@\n-            int stride_am, int stride_ak,\n-            int stride_bk, int stride_bn,\n-            int stride_cm, int stride_cn,\n+            int stride_am, @Constant int stride_ak,\n+            int stride_bk, @Constant int stride_bn,\n+            int stride_cm, @Constant int stride_cn,\n@@ -517,1 +523,1 @@\n-                    compare(expand(offs_k, 0), K - k * BLOCK_SIZE_K, LessThan));\n+                    compare(expand(offs_k, 0), K - k * BLOCK_SIZE_K, LessThan), 0f);\n@@ -519,1 +525,1 @@\n-                    compare(expand(offs_k, 1), K - k * BLOCK_SIZE_K, LessThan));\n+                    compare(expand(offs_k, 1), K - k * BLOCK_SIZE_K, LessThan), 0f);\n@@ -532,1 +538,1 @@\n-        var c = Triton.conv(Float16.class, accumulator);\n+        var c = Triton.conv(float.class, accumulator);\n@@ -551,3 +557,3 @@\n-                new PtrType(Float16.FLOAT_16_TYPE),\n-                new PtrType(Float16.FLOAT_16_TYPE),\n-                new PtrType(Float16.FLOAT_16_TYPE),\n+                new PtrType(JavaType.FLOAT),\n+                new PtrType(JavaType.FLOAT),\n+                new PtrType(JavaType.FLOAT),\n@@ -555,3 +561,3 @@\n-                JavaType.INT, JavaType.INT,\n-                JavaType.INT, JavaType.INT,\n-                JavaType.INT, JavaType.INT,\n+                JavaType.INT, new ConstantType(JavaType.INT, 1),\n+                JavaType.INT, new ConstantType(JavaType.INT, 1),\n+                JavaType.INT, new ConstantType(JavaType.INT, 1),\n","filename":"cr-examples\/triton\/src\/test\/java\/oracle\/code\/triton\/TestMatrix.java","additions":126,"deletions":120,"binary":false,"changes":246,"status":"modified"},{"patch":"@@ -63,4 +63,1 @@\n-                tt.func @\"softmax_kernel_ptr<float>_ptr<float>_1_1_10_64_void\" (%16 : ptr<float>, %17 : ptr<float>)void -> {\n-                    %18 : int = arith.constant @\"1\";\n-                    %19 : int = arith.constant @\"1\";\n-                    %20 : int = arith.constant @\"10\";\n+                tt.func @\"softmax_kernel_ptr<float>_ptr<float>_int_int_int_64_void\" (%16 : ptr<float>, %17 : ptr<float>, %18 : int, %19 : int, %20 : int)void -> {\n@@ -74,1 +71,1 @@\n-                    %28 : tensor<x64, int> = arith.cmpi %24 %27 @\"slt\";\n+                    %28 : tensor<x64, boolean> = arith.cmpi %24 %27 @\"slt\";\n@@ -130,3 +127,3 @@\n-                new ConstantType(JavaType.INT, 1),\n-                new ConstantType(JavaType.INT, 1),\n-                new ConstantType(JavaType.INT, 10),\n+                JavaType.INT,\n+                JavaType.INT,\n+                JavaType.INT,\n@@ -173,1 +170,1 @@\n-                    %28 : tensor<x64, int> = arith.cmpi %24 %27 @\"slt\";\n+                    %28 : tensor<x64, boolean> = arith.cmpi %24 %27 @\"slt\";\n","filename":"cr-examples\/triton\/src\/test\/java\/oracle\/code\/triton\/TestSoftMax.java","additions":6,"deletions":9,"binary":false,"changes":15,"status":"modified"},{"patch":"@@ -31,0 +31,4 @@\n+import java.io.BufferedWriter;\n+import java.io.File;\n+import java.io.FileWriter;\n+import java.io.IOException;\n@@ -110,0 +114,11 @@\n+            String mlirText = MLIRGenerator.transform(actualTritonKernel);\n+            File directory = new File(\"result\");\n+            if (!directory.exists()) {\n+                directory.mkdirs();\n+            }\n+            try (BufferedWriter writer = new BufferedWriter(new FileWriter(\"result\/\" + javaKernelName + \".mlir\"))) {\n+                writer.write(mlirText);\n+            } catch (IOException e) {\n+                e.printStackTrace();\n+            }\n+\n","filename":"cr-examples\/triton\/src\/test\/java\/oracle\/code\/triton\/TritonTestExtension.java","additions":15,"deletions":0,"binary":false,"changes":15,"status":"modified"}]}