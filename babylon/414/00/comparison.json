{"files":[{"patch":"@@ -64,1 +64,1 @@\n-        TENSOR(byte[].class),\n+        TENSOR(Tensor.class),\n@@ -75,1 +75,1 @@\n-        TENSORS(byte[][].class),\n+        TENSORS(Tensor[].class),\n","filename":"cr-examples\/onnx\/opgen\/src\/main\/java\/oracle\/code\/onnx\/OpSchema.java","additions":2,"deletions":2,"binary":false,"changes":4,"status":"modified"},{"patch":"@@ -100,0 +100,1 @@\n+        w.write(\"import \" + ONNX_PACKAGE + \".Tensor;\\n\");\n","filename":"cr-examples\/onnx\/opgen\/src\/main\/java\/oracle\/code\/onnx\/opgen\/OpGen.java","additions":1,"deletions":0,"binary":false,"changes":1,"status":"modified"},{"patch":"@@ -0,0 +1,683 @@\n+\/*\n+ * Copyright (c) 2025, Oracle and\/or its affiliates. All rights reserved.\n+ * DO NOT ALTER OR REMOVE COPYRIGHT NOTICES OR THIS FILE HEADER.\n+ *\n+ * This code is free software; you can redistribute it and\/or modify it\n+ * under the terms of the GNU General Public License version 2 only, as\n+ * published by the Free Software Foundation.  Oracle designates this\n+ * particular file as subject to the \"Classpath\" exception as provided\n+ * by Oracle in the LICENSE file that accompanied this code.\n+ *\n+ * This code is distributed in the hope that it will be useful, but WITHOUT\n+ * ANY WARRANTY; without even the implied warranty of MERCHANTABILITY or\n+ * FITNESS FOR A PARTICULAR PURPOSE.  See the GNU General Public License\n+ * version 2 for more details (a copy is included in the LICENSE file that\n+ * accompanied this code).\n+ *\n+ * You should have received a copy of the GNU General Public License version\n+ * 2 along with this work; if not, write to the Free Software Foundation,\n+ * Inc., 51 Franklin St, Fifth Floor, Boston, MA 02110-1301 USA.\n+ *\n+ * Please contact Oracle, 500 Oracle Parkway, Redwood Shores, CA 94065 USA\n+ * or visit www.oracle.com if you need additional information or have any\n+ * questions.\n+ *\/\n+\n+package oracle.code.onnx.protogen;\n+\n+import java.io.FileOutputStream;\n+import java.io.PrintStream;\n+import java.nio.file.Files;\n+import java.nio.file.Path;\n+import java.util.ArrayList;\n+import java.util.Iterator;\n+import java.util.List;\n+import java.util.regex.Matcher;\n+import java.util.regex.Pattern;\n+import java.util.stream.Stream;\n+\n+public class ProtoGen {\n+\n+    static final String COPYRIGHT_NOTICE = \"\"\"\n+            \/*\n+             * Copyright (c) 2025, Oracle and\/or its affiliates. All rights reserved.\n+             * DO NOT ALTER OR REMOVE COPYRIGHT NOTICES OR THIS FILE HEADER.\n+             *\n+             * This code is free software; you can redistribute it and\/or modify it\n+             * under the terms of the GNU General Public License version 2 only, as\n+             * published by the Free Software Foundation.  Oracle designates this\n+             * particular file as subject to the \"Classpath\" exception as provided\n+             * by Oracle in the LICENSE file that accompanied this code.\n+             *\n+             * This code is distributed in the hope that it will be useful, but WITHOUT\n+             * ANY WARRANTY; without even the implied warranty of MERCHANTABILITY or\n+             * FITNESS FOR A PARTICULAR PURPOSE.  See the GNU General Public License\n+             * version 2 for more details (a copy is included in the LICENSE file that\n+             * accompanied this code).\n+             *\n+             * You should have received a copy of the GNU General Public License version\n+             * 2 along with this work; if not, write to the Free Software Foundation,\n+             * Inc., 51 Franklin St, Fifth Floor, Boston, MA 02110-1301 USA.\n+             *\n+             * Please contact Oracle, 500 Oracle Parkway, Redwood Shores, CA 94065 USA\n+             * or visit www.oracle.com if you need additional information or have any\n+             * questions.\n+             *\/\n+\n+            \"\"\";\n+\n+    static final String PROTOGEN_PACKAGE = \"oracle.code.onnx.proto\";\n+    static final String PROTOGEN_CONSTANTS_CLASS = \"OnnxConstants\";\n+    static final String PROTOGEN_BUILDER_CLASS = \"OnnxBuilder\";\n+    static final String PROTOGEN_MODEL_CLASS = \"OnnxModel\";\n+\n+    static final String SOURCES_PATH = \"src\/main\/java\/\" +PROTOGEN_PACKAGE.replace(\".\", \"\/\") + \"\/\";\n+\n+    private static final String E = \"\\\\s*\";\n+    private static final String C = \"\\\\s*(?<comment>\/\/.*)\";\n+    private static final String OC = C + \"?\";\n+    private static final String NB = \"\\\\s+(?<name>\\\\w+)\\\\s*\\\\{\" + OC;\n+\n+    enum TokenType {\n+        EMPTY(E),\n+        COMMENT(E + C),\n+        FIELD(E + \"(?<flag>optional |repeated |)\\\\s*(?<type>\\\\w+)\\\\s+(?<name>\\\\w+)\\\\s*=\\\\s*(?<index>\\\\d+)\\\\s*(\\\\[.*\\\\])?\\\\s*;\" + OC),\n+        ENUM_ELEMENT(E + \"(?<name>\\\\w+)\\\\s*=\\\\s*(?<value>\\\\w+)\\\\s*;\" + OC),\n+        END(E + \"\\\\}\\\\s*;?\" + OC),\n+        MESSAGE(E + \"message\" + NB),\n+        ENUM(E + \"enum\" + NB),\n+        ONEOF(E + \"oneof\" + NB),\n+        PACKAGE(E + \"package\\\\s+(?<name>\\\\S+)\\\\s*;\" + OC),\n+        RESERVED(E + \"reserved\\\\s+(?<words>.+)\\\\s*;\" + OC),\n+        SYNTAX(E + \"syntax\\\\s*=\\\\s*(?<version>.+)\\\\s*;\" + OC);\n+\n+        final Pattern pattern;\n+\n+        TokenType(String pattern) {\n+            this.pattern = Pattern.compile(pattern);\n+        }\n+    }\n+\n+    record Token(TokenType type, Matcher matcher) {}\n+\n+    record TreeNode(List<String> comments, Token token, List<TreeNode> nested) {}\n+\n+    static Token lineToToken(String line) {\n+        for (var tt : TokenType.values()) {\n+            var m = tt.pattern.matcher(line);\n+            if (m.matches()) {\n+                return new Token(tt, m);\n+            }\n+        }\n+        throw new IllegalArgumentException(line);\n+    }\n+\n+    static void generateConstants(List<TreeNode> tree, PrintStream out) {\n+        out.print(COPYRIGHT_NOTICE);\n+        out.print(\"\"\"\n+                package %1$s;\n+\n+                import java.util.function.IntSupplier;\n+\n+                \/\/ Generated from onnx.in.proto\n+                public final class %2$s {\n+                \"\"\".formatted(PROTOGEN_PACKAGE, PROTOGEN_CONSTANTS_CLASS));\n+        for (TreeNode en : tree.stream().flatMap(n -> Stream.concat(Stream.of(n), n.nested().stream())).filter(n -> n.token().type() == TokenType.ENUM).toList()) {\n+            out.println();\n+            String name = en.token().matcher().group(\"name\");\n+            for (String c : en.comments()) {\n+                out.println(\"    \/\" + c);\n+            }\n+            out.println(\"    public enum \" + name + \" implements IntSupplier {\");\n+            for (TreeNode ev : en.nested()) {\n+                if (ev.token().type() == TokenType.ENUM_ELEMENT) {\n+                    out.println();\n+                    for (String c : ev.comments()) {\n+                        out.println(\"        \/\" + c);\n+                    }\n+                    out.println(\"        \" + ev.token().matcher().group(\"name\") + \"(\" + ev.token().matcher().group(\"value\") + \"),\");\n+                }\n+            }\n+            out.print(\"\"\"\n+                            ;\n+\n+                            final int value;\n+\n+                            %1$s(int value) {\n+                                this.value = value;\n+                            }\n+\n+                            @Override\n+                            public int getAsInt() {\n+                                return value;\n+                            }\n+                        }\n+                    \"\"\".formatted(name));\n+        }\n+        out.println(\"}\");\n+    }\n+\n+    static void generateBuilder(List<TreeNode> tree, PrintStream out) {\n+        out.print(COPYRIGHT_NOTICE);\n+        out.print(\"\"\"\n+                package %1$s;\n+\n+                import java.io.ByteArrayOutputStream;\n+                import java.nio.charset.StandardCharsets;\n+                import java.util.List;\n+                import java.util.function.BiConsumer;\n+                import java.util.function.IntSupplier;\n+\n+                import %1$s.%2$s.*;\n+\n+                \/\/ Generated from onnx.in.proto\n+                public sealed class %3$s<T extends %3$s> {\n+                \"\"\".formatted(PROTOGEN_PACKAGE, PROTOGEN_CONSTANTS_CLASS, PROTOGEN_BUILDER_CLASS));\n+        generateBuilderCode(null, \"    \", tree, out);\n+        out.print(\"\"\"\n+\n+                    \/\/ Implementation\n+\n+                    final ByteArrayOutputStream buf = new ByteArrayOutputStream();\n+\n+                    public byte[] getBytes() {\n+                        return buf.toByteArray();\n+                    }\n+\n+                    @SuppressWarnings(\"unchecked\")\n+                    public <P> T forEach(Iterable<P> sup, BiConsumer<T, ? super P> cons) {\n+                        sup.forEach(p -> cons.accept((T)this, p));\n+                        return (T)this;\n+                    }\n+\n+                    void _encode(long number) {\n+                        for (int i = 64 - Long.numberOfLeadingZeros(number); i > 7; i -= 7) {\n+                            buf.write(0x80 | (int)number & 0x7f);\n+                            number >>= 7;\n+                        }\n+                        buf.write((int)number & 0x7f);\n+                    }\n+\n+                    void _encode(float value) {\n+                        int bits =  Float.floatToRawIntBits(value);\n+                        buf.write((byte)bits);\n+                        buf.write((byte)(bits >> 8));\n+                        buf.write((byte)(bits >> 16));\n+                        buf.write((byte)(bits >> 24));\n+                    }\n+\n+                    void _encode(double value) {\n+                        long bits =  Double.doubleToRawLongBits(value);\n+                        buf.write((byte)bits);\n+                        buf.write((byte)(bits >> 8));\n+                        buf.write((byte)(bits >> 16));\n+                        buf.write((byte)(bits >> 24));\n+                        buf.write((byte)(bits >> 32));\n+                        buf.write((byte)(bits >> 40));\n+                        buf.write((byte)(bits >> 48));\n+                        buf.write((byte)(bits >> 56));\n+                    }\n+\n+                    @SuppressWarnings(\"unchecked\")\n+                    T _f(int fieldIndex, String value) {\n+                        return value == null ? (T)this : _f(fieldIndex, value.getBytes(StandardCharsets.UTF_8));\n+                    }\n+\n+                    @SuppressWarnings(\"unchecked\")\n+                    T _f(int fieldIndex, byte[] bytes) {\n+                        _encode(fieldIndex << 3 | 2);\n+                        _encode(bytes.length);\n+                        buf.writeBytes(bytes);\n+                        return (T)this;\n+                    }\n+\n+                    @SuppressWarnings(\"unchecked\")\n+                    T _f(int fieldIndex, float value) {\n+                        _encode(fieldIndex << 3 | 5);\n+                        _encode(value);\n+                        return (T)this;\n+                    }\n+\n+                    @SuppressWarnings(\"unchecked\")\n+                    T _f(int fieldIndex, float... values) {\n+                        if (values.length == 1) {\n+                            return _f(fieldIndex, values[0]);\n+                        }\n+                        var b = new %1$s();\n+                        for (var v : values) b._encode(v);\n+                        _f(fieldIndex, b);\n+                        return (T)this;\n+                    }\n+\n+                    @SuppressWarnings(\"unchecked\")\n+                    T _f(int fieldIndex, double value) {\n+                        _encode(fieldIndex << 3 | 1);\n+                        _encode(value);\n+                        return (T)this;\n+                    }\n+\n+                    @SuppressWarnings(\"unchecked\")\n+                    T _f(int fieldIndex, double... values) {\n+                        if (values.length == 1) {\n+                            return _f(fieldIndex, values[0]);\n+                        }\n+                        var b = new %1$s();\n+                        for (var v : values) b._encode(v);\n+                        _f(fieldIndex, b);\n+                        return (T)this;\n+                    }\n+\n+                    @SuppressWarnings(\"unchecked\")\n+                    T _f(int fieldIndex, long value) {\n+                        _encode(fieldIndex << 3);\n+                        _encode(value);\n+                        return (T)this;\n+                    }\n+\n+                    @SuppressWarnings(\"unchecked\")\n+                    T _f(int fieldIndex, long... values) {\n+                        if (values.length == 1) {\n+                            return _f(fieldIndex, values[0]);\n+                        }\n+                        var b = new %1$s();\n+                        for (var v : values) b._encode(v);\n+                        _f(fieldIndex, b);\n+                        return (T)this;\n+                    }\n+\n+                    @SuppressWarnings(\"unchecked\")\n+                    T _f(int fieldIndex, int... values) {\n+                        if (values.length == 1) {\n+                            return _f(fieldIndex, values[0]);\n+                        }\n+                        var b = new %1$s();\n+                        for (var v : values) b._encode(v);\n+                        _f(fieldIndex, b);\n+                        return (T)this;\n+                    }\n+\n+                    @SuppressWarnings(\"unchecked\")\n+                    T _f(int fieldIndex, %1$s value) {\n+                        return _f(fieldIndex, value.buf.toByteArray());\n+                    }\n+\n+                    @SuppressWarnings(\"unchecked\")\n+                    T _f(int fieldIndex, IntSupplier value) {\n+                        return _f(fieldIndex, value.getAsInt());\n+                    }\n+                }\n+                \"\"\".formatted(PROTOGEN_BUILDER_CLASS));\n+    }\n+\n+    static void generateBuilderCode(String parentName, String indent, List<TreeNode> tree, PrintStream out) {\n+        for (TreeNode n : tree) {\n+            switch (n.token().type()) {\n+                case MESSAGE, FIELD -> {\n+                    out.println();\n+                    for (String c : n.comments()) out.println(indent + '\/' + c);\n+                    String name = snakeToCamelCase(n.token().matcher().group(\"name\"));\n+                    if (n.token().type() == TokenType.MESSAGE) {\n+                        out.println(indent + \"public static final class \" + name + \" extends \" + PROTOGEN_BUILDER_CLASS + \"<\" + name + \"> {\");\n+                        generateBuilderCode(name, indent + \"    \", n.nested(), out);\n+                        out.println(indent + \"}\");\n+                    } else {\n+                        String type = n.token().matcher().group(\"type\");\n+                        type = switch (type) {\n+                            case \"string\" -> \"String\";\n+                            case \"int32\" -> \"int\";\n+                            case \"int64\" -> \"long\";\n+                            case \"uint64\" -> \"long\";\n+                            case \"bytes\" -> \"byte[]\";\n+                            default -> type;\n+                        };\n+                        if (Character.isLowerCase(type.charAt(0)) && !type.equals(\"byte[]\") && n.token().matcher().group(\"flag\").equals(\"repeated \")) {\n+                            type += \"...\";\n+                        }\n+                        String index = n.token().matcher().group(\"index\");\n+                        out.println(indent + \"public \" + parentName + \" \" + name + \"(\" + type + \" \" + name + \") {return _f(\" + index + \", \" + name + \");}\");\n+                    }\n+                }\n+            }\n+        }\n+    }\n+\n+    static void generateModel(List<TreeNode> tree, PrintStream out) {\n+        out.print(COPYRIGHT_NOTICE);\n+        out.print(\"\"\"\n+                package %1$s;\n+\n+                import java.io.RandomAccessFile;\n+                import java.lang.annotation.ElementType;\n+                import java.lang.annotation.Retention;\n+                import java.lang.annotation.RetentionPolicy;\n+                import java.lang.annotation.Target;\n+                import java.lang.reflect.ParameterizedType;\n+                import java.lang.reflect.RecordComponent;\n+                import java.nio.ByteBuffer;\n+                import java.nio.ByteOrder;\n+                import java.nio.channels.FileChannel;\n+                import java.util.ArrayList;\n+                import java.util.Arrays;\n+                import java.util.List;\n+                import java.util.function.IntSupplier;\n+                import java.util.function.Supplier;\n+\n+                import %1$s.%2$s.*;\n+\n+                \/\/ Generated from onnx.in.proto\n+                public sealed interface %3$s {\n+                \"\"\".formatted(PROTOGEN_PACKAGE, PROTOGEN_CONSTANTS_CLASS, PROTOGEN_MODEL_CLASS));\n+        generateModelCode(\"    \", tree, out);\n+        out.print(\"\"\"\n+\n+                    \/\/ Implementation\n+\n+\n+                    @Retention(RetentionPolicy.RUNTIME)\n+                    @Target(ElementType.RECORD_COMPONENT)\n+                    @interface f {\n+                        int value();\n+                    }\n+\n+                    private static long decodeVarint(ByteBuffer data) {\n+                        long i, shift = 0, value = 0;\n+                        do {\n+                            value |= ((i = data.get()) & 0x7f) << shift;\n+                            shift += 7;\n+                        } while ((i & 0x80) != 0);\n+                        return value;\n+                    }\n+\n+                    private static int countVarInts(ByteBuffer data) {\n+                        long end  = decodeVarint(data);\n+                        int start = data.position();\n+                        end += start;\n+                        int count = 0;\n+                        while (data.position() < end) {\n+                            if ((data.get() & 0x80) == 0) count++;\n+                        }\n+                        data.position(start);\n+                        return count;\n+                    }\n+\n+                    private static int[] readPackedInts(ByteBuffer data) {\n+                        var ret = new int[countVarInts(data)];\n+                        for (int i = 0; i < ret.length; i++) {\n+                            ret[i] = (int)decodeVarint(data);\n+                        }\n+                        return ret;\n+                    }\n+\n+                    private static long[] readPackedLongs(ByteBuffer data) {\n+                        var ret = new long[countVarInts(data)];\n+                        for (int i = 0; i < ret.length; i++) {\n+                            ret[i] = decodeVarint(data);\n+                        }\n+                        return ret;\n+                    }\n+\n+                    private static float[] readPackedFloats(ByteBuffer data) {\n+                        var ret = new float[(int)(decodeVarint(data)\/4)];\n+                        for (int i = 0; i < ret.length; i++) {\n+                            ret[i] = data.getFloat();\n+                        }\n+                        return ret;\n+                    }\n+\n+                    private static double[] readPackedDoubles(ByteBuffer data) {\n+                        var ret = new double[(int)(decodeVarint(data)\/8)];\n+                        for (int i = 0; i < ret.length; i++) {\n+                            ret[i] = data.getDouble();\n+                        }\n+                        return ret;\n+                    }\n+\n+                    private static byte[] readBytes(ByteBuffer data) {\n+                        var bytes = new byte[(int)decodeVarint(data)];\n+                        data.get(bytes);\n+                        return bytes;\n+                    }\n+\n+                    private static Object readData(Class<?> baseType, boolean packed, ByteBuffer bb) {\n+                        if (baseType == Integer.class) {\n+                            return (int)decodeVarint(bb);\n+                        } else if (baseType == int[].class) {\n+                            return packed ? readPackedInts(bb) : new int[]{(int)decodeVarint(bb)};\n+                        } else if (baseType == Long.class) {\n+                            return decodeVarint(bb);\n+                        } else if (baseType == long[].class) {\n+                            return packed ? readPackedLongs(bb) : new long[]{decodeVarint(bb)};\n+                        } else if (baseType == Float.class) {\n+                            return bb.getFloat();\n+                        } else if (baseType == float[].class) {\n+                            return packed ? readPackedFloats(bb) : new float[] {bb.getFloat()};\n+                        } else if (baseType == Double.class) {\n+                            return bb.getDouble();\n+                        } else if (baseType == double[].class) {\n+                            return packed ? readPackedDoubles(bb) : new double[] {bb.getDouble()};\n+                        } else if (baseType == byte[].class) {\n+                            return readBytes(bb);\n+                        } else if (baseType == String.class) {\n+                            return new String(readBytes(bb));\n+                        } else if (baseType.getEnclosingClass() == %1$s.class) {\n+                            int value = (int)decodeVarint(bb);\n+                            for (Object cs : baseType.getEnumConstants()) {\n+                                if (cs instanceof IntSupplier is && is.getAsInt() == value) {\n+                                    return cs;\n+                                }\n+                            }\n+                            throw new IllegalArgumentException(baseType.toString());\n+                        } else {\n+                            var size = decodeVarint(bb);\n+                            int limit = bb.limit();\n+                            var data = readFrom((Class<Record>)baseType, bb.limit(bb.position() + (int)size));\n+                            bb.limit(limit);\n+                            return data;\n+                        }\n+                    }\n+\n+                    private static int getRecordFieldIndex(RecordComponent[] rcs, int fieldIndex) {\n+                        for (int i = 0; i < rcs.length; i++) {\n+                            if (rcs[i].getAnnotation(f.class).value() == fieldIndex) {\n+                                return i;\n+                            }\n+                        }\n+                        throw new IllegalArgumentException(\"Field index \" + fieldIndex + \" not found in \" + rcs[0].getDeclaringRecord());\n+                    }\n+\n+                    private static <T> T readFrom(Class<T> type, ByteBuffer bb) {\n+                        Object[] fieldsData = new Object[type.getRecordComponents().length];\n+                        while (bb.remaining() > 0) {\n+                            long tag = decodeVarint(bb);\n+                            RecordComponent[] rcs = type.getRecordComponents();\n+                            int rfi = getRecordFieldIndex(rcs, (int)tag >> 3);\n+                            boolean packed = (tag & 7) == 2;\n+                            RecordComponent rc = rcs[rfi];\n+                            Class<?> rcType = rc.getType();\n+                            if (rcType == List.class) {\n+                                List list;\n+                                if (fieldsData[rfi] instanceof List l) {\n+                                    list = l;\n+                                } else {\n+                                    list = new ArrayList();\n+                                    fieldsData[rfi] = list;\n+                                }\n+                                Class baseType = (Class)((ParameterizedType)rc.getGenericType()).getActualTypeArguments()[0];\n+                                list.add(readData(baseType, packed, bb));\n+                            } else {\n+                                fieldsData[rfi] = readData(rcType, packed, bb);\n+                            }\n+                        }\n+                        try {\n+                            return (T)type.getDeclaredConstructors()[0].newInstance(fieldsData);\n+                        } catch (ReflectiveOperationException e) {\n+                            throw new RuntimeException(e);\n+                        }\n+                    }\n+\n+                    private static void print(StringBuilder out, int indent, String name, Object value, boolean skipBigData) throws ReflectiveOperationException {\n+                        if (value == null) return;\n+                        out.append(\"  \".repeat(indent)).append(name);\n+                        switch (value) {\n+                            case List l -> {\n+                                out.append(name.endsWith(\"s\") ? \":\" : \"s:\").append(System.lineSeparator());\n+                                for (var el : l) print(out, indent + 1, \"- \" + (name.endsWith(\"s\") ? name.substring(0, name.length() - 1) : name), el, skipBigData);\n+                            }\n+                            case Record r -> {\n+                                out.append(':').append(System.lineSeparator());\n+                                for (var rc : r.getClass().getRecordComponents()) {\n+                                    print(out, indent + 2, rc.getName(), rc.getAccessor().invoke(r), skipBigData);\n+                                }\n+                            }\n+                            case byte[] a ->\n+                                out.append(checkSize(a.length, () -> Arrays.toString(a), skipBigData));\n+                            case long[] a ->\n+                                out.append(checkSize(a.length, () -> Arrays.toString(a), skipBigData));\n+                            case float[] a ->\n+                                out.append(checkSize(a.length, () -> Arrays.toString(a), skipBigData));\n+                            case double[] a ->\n+                                out.append(checkSize(a.length, () -> Arrays.toString(a), skipBigData));\n+                            case String s ->\n+                                out.append(\": \\\\\"\").append(s).append('\"').append(System.lineSeparator());\n+                            default ->\n+                                out.append(\": \").append(value).append(System.lineSeparator());\n+                        }\n+                    }\n+\n+                    static final int SKIP_LIMIT = 1000;\n+\n+                    private static String checkSize(int size, Supplier<String> sup, boolean skipBigData) {\n+                        return \": \" + (skipBigData && size > SKIP_LIMIT ? \"# skipped \" + size + \" values\" : sup.get()) + System.lineSeparator();\n+                    }\n+\n+                    default String toText() {\n+                        return toText(true);\n+                    }\n+\n+                    default String toText(boolean skipBigData) {\n+                        try {\n+                            var sb = new StringBuilder();\n+                            print(sb, 0, \"%2$s\", this, skipBigData);\n+                            return sb.toString();\n+                        } catch (ReflectiveOperationException e) {\n+                            throw new RuntimeException(e);\n+                        }\n+                    }\n+\n+                    public static %2$s.ModelProto readFrom(byte[] onnxProtoModel) {\n+                        return readFrom(ByteBuffer.wrap(onnxProtoModel));\n+                    }\n+\n+                    public static %2$s.ModelProto readFrom(ByteBuffer onnxProtoModel) {\n+                        return readFrom(%2$s.ModelProto.class, onnxProtoModel.order(ByteOrder.LITTLE_ENDIAN));\n+                    }\n+\n+                    public static void main(String... args) throws Exception {\n+                        for (var fName : args) {\n+                            try (var in = new RandomAccessFile(fName, \"r\")) {\n+                                %2$s.ModelProto model = readFrom(in.getChannel().map(FileChannel.MapMode.READ_ONLY, 0, in.length()));\n+                                System.out.println(model.toText());\n+                            }\n+                        }\n+                    }\n+                }\n+                \"\"\".formatted(PROTOGEN_CONSTANTS_CLASS, PROTOGEN_MODEL_CLASS));\n+    }\n+\n+    static void generateModelCode(String indent, List<TreeNode> tree, PrintStream out) {\n+        for (TreeNode n : tree) {\n+            if (n.token().type() == TokenType.MESSAGE) {\n+                out.println();\n+                for (String c : n.comments()) out.println(indent + '\/' + c);\n+                String recordName = n.token().matcher().group(\"name\");\n+                out.println(indent + \"public record \" + recordName + \" (\");\n+                boolean first = true;\n+                for (TreeNode nn : n.nested()) {\n+                    if (nn.token().type() == TokenType.FIELD) {\n+                        if (first) {\n+                            first = false;\n+                        } else {\n+                            out.println(\",\");\n+                        }\n+                        out.println();\n+                        for (String c : nn.comments()) out.println(indent + \"    \/\" + c);\n+                        String name = snakeToCamelCase(nn.token().matcher().group(\"name\"));\n+                        String type = nn.token().matcher().group(\"type\");\n+                        if (nn.token().matcher().group(\"flag\").equals(\"repeated \")) {\n+                            type = switch (type) {\n+                                case \"float\" -> \"List<float[]>\";\n+                                case \"double\" -> \"List<double[]>\";\n+                                case \"string\" -> \"List<String>\";\n+                                case \"int32\" -> \"List<int[]>\";\n+                                case \"int64\", \"uint64\" -> \"List<long[]>\";\n+                                case \"bytes\" -> \"List<byte[]>\";\n+                                default -> \"List<\" + type + \">\";\n+                            };\n+                        } else {\n+                            type = switch (type) {\n+                                case \"float\" -> \"Float\";\n+                                case \"double\" -> \"Double\";\n+                                case \"string\" -> \"String\";\n+                                case \"int32\" -> \"Integer\";\n+                                case \"int64\", \"uint64\" -> \"Long\";\n+                                case \"bytes\" -> \"byte[]\";\n+                                default -> type;\n+                            };\n+                        }\n+                        String index = nn.token().matcher().group(\"index\");\n+                        out.print(indent + \"    @f(\" + index + \") \" + type + \" \" + name);\n+                    }\n+                }\n+                out.println(\") implements \" + PROTOGEN_MODEL_CLASS + \" {\");\n+                generateModelCode(indent + \"    \", n.nested(), out);\n+                out.println(indent + \"}\");\n+            }\n+        }\n+    }\n+\n+    static List<TreeNode> toTree(Iterator<Token> tokens) {\n+        List<TreeNode> nodes = new ArrayList<>();\n+        List<String> comments = new ArrayList<>();\n+        int oneofs = 0;\n+        while (tokens.hasNext()) {\n+            Token t = tokens.next();\n+            switch (t.type()) {\n+                case COMMENT -> comments.add(t.matcher().group(\"comment\"));\n+                case EMPTY -> comments.clear(); \/\/ do not merge isolated comment blocks\n+                case ONEOF -> oneofs++; \/\/ flat ONEOF\n+                case ENUM_ELEMENT, FIELD, RESERVED, SYNTAX, PACKAGE -> {\n+                    if (t.matcher().group(\"comment\") instanceof String c) comments.add(c);\n+                    nodes.add(new TreeNode(comments, t, List.of()));\n+                    comments = new ArrayList<>();\n+                }\n+                case ENUM, MESSAGE -> {\n+                    nodes.add(new TreeNode(comments, t, toTree(tokens)));\n+                    comments = new ArrayList<>();\n+                }\n+                case END -> {\n+                    if (oneofs-- == 0) return nodes;\n+                }\n+            }\n+        }\n+        return nodes;\n+    }\n+\n+    static final Pattern SNAKE = Pattern.compile(\"_([a-z])\");\n+\n+    static String snakeToCamelCase(String name) {\n+        return SNAKE.matcher(name).replaceAll(mr -> mr.group(1).toUpperCase());\n+    }\n+\n+    public static void main(String[] args) throws Exception {\n+        List<TreeNode> tree = toTree(Files.lines(Path.of(\"opgen\/onnx.in.proto\")).map(ProtoGen::lineToToken).iterator());\n+        try (var constants = new PrintStream(new FileOutputStream(SOURCES_PATH + PROTOGEN_CONSTANTS_CLASS + \".java\"))) {\n+            generateConstants(tree, constants);\n+        }\n+        try (var builder = new PrintStream(new FileOutputStream(SOURCES_PATH + PROTOGEN_BUILDER_CLASS + \".java\"))) {\n+            generateBuilder(tree, builder);\n+        }\n+        try (var model = new PrintStream(new FileOutputStream(SOURCES_PATH + PROTOGEN_MODEL_CLASS + \".java\"))) {\n+            generateModel(tree, model);\n+        }\n+    }\n+}\n","filename":"cr-examples\/onnx\/opgen\/src\/main\/java\/oracle\/code\/onnx\/protogen\/ProtoGen.java","additions":683,"deletions":0,"binary":false,"changes":683,"status":"added"},{"patch":"@@ -28,1 +28,0 @@\n-import java.io.ByteArrayOutputStream;\n@@ -30,1 +29,0 @@\n-import java.nio.charset.StandardCharsets;\n@@ -34,1 +32,0 @@\n-import java.util.function.BiConsumer;\n@@ -48,0 +45,2 @@\n+import oracle.code.onnx.proto.OnnxBuilder.*;\n+import oracle.code.onnx.proto.OnnxConstants.*;\n@@ -49,295 +48,1 @@\n-\/\/ Generated from onnx.proto3\n-sealed class OnnxProtoBuilder<T extends OnnxProtoBuilder> {\n-\n-    static final class Attribute extends OnnxProtoBuilder<Attribute> {\n-        Attribute name(String name) {return _f(1, name);}\n-        Attribute ref_attr_name(String ref_attr_name) {return _f(21, ref_attr_name);}\n-        Attribute doc_string(String doc_string) {return _f(13, doc_string);}\n-        Attribute type(int type) {return _f(20, type);}\n-        Attribute f(float f) {return _f(2, f);}\n-        Attribute i(long i) {return _f(3, i);}\n-        Attribute s(byte[] s) {return _f(4, s);}\n-        Attribute t(TensorProto t) {return _f(5, t);}\n-        Attribute g(GraphProto g) {return _f(6, g);}\n-        Attribute sparse_tensor(SparseTensorProto sparse_tensor) {return _f(22, sparse_tensor);}\n-        Attribute tp(TypeProto tp) {return _f(14, tp);}\n-        Attribute floats(float... floats) {return _f(7, floats);}\n-        Attribute ints(long... ints) {return _f(8, ints);}\n-        Attribute strings(byte[] strings) {return _f(9, strings);}\n-        Attribute tensors(TensorProto tensors) {return _f(10, tensors);}\n-        Attribute graphs(GraphProto graphs) {return _f(11, graphs);}\n-        Attribute sparse_tensors(SparseTensorProto sparse_tensors) {return _f(23, sparse_tensors);}\n-        Attribute type_protos(TypeProto type_protos) {return _f(15, type_protos);}\n-    }\n-\n-    static final class ValueInfoProto extends OnnxProtoBuilder<ValueInfoProto> {\n-        ValueInfoProto name(String name) {return _f(1, name);}\n-        ValueInfoProto type(TypeProto type) {return _f(2, type);}\n-        ValueInfoProto doc_string(String doc_string) {return _f(3, doc_string);}\n-        ValueInfoProto metadata_props(StringStringEntryProto metadata_props) {return _f(4, metadata_props);}\n-    }\n-\n-    static final class NodeProto extends OnnxProtoBuilder<NodeProto> {\n-        NodeProto input(String input) {return _f(1, input);}\n-        NodeProto output(String output) {return _f(2, output);}\n-        NodeProto name(String name) {return _f(3, name);}\n-        NodeProto op_type(String op_type) {return _f(4, op_type);}\n-        NodeProto domain(String domain) {return _f(7, domain);}\n-        NodeProto overload(String overload) {return _f(8, overload);}\n-        NodeProto attribute(Attribute attribute) {return _f(5, attribute);}\n-        NodeProto doc_string(String doc_string) {return _f(6, doc_string);}\n-        NodeProto metadata_props(StringStringEntryProto metadata_props) {return _f(9, metadata_props);}\n-    }\n-\n-    static final class TrainingInfoProto extends OnnxProtoBuilder<TrainingInfoProto> {\n-        TrainingInfoProto initialization(GraphProto initialization) {return _f(1, initialization);}\n-        TrainingInfoProto algorithm(GraphProto algorithm) {return _f(2, algorithm);}\n-        TrainingInfoProto initialization_binding(StringStringEntryProto initialization_binding) {return _f(3, initialization_binding);}\n-        TrainingInfoProto update_binding(StringStringEntryProto update_binding) {return _f(4, update_binding);}\n-    }\n-\n-    static final class ModelProto extends OnnxProtoBuilder<ModelProto> {\n-        ModelProto ir_version(long ir_version) {return _f(1, ir_version);}\n-        ModelProto opset_import(OperatorSetIdProto opset_import) {return _f(8, opset_import);}\n-        ModelProto producer_name(String producer_name) {return _f(2, producer_name);}\n-        ModelProto producer_version(String producer_version) {return _f(3, producer_version);}\n-        ModelProto domain(String domain) {return _f(4, domain);}\n-        ModelProto model_version(long model_version) {return _f(5, model_version);}\n-        ModelProto doc_string(String doc_string) {return _f(6, doc_string);}\n-        ModelProto graph(GraphProto graph) {return _f(7, graph);}\n-        ModelProto metadata_props(StringStringEntryProto metadata_props) {return _f(14, metadata_props);}\n-        ModelProto training_info(TrainingInfoProto training_info) {return _f(20, training_info);}\n-        ModelProto functions(FunctionProto functions) {return _f(25, functions);}\n-    }\n-\n-    static final class StringStringEntryProto extends OnnxProtoBuilder<StringStringEntryProto> {\n-        StringStringEntryProto key(String key) {return _f(1, key);}\n-        StringStringEntryProto value(String value) {return _f(2, value);}\n-    }\n-\n-    static final class TensorAnnotation extends OnnxProtoBuilder<TensorAnnotation> {\n-        TensorAnnotation tensor_name(String tensor_name) {return _f(1, tensor_name);}\n-        TensorAnnotation quant_parameter_tensor_names(StringStringEntryProto quant_parameter_tensor_names) {return _f(2, quant_parameter_tensor_names);}\n-    }\n-\n-    static final class GraphProto extends OnnxProtoBuilder<GraphProto> {\n-        GraphProto node(NodeProto node) {return _f(1, node);}\n-        GraphProto name(String name) {return _f(2, name);}\n-        GraphProto initializer(TensorProto initializer) {return _f(5, initializer);}\n-        GraphProto sparse_initializer(SparseTensorProto sparse_initializer) {return _f(15, sparse_initializer);}\n-        GraphProto doc_string(String doc_string) {return _f(10, doc_string);}\n-        GraphProto input(ValueInfoProto input) {return _f(11, input);}\n-        GraphProto output(ValueInfoProto output) {return _f(12, output);}\n-        GraphProto value_info(ValueInfoProto value_info) {return _f(13, value_info);}\n-        GraphProto quantization_annotation(TensorAnnotation quantization_annotation) {return _f(14, quantization_annotation);}\n-        GraphProto metadata_props(StringStringEntryProto metadata_props) {return _f(16, metadata_props);}\n-    }\n-\n-    static final class TensorProto extends OnnxProtoBuilder<TensorProto> {\n-        TensorProto dims(long... dims) {return _f(1, dims);}\n-        TensorProto data_type(int data_type) {return _f(2, data_type);}\n-        TensorProto segment(Segment segment) {return _f(3, segment);}\n-        TensorProto float_data(float... float_data) {return _f(4, float_data);}\n-        TensorProto int32_data(int... int32_data) {return _f(5, int32_data);}\n-        TensorProto string_data(byte[] string_data) {return _f(6, string_data);}\n-        TensorProto int64_data(long... int64_data) {return _f(7, int64_data);}\n-        TensorProto name(String name) {return _f(8, name);}\n-        TensorProto doc_string(String doc_string) {return _f(12, doc_string);}\n-        TensorProto raw_data(byte[] raw_data) {return _f(9, raw_data);}\n-        TensorProto external_data(StringStringEntryProto external_data) {return _f(13, external_data);}\n-        TensorProto data_location(int data_location) {return _f(14, data_location);}\n-        TensorProto double_data(double... double_data) {return _f(10, double_data);}\n-        TensorProto uint64_data(long... uint64_data) {return _f(11, uint64_data);}\n-        TensorProto metadata_props(StringStringEntryProto metadata_props) {return _f(16, metadata_props);}\n-    }\n-\n-    static final class Segment extends OnnxProtoBuilder<Segment> {\n-        Segment begin(long begin) {return _f(1, begin);}\n-        Segment end(long end) {return _f(2, end);}\n-    }\n-\n-    static final class SparseTensorProto extends OnnxProtoBuilder<SparseTensorProto> {\n-        SparseTensorProto values(TensorProto values) {return _f(1, values);}\n-        SparseTensorProto indices(TensorProto indices) {return _f(2, indices);}\n-        SparseTensorProto dims(long... dims) {return _f(3, dims);}\n-    }\n-\n-    static final class TensorShapeProto extends OnnxProtoBuilder<TensorShapeProto> {\n-        TensorShapeProto dim(Dimension dim) {return _f(1, dim);}\n-    }\n-\n-    static final class Dimension extends OnnxProtoBuilder<Dimension> {\n-        Dimension dim_value(long dim_value) {return _f(1, dim_value);}\n-        Dimension dim_param(String dim_param) {return _f(2, dim_param);}\n-        Dimension denotation(String denotation) {return _f(3, denotation);}\n-    }\n-\n-    static final class TypeProto extends OnnxProtoBuilder<TypeProto> {\n-        TypeProto tensor_type(Tensor tensor_type) {return _f(1, tensor_type);}\n-        TypeProto sequence_type(Sequence sequence_type) {return _f(4, sequence_type);}\n-        TypeProto map_type(Map map_type) {return _f(5, map_type);}\n-        TypeProto optional_type(Optional optional_type) {return _f(9, optional_type);}\n-        TypeProto sparse_tensor_type(SparseTensor sparse_tensor_type) {return _f(8, sparse_tensor_type);}\n-        TypeProto denotation(String denotation) {return _f(6, denotation);}\n-    }\n-\n-    static final class Tensor extends OnnxProtoBuilder<Tensor> {\n-        Tensor elem_type(int elem_type) {return _f(1, elem_type);}\n-        Tensor shape(TensorShapeProto shape) {return _f(2, shape);}\n-    }\n-\n-    static final class Sequence extends OnnxProtoBuilder<Sequence> {\n-        Sequence elem_type(TypeProto elem_type) {return _f(1, elem_type);}\n-    }\n-\n-    static final class Map extends OnnxProtoBuilder<Map> {\n-        Map key_type(int key_type) {return _f(1, key_type);}\n-        Map value_type(TypeProto value_type) {return _f(2, value_type);}\n-    }\n-\n-    static final class Optional extends OnnxProtoBuilder<Optional> {\n-        Optional elem_type(TypeProto elem_type) {return _f(1, elem_type);}\n-    }\n-\n-    static final class SparseTensor extends OnnxProtoBuilder<SparseTensor> {\n-        SparseTensor elem_type(int elem_type) {return _f(1, elem_type);}\n-        SparseTensor shape(TensorShapeProto shape) {return _f(2, shape);}\n-    }\n-\n-    static final class OperatorSetIdProto extends OnnxProtoBuilder<OperatorSetIdProto> {\n-        OperatorSetIdProto domain(String domain) {return _f(1, domain);}\n-        OperatorSetIdProto version(long version) {return _f(2, version);}\n-    }\n-\n-    static final class FunctionProto extends OnnxProtoBuilder<FunctionProto> {\n-        FunctionProto name(String name) {return _f(1, name);}\n-        FunctionProto input(String input) {return _f(4, input);}\n-        FunctionProto output(String output) {return _f(5, output);}\n-        FunctionProto attribute(String attribute) {return _f(6, attribute);}\n-        FunctionProto attribute_proto(Attribute attribute_proto) {return _f(11, attribute_proto);}\n-        FunctionProto node(NodeProto node) {return _f(7, node);}\n-        FunctionProto doc_string(String doc_string) {return _f(8, doc_string);}\n-        FunctionProto opset_import(OperatorSetIdProto opset_import) {return _f(9, opset_import);}\n-        FunctionProto domain(String domain) {return _f(10, domain);}\n-        FunctionProto overload(String overload) {return _f(13, overload);}\n-        FunctionProto value_info(ValueInfoProto value_info) {return _f(12, value_info);}\n-        FunctionProto metadata_props(StringStringEntryProto metadata_props) {return _f(14, metadata_props);}\n-    }\n-\n-    final ByteArrayOutputStream buf = new ByteArrayOutputStream();\n-\n-    void _encode(long number) {\n-        for (int i = 64 - Long.numberOfLeadingZeros(number); i > 7; i -= 7) {\n-            buf.write(0x80 | (int)number & 0x7f);\n-            number >>= 7;\n-        }\n-        buf.write((int)number & 0x7f);\n-    }\n-\n-    void _encode(float value) {\n-        int bits =  Float.floatToRawIntBits(value);\n-        buf.write((byte)bits);\n-        buf.write((byte)(bits >> 8));\n-        buf.write((byte)(bits >> 16));\n-        buf.write((byte)(bits >> 24));\n-    }\n-\n-    void _encode(double value) {\n-        long bits =  Double.doubleToRawLongBits(value);\n-        buf.write((byte)bits);\n-        buf.write((byte)(bits >> 8));\n-        buf.write((byte)(bits >> 16));\n-        buf.write((byte)(bits >> 24));\n-        buf.write((byte)(bits >> 32));\n-        buf.write((byte)(bits >> 40));\n-        buf.write((byte)(bits >> 48));\n-        buf.write((byte)(bits >> 56));\n-    }\n-\n-    @SuppressWarnings(\"unchecked\")\n-    T _f(int fieldIndex, String value) {\n-        return value == null ? (T)this : _f(fieldIndex, value.getBytes(StandardCharsets.UTF_8));\n-    }\n-\n-    @SuppressWarnings(\"unchecked\")\n-    T _f(int fieldIndex, byte[] bytes) {\n-        _encode(fieldIndex << 3 | 2);\n-        _encode(bytes.length);\n-        buf.writeBytes(bytes);\n-        return (T)this;\n-    }\n-\n-    @SuppressWarnings(\"unchecked\")\n-    T _f(int fieldIndex, float value) {\n-        _encode(fieldIndex << 3 | 5);\n-        _encode(value);\n-        return (T)this;\n-    }\n-\n-    @SuppressWarnings(\"unchecked\")\n-    T _f(int fieldIndex, float... values) {\n-        if (values.length == 1) {\n-            return _f(fieldIndex, values[0]);\n-        }\n-        var b = new OnnxProtoBuilder();\n-        for (var v : values) b._encode(v);\n-        _f(fieldIndex, b);\n-        return (T)this;\n-    }\n-\n-    @SuppressWarnings(\"unchecked\")\n-    T _f(int fieldIndex, double value) {\n-        _encode(fieldIndex << 3 | 1);\n-        _encode(value);\n-        return (T)this;\n-    }\n-\n-    @SuppressWarnings(\"unchecked\")\n-    T _f(int fieldIndex, double... values) {\n-        if (values.length == 1) {\n-            return _f(fieldIndex, values[0]);\n-        }\n-        var b = new OnnxProtoBuilder();\n-        for (var v : values) b._encode(v);\n-        _f(fieldIndex, b);\n-        return (T)this;\n-    }\n-\n-    @SuppressWarnings(\"unchecked\")\n-    T _f(int fieldIndex, long value) {\n-        _encode(fieldIndex << 3);\n-        _encode(value);\n-        return (T)this;\n-    }\n-\n-    @SuppressWarnings(\"unchecked\")\n-    T _f(int fieldIndex, long... values) {\n-        if (values.length == 1) {\n-            return _f(fieldIndex, values[0]);\n-        }\n-        var b = new OnnxProtoBuilder();\n-        for (var v : values) b._encode(v);\n-        _f(fieldIndex, b);\n-        return (T)this;\n-    }\n-\n-    @SuppressWarnings(\"unchecked\")\n-    T _f(int fieldIndex, int... values) {\n-        if (values.length == 1) {\n-            return _f(fieldIndex, values[0]);\n-        }\n-        var b = new OnnxProtoBuilder();\n-        for (var v : values) b._encode(v);\n-        _f(fieldIndex, b);\n-        return (T)this;\n-    }\n-\n-    @SuppressWarnings(\"unchecked\")   T _f(int fieldIndex, OnnxProtoBuilder value) {\n-        return _f(fieldIndex, value.buf.toByteArray());\n-    }\n-\n-    @SuppressWarnings(\"unchecked\")\n-    <P> T forEach(Iterable<P> sup, BiConsumer<T, ? super P> cons) {\n-        sup.forEach(p -> cons.accept((T)this, p));\n-        return (T)this;\n-    }\n+public final class OnnxProtoBuilder {\n@@ -426,3 +131,3 @@\n-                .ir_version(IR_VERSION)\n-                .opset_import(new OperatorSetIdProto().version(OPSET_VERSION))\n-                .forEach(customImportDomains, (m, d) -> m.opset_import(new OperatorSetIdProto().domain(d)))\n+                .irVersion(IR_VERSION)\n+                .opsetImport(new OperatorSetIdProto().version(OPSET_VERSION))\n+                .forEach(customImportDomains, (m, d) -> m.opsetImport(new OperatorSetIdProto().domain(d)))\n@@ -431,1 +136,1 @@\n-                .buf.toByteArray();\n+                .getBytes();\n@@ -555,1 +260,1 @@\n-                .opset_import(new OperatorSetIdProto().version(OPSET_VERSION));\n+                .opsetImport(new OperatorSetIdProto().version(OPSET_VERSION));\n@@ -561,1 +266,1 @@\n-                .op_type(opName)\n+                .opType(opName)\n@@ -569,1 +274,1 @@\n-                .op_type(opName)\n+                .opType(opName)\n@@ -580,1 +285,1 @@\n-        var t = new Tensor().elem_type(tensorElementType);\n+        var t = new TypeProto.Tensor().elemType(tensorElementType);\n@@ -584,1 +289,1 @@\n-                .type(new TypeProto().tensor_type(t));\n+                .type(new TypeProto().tensorType(t));\n@@ -590,1 +295,1 @@\n-                .data_type(tensor.elementType().id)\n+                .dataType(tensor.elementType().id)\n@@ -592,1 +297,1 @@\n-                .raw_data(tensor.data().toArray(ValueLayout.JAVA_BYTE));\n+                .rawData(tensor.data().toArray(ValueLayout.JAVA_BYTE));\n@@ -595,2 +300,2 @@\n-    static Attribute attribute(String name, Object value) {\n-        var attr = new Attribute().name(name);\n+    static AttributeProto attribute(String name, Object value) {\n+        var attr = new AttributeProto().name(name);\n@@ -599,1 +304,1 @@\n-                attr.type(1).f(f);\n+                attr.type(AttributeType.FLOAT).f(f);\n@@ -602,1 +307,1 @@\n-                attr.type(2).i(l);\n+                attr.type(AttributeType.INT).i(l);\n@@ -605,1 +310,1 @@\n-                attr.type(5).g(g.name(name));\n+                attr.type(AttributeType.GRAPH).g(g.name(name));\n@@ -608,1 +313,1 @@\n-                attr.type(6);\n+                attr.type(AttributeType.FLOATS);\n@@ -612,1 +317,1 @@\n-                attr.type(7);\n+                attr.type(AttributeType.INTS);\n","filename":"cr-examples\/onnx\/src\/main\/java\/oracle\/code\/onnx\/OnnxProtoBuilder.java","additions":21,"deletions":316,"binary":false,"changes":337,"status":"modified"},{"patch":"@@ -52,1 +52,1 @@\n-import oracle.code.onnx.proto.OnnxProtoModel;\n+import oracle.code.onnx.proto.OnnxModel;\n@@ -131,1 +131,1 @@\n-\/\/                System.out.println(OnnxProtoModel.readFrom(protobufModel).toText());\n+\/\/                System.out.println(OnnxModel.readFrom(protobufModel).toText());\n","filename":"cr-examples\/onnx\/src\/main\/java\/oracle\/code\/onnx\/OnnxRuntime.java","additions":2,"deletions":2,"binary":false,"changes":4,"status":"modified"},{"patch":"@@ -33,0 +33,1 @@\n+import oracle.code.onnx.Tensor;\n@@ -5230,1 +5231,1 @@\n-            value(byte[].class, true, null),\n+            value(Tensor.class, true, null),\n@@ -5320,1 +5321,1 @@\n-        Constant(TypeElement resultType, java.util.Optional<Long> value_int, java.util.Optional<float[]> value_floats, java.util.Optional<String[]> value_strings, java.util.Optional<Float> value_float, java.util.Optional<String> value_string, java.util.Optional<long[]> value_ints, java.util.Optional<byte[]> sparse_value, java.util.Optional<byte[]> value) {\n+        Constant(TypeElement resultType, java.util.Optional<Long> value_int, java.util.Optional<float[]> value_floats, java.util.Optional<String[]> value_strings, java.util.Optional<Float> value_float, java.util.Optional<String> value_string, java.util.Optional<long[]> value_ints, java.util.Optional<byte[]> sparse_value, java.util.Optional<Tensor> value) {\n@@ -5369,3 +5370,3 @@\n-        public java.util.Optional<byte[]> value() {\n-            byte[] value = Attribute.value.access(byte[].class, onnxAttributes);\n-            return java.util.Optional.ofNullable(value).map(byte[]::clone);\n+        public java.util.Optional<Tensor> value() {\n+            Tensor value = Attribute.value.access(Tensor.class, onnxAttributes);\n+            return java.util.Optional.ofNullable(value);\n@@ -5376,1 +5377,1 @@\n-    public static Constant Constant(TypeElement resultType, java.util.Optional<Long> value_int, java.util.Optional<float[]> value_floats, java.util.Optional<String[]> value_strings, java.util.Optional<Float> value_float, java.util.Optional<String> value_string, java.util.Optional<long[]> value_ints, java.util.Optional<byte[]> sparse_value, java.util.Optional<byte[]> value) {\n+    public static Constant Constant(TypeElement resultType, java.util.Optional<Long> value_int, java.util.Optional<float[]> value_floats, java.util.Optional<String[]> value_strings, java.util.Optional<Float> value_float, java.util.Optional<String> value_string, java.util.Optional<long[]> value_ints, java.util.Optional<byte[]> sparse_value, java.util.Optional<Tensor> value) {\n@@ -5385,1 +5386,1 @@\n-            value(byte[].class, true, null),\n+            value(Tensor.class, true, null),\n@@ -5497,1 +5498,1 @@\n-        ConstantOfShape(TypeElement resultType, Value input, java.util.Optional<byte[]> value) {\n+        ConstantOfShape(TypeElement resultType, Value input, java.util.Optional<Tensor> value) {\n@@ -5515,3 +5516,3 @@\n-        public java.util.Optional<byte[]> value() {\n-            byte[] value = Attribute.value.access(byte[].class, onnxAttributes);\n-            return java.util.Optional.ofNullable(value).map(byte[]::clone);\n+        public java.util.Optional<Tensor> value() {\n+            Tensor value = Attribute.value.access(Tensor.class, onnxAttributes);\n+            return java.util.Optional.ofNullable(value);\n@@ -5522,1 +5523,1 @@\n-    public static ConstantOfShape ConstantOfShape(TypeElement resultType, Value input, java.util.Optional<byte[]> value) {\n+    public static ConstantOfShape ConstantOfShape(TypeElement resultType, Value input, java.util.Optional<Tensor> value) {\n@@ -13187,1 +13188,1 @@\n-            keys_tensor(byte[].class, true, null),\n+            keys_tensor(Tensor.class, true, null),\n@@ -13191,1 +13192,1 @@\n-            default_tensor(byte[].class, true, null),\n+            default_tensor(Tensor.class, true, null),\n@@ -13193,1 +13194,1 @@\n-            values_tensor(byte[].class, true, null),\n+            values_tensor(Tensor.class, true, null),\n@@ -13308,1 +13309,1 @@\n-        LabelEncoder(TypeElement resultType, Value X, java.util.Optional<String[]> values_strings, java.util.Optional<long[]> keys_int64s, java.util.Optional<byte[]> keys_tensor, java.util.Optional<String[]> keys_strings, java.util.Optional<Float> default_float, java.util.Optional<float[]> keys_floats, java.util.Optional<byte[]> default_tensor, java.util.Optional<Long> default_int64, java.util.Optional<byte[]> values_tensor, java.util.Optional<long[]> values_int64s, java.util.Optional<String> default_string, java.util.Optional<float[]> values_floats) {\n+        LabelEncoder(TypeElement resultType, Value X, java.util.Optional<String[]> values_strings, java.util.Optional<long[]> keys_int64s, java.util.Optional<Tensor> keys_tensor, java.util.Optional<String[]> keys_strings, java.util.Optional<Float> default_float, java.util.Optional<float[]> keys_floats, java.util.Optional<Tensor> default_tensor, java.util.Optional<Long> default_int64, java.util.Optional<Tensor> values_tensor, java.util.Optional<long[]> values_int64s, java.util.Optional<String> default_string, java.util.Optional<float[]> values_floats) {\n@@ -13336,3 +13337,3 @@\n-        public java.util.Optional<byte[]> keys_tensor() {\n-            byte[] keys_tensor = Attribute.keys_tensor.access(byte[].class, onnxAttributes);\n-            return java.util.Optional.ofNullable(keys_tensor).map(byte[]::clone);\n+        public java.util.Optional<Tensor> keys_tensor() {\n+            Tensor keys_tensor = Attribute.keys_tensor.access(Tensor.class, onnxAttributes);\n+            return java.util.Optional.ofNullable(keys_tensor);\n@@ -13356,3 +13357,3 @@\n-        public java.util.Optional<byte[]> default_tensor() {\n-            byte[] default_tensor = Attribute.default_tensor.access(byte[].class, onnxAttributes);\n-            return java.util.Optional.ofNullable(default_tensor).map(byte[]::clone);\n+        public java.util.Optional<Tensor> default_tensor() {\n+            Tensor default_tensor = Attribute.default_tensor.access(Tensor.class, onnxAttributes);\n+            return java.util.Optional.ofNullable(default_tensor);\n@@ -13366,3 +13367,3 @@\n-        public java.util.Optional<byte[]> values_tensor() {\n-            byte[] values_tensor = Attribute.values_tensor.access(byte[].class, onnxAttributes);\n-            return java.util.Optional.ofNullable(values_tensor).map(byte[]::clone);\n+        public java.util.Optional<Tensor> values_tensor() {\n+            Tensor values_tensor = Attribute.values_tensor.access(Tensor.class, onnxAttributes);\n+            return java.util.Optional.ofNullable(values_tensor);\n@@ -13388,1 +13389,1 @@\n-    public static LabelEncoder LabelEncoder(TypeElement resultType, Value X, java.util.Optional<String[]> values_strings, java.util.Optional<long[]> keys_int64s, java.util.Optional<byte[]> keys_tensor, java.util.Optional<String[]> keys_strings, java.util.Optional<Float> default_float, java.util.Optional<float[]> keys_floats, java.util.Optional<byte[]> default_tensor, java.util.Optional<Long> default_int64, java.util.Optional<byte[]> values_tensor, java.util.Optional<long[]> values_int64s, java.util.Optional<String> default_string, java.util.Optional<float[]> values_floats) {\n+    public static LabelEncoder LabelEncoder(TypeElement resultType, Value X, java.util.Optional<String[]> values_strings, java.util.Optional<long[]> keys_int64s, java.util.Optional<Tensor> keys_tensor, java.util.Optional<String[]> keys_strings, java.util.Optional<Float> default_float, java.util.Optional<float[]> keys_floats, java.util.Optional<Tensor> default_tensor, java.util.Optional<Long> default_int64, java.util.Optional<Tensor> values_tensor, java.util.Optional<long[]> values_int64s, java.util.Optional<String> default_string, java.util.Optional<float[]> values_floats) {\n@@ -29235,1 +29236,1 @@\n-            nodes_hitrates(byte[].class, true, null),\n+            nodes_hitrates(Tensor.class, true, null),\n@@ -29240,1 +29241,1 @@\n-            nodes_modes(byte[].class, false, null),\n+            nodes_modes(Tensor.class, false, null),\n@@ -29243,1 +29244,1 @@\n-            leaf_weights(byte[].class, false, null),\n+            leaf_weights(Tensor.class, false, null),\n@@ -29248,2 +29249,2 @@\n-            membership_values(byte[].class, true, null),\n-            nodes_splits(byte[].class, false, null),\n+            membership_values(Tensor.class, true, null),\n+            nodes_splits(Tensor.class, false, null),\n@@ -29360,1 +29361,1 @@\n-        TreeEnsemble(TypeElement resultType, Value X, java.util.Optional<Long> aggregate_function, java.util.Optional<byte[]> nodes_hitrates, long[] nodes_featureids, long[] nodes_falseleafs, java.util.Optional<Long> post_transform, long[] nodes_trueleafs, byte[] nodes_modes, long[] nodes_falsenodeids, long[] nodes_truenodeids, byte[] leaf_weights, long[] leaf_targetids, long[] tree_roots, java.util.Optional<Long> n_targets, java.util.Optional<long[]> nodes_missing_value_tracks_true, java.util.Optional<byte[]> membership_values, byte[] nodes_splits) {\n+        TreeEnsemble(TypeElement resultType, Value X, java.util.Optional<Long> aggregate_function, java.util.Optional<Tensor> nodes_hitrates, long[] nodes_featureids, long[] nodes_falseleafs, java.util.Optional<Long> post_transform, long[] nodes_trueleafs, Tensor nodes_modes, long[] nodes_falsenodeids, long[] nodes_truenodeids, Tensor leaf_weights, long[] leaf_targetids, long[] tree_roots, java.util.Optional<Long> n_targets, java.util.Optional<long[]> nodes_missing_value_tracks_true, java.util.Optional<Tensor> membership_values, Tensor nodes_splits) {\n@@ -29383,3 +29384,3 @@\n-        public java.util.Optional<byte[]> nodes_hitrates() {\n-            byte[] nodes_hitrates = Attribute.nodes_hitrates.access(byte[].class, onnxAttributes);\n-            return java.util.Optional.ofNullable(nodes_hitrates).map(byte[]::clone);\n+        public java.util.Optional<Tensor> nodes_hitrates() {\n+            Tensor nodes_hitrates = Attribute.nodes_hitrates.access(Tensor.class, onnxAttributes);\n+            return java.util.Optional.ofNullable(nodes_hitrates);\n@@ -29408,3 +29409,3 @@\n-        public byte[] nodes_modes() {\n-            byte[] nodes_modes = Attribute.nodes_modes.access(byte[].class, onnxAttributes);\n-            return nodes_modes.clone();\n+        public Tensor nodes_modes() {\n+            Tensor nodes_modes = Attribute.nodes_modes.access(Tensor.class, onnxAttributes);\n+            return nodes_modes;\n@@ -29423,3 +29424,3 @@\n-        public byte[] leaf_weights() {\n-            byte[] leaf_weights = Attribute.leaf_weights.access(byte[].class, onnxAttributes);\n-            return leaf_weights.clone();\n+        public Tensor leaf_weights() {\n+            Tensor leaf_weights = Attribute.leaf_weights.access(Tensor.class, onnxAttributes);\n+            return leaf_weights;\n@@ -29448,3 +29449,3 @@\n-        public java.util.Optional<byte[]> membership_values() {\n-            byte[] membership_values = Attribute.membership_values.access(byte[].class, onnxAttributes);\n-            return java.util.Optional.ofNullable(membership_values).map(byte[]::clone);\n+        public java.util.Optional<Tensor> membership_values() {\n+            Tensor membership_values = Attribute.membership_values.access(Tensor.class, onnxAttributes);\n+            return java.util.Optional.ofNullable(membership_values);\n@@ -29453,3 +29454,3 @@\n-        public byte[] nodes_splits() {\n-            byte[] nodes_splits = Attribute.nodes_splits.access(byte[].class, onnxAttributes);\n-            return nodes_splits.clone();\n+        public Tensor nodes_splits() {\n+            Tensor nodes_splits = Attribute.nodes_splits.access(Tensor.class, onnxAttributes);\n+            return nodes_splits;\n@@ -29460,1 +29461,1 @@\n-    public static TreeEnsemble TreeEnsemble(TypeElement resultType, Value X, java.util.Optional<Long> aggregate_function, java.util.Optional<byte[]> nodes_hitrates, long[] nodes_featureids, long[] nodes_falseleafs, java.util.Optional<Long> post_transform, long[] nodes_trueleafs, byte[] nodes_modes, long[] nodes_falsenodeids, long[] nodes_truenodeids, byte[] leaf_weights, long[] leaf_targetids, long[] tree_roots, java.util.Optional<Long> n_targets, java.util.Optional<long[]> nodes_missing_value_tracks_true, java.util.Optional<byte[]> membership_values, byte[] nodes_splits) {\n+    public static TreeEnsemble TreeEnsemble(TypeElement resultType, Value X, java.util.Optional<Long> aggregate_function, java.util.Optional<Tensor> nodes_hitrates, long[] nodes_featureids, long[] nodes_falseleafs, java.util.Optional<Long> post_transform, long[] nodes_trueleafs, Tensor nodes_modes, long[] nodes_falsenodeids, long[] nodes_truenodeids, Tensor leaf_weights, long[] leaf_targetids, long[] tree_roots, java.util.Optional<Long> n_targets, java.util.Optional<long[]> nodes_missing_value_tracks_true, java.util.Optional<Tensor> membership_values, Tensor nodes_splits) {\n@@ -29474,1 +29475,1 @@\n-            class_weights_as_tensor(byte[].class, true, null),\n+            class_weights_as_tensor(Tensor.class, true, null),\n@@ -29481,1 +29482,1 @@\n-            nodes_hitrates_as_tensor(byte[].class, true, null),\n+            nodes_hitrates_as_tensor(Tensor.class, true, null),\n@@ -29483,1 +29484,1 @@\n-            base_values_as_tensor(byte[].class, true, null),\n+            base_values_as_tensor(Tensor.class, true, null),\n@@ -29489,1 +29490,1 @@\n-            nodes_values_as_tensor(byte[].class, true, null),\n+            nodes_values_as_tensor(Tensor.class, true, null),\n@@ -29602,1 +29603,1 @@\n-        TreeEnsembleClassifier(TypeElement resultType, Value X, java.util.Optional<long[]> classlabels_int64s, java.util.Optional<long[]> class_ids, java.util.Optional<float[]> nodes_hitrates, java.util.Optional<long[]> nodes_featureids, java.util.Optional<long[]> nodes_treeids, java.util.Optional<byte[]> class_weights_as_tensor, java.util.Optional<String> post_transform, java.util.Optional<String[]> nodes_modes, java.util.Optional<long[]> nodes_falsenodeids, java.util.Optional<String[]> classlabels_strings, java.util.Optional<long[]> nodes_truenodeids, java.util.Optional<long[]> nodes_nodeids, java.util.Optional<byte[]> nodes_hitrates_as_tensor, java.util.Optional<float[]> class_weights, java.util.Optional<byte[]> base_values_as_tensor, java.util.Optional<long[]> nodes_missing_value_tracks_true, java.util.Optional<long[]> class_nodeids, java.util.Optional<long[]> class_treeids, java.util.Optional<float[]> base_values, java.util.Optional<float[]> nodes_values, java.util.Optional<byte[]> nodes_values_as_tensor) {\n+        TreeEnsembleClassifier(TypeElement resultType, Value X, java.util.Optional<long[]> classlabels_int64s, java.util.Optional<long[]> class_ids, java.util.Optional<float[]> nodes_hitrates, java.util.Optional<long[]> nodes_featureids, java.util.Optional<long[]> nodes_treeids, java.util.Optional<Tensor> class_weights_as_tensor, java.util.Optional<String> post_transform, java.util.Optional<String[]> nodes_modes, java.util.Optional<long[]> nodes_falsenodeids, java.util.Optional<String[]> classlabels_strings, java.util.Optional<long[]> nodes_truenodeids, java.util.Optional<long[]> nodes_nodeids, java.util.Optional<Tensor> nodes_hitrates_as_tensor, java.util.Optional<float[]> class_weights, java.util.Optional<Tensor> base_values_as_tensor, java.util.Optional<long[]> nodes_missing_value_tracks_true, java.util.Optional<long[]> class_nodeids, java.util.Optional<long[]> class_treeids, java.util.Optional<float[]> base_values, java.util.Optional<float[]> nodes_values, java.util.Optional<Tensor> nodes_values_as_tensor) {\n@@ -29645,3 +29646,3 @@\n-        public java.util.Optional<byte[]> class_weights_as_tensor() {\n-            byte[] class_weights_as_tensor = Attribute.class_weights_as_tensor.access(byte[].class, onnxAttributes);\n-            return java.util.Optional.ofNullable(class_weights_as_tensor).map(byte[]::clone);\n+        public java.util.Optional<Tensor> class_weights_as_tensor() {\n+            Tensor class_weights_as_tensor = Attribute.class_weights_as_tensor.access(Tensor.class, onnxAttributes);\n+            return java.util.Optional.ofNullable(class_weights_as_tensor);\n@@ -29680,3 +29681,3 @@\n-        public java.util.Optional<byte[]> nodes_hitrates_as_tensor() {\n-            byte[] nodes_hitrates_as_tensor = Attribute.nodes_hitrates_as_tensor.access(byte[].class, onnxAttributes);\n-            return java.util.Optional.ofNullable(nodes_hitrates_as_tensor).map(byte[]::clone);\n+        public java.util.Optional<Tensor> nodes_hitrates_as_tensor() {\n+            Tensor nodes_hitrates_as_tensor = Attribute.nodes_hitrates_as_tensor.access(Tensor.class, onnxAttributes);\n+            return java.util.Optional.ofNullable(nodes_hitrates_as_tensor);\n@@ -29690,3 +29691,3 @@\n-        public java.util.Optional<byte[]> base_values_as_tensor() {\n-            byte[] base_values_as_tensor = Attribute.base_values_as_tensor.access(byte[].class, onnxAttributes);\n-            return java.util.Optional.ofNullable(base_values_as_tensor).map(byte[]::clone);\n+        public java.util.Optional<Tensor> base_values_as_tensor() {\n+            Tensor base_values_as_tensor = Attribute.base_values_as_tensor.access(Tensor.class, onnxAttributes);\n+            return java.util.Optional.ofNullable(base_values_as_tensor);\n@@ -29720,3 +29721,3 @@\n-        public java.util.Optional<byte[]> nodes_values_as_tensor() {\n-            byte[] nodes_values_as_tensor = Attribute.nodes_values_as_tensor.access(byte[].class, onnxAttributes);\n-            return java.util.Optional.ofNullable(nodes_values_as_tensor).map(byte[]::clone);\n+        public java.util.Optional<Tensor> nodes_values_as_tensor() {\n+            Tensor nodes_values_as_tensor = Attribute.nodes_values_as_tensor.access(Tensor.class, onnxAttributes);\n+            return java.util.Optional.ofNullable(nodes_values_as_tensor);\n@@ -29727,1 +29728,1 @@\n-    public static TreeEnsembleClassifier TreeEnsembleClassifier(TypeElement resultType, Value X, java.util.Optional<long[]> classlabels_int64s, java.util.Optional<long[]> class_ids, java.util.Optional<float[]> nodes_hitrates, java.util.Optional<long[]> nodes_featureids, java.util.Optional<long[]> nodes_treeids, java.util.Optional<byte[]> class_weights_as_tensor, java.util.Optional<String> post_transform, java.util.Optional<String[]> nodes_modes, java.util.Optional<long[]> nodes_falsenodeids, java.util.Optional<String[]> classlabels_strings, java.util.Optional<long[]> nodes_truenodeids, java.util.Optional<long[]> nodes_nodeids, java.util.Optional<byte[]> nodes_hitrates_as_tensor, java.util.Optional<float[]> class_weights, java.util.Optional<byte[]> base_values_as_tensor, java.util.Optional<long[]> nodes_missing_value_tracks_true, java.util.Optional<long[]> class_nodeids, java.util.Optional<long[]> class_treeids, java.util.Optional<float[]> base_values, java.util.Optional<float[]> nodes_values, java.util.Optional<byte[]> nodes_values_as_tensor) {\n+    public static TreeEnsembleClassifier TreeEnsembleClassifier(TypeElement resultType, Value X, java.util.Optional<long[]> classlabels_int64s, java.util.Optional<long[]> class_ids, java.util.Optional<float[]> nodes_hitrates, java.util.Optional<long[]> nodes_featureids, java.util.Optional<long[]> nodes_treeids, java.util.Optional<Tensor> class_weights_as_tensor, java.util.Optional<String> post_transform, java.util.Optional<String[]> nodes_modes, java.util.Optional<long[]> nodes_falsenodeids, java.util.Optional<String[]> classlabels_strings, java.util.Optional<long[]> nodes_truenodeids, java.util.Optional<long[]> nodes_nodeids, java.util.Optional<Tensor> nodes_hitrates_as_tensor, java.util.Optional<float[]> class_weights, java.util.Optional<Tensor> base_values_as_tensor, java.util.Optional<long[]> nodes_missing_value_tracks_true, java.util.Optional<long[]> class_nodeids, java.util.Optional<long[]> class_treeids, java.util.Optional<float[]> base_values, java.util.Optional<float[]> nodes_values, java.util.Optional<Tensor> nodes_values_as_tensor) {\n@@ -29738,1 +29739,1 @@\n-            target_weights_as_tensor(byte[].class, true, null),\n+            target_weights_as_tensor(Tensor.class, true, null),\n@@ -29750,2 +29751,2 @@\n-            nodes_hitrates_as_tensor(byte[].class, true, null),\n-            base_values_as_tensor(byte[].class, true, null),\n+            nodes_hitrates_as_tensor(Tensor.class, true, null),\n+            base_values_as_tensor(Tensor.class, true, null),\n@@ -29756,1 +29757,1 @@\n-            nodes_values_as_tensor(byte[].class, true, null),\n+            nodes_values_as_tensor(Tensor.class, true, null),\n@@ -29867,1 +29868,1 @@\n-        TreeEnsembleRegressor(TypeElement resultType, Value X, java.util.Optional<String> aggregate_function, java.util.Optional<float[]> nodes_hitrates, java.util.Optional<byte[]> target_weights_as_tensor, java.util.Optional<long[]> nodes_featureids, java.util.Optional<long[]> target_treeids, java.util.Optional<long[]> nodes_treeids, java.util.Optional<String> post_transform, java.util.Optional<String[]> nodes_modes, java.util.Optional<float[]> target_weights, java.util.Optional<long[]> nodes_falsenodeids, java.util.Optional<long[]> target_ids, java.util.Optional<long[]> nodes_truenodeids, java.util.Optional<long[]> target_nodeids, java.util.Optional<long[]> nodes_nodeids, java.util.Optional<byte[]> nodes_hitrates_as_tensor, java.util.Optional<byte[]> base_values_as_tensor, java.util.Optional<Long> n_targets, java.util.Optional<long[]> nodes_missing_value_tracks_true, java.util.Optional<float[]> base_values, java.util.Optional<float[]> nodes_values, java.util.Optional<byte[]> nodes_values_as_tensor) {\n+        TreeEnsembleRegressor(TypeElement resultType, Value X, java.util.Optional<String> aggregate_function, java.util.Optional<float[]> nodes_hitrates, java.util.Optional<Tensor> target_weights_as_tensor, java.util.Optional<long[]> nodes_featureids, java.util.Optional<long[]> target_treeids, java.util.Optional<long[]> nodes_treeids, java.util.Optional<String> post_transform, java.util.Optional<String[]> nodes_modes, java.util.Optional<float[]> target_weights, java.util.Optional<long[]> nodes_falsenodeids, java.util.Optional<long[]> target_ids, java.util.Optional<long[]> nodes_truenodeids, java.util.Optional<long[]> target_nodeids, java.util.Optional<long[]> nodes_nodeids, java.util.Optional<Tensor> nodes_hitrates_as_tensor, java.util.Optional<Tensor> base_values_as_tensor, java.util.Optional<Long> n_targets, java.util.Optional<long[]> nodes_missing_value_tracks_true, java.util.Optional<float[]> base_values, java.util.Optional<float[]> nodes_values, java.util.Optional<Tensor> nodes_values_as_tensor) {\n@@ -29895,3 +29896,3 @@\n-        public java.util.Optional<byte[]> target_weights_as_tensor() {\n-            byte[] target_weights_as_tensor = Attribute.target_weights_as_tensor.access(byte[].class, onnxAttributes);\n-            return java.util.Optional.ofNullable(target_weights_as_tensor).map(byte[]::clone);\n+        public java.util.Optional<Tensor> target_weights_as_tensor() {\n+            Tensor target_weights_as_tensor = Attribute.target_weights_as_tensor.access(Tensor.class, onnxAttributes);\n+            return java.util.Optional.ofNullable(target_weights_as_tensor);\n@@ -29955,3 +29956,3 @@\n-        public java.util.Optional<byte[]> nodes_hitrates_as_tensor() {\n-            byte[] nodes_hitrates_as_tensor = Attribute.nodes_hitrates_as_tensor.access(byte[].class, onnxAttributes);\n-            return java.util.Optional.ofNullable(nodes_hitrates_as_tensor).map(byte[]::clone);\n+        public java.util.Optional<Tensor> nodes_hitrates_as_tensor() {\n+            Tensor nodes_hitrates_as_tensor = Attribute.nodes_hitrates_as_tensor.access(Tensor.class, onnxAttributes);\n+            return java.util.Optional.ofNullable(nodes_hitrates_as_tensor);\n@@ -29960,3 +29961,3 @@\n-        public java.util.Optional<byte[]> base_values_as_tensor() {\n-            byte[] base_values_as_tensor = Attribute.base_values_as_tensor.access(byte[].class, onnxAttributes);\n-            return java.util.Optional.ofNullable(base_values_as_tensor).map(byte[]::clone);\n+        public java.util.Optional<Tensor> base_values_as_tensor() {\n+            Tensor base_values_as_tensor = Attribute.base_values_as_tensor.access(Tensor.class, onnxAttributes);\n+            return java.util.Optional.ofNullable(base_values_as_tensor);\n@@ -29985,3 +29986,3 @@\n-        public java.util.Optional<byte[]> nodes_values_as_tensor() {\n-            byte[] nodes_values_as_tensor = Attribute.nodes_values_as_tensor.access(byte[].class, onnxAttributes);\n-            return java.util.Optional.ofNullable(nodes_values_as_tensor).map(byte[]::clone);\n+        public java.util.Optional<Tensor> nodes_values_as_tensor() {\n+            Tensor nodes_values_as_tensor = Attribute.nodes_values_as_tensor.access(Tensor.class, onnxAttributes);\n+            return java.util.Optional.ofNullable(nodes_values_as_tensor);\n@@ -29992,1 +29993,1 @@\n-    public static TreeEnsembleRegressor TreeEnsembleRegressor(TypeElement resultType, Value X, java.util.Optional<String> aggregate_function, java.util.Optional<float[]> nodes_hitrates, java.util.Optional<byte[]> target_weights_as_tensor, java.util.Optional<long[]> nodes_featureids, java.util.Optional<long[]> target_treeids, java.util.Optional<long[]> nodes_treeids, java.util.Optional<String> post_transform, java.util.Optional<String[]> nodes_modes, java.util.Optional<float[]> target_weights, java.util.Optional<long[]> nodes_falsenodeids, java.util.Optional<long[]> target_ids, java.util.Optional<long[]> nodes_truenodeids, java.util.Optional<long[]> target_nodeids, java.util.Optional<long[]> nodes_nodeids, java.util.Optional<byte[]> nodes_hitrates_as_tensor, java.util.Optional<byte[]> base_values_as_tensor, java.util.Optional<Long> n_targets, java.util.Optional<long[]> nodes_missing_value_tracks_true, java.util.Optional<float[]> base_values, java.util.Optional<float[]> nodes_values, java.util.Optional<byte[]> nodes_values_as_tensor) {\n+    public static TreeEnsembleRegressor TreeEnsembleRegressor(TypeElement resultType, Value X, java.util.Optional<String> aggregate_function, java.util.Optional<float[]> nodes_hitrates, java.util.Optional<Tensor> target_weights_as_tensor, java.util.Optional<long[]> nodes_featureids, java.util.Optional<long[]> target_treeids, java.util.Optional<long[]> nodes_treeids, java.util.Optional<String> post_transform, java.util.Optional<String[]> nodes_modes, java.util.Optional<float[]> target_weights, java.util.Optional<long[]> nodes_falsenodeids, java.util.Optional<long[]> target_ids, java.util.Optional<long[]> nodes_truenodeids, java.util.Optional<long[]> target_nodeids, java.util.Optional<long[]> nodes_nodeids, java.util.Optional<Tensor> nodes_hitrates_as_tensor, java.util.Optional<Tensor> base_values_as_tensor, java.util.Optional<Long> n_targets, java.util.Optional<long[]> nodes_missing_value_tracks_true, java.util.Optional<float[]> base_values, java.util.Optional<float[]> nodes_values, java.util.Optional<Tensor> nodes_values_as_tensor) {\n","filename":"cr-examples\/onnx\/src\/main\/java\/oracle\/code\/onnx\/ir\/OnnxOps.java","additions":85,"deletions":84,"binary":false,"changes":169,"status":"modified"},{"patch":"@@ -0,0 +1,885 @@\n+\/*\n+ * Copyright (c) 2025, Oracle and\/or its affiliates. All rights reserved.\n+ * DO NOT ALTER OR REMOVE COPYRIGHT NOTICES OR THIS FILE HEADER.\n+ *\n+ * This code is free software; you can redistribute it and\/or modify it\n+ * under the terms of the GNU General Public License version 2 only, as\n+ * published by the Free Software Foundation.  Oracle designates this\n+ * particular file as subject to the \"Classpath\" exception as provided\n+ * by Oracle in the LICENSE file that accompanied this code.\n+ *\n+ * This code is distributed in the hope that it will be useful, but WITHOUT\n+ * ANY WARRANTY; without even the implied warranty of MERCHANTABILITY or\n+ * FITNESS FOR A PARTICULAR PURPOSE.  See the GNU General Public License\n+ * version 2 for more details (a copy is included in the LICENSE file that\n+ * accompanied this code).\n+ *\n+ * You should have received a copy of the GNU General Public License version\n+ * 2 along with this work; if not, write to the Free Software Foundation,\n+ * Inc., 51 Franklin St, Fifth Floor, Boston, MA 02110-1301 USA.\n+ *\n+ * Please contact Oracle, 500 Oracle Parkway, Redwood Shores, CA 94065 USA\n+ * or visit www.oracle.com if you need additional information or have any\n+ * questions.\n+ *\/\n+\n+package oracle.code.onnx.proto;\n+\n+import java.io.ByteArrayOutputStream;\n+import java.nio.charset.StandardCharsets;\n+import java.util.List;\n+import java.util.function.BiConsumer;\n+import java.util.function.IntSupplier;\n+\n+import oracle.code.onnx.proto.OnnxConstants.*;\n+\n+\/\/ Generated from onnx.in.proto\n+public sealed class OnnxBuilder<T extends OnnxBuilder> {\n+\n+    \/\/\/ Attributes\n+    \/\/\/\n+    \/\/\/ A named attribute containing either singular float, integer, string, graph,\n+    \/\/\/ and tensor values, or repeated float, integer, string, graph, and tensor values.\n+    \/\/\/ An AttributeProto MUST contain the name field, and *only one* of the\n+    \/\/\/ following content fields, effectively enforcing a C\/C++ union equivalent.\n+    public static final class AttributeProto extends OnnxBuilder<AttributeProto> {\n+\n+        \/\/\/ The name field MUST be present for this version of the IR.\n+        \/\/\/ namespace Attribute\n+        public AttributeProto name(String name) {return _f(1, name);}\n+\n+        \/\/\/ if ref_attr_name is not empty, ref_attr_name is the attribute name in parent function.\n+        \/\/\/ In this case, this AttributeProto does not contain data, and it's a reference of attribute\n+        \/\/\/ in parent scope.\n+        \/\/\/ NOTE: This should ONLY be used in function (sub-graph). It's invalid to be used in main graph.\n+        public AttributeProto refAttrName(String refAttrName) {return _f(21, refAttrName);}\n+\n+        \/\/\/ A human-readable documentation for this attribute. Markdown is allowed.\n+        public AttributeProto docString(String docString) {return _f(13, docString);}\n+\n+        \/\/\/ The type field MUST be present for this version of the IR.\n+        \/\/\/ For 0.0.1 versions of the IR, this field was not defined, and\n+        \/\/\/ implementations needed to use has_field heuristics to determine\n+        \/\/\/ which value field was in use.  For IR_VERSION 0.0.2 or later, this\n+        \/\/\/ field MUST be set and match the f|i|s|t|... field in use.  This\n+        \/\/\/ change was made to accommodate proto3 implementations.\n+        \/\/\/ discriminator that indicates which field below is in use\n+        public AttributeProto type(AttributeType type) {return _f(20, type);}\n+\n+        \/\/\/ Exactly ONE of the following fields must be present for this version of the IR\n+        \/\/\/ float\n+        public AttributeProto f(float f) {return _f(2, f);}\n+\n+        \/\/\/ int\n+        public AttributeProto i(long i) {return _f(3, i);}\n+\n+        \/\/\/ UTF-8 string\n+        public AttributeProto s(byte[] s) {return _f(4, s);}\n+\n+        \/\/\/ tensor value\n+        public AttributeProto t(TensorProto t) {return _f(5, t);}\n+\n+        \/\/\/ graph\n+        public AttributeProto g(GraphProto g) {return _f(6, g);}\n+\n+        \/\/\/ sparse tensor value\n+        public AttributeProto sparseTensor(SparseTensorProto sparseTensor) {return _f(22, sparseTensor);}\n+\n+        \/\/\/ Do not use field below, it's deprecated.\n+        \/\/\/ optional ValueProto v = 12;         \/\/ value - subsumes everything but graph\n+        \/\/\/ type proto\n+        public AttributeProto tp(TypeProto tp) {return _f(14, tp);}\n+\n+        \/\/\/ list of floats\n+        public AttributeProto floats(float... floats) {return _f(7, floats);}\n+\n+        \/\/\/ list of ints\n+        public AttributeProto ints(long... ints) {return _f(8, ints);}\n+\n+        \/\/\/ list of UTF-8 strings\n+        public AttributeProto strings(byte[] strings) {return _f(9, strings);}\n+\n+        \/\/\/ list of tensors\n+        public AttributeProto tensors(TensorProto tensors) {return _f(10, tensors);}\n+\n+        \/\/\/ list of graph\n+        public AttributeProto graphs(GraphProto graphs) {return _f(11, graphs);}\n+\n+        \/\/\/ list of sparse tensors\n+        public AttributeProto sparseTensors(SparseTensorProto sparseTensors) {return _f(23, sparseTensors);}\n+\n+        \/\/\/ list of type protos\n+        public AttributeProto typeProtos(TypeProto typeProtos) {return _f(15, typeProtos);}\n+    }\n+\n+    \/\/\/ Defines information on value, including the name, the type, and\n+    \/\/\/ the shape of the value.\n+    public static final class ValueInfoProto extends OnnxBuilder<ValueInfoProto> {\n+\n+        \/\/\/ This field MUST be present in this version of the IR.\n+        \/\/\/ namespace Value\n+        public ValueInfoProto name(String name) {return _f(1, name);}\n+\n+        \/\/\/ This field MUST be present in this version of the IR for\n+        \/\/\/ inputs and outputs of the top-level graph.\n+        public ValueInfoProto type(TypeProto type) {return _f(2, type);}\n+\n+        \/\/\/ A human-readable documentation for this value. Markdown is allowed.\n+        public ValueInfoProto docString(String docString) {return _f(3, docString);}\n+\n+        \/\/\/ Named metadata values; keys should be distinct.\n+        public ValueInfoProto metadataProps(StringStringEntryProto metadataProps) {return _f(4, metadataProps);}\n+    }\n+\n+    \/\/\/ Nodes\n+    \/\/\/\n+    \/\/\/ Computation graphs are made up of a DAG of nodes, which represent what is\n+    \/\/\/ commonly called a \"layer\" or \"pipeline stage\" in machine learning frameworks.\n+    \/\/\/\n+    \/\/\/ For example, it can be a node of type \"Conv\" that takes in an image, a filter\n+    \/\/\/ tensor and a bias tensor, and produces the convolved output.\n+    public static final class NodeProto extends OnnxBuilder<NodeProto> {\n+\n+        \/\/\/ namespace Value\n+        public NodeProto input(String input) {return _f(1, input);}\n+\n+        \/\/\/ namespace Value\n+        public NodeProto output(String output) {return _f(2, output);}\n+\n+        \/\/\/ An optional identifier for this node in a graph.\n+        \/\/\/ This field MAY be absent in this version of the IR.\n+        \/\/\/ namespace Node\n+        public NodeProto name(String name) {return _f(3, name);}\n+\n+        \/\/\/ The symbolic identifier of the Operator to execute.\n+        \/\/\/ namespace Operator\n+        public NodeProto opType(String opType) {return _f(4, opType);}\n+\n+        \/\/\/ The domain of the OperatorSet that specifies the operator named by op_type.\n+        \/\/\/ namespace Domain\n+        public NodeProto domain(String domain) {return _f(7, domain);}\n+\n+        \/\/\/ Overload identifier, used only to map this to a model-local function.\n+        public NodeProto overload(String overload) {return _f(8, overload);}\n+\n+        \/\/\/ Additional named attributes.\n+        public NodeProto attribute(AttributeProto attribute) {return _f(5, attribute);}\n+\n+        \/\/\/ A human-readable documentation for this node. Markdown is allowed.\n+        public NodeProto docString(String docString) {return _f(6, docString);}\n+\n+        \/\/\/ Named metadata values; keys should be distinct.\n+        public NodeProto metadataProps(StringStringEntryProto metadataProps) {return _f(9, metadataProps);}\n+    }\n+\n+    \/\/\/ Training information\n+    \/\/\/ TrainingInfoProto stores information for training a model.\n+    \/\/\/ In particular, this defines two functionalities: an initialization-step\n+    \/\/\/ and a training-algorithm-step. Initialization resets the model\n+    \/\/\/ back to its original state as if no training has been performed.\n+    \/\/\/ Training algorithm improves the model based on input data.\n+    \/\/\/\n+    \/\/\/ The semantics of the initialization-step is that the initializers\n+    \/\/\/ in ModelProto.graph and in TrainingInfoProto.algorithm are first\n+    \/\/\/ initialized as specified by the initializers in the graph, and then\n+    \/\/\/ updated by the \"initialization_binding\" in every instance in\n+    \/\/\/ ModelProto.training_info.\n+    \/\/\/\n+    \/\/\/ The field \"algorithm\" defines a computation graph which represents a\n+    \/\/\/ training algorithm's step. After the execution of a\n+    \/\/\/ TrainingInfoProto.algorithm, the initializers specified by \"update_binding\"\n+    \/\/\/ may be immediately updated. If the targeted training algorithm contains\n+    \/\/\/ consecutive update steps (such as block coordinate descent methods),\n+    \/\/\/ the user needs to create a TrainingInfoProto for each step.\n+    public static final class TrainingInfoProto extends OnnxBuilder<TrainingInfoProto> {\n+\n+        \/\/\/ This field describes a graph to compute the initial tensors\n+        \/\/\/ upon starting the training process. Initialization graph has no input\n+        \/\/\/ and can have multiple outputs. Usually, trainable tensors in neural\n+        \/\/\/ networks are randomly initialized. To achieve that, for each tensor,\n+        \/\/\/ the user can put a random number operator such as RandomNormal or\n+        \/\/\/ RandomUniform in TrainingInfoProto.initialization.node and assign its\n+        \/\/\/ random output to the specific tensor using \"initialization_binding\".\n+        \/\/\/ This graph can also set the initializers in \"algorithm\" in the same\n+        \/\/\/ TrainingInfoProto; a use case is resetting the number of training\n+        \/\/\/ iteration to zero.\n+        \/\/\/\n+        \/\/\/ By default, this field is an empty graph and its evaluation does not\n+        \/\/\/ produce any output. Thus, no initializer would be changed by default.\n+        public TrainingInfoProto initialization(GraphProto initialization) {return _f(1, initialization);}\n+\n+        \/\/\/ This field represents a training algorithm step. Given required inputs,\n+        \/\/\/ it computes outputs to update initializers in its own or inference graph's\n+        \/\/\/ initializer lists. In general, this field contains loss node, gradient node,\n+        \/\/\/ optimizer node, increment of iteration count.\n+        \/\/\/\n+        \/\/\/ An execution of the training algorithm step is performed by executing the\n+        \/\/\/ graph obtained by combining the inference graph (namely \"ModelProto.graph\")\n+        \/\/\/ and the \"algorithm\" graph. That is, the actual\n+        \/\/\/ input\/initializer\/output\/node\/value_info\/sparse_initializer list of\n+        \/\/\/ the training graph is the concatenation of\n+        \/\/\/ \"ModelProto.graph.input\/initializer\/output\/node\/value_info\/sparse_initializer\"\n+        \/\/\/ and \"algorithm.input\/initializer\/output\/node\/value_info\/sparse_initializer\"\n+        \/\/\/ in that order. This combined graph must satisfy the normal ONNX conditions.\n+        \/\/\/ Now, let's provide a visualization of graph combination for clarity.\n+        \/\/\/ Let the inference graph (i.e., \"ModelProto.graph\") be\n+        \/\/\/    tensor_a, tensor_b -> MatMul -> tensor_c -> Sigmoid -> tensor_d\n+        \/\/\/ and the \"algorithm\" graph be\n+        \/\/\/    tensor_d -> Add -> tensor_e\n+        \/\/\/ The combination process results\n+        \/\/\/    tensor_a, tensor_b -> MatMul -> tensor_c -> Sigmoid -> tensor_d -> Add -> tensor_e\n+        \/\/\/\n+        \/\/\/ Notice that an input of a node in the \"algorithm\" graph may reference the\n+        \/\/\/ output of a node in the inference graph (but not the other way round). Also, inference\n+        \/\/\/ node cannot reference inputs of \"algorithm\". With these restrictions, inference graph\n+        \/\/\/ can always be run independently without training information.\n+        \/\/\/\n+        \/\/\/ By default, this field is an empty graph and its evaluation does not\n+        \/\/\/ produce any output. Evaluating the default training step never\n+        \/\/\/ update any initializers.\n+        public TrainingInfoProto algorithm(GraphProto algorithm) {return _f(2, algorithm);}\n+\n+        \/\/\/ This field specifies the bindings from the outputs of \"initialization\" to\n+        \/\/\/ some initializers in \"ModelProto.graph.initializer\" and\n+        \/\/\/ the \"algorithm.initializer\" in the same TrainingInfoProto.\n+        \/\/\/ See \"update_binding\" below for details.\n+        \/\/\/\n+        \/\/\/ By default, this field is empty and no initializer would be changed\n+        \/\/\/ by the execution of \"initialization\".\n+        public TrainingInfoProto initializationBinding(StringStringEntryProto initializationBinding) {return _f(3, initializationBinding);}\n+\n+        \/\/\/ Gradient-based training is usually an iterative procedure. In one gradient\n+        \/\/\/ descent iteration, we apply\n+        \/\/\/\n+        \/\/\/ x = x - r * g\n+        \/\/\/\n+        \/\/\/ where \"x\" is the optimized tensor, \"r\" stands for learning rate, and \"g\" is\n+        \/\/\/ gradient of \"x\" with respect to a chosen loss. To avoid adding assignments\n+        \/\/\/ into the training graph, we split the update equation into\n+        \/\/\/\n+        \/\/\/ y = x - r * g\n+        \/\/\/ x = y\n+        \/\/\/\n+        \/\/\/ The user needs to save \"y = x - r * g\" into TrainingInfoProto.algorithm. To\n+        \/\/\/ tell that \"y\" should be assigned to \"x\", the field \"update_binding\" may\n+        \/\/\/ contain a key-value pair of strings, \"x\" (key of StringStringEntryProto)\n+        \/\/\/ and \"y\" (value of StringStringEntryProto).\n+        \/\/\/ For a neural network with multiple trainable (mutable) tensors, there can\n+        \/\/\/ be multiple key-value pairs in \"update_binding\".\n+        \/\/\/\n+        \/\/\/ The initializers appears as keys in \"update_binding\" are considered\n+        \/\/\/ mutable variables. This implies some behaviors\n+        \/\/\/ as described below.\n+        \/\/\/\n+        \/\/\/  1. We have only unique keys in all \"update_binding\"s so that two\n+        \/\/\/     variables may not have the same name. This ensures that one\n+        \/\/\/     variable is assigned up to once.\n+        \/\/\/  2. The keys must appear in names of \"ModelProto.graph.initializer\" or\n+        \/\/\/     \"TrainingInfoProto.algorithm.initializer\".\n+        \/\/\/  3. The values must be output names of \"algorithm\" or \"ModelProto.graph.output\".\n+        \/\/\/  4. Mutable variables are initialized to the value specified by the\n+        \/\/\/     corresponding initializer, and then potentially updated by\n+        \/\/\/     \"initializer_binding\"s and \"update_binding\"s in \"TrainingInfoProto\"s.\n+        \/\/\/\n+        \/\/\/ This field usually contains names of trainable tensors\n+        \/\/\/ (in ModelProto.graph), optimizer states such as momentums in advanced\n+        \/\/\/ stochastic gradient methods (in TrainingInfoProto.graph),\n+        \/\/\/ and number of training iterations (in TrainingInfoProto.graph).\n+        \/\/\/\n+        \/\/\/ By default, this field is empty and no initializer would be changed\n+        \/\/\/ by the execution of \"algorithm\".\n+        public TrainingInfoProto updateBinding(StringStringEntryProto updateBinding) {return _f(4, updateBinding);}\n+    }\n+\n+    \/\/\/ Models\n+    \/\/\/\n+    \/\/\/ ModelProto is a top-level file\/container format for bundling a ML model and\n+    \/\/\/ associating its computation graph with metadata.\n+    \/\/\/\n+    \/\/\/ The semantics of the model are described by the associated GraphProto's.\n+    public static final class ModelProto extends OnnxBuilder<ModelProto> {\n+\n+        \/\/\/ The version of the IR this model targets. See Version enum above.\n+        \/\/\/ This field MUST be present.\n+        public ModelProto irVersion(long irVersion) {return _f(1, irVersion);}\n+\n+        \/\/\/ The OperatorSets this model relies on.\n+        \/\/\/ All ModelProtos MUST have at least one entry that\n+        \/\/\/ specifies which version of the ONNX OperatorSet is\n+        \/\/\/ being imported.\n+        \/\/\/\n+        \/\/\/ All nodes in the ModelProto's graph will bind against the operator\n+        \/\/\/ with the same-domain\/same-op_type operator with the HIGHEST version\n+        \/\/\/ in the referenced operator sets.\n+        public ModelProto opsetImport(OperatorSetIdProto opsetImport) {return _f(8, opsetImport);}\n+\n+        \/\/\/ The name of the framework or tool used to generate this model.\n+        \/\/\/ This field SHOULD be present to indicate which implementation\/tool\/framework\n+        \/\/\/ emitted the model.\n+        public ModelProto producerName(String producerName) {return _f(2, producerName);}\n+\n+        \/\/\/ The version of the framework or tool used to generate this model.\n+        \/\/\/ This field SHOULD be present to indicate which implementation\/tool\/framework\n+        \/\/\/ emitted the model.\n+        public ModelProto producerVersion(String producerVersion) {return _f(3, producerVersion);}\n+\n+        \/\/\/ Domain name of the model.\n+        \/\/\/ We use reverse domain names as name space indicators. For example:\n+        \/\/\/ `com.facebook.fair` or `com.microsoft.cognitiveservices`\n+        \/\/\/\n+        \/\/\/ Together with `model_version` and GraphProto.name, this forms the unique identity of\n+        \/\/\/ the graph.\n+        public ModelProto domain(String domain) {return _f(4, domain);}\n+\n+        \/\/\/ The version of the graph encoded. See Version enum below.\n+        public ModelProto modelVersion(long modelVersion) {return _f(5, modelVersion);}\n+\n+        \/\/\/ A human-readable documentation for this model. Markdown is allowed.\n+        public ModelProto docString(String docString) {return _f(6, docString);}\n+\n+        \/\/\/ The parameterized graph that is evaluated to execute the model.\n+        public ModelProto graph(GraphProto graph) {return _f(7, graph);}\n+\n+        \/\/\/ Named metadata values; keys should be distinct.\n+        public ModelProto metadataProps(StringStringEntryProto metadataProps) {return _f(14, metadataProps);}\n+\n+        \/\/\/ Training-specific information. Sequentially executing all stored\n+        \/\/\/ `TrainingInfoProto.algorithm`s and assigning their outputs following\n+        \/\/\/ the corresponding `TrainingInfoProto.update_binding`s is one training\n+        \/\/\/ iteration. Similarly, to initialize the model\n+        \/\/\/ (as if training hasn't happened), the user should sequentially execute\n+        \/\/\/ all stored `TrainingInfoProto.initialization`s and assigns their outputs\n+        \/\/\/ using `TrainingInfoProto.initialization_binding`s.\n+        \/\/\/\n+        \/\/\/ If this field is empty, the training behavior of the model is undefined.\n+        public ModelProto trainingInfo(TrainingInfoProto trainingInfo) {return _f(20, trainingInfo);}\n+\n+        \/\/\/ A list of function protos local to the model.\n+        \/\/\/\n+        \/\/\/ The (domain, name, overload) tuple must be unique across the function protos in this list.\n+        \/\/\/ In case of any conflicts the behavior (whether the model local functions are given higher priority,\n+        \/\/\/ or standard operator sets are given higher priotity or this is treated as error) is defined by\n+        \/\/\/ the runtimes.\n+        \/\/\/\n+        \/\/\/ The operator sets imported by FunctionProto should be compatible with the ones\n+        \/\/\/ imported by ModelProto and other model local FunctionProtos.\n+        \/\/\/ Example, if same operator set say 'A' is imported by a FunctionProto and ModelProto\n+        \/\/\/ or by 2 FunctionProtos then versions for the operator set may be different but,\n+        \/\/\/ the operator schema returned for op_type, domain, version combination\n+        \/\/\/ for both the versions should be same for every node in the function body.\n+        \/\/\/\n+        \/\/\/ One FunctionProto can reference other FunctionProto in the model, however, recursive reference\n+        \/\/\/ is not allowed.\n+        public ModelProto functions(FunctionProto functions) {return _f(25, functions);}\n+    }\n+\n+    \/\/\/ StringStringEntryProto follows the pattern for cross-proto-version maps.\n+    \/\/\/ See https:\/\/developers.google.com\/protocol-buffers\/docs\/proto3#maps\n+    public static final class StringStringEntryProto extends OnnxBuilder<StringStringEntryProto> {\n+\n+        public StringStringEntryProto key(String key) {return _f(1, key);}\n+\n+        public StringStringEntryProto value(String value) {return _f(2, value);}\n+    }\n+\n+    public static final class TensorAnnotation extends OnnxBuilder<TensorAnnotation> {\n+\n+        public TensorAnnotation tensorName(String tensorName) {return _f(1, tensorName);}\n+\n+        \/\/\/ <key, value> pairs to annotate tensor specified by <tensor_name> above.\n+        \/\/\/ The keys used in the mapping below must be pre-defined in ONNX spec.\n+        \/\/\/ For example, for 8-bit linear quantization case, 'SCALE_TENSOR', 'ZERO_POINT_TENSOR' will be pre-defined as\n+        \/\/\/ quantization parameter keys.\n+        public TensorAnnotation quantParameterTensorNames(StringStringEntryProto quantParameterTensorNames) {return _f(2, quantParameterTensorNames);}\n+    }\n+\n+    \/\/\/ Graphs\n+    \/\/\/\n+    \/\/\/ A graph defines the computational logic of a model and is comprised of a parameterized\n+    \/\/\/ list of nodes that form a directed acyclic graph based on their inputs and outputs.\n+    \/\/\/ This is the equivalent of the \"network\" or \"graph\" in many deep learning\n+    \/\/\/ frameworks.\n+    public static final class GraphProto extends OnnxBuilder<GraphProto> {\n+\n+        \/\/\/ The nodes in the graph, sorted topologically.\n+        public GraphProto node(NodeProto node) {return _f(1, node);}\n+\n+        \/\/\/ The name of the graph.\n+        \/\/\/ namespace Graph\n+        public GraphProto name(String name) {return _f(2, name);}\n+\n+        \/\/\/ A list of named tensor values, used to specify constant inputs of the graph.\n+        \/\/\/ Each initializer (both TensorProto as well SparseTensorProto) MUST have a name.\n+        \/\/\/ The name MUST be unique across both initializer and sparse_initializer,\n+        \/\/\/ but the name MAY also appear in the input list.\n+        public GraphProto initializer(TensorProto initializer) {return _f(5, initializer);}\n+\n+        \/\/\/ Initializers (see above) stored in sparse format.\n+        public GraphProto sparseInitializer(SparseTensorProto sparseInitializer) {return _f(15, sparseInitializer);}\n+\n+        \/\/\/ A human-readable documentation for this graph. Markdown is allowed.\n+        public GraphProto docString(String docString) {return _f(10, docString);}\n+\n+        \/\/\/ The inputs and outputs of the graph.\n+        public GraphProto input(ValueInfoProto input) {return _f(11, input);}\n+\n+        public GraphProto output(ValueInfoProto output) {return _f(12, output);}\n+\n+        \/\/\/ Information for the values in the graph. The ValueInfoProto.name's\n+        \/\/\/ must be distinct. It is optional for a value to appear in value_info list.\n+        public GraphProto valueInfo(ValueInfoProto valueInfo) {return _f(13, valueInfo);}\n+\n+        \/\/\/ This field carries information to indicate the mapping among a tensor and its\n+        \/\/\/ quantization parameter tensors. For example:\n+        \/\/\/ For tensor 'a', it may have {'SCALE_TENSOR', 'a_scale'} and {'ZERO_POINT_TENSOR', 'a_zero_point'} annotated,\n+        \/\/\/ which means, tensor 'a_scale' and tensor 'a_zero_point' are scale and zero point of tensor 'a' in the model.\n+        public GraphProto quantizationAnnotation(TensorAnnotation quantizationAnnotation) {return _f(14, quantizationAnnotation);}\n+\n+        \/\/\/ Named metadata values; keys should be distinct.\n+        public GraphProto metadataProps(StringStringEntryProto metadataProps) {return _f(16, metadataProps);}\n+    }\n+\n+    \/\/\/ Tensors\n+    \/\/\/\n+    \/\/\/ A serialized tensor value.\n+    public static final class TensorProto extends OnnxBuilder<TensorProto> {\n+\n+        \/\/\/ The shape of the tensor.\n+        public TensorProto dims(long... dims) {return _f(1, dims);}\n+\n+        \/\/\/ The data type of the tensor.\n+        \/\/\/ This field MUST have a valid TensorProto.DataType value\n+        public TensorProto dataType(int dataType) {return _f(2, dataType);}\n+\n+        \/\/\/ For very large tensors, we may want to store them in chunks, in which\n+        \/\/\/ case the following fields will specify the segment that is stored in\n+        \/\/\/ the current TensorProto.\n+        public static final class Segment extends OnnxBuilder<Segment> {\n+\n+            public Segment begin(long begin) {return _f(1, begin);}\n+\n+            public Segment end(long end) {return _f(2, end);}\n+        }\n+\n+        public TensorProto segment(Segment segment) {return _f(3, segment);}\n+\n+        \/\/\/ For float and complex64 values\n+        \/\/\/ Complex64 tensors are encoded as a single array of floats,\n+        \/\/\/ with the real components appearing in odd numbered positions,\n+        \/\/\/ and the corresponding imaginary component appearing in the\n+        \/\/\/ subsequent even numbered position. (e.g., [1.0 + 2.0i, 3.0 + 4.0i]\n+        \/\/\/ is encoded as [1.0, 2.0 ,3.0 ,4.0]\n+        \/\/\/ When this field is present, the data_type field MUST be FLOAT or COMPLEX64.\n+        public TensorProto floatData(float... floatData) {return _f(4, floatData);}\n+\n+        \/\/\/ For int32, uint8, int8, uint16, int16, uint4, int4, bool, (b)float16, float8, and float4:\n+        \/\/\/ - (b)float16 and float8 values MUST be converted bit-wise into an unsigned integer\n+        \/\/\/   representation before being written to the buffer.\n+        \/\/\/ - Each pair of uint4, int4, and float4 values MUST be packed as two 4-bit elements into a single byte.\n+        \/\/\/   The first element is stored in the 4 least significant bits (LSB),\n+        \/\/\/   and the second element is stored in the 4 most significant bits (MSB).\n+        \/\/\/\n+        \/\/\/ Consequently:\n+        \/\/\/ - For data types with a bit-width of 8 or greater, each `int32_data` stores one element.\n+        \/\/\/ - For 4-bit data types, each `int32_data` stores two elements.\n+        \/\/\/\n+        \/\/\/ When this field is present, the data_type field MUST be\n+        \/\/\/ INT32, INT16, INT8, INT4, UINT16, UINT8, UINT4, BOOL, FLOAT16, BFLOAT16, FLOAT8E4M3FN, FLOAT8E4M3FNUZ, FLOAT8E5M2, FLOAT8E5M2FNUZ, FLOAT4E2M1\n+        public TensorProto int32Data(int... int32Data) {return _f(5, int32Data);}\n+\n+        \/\/\/ For strings.\n+        \/\/\/ Each element of string_data is a UTF-8 encoded Unicode\n+        \/\/\/ string. No trailing null, no leading BOM. The protobuf \"string\"\n+        \/\/\/ scalar type is not used to match ML community conventions.\n+        \/\/\/ When this field is present, the data_type field MUST be STRING\n+        public TensorProto stringData(byte[] stringData) {return _f(6, stringData);}\n+\n+        \/\/\/ For int64.\n+        \/\/\/ When this field is present, the data_type field MUST be INT64\n+        public TensorProto int64Data(long... int64Data) {return _f(7, int64Data);}\n+\n+        \/\/\/ Optionally, a name for the tensor.\n+        \/\/\/ namespace Value\n+        public TensorProto name(String name) {return _f(8, name);}\n+\n+        \/\/\/ A human-readable documentation for this tensor. Markdown is allowed.\n+        public TensorProto docString(String docString) {return _f(12, docString);}\n+\n+        \/\/\/ Serializations can either use one of the fields above, or use this\n+        \/\/\/ raw bytes field. The only exception is the string case, where one is\n+        \/\/\/ required to store the content in the repeated bytes string_data field.\n+        \/\/\/\n+        \/\/\/ When this raw_data field is used to store tensor value, elements MUST\n+        \/\/\/ be stored in as fixed-width, little-endian order.\n+        \/\/\/ Floating-point data types MUST be stored in IEEE 754 format.\n+        \/\/\/ Complex64 elements must be written as two consecutive FLOAT values, real component first.\n+        \/\/\/ Complex128 elements must be written as two consecutive DOUBLE values, real component first.\n+        \/\/\/ Boolean type MUST be written one byte per tensor element (00000001 for true, 00000000 for false).\n+        \/\/\/ uint4 and int4 values must be packed to 4bitx2, the first element is stored in the 4 LSB and the second element is stored in the 4 MSB.\n+        \/\/\/\n+        \/\/\/ Note: the advantage of specific field rather than the raw_data field is\n+        \/\/\/ that in some cases (e.g. int data), protobuf does a better packing via\n+        \/\/\/ variable length storage, and may lead to smaller binary footprint.\n+        \/\/\/ When this field is present, the data_type field MUST NOT be STRING or UNDEFINED\n+        public TensorProto rawData(byte[] rawData) {return _f(9, rawData);}\n+\n+        \/\/\/ Data can be stored inside the protobuf file using type-specific fields or raw_data.\n+        \/\/\/ Alternatively, raw bytes data can be stored in an external file, using the external_data field.\n+        \/\/\/ external_data stores key-value pairs describing data location. Recognized keys are:\n+        \/\/\/ - \"location\" (required) - POSIX filesystem path relative to the directory where the ONNX\n+        \/\/\/                           protobuf model was stored\n+        \/\/\/ - \"offset\" (optional) - position of byte at which stored data begins. Integer stored as string.\n+        \/\/\/                         Offset values SHOULD be multiples 4096 (page size) to enable mmap support.\n+        \/\/\/ - \"length\" (optional) - number of bytes containing data. Integer stored as string.\n+        \/\/\/ - \"checksum\" (optional) - SHA1 digest of file specified in under 'location' key.\n+        public TensorProto externalData(StringStringEntryProto externalData) {return _f(13, externalData);}\n+\n+        \/\/\/ If value not set, data is stored in raw_data (if set) otherwise in type-specified field.\n+        public TensorProto dataLocation(DataLocation dataLocation) {return _f(14, dataLocation);}\n+\n+        \/\/\/ For double\n+        \/\/\/ Complex128 tensors are encoded as a single array of doubles,\n+        \/\/\/ with the real components appearing in odd numbered positions,\n+        \/\/\/ and the corresponding imaginary component appearing in the\n+        \/\/\/ subsequent even numbered position. (e.g., [1.0 + 2.0i, 3.0 + 4.0i]\n+        \/\/\/ is encoded as [1.0, 2.0 ,3.0 ,4.0]\n+        \/\/\/ When this field is present, the data_type field MUST be DOUBLE or COMPLEX128\n+        public TensorProto doubleData(double... doubleData) {return _f(10, doubleData);}\n+\n+        \/\/\/ For uint64 and uint32 values\n+        \/\/\/ When this field is present, the data_type field MUST be\n+        \/\/\/ UINT32 or UINT64\n+        public TensorProto uint64Data(long... uint64Data) {return _f(11, uint64Data);}\n+\n+        \/\/\/ Named metadata values; keys should be distinct.\n+        public TensorProto metadataProps(StringStringEntryProto metadataProps) {return _f(16, metadataProps);}\n+    }\n+\n+    \/\/\/ A serialized sparse-tensor value\n+    public static final class SparseTensorProto extends OnnxBuilder<SparseTensorProto> {\n+\n+        \/\/\/ The sequence of non-default values are encoded as a tensor of shape [NNZ].\n+        \/\/\/ The default-value is zero for numeric tensors, and empty-string for string tensors.\n+        \/\/\/ values must have a non-empty name present which serves as a name for SparseTensorProto\n+        \/\/\/ when used in sparse_initializer list.\n+        public SparseTensorProto values(TensorProto values) {return _f(1, values);}\n+\n+        \/\/\/ The indices of the non-default values, which may be stored in one of two formats.\n+        \/\/\/ (a) Indices can be a tensor of shape [NNZ, rank] with the [i,j]-th value\n+        \/\/\/ corresponding to the j-th index of the i-th value (in the values tensor).\n+        \/\/\/ (b) Indices can be a tensor of shape [NNZ], in which case the i-th value\n+        \/\/\/ must be the linearized-index of the i-th value (in the values tensor).\n+        \/\/\/ The linearized-index can be converted into an index tuple (k_1,...,k_rank)\n+        \/\/\/ using the shape provided below.\n+        \/\/\/ The indices must appear in ascending order without duplication.\n+        \/\/\/ In the first format, the ordering is lexicographic-ordering:\n+        \/\/\/ e.g., index-value [1,4] must appear before [2,1]\n+        public SparseTensorProto indices(TensorProto indices) {return _f(2, indices);}\n+\n+        \/\/\/ The shape of the underlying dense-tensor: [dim_1, dim_2, ... dim_rank]\n+        public SparseTensorProto dims(long... dims) {return _f(3, dims);}\n+    }\n+\n+    \/\/\/ Defines a tensor shape. A dimension can be either an integer value\n+    \/\/\/ or a symbolic variable. A symbolic variable represents an unknown\n+    \/\/\/ dimension.\n+    public static final class TensorShapeProto extends OnnxBuilder<TensorShapeProto> {\n+\n+        public static final class Dimension extends OnnxBuilder<Dimension> {\n+\n+            public Dimension dimValue(long dimValue) {return _f(1, dimValue);}\n+\n+            \/\/\/ namespace Shape\n+            public Dimension dimParam(String dimParam) {return _f(2, dimParam);}\n+\n+            \/\/\/ Standard denotation can optionally be used to denote tensor\n+            \/\/\/ dimensions with standard semantic descriptions to ensure\n+            \/\/\/ that operations are applied to the correct axis of a tensor.\n+            \/\/\/ Refer to https:\/\/github.com\/onnx\/onnx\/blob\/main\/docs\/DimensionDenotation.md#denotation-definition\n+            \/\/\/ for pre-defined dimension denotations.\n+            public Dimension denotation(String denotation) {return _f(3, denotation);}\n+        }\n+\n+        public TensorShapeProto dim(Dimension dim) {return _f(1, dim);}\n+    }\n+\n+    \/\/\/ Types\n+    \/\/\/\n+    \/\/\/ The standard ONNX data types.\n+    public static final class TypeProto extends OnnxBuilder<TypeProto> {\n+\n+        public static final class Tensor extends OnnxBuilder<Tensor> {\n+\n+            \/\/\/ This field MUST NOT have the value of UNDEFINED\n+            \/\/\/ This field MUST have a valid TensorProto.DataType value\n+            \/\/\/ This field MUST be present for this version of the IR.\n+            public Tensor elemType(int elemType) {return _f(1, elemType);}\n+\n+            public Tensor shape(TensorShapeProto shape) {return _f(2, shape);}\n+        }\n+\n+        \/\/\/ repeated T\n+        public static final class Sequence extends OnnxBuilder<Sequence> {\n+\n+            \/\/\/ The type and optional shape of each element of the sequence.\n+            \/\/\/ This field MUST be present for this version of the IR.\n+            public Sequence elemType(TypeProto elemType) {return _f(1, elemType);}\n+        }\n+\n+        \/\/\/ map<K,V>\n+        public static final class Map extends OnnxBuilder<Map> {\n+\n+            \/\/\/ This field MUST have a valid TensorProto.DataType value\n+            \/\/\/ This field MUST be present for this version of the IR.\n+            \/\/\/ This field MUST refer to an integral type ([U]INT{8|16|32|64}) or STRING\n+            public Map keyType(int keyType) {return _f(1, keyType);}\n+\n+            \/\/\/ This field MUST be present for this version of the IR.\n+            public Map valueType(TypeProto valueType) {return _f(2, valueType);}\n+        }\n+\n+        \/\/\/ wrapper for Tensor, Sequence, or Map\n+        public static final class Optional extends OnnxBuilder<Optional> {\n+\n+            \/\/\/ The type and optional shape of the element wrapped.\n+            \/\/\/ This field MUST be present for this version of the IR.\n+            \/\/\/ Possible values correspond to OptionalProto.DataType enum\n+            public Optional elemType(TypeProto elemType) {return _f(1, elemType);}\n+        }\n+\n+        public static final class SparseTensor extends OnnxBuilder<SparseTensor> {\n+\n+            \/\/\/ This field MUST NOT have the value of UNDEFINED\n+            \/\/\/ This field MUST have a valid TensorProto.DataType value\n+            \/\/\/ This field MUST be present for this version of the IR.\n+            public SparseTensor elemType(int elemType) {return _f(1, elemType);}\n+\n+            public SparseTensor shape(TensorShapeProto shape) {return _f(2, shape);}\n+        }\n+\n+        public static final class Opaque extends OnnxBuilder<Opaque> {\n+\n+            \/\/\/ When missing, the domain is the same as the model's.\n+            public Opaque domain(String domain) {return _f(1, domain);}\n+\n+            \/\/\/ The name is optional but significant when provided.\n+            public Opaque name(String name) {return _f(2, name);}\n+        }\n+\n+        \/\/\/ The type of a tensor.\n+        public TypeProto tensorType(Tensor tensorType) {return _f(1, tensorType);}\n+\n+        \/\/\/ The type of a sequence.\n+        public TypeProto sequenceType(Sequence sequenceType) {return _f(4, sequenceType);}\n+\n+        \/\/\/ The type of a map.\n+        public TypeProto mapType(Map mapType) {return _f(5, mapType);}\n+\n+        \/\/\/ The type of an optional.\n+        public TypeProto optionalType(Optional optionalType) {return _f(9, optionalType);}\n+\n+        \/\/\/ Type of the sparse tensor\n+        public TypeProto sparseTensorType(SparseTensor sparseTensorType) {return _f(8, sparseTensorType);}\n+\n+        public TypeProto opaqueType(Opaque opaqueType) {return _f(7, opaqueType);}\n+\n+        \/\/\/ An optional denotation can be used to denote the whole\n+        \/\/\/ type with a standard semantic description as to what is\n+        \/\/\/ stored inside. Refer to https:\/\/github.com\/onnx\/onnx\/blob\/main\/docs\/TypeDenotation.md#type-denotation-definition\n+        \/\/\/ for pre-defined type denotations.\n+        public TypeProto denotation(String denotation) {return _f(6, denotation);}\n+    }\n+\n+    \/\/\/ Operator Sets\n+    \/\/\/\n+    \/\/\/ OperatorSets are uniquely identified by a (domain, opset_version) pair.\n+    public static final class OperatorSetIdProto extends OnnxBuilder<OperatorSetIdProto> {\n+\n+        \/\/\/ The domain of the operator set being identified.\n+        \/\/\/ The empty string (\"\") or absence of this field implies the operator\n+        \/\/\/ set that is defined as part of the ONNX specification.\n+        \/\/\/ This field MUST be present in this version of the IR when referring to any other operator set.\n+        public OperatorSetIdProto domain(String domain) {return _f(1, domain);}\n+\n+        \/\/\/ The version of the operator set being identified.\n+        \/\/\/ This field MUST be present in this version of the IR.\n+        public OperatorSetIdProto version(long version) {return _f(2, version);}\n+    }\n+\n+    public static final class FunctionProto extends OnnxBuilder<FunctionProto> {\n+\n+        \/\/\/ The name of the function, similar to op_type in NodeProto.\n+        \/\/\/ This is part of the unique-id (domain, name, overload) of FunctionProtos in a model.\n+        public FunctionProto name(String name) {return _f(1, name);}\n+\n+        \/\/\/ The inputs and outputs of the function.\n+        public FunctionProto input(String input) {return _f(4, input);}\n+\n+        public FunctionProto output(String output) {return _f(5, output);}\n+\n+        \/\/\/ The attribute parameters of the function.\n+        \/\/\/ It is for function parameters without default values.\n+        public FunctionProto attribute(String attribute) {return _f(6, attribute);}\n+\n+        \/\/\/ The attribute protos of the function.\n+        \/\/\/ It is for function attributes with default values.\n+        \/\/\/ A function attribute shall be represented either as\n+        \/\/\/ a string attribute or an AttributeProto, not both.\n+        public FunctionProto attributeProto(AttributeProto attributeProto) {return _f(11, attributeProto);}\n+\n+        \/\/\/ The nodes in the function.\n+        public FunctionProto node(NodeProto node) {return _f(7, node);}\n+\n+        \/\/\/ A human-readable documentation for this function. Markdown is allowed.\n+        public FunctionProto docString(String docString) {return _f(8, docString);}\n+\n+        public FunctionProto opsetImport(OperatorSetIdProto opsetImport) {return _f(9, opsetImport);}\n+\n+        \/\/\/ The domain which this function belongs to.\n+        \/\/\/ This is part of the unique-id (domain, name, overload) of FunctionProtos in a model.\n+        public FunctionProto domain(String domain) {return _f(10, domain);}\n+\n+        \/\/\/ The overload identifier of the function.\n+        \/\/\/ This is part of the unique-id (domain, name, overload) of FunctionProtos in a model.\n+        public FunctionProto overload(String overload) {return _f(13, overload);}\n+\n+        \/\/\/ Information for the values in the function. The ValueInfoProto.name's\n+        \/\/\/ must be distinct and refer to names in the function (including inputs,\n+        \/\/\/ outputs, and intermediate values). It is optional for a value to appear\n+        \/\/\/ in value_info list.\n+        public FunctionProto valueInfo(ValueInfoProto valueInfo) {return _f(12, valueInfo);}\n+\n+        \/\/\/ Named metadata values; keys should be distinct.\n+        public FunctionProto metadataProps(StringStringEntryProto metadataProps) {return _f(14, metadataProps);}\n+    }\n+\n+    \/\/ Implementation\n+\n+    final ByteArrayOutputStream buf = new ByteArrayOutputStream();\n+\n+    public byte[] getBytes() {\n+        return buf.toByteArray();\n+    }\n+\n+    @SuppressWarnings(\"unchecked\")\n+    public <P> T forEach(Iterable<P> sup, BiConsumer<T, ? super P> cons) {\n+        sup.forEach(p -> cons.accept((T)this, p));\n+        return (T)this;\n+    }\n+\n+    void _encode(long number) {\n+        for (int i = 64 - Long.numberOfLeadingZeros(number); i > 7; i -= 7) {\n+            buf.write(0x80 | (int)number & 0x7f);\n+            number >>= 7;\n+        }\n+        buf.write((int)number & 0x7f);\n+    }\n+\n+    void _encode(float value) {\n+        int bits =  Float.floatToRawIntBits(value);\n+        buf.write((byte)bits);\n+        buf.write((byte)(bits >> 8));\n+        buf.write((byte)(bits >> 16));\n+        buf.write((byte)(bits >> 24));\n+    }\n+\n+    void _encode(double value) {\n+        long bits =  Double.doubleToRawLongBits(value);\n+        buf.write((byte)bits);\n+        buf.write((byte)(bits >> 8));\n+        buf.write((byte)(bits >> 16));\n+        buf.write((byte)(bits >> 24));\n+        buf.write((byte)(bits >> 32));\n+        buf.write((byte)(bits >> 40));\n+        buf.write((byte)(bits >> 48));\n+        buf.write((byte)(bits >> 56));\n+    }\n+\n+    @SuppressWarnings(\"unchecked\")\n+    T _f(int fieldIndex, String value) {\n+        return value == null ? (T)this : _f(fieldIndex, value.getBytes(StandardCharsets.UTF_8));\n+    }\n+\n+    @SuppressWarnings(\"unchecked\")\n+    T _f(int fieldIndex, byte[] bytes) {\n+        _encode(fieldIndex << 3 | 2);\n+        _encode(bytes.length);\n+        buf.writeBytes(bytes);\n+        return (T)this;\n+    }\n+\n+    @SuppressWarnings(\"unchecked\")\n+    T _f(int fieldIndex, float value) {\n+        _encode(fieldIndex << 3 | 5);\n+        _encode(value);\n+        return (T)this;\n+    }\n+\n+    @SuppressWarnings(\"unchecked\")\n+    T _f(int fieldIndex, float... values) {\n+        if (values.length == 1) {\n+            return _f(fieldIndex, values[0]);\n+        }\n+        var b = new OnnxBuilder();\n+        for (var v : values) b._encode(v);\n+        _f(fieldIndex, b);\n+        return (T)this;\n+    }\n+\n+    @SuppressWarnings(\"unchecked\")\n+    T _f(int fieldIndex, double value) {\n+        _encode(fieldIndex << 3 | 1);\n+        _encode(value);\n+        return (T)this;\n+    }\n+\n+    @SuppressWarnings(\"unchecked\")\n+    T _f(int fieldIndex, double... values) {\n+        if (values.length == 1) {\n+            return _f(fieldIndex, values[0]);\n+        }\n+        var b = new OnnxBuilder();\n+        for (var v : values) b._encode(v);\n+        _f(fieldIndex, b);\n+        return (T)this;\n+    }\n+\n+    @SuppressWarnings(\"unchecked\")\n+    T _f(int fieldIndex, long value) {\n+        _encode(fieldIndex << 3);\n+        _encode(value);\n+        return (T)this;\n+    }\n+\n+    @SuppressWarnings(\"unchecked\")\n+    T _f(int fieldIndex, long... values) {\n+        if (values.length == 1) {\n+            return _f(fieldIndex, values[0]);\n+        }\n+        var b = new OnnxBuilder();\n+        for (var v : values) b._encode(v);\n+        _f(fieldIndex, b);\n+        return (T)this;\n+    }\n+\n+    @SuppressWarnings(\"unchecked\")\n+    T _f(int fieldIndex, int... values) {\n+        if (values.length == 1) {\n+            return _f(fieldIndex, values[0]);\n+        }\n+        var b = new OnnxBuilder();\n+        for (var v : values) b._encode(v);\n+        _f(fieldIndex, b);\n+        return (T)this;\n+    }\n+\n+    @SuppressWarnings(\"unchecked\")\n+    T _f(int fieldIndex, OnnxBuilder value) {\n+        return _f(fieldIndex, value.buf.toByteArray());\n+    }\n+\n+    @SuppressWarnings(\"unchecked\")\n+    T _f(int fieldIndex, IntSupplier value) {\n+        return _f(fieldIndex, value.getAsInt());\n+    }\n+}\n","filename":"cr-examples\/onnx\/src\/main\/java\/oracle\/code\/onnx\/proto\/OnnxBuilder.java","additions":885,"deletions":0,"binary":false,"changes":885,"status":"added"},{"patch":"@@ -0,0 +1,305 @@\n+\/*\n+ * Copyright (c) 2025, Oracle and\/or its affiliates. All rights reserved.\n+ * DO NOT ALTER OR REMOVE COPYRIGHT NOTICES OR THIS FILE HEADER.\n+ *\n+ * This code is free software; you can redistribute it and\/or modify it\n+ * under the terms of the GNU General Public License version 2 only, as\n+ * published by the Free Software Foundation.  Oracle designates this\n+ * particular file as subject to the \"Classpath\" exception as provided\n+ * by Oracle in the LICENSE file that accompanied this code.\n+ *\n+ * This code is distributed in the hope that it will be useful, but WITHOUT\n+ * ANY WARRANTY; without even the implied warranty of MERCHANTABILITY or\n+ * FITNESS FOR A PARTICULAR PURPOSE.  See the GNU General Public License\n+ * version 2 for more details (a copy is included in the LICENSE file that\n+ * accompanied this code).\n+ *\n+ * You should have received a copy of the GNU General Public License version\n+ * 2 along with this work; if not, write to the Free Software Foundation,\n+ * Inc., 51 Franklin St, Fifth Floor, Boston, MA 02110-1301 USA.\n+ *\n+ * Please contact Oracle, 500 Oracle Parkway, Redwood Shores, CA 94065 USA\n+ * or visit www.oracle.com if you need additional information or have any\n+ * questions.\n+ *\/\n+\n+package oracle.code.onnx.proto;\n+\n+import java.util.function.IntSupplier;\n+\n+\/\/ Generated from onnx.in.proto\n+public final class OnnxConstants {\n+\n+    \/\/\/ Versioning\n+    \/\/\/\n+    \/\/\/ ONNX versioning is specified in docs\/IR.md and elaborated on in docs\/Versioning.md\n+    \/\/\/\n+    \/\/\/ To be compatible with both proto2 and proto3, we will use a version number\n+    \/\/\/ that is not defined by the default value but an explicit enum number.\n+    public enum Version implements IntSupplier {\n+\n+        \/\/\/ proto3 requires the first enum value to be zero.\n+        \/\/\/ We add this just to appease the compiler.\n+        _START_VERSION(0),\n+\n+        \/\/\/ The version field is always serialized and we will use it to store the\n+        \/\/\/ version that the  graph is generated from. This helps us set up version\n+        \/\/\/ control.\n+        \/\/\/ For the IR, we are using simple numbers starting with 0x00000001,\n+        \/\/\/ which was the version we published on Oct 10, 2017.\n+        IR_VERSION_2017_10_10(0x0000000000000001),\n+\n+        \/\/\/ IR_VERSION 2 published on Oct 30, 2017\n+        \/\/\/ - Added type discriminator to AttributeProto to support proto3 users\n+        IR_VERSION_2017_10_30(0x0000000000000002),\n+\n+        \/\/\/ IR VERSION 3 published on Nov 3, 2017\n+        \/\/\/ - For operator versioning:\n+        \/\/\/    - Added new message OperatorSetIdProto\n+        \/\/\/    - Added opset_import in ModelProto\n+        \/\/\/ - For vendor extensions, added domain in NodeProto\n+        IR_VERSION_2017_11_3(0x0000000000000003),\n+\n+        \/\/\/ IR VERSION 4 published on Jan 22, 2019\n+        \/\/\/ - Relax constraint that initializers should be a subset of graph inputs\n+        \/\/\/ - Add type BFLOAT16\n+        IR_VERSION_2019_1_22(0x0000000000000004),\n+\n+        \/\/\/ IR VERSION 5 published on March 18, 2019\n+        \/\/\/ - Add message TensorAnnotation.\n+        \/\/\/ - Add quantization annotation in GraphProto to map tensor with its scale and zero point quantization parameters.\n+        IR_VERSION_2019_3_18(0x0000000000000005),\n+\n+        \/\/\/ IR VERSION 6 published on Sep 19, 2019\n+        \/\/\/ - Add support for sparse tensor constants stored in model.\n+        \/\/\/   - Add message SparseTensorProto\n+        \/\/\/   - Add sparse initializers\n+        IR_VERSION_2019_9_19(0x0000000000000006),\n+\n+        \/\/\/ IR VERSION 7 published on May 8, 2020\n+        \/\/\/ - Add support to allow function body graph to rely on multiple external opreator sets.\n+        \/\/\/ - Add a list to promote inference graph's initializers to global and\n+        \/\/\/   mutable variables. Global variables are visible in all graphs of the\n+        \/\/\/   stored models.\n+        \/\/\/ - Add message TrainingInfoProto to store initialization\n+        \/\/\/   method and training algorithm. The execution of TrainingInfoProto\n+        \/\/\/   can modify the values of mutable variables.\n+        \/\/\/ - Implicitly add inference graph into each TrainingInfoProto's algorithm.\n+        IR_VERSION_2020_5_8(0x0000000000000007),\n+\n+        \/\/\/ IR VERSION 8 published on July 30, 2021\n+        \/\/\/ Introduce TypeProto.SparseTensor\n+        \/\/\/ Introduce TypeProto.Optional\n+        \/\/\/ Added a list of FunctionProtos local to the model\n+        \/\/\/ Deprecated since_version and operator status from FunctionProto\n+        IR_VERSION_2021_7_30(0x0000000000000008),\n+\n+        \/\/\/ IR VERSION 9 published on May 5, 2023\n+        \/\/\/ Added AttributeProto to FunctionProto so that default attribute values can be set.\n+        \/\/\/ Added FLOAT8E4M3FN, FLOAT8E4M3FNUZ, FLOAT8E5M2, FLOAT8E5M2FNUZ.\n+        IR_VERSION_2023_5_5(0x0000000000000009),\n+\n+        \/\/\/ IR VERSION 10 published on March 25, 2024\n+        \/\/\/ Added UINT4, INT4.\n+        IR_VERSION_2024_3_25(0x000000000000000A),\n+\n+        \/\/\/ IR VERSION 11 published on TBD\n+        \/\/\/ Added FLOAT4E2M1.\n+        IR_VERSION(0x000000000000000B),\n+        ;\n+\n+        final int value;\n+\n+        Version(int value) {\n+            this.value = value;\n+        }\n+\n+        @Override\n+        public int getAsInt() {\n+            return value;\n+        }\n+    }\n+\n+    \/\/\/ Note: this enum is structurally identical to the OpSchema::AttrType\n+    \/\/\/ enum defined in schema.h.  If you rev one, you likely need to rev the other.\n+    public enum AttributeType implements IntSupplier {\n+\n+        UNDEFINED(0),\n+\n+        FLOAT(1),\n+\n+        INT(2),\n+\n+        STRING(3),\n+\n+        TENSOR(4),\n+\n+        GRAPH(5),\n+\n+        SPARSE_TENSOR(11),\n+\n+        TYPE_PROTO(13),\n+\n+        FLOATS(6),\n+\n+        INTS(7),\n+\n+        STRINGS(8),\n+\n+        TENSORS(9),\n+\n+        GRAPHS(10),\n+\n+        SPARSE_TENSORS(12),\n+\n+        TYPE_PROTOS(14),\n+        ;\n+\n+        final int value;\n+\n+        AttributeType(int value) {\n+            this.value = value;\n+        }\n+\n+        @Override\n+        public int getAsInt() {\n+            return value;\n+        }\n+    }\n+\n+    public enum DataType implements IntSupplier {\n+\n+        UNDEFINED(0),\n+\n+        \/\/\/ Basic types.\n+        \/\/\/ float\n+        FLOAT(1),\n+\n+        \/\/\/ uint8_t\n+        UINT8(2),\n+\n+        \/\/\/ int8_t\n+        INT8(3),\n+\n+        \/\/\/ uint16_t\n+        UINT16(4),\n+\n+        \/\/\/ int16_t\n+        INT16(5),\n+\n+        \/\/\/ int32_t\n+        INT32(6),\n+\n+        \/\/\/ int64_t\n+        INT64(7),\n+\n+        \/\/\/ string\n+        STRING(8),\n+\n+        \/\/\/ bool\n+        BOOL(9),\n+\n+        \/\/\/ IEEE754 half-precision floating-point format (16 bits wide).\n+        \/\/\/ This format has 1 sign bit, 5 exponent bits, and 10 mantissa bits.\n+        FLOAT16(10),\n+\n+        DOUBLE(11),\n+\n+        UINT32(12),\n+\n+        UINT64(13),\n+\n+        \/\/\/ complex with float32 real and imaginary components\n+        COMPLEX64(14),\n+\n+        \/\/\/ complex with float64 real and imaginary components\n+        COMPLEX128(15),\n+\n+        \/\/\/ Non-IEEE floating-point format based on IEEE754 single-precision\n+        \/\/\/ floating-point number truncated to 16 bits.\n+        \/\/\/ This format has 1 sign bit, 8 exponent bits, and 7 mantissa bits.\n+        BFLOAT16(16),\n+\n+        \/\/\/ Non-IEEE floating-point format based on papers\n+        \/\/\/ FP8 Formats for Deep Learning, https:\/\/arxiv.org\/abs\/2209.05433,\n+        \/\/\/ 8-bit Numerical Formats For Deep Neural Networks, https:\/\/arxiv.org\/pdf\/2206.02915.pdf.\n+        \/\/\/ Operators supported FP8 are Cast, CastLike, QuantizeLinear, DequantizeLinear.\n+        \/\/\/ The computation usually happens inside a block quantize \/ dequantize\n+        \/\/\/ fused by the runtime.\n+        \/\/\/ float 8, mostly used for coefficients, supports nan, not inf\n+        FLOAT8E4M3FN(17),\n+\n+        \/\/\/ float 8, mostly used for coefficients, supports nan, not inf, no negative zero\n+        FLOAT8E4M3FNUZ(18),\n+\n+        \/\/\/ follows IEEE 754, supports nan, inf, mostly used for gradients\n+        FLOAT8E5M2(19),\n+\n+        \/\/\/ follows IEEE 754, supports nan, not inf, mostly used for gradients, no negative zero\n+        FLOAT8E5M2FNUZ(20),\n+\n+        \/\/\/ 4-bit integer data types\n+        \/\/\/ Unsigned integer in range [0, 15]\n+        UINT4(21),\n+\n+        \/\/\/ Signed integer in range [-8, 7], using two's-complement representation\n+        INT4(22),\n+\n+        \/\/\/ 4-bit floating point data types\n+        FLOAT4E2M1(23),\n+        ;\n+\n+        final int value;\n+\n+        DataType(int value) {\n+            this.value = value;\n+        }\n+\n+        @Override\n+        public int getAsInt() {\n+            return value;\n+        }\n+    }\n+\n+    \/\/\/ Location of the data for this tensor. MUST be one of:\n+    \/\/\/ - DEFAULT - data stored inside the protobuf message. Data is stored in raw_data (if set) otherwise in type-specified field.\n+    \/\/\/ - EXTERNAL - data stored in an external location as described by external_data field.\n+    public enum DataLocation implements IntSupplier {\n+\n+        DEFAULT(0),\n+\n+        EXTERNAL(1),\n+        ;\n+\n+        final int value;\n+\n+        DataLocation(int value) {\n+            this.value = value;\n+        }\n+\n+        @Override\n+        public int getAsInt() {\n+            return value;\n+        }\n+    }\n+\n+    \/\/\/ Operator\/function status.\n+    public enum OperatorStatus implements IntSupplier {\n+\n+        EXPERIMENTAL(0),\n+\n+        STABLE(1),\n+        ;\n+\n+        final int value;\n+\n+        OperatorStatus(int value) {\n+            this.value = value;\n+        }\n+\n+        @Override\n+        public int getAsInt() {\n+            return value;\n+        }\n+    }\n+}\n","filename":"cr-examples\/onnx\/src\/main\/java\/oracle\/code\/onnx\/proto\/OnnxConstants.java","additions":305,"deletions":0,"binary":false,"changes":305,"status":"added"},{"patch":"@@ -0,0 +1,976 @@\n+\/*\n+ * Copyright (c) 2025, Oracle and\/or its affiliates. All rights reserved.\n+ * DO NOT ALTER OR REMOVE COPYRIGHT NOTICES OR THIS FILE HEADER.\n+ *\n+ * This code is free software; you can redistribute it and\/or modify it\n+ * under the terms of the GNU General Public License version 2 only, as\n+ * published by the Free Software Foundation.  Oracle designates this\n+ * particular file as subject to the \"Classpath\" exception as provided\n+ * by Oracle in the LICENSE file that accompanied this code.\n+ *\n+ * This code is distributed in the hope that it will be useful, but WITHOUT\n+ * ANY WARRANTY; without even the implied warranty of MERCHANTABILITY or\n+ * FITNESS FOR A PARTICULAR PURPOSE.  See the GNU General Public License\n+ * version 2 for more details (a copy is included in the LICENSE file that\n+ * accompanied this code).\n+ *\n+ * You should have received a copy of the GNU General Public License version\n+ * 2 along with this work; if not, write to the Free Software Foundation,\n+ * Inc., 51 Franklin St, Fifth Floor, Boston, MA 02110-1301 USA.\n+ *\n+ * Please contact Oracle, 500 Oracle Parkway, Redwood Shores, CA 94065 USA\n+ * or visit www.oracle.com if you need additional information or have any\n+ * questions.\n+ *\/\n+\n+package oracle.code.onnx.proto;\n+\n+import java.io.RandomAccessFile;\n+import java.lang.annotation.ElementType;\n+import java.lang.annotation.Retention;\n+import java.lang.annotation.RetentionPolicy;\n+import java.lang.annotation.Target;\n+import java.lang.reflect.ParameterizedType;\n+import java.lang.reflect.RecordComponent;\n+import java.nio.ByteBuffer;\n+import java.nio.ByteOrder;\n+import java.nio.channels.FileChannel;\n+import java.util.ArrayList;\n+import java.util.Arrays;\n+import java.util.List;\n+import java.util.function.IntSupplier;\n+import java.util.function.Supplier;\n+\n+import oracle.code.onnx.proto.OnnxConstants.*;\n+\n+\/\/ Generated from onnx.in.proto\n+public sealed interface OnnxModel {\n+\n+    \/\/\/ Attributes\n+    \/\/\/\n+    \/\/\/ A named attribute containing either singular float, integer, string, graph,\n+    \/\/\/ and tensor values, or repeated float, integer, string, graph, and tensor values.\n+    \/\/\/ An AttributeProto MUST contain the name field, and *only one* of the\n+    \/\/\/ following content fields, effectively enforcing a C\/C++ union equivalent.\n+    public record AttributeProto (\n+\n+        \/\/\/ The name field MUST be present for this version of the IR.\n+        \/\/\/ namespace Attribute\n+        @f(1) String name,\n+\n+        \/\/\/ if ref_attr_name is not empty, ref_attr_name is the attribute name in parent function.\n+        \/\/\/ In this case, this AttributeProto does not contain data, and it's a reference of attribute\n+        \/\/\/ in parent scope.\n+        \/\/\/ NOTE: This should ONLY be used in function (sub-graph). It's invalid to be used in main graph.\n+        @f(21) String refAttrName,\n+\n+        \/\/\/ A human-readable documentation for this attribute. Markdown is allowed.\n+        @f(13) String docString,\n+\n+        \/\/\/ The type field MUST be present for this version of the IR.\n+        \/\/\/ For 0.0.1 versions of the IR, this field was not defined, and\n+        \/\/\/ implementations needed to use has_field heuristics to determine\n+        \/\/\/ which value field was in use.  For IR_VERSION 0.0.2 or later, this\n+        \/\/\/ field MUST be set and match the f|i|s|t|... field in use.  This\n+        \/\/\/ change was made to accommodate proto3 implementations.\n+        \/\/\/ discriminator that indicates which field below is in use\n+        @f(20) AttributeType type,\n+\n+        \/\/\/ Exactly ONE of the following fields must be present for this version of the IR\n+        \/\/\/ float\n+        @f(2) Float f,\n+\n+        \/\/\/ int\n+        @f(3) Long i,\n+\n+        \/\/\/ UTF-8 string\n+        @f(4) byte[] s,\n+\n+        \/\/\/ tensor value\n+        @f(5) TensorProto t,\n+\n+        \/\/\/ graph\n+        @f(6) GraphProto g,\n+\n+        \/\/\/ sparse tensor value\n+        @f(22) SparseTensorProto sparseTensor,\n+\n+        \/\/\/ Do not use field below, it's deprecated.\n+        \/\/\/ optional ValueProto v = 12;         \/\/ value - subsumes everything but graph\n+        \/\/\/ type proto\n+        @f(14) TypeProto tp,\n+\n+        \/\/\/ list of floats\n+        @f(7) List<float[]> floats,\n+\n+        \/\/\/ list of ints\n+        @f(8) List<long[]> ints,\n+\n+        \/\/\/ list of UTF-8 strings\n+        @f(9) List<byte[]> strings,\n+\n+        \/\/\/ list of tensors\n+        @f(10) List<TensorProto> tensors,\n+\n+        \/\/\/ list of graph\n+        @f(11) List<GraphProto> graphs,\n+\n+        \/\/\/ list of sparse tensors\n+        @f(23) List<SparseTensorProto> sparseTensors,\n+\n+        \/\/\/ list of type protos\n+        @f(15) List<TypeProto> typeProtos) implements OnnxModel {\n+    }\n+\n+    \/\/\/ Defines information on value, including the name, the type, and\n+    \/\/\/ the shape of the value.\n+    public record ValueInfoProto (\n+\n+        \/\/\/ This field MUST be present in this version of the IR.\n+        \/\/\/ namespace Value\n+        @f(1) String name,\n+\n+        \/\/\/ This field MUST be present in this version of the IR for\n+        \/\/\/ inputs and outputs of the top-level graph.\n+        @f(2) TypeProto type,\n+\n+        \/\/\/ A human-readable documentation for this value. Markdown is allowed.\n+        @f(3) String docString,\n+\n+        \/\/\/ Named metadata values; keys should be distinct.\n+        @f(4) List<StringStringEntryProto> metadataProps) implements OnnxModel {\n+    }\n+\n+    \/\/\/ Nodes\n+    \/\/\/\n+    \/\/\/ Computation graphs are made up of a DAG of nodes, which represent what is\n+    \/\/\/ commonly called a \"layer\" or \"pipeline stage\" in machine learning frameworks.\n+    \/\/\/\n+    \/\/\/ For example, it can be a node of type \"Conv\" that takes in an image, a filter\n+    \/\/\/ tensor and a bias tensor, and produces the convolved output.\n+    public record NodeProto (\n+\n+        \/\/\/ namespace Value\n+        @f(1) List<String> input,\n+\n+        \/\/\/ namespace Value\n+        @f(2) List<String> output,\n+\n+        \/\/\/ An optional identifier for this node in a graph.\n+        \/\/\/ This field MAY be absent in this version of the IR.\n+        \/\/\/ namespace Node\n+        @f(3) String name,\n+\n+        \/\/\/ The symbolic identifier of the Operator to execute.\n+        \/\/\/ namespace Operator\n+        @f(4) String opType,\n+\n+        \/\/\/ The domain of the OperatorSet that specifies the operator named by op_type.\n+        \/\/\/ namespace Domain\n+        @f(7) String domain,\n+\n+        \/\/\/ Overload identifier, used only to map this to a model-local function.\n+        @f(8) String overload,\n+\n+        \/\/\/ Additional named attributes.\n+        @f(5) List<AttributeProto> attribute,\n+\n+        \/\/\/ A human-readable documentation for this node. Markdown is allowed.\n+        @f(6) String docString,\n+\n+        \/\/\/ Named metadata values; keys should be distinct.\n+        @f(9) List<StringStringEntryProto> metadataProps) implements OnnxModel {\n+    }\n+\n+    \/\/\/ Training information\n+    \/\/\/ TrainingInfoProto stores information for training a model.\n+    \/\/\/ In particular, this defines two functionalities: an initialization-step\n+    \/\/\/ and a training-algorithm-step. Initialization resets the model\n+    \/\/\/ back to its original state as if no training has been performed.\n+    \/\/\/ Training algorithm improves the model based on input data.\n+    \/\/\/\n+    \/\/\/ The semantics of the initialization-step is that the initializers\n+    \/\/\/ in ModelProto.graph and in TrainingInfoProto.algorithm are first\n+    \/\/\/ initialized as specified by the initializers in the graph, and then\n+    \/\/\/ updated by the \"initialization_binding\" in every instance in\n+    \/\/\/ ModelProto.training_info.\n+    \/\/\/\n+    \/\/\/ The field \"algorithm\" defines a computation graph which represents a\n+    \/\/\/ training algorithm's step. After the execution of a\n+    \/\/\/ TrainingInfoProto.algorithm, the initializers specified by \"update_binding\"\n+    \/\/\/ may be immediately updated. If the targeted training algorithm contains\n+    \/\/\/ consecutive update steps (such as block coordinate descent methods),\n+    \/\/\/ the user needs to create a TrainingInfoProto for each step.\n+    public record TrainingInfoProto (\n+\n+        \/\/\/ This field describes a graph to compute the initial tensors\n+        \/\/\/ upon starting the training process. Initialization graph has no input\n+        \/\/\/ and can have multiple outputs. Usually, trainable tensors in neural\n+        \/\/\/ networks are randomly initialized. To achieve that, for each tensor,\n+        \/\/\/ the user can put a random number operator such as RandomNormal or\n+        \/\/\/ RandomUniform in TrainingInfoProto.initialization.node and assign its\n+        \/\/\/ random output to the specific tensor using \"initialization_binding\".\n+        \/\/\/ This graph can also set the initializers in \"algorithm\" in the same\n+        \/\/\/ TrainingInfoProto; a use case is resetting the number of training\n+        \/\/\/ iteration to zero.\n+        \/\/\/\n+        \/\/\/ By default, this field is an empty graph and its evaluation does not\n+        \/\/\/ produce any output. Thus, no initializer would be changed by default.\n+        @f(1) GraphProto initialization,\n+\n+        \/\/\/ This field represents a training algorithm step. Given required inputs,\n+        \/\/\/ it computes outputs to update initializers in its own or inference graph's\n+        \/\/\/ initializer lists. In general, this field contains loss node, gradient node,\n+        \/\/\/ optimizer node, increment of iteration count.\n+        \/\/\/\n+        \/\/\/ An execution of the training algorithm step is performed by executing the\n+        \/\/\/ graph obtained by combining the inference graph (namely \"ModelProto.graph\")\n+        \/\/\/ and the \"algorithm\" graph. That is, the actual\n+        \/\/\/ input\/initializer\/output\/node\/value_info\/sparse_initializer list of\n+        \/\/\/ the training graph is the concatenation of\n+        \/\/\/ \"ModelProto.graph.input\/initializer\/output\/node\/value_info\/sparse_initializer\"\n+        \/\/\/ and \"algorithm.input\/initializer\/output\/node\/value_info\/sparse_initializer\"\n+        \/\/\/ in that order. This combined graph must satisfy the normal ONNX conditions.\n+        \/\/\/ Now, let's provide a visualization of graph combination for clarity.\n+        \/\/\/ Let the inference graph (i.e., \"ModelProto.graph\") be\n+        \/\/\/    tensor_a, tensor_b -> MatMul -> tensor_c -> Sigmoid -> tensor_d\n+        \/\/\/ and the \"algorithm\" graph be\n+        \/\/\/    tensor_d -> Add -> tensor_e\n+        \/\/\/ The combination process results\n+        \/\/\/    tensor_a, tensor_b -> MatMul -> tensor_c -> Sigmoid -> tensor_d -> Add -> tensor_e\n+        \/\/\/\n+        \/\/\/ Notice that an input of a node in the \"algorithm\" graph may reference the\n+        \/\/\/ output of a node in the inference graph (but not the other way round). Also, inference\n+        \/\/\/ node cannot reference inputs of \"algorithm\". With these restrictions, inference graph\n+        \/\/\/ can always be run independently without training information.\n+        \/\/\/\n+        \/\/\/ By default, this field is an empty graph and its evaluation does not\n+        \/\/\/ produce any output. Evaluating the default training step never\n+        \/\/\/ update any initializers.\n+        @f(2) GraphProto algorithm,\n+\n+        \/\/\/ This field specifies the bindings from the outputs of \"initialization\" to\n+        \/\/\/ some initializers in \"ModelProto.graph.initializer\" and\n+        \/\/\/ the \"algorithm.initializer\" in the same TrainingInfoProto.\n+        \/\/\/ See \"update_binding\" below for details.\n+        \/\/\/\n+        \/\/\/ By default, this field is empty and no initializer would be changed\n+        \/\/\/ by the execution of \"initialization\".\n+        @f(3) List<StringStringEntryProto> initializationBinding,\n+\n+        \/\/\/ Gradient-based training is usually an iterative procedure. In one gradient\n+        \/\/\/ descent iteration, we apply\n+        \/\/\/\n+        \/\/\/ x = x - r * g\n+        \/\/\/\n+        \/\/\/ where \"x\" is the optimized tensor, \"r\" stands for learning rate, and \"g\" is\n+        \/\/\/ gradient of \"x\" with respect to a chosen loss. To avoid adding assignments\n+        \/\/\/ into the training graph, we split the update equation into\n+        \/\/\/\n+        \/\/\/ y = x - r * g\n+        \/\/\/ x = y\n+        \/\/\/\n+        \/\/\/ The user needs to save \"y = x - r * g\" into TrainingInfoProto.algorithm. To\n+        \/\/\/ tell that \"y\" should be assigned to \"x\", the field \"update_binding\" may\n+        \/\/\/ contain a key-value pair of strings, \"x\" (key of StringStringEntryProto)\n+        \/\/\/ and \"y\" (value of StringStringEntryProto).\n+        \/\/\/ For a neural network with multiple trainable (mutable) tensors, there can\n+        \/\/\/ be multiple key-value pairs in \"update_binding\".\n+        \/\/\/\n+        \/\/\/ The initializers appears as keys in \"update_binding\" are considered\n+        \/\/\/ mutable variables. This implies some behaviors\n+        \/\/\/ as described below.\n+        \/\/\/\n+        \/\/\/  1. We have only unique keys in all \"update_binding\"s so that two\n+        \/\/\/     variables may not have the same name. This ensures that one\n+        \/\/\/     variable is assigned up to once.\n+        \/\/\/  2. The keys must appear in names of \"ModelProto.graph.initializer\" or\n+        \/\/\/     \"TrainingInfoProto.algorithm.initializer\".\n+        \/\/\/  3. The values must be output names of \"algorithm\" or \"ModelProto.graph.output\".\n+        \/\/\/  4. Mutable variables are initialized to the value specified by the\n+        \/\/\/     corresponding initializer, and then potentially updated by\n+        \/\/\/     \"initializer_binding\"s and \"update_binding\"s in \"TrainingInfoProto\"s.\n+        \/\/\/\n+        \/\/\/ This field usually contains names of trainable tensors\n+        \/\/\/ (in ModelProto.graph), optimizer states such as momentums in advanced\n+        \/\/\/ stochastic gradient methods (in TrainingInfoProto.graph),\n+        \/\/\/ and number of training iterations (in TrainingInfoProto.graph).\n+        \/\/\/\n+        \/\/\/ By default, this field is empty and no initializer would be changed\n+        \/\/\/ by the execution of \"algorithm\".\n+        @f(4) List<StringStringEntryProto> updateBinding) implements OnnxModel {\n+    }\n+\n+    \/\/\/ Models\n+    \/\/\/\n+    \/\/\/ ModelProto is a top-level file\/container format for bundling a ML model and\n+    \/\/\/ associating its computation graph with metadata.\n+    \/\/\/\n+    \/\/\/ The semantics of the model are described by the associated GraphProto's.\n+    public record ModelProto (\n+\n+        \/\/\/ The version of the IR this model targets. See Version enum above.\n+        \/\/\/ This field MUST be present.\n+        @f(1) Long irVersion,\n+\n+        \/\/\/ The OperatorSets this model relies on.\n+        \/\/\/ All ModelProtos MUST have at least one entry that\n+        \/\/\/ specifies which version of the ONNX OperatorSet is\n+        \/\/\/ being imported.\n+        \/\/\/\n+        \/\/\/ All nodes in the ModelProto's graph will bind against the operator\n+        \/\/\/ with the same-domain\/same-op_type operator with the HIGHEST version\n+        \/\/\/ in the referenced operator sets.\n+        @f(8) List<OperatorSetIdProto> opsetImport,\n+\n+        \/\/\/ The name of the framework or tool used to generate this model.\n+        \/\/\/ This field SHOULD be present to indicate which implementation\/tool\/framework\n+        \/\/\/ emitted the model.\n+        @f(2) String producerName,\n+\n+        \/\/\/ The version of the framework or tool used to generate this model.\n+        \/\/\/ This field SHOULD be present to indicate which implementation\/tool\/framework\n+        \/\/\/ emitted the model.\n+        @f(3) String producerVersion,\n+\n+        \/\/\/ Domain name of the model.\n+        \/\/\/ We use reverse domain names as name space indicators. For example:\n+        \/\/\/ `com.facebook.fair` or `com.microsoft.cognitiveservices`\n+        \/\/\/\n+        \/\/\/ Together with `model_version` and GraphProto.name, this forms the unique identity of\n+        \/\/\/ the graph.\n+        @f(4) String domain,\n+\n+        \/\/\/ The version of the graph encoded. See Version enum below.\n+        @f(5) Long modelVersion,\n+\n+        \/\/\/ A human-readable documentation for this model. Markdown is allowed.\n+        @f(6) String docString,\n+\n+        \/\/\/ The parameterized graph that is evaluated to execute the model.\n+        @f(7) GraphProto graph,\n+\n+        \/\/\/ Named metadata values; keys should be distinct.\n+        @f(14) List<StringStringEntryProto> metadataProps,\n+\n+        \/\/\/ Training-specific information. Sequentially executing all stored\n+        \/\/\/ `TrainingInfoProto.algorithm`s and assigning their outputs following\n+        \/\/\/ the corresponding `TrainingInfoProto.update_binding`s is one training\n+        \/\/\/ iteration. Similarly, to initialize the model\n+        \/\/\/ (as if training hasn't happened), the user should sequentially execute\n+        \/\/\/ all stored `TrainingInfoProto.initialization`s and assigns their outputs\n+        \/\/\/ using `TrainingInfoProto.initialization_binding`s.\n+        \/\/\/\n+        \/\/\/ If this field is empty, the training behavior of the model is undefined.\n+        @f(20) List<TrainingInfoProto> trainingInfo,\n+\n+        \/\/\/ A list of function protos local to the model.\n+        \/\/\/\n+        \/\/\/ The (domain, name, overload) tuple must be unique across the function protos in this list.\n+        \/\/\/ In case of any conflicts the behavior (whether the model local functions are given higher priority,\n+        \/\/\/ or standard operator sets are given higher priotity or this is treated as error) is defined by\n+        \/\/\/ the runtimes.\n+        \/\/\/\n+        \/\/\/ The operator sets imported by FunctionProto should be compatible with the ones\n+        \/\/\/ imported by ModelProto and other model local FunctionProtos.\n+        \/\/\/ Example, if same operator set say 'A' is imported by a FunctionProto and ModelProto\n+        \/\/\/ or by 2 FunctionProtos then versions for the operator set may be different but,\n+        \/\/\/ the operator schema returned for op_type, domain, version combination\n+        \/\/\/ for both the versions should be same for every node in the function body.\n+        \/\/\/\n+        \/\/\/ One FunctionProto can reference other FunctionProto in the model, however, recursive reference\n+        \/\/\/ is not allowed.\n+        @f(25) List<FunctionProto> functions) implements OnnxModel {\n+    }\n+\n+    \/\/\/ StringStringEntryProto follows the pattern for cross-proto-version maps.\n+    \/\/\/ See https:\/\/developers.google.com\/protocol-buffers\/docs\/proto3#maps\n+    public record StringStringEntryProto (\n+\n+        @f(1) String key,\n+\n+        @f(2) String value) implements OnnxModel {\n+    }\n+\n+    public record TensorAnnotation (\n+\n+        @f(1) String tensorName,\n+\n+        \/\/\/ <key, value> pairs to annotate tensor specified by <tensor_name> above.\n+        \/\/\/ The keys used in the mapping below must be pre-defined in ONNX spec.\n+        \/\/\/ For example, for 8-bit linear quantization case, 'SCALE_TENSOR', 'ZERO_POINT_TENSOR' will be pre-defined as\n+        \/\/\/ quantization parameter keys.\n+        @f(2) List<StringStringEntryProto> quantParameterTensorNames) implements OnnxModel {\n+    }\n+\n+    \/\/\/ Graphs\n+    \/\/\/\n+    \/\/\/ A graph defines the computational logic of a model and is comprised of a parameterized\n+    \/\/\/ list of nodes that form a directed acyclic graph based on their inputs and outputs.\n+    \/\/\/ This is the equivalent of the \"network\" or \"graph\" in many deep learning\n+    \/\/\/ frameworks.\n+    public record GraphProto (\n+\n+        \/\/\/ The nodes in the graph, sorted topologically.\n+        @f(1) List<NodeProto> node,\n+\n+        \/\/\/ The name of the graph.\n+        \/\/\/ namespace Graph\n+        @f(2) String name,\n+\n+        \/\/\/ A list of named tensor values, used to specify constant inputs of the graph.\n+        \/\/\/ Each initializer (both TensorProto as well SparseTensorProto) MUST have a name.\n+        \/\/\/ The name MUST be unique across both initializer and sparse_initializer,\n+        \/\/\/ but the name MAY also appear in the input list.\n+        @f(5) List<TensorProto> initializer,\n+\n+        \/\/\/ Initializers (see above) stored in sparse format.\n+        @f(15) List<SparseTensorProto> sparseInitializer,\n+\n+        \/\/\/ A human-readable documentation for this graph. Markdown is allowed.\n+        @f(10) String docString,\n+\n+        \/\/\/ The inputs and outputs of the graph.\n+        @f(11) List<ValueInfoProto> input,\n+\n+        @f(12) List<ValueInfoProto> output,\n+\n+        \/\/\/ Information for the values in the graph. The ValueInfoProto.name's\n+        \/\/\/ must be distinct. It is optional for a value to appear in value_info list.\n+        @f(13) List<ValueInfoProto> valueInfo,\n+\n+        \/\/\/ This field carries information to indicate the mapping among a tensor and its\n+        \/\/\/ quantization parameter tensors. For example:\n+        \/\/\/ For tensor 'a', it may have {'SCALE_TENSOR', 'a_scale'} and {'ZERO_POINT_TENSOR', 'a_zero_point'} annotated,\n+        \/\/\/ which means, tensor 'a_scale' and tensor 'a_zero_point' are scale and zero point of tensor 'a' in the model.\n+        @f(14) List<TensorAnnotation> quantizationAnnotation,\n+\n+        \/\/\/ Named metadata values; keys should be distinct.\n+        @f(16) List<StringStringEntryProto> metadataProps) implements OnnxModel {\n+    }\n+\n+    \/\/\/ Tensors\n+    \/\/\/\n+    \/\/\/ A serialized tensor value.\n+    public record TensorProto (\n+\n+        \/\/\/ The shape of the tensor.\n+        @f(1) List<long[]> dims,\n+\n+        \/\/\/ The data type of the tensor.\n+        \/\/\/ This field MUST have a valid TensorProto.DataType value\n+        @f(2) Integer dataType,\n+\n+        @f(3) Segment segment,\n+\n+        \/\/\/ For float and complex64 values\n+        \/\/\/ Complex64 tensors are encoded as a single array of floats,\n+        \/\/\/ with the real components appearing in odd numbered positions,\n+        \/\/\/ and the corresponding imaginary component appearing in the\n+        \/\/\/ subsequent even numbered position. (e.g., [1.0 + 2.0i, 3.0 + 4.0i]\n+        \/\/\/ is encoded as [1.0, 2.0 ,3.0 ,4.0]\n+        \/\/\/ When this field is present, the data_type field MUST be FLOAT or COMPLEX64.\n+        @f(4) List<float[]> floatData,\n+\n+        \/\/\/ For int32, uint8, int8, uint16, int16, uint4, int4, bool, (b)float16, float8, and float4:\n+        \/\/\/ - (b)float16 and float8 values MUST be converted bit-wise into an unsigned integer\n+        \/\/\/   representation before being written to the buffer.\n+        \/\/\/ - Each pair of uint4, int4, and float4 values MUST be packed as two 4-bit elements into a single byte.\n+        \/\/\/   The first element is stored in the 4 least significant bits (LSB),\n+        \/\/\/   and the second element is stored in the 4 most significant bits (MSB).\n+        \/\/\/\n+        \/\/\/ Consequently:\n+        \/\/\/ - For data types with a bit-width of 8 or greater, each `int32_data` stores one element.\n+        \/\/\/ - For 4-bit data types, each `int32_data` stores two elements.\n+        \/\/\/\n+        \/\/\/ When this field is present, the data_type field MUST be\n+        \/\/\/ INT32, INT16, INT8, INT4, UINT16, UINT8, UINT4, BOOL, FLOAT16, BFLOAT16, FLOAT8E4M3FN, FLOAT8E4M3FNUZ, FLOAT8E5M2, FLOAT8E5M2FNUZ, FLOAT4E2M1\n+        @f(5) List<int[]> int32Data,\n+\n+        \/\/\/ For strings.\n+        \/\/\/ Each element of string_data is a UTF-8 encoded Unicode\n+        \/\/\/ string. No trailing null, no leading BOM. The protobuf \"string\"\n+        \/\/\/ scalar type is not used to match ML community conventions.\n+        \/\/\/ When this field is present, the data_type field MUST be STRING\n+        @f(6) List<byte[]> stringData,\n+\n+        \/\/\/ For int64.\n+        \/\/\/ When this field is present, the data_type field MUST be INT64\n+        @f(7) List<long[]> int64Data,\n+\n+        \/\/\/ Optionally, a name for the tensor.\n+        \/\/\/ namespace Value\n+        @f(8) String name,\n+\n+        \/\/\/ A human-readable documentation for this tensor. Markdown is allowed.\n+        @f(12) String docString,\n+\n+        \/\/\/ Serializations can either use one of the fields above, or use this\n+        \/\/\/ raw bytes field. The only exception is the string case, where one is\n+        \/\/\/ required to store the content in the repeated bytes string_data field.\n+        \/\/\/\n+        \/\/\/ When this raw_data field is used to store tensor value, elements MUST\n+        \/\/\/ be stored in as fixed-width, little-endian order.\n+        \/\/\/ Floating-point data types MUST be stored in IEEE 754 format.\n+        \/\/\/ Complex64 elements must be written as two consecutive FLOAT values, real component first.\n+        \/\/\/ Complex128 elements must be written as two consecutive DOUBLE values, real component first.\n+        \/\/\/ Boolean type MUST be written one byte per tensor element (00000001 for true, 00000000 for false).\n+        \/\/\/ uint4 and int4 values must be packed to 4bitx2, the first element is stored in the 4 LSB and the second element is stored in the 4 MSB.\n+        \/\/\/\n+        \/\/\/ Note: the advantage of specific field rather than the raw_data field is\n+        \/\/\/ that in some cases (e.g. int data), protobuf does a better packing via\n+        \/\/\/ variable length storage, and may lead to smaller binary footprint.\n+        \/\/\/ When this field is present, the data_type field MUST NOT be STRING or UNDEFINED\n+        @f(9) byte[] rawData,\n+\n+        \/\/\/ Data can be stored inside the protobuf file using type-specific fields or raw_data.\n+        \/\/\/ Alternatively, raw bytes data can be stored in an external file, using the external_data field.\n+        \/\/\/ external_data stores key-value pairs describing data location. Recognized keys are:\n+        \/\/\/ - \"location\" (required) - POSIX filesystem path relative to the directory where the ONNX\n+        \/\/\/                           protobuf model was stored\n+        \/\/\/ - \"offset\" (optional) - position of byte at which stored data begins. Integer stored as string.\n+        \/\/\/                         Offset values SHOULD be multiples 4096 (page size) to enable mmap support.\n+        \/\/\/ - \"length\" (optional) - number of bytes containing data. Integer stored as string.\n+        \/\/\/ - \"checksum\" (optional) - SHA1 digest of file specified in under 'location' key.\n+        @f(13) List<StringStringEntryProto> externalData,\n+\n+        \/\/\/ If value not set, data is stored in raw_data (if set) otherwise in type-specified field.\n+        @f(14) DataLocation dataLocation,\n+\n+        \/\/\/ For double\n+        \/\/\/ Complex128 tensors are encoded as a single array of doubles,\n+        \/\/\/ with the real components appearing in odd numbered positions,\n+        \/\/\/ and the corresponding imaginary component appearing in the\n+        \/\/\/ subsequent even numbered position. (e.g., [1.0 + 2.0i, 3.0 + 4.0i]\n+        \/\/\/ is encoded as [1.0, 2.0 ,3.0 ,4.0]\n+        \/\/\/ When this field is present, the data_type field MUST be DOUBLE or COMPLEX128\n+        @f(10) List<double[]> doubleData,\n+\n+        \/\/\/ For uint64 and uint32 values\n+        \/\/\/ When this field is present, the data_type field MUST be\n+        \/\/\/ UINT32 or UINT64\n+        @f(11) List<long[]> uint64Data,\n+\n+        \/\/\/ Named metadata values; keys should be distinct.\n+        @f(16) List<StringStringEntryProto> metadataProps) implements OnnxModel {\n+\n+        \/\/\/ For very large tensors, we may want to store them in chunks, in which\n+        \/\/\/ case the following fields will specify the segment that is stored in\n+        \/\/\/ the current TensorProto.\n+        public record Segment (\n+\n+            @f(1) Long begin,\n+\n+            @f(2) Long end) implements OnnxModel {\n+        }\n+    }\n+\n+    \/\/\/ A serialized sparse-tensor value\n+    public record SparseTensorProto (\n+\n+        \/\/\/ The sequence of non-default values are encoded as a tensor of shape [NNZ].\n+        \/\/\/ The default-value is zero for numeric tensors, and empty-string for string tensors.\n+        \/\/\/ values must have a non-empty name present which serves as a name for SparseTensorProto\n+        \/\/\/ when used in sparse_initializer list.\n+        @f(1) TensorProto values,\n+\n+        \/\/\/ The indices of the non-default values, which may be stored in one of two formats.\n+        \/\/\/ (a) Indices can be a tensor of shape [NNZ, rank] with the [i,j]-th value\n+        \/\/\/ corresponding to the j-th index of the i-th value (in the values tensor).\n+        \/\/\/ (b) Indices can be a tensor of shape [NNZ], in which case the i-th value\n+        \/\/\/ must be the linearized-index of the i-th value (in the values tensor).\n+        \/\/\/ The linearized-index can be converted into an index tuple (k_1,...,k_rank)\n+        \/\/\/ using the shape provided below.\n+        \/\/\/ The indices must appear in ascending order without duplication.\n+        \/\/\/ In the first format, the ordering is lexicographic-ordering:\n+        \/\/\/ e.g., index-value [1,4] must appear before [2,1]\n+        @f(2) TensorProto indices,\n+\n+        \/\/\/ The shape of the underlying dense-tensor: [dim_1, dim_2, ... dim_rank]\n+        @f(3) List<long[]> dims) implements OnnxModel {\n+    }\n+\n+    \/\/\/ Defines a tensor shape. A dimension can be either an integer value\n+    \/\/\/ or a symbolic variable. A symbolic variable represents an unknown\n+    \/\/\/ dimension.\n+    public record TensorShapeProto (\n+\n+        @f(1) List<Dimension> dim) implements OnnxModel {\n+\n+        public record Dimension (\n+\n+            @f(1) Long dimValue,\n+\n+            \/\/\/ namespace Shape\n+            @f(2) String dimParam,\n+\n+            \/\/\/ Standard denotation can optionally be used to denote tensor\n+            \/\/\/ dimensions with standard semantic descriptions to ensure\n+            \/\/\/ that operations are applied to the correct axis of a tensor.\n+            \/\/\/ Refer to https:\/\/github.com\/onnx\/onnx\/blob\/main\/docs\/DimensionDenotation.md#denotation-definition\n+            \/\/\/ for pre-defined dimension denotations.\n+            @f(3) String denotation) implements OnnxModel {\n+        }\n+    }\n+\n+    \/\/\/ Types\n+    \/\/\/\n+    \/\/\/ The standard ONNX data types.\n+    public record TypeProto (\n+\n+        \/\/\/ The type of a tensor.\n+        @f(1) Tensor tensorType,\n+\n+        \/\/\/ The type of a sequence.\n+        @f(4) Sequence sequenceType,\n+\n+        \/\/\/ The type of a map.\n+        @f(5) Map mapType,\n+\n+        \/\/\/ The type of an optional.\n+        @f(9) Optional optionalType,\n+\n+        \/\/\/ Type of the sparse tensor\n+        @f(8) SparseTensor sparseTensorType,\n+\n+        @f(7) Opaque opaqueType,\n+\n+        \/\/\/ An optional denotation can be used to denote the whole\n+        \/\/\/ type with a standard semantic description as to what is\n+        \/\/\/ stored inside. Refer to https:\/\/github.com\/onnx\/onnx\/blob\/main\/docs\/TypeDenotation.md#type-denotation-definition\n+        \/\/\/ for pre-defined type denotations.\n+        @f(6) String denotation) implements OnnxModel {\n+\n+        public record Tensor (\n+\n+            \/\/\/ This field MUST NOT have the value of UNDEFINED\n+            \/\/\/ This field MUST have a valid TensorProto.DataType value\n+            \/\/\/ This field MUST be present for this version of the IR.\n+            @f(1) Integer elemType,\n+\n+            @f(2) TensorShapeProto shape) implements OnnxModel {\n+        }\n+\n+        \/\/\/ repeated T\n+        public record Sequence (\n+\n+            \/\/\/ The type and optional shape of each element of the sequence.\n+            \/\/\/ This field MUST be present for this version of the IR.\n+            @f(1) TypeProto elemType) implements OnnxModel {\n+        }\n+\n+        \/\/\/ map<K,V>\n+        public record Map (\n+\n+            \/\/\/ This field MUST have a valid TensorProto.DataType value\n+            \/\/\/ This field MUST be present for this version of the IR.\n+            \/\/\/ This field MUST refer to an integral type ([U]INT{8|16|32|64}) or STRING\n+            @f(1) Integer keyType,\n+\n+            \/\/\/ This field MUST be present for this version of the IR.\n+            @f(2) TypeProto valueType) implements OnnxModel {\n+        }\n+\n+        \/\/\/ wrapper for Tensor, Sequence, or Map\n+        public record Optional (\n+\n+            \/\/\/ The type and optional shape of the element wrapped.\n+            \/\/\/ This field MUST be present for this version of the IR.\n+            \/\/\/ Possible values correspond to OptionalProto.DataType enum\n+            @f(1) TypeProto elemType) implements OnnxModel {\n+        }\n+\n+        public record SparseTensor (\n+\n+            \/\/\/ This field MUST NOT have the value of UNDEFINED\n+            \/\/\/ This field MUST have a valid TensorProto.DataType value\n+            \/\/\/ This field MUST be present for this version of the IR.\n+            @f(1) Integer elemType,\n+\n+            @f(2) TensorShapeProto shape) implements OnnxModel {\n+        }\n+\n+        public record Opaque (\n+\n+            \/\/\/ When missing, the domain is the same as the model's.\n+            @f(1) String domain,\n+\n+            \/\/\/ The name is optional but significant when provided.\n+            @f(2) String name) implements OnnxModel {\n+        }\n+    }\n+\n+    \/\/\/ Operator Sets\n+    \/\/\/\n+    \/\/\/ OperatorSets are uniquely identified by a (domain, opset_version) pair.\n+    public record OperatorSetIdProto (\n+\n+        \/\/\/ The domain of the operator set being identified.\n+        \/\/\/ The empty string (\"\") or absence of this field implies the operator\n+        \/\/\/ set that is defined as part of the ONNX specification.\n+        \/\/\/ This field MUST be present in this version of the IR when referring to any other operator set.\n+        @f(1) String domain,\n+\n+        \/\/\/ The version of the operator set being identified.\n+        \/\/\/ This field MUST be present in this version of the IR.\n+        @f(2) Long version) implements OnnxModel {\n+    }\n+\n+    public record FunctionProto (\n+\n+        \/\/\/ The name of the function, similar to op_type in NodeProto.\n+        \/\/\/ This is part of the unique-id (domain, name, overload) of FunctionProtos in a model.\n+        @f(1) String name,\n+\n+        \/\/\/ The inputs and outputs of the function.\n+        @f(4) List<String> input,\n+\n+        @f(5) List<String> output,\n+\n+        \/\/\/ The attribute parameters of the function.\n+        \/\/\/ It is for function parameters without default values.\n+        @f(6) List<String> attribute,\n+\n+        \/\/\/ The attribute protos of the function.\n+        \/\/\/ It is for function attributes with default values.\n+        \/\/\/ A function attribute shall be represented either as\n+        \/\/\/ a string attribute or an AttributeProto, not both.\n+        @f(11) List<AttributeProto> attributeProto,\n+\n+        \/\/\/ The nodes in the function.\n+        @f(7) List<NodeProto> node,\n+\n+        \/\/\/ A human-readable documentation for this function. Markdown is allowed.\n+        @f(8) String docString,\n+\n+        @f(9) List<OperatorSetIdProto> opsetImport,\n+\n+        \/\/\/ The domain which this function belongs to.\n+        \/\/\/ This is part of the unique-id (domain, name, overload) of FunctionProtos in a model.\n+        @f(10) String domain,\n+\n+        \/\/\/ The overload identifier of the function.\n+        \/\/\/ This is part of the unique-id (domain, name, overload) of FunctionProtos in a model.\n+        @f(13) String overload,\n+\n+        \/\/\/ Information for the values in the function. The ValueInfoProto.name's\n+        \/\/\/ must be distinct and refer to names in the function (including inputs,\n+        \/\/\/ outputs, and intermediate values). It is optional for a value to appear\n+        \/\/\/ in value_info list.\n+        @f(12) List<ValueInfoProto> valueInfo,\n+\n+        \/\/\/ Named metadata values; keys should be distinct.\n+        @f(14) List<StringStringEntryProto> metadataProps) implements OnnxModel {\n+    }\n+\n+    \/\/ Implementation\n+\n+\n+    @Retention(RetentionPolicy.RUNTIME)\n+    @Target(ElementType.RECORD_COMPONENT)\n+    @interface f {\n+        int value();\n+    }\n+\n+    private static long decodeVarint(ByteBuffer data) {\n+        long i, shift = 0, value = 0;\n+        do {\n+            value |= ((i = data.get()) & 0x7f) << shift;\n+            shift += 7;\n+        } while ((i & 0x80) != 0);\n+        return value;\n+    }\n+\n+    private static int countVarInts(ByteBuffer data) {\n+        long end  = decodeVarint(data);\n+        int start = data.position();\n+        end += start;\n+        int count = 0;\n+        while (data.position() < end) {\n+            if ((data.get() & 0x80) == 0) count++;\n+        }\n+        data.position(start);\n+        return count;\n+    }\n+\n+    private static int[] readPackedInts(ByteBuffer data) {\n+        var ret = new int[countVarInts(data)];\n+        for (int i = 0; i < ret.length; i++) {\n+            ret[i] = (int)decodeVarint(data);\n+        }\n+        return ret;\n+    }\n+\n+    private static long[] readPackedLongs(ByteBuffer data) {\n+        var ret = new long[countVarInts(data)];\n+        for (int i = 0; i < ret.length; i++) {\n+            ret[i] = decodeVarint(data);\n+        }\n+        return ret;\n+    }\n+\n+    private static float[] readPackedFloats(ByteBuffer data) {\n+        var ret = new float[(int)(decodeVarint(data)\/4)];\n+        for (int i = 0; i < ret.length; i++) {\n+            ret[i] = data.getFloat();\n+        }\n+        return ret;\n+    }\n+\n+    private static double[] readPackedDoubles(ByteBuffer data) {\n+        var ret = new double[(int)(decodeVarint(data)\/8)];\n+        for (int i = 0; i < ret.length; i++) {\n+            ret[i] = data.getDouble();\n+        }\n+        return ret;\n+    }\n+\n+    private static byte[] readBytes(ByteBuffer data) {\n+        var bytes = new byte[(int)decodeVarint(data)];\n+        data.get(bytes);\n+        return bytes;\n+    }\n+\n+    private static Object readData(Class<?> baseType, boolean packed, ByteBuffer bb) {\n+        if (baseType == Integer.class) {\n+            return (int)decodeVarint(bb);\n+        } else if (baseType == int[].class) {\n+            return packed ? readPackedInts(bb) : new int[]{(int)decodeVarint(bb)};\n+        } else if (baseType == Long.class) {\n+            return decodeVarint(bb);\n+        } else if (baseType == long[].class) {\n+            return packed ? readPackedLongs(bb) : new long[]{decodeVarint(bb)};\n+        } else if (baseType == Float.class) {\n+            return bb.getFloat();\n+        } else if (baseType == float[].class) {\n+            return packed ? readPackedFloats(bb) : new float[] {bb.getFloat()};\n+        } else if (baseType == Double.class) {\n+            return bb.getDouble();\n+        } else if (baseType == double[].class) {\n+            return packed ? readPackedDoubles(bb) : new double[] {bb.getDouble()};\n+        } else if (baseType == byte[].class) {\n+            return readBytes(bb);\n+        } else if (baseType == String.class) {\n+            return new String(readBytes(bb));\n+        } else if (baseType.getEnclosingClass() == OnnxConstants.class) {\n+            int value = (int)decodeVarint(bb);\n+            for (Object cs : baseType.getEnumConstants()) {\n+                if (cs instanceof IntSupplier is && is.getAsInt() == value) {\n+                    return cs;\n+                }\n+            }\n+            throw new IllegalArgumentException(baseType.toString());\n+        } else {\n+            var size = decodeVarint(bb);\n+            int limit = bb.limit();\n+            var data = readFrom((Class<Record>)baseType, bb.limit(bb.position() + (int)size));\n+            bb.limit(limit);\n+            return data;\n+        }\n+    }\n+\n+    private static int getRecordFieldIndex(RecordComponent[] rcs, int fieldIndex) {\n+        for (int i = 0; i < rcs.length; i++) {\n+            if (rcs[i].getAnnotation(f.class).value() == fieldIndex) {\n+                return i;\n+            }\n+        }\n+        throw new IllegalArgumentException(\"Field index \" + fieldIndex + \" not found in \" + rcs[0].getDeclaringRecord());\n+    }\n+\n+    private static <T> T readFrom(Class<T> type, ByteBuffer bb) {\n+        Object[] fieldsData = new Object[type.getRecordComponents().length];\n+        while (bb.remaining() > 0) {\n+            long tag = decodeVarint(bb);\n+            RecordComponent[] rcs = type.getRecordComponents();\n+            int rfi = getRecordFieldIndex(rcs, (int)tag >> 3);\n+            boolean packed = (tag & 7) == 2;\n+            RecordComponent rc = rcs[rfi];\n+            Class<?> rcType = rc.getType();\n+            if (rcType == List.class) {\n+                List list;\n+                if (fieldsData[rfi] instanceof List l) {\n+                    list = l;\n+                } else {\n+                    list = new ArrayList();\n+                    fieldsData[rfi] = list;\n+                }\n+                Class baseType = (Class)((ParameterizedType)rc.getGenericType()).getActualTypeArguments()[0];\n+                list.add(readData(baseType, packed, bb));\n+            } else {\n+                fieldsData[rfi] = readData(rcType, packed, bb);\n+            }\n+        }\n+        try {\n+            return (T)type.getDeclaredConstructors()[0].newInstance(fieldsData);\n+        } catch (ReflectiveOperationException e) {\n+            throw new RuntimeException(e);\n+        }\n+    }\n+\n+    private static void print(StringBuilder out, int indent, String name, Object value, boolean skipBigData) throws ReflectiveOperationException {\n+        if (value == null) return;\n+        out.append(\"  \".repeat(indent)).append(name);\n+        switch (value) {\n+            case List l -> {\n+                out.append(name.endsWith(\"s\") ? \":\" : \"s:\").append(System.lineSeparator());\n+                for (var el : l) print(out, indent + 1, \"- \" + (name.endsWith(\"s\") ? name.substring(0, name.length() - 1) : name), el, skipBigData);\n+            }\n+            case Record r -> {\n+                out.append(':').append(System.lineSeparator());\n+                for (var rc : r.getClass().getRecordComponents()) {\n+                    print(out, indent + 2, rc.getName(), rc.getAccessor().invoke(r), skipBigData);\n+                }\n+            }\n+            case byte[] a ->\n+                out.append(checkSize(a.length, () -> Arrays.toString(a), skipBigData));\n+            case long[] a ->\n+                out.append(checkSize(a.length, () -> Arrays.toString(a), skipBigData));\n+            case float[] a ->\n+                out.append(checkSize(a.length, () -> Arrays.toString(a), skipBigData));\n+            case double[] a ->\n+                out.append(checkSize(a.length, () -> Arrays.toString(a), skipBigData));\n+            case String s ->\n+                out.append(\": \\\"\").append(s).append('\"').append(System.lineSeparator());\n+            default ->\n+                out.append(\": \").append(value).append(System.lineSeparator());\n+        }\n+    }\n+\n+    static final int SKIP_LIMIT = 1000;\n+\n+    private static String checkSize(int size, Supplier<String> sup, boolean skipBigData) {\n+        return \": \" + (skipBigData && size > SKIP_LIMIT ? \"# skipped \" + size + \" values\" : sup.get()) + System.lineSeparator();\n+    }\n+\n+    default String toText() {\n+        return toText(true);\n+    }\n+\n+    default String toText(boolean skipBigData) {\n+        try {\n+            var sb = new StringBuilder();\n+            print(sb, 0, \"OnnxModel\", this, skipBigData);\n+            return sb.toString();\n+        } catch (ReflectiveOperationException e) {\n+            throw new RuntimeException(e);\n+        }\n+    }\n+\n+    public static OnnxModel.ModelProto readFrom(byte[] onnxProtoModel) {\n+        return readFrom(ByteBuffer.wrap(onnxProtoModel));\n+    }\n+\n+    public static OnnxModel.ModelProto readFrom(ByteBuffer onnxProtoModel) {\n+        return readFrom(OnnxModel.ModelProto.class, onnxProtoModel.order(ByteOrder.LITTLE_ENDIAN));\n+    }\n+\n+    public static void main(String... args) throws Exception {\n+        for (var fName : args) {\n+            try (var in = new RandomAccessFile(fName, \"r\")) {\n+                OnnxModel.ModelProto model = readFrom(in.getChannel().map(FileChannel.MapMode.READ_ONLY, 0, in.length()));\n+                System.out.println(model.toText());\n+            }\n+        }\n+    }\n+}\n","filename":"cr-examples\/onnx\/src\/main\/java\/oracle\/code\/onnx\/proto\/OnnxModel.java","additions":976,"deletions":0,"binary":false,"changes":976,"status":"added"},{"patch":"@@ -1,416 +0,0 @@\n-\/*\n- * Copyright (c) 2025, Oracle and\/or its affiliates. All rights reserved.\n- * DO NOT ALTER OR REMOVE COPYRIGHT NOTICES OR THIS FILE HEADER.\n- *\n- * This code is free software; you can redistribute it and\/or modify it\n- * under the terms of the GNU General Public License version 2 only, as\n- * published by the Free Software Foundation.  Oracle designates this\n- * particular file as subject to the \"Classpath\" exception as provided\n- * by Oracle in the LICENSE file that accompanied this code.\n- *\n- * This code is distributed in the hope that it will be useful, but WITHOUT\n- * ANY WARRANTY; without even the implied warranty of MERCHANTABILITY or\n- * FITNESS FOR A PARTICULAR PURPOSE.  See the GNU General Public License\n- * version 2 for more details (a copy is included in the LICENSE file that\n- * accompanied this code).\n- *\n- * You should have received a copy of the GNU General Public License version\n- * 2 along with this work; if not, write to the Free Software Foundation,\n- * Inc., 51 Franklin St, Fifth Floor, Boston, MA 02110-1301 USA.\n- *\n- * Please contact Oracle, 500 Oracle Parkway, Redwood Shores, CA 94065 USA\n- * or visit www.oracle.com if you need additional information or have any\n- * questions.\n- *\/\n-\n-package oracle.code.onnx.proto;\n-\n-import java.io.RandomAccessFile;\n-import java.lang.annotation.ElementType;\n-import java.lang.annotation.Retention;\n-import java.lang.annotation.RetentionPolicy;\n-import java.lang.annotation.Target;\n-import java.lang.reflect.ParameterizedType;\n-import java.lang.reflect.RecordComponent;\n-import java.nio.ByteBuffer;\n-import java.nio.ByteOrder;\n-import java.nio.channels.FileChannel;\n-import java.util.ArrayList;\n-import java.util.Arrays;\n-import java.util.List;\n-import java.util.function.Supplier;\n-\n-public record OnnxProtoModel (\n-        @f(1) Long irVersion,\n-        @f(2) String producerName,\n-        @f(3) String producerVersion,\n-        @f(4) String domain,\n-        @f(5) Long modelVersion,\n-        @f(6) String docString,\n-        @f(7) GraphProto graph,\n-        @f(8) List<OperatorSetIdProto> opsetImports,\n-        @f(14) List<StringStringEntryProto> metadataProps,\n-        @f(20) List<TrainingInfoProto> trainingInfos,\n-        @f(25) List<FunctionProto> functions) {\n-\n-    public record Attribute (\n-            @f(1) String name,\n-            @f(2) Float f,\n-            @f(3) Long i,\n-            @f(4) byte[] s,\n-            @f(5) TensorProto t,\n-            @f(6) GraphProto g,\n-            @f(7) List<float[]> floats,\n-            @f(8) List<long[]> ints,\n-            @f(9) List<byte[]> strings,\n-            @f(10) List<TensorProto> tensors,\n-            @f(11) List<GraphProto> graphs,\n-            @f(13) String docString,\n-            @f(14) TypeProto tp,\n-            @f(15) List<TypeProto> typeProtos,\n-            @f(20) Integer type,\n-            @f(21) String refAttrName,\n-            @f(22) SparseTensorProto sparseTensor,\n-            @f(23) List<SparseTensorProto> sparseTensors) {\n-    }\n-\n-    public record ValueInfoProto (\n-            @f(1) String name,\n-            @f(2) TypeProto type,\n-            @f(3) String docString,\n-            @f(4) List<StringStringEntryProto> metadataProps) {\n-    }\n-\n-    public record NodeProto (\n-            @f(1) List<String> inputs,\n-            @f(2) List<String> outputs,\n-            @f(3) String name,\n-            @f(4) String opType,\n-            @f(5) List<Attribute> attributes,\n-            @f(6) String docString,\n-            @f(7) String domain,\n-            @f(8) String overload,\n-            @f(9) List<StringStringEntryProto> metadataProps) {\n-    }\n-\n-    public record TrainingInfoProto (\n-            @f(1) GraphProto initialization,\n-            @f(2) GraphProto algorithm,\n-            @f(3) List<StringStringEntryProto> initializationBindings,\n-            @f(4) List<StringStringEntryProto> updateBindings) {\n-    }\n-\n-    public record StringStringEntryProto (\n-            @f(1) String key,\n-            @f(2) String value) {\n-    }\n-\n-    public record TensorAnnotation (\n-            @f(1) String tensorName,\n-            @f(2) List<StringStringEntryProto> quantParameterTensorNames) {\n-    }\n-\n-    public record GraphProto (\n-            @f(1) List<NodeProto> nodes,\n-            @f(2) String name,\n-            @f(5) List<TensorProto> initializers,\n-            @f(10) String docString,\n-            @f(11) List<ValueInfoProto> inputs,\n-            @f(12) List<ValueInfoProto> outputs,\n-            @f(13) List<ValueInfoProto> valueInfos,\n-            @f(14) List<TensorAnnotation> quantizationAnnotations,\n-            @f(15) List<SparseTensorProto> sparseInitializers,\n-            @f(16) List<StringStringEntryProto> metadataProps) {\n-    }\n-\n-    public record TensorProto (\n-            @f(1) List<long[]> dims,\n-            @f(2) Integer dataType,\n-            @f(3) Segment segment,\n-            @f(4) List<float[]> floatData,\n-            @f(5) List<int[]> int32Data,\n-            @f(6) List<byte[]> stringData,\n-            @f(7) List<long[]> int64Data,\n-            @f(8) String name,\n-            @f(9) byte[] rawData,\n-            @f(10) List<double[]> doubleData,\n-            @f(11) List<long[]> uint64Data,\n-            @f(12) String docString,\n-            @f(13) List<StringStringEntryProto> externalData,\n-            @f(14) Long dataLocation,\n-            @f(16) List<StringStringEntryProto> metadataProps) {\n-\n-        public record Segment (\n-                @f(1) Long begin,\n-                @f(2) Long end) {\n-        }\n-    }\n-\n-    public record SparseTensorProto (\n-            @f(1) TensorProto values,\n-            @f(2) TensorProto indices,\n-            @f(3) List<long[]> dims) {\n-    }\n-\n-    public record TensorShapeProto (\n-            @f(1) List<Dimension> dims) {\n-\n-        public record Dimension (\n-                @f(1) Long dimValue,\n-                @f(2) String dimParam,\n-                @f(3) String denotation) {\n-        }\n-    }\n-\n-    public record TypeProto (\n-            @f(1) Tensor tensorType,\n-            @f(4) Sequence sequenceType,\n-            @f(5) Map mapType,\n-            @f(6) String denotation,\n-            @f(8) SparseTensor sparseTensorType,\n-            @f(9) Optional optionalType) {\n-\n-        public record Tensor (\n-                @f(1) Integer elemType,\n-                @f(2) TensorShapeProto shape) {\n-        }\n-\n-        public record Sequence (\n-                @f(1) TypeProto elemType) {\n-        }\n-\n-        public record Map (\n-                @f(1) Integer keyType,\n-                @f(2) TypeProto valueType) {\n-        }\n-\n-        public record Optional (\n-                @f(1) TypeProto elemType) {\n-        }\n-\n-        public record SparseTensor (\n-                @f(1) Integer elemType,\n-                @f(2) TensorShapeProto shape) {\n-        }\n-    }\n-\n-    public record OperatorSetIdProto (\n-            @f(1) String domain,\n-            @f(2) Long version) {\n-    }\n-\n-    public record FunctionProto (\n-            @f(1) String name,\n-            @f(4) List<String> inputs,\n-            @f(5) List<String> outputs,\n-            @f(6) List<String> attributes,\n-            @f(7) List<NodeProto> nodes,\n-            @f(8) String docString,\n-            @f(9) List<OperatorSetIdProto> opsetImports,\n-            @f(10) String domain,\n-            @f(11) List<Attribute> attributeProtos,\n-            @f(12) List<ValueInfoProto> valueInfos,\n-            @f(13) String overload,\n-            @f(14) List<StringStringEntryProto> metadataProps) {\n-    }\n-\n-    @Retention(RetentionPolicy.RUNTIME)\n-    @Target(ElementType.RECORD_COMPONENT)\n-    @interface f {\n-        int value();\n-    }\n-\n-    private static long decodeVarint(ByteBuffer data) {\n-        long i, shift = 0, value = 0;\n-        do {\n-            value |= ((i = data.get()) & 0x7f) << shift;\n-            shift += 7;\n-        } while ((i & 0x80) != 0);\n-        return value;\n-    }\n-\n-    private static int countVarInts(ByteBuffer data) {\n-        long end  = decodeVarint(data);\n-        int start = data.position();\n-        end += start;\n-        int count = 0;\n-        while (data.position() < end) {\n-            if ((data.get() & 0x80) == 0) count++;\n-        }\n-        data.position(start);\n-        return count;\n-    }\n-\n-    private static int[] readPackedInts(ByteBuffer data) {\n-        var ret = new int[countVarInts(data)];\n-        for (int i = 0; i < ret.length; i++) {\n-            ret[i] = (int)decodeVarint(data);\n-        }\n-        return ret;\n-    }\n-\n-    private static long[] readPackedLongs(ByteBuffer data) {\n-        var ret = new long[countVarInts(data)];\n-        for (int i = 0; i < ret.length; i++) {\n-            ret[i] = decodeVarint(data);\n-        }\n-        return ret;\n-    }\n-\n-    private static float[] readPackedFloats(ByteBuffer data) {\n-        var ret = new float[(int)(decodeVarint(data)\/4)];\n-        for (int i = 0; i < ret.length; i++) {\n-            ret[i] = data.getFloat();\n-        }\n-        return ret;\n-    }\n-\n-    private static double[] readPackedDoubles(ByteBuffer data) {\n-        var ret = new double[(int)(decodeVarint(data)\/8)];\n-        for (int i = 0; i < ret.length; i++) {\n-            ret[i] = data.getDouble();\n-        }\n-        return ret;\n-    }\n-\n-    private static byte[] readBytes(ByteBuffer data) {\n-        var bytes = new byte[(int)decodeVarint(data)];\n-        data.get(bytes);\n-        return bytes;\n-    }\n-\n-    private static Object readData(Class<?> baseType, boolean packed, ByteBuffer bb) {\n-        if (baseType == Integer.class) {\n-            return (int)decodeVarint(bb);\n-        } else if (baseType == int[].class) {\n-            return packed ? readPackedInts(bb) : new int[]{(int)decodeVarint(bb)};\n-        } else if (baseType == Long.class) {\n-            return decodeVarint(bb);\n-        } else if (baseType == long[].class) {\n-            return packed ? readPackedLongs(bb) : new long[]{decodeVarint(bb)};\n-        } else if (baseType == Float.class) {\n-            return bb.getFloat();\n-        } else if (baseType == float[].class) {\n-            return packed ? readPackedFloats(bb) : new float[] {bb.getFloat()};\n-        } else if (baseType == Double.class) {\n-            return bb.getDouble();\n-        } else if (baseType == double[].class) {\n-            return packed ? readPackedDoubles(bb) : new double[] {bb.getDouble()};\n-        } else if (baseType == byte[].class) {\n-            return readBytes(bb);\n-        } else if (baseType == String.class) {\n-            return new String(readBytes(bb));\n-        } else {\n-            var size = decodeVarint(bb);\n-            int limit = bb.limit();\n-            var data = readFrom((Class<Record>)baseType, bb.limit(bb.position() + (int)size));\n-            bb.limit(limit);\n-            return data;\n-        }\n-    }\n-\n-    private static int getRecordFieldIndex(RecordComponent[] rcs, int fieldIndex) {\n-        for (int i = 0; i < rcs.length; i++) {\n-            if (rcs[i].getAnnotation(f.class).value() == fieldIndex) {\n-                return i;\n-            }\n-        }\n-        throw new IllegalArgumentException(\"Field index \" + fieldIndex + \" not found in \" + rcs[0].getDeclaringRecord());\n-    }\n-\n-    private static <T> T readFrom(Class<T> type, ByteBuffer bb) {\n-        Object[] fieldsData = new Object[type.getRecordComponents().length];\n-        while (bb.remaining() > 0) {\n-            long tag = decodeVarint(bb);\n-            RecordComponent[] rcs = type.getRecordComponents();\n-            int rfi = getRecordFieldIndex(rcs, (int)tag >> 3);\n-            boolean packed = (tag & 7) == 2;\n-            RecordComponent rc = rcs[rfi];\n-            Class<?> rcType = rc.getType();\n-            if (rcType == List.class) {\n-                List list;\n-                if (fieldsData[rfi] instanceof List l) {\n-                    list = l;\n-                } else {\n-                    list = new ArrayList();\n-                    fieldsData[rfi] = list;\n-                }\n-                Class baseType = (Class)((ParameterizedType)rc.getGenericType()).getActualTypeArguments()[0];\n-                list.add(readData(baseType, packed, bb));\n-            } else {\n-                fieldsData[rfi] = readData(rcType, packed, bb);\n-            }\n-        }\n-        try {\n-            return (T)type.getDeclaredConstructors()[0].newInstance(fieldsData);\n-        } catch (ReflectiveOperationException e) {\n-            throw new RuntimeException(e);\n-        }\n-    }\n-\n-    private static void print(StringBuilder out, int indent, String name, Object value, boolean skipBigData) throws ReflectiveOperationException {\n-        if (value == null) return;\n-        out.append(\"  \".repeat(indent)).append(name);\n-        switch (value) {\n-            case List l -> {\n-                out.append(name.endsWith(\"s\") ? \":\" : \"s:\").append(System.lineSeparator());\n-                for (var el : l) print(out, indent + 1, \"- \" + (name.endsWith(\"s\") ? name.substring(0, name.length() - 1) : name), el, skipBigData);\n-            }\n-            case Record r -> {\n-                out.append(':').append(System.lineSeparator());\n-                for (var rc : r.getClass().getRecordComponents()) {\n-                    print(out, indent + 2, rc.getName(), rc.getAccessor().invoke(r), skipBigData);\n-                }\n-            }\n-            case byte[] a ->\n-                out.append(checkSize(a.length, () -> Arrays.toString(a), skipBigData));\n-            case long[] a ->\n-                out.append(checkSize(a.length, () -> Arrays.toString(a), skipBigData));\n-            case float[] a ->\n-                out.append(checkSize(a.length, () -> Arrays.toString(a), skipBigData));\n-            case double[] a ->\n-                out.append(checkSize(a.length, () -> Arrays.toString(a), skipBigData));\n-            case String s ->\n-                out.append(\": \\\"\").append(s).append('\"').append(System.lineSeparator());\n-            default ->\n-                out.append(\": \").append(value).append(System.lineSeparator());\n-        }\n-    }\n-\n-    private static final int SKIP_LIMIT = 1000;\n-\n-    private static String checkSize(int size, Supplier<String> sup, boolean skipBigData) {\n-        return \": \" + (skipBigData && size > SKIP_LIMIT ? \"# skipped \" + size + \" values\" : sup.get()) + System.lineSeparator();\n-    }\n-\n-    public String toText() {\n-        return toText(true);\n-    }\n-\n-    public String toText(boolean skipBigData) {\n-        try {\n-            var sb = new StringBuilder();\n-            print(sb, 0, \"OnnxProtoModel\", this, skipBigData);\n-            return sb.toString();\n-        } catch (ReflectiveOperationException e) {\n-            throw new RuntimeException(e);\n-        }\n-    }\n-\n-    public static OnnxProtoModel readFrom(byte[] onnxProtoModel) {\n-        return readFrom(ByteBuffer.wrap(onnxProtoModel));\n-    }\n-\n-    public static OnnxProtoModel readFrom(ByteBuffer onnxProtoModel) {\n-        return readFrom(OnnxProtoModel.class, onnxProtoModel.order(ByteOrder.LITTLE_ENDIAN));\n-    }\n-\n-    public static void main(String... args) throws Exception {\n-        for (var fName : args) {\n-            try (var in = new RandomAccessFile(fName, \"r\")) {\n-                OnnxProtoModel model = readFrom(in.getChannel().map(FileChannel.MapMode.READ_ONLY, 0, in.length()));\n-                System.out.println(model.toText());\n-            }\n-        }\n-    }\n-}\n","filename":"cr-examples\/onnx\/src\/main\/java\/oracle\/code\/onnx\/proto\/OnnxProtoModel.java","additions":0,"deletions":416,"binary":false,"changes":416,"status":"deleted"},{"patch":"@@ -32,0 +32,1 @@\n+import static oracle.code.onnx.proto.OnnxBuilder.*;\n@@ -152,1 +153,1 @@\n-                            .opset_import(new OperatorSetIdProto().version(OPSET_VERSION))\n+                            .opsetImport(new OperatorSetIdProto().version(OPSET_VERSION))\n","filename":"cr-examples\/onnx\/src\/test\/java\/oracle\/code\/onnx\/RuntimeTest.java","additions":2,"deletions":1,"binary":false,"changes":3,"status":"modified"},{"patch":"@@ -0,0 +1,338 @@\n+\/*\n+ * Copyright (c) 2025, Oracle and\/or its affiliates. All rights reserved.\n+ * DO NOT ALTER OR REMOVE COPYRIGHT NOTICES OR THIS FILE HEADER.\n+ *\n+ * This code is free software; you can redistribute it and\/or modify it\n+ * under the terms of the GNU General Public License version 2 only, as\n+ * published by the Free Software Foundation.\n+ *\n+ * This code is distributed in the hope that it will be useful, but WITHOUT\n+ * ANY WARRANTY; without even the implied warranty of MERCHANTABILITY or\n+ * FITNESS FOR A PARTICULAR PURPOSE.  See the GNU General Public License\n+ * version 2 for more details (a copy is included in the LICENSE file that\n+ * accompanied this code).\n+ *\n+ * You should have received a copy of the GNU General Public License version\n+ * 2 along with this work; if not, write to the Free Software Foundation,\n+ * Inc., 51 Franklin St, Fifth Floor, Boston, MA 02110-1301 USA.\n+ *\n+ * Please contact Oracle, 500 Oracle Parkway, Redwood Shores, CA 94065 USA\n+ * or visit www.oracle.com if you need additional information or have any\n+ * questions.\n+ *\/\n+\n+package oracle.code.onnx.proto;\n+\n+import java.io.InputStream;\n+import java.io.RandomAccessFile;\n+import java.lang.foreign.Arena;\n+import java.lang.foreign.ValueLayout;\n+import java.nio.ByteOrder;\n+import java.nio.channels.FileChannel;\n+import java.util.ArrayList;\n+import java.util.Arrays;\n+import java.util.HashMap;\n+import java.util.Iterator;\n+import java.util.LinkedHashMap;\n+import java.util.List;\n+import java.util.Map;\n+import java.util.SequencedSet;\n+import java.util.stream.Collectors;\n+import jdk.incubator.code.Block;\n+import jdk.incubator.code.Op;\n+import jdk.incubator.code.TypeElement;\n+import jdk.incubator.code.Value;\n+import jdk.incubator.code.op.CoreOp;\n+import jdk.incubator.code.op.ExternalizableOp;\n+import jdk.incubator.code.op.OpFactory;\n+import jdk.incubator.code.type.FunctionType;\n+import jdk.incubator.code.type.TupleType;\n+import jdk.incubator.code.writer.OpWriter;\n+import oracle.code.onnx.CNNTest;\n+import oracle.code.onnx.OnnxRuntime;\n+import oracle.code.onnx.Tensor;\n+import oracle.code.onnx.ir.OnnxOp;\n+import oracle.code.onnx.ir.OnnxOps;\n+import oracle.code.onnx.ir.OnnxType;\n+import org.junit.jupiter.api.Test;\n+\n+\n+public class OnnxModelTest {\n+\n+    static OnnxType toOnnxType(OnnxModel.TypeProto tp) {\n+        if (tp.tensorType() instanceof OnnxModel.TypeProto.Tensor t) {\n+            return toTensorType(t.elemType());\n+        } else if (tp.optionalType() instanceof OnnxModel.TypeProto.Optional o) {\n+            return OnnxType.optional(toOnnxType(o.elemType()));\n+        } else if (tp.sequenceType()  instanceof OnnxModel.TypeProto.Sequence s) {\n+            return OnnxType.seq(toOnnxType(s.elemType()));\n+        } else if (tp.mapType() instanceof OnnxModel.TypeProto.Map m) {\n+            return OnnxType.map(toKeyType(m.keyType()), toOnnxType(m.valueType()));\n+        } else if (tp.sparseTensorType() instanceof OnnxModel.TypeProto.SparseTensor st) {\n+            throw new UnsupportedOperationException(\"Sparse tensors not supported yet.\"); \/\/ @@@\n+        }\n+        throw new IllegalArgumentException(\"No type specified.\");\n+    }\n+\n+    static FunctionType toFunctionType(OnnxModel.GraphProto g) {\n+        var paramTypes = new ArrayList<TypeElement>();\n+        for (OnnxModel.ValueInfoProto input : g.input()) {\n+            paramTypes.add(toOnnxType(input.type()));\n+        }\n+        for (OnnxModel.TensorProto init : g.initializer()) {\n+            paramTypes.add(toTensorType(init.dataType()));\n+        }\n+        var returnType = g.output().size() == 1\n+                ? toOnnxType(g.output().getFirst().type())\n+                : TupleType.tupleType(g.output().stream().map(OnnxModel.ValueInfoProto::type).map(OnnxModelTest::toOnnxType).toList());\n+        return FunctionType.functionType(returnType, paramTypes);\n+    }\n+\n+    static OnnxType toKeyType(int kt) {\n+        return switch (kt) {\n+            case 2 -> OnnxType.UINT8;\n+            case 3 -> OnnxType.INT8;\n+            case 4 -> OnnxType.UINT16;\n+            case 5 -> OnnxType.INT16;\n+            case 6 -> OnnxType.INT32;\n+            case 7 -> OnnxType.INT64;\n+            case 8 -> OnnxType.STRING;\n+            case 12 -> OnnxType.UINT32;\n+            case 13 -> OnnxType.UINT64;\n+            default -> throw new IllegalArgumentException(\"Invalid key type: \" + kt);\n+        };\n+    }\n+\n+    static OnnxType.TensorType toTensorType(int tt) {\n+        return switch (tt) {\n+            case 1 -> OnnxType.TENSOR_FLOAT32;\n+            case 2 -> OnnxType.TENSOR_UINT8;\n+            case 3 -> OnnxType.TENSOR_INT8;\n+            case 4 -> OnnxType.TENSOR_UINT16;\n+            case 5 -> OnnxType.TENSOR_INT16;\n+            case 6 -> OnnxType.TENSOR_INT32;\n+            case 7 -> OnnxType.TENSOR_INT64;\n+            case 8 -> OnnxType.TENSOR_STRING;\n+            case 9 -> OnnxType.TENSOR_BOOL;\n+            case 10 -> OnnxType.TENSOR_FLOAT16;\n+            case 11 -> OnnxType.TENSOR_FLOAT64;\n+            case 12 -> OnnxType.TENSOR_UINT32;\n+            case 13 -> OnnxType.TENSOR_UINT64;\n+            case 14 -> OnnxType.TENSOR_COMPLEX64;\n+            case 15 -> OnnxType.TENSOR_COMPLEX128;\n+            case 16 -> OnnxType.TENSOR_BFLOAT16;\n+            case 17 -> OnnxType.TENSOR_FLOAT8E4M3FN;\n+            case 18 -> OnnxType.TENSOR_FLOAT8E4M3FNUZ;\n+            case 19 -> OnnxType.TENSOR_FLOAT8E5M2;\n+            case 20 -> OnnxType.TENSOR_FLOAT8E5M2FNUZ;\n+            case 21 -> OnnxType.TENSOR_UINT4;\n+            case 22 -> OnnxType.TENSOR_INT4;\n+            case 23 -> OnnxType.TENSOR_FLOAT4E2M1;\n+            default -> OnnxType.tensor(null);\n+        };\n+    }\n+\n+    static final OpFactory ONNX_FACTORY = OpFactory.OP_FACTORY.get(OnnxOps.class);\n+\n+\n+    record OpWithNames<T extends Op> (T op, List<String> names) {\n+        public String toText() {\n+            var defNamer = OpWriter.CodeItemNamerOption.defaultValue().namer();\n+            var namer = new HashMap<Value, Integer>();\n+            return OpWriter.toText(op, OpWriter.CodeItemNamerOption.of(ci -> ci instanceof Value v ? names.get(namer.computeIfAbsent(v, _ -> namer.size())) : defNamer.apply(ci)));\n+\n+        }\n+    }\n+\n+    static OpWithNames toFuncOp(OnnxModel.GraphProto g) {\n+        var valueMap = new LinkedHashMap<String, Value>();\n+        var func = CoreOp.FuncOp.func(g.name(), toFunctionType(g)).body(fb -> {\n+\n+            { \/\/ fill value map for parameters and initializers\n+                Iterator<Block.Parameter> params = fb.entryBlock().parameters().iterator();\n+                for (OnnxModel.ValueInfoProto input : g.input()) {\n+                    valueMap.put(input.name(), params.next());\n+                }\n+                for (OnnxModel.TensorProto init : g.initializer()) {\n+                    valueMap.put(init.name(), params.next());\n+                }\n+            }\n+\n+            for (OnnxModel.NodeProto n : g.node()) {\n+                \/\/ get the op\n+                ExternalizableOp.ExternalizedOp extOp = new ExternalizableOp.ExternalizedOp(\n+                        n.opType(),\n+                        n.input() == null ? List.of() : n.input().stream().map(valueMap::get).toList(),\n+                        List.of(),\n+                        new OnnxType.TensorType(null),\n+                        n.attribute() == null ? Map.of() : n.attribute().stream().collect(Collectors.toMap(OnnxModel.AttributeProto::name, OnnxModelTest::toAttributeValue)),\n+                        List.of());\n+                OnnxOp rawOp = (OnnxOp)ONNX_FACTORY.constructOpOrFail(extOp);\n+\n+                \/\/ patch the op return type\n+                TypeElement returnType = rawOp.onnxOutputs().size() == 1\n+                        ? inferTypeVariableType(rawOp.onnxOutputs().getFirst().type(), rawOp, n)\n+                        : TupleType.tupleType(rawOp.onnxOutputs().stream().map(o -> inferTypeVariableType(o.type(), rawOp, n)).toList());\n+                extOp = new ExternalizableOp.ExternalizedOp(\n+                        extOp.name(),\n+                        extOp.operands(),\n+                        extOp.successors(),\n+                        returnType,\n+                        extOp.attributes(),\n+                        extOp.bodyDefinitions());\n+                Op.Result res = fb.op((OnnxOp)ONNX_FACTORY.constructOpOrFail(extOp));\n+\n+                \/\/ map outputs\n+                if (rawOp.onnxOutputs().size() == 1) {\n+                    valueMap.put(n.output().getFirst(), res);\n+                } else {\n+                    valueMap.put(n.name(), res);\n+                    for (int i = 0; i < n.output().size(); i++) {\n+                        valueMap.put(n.output().get(i), fb.op(CoreOp.tupleLoad(res, i)));\n+                    }\n+                }\n+            }\n+\n+            if (g.output().size() == 1) {\n+                fb.op(CoreOp._return(valueMap.get(g.output().getFirst().name())));\n+            } else {\n+                Op.Result ret = fb.op(CoreOp.tuple(g.output().stream().map(OnnxModel.ValueInfoProto::name).map(valueMap::get).toList()));\n+                valueMap.put(g.name() + \"_return\", ret);\n+                fb.op(CoreOp._return(ret));\n+            }\n+        });\n+\n+        return new OpWithNames(func, List.of(valueMap.sequencedKeySet().toArray(String[]::new)));\n+    }\n+\n+    static OnnxType inferTypeVariableType(OnnxType type, OnnxOp op, OnnxModel.NodeProto n) {\n+        if (type instanceof OnnxType.TypeVariable tv) {\n+            if (tv.types().size() == 1) {\n+                return tv.types().getFirst();\n+            }\n+            \/\/ search for the same type variable across inputs\n+            for (var ie : op.onnxInputs().entrySet()) {\n+                if (ie.getKey().type().equals(tv)) {\n+                    if (ie.getValue() instanceof Value v && v.type() instanceof OnnxType ot) {\n+                        return ot;\n+                    } else if (ie.getValue() instanceof List l && !l.isEmpty() && l.getFirst() instanceof Value v && v.type() instanceof OnnxType ot) {\n+                        return ot;\n+                    }\n+                }\n+            }\n+\n+            \/\/ special cases\n+            return switch (op) {\n+                case OnnxOps.Cast c ->\n+                    toTensorType((int)c.to());\n+                case OnnxOps.ConstantOfShape cos -> \/\/ get tensor type from tensor attribute\n+                    n.attribute() != null\n+                    && !n.attribute().isEmpty()\n+                    && n.attribute().getFirst().t() instanceof OnnxModel.TensorProto tp\n+                            ? toTensorType(tp.dataType())\n+                            : OnnxType.TENSOR_FLOAT32; \/\/ default\n+                default ->\n+                    throw new IllegalArgumentException(\"Could not infer op type for: \" + op.toText());\n+            };\n+        }\n+        return type;\n+    }\n+\n+    static Object toAttributeValue(OnnxModel.AttributeProto a) {\n+        return switch (a.type()) {\n+            case FLOAT -> a.f();\n+            case INT -> a.i();\n+            case STRING -> a.s();\n+            case TENSOR -> toTensor(a.t());\n+\/\/    GRAPH = 5;\n+\/\/    SPARSE_TENSOR = 11;\n+\/\/    TYPE_PROTO = 13;\n+            case FLOATS -> joinFloatArray(a.floats());\n+            case INTS -> joinLongArray(a.ints());\n+            case STRINGS -> a.strings();\n+            case TENSORS -> a.tensors().stream().map(OnnxModelTest::toTensor).toArray(Tensor[]::new);\n+\/\/    GRAPHS = 10;\n+\/\/    SPARSE_TENSORS = 12;\n+\/\/    TYPE_PROTOS = 14;\n+            default -> throw new UnsupportedOperationException(\"Unsupported \" + a.type());\n+        };\n+    }\n+\n+    static Tensor toTensor(OnnxModel.TensorProto tensorProto) {\n+        \/\/ @@@ floatData, longData, stringData...\n+        \/\/ @@@ externalData\n+        \/\/ @@@ segments\n+        return Tensor.ofShape(joinLongArray(tensorProto.dims()), tensorProto.rawData(), Tensor.ElementType.fromOnnxId(tensorProto.dataType()));\n+    }\n+\n+    static float[] joinFloatArray(List<float[]> floats) {\n+        if (floats == null) return new float[0];\n+        float[] join = new float[floats.stream().mapToInt(f -> f.length).sum()];\n+        int i = 0;\n+        for (float[] f : floats) {\n+            System.arraycopy(f, 0, join, i, f.length);\n+            i += f.length;\n+        }\n+        return join;\n+    }\n+\n+    static long[] joinLongArray(List<long[]> longs) {\n+        if (longs == null) return new long[0];\n+        long[] join = new long[longs.stream().mapToInt(f -> f.length).sum()];\n+        int i = 0;\n+        for (long[] f : longs) {\n+            System.arraycopy(f, 0, join, i, f.length);\n+            i += f.length;\n+        }\n+        return join;\n+    }\n+\n+    @Test\n+    public void cnnLiftTest() throws Exception {\n+        try (InputStream in = CNNTest.class.getResourceAsStream(\"lenet-torchscript.onnx\")) {\n+\n+            \/\/ parse onnx protobuf model\n+            OnnxModel.ModelProto protoModel = OnnxModel.readFrom(in.readAllBytes());\n+\n+\/\/            System.out.println(model.toText());\n+\n+            \/\/ lift the cnnFuncOp from Onnx protobuf model\n+            OpWithNames<CoreOp.FuncOp> cnnFuncOp = toFuncOp(protoModel.graph());\n+\n+            System.out.println(cnnFuncOp.toText());\n+\/\/            System.out.println(cnnFuncOp.op().toText());\n+\n+            \/\/ test the lifted model\n+            try (Arena a = Arena.ofConfined()) {\n+                List<Tensor> inputValues = new ArrayList<>();\n+\n+                \/\/ initializers are extracted from the proto model directly\n+                for (OnnxModel.TensorProto init : protoModel.graph().initializer()) {\n+                    inputValues.add(Tensor.ofShape(a, joinLongArray(init.dims()), init.rawData(), Tensor.ElementType.fromOnnxId(init.dataType())));\n+                }\n+\n+                \/\/ fake image\n+                float[] image = new float[28 * 28];\n+                for (int i = 13; i < 28 * 28; i+=28) {\n+                    image[i] = 1f;\n+                }\n+                inputValues.add(Tensor.ofShape(a, new long[] {1, 1, 28, 28}, image));\n+\n+                \/\/ run\n+                List<Tensor> res = OnnxRuntime.getInstance().run(a, cnnFuncOp.op().body().entryBlock(), inputValues, inputValues.size() - 1);\n+\n+                System.out.println(Arrays.toString(res.getFirst().data().toArray(ValueLayout.JAVA_FLOAT)));\n+            }\n+        }\n+    }\n+\n+    public static void main(String[] args) throws Exception {\n+        for (var fName : args) {\n+            try (var in = new RandomAccessFile(fName, \"r\")) {\n+                OnnxModel.ModelProto model = OnnxModel.readFrom(in.getChannel().map(FileChannel.MapMode.READ_ONLY, 0, in.length()));\n+\/\/                System.out.println(model.toText());\n+                System.out.println(toFuncOp(model.graph()).toText());\n+            }\n+        }\n+    }\n+}\n","filename":"cr-examples\/onnx\/src\/test\/java\/oracle\/code\/onnx\/proto\/OnnxModelTest.java","additions":338,"deletions":0,"binary":false,"changes":338,"status":"added"},{"patch":"@@ -1,329 +0,0 @@\n-\/*\n- * Copyright (c) 2025, Oracle and\/or its affiliates. All rights reserved.\n- * DO NOT ALTER OR REMOVE COPYRIGHT NOTICES OR THIS FILE HEADER.\n- *\n- * This code is free software; you can redistribute it and\/or modify it\n- * under the terms of the GNU General Public License version 2 only, as\n- * published by the Free Software Foundation.\n- *\n- * This code is distributed in the hope that it will be useful, but WITHOUT\n- * ANY WARRANTY; without even the implied warranty of MERCHANTABILITY or\n- * FITNESS FOR A PARTICULAR PURPOSE.  See the GNU General Public License\n- * version 2 for more details (a copy is included in the LICENSE file that\n- * accompanied this code).\n- *\n- * You should have received a copy of the GNU General Public License version\n- * 2 along with this work; if not, write to the Free Software Foundation,\n- * Inc., 51 Franklin St, Fifth Floor, Boston, MA 02110-1301 USA.\n- *\n- * Please contact Oracle, 500 Oracle Parkway, Redwood Shores, CA 94065 USA\n- * or visit www.oracle.com if you need additional information or have any\n- * questions.\n- *\/\n-\n-package oracle.code.onnx.proto;\n-\n-import java.io.InputStream;\n-import java.io.RandomAccessFile;\n-import java.lang.foreign.Arena;\n-import java.lang.foreign.ValueLayout;\n-import java.nio.ByteOrder;\n-import java.nio.channels.FileChannel;\n-import java.util.ArrayList;\n-import java.util.Arrays;\n-import java.util.HashMap;\n-import java.util.Iterator;\n-import java.util.LinkedHashMap;\n-import java.util.List;\n-import java.util.Map;\n-import java.util.SequencedSet;\n-import java.util.stream.Collectors;\n-import jdk.incubator.code.Block;\n-import jdk.incubator.code.Op;\n-import jdk.incubator.code.TypeElement;\n-import jdk.incubator.code.Value;\n-import jdk.incubator.code.op.CoreOp;\n-import jdk.incubator.code.op.ExternalizableOp;\n-import jdk.incubator.code.op.OpFactory;\n-import jdk.incubator.code.type.FunctionType;\n-import jdk.incubator.code.type.TupleType;\n-import jdk.incubator.code.writer.OpWriter;\n-import oracle.code.onnx.CNNTest;\n-import oracle.code.onnx.OnnxRuntime;\n-import oracle.code.onnx.Tensor;\n-import oracle.code.onnx.ir.OnnxOp;\n-import oracle.code.onnx.ir.OnnxOps;\n-import oracle.code.onnx.ir.OnnxType;\n-import org.junit.jupiter.api.Test;\n-\n-\n-public class OnnxProtoModelTest {\n-\n-    static OnnxType toOnnxType(OnnxProtoModel.TypeProto tp) {\n-        if (tp.tensorType() instanceof OnnxProtoModel.TypeProto.Tensor t) {\n-            return toTensorType(t.elemType());\n-        } else if (tp.optionalType() instanceof OnnxProtoModel.TypeProto.Optional o) {\n-            return OnnxType.optional(toOnnxType(o.elemType()));\n-        } else if (tp.sequenceType()  instanceof OnnxProtoModel.TypeProto.Sequence s) {\n-            return OnnxType.seq(toOnnxType(s.elemType()));\n-        } else if (tp.mapType() instanceof OnnxProtoModel.TypeProto.Map m) {\n-            return OnnxType.map(toKeyType(m.keyType()), toOnnxType(m.valueType()));\n-        } else if (tp.sparseTensorType() instanceof OnnxProtoModel.TypeProto.SparseTensor st) {\n-            throw new UnsupportedOperationException(\"Sparse tensors not supported yet.\"); \/\/ @@@\n-        }\n-        throw new IllegalArgumentException(\"No type specified.\");\n-    }\n-\n-    static FunctionType toFunctionType(OnnxProtoModel.GraphProto g) {\n-        var paramTypes = new ArrayList<TypeElement>();\n-        for (OnnxProtoModel.ValueInfoProto input : g.inputs()) {\n-            paramTypes.add(toOnnxType(input.type()));\n-        }\n-        for (OnnxProtoModel.TensorProto init : g.initializers()) {\n-            paramTypes.add(toTensorType(init.dataType()));\n-        }\n-        var returnType = g.outputs().size() == 1\n-                ? toOnnxType(g.outputs().getFirst().type())\n-                : TupleType.tupleType(g.outputs().stream().map(OnnxProtoModel.ValueInfoProto::type).map(OnnxProtoModelTest::toOnnxType).toList());\n-        return FunctionType.functionType(returnType, paramTypes);\n-    }\n-\n-    static OnnxType toKeyType(int kt) {\n-        return switch (kt) {\n-            case 2 -> OnnxType.UINT8;\n-            case 3 -> OnnxType.INT8;\n-            case 4 -> OnnxType.UINT16;\n-            case 5 -> OnnxType.INT16;\n-            case 6 -> OnnxType.INT32;\n-            case 7 -> OnnxType.INT64;\n-            case 8 -> OnnxType.STRING;\n-            case 12 -> OnnxType.UINT32;\n-            case 13 -> OnnxType.UINT64;\n-            default -> throw new IllegalArgumentException(\"Invalid key type: \" + kt);\n-        };\n-    }\n-\n-    static OnnxType.TensorType toTensorType(int tt) {\n-        return switch (tt) {\n-            case 1 -> OnnxType.TENSOR_FLOAT32;\n-            case 2 -> OnnxType.TENSOR_UINT8;\n-            case 3 -> OnnxType.TENSOR_INT8;\n-            case 4 -> OnnxType.TENSOR_UINT16;\n-            case 5 -> OnnxType.TENSOR_INT16;\n-            case 6 -> OnnxType.TENSOR_INT32;\n-            case 7 -> OnnxType.TENSOR_INT64;\n-            case 8 -> OnnxType.TENSOR_STRING;\n-            case 9 -> OnnxType.TENSOR_BOOL;\n-            case 10 -> OnnxType.TENSOR_FLOAT16;\n-            case 11 -> OnnxType.TENSOR_FLOAT64;\n-            case 12 -> OnnxType.TENSOR_UINT32;\n-            case 13 -> OnnxType.TENSOR_UINT64;\n-            case 14 -> OnnxType.TENSOR_COMPLEX64;\n-            case 15 -> OnnxType.TENSOR_COMPLEX128;\n-            case 16 -> OnnxType.TENSOR_BFLOAT16;\n-            case 17 -> OnnxType.TENSOR_FLOAT8E4M3FN;\n-            case 18 -> OnnxType.TENSOR_FLOAT8E4M3FNUZ;\n-            case 19 -> OnnxType.TENSOR_FLOAT8E5M2;\n-            case 20 -> OnnxType.TENSOR_FLOAT8E5M2FNUZ;\n-            case 21 -> OnnxType.TENSOR_UINT4;\n-            case 22 -> OnnxType.TENSOR_INT4;\n-            case 23 -> OnnxType.TENSOR_FLOAT4E2M1;\n-            default -> OnnxType.tensor(null);\n-        };\n-    }\n-\n-    static final OpFactory ONNX_FACTORY = OpFactory.OP_FACTORY.get(OnnxOps.class);\n-\n-\n-    record OpWithNames<T extends Op> (T op, List<String> names) {\n-        public String toText() {\n-            var defNamer = OpWriter.CodeItemNamerOption.defaultValue().namer();\n-            var namer = new HashMap<Value, Integer>();\n-            return OpWriter.toText(op, OpWriter.CodeItemNamerOption.of(ci -> ci instanceof Value v ? names.get(namer.computeIfAbsent(v, _ -> namer.size())) : defNamer.apply(ci)));\n-\n-        }\n-    }\n-\n-    static OpWithNames toFuncOp(OnnxProtoModel.GraphProto g) {\n-        var valueMap = new LinkedHashMap<String, Value>();\n-        var func = CoreOp.FuncOp.func(g.name(), toFunctionType(g)).body(fb -> {\n-\n-            { \/\/ fill value map for parameters and initializers\n-                Iterator<Block.Parameter> params = fb.entryBlock().parameters().iterator();\n-                for (OnnxProtoModel.ValueInfoProto input : g.inputs()) {\n-                    valueMap.put(input.name(), params.next());\n-                }\n-                for (OnnxProtoModel.TensorProto init : g.initializers()) {\n-                    valueMap.put(init.name(), params.next());\n-                }\n-            }\n-\n-            for (OnnxProtoModel.NodeProto n : g.nodes()) {\n-                \/\/ get the op\n-                ExternalizableOp.ExternalizedOp extOp = new ExternalizableOp.ExternalizedOp(\n-                        n.opType(),\n-                        n.inputs() == null ? List.of() : n.inputs().stream().map(valueMap::get).toList(),\n-                        List.of(),\n-                        new OnnxType.TensorType(null),\n-                        n.attributes() == null ? Map.of() : n.attributes().stream().collect(Collectors.toMap(OnnxProtoModel.Attribute::name, OnnxProtoModelTest::toAttributeValue)),\n-                        List.of());\n-                OnnxOp rawOp = (OnnxOp)ONNX_FACTORY.constructOpOrFail(extOp);\n-\n-                \/\/ patch the op return type\n-                TypeElement returnType = rawOp.onnxOutputs().size() == 1\n-                        ? inferTypeVariableType(rawOp.onnxOutputs().getFirst().type(), rawOp, n)\n-                        : TupleType.tupleType(rawOp.onnxOutputs().stream().map(o -> inferTypeVariableType(o.type(), rawOp, n)).toList());\n-                extOp = new ExternalizableOp.ExternalizedOp(\n-                        extOp.name(),\n-                        extOp.operands(),\n-                        extOp.successors(),\n-                        returnType,\n-                        extOp.attributes(),\n-                        extOp.bodyDefinitions());\n-                Op.Result res = fb.op((OnnxOp)ONNX_FACTORY.constructOpOrFail(extOp));\n-\n-                \/\/ map outputs\n-                if (rawOp.onnxOutputs().size() == 1) {\n-                    valueMap.put(n.outputs().getFirst(), res);\n-                } else {\n-                    valueMap.put(n.name(), res);\n-                    for (int i = 0; i < n.outputs().size(); i++) {\n-                        valueMap.put(n.outputs().get(i), fb.op(CoreOp.tupleLoad(res, i)));\n-                    }\n-                }\n-            }\n-\n-            if (g.outputs().size() == 1) {\n-                fb.op(CoreOp._return(valueMap.get(g.outputs().getFirst().name())));\n-            } else {\n-                Op.Result ret = fb.op(CoreOp.tuple(g.outputs().stream().map(OnnxProtoModel.ValueInfoProto::name).map(valueMap::get).toList()));\n-                valueMap.put(g.name() + \"_return\", ret);\n-                fb.op(CoreOp._return(ret));\n-            }\n-        });\n-\n-        return new OpWithNames(func, List.of(valueMap.sequencedKeySet().toArray(String[]::new)));\n-    }\n-\n-    static OnnxType inferTypeVariableType(OnnxType type, OnnxOp op, OnnxProtoModel.NodeProto n) {\n-        if (type instanceof OnnxType.TypeVariable tv) {\n-            if (tv.types().size() == 1) {\n-                return tv.types().getFirst();\n-            }\n-            \/\/ search for the same type variable across inputs\n-            for (var ie : op.onnxInputs().entrySet()) {\n-                if (ie.getKey().type().equals(tv)) {\n-                    if (ie.getValue() instanceof Value v && v.type() instanceof OnnxType ot) {\n-                        return ot;\n-                    } else if (ie.getValue() instanceof List l && !l.isEmpty() && l.getFirst() instanceof Value v && v.type() instanceof OnnxType ot) {\n-                        return ot;\n-                    }\n-                }\n-            }\n-\n-            \/\/ special cases\n-            return switch (op) {\n-                case OnnxOps.Cast c ->\n-                    toTensorType((int)c.to());\n-                case OnnxOps.ConstantOfShape cos -> \/\/ get tensor type from tensor attribute\n-                    n.attributes() != null\n-                    && !n.attributes().isEmpty()\n-                    && n.attributes().getFirst().t() instanceof OnnxProtoModel.TensorProto tp\n-                            ? toTensorType(tp.dataType())\n-                            : OnnxType.TENSOR_FLOAT32; \/\/ default\n-                default ->\n-                    throw new IllegalArgumentException(\"Could not infer op type for: \" + op.toText());\n-            };\n-        }\n-        return type;\n-    }\n-\n-    static Object toAttributeValue(OnnxProtoModel.Attribute a) {\n-        return switch (a.type()) {\n-            case 1 -> a.f();\n-            case 2 -> a.i();\n-            case 3 -> a.s();\n-            case 4 -> a.t().rawData(); \/\/ @@@ need to store all tensor info + data\n-\/\/    GRAPH = 5;\n-\/\/    SPARSE_TENSOR = 11;\n-\/\/    TYPE_PROTO = 13;\n-            case 6 -> joinFloatArray(a.floats());\n-            case 7 -> joinLongArray(a.ints());\n-            case 8 -> a.strings();\n-\/\/    TENSORS = 9;\n-\/\/    GRAPHS = 10;\n-\/\/    SPARSE_TENSORS = 12;\n-\/\/    TYPE_PROTOS = 14;\n-            default -> throw new UnsupportedOperationException(\"Unsupported \" + a.type());\n-        };\n-    }\n-\n-    static float[] joinFloatArray(List<float[]> floats) {\n-        float[] join = new float[floats.stream().mapToInt(f -> f.length).sum()];\n-        int i = 0;\n-        for (float[] f : floats) {\n-            System.arraycopy(f, 0, join, i, f.length);\n-            i += f.length;\n-        }\n-        return join;\n-    }\n-\n-    static long[] joinLongArray(List<long[]> floats) {\n-        long[] join = new long[floats.stream().mapToInt(f -> f.length).sum()];\n-        int i = 0;\n-        for (long[] f : floats) {\n-            System.arraycopy(f, 0, join, i, f.length);\n-            i += f.length;\n-        }\n-        return join;\n-    }\n-\n-    @Test\n-    public void cnnLiftTest() throws Exception {\n-        try (InputStream in = CNNTest.class.getResourceAsStream(\"lenet-torchscript.onnx\")) {\n-\n-            \/\/ parse onnx protobuf model\n-            OnnxProtoModel protoModel = OnnxProtoModel.readFrom(in.readAllBytes());\n-\n-\/\/            System.out.println(model.toText());\n-\n-            \/\/ lift the cnnFuncOp from Onnx protobuf model\n-            OpWithNames<CoreOp.FuncOp> cnnFuncOp = toFuncOp(protoModel.graph());\n-\n-            System.out.println(cnnFuncOp.toText());\n-\/\/            System.out.println(cnnFuncOp.op().toText());\n-\n-            \/\/ test the lifted model\n-            try (Arena a = Arena.ofConfined()) {\n-                List<Tensor> inputValues = new ArrayList<>();\n-\n-                \/\/ initializers are extracted from the proto model directly\n-                for (OnnxProtoModel.TensorProto init : protoModel.graph().initializers()) {\n-                    inputValues.add(Tensor.ofShape(a, joinLongArray(init.dims()), init.rawData(), Tensor.ElementType.fromOnnxId(init.dataType())));\n-                }\n-\n-                \/\/ fake image\n-                float[] image = new float[28 * 28];\n-                for (int i = 13; i < 28 * 28; i+=28) {\n-                    image[i] = 1f;\n-                }\n-                inputValues.add(Tensor.ofShape(a, new long[] {1, 1, 28, 28}, image));\n-\n-                \/\/ run\n-                List<Tensor> res = OnnxRuntime.getInstance().run(a, cnnFuncOp.op().body().entryBlock(), inputValues, inputValues.size() - 1);\n-\n-                System.out.println(Arrays.toString(res.getFirst().data().toArray(ValueLayout.JAVA_FLOAT)));\n-            }\n-        }\n-    }\n-\n-    public static void main(String[] args) throws Exception {\n-        for (var fName : args) {\n-            try (var in = new RandomAccessFile(fName, \"r\")) {\n-                OnnxProtoModel model = OnnxProtoModel.readFrom(in.getChannel().map(FileChannel.MapMode.READ_ONLY, 0, in.length()));\n-\/\/                System.out.println(model.toText());\n-                System.out.println(toFuncOp(model.graph()).toText());\n-            }\n-        }\n-    }\n-}\n","filename":"cr-examples\/onnx\/src\/test\/java\/oracle\/code\/onnx\/proto\/OnnxProtoModelTest.java","additions":0,"deletions":329,"binary":false,"changes":329,"status":"deleted"}]}