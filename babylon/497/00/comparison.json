{"files":[{"patch":"@@ -189,0 +189,93 @@\n+\n+\n+----\n+### HAT runtime environment variable.\n+\n+During ffi-backend development we added some useful flags to pass to native code to allow us to trace calls, inject logging, control options.\n+\n+These should be considered just development flags.\n+\n+At runtime the ffi-backends all communicate from the java side to the native side via a 32 bit 'config' int.\n+\n+The Java side class [hat.backend.ffi.Config](https:\/\/github.com\/openjdk\/babylon\/blob\/code-reflection\/hat\/backends\/ffi\/shared\/src\/main\/java\/hat\/backend\/ffi\/Config.java)\n+\n+Is initialized from an environment variable `HAT` at runtime\n+\n+So for example when we launch\n+```bash\n+java @hat\/run ffi-opencl heal\n+```\n+\n+We can pass config info to `HAT` via either `-DHAT=xxx` or via the `HAT` ENV variable.\n+\n+So for example to get HAT to dump the text form of the kernel code models.\n+\n+```\n+HAT=INFO,SHOW_KERNEL_MODEL java @hat\/run ffi-opencl heal\n+```\n+\n+Or to show generated opencl or cuda code\n+```\n+HAT=INFO,SHOW_CODE java @hat\/run ffi-opencl heal\n+```\n+\n+This is particularly useful for selecting PLATFORM + DEVICE if you have multiple GPU devices.\n+\n+Here we select DEVICE 0 on PLATFORM 0 (actually the default)\n+```\n+HAT=INFO,PLATFORM:0,DEVICE:0,SHOW_CODE ...\n+```\n+or for DEVICE 1 on PLATFORM 1\n+```\n+HAT=INFO,PLATFORM:1,DEVICE:1,SHOW_CODE ...\n+```\n+\n+No the platform and device id's are 4 bits. This probably works for development, but we will need a more expressive way of capturing this via the accelerator selection.\n+\n+This just allows us to test without code changes.\n+\n+To keep he java code and the native code in sync.  The `main` method at the end of\n+ [hat.backend.ffi.Config](https:\/\/github.com\/openjdk\/babylon\/blob\/code-reflection\/hat\/backends\/ffi\/shared\/src\/main\/java\/hat\/backend\/ffi\/Config.java)\n+is actually used to create the C99 header for Cuda and OpenCL ffi-backends.\n+\n+So whenever we change the Java config class we should run the main method to generate the header.   This is not really robust, (but proved way better than trying to remember for all backends) but you need to know, if you add move config bits to the Java side.\n+\n+The Main method uses a variant of the code builders used for C99 style code (CUDA\/OpenCL) to generate the config.h header.\n+\n+This is how we keep the ffi based backends in sync\n+\n+Some more useful `config bits`\n+\n+```\n+HAT=PTX java @hat\/run ffi-cuda ....\n+```\n+Sends PTX generated by our prototype PTX generator to the backend (for CUDA) rather than C99 code.\n+\n+```\n+HAT=INFO java @hat\/run ....\n+```\n+\n+Will dump all of the `config bits` from the native side.\n+\n+At present this yields\n+```\n+native minimizeCopies 0\n+native trace 0\n+native profile 0\n+native showCode 0\n+native showKernelModel 0\n+native showComputeModel 0\n+native info 1\n+native traceCopies 0\n+native traceSkippedCopies 0\n+native traceEnqueues 0\n+native traceCalls 0\n+native showWhy 0\n+native showState 0\n+native ptx 0\n+native interpret 0\n+```\n+Generally the flags all represent single bits (except PLATFORM:n and DEVICE:n) and are set using comma separated uppercase+underscore string forms.\n+\n+\n+So to experiment with `minimizeCopies` and to  `showCode` we set `HAT=MINIMIZE_COPIES,SHOW_CODE`\n","filename":"hat\/docs\/hat-01-03-building-hat.md","additions":93,"deletions":0,"binary":false,"changes":93,"status":"modified"},{"patch":"@@ -0,0 +1,306 @@\n+#### Leverage CodeReflection to expose array view of buffers in kernels\n+----\n+\n+* [Contents](hat-00.md)\n+* House Keeping\n+    * [Project Layout](hat-01-01-project-layout.md)\n+    * [Building Babylon](hat-01-02-building-babylon.md)\n+    * [Building HAT](hat-01-03-building-hat.md)\n+* Programming Model\n+    * [Programming Model](hat-03-programming-model.md)\n+* Interface Mapping\n+    * [Interface Mapping Overview](hat-04-01-interface-mapping.md)\n+    * [Cascade Interface Mapping](hat-04-02-cascade-interface-mapping.md)\n+* Implementation Detail\n+    * [Walkthrough Of Accelerator.compute()](hat-accelerator-compute.md)\n+    * [How we minimize buffer transfers](hat-minimizing-buffer-transfers.md)\n+\n+----\n+\n+Here is the canonical HAT example\n+\n+```java\n+import jdk.incubator.code.CodeReflection;\n+\n+@CodeReflection\n+public class Square {\n+  @CodeReflection\n+  public static void kernel(KernelContext kc, S32Arr s32Arr) {\n+    s32Arr.array(kc.x, s32Arr.array(kc.x) * s32Arr.array(kc.x));\n+  }\n+\n+  @CodeReflection\n+  public static void compute(ComputeContext cc, S32Arr s32Arr) {\n+    cc.dispatchKernel(s32Arr.length(), kc -> kernel(kc, s32Arr));\n+  }\n+}\n+```\n+\n+This code in the kernel has always bothered me.  One downside of using MemorySegment backed buffers,\n+in HAT is that we have made what in array form, would be simple code, look verbose.\n+\n+```java\n+ @CodeReflection\n+  public static void kernel(KernelContext kc, S32Arr s32Arr) {\n+    s32Arr.array(kc.x, s32Arr.array(kc.x) * s32Arr.array(kc.x));\n+  }\n+```\n+\n+But what if we added a method (`int[] arrayView()`) to `S32Arr` to extract a java int array `view` of simple arrays\n+\n+Becomes way more readable.\n+```java\n+ @CodeReflection\n+  public static void kernel(KernelContext kc, S32Arr s32Arr) {\n+    int[] arr = s32Arr.arrayView();\n+    arr[kc.x] *= arr[kc.x];\n+  }\n+```\n+IMHO This makes code more readable.\n+\n+For the GPU this is fine.  We can (thanks to CodeReflection) prove that the array is indeed just a view\n+and we just remove all references to `int arr[]` and replace array accessors with get\/set accessors\n+on the original S32Arr.\n+\n+But what about Java performance?. Won't it suck because we are copying the array in each kernel ;)\n+\n+Well, we can use the same trick, we used for the GPU, we take the transformed model (with array\n+references removed) and create bytecode from that code we and run it.\n+\n+Historically, we just run the original bytecode in the Java MT\/Seq backends, but we don't have to.\n+\n+This helps also with game of life\n+```java\n+  public static void lifePerIdx(int idx, @RO Control control, @RW CellGrid cellGrid) {\n+            int w = cellGrid.width();\n+            int h = cellGrid.height();\n+            int from = control.from();\n+            int to = control.to();\n+            int x = idx % w;\n+            int y = idx \/ w;\n+            byte cell = cellGrid.cell(idx + from);\n+            if (x > 0 && x < (w - 1) && y > 0 && y < (h - 1)) { \/\/ passports please\n+                int count =\n+                        val(cellGrid, from, w, x - 1, y - 1)\n+                                + val(cellGrid, from, w, x - 1, y + 0)\n+                                + val(cellGrid, from, w, x - 1, y + 1)\n+                                + val(cellGrid, from, w, x + 0, y - 1)\n+                                + val(cellGrid, from, w, x + 0, y + 1)\n+                                + val(cellGrid, from, w, x + 1, y + 0)\n+                                + val(cellGrid, from, w, x + 1, y - 1)\n+                                + val(cellGrid, from, w, x + 1, y + 1);\n+                cell = ((count == 3) || ((count == 2) && (cell == ALIVE))) ? ALIVE : DEAD;\/\/ B3\/S23.\n+            }\n+            cellGrid.cell(idx + to, cell);\n+        }\n+```\n+\n+This code uses a helper function `val(grid, offset, w, dx, dy)` to extract the neighbours\n+```java\n+ int count =\n+        val(cellGrid, from, w, x - 1, y - 1)\n+                + val(cellGrid, from, w, x - 1, y + 0)\n+                + val(cellGrid, from, w, x - 1, y + 1)\n+                + val(cellGrid, from, w, x + 0, y - 1)\n+                + val(cellGrid, from, w, x + 0, y + 1)\n+                + val(cellGrid, from, w, x + 1, y + 0)\n+                + val(cellGrid, from, w, x + 1, y - 1)\n+                + val(cellGrid, from, w, x + 1, y + 1);\n+```\n+\n+Val is a bit verbose\n+\n+```java\n+  @CodeReflection\n+        public static int val(@RO CellGrid grid, int from, int w, int x, int y) {\n+            return grid.cell(((long) y * w) + x + from) & 1;\n+        }\n+```\n+\n+```java\n+  @CodeReflection\n+        public static int val(@RO CellGrid grid, int from, int w, int x, int y) {\n+            byte[] bytes = grid.byteView(); \/\/ bit view would be nice ;)\n+            return bytes[ y * w + x + from] & 1;\n+        }\n+```\n+\n+We could now dispense with `val()` and just write\n+\n+```java\n+ byte[] bytes = grid.byteView();\n+ int count =\n+        bytes[(y - 1) * w + x - 1 + from]&1\n+       +bytes[(y + 0) * w + x - 1 + from]&1\n+       +bytes[(y + 1) * w + x - 1 + from]&1\n+       +bytes[(y - 1) * w + x + 0 + from]&1\n+       +bytes[(y + 1) * w + x + 0 + from]&1\n+       +bytes[(y + 0) * w + x + 1 + from]&1\n+       +bytes[(y - 1) * w + x + 1 + from]&1\n+       +bytes[(y + 1) * w + x + 1 + from]&1 ;\n+```\n+\n+BTW My inner verilog programmer has always wondered whether shift and oring each bit into a\n+9 bit value, which we use to index live\/dead state from a prepopulated 512 array (GPU constant memory)\n+would allow us to sidestep the wave divergent conditional :)\n+\n+```java\n+byte[] bytes = grid.byteView();\n+int idx = 0;\n+int to =0;\n+byte[] lookup = new byte[]{};\n+int lookupIdx =\n+        bytes[(y - 1) * w + x - 1 + from]&1 <<0\n+       |bytes[(y + 0) * w + x - 1 + from]&1 <<1\n+       |bytes[(y + 1) * w + x - 1 + from]&1 <<2\n+       |bytes[(y - 1) * w + x + 0 + from]&1 <<3\n+       |bytes[(y - 0) * w + x + 0 + from]&1 <<4 \/\/ current cell added\n+       |bytes[(y + 1) * w + x + 0 + from]&1 <<5\n+       |bytes[(y + 0) * w + x + 1 + from]&1 <<6\n+       |bytes[(y - 1) * w + x + 1 + from]&1 <<7\n+       |bytes[(y + 1) * w + x + 1 + from]&1 <<8 ;\n+\/\/ conditional removed!\n+\n+bytes[idx + to] = lookup[lookupIdx];\n+```\n+\n+So the task here is to process kernel code models and perform the appropriate analysis\n+(for tracking the primitive arrays origin) and transformations to map the array code back to buffer get\/sets\n+\n+----------\n+The arrayView trick actually leads us to other possibilities.\n+\n+Let's look at current NBody code.\n+\n+```java\n+  @CodeReflection\n+    static public void nbodyKernel(@RO KernelContext kc, @RW Universe universe, float mass, float delT, float espSqr) {\n+        float accx = 0.0f;\n+        float accy = 0.0f;\n+        float accz = 0.0f;\n+        Universe.Body me = universe.body(kc.x);\n+\n+        for (int i = 0; i < kc.maxX; i++) {\n+            Universe.Body otherBody = universe.body(i);\n+            float dx = otherBody.x() - me.x();\n+            float dy = otherBody.y() - me.y();\n+            float dz = otherBody.z() - me.z();\n+            float invDist = (float) (1.0f \/ Math.sqrt(((dx * dx) + (dy * dy) + (dz * dz) + espSqr)));\n+            float s = mass * invDist * invDist * invDist;\n+            accx = accx + (s * dx);\n+            accy = accy + (s * dy);\n+            accz = accz + (s * dz);\n+        }\n+        accx = accx * delT;\n+        accy = accy * delT;\n+        accz = accz * delT;\n+        me.x(me.x() + (me.vx() * delT) + accx * .5f * delT);\n+        me.y(me.y() + (me.vy() * delT) + accy * .5f * delT);\n+        me.z(me.z() + (me.vz() * delT) + accz * .5f * delT);\n+        me.vx(me.vx() + accx);\n+        me.vy(me.vy() + accy);\n+        me.vz(me.vz() + accz);\n+    }\n+```\n+\n+Contrast, the above code with the OpenCL code using `float4`\n+\n+```java\n+ __kernel void nbody( __global float4 *xyzPos ,__global float4* xyzVel, float mass, float delT, float espSqr ){\n+      float4 acc = (0.0, 0.0,0.0,0.0);\n+      float4 myPos = xyzPos[get_global_id(0)];\n+      float4 myVel = xyzVel[get_global_id(0)];\n+      for (int i = 0; i < get_global_size(0); i++) {\n+             float4 delta =  xyzPos[i] - myPos;\n+             float invDist =  (float) 1.0\/sqrt((float)((delta.x * delta.x) + (delta.y * delta.y) + (delta.z * delta.z) + espSqr));\n+             float s = mass * invDist * invDist * invDist;\n+             acc= acc + (s * delta);\n+      }\n+      acc = acc*delT;\n+      myPos = myPos + (myVel * delT) + (acc * delT)\/2;\n+      myVel = myVel + acc;\n+      xyzPos[get_global_id(0)] = myPos;\n+      xyzVel[get_global_id(0)] = myVel;\n+}\n+```\n+\n+Thanks to interface mapped segments we can approximate `float4` vector type, In fact the\n+existing `Universe` interface mapped segment actually embeds a `float6` kind of\n+object holding the `x,y,z,vx,vy,vz` values for each `body` (so position and velocity)\n+\n+What if we created generalized `float4` interface mapped views `(x,y,z,w)` as placeholders for true vector types.\n+\n+And modified Universe to hold `pos` and `vel` `float4` arrays.\n+\n+So in Java code we would pass a MemorySegment version of a F32x4Arr.\n+\n+Our java code could then start to approximate the OpenCL code.\n+\n+Except for our lack of operator overloading...\n+\n+```java\n+ void nbody(KernelContext kc,  F32x4Arr xyzPos ,F32x4Arr xyzVel, float mass, float delT, float espSqr ){\n+      float4 acc = float4.of(0.0,0.0,0.0,0.0);\n+      float4[] xyPosArr = xyzPos.float4View();\n+\n+      float4 myPos = xyzPosArr[kc.x];\n+      float4 myVel = xyzVelArr[kc.x];\n+      for (int i = 0; i < kc.max; i++) {\n+             float4 delta =  float4.sub(xyzPosArr[i],myPos); \/\/ yucky but ok\n+             float invDist =  (float) 1.0\/sqrt((float)((delta.x * delta.x) + (delta.y * delta.y) + (delta.z * delta.z) + espSqr));\n+             float s = mass * invDist * invDist * invDist;\n+             acc= float4.add(acc,(s * delta)); \/\/ adding scaler to float4 via overloaded add method\n+      }\n+      acc = float4.mul(acc*delT);  \/\/ scaler * vector\n+      myPos = float4.add(float4.add(myPos,float4.mul(myVel * delT)) , float4.mul(acc,delT\/2));\n+      myVel = float4.add(myVel,acc);\n+      xyzPos[kc.x] = myPos;\n+      xyzVel[kc.x] = myVel;\n+}\n+```\n+The code is more compact, it's still weird though. Because we can't overload operators.\n+\n+Well we can sort of.\n+\n+What if we allowed a `floatView()` call on `float4` ;) which yields float value to be used as a proxy\n+for the `float4` we fetched it from...\n+\n+So we would leverage the anonymity of `var`\n+\n+`var myVelF4 = myVel.floatView()` \/\/ pst var is really a float\n+\n+From the code model we can track the relationship from float views to the original vector...\n+\n+Any action we perform on the 'float' view will be mapped back to calls on the origin, and performed on the actual origin float4.\n+\n+So\n+`myVelF4 + myVelF4`  -> `float4.add(myVel,myVel)` behind the scenes.\n+\n+Yeah, it's a bit crazy. The code would look something like this.  Perhaps this is a 'bridge too far'.\n+\n+```java\n+void nbody(KernelContext kc,  F32x4Arr xyzPos ,F32x4Arr xyzVel, float mass, float delT, float espSqr ){\n+      float4 acc = float4.of(0.0,0.0,0.0,0.0);\n+      var accF4 = acc.floatView(); \/\/ var is really float\n+      float4[] xyPosArr = xyzPos.float4View();\n+\n+      float4 myPos = xyzPosArr[kc.x];\n+      float4 myVel = xyzVelArr[kc.x];\n+      var myPosF4 = myPos.floatView(); \/\/ ;)  var is actually a float tied to myPos\n+      var myVelF4 = myVel.floatView(); \/\/... myVel\n+      for (int i = 0; i < kc.max; i++) {\n+             var bodyF4 = xyzPosArr[i].floatView(); \/\/ bodyF4 is a float\n+             var  deltaF4 = bodyF4 - myPosF4;  \/\/ look ma operator overloading ;)\n+             float invDist =  (float) 1.0\/sqrt((float)((delta.x * delta.x) + (delta.y * delta.y) + (delta.z * delta.z) + espSqr));\n+             float s = mass * invDist * invDist * invDist;\n+             accF4+=s * deltaF4; \/\/ adding scaler to float4 via overloaded add method\n+      }\n+      accF4 = accF4*delT;  \/\/ scalar * vector\n+      myPosF4 = myPosF4 + (myVelF4 * delT) + (accF4 * delT)\/2;\n+      myVelF4 = myVelF4 + accF4;\n+      xyzPos[kc.x] = myPos;\n+      xyzVel[kc.x] = myVel;\n+}\n+```\n+\n+\n","filename":"hat\/docs\/hat-array-and-vector-views.md","additions":306,"deletions":0,"binary":false,"changes":306,"status":"added"},{"patch":"@@ -81,3 +81,1 @@\n-First lets assume there were no automatic transfers, assume we had to define them. we had to explicitly control transfers so we will insert codegc\n-\n-What would our code look likegc\n+First, let's assume there were no automatic transfers, assume we had to define them. We had to explicitly control transfers so we will insert code.\n@@ -85,0 +83,1 @@\n+What would our code look like?\n@@ -140,1 +139,1 @@\n-Alternatively what if the buffers themselves could hold the deviceDirty flags javaDirty?\n+Alternatively, what if the buffers themselves could hold the deviceDirty flags javaDirty?\n@@ -198,1 +197,1 @@\n-Essentially we defer to the kernel dispatch to determine whether buffers are\n+Essentially, we defer to the kernel dispatch to determine whether buffers are\n@@ -201,1 +200,1 @@\n-Psuedo code for dispatch is essentiallygc\n+Pseudo-code for dispatch is essentially\n@@ -237,1 +236,0 @@\n-\n","filename":"hat\/docs\/hat-minimizing-buffer-transfers.md","additions":5,"deletions":7,"binary":false,"changes":12,"status":"modified"},{"patch":"@@ -0,0 +1,58 @@\n+\n+### Refactor the existing code for closing over kernel+compute call chains with ModuleOp\n+----\n+\n+* [Contents](hat-00.md)\n+* House Keeping\n+    * [Project Layout](hat-01-01-project-layout.md)\n+    * [Building Babylon](hat-01-02-building-babylon.md)\n+    * [Building HAT](hat-01-03-building-hat.md)\n+* Programming Model\n+    * [Programming Model](hat-03-programming-model.md)\n+* Interface Mapping\n+    * [Interface Mapping Overview](hat-04-01-interface-mapping.md)\n+    * [Cascade Interface Mapping](hat-04-02-cascade-interface-mapping.md)\n+* Implementation Detail\n+    * [Walkthrough Of Accelerator.compute()](hat-accelerator-compute.md)\n+    * [How we minimize buffer transfers](hat-minimizing-buffer-transfers.md)\n+\n+----\n+\n+At present, we take the CodeModel (FuncOp rooted tree) of the kernel entrypoint, traverse it to\n+locate calls. We recursively traverse those calls and end up closing over the complete 'graph' of reachable calls.\n+\n+We have a test case which demonstrates a much more elegant way of doing this and exposing the whole 'closure' as a ModuleOp.\n+\n+See [test\/jdk\/java\/lang\/reflect\/code\/TestTransitiveInvokeModule.java](https:\/\/github.com\/openjdk\/babylon\/blob\/code-reflection\/test\/jdk\/java\/lang\/reflect\/code\/TestTransitiveInvokeModule.java)\n+\n+In my defen[sc]e ;) The hat code predated ModuleOp...\n+\n+So this task would involve taking the test code for generating this closure and use it rather than\n+the clunky existing version.\n+\n+\n+See [hat\/core\/src\/main\/java\/hat\/callgraph\/CallGraph.java](https:\/\/github.com\/openjdk\/babylon\/blob\/code-reflection\/hat\/core\/src\/main\/java\/hat\/callgraph\/CallGraph.java)\n+\n+And other code in [hat\/core\/src\/main\/java\/hat\/callgraph](https:\/\/github.com\/openjdk\/babylon\/tree\/code-reflection\/hat\/core\/src\/main\/java\/hat\/callgraph)\n+\n+The existing HAT code does also do some tests on the code (especially kernel code) to determine if the model is valid. We\n+\n+We check for allocations, exceptions, we assert that all method parameters are mapped byte buffers or primitives.\n+\n+There are other checks which we might add BTW.\n+\n+As a followup to this 'chore' .. For bonus points as it were\n+\n+Me might Consider inlining trivial methods (the life `var()` example above is an interesting candidate ;)\n+\n+We need to determine which buffers are mutated or just accessed from the kernel call graph.\n+This is especially important for minimising buffer transfers.\n+I planned to do this when I implemented the prototype, but it was tougher than I first thought.\n+\n+Tracing buffer aliasing across calls hurt my head a little.\n+\n+At present, we rely on annotations @RO,@RW and @WO on kernel args (and trust them) we might still need these for\n+compute calls which are not code reflectable. But we should not need them for kernel graphs.\n+\n+\n+\n","filename":"hat\/docs\/hat-replace-kernel-graph-with-module-op.md","additions":58,"deletions":0,"binary":false,"changes":58,"status":"added"}]}