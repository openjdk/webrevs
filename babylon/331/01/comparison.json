{"files":[{"patch":"@@ -205,2 +205,2 @@\n-    public static void printModel(ByteBuffer model) {\n-        ModelProto.print(0, model);\n+    public static void printModel(byte[] model) {\n+        ModelProto.print(0, ByteBuffer.wrap(model));\n","filename":"cr-examples\/onnx\/src\/main\/java\/oracle\/code\/onnx\/OnnxProtoPrinter.java","additions":2,"deletions":2,"binary":false,"changes":4,"status":"modified"},{"patch":"@@ -58,10 +58,8 @@\n-    public static <T> Tensor<T> execute(MethodHandles.Lookup l, OnnxFunction<Tensor<T>> codeLambda) {\n-        var quotable = Op.ofQuotable(codeLambda).orElseThrow();\n-        var lambda = (CoreOp.LambdaOp) quotable.op();\n-\n-        CoreOp.FuncOp onnxFunc;\n-        List<Tensor> arguments;\n-        \/\/ Shortcut for lambda expressions that call just one method\n-        if (singleMethodInvocation(lambda) instanceof\n-                SingleMethod(CoreOp.InvokeOp iop, Map<Value, Value> valueMapping)) {\n-            Method m;\n+    record CachedModel(byte[] protoModel, int[] operandsMapping) {}\n+\n+    static class CachedModelClassValue extends ClassValue<CachedModel> {\n+\n+        private MethodHandles.Lookup l;\n+        private Quoted q;\n+\n+        CachedModel computeIfAbsent(Class<?> lambdaClass, MethodHandles.Lookup l,  Quoted q) {\n@@ -69,3 +67,7 @@\n-                m = iop.invokeDescriptor().resolveToMethod(l, iop.invokeKind());\n-            } catch (ReflectiveOperationException e) {\n-                throw new RuntimeException(e);\n+                this.l = l;\n+                this.q = q;\n+                \/\/ not very nice way to pass additional arguments to computeValue method\n+                return get(lambdaClass);\n+            } finally {\n+                this.l = null;\n+                this.q = null;\n@@ -73,0 +75,1 @@\n+        }\n@@ -74,24 +77,43 @@\n-            CoreOp.FuncOp f = Op.ofMethod(m).orElseThrow();\n-            onnxFunc = OnnxTransformer.transform(l, f);\n-\n-            arguments = iop.operands().stream().toList().stream()\n-                    .map(valueMapping::get)\n-                    .map(v -> quotable.capturedValues().get(v))\n-                    .map(val -> val instanceof CoreOp.Var<?> v ? v.value() : val)\n-                    .map(val -> (Tensor) val)\n-                    .toList();\n-        } else {\n-            var capturedValues = lambda.capturedValues();\n-            var functionType = FunctionType.functionType(lambda.invokableType().returnType(),\n-                    capturedValues.stream().map(Value::type)\n-                            .map(t -> t instanceof VarType vt ? vt.valueType() : t).toList());\n-            onnxFunc = OnnxTransformer.transform(l, CoreOp.func(\"onnxCode\", functionType)\n-                    .body(bb -> {\n-                        bb.context().mapValues(capturedValues, bb.parameters());\n-                        for (Op op : lambda.body().entryBlock().ops()) {\n-                            int i;\n-                            if (op instanceof CoreOp.VarAccessOp.VarLoadOp load &&\n-                                    (i = capturedValues.indexOf(load.varOp().result())) >= 0) {\n-                                bb.context().mapValue(op.result(), bb.parameters().get(i)); \/\/ remap var load result to block param\n-                            } else {\n-                                bb.apply(op);\n+        @Override\n+        protected CachedModel computeValue(Class<?> type) {\n+            var lambda = (CoreOp.LambdaOp) q.op();\n+\n+            CoreOp.FuncOp onnxFunc;\n+            int[] operandsMapping;\n+\n+            \/\/ Shortcut for lambda expressions that call just one method\n+            if (singleMethodInvocation(lambda) instanceof\n+                    SingleMethod(CoreOp.InvokeOp iop, Map<Value, Value> valueMapping)) {\n+                Method m;\n+                try {\n+                    m = iop.invokeDescriptor().resolveToMethod(l, iop.invokeKind());\n+                } catch (ReflectiveOperationException e) {\n+                    throw new RuntimeException(e);\n+                }\n+\n+                CoreOp.FuncOp f = Op.ofMethod(m).orElseThrow();\n+                onnxFunc = OnnxTransformer.transform(l, f);\n+\n+                var operands = iop.operands();\n+                var captured = q.capturedValues().sequencedKeySet().stream().toList();\n+                operandsMapping = new int[iop.operands().size()];\n+                for (int i = 0; i < operandsMapping.length; i++) {\n+                    operandsMapping[i] = captured.indexOf(valueMapping.get(operands.get(i)));\n+                }\n+\n+            } else {\n+                var capturedValues = lambda.capturedValues();\n+                var functionType = FunctionType.functionType(lambda.invokableType().returnType(),\n+                        capturedValues.stream().map(Value::type)\n+                                .map(t -> t instanceof VarType vt ? vt.valueType() : t).toList());\n+                onnxFunc = OnnxTransformer.transform(l, CoreOp.func(\"onnxCode\", functionType)\n+                        .body(bb -> {\n+                            bb.context().mapValues(capturedValues, bb.parameters());\n+                            for (Op op : lambda.body().entryBlock().ops()) {\n+                                int i;\n+                                if (op instanceof CoreOp.VarAccessOp.VarLoadOp load &&\n+                                        (i = capturedValues.indexOf(load.varOp().result())) >= 0) {\n+                                    bb.context().mapValue(op.result(), bb.parameters().get(i)); \/\/ remap var load result to block param\n+                                } else {\n+                                    bb.apply(op);\n+                                }\n@@ -99,2 +121,1 @@\n-                        }\n-                    }));\n+                        }));\n@@ -102,4 +123,6 @@\n-            arguments = quotable.capturedValues().values().stream()\n-                    .map(val -> val instanceof CoreOp.Var<?> v ? v.value() : val)\n-                    .map(val -> (Tensor) val)\n-                    .toList();\n+                operandsMapping = new int[capturedValues.size()];\n+                for (int i = 0; i < operandsMapping.length; i++) {\n+                    operandsMapping[i] = i;\n+                }\n+            }\n+            return new CachedModel(OnnxProtoBuilder.build(onnxFunc.body().entryBlock()), operandsMapping);\n@@ -107,0 +130,15 @@\n+    }\n+\n+    private static final CachedModelClassValue MODEL_CACHE = new CachedModelClassValue();\n+\n+    public static <T> Tensor<T> execute(MethodHandles.Lookup l, OnnxFunction<Tensor<T>> codeLambda) {\n+        var q = Op.ofQuotable(codeLambda).orElseThrow();\n+\n+        var model = MODEL_CACHE.computeIfAbsent(codeLambda.getClass(), l, q);\n+\n+        var captured = q.capturedValues().sequencedValues().toArray();\n+        List<Tensor> arguments = IntStream.of(model.operandsMapping())\n+                .mapToObj(i -> captured[i])\n+                .map(val -> val instanceof CoreOp.Var<?> v ? v.value() : val)\n+                .map(val -> (Tensor) val)\n+                .toList();\n@@ -108,1 +146,1 @@\n-        try (var session = getInstance().createSession(OnnxProtoBuilder.build(onnxFunc.body().entryBlock()))) {\n+        try (var session = getInstance().createSession(model.protoModel())) {\n@@ -110,0 +148,3 @@\n+        } catch (RuntimeException e) {\n+            OnnxProtoPrinter.printModel(model.protoModel());\n+            throw e;\n","filename":"cr-examples\/onnx\/src\/main\/java\/oracle\/code\/onnx\/OnnxRuntime.java","additions":85,"deletions":44,"binary":false,"changes":129,"status":"modified"},{"patch":"@@ -49,1 +49,0 @@\n-import java.util.List;\n@@ -340,13 +339,0 @@\n-    static List<Tensor> loadWeights() throws IOException {\n-        return List.of(floatTensor(\"conv1-weight-float-le\", 6, 1, 5, 5),\n-                       floatTensor(\"conv1-bias-float-le\", 6),\n-                       floatTensor(\"conv2-weight-float-le\", 16, 6, 5, 5),\n-                       floatTensor(\"conv2-bias-float-le\", 16),\n-                       floatTensor(\"fc1-weight-float-le\", 120, 256),\n-                       floatTensor(\"fc1-bias-float-le\", 120),\n-                       floatTensor(\"fc2-weight-float-le\", 84, 120),\n-                       floatTensor(\"fc2-bias-float-le\", 84),\n-                       floatTensor(\"fc3-weight-float-le\", 10, 84),\n-                       floatTensor(\"fc3-bias-float-le\", 10));\n-    }\n-\n@@ -367,3 +353,12 @@\n-        var weights = loadWeights();\n-        test(inputImage -> cnn(weights.get(0), weights.get(1), weights.get(2), weights.get(3), weights.get(4),\n-                               weights.get(5), weights.get(6), weights.get(7), weights.get(8), weights.get(9),\n+        var conv1Weight = floatTensor(\"conv1-weight-float-le\", 6, 1, 5, 5);\n+        var conv1Bias = floatTensor(\"conv1-bias-float-le\", 6);\n+        var conv2Weight = floatTensor(\"conv2-weight-float-le\", 16, 6, 5, 5);\n+        var conv2Bias = floatTensor(\"conv2-bias-float-le\", 16);\n+        var fc1Weight = floatTensor(\"fc1-weight-float-le\", 120, 256);\n+        var fc1Bias = floatTensor(\"fc1-bias-float-le\", 120);\n+        var fc2Weight = floatTensor(\"fc2-weight-float-le\", 84, 120);\n+        var fc2Bias = floatTensor(\"fc2-bias-float-le\", 84);\n+        var fc3Weight = floatTensor(\"fc3-weight-float-le\", 10, 84);\n+        var fc3Bias = floatTensor(\"fc3-bias-float-le\", 10);\n+        test(inputImage -> cnn(conv1Weight, conv1Bias, conv2Weight, conv2Bias,\n+                               fc1Weight, fc1Bias, fc2Weight, fc2Bias, fc3Weight, fc3Bias,\n@@ -375,4 +370,14 @@\n-        var weights = loadWeights();\n-        test(inputImage -> OnnxRuntime.getInstance().run(\n-                    OnnxTransformer.transform(MethodHandles.lookup(), getFuncOp(\"cnn\")).body().entryBlock(),\n-                    Stream.concat(weights.stream(), Stream.of(inputImage)).toList()).getFirst());\n+        var conv1Weight = floatTensor(\"conv1-weight-float-le\", 6, 1, 5, 5);\n+        var conv1Bias = floatTensor(\"conv1-bias-float-le\", 6);\n+        var conv2Weight = floatTensor(\"conv2-weight-float-le\", 16, 6, 5, 5);\n+        var conv2Bias = floatTensor(\"conv2-bias-float-le\", 16);\n+        var fc1Weight = floatTensor(\"fc1-weight-float-le\", 120, 256);\n+        var fc1Bias = floatTensor(\"fc1-bias-float-le\", 120);\n+        var fc2Weight = floatTensor(\"fc2-weight-float-le\", 84, 120);\n+        var fc2Bias = floatTensor(\"fc2-bias-float-le\", 84);\n+        var fc3Weight = floatTensor(\"fc3-weight-float-le\", 10, 84);\n+        var fc3Bias = floatTensor(\"fc3-bias-float-le\", 10);\n+        test(inputImage -> OnnxRuntime.execute(MethodHandles.lookup(), () ->\n+                cnn(conv1Weight, conv1Bias, conv2Weight, conv2Bias,\n+                    fc1Weight, fc1Bias, fc2Weight, fc2Bias, fc3Weight, fc3Bias,\n+                    inputImage)));\n","filename":"cr-examples\/onnx\/src\/test\/java\/oracle\/code\/onnx\/CNNTest.java","additions":26,"deletions":21,"binary":false,"changes":47,"status":"modified"},{"patch":"@@ -5,1 +5,0 @@\n-import java.util.List;\n@@ -7,1 +6,0 @@\n-import java.util.stream.Stream;\n@@ -9,3 +7,0 @@\n-import jdk.incubator.code.Op;\n-import jdk.incubator.code.op.CoreOp;\n-import oracle.code.onnx.compiler.OnnxTransformer;\n@@ -25,0 +20,12 @@\n+        assertEquals(\n+                add(a, a),\n+                OnnxRuntime.execute(MethodHandles.lookup(), () -> add(a, a)));\n+    }\n+\n+    @CodeReflection\n+    public static Tensor<Float> sub(Tensor<Float> a, Tensor<Float> b) {\n+        return OnnxOperators.Sub(a, b);\n+    }\n+\n+    @Test\n+    public void testSub() throws Exception {\n@@ -26,0 +33,1 @@\n+        var a = Tensor.ofFlat(1f, 2, 3);\n@@ -27,2 +35,2 @@\n-                add(a, b),\n-                runModel(\"add\", a, b));\n+                sub(a, b),\n+                OnnxRuntime.execute(MethodHandles.lookup(), () -> sub(a, b)));\n@@ -41,1 +49,1 @@\n-        assertEquals(expected, runModel(\"fconstant\"));\n+        assertEquals(expected, OnnxRuntime.execute(MethodHandles.lookup(), () -> fconstant()));\n@@ -54,1 +62,1 @@\n-        assertEquals(expected, runModel(\"fconstants\"));\n+        assertEquals(expected, OnnxRuntime.execute(MethodHandles.lookup(), () -> fconstants()));\n@@ -67,1 +75,1 @@\n-        assertEquals(expected, runModel(\"lconstant\"));\n+        assertEquals(expected, OnnxRuntime.execute(MethodHandles.lookup(), () -> lconstant()));\n@@ -80,1 +88,1 @@\n-        assertEquals(expected, runModel(\"lconstants\"));\n+        assertEquals(expected, OnnxRuntime.execute(MethodHandles.lookup(), () -> lconstants()));\n@@ -94,1 +102,1 @@\n-                runModel(\"reshapeAndShape\", data, shape));\n+                OnnxRuntime.execute(MethodHandles.lookup(), () -> reshapeAndShape(data, shape)));\n@@ -108,10 +116,1 @@\n-                runModel(\"indicesOfMaxPool\", x));\n-    }\n-\n-    private static Tensor runModel(String name, Tensor... params) throws NoSuchMethodException {\n-        return OnnxRuntime.getInstance().run(getOnnxModel(name).body().entryBlock(), List.of(params)).getFirst();\n-    }\n-\n-    private static CoreOp.FuncOp getOnnxModel(String name) throws NoSuchMethodException {\n-        return OnnxTransformer.transform(MethodHandles.lookup(),\n-                Op.ofMethod(Stream.of(SimpleTest.class.getDeclaredMethods()).filter(m -> m.getName().equals(name)).findFirst().get()).get());\n+                OnnxRuntime.execute(MethodHandles.lookup(), () -> indicesOfMaxPool(x)));\n","filename":"cr-examples\/onnx\/src\/test\/java\/oracle\/code\/onnx\/SimpleTest.java","additions":21,"deletions":22,"binary":false,"changes":43,"status":"modified"}]}