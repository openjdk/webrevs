{"files":[{"patch":"@@ -30,8 +30,1 @@\n-import hat.dialect.HATF16ConvOp;\n-import hat.dialect.HATVectorBinaryOp;\n-import hat.dialect.HATVectorLoadOp;\n-import hat.dialect.HATVectorOfOp;\n-import hat.dialect.HATVectorSelectLoadOp;\n-import hat.dialect.HATVectorSelectStoreOp;\n-import hat.dialect.HATVectorStoreView;\n-import hat.dialect.HATVectorVarOp;\n+import hat.dialect.*;\n@@ -41,0 +34,2 @@\n+import java.util.List;\n+\n@@ -236,0 +231,14 @@\n+    @Override\n+    public CudaHATKernelBuilder hatF16ToFloatConvOp(ScopedCodeBuilderContext builderContext, HATF16ToFloatConvOp hatF16ToFloatConvOp) {\n+        identifier(\"__half2float\").oparen();\n+        Value param =  hatF16ToFloatConvOp.operands().getFirst();\n+        if (param instanceof Op.Result r) {\n+            recurse(builderContext, r.op());\n+        }\n+        if (!hatF16ToFloatConvOp.isLocal()) {\n+            rarrow().identifier(\"value\");\n+        }\n+        cparen();\n+        return self();\n+    }\n+\n@@ -260,0 +269,45 @@\n+\n+    @Override\n+    public CudaHATKernelBuilder hatF16BinaryOp(ScopedCodeBuilderContext buildContext, HATF16BinaryOp hatF16BinaryOp) {\n+        oparen();\n+        Value op1 = hatF16BinaryOp.operands().get(0);\n+        Value op2 = hatF16BinaryOp.operands().get(1);\n+        List<Boolean> references = hatF16BinaryOp.references();\n+        byte f32Mixed = hatF16BinaryOp.getF32();\n+\n+        if (f32Mixed == HATF16BinaryOp.LAST_OP) {\n+            identifier(\"__half2float\").oparen();\n+        }\n+\n+        if (op1 instanceof Op.Result r) {\n+            recurse(buildContext, r.op());\n+        }\n+        if (references.getFirst()) {\n+            rarrow().identifier(\"value\");\n+        }\n+\n+        if (f32Mixed == HATF16BinaryOp.LAST_OP) {\n+            cparen();\n+        }\n+\n+        space().identifier(hatF16BinaryOp.operationType().symbol()).space();\n+\n+        if (f32Mixed == HATF16BinaryOp.FIRST_OP) {\n+            identifier(\"__half2float\").oparen();\n+        }\n+\n+        if (op2 instanceof Op.Result r) {\n+            recurse(buildContext, r.op());\n+        }\n+\n+        if (references.get(1)) {\n+            rarrow().identifier(\"value\");\n+        }\n+\n+        if (f32Mixed == HATF16BinaryOp.FIRST_OP) {\n+            cparen();\n+        }\n+\n+        cparen();\n+        return self();\n+    }\n","filename":"hat\/backends\/ffi\/cuda\/src\/main\/java\/hat\/backend\/ffi\/CudaHATKernelBuilder.java","additions":62,"deletions":8,"binary":false,"changes":70,"status":"modified"},{"patch":"@@ -31,0 +31,1 @@\n+import hat.dialect.HATF16ToFloatConvOp;\n@@ -41,2 +42,0 @@\n-import java.util.List;\n-\n@@ -186,1 +185,1 @@\n-       oparen().halfType().cparen();\n+        oparen().halfType().cparen();\n@@ -213,0 +212,13 @@\n+\n+    @Override\n+    public OpenCLHATKernelBuilder hatF16ToFloatConvOp(ScopedCodeBuilderContext builderContext, HATF16ToFloatConvOp hatF16ToFloatConvOp) {\n+        oparen().halfType().cparen();\n+        Value initValue = hatF16ToFloatConvOp.operands().getFirst();\n+        if (initValue instanceof Op.Result r) {\n+            recurse(builderContext, r.op());\n+        }\n+        if (!hatF16ToFloatConvOp.isLocal()) {\n+            rarrow().identifier(\"value\");\n+        }\n+        return self();\n+    }\n","filename":"hat\/backends\/ffi\/opencl\/src\/main\/java\/hat\/backend\/ffi\/OpenCLHATKernelBuilder.java","additions":15,"deletions":3,"binary":false,"changes":18,"status":"modified"},{"patch":"@@ -31,0 +31,1 @@\n+import hat.dialect.HATF16ToFloatConvOp;\n@@ -41,2 +42,0 @@\n-import java.util.List;\n-\n@@ -214,0 +213,7 @@\n+    @Override\n+    public OpenCLHatKernelBuilder hatF16ToFloatConvOp(ScopedCodeBuilderContext builderContext, HATF16ToFloatConvOp hatF16ToFloatConvOp) {\n+        oparen().halfType().cparen();\n+        identifier(hatF16ToFloatConvOp.varName());\n+        return self();\n+    }\n+\n","filename":"hat\/backends\/jextracted\/opencl\/src\/main\/java\/hat\/backend\/jextracted\/OpenCLHatKernelBuilder.java","additions":8,"deletions":2,"binary":false,"changes":10,"status":"modified"},{"patch":"@@ -0,0 +1,136 @@\n+\/*\n+ * Copyright (c) 2025, Oracle and\/or its affiliates. All rights reserved.\n+ * DO NOT ALTER OR REMOVE COPYRIGHT NOTICES OR THIS FILE HEADER.\n+ *\n+ * This code is free software; you can redistribute it and\/or modify it\n+ * under the terms of the GNU General Public License version 2 only, as\n+ * published by the Free Software Foundation.  Oracle designates this\n+ * particular file as subject to the \"Classpath\" exception as provided\n+ * by Oracle in the LICENSE file that accompanied this code.\n+ *\n+ * This code is distributed in the hope that it will be useful, but WITHOUT\n+ * ANY WARRANTY; without even the implied warranty of MERCHANTABILITY or\n+ * FITNESS FOR A PARTICULAR PURPOSE.  See the GNU General Public License\n+ * version 2 for more details (a copy is included in the LICENSE file that\n+ * accompanied this code).\n+ *\n+ * You should have received a copy of the GNU General Public License version\n+ * 2 along with this work; if not, write to the Free Software Foundation,\n+ * Inc., 51 Franklin St, Fifth Floor, Boston, MA 02110-1301 USA.\n+ *\n+ * Please contact Oracle, 500 Oracle Parkway, Redwood Shores, CA 94065 USA\n+ * or visit www.oracle.com if you need additional information or have any\n+ * questions.\n+ *\/\n+package hat.buffer;\n+\n+\/\/ Interface for Floating Point numbers of 16-bits\n+\/\/ Values are stored in a short format.\n+public interface F16 extends Buffer.Struct {\n+    String HAT_MAPPING_TYPE = \"half\";\n+\n+    short value();\n+    void value(short value);\n+\n+    \/**\n+     * Intrinsic for the HAT compiler to create a new half.\n+     *\/\n+    static F16 of(float value) {\n+        return new F16() {\n+            @Override\n+            public short value() {\n+                return Float.floatToFloat16(value);\n+            }\n+\n+            @Override\n+            public void value(short value) {\n+            }\n+        };\n+    }\n+\n+    static F16 of(short value) {\n+        return new F16() {\n+            @Override\n+            public short value() {\n+                return value;\n+            }\n+\n+            @Override\n+            public void value(short value) {\n+            }\n+        };\n+    }\n+\n+    \/**\n+     * Built-in that can be in HAT Kernel Java code to transform a float into a {@link F16} value.\n+     *\/\n+    static F16 floatToF16(float value) {\n+        return of(value);\n+    }\n+\n+    \/**\n+     * Built-in that can be used in Kernel Code to transform an {@link F16} into a float.\n+     *\n+     * @param value {@link F16}\n+     * @return float\n+     *\/\n+    static float f16ToFloat(F16 value) {\n+        return Float.float16ToFloat(value.value());\n+    }\n+\n+    static F16 add(F16 ha, F16 hb) {\n+        return F16.of(f16ToFloat(ha) + f16ToFloat(hb));\n+    }\n+\n+    static F16 add(float f32, F16 hb) {\n+        return F16.of(f32 + f16ToFloat(hb));\n+    }\n+\n+    static F16 sub(F16 ha, F16 hb) {\n+        return F16.of(f16ToFloat(ha) - f16ToFloat(hb));\n+    }\n+\n+    static F16 sub(float f32, F16 hb) {\n+        return F16.of(f32 - f16ToFloat(hb));\n+    }\n+\n+    static F16 sub(F16 hb, float f32) {\n+        return F16.of(f16ToFloat(hb) - f32);\n+    }\n+\n+    static F16 mul(F16 ha, F16 hb) {\n+        return F16.of(f16ToFloat(ha) * f16ToFloat(hb));\n+    }\n+\n+    static F16 mul(float f32, F16 hb) {\n+        return F16.of(f32 * f16ToFloat(hb));\n+    }\n+\n+    static F16 div(F16 ha, F16 hb) {\n+        return F16.of(f16ToFloat(ha) \/ f16ToFloat(hb));\n+    }\n+\n+    static F16 div(float f32, F16 hb) {\n+        return F16.of(f32 \/ f16ToFloat(hb));\n+    }\n+\n+    static F16 add(F16 hb, float f32) {\n+        return F16.of(f16ToFloat(hb) \/ f32);\n+    }\n+\n+    default F16 add(F16 ha) {\n+        return F16.add(this, ha);\n+    }\n+\n+    default F16 sub(F16 ha) {\n+        return F16.sub(this, ha);\n+    }\n+\n+    default F16 mul(F16 ha) {\n+        return F16.mul(this, ha);\n+    }\n+\n+    default F16 div(F16 ha) {\n+        return F16.div(this, ha);\n+    }\n+}\n\\ No newline at end of file\n","filename":"hat\/core\/src\/main\/java\/hat\/buffer\/F16.java","additions":136,"deletions":0,"binary":false,"changes":136,"status":"added"},{"patch":"@@ -33,49 +33,0 @@\n-    \/\/ Interface for Floating Point numbers of 16-bits\n-    \/\/ Values are stored in a short format.\n-    interface F16 extends Struct {\n-        String HAT_MAPPING_TYPE = \"half\";\n-\n-        short value();\n-        void value(short value);\n-\n-        \/\/ Intrinsic for the HAT compiler to create a\n-        \/\/ new half\n-        String F16_INSTANCE_OF = \"of\";\n-        static F16 of(float value) {\n-            return new F16() {\n-                @Override\n-                public short value() {\n-                    return floatToF16(value);\n-                }\n-\n-                @Override\n-                public void value(short value) {\n-                }\n-            };\n-        }\n-\n-        static short floatToF16(float value) {\n-            return Float.floatToFloat16(value);\n-        }\n-\n-        static float f16ToFloat(short value) {\n-            return Float.float16ToFloat(value);\n-        }\n-\n-        static F16 add(F16 ha, F16 hb) {\n-            return F16.of(f16ToFloat(ha.value()) + f16ToFloat(hb.value()));\n-        }\n-\n-        static F16 sub(F16 ha, F16 hb) {\n-            return F16.of(f16ToFloat(ha.value()) - f16ToFloat(hb.value()));\n-        }\n-\n-        static F16 mul(F16 ha, F16 hb) {\n-            return F16.of(f16ToFloat(ha.value()) * f16ToFloat(hb.value()));\n-        }\n-\n-        static F16 div(F16 ha, F16 hb) {\n-            return F16.of(f16ToFloat(ha.value()) \/ f16ToFloat(hb.value()));\n-        }\n-    }\n-\n","filename":"hat\/core\/src\/main\/java\/hat\/buffer\/F16Array.java","additions":0,"deletions":49,"binary":false,"changes":49,"status":"modified"},{"patch":"@@ -31,0 +31,1 @@\n+import hat.dialect.HATF16ToFloatConvOp;\n@@ -150,0 +151,2 @@\n+    T hatF16ToFloatConvOp(ScopedCodeBuilderContext builderContext, HATF16ToFloatConvOp hatF16ToFloatConvOp);\n+\n@@ -198,0 +201,1 @@\n+            case HATF16ToFloatConvOp $ -> hatF16ToFloatConvOp(buildContext, $);\n","filename":"hat\/core\/src\/main\/java\/hat\/codebuilders\/BabylonOpBuilder.java","additions":4,"deletions":0,"binary":false,"changes":4,"status":"modified"},{"patch":"@@ -27,0 +27,1 @@\n+import hat.buffer.F16;\n@@ -49,2 +50,0 @@\n-import static hat.buffer.F16Array.F16;\n-\n@@ -490,0 +489,1 @@\n+\n@@ -491,0 +491,1 @@\n+                        oparen();\n@@ -513,0 +514,4 @@\n+                   if (OpTk.javaReturnType(invokeOp) instanceof ClassType) {\n+                       cparen();\n+                   }\n+\n","filename":"hat\/core\/src\/main\/java\/hat\/codebuilders\/HATCodeBuilderWithContext.java","additions":7,"deletions":2,"binary":false,"changes":9,"status":"modified"},{"patch":"@@ -37,2 +37,2 @@\n-    public HATF16AddOp(TypeElement typeElement, List<Boolean> references, List<Value> operands) {\n-        super(typeElement, HATF16BinaryOp.OpType.ADD, references, operands);\n+    public HATF16AddOp(TypeElement typeElement, List<Boolean> references, byte f32, List<Value> operands) {\n+        super(typeElement, HATF16BinaryOp.OpType.ADD, references, f32, operands);\n","filename":"hat\/core\/src\/main\/java\/hat\/dialect\/HATF16AddOp.java","additions":2,"deletions":2,"binary":false,"changes":4,"status":"modified"},{"patch":"@@ -39,0 +39,4 @@\n+    protected final byte f32;\n+\n+    public static final byte FIRST_OP = 0x01;\n+    public static final byte LAST_OP = 0x10;\n@@ -57,1 +61,1 @@\n-    public HATF16BinaryOp(TypeElement typeElement, OpType operationType, List<Boolean> references, List<Value> operands) {\n+    public HATF16BinaryOp(TypeElement typeElement, OpType operationType, List<Boolean> references, byte f32, List<Value> operands) {\n@@ -62,0 +66,1 @@\n+        this.f32 = f32;\n@@ -69,0 +74,1 @@\n+        this.f32 = op.f32;\n@@ -89,0 +95,4 @@\n+    public byte getF32() {\n+        return f32;\n+    }\n+\n","filename":"hat\/core\/src\/main\/java\/hat\/dialect\/HATF16BinaryOp.java","additions":11,"deletions":1,"binary":false,"changes":12,"status":"modified"},{"patch":"@@ -37,2 +37,2 @@\n-    public HATF16DivOp(TypeElement typeElement, List<Boolean> references, List<Value> operands) {\n-        super(typeElement, OpType.DIV, references, operands);\n+    public HATF16DivOp(TypeElement typeElement, List<Boolean> references, byte f32, List<Value> operands) {\n+        super(typeElement, OpType.DIV, references, f32, operands);\n","filename":"hat\/core\/src\/main\/java\/hat\/dialect\/HATF16DivOp.java","additions":2,"deletions":2,"binary":false,"changes":4,"status":"modified"},{"patch":"@@ -37,2 +37,2 @@\n-    public HATF16MulOp(TypeElement typeElement, List<Boolean> references, List<Value> operands) {\n-        super(typeElement, OpType.MUL, references, operands);\n+    public HATF16MulOp(TypeElement typeElement, List<Boolean> references, byte f32, List<Value> operands) {\n+        super(typeElement, OpType.MUL, references, f32, operands);\n","filename":"hat\/core\/src\/main\/java\/hat\/dialect\/HATF16MulOp.java","additions":2,"deletions":2,"binary":false,"changes":4,"status":"modified"},{"patch":"@@ -37,2 +37,2 @@\n-    public HATF16SubOp(TypeElement typeElement, List<Boolean> references, List<Value> operands) {\n-        super(typeElement, OpType.SUB, references, operands);\n+    public HATF16SubOp(TypeElement typeElement, List<Boolean> references, byte f32, List<Value> operands) {\n+        super(typeElement, OpType.SUB, references, f32, operands);\n","filename":"hat\/core\/src\/main\/java\/hat\/dialect\/HATF16SubOp.java","additions":2,"deletions":2,"binary":false,"changes":4,"status":"modified"},{"patch":"@@ -0,0 +1,72 @@\n+\/*\n+ * Copyright (c) 2025, Oracle and\/or its affiliates. All rights reserved.\n+ * DO NOT ALTER OR REMOVE COPYRIGHT NOTICES OR THIS FILE HEADER.\n+ *\n+ * This code is free software; you can redistribute it and\/or modify it\n+ * under the terms of the GNU General Public License version 2 only, as\n+ * published by the Free Software Foundation.  Oracle designates this\n+ * particular file as subject to the \"Classpath\" exception as provided\n+ * by Oracle in the LICENSE file that accompanied this code.\n+ *\n+ * This code is distributed in the hope that it will be useful, but WITHOUT\n+ * ANY WARRANTY; without even the implied warranty of MERCHANTABILITY or\n+ * FITNESS FOR A PARTICULAR PURPOSE.  See the GNU General Public License\n+ * version 2 for more details (a copy is included in the LICENSE file that\n+ * accompanied this code).\n+ *\n+ * You should have received a copy of the GNU General Public License version\n+ * 2 along with this work; if not, write to the Free Software Foundation,\n+ * Inc., 51 Franklin St, Fifth Floor, Boston, MA 02110-1301 USA.\n+ *\n+ * Please contact Oracle, 500 Oracle Parkway, Redwood Shores, CA 94065 USA\n+ * or visit www.oracle.com if you need additional information or have any\n+ * questions.\n+ *\/\n+package hat.dialect;\n+\n+import jdk.incubator.code.CopyContext;\n+import jdk.incubator.code.Op;\n+import jdk.incubator.code.OpTransformer;\n+import jdk.incubator.code.TypeElement;\n+import jdk.incubator.code.Value;\n+\n+import java.util.List;\n+import java.util.Map;\n+\n+public class HATF16ToFloatConvOp extends HATF16Op {\n+\n+    private final TypeElement typeElement;\n+    private final boolean isLocal;\n+\n+    public HATF16ToFloatConvOp(TypeElement typeElement, boolean isLocal, List<Value> operands) {\n+        super(\"\", operands);\n+        this.typeElement = typeElement;\n+        this.isLocal = isLocal;\n+    }\n+\n+    public HATF16ToFloatConvOp(HATF16ToFloatConvOp op, CopyContext copyContext) {\n+        super(op, copyContext);\n+        this.typeElement = op.typeElement;\n+        this.isLocal = op.isLocal;\n+    }\n+\n+    @Override\n+    public Op transform(CopyContext copyContext, OpTransformer opTransformer) {\n+        return new HATF16ToFloatConvOp(this, copyContext);\n+    }\n+\n+    @Override\n+    public TypeElement resultType() {\n+        return typeElement;\n+    }\n+\n+    @Override\n+    public Map<String, Object> externalize() {\n+        return Map.of(\"hat.dialect.f16ToFloat\", typeElement);\n+    }\n+\n+    public boolean isLocal() {\n+        return isLocal;\n+    }\n+\n+}\n","filename":"hat\/core\/src\/main\/java\/hat\/dialect\/HATF16ToFloatConvOp.java","additions":72,"deletions":0,"binary":false,"changes":72,"status":"added"},{"patch":"@@ -169,0 +169,16 @@\n+    public static boolean findF16IsLocal(CoreOp.VarAccessOp.VarLoadOp varLoadOp) {\n+        return findF16IsLocal(varLoadOp.operands().getFirst());\n+    }\n+\n+    public static boolean findF16IsLocal(Value v) {\n+        if (v instanceof Op.Result r && r.op() instanceof CoreOp.VarAccessOp.VarLoadOp varLoadOp) {\n+            return findF16IsLocal(varLoadOp);\n+        } else {\n+            \/\/ Leaf of tree -\n+            if (v instanceof CoreOp.Result r && r.op() instanceof HATF16VarOp hatf16VarOp) {\n+                return true;\n+            }\n+            return false;\n+        }\n+    }\n+\n","filename":"hat\/core\/src\/main\/java\/hat\/dialect\/HATPhaseUtils.java","additions":16,"deletions":0,"binary":false,"changes":16,"status":"modified"},{"patch":"@@ -28,1 +28,0 @@\n-import hat.Config;\n@@ -32,1 +31,0 @@\n-import hat.dialect.HATF16BinaryOp;\n@@ -36,0 +34,2 @@\n+import hat.dialect.HATF16ToFloatConvOp;\n+import hat.dialect.HATF16VarLoadOp;\n@@ -426,0 +426,1 @@\n+            case HATF16VarLoadOp o -> 0;\n@@ -431,0 +432,1 @@\n+            case HATF16ToFloatConvOp o -> 1;\n","filename":"hat\/core\/src\/main\/java\/hat\/optools\/OpTk.java","additions":4,"deletions":2,"binary":false,"changes":6,"status":"modified"},{"patch":"@@ -28,1 +28,1 @@\n-import hat.buffer.F16Array;\n+import hat.buffer.F16;\n@@ -35,0 +35,1 @@\n+import hat.dialect.HATF16ToFloatConvOp;\n@@ -44,0 +45,1 @@\n+import jdk.incubator.code.dialect.core.VarType;\n@@ -52,1 +54,1 @@\n-import static hat.buffer.F16Array.F16;\n+import static hat.dialect.HATPhaseUtils.findF16IsLocal;\n@@ -79,1 +81,1 @@\n-        boolean isFP16Operation = invokeClassName.replace(\"$\", \".\").startsWith(F16Array.F16.class.getCanonicalName());\n+        boolean isFP16Operation = invokeClassName.replace(\"$\", \".\").startsWith(F16.class.getCanonicalName());\n@@ -101,0 +103,17 @@\n+    private boolean isOperandF32(CoreOp.VarAccessOp.VarLoadOp varLoadOp) {\n+        return isOperandF32(varLoadOp.operands().get(0));\n+    }\n+\n+    private boolean isOperandF32(Value v) {\n+        if (v instanceof Op.Result r && r.op() instanceof CoreOp.VarAccessOp.VarLoadOp varLoadOp) {\n+            return isOperandF32(varLoadOp);\n+        } else {\n+            if (v instanceof CoreOp.Result r && r.op() instanceof CoreOp.VarOp varOp) {\n+                VarType varType = varOp.resultType();\n+                TypeElement typeElement = varType.valueType();\n+                return typeElement == JavaType.FLOAT;\n+            }\n+            return false;\n+        }\n+    }\n+\n@@ -119,0 +138,10 @@\n+    private void createFloatFromF16(JavaOp.InvokeOp invokeOp, Block.Builder blockBuilder) {\n+        List<Value> operands = invokeOp.operands();\n+        List<Value> outputOperands = blockBuilder.context().getValues(operands);\n+        boolean isLocal = findF16IsLocal(operands.getFirst());\n+        HATF16ToFloatConvOp convOp1 = new HATF16ToFloatConvOp(JavaType.FLOAT, isLocal, outputOperands);\n+        Op.Result op1 = blockBuilder.op(convOp1);\n+        convOp1.setLocation(invokeOp.location());\n+        blockBuilder.context().mapValue(invokeOp.result(), op1);\n+    }\n+\n@@ -132,0 +161,1 @@\n+\n@@ -138,0 +168,7 @@\n+        byte valF32Conversion = 0x00;\n+        if (!isFirstOperandReference && isOperandF32(invokeOp.operands().getFirst())) {\n+            valF32Conversion = HATF16BinaryOp.FIRST_OP;\n+        } else if (!isSecondOperandReference && isOperandF32(invokeOp.operands().get(1))) {\n+            valF32Conversion = HATF16BinaryOp.LAST_OP;\n+        }\n+\n@@ -142,4 +179,4 @@\n-            case ADD -> new HATF16AddOp(typeElement, refList, outputOperands);\n-            case SUB -> new HATF16SubOp(typeElement, refList, outputOperands);\n-            case MUL -> new HATF16MulOp(typeElement, refList, outputOperands);\n-            case DIV -> new HATF16DivOp(typeElement, refList, outputOperands);\n+            case ADD -> new HATF16AddOp(typeElement, refList, valF32Conversion, outputOperands);\n+            case SUB -> new HATF16SubOp(typeElement, refList, valF32Conversion, outputOperands);\n+            case MUL -> new HATF16MulOp(typeElement, refList, valF32Conversion, outputOperands);\n+            case DIV -> new HATF16DivOp(typeElement, refList, valF32Conversion, outputOperands);\n@@ -229,0 +266,5 @@\n+    private boolean isInitMethodForF16(JavaOp.InvokeOp invokeOp) {\n+        return (isFP16Operation(invokeOp, \"of\")\n+                || isFP16Operation(invokeOp, \"floatToF16\"));\n+    }\n+\n@@ -236,1 +278,1 @@\n-                        if (isFP16Operation(invokeOp, F16.F16_INSTANCE_OF) && invokeOp.resultType() != JavaType.VOID) {\n+                        if (isInitMethodForF16(invokeOp) && invokeOp.resultType() != JavaType.VOID) {\n@@ -263,0 +305,26 @@\n+    private CoreOp.FuncOp dialectifyF16ToFloat(CoreOp.FuncOp funcOp) {\n+        var here = OpTk.CallSite.of(this.getClass(), \"dialectifyF16ToFloat\");\n+        before(here,funcOp);\n+        Stream<CodeElement<?, ?>> halfOps = funcOp.elements()\n+                .mapMulti(((codeElement, consumer) -> {\n+                    if (codeElement instanceof JavaOp.InvokeOp invokeOp) {\n+                        if (isMethod(invokeOp, \"f16ToFloat\")\n+                                && invokeOp.resultType() == JavaType.FLOAT) {\n+                            consumer.accept(invokeOp);\n+                        }\n+                    }\n+                }));\n+\n+        Set<CodeElement<?, ?>> nodesInvolved = halfOps.collect(Collectors.toSet());\n+        funcOp = funcOp.transform((blockBuilder, op) -> {\n+            if (!nodesInvolved.contains(op)) {\n+                blockBuilder.op(op);\n+            } else if (op instanceof JavaOp.InvokeOp invokeOp) {\n+                createFloatFromF16(invokeOp, blockBuilder);\n+            }\n+            return blockBuilder;\n+        });\n+        after(here, funcOp);\n+        return funcOp;\n+    }\n+\n@@ -281,1 +349,1 @@\n-            \/\/ Operations\n+            \/\/ F16 Operations\n@@ -286,0 +354,1 @@\n+        funcOp = dialectifyF16ToFloat(funcOp);\n","filename":"hat\/core\/src\/main\/java\/hat\/phases\/HATDialectifyFP16Phase.java","additions":78,"deletions":9,"binary":false,"changes":87,"status":"modified"},{"patch":"@@ -120,1 +120,1 @@\n-        Stream<CodeElement<?, ?>> float4NodesInvolved = funcOp.elements()\n+        Stream<CodeElement<?, ?>> vectorSelectOps = funcOp.elements()\n@@ -123,3 +123,2 @@\n-                        if (isVectorOperation(invokeOp) && invokeOp.resultType() != JavaType.VOID) {\n-                            List<Value> inputOperandsInvoke = invokeOp.operands();\n-                            Value inputOperand = inputOperandsInvoke.getFirst();\n+                        if (isVectorOperation(invokeOp) && (invokeOp.resultType() != JavaType.VOID)) {\n+                            Value inputOperand = invokeOp.operands().getFirst();\n@@ -134,3 +133,2 @@\n-        Set<CodeElement<?, ?>> nodesInvolved = float4NodesInvolved.collect(Collectors.toSet());\n-\n-           funcOp = OpTk.transform(here, funcOp,(blockBuilder, op) -> {\n+        Set<CodeElement<?, ?>> nodesInvolved = vectorSelectOps.collect(Collectors.toSet());\n+        funcOp = OpTk.transform(here, funcOp, (blockBuilder, op) -> {\n@@ -165,1 +163,1 @@\n-       after(here,funcOp);\n+        after(here, funcOp);\n","filename":"hat\/core\/src\/main\/java\/hat\/phases\/HATDialectifyVectorSelectPhase.java","additions":6,"deletions":8,"binary":false,"changes":14,"status":"modified"},{"patch":"@@ -32,0 +32,2 @@\n+import hat.annotations.Kernel;\n+import hat.annotations.Preformatted;\n@@ -33,0 +35,2 @@\n+import hat.buffer.F16;\n+import hat.buffer.F16Array;\n@@ -34,1 +38,0 @@\n-\n@@ -459,0 +462,235 @@\n+    private interface SharedMemoryHalf extends Buffer {\n+        void array(long index, short value);\n+        short array(long index);\n+        Schema<SharedMemoryHalf> schema = Schema.of(SharedMemoryHalf.class,\n+                arr -> arr.array(\"array\", 1024));\n+        static SharedMemoryHalf create(Accelerator accelerator) {\n+            return schema.allocate(accelerator);\n+        }\n+        static SharedMemoryHalf createLocal() {\n+            return schema.allocate(new Accelerator(MethodHandles.lookup(), Backend.FIRST));\n+        }\n+    }\n+\n+    private interface PrivateArrayHalf extends Buffer {\n+        void array(long index, float value);\n+        float array(long index);\n+        Schema<PrivateArrayHalf> schema = Schema.of(PrivateArrayHalf.class,\n+                arr -> arr.array(\"array\", 16));\n+        static PrivateArrayHalf create(Accelerator accelerator) {\n+            return schema.allocate(accelerator);\n+        }\n+        static PrivateArrayHalf createPrivate() {\n+            return schema.allocate(new Accelerator(MethodHandles.lookup(), Backend.FIRST));\n+        }\n+    }\n+\n+    private interface FlatPrivateHalf extends Buffer {\n+        void array(long index, short value);\n+        short array(long index);\n+        Schema<FlatPrivateHalf> schema = Schema.of(FlatPrivateHalf.class,\n+                arr -> arr.array(\"array\", 4));\n+        static FlatPrivateHalf create(Accelerator accelerator) {\n+            return schema.allocate(accelerator);\n+        }\n+        static FlatPrivateHalf createPrivate() {\n+            return schema.allocate(new Accelerator(MethodHandles.lookup(), Backend.FIRST));\n+        }\n+    }\n+\n+    @CodeReflection\n+    @Preformatted(\"\"\"\n+            typedef struct SharedMemoryHalf_s{\n+                half array[1024];\n+            }SharedMemoryHalf_t;\n+\n+            typedef struct PrivateArrayHalf_s{\n+                float array[16];\n+            }PrivateArrayHalf_t;\n+\n+            typedef struct FlatPrivateHalf_s{\n+                half array[4];\n+            }FlatPrivateHalf_t;\n+            \"\"\")\n+    @Kernel(\"\"\"\n+            HAT_KERNEL void matrixMultiplyKernel2DRegisterTilingHalf(\n+                    HAT_GLOBAL_MEM KernelContext_t* kc,\n+                    HAT_GLOBAL_MEM F16Array_t* matrixA,\n+                    HAT_GLOBAL_MEM F16Array_t* matrixB,\n+                    HAT_GLOBAL_MEM F16Array_t* matrixC,\n+                    int size\n+            ){\n+            const int BM = 64;\n+            const int BN = 64;\n+            const int BK = 16;\n+            const int TM = 4;\n+            const int TN = 4;\n+            const int bx = HAT_BIX;\n+            const int by = HAT_BIY;\n+            const int totalResultsBlockTile = BM*BN;\n+            const int numThreadsBlockTile = totalResultsBlockTile\/(TM*TN);\n+            const int linearLocalId = HAT_LIY*HAT_LSX+HAT_LIX;\n+            const int threadCol = HAT_LIX;\n+            const int threadRow = HAT_LIY;\n+            HAT_LOCAL_MEM SharedMemoryHalf_t tileA;\n+            HAT_LOCAL_MEM SharedMemoryHalf_t tileB;\n+            int aFrom = (by*BM)*size;\n+            int bFrom = bx*BN;\n+            const int v = bx*BN;\n+            const int cFrom = (by*BM)*size+v;\n+            const int innerRowA = linearLocalId\/BK;\n+            const int innerColA = linearLocalId%BK;\n+            const int strideA = numThreadsBlockTile\/BK;\n+            const int innerRowB = linearLocalId\/BN;\n+            const int innerColB = linearLocalId%BN;\n+            const int strideB = numThreadsBlockTile\/BN;\n+                PrivateArrayHalf_t threadResults;\n+                FlatPrivateHalf_t regM;\n+                FlatPrivateHalf_t regN;\n+\n+                for(int i = 0; i<TN*TN; i=i+1){\n+                    half init = (half)0.0;\n+                    threadResults.array[(long)i]=(float)init;\n+                }\n+\n+                for(int bkIdx = 0; bkIdx<size; bkIdx=bkIdx+BK){\n+                    for(int loadOffset = 0; loadOffset<BM; loadOffset=loadOffset+strideA){\n+                        tileA.array[(long)((innerRowA+loadOffset)*BK+innerColA)]=(&matrixA->array)[(long)(((innerRowA+loadOffset)*size+innerColA)+aFrom)]->value;\n+                    }\n+                    for(int loadOffset = 0; loadOffset<BK; loadOffset=loadOffset+strideB){\n+                        tileB.array[(long)((innerRowB+loadOffset)*BN+innerColB)]=(&matrixB->array)[(long)(((innerRowB+loadOffset)*size+innerColB)+bFrom)]->value;\n+                    }\n+                    HAT_BARRIER;\n+                    aFrom=aFrom+BK;\n+                    const int f = BK*size;\n+                    bFrom=bFrom+f;\n+                    for(int dotIdx = 0; dotIdx<BK; dotIdx=dotIdx+1){\n+                        for(int i = 0; i<TM; i=i+1){\n+                            regM.array[(long)i]=tileA.array[(long)((threadRow*TM+i)*BK+dotIdx)];\n+                        }\n+                        for(int i = 0; i<TN; i=i+1){\n+                            regN.array[(long)i]=tileB.array[(long)((dotIdx*BN+threadCol*TN)+i)];\n+                        }\n+                        for(int resIdxM = 0; resIdxM<TM; resIdxM=resIdxM+1){\n+                            for(int resIdxN = 0; resIdxN<TN; resIdxN=resIdxN+1){\n+                                half privA = (half)regM.array[(long)resIdxM];\n+                                half privB = (half)regN.array[(long)resIdxN];\n+                                half mul = (privA * privB);\n+                                half acc = (half)threadResults.array[(long)(resIdxM*TN+resIdxN)];\n+                                acc=(acc + mul);\n+                                threadResults.array[(long)(resIdxM*TN+resIdxN)]=(float)acc;\n+                            }\n+                        }\n+                    }\n+                    HAT_BARRIER;\n+                }\n+                for(int resIdxM = 0; resIdxM<TM; resIdxM=resIdxM+1){\n+                    for(int resIdxN = 0; resIdxN<TN; resIdxN=resIdxN+1){\n+                        half result = (half)threadResults.array[(long)(resIdxM*TN+resIdxN)];\n+                        (&matrixC->array)[(long)((((threadRow*TM+resIdxM)*size+threadCol*TN)+resIdxN)+cFrom)]->value=result;\n+                    }\n+                }\n+                return;\n+    }\n+    \"\"\")\n+    public static void matrixMultiplyKernel2DRegisterTilingHalf(@RO KernelContext kc, @RO F16Array matrixA, @RO F16Array matrixB, @RW F16Array matrixC, int size) {\n+\n+        \/\/ Configuration for the kernel: Keep in mind that if you change the following parameters,\n+        \/\/ also change the scheduling (global and local work sizes).\n+        final int BM = 64;\n+        final int BN = 64;\n+        final int BK = 16;\n+        final int TM = 4;\n+        final int TN = 4;\n+\n+        int bx = kc.bix;\n+        int by = kc.biy;\n+\n+        int totalResultsBlockTile = BM * BN;\n+        final int numThreadsBlockTile = totalResultsBlockTile \/ (TM * TN);\n+\n+        final int linearLocalId = kc.liy * kc.lsx + kc.lix;\n+        final int threadCol = kc.lix;\n+        final int threadRow = kc.liy;\n+\n+        SharedMemoryHalf tileA = SharedMemoryHalf.createLocal();\n+        SharedMemoryHalf tileB = SharedMemoryHalf.createLocal();\n+\n+        int aFrom = by * BM * size;\n+        int bFrom = bx * BN;\n+        int v = bx * BN;\n+        int cFrom = (by * BM * size) + (v);\n+\n+        final int innerRowA = linearLocalId \/ BK;\n+        final int innerColA = linearLocalId % BK;\n+\n+        final int strideA = numThreadsBlockTile \/ BK;\n+        final int innerRowB = linearLocalId \/ BN;\n+        final int innerColB = linearLocalId % BN;\n+\n+        int strideB = numThreadsBlockTile \/ BN;\n+\n+        \/\/ Declarations of the arrays in private memory to perform register tiling\n+        PrivateArrayHalf threadResults = PrivateArrayHalf.createPrivate();\n+        FlatPrivateHalf regM = FlatPrivateHalf.createPrivate();\n+        FlatPrivateHalf regN = FlatPrivateHalf.createPrivate();\n+\n+        \/\/ initialize values\n+        for (int i = 0; i < (TN * TN); i++) {\n+            F16 init = F16.of(0.0f);\n+            threadResults.array(i, init.value());\n+        }\n+\n+        \/\/ Each thread loops over the tiles\n+        for (int bkIdx = 0; bkIdx < size; bkIdx += BK) {\n+\n+            \/\/ A) Load data into shared memory for array A\n+            for (int loadOffset = 0; loadOffset < BM; loadOffset += strideA) {\n+                tileA.array((innerRowA + loadOffset) * BK + innerColA, matrixA.array(((innerRowA + loadOffset) * size + innerColA) + aFrom).value());\n+            }\n+\n+            \/\/ B) Load data matrixB into shared memory for array B\n+            for (int loadOffset = 0; loadOffset < BK; loadOffset += strideB) {\n+                tileB.array((innerRowB + loadOffset) * BN + innerColB, matrixB.array(((innerRowB + loadOffset) * size + innerColB) + bFrom).value());\n+            }\n+            kc.barrier();\n+\n+            aFrom += (BK);\n+            int f = BK * size;\n+            bFrom += f;\n+\n+            \/\/ Per-thread, we load the data from the shared memory into register for both\n+            \/\/ array A and array B (matrix A and B), and then perform the reduction within\n+            \/\/ the small region in private memory.\n+            for (int dotIdx = 0; dotIdx < BK; dotIdx++) {\n+                \/\/ block into registers\n+                for (int i = 0; i < TM; i++) {\n+                    regM.array(i,  tileA.array((threadRow * TM + i) * BK + dotIdx));\n+                }\n+                for (int i = 0; i < TN; i++) {\n+                    regN.array(i,  tileB.array(dotIdx * BN + threadCol * TN + i));\n+                }\n+                for (int resIdxM = 0; resIdxM < TM; resIdxM++) {\n+                    for (int resIdxN = 0; resIdxN < TN; resIdxN++) {\n+                        F16 privA = F16.of(regM.array(resIdxM));\n+                        F16 privB = F16.of(regN.array(resIdxN));\n+                        F16 mul = F16.mul(privA, privB);\n+                        F16 acc = F16.of(threadResults.array(resIdxM * TN + resIdxN));\n+                        acc = F16.add(acc, mul);\n+                        threadResults.array((resIdxM * TN + resIdxN), acc.value());\n+                    }\n+                }\n+            }\n+            kc.barrier();\n+        }\n+\n+        \/\/ Finally, we store the results of the reductions for the whole 2D register block into global memory.\n+        \/\/ Essentially, each thread compute a small block of TM * TN sub-block size.\n+        for (int resIdxM = 0; resIdxM < TM; resIdxM++) {\n+            for (int resIdxN = 0; resIdxN < TN; resIdxN++) {\n+                F16 result = F16.of(threadResults.array(resIdxM * TN + resIdxN));\n+                matrixC.array((((threadRow * TM + resIdxM) * size + threadCol * TN + resIdxN) + (cFrom))).value(result.value());\n+            }\n+        }\n+    }\n+\n@@ -561,0 +799,8 @@\n+    @CodeReflection\n+    public static void matrixMultiply2DRegisterTilingHalf(@RO ComputeContext cc, @RO F16Array matrixA, @RO F16Array matrixB, @RW  F16Array matrixC, int globalSize) {\n+        NDRange ndRange = NDRange.of(new NDRange.Global2D(256, 256), new NDRange.Local2D(16, 16));\n+        cc.dispatchKernel(ndRange,\n+                kc -> matrixMultiplyKernel2DRegisterTilingHalf(kc, matrixA, matrixB, matrixC, globalSize)\n+        );\n+    }\n+\n@@ -589,0 +835,14 @@\n+    private static void runSequential(F16Array matrixA, F16Array matrixB, F32Array matrixC, final int size) {\n+        for (int i = 0; i < size; i++) {\n+            for (int j = 0; j < size; j++) {\n+                float sum = 0;\n+                for (int k = 0; k < size; k++) {\n+                    float a = F16.f16ToFloat(matrixA.array((long) i * size + k));\n+                    float b = F16.f16ToFloat(matrixB.array((long) k * size + j));\n+                    sum += a * b;\n+                }\n+                matrixC.array((long) i * size + j, sum);\n+            }\n+        }\n+    }\n+\n@@ -602,0 +862,1 @@\n+        _2DREGISTER_TILING_FP16,\n@@ -624,0 +885,1 @@\n+                case \"2DREGISTERTILING_FP16\" -> Configuration._2DREGISTER_TILING_FP16;\n@@ -641,0 +903,4 @@\n+        F16Array matrixAHalf;\n+        F16Array matrixBHalf;\n+        F16Array matrixCHalf;\n+\n@@ -643,0 +909,4 @@\n+            matrixBHalf = null;\n+            matrixAHalf = null;\n+            matrixCHalf = null;\n+            \/\/matrixC1 = null;\n@@ -657,6 +927,25 @@\n-            matrixA = F32Array.create(accelerator, size * size);\n-            matrixB = F32Array.create(accelerator, size * size);\n-            matrixC = F32Array.create(accelerator, size * size);\n-            for (int j = 0; j < matrixA.length(); j++) {\n-                matrixA.array(j, r.nextFloat());\n-                matrixB.array(j, r.nextFloat());\n+            if (configuration == Configuration._2DREGISTER_TILING_FP16) {\n+                matrixC = null;\n+                matrixB = null;\n+                matrixA = null;\n+                matrixAHalf = F16Array.create(accelerator, size * size);\n+                matrixBHalf = F16Array.create(accelerator, size * size);\n+                \/\/matrixC1 = F32Array.create(accelerator, size * size);\n+                matrixCHalf = F16Array.create(accelerator, size * size);\n+                for (int j = 0; j < matrixAHalf.length(); j++) {\n+                    matrixAHalf.array(j).value(F16.floatToF16(r.nextFloat(1)).value());\n+                    matrixBHalf.array(j).value(F16.floatToF16(r.nextFloat(1)).value());\n+                }\n+            } else {\n+                matrixBHalf = null;\n+                matrixAHalf = null;\n+                matrixCHalf = null;\n+                \/\/matrixC1 = null;\n+               \/\/ matrixCHalf = null;\n+                matrixA = F32Array.create(accelerator, size * size);\n+                matrixB = F32Array.create(accelerator, size * size);\n+                matrixC = F32Array.create(accelerator, size * size);\n+                for (int j = 0; j < matrixA.length(); j++) {\n+                    matrixA.array(j, r.nextFloat());\n+                    matrixB.array(j, r.nextFloat());\n+                }\n@@ -672,0 +961,2 @@\n+        } else if (configuration == Configuration._2DREGISTER_TILING_FP16) {\n+            runSequential(matrixAHalf, matrixAHalf, resultSeq, size);\n@@ -694,0 +985,2 @@\n+                case _2DREGISTER_TILING_FP16 -> accelerator.compute(cc ->\n+                        matrixMultiply2DRegisterTilingHalf(cc, matrixAHalf, matrixBHalf, matrixCHalf, size));\n@@ -707,1 +1000,9 @@\n-                        float gotValue = configuration == Configuration._2DREGISTER_TILING_VECTORIZED? matrixCPad.array(i * size + j) : matrixC.array(i * size + j);\n+                        float gotValue;\n+                        if (configuration == Configuration._2DREGISTER_TILING_VECTORIZED) {\n+                            gotValue = matrixCPad.array(i * size + j);\n+                        } else if (configuration == Configuration._2DREGISTER_TILING_FP16) {\n+                            \/\/gotValue = matrixC1.array(i * size + j);\n+                            gotValue = Float.float16ToFloat(matrixCHalf.array(i * size + j).value());\n+                        } else {\n+                            gotValue = matrixC.array(i * size + j);\n+                        }\n","filename":"hat\/examples\/matmul\/src\/main\/java\/matmul\/Main.java","additions":309,"deletions":8,"binary":false,"changes":317,"status":"modified"},{"patch":"@@ -32,0 +32,2 @@\n+import hat.buffer.Buffer;\n+import hat.buffer.F16;\n@@ -35,0 +37,1 @@\n+import hat.ifacemapper.Schema;\n@@ -42,2 +45,0 @@\n-import static hat.buffer.F16Array.F16;\n-\n@@ -47,0 +48,14 @@\n+\/\/    @Kernel(\"\"\"\n+\/\/            HAT_KERNEL void copy01(\n+\/\/                HAT_GLOBAL_MEM KernelContext_t* kernelContext,\n+\/\/                HAT_GLOBAL_MEM F16Array_t* a,\n+\/\/                HAT_GLOBAL_MEM F16Array_t* b\n+\/\/            ){\n+\/\/                if(HAT_GIX<HAT_GSX){\n+\/\/                    HAT_GLOBAL_MEM F16_t* ha = &a->array[(long)HAT_GIX];\n+\/\/                    HAT_GLOBAL_MEM F16_t* hb = &b->array[(long)HAT_GIX];\n+\/\/                    (&b->array[(long)HAT_GIX])->value=ha->value;\n+\/\/                }\n+\/\/                return;\n+\/\/            }\n+\/\/            \"\"\")\n@@ -49,3 +64,2 @@\n-            F16Array.F16 ha = a.array(kernelContext.gix);\n-            F16Array.F16 hb = b.array(kernelContext.gix);\n-\n+            F16 ha = a.array(kernelContext.gix);\n+            F16 hb = b.array(kernelContext.gix);\n@@ -53,3 +67,2 @@\n-            \/\/b.array(kernelContext.gix).value(ha.value());\n-\n-            hb.value(ha.value());\n+            b.array(kernelContext.gix).value(ha.value());\n+            \/\/hb.value(ha.value());\n@@ -62,2 +75,2 @@\n-            F16Array.F16 ha = a.array(kernelContext.gix);\n-            F16Array.F16 hb = b.array(kernelContext.gix);\n+            F16 ha = a.array(kernelContext.gix);\n+            F16 hb = b.array(kernelContext.gix);\n@@ -65,2 +78,2 @@\n-            F16Array.F16 result = F16.add(ha, hb);\n-            F16Array.F16 hC = c.array(kernelContext.gix);\n+            F16 result = F16.add(ha, hb);\n+            F16 hC = c.array(kernelContext.gix);\n@@ -74,2 +87,2 @@\n-            F16Array.F16 ha = a.array(kernelContext.gix);\n-            F16Array.F16 hb = b.array(kernelContext.gix);\n+            F16 ha = a.array(kernelContext.gix);\n+            F16 hb = b.array(kernelContext.gix);\n@@ -77,2 +90,2 @@\n-            F16Array.F16 result = F16.add(ha, F16.add(hb, hb));\n-            F16Array.F16 hC = c.array(kernelContext.gix);\n+            F16 result = F16.add(ha, F16.add(hb, hb));\n+            F16 hC = c.array(kernelContext.gix);\n@@ -86,9 +99,9 @@\n-            F16Array.F16 ha = a.array(kernelContext.gix);\n-            F16Array.F16 hb = b.array(kernelContext.gix);\n-\n-            F16Array.F16 r1 = F16.mul(ha, hb);\n-            F16Array.F16 r2 = F16.div(ha, hb);\n-            F16Array.F16 r3 = F16.sub(ha, hb);\n-            F16Array.F16 r4 = F16.add(r1, r2);\n-            F16Array.F16 r5 = F16.add(r4, r3);\n-            F16Array.F16 hC = c.array(kernelContext.gix);\n+            F16 ha = a.array(kernelContext.gix);\n+            F16 hb = b.array(kernelContext.gix);\n+\n+            F16 r1 = F16.mul(ha, hb);\n+            F16 r2 = F16.div(ha, hb);\n+            F16 r3 = F16.sub(ha, hb);\n+            F16 r4 = F16.add(r1, r2);\n+            F16 r5 = F16.add(r4, r3);\n+            F16 hC = c.array(kernelContext.gix);\n@@ -102,2 +115,2 @@\n-            F16Array.F16 ha = a.array(kernelContext.gix);\n-            F16Array.F16 initVal = F16.of( 2.1f);\n+            F16 ha = a.array(kernelContext.gix);\n+            F16 initVal = F16.of( 2.1f);\n@@ -111,2 +124,2 @@\n-            F16Array.F16 initVal = F16.of( kernelContext.gix);\n-            F16Array.F16 ha = a.array(kernelContext.gix);\n+            F16 initVal = F16.of( kernelContext.gix);\n+            F16 ha = a.array(kernelContext.gix);\n@@ -117,0 +130,94 @@\n+    @CodeReflection\n+    public static void f16Ops_08(@RO KernelContext kernelContext, @RW F16Array a) {\n+        if (kernelContext.gix < kernelContext.gsx) {\n+            F16 initVal = F16.floatToF16(kernelContext.gix);\n+            F16 ha = a.array(kernelContext.gix);\n+            ha.value(initVal.value());\n+        }\n+    }\n+\n+    @CodeReflection\n+    public static void f16Ops_09(@RO KernelContext kernelContext, @RO F16Array a, @RW F16Array b) {\n+        if (kernelContext.gix < kernelContext.gsx) {\n+            F16 ha = a.array(kernelContext.gix);\n+            float f = F16.f16ToFloat(ha);\n+            F16 result = F16.floatToF16(f);\n+            F16 hb = b.array(kernelContext.gix);\n+            hb.value(result.value());\n+        }\n+    }\n+\n+    @CodeReflection\n+    public static void f16Ops_10(@RO KernelContext kernelContext, @RO F16Array a) {\n+        if (kernelContext.gix < kernelContext.gsx) {\n+            F16 ha = a.array(kernelContext.gix);\n+            F16 f16 = F16.of(1.1f);\n+            float f = F16.f16ToFloat(f16);\n+            F16 result = F16.floatToF16(f);\n+            ha.value(result.value());\n+        }\n+    }\n+\n+    private interface MyLocalArray extends Buffer {\n+        void array(long index, F16 value);\n+        F16 array(long index);\n+        Schema<MyLocalArray> schema = Schema.of(MyLocalArray.class,\n+                        arr -> arr.array(\"array\", 1024));\n+\n+        static MyLocalArray create(Accelerator accelerator) {\n+            return schema.allocate(accelerator);\n+        }\n+        static MyLocalArray createLocal() {\n+            return schema.allocate(new Accelerator(MethodHandles.lookup(), Backend.FIRST));\n+        }\n+    }\n+\n+    @CodeReflection\n+    public static void f16Ops_11(@RO KernelContext kernelContext, @RO F16Array a, @RW F16Array b) {\n+        MyLocalArray sm = MyLocalArray.createLocal();\n+        if (kernelContext.gix < kernelContext.gsx) {\n+            int lix = kernelContext.lix;\n+            F16 ha = a.array(kernelContext.gix);\n+\n+            \/\/ store into local memory\n+            sm.array(lix, ha);\n+\n+            F16 hb = sm.array(lix);\n+            b.array(kernelContext.gix).value(hb.value());\n+        }\n+    }\n+\n+    @CodeReflection\n+    public static void f16Ops_12(@RO KernelContext kernelContext, @RO F16Array a, @RO F16Array b,  @RW F16Array c) {\n+        \/\/ Test the fluent API style\n+        if (kernelContext.gix < kernelContext.gsx) {\n+            F16 ha = a.array(kernelContext.gix);\n+            F16 hb = b.array(kernelContext.gix);\n+            F16 result = ha.add(hb);\n+            c.array(kernelContext.gix).value(result.value());\n+        }\n+    }\n+\n+    @CodeReflection\n+    public static void f16Ops_13(@RO KernelContext kernelContext, @RO F16Array a, @RO F16Array b,  @RW F16Array c) {\n+        \/\/ Test the fluent API style\n+        if (kernelContext.gix < kernelContext.gsx) {\n+            F16 ha = a.array(kernelContext.gix);\n+            F16 hb = b.array(kernelContext.gix);\n+            F16 result = ha.add(hb).sub(hb).mul(ha).div(ha);\n+            c.array(kernelContext.gix).value(result.value());\n+        }\n+    }\n+\n+    @CodeReflection\n+    public static void f16Ops_14(@RO KernelContext kernelContext, @RO F16Array a, @RW F16Array b) {\n+        \/\/ Testing mixed float types\n+        if (kernelContext.gix < kernelContext.gsx) {\n+            F16 ha = a.array(kernelContext.gix);\n+            float myFloat = 32.1f;\n+            F16 result = F16.add(myFloat, ha);\n+            b.array(kernelContext.gix).value(result.value());\n+        }\n+    }\n+\n+\n@@ -153,0 +260,42 @@\n+    @CodeReflection\n+    public static void compute08(@RO ComputeContext computeContext, @RW F16Array a) {\n+        NDRange ndRange = NDRange.of(new NDRange.Global1D(a.length()));\n+        computeContext.dispatchKernel(ndRange, kernelContext -> TestF16Type.f16Ops_08(kernelContext, a));\n+    }\n+\n+    @CodeReflection\n+    public static void compute09(@RO ComputeContext computeContext, @RO F16Array a, @RW F16Array b) {\n+        NDRange ndRange = NDRange.of(new NDRange.Global1D(a.length()));\n+        computeContext.dispatchKernel(ndRange, kernelContext -> TestF16Type.f16Ops_09(kernelContext, a, b));\n+    }\n+\n+    @CodeReflection\n+    public static void compute10(@RO ComputeContext computeContext, @RW F16Array a) {\n+        NDRange ndRange = NDRange.of(new NDRange.Global1D(a.length()));\n+        computeContext.dispatchKernel(ndRange, kernelContext -> TestF16Type.f16Ops_10(kernelContext, a));\n+    }\n+\n+    @CodeReflection\n+    public static void compute11(@RO ComputeContext computeContext, @RO F16Array a, @RW F16Array b) {\n+        NDRange ndRange = NDRange.of(new NDRange.Global1D(a.length()), new NDRange.Local1D(16));\n+        computeContext.dispatchKernel(ndRange, kernelContext -> TestF16Type.f16Ops_11(kernelContext, a, b));\n+    }\n+\n+    @CodeReflection\n+    public static void compute12(@RO ComputeContext computeContext, @RO F16Array a, @RO F16Array b, @RW F16Array c) {\n+        NDRange ndRange = NDRange.of(new NDRange.Global1D(a.length()));\n+        computeContext.dispatchKernel(ndRange, kernelContext -> TestF16Type.f16Ops_12(kernelContext, a, b, c));\n+    }\n+\n+    @CodeReflection\n+    public static void compute13(@RO ComputeContext computeContext, @RO F16Array a, @RO F16Array b, @RW F16Array c) {\n+        NDRange ndRange = NDRange.of(new NDRange.Global1D(a.length()));\n+        computeContext.dispatchKernel(ndRange, kernelContext -> TestF16Type.f16Ops_13(kernelContext, a, b, c));\n+    }\n+\n+    @CodeReflection\n+    public static void compute14(@RO ComputeContext computeContext, @RO F16Array a, @RW F16Array b) {\n+        NDRange ndRange = NDRange.of(new NDRange.Global1D(a.length()));\n+        computeContext.dispatchKernel(ndRange, kernelContext -> TestF16Type.f16Ops_14(kernelContext, a, b));\n+    }\n+\n@@ -162,1 +311,1 @@\n-            arrayA.array(i).value(F16.floatToF16(i));\n+            arrayA.array(i).value(F16.floatToF16(i).value());\n@@ -168,1 +317,1 @@\n-            short val = arrayB.array(i).value();\n+            F16 val = arrayB.array(i);\n@@ -184,2 +333,2 @@\n-            arrayA.array(i).value(F16.floatToF16(random.nextFloat()));\n-            arrayB.array(i).value(F16.floatToF16(random.nextFloat()));\n+            arrayA.array(i).value(F16.floatToF16(random.nextFloat()).value());\n+            arrayB.array(i).value(F16.floatToF16(random.nextFloat()).value());\n@@ -193,1 +342,1 @@\n-            short val = arrayC.array(i).value();\n+            F16 val = arrayC.array(i);\n@@ -211,2 +360,2 @@\n-            arrayA.array(i).value(F16.floatToF16(random.nextFloat()));\n-            arrayB.array(i).value(F16.floatToF16(random.nextFloat()));\n+            arrayA.array(i).value(F16.floatToF16(random.nextFloat()).value());\n+            arrayB.array(i).value(F16.floatToF16(random.nextFloat()).value());\n@@ -220,1 +369,1 @@\n-            short val = arrayC.array(i).value();\n+            F16 val = arrayC.array(i);\n@@ -238,2 +387,2 @@\n-            arrayA.array(i).value(F16.floatToF16(random.nextFloat()));\n-            arrayB.array(i).value(F16.floatToF16(random.nextFloat()));\n+            arrayA.array(i).value(F16.floatToF16(random.nextFloat()).value());\n+            arrayB.array(i).value(F16.floatToF16(random.nextFloat()).value());\n@@ -250,7 +399,7 @@\n-            F16Array.F16 ha = arrayA.array(i);\n-            F16Array.F16 hb = arrayB.array(i);\n-            F16Array.F16 r1 = F16.mul(ha, hb);\n-            F16Array.F16 r2 = F16.div(ha, hb);\n-            F16Array.F16 r3 = F16.sub(ha, hb);\n-            F16Array.F16 r4 = F16.add(r1, r2);\n-            F16Array.F16 r5 = F16.add(r4, r3);\n+            F16 ha = arrayA.array(i);\n+            F16 hb = arrayB.array(i);\n+            F16 r1 = F16.mul(ha, hb);\n+            F16 r2 = F16.div(ha, hb);\n+            F16 r3 = F16.sub(ha, hb);\n+            F16 r4 = F16.add(r1, r2);\n+            F16 r5 = F16.add(r4, r3);\n@@ -269,1 +418,1 @@\n-            arrayA.array(i).value(F16.floatToF16(0.0f));\n+            arrayA.array(i).value(F16.floatToF16(0.0f).value());\n@@ -289,1 +438,1 @@\n-            arrayA.array(i).value(F16.floatToF16(0.0f));\n+            arrayA.array(i).value(F16.floatToF16(0.0f).value());\n@@ -320,0 +469,140 @@\n+    @HatTest\n+    public void testF16_08() {\n+        var accelerator = new Accelerator(MethodHandles.lookup(), Backend.FIRST);\n+\n+        final int size = 256;\n+        F16Array arrayA = F16Array.create(accelerator, size);\n+        for (int i = 0; i < arrayA.length(); i++) {\n+            arrayA.array(i).value(F16.floatToF16(0.0f).value());\n+        }\n+\n+        accelerator.compute(computeContext -> {\n+            TestF16Type.compute08(computeContext, arrayA);\n+        });\n+\n+        for (int i = 0; i < arrayA.length(); i++) {\n+            short val = arrayA.array(i).value();\n+            HatAsserts.assertEquals(i, Float.float16ToFloat(val), 0.01f);\n+        }\n+    }\n+\n+    @HatTest\n+    public void testF16_09() {\n+        var accelerator = new Accelerator(MethodHandles.lookup(), Backend.FIRST);\n+\n+        final int size = 16;\n+        F16Array arrayA = F16Array.create(accelerator, size);\n+        F16Array arrayB = F16Array.create(accelerator, size);\n+\n+        Random r = new Random(73);\n+        for (int i = 0; i < arrayA.length(); i++) {\n+            arrayA.array(i).value(F16.floatToF16(r.nextFloat()).value());\n+        }\n+\n+        accelerator.compute(computeContext -> TestF16Type.compute09(computeContext, arrayA, arrayB));\n+\n+        for (int i = 0; i < arrayB.length(); i++) {\n+            F16 val = arrayB.array(i);\n+            HatAsserts.assertEquals(arrayA.array(i).value(), val.value());\n+        }\n+    }\n+\n+    @HatTest\n+    public void testF16_10() {\n+        var accelerator = new Accelerator(MethodHandles.lookup(), Backend.FIRST);\n+        final int size = 256;\n+        F16Array arrayA = F16Array.create(accelerator, size);\n+\n+        accelerator.compute(computeContext -> TestF16Type.compute10(computeContext, arrayA));\n+\n+        for (int i = 0; i < arrayA.length(); i++) {\n+            F16 val = arrayA.array(i);\n+            HatAsserts.assertEquals(1.1f, F16.f16ToFloat(val), 0.01f);\n+        }\n+    }\n+\n+    @HatTest\n+    public void testF16_11() {\n+        var accelerator = new Accelerator(MethodHandles.lookup(), Backend.FIRST);\n+        final int size = 256;\n+        F16Array arrayA = F16Array.create(accelerator, size);\n+        F16Array arrayB = F16Array.create(accelerator, size);\n+\n+        Random r = new Random(73);\n+        for (int i = 0; i < arrayA.length(); i++) {\n+            arrayA.array(i).value(F16.floatToF16(r.nextFloat()).value());\n+        }\n+\n+        accelerator.compute(computeContext -> TestF16Type.compute11(computeContext, arrayA, arrayB));\n+\n+        for (int i = 0; i < arrayB.length(); i++) {\n+            F16 val = arrayB.array(i);\n+            HatAsserts.assertEquals(arrayA.array(i).value(), val.value());\n+        }\n+    }\n+\n+    @HatTest\n+    public void testF16_12() {\n+        var accelerator = new Accelerator(MethodHandles.lookup(), Backend.FIRST);\n+        final int size = 1024;\n+        F16Array arrayA = F16Array.create(accelerator, size);\n+        F16Array arrayB = F16Array.create(accelerator, size);\n+        F16Array arrayC = F16Array.create(accelerator, size);\n+\n+        Random r = new Random(73);\n+        for (int i = 0; i < arrayA.length(); i++) {\n+            arrayA.array(i).value(F16.floatToF16(r.nextFloat()).value());\n+            arrayB.array(i).value(F16.floatToF16(r.nextFloat()).value());\n+        }\n+\n+        accelerator.compute(computeContext -> TestF16Type.compute12(computeContext, arrayA, arrayB, arrayC));\n+\n+        for (int i = 0; i < arrayB.length(); i++) {\n+            F16 result = arrayC.array(i);\n+            HatAsserts.assertEquals(F16.f16ToFloat(F16.add(arrayA.array(i), arrayB.array(i))), F16.f16ToFloat(result), 0.01f);\n+        }\n+    }\n+\n+    @HatTest\n+    public void testF16_13() {\n+        var accelerator = new Accelerator(MethodHandles.lookup(), Backend.FIRST);\n+        final int size = 1024;\n+        F16Array arrayA = F16Array.create(accelerator, size);\n+        F16Array arrayB = F16Array.create(accelerator, size);\n+        F16Array arrayC = F16Array.create(accelerator, size);\n+\n+        Random r = new Random(73);\n+        for (int i = 0; i < arrayA.length(); i++) {\n+            arrayA.array(i).value(F16.floatToF16(r.nextFloat()).value());\n+            arrayB.array(i).value(F16.floatToF16(r.nextFloat()).value());\n+        }\n+\n+        accelerator.compute(computeContext -> TestF16Type.compute13(computeContext, arrayA, arrayB, arrayC));\n+\n+        for (int i = 0; i < arrayB.length(); i++) {\n+            F16 result = arrayC.array(i);\n+            HatAsserts.assertEquals(F16.f16ToFloat(arrayA.array(i)), F16.f16ToFloat(result), 0.01f);\n+        }\n+    }\n+\n+    @HatTest\n+    public void testF16_14() {\n+        \/\/ Testing mixed types\n+        var accelerator = new Accelerator(MethodHandles.lookup(), Backend.FIRST);\n+        final int size = 1024;\n+        F16Array arrayA = F16Array.create(accelerator, size);\n+        F16Array arrayB = F16Array.create(accelerator, size);\n+\n+        Random r = new Random(73);\n+        for (int i = 0; i < arrayA.length(); i++) {\n+            arrayA.array(i).value(F16.floatToF16(r.nextFloat()).value());\n+        }\n+\n+        accelerator.compute(computeContext -> TestF16Type.compute14(computeContext, arrayA, arrayB));\n+\n+        for (int i = 0; i < arrayB.length(); i++) {\n+            F16 result = arrayB.array(i);\n+            HatAsserts.assertEquals(F16.f16ToFloat(arrayA.array(i)) + 32.1f, F16.f16ToFloat(result), 0.1f);\n+        }\n+    }\n+\n","filename":"hat\/tests\/src\/main\/java\/hat\/test\/TestF16Type.java","additions":337,"deletions":48,"binary":false,"changes":385,"status":"modified"},{"patch":"@@ -33,0 +33,1 @@\n+import hat.buffer.F16;\n@@ -45,1 +46,0 @@\n-import static hat.buffer.F16Array.F16;\n@@ -438,2 +438,2 @@\n-            matrixA.array(j).value(F16.floatToF16(r.nextFloat()));\n-            matrixB.array(j).value(F16.floatToF16(r.nextFloat()));\n+            matrixA.array(j).value(F16.floatToF16(r.nextFloat()).value());\n+            matrixB.array(j).value(F16.floatToF16(r.nextFloat()).value());\n","filename":"hat\/tests\/src\/main\/java\/hat\/test\/TestMatMul.java","additions":3,"deletions":3,"binary":false,"changes":6,"status":"modified"},{"patch":"@@ -31,0 +31,1 @@\n+import hat.dialect.HATF16ToFloatConvOp;\n@@ -221,0 +222,6 @@\n+    @Override\n+    public T hatF16ToFloatConvOp(ScopedCodeBuilderContext builderContext, HATF16ToFloatConvOp hatF16ToFloatConvOp) {\n+        blockComment(\"Float Conv Op Not Implemented\");\n+        return self();\n+    }\n+\n","filename":"hat\/tools\/src\/main\/java\/hat\/tools\/text\/JavaHATCodeBuilder.java","additions":7,"deletions":0,"binary":false,"changes":7,"status":"modified"}]}