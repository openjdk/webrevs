{"files":[{"patch":"@@ -1,1 +1,1 @@\n-### MavenStyleProject using code reflection with a Java-based ONNX programming model.\n+## MavenStyleProject using code reflection with a Java-based ONNX programming model.\n@@ -3,6 +3,1 @@\n-Running the demo:\n-```\n-JAVA_HOME=<path to the Babylon JDK home>;mvn process-test-classes exec:java -Dexec.classpathScope=test -Dexec.mainClass=oracle.code.onnx.MNISTDemo\n-```\n-\n-### Onnx Generation API to create and run LLM Onnx models.\n+### ONNX Runtime running convolution neural network from Java source\n@@ -10,1 +5,1 @@\n-Example of direct execution of existing Onnx LLM model:\n+Running the MNIST demo:\n@@ -12,18 +7,1 @@\n-\/\/ model-specific prompt format\n-static final String PROMPT_TEMPLATE = \"<|...|>%s<|...|><|...|>\";\n-\n-public static void main(String... args) {\n-\n-    \/\/ compatible `libonnxruntime` library must be present in the same folder as `libonnxruntime-genai` library\n-    \/\/ native library extension (.dylib, .so or .dll) is platform specific\n-    System.load(\"path\/To\/libonnxruntime-genai.dylib\");\n-\n-    \/\/ model folder must contain the Onnx model file and all configuration and external data files\n-    try (OnnxGenRuntimeSession session = new OnnxGenRuntimeSession(Path.of(\"path\/To\/Onnx\/Model\/Folder\/\")) {\n-        \/\/ each LLM model has specific prompt format\n-        session.prompt(PROMPT_TEMPLATE.formatted(\"Tell me a joke\"), System.out::print);\n-    }\n-}\n-```\n-\n-Example of a custom LLM Onnx model generation from Java sources and execution:\n+mvn process-test-classes exec:exec -Dexec.executable=<path to the Babylon JDK home>\/bin\/java -Dexec.mainClass=oracle.code.onnx.mnist.MNISTDemo\n@@ -31,4 +9,0 @@\n-\/\/ model-specific prompt format\n-static final String PROMPT_TEMPLATE = \"<|...|>%s<|...|><|...|>\";\n-\n-public static void main(String... args) {\n@@ -36,3 +10,1 @@\n-    \/\/ compatible `libonnxruntime` library must be present in the same folder as `libonnxruntime-genai` library\n-    \/\/ native library extension (.dylib or .so or .dll) is platform specific\n-    System.load(\"path\/To\/libonnxruntime-genai.dylib\");\n+### ONNX GenAI running large language model from Java source.\n@@ -40,2 +12,3 @@\n-    \/\/ instance of a custom Onnx LLM model\n-    MyCustomLLMModel myCustomModelInstance = ...;\n+Setup:\n+ - Download [onnxruntime-genai](https:\/\/github.com\/microsoft\/onnxruntime-genai\/releases) native library coresponding to your system\/architecture, unzip and put it into `cr-examples\/onnx\/lib` folder.\n+ - Download `model.onnx.data`, `tokenizer.json` and `tokenizer_config.json` data files from [Llama-3.2-1B-Instruct-ONNX](https:\/\/huggingface.co\/onnx-community\/Llama-3.2-1B-Instruct-ONNX\/tree\/main\/cpu_and_mobile\/cpu-int4-rtn-block-32-acc-level-4) and put them into `cr-examples\/onnx\/src\/test\/resources\/oracle\/code\/onnx\/llm` folder.\n@@ -43,14 +16,1 @@\n-    \/\/ target model folder must contain all configuration files\n-    \/\/ `genai_config.json` must be configured following way:\n-    \/\/     - model filename to match generated model file name (below)\n-    \/\/     - model inputs to match main model method argument names\n-    \/\/     - model outputs to match main model result record component names\n-    Path targetModelFolder = ...;\n-\n-    \/\/ Onnx model file and external data file are generated to the target model folder\n-    \/\/ and the session is created from the generated model\n-    try (OnnxGenRuntimeSession session = OnnxGenRuntimeSession.buildFromCodeReflection(myCustomModelInstance, \"myMainModelMethod\", targetModelFolder, \"MyModelFileName.onnx\", \"MyDataFileName\")) {\n-        \/\/ each LLM model has specific prompt format\n-        session.prompt(PROMPT_TEMPLATE.formatted(\"Tell me a joke\"), System.out::print);\n-    }\n-}\n+Running the Llama demo:\n@@ -58,2 +18,1 @@\n-\n-Example of a custom LLM Onnx model Java source:\n+mvn process-test-classes exec:exec -Dexec.executable=<path to the Babylon JDK home>\/bin\/java -Dexec.mainClass=oracle.code.onnx.llm.LlamaDemo\n@@ -61,16 +20,0 @@\n-import oracle.code.onnx.Tensor;\n-import jdk.incubator.code.CodeReflection;\n-import static oracle.code.onnx.OnnxOperators.*;\n-\n-public final class MyCustomLLMModel {\n-\n-     public final Tensor<Float> myModelWeights...\n-     public final Tensor<Byte> otherMyModelWeights...\n-\n-     public MyCustomLLMModel(...) {\n-         \/\/ initilize all weight tensors\n-         \/\/ large tensors data can be memory-mapped\n-         this.myModelWeights = ...\n-         this.otherMyModelWeights = ...\n-         ...\n-     }\n@@ -78,16 +21,0 @@\n-     \/\/ custom record with main model method response\n-     public record MyModelResponse(Tensor<Float> logits, Tensor<Float> presentKey0, Tensor<Float> presentValue0, ...) {\n-     }\n-\n-     @CodeReflection\n-     public MyModelResponse myMainModelMethod(Tensor<Long> inputIds, Tensor<Long> attentionMask, Tensor<Float> pastKey0, Tensor<Float> pastValue0, ...) {\n-\n-         \/\/ computation of the model using oracle.code.onnx.OnnxOperators.* method calls\n-         ...\n-         Tensor<Float> logits = MatMul(...\n-\n-         \/\/ composition of the return record\n-         return new MyModelResponse(logits, key0, value0, ...);\n-     }\n-}\n-```\n","filename":"cr-examples\/onnx\/README.md","additions":10,"deletions":83,"binary":false,"changes":93,"status":"modified"},{"patch":"@@ -0,0 +1,2 @@\n+\/libonnxruntime.*\n+\/libonnxruntime-genai.*\n","filename":"cr-examples\/onnx\/lib\/.gitignore","additions":2,"deletions":0,"binary":false,"changes":2,"status":"added"},{"patch":"@@ -99,1 +99,2 @@\n-                    <commandlineArgs>--add-modules jdk.incubator.code ${exec.args}<\/commandlineArgs>\n+                    <classpathScope>test<\/classpathScope>\n+                    <commandlineArgs>--add-modules jdk.incubator.code -classpath %classpath ${exec.mainClass}<\/commandlineArgs>\n","filename":"cr-examples\/onnx\/pom.xml","additions":2,"deletions":1,"binary":false,"changes":3,"status":"modified"},{"patch":"@@ -34,1 +34,0 @@\n-import java.lang.reflect.Constructor;\n@@ -36,1 +35,0 @@\n-import java.lang.reflect.InvocationTargetException;\n@@ -38,1 +36,0 @@\n-import java.lang.reflect.RecordComponent;\n@@ -58,1 +55,0 @@\n-import oracle.code.onnx.proto.OnnxModel;\n","filename":"cr-examples\/onnx\/src\/main\/java\/oracle\/code\/onnx\/OnnxRuntime.java","additions":0,"deletions":4,"binary":false,"changes":4,"status":"modified"},{"patch":"@@ -35,0 +35,1 @@\n+import java.util.Locale;\n@@ -137,0 +138,31 @@\n+    \/**\n+     * Loads {@code onnxruntime-genai} native library from the given folder.\n+     * This method unpacks required {@code onnxruntime} library from dependencies, if missing.\n+     *\n+     * @param libRoot folder with the library\n+     *\/\n+    public static void loadGenAILib(Path libRoot) {\n+        Path runtime = libRoot.resolve(System.mapLibraryName(\"onnxruntime\"));\n+        if (!Files.isRegularFile(runtime)) {\n+            \/\/ onnxruntime-genai requires onnxruntime in the same directory\n+            String arch = System.getProperty(\"os.arch\", \"generic\").toLowerCase(Locale.ENGLISH).startsWith(\"aarch64\") ? \"aarch64\" : \"x64\";\n+            String os = System.getProperty(\"os.name\", \"generic\").toLowerCase(Locale.ENGLISH);\n+            String libResource;\n+            if (os.contains(\"mac\") || os.contains(\"darwin\")) {\n+                libResource = \"\/ai\/onnxruntime\/native\/osx-\" + arch + \"\/libonnxruntime.dylib\";\n+            } else if (os.contains(\"win\")) {\n+                libResource = \"\/ai\/onnxruntime\/native\/win-\" + arch + \"\/libonnxruntime.dll\";\n+            } else if (os.contains(\"nux\")) {\n+                libResource = \"\/ai\/onnxruntime\/native\/linux-\" + arch + \"\/libonnxruntime.so\";\n+            } else {\n+                throw new IllegalStateException(\"Unsupported os:\" + os);\n+            }\n+            try (var libStream = OnnxRuntime.class.getResourceAsStream(libResource)) {\n+                Files.copy(libStream, runtime);\n+            } catch (IOException ioe) {\n+                throw new RuntimeException(ioe);\n+            }\n+        }\n+        System.load(libRoot.resolve(System.mapLibraryName(\"onnxruntime-genai\")).toAbsolutePath().toString());\n+    }\n+\n","filename":"cr-examples\/onnx\/src\/main\/java\/oracle\/code\/onnx\/genai\/OnnxGenRuntimeSession.java","additions":32,"deletions":0,"binary":false,"changes":32,"status":"modified"},{"patch":"@@ -0,0 +1,46 @@\n+\/*\n+ * Copyright (c) 2025, Oracle and\/or its affiliates. All rights reserved.\n+ * DO NOT ALTER OR REMOVE COPYRIGHT NOTICES OR THIS FILE HEADER.\n+ *\n+ * This code is free software; you can redistribute it and\/or modify it\n+ * under the terms of the GNU General Public License version 2 only, as\n+ * published by the Free Software Foundation.\n+ *\n+ * This code is distributed in the hope that it will be useful, but WITHOUT\n+ * ANY WARRANTY; without even the implied warranty of MERCHANTABILITY or\n+ * FITNESS FOR A PARTICULAR PURPOSE.  See the GNU General Public License\n+ * version 2 for more details (a copy is included in the LICENSE file that\n+ * accompanied this code).\n+ *\n+ * You should have received a copy of the GNU General Public License version\n+ * 2 along with this work; if not, write to the Free Software Foundation,\n+ * Inc., 51 Franklin St, Fifth Floor, Boston, MA 02110-1301 USA.\n+ *\n+ * Please contact Oracle, 500 Oracle Parkway, Redwood Shores, CA 94065 USA\n+ * or visit www.oracle.com if you need additional information or have any\n+ * questions.\n+ *\/\n+package oracle.code.onnx.llm;\n+\n+import java.lang.foreign.Arena;\n+import java.nio.file.Path;\n+import oracle.code.onnx.genai.OnnxGenRuntimeSession;\n+\n+public class LlamaDemo {\n+\n+    public static void main(String... args) throws Exception {\n+\n+        OnnxGenRuntimeSession.loadGenAILib(Path.of(\"lib\"));\n+\n+        Path modelRoot = Path.of(LlamaDemo.class.getResource(\"LlamaDemo.class\").toURI()).getParent();\n+        try (Arena arena = Arena.ofConfined()) {\n+            var modelInstance = new LlamaModel(arena);\n+            try (OnnxGenRuntimeSession session = OnnxGenRuntimeSession.buildFromCodeReflection(modelInstance, \"forward\", modelRoot, \"model.onnx\", \"model.data\")) {\n+                session.prompt(\"\"\"\n+                        <|start_header_id|>user<|end_header_id|>Hello, tell me a joke.<|eot_id|>\n+                        <|start_header_id|>assistant<|end_header_id|>\n+                        \"\"\", System.out::print);\n+            }\n+        }\n+    }\n+}\n","filename":"cr-examples\/onnx\/src\/test\/java\/oracle\/code\/onnx\/llm\/LlamaDemo.java","additions":46,"deletions":0,"binary":false,"changes":46,"status":"added"},{"patch":"@@ -0,0 +1,159 @@\n+\/*\n+ * Copyright (c) 2025, Oracle and\/or its affiliates. All rights reserved.\n+ * DO NOT ALTER OR REMOVE COPYRIGHT NOTICES OR THIS FILE HEADER.\n+ *\n+ * This code is free software; you can redistribute it and\/or modify it\n+ * under the terms of the GNU General Public License version 2 only, as\n+ * published by the Free Software Foundation.\n+ *\n+ * This code is distributed in the hope that it will be useful, but WITHOUT\n+ * ANY WARRANTY; without even the implied warranty of MERCHANTABILITY or\n+ * FITNESS FOR A PARTICULAR PURPOSE.  See the GNU General Public License\n+ * version 2 for more details (a copy is included in the LICENSE file that\n+ * accompanied this code).\n+ *\n+ * You should have received a copy of the GNU General Public License version\n+ * 2 along with this work; if not, write to the Free Software Foundation,\n+ * Inc., 51 Franklin St, Fifth Floor, Boston, MA 02110-1301 USA.\n+ *\n+ * Please contact Oracle, 500 Oracle Parkway, Redwood Shores, CA 94065 USA\n+ * or visit www.oracle.com if you need additional information or have any\n+ * questions.\n+ *\/\n+package oracle.code.onnx.llm;\n+\n+import java.io.IOException;\n+import java.lang.foreign.Arena;\n+import jdk.incubator.code.CodeReflection;\n+import oracle.code.onnx.Tensor;\n+import oracle.code.onnx.genai.TensorDataStream;\n+\n+import static java.util.Optional.*;\n+import static oracle.code.onnx.OnnxOperators.*;\n+import static oracle.code.onnx.Tensor.ElementType.*;\n+import oracle.code.onnx.ir.OnnxType;\n+\n+public final class LlamaModel {\n+\n+    public static final int LAYERS = 16;\n+    public static final long BITS = 4,\n+                             BLOCK_SIZE = 32,\n+                             NUM_KEY_VALUE_HEADS = 8,\n+                             ACCURACY_LEVEL = 4,\n+                             VOCAB_SIZE = 128256,\n+                             HEAD_SIZE = 64,\n+                             HIDEN_SIZE = 2048,\n+                             CONTEXT_SIZE = 131072,\n+                             INTERMEDIATE_SIZE = 8192,\n+                             ATTN_WEIGHTS_SIZE = 3072;\n+    public static final float EPSILON = 1.0E-5f,\n+                              SCALE = 0.125f;\n+\n+    public final Tensor<Long> flat1, scalar1;\n+    public final Tensor<Float> tokensWeights, initWeight, cosCache, sinCache, headScales;\n+    public final Tensor<Float>[] postAttentionWeights = new Tensor[LAYERS],\n+                                 inputWeights = new Tensor[LAYERS],\n+                                 attnQkvScales = new Tensor[LAYERS],\n+                                 attnOScales = new Tensor[LAYERS],\n+                                 mlpGateScales = new Tensor[LAYERS],\n+                                 mlpUpScales = new Tensor[LAYERS],\n+                                 mlpDownScales = new Tensor[LAYERS];\n+    public final Tensor<Byte>[] attnQkvWeight = new Tensor[LAYERS],\n+                                attnOWeight = new Tensor[LAYERS],\n+                                mlpGateWeight = new Tensor[LAYERS],\n+                                mlpUpWeight = new Tensor[LAYERS],\n+                                mlpDownWeight = new Tensor[LAYERS];\n+    public final Tensor<Byte> headWeight;\n+\n+    public LlamaModel(Arena arena) throws IOException {\n+        flat1 = Tensor.ofFlat(arena, 1l);\n+        scalar1 = Tensor.ofScalar(arena, 1l);\n+        var modelData = new TensorDataStream(arena, LlamaModel.class.getResource(\"model.onnx.data\").getPath());\n+        tokensWeights = modelData.nextTensor(FLOAT, VOCAB_SIZE, HIDEN_SIZE);\n+        initWeight = modelData.nextTensor(FLOAT, HIDEN_SIZE);\n+        cosCache = modelData.nextTensor(FLOAT, CONTEXT_SIZE, HEAD_SIZE \/ 2);\n+        sinCache = modelData.nextTensor(FLOAT, CONTEXT_SIZE, HEAD_SIZE \/ 2);\n+        for (int i = 0; i < LAYERS; i++) {\n+            postAttentionWeights[i] = modelData.nextTensor(FLOAT, HIDEN_SIZE);\n+            inputWeights[i] = modelData.nextTensor(FLOAT, HIDEN_SIZE);\n+        }\n+        for (int i = 0; i < LAYERS; i++) {\n+            attnQkvWeight[i] = modelData.nextTensor(UINT8, ATTN_WEIGHTS_SIZE, HEAD_SIZE, 16);\n+            attnQkvScales[i] = modelData.nextTensor(FLOAT, ATTN_WEIGHTS_SIZE * HEAD_SIZE);\n+            attnOWeight[i] = modelData.nextTensor(UINT8, HIDEN_SIZE, HEAD_SIZE, 16);\n+            attnOScales[i] = modelData.nextTensor(FLOAT, HIDEN_SIZE * HEAD_SIZE);\n+            mlpGateWeight[i] = modelData.nextTensor(UINT8, INTERMEDIATE_SIZE, HEAD_SIZE, 16);\n+            mlpGateScales[i] = modelData.nextTensor(FLOAT, INTERMEDIATE_SIZE * HEAD_SIZE);\n+            mlpUpWeight[i] = modelData.nextTensor(UINT8, INTERMEDIATE_SIZE, HEAD_SIZE, 16);\n+            mlpUpScales[i] = modelData.nextTensor(FLOAT, INTERMEDIATE_SIZE * HEAD_SIZE);\n+            mlpDownWeight[i] = modelData.nextTensor(UINT8, HIDEN_SIZE, 256, 16);\n+            mlpDownScales[i] = modelData.nextTensor(FLOAT, INTERMEDIATE_SIZE * HEAD_SIZE);\n+        }\n+        headWeight = modelData.nextTensor(UINT8, VOCAB_SIZE, HEAD_SIZE, 16);\n+        headScales = modelData.nextTensor(FLOAT, VOCAB_SIZE * HEAD_SIZE);\n+    }\n+\n+    public record ForwardResponse(Tensor<Float> logits,\n+                                  Tensor<Float>[] presentKey,\n+                                  Tensor<Float>[] presentValue) {\n+    }\n+\n+    @CodeReflection\n+    public ForwardResponse forward(Tensor<Long> inputIds, Tensor<Long> attentionMask, Tensor<Float>[] pastKey, Tensor<Float>[] pastValue) {\n+\n+        Tensor<Integer> amSL = Cast(Sub(ReduceSum(attentionMask, of(flat1), empty(), empty()), flat1), empty(), OnnxType.INT32.id());\n+        Tensor<Integer> amTSL = Cast(Gather(Shape(attentionMask, empty(), empty()), scalar1, of(0l)), empty(), OnnxType.INT32.id());\n+        Tensor<Float> skipBias = Gather(tokensWeights, inputIds, empty());\n+        Tensor<Float> input = LayerNormalization(skipBias, initWeight, empty(), of(EPSILON), of(1l), of(-1l)).Y();\n+\n+        Tensor<Float>[] presentKeys = new Tensor[LAYERS];\n+        Tensor<Float>[] presentValues = new Tensor[LAYERS];\n+\n+        for (int i = 0; i < LAYERS; i++) {\n+            GroupQueryAttention<Float> attn = GroupQueryAttention(\n+                    MatMulNBits(input,\n+                                attnQkvWeight[i],\n+                                attnQkvScales[i], empty(), empty(), empty(), HIDEN_SIZE, ATTN_WEIGHTS_SIZE, of(ACCURACY_LEVEL), BITS, BLOCK_SIZE),\n+                    empty(),\n+                    empty(),\n+                    of(pastKey[i]),\n+                    of(pastValue[i]),\n+                    amSL,\n+                    amTSL,\n+                    of(cosCache),\n+                    of(sinCache), of(1l), NUM_KEY_VALUE_HEADS, empty(), BLOCK_SIZE, of(0l), of(SCALE));\n+\n+            SkipSimplifiedLayerNormalization<Float> postAttnLayernorm = SkipSimplifiedLayerNormalization(\n+                    skipBias,\n+                    MatMulNBits(attn.output(),\n+                                attnOWeight[i],\n+                                attnOScales[i], empty(), empty(), empty(), HIDEN_SIZE, HIDEN_SIZE, of(ACCURACY_LEVEL), BITS, BLOCK_SIZE),\n+                    postAttentionWeights[i], empty(), of(EPSILON));\n+\n+            Tensor<Float> mlpGateProj = MatMulNBits(postAttnLayernorm.output(),\n+                                                    mlpGateWeight[i],\n+                                                    mlpGateScales[i], empty(), empty(), empty(), HIDEN_SIZE, INTERMEDIATE_SIZE, of(ACCURACY_LEVEL), BITS, BLOCK_SIZE);\n+\n+            SkipSimplifiedLayerNormalization<Float> norm = SkipSimplifiedLayerNormalization(postAttnLayernorm.input_skip_bias_sum(),\n+                    MatMulNBits(Mul(Mul(mlpGateProj,\n+                                        Sigmoid(mlpGateProj)),\n+                                    MatMulNBits(postAttnLayernorm.output(),\n+                                                mlpUpWeight[i],\n+                                                mlpUpScales[i], empty(), empty(), empty(), HIDEN_SIZE, INTERMEDIATE_SIZE, of(ACCURACY_LEVEL), BITS, BLOCK_SIZE)),\n+                                mlpDownWeight[i],\n+                                mlpDownScales[i], empty(), empty(), empty(), INTERMEDIATE_SIZE, HIDEN_SIZE, of(ACCURACY_LEVEL), BITS, BLOCK_SIZE),\n+                    inputWeights[i], empty(), of(EPSILON));\n+\n+            input = norm.output();\n+            skipBias = norm.input_skip_bias_sum();\n+            presentKeys[i] = attn.present_key();\n+            presentValues[i] = attn.present_value();\n+        }\n+\n+        Tensor<Float> logits = MatMulNBits(input,\n+                                           headWeight,\n+                                           headScales, empty(), empty(), empty(), HIDEN_SIZE, VOCAB_SIZE, of(ACCURACY_LEVEL), BITS, BLOCK_SIZE);\n+\n+        return new ForwardResponse(logits, presentKeys, presentValues);\n+    }\n+}\n","filename":"cr-examples\/onnx\/src\/test\/java\/oracle\/code\/onnx\/llm\/LlamaModel.java","additions":159,"deletions":0,"binary":false,"changes":159,"status":"added"},{"patch":"@@ -0,0 +1,4 @@\n+\/model.onnx.data\n+\/tokenizer_config.json\n+\/tokenizer.json\n+\n","filename":"cr-examples\/onnx\/src\/test\/resources\/oracle\/code\/onnx\/llm\/.gitignore","additions":4,"deletions":0,"binary":false,"changes":4,"status":"added"},{"patch":"@@ -0,0 +1,53 @@\n+{\n+    \"model\": {\n+        \"bos_token_id\": 128000,\n+        \"context_length\": 131072,\n+        \"decoder\": {\n+            \"session_options\": {\n+                \"log_id\": \"onnxruntime-genai\",\n+                \"provider_options\": []\n+            },\n+            \"filename\": \"model.onnx\",\n+            \"head_size\": 64,\n+            \"hidden_size\": 2048,\n+            \"inputs\": {\n+                \"input_ids\": \"inputIds\",\n+                \"attention_mask\": \"attentionMask\",\n+                \"past_key_names\": \"pastKey.%d\",\n+                \"past_value_names\": \"pastValue.%d\"\n+            },\n+            \"outputs\": {\n+                \"logits\": \"logits\",\n+                \"present_key_names\": \"presentKey.%d\",\n+                \"present_value_names\": \"presentValue.%d\"\n+            },\n+            \"num_attention_heads\": 32,\n+            \"num_hidden_layers\": 16,\n+            \"num_key_value_heads\": 8\n+        },\n+        \"eos_token_id\": [\n+            128001,\n+            128008,\n+            128009\n+        ],\n+        \"pad_token_id\": 128001,\n+        \"type\": \"llama\",\n+        \"vocab_size\": 128256\n+    },\n+    \"search\": {\n+        \"diversity_penalty\": 0.0,\n+        \"do_sample\": true,\n+        \"early_stopping\": true,\n+        \"length_penalty\": 1.0,\n+        \"max_length\": 131072,\n+        \"min_length\": 0,\n+        \"no_repeat_ngram_size\": 0,\n+        \"num_beams\": 1,\n+        \"num_return_sequences\": 1,\n+        \"past_present_share_buffer\": true,\n+        \"repetition_penalty\": 1.0,\n+        \"temperature\": 0.6,\n+        \"top_k\": 1,\n+        \"top_p\": 0.9\n+    }\n+}\n\\ No newline at end of file\n","filename":"cr-examples\/onnx\/src\/test\/resources\/oracle\/code\/onnx\/llm\/genai_config.json","additions":53,"deletions":0,"binary":false,"changes":53,"status":"added"}]}