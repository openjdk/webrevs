{"files":[{"patch":"@@ -33,0 +33,1 @@\n+import jdk.incubator.code.dialect.java.PrimitiveType;\n@@ -72,1 +73,2 @@\n-                .includeSys(\"cuda_fp16.h\");\n+                .includeSys(\"cuda_fp16.h\")\n+                .buildStructSingleMember(\"F16\", \"value\", \"half\");\n@@ -227,0 +229,1 @@\n+        oparen().halfType().cparen().obrace();\n@@ -232,1 +235,1 @@\n-        cparen();\n+        cparen().cbrace();\n@@ -245,0 +248,2 @@\n+        } else if (!hatF16ToFloatConvOp.wasFloat()) {\n+            dot().identifier(\"value\");\n@@ -277,1 +282,0 @@\n-        oparen();\n@@ -283,0 +287,2 @@\n+        oparen().halfType().cparen().obrace().oparen();\n+\n@@ -292,0 +298,2 @@\n+        } else if (op1 instanceof Op.Result r && !(r.op().resultType() instanceof PrimitiveType)) {\n+            dot().identifier(\"value\");\n@@ -310,0 +318,2 @@\n+        } else if (op2 instanceof Op.Result r && !(r.op().resultType() instanceof PrimitiveType)) {\n+            dot().identifier(\"value\");\n@@ -316,1 +326,1 @@\n-        cparen();\n+        cparen().cbrace();\n","filename":"hat\/backends\/ffi\/cuda\/src\/main\/java\/hat\/backend\/ffi\/CudaHATKernelBuilder.java","additions":14,"deletions":4,"binary":false,"changes":18,"status":"modified"},{"patch":"@@ -73,3 +73,2 @@\n-                .hashDefine(\"HAT_BARRIER\", _ -> identifier(\"barrier\").oparen().identifier(\"CLK_LOCAL_MEM_FENCE\").cparen());\n-        \/\/         )\n-        \/\/ );\n+                .hashDefine(\"HAT_BARRIER\", _ -> identifier(\"barrier\").oparen().identifier(\"CLK_LOCAL_MEM_FENCE\").cparen())\n+                .buildStructSingleMember(\"F16\", \"value\", \"half\");\n@@ -195,1 +194,1 @@\n-        oparen().halfType().cparen();\n+        oparen().halfType().cparen().obrace();\n@@ -200,0 +199,1 @@\n+        cbrace();\n@@ -225,3 +225,3 @@\n-        oparen().halfType().cparen();\n-        Value initValue = hatF16ToFloatConvOp.operands().getFirst();\n-        if (initValue instanceof Op.Result r) {\n+        oparen().floatType().cparen();\n+        Value value = hatF16ToFloatConvOp.operands().getFirst();\n+        if (value instanceof Op.Result r) {\n@@ -232,0 +232,2 @@\n+        } else if (!hatF16ToFloatConvOp.wasFloat()) {\n+            dot().identifier(\"value\");\n","filename":"hat\/backends\/ffi\/opencl\/src\/main\/java\/hat\/backend\/ffi\/OpenCLHATKernelBuilder.java","additions":9,"deletions":7,"binary":false,"changes":16,"status":"modified"},{"patch":"@@ -34,0 +34,1 @@\n+import hat.buffer.F16;\n@@ -41,0 +42,1 @@\n+import hat.device.DeviceSchema;\n@@ -50,0 +52,1 @@\n+import java.lang.reflect.Field;\n@@ -54,0 +57,1 @@\n+import java.util.HashSet;\n@@ -163,0 +167,62 @@\n+    private <T extends C99HATKernelBuilder<T>> void generateDeviceTypeStructs(T builder, String toText, Set<String> typedefs) {\n+        \/\/ From here is text processing\n+        String[] split = toText.split(\">\");\n+        \/\/ Each item is a data struct\n+        for (String s : split) {\n+            \/\/ curate: remove first character\n+            s = s.substring(1);\n+            String dsName = s.split(\":\")[0];\n+            if (typedefs.contains(dsName)) {\n+                continue;\n+            }\n+            typedefs.add(dsName);\n+            \/\/ sanitize dsName\n+            dsName = sanitize(dsName);\n+            builder.typedefKeyword()\n+                    .space()\n+                    .structKeyword()\n+                    .space()\n+                    .suffix_s(dsName)\n+                    .obrace()\n+                    .nl();\n+\n+            String[] members = s.split(\";\");\n+\n+            int j = 0;\n+            builder.in();\n+            for (int i = 0; i < members.length; i++) {\n+                String member = members[i];\n+                String[] field = member.split(\":\");\n+                if (i == 0) {\n+                    j = 1;\n+                }\n+                String isArray = field[j++];\n+                String type = field[j++];\n+                String name = field[j++];\n+                String lenValue = \"\";\n+                if (isArray.equals(\"[\")) {\n+                    lenValue = field[j];\n+                }\n+                j = 0;\n+                if (typedefs.contains(type))\n+                    type = sanitize(type) + \"_t\";\n+                else\n+                    type = sanitize(type);\n+\n+                builder.typeName(type)\n+                        .space()\n+                        .identifier(name);\n+\n+                if (isArray.equals(\"[\")) {\n+                    builder.space()\n+                            .osbrace()\n+                            .identifier(lenValue)\n+                            .csbrace();\n+                }\n+                builder.semicolon().nl();\n+            }\n+            builder.out();\n+            builder.cbrace().suffix_t(dsName).semicolon().nl();\n+        }\n+    }\n+\n@@ -216,0 +282,5 @@\n+            Set<String> typedefs = new HashSet<>();\n+\n+            \/\/ Add HAT reserved types\n+            typedefs.add(F16.class.getName());\n+\n@@ -217,1 +288,0 @@\n-                \/\/ 1.1 Load the class dynamically\n@@ -219,4 +289,3 @@\n-                    Class<?> clazz = (Class<?>) ((ClassType) typeElement).resolve(kernelCallGraph.computeContext.accelerator.lookup);\/\/Class.forName(typeElement.toString());\n-                    \/\/System.out.println(\"!!!!!!For  \"+clazz);\n-                    \/\/ TODO: Contract between the Java interface and the user. We require a method called `create` in order for this to work.\n-                    \/\/ 1.2 Obtain the create method\n+                    \/\/ Approach 1: The first approach support iFace and Buffer types to be used in Local and Private memory\n+                    \/\/ TODO: Once we decide to move towards the DeviceType implementation, we will remove this part\n+                    Class<?> clazz = (Class<?>) ((ClassType) typeElement).resolve(kernelCallGraph.computeContext.accelerator.lookup);\n@@ -226,0 +295,14 @@\n+                    if (invoke != null) {\n+                        \/\/ code gen of the struct\n+                        BoundSchema<?> boundSchema = Buffer.getBoundSchema(invoke);\n+                        boundSchema.schema().rootIfaceType.visitTypes(0, t -> {\n+                            if (!already.contains(t)) {\n+                                builder.typedef(boundSchema, t);\n+                                already.add(t);\n+                            }\n+                        });\n+                    } else {\n+                        \/\/ new approach for supporting DeviceTypes\n+                        Field schemaField = clazz.getDeclaredField(\"schema\");\n+                        schemaField.setAccessible(true);\n+                        Object schema = schemaField.get(schemaField);\n@@ -227,6 +310,8 @@\n-                    \/\/ code gen of the struct\n-                    BoundSchema<?> boundSchema = Buffer.getBoundSchema(invoke);\n-                    boundSchema.schema().rootIfaceType.visitTypes(0, t -> {\n-                        if (!already.contains(t)) {\n-                            builder.typedef(boundSchema, t);\n-                            already.add(t);\n+                        Class<?> deviceSchemaClass = Class.forName(DeviceSchema.class.getName());\n+                        Method toTextMethod = deviceSchemaClass.getDeclaredMethod(\"toText\");\n+                        toTextMethod.setAccessible(true);\n+                        String toText = (String) toTextMethod.invoke(schema);\n+                        if (toText != null) {\n+                            generateDeviceTypeStructs(builder, toText, typedefs);\n+                        } else {\n+                            throw new RuntimeException(\"[ERROR] Could not find valid device schema \");\n@@ -234,1 +319,1 @@\n-                    });\n+                    }\n@@ -275,0 +360,12 @@\n+    private String sanitize(String s) {\n+        String[] split1 = s.split(\"\\\\.\");\n+        if (split1.length == 1) {\n+            return s;\n+        }\n+        s = split1[split1.length - 1];\n+        if (s.split(\"\\\\$\").length > 1) {\n+            s = sanitize(s.split(\"\\\\$\")[1]);\n+        }\n+        return s;\n+    }\n+\n","filename":"hat\/backends\/ffi\/shared\/src\/main\/java\/hat\/backend\/ffi\/C99FFIBackend.java","additions":109,"deletions":12,"binary":false,"changes":121,"status":"modified"},{"patch":"@@ -30,1 +30,1 @@\n-    String HAT_MAPPING_TYPE = \"half\";\n+    String HAT_MAPPING_TYPE = F16.class.getSimpleName();\n","filename":"hat\/core\/src\/main\/java\/hat\/buffer\/F16.java","additions":1,"deletions":1,"binary":false,"changes":2,"status":"modified"},{"patch":"@@ -29,1 +29,0 @@\n-import jdk.incubator.code.dialect.java.ClassType;\n@@ -36,1 +35,1 @@\n-    interface F16Impl extends Buffer, F16 {\n+    interface F16Impl extends Struct, F16 {\n","filename":"hat\/core\/src\/main\/java\/hat\/buffer\/F16Array.java","additions":1,"deletions":2,"binary":false,"changes":3,"status":"modified"},{"patch":"@@ -34,0 +34,2 @@\n+import hat.dialect.HATMemoryLoadOp;\n+import hat.dialect.HATPrivateVarInitOp;\n@@ -153,0 +155,4 @@\n+    T hatPrivateVarInitOp(ScopedCodeBuilderContext builderContext, HATPrivateVarInitOp hatPrivateVarInitOp);\n+\n+    T hatMemoryLoadOp(ScopedCodeBuilderContext builderContext, HATMemoryLoadOp hatMemoryLoadOp);\n+\n@@ -183,0 +189,1 @@\n+            case HATPrivateVarInitOp $ -> hatPrivateVarInitOp(buildContext, $);\n@@ -202,0 +209,1 @@\n+            case HATMemoryLoadOp $ -> hatMemoryLoadOp(buildContext, $);\n","filename":"hat\/core\/src\/main\/java\/hat\/codebuilders\/BabylonOpBuilder.java","additions":8,"deletions":0,"binary":false,"changes":8,"status":"modified"},{"patch":"@@ -37,0 +37,2 @@\n+import hat.dialect.HATMemoryLoadOp;\n+import hat.dialect.HATPrivateVarInitOp;\n@@ -241,1 +243,1 @@\n-        typeName(\"half\")\n+        halfType()\n@@ -254,1 +256,1 @@\n-        oparen();\n+\n@@ -259,0 +261,1 @@\n+        oparen().halfType().cparen().obrace().oparen();\n@@ -264,0 +267,2 @@\n+        } else if (op1 instanceof Op.Result r && !(r.op().resultType() instanceof PrimitiveType)) {\n+            dot().identifier(\"value\");\n@@ -273,0 +278,2 @@\n+        } else if (op2 instanceof Op.Result r && !(r.op().resultType() instanceof PrimitiveType)) {\n+            dot().identifier(\"value\");\n@@ -275,1 +282,1 @@\n-        cparen();\n+        cparen().cbrace();\n@@ -282,0 +289,1 @@\n+        dot().identifier(\"value\");\n@@ -315,0 +323,37 @@\n+    @Override\n+    public T hatPrivateVarInitOp(ScopedCodeBuilderContext builderContext, HATPrivateVarInitOp hatPrivateVarInitOp) {\n+        suffix_t(hatPrivateVarInitOp.classType()).space().identifier(hatPrivateVarInitOp.varName());\n+        space().equals().space();\n+        Value operand = hatPrivateVarInitOp.operands().getFirst();\n+        if (operand instanceof Op.Result r) {\n+            recurse(builderContext, r.op());\n+        }\n+        return self();\n+    }\n+\n+    @Override\n+    public T hatMemoryLoadOp(ScopedCodeBuilderContext builderContext, HATMemoryLoadOp hatMemoryLoadOp) {\n+        List<Value> operands = hatMemoryLoadOp.operands();\n+        Value base = operands.get(0);\n+        if (base instanceof Op.Result r) {\n+           recurse(builderContext, r.op());\n+        }\n+        dot().identifier(hatMemoryLoadOp.memberName());\n+\n+        if (operands.size() > 1) {\n+            \/\/ If the hatMemoryLoadOp has more than 1 operand,\n+            \/\/ then we know that the second operand represents\n+            \/\/ an index to access an array, since members, otherwise,\n+            \/\/ will be accessed via structVarName.member1.member2.member3...,  etc.\n+\n+            \/\/ The following code generates [ indexValue ]\n+            osbrace();\n+            Value index = operands.get(1);\n+            if (index instanceof Op.Result r) {\n+                recurse(builderContext, r.op());\n+            }\n+            csbrace();\n+        }\n+        return self();\n+    }\n+\n","filename":"hat\/core\/src\/main\/java\/hat\/codebuilders\/C99HATKernelBuilder.java","additions":48,"deletions":3,"binary":false,"changes":51,"status":"modified"},{"patch":"@@ -27,0 +27,1 @@\n+import hat.buffer.F16;\n@@ -511,1 +512,1 @@\n-        return typeName(\"half\");\n+        return typeName(F16.HAT_MAPPING_TYPE + \"_t\");\n","filename":"hat\/core\/src\/main\/java\/hat\/codebuilders\/CodeBuilder.java","additions":2,"deletions":1,"binary":false,"changes":3,"status":"modified"},{"patch":"@@ -387,0 +387,9 @@\n+\n+    public T buildStructSingleMember(String structName, String member, String type) {\n+        typedefKeyword().space().structKeyword().space().suffix_s(structName)\n+                .obrace().nl()\n+                .in()\n+                    .typeName(type).space().typeName(member).semicolon().nl()\n+                .out().cbrace().space().suffix_t(structName).semicolon().nl();\n+        return self();\n+    }\n","filename":"hat\/core\/src\/main\/java\/hat\/codebuilders\/HATCodeBuilder.java","additions":9,"deletions":0,"binary":false,"changes":9,"status":"modified"},{"patch":"@@ -29,0 +29,1 @@\n+import hat.device.DeviceType;\n@@ -33,0 +34,1 @@\n+import hat.dialect.HATPhaseUtils;\n@@ -344,1 +346,2 @@\n-        return (ifaceType.iface.getName().equals(F16.class.getName()) || ifaceType.iface.getName().equals(F16Array.F16Impl.class.getName()));\n+        return (ifaceType.iface.getName().equals(F16.class.getName())\n+                || ifaceType.iface.getName().equals(F16Array.F16Impl.class.getName()));\n@@ -356,1 +359,1 @@\n-                                typeName(F16.HAT_MAPPING_TYPE);\n+                                typeName(\"half\");\n@@ -440,1 +443,2 @@\n-                || invokeOp.invokeDescriptor().refType().toString().equals(F16.class.getCanonicalName())) {\n+                || invokeOp.invokeDescriptor().refType().toString().equals(F16.class.getCanonicalName())\n+                || HATPhaseUtils.isDeviceTypeInvokeDescriptor(invokeOp)) {\n@@ -497,1 +501,1 @@\n-                        ampersand();\n+                       ampersand();\n","filename":"hat\/core\/src\/main\/java\/hat\/codebuilders\/HATCodeBuilderWithContext.java","additions":8,"deletions":4,"binary":false,"changes":12,"status":"modified"},{"patch":"@@ -0,0 +1,174 @@\n+\/*\n+ * Copyright (c) 2025, Oracle and\/or its affiliates. All rights reserved.\n+ * DO NOT ALTER OR REMOVE COPYRIGHT NOTICES OR THIS FILE HEADER.\n+ *\n+ * This code is free software; you can redistribute it and\/or modify it\n+ * under the terms of the GNU General Public License version 2 only, as\n+ * published by the Free Software Foundation.  Oracle designates this\n+ * particular file as subject to the \"Classpath\" exception as provided\n+ * by Oracle in the LICENSE file that accompanied this code.\n+ *\n+ * This code is distributed in the hope that it will be useful, but WITHOUT\n+ * ANY WARRANTY; without even the implied warranty of MERCHANTABILITY or\n+ * FITNESS FOR A PARTICULAR PURPOSE.  See the GNU General Public License\n+ * version 2 for more details (a copy is included in the LICENSE file that\n+ * accompanied this code).\n+ *\n+ * You should have received a copy of the GNU General Public License version\n+ * 2 along with this work; if not, write to the Free Software Foundation,\n+ * Inc., 51 Franklin St, Fifth Floor, Boston, MA 02110-1301 USA.\n+ *\n+ * Please contact Oracle, 500 Oracle Parkway, Redwood Shores, CA 94065 USA\n+ * or visit www.oracle.com if you need additional information or have any\n+ * questions.\n+ *\/\n+package hat.device;\n+\n+import hat.buffer.F16;\n+\n+import java.lang.reflect.Method;\n+import java.util.ArrayList;\n+import java.util.HashMap;\n+import java.util.HashSet;\n+import java.util.LinkedList;\n+import java.util.List;\n+import java.util.Map;\n+import java.util.Set;\n+import java.util.function.Consumer;\n+\n+public class DeviceSchema<T extends DeviceType> {\n+\n+    private final Class<T> klass;\n+    private final List<List<String>> members = new ArrayList<>();\n+    private final Map<String, Integer> arraySize = new HashMap<>();\n+    private final Map<Class<?>, Consumer<DeviceSchema<T>>> deps = new HashMap<>();\n+    private final StringBuilder representationBuilder = new StringBuilder();\n+    private final Set<String> visited = new HashSet<>();\n+\n+    private static final Map<Class<?>, String> specialTypes = new HashMap<>();\n+\n+    static {\n+        specialTypes.put(F16.class, \"half\");\n+    }\n+\n+    public DeviceSchema(Class<T> klass) {\n+        this.klass = klass;\n+    }\n+    int currentLevel = 0;\n+\n+    public static <T extends DeviceType> DeviceSchema<T> of(Class<T> klass, Consumer<DeviceSchema<T>> schemaBuilder) {\n+        DeviceSchema<T> deviceSchema =  new DeviceSchema<>(klass);\n+        schemaBuilder.accept(deviceSchema);\n+        deviceSchema.materialize();\n+        return deviceSchema;\n+    }\n+\n+    public DeviceSchema<T> withField(String fieldName) {\n+        if (members.isEmpty()) {\n+            members.add(new ArrayList<>());\n+        }\n+        this.members.get(currentLevel).add(fieldName);\n+        return this;\n+    }\n+\n+    public DeviceSchema<T> withArray(String fieldName, int size) {\n+        if (members.isEmpty()) {\n+            members.add(new LinkedList<>());\n+        }\n+        this.members.get(currentLevel).add(fieldName);\n+        arraySize.put(fieldName, size);\n+        return this;\n+    }\n+\n+    public DeviceSchema<T> withDeps(Class<?> klass, Consumer<DeviceSchema<T>> depConsumer) {\n+        \/\/ increment the level\n+        this.currentLevel++;\n+        this.members.add(new LinkedList<>());\n+        deps.put(klass, depConsumer);\n+        depConsumer.accept(this);\n+        materialize(representationBuilder, klass);\n+        return this;\n+    }\n+\n+    private boolean isInterfaceType(Class<?> type) {\n+        return type.isInterface();\n+    }\n+\n+    \/\/ Materialize methods are only reachable within this class.\n+    private void materialize() {\n+        materialize(representationBuilder, klass);\n+    }\n+\n+    \/\/ The following method generates an intermediate representation in text form for each level\n+    \/\/ of the hierarchy.\n+    \/\/ It inspects each type and its members. If a member is also a non-primitive type\n+    \/\/ then it recursively inspect its inner members.\n+    \/\/ We keep track of all generated data structured by maintaining a visited set. Thus,\n+    \/\/ we avoid duplicates in the text form.\n+    private void materialize(StringBuilder sb, Class<?> klass) {\n+        try {\n+            Class<?> aClass = Class.forName(klass.getName());\n+            Method[] declaredMethods = aClass.getDeclaredMethods();\n+            sb.append(\"<\");\n+            sb.append(klass.getName());\n+            sb.append(':');\n+            visited.add(klass.getName());\n+\n+            for (String fieldName : members.get(currentLevel)) {\n+                boolean wasProcessed = false;\n+                for (Method method : declaredMethods) {\n+                    method.setAccessible(true);\n+                    if (method.getName().equals(fieldName)) {\n+                        Class<?> returnType = method.getReturnType();\n+                        if (returnType.equals(void.class)) {\n+                            continue;\n+                        }\n+\n+                        if (isInterfaceType(returnType) && !visited.contains(returnType.getName())) {\n+                            \/\/ inspect the dependency and add it at the front of the string builder\n+                            StringBuilder depsBuilder = new StringBuilder();\n+                            materialize(depsBuilder, returnType);\n+                            sb = depsBuilder.append(sb);\n+                        }\n+\n+                        String type = returnType.getName();\n+                        if (specialTypes.containsKey(klass)) {\n+                            type = specialTypes.get(klass);\n+                        }\n+\n+                        if (arraySize.containsKey(method.getName())) {\n+                            sb.append(\"[\");                        \/\/ Array indicator\n+                            sb.append(\":\");                        \/\/ separator\n+                            sb.append(type);                       \/\/ type\n+                            sb.append(\":\");                        \/\/ separator\n+                            sb.append(method.getName());           \/\/ variableName\n+                            sb.append(\":\");                        \/\/ separator\n+                            sb.append(arraySize.get(method.getName()));  \/\/ Array size\n+                            sb.append(\";\");                        \/\/ member separator\n+                        } else {\n+                            sb.append(\"s\");                         \/\/ scalar indicator\n+                            sb.append(\":\");                         \/\/ separator\n+                            sb.append(type);                        \/\/ type\n+                            sb.append(\":\");                         \/\/ separator\n+                            sb.append(method.getName());            \/\/ var name\n+                            sb.append(\";\");                         \/\/ member separator\n+                        }\n+                        wasProcessed = true;\n+                    }\n+                }\n+                if (!wasProcessed) {\n+                    throw new RuntimeException(\"could not find method \" + fieldName + \" in class \" + klass.getName());\n+                }\n+                currentLevel--;\n+            }\n+\n+        } catch (ClassNotFoundException e) {\n+            IO.println(\"Error during materialization of DeviceType: \" + e.getMessage());\n+        }\n+        sb.append(\">\");\n+    }\n+\n+    public String toText() {\n+        return this.representationBuilder.toString();\n+    }\n+}\n","filename":"hat\/core\/src\/main\/java\/hat\/device\/DeviceSchema.java","additions":174,"deletions":0,"binary":false,"changes":174,"status":"added"},{"patch":"@@ -25,3 +25,1 @@\n-package hat.buffer;\n-\n-public interface HATVector {\n+package hat.device;\n@@ -29,0 +27,1 @@\n+public interface DeviceType {\n","filename":"hat\/core\/src\/main\/java\/hat\/device\/DeviceType.java","additions":2,"deletions":3,"binary":false,"changes":5,"previous_filename":"hat\/core\/src\/main\/java\/hat\/buffer\/HATVector.java","status":"copied"},{"patch":"@@ -40,0 +40,1 @@\n+    private final boolean wasFloat;\n@@ -41,1 +42,1 @@\n-    public HATF16ToFloatConvOp(TypeElement typeElement, boolean isLocal, List<Value> operands) {\n+    public HATF16ToFloatConvOp(TypeElement typeElement, boolean isLocal, boolean wasFloat, List<Value> operands) {\n@@ -45,0 +46,1 @@\n+        this.wasFloat = wasFloat;\n@@ -51,0 +53,1 @@\n+        this.wasFloat = op.wasFloat;\n@@ -72,0 +75,4 @@\n+    public boolean wasFloat() {\n+        return wasFloat;\n+    }\n+\n","filename":"hat\/core\/src\/main\/java\/hat\/dialect\/HATF16ToFloatConvOp.java","additions":8,"deletions":1,"binary":false,"changes":9,"status":"modified"},{"patch":"@@ -0,0 +1,50 @@\n+\/*\n+ * Copyright (c) 2025, Oracle and\/or its affiliates. All rights reserved.\n+ * DO NOT ALTER OR REMOVE COPYRIGHT NOTICES OR THIS FILE HEADER.\n+ *\n+ * This code is free software; you can redistribute it and\/or modify it\n+ * under the terms of the GNU General Public License version 2 only, as\n+ * published by the Free Software Foundation.  Oracle designates this\n+ * particular file as subject to the \"Classpath\" exception as provided\n+ * by Oracle in the LICENSE file that accompanied this code.\n+ *\n+ * This code is distributed in the hope that it will be useful, but WITHOUT\n+ * ANY WARRANTY; without even the implied warranty of MERCHANTABILITY or\n+ * FITNESS FOR A PARTICULAR PURPOSE.  See the GNU General Public License\n+ * version 2 for more details (a copy is included in the LICENSE file that\n+ * accompanied this code).\n+ *\n+ * You should have received a copy of the GNU General Public License version\n+ * 2 along with this work; if not, write to the Free Software Foundation,\n+ * Inc., 51 Franklin St, Fifth Floor, Boston, MA 02110-1301 USA.\n+ *\n+ * Please contact Oracle, 500 Oracle Parkway, Redwood Shores, CA 94065 USA\n+ * or visit www.oracle.com if you need additional information or have any\n+ * questions.\n+ *\/\n+package hat.dialect;\n+\n+import jdk.incubator.code.CopyContext;\n+import jdk.incubator.code.TypeElement;\n+import jdk.incubator.code.Value;\n+import jdk.incubator.code.dialect.java.ClassType;\n+\n+import java.util.List;\n+\n+public abstract class HATMemoryDefOp extends HATOp {\n+    private final String varName;\n+\n+    public HATMemoryDefOp(String varName, List<Value> operands) {\n+        super(operands);\n+        this.varName = varName;\n+    }\n+\n+    protected HATMemoryDefOp(HATMemoryDefOp that, CopyContext cc) {\n+        super(that, cc);\n+        this.varName = that.varName;\n+    }\n+\n+    public String varName() {\n+        return varName;\n+    }\n+}\n","filename":"hat\/core\/src\/main\/java\/hat\/dialect\/HATMemoryDefOp.java","additions":50,"deletions":0,"binary":false,"changes":50,"status":"added"},{"patch":"@@ -0,0 +1,76 @@\n+\/*\n+ * Copyright (c) 2025, Oracle and\/or its affiliates. All rights reserved.\n+ * DO NOT ALTER OR REMOVE COPYRIGHT NOTICES OR THIS FILE HEADER.\n+ *\n+ * This code is free software; you can redistribute it and\/or modify it\n+ * under the terms of the GNU General Public License version 2 only, as\n+ * published by the Free Software Foundation.  Oracle designates this\n+ * particular file as subject to the \"Classpath\" exception as provided\n+ * by Oracle in the LICENSE file that accompanied this code.\n+ *\n+ * This code is distributed in the hope that it will be useful, but WITHOUT\n+ * ANY WARRANTY; without even the implied warranty of MERCHANTABILITY or\n+ * FITNESS FOR A PARTICULAR PURPOSE.  See the GNU General Public License\n+ * version 2 for more details (a copy is included in the LICENSE file that\n+ * accompanied this code).\n+ *\n+ * You should have received a copy of the GNU General Public License version\n+ * 2 along with this work; if not, write to the Free Software Foundation,\n+ * Inc., 51 Franklin St, Fifth Floor, Boston, MA 02110-1301 USA.\n+ *\n+ * Please contact Oracle, 500 Oracle Parkway, Redwood Shores, CA 94065 USA\n+ * or visit www.oracle.com if you need additional information or have any\n+ * questions.\n+ *\/\n+package hat.dialect;\n+\n+import jdk.incubator.code.CopyContext;\n+import jdk.incubator.code.Op;\n+import jdk.incubator.code.OpTransformer;\n+import jdk.incubator.code.TypeElement;\n+import jdk.incubator.code.Value;\n+import jdk.incubator.code.dialect.java.ClassType;\n+import jdk.incubator.code.dialect.java.JavaType;\n+\n+import java.util.List;\n+import java.util.Map;\n+\n+public class HATMemoryLoadOp extends HATMemoryDefOp {\n+\n+    private final TypeElement typeElement;\n+    private final TypeElement invokeResultType;\n+    private final String memberName;\n+\n+    public HATMemoryLoadOp(TypeElement typeElement, TypeElement invokeResultType, String memberName, List<Value> operands) {\n+        super(\"\", operands);\n+        this.typeElement = typeElement;\n+        this.invokeResultType = invokeResultType;\n+        this.memberName = memberName;\n+    }\n+\n+    public HATMemoryLoadOp(HATMemoryLoadOp op, CopyContext copyContext) {\n+        super(op, copyContext);\n+        this.typeElement = op.resultType();\n+        this.invokeResultType = op.invokeResultType;\n+        this.memberName = op.memberName;\n+    }\n+\n+    @Override\n+    public Op transform(CopyContext copyContext, OpTransformer opTransformer) {\n+        return new HATMemoryLoadOp(this, copyContext);\n+    }\n+\n+    @Override\n+    public TypeElement resultType() {\n+        return typeElement;\n+    }\n+\n+    @Override\n+    public Map<String, Object> externalize() {\n+        return Map.of(\"hat.dialect.hatMemoryLoadOp.\" + memberName, typeElement);\n+    }\n+\n+    public String memberName() {\n+        return memberName;\n+    }\n+}\n","filename":"hat\/core\/src\/main\/java\/hat\/dialect\/HATMemoryLoadOp.java","additions":76,"deletions":0,"binary":false,"changes":76,"status":"added"},{"patch":"@@ -27,0 +27,1 @@\n+import hat.device.DeviceType;\n@@ -35,0 +36,2 @@\n+import java.util.Arrays;\n+import java.util.HashSet;\n@@ -36,0 +39,1 @@\n+import java.util.Set;\n@@ -185,0 +189,39 @@\n+    public static void inspectNewLevel(Class<?> interfaceClass, Set<Class<?>> interfaceSet) {\n+        if (interfaceClass != null && interfaceSet.add(interfaceClass)) {\n+            \/\/ only if we add a new interface class, we inspect all interfaces that extends the current inspected class\n+            Arrays.stream(interfaceClass.getInterfaces())\n+                    .forEach(superInterface -> inspectNewLevel(superInterface, interfaceSet));\n+        }\n+    }\n+\n+    public static Set<Class<?>> inspectAllInterfaces(Class<?> klass) {\n+        Set<Class<?>> interfaceSet = new HashSet<>();\n+        while (klass != null) {\n+            Arrays.stream(klass.getInterfaces())\n+                    .forEach(interfaceClass -> inspectNewLevel(interfaceClass, interfaceSet));\n+            klass = klass.getSuperclass();\n+        }\n+        return interfaceSet;\n+    }\n+\n+    public static boolean isDeviceType(JavaOp.InvokeOp invokeOp) {\n+        TypeElement typeElement = invokeOp.resultType();\n+        Set<Class<?>> interfaces = Set.of();\n+        try {\n+            Class<?> aClass = Class.forName(typeElement.toString());\n+            interfaces = inspectAllInterfaces(aClass);\n+        } catch (ClassNotFoundException _) {\n+        }\n+        return interfaces.contains(DeviceType.class);\n+    }\n+\n+    public static boolean isDeviceTypeInvokeDescriptor(JavaOp.InvokeOp invokeOp) {\n+        TypeElement typeElement = invokeOp.invokeDescriptor().refType();\n+        Set<Class<?>> interfaces = Set.of();\n+        try {\n+            Class<?> aClass = Class.forName(typeElement.toString());\n+            interfaces = inspectAllInterfaces(aClass);\n+        } catch (ClassNotFoundException _) {\n+        }\n+        return interfaces.contains(DeviceType.class);\n+    }\n","filename":"hat\/core\/src\/main\/java\/hat\/dialect\/HATPhaseUtils.java","additions":43,"deletions":0,"binary":false,"changes":43,"status":"modified"},{"patch":"@@ -0,0 +1,84 @@\n+\/*\n+ * Copyright (c) 2025, Oracle and\/or its affiliates. All rights reserved.\n+ * DO NOT ALTER OR REMOVE COPYRIGHT NOTICES OR THIS FILE HEADER.\n+ *\n+ * This code is free software; you can redistribute it and\/or modify it\n+ * under the terms of the GNU General Public License version 2 only, as\n+ * published by the Free Software Foundation.  Oracle designates this\n+ * particular file as subject to the \"Classpath\" exception as provided\n+ * by Oracle in the LICENSE file that accompanied this code.\n+ *\n+ * This code is distributed in the hope that it will be useful, but WITHOUT\n+ * ANY WARRANTY; without even the implied warranty of MERCHANTABILITY or\n+ * FITNESS FOR A PARTICULAR PURPOSE.  See the GNU General Public License\n+ * version 2 for more details (a copy is included in the LICENSE file that\n+ * accompanied this code).\n+ *\n+ * You should have received a copy of the GNU General Public License version\n+ * 2 along with this work; if not, write to the Free Software Foundation,\n+ * Inc., 51 Franklin St, Fifth Floor, Boston, MA 02110-1301 USA.\n+ *\n+ * Please contact Oracle, 500 Oracle Parkway, Redwood Shores, CA 94065 USA\n+ * or visit www.oracle.com if you need additional information or have any\n+ * questions.\n+ *\/\n+package hat.dialect;\n+\n+import jdk.incubator.code.CopyContext;\n+import jdk.incubator.code.Op;\n+import jdk.incubator.code.OpTransformer;\n+import jdk.incubator.code.TypeElement;\n+import jdk.incubator.code.Value;\n+import jdk.incubator.code.dialect.java.ClassType;\n+\n+import java.util.List;\n+import java.util.Map;\n+\n+public class HATPrivateVarInitOp extends HATMemoryOp {\n+\n+    private final TypeElement typeElement;\n+    private final ClassType klassType;\n+    private final TypeElement invokeResultType;\n+    private final String varName;\n+\n+    public HATPrivateVarInitOp(String varName, ClassType javaType, TypeElement typeElement, TypeElement invokeResultType, List<Value> operands) {\n+        super(varName, operands);\n+        this.varName = varName;\n+        this.typeElement = typeElement;\n+        this.klassType = javaType;\n+        this.invokeResultType = invokeResultType;\n+    }\n+\n+    public HATPrivateVarInitOp(HATPrivateVarInitOp op, CopyContext copyContext) {\n+        super(op, copyContext);\n+        this.varName = op.varName;\n+        this.typeElement = op.resultType();\n+        this.klassType = op.klassType;\n+        this.invokeResultType = op.invokeResultType;\n+    }\n+\n+    @Override\n+    public Op transform(CopyContext copyContext, OpTransformer opTransformer) {\n+        return new HATPrivateVarInitOp(this, copyContext);\n+    }\n+\n+    @Override\n+    public TypeElement resultType() {\n+        return typeElement;\n+    }\n+\n+    @Override\n+    public Map<String, Object> externalize() {\n+        return Map.of(\"hat.dialect.hatPrivateVarInitOp.\" + varName, typeElement);\n+    }\n+\n+    @Override\n+    public ClassType classType() {\n+        return klassType;\n+    }\n+\n+    @Override\n+    public TypeElement invokeType() {\n+        return invokeResultType;\n+    }\n+}\n","filename":"hat\/core\/src\/main\/java\/hat\/dialect\/HATPrivateVarInitOp.java","additions":84,"deletions":0,"binary":false,"changes":84,"status":"added"},{"patch":"@@ -31,0 +31,1 @@\n+import hat.device.DeviceType;\n@@ -39,0 +40,1 @@\n+import jdk.incubator.code.TypeElement;\n@@ -54,0 +56,1 @@\n+import java.util.Arrays;\n@@ -56,0 +59,1 @@\n+import java.util.HashSet;\n@@ -60,0 +64,1 @@\n+import java.util.Set;\n","filename":"hat\/core\/src\/main\/java\/hat\/optools\/OpTk.java","additions":5,"deletions":0,"binary":false,"changes":5,"status":"modified"},{"patch":"@@ -40,0 +40,1 @@\n+\n@@ -71,18 +72,0 @@\n-\n-    default Set<Class<?>> inspectAllInterfaces(Class<?> klass) {\n-        Set<Class<?>> interfaceSet = new HashSet<>();\n-        while (klass != null) {\n-            Arrays.stream(klass.getInterfaces())\n-                    .forEach(interfaceClass -> inspectNewLevel(interfaceClass, interfaceSet));\n-            klass = klass.getSuperclass();\n-        }\n-        return interfaceSet;\n-    }\n-\n-    default void inspectNewLevel(Class<?> interfaceClass, Set<Class<?>> interfaceSet) {\n-        if (interfaceClass != null && interfaceSet.add(interfaceClass)) {\n-            \/\/ only if we add a new interface class, we inspect all interfaces that extends the current inspected class\n-            Arrays.stream(interfaceClass.getInterfaces())\n-                    .forEach(superInterface -> inspectNewLevel(superInterface, interfaceSet));\n-        }\n-    }\n","filename":"hat\/core\/src\/main\/java\/hat\/phases\/HATDialect.java","additions":1,"deletions":18,"binary":false,"changes":19,"status":"modified"},{"patch":"@@ -29,1 +29,0 @@\n-import hat.buffer.F16Array;\n@@ -84,1 +83,1 @@\n-                && OpTk.isIfaceBufferMethod(accelerator.lookup, invokeOp)\n+                \/\/&& OpTk.isIfaceBufferMethod(accelerator.lookup, invokeOp)\n@@ -143,1 +142,8 @@\n-        HATF16ToFloatConvOp convOp1 = new HATF16ToFloatConvOp(JavaType.FLOAT, isLocal, outputOperands);\n+        boolean wasFloat = false;\n+        Value first = operands.getFirst();\n+        if (first instanceof Op.Result r && r.op() instanceof CoreOp.VarAccessOp.VarLoadOp varLoadOp) {\n+            if  (varLoadOp.resultType().equals(JavaType.FLOAT)) {\n+                wasFloat = true;\n+            }\n+        }\n+        HATF16ToFloatConvOp convOp1 = new HATF16ToFloatConvOp(JavaType.FLOAT, isLocal, wasFloat, outputOperands);\n","filename":"hat\/core\/src\/main\/java\/hat\/phases\/HATDialectifyFP16Phase.java","additions":9,"deletions":3,"binary":false,"changes":12,"status":"modified"},{"patch":"@@ -29,0 +29,1 @@\n+import hat.dialect.HATMemoryLoadOp;\n@@ -30,0 +31,2 @@\n+import hat.dialect.HATPhaseUtils;\n+import hat.dialect.HATPrivateVarInitOp;\n@@ -34,1 +37,0 @@\n-import jdk.incubator.code.CopyContext;\n@@ -36,1 +38,0 @@\n-import jdk.incubator.code.Value;\n@@ -40,0 +41,2 @@\n+import jdk.incubator.code.dialect.java.JavaType;\n+import jdk.incubator.code.dialect.java.PrimitiveType;\n@@ -41,1 +44,2 @@\n-import java.util.ArrayList;\n+import java.util.HashMap;\n+import java.util.HashSet;\n@@ -43,2 +47,1 @@\n-import java.util.List;\n-import java.util.Optional;\n+import java.util.Map;\n@@ -49,0 +52,2 @@\n+import static hat.dialect.HATPhaseUtils.isDeviceTypeInvokeDescriptor;\n+\n@@ -50,0 +55,1 @@\n+\n@@ -51,1 +57,13 @@\n-    @Override  public Accelerator accelerator(){\n+\n+    private static final Set<String> reservedMethods = new HashSet<>();\n+\n+    static {\n+        reservedMethods.add(\"createLocal\");\n+        reservedMethods.add(\"createPrivate\");\n+        reservedMethods.add(\"create\");\n+        reservedMethods.add(\"float2View\");\n+        reservedMethods.add(\"float4View\");\n+    }\n+\n+    @Override\n+    public Accelerator accelerator(){\n@@ -54,0 +72,1 @@\n+\n@@ -57,0 +76,1 @@\n+\n@@ -61,10 +81,0 @@\n-    Stream<JavaOp.InvokeOp> invokeOpOperands(Op op){\n-        return op.operands().stream()\n-                .filter(o -> o instanceof Op.Result result\n-                        && result.op() instanceof JavaOp.InvokeOp invokeOp\n-                        && isIfaceBufferInvokeWithName(invokeOp))\n-                .map(r -> (JavaOp.InvokeOp) (((Op.Result) r).op()));\n-    }\n-\n-\n-\n@@ -73,1 +83,1 @@\n-        var here = OpTk.CallSite.of(PrivatePhase.class, \"apply\");\n+        var here = OpTk.CallSite.of(PrivateMemoryPhase.class, \"HATDialectifyMemoryPhase\");\n@@ -106,1 +116,1 @@\n-            }else{\n+            } else {\n@@ -116,2 +126,2 @@\n-    public static class PrivatePhase extends HATDialectifyMemoryPhase {\n-        public PrivatePhase(Accelerator accelerator) {\n+    public static class PrivateMemoryPhase extends HATDialectifyMemoryPhase {\n+        public PrivateMemoryPhase(Accelerator accelerator) {\n@@ -120,2 +130,8 @@\n-        @Override protected boolean isIfaceBufferInvokeWithName(JavaOp.InvokeOp invokeOp){\n-             return isIfaceBufferInvokeWithName(invokeOp, HATPrivateVarOp.INTRINSIC_NAME);\n+\n+        @Override\n+        protected boolean isIfaceBufferInvokeWithName(JavaOp.InvokeOp invokeOp){\n+            if (isIfaceBufferInvokeWithName(invokeOp, HATPrivateVarOp.INTRINSIC_NAME)) {\n+                return true;\n+            } else {\n+                return isMethod(invokeOp, HATPrivateVarOp.INTRINSIC_NAME) && HATPhaseUtils.isDeviceType(invokeOp);\n+            }\n@@ -124,2 +140,3 @@\n-        @Override protected HATMemoryOp factory(Block.Builder builder, CoreOp.VarOp varOp, JavaOp.InvokeOp invokeOp) {\n-            var op=  new HATPrivateVarOp(\n+        @Override\n+        protected HATMemoryOp factory(Block.Builder builder, CoreOp.VarOp varOp, JavaOp.InvokeOp invokeOp) {\n+            var op = new HATPrivateVarOp(\n@@ -137,1 +154,1 @@\n-    public static class SharedPhase extends HATDialectifyMemoryPhase {\n+    public static class LocalMemoryPhase extends HATDialectifyMemoryPhase {\n@@ -139,1 +156,1 @@\n-        public SharedPhase(Accelerator accelerator) {\n+        public LocalMemoryPhase(Accelerator accelerator) {\n@@ -142,2 +159,8 @@\n-        @Override protected boolean isIfaceBufferInvokeWithName(JavaOp.InvokeOp invokeOp){\n-            return isIfaceBufferInvokeWithName(invokeOp, HATLocalVarOp.INTRINSIC_NAME);\n+\n+        @Override\n+        protected boolean isIfaceBufferInvokeWithName(JavaOp.InvokeOp invokeOp){\n+            if (isIfaceBufferInvokeWithName(invokeOp, HATLocalVarOp.INTRINSIC_NAME)) {\n+                return true;\n+            } else {\n+                return (isMethod(invokeOp, HATLocalVarOp.INTRINSIC_NAME) &&  HATPhaseUtils.isDeviceType(invokeOp));\n+            }\n@@ -146,1 +169,2 @@\n-        @Override protected HATMemoryOp factory(Block.Builder builder, CoreOp.VarOp varOp, JavaOp.InvokeOp invokeOp) {\n+        @Override\n+        protected HATMemoryOp factory(Block.Builder builder, CoreOp.VarOp varOp, JavaOp.InvokeOp invokeOp) {\n@@ -158,0 +182,88 @@\n+\n+    public static class DeviceTypePhase extends HATDialectifyMemoryPhase {\n+\n+        public DeviceTypePhase(Accelerator accelerator) {\n+            super(accelerator);\n+        }\n+\n+        @Override\n+        protected boolean isIfaceBufferInvokeWithName(JavaOp.InvokeOp invokeOp){\n+            if (isIfaceBufferInvokeWithName(invokeOp, HATLocalVarOp.INTRINSIC_NAME)) {\n+                return true;\n+            } else {\n+                return (isMethod(invokeOp, HATLocalVarOp.INTRINSIC_NAME) &&  HATPhaseUtils.isDeviceType(invokeOp));\n+            }\n+        }\n+\n+        private boolean isDeviceTypeReservedMethod(JavaOp.InvokeOp invokeOp){\n+            return reservedMethods.contains(invokeOp.invokeDescriptor().name());\n+        }\n+\n+        private boolean meetConditionsForMemoryLoadOp(JavaOp.InvokeOp invokeOp) {\n+            return isDeviceTypeInvokeDescriptor(invokeOp)\n+                    && (invokeOp.resultType() != JavaType.VOID)\n+                    && (!(invokeOp.resultType() instanceof PrimitiveType))\n+                    && (!isDeviceTypeReservedMethod(invokeOp));\n+        }\n+\n+        @Override\n+        public CoreOp.FuncOp apply(CoreOp.FuncOp funcOp) {\n+            var here = OpTk.CallSite.of(PrivateMemoryPhase.class, \"HATDialectifyMemoryPhase - memoryLoadOp\");\n+            before(here, funcOp);\n+            Map<CoreOp.VarOp, JavaOp.InvokeOp> varTable = new HashMap<>();\n+            Stream<CodeElement<?, ?>> memoryLoadOps = funcOp.elements()\n+                    .mapMulti((codeElement, consumer) -> {\n+                        if (codeElement instanceof JavaOp.InvokeOp invokeOp) {\n+                            if (meetConditionsForMemoryLoadOp(invokeOp)) {\n+                                Op.Result result = invokeOp.result();\n+                                Set<Op.Result> uses = result.uses();\n+                                for (Op.Result use : uses) {\n+                                    if (use.op() instanceof CoreOp.VarOp varOp) {\n+                                        varTable.put(varOp, invokeOp);\n+                                        consumer.accept(invokeOp);\n+                                        consumer.accept(varOp);\n+                                    }\n+                                }\n+                            }\n+                        }\n+                    });\n+\n+            Set<CodeElement<?, ?>> nodesInvolved = memoryLoadOps.collect(Collectors.toSet());\n+            funcOp = OpTk.transform(here, funcOp, (blockBuilder, op) -> {\n+                if (!nodesInvolved.contains(op)) {\n+                    blockBuilder.op(op);\n+                } else if (op instanceof JavaOp.InvokeOp invokeOp) {\n+                    insertHatMemoryLoadOp(blockBuilder, invokeOp);\n+                } else if (op instanceof CoreOp.VarOp varOp) {\n+                    JavaOp.InvokeOp invokeOp = varTable.get(varOp);\n+                    factory(blockBuilder, varOp, invokeOp);\n+                }\n+                return blockBuilder;\n+            });\n+            after(here, funcOp);\n+            return funcOp;\n+        }\n+\n+        private void insertHatMemoryLoadOp(Block.Builder blockBuilder, JavaOp.InvokeOp invokeOp) {\n+            HATMemoryLoadOp loadOp = new HATMemoryLoadOp(invokeOp.resultType(),\n+                    invokeOp.invokeDescriptor().refType(),\n+                    invokeOp.invokeDescriptor().name(),\n+                    blockBuilder.context().getValues(invokeOp.operands()));\n+            Op.Result resultLoad = blockBuilder.op(loadOp);\n+            loadOp.setLocation(invokeOp.location());\n+            blockBuilder.context().mapValue(invokeOp.result(), resultLoad);\n+        }\n+\n+        @Override\n+        protected HATMemoryOp factory(Block.Builder blockBuilder, CoreOp.VarOp varOp, JavaOp.InvokeOp invokeOp) {\n+            HATPrivateVarInitOp privateVarOp = new HATPrivateVarInitOp(varOp.varName(),\n+                    (ClassType) varOp.varValueType(),\n+                    varOp.resultType(),\n+                    invokeOp.invokeDescriptor().refType(),\n+                    blockBuilder.context().getValues(varOp.operands()));\n+            Op.Result op1 = blockBuilder.op(privateVarOp);\n+            privateVarOp.setLocation(varOp.location());\n+            blockBuilder.context().mapValue(varOp.result(), op1);\n+            return privateVarOp;\n+        }\n+    }\n","filename":"hat\/core\/src\/main\/java\/hat\/phases\/HATDialectifyMemoryPhase.java","additions":141,"deletions":29,"binary":false,"changes":170,"status":"modified"},{"patch":"@@ -36,1 +36,1 @@\n-public class HATDialectifyTier implements Function<CoreOp.FuncOp,CoreOp.FuncOp> {\n+public class HATDialectifyTier implements Function<CoreOp.FuncOp, CoreOp.FuncOp> {\n@@ -43,0 +43,1 @@\n+\n@@ -44,2 +45,3 @@\n-        hatPhases.add(new HATDialectifyMemoryPhase.SharedPhase(accelerator));\n-        hatPhases.add(new HATDialectifyMemoryPhase.PrivatePhase(accelerator));\n+        hatPhases.add(new HATDialectifyMemoryPhase.LocalMemoryPhase(accelerator));\n+        hatPhases.add(new HATDialectifyMemoryPhase.PrivateMemoryPhase(accelerator));\n+        hatPhases.add(new HATDialectifyMemoryPhase.DeviceTypePhase(accelerator));\n","filename":"hat\/core\/src\/main\/java\/hat\/phases\/HATDialectifyTier.java","additions":5,"deletions":3,"binary":false,"changes":8,"status":"modified"},{"patch":"@@ -29,0 +29,1 @@\n+import hat.dialect.HATPhaseUtils;\n@@ -113,1 +114,1 @@\n-            interfaces = inspectAllInterfaces(aClass);\n+            interfaces = HATPhaseUtils.inspectAllInterfaces(aClass);\n","filename":"hat\/core\/src\/main\/java\/hat\/phases\/HATDialectifyVectorOpPhase.java","additions":2,"deletions":1,"binary":false,"changes":3,"status":"modified"},{"patch":"@@ -29,0 +29,1 @@\n+import hat.dialect.HATPhaseUtils;\n@@ -79,1 +80,1 @@\n-            interfaces = inspectAllInterfaces(aClass);\n+            interfaces = HATPhaseUtils.inspectAllInterfaces(aClass);\n","filename":"hat\/core\/src\/main\/java\/hat\/phases\/HATDialectifyVectorSelectPhase.java","additions":2,"deletions":1,"binary":false,"changes":3,"status":"modified"},{"patch":"@@ -76,1 +76,1 @@\n-                interfaces = inspectAllInterfaces(aClass);\n+                interfaces = HATPhaseUtils.inspectAllInterfaces(aClass);\n","filename":"hat\/core\/src\/main\/java\/hat\/phases\/HATDialectifyVectorStorePhase.java","additions":1,"deletions":1,"binary":false,"changes":2,"status":"modified"},{"patch":"@@ -32,3 +32,0 @@\n-import hat.annotations.Kernel;\n-import hat.annotations.Preformatted;\n-import hat.buffer.Buffer;\n@@ -40,1 +37,2 @@\n-import hat.ifacemapper.Schema;\n+import hat.device.DeviceSchema;\n+import hat.device.DeviceType;\n@@ -57,1 +55,1 @@\n- *     java @hat\/run ffi-opencl matmul 2D\n+ * java @hat\/run ffi-opencl matmul 2D\n@@ -62,3 +60,3 @@\n- *     <code>\n- *         java @hat\/run ffi-opencl matmul 1D\n- *     <\/code>\n+ * <code>\n+ * java @hat\/run ffi-opencl matmul 1D\n+ * <\/code>\n@@ -117,1 +115,1 @@\n-    private interface MyLocalArrayFixedSize extends Buffer {\n+    private interface MyLocalArrayFixedSize extends DeviceType {\n@@ -119,0 +117,1 @@\n+\n@@ -121,1 +120,1 @@\n-        Schema<MyLocalArrayFixedSize> schema = Schema.of(MyLocalArrayFixedSize.class,\n+        DeviceSchema<MyLocalArrayFixedSize> schema = DeviceSchema.of(MyLocalArrayFixedSize.class,\n@@ -124,1 +123,1 @@\n-                        .array(\"array\", 256));\n+                        .withArray(\"array\", 256));\n@@ -127,1 +126,1 @@\n-            return schema.allocate(accelerator);\n+            return null;\n@@ -131,1 +130,1 @@\n-            return schema.allocate(accelerator);\n+            return null;\n@@ -135,1 +134,1 @@\n-            return schema.allocate(new Accelerator(MethodHandles.lookup(), Backend.FIRST));\n+            return null;\n@@ -157,1 +156,1 @@\n-        for (int tile = 0; tile < (size\/tileSize); tile++) {\n+        for (int tile = 0; tile < (size \/ tileSize); tile++) {\n@@ -181,1 +180,1 @@\n-    private interface SharedMemory extends Buffer {\n+    private interface SharedMemory extends DeviceType {\n@@ -183,0 +182,1 @@\n+\n@@ -184,2 +184,4 @@\n-        Schema<SharedMemory> schema = Schema.of(SharedMemory.class,\n-                arr -> arr.array(\"array\", 1024));\n+\n+        DeviceSchema<SharedMemory> schema = DeviceSchema.of(SharedMemory.class,\n+                arr -> arr.withArray(\"array\", 1024));\n+\n@@ -187,1 +189,1 @@\n-            return schema.allocate(accelerator);\n+            return null;\n@@ -189,0 +191,1 @@\n+\n@@ -190,1 +193,1 @@\n-            return schema.allocate(new Accelerator(MethodHandles.lookup(), Backend.FIRST));\n+            return null;\n@@ -194,1 +197,1 @@\n-    private interface PrivateArray extends Buffer {\n+    private interface PrivateArray extends DeviceType {\n@@ -196,0 +199,1 @@\n+\n@@ -197,2 +201,4 @@\n-        Schema<PrivateArray> schema = Schema.of(PrivateArray.class,\n-                arr -> arr.array(\"array\", 16));\n+\n+        DeviceSchema<PrivateArray> schema = DeviceSchema.of(PrivateArray.class,\n+                arr -> arr.withArray(\"array\", 16));\n+\n@@ -200,1 +206,1 @@\n-            return schema.allocate(accelerator);\n+            return null;\n@@ -202,0 +208,1 @@\n+\n@@ -203,1 +210,1 @@\n-            return schema.allocate(new Accelerator(MethodHandles.lookup(), Backend.FIRST));\n+            return null;\n@@ -207,1 +214,1 @@\n-    private interface FlatPrivate extends Buffer {\n+    private interface FlatPrivate extends DeviceType {\n@@ -209,0 +216,1 @@\n+\n@@ -210,2 +218,4 @@\n-        Schema<FlatPrivate> schema = Schema.of(FlatPrivate.class,\n-                arr -> arr.array(\"array\", 4));\n+\n+        DeviceSchema<FlatPrivate> schema = DeviceSchema.of(FlatPrivate.class,\n+                arr -> arr.withArray(\"array\", 4));\n+\n@@ -213,1 +223,1 @@\n-            return schema.allocate(accelerator);\n+            return null;\n@@ -215,0 +225,1 @@\n+\n@@ -216,1 +227,1 @@\n-            return schema.allocate(new Accelerator(MethodHandles.lookup(), Backend.FIRST));\n+            return null;\n@@ -224,3 +235,3 @@\n-     *     We want to probe that HAT can represent more complex optimisations, and make use of the\n-     *     different levels of the GPU's memory hierarchy, such as shared memory (as in CUDA shared memory),\n-     *     and private memory. This code has been tested on NVIDIA A10 GPUs.\n+     * We want to probe that HAT can represent more complex optimisations, and make use of the\n+     * different levels of the GPU's memory hierarchy, such as shared memory (as in CUDA shared memory),\n+     * and private memory. This code has been tested on NVIDIA A10 GPUs.\n@@ -230,2 +241,2 @@\n-     *     The code has been adapted from CUDA to HAT based on the algorithms presented here:\n-     *     {@url https:\/\/siboehm.com\/articles\/22\/CUDA-MMM}\n+     * The code has been adapted from CUDA to HAT based on the algorithms presented here:\n+     * {@url https:\/\/siboehm.com\/articles\/22\/CUDA-MMM}\n@@ -314,1 +325,1 @@\n-                    regM.array(i,  tileA.array((threadRow * TM + i) * BK + dotIdx));\n+                    regM.array(i, tileA.array((threadRow * TM + i) * BK + dotIdx));\n@@ -317,1 +328,1 @@\n-                    regN.array(i,  tileB.array(dotIdx * BN + threadCol * TN + i));\n+                    regN.array(i, tileB.array(dotIdx * BN + threadCol * TN + i));\n@@ -346,3 +357,3 @@\n-     *     We want to probe that HAT can represent more complex optimisations, and make use of the\n-     *     different levels of the GPU's memory hierarchy, such as shared memory (as in CUDA shared memory),\n-     *     and private memory. This code has been tested on NVIDIA A10 GPUs.\n+     * We want to probe that HAT can represent more complex optimisations, and make use of the\n+     * different levels of the GPU's memory hierarchy, such as shared memory (as in CUDA shared memory),\n+     * and private memory. This code has been tested on NVIDIA A10 GPUs.\n@@ -352,2 +363,2 @@\n-     *     The code has been adapted from CUDA to HAT based on the algorithms presented here:\n-     *     {@url https:\/\/siboehm.com\/articles\/22\/CUDA-MMM}\n+     * The code has been adapted from CUDA to HAT based on the algorithms presented here:\n+     * {@url https:\/\/siboehm.com\/articles\/22\/CUDA-MMM}\n@@ -435,1 +446,1 @@\n-                    regM.array(i,  tileA.array(dotIdx * BM + threadRow * TM + i));\n+                    regM.array(i, tileA.array(dotIdx * BM + threadRow * TM + i));\n@@ -438,1 +449,1 @@\n-                    regN.array(i,  tileB.array(dotIdx * (BN + extraCols) + threadCol * TN + i));\n+                    regN.array(i, tileB.array(dotIdx * (BN + extraCols) + threadCol * TN + i));\n@@ -462,5 +473,7 @@\n-    private interface SharedMemoryHalf extends Buffer {\n-        void array(long index, short value);\n-        short array(long index);\n-        Schema<SharedMemoryHalf> schema = Schema.of(SharedMemoryHalf.class,\n-                arr -> arr.array(\"array\", 1024));\n+    private interface SharedMemoryHalf extends DeviceType {\n+        F16 array(int index);\n+\n+        DeviceSchema<SharedMemoryHalf> schema = DeviceSchema.of(SharedMemoryHalf.class,\n+                arr -> arr.withArray(\"array\", 1024)\n+                        .withDeps(F16.class, half -> half.withField(\"value\")));\n+\n@@ -468,1 +481,1 @@\n-            return schema.allocate(accelerator);\n+            return null;\n@@ -470,0 +483,1 @@\n+\n@@ -471,1 +485,1 @@\n-            return schema.allocate(new Accelerator(MethodHandles.lookup(), Backend.FIRST));\n+            return null;\n@@ -475,5 +489,7 @@\n-    private interface PrivateArrayHalf extends Buffer {\n-        void array(long index, float value);\n-        float array(long index);\n-        Schema<PrivateArrayHalf> schema = Schema.of(PrivateArrayHalf.class,\n-                arr -> arr.array(\"array\", 16));\n+    private interface PrivateArrayHalf extends DeviceType {\n+        F16 array(int index);\n+\n+        DeviceSchema<PrivateArrayHalf> schema = DeviceSchema.of(PrivateArrayHalf.class,\n+                arr -> arr.withArray(\"array\", 16)\n+                        .withDeps(F16.class, half -> half.withField(\"value\")));\n+\n@@ -481,1 +497,1 @@\n-            return schema.allocate(accelerator);\n+            return null;\n@@ -483,0 +499,1 @@\n+\n@@ -484,1 +501,1 @@\n-            return schema.allocate(new Accelerator(MethodHandles.lookup(), Backend.FIRST));\n+            return null;\n@@ -488,5 +505,7 @@\n-    private interface FlatPrivateHalf extends Buffer {\n-        void array(long index, short value);\n-        short array(long index);\n-        Schema<FlatPrivateHalf> schema = Schema.of(FlatPrivateHalf.class,\n-                arr -> arr.array(\"array\", 4));\n+    private interface FlatPrivateHalf extends DeviceType {\n+        F16 array(int index);\n+\n+        DeviceSchema<FlatPrivateHalf> schema = DeviceSchema.of(FlatPrivateHalf.class,\n+                arr -> arr.withArray(\"array\", 4)\n+                        .withDeps(F16.class, half -> half.withField(\"value\")));\n+\n@@ -494,1 +513,1 @@\n-            return schema.allocate(accelerator);\n+            return null;\n@@ -496,0 +515,1 @@\n+\n@@ -497,1 +517,1 @@\n-            return schema.allocate(new Accelerator(MethodHandles.lookup(), Backend.FIRST));\n+            return null;\n@@ -502,94 +522,0 @@\n-    @Preformatted(\"\"\"\n-            typedef struct SharedMemoryHalf_s{\n-                half array[1024];\n-            }SharedMemoryHalf_t;\n-\n-            typedef struct PrivateArrayHalf_s{\n-                float array[16];\n-            }PrivateArrayHalf_t;\n-\n-            typedef struct FlatPrivateHalf_s{\n-                half array[4];\n-            }FlatPrivateHalf_t;\n-            \"\"\")\n-    @Kernel(\"\"\"\n-            HAT_KERNEL void matrixMultiplyKernel2DRegisterTilingHalf(\n-                    HAT_GLOBAL_MEM KernelContext_t* kc,\n-                    HAT_GLOBAL_MEM F16Array_t* matrixA,\n-                    HAT_GLOBAL_MEM F16Array_t* matrixB,\n-                    HAT_GLOBAL_MEM F16Array_t* matrixC,\n-                    int size\n-            ){\n-            const int BM = 64;\n-            const int BN = 64;\n-            const int BK = 16;\n-            const int TM = 4;\n-            const int TN = 4;\n-            const int bx = HAT_BIX;\n-            const int by = HAT_BIY;\n-            const int totalResultsBlockTile = BM*BN;\n-            const int numThreadsBlockTile = totalResultsBlockTile\/(TM*TN);\n-            const int linearLocalId = HAT_LIY*HAT_LSX+HAT_LIX;\n-            const int threadCol = HAT_LIX;\n-            const int threadRow = HAT_LIY;\n-            HAT_LOCAL_MEM SharedMemoryHalf_t tileA;\n-            HAT_LOCAL_MEM SharedMemoryHalf_t tileB;\n-            int aFrom = (by*BM)*size;\n-            int bFrom = bx*BN;\n-            const int v = bx*BN;\n-            const int cFrom = (by*BM)*size+v;\n-            const int innerRowA = linearLocalId\/BK;\n-            const int innerColA = linearLocalId%BK;\n-            const int strideA = numThreadsBlockTile\/BK;\n-            const int innerRowB = linearLocalId\/BN;\n-            const int innerColB = linearLocalId%BN;\n-            const int strideB = numThreadsBlockTile\/BN;\n-                PrivateArrayHalf_t threadResults;\n-                FlatPrivateHalf_t regM;\n-                FlatPrivateHalf_t regN;\n-\n-                for(int i = 0; i<TN*TN; i=i+1){\n-                    half init = (half)0.0;\n-                    threadResults.array[(long)i]=(float)init;\n-                }\n-\n-                for(int bkIdx = 0; bkIdx<size; bkIdx=bkIdx+BK){\n-                    for(int loadOffset = 0; loadOffset<BM; loadOffset=loadOffset+strideA){\n-                        tileA.array[(long)((innerRowA+loadOffset)*BK+innerColA)]=(&matrixA->array)[(long)(((innerRowA+loadOffset)*size+innerColA)+aFrom)]->value;\n-                    }\n-                    for(int loadOffset = 0; loadOffset<BK; loadOffset=loadOffset+strideB){\n-                        tileB.array[(long)((innerRowB+loadOffset)*BN+innerColB)]=(&matrixB->array)[(long)(((innerRowB+loadOffset)*size+innerColB)+bFrom)]->value;\n-                    }\n-                    HAT_BARRIER;\n-                    aFrom=aFrom+BK;\n-                    const int f = BK*size;\n-                    bFrom=bFrom+f;\n-                    for(int dotIdx = 0; dotIdx<BK; dotIdx=dotIdx+1){\n-                        for(int i = 0; i<TM; i=i+1){\n-                            regM.array[(long)i]=tileA.array[(long)((threadRow*TM+i)*BK+dotIdx)];\n-                        }\n-                        for(int i = 0; i<TN; i=i+1){\n-                            regN.array[(long)i]=tileB.array[(long)((dotIdx*BN+threadCol*TN)+i)];\n-                        }\n-                        for(int resIdxM = 0; resIdxM<TM; resIdxM=resIdxM+1){\n-                            for(int resIdxN = 0; resIdxN<TN; resIdxN=resIdxN+1){\n-                                half privA = (half)regM.array[(long)resIdxM];\n-                                half privB = (half)regN.array[(long)resIdxN];\n-                                half mul = (privA * privB);\n-                                half acc = (half)threadResults.array[(long)(resIdxM*TN+resIdxN)];\n-                                acc=(acc + mul);\n-                                threadResults.array[(long)(resIdxM*TN+resIdxN)]=(float)acc;\n-                            }\n-                        }\n-                    }\n-                    HAT_BARRIER;\n-                }\n-                for(int resIdxM = 0; resIdxM<TM; resIdxM=resIdxM+1){\n-                    for(int resIdxN = 0; resIdxN<TN; resIdxN=resIdxN+1){\n-                        half result = (half)threadResults.array[(long)(resIdxM*TN+resIdxN)];\n-                        (&matrixC->array)[(long)((((threadRow*TM+resIdxM)*size+threadCol*TN)+resIdxN)+cFrom)]->value=result;\n-                    }\n-                }\n-                return;\n-    }\n-    \"\"\")\n@@ -641,1 +567,1 @@\n-            threadResults.array(i, init.value());\n+            threadResults.array(i).value(init.value());\n@@ -649,1 +575,2 @@\n-                tileA.array((innerRowA + loadOffset) * BK + innerColA, matrixA.array(((innerRowA + loadOffset) * size + innerColA) + aFrom).value());\n+                F16 ha = matrixA.array(((innerRowA + loadOffset) * size + innerColA) + aFrom);\n+                tileA.array((innerRowA + loadOffset) * BK + innerColA).value(ha.value());\n@@ -654,1 +581,2 @@\n-                tileB.array((innerRowB + loadOffset) * BN + innerColB, matrixB.array(((innerRowB + loadOffset) * size + innerColB) + bFrom).value());\n+                F16 hb = matrixB.array(((innerRowB + loadOffset) * size + innerColB) + bFrom);\n+                tileB.array((innerRowB + loadOffset) * BN + innerColB).value(hb.value());\n@@ -668,1 +596,2 @@\n-                    regM.array(i,  tileA.array((threadRow * TM + i) * BK + dotIdx));\n+                    F16 ha = tileA.array((threadRow * TM + i) * BK + dotIdx);\n+                    regM.array(i).value(ha.value());\n@@ -671,1 +600,2 @@\n-                    regN.array(i,  tileB.array(dotIdx * BN + threadCol * TN + i));\n+                    F16 hb = tileB.array(dotIdx * BN + threadCol * TN + i);\n+                    regN.array(i).value(hb.value());\n@@ -675,2 +605,2 @@\n-                        F16 privA = F16.of(regM.array(resIdxM));\n-                        F16 privB = F16.of(regN.array(resIdxN));\n+                        F16 privA = regM.array(resIdxM);\n+                        F16 privB = regN.array(resIdxN);\n@@ -678,3 +608,3 @@\n-                        F16 acc = F16.of(threadResults.array(resIdxM * TN + resIdxN));\n-                        acc = F16.add(acc, mul);\n-                        threadResults.array((resIdxM * TN + resIdxN), acc.value());\n+                        F16 acc = threadResults.array(resIdxM * TN + resIdxN);\n+                        F16 acc2 = F16.add(acc, mul);   \/\/ FIXME: this is a partial fix until we support expressions such as: acc = acc <OP> val\n+                        threadResults.array((resIdxM * TN + resIdxN)).value(acc2.value());\n@@ -691,1 +621,1 @@\n-                F16 result = F16.of(threadResults.array(resIdxM * TN + resIdxN));\n+                F16 result = threadResults.array(resIdxM * TN + resIdxN);\n@@ -742,1 +672,1 @@\n-    public static void matrixMultiply1D(@RO ComputeContext cc, @RO F32Array matrixA, @RO F32Array matrixB, @RW  F32Array matrixC, int globalSize) {\n+    public static void matrixMultiply1D(@RO ComputeContext cc, @RO F32Array matrixA, @RO F32Array matrixB, @RW F32Array matrixC, int globalSize) {\n@@ -752,1 +682,1 @@\n-    public static void matrixMultiply1DWithFunctionCalls(@RO ComputeContext cc, @RO F32Array matrixA, @RO F32Array matrixB, @RW  F32Array matrixC, int size) {\n+    public static void matrixMultiply1DWithFunctionCalls(@RO ComputeContext cc, @RO F32Array matrixA, @RO F32Array matrixB, @RW F32Array matrixC, int size) {\n@@ -760,1 +690,1 @@\n-    public static void matrixMultiply2D(@RO ComputeContext cc, @RO F32Array matrixA, @RO F32Array matrixB, @RW  F32Array matrixC, int globalSize) {\n+    public static void matrixMultiply2D(@RO ComputeContext cc, @RO F32Array matrixA, @RO F32Array matrixB, @RW F32Array matrixC, int globalSize) {\n@@ -768,1 +698,1 @@\n-    public static void matrixMultiply2DLI(@RO ComputeContext cc, @RO F32Array matrixA, @RO F32Array matrixB, @RW  F32Array matrixC, int globalSize) {\n+    public static void matrixMultiply2DLI(@RO ComputeContext cc, @RO F32Array matrixA, @RO F32Array matrixB, @RW F32Array matrixC, int globalSize) {\n@@ -776,1 +706,1 @@\n-    public static void matrixMultiply2DTiling(@RO ComputeContext cc, @RO F32Array matrixA, @RO F32Array matrixB, @RW  F32Array matrixC, int globalSize) {\n+    public static void matrixMultiply2DTiling(@RO ComputeContext cc, @RO F32Array matrixA, @RO F32Array matrixB, @RW F32Array matrixC, int globalSize) {\n@@ -784,1 +714,1 @@\n-    public static void matrixMultiply2DRegisterTiling(@RO ComputeContext cc, @RO F32Array matrixA, @RO F32Array matrixB, @RW  F32Array matrixC, int globalSize) {\n+    public static void matrixMultiply2DRegisterTiling(@RO ComputeContext cc, @RO F32Array matrixA, @RO F32Array matrixB, @RW F32Array matrixC, int globalSize) {\n@@ -792,1 +722,1 @@\n-    public static void matrixMultiply2DRegisterTilingVectorizedAccesses(@RO ComputeContext cc, @RO F32ArrayPadded matrixA, @RO F32ArrayPadded matrixB, @RW  F32ArrayPadded matrixC, int globalSize) {\n+    public static void matrixMultiply2DRegisterTilingVectorizedAccesses(@RO ComputeContext cc, @RO F32ArrayPadded matrixA, @RO F32ArrayPadded matrixB, @RW F32ArrayPadded matrixC, int globalSize) {\n@@ -800,1 +730,1 @@\n-    public static void matrixMultiply2DRegisterTilingHalf(@RO ComputeContext cc, @RO F16Array matrixA, @RO F16Array matrixB, @RW  F16Array matrixC, int globalSize) {\n+    public static void matrixMultiply2DRegisterTilingHalf(@RO ComputeContext cc, @RO F16Array matrixA, @RO F16Array matrixB, @RW F16Array matrixC, int globalSize) {\n@@ -835,1 +765,1 @@\n-    private static void runSequential(F16Array matrixA, F16Array matrixB, F32Array matrixC, final int size) {\n+    private static void runSequential(F16Array matrixA, F16Array matrixB, F16Array matrixC, final int size) {\n@@ -838,1 +768,1 @@\n-                float sum = 0;\n+                F16 sum = F16.of(0.0f);\n@@ -840,3 +770,3 @@\n-                    float a = F16.f16ToFloat(matrixA.array((long) i * size + k));\n-                    float b = F16.f16ToFloat(matrixB.array((long) k * size + j));\n-                    sum += a * b;\n+                    F16 a = matrixA.array((long) i * size + k);\n+                    F16 b = matrixB.array((long) k * size + j);\n+                    sum = F16.add(sum, F16.mul(a, b));\n@@ -844,1 +774,1 @@\n-                matrixC.array((long) i * size + j, sum);\n+                matrixC.array((long) i * size + j).value(sum.value());\n@@ -867,2 +797,2 @@\n-     * @param args\n-     *      args: <\"1D\"|\"2D\"> for 1D dispatch\n+     *\n+     * @param args args: <\"1D\"|\"2D\"> for 1D dispatch\n@@ -872,2 +802,0 @@\n-        System.out.println(\"[INFO] Running Matrix Multiplication: \");\n-\n@@ -897,0 +825,1 @@\n+        IO.println(\"[INFO] Starting Matrix Multiplication with size: \" + size + \"x\" + size);\n@@ -912,1 +841,0 @@\n-            \/\/matrixC1 = null;\n@@ -933,1 +861,0 @@\n-                \/\/matrixC1 = F32Array.create(accelerator, size * size);\n@@ -936,2 +863,2 @@\n-                    matrixAHalf.array(j).value(F16.floatToF16(r.nextFloat(1)).value());\n-                    matrixBHalf.array(j).value(F16.floatToF16(r.nextFloat(1)).value());\n+                    matrixAHalf.array(j).value(F16.floatToF16(r.nextFloat()).value());\n+                    matrixBHalf.array(j).value(F16.floatToF16(r.nextFloat()).value());\n@@ -943,2 +870,0 @@\n-                \/\/matrixC1 = null;\n-               \/\/ matrixCHalf = null;\n@@ -956,1 +881,7 @@\n-        var resultSeq = F32Array.create(accelerator, size * size);\n+        F32Array resultSeq = null;\n+        F16Array resultSeqHalf = null;\n+        if (configuration == Configuration._2DREGISTER_TILING_FP16) {\n+            resultSeqHalf = F16Array.create(accelerator, size * size);\n+        } else {\n+            resultSeq = F32Array.create(accelerator, size * size);\n+        }\n@@ -962,1 +893,1 @@\n-            runSequential(matrixAHalf, matrixAHalf, resultSeq, size);\n+            runSequential(matrixAHalf, matrixBHalf, resultSeqHalf, size);\n@@ -982,1 +913,1 @@\n-                            matrixMultiply2DRegisterTiling(cc, matrixA, matrixB, matrixC, size));\n+                        matrixMultiply2DRegisterTiling(cc, matrixA, matrixB, matrixC, size));\n@@ -984,1 +915,1 @@\n-                            matrixMultiply2DRegisterTilingVectorizedAccesses(cc, matrixAPad, matrixBPad, matrixCPad, size));\n+                        matrixMultiply2DRegisterTilingVectorizedAccesses(cc, matrixAPad, matrixBPad, matrixCPad, size));\n@@ -999,1 +930,6 @@\n-                        float expectedValue = resultSeq.array(i * size + j);\n+                        float expectedValue;\n+                        if (configuration == Configuration._2DREGISTER_TILING_FP16) {\n+                            expectedValue = F16.f16ToFloat(resultSeqHalf.array(i * size + j));\n+                        } else {\n+                            expectedValue = resultSeq.array(i * size + j);\n+                        }\n@@ -1004,2 +940,1 @@\n-                            \/\/gotValue = matrixC1.array(i * size + j);\n-                            gotValue = Float.float16ToFloat(matrixCHalf.array(i * size + j).value());\n+                            gotValue = F16.f16ToFloat(matrixCHalf.array(i * size + j));\n","filename":"hat\/examples\/matmul\/src\/main\/java\/matmul\/Main.java","additions":136,"deletions":201,"binary":false,"changes":337,"status":"modified"},{"patch":"@@ -42,1 +42,1 @@\n-import static hat.ifacemapper.MappableIface.*;\n+import static hat.ifacemapper.MappableIface.WO;\n","filename":"hat\/tests\/src\/main\/java\/hat\/test\/TestBlackscholes.java","additions":1,"deletions":1,"binary":false,"changes":2,"status":"modified"},{"patch":"@@ -0,0 +1,129 @@\n+\/*\n+ * Copyright (c) 2025, Oracle and\/or its affiliates. All rights reserved.\n+ * DO NOT ALTER OR REMOVE COPYRIGHT NOTICES OR THIS FILE HEADER.\n+ *\n+ * This code is free software; you can redistribute it and\/or modify it\n+ * under the terms of the GNU General Public License version 2 only, as\n+ * published by the Free Software Foundation.  Oracle designates this\n+ * particular file as subject to the \"Classpath\" exception as provided\n+ * by Oracle in the LICENSE file that accompanied this code.\n+ *\n+ * This code is distributed in the hope that it will be useful, but WITHOUT\n+ * ANY WARRANTY; without even the implied warranty of MERCHANTABILITY or\n+ * FITNESS FOR A PARTICULAR PURPOSE.  See the GNU General Public License\n+ * version 2 for more details (a copy is included in the LICENSE file that\n+ * accompanied this code).\n+ *\n+ * You should have received a copy of the GNU General Public License version\n+ * 2 along with this work; if not, write to the Free Software Foundation,\n+ * Inc., 51 Franklin St, Fifth Floor, Boston, MA 02110-1301 USA.\n+ *\n+ * Please contact Oracle, 500 Oracle Parkway, Redwood Shores, CA 94065 USA\n+ * or visit www.oracle.com if you need additional information or have any\n+ * questions.\n+ *\/\n+package hat.test;\n+\n+import hat.buffer.F16;\n+import hat.device.DeviceSchema;\n+import hat.device.DeviceType;\n+import hat.test.annotation.HatTest;\n+import hat.test.engine.HATAsserts;\n+\n+public class TestDeviceType {\n+\n+    public interface MyDeviceArray extends DeviceType {\n+        F16 array(int index);\n+        void array(int index, F16 value);\n+\n+        void x(float x);\n+        float x();\n+\n+        DeviceSchema<MyDeviceArray> schema = DeviceSchema.of(MyDeviceArray.class, builder ->\n+                builder.withArray(\"array\", 2048)\n+                        .withDeps(F16.class, half -> half.withField(\"value\"))\n+                        .withField(\"x\"));\n+\n+        static MyDeviceArray create() {\n+            return null;\n+        }\n+    }\n+\n+    \/\/ This test checks the representation of the prev. array\n+    @HatTest\n+    public void testdevice_type_01() {\n+        MyDeviceArray myDeviceArray = MyDeviceArray.create();\n+        String text = MyDeviceArray.schema.toText();\n+        boolean isEquals = text.equals(\"<hat.buffer.F16:s:half:value;><hat.test.TestDeviceType$MyDeviceArray:[:hat.buffer.F16:array:2048;s:float:x;>\");\n+        HATAsserts.assertTrue(isEquals);\n+    }\n+\n+\n+    \/\/ A way to construct 2D arrays\n+    public interface MyNDRAnge extends DeviceType {\n+        SubRange array(int index);\n+        void array(int index, SubRange value);\n+\n+        interface SubRange {\n+            int range();\n+            void range(int index, int val);\n+        }\n+\n+        DeviceSchema<MyNDRAnge> schema = DeviceSchema.of(MyNDRAnge.class, builder ->\n+                builder.withArray(\"array\", 2048)\n+                        .withDeps(SubRange.class, subrange -> subrange.withArray(\"range\", 64)));\n+\n+        static MyNDRAnge create() {\n+            return null;\n+        }\n+    }\n+\n+    @HatTest\n+    public void testdevice_type_02() {\n+        MyNDRAnge myDeviceArray = MyNDRAnge.create();\n+        String text = MyNDRAnge.schema.toText();\n+        boolean isEquals = text.equals(\"<hat.test.TestDeviceType$MyNDRAnge$SubRange:[:int:range:64;><hat.test.TestDeviceType$MyNDRAnge:[:hat.test.TestDeviceType$MyNDRAnge$SubRange:array:2048;>\");\n+        HATAsserts.assertTrue(isEquals);\n+    }\n+\n+    \/\/ A way to construct 3D arrays\n+    public interface MultiDim extends DeviceType {\n+        _2D array(int index);\n+        void array(int index, _2D value);\n+\n+        interface _2D {\n+            _3D _range2(int index);\n+            void _range2(int index, _3D val);\n+\n+            interface _3D {\n+                int value(int index);\n+                void value(int index, int val);\n+            }\n+        }\n+\n+        DeviceSchema<MultiDim> schema = DeviceSchema.of(MultiDim.class, builder ->\n+                builder.withArray(\"array\", 2048)\n+                        .withDeps(_2D.class,subrange -> subrange.withArray(\"range2\", 64)\n+                                                                                       .withDeps(_2D._3D.class, f -> f.withArray(\"value\", 32))));\n+\n+        static MultiDim create() {\n+            return null;\n+        }\n+    }\n+\n+    @HatTest\n+    public void testdevice_type_03() {\n+        \/\/ This test is expected to fail. It request a member called \"range2\" from the _2D class.\n+        \/\/ However, the method name is \"_range2\". Thus the requested method doen't exits.\n+        try {\n+            MultiDim myDeviceArray = MultiDim.create();\n+            String text = MultiDim.schema.toText();\n+            \/\/ If we request the correct method, the result should be as follows:\n+            boolean isEquals = text.equals(\"<hat.test.TestDeviceType$MultiDim$_2D$_3D:[:int:value:32;><hat.test.TestDeviceType$MultiDim$_2D:[:hat.test.TestDeviceType$MultiDim$_2D$_3D:_range2:64;><hat.test.TestDeviceType$MultiDim:[:hat.test.TestDeviceType$MultiDim$_2D:array:2048;>\");\n+            HATAsserts.assertFalse(isEquals);\n+        } catch (ExceptionInInitializerError e) {\n+            HATAsserts.assertTrue(true);\n+        }\n+    }\n+\n+}\n","filename":"hat\/tests\/src\/main\/java\/hat\/test\/TestDeviceType.java","additions":129,"deletions":0,"binary":false,"changes":129,"status":"added"},{"patch":"@@ -32,1 +32,0 @@\n-import hat.buffer.Buffer;\n@@ -35,0 +34,2 @@\n+import hat.device.DeviceSchema;\n+import hat.device.DeviceType;\n@@ -37,1 +38,0 @@\n-import hat.ifacemapper.Schema;\n@@ -145,3 +145,3 @@\n-    interface MyLocalArray extends Buffer {\n-        void array(long index, F16 value);\n-        F16 array(long index);\n+    public interface DeviceLocalArray extends DeviceType {\n+        F16 array(int index);\n+        \/\/void array(int index, F16 value);\n@@ -149,2 +149,3 @@\n-        Schema<MyLocalArray> schema = Schema.of(MyLocalArray.class,\n-                        arr -> arr.array(\"array\", 1024));\n+        DeviceSchema<DeviceLocalArray> schema = DeviceSchema.of(DeviceLocalArray.class,\n+builder -> builder.withArray(\"array\", 1024)\n+                        .withDeps(F16.class, half -> half.withField(\"value\")));\n@@ -152,2 +153,2 @@\n-        static MyLocalArray create(Accelerator accelerator) {\n-            return schema.allocate(accelerator);\n+        static DeviceLocalArray create(Accelerator accelerator) {\n+            return null;\n@@ -156,2 +157,2 @@\n-        static MyLocalArray createLocal() {\n-            return schema.allocate(new Accelerator(MethodHandles.lookup(), Backend.FIRST));\n+        static DeviceLocalArray createLocal() {\n+            return null;\n@@ -163,1 +164,1 @@\n-        MyLocalArray sm = MyLocalArray.createLocal();\n+        DeviceLocalArray sm = DeviceLocalArray.createLocal();\n@@ -169,1 +170,1 @@\n-            sm.array(lix, ha);\n+            sm.array(lix).value(ha.value());\n@@ -210,0 +211,92 @@\n+    interface DevicePrivateArray extends DeviceType {\n+        F16 array(int index);\n+        \/\/void array(int index, F16 value);\n+\n+        DeviceSchema<DevicePrivateArray> schema = DeviceSchema.of(DevicePrivateArray.class,\n+                builder -> builder.withArray(\"array\", 1024)\n+                        .withDeps(F16.class, half -> half.withField(\"value\")));\n+\n+        static DevicePrivateArray create(Accelerator accelerator) {\n+            return null;\n+        }\n+\n+        static DevicePrivateArray createPrivate() {\n+            return null;\n+        }\n+    }\n+\n+    @CodeReflection\n+    public static void f16Ops_15(@RO KernelContext kernelContext, @RO F16Array a, @RW F16Array b) {\n+        DevicePrivateArray privateArray = DevicePrivateArray.createPrivate();\n+        if (kernelContext.gix < kernelContext.gsx) {\n+            int lix = kernelContext.lix;\n+            F16 ha = a.array(kernelContext.gix);\n+\n+            \/\/ store into the private object\n+            privateArray.array(lix).value(ha.value());\n+\n+            F16 hb = privateArray.array(lix);\n+            b.array(kernelContext.gix).value(hb.value());\n+        }\n+    }\n+\n+    interface DevicePrivateArray2 extends DeviceType {\n+        F16 array(int index);\n+        void array(int index, F16 value);\n+\n+        DeviceSchema<DevicePrivateArray2> schema = DeviceSchema.of(DevicePrivateArray2.class,\n+                builder -> builder.withArray(\"array\", 1024)\n+                        .withDeps(F16.class, half -> half.withField(\"value\")));\n+\n+        static DevicePrivateArray2 create(Accelerator accelerator) {\n+            return null;\n+        }\n+\n+        static DevicePrivateArray2 createPrivate() {\n+            return null;\n+        }\n+    }\n+\n+    @CodeReflection\n+    public static void f16Ops_16(@RO KernelContext kernelContext, @RO F16Array a, @RW F16Array b) {\n+        DevicePrivateArray2 privateArray = DevicePrivateArray2.createPrivate();\n+        if (kernelContext.gix < kernelContext.gsx) {\n+            int lix = kernelContext.lix;\n+            F16 ha = a.array(kernelContext.gix);\n+\n+            \/\/ This is expected to fail on the GPU due to the assigment of different types.\n+            \/\/ ha is a typed F16Impl, which is a subtype of F16.\n+            \/\/ While in Java, this is correct, because F16Impl is an implementation of F16,\n+            \/\/ the GPU code is not aware of this inheritance, then we end up assigning values\n+            \/\/ from different types.\n+            privateArray.array(lix, ha);\n+\n+            F16 hb = privateArray.array(lix);\n+            b.array(kernelContext.gix).value(hb.value());\n+        }\n+    }\n+\n+    @CodeReflection\n+    public static void f16Ops_17(@RO KernelContext kernelContext, @RW F16Array a) {\n+        F16 ha = a.array(0);\n+        F16 hre = F16.add(ha, ha);\n+        hre = F16.add(hre, hre);\n+        a.array(0).value(hre.value());\n+    }\n+\n+    @CodeReflection\n+    public static void f16Ops_18(@RO KernelContext kernelContext, @RW F16Array a) {\n+\n+        F16 ha = a.array(0);\n+        DevicePrivateArray2 privateArray = DevicePrivateArray2.createPrivate();\n+        privateArray.array(0).value(ha.value());\n+\n+        \/\/ Obtain the value from private memory\n+        F16 acc = privateArray.array(0);\n+\n+        \/\/ compute\n+        acc = F16.add(acc, acc);\n+\n+        \/\/ store the result\n+        a.array(0).value(acc.value());\n+    }\n@@ -289,0 +382,24 @@\n+    @CodeReflection\n+    public static void compute15(@RO ComputeContext computeContext, @RO F16Array a, @RW F16Array b) {\n+        NDRange ndRange = NDRange.of(NDRange.Global1D.of(a.length()), NDRange.Local1D.of(16));\n+        computeContext.dispatchKernel(ndRange, kernelContext -> TestF16Type.f16Ops_15(kernelContext, a, b));\n+    }\n+\n+    @CodeReflection\n+    public static void compute16(@RO ComputeContext computeContext, @RO F16Array a, @RW F16Array b) {\n+        NDRange ndRange = NDRange.of(NDRange.Global1D.of(a.length()), NDRange.Local1D.of(16));\n+        computeContext.dispatchKernel(ndRange, kernelContext -> TestF16Type.f16Ops_16(kernelContext, a, b));\n+    }\n+\n+    @CodeReflection\n+    public static void compute17(@RO ComputeContext computeContext, @RW F16Array a) {\n+        NDRange ndRange = NDRange.of(1);\n+        computeContext.dispatchKernel(ndRange, kernelContext -> TestF16Type.f16Ops_17(kernelContext, a));\n+    }\n+\n+    @CodeReflection\n+    public static void compute18(@RO ComputeContext computeContext, @RW F16Array a) {\n+        NDRange ndRange = NDRange.of(1);\n+        computeContext.dispatchKernel(ndRange, kernelContext -> TestF16Type.f16Ops_18(kernelContext, a));\n+    }\n+\n@@ -523,8 +640,1 @@\n-        try {\n-            accelerator.compute(computeContext -> TestF16Type.compute11(computeContext, arrayA, arrayB));\n-        } catch (Throwable e) {\n-            \/\/ We expect this to fail since it is unsupported at the moment,\n-            IO.println(\"-------------------\");\n-            IO.println(e.getMessage());\n-            throw new HATExpectedFailureException(\"Expected to fail due to unsupported use of F16 in local and private memory\");\n-        }\n+        accelerator.compute(computeContext -> TestF16Type.compute11(computeContext, arrayA, arrayB));\n@@ -603,0 +713,74 @@\n+    @HatTest\n+    public void testF16_15() {\n+        var accelerator = new Accelerator(MethodHandles.lookup(), Backend.FIRST);\n+        final int size = 256;\n+        F16Array arrayA = F16Array.create(accelerator, size);\n+        F16Array arrayB = F16Array.create(accelerator, size);\n+\n+        Random r = new Random(73);\n+        for (int i = 0; i < arrayA.length(); i++) {\n+            arrayA.array(i).value(F16.floatToF16(r.nextFloat()).value());\n+        }\n+\n+        accelerator.compute(computeContext -> TestF16Type.compute15(computeContext, arrayA, arrayB));\n+\n+        for (int i = 0; i < arrayB.length(); i++) {\n+            F16 val = arrayB.array(i);\n+            HATAsserts.assertEquals(arrayA.array(i).value(), val.value());\n+        }\n+    }\n+\n+    \/\/@HatTest\n+    public void testF16_16() {\n+        var accelerator = new Accelerator(MethodHandles.lookup(), Backend.FIRST);\n+        final int size = 256;\n+        F16Array arrayA = F16Array.create(accelerator, size);\n+        F16Array arrayB = F16Array.create(accelerator, size);\n+\n+        Random r = new Random(73);\n+        for (int i = 0; i < arrayA.length(); i++) {\n+            arrayA.array(i).value(F16.floatToF16(r.nextFloat()).value());\n+        }\n+\n+        try {\n+            accelerator.compute(computeContext -> TestF16Type.compute16(computeContext, arrayA, arrayB));\n+        } catch (RuntimeException e) {\n+            throw new HATExpectedFailureException(\"Incompatible types in expression `privateArray.array(lix, ha);`\");\n+        }\n+\n+        for (int i = 0; i < arrayB.length(); i++) {\n+            F16 val = arrayB.array(i);\n+            HATAsserts.assertEquals(arrayA.array(i).value(), val.value());\n+        }\n+    }\n+\n+    @HatTest\n+    public void testF16_17() {\n+        var accelerator = new Accelerator(MethodHandles.lookup(), Backend.FIRST);\n+        final int size = 1;\n+        F16Array arrayA = F16Array.create(accelerator, size);\n+\n+        Random r = new Random(73);\n+        arrayA.array(0).value(F16.floatToF16(10).value());\n+\n+        accelerator.compute(computeContext -> TestF16Type.compute17(computeContext, arrayA));\n+\n+        F16 val = arrayA.array(0);\n+        HATAsserts.assertEquals(40.0f, F16.f16ToFloat(val), 0.01f);\n+    }\n+\n+    @HatTest\n+    public void testF16_18() {\n+        var accelerator = new Accelerator(MethodHandles.lookup(), Backend.FIRST);\n+        final int size = 1;\n+        F16Array arrayA = F16Array.create(accelerator, size);\n+\n+        Random r = new Random(73);\n+        arrayA.array(0).value(F16.floatToF16(10).value());\n+\n+        accelerator.compute(computeContext -> TestF16Type.compute18(computeContext, arrayA));\n+\n+        F16 val = arrayA.array(0);\n+        HATAsserts.assertEquals(20.0f, F16.f16ToFloat(val), 0.01f);\n+    }\n+\n","filename":"hat\/tests\/src\/main\/java\/hat\/test\/TestF16Type.java","additions":205,"deletions":21,"binary":false,"changes":226,"status":"modified"},{"patch":"@@ -32,1 +32,0 @@\n-import hat.buffer.Buffer;\n@@ -35,0 +34,2 @@\n+import hat.device.DeviceSchema;\n+import hat.device.DeviceType;\n@@ -37,1 +38,0 @@\n-import hat.ifacemapper.Schema;\n@@ -158,1 +158,1 @@\n-    private interface SharedArray extends Buffer {\n+    private interface SharedArray extends DeviceType {\n@@ -161,2 +161,2 @@\n-        Schema<SharedArray> schema = Schema.of(SharedArray.class,\n-                arr -> arr.array(\"array\", 1024));\n+        DeviceSchema<SharedArray> schema = DeviceSchema.of(SharedArray.class,\n+                arr -> arr.withArray(\"array\", 1024));\n@@ -164,1 +164,1 @@\n-            return schema.allocate(accelerator);\n+            return null;\n@@ -167,1 +167,1 @@\n-            return schema.allocate(new Accelerator(MethodHandles.lookup(), Backend.FIRST));\n+            return null;\n@@ -190,1 +190,1 @@\n-    private interface PrivateMemory extends Buffer {\n+    private interface PrivateMemory extends DeviceType {\n@@ -193,2 +193,2 @@\n-        Schema<PrivateMemory> schema = Schema.of(PrivateMemory.class,\n-                arr -> arr.array(\"array\", 4));\n+        DeviceSchema<PrivateMemory> schema = DeviceSchema.of(PrivateMemory.class,\n+                arr -> arr.withArray(\"array\", 4));\n@@ -196,1 +196,1 @@\n-            return schema.allocate(accelerator);\n+            return null;\n@@ -199,1 +199,1 @@\n-            return schema.allocate(new Accelerator(MethodHandles.lookup(), Backend.FIRST));\n+            return null;\n","filename":"hat\/tests\/src\/main\/java\/hat\/test\/TestFloat2.java","additions":12,"deletions":12,"binary":false,"changes":24,"status":"modified"},{"patch":"@@ -34,0 +34,2 @@\n+import hat.device.DeviceSchema;\n+import hat.device.DeviceType;\n@@ -44,1 +46,1 @@\n-    private interface MySharedArray extends Buffer {\n+    private interface MySharedArray extends DeviceType {\n@@ -48,3 +50,2 @@\n-        Schema<MySharedArray> schema = Schema.of(MySharedArray.class,\n-                myPrivateArray -> myPrivateArray\n-                        .array(\"array\", 16));\n+        DeviceSchema<MySharedArray> schema = DeviceSchema.of(MySharedArray.class,\n+                builder -> builder.withArray(\"array\", 16));\n@@ -53,1 +54,1 @@\n-            return schema.allocate(accelerator, 1);\n+            return null;\n@@ -61,1 +62,0 @@\n-\n","filename":"hat\/tests\/src\/main\/java\/hat\/test\/TestLocal.java","additions":6,"deletions":6,"binary":false,"changes":12,"status":"modified"},{"patch":"@@ -32,1 +32,0 @@\n-import hat.buffer.Buffer;\n@@ -38,1 +37,2 @@\n-import hat.ifacemapper.Schema;\n+import hat.device.DeviceSchema;\n+import hat.device.DeviceType;\n@@ -43,1 +43,0 @@\n-import java.lang.invoke.MethodHandles;\n@@ -96,1 +95,1 @@\n-    private interface MyLocalArrayFixedSize extends Buffer {\n+    private interface MyLocalArrayFixedSize extends DeviceType {\n@@ -100,1 +99,1 @@\n-        Schema<MyLocalArrayFixedSize> schema = Schema.of(MyLocalArrayFixedSize.class,\n+        DeviceSchema<MyLocalArrayFixedSize> schema = DeviceSchema.of(MyLocalArrayFixedSize.class,\n@@ -102,2 +101,1 @@\n-                        \/\/ It is a bound schema, so we fix the size here\n-                        .array(\"array\", 256));\n+                        .withArray(\"array\", 256));\n@@ -106,5 +104,1 @@\n-            return schema.allocate(accelerator);\n-        }\n-\n-        static MyLocalArrayFixedSize createLocal(Accelerator accelerator) {\n-            return schema.allocate(accelerator);\n+            return null;\n@@ -114,1 +108,1 @@\n-            return schema.allocate(new Accelerator(MethodHandles.lookup(), Backend.FIRST));\n+            return null;\n@@ -492,1 +486,1 @@\n-    private interface SharedMemory extends Buffer {\n+    private interface SharedMemory extends DeviceType {\n@@ -495,2 +489,2 @@\n-        Schema<SharedMemory> schema = Schema.of(SharedMemory.class,\n-                arr -> arr.array(\"array\", 1024));\n+        DeviceSchema<SharedMemory> schema = DeviceSchema.of(SharedMemory.class,\n+                arr -> arr.withArray(\"array\", 1024));\n@@ -498,1 +492,1 @@\n-            return schema.allocate(accelerator);\n+            return null;\n@@ -501,1 +495,1 @@\n-            return schema.allocate(new Accelerator(MethodHandles.lookup(), Backend.FIRST));\n+            return null;\n@@ -503,2 +497,1 @@\n-\n-         default void storeFloat4View(Float4 float4, int index) {\n+        default void storeFloat4View(Float4 float4, int index) {\n@@ -508,1 +501,1 @@\n-    private interface PrivateArray extends Buffer {\n+    private interface PrivateArray extends DeviceType {\n@@ -511,2 +504,2 @@\n-        Schema<PrivateArray> schema = Schema.of(PrivateArray.class,\n-                arr -> arr.array(\"array\", 16));\n+        DeviceSchema<PrivateArray> schema = DeviceSchema.of(PrivateArray.class,\n+                arr -> arr.withArray(\"array\", 16));\n@@ -514,1 +507,1 @@\n-            return schema.allocate(accelerator);\n+            return null;\n@@ -517,1 +510,1 @@\n-            return schema.allocate(new Accelerator(MethodHandles.lookup(), Backend.FIRST));\n+            return null;\n@@ -521,1 +514,1 @@\n-    private interface FlatPrivate extends Buffer {\n+    private interface FlatPrivate extends DeviceType {\n@@ -524,2 +517,2 @@\n-        Schema<FlatPrivate> schema = Schema.of(FlatPrivate.class,\n-                arr -> arr.array(\"array\", 4));\n+        DeviceSchema<FlatPrivate> schema = DeviceSchema.of(FlatPrivate.class,\n+                arr -> arr.withArray(\"array\", 4));\n@@ -527,1 +520,1 @@\n-            return schema.allocate(accelerator);\n+            return null;\n@@ -530,1 +523,1 @@\n-            return schema.allocate(new Accelerator(MethodHandles.lookup(), Backend.FIRST));\n+            return null;\n@@ -753,1 +746,0 @@\n-\n@@ -821,0 +813,181 @@\n+\n+    private interface SharedMemoryHalf extends DeviceType {\n+        F16 array(int index);\n+\n+        DeviceSchema<SharedMemoryHalf> schema = DeviceSchema.of(SharedMemoryHalf.class,\n+                arr -> arr.withArray(\"array\", 1024)\n+                        .withDeps(F16.class, half -> half.withField(\"value\")));\n+\n+        static SharedMemoryHalf create(Accelerator accelerator) {\n+            return null;\n+        }\n+\n+        static SharedMemoryHalf createLocal() {\n+            return null;\n+        }\n+    }\n+\n+    private interface PrivateArrayHalf extends DeviceType {\n+        F16 array(int index);\n+\n+        DeviceSchema<PrivateArrayHalf> schema = DeviceSchema.of(PrivateArrayHalf.class,\n+                arr -> arr.withArray(\"array\", 16)\n+                        .withDeps(F16.class, half -> half.withField(\"value\")));\n+\n+        static PrivateArrayHalf create(Accelerator accelerator) {\n+            return null;\n+        }\n+\n+        static PrivateArrayHalf createPrivate() {\n+            return null;\n+        }\n+    }\n+\n+    private interface FlatPrivateHalf extends DeviceType {\n+        F16 array(int index);\n+\n+        DeviceSchema<FlatPrivateHalf> schema = DeviceSchema.of(FlatPrivateHalf.class,\n+                arr -> arr.withArray(\"array\", 4)\n+                        .withDeps(F16.class, half -> half.withField(\"value\")));\n+\n+        static FlatPrivateHalf create(Accelerator accelerator) {\n+            return null;\n+        }\n+\n+        static FlatPrivateHalf createPrivate() {\n+            return null;\n+        }\n+    }\n+\n+    \/\/ Taking from the HAT Examples module\n+    @CodeReflection\n+    public static void matrixMultiplyKernel2DRegisterTilingHalf(@RO KernelContext kc, @RO F16Array matrixA, @RO F16Array matrixB, @RW F16Array matrixC, int size) {\n+        final int BM = 64;\n+        final int BN = 64;\n+        final int BK = 16;\n+        final int TM = 4;\n+        final int TN = 4;\n+\n+        int bx = kc.bix;\n+        int by = kc.biy;\n+\n+        int totalResultsBlockTile = BM * BN;\n+        final int numThreadsBlockTile = totalResultsBlockTile \/ (TM * TN);\n+\n+        final int linearLocalId = kc.liy * kc.lsx + kc.lix;\n+        final int threadCol = kc.lix;\n+        final int threadRow = kc.liy;\n+\n+        SharedMemoryHalf tileA = SharedMemoryHalf.createLocal();\n+        SharedMemoryHalf tileB = SharedMemoryHalf.createLocal();\n+\n+        int aFrom = by * BM * size;\n+        int bFrom = bx * BN;\n+        int v = bx * BN;\n+        int cFrom = (by * BM * size) + (v);\n+\n+        final int innerRowA = linearLocalId \/ BK;\n+        final int innerColA = linearLocalId % BK;\n+\n+        final int strideA = numThreadsBlockTile \/ BK;\n+        final int innerRowB = linearLocalId \/ BN;\n+        final int innerColB = linearLocalId % BN;\n+\n+        int strideB = numThreadsBlockTile \/ BN;\n+\n+        PrivateArrayHalf threadResults = PrivateArrayHalf.createPrivate();\n+        FlatPrivateHalf regM = FlatPrivateHalf.createPrivate();\n+        FlatPrivateHalf regN = FlatPrivateHalf.createPrivate();\n+\n+        for (int i = 0; i < (TN * TN); i++) {\n+            F16 init = F16.of(0.0f);\n+            threadResults.array(i).value(init.value());\n+        }\n+\n+        for (int bkIdx = 0; bkIdx < size; bkIdx += BK) {\n+            for (int loadOffset = 0; loadOffset < BM; loadOffset += strideA) {\n+                F16 ha = matrixA.array(((innerRowA + loadOffset) * size + innerColA) + aFrom);\n+                tileA.array((innerRowA + loadOffset) * BK + innerColA).value(ha.value());\n+            }\n+            for (int loadOffset = 0; loadOffset < BK; loadOffset += strideB) {\n+                F16 hb = matrixB.array(((innerRowB + loadOffset) * size + innerColB) + bFrom);\n+                tileB.array((innerRowB + loadOffset) * BN + innerColB).value(hb.value());\n+            }\n+            kc.barrier();\n+\n+            aFrom += (BK);\n+            int f = BK * size;\n+            bFrom += f;\n+\n+            for (int dotIdx = 0; dotIdx < BK; dotIdx++) {\n+                for (int i = 0; i < TM; i++) {\n+                    F16 ha = tileA.array((threadRow * TM + i) * BK + dotIdx);\n+                    regM.array(i).value(ha.value());\n+                }\n+                for (int i = 0; i < TN; i++) {\n+                    F16 hb = tileB.array(dotIdx * BN + threadCol * TN + i);\n+                    regN.array(i).value(hb.value());\n+                }\n+                for (int resIdxM = 0; resIdxM < TM; resIdxM++) {\n+                    for (int resIdxN = 0; resIdxN < TN; resIdxN++) {\n+                        F16 privA = regM.array(resIdxM);\n+                        F16 privB = regN.array(resIdxN);\n+                        F16 mul = F16.mul(privA, privB);\n+                        F16 acc = threadResults.array(resIdxM * TN + resIdxN);\n+                        F16 acc2 = F16.add(acc, mul);   \/\/ FIXME: this is a partial fix until we support expressions such as: acc = acc <OP> val\n+                        threadResults.array((resIdxM * TN + resIdxN)).value(acc2.value());\n+                    }\n+                }\n+            }\n+            kc.barrier();\n+        }\n+        for (int resIdxM = 0; resIdxM < TM; resIdxM++) {\n+            for (int resIdxN = 0; resIdxN < TN; resIdxN++) {\n+                F16 result = threadResults.array(resIdxM * TN + resIdxN);\n+                matrixC.array((((threadRow * TM + resIdxM) * size + threadCol * TN + resIdxN) + (cFrom))).value(result.value());\n+            }\n+        }\n+    }\n+\n+    @CodeReflection\n+    public static void matrixMultiply2DRegisterTilingHalf(@RO ComputeContext cc, @RO F16Array matrixA, @RO F16Array matrixB, @RW F16Array matrixC, int globalSize) {\n+        NDRange ndRange = NDRange.of(NDRange.Global2D.of(256, 256), NDRange.Local2D.of(16, 16));\n+        cc.dispatchKernel(ndRange,\n+                kc -> matrixMultiplyKernel2DRegisterTilingHalf(kc, matrixA, matrixB, matrixC, globalSize)\n+        );\n+    }\n+\n+    @HatTest\n+    public void matrixMultiply2DRegisterTilingHalf() {\n+        var lookup = java.lang.invoke.MethodHandles.lookup();\n+        var accelerator = new Accelerator(lookup, Backend.FIRST);\n+\n+        final int size = 1024;\n+        var matrixA = F16Array.create(accelerator, size * size);\n+        var matrixB = F16Array.create(accelerator, size * size);\n+\n+        \/\/ Matrix for the results\n+        var matrixC = F16Array.create(accelerator, size * size);\n+        var resultSeq = F16Array.create(accelerator, size * size);\n+\n+        \/\/ Initialize matrices (A and B have the same size)\n+        Random r = new Random(19);\n+        for (int j = 0; j < matrixA.length(); j++) {\n+            matrixA.array(j).value(F16.floatToF16(r.nextFloat()).value());\n+            matrixB.array(j).value(F16.floatToF16(r.nextFloat()).value());\n+        }\n+\n+        accelerator.compute(cc ->\n+                TestMatMul.matrixMultiply2DRegisterTilingHalf(cc, matrixA, matrixB, matrixC, size));\n+\n+        \/\/ Run Seq for reference\n+        runSequential(matrixA, matrixB, resultSeq, size);\n+\n+        for (int i = 0; i < size; i++) {\n+            for (int j = 0; j < size; j++) {\n+                HATAsserts.assertEquals(F16.f16ToFloat(resultSeq.array(i * size + j)),\n+                                        F16.f16ToFloat(matrixC.array(i * size + j)),\n+                                        0.01f);\n+            }\n+        }\n+    }\n","filename":"hat\/tests\/src\/main\/java\/hat\/test\/TestMatMul.java","additions":204,"deletions":31,"binary":false,"changes":235,"status":"modified"},{"patch":"@@ -40,1 +40,1 @@\n-import static hat.ifacemapper.MappableIface.*;\n+import static hat.ifacemapper.MappableIface.RW;\n","filename":"hat\/tests\/src\/main\/java\/hat\/test\/TestParenthesis.java","additions":1,"deletions":1,"binary":false,"changes":2,"status":"modified"},{"patch":"@@ -34,0 +34,2 @@\n+import hat.device.DeviceSchema;\n+import hat.device.DeviceType;\n@@ -46,1 +48,1 @@\n-    private interface PrivateArray extends Buffer {\n+    private interface PrivateArray extends DeviceType {\n@@ -50,1 +52,1 @@\n-        Schema<PrivateArray> schema = Schema.of(PrivateArray.class,\n+        DeviceSchema<PrivateArray> schema = DeviceSchema.of(PrivateArray.class,\n@@ -52,1 +54,1 @@\n-                        .array(\"array\", 1));\n+                        .withArray(\"array\", 1));\n@@ -55,1 +57,1 @@\n-            return schema.allocate(accelerator, 1);\n+            return null;\n","filename":"hat\/tests\/src\/main\/java\/hat\/test\/TestPrivate.java","additions":6,"deletions":4,"binary":false,"changes":10,"status":"modified"},{"patch":"@@ -34,0 +34,2 @@\n+import hat.device.DeviceSchema;\n+import hat.device.DeviceType;\n@@ -44,1 +46,1 @@\n-    private interface MySharedArray extends Buffer {\n+    private interface MySharedArray extends DeviceType {\n@@ -48,3 +50,2 @@\n-        Schema<MySharedArray> schema = Schema.of(MySharedArray.class,\n-                myPrivateArray -> myPrivateArray\n-                        .array(\"array\", 16));\n+        DeviceSchema<MySharedArray> schema = DeviceSchema.of(MySharedArray.class,\n+                builder -> builder.withArray(\"array\", 16));\n@@ -53,1 +54,1 @@\n-            return schema.allocate(accelerator, 1);\n+            return null;\n@@ -57,1 +58,1 @@\n-            return create(new Accelerator(MethodHandles.lookup(), Backend.FIRST));\n+            return null;\n","filename":"hat\/tests\/src\/main\/java\/hat\/test\/TestReductions.java","additions":7,"deletions":6,"binary":false,"changes":13,"status":"modified"},{"patch":"@@ -32,1 +32,0 @@\n-import hat.buffer.Buffer;\n@@ -35,0 +34,2 @@\n+import hat.device.DeviceSchema;\n+import hat.device.DeviceType;\n@@ -37,1 +38,0 @@\n-import hat.ifacemapper.Schema;\n@@ -162,1 +162,1 @@\n-    private interface SharedMemory extends Buffer {\n+    private interface SharedMemory extends DeviceType {\n@@ -165,2 +165,2 @@\n-        Schema<SharedMemory> schema = Schema.of(SharedMemory.class,\n-                arr -> arr.array(\"array\", 1024));\n+        DeviceSchema<SharedMemory> schema = DeviceSchema.of(SharedMemory.class,\n+                arr -> arr.withArray(\"array\", 1024));\n@@ -168,1 +168,1 @@\n-            return schema.allocate(accelerator);\n+            return null;\n@@ -171,1 +171,1 @@\n-            return schema.allocate(new Accelerator(MethodHandles.lookup(), Backend.FIRST));\n+            return null;\n@@ -194,1 +194,1 @@\n-    private interface PrivateMemory extends Buffer {\n+    private interface PrivateMemory extends DeviceType {\n@@ -197,2 +197,2 @@\n-        Schema<PrivateMemory> schema = Schema.of(PrivateMemory.class,\n-                arr -> arr.array(\"array\", 4));\n+        DeviceSchema<PrivateMemory> schema = DeviceSchema.of(PrivateMemory.class,\n+                arr -> arr.withArray(\"array\", 4));\n@@ -200,1 +200,1 @@\n-            return schema.allocate(accelerator);\n+            return null;\n@@ -203,1 +203,1 @@\n-            return schema.allocate(new Accelerator(MethodHandles.lookup(), Backend.FIRST));\n+            return null;\n","filename":"hat\/tests\/src\/main\/java\/hat\/test\/TestVectorTypes.java","additions":12,"deletions":12,"binary":false,"changes":24,"status":"modified"},{"patch":"@@ -91,0 +91,6 @@\n+\n+    public static void assertFalse(boolean isCorrect) {\n+        if (isCorrect) {\n+            throw new HATAssertionError(\"Expected: \" + isCorrect);\n+        }\n+    }\n","filename":"hat\/tests\/src\/main\/java\/hat\/test\/engine\/HATAsserts.java","additions":6,"deletions":0,"binary":false,"changes":6,"status":"modified"},{"patch":"@@ -32,0 +32,2 @@\n+import hat.dialect.HATMemoryLoadOp;\n+import hat.dialect.HATPrivateVarInitOp;\n@@ -228,0 +230,12 @@\n+    @Override\n+    public T hatPrivateVarInitOp(ScopedCodeBuilderContext builderContext, HATPrivateVarInitOp hatPrivateVarInitOp) {\n+        blockComment(\"Private Var Init Op Not Implemented\");\n+        return self();\n+    }\n+\n+    @Override\n+    public T hatMemoryLoadOp(ScopedCodeBuilderContext builderContext, HATMemoryLoadOp hatMemoryLoadOp) {\n+        blockComment(\"Memory Load Op Not Implemented\");\n+        return self();\n+    }\n+\n","filename":"hat\/tools\/src\/main\/java\/hat\/tools\/text\/JavaHATCodeBuilder.java","additions":14,"deletions":0,"binary":false,"changes":14,"status":"modified"}]}