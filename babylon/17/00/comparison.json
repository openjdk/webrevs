{"files":[{"patch":"@@ -0,0 +1,2 @@\n+\/.idea\/\n+\/target\/\n","filename":"cr-examples\/triton\/.gitignore","additions":2,"deletions":0,"binary":false,"changes":2,"status":"added"},{"patch":"@@ -0,0 +1,3 @@\n+Example using code reflection with a Java-based Triton programming model\n+inspired by Triton and its Python programming model, see\n+https:\/\/openai.com\/research\/triton.\n\\ No newline at end of file\n","filename":"cr-examples\/triton\/README.md","additions":3,"deletions":0,"binary":false,"changes":3,"status":"added"},{"patch":"@@ -0,0 +1,83 @@\n+<?xml version=\"1.0\" encoding=\"UTF-8\"?>\n+<!--\n+Copyright (c) 2024, Oracle and\/or its affiliates. All rights reserved.\n+DO NOT ALTER OR REMOVE COPYRIGHT NOTICES OR THIS FILE HEADER.\n+\n+This code is free software; you can redistribute it and\/or modify it\n+under the terms of the GNU General Public License version 2 only, as\n+published by the Free Software Foundation.  Oracle designates this\n+particular file as subject to the \"Classpath\" exception as provided\n+by Oracle in the LICENSE file that accompanied this code.\n+\n+This code is distributed in the hope that it will be useful, but WITHOUT\n+ANY WARRANTY; without even the implied warranty of MERCHANTABILITY or\n+FITNESS FOR A PARTICULAR PURPOSE.  See the GNU General Public License\n+version 2 for more details (a copy is included in the LICENSE file that\n+accompanied this code).\n+\n+You should have received a copy of the GNU General Public License version\n+2 along with this work; if not, write to the Free Software Foundation,\n+Inc., 51 Franklin St, Fifth Floor, Boston, MA 02110-1301 USA.\n+\n+Please contact Oracle, 500 Oracle Parkway, Redwood Shores, CA 94065 USA\n+or visit www.oracle.com if you need additional information or have any\n+questions.\n+-->\n+<project xmlns=\"http:\/\/maven.apache.org\/POM\/4.0.0\"\n+         xmlns:xsi=\"http:\/\/www.w3.org\/2001\/XMLSchema-instance\"\n+         xsi:schemaLocation=\"http:\/\/maven.apache.org\/POM\/4.0.0 http:\/\/maven.apache.org\/xsd\/maven-4.0.0.xsd\">\n+    <modelVersion>4.0.0<\/modelVersion>\n+\n+    <groupId>oracle.code<\/groupId>\n+    <artifactId>triton<\/artifactId>\n+    <version>1.0-SNAPSHOT<\/version>\n+\n+    <properties>\n+        <project.build.sourceEncoding>UTF-8<\/project.build.sourceEncoding>\n+        <maven.compiler.source>23<\/maven.compiler.source>\n+        <maven.compiler.target>23<\/maven.compiler.target>\n+    <\/properties>\n+\n+    <dependencies>\n+        <dependency>\n+            <groupId>org.junit.jupiter<\/groupId>\n+            <artifactId>junit-jupiter-engine<\/artifactId>\n+            <version>5.10.0<\/version>\n+            <scope>test<\/scope>\n+        <\/dependency>\n+    <\/dependencies>\n+\n+    <build>\n+        <pluginManagement>\n+            <plugins>\n+                <plugin>\n+                    <groupId>org.apache.maven.plugins<\/groupId>\n+                    <artifactId>maven-surefire-plugin<\/artifactId>\n+                    <version>3.1.2<\/version>\n+                    <configuration>\n+                        <argLine>--enable-preview\n+                        <\/argLine>\n+                    <\/configuration>\n+                <\/plugin>\n+            <\/plugins>\n+        <\/pluginManagement>\n+        <plugins>\n+            <plugin>\n+                <groupId>org.apache.maven.plugins<\/groupId>\n+                <artifactId>maven-compiler-plugin<\/artifactId>\n+                <version>3.11.0<\/version>\n+                <configuration>\n+                    <compilerArgs>\n+                        <arg>--enable-preview<\/arg>\n+                    <\/compilerArgs>\n+                    <source>${maven.compiler.source}<\/source>\n+                    <target>${maven.compiler.target}<\/target>\n+                    <showDeprecation>true<\/showDeprecation>\n+                    <failOnError>true<\/failOnError>\n+                    <showWarnings>true<\/showWarnings>\n+                    <showDeprecation>true<\/showDeprecation>\n+                <\/configuration>\n+            <\/plugin>\n+        <\/plugins>\n+    <\/build>\n+<\/project>\n","filename":"cr-examples\/triton\/pom.xml","additions":83,"deletions":0,"binary":false,"changes":83,"status":"added"},{"patch":"@@ -0,0 +1,556 @@\n+\/*\n+ * Copyright (c) 2024, Oracle and\/or its affiliates. All rights reserved.\n+ * DO NOT ALTER OR REMOVE COPYRIGHT NOTICES OR THIS FILE HEADER.\n+ *\n+ * This code is free software; you can redistribute it and\/or modify it\n+ * under the terms of the GNU General Public License version 2 only, as\n+ * published by the Free Software Foundation.  Oracle designates this\n+ * particular file as subject to the \"Classpath\" exception as provided\n+ * by Oracle in the LICENSE file that accompanied this code.\n+ *\n+ * This code is distributed in the hope that it will be useful, but WITHOUT\n+ * ANY WARRANTY; without even the implied warranty of MERCHANTABILITY or\n+ * FITNESS FOR A PARTICULAR PURPOSE.  See the GNU General Public License\n+ * version 2 for more details (a copy is included in the LICENSE file that\n+ * accompanied this code).\n+ *\n+ * You should have received a copy of the GNU General Public License version\n+ * 2 along with this work; if not, write to the Free Software Foundation,\n+ * Inc., 51 Franklin St, Fifth Floor, Boston, MA 02110-1301 USA.\n+ *\n+ * Please contact Oracle, 500 Oracle Parkway, Redwood Shores, CA 94065 USA\n+ * or visit www.oracle.com if you need additional information or have any\n+ * questions.\n+ *\/\n+\n+package oracle.code.triton;\n+\n+import java.lang.reflect.code.CopyContext;\n+import java.lang.reflect.code.Op;\n+import java.lang.reflect.code.OpTransformer;\n+import java.lang.reflect.code.Value;\n+import java.lang.reflect.code.descriptor.TypeDesc;\n+import java.lang.reflect.code.op.OpDeclaration;\n+import java.lang.reflect.code.op.OpDefinition;\n+import java.lang.reflect.code.op.OpFactory;\n+import java.lang.reflect.code.op.OpWithDefinition;\n+import java.util.HashMap;\n+import java.util.List;\n+import java.util.Map;\n+\n+public class ArithMathOps {\n+\n+    static abstract class ArithMathOp extends OpWithDefinition {\n+        final TypeDesc resultType;\n+\n+        public ArithMathOp(OpDefinition def) {\n+            super(def);\n+\n+            this.resultType = def.resultType();\n+        }\n+\n+        ArithMathOp(ArithMathOp that, CopyContext cc) {\n+            super(that, cc);\n+\n+            this.resultType = that.resultType;\n+        }\n+\n+        ArithMathOp(String name, TypeDesc resultType, List<? extends Value> operands) {\n+            super(name, operands);\n+\n+            this.resultType = resultType;\n+        }\n+\n+        @Override\n+        public TypeDesc resultType() {\n+            return resultType;\n+        }\n+    }\n+\n+    @OpDeclaration(ConstantOp.NAME)\n+    public static class ConstantOp extends ArithMathOp implements Op.Pure {\n+        public static final String NAME = \"arith.constant\";\n+        public static final String ATTRIBUTE_CONSTANT_VALUE = NAME + \".value\";\n+\n+        final Object value;\n+\n+        public static ConstantOp create(OpDefinition def) {\n+            if (!def.operands().isEmpty()) {\n+                throw new IllegalArgumentException(\"Operation must have zero operands\");\n+            }\n+\n+            Object value = def.extractAttributeValue(ATTRIBUTE_CONSTANT_VALUE,true,\n+                    v -> processConstantValue(def.resultType(), v));\n+            return new ConstantOp(def, value);\n+        }\n+\n+        static Object processConstantValue(TypeDesc t, Object value) {\n+            if (t.equals(TypeDesc.BOOLEAN)) {\n+                if (value instanceof String s) {\n+                    return Boolean.valueOf(s);\n+                } else if (value instanceof Boolean) {\n+                    return value;\n+                }\n+            } else if (t.equals(TypeDesc.BYTE)) {\n+                if (value instanceof String s) {\n+                    return Byte.valueOf(s);\n+                } else if (value instanceof Number n) {\n+                    return n.byteValue();\n+                }\n+            } else if (t.equals(TypeDesc.SHORT)) {\n+                if (value instanceof String s) {\n+                    return Short.valueOf(s);\n+                } else if (value instanceof Number n) {\n+                    return n.shortValue();\n+                }\n+            } else if (t.equals(TypeDesc.CHAR)) {\n+                if (value instanceof String s) {\n+                    return s.charAt(0);\n+                } else if (value instanceof Character) {\n+                    return value;\n+                }\n+            } else if (t.equals(TypeDesc.INT)) {\n+                if (value instanceof String s) {\n+                    return Integer.valueOf(s);\n+                } else if (value instanceof Number n) {\n+                    return n.intValue();\n+                }\n+            } else if (t.equals(TypeDesc.LONG)) {\n+                if (value instanceof String s) {\n+                    return Long.valueOf(s);\n+                } else if (value instanceof Number n) {\n+                    return n.longValue();\n+                }\n+            } else if (t.equals(TypeDesc.FLOAT)) {\n+                if (value instanceof String s) {\n+                    return Float.valueOf(s);\n+                } else if (value instanceof Number n) {\n+                    return n.floatValue();\n+                }\n+            } else if (t.equals(TypeDesc.DOUBLE)) {\n+                if (value instanceof String s) {\n+                    return Double.valueOf(s);\n+                } else if (value instanceof Number n) {\n+                    return n.doubleValue();\n+                }\n+            } else if (t.rawType().equals(TritonOps.TYPE_Tensor)) {\n+                List<TypeDesc> typeDescs = t.typeArguments();\n+                return processConstantValue(typeDescs.getLast(), value);\n+            }\n+\n+            throw new UnsupportedOperationException(\"Unsupported constant type and value: \" + t + \" \" + value);\n+        }\n+\n+        ConstantOp(OpDefinition def, Object value) {\n+            super(def);\n+\n+            this.value = value;\n+        }\n+\n+        ConstantOp(ConstantOp that, CopyContext cc) {\n+            super(that, cc);\n+\n+            this.value = that.value;\n+        }\n+\n+        @Override\n+        public ConstantOp transform(CopyContext cc, OpTransformer ot) {\n+            return new ConstantOp(this, cc);\n+        }\n+\n+        ConstantOp(TypeDesc type, Object value) {\n+            super(NAME, type, List.of());\n+\n+            this.value = value;\n+        }\n+\n+        @Override\n+        public Map<String, Object> attributes() {\n+            HashMap<String, Object> attrs = new HashMap<>(super.attributes());\n+            attrs.put(\"\", value);\n+            return attrs;\n+        }\n+\n+        public Object value() {\n+            return value;\n+        }\n+    }\n+\n+    @OpDeclaration(AddOp.NAME)\n+    public static class AddOp extends ArithMathOp implements Op.Pure {\n+        public static final String NAME = \"arith.add\";\n+\n+        public AddOp(OpDefinition def) {\n+            super(def);\n+        }\n+\n+        AddOp(AddOp that, CopyContext cc) {\n+            super(that, cc);\n+        }\n+\n+        @Override\n+        public AddOp transform(CopyContext cc, OpTransformer ot) {\n+            return new AddOp(this, cc);\n+        }\n+\n+        AddOp(Value a, Value b) {\n+            super(NAME + nameSuffixFromType(a.type(), false), a.type(), List.of(a, b));\n+        }\n+    }\n+\n+    @OpDeclaration(SubOp.NAME)\n+    public static class SubOp extends ArithMathOp implements Op.Pure {\n+        public static final String NAME = \"arith.sub\";\n+\n+        public SubOp(OpDefinition def) {\n+            super(def);\n+        }\n+\n+        SubOp(SubOp that, CopyContext cc) {\n+            super(that, cc);\n+        }\n+\n+        @Override\n+        public SubOp transform(CopyContext cc, OpTransformer ot) {\n+            return new SubOp(this, cc);\n+        }\n+\n+        SubOp(Value a, Value b) {\n+            super(NAME + nameSuffixFromType(a.type(), false), a.type(), List.of(a, b));\n+        }\n+    }\n+\n+    @OpDeclaration(MulOp.NAME)\n+    public static class MulOp extends ArithMathOp implements Op.Pure {\n+        public static final String NAME = \"arith.mul\";\n+\n+        public MulOp(OpDefinition def) {\n+            super(def);\n+        }\n+\n+        MulOp(MulOp that, CopyContext cc) {\n+            super(that, cc);\n+        }\n+\n+        @Override\n+        public MulOp transform(CopyContext cc, OpTransformer ot) {\n+            return new MulOp(this, cc);\n+        }\n+\n+        MulOp(Value a, Value b) {\n+            super(NAME + nameSuffixFromType(a.type(), false), a.type(), List.of(a, b));\n+        }\n+    }\n+\n+    @OpDeclaration(DivOp.NAME)\n+    public static class DivOp extends ArithMathOp implements Op.Pure {\n+        public static final String NAME = \"arith.div\";\n+\n+        public DivOp(OpDefinition def) {\n+            super(def);\n+        }\n+\n+        DivOp(DivOp that, CopyContext cc) {\n+            super(that, cc);\n+        }\n+\n+        @Override\n+        public DivOp transform(CopyContext cc, OpTransformer ot) {\n+            return new DivOp(this, cc);\n+        }\n+\n+        DivOp(Value a, Value b) {\n+            super(NAME + nameSuffixFromType(a.type(), true), a.type(), List.of(a, b));\n+        }\n+    }\n+\n+    @OpDeclaration(RemOp.NAME)\n+    public static class RemOp extends ArithMathOp implements Op.Pure {\n+        public static final String NAME = \"arith.rem\";\n+\n+        public RemOp(OpDefinition def) {\n+            super(def);\n+        }\n+\n+        RemOp(RemOp that, CopyContext cc) {\n+            super(that, cc);\n+        }\n+\n+        @Override\n+        public RemOp transform(CopyContext cc, OpTransformer ot) {\n+            return new RemOp(this, cc);\n+        }\n+\n+        RemOp(Value a, Value b) {\n+            super(NAME + nameSuffixFromType(a.type(), true), a.type(), List.of(a, b));\n+        }\n+    }\n+\n+    @OpDeclaration(AndOp.NAME)\n+    public static class AndOp extends ArithMathOp implements Op.Pure {\n+        public static final String NAME = \"arith.andi\";\n+\n+        public AndOp(OpDefinition def) {\n+            super(def);\n+        }\n+\n+        AndOp(AndOp that, CopyContext cc) {\n+            super(that, cc);\n+        }\n+\n+        @Override\n+        public AndOp transform(CopyContext cc, OpTransformer ot) {\n+            return new AndOp(this, cc);\n+        }\n+\n+        AndOp(Value a, Value b) {\n+            super(NAME, a.type(), List.of(a, b));\n+        }\n+    }\n+\n+    @OpDeclaration(MaxOp.NAME)\n+    public static class MaxOp extends ArithMathOp implements Op.Pure {\n+        public static final String NAME = \"arith.max\";\n+\n+        public MaxOp(OpDefinition def) {\n+            super(def);\n+        }\n+\n+        MaxOp(MaxOp that, CopyContext cc) {\n+            super(that, cc);\n+        }\n+\n+        @Override\n+        public MaxOp transform(CopyContext cc, OpTransformer ot) {\n+            return new MaxOp(this, cc);\n+        }\n+\n+        MaxOp(Value a, Value b) {\n+            super(NAME + maxMinSuffixFromType(a.type()) + nameSuffixFromType(a.type(), true),\n+                    a.type(), List.of(a, b));\n+        }\n+    }\n+\n+    @OpDeclaration(MinOp.NAME)\n+    public static class MinOp extends ArithMathOp implements Op.Pure {\n+        public static final String NAME = \"arith.min\";\n+\n+        public MinOp(OpDefinition def) {\n+            super(def);\n+        }\n+\n+        MinOp(MinOp that, CopyContext cc) {\n+            super(that, cc);\n+        }\n+\n+        @Override\n+        public MinOp transform(CopyContext cc, OpTransformer ot) {\n+            return new MinOp(this, cc);\n+        }\n+\n+        MinOp(Value a, Value b) {\n+            super(NAME + maxMinSuffixFromType(a.type()) + nameSuffixFromType(a.type(), true),\n+                    a.type(), List.of(a, b));\n+        }\n+    }\n+\n+    @OpDeclaration(ExpOp.NAME)\n+    public static class TruncOp extends ArithMathOp implements Op.Pure {\n+        public static final String NAME = \"arith.trunc\";\n+\n+        public TruncOp(OpDefinition def) {\n+            super(def);\n+        }\n+\n+        TruncOp(TruncOp that, CopyContext cc) {\n+            super(that, cc);\n+        }\n+\n+        @Override\n+        public TruncOp transform(CopyContext cc, OpTransformer ot) {\n+            return new TruncOp(this, cc);\n+        }\n+\n+        TruncOp(TypeDesc t, Value a) {\n+            super(NAME + nameSuffixFromType(a.type(), false),\n+                    t, List.of(a));\n+        }\n+    }\n+\n+    @OpDeclaration(ExpOp.NAME)\n+    public static class ExpOp extends ArithMathOp implements Op.Pure {\n+        public static final String NAME = \"math.exp\";\n+\n+        public ExpOp(OpDefinition def) {\n+            super(def);\n+        }\n+\n+        ExpOp(ExpOp that, CopyContext cc) {\n+            super(that, cc);\n+        }\n+\n+        @Override\n+        public ExpOp transform(CopyContext cc, OpTransformer ot) {\n+            return new ExpOp(this, cc);\n+        }\n+\n+        ExpOp(Value a) {\n+            super(NAME, a.type(), List.of(a));\n+        }\n+    }\n+\n+    @OpDeclaration(CompareOp.NAME)\n+    public static class CompareOp extends ArithMathOp implements Op.Pure {\n+        public static final String NAME = \"arith.cmp\";\n+        public static final String ATTRIBUTE_CONSTANT_VALUE = NAME + \".compare\";\n+\n+        public enum CompareKind {\n+            slt\n+        }\n+\n+        final CompareKind ck;\n+\n+        public static CompareOp create(OpDefinition def) {\n+            CompareKind ck = def.extractAttributeValue(ATTRIBUTE_CONSTANT_VALUE, true,\n+                    v -> switch (v) {\n+                        case String s -> CompareKind.valueOf(s);\n+                        case CompareKind k -> k;\n+                        default -> throw new UnsupportedOperationException(\"Unsupported start value:\" + v);\n+                    });\n+            return new CompareOp(def, ck);\n+        }\n+\n+        CompareOp(OpDefinition def, CompareKind ck) {\n+            super(def);\n+\n+            this.ck = ck;\n+        }\n+\n+        CompareOp(CompareOp that, CopyContext cc) {\n+            super(that, cc);\n+\n+            this.ck = that.ck;\n+        }\n+\n+        @Override\n+        public CompareOp transform(CopyContext cc, OpTransformer ot) {\n+            return new CompareOp(this, cc);\n+        }\n+\n+        CompareOp(CompareKind ck, Value a, Value b) {\n+            super(NAME + nameSuffixFromType(a.type(), false), a.type(), List.of(a, b));\n+\n+            this.ck = ck;\n+        }\n+\n+        @Override\n+        public Map<String, Object> attributes() {\n+            HashMap<String, Object> attrs = new HashMap<>(super.attributes());\n+            attrs.put(\"\", ck);\n+            return attrs;\n+        }\n+\n+        public CompareKind kind() {\n+            return ck;\n+        }\n+    }\n+\n+    static String maxMinSuffixFromType(TypeDesc t) {\n+        if (t.rawType().equals(TritonOps.TYPE_Tensor)) {\n+            return maxMinSuffixFromType(t.typeArguments().getLast());\n+        } else if (t.rawType().equals(TritonOps.TYPE_Ptr)) {\n+            return maxMinSuffixFromType(t.typeArguments().getFirst());\n+        } else if (TypeDesc.INT.equals(t)) {\n+            return \"\";\n+        } else if (TypeDesc.FLOAT.equals(t)) {\n+            return \"imum\";\n+        } else {\n+            throw new UnsupportedOperationException(\"Unsupported type: \" + t);\n+        }\n+    }\n+\n+    static String nameSuffixFromType(TypeDesc t, boolean signed) {\n+        if (t.rawType().equals(TritonOps.TYPE_Tensor)) {\n+            return nameSuffixFromType(t.typeArguments().getLast(), signed);\n+        } else if (t.rawType().equals(TritonOps.TYPE_Ptr)) {\n+            return nameSuffixFromType(t.typeArguments().getFirst(), signed);\n+        } else if (TypeDesc.INT.equals(t) || TypeDesc.LONG.equals(t)) {\n+            return (signed ? \"s\" : \"\") + \"i\";\n+        } else if (TypeDesc.FLOAT.equals(t) || TypeDesc.DOUBLE.equals(t) ||\n+                Float16.FLOAT_16_TYPE.equals(t)) {\n+            return \"f\";\n+        } else {\n+            throw new UnsupportedOperationException(\"Unsupported type: \" + t);\n+        }\n+    }\n+\n+    public static final OpFactory FACTORY = def -> {\n+        return switch (def.name()) {\n+            case ConstantOp.NAME -> ConstantOp.create(def);\n+            case ExpOp.NAME -> new ExpOp(def);\n+            case AddOp.NAME + \"i\", AddOp.NAME + \"f\" -> new AddOp(def);\n+            case SubOp.NAME + \"i\", SubOp.NAME + \"f\" -> new SubOp(def);\n+            case MulOp.NAME + \"i\", MulOp.NAME + \"f\" -> new MulOp(def);\n+            case DivOp.NAME + \"si\", DivOp.NAME + \"f\" -> new DivOp(def);\n+            case RemOp.NAME + \"si\", RemOp.NAME + \"f\" -> new DivOp(def);\n+            case AndOp.NAME -> new AndOp(def);\n+            case MaxOp.NAME + \"si\", MaxOp.NAME + \"imumf\" -> new MaxOp(def);\n+            case MinOp.NAME + \"si\", MinOp.NAME + \"imumf\" -> new MinOp(def);\n+            case TruncOp.NAME + \"i\", TruncOp.NAME + \"f\" -> new TruncOp(def);\n+            case CompareOp.NAME + \"i\", CompareOp.NAME + \"f\" -> CompareOp.create(def);\n+            default -> null;\n+        };\n+    };\n+\n+    \/\/ Arith\n+\n+    public static ConstantOp constant(TypeDesc type, Object value) {\n+        return new ConstantOp(type, value);\n+    }\n+\n+    public static MulOp mul(Value a, Value b) {\n+        return new MulOp(a, b);\n+    }\n+\n+    public static AddOp add(Value a, Value b) {\n+        return new AddOp(a, b);\n+    }\n+\n+    public static SubOp sub(Value a, Value b) {\n+        return new SubOp(a, b);\n+    }\n+\n+    public static DivOp div(Value a, Value b) {\n+        return new DivOp(a, b);\n+    }\n+\n+    public static RemOp rem(Value a, Value b) {\n+        return new RemOp(a, b);\n+    }\n+\n+    public static AndOp and(Value a, Value b) {\n+        return new AndOp(a, b);\n+    }\n+\n+    public static MaxOp maximum(Value a, Value b) {\n+        return new MaxOp(a, b);\n+    }\n+\n+    public static MinOp minimum(Value a, Value b) {\n+        return new MinOp(a, b);\n+    }\n+\n+    public static CompareOp cmp(CompareOp.CompareKind ck, Value a, Value b) {\n+        return new CompareOp(ck, a, b);\n+    }\n+\n+    public static TruncOp trunc(TypeDesc type, Value a) {\n+        return new TruncOp(type, a);\n+    }\n+\n+    \/\/ Math\n+\n+    public static ExpOp exp(Value a) {\n+        return new ExpOp(a);\n+    }\n+}\n","filename":"cr-examples\/triton\/src\/main\/java\/oracle\/code\/triton\/ArithMathOps.java","additions":556,"deletions":0,"binary":false,"changes":556,"status":"added"},{"patch":"@@ -0,0 +1,36 @@\n+\/*\n+ * Copyright (c) 2024, Oracle and\/or its affiliates. All rights reserved.\n+ * DO NOT ALTER OR REMOVE COPYRIGHT NOTICES OR THIS FILE HEADER.\n+ *\n+ * This code is free software; you can redistribute it and\/or modify it\n+ * under the terms of the GNU General Public License version 2 only, as\n+ * published by the Free Software Foundation.  Oracle designates this\n+ * particular file as subject to the \"Classpath\" exception as provided\n+ * by Oracle in the LICENSE file that accompanied this code.\n+ *\n+ * This code is distributed in the hope that it will be useful, but WITHOUT\n+ * ANY WARRANTY; without even the implied warranty of MERCHANTABILITY or\n+ * FITNESS FOR A PARTICULAR PURPOSE.  See the GNU General Public License\n+ * version 2 for more details (a copy is included in the LICENSE file that\n+ * accompanied this code).\n+ *\n+ * You should have received a copy of the GNU General Public License version\n+ * 2 along with this work; if not, write to the Free Software Foundation,\n+ * Inc., 51 Franklin St, Fifth Floor, Boston, MA 02110-1301 USA.\n+ *\n+ * Please contact Oracle, 500 Oracle Parkway, Redwood Shores, CA 94065 USA\n+ * or visit www.oracle.com if you need additional information or have any\n+ * questions.\n+ *\/\n+\n+package oracle.code.triton;\n+\n+import java.lang.annotation.ElementType;\n+import java.lang.annotation.Retention;\n+import java.lang.annotation.RetentionPolicy;\n+import java.lang.annotation.Target;\n+\n+@Retention(RetentionPolicy.RUNTIME)\n+@Target(ElementType.PARAMETER)\n+public @interface Constant {\n+}\n","filename":"cr-examples\/triton\/src\/main\/java\/oracle\/code\/triton\/Constant.java","additions":36,"deletions":0,"binary":false,"changes":36,"status":"added"},{"patch":"@@ -0,0 +1,71 @@\n+\/*\n+ * Copyright (c) 2024, Oracle and\/or its affiliates. All rights reserved.\n+ * DO NOT ALTER OR REMOVE COPYRIGHT NOTICES OR THIS FILE HEADER.\n+ *\n+ * This code is free software; you can redistribute it and\/or modify it\n+ * under the terms of the GNU General Public License version 2 only, as\n+ * published by the Free Software Foundation.  Oracle designates this\n+ * particular file as subject to the \"Classpath\" exception as provided\n+ * by Oracle in the LICENSE file that accompanied this code.\n+ *\n+ * This code is distributed in the hope that it will be useful, but WITHOUT\n+ * ANY WARRANTY; without even the implied warranty of MERCHANTABILITY or\n+ * FITNESS FOR A PARTICULAR PURPOSE.  See the GNU General Public License\n+ * version 2 for more details (a copy is included in the LICENSE file that\n+ * accompanied this code).\n+ *\n+ * You should have received a copy of the GNU General Public License version\n+ * 2 along with this work; if not, write to the Free Software Foundation,\n+ * Inc., 51 Franklin St, Fifth Floor, Boston, MA 02110-1301 USA.\n+ *\n+ * Please contact Oracle, 500 Oracle Parkway, Redwood Shores, CA 94065 USA\n+ * or visit www.oracle.com if you need additional information or have any\n+ * questions.\n+ *\/\n+\n+package oracle.code.triton;\n+\n+import java.lang.reflect.Type;\n+import java.lang.reflect.code.descriptor.TypeDesc;\n+import java.util.Objects;\n+\n+public final class ConstantType extends TritonType {\n+    final Type cType;\n+    final Object value;\n+\n+    public ConstantType(Type cType, Object value) {\n+        this.cType = cType;\n+        this.value = value;\n+    }\n+\n+    public Type cType() {\n+        return cType;\n+    }\n+\n+    public Object value() {\n+        return value;\n+    }\n+\n+    @Override\n+    public TypeDesc toDesc() {\n+        return fromType(cType);\n+    }\n+\n+    @Override\n+    public boolean equals(Object o) {\n+        if (this == o) return true;\n+        if (o == null || getClass() != o.getClass()) return false;\n+        ConstantType that = (ConstantType) o;\n+        return Objects.equals(cType, that.cType) && Objects.equals(value, that.value);\n+    }\n+\n+    @Override\n+    public int hashCode() {\n+        return Objects.hash(cType, value);\n+    }\n+\n+    @Override\n+    public String toString() {\n+        return value.toString();\n+    }\n+}\n","filename":"cr-examples\/triton\/src\/main\/java\/oracle\/code\/triton\/ConstantType.java","additions":71,"deletions":0,"binary":false,"changes":71,"status":"added"},{"patch":"@@ -0,0 +1,36 @@\n+\/*\n+ * Copyright (c) 2024, Oracle and\/or its affiliates. All rights reserved.\n+ * DO NOT ALTER OR REMOVE COPYRIGHT NOTICES OR THIS FILE HEADER.\n+ *\n+ * This code is free software; you can redistribute it and\/or modify it\n+ * under the terms of the GNU General Public License version 2 only, as\n+ * published by the Free Software Foundation.  Oracle designates this\n+ * particular file as subject to the \"Classpath\" exception as provided\n+ * by Oracle in the LICENSE file that accompanied this code.\n+ *\n+ * This code is distributed in the hope that it will be useful, but WITHOUT\n+ * ANY WARRANTY; without even the implied warranty of MERCHANTABILITY or\n+ * FITNESS FOR A PARTICULAR PURPOSE.  See the GNU General Public License\n+ * version 2 for more details (a copy is included in the LICENSE file that\n+ * accompanied this code).\n+ *\n+ * You should have received a copy of the GNU General Public License version\n+ * 2 along with this work; if not, write to the Free Software Foundation,\n+ * Inc., 51 Franklin St, Fifth Floor, Boston, MA 02110-1301 USA.\n+ *\n+ * Please contact Oracle, 500 Oracle Parkway, Redwood Shores, CA 94065 USA\n+ * or visit www.oracle.com if you need additional information or have any\n+ * questions.\n+ *\/\n+\n+package oracle.code.triton;\n+\n+import java.lang.reflect.code.descriptor.TypeDesc;\n+\n+\/\/ Fake float 16 type\n+public final class Float16 extends TritonNumber {\n+    static final TypeDesc FLOAT_16_TYPE = TypeDesc.type(Float16.class);\n+\n+    private Float16() {\n+    }\n+}\n","filename":"cr-examples\/triton\/src\/main\/java\/oracle\/code\/triton\/Float16.java","additions":36,"deletions":0,"binary":false,"changes":36,"status":"added"},{"patch":"@@ -0,0 +1,62 @@\n+\/*\n+ * Copyright (c) 2024, Oracle and\/or its affiliates. All rights reserved.\n+ * DO NOT ALTER OR REMOVE COPYRIGHT NOTICES OR THIS FILE HEADER.\n+ *\n+ * This code is free software; you can redistribute it and\/or modify it\n+ * under the terms of the GNU General Public License version 2 only, as\n+ * published by the Free Software Foundation.  Oracle designates this\n+ * particular file as subject to the \"Classpath\" exception as provided\n+ * by Oracle in the LICENSE file that accompanied this code.\n+ *\n+ * This code is distributed in the hope that it will be useful, but WITHOUT\n+ * ANY WARRANTY; without even the implied warranty of MERCHANTABILITY or\n+ * FITNESS FOR A PARTICULAR PURPOSE.  See the GNU General Public License\n+ * version 2 for more details (a copy is included in the LICENSE file that\n+ * accompanied this code).\n+ *\n+ * You should have received a copy of the GNU General Public License version\n+ * 2 along with this work; if not, write to the Free Software Foundation,\n+ * Inc., 51 Franklin St, Fifth Floor, Boston, MA 02110-1301 USA.\n+ *\n+ * Please contact Oracle, 500 Oracle Parkway, Redwood Shores, CA 94065 USA\n+ * or visit www.oracle.com if you need additional information or have any\n+ * questions.\n+ *\/\n+\n+package oracle.code.triton;\n+\n+import java.lang.reflect.Method;\n+import java.lang.reflect.code.op.CoreOps;\n+import java.lang.runtime.CodeReflection;\n+import java.util.Optional;\n+import java.util.stream.Stream;\n+\n+class Functions {\n+    private Functions() {\n+    }\n+\n+    @CodeReflection\n+    static int sum(int a, int b) {\n+        return a + b;\n+    }\n+\n+    @CodeReflection\n+    static int max(int a, int b) {\n+        return Math.max(a, b);\n+    }\n+\n+    @CodeReflection\n+    static int cdiv(int x, int div) {\n+        return (x + div - 1) \/ div;\n+    }\n+\n+    static CoreOps.FuncOp getJavaCodeModel(String name) {\n+        Optional<Method> om = Stream.of(Functions.class.getDeclaredMethods())\n+                .filter(m -> m.getName().equals(name))\n+                .filter(m -> m.getAnnotation(CodeReflection.class) != null)\n+                .findFirst();\n+\n+        Method m = om.get();\n+        return m.getCodeModel().get();\n+    }\n+}\n","filename":"cr-examples\/triton\/src\/main\/java\/oracle\/code\/triton\/Functions.java","additions":62,"deletions":0,"binary":false,"changes":62,"status":"added"},{"patch":"@@ -0,0 +1,35 @@\n+\/*\n+ * Copyright (c) 2024, Oracle and\/or its affiliates. All rights reserved.\n+ * DO NOT ALTER OR REMOVE COPYRIGHT NOTICES OR THIS FILE HEADER.\n+ *\n+ * This code is free software; you can redistribute it and\/or modify it\n+ * under the terms of the GNU General Public License version 2 only, as\n+ * published by the Free Software Foundation.  Oracle designates this\n+ * particular file as subject to the \"Classpath\" exception as provided\n+ * by Oracle in the LICENSE file that accompanied this code.\n+ *\n+ * This code is distributed in the hope that it will be useful, but WITHOUT\n+ * ANY WARRANTY; without even the implied warranty of MERCHANTABILITY or\n+ * FITNESS FOR A PARTICULAR PURPOSE.  See the GNU General Public License\n+ * version 2 for more details (a copy is included in the LICENSE file that\n+ * accompanied this code).\n+ *\n+ * You should have received a copy of the GNU General Public License version\n+ * 2 along with this work; if not, write to the Free Software Foundation,\n+ * Inc., 51 Franklin St, Fifth Floor, Boston, MA 02110-1301 USA.\n+ *\n+ * Please contact Oracle, 500 Oracle Parkway, Redwood Shores, CA 94065 USA\n+ * or visit www.oracle.com if you need additional information or have any\n+ * questions.\n+ *\/\n+\n+package oracle.code.triton;\n+\n+public final class Ptr extends TritonNumber {\n+    private Ptr() {\n+    }\n+\n+    public PtrType type() {\n+        throw new UnsupportedOperationException();\n+    }\n+}\n","filename":"cr-examples\/triton\/src\/main\/java\/oracle\/code\/triton\/Ptr.java","additions":35,"deletions":0,"binary":false,"changes":35,"status":"added"},{"patch":"@@ -0,0 +1,65 @@\n+\/*\n+ * Copyright (c) 2024, Oracle and\/or its affiliates. All rights reserved.\n+ * DO NOT ALTER OR REMOVE COPYRIGHT NOTICES OR THIS FILE HEADER.\n+ *\n+ * This code is free software; you can redistribute it and\/or modify it\n+ * under the terms of the GNU General Public License version 2 only, as\n+ * published by the Free Software Foundation.  Oracle designates this\n+ * particular file as subject to the \"Classpath\" exception as provided\n+ * by Oracle in the LICENSE file that accompanied this code.\n+ *\n+ * This code is distributed in the hope that it will be useful, but WITHOUT\n+ * ANY WARRANTY; without even the implied warranty of MERCHANTABILITY or\n+ * FITNESS FOR A PARTICULAR PURPOSE.  See the GNU General Public License\n+ * version 2 for more details (a copy is included in the LICENSE file that\n+ * accompanied this code).\n+ *\n+ * You should have received a copy of the GNU General Public License version\n+ * 2 along with this work; if not, write to the Free Software Foundation,\n+ * Inc., 51 Franklin St, Fifth Floor, Boston, MA 02110-1301 USA.\n+ *\n+ * Please contact Oracle, 500 Oracle Parkway, Redwood Shores, CA 94065 USA\n+ * or visit www.oracle.com if you need additional information or have any\n+ * questions.\n+ *\/\n+\n+package oracle.code.triton;\n+\n+import java.lang.reflect.Type;\n+import java.lang.reflect.code.descriptor.TypeDesc;\n+import java.util.Objects;\n+\n+public final class PtrType extends TritonType {\n+    final Type rType;\n+\n+    public PtrType(Type rType) {\n+        this.rType = rType;\n+    }\n+\n+    public Type rType() {\n+        return rType;\n+    }\n+\n+    @Override\n+    public TypeDesc toDesc() {\n+        return TypeDesc.type(TritonOps.TYPE_Ptr, fromType(rType));\n+    }\n+\n+    @Override\n+    public boolean equals(Object o) {\n+        if (this == o) return true;\n+        if (o == null || getClass() != o.getClass()) return false;\n+        PtrType ptrType = (PtrType) o;\n+        return Objects.equals(rType, ptrType.rType);\n+    }\n+\n+    @Override\n+    public int hashCode() {\n+        return Objects.hash(rType);\n+    }\n+\n+    @Override\n+    public String toString() {\n+        return toDesc().toString();\n+    }\n+}\n","filename":"cr-examples\/triton\/src\/main\/java\/oracle\/code\/triton\/PtrType.java","additions":65,"deletions":0,"binary":false,"changes":65,"status":"added"},{"patch":"@@ -0,0 +1,171 @@\n+\/*\n+ * Copyright (c) 2024, Oracle and\/or its affiliates. All rights reserved.\n+ * DO NOT ALTER OR REMOVE COPYRIGHT NOTICES OR THIS FILE HEADER.\n+ *\n+ * This code is free software; you can redistribute it and\/or modify it\n+ * under the terms of the GNU General Public License version 2 only, as\n+ * published by the Free Software Foundation.  Oracle designates this\n+ * particular file as subject to the \"Classpath\" exception as provided\n+ * by Oracle in the LICENSE file that accompanied this code.\n+ *\n+ * This code is distributed in the hope that it will be useful, but WITHOUT\n+ * ANY WARRANTY; without even the implied warranty of MERCHANTABILITY or\n+ * FITNESS FOR A PARTICULAR PURPOSE.  See the GNU General Public License\n+ * version 2 for more details (a copy is included in the LICENSE file that\n+ * accompanied this code).\n+ *\n+ * You should have received a copy of the GNU General Public License version\n+ * 2 along with this work; if not, write to the Free Software Foundation,\n+ * Inc., 51 Franklin St, Fifth Floor, Boston, MA 02110-1301 USA.\n+ *\n+ * Please contact Oracle, 500 Oracle Parkway, Redwood Shores, CA 94065 USA\n+ * or visit www.oracle.com if you need additional information or have any\n+ * questions.\n+ *\/\n+\n+package oracle.code.triton;\n+\n+import java.lang.reflect.code.*;\n+import java.lang.reflect.code.descriptor.MethodTypeDesc;\n+import java.lang.reflect.code.descriptor.TypeDesc;\n+import java.lang.reflect.code.op.*;\n+import java.util.ArrayList;\n+import java.util.List;\n+import java.util.function.Consumer;\n+\n+public class SCFOps {\n+\n+    @OpDeclaration(ForOp.NAME)\n+    public static final class ForOp extends OpWithDefinition implements Op.Loop {\n+\n+        public static class Builder {\n+            final Body.Builder ancestorBody;\n+            final List<Value> range;\n+            final MethodTypeDesc loopDescriptor;\n+\n+            Builder(Body.Builder ancestorBody, List<Value> range, MethodTypeDesc loopDescriptor) {\n+                this.ancestorBody = ancestorBody;\n+                this.range = range;\n+                this.loopDescriptor = loopDescriptor;\n+            }\n+\n+            public ForOp body(Consumer<Block.Builder> c) {\n+                Body.Builder body = Body.Builder.of(ancestorBody, loopDescriptor);\n+                c.accept(body.entryBlock());\n+                return new ForOp(range, body);\n+            }\n+\n+            public ForOp body(CopyContext cc, Consumer<Block.Builder> c) {\n+                Body.Builder body = Body.Builder.of(ancestorBody, loopDescriptor, cc);\n+                c.accept(body.entryBlock());\n+                return new ForOp(range, body);\n+            }\n+        }\n+\n+        public static final String NAME = \"scf.for\";\n+\n+        final Body body;\n+\n+        public ForOp(OpDefinition def) {\n+            super(def);\n+\n+            this.body = def.bodyDefinitions().get(0).build(this);\n+        }\n+\n+        ForOp(ForOp that, CopyContext cc, OpTransformer ot) {\n+            super(that, cc);\n+\n+            this.body = that.body.transform(cc, ot).build(this);\n+        }\n+\n+        @Override\n+        public ForOp transform(CopyContext cc, OpTransformer ot) {\n+            return new ForOp(this, cc, ot);\n+        }\n+\n+        ForOp(List<Value> range, Body.Builder bodyBuilder) {\n+            super(NAME, range);\n+\n+            this.body = bodyBuilder.build(this);\n+        }\n+\n+        @Override\n+        public TypeDesc resultType() {\n+            return body.yieldType();\n+        }\n+\n+        @Override\n+        public List<Body> bodies() {\n+            return List.of(body);\n+        }\n+\n+        @Override\n+        public Body loopBody() {\n+            return body;\n+        }\n+    }\n+\n+    @OpDeclaration(YieldOp.NAME)\n+    public static class YieldOp extends OpWithDefinition implements Op.Terminating {\n+        public static final String NAME = \"scf.yield\";\n+\n+        public YieldOp(OpDefinition def) {\n+            super(def);\n+        }\n+\n+        YieldOp(YieldOp that, CopyContext cc) {\n+            super(that, cc);\n+        }\n+\n+        @Override\n+        public YieldOp transform(CopyContext cc, OpTransformer ot) {\n+            return new YieldOp(this, cc);\n+        }\n+\n+        YieldOp(List<Value> values) {\n+            super(NAME, values);\n+        }\n+\n+        @Override\n+        public TypeDesc resultType() {\n+            return TypeDesc.VOID;\n+        }\n+\n+        static TypeDesc yieldType(List<Value> values) {\n+            if (values.size() == 1) {\n+                return values.get(0).type();\n+            } else {\n+                return CoreOps.Tuple.typeFromValues(values);\n+            }\n+        }\n+    }\n+\n+    static public ForOp.Builder for_(Body.Builder ancestorBody,\n+                                     Value start, Value end, Value step,\n+                                     List<Value> iterValues) {\n+        TypeDesc yieldType = (iterValues.size() == 1)\n+                ? iterValues.get(0).type()\n+                : CoreOps.Tuple.typeFromValues(iterValues);\n+\n+        List<TypeDesc> bodyParameterTypes = new ArrayList<>();\n+        bodyParameterTypes.add(start.type());\n+        bodyParameterTypes.addAll(iterValues.stream().map(Value::type).toList());\n+        MethodTypeDesc bodyType = MethodTypeDesc.methodType(yieldType, bodyParameterTypes);\n+\n+        List<Value> operands = new ArrayList<>();\n+        operands.addAll(List.of(start, end, step));\n+        operands.addAll(iterValues);\n+        return new ForOp.Builder(ancestorBody, operands, bodyType);\n+    }\n+\n+\n+    public static final OpFactory FACTORY = OpFactory.OP_FACTORY.get(SCFOps.class);\n+\n+    static public YieldOp yield_(Value... values) {\n+        return yield_(List.of(values));\n+    }\n+\n+    static public YieldOp yield_(List<Value> values) {\n+        return new YieldOp(values);\n+    }\n+}\n","filename":"cr-examples\/triton\/src\/main\/java\/oracle\/code\/triton\/SCFOps.java","additions":171,"deletions":0,"binary":false,"changes":171,"status":"added"},{"patch":"@@ -0,0 +1,162 @@\n+\/*\n+ * Copyright (c) 2024, Oracle and\/or its affiliates. All rights reserved.\n+ * DO NOT ALTER OR REMOVE COPYRIGHT NOTICES OR THIS FILE HEADER.\n+ *\n+ * This code is free software; you can redistribute it and\/or modify it\n+ * under the terms of the GNU General Public License version 2 only, as\n+ * published by the Free Software Foundation.  Oracle designates this\n+ * particular file as subject to the \"Classpath\" exception as provided\n+ * by Oracle in the LICENSE file that accompanied this code.\n+ *\n+ * This code is distributed in the hope that it will be useful, but WITHOUT\n+ * ANY WARRANTY; without even the implied warranty of MERCHANTABILITY or\n+ * FITNESS FOR A PARTICULAR PURPOSE.  See the GNU General Public License\n+ * version 2 for more details (a copy is included in the LICENSE file that\n+ * accompanied this code).\n+ *\n+ * You should have received a copy of the GNU General Public License version\n+ * 2 along with this work; if not, write to the Free Software Foundation,\n+ * Inc., 51 Franklin St, Fifth Floor, Boston, MA 02110-1301 USA.\n+ *\n+ * Please contact Oracle, 500 Oracle Parkway, Redwood Shores, CA 94065 USA\n+ * or visit www.oracle.com if you need additional information or have any\n+ * questions.\n+ *\/\n+\n+package oracle.code.triton;\n+\n+import java.lang.reflect.code.Op;\n+import java.lang.reflect.code.Value;\n+import java.lang.reflect.code.analysis.Patterns;\n+import java.lang.reflect.code.descriptor.TypeDesc;\n+import java.lang.reflect.code.op.CoreOps;\n+import java.lang.reflect.code.op.CoreOps.VarAccessOp.VarLoadOp;\n+import java.lang.reflect.code.op.CoreOps.VarAccessOp.VarStoreOp;\n+import java.lang.reflect.code.op.ExtendedOps;\n+import java.util.ArrayList;\n+import java.util.List;\n+\n+import static java.lang.reflect.code.analysis.Patterns.*;\n+\n+\/\/ @@@ Very basic, limited, and partially correct\n+public class SimpleCountedForLoopInfo {\n+\n+    final ExtendedOps.JavaForOp fop;\n+\n+    SimpleCountedForLoopInfo(ExtendedOps.JavaForOp fop) {\n+        this.fop = fop;\n+\n+        if (fop.init().yieldType().equals(TypeDesc.VOID)) {\n+            throw new IllegalArgumentException(\"Loop variable externally initialized\");\n+        }\n+        if (fop.loopBody().entryBlock().parameters().size() > 1) {\n+            throw new IllegalArgumentException(\"Two or more loop variables\");\n+        }\n+    }\n+\n+    public List<Op> startExpression() {\n+        \/*\n+        ()Var<int> -> {\n+            %12 : int = constant @\"0\";\n+            %13 : Var<int> = var %12 @\"i\";\n+            yield %13;\n+        }\n+         *\/\n+\n+        Patterns.OpPattern p = opP(CoreOps.YieldOp.class,\n+                opP(CoreOps.VarOp.class,\n+                        opResultP()));\n+\n+        \/\/ match against yieldOp\n+        Op yieldOp = fop.init().entryBlock().ops().getLast();\n+        List<Value> matches = Patterns.match(null, yieldOp, p, (matchState, o) -> {\n+            return matchState.matchedOperands();\n+        });\n+        if (matches == null) {\n+            throw new IllegalArgumentException();\n+        }\n+        Op.Result initValue = (Op.Result) matches.get(0);\n+\n+        return traverseOperands(initValue.op());\n+    }\n+\n+    public List<Op> endExpression() {\n+        \/*\n+        (%14 : Var<int>)boolean -> {\n+            %15 : int = var.load %14;\n+            %16 : int = var.load %2;\n+            %17 : boolean = lt %15 %16;\n+            yield %17;\n+        }\n+         *\/\n+\n+        Patterns.OpPattern p = opP(CoreOps.YieldOp.class,\n+                opP(CoreOps.LtOp.class,\n+                        opP(VarLoadOp.class,\n+                                blockParameterP()),\n+                        opResultP()));\n+\n+        \/\/ match against yieldOp\n+        Op yieldOp = fop.cond().entryBlock().ops().getLast();\n+        List<Value> matches = Patterns.match(null, yieldOp, p, (matchState, o) -> {\n+            return matchState.matchedOperands();\n+        });\n+        if (matches == null) {\n+            throw new IllegalArgumentException();\n+        }\n+        Op.Result endValue = (Op.Result) matches.get(1);\n+\n+        return traverseOperands(endValue.op());\n+    }\n+\n+    public List<Op> stepExpression() {\n+        \/*\n+        (%18 : Var<int>)void -> {\n+            %19 : int = var.load %18;\n+            %20 : int = constant @\"1\";\n+            %21 : int = add %19 %20;\n+            var.store %18 %21;\n+            yield;\n+        }\n+         *\/\n+\n+        Patterns.OpPattern p = opP(VarStoreOp.class,\n+                blockParameterP(),\n+                opP(CoreOps.AddOp.class,\n+                        opP(VarLoadOp.class, blockParameterP()),\n+                        opResultP()));\n+\n+        \/\/ Match against last store op\n+        \/\/ @@@ Add Block.prevOp()\n+        Op storeOp = fop.update().entryBlock().ops().get(fop.update().entryBlock().ops().size() - 2);\n+        List<Value> matches = Patterns.match(null, storeOp, p, (matchState, r) -> {\n+            return matchState.matchedOperands();\n+        });\n+        if (matches == null) {\n+            throw new IllegalArgumentException();\n+        }\n+        Op.Result stepValue = (Op.Result) matches.get(2);\n+\n+        return traverseOperands(stepValue.op());\n+    }\n+\n+    static List<Op> traverseOperands(Op op) {\n+        List<Op> ops = new ArrayList<>();\n+        traverseOperands(ops, op);\n+        return ops;\n+    }\n+\n+    \/\/ Hoist the expression\n+    \/\/ @@@ should be pure and independent of the loop variable\n+    static void traverseOperands(List<Op> ops, Op op) {\n+        for (Value operand : op.operands()) {\n+            if (operand.declaringBlock().parentBody() == op.ancestorBody()) {\n+                if (operand instanceof Op.Result r) {\n+                    traverseOperands(ops, r.op());\n+                }\n+            }\n+        }\n+\n+        ops.add(op);\n+    }\n+}\n","filename":"cr-examples\/triton\/src\/main\/java\/oracle\/code\/triton\/SimpleCountedForLoopInfo.java","additions":162,"deletions":0,"binary":false,"changes":162,"status":"added"},{"patch":"@@ -0,0 +1,35 @@\n+\/*\n+ * Copyright (c) 2024, Oracle and\/or its affiliates. All rights reserved.\n+ * DO NOT ALTER OR REMOVE COPYRIGHT NOTICES OR THIS FILE HEADER.\n+ *\n+ * This code is free software; you can redistribute it and\/or modify it\n+ * under the terms of the GNU General Public License version 2 only, as\n+ * published by the Free Software Foundation.  Oracle designates this\n+ * particular file as subject to the \"Classpath\" exception as provided\n+ * by Oracle in the LICENSE file that accompanied this code.\n+ *\n+ * This code is distributed in the hope that it will be useful, but WITHOUT\n+ * ANY WARRANTY; without even the implied warranty of MERCHANTABILITY or\n+ * FITNESS FOR A PARTICULAR PURPOSE.  See the GNU General Public License\n+ * version 2 for more details (a copy is included in the LICENSE file that\n+ * accompanied this code).\n+ *\n+ * You should have received a copy of the GNU General Public License version\n+ * 2 along with this work; if not, write to the Free Software Foundation,\n+ * Inc., 51 Franklin St, Fifth Floor, Boston, MA 02110-1301 USA.\n+ *\n+ * Please contact Oracle, 500 Oracle Parkway, Redwood Shores, CA 94065 USA\n+ * or visit www.oracle.com if you need additional information or have any\n+ * questions.\n+ *\/\n+\n+package oracle.code.triton;\n+\n+public final class Tensor extends TritonNumber {\n+    private Tensor() {\n+    }\n+\n+    public TensorType type() {\n+        throw new UnsupportedOperationException();\n+    }\n+}\n","filename":"cr-examples\/triton\/src\/main\/java\/oracle\/code\/triton\/Tensor.java","additions":35,"deletions":0,"binary":false,"changes":35,"status":"added"},{"patch":"@@ -0,0 +1,88 @@\n+\/*\n+ * Copyright (c) 2024, Oracle and\/or its affiliates. All rights reserved.\n+ * DO NOT ALTER OR REMOVE COPYRIGHT NOTICES OR THIS FILE HEADER.\n+ *\n+ * This code is free software; you can redistribute it and\/or modify it\n+ * under the terms of the GNU General Public License version 2 only, as\n+ * published by the Free Software Foundation.  Oracle designates this\n+ * particular file as subject to the \"Classpath\" exception as provided\n+ * by Oracle in the LICENSE file that accompanied this code.\n+ *\n+ * This code is distributed in the hope that it will be useful, but WITHOUT\n+ * ANY WARRANTY; without even the implied warranty of MERCHANTABILITY or\n+ * FITNESS FOR A PARTICULAR PURPOSE.  See the GNU General Public License\n+ * version 2 for more details (a copy is included in the LICENSE file that\n+ * accompanied this code).\n+ *\n+ * You should have received a copy of the GNU General Public License version\n+ * 2 along with this work; if not, write to the Free Software Foundation,\n+ * Inc., 51 Franklin St, Fifth Floor, Boston, MA 02110-1301 USA.\n+ *\n+ * Please contact Oracle, 500 Oracle Parkway, Redwood Shores, CA 94065 USA\n+ * or visit www.oracle.com if you need additional information or have any\n+ * questions.\n+ *\/\n+\n+package oracle.code.triton;\n+\n+import java.lang.reflect.Type;\n+import java.lang.reflect.code.descriptor.TypeDesc;\n+import java.util.ArrayList;\n+import java.util.List;\n+import java.util.Objects;\n+\n+public final class TensorType extends TritonType {\n+    final Type eType;\n+    final List<Integer> shape;\n+    final int size;\n+\n+    public TensorType(Type eType, List<Integer> shape) {\n+        this.eType = eType;\n+        this.shape = List.copyOf(shape);\n+        int s = 1;\n+        for (Integer i : shape) {\n+            s *= i;\n+        }\n+        this.size = s;\n+    }\n+\n+    public Type eType() {\n+        return eType;\n+    }\n+\n+    public List<Integer> shape() {\n+        return shape;\n+    }\n+\n+    public int size() {\n+        return size;\n+    }\n+\n+    @Override\n+    public TypeDesc toDesc() {\n+        List<TypeDesc> params = new ArrayList<>();\n+        for (int i : shape) {\n+            params.add(TypeDesc.ofString(\"x\" + i));\n+        }\n+        params.add(fromType(eType));\n+        return TypeDesc.type(TritonOps.TYPE_Tensor, params);\n+    }\n+\n+    @Override\n+    public boolean equals(Object o) {\n+        if (this == o) return true;\n+        if (o == null || getClass() != o.getClass()) return false;\n+        TensorType that = (TensorType) o;\n+        return Objects.equals(eType, that.eType) && Objects.equals(shape, that.shape);\n+    }\n+\n+    @Override\n+    public int hashCode() {\n+        return Objects.hash(eType, shape);\n+    }\n+\n+    @Override\n+    public String toString() {\n+        return toDesc().toString();\n+    }\n+}\n","filename":"cr-examples\/triton\/src\/main\/java\/oracle\/code\/triton\/TensorType.java","additions":88,"deletions":0,"binary":false,"changes":88,"status":"added"},{"patch":"@@ -0,0 +1,133 @@\n+\/*\n+ * Copyright (c) 2024, Oracle and\/or its affiliates. All rights reserved.\n+ * DO NOT ALTER OR REMOVE COPYRIGHT NOTICES OR THIS FILE HEADER.\n+ *\n+ * This code is free software; you can redistribute it and\/or modify it\n+ * under the terms of the GNU General Public License version 2 only, as\n+ * published by the Free Software Foundation.  Oracle designates this\n+ * particular file as subject to the \"Classpath\" exception as provided\n+ * by Oracle in the LICENSE file that accompanied this code.\n+ *\n+ * This code is distributed in the hope that it will be useful, but WITHOUT\n+ * ANY WARRANTY; without even the implied warranty of MERCHANTABILITY or\n+ * FITNESS FOR A PARTICULAR PURPOSE.  See the GNU General Public License\n+ * version 2 for more details (a copy is included in the LICENSE file that\n+ * accompanied this code).\n+ *\n+ * You should have received a copy of the GNU General Public License version\n+ * 2 along with this work; if not, write to the Free Software Foundation,\n+ * Inc., 51 Franklin St, Fifth Floor, Boston, MA 02110-1301 USA.\n+ *\n+ * Please contact Oracle, 500 Oracle Parkway, Redwood Shores, CA 94065 USA\n+ * or visit www.oracle.com if you need additional information or have any\n+ * questions.\n+ *\/\n+\n+package oracle.code.triton;\n+\n+import java.lang.reflect.Type;\n+import java.lang.runtime.CodeReflection;\n+import java.util.List;\n+\n+public class Triton {\n+    private Triton() {\n+    }\n+\n+    public static int programId(@Constant int axis) {\n+        throw new UnsupportedOperationException();\n+    }\n+\n+    public static Tensor arange(@Constant int start, @Constant int end) {\n+        throw new UnsupportedOperationException();\n+    }\n+\n+    public static Tensor load(Tensor ptr, Tensor mask) {\n+        throw new UnsupportedOperationException();\n+    }\n+\n+    public static void store(Tensor ptr, Tensor value, Tensor mask) {\n+        throw new UnsupportedOperationException();\n+    }\n+\n+    public static Tensor broadcast(Object o, TensorType type) {\n+        throw new UnsupportedOperationException();\n+    }\n+\n+    public static Tensor expand(Tensor a, int axis) {\n+        throw new UnsupportedOperationException();\n+    }\n+\n+    public static Tensor zeros(Class<?> eType, int... shape)  {\n+        throw new UnsupportedOperationException();\n+    }\n+\n+    public static TensorType joinShape(TensorType a, TensorType b) {\n+        throw new UnsupportedOperationException();\n+    }\n+\n+    public static Tensor dot(Tensor a, Tensor b) {\n+        throw new UnsupportedOperationException();\n+    }\n+\n+    \/\/ Arithmetic\n+\n+    public static Tensor add(Number a, Number b) {\n+        throw new UnsupportedOperationException();\n+    }\n+\n+    public static Tensor sub(Number a, Number b) {\n+        throw new UnsupportedOperationException();\n+    }\n+\n+    public static Tensor mul(Number a, Number b) {\n+        throw new UnsupportedOperationException();\n+    }\n+\n+    public static Tensor div(Number a, Number b) {\n+        throw new UnsupportedOperationException();\n+    }\n+\n+    public static Tensor mod(Number a, Number b) {\n+        throw new UnsupportedOperationException();\n+    }\n+\n+    public static Tensor and(Number a, Number b) {\n+        throw new UnsupportedOperationException();\n+    }\n+\n+    public enum CompareKind {\n+        Equal,\n+        LessThan,\n+        LessThanOrEqual,\n+        GreaterThan,\n+        GreaterThanOrEqual\n+    }\n+\n+    public static Tensor compare(Number a, Number b, @Constant CompareKind ck) {\n+        throw new UnsupportedOperationException();\n+    }\n+\n+    public static Tensor exp(Tensor a) {\n+        throw new UnsupportedOperationException();\n+    }\n+\n+    public static int cdiv(Number x, Number div) {\n+        throw new UnsupportedOperationException();\n+    }\n+\n+    \/\/ Conversions\n+\n+    public static <T extends Number> T conv(Type t, T a) {\n+        throw new UnsupportedOperationException();\n+    }\n+\n+    \/\/ Reductions\n+\n+    public static Tensor max(Tensor a, @Constant int axis) {\n+        throw new UnsupportedOperationException();\n+    }\n+\n+    public static Tensor sum(Tensor a, @Constant int axis) {\n+        throw new UnsupportedOperationException();\n+    }\n+}\n","filename":"cr-examples\/triton\/src\/main\/java\/oracle\/code\/triton\/Triton.java","additions":133,"deletions":0,"binary":false,"changes":133,"status":"added"},{"patch":"@@ -0,0 +1,51 @@\n+\/*\n+ * Copyright (c) 2024, Oracle and\/or its affiliates. All rights reserved.\n+ * DO NOT ALTER OR REMOVE COPYRIGHT NOTICES OR THIS FILE HEADER.\n+ *\n+ * This code is free software; you can redistribute it and\/or modify it\n+ * under the terms of the GNU General Public License version 2 only, as\n+ * published by the Free Software Foundation.  Oracle designates this\n+ * particular file as subject to the \"Classpath\" exception as provided\n+ * by Oracle in the LICENSE file that accompanied this code.\n+ *\n+ * This code is distributed in the hope that it will be useful, but WITHOUT\n+ * ANY WARRANTY; without even the implied warranty of MERCHANTABILITY or\n+ * FITNESS FOR A PARTICULAR PURPOSE.  See the GNU General Public License\n+ * version 2 for more details (a copy is included in the LICENSE file that\n+ * accompanied this code).\n+ *\n+ * You should have received a copy of the GNU General Public License version\n+ * 2 along with this work; if not, write to the Free Software Foundation,\n+ * Inc., 51 Franklin St, Fifth Floor, Boston, MA 02110-1301 USA.\n+ *\n+ * Please contact Oracle, 500 Oracle Parkway, Redwood Shores, CA 94065 USA\n+ * or visit www.oracle.com if you need additional information or have any\n+ * questions.\n+ *\/\n+\n+package oracle.code.triton;\n+\n+abstract class TritonNumber extends Number {\n+    TritonNumber() {\n+    }\n+\n+    @Override\n+    public int intValue() {\n+        throw new UnsupportedOperationException();\n+    }\n+\n+    @Override\n+    public long longValue() {\n+        throw new UnsupportedOperationException();\n+    }\n+\n+    @Override\n+    public float floatValue() {\n+        throw new UnsupportedOperationException();\n+    }\n+\n+    @Override\n+    public double doubleValue() {\n+        throw new UnsupportedOperationException();\n+    }\n+}\n","filename":"cr-examples\/triton\/src\/main\/java\/oracle\/code\/triton\/TritonNumber.java","additions":51,"deletions":0,"binary":false,"changes":51,"status":"added"},{"patch":"@@ -0,0 +1,829 @@\n+\/*\n+ * Copyright (c) 2024, Oracle and\/or its affiliates. All rights reserved.\n+ * DO NOT ALTER OR REMOVE COPYRIGHT NOTICES OR THIS FILE HEADER.\n+ *\n+ * This code is free software; you can redistribute it and\/or modify it\n+ * under the terms of the GNU General Public License version 2 only, as\n+ * published by the Free Software Foundation.  Oracle designates this\n+ * particular file as subject to the \"Classpath\" exception as provided\n+ * by Oracle in the LICENSE file that accompanied this code.\n+ *\n+ * This code is distributed in the hope that it will be useful, but WITHOUT\n+ * ANY WARRANTY; without even the implied warranty of MERCHANTABILITY or\n+ * FITNESS FOR A PARTICULAR PURPOSE.  See the GNU General Public License\n+ * version 2 for more details (a copy is included in the LICENSE file that\n+ * accompanied this code).\n+ *\n+ * You should have received a copy of the GNU General Public License version\n+ * 2 along with this work; if not, write to the Free Software Foundation,\n+ * Inc., 51 Franklin St, Fifth Floor, Boston, MA 02110-1301 USA.\n+ *\n+ * Please contact Oracle, 500 Oracle Parkway, Redwood Shores, CA 94065 USA\n+ * or visit www.oracle.com if you need additional information or have any\n+ * questions.\n+ *\/\n+\n+package oracle.code.triton;\n+\n+import java.lang.reflect.code.*;\n+import java.lang.reflect.code.descriptor.MethodTypeDesc;\n+import java.lang.reflect.code.descriptor.TypeDesc;\n+import java.lang.reflect.code.op.*;\n+import java.util.Collections;\n+import java.util.HashMap;\n+import java.util.List;\n+import java.util.Map;\n+import java.util.function.Consumer;\n+\n+public class TritonOps {\n+    static final TypeDesc TYPE_Ptr = TypeDesc.ofString(\"ptr\");\n+    static final TypeDesc TYPE_Tensor = TypeDesc.ofString(\"tensor\");\n+\n+    static abstract class TritonOp extends OpWithDefinition {\n+        final TypeDesc resultType;\n+\n+        public TritonOp(OpDefinition def) {\n+            super(def);\n+\n+            this.resultType = def.resultType();\n+        }\n+\n+        TritonOp(TritonOp that, CopyContext cc) {\n+            super(that, cc);\n+\n+            this.resultType = that.resultType;\n+        }\n+\n+        TritonOp(String name, TypeDesc resultType, List<? extends Value> operands) {\n+            super(name, operands);\n+\n+            this.resultType = resultType;\n+        }\n+\n+        @Override\n+        public TypeDesc resultType() {\n+            return resultType;\n+        }\n+    }\n+\n+    @OpDeclaration(ModuleOp.NAME)\n+    public static final class ModuleOp extends TritonOp implements Op.Isolated {\n+        public static final String NAME = \"module\";\n+\n+        final Map<String, FuncOp> table;\n+        final Body body;\n+\n+        public ModuleOp(OpDefinition def) {\n+            super(def);\n+\n+            this.body = def.bodyDefinitions().get(0).build(this);\n+            this.table = createTable(body);\n+        }\n+\n+        ModuleOp(ModuleOp that, CopyContext cc, OpTransformer ot) {\n+            super(that, cc);\n+\n+            this.body = that.body.transform(cc, ot).build(this);\n+            this.table = createTable(body);\n+        }\n+\n+        static Map<String, FuncOp> createTable(Body body) {\n+            Map<String, FuncOp> table = new HashMap<>();\n+            for (var op : body.entryBlock().ops()) {\n+                if (op instanceof FuncOp fop) {\n+                    table.put(fop.funcName(), fop);\n+                } else if (op instanceof CoreOps.UnreachableOp _) {\n+                    \/\/ no operation\n+                } else {\n+                    throw new IllegalArgumentException(\"Bad operation in module: \" + op);\n+                }\n+            }\n+            return Collections.unmodifiableMap(table);\n+        }\n+\n+        @Override\n+        public ModuleOp transform(CopyContext cc, OpTransformer ot) {\n+            return new ModuleOp(this, cc, ot);\n+        }\n+\n+        public ModuleOp transform(OpTransformer ot) {\n+            return new ModuleOp(this, CopyContext.create(), ot);\n+        }\n+\n+        ModuleOp(List<FuncOp> functions) {\n+            super(NAME, TypeDesc.VOID,\n+                    List.of());\n+\n+            Body.Builder bodyC = Body.Builder.of(null, MethodTypeDesc.VOID);\n+            Block.Builder entryBlock = bodyC.entryBlock();\n+            Map<String, FuncOp> table = new HashMap<>();\n+            for (FuncOp f : functions) {\n+                entryBlock.op(f);\n+                table.put(f.funcName(), f);\n+            }\n+            entryBlock.op(CoreOps.unreachable());\n+            this.table = Collections.unmodifiableMap(table);\n+            this.body = bodyC.build(this);\n+        }\n+\n+        @Override\n+        public List<Body> bodies() {\n+            return List.of(body);\n+        }\n+\n+        public Map<String, FuncOp> functionTable() {\n+            return table;\n+        }\n+    }\n+\n+    @OpDeclaration(FuncOp.NAME)\n+    public static final class FuncOp extends TritonOp implements Op.Invokable, Op.Isolated, Op.Lowerable {\n+\n+        public static class Builder {\n+            final Body.Builder ancestorBody;\n+            final String funcName;\n+            final MethodTypeDesc funcDescriptor;\n+\n+            Builder(Body.Builder ancestorBody, String funcName, MethodTypeDesc funcDescriptor) {\n+                this.ancestorBody = ancestorBody;\n+                this.funcName = funcName;\n+                this.funcDescriptor = funcDescriptor;\n+            }\n+\n+            public FuncOp body(Consumer<Block.Builder> c) {\n+                Body.Builder body = Body.Builder.of(ancestorBody, funcDescriptor);\n+                c.accept(body.entryBlock());\n+                return new FuncOp(funcName, body);\n+            }\n+        }\n+\n+        public static final String NAME = \"tt.func\";\n+        public static final String ATTRIBUTE_FUNC_NAME = NAME + \".name\";\n+\n+        final String funcName;\n+        final Body body;\n+\n+        public static FuncOp create(OpDefinition def) {\n+            if (!def.operands().isEmpty()) {\n+                throw new IllegalStateException(\"Bad op \" + def.name());\n+            }\n+\n+            String funcName = def.extractAttributeValue(ATTRIBUTE_FUNC_NAME, true,\n+                    v -> switch (v) {\n+                        case String s -> s;\n+                        default -> throw new UnsupportedOperationException(\"Unsupported func name value:\" + v);\n+                    });\n+            return new FuncOp(def, funcName);\n+        }\n+\n+        FuncOp(OpDefinition def, String funcName) {\n+            super(def);\n+\n+            this.funcName = funcName;\n+            this.body = def.bodyDefinitions().get(0).build(this);\n+        }\n+\n+        FuncOp(FuncOp that, CopyContext cc, OpTransformer oa) {\n+            this(that, that.funcName, cc, oa);\n+        }\n+\n+        FuncOp(FuncOp that, String funcName, CopyContext cc, OpTransformer ot) {\n+            super(that, cc);\n+\n+            this.funcName = funcName;\n+            this.body = that.body.transform(cc, ot).build(this);\n+        }\n+\n+        @Override\n+        public FuncOp transform(CopyContext cc, OpTransformer ot) {\n+            return new FuncOp(this, cc, ot);\n+        }\n+\n+        public FuncOp transform(OpTransformer ot) {\n+            return new FuncOp(this, CopyContext.create(), ot);\n+        }\n+\n+        public FuncOp transform(String funcName, OpTransformer ot) {\n+            return new FuncOp(this, funcName, CopyContext.create(), ot);\n+        }\n+\n+        FuncOp(String funcName, Body.Builder bodyBuilder) {\n+            super(NAME, TypeDesc.VOID,\n+                    List.of());\n+\n+            this.funcName = funcName;\n+            this.body = bodyBuilder.build(this);\n+        }\n+\n+        @Override\n+        public List<Body> bodies() {\n+            return List.of(body);\n+        }\n+\n+        @Override\n+        public Map<String, Object> attributes() {\n+            HashMap<String, Object> m = new HashMap<>(super.attributes());\n+            m.put(\"\", funcName);\n+            return Collections.unmodifiableMap(m);\n+        }\n+\n+        @Override\n+        public MethodTypeDesc funcDescriptor() {\n+            return body.descriptor();\n+        }\n+\n+        public String funcName() {\n+            return funcName;\n+        }\n+\n+        @Override\n+        public Body body() {\n+            return body;\n+        }\n+\n+        @Override\n+        public Block.Builder lower(Block.Builder b, OpTransformer _ignore) {\n+            \/\/ Isolate body with respect to ancestor transformations\n+            \/\/ and copy directly without lowering descendant operations\n+            b.op(this, OpTransformer.COPYING_TRANSFORMER);\n+            return b;\n+        }\n+    }\n+\n+    @OpDeclaration(CallOp.NAME)\n+    public static final class CallOp extends TritonOp {\n+        public static final String NAME = \"tt.call\";\n+        public static final String ATTRIBUTE_FUNC_NAME = NAME + \".name\";\n+\n+        final String funcName;\n+\n+        public static CallOp create(OpDefinition def) {\n+            String funcName = def.extractAttributeValue(ATTRIBUTE_FUNC_NAME, true,\n+                    v -> switch (v) {\n+                        case String s -> s;\n+                        default -> throw new UnsupportedOperationException(\"Unsupported func name value:\" + v);\n+                    });\n+\n+            return new CallOp(def, funcName);\n+        }\n+\n+        CallOp(OpDefinition def, String funcName) {\n+            super(def);\n+\n+            this.funcName = funcName;\n+        }\n+\n+        CallOp(CallOp that, CopyContext cc) {\n+            super(that, cc);\n+\n+            this.funcName = that.funcName;\n+        }\n+\n+        @Override\n+        public CallOp transform(CopyContext cc, OpTransformer ot) {\n+            return new CallOp(this, cc);\n+        }\n+\n+        CallOp(String funcName, TypeDesc resultType, List<Value> args) {\n+            super(NAME, resultType, args);\n+\n+            this.funcName = funcName;\n+        }\n+\n+        @Override\n+        public Map<String, Object> attributes() {\n+            HashMap<String, Object> m = new HashMap<>(super.attributes());\n+            m.put(\"\", funcName);\n+            return Collections.unmodifiableMap(m);\n+        }\n+\n+        public String funcName() {\n+            return funcName;\n+        }\n+    }\n+\n+    @OpDeclaration(ReduceOp.NAME)\n+    public static final class ReduceOp extends TritonOp {\n+        \/\/ @@@ SSA transformation does not work with nested ops\n+        \/\/ implements Op.Nested {\n+\n+        public static class Builder {\n+            final Body.Builder ancestorBody;\n+            final int axis;\n+            final Value v;\n+            final MethodTypeDesc reduceDescriptor;\n+\n+            Builder(Body.Builder ancestorBody, int axis, Value v, MethodTypeDesc reduceDescriptor) {\n+                this.ancestorBody = ancestorBody;\n+                this.axis = axis;\n+                this.v = v;\n+                this.reduceDescriptor = reduceDescriptor;\n+            }\n+\n+            public ReduceOp body(Consumer<Block.Builder> c) {\n+                Body.Builder body = Body.Builder.of(ancestorBody, reduceDescriptor);\n+                c.accept(body.entryBlock());\n+                return new ReduceOp(axis, v, body);\n+            }\n+        }\n+\n+        public static final String NAME = \"tt.reduce\";\n+        public static final String ATTRIBUTE_AXIS = \"axis\";\n+\n+        final int axis;\n+        final Body reducer;\n+\n+        public static ReduceOp create(OpDefinition def) {\n+            int axis = def.extractAttributeValue(ATTRIBUTE_AXIS, true,\n+                    v -> switch (v) {\n+                        case String s -> Integer.valueOf(s);\n+                        case Integer i -> i;\n+                        default -> throw new UnsupportedOperationException(\"Unsupported axis value:\" + v);\n+                    });\n+            return new ReduceOp(def, axis);\n+        }\n+\n+        ReduceOp(OpDefinition def, int axis) {\n+            super(def);\n+\n+            this.axis = axis;\n+            this.reducer = def.bodyDefinitions().get(0).build(this);\n+        }\n+\n+        ReduceOp(ReduceOp that, CopyContext cc, OpTransformer ot) {\n+            super(that, cc);\n+\n+            this.axis = that.axis;\n+            this.reducer = that.reducer.transform(cc, ot).build(this);\n+        }\n+\n+        @Override\n+        public ReduceOp transform(CopyContext cc, OpTransformer ot) {\n+            return new ReduceOp(this, cc, ot);\n+        }\n+\n+        ReduceOp(int axis, Value tensor, Body.Builder reducerBuilder) {\n+            super(NAME, reducerBuilder.descriptor().returnType(), List.of(tensor));\n+\n+            this.axis = axis;\n+            this.reducer = reducerBuilder.build(this);\n+        }\n+\n+        @Override\n+        public List<Body> bodies() {\n+            return List.of(reducer);\n+        }\n+\n+        @Override\n+        public Map<String, Object> attributes() {\n+            HashMap<String, Object> m = new HashMap<>(super.attributes());\n+            m.put(ATTRIBUTE_AXIS, axis);\n+            return Collections.unmodifiableMap(m);\n+        }\n+\n+        public int axis() {\n+            return axis;\n+        }\n+\n+        public Body reducer() {\n+            return reducer;\n+        }\n+    }\n+\n+    @OpDeclaration(ReduceReturnOp.NAME)\n+    public static class ReduceReturnOp extends TritonOp implements Op.Terminating {\n+        public static final String NAME = \"tt.reduce.return\";\n+\n+        public ReduceReturnOp(OpDefinition def) {\n+            super(def);\n+        }\n+\n+        ReduceReturnOp(ReduceReturnOp that, CopyContext cc) {\n+            super(that, cc);\n+        }\n+\n+        @Override\n+        public ReduceReturnOp transform(CopyContext cc, OpTransformer ot) {\n+            return new ReduceReturnOp(this, cc);\n+        }\n+\n+        ReduceReturnOp(Value r) {\n+            super(NAME, TypeDesc.VOID, List.of(r));\n+        }\n+    }\n+\n+    @OpDeclaration(GetProgramIdOp.NAME)\n+    public static class GetProgramIdOp extends TritonOp implements Op.Pure {\n+        public static final String NAME = \"tt.get_program_id\";\n+        public static final String ATTRIBUTE_AXIS = \"axis\";\n+\n+        final int axis;\n+\n+        public static GetProgramIdOp create(OpDefinition def) {\n+            int axis = def.extractAttributeValue(ATTRIBUTE_AXIS, true,\n+                    v -> switch (v) {\n+                        case String s -> Integer.valueOf(s);\n+                        case Integer i -> i;\n+                        default -> throw new UnsupportedOperationException(\"Unsupported axis value:\" + v);\n+                    });\n+            return new GetProgramIdOp(def, axis);\n+        }\n+\n+        GetProgramIdOp(OpDefinition def, int axis) {\n+            super(def);\n+\n+            this.axis = axis;\n+        }\n+\n+        GetProgramIdOp(GetProgramIdOp that, CopyContext cc) {\n+            super(that, cc);\n+\n+            this.axis = that.axis;\n+        }\n+\n+        @Override\n+        public GetProgramIdOp transform(CopyContext cc, OpTransformer ot) {\n+            return new GetProgramIdOp(this, cc);\n+        }\n+\n+        GetProgramIdOp(int axis) {\n+            super(NAME, TypeDesc.INT, List.of());\n+\n+            this.axis = axis;\n+        }\n+\n+        @Override\n+        public Map<String, Object> attributes() {\n+            HashMap<String, Object> m = new HashMap<>(super.attributes());\n+            m.put(\"\", axis);\n+            return Collections.unmodifiableMap(m);\n+        }\n+\n+        public int axis() {\n+            return axis;\n+        }\n+    }\n+\n+    @OpDeclaration(MakeRangeOp.NAME)\n+    public static class MakeRangeOp extends TritonOp implements Op.Pure {\n+        public static final String NAME = \"tt.make_range\";\n+        public static final String ATTRIBUTE_START = \"start\";\n+        public static final String ATTRIBUTE_END = \"end\";\n+\n+        final int start;\n+        final int end;\n+\n+        public static MakeRangeOp create(OpDefinition def) {\n+            int start = def.extractAttributeValue(ATTRIBUTE_START, false,\n+                    v -> switch (v) {\n+                        case String s -> Integer.valueOf(s);\n+                        case Integer i -> i;\n+                        default -> throw new UnsupportedOperationException(\"Unsupported start value:\" + v);\n+                    });\n+            int end = def.extractAttributeValue(ATTRIBUTE_END, false,\n+                    v -> switch (v) {\n+                        case String s -> Integer.valueOf(s);\n+                        case Integer i -> i;\n+                        default -> throw new UnsupportedOperationException(\"Unsupported end value:\" + v);\n+                    });\n+            return new MakeRangeOp(def, start, end);\n+        }\n+\n+        MakeRangeOp(OpDefinition def, int start, int end) {\n+            super(def);\n+\n+            this.start = start;\n+            this.end = end;\n+        }\n+\n+        MakeRangeOp(MakeRangeOp that, CopyContext cc) {\n+            super(that, cc);\n+\n+            this.start = that.start;\n+            this.end = that.end;\n+        }\n+\n+        @Override\n+        public MakeRangeOp transform(CopyContext cc, OpTransformer ot) {\n+            return new MakeRangeOp(this, cc);\n+        }\n+\n+        MakeRangeOp(int start, int end) {\n+            super(NAME, tensorType(start, end), List.of());\n+\n+            this.start = start;\n+            this.end = end;\n+        }\n+\n+        static TypeDesc tensorType(int start, int end) {\n+            return new TensorType(int.class, List.of(end - start)).toDesc();\n+        }\n+\n+        @Override\n+        public Map<String, Object> attributes() {\n+            HashMap<String, Object> m = new HashMap<>(super.attributes());\n+            m.put(ATTRIBUTE_START, start);\n+            m.put(ATTRIBUTE_END, end);\n+            return Collections.unmodifiableMap(m);\n+        }\n+    }\n+\n+    @OpDeclaration(ExpandOp.NAME)\n+    public static class ExpandOp extends TritonOp implements Op.Pure {\n+        public static final String NAME = \"tt.expand_dims\";\n+        public static final String ATTRIBUTE_AXIS = \"axis\";\n+\n+        final int axis;\n+\n+        public static ExpandOp create(OpDefinition def) {\n+            int axis = def.extractAttributeValue(ATTRIBUTE_AXIS, true,\n+                    v -> switch (v) {\n+                        case String s -> Integer.valueOf(s);\n+                        case Integer i -> i;\n+                        default -> throw new UnsupportedOperationException(\"Unsupported axis value:\" + v);\n+                    });\n+            return new ExpandOp(def, axis);\n+        }\n+\n+        ExpandOp(OpDefinition def, int axis) {\n+            super(def);\n+\n+            this.axis = axis;\n+        }\n+\n+        ExpandOp(ExpandOp that, CopyContext cc) {\n+            super(that, cc);\n+\n+            this.axis = that.axis;\n+        }\n+\n+        @Override\n+        public ExpandOp transform(CopyContext cc, OpTransformer ot) {\n+            return new ExpandOp(this, cc);\n+        }\n+\n+        ExpandOp(int axis, TypeDesc tensorType, Value v) {\n+            super(NAME, tensorType, List.of(v));\n+\n+            this.axis = axis;\n+        }\n+\n+        @Override\n+        public Map<String, Object> attributes() {\n+            HashMap<String, Object> m = new HashMap<>(super.attributes());\n+            m.put(\"\", axis);\n+            return Collections.unmodifiableMap(m);\n+        }\n+\n+        public int axis() {\n+            return axis;\n+        }\n+    }\n+\n+    @OpDeclaration(SplatOp.NAME)\n+    public static class SplatOp extends TritonOp implements Op.Pure {\n+        public static final String NAME = \"tt.splat\";\n+\n+        public SplatOp(OpDefinition def) {\n+            super(def);\n+        }\n+\n+        SplatOp(SplatOp that, CopyContext cc) {\n+            super(that, cc);\n+        }\n+\n+        @Override\n+        public SplatOp transform(CopyContext cc, OpTransformer ot) {\n+            return new SplatOp(this, cc);\n+        }\n+\n+        SplatOp(TypeDesc tensorType, Value v) {\n+            super(NAME, tensorType, List.of(v));\n+        }\n+    }\n+\n+    @OpDeclaration(BroadcastOp.NAME)\n+    public static class BroadcastOp extends TritonOp implements Op.Pure {\n+        public static final String NAME = \"tt.broadcast\";\n+\n+        public BroadcastOp(OpDefinition def) {\n+            super(def);\n+        }\n+\n+        BroadcastOp(BroadcastOp that, CopyContext cc) {\n+            super(that, cc);\n+        }\n+\n+        @Override\n+        public BroadcastOp transform(CopyContext cc, OpTransformer ot) {\n+            return new BroadcastOp(this, cc);\n+        }\n+\n+        BroadcastOp(TypeDesc tensorType, Value v) {\n+            super(NAME, tensorType, List.of(v));\n+        }\n+    }\n+\n+    @OpDeclaration(AddPtrOp.NAME)\n+    public static class AddPtrOp extends TritonOp implements Op.Pure {\n+        public static final String NAME = \"tt.addptr\";\n+\n+        public AddPtrOp(OpDefinition def) {\n+            super(def);\n+        }\n+\n+        AddPtrOp(AddPtrOp that, CopyContext cc) {\n+            super(that, cc);\n+        }\n+\n+        @Override\n+        public AddPtrOp transform(CopyContext cc, OpTransformer ot) {\n+            return new AddPtrOp(this, cc);\n+        }\n+\n+        AddPtrOp(Value ptr, Value offset) {\n+            super(NAME, ptr.type(), List.of(ptr, offset));\n+        }\n+    }\n+\n+    @OpDeclaration(LoadOp.NAME)\n+    public static class LoadOp extends TritonOp implements Op.Pure {\n+        public static final String NAME = \"tt.load\";\n+\n+        public LoadOp(OpDefinition def) {\n+            super(def);\n+        }\n+\n+        LoadOp(LoadOp that, CopyContext cc) {\n+            super(that, cc);\n+        }\n+\n+        @Override\n+        public LoadOp transform(CopyContext cc, OpTransformer ot) {\n+            return new LoadOp(this, cc);\n+        }\n+\n+        LoadOp(TypeDesc tensorType, Value ptr, Value mask) {\n+            super(NAME, tensorType, List.of(ptr, mask));\n+        }\n+    }\n+\n+    @OpDeclaration(StoreOp.NAME)\n+    public static class StoreOp extends TritonOp {\n+        public static final String NAME = \"tt.store\";\n+\n+        public StoreOp(OpDefinition def) {\n+            super(def);\n+        }\n+\n+        StoreOp(StoreOp that, CopyContext cc) {\n+            super(that, cc);\n+        }\n+\n+        @Override\n+        public StoreOp transform(CopyContext cc, OpTransformer ot) {\n+            return new StoreOp(this, cc);\n+        }\n+\n+        StoreOp(Value ptr, Value v, Value mask) {\n+            super(NAME, TypeDesc.VOID, List.of(ptr, v, mask));\n+        }\n+    }\n+\n+    @OpDeclaration(ReturnOp.NAME)\n+    public static class ReturnOp extends TritonOp implements Op.Terminating {\n+        public static final String NAME = \"tt.return\";\n+\n+        public ReturnOp(OpDefinition def) {\n+            super(def);\n+        }\n+\n+        ReturnOp(ReturnOp that, CopyContext cc) {\n+            super(that, cc);\n+        }\n+\n+        @Override\n+        public ReturnOp transform(CopyContext cc, OpTransformer ot) {\n+            return new ReturnOp(this, cc);\n+        }\n+\n+        ReturnOp() {\n+            super(NAME, TypeDesc.VOID, List.of());\n+        }\n+\n+        ReturnOp(Value v) {\n+            super(NAME, TypeDesc.VOID, List.of(v));\n+        }\n+    }\n+\n+    @OpDeclaration(DotOp.NAME)\n+    public static class DotOp extends TritonOp implements Op.Pure {\n+        public static final String NAME = \"tt.dot\";\n+\n+        public DotOp(OpDefinition def) {\n+            super(def);\n+        }\n+\n+        DotOp(DotOp that, CopyContext cc) {\n+            super(that, cc);\n+        }\n+\n+        @Override\n+        public DotOp transform(CopyContext cc, OpTransformer ot) {\n+            return new DotOp(this, cc);\n+        }\n+\n+        DotOp(TypeDesc tensorType, Value a, Value b) {\n+            super(NAME, tensorType, List.of(a, b));\n+        }\n+    }\n+\n+\n+    public static ModuleOp module(FuncOp... functions) {\n+        return module(List.of(functions));\n+    }\n+\n+    public static ModuleOp module(List<FuncOp> functions) {\n+        return new ModuleOp(List.copyOf(functions));\n+    }\n+\n+    public static FuncOp.Builder func(String funcName, MethodTypeDesc funcDescriptor) {\n+        return new FuncOp.Builder(null, funcName, funcDescriptor);\n+    }\n+\n+    public static FuncOp func(String funcName, Body.Builder body) {\n+        return new FuncOp(funcName, body);\n+    }\n+\n+    public static CallOp call(FuncOp func, Value... args) {\n+        return call(func, List.of(args));\n+    }\n+\n+    public static CallOp call(FuncOp func, List<Value> args) {\n+        return new CallOp(func.funcName(), func.funcDescriptor().returnType(), args);\n+    }\n+\n+    public static ReduceOp.Builder reduce(Body.Builder ancestorBody, int axis, Value tensor,\n+                                          MethodTypeDesc reduceDescriptor) {\n+        return new ReduceOp.Builder(ancestorBody, axis, tensor, reduceDescriptor);\n+    }\n+\n+    public static ReduceOp reduce(int axis, Value tensor, Body.Builder reducerBuilder) {\n+        return new ReduceOp(axis, tensor, reducerBuilder);\n+    }\n+\n+    public static ReduceReturnOp reduceReturn(Value r) {\n+        return new ReduceReturnOp(r);\n+    }\n+\n+    public static GetProgramIdOp getProgramId(int axis) {\n+        \/\/ @@@ 1 <= axis <= 3\n+        return new GetProgramIdOp(axis);\n+    }\n+\n+    public static MakeRangeOp makeRange(int start, int end) {\n+        \/\/ @@@ 0 <= start < end\n+        return new MakeRangeOp(start, end);\n+    }\n+\n+\n+    public static final OpFactory FACTORY = OpFactory.OP_FACTORY.get(TritonOps.class);\n+\n+    public static ExpandOp expand(int axis, TypeDesc tensorType, Value v) {\n+        return new ExpandOp(axis, tensorType, v);\n+    }\n+\n+    \/\/ v is scalar\n+    public static SplatOp splat(TypeDesc tensorType, Value v) {\n+        return new SplatOp(tensorType, v);\n+    }\n+\n+    \/\/ v is tensor\n+    public static BroadcastOp broadcast(TypeDesc tensorType, Value v) {\n+        return new BroadcastOp(tensorType, v);\n+    }\n+\n+    public static AddPtrOp addptr(Value ptr, Value offset) {\n+        return new AddPtrOp(ptr, offset);\n+    }\n+\n+    public static LoadOp load(TypeDesc tensorType, Value ptr, Value mask) {\n+        return new LoadOp(tensorType, ptr, mask);\n+    }\n+\n+    public static StoreOp store(Value ptr, Value v, Value mask) {\n+        return new StoreOp(ptr, v, mask);\n+    }\n+\n+    public static ReturnOp return_() {\n+        return new ReturnOp();\n+    }\n+\n+    public static ReturnOp return_(Value v) {\n+        return new ReturnOp(v);\n+    }\n+\n+    public static DotOp dot(TypeDesc tensorType, Value a, Value b) {\n+        return new DotOp(tensorType, a, b);\n+    }\n+}\n","filename":"cr-examples\/triton\/src\/main\/java\/oracle\/code\/triton\/TritonOps.java","additions":829,"deletions":0,"binary":false,"changes":829,"status":"added"},{"patch":"@@ -0,0 +1,77 @@\n+\/*\n+ * Copyright (c) 2024, Oracle and\/or its affiliates. All rights reserved.\n+ * DO NOT ALTER OR REMOVE COPYRIGHT NOTICES OR THIS FILE HEADER.\n+ *\n+ * This code is free software; you can redistribute it and\/or modify it\n+ * under the terms of the GNU General Public License version 2 only, as\n+ * published by the Free Software Foundation.  Oracle designates this\n+ * particular file as subject to the \"Classpath\" exception as provided\n+ * by Oracle in the LICENSE file that accompanied this code.\n+ *\n+ * This code is distributed in the hope that it will be useful, but WITHOUT\n+ * ANY WARRANTY; without even the implied warranty of MERCHANTABILITY or\n+ * FITNESS FOR A PARTICULAR PURPOSE.  See the GNU General Public License\n+ * version 2 for more details (a copy is included in the LICENSE file that\n+ * accompanied this code).\n+ *\n+ * You should have received a copy of the GNU General Public License version\n+ * 2 along with this work; if not, write to the Free Software Foundation,\n+ * Inc., 51 Franklin St, Fifth Floor, Boston, MA 02110-1301 USA.\n+ *\n+ * Please contact Oracle, 500 Oracle Parkway, Redwood Shores, CA 94065 USA\n+ * or visit www.oracle.com if you need additional information or have any\n+ * questions.\n+ *\/\n+\n+package oracle.code.triton;\n+\n+import java.lang.reflect.code.CopyContext;\n+import java.lang.reflect.code.OpTransformer;\n+import java.lang.reflect.code.Value;\n+import java.lang.reflect.code.descriptor.TypeDesc;\n+import java.lang.reflect.code.op.OpDeclaration;\n+import java.lang.reflect.code.op.OpDefinition;\n+import java.lang.reflect.code.op.OpFactory;\n+import java.lang.reflect.code.op.OpWithDefinition;\n+import java.util.List;\n+\n+public class TritonTestOps {\n+\n+    @OpDeclaration(ConsumeOp.NAME)\n+    public static class ConsumeOp extends OpWithDefinition {\n+        public static final String NAME = \"tt.consume\";\n+\n+        public ConsumeOp(OpDefinition def) {\n+            super(def);\n+        }\n+\n+        ConsumeOp(ConsumeOp that, CopyContext cc) {\n+            super(that, cc);\n+        }\n+\n+        @Override\n+        public ConsumeOp transform(CopyContext cc, OpTransformer ot) {\n+            return new ConsumeOp(this, cc);\n+        }\n+\n+        ConsumeOp(List<Value> values) {\n+            super(NAME, values);\n+        }\n+\n+        @Override\n+        public TypeDesc resultType() {\n+            return TypeDesc.VOID;\n+        }\n+    }\n+\n+\n+    public static final OpFactory FACTORY = OpFactory.OP_FACTORY.get(TritonTestOps.class);\n+\n+    public static ConsumeOp consume(Value... operands) {\n+        return consume(List.of(operands));\n+    }\n+\n+    public static ConsumeOp consume(List<Value> operands) {\n+        return new ConsumeOp(operands);\n+    }\n+}\n","filename":"cr-examples\/triton\/src\/main\/java\/oracle\/code\/triton\/TritonTestOps.java","additions":77,"deletions":0,"binary":false,"changes":77,"status":"added"},{"patch":"@@ -0,0 +1,1255 @@\n+\/*\n+ * Copyright (c) 2024, Oracle and\/or its affiliates. All rights reserved.\n+ * DO NOT ALTER OR REMOVE COPYRIGHT NOTICES OR THIS FILE HEADER.\n+ *\n+ * This code is free software; you can redistribute it and\/or modify it\n+ * under the terms of the GNU General Public License version 2 only, as\n+ * published by the Free Software Foundation.  Oracle designates this\n+ * particular file as subject to the \"Classpath\" exception as provided\n+ * by Oracle in the LICENSE file that accompanied this code.\n+ *\n+ * This code is distributed in the hope that it will be useful, but WITHOUT\n+ * ANY WARRANTY; without even the implied warranty of MERCHANTABILITY or\n+ * FITNESS FOR A PARTICULAR PURPOSE.  See the GNU General Public License\n+ * version 2 for more details (a copy is included in the LICENSE file that\n+ * accompanied this code).\n+ *\n+ * You should have received a copy of the GNU General Public License version\n+ * 2 along with this work; if not, write to the Free Software Foundation,\n+ * Inc., 51 Franklin St, Fifth Floor, Boston, MA 02110-1301 USA.\n+ *\n+ * Please contact Oracle, 500 Oracle Parkway, Redwood Shores, CA 94065 USA\n+ * or visit www.oracle.com if you need additional information or have any\n+ * questions.\n+ *\/\n+\n+package oracle.code.triton;\n+\n+import java.lang.invoke.MethodHandle;\n+import java.lang.invoke.MethodHandles;\n+import java.lang.reflect.Field;\n+import java.lang.reflect.Method;\n+import java.lang.reflect.Type;\n+import java.lang.reflect.code.*;\n+import java.lang.reflect.code.analysis.SSA;\n+import java.lang.reflect.code.descriptor.MethodTypeDesc;\n+import java.lang.reflect.code.descriptor.TypeDesc;\n+import java.lang.reflect.code.op.CoreOps;\n+import java.lang.reflect.code.op.ExtendedOps;\n+import java.util.*;\n+import java.util.concurrent.atomic.AtomicInteger;\n+import java.util.stream.Stream;\n+\n+import static java.lang.reflect.code.descriptor.MethodTypeDesc.methodType;\n+import static java.lang.reflect.code.op.CoreOps.*;\n+\n+public final class TritonTransformer {\n+    private TritonTransformer() {}\n+\n+    static final TypeDesc TYPE_Triton = TypeDesc.type(Triton.class);\n+\n+    static final TypeDesc TYPE_Triton_Test = TypeDesc.ofString(\"oracle.code.triton.TritonTest\");\n+\n+    static final TypeDesc TYPE_Tensor = TypeDesc.type(Tensor.class);\n+\n+    static final TypeDesc TYPE_J_L_MATH = TypeDesc.type(Math.class);\n+\n+    public static <O extends Op & Op.Invokable>\n+    TritonOps.ModuleOp tritonModule(O kernel,\n+                                    Type rType,\n+                                    List<Type> argTypes) {\n+        Map<String, TritonOps.FuncOp> fsymTable = new LinkedHashMap<>();\n+        tritonFunction(kernel, rType, argTypes, fsymTable);\n+        return TritonOps.module(fsymTable.values().stream().toList());\n+    }\n+\n+    public static <O extends Op & Op.Invokable>\n+    TritonOps.FuncOp tritonFunction(O javaKernel,\n+                                    Type rType,\n+                                    List<Type> argTypes,\n+                                    Map<String, TritonOps.FuncOp> fsymTable) {\n+        String name = (javaKernel instanceof FuncOp f) ? f.funcName() : \"kernel\";\n+        String signature = signature(name, rType, argTypes);\n+        if (fsymTable.containsKey(signature)) {\n+            return fsymTable.get(signature);\n+        }\n+\n+        System.out.println(javaKernel.toText());\n+\n+        Map<Value, Type> valueTypeMap = new HashMap<>();\n+        Map<Op, Object> opData = new HashMap<>();\n+        TritonTransformer.typeCheckKernel(javaKernel, argTypes, valueTypeMap, opData);\n+        TritonTransformer.printTypeMap(javaKernel, valueTypeMap);\n+\n+        return TritonTransformer.transformToTritonFunction(javaKernel, signature,\n+                rType, valueTypeMap, opData,\n+                fsymTable);\n+    }\n+\n+    static String signature(String name, Type rType, List<Type> argTypes) {\n+        StringBuilder sb = new StringBuilder(name);\n+\n+        for (Type argType : argTypes) {\n+            sb.append(\"_\");\n+            sb.append(argType);\n+        }\n+        sb.append(\"_\");\n+        sb.append(rType);\n+        return sb.toString();\n+    }\n+\n+    public static <O extends Op & Op.Invokable> void typeCheckKernel(\n+            O kernel, List<Type> argTypes,\n+            Map<Value, Type> valueTypeMap, Map<Op, Object> opData) {\n+        kernel.traverse(null, CodeElement.opVisitor((o, op) -> {\n+            switch (op) {\n+                case Op.Invokable fop -> {\n+                    List<Block.Parameter> parameters = fop.body().entryBlock().parameters();\n+                    for (int i = 0; i < parameters.size(); i++) {\n+                        valueTypeMap.put(parameters.get(i), argTypes.get(i));\n+                    }\n+                }\n+                case VarOp _ -> {\n+                    Value init = op.operands().get(0);\n+                    valueTypeMap.put(op.result(), valueTypeMap.get(init));\n+                }\n+                case VarAccessOp.VarLoadOp _ -> {\n+                    Value var = op.operands().get(0);\n+                    valueTypeMap.put(op.result(), valueTypeMap.get(var));\n+                }\n+                case VarAccessOp.VarStoreOp _ -> {\n+                    Value var = op.operands().get(0);\n+                    Type varType = valueTypeMap.get(var);\n+                    Value v = op.operands().get(1);\n+                    Type vType = valueTypeMap.get(v);\n+                    if (!varType.equals(vType)) {\n+                        throw new IllegalStateException(\"Storing to variable with different type: \"\n+                                + varType + \" <- \" + vType);\n+                    }\n+\n+                    valueTypeMap.put(op.result(), valueTypeMap.get(var));\n+                }\n+                case ConstantOp cop -> {\n+                    TypeDesc td = op.result().type();\n+                    Class<?> type;\n+                    try {\n+                        type = td.resolve(MethodHandles.lookup());\n+                    } catch (ReflectiveOperationException e) {\n+                        throw new UnsupportedOperationException(\"Unsupported type of constant op: \" + td, e);\n+                    }\n+                    if (type.isPrimitive()) {\n+                        valueTypeMap.put(op.result(), new ConstantType(type, cop.value()));\n+                    } else if (type == Class.class) {\n+                        TypeDesc vtd = (TypeDesc) cop.value();\n+                        Class<?> value;\n+                        try {\n+                            value = vtd.resolve(MethodHandles.lookup());\n+                        } catch (ReflectiveOperationException e) {\n+                            throw new RuntimeException(e);\n+                        }\n+                        valueTypeMap.put(op.result(), new ConstantType(type, value));\n+                    } else {\n+                        throw new UnsupportedOperationException(\"Unsupported type of constant op: \" + type);\n+                    }\n+                }\n+                case ArithmeticOperation _ -> {\n+                    Type t = checkWithTypeInterpreter(op, op.opName(), valueTypeMap);\n+                    valueTypeMap.put(op.result(), t);\n+                }\n+                case FieldAccessOp.FieldLoadOp flop -> {\n+                    if (!flop.operands().isEmpty()) {\n+                        throw new IllegalStateException(\"Unsupported field load: \" + flop.fieldDescriptor());\n+                    }\n+\n+                    Field f;\n+                    try {\n+                        f = flop.fieldDescriptor().resolveToMember(MethodHandles.lookup());\n+                    } catch (ReflectiveOperationException e) {\n+                        throw new IllegalStateException(\"Unsupported field load: \" + flop.fieldDescriptor(), e);\n+                    }\n+                    Object value;\n+                    try {\n+                        value = f.get(null);\n+                    } catch (IllegalAccessException e) {\n+                        throw new IllegalStateException(\"Unsupported field load: \" + f, e);\n+                    }\n+                    valueTypeMap.put(op.result(), new ConstantType(f.getType(), value));\n+                }\n+                case InvokeOp iop when iop.invokeDescriptor().refType().equals(TypeDesc.J_L_INTEGER) -> {\n+                    \/\/ Box\n+                    if (iop.invokeDescriptor().name().equals(\"valueOf\")) {\n+                        Value a = op.operands().get(0);\n+                        valueTypeMap.put(op.result(), valueTypeMap.get(a));\n+                    } else {\n+                        throw new UnsupportedOperationException(\"Unsupported invocation on Integer: \" + iop.invokeDescriptor());\n+                    }\n+                }\n+                case InvokeOp iop when iop.invokeDescriptor().refType().equals(TYPE_J_L_MATH) -> {\n+                    String name = iop.invokeDescriptor().name();\n+                    if (name.equals(\"max\") || name.equals(\"min\")) {\n+                        Value a = op.operands().get(0);\n+                        valueTypeMap.put(op.result(), valueTypeMap.get(a));\n+                    } else {\n+                        throw new UnsupportedOperationException(\"Unsupported invocation on Math: \" + iop.invokeDescriptor());\n+                    }\n+                }\n+                case InvokeOp iop when iop.invokeDescriptor().refType().equals(TYPE_Tensor) -> {\n+                    if (iop.invokeDescriptor().name().equals(\"type\")) {\n+                        Value a = op.operands().get(0);\n+                        valueTypeMap.put(op.result(), valueTypeMap.get(a));\n+                    } else {\n+                        throw new UnsupportedOperationException(\"Unsupported invocation on Tensor: \" + iop.invokeDescriptor());\n+                    }\n+                }\n+                case InvokeOp iop when iop.invokeDescriptor().refType().equals(TYPE_Triton) -> {\n+                    Type t = checkWithTypeInterpreter(op, iop.invokeDescriptor().name(), valueTypeMap);\n+                    valueTypeMap.put(op.result(), t);\n+                }\n+                case InvokeOp iop when iop.invokeDescriptor().refType().equals(TYPE_Triton_Test) -> {\n+                    Type t = checkWithTypeInterpreter(op, iop.invokeDescriptor().name(), valueTypeMap);\n+                    valueTypeMap.put(op.result(), t);\n+                }\n+                case ExtendedOps.JavaForOp fop -> {\n+                    SimpleCountedForLoopInfo li = new SimpleCountedForLoopInfo(fop);\n+                    opData.put(fop, li);\n+\n+                    TypeDesc type = fop.init().yieldType();\n+                    assert type.rawType().equals(Var.VAR_TYPE);\n+                    if (type.typeArguments().get(0).equals(TypeDesc.INT)) {\n+                        for (Body b : List.of(fop.cond(), fop.update(), fop.loopBody())) {\n+                            valueTypeMap.put(b.entryBlock().parameters().get(0), int.class);\n+                        }\n+                    } else {\n+                        throw new IllegalStateException();\n+                    }\n+                }\n+                case TestOperation _ -> {\n+                }\n+                case ExtendedOps.JavaContinueOp _ -> {\n+                }\n+                case YieldOp _ -> {\n+                }\n+                case ReturnOp _ -> {\n+                }\n+                default -> throw new UnsupportedOperationException(\"Unsupported operation: \" + op);\n+            }\n+\n+            return null;\n+        }));\n+    }\n+\n+    static Type checkWithTypeInterpreter(Op op, String name, Map<Value, Type> valueTypeMap) {\n+        \/\/ Obtain associated type-based method\n+        MethodHandle mh;\n+        try {\n+            Optional<Method> om = Stream.of(TritonTypeInterpreter.class.getDeclaredMethods())\n+                    .filter(m -> m.getName().equals(name))\n+                    .findFirst();\n+            mh = MethodHandles.lookup().unreflect(\n+                    om.orElseThrow(() -> new NoSuchMethodException(name)));\n+        } catch (ReflectiveOperationException e) {\n+            throw new IllegalStateException(name, e);\n+        }\n+\n+        \/\/ Invoke with the values' types\n+        List<Type> operandTypes = op.operands().stream().map(valueTypeMap::get).toList();\n+        try {\n+            return (Type) mh.invokeWithArguments(operandTypes.toArray(Object[]::new));\n+        } catch (Throwable e) {\n+            throw new IllegalStateException(mh.toString(), e);\n+        }\n+    }\n+\n+    \/\/ @@@ type check tensor shapes\n+    static class TritonTypeInterpreter {\n+        private TritonTypeInterpreter() {\n+        }\n+\n+        \/\/                 int programId(@Constant int axis) {\n+        public static Class<?> programId(ConstantType axis) {\n+            assert axis.cType().equals(int.class);\n+            int axisValue = (int) axis.value();\n+            if (axisValue < 0 || axisValue > 3) {\n+                throw new IllegalStateException();\n+            }\n+\n+            return int.class;\n+        }\n+\n+        \/\/                Tensor arange(@Constant int start, @Constant int end)\n+        public static TensorType arange(ConstantType start, ConstantType end) {\n+            assert start.cType().equals(int.class);\n+            assert end.cType().equals(int.class);\n+\n+            int startValue = (int) start.value();\n+            int endValue = (int) end.value();\n+\n+            return new TensorType(int.class, List.of(endValue - startValue));\n+        }\n+\n+        \/\/                Tensor expand(Tensor a, int axis) {\n+        public static TensorType expand(TensorType a, ConstantType axis) {\n+            assert axis.cType().equals(int.class);\n+            int axisValue = (int) axis.value();\n+\n+            List<Integer> s = new ArrayList<>(a.shape());\n+            if (axisValue < s.size()) {\n+                s.add(axisValue, 1);\n+            } else {\n+                for (int i = 0; i <= (axisValue - s.size()); i++) {\n+                    s.add(1);\n+                }\n+            }\n+            return new TensorType(a.eType(), s);\n+        }\n+\n+        \/\/                Tensor load(Tensor ptr, Tensor mask)\n+        public static TensorType load(TensorType ptr, TensorType mask) {\n+            if (ptr.eType() instanceof PtrType eptr) {\n+                return new TensorType(eptr.rType(), ptr.shape());\n+            }\n+\n+            throw new IllegalStateException();\n+        }\n+\n+        \/\/            void store(Tensor ptr, Tensor value, Tensor mask)\n+        public static void store(TensorType ptr, TensorType value, TensorType mask) {\n+            if (!(ptr.eType() instanceof PtrType)) {\n+                throw new IllegalStateException();\n+            }\n+        }\n+\n+        \/\/                Tensor zeros(TensorType type)\n+        public static TensorType zeros(ConstantType eType, ConstantType... cShape) {\n+            List<Integer> shape = Stream.of(cShape).map(s -> (int) s.value()).toList();\n+            return new TensorType((Type) eType.value(), shape);\n+        }\n+\n+        \/\/                Tensor broadcast(Object o, TensorType type)\n+        public static TensorType broadcast(Type o, TensorType type) {\n+            if (o instanceof TensorType ot) {\n+                \/\/ @@@\n+                if (ot.shape().size() != type.shape().size()) {\n+                    throw new IllegalStateException();\n+                }\n+                o = ot.eType();\n+            } if (o instanceof ConstantType oc) {\n+                o = oc.cType();\n+            }\n+            return new TensorType(o, type.shape());\n+        }\n+\n+        public static TensorType joinShape(TensorType a, TensorType b) {\n+            return checkTensorTypes(a, b);\n+        }\n+\n+        \/\/          Tensor add(Number a, Number b)\n+        \/\/             Ptr add(Ptr a, int offset)\n+        public static Type add(Type a, Type b) {\n+            \/\/ @@@ Pass additional argument for checking ptr\n+            return binary(a, b);\n+        }\n+\n+        public static Type sub(Type a, Type b) {\n+            return binary(a, b);\n+        }\n+\n+        public static Type mul(Type a, Type b) {\n+            return binary(a, b);\n+        }\n+\n+        public static Type div(Type a, Type b) {\n+            return binary(a, b);\n+        }\n+\n+        public static Type mod(Type a, Type b) {\n+            return binary(a, b);\n+        }\n+\n+        public static Type and(Type a, Type b) {\n+            return binary(a, b);\n+        }\n+\n+        public static Type cdiv(Type a, Type b) {\n+            a = reduceScalarType(a);\n+            b = reduceScalarType(a);\n+            if (a != int.class && b != int.class) {\n+                throw new IllegalStateException();\n+            }\n+            return a;\n+        }\n+\n+        \/\/          Number conv(Type t, Number a) {\n+        public static Type conv(ConstantType eType, Type a) {\n+            return convTypes(eType, a);\n+        }\n+\n+        public static Type convTypes(ConstantType eType, Type a) {\n+            if (a instanceof TensorType tb) {\n+                Type e = convScalarTypes(eType, tb.eType());\n+                return new TensorType(e, tb.shape());\n+            } else {\n+                return convScalarTypes(eType, a);\n+            }\n+        }\n+\n+        public static Type convScalarTypes(ConstantType eType, Type a) {\n+            Type t = (Type) eType.value();\n+            if (t == Float16.class && a == float.class) {\n+                return Float16.class;\n+            } else if (t.equals(a)) {\n+                return t;\n+            } else {\n+                \/\/ @@@ Conversions;\n+                throw new IllegalStateException();\n+            }\n+        }\n+\n+        \/\/          Tensor exp(Tensor a)\n+        public static Type exp(Type a) {\n+            return unary(a);\n+        }\n+\n+        static Type unary(Type a) {\n+            return a;\n+        }\n+\n+        \/\/                Tensor compare(Number a, Number b, @Constant CompareKind ck) {\n+        public static Type compare(Type a, Type b, ConstantType kind) {\n+            assert kind.cType().equals(Triton.CompareKind.class);\n+\n+            return binary(a, b);\n+        }\n+\n+        \/\/                Tensor dot(Tensor a, Tensor b)\n+        public static TensorType dot(TensorType a, TensorType b) {\n+            if (a.shape().size() != 2 || b.shape().size() != 2) {\n+                throw new IllegalStateException();\n+            }\n+\n+            if (!a.shape().get(1).equals(b.shape().get(0))) {\n+                throw new IllegalStateException();\n+            }\n+\n+            if (a.eType() != b.eType()) {\n+                \/\/ @@@ Conversion, type checking\n+                throw new IllegalStateException();\n+            }\n+\n+            \/\/ Computed result is tensor of floats, regardless of inputs\n+            return new TensorType(float.class, List.of(a.shape().get(0), b.shape().get(1)));\n+        }\n+\n+\n+        \/\/                Tensor max(Tensor a, @Constant int axis) {\n+        public static Type max(TensorType a, ConstantType axis) {\n+            return reduce(a, axis);\n+        }\n+\n+        \/\/                Tensor sum(Tensor a, @Constant int axis) {\n+        public static Type sum(TensorType a, ConstantType axis) {\n+            return reduce(a, axis);\n+        }\n+\n+        static Type reduce(TensorType a, ConstantType axis) {\n+            assert axis.cType().equals(int.class);\n+            int axisValue = (int) axis.value();\n+            if (axisValue < 0 || axisValue > 3) {\n+                throw new IllegalStateException();\n+            }\n+\n+            List<Integer> reduceShape = new ArrayList<>();\n+            for (int i = 0; i < a.shape().size(); i++) {\n+                if (i != axisValue) {\n+                    reduceShape.add(a.shape().get(i));\n+                } else {\n+                    reduceShape.add(1);\n+                }\n+            }\n+\n+            if (reduceShape.size() == 1 && reduceShape.getFirst() == 1) {\n+                return a.eType();\n+            } else {\n+                return new TensorType(a.eType(), reduceShape);\n+            }\n+        }\n+\n+        \/\/ @@@ Test\n+        public static void consume(Type a) {\n+        }\n+\n+\n+        static Type binary(Type a, Type b) {\n+            if (a instanceof TensorType ta && b instanceof TensorType tb) {\n+                return checkTensorTypes(ta, tb);\n+            } else if (a instanceof TensorType ta) {\n+                return new TensorType(checkScalarTypes(ta.eType(), b), ta.shape());\n+            } else if (b instanceof TensorType tb) {\n+                return new TensorType(checkScalarTypes(a, tb.eType()), tb.shape());\n+            } else {\n+                return checkScalarTypes(a, b);\n+            }\n+        }\n+\n+        static TensorType checkTensorTypes(TensorType a, TensorType b) {\n+            if (a.shape().size() != b.shape().size()) {\n+                \/\/ Shape mismatch\n+                throw new IllegalStateException();\n+            }\n+\n+            List<Integer> s = new ArrayList<>();\n+            for (int i = 0; i < a.shape().size(); i++) {\n+                int ad = a.shape().get(i);\n+                int bd = b.shape().get(i);\n+\n+                \/\/ Expand dimensions\n+                int d;\n+                if (ad == bd) {\n+                    d = ad;\n+                } else {\n+                    if (ad != 1 && bd == 1) {\n+                        d = ad;\n+                    } else if (ad == 1) {\n+                        d = bd;\n+                    } else {\n+                        \/\/ Shape mismatch\n+                        throw new IllegalStateException();\n+                    }\n+                }\n+\n+                s.add(d);\n+            }\n+\n+            Type e = checkScalarTypes(a.eType(), b.eType());\n+            return new TensorType(e, s);\n+        }\n+\n+        static Type checkScalarTypes(Type a, Type b) {\n+            \/\/ @@@ Optional ptr checking\n+            if (a instanceof PtrType) {\n+                if (b != int.class) {\n+                    throw new IllegalStateException();\n+                }\n+            } else if (b instanceof PtrType) {\n+                \/\/ Pointer must be first argument\n+                throw new IllegalStateException();\n+            } else if (a instanceof ConstantType || b instanceof ConstantType) {\n+                return checkScalarTypes(reduceScalarType(a), reduceScalarType(b));\n+            } else if (a != b) {\n+                \/\/ @@@ Conversion\n+                throw new IllegalStateException();\n+            }\n+            return a;\n+        }\n+\n+        static Type reduceScalarType(Type a) {\n+            return a instanceof ConstantType ct ? ct.cType() : a;\n+        }\n+    }\n+\n+    public static <O extends Op & Op.Invokable> TritonOps.FuncOp transformToTritonFunction(\n+            O kernel,\n+            String signature,\n+            Type rType,\n+            Map<Value, Type> valueTypeMap, Map<Op, Object> opData,\n+            Map<String, TritonOps.FuncOp> fsymTable) {\n+        TritonOps.FuncOp ttKernel = TritonOps.func(signature, MethodTypeDesc.methodType(TritonType.fromType(rType)))\n+                .body(fblock -> {\n+                    \/\/ Process kernel parameters\n+                    List<Value> args = new ArrayList<>();\n+                    for (Block.Parameter kp : kernel.body().entryBlock().parameters()) {\n+                        Type type = valueTypeMap.get(kp);\n+                        if (type instanceof ConstantType ct) {\n+                            \/\/ Constant\n+                            Op.Result cr = fblock.op(ArithMathOps.constant(ct.toDesc(), ct.value()));\n+                            args.add(cr);\n+                        } else {\n+                            args.add(fblock.parameter(TritonType.fromType(type)));\n+                        }\n+                    }\n+\n+                    \/\/ Transform kernel body\n+                    fblock.transformBody(kernel.body(), args, (kblock, op) -> {\n+                        return transformToTritonOperation(kblock, op, valueTypeMap, opData, fsymTable);\n+                    });\n+                });\n+\n+        ttKernel = cleanup(ttKernel);\n+        fsymTable.put(ttKernel.funcName(), ttKernel);\n+        return ttKernel;\n+    }\n+\n+    static Block.Builder transformToTritonOperation(Block.Builder kblock, Op op,\n+                                                    Map<Value, Type> valueTypeMap, Map<Op, Object> opData,\n+                                                    Map<String, TritonOps.FuncOp> fsymTable) {\n+        \/\/ @@@ Avoid constructing for each operation -- block builder passed as argument or a scoped value\n+        TritonBuilderInterpreter tbi = new TritonBuilderInterpreter(fsymTable, kblock);\n+        CopyContext cc = kblock.context();\n+        switch (op) {\n+            case VarOp varOp -> {\n+                \/\/ @@@ Cannot copy op because the result type\n+                \/\/     is derived from init type\n+                Value init = cc.getValue(op.operands().get(0));\n+                Op.Result r = kblock.op(var(varOp.varName(), init));\n+                cc.mapValue(op.result(), r);\n+            }\n+            case ConstantOp cop -> {\n+                Type t = valueTypeMap.get(cop.result());\n+                if (t instanceof ConstantType ct) {\n+                    Op.Result r = kblock.op(ArithMathOps.constant(ct.toDesc(), ct.value()));\n+                    cc.mapValue(op.result(), r);\n+                } else {\n+                    kblock.op(op);\n+                }\n+            }\n+            case ArithmeticOperation _ -> {\n+                Value result = tbi.build(op, op.opName(), valueTypeMap);\n+                if (result != null) {\n+                    cc.mapValue(op.result(), result);\n+                }\n+            }\n+            case InvokeOp iop when iop.invokeDescriptor().refType().equals(TypeDesc.J_L_INTEGER) -> {\n+                \/\/ Replace box with its value\n+                Value a = cc.getValue(op.operands().get(0));\n+                cc.mapValue(op.result(), a);\n+            }\n+            case InvokeOp iop when iop.invokeDescriptor().refType().equals(TYPE_J_L_MATH) -> {\n+                String name = iop.invokeDescriptor().name();\n+                if (name.equals(\"max\")) {\n+                    Value a = cc.getValue(op.operands().get(0));\n+                    Value b = cc.getValue(op.operands().get(1));\n+\n+                    Op.Result result = kblock.op(ArithMathOps.maximum(a, b));\n+                    cc.mapValue(op.result(), result);\n+                } else if (name.equals(\"min\")) {\n+                    Value a = cc.getValue(op.operands().get(0));\n+                    Value b = cc.getValue(op.operands().get(1));\n+\n+                    Op.Result result = kblock.op(ArithMathOps.minimum(a, b));\n+                    cc.mapValue(op.result(), result);\n+                }\n+            }\n+            case InvokeOp iop when iop.invokeDescriptor().refType().equals(TYPE_Tensor) -> {\n+                if (iop.invokeDescriptor().name().equals(\"type\")) {\n+                    \/\/ Replace with constant operation to produce tensor type.\n+                    \/\/ Result may be used, but transitively it will be removed due to no uses\n+                    \/\/ contributing to the computation\n+                    Value a = op.operands().get(0);\n+                    TensorType aType = (TensorType) valueTypeMap.get(a);\n+                    Op.Result result = kblock.op(CoreOps.constant(iop.resultType(), aType.toDesc()));\n+                    cc.mapValue(op.result(), result);\n+                    valueTypeMap.put(result, aType);\n+                }\n+                \/\/ Remove\n+            }\n+            case InvokeOp iop when iop.invokeDescriptor().refType().equals(TYPE_Triton) -> {\n+                Value result = tbi.build(op, iop.invokeDescriptor().name(), valueTypeMap);\n+                if (result != null) {\n+                    cc.mapValue(op.result(), result);\n+                }\n+            }\n+            case InvokeOp iop when iop.invokeDescriptor().refType().equals(TYPE_Triton_Test) -> {\n+                Value result = tbi.build(op, iop.invokeDescriptor().name(), valueTypeMap);\n+                if (result != null) {\n+                    cc.mapValue(op.result(), result);\n+                }\n+            }\n+            case ExtendedOps.JavaForOp fop -> {\n+                transformToSCFFor(cc, kblock, fop, valueTypeMap, opData, fsymTable);\n+            }\n+            case ReturnOp rop -> {\n+                if (rop.operands().isEmpty()) {\n+                    kblock.op(TritonOps.return_());\n+                } else {\n+                    kblock.op(TritonOps.return_(\n+                            cc.getValue(rop.returnValue())));\n+                }\n+            }\n+            default -> kblock.op(op);\n+        };\n+        return kblock;\n+    }\n+\n+    static void transformToSCFFor(CopyContext cc, Block.Builder kblock, ExtendedOps.JavaForOp fop,\n+                                  Map<Value, Type> valueTypeMap, Map<Op, Object> opData,\n+                                  Map<String, TritonOps.FuncOp> fsymTable) {\n+        Body body = fop.loopBody();\n+\n+        \/\/ Hoist expressions for start, end, and step\n+        SimpleCountedForLoopInfo li = (SimpleCountedForLoopInfo) opData.get(fop);\n+        Value start = null;\n+        for (Op o : li.startExpression()) {\n+            transformToTritonOperation(kblock, o, valueTypeMap, opData, fsymTable);\n+            start = cc.getValue(o.result());\n+        }\n+        Value end = null;\n+        for (Op o : li.endExpression()) {\n+            transformToTritonOperation(kblock, o, valueTypeMap, opData, fsymTable);\n+            end = cc.getValue(o.result());\n+        }\n+        Value step = null;\n+        for (Op o : li.stepExpression()) {\n+            transformToTritonOperation(kblock, o, valueTypeMap, opData, fsymTable);\n+            step = cc.getValue(o.result());\n+        }\n+\n+        \/\/ Obtain captured vars\n+        \/\/ true == stores\n+        \/\/ false == loads only\n+        Map<Boolean, Set<Value>> capturedVars = capturedVars(body);\n+        Set<Value> capturedAndStoredVars = capturedVars.get(true);\n+\n+        \/\/ Get load values\n+        \/\/ Loaded values are hoisted out of the loop body\n+        Map<Value, Value> loadValues = new HashMap<>();\n+        for (Value v : capturedVars.get(false)) {\n+            Value load = kblock.op(varLoad(cc.getValue(v)));\n+            valueTypeMap.put(load, valueTypeMap.get(v));\n+            loadValues.put(v, load);\n+        }\n+\n+        \/\/ Get iteration values -- represented by captured vars that are stored to in the loop\n+        \/\/ The SCF for operation returns the iteration values of the last loop iteration, which\n+        \/\/ are then to be stored to the iteration variables\n+        List<Value> iterValues = new ArrayList<>();\n+        for (Value v : capturedAndStoredVars) {\n+            iterValues.add(kblock.op(varLoad(cc.getValue(v))));\n+        }\n+\n+        \/\/ @@@ Build in java code model, then transform?\n+        SCFOps.ForOp scffor = SCFOps.for_(kblock.parentBody(), start, end, step, iterValues)\n+                \/\/ Ensure existing context is used\n+                .body(CopyContext.create(cc), builder -> {\n+                    \/\/ Create index var initialized from entry block parameter\n+                    Value index = builder.parameters().get(0);\n+                    valueTypeMap.put(index, int.class);\n+                    Value varIndex = builder.op(var(\"index\", index));\n+                    valueTypeMap.put(varIndex, int.class);\n+                    builder.context().mapValue(body.entryBlock().parameters().get(0), varIndex);\n+\n+                    \/\/ Create iter vars initialized from entry block parameters\n+                    int pi = 1;\n+                    for (Value v : capturedAndStoredVars) {\n+                        Type type = valueTypeMap.get(v);\n+                        Value iter = builder.parameters().get(pi++);\n+                        valueTypeMap.put(iter, type);\n+                        Value varIter = builder.op(var(Integer.toString(pi), iter));\n+                        valueTypeMap.put(varIter, type);\n+                        builder.context().mapValue(v, varIter);\n+                    }\n+\n+                    \/\/ Transform the Java for body into the SCF for body\n+                    builder.transformBody(body, List.of(), (block, op) -> {\n+                        \/\/ Yield iter values\n+                        if (op instanceof ExtendedOps.JavaContinueOp) {\n+                            \/\/ Replace with yield of loaded vars\n+                            List<Value> yieldValues = new ArrayList<>();\n+                            for (Value value : capturedAndStoredVars) {\n+                                Value varIter = block.context().getValue(value);\n+                                Value v = block.op(varLoad(varIter));\n+                                yieldValues.add(v);\n+                            }\n+                            block.op(SCFOps.yield_(yieldValues));\n+                        } else if (op instanceof VarAccessOp.VarLoadOp) {\n+                            \/\/ Replace with value loaded immediately before loop\n+                            Value v = op.operands().get(0);\n+                            if (capturedVars.get(false).contains(v)) {\n+                                block.context().mapValue(op.result(), loadValues.get(v));\n+                            } else {\n+                                block.op(op);\n+                            }\n+                        } else {\n+                            block = transformToTritonOperation(block, op, valueTypeMap, opData, fsymTable);\n+                        }\n+                        return block;\n+                    });\n+                });\n+        Op.Result forResult = kblock.op(scffor);\n+\n+        \/\/ Assign back result to iter vars\n+        if (capturedAndStoredVars.size() == 1) {\n+            for (Value v : capturedAndStoredVars) {\n+                kblock.op(varStore(cc.getValue(v), forResult));\n+            }\n+        } else {\n+            int i = 0;\n+            for (Value v : capturedAndStoredVars) {\n+                kblock.op(varStore(cc.getValue(v),\n+                        kblock.op(tupleLoad(forResult, i++))));\n+            }\n+        }\n+    }\n+\n+    static Map<Boolean, Set<Value>> capturedVars(Body body) {\n+        Map<Boolean, Set<Value>> capturedValues = new HashMap<>();\n+        capturedValues.put(false, new LinkedHashSet<>());\n+        capturedValues.put(true, new LinkedHashSet<>());\n+\n+        capturedVars(capturedValues, new ArrayDeque<>(), body);\n+        return capturedValues;\n+    }\n+\n+    static void capturedVars(Map<Boolean, Set<Value>> capturedVars, Deque<Body> bodyStack, Body body) {\n+        bodyStack.push(body);\n+\n+        for (Block b : body.blocks()) {\n+            for (Op op : b.ops()) {\n+                \/\/ @@@ Nested bodies\n+                if (!op.bodies().isEmpty()) {\n+                    throw new IllegalStateException();\n+                }\n+\/\/                for (Body childBody : op.bodies()) {\n+\/\/                    capturedAndUpdatedVars(capturedValues, bodyStack, childBody);\n+\/\/                }\n+\n+                if (op instanceof VarAccessOp) {\n+                    Value v = op.operands().get(0);\n+                    if (!bodyStack.contains(v.declaringBlock().parentBody())) {\n+                        if (op instanceof VarAccessOp.VarStoreOp) {\n+                            capturedVars.get(true).add(v);\n+                            capturedVars.get(false).remove(v);\n+                        } else if (!capturedVars.get(true).contains(v)) {\n+                            capturedVars.get(false).add(v);\n+                        }\n+                    }\n+                }\n+            }\n+        }\n+\n+        bodyStack.pop();\n+    }\n+\n+    public static final ScopedValue<Boolean> SV_SSA = ScopedValue.newInstance();\n+\n+    static TritonOps.FuncOp cleanup(TritonOps.FuncOp f) {\n+        \/\/ Remove var ops\n+        boolean doSSA = SV_SSA.isBound() ? SV_SSA.get() : true;\n+        if (doSSA) {\n+            f = SSA.transform(f);\n+        }\n+        \/\/ Remove unsued ops\n+        f = f.transform((fblock, op) -> {\n+            if (op instanceof Op.Pure && op.result().uses().isEmpty()) {\n+                return fblock;\n+            } else if (op instanceof VarAccessOp.VarLoadOp && op.result().uses().isEmpty()) {\n+                return fblock;\n+            }\n+\n+            fblock.op(op);\n+            return fblock;\n+        });\n+        return f;\n+    }\n+\n+    static class TritonBuilderInterpreter {\n+        final Map<String, TritonOps.FuncOp> fsymTable;\n+        final Block.Builder block;\n+\n+        TritonBuilderInterpreter(Map<String, TritonOps.FuncOp> fsymTable, Block.Builder block) {\n+            this.fsymTable = fsymTable;\n+            this.block = block;\n+        }\n+\n+        Value build(Op op, String name, Map<Value, Type> valueTypeMap) {\n+            \/\/ Obtain associated type-based method\n+            MethodHandle mh;\n+            try {\n+                Optional<Method> om = Stream.of(TritonBuilderInterpreter.class.getDeclaredMethods())\n+                        .filter(m -> m.getName().equals(name))\n+                        .findFirst();\n+                mh = MethodHandles.lookup().unreflect(\n+                        om.orElseThrow(() -> new NoSuchMethodException(name)));\n+            } catch (ReflectiveOperationException e) {\n+                throw new IllegalStateException(e);\n+            }\n+\n+            List<Object> iArgs = new ArrayList<>();\n+            iArgs.add(this);\n+            iArgs.add(valueTypeMap.get(op.result()));\n+            iArgs.add(op.result());\n+            for (Value o : op.operands()) {\n+                iArgs.add(valueTypeMap.get(o));\n+                iArgs.add(o);\n+            }\n+            try {\n+                return (Value) mh.invokeWithArguments(iArgs.toArray(Object[]::new));\n+            } catch (Throwable e) {\n+                throw new IllegalStateException(e);\n+            }\n+        }\n+\n+\n+        public Value programId(Type rType, Op.Result r,\n+                               ConstantType axisType, Value axis) {\n+            return block.op(TritonOps.getProgramId(\n+                    (int) axisType.value()));\n+        }\n+\n+        public Value arange(TensorType rType, Op.Result r,\n+                            ConstantType startType, Value start,\n+                            ConstantType endType, Value end) {\n+            return block.op(TritonOps.makeRange(\n+                    (int) startType.value(),\n+                    (int) endType.value()));\n+        }\n+\n+        public Value expand(TensorType rType, Op.Result r,\n+                            TensorType aType, Value a,\n+                            ConstantType axisType, Value axis) {\n+            return block.op(TritonOps.expand(\n+                    (int) axisType.value(),\n+                    rType.toDesc(),\n+                    block.context().getValue(a)));\n+        }\n+\n+        public Value zeros(TensorType rType, Op.Result r,\n+                           ConstantType aType, Value a,\n+                           Object... constantsAndValues) {\n+            Object zero;\n+            try {\n+                zero = MethodHandles.zero((Class<?>) aType.value()).invoke();\n+            } catch (Throwable e) {\n+                throw new RuntimeException(e);\n+            }\n+            return block.op(ArithMathOps.constant(rType.toDesc(), zero));\n+        }\n+\n+        public Value load(TensorType rType, Op.Result r,\n+                          TensorType ptrType, Value ptr,\n+                          TensorType maskType, Value mask) {\n+            broadcastConversionRight(ptrType, maskType, mask);\n+            return block.op(TritonOps.load(\n+                    rType.toDesc(),\n+                    block.context().getValue(ptr),\n+                    block.context().getValue(mask)));\n+        }\n+\n+        public Value store(TensorType rType, Op.Result r,\n+                           TensorType ptrType, Value ptr,\n+                           TensorType valueType, Value value,\n+                           TensorType maskType, Value mask) {\n+            broadcastConversionRight(ptrType, valueType, value);\n+            broadcastConversionRight(ptrType, maskType, mask);\n+            return block.op(TritonOps.store(\n+                    block.context().getValue(ptr),\n+                    block.context().getValue(value),\n+                    block.context().getValue(mask)));\n+        }\n+\n+        public Value broadcast(TensorType rType, Op.Result r,\n+                               Type oType, Value o,\n+                               TensorType tensorTypeType, Value tensorType) {\n+            \/\/ @@@ tt.splat with scalar operand, tt.broadcast with tensor operand\n+            if (oType instanceof TensorType) {\n+                return block.op(TritonOps.broadcast(\n+                        rType.toDesc(),\n+                        block.context().getValue(o)));\n+            } else {\n+                return block.op(TritonOps.splat(\n+                        rType.toDesc(),\n+                        block.context().getValue(o)));\n+            }\n+        }\n+\n+        public Value joinShape(TensorType rType, Op.Result r,\n+                               TensorType aType, Value a,\n+                               TensorType bType, Value b) {\n+            \/\/ Replace with constant operation to produce tensor type.\n+            \/\/ Result may be used, but transitively it will be removed due to no uses\n+            \/\/ contributing to the computation\n+            return block.op(CoreOps.constant(TypeDesc.type(TensorType.class), r.type()));\n+        }\n+\n+\n+        public Value add(Type rType, Op.Result r,\n+                         Type aType, Value a,\n+                         Type bType, Value b) {\n+            broadcastConversion(rType, aType, a, bType, b);\n+            a = block.context().getValue(a);\n+            b = block.context().getValue(b);\n+\n+            if (rType instanceof PtrType ||\n+                    rType instanceof TensorType t && t.eType() instanceof PtrType) {\n+                return block.op(TritonOps.addptr(a, b));\n+            } else {\n+                return block.op(ArithMathOps.add(a, b));\n+            }\n+        }\n+\n+        public Value sub(Type rType, Op.Result r,\n+                         Type aType, Value a,\n+                         Type bType, Value b) {\n+            broadcastConversion(rType, aType, a, bType, b);\n+            a = block.context().getValue(a);\n+            b = block.context().getValue(b);\n+\n+            return block.op(ArithMathOps.sub(a, b));\n+        }\n+\n+        public Value mul(Type rType, Op.Result r,\n+                         Type aType, Value a,\n+                         Type bType, Value b) {\n+            broadcastConversion(rType, aType, a, bType, b);\n+            a = block.context().getValue(a);\n+            b = block.context().getValue(b);\n+\n+            return block.op(ArithMathOps.mul(a, b));\n+        }\n+\n+        public Value div(Type rType, Op.Result r,\n+                         Type aType, Value a,\n+                         Type bType, Value b) {\n+            broadcastConversion(rType, aType, a, bType, b);\n+            a = block.context().getValue(a);\n+            b = block.context().getValue(b);\n+\n+            return block.op(ArithMathOps.div(a, b));\n+        }\n+\n+        public Value mod(Type rType, Op.Result r,\n+                         Type aType, Value a,\n+                         Type bType, Value b) {\n+            broadcastConversion(rType, aType, a, bType, b);\n+            a = block.context().getValue(a);\n+            b = block.context().getValue(b);\n+\n+            return block.op(ArithMathOps.rem(a, b));\n+        }\n+\n+        public Value and(Type rType, Op.Result r,\n+                         Type aType, Value a,\n+                         Type bType, Value b) {\n+            broadcastConversion(rType, aType, a, bType, b);\n+            a = block.context().getValue(a);\n+            b = block.context().getValue(b);\n+\n+            return block.op(ArithMathOps.and(a, b));\n+        }\n+\n+        public Value dot(TensorType rType, Op.Result r,\n+                         Type aType, Value a,\n+                         Type bType, Value b) {\n+            a = block.context().getValue(a);\n+            b = block.context().getValue(b);\n+\n+            return block.op(TritonOps.dot(rType.toDesc(), a, b));\n+        }\n+\n+        public Value cdiv(Type rType, Op.Result r,\n+                          Type aType, Value a,\n+                          Type bType, Value b) {\n+            a = block.context().getValue(a);\n+            b = block.context().getValue(b);\n+\n+            TritonOps.FuncOp cdiv = tritonFunction(Functions.getJavaCodeModel(\"cdiv\"),\n+                    rType, List.of(aType, bType),\n+                    fsymTable);\n+            \/\/ @@@ Generalize\n+            List<Value> args = new ArrayList<>();\n+            if (!(aType instanceof ConstantType)) {\n+                args.add(a);\n+            }\n+            if (!(bType instanceof ConstantType)) {\n+                args.add(b);\n+            }\n+            return block.op(TritonOps.call(cdiv, args));\n+        }\n+\n+        public Value conv(Type rType, Op.Result r,\n+                          ConstantType tType, Value t,\n+                          Type aType, Value a) {\n+            a = block.context().getValue(a);\n+\n+            Type rScalarType;\n+            Type aScalarType;\n+            if (rType instanceof TensorType rTensorType && aType instanceof TensorType aTensorType) {\n+                rScalarType = rTensorType.eType();\n+                aScalarType = aTensorType.eType();\n+            } else {\n+                rScalarType = rType;\n+                aScalarType = aType;\n+            }\n+\n+            if (rScalarType == Float16.class && aScalarType == float.class) {\n+                return block.op(ArithMathOps.trunc(TritonType.fromType(rType), a));\n+            } else if (rType.equals(aType)) {\n+                return a;\n+            } else {\n+                throw new IllegalStateException();\n+            }\n+        }\n+\n+        public Value exp(TritonType rType, Op.Result r,\n+                         TritonType aType, Value a) {\n+            return block.op(ArithMathOps.exp(\n+                    block.context().getValue(a)));\n+        }\n+\n+        public Value compare(TensorType rType, Op.Result r,\n+                             Type aType, Value a,\n+                             Type bType, Value b,\n+                             ConstantType compareType, Value compare) {\n+            Triton.CompareKind ck = (Triton.CompareKind) compareType.value();\n+\n+            ArithMathOps.CompareOp.CompareKind ack = switch (ck) {\n+                case LessThan -> ArithMathOps.CompareOp.CompareKind.slt;\n+                default -> throw new UnsupportedOperationException(\"Unsupported comparison: \" + ck);\n+            };\n+\n+            broadcastConversion(rType, aType, a, bType, b);\n+            a = block.context().getValue(a);\n+            b = block.context().getValue(b);\n+\n+            return block.op(ArithMathOps.cmp(ack, a, b));\n+        }\n+\n+\n+        public Value max(Type rType, Op.Result r,\n+                         TensorType xType, Value x,\n+                         ConstantType axisType, Value axis) {\n+            TritonOps.FuncOp f = tritonFunction(Functions.getJavaCodeModel(\"max\"),\n+                    rType, List.of(rType, rType), fsymTable);\n+            return reduce(rType, r, xType, x, axisType, axis, f);\n+        }\n+\n+        public Value sum(Type rType, Op.Result r,\n+                         TensorType xType, Value x,\n+                         ConstantType axisType, Value axis) {\n+            TritonOps.FuncOp f = tritonFunction(Functions.getJavaCodeModel(\"sum\"),\n+                    rType, List.of(rType, rType), fsymTable);\n+            return reduce(rType, r, xType, x, axisType, axis, f);\n+        }\n+\n+        Value reduce(Type rType, Op.Result r,\n+                            TensorType xType, Value x,\n+                            ConstantType axisType, Value axis,\n+                            TritonOps.FuncOp f) {\n+            int axisConstant = (int) axisType.value();\n+\n+            String signature = \"reduce_\" + f.funcName() + \"_\" + axisConstant;\n+            TypeDesc elementType = TritonType.fromType(rType);\n+            TritonOps.FuncOp rf = fsymTable.computeIfAbsent(signature,\n+                    s -> reduce(elementType, xType, axisConstant, s, f));\n+\n+            return block.op(TritonOps.call(rf, block.context().getValue(x)));\n+        }\n+\n+        static TritonOps.FuncOp reduce(TypeDesc elementType,\n+                                       TensorType tensorType,\n+                                       int axisConstant,\n+                                       String name, TritonOps.FuncOp scalarFunc) {\n+            return TritonOps.func(name,\n+                            methodType(elementType, tensorType.toDesc()))\n+                    .body(fblock -> {\n+                        TritonOps.ReduceOp reduceOp = TritonOps.reduce(fblock.parentBody(),\n+                                        axisConstant, fblock.parameters().get(0),\n+                                        methodType(elementType, elementType, elementType))\n+                                .body(rblock -> {\n+                                    Block.Parameter a = rblock.parameters().get(0);\n+                                    Block.Parameter b = rblock.parameters().get(1);\n+                                    Op.Result _r = rblock.op(TritonOps.call(scalarFunc, a, b));\n+                                    rblock.op(TritonOps.reduceReturn(_r));\n+                                });\n+\n+                        Op.Result opr = fblock.op(reduceOp);\n+                        fblock.op(TritonOps.return_(opr));\n+                    });\n+        }\n+\n+        \/\/ @@@ Test\n+        public Value consume(Type rType, Op.Result r,\n+                             Type aType, Value a) {\n+            return block.op(TritonTestOps.consume(block.context().getValue(a)));\n+        }\n+\n+        void broadcastConversion(Type rType,\n+                                 Type aType, Value a,\n+                                 Type bType, Value b) {\n+            Value ma = block.context().getValue(a);\n+            Value mb = block.context().getValue(b);\n+            if (aType instanceof TensorType at && bType instanceof TensorType bTensorType) {\n+                TensorType rTensorType = (TensorType) rType;\n+                if (!at.shape().equals(rTensorType.shape())) {\n+                    ma = block.op(TritonOps.broadcast(rTensorType.toDesc(), ma));\n+                }\n+                if (!bTensorType.shape().equals(rTensorType.shape())) {\n+                    if (rTensorType.eType() instanceof PtrType) {\n+                        bTensorType = new TensorType(bType, rTensorType.shape());\n+                        mb = block.op(TritonOps.broadcast(bTensorType.toDesc(), mb));\n+                    } else {\n+                        mb = block.op(TritonOps.broadcast(rTensorType.toDesc(), mb));\n+                    }\n+                }\n+            } else if (aType instanceof TensorType) {\n+                TensorType rTensorType = (TensorType) rType;\n+                if (rTensorType.eType() instanceof PtrType) {\n+                    TensorType bTensorType = new TensorType(bType, rTensorType.shape());\n+                    mb = block.op(TritonOps.splat(bTensorType.toDesc(), mb));\n+                } else {\n+                    mb = block.op(TritonOps.splat(rTensorType.toDesc(), mb));\n+                }\n+            } else if (bType instanceof TensorType) {\n+                TensorType rTensorType = (TensorType) rType;\n+                ma = block.op(TritonOps.splat(rTensorType.toDesc(), ma));\n+            }\n+            block.context().mapValue(a, ma);\n+            block.context().mapValue(b, mb);\n+        }\n+\n+        void broadcastConversionRight(Type aType,\n+                                      Type bType, Value b) {\n+            Value mb = block.context().getValue(b);\n+            if (aType instanceof TensorType && bType instanceof TensorType bTensorType) {\n+                TensorType aTensorType = (TensorType) aType;\n+                if (!bTensorType.shape().equals(aTensorType.shape())) {\n+                    if (aTensorType.eType() instanceof PtrType) {\n+                        bTensorType = new TensorType(bTensorType.eType(), aTensorType.shape());\n+                        mb = block.op(TritonOps.broadcast(bTensorType.toDesc(), mb));\n+                    } else {\n+                        mb = block.op(TritonOps.broadcast(aTensorType.toDesc(), mb));\n+                    }\n+                }\n+            } else if (aType instanceof TensorType) {\n+                TensorType rTensorType = (TensorType) aType;\n+                if (rTensorType.eType() instanceof PtrType) {\n+                    TensorType bTensorType = new TensorType(bType, rTensorType.shape());\n+                    mb = block.op(TritonOps.splat(bTensorType.toDesc(), mb));\n+                } else {\n+                    mb = block.op(TritonOps.splat(rTensorType.toDesc(), mb));\n+                }\n+            }\n+            block.context().mapValue(b, mb);\n+        }\n+    }\n+\n+    public static <O extends Op & Op.Invokable> void printTypeMap(\n+            O kernel, Map<Value, Type> valueTypeMap) {\n+        AtomicInteger valueId = new AtomicInteger();\n+        Map<Value, Integer> valueIdMap = new LinkedHashMap<>();\n+        kernel.traverse(null, (o, codeElement) -> {\n+            switch (codeElement) {\n+                case FuncOp _ -> {\n+                    \/\/ Ignore\n+                }\n+                case Op op when !op.result().type().equals(TypeDesc.VOID) -> {\n+                    valueIdMap.put(op.result(), valueId.getAndIncrement());\n+                }\n+                case Block block -> {\n+                    for (Block.Parameter parameter : block.parameters()) {\n+                        valueIdMap.put(parameter, valueId.getAndIncrement());\n+                    }\n+                }\n+                default -> {\n+                }\n+            }\n+            return null;\n+        });\n+\n+        valueIdMap.forEach((value, id) -> {\n+            Type type = valueTypeMap.get(value);\n+            if (type != null) {\n+                System.out.println(\"%\" + id + \" : \" + value.type() + \" -> \" + type);\n+            }\n+        });\n+    }\n+}\n","filename":"cr-examples\/triton\/src\/main\/java\/oracle\/code\/triton\/TritonTransformer.java","additions":1255,"deletions":0,"binary":false,"changes":1255,"status":"added"},{"patch":"@@ -0,0 +1,45 @@\n+\/*\n+ * Copyright (c) 2024, Oracle and\/or its affiliates. All rights reserved.\n+ * DO NOT ALTER OR REMOVE COPYRIGHT NOTICES OR THIS FILE HEADER.\n+ *\n+ * This code is free software; you can redistribute it and\/or modify it\n+ * under the terms of the GNU General Public License version 2 only, as\n+ * published by the Free Software Foundation.  Oracle designates this\n+ * particular file as subject to the \"Classpath\" exception as provided\n+ * by Oracle in the LICENSE file that accompanied this code.\n+ *\n+ * This code is distributed in the hope that it will be useful, but WITHOUT\n+ * ANY WARRANTY; without even the implied warranty of MERCHANTABILITY or\n+ * FITNESS FOR A PARTICULAR PURPOSE.  See the GNU General Public License\n+ * version 2 for more details (a copy is included in the LICENSE file that\n+ * accompanied this code).\n+ *\n+ * You should have received a copy of the GNU General Public License version\n+ * 2 along with this work; if not, write to the Free Software Foundation,\n+ * Inc., 51 Franklin St, Fifth Floor, Boston, MA 02110-1301 USA.\n+ *\n+ * Please contact Oracle, 500 Oracle Parkway, Redwood Shores, CA 94065 USA\n+ * or visit www.oracle.com if you need additional information or have any\n+ * questions.\n+ *\/\n+\n+package oracle.code.triton;\n+\n+import java.lang.reflect.Type;\n+import java.lang.reflect.code.descriptor.TypeDesc;\n+\n+public abstract sealed class TritonType implements Type\n+        permits ConstantType, PtrType, TensorType {\n+\n+    public abstract TypeDesc toDesc();\n+\n+    public static TypeDesc fromType(Type t) {\n+        if (t instanceof Class<?> c) {\n+            return TypeDesc.type(c);\n+        } else if (t instanceof TritonType tt) {\n+            return tt.toDesc();\n+        } else {\n+            throw new UnsupportedOperationException(\"Unsupported type: \" + t);\n+        }\n+    }\n+}\n","filename":"cr-examples\/triton\/src\/main\/java\/oracle\/code\/triton\/TritonType.java","additions":45,"deletions":0,"binary":false,"changes":45,"status":"added"},{"patch":"@@ -0,0 +1,228 @@\n+\/*\n+ * Copyright (c) 2024, Oracle and\/or its affiliates. All rights reserved.\n+ * DO NOT ALTER OR REMOVE COPYRIGHT NOTICES OR THIS FILE HEADER.\n+ *\n+ * This code is free software; you can redistribute it and\/or modify it\n+ * under the terms of the GNU General Public License version 2 only, as\n+ * published by the Free Software Foundation.\n+ *\n+ * This code is distributed in the hope that it will be useful, but WITHOUT\n+ * ANY WARRANTY; without even the implied warranty of MERCHANTABILITY or\n+ * FITNESS FOR A PARTICULAR PURPOSE.  See the GNU General Public License\n+ * version 2 for more details (a copy is included in the LICENSE file that\n+ * accompanied this code).\n+ *\n+ * You should have received a copy of the GNU General Public License version\n+ * 2 along with this work; if not, write to the Free Software Foundation,\n+ * Inc., 51 Franklin St, Fifth Floor, Boston, MA 02110-1301 USA.\n+ *\n+ * Please contact Oracle, 500 Oracle Parkway, Redwood Shores, CA 94065 USA\n+ * or visit www.oracle.com if you need additional information or have any\n+ * questions.\n+ *\/\n+\n+package oracle.code.triton;\n+\n+import oracle.code.triton.TritonTestExtension.TritonTestData;\n+import org.junit.jupiter.api.Test;\n+import org.junit.jupiter.api.extension.ExtendWith;\n+\n+import java.lang.reflect.Type;\n+import java.lang.runtime.CodeReflection;\n+import java.util.List;\n+\n+@ExtendWith(TritonTestExtension.class)\n+public class TestAddKernel {\n+\n+    @TritonCodeModel(\"\"\"\n+            module ()void -> {\n+                tt.func @\"add_kernel_ptr<float>_ptr<float>_ptr<float>_int_64_void\" (%0 : ptr<float>, %1 : ptr<float>, %2 : ptr<float>, %3 : int)void -> {\n+                    %4 : int = arith.constant @\"64\";\n+                    %5 : int = tt.get_program_id @\"0\";\n+                    %6 : int = arith.muli %5 %4;\n+                    %7 : tensor<x64, int> = tt.make_range @start=\"0\" @end=\"64\";\n+                    %8 : tensor<x64, int> = tt.splat %6;\n+                    %9 : tensor<x64, int> = arith.addi %8 %7;\n+                    %10 : tensor<x64, int> = tt.splat %3;\n+                    %11 : tensor<x64, int> = arith.cmpi %9 %10 @\"slt\";\n+                    %12 : tensor<x64, ptr<float>> = tt.splat %0;\n+                    %13 : tensor<x64, ptr<float>> = tt.addptr %12 %9;\n+                    %14 : tensor<x64, float> = tt.load %13 %11;\n+                    %15 : tensor<x64, ptr<float>> = tt.splat %1;\n+                    %16 : tensor<x64, ptr<float>> = tt.addptr %15 %9;\n+                    %17 : tensor<x64, float> = tt.load %16 %11;\n+                    %18 : tensor<x64, float> = arith.addf %14 %17;\n+                    %19 : tensor<x64, ptr<float>> = tt.splat %2;\n+                    %20 : tensor<x64, ptr<float>> = tt.addptr %19 %9;\n+                    tt.store %20 %18 %11;\n+                    tt.return;\n+                };\n+                unreachable;\n+            };\n+            \"\"\")\n+    @CodeReflection\n+    static void add_kernel(Ptr x_ptr,  \/\/ *Pointer* to first input vector.\n+                           Ptr y_ptr,  \/\/ *Pointer* to second input vector.\n+                           Ptr output_ptr,  \/\/ *Pointer* to output vector.\n+                           int n_elements,  \/\/ Size of the vector.\n+                           @Constant int BLOCK_SIZE)  \/\/ Number of elements each program should process.\n+    \/\/ NOTE: @Constant so it can be used as a shape value\n+    {\n+        \/\/ There are multiple 'programs' processing different data. We identify which program\n+        \/\/ we are here:\n+        var pid = Triton.programId(0); \/\/ We use a 1D launch grid so axis is 0.\n+        \/\/ This program will process inputs that are offset from the initial data.\n+        \/\/ For instance, if you had a vector of length 256 and block_size of 64, the programs\n+        \/\/ would each access the elements [0:64, 64:128, 128:192, 192:256].\n+        \/\/ Note that offsets is a list of pointers:\n+        var block_start = pid * BLOCK_SIZE;\n+        var range = Triton.arange(0, BLOCK_SIZE);\n+        var offsets = Triton.add(Triton.broadcast(block_start, range.type()), range);\n+        \/\/ Create a mask to guard memory operations against out-of-bounds accesses.\n+        var mask = Triton.compare(offsets, Triton.broadcast(n_elements, offsets.type()), Triton.CompareKind.LessThan);\n+        \/\/ Load x and y from DRAM, masking out any extra elements in case the input is not a\n+        \/\/ multiple of the block size.\n+        var x = Triton.load(Triton.add(Triton.broadcast(x_ptr, offsets.type()), offsets), mask);\n+        var y = Triton.load(Triton.add(Triton.broadcast(y_ptr, offsets.type()), offsets), mask);\n+        var output = Triton.add(x, y);\n+        \/\/ Write x + y back to DRAM.\n+        Triton.store(Triton.add(Triton.broadcast(output_ptr, offsets.type()), offsets), output, mask);\n+    }\n+\n+    @TritonTestExtension.Kernel(\"add_kernel\")\n+    @Test\n+    public void test(TritonTestData t) {\n+        List<Type> argTypes = List.of(\n+                new PtrType(float.class),\n+                new PtrType(float.class),\n+                new PtrType(float.class),\n+                int.class,\n+                new ConstantType(int.class, 64));\n+\n+        t.test(argTypes);\n+    }\n+\n+\n+    @TritonCodeModel(\"\"\"\n+            module ()void -> {\n+                tt.func @\"add_kernel2_ptr<float>_ptr<float>_ptr<float>_int_64_void\" (%0 : ptr<float>, %1 : ptr<float>, %2 : ptr<float>, %3 : int)void -> {\n+                    %4 : int = arith.constant @\"64\";\n+                    %5 : int = tt.get_program_id @\"0\";\n+                    %6 : int = arith.muli %5 %4;\n+                    %7 : tensor<x64, int> = tt.make_range @start=\"0\" @end=\"64\";\n+                    %8 : tensor<x64, int> = tt.splat %6;\n+                    %9 : tensor<x64, int> = arith.addi %8 %7;\n+                    %10 : tensor<x64, int> = tt.splat %3;\n+                    %11 : tensor<x64, int> = arith.cmpi %9 %10 @\"slt\";\n+                    %12 : tensor<x64, ptr<float>> = tt.splat %0;\n+                    %13 : tensor<x64, ptr<float>> = tt.addptr %12 %9;\n+                    %14 : tensor<x64, float> = tt.load %13 %11;\n+                    %15 : tensor<x64, ptr<float>> = tt.splat %1;\n+                    %16 : tensor<x64, ptr<float>> = tt.addptr %15 %9;\n+                    %17 : tensor<x64, float> = tt.load %16 %11;\n+                    %18 : tensor<x64, float> = arith.addf %14 %17;\n+                    %19 : tensor<x64, ptr<float>> = tt.splat %2;\n+                    %20 : tensor<x64, ptr<float>> = tt.addptr %19 %9;\n+                    tt.store %20 %18 %11;\n+                    tt.return;\n+                };\n+                unreachable;\n+            };\n+            \"\"\")\n+    @CodeReflection\n+    static void add_kernel2(Ptr x_ptr,  \/\/ *Pointer* to first input vector.\n+                            Ptr y_ptr,  \/\/ *Pointer* to second input vector.\n+                            Ptr output_ptr,  \/\/ *Pointer* to output vector.\n+                            int n_elements,  \/\/ Size of the vector.\n+                            @Constant int BLOCK_SIZE)  \/\/ Number of elements each program should process.\n+    \/\/ NOTE: @Constant so it can be used as a shape value\n+    {\n+        \/\/ There are multiple 'programs' processing different data. We identify which program\n+        \/\/ we are here:\n+        var pid = Triton.programId(0); \/\/ We use a 1D launch grid so axis is 0.\n+        \/\/ This program will process inputs that are offset from the initial data.\n+        \/\/ For instance, if you had a vector of length 256 and block_size of 64, the programs\n+        \/\/ would each access the elements [0:64, 64:128, 128:192, 192:256].\n+        \/\/ Note that offsets is a list of pointers:\n+        var block_start = pid * BLOCK_SIZE;\n+        var range = Triton.arange(0, BLOCK_SIZE);\n+        var offsets = Triton.add(block_start, range);\n+        \/\/ Create a mask to guard memory operations against out-of-bounds accesses.\n+        var mask = Triton.compare(offsets, n_elements, Triton.CompareKind.LessThan);\n+        \/\/ Load x and y from DRAM, masking out any extra elements in case the input is not a\n+        \/\/ multiple of the block size.\n+        var x = Triton.load(Triton.add(x_ptr, offsets), mask);\n+        var y = Triton.load(Triton.add(y_ptr, offsets), mask);\n+        var output = Triton.add(x, y);\n+        \/\/ Write x + y back to DRAM.\n+        Triton.store(Triton.add(output_ptr, offsets), output, mask);\n+    }\n+\n+    @TritonTestExtension.Kernel(\"add_kernel2\")\n+    @Test\n+    public void test2(TritonTestData t) {\n+        List<Type> argTypes = List.of(\n+                new PtrType(float.class),\n+                new PtrType(float.class),\n+                new PtrType(float.class),\n+                int.class,\n+                new ConstantType(int.class, 64));\n+\n+        t.test(argTypes);\n+    }\n+}\n+\n+\/*\n+@triton.jit\n+def add_kernel(x_ptr,  # *Pointer* to first input vector.\n+               y_ptr,  # *Pointer* to second input vector.\n+               output_ptr,  # *Pointer* to output vector.\n+               n_elements,  # Size of the vector.\n+               BLOCK_SIZE: tl.constexpr,  # Number of elements each program should process.\n+               # NOTE: `constexpr` so it can be used as a shape value.\n+               ):\n+    # There are multiple 'programs' processing different data. We identify which program\n+    # we are here:\n+    pid = tl.program_id(axis=0)  # We use a 1D launch grid so axis is 0.\n+    # This program will process inputs that are offset from the initial data.\n+    # For instance, if you had a vector of length 256 and block_size of 64, the programs\n+    # would each access the elements [0:64, 64:128, 128:192, 192:256].\n+    # Note that offsets is a list of pointers:\n+    block_start = pid * BLOCK_SIZE\n+    offsets = block_start + tl.arange(0, BLOCK_SIZE)\n+    # Create a mask to guard memory operations against out-of-bounds accesses.\n+    mask = offsets < n_elements\n+    # Load x and y from DRAM, masking out any extra elements in case the input is not a\n+    # multiple of the block size.\n+    x = tl.load(x_ptr + offsets, mask=mask)\n+    y = tl.load(y_ptr + offsets, mask=mask)\n+    output = x + y\n+    # Write x + y back to DRAM.\n+    tl.store(output_ptr + offsets, output, mask=mask)\n+*\/\n+\n+\/*\n+module {\n+  tt.func public @add_kernel_0123(%arg0: !tt.ptr<f32, 1> , %arg1: !tt.ptr<f32, 1> , %arg2: !tt.ptr<f32, 1> , %arg3: i32 ) attributes {noinline = false} {\n+    %0 = tt.get_program_id x : i32\n+    %c64_i32 = arith.constant 64 : i32\n+    %1 = arith.muli %0, %c64_i32 : i32\n+    %2 = tt.make_range {end = 64 : i32, start = 0 : i32} : tensor<64xi32>\n+    %3 = tt.splat %1 : (i32) -> tensor<64xi32>\n+    %4 = arith.addi %3, %2 : tensor<64xi32>\n+    %5 = tt.splat %arg3 : (i32) -> tensor<64xi32>\n+    %6 = arith.cmpi slt, %4, %5 : tensor<64xi32>\n+    %7 = tt.splat %arg0 : (!tt.ptr<f32, 1>) -> tensor<64x!tt.ptr<f32, 1>>\n+    %8 = tt.addptr %7, %4 : tensor<64x!tt.ptr<f32, 1>>, tensor<64xi32>\n+    %9 = tt.load %8, %6 {cache = 1 : i32, evict = 1 : i32, isVolatile = false} : tensor<64xf32>\n+    %10 = tt.splat %arg1 : (!tt.ptr<f32, 1>) -> tensor<64x!tt.ptr<f32, 1>>\n+    %11 = tt.addptr %10, %4 : tensor<64x!tt.ptr<f32, 1>>, tensor<64xi32>\n+    %12 = tt.load %11, %6 {cache = 1 : i32, evict = 1 : i32, isVolatile = false} : tensor<64xf32>\n+    %13 = arith.addf %9, %12 : tensor<64xf32>\n+    %14 = tt.splat %arg2 : (!tt.ptr<f32, 1>) -> tensor<64x!tt.ptr<f32, 1>>\n+    %15 = tt.addptr %14, %4 : tensor<64x!tt.ptr<f32, 1>>, tensor<64xi32>\n+    tt.store %15, %13, %6 {cache = 1 : i32, evict = 1 : i32} : tensor<64xf32>\n+    tt.return\n+  }\n+}\n+*\/\n","filename":"cr-examples\/triton\/src\/test\/java\/oracle\/code\/triton\/TestAddKernel.java","additions":228,"deletions":0,"binary":false,"changes":228,"status":"added"},{"patch":"@@ -0,0 +1,128 @@\n+\/*\n+ * Copyright (c) 2024, Oracle and\/or its affiliates. All rights reserved.\n+ * DO NOT ALTER OR REMOVE COPYRIGHT NOTICES OR THIS FILE HEADER.\n+ *\n+ * This code is free software; you can redistribute it and\/or modify it\n+ * under the terms of the GNU General Public License version 2 only, as\n+ * published by the Free Software Foundation.\n+ *\n+ * This code is distributed in the hope that it will be useful, but WITHOUT\n+ * ANY WARRANTY; without even the implied warranty of MERCHANTABILITY or\n+ * FITNESS FOR A PARTICULAR PURPOSE.  See the GNU General Public License\n+ * version 2 for more details (a copy is included in the LICENSE file that\n+ * accompanied this code).\n+ *\n+ * You should have received a copy of the GNU General Public License version\n+ * 2 along with this work; if not, write to the Free Software Foundation,\n+ * Inc., 51 Franklin St, Fifth Floor, Boston, MA 02110-1301 USA.\n+ *\n+ * Please contact Oracle, 500 Oracle Parkway, Redwood Shores, CA 94065 USA\n+ * or visit www.oracle.com if you need additional information or have any\n+ * questions.\n+ *\/\n+\n+package oracle.code.triton;\n+\n+import oracle.code.triton.TritonTestExtension.TritonTestData;\n+import org.junit.jupiter.api.Test;\n+import org.junit.jupiter.api.extension.ExtendWith;\n+\n+import java.lang.reflect.Type;\n+import java.lang.runtime.CodeReflection;\n+import java.util.List;\n+\n+import static oracle.code.triton.Triton.*;\n+import static oracle.code.triton.TritonTest.consume;\n+\n+@ExtendWith(TritonTestExtension.class)\n+public class TestBroadcast {\n+\n+    @TritonCodeModel(\"\"\"\n+            module ()void -> {\n+                tt.func @\"test1_ptr<int>_int_64_void\" (%0 : ptr<int>, %1 : int)void -> {\n+                    %2 : tensor<x64, int> = tt.make_range @start=\"0\" @end=\"64\";\n+                    %3 : tensor<x64, ptr<int>> = tt.splat %0;\n+                    %4 : tensor<x64, ptr<int>> = tt.addptr %3 %2;\n+                    tt.consume %4;\n+                    %5 : tensor<x64, int> = tt.splat %1;\n+                    %6 : tensor<x64, int> = arith.addi %5 %2;\n+                    tt.consume %6;\n+                    %7 : tensor<x64, int> = tt.splat %1;\n+                    %8 : tensor<x64, int> = arith.addi %2 %7;\n+                    tt.consume %8;\n+                    %9 : tensor<x64, int> = tt.splat %1;\n+                    %10 : tensor<x64, int> = arith.addi %9 %2;\n+                    tt.consume %10;\n+                    %11 : tensor<x64, int> = tt.splat %1;\n+                    %12 : tensor<x64, int> = arith.addi %2 %11;\n+                    tt.consume %12;\n+                    tt.return;\n+                };\n+                unreachable;\n+            };\n+            \"\"\")\n+    @CodeReflection\n+    static void test1(Ptr ptr, int a, @Constant int s) {\n+        var t = arange(0, s);\n+        consume(add(ptr, t));\n+        consume(add(a, t));\n+        consume(add(t, a));\n+        consume(add(broadcast(a, t.type()), t));\n+        consume(add(t, broadcast(a, t.type())));\n+    }\n+\n+    @Test\n+    public void test1(TritonTestData t) {\n+        List<Type> argTypes = List.of(\n+                new PtrType(int.class),\n+                int.class,\n+                new ConstantType(int.class, 64));\n+\n+        t.test(argTypes);\n+    }\n+\n+    @TritonCodeModel(\"\"\"\n+            module ()void -> {\n+                tt.func @\"test2_int_64_32_void\" (%1 : int)void -> {\n+                    %2 : tensor<x64, int> = tt.make_range @start=\"0\" @end=\"64\";\n+                    %3 : tensor<x1, x64, int> = tt.expand_dims %2 @\"0\";\n+                    %4 : tensor<x32, int> = tt.make_range @start=\"0\" @end=\"32\";\n+                    %5 : tensor<x32, x1, int> = tt.expand_dims %4 @\"1\";\n+                    %6 : tensor<x1, x64, int> = tt.splat %1;\n+                    %7 : tensor<x1, x64, int> = arith.addi %3 %6;\n+                    tt.consume %7;\n+                    %8 : tensor<x32, x64, int> = tt.broadcast %3;\n+                    %9 : tensor<x32, x64, int> = tt.broadcast %5;\n+                    %10 : tensor<x32, x64, int> = arith.addi %8 %9;\n+                    tt.consume %10;\n+                    tt.return;\n+                };\n+                unreachable;\n+            };\n+            \"\"\")\n+    @CodeReflection\n+    static void test2(int a, @Constant int M, @Constant int N) {\n+        var m = arange(0, M);\n+        var me = expand(m, 0);\n+\n+        var n = arange(0, N);\n+        var ne = expand(n, 1);\n+\n+        var t4 = add(me, a);\n+        consume(t4);\n+\n+        var t3 = add(me, ne);\n+        consume(t3);\n+    }\n+\n+    @Test\n+    public void test2(TritonTestData t) {\n+        List<Type> argTypes = List.of(\n+                int.class,\n+                new ConstantType(int.class, 64),\n+                new ConstantType(int.class, 32)\n+        );\n+\n+        t.test(argTypes);\n+    }\n+}\n\\ No newline at end of file\n","filename":"cr-examples\/triton\/src\/test\/java\/oracle\/code\/triton\/TestBroadcast.java","additions":128,"deletions":0,"binary":false,"changes":128,"status":"added"},{"patch":"@@ -0,0 +1,163 @@\n+\/*\n+ * Copyright (c) 2024, Oracle and\/or its affiliates. All rights reserved.\n+ * DO NOT ALTER OR REMOVE COPYRIGHT NOTICES OR THIS FILE HEADER.\n+ *\n+ * This code is free software; you can redistribute it and\/or modify it\n+ * under the terms of the GNU General Public License version 2 only, as\n+ * published by the Free Software Foundation.\n+ *\n+ * This code is distributed in the hope that it will be useful, but WITHOUT\n+ * ANY WARRANTY; without even the implied warranty of MERCHANTABILITY or\n+ * FITNESS FOR A PARTICULAR PURPOSE.  See the GNU General Public License\n+ * version 2 for more details (a copy is included in the LICENSE file that\n+ * accompanied this code).\n+ *\n+ * You should have received a copy of the GNU General Public License version\n+ * 2 along with this work; if not, write to the Free Software Foundation,\n+ * Inc., 51 Franklin St, Fifth Floor, Boston, MA 02110-1301 USA.\n+ *\n+ * Please contact Oracle, 500 Oracle Parkway, Redwood Shores, CA 94065 USA\n+ * or visit www.oracle.com if you need additional information or have any\n+ * questions.\n+ *\/\n+\n+package oracle.code.triton;\n+\n+import oracle.code.triton.TritonTestExtension.TritonTestData;\n+import org.junit.jupiter.api.Test;\n+import org.junit.jupiter.api.extension.ExtendWith;\n+\n+import java.lang.reflect.Type;\n+import java.lang.runtime.CodeReflection;\n+import java.util.List;\n+\n+import static oracle.code.triton.Triton.*;\n+import static oracle.code.triton.TritonTest.consume;\n+\n+@ExtendWith(TritonTestExtension.class)\n+public class TestCdiv {\n+\n+    @TritonCodeModel(\"\"\"\n+            module ()void -> {\n+                tt.func @\"cdiv_int_int_int\" (%0 : int, %1 : int)int -> {\n+                    %2 : int = arith.addi %0 %1;\n+                    %3 : int = arith.constant @\"1\";\n+                    %4 : int = arith.subi %2 %3;\n+                    %5 : int = arith.divsi %4 %1;\n+                    tt.return %5;\n+                };\n+                tt.func @\"testScalar_int_int_void\" (%6 : int, %7 : int)void -> {\n+                    %8 : int = tt.call %6 %7 @\"cdiv_int_int_int\";\n+                    tt.consume %8;\n+                    tt.return;\n+                };\n+                unreachable;\n+            };\n+            \"\"\")\n+    @CodeReflection\n+    static void testScalar(int a, int b) {\n+        var r1 = cdiv(a, b);\n+        consume(r1);\n+    }\n+\n+    @Test\n+    public void testScalar(TritonTestData t) {\n+        List<Type> argTypes = List.of(\n+                int.class,\n+                int.class);\n+\n+        t.test(argTypes);\n+    }\n+\n+\n+    @TritonCodeModel(\"\"\"\n+            module ()void -> {\n+                tt.func @\"cdiv_int_10_int\" (%0 : int)int -> {\n+                    %1 : int = arith.constant @\"10\";\n+                    %2 : int = arith.addi %0 %1;\n+                    %3 : int = arith.constant @\"1\";\n+                    %4 : int = arith.subi %2 %3;\n+                    %5 : int = arith.divsi %4 %1;\n+                    tt.return %5;\n+                };\n+                tt.func @\"testConstant_int_10_void\" (%6 : int)void -> {\n+                    %7 : int = tt.call %6 @\"cdiv_int_10_int\";\n+                    tt.consume %7;\n+                    tt.return;\n+                };\n+                unreachable;\n+            };\n+            \"\"\")\n+    @CodeReflection\n+    static void testConstant(int a, int b) {\n+        var r1 = cdiv(a, b);\n+        consume(r1);\n+    }\n+\n+    @Test\n+    public void testConstant(TritonTestData t) {\n+        List<Type> argTypes = List.of(\n+                int.class,\n+                new ConstantType(int.class, 10));\n+\n+        t.test(argTypes);\n+    }\n+\n+\n+    @TritonCodeModel(\"\"\"\n+            module ()void -> {\n+                tt.func @\"cdiv_int_int_int\" (%0 : int, %1 : int)int -> {\n+                    %2 : int = arith.addi %0 %1;\n+                    %3 : int = arith.constant @\"1\";\n+                    %4 : int = arith.subi %2 %3;\n+                    %5 : int = arith.divsi %4 %1;\n+                    tt.return %5;\n+                };\n+                tt.func @\"cdiv_int_10_int\" (%6 : int)int -> {\n+                    %7 : int = arith.constant @\"10\";\n+                    %8 : int = arith.addi %6 %7;\n+                    %9 : int = arith.constant @\"1\";\n+                    %10 : int = arith.subi %8 %9;\n+                    %11 : int = arith.divsi %10 %7;\n+                    tt.return %11;\n+                };\n+                tt.func @\"cdiv_10_int_int\" (%12 : int)int -> {\n+                    %13 : int = arith.constant @\"10\";\n+                    %14 : int = arith.addi %13 %12;\n+                    %15 : int = arith.constant @\"1\";\n+                    %16 : int = arith.subi %14 %15;\n+                    %17 : int = arith.divsi %16 %12;\n+                    tt.return %17;\n+                };\n+                tt.func @\"testCalls_int_int_10_void\" (%18 : int, %19 : int)void -> {\n+                    %20 : int = tt.call %18 %19 @\"cdiv_int_int_int\";\n+                    tt.consume %20;\n+                    %21 : int = tt.call %19 %18 @\"cdiv_int_int_int\";\n+                    tt.consume %21;\n+                    %22 : int = tt.call %18 @\"cdiv_int_10_int\";\n+                    tt.consume %22;\n+                    %23 : int = tt.call %18 @\"cdiv_10_int_int\";\n+                    tt.consume %23;\n+                    tt.return;\n+                };\n+                unreachable;\n+            };\n+            \"\"\")\n+    @CodeReflection\n+    static void testCalls(int a, int b, int c) {\n+        consume(cdiv(a, b));\n+        consume(cdiv(b, a));\n+        consume(cdiv(a, c));\n+        consume(cdiv(c, a));\n+    }\n+\n+    @Test\n+    public void testCalls(TritonTestData t) {\n+        List<Type> argTypes = List.of(\n+                int.class,\n+                int.class,\n+                new ConstantType(int.class, 10));\n+\n+        t.test(argTypes);\n+    }\n+}\n\\ No newline at end of file\n","filename":"cr-examples\/triton\/src\/test\/java\/oracle\/code\/triton\/TestCdiv.java","additions":163,"deletions":0,"binary":false,"changes":163,"status":"added"},{"patch":"@@ -0,0 +1,84 @@\n+\/*\n+ * Copyright (c) 2024, Oracle and\/or its affiliates. All rights reserved.\n+ * DO NOT ALTER OR REMOVE COPYRIGHT NOTICES OR THIS FILE HEADER.\n+ *\n+ * This code is free software; you can redistribute it and\/or modify it\n+ * under the terms of the GNU General Public License version 2 only, as\n+ * published by the Free Software Foundation.\n+ *\n+ * This code is distributed in the hope that it will be useful, but WITHOUT\n+ * ANY WARRANTY; without even the implied warranty of MERCHANTABILITY or\n+ * FITNESS FOR A PARTICULAR PURPOSE.  See the GNU General Public License\n+ * version 2 for more details (a copy is included in the LICENSE file that\n+ * accompanied this code).\n+ *\n+ * You should have received a copy of the GNU General Public License version\n+ * 2 along with this work; if not, write to the Free Software Foundation,\n+ * Inc., 51 Franklin St, Fifth Floor, Boston, MA 02110-1301 USA.\n+ *\n+ * Please contact Oracle, 500 Oracle Parkway, Redwood Shores, CA 94065 USA\n+ * or visit www.oracle.com if you need additional information or have any\n+ * questions.\n+ *\/\n+\n+package oracle.code.triton;\n+\n+import oracle.code.triton.TritonTestExtension.TritonTestData;\n+import org.junit.jupiter.api.Test;\n+import org.junit.jupiter.api.extension.ExtendWith;\n+\n+import java.lang.reflect.Type;\n+import java.lang.runtime.CodeReflection;\n+import java.util.List;\n+\n+import static oracle.code.triton.Triton.*;\n+import static oracle.code.triton.TritonTest.consume;\n+\n+@ExtendWith(TritonTestExtension.class)\n+public class TestCountedLoop {\n+\n+    @TritonCodeModel(value = \"\"\"\n+            module ()void -> {\n+                tt.func @\"test1_int_64_void\" (%0 : int)void -> {\n+                    %1 : int = arith.constant @\"64\";\n+                    %2 : tensor<x64, int> = tt.make_range @start=\"0\" @end=\"64\";\n+                    %3 : tensor<x64, int> = tt.make_range @start=\"0\" @end=\"64\";\n+                    %4 : int = arith.constant @\"0\";\n+                    %5 : int = arith.constant @\"1\";\n+                    %6 : java.lang.reflect.code.CoreOps$Tuple<tensor<x64, int>, tensor<x64, int>> = scf.for %4 %0 %5 %2 %3 (%7 : int, %8 : tensor<x64, int>, %9 : tensor<x64, int>)java.lang.reflect.code.CoreOps$Tuple<tensor<x64, int>, tensor<x64, int>> -> {\n+                        %10 : tensor<x64, int> = tt.splat %7;\n+                        %11 : tensor<x64, int> = arith.addi %8 %10;\n+                        %12 : tensor<x64, int> = tt.splat %1;\n+                        %13 : tensor<x64, int> = arith.addi %9 %12;\n+                        scf.yield %11 %13;\n+                    };\n+                    %14 : tensor<x64, int> = tuple.load %6 @\"0\";\n+                    %15 : tensor<x64, int> = tuple.load %6 @\"1\";\n+                    tt.consume %14;\n+                    tt.consume %15;\n+                    tt.return;\n+                };\n+                unreachable;\n+            };\n+            \"\"\")\n+    @CodeReflection\n+    static void test1(int n, @Constant int s) {\n+        var a = arange(0, s);\n+        var b = arange(0, s);\n+        for (int i = 0; i < n; i++) {\n+            a = Triton.add(a, i);\n+            b = Triton.add(b, s);\n+        }\n+        consume(a);\n+        consume(b);\n+    }\n+\n+    @Test\n+    public void test1(TritonTestData t) {\n+        List<Type> argTypes = List.of(\n+                int.class,\n+                new ConstantType(int.class, 64));\n+\n+        t.test(argTypes);\n+    }\n+}\n\\ No newline at end of file\n","filename":"cr-examples\/triton\/src\/test\/java\/oracle\/code\/triton\/TestCountedLoop.java","additions":84,"deletions":0,"binary":false,"changes":84,"status":"added"},{"patch":"@@ -0,0 +1,831 @@\n+\/*\n+ * Copyright (c) 2024, Oracle and\/or its affiliates. All rights reserved.\n+ * DO NOT ALTER OR REMOVE COPYRIGHT NOTICES OR THIS FILE HEADER.\n+ *\n+ * This code is free software; you can redistribute it and\/or modify it\n+ * under the terms of the GNU General Public License version 2 only, as\n+ * published by the Free Software Foundation.\n+ *\n+ * This code is distributed in the hope that it will be useful, but WITHOUT\n+ * ANY WARRANTY; without even the implied warranty of MERCHANTABILITY or\n+ * FITNESS FOR A PARTICULAR PURPOSE.  See the GNU General Public License\n+ * version 2 for more details (a copy is included in the LICENSE file that\n+ * accompanied this code).\n+ *\n+ * You should have received a copy of the GNU General Public License version\n+ * 2 along with this work; if not, write to the Free Software Foundation,\n+ * Inc., 51 Franklin St, Fifth Floor, Boston, MA 02110-1301 USA.\n+ *\n+ * Please contact Oracle, 500 Oracle Parkway, Redwood Shores, CA 94065 USA\n+ * or visit www.oracle.com if you need additional information or have any\n+ * questions.\n+ *\/\n+\n+package oracle.code.triton;\n+\n+import org.junit.jupiter.api.Test;\n+import org.junit.jupiter.api.extension.ExtendWith;\n+\n+import java.lang.reflect.Type;\n+import java.lang.runtime.CodeReflection;\n+import java.util.List;\n+\n+import static oracle.code.triton.Triton.*;\n+import static oracle.code.triton.Triton.CompareKind.*;\n+import static oracle.code.triton.Triton.compare;\n+import static oracle.code.triton.Triton.load;\n+\n+@ExtendWith(TritonTestExtension.class)\n+public class TestMatrix {\n+\n+    @TritonCodeModel(\"\"\"\n+            module ()void -> {\n+                tt.func @\"cdiv_int_32_int\" (%0 : int)int -> {\n+                    %1 : int = arith.constant @\"32\";\n+                    %2 : int = arith.addi %0 %1;\n+                    %3 : int = arith.constant @\"1\";\n+                    %4 : int = arith.subi %2 %3;\n+                    %5 : int = arith.divsi %4 %1;\n+                    tt.return %5;\n+                };\n+                tt.func @\"cdiv_int_64_int\" (%6 : int)int -> {\n+                    %7 : int = arith.constant @\"64\";\n+                    %8 : int = arith.addi %6 %7;\n+                    %9 : int = arith.constant @\"1\";\n+                    %10 : int = arith.subi %8 %9;\n+                    %11 : int = arith.divsi %10 %7;\n+                    tt.return %11;\n+                };\n+                tt.func @\"matmul_kernel_broadcast_ptr<float>_ptr<float>_ptr<float>_int_int_int_int_int_int_int_int_int_32_64_32_8_false_void\" (%12 : ptr<float>, %13 : ptr<float>, %14 : ptr<float>, %15 : int, %16 : int, %17 : int, %18 : int, %19 : int, %20 : int, %21 : int, %22 : int, %23 : int)void -> {\n+                    %24 : int = arith.constant @\"32\";\n+                    %25 : int = arith.constant @\"64\";\n+                    %26 : int = arith.constant @\"32\";\n+                    %27 : int = arith.constant @\"8\";\n+                    %28 : int = tt.get_program_id @\"0\";\n+                    %29 : int = tt.call %15 @\"cdiv_int_32_int\";\n+                    %30 : int = tt.call %16 @\"cdiv_int_64_int\";\n+                    %31 : int = arith.muli %27 %30;\n+                    %32 : int = arith.divsi %28 %31;\n+                    %33 : int = arith.muli %32 %27;\n+                    %34 : int = arith.subi %29 %33;\n+                    %35 : int = arith.minsi %34 %27;\n+                    %36 : int = arith.remsi %28 %35;\n+                    %37 : int = arith.addi %33 %36;\n+                    %38 : int = arith.remsi %28 %31;\n+                    %39 : int = arith.divsi %38 %35;\n+                    %40 : tensor<x32, int> = tt.make_range @start=\"0\" @end=\"32\";\n+                    %41 : int = arith.muli %37 %24;\n+                    %42 : tensor<x32, int> = tt.splat %41;\n+                    %43 : tensor<x32, int> = arith.addi %42 %40;\n+                    %44 : tensor<x32, int> = tt.splat %15;\n+                    %45 : tensor<x32, int> = arith.remsi %43 %44;\n+                    %46 : tensor<x64, int> = tt.make_range @start=\"0\" @end=\"64\";\n+                    %47 : int = arith.muli %39 %25;\n+                    %48 : tensor<x64, int> = tt.splat %47;\n+                    %49 : tensor<x64, int> = arith.addi %48 %46;\n+                    %50 : tensor<x64, int> = tt.splat %16;\n+                    %51 : tensor<x64, int> = arith.remsi %49 %50;\n+                    %52 : tensor<x32, int> = tt.make_range @start=\"0\" @end=\"32\";\n+                    %53 : tensor<x32, x1, int> = tt.expand_dims %45 @\"1\";\n+                    %54 : tensor<x32, x1, int> = tt.splat %18;\n+                    %55 : tensor<x32, x1, int> = arith.muli %53 %54;\n+                    %56 : tensor<x1, x32, int> = tt.expand_dims %52 @\"0\";\n+                    %57 : tensor<x1, x32, int> = tt.splat %19;\n+                    %58 : tensor<x1, x32, int> = arith.muli %56 %57;\n+                    %59 : tensor<x32, x32, ptr<float>> = tt.splat %12;\n+                    %60 : tensor<x32, x32, int> = tt.broadcast %55;\n+                    %61 : tensor<x32, x32, int> = tt.broadcast %58;\n+                    %62 : tensor<x32, x32, int> = arith.addi %60 %61;\n+                    %63 : tensor<x32, x32, ptr<float>> = tt.addptr %59 %62;\n+                    %64 : tensor<x32, x1, int> = tt.expand_dims %52 @\"1\";\n+                    %65 : tensor<x32, x1, int> = tt.splat %20;\n+                    %66 : tensor<x32, x1, int> = arith.muli %64 %65;\n+                    %67 : tensor<x1, x64, int> = tt.expand_dims %51 @\"0\";\n+                    %68 : tensor<x1, x64, int> = tt.splat %21;\n+                    %69 : tensor<x1, x64, int> = arith.muli %67 %68;\n+                    %70 : tensor<x32, x64, ptr<float>> = tt.splat %13;\n+                    %71 : tensor<x32, x64, int> = tt.broadcast %66;\n+                    %72 : tensor<x32, x64, int> = tt.broadcast %69;\n+                    %73 : tensor<x32, x64, int> = arith.addi %71 %72;\n+                    %74 : tensor<x32, x64, ptr<float>> = tt.addptr %70 %73;\n+                    %75 : tensor<x32, x64, float> = arith.constant @\"0.0\";\n+                    %76 : int = arith.constant @\"0\";\n+                    %77 : int = tt.call %17 @\"cdiv_int_32_int\";\n+                    %78 : int = arith.constant @\"1\";\n+                    %79 : java.lang.reflect.code.CoreOps$Tuple<tensor<x32, x64, float>, tensor<x32, x32, ptr<float>>, tensor<x32, x64, ptr<float>>> = scf.for %76 %77 %78 %75 %63 %74 (%80 : int, %81 : tensor<x32, x64, float>, %82 : tensor<x32, x32, ptr<float>>, %83 : tensor<x32, x64, ptr<float>>)java.lang.reflect.code.CoreOps$Tuple<tensor<x32, x64, float>, tensor<x32, x32, ptr<float>>, tensor<x32, x64, ptr<float>>> -> {\n+                        %84 : tensor<x1, x32, int> = tt.expand_dims %52 @\"0\";\n+                        %85 : int = arith.muli %80 %26;\n+                        %86 : int = arith.subi %17 %85;\n+                        %87 : tensor<x1, x32, int> = tt.splat %86;\n+                        %88 : tensor<x1, x32, int> = arith.cmpi %84 %87 @\"slt\";\n+                        %89 : tensor<x32, x32, int> = tt.broadcast %88;\n+                        %90 : tensor<x32, x32, float> = tt.load %82 %89;\n+                        %91 : tensor<x32, x1, int> = tt.expand_dims %52 @\"1\";\n+                        %92 : int = arith.muli %80 %26;\n+                        %93 : int = arith.subi %17 %92;\n+                        %94 : tensor<x32, x1, int> = tt.splat %93;\n+                        %95 : tensor<x32, x1, int> = arith.cmpi %91 %94 @\"slt\";\n+                        %96 : tensor<x32, x64, int> = tt.broadcast %95;\n+                        %97 : tensor<x32, x64, float> = tt.load %83 %96;\n+                        %98 : tensor<x32, x64, float> = tt.dot %90 %97;\n+                        %99 : tensor<x32, x64, float> = arith.addf %81 %98;\n+                        %100 : int = arith.muli %26 %19;\n+                        %101 : tensor<x32, x32, int> = tt.splat %100;\n+                        %102 : tensor<x32, x32, ptr<float>> = tt.addptr %82 %101;\n+                        %103 : int = arith.muli %26 %20;\n+                        %104 : tensor<x32, x64, int> = tt.splat %103;\n+                        %105 : tensor<x32, x64, ptr<float>> = tt.addptr %83 %104;\n+                        scf.yield %99 %102 %105;\n+                    };\n+                    %106 : tensor<x32, x64, float> = tuple.load %79 @\"0\";\n+                    %107 : tensor<x32, x32, ptr<float>> = tuple.load %79 @\"1\";\n+                    %108 : tensor<x32, x64, ptr<float>> = tuple.load %79 @\"2\";\n+                    %109 : int = arith.muli %37 %24;\n+                    %110 : tensor<x32, int> = tt.splat %109;\n+                    %111 : tensor<x32, int> = arith.addi %110 %40;\n+                    %112 : int = arith.muli %39 %25;\n+                    %113 : tensor<x64, int> = tt.splat %112;\n+                    %114 : tensor<x64, int> = arith.addi %113 %46;\n+                    %115 : tensor<x32, x1, int> = tt.expand_dims %111 @\"1\";\n+                    %116 : tensor<x32, x1, int> = tt.splat %22;\n+                    %117 : tensor<x32, x1, int> = arith.muli %115 %116;\n+                    %118 : tensor<x1, x64, int> = tt.expand_dims %114 @\"0\";\n+                    %119 : tensor<x1, x64, int> = tt.splat %23;\n+                    %120 : tensor<x1, x64, int> = arith.muli %118 %119;\n+                    %121 : tensor<x32, x64, ptr<float>> = tt.splat %14;\n+                    %122 : tensor<x32, x64, int> = tt.broadcast %117;\n+                    %123 : tensor<x32, x64, int> = tt.broadcast %120;\n+                    %124 : tensor<x32, x64, int> = arith.addi %122 %123;\n+                    %125 : tensor<x32, x64, ptr<float>> = tt.addptr %121 %124;\n+                    %126 : tensor<x32, x1, int> = tt.expand_dims %111 @\"1\";\n+                    %127 : tensor<x32, x1, int> = tt.splat %15;\n+                    %128 : tensor<x32, x1, int> = arith.cmpi %126 %127 @\"slt\";\n+                    %129 : tensor<x1, x64, int> = tt.expand_dims %114 @\"0\";\n+                    %130 : tensor<x1, x64, int> = tt.splat %16;\n+                    %131 : tensor<x1, x64, int> = arith.cmpi %129 %130 @\"slt\";\n+                    %132 : tensor<x32, x64, int> = tt.broadcast %128;\n+                    %133 : tensor<x32, x64, int> = tt.broadcast %131;\n+                    %134 : tensor<x32, x64, int> = arith.andi %132 %133;\n+                    tt.store %125 %106 %134;\n+                    tt.return;\n+                };\n+                unreachable;\n+            };\n+            \"\"\")\n+    @CodeReflection\n+    static void matmul_kernel_broadcast(\n+            \/\/ Pointers to matrices\n+            Ptr a_ptr, Ptr b_ptr, Ptr c_ptr,\n+            \/\/ Matrix dimensions\n+            int M, int N, int K,\n+            \/\/ The stride variables represent how much to increase the ptr by when moving by 1\n+            \/\/ element in a particular dimension. E.g. `stride_am` is how much to increase `a_ptr`\n+            \/\/ by to get the element one row down (A has M rows).\n+            int stride_am, int stride_ak,\n+            int stride_bk, int stride_bn,\n+            int stride_cm, int stride_cn,\n+            \/\/ Meta-parameters\n+            @Constant int BLOCK_SIZE_M, @Constant int BLOCK_SIZE_N, @Constant int BLOCK_SIZE_K,\n+            @Constant int GROUP_SIZE_M,\n+            @Constant boolean ACTIVATION) {\n+\n+        \/\/ \"\"\"Kernel for computing the matmul C = A x B.\n+        \/\/ A has shape (M, K), B has shape (K, N) and C has shape (M, N)\n+        \/\/ \"\"\"\n+        \/\/ -----------------------------------------------------------\n+        \/\/ Map program ids `pid` to the block of C it should compute.\n+        \/\/ This is done in a grouped ordering to promote L2 data reuse.\n+        \/\/ See above `L2 Cache Optimizations` section for details.\n+        var pid = programId(0);\n+\n+        var num_pid_m = cdiv(M, BLOCK_SIZE_M);\n+        var num_pid_n = cdiv(N, BLOCK_SIZE_N);\n+        var num_pid_in_group = GROUP_SIZE_M * num_pid_n;\n+        var group_id = pid \/ num_pid_in_group;\n+        var first_pid_m = group_id * GROUP_SIZE_M;\n+        var group_size_m = Math.min(num_pid_m - first_pid_m, GROUP_SIZE_M);\n+        var pid_m = first_pid_m + (pid % group_size_m);\n+        var pid_n = (pid % num_pid_in_group) \/ group_size_m;\n+\n+        \/\/ ----------------------------------------------------------\n+        \/\/ Create pointers for the first blocks of A and B.\n+        \/\/ We will advance this pointer as we move in the K direction\n+        \/\/ and accumulate\n+        \/\/ `a_ptrs` is a block of [BLOCK_SIZE_M, BLOCK_SIZE_K] pointers\n+        \/\/ `b_ptrs` is a block of [BLOCK_SIZE_K, BLOCK_SIZE_N] pointers\n+        \/\/ See above `Pointer Arithmetics` section for details\n+        var offs_m = arange(0, BLOCK_SIZE_M);\n+        var offs_am = mod(\n+                add(broadcast(pid_m * BLOCK_SIZE_M, offs_m.type()), offs_m),\n+                broadcast(M, offs_m.type()));\n+        var offs_n = arange(0, BLOCK_SIZE_N);\n+        var offs_bn = mod(\n+                add(broadcast(pid_n * BLOCK_SIZE_N, offs_n.type()), offs_n),\n+                broadcast(N, offs_n.type()));\n+        var offs_k = arange(0, BLOCK_SIZE_K);\n+\n+        var offs_am_e = expand(offs_am, 1);\n+        offs_am_e = mul(offs_am_e, broadcast(stride_am, offs_am_e.type()));\n+        var offs_k_e_0 = expand(offs_k, 0);\n+        offs_k_e_0 = mul(offs_k_e_0, broadcast(stride_ak, offs_k_e_0.type()));\n+        TensorType a_ptrs_t = joinShape(offs_am_e.type(), offs_k_e_0.type());\n+        var a_ptrs = add(broadcast(a_ptr, a_ptrs_t),\n+                add(broadcast(offs_am_e, a_ptrs_t), broadcast(offs_k_e_0, a_ptrs_t)));\n+\n+        var offs_k_e_1 = expand(offs_k, 1);\n+        offs_k_e_1 = mul(offs_k_e_1, broadcast(stride_bk, offs_k_e_1.type()));\n+        var offs_bn_e = expand(offs_bn, 0);\n+        offs_bn_e = mul(offs_bn_e, broadcast(stride_bn, offs_bn_e.type()));\n+        TensorType b_ptrs_t = joinShape(offs_k_e_1.type(), offs_bn_e.type());\n+        var b_ptrs = add(broadcast(b_ptr, b_ptrs_t),\n+                add(broadcast(offs_k_e_1, b_ptrs_t), broadcast(offs_bn_e, b_ptrs_t)));\n+\n+        \/\/ -----------------------------------------------------------\n+        \/\/ Iterate to compute a block of the C matrix.\n+        \/\/ We accumulate into a `[BLOCK_SIZE_M, BLOCK_SIZE_N]` block\n+        \/\/ of fp32 values for higher accuracy.\n+        \/\/ `accumulator` will be converted back to fp16 after the loop.\n+        var accumulator = zeros(float.class, BLOCK_SIZE_M, BLOCK_SIZE_N);\n+        for (int k = 0; k < cdiv(K, BLOCK_SIZE_K); k++) {\n+            \/\/ Load the next block of A and B, generate a mask by checking the K dimension.\n+            \/\/ If it is out of bounds, set it to 0.\n+            var offs_k_m_0 = expand(offs_k, 0);\n+            offs_k_m_0 = compare(offs_k_m_0,\n+                    broadcast(K - k * BLOCK_SIZE_K, offs_k_m_0.type()),\n+                    LessThan);\n+            var a = load(a_ptrs, broadcast(offs_k_m_0, a_ptrs.type()));\n+            var offs_k_m_1 = expand(offs_k, 1);\n+            offs_k_m_1 = compare(offs_k_m_1,\n+                    broadcast(K - k * BLOCK_SIZE_K, offs_k_m_1.type()),\n+                    LessThan);\n+            var b = load(b_ptrs, broadcast(offs_k_m_1, b_ptrs.type()));\n+            \/\/ We accumulate along the K dimension.\n+            accumulator = add(accumulator, dot(a, b));\n+            \/\/ Advance the ptrs to the next K block.\n+            a_ptrs = add(a_ptrs, broadcast(BLOCK_SIZE_K * stride_ak, a_ptrs.type()));\n+            b_ptrs = add(b_ptrs, broadcast(BLOCK_SIZE_K * stride_bk, b_ptrs.type()));\n+        }\n+\n+        \/\/ You can fuse arbitrary activation functions here\n+        \/\/ while the accumulator is still in FP32!\n+\/\/        if (ACTIVATION) {\n+\/\/            \/\/ ...\n+\/\/        }\n+        \/\/ c = Triton.to(activation, tl.float16)\n+        var c = accumulator;\n+\n+        \/\/ -----------------------------------------------------------\n+        \/\/ Write back the block of the output matrix C with masks.\n+        var offs_cm = add(broadcast(pid_m * BLOCK_SIZE_M, offs_m.type()), offs_m);\n+        var offs_cn = add(broadcast(pid_n * BLOCK_SIZE_N, offs_n.type()), offs_n);\n+\n+        var offs_cm_e = expand(offs_cm, 1);\n+        offs_cm_e = mul(offs_cm_e, broadcast(stride_cm, offs_cm_e.type()));\n+        var offs_cn_e = expand(offs_cn, 0);\n+        offs_cn_e = mul(offs_cn_e, broadcast(stride_cn, offs_cn_e.type()));\n+        TensorType c_ptrs_t = joinShape(offs_cm_e.type(), offs_cn_e.type());\n+        var c_ptrs = add(broadcast(c_ptr, c_ptrs_t),\n+                add(broadcast(offs_cm_e, c_ptrs_t), broadcast(offs_cn_e, c_ptrs_t)));\n+\n+        offs_cm_e = expand(offs_cm, 1);\n+        var c_mask_l = compare(offs_cm_e, broadcast(M, offs_cm_e.type()), LessThan);\n+        offs_cn_e = expand(offs_cn, 0);\n+        var c_mask_r = compare(offs_cn_e, broadcast(N, offs_cn_e.type()), LessThan);\n+        var c_mask = and(broadcast(c_mask_l, c_ptrs_t), broadcast(c_mask_r, c_ptrs_t));\n+\n+        store(c_ptrs, c, c_mask);\n+    }\n+\n+    @TritonTestExtension.Kernel(\"matmul_kernel_broadcast\")\n+    @Test\n+    public void testWithBroadcast(TritonTestExtension.TritonTestData t) {\n+        List<Type> argTypes = List.of(\n+                new PtrType(float.class),\n+                new PtrType(float.class),\n+                new PtrType(float.class),\n+                int.class, int.class, int.class,\n+                int.class, int.class,\n+                int.class, int.class,\n+                int.class, int.class,\n+                new ConstantType(int.class, 32), new ConstantType(int.class, 64), new ConstantType(int.class, 32),\n+                new ConstantType(int.class, 8),\n+                new ConstantType(int.class, false));\n+\n+        t.test(argTypes);\n+    }\n+\n+\n+    @TritonCodeModel(\"\"\"\n+            module ()void -> {\n+                tt.func @\"cdiv_int_32_int\" (%0 : int)int -> {\n+                    %1 : int = arith.constant @\"32\";\n+                    %2 : int = arith.addi %0 %1;\n+                    %3 : int = arith.constant @\"1\";\n+                    %4 : int = arith.subi %2 %3;\n+                    %5 : int = arith.divsi %4 %1;\n+                    tt.return %5;\n+                };\n+                tt.func @\"cdiv_int_64_int\" (%6 : int)int -> {\n+                    %7 : int = arith.constant @\"64\";\n+                    %8 : int = arith.addi %6 %7;\n+                    %9 : int = arith.constant @\"1\";\n+                    %10 : int = arith.subi %8 %9;\n+                    %11 : int = arith.divsi %10 %7;\n+                    tt.return %11;\n+                };\n+                tt.func @\"matmul_kernel_ptr<oracle.code.triton.Float16>_ptr<oracle.code.triton.Float16>_ptr<oracle.code.triton.Float16>_int_int_int_int_int_int_int_int_int_32_64_32_8_false_void\" (%12 : ptr<oracle.code.triton.Float16>, %13 : ptr<oracle.code.triton.Float16>, %14 : ptr<oracle.code.triton.Float16>, %15 : int, %16 : int, %17 : int, %18 : int, %19 : int, %20 : int, %21 : int, %22 : int, %23 : int)void -> {\n+                    %24 : int = arith.constant @\"32\";\n+                    %25 : int = arith.constant @\"64\";\n+                    %26 : int = arith.constant @\"32\";\n+                    %27 : int = arith.constant @\"8\";\n+                    %28 : int = tt.get_program_id @\"0\";\n+                    %29 : int = tt.call %15 @\"cdiv_int_32_int\";\n+                    %30 : int = tt.call %16 @\"cdiv_int_64_int\";\n+                    %31 : int = arith.muli %27 %30;\n+                    %32 : int = arith.divsi %28 %31;\n+                    %33 : int = arith.muli %32 %27;\n+                    %34 : int = arith.subi %29 %33;\n+                    %35 : int = arith.minsi %34 %27;\n+                    %36 : int = arith.remsi %28 %35;\n+                    %37 : int = arith.addi %33 %36;\n+                    %38 : int = arith.remsi %28 %31;\n+                    %39 : int = arith.divsi %38 %35;\n+                    %40 : tensor<x32, int> = tt.make_range @start=\"0\" @end=\"32\";\n+                    %41 : int = arith.muli %37 %24;\n+                    %42 : tensor<x32, int> = tt.splat %41;\n+                    %43 : tensor<x32, int> = arith.addi %42 %40;\n+                    %44 : tensor<x32, int> = tt.splat %15;\n+                    %45 : tensor<x32, int> = arith.remsi %43 %44;\n+                    %46 : tensor<x64, int> = tt.make_range @start=\"0\" @end=\"64\";\n+                    %47 : int = arith.muli %39 %25;\n+                    %48 : tensor<x64, int> = tt.splat %47;\n+                    %49 : tensor<x64, int> = arith.addi %48 %46;\n+                    %50 : tensor<x64, int> = tt.splat %16;\n+                    %51 : tensor<x64, int> = arith.remsi %49 %50;\n+                    %52 : tensor<x32, int> = tt.make_range @start=\"0\" @end=\"32\";\n+                    %53 : tensor<x32, x1, int> = tt.expand_dims %45 @\"1\";\n+                    %54 : tensor<x32, x1, int> = tt.splat %18;\n+                    %55 : tensor<x32, x1, int> = arith.muli %53 %54;\n+                    %56 : tensor<x1, x32, int> = tt.expand_dims %52 @\"0\";\n+                    %57 : tensor<x1, x32, int> = tt.splat %19;\n+                    %58 : tensor<x1, x32, int> = arith.muli %56 %57;\n+                    %59 : tensor<x32, x32, int> = tt.broadcast %55;\n+                    %60 : tensor<x32, x32, int> = tt.broadcast %58;\n+                    %61 : tensor<x32, x32, int> = arith.addi %59 %60;\n+                    %62 : tensor<x32, x32, ptr<oracle.code.triton.Float16>> = tt.splat %12;\n+                    %63 : tensor<x32, x32, ptr<oracle.code.triton.Float16>> = tt.addptr %62 %61;\n+                    %64 : tensor<x32, x1, int> = tt.expand_dims %52 @\"1\";\n+                    %65 : tensor<x32, x1, int> = tt.splat %20;\n+                    %66 : tensor<x32, x1, int> = arith.muli %64 %65;\n+                    %67 : tensor<x1, x64, int> = tt.expand_dims %51 @\"0\";\n+                    %68 : tensor<x1, x64, int> = tt.splat %21;\n+                    %69 : tensor<x1, x64, int> = arith.muli %67 %68;\n+                    %70 : tensor<x32, x64, int> = tt.broadcast %66;\n+                    %71 : tensor<x32, x64, int> = tt.broadcast %69;\n+                    %72 : tensor<x32, x64, int> = arith.addi %70 %71;\n+                    %73 : tensor<x32, x64, ptr<oracle.code.triton.Float16>> = tt.splat %13;\n+                    %74 : tensor<x32, x64, ptr<oracle.code.triton.Float16>> = tt.addptr %73 %72;\n+                    %75 : tensor<x32, x64, float> = arith.constant @\"0.0\";\n+                    %76 : int = arith.constant @\"0\";\n+                    %77 : int = tt.call %17 @\"cdiv_int_32_int\";\n+                    %78 : int = arith.constant @\"1\";\n+                    %79 : java.lang.reflect.code.CoreOps$Tuple<tensor<x32, x64, float>, tensor<x32, x32, ptr<oracle.code.triton.Float16>>, tensor<x32, x64, ptr<oracle.code.triton.Float16>>> = scf.for %76 %77 %78 %75 %63 %74 (%80 : int, %81 : tensor<x32, x64, float>, %82 : tensor<x32, x32, ptr<oracle.code.triton.Float16>>, %83 : tensor<x32, x64, ptr<oracle.code.triton.Float16>>)java.lang.reflect.code.CoreOps$Tuple<tensor<x32, x64, float>, tensor<x32, x32, ptr<oracle.code.triton.Float16>>, tensor<x32, x64, ptr<oracle.code.triton.Float16>>> -> {\n+                        %84 : tensor<x1, x32, int> = tt.expand_dims %52 @\"0\";\n+                        %85 : int = arith.muli %80 %26;\n+                        %86 : int = arith.subi %17 %85;\n+                        %87 : tensor<x1, x32, int> = tt.splat %86;\n+                        %88 : tensor<x1, x32, int> = arith.cmpi %84 %87 @\"slt\";\n+                        %89 : tensor<x32, x32, int> = tt.broadcast %88;\n+                        %90 : tensor<x32, x32, oracle.code.triton.Float16> = tt.load %82 %89;\n+                        %91 : tensor<x32, x1, int> = tt.expand_dims %52 @\"1\";\n+                        %92 : int = arith.muli %80 %26;\n+                        %93 : int = arith.subi %17 %92;\n+                        %94 : tensor<x32, x1, int> = tt.splat %93;\n+                        %95 : tensor<x32, x1, int> = arith.cmpi %91 %94 @\"slt\";\n+                        %96 : tensor<x32, x64, int> = tt.broadcast %95;\n+                        %97 : tensor<x32, x64, oracle.code.triton.Float16> = tt.load %83 %96;\n+                        %98 : tensor<x32, x64, float> = tt.dot %90 %97;\n+                        %99 : tensor<x32, x64, float> = arith.addf %81 %98;\n+                        %100 : int = arith.muli %26 %19;\n+                        %101 : tensor<x32, x32, int> = tt.splat %100;\n+                        %102 : tensor<x32, x32, ptr<oracle.code.triton.Float16>> = tt.addptr %82 %101;\n+                        %103 : int = arith.muli %26 %20;\n+                        %104 : tensor<x32, x64, int> = tt.splat %103;\n+                        %105 : tensor<x32, x64, ptr<oracle.code.triton.Float16>> = tt.addptr %83 %104;\n+                        scf.yield %99 %102 %105;\n+                    };\n+                    %106 : tensor<x32, x64, float> = tuple.load %79 @\"0\";\n+                    %107 : tensor<x32, x32, ptr<oracle.code.triton.Float16>> = tuple.load %79 @\"1\";\n+                    %108 : tensor<x32, x64, ptr<oracle.code.triton.Float16>> = tuple.load %79 @\"2\";\n+                    %109 : tensor<x32, x64, oracle.code.triton.Float16> = arith.truncf %106;\n+                    %110 : int = arith.muli %37 %24;\n+                    %111 : tensor<x32, int> = tt.splat %110;\n+                    %112 : tensor<x32, int> = arith.addi %111 %40;\n+                    %113 : int = arith.muli %39 %25;\n+                    %114 : tensor<x64, int> = tt.splat %113;\n+                    %115 : tensor<x64, int> = arith.addi %114 %46;\n+                    %116 : tensor<x32, x1, int> = tt.expand_dims %112 @\"1\";\n+                    %117 : tensor<x32, x1, int> = tt.splat %22;\n+                    %118 : tensor<x32, x1, int> = arith.muli %117 %116;\n+                    %119 : tensor<x1, x64, int> = tt.expand_dims %115 @\"0\";\n+                    %120 : tensor<x1, x64, int> = tt.splat %23;\n+                    %121 : tensor<x1, x64, int> = arith.muli %120 %119;\n+                    %122 : tensor<x32, x64, int> = tt.broadcast %118;\n+                    %123 : tensor<x32, x64, int> = tt.broadcast %121;\n+                    %124 : tensor<x32, x64, int> = arith.addi %122 %123;\n+                    %125 : tensor<x32, x64, ptr<oracle.code.triton.Float16>> = tt.splat %14;\n+                    %126 : tensor<x32, x64, ptr<oracle.code.triton.Float16>> = tt.addptr %125 %124;\n+                    %127 : tensor<x32, x1, int> = tt.expand_dims %112 @\"1\";\n+                    %128 : tensor<x32, x1, int> = tt.splat %15;\n+                    %129 : tensor<x32, x1, int> = arith.cmpi %127 %128 @\"slt\";\n+                    %130 : tensor<x1, x64, int> = tt.expand_dims %115 @\"0\";\n+                    %131 : tensor<x1, x64, int> = tt.splat %16;\n+                    %132 : tensor<x1, x64, int> = arith.cmpi %130 %131 @\"slt\";\n+                    %133 : tensor<x32, x64, int> = tt.broadcast %129;\n+                    %134 : tensor<x32, x64, int> = tt.broadcast %132;\n+                    %135 : tensor<x32, x64, int> = arith.andi %133 %134;\n+                    tt.store %126 %109 %135;\n+                    tt.return;\n+                };\n+                unreachable;\n+            };\n+            \"\"\")\n+    @CodeReflection\n+    static void matmul_kernel(\n+            \/\/ Pointers to matrices\n+            Ptr a_ptr, Ptr b_ptr, Ptr c_ptr,\n+            \/\/ Matrix dimensions\n+            int M, int N, int K,\n+            \/\/ The stride variables represent how much to increase the ptr by when moving by 1\n+            \/\/ element in a particular dimension. E.g. `stride_am` is how much to increase `a_ptr`\n+            \/\/ by to get the element one row down (A has M rows).\n+            int stride_am, int stride_ak,\n+            int stride_bk, int stride_bn,\n+            int stride_cm, int stride_cn,\n+            \/\/ Meta-parameters\n+            @Constant int BLOCK_SIZE_M, @Constant int BLOCK_SIZE_N, @Constant int BLOCK_SIZE_K,\n+            @Constant int GROUP_SIZE_M,\n+            @Constant boolean ACTIVATION) {\n+\n+        \/\/ \"\"\"Kernel for computing the matmul C = A x B.\n+        \/\/ A has shape (M, K), B has shape (K, N) and C has shape (M, N)\n+        \/\/ \"\"\"\n+        \/\/ -----------------------------------------------------------\n+        \/\/ Map program ids `pid` to the block of C it should compute.\n+        \/\/ This is done in a grouped ordering to promote L2 data reuse.\n+        \/\/ See above `L2 Cache Optimizations` section for details.\n+        var pid = programId(0);\n+        var num_pid_m = cdiv(M, BLOCK_SIZE_M);\n+        var num_pid_n = cdiv(N, BLOCK_SIZE_N);\n+        var num_pid_in_group = GROUP_SIZE_M * num_pid_n;\n+        var group_id = pid \/ num_pid_in_group;\n+        var first_pid_m = group_id * GROUP_SIZE_M;\n+        var group_size_m = Math.min(num_pid_m - first_pid_m, GROUP_SIZE_M);\n+        var pid_m = first_pid_m + (pid % group_size_m);\n+        var pid_n = (pid % num_pid_in_group) \/ group_size_m;\n+\n+        \/\/ ----------------------------------------------------------\n+        \/\/ Create pointers for the first blocks of A and B.\n+        \/\/ We will advance this pointer as we move in the K direction\n+        \/\/ and accumulate\n+        \/\/ `a_ptrs` is a block of [BLOCK_SIZE_M, BLOCK_SIZE_K] pointers\n+        \/\/ `b_ptrs` is a block of [BLOCK_SIZE_K, BLOCK_SIZE_N] pointers\n+        \/\/ See above `Pointer Arithmetics` section for details\n+        var offs_m = arange(0, BLOCK_SIZE_M);\n+        var offs_am = mod(add(pid_m * BLOCK_SIZE_M, offs_m), M);\n+        var offs_n = arange(0, BLOCK_SIZE_N);\n+        var offs_bn = mod(add(pid_n * BLOCK_SIZE_N, offs_n), N);\n+        var offs_k = arange(0, BLOCK_SIZE_K);\n+        var a_ptrs = add(a_ptr, add(\n+                mul(expand(offs_am, 1), stride_am),\n+                mul(expand(offs_k, 0), stride_ak)));\n+        var b_ptrs = add(b_ptr, add(\n+                        mul(expand(offs_k, 1), stride_bk),\n+                        mul(expand(offs_bn, 0), stride_bn)));\n+\n+        \/\/ -----------------------------------------------------------\n+        \/\/ Iterate to compute a block of the C matrix.\n+        \/\/ We accumulate into a `[BLOCK_SIZE_M, BLOCK_SIZE_N]` block\n+        \/\/ of fp32 values for higher accuracy.\n+        \/\/ `accumulator` will be converted back to fp16 after the loop.\n+        var accumulator = zeros(float.class, BLOCK_SIZE_M, BLOCK_SIZE_N);\n+        for (int k = 0; k < cdiv(K, BLOCK_SIZE_K); k++) {\n+            \/\/ Load the next block of A and B, generate a mask by checking the K dimension.\n+            \/\/ If it is out of bounds, set it to 0.\n+            var a = load(a_ptrs,\n+                    compare(expand(offs_k, 0), K - k * BLOCK_SIZE_K, LessThan));\n+            var b = load(b_ptrs,\n+                    compare(expand(offs_k, 1), K - k * BLOCK_SIZE_K, LessThan));\n+            \/\/ We accumulate along the K dimension.\n+            accumulator = add(accumulator, dot(a, b));\n+            \/\/ Advance the ptrs to the next K block.\n+            a_ptrs = add(a_ptrs, BLOCK_SIZE_K * stride_ak);\n+            b_ptrs = add(b_ptrs, BLOCK_SIZE_K * stride_bk);\n+        }\n+\n+        \/\/ You can fuse arbitrary activation functions here\n+        \/\/ while the accumulator is still in FP32!\n+\/\/        if (ACTIVATION) {\n+\/\/            \/\/ ...\n+\/\/        }\n+        var c = Triton.conv(Float16.class, accumulator);\n+\n+        \/\/ -----------------------------------------------------------\n+        \/\/ Write back the block of the output matrix C with masks.\n+        var offs_cm = add(pid_m * BLOCK_SIZE_M, offs_m);\n+        var offs_cn = add(pid_n * BLOCK_SIZE_N, offs_n);\n+        var c_ptrs = add(c_ptr, add(\n+                        mul(stride_cm, expand(offs_cm, 1)),\n+                        mul(stride_cn, expand(offs_cn, 0))));\n+        var c_mask = and(\n+                compare(expand(offs_cm, 1), M, LessThan),\n+                compare(expand(offs_cn, 0), N, LessThan));\n+        store(c_ptrs, c, c_mask);\n+    }\n+\n+    @TritonTestExtension.Kernel(\"matmul_kernel\")\n+    @Test\n+    public void test(TritonTestExtension.TritonTestData t) {\n+        List<Type> argTypes = List.of(\n+                new PtrType(Float16.class),\n+                new PtrType(Float16.class),\n+                new PtrType(Float16.class),\n+                int.class, int.class, int.class,\n+                int.class, int.class,\n+                int.class, int.class,\n+                int.class, int.class,\n+                new ConstantType(int.class, 32), new ConstantType(int.class, 64), new ConstantType(int.class, 32),\n+                new ConstantType(int.class, 8),\n+                new ConstantType(int.class, false));\n+\n+        t.test(argTypes);\n+    }\n+\n+}\n+\n+\/*\n+@triton.jit\n+def matmul_kernel(\n+        # Pointers to matrices\n+        a_ptr, b_ptr, c_ptr,\n+        # Matrix dimensions\n+        M, N, K,\n+        # The stride variables represent how much to increase the ptr by when moving by 1\n+        # element in a particular dimension. E.g. `stride_am` is how much to increase `a_ptr`\n+        # by to get the element one row down (A has M rows).\n+        stride_am, stride_ak,  #\n+        stride_bk, stride_bn,  #\n+        stride_cm, stride_cn,\n+        # Meta-parameters\n+        BLOCK_SIZE_M: tl.constexpr, BLOCK_SIZE_N: tl.constexpr, BLOCK_SIZE_K: tl.constexpr,  #\n+        GROUP_SIZE_M: tl.constexpr,  #\n+        ACTIVATION: tl.constexpr  #\n+):\n+    \"\"\"Kernel for computing the matmul C = A x B.\n+    A has shape (M, K), B has shape (K, N) and C has shape (M, N)\n+    \"\"\"\n+    # -----------------------------------------------------------\n+    # Map program ids `pid` to the block of C it should compute.\n+    # This is done in a grouped ordering to promote L2 data reuse.\n+    # See above `L2 Cache Optimizations` section for details.\n+    pid = tl.program_id(axis=0)\n+    num_pid_m = tl.cdiv(M, BLOCK_SIZE_M)\n+    num_pid_n = tl.cdiv(N, BLOCK_SIZE_N)\n+    num_pid_in_group = GROUP_SIZE_M * num_pid_n\n+    group_id = pid \/\/ num_pid_in_group\n+    first_pid_m = group_id * GROUP_SIZE_M\n+    group_size_m = min(num_pid_m - first_pid_m, GROUP_SIZE_M)\n+    pid_m = first_pid_m + (pid % group_size_m)\n+    pid_n = (pid % num_pid_in_group) \/\/ group_size_m\n+\n+    # ----------------------------------------------------------\n+    # Create pointers for the first blocks of A and B.\n+    # We will advance this pointer as we move in the K direction\n+    # and accumulate\n+    # `a_ptrs` is a block of [BLOCK_SIZE_M, BLOCK_SIZE_K] pointers\n+    # `b_ptrs` is a block of [BLOCK_SIZE_K, BLOCK_SIZE_N] pointers\n+    # See above `Pointer Arithmetics` section for details\n+    offs_am = (pid_m * BLOCK_SIZE_M + tl.arange(0, BLOCK_SIZE_M)) % M\n+    offs_bn = (pid_n * BLOCK_SIZE_N + tl.arange(0, BLOCK_SIZE_N)) % N\n+    offs_k = tl.arange(0, BLOCK_SIZE_K)\n+    a_ptrs = a_ptr + (offs_am[:, None] * stride_am + offs_k[None, :] * stride_ak)\n+    b_ptrs = b_ptr + (offs_k[:, None] * stride_bk + offs_bn[None, :] * stride_bn)\n+\n+    # -----------------------------------------------------------\n+    # Iterate to compute a block of the C matrix.\n+    # We accumulate into a `[BLOCK_SIZE_M, BLOCK_SIZE_N]` block\n+    # of fp32 values for higher accuracy.\n+    # `accumulator` will be converted back to fp16 after the loop.\n+    accumulator = tl.zeros((BLOCK_SIZE_M, BLOCK_SIZE_N), dtype=tl.float32)\n+    for k in range(0, tl.cdiv(K, BLOCK_SIZE_K)):\n+        # Load the next block of A and B, generate a mask by checking the K dimension.\n+        # If it is out of bounds, set it to 0.\n+        a = tl.load(a_ptrs, mask=offs_k[None, :] < K - k * BLOCK_SIZE_K, other=0.0)\n+        b = tl.load(b_ptrs, mask=offs_k[:, None] < K - k * BLOCK_SIZE_K, other=0.0)\n+        # We accumulate along the K dimension.\n+        accumulator += tl.dot(a, b)\n+        # Advance the ptrs to the next K block.\n+        a_ptrs += BLOCK_SIZE_K * stride_ak\n+        b_ptrs += BLOCK_SIZE_K * stride_bk\n+    # You can fuse arbitrary activation functions here\n+    # while the accumulator is still in FP32!\n+    if ACTIVATION == \"leaky_relu\":\n+        accumulator = leaky_relu(accumulator)\n+    c = accumulator.to(tl.float16)\n+\n+    # -----------------------------------------------------------\n+    # Write back the block of the output matrix C with masks.\n+    offs_cm = pid_m * BLOCK_SIZE_M + tl.arange(0, BLOCK_SIZE_M)\n+    offs_cn = pid_n * BLOCK_SIZE_N + tl.arange(0, BLOCK_SIZE_N)\n+    c_ptrs = c_ptr + stride_cm * offs_cm[:, None] + stride_cn * offs_cn[None, :]\n+    c_mask = (offs_cm[:, None] < M) & (offs_cn[None, :] < N)\n+    tl.store(c_ptrs, c, mask=c_mask)\n+\n+\n+# We can fuse `leaky_relu` by providing it as an `ACTIVATION` meta-parameter in `_matmul`.\n+@triton.jit\n+def leaky_relu(x):\n+    x = x + 1\n+    return tl.where(x >= 0, x, 0.01 * x)\n+*\/\n+\n+\/*\n+\n+ triton\/python\/triton\/tools\/compile.py \\\n+    --kernel-name matmul_kernel \\\n+    --signature \"*fp16,*fp16,*fp16,i32,i32,i32,i32,i32,i32,i32,i32,i32,32,64,32,8,0\" \\\n+    --grid=1024,1024,1024 \\\n+    03-matrix-multiplication.py\n+\n+BLOCK_SIZE_M = 32\n+BLOCK_SIZE_N = 64\n+BLOCK_SIZE_K = 32\n+GROUP_SIZE_M = 8\n+ACTIVATION = 0\n+\n+module {\n+  tt.func public @matmul_kernel_01234567891011(\n+            %arg0: !tt.ptr<f16, 1>, %arg1: !tt.ptr<f16, 1>, %arg2: !tt.ptr<f16, 1> ,\n+            %arg3: i32, %arg4: i32, %arg5: i32 ,\n+            %arg6: i32, %arg7: i32, %arg8: i32 ,\n+            %%arg9: i32, %arg10: i32, %arg11: i32 ) attributes {noinline = false} {\n+    %0 = tt.get_program_id x : i32\n+    %1 = tt.call @cdiv__i32__1cconstexpr_32_(%arg3) : (i32) -> i32\n+    %2 = tt.call @cdiv__i32__1cconstexpr_64_(%arg4) : (i32) -> i32\n+    %c8_i32 = arith.constant 8 : i32\n+    %3 = arith.muli %2, %c8_i32 : i32\n+    %4 = arith.divsi %0, %3 : i32\n+    %c8_i32_0 = arith.constant 8 : i32\n+    %5 = arith.muli %4, %c8_i32_0 : i32\n+    %6 = arith.subi %1, %5 : i32\n+    %7 = tt.call @minimum__i32__1cconstexpr_8_(%6) : (i32) -> i32\n+    %8 = arith.remsi %0, %7 : i32\n+    %9 = arith.addi %5, %8 : i32\n+    %10 = arith.remsi %0, %3 : i32\n+    %11 = arith.divsi %10, %7 : i32\n+    %c32_i32 = arith.constant 32 : i32\n+    %12 = arith.muli %9, %c32_i32 : i32\n+    %13 = tt.make_range {end = 32 : i32, start = 0 : i32} : tensor<32xi32>\n+    %14 = tt.splat %12 : (i32) -> tensor<32xi32>\n+    %15 = arith.addi %14, %13 : tensor<32xi32>\n+    %16 = tt.splat %arg3 : (i32) -> tensor<32xi32>\n+    %17 = arith.remsi %15, %16 : tensor<32xi32>\n+    %c64_i32 = arith.constant 64 : i32\n+    %18 = arith.muli %11, %c64_i32 : i32\n+    %19 = tt.make_range {end = 64 : i32, start = 0 : i32} : tensor<64xi32>\n+    %20 = tt.splat %18 : (i32) -> tensor<64xi32>\n+    %21 = arith.addi %20, %19 : tensor<64xi32>\n+    %22 = tt.splat %arg4 : (i32) -> tensor<64xi32>\n+    %23 = arith.remsi %21, %22 : tensor<64xi32>\n+    %24 = tt.make_range {end = 32 : i32, start = 0 : i32} : tensor<32xi32>\n+    %25 = tt.expand_dims %17 {axis = 1 : i32} : (tensor<32xi32>) -> tensor<32x1xi32>\n+    %26 = tt.splat %arg6 : (i32) -> tensor<32x1xi32>\n+    %27 = arith.muli %25, %26 : tensor<32x1xi32>\n+    %28 = tt.expand_dims %24 {axis = 0 : i32} : (tensor<32xi32>) -> tensor<1x32xi32>\n+    %29 = tt.splat %arg7 : (i32) -> tensor<1x32xi32>\n+    %30 = arith.muli %28, %29 : tensor<1x32xi32>\n+    %31 = tt.broadcast %27 : (tensor<32x1xi32>) -> tensor<32x32xi32>\n+    %32 = tt.broadcast %30 : (tensor<1x32xi32>) -> tensor<32x32xi32>\n+    %33 = arith.addi %31, %32 : tensor<32x32xi32>\n+    %34 = tt.splat %arg0 : (!tt.ptr<f16, 1>) -> tensor<32x32x!tt.ptr<f16, 1>>\n+    %35 = tt.addptr %34, %33 : tensor<32x32x!tt.ptr<f16, 1>>, tensor<32x32xi32>\n+    %36 = tt.expand_dims %24 {axis = 1 : i32} : (tensor<32xi32>) -> tensor<32x1xi32>\n+    %37 = tt.splat %arg8 : (i32) -> tensor<32x1xi32>\n+    %38 = arith.muli %36, %37 : tensor<32x1xi32>\n+    %39 = tt.expand_dims %23 {axis = 0 : i32} : (tensor<64xi32>) -> tensor<1x64xi32>\n+    %40 = tt.splat %arg9 : (i32) -> tensor<1x64xi32>\n+    %41 = arith.muli %39, %40 : tensor<1x64xi32>\n+    %42 = tt.broadcast %38 : (tensor<32x1xi32>) -> tensor<32x64xi32>\n+    %43 = tt.broadcast %41 : (tensor<1x64xi32>) -> tensor<32x64xi32>\n+    %44 = arith.addi %42, %43 : tensor<32x64xi32>\n+    %45 = tt.splat %arg1 : (!tt.ptr<f16, 1>) -> tensor<32x64x!tt.ptr<f16, 1>>\n+    %46 = tt.addptr %45, %44 : tensor<32x64x!tt.ptr<f16, 1>>, tensor<32x64xi32>\n+    %47 = tt.call @\"zeros____0cconstexpr_(constexpr_32_, constexpr_64_)__1cconstexpr_fp32_\"() : () -> tensor<32x64xf32>\n+    %48 = tt.call @cdiv__i32__1cconstexpr_32_(%arg5) : (i32) -> i32\n+    %c0_i32 = arith.constant 0 : i32\n+    %c1_i32 = arith.constant 1 : i32\n+    %49 = arith.bitcast %c0_i32 : i32 to i32\n+    %50 = arith.bitcast %48 : i32 to i32\n+    %51 = arith.bitcast %c1_i32 : i32 to i32\n+    %52 = llvm.mlir.undef : i32\n+    %53:3 = scf.for %arg12 = %49 to %50 step %51 iter_args(%arg13 = %47, %arg14 = %35, %arg15 = %46) -> (tensor<32x64xf32>, tensor<32x32x!tt.ptr<f16, 1>>, tensor<32x64x!tt.ptr<f16, 1>>)  : i32 {\n+      %83 = tt.expand_dims %24 {axis = 0 : i32} : (tensor<32xi32>) -> tensor<1x32xi32>\n+      %c32_i32_3 = arith.constant 32 : i32\n+      %84 = arith.muli %arg12, %c32_i32_3 : i32\n+      %85 = arith.subi %arg5, %84 : i32\n+      %86 = tt.splat %85 : (i32) -> tensor<1x32xi32>\n+      %87 = arith.cmpi slt, %83, %86 : tensor<1x32xi32>\n+      %cst = arith.constant 0.000000e+00 : f32\n+      %88 = tt.broadcast %87 : (tensor<1x32xi1>) -> tensor<32x32xi1>\n+      %cst_4 = arith.constant dense<0.000000e+00> : tensor<32x32xf32>\n+      %89 = arith.truncf %cst_4 : tensor<32x32xf32> to tensor<32x32xf16>\n+      %90 = tt.load %arg14, %88, %89 {cache = 1 : i32, evict = 1 : i32, isVolatile = false} : tensor<32x32xf16>\n+      %91 = tt.expand_dims %24 {axis = 1 : i32} : (tensor<32xi32>) -> tensor<32x1xi32>\n+      %c32_i32_5 = arith.constant 32 : i32\n+      %92 = arith.muli %arg12, %c32_i32_5 : i32\n+      %93 = arith.subi %arg5, %92 : i32\n+      %94 = tt.splat %93 : (i32) -> tensor<32x1xi32>\n+      %95 = arith.cmpi slt, %91, %94 : tensor<32x1xi32>\n+      %cst_6 = arith.constant 0.000000e+00 : f32\n+      %96 = tt.broadcast %95 : (tensor<32x1xi1>) -> tensor<32x64xi1>\n+      %cst_7 = arith.constant dense<0.000000e+00> : tensor<32x64xf32>\n+      %97 = arith.truncf %cst_7 : tensor<32x64xf32> to tensor<32x64xf16>\n+      %98 = tt.load %arg15, %96, %97 {cache = 1 : i32, evict = 1 : i32, isVolatile = false} : tensor<32x64xf16>\n+      %cst_8 = arith.constant 0.000000e+00 : f32\n+      %cst_9 = arith.constant dense<0.000000e+00> : tensor<32x64xf32>\n+      %99 = tt.dot %90, %98, %cst_9 {allowTF32 = true, maxNumImpreciseAcc = 0 : i32} : tensor<32x32xf16> * tensor<32x64xf16> -> tensor<32x64xf32>\n+      %100 = arith.addf %arg13, %99 : tensor<32x64xf32>\n+      %c32_i32_10 = arith.constant 32 : i32\n+      %101 = arith.muli %arg7, %c32_i32_10 : i32\n+      %102 = tt.splat %101 : (i32) -> tensor<32x32xi32>\n+      %103 = tt.addptr %arg14, %102 : tensor<32x32x!tt.ptr<f16, 1>>, tensor<32x32xi32>\n+      %c32_i32_11 = arith.constant 32 : i32\n+      %104 = arith.muli %arg8, %c32_i32_11 : i32\n+      %105 = tt.splat %104 : (i32) -> tensor<32x64xi32>\n+      %106 = tt.addptr %arg15, %105 : tensor<32x64x!tt.ptr<f16, 1>>, tensor<32x64xi32>\n+      scf.yield %100, %103, %106 : tensor<32x64xf32>, tensor<32x32x!tt.ptr<f16, 1>>, tensor<32x64x!tt.ptr<f16, 1>>\n+    }\n+    %54 = arith.truncf %53#0 : tensor<32x64xf32> to tensor<32x64xf16>\n+    %c32_i32_1 = arith.constant 32 : i32\n+    %55 = arith.muli %9, %c32_i32_1 : i32\n+    %56 = tt.make_range {end = 32 : i32, start = 0 : i32} : tensor<32xi32>\n+    %57 = tt.splat %55 : (i32) -> tensor<32xi32>\n+    %58 = arith.addi %57, %56 : tensor<32xi32>\n+    %c64_i32_2 = arith.constant 64 : i32\n+    %59 = arith.muli %11, %c64_i32_2 : i32\n+    %60 = tt.make_range {end = 64 : i32, start = 0 : i32} : tensor<64xi32>\n+    %61 = tt.splat %59 : (i32) -> tensor<64xi32>\n+    %62 = arith.addi %61, %60 : tensor<64xi32>\n+    %63 = tt.expand_dims %58 {axis = 1 : i32} : (tensor<32xi32>) -> tensor<32x1xi32>\n+    %64 = tt.splat %arg10 : (i32) -> tensor<32x1xi32>\n+    %65 = arith.muli %64, %63 : tensor<32x1xi32>\n+    %66 = tt.splat %arg2 : (!tt.ptr<f16, 1>) -> tensor<32x1x!tt.ptr<f16, 1>>\n+    %67 = tt.addptr %66, %65 : tensor<32x1x!tt.ptr<f16, 1>>, tensor<32x1xi32>\n+    %68 = tt.expand_dims %62 {axis = 0 : i32} : (tensor<64xi32>) -> tensor<1x64xi32>\n+    %69 = tt.splat %arg11 : (i32) -> tensor<1x64xi32>\n+    %70 = arith.muli %69, %68 : tensor<1x64xi32>\n+    %71 = tt.broadcast %67 : (tensor<32x1x!tt.ptr<f16, 1>>) -> tensor<32x64x!tt.ptr<f16, 1>>\n+    %72 = tt.broadcast %70 : (tensor<1x64xi32>) -> tensor<32x64xi32>\n+    %73 = tt.addptr %71, %72 : tensor<32x64x!tt.ptr<f16, 1>>, tensor<32x64xi32>\n+    %74 = tt.expand_dims %58 {axis = 1 : i32} : (tensor<32xi32>) -> tensor<32x1xi32>\n+    %75 = tt.splat %arg3 : (i32) -> tensor<32x1xi32>\n+    %76 = arith.cmpi slt, %74, %75 : tensor<32x1xi32>\n+    %77 = tt.expand_dims %62 {axis = 0 : i32} : (tensor<64xi32>) -> tensor<1x64xi32>\n+    %78 = tt.splat %arg4 : (i32) -> tensor<1x64xi32>\n+    %79 = arith.cmpi slt, %77, %78 : tensor<1x64xi32>\n+    %80 = tt.broadcast %76 : (tensor<32x1xi1>) -> tensor<32x64xi1>\n+    %81 = tt.broadcast %79 : (tensor<1x64xi1>) -> tensor<32x64xi1>\n+    %82 = arith.andi %80, %81 : tensor<32x64xi1>\n+    tt.store %73, %54, %82 {cache = 1 : i32, evict = 1 : i32} : tensor<32x64xf16>\n+    tt.return\n+  }\n+  tt.func private @cdiv__i32__1cconstexpr_32_(%arg0: i32 ) -> i32 attributes {noinline = false} {\n+    %c32_i32 = arith.constant 32 : i32\n+    %0 = arith.addi %arg0, %c32_i32 : i32\n+    %c1_i32 = arith.constant 1 : i32\n+    %1 = arith.subi %0, %c1_i32 : i32\n+    %c32_i32_0 = arith.constant 32 : i32\n+    %2 = arith.divsi %1, %c32_i32_0 : i32\n+    tt.return %2 : i32\n+  }\n+  tt.func private @cdiv__i32__1cconstexpr_64_(%arg0: i32 ) -> i32 attributes {noinline = false} {\n+    %c64_i32 = arith.constant 64 : i32\n+    %0 = arith.addi %arg0, %c64_i32 : i32\n+    %c1_i32 = arith.constant 1 : i32\n+    %1 = arith.subi %0, %c1_i32 : i32\n+    %c64_i32_0 = arith.constant 64 : i32\n+    %2 = arith.divsi %1, %c64_i32_0 : i32\n+    tt.return %2 : i32\n+  }\n+  tt.func private @minimum__i32__1cconstexpr_8_(%arg0: i32 ) -> i32 attributes {noinline = false} {\n+    %c8_i32 = arith.constant 8 : i32\n+    %0 = arith.minsi %arg0, %c8_i32 : i32\n+    tt.return %0 : i32\n+  }\n+  tt.func private @\"zeros____0cconstexpr_(constexpr_32_, constexpr_64_)__1cconstexpr_fp32_\"() -> tensor<32x64xf32> attributes {noinline = false} {\n+    %cst = arith.constant 0.000000e+00 : f32\n+    %cst_0 = arith.constant dense<0.000000e+00> : tensor<32x64xf32>\n+    tt.return %cst_0 : tensor<32x64xf32>\n+  }\n+}\n+ *\/\n","filename":"cr-examples\/triton\/src\/test\/java\/oracle\/code\/triton\/TestMatrix.java","additions":831,"deletions":0,"binary":false,"changes":831,"status":"added"},{"patch":"@@ -0,0 +1,323 @@\n+\/*\n+ * Copyright (c) 2024, Oracle and\/or its affiliates. All rights reserved.\n+ * DO NOT ALTER OR REMOVE COPYRIGHT NOTICES OR THIS FILE HEADER.\n+ *\n+ * This code is free software; you can redistribute it and\/or modify it\n+ * under the terms of the GNU General Public License version 2 only, as\n+ * published by the Free Software Foundation.\n+ *\n+ * This code is distributed in the hope that it will be useful, but WITHOUT\n+ * ANY WARRANTY; without even the implied warranty of MERCHANTABILITY or\n+ * FITNESS FOR A PARTICULAR PURPOSE.  See the GNU General Public License\n+ * version 2 for more details (a copy is included in the LICENSE file that\n+ * accompanied this code).\n+ *\n+ * You should have received a copy of the GNU General Public License version\n+ * 2 along with this work; if not, write to the Free Software Foundation,\n+ * Inc., 51 Franklin St, Fifth Floor, Boston, MA 02110-1301 USA.\n+ *\n+ * Please contact Oracle, 500 Oracle Parkway, Redwood Shores, CA 94065 USA\n+ * or visit www.oracle.com if you need additional information or have any\n+ * questions.\n+ *\/\n+\n+package oracle.code.triton;\n+\n+import oracle.code.triton.TritonTestExtension.Kernel;\n+import oracle.code.triton.TritonTestExtension.TritonTestData;\n+import org.junit.jupiter.api.Test;\n+import org.junit.jupiter.api.extension.ExtendWith;\n+\n+import java.lang.reflect.Type;\n+import java.lang.runtime.CodeReflection;\n+import java.util.List;\n+\n+@ExtendWith(TritonTestExtension.class)\n+public class TestSoftMax {\n+\n+    @TritonCodeModel(\"\"\"\n+            module ()void -> {\n+                tt.func @\"max_float_float_float\" (%0 : float, %1 : float)float -> {\n+                    %2 : float = arith.maximumf %0 %1;\n+                    tt.return %2;\n+                };\n+                tt.func @\"reduce_max_float_float_float_0\" (%3 : tensor<x64, float>)float -> {\n+                    %4 : float = tt.reduce %3 @axis=\"0\" (%5 : float, %6 : float)float -> {\n+                        %7 : float = tt.call %5 %6 @\"max_float_float_float\";\n+                        tt.reduce.return %7;\n+                    };\n+                    tt.return %4;\n+                };\n+                tt.func @\"sum_float_float_float\" (%8 : float, %9 : float)float -> {\n+                    %10 : float = arith.addf %8 %9;\n+                    tt.return %10;\n+                };\n+                tt.func @\"reduce_sum_float_float_float_0\" (%11 : tensor<x64, float>)float -> {\n+                    %12 : float = tt.reduce %11 @axis=\"0\" (%13 : float, %14 : float)float -> {\n+                        %15 : float = tt.call %13 %14 @\"sum_float_float_float\";\n+                        tt.reduce.return %15;\n+                    };\n+                    tt.return %12;\n+                };\n+                tt.func @\"softmax_kernel_ptr<float>_ptr<float>_1_1_10_64_void\" (%16 : ptr<float>, %17 : ptr<float>)void -> {\n+                    %18 : int = arith.constant @\"1\";\n+                    %19 : int = arith.constant @\"1\";\n+                    %20 : int = arith.constant @\"10\";\n+                    %21 : int = tt.get_program_id @\"0\";\n+                    %22 : int = arith.muli %21 %18;\n+                    %23 : ptr<float> = tt.addptr %17 %22;\n+                    %24 : tensor<x64, int> = tt.make_range @start=\"0\" @end=\"64\";\n+                    %25 : tensor<x64, ptr<float>> = tt.splat %23;\n+                    %26 : tensor<x64, ptr<float>> = tt.addptr %25 %24;\n+                    %27 : tensor<x64, int> = tt.splat %20;\n+                    %28 : tensor<x64, int> = arith.cmpi %24 %27 @\"slt\";\n+                    %29 : tensor<x64, float> = tt.load %26 %28;\n+                    %30 : float = tt.call %29 @\"reduce_max_float_float_float_0\";\n+                    %31 : tensor<x64, float> = tt.splat %30;\n+                    %32 : tensor<x64, float> = arith.subf %29 %31;\n+                    %33 : tensor<x64, float> = math.exp %32;\n+                    %34 : float = tt.call %33 @\"reduce_sum_float_float_float_0\";\n+                    %35 : tensor<x64, float> = tt.splat %34;\n+                    %36 : tensor<x64, float> = arith.divf %33 %35;\n+                    %37 : int = arith.muli %21 %19;\n+                    %38 : ptr<float> = tt.addptr %16 %37;\n+                    %39 : tensor<x64, ptr<float>> = tt.splat %38;\n+                    %40 : tensor<x64, ptr<float>> = tt.addptr %39 %24;\n+                    tt.store %40 %36 %28;\n+                    tt.return;\n+                };\n+                unreachable;\n+            };\n+            \"\"\")\n+    @CodeReflection\n+    static void softmax_kernel(Ptr output_ptr,\n+                               Ptr input_ptr,\n+                               int input_row_stride,\n+                               int output_row_stride,\n+                               int n_cols,\n+                               @Constant int BLOCK_SIZE) {\n+        \/\/ The rows of the softmax are independent, so we parallelize across those\n+        var row_idx = Triton.programId(0);\n+        var row_start_ptr = Triton.add(input_ptr, row_idx * input_row_stride);\n+        \/\/ The block size is the next power of two greater than n_cols, so we can fit each\n+        \/\/ row in a single block\n+        var col_offsets = Triton.arange(0, BLOCK_SIZE);\n+        var input_ptrs = Triton.add(Triton.broadcast(row_start_ptr, col_offsets.type()), col_offsets);\n+        \/\/ Load the row into SRAM, using a mask since BLOCK_SIZE may be > than n_cols\n+        var mask = Triton.compare(col_offsets,\n+                Triton.broadcast(n_cols, col_offsets.type()),\n+                Triton.CompareKind.LessThan);\n+        var row = Triton.load(input_ptrs, mask);\n+        \/\/ Subtract maximum for numerical stability\n+        var row_minus_max = Triton.sub(row, Triton.broadcast(Triton.max(row, 0), row.type()));\n+        \/\/ Note that exponentiation in Triton is fast but approximate (i.e., think __expf in CUDA)\n+        var numerator = Triton.exp(row_minus_max);\n+        var denominator = Triton.sum(numerator, 0);\n+        var softmax_output = Triton.div(numerator, Triton.broadcast(denominator, numerator.type()));\n+        \/\/ Write back output to DRAM\n+        var output_row_start_ptr = Triton.add(output_ptr, row_idx * output_row_stride);\n+        var output_ptrs = Triton.add(Triton.broadcast(output_row_start_ptr, col_offsets.type()), col_offsets);\n+        Triton.store(output_ptrs, softmax_output, mask);\n+    }\n+\n+    @Kernel(\"softmax_kernel\")\n+    @Test\n+    public void test(TritonTestData t) {\n+        List<Type> argTypes = List.of(\n+                new PtrType(float.class),\n+                new PtrType(float.class),\n+                new ConstantType(int.class, 1),\n+                new ConstantType(int.class, 1),\n+                new ConstantType(int.class, 10),\n+                new ConstantType(int.class, 64));\n+\n+        t.test(argTypes);\n+    }\n+\n+    @TritonCodeModel(\"\"\"\n+            module ()void -> {\n+                tt.func @\"max_float_float_float\" (%0 : float, %1 : float)float -> {\n+                    %2 : float = arith.maximumf %0 %1;\n+                    tt.return %2;\n+                };\n+                tt.func @\"reduce_max_float_float_float_0\" (%3 : tensor<x64, float>)float -> {\n+                    %4 : float = tt.reduce %3 @axis=\"0\" (%5 : float, %6 : float)float -> {\n+                        %7 : float = tt.call %5 %6 @\"max_float_float_float\";\n+                        tt.reduce.return %7;\n+                    };\n+                    tt.return %4;\n+                };\n+                tt.func @\"sum_float_float_float\" (%8 : float, %9 : float)float -> {\n+                    %10 : float = arith.addf %8 %9;\n+                    tt.return %10;\n+                };\n+                tt.func @\"reduce_sum_float_float_float_0\" (%11 : tensor<x64, float>)float -> {\n+                    %12 : float = tt.reduce %11 @axis=\"0\" (%13 : float, %14 : float)float -> {\n+                        %15 : float = tt.call %13 %14 @\"sum_float_float_float\";\n+                        tt.reduce.return %15;\n+                    };\n+                    tt.return %12;\n+                };\n+                tt.func @\"softmax_kernel2_ptr<float>_ptr<float>_1_1_10_64_void\" (%16 : ptr<float>, %17 : ptr<float>)void -> {\n+                    %18 : int = arith.constant @\"1\";\n+                    %19 : int = arith.constant @\"1\";\n+                    %20 : int = arith.constant @\"10\";\n+                    %21 : int = tt.get_program_id @\"0\";\n+                    %22 : int = arith.muli %21 %18;\n+                    %23 : ptr<float> = tt.addptr %17 %22;\n+                    %24 : tensor<x64, int> = tt.make_range @start=\"0\" @end=\"64\";\n+                    %25 : tensor<x64, ptr<float>> = tt.splat %23;\n+                    %26 : tensor<x64, ptr<float>> = tt.addptr %25 %24;\n+                    %27 : tensor<x64, int> = tt.splat %20;\n+                    %28 : tensor<x64, int> = arith.cmpi %24 %27 @\"slt\";\n+                    %29 : tensor<x64, float> = tt.load %26 %28;\n+                    %30 : float = tt.call %29 @\"reduce_max_float_float_float_0\";\n+                    %31 : tensor<x64, float> = tt.splat %30;\n+                    %32 : tensor<x64, float> = arith.subf %29 %31;\n+                    %33 : tensor<x64, float> = math.exp %32;\n+                    %34 : float = tt.call %33 @\"reduce_sum_float_float_float_0\";\n+                    %35 : tensor<x64, float> = tt.splat %34;\n+                    %36 : tensor<x64, float> = arith.divf %33 %35;\n+                    %37 : int = arith.muli %21 %19;\n+                    %38 : ptr<float> = tt.addptr %16 %37;\n+                    %39 : tensor<x64, ptr<float>> = tt.splat %38;\n+                    %40 : tensor<x64, ptr<float>> = tt.addptr %39 %24;\n+                    tt.store %40 %36 %28;\n+                    tt.return;\n+                };\n+                unreachable;\n+            };\n+            \"\"\")\n+    @CodeReflection\n+    static void softmax_kernel2(Ptr output_ptr,\n+                                Ptr input_ptr,\n+                                int input_row_stride,\n+                                int output_row_stride,\n+                                int n_cols,\n+                                @Constant int BLOCK_SIZE) {\n+        \/\/ The rows of the softmax are independent, so we parallelize across those\n+        var row_idx = Triton.programId(0);\n+        var row_start_ptr = Triton.add(input_ptr, row_idx * input_row_stride);\n+        \/\/ The block size is the next power of two greater than n_cols, so we can fit each\n+        \/\/ row in a single block\n+        var col_offsets = Triton.arange(0, BLOCK_SIZE);\n+        var input_ptrs = Triton.add(row_start_ptr, col_offsets);\n+        \/\/ Load the row into SRAM, using a mask since BLOCK_SIZE may be > than n_cols\n+        var mask = Triton.compare(col_offsets, n_cols, Triton.CompareKind.LessThan);\n+        var row = Triton.load(input_ptrs, mask);\n+        \/\/ Subtract maximum for numerical stability\n+        var row_minus_max = Triton.sub(row, Triton.max(row, 0));\n+        \/\/ Note that exponentiation in Triton is fast but approximate (i.e., think __expf in CUDA)\n+        var numerator = Triton.exp(row_minus_max);\n+        var denominator = Triton.sum(numerator, 0);\n+        var softmax_output = Triton.div(numerator, denominator);\n+        \/\/ Write back output to DRAM\n+        var output_row_start_ptr = Triton.add(output_ptr, row_idx * output_row_stride);\n+        var output_ptrs = Triton.add(output_row_start_ptr, col_offsets);\n+        Triton.store(output_ptrs, softmax_output, mask);\n+    }\n+\n+    @Kernel(\"softmax_kernel2\")\n+    @Test\n+    public void test2(TritonTestData t) {\n+        List<Type> argTypes = List.of(\n+                new PtrType(float.class),\n+                new PtrType(float.class),\n+                new ConstantType(int.class, 1),\n+                new ConstantType(int.class, 1),\n+                new ConstantType(int.class, 10),\n+                new ConstantType(int.class, 64));\n+\n+        t.test(argTypes);\n+    }\n+}\n+\n+\/*\n+def softmax_kernel(output_ptr, input_ptr, input_row_stride, output_row_stride, n_cols, BLOCK_SIZE: tl.constexpr):\n+    # The rows of the softmax are independent, so we parallelize across those\n+    row_idx = tl.program_id(0)\n+    # The stride represents how much we need to increase the pointer to advance 1 row\n+    row_start_ptr = input_ptr + row_idx * input_row_stride\n+    # The block size is the next power of two greater than n_cols, so we can fit each\n+    # row in a single block\n+    col_offsets = tl.arange(0, BLOCK_SIZE)\n+    input_ptrs = row_start_ptr + col_offsets\n+    # Load the row into SRAM, using a mask since BLOCK_SIZE may be > than n_cols\n+    row = tl.load(input_ptrs, mask=col_offsets < n_cols, other=-float('inf'))\n+    # Subtract maximum for numerical stability\n+    row_minus_max = row - tl.max(row, axis=0)\n+    # Note that exponentiation in Triton is fast but approximate (i.e., think __expf in CUDA)\n+    numerator = tl.exp(row_minus_max)\n+    denominator = tl.sum(numerator, axis=0)\n+    softmax_output = numerator \/ denominator\n+    # Write back output to DRAM\n+    output_row_start_ptr = output_ptr + row_idx * output_row_stride\n+    output_ptrs = output_row_start_ptr + col_offsets\n+    tl.store(output_ptrs, softmax_output, mask=col_offsets < n_cols)\n+*\/\n+\n+\/*\n+input_row_stride = 1\n+output_row_stride = 1\n+n_cols=10\n+BLOCK_SIZE=64\n+\n+module {\n+  tt.func public @softmax_kernel_01(%arg0: !tt.ptr<f32, 1> , %arg1: !tt.ptr<f32, 1> ) attributes {noinline = false} {\n+    %0 = tt.get_program_id x : i32\n+    %c1_i32 = arith.constant 1 : i32\n+    %1 = arith.muli %0, %c1_i32 : i32\n+    %2 = tt.addptr %arg1, %1 : !tt.ptr<f32, 1>, i32\n+    %3 = tt.make_range {end = 64 : i32, start = 0 : i32} : tensor<64xi32>\n+    %4 = tt.splat %2 : (!tt.ptr<f32, 1>) -> tensor<64x!tt.ptr<f32, 1>>\n+    %5 = tt.addptr %4, %3 : tensor<64x!tt.ptr<f32, 1>>, tensor<64xi32>\n+    %c10_i32 = arith.constant 10 : i32\n+    %cst = arith.constant dense<10> : tensor<64xi32>\n+    %6 = arith.cmpi slt, %3, %cst : tensor<64xi32>\n+    %cst_0 = arith.constant 0xFF800000 : f32\n+    %cst_1 = arith.constant dense<0xFF800000> : tensor<64xf32>\n+    %7 = tt.load %5, %6, %cst_1 {cache = 1 : i32, evict = 1 : i32, isVolatile = false} : tensor<64xf32>\n+    %8 = tt.call @max__fp32S64S__1cconstexpr_0__2cconstexpr_False__3cconstexpr_True_(%7) : (tensor<64xf32>) -> f32\n+    %9 = tt.splat %8 : (f32) -> tensor<64xf32>\n+    %10 = arith.subf %7, %9 : tensor<64xf32>\n+    %11 = math.exp %10 : tensor<64xf32>\n+    %12 = tt.call @sum__fp32S64S__1cconstexpr_0_(%11) : (tensor<64xf32>) -> f32\n+    %13 = tt.splat %12 : (f32) -> tensor<64xf32>\n+    %14 = arith.divf %11, %13 : tensor<64xf32>\n+    %c1_i32_2 = arith.constant 1 : i32\n+    %15 = arith.muli %0, %c1_i32_2 : i32\n+    %16 = tt.addptr %arg0, %15 : !tt.ptr<f32, 1>, i32\n+    %17 = tt.splat %16 : (!tt.ptr<f32, 1>) -> tensor<64x!tt.ptr<f32, 1>>\n+    %18 = tt.addptr %17, %3 : tensor<64x!tt.ptr<f32, 1>>, tensor<64xi32>\n+    %c10_i32_3 = arith.constant 10 : i32\n+    %cst_4 = arith.constant dense<10> : tensor<64xi32>\n+    %19 = arith.cmpi slt, %3, %cst_4 : tensor<64xi32>\n+    tt.store %18, %14, %19 {cache = 1 : i32, evict = 1 : i32} : tensor<64xf32>\n+    tt.return\n+  }\n+  tt.func private @max__fp32S64S__1cconstexpr_0__2cconstexpr_False__3cconstexpr_True_(%arg0: tensor<64xf32> ) -> f32 attributes {noinline = false} {\n+    %0 = \"tt.reduce\"(%arg0) <{axis = 0 : i32}> ({\n+    ^bb0(%arg1: f32 , %arg2: f32 ):\n+      %1 = tt.call @maximum__fp32_fp32__(%arg1, %arg2) : (f32, f32) -> f32\n+      tt.reduce.return %1 : f32\n+    }) : (tensor<64xf32>) -> f32\n+    tt.return %0 : f32\n+  }\n+  tt.func private @maximum__fp32_fp32__(%arg0: f32 , %arg1: f32 ) -> f32 attributes {noinline = false} {\n+    %0 = arith.maximumf %arg0, %arg1 : f32\n+    tt.return %0 : f32\n+  }\n+  tt.func private @sum__fp32S64S__1cconstexpr_0_(%arg0: tensor<64xf32> ) -> f32 attributes {noinline = false} {\n+    %0 = \"tt.reduce\"(%arg0) <{axis = 0 : i32}> ({\n+    ^bb0(%arg1: f32 , %arg2: f32 ):\n+      %1 = tt.call @_sum_combine__fp32_fp32__(%arg1, %arg2) : (f32, f32) -> f32\n+      tt.reduce.return %1 : f32\n+    }) : (tensor<64xf32>) -> f32\n+    tt.return %0 : f32\n+  }\n+  tt.func private @_sum_combine__fp32_fp32__(%arg0: f32 , %arg1: f32 ) -> f32 attributes {noinline = false} {\n+    %0 = arith.addf %arg0, %arg1 : f32\n+    tt.return %0 : f32\n+  }\n+}\n+*\/\n","filename":"cr-examples\/triton\/src\/test\/java\/oracle\/code\/triton\/TestSoftMax.java","additions":323,"deletions":0,"binary":false,"changes":323,"status":"added"},{"patch":"@@ -0,0 +1,59 @@\n+\/*\n+ * Copyright (c) 2024, Oracle and\/or its affiliates. All rights reserved.\n+ * DO NOT ALTER OR REMOVE COPYRIGHT NOTICES OR THIS FILE HEADER.\n+ *\n+ * This code is free software; you can redistribute it and\/or modify it\n+ * under the terms of the GNU General Public License version 2 only, as\n+ * published by the Free Software Foundation.\n+ *\n+ * This code is distributed in the hope that it will be useful, but WITHOUT\n+ * ANY WARRANTY; without even the implied warranty of MERCHANTABILITY or\n+ * FITNESS FOR A PARTICULAR PURPOSE.  See the GNU General Public License\n+ * version 2 for more details (a copy is included in the LICENSE file that\n+ * accompanied this code).\n+ *\n+ * You should have received a copy of the GNU General Public License version\n+ * 2 along with this work; if not, write to the Free Software Foundation,\n+ * Inc., 51 Franklin St, Fifth Floor, Boston, MA 02110-1301 USA.\n+ *\n+ * Please contact Oracle, 500 Oracle Parkway, Redwood Shores, CA 94065 USA\n+ * or visit www.oracle.com if you need additional information or have any\n+ * questions.\n+ *\/\n+\n+package oracle.code.triton;\n+\n+import org.junit.jupiter.api.Assertions;\n+import org.junit.jupiter.api.Test;\n+import org.junit.jupiter.api.extension.ExtendWith;\n+\n+import java.lang.reflect.Type;\n+import java.lang.runtime.CodeReflection;\n+import java.util.List;\n+\n+import static oracle.code.triton.Triton.zeros;\n+import static oracle.code.triton.TritonTest.consume;\n+\n+@ExtendWith(TritonTestExtension.class)\n+public class TestVariables {\n+\n+    @CodeReflection\n+    static void test1(@Constant int M) {\n+        var m = Triton.arange(0, M);\n+        var e = Triton.expand(m, 0);\n+        consume(e);\n+        \/\/ Store to variable fails since tensor is of different shape\n+        e = Triton.expand(m, 1);\n+        consume(e);\n+    }\n+\n+    @Test\n+    public void test1(TritonTestExtension.TritonTestData t) {\n+        List<Type> argTypes = List.of(\n+                new ConstantType(int.class, 32));\n+\n+        Assertions.assertThrows(IllegalStateException.class, () -> {\n+            t.test(argTypes);\n+        });\n+    }\n+}\n","filename":"cr-examples\/triton\/src\/test\/java\/oracle\/code\/triton\/TestVariables.java","additions":59,"deletions":0,"binary":false,"changes":59,"status":"added"},{"patch":"@@ -0,0 +1,64 @@\n+\/*\n+ * Copyright (c) 2024, Oracle and\/or its affiliates. All rights reserved.\n+ * DO NOT ALTER OR REMOVE COPYRIGHT NOTICES OR THIS FILE HEADER.\n+ *\n+ * This code is free software; you can redistribute it and\/or modify it\n+ * under the terms of the GNU General Public License version 2 only, as\n+ * published by the Free Software Foundation.\n+ *\n+ * This code is distributed in the hope that it will be useful, but WITHOUT\n+ * ANY WARRANTY; without even the implied warranty of MERCHANTABILITY or\n+ * FITNESS FOR A PARTICULAR PURPOSE.  See the GNU General Public License\n+ * version 2 for more details (a copy is included in the LICENSE file that\n+ * accompanied this code).\n+ *\n+ * You should have received a copy of the GNU General Public License version\n+ * 2 along with this work; if not, write to the Free Software Foundation,\n+ * Inc., 51 Franklin St, Fifth Floor, Boston, MA 02110-1301 USA.\n+ *\n+ * Please contact Oracle, 500 Oracle Parkway, Redwood Shores, CA 94065 USA\n+ * or visit www.oracle.com if you need additional information or have any\n+ * questions.\n+ *\/\n+\n+package oracle.code.triton;\n+\n+import org.junit.jupiter.api.Test;\n+import org.junit.jupiter.api.extension.ExtendWith;\n+\n+import java.lang.reflect.Type;\n+import java.lang.runtime.CodeReflection;\n+import java.util.List;\n+\n+import static oracle.code.triton.Triton.*;\n+import static oracle.code.triton.TritonTest.consume;\n+\n+@ExtendWith(TritonTestExtension.class)\n+public class TestZeros {\n+\n+    @TritonCodeModel(\"\"\"\n+            module ()void -> {\n+                tt.func @\"test1_32_64_void\" ()void -> {\n+                    %0 : tensor<x32, x64, float> = arith.constant @\"0.0\";\n+                    tt.consume %0;\n+                    tt.return;\n+                };\n+                unreachable;\n+            };\n+            \"\"\")\n+    @CodeReflection\n+    static void test1(@Constant int M, @Constant int N) {\n+        var t = zeros(float.class, M, N);\n+        consume(t);\n+    }\n+\n+    @Test\n+    public void test1(TritonTestExtension.TritonTestData t) {\n+        List<Type> argTypes = List.of(\n+                new ConstantType(int.class, 32),\n+                new ConstantType(int.class, 64));\n+\n+        t.test(argTypes);\n+    }\n+\n+}\n","filename":"cr-examples\/triton\/src\/test\/java\/oracle\/code\/triton\/TestZeros.java","additions":64,"deletions":0,"binary":false,"changes":64,"status":"added"},{"patch":"@@ -0,0 +1,37 @@\n+\/*\n+ * Copyright (c) 2024, Oracle and\/or its affiliates. All rights reserved.\n+ * DO NOT ALTER OR REMOVE COPYRIGHT NOTICES OR THIS FILE HEADER.\n+ *\n+ * This code is free software; you can redistribute it and\/or modify it\n+ * under the terms of the GNU General Public License version 2 only, as\n+ * published by the Free Software Foundation.\n+ *\n+ * This code is distributed in the hope that it will be useful, but WITHOUT\n+ * ANY WARRANTY; without even the implied warranty of MERCHANTABILITY or\n+ * FITNESS FOR A PARTICULAR PURPOSE.  See the GNU General Public License\n+ * version 2 for more details (a copy is included in the LICENSE file that\n+ * accompanied this code).\n+ *\n+ * You should have received a copy of the GNU General Public License version\n+ * 2 along with this work; if not, write to the Free Software Foundation,\n+ * Inc., 51 Franklin St, Fifth Floor, Boston, MA 02110-1301 USA.\n+ *\n+ * Please contact Oracle, 500 Oracle Parkway, Redwood Shores, CA 94065 USA\n+ * or visit www.oracle.com if you need additional information or have any\n+ * questions.\n+ *\/\n+\n+package oracle.code.triton;\n+\n+import java.lang.annotation.ElementType;\n+import java.lang.annotation.Retention;\n+import java.lang.annotation.RetentionPolicy;\n+import java.lang.annotation.Target;\n+\n+@Target({ElementType.METHOD, ElementType.FIELD})\n+@Retention(RetentionPolicy.RUNTIME)\n+public @interface TritonCodeModel {\n+    String value();\n+\n+    boolean SSA() default true;\n+}\n","filename":"cr-examples\/triton\/src\/test\/java\/oracle\/code\/triton\/TritonCodeModel.java","additions":37,"deletions":0,"binary":false,"changes":37,"status":"added"},{"patch":"@@ -0,0 +1,30 @@\n+\/*\n+ * Copyright (c) 2024, Oracle and\/or its affiliates. All rights reserved.\n+ * DO NOT ALTER OR REMOVE COPYRIGHT NOTICES OR THIS FILE HEADER.\n+ *\n+ * This code is free software; you can redistribute it and\/or modify it\n+ * under the terms of the GNU General Public License version 2 only, as\n+ * published by the Free Software Foundation.\n+ *\n+ * This code is distributed in the hope that it will be useful, but WITHOUT\n+ * ANY WARRANTY; without even the implied warranty of MERCHANTABILITY or\n+ * FITNESS FOR A PARTICULAR PURPOSE.  See the GNU General Public License\n+ * version 2 for more details (a copy is included in the LICENSE file that\n+ * accompanied this code).\n+ *\n+ * You should have received a copy of the GNU General Public License version\n+ * 2 along with this work; if not, write to the Free Software Foundation,\n+ * Inc., 51 Franklin St, Fifth Floor, Boston, MA 02110-1301 USA.\n+ *\n+ * Please contact Oracle, 500 Oracle Parkway, Redwood Shores, CA 94065 USA\n+ * or visit www.oracle.com if you need additional information or have any\n+ * questions.\n+ *\/\n+\n+package oracle.code.triton;\n+\n+public class TritonTest {\n+    public static void consume(Object o) {\n+        throw new UnsupportedOperationException();\n+    }\n+}\n","filename":"cr-examples\/triton\/src\/test\/java\/oracle\/code\/triton\/TritonTest.java","additions":30,"deletions":0,"binary":false,"changes":30,"status":"added"},{"patch":"@@ -0,0 +1,113 @@\n+\/*\n+ * Copyright (c) 2024, Oracle and\/or its affiliates. All rights reserved.\n+ * DO NOT ALTER OR REMOVE COPYRIGHT NOTICES OR THIS FILE HEADER.\n+ *\n+ * This code is free software; you can redistribute it and\/or modify it\n+ * under the terms of the GNU General Public License version 2 only, as\n+ * published by the Free Software Foundation.\n+ *\n+ * This code is distributed in the hope that it will be useful, but WITHOUT\n+ * ANY WARRANTY; without even the implied warranty of MERCHANTABILITY or\n+ * FITNESS FOR A PARTICULAR PURPOSE.  See the GNU General Public License\n+ * version 2 for more details (a copy is included in the LICENSE file that\n+ * accompanied this code).\n+ *\n+ * You should have received a copy of the GNU General Public License version\n+ * 2 along with this work; if not, write to the Free Software Foundation,\n+ * Inc., 51 Franklin St, Fifth Floor, Boston, MA 02110-1301 USA.\n+ *\n+ * Please contact Oracle, 500 Oracle Parkway, Redwood Shores, CA 94065 USA\n+ * or visit www.oracle.com if you need additional information or have any\n+ * questions.\n+ *\/\n+\n+package oracle.code.triton;\n+\n+import org.junit.jupiter.api.Assertions;\n+import org.junit.jupiter.api.extension.ExtensionContext;\n+import org.junit.jupiter.api.extension.ParameterContext;\n+import org.junit.jupiter.api.extension.ParameterResolver;\n+\n+import java.lang.annotation.ElementType;\n+import java.lang.annotation.Retention;\n+import java.lang.annotation.RetentionPolicy;\n+import java.lang.annotation.Target;\n+import java.lang.reflect.Method;\n+import java.lang.reflect.Type;\n+import java.lang.reflect.code.op.CoreOps;\n+import java.lang.reflect.code.parser.OpParser;\n+import java.lang.runtime.CodeReflection;\n+import java.util.List;\n+import java.util.Optional;\n+import java.util.stream.Stream;\n+\n+public class TritonTestExtension implements ParameterResolver {\n+\n+    @Target({ElementType.METHOD, ElementType.FIELD})\n+    @Retention(RetentionPolicy.RUNTIME)\n+    public @interface Kernel {\n+        String value();\n+    }\n+\n+    @Override\n+    public boolean supportsParameter(ParameterContext pc, ExtensionContext ec) {\n+        return pc.getParameter().getType() == TritonTestData.class;\n+    }\n+\n+    @Override\n+    public Object resolveParameter(ParameterContext pc, ExtensionContext ec) {\n+        Kernel k = ec.getRequiredTestMethod().getAnnotation(Kernel.class);\n+        String kernelName = (k != null)\n+            ? k.value()\n+            : ec.getRequiredTestMethod().getName();\n+\n+        return new TritonTestData(ec.getRequiredTestClass(), kernelName);\n+    }\n+\n+    public static class TritonTestData {\n+        final Class<?> testClass;\n+        final String javaKernelName;\n+\n+        public TritonTestData(Class<?> testClass, String javaKernelName) {\n+            this.testClass = testClass;\n+            this.javaKernelName = javaKernelName;\n+        }\n+\n+        public void test(List<Type> argTypes) {\n+            Optional<Method> om = Stream.of(testClass.getDeclaredMethods())\n+                    .filter(m -> m.getName().equals(javaKernelName))\n+                    .filter(m -> m.getAnnotation(CodeReflection.class) != null)\n+                    .findFirst();\n+            Method m = om.get();\n+            TritonCodeModel tcm = m.getAnnotation(TritonCodeModel.class);\n+            boolean doSSA = tcm != null ? tcm.SSA() : true;\n+            test(m.getCodeModel().get(), argTypes, expectedTritonKernel(tcm), doSSA);\n+        }\n+\n+        public TritonOps.ModuleOp expectedTritonKernel(TritonCodeModel tcm) {\n+            if (tcm == null || tcm.value().isEmpty()) {\n+                return null;\n+            }\n+\n+            return (TritonOps.ModuleOp) OpParser.fromString(\n+                    TritonOps.FACTORY.andThen(ArithMathOps.FACTORY)\n+                            .andThen(TritonTestOps.FACTORY)\n+                            .andThen(SCFOps.FACTORY)\n+                            .andThen(CoreOps.FACTORY),\n+                    tcm.value()).get(0);\n+        }\n+\n+        void test(CoreOps.FuncOp javaKernel,\n+                  List<Type> argTypes,\n+                  TritonOps.ModuleOp expectedTritonKernel,\n+                  boolean doSSA) {\n+            TritonOps.ModuleOp actualTritonKernel = ScopedValue.getWhere(TritonTransformer.SV_SSA, doSSA,() -> {\n+                return TritonTransformer.tritonModule(javaKernel, void.class, argTypes);\n+            });\n+\n+            Assertions.assertEquals(actualTritonKernel.toText(),\n+                    expectedTritonKernel == null ? \"NO @TritonCodeModel\" : expectedTritonKernel.toText());\n+        }\n+    }\n+\n+}\n","filename":"cr-examples\/triton\/src\/test\/java\/oracle\/code\/triton\/TritonTestExtension.java","additions":113,"deletions":0,"binary":false,"changes":113,"status":"added"}]}