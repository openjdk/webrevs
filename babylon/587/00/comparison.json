{"files":[{"patch":"@@ -140,0 +140,2 @@\n+    bool isSpecific = kernelContext->isSpecific;\n+\n@@ -161,1 +163,0 @@\n-\n@@ -169,0 +170,6 @@\n+    if (isSpecific) {\n+        blocksPerGridX = kernelContext->maxX;\n+        blocksPerGridY = kernelContext->maxY;\n+        blocksPerGridZ = kernelContext->maxZ;\n+    }\n+\n@@ -177,1 +184,1 @@\n-    if (thread_id != streamCreationThread){\n+    if (thread_id != streamCreationThread) {\n@@ -181,5 +188,5 @@\n-    \/\/ \/\/ CUDA events for timing\n-    \/\/ cudaEvent_t start, stop;\n-    \/\/ cuEventCreate(&start, cudaEventDefault);\n-    \/\/ cuEventCreate(&stop, cudaEventDefault);\n-    \/\/ cuEventRecord(start, 0);\n+\/\/     \/\/ CUDA events for timing\n+\/\/     cudaEvent_t start, stop;\n+\/\/     cuEventCreate(&start, cudaEventDefault);\n+\/\/     cuEventCreate(&stop, cudaEventDefault);\n+\/\/     cuEventRecord(start, 0);\n@@ -194,6 +201,5 @@\n-    \/\/ cuEventRecord(stop, 0);\n-    \/\/ cuEventSynchronize(stop); \/\/ Wait for completion\n-    \/\/\n-    \/\/ float elapsedTimeMs = 0.0f;\n-    \/\/ cuEventElapsedTime(&elapsedTimeMs, start, stop);\n-    \/\/ std::cout << \"Kernel Elapsed Time: \" << elapsedTimeMs << \" ms\\n\";\n+\/\/     cuEventRecord(stop, 0);\n+\/\/     cuEventSynchronize(stop);\n+\/\/     float elapsedTimeMs = 0.0f;\n+\/\/     cuEventElapsedTime(&elapsedTimeMs, start, stop);\n+\/\/     std::cout << \"Kernel Elapsed Time: \" << elapsedTimeMs << \" ms\\n\";\n","filename":"hat\/backends\/ffi\/cuda\/src\/main\/native\/cpp\/cuda_backend_queue.cpp","additions":19,"deletions":13,"binary":false,"changes":32,"status":"modified"},{"patch":"@@ -99,1 +99,0 @@\n-            \/\/kernelContext.dimensions(threadMesh.getDims());\n@@ -119,0 +118,1 @@\n+                kernelContext.isSpecific(computeRange.isSpecificRange());\n@@ -131,0 +131,1 @@\n+\n","filename":"hat\/backends\/ffi\/shared\/src\/main\/java\/hat\/backend\/ffi\/C99FFIBackend.java","additions":2,"deletions":1,"binary":false,"changes":3,"status":"modified"},{"patch":"@@ -396,0 +396,2 @@\n+\n+    bool isSpecific;\n","filename":"hat\/backends\/ffi\/shared\/src\/main\/native\/include\/shared.h","additions":2,"deletions":0,"binary":false,"changes":2,"status":"modified"},{"patch":"@@ -0,0 +1,57 @@\n+\/*\n+ * Copyright (c) 2025, Oracle and\/or its affiliates. All rights reserved.\n+ * DO NOT ALTER OR REMOVE COPYRIGHT NOTICES OR THIS FILE HEADER.\n+ *\n+ * This code is free software; you can redistribute it and\/or modify it\n+ * under the terms of the GNU General Public License version 2 only, as\n+ * published by the Free Software Foundation.  Oracle designates this\n+ * particular file as subject to the \"Classpath\" exception as provided\n+ * by Oracle in the LICENSE file that accompanied this code.\n+ *\n+ * This code is distributed in the hope that it will be useful, but WITHOUT\n+ * ANY WARRANTY; without even the implied warranty of MERCHANTABILITY or\n+ * FITNESS FOR A PARTICULAR PURPOSE.  See the GNU General Public License\n+ * version 2 for more details (a copy is included in the LICENSE file that\n+ * accompanied this code).\n+ *\n+ * You should have received a copy of the GNU General Public License version\n+ * 2 along with this work; if not, write to the Free Software Foundation,\n+ * Inc., 51 Franklin St, Fifth Floor, Boston, MA 02110-1301 USA.\n+ *\n+ * Please contact Oracle, 500 Oracle Parkway, Redwood Shores, CA 94065 USA\n+ * or visit www.oracle.com if you need additional information or have any\n+ * questions.\n+ *\/\n+package hat;\n+\n+public class CUDARange extends ComputeRange {\n+\n+    public CUDARange(GlobalMesh1D globalMesh) {\n+        super(globalMesh);\n+    }\n+\n+    public CUDARange(GlobalMesh1D globalMesh, LocalMesh1D localMesh) {\n+        super(globalMesh, localMesh);\n+    }\n+\n+    public CUDARange(GlobalMesh2D globalMesh) {\n+        super(globalMesh);\n+    }\n+\n+    public CUDARange(GlobalMesh2D globalMesh, LocalMesh2D localMesh) {\n+        super(globalMesh, localMesh);\n+    }\n+\n+    public CUDARange(GlobalMesh3D globalMesh) {\n+        super(globalMesh);\n+    }\n+\n+    public CUDARange(GlobalMesh3D globalMesh, LocalMesh3D localMesh) {\n+        super(globalMesh, localMesh);\n+    }\n+\n+    @Override\n+    public boolean isSpecificRange() {\n+        return true;\n+    }\n+}\n","filename":"hat\/core\/src\/main\/java\/hat\/CUDARange.java","additions":57,"deletions":0,"binary":false,"changes":57,"status":"added"},{"patch":"@@ -38,2 +38,2 @@\n-    final private ThreadMesh globalMesh;\n-    final private ThreadMesh localMesh;\n+    private final ThreadMesh globalMesh;\n+    private final ThreadMesh localMesh;\n@@ -126,0 +126,4 @@\n+\n+    public boolean isSpecificRange() {\n+        return false;\n+    }\n","filename":"hat\/core\/src\/main\/java\/hat\/ComputeRange.java","additions":6,"deletions":2,"binary":false,"changes":8,"status":"modified"},{"patch":"@@ -103,0 +103,3 @@\n+    boolean isSpecific();\n+    void isSpecific(boolean specific);\n+\n@@ -111,1 +114,2 @@\n-                            \"bix\", \"biy\", \"biz\"  \/\/ block id\n+                            \"bix\", \"biy\", \"biz\",   \/\/ block id\n+                            \"isSpecific\"\n@@ -144,0 +148,2 @@\n+        kernelContext.isSpecific(false);\n+\n","filename":"hat\/core\/src\/main\/java\/hat\/buffer\/KernelContext.java","additions":7,"deletions":1,"binary":false,"changes":8,"status":"modified"},{"patch":"@@ -433,0 +433,1 @@\n+\n@@ -477,2 +478,0 @@\n-\n-\n","filename":"hat\/core\/src\/main\/java\/hat\/codebuilders\/HATCodeBuilderWithContext.java","additions":1,"deletions":2,"binary":false,"changes":3,"status":"modified"},{"patch":"@@ -28,0 +28,1 @@\n+import hat.CUDARange;\n@@ -69,1 +70,1 @@\n-    private static final int NUM_ITERATIONS = 100;\n+    private static final int NUM_ITERATIONS = 10;\n@@ -179,0 +180,366 @@\n+    private interface SharedMemory extends Buffer {\n+        void array(long index, float value);\n+        float array(long index);\n+        Schema<SharedMemory> schema = Schema.of(SharedMemory.class,\n+                arr -> arr.array(\"array\", 1024));\n+        static SharedMemory create(Accelerator accelerator) {\n+            return schema.allocate(accelerator);\n+        }\n+        static SharedMemory createLocal() {\n+            return schema.allocate(new Accelerator(MethodHandles.lookup(), Backend.FIRST));\n+        }\n+    }\n+\n+    private interface PrivateArray extends Buffer {\n+        void array(long index, float value);\n+        float array(long index);\n+        Schema<PrivateArray> schema = Schema.of(PrivateArray.class,\n+                arr -> arr.array(\"array\", 64));\n+        static PrivateArray create(Accelerator accelerator) {\n+            return schema.allocate(accelerator);\n+        }\n+        static PrivateArray createPrivate() {\n+            return schema.allocate(new Accelerator(MethodHandles.lookup(), Backend.FIRST));\n+        }\n+    }\n+\n+    private interface FlatPrivate extends Buffer {\n+        void array(long index, float value);\n+        float array(long index);\n+        Schema<FlatPrivate> schema = Schema.of(FlatPrivate.class,\n+                arr -> arr.array(\"array\", 8));\n+        static FlatPrivate create(Accelerator accelerator) {\n+            return schema.allocate(accelerator);\n+        }\n+        static FlatPrivate createPrivate() {\n+            return schema.allocate(new Accelerator(MethodHandles.lookup(), Backend.FIRST));\n+        }\n+    }\n+\n+    \/**\n+     * Algorithm for MatMul using 2D Cache (shared memory), Loop Tiling and 2D Register Tiling.\n+     *\n+     * <p>\n+     *     We want to probe that HAT can represent more complex optimisations, and make use of the\n+     *     different levels of memory hierarchy, such as shared memory (as in CUDA shared memory),\n+     *     and private memory. This code has been tested on NVIDIA A10 GPUs.\n+     * <\/p>\n+     *\n+     * <p>\n+     *     The code has been adapted from CUDA to HAT based on the algorithms presented here:\n+     *     {@url https:\/\/siboehm.com\/articles\/22\/CUDA-MMM}\n+     * <\/p>\n+     *\n+     * @param kc\n+     * @param matrixA\n+     * @param matrixB\n+     * @param matrixC\n+     * @param size\n+     *\/\n+    @CodeReflection\n+    public static void matrixMultiplyKernel2DRegisterTiling(@RO KernelContext kc, @RO F32Array matrixA, @RO F32Array matrixB, @RW F32Array matrixC, int size) {\n+\n+        int bx = kc.bix;\n+        int by = kc.biy;\n+\n+        \/\/ Configuration for the kernel\n+        final int BM = 128;\n+        final int BN = 128;\n+        final int BK = 8;\n+        final int TM = 8;    \/\/ Register Block\n+        final int TN = 8;    \/\/ Register block\n+\n+        int totalResultsBlockTile = BM * BN;\n+        final int numThreadsBlockTile = totalResultsBlockTile \/ (TM * TN);\n+\n+        final int limitA = (BN \/ TN);\n+        final int limitB = (BN \/ TN);\n+        final int threadCol = kc.lix % limitA;\n+        final int threadRow = kc.lix \/ limitB;\n+\n+        SharedMemory tileA = SharedMemory.createLocal();\n+        SharedMemory tileB = SharedMemory.createLocal();\n+\n+        int aFrom = by * BM * size;\n+        int bFrom = bx * BN;\n+        int v = bx * BN;\n+        int cFrom = (by * BM * size) + (v);\n+\n+        final int innerRowA = kc.lix \/ BK;\n+        final int innerColA = kc.lix % BK;\n+\n+        final int strideA = numThreadsBlockTile \/ BK;\n+        final int innerRowB = kc.lix \/ BN;\n+        final int innerColB = kc.lix % BN;\n+\n+        int strideB = numThreadsBlockTile \/ BN;\n+\n+        \/\/ Declarations of the arrays in private memory to perform register tiling\n+        PrivateArray threadResults = PrivateArray.createPrivate();\n+        FlatPrivate regM = FlatPrivate.createPrivate();\n+        FlatPrivate regN = FlatPrivate.createPrivate();\n+\n+        \/\/ initialize values\n+        for (int i = 0; i < (TN * TN); i++) {\n+            threadResults.array(i, 0.0f);\n+        }\n+\n+        \/\/ Each thread loops over the tiles\n+        for (int bkIdx = 0; bkIdx < size; bkIdx += BK) {\n+\n+            \/\/ A) Load data into shared memory for array A\n+            for (int loadOffset = 0; loadOffset < BM; loadOffset += strideA) {\n+                tileA.array((innerRowA + loadOffset) * BK + innerColA,\n+                        matrixA.array(((innerRowA + loadOffset) * size + innerColA) + aFrom));\n+            }\n+\n+            \/\/ B) Load data matrixB into shared memory for array B\n+            for (int loadOffset = 0; loadOffset < BK; loadOffset += strideB) {\n+                tileB.array((innerRowB + loadOffset) * BN + innerColB,\n+                        matrixB.array(((innerRowB + loadOffset) * size + innerColB) + bFrom));\n+            }\n+            kc.barrier();\n+\n+            aFrom += (BK);\n+            int f = BK * size;\n+            bFrom += f;\n+\n+            \/\/ Per-thread, we load the data from the shared memory into register for both\n+            \/\/ array A and array B (matrix A and B), and then perform the reduction within\n+            \/\/ the small region in private memory.\n+            for (int dotIdx = 0; dotIdx < BK; dotIdx++) {\n+                \/\/ block into registers\n+                for (int i = 0; i < TM; i++) {\n+                    regM.array(i,  tileA.array((threadRow * TM + i) * BK + dotIdx));\n+                }\n+                for (int i = 0; i < TN; i++) {\n+                    regN.array(i,  tileB.array(dotIdx * BN + threadCol * TN + i));\n+                }\n+                for (int resIdxM = 0; resIdxM < TM; resIdxM++) {\n+                    for (int resIdxN = 0; resIdxN < TN; resIdxN++) {\n+                        float val = regM.array(resIdxM) * regN.array(resIdxN);\n+                        float acc = threadResults.array(resIdxM * TN + resIdxN);\n+                        acc += val;\n+                        threadResults.array((resIdxM * TN + resIdxN), (acc));\n+                    }\n+                }\n+            }\n+            kc.barrier();\n+        }\n+\n+        \/\/ Finally, we store the results of the reductions for the whole 2D register block into global memory.\n+        \/\/ Essentially, each thread compute a small block of TM * TN sub-block size.\n+        for (int resIdxM = 0; resIdxM < TM; resIdxM++) {\n+            for (int resIdxN = 0; resIdxN < TN; resIdxN++) {\n+                float value = threadResults.array(resIdxM * TN + resIdxN);\n+                matrixC.array((((threadRow * TM + resIdxM) * size + threadCol * TN + resIdxN) + (cFrom)), value);\n+            }\n+        }\n+    }\n+\n+    private interface SharedWithPad extends Buffer {\n+        void array(long index, float value);\n+        float array(long index);\n+        Schema<SharedWithPad> schema = Schema.of(SharedWithPad.class,\n+                arr -> arr.array(\"array\", 1088));   \/\/ The size is BLOCK_M * (TILE_K + 1) to mitigate memory bank conflicts\n+        static SharedWithPad create(Accelerator accelerator) {\n+            return schema.allocate(accelerator);\n+        }\n+        static SharedWithPad createLocal() {\n+            return schema.allocate(new Accelerator(MethodHandles.lookup(), Backend.FIRST));\n+        }\n+    }\n+\n+    private interface SharedWithPadB extends Buffer {\n+        void array(long index, float value);\n+        float array(long index);\n+        Schema<SharedWithPadB> schema = Schema.of(SharedWithPadB.class,\n+                arr -> arr.array(\"array\", 1040));    \/\/ The size is TILE_K * (BLOCk_M + 1) to mitigate memory bank conflicts\n+        static SharedWithPadB create(Accelerator accelerator) {\n+            return schema.allocate(accelerator);\n+        }\n+        static SharedWithPadB createLocal() {\n+            return schema.allocate(new Accelerator(MethodHandles.lookup(), Backend.FIRST));\n+        }\n+    }\n+\n+    private interface PrivateAcc extends Buffer {\n+        void array(long index, float value);\n+        float array(long index);\n+        Schema<PrivateAcc> schema = Schema.of(PrivateAcc.class,\n+                arr -> arr.array(\"array\", 16));\n+        static PrivateAcc create(Accelerator accelerator) {\n+            return schema.allocate(accelerator);\n+        }\n+        static PrivateAcc createPrivate() {\n+            return schema.allocate(new Accelerator(MethodHandles.lookup(), Backend.FIRST));\n+        }\n+    }\n+\n+    private interface PrivateReg extends Buffer {\n+        void array(long index, float value);\n+        float array(long index);\n+        Schema<PrivateReg> schema = Schema.of(PrivateReg.class,\n+                arr -> arr.array(\"array\", 4));\n+        static PrivateReg create(Accelerator accelerator) {\n+            return schema.allocate(accelerator);\n+        }\n+        static PrivateReg createPrivate() {\n+            return schema.allocate(new Accelerator(MethodHandles.lookup(), Backend.FIRST));\n+        }\n+    }\n+\n+    \/**\n+     * The following algorithm is a variant of the previous kernel to show register tiling and 2D thread-block. This algorithm\n+     * is portable across CUDA and OpenCL backend of the HAT compiler\/runtime. It can be tuned by changing the configuration\n+     * parameters at the beginning of the function. Some notes for auto-tuning:\n+     * - Global size must be multiples of local group, and cover (BLOCK_M x BLOCK_N) per group\n+     * - BLOCK_M = WG_M * REG_M, BLOCK_N = WG_N * REG_N\n+     *\n+     * <p>\n+     * Some possible values for tuning: WG_M {8 .. 32}, and WG_N {8 .. 32}, REG_M {1 .. 4}, REG_N {1 .. 4}, TILE_K {16 .. 128}.\n+     * Depending on the GPU used, some of the configuration values might be too high\/illegal.\n+     * <\/p>\n+     *\n+     * <p>\n+     * The goal of this example is to show how HAT could be used to express more complex algorithm and optimizations for GPUs,\n+     * and how HAT could table performance tuning.\n+     * <\/p>\n+     *\n+     * @param kc\n+     * @param matrixA\n+     * @param matrixB\n+     * @param matrixC\n+     * @param size\n+     *\/\n+    @CodeReflection\n+    public static void matrixMultiplyKernel2DRegisterTilingPortable(@RO KernelContext kc, @RO F32Array matrixA, @RO F32Array matrixB, @RW F32Array matrixC, int size) {\n+\n+        \/\/ Configuration for this kernel\n+        final int WG_M = 16;\n+        final int WG_N = 16;\n+        final int REG_M = 4;\n+        final int REG_N = 4;\n+        final int TILE_K = 16;\n+        final int BLOCK_M = WG_M * REG_M;\n+        final int BLOCK_N = WG_N * REG_N;\n+\n+        \/\/ We compute squares matrices for simplification. But the code is\n+        \/\/ prepared to compute any compatible matmul sizes\n+        final int M = size;\n+        final int N = size;\n+        final int K = size;\n+        final int lda = size;\n+        final int ldb = size;\n+        final int ldc = size;\n+\n+        final int lid_m = kc.lix;\n+        final int lid_n = kc.liy;\n+        final int gid_m = kc.bix;\n+        final int gid_n = kc.biy;\n+\n+        \/\/ starting index of the tile in the matrix\n+        final int blockRow = gid_m * BLOCK_M;\n+        final int blockCol = gid_n * BLOCK_N;\n+\n+        \/\/ Within the block, each thread computes REG_M x REG_N micro-tile\n+        \/\/ start at (blockRow + lid_m * REG_M) and (blockCol + lid_n * REG_N)\n+        final int cRowBase = blockRow + lid_m * REG_M;\n+        final int cColBase = blockCol + lid_n * REG_N;\n+\n+        \/\/ Accumulators in private memory. This is set to REG_M x REG_N values\n+        PrivateAcc acc = PrivateAcc.createPrivate();\n+        for (int i = 0; i < REG_M; i++) {\n+            for (int j = 0; j < REG_N; j++) {\n+                acc.array(i * REG_M + j, 0.0f);\n+            }\n+        }\n+\n+        \/\/ Shared memory to store tiles of matrixA and matrixB\n+        SharedWithPad sharedA = SharedWithPad.createLocal();\n+        SharedWithPadB sharedB = SharedWithPadB.createLocal();\n+\n+        \/\/ Padding scales to access local memory.\n+        \/\/ We add +1 to mitigate memory bank conflicts.\n+        final int padA = TILE_K + 1;\n+        final int padB = BLOCK_M + 1;\n+        for (int tileIndex = 0; tileIndex < K; tileIndex += TILE_K) {\n+            \/\/ Load A tile (Block_M x Tile_K)\n+            for (int i = 0; i < REG_M; i++) {\n+                final int aRow = cRowBase + i;\n+                for (int kk = lid_n; kk < TILE_K; kk += WG_N) {\n+                    final int aCol = tileIndex + kk;\n+                    float valA = 0.0f;\n+                    if (aRow < M && aCol < K) {\n+                        valA = matrixA.array(aRow * lda + aCol);\n+                    }\n+                    sharedA.array(((aRow - blockRow) * padA) + kk, valA);\n+                }\n+            }\n+\n+            \/\/ Load B tile (Tile_K x Block_N)\n+            for (int j = 0; j < REG_N; j++) {\n+                final int bCol = cColBase + j;\n+                for (int kk = lid_m; kk < TILE_K; kk += WG_M) {\n+                    final int bRow = tileIndex + kk;\n+                    float valB = 0.0f;\n+                    if (bRow < K && bCol < N) {\n+                        valB = matrixB.array(bRow * ldb + bCol);\n+                    }\n+                    sharedB.array((kk * padB) + (bCol - blockCol), valB);\n+                }\n+            }\n+            kc.barrier();\n+\n+            \/\/ Compute the tile acc += sharedA[:,k] * sharedB[k,:]\n+            for (int kk = 0; kk < TILE_K && (tileIndex + kk) < K; kk++) {\n+                \/\/ Load the A and B operands into registers to reuse for REG_M x REG_N accumulations\n+                PrivateReg aReg = PrivateReg.createPrivate();\n+                PrivateReg bReg = PrivateReg.createPrivate();\n+\n+                \/\/ Fetch a column kk of sharedA to private memory\n+                for (int i = 0; i < REG_M; i++) {\n+                    final int aRowL = (cRowBase + i) - blockRow;\n+                    float valPrivA = 0.0f;\n+                    if ((cRowBase + i) < M) {\n+                        valPrivA = sharedA.array(aRowL * padA + kk);\n+                    }\n+                    aReg.array(i, valPrivA);\n+                }\n+\n+                \/\/ Fetch a row kk of sharedB to private memory\n+                for (int j = 0; j < REG_N; j++) {\n+                    final int bColL = (cColBase + j) - blockCol;\n+                    float valPrivB = 0.0f;\n+                    if ((cColBase + j) < N) {\n+                        valPrivB = sharedB.array(kk * padB + bColL);\n+                    }\n+                    bReg.array(j, valPrivB);\n+                }\n+\n+                \/\/ FMA over the register tile\n+                for (int i = 0; i < REG_M; i++) {\n+                    final float a_ik = aReg.array(i);\n+                    for (int j = 0; j < REG_N; j++) {\n+                        float valRes = acc.array(i * REG_M + j);\n+                        valRes += a_ik * bReg.array(j);\n+                        acc.array(i * REG_M + j, valRes);\n+                    }\n+                }\n+            }\n+            kc.barrier();\n+        }\n+        \/\/ Write back the result of the register tile into matrixC\n+        for (int i = 0; i < REG_M; i++) {\n+            final int row = cRowBase + i;\n+            if (row < M) {\n+                for (int j = 0; j < REG_N; j++) {\n+                    final int col = cColBase + j;\n+                    if (col < N) {\n+                        matrixC.array(row * ldc + col, acc.array(i * REG_M + j));\n+                    }\n+                }\n+            }\n+        }\n+    }\n+\n@@ -265,0 +632,16 @@\n+    @CodeReflection\n+    public static void matrixMultiply2DRegisterTiling(@RO ComputeContext cc, @RO F32Array matrixA, @RO F32Array matrixB, @RW  F32Array matrixC, int globalSize) {\n+        CUDARange cudaRange = new CUDARange(new GlobalMesh2D(8, 8), new LocalMesh2D(256, 1));\n+        cc.dispatchKernel(cudaRange,\n+                kc -> matrixMultiplyKernel2DRegisterTiling(kc, matrixA, matrixB, matrixC, globalSize)\n+        );\n+    }\n+\n+    @CodeReflection\n+    public static void matrixMultiply2DRegisterTilingPortable(@RO ComputeContext cc, @RO F32Array matrixA, @RO F32Array matrixB, @RW  F32Array matrixC, int globalSize) {\n+        ComputeRange cudaRange = new ComputeRange(new GlobalMesh2D(256, 256), new LocalMesh2D(16, 16));\n+        cc.dispatchKernel(cudaRange,\n+                kc -> matrixMultiplyKernel2DRegisterTilingPortable(kc, matrixA, matrixB, matrixC, globalSize)\n+        );\n+    }\n+\n@@ -290,0 +673,2 @@\n+        _2DREGISTER_TILING,\n+        _2DREGISTER_TILING_PORTABLE,\n@@ -298,1 +683,1 @@\n-    public static void main(String[] args) {\n+    static void main(String[] args) {\n@@ -310,0 +695,2 @@\n+                case \"2DREGISTERTILING\" -> Configuration._2DREGISTER_TILING;\n+                case \"2DRTPORTABLE\" -> Configuration._2DREGISTER_TILING_PORTABLE;\n@@ -339,1 +726,6 @@\n-        for (int it = 0; it < NUM_ITERATIONS; it++) {\n+        IO.println(\"BACKEND: \"  + accelerator.backend.getName());\n+        String backendName = accelerator.backend.getName();\n+\n+        if (configuration == Configuration._2DREGISTER_TILING && !backendName.equals(\"hat.backend.ffi.CudaBackend\")) {\n+            throw new UnsupportedOperationException(\"MxM with 2D Register tiling is only supported on CUDA backend\");\n+        }\n@@ -341,0 +733,1 @@\n+        for (int it = 0; it < NUM_ITERATIONS; it++) {\n@@ -345,1 +738,1 @@\n-                        Main.matrixMultiply1D(cc, matrixA, matrixB, matrixC, size));\n+                        matrixMultiply1D(cc, matrixA, matrixB, matrixC, size));\n@@ -347,1 +740,1 @@\n-                        Main.matrixMultiply1DWithFunctionCalls(cc, matrixA, matrixB, matrixC, size));\n+                        matrixMultiply1DWithFunctionCalls(cc, matrixA, matrixB, matrixC, size));\n@@ -349,1 +742,1 @@\n-                        Main.matrixMultiply2D(cc, matrixA, matrixB, matrixC, size));\n+                        matrixMultiply2D(cc, matrixA, matrixB, matrixC, size));\n@@ -351,1 +744,1 @@\n-                        Main.matrixMultiply2DLI(cc, matrixA, matrixB, matrixC, size));\n+                        matrixMultiply2DLI(cc, matrixA, matrixB, matrixC, size));\n@@ -353,1 +746,5 @@\n-                        Main.matrixMultiply2DTiling(cc, matrixA, matrixB, matrixC, size));\n+                        matrixMultiply2DTiling(cc, matrixA, matrixB, matrixC, size));\n+                case _2DREGISTER_TILING -> accelerator.compute(cc ->\n+                            matrixMultiply2DRegisterTiling(cc, matrixA, matrixB, matrixC, size));\n+                case _2DREGISTER_TILING_PORTABLE -> accelerator.compute(cc ->\n+                        matrixMultiply2DRegisterTilingPortable(cc, matrixA, matrixB, matrixC, size));\n@@ -359,1 +756,1 @@\n-            \/\/ If check result is ON, then check first and lat iterations\n+            \/\/ If the check is ON, then check first and lat iterations\n@@ -366,0 +763,1 @@\n+                            IO.println(resultSeq.array(i * size + j) + \" != \" + matrixC.array(i * size + j));\n","filename":"hat\/examples\/matmul\/src\/main\/java\/matmul\/Main.java","additions":407,"deletions":9,"binary":false,"changes":416,"status":"modified"},{"patch":"@@ -399,0 +399,226 @@\n+\n+    private interface SharedWithPad extends Buffer {\n+        void array(long index, float value);\n+        float array(long index);\n+        Schema<SharedWithPad> schema = Schema.of(SharedWithPad.class,\n+                arr -> arr.array(\"array\", 1088));   \/\/ The size is BLOCK_M * (TILE_K + 1) to mitigate memory bank conflicts\n+        static SharedWithPad create(Accelerator accelerator) {\n+            return schema.allocate(accelerator);\n+        }\n+        static SharedWithPad createLocal() {\n+            return schema.allocate(new Accelerator(MethodHandles.lookup(), Backend.FIRST));\n+        }\n+    }\n+\n+    private interface SharedWithPadB extends Buffer {\n+        void array(long index, float value);\n+        float array(long index);\n+        Schema<SharedWithPadB> schema = Schema.of(SharedWithPadB.class,\n+                arr -> arr.array(\"array\", 1040));    \/\/ The size is TILE_K * (BLOCk_M + 1) to mitigate memory bank conflicts\n+        static SharedWithPadB create(Accelerator accelerator) {\n+            return schema.allocate(accelerator);\n+        }\n+        static SharedWithPadB createLocal() {\n+            return schema.allocate(new Accelerator(MethodHandles.lookup(), Backend.FIRST));\n+        }\n+    }\n+\n+    private interface PrivateAcc extends Buffer {\n+        void array(long index, float value);\n+        float array(long index);\n+        Schema<PrivateAcc> schema = Schema.of(PrivateAcc.class,\n+                arr -> arr.array(\"array\", 16));\n+        static PrivateAcc create(Accelerator accelerator) {\n+            return schema.allocate(accelerator);\n+        }\n+        static PrivateAcc createPrivate() {\n+            return schema.allocate(new Accelerator(MethodHandles.lookup(), Backend.FIRST));\n+        }\n+    }\n+\n+    private interface PrivateReg extends Buffer {\n+        void array(long index, float value);\n+        float array(long index);\n+        Schema<PrivateReg> schema = Schema.of(PrivateReg.class,\n+                arr -> arr.array(\"array\", 4));\n+        static PrivateReg create(Accelerator accelerator) {\n+            return schema.allocate(accelerator);\n+        }\n+        static PrivateReg createPrivate() {\n+            return schema.allocate(new Accelerator(MethodHandles.lookup(), Backend.FIRST));\n+        }\n+    }\n+\n+    \/\/ Ported from the HAT examples module.\n+    @CodeReflection\n+    public static void matrixMultiplyKernel2DRegisterTilingPortable(@MappableIface.RO KernelContext kc, @MappableIface.RO F32Array matrixA, @MappableIface.RO F32Array matrixB, @MappableIface.RW F32Array matrixC, int size) {\n+\n+        \/\/ Configuration for this kernel\n+        final int WG_M = 16;\n+        final int WG_N = 16;\n+        final int REG_M = 4;\n+        final int REG_N = 4;\n+        final int TILE_K = 16;\n+        final int BLOCK_M = WG_M * REG_M;\n+        final int BLOCK_N = WG_N * REG_N;\n+\n+        \/\/ We compute squares matrices for simplification. But the code is\n+        \/\/ prepared to compute any compatible matmul sizes\n+        final int M = size;\n+        final int N = size;\n+        final int K = size;\n+        final int lda = size;\n+        final int ldb = size;\n+        final int ldc = size;\n+\n+        final int lid_m = kc.lix;\n+        final int lid_n = kc.liy;\n+        final int gid_m = kc.bix;\n+        final int gid_n = kc.biy;\n+\n+        \/\/ starting index of the tile in the matrix\n+        final int blockRow = gid_m * BLOCK_M;\n+        final int blockCol = gid_n * BLOCK_N;\n+\n+        \/\/ Within the block, each thread computes REG_M x REG_N micro-tile\n+        \/\/ start at (blockRow + lid_m * REG_M) and (blockCol + lid_n * REG_N)\n+        final int cRowBase = blockRow + lid_m * REG_M;\n+        final int cColBase = blockCol + lid_n * REG_N;\n+\n+        \/\/ Accumulators in private memory. This is set to REG_M x REG_N values\n+        PrivateAcc acc = PrivateAcc.createPrivate();\n+        for (int i = 0; i < REG_M; i++) {\n+            for (int j = 0; j < REG_N; j++) {\n+                acc.array(i * REG_M + j, 0.0f);\n+            }\n+        }\n+\n+        \/\/ Shared memory to store tiles of matrixA and matrixB\n+        SharedWithPad sharedA = SharedWithPad.createLocal();\n+        SharedWithPadB sharedB = SharedWithPadB.createLocal();\n+\n+        \/\/ Padding scales to access local memory.\n+        \/\/ We add +1 to mitigate memory bank conflicts.\n+        final int padA = TILE_K + 1;\n+        final int padB = BLOCK_M + 1;\n+        for (int tileIndex = 0; tileIndex < K; tileIndex += TILE_K) {\n+            \/\/ Load A tile (Block_M x Tile_K)\n+            for (int i = 0; i < REG_M; i++) {\n+                final int aRow = cRowBase + i;\n+                for (int kk = lid_n; kk < TILE_K; kk += WG_N) {\n+                    final int aCol = tileIndex + kk;\n+                    float valA = 0.0f;\n+                    if (aRow < M && aCol < K) {\n+                        valA = matrixA.array(aRow * lda + aCol);\n+                    }\n+                    sharedA.array(((aRow - blockRow) * padA) + kk, valA);\n+                }\n+            }\n+\n+            \/\/ Load B tile (Tile_K x Block_N)\n+            for (int j = 0; j < REG_N; j++) {\n+                final int bCol = cColBase + j;\n+                for (int kk = lid_m; kk < TILE_K; kk += WG_M) {\n+                    final int bRow = tileIndex + kk;\n+                    float valB = 0.0f;\n+                    if (bRow < K && bCol < N) {\n+                        valB = matrixB.array(bRow * ldb + bCol);\n+                    }\n+                    sharedB.array((kk * padB) + (bCol - blockCol), valB);\n+                }\n+            }\n+            kc.barrier();\n+\n+            \/\/ Compute the tile acc += sharedA[:,k] * sharedB[k,:]\n+            for (int kk = 0; kk < TILE_K && (tileIndex + kk) < K; kk++) {\n+                \/\/ Load the A and B operands into registers to reuse for REG_M x REG_N accumulations\n+                PrivateReg aReg = PrivateReg.createPrivate();\n+                PrivateReg bReg = PrivateReg.createPrivate();\n+\n+                \/\/ Fetch a column kk of sharedA to private memory\n+                for (int i = 0; i < REG_M; i++) {\n+                    final int aRowL = (cRowBase + i) - blockRow;\n+                    float valPrivA = 0.0f;\n+                    if ((cRowBase + i) < M) {\n+                        valPrivA = sharedA.array(aRowL * padA + kk);\n+                    }\n+                    aReg.array(i, valPrivA);\n+                }\n+\n+                \/\/ Fetch a row kk of sharedB to private memory\n+                for (int j = 0; j < REG_N; j++) {\n+                    final int bColL = (cColBase + j) - blockCol;\n+                    float valPrivB = 0.0f;\n+                    if ((cColBase + j) < N) {\n+                        valPrivB = sharedB.array(kk * padB + bColL);\n+                    }\n+                    bReg.array(j, valPrivB);\n+                }\n+\n+                \/\/ FMA over the register tile\n+                for (int i = 0; i < REG_M; i++) {\n+                    final float a_ik = aReg.array(i);\n+                    for (int j = 0; j < REG_N; j++) {\n+                        float valRes = acc.array(i * REG_M + j);\n+                        valRes += a_ik * bReg.array(j);\n+                        acc.array(i * REG_M + j, valRes);\n+                    }\n+                }\n+            }\n+            kc.barrier();\n+        }\n+        \/\/ Write back the result of the register tile into matrixC\n+        for (int i = 0; i < REG_M; i++) {\n+            final int row = cRowBase + i;\n+            if (row < M) {\n+                for (int j = 0; j < REG_N; j++) {\n+                    final int col = cColBase + j;\n+                    if (col < N) {\n+                        matrixC.array(row * ldc + col, acc.array(i * REG_M + j));\n+                    }\n+                }\n+            }\n+        }\n+    }\n+\n+    @CodeReflection\n+    public static void matrixMultiply2DRegisterTilingPortable(@MappableIface.RO ComputeContext cc, @MappableIface.RO F32Array matrixA, @MappableIface.RO F32Array matrixB, @MappableIface.RW F32Array matrixC, int globalSize) {\n+        ComputeRange cudaRange = new ComputeRange(new GlobalMesh2D(256, 256), new LocalMesh2D(16, 16));\n+        cc.dispatchKernel(cudaRange,\n+                kc -> matrixMultiplyKernel2DRegisterTilingPortable(kc, matrixA, matrixB, matrixC, globalSize)\n+        );\n+    }\n+\n+    @HatTest\n+    public void testMatrixMultiply2DRegisterTiling() {\n+        var lookup = java.lang.invoke.MethodHandles.lookup();\n+        var accelerator = new Accelerator(lookup, Backend.FIRST);\n+\n+        final int size = SIZE;\n+        var matrixA = F32Array.create(accelerator, size * size);\n+        var matrixB = F32Array.create(accelerator, size * size);\n+\n+        \/\/ Matrix for the results\n+        var matrixC = F32Array.create(accelerator, size * size);\n+        var resultSeq = F32Array.create(accelerator, size * size);\n+\n+        \/\/ Initialize matrices (A and B have the same size)\n+        Random r = new Random(19);\n+\n+        for (int j = 0; j < matrixA.length(); j++) {\n+            matrixA.array(j, r.nextFloat());\n+            matrixB.array(j, r.nextFloat());\n+        }\n+\n+        accelerator.compute(cc ->\n+                TestMatMul.matrixMultiply2DRegisterTilingPortable(cc, matrixA, matrixB, matrixC, size));\n+\n+        \/\/ Run Seq for reference\n+        runSequential(matrixA, matrixB, resultSeq, size);\n+\n+        for (int j = 0; j < size; j++) {\n+            for (int i = 0; i < size; i++) {\n+                HatAsserts.assertEquals(resultSeq.array(i * size + j), matrixC.array(i * size + j), 0.01f);\n+            }\n+        }\n+    }\n","filename":"hat\/tests\/src\/main\/java\/oracle\/code\/hat\/TestMatMul.java","additions":226,"deletions":0,"binary":false,"changes":226,"status":"modified"}]}