{"files":[{"patch":"@@ -1,1 +1,1 @@\n-MavenStyleProject using code reflection with a Java-based ONNX programming model.\n+### MavenStyleProject using code reflection with a Java-based ONNX programming model.\n@@ -6,1 +6,88 @@\n-```\n\\ No newline at end of file\n+```\n+\n+### Onnx Generation API to create and run LLM Onnx models.\n+\n+Example of direct execution of existing Onnx LLM model:\n+```\n+\/\/ model-specific prompt format\n+static final String PROMPT_TEMPLATE = \"<|...|>%s<|...|><|...|>\";\n+\n+public static void main(String... args) {\n+\n+    \/\/ compatible `libonnxruntime` library must be present in the same folder as `libonnxruntime-genai` library\n+    \/\/ native library extension (.dylib, .so or .dll) is platform specific\n+    System.load(\"path\/To\/libonnxruntime-genai.dylib\");\n+\n+    \/\/ model folder must contain the Onnx model file and all configuration and external data files\n+    try (OnnxGenRuntimeSession session = new OnnxGenRuntimeSession(Path.of(\"path\/To\/Onnx\/Model\/Folder\/\")) {\n+        \/\/ each LLM model has specific prompt format\n+        session.prompt(PROMPT_TEMPLATE.formatted(\"Tell me a joke\"), System.out::print);\n+    }\n+}\n+```\n+\n+Example of a custom LLM Onnx model generation from Java sources and execution:\n+```\n+\/\/ model-specific prompt format\n+static final String PROMPT_TEMPLATE = \"<|...|>%s<|...|><|...|>\";\n+\n+public static void main(String... args) {\n+\n+    \/\/ compatible `libonnxruntime` library must be present in the same folder as `libonnxruntime-genai` library\n+    \/\/ native library extension (.dylib or .so or .dll) is platform specific\n+    System.load(\"path\/To\/libonnxruntime-genai.dylib\");\n+\n+    \/\/ instance of a custom Onnx LLM model\n+    MyCustomLLMModel myCustomModelInstance = ...;\n+\n+    \/\/ target model folder must contain all configuration files\n+    \/\/ `genai_config.json` must be configured following way:\n+    \/\/     - model filename to match generated model file name (below)\n+    \/\/     - model inputs to match main model method argument names\n+    \/\/     - model outputs to match main model result record component names\n+    Path targetModelFolder = ...;\n+\n+    \/\/ Onnx model file and external data file are generated to the target model folder\n+    \/\/ and the session is created from the generated model\n+    try (OnnxGenRuntimeSession session = OnnxGenRuntimeSession.buildFromCodeReflection(myCustomModelInstance, \"myMainModelMethod\", targetModelFolder, \"MyModelFileName.onnx\", \"MyDataFileName\")) {\n+        \/\/ each LLM model has specific prompt format\n+        session.prompt(PROMPT_TEMPLATE.formatted(\"Tell me a joke\"), System.out::print);\n+    }\n+}\n+```\n+\n+Example of a custom LLM Onnx model Java source:\n+```\n+import oracle.code.onnx.Tensor;\n+import jdk.incubator.code.CodeReflection;\n+import static oracle.code.onnx.OnnxOperators.*;\n+\n+public final class MyCustomLLMModel {\n+\n+     public final Tensor<Float> myModelWeights...\n+     public final Tensor<Byte> otherMyModelWeights...\n+\n+     public MyCustomLLMModel(...) {\n+         \/\/ initilize all weight tensors\n+         \/\/ large tensors data can be memory-mapped\n+         this.myModelWeights = ...\n+         this.otherMyModelWeights = ...\n+         ...\n+     }\n+\n+     \/\/ custom record with main model method response\n+     public record MyModelResponse(Tensor<Float> logits, Tensor<Float> presentKey0, Tensor<Float> presentValue0, ...) {\n+     }\n+\n+     @CodeReflection\n+     public MyModelResponse myMainModelMethod(Tensor<Long> inputIds, Tensor<Long> attentionMask, Tensor<Float> pastKey0, Tensor<Float> pastValue0, ...) {\n+\n+         \/\/ computation of the model using oracle.code.onnx.OnnxOperators.* method calls\n+         ...\n+         Tensor<Float> logits = MatMul(...\n+\n+         \/\/ composition of the return record\n+         return new MyModelResponse(logits, key0, value0, ...);\n+     }\n+}\n+```\n","filename":"cr-examples\/onnx\/README.md","additions":89,"deletions":2,"binary":false,"changes":91,"status":"modified"},{"patch":"@@ -46,0 +46,88 @@\n+\/**\n+ * Class wrapping Onnx Generation API to create and run LLM Onnx model in a session with configured tokenizer and generator.\n+ * <p>\n+ * Example of direct execution of existing Onnx LLM model:\n+ * {@snippet lang=\"java\" :\n+ *  \/\/ model-specific prompt format\n+ *  static final String PROMPT_TEMPLATE = \"<|...|>%s<|...|><|...|>\";\n+ *\n+ *  public static void main(String... args) {\n+ *\n+ *      \/\/ compatible `libonnxruntime` library must be present in the same folder as `libonnxruntime-genai` library\n+ *      \/\/ native library extension (.dylib or .so or .dll) is platform specific\n+ *      System.load(\"path\/To\/libonnxruntime-genai.dylib\");\n+ *\n+ *      \/\/ model folder must contain the Onnx model file and all configuration and external data files\n+ *      try (OnnxGenRuntimeSession session = new OnnxGenRuntimeSession(Path.of(\"path\/To\/Onnx\/Model\/Folder\/\")) {\n+ *          \/\/ each LLM model has specific prompt format\n+ *          session.prompt(PROMPT_TEMPLATE.formatted(\"Tell me a joke\"), System.out::print);\n+ *      }\n+ *   }\n+ * }\n+ * <p>\n+ * Example of a custom LLM Onnx model generation from Java sources and execution:\n+ * {@snippet lang=\"java\" :\n+ *  \/\/ model-specific prompt format\n+ *  static final String PROMPT_TEMPLATE = \"<|...|>%s<|...|><|...|>\";\n+ *\n+ *  public static void main(String... args) {\n+ *\n+ *      \/\/ compatible `libonnxruntime` library must be present in the same folder as `libonnxruntime-genai` library\n+ *      \/\/ native library extension (.dylib or .so or .dll) is platform specific\n+ *      System.load(\"path\/To\/libonnxruntime-genai.dylib\");\n+ *\n+ *      \/\/ instance of a custom Onnx LLM model\n+ *      MyCustomLLMModel myCustomModelInstance = ...;\n+ *\n+ *      \/\/ target model folder must contain all configuration files\n+ *      \/\/ `genai_config.json` must be configured following way:\n+ *      \/\/     - model filename to match generated model file name (below)\n+ *      \/\/     - model inputs to match main model method argument names\n+ *      \/\/     - model outputs to match main model result record component names\n+ *      Path targetModelFolder = ...;\n+ *\n+ *      \/\/ Onnx model file and external data file are generated to the target model folder\n+ *      \/\/ and the session is created from the generated model\n+ *      try (OnnxGenRuntimeSession session = OnnxGenRuntimeSession.buildFromCodeReflection(myCustomModelInstance, \"myMainModelMethod\", targetModelFolder, \"MyModelFileName.onnx\", \"MyDataFileName\")) {\n+ *          \/\/ each LLM model has specific prompt format\n+ *          session.prompt(PROMPT_TEMPLATE.formatted(\"Tell me a joke\"), System.out::print);\n+ *      }\n+ *   }\n+ * }\n+ * <p>\n+ * Example of a custom LLM Onnx model Java source:\n+ * {@snippet lang=\"java\" :\n+ *   import oracle.code.onnx.Tensor;\n+ *   import jdk.incubator.code.CodeReflection;\n+ *   import static oracle.code.onnx.OnnxOperators.*;\n+ *\n+ *   public final class MyCustomLLMModel {\n+ *\n+ *       public final Tensor<Float> myModelWeights...\n+ *       public final Tensor<Byte> otherMyModelWeights...\n+ *\n+ *       public MyCustomLLMModel(...) {\n+ *           \/\/ initilize all weight tensors\n+ *           \/\/ large tensors data can be memory-mapped\n+ *           this.myModelWeights = ...\n+ *           this.otherMyModelWeights = ...\n+ *           ...\n+ *       }\n+ *\n+ *       \/\/ custom record with main model method response\n+ *       public record MyModelResponse(Tensor<Float> logits, Tensor<Float> presentKey0, Tensor<Float> presentValue0, ...) {\n+ *       }\n+ *\n+ *       @CodeReflection\n+ *       public MyModelResponse myMainModelMethod(Tensor<Long> inputIds, Tensor<Long> attentionMask, Tensor<Float> pastKey0, Tensor<Float> pastValue0 ...) {\n+ *\n+ *           \/\/ computation of the model using oracle.code.onnx.OnnxOperators.* method calls\n+ *           ...\n+ *           Tensor<Float> logits = MatMul(...\n+ *\n+ *           \/\/ composition of the return record\n+ *           return new MyModelResponse(logits, key0, value0, ...);\n+ *       }\n+ *   }\n+ * }\n+ *\/\n@@ -48,1 +136,11 @@\n-    public static OnnxGenRuntimeSession buildFromCodeReflection(Object codeReflectionModelInstance, String methodName, String promptTemplate, Path targetOnnxModelDir, String targetOnnxModelFileName, String targetExternalDataFileName) throws IOException {\n+    \/**\n+     * Builds Onnx model from the provided Java model instance and loads it into a constructs the Onnx Generate API session.\n+     * @param codeReflectionModelInstance Instance of a class representing Onnx LLM model.\n+     * @param methodName Main model method name.\n+     * @param targetOnnxModelDir Target folder for generation of Onnx model and external tensor data file.\n+     * @param targetOnnxModelFileName Target Onnx model file name.\n+     * @param targetExternalDataFileName Target external tensor data file name.\n+     * @return a live session instance\n+     * @throws IOException In case of any IO problems during model generation.\n+     *\/\n+    public static OnnxGenRuntimeSession buildFromCodeReflection(Object codeReflectionModelInstance, String methodName, Path targetOnnxModelDir, String targetOnnxModelFileName, String targetExternalDataFileName) throws IOException {\n@@ -66,1 +164,1 @@\n-        return new OnnxGenRuntimeSession(targetOnnxModelDir.toString(), promptTemplate);\n+        return new OnnxGenRuntimeSession(targetOnnxModelDir);\n@@ -71,1 +169,0 @@\n-    private final String promptTemplate;\n@@ -73,1 +170,5 @@\n-    public OnnxGenRuntimeSession(String onnxModelDir, String promptTemplate) {\n+    \/**\n+     * Constructs Onnx Generate API session (including model, tokenizer and generator) from assets stored in the Onnx model directory.\n+     * @param onnxModelDir Path to the Onnx model directory with Onnx model file, data file(s) and configuration files.\n+     *\/\n+    public OnnxGenRuntimeSession(Path onnxModelDir) {\n@@ -76,1 +177,1 @@\n-        model = call(OgaCreateModel(arena.allocateFrom(onnxModelDir), ret));\n+        model = call(OgaCreateModel(arena.allocateFrom(onnxModelDir.toString()), ret));\n@@ -82,1 +183,0 @@\n-        this.promptTemplate = promptTemplate;\n@@ -102,1 +202,6 @@\n-    public void prompt(String userPrompt, Consumer<String> outputConsumer) {\n+    \/**\n+     * Runs generator with the provided prompt and feeds decoded response to the provided consumer.\n+     * @param prompt Text prompt to tokenize and append to the LLM model input.\n+     * @param outputConsumer Consumer receiving decoded model response from the model generator.\n+     *\/\n+    public void prompt(String prompt, Consumer<String> outputConsumer) {\n@@ -105,1 +210,1 @@\n-            call(OgaTokenizerEncode(tokenizer, arena.allocateFrom(promptTemplate.formatted(userPrompt)), inputTokens));\n+            call(OgaTokenizerEncode(tokenizer, arena.allocateFrom(prompt), inputTokens));\n@@ -119,0 +224,3 @@\n+    \/**\n+     * Closes the session and all its related assets (arena, generator, tokenizer and model).\n+     *\/\n@@ -120,1 +228,1 @@\n-    public void close() throws Exception {\n+    public void close() {\n","filename":"cr-examples\/onnx\/src\/main\/java\/oracle\/code\/onnx\/genai\/OnnxGenRuntimeSession.java","additions":117,"deletions":9,"binary":false,"changes":126,"status":"modified"},{"patch":"@@ -560,1 +560,1 @@\n-        \/\/\/ The sequence of non-default values are encoded as a tensor of shape [NNZ].\n+        \/\/\/ The sequence of non-default values are encoded as a tensor of shape \\[NNZ].\n@@ -569,1 +569,1 @@\n-        \/\/\/ (b) Indices can be a tensor of shape [NNZ], in which case the i-th value\n+        \/\/\/ (b) Indices can be a tensor of shape \\[NNZ], in which case the i-th value\n@@ -633,1 +633,1 @@\n-            \/\/\/ This field MUST refer to an integral type ([U]INT{8|16|32|64}) or STRING\n+            \/\/\/ This field MUST refer to an integral type (\\[U]INT{8|16|32|64}) or STRING\n","filename":"cr-examples\/onnx\/src\/main\/java\/oracle\/code\/onnx\/proto\/OnnxBuilder.java","additions":3,"deletions":3,"binary":false,"changes":6,"status":"modified"}]}