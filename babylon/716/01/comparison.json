{"files":[{"patch":"@@ -47,0 +47,17 @@\n+\n+    private CudaHATKernelBuilder half2float() {\n+        return identifier(\"__half2float\");\n+    }\n+\n+    private CudaHATKernelBuilder __nv_bfloat16() {\n+        return identifier(\"__nv_bfloat16\");\n+    }\n+\n+    private CudaHATKernelBuilder __bfloat162float() {\n+        return identifier(\"__bfloat162float\");\n+    }\n+\n+    private CudaHATKernelBuilder reinterpret_cast() {\n+        return keyword(\"reinterpret_cast\");\n+    }\n+\n@@ -73,2 +90,4 @@\n-                .includeSys(\"cuda_fp16.h\")\n-                .buildStructSingleMember(\"F16\", \"value\", \"half\");\n+                .includeSys(\"cuda_fp16.h\", \"cuda_bf16.h\")\n+                .hashDefine(\"BFLOAT16\", _->keyword(\"__nv_bfloat16\"))\n+                .buildStructSingleMember(\"F16\", \"value\", \"half\")\n+                .buildStructSingleMember(\"BF16\", \"value\", \"BFLOAT16\");\n@@ -125,0 +144,3 @@\n+        final String postFixOp1 = \"_1\";\n+        final String postFixOp2 = \"_2\";\n+\n@@ -127,1 +149,1 @@\n-                            .identifier(hatVectorBinaryOp.varName() + \"_1\")\n+                            .identifier(hatVectorBinaryOp.varName() + postFixOp1)\n@@ -129,1 +151,1 @@\n-            hatVectorBinaryOp1.varName(hatVectorBinaryOp.varName() + \"_1\");\n+            hatVectorBinaryOp1.varName(hatVectorBinaryOp.varName() + postFixOp1);\n@@ -135,1 +157,1 @@\n-                    .identifier(hatVectorBinaryOp.varName() + \"_2\")\n+                    .identifier(hatVectorBinaryOp.varName() + postFixOp2)\n@@ -137,1 +159,1 @@\n-            hatVectorBinaryOp2.varName(hatVectorBinaryOp.varName() + \"_2\");\n+            hatVectorBinaryOp2.varName(hatVectorBinaryOp.varName() + postFixOp2);\n@@ -176,1 +198,1 @@\n-        keyword(\"reinterpret_cast\")\n+        reinterpret_cast()\n@@ -229,2 +251,7 @@\n-        oparen().halfType().cparen().obrace();\n-        identifier(\"__float2half\").oparen();\n+        oparen();\n+        ReducedFloatType reducedFloatType = hatF16ConvOp.reducedFloatType();\n+        generateReduceFloatType(reducedFloatType);\n+        cparen().obrace();\n+\n+        buildReducedFloatType(reducedFloatType);\n+        oparen();\n@@ -241,1 +268,2 @@\n-        identifier(\"__half2float\").oparen();\n+        buildReducedFloatType(hatF16ToFloatConvOp.reducedFloatType());\n+        oparen();\n@@ -276,1 +304,1 @@\n-        identifier(\"make_\" + hatVectorOfOp.buildType()).oparen();\n+        composeIdentifier(\"make_\", hatVectorOfOp.buildType()).oparen();\n@@ -284,0 +312,1 @@\n+        ReducedFloatType reducedFloatType = hatF16BinaryOp.reducedFloatType();\n@@ -287,1 +316,3 @@\n-        oparen().halfType().cparen().obrace().oparen();\n+        oparen();\n+        generateReduceFloatType(reducedFloatType);\n+        cparen().obrace().oparen();\n@@ -290,1 +321,1 @@\n-            identifier(\"__half2float\").oparen();\n+            generateReducedFloatConversionToFloat(reducedFloatType);\n@@ -306,1 +337,1 @@\n-        space().identifier(hatF16BinaryOp.operationType().symbol()).space();\n+        space().identifier(hatF16BinaryOp.binaryOperationType().symbol()).space();\n@@ -309,1 +340,1 @@\n-            identifier(\"__half2float\").oparen();\n+            generateReducedFloatConversionToFloat(reducedFloatType);\n@@ -329,0 +360,24 @@\n+\n+    private void buildReducedFloatType(ReducedFloatType reducedFloatType) {\n+        switch (reducedFloatType) {\n+            case ReducedFloatType.HalfFloat _ -> half2float();\n+            case ReducedFloatType.BFloat16 _ -> __nv_bfloat16();\n+            default -> throw new IllegalStateException(\"Unexpected value: \" + reducedFloatType);\n+        }\n+    }\n+\n+    private void generateReduceFloatType(ReducedFloatType reducedFloatType) {\n+        switch (reducedFloatType) {\n+            case ReducedFloatType.HalfFloat _ -> halfType();\n+            case ReducedFloatType.BFloat16 _ -> bfloatType();\n+            default -> throw new IllegalStateException(\"Unexpected value: \" + reducedFloatType);\n+        }\n+    }\n+\n+    private void generateReducedFloatConversionToFloat(ReducedFloatType reducedFloatType) {\n+        switch (reducedFloatType) {\n+            case ReducedFloatType.HalfFloat _ ->  half2float().oparen();\n+            case ReducedFloatType.BFloat16 _ ->  __bfloat162float().oparen();\n+            default -> throw new IllegalStateException(\"Unexpected value: \" + reducedFloatType);\n+        }\n+    }\n","filename":"hat\/backends\/ffi\/cuda\/src\/main\/java\/hat\/backend\/ffi\/CudaHATKernelBuilder.java","additions":70,"deletions":15,"binary":false,"changes":85,"status":"modified"},{"patch":"@@ -39,0 +39,1 @@\n+import hat.dialect.ReducedFloatType;\n@@ -42,0 +43,2 @@\n+import java.util.Objects;\n+\n@@ -44,0 +47,8 @@\n+    public OpenCLHATKernelBuilder vstore(int dims) {\n+        return identifier(\"vstore\" + dims);\n+    }\n+\n+    public OpenCLHATKernelBuilder vload(int dims) {\n+        return identifier(\"vload\" + dims);\n+    }\n+\n@@ -74,1 +85,26 @@\n-                .buildStructSingleMember(\"F16\", \"value\", \"half\");\n+                .hashDefine(\"BFLOAT16\", _ -> keyword(\"ushort\"))\n+                .buildStructSingleMember(\"F16\", \"value\", \"half\")\n+                .buildStructSingleMember(\"BF16\", \"value\", \"BFLOAT16\")\n+                .identifier(\"\"\"\n+                        void byteCopy(void *dest, const void* src, size_t size) {\n+                            unsigned char *c = (unsigned char*)dest;\n+                            unsigned char *s = (unsigned char*)src;\n+                            for (int i = 0; i < size; i++) {\n+                                *c++ = *s++;\n+                            }\n+                        }\n+\n+                        float bfloat162float(ushort bf16) {\n+                            uint bitsRecovered = bf16 << 16;\n+                            float r = bitsRecovered;\n+                            byteCopy(&r, &bitsRecovered, sizeof(r));\n+                            return r;\n+                        }\n+\n+                        ushort float2bfloat16(float f) {\n+                            uint bits;\n+                            byteCopy(&bits, &f, sizeof(bits));\n+                            short bf16 = bits >> 16;\n+                            return bf16;\n+                        }\n+                        \"\"\");\n@@ -87,1 +123,1 @@\n-        identifier(\"vstore\" + hatVectorStoreView.vectorN())\n+        vstore(hatVectorStoreView.vectorN())\n@@ -139,1 +175,1 @@\n-        identifier(\"vload\" + hatVectorLoadOp.vectorN())\n+        vload(hatVectorLoadOp.vectorN())\n@@ -194,1 +230,13 @@\n-        oparen().halfType().cparen().obrace();\n+        ReducedFloatType reducedFloatType = hatF16ConvOp.reducedFloatType();\n+\n+        oparen();\n+        if (reducedFloatType instanceof ReducedFloatType.HalfFloat) {\n+            halfType();\n+        } else if (reducedFloatType instanceof ReducedFloatType.BFloat16) {\n+            bfloatType();\n+        }\n+\n+        cparen().obrace();\n+        if (reducedFloatType instanceof ReducedFloatType.BFloat16) {\n+            builtin_float2bfloat16().oparen();\n+        }\n@@ -199,0 +247,3 @@\n+        if (reducedFloatType instanceof ReducedFloatType.BFloat16) {\n+            cparen();\n+        }\n@@ -225,1 +276,14 @@\n-        oparen().floatType().cparen();\n+\n+        \/\/ Type conversions:\n+        \/\/ half -> float\n+        \/\/ bfloat16 -> float\n+\n+        ReducedFloatType reducedFloatType = hatF16ToFloatConvOp.reducedFloatType();\n+\n+        if (reducedFloatType instanceof ReducedFloatType.HalfFloat) {\n+            \/\/ half -> float\n+            oparen().floatType().cparen();\n+        } else if (reducedFloatType instanceof ReducedFloatType.BFloat16) {\n+            \/\/ bfloat16 -> float\n+            builtin_bfloat162float().oparen();\n+        }\n@@ -235,0 +299,3 @@\n+        if (reducedFloatType instanceof ReducedFloatType.BFloat16) {\n+            cparen();\n+        }\n","filename":"hat\/backends\/ffi\/opencl\/src\/main\/java\/hat\/backend\/ffi\/OpenCLHATKernelBuilder.java","additions":72,"deletions":5,"binary":false,"changes":77,"status":"modified"},{"patch":"@@ -34,2 +34,1 @@\n-import hat.buffer.F16;\n-import hat.buffer.KernelBufferContext;\n+import hat.buffer.*;\n@@ -37,3 +36,0 @@\n-import hat.buffer.ArgArray;\n-import hat.buffer.Buffer;\n-import hat.buffer.BufferTracker;\n@@ -286,0 +282,1 @@\n+            typedefs.add(BF16.class.getName());\n","filename":"hat\/backends\/ffi\/shared\/src\/main\/java\/hat\/backend\/ffi\/C99FFIBackend.java","additions":2,"deletions":5,"binary":false,"changes":7,"status":"modified"},{"patch":"@@ -0,0 +1,124 @@\n+\/*\n+ * Copyright (c) 2025, Oracle and\/or its affiliates. All rights reserved.\n+ * DO NOT ALTER OR REMOVE COPYRIGHT NOTICES OR THIS FILE HEADER.\n+ *\n+ * This code is free software; you can redistribute it and\/or modify it\n+ * under the terms of the GNU General Public License version 2 only, as\n+ * published by the Free Software Foundation.  Oracle designates this\n+ * particular file as subject to the \"Classpath\" exception as provided\n+ * by Oracle in the LICENSE file that accompanied this code.\n+ *\n+ * This code is distributed in the hope that it will be useful, but WITHOUT\n+ * ANY WARRANTY; without even the implied warranty of MERCHANTABILITY or\n+ * FITNESS FOR A PARTICULAR PURPOSE.  See the GNU General Public License\n+ * version 2 for more details (a copy is included in the LICENSE file that\n+ * accompanied this code).\n+ *\n+ * You should have received a copy of the GNU General Public License version\n+ * 2 along with this work; if not, write to the Free Software Foundation,\n+ * Inc., 51 Franklin St, Fifth Floor, Boston, MA 02110-1301 USA.\n+ *\n+ * Please contact Oracle, 500 Oracle Parkway, Redwood Shores, CA 94065 USA\n+ * or visit www.oracle.com if you need additional information or have any\n+ * questions.\n+ *\/\n+package hat.buffer;\n+\n+public interface BF16 {\n+\n+    char value();\n+    void value(char value);\n+\n+    static BF16 of(float value) {\n+        return new BF16() {\n+            @Override\n+            public char value() {\n+                int bits = Float.floatToRawIntBits(value);\n+                bits >>= 16;\n+                return (char) bits;\n+            }\n+\n+            @Override\n+            public void value(char value) {\n+            }\n+        };\n+    }\n+\n+    static BF16 of(char value) {\n+        return new BF16() {\n+            @Override\n+            public char value() {\n+                return value;\n+            }\n+\n+            @Override\n+            public void value(char value) {\n+            }\n+        };\n+    }\n+\n+    static BF16 float2bfloat16(float value) {\n+        return of(value);\n+    }\n+\n+    static float bfloat162float(BF16 value) {\n+        return Float.intBitsToFloat(value.value() << 16);\n+    }\n+\n+    static BF16 add(BF16 ha, BF16 hb) {\n+        return BF16.of(bfloat162float(ha) + bfloat162float(hb));\n+    }\n+\n+    static BF16 add(float f32, BF16 hb) {\n+        return BF16.of(f32 + bfloat162float(hb));\n+    }\n+\n+    static BF16 sub(BF16 ha, BF16 hb) {\n+        return BF16.of(bfloat162float(ha) - bfloat162float(hb));\n+    }\n+\n+    static BF16 sub(float f32, BF16 hb) {\n+        return BF16.of(f32 - bfloat162float(hb));\n+    }\n+\n+    static BF16 sub(BF16 hb, float f32) {\n+        return BF16.of(bfloat162float(hb) - f32);\n+    }\n+\n+    static BF16 mul(BF16 ha, BF16 hb) {\n+        return BF16.of(bfloat162float(ha) * bfloat162float(hb));\n+    }\n+\n+    static BF16 mul(float f32, BF16 hb) {\n+        return BF16.of(f32 * bfloat162float(hb));\n+    }\n+\n+    static BF16 div(BF16 ha, BF16 hb) {\n+        return BF16.of(bfloat162float(ha) \/ bfloat162float(hb));\n+    }\n+\n+    static BF16 div(float f32, BF16 hb) {\n+        return BF16.of(f32 \/ bfloat162float(hb));\n+    }\n+\n+    static BF16 add(BF16 hb, float f32) {\n+        return BF16.of(bfloat162float(hb) \/ f32);\n+    }\n+\n+    default BF16 add(BF16 ha) {\n+        return BF16.add(this, ha);\n+    }\n+\n+    default BF16 sub(BF16 ha) {\n+        return BF16.sub(this, ha);\n+    }\n+\n+    default BF16 mul(BF16 ha) {\n+        return BF16.mul(this, ha);\n+    }\n+\n+    default BF16 div(BF16 ha) {\n+        return BF16.div(this, ha);\n+    }\n+\n+}\n","filename":"hat\/core\/src\/main\/java\/hat\/buffer\/BF16.java","additions":124,"deletions":0,"binary":false,"changes":124,"status":"added"},{"patch":"@@ -0,0 +1,49 @@\n+\/*\n+ * Copyright (c) 2025, Oracle and\/or its affiliates. All rights reserved.\n+ * DO NOT ALTER OR REMOVE COPYRIGHT NOTICES OR THIS FILE HEADER.\n+ *\n+ * This code is free software; you can redistribute it and\/or modify it\n+ * under the terms of the GNU General Public License version 2 only, as\n+ * published by the Free Software Foundation.  Oracle designates this\n+ * particular file as subject to the \"Classpath\" exception as provided\n+ * by Oracle in the LICENSE file that accompanied this code.\n+ *\n+ * This code is distributed in the hope that it will be useful, but WITHOUT\n+ * ANY WARRANTY; without even the implied warranty of MERCHANTABILITY or\n+ * FITNESS FOR A PARTICULAR PURPOSE.  See the GNU General Public License\n+ * version 2 for more details (a copy is included in the LICENSE file that\n+ * accompanied this code).\n+ *\n+ * You should have received a copy of the GNU General Public License version\n+ * 2 along with this work; if not, write to the Free Software Foundation,\n+ * Inc., 51 Franklin St, Fifth Floor, Boston, MA 02110-1301 USA.\n+ *\n+ * Please contact Oracle, 500 Oracle Parkway, Redwood Shores, CA 94065 USA\n+ * or visit www.oracle.com if you need additional information or have any\n+ * questions.\n+ *\/\n+package hat.buffer;\n+\n+import hat.Accelerator;\n+import hat.ifacemapper.Schema;\n+\n+public interface BF16Array extends Buffer {\n+    int length();\n+\n+    BF16Impl array(long index);\n+\n+    interface BF16Impl extends Struct, BF16 {\n+        String NAME = \"F16Impl\";\n+\n+        char value();\n+        void value(char value);\n+    }\n+\n+    Schema<BF16Array> schema = Schema.of(BF16Array.class, bf16array ->\n+            bf16array.arrayLen(\"length\")\n+                     .array(\"array\", bfloat16 -> bfloat16.fields(\"value\")));\n+\n+    static BF16Array create(Accelerator accelerator, int length){\n+        return schema.allocate(accelerator, length);\n+    }\n+}\n\\ No newline at end of file\n","filename":"hat\/core\/src\/main\/java\/hat\/buffer\/BF16Array.java","additions":49,"deletions":0,"binary":false,"changes":49,"status":"added"},{"patch":"@@ -34,0 +34,5 @@\n+import hat.dialect.HATGlobalSizeOp;\n+import hat.dialect.HATGlobalThreadIdOp;\n+import hat.dialect.HATLocalSizeOp;\n+import hat.dialect.HATLocalThreadIdOp;\n+import hat.dialect.HATLocalVarOp;\n@@ -36,0 +41,3 @@\n+import hat.dialect.HATPrivateVarOp;\n+import hat.dialect.HATVectorBinaryOp;\n+import hat.dialect.HATVectorLoadOp;\n@@ -40,2 +48,0 @@\n-import hat.dialect.HATVectorBinaryOp;\n-import hat.dialect.HATVectorLoadOp;\n@@ -43,6 +49,0 @@\n-import hat.dialect.HATGlobalThreadIdOp;\n-import hat.dialect.HATGlobalSizeOp;\n-import hat.dialect.HATLocalSizeOp;\n-import hat.dialect.HATLocalThreadIdOp;\n-import hat.dialect.HATLocalVarOp;\n-import hat.dialect.HATPrivateVarOp;\n@@ -151,1 +151,1 @@\n-    T hatVectorMakeOf(ScopedCodeBuilderContext builderContext, HATVectorMakeOfOp hatVectorMakeOfOp);\n+    T hatVectorMakeOf(ScopedCodeBuilderContext buildContext, HATVectorMakeOfOp hatVectorMakeOfOp);\n@@ -153,1 +153,1 @@\n-    T hatF16ToFloatConvOp(ScopedCodeBuilderContext builderContext, HATF16ToFloatConvOp hatF16ToFloatConvOp);\n+    T hatF16ToFloatConvOp(ScopedCodeBuilderContext buildContext, HATF16ToFloatConvOp hatF16ToFloatConvOp);\n@@ -155,1 +155,1 @@\n-    T hatPrivateVarInitOp(ScopedCodeBuilderContext builderContext, HATPrivateInitVarOp hatPrivateInitVarOp);\n+    T hatPrivateVarInitOp(ScopedCodeBuilderContext buildContext, HATPrivateInitVarOp hatPrivateInitVarOp);\n@@ -157,1 +157,1 @@\n-    T hatMemoryLoadOp(ScopedCodeBuilderContext builderContext, HATMemoryLoadOp hatMemoryLoadOp);\n+    T hatMemoryLoadOp(ScopedCodeBuilderContext buildContext, HATMemoryLoadOp hatMemoryLoadOp);\n","filename":"hat\/core\/src\/main\/java\/hat\/codebuilders\/BabylonOpBuilder.java","additions":12,"deletions":12,"binary":false,"changes":24,"status":"modified"},{"patch":"@@ -27,0 +27,2 @@\n+import hat.buffer.BF16;\n+import hat.buffer.BF16Array;\n@@ -43,0 +45,1 @@\n+import hat.dialect.ReducedFloatType;\n@@ -60,0 +63,31 @@\n+\n+    public T kernelDeclaration(CoreOp.FuncOp funcOp) {\n+        return kernelPrefix().voidType().space().funcName(funcOp);\n+    }\n+\n+    public T functionDeclaration(ScopedCodeBuilderContext codeBuilderContext, JavaType javaType, CoreOp.FuncOp funcOp) {\n+        return functionPrefix().type(codeBuilderContext,javaType).space().funcName(funcOp);\n+    }\n+\n+    public T kernelPrefix() {\n+        return keyword(\"HAT_KERNEL\").space();\n+    }\n+\n+    public T functionPrefix() {\n+        return keyword(\"HAT_FUNC\").space();\n+    }\n+\n+    public T globalPtrPrefix() {\n+        return keyword(\"HAT_GLOBAL_MEM\").space();\n+    }\n+\n+    public T localPtrPrefix() {\n+        return keyword(\"HAT_LOCAL_MEM\").space();\n+    }\n+\n+    public T syncBlockThreads() {\n+        return identifier(\"HAT_BARRIER\");\n+    }\n+\n+    public abstract T defines();\n+\n@@ -96,0 +130,2 @@\n+            \/\/ Check for special types (e.g., FP16)\n+            \/\/ TODO: We need to update this with a custom op, so we avoid direct use of Impls\n@@ -97,0 +133,4 @@\n+        } else if (javaType instanceof ClassType classType && classType.toClassName().equals(BF16.class.getCanonicalName())) {\n+            \/\/ Special type: BFLOAT16\n+            \/\/ TODO: We need to update this with a custom op, so we avoid direct use of Impls\n+            globalPtrPrefix().suffix_t(BF16Array.BF16Impl.class.getSimpleName()).asterisk();\n@@ -102,0 +142,1 @@\n+\n@@ -183,2 +224,0 @@\n-\n-\n@@ -244,3 +283,9 @@\n-        halfType()\n-                .space()\n-                .identifier(hatF16VarOp.varName())\n+\n+        ReducedFloatType reducedFloatType = hatF16VarOp.reducedFloatType();\n+        switch (reducedFloatType) {\n+            case ReducedFloatType.HalfFloat _ -> halfType();\n+            case ReducedFloatType.BFloat16 _ ->  bfloatType();\n+            default -> throw new IllegalStateException(\"Unexpected value: \" + reducedFloatType);\n+        }\n+\n+        space().identifier(hatF16VarOp.varName())\n@@ -255,0 +300,60 @@\n+    private boolean isMixedFirstOperand(byte f32Mixed) {\n+        return f32Mixed != 0 && f32Mixed != HATF16BinaryOp.FIRST_OP;\n+    }\n+\n+    private boolean isMixedSecondOperand(byte f32Mixed) {\n+        return f32Mixed != 0 && f32Mixed != HATF16BinaryOp.LAST_OP;\n+    }\n+\n+    private T binaryOperationsForBfloat16(ScopedCodeBuilderContext buildContext, HATF16BinaryOp hatf16BinaryOp) {\n+        Value op1 = hatf16BinaryOp.operands().get(0);\n+        Value op2 = hatf16BinaryOp.operands().get(1);\n+        List<Boolean> references = hatf16BinaryOp.references();\n+        byte f32Mixed = hatf16BinaryOp.getF32();\n+\n+        oparen().bfloatType()\n+                .cparen().obrace().oparen();\n+\n+        builtin_float2bfloat16()\n+                .oparen();\n+\n+        if (isMixedFirstOperand(f32Mixed) || f32Mixed == 0) {\n+            builtin_bfloat162float().oparen();\n+        }\n+\n+\n+        if (op1 instanceof Op.Result r) {\n+            recurse(buildContext, r.op());\n+        }\n+        if (references.getFirst()) {\n+            rarrow().identifier(\"value\");\n+        } else if (op1 instanceof Op.Result r && !(r.op().resultType() instanceof PrimitiveType)) {\n+            dot().identifier(\"value\");\n+        }\n+\n+        if (isMixedFirstOperand(f32Mixed) || f32Mixed == 0) {\n+            cparen();\n+        }\n+        space().identifier(hatf16BinaryOp.binaryOperationType().symbol()).space();\n+\n+        if (isMixedSecondOperand(f32Mixed) || f32Mixed == 0) {\n+            builtin_bfloat162float().oparen();\n+        }\n+\n+        if (op2 instanceof Op.Result r) {\n+            recurse(buildContext, r.op());\n+        }\n+\n+        if (references.get(1)) {\n+            rarrow().identifier(\"value\");\n+        } else if (op2 instanceof Op.Result r && !(r.op().resultType() instanceof PrimitiveType)) {\n+            dot().identifier(\"value\");\n+        }\n+\n+        if (isMixedSecondOperand(f32Mixed) || f32Mixed == 0) {\n+            cparen();\n+        }\n+        cparen().cparen().cbrace();\n+        return self();\n+    }\n+\n@@ -258,0 +363,5 @@\n+        ReducedFloatType reducedFloatType = hatF16BinaryOp.reducedFloatType();\n+        if (reducedFloatType instanceof ReducedFloatType.BFloat16) {\n+            return binaryOperationsForBfloat16(buildContext, hatF16BinaryOp);\n+        }\n+\n@@ -262,1 +372,3 @@\n-        oparen().halfType().cparen().obrace().oparen();\n+        oparen().halfType();\n+\n+        cparen().obrace().oparen();\n@@ -271,1 +383,1 @@\n-        space().identifier(hatF16BinaryOp.operationType().symbol()).space();\n+        space().identifier(hatF16BinaryOp.binaryOperationType().symbol()).space();\n@@ -360,31 +472,0 @@\n-\n-    public T kernelDeclaration(CoreOp.FuncOp funcOp) {\n-        return kernelPrefix().voidType().space().funcName(funcOp);\n-    }\n-\n-    public T functionDeclaration(ScopedCodeBuilderContext codeBuilderContext, JavaType javaType, CoreOp.FuncOp funcOp) {\n-        return functionPrefix().type(codeBuilderContext,javaType).space().funcName(funcOp);\n-    }\n-\n-    public T kernelPrefix() {\n-        return keyword(\"HAT_KERNEL\").space();\n-    }\n-\n-    public T functionPrefix() {\n-        return keyword(\"HAT_FUNC\").space();\n-    }\n-\n-    public T globalPtrPrefix() {\n-        return keyword(\"HAT_GLOBAL_MEM\").space();\n-    }\n-\n-    public T localPtrPrefix() {\n-        return keyword(\"HAT_LOCAL_MEM\").space();\n-    }\n-\n-    public T syncBlockThreads() {\n-        return identifier(\"HAT_BARRIER\");\n-    }\n-\n-    public abstract T defines();\n-\n","filename":"hat\/core\/src\/main\/java\/hat\/codebuilders\/C99HATKernelBuilder.java","additions":119,"deletions":38,"binary":false,"changes":157,"status":"modified"},{"patch":"@@ -27,0 +27,1 @@\n+import hat.buffer.BF16;\n@@ -515,0 +516,4 @@\n+    public final T bfloatType() {\n+        return typeName(BF16.class.getSimpleName() + \"_t\");\n+    }\n+\n@@ -519,0 +524,1 @@\n+\n@@ -562,0 +568,3 @@\n+    public T builtin(String text) {\n+        return emitText(text);\n+    }\n@@ -563,0 +572,3 @@\n+    public T composeIdentifier(String preffix, String postfix) {\n+        return identifier(preffix + postfix);\n+    }\n","filename":"hat\/core\/src\/main\/java\/hat\/codebuilders\/CodeBuilder.java","additions":12,"deletions":0,"binary":false,"changes":12,"status":"modified"},{"patch":"@@ -28,7 +28,1 @@\n-import hat.dialect.HATF16VarOp;\n-import hat.dialect.HATMemoryOp;\n-import hat.dialect.HATVectorBinaryOp;\n-import hat.dialect.HATVectorLoadOp;\n-import hat.dialect.HATVectorStoreView;\n-import hat.dialect.HATVectorVarLoadOp;\n-import hat.dialect.HATVectorVarOp;\n+import hat.dialect.*;\n@@ -209,0 +203,9 @@\n+    public T builtin_float2bfloat16() {\n+        identifier(\"float2bfloat16\");\n+        return self();\n+    }\n+\n+    public T builtin_bfloat162float() {\n+        identifier(\"bfloat162float\");\n+        return self();\n+    }\n@@ -244,1 +247,1 @@\n-        return nl();\n+        return self();\n","filename":"hat\/core\/src\/main\/java\/hat\/codebuilders\/HATCodeBuilder.java","additions":11,"deletions":8,"binary":false,"changes":19,"status":"modified"},{"patch":"@@ -27,0 +27,2 @@\n+import hat.buffer.BF16;\n+import hat.buffer.BF16Array;\n@@ -29,10 +31,1 @@\n-import hat.dialect.HATBarrierOp;\n-import hat.dialect.HATF16VarOp;\n-import hat.dialect.HATLocalVarOp;\n-import hat.dialect.HATMemoryOp;\n-import hat.dialect.HATPhaseUtils;\n-import hat.dialect.HATPrivateInitVarOp;\n-import hat.dialect.HATPrivateVarOp;\n-import hat.dialect.HATVectorBinaryOp;\n-import hat.dialect.HATVectorLoadOp;\n-import hat.dialect.HATVectorVarOp;\n+import hat.dialect.*;\n@@ -358,0 +351,5 @@\n+    private boolean isbfloat16(Schema.IfaceType ifaceType) {\n+        return (ifaceType.iface.getName().equals(BF16.class.getName())\n+                || ifaceType.iface.getName().equals(BF16Array.BF16Impl.class.getName()));\n+    }\n+\n@@ -368,0 +366,2 @@\n+                            } else if (isbfloat16(ifaceType)) {\n+                                typeName(\"BFLOAT16\");\n@@ -452,0 +452,1 @@\n+                || invokeOp.invokeDescriptor().refType().toString().equals(BF16.class.getCanonicalName())\n","filename":"hat\/core\/src\/main\/java\/hat\/codebuilders\/HATCodeBuilderWithContext.java","additions":11,"deletions":10,"binary":false,"changes":21,"status":"modified"},{"patch":"@@ -37,2 +37,2 @@\n-    public HATF16AddOp(TypeElement typeElement, List<Boolean> references, byte f32, List<Value> operands) {\n-        super(typeElement, HATF16BinaryOp.OpType.ADD, references, f32, operands);\n+    public HATF16AddOp(TypeElement typeElement, ReducedFloatType reducedFloatType, List<Boolean> references, byte f32, List<Value> operands) {\n+        super(typeElement, reducedFloatType, BinaryOpType.ADD, references, f32, operands);\n","filename":"hat\/core\/src\/main\/java\/hat\/dialect\/HATF16AddOp.java","additions":2,"deletions":2,"binary":false,"changes":4,"status":"modified"},{"patch":"@@ -37,1 +37,1 @@\n-    protected final OpType operationType;\n+    protected final BinaryOpType operationType;\n@@ -39,0 +39,1 @@\n+    private final ReducedFloatType reducedFloatType;\n@@ -44,1 +45,1 @@\n-    public enum OpType {\n+    public enum BinaryOpType {\n@@ -52,1 +53,1 @@\n-        OpType(String symbol) {\n+        BinaryOpType(String symbol) {\n@@ -61,1 +62,1 @@\n-    public HATF16BinaryOp(TypeElement typeElement, OpType operationType, List<Boolean> references, byte f32, List<Value> operands) {\n+    public HATF16BinaryOp(TypeElement typeElement, ReducedFloatType reducedFloatType, BinaryOpType operationType, List<Boolean> references, byte f32, List<Value> operands) {\n@@ -67,0 +68,1 @@\n+        this.reducedFloatType = reducedFloatType;\n@@ -75,0 +77,1 @@\n+        this.reducedFloatType = op.reducedFloatType;\n@@ -87,1 +90,1 @@\n-    public OpType operationType() {\n+    public BinaryOpType binaryOperationType() {\n@@ -99,0 +102,3 @@\n+    public ReducedFloatType reducedFloatType() {\n+        return reducedFloatType;\n+    }\n","filename":"hat\/core\/src\/main\/java\/hat\/dialect\/HATF16BinaryOp.java","additions":11,"deletions":5,"binary":false,"changes":16,"status":"modified"},{"patch":"@@ -40,0 +40,1 @@\n+    private final ReducedFloatType reducedFloatType;\n@@ -41,1 +42,1 @@\n-    public HATF16ConvOp(TypeElement typeElement, List<Value> operands) {\n+    public HATF16ConvOp(TypeElement typeElement, ReducedFloatType reducedFloatType, List<Value> operands) {\n@@ -44,0 +45,1 @@\n+        this.reducedFloatType = reducedFloatType;\n@@ -49,0 +51,1 @@\n+        this.reducedFloatType = op.reducedFloatType;\n@@ -66,0 +69,4 @@\n+    public ReducedFloatType reducedFloatType() {\n+        return reducedFloatType;\n+    }\n+\n","filename":"hat\/core\/src\/main\/java\/hat\/dialect\/HATF16ConvOp.java","additions":8,"deletions":1,"binary":false,"changes":9,"status":"modified"},{"patch":"@@ -37,2 +37,2 @@\n-    public HATF16DivOp(TypeElement typeElement, List<Boolean> references, byte f32, List<Value> operands) {\n-        super(typeElement, OpType.DIV, references, f32, operands);\n+    public HATF16DivOp(TypeElement typeElement, ReducedFloatType reducedFloatType, List<Boolean> references, byte f32, List<Value> operands) {\n+        super(typeElement, reducedFloatType, BinaryOpType.DIV, references, f32, operands);\n","filename":"hat\/core\/src\/main\/java\/hat\/dialect\/HATF16DivOp.java","additions":2,"deletions":2,"binary":false,"changes":4,"status":"modified"},{"patch":"@@ -37,2 +37,2 @@\n-    public HATF16MulOp(TypeElement typeElement, List<Boolean> references, byte f32, List<Value> operands) {\n-        super(typeElement, OpType.MUL, references, f32, operands);\n+    public HATF16MulOp(TypeElement typeElement, ReducedFloatType reducedFloatType, List<Boolean> references, byte f32, List<Value> operands) {\n+        super(typeElement, reducedFloatType, BinaryOpType.MUL, references, f32, operands);\n","filename":"hat\/core\/src\/main\/java\/hat\/dialect\/HATF16MulOp.java","additions":2,"deletions":2,"binary":false,"changes":4,"status":"modified"},{"patch":"@@ -53,1 +53,0 @@\n-\n","filename":"hat\/core\/src\/main\/java\/hat\/dialect\/HATF16Op.java","additions":0,"deletions":1,"binary":false,"changes":1,"status":"modified"},{"patch":"@@ -37,2 +37,2 @@\n-    public HATF16SubOp(TypeElement typeElement, List<Boolean> references, byte f32, List<Value> operands) {\n-        super(typeElement, OpType.SUB, references, f32, operands);\n+    public HATF16SubOp(TypeElement typeElement, ReducedFloatType reducedFloatType, List<Boolean> references, byte f32, List<Value> operands) {\n+        super(typeElement, reducedFloatType, BinaryOpType.SUB, references, f32, operands);\n","filename":"hat\/core\/src\/main\/java\/hat\/dialect\/HATF16SubOp.java","additions":2,"deletions":2,"binary":false,"changes":4,"status":"modified"},{"patch":"@@ -41,0 +41,1 @@\n+    private final ReducedFloatType reducedFloatType;\n@@ -42,1 +43,1 @@\n-    public HATF16ToFloatConvOp(TypeElement typeElement, boolean isLocal, boolean wasFloat, List<Value> operands) {\n+    public HATF16ToFloatConvOp(TypeElement typeElement, ReducedFloatType reducedFloatType, boolean isLocal, boolean wasFloat, List<Value> operands) {\n@@ -47,0 +48,1 @@\n+        this.reducedFloatType = reducedFloatType;\n@@ -54,0 +56,1 @@\n+        this.reducedFloatType = op.reducedFloatType;\n@@ -79,0 +82,4 @@\n+    public ReducedFloatType reducedFloatType() {\n+        return reducedFloatType;\n+    }\n+\n","filename":"hat\/core\/src\/main\/java\/hat\/dialect\/HATF16ToFloatConvOp.java","additions":8,"deletions":1,"binary":false,"changes":9,"status":"modified"},{"patch":"@@ -40,0 +40,1 @@\n+    private final ReducedFloatType reducedFloatType;\n@@ -41,1 +42,1 @@\n-    public HATF16VarOp(String varName, VarType typeElement, List<Value> operands) {\n+    public HATF16VarOp(String varName, ReducedFloatType reducedFloatType, VarType typeElement, List<Value> operands) {\n@@ -44,0 +45,1 @@\n+        this.reducedFloatType = reducedFloatType;\n@@ -49,0 +51,1 @@\n+        this.reducedFloatType = op.reducedFloatType;\n@@ -66,0 +69,4 @@\n+    public ReducedFloatType reducedFloatType() {\n+        return reducedFloatType;\n+    }\n+\n","filename":"hat\/core\/src\/main\/java\/hat\/dialect\/HATF16VarOp.java","additions":8,"deletions":1,"binary":false,"changes":9,"status":"modified"},{"patch":"@@ -0,0 +1,40 @@\n+\/*\n+ * Copyright (c) 2025, Oracle and\/or its affiliates. All rights reserved.\n+ * DO NOT ALTER OR REMOVE COPYRIGHT NOTICES OR THIS FILE HEADER.\n+ *\n+ * This code is free software; you can redistribute it and\/or modify it\n+ * under the terms of the GNU General Public License version 2 only, as\n+ * published by the Free Software Foundation.  Oracle designates this\n+ * particular file as subject to the \"Classpath\" exception as provided\n+ * by Oracle in the LICENSE file that accompanied this code.\n+ *\n+ * This code is distributed in the hope that it will be useful, but WITHOUT\n+ * ANY WARRANTY; without even the implied warranty of MERCHANTABILITY or\n+ * FITNESS FOR A PARTICULAR PURPOSE.  See the GNU General Public License\n+ * version 2 for more details (a copy is included in the LICENSE file that\n+ * accompanied this code).\n+ *\n+ * You should have received a copy of the GNU General Public License version\n+ * 2 along with this work; if not, write to the Free Software Foundation,\n+ * Inc., 51 Franklin St, Fifth Floor, Boston, MA 02110-1301 USA.\n+ *\n+ * Please contact Oracle, 500 Oracle Parkway, Redwood Shores, CA 94065 USA\n+ * or visit www.oracle.com if you need additional information or have any\n+ * questions.\n+ *\/\n+package hat.dialect;\n+\n+public interface ReducedFloatType {\n+\n+    interface HalfFloat extends ReducedFloatType {\n+        static HalfFloat of() {\n+            return new HalfFloat() {};\n+        }\n+    }\n+\n+    interface BFloat16 extends ReducedFloatType {\n+        static BFloat16 of() {\n+            return new BFloat16() {};\n+        }\n+    }\n+}\n","filename":"hat\/core\/src\/main\/java\/hat\/dialect\/ReducedFloatType.java","additions":40,"deletions":0,"binary":false,"changes":40,"status":"added"},{"patch":"@@ -28,0 +28,1 @@\n+import hat.buffer.BF16;\n@@ -246,0 +247,5 @@\n+    private static boolean isHATReservedType(JavaOp.InvokeOp invokeOp) {\n+        String invokeRefType = invokeOp.invokeDescriptor().refType().toString();\n+        return invokeRefType.equals(F16.class.getCanonicalName()) || invokeRefType.equals(BF16.class.getCanonicalName());\n+    }\n+\n@@ -247,2 +253,1 @@\n-        return (isAssignable(lookup, javaRefType(invokeOp), MappableIface.class) ||\n-                invokeOp.invokeDescriptor().refType().toString().equals(F16.class.getCanonicalName()));\n+        return (isAssignable(lookup, javaRefType(invokeOp), MappableIface.class) || isHATReservedType(invokeOp));\n","filename":"hat\/core\/src\/main\/java\/hat\/optools\/OpTk.java","additions":7,"deletions":2,"binary":false,"changes":9,"status":"modified"},{"patch":"@@ -28,0 +28,1 @@\n+import hat.buffer.BF16;\n@@ -29,0 +30,1 @@\n+import hat.dialect.ReducedFloatType;\n@@ -49,0 +51,1 @@\n+import java.util.HashMap;\n@@ -50,0 +53,1 @@\n+import java.util.Map;\n@@ -58,1 +62,3 @@\n-    public enum OpMethod {\n+    private final Accelerator accelerator;\n+\n+    private enum BinaryOpMethod {\n@@ -65,1 +71,1 @@\n-        OpMethod(String name) {\n+        BinaryOpMethod(String name) {\n@@ -68,4 +74,0 @@\n-\n-        public String methodName() {\n-            return this.methodName;\n-        }\n@@ -74,1 +76,0 @@\n-    private final Accelerator accelerator;\n@@ -79,1 +80,1 @@\n-    private boolean isFP16Operation(JavaOp.InvokeOp invokeOp, String methodName) {\n+    private ReducedFloatType categorizeReducedFloat(JavaOp.InvokeOp invokeOp) {\n@@ -81,3 +82,16 @@\n-        boolean isFP16Operation = invokeClassName.replace(\"$\", \".\").startsWith(F16.class.getCanonicalName());\n-        return isFP16Operation\n-                \/\/&& OpTk.isIfaceBufferMethod(accelerator.lookup, invokeOp)\n+        invokeClassName = invokeClassName.replace(\"$\", \".\");\n+        if (invokeClassName.equals(F16.class.getName())) {\n+            return ReducedFloatType.HalfFloat.of();\n+        } else if (invokeClassName.equals(BF16.class.getName())) {\n+            return ReducedFloatType.BFloat16.of();\n+        }\n+        return null;\n+    }\n+\n+    private boolean is16BitFloatOperation(JavaOp.InvokeOp invokeOp, String methodName) {\n+        String invokeClassName = invokeOp.invokeDescriptor().refType().toString();\n+        invokeClassName = invokeClassName.replace(\"$\", \".\");\n+        boolean is16BitFloatOperation = invokeClassName.startsWith(F16.class.getCanonicalName()) || invokeClassName.startsWith(BF16.class.getCanonicalName());\n+        return is16BitFloatOperation\n+                \/\/ No need because F16 element is not a Buffer type at the moment\n+                \/\/ && OpTk.isIfaceBufferMethod(accelerator.lookup, invokeOp)\n@@ -120,1 +134,1 @@\n-    private void createF16VarOp(CoreOp.VarOp varOp, Block.Builder blockBuilder) {\n+    private void createF16VarOp(CoreOp.VarOp varOp, Block.Builder blockBuilder, ReducedFloatType reducedFloatType) {\n@@ -123,1 +137,1 @@\n-        HATF16VarOp hatf16VarOp = new HATF16VarOp(varOp.varName(), varOp.resultType(), outputOperands);\n+        HATF16VarOp hatf16VarOp = new HATF16VarOp(varOp.varName(), reducedFloatType, varOp.resultType(), outputOperands);\n@@ -129,1 +143,1 @@\n-    private void createF16ConvOP(JavaOp.InvokeOp invokeOp, Block.Builder blockBuilder) {\n+    private void createF16ConvOP(JavaOp.InvokeOp invokeOp, Block.Builder blockBuilder, ReducedFloatType reducedFloatType) {\n@@ -132,1 +146,1 @@\n-        HATF16ConvOp convOp1 = new HATF16ConvOp(JavaType.VOID, outputOperands);\n+        HATF16ConvOp convOp1 = new HATF16ConvOp(JavaType.VOID, reducedFloatType, outputOperands);\n@@ -138,1 +152,1 @@\n-    private void createFloatFromF16(JavaOp.InvokeOp invokeOp, Block.Builder blockBuilder) {\n+    private void createFloatFromF16(JavaOp.InvokeOp invokeOp, Block.Builder blockBuilder, ReducedFloatType reducedFloatType) {\n@@ -149,1 +163,1 @@\n-        HATF16ToFloatConvOp convOp1 = new HATF16ToFloatConvOp(JavaType.FLOAT, isLocal, wasFloat, outputOperands);\n+        HATF16ToFloatConvOp convOp1 = new HATF16ToFloatConvOp(JavaType.FLOAT, reducedFloatType, isLocal, wasFloat, outputOperands);\n@@ -165,1 +179,1 @@\n-    private void createF16BinaryOp(JavaOp.InvokeOp invokeOp, Block.Builder blockBuilder, OpMethod method) {\n+    private void createF16BinaryOp(JavaOp.InvokeOp invokeOp, Block.Builder blockBuilder, BinaryOpMethod method, ReducedFloatType reducedFloatType) {\n@@ -186,4 +200,4 @@\n-            case ADD -> new HATF16AddOp(typeElement, refList, valF32Conversion, outputOperands);\n-            case SUB -> new HATF16SubOp(typeElement, refList, valF32Conversion, outputOperands);\n-            case MUL -> new HATF16MulOp(typeElement, refList, valF32Conversion, outputOperands);\n-            case DIV -> new HATF16DivOp(typeElement, refList, valF32Conversion, outputOperands);\n+            case ADD -> new HATF16AddOp(typeElement, reducedFloatType, refList, valF32Conversion, outputOperands);\n+            case SUB -> new HATF16SubOp(typeElement, reducedFloatType, refList, valF32Conversion, outputOperands);\n+            case MUL -> new HATF16MulOp(typeElement, reducedFloatType, refList, valF32Conversion, outputOperands);\n+            case DIV -> new HATF16DivOp(typeElement, reducedFloatType, refList, valF32Conversion, outputOperands);\n@@ -197,1 +211,1 @@\n-    private CoreOp.FuncOp dialectifyF16Ops(CoreOp.FuncOp funcOp, OpMethod method) {\n+    private CoreOp.FuncOp dialectifyF16Ops(CoreOp.FuncOp funcOp, BinaryOpMethod method) {\n@@ -201,0 +215,2 @@\n+        Map<Op, ReducedFloatType> reducedFloatsType = new HashMap<>();\n+\n@@ -204,1 +220,1 @@\n-                        if (isFP16Operation(invokeOp, method.methodName) && invokeOp.resultType() != JavaType.VOID) {\n+                        if (is16BitFloatOperation(invokeOp, method.methodName) && invokeOp.resultType() != JavaType.VOID) {\n@@ -207,0 +223,2 @@\n+                            ReducedFloatType category = categorizeReducedFloat(invokeOp);\n+                            reducedFloatsType.put(invokeOp, category);\n@@ -210,0 +228,2 @@\n+                                    reducedFloatsType.put(varOp, category);\n+                                    \/\/ The variable is created only once for a usage in the same scope\n@@ -222,1 +242,1 @@\n-                createF16BinaryOp(invokeOp, blockBuilder, method);\n+                createF16BinaryOp(invokeOp, blockBuilder, method, reducedFloatsType.get(invokeOp));\n@@ -224,1 +244,1 @@\n-                createF16VarOp(varOp, blockBuilder);\n+                createF16VarOp(varOp, blockBuilder, reducedFloatsType.get(varOp));\n@@ -239,1 +259,2 @@\n-                        if (isFP16Operation(invokeOp, \"value\") && invokeOp.resultType() == JavaType.SHORT) {\n+                        if (is16BitFloatOperation(invokeOp, \"value\") &&\n+                                (invokeOp.resultType() == JavaType.SHORT || invokeOp.resultType() == JavaType.CHAR)) {\n@@ -274,2 +295,3 @@\n-        return (isFP16Operation(invokeOp, \"of\")\n-                || isFP16Operation(invokeOp, \"floatToF16\"));\n+        return (is16BitFloatOperation(invokeOp, \"of\")\n+                || is16BitFloatOperation(invokeOp, \"floatToF16\")\n+                || is16BitFloatOperation(invokeOp, \"float2bfloat16\"));\n@@ -282,0 +304,2 @@\n+        Map<Op, ReducedFloatType> reducedFloatsType = new HashMap<>();\n+\n@@ -291,0 +315,3 @@\n+                                    ReducedFloatType reducedFloatType = categorizeReducedFloat(invokeOp);\n+                                    reducedFloatsType.put(varOp, reducedFloatType);\n+                                    reducedFloatsType.put(invokeOp, reducedFloatType);\n@@ -302,1 +329,1 @@\n-                createF16ConvOP(invokeOp, blockBuilder);\n+                createF16ConvOP(invokeOp, blockBuilder, reducedFloatsType.get(invokeOp));\n@@ -304,1 +331,1 @@\n-                createF16VarOp(varOp, blockBuilder);\n+                createF16VarOp(varOp, blockBuilder, reducedFloatsType.get(varOp));\n@@ -315,0 +342,3 @@\n+\n+        Map<Op, ReducedFloatType> reducedFloatsType = new HashMap<>();\n+\n@@ -318,1 +348,1 @@\n-                        if (isMethod(invokeOp, \"f16ToFloat\")\n+                        if ((isMethod(invokeOp, \"f16ToFloat\") || isMethod(invokeOp, \"bfloat162float\"))\n@@ -321,0 +351,1 @@\n+                            reducedFloatsType.put(invokeOp, categorizeReducedFloat(invokeOp));\n@@ -330,1 +361,1 @@\n-                createFloatFromF16(invokeOp, blockBuilder);\n+                createFloatFromF16(invokeOp, blockBuilder, reducedFloatsType.get(invokeOp));\n@@ -355,1 +386,1 @@\n-        for (OpMethod method : OpMethod.values())\n+        for (BinaryOpMethod method : BinaryOpMethod.values())\n","filename":"hat\/core\/src\/main\/java\/hat\/phases\/HATDialectifyFP16Phase.java","additions":65,"deletions":34,"binary":false,"changes":99,"status":"modified"},{"patch":"@@ -28,0 +28,1 @@\n+import hat.buffer.BF16;\n@@ -76,0 +77,1 @@\n+                        isMappableType |= OpTk.isAssignable(MethodHandles.lookup(), javaType, BF16.class);\n","filename":"hat\/core\/src\/main\/java\/hat\/phases\/HATFinalDetectionPhase.java","additions":2,"deletions":0,"binary":false,"changes":2,"status":"modified"},{"patch":"@@ -183,1 +183,2 @@\n-                \"hat.test.TestFloat2\"\n+                \"hat.test.TestFloat2\",\n+                \"hat.test.TestBFloat16Type\",\n","filename":"hat\/hat\/test.java","additions":2,"deletions":1,"binary":false,"changes":3,"status":"modified"},{"patch":"@@ -0,0 +1,698 @@\n+\/*\n+ * Copyright (c) 2025, Oracle and\/or its affiliates. All rights reserved.\n+ * DO NOT ALTER OR REMOVE COPYRIGHT NOTICES OR THIS FILE HEADER.\n+ *\n+ * This code is free software; you can redistribute it and\/or modify it\n+ * under the terms of the GNU General Public License version 2 only, as\n+ * published by the Free Software Foundation.  Oracle designates this\n+ * particular file as subject to the \"Classpath\" exception as provided\n+ * by Oracle in the LICENSE file that accompanied this code.\n+ *\n+ * This code is distributed in the hope that it will be useful, but WITHOUT\n+ * ANY WARRANTY; without even the implied warranty of MERCHANTABILITY or\n+ * FITNESS FOR A PARTICULAR PURPOSE.  See the GNU General Public License\n+ * version 2 for more details (a copy is included in the LICENSE file that\n+ * accompanied this code).\n+ *\n+ * You should have received a copy of the GNU General Public License version\n+ * 2 along with this work; if not, write to the Free Software Foundation,\n+ * Inc., 51 Franklin St, Fifth Floor, Boston, MA 02110-1301 USA.\n+ *\n+ * Please contact Oracle, 500 Oracle Parkway, Redwood Shores, CA 94065 USA\n+ * or visit www.oracle.com if you need additional information or have any\n+ * questions.\n+ *\/\n+package hat.test;\n+\n+import hat.Accelerator;\n+import hat.ComputeContext;\n+import hat.KernelContext;\n+import hat.NDRange;\n+import hat.annotations.Kernel;\n+import hat.backend.Backend;\n+import hat.buffer.BF16;\n+import hat.buffer.BF16Array;\n+import hat.device.DeviceSchema;\n+import hat.device.DeviceType;\n+import hat.test.annotation.HatTest;\n+import hat.test.engine.HATAsserts;\n+import jdk.incubator.code.Reflect;\n+\n+import java.lang.invoke.MethodHandles;\n+import java.util.Random;\n+\n+import static hat.ifacemapper.MappableIface.*;\n+\n+public class TestBFloat16Type {\n+\n+    @Reflect\n+    public static void kernel_copy(@RO KernelContext kernelContext, @RO BF16Array a, @WO BF16Array b) {\n+        if (kernelContext.gix < kernelContext.gsx) {\n+            BF16 ha = a.array(kernelContext.gix);\n+            b.array(kernelContext.gix).value(ha.value());\n+        }\n+    }\n+\n+    @Reflect\n+    public static void bf16_02(@RO KernelContext kernelContext, @RO BF16Array a, @RO BF16Array b, @WO BF16Array c) {\n+        if (kernelContext.gix < kernelContext.gsx) {\n+            BF16 ha = a.array(kernelContext.gix);\n+            BF16 hb = b.array(kernelContext.gix);\n+            BF16 result = BF16.add(ha, hb);\n+            BF16 hc = c.array(kernelContext.gix);\n+            hc.value(result.value());\n+        }\n+    }\n+\n+    @Reflect\n+    public static void bf16_03(@RO KernelContext kernelContext, @RO BF16Array a, @RO BF16Array b, @RW BF16Array c) {\n+        if (kernelContext.gix < kernelContext.gsx) {\n+            BF16 ha = a.array(kernelContext.gix);\n+            BF16 hb = b.array(kernelContext.gix);\n+\n+            BF16 result = BF16.add(ha, BF16.add(hb, hb));\n+            BF16 hC = c.array(kernelContext.gix);\n+            hC.value(result.value());\n+        }\n+    }\n+\n+    @Reflect\n+    public static void bf16_04(@RO KernelContext kernelContext, @RO BF16Array a, @RO BF16Array b, @RW BF16Array c) {\n+        if (kernelContext.gix < kernelContext.gsx) {\n+            BF16 ha = a.array(kernelContext.gix);\n+            BF16 hb = b.array(kernelContext.gix);\n+\n+            BF16 r1 = BF16.mul(ha, hb);\n+            BF16 r2 = BF16.div(ha, hb);\n+            BF16 r3 = BF16.sub(ha, hb);\n+            BF16 r4 = BF16.add(r1, r2);\n+            BF16 r5 = BF16.add(r4, r3);\n+            BF16 hC = c.array(kernelContext.gix);\n+            hC.value(r5.value());\n+        }\n+    }\n+\n+    @Reflect\n+    public static void bf16_05(@RO KernelContext kernelContext, @RW BF16Array a) {\n+        if (kernelContext.gix < kernelContext.gsx) {\n+            BF16 ha = a.array(kernelContext.gix);\n+            BF16 initVal = BF16.of( 2.1f);\n+            ha.value(initVal.value());\n+        }\n+    }\n+\n+    @Reflect\n+    public static void bf16_06(@RO KernelContext kernelContext, @RW BF16Array a) {\n+        if (kernelContext.gix < kernelContext.gsx) {\n+            BF16 initVal = BF16.of(kernelContext.gix);\n+            BF16 ha = a.array(kernelContext.gix);\n+            ha.value(initVal.value());\n+        }\n+    }\n+\n+    @Reflect\n+    public static void bf16_08(@RO KernelContext kernelContext, @RW BF16Array a) {\n+        if (kernelContext.gix < kernelContext.gsx) {\n+            BF16 initVal = BF16.float2bfloat16(kernelContext.gix);\n+            BF16 ha = a.array(kernelContext.gix);\n+            ha.value(initVal.value());\n+        }\n+    }\n+\n+    @Reflect\n+    public static void bf16_09(@RO KernelContext kernelContext, @RO BF16Array a, @WO BF16Array b) {\n+        if (kernelContext.gix < kernelContext.gsx) {\n+            BF16 ha = a.array(kernelContext.gix);\n+            float f = BF16.bfloat162float(ha);\n+            BF16 result = BF16.float2bfloat16(f);\n+            BF16 hb = b.array(kernelContext.gix);\n+            hb.value(result.value());\n+        }\n+    }\n+\n+    @Reflect\n+    public static void bf16_10(@RO KernelContext kernelContext, @RO BF16Array a) {\n+        if (kernelContext.gix < kernelContext.gsx) {\n+            BF16 ha = a.array(kernelContext.gix);\n+            BF16 f16 = BF16.of(1.1f);\n+            float f = BF16.bfloat162float(f16);\n+            BF16 result = BF16.float2bfloat16(f);\n+            ha.value(result.value());\n+        }\n+    }\n+\n+    public interface LocalArray extends DeviceType {\n+        BF16 array(int index);\n+        DeviceSchema<LocalArray> schema = DeviceSchema.of(LocalArray.class,\n+                builder -> builder.withArray(\"array\", 1024)\n+                        .withDeps(BF16.class, bfloat16 -> bfloat16.withField(\"value\")));\n+\n+        static LocalArray  create(Accelerator accelerator) {\n+            return null;\n+        }\n+\n+        static LocalArray createLocal() {\n+            return null;\n+        }\n+    }\n+\n+    @Reflect\n+    public static void bf16_11(@RO KernelContext kernelContext, @RO BF16Array a, @RW BF16Array b) {\n+        LocalArray sm = LocalArray.createLocal();\n+        if (kernelContext.gix < kernelContext.gsx) {\n+            int lix = kernelContext.lix;\n+            BF16 ha = a.array(kernelContext.gix);\n+\n+            sm.array(lix).value(ha.value());\n+            kernelContext.barrier();\n+\n+            BF16 hb = sm.array(lix);\n+            b.array(kernelContext.gix).value(hb.value());\n+        }\n+    }\n+\n+    @Reflect\n+    public static void bf16_12(@RO KernelContext kernelContext, @RO BF16Array a, @RO BF16Array b, @RW BF16Array c) {\n+        \/\/ Test the fluent API style\n+        if (kernelContext.gix < kernelContext.gsx) {\n+            BF16 ha = a.array(kernelContext.gix);\n+            BF16 hb = b.array(kernelContext.gix);\n+            BF16 result = ha.add(hb);\n+            c.array(kernelContext.gix).value(result.value());\n+        }\n+    }\n+\n+    @Reflect\n+    public static void bf16_13(@RO KernelContext kernelContext, @RO BF16Array a, @RO BF16Array b,  @RW BF16Array c) {\n+        \/\/ Test the fluent API style\n+        if (kernelContext.gix < kernelContext.gsx) {\n+            BF16 ha = a.array(kernelContext.gix);\n+            BF16 hb = b.array(kernelContext.gix);\n+            BF16 result = ha.add(hb).sub(hb).mul(ha).div(ha);\n+            c.array(kernelContext.gix).value(result.value());\n+        }\n+    }\n+\n+    @Reflect\n+    public static void bf16_14(@RO KernelContext kernelContext, @RO BF16Array a, @RW BF16Array b) {\n+        \/\/ Testing mixed float types\n+        if (kernelContext.gix < kernelContext.gsx) {\n+            BF16 ha = a.array(kernelContext.gix);\n+            float myFloat = 32.1f;\n+            BF16 result = BF16.add(myFloat, ha);\n+            b.array(kernelContext.gix).value(result.value());\n+        }\n+    }\n+\n+    public interface PrivateArray extends DeviceType {\n+        BF16 array(int index);\n+        DeviceSchema<PrivateArray> schema = DeviceSchema.of(PrivateArray.class,\n+                builder -> builder.withArray(\"array\", 256)\n+                        .withDeps(BF16.class, bfloat16 -> bfloat16.withField(\"value\")));\n+\n+        static PrivateArray  create(Accelerator accelerator) {\n+            return null;\n+        }\n+\n+        static PrivateArray createPrivate() {\n+            return null;\n+        }\n+    }\n+\n+    @Reflect\n+    public static void bf16_15(@RO KernelContext kernelContext, @RO BF16Array a, @RW BF16Array b) {\n+        PrivateArray privateArray = PrivateArray.createPrivate();\n+        if (kernelContext.gix < kernelContext.gsx) {\n+            int lix = kernelContext.lix;\n+            BF16 ha = a.array(kernelContext.gix);\n+            privateArray.array(lix).value(ha.value());\n+            BF16 hb = privateArray.array(lix);\n+            b.array(kernelContext.gix).value(hb.value());\n+        }\n+    }\n+\n+    @Reflect\n+    public static void bf16_16(@RO KernelContext kernelContext, @RW BF16Array a) {\n+        BF16 ha = a.array(0);\n+        BF16 hre = BF16.add(ha, ha);\n+        hre = BF16.add(hre, hre);\n+        a.array(0).value(hre.value());\n+    }\n+\n+    @Reflect\n+    public static void bf16_17(@RO KernelContext kernelContext, @RW BF16Array a) {\n+\n+        BF16 ha = a.array(0);\n+        PrivateArray privateArray = PrivateArray.createPrivate();\n+        privateArray.array(0).value(ha.value());\n+\n+        \/\/ Obtain the value from private memory\n+        BF16 acc = privateArray.array(0);\n+\n+        \/\/ compute\n+        acc = BF16.add(acc, acc);\n+\n+        \/\/ store the result\n+        a.array(0).value(acc.value());\n+    }\n+\n+    @Reflect\n+    public static void compute01(@RO ComputeContext computeContext, @RO BF16Array a, @WO BF16Array b) {\n+        computeContext.dispatchKernel(NDRange.of1D(a.length()), kernelContext -> TestBFloat16Type.kernel_copy(kernelContext, a, b));\n+    }\n+\n+    @Reflect\n+    public static void compute02(@RO ComputeContext computeContext, @RO BF16Array a, @RO BF16Array b, @WO BF16Array c) {\n+        computeContext.dispatchKernel(NDRange.of1D(a.length()),\n+                kernelContext -> TestBFloat16Type.bf16_02(kernelContext, a, b, c));\n+    }\n+\n+    @Reflect\n+    public static void compute03(@RO ComputeContext computeContext, @RO BF16Array a, @RO BF16Array b, @WO BF16Array c) {\n+        computeContext.dispatchKernel(NDRange.of1D(a.length()),\n+                kernelContext -> TestBFloat16Type.bf16_03(kernelContext, a, b, c));\n+    }\n+\n+    @Reflect\n+    public static void compute04(@RO ComputeContext computeContext, @RO BF16Array a, @RO BF16Array b, @WO BF16Array c) {\n+        computeContext.dispatchKernel(NDRange.of1D(a.length()),\n+                kernelContext -> TestBFloat16Type.bf16_04(kernelContext, a, b, c));\n+    }\n+\n+    @Reflect\n+    public static void compute05(@RO ComputeContext computeContext, @RW BF16Array a) {\n+        computeContext.dispatchKernel(NDRange.of1D(a.length()), kernelContext -> TestBFloat16Type.bf16_05(kernelContext, a));\n+    }\n+\n+    @Reflect\n+    public static void compute06(@RO ComputeContext computeContext, @RW BF16Array a) {\n+        computeContext.dispatchKernel(NDRange.of1D(a.length()), kernelContext -> TestBFloat16Type.bf16_06(kernelContext, a));\n+    }\n+\n+    @Reflect\n+    public static void compute08(@RO ComputeContext computeContext, @RW BF16Array a) {\n+        computeContext.dispatchKernel(NDRange.of1D(a.length()), kernelContext -> TestBFloat16Type.bf16_08(kernelContext, a));\n+    }\n+\n+    @Reflect\n+    public static void compute09(@RO ComputeContext computeContext, @RW BF16Array a, @WO BF16Array b) {\n+        computeContext.dispatchKernel(NDRange.of1D(a.length()), kernelContext -> TestBFloat16Type.bf16_09(kernelContext, a, b));\n+    }\n+\n+    @Reflect\n+    public static void compute10(@RO ComputeContext computeContext, @RW BF16Array a) {\n+        computeContext.dispatchKernel(NDRange.of1D(a.length()), kernelContext -> TestBFloat16Type.bf16_10(kernelContext, a));\n+    }\n+\n+    @Reflect\n+    public static void compute11(@RO ComputeContext computeContext, @RO BF16Array a, @RW BF16Array b) {\n+        computeContext.dispatchKernel(NDRange.of1D(a.length(),16), kernelContext -> TestBFloat16Type.bf16_11(kernelContext, a, b));\n+    }\n+\n+    @Reflect\n+    public static void compute12(@RO ComputeContext computeContext, @RO BF16Array a, @RO BF16Array b, @RW BF16Array c) {\n+        computeContext.dispatchKernel(NDRange.of1D(a.length()), kernelContext -> TestBFloat16Type.bf16_12(kernelContext, a, b, c));\n+    }\n+\n+    @Reflect\n+    public static void compute13(@RO ComputeContext computeContext, @RO BF16Array a, @RO BF16Array b, @RW BF16Array c) {\n+        computeContext.dispatchKernel(NDRange.of1D(a.length()), kernelContext -> TestBFloat16Type.bf16_13(kernelContext, a, b, c));\n+    }\n+\n+    @Reflect\n+    public static void compute14(@RO ComputeContext computeContext, @RO BF16Array a, @RW BF16Array b) {\n+        computeContext.dispatchKernel(NDRange.of1D(a.length()), kernelContext -> TestBFloat16Type.bf16_14(kernelContext, a, b));\n+    }\n+\n+    @Reflect\n+    public static void compute15(@RO ComputeContext computeContext, @RO BF16Array a, @RW BF16Array b) {\n+        computeContext.dispatchKernel(NDRange.of1D(a.length()), kernelContext -> TestBFloat16Type.bf16_15(kernelContext, a, b));\n+    }\n+\n+    @Reflect\n+    public static void compute16(@RO ComputeContext computeContext, @RW BF16Array a) {\n+        computeContext.dispatchKernel(NDRange.of1D(1), kernelContext -> TestBFloat16Type.bf16_16(kernelContext, a));\n+    }\n+\n+    @Reflect\n+    public static void compute17(@RO ComputeContext computeContext, @RW BF16Array a) {\n+        computeContext.dispatchKernel(NDRange.of1D(1), kernelContext -> TestBFloat16Type.bf16_17(kernelContext, a));\n+    }\n+\n+    @HatTest\n+    public void test_bfloat16_01() {\n+        final int size = 256;\n+        var accelerator = new Accelerator(MethodHandles.lookup(), Backend.FIRST);\n+\n+        BF16Array arrayA = BF16Array.create(accelerator, size);\n+        BF16Array arrayB = BF16Array.create(accelerator, size);\n+        for (int i = 0; i < size; i++) {\n+            arrayA.array(i).value(BF16.float2bfloat16(i).value());\n+        }\n+\n+        accelerator.compute(computeContext -> TestBFloat16Type.compute01(computeContext, arrayA, arrayB));\n+\n+        for (int i = 0; i < size; i++) {\n+            BF16 result = arrayB.array(i);\n+            HATAsserts.assertEquals((float)i, BF16.bfloat162float(result), 0.001f);\n+        }\n+    }\n+\n+    @HatTest\n+    public void test_bfloat16_02() {\n+        final int size = 256;\n+        var accelerator = new Accelerator(MethodHandles.lookup(), Backend.FIRST);\n+\n+        BF16Array arrayA = BF16Array.create(accelerator, size);\n+        BF16Array arrayB = BF16Array.create(accelerator, size);\n+        BF16Array arrayC = BF16Array.create(accelerator, size);\n+\n+        Random r = new Random(19);\n+        for (int i = 0; i < size; i++) {\n+            arrayA.array(i).value(BF16.float2bfloat16(r.nextFloat()).value());\n+            arrayA.array(i).value(BF16.float2bfloat16(r.nextFloat()).value());\n+        }\n+\n+        accelerator.compute(computeContext -> TestBFloat16Type.compute02(computeContext, arrayA, arrayB, arrayC));\n+\n+        for (int i = 0; i < size; i++) {\n+            BF16 result = arrayC.array(i);\n+            BF16 a = arrayA.array(i);\n+            BF16 b = arrayB.array(i);\n+            float res = BF16.bfloat162float(a) + BF16.bfloat162float(b);\n+            HATAsserts.assertEquals(res, BF16.bfloat162float(result), 0.001f);\n+        }\n+    }\n+    @HatTest\n+    public void test_bfloat16_03() {\n+        var accelerator = new Accelerator(MethodHandles.lookup(), Backend.FIRST);\n+\n+        final int size = 256;\n+        BF16Array arrayA = BF16Array.create(accelerator, size);\n+        BF16Array arrayB = BF16Array.create(accelerator, size);\n+        BF16Array arrayC = BF16Array.create(accelerator, size);\n+\n+        Random random = new Random();\n+        for (int i = 0; i < arrayA.length(); i++) {\n+            arrayA.array(i).value(BF16.float2bfloat16(random.nextFloat()).value());\n+            arrayB.array(i).value(BF16.float2bfloat16(random.nextFloat()).value());\n+        }\n+\n+        accelerator.compute(computeContext -> {\n+            TestBFloat16Type.compute03(computeContext, arrayA, arrayB, arrayC);\n+        });\n+\n+        for (int i = 0; i < arrayC.length(); i++) {\n+            BF16 val = arrayC.array(i);\n+            float fa = BF16.bfloat162float(arrayA.array(i));\n+            float fb = BF16.bfloat162float(arrayB.array(i));\n+            HATAsserts.assertEquals((fa + fb + fb), BF16.bfloat162float(val), 0.01f);\n+        }\n+    }\n+\n+    @HatTest\n+    public void test_bfloat16_04() {\n+        var accelerator = new Accelerator(MethodHandles.lookup(), Backend.FIRST);\n+\n+        final int size = 256;\n+        BF16Array arrayA = BF16Array.create(accelerator, size);\n+        BF16Array arrayB = BF16Array.create(accelerator, size);\n+        BF16Array arrayC = BF16Array.create(accelerator, size);\n+\n+        Random random = new Random();\n+        for (int i = 0; i < arrayA.length(); i++) {\n+            arrayA.array(i).value(BF16.float2bfloat16(random.nextFloat()).value());\n+            arrayB.array(i).value(BF16.float2bfloat16(random.nextFloat()).value());\n+        }\n+\n+        accelerator.compute(computeContext -> {\n+            TestBFloat16Type.compute04(computeContext, arrayA, arrayB, arrayC);\n+        });\n+\n+        for (int i = 0; i < arrayC.length(); i++) {\n+            BF16 gotResult = arrayC.array(i);\n+\n+            \/\/ CPU Computation\n+            BF16 ha = arrayA.array(i);\n+            BF16 hb = arrayB.array(i);\n+            BF16 r1 = BF16.mul(ha, hb);\n+            BF16 r2 = BF16.div(ha, hb);\n+            BF16 r3 = BF16.sub(ha, hb);\n+            BF16 r4 = BF16.add(r1, r2);\n+            BF16 r5 = BF16.add(r4, r3);\n+\n+            HATAsserts.assertEquals(BF16.bfloat162float(r5), BF16.bfloat162float(gotResult), 0.01f);\n+        }\n+    }\n+\n+    @HatTest\n+    public void test_bfloat16_05() {\n+        var accelerator = new Accelerator(MethodHandles.lookup(), Backend.FIRST);\n+\n+        final int size = 16;\n+        BF16Array arrayA = BF16Array.create(accelerator, size);\n+        for (int i = 0; i < arrayA.length(); i++) {\n+            arrayA.array(i).value(BF16.float2bfloat16(0.0f).value());\n+        }\n+\n+        accelerator.compute(computeContext -> {\n+            TestBFloat16Type.compute05(computeContext, arrayA);\n+        });\n+\n+        for (int i = 0; i < arrayA.length(); i++) {\n+            BF16 val = arrayA.array(i);\n+            HATAsserts.assertEquals(2.1f, BF16.bfloat162float(val), 0.01f);\n+        }\n+    }\n+\n+    @HatTest\n+    public void test_bfloat16_06() {\n+        var accelerator = new Accelerator(MethodHandles.lookup(), Backend.FIRST);\n+\n+        final int size = 512;\n+        BF16Array arrayA = BF16Array.create(accelerator, size);\n+        for (int i = 0; i < arrayA.length(); i++) {\n+            arrayA.array(i).value(BF16.float2bfloat16(0.0f).value());\n+        }\n+\n+        accelerator.compute(computeContext -> {\n+            TestBFloat16Type.compute06(computeContext, arrayA);\n+        });\n+\n+        for (int i = 0; i < arrayA.length(); i++) {\n+            BF16 val = arrayA.array(i);\n+            HATAsserts.assertEquals(i, BF16.bfloat162float(val), 0.01f);\n+        }\n+    }\n+\n+    @HatTest\n+    public void test_bfloat16_07() {\n+        \/\/ Test CPU Implementation of BF16\n+        BF16 a = BF16.of(2.5f);\n+        BF16 b = BF16.of(3.5f);\n+        BF16 c = BF16.add(a, b);\n+        HATAsserts.assertEquals((2.5f + 3.5f), BF16.bfloat162float(c), 0.01f);\n+\n+        BF16 d = BF16.sub(a, b);\n+        HATAsserts.assertEquals((2.5f - 3.5f), BF16.bfloat162float(d), 0.01f);\n+\n+        BF16 e = BF16.mul(a, b);\n+        HATAsserts.assertEquals((2.5f * 3.5f), BF16.bfloat162float(e), 0.01f);\n+\n+        BF16 f = BF16.div(a, b);\n+        HATAsserts.assertEquals((2.5f \/ 3.5f), BF16.bfloat162float(f), 0.01f);\n+    }\n+\n+    @HatTest\n+    public void test_bfloat16_08() {\n+        var accelerator = new Accelerator(MethodHandles.lookup(), Backend.FIRST);\n+\n+        final int size = 256;\n+        BF16Array arrayA = BF16Array.create(accelerator, size);\n+        for (int i = 0; i < arrayA.length(); i++) {\n+            arrayA.array(i).value(BF16.float2bfloat16(0.0f).value());\n+        }\n+\n+        accelerator.compute(computeContext -> {\n+            TestBFloat16Type.compute08(computeContext, arrayA);\n+        });\n+\n+        for (int i = 0; i < arrayA.length(); i++) {\n+            BF16 val = arrayA.array(i);\n+            HATAsserts.assertEquals(i, BF16.bfloat162float(val), 0.01f);\n+        }\n+    }\n+\n+    @HatTest\n+    public void test_bfloat16_09() {\n+        var accelerator = new Accelerator(MethodHandles.lookup(), Backend.FIRST);\n+\n+        final int size = 16;\n+        BF16Array arrayA = BF16Array.create(accelerator, size);\n+        BF16Array arrayB = BF16Array.create(accelerator, size);\n+\n+        Random r = new Random(73);\n+        for (int i = 0; i < arrayA.length(); i++) {\n+            arrayA.array(i).value(BF16.float2bfloat16(r.nextFloat()).value());\n+        }\n+\n+        accelerator.compute(computeContext -> TestBFloat16Type.compute09(computeContext, arrayA, arrayB));\n+\n+        for (int i = 0; i < arrayB.length(); i++) {\n+            BF16 val = arrayB.array(i);\n+            HATAsserts.assertEquals(BF16.bfloat162float(arrayA.array(i)), BF16.bfloat162float(val), 0.01f);\n+        }\n+    }\n+\n+    @HatTest\n+    public void test_bfloat16_10() {\n+        var accelerator = new Accelerator(MethodHandles.lookup(), Backend.FIRST);\n+        final int size = 256;\n+        BF16Array arrayA = BF16Array.create(accelerator, size);\n+\n+        accelerator.compute(computeContext -> TestBFloat16Type.compute10(computeContext, arrayA));\n+\n+        for (int i = 0; i < arrayA.length(); i++) {\n+            BF16 val = arrayA.array(i);\n+            HATAsserts.assertEquals(1.1f, BF16.bfloat162float(val), 0.01f);\n+        }\n+    }\n+\n+    @HatTest\n+    public void test_bfloat16_11() {\n+        var accelerator = new Accelerator(MethodHandles.lookup(), Backend.FIRST);\n+        final int size = 256;\n+        BF16Array arrayA = BF16Array.create(accelerator, size);\n+        BF16Array arrayB = BF16Array.create(accelerator, size);\n+\n+        Random r = new Random(73);\n+        for (int i = 0; i < arrayA.length(); i++) {\n+            arrayA.array(i).value(BF16.float2bfloat16(r.nextFloat()).value());\n+        }\n+\n+        accelerator.compute(computeContext -> TestBFloat16Type.compute11(computeContext, arrayA, arrayB));\n+\n+        for (int i = 0; i < arrayB.length(); i++) {\n+            BF16 val = arrayB.array(i);\n+            HATAsserts.assertEquals(arrayA.array(i).value(), val.value());\n+        }\n+    }\n+\n+    @HatTest\n+    public void test_bfloat16_12() {\n+        var accelerator = new Accelerator(MethodHandles.lookup(), Backend.FIRST);\n+        final int size = 1024;\n+        BF16Array arrayA = BF16Array.create(accelerator, size);\n+        BF16Array arrayB = BF16Array.create(accelerator, size);\n+        BF16Array arrayC = BF16Array.create(accelerator, size);\n+\n+        Random r = new Random(73);\n+        for (int i = 0; i < arrayA.length(); i++) {\n+            arrayA.array(i).value(BF16.float2bfloat16(r.nextFloat()).value());\n+            arrayB.array(i).value(BF16.float2bfloat16(r.nextFloat()).value());\n+        }\n+\n+        accelerator.compute(computeContext -> TestBFloat16Type.compute12(computeContext, arrayA, arrayB, arrayC));\n+\n+        for (int i = 0; i < arrayB.length(); i++) {\n+            BF16 result = arrayC.array(i);\n+            HATAsserts.assertEquals(BF16.bfloat162float(BF16.add(arrayA.array(i), arrayB.array(i))), BF16.bfloat162float(result), 0.01f);\n+        }\n+    }\n+\n+    @HatTest\n+    public void test_bfloat16_13() {\n+        var accelerator = new Accelerator(MethodHandles.lookup(), Backend.FIRST);\n+        final int size = 1024;\n+        BF16Array arrayA = BF16Array.create(accelerator, size);\n+        BF16Array arrayB = BF16Array.create(accelerator, size);\n+        BF16Array arrayC = BF16Array.create(accelerator, size);\n+\n+        Random r = new Random(73);\n+        for (int i = 0; i < arrayA.length(); i++) {\n+            arrayA.array(i).value(BF16.float2bfloat16(r.nextFloat()).value());\n+            arrayB.array(i).value(BF16.float2bfloat16(r.nextFloat()).value());\n+        }\n+\n+        accelerator.compute(computeContext -> TestBFloat16Type.compute13(computeContext, arrayA, arrayB, arrayC));\n+\n+        for (int i = 0; i < arrayB.length(); i++) {\n+            BF16 result = arrayC.array(i);\n+            HATAsserts.assertEquals(BF16.bfloat162float(arrayA.array(i)), BF16.bfloat162float(result), 0.01f);\n+        }\n+    }\n+\n+    @HatTest\n+    public void test_bfloat16_14() {\n+        \/\/ Testing mixed types\n+        var accelerator = new Accelerator(MethodHandles.lookup(), Backend.FIRST);\n+        final int size = 1024;\n+        BF16Array arrayA = BF16Array.create(accelerator, size);\n+        BF16Array arrayB = BF16Array.create(accelerator, size);\n+\n+        Random r = new Random(73);\n+        for (int i = 0; i < arrayA.length(); i++) {\n+            arrayA.array(i).value(BF16.float2bfloat16(r.nextFloat()).value());\n+        }\n+\n+        accelerator.compute(computeContext -> TestBFloat16Type.compute14(computeContext, arrayA, arrayB));\n+\n+        for (int i = 0; i < arrayB.length(); i++) {\n+            BF16 result = arrayB.array(i);\n+            HATAsserts.assertEquals(BF16.bfloat162float(arrayA.array(i)) + 32.1f, BF16.bfloat162float(result), 0.1f);\n+        }\n+    }\n+\n+    @HatTest\n+    public void test_bfloat16_15() {\n+        var accelerator = new Accelerator(MethodHandles.lookup(), Backend.FIRST);\n+        final int size = 256;\n+        BF16Array arrayA = BF16Array.create(accelerator, size);\n+        BF16Array arrayB = BF16Array.create(accelerator, size);\n+\n+        Random r = new Random(73);\n+        for (int i = 0; i < arrayA.length(); i++) {\n+            arrayA.array(i).value(BF16.float2bfloat16(r.nextFloat()).value());\n+        }\n+\n+        accelerator.compute(computeContext -> TestBFloat16Type.compute15(computeContext, arrayA, arrayB));\n+\n+        for (int i = 0; i < arrayB.length(); i++) {\n+            BF16 val = arrayB.array(i);\n+            HATAsserts.assertEquals(arrayA.array(i).value(), val.value());\n+        }\n+    }\n+\n+    \/\/ Check accumulators\n+    @HatTest\n+    public void test_bfloat16_16() {\n+        var accelerator = new Accelerator(MethodHandles.lookup(), Backend.FIRST);\n+        final int size = 1;\n+        BF16Array arrayA = BF16Array.create(accelerator, size);\n+\n+        Random r = new Random(73);\n+        arrayA.array(0).value(BF16.float2bfloat16(10).value());\n+\n+        accelerator.compute(computeContext -> TestBFloat16Type.compute16(computeContext, arrayA));\n+\n+        BF16 val = arrayA.array(0);\n+        HATAsserts.assertEquals(40.0f, BF16.bfloat162float(val), 0.01f);\n+    }\n+\n+    \/\/ Check accumulators in private memory\n+    @HatTest\n+    public void test_bfloat16_17() {\n+        var accelerator = new Accelerator(MethodHandles.lookup(), Backend.FIRST);\n+        final int size = 1;\n+        BF16Array arrayA = BF16Array.create(accelerator, size);\n+\n+        Random r = new Random(73);\n+        arrayA.array(0).value(BF16.float2bfloat16(10).value());\n+\n+        accelerator.compute(computeContext -> TestBFloat16Type.compute17(computeContext, arrayA));\n+\n+        BF16 val = arrayA.array(0);\n+        HATAsserts.assertEquals(20.0f, BF16.bfloat162float(val), 0.01f);\n+    }\n+\n+}\n","filename":"hat\/tests\/src\/main\/java\/hat\/test\/TestBFloat16Type.java","additions":698,"deletions":0,"binary":false,"changes":698,"status":"added"},{"patch":"@@ -108,1 +108,1 @@\n-            F16 initVal = F16.of( kernelContext.gix);\n+            F16 initVal = F16.of(kernelContext.gix);\n","filename":"hat\/tests\/src\/main\/java\/hat\/test\/TestF16Type.java","additions":1,"deletions":1,"binary":false,"changes":2,"status":"modified"},{"patch":"@@ -32,0 +32,2 @@\n+import hat.buffer.BF16;\n+import hat.buffer.BF16Array;\n@@ -272,0 +274,14 @@\n+    private static void runSequential(BF16Array matrixA, BF16Array matrixB, BF16Array matrixC, final int size) {\n+        for (int i = 0; i < size; i++) {\n+            for (int j = 0; j < size; j++) {\n+                BF16 sum = BF16.of(0.0f);\n+                for (int k = 0; k < size; k++) {\n+                    BF16 a = matrixA.array((long) i * size + k);\n+                    BF16 b = matrixB.array((long) k * size + j);\n+                    sum = BF16.add(sum, BF16.mul(a, b));\n+                }\n+                matrixC.array((long) i * size + j).value(sum.value());\n+            }\n+        }\n+    }\n+\n@@ -944,0 +960,137 @@\n+    private interface SharedMemoryBfloat16 extends DeviceType {\n+        BF16 array(int index);\n+\n+        DeviceSchema<SharedMemoryBfloat16> schema = DeviceSchema.of(SharedMemoryBfloat16.class,\n+                arr -> arr.withArray(\"array\", 1024)\n+                        .withDeps(BF16.class, half -> half.withField(\"value\")));\n+\n+        static SharedMemoryBfloat16 create(Accelerator accelerator) {\n+            return null;\n+        }\n+\n+        static SharedMemoryBfloat16 createLocal() {\n+            return null;\n+        }\n+    }\n+\n+    private interface PrivateArrayBfloat16 extends DeviceType {\n+        BF16 array(int index);\n+\n+        DeviceSchema<PrivateArrayBfloat16> schema = DeviceSchema.of(PrivateArrayBfloat16.class,\n+                arr -> arr.withArray(\"array\", 16)\n+                        .withDeps(BF16.class, half -> half.withField(\"value\")));\n+\n+        static PrivateArrayBfloat16 create(Accelerator accelerator) {\n+            return null;\n+        }\n+\n+        static PrivateArrayBfloat16 createPrivate() {\n+            return null;\n+        }\n+    }\n+\n+    private interface FlatPrivateBfloat16 extends DeviceType {\n+        BF16 array(int index);\n+\n+        DeviceSchema<FlatPrivateBfloat16> schema = DeviceSchema.of(FlatPrivateBfloat16.class,\n+                arr -> arr.withArray(\"array\", 4)\n+                        .withDeps(BF16.class, half -> half.withField(\"value\")));\n+\n+        static FlatPrivateBfloat16 create(Accelerator accelerator) {\n+            return null;\n+        }\n+\n+        static FlatPrivateBfloat16 createPrivate() {\n+            return null;\n+        }\n+    }\n+\n+    @Reflect\n+    public static void matrixMultiplyKernel2DRegisterTilingBFloat16(@RO KernelContext kc, @RO BF16Array matrixA, @RO BF16Array matrixB, @RW BF16Array matrixC, int size) {\n+        final int BM = 64;\n+        final int BN = 64;\n+        final int BK = 16;\n+        final int TM = 4;\n+        final int TN = 4;\n+\n+        int bx = kc.bix;\n+        int by = kc.biy;\n+\n+        int totalResultsBlockTile = BM * BN;\n+        final int numThreadsBlockTile = totalResultsBlockTile \/ (TM * TN);\n+\n+        final int linearLocalId = kc.liy * kc.lsx + kc.lix;\n+        final int threadCol = kc.lix;\n+        final int threadRow = kc.liy;\n+\n+        SharedMemoryBfloat16 tileA = SharedMemoryBfloat16.createLocal();\n+        SharedMemoryBfloat16 tileB = SharedMemoryBfloat16.createLocal();\n+\n+        int aFrom = by * BM * size;\n+        int bFrom = bx * BN;\n+        int v = bx * BN;\n+        int cFrom = (by * BM * size) + (v);\n+\n+        final int innerRowA = linearLocalId \/ BK;\n+        final int innerColA = linearLocalId % BK;\n+\n+        final int strideA = numThreadsBlockTile \/ BK;\n+        final int innerRowB = linearLocalId \/ BN;\n+        final int innerColB = linearLocalId % BN;\n+\n+        int strideB = numThreadsBlockTile \/ BN;\n+\n+        PrivateArrayBfloat16 threadResults = PrivateArrayBfloat16.createPrivate();\n+        FlatPrivateBfloat16 regM = FlatPrivateBfloat16.createPrivate();\n+        FlatPrivateBfloat16 regN = FlatPrivateBfloat16.createPrivate();\n+\n+        for (int i = 0; i < (TN * TN); i++) {\n+            BF16 init = BF16.of(0.0f);\n+            threadResults.array(i).value(init.value());\n+        }\n+\n+        for (int bkIdx = 0; bkIdx < size; bkIdx += BK) {\n+            for (int loadOffset = 0; loadOffset < BM; loadOffset += strideA) {\n+                BF16 ha = matrixA.array(((innerRowA + loadOffset) * size + innerColA) + aFrom);\n+                tileA.array((innerRowA + loadOffset) * BK + innerColA).value(ha.value());\n+            }\n+            for (int loadOffset = 0; loadOffset < BK; loadOffset += strideB) {\n+                BF16 hb = matrixB.array(((innerRowB + loadOffset) * size + innerColB) + bFrom);\n+                tileB.array((innerRowB + loadOffset) * BN + innerColB).value(hb.value());\n+            }\n+            kc.barrier();\n+\n+            aFrom += (BK);\n+            int f = BK * size;\n+            bFrom += f;\n+\n+            for (int dotIdx = 0; dotIdx < BK; dotIdx++) {\n+                for (int i = 0; i < TM; i++) {\n+                    BF16 ha = tileA.array((threadRow * TM + i) * BK + dotIdx);\n+                    regM.array(i).value(ha.value());\n+                }\n+                for (int i = 0; i < TN; i++) {\n+                    BF16 hb = tileB.array(dotIdx * BN + threadCol * TN + i);\n+                    regN.array(i).value(hb.value());\n+                }\n+                for (int resIdxM = 0; resIdxM < TM; resIdxM++) {\n+                    for (int resIdxN = 0; resIdxN < TN; resIdxN++) {\n+                        BF16 privA = regM.array(resIdxM);\n+                        BF16 privB = regN.array(resIdxN);\n+                        BF16 mul = BF16.mul(privA, privB);\n+                        BF16 acc = threadResults.array(resIdxM * TN + resIdxN);\n+                        acc = BF16.add(acc, mul);\n+                        threadResults.array((resIdxM * TN + resIdxN)).value(acc.value());\n+                    }\n+                }\n+            }\n+            kc.barrier();\n+        }\n+        for (int resIdxM = 0; resIdxM < TM; resIdxM++) {\n+            for (int resIdxN = 0; resIdxN < TN; resIdxN++) {\n+                BF16 result = threadResults.array(resIdxM * TN + resIdxN);\n+                matrixC.array((((threadRow * TM + resIdxM) * size + threadCol * TN + resIdxN) + (cFrom))).value(result.value());\n+            }\n+        }\n+    }\n+\n@@ -951,0 +1104,7 @@\n+    @Reflect\n+    public static void matrixMultiply2DRegisterTilingBFloat16(@RO ComputeContext cc, @RO BF16Array matrixA, @RO BF16Array matrixB, @RW BF16Array matrixC, int globalSize) {\n+        cc.dispatchKernel(NDRange.of2D(256, 256,16, 16),\n+                kc -> matrixMultiplyKernel2DRegisterTilingBFloat16(kc, matrixA, matrixB, matrixC, globalSize)\n+        );\n+    }\n+\n@@ -985,0 +1145,35 @@\n+\n+    @HatTest\n+    public void matrixMultiply2DRegisterTilingBFloat16() {\n+        var lookup = java.lang.invoke.MethodHandles.lookup();\n+        var accelerator = new Accelerator(lookup, Backend.FIRST);\n+\n+        final int size = 1024;\n+        var matrixA = BF16Array.create(accelerator, size * size);\n+        var matrixB = BF16Array.create(accelerator, size * size);\n+\n+        \/\/ Matrix for the results\n+        var matrixC = BF16Array.create(accelerator, size * size);\n+        var resultSeq = BF16Array.create(accelerator, size * size);\n+\n+        \/\/ Initialize matrices (A and B have the same size)\n+        Random r = new Random(19);\n+        for (int j = 0; j < matrixA.length(); j++) {\n+            matrixA.array(j).value(BF16.float2bfloat16(r.nextFloat()).value());\n+            matrixB.array(j).value(BF16.float2bfloat16(r.nextFloat()).value());\n+        }\n+\n+        accelerator.compute(cc ->\n+                TestMatMul.matrixMultiply2DRegisterTilingBFloat16(cc, matrixA, matrixB, matrixC, size));\n+\n+        \/\/ Run Seq for reference\n+        runSequential(matrixA, matrixB, resultSeq, size);\n+\n+        for (int i = 0; i < size; i++) {\n+            for (int j = 0; j < size; j++) {\n+                HATAsserts.assertEquals(BF16.bfloat162float(resultSeq.array(i * size + j)),\n+                        BF16.bfloat162float(matrixC.array(i * size + j)),\n+                        0.01f);\n+            }\n+        }\n+    }\n","filename":"hat\/tests\/src\/main\/java\/hat\/test\/TestMatMul.java","additions":195,"deletions":0,"binary":false,"changes":195,"status":"modified"},{"patch":"@@ -30,0 +30,1 @@\n+import hat.dialect.HATF16BinaryOp;\n@@ -32,0 +33,6 @@\n+import hat.dialect.HATF16VarLoadOp;\n+import hat.dialect.HATF16VarOp;\n+import hat.dialect.HATGlobalSizeOp;\n+import hat.dialect.HATGlobalThreadIdOp;\n+import hat.dialect.HATLocalSizeOp;\n+import hat.dialect.HATLocalThreadIdOp;\n@@ -34,0 +41,2 @@\n+import hat.dialect.HATVectorBinaryOp;\n+import hat.dialect.HATVectorLoadOp;\n@@ -38,5 +47,0 @@\n-import hat.dialect.HATF16BinaryOp;\n-import hat.dialect.HATF16VarLoadOp;\n-import hat.dialect.HATF16VarOp;\n-import hat.dialect.HATVectorBinaryOp;\n-import hat.dialect.HATVectorLoadOp;\n@@ -44,4 +48,0 @@\n-import hat.dialect.HATGlobalThreadIdOp;\n-import hat.dialect.HATGlobalSizeOp;\n-import hat.dialect.HATLocalSizeOp;\n-import hat.dialect.HATLocalThreadIdOp;\n","filename":"hat\/tools\/src\/main\/java\/hat\/tools\/text\/JavaHATCodeBuilder.java","additions":9,"deletions":9,"binary":false,"changes":18,"status":"modified"}]}