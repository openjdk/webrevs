{"files":[{"patch":"@@ -3,23 +3,0 @@\n-## Build\n-\n-CRaC JDK have extended build procedure.\n-\n-1. Build JDK as usual\n-```\n-bash configure\n-make images\n-mv build\/linux-x86_64-server-release\/images\/jdk\/ .\n-```\n-2. Download a build of [modified CRIU](https:\/\/github.com\/CRaC\/criu\/releases\/tag\/release-1.4)\n-3. Extract and copy `criu` binary over a same named file in the JDK\n-```\n-cp criu-dist\/sbin\/criu jdk\/lib\/criu\n-```\n-Grant permissions to allow regular user to run it\n-```\n-sudo chown root:root jdk\/lib\/criu\n-sudo chmod u+s jdk\/lib\/criu\n-```\n-\n-# JDK\n-\n@@ -36,0 +13,16 @@\n+\n+## Building with CRIU\n+\n+To be able to use the default CRIU-based CRaC implementation follow these\n+additional steps after building the JDK:\n+\n+1. Download a build of [modified CRIU](https:\/\/github.com\/CRaC\/criu\/releases\/tag\/release-1.4)\n+2. Extract and copy `criu` binary over a file with the same name in the JDK\n+   ```\n+   cp $CRIU_DIR\/sbin\/criu $JDK_DIR\/lib\/criu\n+   ```\n+3. Grant permissions to allow a regular user to run it\n+   ```\n+   sudo chown root:root $JDK_DIR\/lib\/criu\n+   sudo chmod u+s $JDK_DIR\/lib\/criu\n+   ```\n","filename":"README.md","additions":16,"deletions":23,"binary":false,"changes":39,"status":"modified"},{"patch":"@@ -0,0 +1,144 @@\n+# Portable checkpoint-restore algorithm\n+\n+This document describes the algorithm used by CRaC in the portable mode.\n+\n+On checkpoint, three dumps are created:\n+\n+1. Dump of Java threads' states — includes only user-created threads, while GC, compiler and other VM-created threads\n+   are excluded since their state is not portable.\n+    - If an included thread executes native code the checkpoint gets postponed in hopes that the thread will leave the\n+      native code soon\n+    - Some special cases (e.g. CRaC's own native code) are handled specifically and are allowed to be checkpointed\n+2. Class dump — basically a list of serialized `InstanceKlass`es.\n+3. Heap dump — currently an HPROF dump.\n+\n+The following sections describe the process of restoration from the above files.\n+\n+## Metaspace and heap\n+\n+Restoration of class metadata and objects begins after the second phase of VM initialization because \"only java.base\n+classes can be loaded until phase 2 completes\" while the classes being restored can come from arbitrary modules. At that\n+point a lot of JDK's own Java code has been executed (e.g. foundational classes such as `java.lang.Class` have been\n+loaded and initialized, initialization methods of `java.lang.System` have been executed and even CRaC's own\n+`jdk.crac.Core` with the global context has been used), so many system classes already exist in a new state. And such\n+re-initialization cannot be omitted since it is platform-dependent, e.g. system properties which are set up during the\n+first initialization phase must be retrieved from the current system and not restored from the old one.\n+\n+The current implementation even begins the restoration after the initialization fully completes as to not be bothered\n+by stuff like proper system class loader and security manager initialization, so even some user code can get executed\n+(from a user-provided system class loader, for instance).\n+\n+It is planned to change the implementation to restore all classes before any Java code is executed so that there is no\n+need to match the new and the checkpointed states.\n+\n+### Defining the dumped classes\n+\n+First, class dump is parsed and the dumped classes are defined. The dump is structured in such a way that the classes\n+can be defined as the dump is being parsed: super classes and interfaces of a class as well as its class loader's class\n+all precede the class in the dump.\n+\n+Prior to allocating a class its defining class loader must be prepared if it does not already exist (i.e. if it is not\n+the bootstrap, platform or system loader). Preparation of a class loader consists of its allocation and restoration of\n+a few of its fields used during class definition, such as its name and array of defined classes. The class of the class\n+loader is already defined at this moment which is guaranteed by the class dump structure. The classes of the restored\n+fields' values are also defined since they are all well-known system classes used in the VM initialization process. All\n+objects created during preparation are recorded with their dumped IDs so that they are not re-created during the\n+subsequent heap restoration.\n+\n+After a class is allocated, it is defined. If the class is already defined (i.e. it is a system class) the pre-defined\n+version is used. It is assumed that the pre-defined version is compatible with the one created from the dump, i.e. it is\n+created from the same class file, but this is not checked for performance reasons (only non-product asserts exist). If\n+the pre-defined version has not yet been rewritten but the dumped one was, methods' code is transferred from the dumped\n+class into the pre-defined one. After that, if there was a pre-defined version, the newly-created one is deallocated.\n+\n+> There is a problem with hidden classes in this approach: it is impossible to know if a hidden class has been defined\n+> already or not because there can be an arbitrary amount of hidden classes defined with the same class loader from the\n+> same class file. Because of this the algorithm will always define a new hidden class. This may be a problem if a\n+> pre-defined class and a newly-created class or a restored stack value reference such class, for example:\n+>  - Let's assume that at the time of checkpoint `java.lang.System` and a local variable of the main thread\n+     >    referenced a hidden class `H` created early during the VM initialization (i.e. they referenced an instance of\n+     >    `H` or an instance of `java.lang.Class<H>`).\n+>  - In this case, when we restore, `System` is again initialized early by the VM and so is `H` which becomes referenced\n+     >    by `System` again.\n+>  - Then we start the restoration, the algorithm cannot find `H` since it is hidden and defines a new version of it.\n+>  - After that the restored main thread will use the new version of `H` while `System` uses another one — this is\n+     >    contrary to what was checkpointed.\n+\n+After all dumped classes have been defined, inter-class references of their constant pools are filled. This cannot be\n+performed while parsing since these references can contain cycles: if class `A` references class `B` and `B` references\n+`A` we need to define both before these references can be restored.\n+\n+Then, classes are recorded with their non-defining initiating loaders so that these won't repeat the loading process.\n+Loading constraints are also restored (not implemented yet).\n+\n+### Restoring the heap\n+\n+Firstly, `java.lang.Class` objects mirroring all created classes are recorded as created with their dumped IDs. Their\n+fields which are filled during object creation (e.g. name, class loader and module references) are also recorded.\n+\n+> In fact, all references from the pre-initialized (i.e. defined and initialized during the VM restoration) classes must\n+> also be recorded at this step, or they will be duplicated if referenced from a non-pre-defined class or a stack value\n+> of a thread being restored. See the note about hidden classes above for an example — here the idea is the same. If we\n+> figure out a way to fix this, we'll also fix the hidden classes problem because a reference to a hidden class is a\n+> reference to an object: either its `java.lang.Class` or its instance. The current implementation already pre-records\n+> platform and system loaders to not duplicate them.\n+>\n+> Fixing this is not trivial because we would need to match the new state of the pre-initialized classes with the\n+> dumped one while they may not be compatible. E.g. it's not clear what to do if a field `F` is dumped as containing an\n+> object of class `C1` with two fields but in the new state field `F` contains an object of class `C2` with a single\n+> field or no object at all.\n+>\n+> And also we'll need to account for pre-existing threads which can change the state of classes concurrently\n+> (newly-defined ones are captured by the restoring thread and cannot be tempered with). But the right way to solve this\n+> is probably to block user-threads from being created until the restoration finishes and ensure VM-created threads\n+> won't do this (not currently implemented, cannot use the safepoint mechanism for this since objects allocation is\n+> needed during restoration).\n+\n+Then, each class that has not been pre-initialized is restored (pre-initialized classes are skipped since they already\n+have a new state):\n+\n+- Fields of its `java.lang.Class` object are restored. If a field is already set, it's value is recorded instead — this\n+  can happen because `Class` object can be used even before the corresponding class is initialized. Previously prepared\n+  class loaders are also restored here since the defining loader is referenced by `Class`.\n+  > In fact, we should block concurrent modification of these objects and pre-record references from them for all\n+  pre-defined classes just as we should do for static fields of pre-initialized classes (as noted above).\n+- Its static fields are restored.\n+\n+After a class is restored, its state is set to the target state, e.g. it is marked as initialized.\n+\n+Finally, references from stacks of dumped threads are restored.\n+\n+The restoration of references is recursive: to restore a reference to an object means to find its class, allocate the\n+object, record it with its ID and restore its fields some of which may also be references requiring the same (recursive)\n+restoration process.\n+\n+Instances of many system classes require special treatment because they have platform-dependent fields and even raw\n+pointers which cannot be filled as-is or because they have to be recorded in VM's data structures.\n+\n+> Handling of some system classes even involves Java code invocation. We rely on the fact that this is a well-known code\n+> which won't use any classes not yet fully restored or change the state of the classes.\n+\n+## Threads\n+\n+To restore a threads means to restore its `java.lang.Thread` object, which is done during the heap restoration described\n+above, and to make it resume its execution from the moment it left off.\n+\n+The `main` thread and its Java object is created early during the VM's initialization process, so it is not restored.\n+`main` is also currently the single thread that performs the whole restoration process.\n+\n+After the classes and objects have been restored, `main` creates platform threads for each checkpointed thread stack\n+except its own and makes them wait on a latch. After the threads have been created, `main` releases the latch and the\n+threads start restoring their executions. If there was an execution checkpointed for `main` it then also starts\n+restoring that execution.\n+\n+Restoration of an execution is implemented in a way similar to the deoptimization implementation (some of its parts\n+are even used directly).\n+\n+1. Thread calls into the oldest method on the execution stack (i.e. the one that was called first) replacing its entry\n+   point with an entry into a special stack restoration blob written in assembly.\n+2. The blob calls back into C++ code to convert the checkpointed stack into the format used by the deoptimization\n+   implementation — this is done inside the Java call because there should be no safepoints while the stack exists in\n+   that format.\n+3. The blob creates interpreted stack frames of required sizes and calls into C++ code again to fill them with restored\n+   data.\n+4. After the frames have been restored, the control flow is passed to the interpreter and the execution is resumed.\n","filename":"doc\/portable-cr.md","additions":144,"deletions":0,"binary":false,"changes":144,"status":"added"},{"patch":"@@ -48,0 +48,1 @@\n+#include \"runtime\/cracThreadRestorer.hpp\"\n@@ -2555,0 +2556,170 @@\n+\/\/------------------------------generate_restore_blob---------------------------\n+void SharedRuntime::generate_restore_blob() {\n+  ResourceMark rm;\n+  CodeBuffer buffer(\"restore_blob\", 2048, 1024); \/\/ TODO the numbers are random\n+  auto *masm = new MacroAssembler(&buffer);\n+\n+  \/\/ This is called by the CallStub instead of the entry of the actual method\n+  \/\/ that should have been called. We should recreate the checkpointed stack as\n+  \/\/ a series of interpreter frames and pass the execution flow to its youngest\n+  \/\/ frame.\n+  \/\/\n+  \/\/ The generated code is almost identical to deopt_blob.\n+\n+#ifdef ASSERT\n+  { \/\/ The return pc must be the CallStub return address\n+    Label L;\n+    __ movptr(rscratch1, (uintptr_t) StubRoutines::call_stub_return_address());\n+    __ cmp(lr, rscratch1);\n+    __ br(Assembler::EQ, L);\n+    __ stop(\"SharedRuntime::generate_restore_blob: caller is not CallStub\");\n+    __ bind(L);\n+  }\n+#endif \/\/ ASSERT\n+\n+  \/\/ Call C code that will prepare for us the info about the frames to restore.\n+  \/\/ No blocking or GC can happen, frames are not accessed.\n+  \/\/\n+  \/\/ UnrollBlock* CracThreadRestorer::fetch_frame_info(JavaThread* current)\n+\n+  __ mov(c_rarg0, rthread);\n+  __ lea(rscratch1, RuntimeAddress(CAST_FROM_FN_PTR(address, CracThreadRestorer::fetch_frame_info)));\n+  __ blr(rscratch1);\n+  \/\/ TODO oop map?\n+\n+  \/\/ Load UnrollBlock* into r5\n+  constexpr Register unroll_block = r5;\n+  __ mov(unroll_block, r0);\n+\n+  \/\/ Check for possible stack overflow\n+  __ ldrw(r1, Address(unroll_block, Deoptimization::UnrollBlock::total_frame_sizes_offset()));\n+  __ bang_stack_size(r1, r2);\n+\n+  \/\/ Load address of array of frame pcs into r2\n+  constexpr Register frame_pcs = r2;\n+  __ ldr(frame_pcs, Address(unroll_block, Deoptimization::UnrollBlock::frame_pcs_offset()));\n+#ifdef ASSERT\n+  { \/\/ The first return pc must be the CallStub return address\n+    Label L;\n+    __ movptr(rscratch1, (uintptr_t) StubRoutines::call_stub_return_address());\n+    __ ldr(rscratch2, frame_pcs);\n+    __ cmp(rscratch1, rscratch2);\n+    __ br(Assembler::EQ, L);\n+    __ stop(\"SharedRuntime::generate_restore_blob: oldest frame does not return to CallStub\");\n+    __ bind(L);\n+  }\n+#endif \/\/ ASSERT\n+\n+  \/\/ Load address of array of frame sizes into r4\n+  constexpr Register frame_sizes = r4;\n+  __ ldr(frame_sizes, Address(unroll_block, Deoptimization::UnrollBlock::frame_sizes_offset()));\n+\n+  \/\/ Load counter into r3\n+  constexpr Register frame_cnt = r3;\n+  __ ldrw(frame_cnt, Address(unroll_block, Deoptimization::UnrollBlock::number_of_frames_offset()));\n+#ifdef ASSERT\n+  { \/\/ There must be some frames to push\n+    Label L;\n+    __ cbnz(frame_cnt, L);\n+    __ stop(\"SharedRuntime::generate_restore_blob: no frames provided\");\n+    __ bind(L);\n+  }\n+#endif \/\/ ASSERT\n+\n+  \/\/ Adjust caller's frame to make space for locals\n+  constexpr Register sender_sp = r19_sender_sp;\n+#ifdef ASSERT\n+  { \/\/ Current sp should have been saved into r19 by CallStub\n+    Label L;\n+    __ cmp(sp, sender_sp);\n+    __ br(Assembler::EQ, L);\n+    __ stop(\"SharedRuntime::generate_restore_blob: caller did not save sp into r19\");\n+    __ bind(L);\n+  }\n+#endif \/\/ ASSERT\n+  __ ldrw(r6, Address(unroll_block, Deoptimization::UnrollBlock::caller_adjustment_offset()));\n+  __ sub(sp, sp, r6);\n+\n+  \/\/ QQ what is the purpose of this?\n+  __ mov(rscratch1, (uint64_t)0xDEADDEAD); \/\/ Make a recognizable pattern\n+  __ mov(rscratch2, rscratch1);\n+\n+  \/\/ Push interpreter frames in a loop (stack grows upwards):\n+  \/\/\n+  \/\/ after the loop        { [ stub return pc for n   ]\n+  \/\/ loop iteration n      { [ ...                    ]\n+  \/\/ ...\n+  \/\/                       \/ [ <unfilled>             ] <- sp i\n+  \/\/                      |  [ 0                      ]\n+  \/\/ loop iteration i     |  [ sp i-1                 ]\n+  \/\/                      |  [ fp i-1                 ] <- fp i\n+  \/\/                       \\ [ stub return pc for i-1 ]\n+  \/\/ ...\n+  \/\/                       \/ [ <unfilled>             ] <- sp 2\n+  \/\/                      |  [ 0                      ]\n+  \/\/ loop iteration 2     |  [ sp 1                   ]\n+  \/\/                      |  [ fp 1                   ] <- fp 2\n+  \/\/                       \\ [ stub return pc for 1   ]\n+  \/\/                       \/ [ <unfilled>             ] <- sp 1\n+  \/\/                      |  [ 0                      ]\n+  \/\/ loop iteration 1     |  [ sp 0                   ]\n+  \/\/                      |  [ fp 0                   ] <- fp 1\n+  \/\/                       \\ [ return pc for CallStub ]\n+  \/\/ pushed above          { [ <unfilled locals of 1> ]\n+  \/\/                       \/ [ parameters             ] <- sp 0\n+  \/\/ pushed by CallStub   |             ...\n+  \/\/                       \\ [ ...                    ] <- fp 0\n+  \/\/\n+  Label loop;\n+  __ bind(loop);\n+  __ ldr(r6, Address(__ post(frame_sizes, wordSize))); \/\/ Load frame size\n+  __ sub(r6, r6, 2 * wordSize);                        \/\/ We'll push pc and fp by hand\n+  __ ldr(lr, Address(__ post(frame_pcs, wordSize)));   \/\/ Load return address\n+  __ enter();                                          \/\/ Save return address and old fp, set new fp\n+  __ sub(sp, sp, r6);                                  \/\/ Prolog\n+  \/\/ This value is corrected by layout_activation_impl\n+  __ str(zr, Address(rfp, frame::interpreter_frame_last_sp_offset * wordSize));\n+  __ str(sender_sp, Address(rfp, frame::interpreter_frame_sender_sp_offset * wordSize)); \/\/ Make it walkable\n+  __ mov(sender_sp, sp);                               \/\/ Pass sender_sp to next frame\n+  __ decrement(frame_cnt);                             \/\/ Decrement counter\n+  __ cbnz(frame_cnt, loop);\n+\n+  \/\/ Re-push self-frame\n+  __ ldr(lr, Address(frame_pcs)); \/\/ Load final return address\n+  __ enter();                     \/\/ Save return address and old fp, set new fp\n+\n+  \/\/ Call C code. It should fill the skeletom frames we've pushed.\n+  \/\/\n+  \/\/ void CracThreadRestorer::fill_in_frames(JavaThread* current)\n+\n+  \/\/ Use rfp because the frames look interpreted now.\n+  \/\/ Don't need the precise return PC here, just precise enough to point into this code blob.\n+  __ set_last_Java_frame(sp, rfp, __ pc(), rscratch1);\n+\n+  __ mov(c_rarg0, rthread);\n+  __ lea(rscratch1, RuntimeAddress(CAST_FROM_FN_PTR(address, CracThreadRestorer::fill_in_frames)));\n+  __ blr(rscratch1);\n+  \/\/ TODO oop map?\n+\n+  \/\/ Clear fp AND pc\n+  __ reset_last_Java_frame(true);\n+\n+  \/\/ Pop self-frame\n+  __ leave();                     \/\/ Epilog\n+\n+  \/\/ Jump to interpreter\n+  __ ret(lr);\n+\n+  \/\/ Make sure all code is generated\n+  masm->flush();\n+\n+  \/\/ Our own frame just before the CracThreadRestorer::fill_in_frames() call:\n+  \/\/ [ old fp pushed by enter()                   ] <- fp, sp\n+  \/\/ [ stub return pc for the last skeletal frame ]\n+  \/\/\n+  \/\/ Used in CracThreadRestorer::fill_in_frames() for stack walking\n+  int frame_size_in_words = 2;\n+\n+  _restore_blob = RestoreBlob::create(&buffer, frame_size_in_words);\n+}\n+\n","filename":"src\/hotspot\/cpu\/aarch64\/sharedRuntime_aarch64.cpp","additions":171,"deletions":0,"binary":false,"changes":171,"status":"modified"},{"patch":"@@ -1613,0 +1613,11 @@\n+\/\/------------------------------generate_restore_blob---------------------------\n+void SharedRuntime::generate_restore_blob() {\n+  ResourceMark rm;\n+  CodeBuffer buffer(\"restore_blob\", 128, 128); \/\/ TODO the numbers are random\n+  auto *masm = new MacroAssembler(&buffer);\n+\n+  __ should_not_reach_here(); \/\/ TODO Unimplemented\n+\n+  _restore_blob = RestoreBlob::create(&buffer, 0);\n+}\n+\n","filename":"src\/hotspot\/cpu\/arm\/sharedRuntime_arm.cpp","additions":11,"deletions":0,"binary":false,"changes":11,"status":"modified"},{"patch":"@@ -3155,0 +3155,11 @@\n+\/\/------------------------------generate_restore_blob---------------------------\n+void SharedRuntime::generate_restore_blob() {\n+  ResourceMark rm;\n+  CodeBuffer buffer(\"restore_blob\", 128, 128); \/\/ TODO the numbers are random\n+  auto *masm = new MacroAssembler(&buffer);\n+\n+  __ should_not_reach_here(); \/\/ TODO Unimplemented\n+\n+  _restore_blob = RestoreBlob::create(&buffer, 0);\n+}\n+\n","filename":"src\/hotspot\/cpu\/ppc\/sharedRuntime_ppc.cpp","additions":11,"deletions":0,"binary":false,"changes":11,"status":"modified"},{"patch":"@@ -2443,0 +2443,11 @@\n+\/\/------------------------------generate_restore_blob---------------------------\n+void SharedRuntime::generate_restore_blob() {\n+  ResourceMark rm;\n+  CodeBuffer buffer(\"restore_blob\", 128, 128); \/\/ TODO the numbers are random\n+  auto *masm = new MacroAssembler(&buffer);\n+\n+  __ should_not_reach_here(); \/\/ TODO Unimplemented\n+\n+  _restore_blob = RestoreBlob::create(&buffer, 0);\n+}\n+\n","filename":"src\/hotspot\/cpu\/riscv\/sharedRuntime_riscv.cpp","additions":11,"deletions":0,"binary":false,"changes":11,"status":"modified"},{"patch":"@@ -2725,0 +2725,11 @@\n+\/\/------------------------------generate_restore_blob---------------------------\n+void SharedRuntime::generate_restore_blob() {\n+  ResourceMark rm;\n+  CodeBuffer buffer(\"restore_blob\", 128, 128); \/\/ TODO the numbers are random\n+  auto *masm = new MacroAssembler(&buffer);\n+\n+  __ should_not_reach_here(); \/\/ TODO Unimplemented\n+\n+  _restore_blob = RestoreBlob::create(&buffer, 0);\n+}\n+\n","filename":"src\/hotspot\/cpu\/s390\/sharedRuntime_s390.cpp","additions":11,"deletions":0,"binary":false,"changes":11,"status":"modified"},{"patch":"@@ -2426,0 +2426,10 @@\n+\/\/------------------------------generate_restore_blob---------------------------\n+void SharedRuntime::generate_restore_blob() {\n+  ResourceMark rm;\n+  CodeBuffer buffer(\"restore_blob\", 128, 128); \/\/ TODO the numbers are random\n+  auto *masm = new MacroAssembler(&buffer);\n+\n+  __ should_not_reach_here(); \/\/ TODO Unimplemented\n+\n+  _restore_blob = RestoreBlob::create(&buffer, 0);\n+}\n","filename":"src\/hotspot\/cpu\/x86\/sharedRuntime_x86_32.cpp","additions":10,"deletions":0,"binary":false,"changes":10,"status":"modified"},{"patch":"@@ -51,0 +51,1 @@\n+#include \"runtime\/cracThreadRestorer.hpp\"\n@@ -2893,0 +2894,183 @@\n+\/\/------------------------------generate_restore_blob---------------------------\n+void SharedRuntime::generate_restore_blob() {\n+  ResourceMark rm;\n+  CodeBuffer buffer(\"restore_blob\", 2048, 1024); \/\/ TODO the numbers are random\n+  auto *masm = new MacroAssembler(&buffer);\n+\n+  \/\/ This is called by the CallStub instead of the entry of the actual method\n+  \/\/ that should have been called. We should recreate the checkpointed stack as\n+  \/\/ a series of interpreter frames and pass the execution flow to its youngest\n+  \/\/ frame.\n+  \/\/\n+  \/\/ The generated code is almost identical to deopt_blob.\n+\n+#ifdef ASSERT\n+  { \/\/ The return pc must be the CallStub return address\n+    Label L;\n+    __ cmpptr(Address(rsp, 0), ExternalAddress(StubRoutines::call_stub_return_address()).addr(), rscratch1);\n+    __ jcc(Assembler::equal, L);\n+    __ stop(\"SharedRuntime::generate_restore_blob: caller is not CallStub\");\n+    __ bind(L);\n+  }\n+#endif \/\/ ASSERT\n+\n+  \/\/ Call C code that will prepare for us the info about the frames to restore.\n+  \/\/ No blocking or GC can happen, frames are not accessed.\n+  \/\/\n+  \/\/ UnrollBlock* CracThreadRestorer::fetch_frame_info(JavaThread* current)\n+\n+  __ movptr(rbx, rsp);                      \/\/ Save SP\n+  __ andptr(rsp, -(StackAlignmentInBytes)); \/\/ Fix stack alignment as required by ABI\n+  \/\/ Allocate argument register save area\n+  if (frame::arg_reg_save_area_bytes != 0) {\n+    __ subptr(rsp, frame::arg_reg_save_area_bytes);\n+  }\n+  __ mov(c_rarg0, r15_thread);\n+  __ call(RuntimeAddress(CAST_FROM_FN_PTR(address, CracThreadRestorer::fetch_frame_info)));\n+  \/\/ TODO oop map?\n+  __ movptr(rsp, rbx);                      \/\/ Restore SP\n+\n+  \/\/ Load UnrollBlock* into rdi\n+  constexpr Register unroll_block = rdi;\n+  __ movptr(unroll_block, rax);\n+\n+  \/\/ Check for possible stack overflow\n+  __ movl(rbx, Address(unroll_block, Deoptimization::UnrollBlock::total_frame_sizes_offset()));\n+  __ bang_stack_size(rbx, rcx);\n+\n+  \/\/ Load address of array of frame pcs into rcx\n+  constexpr Register frame_pcs = rcx;\n+  __ movptr(frame_pcs, Address(unroll_block, Deoptimization::UnrollBlock::frame_pcs_offset()));\n+#ifdef ASSERT\n+  { \/\/ The first return pc must be the CallStub return address\n+    Label L;\n+    __ cmpptr(Address(frame_pcs, 0), ExternalAddress(StubRoutines::call_stub_return_address()).addr(), rscratch1);\n+    __ jcc(Assembler::equal, L);\n+    __ stop(\"SharedRuntime::generate_restore_blob: oldest frame does not return to CallStub\");\n+    __ bind(L);\n+  }\n+#endif \/\/ ASSERT\n+\n+  \/\/ Trash the old pc\n+  __ addptr(rsp, wordSize);\n+\n+  \/\/ Load address of array of frame sizes into rsi\n+  constexpr Register frame_sizes = rsi;\n+  __ movptr(frame_sizes, Address(unroll_block, Deoptimization::UnrollBlock::frame_sizes_offset()));\n+\n+  \/\/ Load counter into rdx\n+  constexpr Register frame_cnt = rdx;\n+  __ movl(frame_cnt, Address(unroll_block, Deoptimization::UnrollBlock::number_of_frames_offset()));\n+#ifdef ASSERT\n+  { \/\/ There must be some frames to push\n+    Label L;\n+    __ testl(frame_cnt, frame_cnt);\n+    __ jcc(Assembler::notZero, L);\n+    __ stop(\"SharedRuntime::generate_restore_blob: no frames provided\");\n+    __ bind(L);\n+  }\n+#endif \/\/ ASSERT\n+\n+  \/\/ Adjust caller's frame to make space for locals\n+  constexpr Register sender_sp = r13;\n+#ifdef ASSERT\n+  { \/\/ Current sp should have been saved into r13 by CallStub\n+    Label L;\n+    __ cmpptr(rsp, sender_sp);\n+    __ jcc(Assembler::equal, L);\n+    __ stop(\"SharedRuntime::generate_restore_blob: caller did not save sp into r13\");\n+    __ bind(L);\n+  }\n+#endif \/\/ ASSERT\n+  __ movl(rbx, Address(unroll_block, Deoptimization::UnrollBlock::caller_adjustment_offset()));\n+  __ subptr(rsp, rbx);\n+\n+  \/\/ Push interpreter frames in a loop (stack grows upwards):\n+  \/\/\n+  \/\/ after the loop        { [ stub return pc for n   ]\n+  \/\/ loop iteration n      { [ ...                    ]\n+  \/\/ ...\n+  \/\/                       \/ [ <unfilled>             ] <- sp i\n+  \/\/                      |  [ 0                      ]\n+  \/\/ loop iteration i     |  [ sp i-1                 ]\n+  \/\/                      |  [ fp i-1                 ] <- fp i\n+  \/\/                       \\ [ stub return pc for i-1 ]\n+  \/\/ ...\n+  \/\/                       \/ [ <unfilled>             ] <- sp 2\n+  \/\/                      |  [ 0                      ]\n+  \/\/ loop iteration 2     |  [ sp 1                   ]\n+  \/\/                      |  [ fp 1                   ] <- fp 2\n+  \/\/                       \\ [ stub return pc for 1   ]\n+  \/\/                       \/ [ <unfilled>             ] <- sp 1\n+  \/\/                      |  [ 0                      ]\n+  \/\/ loop iteration 1     |  [ sp 0                   ]\n+  \/\/                      |  [ fp 0                   ] <- fp 1\n+  \/\/                       \\ [ return pc for CallStub ]\n+  \/\/ pushed above          { [ <unfilled locals of 1> ]\n+  \/\/                       \/ [ parameters             ] <- sp 0\n+  \/\/ pushed by CallStub   |             ...\n+  \/\/                       \\ [ ...                    ] <- fp 0\n+  \/\/\n+  Label loop;\n+  __ bind(loop);\n+  __ movptr(rbx, Address(frame_sizes, 0)); \/\/ Load frame size\n+  __ subptr(rbx, 2 * wordSize);            \/\/ We'll push pc and ebp by hand\n+  __ pushptr(Address(frame_pcs, 0));       \/\/ Save return address\n+  __ enter();                              \/\/ Save old & set new ebp\n+  __ subptr(rsp, rbx);                     \/\/ Prolog\n+#ifndef PRODUCT\n+  __ zero_memory(rsp, rbx, 0, rscratch1);  \/\/ Zero unfilled memory to simplify debugging\n+#else \/\/ PRODUCT\n+  \/\/ This value is corrected by layout_activation_impl\n+  __ movptr(Address(rbp, frame::interpreter_frame_last_sp_offset * wordSize), NULL_WORD);\n+#endif \/\/ PRODUCT\n+  __ movptr(Address(rbp, frame::interpreter_frame_sender_sp_offset * wordSize), sender_sp); \/\/ Make it walkable\n+  __ mov(sender_sp, rsp);                  \/\/ Pass sender_sp to next frame\n+  __ addptr(frame_sizes, wordSize);        \/\/ Bump array pointer (sizes)\n+  __ addptr(frame_pcs, wordSize);          \/\/ Bump array pointer (pcs)\n+  __ decrementl(frame_cnt);                \/\/ Decrement counter\n+  __ jcc(Assembler::notZero, loop);\n+  __ pushptr(Address(frame_pcs, 0));       \/\/ Save final return address\n+\n+  \/\/ Push self-frame\n+  __ enter();                              \/\/ Save old & set new ebp\n+\n+  \/\/ Call C code. It should fill the skeletom frames we've pushed.\n+  \/\/\n+  \/\/ void CracThreadRestorer::fill_in_frames(JavaThread* current)\n+\n+  \/\/ Use rbp because the frames look interpreted now.\n+  \/\/ Don't need the precise return PC here, just precise enough to point into this code blob.\n+  __ set_last_Java_frame(noreg, rbp, __ pc(), rscratch1);\n+\n+  __ andptr(rsp, -(StackAlignmentInBytes));  \/\/ Fix stack alignment as required by ABI\n+  \/\/ Allocate argument register save area\n+  if (frame::arg_reg_save_area_bytes != 0) {\n+    __ subptr(rsp, frame::arg_reg_save_area_bytes);\n+  }\n+  __ mov(c_rarg0, r15_thread);\n+  __ call(RuntimeAddress(CAST_FROM_FN_PTR(address, CracThreadRestorer::fill_in_frames)));\n+  \/\/ TODO oop map?\n+\n+  \/\/ Clear fp AND pc\n+  __ reset_last_Java_frame(true);\n+\n+  \/\/ Pop self-frame\n+  __ leave();                              \/\/ Epilog\n+\n+  \/\/ Jump to interpreter\n+  __ ret(0);\n+\n+  \/\/ Make sure all code is generated\n+  masm->flush();\n+\n+  \/\/ Our own frame just before the CracThreadRestorer::fill_in_frames() call:\n+  \/\/ [ old fp pushed by enter()                   ] <- fp, sp\n+  \/\/ [ stub return pc for the last skeletal frame ]\n+  \/\/\n+  \/\/ Used in CracThreadRestorer::fill_in_frames() for stack walking\n+  int frame_size_in_words = 2;\n+\n+  _restore_blob = RestoreBlob::create(&buffer, frame_size_in_words);\n+}\n+\n","filename":"src\/hotspot\/cpu\/x86\/sharedRuntime_x86_64.cpp","additions":184,"deletions":0,"binary":false,"changes":184,"status":"modified"},{"patch":"@@ -106,0 +106,4 @@\n+static RestoreBlob* generate_empty_restore_blob() {\n+  return CAST_FROM_FN_PTR(RestoreBlob*,zero_stub);\n+}\n+\n@@ -111,0 +115,4 @@\n+void SharedRuntime::generate_restore_blob() {\n+  _restore_blob = generate_empty_restore_blob();\n+}\n+\n","filename":"src\/hotspot\/cpu\/zero\/sharedRuntime_zero.cpp","additions":8,"deletions":0,"binary":false,"changes":8,"status":"modified"},{"patch":"@@ -291,0 +291,2 @@\n+  \/\/ ensure won't reuse after completion in case of a retry\n+  _attach_op = NULL;\n","filename":"src\/hotspot\/os\/linux\/crac_linux.cpp","additions":2,"deletions":0,"binary":false,"changes":2,"status":"modified"},{"patch":"@@ -2743,1 +2743,1 @@\n-  if (is_hidden()) { \/\/ Mark methods in hidden classes as 'hidden'.\n+  if (_is_hidden) { \/\/ Mark methods in hidden classes as 'hidden'.\n@@ -3832,1 +3832,1 @@\n-    k->set_source_debug_extension(_sde_buffer, _sde_length);\n+    k->copy_source_debug_extension(_sde_buffer, _sde_length);\n@@ -4003,1 +4003,1 @@\n-void OopMapBlocksBuilder::copy(OopMapBlock* dst) {\n+void OopMapBlocksBuilder::copy(OopMapBlock* dst) const {\n@@ -4072,0 +4072,14 @@\n+void ClassFileParser::check_can_allocate_fast(InstanceKlass* ik) {\n+  \/\/ If it cannot be fast-path allocated, set a bit in the layout helper.\n+  \/\/ See documentation of InstanceKlass::can_be_fastpath_allocated().\n+  assert(ik->size_helper() > 0, \"layout_helper is initialized\");\n+  if ((!RegisterFinalizersAtInit && ik->has_finalizer())\n+      || ik->is_abstract() || ik->is_interface()\n+      || (ik->name() == vmSymbols::java_lang_Class() && ik->class_loader() == nullptr)\n+      || ik->size_helper() >= FastAllocateSizeLimit) {\n+    \/\/ Forbid fast-path allocation.\n+    const jint lh = Klass::instance_layout_helper(ik->size_helper(), true);\n+    ik->set_layout_helper(lh);\n+  }\n+}\n+\n@@ -4133,11 +4147,1 @@\n-  \/\/ If it cannot be fast-path allocated, set a bit in the layout helper.\n-  \/\/ See documentation of InstanceKlass::can_be_fastpath_allocated().\n-  assert(ik->size_helper() > 0, \"layout_helper is initialized\");\n-  if ((!RegisterFinalizersAtInit && ik->has_finalizer())\n-      || ik->is_abstract() || ik->is_interface()\n-      || (ik->name() == vmSymbols::java_lang_Class() && ik->class_loader() == nullptr)\n-      || ik->size_helper() >= FastAllocateSizeLimit) {\n-    \/\/ Forbid fast-path allocation.\n-    const jint lh = Klass::instance_layout_helper(ik->size_helper(), true);\n-    ik->set_layout_helper(lh);\n-  }\n+  check_can_allocate_fast(ik);\n@@ -4159,4 +4163,4 @@\n-static Array<InstanceKlass*>* compute_transitive_interfaces(const InstanceKlass* super,\n-                                                            Array<InstanceKlass*>* local_ifs,\n-                                                            ClassLoaderData* loader_data,\n-                                                            TRAPS) {\n+Array<InstanceKlass*>* ClassFileParser::compute_transitive_interfaces(const InstanceKlass* super,\n+                                                                      Array<InstanceKlass*>* local_ifs,\n+                                                                      ClassLoaderData* loader_data,\n+                                                                      TRAPS) {\n@@ -5044,17 +5048,1 @@\n-int ClassFileParser::static_field_size() const {\n-  assert(_field_info != nullptr, \"invariant\");\n-  return _field_info->_static_field_size;\n-}\n-\n-int ClassFileParser::total_oop_map_count() const {\n-  assert(_field_info != nullptr, \"invariant\");\n-  return _field_info->oop_map_blocks->_nonstatic_oop_map_count;\n-}\n-\n-jint ClassFileParser::layout_size() const {\n-  assert(_field_info != nullptr, \"invariant\");\n-  return _field_info->_instance_size;\n-}\n-\n-static void check_methods_for_intrinsics(const InstanceKlass* ik,\n-                                         const Array<Method*>* methods) {\n+void ClassFileParser::check_methods_for_intrinsics(const InstanceKlass* ik) {\n@@ -5062,0 +5050,2 @@\n+\n+  const Array<Method*> *methods = ik->methods();\n@@ -5161,0 +5151,4 @@\n+  assert(_field_info != nullptr, \"invariant\");\n+  const InstanceKlassSizes sizes{_vtable_size, _itable_size, _field_info->_instance_size, _field_info->_static_field_size,\n+                                 _field_info->oop_map_blocks->_nonstatic_oop_map_count};\n+\n@@ -5162,1 +5156,1 @@\n-    InstanceKlass::allocate_instance_klass(*this, CHECK_NULL);\n+    InstanceKlass::allocate_instance_klass(_loader_data, _class_name, _super_klass, _access_flags, sizes, CHECK_NULL);\n@@ -5164,1 +5158,1 @@\n-  if (is_hidden()) {\n+  if (_is_hidden) {\n@@ -5181,0 +5175,5 @@\n+  \/\/ Set early for internal-to-external class name conversion, e.g. in logs\n+  if (_is_hidden) {\n+    ik->set_is_hidden();\n+  }\n+\n@@ -5264,4 +5263,0 @@\n-  if (_is_hidden) {\n-    ik->set_is_hidden();\n-  }\n-\n@@ -5274,5 +5269,2 @@\n-  const Array<Method*>* const methods = ik->methods();\n-  assert(methods != nullptr, \"invariant\");\n-  const int methods_len = methods->length();\n-\n-  check_methods_for_intrinsics(ik, methods);\n+  assert(ik->methods() != nullptr, \"invariant\");\n+  check_methods_for_intrinsics(ik);\n@@ -5663,1 +5655,1 @@\n-  if (is_hidden()) { \/\/ Add a slot for hidden class name.\n+  if (_is_hidden) { \/\/ Add a slot for hidden class name.\n@@ -5964,1 +5956,1 @@\n-  FieldLayoutBuilder lb(class_name(), super_klass(), _cp, \/*_fields*\/ _temp_field_info,\n+  FieldLayoutBuilder lb(_class_name, _super_klass, _cp, \/*_fields*\/ _temp_field_info,\n@@ -5971,1 +5963,1 @@\n-                                            injected_fields_count, loader_data(), CHECK);\n+                                            injected_fields_count, _loader_data, CHECK);\n@@ -6007,26 +5999,0 @@\n-ReferenceType ClassFileParser::super_reference_type() const {\n-  return _super_klass == nullptr ? REF_NONE : _super_klass->reference_type();\n-}\n-\n-bool ClassFileParser::is_instance_ref_klass() const {\n-  \/\/ Only the subclasses of j.l.r.Reference are InstanceRefKlass.\n-  \/\/ j.l.r.Reference itself is InstanceKlass because InstanceRefKlass denotes a\n-  \/\/ klass requiring special treatment in ref-processing. The abstract\n-  \/\/ j.l.r.Reference cannot be instantiated so doesn't partake in\n-  \/\/ ref-processing.\n-  return is_java_lang_ref_Reference_subclass();\n-}\n-\n-bool ClassFileParser::is_java_lang_ref_Reference_subclass() const {\n-  if (_super_klass == nullptr) {\n-    return false;\n-  }\n-\n-  if (_super_klass->name() == vmSymbols::java_lang_ref_Reference()) {\n-    \/\/ Direct subclass of j.l.r.Reference: Soft|Weak|Final|Phantom\n-    return true;\n-  }\n-\n-  return _super_klass->reference_type() != REF_NONE;\n-}\n-\n","filename":"src\/hotspot\/share\/classfile\/classFileParser.cpp","additions":41,"deletions":75,"binary":false,"changes":116,"status":"modified"},{"patch":"@@ -64,1 +64,1 @@\n-  void copy(OopMapBlock* dst);\n+  void copy(OopMapBlock* dst) const;\n@@ -550,7 +550,0 @@\n-  int static_field_size() const;\n-  int total_oop_map_count() const;\n-  jint layout_size() const;\n-\n-  int vtable_size() const { return _vtable_size; }\n-  int itable_size() const { return _itable_size; }\n-\n@@ -559,13 +552,0 @@\n-  bool is_hidden() const { return _is_hidden; }\n-  bool is_interface() const { return _access_flags.is_interface(); }\n-\n-  ClassLoaderData* loader_data() const { return _loader_data; }\n-  const Symbol* class_name() const { return _class_name; }\n-  const InstanceKlass* super_klass() const { return _super_klass; }\n-\n-  ReferenceType super_reference_type() const;\n-  bool is_instance_ref_klass() const;\n-  bool is_java_lang_ref_Reference_subclass() const;\n-\n-  AccessFlags access_flags() const { return _access_flags; }\n-\n@@ -574,0 +554,7 @@\n+  static Array<InstanceKlass*>* compute_transitive_interfaces(const InstanceKlass* super,\n+                                                              Array<InstanceKlass*>* local_ifs,\n+                                                              ClassLoaderData* loader_data,\n+                                                              TRAPS);\n+  static void check_methods_for_intrinsics(const InstanceKlass* ik);\n+  static void check_can_allocate_fast(InstanceKlass* ik);\n+\n","filename":"src\/hotspot\/share\/classfile\/classFileParser.hpp","additions":8,"deletions":21,"binary":false,"changes":29,"status":"modified"},{"patch":"@@ -209,1 +209,5 @@\n-  _table->do_scan(Thread::current(), all_doit);\n+  if (SafepointSynchronize::is_at_safepoint()) {\n+    _table->do_safepoint_scan(all_doit);\n+  } else {\n+    _table->do_scan(Thread::current(), all_doit);\n+  }\n@@ -224,0 +228,5 @@\n+int Dictionary::number_of_entries() const {\n+  assert_locked_or_safepoint(SystemDictionary_lock);\n+  return _number_of_entries;\n+}\n+\n","filename":"src\/hotspot\/share\/classfile\/dictionary.cpp","additions":10,"deletions":1,"binary":false,"changes":11,"status":"modified"},{"patch":"@@ -76,0 +76,2 @@\n+  int number_of_entries() const;\n+\n","filename":"src\/hotspot\/share\/classfile\/dictionary.hpp","additions":2,"deletions":0,"binary":false,"changes":2,"status":"modified"},{"patch":"@@ -108,1 +108,1 @@\n-FieldLayout::FieldLayout(GrowableArray<FieldInfo>* field_info, ConstantPool* cp) :\n+FieldLayout::FieldLayout(GrowableArrayView<FieldInfo>* field_info, ConstantPool* cp) :\n@@ -502,1 +502,1 @@\n-      GrowableArray<FieldInfo>* field_info, bool is_contended, FieldLayoutInfo* info) :\n+      GrowableArrayView<FieldInfo>* field_info, bool is_contended, FieldLayoutInfo* info) :\n","filename":"src\/hotspot\/share\/classfile\/fieldLayoutBuilder.cpp","additions":2,"deletions":2,"binary":false,"changes":4,"status":"modified"},{"patch":"@@ -167,1 +167,1 @@\n-  GrowableArray<FieldInfo>* _field_info;\n+  GrowableArrayView<FieldInfo>* _field_info;\n@@ -174,1 +174,1 @@\n-  FieldLayout(GrowableArray<FieldInfo>* field_info, ConstantPool* cp);\n+  FieldLayout(GrowableArrayView<FieldInfo>* field_info, ConstantPool* cp);\n@@ -233,1 +233,1 @@\n-  GrowableArray<FieldInfo>* _field_info;\n+  GrowableArrayView<FieldInfo>* _field_info;\n@@ -247,1 +247,1 @@\n-                     GrowableArray<FieldInfo>* field_info, bool is_contended, FieldLayoutInfo* info);\n+                     GrowableArrayView<FieldInfo>* field_info, bool is_contended, FieldLayoutInfo* info);\n","filename":"src\/hotspot\/share\/classfile\/fieldLayoutBuilder.hpp","additions":4,"deletions":4,"binary":false,"changes":8,"status":"modified"},{"patch":"@@ -918,1 +918,4 @@\n-  InstanceKlass::cast(k)->do_local_static_fields(&initialize_static_field, mirror, CHECK);\n+  InstanceKlass* ik = InstanceKlass::cast(k);\n+  if (!ik->is_being_restored()) { \/\/ No need to initialize if going to be restored\n+    ik->do_local_static_fields(&initialize_static_field, mirror, CHECK);\n+  }\n@@ -4515,0 +4518,1 @@\n+int  java_lang_ClassLoader::_classes_offset;\n@@ -4539,1 +4543,2 @@\n-  macro(_parent_offset,          k1, \"parent\",               classloader_signature, false)\n+  macro(_parent_offset,          k1, \"parent\",               classloader_signature, false); \\\n+  macro(_classes_offset,         k1, \"classes\",              arraylist_signature, false)\n@@ -4583,0 +4588,5 @@\n+oop java_lang_ClassLoader::classes(oop loader) {\n+  assert(is_instance(loader), \"loader must be oop\");\n+  return loader->obj_field(_classes_offset);\n+}\n+\n@@ -4652,0 +4662,30 @@\n+void java_lang_ClassLoader::set_parent(oop loader, oop value) {\n+  assert(is_instance(loader), \"loader must be oop\");\n+  assert(loader->obj_field(_parent_offset) == nullptr, \"should not overwrite\");\n+  loader->obj_field_put(_parent_offset, value);\n+}\n+\n+void java_lang_ClassLoader::set_parallelLockMap(oop loader, oop value) {\n+  assert(is_instance(loader), \"loader must be oop\");\n+  assert(loader->obj_field(_parallelCapable_offset) == nullptr, \"should not overwrite\");\n+  loader->obj_field_put(_parallelCapable_offset, value);\n+}\n+\n+void java_lang_ClassLoader::set_name(oop loader, oop value) {\n+  assert(is_instance(loader), \"loader must be oop\");\n+  assert(loader->obj_field(_name_offset) == nullptr, \"should not overwrite\");\n+  loader->obj_field_put(_name_offset, value);\n+}\n+\n+void java_lang_ClassLoader::set_nameAndId(oop loader, oop value) {\n+  assert(is_instance(loader), \"loader must be oop\");\n+  assert(loader->obj_field(_nameAndId_offset) == nullptr, \"should not overwrite\");\n+  loader->obj_field_put(_nameAndId_offset, value);\n+}\n+\n+void java_lang_ClassLoader::set_unnamedModule(oop loader, oop value) {\n+  assert(is_instance(loader), \"loader must be oop\");\n+  assert(loader->obj_field(_unnamedModule_offset) == nullptr, \"should not overwrite\");\n+  loader->obj_field_put(_unnamedModule_offset, value);\n+}\n+\n","filename":"src\/hotspot\/share\/classfile\/javaClasses.cpp","additions":42,"deletions":2,"binary":false,"changes":44,"status":"modified"},{"patch":"@@ -1464,0 +1464,1 @@\n+  static int _classes_offset;\n@@ -1478,0 +1479,1 @@\n+  static oop classes(oop loader);\n@@ -1503,0 +1505,9 @@\n+\n+  \/\/ Class loader restoration\n+  friend class CracHeapRestorer;\n+ private:\n+  static void set_parent(oop loader, oop value);\n+  static void set_parallelLockMap(oop loader, oop value);\n+  static void set_name(oop loader, oop value);\n+  static void set_nameAndId(oop loader, oop value);\n+  static void set_unnamedModule(oop loader, oop value);\n","filename":"src\/hotspot\/share\/classfile\/javaClasses.hpp","additions":11,"deletions":0,"binary":false,"changes":11,"status":"modified"},{"patch":"@@ -904,1 +904,1 @@\n-    define_instance_class(k, class_loader, THREAD);\n+    define_instance_class(k, class_loader, true, THREAD);\n@@ -1339,0 +1339,17 @@\n+void SystemDictionary::record_initiating_loader(InstanceKlass* loaded_class, Handle class_loader, TRAPS) {\n+  assert(loaded_class->class_loader() != class_loader(), \"defining loader already is initiating\");\n+  assert(!loaded_class->is_hidden(), \"hidden classes cannot have non-defining initiating loaders\");\n+\n+  ClassLoaderData* loader_data = class_loader_data(class_loader);\n+  check_constraints(loaded_class, loader_data, false, CHECK);\n+\n+  \/\/ Record dependency for non-parent delegation.\n+  \/\/ This recording keeps the defining class loader of the klass (loaded_class) found\n+  \/\/ from being unloaded while the initiating class loader is loaded\n+  \/\/ even if the reference to the defining class loader is dropped\n+  \/\/ before references to the initiating class loader.\n+  loader_data->record_dependency(loaded_class);\n+\n+  update_dictionary(THREAD, loaded_class, loader_data);\n+}\n+\n@@ -1350,11 +1367,1 @@\n-    ClassLoaderData* loader_data = class_loader_data(class_loader);\n-    check_constraints(loaded_class, loader_data, false, CHECK_NULL);\n-\n-    \/\/ Record dependency for non-parent delegation.\n-    \/\/ This recording keeps the defining class loader of the klass (loaded_class) found\n-    \/\/ from being unloaded while the initiating class loader is loaded\n-    \/\/ even if the reference to the defining class loader is dropped\n-    \/\/ before references to the initiating class loader.\n-    loader_data->record_dependency(loaded_class);\n-\n-    update_dictionary(THREAD, loaded_class, loader_data);\n+    record_initiating_loader(loaded_class, class_loader, CHECK_NULL);\n@@ -1378,1 +1385,1 @@\n-void SystemDictionary::define_instance_class(InstanceKlass* k, Handle class_loader, TRAPS) {\n+void SystemDictionary::define_instance_class(InstanceKlass* k, Handle class_loader, bool publicize, TRAPS) {\n@@ -1410,1 +1417,3 @@\n-  if (k->class_loader() != nullptr) {\n+  if (k->class_loader() != nullptr &&\n+      \/\/ ArrayList is null during CRaC's portable restoration, it is restored later\n+      java_lang_ClassLoader::classes(class_loader()) != nullptr) {\n@@ -1425,3 +1434,6 @@\n-  \/\/ notify jvmti\n-  if (JvmtiExport::should_post_class_load()) {\n-    JvmtiExport::post_class_load(THREAD, k);\n+  if (publicize) {\n+    \/\/ notify jvmti\n+    if (JvmtiExport::should_post_class_load()) {\n+      JvmtiExport::post_class_load(THREAD, k);\n+    }\n+    post_class_define_event(k, loader_data);\n@@ -1429,1 +1441,0 @@\n-  post_class_define_event(k, loader_data);\n@@ -1453,1 +1464,1 @@\n-                                                       InstanceKlass* k, TRAPS) {\n+                                                       InstanceKlass* k, bool restoration, TRAPS) {\n@@ -1463,1 +1474,1 @@\n-    if (is_parallelDefine(class_loader)) {\n+    if (restoration || is_parallelDefine(class_loader)) {\n@@ -1498,1 +1509,1 @@\n-  define_instance_class(k, class_loader, THREAD);\n+  define_instance_class(k, class_loader, \/*publicize:*\/ !restoration, THREAD);\n@@ -1520,1 +1531,1 @@\n-  InstanceKlass* defined_k = find_or_define_helper(class_name, class_loader, k, THREAD);\n+  InstanceKlass* defined_k = find_or_define_helper(class_name, class_loader, k, false, THREAD);\n@@ -1533,0 +1544,49 @@\n+\/\/ Portable CRaC support.\n+InstanceKlass *SystemDictionary::find_or_define_recreated_class(InstanceKlass *k, TRAPS) {\n+  JavaThread* const thread = JavaThread::current();\n+  ClassLoaderData* const loader_data = k->class_loader_data();\n+  guarantee(!java_lang_ClassLoader::is_reflection_class_loader(loader_data->class_loader()),\n+            \"defining class loader must be non-reflectional\");\n+\n+  if (k->is_hidden()) {\n+    if (loader_data->has_class_mirror_holder()) {\n+      precond(k->is_non_strong_hidden());\n+      loader_data->initialize_holder(Handle(thread, k->java_mirror()));\n+    }\n+    k->add_to_hierarchy(thread);\n+    \/\/ Will be linked later, or it can be not dumped as linked if there was a\n+    \/\/ linkage error and the class wasn't deallocated\n+    return k;\n+  }\n+\n+  const Handle loader(thread, loader_data->class_loader());\n+  if (is_parallelCapable(loader)) {\n+    InstanceKlass* const defined = SystemDictionary::find_or_define_helper(k->name(), loader, k, \/*restoration:*\/ true, THREAD);\n+    if (HAS_PENDING_EXCEPTION) {\n+      precond(defined == nullptr);\n+      loader_data->add_to_deallocate_list(k);\n+      return nullptr;\n+    }\n+    postcond(defined != nullptr);\n+    return defined;\n+  }\n+\n+  ObjectLocker ol(loader, thread);\n+\n+  {\n+    MutexLocker mu(THREAD, SystemDictionary_lock);\n+    InstanceKlass* const predefined = loader_data->dictionary()->find_class(thread, k->name());\n+    if (predefined != nullptr) {\n+      return predefined;\n+    }\n+  }\n+\n+  SystemDictionary::define_instance_class(k, loader, \/*publicize:*\/ false, THREAD);\n+  if (HAS_PENDING_EXCEPTION) {\n+    loader_data->add_to_deallocate_list(k);\n+    return nullptr;\n+  }\n+\n+  return k;\n+}\n+\n","filename":"src\/hotspot\/share\/classfile\/systemDictionary.cpp","additions":82,"deletions":22,"binary":false,"changes":104,"status":"modified"},{"patch":"@@ -178,0 +178,8 @@\n+  \/\/ Portable CRaC support: finds a class with the same name and class loader or\n+  \/\/ defines the provided class. Returns the defined class, deallocating the\n+  \/\/ provided one if it was not defined.\n+  static InstanceKlass* find_or_define_recreated_class(InstanceKlass* k, TRAPS);\n+\n+  \/\/ Records the non-defining class loader as an initiating loader of the class.\n+  static void record_initiating_loader(InstanceKlass* loaded_class, Handle class_loader, TRAPS);\n+\n@@ -305,1 +313,1 @@\n-  static void define_instance_class(InstanceKlass* k, Handle class_loader, TRAPS);\n+  static void define_instance_class(InstanceKlass* k, Handle class_loader, bool publicize, TRAPS);\n@@ -308,1 +316,1 @@\n-                                              InstanceKlass* k, TRAPS);\n+                                              InstanceKlass* k, bool restoration, TRAPS);\n","filename":"src\/hotspot\/share\/classfile\/systemDictionary.hpp","additions":10,"deletions":2,"binary":false,"changes":12,"status":"modified"},{"patch":"@@ -1450,1 +1450,2 @@\n-    InstanceKlass* nest_host = caller_ik->nest_host_not_null();\n+    InstanceKlass* nest_host = caller_ik->nest_host_noresolve();\n+    assert(nest_host != nullptr, \"must be\");\n","filename":"src\/hotspot\/share\/classfile\/systemDictionaryShared.cpp","additions":2,"deletions":1,"binary":false,"changes":3,"status":"modified"},{"patch":"@@ -533,0 +533,5 @@\n+  template(hash_name,                                 \"<hash>\")                                   \\\n+  template(is_interned_name,                          \"<is_interned>\")                            \\\n+  template(internal_name_name,                        \"<name>\")                                   \\\n+  template(internal_signature_name,                   \"<signature>\")                              \\\n+  template(internal_kind_name,                        \"<kind>\")                                   \\\n@@ -662,0 +667,1 @@\n+  template(arraylist_signature,                       \"Ljava\/util\/ArrayList;\")                                    \\\n@@ -795,0 +801,1 @@\n+  template(checkpointRestore0_name,                \"checkpointRestore0\")                                          \\\n","filename":"src\/hotspot\/share\/classfile\/vmSymbols.hpp","additions":7,"deletions":0,"binary":false,"changes":7,"status":"modified"},{"patch":"@@ -496,0 +496,23 @@\n+\/\/----------------------------------------------------------------------------------------------------\n+\/\/ Implementation of RestoreBlob\n+\n+RestoreBlob::RestoreBlob(CodeBuffer* cb, int size, int frame_size)\n+: SingletonBlob(\"RestoreBlob\", cb, sizeof(RestoreBlob), size, frame_size, nullptr)\n+{}\n+\n+\n+RestoreBlob* RestoreBlob::create(CodeBuffer* cb, int frame_size) {\n+  RestoreBlob* blob = nullptr;\n+  unsigned int size = CodeBlob::allocation_size(cb, sizeof(RestoreBlob));\n+  ThreadInVMfromUnknown __tiv;  \/\/ get to VM state in case we block on CodeCache_lock\n+  {\n+    MutexLocker mu(CodeCache_lock, Mutex::_no_safepoint_check_flag);\n+    blob = new (size) RestoreBlob(cb, size, frame_size);\n+  }\n+\n+  trace_new_stub(blob, \"RestoreBlob\");\n+\n+  return blob;\n+}\n+\n+\n","filename":"src\/hotspot\/share\/code\/codeBlob.cpp","additions":23,"deletions":0,"binary":false,"changes":23,"status":"modified"},{"patch":"@@ -151,0 +151,1 @@\n+  virtual bool is_restore_stub() const                { return false; }\n@@ -352,1 +353,1 @@\n-\n+\/\/ RuntimeBlob(name, cb, header_size, size, CodeOffsets::frame_never_safe, frame_size, oop_maps)\n@@ -645,0 +646,17 @@\n+\/\/----------------------------------------------------------------------------------------------------\n+\/\/ RestoreBlob\n+\n+class RestoreBlob: public SingletonBlob {\n+ private:\n+  \/\/ Creation support\n+  RestoreBlob(CodeBuffer* cb, int size, int frame_size);\n+\n+ public:\n+  \/\/ Creation\n+  static RestoreBlob* create(CodeBuffer* cb, int frame_size);\n+\n+  \/\/ Typing\n+  bool is_restore_stub() const override          { return true; }\n+};\n+\n+\n","filename":"src\/hotspot\/share\/code\/codeBlob.hpp","additions":19,"deletions":1,"binary":false,"changes":20,"status":"modified"},{"patch":"@@ -65,0 +65,8 @@\n+#ifdef ASSERT\n+bool DependencyContext::is_unused() const {\n+  const nmethodBucket* dependencies = Atomic::load(_dependency_context_addr);\n+  const uint64_t last_cleanup = Atomic::load(_last_cleanup_addr);\n+  return dependencies == nullptr && last_cleanup == 0;\n+}\n+#endif \/\/ ASSERT\n+\n","filename":"src\/hotspot\/share\/code\/dependencyContext.cpp","additions":8,"deletions":0,"binary":false,"changes":8,"status":"modified"},{"patch":"@@ -132,0 +132,4 @@\n+#ifdef ASSERT\n+  bool is_unused() const;\n+#endif \/\/ ASSERT\n+\n","filename":"src\/hotspot\/share\/code\/dependencyContext.hpp","additions":4,"deletions":0,"binary":false,"changes":4,"status":"modified"},{"patch":"@@ -1529,1 +1529,7 @@\n-  BytecodeTracer::trace_interpreter(mh, last_frame.bcp(), tos, tos2);\n+  if (!TraceOperands) {\n+    BytecodeTracer::trace_interpreter(mh, last_frame.bcp(), tos, tos2);\n+  } else {\n+    ttyLocker ttyl;\n+    BytecodeTracer::trace_interpreter(mh, last_frame.bcp(), tos, tos2);\n+    last_frame.get_frame().interpreter_frame_print_values_on(tty);\n+  }\n","filename":"src\/hotspot\/share\/interpreter\/interpreterRuntime.cpp","additions":7,"deletions":1,"binary":false,"changes":8,"status":"modified"},{"patch":"@@ -1886,0 +1886,8 @@\n+\n+Method* LinkResolver::resolve_intrinsic_polymorphic_method(Klass *klass, Symbol *name, Symbol *signature, TRAPS) {\n+  precond(MethodHandles::is_signature_polymorphic_intrinsic_name(klass, name));\n+  const LinkInfo link_info(klass, name, signature);\n+  Method* const result = lookup_polymorphic_method(link_info, nullptr, CHECK_NULL);\n+  postcond(result != nullptr);\n+  return result;\n+}\n","filename":"src\/hotspot\/share\/interpreter\/linkResolver.cpp","additions":8,"deletions":0,"binary":false,"changes":8,"status":"modified"},{"patch":"@@ -360,0 +360,5 @@\n+\n+  \/\/ Portable CRaC support: handle signature polymorphic resolution cases that\n+  \/\/ does not require calling into Java, omit JFR events\n+  static Method* resolve_intrinsic_polymorphic_method(Klass* klass, Symbol* name,\n+                                                      Symbol* signature, TRAPS);\n","filename":"src\/hotspot\/share\/interpreter\/linkResolver.hpp","additions":5,"deletions":0,"binary":false,"changes":5,"status":"modified"},{"patch":"@@ -40,1 +40,1 @@\n-  static const size_t MaxSelections = 256;\n+  static const size_t MaxSelections = 300;\n","filename":"src\/hotspot\/share\/logging\/logSelectionList.hpp","additions":1,"deletions":1,"binary":false,"changes":2,"status":"modified"},{"patch":"@@ -233,0 +233,1 @@\n+  int flags() const { return _flags.as_int(); }\n","filename":"src\/hotspot\/share\/oops\/constMethod.hpp","additions":1,"deletions":0,"binary":false,"changes":1,"status":"modified"},{"patch":"@@ -75,0 +75,1 @@\n+  explicit ConstMethodFlags(int flags) : _flags(flags) {}\n","filename":"src\/hotspot\/share\/oops\/constMethodFlags.hpp","additions":1,"deletions":0,"binary":false,"changes":1,"status":"modified"},{"patch":"@@ -277,0 +277,26 @@\n+\/\/ Portable CRaC support:\n+Klass *ConstantPool::klass_at_put_and_get(int class_index, Klass* k) {\n+  assert(k != nullptr, \"must be valid klass\");\n+  CPKlassSlot kslot = klass_slot_at(class_index);\n+  int resolved_klass_index = kslot.resolved_klass_index();\n+  Klass** adr = resolved_klasses()->adr_at(resolved_klass_index);\n+  Klass* old_k = Atomic::cmpxchg(adr, static_cast<Klass*>(nullptr), k);\n+\n+  \/\/ The interpreter assumes when the tag is stored, the klass is resolved\n+  \/\/ and the Klass* stored in _resolved_klasses is non-null, so we need\n+  \/\/ hardware store ordering here.\n+  \/\/ We also need to CAS to not overwrite an error from a racing thread.\n+  jbyte old_tag = Atomic::cmpxchg((jbyte*)tag_addr_at(class_index),\n+                                  (jbyte)JVM_CONSTANT_UnresolvedClass,\n+                                  (jbyte)JVM_CONSTANT_Class);\n+  switch (old_tag) {\n+    case JVM_CONSTANT_UnresolvedClassInError:\n+      \/\/ Remove klass.\n+      resolved_klasses()->at_put(resolved_klass_index, nullptr);\n+      return nullptr;\n+    case JVM_CONSTANT_UnresolvedClass: return k;\n+    case JVM_CONSTANT_Class:           return old_k;\n+    default: ShouldNotReachHere();     return nullptr;\n+  }\n+}\n+\n","filename":"src\/hotspot\/share\/oops\/constantPool.cpp","additions":26,"deletions":0,"binary":false,"changes":26,"status":"modified"},{"patch":"@@ -85,0 +85,1 @@\n+  friend class CracInstanceClassDumpParser;\n@@ -280,0 +281,2 @@\n+  \/\/ Portable CRaC support:\n+  Klass* klass_at_put_and_get(int class_index, Klass* k);\n@@ -416,1 +419,1 @@\n-  jint int_at(int cp_index) {\n+  jint int_at(int cp_index) const {\n@@ -421,1 +424,1 @@\n-  jlong long_at(int cp_index) {\n+  jlong long_at(int cp_index) const {\n@@ -428,1 +431,1 @@\n-  jfloat float_at(int cp_index) {\n+  jfloat float_at(int cp_index) const {\n@@ -433,1 +436,1 @@\n-  jdouble double_at(int cp_index) {\n+  jdouble double_at(int cp_index) const {\n","filename":"src\/hotspot\/share\/oops\/constantPool.hpp","additions":7,"deletions":4,"binary":false,"changes":11,"status":"modified"},{"patch":"@@ -692,0 +692,17 @@\n+ConstantPoolCache* ConstantPoolCache::allocate_uninitialized(ClassLoaderData* loader_data,\n+                                                             int length,\n+                                                             int resolved_indy_entries_length,\n+                                                             int resolved_field_entries_length,\n+                                                             TRAPS) {\n+  const int size = ConstantPoolCache::size(length);\n+\n+  \/\/ Allocate resolved entry arrays leaving them uninitialized\n+  Array<ResolvedFieldEntry>* resolved_field_entries = resolved_field_entries_length == 0 ? nullptr\n+          : MetadataFactory::new_array<ResolvedFieldEntry>(loader_data, resolved_field_entries_length, CHECK_NULL);\n+  Array<ResolvedIndyEntry>* resolved_indy_entries = resolved_indy_entries_length == 0 ? nullptr\n+          : MetadataFactory::new_array<ResolvedIndyEntry>(loader_data, resolved_indy_entries_length, CHECK_NULL);\n+\n+  return new (loader_data, size, MetaspaceObj::ConstantPoolCacheType, THREAD)\n+              ConstantPoolCache(length, resolved_indy_entries, resolved_field_entries);\n+}\n+\n","filename":"src\/hotspot\/share\/oops\/cpCache.cpp","additions":17,"deletions":0,"binary":false,"changes":17,"status":"modified"},{"patch":"@@ -138,0 +138,2 @@\n+  friend class CracInstanceClassDumpParser;\n+  friend struct CracClassStateRestorer;\n@@ -326,0 +328,1 @@\n+  intx      f2_ord() const                       { return _f2; }\n@@ -424,0 +427,5 @@\n+  ConstantPoolCache(int length,\n+                    Array<ResolvedIndyEntry>* indy_entries,\n+                    Array<ResolvedFieldEntry>* field_entries) :\n+                    _length(length), _constant_pool(nullptr), _gc_epoch(0),\n+                    _resolved_indy_entries(indy_entries), _resolved_field_entries(field_entries) {};\n@@ -435,0 +443,5 @@\n+  static ConstantPoolCache* allocate_uninitialized(ClassLoaderData* loader_data,\n+                                                   int length,\n+                                                   int resolved_indy_entries_length,\n+                                                   int resolved_field_entries_length,\n+                                                   TRAPS);\n","filename":"src\/hotspot\/share\/oops\/cpCache.hpp","additions":13,"deletions":0,"binary":false,"changes":13,"status":"modified"},{"patch":"@@ -118,1 +118,1 @@\n-  return _resolved_field_entries->length();\n+  return _resolved_field_entries != nullptr ? _resolved_field_entries->length() : 0;\n@@ -126,1 +126,1 @@\n-  return _resolved_indy_entries->length();\n+  return _resolved_indy_entries != nullptr ? _resolved_indy_entries->length() : 0;\n","filename":"src\/hotspot\/share\/oops\/cpCache.inline.hpp","additions":2,"deletions":2,"binary":false,"changes":4,"status":"modified"},{"patch":"@@ -51,1 +51,1 @@\n-Array<u1>* FieldInfoStream::create_FieldInfoStream(GrowableArray<FieldInfo>* fields, int java_fields, int injected_fields,\n+Array<u1>* FieldInfoStream::create_FieldInfoStream(GrowableArrayView<FieldInfo>* fields, int java_fields, int injected_fields,\n","filename":"src\/hotspot\/share\/oops\/fieldInfo.cpp","additions":1,"deletions":1,"binary":false,"changes":2,"status":"modified"},{"patch":"@@ -30,0 +30,1 @@\n+#include \"utilities\/growableArray.hpp\"\n@@ -278,1 +279,1 @@\n-  static Array<u1>* create_FieldInfoStream(GrowableArray<FieldInfo>* fields, int java_fields, int injected_fields,\n+  static Array<u1>* create_FieldInfoStream(GrowableArrayView<FieldInfo>* fields, int java_fields, int injected_fields,\n","filename":"src\/hotspot\/share\/oops\/fieldInfo.hpp","additions":2,"deletions":1,"binary":false,"changes":3,"status":"modified"},{"patch":"@@ -29,0 +29,1 @@\n+#include \"utilities\/accessFlags.hpp\"\n@@ -31,2 +32,0 @@\n-class ClassFileParser;\n-\n@@ -46,1 +45,2 @@\n-  InstanceClassLoaderKlass(const ClassFileParser& parser) : InstanceKlass(parser, Kind) {}\n+  InstanceClassLoaderKlass(AccessFlags access_flags, const InstanceKlassSizes& sizes)\n+      : InstanceKlass(access_flags, sizes, Kind) {}\n","filename":"src\/hotspot\/share\/oops\/instanceClassLoaderKlass.hpp","additions":3,"deletions":3,"binary":false,"changes":6,"status":"modified"},{"patch":"@@ -151,1 +151,1 @@\n-                                   const ClassFileParser& parser) {\n+                                   const Klass* super_klass) {\n@@ -159,1 +159,0 @@\n-    const Klass* const super_klass = parser.super_klass();\n@@ -437,9 +436,10 @@\n-InstanceKlass* InstanceKlass::allocate_instance_klass(const ClassFileParser& parser, TRAPS) {\n-  const int size = InstanceKlass::size(parser.vtable_size(),\n-                                       parser.itable_size(),\n-                                       nonstatic_oop_map_size(parser.total_oop_map_count()),\n-                                       parser.is_interface());\n-\n-  const Symbol* const class_name = parser.class_name();\n-  assert(class_name != nullptr, \"invariant\");\n-  ClassLoaderData* loader_data = parser.loader_data();\n+InstanceKlass* InstanceKlass::allocate_instance_klass(ClassLoaderData* loader_data,\n+                                                      const Symbol* name,\n+                                                      const InstanceKlass* super,\n+                                                      AccessFlags access_flags,\n+                                                      const InstanceKlassSizes& sizes,\n+                                                      TRAPS) {\n+  const int size = InstanceKlass::size(sizes.vtable_size,\n+                                       sizes.itable_size,\n+                                       nonstatic_oop_map_size(sizes.total_oop_map_count),\n+                                       access_flags.is_interface());\n@@ -447,0 +447,1 @@\n+  assert(name != nullptr, \"invariant\");\n@@ -451,1 +452,2 @@\n-  if (parser.is_instance_ref_klass()) {\n+  const ReferenceType reference_type = InstanceRefKlass::determine_reference_type(super, name);\n+  if (reference_type != ReferenceType::REF_NONE) {\n@@ -453,2 +455,2 @@\n-    ik = new (loader_data, size, THREAD) InstanceRefKlass(parser);\n-  } else if (class_name == vmSymbols::java_lang_Class()) {\n+    ik = new (loader_data, size, THREAD) InstanceRefKlass(access_flags, sizes, reference_type);\n+  } else if (name == vmSymbols::java_lang_Class()) {\n@@ -456,2 +458,2 @@\n-    ik = new (loader_data, size, THREAD) InstanceMirrorKlass(parser);\n-  } else if (is_stack_chunk_class(class_name, loader_data)) {\n+    ik = new (loader_data, size, THREAD) InstanceMirrorKlass(access_flags, sizes);\n+  } else if (is_stack_chunk_class(name, loader_data)) {\n@@ -459,2 +461,2 @@\n-    ik = new (loader_data, size, THREAD) InstanceStackChunkKlass(parser);\n-  } else if (is_class_loader(class_name, parser)) {\n+    ik = new (loader_data, size, THREAD) InstanceStackChunkKlass(access_flags, sizes);\n+  } else if (is_class_loader(name, super)) {\n@@ -462,1 +464,1 @@\n-    ik = new (loader_data, size, THREAD) InstanceClassLoaderKlass(parser);\n+    ik = new (loader_data, size, THREAD) InstanceClassLoaderKlass(access_flags, sizes);\n@@ -465,1 +467,1 @@\n-    ik = new (loader_data, size, THREAD) InstanceKlass(parser);\n+    ik = new (loader_data, size, THREAD) InstanceKlass(access_flags, sizes);\n@@ -503,1 +505,1 @@\n-InstanceKlass::InstanceKlass(const ClassFileParser& parser, KlassKind kind, ReferenceType reference_type) :\n+InstanceKlass::InstanceKlass(AccessFlags access_flags, const InstanceKlassSizes& sizes, KlassKind kind, ReferenceType reference_type) :\n@@ -509,3 +511,3 @@\n-  _static_field_size(parser.static_field_size()),\n-  _nonstatic_oop_map_size(nonstatic_oop_map_size(parser.total_oop_map_count())),\n-  _itable_len(parser.itable_size()),\n+  _static_field_size(sizes.static_field_size),\n+  _nonstatic_oop_map_size(nonstatic_oop_map_size(sizes.total_oop_map_count)),\n+  _itable_len(sizes.itable_size),\n@@ -518,5 +520,3 @@\n-  set_vtable_length(parser.vtable_size());\n-  set_access_flags(parser.access_flags());\n-  if (parser.is_hidden()) set_is_hidden();\n-  set_layout_helper(Klass::instance_layout_helper(parser.layout_size(),\n-                                                    false));\n+  set_vtable_length(sizes.vtable_size);\n+  set_access_flags(access_flags);\n+  set_layout_helper(Klass::instance_layout_helper(sizes.layout_size, false));\n@@ -526,1 +526,1 @@\n-  assert(size_helper() == parser.layout_size(), \"incorrect size_helper?\");\n+  assert(size_helper() == sizes.layout_size, \"incorrect size_helper?\");\n@@ -773,2 +773,9 @@\n-  \/\/ Another thread is linking this class, wait.\n-  while (is_being_linked() && !is_init_thread(current)) {\n+  \/\/ Another thread is linking or restoring this class, wait.\n+  if (is_being_restored() && log_is_enabled(Warning, crac, class)) {\n+    \/\/ TODO remove this log when deadlock-prone MethodType interning is fixed\n+    ResourceMark rm;\n+    log_warning(crac, class)(\"Linking thread '%s' is waiting for %s to be restored\",\n+                             current->name(), external_name());\n+  }\n+  assert(!(is_being_restored() && is_init_thread(current)), \"restoring thread should not call this\");\n+  while (is_being_restored() || (is_being_linked() && !is_init_thread(current))) {\n@@ -900,27 +907,2 @@\n-      \/\/ relocate jsrs and link methods after they are all rewritten\n-      link_methods(CHECK_false);\n-\n-      \/\/ Initialize the vtable and interface table after\n-      \/\/ methods have been rewritten since rewrite may\n-      \/\/ fabricate new Method*s.\n-      \/\/ also does loader constraint checking\n-      \/\/\n-      \/\/ initialize_vtable and initialize_itable need to be rerun\n-      \/\/ for a shared class if\n-      \/\/ 1) the class is loaded by custom class loader or\n-      \/\/ 2) the class is loaded by built-in class loader but failed to add archived loader constraints or\n-      \/\/ 3) the class was not verified during dump time\n-      bool need_init_table = true;\n-      if (is_shared() && verified_at_dump_time() &&\n-          SystemDictionaryShared::check_linking_constraints(THREAD, this)) {\n-        need_init_table = false;\n-      }\n-      if (need_init_table) {\n-        vtable().initialize_vtable_and_check_constraints(CHECK_false);\n-        itable().initialize_itable_and_check_constraints(CHECK_false);\n-      }\n-#ifdef ASSERT\n-      vtable().verify(tty, true);\n-      \/\/ In case itable verification is ever added.\n-      \/\/ itable().verify(tty, true);\n-#endif\n+      finish_linking(true, CHECK_false);\n+\n@@ -949,0 +931,38 @@\n+void InstanceKlass::finish_linking(bool check_vtable_constraints, TRAPS) {\n+  precond(is_rewritten());\n+\n+  \/\/ relocate jsrs and link methods after they are all rewritten\n+  link_methods(CHECK);\n+\n+  \/\/ Initialize the vtable and interface table after\n+  \/\/ methods have been rewritten since rewrite may\n+  \/\/ fabricate new Method*s.\n+  \/\/ also does loader constraint checking\n+  \/\/\n+  \/\/ initialize_vtable and initialize_itable need to be rerun\n+  \/\/ for a shared class if\n+  \/\/ 1) the class is loaded by custom class loader or\n+  \/\/ 2) the class is loaded by built-in class loader but failed to add archived loader constraints or\n+  \/\/ 3) the class was not verified during dump time\n+  bool need_init_table = true;\n+  if (is_shared() && verified_at_dump_time() &&\n+      SystemDictionaryShared::check_linking_constraints(THREAD, this)) {\n+    need_init_table = false;\n+  }\n+  if (need_init_table) {\n+    static constexpr bool is_debug = DEBUG_ONLY(true) NOT_DEBUG(false);\n+    if (check_vtable_constraints || is_debug) {\n+      vtable().initialize_vtable_and_check_constraints(CHECK);\n+      itable().initialize_itable_and_check_constraints(CHECK);\n+    } else {\n+      vtable().initialize_vtable();\n+      itable().initialize_itable();\n+    }\n+  }\n+#ifdef ASSERT\n+  vtable().verify(tty, true);\n+  \/\/ In case itable verification is ever added.\n+  \/\/ itable().verify(tty, true);\n+#endif\n+}\n+\n@@ -985,0 +1005,11 @@\n+void InstanceKlass::put_initializetion_error(JavaThread* current, Handle init_error) {\n+  MutexLocker ml(current, ClassInitError_lock);\n+  OopHandle elem = OopHandle(Universe::vm_global(), init_error());\n+  bool created;\n+  if (_initialization_error_table == nullptr) {\n+    _initialization_error_table = new (mtClass) InitializationErrorTable();\n+  }\n+  _initialization_error_table->put_if_absent(this, elem, &created);\n+  assert(created, \"Initialization is single threaded\");\n+}\n+\n@@ -1007,8 +1038,1 @@\n-  MutexLocker ml(current, ClassInitError_lock);\n-  OopHandle elem = OopHandle(Universe::vm_global(), init_error());\n-  bool created;\n-  if (_initialization_error_table == nullptr) {\n-    _initialization_error_table = new (mtClass) InitializationErrorTable();\n-  }\n-  _initialization_error_table->put_if_absent(this, elem, &created);\n-  assert(created, \"Initialization is single threaded\");\n+  put_initializetion_error(current, init_error);\n@@ -1027,0 +1051,9 @@\n+oop InstanceKlass::get_initialization_error() {\n+  assert_locked_or_safepoint(ClassInitError_lock);\n+  if (_initialization_error_table == nullptr) {\n+    return nullptr;\n+  }\n+  OopHandle* h = _initialization_error_table->get(this);\n+  return (h != nullptr) ? h->resolve() : nullptr;\n+}\n+\n@@ -1066,2 +1099,9 @@\n-    \/\/ Step 2\n-    while (is_being_initialized() && !is_init_thread(jt)) {\n+    \/\/ Step 2 (ammended with portable CRaC support)\n+    if (is_being_restored() && log_is_enabled(Warning, crac, class)) {\n+      \/\/ TODO remove this log when deadlock-prone MethodType interning is fixed\n+      ResourceMark rm;\n+      log_warning(crac, class)(\"Initializing thread '%s' is waiting for %s to be restored\",\n+                               jt->name(), external_name());\n+      assert(!(is_being_restored() && is_init_thread(jt)), \"restoring thread should not call this\");\n+    }\n+    while (is_being_restored() || (is_being_initialized() && !is_init_thread(jt))) {\n@@ -1227,0 +1267,29 @@\n+\/\/ Like the above but to set being_linked -> linked -> being_initialized in a\n+\/\/ single critical section.\n+void InstanceKlass::set_linked_to_be_initialized_state_and_notify(JavaThread* current) {\n+  MonitorLocker ml(current, _init_monitor);\n+  precond(_init_state == being_linked);\n+\n+  if (UseVtableBasedCHA && Universe::is_fully_initialized()) {\n+    DeoptimizationScope deopt_scope;\n+    {\n+      \/\/ Now mark all code that assumes the class is not linked.\n+      \/\/ Set state under the Compile_lock also.\n+      MutexLocker ml(current, Compile_lock);\n+\n+      set_init_thread(nullptr); \/\/ reset _init_thread before changing _init_state\n+      set_init_state(being_initialized);\n+      set_init_thread(current); \/\/ set it back\n+\n+      CodeCache::mark_dependents_on(&deopt_scope, this);\n+    }\n+    \/\/ Perform the deopt handshake outside Compile_lock.\n+    deopt_scope.deoptimize_marked();\n+  } else {\n+    set_init_thread(nullptr); \/\/ reset _init_thread before changing _init_state\n+    set_init_state(being_initialized);\n+    set_init_thread(current); \/\/ set it back\n+  }\n+  ml.notify_all();\n+}\n+\n@@ -2883,1 +2952,6 @@\n-void InstanceKlass::set_source_debug_extension(const char* array, int length) {\n+void InstanceKlass::set_source_debug_extension(const char* array) {\n+  precond(_source_debug_extension == nullptr);\n+  _source_debug_extension = array;\n+}\n+\n+void InstanceKlass::copy_source_debug_extension(const char* array, int length) {\n@@ -3531,0 +3605,4 @@\n+const char* InstanceKlass::state_name(ClassState state) {\n+  return state_names[state];\n+}\n+\n@@ -3532,1 +3610,1 @@\n-  return state_names[init_state()];\n+  return state_name(init_state());\n","filename":"src\/hotspot\/share\/oops\/instanceKlass.cpp","additions":149,"deletions":71,"binary":false,"changes":220,"status":"modified"},{"patch":"@@ -79,0 +79,9 @@\n+\/\/ Used when allocating a class.\n+struct InstanceKlassSizes {\n+  int vtable_size;                  \/\/ Vtable size in words\n+  int itable_size;                  \/\/ Itable size in words\n+  int layout_size;                  \/\/ Size of instances in words\n+  int static_field_size;            \/\/ Size of static fields in words\n+  unsigned int total_oop_map_count; \/\/ Number of OopMapBlocks used\n+};\n+\n@@ -140,0 +149,1 @@\n+  friend struct CracClassStateRestorer;\n@@ -145,1 +155,1 @@\n-  InstanceKlass(const ClassFileParser& parser, KlassKind kind = Kind, ReferenceType reference_type = REF_NONE);\n+  InstanceKlass(AccessFlags access_flags, const InstanceKlassSizes& sizes, KlassKind kind = Kind, ReferenceType reference_type = REF_NONE);\n@@ -161,0 +171,1 @@\n+  static const char* state_name(ClassState state);\n@@ -162,2 +173,6 @@\n- private:\n-  static InstanceKlass* allocate_instance_klass(const ClassFileParser& parser, TRAPS);\n+  static InstanceKlass* allocate_instance_klass(ClassLoaderData* loader_data,\n+                                                const Symbol* class_name,\n+                                                const InstanceKlass* super,\n+                                                AccessFlags access_flags,\n+                                                const InstanceKlassSizes& sizes,\n+                                                TRAPS);\n@@ -307,0 +322,3 @@\n+  const InstanceKlassFlags &internal_flags() const { return _misc_flags; }\n+  void set_internal_flags(InstanceKlassFlags flags) { _misc_flags = flags; }\n+\n@@ -438,5 +456,0 @@\n-  \/\/ Call this only if you know that the nest host has been initialized.\n-  InstanceKlass* nest_host_not_null() {\n-    assert(_nest_host != nullptr, \"must be\");\n-    return _nest_host;\n-  }\n@@ -450,0 +463,2 @@\n+  \/\/ Returns nest-host class or null if it has not been resolved yet.\n+  InstanceKlass* nest_host_noresolve() const { return _nest_host; }\n@@ -511,0 +526,1 @@\n+  oop get_initialization_error();\n@@ -544,0 +560,1 @@\n+  void finish_linking(bool check_vtable_constraints, TRAPS);\n@@ -664,1 +681,2 @@\n-  void set_source_debug_extension(const char* array, int length);\n+  void set_source_debug_extension(const char* array);\n+  void copy_source_debug_extension(const char* array, int length);\n@@ -709,0 +727,3 @@\n+  bool is_being_restored() const { return _misc_flags.is_being_restored(); }\n+  void set_is_being_restored(bool value) { _misc_flags.set_is_being_restored(value); }\n+\n@@ -832,0 +853,1 @@\n+  void set_linked_to_be_initialized_state_and_notify(JavaThread* current);\n@@ -1096,0 +1118,1 @@\n+  void put_initializetion_error(JavaThread* current, Handle init_error);\n","filename":"src\/hotspot\/share\/oops\/instanceKlass.hpp","additions":32,"deletions":9,"binary":false,"changes":41,"status":"modified"},{"patch":"@@ -70,0 +70,14 @@\n+\n+InstanceKlassFlags InstanceKlassFlags::drop_nonportable_flags() const {\n+  \/\/ - Clear flags dependent on CDS archive dumping -- they need to be set\n+  \/\/   when restoring based on the VM options\n+  const u2 cds_bits = shared_loader_type_bits() | _misc_shared_loading_failed;\n+  return {checked_cast<u2>(_flags & ~cds_bits), _status};\n+}\n+\n+#else \/\/ INCLUDE_CDS\n+\n+InstanceKlassFlags InstanceKlassFlags::drop_nonportable_flags() const {\n+  return *this;\n+}\n+\n","filename":"src\/hotspot\/share\/oops\/instanceKlassFlags.cpp","additions":14,"deletions":0,"binary":false,"changes":14,"status":"modified"},{"patch":"@@ -72,0 +72,1 @@\n+    status(is_being_restored                 , 1 << 5) \/* class is being restored by CRaC *\/ \\\n@@ -93,0 +94,1 @@\n+  InstanceKlassFlags(u2 flags, u1 status) : _flags(flags), _status(status) {}\n@@ -95,0 +97,1 @@\n+  u2 flags() const { return _flags; }\n@@ -107,1 +110,0 @@\n-\n@@ -109,1 +111,0 @@\n-\n@@ -111,0 +112,4 @@\n+\n+  \/\/ Returns a copy with VM-options-depended flags cleared.\n+  InstanceKlassFlags drop_nonportable_flags() const;\n+\n@@ -114,0 +119,1 @@\n+  u1 status() const { return _status; }\n","filename":"src\/hotspot\/share\/oops\/instanceKlassFlags.hpp","additions":8,"deletions":2,"binary":false,"changes":10,"status":"modified"},{"patch":"@@ -31,0 +31,1 @@\n+#include \"utilities\/accessFlags.hpp\"\n@@ -33,2 +34,0 @@\n-class ClassFileParser;\n-\n@@ -53,1 +52,2 @@\n-  InstanceMirrorKlass(const ClassFileParser& parser) : InstanceKlass(parser, Kind) {}\n+  InstanceMirrorKlass(AccessFlags access_flags, const InstanceKlassSizes& sizes)\n+      : InstanceKlass(access_flags, sizes, Kind) {}\n","filename":"src\/hotspot\/share\/oops\/instanceMirrorKlass.hpp","additions":3,"deletions":3,"binary":false,"changes":6,"status":"modified"},{"patch":"@@ -48,3 +48,11 @@\n-static ReferenceType determine_reference_type(const ClassFileParser& parser) {\n-  const ReferenceType rt = parser.super_reference_type();\n-  if (rt != REF_NONE) {\n+\/\/ Only the subclasses of j.l.r.Reference are InstanceRefKlass.\n+\/\/ j.l.r.Reference itself is InstanceKlass because InstanceRefKlass denotes a\n+\/\/ klass requiring special treatment in ref-processing. The abstract\n+\/\/ j.l.r.Reference cannot be instantiated so doesn't partake in\n+\/\/ ref-processing.\n+ReferenceType InstanceRefKlass::determine_reference_type(const InstanceKlass *super_class, const Symbol* const class_name) {\n+  if (super_class == nullptr) {\n+    \/\/ java.lang.Object is not an InstanceRefKlass\n+    return REF_NONE;\n+  }\n+  if (super_class->reference_type() != REF_NONE) {\n@@ -52,1 +60,1 @@\n-    return rt;\n+    return super_class->reference_type();\n@@ -54,4 +62,5 @@\n-\n-  \/\/ Bootstrapping: this is one of the direct subclasses of java.lang.ref.Reference\n-  const Symbol* const name = parser.class_name();\n-  return reference_subclass_name_to_type(name);\n+  if (super_class->name() == vmSymbols::java_lang_ref_Reference()) {\n+    \/\/ Bootstrapping: this is one of the direct subclasses of java.lang.ref.Reference\n+    return reference_subclass_name_to_type(class_name);\n+  }\n+  return REF_NONE;\n@@ -60,3 +69,0 @@\n-InstanceRefKlass::InstanceRefKlass(const ClassFileParser& parser)\n-  : InstanceKlass(parser, Kind, determine_reference_type(parser)) {}\n-\n","filename":"src\/hotspot\/share\/oops\/instanceRefKlass.cpp","additions":17,"deletions":11,"binary":false,"changes":28,"status":"modified"},{"patch":"@@ -29,0 +29,1 @@\n+#include \"utilities\/accessFlags.hpp\"\n@@ -31,2 +32,0 @@\n-class ClassFileParser;\n-\n@@ -55,0 +54,2 @@\n+  static ReferenceType determine_reference_type(const InstanceKlass* super_class, const Symbol* class_name);\n+\n@@ -56,1 +57,2 @@\n-  InstanceRefKlass(const ClassFileParser& parser);\n+  InstanceRefKlass(AccessFlags access_flags, const InstanceKlassSizes& sizes, ReferenceType reference_type)\n+      : InstanceKlass(access_flags, sizes, Kind, reference_type) {};\n","filename":"src\/hotspot\/share\/oops\/instanceRefKlass.hpp","additions":5,"deletions":3,"binary":false,"changes":8,"status":"modified"},{"patch":"@@ -54,2 +54,2 @@\n-InstanceStackChunkKlass::InstanceStackChunkKlass(const ClassFileParser& parser)\n-  : InstanceKlass(parser, Kind) {\n+InstanceStackChunkKlass::InstanceStackChunkKlass(AccessFlags access_flags, const InstanceKlassSizes& sizes)\n+  : InstanceKlass(access_flags, sizes, Kind) {\n","filename":"src\/hotspot\/share\/oops\/instanceStackChunkKlass.cpp","additions":2,"deletions":2,"binary":false,"changes":4,"status":"modified"},{"patch":"@@ -30,0 +30,1 @@\n+#include \"utilities\/accessFlags.hpp\"\n@@ -33,1 +34,0 @@\n-class ClassFileParser;\n@@ -113,1 +113,1 @@\n-  InstanceStackChunkKlass(const ClassFileParser& parser);\n+  InstanceStackChunkKlass(AccessFlags access_flags, const InstanceKlassSizes& sizes);\n","filename":"src\/hotspot\/share\/oops\/instanceStackChunkKlass.hpp","additions":2,"deletions":2,"binary":false,"changes":4,"status":"modified"},{"patch":"@@ -671,1 +671,1 @@\n-  bool is_value_based()                 { return _access_flags.is_value_based_class(); }\n+  bool is_value_based() const           { return _access_flags.is_value_based_class(); }\n","filename":"src\/hotspot\/share\/oops\/klass.hpp","additions":1,"deletions":1,"binary":false,"changes":2,"status":"modified"},{"patch":"@@ -71,1 +71,1 @@\n-    Handle classloader, Symbol* classname, Array<InstanceKlass*>* local_interfaces) {\n+    Handle classloader, const Symbol* classname, Array<InstanceKlass*>* local_interfaces) {\n@@ -280,1 +280,1 @@\n-static bool can_be_overridden(Method* super_method, Handle targetclassloader, Symbol* targetclassname) {\n+static bool can_be_overridden(Method* super_method, Handle targetclassloader, const Symbol* targetclassname) {\n@@ -637,1 +637,1 @@\n-                                         Symbol* classname,\n+                                         const Symbol* classname,\n","filename":"src\/hotspot\/share\/oops\/klassVtable.cpp","additions":3,"deletions":3,"binary":false,"changes":6,"status":"modified"},{"patch":"@@ -82,1 +82,1 @@\n-                                                   Symbol* classname,\n+                                                   const Symbol* classname,\n@@ -118,1 +118,1 @@\n-                                     Symbol* classname,\n+                                     const Symbol* classname,\n","filename":"src\/hotspot\/share\/oops\/klassVtable.hpp","additions":2,"deletions":2,"binary":false,"changes":4,"status":"modified"},{"patch":"@@ -588,0 +588,2 @@\n+  const MethodFlags &statuses() const  { return _flags; }\n+  void set_statuses(MethodFlags flags) { _flags = flags; }\n","filename":"src\/hotspot\/share\/oops\/method.hpp","additions":2,"deletions":0,"binary":false,"changes":2,"status":"modified"},{"patch":"@@ -75,0 +75,1 @@\n+  explicit MethodFlags(jint status) : _status(status) {}\n","filename":"src\/hotspot\/share\/oops\/methodFlags.hpp","additions":1,"deletions":0,"binary":false,"changes":1,"status":"modified"},{"patch":"@@ -110,1 +110,1 @@\n-intptr_t oopDesc::slow_identity_hash() {\n+intptr_t oopDesc::slow_identity_hash(intptr_t hash) {\n@@ -113,1 +113,1 @@\n-  return ObjectSynchronizer::FastHashCode(current, this);\n+  return ObjectSynchronizer::FastHashCode(current, this, \/*suggested_hash=*\/hash);\n","filename":"src\/hotspot\/share\/oops\/oop.cpp","additions":2,"deletions":2,"binary":false,"changes":4,"status":"modified"},{"patch":"@@ -296,3 +296,4 @@\n-  \/\/ identity hash; returns the identity hash key (computes it if necessary)\n-  inline intptr_t identity_hash();\n-  intptr_t slow_identity_hash();\n+  \/\/ identity hash; returns the identity hash key (installs it if necessary)\n+  inline intptr_t identity_hash(intptr_t hash = markWord::no_hash \/*generate*\/);\n+  intptr_t slow_identity_hash(intptr_t hash);\n+  inline intptr_t read_identity_hash() const;\n","filename":"src\/hotspot\/share\/oops\/oop.hpp","additions":4,"deletions":3,"binary":false,"changes":7,"status":"modified"},{"patch":"@@ -45,0 +45,3 @@\n+#ifdef ASSERT\n+#include \"runtime\/safepoint.hpp\"\n+#endif \/\/ ASSERT\n@@ -359,1 +362,1 @@\n-intptr_t oopDesc::identity_hash() {\n+intptr_t oopDesc::identity_hash(intptr_t hash) {\n@@ -368,1 +371,1 @@\n-    return slow_identity_hash();\n+    return slow_identity_hash(hash);\n@@ -372,0 +375,16 @@\n+\/\/ Read the hash without any header modifications.\n+intptr_t oopDesc::read_identity_hash() const {\n+  markWord mrk = mark();\n+  if (mrk.is_neutral()) {\n+    return mrk.hash();\n+  }\n+  assert(SafepointSynchronize::is_at_safepoint(), \"need synchronization\");\n+  assert(!is_forwarded(), \"not used during GC\");\n+  if (mrk.has_displaced_mark_helper()) {\n+    mrk = mrk.displaced_mark_helper();\n+    assert(mrk.is_neutral(), \"must be\");\n+    return mrk.hash();\n+  }\n+  return mrk.hash();\n+}\n+\n","filename":"src\/hotspot\/share\/oops\/oop.inline.hpp","additions":21,"deletions":2,"binary":false,"changes":23,"status":"modified"},{"patch":"@@ -31,0 +31,1 @@\n+#include \"utilities\/globalDefinitions.hpp\"\n@@ -85,0 +86,1 @@\n+  u1 flags()                    const { return _flags;        }\n@@ -132,0 +134,24 @@\n+  \/\/ CRaC\n+  void fill_in_portable(u2 index, u1 tos_state, u1 flags, u1 b1, u1 b2) {\n+    assert(_cpool_index > 0, \"uninitialized\");\n+    assert(tos_state < TosState::number_of_states, \"not a ToS state\");\n+\n+    _field_index = index;\n+    _tos_state = tos_state;\n+    _flags = flags;\n+    _get_code = b1;\n+    _put_code = b2;\n+\n+    \/\/ Check b1 and b2 are expected bytecodes\n+    postcond(b1 == 0 || is_resolved(Bytecodes::cast(b1)));\n+    postcond(b2 == 0 || is_resolved(Bytecodes::cast(b2)));\n+  }\n+  void fill_in_unportable(InstanceKlass* klass) {\n+    precond(klass != nullptr);\n+    assert(_cpool_index > 0, \"uninitialized\");\n+    assert(_get_code > 0 || _put_code > 0, \"unresolved\");\n+    assert(_field_holder == nullptr, \"already filled in\");\n+    _field_holder = klass;\n+    _field_offset = klass->field_offset(_field_index);\n+  }\n+\n","filename":"src\/hotspot\/share\/oops\/resolvedFieldEntry.hpp","additions":26,"deletions":0,"binary":false,"changes":26,"status":"modified"},{"patch":"@@ -74,0 +74,1 @@\n+      num_flags\n@@ -82,0 +83,1 @@\n+  u1 flags()                     const { return _flags;                         }\n@@ -131,0 +133,9 @@\n+  \/\/ CRaC\n+  void fill_in_partial(u2 num_params, u1 return_type, bool has_appendix) {\n+    assert(_cpool_index > 0, \"uninitialized\");\n+    assert(!is_resolved(), \"already resolved\");\n+    _number_of_parameters = num_params;\n+    _return_type = return_type;\n+    set_flags(has_appendix);\n+  }\n+\n","filename":"src\/hotspot\/share\/oops\/resolvedIndyEntry.hpp","additions":11,"deletions":0,"binary":false,"changes":11,"status":"modified"},{"patch":"@@ -76,0 +76,1 @@\n+#include \"runtime\/crac.hpp\"\n@@ -3620,0 +3621,17 @@\n+    \/\/ Restore thread states\n+    \/\/ TODO this should be called by the code that creates the VM\n+    if (CRaCRestoreFrom != nullptr && crac::is_portable_mode()) {\n+      JavaThread* THREAD = JavaThread::current();\n+      crac::restore_threads(THREAD);\n+      if (HAS_PENDING_EXCEPTION) {\n+        HandleMark hm(THREAD);\n+        const Handle e(THREAD, PENDING_EXCEPTION);\n+        CLEAR_PENDING_EXCEPTION;\n+        jio_fprintf(defaultStream::error_stream(), \"Exception in restoring thread \");\n+        \/\/ If another exception occurs while printing it gets thrown away\n+        java_lang_Throwable::java_printStackTrace(e, THREAD);\n+        vm_exit(1); \/\/ To ensure the VM-creating code won't proceed with execution\n+      }\n+      vm_exit(0); \/\/ To ensure the VM-creating code won't proceed with execution\n+    }\n+\n","filename":"src\/hotspot\/share\/prims\/jni.cpp","additions":18,"deletions":0,"binary":false,"changes":18,"status":"modified"},{"patch":"@@ -4389,1 +4389,1 @@\n-  the_class->set_source_debug_extension(\n+  the_class->copy_source_debug_extension(\n","filename":"src\/hotspot\/share\/prims\/jvmtiRedefineClasses.cpp","additions":1,"deletions":1,"binary":false,"changes":2,"status":"modified"},{"patch":"@@ -146,0 +146,4 @@\n+  static bool is_signature_polymorphic_intrinsic_name(Klass* klass, Symbol* name) {\n+    const vmIntrinsics::ID iid = MethodHandles::signature_polymorphic_name_id(klass, name);\n+    return iid != vmIntrinsics::_none && is_signature_polymorphic_intrinsic(iid);\n+  }\n","filename":"src\/hotspot\/share\/prims\/methodHandles.hpp","additions":4,"deletions":0,"binary":false,"changes":4,"status":"modified"},{"patch":"@@ -2248,2 +2248,4 @@\n-bool Arguments::is_restore_option_set(const JavaVMInitArgs* args) {\n-  const char* tail;\n+bool Arguments::has_classic_restore_request(const JavaVMInitArgs* args) {\n+  bool has_restore_option = false;         \/\/ true means there is a restore request\n+  bool has_empty_crengine_option = false;  \/\/ true means it's not for a classic restore\n+\n@@ -2253,2 +2255,19 @@\n-    if (match_option(option, \"-XX:CRaCRestoreFrom\", &tail)) {\n-      return true;\n+\n+    const char* tail;\n+    if (!match_option(option, \"-XX:\", &tail)) {\n+      continue;\n+    }\n+\n+    const char* key;\n+    const char* value;\n+    get_key_value(tail, &key, &value);\n+\n+    if (strcmp(key, \"CREngine\") == 0 && strcmp(value, \"\") == 0) {\n+      has_empty_crengine_option = true;\n+    } else if (strcmp(key, \"CRaCRestoreFrom\") == 0) {\n+      has_restore_option = true;\n+    }\n+\n+    \/\/ Key was copied and needs to be freed\n+    if (key != tail) {\n+      FreeHeap(const_cast<char *>(key));\n@@ -2257,1 +2276,2 @@\n-  return false;\n+\n+  return has_restore_option && !has_empty_crengine_option;\n@@ -3993,1 +4013,1 @@\n-  if (TraceBytecodesAt != 0) {\n+  if (TraceBytecodesAt != 0 || TraceOperands) {\n","filename":"src\/hotspot\/share\/runtime\/arguments.cpp","additions":26,"deletions":6,"binary":false,"changes":32,"status":"modified"},{"patch":"@@ -538,1 +538,1 @@\n-  static bool is_restore_option_set(const JavaVMInitArgs* args);\n+  static bool has_classic_restore_request(const JavaVMInitArgs* args);\n","filename":"src\/hotspot\/share\/runtime\/arguments.hpp","additions":1,"deletions":1,"binary":false,"changes":2,"status":"modified"},{"patch":"@@ -27,0 +27,6 @@\n+#include \"classfile\/javaClasses.hpp\"\n+#include \"classfile\/symbolTable.hpp\"\n+#include \"classfile\/systemDictionary.hpp\"\n+#include \"classfile\/vmClasses.hpp\"\n+#include \"classfile\/vmSymbols.hpp\"\n+#include \"jni.h\"\n@@ -28,0 +34,1 @@\n+#include \"logging\/log.hpp\"\n@@ -30,0 +37,1 @@\n+#include \"memory\/allocation.hpp\"\n@@ -31,2 +39,5 @@\n-#include \"oops\/typeArrayOop.inline.hpp\"\n-#include \"runtime\/crac_structs.hpp\"\n+#include \"memory\/resourceArea.hpp\"\n+#include \"oops\/instanceKlass.hpp\"\n+#include \"oops\/oopsHierarchy.hpp\"\n+#include \"oops\/symbolHandle.hpp\"\n+#include \"os.inline.hpp\"\n@@ -34,0 +45,8 @@\n+#include \"runtime\/cracClassDumpParser.hpp\"\n+#include \"runtime\/cracClassDumper.hpp\"\n+#include \"runtime\/cracHeapRestorer.hpp\"\n+#include \"runtime\/cracStackDumpParser.hpp\"\n+#include \"runtime\/cracStackDumper.hpp\"\n+#include \"runtime\/cracThreadRestorer.hpp\"\n+#include \"runtime\/crac_structs.hpp\"\n+#include \"runtime\/handles.hpp\"\n@@ -35,0 +54,2 @@\n+#include \"runtime\/java.hpp\"\n+#include \"runtime\/javaThread.hpp\"\n@@ -37,0 +58,1 @@\n+#include \"runtime\/thread.hpp\"\n@@ -38,1 +60,0 @@\n-#include \"runtime\/vm_version.hpp\"\n@@ -42,0 +63,1 @@\n+#include \"utilities\/debug.hpp\"\n@@ -43,1 +65,10 @@\n-#include \"os.inline.hpp\"\n+#include \"utilities\/exceptions.hpp\"\n+#include \"utilities\/globalDefinitions.hpp\"\n+#include \"utilities\/growableArray.hpp\"\n+#include \"utilities\/heapDumpParser.hpp\"\n+#include \"utilities\/macros.hpp\"\n+\n+\/\/ Filenames used by the portable mode\n+static constexpr char PMODE_HEAP_DUMP_FILENAME[] = \"heap.hprof\";\n+static constexpr char PMODE_STACK_DUMP_FILENAME[] = \"stacks.bin\";\n+static constexpr char PMODE_CLASS_DUMP_FILENAME[] = \"classes.bin\";\n@@ -52,0 +83,3 @@\n+\/\/ Used by portable restore\n+ParsedCracStackDump *crac::_stack_dump = nullptr;\n+\n@@ -114,0 +148,73 @@\n+bool crac::is_portable_mode() {\n+  return CREngine == nullptr;\n+}\n+\n+\/\/ Checkpoint in portable mode.\n+static VM_Crac::Outcome checkpoint_portable() {\n+#if INCLUDE_SERVICES \/\/ HeapDumper is a service\n+  char path[JVM_MAXPATHLEN];\n+\n+  \/\/ Dump thread stacks\n+  os::snprintf_checked(path, sizeof(path), \"%s%s%s\",\n+                       CRaCCheckpointTo, os::file_separator(), PMODE_STACK_DUMP_FILENAME);\n+  {\n+    const CracStackDumper::Result res = CracStackDumper::dump(path);\n+    switch (res.code()) {\n+      case CracStackDumper::Result::Code::OK: break;\n+      case CracStackDumper::Result::Code::IO_ERROR: {\n+        warning(\"Cannot dump thread stacks into %s: %s\", path, res.io_error_msg());\n+        if (remove(path) != 0) warning(\"Cannot remove %s: %s\", path, os::strerror(errno));\n+        return VM_Crac::Outcome::FAIL; \/\/ User action required\n+      }\n+      case CracStackDumper::Result::Code::NON_JAVA_IN_MID: {\n+        ResourceMark rm;\n+        warning(\"Cannot checkpoint now: thread %s has Java frames interleaved with native frames\",\n+                res.problematic_thread()->name());\n+        if (remove(path) != 0) warning(\"Cannot remove %s: %s\", path, os::strerror(errno));\n+        return VM_Crac::Outcome::FAIL; \/\/ It'll probably take too long to wait until all such frames are gone\n+      }\n+      case CracStackDumper::Result::Code::NON_JAVA_ON_TOP: {\n+        ResourceMark rm;\n+        warning(\"Cannot checkpoint now: thread %s is executing native code\",\n+                res.problematic_thread()->name());\n+        if (remove(path) != 0) warning(\"Cannot remove %s: %s\", path, os::strerror(errno));\n+        return VM_Crac::Outcome::RETRY; \/\/ Hoping the thread will get out of non-Java code shortly\n+      }\n+    }\n+  }\n+\n+  \/\/ Dump classes\n+  os::snprintf_checked(path, sizeof(path), \"%s%s%s\",\n+                       CRaCCheckpointTo, os::file_separator(), PMODE_CLASS_DUMP_FILENAME);\n+  {\n+    const char* err = CracClassDumper::dump(path, false \/* Don't overwrite *\/);\n+    if (err != nullptr) {\n+      warning(\"Cannot dump classes into %s: %s\", path, err);\n+      return VM_Crac::Outcome::FAIL; \/\/ User action required\n+    }\n+  }\n+\n+  \/\/ Dump heap\n+  os::snprintf_checked(path, sizeof(path), \"%s%s%s\",\n+                       CRaCCheckpointTo, os::file_separator(), PMODE_HEAP_DUMP_FILENAME);\n+  {\n+    HeapDumper dumper(false, \/\/ No GC: it's already been performed by crac::checkpoint()\n+                      true); \/\/ Include all j.l.Class objects and injected fields\n+    if (dumper.dump(path,\n+                    nullptr,  \/\/ No additional output\n+                    -1,       \/\/ No compression, TODO: enable this when the parser supports it\n+                    false,    \/\/ Don't overwrite\n+                    HeapDumper::default_num_of_dump_threads()) != 0) {\n+      ResourceMark rm;\n+      warning(\"Cannot dump heap into %s: %s\", path, dumper.error_as_C_string());\n+      return VM_Crac::Outcome::FAIL; \/\/ User action required\n+    }\n+  }\n+\n+  return VM_Crac::Outcome::OK;\n+#else  \/\/ INCLUDE_SERVICES\n+  warning(\"This VM cannot create checkpoints in portable mode: it is compiled without \\\"services\\\" feature\");\n+  return VM_Crac::Outcome::FAIL;\n+#endif  \/\/ INCLUDE_SERVICES\n+}\n+\n@@ -127,0 +234,2 @@\n+  assert(!crac::is_portable_mode(), \"Portable mode requested, should not call this\");\n+\n@@ -133,3 +242,0 @@\n-  if (!CREngine) {\n-    return true;\n-  }\n@@ -310,0 +416,7 @@\n+  \/\/ Clear the state (partially: JCMD connection might be gone) if trying again\n+  if (_outcome == Outcome::RETRY) {\n+    _outcome = Outcome::FAIL;\n+    _failures->clear_and_deallocate();\n+    _restore_parameters.clear();\n+  }\n+\n@@ -329,1 +442,1 @@\n-    _ok = ok;\n+    _outcome = ok ? Outcome::OK : Outcome::FAIL;\n@@ -333,1 +446,1 @@\n-  if (!memory_checkpoint()) {\n+  if (!crac::is_portable_mode() && !memory_checkpoint()) {\n@@ -338,0 +451,1 @@\n+  Outcome outcome = Outcome::OK;\n@@ -343,2 +457,3 @@\n-    int ret = checkpoint_restore(&shmid);\n-    if (ret == JVM_CHECKPOINT_ERROR) {\n+    if (crac::is_portable_mode()) {\n+      outcome = checkpoint_portable();\n+    } else if (checkpoint_restore(&shmid) == JVM_CHECKPOINT_ERROR) {\n@@ -371,1 +486,1 @@\n-  _ok = true;\n+  _outcome = outcome;\n@@ -393,1 +508,1 @@\n-  if (!compute_crengine()) {\n+  if (!is_portable_mode() && !compute_crengine()) {\n@@ -400,1 +515,1 @@\n-static Handle ret_cr(int ret, Handle new_args, Handle new_props, Handle err_codes, Handle err_msgs, TRAPS) {\n+Handle crac::cr_return(int ret, Handle new_args, Handle new_props, Handle err_codes, Handle err_msgs, TRAPS) {\n@@ -418,1 +533,1 @@\n-    return ret_cr(JVM_CHECKPOINT_NONE, Handle(), Handle(), Handle(), Handle(), THREAD);\n+    return cr_return(JVM_CHECKPOINT_NONE, Handle(), Handle(), Handle(), Handle(), THREAD);\n@@ -423,1 +538,1 @@\n-    return ret_cr(JVM_CHECKPOINT_NONE, Handle(), Handle(), Handle(), Handle(), THREAD);\n+    return cr_return(JVM_CHECKPOINT_NONE, Handle(), Handle(), Handle(), Handle(), THREAD);\n@@ -450,3 +565,16 @@\n-  {\n-    MutexLocker ml(Heap_lock);\n-    VMThread::execute(&cr);\n+\n+  \/\/ TODO make these consts configurable\n+  constexpr int RETRIES_NUM = 10;\n+  constexpr int RETRY_TIMEOUT_MS = 100;\n+  for (int i = 0; i <= RETRIES_NUM; i++) {\n+    {\n+      MutexLocker ml(Heap_lock);\n+      VMThread::execute(&cr);\n+    }\n+    if (cr.outcome() != VM_Crac::Outcome::RETRY) {\n+      break;\n+    }\n+    if (i < RETRIES_NUM) {\n+      warning(\"Retry %i\/%i in %i ms...\", i + 1, RETRIES_NUM, RETRY_TIMEOUT_MS);\n+      os::naked_short_sleep(RETRY_TIMEOUT_MS);\n+    }\n@@ -460,1 +588,1 @@\n-  if (cr.ok()) {\n+  if (cr.outcome() == VM_Crac::Outcome::OK) {\n@@ -465,1 +593,1 @@\n-    GrowableArray<const char *>* new_properties = cr.new_properties();\n+    const GrowableArray<const char *>* new_properties = cr.new_properties();\n@@ -476,1 +604,1 @@\n-    return ret_cr(JVM_CHECKPOINT_OK, Handle(THREAD, new_args), props, Handle(), Handle(), THREAD);\n+    return cr_return(JVM_CHECKPOINT_OK, Handle(THREAD, new_args), props, Handle(), Handle(), THREAD);\n@@ -479,1 +607,1 @@\n-  GrowableArray<CracFailDep>* failures = cr.failures();\n+  const GrowableArray<CracFailDep>* failures = cr.failures();\n@@ -493,1 +621,1 @@\n-  return ret_cr(JVM_CHECKPOINT_ERROR, Handle(), Handle(), codes, msgs, THREAD);\n+  return cr_return(JVM_CHECKPOINT_ERROR, Handle(), Handle(), codes, msgs, THREAD);\n@@ -497,0 +625,2 @@\n+  assert(!is_portable_mode(), \"Use crac::restore_portable() instead\");\n+\n@@ -635,0 +765,137 @@\n+\n+static void init_class(Symbol *name, TRAPS) {\n+  Klass *const klass = SystemDictionary::resolve_or_fail(name, true, CHECK);\n+  InstanceKlass::cast(klass)->initialize(CHECK);\n+}\n+\n+\/\/ This is a temporary hack to fix some inconsistencies between the state of\n+\/\/ pre-initialized classes and restored ones.\n+static void init_problematic_system_classes(TRAPS) {\n+  \/\/ Fix Java-side invokedynamic-related errors\n+  init_class(TempNewSymbol(SymbolTable::new_symbol(\"java\/lang\/invoke\/BoundMethodHandle\")), CHECK);\n+\n+  \/\/ Make sure MethodType creation (requires a Java call) won't trigger class initialization\n+  if (vmClasses::MethodType_klass_is_loaded() && vmClasses::MethodType_klass()->is_initialized()) {\n+    init_class(TempNewSymbol(SymbolTable::new_symbol(\"java\/util\/concurrent\/ConcurrentHashMap$ForwardingNode\")), CHECK);\n+  }\n+}\n+\n+\/\/ Restore classes and objects in portable mode.\n+void crac::restore_data(TRAPS) {\n+  assert(is_portable_mode(), \"Use crac::restore() instead\");\n+  precond(CRaCRestoreFrom != nullptr);\n+\n+  \/\/ TODO remove this when we get rid of pre-initialized classes\n+  init_problematic_system_classes(CHECK);\n+\n+  \/\/ Create a top-level resource mark to be able to get resource-allocated\n+  \/\/ strings (e.g. external class names) for assert\/guarantee fails with no fuss\n+  assert(Thread::current()->current_resource_mark() == nullptr, \"no need for this mark?\");\n+  ResourceMark rm;\n+\n+  char path[JVM_MAXPATHLEN];\n+\n+  ParsedHeapDump heap_dump;\n+  os::snprintf_checked(path, sizeof(path), \"%s%s%s\", CRaCRestoreFrom, os::file_separator(), PMODE_HEAP_DUMP_FILENAME);\n+  const char *err_str = HeapDumpParser::parse(path, &heap_dump);\n+  if (err_str != nullptr) {\n+    THROW_MSG(vmSymbols::java_lang_IllegalArgumentException(),\n+              err_msg(\"Cannot parse heap dump %s (%s)\", path, err_str));\n+  }\n+  assert(!heap_dump.utf8s.contains(HeapDump::NULL_ID) &&\n+         !heap_dump.class_dumps.contains(HeapDump::NULL_ID) &&\n+         !heap_dump.instance_dumps.contains(HeapDump::NULL_ID) &&\n+         !heap_dump.obj_array_dumps.contains(HeapDump::NULL_ID) &&\n+         !heap_dump.prim_array_dumps.contains(HeapDump::NULL_ID),\n+         \"records cannot have null ID\");\n+\n+  auto *const stack_dump = new ParsedCracStackDump();\n+  os::snprintf_checked(path, sizeof(path), \"%s%s%s\", CRaCRestoreFrom, os::file_separator(), PMODE_STACK_DUMP_FILENAME);\n+  err_str = CracStackDumpParser::parse(path, stack_dump);\n+  if (err_str != nullptr) {\n+    delete stack_dump;\n+    THROW_MSG(vmSymbols::java_lang_IllegalArgumentException(),\n+              err_msg(\"Cannot parse stack dump %s (%s)\", path, err_str));\n+  }\n+  if (stack_dump->word_size() != oopSize) {\n+    const u2 dumped_word_size = stack_dump->word_size();\n+    delete stack_dump;\n+    THROW_MSG(vmSymbols::java_lang_UnsupportedOperationException(),\n+              err_msg(\"Cannot restore because stack dump comes from an incompatible platform: \"\n+                      \"dumped word size %i != current word size %i\", dumped_word_size, oopSize));\n+  }\n+  STATIC_ASSERT(oopSize == sizeof(intptr_t)); \/\/ Need this to safely cast primititve stack values\n+\n+  \/\/ Use heap allocation to allow the class parser to use resource area for internal purposes\n+  HeapDumpTable<InstanceKlass *, AnyObj::C_HEAP> instance_classes(107, 10000);\n+  HeapDumpTable<ArrayKlass *, AnyObj::C_HEAP> array_classes(107, 10000);\n+\n+  CracHeapRestorer heap_restorer(heap_dump, instance_classes, array_classes, THREAD);\n+  if (HAS_PENDING_EXCEPTION) {\n+    delete stack_dump;\n+    return;\n+  }\n+\n+  os::snprintf_checked(path, sizeof(path), \"%s%s%s\", CRaCRestoreFrom, os::file_separator(), PMODE_CLASS_DUMP_FILENAME);\n+  HeapDumpTable<UnfilledClassInfo, AnyObj::C_HEAP> class_infos(107, 10000);\n+  CracClassDumpParser::parse(path, heap_dump, &heap_restorer, &instance_classes, &array_classes, &class_infos, THREAD);\n+  if (HAS_PENDING_EXCEPTION) {\n+    delete stack_dump;\n+    return;\n+  }\n+\n+  heap_restorer.restore_heap(class_infos, stack_dump->stack_traces(), THREAD); \/\/ Also resolves stack values\n+  if (HAS_PENDING_EXCEPTION) {\n+    delete stack_dump;\n+    return;\n+  }\n+\n+  \/\/ Resolve all methods on the stacks while we have the ID-to-class and ID-to-symbol mappings\n+  for (const auto &stack : stack_dump->stack_traces()) {\n+    for (u4 i = 0; i < stack->frames_num(); i++) {\n+      stack->frame(i).resolve_method(instance_classes, heap_dump.utf8s, THREAD);\n+      if (HAS_PENDING_EXCEPTION) {\n+        delete stack_dump;\n+        return;\n+      }\n+    }\n+  }\n+\n+  \/\/ Save the stacks for thread restoration\n+  precond(_stack_dump == nullptr);\n+  _stack_dump = stack_dump;\n+}\n+\n+void crac::restore_threads(TRAPS) {\n+  assert(is_portable_mode(), \"use crac::restore() instead\");\n+  precond(CRaCRestoreFrom != nullptr);\n+  assert(_stack_dump != nullptr, \"call crac::restore_heap() first\");\n+  assert(java_lang_Thread::thread_id(JavaThread::current()->threadObj()) == 1, \"must be called on the main thread\");\n+\n+  if (_stack_dump->stack_traces().is_nonempty()) {\n+    CracThreadRestorer::prepare(_stack_dump->stack_traces().length());\n+  }\n+\n+  \/\/ The main thread is the first one in the dump (if it's there at all)\n+  CracStackTrace *main_stack_trace = nullptr;\n+  if (_stack_dump->stack_traces().is_nonempty()) {\n+    CracStackTrace *const first_stack_trace = _stack_dump->stack_traces().first();\n+    const oop first_thread_obj = JNIHandles::resolve_non_null(first_stack_trace->thread());\n+    if (first_thread_obj == JavaThread::current()->threadObj()) {\n+      main_stack_trace = first_stack_trace;\n+      _stack_dump->stack_traces().delete_at(0); \/\/ Efficient but changes the order of stack traces\n+    }\n+  }\n+\n+  while (_stack_dump->stack_traces().is_nonempty()) {\n+    CracStackTrace *const stack_trace = _stack_dump->stack_traces().pop();\n+    CracThreadRestorer::restore_on_new_thread(stack_trace, CHECK);\n+  }\n+\n+  delete _stack_dump; \/\/ Is empty by now\n+  _stack_dump = nullptr;\n+\n+  if (main_stack_trace != nullptr) {\n+    CracThreadRestorer::restore_on_current_thread(main_stack_trace, CHECK);\n+  }\n+}\n","filename":"src\/hotspot\/share\/runtime\/crac.cpp","additions":291,"deletions":24,"binary":false,"changes":315,"status":"modified"},{"patch":"@@ -28,0 +28,1 @@\n+#include \"runtime\/cracStackDumpParser.hpp\"\n@@ -29,1 +30,1 @@\n-#include \"utilities\/macros.hpp\"\n+#include \"utilities\/exceptions.hpp\"\n@@ -36,0 +37,3 @@\n+  \/\/ Returns true if using the experimetnal portable mode.\n+  static bool is_portable_mode();\n+\n@@ -39,0 +43,1 @@\n+  \/\/ Restore in the classic mode.\n@@ -41,0 +46,3 @@\n+  static Handle cr_return(int ret, Handle new_args, Handle new_props,\n+                          Handle err_codes, Handle err_msgs, TRAPS);\n+\n@@ -53,0 +61,8 @@\n+  \/\/ Portable mode\n+\n+  \/\/ Restores classes and objects.\n+  static void restore_data(TRAPS);\n+  \/\/ Launches execution of all threads, returns when the current thread finishes\n+  \/\/ its restored execution (if any).\n+  static void restore_threads(TRAPS);\n+\n@@ -60,0 +76,2 @@\n+\n+  static ParsedCracStackDump *_stack_dump;\n","filename":"src\/hotspot\/share\/runtime\/crac.hpp","additions":19,"deletions":1,"binary":false,"changes":20,"status":"modified"},{"patch":"@@ -0,0 +1,1972 @@\n+#include \"precompiled.hpp\"\n+#include \"classfile\/classFileParser.hpp\"\n+#include \"classfile\/classLoaderData.hpp\"\n+#include \"classfile\/dictionary.hpp\"\n+#include \"classfile\/fieldLayoutBuilder.hpp\"\n+#include \"classfile\/javaClasses.hpp\"\n+#include \"classfile\/moduleEntry.hpp\"\n+#include \"classfile\/resolutionErrors.hpp\"\n+#include \"classfile\/systemDictionary.hpp\"\n+#include \"classfile\/vmClasses.hpp\"\n+#include \"classfile\/vmIntrinsics.hpp\"\n+#include \"classfile\/vmSymbols.hpp\"\n+#include \"classfile_constants.h\"\n+#include \"interpreter\/bytecodeStream.hpp\"\n+#include \"interpreter\/bytecodes.hpp\"\n+#include \"interpreter\/linkResolver.hpp\"\n+#include \"jni.h\"\n+#include \"jvm_constants.h\"\n+#include \"logging\/log.hpp\"\n+#include \"memory\/allocation.hpp\"\n+#include \"memory\/metadataFactory.hpp\"\n+#include \"memory\/resourceArea.hpp\"\n+#include \"memory\/universe.hpp\"\n+#include \"metaprogramming\/enableIf.hpp\"\n+#include \"oops\/annotations.hpp\"\n+#include \"oops\/array.hpp\"\n+#include \"oops\/constMethod.hpp\"\n+#include \"oops\/constMethodFlags.hpp\"\n+#include \"oops\/constantPool.hpp\"\n+#include \"oops\/constantPool.inline.hpp\"\n+#include \"oops\/cpCache.hpp\"\n+#include \"oops\/fieldInfo.hpp\"\n+#include \"oops\/fieldInfo.inline.hpp\"\n+#include \"oops\/fieldStreams.inline.hpp\"\n+#include \"oops\/instanceKlass.hpp\"\n+#include \"oops\/instanceKlassFlags.hpp\"\n+#include \"oops\/klassVtable.hpp\"\n+#include \"oops\/method.hpp\"\n+#include \"oops\/method.inline.hpp\"\n+#include \"oops\/methodFlags.hpp\"\n+#include \"oops\/oopsHierarchy.hpp\"\n+#include \"oops\/recordComponent.hpp\"\n+#include \"oops\/resolvedFieldEntry.hpp\"\n+#include \"oops\/resolvedIndyEntry.hpp\"\n+#include \"prims\/jvmtiExport.hpp\"\n+#include \"prims\/jvmtiRedefineClasses.hpp\"\n+#include \"prims\/methodHandles.hpp\"\n+#include \"runtime\/arguments.hpp\"\n+#include \"runtime\/cracClassDumpParser.hpp\"\n+#include \"runtime\/cracClassDumper.hpp\"\n+#include \"runtime\/cracClassStateRestorer.hpp\"\n+#include \"runtime\/cracHeapRestorer.hpp\"\n+#include \"runtime\/globals.hpp\"\n+#include \"runtime\/handles.hpp\"\n+#include \"runtime\/mutexLocker.hpp\"\n+#include \"runtime\/os.hpp\"\n+#include \"runtime\/signature.hpp\"\n+#include \"runtime\/thread.hpp\"\n+#include \"services\/classLoadingService.hpp\"\n+#include \"utilities\/accessFlags.hpp\"\n+#include \"utilities\/basicTypeReader.hpp\"\n+#include \"utilities\/bytes.hpp\"\n+#include \"utilities\/constantTag.hpp\"\n+#include \"utilities\/debug.hpp\"\n+#include \"utilities\/exceptions.hpp\"\n+#include \"utilities\/formatBuffer.hpp\"\n+#include \"utilities\/globalDefinitions.hpp\"\n+#include \"utilities\/growableArray.hpp\"\n+#include \"utilities\/heapDumpParser.hpp\"\n+#include \"utilities\/macros.hpp\"\n+#include \"utilities\/methodKind.hpp\"\n+#include \"utilities\/pair.hpp\"\n+#include \"utilities\/tribool.hpp\"\n+#include <type_traits>\n+#if INCLUDE_JFR\n+#include \"jfr\/support\/jfrTraceIdExtension.hpp\"\n+#endif \/\/ INCLUDE_JFR\n+\n+static constexpr bool IS_ZERO = ZERO_ONLY(true) NOT_ZERO(false);\n+static constexpr bool HAVE_JVMTI = JVMTI_ONLY(true) NOT_JVMTI(false);\n+\n+void ClassDumpReader::set_id_size(u2 value, TRAPS) {\n+  if (!is_supported_id_size(value)) {\n+    THROW_MSG(vmSymbols::java_lang_UnsupportedOperationException(),\n+              err_msg(\"ID size %i is not supported: should be 1, 2, 4 or 8\", value));\n+  }\n+  _id_size = value;\n+}\n+\n+void ClassDumpReader::read_raw(void *buf, size_t size, TRAPS) {\n+  if (!_reader->read_raw(buf, size)) {\n+    log_error(crac, class, parser)(\"Raw reading error (position %zu, size %zu): %s\", _reader->pos(), size, os::strerror(errno));\n+    THROW_MSG(vmSymbols::java_io_IOException(), \"Truncated dump\");\n+  }\n+}\n+\n+template <class T>\n+T ClassDumpReader::read(TRAPS) {\n+  T result;\n+  if (!_reader->read(&result)) {\n+    log_error(crac, class, parser)(\"Basic type reading error (position %zu, size %zu): %s\", _reader->pos(), sizeof(T), os::strerror(errno));\n+    THROW_MSG_(vmSymbols::java_io_IOException(), \"Truncated dump\", {});\n+  }\n+  return result;\n+}\n+\n+bool ClassDumpReader::read_bool(TRAPS) {\n+  const auto byte = read<u1>(CHECK_false);\n+  guarantee(byte <= 1, \"not a boolean: expected 0 or 1, got %i\", byte);\n+  return byte == 1;\n+}\n+\n+HeapDump::ID ClassDumpReader::read_id(bool can_be_null, TRAPS) {\n+  precond(_id_size > 0);\n+  HeapDump::ID result;\n+  if (!_reader->read_uint(&result, _id_size)) {\n+    log_error(crac, class, parser)(\"ID reading error (position %zu, size %i): %s\", _reader->pos(), _id_size, os::strerror(errno));\n+    THROW_MSG_0(vmSymbols::java_io_IOException(), \"Truncated dump\");\n+  }\n+  guarantee(can_be_null || result != HeapDump::NULL_ID, \"unexpected null ID\");\n+  return result;\n+}\n+\n+void ClassDumpReader::skip(size_t size, TRAPS) {\n+  if (!_reader->skip(size)) {\n+    log_error(crac, class, parser)(\"Reading error (position %zu, size %zu): %s\", _reader->pos(), size, os::strerror(errno));\n+    THROW_MSG(vmSymbols::java_io_IOException(), \"Truncated dump\");\n+  }\n+}\n+\n+\/\/ Parses a particular class in a class dump and creates it.\n+class CracInstanceClassDumpParser : public StackObj \/* constructor allocates resources *\/,\n+                                    public ClassDumpReader {\n+ private:\n+  const ParsedHeapDump &_heap_dump;                                       \/\/ Heap dump accompanying the class dump\n+  const HeapDumpTable<InstanceKlass *, AnyObj::C_HEAP> &_created_classes; \/\/ Classes already created (super and interfaces should be here)\n+  const HeapDump::ClassDump &_class_dump;\n+  ClassLoaderData *const _loader_data;\n+\n+  bool _finished = false;\n+  InstanceKlass *_ik = nullptr;\n+  InstanceKlass::ClassState _class_state;\n+  HeapDump::ID _class_initialization_error_id;\n+  \/\/ Contains resource-allocated arrays which can outlive the parser. This\n+  \/\/ imposes restrictions on resource usage in the parser's implementation.\n+  InterclassRefs _interclass_refs;\n+\n+ public:\n+  CracInstanceClassDumpParser(u2 id_size, BasicTypeReader *reader, const ParsedHeapDump &heap_dump,\n+                              const HeapDumpTable<InstanceKlass *, AnyObj::C_HEAP> &created_classes,\n+                              const HeapDump::ClassDump &class_dump, ClassLoaderData *loader_data, TRAPS) :\n+      ClassDumpReader(reader, id_size), _heap_dump(heap_dump),\n+      _created_classes(created_classes), _class_dump(class_dump),\n+      _loader_data(loader_data) {\n+    precond(reader != nullptr);\n+    precond(loader_data != nullptr);\n+    log_trace(crac, class, parser)(\"Parsing instance class \" HDID_FORMAT, class_dump.id);\n+    parse_class(CHECK);\n+    create_class(CHECK);\n+    postcond(_ik != nullptr);\n+    transfer_resolution_errors();\n+    _finished = true;\n+    if (log_is_enabled(Debug, crac, class, parser)) {\n+      ResourceMark rm;\n+      log_debug(crac, class, parser)(\"Parsed and created instance class \" HDID_FORMAT \" (%s)\", class_dump.id, _ik->external_name());\n+    }\n+  }\n+\n+  InstanceKlass *created_class()     const           { precond(_finished); return _ik; }\n+  \/\/ Returned arrays are resource-allocated in the parser's constructor. The\n+  \/\/ caller must ensure there is no resource mark boundaries between the call to\n+  \/\/ the constructor and the usage of the returned arrays.\n+  InterclassRefs interclass_references() const       { precond(_finished); return _interclass_refs; }\n+  InstanceKlass::ClassState class_state() const      { precond(_finished); return _class_state; }\n+  HeapDump::ID class_initialization_error_id() const { precond(_finished); return _class_initialization_error_id; }\n+\n+  ~CracInstanceClassDumpParser() {\n+    if (_finished) {\n+      \/\/ The data has been transfered to the created class which is now\n+      \/\/ responsible for deallocation\n+      return;\n+    }\n+\n+    if (_ik != nullptr) {\n+      \/\/ Do what ClassFileParser does\n+      _loader_data->add_to_deallocate_list(_ik);\n+    }\n+\n+    if (_nest_members != Universe::the_empty_short_array()) {\n+      MetadataFactory::free_array(_loader_data, _nest_members);\n+    }\n+    if (_inner_classes != Universe::the_empty_short_array()) {\n+      MetadataFactory::free_array(_loader_data, _inner_classes);\n+    }\n+    if (_permitted_subclasses != Universe::the_empty_short_array()) {\n+      MetadataFactory::free_array(_loader_data, _inner_classes);\n+    }\n+    if (_source_debug_extension != nullptr) {\n+      FREE_C_HEAP_ARRAY(char, _source_debug_extension);\n+    }\n+    MetadataFactory::free_array(_loader_data, _bsm_operands);\n+    MetadataFactory::free_array(_loader_data, _class_annotations);\n+    MetadataFactory::free_array(_loader_data, _class_type_annotations);\n+    InstanceKlass::deallocate_record_components(_loader_data, _record_components);\n+\n+    SystemDictionary::delete_resolution_error(_cp);\n+    MetadataFactory::free_metadata(_loader_data, _cp);\n+    if (_nest_host_error_message != nullptr) {\n+      FREE_C_HEAP_ARRAY(char, _nest_host_error_message);\n+    }\n+\n+    InstanceKlass::deallocate_interfaces(_loader_data, _super, _local_interfaces, _transitive_interfaces);\n+\n+    if (_original_method_ordering != Universe::the_empty_int_array()) {\n+      MetadataFactory::free_array(_loader_data, _original_method_ordering);\n+    }\n+    InstanceKlass::deallocate_methods(_loader_data, _methods);\n+    if (_default_methods != Universe::the_empty_method_array()) {\n+      MetadataFactory::free_array(_loader_data, _default_methods);\n+    }\n+\n+    MetadataFactory::free_array(_loader_data, _field_info_stream);\n+    MetadataFactory::free_array(_loader_data, _field_statuses);\n+    Annotations::free_contents(_loader_data, _field_annotations);\n+    Annotations::free_contents(_loader_data, _field_type_annotations);\n+\n+    JVMTI_ONLY(os::free(_cached_class_file)); \/\/ Handles nullptr\n+  };\n+\n+ private:\n+  \/\/ First the parsed data is put into these fields, then when there is enough\n+  \/\/ data to allocate the class, the ownership of this data istransfered to it\n+\n+  u2 _minor_version;                                      \/\/ Class file's minor version\n+  u2 _major_version;                                      \/\/ Class file's major version\n+  JVMTI_ONLY(jint _redefinition_version);                 \/\/ Class redefinition version\n+\n+  AccessFlags _class_access_flags;                        \/\/ Class access flags from class file + internal flags from Klass\n+  bool _is_value_based;                                   \/\/ Whether the class is marked value-based in the dump\n+  InstanceKlassFlags _ik_flags;                           \/\/ Internal flags and statuses from InstanceKlass\n+\n+  u2 _source_file_name_index;                             \/\/ SourceFile class attribute\n+  u2 _generic_signature_index;                            \/\/ Signature class attribute\n+  u2 _nest_host_index;                                    \/\/ NestHost class attribute\n+  Array<u2> *_nest_members = nullptr;                     \/\/ NestMembers class attribute\n+  Array<u2> *_inner_classes = nullptr;                    \/\/ InnerClasses and EnclosingMethod class attributes\n+  char *_source_debug_extension = nullptr;                \/\/ SourceDebugExtension class attribute (nul-terminated, heap-allocated)\n+  Array<u2> *_bsm_operands = nullptr;                     \/\/ BootstrapMethods class attribute (gets moved into the ConstantPool as soon as it's ready)\n+  Array<RecordComponent *> *_record_components = nullptr; \/\/ Record class attribute\n+  Array<u2> *_permitted_subclasses = nullptr;             \/\/ PermittedSubclasses class attribute\n+  AnnotationArray *_class_annotations = nullptr;          \/\/ Runtime(In)VisibleAnnotations\n+  AnnotationArray *_class_type_annotations = nullptr;     \/\/ Runtime(In)VisibleTypeAnnotations class attribute\n+\n+  ConstantPool *_cp = nullptr;\n+  struct ResolutionError {\n+    int err_table_index;\n+    Symbol *error;\n+    Symbol *message;\n+    Symbol *cause;\n+    Symbol *cause_message;\n+  };\n+  GrowableArrayCHeap<ResolutionError, mtInternal> _resolution_errors; \/\/ Resolution errors of symbolic links\n+  const char *_nest_host_error_message = nullptr;                     \/\/ NestHost resolution error message (nul-terminated, heap-allocated)\n+\n+  u2 _this_class_index;\n+  InstanceKlass *_super = nullptr;\n+  Array<InstanceKlass *> *_local_interfaces = nullptr;\n+  Array<InstanceKlass *> *_transitive_interfaces = nullptr;\n+\n+  u2 _java_fields_num;\n+  u2 _injected_fields_num;\n+  u2 _static_oop_fields_num;\n+  GrowableArrayCHeap<FieldInfo, mtInternal> _field_infos;\n+  Array<u1> *_field_info_stream = nullptr;\n+  Array<FieldStatus> *_field_statuses = nullptr;\n+  Array<AnnotationArray *> *_field_annotations = nullptr;\n+  Array<AnnotationArray *> *_field_type_annotations = nullptr;\n+\n+  Array<int> *_original_method_ordering = nullptr;\n+  Array<Method *> *_methods = nullptr;\n+  Array<Method *> *_default_methods = nullptr;\n+\n+  JVMTI_ONLY(JvmtiCachedClassFileData *_cached_class_file = nullptr);\n+\n+  \/\/ ###########################################################################\n+  \/\/ Parsing helpers\n+  \/\/ ###########################################################################\n+\n+  template <class UINT_T, ENABLE_IF((std::is_same<UINT_T, u1>::value || std::is_same<UINT_T, u2>::value ||\n+                                     std::is_same<UINT_T, u4>::value || std::is_same<UINT_T, u8>::value))>\n+  void read_uint_array_data(UINT_T *buf, size_t length, TRAPS) {\n+    if (Endian::is_Java_byte_ordering_different()) { \/\/ Have to convert\n+      for (size_t i = 0; i < length; i++) {\n+        buf[i] = read<UINT_T>(CHECK);\n+      }\n+    } else { \/\/ Can read directly\n+      read_raw(buf, length * sizeof(UINT_T), CHECK);\n+    }\n+  }\n+\n+  template <class UINT_T, ENABLE_IF((std::is_same<UINT_T, u1>::value || std::is_same<UINT_T, u2>::value ||\n+                                     std::is_same<UINT_T, u4>::value || std::is_same<UINT_T, u8>::value))>\n+  Array<UINT_T> *read_uint_array(Array<UINT_T> *if_none, TRAPS) {\n+    precond(_loader_data != nullptr);\n+\n+    const auto len = read<u4>(CHECK_NULL);\n+    if (len == CracClassDump::NO_ARRAY_SENTINEL) {\n+      return if_none;\n+    }\n+    guarantee(len <= INT_MAX, \"metadata array length too large: \" UINT32_FORMAT \" > %i\", len, INT_MAX);\n+\n+    Array<UINT_T> *arr = MetadataFactory::new_array<UINT_T>(_loader_data, len, CHECK_NULL);\n+    read_uint_array_data(arr->data(), arr->length(), THREAD);\n+    if (HAS_PENDING_EXCEPTION) {\n+      MetadataFactory::free_array(_loader_data, arr);\n+    }\n+    return arr;\n+  }\n+\n+  Pair<HeapDump::ID, InterclassRefs::MethodDescription> read_method_identification(TRAPS) {\n+    const HeapDump::ID holder_id = read_id(false, CHECK_({}));\n+    const HeapDump::ID name_id = read_id(false, CHECK_({}));\n+    const HeapDump::ID sig_id = read_id(false, CHECK_({}));\n+    const auto kind_raw = read<u1>(CHECK_({}));\n+    guarantee(MethodKind::is_method_kind(kind_raw), \"unrecognized method kind: %i\", kind_raw);\n+    return {holder_id, {name_id, sig_id, static_cast<MethodKind::Enum>(kind_raw)}};\n+  }\n+\n+  \/\/ ###########################################################################\n+  \/\/ Parsing\n+  \/\/ ###########################################################################\n+\n+  void parse_class_state(TRAPS) {\n+    const auto raw_state = read<u1>(CHECK);\n+    guarantee(raw_state == InstanceKlass::loaded || raw_state == InstanceKlass::linked ||\n+              raw_state == InstanceKlass::fully_initialized || raw_state == InstanceKlass::initialization_error,\n+              \"illegal class state: %i\", raw_state);\n+    _class_state = static_cast<InstanceKlass::ClassState>(raw_state);\n+\n+    if (raw_state == InstanceKlass::initialization_error) {\n+      _class_initialization_error_id = read_id(true, CHECK);\n+    } else {\n+      _class_initialization_error_id = HeapDump::NULL_ID;\n+    }\n+\n+    log_trace(crac, class, parser)(\"  Parsed class state\");\n+  }\n+\n+  void parse_class_versions(TRAPS) {\n+    _minor_version = read<u2>(CHECK);\n+    _major_version = read<u2>(CHECK);\n+\n+    const auto redefinition_version = read<jint>(CHECK);\n+#if INCLUDE_JVMTI\n+    _redefinition_version = redefinition_version;\n+#else  \/\/ INCLUDE_JVMTI\n+    \/\/ Note: the fact that this verion is 0 doesn't mean the class hasn't been\n+    \/\/ redefined (overflow is allowed), so we'll also check the corresponding\n+    \/\/ internal flag later.\n+    \/\/\n+    \/\/ Also, not sure this being not 0 will cause any problems when JVM TI isn't\n+    \/\/ included, but under the normal circumstances such situation cannot\n+    \/\/ happen, so abort just to be safe\n+    guarantee(redefinition_version == 0,\n+              \"class has been redefined by a JVM TI agent (has a non-zero redefinition version), \"\n+              \"so this dump can only be restored on VMs that have JVM TI included\");\n+#endif \/\/ INCLUDE_JVMTI\n+\n+    log_trace(crac, class, parser)(\"  Parsed class versions\");\n+  }\n+\n+  void parse_class_flags(TRAPS) {\n+    u4 raw_access_flags = read<u4>(CHECK);\n+    guarantee((raw_access_flags & JVM_ACC_WRITTEN_FLAGS & ~JVM_RECOGNIZED_CLASS_MODIFIERS) == 0,\n+              \"illegal class file flags \" UINT32_FORMAT, raw_access_flags & JVM_ACC_WRITTEN_FLAGS);\n+    guarantee((raw_access_flags & ~JVM_ACC_WRITTEN_FLAGS &\n+               ~(JVM_ACC_HAS_FINALIZER | JVM_ACC_IS_CLONEABLE_FAST | JVM_ACC_IS_HIDDEN_CLASS | JVM_ACC_IS_VALUE_BASED_CLASS)) == 0,\n+              \"unrecognized internal class flags: \" UINT32_FORMAT, raw_access_flags & ~JVM_ACC_WRITTEN_FLAGS);\n+    \/\/ Update flags that depend on VM-options:\n+    raw_access_flags &= ~JVM_ACC_HAS_FINALIZER; \/\/ Will recompute by ourselves\n+    _is_value_based = (raw_access_flags & JVM_ACC_IS_VALUE_BASED_CLASS) != 0; \/\/ Remember for CDS flags\n+    if (DiagnoseSyncOnValueBasedClasses == 0) {\n+      raw_access_flags &= ~JVM_ACC_IS_VALUE_BASED_CLASS;\n+    }\n+    \/\/ Don't use set_flags -- it will drop internal Klass flags\n+    const AccessFlags access_flags(checked_cast<int>(raw_access_flags));\n+    guarantee(!access_flags.is_cloneable_fast() || vmClasses::Cloneable_klass_loaded(), \/\/ Must implement clonable, so we should've created it as interface\n+              \"either dump order is incorrect or internal class flags are inconsistent with the implemented interfaces\");\n+\n+    const u2 internal_flags = read<u2>(CHECK);\n+    const u1 internal_status = read<u1>(CHECK);\n+    InstanceKlassFlags ik_flags(internal_flags, internal_status);\n+    guarantee(!ik_flags.shared_loading_failed() && ik_flags.is_shared_unregistered_class(),\n+              \"illegal internal instance class flags\");\n+    guarantee(!ik_flags.is_being_redefined() && !ik_flags.is_scratch_class() && !ik_flags.is_marked_dependent() && !ik_flags.is_being_restored(),\n+              \"illegal internal instance class statuses\");\n+    guarantee(!ik_flags.declares_nonstatic_concrete_methods() || ik_flags.has_nonstatic_concrete_methods(),\n+              \"inconsistent internal instance class flags\");\n+    guarantee(!ik_flags.declares_nonstatic_concrete_methods() || access_flags.is_interface(),\n+              \"internal instance class flags are not consistent with class access flags\");\n+    guarantee(_class_state < InstanceKlass::ClassState::linked || ik_flags.rewritten(),\n+              \"internal instance class statuses are not consistent with class initialization state\");\n+    if (!HAVE_JVMTI && ik_flags.has_been_redefined()) {\n+      \/\/ At the moment there shouldn't be any problems with just the fact that\n+      \/\/ this flag is set when JVM TI isn't included, but under the normal\n+      \/\/ circumstances such situation cannot happen, so abort just to be safe\n+      THROW_MSG(vmSymbols::java_lang_UnsupportedOperationException(),\n+                \"class has been redefined by a JVM TI agent (has the corresponding flag set), \"\n+                \"making this dump restorable only on VMs that have JVM TI included\");\n+    }\n+    ik_flags.set_is_being_restored(true);\n+\n+    _class_access_flags = access_flags;\n+    _ik_flags = ik_flags;\n+\n+    log_trace(crac, class, parser)(\"  Parsed class flags\");\n+  }\n+\n+  void parse_nest_host_attr(TRAPS) {\n+    _nest_host_index = read<u2>(CHECK);\n+    _interclass_refs.dynamic_nest_host = read_id(true, CHECK);\n+    guarantee(_interclass_refs.dynamic_nest_host == HeapDump::NULL_ID || _class_access_flags.is_hidden_class(),\n+              \"only hidden classes can have a dynamic nest host\");\n+  }\n+\n+  void parse_source_debug_extension_attr(TRAPS) {\n+    const bool has_sde = read_bool(CHECK);\n+    if (!has_sde) {\n+      return; \/\/ No SourceDebugExtension attribute\n+    }\n+\n+    const auto len = read<jint>(CHECK);\n+    \/\/ Will use int adding 1 for the trailing nul. ClassFileParser doesn't\n+    \/\/ validate this for some reason (there is only an assert).\n+    guarantee(len <= INT_MAX - 1, \"SourceDebugExtension length is too large: %i > %i\", len, INT_MAX - 1);\n+\n+    if (!JvmtiExport::can_get_source_debug_extension()) {\n+      \/\/ Skip if SourceDebugExtension won't be retrieved (just as ClassFileParser does)\n+      skip(len, CHECK);\n+      return;\n+    }\n+\n+    _source_debug_extension = NEW_C_HEAP_ARRAY(char, len + 1, mtClass);\n+    read_raw(_source_debug_extension, len, CHECK);\n+    _source_debug_extension[len] = '\\0';\n+  }\n+\n+  void parse_record_attr(TRAPS) {\n+    const auto has_record = read_bool(CHECK);\n+    if (!has_record) {\n+      return;\n+    }\n+\n+    const auto components_num = read<u2>(CHECK);\n+    \/\/ Pre-fill with nulls so that deallocation works correctly if an error occures before the array is filled\n+    _record_components = MetadataFactory::new_array<RecordComponent *>(_loader_data, components_num, nullptr, CHECK);\n+    for (u2 i = 0; i < components_num; i++) {\n+      const auto name_index = read<u2>(CHECK);\n+      const auto descriptor_index = read<u2>(CHECK);\n+      const auto attributes_count = read<u2>(CHECK);\n+      const auto generic_signature_index = read<u2>(CHECK);\n+      AnnotationArray *annotations = read_uint_array<u1>(nullptr, CHECK);\n+      AnnotationArray *type_annotations = read_uint_array<u1>(nullptr, CHECK);\n+\n+      auto *component = RecordComponent::allocate(_loader_data, name_index, descriptor_index,\n+                                                  attributes_count, generic_signature_index,\n+                                                  annotations, type_annotations, CHECK);\n+      _record_components->at_put(i, component);\n+    }\n+  }\n+\n+  void parse_class_attrs(TRAPS) {\n+    _source_file_name_index = read<u2>(CHECK);\n+    _generic_signature_index = read<u2>(CHECK);\n+    parse_nest_host_attr(CHECK);\n+    _nest_members = read_uint_array(Universe::the_empty_short_array(), CHECK);\n+    _inner_classes = read_uint_array(Universe::the_empty_short_array(), CHECK);\n+    parse_source_debug_extension_attr(CHECK);\n+    _bsm_operands = read_uint_array<u2>(nullptr, CHECK);\n+    parse_record_attr(CHECK);\n+    _permitted_subclasses = read_uint_array(Universe::the_empty_short_array(), CHECK);\n+    _class_annotations = read_uint_array<u1>(nullptr, CHECK);\n+    _class_type_annotations = read_uint_array<u1>(nullptr, CHECK);\n+    log_trace(crac, class, parser)(\"  Parsed class attributes\");\n+  }\n+\n+  void parse_resolution_error_symbols(int err_table_index, TRAPS) {\n+    const HeapDump::ID error_sym_id = read_id(true, CHECK);\n+    Symbol *error_sym = error_sym_id == HeapDump::NULL_ID ? nullptr : _heap_dump.get_symbol(error_sym_id);\n+    const HeapDump::ID msg_sym_id = read_id(true, CHECK);\n+    Symbol *msg_sym = msg_sym_id == HeapDump::NULL_ID ? nullptr : _heap_dump.get_symbol(msg_sym_id);\n+    const HeapDump::ID cause_sym_id = read_id(true, CHECK);\n+    Symbol *cause_sym = cause_sym_id == HeapDump::NULL_ID ? nullptr : _heap_dump.get_symbol(cause_sym_id);\n+    const HeapDump::ID cause_msg_sym_id = cause_sym_id == HeapDump::NULL_ID ? HeapDump::NULL_ID : read_id(true, CHECK);\n+    Symbol *cause_msg_sym = cause_msg_sym_id == HeapDump::NULL_ID ? nullptr : _heap_dump.get_symbol(cause_msg_sym_id);\n+\n+    if (err_table_index == _nest_host_index) {\n+      guarantee(_nest_host_error_message == nullptr, \"repeated NestHost resolution error\");\n+      const auto nest_host_err_len = read<u4>(CHECK);\n+      char *const nest_host_error_message = NEW_C_HEAP_ARRAY(char, nest_host_err_len + 1, mtInternal);\n+      read_raw(nest_host_error_message, nest_host_err_len, THREAD);\n+      if (HAS_PENDING_EXCEPTION) {\n+        FREE_C_HEAP_ARRAY(char, nest_host_error_message);\n+      }\n+      nest_host_error_message[nest_host_err_len] = '\\0';\n+      _nest_host_error_message = nest_host_error_message;\n+    }\n+\n+    \/\/ Cannot put directly into the global ResolutionErrorTable yet: it requires\n+    \/\/ the class name to be known\n+    \/\/ TODO it should be more efficient to just put the erros at the end of the\n+    \/\/  class description when the whole InstanceKlass is already available\n+    _resolution_errors.append(\n+      {err_table_index, error_sym, msg_sym, cause_sym, cause_msg_sym}\n+    );\n+  }\n+\n+  void parse_constant_pool(TRAPS) {\n+    const auto pool_len = read<u2>(CHECK);\n+    _cp = ConstantPool::allocate(_loader_data, pool_len, CHECK);\n+    postcond(_cp->length() == pool_len);\n+\n+    const auto classes_num = read<u2>(CHECK);\n+    _cp->allocate_resolved_klasses(_loader_data, classes_num, CHECK);\n+\n+    u2 current_class_i = 0; \/\/ Resolved classes array indexing\n+    log_trace(crac, class, parser)(\"  Parsing %i constant pool slots\", pool_len);\n+    for (u2 pool_i = 1 \/* index 0 is unused *\/; pool_i < pool_len; pool_i++) {\n+      const auto tag = read<u1>(CHECK);\n+      switch (tag) {\n+        case JVM_CONSTANT_Utf8: {\n+          const HeapDump::ID sym_id = read_id(false, CHECK);\n+          Symbol *const sym = _heap_dump.get_symbol(sym_id);\n+          sym->increment_refcount(); \/\/ Ensures it won't be destroyed together with the heap dump\n+          _cp->symbol_at_put(pool_i, sym);\n+          break;\n+        }\n+        case JVM_CONSTANT_NameAndType: {\n+          const auto name_index = read<u2>(CHECK);\n+          const auto type_index = read<u2>(CHECK);\n+          _cp->name_and_type_at_put(pool_i, name_index, type_index);\n+          break;\n+        }\n+\n+        case JVM_CONSTANT_Integer: {\n+          const auto n = read<jint>(CHECK);\n+          _cp->int_at_put(pool_i, n);\n+          break;\n+        }\n+        case JVM_CONSTANT_Float: {\n+          const auto n = read<jfloat>(CHECK);\n+          _cp->float_at_put(pool_i, n);\n+          break;\n+        }\n+        case JVM_CONSTANT_Long: {\n+          const auto n = read<jlong>(CHECK);\n+          _cp->long_at_put(pool_i, n);\n+          guarantee(++pool_i != pool_len, \"long occupies two constant pool slots and thus cannot start on the last slot\");\n+          break;\n+        }\n+        case JVM_CONSTANT_Double: {\n+          const auto n = read<jdouble>(CHECK);\n+          _cp->double_at_put(pool_i, n);\n+          guarantee(++pool_i != pool_len, \"double occupies two constant pool slots and thus cannot start on the last slot\");\n+          break;\n+        }\n+        case JVM_CONSTANT_String: {\n+          const HeapDump::ID sym_id = read_id(false, CHECK);\n+          Symbol *const sym = _heap_dump.get_symbol(sym_id);\n+          _cp->unresolved_string_at_put(pool_i, sym);\n+          \/\/ Resolved String objects will be restored as part of cache restoration\n+          break;\n+        }\n+\n+        case JVM_CONSTANT_Class:\n+        case JVM_CONSTANT_UnresolvedClass:\n+        case JVM_CONSTANT_UnresolvedClassInError: {\n+          guarantee(current_class_i < classes_num, \"more classes in constant pool than specified\");\n+          const auto class_name_index = read<u2>(CHECK);\n+          _cp->unresolved_klass_at_put(pool_i, class_name_index, current_class_i++);\n+\n+          if (tag == JVM_CONSTANT_Class) {\n+            const HeapDump::ID class_id = read_id(false, CHECK);\n+            _interclass_refs.cp_class_refs->append({pool_i, class_id});\n+            if (pool_i == _nest_host_index) {\n+              const bool has_nest_host_res_error = read_bool(CHECK);\n+              if (has_nest_host_res_error) {\n+                parse_resolution_error_symbols(pool_i, CHECK);\n+              }\n+            }\n+          } else if (tag == JVM_CONSTANT_UnresolvedClassInError) {\n+            parse_resolution_error_symbols(pool_i, CHECK);\n+            _cp->tag_at_put(pool_i, JVM_CONSTANT_UnresolvedClassInError);\n+          }\n+          break;\n+        }\n+        case JVM_CONSTANT_Fieldref:\n+        case JVM_CONSTANT_Methodref:\n+        case JVM_CONSTANT_InterfaceMethodref: {\n+          const auto class_index = read<u2>(CHECK);\n+          const auto name_and_type_index = read<u2>(CHECK);\n+          if (tag == JVM_CONSTANT_Fieldref) {\n+            _cp->field_at_put(pool_i, class_index, name_and_type_index);\n+          } else if (tag == JVM_CONSTANT_Methodref) {\n+            _cp->method_at_put(pool_i, class_index, name_and_type_index);\n+          } else {\n+            _cp->interface_method_at_put(pool_i, class_index, name_and_type_index);\n+          }\n+          break;\n+        }\n+        case JVM_CONSTANT_MethodType:\n+        case JVM_CONSTANT_MethodTypeInError: {\n+          const auto mt_index = read<u2>(CHECK);\n+          _cp->method_type_index_at_put(pool_i, mt_index);\n+          if (tag == JVM_CONSTANT_MethodTypeInError) {\n+            parse_resolution_error_symbols(pool_i, CHECK);\n+            _cp->tag_at_put(pool_i, JVM_CONSTANT_MethodTypeInError);\n+          }\n+          break;\n+        }\n+        case JVM_CONSTANT_MethodHandle:\n+        case JVM_CONSTANT_MethodHandleInError: {\n+          const auto mh_kind = read<u1>(CHECK);\n+          const auto mh_index = read<u2>(CHECK);\n+          _cp->method_handle_index_at_put(pool_i, mh_kind, mh_index);\n+          if (tag == JVM_CONSTANT_MethodHandleInError) {\n+            parse_resolution_error_symbols(pool_i, CHECK);\n+            _cp->tag_at_put(pool_i, JVM_CONSTANT_MethodHandleInError);\n+          }\n+          break;\n+        }\n+        case JVM_CONSTANT_Dynamic:\n+        case JVM_CONSTANT_DynamicInError:\n+        case JVM_CONSTANT_InvokeDynamic: {\n+          const auto bsm_attr_index = read<u2>(CHECK);\n+          const auto name_and_type_index = read<u2>(CHECK);\n+          if (tag == JVM_CONSTANT_InvokeDynamic) {\n+            _cp->invoke_dynamic_at_put(pool_i, bsm_attr_index, name_and_type_index);\n+          } else {\n+            _cp->dynamic_constant_at_put(pool_i, bsm_attr_index, name_and_type_index);\n+            _cp->set_has_dynamic_constant();\n+            if (tag == JVM_CONSTANT_DynamicInError) {\n+              parse_resolution_error_symbols(pool_i, CHECK);\n+              _cp->tag_at_put(pool_i, JVM_CONSTANT_DynamicInError);\n+            }\n+          }\n+          break;\n+        }\n+\n+        default:\n+          guarantee(false, \"illegal tag %i at constant pool slot %i\", tag, pool_i);\n+      }\n+    }\n+    guarantee(current_class_i == classes_num, \"less classes in constant pool than specified: %i < %i\", current_class_i, classes_num);\n+\n+    log_trace(crac, class, parser)(\"  Parsed constant pool\");\n+  }\n+\n+  static int prepare_resolved_method_flags(u1 raw_flags) {\n+    using crac_shifts = CracClassDump::ResolvedMethodEntryFlagShift;\n+    using cache_shifts = ConstantPoolCacheEntry;\n+    return static_cast<int>(is_set_nth_bit(raw_flags, crac_shifts::has_local_signature_shift)) << cache_shifts::has_local_signature_shift |\n+           static_cast<int>(is_set_nth_bit(raw_flags, crac_shifts::has_appendix_shift))        << cache_shifts::has_appendix_shift        |\n+           static_cast<int>(is_set_nth_bit(raw_flags, crac_shifts::is_forced_virtual_shift))   << cache_shifts::is_forced_virtual_shift   |\n+           static_cast<int>(is_set_nth_bit(raw_flags, crac_shifts::is_final_shift))            << cache_shifts::is_final_shift            |\n+           static_cast<int>(is_set_nth_bit(raw_flags, crac_shifts::is_vfinal_shift))           << cache_shifts::is_vfinal_shift;\n+  }\n+\n+  void parse_constant_pool_cache(TRAPS) {\n+    precond(_cp != nullptr);\n+\n+    const auto field_entries_len = read<u2>(CHECK);\n+    const auto method_entries_len = read<jint>(CHECK); \/\/ AKA cache length\n+    const auto indy_entries_len = read<jint>(CHECK);\n+    guarantee(method_entries_len >= 0, \"amount of resolved methods cannot be negative\");\n+    guarantee(indy_entries_len >= 0, \"amount of resolved invokedynamic instructions cannot be negative\");\n+\n+    ConstantPoolCache *const cp_cache =\n+      ConstantPoolCache::allocate_uninitialized(_loader_data, method_entries_len, indy_entries_len, field_entries_len, CHECK);\n+    _cp->set_cache(cp_cache); \/\/ Make constant pool responsible for cache deallocation\n+    cp_cache->set_constant_pool(_cp);\n+\n+    for (u2 field_i = 0; field_i < field_entries_len; field_i++) {\n+      const auto cp_index = read<u2>(CHECK);\n+      guarantee(cp_index > 0, \"resolved field entry %i is uninitialized\", field_i);\n+      ResolvedFieldEntry field_entry(cp_index);\n+\n+      const auto get_code = read<u1>(CHECK);\n+      const auto put_code = read<u1>(CHECK);\n+      if (get_code != 0 && get_code != Bytecodes::_getfield && get_code != Bytecodes::_getstatic) {\n+        const char *code_name = Bytecodes::is_defined(get_code) ? Bytecodes::name(Bytecodes::cast(get_code)) :\n+                                                                  static_cast<const char *>(err_msg(\"%i\", get_code));\n+        guarantee(false, \"not a get* bytecode: %s\", code_name);\n+      }\n+      if (put_code != 0 && put_code != Bytecodes::_putfield && put_code != Bytecodes::_putstatic) {\n+        const char *code_name = Bytecodes::is_defined(put_code) ? Bytecodes::name(Bytecodes::cast(put_code)) :\n+                                                                  static_cast<const char *>(err_msg(\"%i\", put_code));\n+        guarantee(false, \"not a put* bytecode: %s\", code_name);\n+      }\n+      guarantee(get_code != 0 || put_code == 0, \"field entry cannot be resolved for put* bytecodes only\");\n+\n+      if (get_code != 0) {\n+        const HeapDump::ID holder_id = read_id(false, CHECK);\n+        _interclass_refs.field_refs->append({field_i, holder_id}); \/\/ Save to resolve later\n+\n+        const auto field_index = read<u2>(CHECK);\n+        const auto tos_state = read<u1>(CHECK);\n+        guarantee(tos_state < TosState::number_of_states, \"illegal resolved field entry ToS state: %i\", tos_state);\n+        const auto flags = read<u1>(CHECK);\n+        field_entry.fill_in_portable(field_index, tos_state, flags, get_code, put_code);\n+      }\n+\n+      *cp_cache->resolved_field_entry_at(field_i) = field_entry;\n+    }\n+\n+    for (int cache_i = 0; cache_i < method_entries_len; cache_i++) {\n+      const auto cp_index = read<u2>(CHECK);\n+      guarantee(cp_index > 0, \"resolved method entry %i is uninitialized\", cache_i);\n+      ConstantPoolCacheEntry cache_entry;\n+      cache_entry.initialize_entry(cp_index);\n+\n+      const auto raw_bytecode1 = read<u1>(CHECK);\n+      const auto raw_bytecode2 = read<u1>(CHECK);\n+      guarantee(Bytecodes::is_defined(raw_bytecode1), \"undefined method resolution bytecode 1: %i\", raw_bytecode1);\n+      guarantee(Bytecodes::is_defined(raw_bytecode2), \"undefined method resolution bytecode 2: %i\", raw_bytecode2);\n+\n+      if (raw_bytecode1 > 0 || raw_bytecode2 > 0) { \/\/ If resolved\n+        const auto flags = read<u1>(CHECK);\n+        guarantee(CracClassDump::is_resolved_method_entry_flags(flags), \"unrecognized resolved method entry flags: \" UINT8_FORMAT_X_0, flags);\n+        const auto tos_state = read<u1>(CHECK);\n+        guarantee(tos_state < TosState::number_of_states, \"illegal resolved method entry ToS state: %i\", tos_state);\n+        const auto params_num = read<u1>(CHECK);\n+        cache_entry.set_method_flags(checked_cast<TosState>(tos_state), prepare_resolved_method_flags(flags), params_num);\n+        postcond(cache_entry.is_method_entry());\n+        postcond(cache_entry.flag_state() == tos_state);\n+        postcond(cache_entry.parameter_size() == params_num);\n+\n+        \/\/ Not readable from the entry until f1 is set\n+        const bool has_appendix = is_set_nth_bit(flags, CracClassDump::has_appendix_shift);\n+\n+        \/\/ f1\n+        bool f1_is_method = false;\n+        HeapDump::ID f1_class_id = HeapDump::NULL_ID;\n+        InterclassRefs::MethodDescription f1_method_desc;\n+        const Bytecodes::Code bytecode1 = Bytecodes::cast(raw_bytecode1);\n+        switch (bytecode1) {\n+          case Bytecodes::_invokestatic:\n+          case Bytecodes::_invokespecial:\n+          case Bytecodes::_invokehandle: {\n+            f1_is_method = true;\n+            const auto method_identification = read_method_identification(CHECK);\n+            f1_class_id = method_identification.first;\n+            f1_method_desc = method_identification.second;\n+            break;\n+          }\n+          case Bytecodes::_invokeinterface:\n+            if (!cache_entry.is_forced_virtual()) {\n+              f1_class_id = read_id(false, CHECK);\n+            }\n+            break;\n+          case 0: \/\/ bytecode1 is not set\n+            break;\n+          default:\n+            guarantee(false, \"illegal method resolution bytecode 1: %s\", Bytecodes::name(bytecode1));\n+        }\n+        if (bytecode1 != 0) {\n+          cache_entry.set_bytecode_1(bytecode1);\n+          postcond(cache_entry.is_resolved(bytecode1));\n+        }\n+\n+        \/\/ f2\n+        HeapDump::ID f2_class_id = HeapDump::NULL_ID;\n+        InterclassRefs::MethodDescription f2_method_desc;\n+        bool cleared_virtual_call = false;\n+        const Bytecodes::Code bytecode2 = Bytecodes::cast(raw_bytecode2);\n+        guarantee(bytecode2 == 0 || bytecode2 == Bytecodes::_invokevirtual, \"illegal method resolution bytecode 2: %s\", Bytecodes::name(bytecode2));\n+        if (cache_entry.is_vfinal() || (bytecode1 == Bytecodes::_invokeinterface && !cache_entry.is_forced_virtual())) {\n+          guarantee(bytecode1 != Bytecodes::_invokestatic && bytecode1 != Bytecodes::_invokehandle,\n+                    \"illegal resolved method data: b1 = %s, b2 = %s, is_vfinal = %s, is_forced_virtual = %s\",\n+                    Bytecodes::name(bytecode1), Bytecodes::name(bytecode2),\n+                    BOOL_TO_STR(cache_entry.is_vfinal()), BOOL_TO_STR(cache_entry.is_forced_virtual()));\n+          const auto method_identification = read_method_identification(CHECK);\n+          f2_class_id = method_identification.first;\n+          f2_method_desc = method_identification.second;\n+        } else if (bytecode1 == Bytecodes::_invokehandle) {\n+          guarantee(bytecode1 != Bytecodes::_invokestatic && bytecode1 != Bytecodes::_invokevirtual && bytecode1 != Bytecodes::_invokeinterface,\n+                    \"illegal resolved method data: b1 = %s, b2 = %s, is_vfinal = %s, is_forced_virtual = %s\",\n+                    Bytecodes::name(bytecode1), Bytecodes::name(bytecode2),\n+                    BOOL_TO_STR(cache_entry.is_vfinal()), BOOL_TO_STR(cache_entry.is_forced_virtual()));\n+          const auto appendix_i = read<jint>(CHECK);\n+          guarantee(appendix_i >= 0, \"index into resolved references array cannot be negative\");\n+          cache_entry.set_f2(appendix_i);\n+        } else if (bytecode2 == Bytecodes::_invokevirtual) {\n+          precond(!cache_entry.is_vfinal());\n+          guarantee(bytecode1 != Bytecodes::_invokestatic && bytecode1 != Bytecodes::_invokehandle && bytecode1 != Bytecodes::_invokeinterface,\n+                    \"illegal resolved method data: b1 = %s, b2 = %s, is_vfinal = %s, is_forced_virtual = %s\",\n+                    Bytecodes::name(bytecode1), Bytecodes::name(bytecode2),\n+                    BOOL_TO_STR(cache_entry.is_vfinal()), BOOL_TO_STR(cache_entry.is_forced_virtual()));\n+          \/\/ f2 was a vtable index which is not portable because vtable depends\n+          \/\/ on method ordering and that depends on Symbol table's memory layout,\n+          \/\/ so clear the entry so it is re-resolved with the new vtable index\n+          \/\/ TODO instead of clearing, find a way to update the vtable index\n+          if (bytecode1 == 0) {\n+            \/\/ Should clear the whole thing\n+            cache_entry.initialize_entry(cp_index);\n+          } else {\n+            \/\/ Clear the only flag that might have been set when resolving the virtual call\n+            intx flags = cache_entry.flags_ord();\n+            clear_nth_bit(flags, ConstantPoolCacheEntry::is_forced_virtual_shift);\n+            cache_entry.set_flags(flags);\n+            postcond(!cache_entry.is_forced_virtual());\n+            postcond(cache_entry.is_method_entry() && cache_entry.flag_state() == tos_state && cache_entry.parameter_size() == params_num);\n+          }\n+          cleared_virtual_call = true;\n+        }\n+        if (bytecode2 != 0 && !cleared_virtual_call) {\n+          cache_entry.set_bytecode_2(bytecode2);\n+          postcond(cache_entry.is_resolved(bytecode2));\n+        }\n+\n+        if (f1_class_id != HeapDump::NULL_ID || f2_class_id != HeapDump::NULL_ID) { \/\/ Save to resolve later\n+          _interclass_refs.method_refs->append({cache_i, f1_is_method, f1_class_id, f1_method_desc, f2_class_id, f2_method_desc});\n+        }\n+      } else {\n+        const bool is_f2_set = read_bool(CHECK);\n+        if (is_f2_set) {\n+          const auto appendix_i = read<jint>(CHECK);\n+          guarantee(appendix_i >= 0, \"index into resolved references array cannot be negative\");\n+          cache_entry.set_f2(appendix_i);\n+        }\n+      }\n+      postcond(cache_entry.bytecode_1() != 0 || cache_entry.bytecode_2() != 0 || \/\/ Either resolved...\n+               (cache_entry.is_f1_null() && cache_entry.flags_ord() == 0));      \/\/ ...or clean (except maybe f2)\n+\n+      *cp_cache->entry_at(cache_i) = cache_entry;\n+    }\n+\n+    for (int indy_i = 0; indy_i < indy_entries_len; indy_i++) {\n+      const auto cp_index = read<u2>(CHECK);\n+      guarantee(cp_index > 0, \"resolved invokedynamic entry %i is uninitialized\", indy_i);\n+      const auto resolved_references_index = read<u2>(CHECK);\n+      ResolvedIndyEntry indy_entry(resolved_references_index, cp_index);\n+\n+      const auto extended_flags = read<u1>(CHECK);\n+      guarantee(extended_flags >> (ResolvedIndyEntry::num_flags + 1) == 0,\n+                \"unrecognized resolved invokedynamic entry flags: \" UINT8_FORMAT_X_0, extended_flags);\n+      const bool is_resolution_failed = is_set_nth_bit(extended_flags, 0); \/\/ TODO define the shift in ResolvedIndyEntry\n+      const bool has_appendix         = is_set_nth_bit(extended_flags, ResolvedIndyEntry::has_appendix_shift);\n+      const bool is_resolved          = is_set_nth_bit(extended_flags, ResolvedIndyEntry::num_flags);\n+      guarantee((is_resolved && !is_resolution_failed) || (!is_resolved && !has_appendix),\n+                \"illegal invokedynamic entry flag combination: \" UINT8_FORMAT_X_0, extended_flags);\n+\n+      if (is_resolved) {\n+        const auto adapter_identification = read_method_identification(CHECK);\n+        const HeapDump::ID adapter_holder_id = adapter_identification.first;\n+        const InterclassRefs::MethodDescription adapter_desc = adapter_identification.second;\n+        _interclass_refs.indy_refs->append({indy_i, adapter_holder_id, adapter_desc}); \/\/ Save to resolve later\n+\n+        const auto adapter_num_params = read<u2>(CHECK);\n+        const auto adapter_ret_type = read<u1>(CHECK);\n+        indy_entry.fill_in_partial(adapter_num_params, adapter_ret_type, has_appendix);\n+      } else if (is_resolution_failed) {\n+        const int indy_res_err_i = ResolutionErrorTable::encode_cpcache_index(ConstantPool::encode_invokedynamic_index(indy_i));\n+        parse_resolution_error_symbols(indy_res_err_i, CHECK);\n+        indy_entry.set_resolution_failed();\n+      }\n+\n+      *cp_cache->resolved_indy_entry_at(indy_i) = indy_entry;\n+    }\n+\n+    \/\/ Mapping from the first part of resolved references back to constant pool\n+    Array<u2> *const reference_map = read_uint_array<u2>(nullptr, CHECK);\n+    cp_cache->set_reference_map(reference_map);\n+\n+    log_trace(crac, class, parser)(\"  Parsed constant pool cache\");\n+  }\n+\n+  void parse_this_class_index(TRAPS) {\n+    const auto this_class_index = read<u2>(CHECK);\n+    guarantee(this_class_index > 0 && this_class_index < _cp->length(),\n+              \"this class index %i is out of constant pool bounds\", this_class_index);\n+    \/\/ Would be nice to assert this points to a resolved class for hidden\n+    \/\/ classes (ClassFileParser performs the resolution in such cases), but we\n+    \/\/ postpone restoring the class references for later\n+    _this_class_index = this_class_index;\n+    if (log_is_enabled(Trace, crac, class, parser)) {\n+      log_trace(crac, class, parser)(\"  Parsed this class index: %s\",\n+                                     _cp->klass_name_at(this_class_index)->as_klass_external_name());\n+    }\n+  }\n+\n+  void find_super(TRAPS) {\n+    if (_class_dump.super_id == HeapDump::NULL_ID) {\n+      log_trace(crac, class, parser)(\"  No super\");\n+      return;\n+    }\n+\n+    InstanceKlass **const super_ptr = _created_classes.get(_class_dump.super_id);\n+    guarantee(super_ptr != nullptr,\n+              \"invalid dump order: class \" HDID_FORMAT \" is dumped ahead of its super class \" HDID_FORMAT,\n+              _class_dump.id, _class_dump.super_id);\n+\n+    InstanceKlass *const super = *super_ptr;\n+    precond(super->is_loaded());\n+    guarantee(!super->is_interface(), \"class %s (ID \" HDID_FORMAT \") cannot be extended by \" HDID_FORMAT \" because it is an interface\",\n+              super->external_name(), _class_dump.super_id, _class_dump.id);\n+    guarantee(!super->has_nonstatic_concrete_methods() || _ik_flags.has_nonstatic_concrete_methods(),\n+              \"internal class flags are not consistent with those of the super class\");\n+\n+    _super = super;\n+    if (log_is_enabled(Trace, crac, class, parser)) {\n+      ResourceMark rm;\n+      log_trace(crac, class, parser)(\"  Found super: %s (%p)\", super->external_name(), super);\n+    }\n+  }\n+\n+  void parse_interfaces(TRAPS) {\n+    const auto interfaces_num = read<u2>(CHECK);\n+    if (interfaces_num == 0) {\n+      _local_interfaces = Universe::the_empty_instance_klass_array();\n+      log_trace(crac, class, parser)(\"  No local interfaces\");\n+      return;\n+    }\n+    log_trace(crac, class, parser)(\"  Parsing %i local interfaces:\", interfaces_num);\n+\n+    _local_interfaces = MetadataFactory::new_array<InstanceKlass *>(_loader_data, interfaces_num, CHECK);\n+    for (u2 i = 0; i < interfaces_num; i++) {\n+      const HeapDump::ID interface_id = read_id(false, CHECK);\n+      InstanceKlass **const interface_ptr = _created_classes.get(interface_id);\n+      guarantee(interface_ptr != nullptr,\n+                \"invalid dump order: class \" HDID_FORMAT \" is dumped ahead of its interface \" HDID_FORMAT,\n+                _class_dump.id, interface_id);\n+\n+      InstanceKlass *const interface = *interface_ptr;\n+      precond(interface->is_loaded());\n+      guarantee(interface->is_interface(),\n+                \"class %s (ID \" HDID_FORMAT \") cannot be implemented by \" HDID_FORMAT \" because it is not an interface\",\n+                interface->external_name(), interface_id, _class_dump.id);\n+      guarantee(!interface->has_nonstatic_concrete_methods() || _ik_flags.has_nonstatic_concrete_methods(),\n+                \"internal class flags are not consistent with those of implemented interfaces\");\n+\n+      _local_interfaces->at_put(i, interface);\n+      if (log_is_enabled(Trace, crac, class, parser)) {\n+        ResourceMark rm;\n+        log_trace(crac, class, parser)(\"    Interface: %s (%p)\", interface->external_name(), interface);\n+      }\n+    }\n+\n+    log_trace(crac, class, parser)(\"  Parsed local interfaces\");\n+  }\n+\n+  void parse_field_annotations(int field_index, int java_fields_num, Array<AnnotationArray *> **const annotations_collection, TRAPS) {\n+    precond(field_index < java_fields_num);\n+    precond(annotations_collection != nullptr);\n+    AnnotationArray *annotations = read_uint_array<u1>(nullptr, CHECK);\n+    if (annotations == nullptr) {\n+      return;\n+    }\n+    if (*annotations_collection == nullptr) {\n+      \/\/ Pre-fill with nulls since some slots may remain unfilled (fields without annotations)\n+      *annotations_collection = MetadataFactory::new_array<AnnotationArray *>(_loader_data, java_fields_num, nullptr, THREAD);\n+      if (HAS_PENDING_EXCEPTION) {\n+        MetadataFactory::free_array(_loader_data, annotations);\n+        return;\n+      }\n+    }\n+    (*annotations_collection)->at_put(field_index, annotations);\n+  }\n+\n+  void parse_fields(TRAPS) {\n+    const auto java_fields_num = read<u2>(CHECK);\n+    const auto injected_fields_num = read<u2>(CHECK);\n+    const u2 total_fields_num = java_fields_num + injected_fields_num;\n+\n+    _field_infos.reserve(total_fields_num);\n+    _field_statuses = MetadataFactory::new_array<FieldStatus>(_loader_data, total_fields_num, CHECK);\n+\n+    _static_oop_fields_num = 0;\n+    for (u2 i = 0; i < total_fields_num; i++) {\n+      const auto name_index = read<u2>(CHECK);\n+      const auto signature_index = read<u2>(CHECK);\n+      const auto raw_access_flags = read<jshort>(CHECK);\n+      guarantee((raw_access_flags & JVM_RECOGNIZED_FIELD_MODIFIERS) == raw_access_flags,\n+                \"unrecognized field access flags: \" INT16_FORMAT_X_0, raw_access_flags);\n+      const auto raw_field_flags = read<u1>(CHECK);\n+      const auto initializer_index = read<u2>(CHECK);\n+      const auto generic_signature_index = read<u2>(CHECK);\n+      const auto contention_group = read<u2>(CHECK);\n+\n+      {\n+        const AccessFlags access_flags(raw_access_flags);\n+        \/\/ Check this to skip interfaces when restoring non-static fields. Omit\n+        \/\/ the rest of field flag validation for simplicity.\n+        guarantee(!_class_access_flags.is_interface() || (access_flags.is_public() && access_flags.is_static() && access_flags.is_final()),\n+                  \"interface fields must be public, static and final\");\n+        const FieldInfo::FieldFlags field_flags(raw_field_flags);\n+        guarantee(field_flags.is_injected() == (i >= java_fields_num), \"injected fields go last\");\n+        guarantee(!field_flags.is_injected() || raw_access_flags == 0,\n+                  \"injected fields don't have any access flags set\");\n+        guarantee(!field_flags.is_contended() || _ik_flags.has_contended_annotations(),\n+                  \"class having contended fields not marked as having contended annotations\");\n+\n+        FieldInfo field_info(access_flags, name_index, signature_index, initializer_index, field_flags);\n+        field_info.set_generic_signature_index(generic_signature_index);\n+        if (field_flags.is_contended()) { \/\/ Must check or it will be set by set_contended_group()\n+          field_info.set_contended_group(contention_group);\n+        }\n+        _field_infos.append(field_info);\n+\n+        if (field_flags.is_injected()) {\n+          _injected_fields_num++;\n+        }\n+        \/\/ Use FieldInfo::signature() and not the raw signature_index to account for injected fields\n+        if (access_flags.is_static() && is_reference_type(Signature::basic_type(field_info.signature(_cp)))) {\n+          _static_oop_fields_num++;\n+        }\n+      }\n+\n+      const auto raw_field_status = read<u1>(CHECK);\n+      _field_statuses->at_put(i, FieldStatus(raw_field_status));\n+\n+      if (i < java_fields_num) { \/\/ Only non-injected fields have annotations\n+        parse_field_annotations(i, java_fields_num, &_field_annotations, CHECK);\n+        parse_field_annotations(i, java_fields_num, &_field_type_annotations, CHECK);\n+      }\n+    }\n+\n+    _java_fields_num = java_fields_num;\n+    _injected_fields_num = injected_fields_num;\n+    log_trace(crac, class, parser)(\"  Parsed fields: %i normal, %i injected\", java_fields_num, injected_fields_num);\n+  }\n+\n+  InlineTableSizes parse_method_inline_table_sizes(const ConstMethodFlags &flags, TRAPS) {\n+    const u2 exception_table_length = !flags.has_exception_table() ? 0 : read<u2>(CHECK_({}));\n+    guarantee(!flags.has_exception_table() || exception_table_length > 0, \"existing exception table cannot be empty\");\n+\n+    const jint compressed_linenumber_size = !flags.has_linenumber_table() ? 0 : read<jint>(CHECK_({}));\n+    guarantee(!flags.has_linenumber_table() || compressed_linenumber_size > 0, \"existing line number table cannot be empty\");\n+\n+    const u2 localvariable_table_length = !flags.has_localvariable_table() ? 0 : read<u2>(CHECK_({}));\n+    guarantee(!flags.has_localvariable_table() || localvariable_table_length > 0, \"existing local variable table cannot be empty\");\n+\n+    const u2 checked_exceptions_length = !flags.has_checked_exceptions() ? 0 : read<u2>(CHECK_({}));\n+    guarantee(!flags.has_checked_exceptions() || checked_exceptions_length > 0, \"existing checked exceptions list cannot be empty\");\n+\n+    const int method_parameters_length = !flags.has_method_parameters() ? -1 : read<u1>(CHECK_({})) \/* can be zero *\/;\n+\n+    const u2 generic_signature_index = !flags.has_generic_signature() ? 0 : read<u2>(CHECK_({}));\n+    guarantee(!flags.has_generic_signature() || (generic_signature_index > 0 && generic_signature_index < _cp->length()),\n+              \"method's signature index %i is out of constant pool bounds\", generic_signature_index);\n+\n+    const jint method_annotations_length = !flags.has_method_annotations() ? 0 : read<jint>(CHECK_({}));\n+    guarantee(!flags.has_method_annotations() || method_annotations_length > 0, \"existing method annotations cannot be empty\");\n+\n+    const jint parameter_annotations_length = !flags.has_parameter_annotations() ? 0 : read<jint>(CHECK_({}));\n+    guarantee(!flags.has_parameter_annotations() || parameter_annotations_length > 0, \"existing method parameter annotations cannot be empty\");\n+\n+    const jint type_annotations_length = !flags.has_type_annotations() ? 0 : read<jint>(CHECK_({}));\n+    guarantee(!flags.has_type_annotations() || type_annotations_length > 0, \"existing method type annotations cannot be empty\");\n+\n+    const jint default_annotations_length = !flags.has_default_annotations() ? 0 : read<jint>(CHECK_({}));\n+    guarantee(!flags.has_default_annotations() || default_annotations_length > 0, \"existing method default annotations cannot be empty\");\n+\n+#define INLINE_TABLE_NAME_PARAM(name) name,\n+    return {\n+      INLINE_TABLES_DO(INLINE_TABLE_NAME_PARAM)\n+      0 \/* end of iteration *\/\n+    };\n+#undef INLINE_TABLE_NAME_PARAM\n+  }\n+\n+  static InlineTableSizes update_method_inline_table_sizes(const InlineTableSizes &orig) {\n+    return {\n+      orig.localvariable_table_length(),\n+      orig.compressed_linenumber_size(),\n+      orig.exception_table_length(),\n+      orig.checked_exceptions_length(),\n+      \/\/ TODO ClassFIleParser does this, but why? What if j.l.r.Parameter gets loaded later?\n+      vmClasses::Parameter_klass_loaded() ? orig.method_parameters_length() : -1,\n+      orig.generic_signature_index(),\n+      orig.method_annotations_length(),\n+      orig.parameter_annotations_length(),\n+      orig.type_annotations_length(),\n+      orig.default_annotations_length(),\n+      0\n+    };\n+  }\n+\n+  static void fixup_bytecodes(Method *method) {\n+    RawBytecodeStream stream(methodHandle(Thread::current(), method));\n+    for (Bytecodes::Code code = stream.raw_next(); !stream.is_last_bytecode(); code = stream.raw_next()) {\n+      guarantee((Bytecodes::is_java_code(code) && code != Bytecodes::_lookupswitch) ||\n+                 code == Bytecodes::_invokehandle ||\n+                 code == Bytecodes::_fast_aldc || code == Bytecodes::_fast_aldc_w ||\n+                 code == Bytecodes::_fast_linearswitch || code == Bytecodes::_fast_binaryswitch ||\n+                 code == Bytecodes::_return_register_finalizer, \"illegal bytecode: %s\", Bytecodes::name(code));\n+\n+      if (Endian::is_Java_byte_ordering_different()) {\n+        const address param_bcp = stream.bcp() + 1;\n+        if (Bytecodes::is_field_code(code) || Bytecodes::is_invoke(code) || code == Bytecodes::_invokehandle) {\n+          if (code == Bytecodes::_invokedynamic) {\n+            Bytes::put_native_u4(param_bcp, Bytes::get_Java_u4(param_bcp));\n+          } else {\n+            Bytes::put_native_u2(param_bcp, Bytes::get_Java_u2(param_bcp));\n+          }\n+          continue;\n+        }\n+        if (code == Bytecodes::_fast_aldc_w) {\n+          Bytes::put_native_u2(param_bcp, Bytes::get_Java_u2(param_bcp));\n+          continue;\n+        }\n+        postcond(!Bytecodes::native_byte_order(code));\n+      }\n+\n+      if (IS_ZERO && code == Bytecodes::_fast_linearswitch && code == Bytecodes::_fast_binaryswitch) {\n+        (*stream.bcp()) = Bytecodes::_lookupswitch;\n+      }\n+    }\n+  }\n+\n+  void parse_code_attr(Method *method, int compressed_linenumber_table_size, TRAPS) {\n+    precond(method->code_size() > 0);\n+\n+    {\n+      const auto max_stack = read<u2>(CHECK);\n+      const auto max_locals = read<u2>(CHECK);\n+      method->set_max_stack(max_stack);\n+      method->set_max_locals(max_locals);\n+    }\n+\n+    read_raw(method->code_base(), method->code_size(), CHECK);\n+    if (_ik_flags.rewritten() && (Endian::is_Java_byte_ordering_different() || IS_ZERO)) {\n+      fixup_bytecodes(method);\n+    }\n+\n+    if (method->has_exception_handler()) {\n+      STATIC_ASSERT(sizeof(ExceptionTableElement) == 4 * sizeof(u2)); \/\/ Check no padding\n+      const size_t len = method->exception_table_length() * sizeof(ExceptionTableElement) \/ sizeof(u2);\n+      read_uint_array_data(reinterpret_cast<u2 *>(method->exception_table_start()), len, CHECK);\n+    }\n+\n+    if (method->has_linenumber_table()) {\n+      assert(compressed_linenumber_table_size > 0, \"checked when parsing\");\n+      read_raw(method->compressed_linenumber_table(), compressed_linenumber_table_size, CHECK);\n+    }\n+    if (method->has_localvariable_table()) {\n+      STATIC_ASSERT(sizeof(LocalVariableTableElement) == 6 * sizeof(u2)); \/\/ Check no padding\n+      const size_t len = method->localvariable_table_length() * sizeof(LocalVariableTableElement) \/ sizeof(u2);\n+      read_uint_array_data(reinterpret_cast<u2 *>(method->localvariable_table_start()), len, CHECK);\n+    }\n+    Array<u1> *const stackmap_table_data = read_uint_array<u1>(nullptr, CHECK);\n+    if (stackmap_table_data != nullptr) {\n+      guarantee(!stackmap_table_data->is_empty(), \"existing stack map table cannot be empty\");\n+      method->set_stackmap_data(stackmap_table_data);\n+    }\n+  }\n+\n+  static void set_method_flags(Method *method, const ConstMethodFlags &flags) {\n+    \/\/ Check flags that are set based on the legths\/sizes we passed\n+    postcond(method->is_overpass() == flags.is_overpass());\n+    postcond(method->has_linenumber_table() == flags.has_linenumber_table());\n+    postcond(method->constMethod()->has_checked_exceptions() == flags.has_checked_exceptions());\n+    postcond(method->has_localvariable_table() == flags.has_localvariable_table());\n+    postcond(method->has_exception_handler() == flags.has_exception_table());\n+    postcond(method->constMethod()->has_generic_signature() == flags.has_generic_signature());\n+    postcond(method->has_method_parameters() == flags.has_method_parameters());\n+    postcond(method->constMethod()->has_method_annotations() == flags.has_method_annotations());\n+    postcond(method->constMethod()->has_parameter_annotations() == flags.has_parameter_annotations());\n+    postcond(method->constMethod()->has_type_annotations() == flags.has_type_annotations());\n+    postcond(method->constMethod()->has_default_annotations() == flags.has_default_annotations());\n+    \/\/ Set the rest of the flags\n+    if (flags.caller_sensitive())       method->set_caller_sensitive();\n+    if (flags.is_hidden())              method->set_is_hidden();\n+    if (flags.has_injected_profile())   method->set_has_injected_profile();\n+    if (flags.reserved_stack_access())  method->set_has_reserved_stack_access();\n+    if (flags.is_scoped())              method->set_scoped();\n+    if (flags.changes_current_thread()) method->set_changes_current_thread();\n+    if (flags.jvmti_mount_transition()) method->set_jvmti_mount_transition();\n+    if (flags.intrinsic_candidate()) {\n+      guarantee(!method->is_synthetic(), \"synthetic method cannot be an intrinsic candidate\");\n+      method->set_intrinsic_candidate();\n+    }\n+  }\n+\n+  void parse_method(Method **method_out, TRAPS) {\n+    precond(method_out != nullptr);\n+\n+    const auto raw_access_flags = read<u2>(CHECK);\n+    guarantee((raw_access_flags & JVM_RECOGNIZED_METHOD_MODIFIERS) == raw_access_flags,\n+              \"unrecognized method access flags: \" UINT16_FORMAT_X_0, raw_access_flags);\n+    const AccessFlags access_flags(raw_access_flags);\n+    guarantee(!access_flags.is_final() || _ik_flags.has_final_method(), \"class with a final method not marked as such\");\n+\n+    const auto raw_flags = read<jint>(CHECK);\n+    const ConstMethodFlags flags(raw_flags);\n+    guarantee(!_class_access_flags.is_hidden_class() || flags.is_hidden(), \"methods of hidden class must be marked hidden\");\n+    guarantee(!flags.has_localvariable_table() || _ik_flags.has_localvariable_table(), \"class with methods with a local variable table not marked as such\");\n+    guarantee(!_class_access_flags.is_interface() || access_flags.is_static() || access_flags.is_abstract() ||\n+              flags.is_overpass() \/* overpasses don't exist in class files and thus don't count as declared *\/ ||\n+              _ik_flags.declares_nonstatic_concrete_methods(),\n+              \"interface with a declared non-static non-abstract method not marked as such\");\n+\n+    const auto raw_statuses = read<jint>(CHECK);\n+    const MethodFlags statuses(raw_statuses);\n+    guarantee(!statuses.queued_for_compilation() && !statuses.is_not_c1_compilable() &&\n+              !statuses.is_not_c2_compilable() && !statuses.is_not_c2_osr_compilable(),\n+              \"illegal internal method statuses: \" INT32_FORMAT_X_0, raw_statuses);\n+\n+    const auto name_index = read<u2>(CHECK);\n+    guarantee(name_index > 0 && name_index < _cp->length(), \"method name index %i is out of constant pool bounds\", name_index);\n+    const auto signature_index = read<u2>(CHECK);\n+    guarantee(signature_index > 0 && signature_index < _cp->length(), \"method descriptor index %i is out of constant pool bounds\", signature_index);\n+    Symbol *const name = _cp->symbol_at(name_index);\n+    Symbol *const signature = _cp->symbol_at(signature_index);\n+\n+    const auto code_size = read<u2>(CHECK);\n+    guarantee(code_size != 0 || (!flags.has_exception_table() && !flags.has_linenumber_table() && !flags.has_localvariable_table()),\n+              \"method cannot have Code attribute's contents in the absence of Code attribute\");\n+    const InlineTableSizes orig_inline_sizes = parse_method_inline_table_sizes(flags, CHECK);\n+\n+    const auto method_type = flags.is_overpass() ? ConstMethod::MethodType::OVERPASS : ConstMethod::MethodType::NORMAL;\n+    InlineTableSizes updated_inline_sizes = update_method_inline_table_sizes(orig_inline_sizes);\n+    Method *const method = Method::allocate(_loader_data, code_size, access_flags, &updated_inline_sizes, method_type, name, CHECK);\n+    *method_out = method; \/\/ Save eagerly to get it deallocated in case of a error\n+\n+    ClassLoadingService::add_class_method_size(method->size() * wordSize); \/\/ ClassFileParser does this, so we do too\n+\n+    method->set_constants(_cp);\n+    method->set_name_index(name_index);\n+    method->set_signature_index(signature_index);\n+    method->constMethod()->compute_from_signature(signature, access_flags.is_static());\n+\n+    if (code_size > 0) {\n+      parse_code_attr(method, orig_inline_sizes.compressed_linenumber_size(), CHECK);\n+    }\n+    if (flags.has_checked_exceptions()) {\n+      STATIC_ASSERT(sizeof(CheckedExceptionElement) == sizeof(u2)); \/\/ Check no padding\n+      const size_t len = method->checked_exceptions_length() * sizeof(CheckedExceptionElement) \/ sizeof(u2);\n+      read_uint_array_data(reinterpret_cast<u2 *>(method->checked_exceptions_start()), len, CHECK);\n+    }\n+    if (flags.has_method_parameters()) {\n+      STATIC_ASSERT(sizeof(MethodParametersElement) == 2 * sizeof(u2)); \/\/ Check no padding\n+      const size_t size = orig_inline_sizes.method_parameters_length() * sizeof(MethodParametersElement);\n+      if (vmClasses::Parameter_klass_loaded()) {\n+        const size_t len = size \/ sizeof(u2);\n+        read_uint_array_data(reinterpret_cast<u2 *>(method->method_parameters_start()), len, CHECK);\n+      } else {\n+        skip(size, CHECK);\n+      }\n+    }\n+    if (flags.has_method_annotations()) {\n+      AnnotationArray *const annots = MetadataFactory::new_array<u1>(_loader_data, orig_inline_sizes.method_annotations_length(), CHECK);\n+      method->constMethod()->set_method_annotations(annots);\n+      read_uint_array_data(annots->data(), annots->length(), CHECK);\n+    }\n+    if (flags.has_parameter_annotations()) {\n+      AnnotationArray *const annots = MetadataFactory::new_array<u1>(_loader_data, orig_inline_sizes.parameter_annotations_length(), CHECK);\n+      method->constMethod()->set_parameter_annotations(annots);\n+      read_uint_array_data(annots->data(), annots->length(), CHECK);\n+    }\n+    if (flags.has_type_annotations()) {\n+      AnnotationArray *const annots = MetadataFactory::new_array<u1>(_loader_data, orig_inline_sizes.type_annotations_length(), CHECK);\n+      method->constMethod()->set_type_annotations(annots);\n+      read_uint_array_data(annots->data(), annots->length(), CHECK);\n+    }\n+    if (flags.has_default_annotations()) {\n+      AnnotationArray *const annots = MetadataFactory::new_array<u1>(_loader_data, orig_inline_sizes.default_annotations_length(), CHECK);\n+      method->constMethod()->set_default_annotations(annots);\n+      read_uint_array_data(annots->data(), annots->length(), CHECK);\n+    }\n+\n+    set_method_flags(method, flags);\n+\n+    const bool is_compiled_lambda_form = read_bool(CHECK);\n+    if (is_compiled_lambda_form) {\n+      precond(method->intrinsic_id() == vmIntrinsics::_none);\n+      method->set_intrinsic_id(vmIntrinsics::_compiledLambdaForm);\n+      postcond(method->is_compiled_lambda_form());\n+    }\n+\n+    method->set_statuses(statuses);\n+\n+    \/\/ Not a guarantee because is_vanilla_constructor() call may take some time\n+    assert(!(_super == nullptr ||\n+             (_super->has_vanilla_constructor() &&\n+              name == vmSymbols::object_initializer_name() &&\n+              signature == vmSymbols::void_method_signature() &&\n+              method->is_vanilla_constructor())) ||\n+           _ik_flags.has_vanilla_constructor(),\n+           \"class with a vanilla constructor not marked as such\");\n+\n+    NOT_PRODUCT(method->verify());\n+  }\n+\n+  NOT_DEBUG(static) TriBool is_finalizer(const Method &method) {\n+    if (!InstanceKlass::is_finalization_enabled()) {\n+      assert(!_class_access_flags.has_finalizer(), \"must have been unset\");\n+      return {};    \/\/ Not a finalizer\n+    }\n+    if (method.name() != vmSymbols::finalize_method_name() || method.signature() != vmSymbols::void_method_signature()) {\n+      return {};    \/\/ Not a finalizer\n+    }\n+    if (method.is_empty_method()) {\n+      return false; \/\/ Empty finalizer\n+    }\n+    return true;    \/\/ Non-empty finalizer\n+  }\n+\n+  void parse_methods(TRAPS) {\n+    const auto methods_num = read<u2>(CHECK);\n+    if (methods_num > 0) {\n+      \/\/ Pre-fill with nulls so that deallocation works correctly if an error occures before the array is filled\n+      _methods = MetadataFactory::new_array<Method *>(_loader_data, methods_num, nullptr, CHECK);\n+    } else {\n+      _methods = Universe::the_empty_method_array();\n+    }\n+    if (JvmtiExport::can_maintain_original_method_order() || Arguments::is_dumping_archive()) {\n+      _original_method_ordering = MetadataFactory::new_array<int>(_loader_data, methods_num, CHECK);\n+    } else {\n+      _original_method_ordering = Universe::the_empty_int_array();\n+    }\n+\n+    TriBool has_finalizer; \/\/ Default - no finalizer, false - empty finalizer, true - non-empty finalizer\n+    for (u2 i = 0; i < methods_num; i++) {\n+      const auto orig_i = read<u2>(CHECK);\n+      guarantee(orig_i < methods_num, \"original method index %i exceeds the number of methods %i\", orig_i, methods_num);\n+      if (_original_method_ordering != Universe::the_empty_int_array()) {\n+        _original_method_ordering->at_put(i, orig_i);\n+      }\n+\n+      parse_method(_methods->adr_at(i), CHECK);\n+\n+      const TriBool is_fin = is_finalizer(*_methods->at(i));\n+      if (!is_fin.is_default()) continue; \/\/ Not a finalizer\n+      guarantee(has_finalizer.is_default(), \"class defines multiple finalizers\");\n+      has_finalizer = is_fin;             \/\/ Set the finalizer info\n+    }\n+    log_trace(crac, class, parser)(\"  Parsed %i methods\", methods_num);\n+\n+    if (has_finalizer || (has_finalizer.is_default() && _super != nullptr && _super->has_finalizer())) {\n+      assert(!InstanceKlass::is_finalization_enabled(), \"has_finalizer should not be set\");\n+      _class_access_flags.set_has_finalizer();\n+    } else {\n+      assert(!_class_access_flags.has_finalizer(), \"must have been unset\");\n+    }\n+\n+    const u2 default_methods_num = read<u2>(CHECK);\n+    if (default_methods_num > 0) {\n+      guarantee(_ik_flags.has_nonstatic_concrete_methods(),\n+                \"class without default methods in its hierarchy should not have default methods\");\n+\n+      \/\/ Pre-fill with nulls so that deallocation works correctly if an error occures before the array is filled\n+      _default_methods = MetadataFactory::new_array<Method *>(_loader_data, default_methods_num, nullptr, CHECK);\n+      for (u2 i = 0; i < default_methods_num; i++) {\n+        const auto method_identification = read_method_identification(CHECK);\n+\n+        const HeapDump::ID holder_id = method_identification.first;\n+        InstanceKlass **const holder_ptr = _created_classes.get(holder_id);\n+        \/\/ Implemented interfaces have been parsed and found as loaded, so if\n+        \/\/ it was one of them we would have found it\n+        guarantee(holder_ptr != nullptr, \"default method %i belongs to a class not implemented by this class\", i);\n+        InstanceKlass &holder = **holder_ptr;\n+        precond(holder.is_loaded());\n+        assert(holder.is_interface(), \"holder %s of default method #%i is not an interface\", holder.external_name(), i);\n+#ifdef ASSERT\n+        bool is_holder_implemented = _super != nullptr && _super->implements_interface(&holder);\n+        for (int local_interf_i = 0; !is_holder_implemented && local_interf_i < _local_interfaces->length(); local_interf_i++) {\n+          const InstanceKlass *local_interface = _local_interfaces->at(local_interf_i);\n+          is_holder_implemented = local_interface->implements_interface(&holder);\n+        }\n+        assert(is_holder_implemented, \"holder %s of default method #%i is not implemented by this class\",\n+               holder.external_name(), i);\n+#endif \/\/ ASSERT\n+\n+        const InterclassRefs::MethodDescription method_desc = method_identification.second;\n+        Symbol *const name = _heap_dump.get_symbol(method_desc.name_id);\n+        Symbol *const sig = _heap_dump.get_symbol(method_desc.sig_id);\n+        Method *const method = CracClassDumpParser::find_method(&holder, name, sig, method_desc.kind, false, CHECK);\n+        guarantee(method != nullptr, \"default method #%i cannot be found as %s method %s\",\n+                  i, MethodKind::name(method_desc.kind), Method::external_name(*holder_ptr, name, sig));\n+        guarantee(method->is_default_method(), \"default method %i resolved to a non-default %s\", i, method->external_name());\n+        _default_methods->at_put(i, method);\n+      }\n+    }\n+    log_trace(crac, class, parser)(\"  Parsed %i default methods\", default_methods_num);\n+  }\n+\n+  void parse_cached_class_file(TRAPS) {\n+    const auto len = read<jint>(CHECK);\n+    if (len == CracClassDump::NO_CACHED_CLASS_FILE_SENTINEL) {\n+      log_trace(crac, class, parser)(\"  No cached class file\");\n+      return;\n+    }\n+\n+#if INCLUDE_JVMTI\n+    _cached_class_file = reinterpret_cast<JvmtiCachedClassFileData *>(\n+      AllocateHeap(offset_of(JvmtiCachedClassFileData, data) + len, mtInternal));\n+    postcond(_cached_class_file != nullptr);\n+\n+    _cached_class_file->length = len;\n+    read_raw(_cached_class_file->data, len, CHECK);\n+#else \/\/ INCLUDE_JVMTI\n+    THROW_MSG(vmSymbols::java_lang_UnsupportedOperationException(),\n+              \"class file has been modified by a JVM TI agent, \"\n+              \"making this dump restorable only on VMs that have JVM TI included\");\n+#endif \/\/ INCLUDE_JVMTI\n+\n+    log_trace(crac, class, parser)(\"  Parsed cached class file\");\n+  }\n+\n+  \/\/ Parses the class dump. Roughly equivalent to ClassFileParser's constructor.\n+  void parse_class(TRAPS) {\n+    parse_class_state(CHECK);\n+    parse_class_versions(CHECK);\n+    parse_class_flags(CHECK);\n+    parse_class_attrs(CHECK);\n+    parse_constant_pool(CHECK);\n+    if (_ik_flags.rewritten()) {\n+      parse_constant_pool_cache(CHECK);\n+    }\n+    parse_this_class_index(CHECK);\n+    find_super(CHECK);\n+    parse_interfaces(CHECK);\n+    parse_fields(CHECK);\n+    parse_methods(CHECK);\n+    parse_cached_class_file(CHECK);\n+    log_trace(crac, class, parser)(\"  Instance class dump parsing completed\");\n+  }\n+\n+  \/\/ ###########################################################################\n+  \/\/ Class creation\n+  \/\/ ###########################################################################\n+\n+  int compute_vtable_size(const Symbol *class_name DEBUG_ONLY(COMMA TRAPS)) {\n+    precond(_transitive_interfaces != nullptr);\n+\n+    ResourceMark rm;\n+    int vtable_size;\n+    int num_mirandas; GrowableArray<Method *> all_mirandas; \/\/ Filled but shouldn't be used (see the comments below)\n+    const Handle loader_h(Thread::current(), _loader_data->class_loader());\n+    klassVtable::compute_vtable_size_and_num_mirandas(&vtable_size, &num_mirandas, &all_mirandas, _super, _methods,\n+                                                      _class_access_flags, _major_version, loader_h, class_name, _local_interfaces);\n+\n+#ifdef ASSERT\n+    \/\/ The parsed methods already include overpass methods which are normally\n+    \/\/ only generated after computing the mirandas above. Because some of the\n+    \/\/ overpasses can be ex-mirandas the mirandas list computed above may be\n+    \/\/ incomplete, so recompute without the overpasses to do the asserts below.\n+    GrowableArray<Method *> methods_no_overpasses_tmp(_methods->length());\n+    for (int i = 0; i < _methods->length(); i++) {\n+      Method *const m = _methods->at(i);\n+      if (!m->is_overpass()) {\n+        methods_no_overpasses_tmp.append(m);\n+      }\n+    }\n+    log_trace(crac, class, parser)(\"  Class has %i overpass methods\", _methods->length() - methods_no_overpasses_tmp.length());\n+\n+    Array<Method *> *const methods_no_overpasses = MetadataFactory::new_array<Method *>(_loader_data, methods_no_overpasses_tmp.length(), CHECK_0);\n+    if (!methods_no_overpasses->is_empty()) {\n+      memcpy(methods_no_overpasses->data(), methods_no_overpasses_tmp.adr_at(0), methods_no_overpasses->length() * sizeof(Method *));\n+    }\n+\n+    int vtable_size_debug;\n+    int num_mirandas_debug; GrowableArray<Method *> all_mirandas_debug; \/\/ These will be the right values\n+    klassVtable::compute_vtable_size_and_num_mirandas(&vtable_size_debug, &num_mirandas_debug, &all_mirandas_debug, _super, methods_no_overpasses,\n+                                                      _class_access_flags, _major_version, loader_h, class_name, _local_interfaces);\n+    assert(vtable_size == vtable_size_debug, \"absence of overpass methods should not change the vtable size\");\n+    assert(num_mirandas <= num_mirandas_debug, \"overpasses might have been mirandas\");\n+\n+    const bool has_miranda_methods = num_mirandas_debug > 0 || (_super != nullptr && _super->has_miranda_methods());\n+    assert(_ik_flags.has_miranda_methods() == has_miranda_methods,\n+          \"internal instance class flag 'has miranda methods' dumped with incorrect value: expected %s\", BOOL_TO_STR(has_miranda_methods));\n+    MetadataFactory::free_array(_loader_data, methods_no_overpasses);\n+#endif\n+\n+    return vtable_size;\n+  }\n+\n+  FieldLayoutInfo compute_field_layout(const Symbol *class_name) {\n+    FieldLayoutInfo field_layout_info; \/\/ Will contain resource-allocated data\n+    FieldLayoutBuilder lb(class_name, _super, _cp, &_field_infos, _ik_flags.is_contended(), &field_layout_info);\n+    lb.build_layout(); \/\/ Fills FieldLayoutInfo and offsets of field infos\n+    return field_layout_info;\n+  }\n+\n+  static Annotations *create_combined_annotations(ClassLoaderData *loader_data,\n+                                                  AnnotationArray *class_annos,\n+                                                  AnnotationArray *class_type_annos,\n+                                                  Array<AnnotationArray *> *field_annos,\n+                                                  Array<AnnotationArray *> *field_type_annos,\n+                                                  TRAPS) {\n+    if (class_annos == nullptr && class_type_annos == nullptr &&\n+        field_annos == nullptr && field_type_annos == nullptr) {\n+      return nullptr; \/\/ Don't create the Annotations object unnecessarily.\n+    }\n+\n+    Annotations* const annotations = Annotations::allocate(loader_data, CHECK_NULL);\n+    annotations->set_class_annotations(class_annos);\n+    annotations->set_class_type_annotations(class_type_annos);\n+    annotations->set_fields_annotations(field_annos);\n+    annotations->set_fields_type_annotations(field_type_annos);\n+    return annotations;\n+  }\n+\n+  void move_data_to_class(TRAPS) {\n+    InstanceKlass &ik = *_ik;\n+    \/\/ Move everything we've parsed so far and null the pointers so that they\n+    \/\/ won't get freed in the destructor\n+\n+    _cp->set_operands(_bsm_operands);\n+    _cp->set_pool_holder(&ik);\n+    ik.set_constants(_cp); \/\/ Must do this before setting the indices below\n+    _bsm_operands = nullptr;\n+    _cp = nullptr;\n+\n+    ik.set_nest_members(_nest_members);\n+    ik.set_inner_classes(_inner_classes);\n+    ik.set_source_debug_extension(_source_debug_extension);\n+    ik.set_record_components(_record_components);\n+    ik.set_permitted_subclasses(_permitted_subclasses);\n+    _nest_members = nullptr;\n+    _inner_classes = nullptr;\n+    _source_debug_extension = nullptr;\n+    _record_components = nullptr;\n+    _permitted_subclasses = nullptr;\n+\n+    Annotations *const combined_annotations = create_combined_annotations(_loader_data,\n+                                                                          _class_annotations, _class_type_annotations,\n+                                                                          _field_annotations, _field_type_annotations,\n+                                                                          CHECK);\n+    ik.set_annotations(combined_annotations);\n+    _class_annotations = nullptr;\n+    _class_type_annotations = nullptr;\n+    _field_annotations = nullptr;\n+    _field_type_annotations = nullptr;\n+\n+    ik.set_fieldinfo_stream(_field_info_stream);\n+    ik.set_fields_status(_field_statuses);\n+    _field_info_stream = nullptr;\n+    _field_statuses = nullptr;\n+\n+    ik.set_methods(_methods);\n+    ik.set_method_ordering(_original_method_ordering);\n+    ik.set_default_methods(_default_methods); \/\/ Vtable indices for these will be set later not to get an allocation exception here\n+    _methods = nullptr;\n+    _original_method_ordering = nullptr;\n+    _default_methods = nullptr;\n+\n+    ik.initialize_supers(_super, _transitive_interfaces, CHECK);\n+    ik.set_local_interfaces(_local_interfaces);\n+    ik.set_transitive_interfaces(_transitive_interfaces);\n+    _local_interfaces = nullptr;\n+    _transitive_interfaces = nullptr;\n+    \/\/ No need to set super to null because the destructor won't free it\n+\n+#if INCLUDE_JVMTI\n+    ik.set_cached_class_file(_cached_class_file);\n+    _cached_class_file = nullptr;\n+#endif \/\/ INCLUDE_JVMTI\n+  }\n+\n+  \/\/ Allocates and fills the class. Roughly equivalent to\n+  \/\/ ClassFileParser::create_instance_class().\n+  void create_class(TRAPS) {\n+    Thread *const thread = Thread::current();\n+    Symbol *const class_name = _heap_dump.get_class_name(_class_dump.id);\n+\n+    \/\/ Allocate the class\n+\n+    \/\/ TODO instead of re-computing the sizes from the ground up save\n+    \/\/  vtable\/itable lengths and quickly compute the sizes based on them\n+    _transitive_interfaces = ClassFileParser::compute_transitive_interfaces(_super, _local_interfaces, _loader_data, CHECK);\n+    Method::sort_methods(_methods); \/\/ Sort before they'll be used in vtable-related computations\n+    const int vtable_size = compute_vtable_size(class_name DEBUG_ONLY(COMMA CHECK));\n+    const int itable_size = !_class_access_flags.is_interface() ? klassItable::compute_itable_size(_transitive_interfaces) : 0;\n+\n+    ResourceMark rm; \/\/ For FieldLayoutInfo contents\n+    FieldLayoutInfo field_layout_info = compute_field_layout(class_name); \/\/ Also fills offsets in _field_infos\n+    _field_info_stream = FieldInfoStream::create_FieldInfoStream(&_field_infos, _java_fields_num, _injected_fields_num, _loader_data, CHECK);\n+    _field_infos.clear_and_deallocate(); \/\/ Don't need them anymore\n+    guarantee(field_layout_info._has_nonstatic_fields == _ik_flags.has_nonstatic_fields(),\n+              \"internal instance class flag 'has nonstatic fields' dumped with incorrect value: expected %s\",\n+              BOOL_TO_STR(field_layout_info._has_nonstatic_fields));\n+\n+    const InstanceKlassSizes ik_sizes{vtable_size, itable_size, field_layout_info._instance_size,\n+                                      field_layout_info._static_field_size,\n+                                      field_layout_info.oop_map_blocks->_nonstatic_oop_map_count};\n+    InstanceKlass &ik = *InstanceKlass::allocate_instance_klass(_loader_data, class_name, _super, _class_access_flags, ik_sizes, CHECK);\n+    _ik = &ik; \/\/ Set eagerly to get it deallocated in case of a error\n+\n+    \/\/ Fill the allocated class\n+\n+    ik.set_class_loader_data(_loader_data);\n+    ik.set_name(class_name);\n+\n+    _loader_data->add_class(&ik, \/* publicize = *\/ false);\n+\n+    ik.set_internal_flags(_ik_flags);\n+\n+    ik.set_nonstatic_field_size(field_layout_info._nonstatic_field_size);\n+    ik.set_static_oop_field_count(_static_oop_fields_num);\n+    \/\/ has_nonstatic_fields is set via internal class flags\n+\n+    move_data_to_class(CHECK); \/\/ Cannot use the majority of the parser's fields from this point on\n+\n+    \/\/ These require constant pool to be set\n+    ik.set_source_file_name_index(_source_file_name_index);\n+    ik.set_generic_signature_index(_generic_signature_index);\n+    ik.set_nest_host_index(_nest_host_index);\n+\n+    \/\/ Method-related flags (including has_miranda_methods) has already been\n+    \/\/ checked, the original method ordering has also been set\n+    \/\/ TODO JVM TI RedefineClasses support may require this to be handeled\n+    \/\/  differently (save\/restore _idnum_allocated_count or take max idnum of\n+    \/\/  all methods in this class and its previous versions)\n+    ik.set_initial_method_idnum(checked_cast<u2>(ik.methods()->length()));\n+\n+    ik.set_this_class_index(_this_class_index);\n+    \/\/ Resolution of this class index for a hidden class will be done later,\n+    \/\/ together with the rest of the class references\n+\n+    ik.set_minor_version(_minor_version);\n+    ik.set_major_version(_major_version);\n+\n+    \/\/ Note: the loader data used here is different from the one used to\n+    \/\/ allocate the class if this is a non-strong hidden class\n+    precond(!java_lang_ClassLoader::is_reflection_class_loader(ik.class_loader())); \/\/ There is a guarantee when obtaining the loader\n+    ik.set_package(ClassLoaderData::class_loader_data_or_null(ik.class_loader()), nullptr, CHECK);\n+\n+    ClassFileParser::check_methods_for_intrinsics(&ik);\n+\n+    \/\/ Update the corresponding CDS flag (which we don't save)\n+    if (_is_value_based) {\n+      ik.set_has_value_based_class_annotation();\n+    }\n+    \/\/ Other annotations- and attributes-related flags and values have already\n+    \/\/ been set\n+\n+    \/\/ Interfaces have been already set, so can do this\n+    klassItable::setup_itable_offset_table(&ik);\n+\n+    const OopMapBlocksBuilder &oop_map_blocks = *field_layout_info.oop_map_blocks;\n+    if (oop_map_blocks._nonstatic_oop_map_count > 0) {\n+      oop_map_blocks.copy(ik.start_of_nonstatic_oop_maps());\n+    }\n+\n+    ClassFileParser::check_can_allocate_fast(&ik);\n+    \/\/ Other \"precomputed\" flags have been checked\/set already\n+\n+    \/\/ Access control checks are skipped for simplicity (if no one tampered with\n+    \/\/ the dump, this should've been checked when loading the class)\n+\n+    precond(ik.is_being_restored()); \/\/ Makes create_mirror() omit static field initialization\n+    java_lang_Class::create_mirror(&ik,\n+                                   Handle(thread, _loader_data->class_loader()),\n+                                   Handle(thread, ik.module()->module()),\n+                                   Handle(), Handle(), \/\/ Prot. domain and class data -- to be restored later\n+                                   CHECK);\n+\n+    if (ik.default_methods() != nullptr) {\n+      precond(ik.has_nonstatic_concrete_methods());\n+      Method::sort_methods(ik.default_methods(), \/*set_idnums=*\/ false);\n+      ik.create_new_default_vtable_indices(ik.default_methods()->length(), CHECK);\n+    }\n+\n+    \/\/ TODO JVMTI redefine\/retransform support: if the class was changed by a\n+    \/\/  class loading hook, set has_default_read_edges flag for its module\n+    \/\/  (that's what ClassFileParser does)\n+\n+    ClassLoadingService::notify_class_loaded(&ik, false);\n+\n+    JFR_ONLY(INIT_ID(&ik));\n+\n+    JVMTI_ONLY(ik.constants()->set_version(_redefinition_version));\n+\n+    DEBUG_ONLY(ik.verify());\n+\n+    if (log_is_enabled(Debug, crac, class, parser)) {\n+      log_debug(crac, class, parser)(\"  Instance class created: %s\", ik.external_name());\n+    }\n+  }\n+\n+  void transfer_resolution_errors() {\n+    precond(_ik != nullptr);\n+    const constantPoolHandle cph(Thread::current(), _ik->constants());\n+    for (int i = 0; i < _resolution_errors.length(); i++) {\n+      ResolutionError &err = *_resolution_errors.adr_at(i);\n+#ifdef ASSERT\n+      {\n+        MutexLocker ml(Thread::current(), SystemDictionary_lock); \/\/ ResolutionErrorTable requires this to be locked\n+        assert(ResolutionErrorTable::find_entry(cph, err.err_table_index) == nullptr, \"duplicated resolution error\");\n+      }\n+#endif \/\/ ASSERT\n+      SystemDictionary::add_resolution_error(cph, err.err_table_index, err.error, err.message, err.cause, err.cause_message);\n+      if (err.err_table_index == _ik->nest_host_index() && _nest_host_error_message != nullptr) {\n+        SystemDictionary::add_nest_host_error(cph, err.err_table_index, _nest_host_error_message);\n+        _nest_host_error_message = nullptr; \/\/ ResolutionErrorTable is now responsible for the deallocation\n+      }\n+    }\n+    guarantee(_nest_host_error_message == nullptr, \"lost NestHost resolution error\");\n+    _resolution_errors.clear_and_deallocate();\n+  }\n+};\n+\n+void CracClassDumpParser::parse(const char *path, const ParsedHeapDump &heap_dump,\n+                                ClassLoaderProvider *loader_provider,\n+                                HeapDumpTable<InstanceKlass *, AnyObj::C_HEAP> *iks,\n+                                HeapDumpTable<ArrayKlass *, AnyObj::C_HEAP> *aks,\n+                                HeapDumpTable<UnfilledClassInfo, AnyObj::C_HEAP> *unfilled_infos, TRAPS) {\n+  precond(path != nullptr && loader_provider != nullptr && iks != nullptr && aks != nullptr);\n+  log_info(crac, class, parser)(\"Started parsing class dump %s\", path);\n+\n+  FileBasicTypeReader reader;\n+  if (!reader.open(path)) {\n+    THROW_MSG(vmSymbols::java_lang_IllegalArgumentException(),\n+              err_msg(\"Cannot open %s for reading: %s\", path, os::strerror(errno)));\n+  }\n+\n+  CracClassDumpParser dump_parser(&reader, heap_dump, loader_provider, iks, aks, unfilled_infos, THREAD);\n+  if (HAS_PENDING_EXCEPTION) {\n+    Handle cause(Thread::current(), PENDING_EXCEPTION);\n+    CLEAR_PENDING_EXCEPTION;\n+    THROW_MSG_CAUSE(vmSymbols::java_lang_IllegalArgumentException(),\n+                    err_msg(\"Failed to create classes from dump %s\", path), cause);\n+  } else {\n+    log_info(crac, class, parser)(\"Successfully parsed class dump %s\", path);\n+  }\n+}\n+\n+Method *CracClassDumpParser::find_method(InstanceKlass *holder,\n+                                         Symbol *name, Symbol *signature, MethodKind::Enum kind,\n+                                         bool lookup_signature_polymorphic, TRAPS) {\n+  precond(holder != nullptr);\n+  if (lookup_signature_polymorphic && MethodHandles::is_signature_polymorphic_intrinsic_name(holder, name)) {\n+    \/\/ Signature polymorphic methods' specializations are dynamically generated,\n+    \/\/ but we only need to treat the basic (non-generic, intrinsic) ones\n+    \/\/ specially because the rest are generated as classes that should be in the\n+    \/\/ dump\n+    return LinkResolver::resolve_intrinsic_polymorphic_method(holder, name, signature, THREAD);\n+  }\n+  return holder->find_local_method(name, signature,\n+                                   MethodKind::as_overpass_lookup_mode(kind),\n+                                   MethodKind::as_static_lookup_mode(kind),\n+                                   Klass::PrivateLookupMode::find);\n+}\n+\n+CracClassDumpParser::CracClassDumpParser(BasicTypeReader *reader, const ParsedHeapDump &heap_dump, ClassLoaderProvider *loader_provider,\n+                                         HeapDumpTable<InstanceKlass *, AnyObj::C_HEAP> *iks, HeapDumpTable<ArrayKlass *, AnyObj::C_HEAP> *aks,\n+                                         HeapDumpTable<UnfilledClassInfo, AnyObj::C_HEAP> *unfilled_infos, TRAPS) :\n+    ClassDumpReader(reader), _heap_dump(heap_dump), _loader_provider(loader_provider), _iks(iks), _aks(aks), _unfilled_infos(unfilled_infos) {\n+  if (Arguments::is_dumping_archive()) {\n+    \/\/ TODO should do something like ClassLoader::record_result() after loading each class\n+    log_warning(crac, class, parser, cds)(\"Classes restored by CRaC will not be included into the CDS archive\");\n+  }\n+  parse_header(CHECK);\n+  parse_primitive_array_classes(CHECK);\n+  {\n+    ResourceMark rm;\n+    const GrowableArray<Pair<HeapDump::ID, InterclassRefs>> &interclass_refs = parse_instance_and_obj_array_classes(CHECK);\n+    for (const Pair<HeapDump::ID, InterclassRefs> &id_to_refs : interclass_refs) {\n+      CracClassStateRestorer::fill_interclass_references(*_iks->get(id_to_refs.first), _heap_dump, *_iks, *_aks, id_to_refs.second, CHECK);\n+    }\n+  }\n+  parse_initiating_loaders(CHECK);\n+};\n+\n+void CracClassDumpParser::parse_header(TRAPS) {\n+  constexpr char HEADER_STR[] = \"CRAC CLASS DUMP 0.1\";\n+\n+  char header_str[sizeof(HEADER_STR)];\n+  read_raw(header_str, sizeof(header_str), CHECK);\n+  header_str[sizeof(header_str) - 1] = '\\0'; \/\/ Ensure nul-terminated\n+  if (strcmp(header_str, HEADER_STR) != 0) {\n+    THROW_MSG(vmSymbols::java_lang_IllegalArgumentException(), err_msg(\"Unknown header string: %s\", header_str));\n+  }\n+\n+  const auto id_size = read<u2>(CHECK);\n+  set_id_size(id_size, CHECK);\n+\n+  const auto compressed_vm_options = read<u1>(CHECK);\n+  guarantee(CracClassDump::is_vm_options(compressed_vm_options), \"unrecognized VM options\");\n+\n+  const bool was_sync_on_value_based_classes_diagnosed = is_set_nth_bit(compressed_vm_options, CracClassDump::is_sync_on_value_based_classes_diagnosed_shift);\n+  const bool were_all_annotation_preserved = is_set_nth_bit(compressed_vm_options, CracClassDump::are_all_annotations_preserved_shift);\n+  if ((DiagnoseSyncOnValueBasedClasses != 0) && !was_sync_on_value_based_classes_diagnosed) {\n+    if (!were_all_annotation_preserved) {\n+      \/\/ TODO either save the InstanceKlass::is_value_based() flag regardless\n+      \/\/  of that option (like CDS does via\n+      \/\/  Klass::has_value_based_class_annotation()) or parse annotations of\n+      \/\/  each class to recompute InstanceKlass::is_value_based()\n+      log_warning(crac, class, parser)(\"Checkpointed VM wasn't diagnosing syncronization on value-based classes, but this VM is requested to (by the corresponding option). \"\n+                                       \"This will not be fulfullied for the restored classes.\");\n+    } else {\n+      log_warning(crac, class, parser)(\"Checkpointed VM wasn't diagnosing syncronization on value-based classes, but this VM is requested to (by the corresponding option). \"\n+                                       \"This will not be fulfullied for the restored classes because the checkpointed VM also preserved RuntimeInvisibleAnnotations \"\n+                                       \"making them indistinguishable from RuntimeVisibleAnnotations.\");\n+    }\n+  }\n+  if (were_all_annotation_preserved != PreserveAllAnnotations) {\n+    log_warning(crac, class, parser)(\"Checkpointed VM %s, but this VM is requested to %s them (by the corresponding option). \"\n+                                      \"This will not be fulfullied for the restored classes.\",\n+                                      were_all_annotation_preserved ? \"preserved RuntimeInvisibleAnnotations making them indistinguishable from RuntimeVisibleAnnotations\" :\n+                                                                      \"didn't preserve RuntimeInvisibleAnnotations\",\n+                                      PreserveAllAnnotations        ? \"preserve\" :\n+                                                                      \"omit\");\n+  }\n+\n+  log_debug(crac, class, parser)(\"Parsed class dump header: ID size = %i\", id_size);\n+}\n+\n+void CracClassDumpParser::parse_obj_array_classes(Klass *bottom_class, TRAPS) {\n+  precond(bottom_class->is_instance_klass() || bottom_class->is_typeArray_klass());\n+  Klass *cur_k = bottom_class;\n+  const auto num_arrays = read<u1>(CHECK);\n+  for (u1 i = 0; i < num_arrays; i++) {\n+    const HeapDump::ID obj_array_class_id = read_id(false, CHECK);\n+    ArrayKlass *const ak = cur_k->array_klass(CHECK);\n+    precond(!_aks->contains(obj_array_class_id));\n+    _aks->put_when_absent(obj_array_class_id, ak);\n+    cur_k = ak;\n+  }\n+\n+  if (log_is_enabled(Trace, crac, class, parser)) {\n+    ResourceMark rm;\n+    log_trace(crac, class, parser)(\"Parsed object array classes with bottom class %s\", bottom_class->external_name());\n+  }\n+}\n+\n+void CracClassDumpParser::parse_primitive_array_classes(TRAPS) {\n+  precond(Universe::is_fully_initialized());\n+  for (u1 t = JVM_T_BOOLEAN; t <= JVM_T_LONG; t++) {\n+    const HeapDump::ID prim_array_class_id = read_id(false, CHECK);\n+    Klass *const tak = Universe::typeArrayKlassObj(static_cast<BasicType>(t));\n+    precond(!_aks->contains(prim_array_class_id));\n+    _aks->put_when_absent(prim_array_class_id, TypeArrayKlass::cast(tak));\n+    parse_obj_array_classes(tak, CHECK);\n+  }\n+  {\n+    const HeapDump::ID filler_array_class_id = read_id(false, CHECK);\n+    Klass *const tak = Universe::fillerArrayKlassObj();\n+    precond(!_aks->contains(filler_array_class_id));\n+    _aks->put_when_absent(filler_array_class_id, TypeArrayKlass::cast(tak));\n+    parse_obj_array_classes(tak, CHECK);\n+  }\n+  log_debug(crac, class, parser)(\"Parsed primitive array classes\");\n+}\n+\n+struct CracClassDumpParser::ClassPreamble {\n+  HeapDump::ID class_id = HeapDump::NULL_ID;\n+  CracClassDump::ClassLoadingKind loading_kind;\n+};\n+\n+CracClassDumpParser::ClassPreamble CracClassDumpParser::parse_instance_class_preamble(TRAPS) {\n+  const HeapDump::ID class_id = read_id(true, CHECK_({}));\n+  if (class_id == HeapDump::NULL_ID) {\n+    return {};\n+  }\n+  assert(!_iks->contains(class_id), \"class \" HDID_FORMAT \" is repeated\", class_id);\n+\n+  const auto loading_kind = read<u1>(CHECK_({}));\n+  guarantee(CracClassDump::is_class_loading_kind(loading_kind), \"class \" HDID_FORMAT \" has unrecognized loading kind %i\",\n+            class_id, loading_kind);\n+\n+  log_debug(crac, class, parser)(\"Parsed instance class preamble: ID \" HDID_FORMAT \", loading kind %i\",\n+                                  class_id, loading_kind);\n+  return {class_id, checked_cast<CracClassDump::ClassLoadingKind>(loading_kind)};\n+}\n+\n+Handle CracClassDumpParser::get_class_loader(HeapDump::ID loader_id, TRAPS) {\n+#ifdef ASSERT\n+  if (loader_id != HeapDump::NULL_ID) {\n+    const HeapDump::InstanceDump &loader_dump = _heap_dump.get_instance_dump(loader_id);\n+    assert(_iks->contains(loader_dump.class_id), \"incorrect dump order: class dumped before its class loader\");\n+    const InstanceKlass &loader_class = **_iks->get(loader_dump.class_id);\n+    if (!loader_class.is_being_restored()) {\n+      precond(loader_class.is_initialized() || loader_class.is_in_error_state());\n+      assert(loader_class.is_initialized(), \"class loader \" HDID_FORMAT \" cannot be used to load classes: \"\n+             \"its class %s has failed to initialize\", loader_id, loader_class.external_name());\n+    } else {\n+      precond(_unfilled_infos->contains(loader_dump.class_id));\n+      assert(_unfilled_infos->get(loader_dump.class_id)->target_state == InstanceKlass::fully_initialized,\n+             \"class loader \" HDID_FORMAT \" cannot be used to load classes: its class %s was not initialized at dump time\",\n+             loader_id, loader_class.external_name());\n+    }\n+  }\n+#endif\n+  const Handle class_loader = _loader_provider->get_class_loader(loader_id, CHECK_NH);\n+  postcond(class_loader.is_null() || class_loader->klass()->is_class_loader_instance_klass());\n+  guarantee(!java_lang_ClassLoader::is_reflection_class_loader(class_loader()),\n+            \"defining loader must be a non-reflection one\");\n+  return class_loader;\n+}\n+\n+InstanceKlass *CracClassDumpParser::parse_and_define_instance_class(const HeapDump::ClassDump &class_dump,\n+                                                                    ClassLoaderData *loader_data,\n+                                                                    InterclassRefs *refs_out, TRAPS) {\n+  const CracInstanceClassDumpParser ik_parser(id_size(), reader(), _heap_dump, *_iks, class_dump, loader_data, THREAD);\n+  if (HAS_PENDING_EXCEPTION) {\n+    const Handle cause(Thread::current(), PENDING_EXCEPTION);\n+    CLEAR_PENDING_EXCEPTION;\n+    THROW_MSG_CAUSE_(vmSymbols::java_lang_Exception(), err_msg(\"Cannot create class \" HDID_FORMAT, class_dump.id), cause, {});\n+  }\n+\n+  InstanceKlass *const ik = CracClassStateRestorer::define_created_class(ik_parser.created_class(), ik_parser.class_state(), CHECK_NULL);\n+  precond(!_iks->contains(class_dump.id));\n+  _iks->put_when_absent(class_dump.id, ik);\n+  _iks->maybe_grow();\n+\n+  precond(!_unfilled_infos->contains(class_dump.id));\n+  if (ik->is_being_restored()) {\n+    _unfilled_infos->put_when_absent(class_dump.id, {ik_parser.class_state(), ik_parser.class_initialization_error_id()});\n+    _unfilled_infos->maybe_grow();\n+  }\n+\n+  *refs_out = ik_parser.interclass_references();\n+\n+  return ik;\n+}\n+\n+GrowableArray<Pair<HeapDump::ID, InterclassRefs>> CracClassDumpParser::parse_instance_and_obj_array_classes(TRAPS) {\n+  GrowableArray<Pair<HeapDump::ID, InterclassRefs>> interclass_refs;\n+  for (ClassPreamble preamble = parse_instance_class_preamble(THREAD);\n+       !HAS_PENDING_EXCEPTION && preamble.class_id != HeapDump::NULL_ID;\n+       preamble = parse_instance_class_preamble(THREAD)) {\n+    assert(!_iks->contains(preamble.class_id), \"instance class \" HDID_FORMAT \" dumped multiple times\", preamble.class_id);\n+\n+    const HeapDump::ClassDump *class_dump = _heap_dump.class_dumps.get(preamble.class_id);\n+    guarantee(class_dump != nullptr, \"class \" HDID_FORMAT \" not found in heap dump\", preamble.class_id);\n+\n+    \/\/ TODO What to do with hidden classes? They have uniquely-generated names,\n+    \/\/  so we won't find them by (class loader, class name) pair even if we\n+    \/\/  iterate through all CLDs of the loader and all classes recorded it these\n+    \/\/  CLD's class lists. This is a problem since we'll restore such classes\n+    \/\/  even if they exist thus duplicating them.\n+\n+    const Handle loader = get_class_loader(class_dump->class_loader_id, CHECK_({}));\n+    \/\/ First register the class loader to ensure its CLD is created\n+    SystemDictionary::register_loader(loader, false);\n+    \/\/ Then get a CLD for this particular class (differs from the above for non-strong hidden classes)\n+    ClassLoaderData *const loader_data = SystemDictionary::register_loader(loader, preamble.loading_kind == CracClassDump::ClassLoadingKind::NON_STRONG_HIDDEN);\n+\n+    InterclassRefs refs;\n+    InstanceKlass *const ik = parse_and_define_instance_class(*class_dump, loader_data, &refs, CHECK_({}));\n+    interclass_refs.append({class_dump->id, refs});\n+\n+    parse_obj_array_classes(ik, CHECK_({}));\n+  }\n+  return interclass_refs;\n+}\n+\n+void CracClassDumpParser::parse_initiating_loaders(TRAPS) {\n+  for (HeapDump::ID loader_id = read_id(true, THREAD);\n+       !HAS_PENDING_EXCEPTION && loader_id != HeapDump::NULL_ID;\n+       loader_id = read_id(true, THREAD)) {\n+    guarantee(loader_id != HeapDump::NULL_ID, \"bootstrap loader cannot be a non-defining initiating loader\");\n+    const Handle loader = get_class_loader(loader_id, CHECK);\n+    assert(loader->klass()->is_class_loader_instance_klass(), HDID_FORMAT \" cannot be an initiating loader: \"\n+           \"its class is %s which is not a class loader class\", loader_id, loader->klass()->external_name());\n+    const auto initiated_classes_num = read<jint>(CHECK);\n+    guarantee(initiated_classes_num >= 0, \"amount of initiated classes cannot be negative\");\n+    for (jint i = 0; i < initiated_classes_num; i++) {\n+      const HeapDump::ID class_id = read_id(false, CHECK);\n+      InstanceKlass **const ik = _iks->get(class_id);\n+      guarantee(ik != nullptr, \"unknown class \" HDID_FORMAT \" dumped as initiated by class loader \" HDID_FORMAT, class_id, loader_id);\n+      SystemDictionary::record_initiating_loader(*ik, loader, CHECK);\n+      log_trace(crac, class)(\"Recorded %s as initiating loader of %s defined by %s\",\n+                             java_lang_ClassLoader::loader_data(loader())->loader_name_and_id(),\n+                             (*ik)->external_name(), (*ik)->class_loader_data()->loader_name_and_id());\n+    }\n+  }\n+  log_debug(crac, class, parser)(\"Parsed initiating loaders\");\n+}\n","filename":"src\/hotspot\/share\/runtime\/cracClassDumpParser.cpp","additions":1972,"deletions":0,"binary":false,"changes":1972,"status":"added"},{"patch":"@@ -0,0 +1,97 @@\n+#ifndef SHARE_RUNTIME_CRACCLASSDUMPPARSER_HPP\n+#define SHARE_RUNTIME_CRACCLASSDUMPPARSER_HPP\n+\n+#include \"memory\/allocation.hpp\"\n+#include \"oops\/instanceKlass.hpp\"\n+#include \"oops\/oopsHierarchy.hpp\"\n+#include \"runtime\/handles.hpp\"\n+#include \"utilities\/basicTypeReader.hpp\"\n+#include \"utilities\/exceptions.hpp\"\n+#include \"utilities\/growableArray.hpp\"\n+#include \"utilities\/heapDumpParser.hpp\"\n+#include \"utilities\/methodKind.hpp\"\n+#include \"utilities\/pair.hpp\"\n+\n+\/\/ Parsed class info that cannot be applied when parsing the dump.\n+struct UnfilledClassInfo {\n+  InstanceKlass::ClassState target_state;     \/\/ State of the class at dump time\n+  HeapDump::ID class_initialization_error_id; \/\/ Exception object ID\n+};\n+\n+\/\/ Convenience BasicTypeReader wrapper.\n+class ClassDumpReader {\n+ private:\n+  BasicTypeReader *const _reader;\n+  u2 _id_size;\n+\n+ protected:\n+  explicit ClassDumpReader(BasicTypeReader *reader, u2 id_size = 0) : _reader(reader), _id_size(id_size) {\n+    assert(_id_size == 0 \/* unset *\/ || is_supported_id_size(id_size), \"unsupported ID size\");\n+  };\n+\n+  static constexpr bool is_supported_id_size(u2 size) {\n+    return size == sizeof(u8) || size == sizeof(u4) || size == sizeof(u2) || size == sizeof(u1);\n+  }\n+\n+  BasicTypeReader *reader() { return _reader; }\n+  u2 id_size() const        { precond(_id_size > 0); return _id_size; }\n+  void set_id_size(u2 value, TRAPS);\n+\n+  void read_raw(void *buf, size_t size, TRAPS);\n+  template <class T> T read(TRAPS);\n+  bool read_bool(TRAPS);\n+  HeapDump::ID read_id(bool can_be_null, TRAPS);\n+  void skip(size_t size, TRAPS);\n+};\n+\n+class ClassLoaderProvider;\n+struct InterclassRefs;\n+\n+\/\/ Parses a CRaC class dump created and restores classes based on it without\n+\/\/ calling their class loaders.\n+\/\/\n+\/\/ Note: to improve the restoration performance it is assumed that the dump\n+\/\/ comes from a trusted source and thus only basic correctness checks are\n+\/\/ performed (and the VM will die if those fail).\n+class CracClassDumpParser: public ClassDumpReader {\n+ public:\n+  static void parse(const char *path, const ParsedHeapDump &heap_dump, ClassLoaderProvider *loader_provider,\n+                    HeapDumpTable<InstanceKlass *,   AnyObj::C_HEAP> *created_iks,\n+                    HeapDumpTable<ArrayKlass *,      AnyObj::C_HEAP> *created_aks,\n+                    HeapDumpTable<UnfilledClassInfo, AnyObj::C_HEAP> *unfilled_infos, TRAPS);\n+\n+  \/\/ Finds a normal or a non-generic signature polymorphic method.\n+  \/\/ TODO find a better place for this\n+  static Method *find_method(InstanceKlass *holder,\n+                             Symbol *name, Symbol *signature, MethodKind::Enum kind,\n+                             bool lookup_signature_polymorphic, TRAPS);\n+\n+ private:\n+  const ParsedHeapDump &_heap_dump;\n+  ClassLoaderProvider *const _loader_provider;\n+\n+  \/\/ Not resource-allocated because that would limit parser's usage of resource area\n+  HeapDumpTable<InstanceKlass *,   AnyObj::C_HEAP> *const _iks;\n+  HeapDumpTable<ArrayKlass *,      AnyObj::C_HEAP> *const _aks;\n+  HeapDumpTable<UnfilledClassInfo, AnyObj::C_HEAP> *const _unfilled_infos;\n+\n+  CracClassDumpParser(BasicTypeReader *reader, const ParsedHeapDump &heap_dump, ClassLoaderProvider *loader_provider,\n+                      HeapDumpTable<InstanceKlass *, AnyObj::C_HEAP> *iks, HeapDumpTable<ArrayKlass *, AnyObj::C_HEAP> *aks,\n+                      HeapDumpTable<UnfilledClassInfo, AnyObj::C_HEAP> *unfilled_infos, TRAPS);\n+\n+  struct ClassPreamble;\n+\n+  void parse_header(TRAPS);\n+  void parse_obj_array_classes(Klass *bottom_class, TRAPS);\n+  void parse_primitive_array_classes(TRAPS);\n+\n+  Handle get_class_loader(HeapDump::ID loader_id, TRAPS);\n+\n+  ClassPreamble parse_instance_class_preamble(TRAPS);\n+  InstanceKlass *parse_and_define_instance_class(const HeapDump::ClassDump &class_dump, ClassLoaderData *loader_data, InterclassRefs *refs_out, TRAPS);\n+  GrowableArray<Pair<HeapDump::ID, InterclassRefs>> parse_instance_and_obj_array_classes(TRAPS);\n+\n+  void parse_initiating_loaders(TRAPS);\n+};\n+\n+#endif \/\/ SHARE_RUNTIME_CRACCLASSDUMPPARSER_HPP\n","filename":"src\/hotspot\/share\/runtime\/cracClassDumpParser.hpp","additions":97,"deletions":0,"binary":false,"changes":97,"status":"added"},{"patch":"@@ -0,0 +1,1261 @@\n+#include \"precompiled.hpp\"\n+#include \"classfile\/classLoaderDataGraph.hpp\"\n+#include \"classfile\/dictionary.hpp\"\n+#include \"classfile\/javaClasses.hpp\"\n+#include \"classfile\/resolutionErrors.hpp\"\n+#include \"classfile\/vmClasses.hpp\"\n+#include \"classfile\/vmSymbols.hpp\"\n+#include \"classfile_constants.h\"\n+#include \"interpreter\/bytecode.hpp\"\n+#include \"interpreter\/bytecodeStream.hpp\"\n+#include \"interpreter\/bytecodes.hpp\"\n+#include \"logging\/log.hpp\"\n+#include \"memory\/allocation.hpp\"\n+#include \"memory\/iterator.hpp\"\n+#include \"memory\/resourceArea.hpp\"\n+#include \"memory\/universe.hpp\"\n+#include \"metaprogramming\/enableIf.hpp\"\n+#include \"oops\/annotations.hpp\"\n+#include \"oops\/array.hpp\"\n+#include \"oops\/constMethod.hpp\"\n+#include \"oops\/constantPool.hpp\"\n+#include \"oops\/cpCache.hpp\"\n+#include \"oops\/cpCache.inline.hpp\"\n+#include \"oops\/fieldInfo.hpp\"\n+#include \"oops\/instanceKlass.hpp\"\n+#include \"oops\/instanceKlassFlags.hpp\"\n+#include \"oops\/klass.hpp\"\n+#include \"oops\/method.hpp\"\n+#include \"oops\/methodFlags.hpp\"\n+#include \"oops\/oopsHierarchy.hpp\"\n+#include \"oops\/recordComponent.hpp\"\n+#include \"oops\/resolvedFieldEntry.hpp\"\n+#include \"oops\/resolvedIndyEntry.hpp\"\n+#include \"oops\/symbol.hpp\"\n+#include \"prims\/jvmtiExport.hpp\"\n+#include \"prims\/jvmtiRedefineClasses.hpp\"\n+#include \"runtime\/arguments.hpp\"\n+#include \"runtime\/cracClassDumper.hpp\"\n+#include \"runtime\/globals.hpp\"\n+#include \"runtime\/handles.hpp\"\n+#include \"runtime\/os.hpp\"\n+#include \"runtime\/safepoint.hpp\"\n+#include \"runtime\/thread.hpp\"\n+#include \"utilities\/accessFlags.hpp\"\n+#include \"utilities\/basicTypeWriter.hpp\"\n+#include \"utilities\/bytes.hpp\"\n+#include \"utilities\/constantTag.hpp\"\n+#include \"utilities\/debug.hpp\"\n+#include \"utilities\/globalDefinitions.hpp\"\n+#include \"utilities\/growableArray.hpp\"\n+#include \"utilities\/heapDumpParser.hpp\"\n+#include \"utilities\/macros.hpp\"\n+#include \"utilities\/methodKind.hpp\"\n+#include \"utilities\/resizeableResourceHash.hpp\"\n+#include <cstdint>\n+#include <limits>\n+#include <type_traits>\n+#ifdef INCLUDE_JVMCI\n+#include \"jvmci\/jvmci_globals.hpp\"\n+#endif\n+#ifdef ASSERT\n+#include \"jvm_constants.h\"\n+#endif \/\/ ASSERT\n+\n+\/\/ Write IDs the same way HPROF heap dumper does.\n+static bool write_symbol_id(BasicTypeWriter *writer, const Symbol *s) {\n+  return writer->write(reinterpret_cast<uintptr_t>(s));\n+}\n+static bool write_object_id(BasicTypeWriter *writer, const oop o) {\n+  return writer->write(cast_from_oop<uintptr_t>(o));\n+}\n+static bool write_class_id(BasicTypeWriter *writer, const Klass &k) {\n+  assert(cast_from_oop<uintptr_t>(k.java_mirror()) != HeapDump::NULL_ID, \"footer assumption\");\n+  return write_object_id(writer, k.java_mirror());\n+}\n+\n+class ClassDumpWriter : public KlassClosure, public CLDClosure {\n+ friend ClassLoaderDataGraph; \/\/ Provide access to do_klass()\n+ private:\n+  BasicTypeWriter *_writer;\n+  const char *_io_error_msg = nullptr;\n+  ResizeableResourceHashtable<const InstanceKlass *, bool> _dumped_classes{107, 1228891};\n+\n+ public:\n+  explicit ClassDumpWriter(BasicTypeWriter *writer) : _writer(writer) {}\n+\n+  const char *io_error_msg() const { return _io_error_msg; }\n+\n+  void write_dump() {\n+    precond(io_error_msg() == nullptr); write_header();\n+    if (io_error_msg() == nullptr)      write_primitive_array_class_ids();\n+    \/\/ Instance and object array classes. Not using loaded_classes_do() because\n+    \/\/ our filter should be quicker.\n+    if (io_error_msg() == nullptr)      write_class_loader_preparation_fields_classes();\n+    if (io_error_msg() == nullptr)      ClassLoaderDataGraph::classes_do(this);\n+    if (io_error_msg() == nullptr)      write_end_sentinel();\n+    log_debug(crac, class, dump)(\"Wrote instance and object array classes\");\n+    if (io_error_msg() == nullptr)      ClassLoaderDataGraph::cld_do(this);\n+    if (io_error_msg() == nullptr)      write_end_sentinel();\n+    log_debug(crac, class, dump)(\"Wrote initiating class loaders info\");\n+    \/\/ TODO write class loading constraints\n+  }\n+\n+ private:\n+  \/\/ ###########################################################################\n+  \/\/ Helpers\n+  \/\/ ###########################################################################\n+\n+#define WRITE(value)           do { if (!_writer->write(value))           { _io_error_msg = os::strerror(errno); return; } } while (false)\n+#define WRITE_SYMBOL_ID(value) do { if (!write_symbol_id(_writer, value)) { _io_error_msg = os::strerror(errno); return; } } while (false)\n+#define WRITE_OBJECT_ID(value) do { if (!write_object_id(_writer, value)) { _io_error_msg = os::strerror(errno); return; } } while (false)\n+#define WRITE_CLASS_ID(value)  do { if (!write_class_id(_writer, value))  { _io_error_msg = os::strerror(errno); return; } } while (false)\n+#define WRITE_RAW(buf, size)   do { if (!_writer->write_raw(buf, size))   { _io_error_msg = os::strerror(errno); return; } } while (false)\n+#define DO_CHECKED(expr)       do { expr; if (_io_error_msg != nullptr) return;                                            } while (false)\n+\n+  void write_obj_array_class_ids(Klass *bottom_class) {\n+    ResourceMark rm;\n+    GrowableArray<const ObjArrayKlass *> oaks;\n+    for (auto *ak = bottom_class->array_klass_or_null();\n+         ak != nullptr;\n+         ak = ak->array_klass_or_null()) {\n+      oaks.append(ObjArrayKlass::cast(ak));\n+    }\n+\n+    assert(oaks.length() + (bottom_class->is_array_klass() ? ArrayKlass::cast(bottom_class)->dimension() : 0) <= 255,\n+           \"arrays can have up to 255 dimensions\");\n+    WRITE(checked_cast<u1>(oaks.length()));\n+    for (const ObjArrayKlass *oak : oaks) {\n+      WRITE_CLASS_ID(*oak);\n+    }\n+  }\n+\n+  template <class UINT_T, ENABLE_IF(std::is_same<UINT_T, u1>::value || std::is_same<UINT_T, u2>::value ||\n+                                    std::is_same<UINT_T, u4>::value || std::is_same<UINT_T, u8>::value)>\n+  void write_uint_array_data(const UINT_T *data, size_t length) {\n+    if (Endian::is_Java_byte_ordering_different() && sizeof(UINT_T) > 1) { \/\/ Have to convert\n+      for (size_t i = 0; i < length; i++) {\n+        WRITE(data[i]);\n+      }\n+    } else { \/\/ Can write as is\n+      WRITE_RAW(data, length * sizeof(UINT_T));\n+    }\n+  }\n+\n+  template <class UINT_T>\n+  void write_uint_array(const Array<UINT_T> *arr) {\n+    STATIC_ASSERT(std::numeric_limits<decltype(arr->length())>::max() < CracClassDump::NO_ARRAY_SENTINEL);\n+    if (arr != nullptr) {\n+      WRITE(checked_cast<u4>(arr->length()));\n+      write_uint_array_data(arr->data(), arr->length());\n+    } else {\n+      WRITE(CracClassDump::NO_ARRAY_SENTINEL);\n+    }\n+  }\n+\n+  \/\/ Note: method idnum cannot be used to identify methods within classes\n+  \/\/ because it depends on method ordering which depends on address of method's\n+  \/\/ name symbol and that is not portable.\n+  void write_method_identification(const Method &m) {\n+    assert(!m.is_old(), \"old methods require holder's redifinition version to also be written\");\n+    WRITE_CLASS_ID(*m.method_holder());\n+    WRITE_SYMBOL_ID(m.name());\n+    WRITE_SYMBOL_ID(m.signature());\n+    WRITE(checked_cast<u1>(MethodKind::of_method(m)));\n+  }\n+\n+  \/\/ ###########################################################################\n+  \/\/ Sections\n+  \/\/ ###########################################################################\n+\n+  \/\/ Creates a bit mask with VM options\/capabilities that influence data stored\n+  \/\/ in classes.\n+  static u1 compress_class_related_vm_options() {\n+    \/\/ Warn about options\/capabilities that may lead to unrecoverable data loss\n+    \/\/ TODO make HotSpot retain this data if a portable checkpoint was requested\n+    if (!JvmtiExport::can_get_source_debug_extension()) {\n+      \/\/ ClassFileParser skips SourceDebugExtension class attribute in such case\n+      log_warning(crac, class, dump, jvmti)(\"SourceDebugExtension class attribute will not be dumped: JVM TI's 'can_get_source_debug_extension' capability is unsupported\");\n+    }\n+    if (JvmtiExport::has_redefined_a_class() && !JvmtiExport::can_maintain_original_method_order() && !Arguments::is_dumping_archive()) {\n+      \/\/ ClassFileParser doesn't save the original method order in such case\n+      log_warning(crac, class, dump, jvmti, cds)(\"Original method order of classes redefined via JVM TI will not be dumped: neither JVM TI's 'can_maintain_original_method_order' capability is supported, nor a CDS archive is to be created\");\n+    }\n+    if (!vmClasses::Parameter_klass_loaded()) {\n+      \/\/ ClassFileParser doesn't save MethodParameters method attribute in such case\n+      \/\/ TODO Why can ClassFileParser do this safely? What if j.l.reflect.Parameter gets loaded after a class is loaded?\n+      ResourceMark rm;\n+      log_warning(crac, class, dump, jvmti, cds)(\"MethodParameters method attribute will not be dumped: parameter reflection hasn't been used (%s is not loaded)\", vmSymbols::java_lang_reflect_Parameter()->as_klass_external_name());\n+    }\n+\n+    \/\/ Check development options -- these should have the expected values in\n+    \/\/ product builds, so no warnings\n+    guarantee(LoadLineNumberTables, \"line number tables cannot be dumped\");\n+    guarantee(LoadLocalVariableTables, \"local variable tables cannot be dumped\");\n+    guarantee(LoadLocalVariableTypeTables, \"local variable type tables cannot be dumped\");\n+    \/\/ BinarySwitchThreshold is also used (in bytecode rewriting) but there is no meaningful value to assert the equality\n+\n+    \/\/ Return options\/capabilities that may lead to recoverable data loss\n+    \/\/ - Not including InstanceKlass::is_finalization_enabled() even though\n+    \/\/   it influences JVM_ACC_HAS_FINALIZER flag (internal in Klass) since it's\n+    \/\/   easily recomputed when parsing methods\n+    using offsets = CracClassDump::VMOptionShift;\n+    return\n+      \/\/ If false JVM_ACC_IS_VALUE_BASED_CLASS flags (internal in Klass'es access_flags) isn't set\n+      static_cast<u1>(DiagnoseSyncOnValueBasedClasses != 0) << offsets::is_sync_on_value_based_classes_diagnosed_shift |\n+      \/\/ If false runtime-invisible annotations are preserved are lost (otherwise they become indistinguishable from the visible ones)\n+      static_cast<u1>(PreserveAllAnnotations)               << offsets::are_all_annotations_preserved_shift;\n+  }\n+\n+  void write_header() {\n+    constexpr char HEADER_STR[] = \"CRAC CLASS DUMP 0.1\";\n+    WRITE_RAW(HEADER_STR, sizeof(HEADER_STR));\n+    WRITE(checked_cast<u2>(oopSize));\n+    WRITE(compress_class_related_vm_options()); \/\/ Also prints warnings if needed\n+    log_debug(crac, class, dump)(\"Wrote class dump header\");\n+  }\n+\n+  void write_primitive_array_class_ids() {\n+    precond(Universe::is_fully_initialized());\n+    for (u1 t = JVM_T_BOOLEAN; t <= JVM_T_LONG; t++) {\n+      log_trace(crac, class, dump)(\"Writing primitive array class ID for %s\", type2name(static_cast<BasicType>(t)));\n+      Klass *const tak = Universe::typeArrayKlassObj(static_cast<BasicType>(t));\n+      WRITE_CLASS_ID(*tak);\n+      DO_CHECKED(write_obj_array_class_ids(tak));\n+    }\n+    {\n+      log_trace(crac, class, dump)(\"Writing filler array class ID\");\n+      Klass *const tak = Universe::fillerArrayKlassObj();\n+      WRITE_CLASS_ID(*tak);\n+      DO_CHECKED(write_obj_array_class_ids(tak));\n+    }\n+    log_debug(crac, class, dump)(\"Wrote primitive array IDs\");\n+  }\n+\n+  void write_class_loader_preparation_fields_classes() {\n+    \/\/ Classes of fields required to prepare a class loader\n+    DO_CHECKED(dump_class_hierarchy(vmClasses::String_klass()));\n+    DO_CHECKED(dump_class_hierarchy(vmClasses::Module_klass()));\n+    DO_CHECKED(dump_class_hierarchy(vmClasses::ConcurrentHashMap_klass()));\n+  }\n+\n+  void do_klass(Klass *k) override {\n+    if (_io_error_msg != nullptr || !k->is_instance_klass()) return;\n+\n+    InstanceKlass *const ik = InstanceKlass::cast(k);\n+    if (ik->is_loaded() && !ik->is_scratch_class()) {\n+      dump_class_hierarchy(ik);\n+    }\n+  }\n+\n+  void write_end_sentinel() {\n+    \/\/ 1. No class would have this ID, so it marks the end of the series of\n+    \/\/    class info dumps.\n+    \/\/ 2. The bootstrap loader is the loader with null ID and we don't write it\n+    \/\/    as an initiating loader, so it marks the end of the series of\n+    \/\/    initiating loader info dumps.\n+    WRITE(HeapDump::NULL_ID);\n+  }\n+\n+  \/\/ ###########################################################################\n+  \/\/ General instance class data\n+  \/\/ ###########################################################################\n+\n+  static CracClassDump::ClassLoadingKind loading_kind(const InstanceKlass &ik) {\n+    using Kind = CracClassDump::ClassLoadingKind;\n+    if (!ik.is_hidden()) {\n+      return Kind::NORMAL;\n+    }\n+    if (ik.is_non_strong_hidden()) {\n+      return Kind::NON_STRONG_HIDDEN;\n+    }\n+    return Kind::STRONG_HIDDEN;\n+  }\n+\n+  \/\/ Writes access flags defined in the class file format as well as internal\n+  \/\/ Klass and InstanceKlass flags.\n+  void write_class_flags(const InstanceKlass &ik) {\n+    \/\/ Access flags defined in class file + internal flags defined in Klass\n+    u4 access_flags = ik.access_flags().as_int();\n+    \/\/ Fix an VM-options-dependent flag if we have CDS\n+    \/\/ TODO make has_value_based_class_annotation also available with CRaC\n+    if (ik.has_value_based_class_annotation() \/* only set when CDS included *\/) {\n+      access_flags |= JVM_ACC_IS_VALUE_BASED_CLASS;\n+    }\n+    WRITE(access_flags);\n+\n+    InstanceKlassFlags internal_flags = ik.internal_flags().drop_nonportable_flags(); \/\/ Copy to be mutated\n+    \/\/ Internal semi-immutable flags defined in InstanceKlass:\n+    \/\/ - Flags dependent on CDS archive dumping have been cleared by\n+    \/\/   drop_nonportable_flags() -- they need to be set when restoring based on\n+    \/\/   the VM options\n+    postcond(!internal_flags.shared_loading_failed() && internal_flags.is_shared_unregistered_class());\n+    \/\/ Note: should_verify_class flag has a complex dependency on multiple CLI\n+    \/\/ arguments and thus is not exactly portable. But it seems logical to\n+    \/\/ just save\/restore its value as is (i.e. the dumping VM decides whether to\n+    \/\/ verify or not), even though it contradicts the general VM options policy\n+    \/\/ to change the behaviour according to the options of the restoring VM.\n+    WRITE(internal_flags.flags());\n+    \/\/ Internal mutable flags (aka statuses) defined in InstanceKlass -- remove\n+    \/\/ all but has_resolved_methods and has_been_redefined:\n+    \/\/ - is_being_redefined -- we are on safepoint, so this status being true\n+    \/\/   means that the class either haven't started being redefined yet or has\n+    \/\/   been redefined already, and since we won't restore the state of the\n+    \/\/   redefinition code (which is native), we drop the flag\n+    internal_flags.set_is_being_redefined(false);\n+    \/\/ - is_scratch_class -- we skip these for the same reason as written above\n+    assert(!internal_flags.is_scratch_class(), \"should have skipped it\");\n+    \/\/ - is_marked_dependent -- is JIT-compilation-related and we don't dump\n+    \/\/   such data (at least for now)\n+    internal_flags.set_is_marked_dependent(false);\n+    \/\/ - is_being_restored -- should not see these on a safepoint\n+    assert(!internal_flags.is_being_restored(), \"should not appear on safepoint\");\n+    WRITE(internal_flags.status());\n+  }\n+\n+  void write_nest_host_attr(const InstanceKlass &ik) {\n+    \/\/ Nest host index from the class file (0 iff none).\n+    \/\/ Resolution error (if any) is dumped with the constant pool.\n+    WRITE(ik.nest_host_index());\n+\n+    \/\/ Have to additionally write the resolved class for hidden classes because\n+    \/\/ it can be a dynamic nest host which may be not the class pointed to by\n+    \/\/ the nest host index\n+    const InstanceKlass *resolved_nest_host = ik.nest_host_noresolve();\n+    if (resolved_nest_host != nullptr && ik.is_hidden()) {\n+      WRITE_CLASS_ID(*resolved_nest_host);\n+    } else {\n+      WRITE_OBJECT_ID(nullptr);\n+    }\n+  }\n+\n+  void write_source_debug_extension_attr(const char *source_debug_extension_str) {\n+    WRITE(static_cast<u1>(source_debug_extension_str != nullptr));\n+    if (source_debug_extension_str != nullptr) {\n+      const u4 len = checked_cast<u4>(strlen(source_debug_extension_str));\n+      WRITE(len);\n+      WRITE_RAW(source_debug_extension_str, len);\n+    }\n+  }\n+\n+  void write_record_attr(const Array<RecordComponent *> *record_components) {\n+    WRITE(static_cast<u1>(record_components != nullptr));\n+    if (record_components == nullptr) {\n+      return;\n+    }\n+\n+    const u2 record_components_num = checked_cast<u2>(record_components->length());\n+    WRITE(record_components_num); \/\/ u2 components_count\n+    for (int comp_i = 0; comp_i < record_components_num; comp_i++) {\n+      const RecordComponent &component = *record_components->at(comp_i);\n+      WRITE(component.name_index());\n+      WRITE(component.descriptor_index());\n+      WRITE(component.attributes_count());\n+      WRITE(component.generic_signature_index());                 \/\/ Signature, 0 iff unspecified\n+      DO_CHECKED(write_uint_array(component.annotations()));      \/\/ Runtime(In)VisibleAnnotations\n+      DO_CHECKED(write_uint_array(component.type_annotations())); \/\/ Runtime(In)VisibleTypeAnnotations\n+    }\n+  }\n+\n+  void write_class_attrs(const InstanceKlass &ik) {\n+    WRITE(ik.source_file_name_index());                        \/\/ SourceFile (0 iff none)\n+    WRITE(ik.generic_signature_index());                       \/\/ Signature (0 iff none)\n+    DO_CHECKED(write_nest_host_attr(ik));\n+    DO_CHECKED(write_uint_array(ik.nest_members() != Universe::the_empty_short_array() ? ik.nest_members() : nullptr));   \/\/ NestMembers (sentinel iff none)\n+    DO_CHECKED(write_uint_array(ik.inner_classes() != Universe::the_empty_short_array() ? ik.inner_classes() : nullptr)); \/\/ InnerClasses, possibly concatenated with EnclosingMethod (sentinel iff none)\n+    DO_CHECKED(write_source_debug_extension_attr(ik.source_debug_extension()));\n+    DO_CHECKED(write_uint_array(ik.constants()->operands()));  \/\/ BootstrapMethods (null if none)\n+    DO_CHECKED(write_record_attr(ik.record_components()));\n+    DO_CHECKED(write_uint_array(ik.permitted_subclasses() != Universe::the_empty_short_array() ? ik.permitted_subclasses() : nullptr)); \/\/ PermittedSubclasses\n+    DO_CHECKED(write_uint_array(ik.class_annotations()));      \/\/ Runtime(In)VisibleAnnotations (null if none)\n+    DO_CHECKED(write_uint_array(ik.class_type_annotations())); \/\/ Runtime(In)VisibleTypeAnnotations (null if none)\n+    \/\/ Synthetic attribute is stored in access flags, others are not available\n+  }\n+\n+  void write_resolution_error_symbols(const ResolutionErrorEntry &entry) {\n+    WRITE_SYMBOL_ID(entry.error());       \/\/ not null unless a special nest host error case\n+    WRITE_SYMBOL_ID(entry.message());     \/\/ null if no message\n+    WRITE_SYMBOL_ID(entry.cause());       \/\/ null if no cause\n+    if (entry.cause() != nullptr) {\n+      WRITE_SYMBOL_ID(entry.cause_msg()); \/\/ null if no cause message\n+    } else {\n+      assert(entry.cause_msg() == nullptr, \"must be null if there is no cause\");\n+    }\n+  }\n+\n+  \/\/ For non-nest-host resolution errors.\n+  void write_resolution_error(const ConstantPool &cp, int err_table_index) {\n+    constantPoolHandle cph(Thread::current(), const_cast<ConstantPool *>(&cp));\n+\n+    \/\/ Not using SystemDictionary::find_resolution_error() to get around the mutex used there (we're on safepoint)\n+    const ResolutionErrorEntry *entry = ResolutionErrorTable::find_entry(cph, err_table_index);\n+    assert(entry != nullptr, \"no resolution error recorded for %i\", err_table_index);\n+    assert(entry->error() != nullptr , \"recorded resolution error cannot be null for a non-nest-host error\");\n+    assert(entry->nest_host_error() == nullptr, \"not for nest host errors\");\n+\n+    write_resolution_error_symbols(*entry);\n+  }\n+\n+  \/\/ For nest host resolution errors.\n+  void write_nest_host_resolution_error_if_exists(const ConstantPool &cp) {\n+    const u2 nest_host_i = cp.pool_holder()->nest_host_index();\n+    constantPoolHandle cph(Thread::current(), const_cast<ConstantPool *>(&cp));\n+\n+    const ResolutionErrorEntry *entry = ResolutionErrorTable::find_entry(cph, nest_host_i);\n+    WRITE(static_cast<u1>(entry != nullptr));\n+    if (entry == nullptr) {\n+      return;\n+    }\n+\n+    write_resolution_error_symbols(*entry);\n+\n+    assert(entry->nest_host_error() != nullptr, \"nest host error always has this\");\n+    const auto nest_host_err_len = checked_cast<u4>(strlen(entry->nest_host_error()));\n+    WRITE(nest_host_err_len);\n+    WRITE_RAW(entry->nest_host_error(), nest_host_err_len);\n+  }\n+\n+  \/\/ Writes constant pool contents, including resolved classes and resolution\n+  \/\/ errors and excluding constant pool cache and indy resolution errors.\n+  void write_constant_pool(const ConstantPool &cp) {\n+    auto &cp_ = const_cast<ConstantPool &>(cp); \/\/ FIXME a ton of ConstantPool's methods that could've been const are not marked as such\n+    WRITE(checked_cast<u2>(cp.length()));\n+    WRITE(checked_cast<u2>(cp.resolved_klasses()->length())); \/\/ To avoid multiple passes during parsing\n+    for (int pool_i = 1 \/* index 0 is unused *\/; pool_i < cp.length(); pool_i++) {\n+      const u1 tag = cp.tag_at(pool_i).value();\n+      WRITE(tag);\n+      switch (tag) {\n+        \/\/ Fundamental structures\n+        case JVM_CONSTANT_Utf8:\n+          WRITE_SYMBOL_ID(cp.symbol_at(pool_i));\n+          break;\n+        case JVM_CONSTANT_NameAndType:\n+          WRITE(cp_.name_ref_index_at(pool_i));\n+          WRITE(cp_.signature_ref_index_at(pool_i));\n+          break;\n+\n+        \/\/ Static constants\n+        case JVM_CONSTANT_Integer: WRITE(cp_.int_at(pool_i));      break;\n+        case JVM_CONSTANT_Float:   WRITE(cp_.float_at(pool_i));    break;\n+        case JVM_CONSTANT_Long:    WRITE(cp_.long_at(pool_i++));   break; \/\/ Next index is unused, so skip it\n+        case JVM_CONSTANT_Double:  WRITE(cp_.double_at(pool_i++)); break; \/\/ Next index is unused, so skip it\n+        case JVM_CONSTANT_String:\n+          \/\/ String entries are also kind of resolved, even though they are not\n+          \/\/ considered symbolic references (JVMS §5.1):\n+          \/\/ a) until a string is queried the first time, only a Symbol* is stored\n+          WRITE_SYMBOL_ID(cp_.unresolved_string_at(pool_i)); \/\/ always not null\n+          \/\/ b) when the string is queried, a j.l.String object is created for it,\n+          \/\/    and all the later queries should return this same object -- a\n+          \/\/    reference to this object is stored in the reolved references array\n+          \/\/    of the cache (it is null until resolved)\n+          break;\n+\n+        \/\/ Symbolic references\n+        case JVM_CONSTANT_Class:\n+        case JVM_CONSTANT_UnresolvedClass:\n+        case JVM_CONSTANT_UnresolvedClassInError:\n+          \/\/ Static data\n+          WRITE(checked_cast<u2>(cp.klass_name_index_at(pool_i)));\n+          \/\/ Resolution state info\n+          if (tag == JVM_CONSTANT_Class) {\n+            \/\/ Not ConstantPool::resolved_klass_at() to get around a redundant aquire (no concurrency on safepoint)\n+            Klass *resolved_class = cp.resolved_klasses()->at(cp.klass_slot_at(pool_i).resolved_klass_index());\n+            assert(resolved_class != nullptr, \"Unresolved class in JVM_CONSTANT_Class slot\");\n+            WRITE_CLASS_ID(*resolved_class);\n+            \/\/ NestHost resolution error may happen even if the referenced class itself was successfully resolved\n+            if (pool_i == cp.pool_holder()->nest_host_index()) {\n+              DO_CHECKED(write_nest_host_resolution_error_if_exists(cp));\n+            }\n+          } else if (tag == JVM_CONSTANT_UnresolvedClassInError) {\n+            DO_CHECKED(write_resolution_error(cp, pool_i));\n+          }\n+          break;\n+        case JVM_CONSTANT_Fieldref:\n+        case JVM_CONSTANT_Methodref:\n+        case JVM_CONSTANT_InterfaceMethodref:\n+          \/\/ Static data\n+          WRITE(cp_.uncached_klass_ref_index_at(pool_i));\n+          WRITE(cp_.uncached_name_and_type_ref_index_at(pool_i));\n+          \/\/ Field\/method resolution ussually consists of:\n+          \/\/ 1. Holder class resolution --- we record results of these during\n+          \/\/    JVM_CONSTANT_(Unresolved)Class(InError) dump, so Klass* is\n+          \/\/    obtainable from the class reference dumped above.\n+          \/\/ 2. Field\/method lookup + access control --- this should produce the\n+          \/\/    same result given the same (Klass*, mathod\/field name, signature)\n+          \/\/    combination, and the last two parts are obtainable via the\n+          \/\/    NameAndType reference dumped above.\n+          \/\/ In such cases we don't need to record any resolution data.\n+          \/\/\n+          \/\/ But there is also a special case of signature-polymorphic\n+          \/\/ invokevirtual calls the resolution process of which is more like it\n+          \/\/ of InvokeDynamic resulting into an adapter method (stored in the\n+          \/\/ cache itself) and \"appendix\" object (stored in resolved references\n+          \/\/ array of the cache) being resolved.\n+          break;\n+        case JVM_CONSTANT_MethodType:\n+        case JVM_CONSTANT_MethodTypeInError:\n+          \/\/ Static data\n+          WRITE(checked_cast<u2>(cp_.method_type_index_at(pool_i)));\n+          \/\/ Resolution state info\n+          if (tag == JVM_CONSTANT_MethodTypeInError) {\n+            DO_CHECKED(write_resolution_error(cp, pool_i));\n+          } else {\n+            \/\/ MethodType object is stored in the resolved references array of the\n+            \/\/ cache (null if unresolved)\n+          }\n+          break;\n+        case JVM_CONSTANT_MethodHandle:\n+        case JVM_CONSTANT_MethodHandleInError:\n+          \/\/ Static data\n+          WRITE(checked_cast<u1>(cp_.method_handle_ref_kind_at(pool_i)));\n+          WRITE(checked_cast<u2>(cp_.method_handle_index_at(pool_i)));\n+          \/\/ Resolution state info\n+          if (tag == JVM_CONSTANT_MethodHandleInError) {\n+            DO_CHECKED(write_resolution_error(cp, pool_i));\n+          } else {\n+            \/\/ MethodHandle object is stored in the resolved references array of\n+            \/\/ the cache (null if unresolved)\n+          }\n+          break;\n+        case JVM_CONSTANT_Dynamic:\n+        case JVM_CONSTANT_DynamicInError:\n+        case JVM_CONSTANT_InvokeDynamic:\n+          \/\/ Static data\n+          WRITE(cp_.bootstrap_methods_attribute_index(pool_i));\n+          WRITE(cp_.bootstrap_name_and_type_ref_index_at(pool_i));\n+          \/\/ Resolution state info\n+          if (tag == JVM_CONSTANT_DynamicInError) {\n+            DO_CHECKED(write_resolution_error(cp, pool_i));\n+          } else {\n+            \/\/ Dynamic: computed constants are stored in the resolved references\n+            \/\/ array of the cache (null if unresolved, primitives are boxed).\n+            \/\/\n+            \/\/ InvokeDynamic:\n+            \/\/ 1. One InvokeDynamic constant pool entry can correspond to multiple\n+            \/\/    cache entries: one for each indy instruction in the class --\n+            \/\/    their info is stored in a special constant pool cache array,\n+            \/\/    appendices are stored in the resolved references array of the\n+            \/\/    cache.\n+            \/\/ 2. Error messages, if any, are written as part of cache dump.\n+          }\n+          break;\n+\n+        default: \/\/ Module tags and internal pool-construction-time tags\n+          ShouldNotReachHere();\n+      }\n+    }\n+\n+    \/\/ - Not writing flags since they are either trivial to obtain from the data\n+    \/\/   written above (has_dynamic_constant), are handeled while working with\n+    \/\/   holder's methods (has_preresolution, on_stack) or are CDS-related and\n+    \/\/   thus JVM-instance-dependent (on_stack, is_shared)\n+    \/\/ - Generic signature index, source file name index and operands are written\n+    \/\/   as the corresponding class attributes\n+  }\n+\n+  \/\/ Writes data from the constant pool cache.check won't crash with NPE when there is no\n+  \/\/  fields\/indys.\n+  void write_constant_pool_cache(const ConstantPoolCache &cp_cache) {\n+    \/\/ Write lengths of the main arrays first to be able to outright allocate\n+    \/\/ the cache when parsing:\n+    \/\/ 1. Field entries:  u2 -- same amount as of Fieldrefs.\n+    const u2 field_entries_len = cp_cache.resolved_field_entries_length();\n+    assert(field_entries_len > 0 || const_cast<ConstantPoolCache &>(cp_cache).resolved_field_entries() == nullptr,\n+           \"allocated resolved fields array is always non-empty\");\n+    WRITE(field_entries_len);\n+    \/\/ 2. Method entries: jint -- can be twice as much as methods (one per\n+    \/\/    Methodref, one or two per InterfaceMethodref) for which we need u2.\n+    WRITE(checked_cast<jint>(cp_cache.length()));\n+    \/\/ 3. Indy entries: jint -- one per indy, and there may be a lot of those in\n+    \/\/    the class, but the array is int-indexed.\n+    const jint indy_entries_len = cp_cache.resolved_indy_entries_length();\n+    assert(indy_entries_len > 0 || const_cast<ConstantPoolCache &>(cp_cache).resolved_indy_entries() == nullptr,\n+           \"allocated resolved indys array is always non-empty\");\n+    WRITE(indy_entries_len);\n+    \/\/ 4. Resolved references: doesn't influence cache allocation, so don't need\n+    \/\/    its length here.\n+\n+    for (int field_i = 0; field_i < field_entries_len; field_i++) {\n+      const ResolvedFieldEntry &field_info = *cp_cache.resolved_field_entry_at(field_i);\n+\n+      assert(field_info.constant_pool_index() > 0, \"uninitialized field entry\");\n+      WRITE(field_info.constant_pool_index());\n+\n+      const u1 get_code = field_info.get_code();\n+      const u1 put_code = field_info.put_code();\n+      assert(!(get_code == 0 && put_code != 0), \"if resolved for put, must be resolved for get as well\");\n+      WRITE(get_code);\n+      WRITE(put_code);\n+\n+      if (get_code != 0) { \/\/ If resolved, write the data\n+        assert(field_info.field_holder() != nullptr, \"must be resolved\");\n+        \/\/ field_offset is omitted since it depends on VM options and the platform\n+        WRITE_CLASS_ID(*field_info.field_holder());\n+        WRITE(field_info.field_index());\n+        WRITE(field_info.tos_state());\n+        WRITE(field_info.flags());\n+      }\n+    }\n+\n+    \/\/ Resolved methods are still stored as generic cache entries, but these\n+    \/\/ aren't used for anything else anymore (fields and indys got moved to\n+    \/\/ separate arrays). The upcoming cache changes will simplify this code.\n+    for (int cache_i = 0; cache_i < cp_cache.length(); cache_i++) {\n+      const ConstantPoolCacheEntry &info = *cp_cache.entry_at(cache_i);\n+\n+      assert(info.constant_pool_index() > 0, \"uninitialized cache entry\");\n+      WRITE(checked_cast<u2>(info.constant_pool_index()));\n+\n+      const Bytecodes::Code bytecode1 = info.bytecode_1();\n+      const Bytecodes::Code bytecode2 = info.bytecode_2();\n+      WRITE(checked_cast<u1>(bytecode1));\n+      WRITE(checked_cast<u1>(bytecode2));\n+\n+      \/\/ This will be simplified when ResolvedMethodEntry replaces ConstantPoolCacheEntry\n+      if (bytecode1 > 0 || bytecode2 > 0) { \/\/ If resolved, write the data\n+        assert(info.is_method_entry(), \"not used for field entries anymore\");\n+        assert(bytecode1 != Bytecodes::_invokedynamic, \"not used for indys anymore\");\n+        \/\/ Flags go first since they, together with the bytecodes, define the contents of f1 and f2\n+        using shifts = CracClassDump::ResolvedMethodEntryFlagShift;\n+        WRITE(checked_cast<u1>(info.has_local_signature() << shifts::has_local_signature_shift |\n+                               info.has_appendix()        << shifts::has_appendix_shift        |\n+                               info.is_forced_virtual()   << shifts::is_forced_virtual_shift   |\n+                               info.is_final()            << shifts::is_final_shift            |\n+                               info.is_vfinal()           << shifts::is_vfinal_shift));\n+        WRITE(checked_cast<u1>(info.flag_state())); \/\/ ToS state\n+        WRITE(checked_cast<u1>(info.parameter_size()));\n+        \/\/ f1\n+        switch (bytecode1) {\n+          case Bytecodes::_invokestatic:\n+          case Bytecodes::_invokespecial:\n+          case Bytecodes::_invokehandle: {\n+            const Method *method = info.f1_as_method(); \/\/ Resolved method for non-virtual calls or adapter method for invokehandle\n+            assert(method != nullptr, \"must be resolved\");\n+            assert(!method->is_old(), \"cache never contains old methods\"); \/\/ Lets us omit holder's redefinition version\n+            write_method_identification(*method);\n+            break;\n+          }\n+          case Bytecodes::_invokeinterface:\n+            if (!info.is_forced_virtual()) {\n+              const Klass *klass = info.f1_as_klass(); \/\/ Resolved interface class\n+              assert(klass != nullptr, \"must be resolved\");\n+              WRITE_CLASS_ID(*klass);\n+              break;\n+            }\n+            \/\/ Fallthrough\n+          case 0: \/\/ bytecode1 is not set\n+            assert(info.is_f1_null(), \"f1 must be unused\");\n+            break;\n+          default:\n+            ShouldNotReachHere();\n+        }\n+        \/\/ f2\n+        if (info.is_vfinal() || (bytecode1 == Bytecodes::_invokeinterface && !info.is_forced_virtual())) {\n+          \/\/ (b1 == invokeinterface && b2 == invokevirtual) is possible here\n+          assert(bytecode1 != Bytecodes::_invokestatic && bytecode1 != Bytecodes::_invokehandle, \"cannot accompany invokevirtual\");\n+          const Method *method = info.is_vfinal() ? info.f2_as_vfinal_method() : info.f2_as_interface_method(); \/\/ Resolved final or interface method\n+          assert(method != nullptr, \"must be resolved\");\n+          assert(!method->is_old(), \"cache never contains old methods\"); \/\/ Lets us omit holder's redefinition version\n+          write_method_identification(*method);\n+        } else if (bytecode1 == Bytecodes::_invokehandle \/* no need to check has_appendix() -- the index is set anyway *\/) {\n+          assert(bytecode1 != Bytecodes::_invokestatic && bytecode1 != Bytecodes::_invokevirtual && bytecode1 != Bytecodes::_invokeinterface,\n+                 \"cannot accompany invokehandle\");\n+          const int appendix_i = info.f2_as_index(); \/\/ Index of appendix in resolved references\n+          assert(appendix_i >= 0 && appendix_i < cp_cache.constant_pool()->resolved_references()->length(), \"invalid appendix index\");\n+          WRITE(checked_cast<jint>(appendix_i));\n+        } else if (bytecode2 == Bytecodes::_invokevirtual) {\n+          precond(!info.is_vfinal());\n+          assert(bytecode1 != Bytecodes::_invokestatic && bytecode1 != Bytecodes::_invokehandle && bytecode1 != Bytecodes::_invokeinterface,\n+                 \"cannot accompany invokevirtual\");\n+          \/\/ f2 is a vtable index which is not portable because vtable depends\n+          \/\/ on method ordering and that depends on Symbol table's memory layout\n+          assert(info.f2_as_index() >= 0, \"invalid vtable index\");\n+          \/\/ TODO write something that will help deduce the new vtable index faster\n+        } else {\n+          assert(info.f2_ord() == 0, \"f2 must be unused\");\n+        }\n+      } else {\n+        \/\/ invokehandle entries have f2 set even when unresolved\n+        const bool is_f2_set = info.f2_ord() != 0;\n+        WRITE(static_cast<u1>(is_f2_set));\n+        if (is_f2_set) {\n+          \/\/ Unresolved invokehandle entry\n+          const int appendix_i = info.f2_as_index(); \/\/ Index of appendix in resolved references\n+          assert(appendix_i >= 0 && appendix_i < cp_cache.constant_pool()->resolved_references()->length(), \"invalid appendix index\");\n+          WRITE(checked_cast<jint>(appendix_i));\n+        }\n+      }\n+    }\n+\n+    for (int indy_i = 0; indy_i < indy_entries_len; indy_i++) {\n+      const ResolvedIndyEntry &indy_info = *cp_cache.resolved_indy_entry_at(indy_i);\n+\n+      assert(indy_info.constant_pool_index() > 0, \"uninitialized indy entry\");\n+      WRITE(indy_info.constant_pool_index());\n+      WRITE(indy_info.resolved_references_index()); \/\/ Why is this u2? Should be int to index the whole resolved references for all indys which are int-indexed themselves.\n+      WRITE(checked_cast<u1>(indy_info.flags() | indy_info.is_resolved() << ResolvedIndyEntry::num_flags));\n+      assert((indy_info.is_resolved() && !indy_info.resolution_failed()) ||\n+             (!indy_info.is_resolved() && !indy_info.has_appendix()), \"illegal state\");\n+\n+      if (indy_info.is_resolved()) {\n+        assert(!indy_info.resolution_failed(), \"cannot be failed if succeeded\");\n+        const Method *adapter = indy_info.method();\n+        assert(adapter != nullptr, \"must be resolved\");\n+        assert(!adapter->is_old(), \"cache never contains old methods\"); \/\/ Lets us omit holder's redefinition version\n+        write_method_identification(*adapter);\n+        WRITE(indy_info.num_parameters());\n+        WRITE(indy_info.return_type());\n+      } else if (indy_info.resolution_failed()) {\n+        const int indy_res_err_i = ResolutionErrorTable::encode_cpcache_index(ConstantPool::encode_invokedynamic_index(indy_i));\n+        DO_CHECKED(write_resolution_error(*cp_cache.constant_pool(), indy_res_err_i));\n+      }\n+    }\n+\n+    \/\/ Resolved references:\n+    \/\/ - String objects created when a String constant pool entry is queried the\n+    \/\/   first time and interned\n+    \/\/ - MethodHandle objects for resolved MethodHandle constant pool entries\n+    \/\/ - MethodType objects for resolved MethodType constant pool entries\n+    \/\/ - Appendix objects created for each invokedynamic\/invokehandle bytecode,\n+    \/\/   as well as for Dynamic constant pool entries\n+    \/\/\n+    \/\/ The array itself is dumped as part of HPROF, so only write the mapping\n+    \/\/ from indices of the first part of resolved references (i.e. excluding\n+    \/\/ appendices) to constant pool indices:\n+    DO_CHECKED(write_uint_array(cp_cache.reference_map())); \/\/ u2 is enough for length (not larger than the constant pool), but using u4 for the null array sentinel\n+  }\n+\n+  void write_interfaces(const Array<InstanceKlass *> &interfaces) {\n+    WRITE(checked_cast<u2>(interfaces.length()));\n+    for (int index = 0; index < interfaces.length(); index++) {\n+      WRITE_CLASS_ID(*interfaces.at(index));\n+    }\n+  }\n+\n+  \/\/ ###########################################################################\n+  \/\/ Fields\n+  \/\/ ###########################################################################\n+\n+  void write_fields(const InstanceKlass &ik) {\n+    \/\/ Cannot write the field info stream as is, even though it is in a portable\n+    \/\/ UNSIGNED5 encoding, because it contains field offsets which aren't\n+    \/\/ portable (depend on the platform and the specified VM options) and\n+    \/\/ UNSIGNED5 doesn't allow content updates, so we cannot change the offsets\n+    \/\/ without re-encoding the whole stream\n+    \/\/ TODO measure if re-encoding will actually be slower\n+    ResourceMark rm;\n+    int java_fields_count;\n+    int injected_fields_count;\n+    const GrowableArray<FieldInfo> &infos = *FieldInfoStream::create_FieldInfoArray(ik.fieldinfo_stream(), &java_fields_count, &injected_fields_count);\n+\n+    \/\/ Field statuses (mutable VM-internal field data)\n+    const Array<FieldStatus> &statuses = *ik.fields_status();\n+\n+    const Array<AnnotationArray *> *annotations = ik.fields_annotations();\n+    const Array<AnnotationArray *> *type_annotations = ik.fields_type_annotations();\n+\n+    assert(java_fields_count + injected_fields_count == ik.total_fields_count() &&\n+           infos.length() == ik.total_fields_count() &&\n+           statuses.length() == ik.total_fields_count() &&\n+           (annotations == nullptr || annotations->length() == java_fields_count) &&\n+           (type_annotations == nullptr || type_annotations->length() == java_fields_count), \"must be\");\n+    WRITE(checked_cast<u2>(java_fields_count));\n+    WRITE(checked_cast<u2>(injected_fields_count));\n+    for (int i = 0; i < infos.length(); i++) {\n+      const FieldInfo &field_info = infos.at(i);\n+      WRITE(field_info.name_index());\n+      WRITE(field_info.signature_index());\n+      assert((field_info.access_flags().as_int() & JVM_RECOGNIZED_FIELD_MODIFIERS) == field_info.access_flags().as_int(), \"illegal field flags\");\n+      WRITE(field_info.access_flags().as_short()); \/\/ Includes Synthetic attribute\n+      WRITE(checked_cast<u1>(field_info.field_flags().as_uint()));\n+      WRITE(field_info.initializer_index());       \/\/ ConstantValue attribute\n+      WRITE(field_info.generic_signature_index()); \/\/ Signature attribute\n+      WRITE(field_info.contention_group());\n+\n+      WRITE(statuses.at(i).as_uint()); \/\/ Includes JVM TI's access\/modification watch flags\n+\n+      assert(field_info.field_flags().is_injected() == (i >= java_fields_count), \"injected fields go last\");\n+      if (i < java_fields_count) {\n+        \/\/ Runtime(In)Visible(Type)Annotations attributes: only non-injected fields have them\n+        DO_CHECKED(write_uint_array(annotations != nullptr      ? annotations->at(i)      : nullptr));\n+        DO_CHECKED(write_uint_array(type_annotations != nullptr ? type_annotations->at(i) : nullptr));\n+      }\n+    }\n+\n+    \/\/ Static fields' values aren't written since they are part of the heap dump\n+  }\n+\n+  \/\/ ###########################################################################\n+  \/\/ Methods\n+  \/\/ ###########################################################################\n+\n+  \/\/ Removes non-portable elements from method statuses (aka mutable internal\n+  \/\/ flags).\n+  static int filter_method_statuses(MethodFlags statuses \/* copy to mutate *\/, bool has_annotations) {\n+    \/\/ Clear the JIT-related bits\n+    statuses.set_queued_for_compilation(false);\n+    statuses.set_is_not_c1_compilable(false);\n+    statuses.set_is_not_c2_compilable(false);\n+    statuses.set_is_not_c2_osr_compilable(false);\n+\n+    if (JVMCI_ONLY(UseJVMCICompiler) NOT_JVMCI(false) && statuses.dont_inline()) {\n+      \/\/ dont_inline status may be set not only via the corresponding\n+      \/\/ annotation, but also by JVMCI -- in the latter case it becomes\n+      \/\/ compiler-dependent and should not be dumped (at least until we also\n+      \/\/ support JIT dumping)\n+      if (has_annotations) {\n+        \/\/ TODO need to parse RuntimeVisibleAnnotations to see if DontInline is\n+        \/\/  there and also check if the holder class can access VM annotations.\n+        \/\/  If PreserveAllAnnotations is set need to do something not to look\n+        \/\/  into RuntimeInvisibleAnnotations.\n+        ResourceMark rm;\n+        log_warning(crac, class, dump, jvmci)(\"Method marked 'don't inline' which is ambiguos when JVM CI JIT is used\");\n+      } else {\n+        \/\/ No RuntimeVisibleAnnotations, so it must have been set by JVMCI\n+        statuses.set_dont_inline(false);\n+      }\n+    }\n+\n+    return statuses.as_int();\n+  }\n+\n+  static jint get_linenumber_table_size(const ConstMethod &cmethod) {\n+    precond(cmethod.has_linenumber_table());\n+    CompressedLineNumberReadStream stream(cmethod.compressed_linenumber_table());\n+    while (stream.read_pair()) {}\n+    const jint size = stream.position();\n+    assert(size > 0, \"existing line number table cannot be empty\");\n+    return size;\n+  }\n+\n+  \/\/ Writes method's bytecodes. Internal bytecodes the usage of which is\n+  \/\/ (mostly) platform-independent are preserved (Zero interpreter will still\n+  \/\/ need some rewriting -- see the related comments below).\n+  void write_bytecodes(const Method &method) {\n+    if (!method.method_holder()->is_rewritten()) {\n+      \/\/ Can just write the whole code buffer as is\n+      assert(method.number_of_breakpoints() == 0, \"class must be linked (and thus rewritten) for breakpoints to exist\");\n+      WRITE_RAW(method.code_base(), method.code_size());\n+      return;\n+    }\n+    \/\/ Else, have to partially revert the rewriting to make the code portable:\n+    \/\/ - Bytecode rewriting done for interpreter optimization (both by the\n+    \/\/   interpreters themselves and the rewriter) depends on the interpreter\n+    \/\/   (e.g. Zero currently doesn't support some fast* internal bytecodes),\n+    \/\/   so have to revert these\n+    \/\/ - Constant pool indices of get\/put, ldc and invoke instructions are\n+    \/\/   rewritten into native byte order by the rewriter, so need to rewrite\n+    \/\/   them back if native endianness differs from Java's\n+    \/\/ TODO decide what to do with breakpoints (for now they are cleared)\n+\n+    \/\/ BytecodeStream reverts internal bytecodes and breakpoints for us\n+    BytecodeStream stream(methodHandle(Thread::current(), const_cast<Method *>(&method)));\n+    for (Bytecodes::Code code = stream.next(); code >= 0; code = stream.next()) {\n+      const Bytecodes::Code raw_code = stream.raw_code(); \/\/ Possibly internal code but not a breakpoint\n+      precond(raw_code != Bytecodes::_breakpoint);\n+\n+      if (Bytecodes::is_field_code(code) || Bytecodes::is_invoke(code)) {\n+        \/\/ If this is actualy an invokehandle write it since it is portable\n+        WRITE(checked_cast<u1>(raw_code != Bytecodes::_invokehandle ? code : Bytecodes::_invokehandle));\n+        \/\/ Convert index byte order: u4 for invokedynamic, u2 for others\n+        if (code == Bytecodes::_invokedynamic) {\n+          WRITE(Bytes::get_native_u4(stream.bcp() + 1));\n+        } else {\n+          WRITE(Bytes::get_native_u2(stream.bcp() + 1));\n+          \/\/ invokeinterface has two additional bytes untoched by the rewriter\n+          if (code == Bytecodes::_invokeinterface) {\n+            WRITE_RAW(stream.bcp() + 3, 2);\n+          }\n+        }\n+        continue;\n+      }\n+      if (raw_code == Bytecodes::_fast_aldc || raw_code == Bytecodes::_fast_aldc_w) {\n+        \/\/ These rewritten versions of ldc and ldc_w are portable, so write them directly\n+        WRITE(checked_cast<u1>(raw_code));\n+        if (raw_code == Bytecodes::_fast_aldc) {\n+          WRITE(checked_cast<u1>(*(stream.bcp() + 1)));\n+        } else {\n+          \/\/ Also, the index needs to be converted from the native byte order\n+          WRITE(Bytes::get_native_u2(stream.bcp() + 1));\n+        }\n+        continue;\n+      }\n+      postcond(!Bytecodes::native_byte_order(code));\n+\n+      if (code == Bytecodes::_lookupswitch) {\n+        \/\/ Template interpreters expect this to always be rewritten. Zero, on\n+        \/\/ the other hand, currently doesn't support the fast versions. So we do\n+        \/\/ the rewriter's job to keep this uniform across all interpreters. The\n+        \/\/ template interpreters' way is chosen to restore faster for them.\n+#ifdef ZERO\n+        assert(raw_code == code, \"Zero doesn't support the rewriting\");\n+        Bytecode_lookupswitch switch_inspector(const_cast<Method *>(&method), stream.bcp());\n+        \/\/ The threshold is fixed in product builds, so this should be portable\n+        const Bytecodes::Code rewritten = switch_inspector.number_of_pairs() < BinarySwitchThreshold ? Bytecodes::_fast_linearswitch\n+                                                                                                     : Bytecodes::_fast_binaryswitch;\n+#else \/\/ ZERO\n+        assert(raw_code != code, \"must be already rewritten\");\n+        const Bytecodes::Code rewritten = raw_code;\n+#endif \/\/ ZERO\n+        WRITE(checked_cast<u1>(rewritten));\n+      } else if (raw_code == Bytecodes::_return_register_finalizer) {\n+        \/\/ This special case of return is portable, so write it as is\n+        WRITE(checked_cast<u1>(raw_code));\n+      } else {\n+        \/\/ Otherwise, write the code as converted and its parameters as raw\n+        WRITE(checked_cast<u1>(stream.is_wide() ? Bytecodes::_wide : code));\n+      }\n+      WRITE_RAW(stream.bcp() + 1, stream.instruction_size() - 1); \/\/ Parameters\n+    }\n+\n+    DEBUG_ONLY(ResourceMark rm;)\n+    assert(stream.is_last_bytecode(), \"error reading bytecodes of %s at index %i\", method.external_name(), stream.bci());\n+  }\n+\n+  void write_code_attr(const Method &method, u4 linenumber_table_size \/* costly to recalculate *\/) {\n+    const ConstMethod &cmethod = *method.constMethod();\n+    precond(cmethod.code_size() > 0); \/\/ Code size is dumped with the rest of the embedded method data sizes\n+\n+    WRITE(cmethod.max_stack());\n+    WRITE(cmethod.max_locals());\n+\n+    DO_CHECKED(write_bytecodes(method)); \/\/ Bytecodes with some of the internal ones preserved\n+\n+    if (cmethod.has_exception_table()) {\n+      \/\/ Length is dumped with the rest of the embedded method data sizes\n+      precond(method.exception_table_length() > 0);\n+      STATIC_ASSERT(sizeof(ExceptionTableElement) == 4 * sizeof(u2)); \/\/ Check no padding\n+      const size_t len = method.exception_table_length() * sizeof(ExceptionTableElement) \/ sizeof(u2);\n+      write_uint_array_data(reinterpret_cast<u2 *>(method.exception_table_start()), len);\n+    }\n+\n+    if (cmethod.has_linenumber_table()) {\n+      \/\/ Table size is dumped with the rest of the embedded method data sizes\n+      precond(linenumber_table_size > 0);\n+      \/\/ Linenumber table is stored in a portable compressed format (a series of\n+      \/\/ single-byte elements and UNSIGNED5-encoded ints from 0 to 65535), so can\n+      \/\/ be dumped as is\n+      WRITE_RAW(cmethod.compressed_linenumber_table(), linenumber_table_size);\n+    }\n+    if (cmethod.has_localvariable_table()) { \/\/ LocalVariableTable and LocalVariableTypeTable\n+      precond(cmethod.localvariable_table_length() > 0);\n+      \/\/ Length is dumped with the rest of the embedded method data sizes\n+      STATIC_ASSERT(sizeof(LocalVariableTableElement) == 6 * sizeof(u2)); \/\/ Check no padding\n+      const size_t len = cmethod.localvariable_table_length() * sizeof(LocalVariableTableElement) \/ sizeof(u2);\n+      write_uint_array_data(reinterpret_cast<u2 *>(cmethod.localvariable_table_start()), len);\n+    }\n+    { \/\/ StackMapTable\n+      assert(cmethod.stackmap_data() == nullptr || !cmethod.stackmap_data()->is_empty(), \"must be non-empty if exists\");\n+      DO_CHECKED(write_uint_array(cmethod.stackmap_data())); \/\/ Null if not specified\n+    }\n+    \/\/ Other code attributes are not available\n+  }\n+\n+  void write_method(const Method &method) {\n+    const ConstMethod &cmethod = *method.constMethod();\n+\n+    \/\/ Note: not writing vtable\/itable index because it is not portable (layout\n+    \/\/ of the tables depends on method ordering which depends on order of\n+    \/\/ method names' symbols in memory)\n+\n+    if (cmethod.method_idnum() != cmethod.orig_method_idnum()) {\n+      \/\/ TODO method ID is not dumped since it is not portable (depends on\n+      \/\/ method ordering which depends on method's name symbol addresses), but\n+      \/\/ what to do with the original ID? It is also non-portable but it should\n+      \/\/ probably be restored somehow...\n+      precond(method.is_obsolete()); \/\/ Implies is_old\n+      log_error(crac, class, dump)(\"Dumping old versions of redefined classes is not supported yet\");\n+      Unimplemented();\n+    }\n+\n+    \/\/ Access flags defined in class file, fits in u2 according to JVMS\n+    assert(method.access_flags().as_int() == (method.access_flags().get_flags() & JVM_RECOGNIZED_METHOD_MODIFIERS), \"only method-related flags should be present\");\n+    WRITE(checked_cast<u2>(method.access_flags().get_flags()));\n+    \/\/ Immutable internal flags\n+    WRITE(checked_cast<jint>(cmethod.flags()));\n+    \/\/ Mutable internal flags (statuses)\n+    WRITE(checked_cast<jint>(filter_method_statuses(method.statuses(), cmethod.has_method_annotations())));\n+\n+    WRITE(cmethod.name_index());\n+    WRITE(cmethod.signature_index());\n+\n+    \/\/ Write lengths\/sizes of all embedded data first to allow the method to be\n+    \/\/ allocated (allocating memory for the data) before reading the data\n+    WRITE(cmethod.code_size()); \/\/ u2 is enough (code_length is limited to 65535 even though occupies u4)\n+    assert(cmethod.code_size() > 0 || \/\/ code_size == 0 iff no Code was specified\n+          (!cmethod.has_exception_table() && !cmethod.has_linenumber_table() && !cmethod.has_localvariable_table()),\n+          \"being parts of Code attribute they cannot exist without it\");\n+    const jint linenumber_table_size = cmethod.has_linenumber_table() ? get_linenumber_table_size(cmethod) : 0;\n+    if (cmethod.has_exception_table())       WRITE(cmethod.exception_table_length());\n+    if (cmethod.has_linenumber_table())      WRITE(linenumber_table_size);\n+    if (cmethod.has_localvariable_table())   WRITE(cmethod.localvariable_table_length());\n+    if (cmethod.has_checked_exceptions())    WRITE(cmethod.checked_exceptions_length());\n+    if (cmethod.has_method_parameters())     WRITE(checked_cast<u1>(cmethod.method_parameters_length())); \/\/ u1 is enough as specified in the class file format\n+    if (cmethod.has_generic_signature())     WRITE(cmethod.generic_signature_index()); \/\/ Signature attribute, participates in the method allocation size calculation\n+    if (cmethod.has_method_annotations())    WRITE(checked_cast<jint>(cmethod.method_annotations_length()));\n+    if (cmethod.has_parameter_annotations()) WRITE(checked_cast<jint>(cmethod.parameter_annotations_length()));\n+    if (cmethod.has_type_annotations())      WRITE(checked_cast<jint>(cmethod.type_annotations_length()));\n+    if (cmethod.has_default_annotations())   WRITE(checked_cast<jint>(cmethod.default_annotations_length()));\n+\n+    \/\/ Now write the data (i.e. method attributes), omitting their lengths\/sizes\n+    if (cmethod.code_size() > 0)             DO_CHECKED(write_code_attr(method, linenumber_table_size));\n+    if (cmethod.has_checked_exceptions()) {\n+      assert(cmethod.checked_exceptions_length() > 0, \"existing stackmap table cannot be empty\");\n+      STATIC_ASSERT(sizeof(CheckedExceptionElement) == sizeof(u2)); \/\/ Check no padding\n+      const size_t len = cmethod.checked_exceptions_length() * sizeof(CheckedExceptionElement) \/ sizeof(u2);\n+      DO_CHECKED(write_uint_array_data(reinterpret_cast<u2 *>(cmethod.checked_exceptions_start()), len));\n+    }\n+    if (cmethod.has_method_parameters()) { \/\/ Does not imply method_parameters_length > 0\n+      STATIC_ASSERT(sizeof(MethodParametersElement) == 2 * sizeof(u2)); \/\/ Check no padding\n+      const size_t len = cmethod.method_parameters_length() * sizeof(MethodParametersElement) \/ sizeof(u2);\n+      DO_CHECKED(write_uint_array_data(reinterpret_cast<u2 *>(cmethod.method_parameters_start()), len));\n+    }\n+    if (cmethod.has_method_annotations()) { \/\/ Runtime(In)VisibleAnnotations\n+      assert(!cmethod.method_annotations()->is_empty(), \"existing method annotations cannot be empty\");\n+      DO_CHECKED(write_uint_array_data(cmethod.method_annotations()->data(), cmethod.method_annotations_length()));\n+    }\n+    if (cmethod.has_parameter_annotations()) { \/\/ Runtime(In)VisibleParameterAnnotations\n+      assert(!cmethod.parameter_annotations()->is_empty(), \"existing parameter annotations cannot be empty\");\n+      DO_CHECKED(write_uint_array_data(cmethod.parameter_annotations()->data(), cmethod.parameter_annotations_length()));\n+    }\n+    if (cmethod.has_type_annotations()) { \/\/ Runtime(In)VisibleTypeAnnotations\n+      assert(!cmethod.type_annotations()->is_empty(), \"existing type annotations cannot be empty\");\n+      DO_CHECKED(write_uint_array_data(cmethod.type_annotations()->data(), cmethod.type_annotations_length()));\n+    }\n+    if (cmethod.has_default_annotations()) { \/\/ AnnotationDefault\n+      assert(!cmethod.default_annotations()->is_empty(), \"existing default annotations cannot be empty\");\n+      DO_CHECKED(write_uint_array_data(cmethod.default_annotations()->data(), cmethod.default_annotations_length()));\n+    }\n+    \/\/ Synthetic attribute is stored in access flags, others are not available\n+\n+    \/\/ TODO examine if any other intrinsics should be dumped\n+    WRITE(checked_cast<u1>(method.is_compiled_lambda_form())); \/\/ ClassFileParser sets this intrinsic based on an annotation\n+  }\n+\n+  void write_methods(const InstanceKlass &ik) {\n+    \/\/ Normal methods, including overpasses\n+    const Array<Method *> &methods = *ik.methods();\n+    const Array<int> &original_ordering = *ik.method_ordering();\n+    assert(&original_ordering == Universe::the_empty_int_array() || methods.length() == original_ordering.length(), \"must be\");\n+    WRITE(checked_cast<u2>(methods.length()));\n+    for (int i = 0; i < methods.length(); i++) {\n+      \/\/ Original index of this method in class file\n+      if (&original_ordering != Universe::the_empty_int_array()) {\n+        WRITE(checked_cast<u2>(original_ordering.at(i)));\n+      } else {\n+        assert(!JvmtiExport::can_maintain_original_method_order() && !Arguments::is_dumping_archive(), \"original method ordering must be available\");\n+        WRITE(checked_cast<u2>(i)); \/\/ Pretend this is the original ordering\n+      }\n+      DO_CHECKED(write_method(*methods.at(i)));\n+    }\n+\n+    \/\/ Descriptions of the default methods, if any\n+    const Array<Method *> *defaults = ik.default_methods();\n+    if (defaults != nullptr) {\n+      assert(ik.has_nonstatic_concrete_methods(), \"must be\");\n+      assert(defaults->length() > 0, \"must not be allocated if there are no defaults\");\n+      WRITE(checked_cast<u2>(defaults->length()));\n+      for (int i = 0; i < defaults->length(); i++) {\n+        const Method &method = *defaults->at(i);\n+        assert(!method.is_old(), \"default methods must not be old\"); \/\/ Lets us omit holder's redefinition version\n+        write_method_identification(method);\n+      }\n+    } else {\n+      WRITE(checked_cast<u2>(0));\n+    }\n+  }\n+\n+  \/\/ ###########################################################################\n+  \/\/ JVM TI-related data\n+  \/\/ ###########################################################################\n+\n+  \/\/ JVM TI RetransformClasses support.\n+  void write_cached_class_file(JvmtiCachedClassFileData *cached_class_file) {\n+    if (cached_class_file == nullptr) {\n+      WRITE(CracClassDump::NO_CACHED_CLASS_FILE_SENTINEL);\n+      return;\n+    }\n+\n+    guarantee(cached_class_file->length >= 0, \"length cannot be negative\");\n+    WRITE(cached_class_file->length);\n+    WRITE_RAW(cached_class_file->data, cached_class_file->length);\n+  }\n+\n+  \/\/ JVM TI RedefineClasses support.\n+  void write_previous_versions(InstanceKlass *ik) {\n+    if (!ik->has_been_redefined()) {\n+      assert(ik->previous_versions() == nullptr, \"only redefined class can have previous versions\");\n+      return;\n+    }\n+\n+    InstanceKlass::purge_previous_versions(ik); \/\/ Remove redundant previous versions\n+    if (ik->previous_versions() != nullptr) {\n+      \/\/ TODO implement previous versions dumping (and fail on restore if the\n+      \/\/  restoring VM won't have JVM TI included)\n+      ResourceMark rm;\n+      log_error(crac, class, dump)(\"Old versions of redefined %s's methods are still executing\", ik->external_name());\n+      Unimplemented();\n+    }\n+  }\n+\n+  \/\/ ###########################################################################\n+  \/\/ Instance and object array classes dumping\n+  \/\/ ###########################################################################\n+\n+  void write_instance_class_data(InstanceKlass *ik) {\n+    precond(ik != nullptr);\n+    if (log_is_enabled(Trace, crac, class, dump)) {\n+      ResourceMark rm;\n+      log_trace(crac, class, dump)(\"Writing instance class data: %s (ID \" UINTX_FORMAT \")\",\n+                                   ik->external_name(), cast_from_oop<uintptr_t>(ik->java_mirror()));\n+    }\n+\n+    WRITE_CLASS_ID(*ik);\n+    WRITE(checked_cast<u1>(loading_kind(*ik)));\n+\n+    assert(ik->is_loaded(), \"too young, must've been filtered out\");\n+    assert(!ik->is_being_linked() && !ik->is_being_initialized(), \"should've failed during stack dumping (linking thread must have an in-VM frame)\");\n+    WRITE(checked_cast<u1>(ik->init_state()));\n+    if (ik->is_in_error_state()) {\n+      WRITE_OBJECT_ID(ik->get_initialization_error()); \/\/ Can be null\n+    }\n+\n+    WRITE(ik->minor_version());\n+    WRITE(ik->major_version());\n+    WRITE(checked_cast<jint>(ik->constants()->version())); \/\/ Version of redefined classes (0 if not redefined), may be negative\n+\n+    DO_CHECKED(write_class_flags(*ik));\n+\n+    DO_CHECKED(write_class_attrs(*ik)); \/\/ Consatant pool parsing depends on NestHost attribute\n+\n+    DO_CHECKED(write_constant_pool(*ik->constants()));\n+    if (ik->is_rewritten()) {\n+      precond(ik->constants()->cache() != nullptr);\n+      DO_CHECKED(write_constant_pool_cache(*ik->constants()->cache()));\n+    }\n+\n+    WRITE(ik->this_class_index());\n+    DO_CHECKED(write_interfaces(*ik->local_interfaces()));\n+\n+    DO_CHECKED(write_fields(*ik));\n+\n+    DO_CHECKED(write_methods(*ik));\n+\n+    DO_CHECKED(write_cached_class_file(ik->get_cached_class_file()));\n+    DO_CHECKED(write_previous_versions(ik));\n+\n+    \/\/ TODO save and restore CDS-related stuff (if there is any that is portable)\n+  }\n+\n+  \/\/ Dumps instance class and its array classes, ensuring its ancestors are\n+  \/\/ dumped first in the required order.\n+  void dump_class_hierarchy(InstanceKlass *ik) {\n+    precond(ik != nullptr);\n+\n+    bool not_dumped_yet;\n+    _dumped_classes.put_if_absent(ik, &not_dumped_yet);\n+    if (!not_dumped_yet) {\n+      return;\n+    }\n+    _dumped_classes.maybe_grow();\n+\n+    if (ik->class_loader() != nullptr) {\n+      const oop loader_parent = java_lang_ClassLoader::parent(ik->class_loader());\n+      if (loader_parent != nullptr) {\n+        DO_CHECKED(dump_class_hierarchy(InstanceKlass::cast(loader_parent->klass())));\n+      }\n+      DO_CHECKED(dump_class_hierarchy(InstanceKlass::cast(ik->class_loader()->klass())));\n+    } else {\n+      assert(ik->class_loader_data()->is_boot_class_loader_data(), \"must be\");\n+    }\n+\n+    if (ik->java_super() != nullptr) {\n+      DO_CHECKED(dump_class_hierarchy(ik->java_super()));\n+    }\n+\n+    const Array<InstanceKlass *> &interfaces = *ik->local_interfaces();\n+    for (int i = 0; i < interfaces.length(); i++) {\n+      DO_CHECKED(dump_class_hierarchy(interfaces.at(i)));\n+    }\n+\n+    DO_CHECKED(write_instance_class_data(ik));\n+    DO_CHECKED(write_obj_array_class_ids(ik));\n+  }\n+\n+  \/\/ ###########################################################################\n+  \/\/ Initiating class loaders info\n+  \/\/ ###########################################################################\n+\n+  void do_cld(ClassLoaderData *cld) override {\n+    if (cld->is_the_null_class_loader_data()) {\n+      \/\/ Bootstrap loader never delegates, so if it is an initiating loader than\n+      \/\/ it is also the defining one, and the defining loaders are known from\n+      \/\/ the heap dump\n+#ifdef ASSERT\n+      struct : public KlassClosure {\n+        void do_klass(Klass *k) override {\n+          assert(k->class_loader() == nullptr, \"must be defined by the boot loader\");\n+        }\n+      } asserter;\n+      cld->dictionary()->all_entries_do(&asserter);\n+#endif \/\/ ASSERT\n+      return;\n+    }\n+    if (cld->has_class_mirror_holder()) {\n+      \/\/ These CLDs are exclusive to the holder\n+      guarantee(cld->dictionary() == nullptr, \"CLDs with mirror holder have no dictionaries\");\n+      return;\n+    }\n+    postcond(cld->class_loader() != nullptr && cld->dictionary() != nullptr);\n+    assert(java_lang_ClassLoader::loader_data(cld->class_loader()) == cld,\n+           \"must be true for CLD without a mirror holder\");\n+\n+    ResourceMark rm;\n+    GrowableArray<const InstanceKlass *> initiated_classes;\n+    \/\/ Find all classes known to the class loader but not defined by it.\n+    struct InitiatedKlassCollector : public KlassClosure {\n+      const ClassLoaderData &cld;\n+      GrowableArray<const InstanceKlass *> &iks;\n+      InitiatedKlassCollector(const ClassLoaderData &cld, GrowableArray<const InstanceKlass *> *iks) :\n+        cld(cld), iks(*iks) {}\n+      void do_klass(Klass *k) override {\n+        precond(k->is_instance_klass());\n+        if (k->class_loader_data() != &cld) {\n+          InstanceKlass *const ik = InstanceKlass::cast(k);\n+          assert(!ik->is_hidden(), \"hidden classes cannot be seen outside of the defining loader\");\n+          iks.append(ik);\n+        }\n+      }\n+    } collector(*cld, &initiated_classes);\n+    cld->dictionary()->all_entries_do(&collector);\n+\n+    if (!initiated_classes.is_empty()) {\n+      WRITE_OBJECT_ID(cld->class_loader());\n+      WRITE(checked_cast<jint>(initiated_classes.length()));\n+      for (const InstanceKlass *ik : initiated_classes) {\n+        WRITE_CLASS_ID(*ik);\n+      }\n+    }\n+  }\n+\n+#undef DO_CHECKED\n+#undef WRITE_RAW\n+#undef WRITE_CLASS_ID\n+#undef WRITE_OBJECT_ID\n+#undef WRITE_SYMBOL_ID\n+#undef WRITE\n+};\n+\n+const char *CracClassDumper::dump(const char *path, bool overwrite) {\n+  guarantee(SafepointSynchronize::is_at_safepoint(), \"need safepoint to ensure classes are not modified concurrently\");\n+  log_info(crac, class, dump)(\"Dumping classes into %s\", path);\n+\n+  FileBasicTypeWriter file_writer;\n+  if (!file_writer.open(path, overwrite)) {\n+    return os::strerror(errno);\n+  }\n+\n+  ClassDumpWriter dump_writer(&file_writer);\n+  dump_writer.write_dump();\n+  return dump_writer.io_error_msg();\n+}\n","filename":"src\/hotspot\/share\/runtime\/cracClassDumper.cpp","additions":1261,"deletions":0,"binary":false,"changes":1261,"status":"added"},{"patch":"@@ -0,0 +1,72 @@\n+#ifndef SHARE_RUNTIME_CRACCLASSDUMPER_HPP\n+#define SHARE_RUNTIME_CRACCLASSDUMPER_HPP\n+\n+#include \"memory\/allStatic.hpp\"\n+#include \"utilities\/globalDefinitions.hpp\"\n+\n+\/\/ Constants used in class dumps.\n+struct CracClassDump : public AllStatic {\n+  \/\/ Kinds of classes with regards to how they were loaded.\n+  enum class ClassLoadingKind : u1 {\n+    NORMAL            = 0,\n+    NON_STRONG_HIDDEN = 1,\n+    STRONG_HIDDEN     = 2,\n+  };\n+\n+  static constexpr bool is_class_loading_kind(u1 val) { return val <= static_cast<u1>(ClassLoadingKind::STRONG_HIDDEN); }\n+\n+  \/\/ Bit positions in compressed VM options.\n+  enum VMOptionShift : u1 {\n+    is_sync_on_value_based_classes_diagnosed_shift = 0,\n+    are_all_annotations_preserved_shift            = 1,\n+  };\n+\n+  static constexpr bool is_vm_options(u1 val) { return val >> (VMOptionShift::are_all_annotations_preserved_shift + 1) == 0; }\n+\n+  \/\/ Bit positions of in resolved method entries' flags.\n+  enum ResolvedMethodEntryFlagShift : u1 {\n+    is_vfinal_shift           = 0,\n+    is_final_shift            = 1,\n+    is_forced_virtual_shift   = 2,\n+    has_appendix_shift        = 3,\n+    has_local_signature_shift = 4,\n+  };\n+\n+  static constexpr bool is_resolved_method_entry_flags(u1 val) { return val >> (ResolvedMethodEntryFlagShift::has_local_signature_shift + 1) == 0; }\n+\n+  \/\/ For null class metadata arrays.\n+  static constexpr u4 NO_ARRAY_SENTINEL = 0xFFFFFFFF;\n+\n+  \/\/ For null cached class file.\n+  static constexpr jint NO_CACHED_CLASS_FILE_SENTINEL = -1;\n+};\n+\n+\/\/ Dumps runtime class data for CRaC portable mode.\n+\/\/\n+\/\/ The dump is expected to be accompanied by an HPROF heap dump from the\n+\/\/ heapDumper, hence IDs used are the same as heapDumper uses and some class\n+\/\/ data available from there is not duplicated.\n+\/\/\n+\/\/ First, after the header, IDs of primitive array classes are dumped, followed\n+\/\/ by dumps of instance classes.\n+\/\/\n+\/\/ Instance classes are sorted so that for any instance class C the following\n+\/\/ instance classes are dumped ahead of C:\n+\/\/ 1. Class of C's class loader.\n+\/\/ 2. Class of C's class loader's parent.\n+\/\/ 3. C's super class.\n+\/\/ 4. Interfaces implemented by C.\n+\/\/ This ordering makes it easier to load classes as the dump is being parsed.\n+\/\/\n+\/\/ Each primitive-array and instance class is followed by IDs of object array\n+\/\/ classes sorted by ascending dimensionality.\n+struct CracClassDumper : public AllStatic {\n+  \/\/ Dumps the data into the specified file, possibly overwriting it if the\n+  \/\/ corresponding parameter is set to true, Returns nullptr on success, or a\n+  \/\/ pointer to a static IO error message otherwise.\n+  \/\/\n+  \/\/ Must be called on a safepoint.\n+  static const char *dump(const char *path, bool overwrite = false);\n+};\n+\n+#endif \/\/ SHARE_RUNTIME_CRACCLASSDUMPER_HPP\n","filename":"src\/hotspot\/share\/runtime\/cracClassDumper.hpp","additions":72,"deletions":0,"binary":false,"changes":72,"status":"added"},{"patch":"@@ -0,0 +1,872 @@\n+#include \"precompiled.hpp\"\n+#include \"classfile\/classLoaderData.hpp\"\n+#include \"classfile\/dictionary.hpp\"\n+#include \"classfile\/systemDictionary.hpp\"\n+#include \"classfile\/vmSymbols.hpp\"\n+#include \"logging\/log.hpp\"\n+#include \"memory\/resourceArea.hpp\"\n+#include \"oops\/array.hpp\"\n+#include \"oops\/arrayKlass.hpp\"\n+#include \"oops\/constantPool.hpp\"\n+#include \"oops\/cpCache.hpp\"\n+#include \"oops\/cpCache.inline.hpp\"\n+#include \"oops\/instanceKlass.hpp\"\n+#include \"oops\/klass.hpp\"\n+#include \"oops\/method.hpp\"\n+#include \"oops\/oopsHierarchy.hpp\"\n+#include \"oops\/resolvedFieldEntry.hpp\"\n+#include \"oops\/resolvedIndyEntry.hpp\"\n+#include \"runtime\/cracClassDumpParser.hpp\"\n+#include \"runtime\/cracClassStateRestorer.hpp\"\n+#include \"runtime\/handles.hpp\"\n+#include \"runtime\/javaThread.hpp\"\n+#include \"runtime\/javaThread.inline.hpp\"\n+#include \"runtime\/mutexLocker.hpp\"\n+#include \"runtime\/thread.hpp\"\n+#include \"utilities\/constantTag.hpp\"\n+#include \"utilities\/debug.hpp\"\n+#include \"utilities\/exceptions.hpp\"\n+#include \"utilities\/formatBuffer.hpp\"\n+#include \"utilities\/heapDumpParser.hpp\"\n+#include \"utilities\/macros.hpp\"\n+#ifdef ASSERT\n+#include \"classfile\/vmClasses.hpp\"\n+#include \"classfile_constants.h\"\n+#include \"interpreter\/bytecodes.hpp\"\n+#include \"oops\/fieldInfo.hpp\"\n+#include \"oops\/fieldStreams.hpp\"\n+#include \"oops\/fieldStreams.inline.hpp\"\n+#include \"prims\/methodHandles.hpp\"\n+#include \"utilities\/globalDefinitions.hpp\"\n+#endif \/\/ ASSERT\n+\n+#ifdef ASSERT\n+\n+\/\/ Asserts to compare the newly re-created and a pre-defined versions of the\n+\/\/ same class. Note that they are run before the newly re-created class is\n+\/\/ linked and before its CP cache is restored, so the related data is not\n+\/\/ compared.\n+\n+static int count_overpasses(const Array<Method *> &methods) {\n+  int num_overpasses = 0;\n+  for (int i = 0; i < methods.length(); i++) {\n+    if (methods.at(i)->is_overpass()) {\n+      num_overpasses++;\n+    }\n+  }\n+  return num_overpasses;\n+}\n+\n+static void assert_constants_match(const ConstantPool &cp1, const ConstantPool &cp2) {\n+  precond(cp1.pool_holder()->name() == cp2.pool_holder()->name());\n+#ifdef ASSERT\n+  if (cp1.length() != cp2.length()) {\n+    cp1.print();\n+    cp2.print();\n+    fatal(\"%s: number of constants differs: %i != %i (constants dumped to stdout)\",\n+          cp1.pool_holder()->external_name(), cp1.length(), cp2.length());\n+  }\n+#endif \/\/ ASSERT\n+\n+  \/\/ Constant pool consists of two parts: the first one comes from the class\n+  \/\/ file while the second one is appended when generating overpass methods. We\n+  \/\/ can only compare the first one because the second is not portable: the\n+  \/\/ order in which overpasses are generated, and thus in which their constants\n+  \/\/ are appended, depends on methods in supers and interfaces which depends on\n+  \/\/ the layout of method name symbols in memory).\n+  const int num_overpasses = count_overpasses(*cp1.pool_holder()->methods());\n+  assert(num_overpasses == count_overpasses(*cp2.pool_holder()->methods()), \"%s: number of overpass methods differs\",\n+         cp1.pool_holder()->external_name());\n+  \/\/ An overpass method may need up to this many new constants:\n+  \/\/ 1. 2 for method's name and type.\n+  \/\/ 2. 8 for method's code (see BytecodeAssembler::assemble_method_error()):\n+  \/\/     new           <error's Class>           +2: UTF8 for class'es name, Class itself\n+  \/\/     dup\n+  \/\/     ldc(_w)       <error's msg String>      +2: UTF8, String\n+  \/\/     invokespecial <error's init Methodref>  +4: 2 UTF8s for init's name and type, NameAndType, Methodref (its Class is the one already added by new)\n+  \/\/     athrow\n+  \/\/ In the worst case the 1st overpass will add all entries listed above:\n+  static constexpr int max_cp_entries_first_overpass = 10;\n+  \/\/ All overpasses use <init>(Ljava\/lang\/String;)V in invoke special, hence\n+  \/\/ its 2 UTF8s and a NameAndType will only be added by the 1st overpass:\n+  static constexpr int max_cp_entries_second_overpass = 7;\n+  \/\/ There are only two error classes used in overpasses -- their UTF8 names,\n+  \/\/ Class entries and init Methodref entries are already accounted for by the\n+  \/\/ 1st and 2nd overpasses above:\n+  static constexpr int max_cp_entries_other_overpass = 4;\n+  \/\/ This is a conservative estimation of the length of the comparable part: its\n+  \/\/ actual length is not less than this\n+  const int comparable_cp_length = MAX2(0, cp1.length() - (num_overpasses >= 1 ? max_cp_entries_first_overpass : 0)\n+                                                        - (num_overpasses >= 2 ? max_cp_entries_second_overpass : 0)\n+                                                        - MAX2(0, num_overpasses - 2) * max_cp_entries_other_overpass);\n+\n+  for (int i = 1; i < comparable_cp_length; i++) {\n+    \/\/ Compare resolved and unresolved versions of the same tag as equal since\n+    \/\/ the version re-created from the dump may have more entries resolved than\n+    \/\/ the pre-defined one (or vise versa)\n+    assert(cp1.tag_at(i).external_value() == cp2.tag_at(i).external_value(),\n+           \"%s: incompatible constant pool tags at slot #%i: %s and %s\",\n+           cp1.pool_holder()->external_name(), i, cp1.tag_at(i).internal_name(), cp2.tag_at(i).internal_name());\n+    \/\/ TODO these should be asserts but in the current demos they are sometimes\n+    \/\/  violated without breaking anything because we don't properly handle\n+    \/\/  platform-dependent JDK classes yet\n+    switch (cp1.tag_at(i).value()) {\n+      case JVM_CONSTANT_Utf8:\n+        if (cp1.symbol_at(i) != cp2.symbol_at(i)) {\n+          ResourceMark rm;\n+          log_warning(crac, class)(\"%s: different symbols at constant pool slot #%i: \\\"%s\\\" != \\\"%s\\\"\",\n+                                   cp1.pool_holder()->external_name(), i, cp1.symbol_at(i)->as_C_string(), cp2.symbol_at(i)->as_C_string());\n+        }\n+        break;\n+      case JVM_CONSTANT_Integer:\n+        if (cp1.int_at(i) != cp2.int_at(i)) {\n+          ResourceMark rm;\n+          log_warning(crac, class)(\"%s: different integers at constant pool slot #%i: %i != %i\",\n+                                   cp1.pool_holder()->external_name(), i, cp1.int_at(i), cp2.int_at(i));\n+        }\n+        break;\n+      case JVM_CONSTANT_Float:\n+        if (cp1.float_at(i) != cp2.float_at(i) && !(g_isnan(cp1.float_at(i)) != 0 && g_isnan(cp2.float_at(i)) != 0)) {\n+          ResourceMark rm;\n+          log_warning(crac, class)(\"%s: different floats at constant pool slot #%i: %f != %f\",\n+                                   cp1.pool_holder()->external_name(), i, cp1.float_at(i), cp2.float_at(i));\n+        }\n+        break;\n+      case JVM_CONSTANT_Long:\n+        if (cp1.long_at(i) != cp2.long_at(i)) {\n+          ResourceMark rm;\n+          log_warning(crac, class)(\"%s: different longs at constant pool slot #%i: \" JLONG_FORMAT \" != \" JLONG_FORMAT,\n+                                   cp1.pool_holder()->external_name(), i, cp1.long_at(i), cp2.long_at(i));\n+        }\n+        break;\n+      case JVM_CONSTANT_Double:\n+        if (cp1.double_at(i) != cp2.double_at(i) && !(g_isnan(cp1.double_at(i)) != 0 && g_isnan(cp2.double_at(i)) != 0)) {\n+          ResourceMark rm;\n+          log_warning(crac, class)(\"%s: different doubles at constant pool slot #%i: %lf != %lf\",\n+                                   cp1.pool_holder()->external_name(), i, cp1.double_at(i), cp2.double_at(i));\n+        }\n+        break;\n+      default: ; \/\/ TODO other types\n+    }\n+  }\n+}\n+\n+static void assert_fields_match(const InstanceKlass &ik1, const InstanceKlass &ik2) {\n+  precond(ik1.name() == ik2.name());\n+  AllFieldStream fs1(&ik1);\n+  AllFieldStream fs2(&ik2);\n+  assert(fs1.num_total_fields() == fs2.num_total_fields(), \"%s: number of fields differs: %i != %i\",\n+         ik1.external_name(), fs1.num_total_fields(), fs2.num_total_fields());\n+  for (; !fs1.done() && !fs2.done(); fs1.next(), fs2.next()) {\n+    assert(fs1.index() == fs2.index(), \"must be\");\n+    assert(fs1.name() == fs2.name() && fs1.signature() == fs2.signature(), \"%s: field %i differs: %s %s and %s %s\",\n+           ik1.external_name(), fs1.index(), fs1.signature()->as_C_string(), fs1.name()->as_C_string(), fs2.signature()->as_C_string(), fs2.name()->as_C_string());\n+    assert(fs1.access_flags().get_flags() == fs2.access_flags().get_flags(), \"%s: different access flags of field %i: \" INT32_FORMAT_X \" != \" INT32_FORMAT_X,\n+           ik1.external_name(), fs1.index(), fs1.access_flags().get_flags(), fs2.access_flags().get_flags());\n+    assert(fs1.field_flags().as_uint() == fs2.field_flags().as_uint(), \"%s: different internal flags of field %i (%s %s): \" UINT32_FORMAT_X \" != \" UINT32_FORMAT_X,\n+           ik1.external_name(), fs1.index(), fs1.signature()->as_C_string(), fs1.name()->as_C_string(), fs1.field_flags().as_uint(), fs2.field_flags().as_uint());\n+    assert(fs1.offset() == fs2.offset(), \"%s: different offset of field %i (%s %s): %i != %i\",\n+           ik1.external_name(), fs1.index(), fs1.signature()->as_C_string(), fs1.name()->as_C_string(), fs1.offset(), fs2.offset());\n+  }\n+  postcond(fs1.done() && fs2.done());\n+}\n+\n+static void assert_methods_match(const InstanceKlass &ik1, const InstanceKlass &ik2) {\n+  precond(ik1.name() == ik2.name());\n+  const Array<Method *> &methods1 = *ik1.methods();\n+  const Array<Method *> &methods2 = *ik2.methods();\n+  assert(methods1.length() == methods2.length(), \"%s: number of methods differs: %i != %i\",\n+         ik1.external_name(), methods1.length(), methods2.length());\n+  for (int i = 0; i < methods1.length(); i++) {\n+    const Method &method1 = *methods1.at(i);\n+    \/\/ Cannot just get by index because the order of methods with the same name may differ\n+    const Method *method2 = ik2.find_local_method(method1.name(), method1.signature(),\n+                                                  method1.is_overpass() ? Klass::OverpassLookupMode::find : Klass::OverpassLookupMode::skip,\n+                                                  method1.is_static() ? Klass::StaticLookupMode::find : Klass::StaticLookupMode::skip,\n+                                                  Klass::PrivateLookupMode::find);\n+    assert(method2 != nullptr, \"%s not found in the second class version\", method1.external_name());\n+\n+    assert(method1.result_type() == method2->result_type(), \"different result type of method %s: %s != %s\",\n+           method1.external_name(), type2name(method1.result_type()), type2name(method2->result_type()));\n+    assert(method1.size_of_parameters() == method2->size_of_parameters(), \"different word-size of parameters of method %s: %i != %i\",\n+           method1.external_name(), method1.size_of_parameters(), method2->size_of_parameters());\n+    assert(method1.num_stack_arg_slots() == method2->num_stack_arg_slots(), \"different number of stack slots for arguments of method %s: %i != %i\",\n+           method1.external_name(), method1.num_stack_arg_slots(), method2->num_stack_arg_slots());\n+    assert(method1.constMethod()->fingerprint() == method2->constMethod()->fingerprint(),\n+           \"different fingerprint of method %s: \" UINT64_FORMAT_X \" != \" UINT64_FORMAT_X,\n+           method1.external_name(), method1.constMethod()->fingerprint(), method2->constMethod()->fingerprint());\n+\n+    assert(method1.access_flags().get_flags() == method2->access_flags().get_flags(), \"different flags of method %s: \" INT32_FORMAT_X \" != \" INT32_FORMAT_X,\n+           method1.external_name(), method1.access_flags().get_flags(), method2->access_flags().get_flags());\n+    assert(method1.constMethod()->flags() == method2->constMethod()->flags(), \"different cmethod flags of method %s: \" INT32_FORMAT_X \" != \" INT32_FORMAT_X,\n+           method1.external_name(), method1.constMethod()->flags(), method2->constMethod()->flags());\n+\n+    assert(method1.max_stack() == method2->max_stack() && method1.max_locals() == method2->max_locals(),\n+           \"different max stack values of method %s: max stack %i and %i, max locals %i and %i\",\n+           method1.external_name(), method1.max_stack(), method2->max_stack(), method1.max_locals(), method2->max_locals());\n+    assert(ik1.is_rewritten() != ik2.is_rewritten() \/* code may change upon rewriting *\/ || method1.code_size() == method2->code_size(),\n+           \"different code size of method %s: %i != %i\", method1.external_name(), method1.code_size(), method2->code_size());\n+  }\n+}\n+\n+#endif \/\/ ASSERT\n+\n+static void swap_constants(InstanceKlass *ik1, InstanceKlass *ik2) {\n+  guarantee(ik1->constants()->length() == ik2->constants()->length(), \"not the same class\");\n+\n+  auto *const tmp = ik1->constants();\n+  ik1->set_constants(ik2->constants());\n+  ik2->set_constants(tmp);\n+\n+  ik1->constants()->set_pool_holder(ik1);\n+  ik2->constants()->set_pool_holder(ik2);\n+}\n+\n+static void swap_methods(InstanceKlass *ik1, InstanceKlass *ik2) {\n+  guarantee(ik1->methods()->length() == ik2->methods()->length(), \"not the same class\");\n+  assert(ik1->methods()->is_empty() ||\n+         (ik1->methods()->at(0)->constants() == ik2->constants() &&\n+          ik2->methods()->at(0)->constants() == ik1->constants()),\n+         \"constant pools must be swapped beforehand\");\n+\n+  auto *const tmp = ik1->methods();\n+  ik1->set_methods(ik2->methods());\n+  ik2->set_methods(tmp);\n+\n+#ifdef ASSERT\n+  for (int i = 0; i < ik1->methods()->length(); i++) {\n+    const Method &method1 = *ik1->methods()->at(i);\n+    const Method &method2 = *ik2->methods()->at(i);\n+\n+    \/\/ The constant pools should have been swapped beforehand\n+    precond(method1.constants() == ik1->constants());\n+    precond(method2.constants() == ik2->constants());\n+    precond(method1.method_holder() == ik1);\n+    precond(method2.method_holder() == ik2);\n+\n+    \/\/ Can only compare names because methods with equal names can be reordered\n+    assert(method1.name() == method2.name(), \"method #%i of %s has different names: %s and %s\",\n+           i, ik1->external_name(), method1.name()->as_C_string(), method2.name()->as_C_string());\n+  }\n+#endif \/\/ ASSERT\n+\n+  \/\/ Note: if this is an interface, pre-defined implementors may refer to\n+  \/\/ swapped-out methods via their default methods arrays so those also need to\n+  \/\/ be updated.\n+  \/\/\n+  \/\/ We assume that we'll visit all implementors later and fix their default\n+  \/\/ methods arrays then (also by swapping because newly created classes always\n+  \/\/ have the updated methods since we handle interfaces prior to implementors).\n+  \/\/\n+  \/\/ TODO rewriter has to also handle this somehow, maybe there is a better way?\n+}\n+\n+static void swap_default_methods(InstanceKlass *ik1, InstanceKlass *ik2) {\n+  auto *const dmethods1 = ik1->default_methods();\n+  auto *const dmethods2 = ik2->default_methods();\n+  precond(dmethods1 != nullptr && dmethods2 != nullptr);\n+  guarantee(dmethods1->length() == dmethods2->length(), \"not the same class\");\n+\n+  ik1->set_default_methods(dmethods2);\n+  ik2->set_default_methods(dmethods1);\n+\n+#ifdef ASSERT\n+  for (int i = 0; i < dmethods1->length(); i++) {\n+    const Method *dmethod1 = dmethods1->at(i);\n+    const Method *dmethod2 = dmethods2->at(i);\n+    \/\/ Can only compare names because methods with equal names can be reordered\n+    assert(dmethod1->name() == dmethod2->name(), \"method #%i of %s has different names: %s and %s\",\n+           i, ik1->external_name(), dmethod1->name()->as_C_string(), dmethod2->name()->as_C_string());\n+  }\n+#endif \/\/ ASSERT\n+}\n+\n+InstanceKlass *CracClassStateRestorer::define_created_class(InstanceKlass *created_ik, InstanceKlass::ClassState target_state, TRAPS) {\n+  precond(created_ik != nullptr && created_ik->is_being_restored() && !created_ik->is_loaded());\n+\n+  \/\/ May get another class if one has been defined already:\n+  \/\/ - created_ik -- what we have parsed from the dump\n+  \/\/ - defined_ik -- what we should use\n+  \/\/ If created_ik != defined_ik the former will be deallocated.\n+  InstanceKlass *const defined_ik = SystemDictionary::find_or_define_recreated_class(created_ik, CHECK_NULL);\n+  postcond(defined_ik->is_loaded());\n+\n+  const bool predefined = defined_ik != created_ik;\n+  assert(!(predefined && defined_ik->is_being_restored()), \"pre-defined class must be unmarked\");\n+\n+  \/\/ Ensure the class won't be used by other threads until it is restored. We do\n+  \/\/ this even if the class was only loaded at the dump time to be able to set\n+  \/\/ resolved class references which may appear during verification (even if it\n+  \/\/ failed in the end). In higher dumped states this also saves other threads\n+  \/\/ from using unfilled CP cache entries, unrestored resolved references array\n+  \/\/ and unrestored static fields. But if the pre-defined class is has already\n+  \/\/ attempted initialization, this won't save from anything.\n+  JavaThread *const thread = JavaThread::current();\n+  {\n+    MonitorLocker ml(defined_ik->init_monitor());\n+    precond(!defined_ik->is_init_thread(thread));\n+    if (defined_ik->is_being_linked() || defined_ik->is_being_initialized()) {\n+      \/\/ Waiting here may lead to a deadlock: we restore and lock from base to\n+      \/\/ derived which is the opposite to what the usual initialization process\n+      \/\/ does. E.g. A subclasses B, we lock B and want A, initialization locks A\n+      \/\/ and wants B -- deadlock.\n+      THROW_MSG_NULL(vmSymbols::java_lang_IllegalStateException(),\n+                     err_msg(\"Class %s is being initialized and restored concurrently\", defined_ik->external_name()));\n+    }\n+    if (defined_ik->init_state() < InstanceKlass::fully_initialized) {\n+        defined_ik->set_is_being_restored(true);\n+      if ((created_ik->is_rewritten() && !(predefined && defined_ik->is_rewritten())) ||\n+          (target_state >= InstanceKlass::linked && !defined_ik->is_linked())) {\n+        defined_ik->set_init_state(InstanceKlass::being_linked);\n+        defined_ik->set_init_thread(thread);\n+      } else if (target_state >= InstanceKlass::fully_initialized && defined_ik->init_state() < InstanceKlass::fully_initialized) {\n+        defined_ik->set_init_state(InstanceKlass::being_initialized);\n+        defined_ik->set_init_thread(thread);\n+      }\n+    }\n+  }\n+  postcond(!defined_ik->is_init_thread(thread) || defined_ik->is_being_restored());\n+  postcond(!defined_ik->is_init_thread(thread) || defined_ik->init_state() < InstanceKlass::fully_initialized);\n+\n+  if (predefined) {\n+    if (log_is_enabled(Debug, crac, class)) {\n+      ResourceMark rm;\n+      const char *current_state_name = (defined_ik->is_rewritten() && !defined_ik->is_linked())             ? \"rewritten\" : defined_ik->init_state_name();\n+      const char *target_state_name =  (created_ik->is_rewritten() && target_state < InstanceKlass::linked) ? \"rewritten\" : InstanceKlass::state_name(target_state);\n+      log_debug(crac, class)(\"Using pre-defined %s (%p instead of %p): current state = %s, target state = %s - defined by %s\",\n+                             defined_ik->external_name(), defined_ik, created_ik, current_state_name, target_state_name, defined_ik->class_loader_data()->loader_name_and_id());\n+    }\n+    assert(created_ik->access_flags().as_int() == defined_ik->access_flags().as_int(),\n+           \"pre-defined %s has different access flags: \" INT32_FORMAT_X \" (dumped) != \" INT32_FORMAT_X \" (pre-defined)\",\n+           created_ik->external_name(), created_ik->access_flags().as_int(), defined_ik->access_flags().as_int());\n+    DEBUG_ONLY(assert_constants_match(*created_ik->constants(), *defined_ik->constants()));\n+    DEBUG_ONLY(assert_fields_match(*created_ik, *defined_ik));\n+    DEBUG_ONLY(assert_methods_match(*created_ik, *defined_ik));\n+\n+    if (created_ik->is_rewritten() && !defined_ik->is_rewritten()) {\n+      precond(defined_ik->is_init_thread(thread));\n+      \/\/ Apply the rewritten state:\n+      \/\/ 1. Swap constant pools: firstly, to save the cache created by us and\n+      \/\/ restore it later, secondly, because the pools may differ in the part\n+      \/\/ created during the overpass methods' generation.\n+      swap_constants(created_ik, defined_ik);\n+      \/\/ 2. Swap methods to save the rewritten ones.\n+      swap_methods(created_ik, defined_ik);\n+      defined_ik->set_rewritten();\n+      if (log_is_enabled(Debug, crac, class)) {\n+        ResourceMark rm;\n+        log_debug(crac, class)(\"Moved dumped rewritten state into pre-defined %s\", defined_ik->external_name());\n+      }\n+    }\n+\n+    \/\/ There may be a super-interface we rewrote so have to update the default\n+    \/\/ methods to ensure there are no references to methods swapped-out of that\n+    \/\/ super-interface\n+    guarantee((created_ik->default_methods() != nullptr) == (defined_ik->default_methods() != nullptr), \"not the same class\");\n+    if (created_ik->default_methods() != nullptr) {\n+      swap_default_methods(created_ik, defined_ik);\n+    }\n+\n+    created_ik->class_loader_data()->add_to_deallocate_list(created_ik);\n+  } else if (log_is_enabled(Debug, crac, class)) {\n+    ResourceMark rm;\n+    const char *current_state_name = (defined_ik->is_rewritten() && !defined_ik->is_linked())             ? \"rewritten\" : defined_ik->init_state_name();\n+    const char *target_state_name =  (created_ik->is_rewritten() && target_state < InstanceKlass::linked) ? \"rewritten\" : InstanceKlass::state_name(target_state);\n+    log_debug(crac, class)(\"Using newly defined %s (%p): current state = %s, target state = %s - defined by %s\",\n+                           defined_ik->external_name(), defined_ik, current_state_name, target_state_name, defined_ik->class_loader_data()->loader_name_and_id());\n+  }\n+#ifdef ASSERT\n+  if (defined_ik->default_methods() != nullptr) {\n+    for (int i = 0; i < defined_ik->default_methods()->length(); i++) {\n+      const InstanceKlass *holder = defined_ik->default_methods()->at(i)->method_holder();\n+      assert(holder->is_loaded(), \"default method %s has unloaded holder %p\", holder->external_name(), holder);\n+    }\n+  }\n+#endif \/\/ ASSERT\n+\n+  if (target_state < InstanceKlass::linked) {\n+    assert(target_state != InstanceKlass::being_linked, \"not supported, shouldn't be dumped\");\n+    return defined_ik;\n+  }\n+  postcond(defined_ik->is_rewritten());\n+  if (!defined_ik->is_linked()) {\n+    precond(defined_ik->is_being_linked() && defined_ik->is_init_thread(thread));\n+    \/\/ Omitting vtable\/itable constraints check since it was done before the dump\n+    defined_ik->finish_linking(false, CHECK_NULL);\n+  }\n+\n+  if (target_state < InstanceKlass::fully_initialized) {\n+    assert(target_state != InstanceKlass::being_initialized, \"not supported, shouldn't be dumped\");\n+    precond(!defined_ik->is_being_initialized());\n+    return defined_ik;\n+  }\n+  precond(defined_ik->init_state() >= InstanceKlass::fully_initialized || defined_ik->is_init_thread(thread));\n+  guarantee(!(target_state == InstanceKlass::fully_initialized && defined_ik->is_in_error_state()) &&\n+            !(target_state == InstanceKlass::initialization_error && defined_ik->is_initialized()),\n+            \"%s is dumped %s, but its initialization has already been re-attempted and %s\",\n+            target_state == InstanceKlass::fully_initialized ? \"as successfully initialized\" : \"with an initialization error\",\n+            defined_ik->is_initialized() ? \"succeeded\" : \"failed\", defined_ik->external_name());\n+  \/\/ Static fields and resolution exception object will be set during heap restoration\n+  return defined_ik;\n+}\n+\n+#ifdef ASSERT\n+\n+static FieldInfo get_field_info(const InstanceKlass &holder, int index) {\n+  for (AllFieldStream fs(&holder); !fs.done(); fs.next()) {\n+    if (fs.index() == index) {\n+      return fs.to_FieldInfo();\n+    }\n+  }\n+  assert(false, \"no field with index %i in %s\", index, holder.external_name());\n+  return {};\n+}\n+\n+static void assert_correctly_resolved_field(const InstanceKlass &cache_holder, int resolved_info_i) {\n+  const ResolvedFieldEntry &resolved_info = *cache_holder.constants()->cache()->resolved_field_entry_at(resolved_info_i);\n+  assert(resolved_info.get_code() != 0, \"unresolved\");\n+\n+  const InstanceKlass &field_holder = *resolved_info.field_holder();\n+  const FieldInfo field_info = get_field_info(field_holder, resolved_info.field_index());\n+\n+  \/\/ Referenced as:\n+  const u2 ref_holder_i = cache_holder.constants()->uncached_klass_ref_index_at(resolved_info.constant_pool_index());\n+  const Klass &ref_holder = *cache_holder.constants()->resolved_klass_at(ref_holder_i);\n+  const Symbol *ref_name = cache_holder.constants()->uncached_name_ref_at(resolved_info.constant_pool_index());\n+  const Symbol *ref_sig = cache_holder.constants()->uncached_signature_ref_at(resolved_info.constant_pool_index());\n+  \/\/ Resolved as:\n+  const Symbol *name = field_info.name(field_holder.constants());\n+  const Symbol *sig = field_info.signature(field_holder.constants());\n+  \/\/ Must match\n+  assert(ref_name == name && ref_sig == sig, \"class %s, field entry #%i: referenced %s %s != resolved %s %s\",\n+         cache_holder.external_name(), resolved_info_i, ref_sig->as_C_string(), ref_name->as_C_string(), sig->as_C_string(), name->as_C_string());\n+  assert(ref_holder.is_subtype_of(const_cast<InstanceKlass *>(&field_holder)),\n+         \"class %s, field entry #%i (%s %s): referenced holder %s must subtype resolved holder %s\",\n+         cache_holder.external_name(), resolved_info_i, sig->as_C_string(), name->as_C_string(),\n+         ref_holder.external_name(), field_holder.external_name());\n+\n+  \/\/ Cached data\n+  assert(checked_cast<u4>(resolved_info.field_offset()) == field_info.offset(), \"class %s, field entry #%i (%s %s::%s): %i != \" UINT32_FORMAT,\n+         cache_holder.external_name(), resolved_info_i, sig->as_C_string(), field_holder.external_name(), name->as_C_string(),\n+         resolved_info.field_offset(), field_info.offset());\n+  assert(resolved_info.tos_state() == as_TosState(Signature::basic_type(sig)), \"class %s, field entry #%i (%s %s::%s): %i != %i\",\n+         cache_holder.external_name(), resolved_info_i, sig->as_C_string(), field_holder.external_name(), name->as_C_string(),\n+         resolved_info.tos_state(), as_TosState(Signature::basic_type(sig)));\n+  assert(resolved_info.is_volatile() == field_info.access_flags().is_volatile(), \"class %s, field entry #%i (%s %s::%s): %s != %s\",\n+         cache_holder.external_name(), resolved_info_i, sig->as_C_string(), field_holder.external_name(), name->as_C_string(),\n+         BOOL_TO_STR(resolved_info.is_volatile()), BOOL_TO_STR(field_info.access_flags().is_volatile()));\n+  assert(resolved_info.is_final() == field_info.access_flags().is_final(), \"class %s, field entry #%i (%s %s::%s): %s != %s\",\n+         cache_holder.external_name(), resolved_info_i, sig->as_C_string(), field_holder.external_name(), name->as_C_string(),\n+         BOOL_TO_STR(resolved_info.is_final()), BOOL_TO_STR(field_info.access_flags().is_final()));\n+}\n+\n+static void assert_correctly_resolved_method(const InstanceKlass &cache_holder, int resolved_info_i) {\n+  const ConstantPoolCacheEntry &resolved_info = *cache_holder.constants()->cache()->entry_at(resolved_info_i);\n+  assert(resolved_info.bytecode_1() != 0 || resolved_info.bytecode_2() != 0, \"unresolved\");\n+  assert(resolved_info.is_method_entry(), \"must be\");\n+\n+  Method *const method = resolved_info.method_if_resolved(constantPoolHandle(Thread::current(), cache_holder.constants()));\n+  assert(method != nullptr, \"class %s, cache entry #%i: cannot obtain the resolved method\",\n+         cache_holder.external_name(), resolved_info_i);\n+\n+  \/\/ Referenced as:\n+  const u2 ref_holder_i = cache_holder.constants()->uncached_klass_ref_index_at(resolved_info.constant_pool_index());\n+  Klass *const ref_holder = cache_holder.constants()->resolved_klass_at(ref_holder_i);\n+  Symbol *const ref_name = cache_holder.constants()->uncached_name_ref_at(resolved_info.constant_pool_index());\n+  Symbol *const ref_sig = cache_holder.constants()->uncached_signature_ref_at(resolved_info.constant_pool_index());\n+  if (resolved_info.bytecode_1() != Bytecodes::_invokehandle) {\n+    \/\/ Resolved as:\n+    InstanceKlass *const holder = method->method_holder();\n+    const Symbol *name = method->name();\n+    const Symbol *sig = method->signature();\n+    \/\/ Must match\n+    assert(ref_name == name, \"class %s, cache entry #%i: referenced %s != resolved %s\",\n+           cache_holder.external_name(), resolved_info_i,\n+           Method::external_name(ref_holder, ref_name, ref_sig), method->external_name());\n+    if (!MethodHandles::is_signature_polymorphic_name(ref_holder, ref_name)) {\n+      assert(ref_sig == sig, \"class %s, cache entry #%i: referenced %s != resolved %s\",\n+             cache_holder.external_name(), resolved_info_i,\n+             Method::external_name(ref_holder, ref_name, ref_sig), method->external_name());\n+      assert(ref_holder->is_subtype_of(holder), \"class %s, cache entry #%i (%s%s): referenced holder %s must subtype resolved holder %s\",\n+             cache_holder.external_name(), resolved_info_i, name->as_C_string(), sig->as_C_string(),\n+             ref_holder->external_name(), holder->external_name());\n+    } else {\n+      assert(ref_holder == holder, \"class %s, cache entry #%i (%s%s): holders %s and %s must match for a signature polymorhic method\",\n+             cache_holder.external_name(), resolved_info_i, name->as_C_string(), sig->as_C_string(),\n+             ref_holder->external_name(), holder->external_name());\n+    }\n+  } else {\n+    assert(MethodHandles::is_signature_polymorphic_name(ref_holder, ref_name),\n+           \"class %s, cache entry #%i: %s.%s(...) resolved as signature polymorphic\",\n+           cache_holder.external_name(), resolved_info_i, ref_holder->external_name(), ref_name->as_C_string());\n+    if (MethodHandles::is_signature_polymorphic_intrinsic_name(ref_holder, ref_name)) {\n+      assert(MethodHandles::is_signature_polymorphic_method(method) &&\n+             MethodHandles::is_signature_polymorphic_intrinsic(method->intrinsic_id()),\n+             \"class %s, cache entry #%i: %s.%s(...) resolved to %s\",\n+             cache_holder.external_name(), resolved_info_i, ref_holder->external_name(), ref_name->as_C_string(),\n+             method->external_name());\n+    } else {\n+      \/\/ Generic case, resolves to a generated adapter\n+    }\n+  }\n+\n+  assert(resolved_info.flag_state() == as_TosState(method->result_type()), \"class %s, cache entry #%i (%s): %i != %i\",\n+         cache_holder.external_name(), resolved_info_i, method->external_name(),\n+         resolved_info.flag_state(), as_TosState(method->result_type()));\n+  assert(resolved_info.parameter_size() == method->size_of_parameters(), \"class %s, cache entry #%i (%s): %i != %i\",\n+         cache_holder.external_name(), resolved_info_i, method->external_name(),\n+         resolved_info.parameter_size(), method->size_of_parameters());\n+  assert(!resolved_info.is_final() || method->is_final_method(), \"class %s, cache entry #%i (%s): non-final cached as final\",\n+         cache_holder.external_name(), resolved_info_i, method->external_name());\n+}\n+\n+static void assert_correct_unresolved_method(const InstanceKlass &cache_holder, int unresolved_info_i) {\n+  const ConstantPoolCacheEntry &unresolved_info = *cache_holder.constants()->cache()->entry_at(unresolved_info_i);\n+  assert(unresolved_info.bytecode_1() == 0 && unresolved_info.bytecode_2() == 0, \"resolved\");\n+\n+  assert(unresolved_info.flags_ord() == 0, \"class %s, cache entry #%i: unresolved entry has flags = \" INTX_FORMAT_X,\n+         cache_holder.external_name(), unresolved_info_i, unresolved_info.flags_ord());\n+  assert(unresolved_info.is_f1_null(), \"class %s, cache entry #%i: unresolved entry has f1 = \" PTR_FORMAT,\n+         cache_holder.external_name(), unresolved_info_i, p2i(unresolved_info.f1_ord()));\n+\n+  \/\/ invokehandle entries have f2 set to appendix index even when unresolved\n+  const intx f2 = unresolved_info.f2_ord();\n+  if (f2 == 0) {\n+    return; \/\/ Note: this can still be an invokehandle entry\n+  }\n+\n+  \/\/ Must be an invokehandle entry, i.e. reference a signature polymorpic method\n+  Symbol *const ref_holder_name = cache_holder.constants()->uncached_klass_ref_at_noresolve(unresolved_info.constant_pool_index());\n+  Symbol *const ref_name = cache_holder.constants()->uncached_name_ref_at(unresolved_info.constant_pool_index());\n+  InstanceKlass *ref_holder = nullptr;\n+  if (ref_holder_name == vmSymbols::java_lang_invoke_MethodHandle()) {\n+    if (!vmClasses::MethodHandle_klass_is_loaded()) {\n+      return;\n+    }\n+    ref_holder = vmClasses::MethodHandle_klass();\n+  } else if (ref_holder_name == vmSymbols::java_lang_invoke_VarHandle()) {\n+    if (!vmClasses::VarHandle_klass_is_loaded()) {\n+      return;\n+    }\n+    ref_holder = vmClasses::VarHandle_klass();\n+  }\n+  assert(ref_holder != nullptr, \"class %s, cache entry #%i: %s.%s(...) resolved as signature polymorphic\",\n+         cache_holder.external_name(), unresolved_info_i, ref_holder_name->as_klass_external_name(), ref_name->as_C_string());\n+  assert(MethodHandles::is_signature_polymorphic_name(ref_holder, ref_name),\n+         \"class %s, cache entry #%i: %s.%s(...) resolved as signature polymorphic\",\n+         cache_holder.external_name(), unresolved_info_i, ref_holder->external_name(), ref_name->as_C_string());\n+\n+  \/\/ Must be an index into resolved references array\n+  assert(f2 >= 0, \"class %s, cache entry #%i: unresolved invokehandle entry has f2 = \" PTR_FORMAT,\n+         cache_holder.external_name(), unresolved_info_i, p2i(unresolved_info.f1_ord()));\n+}\n+\n+static void assert_correctly_resolved_indy(const InstanceKlass &cache_holder, int resolved_info_i) {\n+  const ResolvedIndyEntry &resolved_info = *cache_holder.constants()->cache()->resolved_indy_entry_at(resolved_info_i);\n+\n+  const Method *adapter = resolved_info.method();\n+\n+  assert(resolved_info.return_type() == as_TosState(adapter->result_type()), \"class %s, indy entry #%i (adapter %s): %i != %i\",\n+         cache_holder.external_name(), resolved_info_i, adapter->external_name(),\n+         resolved_info.return_type(), as_TosState(adapter->result_type()));\n+  assert(resolved_info.num_parameters() == adapter->size_of_parameters(), \"class %s, indy entry #%i (adapter %s): %i != %i\",\n+         cache_holder.external_name(), resolved_info_i, adapter->external_name(),\n+         resolved_info.num_parameters(), adapter->size_of_parameters());\n+}\n+\n+#endif \/\/ ASSERT\n+\n+void CracClassStateRestorer::fill_interclass_references(InstanceKlass *ik,\n+                                                        const ParsedHeapDump &heap_dump,\n+                                                        const HeapDumpTable<InstanceKlass *, AnyObj::C_HEAP> &iks,\n+                                                        const HeapDumpTable<ArrayKlass *, AnyObj::C_HEAP> &aks,\n+                                                        const InterclassRefs &refs, TRAPS) {\n+  if (log_is_enabled(Trace, crac, class)) {\n+    ResourceMark rm;\n+    log_trace(crac, class)(\"Filling interclass references of %s\", ik->external_name());\n+  }\n+\n+  if (refs.dynamic_nest_host != HeapDump::NULL_ID) {\n+    assert(ik->is_being_restored() && !ik->is_linked(),\n+           \"only hidden classes have dynamic nest hosts and for now we re-create them all\");\n+    InstanceKlass **const host = iks.get(refs.dynamic_nest_host);\n+    guarantee(host != nullptr, \"unknown class \" HDID_FORMAT \" referenced as a dynamic nest host of %s\",\n+              refs.dynamic_nest_host, ik->external_name());\n+    ik->set_nest_host(*host);\n+  }\n+\n+  ConstantPool *const cp = ik->constants();\n+  for (const InterclassRefs::ClassRef &class_ref : *refs.cp_class_refs) {\n+    Klass *k;\n+    InstanceKlass **const ik_ptr = iks.get(class_ref.class_id);\n+    if (ik_ptr != nullptr) {\n+      k = *ik_ptr;\n+    } else {\n+      ArrayKlass **const ak_ptr = aks.get(class_ref.class_id);\n+      guarantee(ak_ptr != nullptr, \"unknown class \" HDID_FORMAT \" referenced by Class constant pool entry #%i of %s\",\n+                class_ref.class_id, class_ref.index, ik->external_name());\n+      k = *ak_ptr;\n+    }\n+    \/\/ Put the class ensuring we don't overwrite a pre-resolved class\/error\n+    const Klass *k_set = cp->klass_at_put_and_get(class_ref.index, k);\n+    if (k_set != k) {\n+      if (k_set == nullptr) {\n+        guarantee(false, \"incompatible state of pre-defined class %s: its constant pool slot #%i has class resolution error, \"\n+                  \"but it was successfully resolved to %s at class dump time\", ik->external_name(), class_ref.index, k->external_name());\n+      } else {\n+        guarantee(false, \"incompatible state of pre-defined class %s: its constant pool slot #%i is resolved to class %s when %s was expected\",\n+                  ik->external_name(), class_ref.index, k_set->external_name(), k->external_name());\n+      }\n+    }\n+  }\n+  DEBUG_ONLY(ik->constants()->verify_on(nullptr));\n+\n+  \/\/ Restore constant pool cache only if it was created by us because unresolved\n+  \/\/ entries are expected to be partially filled\n+  \/\/ TODO restore constant pool cache even if it was pre-created: check the\n+  \/\/  resolved entries have the expected values, fill the unresolved ones\n+  if (ik->is_linked() \/*pre-linked*\/ || (ik->is_shared() && ik->is_rewritten() \/*pre-rewritten*\/)) {\n+    return;\n+  }\n+  guarantee(ik->is_being_restored(), \"all uninitialized classes being restored must be marked\");\n+\n+  \/\/ Non-rewritten classes don't have a constant pool cache to restore\n+  if (!ik->is_rewritten()) {\n+    assert(!ik->is_init_thread(JavaThread::current()), \"no need for this\");\n+    assert(refs.field_refs->is_empty() && refs.method_refs->is_empty() && refs.indy_refs->is_empty(),\n+           \"class %s has unfilled references for its absent constant pool cache\", ik->external_name());\n+    return;\n+  }\n+  assert(ik->is_being_linked() && ik->is_init_thread(JavaThread::current()), \"must be rewriting the class\");\n+\n+  ConstantPoolCache &cp_cache = *cp->cache();\n+  for (const InterclassRefs::ClassRef &field_ref : *refs.field_refs) {\n+    ResolvedFieldEntry &field_entry = *cp_cache.resolved_field_entry_at(field_ref.index);\n+    InstanceKlass **const holder = iks.get(field_ref.class_id);\n+    guarantee(holder != nullptr, \"unknown class \" HDID_FORMAT \" referenced by resolved field entry #%i of %s\",\n+              field_ref.class_id, field_ref.index, ik->external_name());\n+    assert(field_entry.field_index() < (*holder)->total_fields_count(),\n+           \"class %s, field entry #%i: field holder %s, field index %i >= amount of fields in holder %i\",\n+           ik->external_name(), field_ref.index, (*holder)->external_name(), field_entry.field_index(), (*holder)->total_fields_count());\n+    field_entry.fill_in_unportable(*holder);\n+    postcond(field_entry.field_holder() == *holder);\n+  }\n+  for (const InterclassRefs::MethodRef &method_ref : *refs.method_refs) {\n+    ConstantPoolCacheEntry &cache_entry = *cp_cache.entry_at(method_ref.cache_index);\n+    if (method_ref.f1_class_id != HeapDump::NULL_ID) {\n+      InstanceKlass **const klass = iks.get(method_ref.f1_class_id);\n+      guarantee(klass != nullptr, \"unknown class \" HDID_FORMAT \" referenced by f1 in resolved method entry #%i of %s\",\n+                method_ref.f1_class_id, method_ref.cache_index, ik->external_name());\n+      if (method_ref.f1_is_method) {\n+        Symbol *const name = heap_dump.get_symbol(method_ref.f1_method_desc.name_id);\n+        Symbol *const sig = heap_dump.get_symbol(method_ref.f1_method_desc.sig_id);\n+        Method *const method = CracClassDumpParser::find_method(*klass, name, sig, method_ref.f1_method_desc.kind, true, CHECK);\n+        guarantee(method != nullptr, \"class %s has a resolved method entry #%i with f1 referencing %s method %s that cannot be found\",\n+                  ik->external_name(), method_ref.cache_index, MethodKind::name(method_ref.f1_method_desc.kind),\n+                  Method::name_and_sig_as_C_string(*klass, name, sig));\n+        cache_entry.set_f1(method);\n+        postcond(cache_entry.f1_as_method() == method);\n+      } else {\n+        cache_entry.set_f1(*klass);\n+        postcond(cache_entry.f1_as_klass() == *klass);\n+      }\n+    }\n+    if (method_ref.f2_class_id != HeapDump::NULL_ID) {\n+      InstanceKlass **const holder = iks.get(method_ref.f2_class_id);\n+      guarantee(holder != nullptr, \"unknown class \" HDID_FORMAT \" referenced by f2 in resolved method entry #%i of %s\",\n+                method_ref.f2_class_id, method_ref.cache_index, ik->external_name());\n+\n+      Symbol *const name = heap_dump.get_symbol(method_ref.f2_method_desc.name_id);\n+      Symbol *const sig = heap_dump.get_symbol(method_ref.f2_method_desc.sig_id);\n+      Method *const method = CracClassDumpParser::find_method(*holder, name, sig, method_ref.f2_method_desc.kind, false, CHECK);\n+      guarantee(method != nullptr, \"class %s has a resolved method entry #%i with f2 referencing %s method %s that cannot be found\",\n+                ik->external_name(), method_ref.cache_index, MethodKind::name(method_ref.f2_method_desc.kind),\n+                Method::name_and_sig_as_C_string(*holder, name, sig));\n+\n+#ifdef ASSERT\n+      if (!cache_entry.is_vfinal()) {\n+        assert((*holder)->is_interface(), \"class %s, cache entry #%i, f2 as interface method: holder %s is not an interface\",\n+               ik->external_name(), method_ref.cache_index, (*holder)->external_name());\n+        assert(!cache_entry.is_f1_null() && cache_entry.f1_as_klass()->is_klass() && cache_entry.f1_as_klass()->is_subtype_of(*holder),\n+               \"class %s, cache entry #%i, f2 as interface method: f1 contains class %s which does not implement f2's method's holder %s\",\n+               ik->external_name(), method_ref.cache_index,\n+               cache_entry.is_f1_null() ? \"<null>\" : (cache_entry.f1_as_klass()->is_klass() ? cache_entry.f1_as_klass()->external_name() : \"<not a class>\"),\n+               (*holder)->external_name());\n+      }\n+#endif \/\/ ASSERT\n+\n+      cache_entry.set_f2(reinterpret_cast<intx>(method));\n+      postcond((cache_entry.is_vfinal() ? cache_entry.f2_as_vfinal_method() : cache_entry.f2_as_interface_method()) == method);\n+    }\n+  }\n+  for (const InterclassRefs::IndyAdapterRef &indy_ref : *refs.indy_refs) {\n+    ResolvedIndyEntry &indy_entry = *cp_cache.resolved_indy_entry_at(indy_ref.indy_index);\n+    precond(!indy_entry.resolution_failed());\n+\n+    InstanceKlass **const holder = iks.get(indy_ref.holder_id);\n+    guarantee(holder != nullptr, \"unknown class \" HDID_FORMAT \" referenced by resolved invokedynamic entry #%i of %s\",\n+              indy_ref.holder_id, indy_ref.indy_index, ik->external_name());\n+\n+    Symbol *const name = heap_dump.get_symbol(indy_ref.method_desc.name_id);\n+    Symbol *const sig = heap_dump.get_symbol(indy_ref.method_desc.sig_id);\n+    Method *const method = CracClassDumpParser::find_method(*holder, name, sig, indy_ref.method_desc.kind, false, CHECK);\n+    guarantee(method != nullptr, \"class %s has a resolved invokedynamic entry #%i referencing %s method %s that cannot be found\",\n+              ik->external_name(), indy_ref.indy_index, MethodKind::name(indy_ref.method_desc.kind),\n+              Method::name_and_sig_as_C_string(*holder, name, sig));\n+\n+    indy_entry.adjust_method_entry(method);\n+    postcond(indy_entry.is_resolved() && indy_entry.method() == method);\n+  }\n+\n+#ifdef ASSERT\n+  for (int i = 0; i < cp_cache.resolved_field_entries_length(); i++) {\n+    const ResolvedFieldEntry &field_entry = *cp_cache.resolved_field_entry_at(i);\n+    if (field_entry.get_code() != 0) {\n+      assert_correctly_resolved_field(*ik, i);\n+    }\n+  }\n+  for (int i = 0; i < cp_cache.length(); i++) {\n+    const ConstantPoolCacheEntry &cache_entry = *cp_cache.entry_at(i);\n+    if (cache_entry.bytecode_1() != 0 || cache_entry.bytecode_2() != 0) {\n+      assert_correctly_resolved_method(*ik, i);\n+    } else {\n+      assert_correct_unresolved_method(*ik, i);\n+    }\n+  }\n+  for (int i = 0; i < cp_cache.resolved_indy_entries_length(); i++) {\n+    const ResolvedIndyEntry &indy_entry = *cp_cache.resolved_indy_entry_at(i);\n+    if (indy_entry.is_resolved()) {\n+      assert_correctly_resolved_indy(*ik, i);\n+    }\n+  }\n+#endif \/\/ ASSERT\n+}\n+\n+void CracClassStateRestorer::apply_init_state(InstanceKlass *ik, InstanceKlass::ClassState state, Handle init_error) {\n+  precond(ik->is_loaded() && ik->is_being_restored());\n+  precond(init_error.is_null() || state == InstanceKlass::initialization_error);\n+  ik->set_is_being_restored(false); \/\/ Other threads will remain waiting for the state change if needed\n+\n+  JavaThread *const thread = JavaThread::current();\n+  if (!ik->is_init_thread(thread)) {\n+    return;\n+  }\n+  postcond(ik->is_rewritten());\n+\n+  if (ik->is_being_linked()) {\n+    if (state == InstanceKlass::loaded) {\n+      \/\/ We've rewritten the class but don't want to finish linking it\n+      ik->set_initialization_state_and_notify(InstanceKlass::loaded, thread);\n+      return;\n+    }\n+    if (state == InstanceKlass::linked) {\n+      \/\/ We've linked the class\n+      ik->set_initialization_state_and_notify(InstanceKlass::linked, thread);\n+      return;\n+    }\n+    precond(state == InstanceKlass::fully_initialized || state == InstanceKlass::initialization_error);\n+    \/\/ We've linked the class but also initialized it\n+    ik->set_linked_to_be_initialized_state_and_notify(thread);\n+  }\n+  postcond(ik->is_linked());\n+\n+  precond(ik->is_being_initialized() && ik->is_init_thread(thread));\n+  if (state == InstanceKlass::initialization_error) {\n+    ik->put_initializetion_error(thread, init_error);\n+  }\n+  ik->set_initialization_state_and_notify(state, thread);\n+  postcond(ik->is_initialized() || ik->is_in_error_state());\n+}\n+\n+#ifdef ASSERT\n+\n+static void assert_interfaces_attempted_initialization(const InstanceKlass &initial, const InstanceKlass &current) {\n+  precond(initial.is_initialized() || initial.is_in_error_state());\n+  precond(current.has_nonstatic_concrete_methods());\n+  for (int i = 0; i < current.local_interfaces()->length(); i++) {\n+    const InstanceKlass &interface = *current.local_interfaces()->at(i);\n+    if (interface.declares_nonstatic_concrete_methods()) {\n+      assert(interface.is_initialized() || (current.is_in_error_state() && interface.is_in_error_state()),\n+             \"%s %s %s but its implemented interface with non-static non-abstract methods %s %s\",\n+             initial.is_interface() ? \"interface\" : \"class\", initial.external_name(),\n+             initial.is_initialized() ? \"is initialized\" : \"has failed to initialize\",\n+             interface.external_name(), initial.is_initialized() ? \"is not\" : \"has not attempted to initialize\");\n+    }\n+    if (interface.has_nonstatic_concrete_methods()) {\n+      assert_interfaces_attempted_initialization(initial, interface);\n+    }\n+  }\n+}\n+\n+void CracClassStateRestorer::assert_hierarchy_init_states_are_consistent(const InstanceKlass &ik) {\n+  precond(!ik.is_being_restored());\n+  switch (ik.init_state()) {\n+    case InstanceKlass::allocated:\n+      ShouldNotReachHere(); \/\/ Too young\n+    case InstanceKlass::being_linked:\n+      \/\/ In case some other thread picked up the class after it has been restored\n+      precond(!const_cast<InstanceKlass &>(ik).is_init_thread(JavaThread::current()));\n+    case InstanceKlass::loaded:\n+      \/\/ If the class\/interface is rewritten but not linked then either:\n+      \/\/ 1) it has failed its linkage in which case its super classes and\n+      \/\/    interfaces must be linked, or\n+      \/\/ 2) it was loaded by CDS as rewritten right away in which case no\n+      \/\/    linking has been attempted yet and its super classes and interfaces\n+      \/\/    must also be rewritten (they should also be loaded by CDS).\n+      \/\/ We don't fully check (1) because it is a stricter check and for classes\n+      \/\/ restored from dump these two cases are indifferentiable (they are not\n+      \/\/ marked as CDS-loaded even if they were in the original VM).\n+      if (ik.is_rewritten()) {\n+        if (ik.java_super() != nullptr) {\n+          assert(ik.java_super()->is_rewritten(), \"%s is rewritten but its super class %s is not\",\n+                 ik.external_name(), ik.java_super()->external_name());\n+        }\n+        for (int i = 0; i < ik.local_interfaces()->length(); i++) {\n+          const InstanceKlass &interface = *ik.local_interfaces()->at(i);\n+          assert(ik.java_super()->is_rewritten(), \"%s is rewritten but its implemented interface %s is not\",\n+                 ik.external_name(), interface.external_name());\n+        }\n+      }\n+      return;\n+    case InstanceKlass::being_initialized:\n+      \/\/ In case some other thread picked up the class after it has been restored\n+      precond(!const_cast<InstanceKlass &>(ik).is_init_thread(JavaThread::current()));\n+    case InstanceKlass::linked:\n+      \/\/ Supers and interfaces of linked class\/interface must be linked\n+      if (ik.java_super() != nullptr) {\n+        assert(ik.java_super()->is_linked(), \"%s is linked but its super class %s is not\",\n+                ik.external_name(), ik.java_super()->external_name());\n+      }\n+      for (int i = 0; i < ik.local_interfaces()->length(); i++) {\n+        const InstanceKlass &interface = *ik.local_interfaces()->at(i);\n+        assert(ik.java_super()->is_linked(), \"%s is linked but its implemented interface %s is not\",\n+                ik.external_name(), interface.external_name());\n+      }\n+      return;\n+    case InstanceKlass::fully_initialized:\n+    case InstanceKlass::initialization_error:\n+      \/\/ If this is a class (not interface) that has attempted initialization\n+      \/\/ then supers and interfaces with non-static non-abstract (aka default)\n+      \/\/ methods must have also attempted it (and succeeded, if the class has)\n+      if (!ik.is_interface()) {\n+        if (ik.java_super() != nullptr) {\n+          assert(ik.java_super()->is_initialized() || (ik.is_in_error_state() && ik.java_super()->is_in_error_state()),\n+                 \"class %s %s but its super class %s %s\",\n+                 ik.external_name(), ik.is_initialized() ? \"is initialized\" : \"has failed to initialize\",\n+                 ik.java_super()->external_name(), ik.is_initialized() ? \"is not\" : \"has not attempted to initialize\");\n+        }\n+        if (ik.has_nonstatic_concrete_methods()) {\n+          \/\/ Need to recursively check all interfaces because of situations like\n+          \/\/ \"this class implements interface I1 w\/o default methods which\n+          \/\/ implements interface I2 w\/ default methods\" -- I1 can will be\n+          \/\/ uninitialized but we should check I2 is initialized\n+          assert_interfaces_attempted_initialization(ik, ik);\n+        }\n+      }\n+      return;\n+    default:\n+      ShouldNotReachHere();\n+  }\n+}\n+\n+#endif \/\/ ASSERT\n","filename":"src\/hotspot\/share\/runtime\/cracClassStateRestorer.cpp","additions":872,"deletions":0,"binary":false,"changes":872,"status":"added"},{"patch":"@@ -0,0 +1,77 @@\n+#ifndef SHARE_UTILITIES_CRACCLASSSTATERESTORER_HPP\n+#define SHARE_UTILITIES_CRACCLASSSTATERESTORER_HPP\n+\n+#include \"memory\/allStatic.hpp\"\n+#include \"memory\/allocation.hpp\"\n+#include \"oops\/arrayKlass.hpp\"\n+#include \"oops\/instanceKlass.hpp\"\n+#include \"runtime\/handles.hpp\"\n+#include \"utilities\/exceptions.hpp\"\n+#include \"utilities\/globalDefinitions.hpp\"\n+#include \"utilities\/growableArray.hpp\"\n+#include \"utilities\/heapDumpParser.hpp\"\n+#include \"utilities\/methodKind.hpp\"\n+\n+\/\/ Class may reference other classes and while the class dump format guarantees\n+\/\/ that some of such references (class loader class, super class, etc.) will be\n+\/\/ created before the class itself, there is no such guarantee for all class\n+\/\/ references (since there may be cycles). Thus some interclass references can\n+\/\/ only be filled-in after all classes have been created.\n+struct InterclassRefs : public ResourceObj {\n+  \/\/ Restoring just the constant pool reference to the nest host is insufficient\n+  \/\/ if it is a dynamic nest host which does not come from the constant pool\n+  HeapDump::ID dynamic_nest_host;\n+\n+  struct ClassRef {\n+    u2 index;                     \/\/ Contents depend on the context (see below)\n+    HeapDump::ID class_id;\n+  };\n+  struct MethodDescription {\n+    HeapDump::ID name_id;\n+    HeapDump::ID sig_id;\n+    MethodKind::Enum kind;\n+  };\n+  struct MethodRef {\n+    int cache_index;\n+    bool f1_is_method;\n+    HeapDump::ID f1_class_id;         \/\/ Null ID if unset\n+    MethodDescription f1_method_desc; \/\/ Undefined if f1_is_method == false\n+    HeapDump::ID f2_class_id;         \/\/ Null ID if unset\n+    MethodDescription f2_method_desc; \/\/ Undefined if f2_class_id is unset\n+  };\n+  struct IndyAdapterRef {\n+    int indy_index;\n+    HeapDump::ID holder_id;\n+    MethodDescription method_desc;\n+  };\n+\n+  \/\/ Constant pool class references. Index is the constant pool index.\n+  GrowableArray<ClassRef> *cp_class_refs = new GrowableArray<ClassRef>();\n+  \/\/ Holders of resolved fields. Index is the resolved fields index.\n+  GrowableArray<ClassRef> *field_refs = new GrowableArray<ClassRef>();\n+  \/\/ Class\/method references from resolved methods.\n+  GrowableArray<MethodRef> *method_refs = new GrowableArray<MethodRef>();\n+  \/\/ Adapter method references from resolved invokedynamics.\n+  GrowableArray<IndyAdapterRef> *indy_refs = new GrowableArray<IndyAdapterRef>();\n+};\n+\n+struct CracClassStateRestorer : public AllStatic {\n+  \/\/ Defines the created class and makes the current thread hold its init state\n+  \/\/ if needed. Returns the defined class which may differ from the created one\n+  \/\/ iff the class has been pre-defined.\n+  static InstanceKlass *define_created_class(InstanceKlass *created_ik, InstanceKlass::ClassState target_state, TRAPS);\n+\n+  static void fill_interclass_references(InstanceKlass *ik,\n+                                         const ParsedHeapDump &heap_dump,\n+                                         const HeapDumpTable<InstanceKlass *, AnyObj::C_HEAP> &iks,\n+                                         const HeapDumpTable<ArrayKlass *, AnyObj::C_HEAP> &aks,\n+                                         const InterclassRefs &refs, TRAPS);\n+\n+  static void apply_init_state(InstanceKlass *ik, InstanceKlass::ClassState state, Handle init_error);\n+\n+  \/\/ Checks that initialization state of this class is consistent with the\n+  \/\/ states of its super class and implemented interfaces.\n+  static void assert_hierarchy_init_states_are_consistent(const InstanceKlass &ik) NOT_DEBUG_RETURN;\n+};\n+\n+#endif \/\/ SHARE_UTILITIES_CRACCLASSSTATERESTORER_HPP\n","filename":"src\/hotspot\/share\/runtime\/cracClassStateRestorer.hpp","additions":77,"deletions":0,"binary":false,"changes":77,"status":"added"},{"patch":"@@ -0,0 +1,1733 @@\n+#include \"precompiled.hpp\"\n+#include \"classfile\/javaClasses.hpp\"\n+#include \"classfile\/stringTable.hpp\"\n+#include \"classfile\/symbolTable.hpp\"\n+#include \"classfile\/systemDictionary.hpp\"\n+#include \"classfile\/vmClasses.hpp\"\n+#include \"classfile\/vmSymbols.hpp\"\n+#include \"logging\/log.hpp\"\n+#include \"memory\/oopFactory.hpp\"\n+#include \"memory\/resourceArea.hpp\"\n+#include \"memory\/universe.hpp\"\n+#include \"oops\/arrayKlass.hpp\"\n+#include \"oops\/instanceKlass.hpp\"\n+#include \"oops\/instanceKlass.inline.hpp\"\n+#include \"oops\/markWord.hpp\"\n+#include \"oops\/objArrayKlass.hpp\"\n+#include \"oops\/oopsHierarchy.hpp\"\n+#include \"oops\/symbol.hpp\"\n+#include \"oops\/symbolHandle.hpp\"\n+#include \"runtime\/cracClassDumpParser.hpp\"\n+#include \"runtime\/cracClassStateRestorer.hpp\"\n+#include \"runtime\/cracHeapRestorer.hpp\"\n+#include \"runtime\/cracStackDumpParser.hpp\"\n+#include \"runtime\/fieldDescriptor.hpp\"\n+#include \"runtime\/fieldDescriptor.inline.hpp\"\n+#include \"runtime\/handles.hpp\"\n+#include \"runtime\/javaCalls.hpp\"\n+#include \"runtime\/javaThread.hpp\"\n+#include \"runtime\/reflectionUtils.hpp\"\n+#include \"runtime\/signature.hpp\"\n+#include \"runtime\/thread.hpp\"\n+#include \"runtime\/threadIdentifier.hpp\"\n+#include \"utilities\/bitCast.hpp\"\n+#include \"utilities\/debug.hpp\"\n+#include \"utilities\/exceptions.hpp\"\n+#include \"utilities\/formatBuffer.hpp\"\n+#include \"utilities\/globalDefinitions.hpp\"\n+#include \"utilities\/growableArray.hpp\"\n+#include \"utilities\/heapDumpClasses.hpp\"\n+#include \"utilities\/heapDumpParser.hpp\"\n+#include \"utilities\/hprofTag.hpp\"\n+#include \"utilities\/macros.hpp\"\n+#include \"utilities\/methodKind.hpp\"\n+#ifdef ASSERT\n+#include \"code\/dependencyContext.hpp\"\n+#include \"utilities\/autoRestore.hpp\"\n+#endif \/\/ ASSERT\n+\n+\/\/ #############################################################################\n+\/\/ WellKnownObjects implementation\n+\/\/ #############################################################################\n+\n+void WellKnownObjects::find_well_known_class_loaders(const ParsedHeapDump &heap_dump, TRAPS) {\n+  heap_dump.load_classes.iterate([&](HeapDump::ID _, const HeapDump::LoadClass &lc) -> bool {\n+    const Symbol *name = heap_dump.get_symbol(lc.class_name_id);\n+    if (name == vmSymbols::jdk_internal_loader_ClassLoaders()) {\n+      lookup_builtin_class_loaders(heap_dump, lc);\n+    } else if (name == vmSymbols::java_lang_ClassLoader()) {\n+      lookup_actual_system_class_loader(heap_dump, lc);\n+    }\n+    return (_platform_loader_id == HeapDump::NULL_ID && _builtin_system_loader_id == HeapDump::NULL_ID) ||\n+           _actual_system_loader_id == HeapDump::NULL_ID;\n+  });\n+\n+  const bool platform_found = _platform_loader_id != HeapDump::NULL_ID;\n+  const bool builtin_sys_found = _builtin_system_loader_id != HeapDump::NULL_ID;\n+  const bool actual_sys_found = _actual_system_loader_id != HeapDump::NULL_ID;\n+  guarantee(!actual_sys_found || (platform_found && builtin_sys_found),\n+            \"system class loader cannot be present when built-in class loaders are absent\");\n+  guarantee(!builtin_sys_found || platform_found,\n+            \"built-in system class loader cannot be present when the platform class loader is absent\");\n+  guarantee(!platform_found || (_platform_loader_id != _builtin_system_loader_id &&\n+                                _platform_loader_id != _actual_system_loader_id),\n+            \"platform and system class loaders cannot be the same instance\");\n+\n+  \/\/ If there is a diviation, abort the restoration\n+  if (builtin_sys_found && SystemDictionary::java_system_loader() != nullptr) {\n+    const bool is_dumped_actual_sys_builtin = _builtin_system_loader_id == _actual_system_loader_id;\n+    const bool is_current_actual_sys_builtin = SystemDictionary::java_system_loader()->klass() == vmClasses::jdk_internal_loader_ClassLoaders_AppClassLoader_klass();\n+    if (is_dumped_actual_sys_builtin != is_current_actual_sys_builtin) {\n+      THROW_MSG(vmSymbols::java_lang_UnsupportedOperationException(),\n+                err_msg(\"Dumped system class loader is%s the built-in one while in the current VM it is%s\",\n+                        is_dumped_actual_sys_builtin ? \"\" : \" not\", is_current_actual_sys_builtin ? \"\" : \" not\"));\n+    }\n+  }\n+\n+  log_info(crac)(\"Found well known class loaders' IDs: platform - \"         HDID_FORMAT \", \"\n+                                                      \"built-in system - \"  HDID_FORMAT \", \"\n+                                                      \"actual system - \"    HDID_FORMAT,\n+                 _platform_loader_id, _builtin_system_loader_id, _actual_system_loader_id);\n+}\n+\n+\/\/ This relies on the ClassLoader.get*ClassLoader() implementation detail: the\n+\/\/ built-in platform and system class loaders are stored in\n+\/\/ PLATFORM_LOADER\/APP_LOADER static fields of jdk.internal.loader.ClassLoaders.\n+void WellKnownObjects::lookup_builtin_class_loaders(const ParsedHeapDump &heap_dump,\n+                                                    const HeapDump::LoadClass &jdk_internal_loader_ClassLoaders) {\n+  static constexpr char PLATFORM_LOADER_FIELD_NAME[] = \"PLATFORM_LOADER\";\n+  static constexpr char APP_LOADER_FIELD_NAME[] = \"APP_LOADER\";\n+  precond(heap_dump.get_symbol(jdk_internal_loader_ClassLoaders.class_name_id) ==\n+          vmSymbols::jdk_internal_loader_ClassLoaders());\n+\n+  \/\/ We have a jdk.internal.loader.ClassLoaders but is this the internal one (i.e. boot-loaded)?\n+  const HeapDump::ClassDump &dump = heap_dump.get_class_dump(jdk_internal_loader_ClassLoaders.class_id);\n+  if (dump.class_loader_id != HeapDump::NULL_ID) {\n+    return;\n+  }\n+  \/\/ From now on we know we have THE jdk.internal.loader.ClassLoaders\n+\n+  guarantee(_platform_loader_id == HeapDump::NULL_ID && _builtin_system_loader_id == HeapDump::NULL_ID,\n+            \"class %s dumped multiple times\", vmSymbols::jdk_internal_loader_ClassLoaders()->as_klass_external_name());\n+  for (u2 i = 0; i < dump.static_fields.size(); i++) {\n+    const HeapDump::ClassDump::Field &field_dump = dump.static_fields[i];\n+    if (field_dump.info.type != HPROF_NORMAL_OBJECT) {\n+      continue;\n+    }\n+    const Symbol *field_name = heap_dump.get_symbol(field_dump.info.name_id);\n+    if (field_name->equals(PLATFORM_LOADER_FIELD_NAME)) {\n+      guarantee(_platform_loader_id == HeapDump::NULL_ID,\n+                \"static field %s is repeated in %s dump \" HDID_FORMAT, PLATFORM_LOADER_FIELD_NAME,\n+                vmClasses::jdk_internal_loader_ClassLoaders_klass()->external_name(), dump.id);\n+      _platform_loader_id = field_dump.value.as_object_id;       \/\/ Can be null if VM was dumped before initializing it\n+    } else if (field_name->equals(APP_LOADER_FIELD_NAME)) {\n+      guarantee(_builtin_system_loader_id == HeapDump::NULL_ID,\n+                \"static field %s is repeated in %s dump \" HDID_FORMAT, APP_LOADER_FIELD_NAME,\n+                vmClasses::jdk_internal_loader_ClassLoaders_klass()->external_name(), dump.id);\n+      _builtin_system_loader_id = field_dump.value.as_object_id; \/\/ Can be null if VM was dumped before initializing it\n+    }\n+  }\n+}\n+\n+\/\/ This relies on ClassLoader.getSystemClassLoader() implementation detail: the\n+\/\/ actual system class loader is stored in \"scl\" static field of j.l.ClassLoader.\n+void WellKnownObjects::lookup_actual_system_class_loader(const ParsedHeapDump &heap_dump,\n+                                                         const HeapDump::LoadClass &java_lang_ClassLoader) {\n+  static constexpr char SCL_FIELD_NAME[] = \"scl\";\n+  precond(heap_dump.get_symbol(java_lang_ClassLoader.class_name_id) == vmSymbols::java_lang_ClassLoader());\n+\n+  \/\/ We know we have THE j.l.ClassLoader because classes from java.* packages cannot be non-boot-loaded\n+  const HeapDump::ClassDump &dump = heap_dump.get_class_dump(java_lang_ClassLoader.class_id);\n+  guarantee(dump.class_loader_id == HeapDump::NULL_ID, \"class %s can only be loaded by the bootstrap class loader\",\n+            vmSymbols::java_lang_ClassLoader()->as_klass_external_name());\n+\n+  guarantee(_actual_system_loader_id == HeapDump::NULL_ID, \"class %s dumped multiple times\",\n+            vmSymbols::java_lang_ClassLoader()->as_klass_external_name());\n+  for (u2 i = 0; i < dump.static_fields.size(); i++) {\n+    const HeapDump::ClassDump::Field &field_dump = dump.static_fields[i];\n+    if (field_dump.info.type != HPROF_NORMAL_OBJECT) {\n+      continue;\n+    }\n+    const Symbol *field_name = heap_dump.get_symbol(field_dump.info.name_id);\n+    if (field_name->equals(SCL_FIELD_NAME)) {\n+      guarantee(_actual_system_loader_id == HeapDump::NULL_ID, \"static field %s is repeated in %s dump \" HDID_FORMAT,\n+                SCL_FIELD_NAME, vmSymbols::java_lang_ClassLoader()->as_klass_external_name(), dump.id);\n+      _actual_system_loader_id = field_dump.value.as_object_id; \/\/ Can be null if VM was dumped before initializing it\n+    }\n+  }\n+}\n+\n+static instanceOop get_builtin_system_loader() {\n+  \/\/ SystemDictionary::java_system_loader() gives the actual system loader which\n+  \/\/ is not necessarily the built-in one\n+  const oop loader = SystemDictionary::java_system_loader();\n+  if (loader != nullptr &&\n+      loader->klass() != vmClasses::jdk_internal_loader_ClassLoaders_AppClassLoader_klass()) {\n+    \/\/ TODO need to call into Java (ClassLoaders.appClassLoader()) or retrieve\n+    \/\/ the oop from ClassLoaders::APP_LOADER manually\n+    log_error(crac)(\"User-provided system class loader is not supported yet\");\n+    Unimplemented();\n+  }\n+  return static_cast<instanceOop>(loader);\n+}\n+\n+void WellKnownObjects::put_into(HeapDumpTable<Handle, AnyObj::C_HEAP> *objects) const {\n+  precond(objects->number_of_entries() == 0);\n+  Thread *const thread = JavaThread::current();\n+  if (_platform_loader_id != HeapDump::NULL_ID) {\n+    const auto loader = static_cast<instanceOop>(SystemDictionary::java_platform_loader());\n+    if (loader != nullptr) {\n+      guarantee(loader->klass() == vmClasses::jdk_internal_loader_ClassLoaders_PlatformClassLoader_klass(), \"sanity check\");\n+      objects->put_when_absent(_platform_loader_id, instanceHandle(thread, loader));\n+    }\n+  }\n+  if (_builtin_system_loader_id != HeapDump::NULL_ID) {\n+    const auto loader = get_builtin_system_loader();\n+    if (loader != nullptr) {\n+      objects->put_when_absent(_builtin_system_loader_id, instanceHandle(thread, loader));\n+    }\n+  }\n+  if (_actual_system_loader_id != HeapDump::NULL_ID && _actual_system_loader_id != _builtin_system_loader_id) {\n+    const auto loader = static_cast<instanceOop>(SystemDictionary::java_system_loader());\n+    if (loader != nullptr) {\n+      objects->put_when_absent(_builtin_system_loader_id, instanceHandle(thread, loader));\n+    }\n+  }\n+  objects->maybe_grow();\n+}\n+\n+void WellKnownObjects::get_from(const HeapDumpTable<Handle, AnyObj::C_HEAP> &objects) const {\n+  if (_platform_loader_id != HeapDump::NULL_ID) {\n+    const Handle *restored = objects.get(_platform_loader_id);\n+    if (restored != nullptr) {\n+      const oop existing = SystemDictionary::java_platform_loader();\n+      if (existing != nullptr) {\n+        guarantee(*restored == existing, \"restored platform loader must be the existing one\");\n+      } else {\n+        log_error(crac)(\"Restoration of base class loaders is not implemented\");\n+        Unimplemented();\n+      }\n+    }\n+  }\n+  if (_builtin_system_loader_id != HeapDump::NULL_ID) {\n+    const Handle *restored = objects.get(_builtin_system_loader_id);\n+    if (restored != nullptr) {\n+      const oop existing = get_builtin_system_loader();\n+      if (existing != nullptr) {\n+        guarantee(*restored == existing, \"restored builtin system loader must be the existing one\");\n+      } else {\n+        log_error(crac)(\"Restoration of base class loaders is not implemented\");\n+        Unimplemented();\n+      }\n+    }\n+  }\n+  if (_actual_system_loader_id != HeapDump::NULL_ID) {\n+    const Handle *restored = objects.get(_actual_system_loader_id);\n+    if (restored != nullptr) {\n+      const oop existing = SystemDictionary::java_system_loader();\n+      if (existing != nullptr) {\n+        guarantee(*restored == existing, \"restored actual system loader must be the existing one\");\n+      } else {\n+        log_error(crac)(\"Restoration of base class loaders is not implemented\");\n+        Unimplemented();\n+      }\n+    }\n+  }\n+}\n+\n+\/\/ #############################################################################\n+\/\/ CracHeapRestorer implementation\n+\/\/ #############################################################################\n+\n+\/\/ Helpers\n+\n+InstanceKlass &CracHeapRestorer::get_instance_class(HeapDump::ID id) const {\n+  InstanceKlass **const ik_ptr = _instance_classes.get(id);\n+  guarantee(ik_ptr != nullptr, \"unknown instance class \" HDID_FORMAT \" referenced\", id);\n+  assert(*ik_ptr != nullptr, \"must be\");\n+  return **ik_ptr;\n+}\n+\n+ArrayKlass &CracHeapRestorer::get_array_class(HeapDump::ID id) const {\n+  ArrayKlass **const ak_ptr = _array_classes.get(id);\n+  guarantee(ak_ptr != nullptr, \"unknown array class \" HDID_FORMAT \" referenced\", id);\n+  assert(*ak_ptr != nullptr, \"must be\");\n+  return **ak_ptr;\n+}\n+\n+Handle CracHeapRestorer::get_object_when_present(HeapDump::ID id) const {\n+  assert(id != HeapDump::NULL_ID, \"nulls are not recorded\");\n+  assert(_objects.contains(id), \"object \" HDID_FORMAT \" was expected to be recorded\", id);\n+  return *_objects.get(id);\n+}\n+\n+Handle CracHeapRestorer::get_object_if_present(HeapDump::ID id) const {\n+  assert(id != HeapDump::NULL_ID, \"nulls are not recorded\");\n+  const Handle *obj_h = _objects.get(id);\n+  return obj_h != nullptr ? *obj_h : Handle();\n+}\n+\n+void CracHeapRestorer::put_object_when_absent(HeapDump::ID id, Handle obj) {\n+  assert(id != HeapDump::NULL_ID && obj.not_null(), \"nulls should not be recorded\");\n+  assert(!_objects.contains(id), \"object \" HDID_FORMAT \" was expected to be absent\", id);\n+  _objects.put_when_absent(id, obj);\n+  _objects.maybe_grow();\n+}\n+\n+void CracHeapRestorer::put_object_if_absent(HeapDump::ID id, Handle obj) {\n+  assert(id != HeapDump::NULL_ID && obj.not_null(), \"nulls should not be recorded\");\n+  bool is_absent;\n+  const Handle &res = *_objects.put_if_absent(id, obj, &is_absent);\n+  guarantee(res == obj, \"two different objects restored for ID \" HDID_FORMAT \": \" PTR_FORMAT \" (%s) != \" PTR_FORMAT \" (%s)\",\n+            id, p2i(res()), res->klass()->external_name(), p2i(obj()), obj->klass()->external_name());\n+  if (is_absent) {\n+    _objects.maybe_grow();\n+  }\n+}\n+\n+#ifdef ASSERT\n+static void assert_builtin_class_instance(const ParsedHeapDump &heap_dump, HeapDump::ID obj_id,\n+                                          const Symbol *expected_class_name)  {\n+  precond(obj_id != HeapDump::NULL_ID);\n+  const HeapDump::InstanceDump &dump = heap_dump.get_instance_dump(obj_id);\n+  const Symbol *class_name = heap_dump.get_class_name(dump.class_id);\n+  const HeapDump::ID &class_loader_id = heap_dump.get_class_dump(dump.class_id).class_loader_id;\n+  assert(class_name == expected_class_name && class_loader_id == HeapDump::NULL_ID,\n+         \"expected object \" HDID_FORMAT \" to be of the boot-loaded class %s but its class is %s loaded by \" HDID_FORMAT,\n+         obj_id, expected_class_name->as_klass_external_name(), class_name->as_klass_external_name(), class_loader_id);\n+}\n+#endif \/\/ ASSERT\n+\n+\/\/ Class loader preparation\n+\n+instanceHandle CracHeapRestorer::get_class_loader_parent(const HeapDump::InstanceDump &loader_dump, TRAPS) {\n+  const HeapDump::ID parent_id = _loader_dump_reader.parent(loader_dump);\n+  guarantee(parent_id != loader_dump.id, \"class loader hierarchy circularity: \"\n+            HDID_FORMAT \" references itself as its parent\", loader_dump.id);\n+  const instanceHandle loader = get_class_loader(parent_id, CHECK_({}));\n+  return loader;\n+}\n+\n+instanceHandle CracHeapRestorer::get_class_loader_name(const HeapDump::InstanceDump &loader_dump, bool with_id, TRAPS) {\n+  const HeapDump::ID name_id = with_id ? _loader_dump_reader.nameAndId(loader_dump) : _loader_dump_reader.name(loader_dump);\n+  if (name_id == HeapDump::NULL_ID) {\n+    return {};\n+  }\n+  DEBUG_ONLY(assert_builtin_class_instance(_heap_dump, name_id, vmSymbols::java_lang_String()));\n+\n+  const Handle &str = restore_object(name_id, CHECK_({}));\n+  guarantee(str->klass() == vmClasses::String_klass(), \"class loader \" HDID_FORMAT \" has its '%s' field referencing a %s \"\n+            \"but it must reference a %s\", loader_dump.id, with_id ? \"nameAndId\" : \"name\",\n+            str->klass()->external_name(), vmSymbols::java_lang_String()->as_klass_external_name());\n+\n+  return static_cast<const instanceHandle &>(str);\n+}\n+\n+instanceHandle CracHeapRestorer::get_class_loader_unnamed_module(const HeapDump::InstanceDump &loader_dump, TRAPS) {\n+  const HeapDump::ID unnamed_module_id = _loader_dump_reader.unnamedModule(loader_dump);\n+  guarantee(unnamed_module_id != HeapDump::NULL_ID, \"class loader \" HDID_FORMAT \" cannot be used to load classes: \"\n+            \"its 'unnamedModule' field is not set\", loader_dump.id);\n+  DEBUG_ONLY(assert_builtin_class_instance(_heap_dump, unnamed_module_id, vmSymbols::java_lang_Module()));\n+\n+  const Handle &unnamed_module = restore_object(unnamed_module_id, CHECK_({}));\n+  guarantee(unnamed_module->klass() == vmClasses::Module_klass(),\n+            \"class loader \" HDID_FORMAT \" has its 'unnamedModule' field referencing a %s but it must reference a %s\",\n+            loader_dump.id, unnamed_module->klass()->external_name(), vmSymbols::java_lang_Module()->as_klass_external_name());\n+#ifdef ASSERT\n+  { \/\/ Would be better to check all fields but loader are null and do it before restoring the object, but it's harder\n+    assert(java_lang_Module::name(unnamed_module()) == nullptr,\n+           \"unnamed module of class loader \" HDID_FORMAT \" is not unnamed\", loader_dump.id);\n+    const Handle loader = get_object_when_present(loader_dump.id);\n+    assert(loader == java_lang_Module::loader(unnamed_module()),\n+           \"unnamed module of class loader \" HDID_FORMAT \" belongs to a different class loader\", loader_dump.id);\n+  }\n+#endif \/\/ ASSERT\n+  return static_cast<const instanceHandle &>(unnamed_module);\n+}\n+\n+instanceHandle CracHeapRestorer::get_class_loader_parallel_lock_map(const HeapDump::InstanceDump &loader_dump, TRAPS) {\n+  const HeapDump::ID map_id = _loader_dump_reader.parallelLockMap(loader_dump);\n+  if (map_id == HeapDump::NULL_ID) {\n+    return {};\n+  }\n+  DEBUG_ONLY(assert_builtin_class_instance(_heap_dump, map_id, vmSymbols::java_util_concurrent_ConcurrentHashMap()));\n+\n+  \/\/ Check for null above, so it's either already created or we need to create one\n+  const Handle existing_map = get_object_if_present(map_id);\n+  if (existing_map == nullptr) {\n+    const instanceHandle map = vmClasses::ConcurrentHashMap_klass()->allocate_instance_handle(CHECK_({}));\n+    put_object_when_absent(map_id, map);\n+    return map;\n+  }\n+  guarantee(existing_map->klass() == vmClasses::ConcurrentHashMap_klass(),\n+            \"class loader \" HDID_FORMAT \" has its 'parallelLockMap' field referencing a %s but it must reference a %s\",\n+            loader_dump.id, existing_map->klass()->external_name(), vmClasses::ConcurrentHashMap_klass()->external_name());\n+  precond(existing_map->is_instance());\n+  return *static_cast<const instanceHandle *>(&existing_map);\n+}\n+\n+\/\/ Allocates class loader and restores fields the VM may use for class loading:\n+\/\/ - parent -- to set dependent classes, to get non-reflection loader\n+\/\/ - name and nameAndId -- to create a CLD and print logs\/errors\n+\/\/ - unnamedModule -- used and partially filled when creating the CLD\n+\/\/ - parallelLockMap -- defines whether the class loader is parallel capable,\n+\/\/   only need the null\/not-null fact, so no need to restore its state yet\n+instanceHandle CracHeapRestorer::prepare_class_loader(HeapDump::ID id, TRAPS) {\n+  log_trace(crac)(\"Preparing class loader \" HDID_FORMAT, id);\n+  assert(id != HeapDump::NULL_ID, \"cannot prepare the bootstrap loader\");\n+  const HeapDump::InstanceDump &dump = _heap_dump.get_instance_dump(id);\n+  _loader_dump_reader.ensure_initialized(_heap_dump, dump.class_id);\n+\n+  InstanceKlass &loader_klass = get_instance_class(dump.class_id);\n+  guarantee(loader_klass.is_class_loader_instance_klass(),\n+            \"class loader \" HDID_FORMAT \" is of class %s (\" HDID_FORMAT \") \"\n+            \"which does not subclass %s\", id, loader_klass.external_name(), dump.class_id,\n+            vmSymbols::java_lang_ClassLoader()->as_klass_external_name());\n+  guarantee(loader_klass.is_being_restored() || loader_klass.is_initialized(), \"class loader \" HDID_FORMAT \" cannot be an instance of \"\n+            \"uninitialized class %s (\" HDID_FORMAT \")\", dump.id, loader_klass.external_name(), dump.class_id);\n+  loader_klass.check_valid_for_instantiation(true, CHECK_({}));\n+\n+  const instanceHandle loader = loader_klass.allocate_instance_handle(CHECK_({}));\n+  put_object_when_absent(id, loader); \/\/ Must record right now to be able to find it when restoring unnamedModule\n+  _prepared_loaders.put_when_absent(id, true);\n+  _prepared_loaders.maybe_grow();\n+\n+  {\n+    const instanceHandle parent = get_class_loader_parent(dump, CHECK_({}));\n+    java_lang_ClassLoader::set_parent(loader(), parent());\n+  }\n+  {\n+    const instanceHandle name = get_class_loader_name(dump, \/* with_id = *\/ false, CHECK_({}));\n+    java_lang_ClassLoader::set_name(loader(), name());\n+  }\n+  {\n+    const instanceHandle name_and_id = get_class_loader_name(dump, \/* with_id = *\/ true, CHECK_({}));\n+    java_lang_ClassLoader::set_nameAndId(loader(), name_and_id());\n+  }\n+  {\n+    const instanceHandle unnamedModule = get_class_loader_unnamed_module(dump, CHECK_({}));\n+    java_lang_ClassLoader::set_unnamedModule(loader(), unnamedModule());\n+  }\n+  {\n+    const instanceHandle parallel_lock_map = get_class_loader_parallel_lock_map(dump, CHECK_({}));\n+    java_lang_ClassLoader::set_parallelLockMap(loader(), parallel_lock_map());\n+  }\n+\n+  if (java_lang_ClassLoader::parallelCapable(loader())) { \/\/ Works because we set parallelLockMap above\n+    \/\/ TODO should add it into ClassLoader$ParallelLoaders::loaderTypes array\n+    log_error(crac)(\"Restoration of parallel-capable class loaders is not implemented\");\n+    Unimplemented();\n+  }\n+\n+  if (log_is_enabled(Trace, crac)) {\n+    ResourceMark rm;\n+    log_trace(crac)(\"Prepared class loader \" HDID_FORMAT \" (%s)\", id, loader->klass()->external_name());\n+  }\n+  return loader;\n+}\n+\n+instanceHandle CracHeapRestorer::get_class_loader(HeapDump::ID id, TRAPS) {\n+  if (id == HeapDump::NULL_ID) {\n+    return {}; \/\/ Bootstrap loader\n+  }\n+\n+  const Handle existing_loader = get_object_if_present(id);\n+  if (existing_loader.not_null()) {\n+    guarantee(existing_loader->klass()->is_class_loader_instance_klass(),\n+              \"object \" HDID_FORMAT \" is not a class loader: its class %s does not subclass %s\",\n+              id, existing_loader->klass()->external_name(), vmSymbols::java_lang_ClassLoader()->as_klass_external_name());\n+    precond(existing_loader->is_instance());\n+    return *static_cast<const instanceHandle *>(&existing_loader);\n+  }\n+\n+  precond(!_prepared_loaders.contains(id));\n+  const instanceHandle loader = prepare_class_loader(id, CHECK_({})); \/\/ Allocate and partially restore the loader\n+  postcond(_prepared_loaders.contains(id) && get_object_when_present(id) == loader());\n+  guarantee(loader.not_null() && loader->klass()->is_class_loader_instance_klass(), \"must be a class loader\");\n+\n+  return loader;\n+}\n+\n+\/\/ Heap restoration driver\n+\n+static bool is_jdk_crac_Core(const InstanceKlass &ik) {\n+  if (ik.name() == vmSymbols::jdk_crac_Core() && ik.class_loader_data()->is_the_null_class_loader_data()) {\n+    assert(ik.is_initialized(), \"%s is not pre-initialized\", ik.external_name());\n+    return true;\n+  }\n+  return false;\n+}\n+\n+void CracHeapRestorer::restore_heap(const HeapDumpTable<UnfilledClassInfo, AnyObj::C_HEAP> &class_infos,\n+                                    const GrowableArrayView<CracStackTrace *> &stack_traces, TRAPS) {\n+  log_info(crac)(\"Started heap restoration\");\n+  HandleMark hm(Thread::current());\n+\n+  \/\/ Before actually restoring anything, record existing objects so that they\n+  \/\/ are not re-created\n+  \/\/ TODO Only a few pre-existing objects are recorded. Recording them all would\n+  \/\/  require non-trivial matching since there already exists a new state.\n+  \/\/  Instead in future we'll move this into a more early VM init state before\n+  \/\/  any of Java codes have been executed so that we'll only need to record\n+  \/\/  a statically known amount of objects (mainly primitive class mirrors).\n+  record_main_thread(stack_traces);\n+  _heap_dump.class_dumps.iterate([&](HeapDump::ID _, const HeapDump::ClassDump &dump) -> bool {\n+    find_and_record_class_mirror(dump, CHECK_false);\n+    return true;\n+  });\n+  if (HAS_PENDING_EXCEPTION) {\n+    return;\n+  }\n+\n+  \/\/ Restore objects reachable from classes being restored.\n+  \/\/ TODO should also restore array and primitive mirrors?\n+  _instance_classes.iterate([&](HeapDump::ID class_id, InstanceKlass *ik) -> bool {\n+    if (!ik->is_being_restored()) {\n+      \/\/ TODO jdk.crac.Core is pre-initialized but we need to restore its fields\n+      \/\/  since the global resource context is among them. This discards the new\n+      \/\/  global context but we assume it is a subset of the restored one. Such\n+      \/\/  special treatment should be removed when we implement restoration of\n+      \/\/  all classes (it should stop being pre-initialized then).\n+      if (is_jdk_crac_Core(*ik)) {\n+        const HeapDump::ClassDump &dump = _heap_dump.get_class_dump(class_id);\n+        restore_static_fields(ik, dump, CHECK_false);\n+      }\n+      return true; \/\/ Skip pre-initialized since they may already have a new state\n+    }\n+\n+    precond(class_infos.contains(class_id));\n+    const UnfilledClassInfo &info = *class_infos.get(class_id);\n+\n+    restore_class_mirror(class_id, CHECK_false);\n+\n+    Handle init_error;\n+    if (info.class_initialization_error_id != HeapDump::NULL_ID) {\n+      init_error = restore_object(info.class_initialization_error_id, CHECK_false);\n+      guarantee(init_error->is_instance(), \"%s's initialization exception \" HDID_FORMAT \" is an array\",\n+                init_error->klass()->external_name(), info.class_initialization_error_id);\n+    }\n+    CracClassStateRestorer::apply_init_state(ik, info.target_state, init_error);\n+\n+    return true;\n+  });\n+  if (HAS_PENDING_EXCEPTION) {\n+    return;\n+  }\n+#ifdef ASSERT\n+  _instance_classes.iterate_all([](HeapDump::ID _, const InstanceKlass *ik) {\n+    assert(!ik->is_being_restored(), \"%s has not been restored\", ik->external_name());\n+    CracClassStateRestorer::assert_hierarchy_init_states_are_consistent(*ik);\n+  });\n+#endif \/\/ ASSERT\n+  guarantee(_prepared_loaders.number_of_entries() == 0, \"some prepared class loaders have not defined any classes\");\n+\n+  \/\/ Restore objects reachable from the thread stacks\n+  for (auto *const trace : stack_traces) {\n+    const Handle thread = restore_object(trace->thread_id(), CHECK);\n+    trace->put_thread(thread);\n+\n+    for (u4 frame_i = 0; frame_i < trace->frames_num(); frame_i++) {\n+      const CracStackTrace::Frame &frame = trace->frame(frame_i);\n+      const u2 num_locals = frame.locals().length();\n+      for (u2 loc_i = 0; loc_i < num_locals; loc_i++) {\n+        CracStackTrace::Frame::Value &value = *frame.locals().adr_at(loc_i);\n+        assert(value.type() == CracStackTrace::Frame::Value::Type::PRIM ||\n+               value.type() == CracStackTrace::Frame::Value::Type::REF, \"must be\");\n+        if (value.type() == CracStackTrace::Frame::Value::Type::REF) {\n+          const Handle obj = restore_object(value.as_obj_id(), CHECK);\n+          value = CracStackTrace::Frame::Value::of_obj(obj);\n+        }\n+      }\n+      const u2 num_operands = frame.operands().length();\n+      for (u2 op_i = 0; op_i < num_operands; op_i++) {\n+        CracStackTrace::Frame::Value &value = *frame.operands().adr_at(op_i);\n+        assert(value.type() == CracStackTrace::Frame::Value::Type::PRIM ||\n+               value.type() == CracStackTrace::Frame::Value::Type::REF, \"must be\");\n+        if (value.type() == CracStackTrace::Frame::Value::Type::REF) {\n+          const Handle obj = restore_object(value.as_obj_id(), CHECK);\n+          value = CracStackTrace::Frame::Value::of_obj(obj);\n+        }\n+      }\n+      const int num_monitors = frame.monitor_owners().length();\n+      for (int mon_i = 0; mon_i < num_monitors; mon_i++) {\n+        CracStackTrace::Frame::Value &monitor_owner = *frame.monitor_owners().adr_at(mon_i);\n+        assert(monitor_owner.type() == CracStackTrace::Frame::Value::Type::REF, \"must be\");\n+        const Handle obj = restore_object(monitor_owner.as_obj_id(), CHECK);\n+        monitor_owner = CracStackTrace::Frame::Value::of_obj(obj);\n+      }\n+    }\n+  }\n+\n+  _well_known_objects.get_from(_objects);\n+  log_info(crac)(\"Finished heap restoration\");\n+}\n+\n+\/\/ Recording of existing objects\n+\n+\/\/ Finds j.l.Class object corresponding to the class dump and records it.\n+void CracHeapRestorer::find_and_record_class_mirror(const HeapDump::ClassDump &class_dump, TRAPS) {\n+  Thread *const current = Thread::current();\n+\n+  const HeapDump::InstanceDump &mirror_dump = _heap_dump.get_instance_dump(class_dump.id);\n+  _mirror_dump_reader.ensure_initialized(_heap_dump, mirror_dump.class_id);\n+  using MirrorType = HeapDumpClasses::java_lang_Class::Kind;\n+  switch (_mirror_dump_reader.kind(mirror_dump)) {\n+    case MirrorType::INSTANCE: {\n+      const InstanceKlass &ik = get_instance_class(class_dump.id);\n+      const instanceHandle mirror(current, static_cast<instanceOop>(ik.java_mirror()));\n+      record_class_mirror(mirror, mirror_dump, CHECK);\n+      break;\n+    }\n+    case MirrorType::ARRAY: {\n+      const ArrayKlass &ak = get_array_class(class_dump.id);\n+      const instanceHandle mirror(current, static_cast<instanceOop>(ak.java_mirror()));\n+      record_class_mirror(mirror, mirror_dump, CHECK);\n+\n+      \/\/ Primitive mirrors are also recorded here because they don't have a\n+      \/\/ Klass to be dumped with directly but always have a TypeArrayKlass\n+      if (ak.is_typeArray_klass() && &ak != Universe::fillerArrayKlassObj() \/* same as int[] *\/) {\n+        const oop prim_mirror_obj = java_lang_Class::component_mirror(mirror());\n+        assert(prim_mirror_obj != nullptr, \"type array's mirror must have a component mirror\");\n+        const instanceHandle prim_mirror(current, static_cast<instanceOop>(prim_mirror_obj));\n+\n+        const HeapDump::ID prim_mirror_dump_id = _mirror_dump_reader.componentType(mirror_dump);\n+        guarantee(prim_mirror_dump_id != HeapDump::NULL_ID, \"primitive array \" HDID_FORMAT \" has no component type\",\n+                  prim_mirror_dump_id);\n+        const HeapDump::InstanceDump &prim_mirror_dump = _heap_dump.get_instance_dump(prim_mirror_dump_id);\n+\n+        record_class_mirror(prim_mirror, prim_mirror_dump, CHECK);\n+      }\n+      break;\n+    }\n+    case MirrorType::PRIMITIVE:\n+      \/\/ Class dumps are only created from InstanceKlasses and ArrayKlasses\n+      guarantee(false, \"instance or array class \" HDID_FORMAT \" has a primitive type mirror\", class_dump.id);\n+  }\n+}\n+\n+void CracHeapRestorer::record_class_mirror(instanceHandle mirror, const HeapDump::InstanceDump &mirror_dump, TRAPS) {\n+  if (log_is_enabled(Trace, crac)) {\n+    Klass *mirrored_class;\n+    const BasicType bt = java_lang_Class::as_BasicType(mirror(), &mirrored_class);\n+    const char *type_name = is_reference_type(bt) ? mirrored_class->external_name() : type2name(bt);\n+    log_trace(crac)(\"Recording class mirror \" HDID_FORMAT \" of %s\", mirror_dump.id, type_name);\n+  }\n+  precond(!_objects.contains(mirror_dump.id) && mirror.not_null());\n+  put_object_when_absent(mirror_dump.id, mirror);\n+\n+  _mirror_dump_reader.ensure_initialized(_heap_dump, mirror_dump.class_id);\n+\n+  const HeapDump::ID module_id = _mirror_dump_reader.module(mirror_dump);\n+  const auto module_obj = static_cast<instanceOop>(java_lang_Class::module(mirror()));\n+  assert(module_obj != nullptr, \"module must be set\");\n+  put_object_if_absent(module_id, instanceHandle(Thread::current(), module_obj)); \/\/ Can be pre-recorded via another class from this module\n+\n+  \/\/ Name can be initialized concurrently, so if it was dumped, initialize and\n+  \/\/ record it eagerly\n+  const HeapDump::ID name_id = _mirror_dump_reader.name(mirror_dump);\n+  if (name_id != HeapDump::NULL_ID) {\n+    const oop name_oop = java_lang_Class::name(mirror, CHECK);\n+    const auto name_obj = static_cast<instanceOop>(name_oop);\n+    put_object_if_absent(name_id, instanceHandle(Thread::current(), name_obj)); \/\/ Checks it's either absent or set to the same oop\n+  }\n+\n+#ifdef ASSERT\n+  \/\/ TODO would be more accurate to check the classLoader field of the mirror dump itself\n+  if (!java_lang_Class::is_primitive(mirror())) {\n+    const HeapDump::ID loader_id = _heap_dump.get_class_dump(mirror_dump.id).class_loader_id;\n+    assert(loader_id == HeapDump::NULL_ID || get_object_when_present(loader_id) == java_lang_Class::class_loader(mirror()),\n+           \"class loader must already be recorded\");\n+  }\n+\n+  const HeapDump::ID component_mirror_id = _mirror_dump_reader.componentType(mirror_dump);\n+  const oop expected_component_mirror = java_lang_Class::component_mirror(mirror());\n+  assert((component_mirror_id == HeapDump::NULL_ID) == (expected_component_mirror == nullptr),\n+         \"component mirror must be dumped iff it exists in the runtime\");\n+  if (component_mirror_id != HeapDump::NULL_ID) {\n+    assert(_heap_dump.instance_dumps.contains(component_mirror_id), \"unknown component mirror \" HDID_FORMAT, component_mirror_id);\n+    const Handle component_mirror = get_object_if_present(component_mirror_id); \/\/ May not be recorded yet\n+    if (component_mirror.not_null()) {\n+      assert(component_mirror == expected_component_mirror, \"unexpected component mirror recorded as \" HDID_FORMAT, component_mirror_id);\n+    } else {\n+      assert(java_lang_Class::is_primitive(expected_component_mirror) || _heap_dump.class_dumps.contains(component_mirror_id),\n+             \"non-primitive component mirror \" HDID_FORMAT \" corresponds to no class\", component_mirror_id);\n+    }\n+  }\n+#endif\n+\n+  \/\/ TODO for the pre-created mirrors, should we fill the rest of the mirror\n+  \/\/  instance fields + class static fields?\n+  \/\/  - If we do, it's not straight forward because the fields may have\n+  \/\/    different values of different classes than they were when dumped\n+  \/\/  - If we don't and these values were references somewhere in the dump,\n+  \/\/    they will be restored and thus duplicated\n+}\n+\n+\/\/ Assuming the current thread is the main (aka primordial) thread, records its\n+\/\/ j.l.Thread object and some of its fields.\n+\/\/\n+\/\/ TODO\n+\/\/  1. If the main thread had finished its execution upon checkpoint we won't\n+\/\/     find and pre-record it and thus things like the main thread group will be\n+\/\/     duplicated. This won't be an issue if we'll be able to start the\n+\/\/     restoration before all these get re-created.\n+\/\/  2. It is possible that the main thread had finished its execution but was\n+\/\/     retained as an object -- this case is currently not fully supported and\n+\/\/     will result in an \"unimplemented\" error when restoring that object.\n+void CracHeapRestorer::record_main_thread(const GrowableArrayView<CracStackTrace *> &stack_traces) {\n+  if (stack_traces.is_empty()) {\n+    return; \/\/ No threads dumped\n+  }\n+\n+  JavaThread *const main_thread = JavaThread::current();\n+  const instanceHandle main_thread_obj(main_thread, static_cast<instanceOop>(main_thread->threadObj()));\n+  guarantee(java_lang_Thread::thread_id(JavaThread::current()->threadObj()) == 1,\n+            \"heap restoration must be called from the main thread\");\n+\n+  \/\/ Threads are dumped from oldest to newest, so if the main thread was alive\n+  \/\/ at the dump creation time, it is the first one\n+  const HeapDump::ID thread_id = stack_traces.first()->thread_id();\n+  const HeapDump::InstanceDump &thread_dump = _heap_dump.get_instance_dump(thread_id);\n+\n+  HeapDumpClasses::java_lang_Thread thread_reader;\n+  thread_reader.ensure_initialized(_heap_dump, thread_dump.class_id);\n+  if (thread_reader.tid(thread_dump) != 1) {\n+    return; \/\/ Main thread was not dumped\n+  }\n+\n+  put_object_when_absent(thread_id, main_thread_obj);\n+\n+  \/\/ TODO only some fields are recorded here, may need to add more\n+#define FLD_HANDLE(f) const instanceHandle f(main_thread, static_cast<instanceOop>(java_lang_Thread::f(main_thread_obj())))\n+  FLD_HANDLE(name);\n+  FLD_HANDLE(holder);\n+  FLD_HANDLE(inherited_access_control_context);\n+#undef FLD_HANDLE\n+  put_object_if_absent(thread_reader.name(thread_dump), name); \/\/ May be present, e.g. as a name of a prepared class loader\n+  put_object_when_absent(thread_reader.holder(thread_dump), holder);\n+  put_object_when_absent(thread_reader.inheritedAccessControlContext(thread_dump), inherited_access_control_context);\n+}\n+\n+\/\/ Field restoration\n+\/\/ TODO use other means to iterate over fields: FieldStream performs a linear\n+\/\/  search for each field\n+\n+#ifdef ASSERT\n+\n+static bool is_same_basic_type(const Symbol *signature, BasicType dump_t, bool allow_intptr_t = false) {\n+  const BasicType sig_t = Signature::basic_type(signature);\n+  return sig_t == dump_t ||\n+         \/\/ Heap dump uses T_OBJECT for arrays\n+         (sig_t == T_ARRAY && dump_t == T_OBJECT) ||\n+         \/\/ Java equivalent of intptr_t is platform-dependent\n+         (allow_intptr_t && signature == vmSymbols::intptr_signature() && (dump_t == T_INT || dump_t == T_LONG));\n+}\n+\n+static Klass *get_ref_field_type(const InstanceKlass &holder, Symbol *signature) {\n+  Thread *const thread = Thread::current();\n+  const Handle holder_loader = Handle(thread, holder.class_loader());\n+  if (Signature::has_envelope(signature)) {\n+    const TempNewSymbol class_name = Signature::strip_envelope(signature);\n+    return SystemDictionary::find_constrained_instance_or_array_klass(thread, class_name, holder_loader);\n+  }\n+  return SystemDictionary::find_constrained_instance_or_array_klass(thread, signature, holder_loader);\n+}\n+\n+#endif \/\/ ASSERT\n+\n+void CracHeapRestorer::set_field(instanceHandle obj, const FieldStream &fs, const HeapDump::BasicValue &val, TRAPS) {\n+  precond(obj.not_null());\n+  DEBUG_ONLY(const InstanceKlass &field_holder = *fs.field_descriptor().field_holder());\n+  assert(!fs.access_flags().is_static() || field_holder.init_state() < InstanceKlass::fully_initialized,\n+         \"trying to modify static field %s of pre-initialized class %s\", fs.name()->as_C_string(), field_holder.external_name());\n+  \/\/ Static fields of pre-defined classes already have their initial values set\n+  \/\/ but we can overwrite them until the class is marked initialized\n+  DEBUG_ONLY(const bool prefilled = fs.access_flags().is_static() && fs.field_descriptor().has_initial_value());\n+  switch (Signature::basic_type(fs.signature())) {\n+    case T_OBJECT:\n+    case T_ARRAY: {\n+      precond(prefilled || obj->obj_field(fs.offset()) == nullptr);\n+      const Handle restored = restore_object(val.as_object_id, CHECK);\n+#ifdef ASSERT\n+      if (restored.not_null()) {\n+        Klass *const field_type = get_ref_field_type(field_holder, fs.signature());\n+        \/\/ TODO until restoration of loader constraints is implemented we may get null here\n+        \/\/ assert(field_type != nullptr, \"field's type must be loaded since the field is assigned\");\n+        if (field_type != nullptr) {\n+          assert(restored->klass()->is_subtype_of(field_type), \"field of type %s cannot be assigned a value of class %s\",\n+                 fs.signature()->as_C_string(), restored->klass()->external_name());\n+        } else {\n+          log_warning(crac, class)(\"Loader constraint absent: %s should be constrained on loading %s\",\n+                                   field_holder.class_loader_data()->loader_name_and_id(), fs.signature()->as_C_string());\n+        }\n+      }\n+#endif \/\/ ASSERT\n+      obj->obj_field_put(fs.offset(), restored());\n+      break;\n+    }\n+    case T_BOOLEAN: precond(prefilled || obj->bool_field(fs.offset()) == false); obj->bool_field_put(fs.offset(), val.as_boolean);  break;\n+    case T_CHAR:    precond(prefilled || obj->char_field(fs.offset()) == 0);     obj->char_field_put(fs.offset(), val.as_char);     break;\n+    case T_FLOAT:   precond(prefilled || obj->float_field(fs.offset()) == 0.0F); obj->float_field_put(fs.offset(), val.as_float);   break;\n+    case T_DOUBLE:  precond(prefilled || obj->double_field(fs.offset()) == 0.0); obj->double_field_put(fs.offset(), val.as_double); break;\n+    case T_BYTE:    precond(prefilled || obj->byte_field(fs.offset()) == 0);     obj->byte_field_put(fs.offset(), val.as_byte);     break;\n+    case T_SHORT:   precond(prefilled || obj->short_field(fs.offset()) == 0);    obj->short_field_put(fs.offset(), val.as_short);   break;\n+    case T_INT:     precond(prefilled || obj->int_field(fs.offset()) == 0);      obj->int_field_put(fs.offset(), val.as_int);       break;\n+    case T_LONG:    precond(prefilled || obj->long_field(fs.offset()) == 0);     obj->long_field_put(fs.offset(), val.as_long);     break;\n+    default:        ShouldNotReachHere();\n+  }\n+}\n+\n+template<class OBJ_DUMP_T>\n+static void restore_identity_hash(oop obj, const OBJ_DUMP_T &dump) {\n+  const auto hash = bit_cast<jint>(dump.stack_trace_serial); \/\/ We use HPROF's stack_trace_serial to store identity hash\n+  guarantee((hash & markWord::hash_mask) == checked_cast<decltype(markWord::hash_mask)>(hash), \"identity hash too big: %i\", hash);\n+  if (hash == markWord::no_hash) {\n+    return; \/\/ No hash computed at dump time, nothing to restore\n+  }\n+\n+  log_trace(crac)(\"Restoring \" HDID_FORMAT \": identity hash\", dump.id);\n+  const intptr_t installed_hash = obj->identity_hash(hash);\n+  if (installed_hash != hash) {\n+#ifdef ASSERT\n+    if (obj->klass()->is_instance_klass()) {\n+      const InstanceKlass *ik = InstanceKlass::cast(obj->klass());\n+      assert(!ik->is_being_restored() && ik->is_initialized(), \"can only happen to pre-initialized classes\");\n+    } else if (obj->klass()->is_objArray_klass()) {\n+      const Klass *bk = ObjArrayKlass::cast(obj->klass())->bottom_klass();\n+      const InstanceKlass *ik = InstanceKlass::cast(bk);\n+      assert(!ik->is_being_restored() && ik->is_initialized(), \"can only happen to pre-initialized classes\");\n+    } else {\n+      assert(obj->klass()->is_typeArray_klass(), \"must be\"); \/\/ No InstanceKlass to check\n+    }\n+#endif \/\/ ASSERT\n+    if (log_is_enabled(Info, crac)) {\n+      ResourceMark rm;\n+      log_info(crac)(\"Pre-created object \" INTPTR_FORMAT \" (%s) differs in identity hash: saved with %i, now got \" INTX_FORMAT,\n+                     cast_from_oop<intptr_t>(obj), obj->klass()->external_name(), hash, installed_hash);\n+    }\n+  }\n+}\n+\n+bool CracHeapRestorer::set_class_loader_instance_field_if_special(instanceHandle obj, const HeapDump::InstanceDump &dump,\n+                                                                  const FieldStream &obj_fs, const DumpedInstanceFieldStream &dump_fs, TRAPS) {\n+  precond(obj->klass()->is_subclass_of(vmClasses::ClassLoader_klass()));\n+  precond(!obj_fs.access_flags().is_static());\n+  if (obj_fs.field_descriptor().field_holder() != vmClasses::ClassLoader_klass()) {\n+    return false;\n+  }\n+\n+  const Symbol *field_name = obj_fs.name();\n+\n+  \/\/ Skip the CLD pointer which is set when registering the loader\n+  if (field_name == vmSymbols::loader_data_name()) {\n+    return true;\n+  }\n+\n+  \/\/ Restoration is only called for prepared or just allocated class loaders\n+  \/\/ Note: don't check _prepared_loaders here because we pop the loader from\n+  \/\/ there before restoring it, and also this should be more efficient\n+  const bool is_prepared = java_lang_ClassLoader::unnamedModule(obj()) != nullptr;\n+  if (!is_prepared) {\n+    \/\/ The loader has just been allocated by us and has not been prepared, so\n+    \/\/ can restore it as a general object\n+    return false;\n+  }\n+\n+  \/\/ Skip the fields already restored during the preparation\n+  if (field_name == vmSymbols::parent_name() || field_name == vmSymbols::name_name() ||\n+      field_name->equals(\"nameAndId\") || field_name->equals(\"unnamedModule\")) {\n+    precond(dump_fs.type() == T_OBJECT);\n+    const HeapDump::ID obj_id = dump_fs.value().as_object_id;\n+    assert(obj_id == HeapDump::NULL_ID && obj->obj_field(obj_fs.offset()) == nullptr ||\n+           get_object_when_present(obj_id) == obj->obj_field(obj_fs.offset()),\n+           \"either null or recorded with same value\");\n+    return true;\n+  }\n+\n+  \/\/ When preparing, parallelLockMap is only allocated and left unrestored, so\n+  \/\/ restore it now\n+  if (field_name->equals(\"parallelLockMap\")) {\n+    precond(dump_fs.type() == T_OBJECT);\n+    const HeapDump::ID parallel_lock_map_id = dump_fs.value().as_object_id;\n+    const oop parallel_lock_map = obj->obj_field(obj_fs.offset());\n+    if (parallel_lock_map != nullptr) {\n+      assert(parallel_lock_map->klass() == vmClasses::ConcurrentHashMap_klass(), \"must be\");\n+      assert(get_object_when_present(parallel_lock_map_id) == parallel_lock_map, \"must be recorded when preparing\");\n+      const HeapDump::InstanceDump &parallel_lock_map_dump = _heap_dump.get_instance_dump(parallel_lock_map_id);\n+      restore_identity_hash(parallel_lock_map, parallel_lock_map_dump);\n+      restore_instance_fields(obj, parallel_lock_map_dump, CHECK_false);\n+    } else {\n+      assert(parallel_lock_map_id == HeapDump::NULL_ID, \"must be\");\n+    }\n+    return true;\n+  }\n+\n+  \/\/ The rest of the fields are untouched by the preparation and should be\n+  \/\/ restored as usual\n+  return false;\n+}\n+\n+bool CracHeapRestorer::set_class_mirror_instance_field_if_special(instanceHandle obj, const HeapDump::InstanceDump &dump,\n+                                                                  const FieldStream &obj_fs, const DumpedInstanceFieldStream &dump_fs, TRAPS) {\n+  assert(obj_fs.field_descriptor().field_holder() == vmClasses::Class_klass(), \"must be\");\n+  precond(!obj_fs.access_flags().is_static());\n+  const Symbol *field_name = obj_fs.name();\n+\n+  \/\/ Skip primitive fields set when creating the mirror\n+  if (field_name == vmSymbols::klass_name() || field_name == vmSymbols::array_klass_name() ||\n+      field_name == vmSymbols::oop_size_name() || field_name == vmSymbols::static_oop_field_count_name()) {\n+    return true;\n+  }\n+  \/\/ Component class mirror (aka component type) is also set when creating the\n+  \/\/ mirror iff it corresponds to an array class, and it must be already\n+  \/\/ recorded because we pre-record all mirors\n+  if (field_name == vmSymbols::componentType_name()) {\n+#ifdef ASSERT\n+    precond(dump_fs.type() == T_OBJECT);\n+    const HeapDump::ID component_mirror_id = dump_fs.value().as_object_id;\n+    if (component_mirror_id != HeapDump::NULL_ID) {\n+      assert(java_lang_Class::as_Klass(obj())->is_array_klass(),\n+              \"a %s<%s> object has 'componentType' dumped referencing \" HDID_FORMAT \" when it represents a non-array class\",\n+              vmSymbols::java_lang_Class()->as_klass_external_name(), java_lang_Class::as_Klass(obj())->external_name(), component_mirror_id);\n+      const oop component_mirror = obj->obj_field(obj_fs.offset());\n+      assert(component_mirror != nullptr, \"array class mirror must have its component mirror set\");\n+      assert(get_object_when_present(component_mirror_id) == component_mirror, \"component class mirror must be pre-recorded\");\n+    } else {\n+      assert(java_lang_Class::as_Klass(obj())->is_instance_klass(),\n+              \"a %s<%s> object has 'componentType' dumped as null when it represents an array class\",\n+              vmSymbols::java_lang_Class()->as_klass_external_name(), java_lang_Class::as_Klass(obj())->external_name());\n+      assert(obj->obj_field(obj_fs.offset()) == nullptr, \"instance class mirror cannot have its component mirror set\");\n+    }\n+#endif \/\/ ASSERT\n+    return true;\n+  }\n+  \/\/ Module is also set when creating the mirror and is pre-recorded\n+  if (field_name->equals(\"module\")) {\n+#ifdef ASSERT\n+    precond(dump_fs.type() == T_OBJECT);\n+    const HeapDump::ID module_id = dump_fs.value().as_object_id;\n+    const oop module = obj->obj_field(obj_fs.offset());\n+    assert(module_id != HeapDump::NULL_ID && module != nullptr, \"mirror's module is always not null\");\n+    assert(get_object_when_present(module_id) == module, \"mirror's module must be pre-recorded\");\n+#endif \/\/ ASSERT\n+    return true;\n+  }\n+\n+  \/\/ Name can be set concurrently and thus is pre-recorded if it existed at dump time\n+  if (field_name == vmSymbols::name_name()) {\n+#ifdef ASSERT\n+    precond(dump_fs.type() == T_OBJECT);\n+    const HeapDump::ID name_id = dump_fs.value().as_object_id;\n+    if (name_id != HeapDump::NULL_ID) {\n+      const oop name = obj->obj_field(obj_fs.offset());\n+      assert(name != nullptr, \"non-null-dumped mirror's name must be pre-initialized\");\n+      assert(get_object_when_present(name_id) == name, \"non-null-dumped mirror's name must be pre-recorded\");\n+    }\n+#endif \/\/ ASSERT\n+    return true;\n+  }\n+\n+  \/\/ If the defining loader is a prepared one we should restore the fields\n+  \/\/ unfilled by its preparation, and unmark the loader as prepared so that\n+  \/\/ this won't be repeated when restoring other classes defined by the loader\n+  if (field_name == vmSymbols::classLoader_name()) {\n+    precond(dump_fs.type() == T_OBJECT);\n+    const HeapDump::ID loader_id = dump_fs.value().as_object_id;\n+    assert(loader_id == HeapDump::NULL_ID || _objects.contains(loader_id), \"used loaders must already be recorded\");\n+    if (loader_id != HeapDump::NULL_ID && _prepared_loaders.remove(loader_id)) { \/\/ If the loader is prepared\n+      const oop loader = obj->obj_field(obj_fs.offset());\n+      precond(java_lang_ClassLoader::is_instance(loader));\n+      const instanceHandle loader_h(Thread::current(), static_cast<instanceOop>(loader));\n+      \/\/ We use this fact to distinguish prepared loaders from the unprepared\n+      \/\/ ones when restoring them\n+      assert(java_lang_ClassLoader::unnamedModule(loader) != nullptr, \"preparation must set the unnamed module\");\n+      const HeapDump::InstanceDump &loader_dump = _heap_dump.get_instance_dump(loader_id);\n+      restore_identity_hash(loader_h(), loader_dump);\n+      restore_instance_fields(loader_h, loader_dump, CHECK_false);\n+    }\n+    return true;\n+  }\n+\n+  \/\/ Incremented by the VM when the mirrored class is redefined, and it might\n+  \/\/ have been, so keep the new value\n+  if (field_name == vmSymbols::classRedefinedCount_name()) {\n+    assert(dump_fs.type() == T_INT, \"must be\");\n+    \/\/ TODO JVM TI's RedefineClasses support will require this to be revised\n+    guarantee(dump_fs.value().as_int == 0, \"redefined classes are not dumped\");\n+    return true;\n+  }\n+\n+  \/\/ Mirrors of pre-defined classes may have some fields already set\n+  \/\/ TODO ...and also the mirrors may be accessed concurrently -- this may break\n+  \/\/  something. We can get rid of this if we figure out how to pre-record\n+  \/\/  all pre-existing objects and block other threads from creating new ones\n+  \/\/  until the restoration completes.\n+  assert(is_reference_type(Signature::basic_type(obj_fs.signature())), \"all primitives are handled above\");\n+  const oop preexisting = obj->obj_field(obj_fs.offset());\n+  if (preexisting != nullptr) {\n+    precond(dump_fs.type() == T_OBJECT);\n+    Handle preexisting_h;\n+    if (preexisting->is_instance()) {            preexisting_h = instanceHandle(Thread::current(),  static_cast<instanceOop>(preexisting)); }\n+    else if (preexisting->is_objArray()) {       preexisting_h = objArrayHandle(Thread::current(),  static_cast<objArrayOop>(preexisting)); }\n+    else { precond(preexisting->is_typeArray()); preexisting_h = typeArrayHandle(Thread::current(), static_cast<typeArrayOop>(preexisting)); }\n+    put_object_if_absent(dump_fs.value().as_object_id, preexisting_h); \/\/ Also ensures there is no overwriting\n+    return true;\n+  }\n+\n+  return false;\n+}\n+\n+bool CracHeapRestorer::set_thread_instance_field_if_special(instanceHandle obj, const HeapDump::InstanceDump &dump,\n+                                                            const FieldStream &obj_fs, const DumpedInstanceFieldStream &dump_fs, TRAPS) {\n+  precond(obj->klass()->is_subclass_of(vmClasses::Thread_klass()));\n+  precond(!obj_fs.access_flags().is_static());\n+  if (obj_fs.field_descriptor().field_holder() != vmClasses::Thread_klass()) {\n+    return false;\n+  }\n+\n+  \/\/ Raw pointer to a JvmtiThreadState\n+  if (obj_fs.name() == vmSymbols::jvmti_thread_state_name()) {\n+    guarantee(dump_fs.type() == T_INT || dump_fs.type() == T_LONG, \"must be a Java equivalent of intptr_t\");\n+    const bool had_jvmti_state = (dump_fs.type() == T_INT ? dump_fs.value().as_int : dump_fs.value().as_long) != 0;\n+    if (had_jvmti_state) {\n+      log_error(crac)(\"Cannot restore thread object \" HDID_FORMAT \": it had a JVM TI state\", dump.id);\n+      Unimplemented(); \/\/ TODO JVM TI support\n+    }\n+    assert(obj->address_field(obj_fs.offset()) == nullptr, \"must be cleared when allocated\");\n+    return true;\n+  }\n+\n+  \/\/ Raw pointer to a JavaThread, it will be filled later when restoring the\n+  \/\/ thread's execution\n+  \/\/ TODO would be nice to assert it will actually be restored (i.e. the thread\n+  \/\/  is in the stack dump)\n+  if (obj_fs.name()->equals(\"eetop\")) {\n+    assert(obj->long_field(obj_fs.offset()) == 0, \"must be cleared when allocated\");\n+    return true;\n+  }\n+\n+  \/\/ Thread ID which must be unique across the VM\n+  if (obj_fs.name()->equals(\"tid\")) {\n+    assert(dump_fs.type() == T_LONG, \"must be\");\n+    const jlong tid = dump_fs.value().as_long;\n+    if (tid == 1) {\n+      \/\/ This is the main thread which hasn't been pre-recorded meaning it\n+      \/\/ wasn't alive at dump time\n+      log_error(crac)(\"Restoration of non-alive main thread is unsupported\");\n+      Unimplemented();\n+    }\n+    \/\/ TODO this only ensures the ID won't be claimed later, need to also check\n+    \/\/  it hasn't already been claimed; also, these increments are inefficient\n+    while (ThreadIdentifier::next() < tid) {}\n+    obj->long_field_put(obj_fs.offset(), tid);\n+    return true;\n+  }\n+\n+  return false;\n+}\n+\n+bool CracHeapRestorer::set_string_instance_field_if_special(instanceHandle obj, const HeapDump::InstanceDump &dump,\n+                                                            const FieldStream &obj_fs, const DumpedInstanceFieldStream &dump_fs, TRAPS) {\n+  assert(obj_fs.field_descriptor().field_holder() == vmClasses::String_klass(), \"must be\");\n+  precond(!obj_fs.access_flags().is_static());\n+\n+  \/\/ Flags are internal and depend on VM options. They will be set as needed,\n+  \/\/ so just ignore them.\n+  if (obj_fs.name() == vmSymbols::flags_name()) {\n+    return true;\n+  }\n+\n+  \/\/ Interning is handled separately\n+  assert(obj_fs.name() != vmSymbols::is_interned_name(), \"not a real field\");\n+\n+  return false;\n+}\n+\n+bool CracHeapRestorer::set_member_name_instance_field_if_special(instanceHandle obj, const HeapDump::InstanceDump &dump,\n+                                                                 const FieldStream &obj_fs, const DumpedInstanceFieldStream &dump_fs, TRAPS) {\n+  assert(obj_fs.field_descriptor().field_holder() == vmClasses::MemberName_klass(), \"must be\");\n+  precond(!obj_fs.access_flags().is_static());\n+\n+  \/\/ VM-internal intptr_t field\n+  if (obj_fs.name() == vmSymbols::vmindex_name()) {\n+    const BasicType basic_type = dump_fs.type();\n+    guarantee(basic_type == T_INT || basic_type == T_LONG, \"must be a Java equivalent of intptr_t\");\n+\n+    const HeapDump::BasicValue val = dump_fs.value();\n+    const auto vmindex = checked_cast<intptr_t>(basic_type == T_INT ? val.as_int : val.as_long);\n+\n+    _member_name_dump_reader.ensure_initialized(_heap_dump, dump.class_id);\n+    if (_member_name_dump_reader.method(dump) != HeapDump::NULL_ID) {\n+      \/\/ vmindex is set to a vtable\/itable index which is portable\n+      java_lang_invoke_MemberName::set_vmindex(obj(), vmindex);\n+    } else if (_member_name_dump_reader.is_field(dump) && _member_name_dump_reader.is_resolved(dump)) {\n+      \/\/ vmindex is set to a field offset which is not portable\n+      \/\/ TODOs:\n+      \/\/  1. Checking is_resolved is not enough: checkpoint may be created after\n+      \/\/     the resolution happens but before the indicator is set, so we may\n+      \/\/     lose some of the resolved objects.\n+      \/\/  2. Implement restoration when 'name' and\/or 'type' fields are not set.\n+\n+      const HeapDump::ID holder_id = _member_name_dump_reader.clazz(dump);\n+      guarantee(holder_id != HeapDump::NULL_ID, \"holder of resolved field must be set\");\n+      const InstanceKlass &holder = get_instance_class(holder_id);\n+\n+      const HeapDump::ID name_id = _member_name_dump_reader.name(dump);\n+      if (name_id == HeapDump::NULL_ID) {\n+        log_error(crac)(\"Restoration of resolved field-referensing %s with 'name' not set is not implemented\",\n+                        vmSymbols::java_lang_invoke_MemberName()->as_klass_external_name());\n+        Unimplemented();\n+      }\n+      const Handle name_str = restore_object(name_id, CHECK_false);\n+      const TempNewSymbol name = java_lang_String::as_symbol(name_str());\n+\n+      const HeapDump::ID type_id = _member_name_dump_reader.type(dump);\n+      if (type_id == HeapDump::NULL_ID) {\n+        log_error(crac)(\"Restoration of resolved field-referensing %s with 'type' not set is not implemented\",\n+                        vmSymbols::java_lang_invoke_MemberName()->as_klass_external_name());\n+        Unimplemented();\n+      }\n+      const Handle type_mirror = get_object_when_present(type_id); \/\/ Must be a non-void mirror, so should be pre-recorded\n+      TempNewSymbol signature;\n+      {\n+        Klass *k;\n+        const BasicType bt = java_lang_Class::as_BasicType(type_mirror(), &k);\n+        if (is_java_primitive(bt)) {\n+          signature = vmSymbols::type_signature(bt);\n+          signature->increment_refcount(); \/\/ TempNewSymbol will decrement this\n+        } else {\n+          ResourceMark rm;\n+          signature = SymbolTable::new_symbol(k->signature_name());\n+        }\n+      }\n+\n+      fieldDescriptor fd;\n+      const bool found = holder.find_local_field(name, signature, &fd);\n+      guarantee(found, \"cannot find field %s %s::%s resolved by %s \" HDID_FORMAT,\n+                signature->as_C_string(), holder.external_name(), name->as_C_string(),\n+                vmSymbols::java_lang_invoke_MemberName()->as_klass_external_name(), dump.id);\n+\n+      java_lang_invoke_MemberName::set_vmindex(obj(), fd.offset());\n+    } else {\n+      guarantee(vmindex == 0, \"only set for resolved methods and fields\");\n+    }\n+\n+    return true;\n+  }\n+\n+  return false;\n+}\n+\n+bool CracHeapRestorer::set_call_site_instance_field_if_special(instanceHandle obj, const HeapDump::InstanceDump &dump,\n+                                                               const FieldStream &obj_fs, const DumpedInstanceFieldStream &dump_fs, TRAPS) {\n+  precond(obj.not_null() && java_lang_invoke_CallSite::is_instance(obj()));\n+  precond(!obj_fs.access_flags().is_static());\n+\n+  \/\/ CallSiteContext contains compilation-related data that should be cleared;\n+  \/\/ the context itself has a special deallocation policy and must be registered\n+  if (obj_fs.name() == vmSymbols::context_name()) {\n+    assert(obj_fs.field_descriptor().field_holder() == vmClasses::CallSite_klass(), \"permitted subclasses don't have such field\");\n+\n+    precond(dump_fs.type() == T_OBJECT);\n+    const HeapDump::ID context_id = dump_fs.value().as_object_id;\n+    guarantee(context_id != HeapDump::NULL_ID, \"class site must have a context\");\n+\n+    Handle context = get_object_if_present(context_id);\n+    if (context.is_null()) { \/\/ ID is not null so this means the context has not yet been restored\n+      const HeapDump::InstanceDump &context_dump = _heap_dump.get_instance_dump(context_id);\n+\n+      InstanceKlass &context_class = get_instance_class(context_dump.class_id);\n+      assert(context_class.name() == vmSymbols::java_lang_invoke_MethodHandleNatives_CallSiteContext() &&\n+            context_class.class_loader_data()->is_the_null_class_loader_data(), \"expected boot-loaded %s, got %s loaded by %s\",\n+            vmSymbols::java_lang_invoke_MethodHandleNatives_CallSiteContext()->as_klass_external_name(),\n+            context_class.external_name(), context_class.class_loader_data()->loader_name_and_id());\n+\n+      \/\/ Allocate a new context and register it with this call site\n+      \/\/ If this'll be failing, restore CallSiteContext before the rest of the classes\n+      guarantee(context_class.is_initialized(), \"no need to pre-initialize %s\", context_class.external_name());\n+      JavaValue result(T_OBJECT);\n+      const TempNewSymbol make_name = SymbolTable::new_symbol(\"make\");\n+      const TempNewSymbol make_sig = SymbolTable::new_symbol(\"(Ljava\/lang\/invoke\/CallSite;)Ljava\/lang\/invoke\/MethodHandleNatives$CallSiteContext;\");\n+      JavaCalls::call_static(&result, &context_class, make_name, make_sig, obj, CHECK_false);\n+\n+      context = instanceHandle(Thread::current(), static_cast<instanceOop>(result.get_oop()));\n+      put_object_when_absent(context_id, context); \/\/ Should still be absent\n+    } else {\n+      DEBUG_ONLY(const DependencyContext vmcontext = java_lang_invoke_MethodHandleNatives_CallSiteContext::vmdependencies(context()));\n+      assert(vmcontext.is_unused(), \"must be\");\n+      \/\/ TODO register the context with this call site (CallSiteContext::make()\n+      \/\/  does this for us in the above case)\n+    }\n+\n+    obj->obj_field_put(obj_fs.offset(), context());\n+    return true;\n+  }\n+\n+  return false;\n+}\n+\n+bool CracHeapRestorer::set_call_site_context_instance_field_if_special(instanceHandle obj, const HeapDump::InstanceDump &dump,\n+                                                                       const FieldStream &obj_fs, const DumpedInstanceFieldStream &dump_fs, TRAPS) {\n+  precond(obj.not_null() && java_lang_invoke_MethodHandleNatives_CallSiteContext::is_instance(obj()));\n+  precond(!obj_fs.access_flags().is_static());\n+\n+  \/\/ CallSiteContext contains compilation-related data that should be cleared\n+  assert(obj_fs.field_descriptor().field_flags().is_injected(), \"all %s fields are injected\",\n+         vmSymbols::java_lang_invoke_MethodHandleNatives_CallSiteContext()->as_klass_external_name());\n+  assert(obj->address_field(obj_fs.offset()) == nullptr, \"must be cleared when allocated\");\n+\n+  return true;\n+}\n+\n+void CracHeapRestorer::restore_special_instance_fields(instanceHandle obj, const HeapDump::InstanceDump &dump,\n+                                                       set_instance_field_if_special_ptr_t set_field_if_special, TRAPS) {\n+  precond(obj.not_null());\n+  FieldStream obj_fs(InstanceKlass::cast(obj->klass()), false,  \/\/ Include supers\n+                                                        true,   \/\/ Exclude interfaces: they only have static fields\n+                                                        false); \/\/ Include injected fields\n+  DumpedInstanceFieldStream dump_fs(_heap_dump, dump);\n+  while (!obj_fs.eos() && !dump_fs.eos()) {\n+    if (obj_fs.access_flags().is_static()) {\n+      obj_fs.next();\n+      continue;\n+    }\n+\n+    assert(obj_fs.name() == dump_fs.name(),\n+           \"conflict at field #%i of object \" HDID_FORMAT \" : dumped '%s' is '%s' in the runtime\",\n+           obj_fs.index(), dump.id, dump_fs.name()->as_C_string(), obj_fs.name()->as_C_string());\n+    assert(is_same_basic_type(obj_fs.signature(), dump_fs.type(), true),\n+           \"conflict at field #%i of object \" HDID_FORMAT \": cannot assign dumped '%s' value to a '%s' field\",\n+           obj_fs.index(), dump.id, type2name(dump_fs.type()), obj_fs.signature()->as_C_string());\n+    if (log_is_enabled(Trace, crac)) {\n+      ResourceMark rm;\n+      log_trace(crac)(\"Restoring \" HDID_FORMAT \": potentially-special instance field %s\", dump.id, obj_fs.name()->as_C_string());\n+    }\n+\n+    const HeapDump::BasicValue val = dump_fs.value();\n+    const bool is_special = (this->*set_field_if_special)(obj, dump, obj_fs, dump_fs, CHECK);\n+    if (!is_special) {\n+      set_field(obj, obj_fs, val, CHECK);\n+    }\n+\n+    obj_fs.next();\n+    dump_fs.next();\n+  }\n+\n+#ifdef ASSERT\n+  u4 unfilled_bytes = 0;\n+  for (; !obj_fs.eos(); obj_fs.next()) {\n+    if (!obj_fs.access_flags().is_static()) {\n+      const BasicType type = Signature::basic_type(obj_fs.signature());\n+      unfilled_bytes += HeapDump::value_size(type, _heap_dump.id_size);\n+    }\n+  }\n+  assert(unfilled_bytes == 0,\n+         \"object \" HDID_FORMAT \" has less non-static fields' data dumped than needed by its class %s and its super classes: \"\n+         \"only \" UINT32_FORMAT \" bytes dumped, but additional \" UINT32_FORMAT \" bytes are expected\",\n+         dump.id, obj->klass()->external_name(), dump.fields_data.size(), unfilled_bytes);\n+  if (java_lang_String::is_instance(obj())) {\n+    \/\/ There is a fake is_interned field in j.l.String instance dumps\n+    assert(!dump_fs.eos(), \"%s field missing from %s instance dump \" HDID_FORMAT,\n+           vmSymbols::is_interned_name()->as_C_string(), vmSymbols::java_lang_String()->as_klass_external_name(), dump.id);\n+    assert(dump_fs.name() == vmSymbols::is_interned_name(), \"unexpected field %s in %s instance dump \" HDID_FORMAT,\n+           dump_fs.name()->as_C_string(), vmSymbols::java_lang_String()->as_klass_external_name(), dump.id);\n+    dump_fs.next();\n+  }\n+  assert(dump_fs.eos(),\n+         \"object \" HDID_FORMAT \" has more non-static fields' data dumped than needed by its class %s and its super classes\",\n+         dump.id, obj->klass()->external_name());\n+#endif \/\/ ASSERT\n+}\n+\n+\/\/ This is faster than the special case above as it does not require querying\n+\/\/ dumps of all classes (direct and super) of the instance.\n+void CracHeapRestorer::restore_ordinary_instance_fields(instanceHandle obj, const HeapDump::InstanceDump &dump, TRAPS) {\n+  precond(obj.not_null());\n+  FieldStream fs(InstanceKlass::cast(obj->klass()), false,  \/\/ Include supers\n+                                                    true,   \/\/ Exclude interfaces: they only have static fields\n+                                                    false); \/\/ Include injected fields\n+  u4 dump_offset = 0;\n+  for (; !fs.eos() && dump_offset < dump.fields_data.size(); fs.next()) {\n+    if (fs.access_flags().is_static()) {\n+      continue;\n+    }\n+    if (log_is_enabled(Trace, crac)) {\n+      ResourceMark rm;\n+      log_trace(crac)(\"Restoring \" HDID_FORMAT \": ordinary instance field %s\", dump.id, fs.name()->as_C_string());\n+    }\n+\n+    const BasicType type = Signature::basic_type(fs.signature());\n+    const u4 type_size = HeapDump::value_size(type, _heap_dump.id_size);\n+    guarantee(dump_offset + type_size <= dump.fields_data.size(),\n+              \"object \" HDID_FORMAT \" has less non-static fields' data dumped than needed by its class %s and its super classes: \"\n+              \"read \" UINT32_FORMAT \" bytes and expect at least \" UINT32_FORMAT \" more to read %s value, but only \" UINT32_FORMAT \" bytes left\",\n+              dump.id, obj->klass()->external_name(), dump_offset, type_size, type2name(type), dump.fields_data.size() - dump_offset);\n+    const HeapDump::BasicValue val = dump.read_field(dump_offset, type, _heap_dump.id_size);\n+    set_field(obj, fs, val, CHECK);\n+\n+    dump_offset += type_size;\n+  }\n+\n+#ifdef ASSERT\n+  u4 unfilled_bytes = 0;\n+  for (; !fs.eos(); fs.next()) {\n+    if (!fs.access_flags().is_static()) {\n+      const BasicType type = Signature::basic_type(fs.signature());\n+      unfilled_bytes += HeapDump::value_size(type, _heap_dump.id_size);\n+    }\n+  }\n+  assert(unfilled_bytes == 0,\n+         \"object \" HDID_FORMAT \" has less non-static fields' data dumped than needed by its class %s and its super classes: \"\n+         \"only \" UINT32_FORMAT \" bytes dumped, but additional \" UINT32_FORMAT \" bytes are expected\",\n+         dump.id, obj->klass()->external_name(), dump.fields_data.size(), unfilled_bytes);\n+  assert(dump_offset == dump.fields_data.size(),\n+         \"object \" HDID_FORMAT \" has more non-static fields' data dumped than needed by its class %s and its super classes: \"\n+         UINT32_FORMAT \" bytes dumped, but only \" UINT32_FORMAT \" expected\",\n+         dump.id, obj->klass()->external_name(), dump.fields_data.size(), dump_offset);\n+#endif \/\/ ASSERT\n+}\n+\n+void CracHeapRestorer::restore_instance_fields(instanceHandle obj, const HeapDump::InstanceDump &dump, TRAPS) {\n+  \/\/ ResolvedMethodName is restored in a special manner as a whole\n+  assert(!java_lang_invoke_ResolvedMethodName::is_instance(obj()), \"should not be manually restoring fields of this instance\");\n+\n+  if (obj->klass()->is_class_loader_instance_klass()) {\n+    restore_special_instance_fields(obj, dump, &CracHeapRestorer::set_class_loader_instance_field_if_special, CHECK);\n+  } else if (obj->klass()->is_mirror_instance_klass()) {\n+    restore_special_instance_fields(obj, dump, &CracHeapRestorer::set_class_mirror_instance_field_if_special, CHECK);\n+  } else if (obj->klass() == vmClasses::String_klass()) {\n+    restore_special_instance_fields(obj, dump, &CracHeapRestorer::set_string_instance_field_if_special, CHECK);\n+  } else if (obj->klass() == vmClasses::MemberName_klass()) {\n+    restore_special_instance_fields(obj, dump, &CracHeapRestorer::set_member_name_instance_field_if_special, CHECK);\n+  } else if (obj->klass() == vmClasses::CallSite_klass() || obj->klass()->super() == vmClasses::CallSite_klass()) {\n+    restore_special_instance_fields(obj, dump, &CracHeapRestorer::set_call_site_instance_field_if_special, CHECK);\n+  } else if (obj->klass()->class_loader_data()->is_the_null_class_loader_data() &&\n+             obj->klass()->name() == vmSymbols::java_lang_invoke_MethodHandleNatives_CallSiteContext()) {\n+    restore_special_instance_fields(obj, dump, &CracHeapRestorer::set_call_site_context_instance_field_if_special, CHECK);\n+  } else if (obj->klass()->is_subclass_of(vmClasses::Thread_klass())) {\n+    restore_special_instance_fields(obj, dump, &CracHeapRestorer::set_thread_instance_field_if_special, CHECK);\n+  } else { \/\/ TODO other special cases (need to check all classes from javaClasses)\n+    precond(!java_lang_invoke_CallSite::is_instance(obj()));\n+    restore_ordinary_instance_fields(obj, dump, CHECK);\n+  }\n+}\n+\n+bool CracHeapRestorer::set_static_field_if_special(instanceHandle mirror, const FieldStream &fs, const HeapDump::BasicValue &val, TRAPS) {\n+  precond(fs.access_flags().is_static());\n+\n+  \/\/ Array classes don't have static fields\n+  const InstanceKlass *ik = InstanceKlass::cast(java_lang_Class::as_Klass(mirror()));\n+\n+  \/\/ j.l.r.SoftReference::clock is set by the GC (notably, it is done even\n+  \/\/ before the class is initialized)\n+  if (ik == vmClasses::SoftReference_klass() && fs.name()->equals(\"clock\")) {\n+    return true;\n+  }\n+\n+  \/\/ jdk.crac.Core is the only pre-initialized class we restore and thus\n+  \/\/ overwrite its pre-filled fields which is not expected in the general path\n+  if (is_jdk_crac_Core(*ik)) {\n+    precond(ik->is_initialized());\n+    const Symbol *field_name = fs.name();\n+    const BasicType field_type = Signature::basic_type(fs.signature());\n+    if (field_type == T_OBJECT) {\n+      assert(field_name->equals(\"globalContext\") || field_name->equals(\"checkpointRestoreLock\"), \"must be\");\n+      guarantee(val.as_object_id != HeapDump::NULL_ID, \"global context and C\/R lock must exist\");\n+      const Handle restored = restore_object(val.as_object_id, CHECK_false);\n+      mirror->obj_field_put(fs.offset(), restored());\n+    } else if (field_name->equals(\"checkpointInProgress\")) {\n+      assert(field_type == T_BOOLEAN, \"must be\");\n+      guarantee(val.as_boolean, \"no checkpoint was in progress?!\");\n+      mirror->bool_field_put(fs.offset(), checked_cast<jboolean>(true));\n+    } else {\n+      \/\/ Should be a static final primitive already set to the same value\n+#ifdef ASSERT\n+      switch (field_type) {\n+        case T_BOOLEAN: assert(mirror->bool_field(fs.offset()) == val.as_boolean, \"must be\"); break;\n+        case T_INT:     assert(mirror->int_field(fs.offset()) == val.as_int, \"must be\");      break;\n+        case T_LONG:    assert(mirror->long_field(fs.offset()) == val.as_long, \"must be\");    break;\n+        default: ShouldNotReachHere();\n+      }\n+#endif \/\/ ASSERT\n+    }\n+    return true;\n+  }\n+\n+  \/\/ TODO other special cases (need to check all classes from javaClasses)\n+  return false;\n+}\n+\n+static void set_resolved_references(InstanceKlass *ik, Handle resolved_refs) {\n+  \/\/ If resolved references are dumped, they should not be null\n+  guarantee(ik->is_rewritten(), \"class %s cannot have resolved references because it has not been rewritten\",\n+            ik->external_name());\n+  guarantee(resolved_refs.not_null(), \"rewritten class %s has null resolved references dumped\",\n+            ik->external_name());\n+  guarantee(resolved_refs->klass()->is_objArray_klass() &&\n+            ObjArrayKlass::cast(resolved_refs->klass())->element_klass() == vmClasses::Object_klass(),\n+            \"class %s has resolved references of illegal type\", ik->external_name());\n+\n+  assert(ik->constants()->cache() != nullptr, \"rewritten class must have a CP cache\");\n+  if (ik->constants()->resolved_references() == nullptr) {\n+    ik->constants()->cache()->set_resolved_references(ik->class_loader_data()->add_handle(resolved_refs));\n+    return;\n+  }\n+\n+  for (InstanceKlass *prev_ver = ik->previous_versions(); prev_ver != nullptr; prev_ver = prev_ver->previous_versions()) {\n+    guarantee(prev_ver->is_rewritten(), \"there are more resolved references dumped for %s than expected\",\n+              prev_ver->external_name());\n+    assert(prev_ver->constants()->cache() != nullptr, \"rewritten class must have a CP cache\");\n+    if (prev_ver->constants()->resolved_references() == nullptr) {\n+      prev_ver->constants()->cache()->set_resolved_references(prev_ver->class_loader_data()->add_handle(resolved_refs));\n+      return;\n+    }\n+  }\n+\n+  guarantee(false, \"there are more resolved references dumped for %s than expected\", ik->external_name());\n+  ShouldNotReachHere();\n+}\n+\n+void CracHeapRestorer::restore_static_fields(InstanceKlass *ik, const HeapDump::ClassDump &dump, TRAPS) {\n+  instanceHandle mirror(Thread::current(), static_cast<instanceOop>(ik->java_mirror()));\n+\n+  FieldStream fs(ik, true,  \/\/ Only fields declared in this class\/interface directly\n+                     true,  \/\/ This doesn't metter when the above is true\n+                     true); \/\/ Exclude injected fields: ther are always non-static\n+  u2 static_i = 0;\n+  while (!fs.eos() && static_i < dump.static_fields.size()) {\n+    if (!fs.access_flags().is_static()) {\n+      fs.next();\n+      continue;\n+    }\n+    if (log_is_enabled(Trace, crac)) {\n+      ResourceMark rm;\n+      log_trace(crac)(\"Restoring \" HDID_FORMAT \": static field %s\", dump.id, fs.name()->as_C_string());\n+    }\n+\n+    const HeapDump::ClassDump::Field &field = dump.static_fields[static_i++];\n+    const Symbol *field_name = _heap_dump.get_symbol(field.info.name_id);\n+    guarantee(field_name != vmSymbols::resolved_references_name(),\n+              \"class %s (ID \" HDID_FORMAT \") has resolved references dumped before some of the actual static fields\",\n+              ik->external_name(), dump.id);\n+\n+    assert(fs.name() == field_name && is_same_basic_type(fs.signature(), HeapDump::htype2btype(field.info.type)),\n+           \"expected static field #%i of class %s (ID \" HDID_FORMAT \") to be %s %s but it is %s %s in the dump\",\n+           static_i, ik->external_name(), dump.id,\n+           type2name(Signature::basic_type(fs.signature())), fs.name()->as_C_string(),\n+           type2name(HeapDump::htype2btype(field.info.type)), field_name->as_C_string());\n+    const bool is_special = set_static_field_if_special(mirror, fs, field.value, CHECK);\n+    if (!is_special) {\n+      set_field(mirror, fs, field.value, CHECK);\n+    }\n+\n+    fs.next();\n+  }\n+\n+#ifdef ASSERT\n+  u2 unfilled_fields_num = 0;\n+  for (; !fs.eos(); fs.next()) {\n+    if (fs.access_flags().is_static()) {\n+      unfilled_fields_num++;\n+    }\n+  }\n+  assert(unfilled_fields_num == 0, \"class %s (ID \" HDID_FORMAT \") has not enough static fields dumped: expected %i more\",\n+         ik->external_name(), dump.id, unfilled_fields_num);\n+\n+  { \/\/ HeapDumper includes constant pool's resolved references as static fields\n+    const AutoSaveRestore<u2> save_restore_static_i(static_i);\n+    while (static_i < dump.static_fields.size()) {\n+      const HeapDump::ClassDump::Field &field = dump.static_fields[static_i++];\n+      const Symbol *field_name = _heap_dump.get_symbol(field.info.name_id);\n+      assert(field_name == vmSymbols::resolved_references_name(),\n+             \"class %s (ID \" HDID_FORMAT \") has excess static field dumped: %s\",\n+             ik->external_name(), dump.id, field_name->as_C_string());\n+    }\n+  }\n+#endif \/\/ ASSERT\n+\n+  \/\/ Restore resolved references if they are not pre-created\n+  if (ik->is_linked() \/*pre-linked*\/ || (ik->is_rewritten() && ik->is_shared()) \/*pre-rewritten*\/) {\n+    return;\n+  }\n+  while (static_i < dump.static_fields.size()) {\n+    log_trace(crac)(\"Restoring \" HDID_FORMAT \": resolved references (pseudo static field #%i)\", dump.id, static_i);\n+    const HeapDump::ClassDump::Field &field = dump.static_fields[static_i++];\n+    guarantee(field.info.type == HPROF_NORMAL_OBJECT, \"resolved references dumped as %s: static field #%i of %s (ID \" HDID_FORMAT \")\",\n+              type2name(HeapDump::htype2btype(field.info.type)), (static_i - 1), ik->external_name(), dump.id);\n+    const Handle restored = restore_object(field.value.as_object_id, CHECK);\n+    set_resolved_references(ik, restored);\n+  }\n+}\n+\n+\/\/ Object restoration\n+\n+void CracHeapRestorer::restore_class_mirror(HeapDump::ID id, TRAPS) {\n+  if (log_is_enabled(Trace, crac)) {\n+    ResourceMark rm;\n+    const char *type_name;\n+    const HeapDump::LoadClass *lc = _heap_dump.load_classes.get(id);\n+    if (lc != nullptr) {\n+      type_name = _heap_dump.get_symbol(lc->class_name_id)->as_klass_external_name();\n+    } else {\n+      type_name = \"a primitive type\";\n+    }\n+    log_trace(crac)(\"Restoring mirror \" HDID_FORMAT \" of %s\", id, type_name);\n+  }\n+\n+  \/\/ Instance mirrors must be pre-recorded\n+  instanceHandle mirror;\n+  {\n+    Handle mirror_h = get_object_when_present(id);\n+    assert(mirror_h->is_instance(), \"mirrors are instances\");\n+    mirror = *static_cast<instanceHandle *>(&mirror_h);\n+  }\n+\n+  \/\/ Side-effect: finishes restoration of the class loader if only prepared\n+  const HeapDump::InstanceDump &mirror_dump = _heap_dump.get_instance_dump(id);\n+  restore_identity_hash(mirror(), mirror_dump);\n+  restore_instance_fields(mirror, mirror_dump, CHECK);\n+\n+  Klass *const mirrored_k = java_lang_Class::as_Klass(mirror());\n+  if (mirrored_k != nullptr && mirrored_k->is_instance_klass()) {\n+    const HeapDump::ClassDump &dump = _heap_dump.get_class_dump(id);\n+    \/\/ Side-effect: restores resolved references array of the constant pool\n+    restore_static_fields(InstanceKlass::cast(mirrored_k), dump, CHECK);\n+  }\n+\n+  if (log_is_enabled(Trace, crac)) {\n+    ResourceMark rm;\n+    const char *type_name;\n+    if (mirrored_k != nullptr) {\n+      type_name = mirrored_k->external_name();\n+    } else {\n+      type_name = type2name(java_lang_Class::as_BasicType(mirror()));\n+    }\n+    log_trace(crac)(\"Restored mirror \" HDID_FORMAT \" of %s\", id, type_name);\n+  }\n+}\n+\n+Handle CracHeapRestorer::restore_object(HeapDump::ID id, TRAPS) {\n+  if (id == HeapDump::NULL_ID) {\n+    return {};\n+  }\n+  const Handle *ready = _objects.get(id);\n+  if (ready != nullptr) {\n+    return *ready;\n+  }\n+\n+  const HeapDump::InstanceDump *instance_dump = _heap_dump.instance_dumps.get(id);\n+  if (instance_dump != nullptr) {\n+    assert(!_instance_classes.contains(id) && !_array_classes.contains(id), \"unrecorded class mirror \" HDID_FORMAT, id);\n+    assert(!_heap_dump.obj_array_dumps.contains(id) && !_heap_dump.prim_array_dumps.contains(id),\n+           \"object \" HDID_FORMAT \" duplicated in multiple dump categories: instance and some kind of array\", id);\n+    return restore_instance(*instance_dump, CHECK_NH);\n+  }\n+\n+  const HeapDump::ObjArrayDump  *obj_array_dump  = _heap_dump.obj_array_dumps.get(id);\n+  if (obj_array_dump != nullptr) {\n+    assert(!_heap_dump.prim_array_dumps.contains(id),\n+           \"object \" HDID_FORMAT \" duplicated in multiple dump categories: object and primitive array\", id);\n+    return restore_obj_array(*obj_array_dump, CHECK_NH);\n+  }\n+\n+  const HeapDump::PrimArrayDump *prim_array_dump = _heap_dump.prim_array_dumps.get(id);\n+  guarantee(prim_array_dump != nullptr, \"object \" HDID_FORMAT \" not found in the heap dump\", id);\n+  return restore_prim_array(*prim_array_dump, CHECK_NH);\n+}\n+\n+\/\/ Void mirror is the only class mirror that we don't pre-record.\n+instanceHandle CracHeapRestorer::get_void_mirror(const HeapDump::InstanceDump &dump) {\n+  _mirror_dump_reader.ensure_initialized(_heap_dump, dump.class_id); \/\/ Also checks this is a mirror dump\n+  guarantee(_mirror_dump_reader.mirrors_void(dump), \"unrecorded non-void class mirror \" HDID_FORMAT, dump.id);\n+  const instanceHandle mirror(Thread::current(), static_cast<instanceOop>(Universe::void_mirror()));\n+  return mirror;\n+}\n+\n+\/\/ Strings may be interned.\n+instanceHandle CracHeapRestorer::get_string(const HeapDump::InstanceDump &dump, TRAPS) {\n+  const instanceHandle str = vmClasses::String_klass()->allocate_instance_handle(CHECK_({}));\n+  \/\/ String's fields don't reference it back so it's safe to restore them before recording the string\n+  restore_instance_fields(str, dump, CHECK_({}));\n+\n+  _string_dump_reader.ensure_initialized(_heap_dump, dump.class_id);\n+  if (_string_dump_reader.is_interned(dump)) {\n+    const oop interned = StringTable::intern(str(), CHECK_({}));\n+    return {Thread::current(), static_cast<instanceOop>(interned)};\n+  }\n+\n+  \/\/ Identity hash is to be restored by the caller\n+  return str;\n+}\n+\n+\/\/ ResolvedMethodNames are interned by the VM.\n+instanceHandle CracHeapRestorer::get_resolved_method_name(const HeapDump::InstanceDump &dump, TRAPS) {\n+  _resolved_method_name_dump_reader.ensure_initialized(_heap_dump, dump.class_id);\n+\n+  const HeapDump::ID holder_id = _resolved_method_name_dump_reader.vmholder(dump);\n+  InstanceKlass *const holder = &get_instance_class(holder_id);\n+\n+  const HeapDump::ID name_id = _resolved_method_name_dump_reader.method_name_id(dump);\n+  Symbol *const name = _heap_dump.get_symbol(name_id);\n+\n+  const HeapDump::ID sig_id = _resolved_method_name_dump_reader.method_signature_id(dump);\n+  Symbol *const sig = _heap_dump.get_symbol(sig_id);\n+\n+  const jbyte kind_raw = _resolved_method_name_dump_reader.method_kind(dump);\n+  guarantee(MethodKind::is_method_kind(kind_raw), \"illegal resolved method kind: %i\", kind_raw);\n+  const auto kind = static_cast<MethodKind::Enum>(kind_raw);\n+\n+  Method *const m = CracClassDumpParser::find_method(holder, name, sig, kind, true, CHECK_({}));\n+  const methodHandle resolved_method(Thread::current(), m);\n+\n+  \/\/ If this'll be failing, restore ResolvedMethodName before the rest of the classes\n+  guarantee(vmClasses::ResolvedMethodName_klass()->is_initialized(), \"need to pre-initialize %s\",\n+            vmSymbols::java_lang_invoke_ResolvedMethodName()->as_klass_external_name());\n+  const oop method_name_o = java_lang_invoke_ResolvedMethodName::find_resolved_method(resolved_method, CHECK_({}));\n+\n+  return {Thread::current(), static_cast<instanceOop>(method_name_o)};\n+}\n+\n+\/\/ MethodTypes are interned on the Java side.\n+instanceHandle CracHeapRestorer::get_method_type(const HeapDump::InstanceDump &dump, TRAPS) {\n+  \/\/ TODO this check is actually not enough and we can get a deadlock when\n+  \/\/  calling into Java below if that method has not been called before the\n+  \/\/  restoration began (this really can happen, I've been a witness...)\n+  assert(vmClasses::MethodType_klass()->is_initialized(), \"no need for this if no cache is pre-initialized\");\n+\n+  _method_type_dump_reader.ensure_initialized(_heap_dump, dump.class_id);\n+  const HeapDump::ID rtype_id = _method_type_dump_reader.rtype(dump);\n+  const HeapDump::ID ptypes_id = _method_type_dump_reader.ptypes(dump);\n+\n+  \/\/ These are class mirrors so it's safe to restore them before recording the MethodType\n+  const Handle rtype = restore_object(rtype_id, CHECK_({})); \/\/ Can be a void mirror so must restore\n+  const Handle ptypes = restore_object(ptypes_id, CHECK_({}));\n+\n+  JavaValue res(T_OBJECT);\n+  const TempNewSymbol name = SymbolTable::new_symbol(\"methodType\");\n+  const TempNewSymbol sig = SymbolTable::new_symbol(\"(Ljava\/lang\/Class;[Ljava\/lang\/Class;Z)Ljava\/lang\/invoke\/MethodType;\");\n+  JavaCallArguments args;\n+  args.push_oop(rtype);\n+  args.push_oop(ptypes);\n+  args.push_int(static_cast<jboolean>(true)); \/\/ trusted\n+  JavaCalls::call_static(&res, vmClasses::MethodType_klass(), name, sig, &args, CHECK_({}));\n+\n+  const instanceHandle mt(Thread::current(), static_cast<instanceOop>(res.get_oop()));\n+  guarantee(mt.not_null() && mt->is_instance(), \"must be\");\n+\n+  \/\/ The interned MethodType can have some fields already set, need to synchronize\n+  assert(rtype == java_lang_invoke_MethodType::rtype(mt()), \"there can only be one mirror of a class\");\n+  if (ptypes != java_lang_invoke_MethodType::ptypes(mt())) {\n+    const Handle actual_ptypes(Thread::current(), java_lang_invoke_MethodType::ptypes(mt()));\n+    _objects.put(ptypes_id, actual_ptypes);\n+  }\n+  \/\/ TODO restore\/record the rest of the fields\n+\n+  return mt;\n+}\n+\n+instanceHandle CracHeapRestorer::restore_instance(const HeapDump::InstanceDump &dump, TRAPS) {\n+  assert(!_objects.contains(dump.id), \"use restore_object() instead\");\n+  log_trace(crac)(\"Restoring instance \" HDID_FORMAT, dump.id);\n+\n+  InstanceKlass &ik = get_instance_class(dump.class_id);\n+  guarantee(ik.is_being_restored() || ik.is_initialized(),\n+            \"object \" HDID_FORMAT \" is an instance of pre-defined uninitialized class %s (\" HDID_FORMAT \")\",\n+            dump.id, ik.external_name(), dump.class_id);\n+\n+  instanceHandle obj;\n+  if (ik.is_mirror_instance_klass()) {\n+    \/\/ This must be the void mirror because every other one is pre-recorded\n+    obj = get_void_mirror(dump);\n+    record_class_mirror(obj, dump, CHECK_({}));\n+  } else {\n+    NOT_PRODUCT(ik.check_valid_for_instantiation(true, CHECK_({})));\n+    bool generic_class = false;\n+    if (&ik == vmClasses::String_klass()) {\n+      obj = get_string(dump, CHECK_({}));\n+    } else if (&ik == vmClasses::ResolvedMethodName_klass()) {\n+      obj = get_resolved_method_name(dump, CHECK_({}));\n+    } else if (&ik == vmClasses::MethodType_klass() && ik.is_initialized()) {\n+      obj = get_method_type(dump, CHECK_({}));\n+    } else {\n+      obj = ik.allocate_instance_handle(CHECK_({}));\n+      generic_class = true;\n+    }\n+    put_object_when_absent(dump.id, obj);\n+    restore_identity_hash(obj(), dump);\n+    if (generic_class) { \/\/ Special cases get their fields restored above\n+      restore_instance_fields(obj, dump, CHECK_({}));\n+    }\n+  }\n+\n+  if (log_is_enabled(Trace, crac)) {\n+    ResourceMark rm;\n+    log_trace(crac)(\"Restored instance \" HDID_FORMAT \" of %s\", dump.id, ik.external_name());\n+  }\n+  return obj;\n+}\n+\n+objArrayHandle CracHeapRestorer::restore_obj_array(const HeapDump::ObjArrayDump &dump, TRAPS) {\n+  assert(!_objects.contains(dump.id), \"use restore_object() instead\");\n+  log_trace(crac)(\"Restoring object array \" HDID_FORMAT, dump.id);\n+\n+  ObjArrayKlass *oak;\n+  {\n+    ArrayKlass &ak = get_array_class(dump.array_class_id);\n+    guarantee(ak.is_objArray_klass(), \"object array \" HDID_FORMAT \" has a primitive array class\", dump.id);\n+    oak = ObjArrayKlass::cast(&ak);\n+  }\n+\n+  guarantee(dump.elem_ids.size() <= INT_MAX, \"object array \" HDID_FORMAT \" is too long: \"\n+            UINT32_FORMAT \" > %i\", dump.id, dump.elem_ids.size(), INT_MAX);\n+  const int length = checked_cast<int>(dump.elem_ids.size());\n+\n+  objArrayHandle array;\n+  {\n+    const objArrayOop o = oak->allocate(length, CHECK_({}));\n+    array = objArrayHandle(Thread::current(), o);\n+  }\n+  put_object_when_absent(dump.id, array); \/\/ Record first to be able to find in case of circular references\n+\n+  restore_identity_hash(array(), dump);\n+\n+  for (int i = 0; i < length; i++) {\n+    const Handle elem = restore_object(dump.elem_ids[i], CHECK_({}));\n+    assert(elem == nullptr || elem->klass()->is_subtype_of(oak->element_klass()),\n+           \"object array \" HDID_FORMAT \" is expected to have elements of type %s, \"\n+           \"but its element #%i has class %s which is not a subtype of the element type\",\n+           dump.id, oak->element_klass()->external_name(), i, elem->klass()->external_name());\n+    array->obj_at_put(i, elem());\n+  }\n+\n+  if (log_is_enabled(Trace, crac)) {\n+    ResourceMark rm;\n+    log_trace(crac)(\"Restored object array \" HDID_FORMAT \" of %s\", dump.id, oak->external_name());\n+  }\n+  return array;\n+}\n+\n+typeArrayHandle CracHeapRestorer::restore_prim_array(const HeapDump::PrimArrayDump &dump, TRAPS) {\n+  assert(!_objects.contains(dump.id), \"use restore_object() instead\");\n+  log_trace(crac)(\"Restoring primitive array \" HDID_FORMAT, dump.id);\n+\n+  guarantee(dump.elems_num <= INT_MAX, \"primitive array \" HDID_FORMAT \" is too long: \"\n+            UINT32_FORMAT \" > %i\", dump.id, dump.elems_num, INT_MAX);\n+  const int length = checked_cast<int>(dump.elems_num);\n+  const BasicType elem_type = HeapDump::htype2btype(dump.elem_type);\n+\n+  const typeArrayOop array = oopFactory::new_typeArray_nozero(elem_type, length, CHECK_({}));\n+  restore_identity_hash(array, dump);\n+  precond(static_cast<size_t>(length) * type2aelembytes(elem_type) == dump.elems_data.size());\n+  if (length > 0) {\n+    memcpy(array->base(elem_type), dump.elems_data.mem(), dump.elems_data.size());\n+  }\n+\n+  const typeArrayHandle array_h(Thread::current(), array);\n+  put_object_when_absent(dump.id, array_h);\n+\n+  if (log_is_enabled(Trace, crac)) {\n+    ResourceMark rm;\n+    log_trace(crac)(\"Restored primitive array \" HDID_FORMAT \" of %s\", dump.id, array->klass()->external_name());\n+  }\n+  return array_h;\n+}\n","filename":"src\/hotspot\/share\/runtime\/cracHeapRestorer.cpp","additions":1733,"deletions":0,"binary":false,"changes":1733,"status":"added"},{"patch":"@@ -0,0 +1,148 @@\n+#ifndef SHARE_RUNTIME_CRACHEAPRESTORER_HPP\n+#define SHARE_RUNTIME_CRACHEAPRESTORER_HPP\n+\n+#include \"memory\/allocation.hpp\"\n+#include \"oops\/oopsHierarchy.hpp\"\n+#include \"runtime\/handles.hpp\"\n+#include \"runtime\/reflectionUtils.hpp\"\n+#include \"utilities\/exceptions.hpp\"\n+#include \"utilities\/growableArray.hpp\"\n+#include \"utilities\/heapDumpClasses.hpp\"\n+#include \"utilities\/heapDumpParser.hpp\"\n+\n+\/\/ Matches objects important to the VM with their IDs in the dump.\n+class WellKnownObjects {\n+ public:\n+  explicit WellKnownObjects(const ParsedHeapDump &heap_dump, TRAPS) {\n+    find_well_known_class_loaders(heap_dump, CHECK);\n+    \/\/ TODO other well-known objects (from Universe, security manager etc.)\n+  }\n+\n+  \/\/ Adds the collected well-known objects into the table. Should be called\n+  \/\/ before restoring any objects to avoid re-creating the existing well-known\n+  \/\/ objects.\n+  void put_into(HeapDumpTable<Handle, AnyObj::C_HEAP> *objects) const;\n+\n+  \/\/ Sets the well-known objects that are not yet set in this VM and checks\n+  \/\/ that the ones that are set have the specified values.\n+  void get_from(const HeapDumpTable<Handle, AnyObj::C_HEAP> &objects) const;\n+\n+ private:\n+  HeapDump::ID _platform_loader_id = HeapDump::NULL_ID;       \/\/ Built-in platform loader\n+  HeapDump::ID _builtin_system_loader_id = HeapDump::NULL_ID; \/\/ Built-in system loader\n+  HeapDump::ID _actual_system_loader_id = HeapDump::NULL_ID;  \/\/ Either the built-in system loader or a user-provided one\n+\n+  void find_well_known_class_loaders(const ParsedHeapDump &heap_dump, TRAPS);\n+\n+  void lookup_builtin_class_loaders(const ParsedHeapDump &heap_dump,\n+                                    const HeapDump::LoadClass &jdk_internal_loader_ClassLoaders);\n+\n+  void lookup_actual_system_class_loader(const ParsedHeapDump &heap_dump,\n+                                         const HeapDump::LoadClass &java_lang_ClassLoader);\n+};\n+\n+\/\/ Interface for providing partially restored ClassLoaders for class definition.\n+class ClassLoaderProvider : public StackObj {\n+ public:\n+  \/\/ Returns a ClassLoader object with the requested ID.\n+  \/\/\n+  \/\/ If the object has previously been allocated the same object is returned.\n+  \/\/ Otherwise, the object is allocated.\n+  virtual instanceHandle get_class_loader(HeapDump::ID id, TRAPS) = 0;\n+};\n+\n+struct UnfilledClassInfo;\n+class CracStackTrace;\n+\n+\/\/ Restores heap based on an HPROF dump created by HeapDumper (there are some\n+\/\/ assumptions that are not guaranteed by the general HPROF standard).\n+class CracHeapRestorer : public ClassLoaderProvider {\n+ public:\n+  \/\/ Allocates resources, caller must set a resource mark.\n+  CracHeapRestorer(const ParsedHeapDump &heap_dump,\n+                   const HeapDumpTable<InstanceKlass *, AnyObj::C_HEAP> &instance_classes,\n+                   const HeapDumpTable<ArrayKlass *, AnyObj::C_HEAP> &array_classes,\n+                   TRAPS) :\n+      _heap_dump(heap_dump), _instance_classes(instance_classes), _array_classes(array_classes),\n+      _well_known_objects(heap_dump, THREAD) {\n+    if (HAS_PENDING_EXCEPTION) {\n+      return;\n+    }\n+    _well_known_objects.put_into(&_objects);\n+  };\n+\n+  instanceHandle get_class_loader(HeapDump::ID id, TRAPS) override;\n+\n+  void restore_heap(const HeapDumpTable<UnfilledClassInfo, AnyObj::C_HEAP> &class_infos,\n+                    const GrowableArrayView<CracStackTrace *> &stack_traces, TRAPS);\n+\n+ private:\n+  const ParsedHeapDump &_heap_dump;\n+  const HeapDumpTable<InstanceKlass *, AnyObj::C_HEAP> &_instance_classes;\n+  const HeapDumpTable<ArrayKlass *, AnyObj::C_HEAP> &_array_classes;\n+\n+  const WellKnownObjects _well_known_objects;\n+\n+  \/\/ Not resource-allocated because that would limit resource usage between\n+  \/\/ getting class loaders and restoring the heap\n+  HeapDumpTable<Handle, AnyObj::C_HEAP> _objects{1009, 100000};\n+  HeapDumpTable<bool, AnyObj::C_HEAP> _prepared_loaders{3, 127};\n+\n+  HeapDumpClasses::java_lang_ClassLoader _loader_dump_reader;\n+  HeapDumpClasses::java_lang_Class _mirror_dump_reader;\n+  HeapDumpClasses::java_lang_String _string_dump_reader;\n+  HeapDumpClasses::java_lang_invoke_ResolvedMethodName _resolved_method_name_dump_reader;\n+  HeapDumpClasses::java_lang_invoke_MemberName _member_name_dump_reader;\n+  HeapDumpClasses::java_lang_invoke_MethodType _method_type_dump_reader;\n+\n+  InstanceKlass &get_instance_class(HeapDump::ID id) const;\n+  ArrayKlass &get_array_class(HeapDump::ID id) const;\n+\n+  Handle get_object_when_present(HeapDump::ID id) const; \/\/ Always not null (nulls aren't recorded)\n+  Handle get_object_if_present(HeapDump::ID id) const;   \/\/ Null iff the object has not been recorded yet\n+  void put_object_when_absent(HeapDump::ID id, Handle obj);\n+  void put_object_if_absent(HeapDump::ID id, Handle obj);\n+\n+  \/\/ Partially restores the class loader so it can be used for class definition.\n+  instanceHandle prepare_class_loader(HeapDump::ID id, TRAPS);\n+  instanceHandle get_class_loader_parent(const HeapDump::InstanceDump &loader_dump, TRAPS);\n+  instanceHandle get_class_loader_name(const HeapDump::InstanceDump &loader_dump, bool with_id, TRAPS);\n+  instanceHandle get_class_loader_unnamed_module(const HeapDump::InstanceDump &loader_dump, TRAPS);\n+  instanceHandle get_class_loader_parallel_lock_map(const HeapDump::InstanceDump &loader_dump, TRAPS);\n+\n+  void find_and_record_class_mirror(const HeapDump::ClassDump &class_dump, TRAPS);\n+  void record_class_mirror(instanceHandle mirror, const HeapDump::InstanceDump &mirror_dump, TRAPS);\n+  void record_main_thread(const GrowableArrayView<CracStackTrace *> &stack_traces);\n+\n+  void set_field(instanceHandle obj, const FieldStream &fs, const HeapDump::BasicValue &val, TRAPS);\n+#define set_instance_field_if_special_signature(name) \\\n+  bool name(instanceHandle, const HeapDump::InstanceDump &, const FieldStream &, const DumpedInstanceFieldStream &, TRAPS);\n+  using set_instance_field_if_special_ptr_t = set_instance_field_if_special_signature((CracHeapRestorer::*));\n+  set_instance_field_if_special_signature(set_class_loader_instance_field_if_special);\n+  set_instance_field_if_special_signature(set_class_mirror_instance_field_if_special);\n+  set_instance_field_if_special_signature(set_thread_instance_field_if_special);\n+  set_instance_field_if_special_signature(set_string_instance_field_if_special);\n+  set_instance_field_if_special_signature(set_member_name_instance_field_if_special);\n+  set_instance_field_if_special_signature(set_call_site_instance_field_if_special);\n+  set_instance_field_if_special_signature(set_call_site_context_instance_field_if_special);\n+#undef set_instance_field_if_special_signature\n+  void restore_special_instance_fields(instanceHandle obj, const HeapDump::InstanceDump &dump,\n+                                       set_instance_field_if_special_ptr_t set_field_if_special, TRAPS);\n+  void restore_ordinary_instance_fields(instanceHandle obj, const HeapDump::InstanceDump &dump, TRAPS);\n+  void restore_instance_fields(instanceHandle obj, const HeapDump::InstanceDump &dump, TRAPS);\n+  bool set_static_field_if_special(instanceHandle mirror, const FieldStream &fs, const HeapDump::BasicValue &val, TRAPS);\n+  void restore_static_fields(InstanceKlass *ik, const HeapDump::ClassDump &dump, TRAPS);\n+\n+  instanceHandle get_void_mirror(const HeapDump::InstanceDump &dump);\n+  instanceHandle get_string(const HeapDump::InstanceDump &dump, TRAPS);\n+  instanceHandle get_resolved_method_name(const HeapDump::InstanceDump &dump, TRAPS);\n+  instanceHandle get_method_type(const HeapDump::InstanceDump &dump, TRAPS);\n+\n+  void restore_class_mirror(HeapDump::ID id, TRAPS);\n+  Handle restore_object(HeapDump::ID id, TRAPS);\n+  instanceHandle restore_instance(const HeapDump::InstanceDump &dump, TRAPS);\n+  objArrayHandle restore_obj_array(const HeapDump::ObjArrayDump &dump, TRAPS);\n+  typeArrayHandle restore_prim_array(const HeapDump::PrimArrayDump &dump, TRAPS);\n+};\n+\n+#endif \/\/ SHARE_RUNTIME_CRACHEAPRESTORER_HPP\n","filename":"src\/hotspot\/share\/runtime\/cracHeapRestorer.hpp","additions":148,"deletions":0,"binary":false,"changes":148,"status":"added"},{"patch":"@@ -0,0 +1,367 @@\n+#include \"precompiled.hpp\"\n+#include \"logging\/log.hpp\"\n+#include \"memory\/allocation.hpp\"\n+#include \"oops\/instanceKlass.hpp\"\n+#include \"oops\/method.hpp\"\n+#include \"runtime\/cracClassDumpParser.hpp\"\n+#include \"runtime\/cracStackDumpParser.hpp\"\n+#include \"runtime\/cracStackDumper.hpp\"\n+#include \"runtime\/handles.hpp\"\n+#include \"runtime\/jniHandles.hpp\"\n+#include \"runtime\/jniHandles.inline.hpp\"\n+#include \"runtime\/os.hpp\"\n+#include \"utilities\/basicTypeReader.hpp\"\n+#include \"utilities\/debug.hpp\"\n+#include \"utilities\/globalDefinitions.hpp\"\n+#include \"utilities\/growableArray.hpp\"\n+#include \"utilities\/heapDumpParser.hpp\"\n+#include \"utilities\/methodKind.hpp\"\n+\n+CracStackTrace::Frame::Value::Value(const CracStackTrace::Frame::Value &other) : _type(other.type()) {\n+  switch (type()) {\n+    case Type::EMPTY: break;\n+    case Type::PRIM:  _prim = other.as_primitive(); break;\n+    case Type::REF:   _obj_id = other.as_obj_id();  break;\n+    case Type::OBJ: {\n+      \/\/ The value owns the handle, so must make a new one\n+      log_debug(crac, stacktrace)(\"Copying a resolved stack value\");\n+      const Handle h = Handle(Thread::current(), JNIHandles::resolve(other.as_obj()));\n+      _obj = JNIHandles::make_global(h);\n+      break;\n+    }\n+    default:          ShouldNotReachHere();\n+  }\n+}\n+\n+\/\/ Note: 'other' is a local copy, so we rely on the copy-constructor above\n+CracStackTrace::Frame::Value &CracStackTrace::Frame::Value::operator=(CracStackTrace::Frame::Value other) {\n+  swap(_type, other._type);\n+  switch (type()) {\n+    case Type::EMPTY: break;\n+    case Type::PRIM:  swap(_prim, other._prim); break;\n+    case Type::REF:   swap(_obj_id, other._obj_id); break;\n+    case Type::OBJ:   swap(_obj, other._obj); break;\n+    default:          ShouldNotReachHere();\n+  }\n+  return *this; \/\/ 'other' gets destroyed\n+}\n+\n+Method *CracStackTrace::Frame::resolve_method(const HeapDumpTable<InstanceKlass *, AnyObj::C_HEAP> &classes,\n+                                              const ParsedHeapDump::RecordTable<HeapDump::UTF8> &symbols, TRAPS) {\n+  if (_resolved_method != nullptr) {\n+    return method();\n+  }\n+\n+  InstanceKlass *holder;\n+  {\n+    InstanceKlass **const c = classes.get(method_holder_id());\n+    guarantee(c != nullptr, \"unknown class ID \" SDID_FORMAT, method_holder_id());\n+    holder = InstanceKlass::cast(*c);\n+  }\n+  assert(holder->is_linked(), \"trying to execute method of unlinked class\");\n+\n+  Symbol *name;\n+  {\n+    const HeapDump::UTF8 *r = symbols.get(method_name_id());\n+    guarantee(r != nullptr, \"unknown method name ID \" SDID_FORMAT, method_name_id());\n+    name = r->sym;\n+  }\n+\n+  Symbol *sig;\n+  {\n+    const HeapDump::UTF8 *r = symbols.get(method_sig_id());\n+    guarantee(r != nullptr, \"unknown method signature ID \" SDID_FORMAT, method_sig_id());\n+    sig = r->sym;\n+  }\n+\n+\n+  Method *const method = CracClassDumpParser::find_method(holder, name, sig, method_kind(), true, CHECK_NULL);\n+  guarantee(method != nullptr, \"method %s not found\", Method::external_name(holder, name, sig));\n+  _resolved_method = method;\n+\n+  return method;\n+}\n+\n+\/\/ Header parsing errors\n+constexpr char ERR_INVAL_HEADER_STR[] = \"invalid header string\";\n+constexpr char ERR_INVAL_ID_SIZE[] = \"invalid ID size format\";\n+constexpr char ERR_UNSUPPORTED_ID_SIZE[] = \"unsupported ID size\";\n+\/\/ Stack trace parsing errors\n+constexpr char ERR_INVAL_STACK_PREAMBLE[] = \"invalid stack trace preamble\";\n+constexpr char ERR_INVAL_FRAME[] = \"invalid frame contents\";\n+\n+static constexpr bool is_supported_word_size(u2 size) {\n+  return size == sizeof(u8) || size == sizeof(u4);\n+}\n+\n+class StackTracesParser : public StackObj {\n+ public:\n+  StackTracesParser(FileBasicTypeReader *reader, GrowableArrayCHeap<CracStackTrace *, mtInternal> *out, u2 word_size)\n+      : _reader(reader), _out(out), _word_size(word_size) {\n+    precond(_reader != nullptr && _out != nullptr && is_supported_word_size(_word_size));\n+  }\n+\n+  const char *parse_stacks() {\n+    log_debug(crac, stacktrace, parser)(\"Parsing stack traces\");\n+\n+    while (true) {\n+      TracePreamble preamble;\n+      if (!parse_stack_preamble(&preamble)) {\n+        return ERR_INVAL_STACK_PREAMBLE;\n+      }\n+      if (preamble.finish) {\n+        break;\n+      }\n+      log_debug(crac, stacktrace, parser)(\"Parsing \" UINT32_FORMAT \" frame(s) of thread \" SDID_FORMAT,\n+                                          preamble.frames_num, preamble.thread_id);\n+\n+      auto *const trace = new CracStackTrace(preamble.thread_id, preamble.frames_num);\n+      for (u4 i = 0; i < trace->frames_num(); i++) {\n+        log_trace(crac, stacktrace, parser)(\"Parsing frame \" UINT32_FORMAT \" (youngest first)\", i);\n+        \/\/ Frames are dumped from youngest to oldest but we store them in\n+        \/\/ reverse so that the youngest frame is last (i.e. is actually on top)\n+        if (!parse_frame(&trace->frame(trace->frames_num() - 1 - i))) {\n+          delete trace;\n+          return ERR_INVAL_FRAME;\n+        }\n+      }\n+      _out->append(trace);\n+    }\n+\n+    _out->shrink_to_fit();\n+\n+    return nullptr;\n+  }\n+\n+ private:\n+  FileBasicTypeReader *const _reader;\n+  GrowableArrayCHeap<CracStackTrace *, mtInternal> *const _out;\n+  const u2 _word_size;\n+\n+  struct TracePreamble {\n+    bool finish;\n+    CracStackTrace::ID thread_id;\n+    u4 frames_num;\n+  };\n+\n+  bool parse_stack_preamble(TracePreamble *preamble) {\n+    \/\/ Thread ID\n+    precond(_word_size <= sizeof(CracStackTrace::ID));\n+    u1 buf[sizeof(CracStackTrace::ID)]; \/\/ Using _word_size as a size would cause error C2131 on MSVC\n+    \/\/ Read the first byte separately to detect a possible correct EOF\n+    if (!_reader->read_raw(buf, 1)) {\n+      if (_reader->eos()) {\n+        preamble->finish = true;\n+        return true;\n+      }\n+      log_error(crac, stacktrace, parser)(\"Failed to read thread ID\");\n+      return false;\n+    }\n+    \/\/ Read the rest of the ID\n+    if (!_reader->read_raw(buf + 1, _word_size - 1)) {\n+      log_error(crac, stacktrace, parser)(\"Failed to read thread ID\");\n+      return false;\n+    }\n+    \/\/ Convert to the ID type\n+    switch (_word_size) {\n+      case sizeof(u4): preamble->thread_id = Bytes::get_Java_u4(buf); break;\n+      case sizeof(u8): preamble->thread_id = Bytes::get_Java_u8(buf); break;\n+      default: ShouldNotReachHere();\n+    }\n+\n+    \/\/ Number of frames dumped\n+    if (!_reader->read(&preamble->frames_num)) {\n+      log_error(crac, stacktrace, parser)(\"Failed to read number of frames in stack of thread \" SDID_FORMAT,\n+                                          preamble->thread_id);\n+      return false;\n+    }\n+\n+    preamble->finish = false;\n+    return true;\n+  }\n+\n+  bool parse_method_kind(MethodKind::Enum *kind) {\n+    u1 raw_kind;\n+    if (!_reader->read(&raw_kind)) {\n+      log_error(crac, stacktrace, parser)(\"Failed to read method signature ID\");\n+      return false;\n+    }\n+    if (!MethodKind::is_method_kind(raw_kind)) {\n+      log_error(crac, stacktrace, parser)(\"Unknown method kind: %i\", raw_kind);\n+      return false;\n+    }\n+    *kind = static_cast<MethodKind::Enum>(raw_kind);\n+    return true;\n+  }\n+\n+  bool parse_frame(CracStackTrace::Frame *frame) {\n+    HeapDump::ID method_name_id;\n+    if (!_reader->read_uint(&method_name_id, _word_size)) {\n+      log_error(crac, stacktrace, parser)(\"Failed to read method name ID\");\n+      return false;\n+    }\n+    frame->set_method_name_id(method_name_id);\n+\n+    HeapDump::ID method_sig_id;\n+    if (!_reader->read_uint(&method_sig_id, _word_size)) {\n+      log_error(crac, stacktrace, parser)(\"Failed to read method signature ID\");\n+      return false;\n+    }\n+    frame->set_method_sig_id(method_sig_id);\n+\n+    MethodKind::Enum method_kind;\n+    if (!parse_method_kind(&method_kind)) {\n+      return false;\n+    }\n+    frame->set_method_kind(method_kind);\n+\n+    HeapDump::ID method_holder_id;\n+    if (!_reader->read_uint(&method_holder_id, _word_size)) {\n+      log_error(crac, stacktrace, parser)(\"Failed to read class ID\");\n+      return false;\n+    }\n+    frame->set_method_holder_id(method_holder_id);\n+\n+    u2 bci;\n+    if (!_reader->read(&bci)) {\n+      log_error(crac, stacktrace, parser)(\"Failed to read BCI\");\n+      return false;\n+    }\n+    frame->set_bci(bci);\n+\n+    log_trace(crac, stacktrace, parser)(\"Parsing locals\");\n+    if (!parse_stack_values(&frame->locals())) {\n+      return false;\n+    }\n+\n+    log_trace(crac, stacktrace, parser)(\"Parsing operands\");\n+    if (!parse_stack_values(&frame->operands())) {\n+      return false;\n+    }\n+\n+    log_trace(crac, stacktrace, parser)(\"Parsing monitors\");\n+    if (!parse_monitors(&frame->monitor_owners())) {\n+      return false;\n+    }\n+\n+    return true;\n+  }\n+\n+  bool parse_stack_values(GrowableArrayCHeap<CracStackTrace::Frame::Value, mtInternal> *values) {\n+    u2 values_num;\n+    if (!_reader->read(&values_num)) {\n+      log_error(crac, stacktrace, parser)(\"Failed to read the number of values\");\n+      return false;\n+    }\n+    values->reserve(values_num);\n+    log_trace(crac, stacktrace, parser)(\"Parsing %i value(s)\", values_num);\n+\n+    for (u2 i = 0; i < values_num; i++) {\n+      u1 type;\n+      if (!_reader->read(&type)) {\n+        log_error(crac, stacktrace, parser)(\"Failed to read the type of value #%i\", i);\n+        return false;\n+      }\n+\n+      if (type == DumpedStackValueType::PRIMITIVE) {\n+        u8 prim;\n+        if (!_reader->read_uint(&prim, _word_size)) {\n+          log_error(crac, stacktrace, parser)(\"Failed to read value #%i as a primitive\", i);\n+          return false;\n+        }\n+        values->append(CracStackTrace::Frame::Value::of_primitive(prim));\n+      } else if (type == DumpedStackValueType::REFERENCE) {\n+        CracStackTrace::ID id;\n+        if (!_reader->read_uint(&id, _word_size)) {\n+          log_error(crac, stacktrace, parser)(\"Failed to read value #%i as a reference\", i);\n+          return false;\n+        }\n+        values->append(CracStackTrace::Frame::Value::of_obj_id(id));\n+      } else {\n+        log_error(crac, stacktrace, parser)(\"Unknown type of value #%i: \" UINT8_FORMAT_X_0, i, type);\n+        return false;\n+      }\n+    }\n+\n+    return true;\n+  }\n+\n+  bool parse_monitors(GrowableArrayCHeap<CracStackTrace::Frame::Value, mtInternal> *monitor_owners) {\n+    u4 monitors_num;\n+    if (!_reader->read(&monitors_num)) {\n+      log_error(crac, stacktrace, parser)(\"Failed to read the number of monitors\");\n+      return false;\n+    }\n+    if (monitors_num > INT_MAX) {\n+      log_error(crac, stacktrace, parser)(\"Too many monitors: \" UINT32_FORMAT \" > %i\", monitors_num, INT_MAX);\n+      return false;\n+    }\n+    monitor_owners->reserve(checked_cast<int>(monitors_num));\n+    log_trace(crac, stacktrace, parser)(\"Parsing %i monitor(s)\", checked_cast<int>(monitors_num));\n+\n+    for (int i = 0; i < checked_cast<int>(monitors_num); i++) {\n+      CracStackTrace::ID id;\n+      if (!_reader->read_uint(&id, _word_size)) {\n+        log_error(crac, stacktrace, parser)(\"Failed to read owner ID of monitor #%i\", i);\n+        return false;\n+      }\n+      monitor_owners->append(CracStackTrace::Frame::Value::of_obj_id(id));\n+    }\n+\n+    return true;\n+  }\n+};\n+\n+static const char *parse_header(BasicTypeReader *reader, u2 *word_size) {\n+  constexpr char HEADER_STR[] = \"CRAC STACK DUMP 0.1\";\n+\n+  char header_str[sizeof(HEADER_STR)];\n+  if (!reader->read_raw(header_str, sizeof(header_str))) {\n+    log_error(crac, stacktrace, parser)(\"Failed to read header string\");\n+    return ERR_INVAL_HEADER_STR;\n+  }\n+  header_str[sizeof(header_str) - 1] = '\\0'; \/\/ Ensure nul-terminated\n+  if (strcmp(header_str, HEADER_STR) != 0) {\n+    log_error(crac, stacktrace, parser)(\"Unknown header string: %s\", header_str);\n+    return ERR_INVAL_HEADER_STR;\n+  }\n+\n+  if (!reader->read(word_size)) {\n+    log_error(crac, stacktrace, parser)(\"Failed to read word size\");\n+    return ERR_INVAL_ID_SIZE;\n+  }\n+  if (!is_supported_word_size(*word_size)) {\n+    log_error(crac, stacktrace, parser)(\"Word size %i is not supported: should be 4 or 8\", *word_size);\n+    return ERR_UNSUPPORTED_ID_SIZE;\n+  }\n+\n+  return nullptr;\n+}\n+\n+const char *CracStackDumpParser::parse(const char *path, ParsedCracStackDump *out) {\n+  precond(path != nullptr);\n+  precond(out != nullptr);\n+\n+  log_info(crac, stacktrace, parser)(\"Started parsing %s\", path);\n+\n+  FileBasicTypeReader reader;\n+  if (!reader.open(path)) {\n+    log_error(crac, stacktrace, parser)(\"Failed to open %s: %s\", path, os::strerror(errno));\n+  }\n+\n+  u2 word_size;\n+  const char *err_msg = parse_header(&reader, &word_size);\n+  if (err_msg != nullptr) {\n+    return err_msg;\n+  }\n+  log_debug(crac, stacktrace, parser)(\"Word size: %i\", word_size);\n+  out->set_word_size(word_size);\n+\n+  err_msg = StackTracesParser(&reader, &out->stack_traces(), word_size).parse_stacks();\n+  if (err_msg == nullptr) {\n+    log_info(crac, stacktrace, parser)(\"Successfully parsed %s\", path);\n+  } else {\n+    log_info(crac, stacktrace, parser)(\"Position in %s after error: %zu\", path, reader.pos());\n+  }\n+  return err_msg;\n+}\n","filename":"src\/hotspot\/share\/runtime\/cracStackDumpParser.cpp","additions":367,"deletions":0,"binary":false,"changes":367,"status":"added"},{"patch":"@@ -0,0 +1,177 @@\n+#ifndef SHARE_RUNTIME_CRACSTACKDUMPPARSER_HPP\n+#define SHARE_RUNTIME_CRACSTACKDUMPPARSER_HPP\n+\n+#include \"jni.h\"\n+#include \"memory\/allocation.hpp\"\n+#include \"oops\/instanceKlass.hpp\"\n+#include \"runtime\/handles.hpp\"\n+#include \"runtime\/jniHandles.hpp\"\n+#include \"utilities\/debug.hpp\"\n+#include \"utilities\/exceptions.hpp\"\n+#include \"utilities\/globalDefinitions.hpp\"\n+#include \"utilities\/growableArray.hpp\"\n+#include \"utilities\/heapDumpParser.hpp\"\n+#include \"utilities\/methodKind.hpp\"\n+#include <type_traits>\n+\n+\/\/ Note: stack info parsing and usage happen in different resource and handle\n+\/\/ scopes -- that is why everything here is heap-allocated and JNI handles are\n+\/\/ used for OOPs\n+\n+\/\/ Format for stack dump IDs.\n+#define SDID_FORMAT UINT64_FORMAT\n+\n+\/\/ Parsed stack trace.\n+class CracStackTrace : public CHeapObj<mtInternal> {\n+ public:\n+  using ID = u8; \/\/ ID type always fits into 8 bytes\n+\n+  class Frame : public CHeapObj<mtInternal> {\n+   public:\n+    class Value {\n+     public:\n+      enum class Type : u1 {\n+        EMPTY, \/\/ Unfilled\n+        PRIM,  \/\/ Primitive stack value\n+        REF,   \/\/ Unresolved reference ID\n+        OBJ    \/\/ Resolved reference (JNI handle owned by this value)\n+      };\n+\n+      Value() = default;\n+      inline static Value of_primitive(u8 val) { return {Type::PRIM, val}; }\n+      inline static Value of_obj_id(ID id)     { return {Type::REF, id}; }\n+      inline static Value of_obj(Handle obj)   { return {JNIHandles::make_global(obj)}; }\n+\n+      \/\/ Copying: creates a new JNI handle if resolved.\n+      Value(const Value &other);\n+      Value &operator=(Value other);\n+\n+      ~Value() {\n+        if (type() == Type::OBJ) {\n+          JNIHandles::destroy_global(as_obj());\n+        }\n+      }\n+\n+      inline Type type() const       { return _type; }\n+\n+      inline u8 as_primitive() const { precond(type() == Type::PRIM); return _prim; }\n+      inline ID as_obj_id() const    { precond(type() == Type::REF);  return _obj_id; }\n+      inline jobject as_obj() const  { precond(type() == Type::OBJ);  return _obj; }\n+\n+     private:\n+      Type _type = Type::EMPTY;\n+      union {\n+        u8 _prim;     \/\/ If the stack slot is 4 bytes only the low half of u8 is used\n+        ID _obj_id;\n+        jobject _obj; \/\/ JNI handle\n+      };\n+\n+      Value(jobject obj) : _type(Type::OBJ), _obj(obj) {} \/\/ Implicit for no-copy return in of_obj()\n+      Value(Type type, u8 value) : _type(type), _prim(value) {\n+        STATIC_ASSERT((std::is_same<decltype(_prim), decltype(_obj_id)>::value));\n+        assert(type == Type::PRIM || type == Type::REF, \"use the other constructors\");\n+      }\n+    };\n+\n+    Method *resolve_method(const HeapDumpTable<InstanceKlass *, AnyObj::C_HEAP> &classes,\n+                           const ParsedHeapDump::RecordTable<HeapDump::UTF8> &symbols, TRAPS);\n+    Method *method() const { assert(_resolved_method != nullptr, \"unresolved\"); return _resolved_method; }\n+\n+    ID method_name_id() const                   { return _method_name_id; };\n+    void set_method_name_id(ID id)              { _method_name_id = id; }\n+\n+    ID method_sig_id() const                    { return _method_sig_id; };\n+    void set_method_sig_id(ID id)               { _method_sig_id = id; }\n+\n+    MethodKind::Enum method_kind() const        { return _method_kind; };\n+    void set_method_kind(MethodKind::Enum kind) { _method_kind = kind; }\n+\n+    ID method_holder_id() const                 { return _method_holder_id; };\n+    void set_method_holder_id(ID id)            { _method_holder_id = id; }\n+\n+    u2 bci() const                              { return _bci; }\n+    void set_bci(u2 bci)                        { _bci = bci;  }\n+\n+    const GrowableArrayCHeap<Value, mtInternal> &locals() const         { return _locals; }\n+    GrowableArrayCHeap<Value, mtInternal> &locals()                     { return _locals; }\n+\n+    const GrowableArrayCHeap<Value, mtInternal> &operands() const       { return _operands; }\n+    GrowableArrayCHeap<Value, mtInternal> &operands()                   { return _operands; }\n+\n+    const GrowableArrayCHeap<Value, mtInternal> &monitor_owners() const { return _monitor_owners; }\n+    GrowableArrayCHeap<Value, mtInternal> &monitor_owners()             { return _monitor_owners; }\n+\n+   private:\n+    ID _method_name_id;\n+    ID _method_sig_id;\n+    MethodKind::Enum _method_kind;\n+    ID _method_holder_id;\n+    Method *_resolved_method = nullptr;\n+\n+    u2 _bci;\n+\n+    GrowableArrayCHeap<Value, mtInternal> _locals;\n+    GrowableArrayCHeap<Value, mtInternal> _operands;\n+    GrowableArrayCHeap<Value, mtInternal> _monitor_owners;\n+  };\n+\n+  CracStackTrace(ID thread_id, u4 frames_num)\n+      : _thread_id(thread_id),  _frames_num(frames_num), _frames(new Frame[_frames_num]) {}\n+\n+  NONCOPYABLE(CracStackTrace);\n+\n+  ~CracStackTrace() {\n+    delete[] _frames;\n+    JNIHandles::destroy_global(_thread);\n+  }\n+\n+  \/\/ ID of the thread whose stack this is.\n+  \/\/ Note: this is a dump-internal ID, not the ID stored in the thread object.\n+  ID thread_id() const           { return _thread_id; }\n+\n+  \/\/ Resolved thread object. Must be set first.\n+  jobject thread() const         { precond(_thread != nullptr); return _thread; }\n+  void put_thread(Handle t)      { precond(_thread == nullptr && t.not_null()); _thread = JNIHandles::make_global(t); }\n+\n+  \/\/ Number of frames in the stack.\n+  u4 frames_num() const          { return _frames_num; }\n+  \/\/ Frames from oldest to youngest.\n+  const Frame &frame(u4 i) const { precond(i < frames_num()); return _frames[i]; }\n+  Frame &frame(u4 i)             { precond(i < frames_num()); return _frames[i]; }\n+  \/\/ Remove the youngest frame.\n+  void pop()                     { precond(frames_num() > 0); _frames_num--; }\n+\n+ private:\n+  const ID _thread_id;\n+  jobject _thread = nullptr; \/\/ JNI handle to the resolved thread object\n+  u4 _frames_num;\n+  Frame *const _frames;\n+};\n+\n+class ParsedCracStackDump : public CHeapObj<mtInternal> {\n+ public:\n+  ~ParsedCracStackDump() {\n+    for (auto *_stack_trace : _stack_traces) {\n+      delete _stack_trace;\n+    }\n+  }\n+\n+  \/\/ Size of IDs and stack slots in the dump.\n+  u2 word_size() const                                         { return _word_size; }\n+  void set_word_size(u2 value)                                 { _word_size = value; }\n+  \/\/ Parsed stack traces.\n+  const GrowableArrayView<CracStackTrace *> &stack_traces() const  { return _stack_traces; }\n+  GrowableArrayCHeap<CracStackTrace *, mtInternal> &stack_traces() { return _stack_traces; }\n+\n+ private:\n+  u2 _word_size = 0;\n+  GrowableArrayCHeap<CracStackTrace *, mtInternal> _stack_traces;\n+};\n+\n+struct CracStackDumpParser : public AllStatic {\n+  \/\/ Parses the stack dump in path filling the out container. Returns nullptr on\n+  \/\/ success or a pointer to a static error message otherwise.\n+  static const char *parse(const char *path, ParsedCracStackDump *out);\n+};\n+\n+#endif \/\/ SHARE_RUNTIME_CRACSTACKDUMPPARSER_HPP\n","filename":"src\/hotspot\/share\/runtime\/cracStackDumpParser.hpp","additions":177,"deletions":0,"binary":false,"changes":177,"status":"added"},{"patch":"@@ -0,0 +1,438 @@\n+#include \"precompiled.hpp\"\n+#include \"classfile\/javaClasses.hpp\"\n+#include \"classfile\/vmIntrinsics.hpp\"\n+#include \"classfile\/vmSymbols.hpp\"\n+#include \"interpreter\/bytecodes.hpp\"\n+#include \"logging\/log.hpp\"\n+#include \"memory\/allocation.hpp\"\n+#include \"memory\/resourceArea.hpp\"\n+#include \"memory\/universe.hpp\"\n+#include \"oops\/oopsHierarchy.hpp\"\n+#include \"runtime\/cracStackDumper.hpp\"\n+#include \"runtime\/javaThread.hpp\"\n+#include \"runtime\/objectMonitor.hpp\"\n+#include \"runtime\/os.hpp\"\n+#include \"runtime\/safepoint.hpp\"\n+#include \"runtime\/stackValue.hpp\"\n+#include \"runtime\/stackValueCollection.hpp\"\n+#include \"runtime\/threadSMR.hpp\"\n+#include \"runtime\/vframe.hpp\"\n+#include \"runtime\/vframe.inline.hpp\"\n+#include \"runtime\/vframe_hp.hpp\"\n+#include \"utilities\/basicTypeWriter.hpp\"\n+#include \"utilities\/debug.hpp\"\n+#include \"utilities\/globalDefinitions.hpp\"\n+#include \"utilities\/growableArray.hpp\"\n+#include \"utilities\/macros.hpp\"\n+#include \"utilities\/methodKind.hpp\"\n+\n+static uintptr_t oop2uint(oop o) {\n+  STATIC_ASSERT(sizeof(uintptr_t) == sizeof(intptr_t)); \/\/ Primitive stack slots\n+  STATIC_ASSERT(sizeof(uintptr_t) == oopSize);          \/\/ IDs\n+  return cast_from_oop<uintptr_t>(o);\n+}\n+\n+\/\/ Retrieves Java vframes from all non-internal Java threads in the VM.\n+class ThreadStackStream : public StackObj {\n+ public:\n+  enum class Status { OK, END, NON_JAVA_IN_MID, NON_JAVA_ON_TOP };\n+\n+  Status next() {\n+    if (!_started) {\n+      _started = true;\n+    } else {\n+      _thread_i++;\n+    }\n+\n+    JavaThread *thread;\n+    for (; _thread_i < _tlh.length(); _thread_i++) {\n+      thread = _tlh.thread_at(_thread_i);\n+      assert(thread->thread_state() == _thread_in_native || thread->thread_state() == _thread_blocked,\n+             \"must be on safepoint: either blocked or in native code\");\n+      if (should_include(*thread)) {\n+        break;\n+      }\n+      if (log_is_enabled(Debug, crac, stacktrace, dump)) {\n+        ResourceMark rm;\n+        log_debug(crac, stacktrace, dump)(\"Skipping thread \" UINTX_FORMAT \" (%s)\", oop2uint(thread->threadObj()), thread->name());\n+      }\n+    }\n+    if (_thread_i == _tlh.length()) {\n+      return Status::END;\n+    }\n+    postcond(thread != nullptr);\n+\n+    if (log_is_enabled(Debug, crac, stacktrace, dump)) {\n+      ResourceMark rm;\n+      log_debug(crac, stacktrace, dump)(\"Dumping thread \" UINTX_FORMAT \" (%s): state = %s\",\n+                                        oop2uint(thread->threadObj()), thread->name(), thread->thread_state_name());\n+    }\n+\n+    _frames.clear();\n+    vframeStream vfs(thread, \/*stop_at_java_call_stub=*\/ true);\n+\n+    if (!vfs.at_end() && vfs.method()->is_native() && !is_special_native_method(*vfs.method())) {\n+      if (log_is_enabled(Debug, crac, stacktrace, dump)) {\n+        ResourceMark rm;\n+        log_debug(crac, stacktrace, dump)(\"Thread \" UINTX_FORMAT \" (%s) is executing native method %s\",\n+                                          oop2uint(thread->threadObj()), thread->name(), vfs.method()->external_name());\n+      }\n+      return Status::NON_JAVA_ON_TOP;\n+    }\n+\n+    DEBUG_ONLY(bool is_youngest_frame = true);\n+    for (; !vfs.at_end(); vfs.next()) {\n+      assert(is_youngest_frame || !vfs.method()->is_native(), \"only the youngest frame can be native\");\n+      DEBUG_ONLY(is_youngest_frame = false);\n+      if (vfs.method()->is_old()) {\n+        ResourceMark rm;\n+        log_warning(crac, stacktrace, dump)(\"JVM TI support will be required on restore: thread %s executes an old version of %s\",\n+                                            thread->name(), vfs.method()->external_name());\n+        Unimplemented(); \/\/ TODO extend dump format with method holder's redefinition version\n+      }\n+      _frames.push(vfs.asJavaVFrame());\n+    }\n+\n+    if (_frames.is_empty() || vfs.reached_first_entry_frame()) {\n+      return Status::OK;\n+    }\n+\n+    if (log_is_enabled(Debug, crac, stacktrace, dump)) {\n+      ResourceMark rm;\n+      log_debug(crac, stacktrace, dump)(\"Thread \" UINTX_FORMAT \" (%s) has intermediate non-Java frame after %i Java frames\",\n+                                        oop2uint(thread->threadObj()), thread->name(), _frames.length());\n+    }\n+    return Status::NON_JAVA_IN_MID;\n+  }\n+\n+  JavaThread *thread()                            const { assert(_started, \"call next() first\"); return _tlh.thread_at(_thread_i); }\n+  const GrowableArrayView<javaVFrame *> &frames() const { assert(_started, \"call next() first\"); return _frames; };\n+\n+ private:\n+  bool _started = false;\n+  ThreadsListHandle _tlh;\n+  uint _thread_i = 0;\n+  GrowableArray<javaVFrame *> _frames;\n+\n+  \/\/ Whether this thread should be included in the dump.\n+  static bool should_include(const JavaThread &thread) {\n+    if (thread.is_exiting() ||\n+        thread.is_hidden_from_external_view() ||\n+        thread.is_Compiler_thread() ||\n+        thread.is_Notification_thread() ||\n+        thread.is_AttachListener_thread() || \/\/ TODO jcmd support will probably require this to be treated specially\n+        thread.is_jvmti_agent_thread()) {    \/\/ TODO JVM TI support: these are user-provided, need to think it through\n+      return false;\n+    }\n+    \/\/ TODO\n+    \/\/ 1. This way of identification is not fully accurate: the user can also\n+    \/\/    create threads that would match.\n+    \/\/ 2. All threads identified below, except Signal Dispatcher, are created\n+    \/\/    from Java, so we'll include them too when restoration of system\n+    \/\/    classes is supported.\n+    const oop tg = java_lang_Thread::threadGroup(thread.threadObj());\n+    if (tg == Universe::system_thread_group()) {\n+      ResourceMark rm;\n+      const char *thread_name = thread.name();\n+      if (strcmp(thread_name, \"Signal Dispatcher\") == 0 ||\n+          strcmp(thread_name, \"Finalizer\") == 0 ||\n+          strcmp(thread_name, \"Reference Handler\") == 0) {\n+        return false;\n+      }\n+    } else {\n+      ResourceMark rm;\n+      if (strcmp(thread.name(), \"Common-Cleaner\") == 0 &&\n+          strcmp(java_lang_ThreadGroup::name(tg), \"InnocuousThreadGroup\") == 0) {\n+        return false;\n+      }\n+    }\n+    return true;\n+  }\n+\n+  \/\/ Whether this is a native method known how to restore.\n+  static bool is_special_native_method(const Method &m) {\n+    precond(m.is_native());\n+    const InstanceKlass &holder = *m.method_holder();\n+    return \/\/ CRaC's C\/R method\n+           (holder.name() == vmSymbols::jdk_crac_Core() &&\n+            holder.class_loader_data()->is_the_null_class_loader_data() &&\n+            m.name() == vmSymbols::checkpointRestore0_name()) ||\n+           \/\/ Unsafe.park(...)\n+           m.intrinsic_id() == vmIntrinsics::_park;\n+  }\n+};\n+\n+class StackDumpWriter : public StackObj {\n+ private:\n+  BasicTypeWriter *_writer;\n+\n+#define WRITE(value) do { if (!_writer->write(value))                     return false; } while (false)\n+#define WRITE_RAW(value, size) do { if (!_writer->write_raw(value, size)) return false; } while (false)\n+#define WRITE_CASTED(t, value) do { if (!_writer->write<t>(value))        return false; } while (false)\n+\n+ public:\n+  explicit StackDumpWriter(BasicTypeWriter *writer) : _writer(writer) {}\n+\n+  bool write_header() {\n+    constexpr char HEADER[] = \"CRAC STACK DUMP 0.1\";\n+    WRITE_RAW(HEADER, sizeof(HEADER));\n+\n+    STATIC_ASSERT(sizeof(uintptr_t) == sizeof(u4) || sizeof(uintptr_t) == sizeof(u8));\n+    WRITE_CASTED(u2, sizeof(uintptr_t)); \/\/ Word size\n+\n+    return true;\n+  }\n+\n+  bool write_stack(const JavaThread *thread, const GrowableArrayView<javaVFrame *> &frames) {\n+    log_trace(crac, stacktrace, dump)(\"Stack for thread \" UINTX_FORMAT \" (%s)\", oop2uint(thread->threadObj()), thread->name());\n+    WRITE(oop2uint(thread->threadObj())); \/\/ Thread ID\n+\n+    log_trace(crac, stacktrace, dump)(\"%i frames\", frames.length());\n+    WRITE_CASTED(u4, frames.length()); \/\/ Number of frames in the stack\n+\n+    for (int i = 0; i < frames.length(); i++) {\n+      const javaVFrame &frame = *frames.at(i);\n+      if (log_is_enabled(Trace, crac, stacktrace, dump)) {\n+        if (frame.is_interpreted_frame()) {\n+          log_trace(crac, stacktrace, dump)(\"== Interpreted frame ==\");\n+        } else {\n+          precond(frame.is_compiled_frame());\n+          log_trace(crac, stacktrace, dump)(\"==  Compiled frame   ==\");\n+          \/\/ TODO do something like Deoptimization::rematerialize_objects(...)\n+          \/\/  and maybe also Deoptimization::restore_eliminated_locks(...)\n+        }\n+      }\n+\n+      if (!write_method(frame)) {\n+        return false;\n+      }\n+\n+      const u2 bci = get_bci(frame, i == 0);\n+      if (log_is_enabled(Trace, crac, stacktrace, dump)) {\n+        const char *code_name;\n+        if (!frame.method()->is_native()) {\n+          code_name = Bytecodes::name(frame.method()->java_code_at(bci));\n+        } else {\n+          assert(bci == 0, \"no bytecodes in a native method\");\n+          code_name = \"native entrance\";\n+        }\n+        log_trace(crac, stacktrace, dump)(\"BCI: %i (%s)\", bci, code_name);\n+      }\n+      WRITE(bci);\n+\n+      log_trace(crac, stacktrace, dump)(\"Locals:\");\n+      {\n+        ResourceMark rm;\n+        if (!write_stack_values(*frame.locals())) {\n+          return false;\n+        }\n+      }\n+\n+      log_trace(crac, stacktrace, dump)(\"Operands:\");\n+      {\n+        ResourceMark rm;\n+        if (!write_stack_values(get_operands(frame, i == 0))) {\n+          return false;\n+        }\n+      }\n+\n+      log_trace(crac, stacktrace, dump)(\"Monitors:\");\n+      {\n+        ResourceMark rm;\n+        if (!write_monitors(*frame.locked_monitors() \/* skips waiting and pending monitors *\/)) {\n+          return false;\n+        }\n+      }\n+\n+      log_trace(crac, stacktrace, dump)(\"=======================\");\n+    }\n+\n+    return true;\n+  }\n+\n+ private:\n+  static u2 get_bci(const javaVFrame &frame, bool is_youngest) {\n+    u2 bci = checked_cast<u2>(frame.bci()); \/\/ u2 is enough -- guaranteed by JVMS §4.7.3 (code_length max value)\n+\n+    if (!is_youngest) {\n+      assert(Bytecodes::is_invoke(frame.method()->java_code_at(bci)), \"non-youngest frames must be invoking\");\n+      return bci;\n+    }\n+\n+    \/\/ In the youngest frame we need to determine whether the BCI points to the\n+    \/\/ bytecode to be executed next or to the one that has already been executed\n+    if (frame.is_interpreted_frame()) {\n+      \/\/ Template interpreters increment BCI before blocking in monitorenter, undo that\n+      if (ZERO_ONLY(false &&) frame.thread()->current_pending_monitor() != nullptr) {\n+        if (bci > 0) { \/\/ Blocked on monitorenter\n+          static const int monitorenter_code_len = Bytecodes::length_for(Bytecodes::_monitorenter);\n+          log_trace(crac, stacktrace, dump)(\"moving BCI: %i -> %i\", bci, bci - monitorenter_code_len);\n+          guarantee(bci >= monitorenter_code_len, \"BCI %i is too small to point to one past monitorenter\", bci);\n+          bci -= monitorenter_code_len;\n+          assert(frame.method()->code_at(bci) == Bytecodes::_monitorenter, \"trying to enter a monitor in %s\",\n+                 Bytecodes::name(frame.method()->code_at(bci)));\n+        } else { \/\/ Blocked while entering a synchronized method\n+          assert(frame.method()->is_synchronized(), \"blocked while entering a non-synchronized method?\");\n+        }\n+      }\n+      \/\/ In any other case we assume that in interpreted frames BCI points to\n+      \/\/ the next code to be executed\n+      \/\/ TODO is the assumption actually always true?\n+    } else { \/\/ Compiled\n+      assert(frame.is_compiled_frame(), \"must be\");\n+      \/\/ In compiled frames we rely on should_reexecute()\n+      \/\/ TODO is exec_mode used by deoptimization to decide on re-execution also\n+      \/\/  important for us here?\n+      if (!frame.method()->is_native() && !static_cast<const compiledVFrame &>(frame).should_reexecute()) {\n+        const int code_len = Bytecodes::length_at(frame.method(), frame.method()->bcp_from(frame.bci()));\n+        log_trace(crac, stacktrace, dump)(\"moving BCI: %i -> %i\", bci, bci + code_len);\n+        guarantee(bci + code_len <= UINT16_MAX, \"overflow\");\n+        bci += code_len;\n+      }\n+    }\n+\n+    guarantee(frame.method()->validate_bci(bci) >= 0, \"invalid BCI %i for %s\", bci, frame.method()->external_name());\n+    return bci;\n+  }\n+\n+  \/\/ Fix-up top of the operand stack if needed.\n+  static StackValueCollection &get_operands(const javaVFrame &frame, bool is_youngest) {\n+    StackValueCollection &operands = *frame.expressions();\n+\n+    \/\/ Template interpreters pop ToS before blocking in monitorenter, undo that\n+    if (is_youngest &&\n+        ZERO_ONLY(false &&) frame.is_interpreted_frame() &&\n+        frame.thread()->current_pending_monitor() != nullptr &&\n+        frame.bci() > 0) { \/\/ Blocked with BCI == 0 means blocked trying to enter a synchronized method\n+      assert(!frame.method()->is_native(), \"BCI cannot be > 0 in a native frame\");\n+      const oop monitor_obj = frame.thread()->current_pending_monitor()->object();\n+      log_trace(crac, stacktrace, dump)(\"pushing \" UINTX_FORMAT \" as missing monitor object\", oop2uint(monitor_obj));\n+      const Handle h(Thread::current(), monitor_obj);\n+      operands.add(new StackValue(h));\n+    }\n+\n+    return operands;\n+  }\n+\n+  bool write_method(const javaVFrame &frame) {\n+    Method *method = frame.method();\n+\n+    Symbol *name = method->name();\n+    if (log_is_enabled(Trace, crac, stacktrace, dump)) {\n+      ResourceMark rm;\n+      log_trace(crac, stacktrace, dump)(\"Method name: \" UINTX_FORMAT \" - %s\", reinterpret_cast<uintptr_t>(name), name->as_C_string());\n+    }\n+    WRITE(reinterpret_cast<uintptr_t>(name));\n+\n+    Symbol *signature = method->signature();\n+    if (log_is_enabled(Trace, crac, stacktrace, dump)) {\n+      ResourceMark rm;\n+      log_trace(crac, stacktrace, dump)(\"Method signature: \" UINTX_FORMAT \" - %s\", reinterpret_cast<uintptr_t>(signature), signature->as_C_string());\n+    }\n+    WRITE(reinterpret_cast<uintptr_t>(signature));\n+\n+    const MethodKind::Enum kind = MethodKind::of_method(*method);\n+    log_trace(crac, stacktrace, dump)(\"Method kind: %s\", MethodKind::name(kind));\n+    WRITE(checked_cast<u1>(kind));\n+\n+    InstanceKlass *holder = method->method_holder();\n+    if (log_is_enabled(Trace, crac, stacktrace, dump)) {\n+      ResourceMark rm;\n+      log_trace(crac, stacktrace, dump)(\"Class: \" UINTX_FORMAT \" - %s\", oop2uint(holder->java_mirror()), holder->external_name());\n+    }\n+    WRITE(oop2uint(holder->java_mirror()));\n+\n+    return true;\n+  }\n+\n+  bool write_stack_values(const StackValueCollection &values) {\n+    assert(values.size() <= UINT16_MAX, \"guaranteed by JVMS §4.11\");\n+    log_trace(crac, stacktrace, dump)(\"%i values\", values.size());\n+    WRITE_CASTED(u2, values.size());\n+\n+    for (int i = 0; i < values.size(); i++) {\n+      const StackValue &value = *values.at(i);\n+      switch (value.type()) {\n+        case T_INT:\n+          log_trace(crac, stacktrace, dump)(\"  %i - primitive: \" INTX_FORMAT \" (intptr), \" INT32_FORMAT \" (jint), \" UINTX_FORMAT_X \" (hex)\",\n+                                            i, value.get_intptr(), value.get_jint(), value.get_intptr());\n+          WRITE_CASTED(u1, DumpedStackValueType::PRIMITIVE);\n+          WRITE_CASTED(uintptr_t, value.get_intptr()); \/\/ Write the whole slot, i.e. 4 or 8 bytes\n+          break;\n+        case T_OBJECT:\n+          log_trace(crac, stacktrace, dump)(\"  %i - oop: \" UINTX_FORMAT \"%s\",\n+                                            i, oop2uint(value.get_obj()()), value.obj_is_scalar_replaced() ? \" (scalar-replaced)\" : \"\");\n+          guarantee(!value.obj_is_scalar_replaced(), \"scalar-replaced objects should have been rematerialized\");\n+          WRITE_CASTED(u1, DumpedStackValueType::REFERENCE);\n+          WRITE(oop2uint(value.get_obj()()));\n+          break;\n+        case T_CONFLICT: \/\/ Compiled frames may contain these\n+          log_trace(crac, stacktrace, dump)(\"  %i - dead (dumping as 0)\", i);\n+          WRITE_CASTED(u1, DumpedStackValueType::PRIMITIVE);\n+          \/\/ Deopt code says this should be zero\/null in case it is actually\n+          \/\/ a reference to prevent GC from following it\n+          WRITE_CASTED(uintptr_t, 0);\n+          break;\n+        default:\n+          ShouldNotReachHere();\n+      }\n+    }\n+\n+    return true;\n+  }\n+\n+  bool write_monitors(const GrowableArray<MonitorInfo *> &monitors) {\n+    log_trace(crac, stacktrace, dump)(\"%i monitors\", monitors.length());\n+    WRITE_CASTED(u4, monitors.length());\n+\n+    for (const MonitorInfo *monitor : monitors) {\n+      guarantee(!monitor->eliminated(), \"eliminated locks should have been restored\"); \/\/ TODO can just treat them as normal?\n+      guarantee(!monitor->owner_is_scalar_replaced(), \"scalar-replaced objects should have been rematerialized\");\n+      const oop owner = monitor->owner();\n+      precond(owner != nullptr);\n+      log_trace(crac, stacktrace, dump)(\"  \" UINTX_FORMAT, oop2uint(owner));\n+      WRITE(oop2uint(owner));\n+    }\n+\n+    return true;\n+  }\n+\n+#undef WRITE_CASTED\n+#undef WRITE_RAW\n+#undef WRITE\n+};\n+\n+CracStackDumper::Result CracStackDumper::dump(const char *path, bool overwrite) {\n+  assert(SafepointSynchronize::is_at_safepoint(),\n+         \"need safepoint so threads won't change their states after we check them\");\n+  log_info(crac, stacktrace, dump)(\"Dumping thread stacks into %s\", path);\n+\n+  FileBasicTypeWriter file_writer;\n+  if (!file_writer.open(path, overwrite)) {\n+    return {Result::Code::IO_ERROR, os::strerror(errno)};\n+  }\n+\n+  StackDumpWriter dump_writer(&file_writer);\n+  if (!dump_writer.write_header()) {\n+    return {Result::Code::IO_ERROR, \"failed to write into the opened file\"};\n+  }\n+\n+  ResourceMark rm; \/\/ Frames are resource-allocated\n+  ThreadStackStream tss;\n+  ThreadStackStream::Status tss_status = tss.next();\n+  for (; tss_status == ThreadStackStream::Status::OK; tss_status = tss.next()) {\n+    if (!dump_writer.write_stack(tss.thread(), tss.frames())) {\n+      return {Result::Code::IO_ERROR, \"failed to write into the opened file\"};\n+    }\n+  }\n+  switch (tss_status) {\n+    case ThreadStackStream::Status::OK:              ShouldNotReachHere();\n+    case ThreadStackStream::Status::END:             return {};\n+    case ThreadStackStream::Status::NON_JAVA_ON_TOP: return {Result::Code::NON_JAVA_ON_TOP, tss.thread()};\n+    case ThreadStackStream::Status::NON_JAVA_IN_MID: return {Result::Code::NON_JAVA_IN_MID, tss.thread()};\n+  }\n+\n+  \/\/ Make the compiler happy\n+  ShouldNotReachHere();\n+  return {};\n+}\n","filename":"src\/hotspot\/share\/runtime\/cracStackDumper.cpp","additions":438,"deletions":0,"binary":false,"changes":438,"status":"added"},{"patch":"@@ -0,0 +1,98 @@\n+#ifndef SHARE_RUNTIME_CRACSTACKDUMPER_HPP\n+#define SHARE_RUNTIME_CRACSTACKDUMPER_HPP\n+\n+#include \"memory\/allStatic.hpp\"\n+#include \"runtime\/javaThread.hpp\"\n+#include \"utilities\/globalDefinitions.hpp\"\n+\n+\/\/ Thread stack dumping in the big-endian binary format described below.\n+\/\/\n+\/\/ Header:\n+\/\/   u1... -- null-termiminated string \"CRAC STACK DUMP 0.1\"\n+\/\/   u2    -- word size in bytes:\n+\/\/            4 -- IDs and primitives (PRs) are 4 byte, longs and doubles are\n+\/\/                 split in half into their slot pairs with the most significant\n+\/\/                 bits placed in the first slot\n+\/\/            8 -- IDs and primitives (PRs) are 8 byte, longs and doubles are\n+\/\/                 stored in the second slot of their slot pairs, while the\n+\/\/                 contents of the first slot are unspecified\n+\/\/   TODO to get rid of this 32-\/64-bit difference in how primitives are stored\n+\/\/    we need to be able to differentiate between longs\/doubles and other\n+\/\/    primitives when dumping the stack. This data is kinda available for\n+\/\/    compiled frames (see StackValue::create_stack_value() which creates\n+\/\/    StackValues for compiled frames), but not for interpreted ones.\n+\/\/ Stack traces:\n+\/\/   ID -- ID of the Thread object\n+\/\/   u4 -- number of frames that follow\n+\/\/   Frames, from youngest to oldest:\n+\/\/     ID -- ID of the method name String object\n+\/\/     ID -- ID of the method signature String object\n+\/\/     u1 -- method kind:\n+\/\/           0 -- static method\n+\/\/           1 -- non-static, non-overpass method\n+\/\/           2 -- overpass method\n+\/\/     ID -- ID of the Class object of the method's class\n+\/\/           TODO JVM TI Redefine\/RetransformClass support: add method holder's\n+\/\/                redefinition version to select the right one on restore.\n+\/\/     u2 -- bytecode index (BCI) of the current bytecode: for the youngest\n+\/\/           frame this specifies the bytecode to be executed, and for the rest\n+\/\/           of the frames this specifies the invoke bytecode being executed\n+\/\/     u2 -- number of locals that follow\n+\/\/     Locals array:\n+\/\/       u1    -- type: 0 -- primitive, 1 -- object reference\n+\/\/       u1... -- value: PR if the type is 0, ID if the type is 1\n+\/\/     u2 -- number of operands that follow\n+\/\/     Operand stack, from oldest to youngest:\n+\/\/       u1    -- type (same as for locals)\n+\/\/       u1... -- value (same as for locals)\n+\/\/     u4 -- number of monitors that follow\n+\/\/     Monitors:\n+\/\/       ID    -- ID of the object owning the monitor\n+\n+\/\/ Types of dumped locals and operands.\n+enum DumpedStackValueType : u1 { PRIMITIVE, REFERENCE };\n+\n+\/\/ Dumps Java frames (until the first CallStub) of non-internal Java threads.\n+\/\/ Threads are dumped in the order they were created (oldest first), dumped IDs\n+\/\/ are oops to be compatible with HeapDumper's object IDs.\n+struct CracStackDumper : public AllStatic {\n+  class Result {\n+   public:\n+    enum class Code {\n+      OK,              \/\/ Success\n+      IO_ERROR,        \/\/ File IO error, static message is in io_error_msg\n+      NON_JAVA_ON_TOP, \/\/ problematic_thread is running native code\n+      NON_JAVA_IN_MID  \/\/ problematic_thread is running Java code but with a native frame somewhere deeper in its stack\n+    };\n+\n+    Result() : _code(Code::OK) {}\n+\n+    Result(Code code, const char *io_error_msg) : _code(Code::OK), _io_error_msg(io_error_msg) {\n+      assert(code == Code::IO_ERROR && io_error_msg != nullptr, \"Use another constructor for this code\");\n+    }\n+\n+    Result(Code code, JavaThread *problematic_thread) : _code(code), _problematic_thread(problematic_thread) {\n+      assert(code > Code::IO_ERROR && problematic_thread != nullptr, \"Use another constructor for this code\");\n+    }\n+\n+    Code code()                      const { return _code; }\n+    \/\/ If the code indicates an IO error, holds its description. Null otherwise.\n+    const char *io_error_msg()       const { return _io_error_msg; }\n+    \/\/ If the code indicates a non-IO error, holds the thread for which stack\n+    \/\/ dump failed. Null otherwise.\n+    JavaThread *problematic_thread() const { return _problematic_thread; }\n+\n+   private:\n+    const Code _code;\n+    const char *const _io_error_msg = nullptr;\n+    JavaThread *const _problematic_thread = nullptr;\n+  };\n+\n+  \/\/ Dumps the stacks into the specified file, possibly overwriting it if the\n+  \/\/ corresponding parameter is set to true.\n+  \/\/\n+  \/\/ Must be called on safepoint.\n+  static Result dump(const char *path, bool overwrite = false);\n+};\n+\n+#endif \/\/ SHARE_RUNTIME_CRACSTACKDUMPER_HPP\n","filename":"src\/hotspot\/share\/runtime\/cracStackDumper.hpp","additions":98,"deletions":0,"binary":false,"changes":98,"status":"added"},{"patch":"@@ -0,0 +1,566 @@\n+#include \"precompiled.hpp\"\n+#include \"classfile\/javaClasses.hpp\"\n+#include \"classfile\/vmClasses.hpp\"\n+#include \"classfile\/vmIntrinsics.hpp\"\n+#include \"classfile\/vmSymbols.hpp\"\n+#include \"code\/debugInfoRec.hpp\"\n+#include \"interpreter\/interpreter.hpp\"\n+#include \"jvm.h\"\n+#include \"logging\/log.hpp\"\n+#include \"memory\/allocation.hpp\"\n+#include \"memory\/resourceArea.hpp\"\n+#include \"oops\/instanceKlass.hpp\"\n+#include \"oops\/method.hpp\"\n+#include \"oops\/symbol.hpp\"\n+#include \"runtime\/basicLock.hpp\"\n+#include \"runtime\/crac.hpp\"\n+#include \"runtime\/cracStackDumpParser.hpp\"\n+#include \"runtime\/cracThreadRestorer.hpp\"\n+#include \"runtime\/deoptimization.hpp\"\n+#include \"runtime\/frame.hpp\"\n+#include \"runtime\/handles.hpp\"\n+#include \"runtime\/interfaceSupport.inline.hpp\"\n+#include \"runtime\/javaCalls.hpp\"\n+#include \"runtime\/jniHandles.hpp\"\n+#include \"runtime\/jniHandles.inline.hpp\"\n+#include \"runtime\/monitorChunk.hpp\"\n+#include \"runtime\/mutexLocker.hpp\"\n+#include \"runtime\/semaphore.hpp\"\n+#include \"runtime\/signature.hpp\"\n+#include \"runtime\/stackValue.hpp\"\n+#include \"runtime\/stackValueCollection.hpp\"\n+#include \"runtime\/stubRoutines.hpp\"\n+#include \"runtime\/synchronizer.hpp\"\n+#include \"runtime\/vframeArray.hpp\"\n+#include \"utilities\/barrier.hpp\"\n+#include \"utilities\/bitCast.hpp\"\n+#include \"utilities\/debug.hpp\"\n+#include \"utilities\/globalDefinitions.hpp\"\n+#include \"utilities\/growableArray.hpp\"\n+#include \"utilities\/macros.hpp\"\n+\n+Barrier *CracThreadRestorer::_start_barrier = nullptr;\n+\n+void CracThreadRestorer::prepare(uint num_threads) {\n+  precond(_start_barrier == nullptr);\n+  _start_barrier = new Barrier(num_threads);\n+}\n+\n+static jlong log_tid(const JavaThread *thread) {\n+  return java_lang_Thread::thread_id(thread->threadObj());\n+}\n+\n+\/\/ Same jlong -> size_t conversion as JVM_StartThread performs.\n+static size_t get_stack_size(oop thread_obj) {\n+  const jlong raw_stack_size = java_lang_Thread::stackSize(thread_obj);\n+  if (raw_stack_size <= 0) {\n+    return 0;\n+  }\n+#ifndef  _LP64\n+  if (raw_stack_size > SIZE_MAX) {\n+    return SIZE_MAX;\n+  }\n+#endif \/\/ _LP64\n+  return checked_cast<size_t>(raw_stack_size);\n+}\n+\n+void CracThreadRestorer::restore_on_new_thread(CracStackTrace *stack, TRAPS) {\n+  const jobject thread_obj = stack->thread();\n+\n+  \/\/ Prepare a JavaThread in the same fashion as JVM_StartThread does\n+  JavaThread *thread;\n+  {\n+    MutexLocker ml(Threads_lock);\n+    const size_t stack_size = get_stack_size(JNIHandles::resolve_non_null(thread_obj));\n+    thread = new JavaThread(&restore_current_thread_impl, stack_size);\n+    if (thread->osthread() != nullptr) {\n+      HandleMark hm(JavaThread::current());\n+      thread->prepare(thread_obj);\n+    }\n+  }\n+  if (thread->osthread() == nullptr) {\n+    thread->smr_delete();\n+    THROW_MSG(vmSymbols::java_lang_OutOfMemoryError(), os::native_thread_creation_failed_msg());\n+  }\n+\n+  \/\/ Make the stack available to the restoration code (the thread now owns it)\n+  thread->set_crac_stack(stack);\n+\n+  \/\/ The thread will wait for the start signal\n+  Thread::start(thread);\n+}\n+\n+void CracThreadRestorer::restore_on_current_thread(CracStackTrace *stack, TRAPS) {\n+  JavaThread *const current = JavaThread::current();\n+  assert(JNIHandles::resolve_non_null(stack->thread()) == current->threadObj(), \"wrong stack trace\");\n+  current->set_crac_stack(stack); \/\/ Restoration code expects the stack here\n+  restore_current_thread_impl(current, CHECK);\n+}\n+\n+\/\/ Make this second-youngest frame the youngest faking the result of the\n+\/\/ callee (i.e. the current youngest) frame.\n+static void transform_to_youngest(CracStackTrace::Frame *frame, Handle callee_result) {\n+  const Bytecodes::Code code = frame->method()->code_at(frame->bci());\n+  assert(Bytecodes::is_invoke(code), \"non-youngest frames must be invoking, got %s\", Bytecodes::name(code));\n+\n+  \/\/ Push the result onto the operand stack\n+  if (callee_result.not_null()) {\n+    const auto operands_num = frame->operands().length();\n+    assert(operands_num < frame->method()->max_stack(), \"cannot push return value: all %i slots taken\",\n+           frame->method()->max_stack());\n+    frame->operands().reserve(operands_num + 1); \/\/ Not bare append because it may allocate more than one slot\n+    \/\/ FIXME append() creates a copy but accepts a reference so no copy elision can occur\n+    frame->operands().append({}); \/\/ Cheap empty->empty copy, empty->empty swap\n+    *frame->operands().adr_at(operands_num) = CracStackTrace::Frame::Value::of_obj(callee_result); \/\/ Cheap resolved->empty swap\n+  }\n+\n+  \/\/ Increment the BCI past the invoke bytecode\n+  const int code_len = Bytecodes::length_for(code);\n+  assert(code_len > 0, \"invoke codes don't need special length calculation\");\n+  frame->set_bci(frame->bci() + code_len);\n+  assert(frame->method()->validate_bci(frame->bci()) >= 0, \"transformed to invalid BCI %i\", frame->bci());\n+}\n+\n+\/\/ If the youngest frame represents special method requiring a fixup, applies\n+\/\/ the fixup.\n+static void fixup_youngest_frame_if_special(CracStackTrace *stack, TRAPS) {\n+  if (stack->frames_num() == 0) {\n+    return;\n+  }\n+\n+  const Method &youngest_m = *stack->frame(stack->frames_num() - 1).method();\n+  if (!youngest_m.is_native()) { \/\/ Only native methods are special\n+    return;\n+  }\n+  const InstanceKlass &holder = *youngest_m.method_holder();\n+\n+  if (holder.name() == vmSymbols::jdk_crac_Core() && holder.class_loader_data()->is_the_null_class_loader_data() &&\n+      youngest_m.name() == vmSymbols::checkpointRestore0_name()) {\n+    \/\/ Checkpoint initiation method: handled by imitating a successful return\n+\n+    \/\/ Pop the native frame\n+    stack->pop();\n+\n+    if (stack->frames_num() == 0) {\n+      return; \/\/ No Java caller (e.g. called from JNI)\n+    }\n+\n+    \/\/ Create the return value indicating the successful restoration\n+    HandleMark hm(Thread::current()); \/\/ The handle will either become an oop or a JNI handle\n+    const Handle bundle_h = crac::cr_return(JVM_CHECKPOINT_OK, {}, {}, {}, {}, CHECK);\n+\n+    \/\/ Push the return value onto the caller's operand stack and move to the next bytecode\n+    CracStackTrace::Frame &caller = stack->frame(stack->frames_num() - 1);\n+    transform_to_youngest(&caller, bundle_h);\n+  } else if (youngest_m.intrinsic_id() == vmIntrinsics::_park) {\n+    assert(holder.name() == vmSymbols::jdk_internal_misc_Unsafe() &&\n+           holder.class_loader_data()->is_the_null_class_loader_data() &&\n+           youngest_m.name() == vmSymbols::park_name(), \"must be\");\n+    \/\/ Unsafe.park(...): we use the fact that the method's specification allows\n+    \/\/ it to return spuriously, i.e. for no particular reason\n+\n+    \/\/ Pop the native frame\n+    stack->pop();\n+    \/\/ Move to the next bytecode in the caller's frame\n+    CracStackTrace::Frame &caller = stack->frame(stack->frames_num() - 1);\n+    transform_to_youngest(&caller, Handle() \/*don't place any return value*\/);\n+  } else if (&holder == vmClasses::Object_klass() && youngest_m.name() == vmSymbols::wait_name()) {\n+    \/\/ TODO simulate spurious wakeup: if the thread was not waiting (haven't\n+    \/\/  started or has already finished) then just pop the frame, otherwise pop\n+    \/\/  the frame and set some flag so that the thread enters the monitor before\n+    \/\/  resuming its execution. Need to add more info to the stack dump to\n+    \/\/  support this.\n+    log_error(crac)(\"Restoring %s is not implemented\", youngest_m.external_name());\n+    Unimplemented();\n+  } else {\n+    log_error(crac)(\"Unknown native method encountered: %s\", youngest_m.external_name());\n+    ShouldNotReachHere();\n+  }\n+}\n+\n+\/\/ Fills the provided arguments with null-values according to the provided\n+\/\/ signature.\n+class NullArgumentsFiller : public SignatureIterator {\n+  friend class SignatureIterator;  \/\/ so do_parameters_on can call do_type\n+ public:\n+  NullArgumentsFiller(Symbol *signature, JavaCallArguments *args) : SignatureIterator(signature), _args(args) {\n+    do_parameters_on(this);\n+  }\n+\n+ private:\n+  JavaCallArguments *_args;\n+\n+  void do_type(BasicType type) {\n+    switch (type) {\n+      case T_BYTE:\n+      case T_BOOLEAN:\n+      case T_CHAR:\n+      case T_SHORT:\n+      case T_INT:    _args->push_int(0);        break;\n+      case T_FLOAT:  _args->push_float(0);      break;\n+      case T_LONG:   _args->push_long(0);       break;\n+      case T_DOUBLE: _args->push_double(0);     break;\n+      case T_ARRAY:\n+      case T_OBJECT: _args->push_oop(Handle()); break;\n+      default:       ShouldNotReachHere();\n+    }\n+  }\n+};\n+\n+\/\/ Initiates thread restoration and won't return until the restored execution\n+\/\/ completes.\n+\/\/\n+\/\/ The process of thread restoration is as follows:\n+\/\/ 1. This method is called to make a Java-call to the initial method (the\n+\/\/ oldest one in the stack) with the snapshotted arguments, replacing its entry\n+\/\/ point with an entry into assembly restoration code (RestoreBlob).\n+\/\/ 2. Java-call places a CallStub frame for the initial method and calls\n+\/\/ RestoreBlob.\n+\/\/ 3. RestoreBlob calls fetch_frame_info() which prepares restoration info based\n+\/\/ on the stack snapshot. This cannot be perfomed directly in step 1: a\n+\/\/ safepoint can occur on step 2 which the prepared data won't survive.\n+\/\/ 4. RestoreBlob reads the prepared restoration info and creates so-called\n+\/\/ skeletal frames which are walkable interpreter frames of proper sizes but\n+\/\/ with monitors, locals, expression stacks, etc. unfilled.\n+\/\/ 5. RestoreBlob calls fill_in_frames() which also reads the prepared\n+\/\/ restoration info and fills the skeletal frames.\n+\/\/ 6. RestoreBlob jumps into the interpreter to start executing the youngest\n+\/\/ restored stack frame.\n+void CracThreadRestorer::restore_current_thread_impl(JavaThread *current, TRAPS) {\n+  assert(current == JavaThread::current(), \"must be\");\n+  if (log_is_enabled(Info, crac)) {\n+    ResourceMark rm;\n+    log_info(crac)(\"Thread \" JLONG_FORMAT \" (%s): starting restoration\", log_tid(current), current->name());\n+  }\n+\n+  \/\/ Get the stack trace to restore\n+  CracStackTrace *stack = current->crac_stack();\n+  assert(stack != nullptr, \"no stack to restore\");\n+\n+  \/\/ Check if there are special frames requiring fixup, this may pop some frames\n+  fixup_youngest_frame_if_special(stack, CHECK);\n+\n+  \/\/ Early return if empty: stack restoration does not account for this corner case\n+  if (stack->frames_num() == 0) {\n+    log_info(crac)(\"Thread \" JLONG_FORMAT \": no frames to restore\", log_tid(current));\n+    delete stack;\n+    current->set_crac_stack(nullptr);\n+    return;\n+  }\n+\n+  const CracStackTrace::Frame &oldest_frame = stack->frame(0);\n+  Method *const method = oldest_frame.method();\n+\n+  JavaCallArguments args;\n+  \/\/ Need to set the receiver (if any): it will be read during the Java call\n+  if (!method->is_static()) {\n+    guarantee(oldest_frame.locals().is_nonempty(), \"must have 'this' as the first local\");\n+    const CracStackTrace::Frame::Value &receiver = oldest_frame.locals().at(0); \/\/ at(0) returns a ref in contrast to first()\n+    args.set_receiver(Handle(current, JNIHandles::resolve_non_null(receiver.as_obj())));\n+  }\n+  \/\/ The actual values will be filled by the RestoreStub, we just need the Java\n+  \/\/ call code to allocate the right amount of space\n+  NullArgumentsFiller(method->signature(), &args);\n+  \/\/ Make the CallStub call RestoreStub instead of the actual method entry\n+  args.set_use_restore_stub(true);\n+\n+  if (log_is_enabled(Info, crac)) {\n+    ResourceMark rm;\n+    log_debug(crac)(\"Thread \" JLONG_FORMAT \": calling %s\", log_tid(current), method->external_name());\n+  }\n+  JavaValue result(method->result_type());\n+  JavaCalls::call(&result, methodHandle(current, method), &args, CHECK);\n+  \/\/ The stack snapshot has been freed already by now\n+\n+  log_info(crac)(\"Thread \" JLONG_FORMAT \": restored execution completed\", log_tid(current));\n+}\n+\n+class vframeRestoreArrayElement : public vframeArrayElement {\n+ public:\n+  void fill_in(const CracStackTrace::Frame &snapshot, bool reexecute) {\n+    _method = snapshot.method();\n+\n+    if (_method->is_synchronized() && snapshot.monitor_owners().is_empty()) {\n+      \/\/ The thread was blocked trying to enter the synchronized method\n+      guarantee(snapshot.bci() == 0, \"executing a synchronized method without holding its monitor\");\n+      _bci = SynchronizationEntryBCI;\n+      \/\/ TODO set_pending_monitorenter() is inside #if INCLUDE_JVMCI, the\n+      \/\/  related deopt entry code is too and it also checks EnableJVMCI\n+      \/\/ JavaThread::current()->set_pending_monitorenter(true);\n+      log_error(crac)(\"Restoring unentered synchronized methods is not implemented\");\n+      Unimplemented();\n+    } else {\n+      _bci = snapshot.bci();\n+    }\n+    guarantee(_method->validate_bci(_bci) == _bci, \"invalid bytecode index %i\", _bci);\n+\n+    _reexecute = reexecute;\n+\n+    _locals = stack_values_from_frame(snapshot.locals());\n+    _expressions = stack_values_from_frame(snapshot.operands());\n+\n+    _monitors = enter_monitors(snapshot.monitor_owners());\n+    DEBUG_ONLY(_removed_monitors = false;)\n+  }\n+\n+ private:\n+  static StackValueCollection *stack_values_from_frame(const GrowableArrayCHeap<CracStackTrace::Frame::Value, mtInternal> &src) {\n+    auto *const stack_values = new StackValueCollection(src.length()); \/\/ size == 0 until we actually add the values\n+    \/\/ Cannot use the array iterator as it creates copies and we cannot copy\n+    \/\/ resolved reference values in this scope (it requires a Handle allocation)\n+    for (int i = 0; i < src.length(); i++) {\n+      const auto &src_value = *src.adr_at(i);\n+      switch (src_value.type()) {\n+        \/\/ At checkpoint this was either a T_INT or a T_CONFLICT StackValue,\n+        \/\/ in the later case it should have been dumped as 0 for us\n+        case CracStackTrace::Frame::Value::Type::PRIM: {\n+          \/\/ We've checked that stack slot size of the dump equals ours (right\n+          \/\/ after parsing), so the cast is safe\n+          LP64_ONLY(const u8 val = src_value.as_primitive());  \/\/ Take the whole u8\n+          NOT_LP64(const u4 val = src_value.as_primitive());   \/\/ Take the low half\n+          const auto int_stack_slot = bit_cast<intptr_t>(val); \/\/ 4 or 8 byte slot depending on the platform\n+          stack_values->add(new StackValue(int_stack_slot));\n+          break;\n+        }\n+        \/\/ At checkpoint this was a T_OBJECT StackValue\n+        case CracStackTrace::Frame::Value::Type::OBJ: {\n+          const oop o = JNIHandles::resolve(src_value.as_obj()); \/\/ May be null\n+          \/\/ Unpacking code of vframeArrayElement expects a raw oop\n+          stack_values->add(new StackValue(cast_from_oop<intptr_t>(o), T_OBJECT));\n+          break;\n+        }\n+        default:\n+          ShouldNotReachHere();\n+      }\n+    }\n+    return stack_values;\n+  }\n+\n+  \/\/ TODO defer entering the monitors until we are unpacking -- this will allow\n+  \/\/  to omit monitor inflation\n+  static MonitorChunk *enter_monitors(const GrowableArrayCHeap<CracStackTrace::Frame::Value, mtInternal> &monitor_owners) {\n+    if (monitor_owners.is_empty()) {\n+      return nullptr;\n+    }\n+\n+    JavaThread *const current = JavaThread::current();\n+    HandleMark hm(current);\n+\n+    auto *const monitor_chunk = new MonitorChunk(monitor_owners.length());\n+    current->add_monitor_chunk(monitor_chunk);\n+\n+    for (int i = 0; i < monitor_chunk->number_of_monitors(); i++) {\n+      const oop owner = JNIHandles::resolve_non_null(monitor_owners.adr_at(i)->as_obj());\n+      log_trace(crac)(\"Thread \" JLONG_FORMAT \":   entering monitor of \" PTR_FORMAT, log_tid(current), p2i(owner));\n+\n+      \/\/ Temporary lock -- it will later be replaced with the real on-stack lock\n+      \/\/ and deleted\n+      BasicObjectLock *const bol = monitor_chunk->at(i);\n+      bol->set_obj(owner);\n+\n+      \/\/ Inflation notes:\n+      \/\/ 1. Need to inflate because otherwise when using the legacy lightweight\n+      \/\/ locking owner's header will point to the temporary lock (and this link\n+      \/\/ won't be updated).\n+      \/\/ 2. Inflation must be performed before entering because otherwise a link\n+      \/\/ to the temporary lock will be stored into the monitor which will lead\n+      \/\/ to failed ownership checks (the lock is expected to be on the thread's\n+      \/\/ stack which is false for the temporary lock).\n+      ObjectSynchronizer::inflate_helper(owner);\n+      \/\/ Stack values we've created are not safepoint-safe, but we should only\n+      \/\/ enter monitors this thread owned so no safepoint should occur here\n+      \/\/ (JRT_LEAF also creates a NoSafepointVerifier to check for this)\n+      ObjectSynchronizer::enter(Handle(current, owner), bol->lock(), current);\n+      postcond(owner->is_locked());\n+    }\n+\n+    return monitor_chunk;\n+  }\n+};\n+\n+class vframeRestoreArray : public vframeArray {\n+ public:\n+  static vframeRestoreArray *allocate(const CracStackTrace &stack) {\n+    guarantee(stack.frames_num() <= INT_MAX, \"stack trace of thread \" SDID_FORMAT \" is too long: \" UINT32_FORMAT \" > %i\",\n+              stack.thread_id(), stack.frames_num(), INT_MAX);\n+    auto *const result = reinterpret_cast<vframeRestoreArray *>(AllocateHeap(sizeof(vframeRestoreArray) + \/\/ fixed part\n+                                                                             sizeof(vframeRestoreArrayElement) * (stack.frames_num() - 1), \/\/ variable part\n+                                                                             mtInternal));\n+    result->_frames = static_cast<int>(stack.frames_num());\n+    result->set_unroll_block(nullptr); \/\/ The actual value should be set by the caller later\n+\n+    \/\/ We don't use these\n+    result->_owner_thread = nullptr; \/\/ Would have been JavaThread::current()\n+    result->_sender = frame();       \/\/ Will be the CallStub frame called before the restored frames\n+    result->_caller = frame();       \/\/ Seems to be the same as _sender\n+    result->_original = frame();     \/\/ Deoptimized frame which we don't have\n+\n+    result->fill_in(stack);\n+    return result;\n+  }\n+\n+ private:\n+  void fill_in(const CracStackTrace &stack) {\n+    _frame_size = 0; \/\/ Unused (no frame is being deoptimized)\n+    const JavaThread *current = DEBUG_ONLY(true ||) log_is_enabled(Trace, crac) ? JavaThread::current() : nullptr;\n+    assert(current->monitor_chunks() == nullptr, \"deoptimization in progress?\");\n+\n+    \/\/ vframeRestoreArray: the first frame is the youngest, the last is the oldest\n+    \/\/ CracStackTrace:     the first frame is the oldest, the last is the youngest\n+    log_trace(crac)(\"Thread \" JLONG_FORMAT \": filling stack trace \" SDID_FORMAT, log_tid(current), stack.thread_id());\n+    precond(frames() == checked_cast<int>(stack.frames_num()));\n+    for (int i = 0; i < frames(); i++) {\n+      if (log_is_enabled(Trace, crac)) {\n+        ResourceMark rm;\n+        log_trace(crac)(\"Thread \" JLONG_FORMAT \": filling frame %i (%s)\",\n+                        log_tid(current), i, stack.frame(frames() - 1 - i).method()->external_name());\n+      }\n+\n+      auto *const elem = static_cast<vframeRestoreArrayElement *>(element(i));\n+      \/\/ Note: youngest frame's BCI is always re-executed -- this is important\n+      \/\/ because otherwise deopt's unpacking code will try to use ToS caching\n+      \/\/ which we don't account for\n+      elem->fill_in(stack.frame(frames() - 1 - i), \/*reexecute when youngest*\/ i == 0);\n+      assert(!elem->method()->is_native(), \"native methods are not restored\");\n+    }\n+  }\n+};\n+\n+\/\/ Called by RestoreBlob to get the info about the frames to restore. This is\n+\/\/ analogous to Deoptimization::fetch_unroll_info() except that we fetch the\n+\/\/ info from the stack snapshot instead of a deoptee frame. This is also a leaf\n+\/\/ (in contrast with fetch_unroll_info) since no reallocation is needed (see the\n+\/\/ comment before fetch_unroll_info).\n+JRT_LEAF(Deoptimization::UnrollBlock *, CracThreadRestorer::fetch_frame_info(JavaThread *current))\n+  precond(current == JavaThread::current());\n+  log_debug(crac)(\"Thread \" JLONG_FORMAT \": fetching frame info\", log_tid(current));\n+\n+  \/\/ Heap-allocated resource mark to use resource-allocated StackValues\n+  \/\/ and free them before starting executing the restored code\n+  guarantee(current->deopt_mark() == nullptr, \"No deopt should be pending\");\n+  current->set_deopt_mark(new DeoptResourceMark(current));\n+\n+  \/\/ Create vframe descriptions based on the stack snapshot -- no safepoint\n+  \/\/ should happen after this array is filled until we're done with it\n+  vframeRestoreArray *array;\n+  {\n+    \/\/ Reset NoHandleMark created by JRT_LEAF (see related comments in\n+    \/\/ Deoptimization::unpack_frames() on why this is ok). Handles are used\n+    \/\/ when entering monitors.\n+    ResetNoHandleMark rnhm;\n+\n+    const CracStackTrace *stack = current->crac_stack();\n+    assert(stack->frames_num() > 0, \"should be checked when starting\");\n+\n+    array = vframeRestoreArray::allocate(*stack);\n+    postcond(array->frames() == static_cast<int>(stack->frames_num()));\n+\n+    delete stack;\n+    current->set_crac_stack(nullptr);\n+  }\n+  postcond(array->frames() > 0);\n+  log_trace(crac)(\"Thread \" JLONG_FORMAT \": filled frame array (%i frames)\", log_tid(current), array->frames());\n+\n+  \/\/ Determine sizes and return pcs of the constructed frames.\n+  \/\/\n+  \/\/ The order of frames is the reverse of the array above:\n+  \/\/ frame_sizes and frame_pcs: 0th -- the oldest frame,   nth -- the youngest.\n+  \/\/ vframeRestoreArray *array: 0th -- the youngest frame, nth -- the oldest.\n+  auto *const frame_sizes = NEW_C_HEAP_ARRAY(intptr_t, array->frames(), mtInternal);\n+  \/\/ +1 because the last element is an address to jump into the interpreter\n+  auto *const frame_pcs = NEW_C_HEAP_ARRAY(address, array->frames() + 1, mtInternal);\n+  \/\/ Create an interpreter return address for the assembly code to use as its\n+  \/\/ return address so the skeletal frames are perfectly walkable\n+  frame_pcs[array->frames()] = Interpreter::deopt_entry(vtos, 0);\n+\n+  \/\/ We start from the youngest frame, which has no callee\n+  int callee_params = 0;\n+  int callee_locals = 0;\n+  for (int i = 0; i < array->frames(); i++) {\n+    \/\/ Deopt code uses this to account for possible JVMTI's PopFrame function\n+    \/\/ usage which is irrelevant in our case\n+    static constexpr int popframe_extra_args = 0;\n+\n+    \/\/ i == 0 is the youngest frame, i == array->frames() - 1 is the oldest\n+    frame_sizes[array->frames() - i - 1] =\n+        BytesPerWord * array->element(i)->on_stack_size(callee_params, callee_locals, i == 0, popframe_extra_args);\n+\n+    frame_pcs[array->frames() - i - 1] = i < array->frames() - 1 ?\n+    \/\/ Setting the pcs the same way as the deopt code does. It is needed to\n+    \/\/ identify the skeleton frames as interpreted and make them walkable. The\n+    \/\/ correct pcs will be patched later when filling the frames.\n+                                         Interpreter::deopt_entry(vtos, 0) - frame::pc_return_offset :\n+    \/\/ The oldest frame always returns to CallStub\n+                                         StubRoutines::call_stub_return_address();\n+\n+    callee_params = array->element(i)->method()->size_of_parameters();\n+    callee_locals = array->element(i)->method()->max_locals();\n+  }\n+\n+  \/\/ Adjustment of the CallStub to accomodate the locals of the oldest restored\n+  \/\/ frame, if any\n+  const int caller_adjustment = Deoptimization::last_frame_adjust(callee_params, callee_locals);\n+\n+  auto *const info = new Deoptimization::UnrollBlock(\n+    0,                           \/\/ Deoptimized frame size, unused (no frame is being deoptimized)\n+    caller_adjustment * BytesPerWord,\n+    0,                           \/\/ Amount of params in the CallStub frame, unused (known via the oldest frame's method)\n+    array->frames(),\n+    frame_sizes,\n+    frame_pcs,\n+    BasicType::T_ILLEGAL,        \/\/ Return type, unused (we are not in the process of returning a value)\n+    Deoptimization::Unpack_deopt \/\/ fill_in_frames() always specifies Unpack_deopt, regardless of what's set here\n+  );\n+  array->set_unroll_block(info);\n+\n+  guarantee(current->vframe_array_head() == nullptr, \"no deopt should be pending\");\n+  current->set_vframe_array_head(array);\n+\n+  log_debug(crac)(\"Thread \" JLONG_FORMAT \": frame info fetched\", log_tid(current));\n+  return info;\n+JRT_END\n+\n+\/\/ Called by RestoreBlob after skeleton frames have been pushed on stack to fill\n+\/\/ them. This is analogous to Deoptimization::unpack_frames().\n+JRT_LEAF(void, CracThreadRestorer::fill_in_frames(JavaThread *current))\n+  precond(current == JavaThread::current());\n+  log_debug(crac)(\"Thread \" JLONG_FORMAT \": filling skeletal frames\", log_tid(current));\n+\n+  \/\/ Reset NoHandleMark created by JRT_LEAF (see related comments in\n+  \/\/ Deoptimization::unpack_frames() on why this is ok). Handles are used e.g.\n+  \/\/ in trace printing.\n+  ResetNoHandleMark rnhm;\n+  HandleMark hm(current);\n+\n+  \/\/ Array created by fetch_frame_info()\n+  vframeArray *const array = current->vframe_array_head();\n+  \/\/ Java frame between the skeleton frames and the frame of this function\n+  const frame unpack_frame = current->last_frame();\n+  \/\/ Amount of parameters in the CallStub frame = amount of parameters of the\n+  \/\/ oldest skeleton frame\n+  const int initial_caller_parameters = array->element(array->frames() - 1)->method()->size_of_parameters();\n+\n+  \/\/ TODO save, clear, restore last Java sp like the deopt code does?\n+\n+  assert(current->deopt_compiled_method() == nullptr, \"no method is being deoptimized\");\n+  guarantee(current->frames_to_pop_failed_realloc() == 0,\n+            \"we don't deoptimize, so no reallocations of scalar replaced objects can happen and fail\");\n+  array->unpack_to_stack(unpack_frame, Deoptimization::Unpack_deopt \/* TODO this or reexecute? *\/, initial_caller_parameters);\n+  log_debug(crac)(\"Thread \" JLONG_FORMAT \": skeletal frames filled\", log_tid(current));\n+\n+  \/\/ Cleanup, analogous to Deoptimization::cleanup_deopt_info()\n+  current->set_vframe_array_head(nullptr);\n+  delete array->unroll_block(); \/\/ Also deletes frame_sizes and frame_pcs\n+  delete array;\n+  delete current->deopt_mark();\n+  current->set_deopt_mark(nullptr);\n+\n+  \/\/ TODO more verifications, like the ones Deoptimization::unpack_frames() does\n+  DEBUG_ONLY(current->validate_frame_layout();)\n+\n+  \/\/ Wait for all threads to complete their restoration so that we won't enter\n+  \/\/ someone else's monitors while executing Java code. This barrier can be\n+  \/\/ placed anywhere after restored monitors have been entered.\n+  log_debug(crac)(\"Thread \" JLONG_FORMAT \": arrived at start barrier\", log_tid(current));\n+  _start_barrier->arrive();\n+JRT_END\n","filename":"src\/hotspot\/share\/runtime\/cracThreadRestorer.cpp","additions":566,"deletions":0,"binary":false,"changes":566,"status":"added"},{"patch":"@@ -0,0 +1,42 @@\n+#ifndef SHARE_RUNTIME_CRACTHREADRESTORER_HPP\n+#define SHARE_RUNTIME_CRACTHREADRESTORER_HPP\n+\n+#include \"memory\/allStatic.hpp\"\n+#include \"runtime\/cracStackDumpParser.hpp\"\n+#include \"runtime\/deoptimization.hpp\"\n+#include \"runtime\/javaThread.hpp\"\n+#include \"utilities\/exceptions.hpp\"\n+\n+class Barrier;\n+\n+\/\/ Thread restoration for CRaC's portable mode.\n+\/\/\n+\/\/ It is intended to be used as follows:\n+\/\/ 1. A pre-existing Java thread (typically the main thread) initiates\n+\/\/    restoration of other threads.\n+\/\/ 2. The pre-existing thread restores its own execution.\n+\/\/\n+\/\/ After that the restorer cannot be reused.\n+class CracThreadRestorer : public AllStatic {\n+ public:\n+  \/\/ Prepares this class to restore the specified amount of threads. The threads\n+  \/\/ will wait for all of them to get restored before starting executing Java.\n+  static void prepare(uint num_threads);\n+  \/\/ Creates a new JavaThread and asynchronously restores the provided stack.\n+  static void restore_on_new_thread(CracStackTrace *stack, TRAPS);\n+  \/\/ Synchronously restores the provided stack on the current thread.\n+  static void restore_on_current_thread(CracStackTrace *stack, TRAPS);\n+\n+  \/\/ Called by RestoreStub to prepare information about frames to restore.\n+  static Deoptimization::UnrollBlock *fetch_frame_info(JavaThread *current);\n+  \/\/ Called by RestoreStub to fill in the skeletal frames just created.\n+  static void fill_in_frames(JavaThread *current);\n+\n+ private:\n+  \/\/ TODO delete the barrier after the threads have been restored.\n+  static Barrier *_start_barrier;\n+\n+  static void restore_current_thread_impl(JavaThread *current, TRAPS);\n+};\n+\n+#endif \/\/ SHARE_RUNTIME_CRACTHREADRESTORER_HPP\n","filename":"src\/hotspot\/share\/runtime\/cracThreadRestorer.hpp","additions":42,"deletions":0,"binary":false,"changes":42,"status":"added"},{"patch":"@@ -159,0 +159,8 @@\n+  void clear() {\n+    if (_raw_content) {\n+      FREE_C_HEAP_ARRAY(char, _raw_content);\n+    }\n+    _raw_content = NULL;\n+    _properties->clear_and_deallocate();\n+    _args = NULL;\n+  }\n@@ -162,1 +170,5 @@\n-  jarray _fd_arr;\n+ public:\n+  enum class Outcome { OK, FAIL, RETRY };\n+\n+ private:\n+  const jarray _fd_arr;\n@@ -164,2 +176,2 @@\n-  bool _ok;\n-  GrowableArray<CracFailDep>* _failures;\n+  Outcome _outcome;\n+  GrowableArray<CracFailDep>* const _failures;\n@@ -176,1 +188,1 @@\n-    _ok(false),\n+    _outcome(Outcome::FAIL),\n@@ -189,2 +201,2 @@\n-  GrowableArray<CracFailDep>* failures() { return _failures; }\n-  bool ok() { return _ok; }\n+  const GrowableArray<CracFailDep>* failures() const { return _failures; }\n+  Outcome outcome() const { return _outcome; }\n@@ -192,1 +204,1 @@\n-  GrowableArray<const char *>* new_properties() { return _restore_parameters.properties(); }\n+  const GrowableArray<const char *>* new_properties() const { return _restore_parameters.properties(); }\n","filename":"src\/hotspot\/share\/runtime\/crac_structs.hpp","additions":19,"deletions":7,"binary":false,"changes":26,"status":"modified"},{"patch":"@@ -602,0 +602,40 @@\n+#ifndef PRODUCT\n+\/\/ Returns true if x is an oop. If it isn't, may return either true or false,\n+\/\/ but most likely false.\n+static bool is_probably_oop(intptr_t x) {\n+  const oop o = cast_to_oop(x);\n+  return oopDesc::is_oop(o) && o->klass_or_null() != nullptr && o->klass()->is_klass();\n+}\n+#endif\n+\n+void frame::interpreter_frame_print_values_on(outputStream* st) const {\n+#ifndef PRODUCT\n+  assert(is_interpreted_frame(), \"Not an interpreted frame\");\n+  jint i;\n+  for (i = 0; i < interpreter_frame_method()->max_locals(); i++ ) {\n+    intptr_t x = *interpreter_frame_local_at(i);\n+    st->print(\" - local  [\" INTPTR_FORMAT \"]\", x);\n+    st->fill_to(23);\n+    st->print(\"; #%d\", i);\n+    if (is_probably_oop(x)) {\n+      ResourceMark rm;\n+      \/\/ May crash if not actually an oop but ok for debugging\n+      st->print(\" - probably %s oop\", cast_to_oop(x)->klass()->internal_name());\n+    }\n+    st->cr();\n+  }\n+  for (i = interpreter_frame_expression_stack_size() - 1; i >= 0; --i ) {\n+    intptr_t x = *interpreter_frame_expression_stack_at(i);\n+    st->print(\" - stack  [\" INTPTR_FORMAT \"]\", x);\n+    st->fill_to(23);\n+    st->print(\"; #%d\", i);\n+    if (is_probably_oop(x)) {\n+      ResourceMark rm;\n+      \/\/ May crash if not actually an oop but ok for debugging\n+      st->print(\" - probably %s oop\", cast_to_oop(x)->klass()->internal_name());\n+    }\n+    st->cr();\n+  }\n+#endif\n+}\n+\n","filename":"src\/hotspot\/share\/runtime\/frame.cpp","additions":40,"deletions":0,"binary":false,"changes":40,"status":"modified"},{"patch":"@@ -424,0 +424,1 @@\n+  void interpreter_frame_print_values_on(outputStream* st) const;\n","filename":"src\/hotspot\/share\/runtime\/frame.hpp","additions":1,"deletions":0,"binary":false,"changes":1,"status":"modified"},{"patch":"@@ -860,0 +860,3 @@\n+  develop(bool, TraceOperands, false,                                       \\\n+          \"Extend bytecode trace with operand stack dumps\")                 \\\n+                                                                            \\\n@@ -2004,1 +2007,2 @@\n-      \"Path or name of a program implementing checkpoint\/restore and \"      \\\n+      \"Either (1) or (2). \"                                                 \\\n+      \"(1) Path or name of a program implementing checkpoint\/restore and \"  \\\n@@ -2008,1 +2012,2 @@\n-      \"should be escaped with a backslash character ('\\\\').\")               \\\n+      \"should be escaped with a backslash character ('\\\\'). \"               \\\n+      \"(2) Empty string — use an experimental portable CR implementation.\") \\\n","filename":"src\/hotspot\/share\/runtime\/globals.hpp","additions":7,"deletions":2,"binary":false,"changes":9,"status":"modified"},{"patch":"@@ -367,0 +367,6 @@\n+  if (args->use_restore_stub()) {\n+#if INCLUDE_JVMCI\n+    precond(args->alternative_target().is_null());\n+#endif \/\/ INCLUDE_JVMCI\n+    entry_point = SharedRuntime::get_restore_stub();\n+  }\n","filename":"src\/hotspot\/share\/runtime\/javaCalls.cpp","additions":6,"deletions":0,"binary":false,"changes":6,"status":"modified"},{"patch":"@@ -93,0 +93,3 @@\n+  \/\/ TODO incorporate this into _alternative_target?\n+  bool        _use_restore_stub = false; \/\/ Use RestoreBlob as an entry point instead of the normal\n+                                         \/\/ method entry\n@@ -139,0 +142,7 @@\n+  void set_use_restore_stub(bool value) {\n+    _use_restore_stub = value;\n+  }\n+  bool use_restore_stub() const {\n+    return _use_restore_stub;\n+  }\n+\n","filename":"src\/hotspot\/share\/runtime\/javaCalls.hpp","additions":10,"deletions":0,"binary":false,"changes":10,"status":"modified"},{"patch":"@@ -416,0 +416,1 @@\n+  _crac_stack(nullptr),\n@@ -1457,16 +1458,0 @@\n-const char* _get_thread_state_name(JavaThreadState _thread_state) {\n-  switch (_thread_state) {\n-  case _thread_uninitialized:     return \"_thread_uninitialized\";\n-  case _thread_new:               return \"_thread_new\";\n-  case _thread_new_trans:         return \"_thread_new_trans\";\n-  case _thread_in_native:         return \"_thread_in_native\";\n-  case _thread_in_native_trans:   return \"_thread_in_native_trans\";\n-  case _thread_in_vm:             return \"_thread_in_vm\";\n-  case _thread_in_vm_trans:       return \"_thread_in_vm_trans\";\n-  case _thread_in_Java:           return \"_thread_in_Java\";\n-  case _thread_in_Java_trans:     return \"_thread_in_Java_trans\";\n-  case _thread_blocked:           return \"_thread_blocked\";\n-  case _thread_blocked_trans:     return \"_thread_blocked_trans\";\n-  default:                        return \"unknown thread state\";\n-  }\n-}\n@@ -1475,1 +1460,1 @@\n-  st->print_cr(\"   JavaThread state: %s\", _get_thread_state_name(_thread_state));\n+  st->print_cr(\"   JavaThread state: %s\", thread_state_name());\n@@ -1538,1 +1523,1 @@\n-  st->print(\"%s\", _get_thread_state_name(_thread_state));\n+  st->print(\"%s\", thread_state_name());\n","filename":"src\/hotspot\/share\/runtime\/javaThread.cpp","additions":3,"deletions":18,"binary":false,"changes":21,"status":"modified"},{"patch":"@@ -48,0 +48,1 @@\n+#include \"utilities\/sizes.hpp\"\n@@ -75,0 +76,2 @@\n+class CracStackTrace;\n+\n@@ -145,0 +148,3 @@\n+  \/\/ Portable CRaC support\n+  CracStackTrace *_crac_stack; \/\/ Stack trace to be restored on this thread\n+\n@@ -559,0 +565,1 @@\n+  inline const char *thread_state_name() const;\n@@ -701,0 +708,3 @@\n+  CracStackTrace *crac_stack()                   { return _crac_stack; }\n+  void set_crac_stack(CracStackTrace *stack)     { precond(_crac_stack == nullptr || stack == nullptr); _crac_stack = stack; }\n+\n@@ -782,1 +792,0 @@\n-\n@@ -784,0 +793,1 @@\n+  static ByteSize vframe_array_head_offset()     { return byte_offset_of(JavaThread, _vframe_array_head); }\n","filename":"src\/hotspot\/share\/runtime\/javaThread.hpp","additions":11,"deletions":1,"binary":false,"changes":12,"status":"modified"},{"patch":"@@ -167,0 +167,4 @@\n+inline const char *JavaThread::thread_state_name() const {\n+  return ::thread_state_name(_thread_state);\n+}\n+\n","filename":"src\/hotspot\/share\/runtime\/javaThread.inline.hpp","additions":4,"deletions":0,"binary":false,"changes":4,"status":"modified"},{"patch":"@@ -44,1 +44,1 @@\n-\n+  bool is_Notification_thread() const override { return true; }\n","filename":"src\/hotspot\/share\/runtime\/notificationThread.hpp","additions":1,"deletions":1,"binary":false,"changes":2,"status":"modified"},{"patch":"@@ -72,1 +72,1 @@\n-int FieldStream::length() { return _klass->java_fields_count(); }\n+int FieldStream::length() { return _java_only ? _klass->java_fields_count() : _klass->total_fields_count(); }\n","filename":"src\/hotspot\/share\/runtime\/reflectionUtils.cpp","additions":1,"deletions":1,"binary":false,"changes":2,"status":"modified"},{"patch":"@@ -125,0 +125,1 @@\n+  bool _java_only; \/\/ If false, include injected fields\n@@ -128,2 +129,2 @@\n-  FieldStream(InstanceKlass* klass, bool local_only, bool classes_only)\n-    : KlassStream(klass, local_only, classes_only, false) {\n+  FieldStream(InstanceKlass* klass, bool local_only, bool classes_only, bool java_only = true)\n+    : KlassStream(klass, local_only, classes_only, false), _java_only(java_only) {\n","filename":"src\/hotspot\/share\/runtime\/reflectionUtils.hpp","additions":3,"deletions":2,"binary":false,"changes":5,"status":"modified"},{"patch":"@@ -94,0 +94,1 @@\n+RestoreBlob*        SharedRuntime::_restore_blob;\n@@ -132,0 +133,2 @@\n+\n+  generate_restore_blob();\n","filename":"src\/hotspot\/share\/runtime\/sharedRuntime.cpp","additions":3,"deletions":0,"binary":false,"changes":3,"status":"modified"},{"patch":"@@ -64,0 +64,2 @@\n+  static RestoreBlob*        _restore_blob;\n+\n@@ -85,0 +87,1 @@\n+  static void           generate_restore_blob();\n@@ -249,0 +252,5 @@\n+  static address get_restore_stub() {\n+    precond(_restore_blob != nullptr);\n+    return _restore_blob->entry_point();\n+  }\n+\n","filename":"src\/hotspot\/share\/runtime\/sharedRuntime.hpp","additions":8,"deletions":0,"binary":false,"changes":8,"status":"modified"},{"patch":"@@ -313,0 +313,1 @@\n+  static address call_stub_return_address()                { return _call_stub_return_address; }\n","filename":"src\/hotspot\/share\/runtime\/stubRoutines.hpp","additions":1,"deletions":0,"binary":false,"changes":1,"status":"modified"},{"patch":"@@ -918,1 +918,1 @@\n-intptr_t ObjectSynchronizer::FastHashCode(Thread* current, oop obj) {\n+intptr_t ObjectSynchronizer::FastHashCode(Thread* current, oop obj, intptr_t suggested_hash) {\n@@ -934,1 +934,1 @@\n-      hash = get_next_hash(current, obj);  \/\/ get a new hash\n+      hash = suggested_hash != 0 ? suggested_hash : get_next_hash(current, obj);  \/\/ get a new hash\n@@ -1008,1 +1008,1 @@\n-      hash = get_next_hash(current, obj);  \/\/ get a new hash\n+      hash = suggested_hash != 0 ? suggested_hash : get_next_hash(current, obj);  \/\/ get a new hash\n","filename":"src\/hotspot\/share\/runtime\/synchronizer.cpp","additions":3,"deletions":3,"binary":false,"changes":6,"status":"modified"},{"patch":"@@ -165,1 +165,1 @@\n-  static intptr_t FastHashCode(Thread* current, oop obj);\n+  static intptr_t FastHashCode(Thread* current, oop obj, intptr_t suggested_hash = markWord::no_hash);\n","filename":"src\/hotspot\/share\/runtime\/synchronizer.hpp","additions":1,"deletions":1,"binary":false,"changes":2,"status":"modified"},{"patch":"@@ -326,0 +326,1 @@\n+  virtual bool is_Notification_thread() const        { return false; }\n","filename":"src\/hotspot\/share\/runtime\/thread.hpp","additions":1,"deletions":0,"binary":false,"changes":1,"status":"modified"},{"patch":"@@ -412,1 +412,1 @@\n-  if (Arguments::is_restore_option_set(args)) {\n+  if (Arguments::has_classic_restore_request(args)) {\n@@ -429,0 +429,1 @@\n+  \/\/ Perform classic CRaC restore if requested\n@@ -839,0 +840,8 @@\n+  \/\/ Perform portable CRaC restore if requested\n+  \/\/ TODO do this before any Java code is executed\n+  if (CRaCRestoreFrom != nullptr && crac::is_portable_mode()) {\n+    \/\/ TODO honor CRaCIgnoreRestoreIfUnavailable (will have to differentiate\n+    \/\/ between errors and exceptions)\n+    crac::restore_data(CHECK_JNI_ERR);\n+  }\n+\n","filename":"src\/hotspot\/share\/runtime\/threads.cpp","additions":10,"deletions":1,"binary":false,"changes":11,"status":"modified"},{"patch":"@@ -123,1 +123,1 @@\n-GrowableArray<MonitorInfo*>* javaVFrame::locked_monitors() {\n+GrowableArray<MonitorInfo*>* javaVFrame::locked_monitors() const {\n","filename":"src\/hotspot\/share\/runtime\/vframe.cpp","additions":1,"deletions":1,"binary":false,"changes":2,"status":"modified"},{"patch":"@@ -138,1 +138,1 @@\n-  GrowableArray<MonitorInfo*>* locked_monitors();\n+  GrowableArray<MonitorInfo*>* locked_monitors() const;\n@@ -313,0 +313,1 @@\n+  bool reached_first_entry_frame() const { return _frame.is_entry_frame() && _frame.entry_frame_is_first(); }\n","filename":"src\/hotspot\/share\/runtime\/vframe.hpp","additions":2,"deletions":1,"binary":false,"changes":3,"status":"modified"},{"patch":"@@ -218,0 +218,3 @@\n+  \/\/ thread->deopt_compiled_method() == nullptr when this is used for thread restoration instead of\n+  \/\/ deoptimization.\n+  \/\/\n@@ -221,2 +224,3 @@\n-  assert(thread->deopt_compiled_method() != nullptr, \"compiled method should be known\");\n-  guarantee(realloc_failure_exception || !(thread->deopt_compiled_method()->is_compiled_by_c2() &&\n+  guarantee(thread->deopt_compiled_method() == nullptr ||\n+            realloc_failure_exception ||\n+            !(thread->deopt_compiled_method()->is_compiled_by_c2() &&\n@@ -558,1 +562,1 @@\n-void vframeArray::unpack_to_stack(frame &unpack_frame, int exec_mode, int caller_actual_parameters) {\n+void vframeArray::unpack_to_stack(const frame &unpack_frame, int exec_mode, int caller_actual_parameters) {\n","filename":"src\/hotspot\/share\/runtime\/vframeArray.cpp","additions":7,"deletions":3,"binary":false,"changes":10,"status":"modified"},{"patch":"@@ -35,0 +35,1 @@\n+#include \"utilities\/sizes.hpp\"\n@@ -54,1 +55,1 @@\n-  private:\n+  protected:\n@@ -125,1 +126,1 @@\n- private:\n+ protected:\n@@ -196,1 +197,1 @@\n-  void unpack_to_stack(frame &unpack_frame, int exec_mode, int caller_actual_parameters);\n+  void unpack_to_stack(const frame &unpack_frame, int exec_mode, int caller_actual_parameters);\n","filename":"src\/hotspot\/share\/runtime\/vframeArray.hpp","additions":4,"deletions":3,"binary":false,"changes":7,"status":"modified"},{"patch":"@@ -30,0 +30,1 @@\n+#include \"classfile\/stringTable.hpp\"\n@@ -62,0 +63,2 @@\n+#include \"utilities\/bitCast.hpp\"\n+#include \"utilities\/hprofTag.hpp\"\n@@ -63,0 +66,1 @@\n+#include \"utilities\/methodKind.hpp\"\n@@ -329,50 +333,0 @@\n-\n-\/\/ HPROF tags\n-\n-enum hprofTag : u1 {\n-  \/\/ top-level records\n-  HPROF_UTF8                    = 0x01,\n-  HPROF_LOAD_CLASS              = 0x02,\n-  HPROF_UNLOAD_CLASS            = 0x03,\n-  HPROF_FRAME                   = 0x04,\n-  HPROF_TRACE                   = 0x05,\n-  HPROF_ALLOC_SITES             = 0x06,\n-  HPROF_HEAP_SUMMARY            = 0x07,\n-  HPROF_START_THREAD            = 0x0A,\n-  HPROF_END_THREAD              = 0x0B,\n-  HPROF_HEAP_DUMP               = 0x0C,\n-  HPROF_CPU_SAMPLES             = 0x0D,\n-  HPROF_CONTROL_SETTINGS        = 0x0E,\n-\n-  \/\/ 1.0.2 record types\n-  HPROF_HEAP_DUMP_SEGMENT       = 0x1C,\n-  HPROF_HEAP_DUMP_END           = 0x2C,\n-\n-  \/\/ field types\n-  HPROF_ARRAY_OBJECT            = 0x01,\n-  HPROF_NORMAL_OBJECT           = 0x02,\n-  HPROF_BOOLEAN                 = 0x04,\n-  HPROF_CHAR                    = 0x05,\n-  HPROF_FLOAT                   = 0x06,\n-  HPROF_DOUBLE                  = 0x07,\n-  HPROF_BYTE                    = 0x08,\n-  HPROF_SHORT                   = 0x09,\n-  HPROF_INT                     = 0x0A,\n-  HPROF_LONG                    = 0x0B,\n-\n-  \/\/ data-dump sub-records\n-  HPROF_GC_ROOT_UNKNOWN         = 0xFF,\n-  HPROF_GC_ROOT_JNI_GLOBAL      = 0x01,\n-  HPROF_GC_ROOT_JNI_LOCAL       = 0x02,\n-  HPROF_GC_ROOT_JAVA_FRAME      = 0x03,\n-  HPROF_GC_ROOT_NATIVE_STACK    = 0x04,\n-  HPROF_GC_ROOT_STICKY_CLASS    = 0x05,\n-  HPROF_GC_ROOT_THREAD_BLOCK    = 0x06,\n-  HPROF_GC_ROOT_MONITOR_USED    = 0x07,\n-  HPROF_GC_ROOT_THREAD_OBJ      = 0x08,\n-  HPROF_GC_CLASS_DUMP           = 0x20,\n-  HPROF_GC_INSTANCE_DUMP        = 0x21,\n-  HPROF_GC_OBJ_ARRAY_DUMP       = 0x22,\n-  HPROF_GC_PRIM_ARRAY_DUMP      = 0x23\n-};\n-\n@@ -735,1 +689,1 @@\n-  static u4 instance_size(Klass* k);\n+  static u4 instance_size(Klass* k, bool with_injected_fields);\n@@ -748,1 +702,1 @@\n-  static void dump_instance_fields(AbstractDumpWriter* writer, oop o);\n+  static void dump_instance_fields(AbstractDumpWriter* writer, oop o, bool with_injected);\n@@ -750,1 +704,1 @@\n-  static u2 get_instance_fields_count(InstanceKlass* ik);\n+  static u2 get_instance_fields_count(InstanceKlass* ik, bool with_injected);\n@@ -752,1 +706,1 @@\n-  static void dump_instance_field_descriptors(AbstractDumpWriter* writer, Klass* k);\n+  static void dump_instance_field_descriptors(AbstractDumpWriter* writer, Klass* k, bool with_injected);\n@@ -754,1 +708,1 @@\n-  static void dump_instance(AbstractDumpWriter* writer, oop o);\n+  static void dump_instance(AbstractDumpWriter* writer, oop o, bool extended);\n@@ -756,1 +710,1 @@\n-  static void dump_instance_class(AbstractDumpWriter* writer, Klass* k);\n+  static void dump_instance_class(AbstractDumpWriter* writer, Klass* k, bool with_injected_fields);\n@@ -761,1 +715,1 @@\n-  static void dump_object_array(AbstractDumpWriter* writer, objArrayOop array);\n+  static void dump_object_array(AbstractDumpWriter* writer, objArrayOop array, bool extended);\n@@ -763,1 +717,1 @@\n-  static void dump_prim_array(AbstractDumpWriter* writer, typeArrayOop array);\n+  static void dump_prim_array(AbstractDumpWriter* writer, typeArrayOop array, bool extended);\n@@ -838,7 +792,0 @@\n-template<typename T, typename F> T bit_cast(F from) { \/\/ replace with the real thing when we can use c++20\n-  T to;\n-  static_assert(sizeof(to) == sizeof(from), \"must be of the same size\");\n-  memcpy(&to, &from, sizeof(to));\n-  return to;\n-}\n-\n@@ -928,1 +875,1 @@\n-u4 DumperSupport::instance_size(Klass* k) {\n+u4 DumperSupport::instance_size(Klass* k, bool with_injected_fields) {\n@@ -932,1 +879,1 @@\n-  for (FieldStream fld(ik, false, false); !fld.eos(); fld.next()) {\n+  for (FieldStream fld(ik, false, false, !with_injected_fields); !fld.eos(); fld.next()) {\n@@ -937,0 +884,11 @@\n+\n+  if (with_injected_fields) {\n+    if (k == vmClasses::String_klass()) {\n+      \/\/ is_interned flag\n+      size++;\n+    } else if (k == vmClasses::ResolvedMethodName_klass()) {\n+      \/\/ Injected Method* identification\n+      size += 2 * sig2size(vmSymbols::intptr_signature()) \/*name and signature*\/ + 1 \/*kind*\/;\n+    }\n+  }\n+\n@@ -1009,1 +967,1 @@\n-void DumperSupport::dump_instance_fields(AbstractDumpWriter* writer, oop o) {\n+void DumperSupport::dump_instance_fields(AbstractDumpWriter* writer, oop o, bool with_injected) {\n@@ -1012,1 +970,1 @@\n-  for (FieldStream fld(ik, false, false); !fld.eos(); fld.next()) {\n+  for (FieldStream fld(ik, false, false, !with_injected); !fld.eos(); fld.next()) {\n@@ -1018,0 +976,22 @@\n+\n+  if (with_injected) {\n+    if (ik == vmClasses::String_klass()) {\n+      bool is_interned;\n+      {\n+        Thread* current_thread = Thread::current();\n+        ResourceMark rm(current_thread);\n+        HandleMark hm(current_thread);\n+        int len;\n+        \/\/ FIXME returns null if resource alloc fails: report error in such case\n+        const jchar* str = java_lang_String::as_unicode_string_or_null(o, len);\n+        is_interned = str != nullptr && StringTable::lookup(str, len) != nullptr;\n+      }\n+      writer->write_u1(static_cast<u1>(is_interned));\n+    } else if (ik == vmClasses::ResolvedMethodName_klass()) {\n+      \/\/ Injected Method* identification: name, signature, kind\n+      const Method* m = java_lang_invoke_ResolvedMethodName::vmtarget(o);\n+      writer->write_symbolID(m->name());\n+      writer->write_symbolID(m->signature());\n+      writer->write_u1(checked_cast<u1>(MethodKind::of_method(*m)));\n+    }\n+  }\n@@ -1021,1 +1001,1 @@\n-u2 DumperSupport::get_instance_fields_count(InstanceKlass* ik) {\n+u2 DumperSupport::get_instance_fields_count(InstanceKlass* ik, bool with_injected) {\n@@ -1024,1 +1004,1 @@\n-  for (FieldStream fldc(ik, true, true); !fldc.eos(); fldc.next()) {\n+  for (FieldStream fldc(ik, true, true, !with_injected); !fldc.eos(); fldc.next()) {\n@@ -1028,0 +1008,10 @@\n+  if (with_injected) {\n+    if (ik == vmClasses::String_klass()) {\n+      \/\/ is_interned flag\n+      field_count++;\n+    } else if (ik == vmClasses::ResolvedMethodName_klass()) {\n+      \/\/ Injected Method* identification: name, signature, kind\n+      field_count += 3;\n+    }\n+  }\n+\n@@ -1032,1 +1022,1 @@\n-void DumperSupport::dump_instance_field_descriptors(AbstractDumpWriter* writer, Klass* k) {\n+void DumperSupport::dump_instance_field_descriptors(AbstractDumpWriter* writer, Klass* k, bool with_injected) {\n@@ -1036,1 +1026,1 @@\n-  for (FieldStream fld(ik, true, true); !fld.eos(); fld.next()) {\n+  for (FieldStream fld(ik, true, true, !with_injected); !fld.eos(); fld.next()) {\n@@ -1044,0 +1034,19 @@\n+\n+  if (with_injected) {\n+    if (ik == vmClasses::String_klass()) {\n+      \/\/ is_interned flag\n+      writer->write_symbolID(vmSymbols::is_interned_name());\n+      writer->write_u1(type2tag(T_BOOLEAN));\n+    } else if (ik == vmClasses::ResolvedMethodName_klass()) {\n+      \/\/ Injected Method* identification: name, signature, kind\n+      \/\/ Method name\n+      writer->write_symbolID(vmSymbols::internal_name_name());      \/\/ name\n+      writer->write_u1(sig2tag(vmSymbols::intptr_signature()));     \/\/ type\n+      \/\/ Method signature\n+      writer->write_symbolID(vmSymbols::internal_signature_name()); \/\/ name\n+      writer->write_u1(sig2tag(vmSymbols::intptr_signature()));     \/\/ type\n+      \/\/ Method kind\n+      writer->write_symbolID(vmSymbols::internal_kind_name());      \/\/ name\n+      writer->write_u1(type2tag(T_BYTE));                           \/\/ type\n+    }\n+  }\n@@ -1047,1 +1056,1 @@\n-void DumperSupport::dump_instance(AbstractDumpWriter* writer, oop o) {\n+void DumperSupport::dump_instance(AbstractDumpWriter* writer, oop o, bool extended) {\n@@ -1049,1 +1058,1 @@\n-  u4 is = instance_size(ik);\n+  u4 is = instance_size(ik, extended);\n@@ -1054,1 +1063,7 @@\n-  writer->write_u4(STACK_TRACE_ID);\n+\n+  if (!extended) {\n+    writer->write_u4(STACK_TRACE_ID);\n+  } else {\n+    \/\/ Note: this does not adhere to HPROF spec\n+    writer->write_u4(checked_cast<jint>(o->read_identity_hash()));\n+  }\n@@ -1063,1 +1078,1 @@\n-  dump_instance_fields(writer, o);\n+  dump_instance_fields(writer, o, extended);\n@@ -1069,1 +1084,1 @@\n-void DumperSupport::dump_instance_class(AbstractDumpWriter* writer, Klass* k) {\n+void DumperSupport::dump_instance_class(AbstractDumpWriter* writer, Klass* k, bool with_injected_fields) {\n@@ -1081,1 +1096,1 @@\n-  u2 instance_fields_count = get_instance_fields_count(ik);\n+  u2 instance_fields_count = get_instance_fields_count(ik, with_injected_fields);\n@@ -1108,1 +1123,1 @@\n-  writer->write_u4(DumperSupport::instance_size(ik));\n+  writer->write_u4(DumperSupport::instance_size(ik, with_injected_fields));\n@@ -1119,1 +1134,1 @@\n-  dump_instance_field_descriptors(writer, ik);\n+  dump_instance_field_descriptors(writer, ik, with_injected_fields);\n@@ -1189,1 +1204,1 @@\n-void DumperSupport::dump_object_array(AbstractDumpWriter* writer, objArrayOop array) {\n+void DumperSupport::dump_object_array(AbstractDumpWriter* writer, objArrayOop array, bool extended) {\n@@ -1197,1 +1212,8 @@\n-  writer->write_u4(STACK_TRACE_ID);\n+\n+  if (!extended) {\n+    writer->write_u4(STACK_TRACE_ID);\n+  } else {\n+    \/\/ Note: this does not adhere to HPROF spec\n+    writer->write_u4(checked_cast<jint>(array->read_identity_hash()));\n+  }\n+\n@@ -1223,1 +1245,1 @@\n-void DumperSupport::dump_prim_array(AbstractDumpWriter* writer, typeArrayOop array) {\n+void DumperSupport::dump_prim_array(AbstractDumpWriter* writer, typeArrayOop array, bool extended) {\n@@ -1235,1 +1257,8 @@\n-  writer->write_u4(STACK_TRACE_ID);\n+\n+  if (!extended) {\n+    writer->write_u4(STACK_TRACE_ID);\n+  } else {\n+    \/\/ Note: this does not adhere to HPROF spec\n+    writer->write_u4(checked_cast<jint>(array->read_identity_hash()));\n+  }\n+\n@@ -1356,0 +1385,3 @@\n+  char* s = sym->as_utf8();\n+  DumperSupport::write_header(writer(), HPROF_UTF8, oopSize + len);\n+  writer()->write_symbolID(sym);\n@@ -1357,3 +1389,0 @@\n-    char* s = sym->as_utf8();\n-    DumperSupport::write_header(writer(), HPROF_UTF8, oopSize + len);\n-    writer()->write_symbolID(sym);\n@@ -1364,0 +1393,23 @@\n+\/\/ Support class used to generate HPROF_GC_CLASS_DUMP records from classes.\n+\n+class ClassDumper : public LockedClassesDo {\n+ private:\n+  AbstractDumpWriter* _writer;\n+  bool _with_injected_fields;\n+  AbstractDumpWriter* writer() const                { return _writer; }\n+ public:\n+  ClassDumper(AbstractDumpWriter* writer, bool with_injected_fields) {\n+    _writer = writer;\n+    _with_injected_fields = with_injected_fields;\n+  }\n+  void do_klass(Klass* k) override;\n+};\n+\n+void ClassDumper::do_klass(Klass* k) {\n+  if (k->is_instance_klass()) {\n+    DumperSupport::dump_instance_class(writer(), k, _with_injected_fields);\n+  } else {\n+    DumperSupport::dump_array_class(writer(), k);\n+  }\n+}\n+\n@@ -1455,0 +1507,1 @@\n+  bool _extended;\n@@ -1458,1 +1511,1 @@\n-  HeapObjectDumper(AbstractDumpWriter* writer) {\n+  HeapObjectDumper(AbstractDumpWriter* writer, bool extended) {\n@@ -1460,0 +1513,1 @@\n+    _extended = extended;\n@@ -1468,4 +1522,2 @@\n-  if (o->klass() == vmClasses::Class_klass()) {\n-    if (!java_lang_Class::is_primitive(o)) {\n-      return;\n-    }\n+  if (!_extended && o->klass() == vmClasses::Class_klass() && !java_lang_Class::is_primitive(o)) {\n+    return;\n@@ -1481,1 +1533,1 @@\n-    DumperSupport::dump_instance(writer(), o);\n+    DumperSupport::dump_instance(writer(), o, _extended);\n@@ -1484,1 +1536,1 @@\n-    DumperSupport::dump_object_array(writer(), objArrayOop(o));\n+    DumperSupport::dump_object_array(writer(), objArrayOop(o), _extended);\n@@ -1487,1 +1539,1 @@\n-    DumperSupport::dump_prim_array(writer(), typeArrayOop(o));\n+    DumperSupport::dump_prim_array(writer(), typeArrayOop(o), _extended);\n@@ -1556,1 +1608,3 @@\n-  assert(!SafepointSynchronize::is_at_safepoint(), \"merging happens outside safepoint\");\n+  \/\/ CRaC's portable mode dumps heap entirely on a safepoint making this fail\n+  \/\/ assert(!SafepointSynchronize::is_at_safepoint(), \"merging happens outside safepoint\");\n+\n@@ -1585,1 +1639,3 @@\n-  assert(!SafepointSynchronize::is_at_safepoint(), \"merging happens outside safepoint\");\n+  \/\/ CRaC's portable mode dumps heap entirely on a safepoint making this fail\n+  \/\/ assert(!SafepointSynchronize::is_at_safepoint(), \"merging happens outside safepoint\");\n+\n@@ -1632,0 +1688,1 @@\n+  bool                    _extended;\n@@ -1668,3 +1725,0 @@\n-  \/\/ writes a HPROF_GC_CLASS_DUMP record for the given class\n-  static void do_class_dump(Klass* k);\n-\n@@ -1683,1 +1737,1 @@\n-  VM_HeapDumper(DumpWriter* writer, bool gc_before_heap_dump, bool oome, uint num_dump_threads) :\n+  VM_HeapDumper(DumpWriter* writer, bool gc_before_heap_dump, bool extended, bool oome, uint num_dump_threads) :\n@@ -1691,0 +1745,1 @@\n+    _extended = extended;\n@@ -1776,9 +1831,0 @@\n-\/\/ writes a HPROF_GC_CLASS_DUMP record for the given class\n-void VM_HeapDumper::do_class_dump(Klass* k) {\n-  if (k->is_instance_klass()) {\n-    DumperSupport::dump_instance_class(writer(), k);\n-  } else {\n-    DumperSupport::dump_array_class(writer(), k);\n-  }\n-}\n-\n@@ -2051,2 +2097,2 @@\n-      LockedClassesDo locked_dump_class(&do_class_dump);\n-      ClassLoaderDataGraph::classes_do(&locked_dump_class);\n+      ClassDumper class_dumper(writer(), _extended);\n+      ClassLoaderDataGraph::classes_do(&class_dumper);\n@@ -2082,1 +2128,1 @@\n-    HeapObjectDumper obj_dumper(writer());\n+    HeapObjectDumper obj_dumper(writer(), _extended);\n@@ -2094,1 +2140,1 @@\n-      HeapObjectDumper obj_dumper(local_writer);\n+      HeapObjectDumper obj_dumper(local_writer, _extended);\n@@ -2203,1 +2249,1 @@\n-  VM_HeapDumper dumper(&writer, _gc_before_heap_dump, _oome, num_dump_threads);\n+  VM_HeapDumper dumper(&writer, _gc_before_heap_dump, _extended, _oome, num_dump_threads);\n@@ -2382,0 +2428,1 @@\n+                    false \/* no injected fields *\/,\n","filename":"src\/hotspot\/share\/services\/heapDumper.cpp","additions":163,"deletions":116,"binary":false,"changes":279,"status":"modified"},{"patch":"@@ -39,0 +39,1 @@\n+  bool _extended; \/\/ Includes j.l.Class objects of reference types and injected fields\n@@ -42,2 +43,3 @@\n-  HeapDumper(bool gc_before_heap_dump, bool oome) :\n-    _error(nullptr), _gc_before_heap_dump(gc_before_heap_dump), _oome(oome) { }\n+  HeapDumper(bool gc_before_heap_dump, bool extended, bool oome) :\n+    _error(nullptr), _gc_before_heap_dump(gc_before_heap_dump),\n+    _extended(extended), _oome(oome) { }\n@@ -55,2 +57,2 @@\n-  HeapDumper(bool gc_before_heap_dump) :\n-    _error(nullptr), _gc_before_heap_dump(gc_before_heap_dump), _oome(false) { }\n+  HeapDumper(bool gc_before_heap_dump, bool with_injected_fields = false) :\n+    HeapDumper(gc_before_heap_dump, with_injected_fields, false) {}\n","filename":"src\/hotspot\/share\/services\/heapDumper.hpp","additions":6,"deletions":4,"binary":false,"changes":10,"status":"modified"},{"patch":"@@ -99,0 +99,1 @@\n+  friend class CracInstanceClassDumpParser;\n","filename":"src\/hotspot\/share\/utilities\/accessFlags.hpp","additions":1,"deletions":0,"binary":false,"changes":1,"status":"modified"},{"patch":"@@ -0,0 +1,22 @@\n+#include \"precompiled.hpp\"\n+#include \"runtime\/atomic.hpp\"\n+#include \"utilities\/barrier.hpp\"\n+\n+void Barrier::arrive() {\n+  \/\/ Increment the number of threads arrived\n+  Atomic::inc(&_num_threads_ready);\n+\n+  assert(_num_threads_ready <= _num_threads_total,\n+         \"too many threads arrived: \" UINT32_FORMAT \" > \" UINT32_FORMAT,\n+         _num_threads_ready, _num_threads_total);\n+\n+  if (_num_threads_ready == _num_threads_total) {\n+    \/\/ All threads have arrived, wake up the waiting ones.\n+    \/\/ Note: this code can be called by multiple threads, not only the last one,\n+    \/\/ so the semaphore counter can reach up to num_threads_total^2.\n+    _sem.signal(_num_threads_total);\n+  } else {\n+    \/\/ Wait for more threads to arrive\n+    _sem.wait();\n+  }\n+}\n","filename":"src\/hotspot\/share\/utilities\/barrier.cpp","additions":22,"deletions":0,"binary":false,"changes":22,"status":"added"},{"patch":"@@ -0,0 +1,27 @@\n+#ifndef SHARE_UTILITIES_BARRIER_HPP\n+#define SHARE_UTILITIES_BARRIER_HPP\n+\n+#include \"memory\/allocation.hpp\"\n+#include \"runtime\/semaphore.hpp\"\n+#include \"utilities\/globalDefinitions.hpp\"\n+\n+\/\/ Allows an arbitrary amount of threads to wait for each other before\n+\/\/ proceeding past the barrier.\n+\/\/\n+\/\/ The barrier is not reusable: once the specified number of threads have\n+\/\/ arrived it should not be used anymore.\n+\/\/\n+\/\/ Before deleting the barrier make sure all threads have left its methods.\n+class Barrier : public CHeapObj<mtInternal> {\n+ public:\n+  explicit Barrier(uint num_threads) : _num_threads_total(num_threads) {};\n+\n+  void arrive();\n+\n+ private:\n+  const uint _num_threads_total;\n+  volatile uint _num_threads_ready = 0;\n+  Semaphore _sem{0};\n+};\n+\n+#endif \/\/ SHARE_UTILITIES_BARRIER_HPP\n","filename":"src\/hotspot\/share\/utilities\/barrier.hpp","additions":27,"deletions":0,"binary":false,"changes":27,"status":"added"},{"patch":"@@ -0,0 +1,120 @@\n+#include \"precompiled.hpp\"\n+#include \"logging\/log.hpp\"\n+#include \"runtime\/os.hpp\"\n+#include \"utilities\/basicTypeReader.hpp\"\n+#include \"utilities\/bitCast.hpp\"\n+\n+bool BasicTypeReader::read(jfloat *out) {\n+  u4 tmp;\n+  if (!read(&tmp)) {\n+    return false;\n+  }\n+  *out = bit_cast<jfloat>(tmp);\n+  return true;\n+}\n+\n+bool BasicTypeReader::read(jdouble *out) {\n+  u8 tmp;\n+  if (!read(&tmp)) {\n+    return false;\n+  }\n+  *out = bit_cast<jdouble>(tmp);\n+  return true;\n+}\n+\n+bool BasicTypeReader::read_uint(u8 *out, size_t size) {\n+  precond(size == sizeof(u1) || size == sizeof(u2) || size == sizeof(u4) || size == sizeof(u8));\n+  switch (size) {\n+    case sizeof(u1): {\n+      u1 id;\n+      if (!read(&id)) {\n+        return false;\n+      }\n+      *out = id;\n+      return true;\n+    }\n+    case sizeof(u2): {\n+      u2 id;\n+      if (!read(&id)) {\n+        return false;\n+      }\n+      *out = id;\n+      return true;\n+    }\n+    case sizeof(u4): {\n+      u4 id;\n+      if (!read(&id)) {\n+        return false;\n+      }\n+      *out = id;\n+      return true;\n+    }\n+    case sizeof(u8): {\n+      u8 id;\n+      if (!read(&id)) {\n+        return false;\n+      }\n+      *out = id;\n+      return true;\n+    }\n+    default:\n+      ShouldNotReachHere();\n+      return false;\n+  }\n+}\n+\n+bool FileBasicTypeReader::open(const char *path) {\n+  assert(path != nullptr, \"cannot read from null path\");\n+\n+  close();\n+  errno = 0; \/\/ If close() errored, a warning has already been issued\n+\n+  FILE *file = os::fopen(path, \"rb\");\n+  if (file == nullptr) {\n+    guarantee(errno != 0, \"fopen should set errno on error\");\n+    return false;\n+  }\n+\n+  _file = file;\n+  return true;\n+}\n+\n+void FileBasicTypeReader::close() {\n+  if (_file == nullptr) {\n+    return;\n+  }\n+  if (log_is_enabled(Warning, data)) {\n+    int fd = os::get_fileno(_file);\n+    errno = -1; \/\/ To get \"Unknown error\" from os::strerror() if fclose won't set errno\n+    if (fclose(_file) != 0) {\n+      if (fd != -1) {\n+        log_warning(data)(\"Failed to close file with FD %i after reading: %s\", fd, os::strerror(errno));\n+      } else {\n+        log_warning(data)(\"Failed to close a file after reading: %s\", os::strerror(errno));\n+      }\n+    }\n+  } else {\n+    fclose(_file);\n+  }\n+}\n+\n+bool FileBasicTypeReader::skip(size_t size) {\n+    precond(_file != nullptr);\n+    while (size > LONG_MAX) {\n+      if (fseek(_file, LONG_MAX, SEEK_CUR) != 0) {\n+        return false;\n+      }\n+      size -= LONG_MAX;\n+    }\n+    return fseek(_file, checked_cast<long>(size), SEEK_CUR) == 0;\n+  }\n+\n+size_t FileBasicTypeReader::pos() const {\n+  precond(_file != nullptr);\n+  long pos = ftell(_file);\n+  if (pos < 0) {\n+    log_warning(data)(\"Failed to get position in a file after reading: %s\", os::strerror(errno));\n+    pos = 0;\n+  }\n+  return pos;\n+}\n","filename":"src\/hotspot\/share\/utilities\/basicTypeReader.cpp","additions":120,"deletions":0,"binary":false,"changes":120,"status":"added"},{"patch":"@@ -0,0 +1,78 @@\n+#ifndef SHARE_UTILITIES_BASICTYPEREADER_HPP\n+#define SHARE_UTILITIES_BASICTYPEREADER_HPP\n+\n+#include \"metaprogramming\/enableIf.hpp\"\n+#include \"utilities\/bytes.hpp\"\n+#include \"utilities\/globalDefinitions.hpp\"\n+\n+\/\/ Abstarct class for reading Java-ordered (i.e. in big-endian) bytes as basic\n+\/\/ types.\n+class BasicTypeReader {\n+ public:\n+  \/\/ Reads size bytes into buf. Returns true on success.\n+  virtual bool read_raw(void *buf, size_t size) = 0;\n+\n+  \/\/ Reads integral types (and boolean as a byte).\n+  template <class T, ENABLE_IF(std::is_integral<T>::value && (sizeof(T) == sizeof(u1) ||\n+                                                              sizeof(T) == sizeof(u2) ||\n+                                                              sizeof(T) == sizeof(u4) ||\n+                                                              sizeof(T) == sizeof(u8)))>\n+  bool read(T *out) {\n+    if (!read_raw(out, sizeof(T))) {\n+      return false;\n+    }\n+    switch (sizeof(T)) {\n+      case sizeof(u1): break;\n+      case sizeof(u2): *out = Bytes::get_Java_u2(static_cast<address>(static_cast<void *>(out))); break;\n+      case sizeof(u4): *out = Bytes::get_Java_u4(static_cast<address>(static_cast<void *>(out))); break;\n+      case sizeof(u8): *out = Bytes::get_Java_u8(static_cast<address>(static_cast<void *>(out))); break;\n+    }\n+    return true;\n+  }\n+\n+  bool read(jfloat *out);\n+  bool read(jdouble *out);\n+\n+  \/\/ Reads either u1, u2, u4, or u8 into out based on the provided size.\n+  bool read_uint(u8 *out, size_t size);\n+\n+  \/\/ Skips size bytes. Returns true on success.\n+  virtual bool skip(size_t size) = 0;\n+  \/\/ Returns the current reading position.\n+  virtual size_t pos() const = 0;\n+  \/\/ Tells whether the end of stream has been reached.\n+  virtual bool eos() const = 0;\n+\n+ protected:\n+  ~BasicTypeReader() = default;\n+};\n+\n+\/\/ Reads from a binary file.\n+class FileBasicTypeReader : public BasicTypeReader {\n+ public:\n+  ~FileBasicTypeReader() { close(); }\n+\n+  bool open(const char *path);\n+\n+  bool read_raw(void *buf, size_t size) override {\n+    precond(_file != nullptr);\n+    precond(buf != nullptr || size == 0);\n+    return size == 0 || fread(buf, size, 1, _file) == 1;\n+  }\n+\n+  bool skip(size_t size) override;\n+\n+  size_t pos() const override;\n+\n+  bool eos() const override {\n+    precond(_file != nullptr);\n+    return feof(_file) != 0;\n+  }\n+\n+ private:\n+  FILE *_file = nullptr;\n+\n+  void close();\n+};\n+\n+#endif \/\/ SHARE_UTILITIES_BASICTYPEREADER_HPP\n","filename":"src\/hotspot\/share\/utilities\/basicTypeReader.hpp","additions":78,"deletions":0,"binary":false,"changes":78,"status":"added"},{"patch":"@@ -0,0 +1,45 @@\n+#include \"precompiled.hpp\"\n+#include \"logging\/log.hpp\"\n+#include \"runtime\/os.hpp\"\n+#include \"utilities\/basicTypeWriter.hpp\"\n+\n+bool FileBasicTypeWriter::open(const char *path, bool overwrite) {\n+  assert(path != nullptr, \"cannot write to null path\");\n+\n+  close();\n+  errno = 0; \/\/ If close() errored, a warning has already been issued\n+\n+  if (!overwrite && os::file_exists(path)) {\n+    errno = EEXIST;\n+    return false;\n+  }\n+  errno = 0; \/\/ os::file_exists() may use functions that set errno\n+\n+  FILE *file = os::fopen(path, \"wb\");\n+  if (file == nullptr) {\n+    guarantee(errno != 0, \"fopen should set errno on error\");\n+    return false;\n+  }\n+\n+  _file = file;\n+  return true;\n+}\n+\n+void FileBasicTypeWriter::close() {\n+  if (_file == nullptr) {\n+    return;\n+  }\n+  if (log_is_enabled(Warning, data)) {\n+    int fd = os::get_fileno(_file);\n+    errno = -1; \/\/ To get \"Unknown error\" from os::strerror() if fclose won't set errno\n+    if (fclose(_file) != 0) {\n+      if (fd != -1) {\n+        log_warning(data)(\"Failed to close file with FD %i after writing: %s\", fd, os::strerror(errno));\n+      } else {\n+        log_warning(data)(\"Failed to close a file after writing: %s\", os::strerror(errno));\n+      }\n+    }\n+  } else {\n+    fclose(_file);\n+  }\n+}\n","filename":"src\/hotspot\/share\/utilities\/basicTypeWriter.cpp","additions":45,"deletions":0,"binary":false,"changes":45,"status":"added"},{"patch":"@@ -0,0 +1,56 @@\n+#ifndef SHARE_UTILITIES_BASICTYPEWRITER_HPP\n+#define SHARE_UTILITIES_BASICTYPEWRITER_HPP\n+\n+#include \"metaprogramming\/enableIf.hpp\"\n+#include \"utilities\/bitCast.hpp\"\n+#include \"utilities\/bytes.hpp\"\n+#include \"utilities\/debug.hpp\"\n+#include <type_traits>\n+\n+class BasicTypeWriter {\n+ public:\n+  \/\/ Writes size bytes into buf. Returns true on success.\n+  virtual bool write_raw(const void *buf, size_t size) = 0;\n+\n+  \/\/ Writes integral types (and boolean as a byte).\n+  template <class T, ENABLE_IF(std::is_integral<T>::value &&\n+                               (sizeof(T) == sizeof(u1) || sizeof(T) == sizeof(u2) ||\n+                                sizeof(T) == sizeof(u4) || sizeof(T) == sizeof(u8)))>\n+  bool write(T value) {\n+    T tmp;\n+    switch (sizeof(value)) {\n+      case sizeof(u1): tmp = value;                                                break;\n+      case sizeof(u2): Bytes::put_Java_u2(reinterpret_cast<address>(&tmp), value); break;\n+      case sizeof(u4): Bytes::put_Java_u4(reinterpret_cast<address>(&tmp), value); break;\n+      case sizeof(u8): Bytes::put_Java_u8(reinterpret_cast<address>(&tmp), value); break;\n+    }\n+    return write_raw(&tmp, sizeof(tmp));\n+  }\n+\n+  bool write(jfloat value)  { return write(bit_cast<u4>(value)); }\n+  bool write(jdouble value) { return write(bit_cast<u8>(value)); }\n+\n+ protected:\n+  ~BasicTypeWriter() = default;\n+};\n+\n+\/\/ Writes into a binary file.\n+class FileBasicTypeWriter : public BasicTypeWriter {\n+ public:\n+  ~FileBasicTypeWriter() { close(); };\n+\n+  bool open(const char *path, bool overwrite = false);\n+\n+  bool write_raw(const void *buf, size_t size) override {\n+    precond(_file != nullptr);\n+    precond(buf != nullptr || size == 0);\n+    return size == 0 || fwrite(buf, size, 1, _file) == 1;\n+  }\n+\n+ private:\n+  FILE *_file = nullptr;\n+\n+  void close();\n+};\n+\n+#endif \/\/ SHARE_UTILITIES_BASICTYPEWRITER_HPP\n","filename":"src\/hotspot\/share\/utilities\/basicTypeWriter.hpp","additions":56,"deletions":0,"binary":false,"changes":56,"status":"added"},{"patch":"@@ -0,0 +1,40 @@\n+\/*\n+ * Copyright (c) 2005, 2023, Oracle and\/or its affiliates. All rights reserved.\n+ * Copyright (c) 2023, Alibaba Group Holding Limited. All rights reserved.\n+ * DO NOT ALTER OR REMOVE COPYRIGHT NOTICES OR THIS FILE HEADER.\n+ *\n+ * This code is free software; you can redistribute it and\/or modify it\n+ * under the terms of the GNU General Public License version 2 only, as\n+ * published by the Free Software Foundation.\n+ *\n+ * This code is distributed in the hope that it will be useful, but WITHOUT\n+ * ANY WARRANTY; without even the implied warranty of MERCHANTABILITY or\n+ * FITNESS FOR A PARTICULAR PURPOSE.  See the GNU General Public License\n+ * version 2 for more details (a copy is included in the LICENSE file that\n+ * accompanied this code).\n+ *\n+ * You should have received a copy of the GNU General Public License version\n+ * 2 along with this work; if not, write to the Free Software Foundation,\n+ * Inc., 51 Franklin St, Fifth Floor, Boston, MA 02110-1301 USA.\n+ *\n+ * Please contact Oracle, 500 Oracle Parkway, Redwood Shores, CA 94065 USA\n+ * or visit www.oracle.com if you need additional information or have any\n+ * questions.\n+ *\n+ *\/\n+\n+#ifndef SHARE_UTILITIES_BITCAST_HPP\n+#define SHARE_UTILITIES_BITCAST_HPP\n+\n+#include \"utilities\/debug.hpp\"\n+\n+\/\/ Replace with the real thing when we can use C++20.\n+template <typename T, typename F>\n+inline T bit_cast(F from) {\n+  T to;\n+  STATIC_ASSERT(sizeof(to) == sizeof(from));\n+  memcpy(&to, &from, sizeof(to));\n+  return to;\n+}\n+\n+#endif  \/\/ SHARE_UTILITIES_BITCAST_HPP\n","filename":"src\/hotspot\/share\/utilities\/bitCast.hpp","additions":40,"deletions":0,"binary":false,"changes":40,"status":"added"},{"patch":"@@ -103,0 +103,16 @@\n+jbyte constantTag::external_value() const {\n+  if (_tag < JVM_CONSTANT_InternalMin) {\n+    return _tag;\n+  }\n+  switch (_tag) {\n+    case JVM_CONSTANT_UnresolvedClass:\n+    case JVM_CONSTANT_UnresolvedClassInError:\n+    case JVM_CONSTANT_ClassIndex:          return JVM_CONSTANT_Class;\n+    case JVM_CONSTANT_StringIndex:         return JVM_CONSTANT_String;\n+    case JVM_CONSTANT_MethodHandleInError: return JVM_CONSTANT_MethodHandle;\n+    case JVM_CONSTANT_MethodTypeInError:   return JVM_CONSTANT_MethodType;\n+    case JVM_CONSTANT_DynamicInError:      return JVM_CONSTANT_Dynamic;\n+    default: ShouldNotReachHere();         return JVM_CONSTANT_Invalid;\n+  }\n+}\n+\n","filename":"src\/hotspot\/share\/utilities\/constantTag.cpp","additions":16,"deletions":0,"binary":false,"changes":16,"status":"modified"},{"patch":"@@ -154,0 +154,1 @@\n+  jbyte external_value() const;\n","filename":"src\/hotspot\/share\/utilities\/constantTag.hpp","additions":1,"deletions":0,"binary":false,"changes":1,"status":"modified"},{"patch":"@@ -0,0 +1,63 @@\n+#ifndef SHARE_UTILITIES_EXTENDABLE_ARRAY_HPP\n+#define SHARE_UTILITIES_EXTENDABLE_ARRAY_HPP\n+\n+#include \"memory\/allocation.hpp\"\n+#include \"runtime\/os.hpp\"\n+\n+\/\/ C-heap array which can be dynamically extended.\n+\/\/\n+\/\/ Differs from a heap-allocated GrowableArray in that:\n+\/\/ 1. Size == capacity.\n+\/\/ 2. Accessible elements are allowed to be left uninitialized.\n+\/\/ 3. Index type can be specified via a template parameter.\n+template <class ElemT, class SizeT = size_t, MEMFLAGS F = mtInternal>\n+class ExtendableArray : public CHeapObj<F> {\n+ private:\n+  SizeT _size;\n+  ElemT *_mem = nullptr;\n+\n+ public:\n+  explicit ExtendableArray(SizeT size) : _size(size) {\n+    guarantee(size >= 0, \"Size cannot be negative\");\n+    if (size == 0) {\n+      return;\n+    }\n+    _mem = static_cast<ElemT *>(os::malloc(size * sizeof(ElemT), mtInternal));\n+    if (mem() == nullptr) {\n+      vm_exit_out_of_memory(size * sizeof(ElemT), OOM_MALLOC_ERROR, \"extendable array construction\");\n+    }\n+  }\n+\n+  ExtendableArray() : ExtendableArray(0){};\n+  ~ExtendableArray() { os::free(mem()); }\n+  NONCOPYABLE(ExtendableArray);\n+\n+  SizeT size() const { return _size; }\n+  ElemT *mem() const { return _mem; }\n+\n+  void extend(SizeT new_size) {\n+    precond(new_size >= size());\n+    if (new_size == size()) {\n+      return;\n+    }\n+    _mem = static_cast<ElemT *>(os::realloc(mem(), new_size * sizeof(ElemT), mtInternal));\n+    if (mem() == nullptr) {\n+      vm_exit_out_of_memory(new_size * sizeof(ElemT), OOM_MALLOC_ERROR, \"extendable array extension\");\n+    }\n+    _size = new_size;\n+  }\n+\n+  ElemT &operator[](SizeT index) {\n+    precond(0 <= index && index < _size);\n+    precond(_mem != nullptr);\n+    return _mem[index];\n+  }\n+\n+  const ElemT &operator[](SizeT index) const {\n+    precond(0 <= index && index < _size);\n+    precond(_mem != nullptr);\n+    return _mem[index];\n+  }\n+};\n+\n+#endif \/\/ SHARE_UTILITIES_EXTENDABLE_ARRAY_HPP\n","filename":"src\/hotspot\/share\/utilities\/extendableArray.hpp","additions":63,"deletions":0,"binary":false,"changes":63,"status":"added"},{"patch":"@@ -1026,0 +1026,17 @@\n+constexpr const char* thread_state_name(JavaThreadState state) {\n+  switch (state) {\n+    case _thread_uninitialized:     return \"_thread_uninitialized\";\n+    case _thread_new:               return \"_thread_new\";\n+    case _thread_new_trans:         return \"_thread_new_trans\";\n+    case _thread_in_native:         return \"_thread_in_native\";\n+    case _thread_in_native_trans:   return \"_thread_in_native_trans\";\n+    case _thread_in_vm:             return \"_thread_in_vm\";\n+    case _thread_in_vm_trans:       return \"_thread_in_vm_trans\";\n+    case _thread_in_Java:           return \"_thread_in_Java\";\n+    case _thread_in_Java_trans:     return \"_thread_in_Java_trans\";\n+    case _thread_blocked:           return \"_thread_blocked\";\n+    case _thread_blocked_trans:     return \"_thread_blocked_trans\";\n+    default:                        return \"unknown thread state\";\n+  }\n+}\n+\n","filename":"src\/hotspot\/share\/utilities\/globalDefinitions.hpp","additions":17,"deletions":0,"binary":false,"changes":17,"status":"modified"},{"patch":"@@ -0,0 +1,484 @@\n+#include \"precompiled.hpp\"\n+#include \"classfile\/javaClasses.hpp\"\n+#include \"classfile\/vmSymbols.hpp\"\n+#include \"oops\/symbol.hpp\"\n+#include \"utilities\/bitCast.hpp\"\n+#include \"utilities\/debug.hpp\"\n+#include \"utilities\/globalDefinitions.hpp\"\n+#include \"utilities\/heapDumpClasses.hpp\"\n+#include \"utilities\/heapDumpParser.hpp\"\n+#include \"utilities\/macros.hpp\"\n+\n+static bool symbol_equals(const Symbol *actual, const Symbol *expected) {\n+  return actual == expected;\n+}\n+static bool symbol_equals(const Symbol *actual, const char *expected) {\n+  return actual->equals(expected);\n+}\n+\n+#define DEFINE_FIND_CLASS_DUMP_FROM_SUBCLASS_DUMP(klass)                                                             \\\n+  static const HeapDump::ClassDump *find_##klass##_dump(const ParsedHeapDump &heap_dump, HeapDump::ID subclass_id) { \\\n+    const HeapDump::ClassDump *class_dump = &heap_dump.get_class_dump(subclass_id);                                  \\\n+    \/* Assuming there is no inheritance circularity or this will be an endless loop *\/                               \\\n+    while (class_dump->class_loader_id != HeapDump::NULL_ID ||                                                       \\\n+           heap_dump.get_class_name(class_dump->id) != vmSymbols::klass()) {                                         \\\n+      if (class_dump->super_id == HeapDump::NULL_ID) {                                                               \\\n+        return nullptr;                                                                                              \\\n+      }                                                                                                              \\\n+      class_dump = &heap_dump.get_class_dump(class_dump->super_id);                                                  \\\n+    }                                                                                                                \\\n+    return class_dump;                                                                                               \\\n+  }\n+\n+#define NO_DUMP_FIELDS_DO(...)\n+\n+STATIC_ASSERT((std::is_same<u4, juint>()));\n+#define DEFINE_OFFSET_FROM_START_LOCAL(klass, name, ...) u4 name##_offset_from_start = max_juint;\n+\n+#define CREATE_FIELD_OFFSET_CASE(klass, name, name_sym, basic_type, ...)                                                       \\\n+  if (symbol_equals(field_name, name_sym)) {                                                                                   \\\n+    guarantee(name##_offset_from_start == max_juint, \"non-static field %s::%s dumped multiple times in \" HDID_FORMAT,          \\\n+              vmSymbols::klass()->as_klass_external_name(), field_name->as_C_string(), klass##_dump.id);                       \\\n+    guarantee(field_type == (basic_type), \"illegal type of non-static field %s::%s (ID \" HDID_FORMAT \"): expected %s, got %s\", \\\n+              vmSymbols::klass()->as_klass_external_name(), field_name->as_C_string(), klass##_dump.id,                        \\\n+              type2name(basic_type), type2name(field_type));                                                                   \\\n+    name##_offset_from_start = total_offset_from_start;                                                                        \\\n+  } else\n+\n+#define CREATE_PTR_FIELD_OFFSET_CASE(klass, name, name_sym, ...)                                                           \\\n+  if (symbol_equals(field_name, name_sym)) {                                                                               \\\n+    guarantee(name##_offset_from_start == max_juint, \"non-static field %s::%s dumped multiple times in \" HDID_FORMAT,      \\\n+              vmSymbols::klass()->as_klass_external_name(), field_name->as_C_string(), klass##_dump.id);                   \\\n+    const u4 field_size = is_java_primitive(field_type) ? type2aelembytes(field_type) : heap_dump.id_size;                 \\\n+    name##_offset_from_start = total_offset_from_start;                                                                    \\\n+    if (_ptr_type == T_ILLEGAL) {                                                                                          \\\n+      guarantee(field_type == T_INT || field_type == T_LONG,                                                               \\\n+                \"illegal type of non-static raw pointer field %s::%s (ID \" HDID_FORMAT \"): expected int or long, got %s\",  \\\n+                vmSymbols::klass()->as_klass_external_name(), field_name->as_C_string(), klass##_dump.id,                  \\\n+                type2name(field_type));                                                                                    \\\n+      _ptr_type = field_type;                                                                                              \\\n+    } else {                                                                                                               \\\n+      precond(_ptr_type == T_INT || _ptr_type == T_LONG);                                                                  \\\n+      guarantee(field_type == _ptr_type, \"%s object \" HDID_FORMAT \" has non-static raw pointer fields of different types\", \\\n+                vmSymbols::klass()->as_klass_external_name(), klass##_dump.id);                                            \\\n+    }                                                                                                                      \\\n+  } else\n+\n+#define CHECK_FIELD_FOUND(klass, name, ...) name##_offset_from_start < max_juint &&\n+\n+#define SET_FIELD_OFFSET(klass, name, ...) _##name##_offset = total_offset_from_start - name##_offset_from_start;\n+\n+#define INITIALIZE_OFFSETS(klass, FIELDS_DO, PTR_FIELDS_DO)                                                     \\\n+  FIELDS_DO(DEFINE_OFFSET_FROM_START_LOCAL)                                                                     \\\n+  PTR_FIELDS_DO(DEFINE_OFFSET_FROM_START_LOCAL)                                                                 \\\n+  u4 total_offset_from_start = 0;                                                                               \\\n+  for (u2 i = 0; i < klass##_dump.instance_field_infos.size(); i++) {                                           \\\n+    const HeapDump::ClassDump::Field::Info &field_info = klass##_dump.instance_field_infos[i];                  \\\n+    const BasicType field_type = HeapDump::htype2btype(field_info.type);                                        \\\n+    const Symbol *field_name = heap_dump.get_symbol(field_info.name_id);                                        \\\n+    FIELDS_DO(CREATE_FIELD_OFFSET_CASE)                                                                         \\\n+    PTR_FIELDS_DO(CREATE_PTR_FIELD_OFFSET_CASE)                                                                 \\\n+    { \/* final else branch *\/ }                                                                                 \\\n+    total_offset_from_start += is_java_primitive(field_type) ? type2aelembytes(field_type) : heap_dump.id_size; \\\n+  }                                                                                                             \\\n+  guarantee(FIELDS_DO(CHECK_FIELD_FOUND) PTR_FIELDS_DO(CHECK_FIELD_FOUND) \/* && *\/ true,                        \\\n+            \"some non-static fields are missing from %s class dump \" HDID_FORMAT,                               \\\n+            vmSymbols::klass()->as_klass_external_name(), klass##_dump.id);                                     \\\n+  FIELDS_DO(SET_FIELD_OFFSET)                                                                                   \\\n+  PTR_FIELDS_DO(SET_FIELD_OFFSET)\n+\n+#define ASSERT_INITIALIZED_WITH_SAME_ID(klass)                                                             \\\n+  precond(_##klass##_id != HeapDump::NULL_ID);                                                             \\\n+  assert(_##klass##_id == klass##_id,                                                                      \\\n+         \"%s class dump already found with different ID: old ID = \" HDID_FORMAT \", new ID = \" HDID_FORMAT, \\\n+         vmSymbols::klass()->as_klass_external_name(), _##klass##_id, klass##_id);\n+\n+#define DEFINE_GET_FIELD_METHOD(klass, name, name_sym, basic_type, c_type, type_name)                                          \\\n+  c_type HeapDumpClasses::klass::name(const HeapDump::InstanceDump &dump) const {                                              \\\n+    precond(is_initialized() && _##name##_offset >= (is_java_primitive(basic_type) ? type2aelembytes(basic_type) : _id_size)); \\\n+    guarantee(dump.fields_data.size() >= _##name##_offset,                                                                     \\\n+              \"%s object \" HDID_FORMAT \" has not enough non-static field data to store its '\" #name \"' field\",                 \\\n+              vmSymbols::klass()->as_klass_external_name(), dump.id);                                                          \\\n+    return dump.read_field(dump.fields_data.size() - _##name##_offset, basic_type, _id_size).as_##type_name;                   \\\n+  }\n+\n+#define DEFINE_GET_PTR_FIELD_METHOD(klass, name, name_sym)                                                             \\\n+  jlong HeapDumpClasses::klass::name(const HeapDump::InstanceDump &dump) const {                                       \\\n+    precond(is_initialized() && _##name##_offset >= checked_cast<u4>(type2aelembytes(_ptr_type)));                     \\\n+    guarantee(dump.fields_data.size() >= _##name##_offset,                                                             \\\n+              \"%s object \" HDID_FORMAT \" has not enough non-static field data to store its '\" #name \"' field\",         \\\n+              vmSymbols::klass()->as_klass_external_name(), dump.id);                                                  \\\n+    const HeapDump::BasicValue val = dump.read_field(dump.fields_data.size() - _##name##_offset, _ptr_type, _id_size); \\\n+    if (_ptr_type == T_INT) {                                                                                          \\\n+      return val.as_int;                                                                                               \\\n+    }                                                                                                                  \\\n+    assert(_ptr_type == T_LONG, \"must be\");                                                                            \\\n+    return val.as_long;                                                                                                \\\n+  }\n+\n+\n+\/\/ java.lang.ClassLoader\n+\n+#ifdef ASSERT\n+static bool is_class_loader_class_dump(const ParsedHeapDump &heap_dump, const HeapDump::ClassDump &dump) {\n+  const bool has_right_name_and_loader = heap_dump.get_class_name(dump.id) == vmSymbols::java_lang_ClassLoader() &&\n+                                         dump.class_loader_id == HeapDump::NULL_ID;\n+  if (!has_right_name_and_loader) {\n+    return false;\n+  }\n+\n+  assert(dump.super_id != HeapDump::NULL_ID, \"illegal super in %s dump \" HDID_FORMAT \": expected %s, got none\",\n+         vmSymbols::java_lang_ClassLoader()->as_klass_external_name(), dump.id,\n+         vmSymbols::java_lang_Object()->as_klass_external_name());\n+\n+  const HeapDump::ClassDump &super_dump = heap_dump.get_class_dump(dump.super_id);\n+  assert(heap_dump.get_class_name(super_dump.id) == vmSymbols::java_lang_Object() &&\n+         super_dump.class_loader_id == HeapDump::NULL_ID, \"illegal super in %s dump \" HDID_FORMAT \": expected %s, got %s\",\n+         vmSymbols::java_lang_ClassLoader()->as_klass_external_name(), dump.id,\n+         heap_dump.get_class_name(super_dump.id)->as_klass_external_name(),\n+         vmSymbols::java_lang_Object()->as_klass_external_name());\n+\n+  return true;\n+}\n+#endif \/\/ ASSERT\n+\n+DEFINE_FIND_CLASS_DUMP_FROM_SUBCLASS_DUMP(java_lang_ClassLoader)\n+\n+void HeapDumpClasses::java_lang_ClassLoader::ensure_initialized(const ParsedHeapDump &heap_dump, HeapDump::ID loader_class_id) {\n+  precond(loader_class_id != HeapDump::NULL_ID);\n+  if (!is_initialized()) {\n+    const HeapDump::ClassDump *java_lang_ClassLoader_dump_ptr = find_java_lang_ClassLoader_dump(heap_dump, loader_class_id);\n+    guarantee(java_lang_ClassLoader_dump_ptr != nullptr, \"cannot find %s as a super-class of \" HDID_FORMAT,\n+              vmSymbols::java_lang_ClassLoader()->as_klass_external_name(), loader_class_id);\n+    const HeapDump::ClassDump &java_lang_ClassLoader_dump = *java_lang_ClassLoader_dump_ptr;\n+    precond(is_class_loader_class_dump(heap_dump, java_lang_ClassLoader_dump));\n+    INITIALIZE_OFFSETS(java_lang_ClassLoader, CLASSLOADER_DUMP_FIELDS_DO, NO_DUMP_FIELDS_DO)\n+    DEBUG_ONLY(_java_lang_ClassLoader_id = java_lang_ClassLoader_dump.id);\n+    _id_size = heap_dump.id_size;\n+  } else {\n+#ifdef ASSERT\n+    const HeapDump::ClassDump *java_lang_ClassLoader_dump_ptr = find_java_lang_ClassLoader_dump(heap_dump, loader_class_id);\n+    assert(java_lang_ClassLoader_dump_ptr != nullptr, \"cannot find %s as a super-class of \" HDID_FORMAT,\n+           vmSymbols::java_lang_ClassLoader()->as_klass_external_name(), loader_class_id);\n+    const HeapDump::ID java_lang_ClassLoader_id = java_lang_ClassLoader_dump_ptr->id;\n+    ASSERT_INITIALIZED_WITH_SAME_ID(java_lang_ClassLoader)\n+#endif \/\/ ASSERT\n+  }\n+  postcond(is_initialized());\n+}\n+\n+CLASSLOADER_DUMP_FIELDS_DO(DEFINE_GET_FIELD_METHOD)\n+\n+\n+\/\/ java.lang.Class\n+\n+#ifdef ASSERT\n+static bool is_class_mirror_class_dump(const ParsedHeapDump &heap_dump, const HeapDump::ClassDump &dump) {\n+  const bool has_right_name_and_loader = heap_dump.get_class_name(dump.id) == vmSymbols::java_lang_Class() &&\n+                                         dump.class_loader_id == HeapDump::NULL_ID;\n+  if (!has_right_name_and_loader) {\n+    return false;\n+  }\n+\n+  assert(dump.super_id != HeapDump::NULL_ID, \"illegal super in %s dump \" HDID_FORMAT \": expected %s, got none\",\n+         vmSymbols::java_lang_Class()->as_klass_external_name(), dump.id,\n+         vmSymbols::java_lang_Object()->as_klass_external_name());\n+\n+  const HeapDump::ClassDump &super_dump = heap_dump.get_class_dump(dump.super_id);\n+  assert(heap_dump.get_class_name(super_dump.id) == vmSymbols::java_lang_Object() &&\n+         super_dump.class_loader_id == HeapDump::NULL_ID, \"illegal super in %s dump \" HDID_FORMAT \": expected %s, got %s\",\n+         vmSymbols::java_lang_Class()->as_klass_external_name(), dump.id,\n+         heap_dump.get_class_name(super_dump.id)->as_klass_external_name(),\n+         vmSymbols::java_lang_Object()->as_klass_external_name());\n+\n+  return true;\n+}\n+#endif \/\/ ASSERT\n+\n+void HeapDumpClasses::java_lang_Class::ensure_initialized(const ParsedHeapDump &heap_dump, HeapDump::ID java_lang_Class_id) {\n+  precond(java_lang_Class_id != HeapDump::NULL_ID);\n+  if (!is_initialized()) {\n+    const HeapDump::ClassDump &java_lang_Class_dump = heap_dump.get_class_dump(java_lang_Class_id);\n+    precond(is_class_mirror_class_dump(heap_dump, java_lang_Class_dump));\n+    INITIALIZE_OFFSETS(java_lang_Class, CLASSMIRROR_DUMP_FIELDS_DO, CLASSMIRROR_DUMP_PTR_FIELDS_DO)\n+    DEBUG_ONLY(_java_lang_Class_id = java_lang_Class_id);\n+    _id_size = heap_dump.id_size;\n+  } else {\n+    ASSERT_INITIALIZED_WITH_SAME_ID(java_lang_Class)\n+  }\n+  postcond(is_initialized());\n+}\n+\n+CLASSMIRROR_DUMP_FIELDS_DO(DEFINE_GET_FIELD_METHOD)\n+CLASSMIRROR_DUMP_PTR_FIELDS_DO(DEFINE_GET_PTR_FIELD_METHOD)\n+\n+HeapDumpClasses::java_lang_Class::Kind HeapDumpClasses::java_lang_Class::kind(const HeapDump::InstanceDump &dump) const {\n+  const bool has_klass = klass(dump) != 0;\n+  const bool has_component = componentType(dump) != 0;\n+  if (has_klass) {\n+    return has_component ? Kind::ARRAY : Kind::INSTANCE;\n+  }\n+  guarantee(!has_component, \"%s object \" HDID_FORMAT \" representing a primitive type cannot have a component type\",\n+            vmSymbols::java_lang_Class()->as_klass_external_name(), dump.id);\n+  return Kind::PRIMITIVE;\n+}\n+\n+\n+\/\/ java.lang.Thread\n+\n+#ifdef ASSERT\n+static bool is_thread_class_dump(const ParsedHeapDump &heap_dump, const HeapDump::ClassDump &dump) {\n+  const bool has_right_name_and_loader = heap_dump.get_class_name(dump.id) == vmSymbols::java_lang_Thread() &&\n+                                         dump.class_loader_id == HeapDump::NULL_ID;\n+  if (!has_right_name_and_loader) {\n+    return false;\n+  }\n+\n+  assert(dump.super_id != HeapDump::NULL_ID, \"illegal super in %s dump \" HDID_FORMAT \": expected %s, got none\",\n+         vmSymbols::java_lang_Thread()->as_klass_external_name(), dump.id,\n+         vmSymbols::java_lang_Object()->as_klass_external_name());\n+\n+  const HeapDump::ClassDump &super_dump = heap_dump.get_class_dump(dump.super_id);\n+  assert(heap_dump.get_class_name(super_dump.id) == vmSymbols::java_lang_Object() &&\n+         super_dump.class_loader_id == HeapDump::NULL_ID, \"illegal super in %s dump \" HDID_FORMAT \": expected %s, got %s\",\n+         vmSymbols::java_lang_Thread()->as_klass_external_name(), dump.id,\n+         heap_dump.get_class_name(super_dump.id)->as_klass_external_name(),\n+         vmSymbols::java_lang_Object()->as_klass_external_name());\n+\n+  return true;\n+}\n+#endif \/\/ ASSERT\n+\n+DEFINE_FIND_CLASS_DUMP_FROM_SUBCLASS_DUMP(java_lang_Thread)\n+\n+void HeapDumpClasses::java_lang_Thread::ensure_initialized(const ParsedHeapDump &heap_dump, HeapDump::ID thread_class_id) {\n+  precond(thread_class_id != HeapDump::NULL_ID);\n+  if (!is_initialized()) {\n+    const HeapDump::ClassDump *java_lang_Thread_dump_ptr = find_java_lang_Thread_dump(heap_dump, thread_class_id);\n+    guarantee(java_lang_Thread_dump_ptr != nullptr, \"cannot find %s as a super-class of \" HDID_FORMAT,\n+              vmSymbols::java_lang_Thread()->as_klass_external_name(), thread_class_id);\n+    const HeapDump::ClassDump &java_lang_Thread_dump = *java_lang_Thread_dump_ptr;\n+    precond(is_thread_class_dump(heap_dump, java_lang_Thread_dump));\n+    INITIALIZE_OFFSETS(java_lang_Thread, THREAD_DUMP_FIELDS_DO, NO_DUMP_FIELDS_DO)\n+    DEBUG_ONLY(_java_lang_Thread_id = java_lang_Thread_dump.id);\n+    _id_size = heap_dump.id_size;\n+  } else {\n+#ifdef ASSERT\n+    const HeapDump::ClassDump *java_lang_Thread_dump_ptr = find_java_lang_Thread_dump(heap_dump, thread_class_id);\n+    assert(java_lang_Thread_dump_ptr != nullptr, \"cannot find %s as a super-class of \" HDID_FORMAT,\n+           vmSymbols::java_lang_Thread()->as_klass_external_name(), thread_class_id);\n+    const HeapDump::ID java_lang_Thread_id = java_lang_Thread_dump_ptr->id;\n+    ASSERT_INITIALIZED_WITH_SAME_ID(java_lang_Thread)\n+#endif \/\/ ASSERT\n+  }\n+  postcond(is_initialized());\n+}\n+\n+THREAD_DUMP_FIELDS_DO(DEFINE_GET_FIELD_METHOD)\n+\n+\n+\/\/ java.lang.String\n+\n+#ifdef ASSERT\n+static bool is_string_class_dump(const ParsedHeapDump &heap_dump, const HeapDump::ClassDump &dump) {\n+  const bool has_right_name_and_loader = heap_dump.get_class_name(dump.id) == vmSymbols::java_lang_String() &&\n+                                         dump.class_loader_id == HeapDump::NULL_ID;\n+  if (!has_right_name_and_loader) {\n+    return false;\n+  }\n+\n+  assert(dump.super_id != HeapDump::NULL_ID, \"illegal super in %s dump \" HDID_FORMAT \": expected %s, got none\",\n+         vmSymbols::java_lang_String()->as_klass_external_name(), dump.id,\n+         vmSymbols::java_lang_Object()->as_klass_external_name());\n+\n+  const HeapDump::ClassDump &super_dump = heap_dump.get_class_dump(dump.super_id);\n+  assert(heap_dump.get_class_name(super_dump.id) == vmSymbols::java_lang_Object() &&\n+         super_dump.class_loader_id == HeapDump::NULL_ID, \"illegal super in %s dump \" HDID_FORMAT \": expected %s, got %s\",\n+         vmSymbols::java_lang_String()->as_klass_external_name(), dump.id,\n+         heap_dump.get_class_name(super_dump.id)->as_klass_external_name(),\n+         vmSymbols::java_lang_Object()->as_klass_external_name());\n+\n+  return true;\n+}\n+#endif \/\/ ASSERT\n+\n+void HeapDumpClasses::java_lang_String::ensure_initialized(const ParsedHeapDump &heap_dump, HeapDump::ID java_lang_String_id) {\n+  precond(java_lang_String_id != HeapDump::NULL_ID);\n+  if (!is_initialized()) {\n+    const HeapDump::ClassDump &java_lang_String_dump = heap_dump.get_class_dump(java_lang_String_id);\n+    precond(is_string_class_dump(heap_dump, java_lang_String_dump));\n+    INITIALIZE_OFFSETS(java_lang_String, STRING_DUMP_FIELDS_DO, NO_DUMP_FIELDS_DO)\n+    DEBUG_ONLY(_java_lang_String_id = java_lang_String_id);\n+    _id_size = heap_dump.id_size;\n+  } else {\n+    ASSERT_INITIALIZED_WITH_SAME_ID(java_lang_String)\n+  }\n+  postcond(is_initialized());\n+}\n+\n+STRING_DUMP_FIELDS_DO(DEFINE_GET_FIELD_METHOD)\n+\n+\n+\/\/ java.lang.invoke.ResolvedMethodName\n+\n+#ifdef ASSERT\n+static bool is_resolved_method_name_class_dump(const ParsedHeapDump &heap_dump, const HeapDump::ClassDump &dump) {\n+  const bool has_right_name_and_loader = heap_dump.get_class_name(dump.id) == vmSymbols::java_lang_invoke_ResolvedMethodName() &&\n+                                         dump.class_loader_id == HeapDump::NULL_ID;\n+  if (!has_right_name_and_loader) {\n+    return false;\n+  }\n+\n+  assert(dump.super_id != HeapDump::NULL_ID, \"illegal super in %s dump \" HDID_FORMAT \": expected %s, got none\",\n+         vmSymbols::java_lang_invoke_ResolvedMethodName()->as_klass_external_name(), dump.id,\n+         vmSymbols::java_lang_Object()->as_klass_external_name());\n+\n+  const HeapDump::ClassDump &super_dump = heap_dump.get_class_dump(dump.super_id);\n+  assert(heap_dump.get_class_name(super_dump.id) == vmSymbols::java_lang_Object() &&\n+         super_dump.class_loader_id == HeapDump::NULL_ID, \"illegal super in %s dump \" HDID_FORMAT \": expected %s, got %s\",\n+         vmSymbols::java_lang_invoke_ResolvedMethodName()->as_klass_external_name(), dump.id,\n+         heap_dump.get_class_name(super_dump.id)->as_klass_external_name(),\n+         vmSymbols::java_lang_Object()->as_klass_external_name());\n+\n+  return true;\n+}\n+#endif \/\/ ASSERT\n+\n+void HeapDumpClasses::java_lang_invoke_ResolvedMethodName::ensure_initialized(const ParsedHeapDump &heap_dump, HeapDump::ID java_lang_invoke_ResolvedMethodName_id) {\n+  precond(java_lang_invoke_ResolvedMethodName_id != HeapDump::NULL_ID);\n+  if (!is_initialized()) {\n+    const HeapDump::ClassDump &java_lang_invoke_ResolvedMethodName_dump = heap_dump.get_class_dump(java_lang_invoke_ResolvedMethodName_id);\n+    precond(is_resolved_method_name_class_dump(heap_dump, java_lang_invoke_ResolvedMethodName_dump));\n+    INITIALIZE_OFFSETS(java_lang_invoke_ResolvedMethodName, RESOLVEDMETHODNAME_DUMP_FIELDS_DO, RESOLVEDMETHODNAME_DUMP_PTR_FIELDS_DO)\n+    DEBUG_ONLY(_java_lang_invoke_ResolvedMethodName_id = java_lang_invoke_ResolvedMethodName_id);\n+    _id_size = heap_dump.id_size;\n+  } else {\n+    ASSERT_INITIALIZED_WITH_SAME_ID(java_lang_invoke_ResolvedMethodName)\n+  }\n+  postcond(is_initialized());\n+}\n+\n+RESOLVEDMETHODNAME_DUMP_FIELDS_DO(DEFINE_GET_FIELD_METHOD)\n+\n+HeapDump::ID HeapDumpClasses::java_lang_invoke_ResolvedMethodName::method_name_id(const HeapDump::InstanceDump &dump) const {\n+  precond(is_initialized() && _method_name_id_offset >= checked_cast<u4>(type2aelembytes(_ptr_type)));\n+  guarantee(dump.fields_data.size() >= _method_name_id_offset,\n+            \"%s object \" HDID_FORMAT \" has not enough non-static field data to store its '%s' field\",\n+            vmSymbols::java_lang_invoke_ResolvedMethodName()->as_klass_external_name(), dump.id, vmSymbols::internal_name_name()->as_C_string());\n+  const HeapDump::BasicValue val = dump.read_field(dump.fields_data.size() - _method_name_id_offset, _ptr_type, _id_size);\n+  precond(_ptr_type == T_INT || _ptr_type == T_LONG);\n+  return _ptr_type == T_INT ? bit_cast<u4>(val.as_int) : bit_cast<HeapDump::ID>(val.as_long);\n+}\n+\n+HeapDump::ID HeapDumpClasses::java_lang_invoke_ResolvedMethodName::method_signature_id(const HeapDump::InstanceDump &dump) const {\n+  precond(is_initialized() && _method_signature_id_offset >= checked_cast<u4>(type2aelembytes(_ptr_type)));\n+  guarantee(dump.fields_data.size() >= _method_signature_id_offset,\n+            \"%s object \" HDID_FORMAT \" has not enough non-static field data to store its '%s' field\",\n+            vmSymbols::java_lang_invoke_ResolvedMethodName()->as_klass_external_name(), dump.id, vmSymbols::internal_signature_name()->as_C_string());\n+  const HeapDump::BasicValue val = dump.read_field(dump.fields_data.size() - _method_signature_id_offset, _ptr_type, _id_size);\n+  precond(_ptr_type == T_INT || _ptr_type == T_LONG);\n+  return _ptr_type == T_INT ? bit_cast<u4>(val.as_int) : bit_cast<HeapDump::ID>(val.as_long);\n+}\n+\n+\n+\/\/ java.lang.invoke.MemberName\n+\n+#ifdef ASSERT\n+static bool is_member_name_class_dump(const ParsedHeapDump &heap_dump, const HeapDump::ClassDump &dump) {\n+  const bool has_right_name_and_loader = heap_dump.get_class_name(dump.id) == vmSymbols::java_lang_invoke_MemberName() &&\n+                                         dump.class_loader_id == HeapDump::NULL_ID;\n+  if (!has_right_name_and_loader) {\n+    return false;\n+  }\n+\n+  assert(dump.super_id != HeapDump::NULL_ID, \"illegal super in %s dump \" HDID_FORMAT \": expected %s, got none\",\n+         vmSymbols::java_lang_invoke_MemberName()->as_klass_external_name(), dump.id,\n+         vmSymbols::java_lang_Object()->as_klass_external_name());\n+\n+  const HeapDump::ClassDump &super_dump = heap_dump.get_class_dump(dump.super_id);\n+  assert(heap_dump.get_class_name(super_dump.id) == vmSymbols::java_lang_Object() &&\n+         super_dump.class_loader_id == HeapDump::NULL_ID, \"illegal super in %s dump \" HDID_FORMAT \": expected %s, got %s\",\n+         vmSymbols::java_lang_invoke_MemberName()->as_klass_external_name(), dump.id,\n+         heap_dump.get_class_name(super_dump.id)->as_klass_external_name(),\n+         vmSymbols::java_lang_Object()->as_klass_external_name());\n+\n+  return true;\n+}\n+#endif \/\/ ASSERT\n+\n+void HeapDumpClasses::java_lang_invoke_MemberName::ensure_initialized(const ParsedHeapDump &heap_dump, HeapDump::ID java_lang_invoke_MemberName_id) {\n+  precond(java_lang_invoke_MemberName_id != HeapDump::NULL_ID);\n+  if (!is_initialized()) {\n+    const HeapDump::ClassDump &java_lang_invoke_MemberName_dump = heap_dump.get_class_dump(java_lang_invoke_MemberName_id);\n+    precond(is_member_name_class_dump(heap_dump, java_lang_invoke_MemberName_dump));\n+    INITIALIZE_OFFSETS(java_lang_invoke_MemberName, MEMBERNAME_DUMP_FIELDS_DO, MEMBERNAME_DUMP_PTR_FIELDS_DO)\n+    DEBUG_ONLY(_java_lang_invoke_MemberName_id = java_lang_invoke_MemberName_id);\n+    _id_size = heap_dump.id_size;\n+  } else {\n+    ASSERT_INITIALIZED_WITH_SAME_ID(java_lang_invoke_MemberName)\n+  }\n+  postcond(is_initialized());\n+}\n+\n+MEMBERNAME_DUMP_FIELDS_DO(DEFINE_GET_FIELD_METHOD)\n+MEMBERNAME_DUMP_PTR_FIELDS_DO(DEFINE_GET_PTR_FIELD_METHOD)\n+\n+bool HeapDumpClasses::java_lang_invoke_MemberName::is_field(const HeapDump::InstanceDump &dump) const {\n+  const jint fs = flags(dump);\n+  return (fs & ::java_lang_invoke_MemberName::MN_IS_FIELD) != 0;\n+}\n+\n+\n+\/\/ java.lang.invoke.MethodType\n+\n+#ifdef ASSERT\n+static bool is_method_type_class_dump(const ParsedHeapDump &heap_dump, const HeapDump::ClassDump &dump) {\n+  const bool has_right_name_and_loader = heap_dump.get_class_name(dump.id) == vmSymbols::java_lang_invoke_MethodType() &&\n+                                         dump.class_loader_id == HeapDump::NULL_ID;\n+  if (!has_right_name_and_loader) {\n+    return false;\n+  }\n+\n+  assert(dump.super_id != HeapDump::NULL_ID, \"illegal super in %s dump \" HDID_FORMAT \": expected %s, got none\",\n+         vmSymbols::java_lang_invoke_MethodType()->as_klass_external_name(), dump.id,\n+         vmSymbols::java_lang_Object()->as_klass_external_name());\n+\n+  const HeapDump::ClassDump &super_dump = heap_dump.get_class_dump(dump.super_id);\n+  assert(heap_dump.get_class_name(super_dump.id) == vmSymbols::java_lang_Object() &&\n+         super_dump.class_loader_id == HeapDump::NULL_ID, \"illegal super in %s dump \" HDID_FORMAT \": expected %s, got %s\",\n+         vmSymbols::java_lang_invoke_MethodType()->as_klass_external_name(), dump.id,\n+         heap_dump.get_class_name(super_dump.id)->as_klass_external_name(),\n+         vmSymbols::java_lang_Object()->as_klass_external_name());\n+\n+  return true;\n+}\n+#endif \/\/ ASSERT\n+\n+void HeapDumpClasses::java_lang_invoke_MethodType::ensure_initialized(const ParsedHeapDump &heap_dump, HeapDump::ID java_lang_invoke_MethodType_id) {\n+  precond(java_lang_invoke_MethodType_id != HeapDump::NULL_ID);\n+  if (!is_initialized()) {\n+    const HeapDump::ClassDump &java_lang_invoke_MethodType_dump = heap_dump.get_class_dump(java_lang_invoke_MethodType_id);\n+    precond(is_method_type_class_dump(heap_dump, java_lang_invoke_MethodType_dump));\n+    INITIALIZE_OFFSETS(java_lang_invoke_MethodType, METHODTYPE_DUMP_FIELDS_DO, NO_DUMP_FIELDS_DO)\n+    DEBUG_ONLY(_java_lang_invoke_MethodType_id = java_lang_invoke_MethodType_id);\n+    _id_size = heap_dump.id_size;\n+  } else {\n+    ASSERT_INITIALIZED_WITH_SAME_ID(java_lang_invoke_MethodType)\n+  }\n+  postcond(is_initialized());\n+}\n+\n+METHODTYPE_DUMP_FIELDS_DO(DEFINE_GET_FIELD_METHOD)\n+\n+\n+#undef DEFINE_GET_PTR_FIELD_METHOD\n+#undef DEFINE_GET_FIELD_METHOD\n+#undef ASSERT_INITIALIZED_WITH_SAME_ID\n+#undef INITIALIZE_OFFSETS\n+#undef SET_FIELD_OFFSET\n+#undef CHECK_FIELD_FOUND\n+#undef CREATE_PTR_FIELD_OFFSET_CASE\n+#undef CREATE_FIELD_OFFSET_CASE\n+#undef DEFINE_OFFSET_FROM_START_LOCAL\n+#undef DEFINE_INSTANCE_DUMP_CHECK\n+#undef NO_DUMP_FIELDS_DO\n","filename":"src\/hotspot\/share\/utilities\/heapDumpClasses.cpp","additions":484,"deletions":0,"binary":false,"changes":484,"status":"added"},{"patch":"@@ -0,0 +1,211 @@\n+#ifndef SHARE_UTILITIES_HEAPDUMPCLASSES_HPP\n+#define SHARE_UTILITIES_HEAPDUMPCLASSES_HPP\n+\n+#include \"memory\/allStatic.hpp\"\n+#include \"utilities\/globalDefinitions.hpp\"\n+#include \"utilities\/heapDumpParser.hpp\"\n+\n+#define DEFINE_OFFSET_FIELD(klass, name, ...) \\\n+  u4 _##name##_offset;\n+#define DECLARE_GET_FIELD_METHOD(klass, name, name_sym, basic_type, c_type, type_name) \\\n+  c_type name(const HeapDump::InstanceDump &dump) const;\n+\/\/ \"Pointer\" fields are internal fields of intptr type: their Java type is\n+\/\/ either int or long depending on the CPU architecture\n+#define DECLARE_GET_PTR_FIELD_METHOD(klass, name, name_sym) \\\n+  jlong name(const HeapDump::InstanceDump &dump) const;\n+\n+\/\/ Helper classes for parsing HPROF-dumped instance fields of well-known\n+\/\/ classes and their sub-classes.\n+\/\/\n+\/\/ Classes and fields are to be added on-demand.\n+struct HeapDumpClasses : public AllStatic {\n+\n+#define CLASSLOADER_DUMP_FIELDS_DO(macro)                                                             \\\n+  macro(java_lang_ClassLoader, parent, vmSymbols::parent_name(), T_OBJECT, HeapDump::ID, object_id)   \\\n+  macro(java_lang_ClassLoader, name, vmSymbols::name_name(), T_OBJECT, HeapDump::ID, object_id)       \\\n+  macro(java_lang_ClassLoader, nameAndId, \"nameAndId\", T_OBJECT, HeapDump::ID, object_id)             \\\n+  macro(java_lang_ClassLoader, unnamedModule, \"unnamedModule\", T_OBJECT, HeapDump::ID, object_id)     \\\n+  macro(java_lang_ClassLoader, parallelLockMap, \"parallelLockMap\", T_OBJECT, HeapDump::ID, object_id)\n+\n+  class java_lang_ClassLoader {\n+   private:\n+    u4 _id_size = 0;\n+    CLASSLOADER_DUMP_FIELDS_DO(DEFINE_OFFSET_FIELD)\n+    DEBUG_ONLY(HeapDump::ID _java_lang_ClassLoader_id = HeapDump::NULL_ID);\n+\n+   public:\n+    void ensure_initialized(const ParsedHeapDump &heap_dump, HeapDump::ID loader_class_id);\n+    CLASSLOADER_DUMP_FIELDS_DO(DECLARE_GET_FIELD_METHOD)\n+\n+   private:\n+    bool is_initialized() const { return _id_size > 0; }\n+  };\n+\n+\n+#define CLASSMIRROR_DUMP_FIELDS_DO(macro)                                                                   \\\n+  macro(java_lang_Class, name, vmSymbols::name_name(), T_OBJECT, HeapDump::ID, object_id)                   \\\n+  macro(java_lang_Class, module, \"module\", T_OBJECT, HeapDump::ID, object_id)                               \\\n+  macro(java_lang_Class, componentType, vmSymbols::componentType_name(), T_OBJECT, HeapDump::ID, object_id)\n+#define CLASSMIRROR_DUMP_PTR_FIELDS_DO(macro)                        \\\n+  macro(java_lang_Class, klass, vmSymbols::klass_name())             \\\n+  macro(java_lang_Class, array_klass, vmSymbols::array_klass_name())\n+\n+  \/\/ Requires the heap dump to include injected fields.\n+  class java_lang_Class {\n+   private:\n+    u4 _id_size = 0;\n+    BasicType _ptr_type = T_ILLEGAL;\n+    CLASSMIRROR_DUMP_FIELDS_DO(DEFINE_OFFSET_FIELD)\n+    CLASSMIRROR_DUMP_PTR_FIELDS_DO(DEFINE_OFFSET_FIELD)\n+    DEBUG_ONLY(HeapDump::ID _java_lang_Class_id = HeapDump::NULL_ID);\n+\n+   public:\n+    void ensure_initialized(const ParsedHeapDump &heap_dump, HeapDump::ID java_lang_Class_id);\n+\n+    CLASSMIRROR_DUMP_FIELDS_DO(DECLARE_GET_FIELD_METHOD)\n+    CLASSMIRROR_DUMP_PTR_FIELDS_DO(DECLARE_GET_PTR_FIELD_METHOD)\n+\n+    enum class Kind { INSTANCE, ARRAY, PRIMITIVE };\n+    Kind kind(const HeapDump::InstanceDump &dump) const;\n+    bool is_instance_kind(const HeapDump::InstanceDump &dump) const  { return kind(dump) == Kind::INSTANCE; }\n+    bool is_array_kind(const HeapDump::InstanceDump &dump) const     { return kind(dump) == Kind::ARRAY; }\n+    bool is_primitive_kind(const HeapDump::InstanceDump &dump) const { return kind(dump) == Kind::PRIMITIVE; }\n+\n+    bool mirrors_void(const HeapDump::InstanceDump &dump) const {\n+      \/\/ Void is the only \"primitive type\" without an array class\n+      return is_primitive_kind(dump) && array_klass(dump) == 0;\n+    }\n+\n+   private:\n+    bool is_initialized() const { return _id_size > 0; }\n+  };\n+\n+\n+#define THREAD_DUMP_FIELDS_DO(macro)                                                                                                         \\\n+  macro(java_lang_Thread, tid, \"tid\", T_LONG, jlong, long)                                                                                   \\\n+  macro(java_lang_Thread, name, vmSymbols::name_name(), T_OBJECT, HeapDump::ID, object_id)                                                   \\\n+  macro(java_lang_Thread, holder, \"holder\", T_OBJECT, HeapDump::ID, object_id)                                                               \\\n+  macro(java_lang_Thread, inheritedAccessControlContext, vmSymbols::inheritedAccessControlContext_name(), T_OBJECT, HeapDump::ID, object_id)\n+\n+  class java_lang_Thread {\n+   private:\n+    u4 _id_size = 0;\n+    THREAD_DUMP_FIELDS_DO(DEFINE_OFFSET_FIELD)\n+    DEBUG_ONLY(HeapDump::ID _java_lang_Thread_id = HeapDump::NULL_ID);\n+\n+   public:\n+    void ensure_initialized(const ParsedHeapDump &heap_dump, HeapDump::ID thread_class_id);\n+\n+    THREAD_DUMP_FIELDS_DO(DECLARE_GET_FIELD_METHOD)\n+\n+   private:\n+    bool is_initialized() const { return _id_size > 0; }\n+  };\n+\n+\n+#define STRING_DUMP_FIELDS_DO(macro)                                                            \\\n+  macro(java_lang_String, is_interned, vmSymbols::is_interned_name(), T_BOOLEAN, bool, boolean)\n+\n+  \/\/ Requires the heap dump to include injected fields.\n+  class java_lang_String {\n+   private:\n+    u4 _id_size = 0;\n+    STRING_DUMP_FIELDS_DO(DEFINE_OFFSET_FIELD)\n+    DEBUG_ONLY(HeapDump::ID _java_lang_String_id = HeapDump::NULL_ID);\n+\n+   public:\n+    void ensure_initialized(const ParsedHeapDump &heap_dump, HeapDump::ID java_lang_String_id);\n+\n+    STRING_DUMP_FIELDS_DO(DECLARE_GET_FIELD_METHOD)\n+\n+   private:\n+    bool is_initialized() const { return _id_size > 0; }\n+  };\n+\n+\n+#define RESOLVEDMETHODNAME_DUMP_FIELDS_DO(macro)                                                                      \\\n+  macro(java_lang_invoke_ResolvedMethodName, vmholder, vmSymbols::vmholder_name(), T_OBJECT, HeapDump::ID, object_id) \\\n+  macro(java_lang_invoke_ResolvedMethodName, method_kind, vmSymbols::internal_kind_name(), T_BYTE, jbyte, byte)\n+#define RESOLVEDMETHODNAME_DUMP_PTR_FIELDS_DO(macro)                                                    \\\n+  macro(java_lang_invoke_ResolvedMethodName, method_name_id, vmSymbols::internal_name_name())           \\\n+  macro(java_lang_invoke_ResolvedMethodName, method_signature_id, vmSymbols::internal_signature_name())\n+\n+  \/\/ Requires the heap dump to include injected fields and 3 additional\n+  \/\/ identification fake-fields for 'vmtarget' field of ResolvedMethodName.\n+  class java_lang_invoke_ResolvedMethodName {\n+   private:\n+    u4 _id_size = 0;\n+    BasicType _ptr_type = T_ILLEGAL;\n+    RESOLVEDMETHODNAME_DUMP_FIELDS_DO(DEFINE_OFFSET_FIELD)\n+    RESOLVEDMETHODNAME_DUMP_PTR_FIELDS_DO(DEFINE_OFFSET_FIELD)\n+    DEBUG_ONLY(HeapDump::ID _java_lang_invoke_ResolvedMethodName_id = HeapDump::NULL_ID);\n+\n+   public:\n+    void ensure_initialized(const ParsedHeapDump &heap_dump, HeapDump::ID java_lang_invoke_ResolvedMethodName_id);\n+\n+    RESOLVEDMETHODNAME_DUMP_FIELDS_DO(DECLARE_GET_FIELD_METHOD)\n+    HeapDump::ID method_name_id(const HeapDump::InstanceDump &dump) const;\n+    HeapDump::ID method_signature_id(const HeapDump::InstanceDump &dump) const;\n+\n+   private:\n+    bool is_initialized() const { return _id_size > 0; }\n+  };\n+\n+\n+#define MEMBERNAME_DUMP_FIELDS_DO(macro)                                                                          \\\n+  macro(java_lang_invoke_MemberName, clazz, vmSymbols::clazz_name(), T_OBJECT, HeapDump::ID, object_id)           \\\n+  macro(java_lang_invoke_MemberName, name, vmSymbols::name_name(), T_OBJECT, HeapDump::ID, object_id)             \\\n+  macro(java_lang_invoke_MemberName, type, vmSymbols::type_name(), T_OBJECT, HeapDump::ID, object_id)             \\\n+  macro(java_lang_invoke_MemberName, flags, vmSymbols::flags_name(), T_INT, jint, int)                            \\\n+  macro(java_lang_invoke_MemberName, method, vmSymbols::method_name(), T_OBJECT, HeapDump::ID, object_id)         \\\n+  macro(java_lang_invoke_MemberName, resolution, \"resolution\", T_OBJECT, HeapDump::ID, object_id)\n+#define MEMBERNAME_DUMP_PTR_FIELDS_DO(macro)                             \\\n+  macro(java_lang_invoke_MemberName, vmindex, vmSymbols::vmindex_name())\n+\n+  \/\/ Requires the heap dump to include injected fields.\n+  class java_lang_invoke_MemberName {\n+   private:\n+    u4 _id_size = 0;\n+    BasicType _ptr_type = T_ILLEGAL;\n+    MEMBERNAME_DUMP_FIELDS_DO(DEFINE_OFFSET_FIELD)\n+    MEMBERNAME_DUMP_PTR_FIELDS_DO(DEFINE_OFFSET_FIELD)\n+    DEBUG_ONLY(HeapDump::ID _java_lang_invoke_MemberName_id = HeapDump::NULL_ID);\n+\n+   public:\n+    void ensure_initialized(const ParsedHeapDump &heap_dump, HeapDump::ID java_lang_invoke_MemberName_id);\n+\n+    MEMBERNAME_DUMP_FIELDS_DO(DECLARE_GET_FIELD_METHOD)\n+    MEMBERNAME_DUMP_PTR_FIELDS_DO(DECLARE_GET_PTR_FIELD_METHOD)\n+\n+    bool is_field(const HeapDump::InstanceDump &dump) const;\n+    bool is_resolved(const HeapDump::InstanceDump &dump) const { return resolution(dump) == HeapDump::NULL_ID; }\n+\n+   private:\n+    bool is_initialized() const { return _id_size > 0; }\n+  };\n+\n+\n+#define METHODTYPE_DUMP_FIELDS_DO(macro)                                                  \\\n+  macro(java_lang_invoke_MethodType, rtype, \"rtype\", T_OBJECT, HeapDump::ID, object_id)   \\\n+  macro(java_lang_invoke_MethodType, ptypes, \"ptypes\", T_OBJECT, HeapDump::ID, object_id)\n+\n+  class java_lang_invoke_MethodType {\n+   private:\n+    u4 _id_size = 0;\n+    METHODTYPE_DUMP_FIELDS_DO(DEFINE_OFFSET_FIELD)\n+    DEBUG_ONLY(HeapDump::ID _java_lang_invoke_MethodType_id = HeapDump::NULL_ID);\n+\n+   public:\n+    void ensure_initialized(const ParsedHeapDump &heap_dump, HeapDump::ID java_lang_invoke_MethodType_id);\n+\n+    METHODTYPE_DUMP_FIELDS_DO(DECLARE_GET_FIELD_METHOD)\n+\n+   private:\n+    bool is_initialized() const { return _id_size > 0; }\n+  };\n+};\n+\n+#undef DECLARE_GET_FIELD_METHOD\n+#undef DEFINE_OFFSET_FIELD\n+\n+#endif \/\/ SHARE_UTILITIES_HEAPDUMPCLASSES_HPP\n","filename":"src\/hotspot\/share\/utilities\/heapDumpClasses.hpp","additions":211,"deletions":0,"binary":false,"changes":211,"status":"added"},{"patch":"@@ -0,0 +1,898 @@\n+#include \"precompiled.hpp\"\n+#include \"classfile\/symbolTable.hpp\"\n+#include \"logging\/log.hpp\"\n+#include \"runtime\/os.hpp\"\n+#include \"runtime\/timerTrace.hpp\"\n+#include \"utilities\/basicTypeReader.hpp\"\n+#include \"utilities\/debug.hpp\"\n+#include \"utilities\/extendableArray.hpp\"\n+#include \"utilities\/globalDefinitions.hpp\"\n+#include \"utilities\/heapDumpParser.hpp\"\n+#include \"utilities\/hprofTag.hpp\"\n+\n+constexpr HeapDump::ID HeapDump::NULL_ID;\n+\n+constexpr char ERR_INVAL_HEADER_STR[] = \"invalid header string\";\n+constexpr char ERR_INVAL_ID_SIZE[] = \"invalid ID size format\";\n+constexpr char ERR_UNSUPPORTED_ID_SIZE[] = \"unsupported ID size\";\n+constexpr char ERR_INVAL_DUMP_TIMESTAMP[] = \"invalid dump timestamp format\";\n+\n+constexpr char ERR_INVAL_RECORD_PREAMBLE[] = \"invalid (sub-)record preamble\";\n+constexpr char ERR_INVAL_RECORD_BODY[] = \"invalid (sub-)record body\";\n+constexpr char ERR_INVAL_RECORD_TAG_POS[] = \"illegal position of a (sub-)record tag\";\n+constexpr char ERR_UNKNOWN_RECORD_TAG[] = \"unknown (sub-)record tag\";\n+\n+constexpr char ERR_REPEATED_ID[] = \"found a repeated ID\";\n+\n+\/\/ For logging.\n+const char *hprof_version2str(HeapDump::Version version) {\n+  switch (version) {\n+    case HeapDump::Version::V102:    return \"v1.0.2\";\n+    case HeapDump::Version::V101:    return \"v1.0.1\";\n+    case HeapDump::Version::UNKNOWN: return \"<unknown version>\";\n+    default: ShouldNotReachHere();   return nullptr;\n+  }\n+}\n+\n+static constexpr bool is_supported_id_size(u4 size) {\n+  return size == sizeof(u8) || size == sizeof(u4) || size == sizeof(u2) || size == sizeof(u1);\n+}\n+\n+class RecordsParser : public StackObj {\n+ public:\n+  RecordsParser(FileBasicTypeReader *reader, ParsedHeapDump *out, HeapDump::Version version, u4 id_size)\n+      : _reader(reader), _out(out), _version(version), _id_size(id_size) {\n+    precond(_reader != nullptr && _out != nullptr && _version != HeapDump::Version::UNKNOWN && is_supported_id_size(_id_size));\n+  }\n+\n+  const char *parse_records() {\n+    State state;\n+    const char *err_msg = nullptr;\n+\n+    log_debug(heapdump, parser)(\"Parsing records\");\n+\n+    while (!_reader->eos() && err_msg == nullptr) {\n+      switch (state.position()) {\n+        case State::Position::TOPLEVEL:\n+          err_msg = step_toplevel(&state);\n+          break;\n+        case State::Position::AMONG_HEAP_DUMP_SEGMENTS:\n+          err_msg = step_heap_segments(&state);\n+          break;\n+        case State::Position::IN_HEAP_DUMP_SEGMENT:\n+          precond(_version >= HeapDump::Version::V102);\n+        case State::Position::IN_HEAP_DUMP:\n+          err_msg = step_heap_dump(&state);\n+      }\n+    }\n+\n+    return err_msg;\n+  }\n+\n+ private:\n+  FileBasicTypeReader *_reader;\n+  ParsedHeapDump *_out;\n+\n+  HeapDump::Version _version;\n+  u4 _id_size;\n+\n+  ExtendableArray<char, u4> _sym_buf{1 * M};\n+\n+  \/\/ Monitors parsing state and correctness of its transitions.\n+  class State {\n+   public:\n+    enum class Position {\n+      \/\/ Parsing top-level records.\n+      TOPLEVEL,\n+      \/\/ Parsing HPROF_HEAP_DUMP subrecords.\n+      IN_HEAP_DUMP,\n+      \/\/ Parsing HPROF_HEAP_DUMP_SEGMENT subrecords.\n+      IN_HEAP_DUMP_SEGMENT,\n+      \/\/ Just finished parsing a HPROF_HEAP_DUMP_SEGMENT.\n+      AMONG_HEAP_DUMP_SEGMENTS\n+    };\n+\n+    \/\/ Where the parser currently is.\n+    Position position() const { return _position; }\n+\n+    \/\/ When found a HPROF_HEAP_DUMP.\n+    bool enter_heap_dump(u4 size) {\n+      if (position() != Position::TOPLEVEL) {\n+        log_error(heapdump, parser)(\"Illegal position transition: %s -> %s\",\n+                                   pos2str(position()), pos2str(Position::IN_HEAP_DUMP));\n+        return false;\n+      }\n+      precond(_remaining_record_size == 0);\n+\n+      if (size > 0) {\n+        log_debug(heapdump, parser)(\"Position transition: %s -> %s (size \" UINT32_FORMAT \")\",\n+                                   pos2str(position()), pos2str(Position::IN_HEAP_DUMP), size);\n+        _position = Position::IN_HEAP_DUMP;\n+        _remaining_record_size = size;\n+      } else {\n+        log_debug(heapdump, parser)(\"Got HPROF_HEAP_DUMP of size 0 -- no position transition\");\n+      }\n+\n+      return true;\n+    }\n+\n+    \/\/ When found a HPROF_HEAP_DUMP_SEGMENT.\n+    bool enter_heap_dump_segment(u4 size) {\n+      if (position() != Position::AMONG_HEAP_DUMP_SEGMENTS && position() != Position::TOPLEVEL) {\n+        log_error(heapdump, parser)(\"Illegal position transition: %s -> %s\",\n+                                   pos2str(position()), pos2str(Position::IN_HEAP_DUMP_SEGMENT));\n+        return false;\n+      }\n+      precond(_remaining_record_size == 0);\n+\n+      if (size > 0) {\n+        log_debug(heapdump, parser)(\"Position transition: %s -> %s (size \" UINT32_FORMAT \")\",\n+                                   pos2str(position()), pos2str(Position::IN_HEAP_DUMP_SEGMENT), size);\n+        _position = Position::IN_HEAP_DUMP_SEGMENT;\n+        _remaining_record_size = size;\n+      } else {\n+        log_debug(heapdump, parser)(\"Got HPROF_HEAP_DUMP_SEGMENT of size 0 -- position transition: %s -> %s\",\n+                                   pos2str(position()), pos2str(Position::AMONG_HEAP_DUMP_SEGMENTS));\n+        _position = Position::AMONG_HEAP_DUMP_SEGMENTS;\n+      }\n+\n+      return true;\n+    }\n+\n+    \/\/ When found a HPROF_HEAP_DUMP_END.\n+    bool exit_heap_dump_segments() {\n+      \/\/ Allow top-level position for sequences of zero segments\n+      if (position() != Position::AMONG_HEAP_DUMP_SEGMENTS && position() != Position::TOPLEVEL) {\n+        log_error(heapdump, parser)(\"Illegal position transition: %s -> %s\", pos2str(position()), pos2str(Position::TOPLEVEL));\n+        return false;\n+      }\n+      assert(_remaining_record_size == 0, \"must be 0 outside a record\");\n+      log_debug(heapdump, parser)(\"Position transition: %s -> %s\", pos2str(position()), pos2str(Position::TOPLEVEL));\n+      _position = Position::TOPLEVEL;\n+      return true;\n+    }\n+\n+    \/\/ When parsed the specified portion of the current record.\n+    bool reduce_remaining_record_size(u4 amount) {\n+      assert(position() != Position::TOPLEVEL && position() != Position::AMONG_HEAP_DUMP_SEGMENTS, \"must be inside a record\");\n+      assert(_remaining_record_size > 0, \"must be > 0 inside a record\");\n+\n+      if (_remaining_record_size < amount) {\n+        log_error(heapdump, parser)(\"Tried to read \" UINT32_FORMAT \" bytes \"\n+                                   \"from a subrecord with \" UINT32_FORMAT \" bytes left\",\n+                                   amount, _remaining_record_size);\n+        return false;\n+      }\n+\n+      _remaining_record_size -= amount;\n+\n+      if (_remaining_record_size == 0) {\n+        if (position() == Position::IN_HEAP_DUMP) {\n+          log_debug(heapdump, parser)(\"Position transition: %s -> %s\", pos2str(position()), pos2str(Position::TOPLEVEL));\n+          _position = Position::TOPLEVEL;\n+        } else if (position() == Position::IN_HEAP_DUMP_SEGMENT) {\n+          log_debug(heapdump, parser)(\"Position transition: %s -> %s\", pos2str(position()), pos2str(Position::AMONG_HEAP_DUMP_SEGMENTS));\n+          _position = Position::AMONG_HEAP_DUMP_SEGMENTS;\n+        } else {\n+          ShouldNotReachHere(); \/\/ We should be inside a record\n+        }\n+      }\n+\n+      return true;\n+    }\n+\n+   private:\n+    Position _position = Position::TOPLEVEL;\n+    u4 _remaining_record_size = 0;\n+\n+    \/\/ For logging.\n+    static const char *pos2str(Position position) {\n+      switch (position) {\n+        case Position::TOPLEVEL:                 return \"TOPLEVEL\";\n+        case Position::IN_HEAP_DUMP:             return \"IN_HEAP_DUMP\";\n+        case Position::IN_HEAP_DUMP_SEGMENT:     return \"IN_HEAP_DUMP_SEGMENT\";\n+        case Position::AMONG_HEAP_DUMP_SEGMENTS: return \"AMONG_HEAP_DUMP_SEGMENTS\";\n+        default: ShouldNotReachHere();           return nullptr;\n+      }\n+    }\n+  };\n+\n+  \/\/ High-level parsing\n+\n+  const char *step_toplevel(State *state) {\n+    precond(state->position() == State::Position::TOPLEVEL);\n+\n+    RecordPreamble preamble;\n+    if (!parse_record_preamble(&preamble)) {\n+      return ERR_INVAL_RECORD_PREAMBLE;\n+    }\n+    if (preamble.finish) {\n+      return nullptr;\n+    }\n+    log_trace(heapdump, parser)(\"Record (toplevel): tag \" UINT8_FORMAT_X_0 \", size \" UINT32_FORMAT,\n+                               preamble.tag, preamble.body_size);\n+\n+    Result body_res = Result::OK;\n+    switch (preamble.tag) {\n+      case HPROF_UTF8:\n+        body_res = parse_UTF8(preamble.body_size, &_out->utf8s);\n+        break;\n+      case HPROF_LOAD_CLASS:\n+        body_res = parse_load_class(preamble.body_size, &_out->load_classes);\n+        break;\n+      case HPROF_HEAP_DUMP:\n+        if (!state->enter_heap_dump(preamble.body_size)) {\n+          return ERR_INVAL_RECORD_TAG_POS;\n+        }\n+        break;\n+      case HPROF_HEAP_DUMP_SEGMENT:\n+        if (_version < HeapDump::Version::V102) {\n+          log_error(heapdump, parser)(\"HPROF_HEAP_DUMP_SEGMENT is not allowed in HPROF %s\", hprof_version2str(_version));\n+          return ERR_UNKNOWN_RECORD_TAG;\n+        }\n+        if (!state->enter_heap_dump_segment(preamble.body_size)) {\n+          return ERR_INVAL_RECORD_TAG_POS;\n+        }\n+        break;\n+      case HPROF_HEAP_DUMP_END:\n+        if (_version < HeapDump::Version::V102) {\n+          log_error(heapdump, parser)(\"HPROF_HEAP_DUMP_END is not allowed in HPROF %s\", hprof_version2str(_version));\n+          return ERR_UNKNOWN_RECORD_TAG;\n+        }\n+        if (preamble.body_size != 0) {\n+          log_error(heapdump, parser)(\n+              \"HPROF_HEAP_DUMP_END must have no body, \"\n+              \"but its preamble specifies it to have \" UINT32_FORMAT \" bytes\",\n+              preamble.body_size);\n+          return ERR_INVAL_RECORD_PREAMBLE;\n+        }\n+        \/\/ Assume this terminates a sequence of zero heap dump segments\n+        if (!state->exit_heap_dump_segments()) {\n+          return ERR_INVAL_RECORD_TAG_POS;\n+        }\n+        break;\n+      case HPROF_UNLOAD_CLASS:\n+      case HPROF_FRAME:\n+      case HPROF_TRACE:\n+      case HPROF_ALLOC_SITES:\n+      case HPROF_HEAP_SUMMARY:\n+      case HPROF_START_THREAD:\n+      case HPROF_END_THREAD:\n+      case HPROF_CPU_SAMPLES:\n+      case HPROF_CONTROL_SETTINGS:\n+        if (!_reader->skip(preamble.body_size)) {\n+          log_error(heapdump, parser)(\"Failed to read past a \" UINT8_FORMAT_X_0 \" tagged record body (\" UINT32_FORMAT \" bytes)\",\n+                                     preamble.tag, preamble.body_size);\n+          body_res = Result::FAILED;\n+        }\n+        break;\n+      default:\n+        log_error(heapdump, parser)(\"Unknown record tag: \" UINT8_FORMAT_X_0, preamble.tag);\n+        return ERR_UNKNOWN_RECORD_TAG;\n+    }\n+\n+    switch (body_res) {\n+      case Result::OK:\n+        return nullptr;\n+      case Result::FAILED:\n+        return ERR_INVAL_RECORD_BODY;\n+      case Result::REPEATED_ID:\n+        return ERR_REPEATED_ID;\n+      default:\n+        ShouldNotReachHere();\n+        return nullptr; \/\/ Make compilers happy\n+    }\n+  }\n+\n+  const char *step_heap_segments(State *state) {\n+    precond(state->position() == State::Position::AMONG_HEAP_DUMP_SEGMENTS);\n+    precond(_version >= HeapDump::Version::V102);\n+\n+    RecordPreamble preamble;\n+    if (!parse_record_preamble(&preamble)) {\n+      return ERR_INVAL_RECORD_PREAMBLE;\n+    }\n+    if (preamble.finish) {\n+      log_error(heapdump, parser)(\"Reached EOF, but HPROF_HEAP_DUMP_END was expected\");\n+      return ERR_INVAL_RECORD_PREAMBLE;\n+    }\n+    log_trace(heapdump, parser)(\"Record (heap segments): tag \" UINT8_FORMAT_X_0 \", size \" UINT32_FORMAT,\n+                               preamble.tag, preamble.body_size);\n+\n+    switch (preamble.tag) {\n+      case HPROF_HEAP_DUMP_SEGMENT:\n+        if (state->enter_heap_dump_segment(preamble.body_size)) {\n+          return nullptr;\n+        }\n+        return ERR_INVAL_RECORD_TAG_POS;\n+      case HPROF_HEAP_DUMP_END:\n+        if (preamble.body_size != 0) {\n+          log_error(heapdump, parser)(\n+              \"HPROF_HEAP_DUMP_END must have no body, \"\n+              \"but its preamble specifies it to have \" UINT32_FORMAT \" bytes\",\n+              preamble.body_size);\n+          return ERR_INVAL_RECORD_PREAMBLE;\n+        }\n+        if (state->exit_heap_dump_segments()) {\n+          return nullptr;\n+        }\n+        return ERR_INVAL_RECORD_TAG_POS;\n+      case HPROF_UTF8:\n+      case HPROF_LOAD_CLASS:\n+      case HPROF_UNLOAD_CLASS:\n+      case HPROF_FRAME:\n+      case HPROF_TRACE:\n+      case HPROF_ALLOC_SITES:\n+      case HPROF_HEAP_SUMMARY:\n+      case HPROF_HEAP_DUMP:\n+      case HPROF_START_THREAD:\n+      case HPROF_END_THREAD:\n+      case HPROF_CPU_SAMPLES:\n+      case HPROF_CONTROL_SETTINGS:\n+        log_error(heapdump, parser)(\"Record tag \" UINT8_FORMAT_X_0 \" is not allowed among heap dump segments\", preamble.tag);\n+        return ERR_INVAL_RECORD_TAG_POS;\n+      default:\n+        log_error(heapdump, parser)(\"Unknown record tag: \" UINT8_FORMAT_X_0, preamble.tag);\n+        return ERR_UNKNOWN_RECORD_TAG;\n+    }\n+  }\n+\n+  const char *step_heap_dump(State *state) {\n+    precond(state->position() == State::Position::IN_HEAP_DUMP ||\n+            state->position() == State::Position::IN_HEAP_DUMP_SEGMENT);\n+\n+    u1 tag;\n+    if (!parse_subrecord_tag(&tag) || !state->reduce_remaining_record_size(sizeof(u1))) {\n+      return ERR_INVAL_RECORD_PREAMBLE;\n+    }\n+    log_trace(heapdump, parser)(\"Subrecord: tag \" UINT8_FORMAT_X_0, tag);\n+\n+    Result body_res;\n+    u4 body_size;\n+    switch (tag) {\n+      case HPROF_GC_CLASS_DUMP:      body_res = parse_class_dump(     &_out->class_dumps,      &body_size); break;\n+      case HPROF_GC_INSTANCE_DUMP:   body_res = parse_instance_dump(  &_out->instance_dumps,   &body_size); break;\n+      case HPROF_GC_OBJ_ARRAY_DUMP:  body_res = parse_obj_array_dump( &_out->obj_array_dumps,  &body_size); break;\n+      case HPROF_GC_PRIM_ARRAY_DUMP: body_res = parse_prim_array_dump(&_out->prim_array_dumps, &body_size); break;\n+      default: \/\/ Other subrecord types are skipped\n+        switch (tag) {\n+          case HPROF_GC_ROOT_UNKNOWN:\n+          case HPROF_GC_ROOT_STICKY_CLASS:\n+          case HPROF_GC_ROOT_MONITOR_USED:\n+            body_size = _id_size;\n+            break;\n+          case HPROF_GC_ROOT_JNI_GLOBAL:\n+            body_size = 2 * _id_size;\n+            break;\n+          case HPROF_GC_ROOT_JNI_LOCAL:\n+          case HPROF_GC_ROOT_JAVA_FRAME:\n+          case HPROF_GC_ROOT_THREAD_OBJ:\n+            body_size = _id_size + 2 * sizeof(u4);\n+            break;\n+          case HPROF_GC_ROOT_NATIVE_STACK:\n+          case HPROF_GC_ROOT_THREAD_BLOCK:\n+            body_size = _id_size + sizeof(u4);\n+            break;\n+          default:\n+            return ERR_UNKNOWN_RECORD_TAG;\n+        }\n+        body_res = _reader->skip(body_size) ? Result::OK : Result::FAILED;\n+        if (body_res == Result::FAILED) {\n+          log_error(heapdump, parser)(\"Failed to read past a \" UINT8_FORMAT_X_0 \" tagged subrecord body (\" UINT32_FORMAT \" bytes)\",\n+                                     tag, body_size);\n+        }\n+    }\n+\n+    switch (body_res) {\n+      case Result::OK:\n+        if (state->reduce_remaining_record_size(body_size)) {\n+          return nullptr;\n+        }\n+        return ERR_INVAL_RECORD_BODY;\n+      case Result::FAILED:\n+        return ERR_INVAL_RECORD_BODY;\n+      case Result::REPEATED_ID:\n+        return ERR_REPEATED_ID;\n+      default:\n+        ShouldNotReachHere();\n+        return nullptr;\n+    }\n+  }\n+\n+  \/\/ (Sub-)record preamble parsing\n+\n+  struct RecordPreamble {\n+    bool finish;\n+    u1 tag;\n+    u4 body_size;\n+  };\n+\n+  bool parse_record_preamble(RecordPreamble *preamble) {\n+    if (!_reader->read(&preamble->tag)) {\n+      if (_reader->eos()) {\n+        preamble->finish = true;\n+        return true;\n+      }\n+      log_error(heapdump, parser)(\"Failed to read a record tag\");\n+      return false;\n+    }\n+    preamble->finish = false;\n+    if (!_reader->skip(sizeof(u4)) || !_reader->read(&preamble->body_size)) {\n+      log_error(heapdump, parser)(\"Failed to parse a record preamble after tag \" UINT8_FORMAT_X_0, preamble->tag);\n+      return false;\n+    }\n+    return true;\n+  }\n+\n+  bool parse_subrecord_tag(u1 *tag) {\n+    if (_reader->read(tag)) {\n+      return true;\n+    }\n+    log_error(heapdump, parser)(\"Failed to read a subrecord tag\");\n+    return false;\n+  }\n+\n+  \/\/ (Sub-)record body parsing\n+\n+  enum class Result {\n+    OK,\n+    \/\/ Parsing failed because the record was ill-formatted.\n+    FAILED,\n+    \/\/ A record with the same ID has been already parsed before.\n+    REPEATED_ID\n+  };\n+\n+#define ALLOC_NEW_RECORD(hashtable, id, record_group_name)                                                          \\\n+  bool is_new;                                                                                                      \\\n+  auto *record = (hashtable)->put_if_absent(id, &is_new);                                                           \\\n+  if (!is_new) {                                                                                                    \\\n+    log_error(heapdump, parser)(\"Multiple occurences of ID \" UINT64_FORMAT \" in %s records\", id, record_group_name); \\\n+    return Result::REPEATED_ID;                                                                                     \\\n+  }                                                                                                                 \\\n+  (hashtable)->maybe_grow()\n+\n+#define READ_INTO_OR_FAIL(ptr, what)                         \\\n+  do {                                                       \\\n+    if (!_reader->read(ptr)) {                               \\\n+      log_error(heapdump, parser)(\"Failed to read %s\", what); \\\n+      return Result::FAILED;                                 \\\n+    }                                                        \\\n+  } while (false)\n+\n+#define READ_OR_FAIL(type, var, what) \\\n+  type var;                           \\\n+  READ_INTO_OR_FAIL(&(var), what)\n+\n+#define READ_ID_INTO_OR_FAIL(ptr, what)                      \\\n+  do {                                                       \\\n+    if (!_reader->read_uint(ptr, _id_size)) {                \\\n+      log_error(heapdump, parser)(\"Failed to read %s\", what); \\\n+      return Result::FAILED;                                 \\\n+    }                                                        \\\n+  } while (false)\n+\n+#define READ_ID_OR_FAIL(var, what) \\\n+  HeapDump::ID var;                     \\\n+  READ_ID_INTO_OR_FAIL(&(var), what)\n+\n+  Result parse_UTF8(u4 size, decltype(ParsedHeapDump::utf8s) *out) {\n+    if (size < _id_size) {\n+      log_error(heapdump, parser)(\"Too small size specified for HPROF_UTF8\");\n+      return Result::FAILED;\n+    }\n+\n+    READ_ID_OR_FAIL(id, \"HPROF_UTF8 ID\");\n+    ALLOC_NEW_RECORD(out, id, \"HPROF_UTF8\");\n+    record->id = id;\n+\n+    u4 sym_size = size - _id_size;\n+    if (sym_size > INT_MAX) {\n+      \/\/ SymbolTable::new_symbol() takes length as an int\n+      log_error(heapdump, parser)(\"HPROF_UTF8 symbol is too large for the symbol table: \" UINT32_FORMAT \" > %i\",\n+                                 sym_size, INT_MAX);\n+      return Result::FAILED;\n+    }\n+    if (sym_size > _sym_buf.size()) {\n+      _sym_buf.extend(sym_size);\n+    }\n+\n+    if (!_reader->read_raw(_sym_buf.mem(), sym_size)) {\n+      log_error(heapdump, parser)(\"Failed to read HPROF_UTF8 symbol bytes\");\n+      return Result::FAILED;\n+    }\n+\n+    record->sym = TempNewSymbol(SymbolTable::new_symbol(_sym_buf.mem(), static_cast<int>(sym_size)));\n+\n+    return Result::OK;\n+  }\n+\n+  Result parse_load_class(u4 size, decltype(ParsedHeapDump::load_classes) *out) {\n+    if (size != 2 * (sizeof(u4) + _id_size)) {\n+      log_error(heapdump, parser)(\"Too small size specified for HPROF_LOAD_CLASS\");\n+      return Result::FAILED;\n+    }\n+\n+    READ_OR_FAIL(u4, serial, \"HPROF_LOAD_CLASS serial\");\n+    READ_ID_OR_FAIL(class_id, \"HPROF_LOAD_CLASS class ID\");\n+\n+    ALLOC_NEW_RECORD(out, class_id, \"HPROF_LOAD_CLASS\");\n+    record->serial = serial;\n+    record->class_id = class_id;\n+\n+    READ_INTO_OR_FAIL(&record->stack_trace_serial, \"HPROF_LOAD_CLASS stack trace serial\");\n+    READ_ID_INTO_OR_FAIL(&record->class_name_id, \"HPROF_LOAD_CLASS class name ID\");\n+\n+    return Result::OK;\n+  }\n+\n+  bool read_basic_value(u1 type, HeapDump::BasicValue *value_out, u4 *size_out) {\n+    switch (type) {\n+      case HPROF_NORMAL_OBJECT:\n+        if (!_reader->read_uint(&value_out->as_object_id, _id_size)) return false;\n+        *size_out = _id_size;\n+        break;\n+      case HPROF_BOOLEAN:\n+        if (!_reader->read(&value_out->as_boolean)) return false;\n+        *size_out = sizeof(value_out->as_boolean);\n+        break;\n+      case HPROF_CHAR:\n+        if (!_reader->read(&value_out->as_char)) return false;\n+        *size_out = sizeof(value_out->as_char);\n+        break;\n+      case HPROF_FLOAT:\n+        if (!_reader->read(&value_out->as_float)) return false;\n+        *size_out = sizeof(value_out->as_float);\n+        break;\n+      case HPROF_DOUBLE:\n+        if (!_reader->read(&value_out->as_double)) return false;\n+        *size_out = sizeof(value_out->as_double);\n+        break;\n+      case HPROF_BYTE:\n+        if (!_reader->read(&value_out->as_byte)) return false;\n+        *size_out = sizeof(value_out->as_byte);\n+        break;\n+      case HPROF_SHORT:\n+        if (!_reader->read(&value_out->as_short)) return false;\n+        *size_out = sizeof(value_out->as_short);\n+        break;\n+      case HPROF_INT:\n+        if (!_reader->read(&value_out->as_int)) return false;\n+        *size_out = sizeof(value_out->as_int);\n+        break;\n+      case HPROF_LONG:\n+        if (!_reader->read(&value_out->as_long)) return false;\n+        *size_out = sizeof(value_out->as_long);\n+        break;\n+      default:\n+        *size_out = 0;\n+    }\n+    return true;\n+  }\n+\n+  Result parse_class_dump(decltype(ParsedHeapDump::class_dumps) *out, u4 *record_size) {\n+    \/\/ Array sizes will be added dynamically\n+    *record_size = 7 * _id_size + 2 * sizeof(u4) + 3 * sizeof(u2);\n+\n+    READ_ID_OR_FAIL(id, \"HPROF_GC_CLASS_DUMP ID\");\n+\n+    ALLOC_NEW_RECORD(out, id, \"HPROF_GC_CLASS_DUMP\");\n+    assert(record->constant_pool.size() == 0 && record->static_fields.size() == 0 && record->instance_field_infos.size() == 0,\n+           \"newly allocated record must be empty\");\n+    record->id = id;\n+\n+    READ_INTO_OR_FAIL(&record->stack_trace_serial, \"HPROF_GC_CLASS_DUMP stack trace serial\");\n+    READ_ID_INTO_OR_FAIL(&record->super_id, \"HPROF_GC_CLASS_DUMP super ID\");\n+    READ_ID_INTO_OR_FAIL(&record->class_loader_id, \"HPROF_GC_CLASS_DUMP class loader ID\");\n+    READ_ID_INTO_OR_FAIL(&record->signers_id, \"HPROF_GC_CLASS_DUMP signers ID\");\n+    READ_ID_INTO_OR_FAIL(&record->protection_domain_id, \"HPROF_GC_CLASS_DUMP protection domain ID\");\n+\n+    \/\/ Reserved\n+    if (!_reader->skip(2 * _id_size)) {\n+      log_error(heapdump, parser)(\"Failed to read past reserved fields of HPROF_GC_CLASS_DUMP\");\n+      return Result::FAILED;\n+    }\n+\n+    READ_INTO_OR_FAIL(&record->instance_size, \"HPROF_GC_CLASS_DUMP instance size\");\n+\n+    READ_OR_FAIL(u2, constant_pool_size, \"HPROF_GC_CLASS_DUMP constant pool size\");\n+    record->constant_pool.extend(constant_pool_size);\n+    for (u2 i = 0; i < constant_pool_size; i++) {\n+      auto &constant = record->constant_pool[i];\n+      READ_INTO_OR_FAIL(&constant.index, \"HPROF_GC_CLASS_DUMP constant index\");\n+      READ_INTO_OR_FAIL(&constant.type, \"HPROF_GC_CLASS_DUMP constant type\");\n+\n+      u4 value_size;\n+      if (!read_basic_value(constant.type, &constant.value, &value_size)) {\n+        log_error(heapdump, parser)(\"Failed to read a constant's value in HPROF_GC_CLASS_DUMP\");\n+        return Result::FAILED;\n+      }\n+      if (value_size == 0) {\n+        log_error(heapdump, parser)(\"Unknown constant type in HPROF_GC_CLASS_DUMP: \" UINT8_FORMAT_X_0, constant.type);\n+        return Result::FAILED;\n+      }\n+      *record_size += sizeof(u2) + sizeof(u1) + value_size;\n+    }\n+\n+    READ_OR_FAIL(u2, static_fields_num, \"HPROF_GC_CLASS_DUMP static fields number\");\n+    record->static_fields.extend(static_fields_num);\n+    for (u2 i = 0; i < static_fields_num; i++) {\n+      auto &field = record->static_fields[i];\n+      READ_ID_INTO_OR_FAIL(&field.info.name_id, \"HPROF_GC_CLASS_DUMP static field name ID\");\n+      READ_INTO_OR_FAIL(&field.info.type, \"HPROF_GC_CLASS_DUMP static field type\");\n+\n+      u4 value_size;\n+      if (!read_basic_value(field.info.type, &field.value, &value_size)) {\n+        log_error(heapdump, parser)(\"Failed to read a static field's value in HPROF_GC_CLASS_DUMP\");\n+        return Result::FAILED;\n+      }\n+      if (value_size == 0) {\n+        log_error(heapdump, parser)(\"Unknown static field type in HPROF_GC_CLASS_DUMP: \" UINT8_FORMAT_X_0, field.info.type);\n+        return Result::FAILED;\n+      }\n+      *record_size += _id_size + sizeof(u1) + value_size;\n+    }\n+\n+    READ_OR_FAIL(u2, instance_fields_num, \"HPROF_GC_CLASS_DUMP instance fields number\");\n+    record->instance_field_infos.extend(instance_fields_num);\n+    for (u2 i = 0; i < instance_fields_num; i++) {\n+      auto &field_info = record->instance_field_infos[i];\n+      READ_ID_INTO_OR_FAIL(&field_info.name_id, \"HPROF_GC_CLASS_DUMP instance field name ID\");\n+      READ_INTO_OR_FAIL(&field_info.type, \"HPROF_GC_CLASS_DUMP instance field type\");\n+    }\n+    *record_size += instance_fields_num * (_id_size + sizeof(u1));\n+\n+    return Result::OK;\n+  }\n+\n+  Result parse_instance_dump(decltype(ParsedHeapDump::instance_dumps) *out, u4 *record_size) {\n+    READ_ID_OR_FAIL(id, \"HPROF_GC_INSTANCE_DUMP ID\");\n+\n+    ALLOC_NEW_RECORD(out, id, \"HPROF_GC_INSTANCE_DUMP\");\n+    assert(record->fields_data.size() == 0, \"newly allocated record must be empty\");\n+    record->id = id;\n+\n+    READ_INTO_OR_FAIL(&record->stack_trace_serial, \"HPROF_GC_INSTANCE_DUMP stack trace serial\");\n+    READ_ID_INTO_OR_FAIL(&record->class_id, \"HPROF_GC_INSTANCE_DUMP class ID\");\n+\n+    READ_OR_FAIL(u4, fields_data_size, \"HPROF_GC_INSTANCE_DUMP fields data size\");\n+    record->fields_data.extend(fields_data_size);\n+    if (!_reader->read_raw(record->fields_data.mem(), fields_data_size)) {\n+      log_error(heapdump, parser)(\"Failed to read HPROF_GC_INSTANCE_DUMP fields data\");\n+      return Result::FAILED;\n+    }\n+\n+    *record_size = 2 * _id_size + 2 * sizeof(u4) + fields_data_size;\n+    return Result::OK;\n+  }\n+\n+  Result parse_obj_array_dump(decltype(ParsedHeapDump::obj_array_dumps) *out, u4 *record_size) {\n+    READ_ID_OR_FAIL(id, \"HPROF_GC_OBJ_ARRAY_DUMP ID\");\n+\n+    ALLOC_NEW_RECORD(out, id, \"HPROF_GC_OBJ_ARRAY_DUMP\");\n+    assert(record->elem_ids.size() == 0, \"newly allocated record must be empty\");\n+    record->id = id;\n+\n+    READ_INTO_OR_FAIL(&record->stack_trace_serial, \"HPROF_GC_OBJ_ARRAY_DUMP stack trace serial\");\n+    READ_OR_FAIL(u4, elems_num, \"HPROF_GC_OBJ_ARRAY_DUMP elements number\");\n+    READ_ID_INTO_OR_FAIL(&record->array_class_id, \"HPROF_GC_OBJ_ARRAY_DUMP array class ID\");\n+\n+    record->elem_ids.extend(elems_num);\n+    for (u4 i = 0; i < elems_num; i++) {\n+      READ_ID_INTO_OR_FAIL(&record->elem_ids[i], \"HPROF_GC_OBJ_ARRAY_DUMP element ID\");\n+    }\n+    *record_size = 2 * _id_size + 2 * sizeof(u4) + elems_num * _id_size;\n+\n+    return Result::OK;\n+  }\n+\n+  Result parse_prim_array_dump(decltype(ParsedHeapDump::prim_array_dumps) *out, u4 *record_size) {\n+    READ_ID_OR_FAIL(id, \"HPROF_GC_PRIM_ARRAY_DUMP ID\");\n+\n+    ALLOC_NEW_RECORD(out, id, \"HPROF_GC_PRIM_ARRAY_DUMP\");\n+    assert(record->elems_data.size() == 0, \"newly allocated record must be empty\");\n+    record->id = id;\n+\n+    READ_INTO_OR_FAIL(&record->stack_trace_serial, \"HPROF_GC_PRIM_ARRAY_DUMP stack trace serial\");\n+    READ_INTO_OR_FAIL(&record->elems_num, \"HPROF_GC_PRIM_ARRAY_DUMP elements number\");\n+    READ_INTO_OR_FAIL(&record->elem_type, \"HPROF_GC_PRIM_ARRAY_DUMP element type\");\n+\n+    const BasicType elem_type = HeapDump::htype2btype(record->elem_type);\n+    if (elem_type == T_ILLEGAL) {\n+      log_error(heapdump, parser)(\"Unknown element type in HPROF_GC_PRIM_ARRAY_DUMP: \" UINT8_FORMAT_X_0, record->elem_type);\n+      return Result::FAILED;\n+    }\n+    if (!is_java_primitive(elem_type)) {\n+      log_error(heapdump, parser)(\"Illegal element type in HPROF_GC_PRIM_ARRAY_DUMP: \" UINT8_FORMAT_X_0, record->elem_type);\n+      return Result::FAILED;\n+    }\n+    u1 elem_size = type2aelembytes(elem_type);\n+    u4 elems_data_size = record->elems_num * elem_size;\n+\n+    record->elems_data.extend(elems_data_size);\n+    if (!Endian::is_Java_byte_ordering_different() || elem_size == 1) {\n+      \/\/ Can save the data as is\n+      if (!_reader->read_raw(record->elems_data.mem(), elems_data_size)) {\n+        log_error(heapdump, parser)(\"Failed to read HPROF_GC_PRIM_ARRAY_DUMP elements data\");\n+        return Result::FAILED;\n+      }\n+    } else {\n+      \/\/ Have to save each value byte-wise backwards\n+      for (u1 *elem = record->elems_data.mem(); elem < record->elems_data.mem() + elems_data_size; elem += elem_size) {\n+        for (u1 *byte = elem + elem_size - 1; byte >= elem; byte--) {\n+          READ_INTO_OR_FAIL(byte, \"HPROF_GC_PRIM_ARRAY_DUMP elements data\");\n+        }\n+      }\n+    }\n+\n+    *record_size = _id_size + 2 * sizeof(u4) + sizeof(u1) + elems_data_size;\n+    return Result::OK;\n+  }\n+\n+#undef ALLOC_NEW_RECORD\n+#undef READ_INTO_OR_FAIL\n+#undef READ_OR_FAIL\n+#undef READ_ID_INTO_OR_FAIL\n+#undef READ_ID_OR_FAIL\n+};\n+\n+static HeapDump::Version parse_header(BasicTypeReader *reader) {\n+  constexpr char HEADER_STR_102[] = \"JAVA PROFILE 1.0.2\";\n+  constexpr char HEADER_STR_101[] = \"JAVA PROFILE 1.0.1\";\n+  STATIC_ASSERT(sizeof(HEADER_STR_102) == sizeof(HEADER_STR_101));\n+\n+  char header_str[sizeof(HEADER_STR_102)];\n+  if (!reader->read_raw(header_str, sizeof(header_str))) {\n+    log_error(heapdump, parser)(\"Failed to read header string\");\n+    return HeapDump::Version::UNKNOWN;\n+  }\n+  header_str[sizeof(header_str) - 1] = '\\0'; \/\/ Ensure nul-terminated\n+\n+  if (strcmp(header_str, HEADER_STR_102) == 0) {\n+    return HeapDump::Version::V102;\n+  }\n+  if (strcmp(header_str, HEADER_STR_101) == 0) {\n+    return HeapDump::Version::V101;\n+  }\n+\n+  log_error(heapdump, parser)(\"Unknown header string: %s\", header_str);\n+  return HeapDump::Version::UNKNOWN;\n+}\n+\n+static const char *parse_id_size(BasicTypeReader *reader, u4 *out) {\n+  if (!reader->read(out)) {\n+    log_error(heapdump, parser)(\"Failed to read ID size\");\n+    return ERR_INVAL_ID_SIZE;\n+  }\n+  if (!is_supported_id_size(*out)) {\n+    log_error(heapdump, parser)(\"ID size \" UINT32_FORMAT \" is not supported -- use 1, 2, 4, or 8\", *out);\n+    return ERR_UNSUPPORTED_ID_SIZE;\n+  }\n+  return nullptr;\n+}\n+\n+const char *HeapDumpParser::parse(const char *path, ParsedHeapDump *out) {\n+  guarantee(path != nullptr, \"cannot parse from null path\");\n+  guarantee(out != nullptr, \"cannot save results into null container\");\n+\n+  log_info(heapdump, parser)(\"Started parsing heap dump %s\", path);\n+  TraceTime timer(\"Heap dump parsing timer\", TRACETIME_LOG(Info, heapdump, parser));\n+\n+  FileBasicTypeReader reader;\n+  if (!reader.open(path)) {\n+    log_error(heapdump, parser)(\"Failed to open %s: %s\", path, os::strerror(errno));\n+    return os::strerror(errno);\n+  }\n+\n+  HeapDump::Version version = parse_header(&reader);\n+  if (version == HeapDump::Version::UNKNOWN) {\n+    return ERR_INVAL_HEADER_STR;\n+  }\n+  log_debug(heapdump, parser)(\"HPROF version: %s\", hprof_version2str(version));\n+\n+  u4 id_size;\n+  const char *err_msg = parse_id_size(&reader, &id_size);\n+  if (err_msg != nullptr) {\n+    return err_msg;\n+  }\n+  log_debug(heapdump, parser)(\"ID size: \" UINT32_FORMAT, id_size);\n+  out->id_size = id_size;\n+\n+  \/\/ Skip dump timestamp\n+  if (!reader.skip(2 * sizeof(u4))) {\n+    log_error(heapdump, parser)(\"Failed to read past heap dump timestamp\");\n+    return ERR_INVAL_DUMP_TIMESTAMP;\n+  }\n+\n+  err_msg = RecordsParser(&reader, out, version, id_size).parse_records();\n+  if (err_msg == nullptr) {\n+    log_info(heapdump, parser)(\"Successfully parsed %s\", path);\n+  } else {\n+    log_info(heapdump, parser)(\"Position in %s after error: %zu\", path, reader.pos());\n+  }\n+  return err_msg;\n+}\n+\n+\n+\/\/ Reads from the specified address.\n+class AddressBasicTypeReader : public BasicTypeReader {\n+ public:\n+  AddressBasicTypeReader(const void *from, size_t max_size) : _from(from), _max_size(max_size) {}\n+\n+  bool read_raw(void *buf, size_t size) override {\n+    precond(buf != nullptr || size == 0);\n+    if (size > _max_size) {\n+      return false;\n+    }\n+    memcpy(buf, _from, size);\n+    return true;\n+  }\n+\n+  \/\/ Reading from a specific address so this does not make sense.\n+  bool skip(size_t size) override { ShouldNotCallThis(); return false; }\n+  size_t pos() const override     { return 0; }\n+  bool eos() const override       { return _max_size == 0; }\n+\n+ private:\n+  const void *_from;\n+  size_t _max_size;\n+};\n+\n+HeapDump::BasicValue HeapDump::InstanceDump::read_field(u4 offset, BasicType type, u4 id_size) const {\n+  AddressBasicTypeReader reader(&fields_data[offset], fields_data.size() - offset);\n+  HeapDump::BasicValue val;\n+  switch (type) {\n+    case T_OBJECT:\n+    case T_ARRAY:   guarantee(reader.read_uint(&val.as_object_id, id_size), \"out of bounds\"); break;\n+    case T_BOOLEAN: guarantee(reader.read(&val.as_boolean)                , \"out of bounds\"); break;\n+    case T_CHAR:    guarantee(reader.read(&val.as_char)                   , \"out of bounds\"); break;\n+    case T_FLOAT:   guarantee(reader.read(&val.as_float)                  , \"out of bounds\"); break;\n+    case T_DOUBLE:  guarantee(reader.read(&val.as_double)                 , \"out of bounds\"); break;\n+    case T_BYTE:    guarantee(reader.read(&val.as_byte)                   , \"out of bounds\"); break;\n+    case T_SHORT:   guarantee(reader.read(&val.as_short)                  , \"out of bounds\"); break;\n+    case T_INT:     guarantee(reader.read(&val.as_int)                    , \"out of bounds\"); break;\n+    case T_LONG:    guarantee(reader.read(&val.as_long)                   , \"out of bounds\"); break;\n+    default:        ShouldNotReachHere();\n+  }\n+  return val;\n+}\n+\n+\n+void DumpedInstanceFieldStream::next() {\n+  precond(!eos());\n+  _field_offset += HeapDump::value_size(type(), _heap_dump.id_size);\n+  _field_index++;\n+}\n+\n+bool DumpedInstanceFieldStream::eos() {\n+  if (_field_index < _current_class_dump->instance_field_infos.size()) {\n+    return false;\n+  }\n+  if (_current_class_dump->super_id == HeapDump::NULL_ID) {\n+    return true;\n+  }\n+  _field_index = 0;\n+  _current_class_dump = &_heap_dump.get_class_dump(_current_class_dump->super_id);\n+  return eos(); \/\/ Check again to skip the class if it has no non-static fields\n+}\n+\n+Symbol *DumpedInstanceFieldStream::name() const {\n+  precond(_field_index < _current_class_dump->instance_field_infos.size());\n+  const HeapDump::ID name_id = _current_class_dump->instance_field_infos[_field_index].name_id;\n+  return _heap_dump.get_symbol(name_id);\n+}\n+\n+BasicType DumpedInstanceFieldStream::type() const {\n+  precond(_field_index < _current_class_dump->instance_field_infos.size());\n+  const u1 t = _current_class_dump->instance_field_infos[_field_index].type;\n+  return HeapDump::htype2btype(t);\n+}\n+\n+HeapDump::BasicValue DumpedInstanceFieldStream::value() const {\n+  const BasicType t = type();\n+  guarantee(_field_offset + HeapDump::value_size(t, _heap_dump.id_size) <= _instance_dump.fields_data.size(),\n+            \"object \" HDID_FORMAT \" has less non-static fields' data dumped than specified by its direct class and super-classes: \"\n+            \"read \" UINT32_FORMAT \" bytes and expect at least \" UINT32_FORMAT \" more to read %s value, but only \" UINT32_FORMAT \" bytes left\",\n+            _instance_dump.id, _field_offset, HeapDump::value_size(t, _heap_dump.id_size), type2name(type()),\n+            _instance_dump.fields_data.size() - _field_offset);\n+  return _instance_dump.read_field(_field_offset, t, _heap_dump.id_size);\n+}\n","filename":"src\/hotspot\/share\/utilities\/heapDumpParser.cpp","additions":898,"deletions":0,"binary":false,"changes":898,"status":"added"},{"patch":"@@ -0,0 +1,213 @@\n+#ifndef SHARE_UTILITIES_HEAPDUMPPARSER_HPP\n+#define SHARE_UTILITIES_HEAPDUMPPARSER_HPP\n+\n+#include \"memory\/allocation.hpp\"\n+#include \"oops\/symbolHandle.hpp\"\n+#include \"utilities\/debug.hpp\"\n+#include \"utilities\/extendableArray.hpp\"\n+#include \"utilities\/globalDefinitions.hpp\"\n+#include \"utilities\/hprofTag.hpp\"\n+#include \"utilities\/resizeableResourceHash.hpp\"\n+\n+\/\/ Format for heap dump IDs.\n+#define HDID_FORMAT UINT64_FORMAT\n+\n+\/\/ Relevant HPROF records. See HPROF binary format for details.\n+struct HeapDump : AllStatic {\n+  \/\/ Assuming HPROF ID type fits into 8 bytes. This is checked when parsing.\n+  using ID = u8;\n+  \/\/ Represents a null object reference (this is a convention and not a part of\n+  \/\/ HPROF specification).\n+  static constexpr ID NULL_ID = 0;\n+\n+  enum class Version { UNKNOWN, V101, V102 };\n+\n+  union BasicValue {\n+    ID as_object_id;\n+    jboolean as_boolean;\n+    jchar as_char;\n+    jfloat as_float;\n+    jdouble as_double;\n+    jbyte as_byte;\n+    jshort as_short;\n+    jint as_int;\n+    jlong as_long;\n+  };\n+\n+  struct UTF8 {\n+    ID id;\n+    TempNewSymbol sym;\n+  };\n+\n+  struct LoadClass {\n+    u4 serial;\n+    ID class_id;\n+    u4 stack_trace_serial;\n+    ID class_name_id;\n+  };\n+\n+  struct ClassDump {\n+    struct ConstantPoolEntry {\n+      u2 index;\n+      u1 type;\n+      BasicValue value;\n+    };\n+\n+    struct Field {\n+      struct Info {\n+        ID name_id;\n+        u1 type;\n+      } info;\n+      BasicValue value;\n+    };\n+\n+    ID id;\n+    u4 stack_trace_serial;\n+    ID super_id;\n+    ID class_loader_id;\n+    ID signers_id;\n+    ID protection_domain_id;\n+\n+    u4 instance_size;\n+\n+    ExtendableArray<ConstantPoolEntry, u2> constant_pool = {}; \/\/ = {} allows aggregate initialization without missing-field-initializers warnings\n+    ExtendableArray<Field, u2> static_fields = {};\n+    ExtendableArray<Field::Info, u2> instance_field_infos = {};\n+  };\n+\n+  struct InstanceDump {\n+    ID id;\n+    u4 stack_trace_serial;\n+    ID class_id;\n+    \/\/ Raw binary data: use read_field() to read it in the correct byte order.\n+    ExtendableArray<u1, u4> fields_data = {};\n+\n+    \/\/ Reads a field from fields data. The caller is responsible for providing\n+    \/\/ the right offset and type.\n+    BasicValue read_field(u4 offset, BasicType type, u4 id_size) const;\n+  };\n+\n+  struct ObjArrayDump {\n+    ID id;\n+    u4 stack_trace_serial;\n+    ID array_class_id;\n+    ExtendableArray<ID, u4> elem_ids = {};\n+  };\n+\n+  struct PrimArrayDump {\n+    ID id;\n+    u4 stack_trace_serial;\n+    u4 elems_num;\n+    u1 elem_type;\n+    \/\/ Elements data, already in the correct byte order.\n+    ExtendableArray<u1, u8> elems_data = {}; \/\/ u8 to index 2^32 (u4 holds # of elems) * 8 (max elem size) bytes\n+  };\n+\n+  static constexpr BasicType htype2btype(u1 hprof_type) {\n+    switch (hprof_type) {\n+      case HPROF_BOOLEAN:       return T_BOOLEAN;\n+      case HPROF_CHAR:          return T_CHAR;\n+      case HPROF_FLOAT:         return T_FLOAT;\n+      case HPROF_DOUBLE:        return T_DOUBLE;\n+      case HPROF_BYTE:          return T_BYTE;\n+      case HPROF_SHORT:         return T_SHORT;\n+      case HPROF_INT:           return T_INT;\n+      case HPROF_LONG:          return T_LONG;\n+      case HPROF_NORMAL_OBJECT: return T_OBJECT;\n+      default:                  return T_ILLEGAL; \/\/ Includes HPROF_ARRAY_OBJECT which is not used\n+    }\n+  }\n+\n+  static u4 value_size(BasicType btype, u4 id_size) {\n+    precond(is_java_type(btype));\n+    return is_java_primitive(btype) ? type2aelembytes(btype) : id_size;\n+  }\n+};\n+\n+template <class V, AnyObj::allocation_type ALLOC_TYPE = AnyObj::RESOURCE_AREA>\n+using HeapDumpTable = ResizeableResourceHashtable<HeapDump::ID, V, ALLOC_TYPE>;\n+\n+struct ParsedHeapDump : public CHeapObj<mtInternal> {\n+  template <class V>\n+  using RecordTable = HeapDumpTable<V, AnyObj::C_HEAP \/*for destructors to be called*\/>;\n+\n+  \/\/ Actual size of IDs in the dump.\n+  u4 id_size;\n+\n+  RecordTable<HeapDump::UTF8>          utf8s            {INITIAL_TABLE_SIZE, MAX_TABLE_SIZE};\n+  RecordTable<HeapDump::LoadClass>     load_classes     {INITIAL_TABLE_SIZE, MAX_TABLE_SIZE};\n+  RecordTable<HeapDump::ClassDump>     class_dumps      {INITIAL_TABLE_SIZE, MAX_TABLE_SIZE};\n+  RecordTable<HeapDump::InstanceDump>  instance_dumps   {INITIAL_TABLE_SIZE, MAX_TABLE_SIZE};\n+  RecordTable<HeapDump::ObjArrayDump>  obj_array_dumps  {INITIAL_TABLE_SIZE, MAX_TABLE_SIZE};\n+  RecordTable<HeapDump::PrimArrayDump> prim_array_dumps {INITIAL_TABLE_SIZE, MAX_TABLE_SIZE};\n+\n+  Symbol *get_symbol(HeapDump::ID id) const {\n+    const HeapDump::UTF8 *utf8 = utf8s.get(id);\n+    guarantee(utf8 != nullptr, \"UTF-8 record \" HDID_FORMAT \" is not in the heap dump\", id);\n+    assert(utf8->sym != nullptr, \"must be\");\n+    return utf8->sym;\n+  }\n+\n+  Symbol *get_class_name(HeapDump::ID class_id) const {\n+    const HeapDump::LoadClass *lc = load_classes.get(class_id);\n+    guarantee(lc != nullptr, \"LoadClass record \" HDID_FORMAT \" is not in the heap dump\", class_id);\n+    Symbol *const name = get_symbol(lc->class_name_id);\n+    return name;\n+  }\n+\n+  const HeapDump::ClassDump &get_class_dump(HeapDump::ID id) const {\n+    HeapDump::ClassDump *const dump = class_dumps.get(id);\n+    guarantee(dump != nullptr, \"ClassDump record \" HDID_FORMAT \" is not in the heap dump\", id);\n+    return *dump;\n+  }\n+\n+  const HeapDump::InstanceDump &get_instance_dump(HeapDump::ID id) const {\n+    HeapDump::InstanceDump *const dump = instance_dumps.get(id);\n+    guarantee(dump != nullptr, \"InstanceDump record \" HDID_FORMAT \" is not in the heap dump\", id);\n+    return *dump;\n+  }\n+\n+ private:\n+  \/\/ Odd primes picked from ResizeableResourceHashtable.cpp\n+  static const int INITIAL_TABLE_SIZE = 1009;\n+  static const int MAX_TABLE_SIZE = 1228891;\n+};\n+\n+\/\/ Reads field values from an instance dump.\n+\/\/\n+\/\/ Usage:\n+\/\/\n+\/\/    for (DumpedInstanceFieldStream st(heap_dump, inst_dump); !st.eos(); st.next()) {\n+\/\/      Symbol* field_name = st.name();\n+\/\/      ...\n+\/\/    }\n+class DumpedInstanceFieldStream {\n+ private:\n+  const ParsedHeapDump &_heap_dump;\n+  const HeapDump::InstanceDump &_instance_dump;\n+\n+  const HeapDump::ClassDump *_current_class_dump;\n+  u2 _field_index = 0;  \/\/ Index in the current class\n+  u4 _field_offset = 0; \/\/ Offset into the instance field data\n+\n+ public:\n+  DumpedInstanceFieldStream(const ParsedHeapDump &heap_dump, const HeapDump::InstanceDump &dump) :\n+    _heap_dump(heap_dump), _instance_dump(dump), _current_class_dump(&heap_dump.get_class_dump(dump.class_id)) {}\n+\n+  void next();\n+  bool eos();\n+\n+  Symbol *name() const;\n+  BasicType type() const;\n+  HeapDump::BasicValue value() const;\n+};\n+\n+\/\/ Parses HPROF heap dump.\n+struct HeapDumpParser : public AllStatic {\n+  \/\/ Parses the heap dump in path filling the out container. Returns nullptr on\n+  \/\/ success or a pointer to a static error message otherwise. If a error\n+  \/\/ occurs, out may contain unfilled records.\n+  static const char *parse(const char *path, ParsedHeapDump *out);\n+};\n+\n+#endif \/\/ SHARE_UTILITIES_HEAPDUMPPARSER_HPP\n","filename":"src\/hotspot\/share\/utilities\/heapDumpParser.hpp","additions":213,"deletions":0,"binary":false,"changes":213,"status":"added"},{"patch":"@@ -0,0 +1,79 @@\n+\/*\n+ * Copyright (c) 2005, 2023, Oracle and\/or its affiliates. All rights reserved.\n+ * Copyright (c) 2023, Alibaba Group Holding Limited. All rights reserved.\n+ * DO NOT ALTER OR REMOVE COPYRIGHT NOTICES OR THIS FILE HEADER.\n+ *\n+ * This code is free software; you can redistribute it and\/or modify it\n+ * under the terms of the GNU General Public License version 2 only, as\n+ * published by the Free Software Foundation.\n+ *\n+ * This code is distributed in the hope that it will be useful, but WITHOUT\n+ * ANY WARRANTY; without even the implied warranty of MERCHANTABILITY or\n+ * FITNESS FOR A PARTICULAR PURPOSE.  See the GNU General Public License\n+ * version 2 for more details (a copy is included in the LICENSE file that\n+ * accompanied this code).\n+ *\n+ * You should have received a copy of the GNU General Public License version\n+ * 2 along with this work; if not, write to the Free Software Foundation,\n+ * Inc., 51 Franklin St, Fifth Floor, Boston, MA 02110-1301 USA.\n+ *\n+ * Please contact Oracle, 500 Oracle Parkway, Redwood Shores, CA 94065 USA\n+ * or visit www.oracle.com if you need additional information or have any\n+ * questions.\n+ *\n+ *\/\n+\n+#ifndef SHARE_UTILITIES_HPROFTAG_HPP\n+#define SHARE_UTILITIES_HPROFTAG_HPP\n+\n+#include \"utilities\/globalDefinitions.hpp\"\n+\n+\/\/ HPROF tags\n+enum hprofTag : u1 {\n+  \/\/ top-level records\n+  HPROF_UTF8                    = 0x01,\n+  HPROF_LOAD_CLASS              = 0x02,\n+  HPROF_UNLOAD_CLASS            = 0x03,\n+  HPROF_FRAME                   = 0x04,\n+  HPROF_TRACE                   = 0x05,\n+  HPROF_ALLOC_SITES             = 0x06,\n+  HPROF_HEAP_SUMMARY            = 0x07,\n+  HPROF_START_THREAD            = 0x0A,\n+  HPROF_END_THREAD              = 0x0B,\n+  HPROF_HEAP_DUMP               = 0x0C,\n+  HPROF_CPU_SAMPLES             = 0x0D,\n+  HPROF_CONTROL_SETTINGS        = 0x0E,\n+\n+  \/\/ 1.0.2 record types\n+  HPROF_HEAP_DUMP_SEGMENT       = 0x1C,\n+  HPROF_HEAP_DUMP_END           = 0x2C,\n+\n+  \/\/ field types\n+  HPROF_ARRAY_OBJECT            = 0x01,\n+  HPROF_NORMAL_OBJECT           = 0x02,\n+  HPROF_BOOLEAN                 = 0x04,\n+  HPROF_CHAR                    = 0x05,\n+  HPROF_FLOAT                   = 0x06,\n+  HPROF_DOUBLE                  = 0x07,\n+  HPROF_BYTE                    = 0x08,\n+  HPROF_SHORT                   = 0x09,\n+  HPROF_INT                     = 0x0A,\n+  HPROF_LONG                    = 0x0B,\n+\n+  \/\/ data-dump sub-records\n+  HPROF_GC_ROOT_UNKNOWN         = 0xFF,\n+  HPROF_GC_ROOT_JNI_GLOBAL      = 0x01,\n+  HPROF_GC_ROOT_JNI_LOCAL       = 0x02,\n+  HPROF_GC_ROOT_JAVA_FRAME      = 0x03,\n+  HPROF_GC_ROOT_NATIVE_STACK    = 0x04,\n+  HPROF_GC_ROOT_STICKY_CLASS    = 0x05,\n+  HPROF_GC_ROOT_THREAD_BLOCK    = 0x06,\n+  HPROF_GC_ROOT_MONITOR_USED    = 0x07,\n+  HPROF_GC_ROOT_THREAD_OBJ      = 0x08,\n+  HPROF_GC_CLASS_DUMP           = 0x20,\n+  HPROF_GC_INSTANCE_DUMP        = 0x21,\n+  HPROF_GC_OBJ_ARRAY_DUMP       = 0x22,\n+  HPROF_GC_PRIM_ARRAY_DUMP      = 0x23\n+};\n+\n+#endif \/\/ SHARE_UTILITIES_HPROFTAG_HPP\n","filename":"src\/hotspot\/share\/utilities\/hprofTag.hpp","additions":79,"deletions":0,"binary":false,"changes":79,"status":"added"},{"patch":"@@ -0,0 +1,45 @@\n+#ifndef SHARE_UTILITIES_METHODKIND_HPP\n+#define SHARE_UTILITIES_METHODKIND_HPP\n+\n+#include \"memory\/allStatic.hpp\"\n+#include \"oops\/klass.hpp\"\n+#include \"oops\/method.hpp\"\n+#include \"utilities\/debug.hpp\"\n+#include \"utilities\/globalDefinitions.hpp\"\n+\n+\/\/ Kinds of methods.\n+\/\/\n+\/\/ According to InstanceKlass::find_local_method(), a class can have separate\n+\/\/ methods with the same name and signature for each of these kinds.\n+struct MethodKind : public AllStatic {\n+  enum class Enum: u1 {\n+    STATIC   = 0,\n+    INSTANCE = 1, \/\/ Non-static, non-overpass\n+    OVERPASS = 2,\n+  };\n+\n+  static Enum of_method(const Method &m) {\n+    assert(!(m.is_static() && m.is_overpass()), \"overpass cannot be static\");\n+    return m.is_static() ? Enum::STATIC : (m.is_overpass() ? Enum::OVERPASS : Enum::INSTANCE);\n+  }\n+\n+  static constexpr bool is_method_kind(u1 val) { return val <= static_cast<u1>(Enum::OVERPASS); }\n+\n+  static constexpr Klass::StaticLookupMode as_static_lookup_mode(MethodKind::Enum kind) {\n+    return kind == Enum::STATIC ? Klass::StaticLookupMode::find : Klass::StaticLookupMode::skip;\n+  }\n+  static constexpr Klass::OverpassLookupMode as_overpass_lookup_mode(MethodKind::Enum kind) {\n+    return kind == Enum::OVERPASS ? Klass::OverpassLookupMode::find : Klass::OverpassLookupMode::skip;\n+  }\n+\n+  static constexpr const char *name(MethodKind::Enum kind) {\n+    switch (kind) {\n+      case Enum::STATIC:   return \"static\";\n+      case Enum::OVERPASS: return \"overpass\";\n+      case Enum::INSTANCE: return \"non-static non-overpass\";\n+      default: ShouldNotReachHere(); return nullptr;\n+    }\n+  }\n+};\n+\n+#endif \/\/ SHARE_UTILITIES_METHODKIND_HPP\n","filename":"src\/hotspot\/share\/utilities\/methodKind.hpp","additions":45,"deletions":0,"binary":false,"changes":45,"status":"added"},{"patch":"@@ -29,0 +29,2 @@\n+import jdk.crac.Context;\n+import jdk.crac.Resource;\n@@ -30,0 +32,2 @@\n+import jdk.internal.crac.Core;\n+import jdk.internal.crac.JDKResource;\n@@ -56,3 +60,3 @@\n-    private static final int SIZEOF_EPOLLEVENT   = eventSize();\n-    private static final int OFFSETOF_EVENTS     = eventsOffset();\n-    private static final int OFFSETOF_FD         = dataOffset();\n+    private static int SIZEOF_EPOLLEVENT   = eventSize();\n+    private static int OFFSETOF_EVENTS     = eventsOffset();\n+    private static int OFFSETOF_FD         = dataOffset();\n@@ -72,0 +76,18 @@\n+    \/\/ Portable CRaC support\n+    private static final JDKResource nativeInitResource = new JDKResource() {\n+        @Override\n+        public void beforeCheckpoint(Context<? extends Resource> context) {\n+        };\n+\n+        @Override\n+        public void afterRestore(Context<? extends Resource> context) {\n+            IOUtil.load();\n+            SIZEOF_EPOLLEVENT   = eventSize();\n+            OFFSETOF_EVENTS     = eventsOffset();\n+            OFFSETOF_FD         = dataOffset();\n+        };\n+    };\n+    static {\n+        Core.Priority.NORMAL.getContext().register(nativeInitResource);\n+    }\n+\n","filename":"src\/java.base\/linux\/classes\/sun\/nio\/ch\/EPoll.java","additions":25,"deletions":3,"binary":false,"changes":28,"status":"modified"},{"patch":"@@ -30,1 +30,2 @@\n-import jdk.internal.crac.OpenResourcePolicies;\n+import jdk.crac.Context;\n+import jdk.crac.Resource;\n@@ -33,0 +34,1 @@\n+import jdk.internal.crac.Core;\n@@ -34,0 +36,2 @@\n+import jdk.internal.crac.JDKResource;\n+import jdk.internal.crac.OpenResourcePolicies;\n@@ -1259,0 +1263,11 @@\n+    private static final JDKResource nativeInitResource = new JDKResource() {\n+        @Override\n+        public void beforeCheckpoint(Context<? extends Resource> context) {\n+        }\n+\n+        @Override\n+        public void afterRestore(Context<? extends Resource> context) {\n+            initIDs();\n+        }\n+    };\n+\n@@ -1261,0 +1276,2 @@\n+        \/\/ Must be restored before restoring instances\n+        Core.Priority.FILE_DESCRIPTORS.getContext().register(nativeInitResource);\n","filename":"src\/java.base\/share\/classes\/java\/io\/RandomAccessFile.java","additions":18,"deletions":1,"binary":false,"changes":19,"status":"modified"},{"patch":"@@ -27,0 +27,2 @@\n+import jdk.crac.Context;\n+import jdk.crac.Resource;\n@@ -28,0 +30,2 @@\n+import jdk.internal.crac.Core;\n+import jdk.internal.crac.JDKResource;\n@@ -68,1 +72,1 @@\n-    private static final ProcessHandleImpl current;\n+    private static ProcessHandleImpl current;\n@@ -76,0 +80,16 @@\n+    \/**\n+     * Portable CRaC support.\n+     *\/\n+    private static final JDKResource nativeInitResource = new JDKResource() {\n+        @Override\n+        public void beforeCheckpoint(Context<? extends Resource> context) {\n+        };\n+\n+        @Override\n+        public void afterRestore(Context<? extends Resource> context) {\n+            initNative();\n+            long pid = getCurrentPid0();\n+            current = new ProcessHandleImpl(pid, isAlive0(pid));\n+        };\n+    };\n+\n@@ -80,0 +100,1 @@\n+        Core.Priority.NORMAL.getContext().register(nativeInitResource);\n","filename":"src\/java.base\/share\/classes\/java\/lang\/ProcessHandleImpl.java","additions":22,"deletions":1,"binary":false,"changes":23,"status":"modified"},{"patch":"@@ -34,0 +34,6 @@\n+\n+import jdk.crac.Context;\n+import jdk.crac.Resource;\n+import jdk.internal.crac.Core;\n+import jdk.internal.crac.JDKResource;\n+\n@@ -371,0 +377,11 @@\n+    private static final JDKResource nativeInitResource = new JDKResource() {\n+        @Override\n+        public void beforeCheckpoint(Context<? extends Resource> context) {\n+        };\n+\n+        @Override\n+        public void afterRestore(Context<? extends Resource> context) {\n+            init();\n+        };\n+    };\n+\n@@ -372,1 +389,4 @@\n-    static { init(); }\n+    static {\n+        init();\n+        Core.Priority.NORMAL.getContext().register(nativeInitResource);\n+    }\n","filename":"src\/java.base\/share\/classes\/java\/net\/Inet6Address.java","additions":21,"deletions":1,"binary":false,"changes":22,"status":"modified"},{"patch":"@@ -417,0 +417,2 @@\n+                jdk.internal.loader.BootLoader.loadLibrary(\"net\");\n+                init();\n","filename":"src\/java.base\/share\/classes\/java\/net\/InetAddress.java","additions":2,"deletions":0,"binary":false,"changes":2,"status":"modified"},{"patch":"@@ -34,0 +34,4 @@\n+import jdk.crac.Context;\n+import jdk.crac.Resource;\n+import jdk.internal.crac.Core;\n+import jdk.internal.crac.JDKResource;\n@@ -105,1 +109,1 @@\n-    private final Cleaner cleaner;\n+    private Cleaner cleaner;\n@@ -119,3 +123,0 @@\n-    \/\/ Primary constructor\n-    \/\/\n-    Direct$Type$Buffer$RW$(int cap) {                   \/\/ package-private\n@@ -123,1 +124,8 @@\n-        super(-1, 0, cap, cap, null);\n+\n+    \/\/ TODO currently this is only used when the buffer itself is responsible\n+    \/\/  for the allocation, but ideally this should be used in all cases to\n+    \/\/  update the address after restore\n+    private final JDKResource resource;\n+\n+    private void allocate() {\n+        int cap = capacity();\n@@ -151,0 +159,10 @@\n+    }\n+\n+#end[rw]\n+\n+    \/\/ Primary constructor\n+    \/\/\n+    Direct$Type$Buffer$RW$(int cap) {                   \/\/ package-private\n+#if[rw]\n+        super(-1, 0, cap, cap, null);\n+        allocate();\n@@ -152,0 +170,23 @@\n+\n+        resource = new JDKResource() {\n+            private byte[] buf;\n+\n+            @Override\n+            public void beforeCheckpoint(Context<? extends Resource> context) {\n+                buf = new byte[cap];\n+                getArray(0, buf, 0, cap);\n+                cleaner.clean();\n+                cleaner = null;\n+                address = 0; \/\/ Simplifies debugging\n+            };\n+\n+            @Override\n+            public void afterRestore(Context<? extends Resource> context) {\n+                allocate();\n+                putArray(0, buf, 0, cap);\n+                buf = null;\n+            };\n+        };\n+        \/\/ Must have a priority higher than that of file descriptors because\n+        \/\/ file descriptor policies are read from such buffer\n+        Core.Priority.POST_FILE_DESCRIPTORS.getContext().register(resource);\n@@ -167,0 +208,1 @@\n+        resource = null;\n@@ -177,0 +219,1 @@\n+        resource = null;\n@@ -187,0 +230,1 @@\n+        resource = null;\n@@ -219,0 +263,1 @@\n+        resource = null;\n@@ -246,0 +291,1 @@\n+        resource = null;\n","filename":"src\/java.base\/share\/classes\/java\/nio\/Direct-X-Buffer.java.template","additions":51,"deletions":5,"binary":false,"changes":56,"status":"modified"},{"patch":"@@ -946,1 +946,1 @@\n-    private $Type$Buffer getArray(int index, $type$[] dst, int offset, int length) {\n+    $Type$Buffer getArray(int index, $type$[] dst, int offset, int length) {\n","filename":"src\/java.base\/share\/classes\/java\/nio\/X-Buffer.java.template","additions":1,"deletions":1,"binary":false,"changes":2,"status":"modified"},{"patch":"@@ -39,0 +39,2 @@\n+import jdk.crac.Context;\n+import jdk.crac.Resource;\n@@ -43,0 +45,2 @@\n+import jdk.internal.crac.Core;\n+import jdk.internal.crac.JDKResource;\n@@ -121,0 +125,3 @@\n+        \/\/ TODO when portable CRaC supports restoration of all classes this won't be needed anymore\n+        private static final JDKResource secretsInitResource;\n+\n@@ -122,1 +129,1 @@\n-            SharedSecrets.setJavaUtilCollectionAccess(new JavaUtilCollectionAccess() {\n+            final JavaUtilCollectionAccess access = new JavaUtilCollectionAccess() {\n@@ -129,1 +136,13 @@\n-            });\n+            };\n+            SharedSecrets.setJavaUtilCollectionAccess(access);\n+            secretsInitResource = new JDKResource() {\n+                @Override\n+                public void beforeCheckpoint(Context<? extends Resource> context) {\n+                };\n+\n+                @Override\n+                public void afterRestore(Context<? extends Resource> context) {\n+                    SharedSecrets.setJavaUtilCollectionAccess(access);\n+                };\n+            };\n+            Core.Priority.NORMAL.getContext().register(secretsInitResource);\n","filename":"src\/java.base\/share\/classes\/java\/util\/ImmutableCollections.java","additions":21,"deletions":2,"binary":false,"changes":23,"status":"modified"},{"patch":"@@ -28,0 +28,2 @@\n+import jdk.crac.Context;\n+import jdk.crac.Resource;\n@@ -30,0 +32,2 @@\n+import jdk.internal.crac.Core;\n+import jdk.internal.crac.JDKResource;\n@@ -200,0 +204,18 @@\n+    \/\/ TODO remove this when portable CRaC becomes able to restore SharedSecrets\n+    private static final JDKResource secretsReinitResource = new JDKResource() {\n+        @Override\n+        public void beforeCheckpoint(Context<? extends Resource> context) {\n+        }\n+\n+        @Override\n+        public void afterRestore(Context<? extends Resource> context) {\n+            if (SharedSecrets.javaUtilJarAccess() == null) {\n+                SharedSecrets.setJavaUtilJarAccess(new JavaUtilJarAccessImpl());\n+            }\n+        }\n+    };\n+\n+    static {\n+        Core.Priority.NORMAL.getContext().register(secretsReinitResource);\n+    }\n+\n","filename":"src\/java.base\/share\/classes\/java\/util\/jar\/JarFile.java","additions":22,"deletions":0,"binary":false,"changes":22,"status":"modified"},{"patch":"@@ -33,0 +33,4 @@\n+import jdk.crac.Context;\n+import jdk.crac.Resource;\n+import jdk.internal.crac.Core;\n+import jdk.internal.crac.JDKResource;\n@@ -117,1 +121,1 @@\n-    static {\n+    private static void initNatives() {\n@@ -122,0 +126,16 @@\n+    private static final JDKResource nativeInitResource = new JDKResource() {\n+        @Override\n+        public void beforeCheckpoint(Context<? extends Resource> context) {\n+        }\n+\n+        @Override\n+        public void afterRestore(Context<? extends Resource> context) {\n+            initNatives();\n+        }\n+    };\n+\n+    static {\n+        initNatives();\n+        Core.Priority.NORMAL.getContext().register(nativeInitResource);\n+    }\n+\n","filename":"src\/java.base\/share\/classes\/java\/util\/zip\/Inflater.java","additions":21,"deletions":1,"binary":false,"changes":22,"status":"modified"},{"patch":"@@ -60,0 +60,1 @@\n+        POST_FILE_DESCRIPTORS(new BlockingOrderedContext<>()),\n@@ -68,0 +69,1 @@\n+        NATIVE_THREAD(new BlockingOrderedContext<>()),\n","filename":"src\/java.base\/share\/classes\/jdk\/internal\/crac\/Core.java","additions":2,"deletions":0,"binary":false,"changes":2,"status":"modified"},{"patch":"@@ -76,1 +76,1 @@\n-    private final ByteBuffer memoryMap;\n+    private ByteBuffer memoryMap;\n@@ -80,4 +80,4 @@\n-    private final IntBuffer redirect;\n-    private final IntBuffer offsets;\n-    private final ByteBuffer locations;\n-    private final ByteBuffer strings;\n+    private IntBuffer redirect;\n+    private IntBuffer offsets;\n+    private ByteBuffer locations;\n+    private ByteBuffer strings;\n@@ -110,1 +110,0 @@\n-            registerIfCRaCPresent();\n@@ -146,0 +145,42 @@\n+        createBuffers();\n+\n+        stringsReader = new ImageStringsReader(this);\n+        decompressor = new Decompressor();\n+\n+        \/\/ Register a resource to handle the file channel and the buffers\n+        registerIfCRaCPresent();\n+    }\n+\n+    private void createMemoryMap() throws IOException {\n+        ByteBuffer map;\n+\n+        if (USE_JVM_MAP && BasicImageReader.class.getClassLoader() == null) {\n+            \/\/ Check to see if the jvm has opened the file using libjimage\n+            \/\/ native entry when loading the image for this runtime\n+            map = NativeImageBuffer.getNativeMap(name);\n+         } else {\n+            map = null;\n+        }\n+\n+        \/\/ Open the file only if no memory map yet or is 32 bit jvm\n+        if (map != null && MAP_ALL) {\n+            channel = null;\n+        } else {\n+            channel = openFileChannel();\n+        }\n+\n+        \/\/ If no memory map yet and 64 bit jvm then memory map entire file\n+        if (MAP_ALL && map == null) {\n+            map = channel.map(FileChannel.MapMode.READ_ONLY, 0, channel.size());\n+        }\n+\n+        \/\/ If no memory map yet then must be 32 bit jvm not previously mapped\n+        if (map == null) {\n+            \/\/ Just map the image index\n+            map = channel.map(FileChannel.MapMode.READ_ONLY, 0, indexSize);\n+        }\n+\n+        memoryMap = map.asReadOnlyBuffer();\n+    }\n+\n+    private void createBuffers() throws IOException {\n@@ -154,3 +195,0 @@\n-\n-        stringsReader = new ImageStringsReader(this);\n-        decompressor = new Decompressor();\n@@ -187,2 +225,0 @@\n-                            channel.close();\n-                        } else if (\"afterRestore\".equals(method.getName())) {\n@@ -190,1 +226,1 @@\n-                                channel = openFileChannel();\n+                                channel.close();\n@@ -192,0 +228,3 @@\n+                        } else if (\"afterRestore\".equals(method.getName())) {\n+                            createMemoryMap();\n+                            createBuffers();\n","filename":"src\/java.base\/share\/classes\/jdk\/internal\/jimage\/BasicImageReader.java","additions":51,"deletions":12,"binary":false,"changes":63,"status":"modified"},{"patch":"@@ -27,0 +27,4 @@\n+import java.lang.reflect.InvocationHandler;\n+import java.lang.reflect.InvocationTargetException;\n+import java.lang.reflect.Method;\n+import java.lang.reflect.Proxy;\n@@ -38,0 +42,2 @@\n+    private static Object nativeInitResource;\n+\n@@ -39,0 +45,5 @@\n+        loadNativeLibrary();\n+        registerIfCRaCPresent();\n+    }\n+\n+    private static void loadNativeLibrary() {\n@@ -48,0 +59,52 @@\n+    \/\/ Since this class must be compatible with JDK 8 and any non-CRaC JDK due\n+    \/\/ to being part of jrtfs.jar we must register this to CRaC via reflection.\n+    private static void registerIfCRaCPresent() {\n+        try {\n+            Class<?> priorityClass = Class.forName(\"jdk.internal.crac.Core$Priority\");\n+            Class<?> jdkResourceClass = Class.forName(\"jdk.internal.crac.JDKResource\");\n+            Class<?> resourceClass = Class.forName(\"jdk.crac.Resource\");\n+            Object[] priorities = priorityClass.getEnumConstants();\n+            if (priorities == null) {\n+                return;\n+            }\n+            Object normalPriority = null;\n+            for (Object priority : priorities) {\n+                if (\"NORMAL\".equals(priority.toString())) {\n+                    normalPriority = priority;\n+                }\n+            }\n+            if (normalPriority == null) {\n+                throw new IllegalStateException();\n+            }\n+            try {\n+                Method getContext = priorityClass.getMethod(\"getContext\");\n+                Object ctx = getContext.invoke(normalPriority);\n+                Method register = ctx.getClass().getMethod(\"register\", resourceClass);\n+                nativeInitResource = Proxy.newProxyInstance(null, new Class<?>[] { jdkResourceClass }, new InvocationHandler() {\n+                    @Override\n+                    public Object invoke(Object proxy, Method method, Object[] args) throws Throwable {\n+                        if (\"beforeCheckpoint\".equals(method.getName())) {\n+                            \/\/ Do nothing\n+                        } else if (\"afterRestore\".equals(method.getName())) {\n+                            loadNativeLibrary();\n+                        } else if (\"toString\".equals(method.getName())) {\n+                            return toString();\n+                        } else if (\"hashCode\".equals(method.getName())) {\n+                            return hashCode();\n+                        } else if (\"equals\".equals(method.getName())) {\n+                            return equals(args[0]);\n+                        } else {\n+                            throw new UnsupportedOperationException(method.toString());\n+                        }\n+                        return null;\n+                    }\n+                });\n+                register.invoke(ctx, nativeInitResource);\n+            } catch (NoSuchMethodException | InvocationTargetException | IllegalAccessException e) {\n+                throw new IllegalStateException(e);\n+            }\n+        } catch (ClassNotFoundException e) {\n+            \/\/ ignored if class not present\n+        }\n+    }\n+\n","filename":"src\/java.base\/share\/classes\/jdk\/internal\/jimage\/NativeImageBuffer.java","additions":63,"deletions":0,"binary":false,"changes":63,"status":"modified"},{"patch":"@@ -33,1 +33,4 @@\n-\n+import jdk.crac.Context;\n+import jdk.crac.Resource;\n+import jdk.internal.crac.Core;\n+import jdk.internal.crac.JDKResource;\n@@ -434,0 +437,11 @@\n+    private static final JDKResource nativeInitResource = new JDKResource() {\n+        @Override\n+        public void beforeCheckpoint(Context<? extends Resource> context) {\n+        }\n+\n+        @Override\n+        public void afterRestore(Context<? extends Resource> context) {\n+            registerNatives();\n+        }\n+    };\n+\n@@ -436,0 +450,1 @@\n+        Core.Priority.NORMAL.getContext().register(nativeInitResource);\n","filename":"src\/java.base\/share\/classes\/jdk\/internal\/perf\/Perf.java","additions":16,"deletions":1,"binary":false,"changes":17,"status":"modified"},{"patch":"@@ -33,0 +33,5 @@\n+import jdk.crac.Context;\n+import jdk.crac.Resource;\n+import jdk.internal.crac.Core;\n+import jdk.internal.crac.JDKResource;\n+\n@@ -62,1 +67,33 @@\n-    private final LongBuffer lb;\n+    private LongBuffer lb;\n+\n+    \/\/ Portable CRaC cannot restore perf buffers\n+    private final class BufferResource implements JDKResource {\n+        private final int type; \/\/ TODO retrieve from native Perf?\n+        private long savedValue;\n+\n+        BufferResource(int type) {\n+            this.type = type;\n+        }\n+\n+        @Override\n+        public void beforeCheckpoint(Context<? extends Resource> context) {\n+            savedValue = get();\n+        }\n+\n+        @Override\n+        public void afterRestore(Context<? extends Resource> context) {\n+            ByteBuffer bb;\n+            try {\n+                bb = perf.createLong(name, type, U_None, 0L);\n+            } catch (IllegalArgumentException ignored) {\n+                \/\/ This C\/R implementation can restore perf state\n+                assert get() == savedValue;\n+                return;\n+            }\n+            bb.order(ByteOrder.nativeOrder());\n+            lb = bb.asLongBuffer();\n+            set(savedValue);\n+        }\n+    }\n+\n+    private final BufferResource bufferResource;\n@@ -69,0 +106,2 @@\n+        bufferResource = new BufferResource(type);\n+        Core.Priority.NORMAL.getContext().register(bufferResource);\n","filename":"src\/java.base\/share\/classes\/jdk\/internal\/perf\/PerfCounter.java","additions":40,"deletions":1,"binary":false,"changes":41,"status":"modified"},{"patch":"@@ -170,0 +170,1 @@\n+        java.management,\n","filename":"src\/java.base\/share\/classes\/module-info.java","additions":1,"deletions":0,"binary":false,"changes":1,"status":"modified"},{"patch":"@@ -31,0 +31,5 @@\n+import jdk.crac.Context;\n+import jdk.crac.Resource;\n+import jdk.internal.crac.Core;\n+import jdk.internal.crac.JDKResource;\n+import jdk.internal.misc.Unsafe;\n@@ -37,0 +42,13 @@\n+    private final JDKResource resource;\n+\n+    private void allocate(int size, boolean pageAligned) {\n+        if (!pageAligned) {\n+            this.allocationAddress = unsafe.allocateMemory(size);\n+            this.address = this.allocationAddress;\n+        } else {\n+            int ps = pageSize();\n+            long a = unsafe.allocateMemory(size + ps);\n+            this.allocationAddress = a;\n+            this.address = a + ps - (a & (ps - 1));\n+        }\n+    }\n@@ -53,1 +71,23 @@\n-        super(size, pageAligned);\n+        allocate(size, pageAligned);\n+        resource = new JDKResource() {\n+            private byte[] data;\n+\n+            @Override\n+            public void beforeCheckpoint(Context<? extends Resource> context) {\n+                if (allocationAddress != 0) {\n+                    data = new byte[size];\n+                    unsafe.copyMemory(null, address, data, Unsafe.ARRAY_BYTE_BASE_OFFSET, size);\n+                    free();\n+                }\n+            };\n+\n+            @Override\n+            public void afterRestore(Context<? extends Resource> context) {\n+                if (data != null) {\n+                    allocate(size, pageAligned);\n+                    unsafe.copyMemory(data, Unsafe.ARRAY_BYTE_BASE_OFFSET, null, address, size);\n+                    data = null;\n+                }\n+            };\n+        };\n+        Core.Priority.NORMAL.getContext().register(resource);\n","filename":"src\/java.base\/share\/classes\/sun\/nio\/ch\/AllocatedNativeObject.java","additions":41,"deletions":1,"binary":false,"changes":42,"status":"modified"},{"patch":"@@ -32,0 +32,4 @@\n+import jdk.crac.Context;\n+import jdk.crac.Resource;\n+import jdk.internal.crac.Core;\n+import jdk.internal.crac.JDKResource;\n@@ -227,1 +231,1 @@\n-            long bytesWritten = nd.writev(fd, vec.address, iov_len);\n+            long bytesWritten = nd.writev(fd, vec.address(), iov_len);\n@@ -434,1 +438,1 @@\n-            long bytesRead = nd.readv(fd, vec.address, iov_len);\n+            long bytesRead = nd.readv(fd, vec.address(), iov_len);\n@@ -604,0 +608,2 @@\n+    private static final JDKResource nativeInitResource;\n+\n@@ -613,0 +619,15 @@\n+        nativeInitResource = new JDKResource() {\n+            @Override\n+            public void beforeCheckpoint(Context<? extends Resource> context) {\n+            };\n+\n+            @Override\n+            public void afterRestore(Context<? extends Resource> context) {\n+                jdk.internal.loader.BootLoader.loadLibrary(\"net\");\n+                jdk.internal.loader.BootLoader.loadLibrary(\"nio\");\n+                initIDs();\n+            };\n+        };\n+        \/\/ Must have a priority higher than that of file descriptors because\n+        \/\/ used when reading file descriptor policies\n+        Core.Priority.POST_FILE_DESCRIPTORS.getContext().register(nativeInitResource);\n","filename":"src\/java.base\/share\/classes\/sun\/nio\/ch\/IOUtil.java","additions":23,"deletions":2,"binary":false,"changes":25,"status":"modified"},{"patch":"@@ -68,3 +68,0 @@\n-    \/\/ Base address of this array\n-    final long address;\n-\n@@ -94,1 +91,0 @@\n-        this.address   = vecArray.address();\n@@ -112,0 +108,7 @@\n+    \/**\n+     * Base address of this array.\n+     *\/\n+    long address() {\n+        return vecArray.address(); \/\/ Can change because of checkpoint-restore\n+    }\n+\n","filename":"src\/java.base\/share\/classes\/sun\/nio\/ch\/IOVecWrapper.java","additions":7,"deletions":4,"binary":false,"changes":11,"status":"modified"},{"patch":"@@ -52,1 +52,1 @@\n-    private final long address;\n+    protected long address;\n@@ -73,11 +73,1 @@\n-    protected NativeObject(int size, boolean pageAligned) {\n-        if (!pageAligned) {\n-            this.allocationAddress = unsafe.allocateMemory(size);\n-            this.address = this.allocationAddress;\n-        } else {\n-            int ps = pageSize();\n-            long a = unsafe.allocateMemory(size + ps);\n-            this.allocationAddress = a;\n-            this.address = a + ps - (a & (ps - 1));\n-        }\n-    }\n+    protected NativeObject() {}\n","filename":"src\/java.base\/share\/classes\/sun\/nio\/ch\/NativeObject.java","additions":2,"deletions":12,"binary":false,"changes":14,"status":"modified"},{"patch":"@@ -53,0 +53,4 @@\n+import jdk.crac.Context;\n+import jdk.crac.Resource;\n+import jdk.internal.crac.Core;\n+import jdk.internal.crac.JDKResource;\n@@ -797,0 +801,2 @@\n+    private static final JDKResource nativeInitResource;\n+\n@@ -818,0 +824,11 @@\n+        nativeInitResource = new JDKResource() {\n+            @Override\n+            public void beforeCheckpoint(Context<? extends Resource> context) {\n+            };\n+\n+            @Override\n+            public void afterRestore(Context<? extends Resource> context) {\n+                initIDs();\n+            };\n+        };\n+        Core.Priority.NORMAL.getContext().register(nativeInitResource);\n","filename":"src\/java.base\/share\/classes\/sun\/nio\/ch\/Net.java","additions":17,"deletions":0,"binary":false,"changes":17,"status":"modified"},{"patch":"@@ -29,0 +29,4 @@\n+import jdk.crac.Context;\n+import jdk.crac.Resource;\n+import jdk.internal.crac.Core;\n+import jdk.internal.crac.JDKResource;\n@@ -39,1 +43,1 @@\n-    private final long address;\n+    private long address;\n@@ -41,1 +45,1 @@\n-    private final Cleanable cleanable;\n+    private Cleanable cleanable;\n@@ -47,0 +51,2 @@\n+    private final JDKResource resource;\n+\n@@ -62,0 +68,29 @@\n+\n+        \/\/ TODO clear NativeBuffers from released buffers first so that unused\n+        \/\/  data is not restored\n+        this.resource = new JDKResource() {\n+            private byte[] data;\n+\n+            @Override\n+            public void beforeCheckpoint(Context<? extends Resource> context) {\n+                if (address != 0) {\n+                    data = new byte[size];\n+                    unsafe.copyMemory(null, address, data, Unsafe.ARRAY_BYTE_BASE_OFFSET, size);\n+                    free();\n+                }\n+            };\n+\n+            @Override\n+            public void afterRestore(Context<? extends Resource> context) {\n+                if (data != null) {\n+                    address = unsafe.allocateMemory(size);\n+                    cleanable = CleanerFactory.cleaner()\n+                                              .register(this, new Deallocator(address));\n+                    unsafe.copyMemory(data, Unsafe.ARRAY_BYTE_BASE_OFFSET, null, address, size);\n+                    data = null;\n+                }\n+            };\n+        };\n+        \/\/ Must have a priority higher than that of file descriptors because\n+        \/\/ used when reading file descriptor policies\n+        Core.Priority.POST_FILE_DESCRIPTORS.getContext().register(resource);\n@@ -83,0 +118,1 @@\n+        address = 0;\n","filename":"src\/java.base\/share\/classes\/sun\/nio\/fs\/NativeBuffer.java","additions":38,"deletions":2,"binary":false,"changes":40,"status":"modified"},{"patch":"@@ -28,0 +28,4 @@\n+import jdk.crac.Context;\n+import jdk.crac.Resource;\n+import jdk.internal.crac.Core;\n+import jdk.internal.crac.JDKResource;\n@@ -41,0 +45,1 @@\n+    private static final JDKResource nativeInitResource;\n@@ -106,0 +111,13 @@\n+        nativeInitResource = new JDKResource() {\n+            @Override\n+            public void beforeCheckpoint(Context<? extends Resource> context) {\n+            };\n+\n+            @Override\n+            public void afterRestore(Context<? extends Resource> context) {\n+                init();\n+            };\n+        };\n+        \/\/ TODO figure out why lower priority makes some crac\/fileDescriptors\n+        \/\/  tests never complete\n+        Core.Priority.NATIVE_THREAD.getContext().register(nativeInitResource);\n","filename":"src\/java.base\/unix\/classes\/sun\/nio\/ch\/NativeThread.java","additions":18,"deletions":0,"binary":false,"changes":18,"status":"modified"},{"patch":"@@ -59,0 +59,4 @@\n+import jdk.crac.Context;\n+import jdk.crac.Resource;\n+import jdk.internal.crac.Core;\n+import jdk.internal.crac.JDKResource;\n@@ -1019,0 +1023,11 @@\n+    private static final JDKResource nativeInitResource = new JDKResource() {\n+        @Override\n+        public void beforeCheckpoint(Context<? extends Resource> context) {\n+        }\n+\n+        @Override\n+        public void afterRestore(Context<? extends Resource> context) {\n+            loadNativeLib();\n+        }\n+    };\n+\n@@ -1021,0 +1036,1 @@\n+        Core.Priority.NORMAL.getContext().register(nativeInitResource);\n","filename":"src\/java.management\/share\/classes\/java\/lang\/management\/ManagementFactory.java","additions":16,"deletions":0,"binary":false,"changes":16,"status":"modified"},{"patch":"@@ -0,0 +1,287 @@\n+#include \"precompiled.hpp\"\n+#include \"unittest.hpp\"\n+#include \"runtime\/cracStackDumpParser.hpp\"\n+#include \"utilities\/growableArray.hpp\"\n+#include \"utilities\/methodKind.hpp\"\n+\n+constexpr char TEST_FILENAME[] = \"stackDumpParser_test.hprof\";\n+\n+static void fill_test_file(const char *contents, size_t size) {\n+  FILE *file = os::fopen(TEST_FILENAME, \"wb\");\n+  ASSERT_NE(nullptr, file) << \"Cannot open \" << TEST_FILENAME << \" for writing: \" << os::strerror(errno);\n+  EXPECT_EQ(1U, fwrite(contents, size, 1, file)) << \"Cannot write test data into \" << TEST_FILENAME << \": \" << os::strerror(errno);\n+  ASSERT_EQ(0, fclose(file)) << \"Cannot close the test file: \" << os::strerror(errno);\n+}\n+\n+static void check_stack_values(const GrowableArrayView<CracStackTrace::Frame::Value> &expected_values,\n+                               const GrowableArrayView<CracStackTrace::Frame::Value> &actual_values) {\n+  ASSERT_EQ(expected_values.length(), actual_values.length());\n+  for (int i = 0; i < expected_values.length(); i++) {\n+    const CracStackTrace::Frame::Value &expected = expected_values.at(i);\n+    const CracStackTrace::Frame::Value &actual = actual_values.at(i);\n+    ASSERT_NE(expected.type(), CracStackTrace::Frame::Value::Type::EMPTY); \/\/ Sanity check\n+    ASSERT_NE(expected.type(), CracStackTrace::Frame::Value::Type::OBJ);   \/\/ Sanity check\n+    EXPECT_EQ(expected.type(), actual.type()) << \"Wrong type of value #\" << i;\n+    if (expected.type() == CracStackTrace::Frame::Value::Type::REF) {\n+      EXPECT_EQ(expected.as_obj_id(), actual.as_obj_id()) << \"Wrong obj ref #\" << i;\n+    } else {\n+      EXPECT_EQ(expected.as_primitive(), actual.as_primitive()) << \"Wrong primitive #\" << i;\n+    }\n+  }\n+}\n+\n+static void check_stack_frames(const CracStackTrace &expected_trace,\n+                               const CracStackTrace &actual_trace) {\n+  EXPECT_EQ(expected_trace.thread_id(), actual_trace.thread_id());\n+  ASSERT_EQ(expected_trace.frames_num(), expected_trace.frames_num());\n+\n+  for (u4 i = 0; i < expected_trace.frames_num(); i++) {\n+    const CracStackTrace::Frame &expected_frame = expected_trace.frame(i);\n+    const CracStackTrace::Frame &actual_frame = actual_trace.frame(i);\n+\n+    EXPECT_EQ(expected_frame.method_name_id(),   actual_frame.method_name_id());\n+    EXPECT_EQ(expected_frame.method_sig_id(),    actual_frame.method_sig_id());\n+    EXPECT_EQ(expected_frame.method_kind(),      actual_frame.method_kind());\n+    EXPECT_EQ(expected_frame.method_holder_id(), actual_frame.method_holder_id());\n+    EXPECT_EQ(expected_frame.bci(),              actual_frame.bci());\n+\n+    check_stack_values(expected_frame.locals(),   actual_frame.locals());\n+    EXPECT_FALSE(testing::Test::HasFatalFailure() || testing::Test::HasNonfatalFailure()) << \"Wrong locals parsing in frame \" << i;\n+    check_stack_values(expected_frame.operands(), actual_frame.operands());\n+    EXPECT_FALSE(testing::Test::HasFatalFailure() || testing::Test::HasNonfatalFailure()) << \"Wrong operands parsing in frame \" << i;\n+    check_stack_values(expected_frame.monitor_owners(), actual_frame.monitor_owners());\n+    EXPECT_FALSE(testing::Test::HasFatalFailure() || testing::Test::HasNonfatalFailure()) << \"Wrong monitors parsing in frame \" << i;\n+  }\n+}\n+\n+static constexpr char CONTENTS_NO_TRACES[] =\n+    \"CRAC STACK DUMP 0.1\\0\" \/\/ Header\n+    \"\\x00\\x04\"              \/\/ Word size\n+    ;\n+\n+TEST(CracStackDumpParser, no_stack_traces) {\n+  fill_test_file(CONTENTS_NO_TRACES, sizeof(CONTENTS_NO_TRACES) - 1);\n+  ASSERT_FALSE(testing::Test::HasFatalFailure() || testing::Test::HasNonfatalFailure());\n+\n+  ParsedCracStackDump stack_dump;\n+  const char *err_msg = CracStackDumpParser::parse(TEST_FILENAME, &stack_dump);\n+  ASSERT_EQ(nullptr, err_msg) << \"Parsing error: \" << err_msg;\n+\n+  EXPECT_EQ(4U, stack_dump.word_size());\n+  EXPECT_EQ(0, stack_dump.stack_traces().length());\n+}\n+\n+static constexpr char CONTENTS_EMPTY_TRACE[] =\n+    \"CRAC STACK DUMP 0.1\\0\" \/\/ Header\n+    \"\\x00\\x04\"              \/\/ Word size\n+\n+    \"\\xab\\xcd\\xef\\x95\"      \/\/ Thread ID\n+    \"\\x00\\x00\\x00\\x00\"      \/\/ Number of frames\n+    ;\n+\n+TEST(CracStackDumpParser, empty_stack_trace) {\n+  fill_test_file(CONTENTS_EMPTY_TRACE, sizeof(CONTENTS_EMPTY_TRACE) - 1);\n+  ASSERT_FALSE(testing::Test::HasFatalFailure() || testing::Test::HasNonfatalFailure());\n+\n+  ParsedCracStackDump stack_dump;\n+  const char *err_msg = CracStackDumpParser::parse(TEST_FILENAME, &stack_dump);\n+  ASSERT_EQ(nullptr, err_msg) << \"Parsing error: \" << err_msg;\n+\n+  EXPECT_EQ(4U, stack_dump.word_size());\n+  ASSERT_EQ(1, stack_dump.stack_traces().length());\n+\n+  CracStackTrace expected_trace(\/* thread ID *\/ 0xabcdef95, \/* frames num *\/ 0);\n+\n+  check_stack_frames(expected_trace, *stack_dump.stack_traces().at(0));\n+  ASSERT_FALSE(testing::Test::HasFatalFailure() || testing::Test::HasNonfatalFailure());\n+}\n+\n+static constexpr char CONTENTS_NO_STACK_VALUES[] =\n+    \"CRAC STACK DUMP 0.1\\0\" \/\/ Header\n+    \"\\x00\\x04\"              \/\/ Word size\n+\n+    \"\\xab\\xcd\\xef\\x95\"      \/\/ Thread ID\n+    \"\\x00\\x00\\x00\\x01\"      \/\/ Number of frames\n+      \"\\x12\\x34\\x56\\x78\"      \/\/ Method name ID\n+      \"\\x87\\x65\\x43\\x21\"      \/\/ Method signature ID\n+      \"\\x00\"                  \/\/ Method kind - static\n+      \"\\x87\\x65\\x43\\x22\"      \/\/ Class ID\n+      \"\\x12\\x34\"              \/\/ BCI\n+      \"\\x00\\x00\"              \/\/ Locals num\n+      \"\\x00\\x00\"              \/\/ Operands num\n+      \"\\x00\\x00\\x00\\x00\"      \/\/ Monitors num\n+    ;\n+\n+TEST(CracStackDumpParser, stack_frame_with_no_stack_values) {\n+  fill_test_file(CONTENTS_NO_STACK_VALUES, sizeof(CONTENTS_NO_STACK_VALUES) - 1);\n+  ASSERT_FALSE(testing::Test::HasFatalFailure() || testing::Test::HasNonfatalFailure());\n+\n+  ParsedCracStackDump stack_dump;\n+  const char *err_msg = CracStackDumpParser::parse(TEST_FILENAME, &stack_dump);\n+  ASSERT_EQ(nullptr, err_msg) << \"Parsing error: \" << err_msg;\n+\n+  EXPECT_EQ(4U, stack_dump.word_size());\n+  ASSERT_EQ(1, stack_dump.stack_traces().length());\n+\n+  CracStackTrace expected_trace(\/* thread ID *\/ 0xabcdef95, \/* frames num *\/ 1);\n+\n+  auto &expected_frame = expected_trace.frame(0);\n+  expected_frame.set_method_name_id(0x12345678);\n+  expected_frame.set_method_sig_id(0x87654321);\n+  expected_frame.set_method_kind(MethodKind::Enum::STATIC);\n+  expected_frame.set_method_holder_id(0x87654322);\n+  expected_frame.set_bci(0x1234);\n+\n+  check_stack_frames(expected_trace, *stack_dump.stack_traces().at(0));\n+  ASSERT_FALSE(testing::Test::HasFatalFailure() || testing::Test::HasNonfatalFailure());\n+}\n+\n+static constexpr char CONTENTS_CORRECT_STACK_VALUES[] =\n+    \"CRAC STACK DUMP 0.1\\0\"              \/\/ Header\n+    \"\\x00\\x08\"                           \/\/ Word size\n+\n+    \"\\xab\\xcd\\xef\\x95\\xba\\xdc\\xfe\\x96\"   \/\/ Thread ID\n+    \"\\x00\\x00\\x00\\x01\"                   \/\/ Number of frames\n+      \"\\x12\\x34\\x56\\x78\\x01\\x23\\x45\\x67\"   \/\/ Method name ID\n+      \"\\x87\\x65\\x12\\x34\\x56\\x78\\x43\\x21\"   \/\/ Method signature ID\n+      \"\\x01\"                               \/\/ Method kind = instance\n+      \"\\x87\\x65\\x43\\x12\\x34\\x56\\x78\\x22\"   \/\/ Class ID\n+      \"\\x12\\x34\"                           \/\/ BCI\n+      \"\\x00\\x03\"                           \/\/ Locals num\n+        \"\\x00\"                               \/\/ Type = primitive\n+        \"\\x00\\x00\\x00\\x00\\xab\\xcd\\xef\\xab\"   \/\/ Value\n+        \"\\x00\"                               \/\/ Type = primitive\n+        \"\\xde\\xad\\xde\\xaf\\x00\\x00\\x00\\x00\"   \/\/ Value\n+        \"\\x00\"                               \/\/ Type = primitive\n+        \"\\x01\\x23\\x45\\x67\\x89\\xab\\xcd\\xef\"   \/\/ Value\n+      \"\\x00\\x02\"                           \/\/ Operands num\n+        \"\\x01\"                               \/\/ Type = object reference\n+        \"\\x00\\x00\\x7f\\xfa\\x40\\x05\\x65\\x50\"   \/\/ Value\n+        \"\\x00\"                               \/\/ Type = primitive\n+        \"\\x00\\x00\\x00\\x00\\x56\\x78\\x90\\xab\"   \/\/ Value\n+      \"\\x00\\x00\\x00\\x01\"                   \/\/ Monitors num\n+        \"\\x00\\x00\\x7f\\xfa\\x40\\x05\\x65\\x50\"   \/\/ Monitor owner ID\n+    ;\n+\n+TEST(CracStackDumpParser, stack_frame_with_correct_stack_values) {\n+  fill_test_file(CONTENTS_CORRECT_STACK_VALUES, sizeof(CONTENTS_CORRECT_STACK_VALUES) - 1);\n+  ASSERT_FALSE(testing::Test::HasFatalFailure() || testing::Test::HasNonfatalFailure());\n+\n+  ParsedCracStackDump stack_dump;\n+  const char *err_msg = CracStackDumpParser::parse(TEST_FILENAME, &stack_dump);\n+  ASSERT_EQ(nullptr, err_msg) << \"Parsing error: \" << err_msg;\n+\n+  EXPECT_EQ(8U, stack_dump.word_size());\n+  ASSERT_EQ(1, stack_dump.stack_traces().length());\n+\n+  CracStackTrace expected_trace(\/* thread ID *\/ 0xabcdef95badcfe96, \/* frames num *\/ 1);\n+\n+  auto &expected_frame = expected_trace.frame(0);\n+  expected_frame.set_method_name_id(0x1234567801234567);\n+  expected_frame.set_method_sig_id(0x8765123456784321);\n+  expected_frame.set_method_kind(MethodKind::Enum::INSTANCE);\n+  expected_frame.set_method_holder_id(0x8765431234567822);\n+  expected_frame.set_bci(0x1234);\n+  expected_frame.locals().append(CracStackTrace::Frame::Value::of_primitive(0x00000000abcdefab));\n+  expected_frame.locals().append(CracStackTrace::Frame::Value::of_primitive(0xdeaddeaf00000000));\n+  expected_frame.locals().append(CracStackTrace::Frame::Value::of_primitive(0x0123456789abcdef));\n+  expected_frame.operands().append(CracStackTrace::Frame::Value::of_obj_id(0x00007ffa40056550));\n+  expected_frame.operands().append(CracStackTrace::Frame::Value::of_primitive(0x00000000567890ab));\n+  expected_frame.monitor_owners().append(CracStackTrace::Frame::Value::of_obj_id(0x00007ffa40056550));\n+\n+  check_stack_frames(expected_trace, *stack_dump.stack_traces().at(0));\n+  ASSERT_FALSE(testing::Test::HasFatalFailure() || testing::Test::HasNonfatalFailure());\n+}\n+\n+static constexpr char CONTENTS_MULTIPLE_STACKS[] =\n+    \"CRAC STACK DUMP 0.1\\0\" \/\/ Header\n+    \"\\x00\\x04\"              \/\/ Word size\n+\n+    \"\\xab\\xcd\\xef\\x95\"      \/\/ Thread ID\n+    \"\\x00\\x00\\x00\\x02\"      \/\/ Number of frames\n+      \"\\xab\\xac\\xab\\xaa\"      \/\/ Method name ID\n+      \"\\xba\\xba\\xfe\\xda\"      \/\/ Method signature ID\n+      \"\\x00\"                  \/\/ Method kind = static\n+      \"\\x87\\x65\\x43\\x21\"      \/\/ Class ID\n+      \"\\x00\\x05\"              \/\/ BCI\n+      \"\\x00\\x01\"              \/\/ Locals num\n+        \"\\x00\"                  \/\/ Type = 4-byte primitive\n+        \"\\xab\\xcd\\xef\\xab\"      \/\/ Value\n+      \"\\x00\\x00\"              \/\/ Operands num\n+      \"\\x00\\x00\\x00\\x03\"      \/\/ Monitors num\n+        \"\\x7f\\xab\\xcd\\x35\"      \/\/ Monitor owner ID\n+        \"\\x7f\\xcd\\x01\\x23\"      \/\/ Monitor owner ID\n+        \"\\x7f\\xef\\x45\\x67\"      \/\/ Monitor owner ID\n+\n+      \"\\xba\\xca\\xba\\xca\"      \/\/ Method name ID\n+      \"\\xcc\\xdd\\xbb\\xaf\"      \/\/ Method signature ID\n+      \"\\x01\"                  \/\/ Method kind = instance\n+      \"\\x01\\x23\\x78\\x32\"      \/\/ Class ID\n+      \"\\x00\\x10\"              \/\/ BCI\n+      \"\\x00\\x00\"              \/\/ Locals num\n+      \"\\x00\\x00\"              \/\/ Operands num\n+      \"\\x00\\x00\\x00\\x00\"      \/\/ Monitors num\n+\n+    \"\\x00\\x11\\x32\\x09\"      \/\/ Thread ID\n+    \"\\x00\\x00\\x00\\x01\"      \/\/ Number of frames\n+      \"\\xfe\\xfe\\xca\\xca\"      \/\/ Method name ID\n+      \"\\x34\\x43\\x78\\x22\"      \/\/ Method signature ID\n+      \"\\x02\"                  \/\/ Method kind = overpass\n+      \"\\x21\\x21\\x74\\x55\"      \/\/ Class ID\n+      \"\\x00\\xfa\"              \/\/ BCI\n+      \"\\x00\\x00\"              \/\/ Locals num\n+      \"\\x00\\x02\"              \/\/ Operands num\n+        \"\\x01\"                  \/\/ Type = primitive\n+        \"\\x01\\x23\\x45\\x67\"      \/\/ Value\n+        \"\\x01\"                  \/\/ Type = primitive\n+        \"\\x89\\xab\\xcd\\xef\"      \/\/ Value\n+      \"\\x00\\x00\\x00\\x00\"      \/\/ Monitors num\n+    ;\n+\n+TEST(CracStackDumpParser, multiple_stacks_dumped) {\n+  fill_test_file(CONTENTS_MULTIPLE_STACKS, sizeof(CONTENTS_MULTIPLE_STACKS) - 1);\n+  ASSERT_FALSE(testing::Test::HasFatalFailure() || testing::Test::HasNonfatalFailure());\n+\n+  ParsedCracStackDump stack_dump;\n+  const char *err_msg = CracStackDumpParser::parse(TEST_FILENAME, &stack_dump);\n+  ASSERT_EQ(nullptr, err_msg) << \"Parsing error: \" << err_msg;\n+\n+  EXPECT_EQ(4U, stack_dump.word_size());\n+  ASSERT_EQ(2, stack_dump.stack_traces().length());\n+\n+  \/\/ Frames are dumped from youngest to oldest but stored in reverse (so that\n+  \/\/ the youngest is on top), so the frame indices are reversed here\n+  CracStackTrace expected_trace_1(\/* thread ID *\/ 0xabcdef95, \/* frames num *\/ 2);\n+  \/\/ First in the dump, last in the parsed array\n+  expected_trace_1.frame(1).set_method_name_id(0xabacabaa);\n+  expected_trace_1.frame(1).set_method_sig_id(0xbabafeda);\n+  expected_trace_1.frame(1).set_method_kind(MethodKind::Enum::STATIC);\n+  expected_trace_1.frame(1).set_method_holder_id(0x87654321);\n+  expected_trace_1.frame(1).set_bci(5);\n+  expected_trace_1.frame(1).locals().append(CracStackTrace::Frame::Value::of_primitive(0xabcdefab));\n+  expected_trace_1.frame(1).monitor_owners().append(CracStackTrace::Frame::Value::of_obj_id(0x7fabcd35));\n+  expected_trace_1.frame(1).monitor_owners().append(CracStackTrace::Frame::Value::of_obj_id(0x7fcd0123));\n+  expected_trace_1.frame(1).monitor_owners().append(CracStackTrace::Frame::Value::of_obj_id(0x7fef4567));\n+  \/\/ Last in the dump, first in the parsed array\n+  expected_trace_1.frame(0).set_method_name_id(0xbacabaca);\n+  expected_trace_1.frame(0).set_method_sig_id(0xccddbbaf);\n+  expected_trace_1.frame(0).set_method_kind(MethodKind::Enum::INSTANCE);\n+  expected_trace_1.frame(0).set_method_holder_id(0x01237832);\n+  expected_trace_1.frame(0).set_bci(0x10);\n+\n+  check_stack_frames(expected_trace_1, *stack_dump.stack_traces().at(0));\n+  ASSERT_FALSE(testing::Test::HasFatalFailure() || testing::Test::HasNonfatalFailure()) << \"Wrong parsing of trace #1\";\n+\n+  CracStackTrace expected_trace_2(\/* thread ID *\/ 0x00113209, \/* frames num *\/ 1);\n+\n+  expected_trace_2.frame(0).set_method_name_id(0xfefecaca);\n+  expected_trace_2.frame(0).set_method_sig_id(0x34437822);\n+  expected_trace_2.frame(0).set_method_kind(MethodKind::Enum::OVERPASS);\n+  expected_trace_2.frame(0).set_method_holder_id(0x21217455);\n+  expected_trace_2.frame(0).set_bci(0xfa);\n+  expected_trace_2.frame(0).operands().append(CracStackTrace::Frame::Value::of_primitive(0x01234567));\n+  expected_trace_2.frame(0).operands().append(CracStackTrace::Frame::Value::of_primitive(0x89abcdef));\n+\n+  check_stack_frames(expected_trace_1, *stack_dump.stack_traces().at(0));\n+  ASSERT_FALSE(testing::Test::HasFatalFailure() || testing::Test::HasNonfatalFailure()) << \"Wrong parsing of trace #2\";\n+}\n","filename":"test\/hotspot\/gtest\/runtime\/test_cracStackDumpParser.cpp","additions":287,"deletions":0,"binary":false,"changes":287,"status":"added"},{"patch":"@@ -0,0 +1,619 @@\n+#include \"precompiled.hpp\"\n+#include \"memory\/resourceArea.hpp\"\n+#include \"runtime\/os.hpp\"\n+#include \"unittest.hpp\"\n+#include \"utilities\/extendableArray.hpp\"\n+#include \"utilities\/globalDefinitions.hpp\"\n+#include \"utilities\/heapDumpParser.hpp\"\n+\n+constexpr char TEST_FILENAME[] = \"heapDumpParser_test.hprof\";\n+\n+static void fill_test_file(const char *contents, size_t size) {\n+  FILE *file = os::fopen(TEST_FILENAME, \"wb\");\n+  ASSERT_NE(nullptr, file) << \"Cannot open \" << TEST_FILENAME << \" for writing: \" << os::strerror(errno);\n+  EXPECT_EQ(1U, fwrite(contents, size, 1, file)) << \"Cannot write test data into \" << TEST_FILENAME << \": \" << os::strerror(errno);\n+  ASSERT_EQ(0, fclose(file)) << \"Cannot close the test file: \" << os::strerror(errno);\n+}\n+\n+struct RecordAmounts {\n+  size_t utf8;\n+  size_t load_class;\n+  size_t class_dump;\n+  size_t instance_dump;\n+  size_t obj_array_dump;\n+  size_t prim_array_dump;\n+};\n+\n+static void check_record_amounts(const RecordAmounts &expected, const ParsedHeapDump &actual) {\n+  EXPECT_EQ(expected.utf8,            static_cast<size_t>(actual.utf8s.number_of_entries()));\n+  EXPECT_EQ(expected.load_class,      static_cast<size_t>(actual.load_classes.number_of_entries()));\n+  EXPECT_EQ(expected.class_dump,      static_cast<size_t>(actual.class_dumps.number_of_entries()));\n+  EXPECT_EQ(expected.instance_dump,   static_cast<size_t>(actual.instance_dumps.number_of_entries()));\n+  EXPECT_EQ(expected.obj_array_dump,  static_cast<size_t>(actual.obj_array_dumps.number_of_entries()));\n+  EXPECT_EQ(expected.prim_array_dump, static_cast<size_t>(actual.prim_array_dumps.number_of_entries()));\n+}\n+\n+template <class ElemT, class SizeT, class Eq>\n+static void check_array_eq(const ExtendableArray<ElemT, SizeT> &l, const ExtendableArray<ElemT, SizeT> &r,\n+                           const char *array_name, const Eq &eq) {\n+  ASSERT_EQ(l.size(), r.size());\n+  for (SizeT i = 0; i < l.size(); i++) {\n+    EXPECT_PRED2(eq, l[i], r[i]) << array_name << \" array differs on i = \" << i;\n+  }\n+}\n+\n+static bool basic_value_eq(HeapDump::BasicValue l, HeapDump::BasicValue r, u1 type) {\n+  switch (type) {\n+    case HPROF_BOOLEAN:\n+      return l.as_boolean == r.as_boolean;\n+    case HPROF_CHAR:\n+      return l.as_char == r.as_char;\n+    case HPROF_FLOAT:\n+      return l.as_float == r.as_float;\n+    case HPROF_DOUBLE:\n+      return l.as_double == r.as_double;\n+    case HPROF_BYTE:\n+      return l.as_byte == r.as_byte;\n+    case HPROF_SHORT:\n+      return l.as_short == r.as_short;\n+    case HPROF_INT:\n+      return l.as_int == r.as_int;\n+    case HPROF_LONG:\n+      return l.as_long == r.as_long;\n+    default:\n+      EXPECT_TRUE(false) << \"Unknown basic value type: \" << type;\n+      ShouldNotReachHere();\n+      return false; \/\/ Make compilers happy\n+  }\n+}\n+\n+static constexpr char CONTENTS_UTF8[] =\n+    \"JAVA PROFILE 1.0.1\\0\"                                 \/\/ Header\n+    \"\\x00\\x00\\x00\\x04\"                                     \/\/ ID size\n+    \"\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\"                     \/\/ Dump timestamp\n+\n+    \"\\x01\"                                                 \/\/ HPROF_UTF8 tag\n+    \"\\x00\\x00\\x00\\x00\"                                     \/\/ Record timestamp\n+    \"\\x00\\x00\\x00\\x11\"                                     \/\/ Body size\n+\n+    \"\\x07\\x5b\\xcd\\x15\"                                     \/\/ ID = 123456789\n+    \"\\x48\\x65\\x6c\\x6c\\x6f\\x2c\\x20\\x77\\x6f\\x72\\x6c\\x64\\x21\" \/\/ \"Hello, world!\" in UTF-8\n+    ;\n+\n+TEST_VM(HeapDumpParser, single_utf8_record) {\n+  fill_test_file(CONTENTS_UTF8, sizeof(CONTENTS_UTF8) - 1);\n+  ASSERT_FALSE(testing::Test::HasFatalFailure() || testing::Test::HasNonfatalFailure());\n+\n+  ParsedHeapDump heap_dump;\n+  const char *err_msg = HeapDumpParser::parse(TEST_FILENAME, &heap_dump);\n+  ASSERT_EQ(nullptr, err_msg) << \"Parsing error: \" << err_msg;\n+\n+  check_record_amounts({1 \/* UTF-8 *\/, 0, 0, 0, 0, 0}, heap_dump);\n+  ASSERT_FALSE(testing::Test::HasNonfatalFailure()) << \"Unexpected amounts of records parsed\";\n+\n+  constexpr HeapDump::ID expected_id = 123456789;\n+  const char expected_str[] = \"Hello, world!\";\n+\n+  auto *record = heap_dump.utf8s.get(expected_id);\n+  ASSERT_NE(nullptr, record) << \"Record not found under the expected ID\";\n+\n+  EXPECT_EQ(expected_id, record->id);\n+  {\n+    ResourceMark rm;\n+    EXPECT_STREQ(expected_str, record->sym->as_C_string());\n+  }\n+}\n+\n+static constexpr char CONTENTS_LOAD_CLASS[] =\n+    \"JAVA PROFILE 1.0.1\\0\"             \/\/ Header\n+    \"\\x00\\x00\\x00\\x08\"                 \/\/ ID size\n+    \"\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\" \/\/ Dump timestamp\n+\n+    \"\\x02\"                             \/\/ HPROF_LOAD_CLASS tag\n+    \"\\x00\\x00\\x00\\x00\"                 \/\/ Record timestamp\n+    \"\\x00\\x00\\x00\\x18\"                 \/\/ Body size\n+\n+    \"\\x01\\x02\\x03\\x04\"                 \/\/ class serial\n+    \"\\x00\\x00\\x00\\x06\\xc7\\x93\\x73\\xb8\" \/\/ class ID\n+    \"\\x00\\x00\\x00\\x01\"                 \/\/ stack trace serial\n+    \"\\x00\\x00\\x7f\\xfa\\x40\\x05\\x65\\x50\" \/\/ class name ID\n+    ;\n+\n+TEST_VM(HeapDumpParser, single_load_class_record) {\n+  fill_test_file(CONTENTS_LOAD_CLASS, sizeof(CONTENTS_LOAD_CLASS) - 1);\n+  ASSERT_FALSE(testing::Test::HasFatalFailure() || testing::Test::HasNonfatalFailure());\n+\n+  ParsedHeapDump heap_dump;\n+  const char *err_msg = HeapDumpParser::parse(TEST_FILENAME, &heap_dump);\n+  ASSERT_EQ(nullptr, err_msg) << \"Parsing error: \" << err_msg;\n+\n+  check_record_amounts({0, 1 \/* load class *\/, 0, 0, 0, 0}, heap_dump);\n+  ASSERT_FALSE(testing::Test::HasNonfatalFailure()) << \"Unexpected amounts of records parsed\";\n+\n+  constexpr HeapDump::LoadClass expected = {\n+      0x01020304,         \/\/ class serial\n+      0x00000006c79373b8, \/\/ class ID\n+      0x00000001,         \/\/ stack trace serial\n+      0x00007ffa40056550  \/\/ class name ID\n+  };\n+\n+  auto *record = heap_dump.load_classes.get(expected.class_id);\n+  ASSERT_NE(nullptr, record) << \"Record not found under the expected ID\";\n+\n+  EXPECT_EQ(expected.serial, record->serial);\n+  EXPECT_EQ(expected.class_id, record->class_id);\n+  EXPECT_EQ(expected.stack_trace_serial, record->stack_trace_serial);\n+  EXPECT_EQ(expected.class_name_id, record->class_name_id);\n+}\n+\n+static constexpr char CONTENTS_CLASS_DUMP[] =\n+    \"JAVA PROFILE 1.0.1\\0\"               \/\/ Header\n+    \"\\x00\\x00\\x00\\x08\"                   \/\/ ID size\n+    \"\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\"   \/\/ Dump timestamp\n+\n+    \"\\x0c\"                               \/\/ HPROF_HEAP_DUMP tag\n+    \"\\x00\\x00\\x00\\x00\"                   \/\/ Record timestamp\n+    \"\\x00\\x00\\x00\\x7e\"                   \/\/ Body size\n+\n+    \"\\x20\"                               \/\/ HPROF_GC_CLASS_DUMP tag\n+\n+    \"\\x00\\x00\\x00\\x06\\xc7\\x93\\x73\\xf8\"   \/\/ class ID\n+    \"\\x12\\x34\\x56\\x78\"                   \/\/ stack trace serial\n+    \"\\x00\\x00\\x00\\x06\\xc7\\x93\\x3a\\x58\"   \/\/ super ID\n+    \"\\x00\\x00\\x00\\x06\\xc7\\x92\\x29\\x38\"   \/\/ class loader ID\n+    \"\\x00\\x00\\x00\\x06\\xc7\\x90\\x31\\x5f\"   \/\/ signers ID\n+    \"\\x00\\x00\\x00\\x06\\xc7\\x8d\\x85\\xc0\"   \/\/ protection domain ID\n+    \"\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\"   \/\/ reserved\n+    \"\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\"   \/\/ reserved\n+    \"\\x00\\x00\\x00\\x18\"                   \/\/ instance size\n+    \"\\x00\\x01\"                           \/\/ constant pool size\n+      \"\\x00\\x01\"                           \/\/ index\n+      \"\\x09\"                               \/\/ type = short\n+      \"\\x67\\x89\"                           \/\/ value\n+    \"\\x00\\x02\"                           \/\/ static fields number\n+      \"\\x00\\x00\\x7f\\xfa\\x2c\\x13\\xca\\xd0\"   \/\/ name ID\n+      \"\\x04\"                               \/\/ type = boolean\n+      \"\\x01\"                               \/\/ value\n+      \"\\x00\\x00\\x7f\\xfa\\x94\\x00\\x98\\x18\"   \/\/ name ID\n+      \"\\x0a\"                               \/\/ type = int\n+      \"\\x12\\xab\\xcd\\xef\"                   \/\/ value\n+    \"\\x00\\x03\"                           \/\/ instance fields number\n+      \"\\x00\\x00\\x7f\\xfa\\x90\\x16\\xad\\x30\"   \/\/ name ID\n+      \"\\x05\"                               \/\/ type = char\n+      \"\\x00\\x00\\x7f\\xfa\\x94\\x00\\x98\\x18\"   \/\/ name ID\n+      \"\\x02\"                               \/\/ type = object\n+      \"\\x00\\x00\\x7f\\xfa\\x90\\x3c\\x5a\\xf8\"   \/\/ name ID\n+      \"\\x0b\"                               \/\/ type = long\n+    ;\n+\n+TEST_VM(HeapDumpParser, single_class_dump_subrecord) {\n+  fill_test_file(CONTENTS_CLASS_DUMP, sizeof(CONTENTS_CLASS_DUMP) - 1);\n+  ASSERT_FALSE(testing::Test::HasFatalFailure() || testing::Test::HasNonfatalFailure());\n+\n+  ParsedHeapDump heap_dump;\n+  const char *err_msg = HeapDumpParser::parse(TEST_FILENAME, &heap_dump);\n+  ASSERT_EQ(nullptr, err_msg) << \"Parsing error: \" << err_msg;\n+\n+  check_record_amounts({0, 0, 1 \/* class dump *\/, 0, 0, 0}, heap_dump);\n+  ASSERT_FALSE(testing::Test::HasNonfatalFailure()) << \"Unexpected amounts of records parsed\";\n+\n+  HeapDump::ClassDump expected = {\n+      0x00000006c79373f8, \/\/ ID\n+      0x12345678,         \/\/ stack trace serial\n+      0x00000006c7933a58, \/\/ super ID\n+      0x00000006c7922938, \/\/ class loader ID\n+      0x00000006c790315f, \/\/ signers ID\n+      0x00000006c78d85c0, \/\/ protection domain ID\n+      0x00000018          \/\/ instance size\n+  };\n+  expected.constant_pool.extend(1);\n+  expected.constant_pool[0] = {\/* index *\/ 0x01, \/* type *\/ 0x09, \/* value *\/ {0x6789}};\n+  expected.static_fields.extend(2);\n+  expected.static_fields[0] = {{\/* name ID *\/ 0x00007ffa2c13cad0, \/* type *\/ 0x04}, \/* value *\/ {0x01}};\n+  expected.static_fields[1] = {{\/* name ID *\/ 0x00007ffa94009818, \/* type *\/ 0x0a}, \/* value *\/ {0x12abcdef}};\n+  expected.instance_field_infos.extend(3);\n+  expected.instance_field_infos[0] = {\/* name ID *\/ 0x00007ffa9016ad30, \/* type *\/ 0x05};\n+  expected.instance_field_infos[1] = {\/* name ID *\/ 0x00007ffa94009818, \/* type *\/ 0x02};\n+  expected.instance_field_infos[2] = {\/* name ID *\/ 0x00007ffa903c5af8, \/* type *\/ 0x0b};\n+\n+  auto *record = heap_dump.class_dumps.get(expected.id);\n+  ASSERT_NE(nullptr, record) << \"Record not found under the expected ID\";\n+\n+  EXPECT_EQ(expected.id,                   record->id);\n+  EXPECT_EQ(expected.stack_trace_serial,   record->stack_trace_serial);\n+  EXPECT_EQ(expected.super_id,             record->super_id);\n+  EXPECT_EQ(expected.class_loader_id,      record->class_loader_id);\n+  EXPECT_EQ(expected.signers_id,           record->signers_id);\n+  EXPECT_EQ(expected.protection_domain_id, record->protection_domain_id);\n+  EXPECT_EQ(expected.instance_size,        record->instance_size);\n+  check_array_eq(expected.constant_pool, record->constant_pool, \"Constant pool\", [&](const auto &l, const auto &r) {\n+    return l.index == r.index && l.type == r.type && basic_value_eq(l.value, r.value, l.type);\n+  });\n+  check_array_eq(expected.static_fields, record->static_fields, \"Static fields\", [&](const auto &l, const auto &r) {\n+    return l.info.name_id == r.info.name_id && l.info.type == r.info.type && basic_value_eq(l.value, r.value, l.info.type);\n+  });\n+  check_array_eq(expected.instance_field_infos, record->instance_field_infos, \"Instance field infos\",\n+                 [&](const auto &l, const auto &r) { return l.name_id == r.name_id && l.type == r.type; });\n+}\n+\n+static constexpr char CONTENTS_INSTANCE_DUMP[] =\n+    \"JAVA PROFILE 1.0.1\\0\"             \/\/ Header\n+    \"\\x00\\x00\\x00\\x08\"                 \/\/ ID size\n+    \"\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\" \/\/ Dump timestamp\n+\n+    \"\\x0c\"                             \/\/ HPROF_HEAP_DUMP tag\n+    \"\\x00\\x00\\x00\\x00\"                 \/\/ Record timestamp\n+    \"\\x00\\x00\\x00\\x1f\"                 \/\/ Body size\n+\n+    \"\\x21\"                             \/\/ HPROF_GC_INSTANCE_DUMP tag\n+\n+    \"\\x00\\x00\\x00\\x06\\xc7\\x56\\x78\\x90\" \/\/ ID\n+    \"\\x87\\x65\\x43\\x21\"                 \/\/ stack trace serial\n+    \"\\x00\\x00\\x00\\x06\\xc7\\x93\\x73\\xf8\" \/\/ class ID\n+    \"\\x00\\x00\\x00\\x06\"                 \/\/ following field bytes num\n+      \"\\x00\\x00\\x43\\x21\"                 \/\/ int field\n+      \"\\x67\\x89\"                         \/\/ short field\n+    ;\n+\n+TEST_VM(HeapDumpParser, single_instance_dump_subrecord) {\n+  fill_test_file(CONTENTS_INSTANCE_DUMP, sizeof(CONTENTS_INSTANCE_DUMP) - 1);\n+  ASSERT_FALSE(testing::Test::HasFatalFailure() || testing::Test::HasNonfatalFailure());\n+\n+  ParsedHeapDump heap_dump;\n+  const char *err_msg = HeapDumpParser::parse(TEST_FILENAME, &heap_dump);\n+  ASSERT_EQ(nullptr, err_msg) << \"Parsing error: \" << err_msg;\n+\n+  check_record_amounts({0, 0, 0, 1 \/* instance dump *\/, 0, 0}, heap_dump);\n+  ASSERT_FALSE(testing::Test::HasNonfatalFailure()) << \"Unexpected amounts of records parsed\";\n+\n+  HeapDump::InstanceDump expected = {\n+      0x00000006c7567890, \/\/ ID\n+      0x87654321,         \/\/ stack trace serial\n+      0x00000006c79373f8  \/\/ class ID\n+  };\n+  expected.fields_data.extend(sizeof(jint) + sizeof(jshort));\n+  memcpy(expected.fields_data.mem(), \"\\x00\\x00\\x43\\x21\\x67\\x89\", sizeof(jint) + sizeof(jshort));\n+\n+  auto *record = heap_dump.instance_dumps.get(expected.id);\n+  ASSERT_NE(nullptr, record) << \"Record not found under the expected ID\";\n+\n+  EXPECT_EQ(expected.id,                 record->id);\n+  EXPECT_EQ(expected.stack_trace_serial, record->stack_trace_serial);\n+  EXPECT_EQ(expected.class_id,           record->class_id);\n+  check_array_eq(expected.fields_data, record->fields_data, \"Fields data\", std::equal_to<>());\n+}\n+\n+static constexpr char CONTENTS_INSTANCE_DUMP_FIELD_READING[] =\n+    \"JAVA PROFILE 1.0.1\\0\"             \/\/ Header\n+    \"\\x00\\x00\\x00\\x04\"                 \/\/ ID size\n+    \"\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\" \/\/ Dump timestamp\n+\n+    \"\\x0c\"                             \/\/ HPROF_HEAP_DUMP tag\n+    \"\\x00\\x00\\x00\\x00\"                 \/\/ Record timestamp\n+    \"\\x00\\x00\\x00\\x20\"                 \/\/ Body size\n+\n+    \"\\x21\"                             \/\/ HPROF_GC_INSTANCE_DUMP tag\n+\n+    \"\\x00\\x00\\x00\\x00\"                 \/\/ ID\n+    \"\\x00\\x00\\x00\\x00\"                 \/\/ stack trace serial\n+    \"\\x00\\x00\\x00\\x00\"                 \/\/ class ID\n+    \"\\x00\\x00\\x00\\x0f\"                 \/\/ following field bytes num\n+      \"\\x12\\x34\"                         \/\/ short = 4660\n+      \"\\xf2\\xf4\\xf6\\xf8\"                 \/\/ int = -218827016\n+      \"\\x01\"                             \/\/ boolean = true\n+      \"\\x43\\x40\\x91\\x80\"                 \/\/ float = 192.568359375 (exact)\n+      \"\\xc7\\x89\\x91\\x24\"                 \/\/ ID\n+    ;\n+\n+TEST_VM(HeapDumpParser, instance_dump_read_field) {\n+  fill_test_file(CONTENTS_INSTANCE_DUMP_FIELD_READING, sizeof(CONTENTS_INSTANCE_DUMP_FIELD_READING) - 1);\n+  ASSERT_FALSE(testing::Test::HasFatalFailure() || testing::Test::HasNonfatalFailure());\n+\n+  ParsedHeapDump heap_dump;\n+  const char *err_msg = HeapDumpParser::parse(TEST_FILENAME, &heap_dump);\n+  ASSERT_EQ(nullptr, err_msg) << \"Parsing error: \" << err_msg;\n+\n+  check_record_amounts({0, 0, 0, 1 \/* instance dump *\/, 0, 0}, heap_dump);\n+  ASSERT_FALSE(testing::Test::HasNonfatalFailure()) << \"Unexpected amounts of records parsed\";\n+\n+  constexpr u4           expected_id_size = 0x00000004;\n+  constexpr HeapDump::ID expected_id      = 0;\n+  constexpr jshort       expected_field1  = 4660;\n+  constexpr jint         expected_field2  = -218827016;\n+  constexpr jboolean     expected_field3  = 1;\n+  constexpr jfloat       expected_field4  = 192.568359375F;\n+  constexpr HeapDump::ID expected_field5  = 0xc7899124;\n+  constexpr u2 expected_field_data_size = sizeof(expected_field1) + sizeof(expected_field2) + sizeof(expected_field3) +\n+                                          sizeof(expected_field4) + expected_id_size;\n+\n+  auto *record = heap_dump.instance_dumps.get(0);\n+  ASSERT_NE(nullptr, record) << \"Record not found under the expected ID\";\n+\n+  ASSERT_EQ(expected_field_data_size, record->fields_data.size());\n+\n+  EXPECT_EQ(expected_field1, record->read_field(0, T_SHORT, expected_id_size).as_short);\n+  EXPECT_EQ(expected_field2, record->read_field(sizeof(expected_field1),\n+                                                T_INT, expected_id_size).as_int);\n+  EXPECT_EQ(expected_field3, record->read_field(sizeof(expected_field1) + sizeof(expected_field2),\n+                                                T_BOOLEAN, expected_id_size).as_boolean);\n+  EXPECT_EQ(expected_field4, record->read_field(sizeof(expected_field1) + sizeof(expected_field2) + sizeof(expected_field3),\n+                                                T_FLOAT, expected_id_size).as_float);\n+  EXPECT_EQ(expected_field5, record->read_field(sizeof(expected_field1) + sizeof(expected_field2) + sizeof(expected_field3) + sizeof(expected_field4),\n+                                                T_OBJECT, expected_id_size).as_object_id);\n+}\n+\n+static constexpr char CONTENTS_OBJ_ARRAY_DUMP[] =\n+    \"JAVA PROFILE 1.0.1\\0\"             \/\/ Header\n+    \"\\x00\\x00\\x00\\x04\"                 \/\/ ID size\n+    \"\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\" \/\/ Dump timestamp\n+\n+    \"\\x0c\"                             \/\/ HPROF_HEAP_DUMP tag\n+    \"\\x00\\x00\\x00\\x00\"                 \/\/ Record timestamp\n+    \"\\x00\\x00\\x00\\x1d\"                 \/\/ Body size\n+\n+    \"\\x22\"                             \/\/ HPROF_GC_OBJ_ARRAY_DUMP tag\n+\n+    \"\\xc7\\x89\\x91\\x24\"                 \/\/ ID\n+    \"\\x13\\x24\\x35\\x46\"                 \/\/ stack trace serial\n+    \"\\x00\\x00\\x00\\x03\"                 \/\/ elements num\n+    \"\\xc7\\x43\\xab\\xd8\"                 \/\/ array class ID\n+      \"\\x12\\x34\\x56\\x78\"                 \/\/ elem 0 ID\n+      \"\\x9a\\xbc\\xde\\xf4\"                 \/\/ elem 1 ID\n+      \"\\x32\\x10\\xff\\x60\"                 \/\/ elem 2 ID\n+    ;\n+\n+TEST_VM(HeapDumpParser, single_obj_array_dump_subrecord) {\n+  fill_test_file(CONTENTS_OBJ_ARRAY_DUMP, sizeof(CONTENTS_OBJ_ARRAY_DUMP) - 1);\n+  ASSERT_FALSE(testing::Test::HasFatalFailure() || testing::Test::HasNonfatalFailure());\n+\n+  ParsedHeapDump heap_dump;\n+  const char *err_msg = HeapDumpParser::parse(TEST_FILENAME, &heap_dump);\n+  ASSERT_EQ(nullptr, err_msg) << \"Parsing error: \" << err_msg;\n+\n+  check_record_amounts({0, 0, 0, 0, 1 \/* obj array dump *\/, 0}, heap_dump);\n+  ASSERT_FALSE(testing::Test::HasNonfatalFailure()) << \"Unexpected amounts of records parsed\";\n+\n+  HeapDump::ObjArrayDump expected = {\n+      0xc7899124, \/\/ ID\n+      0x13243546, \/\/ stack trace serial\n+      0xc743abd8  \/\/ array class ID\n+  };\n+  expected.elem_ids.extend(3);\n+  expected.elem_ids[0] = 0x12345678;\n+  expected.elem_ids[1] = 0x9abcdef4;\n+  expected.elem_ids[2] = 0x3210ff60;\n+\n+  auto *record = heap_dump.obj_array_dumps.get(expected.id);\n+  ASSERT_NE(nullptr, record) << \"Record not found under the expected ID\";\n+\n+  EXPECT_EQ(expected.id, record->id);\n+  EXPECT_EQ(expected.stack_trace_serial, record->stack_trace_serial);\n+  EXPECT_EQ(expected.array_class_id, record->array_class_id);\n+  check_array_eq(expected.elem_ids, record->elem_ids, \"Element IDs\", std::equal_to<>());\n+}\n+\n+static constexpr char CONTENTS_PRIM_ARRAY_DUMP[] =\n+    \"JAVA PROFILE 1.0.1\\0\"             \/\/ Header\n+    \"\\x00\\x00\\x00\\x08\"                 \/\/ ID size\n+    \"\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\" \/\/ Dump timestamp\n+\n+    \"\\x0c\"                             \/\/ HPROF_HEAP_DUMP tag\n+    \"\\x00\\x00\\x00\\x00\"                 \/\/ Record timestamp\n+    \"\\x00\\x00\\x00\\x16\"                 \/\/ Body size\n+\n+    \"\\x23\"                             \/\/ HPROF_GC_PRIM_ARRAY_DUMP tag\n+\n+    \"\\xfa\\xbc\\xde\\xf0\\x12\\x34\\x56\\x78\" \/\/ ID\n+    \"\\x13\\x24\\x35\\x46\"                 \/\/ stack trace serial\n+    \"\\x00\\x00\\x00\\x02\"                 \/\/ elements num\n+    \"\\x09\"                             \/\/ element type = short\n+      \"\\x12\\x34\"                         \/\/ elem 0 = 4660\n+      \"\\xff\\xff\"                         \/\/ elem 1 = -1\n+    ;\n+\n+TEST_VM(HeapDumpParser, single_prim_array_dump_subrecord) {\n+  fill_test_file(CONTENTS_PRIM_ARRAY_DUMP, sizeof(CONTENTS_PRIM_ARRAY_DUMP) - 1);\n+  ASSERT_FALSE(testing::Test::HasFatalFailure() || testing::Test::HasNonfatalFailure());\n+\n+  ParsedHeapDump heap_dump;\n+  const char *err_msg = HeapDumpParser::parse(TEST_FILENAME, &heap_dump);\n+  ASSERT_EQ(nullptr, err_msg) << \"Parsing error: \" << err_msg;\n+\n+  check_record_amounts({0, 0, 0, 0, 0, 1 \/* prim array dump *\/}, heap_dump);\n+  ASSERT_FALSE(testing::Test::HasNonfatalFailure()) << \"Unexpected amounts of records parsed\";\n+\n+  HeapDump::PrimArrayDump expected = {\n+      0xfabcdef012345678, \/\/ ID\n+      0x13243546,         \/\/ stack trace serial\n+      0x00000002,         \/\/ elements num\n+      0x09                \/\/ element type\n+  };\n+  expected.elems_data.extend(expected.elems_num * sizeof(jshort));\n+  constexpr jshort expected_elems[] = {4660, -1};\n+  memcpy(expected.elems_data.mem(), &expected_elems, expected.elems_num * sizeof(jshort));\n+\n+  auto *record = heap_dump.prim_array_dumps.get(expected.id);\n+  ASSERT_NE(nullptr, record) << \"Record not found under the expected ID\";\n+\n+  EXPECT_EQ(expected.id, record->id);\n+  EXPECT_EQ(expected.stack_trace_serial, record->stack_trace_serial);\n+  EXPECT_EQ(expected.elems_num, record->elems_num);\n+  EXPECT_EQ(expected.elem_type, record->elem_type);\n+  check_array_eq(expected.elems_data, record->elems_data, \"Elements data\", std::equal_to<>());\n+}\n+\n+static constexpr char CONTENTS_BASIC_VALUES[] =\n+    \"JAVA PROFILE 1.0.1\\0\"               \/\/ Header\n+    \"\\x00\\x00\\x00\\x08\"                   \/\/ ID size\n+    \"\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\"   \/\/ Dump timestamp\n+\n+    \"\\x0c\"                               \/\/ HPROF_HEAP_DUMP tag\n+    \"\\x00\\x00\\x00\\x00\"                   \/\/ Record timestamp\n+    \"\\x00\\x00\\x00\\x88\"                   \/\/ Body size\n+\n+    \"\\x20\"                               \/\/ HPROF_GC_CLASS_DUMP tag\n+\n+    \"\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\"   \/\/ class ID\n+    \"\\x00\\x00\\x00\\x00\"                   \/\/ stack trace serial\n+    \"\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\"   \/\/ super ID\n+    \"\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\"   \/\/ class loader ID\n+    \"\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\"   \/\/ signers ID\n+    \"\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\"   \/\/ protection domain ID\n+    \"\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\"   \/\/ reserved\n+    \"\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\"   \/\/ reserved\n+    \"\\x00\\x00\\x00\\x25\"                   \/\/ instance size\n+    \"\\x00\\x09\"                           \/\/ constant pool size\n+      \"\\x00\\x01\"                           \/\/ index\n+      \"\\x02\"                               \/\/ type = object\n+      \"\\x00\\x00\\x00\\x06\\xc7\\x92\\x53\\x98\"   \/\/ value\n+\n+      \"\\x00\\x02\"                           \/\/ index\n+      \"\\x04\"                               \/\/ type = boolean\n+      \"\\x01\"                               \/\/ value = true\n+\n+      \"\\x00\\x03\"                           \/\/ index\n+      \"\\x05\"                               \/\/ type = char\n+      \"\\x00\\x4a\"                           \/\/ value = 'J'\n+\n+      \"\\x00\\x04\"                           \/\/ index\n+      \"\\x06\"                               \/\/ type = float\n+      \"\\x43\\x40\\x91\\x80\"                   \/\/ value = 192.568359375 (exact)\n+\n+      \"\\x00\\x05\"                           \/\/ index\n+      \"\\x07\"                               \/\/ type = double\n+      \"\\x43\\x11\\x8b\\x54\\xf2\\x2a\\xeb\\x01\"   \/\/ value = 1234567890123456.25 (exact)\n+\n+      \"\\x00\\x06\"                           \/\/ index\n+      \"\\x08\"                               \/\/ type = byte\n+      \"\\x79\"                               \/\/ value = 121\n+\n+      \"\\x00\\x07\"                           \/\/ index\n+      \"\\x09\"                               \/\/ type = short\n+      \"\\x2f\\x59\"                           \/\/ value = 12121\n+\n+      \"\\x00\\x08\"                           \/\/ index\n+      \"\\x0a\"                               \/\/ type = int\n+      \"\\x07\\x39\\x8c\\xd9\"                   \/\/ value = 121212121\n+\n+      \"\\x00\\x09\"                           \/\/ index\n+      \"\\x0b\"                               \/\/ type = long\n+      \"\\x7f\\xff\\xff\\xff\\xff\\xff\\xff\\xff\"   \/\/ value = 9223372036854775807\n+    \"\\x00\\x00\"                           \/\/ static fields number\n+    \"\\x00\\x00\"                           \/\/ instance fields number\n+    ;\n+\n+TEST_VM(HeapDumpParser, basic_values_get_right_values) {\n+  fill_test_file(CONTENTS_BASIC_VALUES, sizeof(CONTENTS_BASIC_VALUES) - 1);\n+  ASSERT_FALSE(testing::Test::HasFatalFailure() || testing::Test::HasNonfatalFailure());\n+\n+  ParsedHeapDump heap_dump;\n+  const char *err_msg = HeapDumpParser::parse(TEST_FILENAME, &heap_dump);\n+  ASSERT_EQ(nullptr, err_msg) << \"Parsing error: \" << err_msg;\n+\n+  check_record_amounts({0, 0, 1 \/* class dump *\/, 0, 0, 0}, heap_dump);\n+  ASSERT_FALSE(testing::Test::HasNonfatalFailure()) << \"Unexpected amounts of records parsed\";\n+\n+  auto *record = heap_dump.class_dumps.get(0);\n+  ASSERT_NE(nullptr, record) << \"Record not found under the expected ID\";\n+\n+  const auto &basic_values = record->constant_pool;\n+\n+  EXPECT_EQ(static_cast<HeapDump::ID>(0x00000006c7925398),  basic_values[0].value.as_object_id);\n+  EXPECT_EQ(static_cast<jboolean>    (true),                basic_values[1].value.as_boolean);\n+  EXPECT_EQ(static_cast<jchar>       ('J'),                 basic_values[2].value.as_char);\n+  EXPECT_EQ(static_cast<jfloat>      (192.568359375F),      basic_values[3].value.as_float);\n+  EXPECT_EQ(static_cast<jdouble>     (1234567890123456.25), basic_values[4].value.as_double);\n+  EXPECT_EQ(static_cast<jbyte>       (121),                 basic_values[5].value.as_byte);\n+  EXPECT_EQ(static_cast<jshort>      (12121),               basic_values[6].value.as_short);\n+  EXPECT_EQ(static_cast<jint>        (121212121),           basic_values[7].value.as_int);\n+  EXPECT_EQ(static_cast<jlong>       (9223372036854775807), basic_values[8].value.as_long);\n+}\n+\n+static constexpr char CONTENTS_SPECIAL_FLOATS[] =\n+    \"JAVA PROFILE 1.0.1\\0\"             \/\/ Header\n+    \"\\x00\\x00\\x00\\x04\"                 \/\/ ID size\n+    \"\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\" \/\/ Dump timestamp\n+\n+    \"\\x0c\"                             \/\/ HPROF_HEAP_DUMP tag\n+    \"\\x00\\x00\\x00\\x00\"                 \/\/ Record timestamp\n+    \"\\x00\\x00\\x00\\x6A\"                 \/\/ Body size\n+\n+    \"\\x20\"                             \/\/ HPROF_GC_CLASS_DUMP tag\n+\n+    \"\\x00\\x00\\x00\\x00\"                 \/\/ class ID\n+    \"\\x00\\x00\\x00\\x00\"                 \/\/ stack trace serial\n+    \"\\x00\\x00\\x00\\x00\"                 \/\/ super ID\n+    \"\\x00\\x00\\x00\\x00\"                 \/\/ class loader ID\n+    \"\\x00\\x00\\x00\\x00\"                 \/\/ signers ID\n+    \"\\x00\\x00\\x00\\x00\"                 \/\/ protection domain ID\n+    \"\\x00\\x00\\x00\\x00\"                 \/\/ reserved\n+    \"\\x00\\x00\\x00\\x00\"                 \/\/ reserved\n+    \"\\x00\\x00\\x00\\x24\"                 \/\/ instance size\n+    \"\\x00\\x09\"                         \/\/ constant pool size 39\n+      \"\\x00\\x01\"                         \/\/ index\n+      \"\\x06\"                             \/\/ type = float\n+      \"\\x43\\x00\\x00\\x00\"                 \/\/ value = 128 -- normal value\n+\n+      \"\\x00\\x02\"                         \/\/ index\n+      \"\\x06\"                             \/\/ type = float\n+      \"\\xc3\\x00\\x00\\x00\"                 \/\/ value = -128 -- normal value, sign bit set\n+\n+      \"\\x00\\x03\"                         \/\/ index\n+      \"\\x06\"                             \/\/ type = float\n+      \"\\x00\\x00\\x00\\x00\"                 \/\/ value = +0\n+\n+      \"\\x00\\x04\"                         \/\/ index\n+      \"\\x06\"                             \/\/ type = float\n+      \"\\x80\\x00\\x00\\x00\"                 \/\/ value = -0\n+\n+      \"\\x00\\x05\"                         \/\/ index\n+      \"\\x06\"                             \/\/ type = float\n+      \"\\x7f\\x80\\x00\\x00\"                 \/\/ value = +inf\n+\n+      \"\\x00\\x06\"                         \/\/ index\n+      \"\\x06\"                             \/\/ type = float\n+      \"\\xff\\x80\\x00\\x00\"                 \/\/ value = -inf\n+\n+      \"\\x00\\x07\"                         \/\/ index\n+      \"\\x06\"                             \/\/ type = float\n+      \"\\x7f\\xff\\xff\\xff\"                 \/\/ value = nan\n+\n+      \"\\x00\\x08\"                         \/\/ index\n+      \"\\x06\"                             \/\/ type = float\n+      \"\\xff\\x80\\x00\\x01\"                 \/\/ value = nan (another variant)\n+\n+      \"\\x00\\x09\"                         \/\/ index\n+      \"\\x06\"                             \/\/ type = float\n+      \"\\x7f\\xc0\\x00\\x00\"                 \/\/ value = nan (another variant)\n+    \"\\x00\\x00\"                         \/\/ static fields number\n+    \"\\x00\\x00\"                         \/\/ instance fields number\n+    ;\n+\n+TEST_VM(HeapDumpParser, parsing_special_float_values) {\n+  fill_test_file(CONTENTS_SPECIAL_FLOATS, sizeof(CONTENTS_SPECIAL_FLOATS) - 1);\n+  ASSERT_FALSE(testing::Test::HasFatalFailure() || testing::Test::HasNonfatalFailure());\n+\n+  ParsedHeapDump heap_dump;\n+  const char *err_msg = HeapDumpParser::parse(TEST_FILENAME, &heap_dump);\n+  ASSERT_EQ(nullptr, err_msg) << \"Parsing error: \" << err_msg;\n+\n+  check_record_amounts({0, 0, 1 \/* class dump *\/, 0, 0, 0}, heap_dump);\n+  ASSERT_FALSE(testing::Test::HasNonfatalFailure()) << \"Unexpected amounts of records parsed\";\n+\n+  auto *record = heap_dump.class_dumps.get(0);\n+  ASSERT_NE(nullptr, record) << \"Record not found under the expected ID\";\n+\n+  const auto &floats = record->constant_pool;\n+\n+  STATIC_ASSERT(std::numeric_limits<jfloat>::is_iec559);\n+\n+  EXPECT_EQ(128.0F,                                       floats[0].value.as_float);\n+  EXPECT_EQ(-128.0F,                                      floats[1].value.as_float);\n+  EXPECT_EQ(0.0F,                                         floats[2].value.as_float);\n+  EXPECT_EQ(-0.0F,                                        floats[3].value.as_float);\n+  EXPECT_EQ(std::numeric_limits<jfloat>::infinity(),      floats[4].value.as_float);\n+  EXPECT_EQ(-std::numeric_limits<jfloat>::infinity(),     floats[5].value.as_float);\n+  EXPECT_PRED1(static_cast<bool (*)(jfloat)>(std::isnan), floats[6].value.as_float);\n+  EXPECT_PRED1(static_cast<bool (*)(jfloat)>(std::isnan), floats[7].value.as_float);\n+  EXPECT_PRED1(static_cast<bool (*)(jfloat)>(std::isnan), floats[8].value.as_float);\n+}\n","filename":"test\/hotspot\/gtest\/utilities\/test_heapDumpParser.cpp","additions":619,"deletions":0,"binary":false,"changes":619,"status":"added"},{"patch":"@@ -24,0 +24,1 @@\n+import jdk.crac.*;\n@@ -34,2 +35,0 @@\n-import java.nio.file.Files;\n-import java.nio.file.Path;\n@@ -53,2 +52,4 @@\n-    @CracTestArg(value = 1, optional = true)\n-    String checkFile;\n+    \/\/ A file mounted on restore was used to identify a restore but now it is\n+    \/\/ not possible to check for file existance until restoration completes (the\n+    \/\/ check would use native buffers which are restorable resources themselves)\n+    Resource restoreListener;\n@@ -67,1 +68,1 @@\n-                .args(CracTest.args(TEST_HOSTNAME, \"\/second-run\"));\n+                .args(CracTest.args(TEST_HOSTNAME));\n@@ -87,3 +88,1 @@\n-                    \"--add-host\", TEST_HOSTNAME + \":192.168.56.78\",\n-                    \"--volume\", Utils.TEST_CLASSES + \":\/second-run\"); \/\/ any file\/dir suffices\n-\n+                    \"--add-host\", TEST_HOSTNAME + \":192.168.56.78\");\n@@ -101,2 +100,2 @@\n-        if (ip == null || checkFile == null) {\n-            System.err.println(\"Args: <ip address> <check file path>\");\n+        if (ip == null) {\n+            System.err.println(\"Args: <ip address>\");\n@@ -105,0 +104,12 @@\n+\n+        restoreListener = new Resource() {\n+            @Override\n+            public void beforeCheckpoint(Context<? extends Resource> context) {\n+            }\n+            @Override\n+            public void afterRestore(Context<? extends Resource> context) {\n+                restoreListener = null;\n+            }\n+        };\n+        Core.getGlobalContext().register(restoreListener);\n+\n@@ -106,1 +117,1 @@\n-        while (!Files.exists(Path.of(checkFile))) {\n+        while (restoreListener != null) { \/\/ While not restored\n","filename":"test\/jdk\/jdk\/crac\/java\/net\/InetAddress\/ResolveTest.java","additions":22,"deletions":11,"binary":false,"changes":33,"status":"modified"},{"patch":"@@ -0,0 +1,71 @@\n+import java.util.HashMap;\n+import jdk.crac.Core;\n+\n+\/**\n+ * Restoration of VM-provided identity hash codes.\n+ *\n+ * <p> Creates objects with non-overriden hash code generation method and checks\n+ * that the hash codes stay the same across the C\/R boundary.\n+ *\n+ * <p> How to run:\n+ * <pre> {@code\n+ * $ java -XX:CREngine= -XX:CRaCCheckpointTo=cr Hashcodes.java\n+ * > Right test hash: 1204088028\n+ * > Right key 1 value\n+ * > Right key 2 value\n+ *\n+ * $ java -XX:CREngine= -XX:CRaCRestoreFrom=cr\n+ * > Right test hash: 1204088028\n+ * > Right key 1 value\n+ * > Right key 2 value\n+ * } <\/pre>\n+ *\/\n+public class Hashcodes {\n+    private static class Key {}\n+\n+    public static void main(String[] args) throws Exception {\n+        final var hashtable = new HashMap<Key, String>();\n+\n+        final Key key1 = new Key();\n+        final String expectedVal1 = \"value 1\";\n+        hashtable.put(key1, expectedVal1);\n+\n+        final Key key2 = new Key();\n+        final String expectedVal2 = \"value 2\";\n+        hashtable.put(key2, expectedVal2);\n+\n+        final var testKey = new Key();\n+        final int expectedTestKeyHash = testKey.hashCode();\n+\n+        Core.checkpointRestore();\n+\n+        boolean success = true;\n+\n+        if (testKey.hashCode() == expectedTestKeyHash) {\n+            System.out.println(\"Right test hash: \" + expectedTestKeyHash);\n+        } else {\n+            System.out.println(\"Wrong test hash: expected \" + expectedTestKeyHash + \", got \" + testKey.hashCode());\n+            success = false;\n+        }\n+\n+        final String actualVal1 = hashtable.get(key1);\n+        if (actualVal1 == expectedVal1) {\n+            System.out.println(\"Right key 1 value\");\n+        } else {\n+            System.out.println(\"Wrong key 1 value: expected \" + expectedVal1 + \", got \" + actualVal1);\n+            success = false;\n+        }\n+\n+        final String actualVal2 = hashtable.get(key2);\n+        if (actualVal2 == expectedVal2) {\n+            System.out.println(\"Right key 2 value\");\n+        } else {\n+            System.out.println(\"Wrong key 2 value: expected \" + expectedVal2 + \", got \" + actualVal2);\n+            success = false;\n+        }\n+\n+        if (!success) {\n+            throw new IllegalStateException(\"Test failed\");\n+        }\n+    }\n+}\n","filename":"test\/jdk\/jdk\/crac\/portable\/Hashcodes.java","additions":71,"deletions":0,"binary":false,"changes":71,"status":"added"},{"patch":"@@ -0,0 +1,79 @@\n+import jdk.crac.Core;\n+\n+\/**\n+ * Restoration of monitor states.\n+ *\n+ * <p> Creates a monitor shared between two threads. The main thread enters the\n+ * monitor and initiates C\/R while the secondary thread is blocked. After the\n+ * restoration it remains blocked until the main thread exits the monitor.\n+ *\n+ * <p> How to run:\n+ * <pre> {@code\n+ * $ java -XX:CREngine= -XX:CRaCCheckpointTo=cr Monitors.java\n+ * > Thread[#1,main,5,main]: before synchronized\n+ * > Thread[#1,main,5,main]: inside synchronized\n+ * > Thread[#25,secondary,5,main]: before synchronized\n+ * > Thread[#1,main,5,main]: checkpointing\n+ * > # Checkpoint occurs\n+ * > # ... (the rest of the output omitted)\n+ *\n+ * $ java -XX:CREngine= -XX:CRaCRestoreFrom=cr\n+ * > Thread[#1,main,5,main]: restored\n+ * > Thread[#1,main,5,main]: deeply synchronized\n+ * > Thread[#1,main,5,main]: leaving synchronized\n+ * > Thread[#1,main,5,main]: after synchronized\n+ * > Thread[#25,secondary,5,main]: inside synchronized\n+ * > Thread[#25,secondary,5,main]: deeply synchronized\n+ * > Thread[#25,secondary,5,main]: leaving synchronized\n+ * > Thread[#25,secondary,5,main]: after synchronized\n+ * } <\/pre>\n+ *\/\n+public class Monitors {\n+    private static final Object sharedMonitor = new Object();\n+\n+    private static void primaryThreadEntry() throws Exception {\n+        final Thread secondary = new Thread(Monitors::secondaryThreadEntry, \"secondary\");\n+\n+        final String threadStr = Thread.currentThread().toString();\n+        System.out.println(threadStr + \": before synchronized\");\n+        synchronized(sharedMonitor) {\n+            System.out.println(threadStr + \": inside synchronized\");\n+            secondary.start();\n+            synchronized(sharedMonitor) {\n+                System.out.println(threadStr + \": checkpointing\");\n+                Core.checkpointRestore();\n+                System.out.println(threadStr + \": restored\");\n+                Thread.sleep(1000); \/\/ Let the secondary thread wake up\n+                synchronized(sharedMonitor) {\n+                    System.out.println(threadStr + \": deeply synchronized\");\n+                }\n+            }\n+            System.out.println(threadStr + \": leaving synchronized\");\n+        }\n+        System.out.println(threadStr + \": after synchronized\");\n+\n+        \/\/ TODO currently when the restored main thread dies the whole VM dies\n+        secondary.join();\n+    }\n+\n+    private static void secondaryThreadEntry() {\n+        final String threadStr = Thread.currentThread().toString();\n+        System.out.println(threadStr + \": before synchronized\");\n+        synchronized(sharedMonitor) {\n+            System.out.println(threadStr + \": inside synchronized\");\n+            synchronized(sharedMonitor) {\n+                synchronized(sharedMonitor) {\n+                    synchronized(sharedMonitor) {\n+                        System.out.println(threadStr + \": deeply synchronized\");\n+                    }\n+                }\n+            }\n+            System.out.println(threadStr + \": leaving synchronized\");\n+        }\n+        System.out.println(threadStr + \": after synchronized\");\n+    }\n+\n+    public static void main(String[] args) throws Exception {\n+        primaryThreadEntry();\n+    }\n+}\n","filename":"test\/jdk\/jdk\/crac\/portable\/Monitors.java","additions":79,"deletions":0,"binary":false,"changes":79,"status":"added"},{"patch":"@@ -0,0 +1,4 @@\n+# Tests for portable CRaC\n+\n+Currently the tests should be run manually. Each test file contains the\n+necessary instructions.\n","filename":"test\/jdk\/jdk\/crac\/portable\/README.md","additions":4,"deletions":0,"binary":false,"changes":4,"status":"added"},{"patch":"@@ -0,0 +1,50 @@\n+import jdk.crac.Core;\n+\n+\/**\n+ * Restoration of main thread's execution stack.\n+ *\n+ * <p> Invokes a recursive function to create a deep call stack and performs a\n+ * checkpoint there.\n+ *\n+ * <p> How to run:\n+ * <pre> {@code\n+ * $ java -XX:CREngine= -XX:CRaCCheckpointTo=cr RecursionCounter.java\n+ * > -\n+ * > --\n+ * > ---\n+ * > # Checkpoint occurs\n+ * > ----\n+ * > ---\n+ * > --\n+ * > -\n+ *\n+ * $ java -XX:CREngine= -XX:CRaCRestoreFrom=cr\n+ * > ----\n+ * > ---\n+ * > --\n+ * > -\n+ * }\n+ * <\/pre>\n+ *\/\n+public class RecursionCounter {\n+    private static void recurse(int depth, int maxDepth) throws Exception {\n+        final var depthStr = \"-\".repeat(depth);\n+\n+        Thread.sleep(200);\n+        System.out.println(depthStr);\n+\n+        if (depth < maxDepth) {\n+            recurse(depth + 1, maxDepth);\n+        } else {\n+            Core.checkpointRestore();\n+            System.out.println(depthStr + \"-\");\n+        }\n+\n+        Thread.sleep(200);\n+        System.out.println(depthStr);\n+    }\n+\n+    public static void main(String[] args) throws Exception {\n+        recurse(1, 10);\n+    }\n+}\n","filename":"test\/jdk\/jdk\/crac\/portable\/RecursionCounter.java","additions":50,"deletions":0,"binary":false,"changes":50,"status":"added"},{"patch":"@@ -0,0 +1,56 @@\n+import java.text.SimpleDateFormat;\n+import java.util.Date;\n+\n+import jdk.crac.Context;\n+import jdk.crac.Core;\n+import jdk.crac.Resource;\n+\n+\/**\n+ * Restoration of CRaC resources.\n+ *\n+ * <p> Registers a resource that records its creation time and performs\n+ * checkpoint-restore to demonstrate that the callbacks of the resource are\n+ * executed as expected and the state of the resource is restored.\n+ *\n+ * <p> How to run:\n+ * <pre> {@code\n+ * $ java -XX:CREngine= -XX:CRaCCheckpointTo=cr ResourceCallbacks.java\n+ * > Resource creation time: 13:16:06:486 05.04.2024\n+ * > Time before checkpoint: 13:16:06:670 05.04.2024\n+ * > # Checkpoint occurs\n+ * > Resource creation time: 13:16:06:486 05.04.2024\n+ * > Time after restore:     13:16:07:583 05.04.2024\n+ *\n+ * $ java -XX:CREngine= -XX:CRaCRestoreFrom=cr\n+ * > Resource creation time: 13:16:06:486 05.04.2024\n+ * > Time after restore:     13:16:40:875 05.04.2024\n+ * }\n+ * <\/pre>\n+ *\/\n+public class ResourceCallbacks {\n+    public static void main(String[] args) throws Exception {\n+        final var resource = new MyResource();\n+        Core.getGlobalContext().register(resource);\n+        Core.checkpointRestore();\n+    }\n+}\n+\n+class MyResource implements Resource {\n+    final long creationTime = System.currentTimeMillis();\n+\n+    private static String timeToString(long millis) {\n+        return (new SimpleDateFormat(\"HH:mm:ss:SSS dd.MM.yyyy\")).format(new Date(millis));\n+    }\n+\n+    @Override\n+    public void beforeCheckpoint(Context<?> context) {\n+        System.out.println(\"Resource creation time: \" + timeToString(creationTime));\n+        System.out.println(\"Time before checkpoint: \" + timeToString(System.currentTimeMillis()));\n+    }\n+\n+    @Override\n+    public void afterRestore(Context<?> context) {\n+        System.out.println(\"Resource creation time: \" + timeToString(creationTime));\n+        System.out.println(\"Time after restore:     \" + timeToString(System.currentTimeMillis()));\n+    }\n+}\n","filename":"test\/jdk\/jdk\/crac\/portable\/ResourceCallbacks.java","additions":56,"deletions":0,"binary":false,"changes":56,"status":"added"},{"patch":"@@ -0,0 +1,96 @@\n+import jdk.crac.Core;\n+\n+import java.util.Random;\n+\n+\/**\n+ * Restoration of multiple threads.\n+ *\n+ * <p> The main threads starts concurrent threads, performs checkpoint-restore\n+ * and then interacts with the threads to ensure they are still properly\n+ * functioning.\n+ *\n+ * <p> How to run:\n+ * <pre> {@code\n+ * $ java -XX:CREngine= -XX:CRaCCheckpointTo=cr Threads.java\n+ * > My thread #2: in initial state\n+ * > ...\n+ * > My thread #9: in initial state\n+ * > Checkpointing\n+ * > ...\n+ *\n+ * $ java -XX:CREngine= -XX:CRaCRestoreFrom=cr\n+ * > Restored\n+ * > My thread #4: in initial state\n+ * > ...\n+ * > My thread #3: in initial state\n+ * > Changing states\n+ * > Changed state of My thread #0\n+ * > ...\n+ * > Changed state of My thread #9\n+ * > My thread #2: in new state (2)\n+ * > ...\n+ * > My thread #3: in new state (3)\n+ * }\n+ * <\/pre>\n+ *\/\n+public class Threads {\n+    static final Random random = new Random();\n+\n+    \/\/ TODO replace with Thread.sleep() when C\/R inside it is supported\n+    static String work(double period) {\n+        double x = random.nextDouble();\n+        for (double y = -period \/ 2; y < period \/ 2; y++) {\n+            x += (y \/ 2) % x;\n+        }\n+        return x % 2 == 0 ? \"\" : \" \"; \/\/ Blackhole\n+    }\n+\n+    public static void main(String[] args) throws Exception {\n+        String blackhole;\n+\n+        \/\/ Start some threads\n+        final var threads = new MyThread[10];\n+        for (int i = 0; i < threads.length; i++) {\n+            final var t = new MyThread(i);\n+            t.start();\n+            threads[i] = t;\n+        }\n+\n+        \/\/ Waste some time to let the threads print something concurrently\n+        blackhole = work(70_000_000);\n+\n+        System.out.println(\"Checkpointing\" + blackhole);\n+        Core.checkpointRestore();\n+        System.out.println(\"Restored\");\n+\n+        \/\/ Waste some time to let the threads print something concurrently again\n+        blackhole = work(70_000_000);\n+        System.out.println(\"Changing states\" + blackhole);\n+\n+        \/\/ Change threads' state to check the objects we have are the actual\n+        \/\/ handles to the restored threads\n+        for (int i = 0; i < threads.length; i++) {\n+            final MyThread t = threads[i];\n+            t.strToPrint = \"in new state (\" + i + \")\";\n+            System.out.println(\"Changed state of \" + t.getName());\n+        }\n+        work(70_000_000);\n+    }\n+}\n+\n+class MyThread extends Thread {\n+    String strToPrint = \"in initial state\";\n+\n+    MyThread(int num) {\n+        super(\"My thread #\" + num);\n+        setDaemon(true);\n+    }\n+\n+    @Override\n+    public void run() {\n+      while (true) {\n+        Threads.work(50_000_000);\n+        System.out.println(getName() + \": \" + strToPrint);\n+      }\n+    }\n+}\n","filename":"test\/jdk\/jdk\/crac\/portable\/Threads.java","additions":96,"deletions":0,"binary":false,"changes":96,"status":"added"},{"patch":"@@ -0,0 +1,57 @@\n+#!\/bin\/sh\n+\n+# Hand-crafted launcher for portable CRaC tests. To be removed as soon as the\n+# tests are integrated with JTReg.\n+#\n+# Parameters:\n+# - BUILD_DIR -- path to the build directory where a JDK image has been built\n+\n+BUILD_DIR=$1\n+if [ ! -d \"$BUILD_DIR\" ]; then\n+  echo \"Usage: $0 BUILD_DIR\"\n+  exit 1\n+fi\n+\n+JAVA_HOME=\"$BUILD_DIR\/images\/jdk\"\n+JAVA_HOME_EXP=\"$BUILD_DIR\/jdk\"\n+if [ ! -x \"$JAVA_HOME\/bin\/java\" ] | [ ! -x \"$JAVA_HOME\/bin\/javac\" ]; then\n+  echo \"Cannot find JDK in $JAVA_HOME\"\n+  exit 1\n+fi\n+if [ ! -x \"$JAVA_HOME_EXP\" ]; then\n+  echo \"Cannot find exploded JDK in $JAVA_HOME_EXP\"\n+  exit 1\n+fi\n+\n+for JAVA_FILE in *.java; do\n+  PROG=${JAVA_FILE%.java}\n+  printf \"\\n\\n##### %s\\n\" \"$PROG\"\n+\n+  \"$JAVA_HOME\/bin\/javac\" \"$JAVA_FILE\"\n+\n+  printf \"\\n%s: normal + compiled\\n\" \"$PROG\"\n+  rm -rf cr\n+  \"$JAVA_HOME\/bin\/java\" -XX:CREngine= -XX:CRaCCheckpointTo=cr \"$PROG\"\n+  printf \"\\n\"\n+  \"$JAVA_HOME\/bin\/java\" -XX:CREngine= -XX:CRaCRestoreFrom=cr\n+\n+  printf \"\\n%s: normal + launched\\n\" \"$PROG\"\n+  rm -rf cr\n+  \"$JAVA_HOME\/bin\/java\" -XX:CREngine= -XX:CRaCCheckpointTo=cr \"$JAVA_FILE\"\n+  printf \"\\n\"\n+  \"$JAVA_HOME\/bin\/java\" -XX:CREngine= -XX:CRaCRestoreFrom=cr\n+\n+  printf \"\\n%s: exploded + compiled\\n\" \"$PROG\"\n+  rm -rf cr\n+  \"$JAVA_HOME_EXP\/bin\/java\" -XX:CREngine= -XX:CRaCCheckpointTo=cr \"$PROG\"\n+  printf \"\\n\"\n+  \"$JAVA_HOME_EXP\/bin\/java\" -XX:CREngine= -XX:CRaCRestoreFrom=cr\n+\n+  printf \"\\n%s: exploded + launched\\n\" \"$PROG\"\n+  rm -rf cr\n+  \"$JAVA_HOME_EXP\/bin\/java\" -XX:CREngine= -XX:CRaCCheckpointTo=cr \"$JAVA_FILE\"\n+  printf \"\\n\"\n+  \"$JAVA_HOME_EXP\/bin\/java\" -XX:CREngine= -XX:CRaCRestoreFrom=cr\n+done\n+\n+rm -rf cr\n","filename":"test\/jdk\/jdk\/crac\/portable\/run.sh","additions":57,"deletions":0,"binary":false,"changes":57,"status":"added"},{"patch":"@@ -230,1 +230,1 @@\n-                        new StackTraceElement(\"jdk.proxy1.$Proxy0\", \"hashCode\", null, -1),\n+                        new StackTraceElement(\"jdk.proxy2.$Proxy1\", \"hashCode\", null, -1),\n","filename":"test\/langtools\/jdk\/jshell\/ExceptionsTest.java","additions":1,"deletions":1,"binary":false,"changes":2,"status":"modified"}]}