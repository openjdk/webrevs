{"files":[{"patch":"@@ -83,0 +83,2 @@\n+    -taglet build.tools.taglet.PreviewNote \\\n+    --preview-note-tag previewNote \\\n","filename":"make\/Docs.gmk","additions":2,"deletions":0,"binary":false,"changes":2,"status":"modified"},{"patch":"@@ -36,2 +36,2 @@\n-ALPINE_LINUX_X64_BOOT_JDK_URL=https:\/\/github.com\/adoptium\/temurin24-binaries\/releases\/download\/jdk-24%2B36\/OpenJDK24U-jdk_aarch64_alpine-linux_hotspot_24_36.tar.gz\n-ALPINE_LINUX_X64_BOOT_JDK_SHA256=4a673456aa6e726b86108a095a21868b7ebcdde050a92b3073d50105ff92f07f\n+ALPINE_LINUX_X64_BOOT_JDK_URL=https:\/\/github.com\/adoptium\/temurin24-binaries\/releases\/download\/jdk-24%2B36\/OpenJDK24U-jdk_x64_alpine-linux_hotspot_24_36.tar.gz\n+ALPINE_LINUX_X64_BOOT_JDK_SHA256=a642608f0da78344ee6812fb1490b8bc1d7ad5a18064c70994d6f330568c51cb\n","filename":"make\/conf\/github-actions.conf","additions":2,"deletions":2,"binary":false,"changes":4,"status":"modified"},{"patch":"@@ -1714,1 +1714,1 @@\n-  if (supports_fma() && UseSSE >= 2) { \/\/ Check UseSSE since FMA code uses SSE instructions\n+  if (supports_fma()) {\n@@ -1783,9 +1783,0 @@\n-#ifdef COMPILER2\n-  if (UseFPUForSpilling) {\n-    if (UseSSE < 2) {\n-      \/\/ Only supported with SSE2+\n-      FLAG_SET_DEFAULT(UseFPUForSpilling, false);\n-    }\n-  }\n-#endif\n-\n@@ -1794,5 +1785,1 @@\n-  if (UseSSE < 2) {\n-    \/\/ Vectors (in XMM) are only supported with SSE2+\n-    \/\/ SSE is always 2 on x64.\n-    max_vector_size = 0;\n-  } else if (UseAVX == 0 || !os_supports_avx_vectors()) {\n+  if (UseAVX == 0 || !os_supports_avx_vectors()) {\n@@ -2313,1 +2300,1 @@\n-  if (!UseFastStosb && UseSSE >= 2 && UseUnalignedLoadStores) {\n+  if (!UseFastStosb && UseUnalignedLoadStores) {\n@@ -2428,1 +2415,1 @@\n-    if (AllocatePrefetchStyle <= 0 || (UseSSE == 0 && !supports_3dnow_prefetch())) {\n+    if (AllocatePrefetchStyle <= 0) {\n@@ -2432,1 +2419,7 @@\n-      if (UseSSE == 0 && supports_3dnow_prefetch()) {\n+      if (AllocatePrefetchInstr == 0) {\n+        log->print(\"PREFETCHNTA\");\n+      } else if (AllocatePrefetchInstr == 1) {\n+        log->print(\"PREFETCHT0\");\n+      } else if (AllocatePrefetchInstr == 2) {\n+        log->print(\"PREFETCHT2\");\n+      } else if (AllocatePrefetchInstr == 3) {\n@@ -2434,10 +2427,0 @@\n-      } else if (UseSSE >= 1) {\n-        if (AllocatePrefetchInstr == 0) {\n-          log->print(\"PREFETCHNTA\");\n-        } else if (AllocatePrefetchInstr == 1) {\n-          log->print(\"PREFETCHT0\");\n-        } else if (AllocatePrefetchInstr == 2) {\n-          log->print(\"PREFETCHT2\");\n-        } else if (AllocatePrefetchInstr == 3) {\n-          log->print(\"PREFETCHW\");\n-        }\n@@ -3635,0 +3618,4 @@\n+  if (ext_cpuid1_ecx.bits.lzcnt != 0)\n+    result |= CPU_LZCNT;\n+  if (ext_cpuid1_ecx.bits.prefetchw != 0)\n+    result |= CPU_3DNOW_PREFETCH;\n@@ -3653,0 +3640,2 @@\n+  if (sef_cpuid7_ebx.bits.clwb != 0)\n+    result |= CPU_CLWB;\n@@ -3658,1 +3647,1 @@\n-  \/\/ AMD|Hygon features.\n+  \/\/ AMD|Hygon additional features.\n@@ -3660,2 +3649,2 @@\n-    if ((ext_cpuid1_edx.bits.tdnow != 0) ||\n-        (ext_cpuid1_ecx.bits.prefetchw != 0))\n+    \/\/ PREFETCHW was checked above, check TDNOW here.\n+    if ((ext_cpuid1_edx.bits.tdnow != 0))\n@@ -3663,2 +3652,0 @@\n-    if (ext_cpuid1_ecx.bits.lzcnt != 0)\n-      result |= CPU_LZCNT;\n@@ -3669,1 +3656,1 @@\n-  \/\/ Intel features.\n+  \/\/ Intel additional features.\n@@ -3671,9 +3658,0 @@\n-    if (ext_cpuid1_ecx.bits.lzcnt != 0) {\n-      result |= CPU_LZCNT;\n-    }\n-    if (ext_cpuid1_ecx.bits.prefetchw != 0) {\n-      result |= CPU_3DNOW_PREFETCH;\n-    }\n-    if (sef_cpuid7_ebx.bits.clwb != 0) {\n-      result |= CPU_CLWB;\n-    }\n@@ -3682,1 +3660,0 @@\n-\n@@ -3687,1 +3664,1 @@\n-  \/\/ ZX features.\n+  \/\/ ZX additional features.\n@@ -3689,6 +3666,4 @@\n-    if (ext_cpuid1_ecx.bits.lzcnt != 0) {\n-      result |= CPU_LZCNT;\n-    }\n-    if (ext_cpuid1_ecx.bits.prefetchw != 0) {\n-      result |= CPU_3DNOW_PREFETCH;\n-    }\n+    \/\/ We do not know if these are supported by ZX, so we cannot trust\n+    \/\/ common CPUID bit for them.\n+    assert((result & CPU_CLWB) == 0, \"Check if it is supported?\");\n+    result &= ~CPU_CLWB;\n","filename":"src\/hotspot\/cpu\/x86\/vm_version_x86.cpp","additions":26,"deletions":51,"binary":false,"changes":77,"status":"modified"},{"patch":"@@ -1312,20 +1312,12 @@\n-  if (Arguments::sun_java_launcher_is_altjvm()) {\n-    \/\/ Support for the java launcher's '-XXaltjvm=<path>' option. Typical\n-    \/\/ value for buf is \"<JAVA_HOME>\/jre\/lib\/<vmtype>\/libjvm.so\".\n-    \/\/ If \"\/jre\/lib\/\" appears at the right place in the string, then\n-    \/\/ assume we are installed in a JDK and we're done. Otherwise, check\n-    \/\/ for a JAVA_HOME environment variable and fix up the path so it\n-    \/\/ looks like libjvm.so is installed there (append a fake suffix\n-    \/\/ hotspot\/libjvm.so).\n-    const char *p = buf + strlen(buf) - 1;\n-    for (int count = 0; p > buf && count < 4; ++count) {\n-      for (--p; p > buf && *p != '\/'; --p)\n-        \/* empty *\/ ;\n-    }\n-\n-    if (strncmp(p, \"\/jre\/lib\/\", 9) != 0) {\n-      \/\/ Look for JAVA_HOME in the environment.\n-      char* java_home_var = ::getenv(\"JAVA_HOME\");\n-      if (java_home_var != nullptr && java_home_var[0] != 0) {\n-        char* jrelib_p;\n-        int len;\n+  \/\/ If executing unit tests we require JAVA_HOME to point to the real JDK.\n+  if (Arguments::executing_unit_tests()) {\n+    \/\/ Look for JAVA_HOME in the environment.\n+    char* java_home_var = ::getenv(\"JAVA_HOME\");\n+    if (java_home_var != nullptr && java_home_var[0] != 0) {\n+\n+      \/\/ Check the current module name \"libjvm.so\".\n+      const char* p = strrchr(buf, '\/');\n+      if (p == nullptr) {\n+        return;\n+      }\n+      assert(strstr(p, \"\/libjvm\") == p, \"invalid library name\");\n@@ -1333,6 +1325,5 @@\n-        \/\/ Check the current module name \"libjvm.so\".\n-        p = strrchr(buf, '\/');\n-        if (p == nullptr) {\n-          return;\n-        }\n-        assert(strstr(p, \"\/libjvm\") == p, \"invalid library name\");\n+      stringStream ss(buf, buflen);\n+      rp = os::realpath(java_home_var, buf, buflen);\n+      if (rp == nullptr) {\n+        return;\n+      }\n@@ -1340,1 +1331,11 @@\n-        rp = os::realpath(java_home_var, buf, buflen);\n+      assert((int)strlen(buf) < buflen, \"Ran out of buffer room\");\n+      ss.print(\"%s\/lib\", buf);\n+\n+      if (0 == access(buf, F_OK)) {\n+        \/\/ Use current module name \"libjvm.so\"\n+        ss.print(\"\/%s\/libjvm%s\", Abstract_VM_Version::vm_variant(), JNI_LIB_SUFFIX);\n+        assert(strcmp(buf + strlen(buf) - strlen(JNI_LIB_SUFFIX), JNI_LIB_SUFFIX) == 0,\n+               \"buf has been truncated\");\n+      } else {\n+        \/\/ Go back to path of .so\n+        rp = os::realpath((char *)dlinfo.dli_fname, buf, buflen);\n@@ -1344,22 +1345,0 @@\n-\n-        \/\/ determine if this is a legacy image or modules image\n-        \/\/ modules image doesn't have \"jre\" subdirectory\n-        len = strlen(buf);\n-        assert(len < buflen, \"Ran out of buffer room\");\n-        jrelib_p = buf + len;\n-        snprintf(jrelib_p, buflen-len, \"\/jre\/lib\");\n-        if (0 != access(buf, F_OK)) {\n-          snprintf(jrelib_p, buflen-len, \"\/lib\");\n-        }\n-\n-        if (0 == access(buf, F_OK)) {\n-          \/\/ Use current module name \"libjvm.so\"\n-          len = strlen(buf);\n-          snprintf(buf + len, buflen-len, \"\/hotspot\/libjvm.so\");\n-        } else {\n-          \/\/ Go back to path of .so\n-          rp = os::realpath((char *)dlinfo.dli_fname, buf, buflen);\n-          if (rp == nullptr) {\n-            return;\n-          }\n-        }\n","filename":"src\/hotspot\/os\/aix\/os_aix.cpp","additions":28,"deletions":49,"binary":false,"changes":77,"status":"modified"},{"patch":"@@ -1508,14 +1508,15 @@\n-  if (Arguments::sun_java_launcher_is_altjvm()) {\n-    \/\/ Support for the java launcher's '-XXaltjvm=<path>' option. Typical\n-    \/\/ value for buf is \"<JAVA_HOME>\/jre\/lib\/<arch>\/<vmtype>\/libjvm.so\"\n-    \/\/ or \"<JAVA_HOME>\/jre\/lib\/<vmtype>\/libjvm.dylib\". If \"\/jre\/lib\/\"\n-    \/\/ appears at the right place in the string, then assume we are\n-    \/\/ installed in a JDK and we're done. Otherwise, check for a\n-    \/\/ JAVA_HOME environment variable and construct a path to the JVM\n-    \/\/ being overridden.\n-\n-    const char *p = buf + strlen(buf) - 1;\n-    for (int count = 0; p > buf && count < 5; ++count) {\n-      for (--p; p > buf && *p != '\/'; --p)\n-        \/* empty *\/ ;\n-    }\n+  \/\/ If executing unit tests we require JAVA_HOME to point to the real JDK.\n+  if (Arguments::executing_unit_tests()) {\n+    \/\/ Look for JAVA_HOME in the environment.\n+    char* java_home_var = ::getenv(\"JAVA_HOME\");\n+    if (java_home_var != nullptr && java_home_var[0] != 0) {\n+\n+      \/\/ Check the current module name \"libjvm\"\n+      const char* p = strrchr(buf, '\/');\n+      assert(strstr(p, \"\/libjvm\") == p, \"invalid library name\");\n+\n+      stringStream ss(buf, buflen);\n+      rp = os::realpath(java_home_var, buf, buflen);\n+      if (rp == nullptr) {\n+        return;\n+      }\n@@ -1523,6 +1524,3 @@\n-    if (strncmp(p, \"\/jre\/lib\/\", 9) != 0) {\n-      \/\/ Look for JAVA_HOME in the environment.\n-      char* java_home_var = ::getenv(\"JAVA_HOME\");\n-      if (java_home_var != nullptr && java_home_var[0] != 0) {\n-        char* jrelib_p;\n-        int len;\n+      assert((int)strlen(buf) < buflen, \"Ran out of buffer space\");\n+      \/\/ Add the appropriate library and JVM variant subdirs\n+      ss.print(\"%s\/lib\/%s\", buf, Abstract_VM_Version::vm_variant());\n@@ -1530,3 +1528,4 @@\n-        \/\/ Check the current module name \"libjvm\"\n-        p = strrchr(buf, '\/');\n-        assert(strstr(p, \"\/libjvm\") == p, \"invalid library name\");\n+      if (0 != access(buf, F_OK)) {\n+        ss.reset();\n+        ss.print(\"%s\/lib\", buf);\n+      }\n@@ -1534,1 +1533,11 @@\n-        rp = os::realpath(java_home_var, buf, buflen);\n+      \/\/ If the path exists within JAVA_HOME, add the JVM library name\n+      \/\/ to complete the path to JVM being overridden.  Otherwise fallback\n+      \/\/ to the path to the current library.\n+      if (0 == access(buf, F_OK)) {\n+        \/\/ Use current module name \"libjvm\"\n+        ss.print(\"\/libjvm%s\", JNI_LIB_SUFFIX);\n+        assert(strcmp(buf + strlen(buf) - strlen(JNI_LIB_SUFFIX), JNI_LIB_SUFFIX) == 0,\n+               \"buf has been truncated\");\n+      } else {\n+        \/\/ Fall back to path of current library\n+        rp = os::realpath(dli_fname, buf, buflen);\n@@ -1538,35 +1547,0 @@\n-\n-        \/\/ determine if this is a legacy image or modules image\n-        \/\/ modules image doesn't have \"jre\" subdirectory\n-        len = strlen(buf);\n-        assert(len < buflen, \"Ran out of buffer space\");\n-        jrelib_p = buf + len;\n-\n-        \/\/ Add the appropriate library subdir\n-        snprintf(jrelib_p, buflen-len, \"\/jre\/lib\");\n-        if (0 != access(buf, F_OK)) {\n-          snprintf(jrelib_p, buflen-len, \"\/lib\");\n-        }\n-\n-        \/\/ Add the appropriate JVM variant subdir\n-        len = strlen(buf);\n-        jrelib_p = buf + len;\n-        snprintf(jrelib_p, buflen-len, \"\/%s\", Abstract_VM_Version::vm_variant());\n-        if (0 != access(buf, F_OK)) {\n-          snprintf(jrelib_p, buflen-len, \"%s\", \"\");\n-        }\n-\n-        \/\/ If the path exists within JAVA_HOME, add the JVM library name\n-        \/\/ to complete the path to JVM being overridden.  Otherwise fallback\n-        \/\/ to the path to the current library.\n-        if (0 == access(buf, F_OK)) {\n-          \/\/ Use current module name \"libjvm\"\n-          len = strlen(buf);\n-          snprintf(buf + len, buflen-len, \"\/libjvm%s\", JNI_LIB_SUFFIX);\n-        } else {\n-          \/\/ Fall back to path of current library\n-          rp = os::realpath(dli_fname, buf, buflen);\n-          if (rp == nullptr) {\n-            return;\n-          }\n-        }\n","filename":"src\/hotspot\/os\/bsd\/os_bsd.cpp","additions":33,"deletions":59,"binary":false,"changes":92,"status":"modified"},{"patch":"@@ -2793,13 +2793,12 @@\n-  if (Arguments::sun_java_launcher_is_altjvm()) {\n-    \/\/ Support for the java launcher's '-XXaltjvm=<path>' option. Typical\n-    \/\/ value for buf is \"<JAVA_HOME>\/jre\/lib\/<vmtype>\/libjvm.so\".\n-    \/\/ If \"\/jre\/lib\/\" appears at the right place in the string, then\n-    \/\/ assume we are installed in a JDK and we're done. Otherwise, check\n-    \/\/ for a JAVA_HOME environment variable and fix up the path so it\n-    \/\/ looks like libjvm.so is installed there (append a fake suffix\n-    \/\/ hotspot\/libjvm.so).\n-    const char *p = buf + strlen(buf) - 1;\n-    for (int count = 0; p > buf && count < 5; ++count) {\n-      for (--p; p > buf && *p != '\/'; --p)\n-        \/* empty *\/ ;\n-    }\n+  \/\/ If executing unit tests we require JAVA_HOME to point to the real JDK.\n+  if (Arguments::executing_unit_tests()) {\n+    \/\/ Look for JAVA_HOME in the environment.\n+    char* java_home_var = ::getenv(\"JAVA_HOME\");\n+    if (java_home_var != nullptr && java_home_var[0] != 0) {\n+\n+      \/\/ Check the current module name \"libjvm.so\".\n+      const char* p = strrchr(buf, '\/');\n+      if (p == nullptr) {\n+        return;\n+      }\n+      assert(strstr(p, \"\/libjvm\") == p, \"invalid library name\");\n@@ -2807,6 +2806,5 @@\n-    if (strncmp(p, \"\/jre\/lib\/\", 9) != 0) {\n-      \/\/ Look for JAVA_HOME in the environment.\n-      char* java_home_var = ::getenv(\"JAVA_HOME\");\n-      if (java_home_var != nullptr && java_home_var[0] != 0) {\n-        char* jrelib_p;\n-        int len;\n+      stringStream ss(buf, buflen);\n+      rp = os::realpath(java_home_var, buf, buflen);\n+      if (rp == nullptr) {\n+        return;\n+      }\n@@ -2814,6 +2812,2 @@\n-        \/\/ Check the current module name \"libjvm.so\".\n-        p = strrchr(buf, '\/');\n-        if (p == nullptr) {\n-          return;\n-        }\n-        assert(strstr(p, \"\/libjvm\") == p, \"invalid library name\");\n+      assert((int)strlen(buf) < buflen, \"Ran out of buffer room\");\n+      ss.print(\"%s\/lib\", buf);\n@@ -2821,1 +2815,8 @@\n-        rp = os::realpath(java_home_var, buf, buflen);\n+      if (0 == access(buf, F_OK)) {\n+        \/\/ Use current module name \"libjvm.so\"\n+        ss.print(\"\/%s\/libjvm%s\", Abstract_VM_Version::vm_variant(), JNI_LIB_SUFFIX);\n+        assert(strcmp(buf + strlen(buf) - strlen(JNI_LIB_SUFFIX), JNI_LIB_SUFFIX) == 0,\n+               \"buf has been truncated\");\n+      } else {\n+        \/\/ Go back to path of .so\n+        rp = os::realpath(dli_fname, buf, buflen);\n@@ -2825,22 +2826,0 @@\n-\n-        \/\/ determine if this is a legacy image or modules image\n-        \/\/ modules image doesn't have \"jre\" subdirectory\n-        len = checked_cast<int>(strlen(buf));\n-        assert(len < buflen, \"Ran out of buffer room\");\n-        jrelib_p = buf + len;\n-        snprintf(jrelib_p, buflen-len, \"\/jre\/lib\");\n-        if (0 != access(buf, F_OK)) {\n-          snprintf(jrelib_p, buflen-len, \"\/lib\");\n-        }\n-\n-        if (0 == access(buf, F_OK)) {\n-          \/\/ Use current module name \"libjvm.so\"\n-          len = (int)strlen(buf);\n-          snprintf(buf + len, buflen-len, \"\/hotspot\/libjvm.so\");\n-        } else {\n-          \/\/ Go back to path of .so\n-          rp = os::realpath(dli_fname, buf, buflen);\n-          if (rp == nullptr) {\n-            return;\n-          }\n-        }\n","filename":"src\/hotspot\/os\/linux\/os_linux.cpp","additions":27,"deletions":48,"binary":false,"changes":75,"status":"modified"},{"patch":"@@ -2252,5 +2252,2 @@\n-  if (Arguments::sun_java_launcher_is_altjvm()) {\n-    \/\/ Support for the java launcher's '-XXaltjvm=<path>' option. Check\n-    \/\/ for a JAVA_HOME environment variable and fix up the path so it\n-    \/\/ looks like jvm.dll is installed there (append a fake suffix\n-    \/\/ hotspot\/jvm.dll).\n+  \/\/ If executing unit tests we require JAVA_HOME to point to the real JDK.\n+  if (Arguments::executing_unit_tests()) {\n@@ -2260,12 +2257,5 @@\n-      strncpy(buf, java_home_var, buflen);\n-\n-      \/\/ determine if this is a legacy image or modules image\n-      \/\/ modules image doesn't have \"jre\" subdirectory\n-      size_t len = strlen(buf);\n-      char* jrebin_p = buf + len;\n-      jio_snprintf(jrebin_p, buflen-len, \"\\\\jre\\\\bin\\\\\");\n-      if (0 != _access(buf, 0)) {\n-        jio_snprintf(jrebin_p, buflen-len, \"\\\\bin\\\\\");\n-      }\n-      len = strlen(buf);\n-      jio_snprintf(buf + len, buflen-len, \"hotspot\\\\jvm.dll\");\n+      stringStream ss(buf, buflen);\n+      ss.print(\"%s\\\\bin\\\\%s\\\\jvm%s\",\n+               java_home_var, Abstract_VM_Version::vm_variant(), JNI_LIB_SUFFIX);\n+      assert(strcmp(buf + strlen(buf) - strlen(JNI_LIB_SUFFIX), JNI_LIB_SUFFIX) == 0,\n+             \"buf has been truncated\");\n","filename":"src\/hotspot\/os\/windows\/os_windows.cpp","additions":7,"deletions":17,"binary":false,"changes":24,"status":"modified"},{"patch":"@@ -308,0 +308,14 @@\n+bool ClassPathZipEntry::has_entry(JavaThread* current, const char* name) {\n+  ThreadToNativeFromVM ttn(current);\n+  \/\/ check whether zip archive contains name\n+  jint name_len;\n+  jint filesize;\n+  jzentry* entry = ZipLibrary::find_entry(_zip, name, &filesize, &name_len);\n+  if (entry == nullptr) {\n+    return false;\n+  } else {\n+     ZipLibrary::free_entry(_zip, entry);\n+    return true;\n+  }\n+}\n+\n","filename":"src\/hotspot\/share\/classfile\/classLoader.cpp","additions":14,"deletions":0,"binary":false,"changes":14,"status":"modified"},{"patch":"@@ -96,0 +96,1 @@\n+  bool has_entry(JavaThread* current, const char* name);\n","filename":"src\/hotspot\/share\/classfile\/classLoader.hpp","additions":1,"deletions":0,"binary":false,"changes":1,"status":"modified"},{"patch":"@@ -163,1 +163,1 @@\n-    const size_t freed = ZHeap::heap()->free_empty_pages(selector->empty_pages());\n+    const size_t freed = ZHeap::heap()->free_empty_pages(_id, selector->empty_pages());\n@@ -193,11 +193,0 @@\n-        \/\/ Note that the seqnum can change under our feet here as the page\n-        \/\/ can be concurrently freed and recycled by a concurrent generation\n-        \/\/ collection. However this property is stable across such transitions.\n-        \/\/ If it was not relocatable before recycling, then it won't be\n-        \/\/ relocatable after it gets recycled either, as the seqnum atomically\n-        \/\/ becomes allocating for the given generation. The opposite property\n-        \/\/ also holds: if the page is relocatable, then it can't have been\n-        \/\/ concurrently freed; if it was re-allocated it would not be\n-        \/\/ relocatable, and if it was not re-allocated we know that it was\n-        \/\/ allocated earlier than mark start of the current generation\n-        \/\/ collection.\n@@ -216,9 +205,8 @@\n-        \/\/ An active iterator blocks immediate recycle and delete of pages.\n-        \/\/ The intent it to allow the code that iterates over the pages to\n-        \/\/ safely read the properties of the pages without them being changed\n-        \/\/ by another thread. However, this function both iterates over the\n-        \/\/ pages AND frees\/recycles them. We \"yield\" the iterator, so that we\n-        \/\/ can perform immediate recycling (as long as no other thread is\n-        \/\/ iterating over the pages). The contract is that the pages that are\n-        \/\/ about to be freed are \"owned\" by this thread, and no other thread\n-        \/\/ will change their states.\n+        \/\/ An active iterator blocks immediate deletion of pages. The intent is\n+        \/\/ to allow the code that iterates over pages to safely read properties\n+        \/\/ of the pages without them being freed\/deleted. However, this\n+        \/\/ function both iterates over the pages AND frees them. We \"yield\" the\n+        \/\/ iterator, so that we can perform immediate deletion (as long as no\n+        \/\/ other thread is iterating over the pages). The contract is that the\n+        \/\/ pages that are about to be freed are \"owned\" by this thread, and no\n+        \/\/ other thread will change their states.\n@@ -937,1 +925,1 @@\n-  _page_allocator->promote_used(from_page->size());\n+  _page_allocator->promote_used(from_page, to_page);\n@@ -946,1 +934,1 @@\n-  _page_allocator->promote_used(from_page->size());\n+  _page_allocator->promote_used(from_page, to_page);\n","filename":"src\/hotspot\/share\/gc\/z\/zGeneration.cpp","additions":11,"deletions":23,"binary":false,"changes":34,"status":"modified"},{"patch":"@@ -26,0 +26,2 @@\n+#include \"gc\/z\/zAddress.hpp\"\n+#include \"gc\/z\/zAllocationFlags.hpp\"\n@@ -34,0 +36,2 @@\n+#include \"gc\/z\/zMappedCache.hpp\"\n+#include \"gc\/z\/zNUMA.inline.hpp\"\n@@ -37,1 +41,2 @@\n-#include \"gc\/z\/zPageCache.hpp\"\n+#include \"gc\/z\/zPageType.hpp\"\n+#include \"gc\/z\/zPhysicalMemoryManager.hpp\"\n@@ -42,1 +47,3 @@\n-#include \"gc\/z\/zUnmapper.hpp\"\n+#include \"gc\/z\/zValue.inline.hpp\"\n+#include \"gc\/z\/zVirtualMemory.inline.hpp\"\n+#include \"gc\/z\/zVirtualMemoryManager.inline.hpp\"\n@@ -46,0 +53,2 @@\n+#include \"memory\/allocation.hpp\"\n+#include \"nmt\/memTag.hpp\"\n@@ -50,0 +59,1 @@\n+#include \"utilities\/align.hpp\"\n@@ -52,0 +62,6 @@\n+#include \"utilities\/ticks.hpp\"\n+#include \"utilities\/vmError.hpp\"\n+\n+#include <cmath>\n+\n+class ZMemoryAllocation;\n@@ -54,1 +70,1 @@\n-static const ZStatCounter       ZCounterPageCacheFlush(\"Memory\", \"Page Cache Flush\", ZStatUnitBytesPerSecond);\n+static const ZStatCounter       ZCounterMappedCacheHarvest(\"Memory\", \"Mapped Cache Harvest\", ZStatUnitBytesPerSecond);\n@@ -58,6 +74,8 @@\n-ZSafePageRecycle::ZSafePageRecycle(ZPageAllocator* page_allocator)\n-  : _page_allocator(page_allocator),\n-    _unsafe_to_recycle() {}\n-\n-void ZSafePageRecycle::activate() {\n-  _unsafe_to_recycle.activate();\n+static void check_numa_mismatch(const ZVirtualMemory& vmem, uint32_t desired_id) {\n+  if (ZNUMA::is_enabled()) {\n+    \/\/ Check if memory ended up on desired NUMA node or not\n+    const uint32_t actual_id = ZNUMA::memory_id(untype(ZOffset::address(vmem.start())));\n+    if (actual_id != desired_id) {\n+      log_debug(gc, heap)(\"NUMA Mismatch: desired %d, actual %d\", desired_id, actual_id);\n+    }\n+  }\n@@ -66,4 +84,23 @@\n-void ZSafePageRecycle::deactivate() {\n-  auto delete_function = [&](ZPage* page) {\n-    _page_allocator->safe_destroy_page(page);\n-  };\n+class ZMemoryAllocation : public CHeapObj<mtGC> {\n+private:\n+  const size_t           _size;\n+  ZPartition*            _partition;\n+  ZVirtualMemory         _satisfied_from_cache_vmem;\n+  ZArray<ZVirtualMemory> _partial_vmems;\n+  int                    _num_harvested;\n+  size_t                 _harvested;\n+  size_t                 _increased_capacity;\n+  size_t                 _committed_capacity;\n+  bool                   _commit_failed;\n+\n+  explicit ZMemoryAllocation(const ZMemoryAllocation& other)\n+    : ZMemoryAllocation(other._size) {\n+    \/\/ Transfer the partition\n+    set_partition(other._partition);\n+\n+    \/\/ Reserve space for the partial vmems\n+    _partial_vmems.reserve(other._partial_vmems.length() + (other._satisfied_from_cache_vmem.is_null() ? 1 : 0));\n+\n+    \/\/ Transfer the claimed capacity\n+    transfer_claimed_capacity(other);\n+  }\n@@ -71,2 +108,5 @@\n-  _unsafe_to_recycle.deactivate_and_apply(delete_function);\n-}\n+  ZMemoryAllocation(const ZMemoryAllocation& a1, const ZMemoryAllocation& a2)\n+    : ZMemoryAllocation(a1._size + a2._size) {\n+    \/\/ Transfer the partition\n+    assert(a1._partition == a2._partition, \"only merge with same partition\");\n+    set_partition(a1._partition);\n@@ -74,5 +114,8 @@\n-ZPage* ZSafePageRecycle::register_and_clone_if_activated(ZPage* page) {\n-  if (!_unsafe_to_recycle.is_activated()) {\n-    \/\/ The page has no concurrent readers.\n-    \/\/ Recycle original page.\n-    return page;\n+    \/\/ Reserve space for the partial vmems\n+    const int num_vmems_a1 = a1._partial_vmems.length() + (a1._satisfied_from_cache_vmem.is_null() ? 1 : 0);\n+    const int num_vmems_a2 = a2._partial_vmems.length() + (a2._satisfied_from_cache_vmem.is_null() ? 1 : 0);\n+    _partial_vmems.reserve(num_vmems_a1 + num_vmems_a2);\n+\n+    \/\/ Transfer the claimed capacity\n+    transfer_claimed_capacity(a1);\n+    transfer_claimed_capacity(a2);\n@@ -81,2 +124,6 @@\n-  \/\/ The page could have concurrent readers.\n-  \/\/ It would be unsafe to recycle this page at this point.\n+  void transfer_claimed_capacity(const ZMemoryAllocation& from) {\n+    assert(from._committed_capacity == 0, \"Unexpected value %zu\", from._committed_capacity);\n+    assert(!from._commit_failed, \"Unexpected value\");\n+\n+    \/\/ Transfer increased capacity\n+    _increased_capacity += from._increased_capacity;\n@@ -84,9 +131,12 @@\n-  \/\/ As soon as the page is added to _unsafe_to_recycle, it\n-  \/\/ must not be used again. Hence, the extra double-checked\n-  \/\/ locking to only clone the page if it is believed to be\n-  \/\/ unsafe to recycle the page.\n-  ZPage* const cloned_page = page->clone_limited();\n-  if (!_unsafe_to_recycle.add_if_activated(page)) {\n-    \/\/ It became safe to recycle the page after the is_activated check\n-    delete cloned_page;\n-    return page;\n+    \/\/ Transfer satisfying vmem or partial mappings\n+    const ZVirtualMemory vmem = from._satisfied_from_cache_vmem;\n+    if (!vmem.is_null()) {\n+      assert(_partial_vmems.is_empty(), \"Must either have result or partial vmems\");\n+      _partial_vmems.push(vmem);\n+      _num_harvested += 1;\n+      _harvested += vmem.size();\n+    } else {\n+      _partial_vmems.appendAll(&from._partial_vmems);\n+      _num_harvested += from._num_harvested;\n+      _harvested += from._harvested;\n+    }\n@@ -95,4 +145,246 @@\n-  \/\/ The original page has been registered to be deleted by another thread.\n-  \/\/ Recycle the cloned page.\n-  return cloned_page;\n-}\n+public:\n+  explicit ZMemoryAllocation(size_t size)\n+    : _size(size),\n+      _partition(nullptr),\n+      _satisfied_from_cache_vmem(),\n+      _partial_vmems(0),\n+      _num_harvested(0),\n+      _harvested(0),\n+      _increased_capacity(0),\n+      _committed_capacity(0),\n+      _commit_failed(false) {}\n+\n+  void reset_for_retry() {\n+    assert(_satisfied_from_cache_vmem.is_null(), \"Incompatible with reset\");\n+\n+    _partition = nullptr;\n+    _partial_vmems.clear();\n+    _num_harvested = 0;\n+    _harvested = 0;\n+    _increased_capacity = 0;\n+    _committed_capacity = 0;\n+    _commit_failed = false;\n+  }\n+\n+  size_t size() const {\n+    return _size;\n+  }\n+\n+  ZPartition& partition() const {\n+    assert(_partition != nullptr, \"Should have been initialized\");\n+    return *_partition;\n+  }\n+\n+  void set_partition(ZPartition* partition) {\n+    assert(_partition == nullptr, \"Should be initialized only once\");\n+    _partition = partition;\n+  }\n+\n+  ZVirtualMemory satisfied_from_cache_vmem() const {\n+    return _satisfied_from_cache_vmem;\n+  }\n+\n+  void set_satisfied_from_cache_vmem(ZVirtualMemory vmem) {\n+    precond(_satisfied_from_cache_vmem.is_null());\n+    precond(vmem.size() == size());\n+    precond(_partial_vmems.is_empty());\n+\n+    _satisfied_from_cache_vmem = vmem;\n+  }\n+\n+  ZArray<ZVirtualMemory>* partial_vmems() {\n+    return &_partial_vmems;\n+  }\n+\n+  const ZArray<ZVirtualMemory>* partial_vmems() const {\n+    return &_partial_vmems;\n+  }\n+\n+  int num_harvested() const {\n+    return _num_harvested;\n+  }\n+\n+  size_t harvested() const {\n+    return _harvested;\n+  }\n+\n+  void set_harvested(int num_harvested, size_t harvested) {\n+    _num_harvested = num_harvested;\n+    _harvested = harvested;\n+  }\n+\n+  size_t increased_capacity() const {\n+    return _increased_capacity;\n+  }\n+\n+  void set_increased_capacity(size_t increased_capacity) {\n+    _increased_capacity = increased_capacity;\n+  }\n+\n+  size_t committed_capacity() const {\n+    return _committed_capacity;\n+  }\n+\n+  void set_committed_capacity(size_t committed_capacity) {\n+    assert(_committed_capacity == 0, \"Should only commit once\");\n+    _committed_capacity = committed_capacity;\n+    _commit_failed = committed_capacity != _increased_capacity;\n+  }\n+\n+  bool commit_failed() const {\n+    return _commit_failed;\n+  }\n+\n+  static void destroy(ZMemoryAllocation* allocation) {\n+    delete allocation;\n+  }\n+\n+  static void merge(const ZMemoryAllocation& allocation, ZMemoryAllocation** merge_location) {\n+    ZMemoryAllocation* const other_allocation = *merge_location;\n+    if (other_allocation == nullptr) {\n+      \/\/ First allocation, allocate new partition\n+      *merge_location = new ZMemoryAllocation(allocation);\n+    } else {\n+      \/\/ Merge with other allocation\n+      *merge_location = new ZMemoryAllocation(allocation, *other_allocation);\n+\n+      \/\/ Delete old allocation\n+      delete other_allocation;\n+    }\n+  }\n+};\n+\n+class ZSinglePartitionAllocation {\n+private:\n+  ZMemoryAllocation _allocation;\n+\n+public:\n+  ZSinglePartitionAllocation(size_t size)\n+    : _allocation(size) {}\n+\n+  size_t size() const {\n+    return _allocation.size();\n+  }\n+\n+  ZMemoryAllocation* allocation() {\n+    return &_allocation;\n+  }\n+\n+  const ZMemoryAllocation* allocation() const {\n+    return &_allocation;\n+  }\n+\n+  void reset_for_retry() {\n+    _allocation.reset_for_retry();\n+  }\n+};\n+\n+class ZMultiPartitionAllocation : public StackObj {\n+private:\n+  const size_t               _size;\n+  ZArray<ZMemoryAllocation*> _allocations;\n+\n+public:\n+  ZMultiPartitionAllocation(size_t size)\n+    : _size(size),\n+      _allocations(0) {}\n+\n+  ~ZMultiPartitionAllocation() {\n+    for (ZMemoryAllocation* allocation : _allocations) {\n+      ZMemoryAllocation::destroy(allocation);\n+    }\n+  }\n+\n+  void initialize() {\n+    precond(_allocations.is_empty());\n+\n+    \/\/ The multi-partition allocation creates at most one allocation per partition.\n+    const int length = (int)ZNUMA::count();\n+\n+    _allocations.reserve(length);\n+  }\n+\n+  void reset_for_retry() {\n+    for (ZMemoryAllocation* allocation : _allocations) {\n+      ZMemoryAllocation::destroy(allocation);\n+    }\n+    _allocations.clear();\n+  }\n+\n+  size_t size() const {\n+    return _size;\n+  }\n+\n+  ZArray<ZMemoryAllocation*>* allocations() {\n+    return &_allocations;\n+  }\n+\n+  const ZArray<ZMemoryAllocation*>* allocations() const {\n+    return &_allocations;\n+  }\n+\n+  void register_allocation(const ZMemoryAllocation& allocation) {\n+    ZMemoryAllocation** const slot = allocation_slot(allocation.partition().numa_id());\n+\n+    ZMemoryAllocation::merge(allocation, slot);\n+  }\n+\n+  ZMemoryAllocation** allocation_slot(uint32_t numa_id) {\n+    \/\/ Try to find an existing allocation for numa_id\n+    for (int i = 0; i < _allocations.length(); ++i) {\n+      ZMemoryAllocation** const slot_addr = _allocations.adr_at(i);\n+      ZMemoryAllocation* const allocation = *slot_addr;\n+      if (allocation->partition().numa_id() == numa_id) {\n+        \/\/ Found an existing slot\n+        return slot_addr;\n+      }\n+    }\n+\n+    \/\/ Push an empty slot for the numa_id\n+    _allocations.push(nullptr);\n+\n+    \/\/ Return the address of the slot\n+    return &_allocations.last();\n+  }\n+\n+  int sum_num_harvested_vmems() const {\n+    int total = 0;\n+\n+    for (const ZMemoryAllocation* allocation : _allocations) {\n+      total += allocation->num_harvested();\n+    }\n+\n+    return total;\n+  }\n+\n+  size_t sum_harvested() const {\n+    size_t total = 0;\n+\n+    for (const ZMemoryAllocation* allocation : _allocations) {\n+      total += allocation->harvested();\n+    }\n+\n+    return total;\n+  }\n+\n+  size_t sum_committed_increased_capacity() const {\n+    size_t total = 0;\n+\n+    for (const ZMemoryAllocation* allocation : _allocations) {\n+      total += allocation->committed_capacity();\n+    }\n+\n+    return total;\n+  }\n+};\n+\n+struct ZPageAllocationStats {\n+  int    _num_harvested_vmems;\n+  size_t _total_harvested;\n+  size_t _total_committed_capacity;\n+\n+  ZPageAllocationStats(int num_harvested_vmems, size_t total_harvested, size_t total_committed_capacity)\n+    : _num_harvested_vmems(num_harvested_vmems),\n+      _total_harvested(total_harvested),\n+      _total_committed_capacity(total_committed_capacity) {}\n+};\n@@ -107,0 +399,2 @@\n+  const ZPageAge             _age;\n+  const Ticks                _start_timestamp;\n@@ -109,3 +403,4 @@\n-  size_t                     _flushed;\n-  size_t                     _committed;\n-  ZList<ZPage>               _pages;\n+  const uint32_t             _initiating_numa_id;\n+  bool                       _is_multi_partition;\n+  ZSinglePartitionAllocation _single_partition_allocation;\n+  ZMultiPartitionAllocation  _multi_partition_allocation;\n@@ -116,1 +411,1 @@\n-  ZPageAllocation(ZPageType type, size_t size, ZAllocationFlags flags)\n+  ZPageAllocation(ZPageType type, size_t size, ZAllocationFlags flags, ZPageAge age)\n@@ -120,0 +415,2 @@\n+      _age(age),\n+      _start_timestamp(Ticks::now()),\n@@ -122,3 +419,4 @@\n-      _flushed(0),\n-      _committed(0),\n-      _pages(),\n+      _initiating_numa_id(ZNUMA::id()),\n+      _is_multi_partition(false),\n+      _single_partition_allocation(size),\n+      _multi_partition_allocation(size),\n@@ -128,0 +426,6 @@\n+  void reset_for_retry() {\n+    _is_multi_partition = false;\n+    _single_partition_allocation.reset_for_retry();\n+    _multi_partition_allocation.reset_for_retry();\n+  }\n+\n@@ -140,0 +444,4 @@\n+  ZPageAge age() const {\n+    return _age;\n+  }\n+\n@@ -148,2 +456,6 @@\n-  size_t flushed() const {\n-    return _flushed;\n+  uint32_t initiating_numa_id() const {\n+    return _initiating_numa_id;\n+  }\n+\n+  bool is_multi_partition() const {\n+    return _is_multi_partition;\n@@ -152,2 +464,4 @@\n-  void set_flushed(size_t flushed) {\n-    _flushed = flushed;\n+  void initiate_multi_partition_allocation() {\n+    assert(!_is_multi_partition, \"Reinitialization?\");\n+    _is_multi_partition = true;\n+    _multi_partition_allocation.initialize();\n@@ -156,2 +470,4 @@\n-  size_t committed() const {\n-    return _committed;\n+  ZMultiPartitionAllocation* multi_partition_allocation() {\n+    assert(_is_multi_partition, \"multi-partition allocation must be initiated\");\n+\n+    return &_multi_partition_allocation;\n@@ -160,2 +476,4 @@\n-  void set_committed(size_t committed) {\n-    _committed = committed;\n+  const ZMultiPartitionAllocation* multi_partition_allocation() const {\n+    assert(_is_multi_partition, \"multi-partition allocation must be initiated\");\n+\n+    return &_multi_partition_allocation;\n@@ -164,2 +482,18 @@\n-  bool wait() {\n-    return _stall_result.get();\n+  ZSinglePartitionAllocation* single_partition_allocation() {\n+    assert(!_is_multi_partition, \"multi-partition allocation must not have been initiated\");\n+\n+    return &_single_partition_allocation;\n+  }\n+\n+  const ZSinglePartitionAllocation* single_partition_allocation() const {\n+    assert(!_is_multi_partition, \"multi-partition allocation must not have been initiated\");\n+\n+    return &_single_partition_allocation;\n+  }\n+\n+  ZVirtualMemory satisfied_from_cache_vmem() const {\n+    precond(!_is_multi_partition);\n+\n+    const ZMemoryAllocation* const allocation = _single_partition_allocation.allocation();\n+\n+    return allocation->satisfied_from_cache_vmem();\n@@ -168,2 +502,2 @@\n-  ZList<ZPage>* pages() {\n-    return &_pages;\n+  bool wait() {\n+    return _stall_result.get();\n@@ -179,27 +513,12 @@\n-};\n-\n-ZPageAllocator::ZPageAllocator(size_t min_capacity,\n-                               size_t initial_capacity,\n-                               size_t soft_max_capacity,\n-                               size_t max_capacity)\n-  : _lock(),\n-    _cache(),\n-    _virtual(max_capacity),\n-    _physical(max_capacity),\n-    _min_capacity(min_capacity),\n-    _initial_capacity(initial_capacity),\n-    _max_capacity(max_capacity),\n-    _current_max_capacity(max_capacity),\n-    _capacity(0),\n-    _claimed(0),\n-    _used(0),\n-    _used_generations{0, 0},\n-    _collection_stats{{0, 0}, {0, 0}},\n-    _stalled(),\n-    _unmapper(new ZUnmapper(this)),\n-    _uncommitter(new ZUncommitter(this)),\n-    _safe_destroy(),\n-    _safe_recycle(this),\n-    _initialized(false) {\n-  if (!_virtual.is_initialized() || !_physical.is_initialized()) {\n-    return;\n+  ZPageAllocationStats stats() const {\n+    if (_is_multi_partition) {\n+      return ZPageAllocationStats(\n+          _multi_partition_allocation.sum_num_harvested_vmems(),\n+          _multi_partition_allocation.sum_harvested(),\n+          _multi_partition_allocation.sum_committed_increased_capacity());\n+    } else {\n+      return ZPageAllocationStats(\n+          _single_partition_allocation.allocation()->num_harvested(),\n+          _single_partition_allocation.allocation()->harvested(),\n+          _single_partition_allocation.allocation()->committed_capacity());\n+    }\n@@ -209,8 +528,16 @@\n-  log_info_p(gc, init)(\"Min Capacity: %zuM\", min_capacity \/ M);\n-  log_info_p(gc, init)(\"Initial Capacity: %zuM\", initial_capacity \/ M);\n-  log_info_p(gc, init)(\"Max Capacity: %zuM\", max_capacity \/ M);\n-  log_info_p(gc, init)(\"Soft Max Capacity: %zuM\", soft_max_capacity \/ M);\n-  if (ZPageSizeMedium > 0) {\n-    log_info_p(gc, init)(\"Medium Page Size: %zuM\", ZPageSizeMedium \/ M);\n-  } else {\n-    log_info_p(gc, init)(\"Medium Page Size: N\/A\");\n+  void send_event(bool successful) {\n+    EventZPageAllocation event;\n+\n+    Ticks end_timestamp = Ticks::now();\n+    const ZPageAllocationStats st = stats();\n+\n+    event.commit(_start_timestamp,\n+                 end_timestamp,\n+                 (u8)_type,\n+                 _size,\n+                 st._total_harvested,\n+                 st._total_committed_capacity,\n+                 (unsigned)st._num_harvested_vmems,\n+                 _is_multi_partition,\n+                 successful,\n+                 _flags.non_blocking());\n@@ -218,7 +545,1 @@\n-  log_info_p(gc, init)(\"Pre-touch: %s\", AlwaysPreTouch ? \"Enabled\" : \"Disabled\");\n-\n-  \/\/ Warn if system limits could stop us from reaching max capacity\n-  _physical.warn_commit_limits(max_capacity);\n-\n-  \/\/ Check if uncommit should and can be enabled\n-  _physical.try_enable_uncommit(min_capacity, max_capacity);\n+};\n@@ -226,2 +547,2 @@\n-  \/\/ Successfully initialized\n-  _initialized = true;\n+const ZVirtualMemoryManager& ZPartition::virtual_memory_manager() const {\n+  return _page_allocator->_virtual;\n@@ -230,2 +551,2 @@\n-bool ZPageAllocator::is_initialized() const {\n-  return _initialized;\n+ZVirtualMemoryManager& ZPartition::virtual_memory_manager() {\n+  return _page_allocator->_virtual;\n@@ -234,4 +555,3 @@\n-class ZPreTouchTask : public ZTask {\n-private:\n-  volatile uintptr_t _current;\n-  const uintptr_t    _end;\n+const ZPhysicalMemoryManager& ZPartition::physical_memory_manager() const {\n+  return _page_allocator->_physical;\n+}\n@@ -239,5 +559,3 @@\n-  static void pretouch(zaddress zaddr, size_t size) {\n-    const uintptr_t addr = untype(zaddr);\n-    const size_t page_size = ZLargePages::is_explicit() ? ZGranuleSize : os::vm_page_size();\n-    os::pretouch_memory((void*)addr, (void*)(addr + size), page_size);\n-  }\n+ZPhysicalMemoryManager& ZPartition::physical_memory_manager() {\n+  return _page_allocator->_physical;\n+}\n@@ -245,5 +563,1 @@\n-public:\n-  ZPreTouchTask(zoffset start, zoffset_end end)\n-    : ZTask(\"ZPreTouchTask\"),\n-      _current(untype(start)),\n-      _end(untype(end)) {}\n+#ifdef ASSERT\n@@ -251,2 +565,2 @@\n-  virtual void work() {\n-    const size_t size = ZGranuleSize;\n+void ZPartition::verify_virtual_memory_multi_partition_association(const ZVirtualMemory& vmem) const {\n+  const ZVirtualMemoryManager& manager = virtual_memory_manager();\n@@ -254,7 +568,4 @@\n-    for (;;) {\n-      \/\/ Claim an offset for this thread\n-      const uintptr_t claimed = Atomic::fetch_then_add(&_current, size);\n-      if (claimed >= _end) {\n-        \/\/ Done\n-        break;\n-      }\n+  assert(manager.is_in_multi_partition(vmem),\n+         \"Virtual memory must be associated with the extra space \"\n+         \"actual: %u\", virtual_memory_manager().lookup_partition_id(vmem));\n+}\n@@ -262,3 +573,2 @@\n-      \/\/ At this point we know that we have a valid zoffset \/ zaddress.\n-      const zoffset offset = to_zoffset(claimed);\n-      const zaddress addr = ZOffset::address(offset);\n+void ZPartition::verify_virtual_memory_association(const ZVirtualMemory& vmem, bool check_multi_partition) const {\n+  const ZVirtualMemoryManager& manager = virtual_memory_manager();\n@@ -266,3 +576,4 @@\n-      \/\/ Pre-touch the granule\n-      pretouch(addr, size);\n-    }\n+  if (check_multi_partition && manager.is_in_multi_partition(vmem)) {\n+    \/\/ We allow claim\/free\/commit physical operation in multi-partition allocations\n+    \/\/ to use virtual memory associated with the extra space.\n+    return;\n@@ -270,5 +581,5 @@\n-};\n-bool ZPageAllocator::prime_cache(ZWorkers* workers, size_t size) {\n-  ZAllocationFlags flags;\n-  flags.set_non_blocking();\n-  flags.set_low_address();\n+  const uint32_t vmem_numa_id = virtual_memory_manager().lookup_partition_id(vmem);\n+  assert(_numa_id == vmem_numa_id,\n+         \"Virtual memory must be associated with the current partition \"\n+         \"expected: %u, actual: %u\", _numa_id, vmem_numa_id);\n+}\n@@ -277,3 +588,3 @@\n-  ZPage* const page = alloc_page(ZPageType::large, size, flags, ZPageAge::eden);\n-  if (page == nullptr) {\n-    return false;\n+void ZPartition::verify_virtual_memory_association(const ZArray<ZVirtualMemory>* vmems) const {\n+  for (const ZVirtualMemory& vmem : *vmems) {\n+    verify_virtual_memory_association(vmem);\n@@ -281,0 +592,1 @@\n+}\n@@ -282,5 +594,5 @@\n-  if (AlwaysPreTouch) {\n-    \/\/ Pre-touch page\n-    ZPreTouchTask task(page->start(), page->end());\n-    workers->run_all(&task);\n-  }\n+void ZPartition::verify_memory_allocation_association(const ZMemoryAllocation* allocation) const {\n+  assert(this == &allocation->partition(),\n+         \"Memory allocation must be associated with the current partition \"\n+         \"expected: %u, actual: %u\", _numa_id, allocation->partition().numa_id());\n+}\n@@ -288,1 +600,1 @@\n-  free_page(page, false \/* allow_defragment *\/);\n+#endif \/\/ ASSERT\n@@ -290,2 +602,14 @@\n-  return true;\n-}\n+ZPartition::ZPartition(uint32_t numa_id, ZPageAllocator* page_allocator)\n+  : _page_allocator(page_allocator),\n+    _cache(),\n+    _uncommitter(numa_id, this),\n+    _min_capacity(ZNUMA::calculate_share(numa_id, page_allocator->min_capacity())),\n+    _max_capacity(ZNUMA::calculate_share(numa_id, page_allocator->max_capacity())),\n+    _current_max_capacity(_max_capacity),\n+    _capacity(0),\n+    _claimed(0),\n+    _used(0),\n+    _last_commit(0.0),\n+    _last_uncommit(0.0),\n+    _to_uncommit(0),\n+    _numa_id(numa_id) {}\n@@ -293,2 +617,2 @@\n-size_t ZPageAllocator::initial_capacity() const {\n-  return _initial_capacity;\n+uint32_t ZPartition::numa_id() const {\n+  return _numa_id;\n@@ -297,2 +621,2 @@\n-size_t ZPageAllocator::min_capacity() const {\n-  return _min_capacity;\n+size_t ZPartition::available() const {\n+  return _current_max_capacity - _used - _claimed;\n@@ -301,3 +625,2 @@\n-size_t ZPageAllocator::max_capacity() const {\n-  return _max_capacity;\n-}\n+size_t ZPartition::increase_capacity(size_t size) {\n+  const size_t increased = MIN2(size, _current_max_capacity - _capacity);\n@@ -305,6 +628,3 @@\n-size_t ZPageAllocator::soft_max_capacity() const {\n-  \/\/ Note that SoftMaxHeapSize is a manageable flag\n-  const size_t soft_max_capacity = Atomic::load(&SoftMaxHeapSize);\n-  const size_t current_max_capacity = Atomic::load(&_current_max_capacity);\n-  return MIN2(soft_max_capacity, current_max_capacity);\n-}\n+  if (increased > 0) {\n+    \/\/ Update atomically since we have concurrent readers\n+    Atomic::add(&_capacity, increased);\n@@ -312,2 +632,6 @@\n-size_t ZPageAllocator::capacity() const {\n-  return Atomic::load(&_capacity);\n+    _last_commit = os::elapsedTime();\n+    _last_uncommit = 0;\n+    _cache.reset_min();\n+  }\n+\n+  return increased;\n@@ -316,2 +640,15 @@\n-size_t ZPageAllocator::used() const {\n-  return Atomic::load(&_used);\n+void ZPartition::decrease_capacity(size_t size, bool set_max_capacity) {\n+  \/\/ Update capacity atomically since we have concurrent readers\n+  Atomic::sub(&_capacity, size);\n+\n+  \/\/ Adjust current max capacity to avoid further attempts to increase capacity\n+  if (set_max_capacity) {\n+    const size_t current_max_capacity_before = _current_max_capacity;\n+    Atomic::store(&_current_max_capacity, _capacity);\n+\n+    log_debug_p(gc)(\"Forced to lower max partition (%u) capacity from \"\n+                    \"%zuM(%.0f%%) to %zuM(%.0f%%)\",\n+                    _numa_id,\n+                    current_max_capacity_before \/ M, percent_of(current_max_capacity_before, _max_capacity),\n+                    _current_max_capacity \/ M, percent_of(_current_max_capacity, _max_capacity));\n+  }\n@@ -320,2 +657,5 @@\n-size_t ZPageAllocator::used_generation(ZGenerationId id) const {\n-  return Atomic::load(&_used_generations[(int)id]);\n+void ZPartition::increase_used(size_t size) {\n+  \/\/ The partition usage tracking is only read and updated under the page\n+  \/\/ allocator lock. Usage statistics for generations and GC cycles are\n+  \/\/ collected on the ZPageAllocator level.\n+  _used += size;\n@@ -324,6 +664,710 @@\n-size_t ZPageAllocator::unused() const {\n-  const ssize_t capacity = (ssize_t)Atomic::load(&_capacity);\n-  const ssize_t used = (ssize_t)Atomic::load(&_used);\n-  const ssize_t claimed = (ssize_t)Atomic::load(&_claimed);\n-  const ssize_t unused = capacity - used - claimed;\n-  return unused > 0 ? (size_t)unused : 0;\n+void ZPartition::decrease_used(size_t size) {\n+  \/\/ The partition usage tracking is only read and updated under the page\n+  \/\/ allocator lock. Usage statistics for generations and GC cycles are\n+  \/\/ collected on the ZPageAllocator level.\n+  _used -= size;\n+}\n+\n+void ZPartition::free_memory(const ZVirtualMemory& vmem) {\n+  const size_t size = vmem.size();\n+\n+  \/\/ Cache the vmem\n+  _cache.insert(vmem);\n+\n+  \/\/ Update accounting\n+  decrease_used(size);\n+}\n+\n+void ZPartition::claim_from_cache_or_increase_capacity(ZMemoryAllocation* allocation) {\n+  const size_t size = allocation->size();\n+  ZArray<ZVirtualMemory>* const out = allocation->partial_vmems();\n+\n+  \/\/ We are guaranteed to succeed the claiming of capacity here\n+  assert(available() >= size, \"Must be\");\n+\n+  \/\/ Associate the allocation with this partition.\n+  allocation->set_partition(this);\n+\n+  \/\/ Try to allocate one contiguous vmem\n+  ZVirtualMemory vmem = _cache.remove_contiguous(size);\n+  if (!vmem.is_null()) {\n+    \/\/ Found a satisfying vmem in the cache\n+    allocation->set_satisfied_from_cache_vmem(vmem);\n+\n+    \/\/ Done\n+    return;\n+  }\n+\n+  \/\/ Try increase capacity\n+  const size_t increased_capacity = increase_capacity(size);\n+\n+  allocation->set_increased_capacity(increased_capacity);\n+\n+  if (increased_capacity == size) {\n+    \/\/ Capacity increase covered the entire request, done.\n+    return;\n+  }\n+\n+  \/\/ Could not increase capacity enough to satisfy the allocation completely.\n+  \/\/ Try removing multiple vmems from the mapped cache.\n+  const size_t remaining = size - increased_capacity;\n+  const size_t harvested = _cache.remove_discontiguous(remaining, out);\n+  const int num_harvested = out->length();\n+\n+  allocation->set_harvested(num_harvested, harvested);\n+\n+  assert(harvested + increased_capacity == size,\n+         \"Mismatch harvested: %zu increased_capacity: %zu size: %zu\",\n+         harvested, increased_capacity, size);\n+\n+  return;\n+}\n+\n+bool ZPartition::claim_capacity(ZMemoryAllocation* allocation) {\n+  const size_t size = allocation->size();\n+\n+  if (available() < size) {\n+    \/\/ Out of memory\n+    return false;\n+  }\n+\n+  claim_from_cache_or_increase_capacity(allocation);\n+\n+  \/\/ Updated used statistics\n+  increase_used(size);\n+\n+  \/\/ Success\n+  return true;\n+}\n+\n+size_t ZPartition::uncommit(uint64_t* timeout, uintx delay) {\n+  ZArray<ZVirtualMemory> flushed_vmems;\n+  size_t flushed = 0;\n+\n+  {\n+    \/\/ We need to join the suspendible thread set while manipulating capacity\n+    \/\/ and used, to make sure GC safepoints will have a consistent view.\n+    SuspendibleThreadSetJoiner sts_joiner;\n+    ZLocker<ZLock> locker(&_page_allocator->_lock);\n+\n+    const double now = os::elapsedTime();\n+    const double time_since_last_commit = std::floor(now - _last_commit);\n+    const double time_since_last_uncommit = std::floor(now - _last_uncommit);\n+\n+    if (time_since_last_commit < double(delay)) {\n+      \/\/ We have committed within the delay, stop uncommitting.\n+      *timeout = uint64_t(double(delay) - time_since_last_commit);\n+      return 0;\n+    }\n+\n+    \/\/ We flush out and uncommit chunks at a time (~0.8% of the max capacity,\n+    \/\/ but at least one granule and at most 256M), in case demand for memory\n+    \/\/ increases while we are uncommitting.\n+    const size_t limit_upper_bound = MAX2(ZGranuleSize, align_down(256 * M \/ ZNUMA::count(), ZGranuleSize));\n+    const size_t limit = MIN2(align_up(_current_max_capacity >> 7, ZGranuleSize), limit_upper_bound);\n+\n+    if (limit == 0) {\n+      \/\/ This may occur if the current max capacity for this partition is 0\n+\n+      \/\/ Set timeout to delay\n+      *timeout = delay;\n+      return 0;\n+    }\n+\n+    if (time_since_last_uncommit < double(delay)) {\n+      \/\/ We are in the uncommit phase\n+      const size_t num_uncommits_left = _to_uncommit \/ limit;\n+      const double time_left = double(delay) - time_since_last_uncommit;\n+      if (time_left < *timeout * num_uncommits_left) {\n+        \/\/ Running out of time, speed up.\n+        uint64_t new_timeout = uint64_t(std::floor(time_left \/ double(num_uncommits_left + 1)));\n+        *timeout = new_timeout;\n+      }\n+    } else {\n+      \/\/ We are about to start uncommitting\n+      _to_uncommit = _cache.reset_min();\n+      _last_uncommit = now;\n+\n+      const size_t split = _to_uncommit \/ limit + 1;\n+      uint64_t new_timeout = delay \/ split;\n+      *timeout = new_timeout;\n+    }\n+\n+    \/\/ Never uncommit below min capacity.\n+    const size_t retain = MAX2(_used, _min_capacity);\n+    const size_t release = _capacity - retain;\n+    const size_t flush = MIN3(release, limit, _to_uncommit);\n+\n+    if (flush == 0) {\n+      \/\/ Nothing to flush\n+      return 0;\n+    }\n+\n+    \/\/ Flush memory from the mapped cache to uncommit\n+    flushed = _cache.remove_from_min(flush, &flushed_vmems);\n+    if (flushed == 0) {\n+      \/\/ Nothing flushed\n+      return 0;\n+    }\n+\n+    \/\/ Record flushed memory as claimed and how much we've flushed for this partition\n+    Atomic::add(&_claimed, flushed);\n+    _to_uncommit -= flushed;\n+  }\n+\n+  \/\/ Unmap and uncommit flushed memory\n+  for (const ZVirtualMemory vmem : flushed_vmems) {\n+    unmap_virtual(vmem);\n+    uncommit_physical(vmem);\n+    free_physical(vmem);\n+    free_virtual(vmem);\n+  }\n+\n+  {\n+    SuspendibleThreadSetJoiner sts_joiner;\n+    ZLocker<ZLock> locker(&_page_allocator->_lock);\n+\n+    \/\/ Adjust claimed and capacity to reflect the uncommit\n+    Atomic::sub(&_claimed, flushed);\n+    decrease_capacity(flushed, false \/* set_max_capacity *\/);\n+  }\n+\n+  return flushed;\n+}\n+\n+void ZPartition::sort_segments_physical(const ZVirtualMemory& vmem) {\n+  verify_virtual_memory_association(vmem, true \/* check_multi_partition *\/);\n+\n+  ZPhysicalMemoryManager& manager = physical_memory_manager();\n+\n+  \/\/ Sort physical segments\n+  manager.sort_segments_physical(vmem);\n+}\n+\n+void ZPartition::claim_physical(const ZVirtualMemory& vmem) {\n+  verify_virtual_memory_association(vmem, true \/* check_multi_partition *\/);\n+\n+  ZPhysicalMemoryManager& manager = physical_memory_manager();\n+\n+  \/\/ Alloc physical memory\n+  manager.alloc(vmem, _numa_id);\n+}\n+\n+void ZPartition::free_physical(const ZVirtualMemory& vmem) {\n+  verify_virtual_memory_association(vmem, true \/* check_multi_partition *\/);\n+\n+  ZPhysicalMemoryManager& manager = physical_memory_manager();\n+\n+  \/\/ Free physical memory\n+  manager.free(vmem, _numa_id);\n+}\n+\n+size_t ZPartition::commit_physical(const ZVirtualMemory& vmem) {\n+  verify_virtual_memory_association(vmem, true \/* check_multi_partition *\/);\n+\n+  ZPhysicalMemoryManager& manager = physical_memory_manager();\n+\n+  \/\/ Commit physical memory\n+  return manager.commit(vmem, _numa_id);\n+}\n+\n+size_t ZPartition::uncommit_physical(const ZVirtualMemory& vmem) {\n+  assert(ZUncommit, \"should not uncommit when uncommit is disabled\");\n+  verify_virtual_memory_association(vmem);\n+\n+  ZPhysicalMemoryManager& manager = physical_memory_manager();\n+\n+  \/\/ Uncommit physical memory\n+  return manager.uncommit(vmem);\n+}\n+\n+void ZPartition::map_virtual(const ZVirtualMemory& vmem) {\n+  verify_virtual_memory_association(vmem);\n+\n+  ZPhysicalMemoryManager& manager = physical_memory_manager();\n+\n+  \/\/ Map virtual memory to physical memory\n+  manager.map(vmem, _numa_id);\n+}\n+\n+void ZPartition::unmap_virtual(const ZVirtualMemory& vmem) {\n+  verify_virtual_memory_association(vmem);\n+\n+  ZPhysicalMemoryManager& manager = physical_memory_manager();\n+\n+  \/\/ Unmap virtual memory from physical memory\n+  manager.unmap(vmem);\n+}\n+\n+void ZPartition::map_virtual_from_multi_partition(const ZVirtualMemory& vmem) {\n+  verify_virtual_memory_multi_partition_association(vmem);\n+\n+  ZPhysicalMemoryManager& manager = physical_memory_manager();\n+\n+  \/\/ Sort physical segments\n+  manager.sort_segments_physical(vmem);\n+\n+  \/\/ Map virtual memory to physical memory\n+  manager.map(vmem, _numa_id);\n+}\n+\n+void ZPartition::unmap_virtual_from_multi_partition(const ZVirtualMemory& vmem) {\n+  verify_virtual_memory_multi_partition_association(vmem);\n+\n+  ZPhysicalMemoryManager& manager = physical_memory_manager();\n+\n+  \/\/ Unmap virtual memory from physical memory\n+  manager.unmap(vmem);\n+}\n+\n+ZVirtualMemory ZPartition::claim_virtual(size_t size) {\n+  ZVirtualMemoryManager& manager = virtual_memory_manager();\n+\n+  return manager.remove_from_low(size, _numa_id);\n+}\n+\n+size_t ZPartition::claim_virtual(size_t size, ZArray<ZVirtualMemory>* vmems_out) {\n+  ZVirtualMemoryManager& manager = virtual_memory_manager();\n+\n+  return manager.remove_from_low_many_at_most(size, _numa_id, vmems_out);\n+}\n+\n+void ZPartition::free_virtual(const ZVirtualMemory& vmem) {\n+  verify_virtual_memory_association(vmem);\n+\n+  ZVirtualMemoryManager& manager = virtual_memory_manager();\n+\n+  \/\/ Free virtual memory\n+  manager.insert(vmem, _numa_id);\n+}\n+\n+void ZPartition::free_and_claim_virtual_from_low_many(const ZVirtualMemory& vmem, ZArray<ZVirtualMemory>* vmems_out) {\n+  verify_virtual_memory_association(vmem);\n+\n+  ZVirtualMemoryManager& manager = virtual_memory_manager();\n+\n+  \/\/ Shuffle virtual memory\n+  manager.insert_and_remove_from_low_many(vmem, _numa_id, vmems_out);\n+}\n+\n+ZVirtualMemory ZPartition::free_and_claim_virtual_from_low_exact_or_many(size_t size, ZArray<ZVirtualMemory>* vmems_in_out) {\n+  verify_virtual_memory_association(vmems_in_out);\n+\n+  ZVirtualMemoryManager& manager = virtual_memory_manager();\n+\n+  \/\/ Shuffle virtual memory\n+  return manager.insert_and_remove_from_low_exact_or_many(size, _numa_id, vmems_in_out);\n+}\n+\n+static void pretouch_memory(zoffset start, size_t size) {\n+  \/\/ At this point we know that we have a valid zoffset \/ zaddress.\n+  const zaddress zaddr = ZOffset::address(start);\n+  const uintptr_t addr = untype(zaddr);\n+  const size_t page_size = ZLargePages::is_explicit() ? ZGranuleSize : os::vm_page_size();\n+  os::pretouch_memory((void*)addr, (void*)(addr + size), page_size);\n+}\n+\n+class ZPreTouchTask : public ZTask {\n+private:\n+  volatile uintptr_t _current;\n+  const uintptr_t    _end;\n+\n+public:\n+  ZPreTouchTask(zoffset start, zoffset_end end)\n+    : ZTask(\"ZPreTouchTask\"),\n+      _current(untype(start)),\n+      _end(untype(end)) {}\n+\n+  virtual void work() {\n+    const size_t size = ZGranuleSize;\n+\n+    for (;;) {\n+      \/\/ Claim an offset for this thread\n+      const uintptr_t claimed = Atomic::fetch_then_add(&_current, size);\n+      if (claimed >= _end) {\n+        \/\/ Done\n+        break;\n+      }\n+\n+      \/\/ At this point we know that we have a valid zoffset \/ zaddress.\n+      const zoffset offset = to_zoffset(claimed);\n+\n+      \/\/ Pre-touch the granule\n+      pretouch_memory(offset, size);\n+    }\n+  }\n+};\n+\n+bool ZPartition::prime(ZWorkers* workers, size_t size) {\n+  if (size == 0) {\n+    return true;\n+  }\n+\n+  ZArray<ZVirtualMemory> vmems;\n+\n+  \/\/ Claim virtual memory\n+  const size_t claimed_size = claim_virtual(size, &vmems);\n+\n+  \/\/ The partition must have size available in virtual memory when priming.\n+  assert(claimed_size == size, \"must succeed %zx == %zx\", claimed_size, size);\n+\n+  \/\/ Increase capacity\n+  increase_capacity(claimed_size);\n+\n+  for (ZVirtualMemory vmem : vmems) {\n+    \/\/ Claim the backing physical memory\n+    claim_physical(vmem);\n+\n+    \/\/ Commit the claimed physical memory\n+    const size_t committed = commit_physical(vmem);\n+\n+    if (committed != vmem.size()) {\n+      \/\/ This is a failure state. We do not cleanup the maybe partially committed memory.\n+      return false;\n+    }\n+\n+    map_virtual(vmem);\n+\n+    check_numa_mismatch(vmem, _numa_id);\n+\n+    if (AlwaysPreTouch) {\n+      \/\/ Pre-touch memory\n+      ZPreTouchTask task(vmem.start(), vmem.end());\n+      workers->run_all(&task);\n+    }\n+\n+    \/\/ We don't have to take a lock here as no other threads will access the cache\n+    \/\/ until we're finished\n+    _cache.insert(vmem);\n+  }\n+\n+  return true;\n+}\n+\n+ZVirtualMemory ZPartition::prepare_harvested_and_claim_virtual(ZMemoryAllocation* allocation) {\n+  verify_memory_allocation_association(allocation);\n+\n+  \/\/ Unmap virtual memory\n+  for (const ZVirtualMemory vmem : *allocation->partial_vmems()) {\n+    unmap_virtual(vmem);\n+  }\n+\n+  const size_t harvested = allocation->harvested();\n+  const int granule_count = (int)(harvested >> ZGranuleSizeShift);\n+  ZPhysicalMemoryManager& manager = physical_memory_manager();\n+\n+  \/\/ Stash segments\n+  ZArray<zbacking_index> stash(granule_count);\n+  manager.stash_segments(*allocation->partial_vmems(), &stash);\n+\n+  \/\/ Shuffle virtual memory. We attempt to allocate enough memory to cover the\n+  \/\/ entire allocation size, not just for the harvested memory.\n+  const ZVirtualMemory result = free_and_claim_virtual_from_low_exact_or_many(allocation->size(), allocation->partial_vmems());\n+\n+  \/\/ Restore segments\n+  if (!result.is_null()) {\n+    \/\/ Got exact match. Restore stashed physical segments for the harvested part.\n+    manager.restore_segments(result.first_part(harvested), stash);\n+  } else {\n+    \/\/ Got many partial vmems\n+    manager.restore_segments(*allocation->partial_vmems(), stash);\n+  }\n+\n+  if (result.is_null()) {\n+    \/\/ Before returning harvested memory to the cache it must be mapped.\n+    for (const ZVirtualMemory vmem : *allocation->partial_vmems()) {\n+      map_virtual(vmem);\n+    }\n+  }\n+\n+  return result;\n+}\n+\n+void ZPartition::copy_physical_segments_to_partition(const ZVirtualMemory& at, const ZVirtualMemory& from) {\n+  verify_virtual_memory_association(at);\n+  verify_virtual_memory_association(from, true \/* check_multi_partition *\/);\n+\n+  ZPhysicalMemoryManager& manager = physical_memory_manager();\n+\n+  \/\/ Copy segments\n+  manager.copy_physical_segments(at, from);\n+}\n+\n+void ZPartition::copy_physical_segments_from_partition(const ZVirtualMemory& at, const ZVirtualMemory& to) {\n+  verify_virtual_memory_association(at);\n+  verify_virtual_memory_association(to, true \/* check_multi_partition *\/);\n+\n+  ZPhysicalMemoryManager& manager = physical_memory_manager();\n+\n+\n+  \/\/ Copy segments\n+  manager.copy_physical_segments(to, at);\n+}\n+\n+void ZPartition::commit_increased_capacity(ZMemoryAllocation* allocation, const ZVirtualMemory& vmem) {\n+  assert(allocation->increased_capacity() > 0, \"Nothing to commit\");\n+\n+  const size_t already_committed = allocation->harvested();\n+\n+  const ZVirtualMemory already_committed_vmem = vmem.first_part(already_committed);\n+  const ZVirtualMemory to_be_committed_vmem = vmem.last_part(already_committed);\n+\n+  \/\/ Try to commit the uncommitted physical memory\n+  const size_t committed = commit_physical(to_be_committed_vmem);\n+\n+  \/\/ Keep track of the committed amount\n+  allocation->set_committed_capacity(committed);\n+}\n+\n+void ZPartition::map_memory(ZMemoryAllocation* allocation, const ZVirtualMemory& vmem) {\n+  sort_segments_physical(vmem);\n+  map_virtual(vmem);\n+\n+  check_numa_mismatch(vmem, allocation->partition().numa_id());\n+}\n+\n+void ZPartition::free_memory_alloc_failed(ZMemoryAllocation* allocation) {\n+  verify_memory_allocation_association(allocation);\n+\n+  \/\/ Only decrease the overall used and not the generation used,\n+  \/\/ since the allocation failed and generation used wasn't bumped.\n+  decrease_used(allocation->size());\n+\n+  size_t freed = 0;\n+\n+  \/\/ Free mapped memory\n+  for (const ZVirtualMemory vmem : *allocation->partial_vmems()) {\n+    freed += vmem.size();\n+    _cache.insert(vmem);\n+  }\n+  assert(allocation->harvested() + allocation->committed_capacity() == freed, \"must have freed all\");\n+\n+  \/\/ Adjust capacity to reflect the failed capacity increase\n+  const size_t remaining = allocation->size() - freed;\n+  if (remaining > 0) {\n+    const bool set_max_capacity = allocation->commit_failed();\n+    decrease_capacity(remaining, set_max_capacity);\n+  }\n+}\n+\n+void ZPartition::threads_do(ThreadClosure* tc) const {\n+  tc->do_thread(const_cast<ZUncommitter*>(&_uncommitter));\n+}\n+\n+void ZPartition::print_on(outputStream* st) const {\n+  st->print(\"Partition %u\", _numa_id);\n+  st->fill_to(17);\n+  st->print_cr(\"used %zuM, capacity %zuM, max capacity %zuM\",\n+               _used \/ M, _capacity \/ M, _max_capacity \/ M);\n+\n+  streamIndentor indentor(st, 1);\n+  print_cache_on(st);\n+}\n+\n+void ZPartition::print_cache_on(outputStream* st) const {\n+  _cache.print_on(st);\n+}\n+\n+void ZPartition::print_extended_on_error(outputStream* st) const {\n+  st->print_cr(\"Partition %u\", _numa_id);\n+\n+  streamIndentor indentor(st, 1);\n+\n+  _cache.print_extended_on(st);\n+}\n+\n+class ZMultiPartitionTracker : CHeapObj<mtGC> {\n+private:\n+  struct Element {\n+    ZVirtualMemory _vmem;\n+    ZPartition*    _partition;\n+  };\n+\n+  ZArray<Element> _map;\n+\n+  ZMultiPartitionTracker(int capacity)\n+    : _map(capacity) {}\n+\n+  const ZArray<Element>* map() const {\n+    return &_map;\n+  }\n+\n+  ZArray<Element>* map() {\n+    return &_map;\n+  }\n+\n+public:\n+  void prepare_memory_for_free(const ZVirtualMemory& vmem, ZArray<ZVirtualMemory>* vmems_out) const {\n+    \/\/ Remap memory back to original partition\n+    for (const Element partial_allocation : *map()) {\n+      ZVirtualMemory remaining_vmem = partial_allocation._vmem;\n+      ZPartition& partition = *partial_allocation._partition;\n+\n+      const size_t size = remaining_vmem.size();\n+\n+      \/\/ Allocate new virtual address ranges\n+      const int start_index = vmems_out->length();\n+      const size_t claimed_virtual = partition.claim_virtual(remaining_vmem.size(), vmems_out);\n+\n+      \/\/ We are holding memory associated with this partition, and we do not\n+      \/\/ overcommit virtual memory claiming. So virtual memory must always\n+      \/\/ be available.\n+      assert(claimed_virtual == size, \"must succeed\");\n+\n+      \/\/ Remap to the newly allocated virtual address ranges\n+      for (const ZVirtualMemory& to_vmem : vmems_out->slice_back(start_index)) {\n+        const ZVirtualMemory from_vmem = remaining_vmem.shrink_from_front(to_vmem.size());\n+\n+        \/\/ Copy physical segments\n+        partition.copy_physical_segments_to_partition(to_vmem, from_vmem);\n+\n+        \/\/ Unmap from_vmem\n+        partition.unmap_virtual_from_multi_partition(from_vmem);\n+\n+        \/\/ Map to_vmem\n+        partition.map_virtual(to_vmem);\n+      }\n+      assert(remaining_vmem.size() == 0, \"must have mapped all claimed virtual memory\");\n+    }\n+  }\n+\n+  static void destroy(const ZMultiPartitionTracker* tracker) {\n+    delete tracker;\n+  }\n+\n+  static ZMultiPartitionTracker* create(const ZMultiPartitionAllocation* multi_partition_allocation, const ZVirtualMemory& vmem) {\n+    const ZArray<ZMemoryAllocation*>* const partial_allocations = multi_partition_allocation->allocations();\n+\n+    ZMultiPartitionTracker* const tracker = new ZMultiPartitionTracker(partial_allocations->length());\n+\n+    ZVirtualMemory remaining = vmem;\n+\n+    \/\/ Each partial allocation is mapped to the virtual memory in order\n+    for (ZMemoryAllocation* partial_allocation : *partial_allocations) {\n+      \/\/ Track each separate vmem's partition\n+      const ZVirtualMemory partial_vmem = remaining.shrink_from_front(partial_allocation->size());\n+      ZPartition* const partition = &partial_allocation->partition();\n+      tracker->map()->push({partial_vmem, partition});\n+    }\n+\n+    return tracker;\n+  }\n+};\n+\n+ZPageAllocator::ZPageAllocator(size_t min_capacity,\n+                               size_t initial_capacity,\n+                               size_t soft_max_capacity,\n+                               size_t max_capacity)\n+  : _lock(),\n+    _virtual(max_capacity),\n+    _physical(max_capacity),\n+    _min_capacity(min_capacity),\n+    _max_capacity(max_capacity),\n+    _used(0),\n+    _used_generations{0,0},\n+    _collection_stats{{0, 0},{0, 0}},\n+    _partitions(ZValueIdTagType{}, this),\n+    _stalled(),\n+    _safe_destroy(),\n+    _initialized(false) {\n+\n+  if (!_virtual.is_initialized() || !_physical.is_initialized()) {\n+    return;\n+  }\n+\n+  log_info_p(gc, init)(\"Min Capacity: %zuM\", min_capacity \/ M);\n+  log_info_p(gc, init)(\"Initial Capacity: %zuM\", initial_capacity \/ M);\n+  log_info_p(gc, init)(\"Max Capacity: %zuM\", max_capacity \/ M);\n+  log_info_p(gc, init)(\"Soft Max Capacity: %zuM\", soft_max_capacity \/ M);\n+  if (ZPageSizeMedium > 0) {\n+    log_info_p(gc, init)(\"Medium Page Size: %zuM\", ZPageSizeMedium \/ M);\n+  } else {\n+    log_info_p(gc, init)(\"Medium Page Size: N\/A\");\n+  }\n+  log_info_p(gc, init)(\"Pre-touch: %s\", AlwaysPreTouch ? \"Enabled\" : \"Disabled\");\n+\n+  \/\/ Warn if system limits could stop us from reaching max capacity\n+  _physical.warn_commit_limits(max_capacity);\n+\n+  \/\/ Check if uncommit should and can be enabled\n+  _physical.try_enable_uncommit(min_capacity, max_capacity);\n+\n+  \/\/ Successfully initialized\n+  _initialized = true;\n+}\n+\n+bool ZPageAllocator::is_initialized() const {\n+  return _initialized;\n+}\n+\n+bool ZPageAllocator::prime_cache(ZWorkers* workers, size_t size) {\n+  ZPartitionIterator iter = partition_iterator();\n+  for (ZPartition* partition; iter.next(&partition);) {\n+    const uint32_t numa_id = partition->numa_id();\n+    const size_t to_prime = ZNUMA::calculate_share(numa_id, size);\n+\n+    if (!partition->prime(workers, to_prime)) {\n+      return false;\n+    }\n+  }\n+\n+  return true;\n+}\n+\n+size_t ZPageAllocator::min_capacity() const {\n+  return _min_capacity;\n+}\n+\n+size_t ZPageAllocator::max_capacity() const {\n+  return _max_capacity;\n+}\n+\n+size_t ZPageAllocator::soft_max_capacity() const {\n+  const size_t current_max_capacity = ZPageAllocator::current_max_capacity();\n+  const size_t soft_max_heapsize = Atomic::load(&SoftMaxHeapSize);\n+  return MIN2(soft_max_heapsize, current_max_capacity);\n+}\n+\n+size_t ZPageAllocator::current_max_capacity() const {\n+  size_t current_max_capacity = 0;\n+\n+  ZPartitionConstIterator iter = partition_iterator();\n+  for (const ZPartition* partition; iter.next(&partition);) {\n+    current_max_capacity += Atomic::load(&partition->_current_max_capacity);\n+  }\n+\n+  return current_max_capacity;\n+}\n+\n+size_t ZPageAllocator::capacity() const {\n+  size_t capacity = 0;\n+\n+  ZPartitionConstIterator iter = partition_iterator();\n+  for (const ZPartition* partition; iter.next(&partition);) {\n+    capacity += Atomic::load(&partition->_capacity);\n+  }\n+\n+  return capacity;\n+}\n+\n+size_t ZPageAllocator::used() const {\n+  return Atomic::load(&_used);\n+}\n+\n+size_t ZPageAllocator::used_generation(ZGenerationId id) const {\n+  return Atomic::load(&_used_generations[(int)id]);\n+}\n+\n+size_t ZPageAllocator::unused() const {\n+  const ssize_t used = (ssize_t)ZPageAllocator::used();\n+  ssize_t capacity = 0;\n+  ssize_t claimed = 0;\n+\n+  ZPartitionConstIterator iter = partition_iterator();\n+  for (const ZPartition* partition; iter.next(&partition);) {\n+    capacity += (ssize_t)Atomic::load(&partition->_capacity);\n+    claimed += (ssize_t)Atomic::load(&partition->_claimed);\n+  }\n+\n+  const ssize_t unused = capacity - used - claimed;\n+  return unused > 0 ? (size_t)unused : 0;\n@@ -334,0 +1378,1 @@\n+\n@@ -337,1 +1382,1 @@\n-                             _capacity,\n+                             capacity(),\n@@ -348,4 +1393,43 @@\n-void ZPageAllocator::reset_statistics(ZGenerationId id) {\n-  assert(SafepointSynchronize::is_at_safepoint(), \"Should be at safepoint\");\n-  _collection_stats[(int)id]._used_high = _used;\n-  _collection_stats[(int)id]._used_low = _used;\n+void ZPageAllocator::reset_statistics(ZGenerationId id) {\n+  assert(SafepointSynchronize::is_at_safepoint(), \"Should be at safepoint\");\n+#ifdef ASSERT\n+  {\n+    \/\/ We may free without safepoint synchronization, take the lock to get\n+    \/\/ consistent values.\n+    ZLocker<ZLock> locker(&_lock);\n+    size_t total_used = 0;\n+\n+    ZPartitionIterator iter(&_partitions);\n+    for (ZPartition* partition; iter.next(&partition);) {\n+      total_used += partition->_used;\n+    }\n+\n+    assert(total_used == _used, \"Must be consistent at safepoint %zu == %zu\", total_used, _used);\n+  }\n+#endif\n+\n+  \/\/ Read once, we may have concurrent writers.\n+  const size_t used = Atomic::load(&_used);\n+\n+  _collection_stats[(int)id]._used_high = used;\n+  _collection_stats[(int)id]._used_low = used;\n+}\n+\n+void ZPageAllocator::increase_used_generation(ZGenerationId id, size_t size) {\n+  \/\/ Update atomically since we have concurrent readers and writers\n+  Atomic::add(&_used_generations[(int)id], size, memory_order_relaxed);\n+}\n+\n+void ZPageAllocator::decrease_used_generation(ZGenerationId id, size_t size) {\n+  \/\/ Update atomically since we have concurrent readers and writers\n+  Atomic::sub(&_used_generations[(int)id], size, memory_order_relaxed);\n+}\n+\n+void ZPageAllocator::promote_used(const ZPage* from, const ZPage* to) {\n+  assert(from->start() == to->start(), \"pages start at same offset\");\n+  assert(from->size() == to->size(),   \"pages are the same size\");\n+  assert(from->age() != ZPageAge::old, \"must be promotion\");\n+  assert(to->age() == ZPageAge::old,   \"must be promotion\");\n+\n+  decrease_used_generation(ZGenerationId::young, to->size());\n+  increase_used_generation(ZGenerationId::old, to->size());\n@@ -354,2 +1438,5 @@\n-size_t ZPageAllocator::increase_capacity(size_t size) {\n-  const size_t increased = MIN2(size, _current_max_capacity - _capacity);\n+static void check_out_of_memory_during_initialization() {\n+  if (!is_init_completed()) {\n+    vm_exit_during_initialization(\"java.lang.OutOfMemoryError\", \"Java heap too small\");\n+  }\n+}\n@@ -357,3 +1444,4 @@\n-  if (increased > 0) {\n-    \/\/ Update atomically since we have concurrent readers\n-    Atomic::add(&_capacity, increased);\n+ZPage* ZPageAllocator::alloc_page(ZPageType type, size_t size, ZAllocationFlags flags, ZPageAge age) {\n+  EventZPageAllocation event;\n+\n+  ZPageAllocation allocation(type, size, flags, age);\n@@ -361,6 +1449,4 @@\n-    \/\/ Record time of last commit. When allocation, we prefer increasing\n-    \/\/ the capacity over flushing the cache. That means there could be\n-    \/\/ expired pages in the cache at this time. However, since we are\n-    \/\/ increasing the capacity we are obviously in need of committed\n-    \/\/ memory and should therefore not be uncommitting memory.\n-    _cache.set_last_commit();\n+  \/\/ Allocate the page\n+  ZPage* const page = alloc_page_inner(&allocation);\n+  if (page == nullptr) {\n+    return nullptr;\n@@ -369,1 +1455,23 @@\n-  return increased;\n+  \/\/ Update allocation statistics. Exclude gc relocations to avoid\n+  \/\/ artificial inflation of the allocation rate during relocation.\n+  if (!flags.gc_relocation() && is_init_completed()) {\n+    \/\/ Note that there are two allocation rate counters, which have\n+    \/\/ different purposes and are sampled at different frequencies.\n+    ZStatInc(ZCounterMutatorAllocationRate, size);\n+    ZStatMutatorAllocRate::sample_allocation(size);\n+  }\n+\n+  const ZPageAllocationStats stats = allocation.stats();\n+  const int num_harvested_vmems = stats._num_harvested_vmems;\n+  const size_t harvested = stats._total_harvested;\n+  const size_t committed = stats._total_committed_capacity;\n+\n+  if (harvested > 0) {\n+    ZStatInc(ZCounterMappedCacheHarvest, harvested);\n+    log_debug(gc, heap)(\"Mapped Cache Harvested: %zuM (%d)\", harvested \/ M, num_harvested_vmems);\n+  }\n+\n+  \/\/ Send event for successful allocation\n+  allocation.send_event(true \/* successful *\/);\n+\n+  return page;\n@@ -372,3 +1480,3 @@\n-void ZPageAllocator::decrease_capacity(size_t size, bool set_max_capacity) {\n-  \/\/ Update atomically since we have concurrent readers\n-  Atomic::sub(&_capacity, size);\n+bool ZPageAllocator::alloc_page_stall(ZPageAllocation* allocation) {\n+  ZStatTimer timer(ZCriticalPhaseAllocationStall);\n+  EventZAllocationStall event;\n@@ -376,6 +1484,2 @@\n-  if (set_max_capacity) {\n-    \/\/ Adjust current max capacity to avoid further attempts to increase capacity\n-    log_error_p(gc)(\"Forced to lower max Java heap size from \"\n-                    \"%zuM(%.0f%%) to %zuM(%.0f%%)\",\n-                    _current_max_capacity \/ M, percent_of(_current_max_capacity, _max_capacity),\n-                    _capacity \/ M, percent_of(_capacity, _max_capacity));\n+  \/\/ We can only block if the VM is fully initialized\n+  check_out_of_memory_during_initialization();\n@@ -383,2 +1487,70 @@\n-    \/\/ Update atomically since we have concurrent readers\n-    Atomic::store(&_current_max_capacity, _capacity);\n+  \/\/ Start asynchronous minor GC\n+  const ZDriverRequest request(GCCause::_z_allocation_stall, ZYoungGCThreads, 0);\n+  ZDriver::minor()->collect(request);\n+\n+  \/\/ Wait for allocation to complete or fail\n+  const bool result = allocation->wait();\n+\n+  {\n+    \/\/ Guard deletion of underlying semaphore. This is a workaround for\n+    \/\/ a bug in sem_post() in glibc < 2.21, where it's not safe to destroy\n+    \/\/ the semaphore immediately after returning from sem_wait(). The\n+    \/\/ reason is that sem_post() can touch the semaphore after a waiting\n+    \/\/ thread have returned from sem_wait(). To avoid this race we are\n+    \/\/ forcing the waiting thread to acquire\/release the lock held by the\n+    \/\/ posting thread. https:\/\/sourceware.org\/bugzilla\/show_bug.cgi?id=12674\n+    ZLocker<ZLock> locker(&_lock);\n+  }\n+\n+  \/\/ Send event\n+  event.commit((u8)allocation->type(), allocation->size());\n+\n+  return result;\n+}\n+\n+ZPage* ZPageAllocator::alloc_page_inner(ZPageAllocation* allocation) {\n+retry:\n+\n+  \/\/ Claim the capacity needed for this allocation.\n+  \/\/\n+  \/\/ The claimed capacity comes from memory already mapped in the cache, or\n+  \/\/ from increasing the capacity. The increased capacity allows us to allocate\n+  \/\/ physical memory from the physical memory manager later on.\n+  \/\/\n+  \/\/ Note that this call might block in a safepoint if the non-blocking flag is\n+  \/\/ not set.\n+  if (!claim_capacity_or_stall(allocation)) {\n+    \/\/ Out of memory\n+    return nullptr;\n+  }\n+\n+  \/\/ If the entire claimed capacity came from claiming a single vmem from the\n+  \/\/ mapped cache then the allocation has been satisfied and we are done.\n+  const ZVirtualMemory cached_vmem = satisfied_from_cache_vmem(allocation);\n+  if (!cached_vmem.is_null()) {\n+    return create_page(allocation, cached_vmem);\n+  }\n+\n+  \/\/ We couldn't find a satisfying vmem in the cache, so we need to build one.\n+\n+  \/\/ Claim virtual memory, either from remapping harvested vmems from the\n+  \/\/ mapped cache or by claiming it straight from the virtual memory manager.\n+  const ZVirtualMemory vmem = claim_virtual_memory(allocation);\n+  if (vmem.is_null()) {\n+    log_error(gc)(\"Out of address space\");\n+    free_after_alloc_page_failed(allocation);\n+\n+    \/\/ Crash in debug builds for more information\n+    DEBUG_ONLY(fatal(\"Out of address space\");)\n+\n+    return nullptr;\n+  }\n+\n+  \/\/ Claim physical memory for the increased capacity. The previous claiming of\n+  \/\/ capacity guarantees that this will succeed.\n+  claim_physical_for_increased_capacity(allocation, vmem);\n+\n+  \/\/ Commit memory for the increased capacity and map the entire vmem.\n+  if (!commit_and_map(allocation, vmem)) {\n+    free_after_alloc_page_failed(allocation);\n+    goto retry;\n@@ -386,0 +1558,2 @@\n+\n+  return create_page(allocation, vmem);\n@@ -388,8 +1562,3 @@\n-void ZPageAllocator::increase_used(size_t size) {\n-  \/\/ We don't track generation usage here because this page\n-  \/\/ could be allocated by a thread that satisfies a stalling\n-  \/\/ allocation. The stalled thread can wake up and potentially\n-  \/\/ realize that the page alloc should be undone. If the alloc\n-  \/\/ and the undo gets separated by a safepoint, the generation\n-  \/\/ statistics could se a decreasing used value between mark\n-  \/\/ start and mark end.\n+bool ZPageAllocator::claim_capacity_or_stall(ZPageAllocation* allocation) {\n+  {\n+    ZLocker<ZLock> locker(&_lock);\n@@ -397,2 +1566,4 @@\n-  \/\/ Update atomically since we have concurrent readers\n-  const size_t used = Atomic::add(&_used, size);\n+    \/\/ Try to claim memory\n+    if (claim_capacity(allocation)) {\n+      \/\/ Keep track of usage\n+      increase_used(allocation->size());\n@@ -400,4 +1571,7 @@\n-  \/\/ Update used high\n-  for (auto& stats : _collection_stats) {\n-    if (used > stats._used_high) {\n-      stats._used_high = used;\n+      return true;\n+    }\n+\n+    \/\/ Failed to claim memory\n+    if (allocation->flags().non_blocking()) {\n+      \/\/ Don't stall\n+      return false;\n@@ -405,0 +1579,3 @@\n+\n+    \/\/ Enqueue allocation request\n+    _stalled.insert_last(allocation);\n@@ -406,0 +1583,3 @@\n+\n+  \/\/ Stall\n+  return alloc_page_stall(allocation);\n@@ -408,3 +1588,4 @@\n-void ZPageAllocator::decrease_used(size_t size) {\n-  \/\/ Update atomically since we have concurrent readers\n-  const size_t used = Atomic::sub(&_used, size);\n+bool ZPageAllocator::claim_capacity(ZPageAllocation* allocation) {\n+  const uint32_t start_numa_id = allocation->initiating_numa_id();\n+  const uint32_t start_partition = start_numa_id;\n+  const uint32_t num_partitions = _partitions.count();\n@@ -412,4 +1593,83 @@\n-  \/\/ Update used low\n-  for (auto& stats : _collection_stats) {\n-    if (used < stats._used_low) {\n-      stats._used_low = used;\n+  \/\/ Round robin single-partition claiming\n+\n+  for (uint32_t i = 0; i < num_partitions; ++i) {\n+    const uint32_t partition_id = (start_partition + i) % num_partitions;\n+\n+    if (claim_capacity_single_partition(allocation->single_partition_allocation(), partition_id)) {\n+      return true;\n+    }\n+  }\n+\n+  if (!is_multi_partition_enabled() || sum_available() < allocation->size()) {\n+    \/\/ Multi-partition claiming is not possible\n+    return false;\n+  }\n+\n+  \/\/ Multi-partition claiming\n+\n+  \/\/ Flip allocation to multi-partition allocation\n+  allocation->initiate_multi_partition_allocation();\n+\n+  ZMultiPartitionAllocation* const multi_partition_allocation = allocation->multi_partition_allocation();\n+\n+  claim_capacity_multi_partition(multi_partition_allocation, start_partition);\n+\n+  return true;\n+}\n+\n+bool ZPageAllocator::claim_capacity_single_partition(ZSinglePartitionAllocation* single_partition_allocation, uint32_t partition_id) {\n+  ZPartition& partition = _partitions.get(partition_id);\n+\n+  return partition.claim_capacity(single_partition_allocation->allocation());\n+}\n+\n+void ZPageAllocator::claim_capacity_multi_partition(ZMultiPartitionAllocation* multi_partition_allocation, uint32_t start_partition) {\n+  const size_t size = multi_partition_allocation->size();\n+  const uint32_t num_partitions = _partitions.count();\n+  const size_t split_size = align_up(size \/ num_partitions, ZGranuleSize);\n+\n+  size_t remaining = size;\n+\n+  const auto do_claim_one_partition = [&](ZPartition& partition, bool claim_evenly) {\n+    if (remaining == 0) {\n+      \/\/ All memory claimed\n+      return false;\n+    }\n+\n+    const size_t max_alloc_size = claim_evenly ? MIN2(split_size, remaining) : remaining;\n+\n+    \/\/ This guarantees that claim_physical below will succeed\n+    const size_t alloc_size = MIN2(max_alloc_size, partition.available());\n+\n+    \/\/ Skip over empty allocations\n+    if (alloc_size == 0) {\n+      \/\/ Continue\n+      return true;\n+    }\n+\n+    ZMemoryAllocation partial_allocation(alloc_size);\n+\n+    \/\/ Claim capacity for this allocation - this should succeed\n+    const bool result = partition.claim_capacity(&partial_allocation);\n+    assert(result, \"Should have succeeded\");\n+\n+    \/\/ Register allocation\n+    multi_partition_allocation->register_allocation(partial_allocation);\n+\n+    \/\/ Update remaining\n+    remaining -= alloc_size;\n+\n+    \/\/ Continue\n+    return true;\n+  };\n+\n+  \/\/ Loops over every partition and claims memory\n+  const auto do_claim_each_partition = [&](bool claim_evenly) {\n+    for (uint32_t i = 0; i < num_partitions; ++i) {\n+      const uint32_t partition_id = (start_partition + i) % num_partitions;\n+      ZPartition& partition = _partitions.get(partition_id);\n+\n+      if (!do_claim_one_partition(partition, claim_evenly)) {\n+        \/\/ All memory claimed\n+        break;\n+      }\n@@ -417,0 +1677,18 @@\n+  };\n+\n+  \/\/ Try to claim from multiple partitions\n+\n+  \/\/ Try to claim up to split_size on each partition\n+  do_claim_each_partition(true  \/* claim_evenly *\/);\n+\n+  \/\/ Try claim the remaining\n+  do_claim_each_partition(false \/* claim_evenly *\/);\n+\n+  assert(remaining == 0, \"Must have claimed capacity for the whole allocation\");\n+}\n+\n+ZVirtualMemory ZPageAllocator::satisfied_from_cache_vmem(const ZPageAllocation* allocation) const {\n+  if (allocation->is_multi_partition()) {\n+    \/\/ Multi-partition allocations are always harvested and\/or committed, so\n+    \/\/ there's never a satisfying vmem from the caches.\n+    return {};\n@@ -418,0 +1696,2 @@\n+\n+  return allocation->satisfied_from_cache_vmem();\n@@ -420,3 +1700,10 @@\n-void ZPageAllocator::increase_used_generation(ZGenerationId id, size_t size) {\n-  \/\/ Update atomically since we have concurrent readers\n-  Atomic::add(&_used_generations[(int)id], size, memory_order_relaxed);\n+ZVirtualMemory ZPageAllocator::claim_virtual_memory(ZPageAllocation* allocation) {\n+  \/\/ Note: that the single-partition performs \"shuffling\" of already harvested\n+  \/\/ vmem(s), while the multi-partition searches for available virtual memory\n+  \/\/ area without shuffling.\n+\n+  if (allocation->is_multi_partition()) {\n+    return claim_virtual_memory_multi_partition(allocation->multi_partition_allocation());\n+  } else {\n+    return claim_virtual_memory_single_partition(allocation->single_partition_allocation());\n+  }\n@@ -425,3 +1712,12 @@\n-void ZPageAllocator::decrease_used_generation(ZGenerationId id, size_t size) {\n-  \/\/ Update atomically since we have concurrent readers\n-  Atomic::sub(&_used_generations[(int)id], size, memory_order_relaxed);\n+ZVirtualMemory ZPageAllocator::claim_virtual_memory_single_partition(ZSinglePartitionAllocation* single_partition_allocation) {\n+  ZMemoryAllocation* const allocation = single_partition_allocation->allocation();\n+  ZPartition& partition = allocation->partition();\n+\n+  if (allocation->harvested() > 0) {\n+    \/\/ We claim virtual memory from the harvested vmems and perhaps also\n+    \/\/ allocate more to match the allocation request.\n+    return partition.prepare_harvested_and_claim_virtual(allocation);\n+  } else {\n+    \/\/ Just try to claim virtual memory\n+    return partition.claim_virtual(allocation->size());\n+  }\n@@ -430,3 +1726,12 @@\n-void ZPageAllocator::promote_used(size_t size) {\n-  decrease_used_generation(ZGenerationId::young, size);\n-  increase_used_generation(ZGenerationId::old, size);\n+ZVirtualMemory ZPageAllocator::claim_virtual_memory_multi_partition(ZMultiPartitionAllocation* multi_partition_allocation) {\n+  const size_t size = multi_partition_allocation->size();\n+\n+  const ZVirtualMemory vmem = _virtual.remove_from_low_multi_partition(size);\n+  if (!vmem.is_null()) {\n+    \/\/ Copy claimed multi-partition vmems, we leave the old vmems mapped until\n+    \/\/ after we have committed. In case committing fails we can simply\n+    \/\/ reinsert the initial vmems.\n+    copy_claimed_physical_multi_partition(multi_partition_allocation, vmem);\n+  }\n+\n+  return vmem;\n@@ -435,3 +1740,20 @@\n-bool ZPageAllocator::commit_page(ZPage* page) {\n-  \/\/ Commit physical memory\n-  return _physical.commit(page->physical_memory());\n+void ZPageAllocator::copy_claimed_physical_multi_partition(ZMultiPartitionAllocation* multi_partition_allocation, const ZVirtualMemory& vmem) {\n+  \/\/ Start at the new dest offset\n+  ZVirtualMemory remaining_dest_vmem = vmem;\n+\n+  for (const ZMemoryAllocation* partial_allocation : *multi_partition_allocation->allocations()) {\n+    \/\/ Split off the partial allocation's destination vmem\n+    ZVirtualMemory partial_dest_vmem = remaining_dest_vmem.shrink_from_front(partial_allocation->size());\n+\n+    \/\/ Get the partial allocation's partition\n+    ZPartition& partition = partial_allocation->partition();\n+\n+    \/\/ Copy all physical segments from the partition to the destination vmem\n+    for (const ZVirtualMemory from_vmem : *partial_allocation->partial_vmems()) {\n+      \/\/ Split off destination\n+      const ZVirtualMemory to_vmem = partial_dest_vmem.shrink_from_front(from_vmem.size());\n+\n+      \/\/ Copy physical segments\n+      partition.copy_physical_segments_from_partition(from_vmem, to_vmem);\n+    }\n+  }\n@@ -440,3 +1762,7 @@\n-void ZPageAllocator::uncommit_page(ZPage* page) {\n-  if (!ZUncommit) {\n-    return;\n+void ZPageAllocator::claim_physical_for_increased_capacity(ZPageAllocation* allocation, const ZVirtualMemory& vmem) {\n+  assert(allocation->size() == vmem.size(), \"vmem should be the final entry\");\n+\n+  if (allocation->is_multi_partition()) {\n+    claim_physical_for_increased_capacity_multi_partition(allocation->multi_partition_allocation(), vmem);\n+  } else {\n+    claim_physical_for_increased_capacity_single_partition(allocation->single_partition_allocation(), vmem);\n@@ -444,0 +1770,1 @@\n+}\n@@ -445,2 +1772,2 @@\n-  \/\/ Uncommit physical memory\n-  _physical.uncommit(page->physical_memory());\n+void ZPageAllocator::claim_physical_for_increased_capacity_single_partition(ZSinglePartitionAllocation* single_partition_allocation, const ZVirtualMemory& vmem) {\n+  claim_physical_for_increased_capacity(single_partition_allocation->allocation(), vmem);\n@@ -449,3 +1776,7 @@\n-void ZPageAllocator::map_page(const ZPage* page) const {\n-  \/\/ Map physical memory\n-  _physical.map(page->start(), page->physical_memory());\n+void ZPageAllocator::claim_physical_for_increased_capacity_multi_partition(const ZMultiPartitionAllocation* multi_partition_allocation, const ZVirtualMemory& vmem) {\n+  ZVirtualMemory remaining = vmem;\n+\n+  for (ZMemoryAllocation* allocation : *multi_partition_allocation->allocations()) {\n+    const ZVirtualMemory partial = remaining.shrink_from_front(allocation->size());\n+    claim_physical_for_increased_capacity(allocation, partial);\n+  }\n@@ -454,3 +1785,18 @@\n-void ZPageAllocator::unmap_page(const ZPage* page) const {\n-  \/\/ Unmap physical memory\n-  _physical.unmap(page->start(), page->size());\n+void ZPageAllocator::claim_physical_for_increased_capacity(ZMemoryAllocation* allocation, const ZVirtualMemory& vmem) {\n+  \/\/ The previously harvested memory is memory that has already been committed\n+  \/\/ and mapped. The rest of the vmem gets physical memory assigned here and\n+  \/\/ will be committed in a subsequent function.\n+\n+  const size_t already_committed = allocation->harvested();\n+  const size_t non_committed = allocation->size() - already_committed;\n+  const size_t increased_capacity = allocation->increased_capacity();\n+\n+  assert(non_committed == increased_capacity,\n+         \"Mismatch non_committed: \" PTR_FORMAT \" increased_capacity: \" PTR_FORMAT,\n+         non_committed, increased_capacity);\n+\n+  if (non_committed > 0) {\n+    ZPartition& partition = allocation->partition();\n+    ZVirtualMemory non_committed_vmem = vmem.last_part(already_committed);\n+    partition.claim_physical(non_committed_vmem);\n+  }\n@@ -459,3 +1805,8 @@\n-void ZPageAllocator::safe_destroy_page(ZPage* page) {\n-  \/\/ Destroy page safely\n-  _safe_destroy.schedule_delete(page);\n+bool ZPageAllocator::commit_and_map(ZPageAllocation* allocation, const ZVirtualMemory& vmem) {\n+  assert(allocation->size() == vmem.size(), \"vmem should be the final entry\");\n+\n+  if (allocation->is_multi_partition()) {\n+    return commit_and_map_multi_partition(allocation->multi_partition_allocation(), vmem);\n+  } else {\n+    return commit_and_map_single_partition(allocation->single_partition_allocation(), vmem);\n+  }\n@@ -464,3 +1815,2 @@\n-void ZPageAllocator::destroy_page(ZPage* page) {\n-  \/\/ Free virtual memory\n-  _virtual.free(page->virtual_memory());\n+bool ZPageAllocator::commit_and_map_single_partition(ZSinglePartitionAllocation* single_partition_allocation, const ZVirtualMemory& vmem) {\n+  const bool commit_successful = commit_single_partition(single_partition_allocation, vmem);\n@@ -468,2 +1818,120 @@\n-  \/\/ Free physical memory\n-  _physical.free(page->physical_memory());\n+  \/\/ Map the vmem\n+  map_committed_single_partition(single_partition_allocation, vmem);\n+\n+  if (commit_successful) {\n+    return true;\n+  }\n+\n+  \/\/ Commit failed\n+  cleanup_failed_commit_single_partition(single_partition_allocation, vmem);\n+\n+  return false;\n+}\n+\n+bool ZPageAllocator::commit_and_map_multi_partition(ZMultiPartitionAllocation* multi_partition_allocation, const ZVirtualMemory& vmem) {\n+  if (commit_multi_partition(multi_partition_allocation, vmem)) {\n+    \/\/ Commit successful\n+\n+    \/\/ Unmap harvested vmems\n+    unmap_harvested_multi_partition(multi_partition_allocation);\n+\n+    \/\/ Map the vmem\n+    map_committed_multi_partition(multi_partition_allocation, vmem);\n+\n+    return true;\n+  }\n+\n+  \/\/ Commit failed\n+  cleanup_failed_commit_multi_partition(multi_partition_allocation, vmem);\n+\n+  return false;\n+}\n+\n+void ZPageAllocator::commit(ZMemoryAllocation* allocation, const ZVirtualMemory& vmem) {\n+  ZPartition& partition = allocation->partition();\n+\n+  if (allocation->increased_capacity() > 0) {\n+    \/\/ Commit memory\n+    partition.commit_increased_capacity(allocation, vmem);\n+  }\n+}\n+\n+bool ZPageAllocator::commit_single_partition(ZSinglePartitionAllocation* single_partition_allocation, const ZVirtualMemory& vmem) {\n+  ZMemoryAllocation* const allocation = single_partition_allocation->allocation();\n+\n+  commit(allocation, vmem);\n+\n+  return !allocation->commit_failed();\n+}\n+\n+bool ZPageAllocator::commit_multi_partition(ZMultiPartitionAllocation* multi_partition_allocation, const ZVirtualMemory& vmem) {\n+  bool commit_failed = false;\n+  ZVirtualMemory remaining = vmem;\n+  for (ZMemoryAllocation* const allocation : *multi_partition_allocation->allocations()) {\n+    \/\/ Split off the partial allocation's memory range\n+    const ZVirtualMemory partial_vmem = remaining.shrink_from_front(allocation->size());\n+\n+    commit(allocation, partial_vmem);\n+\n+    \/\/ Keep track if any partial allocation failed to commit\n+    commit_failed |= allocation->commit_failed();\n+  }\n+\n+  assert(remaining.size() == 0, \"all memory must be accounted for\");\n+\n+  return !commit_failed;\n+}\n+\n+void ZPageAllocator::unmap_harvested_multi_partition(ZMultiPartitionAllocation* multi_partition_allocation) {\n+  for (ZMemoryAllocation* const allocation : *multi_partition_allocation->allocations()) {\n+    ZPartition& partition = allocation->partition();\n+    ZArray<ZVirtualMemory>* const partial_vmems = allocation->partial_vmems();\n+\n+    \/\/ Unmap harvested vmems\n+    while (!partial_vmems->is_empty()) {\n+      const ZVirtualMemory to_unmap = partial_vmems->pop();\n+      partition.unmap_virtual(to_unmap);\n+      partition.free_virtual(to_unmap);\n+    }\n+  }\n+}\n+\n+void ZPageAllocator::map_committed_single_partition(ZSinglePartitionAllocation* single_partition_allocation, const ZVirtualMemory& vmem) {\n+  ZMemoryAllocation* const allocation = single_partition_allocation->allocation();\n+  ZPartition& partition = allocation->partition();\n+\n+  const size_t total_committed = allocation->harvested() + allocation->committed_capacity();\n+  const ZVirtualMemory total_committed_vmem = vmem.first_part(total_committed);\n+\n+  if (total_committed_vmem.size() > 0)  {\n+    \/\/ Map all the committed memory\n+    partition.map_memory(allocation, total_committed_vmem);\n+  }\n+}\n+\n+void ZPageAllocator::map_committed_multi_partition(ZMultiPartitionAllocation* multi_partition_allocation, const ZVirtualMemory& vmem) {\n+  ZVirtualMemory remaining = vmem;\n+  for (ZMemoryAllocation* const allocation : *multi_partition_allocation->allocations()) {\n+    assert(!allocation->commit_failed(), \"Sanity check\");\n+\n+    ZPartition& partition = allocation->partition();\n+\n+    \/\/ Split off the partial allocation's memory range\n+    const ZVirtualMemory to_vmem = remaining.shrink_from_front(allocation->size());\n+\n+    \/\/ Map the partial_allocation to partial_vmem\n+    partition.map_virtual_from_multi_partition(to_vmem);\n+  }\n+\n+  assert(remaining.size() == 0, \"all memory must be accounted for\");\n+}\n+\n+void ZPageAllocator::cleanup_failed_commit_single_partition(ZSinglePartitionAllocation* single_partition_allocation, const ZVirtualMemory& vmem) {\n+  ZMemoryAllocation* const allocation = single_partition_allocation->allocation();\n+\n+  assert(allocation->commit_failed(), \"Must have failed to commit\");\n+\n+  const size_t committed = allocation->committed_capacity();\n+  const ZVirtualMemory non_harvested_vmem = vmem.last_part(allocation->harvested());\n+  const ZVirtualMemory committed_vmem = non_harvested_vmem.first_part(committed);\n+  const ZVirtualMemory non_committed_vmem = non_harvested_vmem.last_part(committed);\n@@ -471,3 +1939,6 @@\n-  \/\/ Destroy page safely\n-  safe_destroy_page(page);\n-}\n+  if (committed_vmem.size() > 0) {\n+    \/\/ Register the committed and mapped memory. We insert the committed\n+    \/\/ memory into partial_vmems so that it will be inserted into the cache\n+    \/\/ in a subsequent step.\n+    allocation->partial_vmems()->append(committed_vmem);\n+  }\n@@ -475,8 +1946,4 @@\n-bool ZPageAllocator::should_defragment(const ZPage* page) const {\n-  \/\/ A small page can end up at a high address (second half of the address space)\n-  \/\/ if we've split a larger page or we have a constrained address space. To help\n-  \/\/ fight address space fragmentation we remap such pages to a lower address, if\n-  \/\/ a lower address is available.\n-  return page->type() == ZPageType::small &&\n-         page->start() >= to_zoffset(_virtual.reserved() \/ 2) &&\n-         page->start() > _virtual.lowest_available_address();\n+  \/\/ Free the virtual and physical memory we fetched to use but failed to commit\n+  ZPartition& partition = allocation->partition();\n+  partition.free_physical(non_committed_vmem);\n+  partition.free_virtual(non_committed_vmem);\n@@ -485,6 +1952,5 @@\n-ZPage* ZPageAllocator::defragment_page(ZPage* page) {\n-  \/\/ Harvest the physical memory (which is committed)\n-  ZPhysicalMemory pmem;\n-  ZPhysicalMemory& old_pmem = page->physical_memory();\n-  pmem.add_segments(old_pmem);\n-  old_pmem.remove_segments();\n+void ZPageAllocator::cleanup_failed_commit_multi_partition(ZMultiPartitionAllocation* multi_partition_allocation, const ZVirtualMemory& vmem) {\n+  ZVirtualMemory remaining = vmem;\n+  for (ZMemoryAllocation* const allocation : *multi_partition_allocation->allocations()) {\n+    \/\/ Split off the partial allocation's memory range\n+    const ZVirtualMemory partial_vmem = remaining.shrink_from_front(allocation->size());\n@@ -492,1 +1958,5 @@\n-  _unmapper->unmap_and_destroy_page(page);\n+    if (allocation->harvested() == allocation->size()) {\n+      \/\/ Everything is harvested, the mappings are already in the partial_vmems,\n+      \/\/ nothing to cleanup.\n+      continue;\n+    }\n@@ -494,2 +1964,4 @@\n-  \/\/ Allocate new virtual memory at a low address\n-  const ZVirtualMemory vmem = _virtual.alloc(pmem.size(), true \/* force_low_address *\/);\n+    const size_t committed = allocation->committed_capacity();\n+    const ZVirtualMemory non_harvested_vmem = vmem.last_part(allocation->harvested());\n+    const ZVirtualMemory committed_vmem = non_harvested_vmem.first_part(committed);\n+    const ZVirtualMemory non_committed_vmem = non_harvested_vmem.last_part(committed);\n@@ -497,3 +1969,1 @@\n-  \/\/ Create the new page and map it\n-  ZPage* new_page = new ZPage(ZPageType::small, vmem, pmem);\n-  map_page(new_page);\n+    ZPartition& partition = allocation->partition();\n@@ -501,2 +1971,6 @@\n-  \/\/ Update statistics\n-  ZStatInc(ZCounterDefragment);\n+    if (allocation->commit_failed()) {\n+      \/\/ Free the physical memory we failed to commit. Virtual memory is later\n+      \/\/ freed for the entire multi-partition allocation after all memory\n+      \/\/ allocations have been visited.\n+      partition.free_physical(non_committed_vmem);\n+    }\n@@ -504,2 +1978,4 @@\n-  return new_page;\n-}\n+    if (committed_vmem.size() == 0) {\n+      \/\/ Nothing committed, nothing more to cleanup\n+      continue;\n+    }\n@@ -507,4 +1983,2 @@\n-bool ZPageAllocator::is_alloc_allowed(size_t size) const {\n-  const size_t available = _current_max_capacity - _used - _claimed;\n-  return available >= size;\n-}\n+    \/\/ Remove the harvested part\n+    const ZVirtualMemory non_harvest_vmem = partial_vmem.last_part(allocation->harvested());\n@@ -512,5 +1986,1 @@\n-bool ZPageAllocator::alloc_page_common_inner(ZPageType type, size_t size, ZList<ZPage>* pages) {\n-  if (!is_alloc_allowed(size)) {\n-    \/\/ Out of memory\n-    return false;\n-  }\n+    ZArray<ZVirtualMemory>* const partial_vmems = allocation->partial_vmems();\n@@ -518,7 +1988,2 @@\n-  \/\/ Try allocate from the page cache\n-  ZPage* const page = _cache.alloc_page(type, size);\n-  if (page != nullptr) {\n-    \/\/ Success\n-    pages->insert_last(page);\n-    return true;\n-  }\n+    \/\/ Keep track of the start index\n+    const int start_index = partial_vmems->length();\n@@ -526,8 +1991,2 @@\n-  \/\/ Try increase capacity\n-  const size_t increased = increase_capacity(size);\n-  if (increased < size) {\n-    \/\/ Could not increase capacity enough to satisfy the allocation\n-    \/\/ completely. Flush the page cache to satisfy the remainder.\n-    const size_t remaining = size - increased;\n-    _cache.flush_for_allocation(remaining, pages);\n-  }\n+    \/\/ Claim virtual memory for the committed part\n+    const size_t claimed_virtual = partition.claim_virtual(committed, partial_vmems);\n@@ -535,3 +1994,4 @@\n-  \/\/ Success\n-  return true;\n-}\n+    \/\/ We are holding memory associated with this partition, and we do not\n+    \/\/ overcommit virtual memory claiming. So virtual memory must always be\n+    \/\/ available.\n+    assert(claimed_virtual == committed, \"must succeed\");\n@@ -539,5 +1999,1 @@\n-bool ZPageAllocator::alloc_page_common(ZPageAllocation* allocation) {\n-  const ZPageType type = allocation->type();\n-  const size_t size = allocation->size();\n-  const ZAllocationFlags flags = allocation->flags();\n-  ZList<ZPage>* const pages = allocation->pages();\n+    \/\/ Associate and map the physical memory with the partial vmems\n@@ -545,4 +2001,3 @@\n-  if (!alloc_page_common_inner(type, size, pages)) {\n-    \/\/ Out of memory\n-    return false;\n-  }\n+    ZVirtualMemory remaining_committed_vmem = committed_vmem;\n+    for (const ZVirtualMemory& to_vmem : partial_vmems->slice_back(start_index)) {\n+      const ZVirtualMemory from_vmem = remaining_committed_vmem.shrink_from_front(to_vmem.size());\n@@ -550,2 +2005,2 @@\n-  \/\/ Updated used statistics\n-  increase_used(size);\n+      \/\/ Copy physical mappings\n+      partition.copy_physical_segments_to_partition(to_vmem, from_vmem);\n@@ -553,3 +2008,3 @@\n-  \/\/ Success\n-  return true;\n-}\n+      \/\/ Map memory\n+      partition.map_virtual(to_vmem);\n+    }\n@@ -557,3 +2012,1 @@\n-static void check_out_of_memory_during_initialization() {\n-  if (!is_init_completed()) {\n-    vm_exit_during_initialization(\"java.lang.OutOfMemoryError\", \"Java heap too small\");\n+    assert(remaining_committed_vmem.size() == 0, \"all memory must be accounted for\");\n@@ -561,4 +2014,1 @@\n-}\n-bool ZPageAllocator::alloc_page_stall(ZPageAllocation* allocation) {\n-  ZStatTimer timer(ZCriticalPhaseAllocationStall);\n-  EventZAllocationStall event;\n+  assert(remaining.size() == 0, \"all memory must be accounted for\");\n@@ -567,2 +2017,3 @@\n-  \/\/ We can only block if the VM is fully initialized\n-  check_out_of_memory_during_initialization();\n+  \/\/ Free the unused virtual memory\n+  _virtual.insert_multi_partition(vmem);\n+}\n@@ -570,3 +2021,3 @@\n-  \/\/ Start asynchronous minor GC\n-  const ZDriverRequest request(GCCause::_z_allocation_stall, ZYoungGCThreads, 0);\n-  ZDriver::minor()->collect(request);\n+void ZPageAllocator::free_after_alloc_page_failed(ZPageAllocation* allocation) {\n+  \/\/ Send event for failed allocation\n+  allocation->send_event(false \/* successful *\/);\n@@ -574,2 +2025,1 @@\n-  \/\/ Wait for allocation to complete or fail\n-  const bool result = allocation->wait();\n+  ZLocker<ZLock> locker(&_lock);\n@@ -577,10 +2027,2 @@\n-  {\n-    \/\/ Guard deletion of underlying semaphore. This is a workaround for\n-    \/\/ a bug in sem_post() in glibc < 2.21, where it's not safe to destroy\n-    \/\/ the semaphore immediately after returning from sem_wait(). The\n-    \/\/ reason is that sem_post() can touch the semaphore after a waiting\n-    \/\/ thread have returned from sem_wait(). To avoid this race we are\n-    \/\/ forcing the waiting thread to acquire\/release the lock held by the\n-    \/\/ posting thread. https:\/\/sourceware.org\/bugzilla\/show_bug.cgi?id=12674\n-    ZLocker<ZLock> locker(&_lock);\n-  }\n+  \/\/ Free memory\n+  free_memory_alloc_failed(allocation);\n@@ -588,2 +2030,2 @@\n-  \/\/ Send event\n-  event.commit((u8)allocation->type(), allocation->size());\n+  \/\/ Keep track of usage\n+  decrease_used(allocation->size());\n@@ -591,1 +2033,5 @@\n-  return result;\n+  \/\/ Reset allocation for a potential retry\n+  allocation->reset_for_retry();\n+\n+  \/\/ Try satisfy stalled allocations\n+  satisfy_stalled();\n@@ -594,3 +2040,3 @@\n-bool ZPageAllocator::alloc_page_or_stall(ZPageAllocation* allocation) {\n-  {\n-    ZLocker<ZLock> locker(&_lock);\n+void ZPageAllocator::free_memory_alloc_failed(ZPageAllocation* allocation) {\n+  \/\/ The current max capacity may be decreased, store the value before freeing memory\n+  const size_t current_max_capacity_before = current_max_capacity();\n@@ -598,4 +2044,5 @@\n-    if (alloc_page_common(allocation)) {\n-      \/\/ Success\n-      return true;\n-    }\n+  if (allocation->is_multi_partition()) {\n+    free_memory_alloc_failed_multi_partition(allocation->multi_partition_allocation());\n+  } else {\n+    free_memory_alloc_failed_single_partition(allocation->single_partition_allocation());\n+  }\n@@ -603,5 +2050,1 @@\n-    \/\/ Failed\n-    if (allocation->flags().non_blocking()) {\n-      \/\/ Don't stall\n-      return false;\n-    }\n+  const size_t current_max_capacity_after = current_max_capacity();\n@@ -609,2 +2052,5 @@\n-    \/\/ Enqueue allocation request\n-    _stalled.insert_last(allocation);\n+  if (current_max_capacity_before != current_max_capacity_after) {\n+    log_error_p(gc)(\"Forced to lower max Java heap size from \"\n+                    \"%zuM(%.0f%%) to %zuM(%.0f%%)\",\n+                    current_max_capacity_before \/ M, percent_of(current_max_capacity_before, _max_capacity),\n+                    current_max_capacity_after \/ M, percent_of(current_max_capacity_after, _max_capacity));\n@@ -612,3 +2058,0 @@\n-\n-  \/\/ Stall\n-  return alloc_page_stall(allocation);\n@@ -617,2 +2060,3 @@\n-ZPage* ZPageAllocator::alloc_page_create(ZPageAllocation* allocation) {\n-  const size_t size = allocation->size();\n+void ZPageAllocator::free_memory_alloc_failed_single_partition(ZSinglePartitionAllocation* single_partition_allocation) {\n+  free_memory_alloc_failed(single_partition_allocation->allocation());\n+}\n@@ -620,8 +2064,3 @@\n-  \/\/ Allocate virtual memory. To make error handling a lot more straight\n-  \/\/ forward, we allocate virtual memory before destroying flushed pages.\n-  \/\/ Flushed pages are also unmapped and destroyed asynchronously, so we\n-  \/\/ can't immediately reuse that part of the address space anyway.\n-  const ZVirtualMemory vmem = _virtual.alloc(size, allocation->flags().low_address());\n-  if (vmem.is_null()) {\n-    log_error(gc)(\"Out of address space\");\n-    return nullptr;\n+void ZPageAllocator::free_memory_alloc_failed_multi_partition(ZMultiPartitionAllocation* multi_partition_allocation) {\n+  for (ZMemoryAllocation* allocation : *multi_partition_allocation->allocations()) {\n+    free_memory_alloc_failed(allocation);\n@@ -629,0 +2068,1 @@\n+}\n@@ -630,2 +2070,2 @@\n-  ZPhysicalMemory pmem;\n-  size_t flushed = 0;\n+void ZPageAllocator::free_memory_alloc_failed(ZMemoryAllocation* allocation) {\n+  ZPartition& partition = allocation->partition();\n@@ -633,4 +2073,2 @@\n-  \/\/ Harvest physical memory from flushed pages\n-  ZListRemoveIterator<ZPage> iter(allocation->pages());\n-  for (ZPage* page; iter.next(&page);) {\n-    flushed += page->size();\n+  partition.free_memory_alloc_failed(allocation);\n+}\n@@ -638,4 +2076,10 @@\n-    \/\/ Harvest flushed physical memory\n-    ZPhysicalMemory& fmem = page->physical_memory();\n-    pmem.add_segments(fmem);\n-    fmem.remove_segments();\n+ZPage* ZPageAllocator::create_page(ZPageAllocation* allocation, const ZVirtualMemory& vmem) {\n+  \/\/ We don't track generation usage when claiming capacity, because this page\n+  \/\/ could have been allocated by a thread that satisfies a stalling allocation.\n+  \/\/ The stalled thread can wake up and potentially realize that the page alloc\n+  \/\/ should be undone. If the alloc and the undo gets separated by a safepoint,\n+  \/\/ the generation statistics could se a decreasing used value between mark\n+  \/\/ start and mark end. At this point an allocation will be successful, so we\n+  \/\/ update the generation usage.\n+  const ZGenerationId id = allocation->age() == ZPageAge::old ? ZGenerationId::old : ZGenerationId::young;\n+  increase_used_generation(id, allocation->size());\n@@ -643,3 +2087,2 @@\n-    \/\/ Unmap and destroy page\n-    _unmapper->unmap_and_destroy_page(page);\n-  }\n+  const ZPageType type = allocation->type();\n+  const ZPageAge age = allocation->age();\n@@ -647,2 +2090,3 @@\n-  if (flushed > 0) {\n-    allocation->set_flushed(flushed);\n+  if (allocation->is_multi_partition()) {\n+    const ZMultiPartitionAllocation* const multi_partition_allocation = allocation->multi_partition_allocation();\n+    ZMultiPartitionTracker* const tracker = ZMultiPartitionTracker::create(multi_partition_allocation, vmem);\n@@ -650,3 +2094,1 @@\n-    \/\/ Update statistics\n-    ZStatInc(ZCounterPageCacheFlush, flushed);\n-    log_debug(gc, heap)(\"Page Cache Flushed: %zuM\", flushed \/ M);\n+    return new ZPage(type, age, vmem, tracker);\n@@ -655,8 +2097,2 @@\n-  \/\/ Allocate any remaining physical memory. Capacity and used has\n-  \/\/ already been adjusted, we just need to fetch the memory, which\n-  \/\/ is guaranteed to succeed.\n-  if (flushed < size) {\n-    const size_t remaining = size - flushed;\n-    allocation->set_committed(remaining);\n-    _physical.alloc(pmem, remaining);\n-  }\n+  const ZSinglePartitionAllocation* const single_partition_allocation = allocation->single_partition_allocation();\n+  const uint32_t partition_id = single_partition_allocation->allocation()->partition().numa_id();\n@@ -664,2 +2100,1 @@\n-  \/\/ Create new page\n-  return new ZPage(allocation->type(), vmem, pmem);\n+  return new ZPage(type, age, vmem, partition_id);\n@@ -668,6 +2103,5 @@\n-bool ZPageAllocator::is_alloc_satisfied(ZPageAllocation* allocation) const {\n-  \/\/ The allocation is immediately satisfied if the list of pages contains\n-  \/\/ exactly one page, with the type and size that was requested. However,\n-  \/\/ even if the allocation is immediately satisfied we might still want to\n-  \/\/ return false here to force the page to be remapped to fight address\n-  \/\/ space fragmentation.\n+void ZPageAllocator::prepare_memory_for_free(ZPage* page, ZArray<ZVirtualMemory>* vmems) {\n+  \/\/ Extract memory and destroy the page\n+  const ZVirtualMemory vmem = page->virtual_memory();\n+  const ZPageType page_type = page->type();\n+  const ZMultiPartitionTracker* const tracker = page->multi_partition_tracker();\n@@ -675,3 +2109,12 @@\n-  if (allocation->pages()->size() != 1) {\n-    \/\/ Not a single page\n-    return false;\n+  safe_destroy_page(page);\n+\n+  \/\/ Multi-partition memory is always remapped\n+  if (tracker != nullptr) {\n+    tracker->prepare_memory_for_free(vmem, vmems);\n+\n+    \/\/ Free the virtual memory\n+    _virtual.insert_multi_partition(vmem);\n+\n+    \/\/ Destroy the tracker\n+    ZMultiPartitionTracker::destroy(tracker);\n+    return;\n@@ -680,5 +2123,4 @@\n-  const ZPage* const page = allocation->pages()->first();\n-  if (page->type() != allocation->type() ||\n-      page->size() != allocation->size()) {\n-    \/\/ Wrong type or size\n-    return false;\n+  \/\/ Try to remap and defragment if page is large\n+  if (page_type == ZPageType::large) {\n+    remap_and_defragment(vmem, vmems);\n+    return;\n@@ -687,2 +2129,2 @@\n-  \/\/ Allocation immediately satisfied\n-  return true;\n+  \/\/ Leave the memory untouched\n+  vmems->append(vmem);\n@@ -691,5 +2133,2 @@\n-ZPage* ZPageAllocator::alloc_page_finalize(ZPageAllocation* allocation) {\n-  \/\/ Fast path\n-  if (is_alloc_satisfied(allocation)) {\n-    return allocation->pages()->remove_first();\n-  }\n+void ZPageAllocator::remap_and_defragment(const ZVirtualMemory& vmem, ZArray<ZVirtualMemory>* vmems_out) {\n+  ZPartition& partition = partition_from_vmem(vmem);\n@@ -697,5 +2136,4 @@\n-  \/\/ Slow path\n-  ZPage* const page = alloc_page_create(allocation);\n-  if (page == nullptr) {\n-    \/\/ Out of address space\n-    return nullptr;\n+  \/\/ If no lower address can be found, don't remap\/defrag\n+  if (_virtual.lowest_available_address(partition.numa_id()) > vmem.start()) {\n+    vmems_out->append(vmem);\n+    return;\n@@ -704,6 +2142,1 @@\n-  \/\/ Commit page\n-  if (commit_page(page)) {\n-    \/\/ Success\n-    map_page(page);\n-    return page;\n-  }\n+  ZStatInc(ZCounterDefragment);\n@@ -711,5 +2144,2 @@\n-  \/\/ Failed or partially failed. Split of any successfully committed\n-  \/\/ part of the page into a new page and insert it into list of pages,\n-  \/\/ so that it will be re-inserted into the page cache.\n-  ZPage* const committed_page = page->split_committed();\n-  destroy_page(page);\n+  \/\/ Synchronously unmap the virtual memory\n+  partition.unmap_virtual(vmem);\n@@ -717,4 +2147,3 @@\n-  if (committed_page != nullptr) {\n-    map_page(committed_page);\n-    allocation->pages()->insert_last(committed_page);\n-  }\n+  \/\/ Stash segments\n+  ZArray<zbacking_index> stash(vmem.granule_count());\n+  _physical.stash_segments(vmem, &stash);\n@@ -722,2 +2151,3 @@\n-  return nullptr;\n-}\n+  \/\/ Shuffle vmem - put new vmems in vmems_out\n+  const int start_index = vmems_out->length();\n+  partition.free_and_claim_virtual_from_low_many(vmem, vmems_out);\n@@ -725,2 +2155,3 @@\n-ZPage* ZPageAllocator::alloc_page(ZPageType type, size_t size, ZAllocationFlags flags, ZPageAge age) {\n-  EventZPageAllocation event;\n+  \/\/ The output array may contain results from other defragmentations as well,\n+  \/\/ so we only operate on the result(s) we just got.\n+  ZArraySlice<ZVirtualMemory> defragmented_vmems = vmems_out->slice_back(start_index);\n@@ -728,12 +2159,2 @@\n-retry:\n-  ZPageAllocation allocation(type, size, flags);\n-\n-  \/\/ Allocate one or more pages from the page cache. If the allocation\n-  \/\/ succeeds but the returned pages don't cover the complete allocation,\n-  \/\/ then finalize phase is allowed to allocate the remaining memory\n-  \/\/ directly from the physical memory manager. Note that this call might\n-  \/\/ block in a safepoint if the non-blocking flag is not set.\n-  if (!alloc_page_or_stall(&allocation)) {\n-    \/\/ Out of memory\n-    return nullptr;\n-  }\n+  \/\/ Restore segments\n+  _physical.restore_segments(defragmented_vmems, stash);\n@@ -741,6 +2162,4 @@\n-  ZPage* const page = alloc_page_finalize(&allocation);\n-  if (page == nullptr) {\n-    \/\/ Failed to commit or map. Clean up and retry, in the hope that\n-    \/\/ we can still allocate by flushing the page cache (more aggressively).\n-    free_pages_alloc_failed(&allocation);\n-    goto retry;\n+  \/\/ Map and pre-touch\n+  for (const ZVirtualMemory& claimed_vmem : defragmented_vmems) {\n+    partition.map_virtual(claimed_vmem);\n+    pretouch_memory(claimed_vmem.start(), claimed_vmem.size());\n@@ -748,0 +2167,1 @@\n+}\n@@ -749,5 +2169,2 @@\n-  \/\/ The generation's used is tracked here when the page is handed out\n-  \/\/ to the allocating thread. The overall heap \"used\" is tracked in\n-  \/\/ the lower-level allocation code.\n-  const ZGenerationId id = age == ZPageAge::old ? ZGenerationId::old : ZGenerationId::young;\n-  increase_used_generation(id, size);\n+void ZPageAllocator::free_memory(ZArray<ZVirtualMemory>* vmems) {\n+  ZLocker<ZLock> locker(&_lock);\n@@ -755,9 +2172,3 @@\n-  \/\/ Reset page. This updates the page's sequence number and must\n-  \/\/ be done after we potentially blocked in a safepoint (stalled)\n-  \/\/ where the global sequence number was updated.\n-  page->reset(age);\n-  page->reset_top_for_allocation();\n-  page->reset_livemap();\n-  if (age == ZPageAge::old) {\n-    page->remset_alloc();\n-  }\n+  \/\/ Free the vmems\n+  for (const ZVirtualMemory vmem : *vmems) {\n+    ZPartition& partition = partition_from_vmem(vmem);\n@@ -765,8 +2176,2 @@\n-  \/\/ Update allocation statistics. Exclude gc relocations to avoid\n-  \/\/ artificial inflation of the allocation rate during relocation.\n-  if (!flags.gc_relocation() && is_init_completed()) {\n-    \/\/ Note that there are two allocation rate counters, which have\n-    \/\/ different purposes and are sampled at different frequencies.\n-    ZStatInc(ZCounterMutatorAllocationRate, size);\n-    ZStatMutatorAllocRate::sample_allocation(size);\n-  }\n+    \/\/ Free the vmem\n+    partition.free_memory(vmem);\n@@ -774,3 +2179,3 @@\n-  \/\/ Send event\n-  event.commit((u8)type, size, allocation.flushed(), allocation.committed(),\n-               page->physical_memory().nsegments(), flags.non_blocking());\n+    \/\/ Keep track of usage\n+    decrease_used(vmem.size());\n+  }\n@@ -778,1 +2183,2 @@\n-  return page;\n+  \/\/ Try satisfy stalled allocations\n+  satisfy_stalled();\n@@ -789,1 +2195,1 @@\n-    if (!alloc_page_common(allocation)) {\n+    if (!claim_capacity(allocation)) {\n@@ -794,0 +2200,3 @@\n+    \/\/ Keep track of usage\n+    increase_used(allocation->size());\n+\n@@ -802,15 +2211,2 @@\n-ZPage* ZPageAllocator::prepare_to_recycle(ZPage* page, bool allow_defragment) {\n-  \/\/ Make sure we have a page that is safe to recycle\n-  ZPage* const to_recycle = _safe_recycle.register_and_clone_if_activated(page);\n-\n-  \/\/ Defragment the page before recycle if allowed and needed\n-  if (allow_defragment && should_defragment(to_recycle)) {\n-    return defragment_page(to_recycle);\n-  }\n-\n-  \/\/ Remove the remset before recycling\n-  if (to_recycle->is_old() && to_recycle == page) {\n-    to_recycle->remset_delete();\n-  }\n-\n-  return to_recycle;\n+bool ZPageAllocator::is_multi_partition_enabled() const {\n+  return _virtual.is_multi_partition_enabled();\n@@ -819,6 +2215,2 @@\n-void ZPageAllocator::recycle_page(ZPage* page) {\n-  \/\/ Set time when last used\n-  page->set_last_used();\n-\n-  \/\/ Cache page\n-  _cache.free_page(page);\n+const ZPartition& ZPageAllocator::partition_from_partition_id(uint32_t numa_id) const {\n+  return _partitions.get(numa_id);\n@@ -827,18 +2219,2 @@\n-void ZPageAllocator::free_page(ZPage* page, bool allow_defragment) {\n-  const ZGenerationId generation_id = page->generation_id();\n-\n-  \/\/ Prepare page for recycling before taking the lock\n-  ZPage* const to_recycle = prepare_to_recycle(page, allow_defragment);\n-\n-  ZLocker<ZLock> locker(&_lock);\n-\n-  \/\/ Update used statistics\n-  const size_t size = to_recycle->size();\n-  decrease_used(size);\n-  decrease_used_generation(generation_id, size);\n-\n-  \/\/ Free page\n-  recycle_page(to_recycle);\n-\n-  \/\/ Try satisfy stalled allocations\n-  satisfy_stalled();\n+ZPartition& ZPageAllocator::partition_from_partition_id(uint32_t numa_id) {\n+  return _partitions.get(numa_id);\n@@ -847,14 +2223,3 @@\n-void ZPageAllocator::free_pages(const ZArray<ZPage*>* pages) {\n-  ZArray<ZPage*> to_recycle_pages;\n-\n-  size_t young_size = 0;\n-  size_t old_size = 0;\n-\n-  \/\/ Prepare pages for recycling before taking the lock\n-  ZArrayIterator<ZPage*> pages_iter(pages);\n-  for (ZPage* page; pages_iter.next(&page);) {\n-    if (page->is_young()) {\n-      young_size += page->size();\n-    } else {\n-      old_size += page->size();\n-    }\n+ZPartition& ZPageAllocator::partition_from_vmem(const ZVirtualMemory& vmem) {\n+  return partition_from_partition_id(_virtual.lookup_partition_id(vmem));\n+}\n@@ -862,2 +2227,2 @@\n-    \/\/ Prepare to recycle\n-    ZPage* const to_recycle = prepare_to_recycle(page, true \/* allow_defragment *\/);\n+size_t ZPageAllocator::sum_available() const {\n+  size_t total = 0;\n@@ -865,2 +2230,3 @@\n-    \/\/ Register for recycling\n-    to_recycle_pages.push(to_recycle);\n+  ZPartitionConstIterator iter = partition_iterator();\n+  for (const ZPartition* partition; iter.next(&partition);) {\n+    total += partition->available();\n@@ -869,1 +2235,2 @@\n-  ZLocker<ZLock> locker(&_lock);\n+  return total;\n+}\n@@ -871,4 +2238,3 @@\n-  \/\/ Update used statistics\n-  decrease_used(young_size + old_size);\n-  decrease_used_generation(ZGenerationId::young, young_size);\n-  decrease_used_generation(ZGenerationId::old, old_size);\n+void ZPageAllocator::increase_used(size_t size) {\n+  \/\/ Update atomically since we have concurrent readers\n+  const size_t used = Atomic::add(&_used, size);\n@@ -876,4 +2242,5 @@\n-  \/\/ Free pages\n-  ZArrayIterator<ZPage*> iter(&to_recycle_pages);\n-  for (ZPage* page; iter.next(&page);) {\n-    recycle_page(page);\n+  \/\/ Update used high\n+  for (auto& stats : _collection_stats) {\n+    if (used > stats._used_high) {\n+      stats._used_high = used;\n+    }\n@@ -881,3 +2248,0 @@\n-\n-  \/\/ Try satisfy stalled allocations\n-  satisfy_stalled();\n@@ -886,11 +2250,3 @@\n-void ZPageAllocator::free_pages_alloc_failed(ZPageAllocation* allocation) {\n-  \/\/ The page(s) in the allocation are either taken from the cache or a newly\n-  \/\/ created, mapped and commited ZPage. These page(s) have not been inserted in\n-  \/\/ the page table, nor allocated a remset, so prepare_to_recycle is not required.\n-  ZLocker<ZLock> locker(&_lock);\n-\n-  \/\/ Only decrease the overall used and not the generation used,\n-  \/\/ since the allocation failed and generation used wasn't bumped.\n-  decrease_used(allocation->size());\n-\n-  size_t freed = 0;\n+void ZPageAllocator::decrease_used(size_t size) {\n+  \/\/ Update atomically since we have concurrent readers\n+  const size_t used = Atomic::sub(&_used, size);\n@@ -898,5 +2254,5 @@\n-  \/\/ Free any allocated\/flushed pages\n-  ZListRemoveIterator<ZPage> iter(allocation->pages());\n-  for (ZPage* page; iter.next(&page);) {\n-    freed += page->size();\n-    recycle_page(page);\n+  \/\/ Update used low\n+  for (auto& stats : _collection_stats) {\n+    if (used < stats._used_low) {\n+      stats._used_low = used;\n+    }\n@@ -904,7 +2260,0 @@\n-\n-  \/\/ Adjust capacity and used to reflect the failed capacity increase\n-  const size_t remaining = allocation->size() - freed;\n-  decrease_capacity(remaining, true \/* set_max_capacity *\/);\n-\n-  \/\/ Try satisfy stalled allocations\n-  satisfy_stalled();\n@@ -913,5 +2262,4 @@\n-size_t ZPageAllocator::uncommit(uint64_t* timeout, uintx delay) {\n-  \/\/ We need to join the suspendible thread set while manipulating capacity and\n-  \/\/ used, to make sure GC safepoints will have a consistent view.\n-  ZList<ZPage> pages;\n-  size_t flushed;\n+void ZPageAllocator::safe_destroy_page(ZPage* page) {\n+  \/\/ Destroy page safely\n+  _safe_destroy.schedule_delete(page);\n+}\n@@ -919,3 +2267,4 @@\n-  {\n-    SuspendibleThreadSetJoiner sts_joiner;\n-    ZLocker<ZLock> locker(&_lock);\n+void ZPageAllocator::free_page(ZPage* page) {\n+  \/\/ Extract the id from the page\n+  const ZGenerationId id = page->generation_id();\n+  const size_t size = page->size();\n@@ -923,7 +2272,3 @@\n-    \/\/ Never uncommit below min capacity. We flush out and uncommit chunks at\n-    \/\/ a time (~0.8% of the max capacity, but at least one granule and at most\n-    \/\/ 256M), in case demand for memory increases while we are uncommitting.\n-    const size_t retain = MAX2(_used, _min_capacity);\n-    const size_t release = _capacity - retain;\n-    const size_t limit = MIN2(align_up(_current_max_capacity >> 7, ZGranuleSize), 256 * M);\n-    const size_t flush = MIN2(release, limit);\n+  \/\/ Extract vmems and destroy the page\n+  ZArray<ZVirtualMemory> vmems;\n+  prepare_memory_for_free(page, &vmems);\n@@ -931,6 +2276,2 @@\n-    \/\/ Flush pages to uncommit\n-    flushed = _cache.flush_for_uncommit(flush, &pages, timeout, delay);\n-    if (flushed == 0) {\n-      \/\/ Nothing flushed\n-      return 0;\n-    }\n+  \/\/ Updated used statistics\n+  decrease_used_generation(id, size);\n@@ -938,3 +2279,3 @@\n-    \/\/ Record flushed pages as claimed\n-    Atomic::add(&_claimed, flushed);\n-  }\n+  \/\/ Free the extracted vmems\n+  free_memory(&vmems);\n+}\n@@ -942,7 +2283,6 @@\n-  \/\/ Unmap, uncommit, and destroy flushed pages\n-  ZListRemoveIterator<ZPage> iter(&pages);\n-  for (ZPage* page; iter.next(&page);) {\n-    unmap_page(page);\n-    uncommit_page(page);\n-    destroy_page(page);\n-  }\n+void ZPageAllocator::free_pages(ZGenerationId id, const ZArray<ZPage*>* pages) {\n+  \/\/ Prepare memory from pages to be cached\n+  ZArray<ZVirtualMemory> vmems;\n+  for (ZPage* page : *pages) {\n+    assert(page->generation_id() == id, \"All pages must be from the same generation\");\n+    const size_t size = page->size();\n@@ -950,3 +2290,2 @@\n-  {\n-    SuspendibleThreadSetJoiner sts_joiner;\n-    ZLocker<ZLock> locker(&_lock);\n+    \/\/ Extract vmems and destroy the page\n+    prepare_memory_for_free(page, &vmems);\n@@ -954,3 +2293,2 @@\n-    \/\/ Adjust claimed and capacity to reflect the uncommit\n-    Atomic::sub(&_claimed, flushed);\n-    decrease_capacity(flushed, false \/* set_max_capacity *\/);\n+    \/\/ Updated used statistics\n+    decrease_used_generation(id, size);\n@@ -959,1 +2297,2 @@\n-  return flushed;\n+  \/\/ Free the extracted vmems\n+  free_memory(&vmems);\n@@ -970,8 +2309,0 @@\n-void ZPageAllocator::enable_safe_recycle() const {\n-  _safe_recycle.activate();\n-}\n-\n-void ZPageAllocator::disable_safe_recycle() const {\n-  _safe_recycle.deactivate();\n-}\n-\n@@ -1048,0 +2379,8 @@\n+ZPartitionConstIterator ZPageAllocator::partition_iterator() const {\n+  return ZPartitionConstIterator(&_partitions);\n+}\n+\n+ZPartitionIterator ZPageAllocator::partition_iterator() {\n+  return ZPartitionIterator(&_partitions);\n+}\n+\n@@ -1049,2 +2388,81 @@\n-  tc->do_thread(_unmapper);\n-  tc->do_thread(_uncommitter);\n+  ZPartitionConstIterator iter = partition_iterator();\n+  for (const ZPartition* partition; iter.next(&partition);) {\n+    partition->threads_do(tc);\n+  }\n+}\n+\n+void ZPageAllocator::print_on(outputStream* st) const {\n+  ZLocker<ZLock> lock(&_lock);\n+  print_on_inner(st);\n+}\n+\n+static bool try_lock_on_error(ZLock* lock) {\n+  if (VMError::is_error_reported() && VMError::is_error_reported_in_current_thread()) {\n+    return lock->try_lock();\n+  }\n+\n+  lock->lock();\n+\n+  return true;\n+}\n+\n+void ZPageAllocator::print_extended_on_error(outputStream* st) const {\n+  st->print_cr(\"ZMappedCache:\");\n+\n+  streamIndentor indentor(st, 1);\n+\n+  if (!try_lock_on_error(&_lock)) {\n+    \/\/ We can't print without taking the lock since printing the contents of\n+    \/\/ the cache requires iterating over the nodes in the cache's tree, which\n+    \/\/ is not thread-safe.\n+    st->print_cr(\"<Skipped>\");\n+\n+    return;\n+  }\n+\n+  \/\/ Print each partition's cache content\n+  ZPartitionConstIterator iter = partition_iterator();\n+  for (const ZPartition* partition; iter.next(&partition);) {\n+    partition->print_extended_on_error(st);\n+  }\n+\n+  _lock.unlock();\n+}\n+\n+void ZPageAllocator::print_on_error(outputStream* st) const {\n+  const bool locked = try_lock_on_error(&_lock);\n+\n+  if (!locked) {\n+    st->print_cr(\"<Without lock>\");\n+  }\n+\n+  \/\/ Print information even though we have not successfully taken the lock.\n+  \/\/ This is thread-safe, but may produce inconsistent results.\n+  print_on_inner(st);\n+\n+  if (locked) {\n+    _lock.unlock();\n+  }\n+}\n+\n+void ZPageAllocator::print_on_inner(outputStream* st) const {\n+  \/\/ Print total usage\n+  st->print(\"ZHeap\");\n+  st->fill_to(17);\n+  st->print_cr(\"used %zuM, capacity %zuM, max capacity %zuM\",\n+               used() \/ M, capacity() \/ M, max_capacity() \/ M);\n+\n+  \/\/ Print per-partition\n+\n+  streamIndentor indentor(st, 1);\n+\n+  if (_partitions.count() == 1) {\n+    \/\/ The summary printing is redundant if we only have one partition\n+    _partitions.get(0).print_cache_on(st);\n+    return;\n+  }\n+\n+  ZPartitionConstIterator iter = partition_iterator();\n+  for (const ZPartition* partition; iter.next(&partition);) {\n+    partition->print_on(st);\n+  }\n@@ -1056,4 +2474,8 @@\n-  do {\n-    flushed = uncommit(&timeout, 0);\n-    uncommitted += flushed;\n-  } while (flushed > 0);\n+  ZPartitionIterator iter = partition_iterator();\n+  for (ZPartition* partition; iter.next(&partition);) {\n+    partition->_cache.reset_min();\n+    do {\n+      flushed = partition->uncommit(&timeout, 0);\n+      uncommitted += flushed;\n+    } while (flushed > 0);\n+  }\n","filename":"src\/hotspot\/share\/gc\/z\/zPageAllocator.cpp","additions":2056,"deletions":634,"binary":false,"changes":2690,"status":"modified"},{"patch":"@@ -2,1 +2,1 @@\n- * Copyright (c) 2015, 2024, Oracle and\/or its affiliates. All rights reserved.\n+ * Copyright (c) 2015, 2025, Oracle and\/or its affiliates. All rights reserved.\n@@ -27,0 +27,1 @@\n+#include \"gc\/z\/zAddress.hpp\"\n@@ -29,0 +30,2 @@\n+#include \"gc\/z\/zGenerationId.hpp\"\n+#include \"gc\/z\/zGranuleMap.hpp\"\n@@ -31,0 +34,2 @@\n+#include \"gc\/z\/zMappedCache.hpp\"\n+#include \"gc\/z\/zPage.hpp\"\n@@ -32,2 +37,1 @@\n-#include \"gc\/z\/zPageCache.hpp\"\n-#include \"gc\/z\/zPhysicalMemory.hpp\"\n+#include \"gc\/z\/zPhysicalMemoryManager.hpp\"\n@@ -36,1 +40,4 @@\n-#include \"gc\/z\/zVirtualMemory.hpp\"\n+#include \"gc\/z\/zUncommitter.hpp\"\n+#include \"gc\/z\/zValue.hpp\"\n+#include \"gc\/z\/zVirtualMemoryManager.hpp\"\n+#include \"utilities\/ostream.hpp\"\n@@ -40,0 +47,2 @@\n+class ZMemoryAllocation;\n+class ZMultiPartitionAllocation;\n@@ -43,0 +52,3 @@\n+class ZSegmentStash;\n+class ZSinglePartitionAllocation;\n+class ZVirtualMemory;\n@@ -44,3 +56,4 @@\n-class ZUncommitter;\n-class ZUnmapper;\n-class ZSafePageRecycle {\n+class ZPartition {\n+  friend class VMStructs;\n+  friend class ZPageAllocator;\n+\n@@ -49,2 +62,24 @@\n-  ZPageAllocator*        _page_allocator;\n-  ZActivatedArray<ZPage> _unsafe_to_recycle;\n+  ZPageAllocator* const _page_allocator;\n+  ZMappedCache          _cache;\n+  ZUncommitter          _uncommitter;\n+  const size_t          _min_capacity;\n+  const size_t          _max_capacity;\n+  volatile size_t       _current_max_capacity;\n+  volatile size_t       _capacity;\n+  volatile size_t       _claimed;\n+  size_t                _used;\n+  double                _last_commit;\n+  double                _last_uncommit;\n+  size_t                _to_uncommit;\n+  const uint32_t        _numa_id;\n+\n+  const ZVirtualMemoryManager& virtual_memory_manager() const;\n+  ZVirtualMemoryManager& virtual_memory_manager();\n+\n+  const ZPhysicalMemoryManager& physical_memory_manager() const;\n+  ZPhysicalMemoryManager& physical_memory_manager();\n+\n+  void verify_virtual_memory_multi_partition_association(const ZVirtualMemory& vmem) const NOT_DEBUG_RETURN;\n+  void verify_virtual_memory_association(const ZVirtualMemory& vmem, bool check_multi_partition = false) const NOT_DEBUG_RETURN;\n+  void verify_virtual_memory_association(const ZArray<ZVirtualMemory>* vmems) const NOT_DEBUG_RETURN;\n+  void verify_memory_allocation_association(const ZMemoryAllocation* allocation) const NOT_DEBUG_RETURN;\n@@ -53,1 +88,18 @@\n-  ZSafePageRecycle(ZPageAllocator* page_allocator);\n+  ZPartition(uint32_t numa_id, ZPageAllocator* page_allocator);\n+\n+  uint32_t numa_id() const;\n+\n+  size_t available() const;\n+\n+  size_t increase_capacity(size_t size);\n+  void decrease_capacity(size_t size, bool set_max_capacity);\n+\n+  void increase_used(size_t size);\n+  void decrease_used(size_t size);\n+\n+  void free_memory(const ZVirtualMemory& vmem);\n+\n+  void claim_from_cache_or_increase_capacity(ZMemoryAllocation* allocation);\n+  bool claim_capacity(ZMemoryAllocation* allocation);\n+\n+  size_t uncommit(uint64_t* timeout, uintx delay);\n@@ -55,2 +107,1 @@\n-  void activate();\n-  void deactivate();\n+  void sort_segments_physical(const ZVirtualMemory& vmem);\n@@ -58,1 +109,35 @@\n-  ZPage* register_and_clone_if_activated(ZPage* page);\n+  void claim_physical(const ZVirtualMemory& vmem);\n+  void free_physical(const ZVirtualMemory& vmem);\n+  size_t commit_physical(const ZVirtualMemory& vmem);\n+  size_t uncommit_physical(const ZVirtualMemory& vmem);\n+\n+  void map_virtual(const ZVirtualMemory& vmem);\n+  void unmap_virtual(const ZVirtualMemory& vmem);\n+\n+  void map_virtual_from_multi_partition(const ZVirtualMemory& vmem);\n+  void unmap_virtual_from_multi_partition(const ZVirtualMemory& vmem);\n+\n+  ZVirtualMemory claim_virtual(size_t size);\n+  size_t claim_virtual(size_t size, ZArray<ZVirtualMemory>* vmems_out);\n+  void free_virtual(const ZVirtualMemory& vmem);\n+\n+  void free_and_claim_virtual_from_low_many(const ZVirtualMemory& vmem, ZArray<ZVirtualMemory>* vmems_out);\n+  ZVirtualMemory free_and_claim_virtual_from_low_exact_or_many(size_t size, ZArray<ZVirtualMemory>* vmems_in_out);\n+\n+  bool prime(ZWorkers* workers, size_t size);\n+\n+  ZVirtualMemory prepare_harvested_and_claim_virtual(ZMemoryAllocation* allocation);\n+\n+  void copy_physical_segments_to_partition(const ZVirtualMemory& at, const ZVirtualMemory& from);\n+  void copy_physical_segments_from_partition(const ZVirtualMemory& at, const ZVirtualMemory& to);\n+\n+  void commit_increased_capacity(ZMemoryAllocation* allocation, const ZVirtualMemory& vmem);\n+  void map_memory(ZMemoryAllocation* allocation, const ZVirtualMemory& vmem);\n+\n+  void free_memory_alloc_failed(ZMemoryAllocation* allocation);\n+\n+  void threads_do(ThreadClosure* tc) const;\n+\n+  void print_on(outputStream* st) const;\n+  void print_cache_on(outputStream* st) const;\n+  void print_extended_on_error(outputStream* st) const;\n@@ -61,0 +146,3 @@\n+using ZPartitionIterator = ZPerNUMAIterator<ZPartition>;\n+using ZPartitionConstIterator = ZPerNUMAConstIterator<ZPartition>;\n+\n@@ -63,1 +151,2 @@\n-  friend class ZUnmapper;\n+  friend class ZMultiPartitionTracker;\n+  friend class ZPartition;\n@@ -67,12 +156,7 @@\n-  mutable ZLock              _lock;\n-  ZPageCache                 _cache;\n-  ZVirtualMemoryManager      _virtual;\n-  ZPhysicalMemoryManager     _physical;\n-  const size_t               _min_capacity;\n-  const size_t               _initial_capacity;\n-  const size_t               _max_capacity;\n-  volatile size_t            _current_max_capacity;\n-  volatile size_t            _capacity;\n-  volatile size_t            _claimed;\n-  volatile size_t            _used;\n-  size_t                     _used_generations[2];\n+  mutable ZLock               _lock;\n+  ZVirtualMemoryManager       _virtual;\n+  ZPhysicalMemoryManager      _physical;\n+  const size_t                _min_capacity;\n+  const size_t                _max_capacity;\n+  volatile size_t             _used;\n+  volatile size_t             _used_generations[2];\n@@ -80,9 +164,7 @@\n-    size_t                   _used_high;\n-    size_t                   _used_low;\n-  } _collection_stats[2];\n-  ZList<ZPageAllocation>     _stalled;\n-  ZUnmapper*                 _unmapper;\n-  ZUncommitter*              _uncommitter;\n-  mutable ZSafeDelete<ZPage> _safe_destroy;\n-  mutable ZSafePageRecycle   _safe_recycle;\n-  bool                       _initialized;\n+    size_t _used_high;\n+    size_t _used_low;\n+  }                           _collection_stats[2];\n+  ZPerNUMA<ZPartition>        _partitions;\n+  ZList<ZPageAllocation>      _stalled;\n+  mutable ZSafeDelete<ZPage>  _safe_destroy;\n+  bool                        _initialized;\n@@ -90,2 +172,2 @@\n-  size_t increase_capacity(size_t size);\n-  void decrease_capacity(size_t size, bool set_max_capacity);\n+  bool alloc_page_stall(ZPageAllocation* allocation);\n+  ZPage* alloc_page_inner(ZPageAllocation* allocation);\n@@ -93,2 +175,4 @@\n-  void increase_used(size_t size);\n-  void decrease_used(size_t size);\n+  bool claim_capacity_or_stall(ZPageAllocation* allocation);\n+  bool claim_capacity(ZPageAllocation* allocation);\n+  bool claim_capacity_single_partition(ZSinglePartitionAllocation* single_partition_allocation, uint32_t partition_id);\n+  void claim_capacity_multi_partition(ZMultiPartitionAllocation* multi_partition_allocation, uint32_t start_partition);\n@@ -96,2 +180,1 @@\n-  void increase_used_generation(ZGenerationId id, size_t size);\n-  void decrease_used_generation(ZGenerationId id, size_t size);\n+  ZVirtualMemory satisfied_from_cache_vmem(const ZPageAllocation* allocation) const;\n@@ -99,2 +182,3 @@\n-  bool commit_page(ZPage* page);\n-  void uncommit_page(ZPage* page);\n+  ZVirtualMemory claim_virtual_memory(ZPageAllocation* allocation);\n+  ZVirtualMemory claim_virtual_memory_single_partition(ZSinglePartitionAllocation* single_partition_allocation);\n+  ZVirtualMemory claim_virtual_memory_multi_partition(ZMultiPartitionAllocation* multi_partition_allocation);\n@@ -102,2 +186,1 @@\n-  void map_page(const ZPage* page) const;\n-  void unmap_page(const ZPage* page) const;\n+  void copy_claimed_physical_multi_partition(ZMultiPartitionAllocation* multi_partition_allocation, const ZVirtualMemory& vmem);\n@@ -105,1 +188,4 @@\n-  void destroy_page(ZPage* page);\n+  void claim_physical_for_increased_capacity(ZPageAllocation* allocation, const ZVirtualMemory& vmem);\n+  void claim_physical_for_increased_capacity_single_partition(ZSinglePartitionAllocation* allocation, const ZVirtualMemory& vmem);\n+  void claim_physical_for_increased_capacity_multi_partition(const ZMultiPartitionAllocation* multi_partition_allocation, const ZVirtualMemory& vmem);\n+  void claim_physical_for_increased_capacity(ZMemoryAllocation* allocation, const ZVirtualMemory& vmem);\n@@ -107,2 +193,3 @@\n-  bool should_defragment(const ZPage* page) const;\n-  ZPage* defragment_page(ZPage* page);\n+  bool commit_and_map(ZPageAllocation* allocation, const ZVirtualMemory& vmem);\n+  bool commit_and_map_single_partition(ZSinglePartitionAllocation* single_partition_allocation, const ZVirtualMemory& vmem);\n+  bool commit_and_map_multi_partition(ZMultiPartitionAllocation* multi_partition_allocation, const ZVirtualMemory& vmem);\n@@ -110,1 +197,3 @@\n-  bool is_alloc_allowed(size_t size) const;\n+  void commit(ZMemoryAllocation* allocation, const ZVirtualMemory& vmem);\n+  bool commit_single_partition(ZSinglePartitionAllocation* single_partition_allocation, const ZVirtualMemory& vmem);\n+  bool commit_multi_partition(ZMultiPartitionAllocation* multi_partition_allocation, const ZVirtualMemory& vmem);\n@@ -112,8 +201,20 @@\n-  bool alloc_page_common_inner(ZPageType type, size_t size, ZList<ZPage>* pages);\n-  bool alloc_page_common(ZPageAllocation* allocation);\n-  bool alloc_page_stall(ZPageAllocation* allocation);\n-  bool alloc_page_or_stall(ZPageAllocation* allocation);\n-  bool is_alloc_satisfied(ZPageAllocation* allocation) const;\n-  ZPage* alloc_page_create(ZPageAllocation* allocation);\n-  ZPage* alloc_page_finalize(ZPageAllocation* allocation);\n-  void free_pages_alloc_failed(ZPageAllocation* allocation);\n+  void unmap_harvested_multi_partition(ZMultiPartitionAllocation* multi_partition_allocation);\n+\n+  void map_committed_single_partition(ZSinglePartitionAllocation* single_partition_allocation, const ZVirtualMemory& vmem);\n+  void map_committed_multi_partition(ZMultiPartitionAllocation* multi_partition_allocation, const ZVirtualMemory& vmem);\n+\n+  void cleanup_failed_commit_single_partition(ZSinglePartitionAllocation* single_partition_allocation, const ZVirtualMemory& vmem);\n+  void cleanup_failed_commit_multi_partition(ZMultiPartitionAllocation* multi_partition_allocation, const ZVirtualMemory& vmem);\n+\n+  void free_after_alloc_page_failed(ZPageAllocation* allocation);\n+\n+  void free_memory_alloc_failed(ZPageAllocation* allocation);\n+  void free_memory_alloc_failed_single_partition(ZSinglePartitionAllocation* single_partition_allocation);\n+  void free_memory_alloc_failed_multi_partition(ZMultiPartitionAllocation* multi_partition_allocation);\n+  void free_memory_alloc_failed(ZMemoryAllocation* allocation);\n+\n+  ZPage* create_page(ZPageAllocation* allocation, const ZVirtualMemory& vmem);\n+\n+  void prepare_memory_for_free(ZPage* page, ZArray<ZVirtualMemory>* vmems);\n+  void remap_and_defragment(const ZVirtualMemory& vmem, ZArray<ZVirtualMemory>* vmems_out);\n+  void free_memory(ZArray<ZVirtualMemory>* vmems);\n@@ -123,1 +224,10 @@\n-  size_t uncommit(uint64_t* timeout, uintx delay);\n+  bool is_multi_partition_enabled() const;\n+\n+  const ZPartition& partition_from_partition_id(uint32_t partition_id) const;\n+  ZPartition&       partition_from_partition_id(uint32_t partition_id);\n+  ZPartition&       partition_from_vmem(const ZVirtualMemory& vmem);\n+\n+  size_t sum_available() const;\n+\n+  void increase_used(size_t size);\n+  void decrease_used(size_t size);\n@@ -128,0 +238,2 @@\n+  void print_on_inner(outputStream* st) const;\n+\n@@ -138,1 +250,0 @@\n-  size_t initial_capacity() const;\n@@ -142,0 +253,1 @@\n+  size_t current_max_capacity() const;\n@@ -147,1 +259,4 @@\n-  void promote_used(size_t size);\n+  void increase_used_generation(ZGenerationId id, size_t size);\n+  void decrease_used_generation(ZGenerationId id, size_t size);\n+\n+  void promote_used(const ZPage* from, const ZPage* to);\n@@ -154,4 +269,2 @@\n-  ZPage* prepare_to_recycle(ZPage* page, bool allow_defragment);\n-  void recycle_page(ZPage* page);\n-  void free_page(ZPage* page, bool allow_defragment);\n-  void free_pages(const ZArray<ZPage*>* pages);\n+  void free_page(ZPage* page);\n+  void free_pages(ZGenerationId id, const ZArray<ZPage*>* pages);\n@@ -163,3 +276,0 @@\n-  void enable_safe_recycle() const;\n-  void disable_safe_recycle() const;\n-\n@@ -171,0 +281,3 @@\n+  ZPartitionConstIterator partition_iterator() const;\n+  ZPartitionIterator partition_iterator();\n+\n@@ -173,0 +286,4 @@\n+  void print_on(outputStream* st) const;\n+  void print_extended_on_error(outputStream* st) const;\n+  void print_on_error(outputStream* st) const;\n+\n@@ -178,12 +295,12 @@\n-  size_t _min_capacity;\n-  size_t _max_capacity;\n-  size_t _soft_max_capacity;\n-  size_t _capacity;\n-  size_t _used;\n-  size_t _used_high;\n-  size_t _used_low;\n-  size_t _used_generation;\n-  size_t _freed;\n-  size_t _promoted;\n-  size_t _compacted;\n-  size_t _allocation_stalls;\n+  const size_t _min_capacity;\n+  const size_t _max_capacity;\n+  const size_t _soft_max_capacity;\n+  const size_t _capacity;\n+  const size_t _used;\n+  const size_t _used_high;\n+  const size_t _used_low;\n+  const size_t _used_generation;\n+  const size_t _freed;\n+  const size_t _promoted;\n+  const size_t _compacted;\n+  const size_t _allocation_stalls;\n","filename":"src\/hotspot\/share\/gc\/z\/zPageAllocator.hpp","additions":196,"deletions":79,"binary":false,"changes":275,"status":"modified"},{"patch":"@@ -34,2 +34,3 @@\n-ZUncommitter::ZUncommitter(ZPageAllocator* page_allocator)\n-  : _page_allocator(page_allocator),\n+ZUncommitter::ZUncommitter(uint32_t id, ZPartition* partition)\n+  : _id(id),\n+    _partition(partition),\n@@ -38,1 +39,1 @@\n-  set_name(\"ZUncommitter\");\n+  set_name(\"ZUncommitter#%u\", id);\n@@ -49,1 +50,1 @@\n-    log_debug(gc, heap)(\"Uncommit Timeout: \" UINT64_FORMAT \"s\", timeout);\n+    log_debug(gc, heap)(\"Uncommitter (%u) Timeout: \" UINT64_FORMAT \"s\", _id, timeout);\n@@ -66,1 +67,1 @@\n-    size_t uncommitted = 0;\n+    size_t total_uncommitted = 0;\n@@ -70,2 +71,2 @@\n-      const size_t flushed = _page_allocator->uncommit(&timeout, ZUncommitDelay);\n-      if (flushed == 0) {\n+      const size_t uncommitted = _partition->uncommit(&timeout, ZUncommitDelay);\n+      if (uncommitted == 0) {\n@@ -76,1 +77,1 @@\n-      uncommitted += flushed;\n+      total_uncommitted += uncommitted;\n@@ -79,1 +80,1 @@\n-    if (uncommitted > 0) {\n+    if (total_uncommitted > 0) {\n@@ -81,3 +82,3 @@\n-      ZStatInc(ZCounterUncommit, uncommitted);\n-      log_info(gc, heap)(\"Uncommitted: %zuM(%.0f%%)\",\n-                         uncommitted \/ M, percent_of(uncommitted, ZHeap::heap()->max_capacity()));\n+      ZStatInc(ZCounterUncommit, total_uncommitted);\n+      log_info(gc, heap)(\"Uncommitter (%u) Uncommitted: %zuM(%.0f%%)\",\n+                         _id, total_uncommitted \/ M, percent_of(total_uncommitted, ZHeap::heap()->max_capacity()));\n@@ -86,1 +87,1 @@\n-      event.commit(uncommitted);\n+      event.commit(total_uncommitted);\n","filename":"src\/hotspot\/share\/gc\/z\/zUncommitter.cpp","additions":14,"deletions":13,"binary":false,"changes":27,"status":"modified"},{"patch":"@@ -370,24 +370,0 @@\n-\n-AsyncLogWriter::BufferUpdater::BufferUpdater(size_t newsize) {\n-  ConsumerLocker clocker;\n-  auto p = AsyncLogWriter::_instance;\n-\n-  _buf1 = p->_buffer;\n-  _buf2 = p->_buffer_staging;\n-  p->_buffer = new Buffer(newsize);\n-  p->_buffer_staging = new Buffer(newsize);\n-}\n-\n-AsyncLogWriter::BufferUpdater::~BufferUpdater() {\n-  AsyncLogWriter::flush();\n-  auto p = AsyncLogWriter::_instance;\n-\n-  {\n-    ConsumerLocker clocker;\n-\n-    delete p->_buffer;\n-    delete p->_buffer_staging;\n-    p->_buffer = _buf1;\n-    p->_buffer_staging = _buf2;\n-  }\n-}\n","filename":"src\/hotspot\/share\/logging\/logAsyncWriter.cpp","additions":0,"deletions":24,"binary":false,"changes":24,"status":"modified"},{"patch":"@@ -204,10 +204,0 @@\n-  \/\/ for testing-only\n-  class BufferUpdater {\n-    Buffer* _buf1;\n-    Buffer* _buf2;\n-\n-   public:\n-    BufferUpdater(size_t newsize);\n-    ~BufferUpdater();\n-  };\n-\n","filename":"src\/hotspot\/share\/logging\/logAsyncWriter.hpp","additions":0,"deletions":10,"binary":false,"changes":10,"status":"modified"},{"patch":"@@ -1592,1 +1592,1 @@\n-      fd.reinitialize(k, fs.index());\n+      fd.reinitialize(k, fs.to_FieldInfo());\n","filename":"src\/hotspot\/share\/prims\/jvm.cpp","additions":1,"deletions":1,"binary":false,"changes":2,"status":"modified"},{"patch":"@@ -421,1 +421,1 @@\n-JvmtiExport::get_jvmti_thread_state(JavaThread *thread) {\n+JvmtiExport::get_jvmti_thread_state(JavaThread *thread, bool allow_suspend) {\n@@ -425,0 +425,6 @@\n+    if (allow_suspend && thread->is_suspended()) {\n+      \/\/ Suspend here if thread_started got a suspend request during its execution.\n+      \/\/ Within thread_started we could block on a VM mutex and pick up a suspend\n+      \/\/ request from debug agent which we need to honor before proceeding.\n+      ThreadBlockInVM tbivm(thread, true \/* allow suspend *\/);\n+    }\n@@ -1379,0 +1385,4 @@\n+    \/\/ All events can be disabled if current thread is doing a Java upcall originated by JVMTI.\n+    \/\/ ClassLoad events are important for JDWP agent but not expected during such upcalls.\n+    \/\/ Catch if this invariant is broken.\n+    assert(!thread->is_in_java_upcall(), \"unexpected ClassLoad event during JVMTI upcall\");\n@@ -1416,0 +1426,4 @@\n+    \/\/ All events can be disabled if current thread is doing a Java upcall originated by JVMTI.\n+    \/\/ ClassPrepare events are important for JDWP agent but not expected during such upcalls.\n+    \/\/ Catch if this invariant is broken.\n+    assert(!thread->is_in_java_upcall(), \"unexpected ClassPrepare event during JVMTI upcall\");\n@@ -2631,1 +2645,1 @@\n-  JvmtiThreadState *state = get_jvmti_thread_state(thread);\n+  JvmtiThreadState *state = get_jvmti_thread_state(thread, false \/* allow_suspend *\/);\n","filename":"src\/hotspot\/share\/prims\/jvmtiExport.cpp","additions":16,"deletions":2,"binary":false,"changes":18,"status":"modified"},{"patch":"@@ -312,1 +312,5 @@\n-  static JvmtiThreadState* get_jvmti_thread_state(JavaThread *thread);\n+  \/\/ The 'allow_suspend' parameter is passed as 'true' by default which work for almost all call sites.\n+  \/\/ It means that a suspend point need to be organized by this function for virtual threads if the call\n+  \/\/ to jvmtiEventController::thread_started hits a safepoint and gets a new suspend request.\n+  \/\/ The 'allow_suspend' parameter must be passed as 'false' if thread is holding a VM lock.\n+  static JvmtiThreadState* get_jvmti_thread_state(JavaThread *thread, bool allow_suspend = true);\n","filename":"src\/hotspot\/share\/prims\/jvmtiExport.hpp","additions":5,"deletions":1,"binary":false,"changes":6,"status":"modified"},{"patch":"@@ -140,0 +140,3 @@\n+\/\/ The VM info string should be a constant, but its value cannot be finalized until after VM arguments\n+\/\/ have been fully processed. And we want to avoid dynamic memory allocation which will cause ASAN\n+\/\/ report error, so we enumerate all the cases by static const string value.\n@@ -141,1 +144,0 @@\n-  const char* mode;\n@@ -144,2 +146,5 @@\n-      mode = \"interpreted mode\";\n-      break;\n+      if (is_vm_statically_linked()) {\n+        return CDSConfig::is_using_archive() ? \"interpreted mode, static, sharing\" : \"interpreted mode, static\";\n+      } else {\n+        return CDSConfig::is_using_archive() ? \"interpreted mode, sharing\" : \"interpreted mode\";\n+      }\n@@ -147,2 +152,6 @@\n-      if (CompilationModeFlag::quick_only()) {\n-        mode = \"mixed mode, emulated-client\";\n+      if (is_vm_statically_linked()) {\n+        if (CompilationModeFlag::quick_only()) {\n+          return CDSConfig::is_using_archive() ? \"mixed mode, emulated-client, static, sharing\" : \"mixed mode, emulated-client, static\";\n+        } else {\n+          return CDSConfig::is_using_archive() ? \"mixed mode, static, sharing\" : \"mixed mode, static\";\n+         }\n@@ -150,1 +159,5 @@\n-        mode = \"mixed mode\";\n+        if (CompilationModeFlag::quick_only()) {\n+          return CDSConfig::is_using_archive() ? \"mixed mode, emulated-client, sharing\" : \"mixed mode, emulated-client\";\n+        } else {\n+          return CDSConfig::is_using_archive() ? \"mixed mode, sharing\" : \"mixed mode\";\n+        }\n@@ -152,3 +165,5 @@\n-      break;\n-      if (CompilationModeFlag::quick_only()) {\n-        mode = \"compiled mode, emulated-client\";\n+      if (is_vm_statically_linked()) {\n+        if (CompilationModeFlag::quick_only()) {\n+          return CDSConfig::is_using_archive() ? \"compiled mode, emulated-client, static, sharing\" : \"compiled mode, emulated-client, static\";\n+        }\n+        return CDSConfig::is_using_archive() ? \"compiled mode, static, sharing\" : \"compiled mode, static\";\n@@ -157,1 +172,4 @@\n-        mode = \"compiled mode\";\n+        if (CompilationModeFlag::quick_only()) {\n+          return CDSConfig::is_using_archive() ? \"compiled mode, emulated-client, sharing\" : \"compiled mode, emulated-client\";\n+        }\n+        return CDSConfig::is_using_archive() ? \"compiled mode, sharing\" : \"compiled mode\";\n@@ -159,16 +177,2 @@\n-      break;\n-    default:\n-      ShouldNotReachHere();\n-\n-  const char* static_info = \", static\";\n-  const char* sharing_info = \", sharing\";\n-  size_t len = strlen(mode) +\n-               (is_vm_statically_linked() ? strlen(static_info) : 0) +\n-               (CDSConfig::is_using_archive() ? strlen(sharing_info) : 0) +\n-               1;\n-  char* vm_info = NEW_C_HEAP_ARRAY(char, len, mtInternal);\n-  \/\/ jio_snprintf places null character in the last character.\n-  jio_snprintf(vm_info, len, \"%s%s%s\", mode,\n-               is_vm_statically_linked() ? static_info : \"\",\n-               CDSConfig::is_using_archive() ? sharing_info : \"\");\n-  return vm_info;\n+  ShouldNotReachHere();\n+  return \"\";\n","filename":"src\/hotspot\/share\/runtime\/abstract_vm_version.cpp","additions":30,"deletions":26,"binary":false,"changes":56,"status":"modified"},{"patch":"@@ -98,1 +98,1 @@\n-bool   Arguments::_sun_java_launcher_is_altjvm  = false;\n+bool   Arguments::_executing_unit_tests         = false;\n@@ -362,1 +362,1 @@\n-  \/\/ See if sun.java.launcher or sun.java.launcher.is_altjvm is defined.\n+  \/\/ See if sun.java.launcher is defined.\n@@ -373,4 +373,2 @@\n-    if (match_option(option, \"-Dsun.java.launcher.is_altjvm=\", &tail)) {\n-      if (strcmp(tail, \"true\") == 0) {\n-        _sun_java_launcher_is_altjvm = true;\n-      }\n+    if (match_option(option, \"-XX:+ExecutingUnitTests\")) {\n+      _executing_unit_tests = true;\n@@ -533,0 +531,3 @@\n+#ifdef _LP64\n+  { \"UseCompressedClassPointers\",   JDK_Version::jdk(25),  JDK_Version::jdk(26), JDK_Version::undefined() },\n+#endif\n@@ -1310,4 +1311,0 @@\n-  } else if (strcmp(key, \"sun.java.launcher.is_altjvm\") == 0) {\n-    \/\/ sun.java.launcher.is_altjvm property is\n-    \/\/ private and is processed in process_sun_java_launcher_properties();\n-    \/\/ the sun.java.launcher property is passed on to the java application\n@@ -1802,2 +1799,2 @@\n-bool Arguments::sun_java_launcher_is_altjvm() {\n-  return _sun_java_launcher_is_altjvm;\n+bool Arguments::executing_unit_tests() {\n+  return _executing_unit_tests;\n","filename":"src\/hotspot\/share\/runtime\/arguments.cpp","additions":9,"deletions":12,"binary":false,"changes":21,"status":"modified"},{"patch":"@@ -242,2 +242,2 @@\n-  \/\/ was this VM created via the -XXaltjvm=<path> option\n-  static bool   _sun_java_launcher_is_altjvm;\n+  \/\/ was this VM created with the -XX:+ExecutingUnitTests option\n+  static bool _executing_unit_tests;\n@@ -438,2 +438,2 @@\n-  \/\/ -Dsun.java.launcher.is_altjvm\n-  static bool sun_java_launcher_is_altjvm();\n+  \/\/ -XX:+ExecutingUnitTests\n+  static bool executing_unit_tests();\n","filename":"src\/hotspot\/share\/runtime\/arguments.hpp","additions":4,"deletions":4,"binary":false,"changes":8,"status":"modified"},{"patch":"@@ -131,1 +131,1 @@\n-          \"Use 32-bit class pointers in 64-bit VM. \"                        \\\n+          \"(Deprecated) Use 32-bit class pointers in 64-bit VM. \"           \\\n@@ -333,0 +333,2 @@\n+  product(bool, UseKyberIntrinsics, false, DIAGNOSTIC,                      \\\n+          \"Use intrinsics for the vectorized version of Kyber\")             \\\n@@ -1173,3 +1175,0 @@\n-  develop(bool, VerifyFPU, false,                                           \\\n-          \"Verify FPU state (check for NaN's, etc.)\")                       \\\n-                                                                            \\\n","filename":"src\/hotspot\/share\/runtime\/globals.hpp","additions":3,"deletions":4,"binary":false,"changes":7,"status":"modified"},{"patch":"@@ -334,0 +334,1 @@\n+  bool                  _is_in_java_upcall;              \/\/ JVMTI is doing a Java upcall, so JVMTI events must be hidden\n@@ -725,0 +726,3 @@\n+  bool is_in_java_upcall() const                 { return _is_in_java_upcall; }\n+  void toggle_is_in_java_upcall()                { _is_in_java_upcall = !_is_in_java_upcall; };\n+\n@@ -731,1 +735,2 @@\n-  bool should_hide_jvmti_events() const          { return _is_in_VTMS_transition || _is_disable_suspend; }\n+  \/\/ - JVMTI is making a Java upcall (_is_in_java_upcall)\n+  bool should_hide_jvmti_events() const          { return _is_in_VTMS_transition || _is_disable_suspend || _is_in_java_upcall; }\n","filename":"src\/hotspot\/share\/runtime\/javaThread.hpp","additions":6,"deletions":1,"binary":false,"changes":7,"status":"modified"},{"patch":"@@ -1537,0 +1537,50 @@\n+static char* _image_release_file_content = nullptr;\n+\n+void os::read_image_release_file() {\n+  assert(_image_release_file_content == nullptr, \"release file content must not be already set\");\n+  const char* home = Arguments::get_java_home();\n+  stringStream ss;\n+  ss.print(\"%s\/release\", home);\n+\n+  FILE* file = fopen(ss.base(), \"rb\");\n+  if (file == nullptr) {\n+    return;\n+  }\n+  fseek(file, 0, SEEK_END);\n+  long sz = ftell(file);\n+  if (sz == -1) {\n+    return;\n+  }\n+  fseek(file, 0, SEEK_SET);\n+\n+  char* tmp = (char*) os::malloc(sz + 1, mtInternal);\n+  if (tmp == nullptr) {\n+    fclose(file);\n+    return;\n+  }\n+\n+  size_t elements_read = fread(tmp, 1, sz, file);\n+  if (elements_read < (size_t)sz) {\n+    tmp[elements_read] = '\\0';\n+  } else {\n+    tmp[sz] = '\\0';\n+  }\n+  \/\/ issues with \\r in line endings on Windows, so better replace those\n+  for (size_t i = 0; i < elements_read; i++) {\n+    if (tmp[i] == '\\r') {\n+      tmp[i] = ' ';\n+    }\n+  }\n+  Atomic::release_store(&_image_release_file_content, tmp);\n+  fclose(file);\n+}\n+\n+void os::print_image_release_file(outputStream* st) {\n+  char* ifrc = Atomic::load_acquire(&_image_release_file_content);\n+  if (ifrc != nullptr) {\n+    st->print_cr(\"%s\", ifrc);\n+  } else {\n+    st->print_cr(\"<release file has not been read>\");\n+  }\n+}\n+\n@@ -2547,1 +2597,1 @@\n-#ifdef WINDOWS\n+#ifdef _WINDOWS\n","filename":"src\/hotspot\/share\/runtime\/os.cpp","additions":51,"deletions":1,"binary":false,"changes":52,"status":"modified"},{"patch":"@@ -680,0 +680,5 @@\n+\n+  \/\/ read\/store and print the release file of the image\n+  static void read_image_release_file();\n+  static void print_image_release_file(outputStream* st);\n+\n","filename":"src\/hotspot\/share\/runtime\/os.hpp","additions":5,"deletions":0,"binary":false,"changes":5,"status":"modified"},{"patch":"@@ -438,0 +438,13 @@\n+\/\/ One-shot PeriodicTask subclass for reading the release file\n+class ReadReleaseFileTask : public PeriodicTask {\n+ public:\n+  ReadReleaseFileTask() : PeriodicTask(100) {}\n+\n+  virtual void task() {\n+    os::read_image_release_file();\n+\n+    \/\/ Reclaim our storage and disenroll ourself.\n+    delete this;\n+  }\n+};\n+\n@@ -616,0 +629,4 @@\n+  \/\/ Have the WatcherThread read the release file in the background.\n+  ReadReleaseFileTask* read_task = new ReadReleaseFileTask();\n+  read_task->enroll();\n+\n","filename":"src\/hotspot\/share\/runtime\/threads.cpp","additions":17,"deletions":0,"binary":false,"changes":17,"status":"modified"},{"patch":"@@ -38,0 +38,1 @@\n+typedef void(*ZIP_FreeEntry_t)(jzfile *zip, jzentry *entry);\n@@ -47,0 +48,1 @@\n+static ZIP_FreeEntry_t ZIP_FreeEntry = nullptr;\n@@ -84,0 +86,1 @@\n+  ZIP_FreeEntry = CAST_TO_FN_PTR(ZIP_FreeEntry_t, dll_lookup(\"ZIP_FreeEntry\", path, vm_exit_on_failure));\n@@ -182,0 +185,6 @@\n+void ZipLibrary::free_entry(jzfile* zip, jzentry* entry) {\n+  initialize();\n+  assert(ZIP_FreeEntry != nullptr, \"invariant\");\n+  ZIP_FreeEntry(zip, entry);\n+}\n+\n","filename":"src\/hotspot\/share\/utilities\/zipLibrary.cpp","additions":9,"deletions":0,"binary":false,"changes":9,"status":"modified"},{"patch":"@@ -2,1 +2,1 @@\n- * Copyright (c) 2023, Oracle and\/or its affiliates. All rights reserved.\n+ * Copyright (c) 2023, 2025, Oracle and\/or its affiliates. All rights reserved.\n@@ -49,0 +49,1 @@\n+  static void free_entry(jzfile* zip, jzentry* entry);\n","filename":"src\/hotspot\/share\/utilities\/zipLibrary.hpp","additions":2,"deletions":1,"binary":false,"changes":3,"status":"modified"},{"patch":"@@ -2,1 +2,1 @@\n- * Copyright (c) 2002, 2024, Oracle and\/or its affiliates. All rights reserved.\n+ * Copyright (c) 2002, 2025, Oracle and\/or its affiliates. All rights reserved.\n@@ -304,0 +304,1 @@\n+    @SuppressWarnings(\"removal\")\n","filename":"src\/java.base\/share\/classes\/jdk\/internal\/access\/SharedSecrets.java","additions":2,"deletions":1,"binary":false,"changes":3,"status":"modified"},{"patch":"@@ -2,1 +2,1 @@\n- * Copyright (c) 1998, 2024, Oracle and\/or its affiliates. All rights reserved.\n+ * Copyright (c) 1998, 2025, Oracle and\/or its affiliates. All rights reserved.\n@@ -128,4 +128,0 @@\n-\/* Reports a system error message to stderr or a window *\/\n-JNIEXPORT void JNICALL\n-JLI_ReportErrorMessageSys(const char * message, ...);\n-\n","filename":"src\/java.base\/share\/native\/libjli\/java.h","additions":1,"deletions":5,"binary":false,"changes":6,"status":"modified"},{"patch":"@@ -2,1 +2,1 @@\n- * Copyright (c) 1995, 2024, Oracle and\/or its affiliates. All rights reserved.\n+ * Copyright (c) 1995, 2025, Oracle and\/or its affiliates. All rights reserved.\n@@ -1119,1 +1119,1 @@\n-void\n+JNIEXPORT void\n","filename":"src\/java.base\/share\/native\/libzip\/zip_util.c","additions":2,"deletions":2,"binary":false,"changes":4,"status":"modified"},{"patch":"@@ -2,1 +2,1 @@\n- * Copyright (c) 1995, 2022, Oracle and\/or its affiliates. All rights reserved.\n+ * Copyright (c) 1995, 2025, Oracle and\/or its affiliates. All rights reserved.\n@@ -279,1 +279,1 @@\n-void\n+JNIEXPORT void\n","filename":"src\/java.base\/share\/native\/libzip\/zip_util.h","additions":2,"deletions":2,"binary":false,"changes":4,"status":"modified"},{"patch":"@@ -2,1 +2,1 @@\n- * Copyright (c) 2000, 2023, Oracle and\/or its affiliates. All rights reserved.\n+ * Copyright (c) 2000, 2025, Oracle and\/or its affiliates. All rights reserved.\n@@ -132,1 +132,1 @@\n-            if (iter->ifa_name[0] != '\\0') {\n+            if (iter->ifa_name[0] != '\\0' && (iter->ifa_flags & IFF_UP) == IFF_UP) {\n@@ -169,1 +169,1 @@\n-        if (iter->ifa_addr != NULL) {\n+        if (iter->ifa_addr != NULL && (iter->ifa_flags & IFF_UP) == IFF_UP) {\n","filename":"src\/java.base\/unix\/native\/libnet\/Inet6AddressImpl.c","additions":3,"deletions":3,"binary":false,"changes":6,"status":"modified"},{"patch":"@@ -502,0 +502,2 @@\n+sun\/java2d\/ClassCastExceptionForInvalidSurface.java 8354097 linux-x64\n+sun\/java2d\/GdiRendering\/ClipShapeRendering.java 8354097 linux-x64\n@@ -734,0 +736,1 @@\n+java\/util\/logging\/LoggingDeadlock5.java       8354424 generic-all\n","filename":"test\/jdk\/ProblemList.txt","additions":3,"deletions":0,"binary":false,"changes":3,"status":"modified"}]}