{"files":[{"patch":"@@ -2,1 +2,1 @@\n-project=jdk-updates\n+project=shenandoah\n@@ -4,0 +4,1 @@\n+<<<<<<< HEAD\n@@ -5,0 +6,3 @@\n+=======\n+version=21\n+>>>>>>> 666656136f7f15905ae770888951d832f46b0535\n@@ -25,1 +29,1 @@\n-reviewers=1\n+committers=1\n","filename":".jcheck\/conf","additions":6,"deletions":2,"binary":false,"changes":8,"status":"modified"},{"patch":"@@ -2,0 +2,1 @@\n+<<<<<<< HEAD\n@@ -4,0 +5,4 @@\n+=======\n+ * Copyright (c) 2018, 2021, Red Hat, Inc. All rights reserved.\n+ * Copyright (c) 2012, 2022 SAP SE. All rights reserved.\n+>>>>>>> 666656136f7f15905ae770888951d832f46b0535\n@@ -110,0 +115,13 @@\n+<<<<<<< HEAD\n+=======\n+      if (ShenandoahCardBarrier) {\n+        post_barrier(access, access.resolved_addr(), new_value.result());\n+      }\n+\n+      if (support_IRIW_for_not_multiple_copy_atomic_cpu) {\n+        __ membar_acquire();\n+      } else {\n+        __ membar();\n+      }\n+\n+>>>>>>> 666656136f7f15905ae770888951d832f46b0535\n@@ -114,1 +132,7 @@\n-  return BarrierSetC1::atomic_cmpxchg_at_resolved(access, cmp_value, new_value);\n+  LIR_Opr result = BarrierSetC1::atomic_cmpxchg_at_resolved(access, cmp_value, new_value);\n+\n+  if (ShenandoahCardBarrier && access.is_oop()) {\n+    post_barrier(access, access.resolved_addr(), new_value.result());\n+  }\n+\n+  return result;\n@@ -144,0 +168,4 @@\n+\n+    if (ShenandoahCardBarrier) {\n+      post_barrier(access, access.resolved_addr(), result);\n+    }\n","filename":"src\/hotspot\/cpu\/ppc\/gc\/shenandoah\/c1\/shenandoahBarrierSetC1_ppc.cpp","additions":29,"deletions":1,"binary":false,"changes":30,"status":"modified"},{"patch":"@@ -3,0 +3,1 @@\n+ * Copyright Amazon.com Inc. or its affiliates. All Rights Reserved.\n@@ -33,0 +34,1 @@\n+#include \"gc\/shenandoah\/shenandoahGeneration.hpp\"\n@@ -42,0 +44,1 @@\n+#include \"gc\/shenandoah\/shenandoahScanRemembered.inline.hpp\"\n@@ -47,0 +50,1 @@\n+template <ShenandoahGenerationType GENERATION>\n@@ -59,1 +63,2 @@\n-    ShenandoahConcurrentWorkerSession worker_session(worker_id);\n+    ShenandoahParallelWorkerSession worker_session(worker_id);\n+    ShenandoahWorkerTimingsTracker timer(ShenandoahPhaseTimings::conc_mark, ShenandoahPhaseTimings::ParallelMark, worker_id, true);\n@@ -61,0 +66,1 @@\n+<<<<<<< HEAD\n@@ -62,0 +68,3 @@\n+=======\n+    ShenandoahReferenceProcessor* rp = heap->active_generation()->ref_processor();\n+>>>>>>> 666656136f7f15905ae770888951d832f46b0535\n@@ -64,1 +73,1 @@\n-    _cm->mark_loop(worker_id, _terminator, rp,\n+    _cm->mark_loop(GENERATION, worker_id, _terminator, rp,\n@@ -75,1 +84,0 @@\n-  uintx _claim_token;\n@@ -94,0 +102,1 @@\n+template<ShenandoahGenerationType GENERATION>\n@@ -109,1 +118,0 @@\n-    ShenandoahReferenceProcessor* rp = heap->ref_processor();\n@@ -111,0 +119,1 @@\n+    ShenandoahReferenceProcessor* rp = heap->active_generation()->ref_processor();\n@@ -115,0 +124,1 @@\n+      ShenandoahObjToScanQueue* old_q = _cm->get_old_queue(worker_id);\n@@ -116,1 +126,1 @@\n-      ShenandoahSATBBufferClosure cl(q);\n+      ShenandoahSATBBufferClosure<GENERATION> cl(q, old_q);\n@@ -121,1 +131,1 @@\n-      ShenandoahMarkRefsClosure             mark_cl(q, rp);\n+      ShenandoahMarkRefsClosure<GENERATION> mark_cl(q, rp, old_q);\n@@ -126,1 +136,1 @@\n-    _cm->mark_loop(worker_id, _terminator, rp,\n+    _cm->mark_loop(GENERATION, worker_id, _terminator, rp,\n@@ -134,2 +144,2 @@\n-ShenandoahConcurrentMark::ShenandoahConcurrentMark() :\n-  ShenandoahMark() {}\n+ShenandoahConcurrentMark::ShenandoahConcurrentMark(ShenandoahGeneration* generation) :\n+  ShenandoahMark(generation) {}\n@@ -138,0 +148,1 @@\n+template<ShenandoahGenerationType GENERATION>\n@@ -143,0 +154,1 @@\n+  ShenandoahObjToScanQueueSet* const  _old_queue_set;\n@@ -147,0 +159,1 @@\n+                                    ShenandoahObjToScanQueueSet* old,\n@@ -153,4 +166,6 @@\n-ShenandoahMarkConcurrentRootsTask::ShenandoahMarkConcurrentRootsTask(ShenandoahObjToScanQueueSet* qs,\n-                                                                     ShenandoahReferenceProcessor* rp,\n-                                                                     ShenandoahPhaseTimings::Phase phase,\n-                                                                     uint nworkers) :\n+template<ShenandoahGenerationType GENERATION>\n+ShenandoahMarkConcurrentRootsTask<GENERATION>::ShenandoahMarkConcurrentRootsTask(ShenandoahObjToScanQueueSet* qs,\n+                                                                                 ShenandoahObjToScanQueueSet* old,\n+                                                                                 ShenandoahReferenceProcessor* rp,\n+                                                                                 ShenandoahPhaseTimings::Phase phase,\n+                                                                                 uint nworkers) :\n@@ -160,0 +175,1 @@\n+  _old_queue_set(old),\n@@ -164,1 +180,2 @@\n-void ShenandoahMarkConcurrentRootsTask::work(uint worker_id) {\n+template<ShenandoahGenerationType GENERATION>\n+void ShenandoahMarkConcurrentRootsTask<GENERATION>::work(uint worker_id) {\n@@ -167,1 +184,3 @@\n-  ShenandoahMarkRefsClosure cl(q, _rp);\n+  ShenandoahObjToScanQueue* old_q = (_old_queue_set == nullptr) ?\n+          nullptr : _old_queue_set->queue(worker_id);\n+  ShenandoahMarkRefsClosure<GENERATION> cl(q, _rp, old_q);\n@@ -175,2 +194,0 @@\n-  TASKQUEUE_STATS_ONLY(task_queues()->reset_taskqueue_stats());\n-\n@@ -178,5 +195,31 @@\n-  ShenandoahReferenceProcessor* rp = heap->ref_processor();\n-  task_queues()->reserve(workers->active_workers());\n-  ShenandoahMarkConcurrentRootsTask task(task_queues(), rp, ShenandoahPhaseTimings::conc_mark_roots, workers->active_workers());\n-\n-  workers->run_task(&task);\n+  ShenandoahReferenceProcessor* rp = _generation->ref_processor();\n+  _generation->reserve_task_queues(workers->active_workers());\n+  switch (_generation->type()) {\n+    case YOUNG: {\n+      ShenandoahMarkConcurrentRootsTask<YOUNG> task(task_queues(), old_task_queues(), rp,\n+                                                    ShenandoahPhaseTimings::conc_mark_roots, workers->active_workers());\n+      workers->run_task(&task);\n+      break;\n+    }\n+    case GLOBAL_GEN: {\n+      assert(old_task_queues() == nullptr, \"Global mark should not have old gen mark queues\");\n+      ShenandoahMarkConcurrentRootsTask<GLOBAL_GEN> task(task_queues(), nullptr, rp,\n+                                                         ShenandoahPhaseTimings::conc_mark_roots, workers->active_workers());\n+      workers->run_task(&task);\n+      break;\n+    }\n+    case GLOBAL_NON_GEN: {\n+      assert(old_task_queues() == nullptr, \"Non-generational mark should not have old gen mark queues\");\n+      ShenandoahMarkConcurrentRootsTask<GLOBAL_NON_GEN> task(task_queues(), nullptr, rp,\n+                                                         ShenandoahPhaseTimings::conc_mark_roots, workers->active_workers());\n+      workers->run_task(&task);\n+      break;\n+    }\n+    case OLD: {\n+      \/\/ We use a YOUNG generation cycle to bootstrap concurrent old marking.\n+      ShouldNotReachHere();\n+      break;\n+    }\n+    default:\n+      ShouldNotReachHere();\n+  }\n@@ -207,3 +250,34 @@\n-    TaskTerminator terminator(nworkers, task_queues());\n-    ShenandoahConcurrentMarkingTask task(this, &terminator);\n-    workers->run_task(&task);\n+    switch (_generation->type()) {\n+      case YOUNG: {\n+        \/\/ Clear any old\/partial local census data before the start of marking.\n+        heap->age_census()->reset_local();\n+        assert(heap->age_census()->is_clear_local(), \"Error\");\n+        TaskTerminator terminator(nworkers, task_queues());\n+        ShenandoahConcurrentMarkingTask<YOUNG> task(this, &terminator);\n+        workers->run_task(&task);\n+        break;\n+      }\n+      case OLD: {\n+        TaskTerminator terminator(nworkers, task_queues());\n+        ShenandoahConcurrentMarkingTask<OLD> task(this, &terminator);\n+        workers->run_task(&task);\n+        break;\n+      }\n+      case GLOBAL_GEN: {\n+        \/\/ Clear any old\/partial local census data before the start of marking.\n+        heap->age_census()->reset_local();\n+        assert(heap->age_census()->is_clear_local(), \"Error\");\n+        TaskTerminator terminator(nworkers, task_queues());\n+        ShenandoahConcurrentMarkingTask<GLOBAL_GEN> task(this, &terminator);\n+        workers->run_task(&task);\n+        break;\n+      }\n+      case GLOBAL_NON_GEN: {\n+        TaskTerminator terminator(nworkers, task_queues());\n+        ShenandoahConcurrentMarkingTask<GLOBAL_NON_GEN> task(this, &terminator);\n+        workers->run_task(&task);\n+        break;\n+      }\n+      default:\n+        ShouldNotReachHere();\n+    }\n@@ -236,3 +310,2 @@\n-  ShenandoahHeap* const heap = ShenandoahHeap::heap();\n-  heap->set_concurrent_mark_in_progress(false);\n-  heap->mark_complete_marking_context();\n+  _generation->set_concurrent_mark_in_progress(false);\n+  _generation->set_mark_complete();\n@@ -258,2 +331,0 @@\n-  ShenandoahFinalMarkingTask task(this, &terminator, ShenandoahStringDedup::is_enabled());\n-  heap->workers()->run_task(&task);\n@@ -261,2 +332,24 @@\n-  assert(task_queues()->is_empty(), \"Should be empty\");\n-}\n+  switch (_generation->type()) {\n+    case YOUNG:{\n+      ShenandoahFinalMarkingTask<YOUNG> task(this, &terminator, ShenandoahStringDedup::is_enabled());\n+      heap->workers()->run_task(&task);\n+      break;\n+    }\n+    case OLD:{\n+      ShenandoahFinalMarkingTask<OLD> task(this, &terminator, ShenandoahStringDedup::is_enabled());\n+      heap->workers()->run_task(&task);\n+      break;\n+    }\n+    case GLOBAL_GEN:{\n+      ShenandoahFinalMarkingTask<GLOBAL_GEN> task(this, &terminator, ShenandoahStringDedup::is_enabled());\n+      heap->workers()->run_task(&task);\n+      break;\n+    }\n+    case GLOBAL_NON_GEN:{\n+      ShenandoahFinalMarkingTask<GLOBAL_NON_GEN> task(this, &terminator, ShenandoahStringDedup::is_enabled());\n+      heap->workers()->run_task(&task);\n+      break;\n+    }\n+    default:\n+      ShouldNotReachHere();\n+  }\n@@ -265,4 +358,1 @@\n-void ShenandoahConcurrentMark::cancel() {\n-  clear();\n-  ShenandoahReferenceProcessor* rp = ShenandoahHeap::heap()->ref_processor();\n-  rp->abandon_partial_discovery();\n+  assert(task_queues()->is_empty(), \"Should be empty\");\n","filename":"src\/hotspot\/share\/gc\/shenandoah\/shenandoahConcurrentMark.cpp","additions":126,"deletions":36,"binary":false,"changes":162,"status":"modified"},{"patch":"@@ -3,0 +3,1 @@\n+ * Copyright Amazon.com Inc. or its affiliates. All Rights Reserved.\n@@ -27,0 +28,2 @@\n+#include \"gc\/shenandoah\/shenandoahAffiliation.hpp\"\n+#include \"gc\/shenandoah\/shenandoahBarrierSet.hpp\"\n@@ -31,0 +34,3 @@\n+#include \"gc\/shenandoah\/shenandoahOldGeneration.hpp\"\n+#include \"gc\/shenandoah\/shenandoahScanRemembered.inline.hpp\"\n+#include \"gc\/shenandoah\/shenandoahYoungGeneration.hpp\"\n@@ -35,0 +41,371 @@\n+ShenandoahSetsOfFree::ShenandoahSetsOfFree(size_t max_regions, ShenandoahFreeSet* free_set) :\n+    _max(max_regions),\n+    _free_set(free_set),\n+    _region_size_bytes(ShenandoahHeapRegion::region_size_bytes())\n+{\n+  _membership = NEW_C_HEAP_ARRAY(ShenandoahFreeMemoryType, max_regions, mtGC);\n+  clear_internal();\n+}\n+\n+ShenandoahSetsOfFree::~ShenandoahSetsOfFree() {\n+  FREE_C_HEAP_ARRAY(ShenandoahFreeMemoryType, _membership);\n+}\n+\n+\n+void ShenandoahSetsOfFree::clear_internal() {\n+  for (size_t idx = 0; idx < _max; idx++) {\n+    _membership[idx] = NotFree;\n+  }\n+\n+  for (size_t idx = 0; idx < NumFreeSets; idx++) {\n+    _leftmosts[idx] = _max;\n+    _rightmosts[idx] = 0;\n+    _leftmosts_empty[idx] = _max;\n+    _rightmosts_empty[idx] = 0;\n+    _capacity_of[idx] = 0;\n+    _used_by[idx] = 0;\n+  }\n+\n+  _left_to_right_bias[Mutator] = true;\n+  _left_to_right_bias[Collector] = false;\n+  _left_to_right_bias[OldCollector] = false;\n+\n+  _region_counts[Mutator] = 0;\n+  _region_counts[Collector] = 0;\n+  _region_counts[OldCollector] = 0;\n+  _region_counts[NotFree] = _max;\n+}\n+\n+void ShenandoahSetsOfFree::clear_all() {\n+  clear_internal();\n+}\n+\n+void ShenandoahSetsOfFree::increase_used(ShenandoahFreeMemoryType which_set, size_t bytes) {\n+  assert (which_set > NotFree && which_set < NumFreeSets, \"Set must correspond to a valid freeset\");\n+  _used_by[which_set] += bytes;\n+  assert (_used_by[which_set] <= _capacity_of[which_set],\n+          \"Must not use (\" SIZE_FORMAT \") more than capacity (\" SIZE_FORMAT \") after increase by \" SIZE_FORMAT,\n+          _used_by[which_set], _capacity_of[which_set], bytes);\n+}\n+\n+inline void ShenandoahSetsOfFree::shrink_bounds_if_touched(ShenandoahFreeMemoryType set, size_t idx) {\n+  if (idx == _leftmosts[set]) {\n+    while ((_leftmosts[set] < _max) && !in_free_set(_leftmosts[set], set)) {\n+      _leftmosts[set]++;\n+    }\n+    if (_leftmosts_empty[set] < _leftmosts[set]) {\n+      \/\/ This gets us closer to where we need to be; we'll scan further when leftmosts_empty is requested.\n+      _leftmosts_empty[set] = _leftmosts[set];\n+    }\n+  }\n+  if (idx == _rightmosts[set]) {\n+    while (_rightmosts[set] > 0 && !in_free_set(_rightmosts[set], set)) {\n+      _rightmosts[set]--;\n+    }\n+    if (_rightmosts_empty[set] > _rightmosts[set]) {\n+      \/\/ This gets us closer to where we need to be; we'll scan further when rightmosts_empty is requested.\n+      _rightmosts_empty[set] = _rightmosts[set];\n+    }\n+  }\n+}\n+\n+inline void ShenandoahSetsOfFree::expand_bounds_maybe(ShenandoahFreeMemoryType set, size_t idx, size_t region_capacity) {\n+  if (region_capacity == _region_size_bytes) {\n+    if (_leftmosts_empty[set] > idx) {\n+      _leftmosts_empty[set] = idx;\n+    }\n+    if (_rightmosts_empty[set] < idx) {\n+      _rightmosts_empty[set] = idx;\n+    }\n+  }\n+  if (_leftmosts[set] > idx) {\n+    _leftmosts[set] = idx;\n+  }\n+  if (_rightmosts[set] < idx) {\n+    _rightmosts[set] = idx;\n+  }\n+}\n+\n+void ShenandoahSetsOfFree::remove_from_free_sets(size_t idx) {\n+  assert (idx < _max, \"index is sane: \" SIZE_FORMAT \" < \" SIZE_FORMAT, idx, _max);\n+  ShenandoahFreeMemoryType orig_set = membership(idx);\n+  assert (orig_set > NotFree && orig_set < NumFreeSets, \"Cannot remove from free sets if not already free\");\n+  _membership[idx] = NotFree;\n+  shrink_bounds_if_touched(orig_set, idx);\n+\n+  _region_counts[orig_set]--;\n+  _region_counts[NotFree]++;\n+}\n+\n+\n+void ShenandoahSetsOfFree::make_free(size_t idx, ShenandoahFreeMemoryType which_set, size_t region_capacity) {\n+  assert (idx < _max, \"index is sane: \" SIZE_FORMAT \" < \" SIZE_FORMAT, idx, _max);\n+  assert (_membership[idx] == NotFree, \"Cannot make free if already free\");\n+  assert (which_set > NotFree && which_set < NumFreeSets, \"selected free set must be valid\");\n+  _membership[idx] = which_set;\n+  _capacity_of[which_set] += region_capacity;\n+  expand_bounds_maybe(which_set, idx, region_capacity);\n+\n+  _region_counts[NotFree]--;\n+  _region_counts[which_set]++;\n+}\n+\n+void ShenandoahSetsOfFree::move_to_set(size_t idx, ShenandoahFreeMemoryType new_set, size_t region_capacity) {\n+  assert (idx < _max, \"index is sane: \" SIZE_FORMAT \" < \" SIZE_FORMAT, idx, _max);\n+  assert ((new_set > NotFree) && (new_set < NumFreeSets), \"New set must be valid\");\n+  ShenandoahFreeMemoryType orig_set = _membership[idx];\n+  assert ((orig_set > NotFree) && (orig_set < NumFreeSets), \"Cannot move free unless already free\");\n+  \/\/ Expected transitions:\n+  \/\/  During rebuild: Mutator => Collector\n+  \/\/                  Mutator empty => Collector\n+  \/\/  During flip_to_gc:\n+  \/\/                  Mutator empty => Collector\n+  \/\/                  Mutator empty => Old Collector\n+  \/\/ At start of update refs:\n+  \/\/                  Collector => Mutator\n+  \/\/                  OldCollector Empty => Mutator\n+  assert (((region_capacity <= _region_size_bytes) &&\n+           ((orig_set == Mutator) && (new_set == Collector)) ||\n+           ((orig_set == Collector) && (new_set == Mutator))) ||\n+          ((region_capacity == _region_size_bytes) &&\n+           ((orig_set == Mutator) && (new_set == Collector)) ||\n+           ((orig_set == OldCollector) && (new_set == Mutator)) ||\n+           (new_set == OldCollector)), \"Unexpected movement between sets\");\n+\n+  _membership[idx] = new_set;\n+  _capacity_of[orig_set] -= region_capacity;\n+  shrink_bounds_if_touched(orig_set, idx);\n+\n+  _capacity_of[new_set] += region_capacity;\n+  expand_bounds_maybe(new_set, idx, region_capacity);\n+\n+  _region_counts[orig_set]--;\n+  _region_counts[new_set]++;\n+}\n+\n+inline ShenandoahFreeMemoryType ShenandoahSetsOfFree::membership(size_t idx) const {\n+  assert (idx < _max, \"index is sane: \" SIZE_FORMAT \" < \" SIZE_FORMAT, idx, _max);\n+  return _membership[idx];\n+}\n+\n+  \/\/ Returns true iff region idx is in the test_set free_set.  Before returning true, asserts that the free\n+  \/\/ set is not empty.  Requires that test_set != NotFree or NumFreeSets.\n+inline bool ShenandoahSetsOfFree::in_free_set(size_t idx, ShenandoahFreeMemoryType test_set) const {\n+  assert (idx < _max, \"index is sane: \" SIZE_FORMAT \" < \" SIZE_FORMAT, idx, _max);\n+  if (_membership[idx] == test_set) {\n+    assert (test_set == NotFree || _free_set->alloc_capacity(idx) > 0, \"Free regions must have alloc capacity\");\n+    return true;\n+  } else {\n+    return false;\n+  }\n+}\n+\n+inline size_t ShenandoahSetsOfFree::leftmost(ShenandoahFreeMemoryType which_set) const {\n+  assert (which_set > NotFree && which_set < NumFreeSets, \"selected free set must be valid\");\n+  size_t idx = _leftmosts[which_set];\n+  if (idx >= _max) {\n+    return _max;\n+  } else {\n+    assert (in_free_set(idx, which_set), \"left-most region must be free\");\n+    return idx;\n+  }\n+}\n+\n+inline size_t ShenandoahSetsOfFree::rightmost(ShenandoahFreeMemoryType which_set) const {\n+  assert (which_set > NotFree && which_set < NumFreeSets, \"selected free set must be valid\");\n+  size_t idx = _rightmosts[which_set];\n+  assert ((_leftmosts[which_set] == _max) || in_free_set(idx, which_set), \"right-most region must be free\");\n+  return idx;\n+}\n+\n+inline bool ShenandoahSetsOfFree::is_empty(ShenandoahFreeMemoryType which_set) const {\n+  assert (which_set > NotFree && which_set < NumFreeSets, \"selected free set must be valid\");\n+  return (leftmost(which_set) > rightmost(which_set));\n+}\n+\n+size_t ShenandoahSetsOfFree::leftmost_empty(ShenandoahFreeMemoryType which_set) {\n+  assert (which_set > NotFree && which_set < NumFreeSets, \"selected free set must be valid\");\n+  for (size_t idx = _leftmosts_empty[which_set]; idx < _max; idx++) {\n+    if ((membership(idx) == which_set) && (_free_set->alloc_capacity(idx) == _region_size_bytes)) {\n+      _leftmosts_empty[which_set] = idx;\n+      return idx;\n+    }\n+  }\n+  _leftmosts_empty[which_set] = _max;\n+  _rightmosts_empty[which_set] = 0;\n+  return _max;\n+}\n+\n+inline size_t ShenandoahSetsOfFree::rightmost_empty(ShenandoahFreeMemoryType which_set) {\n+  assert (which_set > NotFree && which_set < NumFreeSets, \"selected free set must be valid\");\n+  for (intptr_t idx = _rightmosts_empty[which_set]; idx >= 0; idx--) {\n+    if ((membership(idx) == which_set) && (_free_set->alloc_capacity(idx) == _region_size_bytes)) {\n+      _rightmosts_empty[which_set] = idx;\n+      return idx;\n+    }\n+  }\n+  _leftmosts_empty[which_set] = _max;\n+  _rightmosts_empty[which_set] = 0;\n+  return 0;\n+}\n+\n+inline bool ShenandoahSetsOfFree::alloc_from_left_bias(ShenandoahFreeMemoryType which_set) {\n+  assert (which_set > NotFree && which_set < NumFreeSets, \"selected free set must be valid\");\n+  return _left_to_right_bias[which_set];\n+}\n+\n+void ShenandoahSetsOfFree::establish_alloc_bias(ShenandoahFreeMemoryType which_set) {\n+  ShenandoahHeap* heap = ShenandoahHeap::heap();\n+  shenandoah_assert_heaplocked();\n+  assert (which_set > NotFree && which_set < NumFreeSets, \"selected free set must be valid\");\n+\n+  size_t middle = (_leftmosts[which_set] + _rightmosts[which_set]) \/ 2;\n+  size_t available_in_first_half = 0;\n+  size_t available_in_second_half = 0;\n+\n+  for (size_t index = _leftmosts[which_set]; index < middle; index++) {\n+    if (in_free_set(index, which_set)) {\n+      ShenandoahHeapRegion* r = heap->get_region(index);\n+      available_in_first_half += r->free();\n+    }\n+  }\n+  for (size_t index = middle; index <= _rightmosts[which_set]; index++) {\n+    if (in_free_set(index, which_set)) {\n+      ShenandoahHeapRegion* r = heap->get_region(index);\n+      available_in_second_half += r->free();\n+    }\n+  }\n+\n+  \/\/ We desire to first consume the sparsely distributed regions in order that the remaining regions are densely packed.\n+  \/\/ Densely packing regions reduces the effort to search for a region that has sufficient memory to satisfy a new allocation\n+  \/\/ request.  Regions become sparsely distributed following a Full GC, which tends to slide all regions to the front of the\n+  \/\/ heap rather than allowing survivor regions to remain at the high end of the heap where we intend for them to congregate.\n+\n+  \/\/ TODO: In the future, we may modify Full GC so that it slides old objects to the end of the heap and young objects to the\n+  \/\/ front of the heap. If this is done, we can always search survivor Collector and OldCollector regions right to left.\n+  _left_to_right_bias[which_set] = (available_in_second_half > available_in_first_half);\n+}\n+\n+#ifdef ASSERT\n+void ShenandoahSetsOfFree::assert_bounds() {\n+\n+  size_t leftmosts[NumFreeSets];\n+  size_t rightmosts[NumFreeSets];\n+  size_t empty_leftmosts[NumFreeSets];\n+  size_t empty_rightmosts[NumFreeSets];\n+\n+  for (int i = 0; i < NumFreeSets; i++) {\n+    leftmosts[i] = _max;\n+    empty_leftmosts[i] = _max;\n+    rightmosts[i] = 0;\n+    empty_rightmosts[i] = 0;\n+  }\n+\n+  for (size_t i = 0; i < _max; i++) {\n+    ShenandoahFreeMemoryType set = membership(i);\n+    switch (set) {\n+      case NotFree:\n+        break;\n+\n+      case Mutator:\n+      case Collector:\n+      case OldCollector:\n+      {\n+        size_t capacity = _free_set->alloc_capacity(i);\n+        bool is_empty = (capacity == _region_size_bytes);\n+        assert(capacity > 0, \"free regions must have allocation capacity\");\n+        if (i < leftmosts[set]) {\n+          leftmosts[set] = i;\n+        }\n+        if (is_empty && (i < empty_leftmosts[set])) {\n+          empty_leftmosts[set] = i;\n+        }\n+        if (i > rightmosts[set]) {\n+          rightmosts[set] = i;\n+        }\n+        if (is_empty && (i > empty_rightmosts[set])) {\n+          empty_rightmosts[set] = i;\n+        }\n+        break;\n+      }\n+\n+      case NumFreeSets:\n+      default:\n+        ShouldNotReachHere();\n+    }\n+  }\n+\n+  \/\/ Performance invariants. Failing these would not break the free set, but performance would suffer.\n+  assert (leftmost(Mutator) <= _max, \"leftmost in bounds: \"  SIZE_FORMAT \" < \" SIZE_FORMAT, leftmost(Mutator),  _max);\n+  assert (rightmost(Mutator) < _max, \"rightmost in bounds: \"  SIZE_FORMAT \" < \" SIZE_FORMAT, rightmost(Mutator),  _max);\n+\n+  assert (leftmost(Mutator) == _max || in_free_set(leftmost(Mutator), Mutator),\n+          \"leftmost region should be free: \" SIZE_FORMAT,  leftmost(Mutator));\n+  assert (leftmost(Mutator) == _max || in_free_set(rightmost(Mutator), Mutator),\n+          \"rightmost region should be free: \" SIZE_FORMAT, rightmost(Mutator));\n+\n+  \/\/ If Mutator set is empty, leftmosts will both equal max, rightmosts will both equal zero.  Likewise for empty region sets.\n+  size_t beg_off = leftmosts[Mutator];\n+  size_t end_off = rightmosts[Mutator];\n+  assert (beg_off >= leftmost(Mutator),\n+          \"free regions before the leftmost: \" SIZE_FORMAT \", bound \" SIZE_FORMAT, beg_off, leftmost(Mutator));\n+  assert (end_off <= rightmost(Mutator),\n+          \"free regions past the rightmost: \" SIZE_FORMAT \", bound \" SIZE_FORMAT,  end_off, rightmost(Mutator));\n+\n+  beg_off = empty_leftmosts[Mutator];\n+  end_off = empty_rightmosts[Mutator];\n+  assert (beg_off >= leftmost_empty(Mutator),\n+          \"free empty regions before the leftmost: \" SIZE_FORMAT \", bound \" SIZE_FORMAT, beg_off, leftmost_empty(Mutator));\n+  assert (end_off <= rightmost_empty(Mutator),\n+          \"free empty regions past the rightmost: \" SIZE_FORMAT \", bound \" SIZE_FORMAT,  end_off, rightmost_empty(Mutator));\n+\n+  \/\/ Performance invariants. Failing these would not break the free set, but performance would suffer.\n+  assert (leftmost(Collector) <= _max, \"leftmost in bounds: \"  SIZE_FORMAT \" < \" SIZE_FORMAT, leftmost(Collector),  _max);\n+  assert (rightmost(Collector) < _max, \"rightmost in bounds: \"  SIZE_FORMAT \" < \" SIZE_FORMAT, rightmost(Collector),  _max);\n+\n+  assert (leftmost(Collector) == _max || in_free_set(leftmost(Collector), Collector),\n+          \"leftmost region should be free: \" SIZE_FORMAT,  leftmost(Collector));\n+  assert (leftmost(Collector) == _max || in_free_set(rightmost(Collector), Collector),\n+          \"rightmost region should be free: \" SIZE_FORMAT, rightmost(Collector));\n+\n+  \/\/ If Collector set is empty, leftmosts will both equal max, rightmosts will both equal zero.  Likewise for empty region sets.\n+  beg_off = leftmosts[Collector];\n+  end_off = rightmosts[Collector];\n+  assert (beg_off >= leftmost(Collector),\n+          \"free regions before the leftmost: \" SIZE_FORMAT \", bound \" SIZE_FORMAT, beg_off, leftmost(Collector));\n+  assert (end_off <= rightmost(Collector),\n+          \"free regions past the rightmost: \" SIZE_FORMAT \", bound \" SIZE_FORMAT,  end_off, rightmost(Collector));\n+\n+  beg_off = empty_leftmosts[Collector];\n+  end_off = empty_rightmosts[Collector];\n+  assert (beg_off >= leftmost_empty(Collector),\n+          \"free empty regions before the leftmost: \" SIZE_FORMAT \", bound \" SIZE_FORMAT, beg_off, leftmost_empty(Collector));\n+  assert (end_off <= rightmost_empty(Collector),\n+          \"free empty regions past the rightmost: \" SIZE_FORMAT \", bound \" SIZE_FORMAT,  end_off, rightmost_empty(Collector));\n+\n+  \/\/ Performance invariants. Failing these would not break the free set, but performance would suffer.\n+  assert (leftmost(OldCollector) <= _max, \"leftmost in bounds: \"  SIZE_FORMAT \" < \" SIZE_FORMAT, leftmost(OldCollector),  _max);\n+  assert (rightmost(OldCollector) < _max, \"rightmost in bounds: \"  SIZE_FORMAT \" < \" SIZE_FORMAT, rightmost(OldCollector),  _max);\n+\n+  assert (leftmost(OldCollector) == _max || in_free_set(leftmost(OldCollector), OldCollector),\n+          \"leftmost region should be free: \" SIZE_FORMAT,  leftmost(OldCollector));\n+  assert (leftmost(OldCollector) == _max || in_free_set(rightmost(OldCollector), OldCollector),\n+          \"rightmost region should be free: \" SIZE_FORMAT, rightmost(OldCollector));\n+\n+  \/\/ If OldCollector set is empty, leftmosts will both equal max, rightmosts will both equal zero.  Likewise for empty region sets.\n+  beg_off = leftmosts[OldCollector];\n+  end_off = rightmosts[OldCollector];\n+  assert (beg_off >= leftmost(OldCollector),\n+          \"free regions before the leftmost: \" SIZE_FORMAT \", bound \" SIZE_FORMAT, beg_off, leftmost(OldCollector));\n+  assert (end_off <= rightmost(OldCollector),\n+          \"free regions past the rightmost: \" SIZE_FORMAT \", bound \" SIZE_FORMAT,  end_off, rightmost(OldCollector));\n+\n+  beg_off = empty_leftmosts[OldCollector];\n+  end_off = empty_rightmosts[OldCollector];\n+  assert (beg_off >= leftmost_empty(OldCollector),\n+          \"free empty regions before the leftmost: \" SIZE_FORMAT \", bound \" SIZE_FORMAT, beg_off, leftmost_empty(OldCollector));\n+  assert (end_off <= rightmost_empty(OldCollector),\n+          \"free empty regions past the rightmost: \" SIZE_FORMAT \", bound \" SIZE_FORMAT,  end_off, rightmost_empty(OldCollector));\n+}\n+#endif\n+\n@@ -37,3 +414,1 @@\n-  _mutator_free_bitmap(max_regions, mtGC),\n-  _collector_free_bitmap(max_regions, mtGC),\n-  _max(max_regions)\n+  _free_sets(max_regions, this)\n@@ -44,1 +419,5 @@\n-void ShenandoahFreeSet::increase_used(size_t num_bytes) {\n+\/\/ This allocates from a region within the old_collector_set.  If affiliation equals OLD, the allocation must be taken\n+\/\/ from a region that is_old().  Otherwise, affiliation should be FREE, in which case this will put a previously unaffiliated\n+\/\/ region into service.\n+HeapWord* ShenandoahFreeSet::allocate_old_with_affiliation(ShenandoahAffiliation affiliation,\n+                                                           ShenandoahAllocRequest& req, bool& in_new_region) {\n@@ -46,1 +425,0 @@\n-  _used += num_bytes;\n@@ -48,2 +426,36 @@\n-  assert(_used <= _capacity, \"must not use more than we have: used: \" SIZE_FORMAT\n-         \", capacity: \" SIZE_FORMAT \", num_bytes: \" SIZE_FORMAT, _used, _capacity, num_bytes);\n+  size_t rightmost =\n+    (affiliation == ShenandoahAffiliation::FREE)? _free_sets.rightmost_empty(OldCollector): _free_sets.rightmost(OldCollector);\n+  size_t leftmost =\n+    (affiliation == ShenandoahAffiliation::FREE)? _free_sets.leftmost_empty(OldCollector): _free_sets.leftmost(OldCollector);\n+  if (_free_sets.alloc_from_left_bias(OldCollector)) {\n+    \/\/ This mode picks up stragglers left by a full GC\n+    for (size_t idx = leftmost; idx <= rightmost; idx++) {\n+      if (_free_sets.in_free_set(idx, OldCollector)) {\n+        ShenandoahHeapRegion* r = _heap->get_region(idx);\n+        assert(r->is_trash() || !r->is_affiliated() || r->is_old(), \"old_collector_set region has bad affiliation\");\n+        if (r->affiliation() == affiliation) {\n+          HeapWord* result = try_allocate_in(r, req, in_new_region);\n+          if (result != nullptr) {\n+            return result;\n+          }\n+        }\n+      }\n+    }\n+  } else {\n+    \/\/ This mode picks up stragglers left by a previous concurrent GC\n+    for (size_t count = rightmost + 1; count > leftmost; count--) {\n+      \/\/ size_t is unsigned, need to dodge underflow when _leftmost = 0\n+      size_t idx = count - 1;\n+      if (_free_sets.in_free_set(idx, OldCollector)) {\n+        ShenandoahHeapRegion* r = _heap->get_region(idx);\n+        assert(r->is_trash() || !r->is_affiliated() || r->is_old(), \"old_collector_set region has bad affiliation\");\n+        if (r->affiliation() == affiliation) {\n+          HeapWord* result = try_allocate_in(r, req, in_new_region);\n+          if (result != nullptr) {\n+            return result;\n+          }\n+        }\n+      }\n+    }\n+  }\n+  return nullptr;\n@@ -52,4 +464,9 @@\n-bool ShenandoahFreeSet::is_mutator_free(size_t idx) const {\n-  assert (idx < _max, \"index is sane: \" SIZE_FORMAT \" < \" SIZE_FORMAT \" (left: \" SIZE_FORMAT \", right: \" SIZE_FORMAT \")\",\n-          idx, _max, _mutator_leftmost, _mutator_rightmost);\n-  return _mutator_free_bitmap.at(idx);\n+void ShenandoahFreeSet::add_old_collector_free_region(ShenandoahHeapRegion* region) {\n+  shenandoah_assert_heaplocked();\n+  size_t idx = region->index();\n+  size_t capacity = alloc_capacity(region);\n+  assert(_free_sets.membership(idx) == NotFree, \"Regions promoted in place should not be in any free set\");\n+  if (capacity >= PLAB::min_size() * HeapWordSize) {\n+    _free_sets.make_free(idx, OldCollector, capacity);\n+    _heap->augment_promo_reserve(capacity);\n+  }\n@@ -58,4 +475,23 @@\n-bool ShenandoahFreeSet::is_collector_free(size_t idx) const {\n-  assert (idx < _max, \"index is sane: \" SIZE_FORMAT \" < \" SIZE_FORMAT \" (left: \" SIZE_FORMAT \", right: \" SIZE_FORMAT \")\",\n-          idx, _max, _collector_leftmost, _collector_rightmost);\n-  return _collector_free_bitmap.at(idx);\n+HeapWord* ShenandoahFreeSet::allocate_with_affiliation(ShenandoahAffiliation affiliation,\n+                                                       ShenandoahAllocRequest& req, bool& in_new_region) {\n+  shenandoah_assert_heaplocked();\n+  size_t rightmost =\n+    (affiliation == ShenandoahAffiliation::FREE)? _free_sets.rightmost_empty(Collector): _free_sets.rightmost(Collector);\n+  size_t leftmost =\n+    (affiliation == ShenandoahAffiliation::FREE)? _free_sets.leftmost_empty(Collector): _free_sets.leftmost(Collector);\n+  for (size_t c = rightmost + 1; c > leftmost; c--) {\n+    \/\/ size_t is unsigned, need to dodge underflow when _leftmost = 0\n+    size_t idx = c - 1;\n+    if (_free_sets.in_free_set(idx, Collector)) {\n+      ShenandoahHeapRegion* r = _heap->get_region(idx);\n+      if (r->affiliation() == affiliation) {\n+        HeapWord* result = try_allocate_in(r, req, in_new_region);\n+        if (result != nullptr) {\n+          return result;\n+        }\n+      }\n+    }\n+  }\n+  log_debug(gc, free)(\"Could not allocate collector region with affiliation: %s for request \" PTR_FORMAT,\n+                      shenandoah_affiliation_name(affiliation), p2i(&req));\n+  return nullptr;\n@@ -65,0 +501,2 @@\n+  shenandoah_assert_heaplocked();\n+\n@@ -77,0 +515,27 @@\n+  \/\/ Overwrite with non-zero (non-NULL) values only if necessary for allocation bookkeeping.\n+\n+  bool allow_new_region = true;\n+  if (_heap->mode()->is_generational()) {\n+    switch (req.affiliation()) {\n+      case ShenandoahAffiliation::OLD_GENERATION:\n+        \/\/ Note: unsigned result from free_unaffiliated_regions() will never be less than zero, but it may equal zero.\n+        if (_heap->old_generation()->free_unaffiliated_regions() <= 0) {\n+          allow_new_region = false;\n+        }\n+        break;\n+\n+      case ShenandoahAffiliation::YOUNG_GENERATION:\n+        \/\/ Note: unsigned result from free_unaffiliated_regions() will never be less than zero, but it may equal zero.\n+        if (_heap->young_generation()->free_unaffiliated_regions() <= 0) {\n+          allow_new_region = false;\n+        }\n+        break;\n+\n+      case ShenandoahAffiliation::FREE:\n+        fatal(\"Should request affiliation\");\n+\n+      default:\n+        ShouldNotReachHere();\n+        break;\n+    }\n+  }\n@@ -80,1 +545,0 @@\n-\n@@ -82,5 +546,13 @@\n-      for (size_t idx = _mutator_leftmost; idx <= _mutator_rightmost; idx++) {\n-        if (is_mutator_free(idx)) {\n-          HeapWord* result = try_allocate_in(_heap->get_region(idx), req, in_new_region);\n-          if (result != nullptr) {\n-            return result;\n+      \/\/ Allocate within mutator free from high memory to low so as to preserve low memory for humongous allocations\n+      if (!_free_sets.is_empty(Mutator)) {\n+        \/\/ Use signed idx.  Otherwise, loop will never terminate.\n+        int leftmost = (int) _free_sets.leftmost(Mutator);\n+        for (int idx = (int) _free_sets.rightmost(Mutator); idx >= leftmost; idx--) {\n+          ShenandoahHeapRegion* r = _heap->get_region(idx);\n+          if (_free_sets.in_free_set(idx, Mutator) && (allow_new_region || r->is_affiliated())) {\n+            \/\/ try_allocate_in() increases used if the allocation is successful.\n+            HeapWord* result;\n+            size_t min_size = (req.type() == ShenandoahAllocRequest::_alloc_tlab)? req.min_size(): req.size();\n+            if ((alloc_capacity(r) >= min_size) && ((result = try_allocate_in(r, req, in_new_region)) != nullptr)) {\n+              return result;\n+            }\n@@ -90,1 +562,0 @@\n-\n@@ -95,2 +566,2 @@\n-    case ShenandoahAllocRequest::_alloc_shared_gc: {\n-      \/\/ size_t is unsigned, need to dodge underflow when _leftmost = 0\n+      \/\/ GCLABs are for evacuation so we must be in evacuation phase.  If this allocation is successful, increment\n+      \/\/ the relevant evac_expended rather than used value.\n@@ -98,5 +569,34 @@\n-      \/\/ Fast-path: try to allocate in the collector view first\n-      for (size_t c = _collector_rightmost + 1; c > _collector_leftmost; c--) {\n-        size_t idx = c - 1;\n-        if (is_collector_free(idx)) {\n-          HeapWord* result = try_allocate_in(_heap->get_region(idx), req, in_new_region);\n+    case ShenandoahAllocRequest::_alloc_plab:\n+      \/\/ PLABs always reside in old-gen and are only allocated during evacuation phase.\n+\n+    case ShenandoahAllocRequest::_alloc_shared_gc: {\n+      if (!_heap->mode()->is_generational()) {\n+        \/\/ size_t is unsigned, need to dodge underflow when _leftmost = 0\n+        \/\/ Fast-path: try to allocate in the collector view first\n+        for (size_t c = _free_sets.rightmost(Collector) + 1; c > _free_sets.leftmost(Collector); c--) {\n+          size_t idx = c - 1;\n+          if (_free_sets.in_free_set(idx, Collector)) {\n+            HeapWord* result = try_allocate_in(_heap->get_region(idx), req, in_new_region);\n+            if (result != nullptr) {\n+              return result;\n+            }\n+          }\n+        }\n+      } else {\n+        \/\/ First try to fit into a region that is already in use in the same generation.\n+        HeapWord* result;\n+        if (req.is_old()) {\n+          result = allocate_old_with_affiliation(req.affiliation(), req, in_new_region);\n+        } else {\n+          result = allocate_with_affiliation(req.affiliation(), req, in_new_region);\n+        }\n+        if (result != nullptr) {\n+          return result;\n+        }\n+        if (allow_new_region) {\n+          \/\/ Then try a free region that is dedicated to GC allocations.\n+          if (req.is_old()) {\n+            result = allocate_old_with_affiliation(FREE, req, in_new_region);\n+          } else {\n+            result = allocate_with_affiliation(FREE, req, in_new_region);\n+          }\n@@ -108,1 +608,0 @@\n-\n@@ -114,10 +613,35 @@\n-      \/\/ Try to steal the empty region from the mutator view\n-      for (size_t c = _mutator_rightmost + 1; c > _mutator_leftmost; c--) {\n-        size_t idx = c - 1;\n-        if (is_mutator_free(idx)) {\n-          ShenandoahHeapRegion* r = _heap->get_region(idx);\n-          if (can_allocate_from(r)) {\n-            flip_to_gc(r);\n-            HeapWord *result = try_allocate_in(r, req, in_new_region);\n-            if (result != nullptr) {\n-              return result;\n+      if (!allow_new_region && req.is_old() && (_heap->young_generation()->free_unaffiliated_regions() > 0)) {\n+        \/\/ This allows us to flip a mutator region to old_collector\n+        allow_new_region = true;\n+      }\n+\n+      \/\/ We should expand old-gen if this can prevent an old-gen evacuation failure.  We don't care so much about\n+      \/\/ promotion failures since they can be mitigated in a subsequent GC pass.  Would be nice to know if this\n+      \/\/ allocation request is for evacuation or promotion.  Individual threads limit their use of PLAB memory for\n+      \/\/ promotions, so we already have an assurance that any additional memory set aside for old-gen will be used\n+      \/\/ only for old-gen evacuations.\n+\n+      \/\/ Also TODO:\n+      \/\/ if (GC is idle (out of cycle) and mutator allocation fails and there is memory reserved in Collector\n+      \/\/ or OldCollector sets, transfer a region of memory so that we can satisfy the allocation request, and\n+      \/\/ immediately trigger the start of GC.  Is better to satisfy the allocation than to trigger out-of-cycle\n+      \/\/ allocation failure (even if this means we have a little less memory to handle evacuations during the\n+      \/\/ subsequent GC pass).\n+\n+      if (allow_new_region) {\n+        \/\/ Try to steal an empty region from the mutator view.\n+        for (size_t c = _free_sets.rightmost_empty(Mutator) + 1; c > _free_sets.leftmost_empty(Mutator); c--) {\n+          size_t idx = c - 1;\n+          if (_free_sets.in_free_set(idx, Mutator)) {\n+            ShenandoahHeapRegion* r = _heap->get_region(idx);\n+            if (can_allocate_from(r)) {\n+              if (req.is_old()) {\n+                flip_to_old_gc(r);\n+              } else {\n+                flip_to_gc(r);\n+              }\n+              HeapWord *result = try_allocate_in(r, req, in_new_region);\n+              if (result != nullptr) {\n+                log_debug(gc, free)(\"Flipped region \" SIZE_FORMAT \" to gc for request: \" PTR_FORMAT, idx, p2i(&req));\n+                return result;\n+              }\n@@ -132,1 +656,0 @@\n-\n@@ -138,1 +661,0 @@\n-\n@@ -142,2 +664,48 @@\n-HeapWord* ShenandoahFreeSet::try_allocate_in(ShenandoahHeapRegion* r, ShenandoahAllocRequest& req, bool& in_new_region) {\n-  assert (!has_no_alloc_capacity(r), \"Performance: should avoid full regions on this path: \" SIZE_FORMAT, r->index());\n+\/\/ This work method takes an argument corresponding to the number of bytes\n+\/\/ free in a region, and returns the largest amount in heapwords that can be allocated\n+\/\/ such that both of the following conditions are satisfied:\n+\/\/\n+\/\/ 1. it is a multiple of card size\n+\/\/ 2. any remaining shard may be filled with a filler object\n+\/\/\n+\/\/ The idea is that the allocation starts and ends at card boundaries. Because\n+\/\/ a region ('s end) is card-aligned, the remainder shard that must be filled is\n+\/\/ at the start of the free space.\n+\/\/\n+\/\/ This is merely a helper method to use for the purpose of such a calculation.\n+size_t get_usable_free_words(size_t free_bytes) {\n+  \/\/ e.g. card_size is 512, card_shift is 9, min_fill_size() is 8\n+  \/\/      free is 514\n+  \/\/      usable_free is 512, which is decreased to 0\n+  size_t usable_free = (free_bytes \/ CardTable::card_size()) << CardTable::card_shift();\n+  assert(usable_free <= free_bytes, \"Sanity check\");\n+  if ((free_bytes != usable_free) && (free_bytes - usable_free < ShenandoahHeap::min_fill_size() * HeapWordSize)) {\n+    \/\/ After aligning to card multiples, the remainder would be smaller than\n+    \/\/ the minimum filler object, so we'll need to take away another card's\n+    \/\/ worth to construct a filler object.\n+    if (usable_free >= CardTable::card_size()) {\n+      usable_free -= CardTable::card_size();\n+    } else {\n+      assert(usable_free == 0, \"usable_free is a multiple of card_size and card_size > min_fill_size\");\n+    }\n+  }\n+\n+  return usable_free \/ HeapWordSize;\n+}\n+\n+\/\/ Given a size argument, which is a multiple of card size, a request struct\n+\/\/ for a PLAB, and an old region, return a pointer to the allocated space for\n+\/\/ a PLAB which is card-aligned and where any remaining shard in the region\n+\/\/ has been suitably filled by a filler object.\n+\/\/ It is assumed (and assertion-checked) that such an allocation is always possible.\n+HeapWord* ShenandoahFreeSet::allocate_aligned_plab(size_t size, ShenandoahAllocRequest& req, ShenandoahHeapRegion* r) {\n+  assert(_heap->mode()->is_generational(), \"PLABs are only for generational mode\");\n+  assert(r->is_old(), \"All PLABs reside in old-gen\");\n+  assert(!req.is_mutator_alloc(), \"PLABs should not be allocated by mutators.\");\n+  assert(size % CardTable::card_size_in_words() == 0, \"size must be multiple of card table size, was \" SIZE_FORMAT, size);\n+\n+  HeapWord* result = r->allocate_aligned(size, req, CardTable::card_size());\n+  assert(result != nullptr, \"Allocation cannot fail\");\n+  assert(r->top() <= r->end(), \"Allocation cannot span end of region\");\n+  assert(req.actual_size() == size, \"Should not have needed to adjust size for PLAB.\");\n+  assert(((uintptr_t) result) % CardTable::card_size_in_words() == 0, \"PLAB start must align with card boundary\");\n@@ -145,2 +713,6 @@\n-  if (_heap->is_concurrent_weak_root_in_progress() &&\n-      r->is_trash()) {\n+  return result;\n+}\n+\n+HeapWord* ShenandoahFreeSet::try_allocate_in(ShenandoahHeapRegion* r, ShenandoahAllocRequest& req, bool& in_new_region) {\n+  assert (has_alloc_capacity(r), \"Performance: should avoid full regions on this path: \" SIZE_FORMAT, r->index());\n+  if (_heap->is_concurrent_weak_root_in_progress() && r->is_trash()) {\n@@ -151,0 +723,1 @@\n+<<<<<<< HEAD\n@@ -161,0 +734,17 @@\n+=======\n+  if (!r->is_affiliated()) {\n+    ShenandoahMarkingContext* const ctx = _heap->complete_marking_context();\n+    r->set_affiliation(req.affiliation());\n+    if (r->is_old()) {\n+      \/\/ Any OLD region allocated during concurrent coalesce-and-fill does not need to be coalesced and filled because\n+      \/\/ all objects allocated within this region are above TAMS (and thus are implicitly marked).  In case this is an\n+      \/\/ OLD region and concurrent preparation for mixed evacuations visits this region before the start of the next\n+      \/\/ old-gen concurrent mark (i.e. this region is allocated following the start of old-gen concurrent mark but before\n+      \/\/ concurrent preparations for mixed evacuations are completed), we mark this region as not requiring any\n+      \/\/ coalesce-and-fill processing.\n+      r->end_preemptible_coalesce_and_fill();\n+      _heap->clear_cards_for(r);\n+      _heap->old_generation()->increment_affiliated_region_count();\n+    } else {\n+      _heap->young_generation()->increment_affiliated_region_count();\n+>>>>>>> 666656136f7f15905ae770888951d832f46b0535\n@@ -162,3 +752,60 @@\n-    if (size >= req.min_size()) {\n-      result = r->allocate(size, req.type());\n-      assert (result != nullptr, \"Allocation must succeed: free \" SIZE_FORMAT \", actual \" SIZE_FORMAT, free, size);\n+\n+    assert(ctx->top_at_mark_start(r) == r->bottom(), \"Newly established allocation region starts with TAMS equal to bottom\");\n+    assert(ctx->is_bitmap_clear_range(ctx->top_bitmap(r), r->end()), \"Bitmap above top_bitmap() must be clear\");\n+  } else if (r->affiliation() != req.affiliation()) {\n+    assert(_heap->mode()->is_generational(), \"Request for %s from %s region should only happen in generational mode.\",\n+           req.affiliation_name(), r->affiliation_name());\n+    return nullptr;\n+  }\n+\n+  in_new_region = r->is_empty();\n+  HeapWord* result = nullptr;\n+\n+  if (in_new_region) {\n+    log_debug(gc, free)(\"Using new region (\" SIZE_FORMAT \") for %s (\" PTR_FORMAT \").\",\n+                       r->index(), ShenandoahAllocRequest::alloc_type_to_string(req.type()), p2i(&req));\n+  }\n+\n+  \/\/ req.size() is in words, r->free() is in bytes.\n+  if (ShenandoahElasticTLAB && req.is_lab_alloc()) {\n+    if (req.type() == ShenandoahAllocRequest::_alloc_plab) {\n+      assert(_heap->mode()->is_generational(), \"PLABs are only for generational mode\");\n+      assert(_free_sets.in_free_set(r->index(), OldCollector), \"PLABS must be allocated in old_collector_free regions\");\n+      \/\/ Need to assure that plabs are aligned on multiple of card region.\n+      \/\/ Since we have Elastic TLABs, align sizes up. They may be decreased to fit in the usable\n+      \/\/ memory remaining in the region (which will also be aligned to cards).\n+      size_t adjusted_size = align_up(req.size(), CardTable::card_size_in_words());\n+      size_t adjusted_min_size = align_up(req.min_size(), CardTable::card_size_in_words());\n+      size_t usable_free = get_usable_free_words(r->free());\n+\n+      if (adjusted_size > usable_free) {\n+        adjusted_size = usable_free;\n+      }\n+\n+      if (adjusted_size >= adjusted_min_size) {\n+        result = allocate_aligned_plab(adjusted_size, req, r);\n+      }\n+      \/\/ Otherwise, leave result == nullptr because the adjusted size is smaller than min size.\n+    } else {\n+      \/\/ This is a GCLAB or a TLAB allocation\n+      size_t adjusted_size = req.size();\n+      size_t free = align_down(r->free() >> LogHeapWordSize, MinObjAlignment);\n+      if (adjusted_size > free) {\n+        adjusted_size = free;\n+      }\n+      if (adjusted_size >= req.min_size()) {\n+        result = r->allocate(adjusted_size, req);\n+        assert (result != nullptr, \"Allocation must succeed: free \" SIZE_FORMAT \", actual \" SIZE_FORMAT, free, adjusted_size);\n+        req.set_actual_size(adjusted_size);\n+      } else {\n+        log_trace(gc, free)(\"Failed to shrink TLAB or GCLAB request (\" SIZE_FORMAT \") in region \" SIZE_FORMAT \" to \" SIZE_FORMAT\n+                           \" because min_size() is \" SIZE_FORMAT, req.size(), r->index(), adjusted_size, req.min_size());\n+      }\n+    }\n+  } else if (req.is_lab_alloc() && req.type() == ShenandoahAllocRequest::_alloc_plab) {\n+\n+    \/\/ inelastic PLAB\n+    size_t size = req.size();\n+    size_t usable_free = get_usable_free_words(r->free());\n+    if (size <= usable_free) {\n+      result = allocate_aligned_plab(size, req, r);\n@@ -167,1 +814,6 @@\n-    result = r->allocate(size, req.type());\n+    size_t size = req.size();\n+    result = r->allocate(size, req);\n+    if (result != nullptr) {\n+      \/\/ Record actual allocation size\n+      req.set_actual_size(size);\n+    }\n@@ -170,0 +822,1 @@\n+  ShenandoahGeneration* generation = _heap->generation_for(req.affiliation());\n@@ -173,7 +826,12 @@\n-      increase_used(size * HeapWordSize);\n-    }\n-\n-    \/\/ Record actual allocation size\n-    req.set_actual_size(size);\n-\n-    if (req.is_gc_alloc()) {\n+      assert(req.is_young(), \"Mutator allocations always come from young generation.\");\n+      _free_sets.increase_used(Mutator, req.actual_size() * HeapWordSize);\n+    } else {\n+      assert(req.is_gc_alloc(), \"Should be gc_alloc since req wasn't mutator alloc\");\n+\n+      \/\/ For GC allocations, we advance update_watermark because the objects relocated into this memory during\n+      \/\/ evacuation are not updated during evacuation.  For both young and old regions r, it is essential that all\n+      \/\/ PLABs be made parsable at the end of evacuation.  This is enabled by retiring all plabs at end of evacuation.\n+      \/\/ TODO: Making a PLAB parsable involves placing a filler object in its remnant memory but does not require\n+      \/\/ that the PLAB be disabled for all future purposes.  We may want to introduce a new service to make the\n+      \/\/ PLABs parsable while still allowing the PLAB to serve future allocation requests that arise during the\n+      \/\/ next evacuation pass.\n@@ -181,0 +839,4 @@\n+      if (r->is_old()) {\n+        assert(req.type() != ShenandoahAllocRequest::_alloc_gclab, \"old-gen allocations use PLAB or shared allocation\");\n+        \/\/ for plabs, we'll sort the difference between evac and promotion usage when we retire the plab\n+      }\n@@ -184,2 +846,2 @@\n-  if (result == nullptr || has_no_alloc_capacity(r)) {\n-    \/\/ Region cannot afford this or future allocations. Retire it.\n+  if (result == nullptr || alloc_capacity(r) < PLAB::min_size() * HeapWordSize) {\n+    \/\/ Region cannot afford this and is likely to not afford future allocations. Retire it.\n@@ -188,3 +850,2 @@\n-    \/\/ fit, but the next small one would, we are risking to inflate scan times when lots of\n-    \/\/ almost-full regions precede the fully-empty region where we want allocate the entire TLAB.\n-    \/\/ TODO: Record first fully-empty region, and use that for large allocations\n+    \/\/ fit but the next small one would, we are risking to inflate scan times when lots of\n+    \/\/ almost-full regions precede the fully-empty region where we want to allocate the entire TLAB.\n@@ -193,0 +854,1 @@\n+    size_t idx = r->index();\n@@ -196,2 +858,3 @@\n-        increase_used(waste);\n-        _heap->notify_mutator_alloc_words(waste >> LogHeapWordSize, true);\n+        _free_sets.increase_used(Mutator, waste);\n+        \/\/ This one request could cause several regions to be \"retired\", so we must accumulate the waste\n+        req.set_waste((waste >> LogHeapWordSize) + req.waste());\n@@ -199,0 +862,4 @@\n+      assert(_free_sets.membership(idx) == Mutator, \"Must be mutator free: \" SIZE_FORMAT, idx);\n+    } else {\n+      assert(_free_sets.membership(idx) == Collector || _free_sets.membership(idx) == OldCollector,\n+             \"Must be collector or old-collector free: \" SIZE_FORMAT, idx);\n@@ -200,9 +867,3 @@\n-\n-    size_t num = r->index();\n-    _collector_free_bitmap.clear_bit(num);\n-    _mutator_free_bitmap.clear_bit(num);\n-    \/\/ Touched the bounds? Need to update:\n-    if (touches_bounds(num)) {\n-      adjust_bounds();\n-    }\n-    assert_bounds();\n+    \/\/ This region is no longer considered free (in any set)\n+    _free_sets.remove_from_free_sets(idx);\n+    _free_sets.assert_bounds();\n@@ -213,32 +874,0 @@\n-bool ShenandoahFreeSet::touches_bounds(size_t num) const {\n-  return num == _collector_leftmost || num == _collector_rightmost || num == _mutator_leftmost || num == _mutator_rightmost;\n-}\n-\n-void ShenandoahFreeSet::recompute_bounds() {\n-  \/\/ Reset to the most pessimistic case:\n-  _mutator_rightmost = _max - 1;\n-  _mutator_leftmost = 0;\n-  _collector_rightmost = _max - 1;\n-  _collector_leftmost = 0;\n-\n-  \/\/ ...and adjust from there\n-  adjust_bounds();\n-}\n-\n-void ShenandoahFreeSet::adjust_bounds() {\n-  \/\/ Rewind both mutator bounds until the next bit.\n-  while (_mutator_leftmost < _max && !is_mutator_free(_mutator_leftmost)) {\n-    _mutator_leftmost++;\n-  }\n-  while (_mutator_rightmost > 0 && !is_mutator_free(_mutator_rightmost)) {\n-    _mutator_rightmost--;\n-  }\n-  \/\/ Rewind both collector bounds until the next bit.\n-  while (_collector_leftmost < _max && !is_collector_free(_collector_leftmost)) {\n-    _collector_leftmost++;\n-  }\n-  while (_collector_rightmost > 0 && !is_collector_free(_collector_rightmost)) {\n-    _collector_rightmost--;\n-  }\n-}\n-\n@@ -251,3 +880,13 @@\n-  \/\/ No regions left to satisfy allocation, bye.\n-  if (num > mutator_count()) {\n-    return nullptr;\n+  assert(req.is_young(), \"Humongous regions always allocated in YOUNG\");\n+  ShenandoahGeneration* generation = _heap->generation_for(req.affiliation());\n+\n+  \/\/ Check if there are enough regions left to satisfy allocation.\n+  if (_heap->mode()->is_generational()) {\n+    size_t avail_young_regions = generation->free_unaffiliated_regions();\n+    if (num > _free_sets.count(Mutator) || (num > avail_young_regions)) {\n+      return nullptr;\n+    }\n+  } else {\n+    if (num > _free_sets.count(Mutator)) {\n+      return nullptr;\n+    }\n@@ -259,1 +898,1 @@\n-  size_t beg = _mutator_leftmost;\n+  size_t beg = _free_sets.leftmost(Mutator);\n@@ -263,1 +902,1 @@\n-    if (end >= _max) {\n+    if (end >= _free_sets.max()) {\n@@ -270,1 +909,1 @@\n-    if (!is_mutator_free(end) || !can_allocate_from(_heap->get_region(end))) {\n+    if (!_free_sets.in_free_set(end, Mutator) || !can_allocate_from(_heap->get_region(end))) {\n@@ -285,0 +924,1 @@\n+  ShenandoahMarkingContext* const ctx = _heap->complete_marking_context();\n@@ -308,0 +948,2 @@\n+    r->set_affiliation(req.affiliation());\n+    r->set_update_watermark(r->bottom());\n@@ -310,1 +952,2 @@\n-    _mutator_free_bitmap.clear_bit(r->index());\n+    \/\/ While individual regions report their true use, all humongous regions are marked used in the free set.\n+    _free_sets.remove_from_free_sets(r->index());\n@@ -312,0 +955,1 @@\n+  _heap->young_generation()->increase_affiliated_region_count(num);\n@@ -313,4 +957,4 @@\n-  \/\/ While individual regions report their true use, all humongous regions are\n-  \/\/ marked used in the free set.\n-  increase_used(ShenandoahHeapRegion::region_size_bytes() * num);\n-\n+  size_t total_humongous_size = ShenandoahHeapRegion::region_size_bytes() * num;\n+  _free_sets.increase_used(Mutator, total_humongous_size);\n+  _free_sets.assert_bounds();\n+  req.set_actual_size(words_size);\n@@ -318,7 +962,1 @@\n-    \/\/ Record this remainder as allocation waste\n-    _heap->notify_mutator_alloc_words(ShenandoahHeapRegion::region_size_words() - remainder, true);\n-  }\n-\n-  \/\/ Allocated at left\/rightmost? Move the bounds appropriately.\n-  if (beg == _mutator_leftmost || end == _mutator_rightmost) {\n-    adjust_bounds();\n+    req.set_waste(ShenandoahHeapRegion::region_size_words() - remainder);\n@@ -326,3 +964,0 @@\n-  assert_bounds();\n-\n-  req.set_actual_size(words_size);\n@@ -332,1 +967,4 @@\n-bool ShenandoahFreeSet::can_allocate_from(ShenandoahHeapRegion *r) {\n+\/\/ Returns true iff this region is entirely available, either because it is empty() or because it has been found to represent\n+\/\/ immediate trash and we'll be able to immediately recycle it.  Note that we cannot recycle immediate trash if\n+\/\/ concurrent weak root processing is in progress.\n+bool ShenandoahFreeSet::can_allocate_from(ShenandoahHeapRegion *r) const {\n@@ -336,1 +974,11 @@\n-size_t ShenandoahFreeSet::alloc_capacity(ShenandoahHeapRegion *r) {\n+bool ShenandoahFreeSet::can_allocate_from(size_t idx) const {\n+  ShenandoahHeapRegion* r = _heap->get_region(idx);\n+  return can_allocate_from(r);\n+}\n+\n+size_t ShenandoahFreeSet::alloc_capacity(size_t idx) const {\n+  ShenandoahHeapRegion* r = _heap->get_region(idx);\n+  return alloc_capacity(r);\n+}\n+\n+size_t ShenandoahFreeSet::alloc_capacity(ShenandoahHeapRegion *r) const {\n@@ -345,2 +993,2 @@\n-bool ShenandoahFreeSet::has_no_alloc_capacity(ShenandoahHeapRegion *r) {\n-  return alloc_capacity(r) == 0;\n+bool ShenandoahFreeSet::has_alloc_capacity(ShenandoahHeapRegion *r) const {\n+  return alloc_capacity(r) > 0;\n@@ -351,1 +999,0 @@\n-    _heap->decrease_used(r->used());\n@@ -370,1 +1017,1 @@\n-void ShenandoahFreeSet::flip_to_gc(ShenandoahHeapRegion* r) {\n+void ShenandoahFreeSet::flip_to_old_gc(ShenandoahHeapRegion* r) {\n@@ -373,1 +1020,2 @@\n-  assert(_mutator_free_bitmap.at(idx), \"Should be in mutator view\");\n+  assert(_free_sets.in_free_set(idx, Mutator), \"Should be in mutator view\");\n+  \/\/ Note: can_allocate_from(r) means r is entirely empty\n@@ -376,4 +1024,18 @@\n-  _mutator_free_bitmap.clear_bit(idx);\n-  _collector_free_bitmap.set_bit(idx);\n-  _collector_leftmost = MIN2(idx, _collector_leftmost);\n-  _collector_rightmost = MAX2(idx, _collector_rightmost);\n+  size_t region_capacity = alloc_capacity(r);\n+  _free_sets.move_to_set(idx, OldCollector, region_capacity);\n+  _free_sets.assert_bounds();\n+  _heap->augment_old_evac_reserve(region_capacity);\n+  bool transferred = _heap->generation_sizer()->transfer_to_old(1);\n+  if (!transferred) {\n+    log_warning(gc, free)(\"Forcing transfer of \" SIZE_FORMAT \" to old reserve.\", idx);\n+    _heap->generation_sizer()->force_transfer_to_old(1);\n+  }\n+  \/\/ We do not ensure that the region is no longer trash, relying on try_allocate_in(), which always comes next,\n+  \/\/ to recycle trash before attempting to allocate anything in the region.\n+}\n+\n+void ShenandoahFreeSet::flip_to_gc(ShenandoahHeapRegion* r) {\n+  size_t idx = r->index();\n+\n+  assert(_free_sets.in_free_set(idx, Mutator), \"Should be in mutator view\");\n+  assert(can_allocate_from(r), \"Should not be allocated\");\n@@ -381,1 +1043,3 @@\n-  _capacity -= alloc_capacity(r);\n+  size_t region_capacity = alloc_capacity(r);\n+  _free_sets.move_to_set(idx, Collector, region_capacity);\n+  _free_sets.assert_bounds();\n@@ -383,4 +1047,2 @@\n-  if (touches_bounds(idx)) {\n-    adjust_bounds();\n-  }\n-  assert_bounds();\n+  \/\/ We do not ensure that the region is no longer trash, relying on try_allocate_in(), which always comes next,\n+  \/\/ to recycle trash before attempting to allocate anything in the region.\n@@ -395,8 +1057,1 @@\n-  _mutator_free_bitmap.clear();\n-  _collector_free_bitmap.clear();\n-  _mutator_leftmost = _max;\n-  _mutator_rightmost = 0;\n-  _collector_leftmost = _max;\n-  _collector_rightmost = 0;\n-  _capacity = 0;\n-  _used = 0;\n+  _free_sets.clear_all();\n@@ -405,4 +1060,13 @@\n-void ShenandoahFreeSet::rebuild() {\n-  shenandoah_assert_heaplocked();\n-  clear();\n-\n+\/\/ This function places all is_old() regions that have allocation capacity into the old_collector set.  It places\n+\/\/ all other regions (not is_old()) that have allocation capacity into the mutator_set.  Subsequently, we will\n+\/\/ move some of the mutator regions into the collector set or old_collector set with the intent of packing\n+\/\/ old_collector memory into the highest (rightmost) addresses of the heap and the collector memory into the\n+\/\/ next highest addresses of the heap, with mutator memory consuming the lowest addresses of the heap.\n+void ShenandoahFreeSet::find_regions_with_alloc_capacity(size_t &young_cset_regions, size_t &old_cset_regions,\n+                                                         size_t &first_old_region, size_t &last_old_region,\n+                                                         size_t &old_region_count) {\n+  first_old_region = _heap->num_regions();\n+  last_old_region = 0;\n+  old_region_count = 0;\n+  old_cset_regions = 0;\n+  young_cset_regions = 0;\n@@ -411,0 +1075,15 @@\n+    if (region->is_trash()) {\n+      \/\/ Trashed regions represent regions that had been in the collection set but have not yet been \"cleaned up\".\n+      if (region->is_old()) {\n+        old_cset_regions++;\n+      } else {\n+        assert(region->is_young(), \"Trashed region should be old or young\");\n+        young_cset_regions++;\n+      }\n+    } else if (region->is_old() && region->is_regular()) {\n+      old_region_count++;\n+      if (first_old_region > idx) {\n+        first_old_region = idx;\n+      }\n+      last_old_region = idx;\n+    }\n@@ -412,1 +1091,22 @@\n-      assert(!region->is_cset(), \"Shouldn't be adding those to the free set\");\n+      assert(!region->is_cset(), \"Shouldn't be adding cset regions to the free set\");\n+      assert(_free_sets.in_free_set(idx, NotFree), \"We are about to make region free; it should not be free already\");\n+\n+      \/\/ Do not add regions that would almost surely fail allocation\n+      if (alloc_capacity(region) < PLAB::min_size() * HeapWordSize) continue;\n+\n+      if (region->is_old()) {\n+        _free_sets.make_free(idx, OldCollector, alloc_capacity(region));\n+        log_debug(gc, free)(\n+          \"  Adding Region \" SIZE_FORMAT  \" (Free: \" SIZE_FORMAT \"%s, Used: \" SIZE_FORMAT \"%s) to old collector set\",\n+          idx, byte_size_in_proper_unit(region->free()), proper_unit_for_byte_size(region->free()),\n+          byte_size_in_proper_unit(region->used()), proper_unit_for_byte_size(region->used()));\n+      } else {\n+        _free_sets.make_free(idx, Mutator, alloc_capacity(region));\n+        log_debug(gc, free)(\n+          \"  Adding Region \" SIZE_FORMAT \" (Free: \" SIZE_FORMAT \"%s, Used: \" SIZE_FORMAT \"%s) to mutator set\",\n+          idx, byte_size_in_proper_unit(region->free()), proper_unit_for_byte_size(region->free()),\n+          byte_size_in_proper_unit(region->used()), proper_unit_for_byte_size(region->used()));\n+      }\n+    }\n+  }\n+}\n@@ -414,2 +1114,20 @@\n-      \/\/ Do not add regions that would surely fail allocation\n-      if (has_no_alloc_capacity(region)) continue;\n+\/\/ Move no more than cset_regions from the existing Collector and OldCollector free sets to the Mutator free set.\n+\/\/ This is called from outside the heap lock.\n+void ShenandoahFreeSet::move_collector_sets_to_mutator(size_t max_xfer_regions) {\n+  size_t region_size_bytes = ShenandoahHeapRegion::region_size_bytes();\n+  size_t collector_empty_xfer = 0;\n+  size_t collector_not_empty_xfer = 0;\n+  size_t old_collector_empty_xfer = 0;\n+\n+  \/\/ Process empty regions within the Collector free set\n+  if ((max_xfer_regions > 0) && (_free_sets.leftmost_empty(Collector) <= _free_sets.rightmost_empty(Collector))) {\n+    ShenandoahHeapLocker locker(_heap->lock());\n+    for (size_t idx = _free_sets.leftmost_empty(Collector);\n+         (max_xfer_regions > 0) && (idx <= _free_sets.rightmost_empty(Collector)); idx++) {\n+      if (_free_sets.in_free_set(idx, Collector) && can_allocate_from(idx)) {\n+        _free_sets.move_to_set(idx, Mutator, region_size_bytes);\n+        max_xfer_regions--;\n+        collector_empty_xfer += region_size_bytes;\n+      }\n+    }\n+  }\n@@ -417,2 +1135,17 @@\n-      _capacity += alloc_capacity(region);\n-      assert(_used <= _capacity, \"must not use more than we have\");\n+  \/\/ Process empty regions within the OldCollector free set\n+  size_t old_collector_regions = 0;\n+  if ((max_xfer_regions > 0) && (_free_sets.leftmost_empty(OldCollector) <= _free_sets.rightmost_empty(OldCollector))) {\n+    ShenandoahHeapLocker locker(_heap->lock());\n+    for (size_t idx = _free_sets.leftmost_empty(OldCollector);\n+         (max_xfer_regions > 0) && (idx <= _free_sets.rightmost_empty(OldCollector)); idx++) {\n+      if (_free_sets.in_free_set(idx, OldCollector) && can_allocate_from(idx)) {\n+        _free_sets.move_to_set(idx, Mutator, region_size_bytes);\n+        max_xfer_regions--;\n+        old_collector_empty_xfer += region_size_bytes;\n+        old_collector_regions++;\n+      }\n+    }\n+    if (old_collector_regions > 0) {\n+      _heap->generation_sizer()->transfer_to_young(old_collector_regions);\n+    }\n+  }\n@@ -420,2 +1153,10 @@\n-      assert(!is_mutator_free(idx), \"We are about to add it, it shouldn't be there already\");\n-      _mutator_free_bitmap.set_bit(idx);\n+  \/\/ If there are any non-empty regions within Collector set, we can also move them to the Mutator free set\n+  if ((max_xfer_regions > 0) && (_free_sets.leftmost(Collector) <= _free_sets.rightmost(Collector))) {\n+    ShenandoahHeapLocker locker(_heap->lock());\n+    for (size_t idx = _free_sets.leftmost(Collector); (max_xfer_regions > 0) && (idx <= _free_sets.rightmost(Collector)); idx++) {\n+      size_t alloc_capacity = this->alloc_capacity(idx);\n+      if (_free_sets.in_free_set(idx, Collector) && (alloc_capacity > 0)) {\n+        _free_sets.move_to_set(idx, Mutator, alloc_capacity);\n+        max_xfer_regions--;\n+        collector_not_empty_xfer += alloc_capacity;\n+      }\n@@ -425,3 +1166,8 @@\n-  \/\/ Evac reserve: reserve trailing space for evacuations\n-  size_t to_reserve = _heap->max_capacity() \/ 100 * ShenandoahEvacReserve;\n-  size_t reserved = 0;\n+  size_t collector_xfer = collector_empty_xfer + collector_not_empty_xfer;\n+  size_t total_xfer = collector_xfer + old_collector_empty_xfer;\n+  log_info(gc, free)(\"At start of update refs, moving \" SIZE_FORMAT \"%s to Mutator free set from Collector Reserve (\"\n+                     SIZE_FORMAT \"%s) and from Old Collector Reserve (\" SIZE_FORMAT \"%s)\",\n+                     byte_size_in_proper_unit(total_xfer), proper_unit_for_byte_size(total_xfer),\n+                     byte_size_in_proper_unit(collector_xfer), proper_unit_for_byte_size(collector_xfer),\n+                     byte_size_in_proper_unit(old_collector_empty_xfer), proper_unit_for_byte_size(old_collector_empty_xfer));\n+}\n@@ -429,2 +1175,0 @@\n-  for (size_t idx = _heap->num_regions() - 1; idx > 0; idx--) {\n-    if (reserved >= to_reserve) break;\n@@ -432,7 +1176,141 @@\n-    ShenandoahHeapRegion* region = _heap->get_region(idx);\n-    if (_mutator_free_bitmap.at(idx) && can_allocate_from(region)) {\n-      _mutator_free_bitmap.clear_bit(idx);\n-      _collector_free_bitmap.set_bit(idx);\n-      size_t ac = alloc_capacity(region);\n-      _capacity -= ac;\n-      reserved += ac;\n+\/\/ Overwrite arguments to represent the amount of memory in each generation that is about to be recycled\n+void ShenandoahFreeSet::prepare_to_rebuild(size_t &young_cset_regions, size_t &old_cset_regions,\n+                                           size_t &first_old_region, size_t &last_old_region, size_t &old_region_count) {\n+  shenandoah_assert_heaplocked();\n+  \/\/ This resets all state information, removing all regions from all sets.\n+  clear();\n+  log_debug(gc, free)(\"Rebuilding FreeSet\");\n+\n+  \/\/ This places regions that have alloc_capacity into the old_collector set if they identify as is_old() or the\n+  \/\/ mutator set otherwise.\n+  find_regions_with_alloc_capacity(young_cset_regions, old_cset_regions, first_old_region, last_old_region, old_region_count);\n+}\n+\n+void ShenandoahFreeSet::rebuild(size_t young_cset_regions, size_t old_cset_regions) {\n+  shenandoah_assert_heaplocked();\n+  size_t young_reserve, old_reserve;\n+  size_t region_size_bytes = ShenandoahHeapRegion::region_size_bytes();\n+\n+  size_t old_capacity = _heap->old_generation()->max_capacity();\n+  size_t old_available = _heap->old_generation()->available();\n+  size_t old_unaffiliated_regions = _heap->old_generation()->free_unaffiliated_regions();\n+  size_t young_capacity = _heap->young_generation()->max_capacity();\n+  size_t young_available = _heap->young_generation()->available();\n+  size_t young_unaffiliated_regions = _heap->young_generation()->free_unaffiliated_regions();\n+\n+  old_unaffiliated_regions += old_cset_regions;\n+  old_available += old_cset_regions * region_size_bytes;\n+  young_unaffiliated_regions += young_cset_regions;\n+  young_available += young_cset_regions * region_size_bytes;\n+\n+  \/\/ Consult old-region surplus and deficit to make adjustments to current generation capacities and availability.\n+  \/\/ The generation region transfers take place after we rebuild.\n+  size_t old_region_surplus = _heap->get_old_region_surplus();\n+  size_t old_region_deficit = _heap->get_old_region_deficit();\n+\n+  if (old_region_surplus > 0) {\n+    size_t xfer_bytes = old_region_surplus * region_size_bytes;\n+    assert(old_region_surplus <= old_unaffiliated_regions, \"Cannot transfer regions that are affiliated\");\n+    old_capacity -= xfer_bytes;\n+    old_available -= xfer_bytes;\n+    old_unaffiliated_regions -= old_region_surplus;\n+    young_capacity += xfer_bytes;\n+    young_available += xfer_bytes;\n+    young_unaffiliated_regions += old_region_surplus;\n+  } else if (old_region_deficit > 0) {\n+    size_t xfer_bytes = old_region_deficit * region_size_bytes;\n+    assert(old_region_deficit <= young_unaffiliated_regions, \"Cannot transfer regions that are affiliated\");\n+    old_capacity += xfer_bytes;\n+    old_available += xfer_bytes;\n+    old_unaffiliated_regions += old_region_deficit;\n+    young_capacity -= xfer_bytes;\n+    young_available -= xfer_bytes;\n+    young_unaffiliated_regions -= old_region_deficit;\n+  }\n+\n+  \/\/ Evac reserve: reserve trailing space for evacuations, with regions reserved for old evacuations placed to the right\n+  \/\/ of regions reserved of young evacuations.\n+  if (!_heap->mode()->is_generational()) {\n+    young_reserve = (_heap->max_capacity() \/ 100) * ShenandoahEvacReserve;\n+    old_reserve = 0;\n+  } else {\n+    \/\/ All allocations taken from the old collector set are performed by GC, generally using PLABs for both\n+    \/\/ promotions and evacuations.  The partition between which old memory is reserved for evacuation and\n+    \/\/ which is reserved for promotion is enforced using thread-local variables that prescribe intentons for\n+    \/\/ each PLAB's available memory.\n+    if (_heap->has_evacuation_reserve_quantities()) {\n+      \/\/ We are rebuilding at the end of final mark, having already established evacuation budgets for this GC pass.\n+      young_reserve = _heap->get_young_evac_reserve();\n+      old_reserve = _heap->get_promoted_reserve() + _heap->get_old_evac_reserve();\n+      assert(old_reserve <= old_available,\n+             \"Cannot reserve (\" SIZE_FORMAT \" + \" SIZE_FORMAT\") more OLD than is available: \" SIZE_FORMAT,\n+             _heap->get_promoted_reserve(), _heap->get_old_evac_reserve(), old_available);\n+    } else {\n+      \/\/ We are rebuilding at end of GC, so we set aside budgets specified on command line (or defaults)\n+      young_reserve = (young_capacity * ShenandoahEvacReserve) \/ 100;\n+      \/\/ The auto-sizer has already made old-gen large enough to hold all anticipated evacuations and promotions.\n+      \/\/ Affiliated old-gen regions are already in the OldCollector free set.  Add in the relevant number of\n+      \/\/ unaffiliated regions.\n+      old_reserve = old_available;\n+    }\n+  }\n+\n+  \/\/ Old available regions that have less than PLAB::min_size() of available memory are not placed into the OldCollector\n+  \/\/ free set.  Because of this, old_available may not have enough memory to represent the intended reserve.  Adjust\n+  \/\/ the reserve downward to account for this possibility. This loss is part of the reason why the original budget\n+  \/\/ was adjusted with ShenandoahOldEvacWaste and ShenandoahOldPromoWaste multipliers.\n+  if (old_reserve > _free_sets.capacity_of(OldCollector) + old_unaffiliated_regions * region_size_bytes) {\n+    old_reserve = _free_sets.capacity_of(OldCollector) + old_unaffiliated_regions * region_size_bytes;\n+  }\n+\n+  if (young_reserve > young_unaffiliated_regions * region_size_bytes) {\n+    young_reserve = young_unaffiliated_regions * region_size_bytes;\n+  }\n+\n+  reserve_regions(young_reserve, old_reserve);\n+  _free_sets.establish_alloc_bias(OldCollector);\n+  _free_sets.assert_bounds();\n+  log_status();\n+}\n+\n+\/\/ Having placed all regions that have allocation capacity into the mutator set if they identify as is_young()\n+\/\/ or into the old collector set if they identify as is_old(), move some of these regions from the mutator set\n+\/\/ into the collector set or old collector set in order to assure that the memory available for allocations within\n+\/\/ the collector set is at least to_reserve, and the memory available for allocations within the old collector set\n+\/\/ is at least to_reserve_old.\n+void ShenandoahFreeSet::reserve_regions(size_t to_reserve, size_t to_reserve_old) {\n+  for (size_t i = _heap->num_regions(); i > 0; i--) {\n+    size_t idx = i - 1;\n+    ShenandoahHeapRegion* r = _heap->get_region(idx);\n+    if (!_free_sets.in_free_set(idx, Mutator)) {\n+      continue;\n+    }\n+\n+    size_t ac = alloc_capacity(r);\n+    assert (ac > 0, \"Membership in free set implies has capacity\");\n+    assert (!r->is_old(), \"mutator_is_free regions should not be affiliated OLD\");\n+\n+    bool move_to_old = _free_sets.capacity_of(OldCollector) < to_reserve_old;\n+    bool move_to_young = _free_sets.capacity_of(Collector) < to_reserve;\n+\n+    if (!move_to_old && !move_to_young) {\n+      \/\/ We've satisfied both to_reserve and to_reserved_old\n+      break;\n+    }\n+\n+    if (move_to_old) {\n+      if (r->is_trash() || !r->is_affiliated()) {\n+        \/\/ OLD regions that have available memory are already in the old_collector free set\n+        _free_sets.move_to_set(idx, OldCollector, ac);\n+        log_debug(gc, free)(\"  Shifting region \" SIZE_FORMAT \" from mutator_free to old_collector_free\", idx);\n+        continue;\n+      }\n+    }\n+\n+    if (move_to_young) {\n+      \/\/ Note: In a previous implementation, regions were only placed into the survivor space (collector_is_free) if\n+      \/\/ they were entirely empty.  I'm not sure I understand the rationale for that.  That alternative behavior would\n+      \/\/ tend to mix survivor objects with ephemeral objects, making it more difficult to reclaim the memory for the\n+      \/\/ ephemeral objects.  It also delays aging of regions, causing promotion in place to be delayed.\n+      _free_sets.move_to_set(idx, Collector, ac);\n+      log_debug(gc)(\"  Shifting region \" SIZE_FORMAT \" from mutator_free to collector_free\", idx);\n@@ -442,2 +1320,12 @@\n-  recompute_bounds();\n-  assert_bounds();\n+  if (LogTarget(Info, gc, free)::is_enabled()) {\n+    size_t old_reserve = _free_sets.capacity_of(OldCollector);\n+    if (old_reserve < to_reserve_old) {\n+      log_info(gc, free)(\"Wanted \" PROPERFMT \" for old reserve, but only reserved: \" PROPERFMT,\n+                         PROPERFMTARGS(to_reserve_old), PROPERFMTARGS(old_reserve));\n+    }\n+    size_t young_reserve = _free_sets.capacity_of(Collector);\n+    if (young_reserve < to_reserve) {\n+      log_info(gc, free)(\"Wanted \" PROPERFMT \" for young reserve, but only reserved: \" PROPERFMT,\n+                         PROPERFMTARGS(to_reserve), PROPERFMTARGS(young_reserve));\n+    }\n+  }\n@@ -449,1 +1337,91 @@\n-  LogTarget(Info, gc, ergo) lt;\n+#ifdef ASSERT\n+  \/\/ Dump of the FreeSet details is only enabled if assertions are enabled\n+  if (LogTarget(Debug, gc, free)::is_enabled()) {\n+#define BUFFER_SIZE 80\n+    size_t retired_old = 0;\n+    size_t retired_old_humongous = 0;\n+    size_t retired_young = 0;\n+    size_t retired_young_humongous = 0;\n+    size_t region_size_bytes = ShenandoahHeapRegion::region_size_bytes();\n+    size_t retired_young_waste = 0;\n+    size_t retired_old_waste = 0;\n+    size_t consumed_collector = 0;\n+    size_t consumed_old_collector = 0;\n+    size_t consumed_mutator = 0;\n+    size_t available_old = 0;\n+    size_t available_young = 0;\n+    size_t available_mutator = 0;\n+    size_t available_collector = 0;\n+    size_t available_old_collector = 0;\n+\n+    char buffer[BUFFER_SIZE];\n+    for (uint i = 0; i < BUFFER_SIZE; i++) {\n+      buffer[i] = '\\0';\n+    }\n+    log_debug(gc, free)(\"FreeSet map legend:\"\n+                       \" M:mutator_free C:collector_free O:old_collector_free\"\n+                       \" H:humongous ~:retired old _:retired young\");\n+    log_debug(gc, free)(\" mutator free range [\" SIZE_FORMAT \"..\" SIZE_FORMAT \"], \"\n+                       \" collector free range [\" SIZE_FORMAT \"..\" SIZE_FORMAT \"], \"\n+                       \"old collector free range [\" SIZE_FORMAT \"..\" SIZE_FORMAT \"] allocates from %s\",\n+                       _free_sets.leftmost(Mutator), _free_sets.rightmost(Mutator),\n+                       _free_sets.leftmost(Collector), _free_sets.rightmost(Collector),\n+                       _free_sets.leftmost(OldCollector), _free_sets.rightmost(OldCollector),\n+                       _free_sets.alloc_from_left_bias(OldCollector)? \"left to right\": \"right to left\");\n+\n+    for (uint i = 0; i < _heap->num_regions(); i++) {\n+      ShenandoahHeapRegion *r = _heap->get_region(i);\n+      uint idx = i % 64;\n+      if ((i != 0) && (idx == 0)) {\n+        log_debug(gc, free)(\" %6u: %s\", i-64, buffer);\n+      }\n+      if (_free_sets.in_free_set(i, Mutator)) {\n+        assert(!r->is_old(), \"Old regions should not be in mutator_free set\");\n+        size_t capacity = alloc_capacity(r);\n+        available_mutator += capacity;\n+        consumed_mutator += region_size_bytes - capacity;\n+        buffer[idx] = (capacity == region_size_bytes)? 'M': 'm';\n+      } else if (_free_sets.in_free_set(i, Collector)) {\n+        assert(!r->is_old(), \"Old regions should not be in collector_free set\");\n+        size_t capacity = alloc_capacity(r);\n+        available_collector += capacity;\n+        consumed_collector += region_size_bytes - capacity;\n+        buffer[idx] = (capacity == region_size_bytes)? 'C': 'c';\n+      } else if (_free_sets.in_free_set(i, OldCollector)) {\n+        size_t capacity = alloc_capacity(r);\n+        available_old_collector += capacity;\n+        consumed_old_collector += region_size_bytes - capacity;\n+        buffer[idx] = (capacity == region_size_bytes)? 'O': 'o';\n+      } else if (r->is_humongous()) {\n+        if (r->is_old()) {\n+          buffer[idx] = 'H';\n+          retired_old_humongous += region_size_bytes;\n+        } else {\n+          buffer[idx] = 'h';\n+          retired_young_humongous += region_size_bytes;\n+        }\n+      } else {\n+        if (r->is_old()) {\n+          buffer[idx] = '~';\n+          retired_old_waste += alloc_capacity(r);\n+          retired_old += region_size_bytes;\n+        } else {\n+          buffer[idx] = '_';\n+          retired_young_waste += alloc_capacity(r);\n+          retired_young += region_size_bytes;\n+        }\n+      }\n+    }\n+    uint remnant = _heap->num_regions() % 64;\n+    if (remnant > 0) {\n+      buffer[remnant] = '\\0';\n+    } else {\n+      remnant = 64;\n+    }\n+    log_debug(gc, free)(\" %6u: %s\", (uint) (_heap->num_regions() - remnant), buffer);\n+    size_t total_young = retired_young + retired_young_humongous;\n+    size_t total_old = retired_old + retired_old_humongous;\n+  }\n+#endif\n+\n+  LogTarget(Info, gc, free) lt;\n@@ -464,2 +1442,2 @@\n-      for (size_t idx = _mutator_leftmost; idx <= _mutator_rightmost; idx++) {\n-        if (is_mutator_free(idx)) {\n+      for (size_t idx = _free_sets.leftmost(Mutator); idx <= _free_sets.rightmost(Mutator); idx++) {\n+        if (_free_sets.in_free_set(idx, Mutator)) {\n@@ -468,1 +1446,0 @@\n-\n@@ -470,1 +1447,0 @@\n-\n@@ -481,1 +1457,0 @@\n-\n@@ -484,1 +1459,0 @@\n-\n@@ -493,0 +1467,4 @@\n+      assert(free == total_free, \"Sum of free within mutator regions (\" SIZE_FORMAT\n+             \") should match mutator capacity (\" SIZE_FORMAT \") minus mutator used (\" SIZE_FORMAT \")\",\n+             total_free, capacity(), used());\n+\n@@ -509,2 +1487,2 @@\n-      if (mutator_count() > 0) {\n-        frag_int = (100 * (total_used \/ mutator_count()) \/ ShenandoahHeapRegion::region_size_bytes());\n+      if (_free_sets.count(Mutator) > 0) {\n+        frag_int = (100 * (total_used \/ _free_sets.count(Mutator)) \/ ShenandoahHeapRegion::region_size_bytes());\n@@ -515,0 +1493,2 @@\n+      ls.print(\"Used: \" SIZE_FORMAT \"%s, Mutator Free: \" SIZE_FORMAT,\n+               byte_size_in_proper_unit(total_used), proper_unit_for_byte_size(total_used), _free_sets.count(Mutator));\n@@ -520,0 +1500,1 @@\n+      size_t total_used = 0;\n@@ -521,2 +1502,2 @@\n-      for (size_t idx = _collector_leftmost; idx <= _collector_rightmost; idx++) {\n-        if (is_collector_free(idx)) {\n+      for (size_t idx = _free_sets.leftmost(Collector); idx <= _free_sets.rightmost(Collector); idx++) {\n+        if (_free_sets.in_free_set(idx, Collector)) {\n@@ -527,0 +1508,1 @@\n+          total_used += r->used();\n@@ -529,0 +1511,10 @@\n+      ls.print(\" Collector Reserve: \" SIZE_FORMAT \"%s, Max: \" SIZE_FORMAT \"%s; Used: \" SIZE_FORMAT \"%s\",\n+               byte_size_in_proper_unit(total_free), proper_unit_for_byte_size(total_free),\n+               byte_size_in_proper_unit(max),        proper_unit_for_byte_size(max),\n+               byte_size_in_proper_unit(total_used), proper_unit_for_byte_size(total_used));\n+    }\n+\n+    if (_heap->mode()->is_generational()) {\n+      size_t max = 0;\n+      size_t total_free = 0;\n+      size_t total_used = 0;\n@@ -530,1 +1522,10 @@\n-      ls.print_cr(\"Reserve: \" SIZE_FORMAT \"%s, Max: \" SIZE_FORMAT \"%s\",\n+      for (size_t idx = _free_sets.leftmost(OldCollector); idx <= _free_sets.rightmost(OldCollector); idx++) {\n+        if (_free_sets.in_free_set(idx, OldCollector)) {\n+          ShenandoahHeapRegion *r = _heap->get_region(idx);\n+          size_t free = alloc_capacity(r);\n+          max = MAX2(max, free);\n+          total_free += free;\n+          total_used += r->used();\n+        }\n+      }\n+      ls.print_cr(\" Old Collector Reserve: \" SIZE_FORMAT \"%s, Max: \" SIZE_FORMAT \"%s; Used: \" SIZE_FORMAT \"%s\",\n@@ -532,1 +1533,2 @@\n-                  byte_size_in_proper_unit(max),        proper_unit_for_byte_size(max));\n+                  byte_size_in_proper_unit(max),        proper_unit_for_byte_size(max),\n+                  byte_size_in_proper_unit(total_used), proper_unit_for_byte_size(total_used));\n@@ -539,1 +1541,0 @@\n-  assert_bounds();\n@@ -541,0 +1542,1 @@\n+  \/\/ Allocation request is known to satisfy all memory budgeting constraints.\n@@ -547,0 +1549,1 @@\n+      case ShenandoahAllocRequest::_alloc_plab:\n@@ -565,2 +1568,2 @@\n-  for (size_t index = _mutator_leftmost; index <= _mutator_rightmost; index++) {\n-    if (index < _max && is_mutator_free(index)) {\n+  for (size_t index = _free_sets.leftmost(Mutator); index <= _free_sets.rightmost(Mutator); index++) {\n+    if (index < _free_sets.max() && _free_sets.in_free_set(index, Mutator)) {\n@@ -579,3 +1582,3 @@\n-  out->print_cr(\"Mutator Free Set: \" SIZE_FORMAT \"\", mutator_count());\n-  for (size_t index = _mutator_leftmost; index <= _mutator_rightmost; index++) {\n-    if (is_mutator_free(index)) {\n+  out->print_cr(\"Mutator Free Set: \" SIZE_FORMAT \"\", _free_sets.count(Mutator));\n+  for (size_t index = _free_sets.leftmost(Mutator); index <= _free_sets.rightmost(Mutator); index++) {\n+    if (_free_sets.in_free_set(index, Mutator)) {\n@@ -585,3 +1588,3 @@\n-  out->print_cr(\"Collector Free Set: \" SIZE_FORMAT \"\", collector_count());\n-  for (size_t index = _collector_leftmost; index <= _collector_rightmost; index++) {\n-    if (is_collector_free(index)) {\n+  out->print_cr(\"Collector Free Set: \" SIZE_FORMAT \"\", _free_sets.count(Collector));\n+  for (size_t index = _free_sets.leftmost(Collector); index <= _free_sets.rightmost(Collector); index++) {\n+    if (_free_sets.in_free_set(index, Collector)) {\n@@ -591,0 +1594,8 @@\n+  if (_heap->mode()->is_generational()) {\n+    out->print_cr(\"Old Collector Free Set: \" SIZE_FORMAT \"\", _free_sets.count(OldCollector));\n+    for (size_t index = _free_sets.leftmost(OldCollector); index <= _free_sets.rightmost(OldCollector); index++) {\n+      if (_free_sets.in_free_set(index, OldCollector)) {\n+        _heap->get_region(index)->print_on(out);\n+      }\n+    }\n+  }\n@@ -619,2 +1630,2 @@\n-  for (size_t index = _mutator_leftmost; index <= _mutator_rightmost; index++) {\n-    if (is_mutator_free(index)) {\n+  for (size_t index = _free_sets.leftmost(Mutator); index <= _free_sets.rightmost(Mutator); index++) {\n+    if (_free_sets.in_free_set(index, Mutator)) {\n@@ -657,2 +1668,2 @@\n-  for (size_t index = _mutator_leftmost; index <= _mutator_rightmost; index++) {\n-    if (is_mutator_free(index)) {\n+  for (size_t index = _free_sets.leftmost(Mutator); index <= _free_sets.rightmost(Mutator); index++) {\n+    if (_free_sets.in_free_set(index, Mutator)) {\n@@ -683,27 +1694,0 @@\n-#ifdef ASSERT\n-void ShenandoahFreeSet::assert_bounds() const {\n-  \/\/ Performance invariants. Failing these would not break the free set, but performance\n-  \/\/ would suffer.\n-  assert (_mutator_leftmost <= _max, \"leftmost in bounds: \"  SIZE_FORMAT \" < \" SIZE_FORMAT, _mutator_leftmost,  _max);\n-  assert (_mutator_rightmost < _max, \"rightmost in bounds: \" SIZE_FORMAT \" < \" SIZE_FORMAT, _mutator_rightmost, _max);\n-\n-  assert (_mutator_leftmost == _max || is_mutator_free(_mutator_leftmost),  \"leftmost region should be free: \" SIZE_FORMAT,  _mutator_leftmost);\n-  assert (_mutator_rightmost == 0   || is_mutator_free(_mutator_rightmost), \"rightmost region should be free: \" SIZE_FORMAT, _mutator_rightmost);\n-\n-  size_t beg_off = _mutator_free_bitmap.find_first_set_bit(0);\n-  size_t end_off = _mutator_free_bitmap.find_first_set_bit(_mutator_rightmost + 1);\n-  assert (beg_off >= _mutator_leftmost, \"free regions before the leftmost: \" SIZE_FORMAT \", bound \" SIZE_FORMAT, beg_off, _mutator_leftmost);\n-  assert (end_off == _max,      \"free regions past the rightmost: \" SIZE_FORMAT \", bound \" SIZE_FORMAT,  end_off, _mutator_rightmost);\n-\n-  assert (_collector_leftmost <= _max, \"leftmost in bounds: \"  SIZE_FORMAT \" < \" SIZE_FORMAT, _collector_leftmost,  _max);\n-  assert (_collector_rightmost < _max, \"rightmost in bounds: \" SIZE_FORMAT \" < \" SIZE_FORMAT, _collector_rightmost, _max);\n-\n-  assert (_collector_leftmost == _max || is_collector_free(_collector_leftmost),  \"leftmost region should be free: \" SIZE_FORMAT,  _collector_leftmost);\n-  assert (_collector_rightmost == 0   || is_collector_free(_collector_rightmost), \"rightmost region should be free: \" SIZE_FORMAT, _collector_rightmost);\n-\n-  beg_off = _collector_free_bitmap.find_first_set_bit(0);\n-  end_off = _collector_free_bitmap.find_first_set_bit(_collector_rightmost + 1);\n-  assert (beg_off >= _collector_leftmost, \"free regions before the leftmost: \" SIZE_FORMAT \", bound \" SIZE_FORMAT, beg_off, _collector_leftmost);\n-  assert (end_off == _max,      \"free regions past the rightmost: \" SIZE_FORMAT \", bound \" SIZE_FORMAT,  end_off, _collector_rightmost);\n-}\n-#endif\n","filename":"src\/hotspot\/share\/gc\/shenandoah\/shenandoahFreeSet.cpp","additions":1212,"deletions":228,"binary":false,"changes":1440,"status":"modified"},{"patch":"@@ -4,0 +4,1 @@\n+ * Copyright Amazon.com Inc. or its affiliates. All Rights Reserved.\n@@ -38,0 +39,4 @@\n+#include \"gc\/shenandoah\/shenandoahAgeCensus.hpp\"\n+#include \"gc\/shenandoah\/heuristics\/shenandoahOldHeuristics.hpp\"\n+#include \"gc\/shenandoah\/heuristics\/shenandoahYoungHeuristics.hpp\"\n+#include \"gc\/shenandoah\/shenandoahAllocRequest.hpp\"\n@@ -39,0 +44,1 @@\n+#include \"gc\/shenandoah\/shenandoahCardTable.hpp\"\n@@ -45,0 +51,1 @@\n+#include \"gc\/shenandoah\/shenandoahRegulatorThread.hpp\"\n@@ -46,0 +53,1 @@\n+#include \"gc\/shenandoah\/shenandoahGlobalGeneration.hpp\"\n@@ -55,0 +63,1 @@\n+#include \"gc\/shenandoah\/shenandoahOldGeneration.hpp\"\n@@ -61,0 +70,1 @@\n+#include \"gc\/shenandoah\/shenandoahScanRemembered.inline.hpp\"\n@@ -68,0 +78,2 @@\n+#include \"gc\/shenandoah\/shenandoahYoungGeneration.hpp\"\n+#include \"gc\/shenandoah\/mode\/shenandoahGenerationalMode.hpp\"\n@@ -71,0 +83,2 @@\n+#include \"utilities\/globalDefinitions.hpp\"\n+\n@@ -162,3 +176,0 @@\n-  \/\/ Now we know the number of regions, initialize the heuristics.\n-  initialize_heuristics();\n-\n@@ -180,0 +191,3 @@\n+  \/\/ Now we know the number of regions and heap sizes, initialize the heuristics.\n+  initialize_heuristics_generations();\n+\n@@ -195,0 +209,3 @@\n+  os::trace_page_sizes_for_requested_size(\"Heap\",\n+                                          max_byte_size, heap_rs.page_size(), heap_alignment,\n+                                          heap_rs.base(), heap_rs.size());\n@@ -214,0 +231,28 @@\n+  BarrierSet::set_barrier_set(new ShenandoahBarrierSet(this, _heap_region));\n+\n+  \/\/\n+  \/\/ After reserving the Java heap, create the card table, barriers, and workers, in dependency order\n+  \/\/\n+  if (mode()->is_generational()) {\n+    ShenandoahDirectCardMarkRememberedSet *rs;\n+    ShenandoahCardTable* card_table = ShenandoahBarrierSet::barrier_set()->card_table();\n+    size_t card_count = card_table->cards_required(heap_rs.size() \/ HeapWordSize);\n+    rs = new ShenandoahDirectCardMarkRememberedSet(ShenandoahBarrierSet::barrier_set()->card_table(), card_count);\n+    _card_scan = new ShenandoahScanRemembered<ShenandoahDirectCardMarkRememberedSet>(rs);\n+\n+    \/\/ Age census structure\n+    _age_census = new ShenandoahAgeCensus();\n+  }\n+\n+  _workers = new ShenandoahWorkerThreads(\"Shenandoah GC Threads\", _max_workers);\n+  if (_workers == nullptr) {\n+    vm_exit_during_initialization(\"Failed necessary allocation.\");\n+  } else {\n+    _workers->initialize_workers();\n+  }\n+\n+  if (ParallelGCThreads > 1) {\n+    _safepoint_workers = new ShenandoahWorkerThreads(\"Safepoint Cleanup Thread\", ParallelGCThreads);\n+    _safepoint_workers->initialize_workers();\n+  }\n+\n@@ -218,2 +263,2 @@\n-  _bitmap_size = ShenandoahMarkBitMap::compute_size(heap_rs.size());\n-  _bitmap_size = align_up(_bitmap_size, bitmap_page_size);\n+  size_t bitmap_size_orig = ShenandoahMarkBitMap::compute_size(heap_rs.size());\n+  _bitmap_size = align_up(bitmap_size_orig, bitmap_page_size);\n@@ -245,0 +290,4 @@\n+  os::trace_page_sizes_for_requested_size(\"Mark Bitmap\",\n+                                          bitmap_size_orig, bitmap.page_size(), bitmap_page_size,\n+                                          bitmap.base(),\n+                                          bitmap.size());\n@@ -257,1 +306,1 @@\n-  _marking_context = new ShenandoahMarkingContext(_heap_region, _bitmap_region, _num_regions, _max_workers);\n+  _marking_context = new ShenandoahMarkingContext(_heap_region, _bitmap_region, _num_regions);\n@@ -261,0 +310,4 @@\n+    os::trace_page_sizes_for_requested_size(\"Verify Bitmap\",\n+                                            bitmap_size_orig, verify_bitmap.page_size(), bitmap_page_size,\n+                                            verify_bitmap.base(),\n+                                            verify_bitmap.size());\n@@ -272,1 +325,13 @@\n-  ReservedSpace aux_bitmap(_bitmap_size, bitmap_page_size);\n+  size_t aux_bitmap_page_size = bitmap_page_size;\n+#ifdef LINUX\n+  \/\/ In THP \"advise\" mode, we refrain from advising the system to use large pages\n+  \/\/ since we know these commits will be short lived, and there is no reason to trash\n+  \/\/ the THP area with this bitmap.\n+  if (UseTransparentHugePages) {\n+    aux_bitmap_page_size = os::vm_page_size();\n+  }\n+#endif\n+  ReservedSpace aux_bitmap(_bitmap_size, aux_bitmap_page_size);\n+  os::trace_page_sizes_for_requested_size(\"Aux Bitmap\",\n+                                          bitmap_size_orig, aux_bitmap.page_size(), aux_bitmap_page_size,\n+                                          aux_bitmap.base(), aux_bitmap.size());\n@@ -282,2 +347,3 @@\n-  size_t region_storage_size = align_up(region_align * _num_regions, region_page_size);\n-  region_storage_size = align_up(region_storage_size, os::vm_allocation_granularity());\n+  size_t region_storage_size_orig = region_align * _num_regions;\n+  size_t region_storage_size = align_up(region_storage_size_orig,\n+                                        MAX2(region_page_size, os::vm_allocation_granularity()));\n@@ -286,0 +352,3 @@\n+  os::trace_page_sizes_for_requested_size(\"Region Storage\",\n+                                          region_storage_size_orig, region_storage.page_size(), region_page_size,\n+                                          region_storage.base(), region_storage.size());\n@@ -296,2 +365,3 @@\n-    size_t cset_align = MAX2<size_t>(os::vm_page_size(), os::vm_allocation_granularity());\n-    size_t cset_size = align_up(((size_t) sh_rs.base() + sh_rs.size()) >> ShenandoahHeapRegion::region_size_bytes_shift(), cset_align);\n+    const size_t cset_align = MAX2<size_t>(os::vm_page_size(), os::vm_allocation_granularity());\n+    const size_t cset_size = align_up(((size_t) sh_rs.base() + sh_rs.size()) >> ShenandoahHeapRegion::region_size_bytes_shift(), cset_align);\n+    const size_t cset_page_size = os::vm_page_size();\n@@ -301,0 +371,1 @@\n+    ReservedSpace cset_rs;\n@@ -305,1 +376,1 @@\n-      ReservedSpace cset_rs(cset_size, cset_align, os::vm_page_size(), req_addr);\n+      cset_rs = ReservedSpace(cset_size, cset_align, cset_page_size, req_addr);\n@@ -314,1 +385,1 @@\n-      ReservedSpace cset_rs(cset_size, cset_align, os::vm_page_size());\n+      cset_rs = ReservedSpace(cset_size, cset_align, os::vm_page_size());\n@@ -317,0 +388,4 @@\n+    os::trace_page_sizes_for_requested_size(\"Collection Set\",\n+                                            cset_size, cset_rs.page_size(), cset_page_size,\n+                                            cset_rs.base(),\n+                                            cset_rs.size());\n@@ -320,0 +395,1 @@\n+  _affiliations = NEW_C_HEAP_ARRAY(uint8_t, _num_regions, mtGC);\n@@ -325,0 +401,1 @@\n+\n@@ -336,0 +413,2 @@\n+\n+      _affiliations[i] = ShenandoahAffiliation::FREE;\n@@ -340,0 +419,1 @@\n+    size_t young_cset_regions, old_cset_regions;\n@@ -341,1 +421,4 @@\n-    _free_set->rebuild();\n+    \/\/ We are initializing free set.  We ignore cset region tallies.\n+    size_t first_old, last_old, num_old;\n+    _free_set->prepare_to_rebuild(young_cset_regions, old_cset_regions, first_old, last_old, num_old);\n+    _free_set->rebuild(young_cset_regions, old_cset_regions);\n@@ -403,0 +486,1 @@\n+  _regulator_thread = new ShenandoahRegulatorThread(_control_thread);\n@@ -404,1 +488,1 @@\n-  ShenandoahInitLogger::print();\n+  print_init_logger();\n@@ -409,1 +493,35 @@\n-void ShenandoahHeap::initialize_mode() {\n+void ShenandoahHeap::print_init_logger() const {\n+  ShenandoahInitLogger::print();\n+}\n+\n+size_t ShenandoahHeap::max_size_for(ShenandoahGeneration* generation) const {\n+  switch (generation->type()) {\n+    case YOUNG:\n+      return _generation_sizer.max_young_size();\n+    case OLD:\n+      return max_capacity() - _generation_sizer.min_young_size();\n+    case GLOBAL_GEN:\n+    case GLOBAL_NON_GEN:\n+      return max_capacity();\n+    default:\n+      ShouldNotReachHere();\n+      return 0;\n+  }\n+}\n+\n+size_t ShenandoahHeap::min_size_for(ShenandoahGeneration* generation) const {\n+  switch (generation->type()) {\n+    case YOUNG:\n+      return _generation_sizer.min_young_size();\n+    case OLD:\n+      return max_capacity() - _generation_sizer.max_young_size();\n+    case GLOBAL_GEN:\n+    case GLOBAL_NON_GEN:\n+      return min_capacity();\n+    default:\n+      ShouldNotReachHere();\n+      return 0;\n+  }\n+}\n+\n+void ShenandoahHeap::initialize_heuristics_generations() {\n@@ -417,0 +535,2 @@\n+    } else if (strcmp(ShenandoahGCMode, \"generational\") == 0) {\n+      _gc_mode = new ShenandoahGenerationalMode();\n@@ -434,1 +554,0 @@\n-}\n@@ -436,3 +555,9 @@\n-void ShenandoahHeap::initialize_heuristics() {\n-  assert(_gc_mode != nullptr, \"Must be initialized\");\n-  _heuristics = _gc_mode->initialize_heuristics();\n+  \/\/ Max capacity is the maximum _allowed_ capacity. That is, the maximum allowed capacity\n+  \/\/ for old would be total heap - minimum capacity of young. This means the sum of the maximum\n+  \/\/ allowed for old and young could exceed the total heap size. It remains the case that the\n+  \/\/ _actual_ capacity of young + old = total.\n+  _generation_sizer.heap_size_changed(max_capacity());\n+  size_t initial_capacity_young = _generation_sizer.max_young_size();\n+  size_t max_capacity_young = _generation_sizer.max_young_size();\n+  size_t initial_capacity_old = max_capacity() - max_capacity_young;\n+  size_t max_capacity_old = max_capacity() - initial_capacity_young;\n@@ -440,9 +565,7 @@\n-  if (_heuristics->is_diagnostic() && !UnlockDiagnosticVMOptions) {\n-    vm_exit_during_initialization(\n-            err_msg(\"Heuristics \\\"%s\\\" is diagnostic, and must be enabled via -XX:+UnlockDiagnosticVMOptions.\",\n-                    _heuristics->name()));\n-  }\n-  if (_heuristics->is_experimental() && !UnlockExperimentalVMOptions) {\n-    vm_exit_during_initialization(\n-            err_msg(\"Heuristics \\\"%s\\\" is experimental, and must be enabled via -XX:+UnlockExperimentalVMOptions.\",\n-                    _heuristics->name()));\n+  _young_generation = new ShenandoahYoungGeneration(_max_workers, max_capacity_young, initial_capacity_young);\n+  _old_generation = new ShenandoahOldGeneration(_max_workers, max_capacity_old, initial_capacity_old);\n+  _global_generation = new ShenandoahGlobalGeneration(_gc_mode->is_generational(), _max_workers, max_capacity(), max_capacity());\n+  _global_generation->initialize_heuristics(_gc_mode);\n+  if (mode()->is_generational()) {\n+    _young_generation->initialize_heuristics(_gc_mode);\n+    _old_generation->initialize_heuristics(_gc_mode);\n@@ -450,0 +573,1 @@\n+  _evac_tracker = new ShenandoahEvacuationTracker(mode()->is_generational());\n@@ -459,0 +583,1 @@\n+  _gc_generation(nullptr),\n@@ -460,1 +585,1 @@\n-  _used(0),\n+  _promotion_potential(0),\n@@ -462,2 +587,1 @@\n-  _bytes_allocated_since_gc_start(0),\n-  _max_workers(MAX2(ConcGCThreads, ParallelGCThreads)),\n+  _max_workers(MAX3(ConcGCThreads, ParallelGCThreads, 1U)),\n@@ -469,0 +593,1 @@\n+  _affiliations(nullptr),\n@@ -470,0 +595,9 @@\n+  _promoted_reserve(0),\n+  _old_evac_reserve(0),\n+  _young_evac_reserve(0),\n+  _age_census(nullptr),\n+  _has_evacuation_reserve_quantities(false),\n+  _cancel_requested_time(0),\n+  _young_generation(nullptr),\n+  _global_generation(nullptr),\n+  _old_generation(nullptr),\n@@ -471,0 +605,1 @@\n+  _regulator_thread(nullptr),\n@@ -472,2 +607,0 @@\n-  _gc_mode(nullptr),\n-  _heuristics(nullptr),\n@@ -478,0 +611,3 @@\n+  _evac_tracker(nullptr),\n+  _mmu_tracker(),\n+  _generation_sizer(),\n@@ -480,0 +616,2 @@\n+  _young_gen_memory_pool(nullptr),\n+  _old_gen_memory_pool(nullptr),\n@@ -485,1 +623,2 @@\n-  _ref_processor(new ShenandoahReferenceProcessor(MAX2(_max_workers, 1U))),\n+  _old_regions_surplus(0),\n+  _old_regions_deficit(0),\n@@ -493,1 +632,2 @@\n-  _collection_set(nullptr)\n+  _collection_set(nullptr),\n+  _card_scan(nullptr)\n@@ -495,17 +635,0 @@\n-  \/\/ Initialize GC mode early, so we can adjust barrier support\n-  initialize_mode();\n-  BarrierSet::set_barrier_set(new ShenandoahBarrierSet(this));\n-\n-  _max_workers = MAX2(_max_workers, 1U);\n-  _workers = new ShenandoahWorkerThreads(\"Shenandoah GC Threads\", _max_workers);\n-  if (_workers == nullptr) {\n-    vm_exit_during_initialization(\"Failed necessary allocation.\");\n-  } else {\n-    _workers->initialize_workers();\n-  }\n-\n-  if (ParallelGCThreads > 1) {\n-    _safepoint_workers = new ShenandoahWorkerThreads(\"Safepoint Cleanup Thread\",\n-                                                ParallelGCThreads);\n-    _safepoint_workers->initialize_workers();\n-  }\n@@ -518,29 +641,0 @@\n-class ShenandoahResetBitmapTask : public WorkerTask {\n-private:\n-  ShenandoahRegionIterator _regions;\n-\n-public:\n-  ShenandoahResetBitmapTask() :\n-    WorkerTask(\"Shenandoah Reset Bitmap\") {}\n-\n-  void work(uint worker_id) {\n-    ShenandoahHeapRegion* region = _regions.next();\n-    ShenandoahHeap* heap = ShenandoahHeap::heap();\n-    ShenandoahMarkingContext* const ctx = heap->marking_context();\n-    while (region != nullptr) {\n-      if (heap->is_bitmap_slice_committed(region)) {\n-        ctx->clear_bitmap(region);\n-      }\n-      region = _regions.next();\n-    }\n-  }\n-};\n-\n-void ShenandoahHeap::reset_mark_bitmap() {\n-  assert_gc_workers(_workers->active_workers());\n-  mark_incomplete_marking_context();\n-\n-  ShenandoahResetBitmapTask task;\n-  _workers->run_task(&task);\n-}\n-\n@@ -561,1 +655,2 @@\n-  if (is_concurrent_mark_in_progress())        st->print(\"marking, \");\n+  if (is_concurrent_old_mark_in_progress())    st->print(\"old marking, \");\n+  if (is_concurrent_young_mark_in_progress())  st->print(\"young marking, \");\n@@ -612,0 +707,2 @@\n+  _mmu_tracker.initialize();\n+\n@@ -625,2 +722,0 @@\n-  _heuristics->initialize();\n-\n@@ -630,0 +725,24 @@\n+ShenandoahHeuristics* ShenandoahHeap::heuristics() {\n+  return _global_generation->heuristics();\n+}\n+\n+ShenandoahOldHeuristics* ShenandoahHeap::old_heuristics() {\n+  return (ShenandoahOldHeuristics*) _old_generation->heuristics();\n+}\n+\n+ShenandoahYoungHeuristics* ShenandoahHeap::young_heuristics() {\n+  return (ShenandoahYoungHeuristics*) _young_generation->heuristics();\n+}\n+\n+bool ShenandoahHeap::doing_mixed_evacuations() {\n+  return _old_generation->state() == ShenandoahOldGeneration::EVACUATING;\n+}\n+\n+bool ShenandoahHeap::is_old_bitmap_stable() const {\n+  return _old_generation->is_mark_complete();\n+}\n+\n+bool ShenandoahHeap::is_gc_generation_young() const {\n+  return _gc_generation != nullptr && _gc_generation->is_young();\n+}\n+\n@@ -631,1 +750,1 @@\n-  return Atomic::load(&_used);\n+  return global_generation()->used();\n@@ -648,2 +767,43 @@\n-void ShenandoahHeap::increase_used(size_t bytes) {\n-  Atomic::add(&_used, bytes, memory_order_relaxed);\n+\/\/ For tracking usage based on allocations, it should be the case that:\n+\/\/ * The sum of regions::used == heap::used\n+\/\/ * The sum of a generation's regions::used == generation::used\n+\/\/ * The sum of a generation's humongous regions::free == generation::humongous_waste\n+\/\/ These invariants are checked by the verifier on GC safepoints.\n+\/\/\n+\/\/ Additional notes:\n+\/\/ * When a mutator's allocation request causes a region to be retired, the\n+\/\/   free memory left in that region is considered waste. It does not contribute\n+\/\/   to the usage, but it _does_ contribute to allocation rate.\n+\/\/ * The bottom of a PLAB must be aligned on card size. In some cases this will\n+\/\/   require padding in front of the PLAB (a filler object). Because this padding\n+\/\/   is included in the region's used memory we include the padding in the usage\n+\/\/   accounting as waste.\n+\/\/ * Mutator allocations are used to compute an allocation rate. They are also\n+\/\/   sent to the Pacer for those purposes.\n+\/\/ * There are three sources of waste:\n+\/\/  1. The padding used to align a PLAB on card size\n+\/\/  2. Region's free is less than minimum TLAB size and is retired\n+\/\/  3. The unused portion of memory in the last region of a humongous object\n+void ShenandoahHeap::increase_used(const ShenandoahAllocRequest& req) {\n+  size_t actual_bytes = req.actual_size() * HeapWordSize;\n+  size_t wasted_bytes = req.waste() * HeapWordSize;\n+  ShenandoahGeneration* generation = generation_for(req.affiliation());\n+\n+  if (req.is_gc_alloc()) {\n+    assert(wasted_bytes == 0 || req.type() == ShenandoahAllocRequest::_alloc_plab, \"Only PLABs have waste\");\n+    increase_used(generation, actual_bytes + wasted_bytes);\n+  } else {\n+    assert(req.is_mutator_alloc(), \"Expected mutator alloc here\");\n+    \/\/ padding and actual size both count towards allocation counter\n+    generation->increase_allocated(actual_bytes + wasted_bytes);\n+\n+    \/\/ only actual size counts toward usage for mutator allocations\n+    increase_used(generation, actual_bytes);\n+\n+    \/\/ notify pacer of both actual size and waste\n+    notify_mutator_alloc_words(req.actual_size(), req.waste());\n+\n+    if (wasted_bytes > 0 && req.actual_size() > ShenandoahHeapRegion::humongous_threshold_words()) {\n+      increase_humongous_waste(generation,wasted_bytes);\n+    }\n+  }\n@@ -652,2 +812,5 @@\n-void ShenandoahHeap::set_used(size_t bytes) {\n-  Atomic::store(&_used, bytes);\n+void ShenandoahHeap::increase_humongous_waste(ShenandoahGeneration* generation, size_t bytes) {\n+  generation->increase_humongous_waste(bytes);\n+  if (!generation->is_global()) {\n+    global_generation()->increase_humongous_waste(bytes);\n+  }\n@@ -656,3 +819,5 @@\n-void ShenandoahHeap::decrease_used(size_t bytes) {\n-  assert(used() >= bytes, \"never decrease heap size by more than we've left\");\n-  Atomic::sub(&_used, bytes, memory_order_relaxed);\n+void ShenandoahHeap::decrease_humongous_waste(ShenandoahGeneration* generation, size_t bytes) {\n+  generation->decrease_humongous_waste(bytes);\n+  if (!generation->is_global()) {\n+    global_generation()->decrease_humongous_waste(bytes);\n+  }\n@@ -661,2 +826,5 @@\n-void ShenandoahHeap::increase_allocated(size_t bytes) {\n-  Atomic::add(&_bytes_allocated_since_gc_start, bytes, memory_order_relaxed);\n+void ShenandoahHeap::increase_used(ShenandoahGeneration* generation, size_t bytes) {\n+  generation->increase_used(bytes);\n+  if (!generation->is_global()) {\n+    global_generation()->increase_used(bytes);\n+  }\n@@ -665,4 +833,4 @@\n-void ShenandoahHeap::notify_mutator_alloc_words(size_t words, bool waste) {\n-  size_t bytes = words * HeapWordSize;\n-  if (!waste) {\n-    increase_used(bytes);\n+void ShenandoahHeap::decrease_used(ShenandoahGeneration* generation, size_t bytes) {\n+  generation->decrease_used(bytes);\n+  if (!generation->is_global()) {\n+    global_generation()->decrease_used(bytes);\n@@ -670,1 +838,3 @@\n-  increase_allocated(bytes);\n+}\n+\n+void ShenandoahHeap::notify_mutator_alloc_words(size_t words, size_t waste) {\n@@ -673,2 +843,2 @@\n-    if (waste) {\n-      pacer()->claim_for_alloc(words, true);\n+    if (waste > 0) {\n+      pacer()->claim_for_alloc(waste, true);\n@@ -710,6 +880,0 @@\n-bool ShenandoahHeap::is_in(const void* p) const {\n-  HeapWord* heap_base = (HeapWord*) base();\n-  HeapWord* last_region_end = heap_base + ShenandoahHeapRegion::region_size_words() * num_regions();\n-  return p >= heap_base && p < last_region_end;\n-}\n-\n@@ -743,0 +907,65 @@\n+    regulator_thread()->notify_heap_changed();\n+  }\n+}\n+\n+void ShenandoahHeap::handle_old_evacuation(HeapWord* obj, size_t words, bool promotion) {\n+  \/\/ Only register the copy of the object that won the evacuation race.\n+  card_scan()->register_object_without_lock(obj);\n+\n+  \/\/ Mark the entire range of the evacuated object as dirty.  At next remembered set scan,\n+  \/\/ we will clear dirty bits that do not hold interesting pointers.  It's more efficient to\n+  \/\/ do this in batch, in a background GC thread than to try to carefully dirty only cards\n+  \/\/ that hold interesting pointers right now.\n+  card_scan()->mark_range_as_dirty(obj, words);\n+\n+  if (promotion) {\n+    \/\/ This evacuation was a promotion, track this as allocation against old gen\n+    old_generation()->increase_allocated(words * HeapWordSize);\n+  }\n+}\n+\n+void ShenandoahHeap::handle_old_evacuation_failure() {\n+  if (_old_gen_oom_evac.try_set()) {\n+    log_info(gc)(\"Old gen evac failure.\");\n+  }\n+}\n+\n+void ShenandoahHeap::report_promotion_failure(Thread* thread, size_t size) {\n+  \/\/ We squelch excessive reports to reduce noise in logs.\n+  const size_t MaxReportsPerEpoch = 4;\n+  static size_t last_report_epoch = 0;\n+  static size_t epoch_report_count = 0;\n+\n+  size_t promotion_reserve;\n+  size_t promotion_expended;\n+\n+  size_t gc_id = control_thread()->get_gc_id();\n+\n+  if ((gc_id != last_report_epoch) || (epoch_report_count++ < MaxReportsPerEpoch)) {\n+    {\n+      \/\/ Promotion failures should be very rare.  Invest in providing useful diagnostic info.\n+      ShenandoahHeapLocker locker(lock());\n+      promotion_reserve = get_promoted_reserve();\n+      promotion_expended = get_promoted_expended();\n+    }\n+    PLAB* plab = ShenandoahThreadLocalData::plab(thread);\n+    size_t words_remaining = (plab == nullptr)? 0: plab->words_remaining();\n+    const char* promote_enabled = ShenandoahThreadLocalData::allow_plab_promotions(thread)? \"enabled\": \"disabled\";\n+    ShenandoahGeneration* old_gen = old_generation();\n+    size_t old_capacity = old_gen->max_capacity();\n+    size_t old_usage = old_gen->used();\n+    size_t old_free_regions = old_gen->free_unaffiliated_regions();\n+\n+    log_info(gc, ergo)(\"Promotion failed, size \" SIZE_FORMAT \", has plab? %s, PLAB remaining: \" SIZE_FORMAT\n+                       \", plab promotions %s, promotion reserve: \" SIZE_FORMAT \", promotion expended: \" SIZE_FORMAT\n+                       \", old capacity: \" SIZE_FORMAT \", old_used: \" SIZE_FORMAT \", old unaffiliated regions: \" SIZE_FORMAT,\n+                       size * HeapWordSize, plab == nullptr? \"no\": \"yes\",\n+                       words_remaining * HeapWordSize, promote_enabled, promotion_reserve, promotion_expended,\n+                       old_capacity, old_usage, old_free_regions);\n+\n+    if ((gc_id == last_report_epoch) && (epoch_report_count >= MaxReportsPerEpoch)) {\n+      log_info(gc, ergo)(\"Squelching additional promotion failure reports for current epoch\");\n+    } else if (gc_id != last_report_epoch) {\n+      last_report_epoch = gc_id;\n+      epoch_report_count = 1;\n+    }\n@@ -752,0 +981,8 @@\n+\n+  \/\/ Limit growth of GCLABs to ShenandoahMaxEvacLABRatio * the minimum size.  This enables more equitable distribution of\n+  \/\/ available evacuation buidget between the many threads that are coordinating in the evacuation effort.\n+  if (ShenandoahMaxEvacLABRatio > 0) {\n+    log_debug(gc, free)(\"Allocate new gclab: \" SIZE_FORMAT \", \" SIZE_FORMAT, new_size, PLAB::min_size() * ShenandoahMaxEvacLABRatio);\n+    new_size = MIN2(new_size, PLAB::min_size() * ShenandoahMaxEvacLABRatio);\n+  }\n+\n@@ -763,0 +1000,1 @@\n+    log_debug(gc, free)(\"New gclab size (\" SIZE_FORMAT \") is too small for \" SIZE_FORMAT, new_size, size);\n@@ -795,0 +1033,246 @@\n+\/\/ Establish a new PLAB and allocate size HeapWords within it.\n+HeapWord* ShenandoahHeap::allocate_from_plab_slow(Thread* thread, size_t size, bool is_promotion) {\n+  \/\/ New object should fit the PLAB size\n+  size_t min_size = MAX2(size, PLAB::min_size());\n+\n+  \/\/ Figure out size of new PLAB, looking back at heuristics. Expand aggressively.\n+  size_t cur_size = ShenandoahThreadLocalData::plab_size(thread);\n+  if (cur_size == 0) {\n+    cur_size = PLAB::min_size();\n+  }\n+  size_t future_size = cur_size * 2;\n+  \/\/ Limit growth of PLABs to ShenandoahMaxEvacLABRatio * the minimum size.  This enables more equitable distribution of\n+  \/\/ available evacuation buidget between the many threads that are coordinating in the evacuation effort.\n+  if (ShenandoahMaxEvacLABRatio > 0) {\n+    future_size = MIN2(future_size, PLAB::min_size() * ShenandoahMaxEvacLABRatio);\n+  }\n+  future_size = MIN2(future_size, PLAB::max_size());\n+  future_size = MAX2(future_size, PLAB::min_size());\n+\n+  size_t unalignment = future_size % CardTable::card_size_in_words();\n+  if (unalignment != 0) {\n+    future_size = future_size - unalignment + CardTable::card_size_in_words();\n+  }\n+\n+  \/\/ Record new heuristic value even if we take any shortcut. This captures\n+  \/\/ the case when moderately-sized objects always take a shortcut. At some point,\n+  \/\/ heuristics should catch up with them.  Note that the requested cur_size may\n+  \/\/ not be honored, but we remember that this is the preferred size.\n+  ShenandoahThreadLocalData::set_plab_size(thread, future_size);\n+  if (cur_size < size) {\n+    \/\/ The PLAB to be allocated is still not large enough to hold the object. Fall back to shared allocation.\n+    \/\/ This avoids retiring perfectly good PLABs in order to represent a single large object allocation.\n+    return nullptr;\n+  }\n+\n+  \/\/ Retire current PLAB, and allocate a new one.\n+  PLAB* plab = ShenandoahThreadLocalData::plab(thread);\n+  if (plab->words_remaining() < PLAB::min_size()) {\n+    \/\/ Retire current PLAB, and allocate a new one.\n+    \/\/ CAUTION: retire_plab may register the remnant filler object with the remembered set scanner without a lock.  This\n+    \/\/ is safe iff it is assured that each PLAB is a whole-number multiple of card-mark memory size and each PLAB is\n+    \/\/ aligned with the start of a card's memory range.\n+    retire_plab(plab, thread);\n+\n+    size_t actual_size = 0;\n+    \/\/ allocate_new_plab resets plab_evacuated and plab_promoted and disables promotions if old-gen available is\n+    \/\/ less than the remaining evacuation need.  It also adjusts plab_preallocated and expend_promoted if appropriate.\n+    HeapWord* plab_buf = allocate_new_plab(min_size, cur_size, &actual_size);\n+    if (plab_buf == nullptr) {\n+      if (min_size == PLAB::min_size()) {\n+        \/\/ Disable plab promotions for this thread because we cannot even allocate a plab of minimal size.  This allows us\n+        \/\/ to fail faster on subsequent promotion attempts.\n+        ShenandoahThreadLocalData::disable_plab_promotions(thread);\n+      }\n+      return NULL;\n+    } else {\n+      ShenandoahThreadLocalData::enable_plab_retries(thread);\n+    }\n+    assert (size <= actual_size, \"allocation should fit\");\n+    if (ZeroTLAB) {\n+      \/\/ ..and clear it.\n+      Copy::zero_to_words(plab_buf, actual_size);\n+    } else {\n+      \/\/ ...and zap just allocated object.\n+#ifdef ASSERT\n+      \/\/ Skip mangling the space corresponding to the object header to\n+      \/\/ ensure that the returned space is not considered parsable by\n+      \/\/ any concurrent GC thread.\n+      size_t hdr_size = oopDesc::header_size();\n+      Copy::fill_to_words(plab_buf + hdr_size, actual_size - hdr_size, badHeapWordVal);\n+#endif \/\/ ASSERT\n+    }\n+    plab->set_buf(plab_buf, actual_size);\n+    if (is_promotion && !ShenandoahThreadLocalData::allow_plab_promotions(thread)) {\n+      return nullptr;\n+    }\n+    return plab->allocate(size);\n+  } else {\n+    \/\/ If there's still at least min_size() words available within the current plab, don't retire it.  Let's gnaw\n+    \/\/ away on this plab as long as we can.  Meanwhile, return nullptr to force this particular allocation request\n+    \/\/ to be satisfied with a shared allocation.  By packing more promotions into the previously allocated PLAB, we\n+    \/\/ reduce the likelihood of evacuation failures, and we we reduce the need for downsizing our PLABs.\n+    return nullptr;\n+  }\n+}\n+\n+\/\/ TODO: It is probably most efficient to register all objects (both promotions and evacuations) that were allocated within\n+\/\/ this plab at the time we retire the plab.  A tight registration loop will run within both code and data caches.  This change\n+\/\/ would allow smaller and faster in-line implementation of alloc_from_plab().  Since plabs are aligned on card-table boundaries,\n+\/\/ this object registration loop can be performed without acquiring a lock.\n+void ShenandoahHeap::retire_plab(PLAB* plab, Thread* thread) {\n+  \/\/ We don't enforce limits on plab_evacuated.  We let it consume all available old-gen memory in order to reduce\n+  \/\/ probability of an evacuation failure.  We do enforce limits on promotion, to make sure that excessive promotion\n+  \/\/ does not result in an old-gen evacuation failure.  Note that a failed promotion is relatively harmless.  Any\n+  \/\/ object that fails to promote in the current cycle will be eligible for promotion in a subsequent cycle.\n+\n+  \/\/ When the plab was instantiated, its entirety was treated as if the entire buffer was going to be dedicated to\n+  \/\/ promotions.  Now that we are retiring the buffer, we adjust for the reality that the plab is not entirely promotions.\n+  \/\/  1. Some of the plab may have been dedicated to evacuations.\n+  \/\/  2. Some of the plab may have been abandoned due to waste (at the end of the plab).\n+  size_t not_promoted =\n+    ShenandoahThreadLocalData::get_plab_preallocated_promoted(thread) - ShenandoahThreadLocalData::get_plab_promoted(thread);\n+  ShenandoahThreadLocalData::reset_plab_promoted(thread);\n+  ShenandoahThreadLocalData::reset_plab_evacuated(thread);\n+  ShenandoahThreadLocalData::set_plab_preallocated_promoted(thread, 0);\n+  if (not_promoted > 0) {\n+    unexpend_promoted(not_promoted);\n+  }\n+  size_t waste = plab->waste();\n+  HeapWord* top = plab->top();\n+  plab->retire();\n+  if (top != nullptr && plab->waste() > waste && is_in_old(top)) {\n+    \/\/ If retiring the plab created a filler object, then we\n+    \/\/ need to register it with our card scanner so it can\n+    \/\/ safely walk the region backing the plab.\n+    log_debug(gc)(\"retire_plab() is registering remnant of size \" SIZE_FORMAT \" at \" PTR_FORMAT,\n+                  plab->waste() - waste, p2i(top));\n+    card_scan()->register_object_without_lock(top);\n+  }\n+}\n+\n+void ShenandoahHeap::retire_plab(PLAB* plab) {\n+  Thread* thread = Thread::current();\n+  retire_plab(plab, thread);\n+}\n+\n+void ShenandoahHeap::cancel_old_gc() {\n+  shenandoah_assert_safepoint();\n+  assert(_old_generation != nullptr, \"Should only have mixed collections in generation mode.\");\n+  if (_old_generation->state() == ShenandoahOldGeneration::WAITING_FOR_BOOTSTRAP) {\n+    assert(!old_generation()->is_concurrent_mark_in_progress(), \"Cannot be marking in IDLE\");\n+    assert(!old_heuristics()->has_coalesce_and_fill_candidates(), \"Cannot have coalesce and fill candidates in IDLE\");\n+    assert(!old_heuristics()->unprocessed_old_collection_candidates(), \"Cannot have mixed collection candidates in IDLE\");\n+    assert(!young_generation()->is_bootstrap_cycle(), \"Cannot have old mark queues if IDLE\");\n+  } else {\n+    log_info(gc)(\"Terminating old gc cycle.\");\n+    \/\/ Stop marking\n+    old_generation()->cancel_marking();\n+    \/\/ Stop tracking old regions\n+    old_heuristics()->abandon_collection_candidates();\n+    \/\/ Remove old generation access to young generation mark queues\n+    young_generation()->set_old_gen_task_queues(nullptr);\n+    \/\/ Transition to IDLE now.\n+    _old_generation->transition_to(ShenandoahOldGeneration::WAITING_FOR_BOOTSTRAP);\n+  }\n+}\n+\n+\/\/ Make sure old-generation is large enough, but no larger than is necessary, to hold mixed evacuations\n+\/\/ and promotions, if we anticipate either. Any deficit is provided by the young generation, subject to\n+\/\/ xfer_limit, and any excess is transferred to the young generation.\n+\/\/ xfer_limit is the maximum we're able to transfer from young to old.\n+void ShenandoahHeap::adjust_generation_sizes_for_next_cycle(\n+  size_t xfer_limit, size_t young_cset_regions, size_t old_cset_regions) {\n+\n+  \/\/ We can limit the old reserve to the size of anticipated promotions:\n+  \/\/ max_old_reserve is an upper bound on memory evacuated from old and promoted to old,\n+  \/\/ clamped by the old generation space available.\n+  \/\/\n+  \/\/ Here's the algebra.\n+  \/\/ Let SOEP = ShenandoahOldEvacRatioPercent,\n+  \/\/     OE = old evac,\n+  \/\/     YE = young evac, and\n+  \/\/     TE = total evac = OE + YE\n+  \/\/ By definition:\n+  \/\/            SOEP\/100 = OE\/TE\n+  \/\/                     = OE\/(OE+YE)\n+  \/\/  => SOEP\/(100-SOEP) = OE\/((OE+YE)-OE)      \/\/ componendo-dividendo: If a\/b = c\/d, then a\/(b-a) = c\/(d-c)\n+  \/\/                     = OE\/YE\n+  \/\/  =>              OE = YE*SOEP\/(100-SOEP)\n+\n+  \/\/ We have to be careful in the event that SOEP is set to 100 by the user.\n+  assert(ShenandoahOldEvacRatioPercent <= 100, \"Error\");\n+  const size_t old_available = old_generation()->available();\n+  \/\/ The free set will reserve this amount of memory to hold young evacuations\n+  const size_t young_reserve = (young_generation()->max_capacity() * ShenandoahEvacReserve) \/ 100;\n+  const size_t max_old_reserve = (ShenandoahOldEvacRatioPercent == 100) ?\n+     old_available : MIN2((young_reserve * ShenandoahOldEvacRatioPercent) \/ (100 - ShenandoahOldEvacRatioPercent),\n+                          old_available);\n+\n+  const size_t region_size_bytes = ShenandoahHeapRegion::region_size_bytes();\n+\n+  \/\/ Decide how much old space we should reserve for a mixed collection\n+  size_t reserve_for_mixed = 0;\n+  const size_t mixed_candidates = old_heuristics()->unprocessed_old_collection_candidates();\n+  const bool doing_mixed = (mixed_candidates > 0);\n+  if (doing_mixed) {\n+    \/\/ We want this much memory to be unfragmented in order to reliably evacuate old.  This is conservative because we\n+    \/\/ may not evacuate the entirety of unprocessed candidates in a single mixed evacuation.\n+    size_t max_evac_need = (size_t)\n+      (old_heuristics()->unprocessed_old_collection_candidates_live_memory() * ShenandoahOldEvacWaste);\n+    assert(old_available >= old_generation()->free_unaffiliated_regions() * region_size_bytes,\n+           \"Unaffiliated available must be less than total available\");\n+    size_t old_fragmented_available =\n+      old_available - old_generation()->free_unaffiliated_regions() * region_size_bytes;\n+    reserve_for_mixed = max_evac_need + old_fragmented_available;\n+    if (reserve_for_mixed > max_old_reserve) {\n+      reserve_for_mixed = max_old_reserve;\n+    }\n+  }\n+\n+  \/\/ Decide how much space we should reserve for promotions from young\n+  size_t reserve_for_promo = 0;\n+  const size_t promo_load = get_promotion_potential();\n+  const bool doing_promotions = promo_load > 0;\n+  if (doing_promotions) {\n+    \/\/ We're promoting and have a bound on the maximum amount that can be promoted\n+    const size_t available_for_promotions = max_old_reserve - reserve_for_mixed;\n+    reserve_for_promo = MIN2((size_t)(promo_load * ShenandoahPromoEvacWaste), available_for_promotions);\n+  }\n+\n+  \/\/ This is the total old we want to ideally reserve\n+  const size_t old_reserve = reserve_for_mixed + reserve_for_promo;\n+  assert(old_reserve <= max_old_reserve, \"cannot reserve more than max for old evacuations\");\n+\n+  \/\/ We now check if the old generation is running a surplus or a deficit.\n+  size_t old_region_deficit = 0;\n+  size_t old_region_surplus = 0;\n+\n+  const size_t max_old_available = old_generation()->available() + old_cset_regions * region_size_bytes;\n+  if (max_old_available >= old_reserve) {\n+    \/\/ We are running a surplus, so the old region surplus can go to young\n+    const size_t old_surplus = max_old_available - old_reserve;\n+    old_region_surplus = old_surplus \/ region_size_bytes;\n+    const size_t unaffiliated_old_regions = old_generation()->free_unaffiliated_regions() + old_cset_regions;\n+    old_region_surplus = MIN2(old_region_surplus, unaffiliated_old_regions);\n+  } else {\n+    \/\/ We are running a deficit which we'd like to fill from young.\n+    \/\/ Ignore that this will directly impact young_generation()->max_capacity(),\n+    \/\/ indirectly impacting young_reserve and old_reserve.  These computations are conservative.\n+    const size_t old_need = old_reserve - max_old_available;\n+    \/\/ The old region deficit (rounded up) will come from young\n+    old_region_deficit = (old_need + region_size_bytes - 1) \/ region_size_bytes;\n+\n+    \/\/ Round down the regions we can transfer from young to old. If we're running short\n+    \/\/ on young-gen memory, we restrict the xfer. Old-gen collection activities will be\n+    \/\/ curtailed if the budget is restricted.\n+    const size_t max_old_region_xfer = xfer_limit \/ region_size_bytes;\n+    old_region_deficit = MIN2(old_region_deficit, max_old_region_xfer);\n+  }\n+  assert(old_region_deficit == 0 || old_region_surplus == 0, \"Only surplus or deficit, never both\");\n+\n+  set_old_region_surplus(old_region_surplus);\n+  set_old_region_deficit(old_region_deficit);\n+}\n+\n+\/\/ Called from stubs in JIT code or interpreter\n@@ -799,1 +1283,1 @@\n-  HeapWord* res = allocate_memory(req);\n+  HeapWord* res = allocate_memory(req, false);\n@@ -812,1 +1296,1 @@\n-  HeapWord* res = allocate_memory(req);\n+  HeapWord* res = allocate_memory(req, false);\n@@ -821,1 +1305,23 @@\n-HeapWord* ShenandoahHeap::allocate_memory(ShenandoahAllocRequest& req) {\n+HeapWord* ShenandoahHeap::allocate_new_plab(size_t min_size,\n+                                            size_t word_size,\n+                                            size_t* actual_size) {\n+  \/\/ Align requested sizes to card sized multiples\n+  size_t words_in_card = CardTable::card_size_in_words();\n+  size_t align_mask = ~(words_in_card - 1);\n+  min_size = (min_size + words_in_card - 1) & align_mask;\n+  word_size = (word_size + words_in_card - 1) & align_mask;\n+  ShenandoahAllocRequest req = ShenandoahAllocRequest::for_plab(min_size, word_size);\n+  \/\/ Note that allocate_memory() sets a thread-local flag to prohibit further promotions by this thread\n+  \/\/ if we are at risk of infringing on the old-gen evacuation budget.\n+  HeapWord* res = allocate_memory(req, false);\n+  if (res != nullptr) {\n+    *actual_size = req.actual_size();\n+  } else {\n+    *actual_size = 0;\n+  }\n+  return res;\n+}\n+\n+\/\/ is_promotion is true iff this allocation is known for sure to hold the result of young-gen evacuation\n+\/\/ to old-gen.  plab allocates are not known as such, since they may hold old-gen evacuations.\n+HeapWord* ShenandoahHeap::allocate_memory(ShenandoahAllocRequest& req, bool is_promotion) {\n@@ -833,1 +1339,1 @@\n-      result = allocate_memory_under_lock(req, in_new_region);\n+      result = allocate_memory_under_lock(req, in_new_region, is_promotion);\n@@ -847,1 +1353,1 @@\n-      result = allocate_memory_under_lock(req, in_new_region);\n+      result = allocate_memory_under_lock(req, in_new_region, is_promotion);\n@@ -849,0 +1355,1 @@\n+\n@@ -851,1 +1358,1 @@\n-    result = allocate_memory_under_lock(req, in_new_region);\n+    result = allocate_memory_under_lock(req, in_new_region, is_promotion);\n@@ -858,0 +1365,1 @@\n+    regulator_thread()->notify_heap_changed();\n@@ -860,0 +1368,8 @@\n+  if (result == nullptr) {\n+    req.set_actual_size(0);\n+  }\n+\n+  \/\/ This is called regardless of the outcome of the allocation to account\n+  \/\/ for any waste created by retiring regions with this request.\n+  increase_used(req);\n+\n@@ -869,2 +1385,0 @@\n-      notify_mutator_alloc_words(actual, false);\n-\n@@ -877,2 +1391,0 @@\n-    } else {\n-      increase_used(actual*HeapWordSize);\n@@ -885,3 +1397,182 @@\n-HeapWord* ShenandoahHeap::allocate_memory_under_lock(ShenandoahAllocRequest& req, bool& in_new_region) {\n-  ShenandoahHeapLocker locker(lock());\n-  return _free_set->allocate(req, in_new_region);\n+HeapWord* ShenandoahHeap::allocate_memory_under_lock(ShenandoahAllocRequest& req, bool& in_new_region, bool is_promotion) {\n+  bool try_smaller_lab_size = false;\n+  size_t smaller_lab_size;\n+  {\n+    \/\/ promotion_eligible pertains only to PLAB allocations, denoting that the PLAB is allowed to allocate for promotions.\n+    bool promotion_eligible = false;\n+    bool allow_allocation = true;\n+    bool plab_alloc = false;\n+    size_t requested_bytes = req.size() * HeapWordSize;\n+    HeapWord* result = nullptr;\n+    ShenandoahHeapLocker locker(lock());\n+    Thread* thread = Thread::current();\n+\n+    if (mode()->is_generational()) {\n+      if (req.affiliation() == YOUNG_GENERATION) {\n+        if (req.is_mutator_alloc()) {\n+          size_t young_words_available = young_generation()->available() \/ HeapWordSize;\n+          if (ShenandoahElasticTLAB && req.is_lab_alloc() && (req.min_size() < young_words_available)) {\n+            \/\/ Allow ourselves to try a smaller lab size even if requested_bytes <= young_available.  We may need a smaller\n+            \/\/ lab size because young memory has become too fragmented.\n+            try_smaller_lab_size = true;\n+            smaller_lab_size = (young_words_available < req.size())? young_words_available: req.size();\n+          } else if (req.size() > young_words_available) {\n+            \/\/ Can't allocate because even min_size() is larger than remaining young_available\n+            log_info(gc, ergo)(\"Unable to shrink %s alloc request of minimum size: \" SIZE_FORMAT\n+                               \", young words available: \" SIZE_FORMAT, req.type_string(),\n+                               HeapWordSize * (req.is_lab_alloc()? req.min_size(): req.size()), young_words_available);\n+            return nullptr;\n+          }\n+        }\n+      } else {                    \/\/ reg.affiliation() == OLD_GENERATION\n+        assert(req.type() != ShenandoahAllocRequest::_alloc_gclab, \"GCLAB pertains only to young-gen memory\");\n+        if (req.type() ==  ShenandoahAllocRequest::_alloc_plab) {\n+          plab_alloc = true;\n+          size_t promotion_avail = get_promoted_reserve();\n+          size_t promotion_expended = get_promoted_expended();\n+          if (promotion_expended + requested_bytes > promotion_avail) {\n+            promotion_avail = 0;\n+            if (get_old_evac_reserve() == 0) {\n+              \/\/ There are no old-gen evacuations in this pass.  There's no value in creating a plab that cannot\n+              \/\/ be used for promotions.\n+              allow_allocation = false;\n+            }\n+          } else {\n+            promotion_avail = promotion_avail - (promotion_expended + requested_bytes);\n+            promotion_eligible = true;\n+          }\n+        } else if (is_promotion) {\n+          \/\/ This is a shared alloc for promotion\n+          size_t promotion_avail = get_promoted_reserve();\n+          size_t promotion_expended = get_promoted_expended();\n+          if (promotion_expended + requested_bytes > promotion_avail) {\n+            promotion_avail = 0;\n+          } else {\n+            promotion_avail = promotion_avail - (promotion_expended + requested_bytes);\n+          }\n+          if (promotion_avail == 0) {\n+            \/\/ We need to reserve the remaining memory for evacuation.  Reject this allocation.  The object will be\n+            \/\/ evacuated to young-gen memory and promoted during a future GC pass.\n+            return nullptr;\n+          }\n+          \/\/ Else, we'll allow the allocation to proceed.  (Since we hold heap lock, the tested condition remains true.)\n+        } else {\n+          \/\/ This is a shared allocation for evacuation.  Memory has already been reserved for this purpose.\n+        }\n+      }\n+    } \/\/ This ends the is_generational() block\n+\n+    \/\/ First try the original request.  If TLAB request size is greater than available, allocate() will attempt to downsize\n+    \/\/ request to fit within available memory.\n+    result = (allow_allocation)? _free_set->allocate(req, in_new_region): nullptr;\n+    if (result != nullptr) {\n+      if (req.is_old()) {\n+        ShenandoahThreadLocalData::reset_plab_promoted(thread);\n+        if (req.is_gc_alloc()) {\n+          bool disable_plab_promotions = false;\n+          if (req.type() ==  ShenandoahAllocRequest::_alloc_plab) {\n+            if (promotion_eligible) {\n+              size_t actual_size = req.actual_size() * HeapWordSize;\n+              \/\/ The actual size of the allocation may be larger than the requested bytes (due to alignment on card boundaries).\n+              \/\/ If this puts us over our promotion budget, we need to disable future PLAB promotions for this thread.\n+              if (get_promoted_expended() + actual_size <= get_promoted_reserve()) {\n+                \/\/ Assume the entirety of this PLAB will be used for promotion.  This prevents promotion from overreach.\n+                \/\/ When we retire this plab, we'll unexpend what we don't really use.\n+                ShenandoahThreadLocalData::enable_plab_promotions(thread);\n+                expend_promoted(actual_size);\n+                assert(get_promoted_expended() <= get_promoted_reserve(), \"Do not expend more promotion than budgeted\");\n+                ShenandoahThreadLocalData::set_plab_preallocated_promoted(thread, actual_size);\n+              } else {\n+                disable_plab_promotions = true;\n+              }\n+            } else {\n+              disable_plab_promotions = true;\n+            }\n+            if (disable_plab_promotions) {\n+              \/\/ Disable promotions in this thread because entirety of this PLAB must be available to hold old-gen evacuations.\n+              ShenandoahThreadLocalData::disable_plab_promotions(thread);\n+              ShenandoahThreadLocalData::set_plab_preallocated_promoted(thread, 0);\n+            }\n+          } else if (is_promotion) {\n+            \/\/ Shared promotion.  Assume size is requested_bytes.\n+            expend_promoted(requested_bytes);\n+            assert(get_promoted_expended() <= get_promoted_reserve(), \"Do not expend more promotion than budgeted\");\n+          }\n+        }\n+\n+        \/\/ Register the newly allocated object while we're holding the global lock since there's no synchronization\n+        \/\/ built in to the implementation of register_object().  There are potential races when multiple independent\n+        \/\/ threads are allocating objects, some of which might span the same card region.  For example, consider\n+        \/\/ a card table's memory region within which three objects are being allocated by three different threads:\n+        \/\/\n+        \/\/ objects being \"concurrently\" allocated:\n+        \/\/    [-----a------][-----b-----][--------------c------------------]\n+        \/\/            [---- card table memory range --------------]\n+        \/\/\n+        \/\/ Before any objects are allocated, this card's memory range holds no objects.  Note that allocation of object a\n+        \/\/   wants to set the starts-object, first-start, and last-start attributes of the preceding card region.\n+        \/\/   allocation of object b wants to set the starts-object, first-start, and last-start attributes of this card region.\n+        \/\/   allocation of object c also wants to set the starts-object, first-start, and last-start attributes of this\n+        \/\/   card region.\n+        \/\/\n+        \/\/ The thread allocating b and the thread allocating c can \"race\" in various ways, resulting in confusion, such as\n+        \/\/ last-start representing object b while first-start represents object c.  This is why we need to require all\n+        \/\/ register_object() invocations to be \"mutually exclusive\" with respect to each card's memory range.\n+        ShenandoahHeap::heap()->card_scan()->register_object(result);\n+      }\n+    } else {\n+      \/\/ The allocation failed.  If this was a plab allocation, We've already retired it and no longer have a plab.\n+      if (req.is_old() && req.is_gc_alloc() && (req.type() == ShenandoahAllocRequest::_alloc_plab)) {\n+        \/\/ We don't need to disable PLAB promotions because there is no PLAB.  We leave promotions enabled because\n+        \/\/ this allows the surrounding infrastructure to retry alloc_plab_slow() with a smaller PLAB size.\n+        ShenandoahThreadLocalData::set_plab_preallocated_promoted(thread, 0);\n+      }\n+    }\n+    if ((result != nullptr) || !try_smaller_lab_size) {\n+      return result;\n+    }\n+    \/\/ else, fall through to try_smaller_lab_size\n+  } \/\/ This closes the block that holds the heap lock, releasing the lock.\n+\n+  \/\/ We failed to allocate the originally requested lab size.  Let's see if we can allocate a smaller lab size.\n+  if (req.size() == smaller_lab_size) {\n+    \/\/ If we were already trying to allocate min size, no value in attempting to repeat the same.  End the recursion.\n+    return nullptr;\n+  }\n+\n+  \/\/ We arrive here if the tlab allocation request can be resized to fit within young_available\n+  assert((req.affiliation() == YOUNG_GENERATION) && req.is_lab_alloc() && req.is_mutator_alloc() &&\n+         (smaller_lab_size < req.size()), \"Only shrink allocation request size for TLAB allocations\");\n+\n+  \/\/ By convention, ShenandoahAllocationRequest is primarily read-only.  The only mutable instance data is represented by\n+  \/\/ actual_size(), which is overwritten with the size of the allocaion when the allocation request is satisfied.  We use a\n+  \/\/ recursive call here rather than introducing new methods to mutate the existing ShenandoahAllocationRequest argument.\n+  \/\/ Mutation of the existing object might result in astonishing results if calling contexts assume the content of immutable\n+  \/\/ fields remain constant.  The original TLAB allocation request was for memory that exceeded the current capacity.  We'll\n+  \/\/ attempt to allocate a smaller TLAB.  If this is successful, we'll update actual_size() of our incoming\n+  \/\/ ShenandoahAllocRequest.  If the recursive request fails, we'll simply return nullptr.\n+\n+  \/\/ Note that we've relinquished the HeapLock and some other thread may perform additional allocation before our recursive\n+  \/\/ call reacquires the lock.  If that happens, we will need another recursive call to further reduce the size of our request\n+  \/\/ for each time another thread allocates young memory during the brief intervals that the heap lock is available to\n+  \/\/ interfering threads.  We expect this interference to be rare.  The recursion bottoms out when young_available is\n+  \/\/ smaller than req.min_size().  The inner-nested call to allocate_memory_under_lock() uses the same min_size() value\n+  \/\/ as this call, but it uses a preferred size() that is smaller than our preferred size, and is no larger than what we most\n+  \/\/ recently saw as the memory currently available within the young generation.\n+\n+  \/\/ TODO: At the expense of code clarity, we could rewrite this recursive solution to use iteration.  We need at most one\n+  \/\/ extra instance of the ShenandoahAllocRequest, which we can re-initialize multiple times inside a loop, with one iteration\n+  \/\/ of the loop required for each time the existing solution would recurse.  An iterative solution would be more efficient\n+  \/\/ in CPU time and stack memory utilization.  The expectation is that it is very rare that we would recurse more than once\n+  \/\/ so making this change is not currently seen as a high priority.\n+\n+  ShenandoahAllocRequest smaller_req = ShenandoahAllocRequest::for_tlab(req.min_size(), smaller_lab_size);\n+\n+  \/\/ Note that shrinking the preferred size gets us past the gatekeeper that checks whether there's available memory to\n+  \/\/ satisfy the allocation request.  The reality is the actual TLAB size is likely to be even smaller, because it will\n+  \/\/ depend on how much memory is available within mutator regions that are not yet fully used.\n+  HeapWord* result = allocate_memory_under_lock(smaller_req, in_new_region, is_promotion);\n+  if (result != nullptr) {\n+    req.set_actual_size(smaller_req.actual_size());\n+  }\n+  return result;\n@@ -893,1 +1584,1 @@\n-  return allocate_memory(req);\n+  return allocate_memory(req, false);\n@@ -902,2 +1593,2 @@\n-  if (heuristics()->can_unload_classes()) {\n-    ShenandoahHeuristics* h = heuristics();\n+  ShenandoahHeuristics* h = global_generation()->heuristics();\n+  if (h->can_unload_classes()) {\n@@ -982,0 +1673,1 @@\n+\n@@ -987,0 +1679,90 @@\n+      if (_sh->check_cancelled_gc_and_yield(_concurrent)) {\n+        break;\n+      }\n+    }\n+  }\n+};\n+\n+\/\/ Unlike ShenandoahEvacuationTask, this iterates over all regions rather than just the collection set.\n+\/\/ This is needed in order to promote humongous start regions if age() >= tenure threshold.\n+class ShenandoahGenerationalEvacuationTask : public WorkerTask {\n+private:\n+  ShenandoahHeap* const _sh;\n+  ShenandoahRegionIterator *_regions;\n+  bool _concurrent;\n+  uint _tenuring_threshold;\n+\n+public:\n+  ShenandoahGenerationalEvacuationTask(ShenandoahHeap* sh,\n+                                       ShenandoahRegionIterator* iterator,\n+                                       bool concurrent) :\n+    WorkerTask(\"Shenandoah Evacuation\"),\n+    _sh(sh),\n+    _regions(iterator),\n+    _concurrent(concurrent),\n+    _tenuring_threshold(0)\n+  {\n+    if (_sh->mode()->is_generational()) {\n+      _tenuring_threshold = _sh->age_census()->tenuring_threshold();\n+    }\n+  }\n+\n+  void work(uint worker_id) {\n+    if (_concurrent) {\n+      ShenandoahConcurrentWorkerSession worker_session(worker_id);\n+      ShenandoahSuspendibleThreadSetJoiner stsj(ShenandoahSuspendibleWorkers);\n+      ShenandoahEvacOOMScope oom_evac_scope;\n+      do_work();\n+    } else {\n+      ShenandoahParallelWorkerSession worker_session(worker_id);\n+      ShenandoahEvacOOMScope oom_evac_scope;\n+      do_work();\n+    }\n+  }\n+\n+private:\n+  void do_work() {\n+    ShenandoahConcurrentEvacuateRegionObjectClosure cl(_sh);\n+    ShenandoahHeapRegion* r;\n+    ShenandoahMarkingContext* const ctx = ShenandoahHeap::heap()->marking_context();\n+    size_t region_size_bytes = ShenandoahHeapRegion::region_size_bytes();\n+    size_t old_garbage_threshold = (region_size_bytes * ShenandoahOldGarbageThreshold) \/ 100;\n+    while ((r = _regions->next()) != nullptr) {\n+      log_debug(gc)(\"GenerationalEvacuationTask do_work(), looking at %s region \" SIZE_FORMAT \", (age: %d) [%s, %s, %s]\",\n+                    r->is_old()? \"old\": r->is_young()? \"young\": \"free\", r->index(), r->age(),\n+                    r->is_active()? \"active\": \"inactive\",\n+                    r->is_humongous()? (r->is_humongous_start()? \"humongous_start\": \"humongous_continuation\"): \"regular\",\n+                    r->is_cset()? \"cset\": \"not-cset\");\n+\n+      if (r->is_cset()) {\n+        assert(r->has_live(), \"Region \" SIZE_FORMAT \" should have been reclaimed early\", r->index());\n+        _sh->marked_object_iterate(r, &cl);\n+        if (ShenandoahPacing) {\n+          _sh->pacer()->report_evac(r->used() >> LogHeapWordSize);\n+        }\n+      } else if (r->is_young() && r->is_active() && (r->age() >= _tenuring_threshold)) {\n+        HeapWord* tams = ctx->top_at_mark_start(r);\n+        if (r->is_humongous_start()) {\n+          \/\/ We promote humongous_start regions along with their affiliated continuations during evacuation rather than\n+          \/\/ doing this work during a safepoint.  We cannot put humongous regions into the collection set because that\n+          \/\/ triggers the load-reference barrier (LRB) to copy on reference fetch.\n+          r->promote_humongous();\n+        } else if (r->is_regular() && (r->get_top_before_promote() != nullptr)) {\n+          assert(r->garbage_before_padded_for_promote() < old_garbage_threshold,\n+                 \"Region \" SIZE_FORMAT \" has too much garbage for promotion\", r->index());\n+          assert(r->get_top_before_promote() == tams,\n+                 \"Region \" SIZE_FORMAT \" has been used for allocations before promotion\", r->index());\n+          \/\/ Likewise, we cannot put promote-in-place regions into the collection set because that would also trigger\n+          \/\/ the LRB to copy on reference fetch.\n+          r->promote_in_place();\n+        }\n+        \/\/ Aged humongous continuation regions are handled with their start region.  If an aged regular region has\n+        \/\/ more garbage than ShenandoahOldGarbageTrheshold, we'll promote by evacuation.  If there is room for evacuation\n+        \/\/ in this cycle, the region will be in the collection set.  If there is not room, the region will be promoted\n+        \/\/ by evacuation in some future GC cycle.\n+\n+        \/\/ If an aged regular region has received allocations during the current cycle, we do not promote because the\n+        \/\/ newly allocated objects do not have appropriate age; this region's age will be reset to zero at end of cycle.\n+      }\n+      \/\/ else, region is free, or OLD, or not in collection set, or humongous_continuation,\n+      \/\/ or is young humongous_start that is too young to be promoted\n@@ -996,2 +1778,8 @@\n-  ShenandoahEvacuationTask task(this, _collection_set, concurrent);\n-  workers()->run_task(&task);\n+  if (ShenandoahHeap::heap()->mode()->is_generational()) {\n+    ShenandoahRegionIterator regions;\n+    ShenandoahGenerationalEvacuationTask task(this, &regions, concurrent);\n+    workers()->run_task(&task);\n+  } else {\n+    ShenandoahEvacuationTask task(this, _collection_set, concurrent);\n+    workers()->run_task(&task);\n+  }\n@@ -1027,1 +1815,1 @@\n-void ShenandoahHeap::trash_humongous_region_at(ShenandoahHeapRegion* start) {\n+size_t ShenandoahHeap::trash_humongous_region_at(ShenandoahHeapRegion* start) {\n@@ -1047,0 +1835,1 @@\n+  return required_regions;\n@@ -1056,0 +1845,4 @@\n+\n+    PLAB* plab = ShenandoahThreadLocalData::plab(thread);\n+    assert(plab != nullptr, \"PLAB should be initialized for %s\", thread->name());\n+    assert(plab->words_remaining() == 0, \"PLAB should not need retirement\");\n@@ -1071,0 +1864,11 @@\n+\n+    PLAB* plab = ShenandoahThreadLocalData::plab(thread);\n+    assert(plab != nullptr, \"PLAB should be initialized for %s\", thread->name());\n+\n+    \/\/ There are two reasons to retire all plabs between old-gen evacuation passes.\n+    \/\/  1. We need to make the plab memory parsable by remembered-set scanning.\n+    \/\/  2. We need to establish a trustworthy UpdateWaterMark value within each old-gen heap region\n+    ShenandoahHeap::heap()->retire_plab(plab, thread);\n+    if (_resize && ShenandoahThreadLocalData::plab_size(thread) > 0) {\n+      ShenandoahThreadLocalData::set_plab_size(thread, 0);\n+    }\n@@ -1130,0 +1934,1 @@\n+<<<<<<< HEAD\n@@ -1133,0 +1938,13 @@\n+=======\n+  if (ShenandoahElasticTLAB) {\n+    if (mode()->is_generational()) {\n+      return MIN2(ShenandoahHeapRegion::max_tlab_size_bytes(), young_generation()->available());\n+    } else {\n+      \/\/ With Elastic TLABs, return the max allowed size, and let the allocation path\n+      \/\/ figure out the safe size for current allocation.\n+      return ShenandoahHeapRegion::max_tlab_size_bytes();\n+    }\n+  } else {\n+    return MIN2(_free_set->unsafe_peek_free(), ShenandoahHeapRegion::max_tlab_size_bytes());\n+  }\n+>>>>>>> 666656136f7f15905ae770888951d832f46b0535\n@@ -1172,0 +1990,4 @@\n+  if (_shenandoah_policy->is_at_shutdown()) {\n+    return;\n+  }\n+\n@@ -1173,0 +1995,1 @@\n+  tcl->do_thread(_regulator_thread);\n@@ -1192,0 +2015,4 @@\n+    ls.cr();\n+\n+    evac_tracker()->print_global_on(&ls);\n+\n@@ -1197,0 +2024,18 @@\n+void ShenandoahHeap::on_cycle_start(GCCause::Cause cause, ShenandoahGeneration* generation) {\n+  set_gc_cause(cause);\n+  set_gc_generation(generation);\n+\n+  shenandoah_policy()->record_cycle_start();\n+  generation->heuristics()->record_cycle_start();\n+}\n+\n+void ShenandoahHeap::on_cycle_end(ShenandoahGeneration* generation) {\n+  generation->heuristics()->record_cycle_end();\n+  if (mode()->is_generational() && generation->is_global()) {\n+    \/\/ If we just completed a GLOBAL GC, claim credit for completion of young-gen and old-gen GC as well\n+    young_generation()->heuristics()->record_cycle_end();\n+    old_generation()->heuristics()->record_cycle_end();\n+  }\n+  set_gc_cause(GCCause::_no_gc);\n+}\n+\n@@ -1518,23 +2363,0 @@\n-class ShenandoahInitMarkUpdateRegionStateClosure : public ShenandoahHeapRegionClosure {\n-private:\n-  ShenandoahMarkingContext* const _ctx;\n-public:\n-  ShenandoahInitMarkUpdateRegionStateClosure() : _ctx(ShenandoahHeap::heap()->marking_context()) {}\n-\n-  void heap_region_do(ShenandoahHeapRegion* r) {\n-    assert(!r->has_live(), \"Region \" SIZE_FORMAT \" should have no live data\", r->index());\n-    if (r->is_active()) {\n-      \/\/ Check if region needs updating its TAMS. We have updated it already during concurrent\n-      \/\/ reset, so it is very likely we don't need to do another write here.\n-      if (_ctx->top_at_mark_start(r) != r->top()) {\n-        _ctx->capture_top_at_mark_start(r);\n-      }\n-    } else {\n-      assert(_ctx->top_at_mark_start(r) == r->top(),\n-             \"Region \" SIZE_FORMAT \" should already have correct TAMS\", r->index());\n-    }\n-  }\n-\n-  bool is_thread_safe() { return true; }\n-};\n-\n@@ -1556,99 +2378,0 @@\n-class ShenandoahResetUpdateRegionStateClosure : public ShenandoahHeapRegionClosure {\n-private:\n-  ShenandoahMarkingContext* const _ctx;\n-public:\n-  ShenandoahResetUpdateRegionStateClosure() : _ctx(ShenandoahHeap::heap()->marking_context()) {}\n-\n-  void heap_region_do(ShenandoahHeapRegion* r) {\n-    if (r->is_active()) {\n-      \/\/ Reset live data and set TAMS optimistically. We would recheck these under the pause\n-      \/\/ anyway to capture any updates that happened since now.\n-      r->clear_live_data();\n-      _ctx->capture_top_at_mark_start(r);\n-    }\n-  }\n-\n-  bool is_thread_safe() { return true; }\n-};\n-\n-void ShenandoahHeap::prepare_gc() {\n-  reset_mark_bitmap();\n-\n-  ShenandoahResetUpdateRegionStateClosure cl;\n-  parallel_heap_region_iterate(&cl);\n-}\n-\n-class ShenandoahFinalMarkUpdateRegionStateClosure : public ShenandoahHeapRegionClosure {\n-private:\n-  ShenandoahMarkingContext* const _ctx;\n-  ShenandoahHeapLock* const _lock;\n-\n-public:\n-  ShenandoahFinalMarkUpdateRegionStateClosure() :\n-    _ctx(ShenandoahHeap::heap()->complete_marking_context()), _lock(ShenandoahHeap::heap()->lock()) {}\n-\n-  void heap_region_do(ShenandoahHeapRegion* r) {\n-    if (r->is_active()) {\n-      \/\/ All allocations past TAMS are implicitly live, adjust the region data.\n-      \/\/ Bitmaps\/TAMS are swapped at this point, so we need to poll complete bitmap.\n-      HeapWord *tams = _ctx->top_at_mark_start(r);\n-      HeapWord *top = r->top();\n-      if (top > tams) {\n-        r->increase_live_data_alloc_words(pointer_delta(top, tams));\n-      }\n-\n-      \/\/ We are about to select the collection set, make sure it knows about\n-      \/\/ current pinning status. Also, this allows trashing more regions that\n-      \/\/ now have their pinning status dropped.\n-      if (r->is_pinned()) {\n-        if (r->pin_count() == 0) {\n-          ShenandoahHeapLocker locker(_lock);\n-          r->make_unpinned();\n-        }\n-      } else {\n-        if (r->pin_count() > 0) {\n-          ShenandoahHeapLocker locker(_lock);\n-          r->make_pinned();\n-        }\n-      }\n-\n-      \/\/ Remember limit for updating refs. It's guaranteed that we get no\n-      \/\/ from-space-refs written from here on.\n-      r->set_update_watermark_at_safepoint(r->top());\n-    } else {\n-      assert(!r->has_live(), \"Region \" SIZE_FORMAT \" should have no live data\", r->index());\n-      assert(_ctx->top_at_mark_start(r) == r->top(),\n-             \"Region \" SIZE_FORMAT \" should have correct TAMS\", r->index());\n-    }\n-  }\n-\n-  bool is_thread_safe() { return true; }\n-};\n-\n-void ShenandoahHeap::prepare_regions_and_collection_set(bool concurrent) {\n-  assert(!is_full_gc_in_progress(), \"Only for concurrent and degenerated GC\");\n-  {\n-    ShenandoahGCPhase phase(concurrent ? ShenandoahPhaseTimings::final_update_region_states :\n-                                         ShenandoahPhaseTimings::degen_gc_final_update_region_states);\n-    ShenandoahFinalMarkUpdateRegionStateClosure cl;\n-    parallel_heap_region_iterate(&cl);\n-\n-    assert_pinned_region_status();\n-  }\n-\n-  {\n-    ShenandoahGCPhase phase(concurrent ? ShenandoahPhaseTimings::choose_cset :\n-                                         ShenandoahPhaseTimings::degen_gc_choose_cset);\n-    ShenandoahHeapLocker locker(lock());\n-    _collection_set->clear();\n-    heuristics()->choose_collection_set(_collection_set);\n-  }\n-\n-  {\n-    ShenandoahGCPhase phase(concurrent ? ShenandoahPhaseTimings::final_rebuild_freeset :\n-                                         ShenandoahPhaseTimings::degen_gc_final_rebuild_freeset);\n-    ShenandoahHeapLocker locker(lock());\n-    _free_set->rebuild();\n-  }\n-}\n-\n@@ -1665,1 +2388,1 @@\n-  ref_processor()->process_references(phase, workers(), false \/* concurrent *\/);\n+  active_generation()->ref_processor()->process_references(phase, workers(), false \/* concurrent *\/);\n@@ -1696,4 +2419,60 @@\n-void ShenandoahHeap::set_concurrent_mark_in_progress(bool in_progress) {\n-  assert(!has_forwarded_objects(), \"Not expected before\/after mark phase\");\n-  set_gc_state_mask(MARKING, in_progress);\n-  ShenandoahBarrierSet::satb_mark_queue_set().set_active_all_threads(in_progress, !in_progress);\n+void ShenandoahHeap::set_evacuation_reserve_quantities(bool is_valid) {\n+  _has_evacuation_reserve_quantities = is_valid;\n+}\n+\n+void ShenandoahHeap::set_concurrent_young_mark_in_progress(bool in_progress) {\n+  uint mask;\n+  assert(!has_forwarded_objects(), \"Young marking is not concurrent with evacuation\");\n+  if (!in_progress && is_concurrent_old_mark_in_progress()) {\n+    assert(mode()->is_generational(), \"Only generational GC has old marking\");\n+    assert(_gc_state.is_set(MARKING), \"concurrent_old_marking_in_progress implies MARKING\");\n+    \/\/ If old-marking is in progress when we turn off YOUNG_MARKING, leave MARKING (and OLD_MARKING) on\n+    mask = YOUNG_MARKING;\n+  } else {\n+    mask = MARKING | YOUNG_MARKING;\n+  }\n+  set_gc_state_mask(mask, in_progress);\n+  manage_satb_barrier(in_progress);\n+}\n+\n+void ShenandoahHeap::set_concurrent_old_mark_in_progress(bool in_progress) {\n+#ifdef ASSERT\n+  \/\/ has_forwarded_objects() iff UPDATEREFS or EVACUATION\n+  bool has_forwarded = has_forwarded_objects();\n+  bool updating_or_evacuating = _gc_state.is_set(UPDATEREFS | EVACUATION);\n+  bool evacuating = _gc_state.is_set(EVACUATION);\n+  assert ((has_forwarded == updating_or_evacuating) || (evacuating && !has_forwarded && collection_set()->is_empty()),\n+          \"Updating or evacuating iff has forwarded objects, or if evacuation phase is promoting in place without forwarding\");\n+#endif\n+  if (!in_progress && is_concurrent_young_mark_in_progress()) {\n+    \/\/ If young-marking is in progress when we turn off OLD_MARKING, leave MARKING (and YOUNG_MARKING) on\n+    assert(_gc_state.is_set(MARKING), \"concurrent_young_marking_in_progress implies MARKING\");\n+    set_gc_state_mask(OLD_MARKING, in_progress);\n+  } else {\n+    set_gc_state_mask(MARKING | OLD_MARKING, in_progress);\n+  }\n+  manage_satb_barrier(in_progress);\n+}\n+\n+bool ShenandoahHeap::is_prepare_for_old_mark_in_progress() const {\n+  return old_generation()->state() == ShenandoahOldGeneration::FILLING;\n+}\n+\n+void ShenandoahHeap::set_aging_cycle(bool in_progress) {\n+  _is_aging_cycle.set_cond(in_progress);\n+}\n+\n+void ShenandoahHeap::manage_satb_barrier(bool active) {\n+  if (is_concurrent_mark_in_progress()) {\n+    \/\/ Ignore request to deactivate barrier while concurrent mark is in progress.\n+    \/\/ Do not attempt to re-activate the barrier if it is already active.\n+    if (active && !ShenandoahBarrierSet::satb_mark_queue_set().is_active()) {\n+      ShenandoahBarrierSet::satb_mark_queue_set().set_active_all_threads(active, !active);\n+    }\n+  } else {\n+    \/\/ No concurrent marking is in progress so honor request to deactivate,\n+    \/\/ but only if the barrier is already active.\n+    if (!active && ShenandoahBarrierSet::satb_mark_queue_set().is_active()) {\n+      ShenandoahBarrierSet::satb_mark_queue_set().set_active_all_threads(active, !active);\n+    }\n+  }\n@@ -1732,0 +2511,8 @@\n+void ShenandoahHeap::cancel_concurrent_mark() {\n+  _young_generation->cancel_marking();\n+  _old_generation->cancel_marking();\n+  _global_generation->cancel_marking();\n+\n+  ShenandoahBarrierSet::satb_mark_queue_set().abandon_partial_marking();\n+}\n+\n@@ -1737,0 +2524,1 @@\n+    _cancel_requested_time = os::elapsedTime();\n@@ -1747,1 +2535,1 @@\n-  \/\/ Step 0. Notify policy to disable event recording.\n+  \/\/ Step 1. Notify policy to disable event recording and prevent visiting gc threads during shutdown\n@@ -1750,1 +2538,4 @@\n-  \/\/ Step 1. Notify control thread that we are in shutdown.\n+  \/\/ Step 2. Stop requesting collections.\n+  regulator_thread()->stop();\n+\n+  \/\/ Step 3. Notify control thread that we are in shutdown.\n@@ -1755,1 +2546,1 @@\n-  \/\/ Step 2. Notify GC workers that we are cancelling GC.\n+  \/\/ Step 4. Notify GC workers that we are cancelling GC.\n@@ -1758,1 +2549,1 @@\n-  \/\/ Step 3. Wait until GC worker exits normally.\n+  \/\/ Step 5. Wait until GC worker exits normally.\n@@ -1852,0 +2643,1 @@\n+<<<<<<< HEAD\n@@ -1854,0 +2646,4 @@\n+=======\n+address ShenandoahHeap::gc_state_addr() {\n+  return (address) ShenandoahHeap::heap()->_gc_state.addr_of();\n+>>>>>>> 666656136f7f15905ae770888951d832f46b0535\n@@ -1857,1 +2653,6 @@\n-  Atomic::store(&_bytes_allocated_since_gc_start, (size_t)0);\n+  if (mode()->is_generational()) {\n+    young_generation()->reset_bytes_allocated_since_gc_start();\n+    old_generation()->reset_bytes_allocated_since_gc_start();\n+  }\n+\n+  global_generation()->reset_bytes_allocated_since_gc_start();\n@@ -1921,2 +2722,4 @@\n-    assert((r->is_pinned() && r->pin_count() > 0) || (!r->is_pinned() && r->pin_count() == 0),\n-           \"Region \" SIZE_FORMAT \" pinning status is inconsistent\", i);\n+    if (active_generation()->contains(r)) {\n+      assert((r->is_pinned() && r->pin_count() > 0) || (!r->is_pinned() && r->pin_count() == 0),\n+             \"Region \" SIZE_FORMAT \" pinning status is inconsistent\", i);\n+    }\n@@ -1982,0 +2785,2 @@\n+  ShenandoahRegionChunkIterator* _work_chunks;\n+\n@@ -1983,1 +2788,2 @@\n-  ShenandoahUpdateHeapRefsTask(ShenandoahRegionIterator* regions) :\n+  explicit ShenandoahUpdateHeapRefsTask(ShenandoahRegionIterator* regions,\n+                                        ShenandoahRegionChunkIterator* work_chunks) :\n@@ -1986,1 +2792,4 @@\n-    _regions(regions) {\n+    _regions(regions),\n+    _work_chunks(work_chunks)\n+  {\n+    log_info(gc, remset)(\"Scan remembered set using bitmap: %s\", BOOL_TO_STR(_heap->is_old_bitmap_stable()));\n@@ -1993,1 +2802,1 @@\n-      do_work<ShenandoahConcUpdateRefsClosure>();\n+      do_work<ShenandoahConcUpdateRefsClosure>(worker_id);\n@@ -1996,1 +2805,1 @@\n-      do_work<ShenandoahSTWUpdateRefsClosure>();\n+      do_work<ShenandoahSTWUpdateRefsClosure>(worker_id);\n@@ -2002,1 +2811,1 @@\n-  void do_work() {\n+  void do_work(uint worker_id) {\n@@ -2004,0 +2813,10 @@\n+    if (CONCURRENT && (worker_id == 0)) {\n+      \/\/ We ask the first worker to replenish the Mutator free set by moving regions previously reserved to hold the\n+      \/\/ results of evacuation.  These reserves are no longer necessary because evacuation has completed.\n+      size_t cset_regions = _heap->collection_set()->count();\n+      \/\/ We cannot transfer any more regions than will be reclaimed when the existing collection set is recycled, because\n+      \/\/ we need the reclaimed collection set regions to replenish the collector reserves\n+      _heap->free_set()->move_collector_sets_to_mutator(cset_regions);\n+    }\n+    \/\/ If !CONCURRENT, there's no value in expanding Mutator free set\n+\n@@ -2005,1 +2824,4 @@\n-    ShenandoahMarkingContext* const ctx = _heap->complete_marking_context();\n+    \/\/ We update references for global, old, and young collections.\n+    assert(_heap->active_generation()->is_mark_complete(), \"Expected complete marking\");\n+    ShenandoahMarkingContext* const ctx = _heap->marking_context();\n+    bool is_mixed = _heap->collection_set()->has_old_regions();\n@@ -2009,0 +2831,3 @@\n+\n+      log_debug(gc)(\"ShenandoahUpdateHeapRefsTask::do_work(%u) looking at region \" SIZE_FORMAT, worker_id, r->index());\n+      bool region_progress = false;\n@@ -2010,1 +2835,31 @@\n-        _heap->marked_object_oop_iterate(r, &cl, update_watermark);\n+        if (!_heap->mode()->is_generational() || r->is_young()) {\n+          _heap->marked_object_oop_iterate(r, &cl, update_watermark);\n+          region_progress = true;\n+        } else if (r->is_old()) {\n+          if (_heap->active_generation()->is_global()) {\n+            \/\/ Note that GLOBAL collection is not as effectively balanced as young and mixed cycles.  This is because\n+            \/\/ concurrent GC threads are parceled out entire heap regions of work at a time and there\n+            \/\/ is no \"catchup phase\" consisting of remembered set scanning, during which parcels of work are smaller\n+            \/\/ and more easily distributed more fairly across threads.\n+\n+            \/\/ TODO: Consider an improvement to load balance GLOBAL GC.\n+            _heap->marked_object_oop_iterate(r, &cl, update_watermark);\n+            region_progress = true;\n+          }\n+          \/\/ Otherwise, this is an old region in a young or mixed cycle.  Process it during a second phase, below.\n+          \/\/ Don't bother to report pacing progress in this case.\n+        } else {\n+          \/\/ Because updating of references runs concurrently, it is possible that a FREE inactive region transitions\n+          \/\/ to a non-free active region while this loop is executing.  Whenever this happens, the changing of a region's\n+          \/\/ active status may propagate at a different speed than the changing of the region's affiliation.\n+\n+          \/\/ When we reach this control point, it is because a race has allowed a region's is_active() status to be seen\n+          \/\/ by this thread before the region's affiliation() is seen by this thread.\n+\n+          \/\/ It's ok for this race to occur because the newly transformed region does not have any references to be\n+          \/\/ updated.\n+\n+          assert(r->get_update_watermark() == r->bottom(),\n+                 \"%s Region \" SIZE_FORMAT \" is_active but not recognized as YOUNG or OLD so must be newly transitioned from FREE\",\n+                 r->affiliation_name(), r->index());\n+        }\n@@ -2012,1 +2867,1 @@\n-      if (ShenandoahPacing) {\n+      if (region_progress && ShenandoahPacing) {\n@@ -2020,0 +2875,114 @@\n+\n+    if (_heap->mode()->is_generational() && !_heap->active_generation()->is_global()) {\n+      \/\/ Since this is generational and not GLOBAL, we have to process the remembered set.  There's no remembered\n+      \/\/ set processing if not in generational mode or if GLOBAL mode.\n+\n+      \/\/ After this thread has exhausted its traditional update-refs work, it continues with updating refs within remembered set.\n+      \/\/ The remembered set workload is better balanced between threads, so threads that are \"behind\" can catch up with other\n+      \/\/ threads during this phase, allowing all threads to work more effectively in parallel.\n+      struct ShenandoahRegionChunk assignment;\n+      RememberedScanner* scanner = _heap->card_scan();\n+\n+      while (!_heap->check_cancelled_gc_and_yield(CONCURRENT) && _work_chunks->next(&assignment)) {\n+        \/\/ Keep grabbing next work chunk to process until finished, or asked to yield\n+        ShenandoahHeapRegion* r = assignment._r;\n+        if (r->is_active() && !r->is_cset() && r->is_old()) {\n+          HeapWord* start_of_range = r->bottom() + assignment._chunk_offset;\n+          HeapWord* end_of_range = r->get_update_watermark();\n+          if (end_of_range > start_of_range + assignment._chunk_size) {\n+            end_of_range = start_of_range + assignment._chunk_size;\n+          }\n+\n+          \/\/ Old region in a young cycle or mixed cycle.\n+          if (is_mixed) {\n+            \/\/ TODO: For mixed evac, consider building an old-gen remembered set that allows restricted updating\n+            \/\/ within old-gen HeapRegions.  This remembered set can be constructed by old-gen concurrent marking\n+            \/\/ and augmented by card marking.  For example, old-gen concurrent marking can remember for each old-gen\n+            \/\/ card which other old-gen regions it refers to: none, one-other specifically, multiple-other non-specific.\n+            \/\/ Update-references when _mixed_evac processess each old-gen memory range that has a traditional DIRTY\n+            \/\/ card or if the \"old-gen remembered set\" indicates that this card holds pointers specifically to an\n+            \/\/ old-gen region in the most recent collection set, or if this card holds pointers to other non-specific\n+            \/\/ old-gen heap regions.\n+\n+            if (r->is_humongous()) {\n+              if (start_of_range < end_of_range) {\n+                \/\/ Need to examine both dirty and clean cards during mixed evac.\n+                r->oop_iterate_humongous_slice(&cl, false, start_of_range, assignment._chunk_size, true);\n+              }\n+            } else {\n+              \/\/ Since this is mixed evacuation, old regions that are candidates for collection have not been coalesced\n+              \/\/ and filled.  Use mark bits to find objects that need to be updated.\n+              \/\/\n+              \/\/ Future TODO: establish a second remembered set to identify which old-gen regions point to other old-gen\n+              \/\/ regions which are in the collection set for a particular mixed evacuation.\n+              if (start_of_range < end_of_range) {\n+                HeapWord* p = nullptr;\n+                size_t card_index = scanner->card_index_for_addr(start_of_range);\n+                \/\/ In case last object in my range spans boundary of my chunk, I may need to scan all the way to top()\n+                ShenandoahObjectToOopBoundedClosure<T> objs(&cl, start_of_range, r->top());\n+\n+                \/\/ Any object that begins in a previous range is part of a different scanning assignment.  Any object that\n+                \/\/ starts after end_of_range is also not my responsibility.  (Either allocated during evacuation, so does\n+                \/\/ not hold pointers to from-space, or is beyond the range of my assigned work chunk.)\n+\n+                \/\/ Find the first object that begins in my range, if there is one.\n+                p = start_of_range;\n+                oop obj = cast_to_oop(p);\n+                HeapWord* tams = ctx->top_at_mark_start(r);\n+                if (p >= tams) {\n+                  \/\/ We cannot use ctx->is_marked(obj) to test whether an object begins at this address.  Instead,\n+                  \/\/ we need to use the remembered set crossing map to advance p to the first object that starts\n+                  \/\/ within the enclosing card.\n+\n+                  while (true) {\n+                    HeapWord* first_object = scanner->first_object_in_card(card_index);\n+                    if (first_object != nullptr) {\n+                      p = first_object;\n+                      break;\n+                    } else if (scanner->addr_for_card_index(card_index + 1) < end_of_range) {\n+                      card_index++;\n+                    } else {\n+                      \/\/ Force the loop that follows to immediately terminate.\n+                      p = end_of_range;\n+                      break;\n+                    }\n+                  }\n+                  obj = cast_to_oop(p);\n+                  \/\/ Note: p may be >= end_of_range\n+                } else if (!ctx->is_marked(obj)) {\n+                  p = ctx->get_next_marked_addr(p, tams);\n+                  obj = cast_to_oop(p);\n+                  \/\/ If there are no more marked objects before tams, this returns tams.\n+                  \/\/ Note that tams is either >= end_of_range, or tams is the start of an object that is marked.\n+                }\n+                while (p < end_of_range) {\n+                  \/\/ p is known to point to the beginning of marked object obj\n+                  objs.do_object(obj);\n+                  HeapWord* prev_p = p;\n+                  p += obj->size();\n+                  if (p < tams) {\n+                    p = ctx->get_next_marked_addr(p, tams);\n+                    \/\/ If there are no more marked objects before tams, this returns tams.  Note that tams is\n+                    \/\/ either >= end_of_range, or tams is the start of an object that is marked.\n+                  }\n+                  assert(p != prev_p, \"Lack of forward progress\");\n+                  obj = cast_to_oop(p);\n+                }\n+              }\n+            }\n+          } else {\n+            \/\/ This is a young evac..\n+            if (start_of_range < end_of_range) {\n+              size_t cluster_size =\n+                CardTable::card_size_in_words() * ShenandoahCardCluster<ShenandoahDirectCardMarkRememberedSet>::CardsPerCluster;\n+              size_t clusters = assignment._chunk_size \/ cluster_size;\n+              assert(clusters * cluster_size == assignment._chunk_size, \"Chunk assignment must align on cluster boundaries\");\n+              scanner->process_region_slice(r, assignment._chunk_offset, clusters, end_of_range, &cl, true, worker_id);\n+            }\n+          }\n+          if (ShenandoahPacing && (start_of_range < end_of_range)) {\n+            _heap->pacer()->report_updaterefs(pointer_delta(end_of_range, start_of_range));\n+          }\n+        }\n+      }\n+    }\n@@ -2025,0 +2994,2 @@\n+  uint nworkers = workers()->active_workers();\n+  ShenandoahRegionChunkIterator work_list(nworkers);\n@@ -2027,1 +2998,1 @@\n-    ShenandoahUpdateHeapRefsTask<true> task(&_update_refs_iterator);\n+    ShenandoahUpdateHeapRefsTask<true> task(&_update_refs_iterator, &work_list);\n@@ -2030,1 +3001,1 @@\n-    ShenandoahUpdateHeapRefsTask<false> task(&_update_refs_iterator);\n+    ShenandoahUpdateHeapRefsTask<false> task(&_update_refs_iterator, &work_list);\n@@ -2033,0 +3004,3 @@\n+  if (ShenandoahEnableCardStats && card_scan()!=nullptr) { \/\/ generational check proxy\n+    card_scan()->log_card_stats(nworkers, CARD_STAT_UPDATE_REFS);\n+  }\n@@ -2035,1 +3009,0 @@\n-\n@@ -2038,0 +3011,1 @@\n+  ShenandoahMarkingContext* _ctx;\n@@ -2039,0 +3013,1 @@\n+  bool _is_generational;\n@@ -2041,1 +3016,3 @@\n-  ShenandoahFinalUpdateRefsUpdateRegionStateClosure() : _lock(ShenandoahHeap::heap()->lock()) {}\n+  ShenandoahFinalUpdateRefsUpdateRegionStateClosure(\n+    ShenandoahMarkingContext* ctx) : _ctx(ctx), _lock(ShenandoahHeap::heap()->lock()),\n+                                     _is_generational(ShenandoahHeap::heap()->mode()->is_generational()) { }\n@@ -2044,0 +3021,21 @@\n+\n+    \/\/ Maintenance of region age must follow evacuation in order to account for evacuation allocations within survivor\n+    \/\/ regions.  We consult region age during the subsequent evacuation to determine whether certain objects need to\n+    \/\/ be promoted.\n+    if (_is_generational && r->is_young() && r->is_active()) {\n+      HeapWord *tams = _ctx->top_at_mark_start(r);\n+      HeapWord *top = r->top();\n+\n+      \/\/ Allocations move the watermark when top moves.  However compacting\n+      \/\/ objects will sometimes lower top beneath the watermark, after which,\n+      \/\/ attempts to read the watermark will assert out (watermark should not be\n+      \/\/ higher than top).\n+      if (top > tams) {\n+        \/\/ There have been allocations in this region since the start of the cycle.\n+        \/\/ Any objects new to this region must not assimilate elevated age.\n+        r->reset_age();\n+      } else if (ShenandoahHeap::heap()->is_aging_cycle()) {\n+        r->increment_age();\n+      }\n+    }\n+\n@@ -2046,1 +3044,0 @@\n-\n@@ -2073,1 +3070,1 @@\n-    ShenandoahFinalUpdateRefsUpdateRegionStateClosure cl;\n+    ShenandoahFinalUpdateRefsUpdateRegionStateClosure cl (active_generation()->complete_marking_context());\n@@ -2088,6 +3085,84 @@\n-  {\n-    ShenandoahGCPhase phase(concurrent ?\n-                            ShenandoahPhaseTimings::final_update_refs_rebuild_freeset :\n-                            ShenandoahPhaseTimings::degen_gc_final_update_refs_rebuild_freeset);\n-    ShenandoahHeapLocker locker(lock());\n-    _free_set->rebuild();\n+  ShenandoahGCPhase phase(concurrent ?\n+                          ShenandoahPhaseTimings::final_update_refs_rebuild_freeset :\n+                          ShenandoahPhaseTimings::degen_gc_final_update_refs_rebuild_freeset);\n+  size_t region_size_bytes = ShenandoahHeapRegion::region_size_bytes();\n+  ShenandoahHeapLocker locker(lock());\n+  size_t young_cset_regions, old_cset_regions;\n+  size_t first_old_region, last_old_region, old_region_count;\n+  _free_set->prepare_to_rebuild(young_cset_regions, old_cset_regions, first_old_region, last_old_region, old_region_count);\n+  \/\/ If there are no old regions, first_old_region will be greater than last_old_region\n+  assert((first_old_region > last_old_region) ||\n+         ((last_old_region + 1 - first_old_region >= old_region_count) &&\n+          get_region(first_old_region)->is_old() && get_region(last_old_region)->is_old()),\n+         \"sanity: old_region_count: \" SIZE_FORMAT \", first_old_region: \" SIZE_FORMAT \", last_old_region: \" SIZE_FORMAT,\n+         old_region_count, first_old_region, last_old_region);\n+\n+  if (mode()->is_generational()) {\n+    assert(verify_generation_usage(true, old_generation()->used_regions(),\n+                                   old_generation()->used(), old_generation()->get_humongous_waste(),\n+                                   true, young_generation()->used_regions(),\n+                                   young_generation()->used(), young_generation()->get_humongous_waste()),\n+           \"Generation accounts are inaccurate\");\n+\n+    \/\/ The computation of bytes_of_allocation_runway_before_gc_trigger is quite conservative so consider all of this\n+    \/\/ available for transfer to old. Note that transfer of humongous regions does not impact available.\n+    size_t allocation_runway = young_heuristics()->bytes_of_allocation_runway_before_gc_trigger(young_cset_regions);\n+    adjust_generation_sizes_for_next_cycle(allocation_runway, young_cset_regions, old_cset_regions);\n+\n+    \/\/ Total old_available may have been expanded to hold anticipated promotions.  We trigger if the fragmented available\n+    \/\/ memory represents more than 16 regions worth of data.  Note that fragmentation may increase when we promote regular\n+    \/\/ regions in place when many of these regular regions have an abundant amount of available memory within them.  Fragmentation\n+    \/\/ will decrease as promote-by-copy consumes the available memory within these partially consumed regions.\n+    \/\/\n+    \/\/ We consider old-gen to have excessive fragmentation if more than 12.5% of old-gen is free memory that resides\n+    \/\/ within partially consumed regions of memory.\n+  }\n+  \/\/ Rebuild free set based on adjusted generation sizes.\n+  _free_set->rebuild(young_cset_regions, old_cset_regions);\n+\n+  if (mode()->is_generational() && (ShenandoahGenerationalHumongousReserve > 0)) {\n+    size_t old_region_span = (first_old_region <= last_old_region)? (last_old_region + 1 - first_old_region): 0;\n+    size_t allowed_old_gen_span = num_regions() - (ShenandoahGenerationalHumongousReserve * num_regions() \/ 100);\n+\n+    \/\/ Tolerate lower density if total span is small.  Here's the implementation:\n+    \/\/   if old_gen spans more than 100% and density < 75%, trigger old-defrag\n+    \/\/   else if old_gen spans more than 87.5% and density < 62.5%, trigger old-defrag\n+    \/\/   else if old_gen spans more than 75% and density < 50%, trigger old-defrag\n+    \/\/   else if old_gen spans more than 62.5% and density < 37.5%, trigger old-defrag\n+    \/\/   else if old_gen spans more than 50% and density < 25%, trigger old-defrag\n+    \/\/\n+    \/\/ A previous implementation was more aggressive in triggering, resulting in degraded throughput when\n+    \/\/ humongous allocation was not required.\n+\n+    ShenandoahGeneration* old_gen = old_generation();\n+    size_t old_available = old_gen->available();\n+    size_t region_size_bytes = ShenandoahHeapRegion::region_size_bytes();\n+    size_t old_unaffiliated_available = old_gen->free_unaffiliated_regions() * region_size_bytes;\n+    assert(old_available >= old_unaffiliated_available, \"sanity\");\n+    size_t old_fragmented_available = old_available - old_unaffiliated_available;\n+\n+    size_t old_bytes_consumed = old_region_count * region_size_bytes - old_fragmented_available;\n+    size_t old_bytes_spanned = old_region_span * region_size_bytes;\n+    double old_density = ((double) old_bytes_consumed) \/ old_bytes_spanned;\n+\n+    uint eighths = 8;\n+    for (uint i = 0; i < 5; i++) {\n+      size_t span_threshold = eighths * allowed_old_gen_span \/ 8;\n+      double density_threshold = (eighths - 2) \/ 8.0;\n+      if ((old_region_span >= span_threshold) && (old_density < density_threshold)) {\n+        old_heuristics()->trigger_old_is_fragmented(old_density, first_old_region, last_old_region);\n+        break;\n+      }\n+      eighths--;\n+    }\n+\n+    size_t old_used = old_generation()->used() + old_generation()->get_humongous_waste();\n+    size_t trigger_threshold = old_generation()->usage_trigger_threshold();\n+    \/\/ Detects unsigned arithmetic underflow\n+    assert(old_used <= capacity(),\n+           \"Old used (\" SIZE_FORMAT \", \" SIZE_FORMAT\") must not be more than heap capacity (\" SIZE_FORMAT \")\",\n+           old_generation()->used(), old_generation()->get_humongous_waste(), capacity());\n+\n+    if (old_used > trigger_threshold) {\n+      old_heuristics()->trigger_old_has_grown();\n+    }\n@@ -2208,3 +3283,12 @@\n-  _memory_pool = new ShenandoahMemoryPool(this);\n-  _cycle_memory_manager.add_pool(_memory_pool);\n-  _stw_memory_manager.add_pool(_memory_pool);\n+  if (mode()->is_generational()) {\n+    _young_gen_memory_pool = new ShenandoahYoungGenMemoryPool(this);\n+    _old_gen_memory_pool = new ShenandoahOldGenMemoryPool(this);\n+    _cycle_memory_manager.add_pool(_young_gen_memory_pool);\n+    _cycle_memory_manager.add_pool(_old_gen_memory_pool);\n+    _stw_memory_manager.add_pool(_young_gen_memory_pool);\n+    _stw_memory_manager.add_pool(_old_gen_memory_pool);\n+  } else {\n+    _memory_pool = new ShenandoahMemoryPool(this);\n+    _cycle_memory_manager.add_pool(_memory_pool);\n+    _stw_memory_manager.add_pool(_memory_pool);\n+  }\n@@ -2222,1 +3306,6 @@\n-  memory_pools.append(_memory_pool);\n+  if (mode()->is_generational()) {\n+    memory_pools.append(_young_gen_memory_pool);\n+    memory_pools.append(_old_gen_memory_pool);\n+  } else {\n+    memory_pools.append(_memory_pool);\n+  }\n@@ -2227,1 +3316,1 @@\n-  return _memory_pool->get_memory_usage();\n+  return MemoryUsage(_initial_size, used(), committed(), max_capacity());\n@@ -2265,0 +3354,1 @@\n+\n@@ -2292,0 +3382,103 @@\n+\n+void ShenandoahHeap::transfer_old_pointers_from_satb() {\n+  _old_generation->transfer_pointers_from_satb();\n+}\n+\n+template<>\n+void ShenandoahGenerationRegionClosure<YOUNG>::heap_region_do(ShenandoahHeapRegion* region) {\n+  \/\/ Visit young regions\n+  if (region->is_young()) {\n+    _cl->heap_region_do(region);\n+  }\n+}\n+\n+template<>\n+void ShenandoahGenerationRegionClosure<OLD>::heap_region_do(ShenandoahHeapRegion* region) {\n+  \/\/ Visit old regions\n+  if (region->is_old()) {\n+    _cl->heap_region_do(region);\n+  }\n+}\n+\n+template<>\n+void ShenandoahGenerationRegionClosure<GLOBAL_GEN>::heap_region_do(ShenandoahHeapRegion* region) {\n+  _cl->heap_region_do(region);\n+}\n+\n+template<>\n+void ShenandoahGenerationRegionClosure<GLOBAL_NON_GEN>::heap_region_do(ShenandoahHeapRegion* region) {\n+  _cl->heap_region_do(region);\n+}\n+\n+bool ShenandoahHeap::verify_generation_usage(bool verify_old, size_t old_regions, size_t old_bytes, size_t old_waste,\n+                                             bool verify_young, size_t young_regions, size_t young_bytes, size_t young_waste) {\n+  size_t tally_old_regions = 0;\n+  size_t tally_old_bytes = 0;\n+  size_t tally_old_waste = 0;\n+  size_t tally_young_regions = 0;\n+  size_t tally_young_bytes = 0;\n+  size_t tally_young_waste = 0;\n+\n+  shenandoah_assert_heaplocked_or_safepoint();\n+  for (size_t i = 0; i < num_regions(); i++) {\n+    ShenandoahHeapRegion* r = get_region(i);\n+    if (r->is_old()) {\n+      tally_old_regions++;\n+      tally_old_bytes += r->used();\n+      if (r->is_humongous()) {\n+        ShenandoahHeapRegion* start = r->humongous_start_region();\n+        HeapWord* obj_addr = start->bottom();\n+        oop obj = cast_to_oop(obj_addr);\n+        size_t word_size = obj->size();\n+        HeapWord* end_addr = obj_addr + word_size;\n+        if (end_addr <= r->end()) {\n+          tally_old_waste += (r->end() - end_addr) * HeapWordSize;\n+        }\n+      }\n+    } else if (r->is_young()) {\n+      tally_young_regions++;\n+      tally_young_bytes += r->used();\n+      if (r->is_humongous()) {\n+        ShenandoahHeapRegion* start = r->humongous_start_region();\n+        HeapWord* obj_addr = start->bottom();\n+        oop obj = cast_to_oop(obj_addr);\n+        size_t word_size = obj->size();\n+        HeapWord* end_addr = obj_addr + word_size;\n+        if (end_addr <= r->end()) {\n+          tally_young_waste += (r->end() - end_addr) * HeapWordSize;\n+        }\n+      }\n+    }\n+  }\n+  if (verify_young &&\n+      ((young_regions != tally_young_regions) || (young_bytes != tally_young_bytes) || (young_waste != tally_young_waste))) {\n+    return false;\n+  } else if (verify_old &&\n+             ((old_regions != tally_old_regions) || (old_bytes != tally_old_bytes) || (old_waste != tally_old_waste))) {\n+    return false;\n+  } else {\n+    return true;\n+  }\n+}\n+\n+ShenandoahGeneration* ShenandoahHeap::generation_for(ShenandoahAffiliation affiliation) const {\n+  if (!mode()->is_generational()) {\n+    return global_generation();\n+  } else if (affiliation == YOUNG_GENERATION) {\n+    return young_generation();\n+  } else if (affiliation == OLD_GENERATION) {\n+    return old_generation();\n+  }\n+\n+  ShouldNotReachHere();\n+  return nullptr;\n+}\n+\n+void ShenandoahHeap::log_heap_status(const char* msg) const {\n+  if (mode()->is_generational()) {\n+    young_generation()->log_status(msg);\n+    old_generation()->log_status(msg);\n+  } else {\n+    global_generation()->log_status(msg);\n+  }\n+}\n","filename":"src\/hotspot\/share\/gc\/shenandoah\/shenandoahHeap.cpp","additions":1479,"deletions":286,"binary":false,"changes":1765,"status":"modified"},{"patch":"@@ -4,0 +4,1 @@\n+ * Copyright Amazon.com Inc. or its affiliates. All Rights Reserved.\n@@ -29,0 +30,1 @@\n+#include \"gc\/shared\/ageTable.hpp\"\n@@ -32,0 +34,2 @@\n+#include \"gc\/shenandoah\/shenandoahAgeCensus.hpp\"\n+#include \"gc\/shenandoah\/heuristics\/shenandoahSpaceInfo.hpp\"\n@@ -34,0 +38,1 @@\n+#include \"gc\/shenandoah\/shenandoahAsserts.hpp\"\n@@ -36,0 +41,3 @@\n+#include \"gc\/shenandoah\/shenandoahEvacTracker.hpp\"\n+#include \"gc\/shenandoah\/shenandoahGenerationType.hpp\"\n+#include \"gc\/shenandoah\/shenandoahMmuTracker.hpp\"\n@@ -37,0 +45,1 @@\n+#include \"gc\/shenandoah\/shenandoahScanRemembered.hpp\"\n@@ -46,0 +55,1 @@\n+class PLAB;\n@@ -48,0 +58,1 @@\n+class ShenandoahRegulatorThread;\n@@ -50,0 +61,3 @@\n+class ShenandoahGeneration;\n+class ShenandoahYoungGeneration;\n+class ShenandoahOldGeneration;\n@@ -51,0 +65,2 @@\n+class ShenandoahOldHeuristics;\n+class ShenandoahYoungHeuristics;\n@@ -52,1 +68,0 @@\n-class ShenandoahMode;\n@@ -62,0 +77,1 @@\n+class ShenandoahMode;\n@@ -111,0 +127,10 @@\n+template<ShenandoahGenerationType GENERATION>\n+class ShenandoahGenerationRegionClosure : public ShenandoahHeapRegionClosure {\n+ public:\n+  explicit ShenandoahGenerationRegionClosure(ShenandoahHeapRegionClosure* cl) : _cl(cl) {}\n+  void heap_region_do(ShenandoahHeapRegion* r);\n+  virtual bool is_thread_safe() { return _cl->is_thread_safe(); }\n+ private:\n+  ShenandoahHeapRegionClosure* _cl;\n+};\n+\n@@ -128,0 +154,1 @@\n+  friend class ShenandoahOldGC;\n@@ -136,0 +163,1 @@\n+  ShenandoahGeneration* _gc_generation;\n@@ -142,0 +170,17 @@\n+  ShenandoahGeneration* active_generation() const {\n+    \/\/ last or latest generation might be a better name here.\n+    return _gc_generation;\n+  }\n+\n+  void set_gc_generation(ShenandoahGeneration* generation) {\n+    _gc_generation = generation;\n+  }\n+\n+  ShenandoahHeuristics* heuristics();\n+  ShenandoahOldHeuristics* old_heuristics();\n+  ShenandoahYoungHeuristics* young_heuristics();\n+\n+  bool doing_mixed_evacuations();\n+  bool is_old_bitmap_stable() const;\n+  bool is_gc_generation_young() const;\n+\n@@ -153,3 +198,2 @@\n-  void initialize_mode();\n-  void initialize_heuristics();\n-\n+  void initialize_heuristics_generations();\n+  virtual void print_init_logger() const;\n@@ -168,0 +212,3 @@\n+  bool verify_generation_usage(bool verify_old, size_t old_regions, size_t old_bytes, size_t old_waste,\n+                               bool verify_young, size_t young_regions, size_t young_bytes, size_t young_waste);\n+\n@@ -176,2 +223,7 @@\n-           size_t _initial_size;\n-           size_t _minimum_size;\n+  size_t _initial_size;\n+  size_t _minimum_size;\n+  size_t _promotion_potential;\n+  size_t _pad_for_promote_in_place;    \/\/ bytes of filler\n+  size_t _promotable_humongous_regions;\n+  size_t _regular_regions_promoted_in_place;\n+\n@@ -180,1 +232,0 @@\n-  volatile size_t _used;\n@@ -182,1 +233,0 @@\n-  volatile size_t _bytes_allocated_since_gc_start;\n@@ -185,0 +235,2 @@\n+  void increase_used(const ShenandoahAllocRequest& req);\n+\n@@ -186,3 +238,4 @@\n-  void increase_used(size_t bytes);\n-  void decrease_used(size_t bytes);\n-  void set_used(size_t bytes);\n+  void increase_used(ShenandoahGeneration* generation, size_t bytes);\n+  void decrease_used(ShenandoahGeneration* generation, size_t bytes);\n+  void increase_humongous_waste(ShenandoahGeneration* generation, size_t bytes);\n+  void decrease_humongous_waste(ShenandoahGeneration* generation, size_t bytes);\n@@ -192,1 +245,0 @@\n-  void increase_allocated(size_t bytes);\n@@ -194,1 +246,0 @@\n-  size_t bytes_allocated_since_gc_start();\n@@ -230,0 +281,1 @@\n+  uint8_t* _affiliations;       \/\/ Holds array of enum ShenandoahAffiliation, including FREE status in non-generational mode\n@@ -239,1 +291,1 @@\n-  inline ShenandoahHeapRegion* const heap_region_containing(const void* addr) const;\n+  inline ShenandoahHeapRegion* heap_region_containing(const void* addr) const;\n@@ -242,1 +294,1 @@\n-  inline ShenandoahHeapRegion* const get_region(size_t region_idx) const;\n+  inline ShenandoahHeapRegion* get_region(size_t region_idx) const;\n@@ -247,0 +299,2 @@\n+  inline ShenandoahMmuTracker* mmu_tracker() { return &_mmu_tracker; };\n+\n@@ -262,0 +316,1 @@\n+    \/\/ For generational mode, it means either young or old marking, or both.\n@@ -272,0 +327,6 @@\n+\n+    \/\/ Young regions are under marking, need SATB barriers.\n+    YOUNG_MARKING_BITPOS = 5,\n+\n+    \/\/ Old regions are under marking, need SATB barriers.\n+    OLD_MARKING_BITPOS = 6\n@@ -281,0 +342,2 @@\n+    YOUNG_MARKING = 1 << YOUNG_MARKING_BITPOS,\n+    OLD_MARKING   = 1 << OLD_MARKING_BITPOS\n@@ -291,0 +354,31 @@\n+  \/\/ TODO: Revisit the following comment.  It may not accurately represent the true behavior when evacuations fail due to\n+  \/\/ difficulty finding memory to hold evacuated objects.\n+  \/\/\n+  \/\/ Note that the typical total expenditure on evacuation is less than the associated evacuation reserve because we generally\n+  \/\/ reserve ShenandoahEvacWaste (> 1.0) times the anticipated evacuation need.  In the case that there is an excessive amount\n+  \/\/ of waste, it may be that one thread fails to grab a new GCLAB, this does not necessarily doom the associated evacuation\n+  \/\/ effort.  If this happens, the requesting thread blocks until some other thread manages to evacuate the offending object.\n+  \/\/ Only after \"all\" threads fail to evacuate an object do we consider the evacuation effort to have failed.\n+\n+  size_t _promoted_reserve;            \/\/ Bytes reserved within old-gen to hold the results of promotion\n+  volatile size_t _promoted_expended;  \/\/ Bytes of old-gen memory expended on promotions\n+\n+  size_t _old_evac_reserve;            \/\/ Bytes reserved within old-gen to hold evacuated objects from old-gen collection set\n+  size_t _young_evac_reserve;          \/\/ Bytes reserved within young-gen to hold evacuated objects from young-gen collection set\n+\n+  ShenandoahAgeCensus* _age_census;    \/\/ Age census used for adapting tenuring threshold in generational mode\n+\n+  \/\/ At the end of final mark, but before we begin evacuating, heuristics calculate how much memory is required to\n+  \/\/ hold the results of evacuating to young-gen and to old-gen.  These quantitites, stored in _promoted_reserve,\n+  \/\/ _old_evac_reserve, and _young_evac_reserve, are consulted prior to rebuilding the free set (ShenandoahFreeSet)\n+  \/\/ in preparation for evacuation.  When the free set is rebuilt, we make sure to reserve sufficient memory in the\n+  \/\/ collector and old_collector sets to hold if _has_evacuation_reserve_quantities is true.  The other time we\n+  \/\/ rebuild the freeset is at the end of GC, as we prepare to idle GC until the next trigger.  In this case,\n+  \/\/ _has_evacuation_reserve_quantities is false because we don't yet know how much memory will need to be evacuated\n+  \/\/ in the next GC cycle.  When _has_evacuation_reserve_quantities is false, the free set rebuild operation reserves\n+  \/\/ for the collector and old_collector sets based on alternative mechanisms, such as ShenandoahEvacReserve,\n+  \/\/ ShenandoahOldEvacReserve, and ShenandoahOldCompactionReserve.  In a future planned enhancement, the reserve\n+  \/\/ for old_collector set when not _has_evacuation_reserve_quantities is based in part on anticipated promotion as\n+  \/\/ determined by analysis of live data found during the previous GC pass which is one less than the current tenure age.\n+  bool _has_evacuation_reserve_quantities;\n+\n@@ -297,1 +391,3 @@\n-  void set_concurrent_mark_in_progress(bool in_progress);\n+  void set_evacuation_reserve_quantities(bool is_valid);\n+  void set_concurrent_young_mark_in_progress(bool in_progress);\n+  void set_concurrent_old_mark_in_progress(bool in_progress);\n@@ -307,0 +403,2 @@\n+  void set_aging_cycle(bool cond);\n+\n@@ -309,0 +407,1 @@\n+  inline bool has_evacuation_reserve_quantities() const;\n@@ -310,0 +409,2 @@\n+  inline bool is_concurrent_young_mark_in_progress() const;\n+  inline bool is_concurrent_old_mark_in_progress() const;\n@@ -320,0 +421,37 @@\n+  bool is_prepare_for_old_mark_in_progress() const;\n+  inline bool is_aging_cycle() const;\n+\n+  inline void clear_promotion_potential() { _promotion_potential = 0; };\n+  inline void set_promotion_potential(size_t val) { _promotion_potential = val; };\n+  inline size_t get_promotion_potential() { return _promotion_potential; };\n+\n+  inline void set_pad_for_promote_in_place(size_t pad) { _pad_for_promote_in_place = pad; }\n+  inline size_t get_pad_for_promote_in_place() { return _pad_for_promote_in_place; }\n+\n+  inline void reserve_promotable_humongous_regions(size_t region_count) { _promotable_humongous_regions = region_count; }\n+  inline void reserve_promotable_regular_regions(size_t region_count) { _regular_regions_promoted_in_place = region_count; }\n+\n+  inline size_t get_promotable_humongous_regions() { return _promotable_humongous_regions; }\n+  inline size_t get_regular_regions_promoted_in_place() { return _regular_regions_promoted_in_place; }\n+\n+  \/\/ Returns previous value\n+  inline size_t set_promoted_reserve(size_t new_val);\n+  inline size_t get_promoted_reserve() const;\n+  inline void augment_promo_reserve(size_t increment);\n+\n+  inline void reset_promoted_expended();\n+  inline size_t expend_promoted(size_t increment);\n+  inline size_t unexpend_promoted(size_t decrement);\n+  inline size_t get_promoted_expended();\n+\n+  \/\/ Returns previous value\n+  inline size_t set_old_evac_reserve(size_t new_val);\n+  inline size_t get_old_evac_reserve() const;\n+  inline void augment_old_evac_reserve(size_t increment);\n+\n+  \/\/ Returns previous value\n+  inline size_t set_young_evac_reserve(size_t new_val);\n+  inline size_t get_young_evac_reserve() const;\n+\n+  \/\/ Return the age census object for young gen (in generational mode)\n+  inline ShenandoahAgeCensus* age_census() const;\n@@ -322,0 +460,2 @@\n+  void manage_satb_barrier(bool active);\n+\n@@ -332,0 +472,1 @@\n+  double _cancel_requested_time;\n@@ -333,0 +474,5 @@\n+\n+  \/\/ Returns true if cancel request was successfully communicated.\n+  \/\/ Returns false if some other thread already communicated cancel\n+  \/\/ request.  A true return value does not mean GC has been\n+  \/\/ cancelled, only that the process of cancelling GC has begun.\n@@ -336,0 +482,1 @@\n+<<<<<<< HEAD\n@@ -337,0 +484,2 @@\n+=======\n+>>>>>>> 666656136f7f15905ae770888951d832f46b0535\n@@ -340,1 +489,1 @@\n-  inline void clear_cancelled_gc();\n+  inline void clear_cancelled_gc(bool clear_oom_handler = true);\n@@ -342,0 +491,1 @@\n+  void cancel_concurrent_mark();\n@@ -351,3 +501,0 @@\n-  \/\/ Reset bitmap, prepare regions for new GC cycle\n-  void prepare_gc();\n-  void prepare_regions_and_collection_set(bool concurrent);\n@@ -366,1 +513,0 @@\n-  void rebuild_free_set(bool concurrent);\n@@ -371,0 +517,1 @@\n+  void rebuild_free_set(bool concurrent);\n@@ -377,0 +524,4 @@\n+  ShenandoahYoungGeneration* _young_generation;\n+  ShenandoahGeneration*      _global_generation;\n+  ShenandoahOldGeneration*   _old_generation;\n+\n@@ -378,0 +529,1 @@\n+  ShenandoahRegulatorThread* _regulator_thread;\n@@ -380,1 +532,0 @@\n-  ShenandoahHeuristics*      _heuristics;\n@@ -385,1 +536,4 @@\n-  ShenandoahPhaseTimings*    _phase_timings;\n+  ShenandoahPhaseTimings*       _phase_timings;\n+  ShenandoahEvacuationTracker*  _evac_tracker;\n+  ShenandoahMmuTracker          _mmu_tracker;\n+  ShenandoahGenerationSizer     _generation_sizer;\n@@ -387,1 +541,1 @@\n-  ShenandoahControlThread*   control_thread()          { return _control_thread;    }\n+  ShenandoahRegulatorThread* regulator_thread()        { return _regulator_thread;  }\n@@ -390,0 +544,10 @@\n+  ShenandoahControlThread*   control_thread()          { return _control_thread;    }\n+  ShenandoahYoungGeneration* young_generation()  const { return _young_generation;  }\n+  ShenandoahGeneration*      global_generation() const { return _global_generation; }\n+  ShenandoahOldGeneration*   old_generation()    const { return _old_generation;    }\n+  ShenandoahGeneration*      generation_for(ShenandoahAffiliation affiliation) const;\n+  const ShenandoahGenerationSizer* generation_sizer()  const { return &_generation_sizer;  }\n+\n+  size_t max_size_for(ShenandoahGeneration* generation) const;\n+  size_t min_size_for(ShenandoahGeneration* generation) const;\n+\n@@ -392,1 +556,0 @@\n-  ShenandoahHeuristics*      heuristics()        const { return _heuristics;        }\n@@ -396,1 +559,5 @@\n-  ShenandoahPhaseTimings*    phase_timings()     const { return _phase_timings;     }\n+  ShenandoahPhaseTimings*      phase_timings()   const { return _phase_timings;     }\n+  ShenandoahEvacuationTracker* evac_tracker()    const { return  _evac_tracker;     }\n+\n+  void on_cycle_start(GCCause::Cause cause, ShenandoahGeneration* generation);\n+  void on_cycle_end(ShenandoahGeneration* generation);\n@@ -405,0 +572,3 @@\n+  MemoryPool*                  _young_gen_memory_pool;\n+  MemoryPool*                  _old_gen_memory_pool;\n+\n@@ -413,1 +583,1 @@\n-  ShenandoahMonitoringSupport* monitoring_support()          { return _monitoring_support;    }\n+  ShenandoahMonitoringSupport* monitoring_support() const    { return _monitoring_support;    }\n@@ -424,8 +594,0 @@\n-\/\/ ---------- Reference processing\n-\/\/\n-private:\n-  ShenandoahReferenceProcessor* const _ref_processor;\n-\n-public:\n-  ShenandoahReferenceProcessor* ref_processor() { return _ref_processor; }\n-\n@@ -435,0 +597,1 @@\n+  ShenandoahSharedFlag  _is_aging_cycle;\n@@ -450,0 +613,3 @@\n+  inline void assert_lock_for_affiliation(ShenandoahAffiliation orig_affiliation,\n+                                          ShenandoahAffiliation new_affiliation);\n+\n@@ -463,1 +629,11 @@\n-  bool is_in(const void* p) const override;\n+  inline bool is_in(const void* p) const override;\n+\n+  inline bool is_in_active_generation(oop obj) const;\n+  inline bool is_in_young(const void* p) const;\n+  inline bool is_in_old(const void* p) const;\n+  inline bool is_old(oop pobj) const;\n+\n+  inline ShenandoahAffiliation region_affiliation(const ShenandoahHeapRegion* r);\n+  inline void set_affiliation(ShenandoahHeapRegion* r, ShenandoahAffiliation new_affiliation);\n+\n+  inline ShenandoahAffiliation region_affiliation(size_t index);\n@@ -517,1 +693,6 @@\n-  HeapWord* allocate_memory_under_lock(ShenandoahAllocRequest& request, bool& in_new_region);\n+  \/\/ How many bytes to transfer between old and young after we have finished recycling collection set regions?\n+  size_t _old_regions_surplus;\n+  size_t _old_regions_deficit;\n+\n+  HeapWord* allocate_memory_under_lock(ShenandoahAllocRequest& request, bool& in_new_region, bool is_promotion);\n+\n@@ -522,0 +703,4 @@\n+  inline HeapWord* allocate_from_plab(Thread* thread, size_t size, bool is_promotion);\n+  HeapWord* allocate_from_plab_slow(Thread* thread, size_t size, bool is_promotion);\n+  HeapWord* allocate_new_plab(size_t min_size, size_t word_size, size_t* actual_size);\n+\n@@ -523,1 +708,1 @@\n-  HeapWord* allocate_memory(ShenandoahAllocRequest& request);\n+  HeapWord* allocate_memory(ShenandoahAllocRequest& request, bool is_promotion);\n@@ -529,1 +714,1 @@\n-  void notify_mutator_alloc_words(size_t words, bool waste);\n+  void notify_mutator_alloc_words(size_t words, size_t waste);\n@@ -543,0 +728,6 @@\n+  inline void set_old_region_surplus(size_t surplus) { _old_regions_surplus = surplus; };\n+  inline void set_old_region_deficit(size_t deficit) { _old_regions_deficit = deficit; };\n+\n+  inline size_t get_old_region_surplus() { return _old_regions_surplus; };\n+  inline size_t get_old_region_deficit() { return _old_regions_deficit; };\n+\n@@ -567,2 +758,0 @@\n-  inline void mark_complete_marking_context();\n-  inline void mark_incomplete_marking_context();\n@@ -579,2 +768,0 @@\n-  void reset_mark_bitmap();\n-\n@@ -600,0 +787,5 @@\n+  ShenandoahSharedFlag _old_gen_oom_evac;\n+\n+  inline oop try_evacuate_object(oop src, Thread* thread, ShenandoahHeapRegion* from_region, ShenandoahAffiliation target_gen);\n+  void handle_old_evacuation(HeapWord* obj, size_t words, bool promotion);\n+  void handle_old_evacuation_failure();\n@@ -602,0 +794,2 @@\n+  void report_promotion_failure(Thread* thread, size_t size);\n+\n@@ -612,1 +806,1 @@\n-  \/\/ Evacuates object src. Returns the evacuated object, either evacuated\n+  \/\/ Evacuates or promotes object src. Returns the evacuated object, either evacuated\n@@ -620,0 +814,17 @@\n+  inline bool clear_old_evacuation_failure();\n+\n+\/\/ ---------- Generational support\n+\/\/\n+private:\n+  RememberedScanner* _card_scan;\n+\n+public:\n+  inline RememberedScanner* card_scan() { return _card_scan; }\n+  void clear_cards_for(ShenandoahHeapRegion* region);\n+  void mark_card_as_dirty(void* location);\n+  void retire_plab(PLAB* plab);\n+  void retire_plab(PLAB* plab, Thread* thread);\n+  void cancel_old_gc();\n+\n+  void adjust_generation_sizes_for_next_cycle(size_t old_xfer_limit, size_t young_cset_regions, size_t old_cset_regions);\n+\n@@ -641,1 +852,12 @@\n-  void trash_humongous_region_at(ShenandoahHeapRegion *r);\n+  size_t trash_humongous_region_at(ShenandoahHeapRegion *r);\n+\n+  static inline void increase_object_age(oop obj, uint additional_age);\n+\n+  \/\/ Return the object's age, or a sentinel value when the age can't\n+  \/\/ necessarily be determined because of concurrent locking by the\n+  \/\/ mutator\n+  static inline uint get_object_age(oop obj);\n+\n+  void transfer_old_pointers_from_satb();\n+\n+  void log_heap_status(const char *msg) const;\n","filename":"src\/hotspot\/share\/gc\/shenandoah\/shenandoahHeap.hpp","additions":267,"deletions":45,"binary":false,"changes":312,"status":"modified"},{"patch":"@@ -3,0 +3,1 @@\n+ * Copyright Amazon.com Inc. or its affiliates. All Rights Reserved.\n@@ -28,0 +29,1 @@\n+#include \"gc\/shared\/ageTable.hpp\"\n@@ -37,0 +39,4 @@\n+<<<<<<< HEAD\n+=======\n+\n+>>>>>>> 666656136f7f15905ae770888951d832f46b0535\n@@ -38,0 +44,1 @@\n+  ShenandoahGeneration* const _generation;\n@@ -39,0 +46,1 @@\n+  ShenandoahObjToScanQueueSet* const _old_gen_task_queues;\n@@ -41,1 +49,1 @@\n-  ShenandoahMark();\n+  ShenandoahMark(ShenandoahGeneration* generation);\n@@ -44,4 +52,2 @@\n-  template<class T>\n-  static inline void mark_through_ref(T* p, ShenandoahObjToScanQueue* q, ShenandoahMarkingContext* const mark_context, bool weak);\n-\n-  static void clear();\n+  template<class T, ShenandoahGenerationType GENERATION>\n+  static inline void mark_through_ref(T* p, ShenandoahObjToScanQueue* q, ShenandoahObjToScanQueue* old_q, ShenandoahMarkingContext* const mark_context, bool weak);\n@@ -55,0 +61,4 @@\n+  ShenandoahObjToScanQueueSet* old_task_queues() {\n+    return _old_gen_task_queues;\n+  }\n+\n@@ -56,0 +66,3 @@\n+  inline ShenandoahObjToScanQueue* get_old_queue(uint index) const;\n+\n+  inline ShenandoahGeneration* generation() { return _generation; };\n@@ -57,1 +70,0 @@\n-\/\/ ---------- Marking loop and tasks\n@@ -59,2 +71,4 @@\n-  template <class T, StringDedupMode STRING_DEDUP>\n-  inline void do_task(ShenandoahObjToScanQueue* q, T* cl, ShenandoahLiveData* live_data, StringDedup::Requests* const req, ShenandoahMarkTask* task);\n+\/\/ ---------- Marking loop and tasks\n+\n+  template <class T, ShenandoahGenerationType GENERATION, StringDedupMode STRING_DEDUP>\n+  inline void do_task(ShenandoahObjToScanQueue* q, T* cl, ShenandoahLiveData* live_data, StringDedup::Requests* const req, ShenandoahMarkTask* task, uint worker_id);\n@@ -68,1 +82,2 @@\n-  inline void count_liveness(ShenandoahLiveData* live_data, oop obj);\n+  template <ShenandoahGenerationType GENERATION>\n+  inline void count_liveness(ShenandoahLiveData* live_data, oop obj, uint worker_id);\n@@ -70,1 +85,1 @@\n-  template <class T, bool CANCELLABLE,StringDedupMode STRING_DEDUP>\n+  template <class T, ShenandoahGenerationType GENERATION, bool CANCELLABLE, StringDedupMode STRING_DEDUP>\n@@ -73,2 +88,9 @@\n-  template <bool CANCELLABLE, StringDedupMode STRING_DEDUP>\n-  void mark_loop_prework(uint worker_id, TaskTerminator *terminator, ShenandoahReferenceProcessor *rp, StringDedup::Requests* const req);\n+  template <ShenandoahGenerationType GENERATION, bool CANCELLABLE, StringDedupMode STRING_DEDUP>\n+  void mark_loop_prework(uint worker_id, TaskTerminator *terminator, ShenandoahReferenceProcessor *rp, StringDedup::Requests* const req, bool update_refs);\n+\n+  template <ShenandoahGenerationType GENERATION>\n+  static bool in_generation(ShenandoahHeap* const heap, oop obj);\n+\n+  static void mark_ref(ShenandoahObjToScanQueue* q,\n+                       ShenandoahMarkingContext* const mark_context,\n+                       bool weak, oop obj);\n@@ -79,1 +101,5 @@\n-  void mark_loop(uint worker_id, TaskTerminator* terminator, ShenandoahReferenceProcessor *rp,\n+  template<bool CANCELLABLE, StringDedupMode STRING_DEDUP>\n+  void mark_loop(ShenandoahGenerationType generation, uint worker_id, TaskTerminator* terminator, ShenandoahReferenceProcessor *rp,\n+                 StringDedup::Requests* const req);\n+\n+  void mark_loop(ShenandoahGenerationType generation, uint worker_id, TaskTerminator* terminator, ShenandoahReferenceProcessor *rp,\n@@ -84,1 +110,0 @@\n-\n","filename":"src\/hotspot\/share\/gc\/shenandoah\/shenandoahMark.hpp","additions":39,"deletions":14,"binary":false,"changes":53,"status":"modified"},{"patch":"@@ -2,1 +2,1 @@\n- * Copyright (c) 2020, Oracle and\/or its affiliates. All rights reserved.\n+ * Copyright (c) 2020, 2023, Oracle and\/or its affiliates. All rights reserved.\n@@ -4,0 +4,1 @@\n+ * Copyright Amazon.com Inc. or its affiliates. All Rights Reserved.\n@@ -37,0 +38,80 @@\n+  product(uintx, ShenandoahGenerationalHumongousReserve, 0, EXPERIMENTAL,   \\\n+          \"(Generational mode only) What percent of the heap should be \"    \\\n+          \"reserved for humongous objects if possible.  Old-generation \"    \\\n+          \"collections will endeavor to evacuate old-gen regions within \"   \\\n+          \"this reserved area even if these regions do not contain high \"   \\\n+          \"percentage of garbage.  Setting a larger value will cause \"      \\\n+          \"more frequent old-gen collections.  A smaller value will \"       \\\n+          \"increase the likelihood that humongous object allocations \"      \\\n+          \"fail, resulting in stop-the-world full GCs.\")                    \\\n+          range(0,100)                                                      \\\n+                                                                            \\\n+  product(double, ShenandoahMinOldGenGrowthPercent, 12.5, EXPERIMENTAL,     \\\n+          \"(Generational mode only) If the usage within old generation \"    \\\n+          \"has grown by at least this percent of its live memory size \"     \\\n+          \"at completion of the most recent old-generation marking \"        \\\n+          \"effort, heuristics may trigger the start of a new old-gen \"      \\\n+          \"collection.\")                                                    \\\n+          range(0.0,100.0)                                                  \\\n+                                                                            \\\n+  product(uintx, ShenandoahIgnoreOldGrowthBelowPercentage,10, EXPERIMENTAL, \\\n+          \"(Generational mode only) If the total usage of the old \"         \\\n+          \"generation is smaller than this percent, we do not trigger \"     \\\n+          \"old gen collections even if old has grown, except when \"         \\\n+          \"ShenandoahGenerationalDoNotIgnoreGrowthAfterYoungCycles \"        \\\n+          \"consecutive cycles have been completed following the \"           \\\n+          \"preceding old-gen collection.\")                                  \\\n+          range(0,100)                                                      \\\n+                                                                            \\\n+  product(uintx, ShenandoahDoNotIgnoreGrowthAfterYoungCycles,               \\\n+          50, EXPERIMENTAL,                                                 \\\n+          \"(Generational mode only) Even if the usage of old generation \"   \\\n+          \"is below ShenandoahIgnoreOldGrowthBelowPercentage, \"             \\\n+          \"trigger an old-generation mark if old has grown and this \"       \\\n+          \"many consecutive young-gen collections have been \"               \\\n+          \"completed following the preceding old-gen collection.\")          \\\n+                                                                            \\\n+  product(bool, ShenandoahGenerationalCensusAtEvac, false, EXPERIMENTAL,    \\\n+          \"(Generational mode only) Object age census at evacuation, \"      \\\n+          \"rather than during marking.\")                                    \\\n+                                                                            \\\n+  product(bool, ShenandoahGenerationalAdaptiveTenuring, true, EXPERIMENTAL, \\\n+          \"(Generational mode only) Dynamically adapt tenuring age.\")       \\\n+                                                                            \\\n+  product(bool, ShenandoahGenerationalCensusIgnoreOlderCohorts, true,       \\\n+                                                               EXPERIMENTAL,\\\n+          \"(Generational mode only) Ignore mortality rates older than the \" \\\n+          \"oldest cohort under the tenuring age for the last cycle.\" )      \\\n+                                                                            \\\n+  product(uintx, ShenandoahGenerationalMinTenuringAge, 1, EXPERIMENTAL,     \\\n+          \"(Generational mode only) Floor for adaptive tenuring age. \"      \\\n+          \"Setting floor and ceiling to the same value fixes the tenuring \" \\\n+          \"age; setting both to 1 simulates a poor approximation to \"       \\\n+          \"AlwaysTenure, and setting both to 16 simulates NeverTenure.\")    \\\n+          range(1,16)                                                       \\\n+                                                                            \\\n+  product(uintx, ShenandoahGenerationalMaxTenuringAge, 15, EXPERIMENTAL,    \\\n+          \"(Generational mode only) Ceiling for adaptive tenuring age. \"    \\\n+          \"Setting floor and ceiling to the same value fixes the tenuring \" \\\n+          \"age; setting both to 1 simulates a poor approximation to \"       \\\n+          \"AlwaysTenure, and setting both to 16 simulates NeverTenure.\")    \\\n+          range(1,16)                                                       \\\n+                                                                            \\\n+  product(double, ShenandoahGenerationalTenuringMortalityRateThreshold,     \\\n+                                                         0.1, EXPERIMENTAL, \\\n+          \"(Generational mode only) Cohort mortality rates below this \"     \\\n+          \"value will be treated as indicative of longevity, leading to \"   \\\n+          \"tenuring. A lower value delays tenuring, a higher value hastens \"\\\n+          \"it. Used only when ShenandoahGenerationalhenAdaptiveTenuring is \"\\\n+          \"enabled.\")                                                       \\\n+          range(0.001,0.999)                                                \\\n+                                                                            \\\n+  product(size_t, ShenandoahGenerationalTenuringCohortPopulationThreshold,  \\\n+                                                         4*K, EXPERIMENTAL, \\\n+          \"(Generational mode only) Cohorts whose population is lower than \"\\\n+          \"this value in the previous census are ignored wrt tenuring \"     \\\n+          \"decisions. Effectively this makes then tenurable as soon as all \"\\\n+          \"older cohorts are. Set this value to the largest cohort \"        \\\n+          \"population volume that you are comfortable ignoring when making \"\\\n+          \"tenuring decisions.\")                                            \\\n+                                                                            \\\n@@ -65,1 +146,2 @@\n-          \" passive - stop the world GC only (either degenerated or full)\") \\\n+          \" passive - stop the world GC only (either degenerated or full);\" \\\n+          \" generational - generational concurrent GC\")                     \\\n@@ -79,0 +161,18 @@\n+<<<<<<< HEAD\n+=======\n+  product(uintx, ShenandoahExpeditePromotionsThreshold, 5, EXPERIMENTAL,    \\\n+          \"When Shenandoah expects to promote at least this percentage \"    \\\n+          \"of the young generation, trigger a young collection to \"         \\\n+          \"expedite these promotions.\")                                     \\\n+          range(0,100)                                                      \\\n+                                                                            \\\n+  product(uintx, ShenandoahExpediteMixedThreshold, 10, EXPERIMENTAL,        \\\n+          \"When there are this many old regions waiting to be collected, \"  \\\n+          \"trigger a mixed collection immediately.\")                        \\\n+                                                                            \\\n+  product(uintx, ShenandoahUnloadClassesFrequency, 1, EXPERIMENTAL,         \\\n+          \"Unload the classes every Nth cycle. Normally affects concurrent \"\\\n+          \"GC cycles, as degenerated and full GCs would try to unload \"     \\\n+          \"classes regardless. Set to zero to disable class unloading.\")    \\\n+                                                                            \\\n+>>>>>>> 666656136f7f15905ae770888951d832f46b0535\n@@ -87,0 +187,14 @@\n+  product(uintx, ShenandoahOldGarbageThreshold, 15, EXPERIMENTAL,           \\\n+          \"How much garbage an old region has to contain before it would \"  \\\n+          \"be taken for collection.\")                                       \\\n+          range(0,100)                                                      \\\n+                                                                            \\\n+  product(uintx, ShenandoahIgnoreGarbageThreshold, 5, EXPERIMENTAL,         \\\n+          \"When less than this amount of garbage (as a percentage of \"      \\\n+          \"region size) exists within a region, the region will not be \"    \\\n+          \"added to the collection set, even when the heuristic has \"       \\\n+          \"chosen to aggressively add regions with less than \"              \\\n+          \"ShenandoahGarbageThreshold amount of garbage into the \"          \\\n+          \"collection set.\")                                                \\\n+          range(0,100)                                                      \\\n+                                                                            \\\n@@ -88,4 +202,6 @@\n-          \"How much heap should be free before some heuristics trigger the \"\\\n-          \"initial (learning) cycles. Affects cycle frequency on startup \"  \\\n-          \"and after drastic state changes, e.g. after degenerated\/full \"   \\\n-          \"GC cycles. In percents of (soft) max heap size.\")                \\\n+          \"When less than this amount of memory is free within the\"         \\\n+          \"heap or generation, trigger a learning cycle if we are \"         \\\n+          \"in learning mode.  Learning mode happens during initialization \" \\\n+          \"and following a drastic state change, such as following a \"      \\\n+          \"degenerated or Full GC cycle.  In percents of soft max \"         \\\n+          \"heap size.\")                                                     \\\n@@ -95,3 +211,5 @@\n-          \"How much heap should be free before most heuristics trigger the \"\\\n-          \"collection, even without other triggers. Provides the safety \"   \\\n-          \"margin for many heuristics. In percents of (soft) max heap size.\")\\\n+          \"Percentage of free heap memory (or young generation, in \"        \\\n+          \"generational mode) below which most heuristics trigger \"         \\\n+          \"collection independent of other triggers. Provides a safety \"    \\\n+          \"margin for many heuristics. In percents of (soft) max heap \"     \\\n+          \"size.\")                                                          \\\n@@ -113,1 +231,1 @@\n-  product(uintx, ShenandoahLearningSteps, 5, EXPERIMENTAL,                  \\\n+  product(uintx, ShenandoahLearningSteps, 10, EXPERIMENTAL,                 \\\n@@ -118,1 +236,1 @@\n-  product(uintx, ShenandoahImmediateThreshold, 90, EXPERIMENTAL,            \\\n+  product(uintx, ShenandoahImmediateThreshold, 70, EXPERIMENTAL,            \\\n@@ -147,1 +265,1 @@\n-  product(double, ShenandoahAdaptiveDecayFactor, 0.5, EXPERIMENTAL,         \\\n+  product(double, ShenandoahAdaptiveDecayFactor, 0.1, EXPERIMENTAL,         \\\n@@ -153,0 +271,10 @@\n+  product(bool, ShenandoahAdaptiveIgnoreShortCycles, true, EXPERIMENTAL,    \\\n+          \"The adaptive heuristic tracks a moving average of cycle \"        \\\n+          \"times in order to start a gc before memory is exhausted. \"       \\\n+          \"In some cases, Shenandoah may skip the evacuation and update \"   \\\n+          \"reference phases, resulting in a shorter cycle. These may skew \" \\\n+          \"the average cycle time downward and may cause the heuristic \"    \\\n+          \"to wait too long to start a cycle. Disabling this will have \"    \\\n+          \"the gc run less often, which will reduce CPU utilization, but\"   \\\n+          \"increase the risk of degenerated cycles.\")                       \\\n+                                                                            \\\n@@ -160,0 +288,10 @@\n+  product(uintx, ShenandoahGuaranteedOldGCInterval, 10*60*1000, EXPERIMENTAL, \\\n+          \"Run a collection of the old generation at least this often. \"    \\\n+          \"Heuristics may trigger collections more frequently. Time is in \" \\\n+          \"milliseconds. Setting this to 0 disables the feature.\")          \\\n+                                                                            \\\n+  product(uintx, ShenandoahGuaranteedYoungGCInterval, 5*60*1000,  EXPERIMENTAL,  \\\n+          \"Run a collection of the young generation at least this often. \"  \\\n+          \"Heuristics may trigger collections more frequently. Time is in \" \\\n+          \"milliseconds. Setting this to 0 disables the feature.\")          \\\n+                                                                            \\\n@@ -218,0 +356,7 @@\n+<<<<<<< HEAD\n+=======\n+  product(bool, ShenandoahElasticTLAB, true, DIAGNOSTIC,                    \\\n+          \"Use Elastic TLABs with Shenandoah. This allows Shenandoah to \"   \\\n+          \"decrease the size of a TLAB to fit in a region's remaining space\") \\\n+                                                                            \\\n+>>>>>>> 666656136f7f15905ae770888951d832f46b0535\n@@ -219,4 +364,12 @@\n-          \"How much of heap to reserve for evacuations. Larger values make \"\\\n-          \"GC evacuate more live objects on every cycle, while leaving \"    \\\n-          \"less headroom for application to allocate in. In percents of \"   \\\n-          \"total heap size.\")                                               \\\n+          \"How much of (young-generation) heap to reserve for \"             \\\n+          \"(young-generation) evacuations.  Larger values allow GC to \"     \\\n+          \"evacuate more live objects on every cycle, while leaving \"       \\\n+          \"less headroom for application to allocate while GC is \"          \\\n+          \"evacuating and updating references. This parameter is \"          \\\n+          \"consulted at the end of marking, before selecting the \"          \\\n+          \"collection set.  If available memory at this time is smaller \"   \\\n+          \"than the indicated reserve, the bound on collection set size is \"\\\n+          \"adjusted downward.  The size of a generational mixed \"           \\\n+          \"evacuation collection set (comprised of both young and old \"     \\\n+          \"regions) is also bounded by this parameter.  In percents of \"    \\\n+          \"total (young-generation) heap size.\")                            \\\n@@ -229,1 +382,10 @@\n-          \"GC cycle.\")                                                      \\\n+          \"GC cycle.  Smaller values increase the risk of evacuation \"      \\\n+          \"failures, which will trigger stop-the-world Full GC passes.\")    \\\n+          range(1.0,100.0)                                                  \\\n+                                                                            \\\n+  product(double, ShenandoahOldEvacWaste, 1.4, EXPERIMENTAL,                \\\n+          \"How much waste evacuations produce within the reserved space. \"  \\\n+          \"Larger values make evacuations more resilient against \"          \\\n+          \"evacuation conflicts, at expense of evacuating less on each \"    \\\n+          \"GC cycle.  Smaller values increase the risk of evacuation \"      \\\n+          \"failures, which will trigger stop-the-world Full GC passes.\")    \\\n@@ -232,0 +394,28 @@\n+  product(double, ShenandoahPromoEvacWaste, 1.2, EXPERIMENTAL,              \\\n+          \"How much waste promotions produce within the reserved space. \"   \\\n+          \"Larger values make evacuations more resilient against \"          \\\n+          \"evacuation conflicts, at expense of promoting less on each \"     \\\n+          \"GC cycle.  Smaller values increase the risk of evacuation \"      \\\n+          \"failures, which will trigger stop-the-world Full GC passes.\")    \\\n+          range(1.0,100.0)                                                  \\\n+                                                                            \\\n+  product(uintx, ShenandoahMaxEvacLABRatio, 0, EXPERIMENTAL,                \\\n+          \"Potentially, each running thread maintains a PLAB for \"          \\\n+          \"evacuating objects into old-gen memory and a GCLAB for \"         \\\n+          \"evacuating objects into young-gen memory.  Each time a thread \"  \\\n+          \"exhausts its PLAB or GCLAB, a new local buffer is allocated. \"   \\\n+          \"By default, the new buffer is twice the size of the previous \"   \\\n+          \"buffer.  The sizes are reset to the minimum at the start of \"    \\\n+          \"each GC pass.  This parameter limits the growth of evacuation \"  \\\n+          \"buffer sizes to its value multiplied by the minimum buffer \"     \\\n+          \"size.  A higher value allows evacuation allocations to be more \" \\\n+          \"efficient because less synchronization is required by \"          \\\n+          \"individual threads.  However, a larger value increases the \"     \\\n+          \"likelihood of evacuation failures, leading to long \"             \\\n+          \"stop-the-world pauses.  This is because a large value \"          \\\n+          \"allows individual threads to consume large percentages of \"      \\\n+          \"the total evacuation budget without necessarily effectively \"    \\\n+          \"filling their local evacuation buffers with evacuated \"          \\\n+          \"objects.  A value of zero means no maximum size is enforced.\")   \\\n+          range(0, 1024)                                                    \\\n+                                                                            \\\n@@ -238,0 +428,35 @@\n+  product(uintx, ShenandoahOldEvacRatioPercent, 75, EXPERIMENTAL,           \\\n+          \"The maximum proportion of evacuation from old-gen memory, \"      \\\n+          \"expressed as a percentage. The default value 75 denotes that no\" \\\n+          \"more than 75% of the collection set evacuation workload may be \" \\\n+          \"towards evacuation of old-gen heap regions. This limits both the\"\\\n+          \"promotion of aged regions and the compaction of existing old \"   \\\n+          \"regions.  A value of 75 denotes that the total evacuation work\"  \\\n+          \"may increase to up to four times the young gen evacuation work.\" \\\n+          \"A larger value allows quicker promotion and allows\"              \\\n+          \"a smaller number of mixed evacuations to process \"               \\\n+          \"the entire list of old-gen collection candidates at the cost \"   \\\n+          \"of an increased disruption of the normal cadence of young-gen \"  \\\n+          \"collections.  A value of 100 allows a mixed evacuation to \"      \\\n+          \"focus entirely on old-gen memory, allowing no young-gen \"        \\\n+          \"regions to be collected, likely resulting in subsequent \"        \\\n+          \"allocation failures because the allocation pool is not \"         \\\n+          \"replenished.  A value of 0 allows a mixed evacuation to\"         \\\n+          \"focus entirely on young-gen memory, allowing no old-gen \"        \\\n+          \"regions to be collected, likely resulting in subsequent \"        \\\n+          \"promotion failures and triggering of stop-the-world full GC \"    \\\n+          \"events.\")                                                        \\\n+          range(0,100)                                                      \\\n+                                                                            \\\n+  product(uintx, ShenandoahMinYoungPercentage, 20, EXPERIMENTAL,            \\\n+          \"The minimum percentage of the heap to use for the young \"        \\\n+          \"generation. Heuristics will not adjust the young generation \"    \\\n+          \"to be less than this.\")                                          \\\n+          range(0, 100)                                                     \\\n+                                                                            \\\n+  product(uintx, ShenandoahMaxYoungPercentage, 100, EXPERIMENTAL,           \\\n+          \"The maximum percentage of the heap to use for the young \"        \\\n+          \"generation. Heuristics will not adjust the young generation \"    \\\n+          \"to be more than this.\")                                          \\\n+          range(0, 100)                                                     \\\n+                                                                            \\\n@@ -289,0 +514,4 @@\n+  product(uintx, ShenandoahOOMGCRetries, 3, EXPERIMENTAL,                   \\\n+          \"How many GCs should happen before we throw OutOfMemoryException \"\\\n+          \"for allocation request, including at least one Full GC.\")        \\\n+                                                                            \\\n@@ -307,0 +536,9 @@\n+  product(uintx, ShenandoahCoalesceChance, 0, DIAGNOSTIC,                   \\\n+          \"Testing: Abandon remaining mixed collections with this \"         \\\n+          \"likelihood. Following each mixed collection, abandon all \"       \\\n+          \"remaining mixed collection candidate regions with likelihood \"   \\\n+          \"ShenandoahCoalesceChance. Abandoning a mixed collection will \"   \\\n+          \"cause the old regions to be made parsable, rather than being \"   \\\n+          \"evacuated.\")                                                     \\\n+          range(0, 100)                                                     \\\n+                                                                            \\\n@@ -338,0 +576,4 @@\n+  product(bool, ShenandoahCardBarrier, false, DIAGNOSTIC,                   \\\n+          \"Turn on\/off card-marking post-write barrier in Shenandoah: \"     \\\n+          \" true when ShenandoahGCMode is generational, false otherwise\")   \\\n+                                                                            \\\n@@ -353,0 +595,1 @@\n+<<<<<<< HEAD\n@@ -355,0 +598,38 @@\n+=======\n+  product(bool, ShenandoahLoopOptsAfterExpansion, true, DIAGNOSTIC,         \\\n+          \"Attempt more loop opts after barrier expansion.\")                \\\n+                                                                            \\\n+  product(bool, ShenandoahSelfFixing, true, DIAGNOSTIC,                     \\\n+          \"Fix references with load reference barrier. Disabling this \"     \\\n+          \"might degrade performance.\")                                     \\\n+                                                                            \\\n+  product(uintx, ShenandoahOldCompactionReserve, 8, EXPERIMENTAL,           \\\n+          \"During generational GC, prevent promotions from filling \"        \\\n+          \"this number of heap regions.  These regions are reserved \"       \\\n+          \"for the purpose of supporting compaction of old-gen \"            \\\n+          \"memory.  Otherwise, old-gen memory cannot be compacted.\")        \\\n+          range(0, 128)                                                     \\\n+                                                                            \\\n+  product(bool, ShenandoahAllowOldMarkingPreemption, true, DIAGNOSTIC,      \\\n+          \"Allow young generation collections to suspend concurrent\"        \\\n+          \" marking in the old generation.\")                                \\\n+                                                                            \\\n+  product(uintx, ShenandoahAgingCyclePeriod, 1, EXPERIMENTAL,               \\\n+          \"With generational mode, increment the age of objects and\"        \\\n+          \"regions each time this many young-gen GC cycles are completed.\") \\\n+                                                                            \\\n+  notproduct(bool, ShenandoahEnableCardStats, false,                        \\\n+          \"Enable statistics collection related to clean & dirty cards\")    \\\n+                                                                            \\\n+  notproduct(int, ShenandoahCardStatsLogInterval, 50,                       \\\n+          \"Log cumulative card stats every so many remembered set or \"      \\\n+          \"update refs scans\")                                              \\\n+                                                                            \\\n+  product(uintx, ShenandoahMinimumOldMarkTimeMs, 100, EXPERIMENTAL,         \\\n+         \"Minimum amount of time in milliseconds to run old marking \"       \\\n+         \"before a young collection is allowed to run. This is intended \"   \\\n+         \"to prevent starvation of the old collector. Setting this to \"     \\\n+         \"0 will allow back to back young collections to run during old \"   \\\n+         \"marking.\")                                                        \\\n+  \/\/ end of GC_SHENANDOAH_FLAGS\n+>>>>>>> 666656136f7f15905ae770888951d832f46b0535\n","filename":"src\/hotspot\/share\/gc\/shenandoah\/shenandoah_globals.hpp","additions":298,"deletions":17,"binary":false,"changes":315,"status":"modified"},{"patch":"@@ -0,0 +1,80 @@\n+\/*\n+ * Copyright (c) 2018, Red Hat, Inc. All rights reserved.\n+ * Copyright Amazon.com Inc. or its affiliates. All Rights Reserved.\n+ * DO NOT ALTER OR REMOVE COPYRIGHT NOTICES OR THIS FILE HEADER.\n+ *\n+ * This code is free software; you can redistribute it and\/or modify it\n+ * under the terms of the GNU General Public License version 2 only, as\n+ * published by the Free Software Foundation.\n+ *\n+ * This code is distributed in the hope that it will be useful, but WITHOUT\n+ * ANY WARRANTY; without even the implied warranty of MERCHANTABILITY or\n+ * FITNESS FOR A PARTICULAR PURPOSE.  See the GNU General Public License\n+ * version 2 for more details (a copy is included in the LICENSE file that\n+ * accompanied this code).\n+ *\n+ * You should have received a copy of the GNU General Public License version\n+ * 2 along with this work; if not, write to the Free Software Foundation,\n+ * Inc., 51 Franklin St, Fifth Floor, Boston, MA 02110-1301 USA.\n+ *\n+ * Please contact Oracle, 500 Oracle Parkway, Redwood Shores, CA 94065 USA\n+ * or visit www.oracle.com if you need additional information or have any\n+ * questions.\n+ *\n+ *\/\n+\n+\/*\n+ * @test id=default\n+ * @key randomness\n+ * @summary Test that Shenandoah is able to work with elastic TLABs\n+ * @requires vm.gc.Shenandoah\n+ * @library \/test\/lib\n+ *\n+ * @run main\/othervm -XX:+UnlockDiagnosticVMOptions -XX:+UnlockExperimentalVMOptions -XX:+UseShenandoahGC -Xmx1g -XX:-UseTLAB -XX:-ShenandoahElasticTLAB -XX:+ShenandoahVerify TestElasticTLAB\n+ * @run main\/othervm -XX:+UnlockDiagnosticVMOptions -XX:+UnlockExperimentalVMOptions -XX:+UseShenandoahGC -Xmx1g -XX:-UseTLAB -XX:-ShenandoahElasticTLAB                       TestElasticTLAB\n+ * @run main\/othervm -XX:+UnlockDiagnosticVMOptions -XX:+UnlockExperimentalVMOptions -XX:+UseShenandoahGC -Xmx1g -XX:-UseTLAB -XX:+ShenandoahElasticTLAB -XX:+ShenandoahVerify TestElasticTLAB\n+ * @run main\/othervm -XX:+UnlockDiagnosticVMOptions -XX:+UnlockExperimentalVMOptions -XX:+UseShenandoahGC -Xmx1g -XX:-UseTLAB -XX:+ShenandoahElasticTLAB                       TestElasticTLAB\n+ * @run main\/othervm -XX:+UnlockDiagnosticVMOptions -XX:+UnlockExperimentalVMOptions -XX:+UseShenandoahGC -Xmx1g -XX:+UseTLAB -XX:-ShenandoahElasticTLAB -XX:+ShenandoahVerify TestElasticTLAB\n+ * @run main\/othervm -XX:+UnlockDiagnosticVMOptions -XX:+UnlockExperimentalVMOptions -XX:+UseShenandoahGC -Xmx1g -XX:+UseTLAB -XX:-ShenandoahElasticTLAB                       TestElasticTLAB\n+ * @run main\/othervm -XX:+UnlockDiagnosticVMOptions -XX:+UnlockExperimentalVMOptions -XX:+UseShenandoahGC -Xmx1g -XX:+UseTLAB -XX:+ShenandoahElasticTLAB -XX:+ShenandoahVerify TestElasticTLAB\n+ * @run main\/othervm -XX:+UnlockDiagnosticVMOptions -XX:+UnlockExperimentalVMOptions -XX:+UseShenandoahGC -Xmx1g -XX:+UseTLAB -XX:+ShenandoahElasticTLAB                       TestElasticTLAB\n+ *\/\n+\n+\/*\n+ * @test id=generational\n+ * @key randomness\n+ * @summary Test that Shenandoah is able to work with elastic TLABs\n+ * @requires vm.gc.Shenandoah\n+ * @library \/test\/lib\n+ *\n+ * @run main\/othervm -XX:+UnlockDiagnosticVMOptions -XX:+UnlockExperimentalVMOptions -XX:+UseShenandoahGC -XX:ShenandoahGCMode=generational -Xmx1g -XX:-UseTLAB -XX:-ShenandoahElasticTLAB -XX:+ShenandoahVerify TestElasticTLAB\n+ * @run main\/othervm -XX:+UnlockDiagnosticVMOptions -XX:+UnlockExperimentalVMOptions -XX:+UseShenandoahGC -XX:ShenandoahGCMode=generational -Xmx1g -XX:-UseTLAB -XX:-ShenandoahElasticTLAB                       TestElasticTLAB\n+ * @run main\/othervm -XX:+UnlockDiagnosticVMOptions -XX:+UnlockExperimentalVMOptions -XX:+UseShenandoahGC -XX:ShenandoahGCMode=generational -Xmx1g -XX:-UseTLAB -XX:+ShenandoahElasticTLAB -XX:+ShenandoahVerify TestElasticTLAB\n+ * @run main\/othervm -XX:+UnlockDiagnosticVMOptions -XX:+UnlockExperimentalVMOptions -XX:+UseShenandoahGC -XX:ShenandoahGCMode=generational -Xmx1g -XX:-UseTLAB -XX:+ShenandoahElasticTLAB                       TestElasticTLAB\n+ * @run main\/othervm -XX:+UnlockDiagnosticVMOptions -XX:+UnlockExperimentalVMOptions -XX:+UseShenandoahGC -XX:ShenandoahGCMode=generational -Xmx1g -XX:+UseTLAB -XX:-ShenandoahElasticTLAB -XX:+ShenandoahVerify TestElasticTLAB\n+ * @run main\/othervm -XX:+UnlockDiagnosticVMOptions -XX:+UnlockExperimentalVMOptions -XX:+UseShenandoahGC -XX:ShenandoahGCMode=generational -Xmx1g -XX:+UseTLAB -XX:-ShenandoahElasticTLAB                       TestElasticTLAB\n+ * @run main\/othervm -XX:+UnlockDiagnosticVMOptions -XX:+UnlockExperimentalVMOptions -XX:+UseShenandoahGC -XX:ShenandoahGCMode=generational -Xmx1g -XX:+UseTLAB -XX:+ShenandoahElasticTLAB -XX:+ShenandoahVerify TestElasticTLAB\n+ * @run main\/othervm -XX:+UnlockDiagnosticVMOptions -XX:+UnlockExperimentalVMOptions -XX:+UseShenandoahGC -XX:ShenandoahGCMode=generational -Xmx1g -XX:+UseTLAB -XX:+ShenandoahElasticTLAB                       TestElasticTLAB\n+ *\/\n+\n+import java.util.Random;\n+import jdk.test.lib.Utils;\n+\n+public class TestElasticTLAB {\n+\n+    static final long TARGET_MB = Long.getLong(\"target\", 10_000); \/\/ 10 Gb allocation\n+\n+    static volatile Object sink;\n+\n+    public static void main(String[] args) throws Exception {\n+        final int min = 0;\n+        final int max = 384 * 1024;\n+        long count = TARGET_MB * 1024 * 1024 \/ (16 + 4 * (min + (max - min) \/ 2));\n+\n+        Random r = Utils.getRandomInstance();\n+        for (long c = 0; c < count; c++) {\n+            sink = new int[min + r.nextInt(max - min)];\n+        }\n+    }\n+\n+}\n","filename":"test\/hotspot\/jtreg\/gc\/shenandoah\/TestElasticTLAB.java","additions":80,"deletions":0,"binary":false,"changes":80,"status":"added"},{"patch":"@@ -3,0 +3,1 @@\n+ * Copyright Amazon.com Inc. or its affiliates. All Rights Reserved.\n@@ -44,0 +45,27 @@\n+<<<<<<< HEAD\n+=======\n+ * @test id=default-debug\n+ * @summary Tests that we pass at least one jcstress-like test with all verification turned on\n+ * @requires vm.gc.Shenandoah\n+ * @requires vm.debug\n+ * @modules java.base\/jdk.internal.misc\n+ *          java.management\n+ *\n+ * @run main\/othervm -Xmx1g -Xms1g -XX:+UnlockExperimentalVMOptions -XX:+UnlockDiagnosticVMOptions\n+ *      -XX:+UseShenandoahGC -XX:ShenandoahGCHeuristics=adaptive\n+ *      -XX:+ShenandoahVerify -XX:+ShenandoahVerifyOptoBarriers\n+ *      TestVerifyJCStress\n+ *\n+ * @run main\/othervm -Xmx1g -Xms1g -XX:+UnlockExperimentalVMOptions -XX:+UnlockDiagnosticVMOptions\n+ *      -XX:+UseShenandoahGC -XX:ShenandoahGCHeuristics=compact\n+ *      -XX:+ShenandoahVerify -XX:+ShenandoahVerifyOptoBarriers\n+ *      TestVerifyJCStress\n+ *\n+ * @run main\/othervm -Xmx1g -Xms1g -XX:+UnlockExperimentalVMOptions -XX:+UnlockDiagnosticVMOptions\n+ *      -XX:+UseShenandoahGC -XX:ShenandoahGCHeuristics=adaptive -XX:ShenandoahGCMode=generational\n+ *      -XX:+ShenandoahVerify -XX:+ShenandoahVerifyOptoBarriers\n+ *      TestVerifyJCStress\n+ *\/\n+\n+\/*\n+>>>>>>> 666656136f7f15905ae770888951d832f46b0535\n@@ -59,0 +87,5 @@\n+ *\n+ * @run main\/othervm -Xmx1g -Xms1g -XX:+UnlockExperimentalVMOptions -XX:+UnlockDiagnosticVMOptions\n+ *      -XX:+UseShenandoahGC -XX:ShenandoahGCHeuristics=adaptive -XX:ShenandoahGCMode=generational\n+ *      -XX:+ShenandoahVerify\n+ *      TestVerifyJCStress\n","filename":"test\/hotspot\/jtreg\/gc\/shenandoah\/TestVerifyJCStress.java","additions":33,"deletions":0,"binary":false,"changes":33,"status":"modified"}]}