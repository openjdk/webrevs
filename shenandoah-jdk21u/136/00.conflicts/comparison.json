{"files":[{"patch":"@@ -3,0 +3,1 @@\n+ * Copyright Amazon.com Inc. or its affiliates. All Rights Reserved.\n@@ -27,0 +28,2 @@\n+#include \"gc\/shenandoah\/shenandoahAffiliation.hpp\"\n+#include \"gc\/shenandoah\/shenandoahBarrierSet.hpp\"\n@@ -31,0 +34,5 @@\n+#include \"gc\/shenandoah\/shenandoahOldGeneration.hpp\"\n+#include \"gc\/shenandoah\/shenandoahScanRemembered.inline.hpp\"\n+#include \"gc\/shenandoah\/shenandoahYoungGeneration.hpp\"\n+#include \"gc\/shenandoah\/shenandoahSimpleBitMap.hpp\"\n+#include \"gc\/shenandoah\/shenandoahSimpleBitMap.inline.hpp\"\n@@ -35,0 +43,626 @@\n+static const char* partition_name(ShenandoahFreeSetPartitionId t) {\n+  switch (t) {\n+    case ShenandoahFreeSetPartitionId::NotFree: return \"NotFree\";\n+    case ShenandoahFreeSetPartitionId::Mutator: return \"Mutator\";\n+    case ShenandoahFreeSetPartitionId::Collector: return \"Collector\";\n+    case ShenandoahFreeSetPartitionId::OldCollector: return \"OldCollector\";\n+    default:\n+      ShouldNotReachHere();\n+      return \"Unrecognized\";\n+  }\n+}\n+\n+#ifndef PRODUCT\n+void ShenandoahRegionPartitions::dump_bitmap() const {\n+  log_debug(gc)(\"Mutator range [\" SSIZE_FORMAT \", \" SSIZE_FORMAT \"], Collector range [\" SSIZE_FORMAT \", \" SSIZE_FORMAT\n+                \"], Old Collector range [\" SSIZE_FORMAT \", \" SSIZE_FORMAT \"]\",\n+                _leftmosts[int(ShenandoahFreeSetPartitionId::Mutator)],\n+                _rightmosts[int(ShenandoahFreeSetPartitionId::Mutator)],\n+                _leftmosts[int(ShenandoahFreeSetPartitionId::Collector)],\n+                _rightmosts[int(ShenandoahFreeSetPartitionId::Collector)],\n+                _leftmosts[int(ShenandoahFreeSetPartitionId::OldCollector)],\n+                _rightmosts[int(ShenandoahFreeSetPartitionId::OldCollector)]);\n+  log_debug(gc)(\"Empty Mutator range [\" SSIZE_FORMAT \", \" SSIZE_FORMAT\n+                \"], Empty Collector range [\" SSIZE_FORMAT \", \" SSIZE_FORMAT\n+                \"], Empty Old Collecto range [\" SSIZE_FORMAT \", \" SSIZE_FORMAT \"]\",\n+                _leftmosts_empty[int(ShenandoahFreeSetPartitionId::Mutator)],\n+                _rightmosts_empty[int(ShenandoahFreeSetPartitionId::Mutator)],\n+                _leftmosts_empty[int(ShenandoahFreeSetPartitionId::Collector)],\n+                _rightmosts_empty[int(ShenandoahFreeSetPartitionId::Collector)],\n+                _leftmosts_empty[int(ShenandoahFreeSetPartitionId::OldCollector)],\n+                _rightmosts_empty[int(ShenandoahFreeSetPartitionId::OldCollector)]);\n+\n+  log_debug(gc)(\"%6s: %18s %18s %18s %18s\", \"index\", \"Mutator Bits\", \"Collector Bits\", \"Old Collector Bits\", \"NotFree Bits\");\n+  dump_bitmap_range(0, _max-1);\n+}\n+\n+void ShenandoahRegionPartitions::dump_bitmap_range(idx_t start_region_idx, idx_t end_region_idx) const {\n+  assert((start_region_idx >= 0) && (start_region_idx < (idx_t) _max), \"precondition\");\n+  assert((end_region_idx >= 0) && (end_region_idx < (idx_t) _max), \"precondition\");\n+  idx_t aligned_start = _membership[int(ShenandoahFreeSetPartitionId::Mutator)].aligned_index(start_region_idx);\n+  idx_t aligned_end = _membership[int(ShenandoahFreeSetPartitionId::Mutator)].aligned_index(end_region_idx);\n+  idx_t alignment = _membership[int(ShenandoahFreeSetPartitionId::Mutator)].alignment();\n+  while (aligned_start <= aligned_end) {\n+    dump_bitmap_row(aligned_start);\n+    aligned_start += alignment;\n+  }\n+}\n+\n+void ShenandoahRegionPartitions::dump_bitmap_row(idx_t region_idx) const {\n+  assert((region_idx >= 0) && (region_idx < (idx_t) _max), \"precondition\");\n+  idx_t aligned_idx = _membership[int(ShenandoahFreeSetPartitionId::Mutator)].aligned_index(region_idx);\n+  uintx mutator_bits = _membership[int(ShenandoahFreeSetPartitionId::Mutator)].bits_at(aligned_idx);\n+  uintx collector_bits = _membership[int(ShenandoahFreeSetPartitionId::Collector)].bits_at(aligned_idx);\n+  uintx old_collector_bits = _membership[int(ShenandoahFreeSetPartitionId::OldCollector)].bits_at(aligned_idx);\n+  uintx free_bits = mutator_bits | collector_bits | old_collector_bits;\n+  uintx notfree_bits =  ~free_bits;\n+  log_debug(gc)(SSIZE_FORMAT_W(6) \": \" SIZE_FORMAT_X_0 \" 0x\" SIZE_FORMAT_X_0 \" 0x\" SIZE_FORMAT_X_0 \" 0x\" SIZE_FORMAT_X_0,\n+                aligned_idx, mutator_bits, collector_bits, old_collector_bits, notfree_bits);\n+}\n+#endif\n+\n+ShenandoahRegionPartitions::ShenandoahRegionPartitions(size_t max_regions, ShenandoahFreeSet* free_set) :\n+    _max(max_regions),\n+    _region_size_bytes(ShenandoahHeapRegion::region_size_bytes()),\n+    _free_set(free_set),\n+    _membership{ ShenandoahSimpleBitMap(max_regions), ShenandoahSimpleBitMap(max_regions) , ShenandoahSimpleBitMap(max_regions) }\n+{\n+  make_all_regions_unavailable();\n+}\n+\n+inline bool ShenandoahFreeSet::can_allocate_from(ShenandoahHeapRegion *r) const {\n+  return r->is_empty() || (r->is_trash() && !_heap->is_concurrent_weak_root_in_progress());\n+}\n+\n+inline bool ShenandoahFreeSet::can_allocate_from(size_t idx) const {\n+  ShenandoahHeapRegion* r = _heap->get_region(idx);\n+  return can_allocate_from(r);\n+}\n+\n+inline size_t ShenandoahFreeSet::alloc_capacity(ShenandoahHeapRegion *r) const {\n+  if (r->is_trash()) {\n+    \/\/ This would be recycled on allocation path\n+    return ShenandoahHeapRegion::region_size_bytes();\n+  } else {\n+    return r->free();\n+  }\n+}\n+\n+inline size_t ShenandoahFreeSet::alloc_capacity(size_t idx) const {\n+  ShenandoahHeapRegion* r = _heap->get_region(idx);\n+  return alloc_capacity(r);\n+}\n+\n+inline bool ShenandoahFreeSet::has_alloc_capacity(ShenandoahHeapRegion *r) const {\n+  return alloc_capacity(r) > 0;\n+}\n+\n+inline idx_t ShenandoahRegionPartitions::leftmost(ShenandoahFreeSetPartitionId which_partition) const {\n+  assert (which_partition < NumPartitions, \"selected free partition must be valid\");\n+  idx_t idx = _leftmosts[int(which_partition)];\n+  if (idx >= _max) {\n+    return _max;\n+  } else {\n+    \/\/ Cannot assert that membership[which_partition.is_set(idx) because this helper method may be used\n+    \/\/ to query the original value of leftmost when leftmost must be adjusted because the interval representing\n+    \/\/ which_partition is shrinking after the region that used to be leftmost is retired.\n+    return idx;\n+  }\n+}\n+\n+inline idx_t ShenandoahRegionPartitions::rightmost(ShenandoahFreeSetPartitionId which_partition) const {\n+  assert (which_partition < NumPartitions, \"selected free partition must be valid\");\n+  idx_t idx = _rightmosts[int(which_partition)];\n+  \/\/ Cannot assert that membership[which_partition.is_set(idx) because this helper method may be used\n+  \/\/ to query the original value of leftmost when leftmost must be adjusted because the interval representing\n+  \/\/ which_partition is shrinking after the region that used to be leftmost is retired.\n+  return idx;\n+}\n+\n+void ShenandoahRegionPartitions::make_all_regions_unavailable() {\n+  for (size_t partition_id = 0; partition_id < IntNumPartitions; partition_id++) {\n+    _membership[partition_id].clear_all();\n+    _leftmosts[partition_id] = _max;\n+    _rightmosts[partition_id] = -1;\n+    _leftmosts_empty[partition_id] = _max;\n+    _rightmosts_empty[partition_id] = -1;;\n+    _capacity[partition_id] = 0;\n+    _used[partition_id] = 0;\n+  }\n+  _region_counts[int(ShenandoahFreeSetPartitionId::Mutator)] = _region_counts[int(ShenandoahFreeSetPartitionId::Collector)] = 0;\n+}\n+\n+void ShenandoahRegionPartitions::establish_mutator_intervals(idx_t mutator_leftmost, idx_t mutator_rightmost,\n+                                                             idx_t mutator_leftmost_empty, idx_t mutator_rightmost_empty,\n+                                                             size_t mutator_region_count, size_t mutator_used) {\n+  _leftmosts[int(ShenandoahFreeSetPartitionId::Mutator)] = mutator_leftmost;\n+  _rightmosts[int(ShenandoahFreeSetPartitionId::Mutator)] = mutator_rightmost;\n+  _leftmosts_empty[int(ShenandoahFreeSetPartitionId::Mutator)] = mutator_leftmost_empty;\n+  _rightmosts_empty[int(ShenandoahFreeSetPartitionId::Mutator)] = mutator_rightmost_empty;\n+\n+  _region_counts[int(ShenandoahFreeSetPartitionId::Mutator)] = mutator_region_count;\n+  _used[int(ShenandoahFreeSetPartitionId::Mutator)] = mutator_used;\n+  _capacity[int(ShenandoahFreeSetPartitionId::Mutator)] = mutator_region_count * _region_size_bytes;\n+\n+  _leftmosts[int(ShenandoahFreeSetPartitionId::Collector)] = _max;\n+  _rightmosts[int(ShenandoahFreeSetPartitionId::Collector)] = -1;\n+  _leftmosts_empty[int(ShenandoahFreeSetPartitionId::Collector)] = _max;\n+  _rightmosts_empty[int(ShenandoahFreeSetPartitionId::Collector)] = -1;\n+\n+  _region_counts[int(ShenandoahFreeSetPartitionId::Collector)] = 0;\n+  _used[int(ShenandoahFreeSetPartitionId::Collector)] = 0;\n+  _capacity[int(ShenandoahFreeSetPartitionId::Collector)] = 0;\n+}\n+\n+void ShenandoahRegionPartitions::establish_old_collector_intervals(idx_t old_collector_leftmost, idx_t old_collector_rightmost,\n+                                                                   idx_t old_collector_leftmost_empty,\n+                                                                   idx_t old_collector_rightmost_empty,\n+                                                                   size_t old_collector_region_count, size_t old_collector_used) {\n+  _leftmosts[int(ShenandoahFreeSetPartitionId::OldCollector)] = old_collector_leftmost;\n+  _rightmosts[int(ShenandoahFreeSetPartitionId::OldCollector)] = old_collector_rightmost;\n+  _leftmosts_empty[int(ShenandoahFreeSetPartitionId::OldCollector)] = old_collector_leftmost_empty;\n+  _rightmosts_empty[int(ShenandoahFreeSetPartitionId::OldCollector)] = old_collector_rightmost_empty;\n+\n+  _region_counts[int(ShenandoahFreeSetPartitionId::OldCollector)] = old_collector_region_count;\n+  _used[int(ShenandoahFreeSetPartitionId::OldCollector)] = old_collector_used;\n+  _capacity[int(ShenandoahFreeSetPartitionId::OldCollector)] = old_collector_region_count * _region_size_bytes;\n+}\n+\n+void ShenandoahRegionPartitions::increase_used(ShenandoahFreeSetPartitionId which_partition, size_t bytes) {\n+  assert (which_partition < NumPartitions, \"Partition must be valid\");\n+  _used[int(which_partition)] += bytes;\n+  assert (_used[int(which_partition)] <= _capacity[int(which_partition)],\n+          \"Must not use (\" SIZE_FORMAT \") more than capacity (\" SIZE_FORMAT \") after increase by \" SIZE_FORMAT,\n+          _used[int(which_partition)], _capacity[int(which_partition)], bytes);\n+}\n+\n+inline void ShenandoahRegionPartitions::shrink_interval_if_range_modifies_either_boundary(\n+  ShenandoahFreeSetPartitionId partition, idx_t low_idx, idx_t high_idx) {\n+  assert((low_idx <= high_idx) && (low_idx >= 0) && (high_idx < _max), \"Range must span legal index values\");\n+  if (low_idx == leftmost(partition)) {\n+    assert (!_membership[int(partition)].is_set(low_idx), \"Do not shrink interval if region not removed\");\n+    if (high_idx + 1 == _max) {\n+      _leftmosts[int(partition)] = _max;\n+    } else {\n+      _leftmosts[int(partition)] = find_index_of_next_available_region(partition, high_idx + 1);\n+    }\n+    if (_leftmosts_empty[int(partition)] < _leftmosts[int(partition)]) {\n+      \/\/ This gets us closer to where we need to be; we'll scan further when leftmosts_empty is requested.\n+      _leftmosts_empty[int(partition)] = _leftmosts[int(partition)];\n+    }\n+  }\n+  if (high_idx == _rightmosts[int(partition)]) {\n+    assert (!_membership[int(partition)].is_set(high_idx), \"Do not shrink interval if region not removed\");\n+    if (low_idx == 0) {\n+      _rightmosts[int(partition)] = -1;\n+    } else {\n+      _rightmosts[int(partition)] = find_index_of_previous_available_region(partition, low_idx - 1);\n+    }\n+    if (_rightmosts_empty[int(partition)] > _rightmosts[int(partition)]) {\n+      \/\/ This gets us closer to where we need to be; we'll scan further when rightmosts_empty is requested.\n+      _rightmosts_empty[int(partition)] = _rightmosts[int(partition)];\n+    }\n+  }\n+  if (_leftmosts[int(partition)] > _rightmosts[int(partition)]) {\n+    _leftmosts[int(partition)] = _max;\n+    _rightmosts[int(partition)] = -1;\n+    _leftmosts_empty[int(partition)] = _max;\n+    _rightmosts_empty[int(partition)] = -1;\n+  }\n+}\n+\n+inline void ShenandoahRegionPartitions::shrink_interval_if_boundary_modified(ShenandoahFreeSetPartitionId partition, idx_t idx) {\n+  shrink_interval_if_range_modifies_either_boundary(partition, idx, idx);\n+}\n+\n+inline void ShenandoahRegionPartitions::expand_interval_if_boundary_modified(ShenandoahFreeSetPartitionId partition,\n+                                                                             idx_t idx, size_t region_available) {\n+  if (_leftmosts[int(partition)] > idx) {\n+    _leftmosts[int(partition)] = idx;\n+  }\n+  if (_rightmosts[int(partition)] < idx) {\n+    _rightmosts[int(partition)] = idx;\n+  }\n+  if (region_available == _region_size_bytes) {\n+    if (_leftmosts_empty[int(partition)] > idx) {\n+      _leftmosts_empty[int(partition)] = idx;\n+    }\n+    if (_rightmosts_empty[int(partition)] < idx) {\n+      _rightmosts_empty[int(partition)] = idx;\n+    }\n+  }\n+}\n+\n+void ShenandoahRegionPartitions::retire_range_from_partition(\n+  ShenandoahFreeSetPartitionId partition, idx_t low_idx, idx_t high_idx) {\n+\n+  \/\/ Note: we may remove from free partition even if region is not entirely full, such as when available < PLAB::min_size()\n+  assert ((low_idx < _max) && (high_idx < _max), \"Both indices are sane: \" SIZE_FORMAT \" and \" SIZE_FORMAT \" < \" SIZE_FORMAT,\n+          low_idx, high_idx, _max);\n+  assert (partition < NumPartitions, \"Cannot remove from free partitions if not already free\");\n+\n+  for (idx_t idx = low_idx; idx <= high_idx; idx++) {\n+    assert (in_free_set(partition, idx), \"Must be in partition to remove from partition\");\n+    _membership[int(partition)].clear_bit(idx);\n+  }\n+  _region_counts[int(partition)] -= high_idx + 1 - low_idx;\n+  shrink_interval_if_range_modifies_either_boundary(partition, low_idx, high_idx);\n+}\n+\n+void ShenandoahRegionPartitions::retire_from_partition(ShenandoahFreeSetPartitionId partition, idx_t idx, size_t used_bytes) {\n+\n+  \/\/ Note: we may remove from free partition even if region is not entirely full, such as when available < PLAB::min_size()\n+  assert (idx < _max, \"index is sane: \" SIZE_FORMAT \" < \" SIZE_FORMAT, idx, _max);\n+  assert (partition < NumPartitions, \"Cannot remove from free partitions if not already free\");\n+  assert (in_free_set(partition, idx), \"Must be in partition to remove from partition\");\n+\n+  if (used_bytes < _region_size_bytes) {\n+    \/\/ Count the alignment pad remnant of memory as used when we retire this region\n+    increase_used(partition, _region_size_bytes - used_bytes);\n+  }\n+  _membership[int(partition)].clear_bit(idx);\n+  shrink_interval_if_boundary_modified(partition, idx);\n+  _region_counts[int(partition)]--;\n+}\n+\n+void ShenandoahRegionPartitions::make_free(idx_t idx, ShenandoahFreeSetPartitionId which_partition, size_t available) {\n+  assert (idx < _max, \"index is sane: \" SIZE_FORMAT \" < \" SIZE_FORMAT, idx, _max);\n+  assert (membership(idx) == ShenandoahFreeSetPartitionId::NotFree, \"Cannot make free if already free\");\n+  assert (which_partition < NumPartitions, \"selected free partition must be valid\");\n+  assert (available <= _region_size_bytes, \"Available cannot exceed region size\");\n+\n+  _membership[int(which_partition)].set_bit(idx);\n+  _capacity[int(which_partition)] += _region_size_bytes;\n+  _used[int(which_partition)] += _region_size_bytes - available;\n+  expand_interval_if_boundary_modified(which_partition, idx, available);\n+  _region_counts[int(which_partition)]++;\n+}\n+\n+bool ShenandoahRegionPartitions::is_mutator_partition(ShenandoahFreeSetPartitionId p) {\n+  return (p == ShenandoahFreeSetPartitionId::Mutator);\n+}\n+\n+bool ShenandoahRegionPartitions::is_young_collector_partition(ShenandoahFreeSetPartitionId p) {\n+  return (p == ShenandoahFreeSetPartitionId::Collector);\n+}\n+\n+bool ShenandoahRegionPartitions::is_old_collector_partition(ShenandoahFreeSetPartitionId p) {\n+  return (p == ShenandoahFreeSetPartitionId::OldCollector);\n+}\n+\n+bool ShenandoahRegionPartitions::available_implies_empty(size_t available_in_region) {\n+  return (available_in_region == _region_size_bytes);\n+}\n+\n+\n+void ShenandoahRegionPartitions::move_from_partition_to_partition(idx_t idx, ShenandoahFreeSetPartitionId orig_partition,\n+                                                                  ShenandoahFreeSetPartitionId new_partition, size_t available) {\n+  ShenandoahHeapRegion* r = ShenandoahHeap::heap()->get_region(idx);\n+  assert (idx < _max, \"index is sane: \" SIZE_FORMAT \" < \" SIZE_FORMAT, idx, _max);\n+  assert (orig_partition < NumPartitions, \"Original partition must be valid\");\n+  assert (new_partition < NumPartitions, \"New partition must be valid\");\n+  assert (available <= _region_size_bytes, \"Available cannot exceed region size\");\n+  assert (_membership[int(orig_partition)].is_set(idx), \"Cannot move from partition unless in partition\");\n+  assert ((r != nullptr) && ((r->is_trash() && (available == _region_size_bytes)) ||\n+                             (r->used() + available == _region_size_bytes)),\n+          \"Used: \" SIZE_FORMAT \" + available: \" SIZE_FORMAT \" should equal region size: \" SIZE_FORMAT,\n+          ShenandoahHeap::heap()->get_region(idx)->used(), available, _region_size_bytes);\n+\n+  \/\/ Expected transitions:\n+  \/\/  During rebuild:         Mutator => Collector\n+  \/\/                          Mutator empty => Collector\n+  \/\/                          Mutator empty => OldCollector\n+  \/\/  During flip_to_gc:      Mutator empty => Collector\n+  \/\/                          Mutator empty => OldCollector\n+  \/\/ At start of update refs: Collector => Mutator\n+  \/\/                          OldCollector Empty => Mutator\n+  assert ((is_mutator_partition(orig_partition) && is_young_collector_partition(new_partition)) ||\n+          (is_mutator_partition(orig_partition) &&\n+           available_implies_empty(available) && is_old_collector_partition(new_partition)) ||\n+          (is_young_collector_partition(orig_partition) && is_mutator_partition(new_partition)) ||\n+          (is_old_collector_partition(orig_partition)\n+           && available_implies_empty(available) && is_mutator_partition(new_partition)),\n+          \"Unexpected movement between partitions, available: \" SIZE_FORMAT \", _region_size_bytes: \" SIZE_FORMAT\n+          \", orig_partition: %s, new_partition: %s\",\n+          available, _region_size_bytes, partition_name(orig_partition), partition_name(new_partition));\n+\n+  size_t used = _region_size_bytes - available;\n+  assert (_used[int(orig_partition)] >= used,\n+          \"Orig partition used: \" SIZE_FORMAT \" must exceed moved used: \" SIZE_FORMAT \" within region \" SSIZE_FORMAT,\n+          _used[int(orig_partition)], used, idx);\n+\n+  _membership[int(orig_partition)].clear_bit(idx);\n+  _membership[int(new_partition)].set_bit(idx);\n+\n+  _capacity[int(orig_partition)] -= _region_size_bytes;\n+  _used[int(orig_partition)] -= used;\n+  shrink_interval_if_boundary_modified(orig_partition, idx);\n+\n+  _capacity[int(new_partition)] += _region_size_bytes;;\n+  _used[int(new_partition)] += used;\n+  expand_interval_if_boundary_modified(new_partition, idx, available);\n+\n+  _region_counts[int(orig_partition)]--;\n+  _region_counts[int(new_partition)]++;\n+}\n+\n+const char* ShenandoahRegionPartitions::partition_membership_name(idx_t idx) const {\n+  return partition_name(membership(idx));\n+}\n+\n+inline ShenandoahFreeSetPartitionId ShenandoahRegionPartitions::membership(idx_t idx) const {\n+  assert (idx < _max, \"index is sane: \" SIZE_FORMAT \" < \" SIZE_FORMAT, idx, _max);\n+  ShenandoahFreeSetPartitionId result = ShenandoahFreeSetPartitionId::NotFree;\n+  for (uint partition_id = 0; partition_id < UIntNumPartitions; partition_id++) {\n+    if (_membership[partition_id].is_set(idx)) {\n+      assert(result == ShenandoahFreeSetPartitionId::NotFree, \"Region should reside in only one partition\");\n+      result = (ShenandoahFreeSetPartitionId) partition_id;\n+    }\n+  }\n+  return result;\n+}\n+\n+#ifdef ASSERT\n+inline bool ShenandoahRegionPartitions::partition_id_matches(idx_t idx, ShenandoahFreeSetPartitionId test_partition) const {\n+  assert (idx < _max, \"index is sane: \" SIZE_FORMAT \" < \" SIZE_FORMAT, idx, _max);\n+  assert (test_partition < ShenandoahFreeSetPartitionId::NotFree, \"must be a valid partition\");\n+\n+  return membership(idx) == test_partition;\n+}\n+#endif\n+\n+inline bool ShenandoahRegionPartitions::is_empty(ShenandoahFreeSetPartitionId which_partition) const {\n+  assert (which_partition < NumPartitions, \"selected free partition must be valid\");\n+  return (leftmost(which_partition) > rightmost(which_partition));\n+}\n+\n+inline idx_t ShenandoahRegionPartitions::find_index_of_next_available_region(\n+  ShenandoahFreeSetPartitionId which_partition, idx_t start_index) const {\n+  idx_t rightmost_idx = rightmost(which_partition);\n+  idx_t leftmost_idx = leftmost(which_partition);\n+  if ((rightmost_idx < leftmost_idx) || (start_index > rightmost_idx)) return _max;\n+  if (start_index < leftmost_idx) {\n+    start_index = leftmost_idx;\n+  }\n+  idx_t result = _membership[int(which_partition)].find_first_set_bit(start_index, rightmost_idx + 1);\n+  if (result > rightmost_idx) {\n+    result = _max;\n+  }\n+  assert (result >= start_index, \"Requires progress\");\n+  return result;\n+}\n+\n+inline idx_t ShenandoahRegionPartitions::find_index_of_previous_available_region(\n+  ShenandoahFreeSetPartitionId which_partition, idx_t last_index) const {\n+  idx_t rightmost_idx = rightmost(which_partition);\n+  idx_t leftmost_idx = leftmost(which_partition);\n+  \/\/ if (leftmost_idx == max) then (last_index < leftmost_idx)\n+  if (last_index < leftmost_idx) return -1;\n+  if (last_index > rightmost_idx) {\n+    last_index = rightmost_idx;\n+  }\n+  idx_t result = _membership[int(which_partition)].find_last_set_bit(-1, last_index);\n+  if (result < leftmost_idx) {\n+    result = -1;\n+  }\n+  assert (result <= last_index, \"Requires progress\");\n+  return result;\n+}\n+\n+inline idx_t ShenandoahRegionPartitions::find_index_of_next_available_cluster_of_regions(\n+  ShenandoahFreeSetPartitionId which_partition, idx_t start_index, size_t cluster_size) const {\n+  idx_t rightmost_idx = rightmost(which_partition);\n+  idx_t leftmost_idx = leftmost(which_partition);\n+  if ((rightmost_idx < leftmost_idx) || (start_index > rightmost_idx)) return _max;\n+  idx_t result = _membership[int(which_partition)].find_first_consecutive_set_bits(start_index, rightmost_idx + 1, cluster_size);\n+  if (result > rightmost_idx) {\n+    result = _max;\n+  }\n+  assert (result >= start_index, \"Requires progress\");\n+  return result;\n+}\n+\n+inline idx_t ShenandoahRegionPartitions::find_index_of_previous_available_cluster_of_regions(\n+  ShenandoahFreeSetPartitionId which_partition, idx_t last_index, size_t cluster_size) const {\n+  idx_t leftmost_idx = leftmost(which_partition);\n+  \/\/ if (leftmost_idx == max) then (last_index < leftmost_idx)\n+  if (last_index < leftmost_idx) return -1;\n+  idx_t result = _membership[int(which_partition)].find_last_consecutive_set_bits(leftmost_idx - 1, last_index, cluster_size);\n+  if (result <= leftmost_idx) {\n+    result = -1;\n+  }\n+  assert (result <= last_index, \"Requires progress\");\n+  return result;\n+}\n+\n+idx_t ShenandoahRegionPartitions::leftmost_empty(ShenandoahFreeSetPartitionId which_partition) {\n+  assert (which_partition < NumPartitions, \"selected free partition must be valid\");\n+  idx_t max_regions = _max;\n+  if (_leftmosts_empty[int(which_partition)] == _max) {\n+    return _max;\n+  }\n+  for (idx_t idx = find_index_of_next_available_region(which_partition, _leftmosts_empty[int(which_partition)]);\n+       idx < max_regions; ) {\n+    assert(in_free_set(which_partition, idx), \"Boundaries or find_last_set_bit failed: \" SSIZE_FORMAT, idx);\n+    if (_free_set->alloc_capacity(idx) == _region_size_bytes) {\n+      _leftmosts_empty[int(which_partition)] = idx;\n+      return idx;\n+    }\n+    idx = find_index_of_next_available_region(which_partition, idx + 1);\n+  }\n+  _leftmosts_empty[int(which_partition)] = _max;\n+  _rightmosts_empty[int(which_partition)] = -1;\n+  return _max;\n+}\n+\n+idx_t ShenandoahRegionPartitions::rightmost_empty(ShenandoahFreeSetPartitionId which_partition) {\n+  assert (which_partition < NumPartitions, \"selected free partition must be valid\");\n+  if (_rightmosts_empty[int(which_partition)] < 0) {\n+    return -1;\n+  }\n+  for (idx_t idx = find_index_of_previous_available_region(which_partition, _rightmosts_empty[int(which_partition)]);\n+       idx >= 0; ) {\n+    assert(in_free_set(which_partition, idx), \"Boundaries or find_last_set_bit failed: \" SSIZE_FORMAT, idx);\n+    if (_free_set->alloc_capacity(idx) == _region_size_bytes) {\n+      _rightmosts_empty[int(which_partition)] = idx;\n+      return idx;\n+    }\n+    idx = find_index_of_previous_available_region(which_partition, idx - 1);\n+  }\n+  _leftmosts_empty[int(which_partition)] = _max;\n+  _rightmosts_empty[int(which_partition)] = -1;\n+  return -1;\n+}\n+\n+\n+#ifdef ASSERT\n+void ShenandoahRegionPartitions::assert_bounds() {\n+\n+  idx_t leftmosts[UIntNumPartitions];\n+  idx_t rightmosts[UIntNumPartitions];\n+  idx_t empty_leftmosts[UIntNumPartitions];\n+  idx_t empty_rightmosts[UIntNumPartitions];\n+\n+  for (uint i = 0; i < UIntNumPartitions; i++) {\n+    leftmosts[i] = _max;\n+    empty_leftmosts[i] = _max;\n+    rightmosts[i] = -1;\n+    empty_rightmosts[i] = -1;\n+  }\n+\n+  for (idx_t i = 0; i < _max; i++) {\n+    ShenandoahFreeSetPartitionId partition = membership(i);\n+    switch (partition) {\n+      case ShenandoahFreeSetPartitionId::NotFree:\n+        break;\n+\n+      case ShenandoahFreeSetPartitionId::Mutator:\n+      case ShenandoahFreeSetPartitionId::Collector:\n+      case ShenandoahFreeSetPartitionId::OldCollector:\n+      {\n+        size_t capacity = _free_set->alloc_capacity(i);\n+        bool is_empty = (capacity == _region_size_bytes);\n+        assert(capacity > 0, \"free regions must have allocation capacity\");\n+        if (i < leftmosts[int(partition)]) {\n+          leftmosts[int(partition)] = i;\n+        }\n+        if (is_empty && (i < empty_leftmosts[int(partition)])) {\n+          empty_leftmosts[int(partition)] = i;\n+        }\n+        if (i > rightmosts[int(partition)]) {\n+          rightmosts[int(partition)] = i;\n+        }\n+        if (is_empty && (i > empty_rightmosts[int(partition)])) {\n+          empty_rightmosts[int(partition)] = i;\n+        }\n+        break;\n+      }\n+\n+      default:\n+        ShouldNotReachHere();\n+    }\n+  }\n+\n+  \/\/ Performance invariants. Failing these would not break the free partition, but performance would suffer.\n+  assert (leftmost(ShenandoahFreeSetPartitionId::Mutator) <= _max,\n+          \"leftmost in bounds: \"  SSIZE_FORMAT \" < \" SSIZE_FORMAT, leftmost(ShenandoahFreeSetPartitionId::Mutator),  _max);\n+  assert (rightmost(ShenandoahFreeSetPartitionId::Mutator) < _max,\n+          \"rightmost in bounds: \"  SSIZE_FORMAT \" < \" SSIZE_FORMAT, rightmost(ShenandoahFreeSetPartitionId::Mutator),  _max);\n+\n+  assert (leftmost(ShenandoahFreeSetPartitionId::Mutator) == _max\n+          || partition_id_matches(leftmost(ShenandoahFreeSetPartitionId::Mutator), ShenandoahFreeSetPartitionId::Mutator),\n+          \"leftmost region should be free: \" SSIZE_FORMAT,  leftmost(ShenandoahFreeSetPartitionId::Mutator));\n+  assert (leftmost(ShenandoahFreeSetPartitionId::Mutator) == _max\n+          || partition_id_matches(rightmost(ShenandoahFreeSetPartitionId::Mutator), ShenandoahFreeSetPartitionId::Mutator),\n+          \"rightmost region should be free: \" SSIZE_FORMAT, rightmost(ShenandoahFreeSetPartitionId::Mutator));\n+\n+  \/\/ If Mutator partition is empty, leftmosts will both equal max, rightmosts will both equal zero.\n+  \/\/ Likewise for empty region partitions.\n+  idx_t beg_off = leftmosts[int(ShenandoahFreeSetPartitionId::Mutator)];\n+  idx_t end_off = rightmosts[int(ShenandoahFreeSetPartitionId::Mutator)];\n+  assert (beg_off >= leftmost(ShenandoahFreeSetPartitionId::Mutator),\n+          \"free regions before the leftmost: \" SSIZE_FORMAT \", bound \" SSIZE_FORMAT,\n+          beg_off, leftmost(ShenandoahFreeSetPartitionId::Mutator));\n+  assert (end_off <= rightmost(ShenandoahFreeSetPartitionId::Mutator),\n+          \"free regions past the rightmost: \" SSIZE_FORMAT \", bound \" SSIZE_FORMAT,\n+          end_off, rightmost(ShenandoahFreeSetPartitionId::Mutator));\n+\n+  beg_off = empty_leftmosts[int(ShenandoahFreeSetPartitionId::Mutator)];\n+  end_off = empty_rightmosts[int(ShenandoahFreeSetPartitionId::Mutator)];\n+  assert (beg_off >= leftmost_empty(ShenandoahFreeSetPartitionId::Mutator),\n+          \"free empty regions before the leftmost: \" SSIZE_FORMAT \", bound \" SSIZE_FORMAT,\n+          beg_off, leftmost_empty(ShenandoahFreeSetPartitionId::Mutator));\n+  assert (end_off <= rightmost_empty(ShenandoahFreeSetPartitionId::Mutator),\n+          \"free empty regions past the rightmost: \" SSIZE_FORMAT \", bound \" SSIZE_FORMAT,\n+          end_off, rightmost_empty(ShenandoahFreeSetPartitionId::Mutator));\n+\n+  \/\/ Performance invariants. Failing these would not break the free partition, but performance would suffer.\n+  assert (leftmost(ShenandoahFreeSetPartitionId::Collector) <= _max, \"leftmost in bounds: \"  SSIZE_FORMAT \" < \" SSIZE_FORMAT,\n+          leftmost(ShenandoahFreeSetPartitionId::Collector),  _max);\n+  assert (rightmost(ShenandoahFreeSetPartitionId::Collector) < _max, \"rightmost in bounds: \"  SSIZE_FORMAT \" < \" SSIZE_FORMAT,\n+          rightmost(ShenandoahFreeSetPartitionId::Collector),  _max);\n+\n+  assert (leftmost(ShenandoahFreeSetPartitionId::Collector) == _max\n+          || partition_id_matches(leftmost(ShenandoahFreeSetPartitionId::Collector), ShenandoahFreeSetPartitionId::Collector),\n+          \"leftmost region should be free: \" SSIZE_FORMAT,  leftmost(ShenandoahFreeSetPartitionId::Collector));\n+  assert (leftmost(ShenandoahFreeSetPartitionId::Collector) == _max\n+          || partition_id_matches(rightmost(ShenandoahFreeSetPartitionId::Collector), ShenandoahFreeSetPartitionId::Collector),\n+          \"rightmost region should be free: \" SSIZE_FORMAT, rightmost(ShenandoahFreeSetPartitionId::Collector));\n+\n+  \/\/ If Collector partition is empty, leftmosts will both equal max, rightmosts will both equal zero.\n+  \/\/ Likewise for empty region partitions.\n+  beg_off = leftmosts[int(ShenandoahFreeSetPartitionId::Collector)];\n+  end_off = rightmosts[int(ShenandoahFreeSetPartitionId::Collector)];\n+  assert (beg_off >= leftmost(ShenandoahFreeSetPartitionId::Collector),\n+          \"free regions before the leftmost: \" SSIZE_FORMAT \", bound \" SSIZE_FORMAT,\n+          beg_off, leftmost(ShenandoahFreeSetPartitionId::Collector));\n+  assert (end_off <= rightmost(ShenandoahFreeSetPartitionId::Collector),\n+          \"free regions past the rightmost: \" SSIZE_FORMAT \", bound \" SSIZE_FORMAT,\n+          end_off, rightmost(ShenandoahFreeSetPartitionId::Collector));\n+\n+  beg_off = empty_leftmosts[int(ShenandoahFreeSetPartitionId::Collector)];\n+  end_off = empty_rightmosts[int(ShenandoahFreeSetPartitionId::Collector)];\n+  assert (beg_off >= _leftmosts_empty[int(ShenandoahFreeSetPartitionId::Collector)],\n+          \"free empty regions before the leftmost: \" SSIZE_FORMAT \", bound \" SSIZE_FORMAT,\n+          beg_off, leftmost_empty(ShenandoahFreeSetPartitionId::Collector));\n+  assert (end_off <= _rightmosts_empty[int(ShenandoahFreeSetPartitionId::Collector)],\n+          \"free empty regions past the rightmost: \" SSIZE_FORMAT \", bound \" SSIZE_FORMAT,\n+          end_off, rightmost_empty(ShenandoahFreeSetPartitionId::Collector));\n+\n+  \/\/ Performance invariants. Failing these would not break the free partition, but performance would suffer.\n+  assert (leftmost(ShenandoahFreeSetPartitionId::OldCollector) <= _max, \"leftmost in bounds: \"  SSIZE_FORMAT \" < \" SSIZE_FORMAT,\n+          leftmost(ShenandoahFreeSetPartitionId::OldCollector),  _max);\n+  assert (rightmost(ShenandoahFreeSetPartitionId::OldCollector) < _max, \"rightmost in bounds: \"  SSIZE_FORMAT \" < \" SSIZE_FORMAT,\n+          rightmost(ShenandoahFreeSetPartitionId::OldCollector),  _max);\n+\n+  assert (leftmost(ShenandoahFreeSetPartitionId::OldCollector) == _max\n+          || partition_id_matches(leftmost(ShenandoahFreeSetPartitionId::OldCollector),\n+                                  ShenandoahFreeSetPartitionId::OldCollector),\n+          \"leftmost region should be free: \" SSIZE_FORMAT,  leftmost(ShenandoahFreeSetPartitionId::OldCollector));\n+  assert (leftmost(ShenandoahFreeSetPartitionId::OldCollector) == _max\n+          || partition_id_matches(rightmost(ShenandoahFreeSetPartitionId::OldCollector),\n+                                  ShenandoahFreeSetPartitionId::OldCollector),\n+          \"rightmost region should be free: \" SSIZE_FORMAT, rightmost(ShenandoahFreeSetPartitionId::OldCollector));\n+\n+  \/\/ If OldCollector partition is empty, leftmosts will both equal max, rightmosts will both equal zero.\n+  \/\/ Likewise for empty region partitions.\n+  beg_off = leftmosts[int(ShenandoahFreeSetPartitionId::OldCollector)];\n+  end_off = rightmosts[int(ShenandoahFreeSetPartitionId::OldCollector)];\n+  assert (beg_off >= leftmost(ShenandoahFreeSetPartitionId::OldCollector),\n+          \"free regions before the leftmost: \" SSIZE_FORMAT \", bound \" SSIZE_FORMAT,\n+          beg_off, leftmost(ShenandoahFreeSetPartitionId::OldCollector));\n+  assert (end_off <= rightmost(ShenandoahFreeSetPartitionId::OldCollector),\n+          \"free regions past the rightmost: \" SSIZE_FORMAT \", bound \" SSIZE_FORMAT,\n+          end_off, rightmost(ShenandoahFreeSetPartitionId::OldCollector));\n+\n+  beg_off = empty_leftmosts[int(ShenandoahFreeSetPartitionId::OldCollector)];\n+  end_off = empty_rightmosts[int(ShenandoahFreeSetPartitionId::OldCollector)];\n+  assert (beg_off >= _leftmosts_empty[int(ShenandoahFreeSetPartitionId::OldCollector)],\n+          \"free empty regions before the leftmost: \" SSIZE_FORMAT \", bound \" SSIZE_FORMAT,\n+          beg_off, leftmost_empty(ShenandoahFreeSetPartitionId::OldCollector));\n+  assert (end_off <= _rightmosts_empty[int(ShenandoahFreeSetPartitionId::OldCollector)],\n+          \"free empty regions past the rightmost: \" SSIZE_FORMAT \", bound \" SSIZE_FORMAT,\n+          end_off, rightmost_empty(ShenandoahFreeSetPartitionId::OldCollector));\n+}\n+#endif\n+\n@@ -37,3 +671,3 @@\n-  _mutator_free_bitmap(max_regions, mtGC),\n-  _collector_free_bitmap(max_regions, mtGC),\n-  _max(max_regions)\n+  _partitions(max_regions, this),\n+  _trash_regions(NEW_C_HEAP_ARRAY(ShenandoahHeapRegion*, max_regions, mtGC)),\n+  _alloc_bias_weight(0)\n@@ -44,1 +678,1 @@\n-void ShenandoahFreeSet::increase_used(size_t num_bytes) {\n+void ShenandoahFreeSet::add_promoted_in_place_region_to_old_collector(ShenandoahHeapRegion* region) {\n@@ -46,10 +680,9 @@\n-  _used += num_bytes;\n-\n-  assert(_used <= _capacity, \"must not use more than we have: used: \" SIZE_FORMAT\n-         \", capacity: \" SIZE_FORMAT \", num_bytes: \" SIZE_FORMAT, _used, _capacity, num_bytes);\n-}\n-\n-bool ShenandoahFreeSet::is_mutator_free(size_t idx) const {\n-  assert (idx < _max, \"index is sane: \" SIZE_FORMAT \" < \" SIZE_FORMAT \" (left: \" SIZE_FORMAT \", right: \" SIZE_FORMAT \")\",\n-          idx, _max, _mutator_leftmost, _mutator_rightmost);\n-  return _mutator_free_bitmap.at(idx);\n+  size_t plab_min_size_in_bytes = ShenandoahGenerationalHeap::heap()->plab_min_size() * HeapWordSize;\n+  size_t idx = region->index();\n+  size_t capacity = alloc_capacity(region);\n+  assert(_partitions.membership(idx) == ShenandoahFreeSetPartitionId::NotFree,\n+         \"Regions promoted in place should have been excluded from Mutator partition\");\n+  if (capacity >= plab_min_size_in_bytes) {\n+    _partitions.make_free(idx, ShenandoahFreeSetPartitionId::OldCollector, capacity);\n+    _heap->old_generation()->augment_promoted_reserve(capacity);\n+  }\n@@ -58,4 +691,37 @@\n-bool ShenandoahFreeSet::is_collector_free(size_t idx) const {\n-  assert (idx < _max, \"index is sane: \" SIZE_FORMAT \" < \" SIZE_FORMAT \" (left: \" SIZE_FORMAT \", right: \" SIZE_FORMAT \")\",\n-          idx, _max, _collector_leftmost, _collector_rightmost);\n-  return _collector_free_bitmap.at(idx);\n+HeapWord* ShenandoahFreeSet::allocate_from_partition_with_affiliation(ShenandoahFreeSetPartitionId which_partition,\n+                                                                      ShenandoahAffiliation affiliation,\n+                                                                      ShenandoahAllocRequest& req, bool& in_new_region) {\n+  shenandoah_assert_heaplocked();\n+  idx_t rightmost_collector = ((affiliation == ShenandoahAffiliation::FREE)?\n+                               _partitions.rightmost_empty(which_partition): _partitions.rightmost(which_partition));\n+  idx_t leftmost_collector = ((affiliation == ShenandoahAffiliation::FREE)?\n+                              _partitions.leftmost_empty(which_partition): _partitions.leftmost(which_partition));\n+  if (_partitions.alloc_from_left_bias(which_partition)) {\n+    for (idx_t idx = leftmost_collector; idx <= rightmost_collector; ) {\n+      assert(_partitions.in_free_set(which_partition, idx), \"Boundaries or find_prev_last_bit failed: \" SSIZE_FORMAT, idx);\n+      ShenandoahHeapRegion* r = _heap->get_region(idx);\n+      if (r->affiliation() == affiliation) {\n+        HeapWord* result = try_allocate_in(r, req, in_new_region);\n+        if (result != nullptr) {\n+          return result;\n+        }\n+      }\n+      idx = _partitions.find_index_of_next_available_region(which_partition, idx + 1);\n+    }\n+  } else {\n+    for (idx_t idx = rightmost_collector; idx >= leftmost_collector; ) {\n+      assert(_partitions.in_free_set(which_partition, idx),\n+             \"Boundaries or find_prev_last_bit failed: \" SSIZE_FORMAT, idx);\n+      ShenandoahHeapRegion* r = _heap->get_region(idx);\n+      if (r->affiliation() == affiliation) {\n+        HeapWord* result = try_allocate_in(r, req, in_new_region);\n+        if (result != nullptr) {\n+          return result;\n+        }\n+      }\n+      idx = _partitions.find_index_of_previous_available_region(which_partition, idx - 1);\n+    }\n+  }\n+  log_debug(gc, free)(\"Could not allocate collector region with affiliation: %s for request \" PTR_FORMAT,\n+                      shenandoah_affiliation_name(affiliation), p2i(&req));\n+  return nullptr;\n@@ -65,0 +731,2 @@\n+  shenandoah_assert_heaplocked();\n+\n@@ -70,3 +738,3 @@\n-  \/\/ Allocations are biased: new application allocs go to beginning of the heap, and GC allocs\n-  \/\/ go to the end. This makes application allocation faster, because we would clear lots\n-  \/\/ of regions from the beginning most of the time.\n+  \/\/ Allocations are biased: GC allocations are taken from the high end of the heap.  Regular (and TLAB)\n+  \/\/ mutator allocations are taken from the middle of heap, below the memory reserved for Collector.\n+  \/\/ Humongous mutator allocations are taken from the bottom of the heap.\n@@ -74,2 +742,23 @@\n-  \/\/ Free set maintains mutator and collector views, and normally they allocate in their views only,\n-  \/\/ unless we special cases for stealing and mixed allocations.\n+  \/\/ Free set maintains mutator and collector partitions.  Normally, each allocates only from its partition,\n+  \/\/ except in special cases when the collector steals regions from the mutator partition.\n+\n+  \/\/ Overwrite with non-zero (non-NULL) values only if necessary for allocation bookkeeping.\n+  bool allow_new_region = true;\n+  if (_heap->mode()->is_generational()) {\n+    switch (req.affiliation()) {\n+      case ShenandoahAffiliation::OLD_GENERATION:\n+        \/\/ Note: unsigned result from free_unaffiliated_regions() will never be less than zero, but it may equal zero.\n+        if (_heap->old_generation()->free_unaffiliated_regions() <= 0) {\n+          allow_new_region = false;\n+        }\n+        break;\n+\n+      case ShenandoahAffiliation::YOUNG_GENERATION:\n+        \/\/ Note: unsigned result from free_unaffiliated_regions() will never be less than zero, but it may equal zero.\n+        if (_heap->young_generation()->free_unaffiliated_regions() <= 0) {\n+          allow_new_region = false;\n+        }\n+        break;\n+\n+      case ShenandoahAffiliation::FREE:\n+        fatal(\"Should request affiliation\");\n@@ -77,0 +766,5 @@\n+      default:\n+        ShouldNotReachHere();\n+        break;\n+    }\n+  }\n@@ -80,1 +774,0 @@\n-\n@@ -82,5 +775,59 @@\n-      for (size_t idx = _mutator_leftmost; idx <= _mutator_rightmost; idx++) {\n-        if (is_mutator_free(idx)) {\n-          HeapWord* result = try_allocate_in(_heap->get_region(idx), req, in_new_region);\n-          if (result != nullptr) {\n-            return result;\n+      if (_alloc_bias_weight-- <= 0) {\n+        \/\/ We have observed that regions not collected in previous GC cycle tend to congregate at one end or the other\n+        \/\/ of the heap.  Typically, these are the more recently engaged regions and the objects in these regions have not\n+        \/\/ yet had a chance to die (and\/or are treated as floating garbage).  If we use the same allocation bias on each\n+        \/\/ GC pass, these \"most recently\" engaged regions for GC pass N will also be the \"most recently\" engaged regions\n+        \/\/ for GC pass N+1, and the relatively large amount of live data and\/or floating garbage introduced\n+        \/\/ during the most recent GC pass may once again prevent the region from being collected.  We have found that\n+        \/\/ alternating the allocation behavior between GC passes improves evacuation performance by 3-7% on certain\n+        \/\/ benchmarks.  In the best case, this has the effect of consuming these partially consumed regions before\n+        \/\/ the start of the next mark cycle so all of their garbage can be efficiently reclaimed.\n+        \/\/\n+        \/\/ First, finish consuming regions that are already partially consumed so as to more tightly limit ranges of\n+        \/\/ available regions.  Other potential benefits:\n+        \/\/  1. Eventual collection set has fewer regions because we have packed newly allocated objects into fewer regions\n+        \/\/  2. We preserve the \"empty\" regions longer into the GC cycle, reducing likelihood of allocation failures\n+        \/\/     late in the GC cycle.\n+        idx_t non_empty_on_left = (_partitions.leftmost_empty(ShenandoahFreeSetPartitionId::Mutator)\n+                                     - _partitions.leftmost(ShenandoahFreeSetPartitionId::Mutator));\n+        idx_t non_empty_on_right = (_partitions.rightmost(ShenandoahFreeSetPartitionId::Mutator)\n+                                      - _partitions.rightmost_empty(ShenandoahFreeSetPartitionId::Mutator));\n+        _partitions.set_bias_from_left_to_right(ShenandoahFreeSetPartitionId::Mutator, (non_empty_on_right < non_empty_on_left));\n+        _alloc_bias_weight = _InitialAllocBiasWeight;\n+      }\n+      if (!_partitions.alloc_from_left_bias(ShenandoahFreeSetPartitionId::Mutator)) {\n+        \/\/ Allocate within mutator free from high memory to low so as to preserve low memory for humongous allocations\n+        if (!_partitions.is_empty(ShenandoahFreeSetPartitionId::Mutator)) {\n+          \/\/ Use signed idx.  Otherwise, loop will never terminate.\n+          idx_t leftmost = _partitions.leftmost(ShenandoahFreeSetPartitionId::Mutator);\n+          for (idx_t idx = _partitions.rightmost(ShenandoahFreeSetPartitionId::Mutator); idx >= leftmost; ) {\n+            assert(_partitions.in_free_set(ShenandoahFreeSetPartitionId::Mutator, idx),\n+                   \"Boundaries or find_last_set_bit failed: \" SSIZE_FORMAT, idx);\n+            ShenandoahHeapRegion* r = _heap->get_region(idx);\n+            \/\/ try_allocate_in() increases used if the allocation is successful.\n+            HeapWord* result;\n+            size_t min_size = (req.type() == ShenandoahAllocRequest::_alloc_tlab)? req.min_size(): req.size();\n+            if ((alloc_capacity(r) >= min_size) && ((result = try_allocate_in(r, req, in_new_region)) != nullptr)) {\n+              return result;\n+            }\n+            idx = _partitions.find_index_of_previous_available_region(ShenandoahFreeSetPartitionId::Mutator, idx - 1);\n+          }\n+        }\n+      } else {\n+        \/\/ Allocate from low to high memory.  This keeps the range of fully empty regions more tightly packed.\n+        \/\/ Note that the most recently allocated regions tend not to be evacuated in a given GC cycle.  So this\n+        \/\/ tends to accumulate \"fragmented\" uncollected regions in high memory.\n+        if (!_partitions.is_empty(ShenandoahFreeSetPartitionId::Mutator)) {\n+          \/\/ Use signed idx.  Otherwise, loop will never terminate.\n+          idx_t rightmost = _partitions.rightmost(ShenandoahFreeSetPartitionId::Mutator);\n+          for (idx_t idx = _partitions.leftmost(ShenandoahFreeSetPartitionId::Mutator); idx <= rightmost; ) {\n+            assert(_partitions.in_free_set(ShenandoahFreeSetPartitionId::Mutator, idx),\n+                   \"Boundaries or find_last_set_bit failed: \" SSIZE_FORMAT, idx);\n+            ShenandoahHeapRegion* r = _heap->get_region(idx);\n+            \/\/ try_allocate_in() increases used if the allocation is successful.\n+            HeapWord* result;\n+            size_t min_size = (req.type() == ShenandoahAllocRequest::_alloc_tlab)? req.min_size(): req.size();\n+            if ((alloc_capacity(r) >= min_size) && ((result = try_allocate_in(r, req, in_new_region)) != nullptr)) {\n+              return result;\n+            }\n+            idx = _partitions.find_index_of_next_available_region(ShenandoahFreeSetPartitionId::Mutator, idx + 1);\n@@ -90,1 +837,0 @@\n-\n@@ -95,2 +841,1 @@\n-    case ShenandoahAllocRequest::_alloc_shared_gc: {\n-      \/\/ size_t is unsigned, need to dodge underflow when _leftmost = 0\n+      \/\/ GCLABs are for evacuation so we must be in evacuation phase.\n@@ -98,0 +843,5 @@\n+    case ShenandoahAllocRequest::_alloc_plab: {\n+      \/\/ PLABs always reside in old-gen and are only allocated during\n+      \/\/ evacuation phase.\n+\n+    case ShenandoahAllocRequest::_alloc_shared_gc: {\n@@ -99,7 +849,13 @@\n-      for (size_t c = _collector_rightmost + 1; c > _collector_leftmost; c--) {\n-        size_t idx = c - 1;\n-        if (is_collector_free(idx)) {\n-          HeapWord* result = try_allocate_in(_heap->get_region(idx), req, in_new_region);\n-          if (result != nullptr) {\n-            return result;\n-          }\n+      HeapWord* result;\n+      result = allocate_from_partition_with_affiliation(req.is_old()? ShenandoahFreeSetPartitionId::OldCollector:\n+                                                        ShenandoahFreeSetPartitionId::Collector,\n+                                                        req.affiliation(), req, in_new_region);\n+      if (result != nullptr) {\n+        return result;\n+      } else if (allow_new_region) {\n+        \/\/ Try a free region that is dedicated to GC allocations.\n+        result = allocate_from_partition_with_affiliation(req.is_old()? ShenandoahFreeSetPartitionId::OldCollector:\n+                                                          ShenandoahFreeSetPartitionId::Collector,\n+                                                          ShenandoahAffiliation::FREE, req, in_new_region);\n+        if (result != nullptr) {\n+          return result;\n@@ -113,0 +869,4 @@\n+      if (!allow_new_region && req.is_old() && (_heap->young_generation()->free_unaffiliated_regions() > 0)) {\n+        \/\/ This allows us to flip a mutator region to old_collector\n+        allow_new_region = true;\n+      }\n@@ -114,4 +874,12 @@\n-      \/\/ Try to steal the empty region from the mutator view\n-      for (size_t c = _mutator_rightmost + 1; c > _mutator_leftmost; c--) {\n-        size_t idx = c - 1;\n-        if (is_mutator_free(idx)) {\n+      \/\/ We should expand old-gen if this can prevent an old-gen evacuation failure.  We don't care so much about\n+      \/\/ promotion failures since they can be mitigated in a subsequent GC pass.  Would be nice to know if this\n+      \/\/ allocation request is for evacuation or promotion.  Individual threads limit their use of PLAB memory for\n+      \/\/ promotions, so we already have an assurance that any additional memory set aside for old-gen will be used\n+      \/\/ only for old-gen evacuations.\n+      if (allow_new_region) {\n+        \/\/ Try to steal an empty region from the mutator view.\n+        idx_t rightmost_mutator = _partitions.rightmost_empty(ShenandoahFreeSetPartitionId::Mutator);\n+        idx_t leftmost_mutator =  _partitions.leftmost_empty(ShenandoahFreeSetPartitionId::Mutator);\n+        for (idx_t idx = rightmost_mutator; idx >= leftmost_mutator; ) {\n+          assert(_partitions.in_free_set(ShenandoahFreeSetPartitionId::Mutator, idx),\n+                 \"Boundaries or find_prev_last_bit failed: \" SSIZE_FORMAT, idx);\n@@ -120,4 +888,4 @@\n-            flip_to_gc(r);\n-            HeapWord *result = try_allocate_in(r, req, in_new_region);\n-            if (result != nullptr) {\n-              return result;\n+            if (req.is_old()) {\n+              flip_to_old_gc(r);\n+            } else {\n+              flip_to_gc(r);\n@@ -125,0 +893,4 @@\n+            \/\/ Region r is entirely empty.  If try_allocat_in fails on region r, something else is really wrong.\n+            \/\/ Don't bother to retry with other regions.\n+            log_debug(gc, free)(\"Flipped region \" SIZE_FORMAT \" to gc for request: \" PTR_FORMAT, idx, p2i(&req));\n+            return try_allocate_in(r, req, in_new_region);\n@@ -126,0 +898,1 @@\n+          idx = _partitions.find_index_of_previous_available_region(ShenandoahFreeSetPartitionId::Mutator, idx - 1);\n@@ -128,5 +901,2 @@\n-\n-      \/\/ No dice. Do not try to mix mutator and GC allocations, because\n-      \/\/ URWM moves due to GC allocations would expose unparsable mutator\n-      \/\/ allocations.\n-\n+      \/\/ No dice. Do not try to mix mutator and GC allocations, because adjusting region UWM\n+      \/\/ due to GC allocations would expose unparsable mutator allocations.\n@@ -135,0 +905,1 @@\n+    }\n@@ -138,1 +909,0 @@\n-\n@@ -142,2 +912,28 @@\n-HeapWord* ShenandoahFreeSet::try_allocate_in(ShenandoahHeapRegion* r, ShenandoahAllocRequest& req, bool& in_new_region) {\n-  assert (!has_no_alloc_capacity(r), \"Performance: should avoid full regions on this path: \" SIZE_FORMAT, r->index());\n+\/\/ This work method takes an argument corresponding to the number of bytes\n+\/\/ free in a region, and returns the largest amount in heapwords that can be allocated\n+\/\/ such that both of the following conditions are satisfied:\n+\/\/\n+\/\/ 1. it is a multiple of card size\n+\/\/ 2. any remaining shard may be filled with a filler object\n+\/\/\n+\/\/ The idea is that the allocation starts and ends at card boundaries. Because\n+\/\/ a region ('s end) is card-aligned, the remainder shard that must be filled is\n+\/\/ at the start of the free space.\n+\/\/\n+\/\/ This is merely a helper method to use for the purpose of such a calculation.\n+size_t ShenandoahFreeSet::get_usable_free_words(size_t free_bytes) const {\n+  \/\/ e.g. card_size is 512, card_shift is 9, min_fill_size() is 8\n+  \/\/      free is 514\n+  \/\/      usable_free is 512, which is decreased to 0\n+  size_t usable_free = (free_bytes \/ CardTable::card_size()) << CardTable::card_shift();\n+  assert(usable_free <= free_bytes, \"Sanity check\");\n+  if ((free_bytes != usable_free) && (free_bytes - usable_free < ShenandoahHeap::min_fill_size() * HeapWordSize)) {\n+    \/\/ After aligning to card multiples, the remainder would be smaller than\n+    \/\/ the minimum filler object, so we'll need to take away another card's\n+    \/\/ worth to construct a filler object.\n+    if (usable_free >= CardTable::card_size()) {\n+      usable_free -= CardTable::card_size();\n+    } else {\n+      assert(usable_free == 0, \"usable_free is a multiple of card_size and card_size > min_fill_size\");\n+    }\n+  }\n@@ -145,2 +941,24 @@\n-  if (_heap->is_concurrent_weak_root_in_progress() &&\n-      r->is_trash()) {\n+  return usable_free \/ HeapWordSize;\n+}\n+\n+\/\/ Given a size argument, which is a multiple of card size, a request struct\n+\/\/ for a PLAB, and an old region, return a pointer to the allocated space for\n+\/\/ a PLAB which is card-aligned and where any remaining shard in the region\n+\/\/ has been suitably filled by a filler object.\n+\/\/ It is assumed (and assertion-checked) that such an allocation is always possible.\n+HeapWord* ShenandoahFreeSet::allocate_aligned_plab(size_t size, ShenandoahAllocRequest& req, ShenandoahHeapRegion* r) {\n+  assert(_heap->mode()->is_generational(), \"PLABs are only for generational mode\");\n+  assert(r->is_old(), \"All PLABs reside in old-gen\");\n+  assert(!req.is_mutator_alloc(), \"PLABs should not be allocated by mutators.\");\n+  assert(is_aligned(size, CardTable::card_size_in_words()), \"Align by design\");\n+\n+  HeapWord* result = r->allocate_aligned(size, req, CardTable::card_size());\n+  assert(result != nullptr, \"Allocation cannot fail\");\n+  assert(r->top() <= r->end(), \"Allocation cannot span end of region\");\n+  assert(is_aligned(result, CardTable::card_size_in_words()), \"Align by design\");\n+  return result;\n+}\n+\n+HeapWord* ShenandoahFreeSet::try_allocate_in(ShenandoahHeapRegion* r, ShenandoahAllocRequest& req, bool& in_new_region) {\n+  assert (has_alloc_capacity(r), \"Performance: should avoid full regions on this path: \" SIZE_FORMAT, r->index());\n+  if (_heap->is_concurrent_weak_root_in_progress() && r->is_trash()) {\n@@ -149,1 +967,1 @@\n-\n+  HeapWord* result = nullptr;\n@@ -151,1 +969,0 @@\n-\n@@ -154,2 +971,16 @@\n-  HeapWord* result = nullptr;\n-  size_t size = req.size();\n+  if (in_new_region) {\n+    log_debug(gc)(\"Using new region (\" SIZE_FORMAT \") for %s (\" PTR_FORMAT \").\",\n+                       r->index(), ShenandoahAllocRequest::alloc_type_to_string(req.type()), p2i(&req));\n+    assert(!r->is_affiliated(), \"New region \" SIZE_FORMAT \" should be unaffiliated\", r->index());\n+    r->set_affiliation(req.affiliation());\n+    if (r->is_old()) {\n+      \/\/ Any OLD region allocated during concurrent coalesce-and-fill does not need to be coalesced and filled because\n+      \/\/ all objects allocated within this region are above TAMS (and thus are implicitly marked).  In case this is an\n+      \/\/ OLD region and concurrent preparation for mixed evacuations visits this region before the start of the next\n+      \/\/ old-gen concurrent mark (i.e. this region is allocated following the start of old-gen concurrent mark but before\n+      \/\/ concurrent preparations for mixed evacuations are completed), we mark this region as not requiring any\n+      \/\/ coalesce-and-fill processing.\n+      r->end_preemptible_coalesce_and_fill();\n+      _heap->old_generation()->clear_cards_for(r);\n+    }\n+    _heap->generation_for(r->affiliation())->increment_affiliated_region_count();\n@@ -157,4 +988,13 @@\n-  if (req.is_lab_alloc()) {\n-    size_t free = align_down(r->free() >> LogHeapWordSize, MinObjAlignment);\n-    if (size > free) {\n-      size = free;\n+#ifdef ASSERT\n+    ShenandoahMarkingContext* const ctx = _heap->complete_marking_context();\n+    assert(ctx->top_at_mark_start(r) == r->bottom(), \"Newly established allocation region starts with TAMS equal to bottom\");\n+    assert(ctx->is_bitmap_clear_range(ctx->top_bitmap(r), r->end()), \"Bitmap above top_bitmap() must be clear\");\n+#endif\n+    log_debug(gc)(\"Using new region (\" SIZE_FORMAT \") for %s (\" PTR_FORMAT \").\",\n+                       r->index(), ShenandoahAllocRequest::alloc_type_to_string(req.type()), p2i(&req));\n+  } else {\n+    assert(r->is_affiliated(), \"Region \" SIZE_FORMAT \" that is not new should be affiliated\", r->index());\n+    if (r->affiliation() != req.affiliation()) {\n+      assert(_heap->mode()->is_generational(), \"Request for %s from %s region should only happen in generational mode.\",\n+             req.affiliation_name(), r->affiliation_name());\n+      return nullptr;\n@@ -162,3 +1002,43 @@\n-    if (size >= req.min_size()) {\n-      result = r->allocate(size, req.type());\n-      assert (result != nullptr, \"Allocation must succeed: free \" SIZE_FORMAT \", actual \" SIZE_FORMAT, free, size);\n+  }\n+\n+  \/\/ req.size() is in words, r->free() is in bytes.\n+  if (req.is_lab_alloc()) {\n+    size_t adjusted_size = req.size();\n+    size_t free = r->free();    \/\/ free represents bytes available within region r\n+    if (req.type() == ShenandoahAllocRequest::_alloc_plab) {\n+      \/\/ This is a PLAB allocation\n+      assert(_heap->mode()->is_generational(), \"PLABs are only for generational mode\");\n+      assert(_partitions.in_free_set(ShenandoahFreeSetPartitionId::OldCollector, r->index()),\n+             \"PLABS must be allocated in old_collector_free regions\");\n+\n+      \/\/ Need to assure that plabs are aligned on multiple of card region\n+      \/\/ Convert free from unaligned bytes to aligned number of words\n+      size_t usable_free = get_usable_free_words(free);\n+      if (adjusted_size > usable_free) {\n+        adjusted_size = usable_free;\n+      }\n+      adjusted_size = align_down(adjusted_size, CardTable::card_size_in_words());\n+      if (adjusted_size >= req.min_size()) {\n+        result = allocate_aligned_plab(adjusted_size, req, r);\n+        assert(result != nullptr, \"allocate must succeed\");\n+        req.set_actual_size(adjusted_size);\n+      } else {\n+        \/\/ Otherwise, leave result == nullptr because the adjusted size is smaller than min size.\n+        log_trace(gc, free)(\"Failed to shrink PLAB request (\" SIZE_FORMAT \") in region \" SIZE_FORMAT \" to \" SIZE_FORMAT\n+                            \" because min_size() is \" SIZE_FORMAT, req.size(), r->index(), adjusted_size, req.min_size());\n+      }\n+    } else {\n+      \/\/ This is a GCLAB or a TLAB allocation\n+      \/\/ Convert free from unaligned bytes to aligned number of words\n+      free = align_down(free >> LogHeapWordSize, MinObjAlignment);\n+      if (adjusted_size > free) {\n+        adjusted_size = free;\n+      }\n+      if (adjusted_size >= req.min_size()) {\n+        result = r->allocate(adjusted_size, req);\n+        assert (result != nullptr, \"Allocation must succeed: free \" SIZE_FORMAT \", actual \" SIZE_FORMAT, free, adjusted_size);\n+        req.set_actual_size(adjusted_size);\n+      } else {\n+        log_trace(gc, free)(\"Failed to shrink TLAB or GCLAB request (\" SIZE_FORMAT \") in region \" SIZE_FORMAT \" to \" SIZE_FORMAT\n+                            \" because min_size() is \" SIZE_FORMAT, req.size(), r->index(), adjusted_size, req.min_size());\n+      }\n@@ -167,1 +1047,6 @@\n-    result = r->allocate(size, req.type());\n+    size_t size = req.size();\n+    result = r->allocate(size, req);\n+    if (result != nullptr) {\n+      \/\/ Record actual allocation size\n+      req.set_actual_size(size);\n+    }\n@@ -173,5 +1058,4 @@\n-      increase_used(size * HeapWordSize);\n-    }\n-\n-    \/\/ Record actual allocation size\n-    req.set_actual_size(size);\n+      assert(req.is_young(), \"Mutator allocations always come from young generation.\");\n+      _partitions.increase_used(ShenandoahFreeSetPartitionId::Mutator, req.actual_size() * HeapWordSize);\n+    } else {\n+      assert(req.is_gc_alloc(), \"Should be gc_alloc since req wasn't mutator alloc\");\n@@ -179,1 +1063,3 @@\n-    if (req.is_gc_alloc()) {\n+      \/\/ For GC allocations, we advance update_watermark because the objects relocated into this memory during\n+      \/\/ evacuation are not updated during evacuation.  For both young and old regions r, it is essential that all\n+      \/\/ PLABs be made parsable at the end of evacuation.  This is enabled by retiring all plabs at end of evacuation.\n@@ -181,0 +1067,7 @@\n+      if (r->is_old()) {\n+        _partitions.increase_used(ShenandoahFreeSetPartitionId::OldCollector, req.actual_size() * HeapWordSize);\n+        assert(req.type() != ShenandoahAllocRequest::_alloc_gclab, \"old-gen allocations use PLAB or shared allocation\");\n+        \/\/ for plabs, we'll sort the difference between evac and promotion usage when we retire the plab\n+      } else {\n+        _partitions.increase_used(ShenandoahFreeSetPartitionId::Collector, req.actual_size() * HeapWordSize);\n+      }\n@@ -184,7 +1077,9 @@\n-  if (result == nullptr || has_no_alloc_capacity(r)) {\n-    \/\/ Region cannot afford this or future allocations. Retire it.\n-    \/\/\n-    \/\/ While this seems a bit harsh, especially in the case when this large allocation does not\n-    \/\/ fit, but the next small one would, we are risking to inflate scan times when lots of\n-    \/\/ almost-full regions precede the fully-empty region where we want allocate the entire TLAB.\n-    \/\/ TODO: Record first fully-empty region, and use that for large allocations\n+  static const size_t min_capacity = (size_t) (ShenandoahHeapRegion::region_size_bytes() * (1.0 - 1.0 \/ ShenandoahEvacWaste));\n+  size_t ac = alloc_capacity(r);\n+\n+  if (((result == nullptr) && (ac < min_capacity)) || (alloc_capacity(r) < PLAB::min_size() * HeapWordSize)) {\n+    \/\/ Regardless of whether this allocation succeeded, if the remaining memory is less than PLAB:min_size(), retire this region.\n+    \/\/ Note that retire_from_partition() increases used to account for waste.\n+\n+    \/\/ Also, if this allocation request failed and the consumed within this region * ShenandoahEvacWaste > region size,\n+    \/\/ then retire the region so that subsequent searches can find available memory more quickly.\n@@ -192,1 +1087,2 @@\n-    \/\/ Record the remainder as allocation waste\n+    size_t idx = r->index();\n+    ShenandoahFreeSetPartitionId orig_partition;\n@@ -194,4 +1090,11 @@\n-      size_t waste = r->free();\n-      if (waste > 0) {\n-        increase_used(waste);\n-        _heap->notify_mutator_alloc_words(waste >> LogHeapWordSize, true);\n+      orig_partition = ShenandoahFreeSetPartitionId::Mutator;\n+    } else if (req.type() == ShenandoahAllocRequest::_alloc_gclab) {\n+      orig_partition = ShenandoahFreeSetPartitionId::Collector;\n+    } else if (req.type() == ShenandoahAllocRequest::_alloc_plab) {\n+      orig_partition = ShenandoahFreeSetPartitionId::OldCollector;\n+    } else {\n+      assert(req.type() == ShenandoahAllocRequest::_alloc_shared_gc, \"Unexpected allocation type\");\n+      if (req.is_old()) {\n+        orig_partition = ShenandoahFreeSetPartitionId::OldCollector;\n+      } else {\n+        orig_partition = ShenandoahFreeSetPartitionId::Collector;\n@@ -200,9 +1103,2 @@\n-\n-    size_t num = r->index();\n-    _collector_free_bitmap.clear_bit(num);\n-    _mutator_free_bitmap.clear_bit(num);\n-    \/\/ Touched the bounds? Need to update:\n-    if (touches_bounds(num)) {\n-      adjust_bounds();\n-    }\n-    assert_bounds();\n+    _partitions.retire_from_partition(orig_partition, idx, r->used());\n+    _partitions.assert_bounds();\n@@ -213,32 +1109,0 @@\n-bool ShenandoahFreeSet::touches_bounds(size_t num) const {\n-  return num == _collector_leftmost || num == _collector_rightmost || num == _mutator_leftmost || num == _mutator_rightmost;\n-}\n-\n-void ShenandoahFreeSet::recompute_bounds() {\n-  \/\/ Reset to the most pessimistic case:\n-  _mutator_rightmost = _max - 1;\n-  _mutator_leftmost = 0;\n-  _collector_rightmost = _max - 1;\n-  _collector_leftmost = 0;\n-\n-  \/\/ ...and adjust from there\n-  adjust_bounds();\n-}\n-\n-void ShenandoahFreeSet::adjust_bounds() {\n-  \/\/ Rewind both mutator bounds until the next bit.\n-  while (_mutator_leftmost < _max && !is_mutator_free(_mutator_leftmost)) {\n-    _mutator_leftmost++;\n-  }\n-  while (_mutator_rightmost > 0 && !is_mutator_free(_mutator_rightmost)) {\n-    _mutator_rightmost--;\n-  }\n-  \/\/ Rewind both collector bounds until the next bit.\n-  while (_collector_leftmost < _max && !is_collector_free(_collector_leftmost)) {\n-    _collector_leftmost++;\n-  }\n-  while (_collector_rightmost > 0 && !is_collector_free(_collector_rightmost)) {\n-    _collector_rightmost--;\n-  }\n-}\n-\n@@ -246,0 +1110,1 @@\n+  assert(req.is_mutator_alloc(), \"All humongous allocations are performed by mutator\");\n@@ -249,1 +1114,1 @@\n-  size_t num = ShenandoahHeapRegion::required_regions(words_size * HeapWordSize);\n+  idx_t num = ShenandoahHeapRegion::required_regions(words_size * HeapWordSize);\n@@ -251,2 +1116,5 @@\n-  \/\/ No regions left to satisfy allocation, bye.\n-  if (num > mutator_count()) {\n+  assert(req.is_young(), \"Humongous regions always allocated in YOUNG\");\n+  ShenandoahGeneration* generation = _heap->generation_for(req.affiliation());\n+\n+  \/\/ Check if there are enough regions left to satisfy allocation.\n+  if (num > (idx_t) _partitions.count(ShenandoahFreeSetPartitionId::Mutator)) {\n@@ -256,0 +1124,4 @@\n+  idx_t start_range = _partitions.leftmost_empty(ShenandoahFreeSetPartitionId::Mutator);\n+  idx_t end_range = _partitions.rightmost_empty(ShenandoahFreeSetPartitionId::Mutator) + 1;\n+  idx_t last_possible_start = end_range - num;\n+\n@@ -258,3 +1130,7 @@\n-\n-  size_t beg = _mutator_leftmost;\n-  size_t end = beg;\n+  idx_t beg = _partitions.find_index_of_next_available_cluster_of_regions(ShenandoahFreeSetPartitionId::Mutator,\n+                                                                          start_range, num);\n+  if (beg > last_possible_start) {\n+    \/\/ Hit the end, goodbye\n+    return nullptr;\n+  }\n+  idx_t end = beg;\n@@ -263,11 +1139,26 @@\n-    if (end >= _max) {\n-      \/\/ Hit the end, goodbye\n-      return nullptr;\n-    }\n-\n-    \/\/ If regions are not adjacent, then current [beg; end] is useless, and we may fast-forward.\n-    \/\/ If region is not completely free, the current [beg; end] is useless, and we may fast-forward.\n-    if (!is_mutator_free(end) || !can_allocate_from(_heap->get_region(end))) {\n-      end++;\n-      beg = end;\n-      continue;\n+    \/\/ We've confirmed num contiguous regions belonging to Mutator partition, so no need to confirm membership.\n+    \/\/ If region is not completely free, the current [beg; end] is useless, and we may fast-forward.  If we can extend\n+    \/\/ the existing range, we can exploit that certain regions are already known to be in the Mutator free set.\n+    while (!can_allocate_from(_heap->get_region(end))) {\n+      \/\/ region[end] is not empty, so we restart our search after region[end]\n+      idx_t slide_delta = end + 1 - beg;\n+      if (beg + slide_delta > last_possible_start) {\n+        \/\/ no room to slide\n+        return nullptr;\n+      }\n+      for (idx_t span_end = beg + num; slide_delta > 0; slide_delta--) {\n+        if (!_partitions.in_free_set(ShenandoahFreeSetPartitionId::Mutator, span_end)) {\n+          beg = _partitions.find_index_of_next_available_cluster_of_regions(ShenandoahFreeSetPartitionId::Mutator,\n+                                                                            span_end + 1, num);\n+          break;\n+        } else {\n+          beg++;\n+          span_end++;\n+        }\n+      }\n+      \/\/ Here, either beg identifies a range of num regions all of which are in the Mutator free set, or beg > last_possible_start\n+      if (beg > last_possible_start) {\n+        \/\/ Hit the end, goodbye\n+        return nullptr;\n+      }\n+      end = beg;\n@@ -285,1 +1176,0 @@\n-\n@@ -287,1 +1177,1 @@\n-  for (size_t i = beg; i <= end; i++) {\n+  for (idx_t i = beg; i <= end; i++) {\n@@ -308,0 +1198,2 @@\n+    r->set_affiliation(req.affiliation());\n+    r->set_update_watermark(r->bottom());\n@@ -309,2 +1201,0 @@\n-\n-    _mutator_free_bitmap.clear_bit(r->index());\n@@ -312,5 +1202,1 @@\n-\n-  \/\/ While individual regions report their true use, all humongous regions are\n-  \/\/ marked used in the free set.\n-  increase_used(ShenandoahHeapRegion::region_size_bytes() * num);\n-\n+  generation->increase_affiliated_region_count(num);\n@@ -322,5 +1208,2 @@\n-  \/\/ Allocated at left\/rightmost? Move the bounds appropriately.\n-  if (beg == _mutator_leftmost || end == _mutator_rightmost) {\n-    adjust_bounds();\n-  }\n-  assert_bounds();\n+  \/\/ retire_range_from_partition() will adjust bounds on Mutator free set if appropriate\n+  _partitions.retire_range_from_partition(ShenandoahFreeSetPartitionId::Mutator, beg, end);\n@@ -328,0 +1211,3 @@\n+  size_t total_humongous_size = ShenandoahHeapRegion::region_size_bytes() * num;\n+  _partitions.increase_used(ShenandoahFreeSetPartitionId::Mutator, total_humongous_size);\n+  _partitions.assert_bounds();\n@@ -329,13 +1215,2 @@\n-  return _heap->get_region(beg)->bottom();\n-}\n-\n-bool ShenandoahFreeSet::can_allocate_from(ShenandoahHeapRegion *r) {\n-  return r->is_empty() || (r->is_trash() && !_heap->is_concurrent_weak_root_in_progress());\n-}\n-\n-size_t ShenandoahFreeSet::alloc_capacity(ShenandoahHeapRegion *r) {\n-  if (r->is_trash()) {\n-    \/\/ This would be recycled on allocation path\n-    return ShenandoahHeapRegion::region_size_bytes();\n-  } else {\n-    return r->free();\n+  if (remainder != 0) {\n+    req.set_waste(ShenandoahHeapRegion::region_size_words() - remainder);\n@@ -343,0 +1218,1 @@\n+  return _heap->get_region(beg)->bottom();\n@@ -345,5 +1221,1 @@\n-bool ShenandoahFreeSet::has_no_alloc_capacity(ShenandoahHeapRegion *r) {\n-  return alloc_capacity(r) == 0;\n-}\n-\n-void ShenandoahFreeSet::try_recycle_trashed(ShenandoahHeapRegion *r) {\n+void ShenandoahFreeSet::try_recycle_trashed(ShenandoahHeapRegion* r) {\n@@ -351,1 +1223,0 @@\n-    _heap->decrease_used(r->used());\n@@ -359,1 +1230,1 @@\n-\n+  size_t count = 0;\n@@ -363,2 +1234,1 @@\n-      ShenandoahHeapLocker locker(_heap->lock());\n-      try_recycle_trashed(r);\n+      _trash_regions[count++] = r;\n@@ -366,1 +1236,41 @@\n-    SpinPause(); \/\/ allow allocators to take the lock\n+  }\n+\n+  size_t total_batches = 0;\n+  jlong batch_start_time = 0;\n+  jlong recycle_trash_start_time = os::javaTimeNanos();    \/\/ This value will be treated as the initial batch_start_time\n+  jlong batch_end_time = recycle_trash_start_time;\n+  \/\/ Process as many batches as can be processed within 10 us.\n+  static constexpr jlong deadline_ns = 10000;               \/\/ 10 us\n+  size_t idx = 0;\n+  jlong predicted_next_batch_end_time;\n+  jlong batch_process_time_estimate = 0;\n+  while (idx < count) {\n+    if (idx > 0) {\n+      os::naked_yield(); \/\/ Yield to allow allocators to take the lock, except on the first iteration\n+    }\n+    \/\/ Avoid another call to javaTimeNanos() if we already know time at which last batch ended\n+    batch_start_time = batch_end_time;\n+    const jlong deadline = batch_start_time + deadline_ns;\n+\n+    ShenandoahHeapLocker locker(_heap->lock());\n+    do {\n+      \/\/ Measurements on typical 2024 hardware suggest it typically requires between 1400 and 2000 ns to process a batch of\n+      \/\/ 32 regions, assuming low contention with other threads.  Sometimes this goes higher, when mutator threads\n+      \/\/ are contending for CPU cores and\/or the heap lock.  On this hardware with a 10 us deadline, we expect 3-6 batches\n+      \/\/ to be processed between yields most of the time.\n+      \/\/\n+      \/\/ Note that deadline is enforced since the end of previous batch.  In the case that yield() or acquisition of heap lock\n+      \/\/ takes a \"long time\", we will have less time to process regions, but we will always process at least one batch between\n+      \/\/ yields.  Yielding more frequently when there is heavy contention for the heap lock or for CPU cores is considered the\n+      \/\/ right thing to do.\n+      const size_t REGIONS_PER_BATCH = 32;\n+      size_t max_idx = MIN2(count, idx + REGIONS_PER_BATCH);\n+      while (idx < max_idx) {\n+        try_recycle_trashed(_trash_regions[idx++]);\n+      }\n+      total_batches++;\n+      batch_end_time = os::javaTimeNanos();\n+      \/\/ Estimate includes historic combination of yield times and heap lock acquisition times.\n+      batch_process_time_estimate = (batch_end_time - recycle_trash_start_time) \/ total_batches;\n+      predicted_next_batch_end_time = batch_end_time + batch_process_time_estimate;\n+    } while ((idx < count) && (predicted_next_batch_end_time < deadline));\n@@ -370,1 +1280,1 @@\n-void ShenandoahFreeSet::flip_to_gc(ShenandoahHeapRegion* r) {\n+void ShenandoahFreeSet::flip_to_old_gc(ShenandoahHeapRegion* r) {\n@@ -373,1 +1283,1 @@\n-  assert(_mutator_free_bitmap.at(idx), \"Should be in mutator view\");\n+  assert(_partitions.partition_id_matches(idx, ShenandoahFreeSetPartitionId::Mutator), \"Should be in mutator view\");\n@@ -376,4 +1286,14 @@\n-  _mutator_free_bitmap.clear_bit(idx);\n-  _collector_free_bitmap.set_bit(idx);\n-  _collector_leftmost = MIN2(idx, _collector_leftmost);\n-  _collector_rightmost = MAX2(idx, _collector_rightmost);\n+  ShenandoahGenerationalHeap* gen_heap = ShenandoahGenerationalHeap::heap();\n+  size_t region_capacity = alloc_capacity(r);\n+  _partitions.move_from_partition_to_partition(idx, ShenandoahFreeSetPartitionId::Mutator,\n+                                               ShenandoahFreeSetPartitionId::OldCollector, region_capacity);\n+  _partitions.assert_bounds();\n+  _heap->old_generation()->augment_evacuation_reserve(region_capacity);\n+  bool transferred = gen_heap->generation_sizer()->transfer_to_old(1);\n+  if (!transferred) {\n+    log_warning(gc, free)(\"Forcing transfer of \" SIZE_FORMAT \" to old reserve.\", idx);\n+    gen_heap->generation_sizer()->force_transfer_to_old(1);\n+  }\n+  \/\/ We do not ensure that the region is no longer trash, relying on try_allocate_in(), which always comes next,\n+  \/\/ to recycle trash before attempting to allocate anything in the region.\n+}\n@@ -381,1 +1301,2 @@\n-  _capacity -= alloc_capacity(r);\n+void ShenandoahFreeSet::flip_to_gc(ShenandoahHeapRegion* r) {\n+  size_t idx = r->index();\n@@ -383,4 +1304,10 @@\n-  if (touches_bounds(idx)) {\n-    adjust_bounds();\n-  }\n-  assert_bounds();\n+  assert(_partitions.partition_id_matches(idx, ShenandoahFreeSetPartitionId::Mutator), \"Should be in mutator view\");\n+  assert(can_allocate_from(r), \"Should not be allocated\");\n+\n+  size_t ac = alloc_capacity(r);\n+  _partitions.move_from_partition_to_partition(idx, ShenandoahFreeSetPartitionId::Mutator,\n+                                               ShenandoahFreeSetPartitionId::Collector, ac);\n+  _partitions.assert_bounds();\n+\n+  \/\/ We do not ensure that the region is no longer trash, relying on try_allocate_in(), which always comes next,\n+  \/\/ to recycle trash before attempting to allocate anything in the region.\n@@ -395,8 +1322,6 @@\n-  _mutator_free_bitmap.clear();\n-  _collector_free_bitmap.clear();\n-  _mutator_leftmost = _max;\n-  _mutator_rightmost = 0;\n-  _collector_leftmost = _max;\n-  _collector_rightmost = 0;\n-  _capacity = 0;\n-  _used = 0;\n+  _partitions.make_all_regions_unavailable();\n+\n+  _alloc_bias_weight = 0;\n+  _partitions.set_bias_from_left_to_right(ShenandoahFreeSetPartitionId::Mutator, true);\n+  _partitions.set_bias_from_left_to_right(ShenandoahFreeSetPartitionId::Collector, false);\n+  _partitions.set_bias_from_left_to_right(ShenandoahFreeSetPartitionId::OldCollector, false);\n@@ -405,3 +1330,4 @@\n-void ShenandoahFreeSet::rebuild() {\n-  shenandoah_assert_heaplocked();\n-  clear();\n+void ShenandoahFreeSet::find_regions_with_alloc_capacity(size_t &young_cset_regions, size_t &old_cset_regions,\n+                                                         size_t &first_old_region, size_t &last_old_region,\n+                                                         size_t &old_region_count) {\n+  clear_internal();\n@@ -409,1 +1335,25 @@\n-  for (size_t idx = 0; idx < _heap->num_regions(); idx++) {\n+  first_old_region = _heap->num_regions();\n+  last_old_region = 0;\n+  old_region_count = 0;\n+  old_cset_regions = 0;\n+  young_cset_regions = 0;\n+\n+  size_t region_size_bytes = _partitions.region_size_bytes();\n+  size_t max_regions = _partitions.max_regions();\n+\n+  size_t mutator_leftmost = max_regions;\n+  size_t mutator_rightmost = 0;\n+  size_t mutator_leftmost_empty = max_regions;\n+  size_t mutator_rightmost_empty = 0;\n+  size_t mutator_regions = 0;\n+  size_t mutator_used = 0;\n+\n+  size_t old_collector_leftmost = max_regions;\n+  size_t old_collector_rightmost = 0;\n+  size_t old_collector_leftmost_empty = max_regions;\n+  size_t old_collector_rightmost_empty = 0;\n+  size_t old_collector_regions = 0;\n+  size_t old_collector_used = 0;\n+\n+  size_t num_regions = _heap->num_regions();\n+  for (size_t idx = 0; idx < num_regions; idx++) {\n@@ -411,0 +1361,17 @@\n+    if (region->is_trash()) {\n+      \/\/ Trashed regions represent regions that had been in the collection partition but have not yet been \"cleaned up\".\n+      \/\/ The cset regions are not \"trashed\" until we have finished update refs.\n+      if (region->is_old()) {\n+        old_cset_regions++;\n+      } else {\n+        assert(region->is_young(), \"Trashed region should be old or young\");\n+        young_cset_regions++;\n+      }\n+    } else if (region->is_old()) {\n+      \/\/ count both humongous and regular regions, but don't count trash (cset) regions.\n+      old_region_count++;\n+      if (first_old_region > idx) {\n+        first_old_region = idx;\n+      }\n+      last_old_region = idx;\n+    }\n@@ -412,1 +1379,1 @@\n-      assert(!region->is_cset(), \"Shouldn't be adding those to the free set\");\n+      assert(!region->is_cset(), \"Shouldn't be adding cset regions to the free set\");\n@@ -414,2 +1381,78 @@\n-      \/\/ Do not add regions that would surely fail allocation\n-      if (has_no_alloc_capacity(region)) continue;\n+      \/\/ Do not add regions that would almost surely fail allocation\n+      size_t ac = alloc_capacity(region);\n+      if (ac > PLAB::min_size() * HeapWordSize) {\n+        if (region->is_trash() || !region->is_old()) {\n+          \/\/ Both young and old collected regions (trashed) are placed into the Mutator set\n+          _partitions.raw_assign_membership(idx, ShenandoahFreeSetPartitionId::Mutator);\n+          if (idx < mutator_leftmost) {\n+            mutator_leftmost = idx;\n+          }\n+          if (idx > mutator_rightmost) {\n+            mutator_rightmost = idx;\n+          }\n+          if (ac == region_size_bytes) {\n+            if (idx < mutator_leftmost_empty) {\n+              mutator_leftmost_empty = idx;\n+            }\n+            if (idx > mutator_rightmost_empty) {\n+              mutator_rightmost_empty = idx;\n+            }\n+          }\n+          mutator_regions++;\n+          mutator_used += (region_size_bytes - ac);\n+        } else {\n+          \/\/ !region->is_trash() && region is_old()\n+          _partitions.raw_assign_membership(idx, ShenandoahFreeSetPartitionId::OldCollector);\n+          if (idx < old_collector_leftmost) {\n+            old_collector_leftmost = idx;\n+          }\n+          if (idx > old_collector_rightmost) {\n+            old_collector_rightmost = idx;\n+          }\n+          if (ac == region_size_bytes) {\n+            if (idx < old_collector_leftmost_empty) {\n+              old_collector_leftmost_empty = idx;\n+            }\n+            if (idx > old_collector_rightmost_empty) {\n+              old_collector_rightmost_empty = idx;\n+            }\n+          }\n+          old_collector_regions++;\n+          old_collector_used += (region_size_bytes - ac);\n+        }\n+      }\n+    }\n+  }\n+  log_debug(gc)(\"  At end of prep_to_rebuild, mutator_leftmost: \" SIZE_FORMAT\n+                \", mutator_rightmost: \" SIZE_FORMAT\n+                \", mutator_leftmost_empty: \" SIZE_FORMAT\n+                \", mutator_rightmost_empty: \" SIZE_FORMAT\n+                \", mutator_regions: \" SIZE_FORMAT\n+                \", mutator_used: \" SIZE_FORMAT,\n+                mutator_leftmost, mutator_rightmost, mutator_leftmost_empty, mutator_rightmost_empty,\n+                mutator_regions, mutator_used);\n+\n+  log_debug(gc)(\"  old_collector_leftmost: \" SIZE_FORMAT\n+                \", old_collector_rightmost: \" SIZE_FORMAT\n+                \", old_collector_leftmost_empty: \" SIZE_FORMAT\n+                \", old_collector_rightmost_empty: \" SIZE_FORMAT\n+                \", old_collector_regions: \" SIZE_FORMAT\n+                \", old_collector_used: \" SIZE_FORMAT,\n+                old_collector_leftmost, old_collector_rightmost, old_collector_leftmost_empty, old_collector_rightmost_empty,\n+                old_collector_regions, old_collector_used);\n+\n+  idx_t rightmost_idx = (mutator_leftmost == max_regions)? -1: (idx_t) mutator_rightmost;\n+  idx_t rightmost_empty_idx = (mutator_leftmost_empty == max_regions)? -1: (idx_t) mutator_rightmost_empty;\n+  _partitions.establish_mutator_intervals(mutator_leftmost, rightmost_idx, mutator_leftmost_empty, rightmost_empty_idx,\n+                                          mutator_regions, mutator_used);\n+  rightmost_idx = (old_collector_leftmost == max_regions)? -1: (idx_t) old_collector_rightmost;\n+  rightmost_empty_idx = (old_collector_leftmost_empty == max_regions)? -1: (idx_t) old_collector_rightmost_empty;\n+  _partitions.establish_old_collector_intervals(old_collector_leftmost, rightmost_idx, old_collector_leftmost_empty,\n+                                                rightmost_empty_idx, old_collector_regions, old_collector_used);\n+  log_debug(gc)(\"  After find_regions_with_alloc_capacity(), Mutator range [\" SSIZE_FORMAT \", \" SSIZE_FORMAT \"],\"\n+                \"  Old Collector range [\" SSIZE_FORMAT \", \" SSIZE_FORMAT \"]\",\n+                _partitions.leftmost(ShenandoahFreeSetPartitionId::Mutator),\n+                _partitions.rightmost(ShenandoahFreeSetPartitionId::Mutator),\n+                _partitions.leftmost(ShenandoahFreeSetPartitionId::OldCollector),\n+                _partitions.rightmost(ShenandoahFreeSetPartitionId::OldCollector));\n+}\n@@ -417,2 +1460,20 @@\n-      _capacity += alloc_capacity(region);\n-      assert(_used <= _capacity, \"must not use more than we have\");\n+\/\/ Returns number of regions transferred, adds transferred bytes to var argument bytes_transferred\n+size_t ShenandoahFreeSet::transfer_empty_regions_from_collector_set_to_mutator_set(ShenandoahFreeSetPartitionId which_collector,\n+                                                                                   size_t max_xfer_regions,\n+                                                                                   size_t& bytes_transferred) {\n+  shenandoah_assert_heaplocked();\n+  size_t region_size_bytes = ShenandoahHeapRegion::region_size_bytes();\n+  size_t transferred_regions = 0;\n+  idx_t rightmost = _partitions.rightmost_empty(which_collector);\n+  for (idx_t idx = _partitions.leftmost_empty(which_collector); (transferred_regions < max_xfer_regions) && (idx <= rightmost); ) {\n+    assert(_partitions.in_free_set(which_collector, idx), \"Boundaries or find_first_set_bit failed: \" SSIZE_FORMAT, idx);\n+    \/\/ Note: can_allocate_from() denotes that region is entirely empty\n+    if (can_allocate_from(idx)) {\n+      _partitions.move_from_partition_to_partition(idx, which_collector, ShenandoahFreeSetPartitionId::Mutator, region_size_bytes);\n+      transferred_regions++;\n+      bytes_transferred += region_size_bytes;\n+    }\n+    idx = _partitions.find_index_of_next_available_region(which_collector, idx + 1);\n+  }\n+  return transferred_regions;\n+}\n@@ -420,2 +1481,14 @@\n-      assert(!is_mutator_free(idx), \"We are about to add it, it shouldn't be there already\");\n-      _mutator_free_bitmap.set_bit(idx);\n+\/\/ Returns number of regions transferred, adds transferred bytes to var argument bytes_transferred\n+size_t ShenandoahFreeSet::transfer_non_empty_regions_from_collector_set_to_mutator_set(ShenandoahFreeSetPartitionId collector_id,\n+                                                                                       size_t max_xfer_regions,\n+                                                                                       size_t& bytes_transferred) {\n+  shenandoah_assert_heaplocked();\n+  size_t transferred_regions = 0;\n+  idx_t rightmost = _partitions.rightmost(collector_id);\n+  for (idx_t idx = _partitions.leftmost(collector_id); (transferred_regions < max_xfer_regions) && (idx <= rightmost); ) {\n+    assert(_partitions.in_free_set(collector_id, idx), \"Boundaries or find_first_set_bit failed: \" SSIZE_FORMAT, idx);\n+    size_t ac = alloc_capacity(idx);\n+    if (ac > 0) {\n+      _partitions.move_from_partition_to_partition(idx, collector_id, ShenandoahFreeSetPartitionId::Mutator, ac);\n+      transferred_regions++;\n+      bytes_transferred += ac;\n@@ -423,0 +1496,1 @@\n+    idx = _partitions.find_index_of_next_available_region(ShenandoahFreeSetPartitionId::Collector, idx + 1);\n@@ -424,0 +1498,2 @@\n+  return transferred_regions;\n+}\n@@ -425,3 +1501,13 @@\n-  \/\/ Evac reserve: reserve trailing space for evacuations\n-  size_t to_reserve = _heap->max_capacity() \/ 100 * ShenandoahEvacReserve;\n-  size_t reserved = 0;\n+void ShenandoahFreeSet::move_regions_from_collector_to_mutator(size_t max_xfer_regions) {\n+  size_t collector_xfer = 0;\n+  size_t old_collector_xfer = 0;\n+\n+  \/\/ Process empty regions within the Collector free partition\n+  if ((max_xfer_regions > 0) &&\n+      (_partitions.leftmost_empty(ShenandoahFreeSetPartitionId::Collector)\n+       <= _partitions.rightmost_empty(ShenandoahFreeSetPartitionId::Collector))) {\n+    ShenandoahHeapLocker locker(_heap->lock());\n+    max_xfer_regions -=\n+      transfer_empty_regions_from_collector_set_to_mutator_set(ShenandoahFreeSetPartitionId::Collector, max_xfer_regions,\n+                                                               collector_xfer);\n+  }\n@@ -429,2 +1515,13 @@\n-  for (size_t idx = _heap->num_regions() - 1; idx > 0; idx--) {\n-    if (reserved >= to_reserve) break;\n+  \/\/ Process empty regions within the OldCollector free partition\n+  if ((max_xfer_regions > 0) &&\n+      (_partitions.leftmost_empty(ShenandoahFreeSetPartitionId::OldCollector)\n+       <= _partitions.rightmost_empty(ShenandoahFreeSetPartitionId::OldCollector))) {\n+    ShenandoahHeapLocker locker(_heap->lock());\n+    size_t old_collector_regions =\n+      transfer_empty_regions_from_collector_set_to_mutator_set(ShenandoahFreeSetPartitionId::OldCollector, max_xfer_regions,\n+                                                               old_collector_xfer);\n+    max_xfer_regions -= old_collector_regions;\n+    if (old_collector_regions > 0) {\n+      ShenandoahGenerationalHeap::cast(_heap)->generation_sizer()->transfer_to_young(old_collector_regions);\n+    }\n+  }\n@@ -432,7 +1529,53 @@\n-    ShenandoahHeapRegion* region = _heap->get_region(idx);\n-    if (_mutator_free_bitmap.at(idx) && can_allocate_from(region)) {\n-      _mutator_free_bitmap.clear_bit(idx);\n-      _collector_free_bitmap.set_bit(idx);\n-      size_t ac = alloc_capacity(region);\n-      _capacity -= ac;\n-      reserved += ac;\n+  \/\/ If there are any non-empty regions within Collector partition, we can also move them to the Mutator free partition\n+  if ((max_xfer_regions > 0) && (_partitions.leftmost(ShenandoahFreeSetPartitionId::Collector)\n+                                 <= _partitions.rightmost(ShenandoahFreeSetPartitionId::Collector))) {\n+    ShenandoahHeapLocker locker(_heap->lock());\n+    max_xfer_regions -=\n+      transfer_non_empty_regions_from_collector_set_to_mutator_set(ShenandoahFreeSetPartitionId::Collector, max_xfer_regions,\n+                                                                   collector_xfer);\n+  }\n+\n+  size_t total_xfer = collector_xfer + old_collector_xfer;\n+  log_info(gc, ergo)(\"At start of update refs, moving \" SIZE_FORMAT \"%s to Mutator free set from Collector Reserve (\"\n+                     SIZE_FORMAT \"%s) and from Old Collector Reserve (\" SIZE_FORMAT \"%s)\",\n+                     byte_size_in_proper_unit(total_xfer), proper_unit_for_byte_size(total_xfer),\n+                     byte_size_in_proper_unit(collector_xfer), proper_unit_for_byte_size(collector_xfer),\n+                     byte_size_in_proper_unit(old_collector_xfer), proper_unit_for_byte_size(old_collector_xfer));\n+}\n+\n+\n+\/\/ Overwrite arguments to represent the amount of memory in each generation that is about to be recycled\n+void ShenandoahFreeSet::prepare_to_rebuild(size_t &young_cset_regions, size_t &old_cset_regions,\n+                                           size_t &first_old_region, size_t &last_old_region, size_t &old_region_count) {\n+  shenandoah_assert_heaplocked();\n+  \/\/ This resets all state information, removing all regions from all sets.\n+  clear();\n+  log_debug(gc, free)(\"Rebuilding FreeSet\");\n+\n+  \/\/ This places regions that have alloc_capacity into the old_collector set if they identify as is_old() or the\n+  \/\/ mutator set otherwise.  All trashed (cset) regions are affiliated young and placed in mutator set.\n+  find_regions_with_alloc_capacity(young_cset_regions, old_cset_regions, first_old_region, last_old_region, old_region_count);\n+}\n+\n+void ShenandoahFreeSet::establish_generation_sizes(size_t young_region_count, size_t old_region_count) {\n+  assert(young_region_count + old_region_count == ShenandoahHeap::heap()->num_regions(), \"Sanity\");\n+  if (ShenandoahHeap::heap()->mode()->is_generational()) {\n+    ShenandoahGenerationalHeap* heap = ShenandoahGenerationalHeap::heap();\n+    ShenandoahOldGeneration* old_gen = heap->old_generation();\n+    ShenandoahYoungGeneration* young_gen = heap->young_generation();\n+    size_t region_size_bytes = ShenandoahHeapRegion::region_size_bytes();\n+\n+    size_t original_old_capacity = old_gen->max_capacity();\n+    size_t new_old_capacity = old_region_count * region_size_bytes;\n+    size_t new_young_capacity = young_region_count * region_size_bytes;\n+    old_gen->set_capacity(new_old_capacity);\n+    young_gen->set_capacity(new_young_capacity);\n+\n+    if (new_old_capacity > original_old_capacity) {\n+      size_t region_count = (new_old_capacity - original_old_capacity) \/ region_size_bytes;\n+      log_info(gc)(\"Transfer \" SIZE_FORMAT \" region(s) from %s to %s, yielding increased size: \" PROPERFMT,\n+                   region_count, young_gen->name(), old_gen->name(), PROPERFMTARGS(new_old_capacity));\n+    } else if (new_old_capacity < original_old_capacity) {\n+      size_t region_count = (original_old_capacity - new_old_capacity) \/ region_size_bytes;\n+      log_info(gc)(\"Transfer \" SIZE_FORMAT \" region(s) from %s to %s, yielding increased size: \" PROPERFMT,\n+                   region_count, old_gen->name(), young_gen->name(), PROPERFMTARGS(new_young_capacity));\n@@ -440,0 +1583,16 @@\n+    \/\/ This balances generations, so clear any pending request to balance.\n+    old_gen->set_region_balance(0);\n+  }\n+}\n+\n+void ShenandoahFreeSet::finish_rebuild(size_t young_cset_regions, size_t old_cset_regions, size_t old_region_count,\n+                                       bool have_evacuation_reserves) {\n+  shenandoah_assert_heaplocked();\n+  size_t young_reserve(0), old_reserve(0);\n+\n+  if (_heap->mode()->is_generational()) {\n+    compute_young_and_old_reserves(young_cset_regions, old_cset_regions, have_evacuation_reserves,\n+                                   young_reserve, old_reserve);\n+  } else {\n+    young_reserve = (_heap->max_capacity() \/ 100) * ShenandoahEvacReserve;\n+    old_reserve = 0;\n@@ -442,2 +1601,202 @@\n-  recompute_bounds();\n-  assert_bounds();\n+  \/\/ Move some of the mutator regions in the Collector and OldCollector partitions in order to satisfy\n+  \/\/ young_reserve and old_reserve.\n+  reserve_regions(young_reserve, old_reserve, old_region_count);\n+  size_t young_region_count = _heap->num_regions() - old_region_count;\n+  establish_generation_sizes(young_region_count, old_region_count);\n+  establish_old_collector_alloc_bias();\n+  _partitions.assert_bounds();\n+  log_status();\n+}\n+\n+void ShenandoahFreeSet::compute_young_and_old_reserves(size_t young_cset_regions, size_t old_cset_regions,\n+                                                       bool have_evacuation_reserves,\n+                                                       size_t& young_reserve_result, size_t& old_reserve_result) const {\n+  shenandoah_assert_generational();\n+  const size_t region_size_bytes = ShenandoahHeapRegion::region_size_bytes();\n+\n+  ShenandoahOldGeneration* const old_generation = _heap->old_generation();\n+  size_t old_available = old_generation->available();\n+  size_t old_unaffiliated_regions = old_generation->free_unaffiliated_regions();\n+  ShenandoahYoungGeneration* const young_generation = _heap->young_generation();\n+  size_t young_capacity = young_generation->max_capacity();\n+  size_t young_unaffiliated_regions = young_generation->free_unaffiliated_regions();\n+\n+  \/\/ Add in the regions we anticipate to be freed by evacuation of the collection set\n+  old_unaffiliated_regions += old_cset_regions;\n+  young_unaffiliated_regions += young_cset_regions;\n+\n+  \/\/ Consult old-region balance to make adjustments to current generation capacities and availability.\n+  \/\/ The generation region transfers take place after we rebuild.\n+  const ssize_t old_region_balance = old_generation->get_region_balance();\n+  if (old_region_balance != 0) {\n+#ifdef ASSERT\n+    if (old_region_balance > 0) {\n+      assert(old_region_balance <= checked_cast<ssize_t>(old_unaffiliated_regions), \"Cannot transfer regions that are affiliated\");\n+    } else {\n+      assert(0 - old_region_balance <= checked_cast<ssize_t>(young_unaffiliated_regions), \"Cannot transfer regions that are affiliated\");\n+    }\n+#endif\n+\n+    ssize_t xfer_bytes = old_region_balance * checked_cast<ssize_t>(region_size_bytes);\n+    old_available -= xfer_bytes;\n+    old_unaffiliated_regions -= old_region_balance;\n+    young_capacity += xfer_bytes;\n+    young_unaffiliated_regions += old_region_balance;\n+  }\n+\n+  \/\/ All allocations taken from the old collector set are performed by GC, generally using PLABs for both\n+  \/\/ promotions and evacuations.  The partition between which old memory is reserved for evacuation and\n+  \/\/ which is reserved for promotion is enforced using thread-local variables that prescribe intentions for\n+  \/\/ each PLAB's available memory.\n+  if (have_evacuation_reserves) {\n+    \/\/ We are rebuilding at the end of final mark, having already established evacuation budgets for this GC pass.\n+    const size_t promoted_reserve = old_generation->get_promoted_reserve();\n+    const size_t old_evac_reserve = old_generation->get_evacuation_reserve();\n+    young_reserve_result = young_generation->get_evacuation_reserve();\n+    old_reserve_result = promoted_reserve + old_evac_reserve;\n+    assert(old_reserve_result <= old_available,\n+           \"Cannot reserve (\" SIZE_FORMAT \" + \" SIZE_FORMAT\") more OLD than is available: \" SIZE_FORMAT,\n+           promoted_reserve, old_evac_reserve, old_available);\n+  } else {\n+    \/\/ We are rebuilding at end of GC, so we set aside budgets specified on command line (or defaults)\n+    young_reserve_result = (young_capacity * ShenandoahEvacReserve) \/ 100;\n+    \/\/ The auto-sizer has already made old-gen large enough to hold all anticipated evacuations and promotions.\n+    \/\/ Affiliated old-gen regions are already in the OldCollector free set.  Add in the relevant number of\n+    \/\/ unaffiliated regions.\n+    old_reserve_result = old_available;\n+  }\n+\n+  \/\/ Old available regions that have less than PLAB::min_size() of available memory are not placed into the OldCollector\n+  \/\/ free set.  Because of this, old_available may not have enough memory to represent the intended reserve.  Adjust\n+  \/\/ the reserve downward to account for this possibility. This loss is part of the reason why the original budget\n+  \/\/ was adjusted with ShenandoahOldEvacWaste and ShenandoahOldPromoWaste multipliers.\n+  if (old_reserve_result >\n+      _partitions.capacity_of(ShenandoahFreeSetPartitionId::OldCollector) + old_unaffiliated_regions * region_size_bytes) {\n+    old_reserve_result =\n+      _partitions.capacity_of(ShenandoahFreeSetPartitionId::OldCollector) + old_unaffiliated_regions * region_size_bytes;\n+  }\n+\n+  if (young_reserve_result > young_unaffiliated_regions * region_size_bytes) {\n+    young_reserve_result = young_unaffiliated_regions * region_size_bytes;\n+  }\n+}\n+\n+\/\/ Having placed all regions that have allocation capacity into the mutator set if they identify as is_young()\n+\/\/ or into the old collector set if they identify as is_old(), move some of these regions from the mutator set\n+\/\/ into the collector set or old collector set in order to assure that the memory available for allocations within\n+\/\/ the collector set is at least to_reserve and the memory available for allocations within the old collector set\n+\/\/ is at least to_reserve_old.\n+void ShenandoahFreeSet::reserve_regions(size_t to_reserve, size_t to_reserve_old, size_t &old_region_count) {\n+  for (size_t i = _heap->num_regions(); i > 0; i--) {\n+    size_t idx = i - 1;\n+    ShenandoahHeapRegion* r = _heap->get_region(idx);\n+    if (!_partitions.in_free_set(ShenandoahFreeSetPartitionId::Mutator, idx)) {\n+      continue;\n+    }\n+\n+    size_t ac = alloc_capacity(r);\n+    assert (ac > 0, \"Membership in free set implies has capacity\");\n+    assert (!r->is_old() || r->is_trash(), \"Except for trash, mutator_is_free regions should not be affiliated OLD\");\n+\n+    bool move_to_old_collector = _partitions.available_in(ShenandoahFreeSetPartitionId::OldCollector) < to_reserve_old;\n+    bool move_to_collector = _partitions.available_in(ShenandoahFreeSetPartitionId::Collector) < to_reserve;\n+\n+    if (!move_to_collector && !move_to_old_collector) {\n+      \/\/ We've satisfied both to_reserve and to_reserved_old\n+      break;\n+    }\n+\n+    if (move_to_old_collector) {\n+      \/\/ We give priority to OldCollector partition because we desire to pack OldCollector regions into higher\n+      \/\/ addresses than Collector regions.  Presumably, OldCollector regions are more \"stable\" and less likely to\n+      \/\/ be collected in the near future.\n+      if (r->is_trash() || !r->is_affiliated()) {\n+        \/\/ OLD regions that have available memory are already in the old_collector free set.\n+        _partitions.move_from_partition_to_partition(idx, ShenandoahFreeSetPartitionId::Mutator,\n+                                                     ShenandoahFreeSetPartitionId::OldCollector, ac);\n+        log_debug(gc)(\"  Shifting region \" SIZE_FORMAT \" from mutator_free to old_collector_free\", idx);\n+        log_debug(gc)(\"  Shifted Mutator range [\" SSIZE_FORMAT \", \" SSIZE_FORMAT \"],\"\n+                      \"  Old Collector range [\" SSIZE_FORMAT \", \" SSIZE_FORMAT \"]\",\n+                      _partitions.leftmost(ShenandoahFreeSetPartitionId::Mutator),\n+                      _partitions.rightmost(ShenandoahFreeSetPartitionId::Mutator),\n+                      _partitions.leftmost(ShenandoahFreeSetPartitionId::OldCollector),\n+                      _partitions.rightmost(ShenandoahFreeSetPartitionId::OldCollector));\n+        old_region_count++;\n+        continue;\n+      }\n+    }\n+\n+    if (move_to_collector) {\n+      \/\/ Note: In a previous implementation, regions were only placed into the survivor space (collector_is_free) if\n+      \/\/ they were entirely empty.  This has the effect of causing new Mutator allocation to reside next to objects\n+      \/\/ that have already survived at least one GC, mixing ephemeral with longer-lived objects in the same region.\n+      \/\/ Any objects that have survived a GC are less likely to immediately become garbage, so a region that contains\n+      \/\/ survivor objects is less likely to be selected for the collection set.  This alternative implementation allows\n+      \/\/ survivor regions to continue accumulating other survivor objects, and makes it more likely that ephemeral objects\n+      \/\/ occupy regions comprised entirely of ephemeral objects.  These regions are highly likely to be included in the next\n+      \/\/ collection set, and they are easily evacuated because they have low density of live objects.\n+      _partitions.move_from_partition_to_partition(idx, ShenandoahFreeSetPartitionId::Mutator,\n+                                                   ShenandoahFreeSetPartitionId::Collector, ac);\n+      log_debug(gc)(\"  Shifting region \" SIZE_FORMAT \" from mutator_free to collector_free\", idx);\n+      log_debug(gc)(\"  Shifted Mutator range [\" SSIZE_FORMAT \", \" SSIZE_FORMAT \"],\"\n+                    \"  Collector range [\" SSIZE_FORMAT \", \" SSIZE_FORMAT \"]\",\n+                    _partitions.leftmost(ShenandoahFreeSetPartitionId::Mutator),\n+                    _partitions.rightmost(ShenandoahFreeSetPartitionId::Mutator),\n+                    _partitions.leftmost(ShenandoahFreeSetPartitionId::Collector),\n+                    _partitions.rightmost(ShenandoahFreeSetPartitionId::Collector));\n+    }\n+  }\n+\n+  if (LogTarget(Info, gc, free)::is_enabled()) {\n+    size_t old_reserve = _partitions.available_in(ShenandoahFreeSetPartitionId::OldCollector);\n+    if (old_reserve < to_reserve_old) {\n+      log_info(gc, free)(\"Wanted \" PROPERFMT \" for old reserve, but only reserved: \" PROPERFMT,\n+                         PROPERFMTARGS(to_reserve_old), PROPERFMTARGS(old_reserve));\n+    }\n+    size_t reserve = _partitions.available_in(ShenandoahFreeSetPartitionId::Collector);\n+    if (reserve < to_reserve) {\n+      log_debug(gc)(\"Wanted \" PROPERFMT \" for young reserve, but only reserved: \" PROPERFMT,\n+                    PROPERFMTARGS(to_reserve), PROPERFMTARGS(reserve));\n+    }\n+  }\n+}\n+\n+void ShenandoahFreeSet::establish_old_collector_alloc_bias() {\n+  ShenandoahHeap* heap = ShenandoahHeap::heap();\n+  shenandoah_assert_heaplocked();\n+\n+  idx_t left_idx = _partitions.leftmost(ShenandoahFreeSetPartitionId::OldCollector);\n+  idx_t right_idx = _partitions.rightmost(ShenandoahFreeSetPartitionId::OldCollector);\n+  idx_t middle = (left_idx + right_idx) \/ 2;\n+  size_t available_in_first_half = 0;\n+  size_t available_in_second_half = 0;\n+\n+  for (idx_t index = left_idx; index < middle; index++) {\n+    if (_partitions.in_free_set(ShenandoahFreeSetPartitionId::OldCollector, index)) {\n+      ShenandoahHeapRegion* r = heap->get_region((size_t) index);\n+      available_in_first_half += r->free();\n+    }\n+  }\n+  for (idx_t index = middle; index <= right_idx; index++) {\n+    if (_partitions.in_free_set(ShenandoahFreeSetPartitionId::OldCollector, index)) {\n+      ShenandoahHeapRegion* r = heap->get_region(index);\n+      available_in_second_half += r->free();\n+    }\n+  }\n+\n+  \/\/ We desire to first consume the sparsely distributed regions in order that the remaining regions are densely packed.\n+  \/\/ Densely packing regions reduces the effort to search for a region that has sufficient memory to satisfy a new allocation\n+  \/\/ request.  Regions become sparsely distributed following a Full GC, which tends to slide all regions to the front of the\n+  \/\/ heap rather than allowing survivor regions to remain at the high end of the heap where we intend for them to congregate.\n+  _partitions.set_bias_from_left_to_right(ShenandoahFreeSetPartitionId::OldCollector,\n+                                          (available_in_second_half > available_in_first_half));\n+}\n+\n+void ShenandoahFreeSet::log_status_under_lock() {\n+  \/\/ Must not be heap locked, it acquires heap lock only when log is enabled\n+  shenandoah_assert_not_heaplocked();\n+  if (LogTarget(Info, gc, free)::is_enabled()\n+      DEBUG_ONLY(|| LogTarget(Debug, gc, free)::is_enabled())) {\n+    ShenandoahHeapLocker locker(_heap->lock());\n+    log_status();\n+  }\n@@ -449,1 +1808,94 @@\n-  LogTarget(Info, gc, ergo) lt;\n+#ifdef ASSERT\n+  \/\/ Dump of the FreeSet details is only enabled if assertions are enabled\n+  if (LogTarget(Debug, gc, free)::is_enabled()) {\n+#define BUFFER_SIZE 80\n+    size_t retired_old = 0;\n+    size_t retired_old_humongous = 0;\n+    size_t retired_young = 0;\n+    size_t retired_young_humongous = 0;\n+    size_t region_size_bytes = ShenandoahHeapRegion::region_size_bytes();\n+    size_t retired_young_waste = 0;\n+    size_t retired_old_waste = 0;\n+    size_t consumed_collector = 0;\n+    size_t consumed_old_collector = 0;\n+    size_t consumed_mutator = 0;\n+    size_t available_old = 0;\n+    size_t available_young = 0;\n+    size_t available_mutator = 0;\n+    size_t available_collector = 0;\n+    size_t available_old_collector = 0;\n+\n+    char buffer[BUFFER_SIZE];\n+    for (uint i = 0; i < BUFFER_SIZE; i++) {\n+      buffer[i] = '\\0';\n+    }\n+\n+    log_debug(gc)(\"FreeSet map legend:\"\n+                       \" M:mutator_free C:collector_free O:old_collector_free\"\n+                       \" H:humongous ~:retired old _:retired young\");\n+    log_debug(gc)(\" mutator free range [\" SIZE_FORMAT \"..\" SIZE_FORMAT \"] allocating from %s, \"\n+                  \" collector free range [\" SIZE_FORMAT \"..\" SIZE_FORMAT \"], \"\n+                  \"old collector free range [\" SIZE_FORMAT \"..\" SIZE_FORMAT \"] allocates from %s\",\n+                  _partitions.leftmost(ShenandoahFreeSetPartitionId::Mutator),\n+                  _partitions.rightmost(ShenandoahFreeSetPartitionId::Mutator),\n+                  _partitions.alloc_from_left_bias(ShenandoahFreeSetPartitionId::Mutator)? \"left to right\": \"right to left\",\n+                  _partitions.leftmost(ShenandoahFreeSetPartitionId::Collector),\n+                  _partitions.rightmost(ShenandoahFreeSetPartitionId::Collector),\n+                  _partitions.leftmost(ShenandoahFreeSetPartitionId::OldCollector),\n+                  _partitions.rightmost(ShenandoahFreeSetPartitionId::OldCollector),\n+                  _partitions.alloc_from_left_bias(ShenandoahFreeSetPartitionId::OldCollector)? \"left to right\": \"right to left\");\n+\n+    for (uint i = 0; i < _heap->num_regions(); i++) {\n+      ShenandoahHeapRegion *r = _heap->get_region(i);\n+      uint idx = i % 64;\n+      if ((i != 0) && (idx == 0)) {\n+        log_debug(gc)(\" %6u: %s\", i-64, buffer);\n+      }\n+      if (_partitions.in_free_set(ShenandoahFreeSetPartitionId::Mutator, i)) {\n+        size_t capacity = alloc_capacity(r);\n+        assert(!r->is_old() || r->is_trash(), \"Old regions except trash regions should not be in mutator_free set\");\n+        available_mutator += capacity;\n+        consumed_mutator += region_size_bytes - capacity;\n+        buffer[idx] = (capacity == region_size_bytes)? 'M': 'm';\n+      } else if (_partitions.in_free_set(ShenandoahFreeSetPartitionId::Collector, i)) {\n+        size_t capacity = alloc_capacity(r);\n+        assert(!r->is_old() || r->is_trash(), \"Old regions except trash regions should not be in collector_free set\");\n+        available_collector += capacity;\n+        consumed_collector += region_size_bytes - capacity;\n+        buffer[idx] = (capacity == region_size_bytes)? 'C': 'c';\n+      } else if (_partitions.in_free_set(ShenandoahFreeSetPartitionId::OldCollector, i)) {\n+        size_t capacity = alloc_capacity(r);\n+        available_old_collector += capacity;\n+        consumed_old_collector += region_size_bytes - capacity;\n+        buffer[idx] = (capacity == region_size_bytes)? 'O': 'o';\n+      } else if (r->is_humongous()) {\n+        if (r->is_old()) {\n+          buffer[idx] = 'H';\n+          retired_old_humongous += region_size_bytes;\n+        } else {\n+          buffer[idx] = 'h';\n+          retired_young_humongous += region_size_bytes;\n+        }\n+      } else {\n+        if (r->is_old()) {\n+          buffer[idx] = '~';\n+          retired_old_waste += alloc_capacity(r);\n+          retired_old += region_size_bytes;\n+        } else {\n+          buffer[idx] = '_';\n+          retired_young_waste += alloc_capacity(r);\n+          retired_young += region_size_bytes;\n+        }\n+      }\n+    }\n+    uint remnant = _heap->num_regions() % 64;\n+    if (remnant > 0) {\n+      buffer[remnant] = '\\0';\n+    } else {\n+      remnant = 64;\n+    }\n+    log_debug(gc)(\" %6u: %s\", (uint) (_heap->num_regions() - remnant), buffer);\n+  }\n+#endif\n+\n+  LogTarget(Info, gc, free) lt;\n@@ -455,1 +1907,1 @@\n-      size_t last_idx = 0;\n+      idx_t last_idx = 0;\n@@ -464,2 +1916,3 @@\n-      for (size_t idx = _mutator_leftmost; idx <= _mutator_rightmost; idx++) {\n-        if (is_mutator_free(idx)) {\n+      for (idx_t idx = _partitions.leftmost(ShenandoahFreeSetPartitionId::Mutator);\n+           idx <= _partitions.rightmost(ShenandoahFreeSetPartitionId::Mutator); idx++) {\n+        if (_partitions.in_free_set(ShenandoahFreeSetPartitionId::Mutator, idx)) {\n@@ -468,1 +1921,0 @@\n-\n@@ -470,1 +1922,0 @@\n-\n@@ -481,1 +1932,0 @@\n-\n@@ -484,1 +1934,0 @@\n-\n@@ -493,0 +1942,4 @@\n+      \/\/ Since certain regions that belonged to the Mutator free partition at the time of most recent rebuild may have been\n+      \/\/ retired, the sum of used and capacities within regions that are still in the Mutator free partition may not match\n+      \/\/ my internally tracked values of used() and free().\n+      assert(free == total_free, \"Free memory should match\");\n@@ -509,2 +1962,3 @@\n-      if (mutator_count() > 0) {\n-        frag_int = (100 * (total_used \/ mutator_count()) \/ ShenandoahHeapRegion::region_size_bytes());\n+      if (_partitions.count(ShenandoahFreeSetPartitionId::Mutator) > 0) {\n+        frag_int = (100 * (total_used \/ _partitions.count(ShenandoahFreeSetPartitionId::Mutator))\n+                    \/ ShenandoahHeapRegion::region_size_bytes());\n@@ -515,0 +1969,3 @@\n+      ls.print(\"Used: \" SIZE_FORMAT \"%s, Mutator Free: \" SIZE_FORMAT,\n+               byte_size_in_proper_unit(total_used), proper_unit_for_byte_size(total_used),\n+               _partitions.count(ShenandoahFreeSetPartitionId::Mutator));\n@@ -520,0 +1977,1 @@\n+      size_t total_used = 0;\n@@ -521,2 +1979,3 @@\n-      for (size_t idx = _collector_leftmost; idx <= _collector_rightmost; idx++) {\n-        if (is_collector_free(idx)) {\n+      for (idx_t idx = _partitions.leftmost(ShenandoahFreeSetPartitionId::Collector);\n+           idx <= _partitions.rightmost(ShenandoahFreeSetPartitionId::Collector); idx++) {\n+        if (_partitions.in_free_set(ShenandoahFreeSetPartitionId::Collector, idx)) {\n@@ -527,0 +1986,1 @@\n+          total_used += r->used();\n@@ -529,0 +1989,5 @@\n+      ls.print(\" Collector Reserve: \" SIZE_FORMAT \"%s, Max: \" SIZE_FORMAT \"%s; Used: \" SIZE_FORMAT \"%s\",\n+               byte_size_in_proper_unit(total_free), proper_unit_for_byte_size(total_free),\n+               byte_size_in_proper_unit(max),        proper_unit_for_byte_size(max),\n+               byte_size_in_proper_unit(total_used), proper_unit_for_byte_size(total_used));\n+    }\n@@ -530,1 +1995,16 @@\n-      ls.print_cr(\"Reserve: \" SIZE_FORMAT \"%s, Max: \" SIZE_FORMAT \"%s\",\n+    if (_heap->mode()->is_generational()) {\n+      size_t max = 0;\n+      size_t total_free = 0;\n+      size_t total_used = 0;\n+\n+      for (idx_t idx = _partitions.leftmost(ShenandoahFreeSetPartitionId::OldCollector);\n+           idx <= _partitions.rightmost(ShenandoahFreeSetPartitionId::OldCollector); idx++) {\n+        if (_partitions.in_free_set(ShenandoahFreeSetPartitionId::OldCollector, idx)) {\n+          ShenandoahHeapRegion *r = _heap->get_region(idx);\n+          size_t free = alloc_capacity(r);\n+          max = MAX2(max, free);\n+          total_free += free;\n+          total_used += r->used();\n+        }\n+      }\n+      ls.print_cr(\" Old Collector Reserve: \" SIZE_FORMAT \"%s, Max: \" SIZE_FORMAT \"%s; Used: \" SIZE_FORMAT \"%s\",\n@@ -532,1 +2012,2 @@\n-                  byte_size_in_proper_unit(max),        proper_unit_for_byte_size(max));\n+                  byte_size_in_proper_unit(max),        proper_unit_for_byte_size(max),\n+                  byte_size_in_proper_unit(total_used), proper_unit_for_byte_size(total_used));\n@@ -539,3 +2020,1 @@\n-  assert_bounds();\n-\n-  if (req.size() > ShenandoahHeapRegion::humongous_threshold_words()) {\n+  if (ShenandoahHeapRegion::requires_humongous(req.size())) {\n@@ -547,0 +2026,1 @@\n+      case ShenandoahAllocRequest::_alloc_plab:\n@@ -550,2 +2030,1 @@\n-        assert(false, \"Trying to allocate TLAB larger than the humongous threshold: \" SIZE_FORMAT \" > \" SIZE_FORMAT,\n-               req.size(), ShenandoahHeapRegion::humongous_threshold_words());\n+        assert(false, \"Trying to allocate TLAB in humongous region: \" SIZE_FORMAT, req.size());\n@@ -562,16 +2041,0 @@\n-size_t ShenandoahFreeSet::unsafe_peek_free() const {\n-  \/\/ Deliberately not locked, this method is unsafe when free set is modified.\n-\n-  for (size_t index = _mutator_leftmost; index <= _mutator_rightmost; index++) {\n-    if (index < _max && is_mutator_free(index)) {\n-      ShenandoahHeapRegion* r = _heap->get_region(index);\n-      if (r->free() >= MinTLABSize) {\n-        return r->free();\n-      }\n-    }\n-  }\n-\n-  \/\/ It appears that no regions left\n-  return 0;\n-}\n-\n@@ -579,5 +2042,15 @@\n-  out->print_cr(\"Mutator Free Set: \" SIZE_FORMAT \"\", mutator_count());\n-  for (size_t index = _mutator_leftmost; index <= _mutator_rightmost; index++) {\n-    if (is_mutator_free(index)) {\n-      _heap->get_region(index)->print_on(out);\n-    }\n+  out->print_cr(\"Mutator Free Set: \" SIZE_FORMAT \"\", _partitions.count(ShenandoahFreeSetPartitionId::Mutator));\n+  idx_t rightmost = _partitions.rightmost(ShenandoahFreeSetPartitionId::Mutator);\n+  for (idx_t index = _partitions.leftmost(ShenandoahFreeSetPartitionId::Mutator); index <= rightmost; ) {\n+    assert(_partitions.in_free_set(ShenandoahFreeSetPartitionId::Mutator, index),\n+           \"Boundaries or find_first_set_bit failed: \" SSIZE_FORMAT, index);\n+    _heap->get_region(index)->print_on(out);\n+    index = _partitions.find_index_of_next_available_region(ShenandoahFreeSetPartitionId::Mutator, index + 1);\n+  }\n+  out->print_cr(\"Collector Free Set: \" SIZE_FORMAT \"\", _partitions.count(ShenandoahFreeSetPartitionId::Collector));\n+  rightmost = _partitions.rightmost(ShenandoahFreeSetPartitionId::Collector);\n+  for (idx_t index = _partitions.leftmost(ShenandoahFreeSetPartitionId::Collector); index <= rightmost; ) {\n+    assert(_partitions.in_free_set(ShenandoahFreeSetPartitionId::Collector, index),\n+           \"Boundaries or find_first_set_bit failed: \" SSIZE_FORMAT, index);\n+    _heap->get_region(index)->print_on(out);\n+    index = _partitions.find_index_of_next_available_region(ShenandoahFreeSetPartitionId::Collector, index + 1);\n@@ -585,4 +2058,7 @@\n-  out->print_cr(\"Collector Free Set: \" SIZE_FORMAT \"\", collector_count());\n-  for (size_t index = _collector_leftmost; index <= _collector_rightmost; index++) {\n-    if (is_collector_free(index)) {\n-      _heap->get_region(index)->print_on(out);\n+  if (_heap->mode()->is_generational()) {\n+    out->print_cr(\"Old Collector Free Set: \" SIZE_FORMAT \"\", _partitions.count(ShenandoahFreeSetPartitionId::OldCollector));\n+    for (idx_t index = _partitions.leftmost(ShenandoahFreeSetPartitionId::OldCollector);\n+         index <= _partitions.rightmost(ShenandoahFreeSetPartitionId::OldCollector); index++) {\n+      if (_partitions.in_free_set(ShenandoahFreeSetPartitionId::OldCollector, index)) {\n+        _heap->get_region(index)->print_on(out);\n+      }\n@@ -593,21 +2069,0 @@\n-\/*\n- * Internal fragmentation metric: describes how fragmented the heap regions are.\n- *\n- * It is derived as:\n- *\n- *               sum(used[i]^2, i=0..k)\n- *   IF = 1 - ------------------------------\n- *              C * sum(used[i], i=0..k)\n- *\n- * ...where k is the number of regions in computation, C is the region capacity, and\n- * used[i] is the used space in the region.\n- *\n- * The non-linearity causes IF to be lower for the cases where the same total heap\n- * used is densely packed. For example:\n- *   a) Heap is completely full  => IF = 0\n- *   b) Heap is half full, first 50% regions are completely full => IF = 0\n- *   c) Heap is half full, each region is 50% full => IF = 1\/2\n- *   d) Heap is quarter full, first 50% regions are completely full => IF = 0\n- *   e) Heap is quarter full, each region is 25% full => IF = 3\/4\n- *   f) Heap has one small object per each region => IF =~ 1\n- *\/\n@@ -618,0 +2073,1 @@\n+<<<<<<< HEAD\n@@ -625,0 +2081,11 @@\n+=======\n+  idx_t rightmost = _partitions.rightmost(ShenandoahFreeSetPartitionId::Mutator);\n+  for (idx_t index = _partitions.leftmost(ShenandoahFreeSetPartitionId::Mutator); index <= rightmost; ) {\n+    assert(_partitions.in_free_set(ShenandoahFreeSetPartitionId::Mutator, index),\n+           \"Boundaries or find_first_set_bit failed: \" SSIZE_FORMAT, index);\n+    ShenandoahHeapRegion* r = _heap->get_region(index);\n+    size_t used = r->used();\n+    squared += used * used;\n+    linear += used;\n+    index = _partitions.find_index_of_next_available_region(ShenandoahFreeSetPartitionId::Mutator, index + 1);\n+>>>>>>> c0ea97ad77a1e80ae96cecb9b9ddb672f029b1f1\n@@ -635,13 +2102,0 @@\n-\/*\n- * External fragmentation metric: describes how fragmented the heap is.\n- *\n- * It is derived as:\n- *\n- *   EF = 1 - largest_contiguous_free \/ total_free\n- *\n- * For example:\n- *   a) Heap is completely empty => EF = 0\n- *   b) Heap is completely full => EF = 0\n- *   c) Heap is first-half full => EF = 1\/2\n- *   d) Heap is half full, full and empty regions interleave => EF =~ 1\n- *\/\n@@ -649,1 +2103,1 @@\n-  size_t last_idx = 0;\n+  idx_t last_idx = 0;\n@@ -655,10 +2109,9 @@\n-  for (size_t index = _mutator_leftmost; index <= _mutator_rightmost; index++) {\n-    if (is_mutator_free(index)) {\n-      ShenandoahHeapRegion* r = _heap->get_region(index);\n-      if (r->is_empty()) {\n-        free += ShenandoahHeapRegion::region_size_bytes();\n-        if (last_idx + 1 == index) {\n-          empty_contig++;\n-        } else {\n-          empty_contig = 1;\n-        }\n+  idx_t rightmost = _partitions.rightmost(ShenandoahFreeSetPartitionId::Mutator);\n+  for (idx_t index = _partitions.leftmost(ShenandoahFreeSetPartitionId::Mutator); index <= rightmost; ) {\n+    assert(_partitions.in_free_set(ShenandoahFreeSetPartitionId::Mutator, index),\n+           \"Boundaries or find_first_set_bit failed: \" SSIZE_FORMAT, index);\n+    ShenandoahHeapRegion* r = _heap->get_region(index);\n+    if (r->is_empty()) {\n+      free += ShenandoahHeapRegion::region_size_bytes();\n+      if (last_idx + 1 == index) {\n+        empty_contig++;\n@@ -666,1 +2119,1 @@\n-        empty_contig = 0;\n+        empty_contig = 1;\n@@ -668,3 +2121,2 @@\n-\n-      max_contig = MAX2(max_contig, empty_contig);\n-      last_idx = index;\n+    } else {\n+      empty_contig = 0;\n@@ -672,0 +2124,3 @@\n+    max_contig = MAX2(max_contig, empty_contig);\n+    last_idx = index;\n+    index = _partitions.find_index_of_next_available_region(ShenandoahFreeSetPartitionId::Mutator, index + 1);\n@@ -681,27 +2136,0 @@\n-#ifdef ASSERT\n-void ShenandoahFreeSet::assert_bounds() const {\n-  \/\/ Performance invariants. Failing these would not break the free set, but performance\n-  \/\/ would suffer.\n-  assert (_mutator_leftmost <= _max, \"leftmost in bounds: \"  SIZE_FORMAT \" < \" SIZE_FORMAT, _mutator_leftmost,  _max);\n-  assert (_mutator_rightmost < _max, \"rightmost in bounds: \" SIZE_FORMAT \" < \" SIZE_FORMAT, _mutator_rightmost, _max);\n-\n-  assert (_mutator_leftmost == _max || is_mutator_free(_mutator_leftmost),  \"leftmost region should be free: \" SIZE_FORMAT,  _mutator_leftmost);\n-  assert (_mutator_rightmost == 0   || is_mutator_free(_mutator_rightmost), \"rightmost region should be free: \" SIZE_FORMAT, _mutator_rightmost);\n-\n-  size_t beg_off = _mutator_free_bitmap.find_first_set_bit(0);\n-  size_t end_off = _mutator_free_bitmap.find_first_set_bit(_mutator_rightmost + 1);\n-  assert (beg_off >= _mutator_leftmost, \"free regions before the leftmost: \" SIZE_FORMAT \", bound \" SIZE_FORMAT, beg_off, _mutator_leftmost);\n-  assert (end_off == _max,      \"free regions past the rightmost: \" SIZE_FORMAT \", bound \" SIZE_FORMAT,  end_off, _mutator_rightmost);\n-\n-  assert (_collector_leftmost <= _max, \"leftmost in bounds: \"  SIZE_FORMAT \" < \" SIZE_FORMAT, _collector_leftmost,  _max);\n-  assert (_collector_rightmost < _max, \"rightmost in bounds: \" SIZE_FORMAT \" < \" SIZE_FORMAT, _collector_rightmost, _max);\n-\n-  assert (_collector_leftmost == _max || is_collector_free(_collector_leftmost),  \"leftmost region should be free: \" SIZE_FORMAT,  _collector_leftmost);\n-  assert (_collector_rightmost == 0   || is_collector_free(_collector_rightmost), \"rightmost region should be free: \" SIZE_FORMAT, _collector_rightmost);\n-\n-  beg_off = _collector_free_bitmap.find_first_set_bit(0);\n-  end_off = _collector_free_bitmap.find_first_set_bit(_collector_rightmost + 1);\n-  assert (beg_off >= _collector_leftmost, \"free regions before the leftmost: \" SIZE_FORMAT \", bound \" SIZE_FORMAT, beg_off, _collector_leftmost);\n-  assert (end_off == _max,      \"free regions past the rightmost: \" SIZE_FORMAT \", bound \" SIZE_FORMAT,  end_off, _collector_rightmost);\n-}\n-#endif\n","filename":"src\/hotspot\/share\/gc\/shenandoah\/shenandoahFreeSet.cpp","additions":1774,"deletions":346,"binary":false,"changes":2120,"status":"modified"}]}