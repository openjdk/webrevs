{"files":[{"patch":"@@ -75,4 +75,3 @@\n-bool AgeTable::is_clear() {\n-  size_t total = 0;\n-  for (size_t* p = sizes; p < sizes + table_size; ++p) {\n-    total += *p;\n+bool AgeTable::is_clear() const {\n+  for (const size_t* p = sizes; p < sizes + table_size; ++p) {\n+    if (*p != 0) return false;\n@@ -80,1 +79,1 @@\n-  return total == 0;\n+  return true;\n","filename":"src\/hotspot\/share\/gc\/shared\/ageTable.cpp","additions":4,"deletions":5,"binary":false,"changes":9,"status":"modified"},{"patch":"@@ -56,0 +56,2 @@\n+\n+#ifndef PRODUCT\n@@ -57,1 +59,2 @@\n-  bool is_clear() PRODUCT_RETURN0;\n+  bool is_clear() const;\n+#endif \/\/ !PRODUCT\n","filename":"src\/hotspot\/share\/gc\/shared\/ageTable.hpp","additions":4,"deletions":1,"binary":false,"changes":5,"status":"modified"},{"patch":"@@ -389,0 +389,6 @@\n+        \"array_partition_stub\",\n+        { { TypeFunc::Parms, ShenandoahStore }, { TypeFunc::Parms+4, ShenandoahStore },   { -1, ShenandoahNone },\n+          { -1, ShenandoahNone },                { -1, ShenandoahNone },                  { -1, ShenandoahNone } },\n+        \"arraysort_stub\",\n+        { { TypeFunc::Parms, ShenandoahStore },  { -1, ShenandoahNone },                  { -1, ShenandoahNone },\n+          { -1,  ShenandoahNone},                 { -1,  ShenandoahNone},                 { -1,  ShenandoahNone} },\n@@ -2171,1 +2177,1 @@\n-              assert(c->is_Loop() && j == LoopNode::LoopBackControl || _phase->C->has_irreducible_loop() || has_never_branch(_phase->C->root()), \"\");\n+              assert((c->is_Loop() && j == LoopNode::LoopBackControl) || _phase->C->has_irreducible_loop() || has_never_branch(_phase->C->root()), \"\");\n","filename":"src\/hotspot\/share\/gc\/shenandoah\/c2\/shenandoahSupport.cpp","additions":7,"deletions":1,"binary":false,"changes":8,"status":"modified"},{"patch":"@@ -31,1 +31,1 @@\n-#include \"gc\/shenandoah\/shenandoahHeap.inline.hpp\"\n+#include \"gc\/shenandoah\/shenandoahGenerationalHeap.hpp\"\n@@ -33,0 +33,1 @@\n+#include \"gc\/shenandoah\/shenandoahOldGeneration.hpp\"\n@@ -43,1 +44,1 @@\n-  ShenandoahHeap* heap = ShenandoahHeap::heap();\n+  auto heap = ShenandoahGenerationalHeap::heap();\n@@ -151,2 +152,2 @@\n-  heap->reserve_promotable_humongous_regions(humongous_regions_promoted);\n-  heap->reserve_promotable_regular_regions(regular_regions_promoted_in_place);\n+  heap->old_generation()->set_expected_humongous_region_promotions(humongous_regions_promoted);\n+  heap->old_generation()->set_expected_regular_region_promotions(regular_regions_promoted_in_place);\n","filename":"src\/hotspot\/share\/gc\/shenandoah\/heuristics\/shenandoahGenerationalHeuristics.cpp","additions":5,"deletions":4,"binary":false,"changes":9,"status":"modified"},{"patch":"@@ -30,1 +30,1 @@\n-#include \"gc\/shenandoah\/shenandoahHeap.inline.hpp\"\n+#include \"gc\/shenandoah\/shenandoahGenerationalHeap.hpp\"\n@@ -82,1 +82,1 @@\n-  ShenandoahHeap* heap = ShenandoahHeap::heap();\n+  auto heap = ShenandoahGenerationalHeap::heap();\n@@ -89,1 +89,3 @@\n-  size_t max_young_cset = (size_t) (heap->get_young_evac_reserve() \/ ShenandoahEvacWaste);\n+  size_t young_evac_reserve = heap->young_generation()->get_evacuation_reserve();\n+  size_t old_evac_reserve = heap->old_generation()->get_evacuation_reserve();\n+  size_t max_young_cset = (size_t) (young_evac_reserve \/ ShenandoahEvacWaste);\n@@ -91,1 +93,1 @@\n-  size_t max_old_cset = (size_t) (heap->get_old_evac_reserve() \/ ShenandoahOldEvacWaste);\n+  size_t max_old_cset = (size_t) (old_evac_reserve \/ ShenandoahOldEvacWaste);\n@@ -171,2 +173,2 @@\n-    heap->set_young_evac_reserve(heap->get_young_evac_reserve() - regions_transferred_to_old * region_size_bytes);\n-    heap->set_old_evac_reserve(heap->get_old_evac_reserve() + regions_transferred_to_old * region_size_bytes);\n+    heap->young_generation()->set_evacuation_reserve(young_evac_reserve - regions_transferred_to_old * region_size_bytes);\n+    heap->old_generation()->set_evacuation_reserve(old_evac_reserve + regions_transferred_to_old * region_size_bytes);\n","filename":"src\/hotspot\/share\/gc\/shenandoah\/heuristics\/shenandoahGlobalHeuristics.cpp","additions":8,"deletions":6,"binary":false,"changes":14,"status":"modified"},{"patch":"@@ -157,0 +157,3 @@\n+\n+  \/\/ This indicates whether or not the current cycle should unload classes.\n+  \/\/ It does NOT indicate that a cycle should be started.\n","filename":"src\/hotspot\/share\/gc\/shenandoah\/heuristics\/shenandoahHeuristics.hpp","additions":3,"deletions":0,"binary":false,"changes":3,"status":"modified"},{"patch":"@@ -30,2 +30,1 @@\n-#include \"gc\/shenandoah\/shenandoahHeap.hpp\"\n-#include \"gc\/shenandoah\/shenandoahHeapRegion.inline.hpp\"\n+#include \"gc\/shenandoah\/shenandoahGenerationalHeap.hpp\"\n@@ -35,3 +34,0 @@\n-#define BYTES_FORMAT    SIZE_FORMAT \"%s\"\n-#define FORMAT_BYTES(b) byte_size_in_proper_unit(b), proper_unit_for_byte_size(b)\n-\n@@ -75,1 +71,1 @@\n-  ShenandoahHeap* heap = ShenandoahHeap::heap();\n+  auto heap = ShenandoahGenerationalHeap::heap();\n@@ -91,1 +87,2 @@\n-  size_t old_evacuation_budget = (size_t) ((double) heap->get_old_evac_reserve() \/ ShenandoahOldEvacWaste);\n+  const size_t old_evacuation_reserve = heap->old_generation()->get_evacuation_reserve();\n+  const size_t old_evacuation_budget = (size_t) ((double) old_evacuation_reserve \/ ShenandoahOldEvacWaste);\n@@ -224,2 +221,2 @@\n-                   \"Old evacuation budget: \" BYTES_FORMAT \", Remaining evacuation budget: \" BYTES_FORMAT\n-                   \", Lost capacity: \" BYTES_FORMAT\n+                   \"Old evacuation budget: \" PROPERFMT \", Remaining evacuation budget: \" PROPERFMT\n+                   \", Lost capacity: \" PROPERFMT\n@@ -227,3 +224,3 @@\n-                   FORMAT_BYTES(heap->get_old_evac_reserve()),\n-                   FORMAT_BYTES(remaining_old_evacuation_budget),\n-                   FORMAT_BYTES(lost_evacuation_capacity),\n+                   PROPERFMTARGS(old_evacuation_reserve),\n+                   PROPERFMTARGS(remaining_old_evacuation_budget),\n+                   PROPERFMTARGS(lost_evacuation_capacity),\n@@ -312,0 +309,1 @@\n+  const size_t num_regions = heap->num_regions();\n@@ -314,1 +312,0 @@\n-  size_t num_regions = heap->num_regions();\n@@ -426,2 +423,2 @@\n-    size_t first_unselected_old_region = candidates[_last_old_collection_candidate]._region->index();\n-    size_t last_unselected_old_region = candidates[cand_idx - 1]._region->index();\n+    const size_t first_unselected_old_region = candidates[_last_old_collection_candidate]._region->index();\n+    const size_t last_unselected_old_region = candidates[cand_idx - 1]._region->index();\n@@ -433,1 +430,1 @@\n-    size_t bound_on_additional_regions = cand_idx \/ MAX_FRACTION_OF_HUMONGOUS_DEFRAG_REGIONS;\n+    const size_t bound_on_additional_regions = cand_idx \/ MAX_FRACTION_OF_HUMONGOUS_DEFRAG_REGIONS;\n@@ -441,2 +438,2 @@\n-      size_t region_garbage = candidates[_last_old_collection_candidate]._region->garbage();\n-      size_t region_free = r->free();\n+      const size_t region_garbage = candidates[_last_old_collection_candidate]._region->garbage();\n+      const size_t region_free = r->free();\n@@ -456,3 +453,3 @@\n-  size_t collectable_garbage = immediate_garbage + candidates_garbage;\n-  size_t old_candidates = _last_old_collection_candidate;\n-  size_t mixed_evac_live = old_candidates * region_size_bytes - (candidates_garbage + unfragmented);\n+  const size_t collectable_garbage = immediate_garbage + candidates_garbage;\n+  const size_t old_candidates = _last_old_collection_candidate;\n+  const size_t mixed_evac_live = old_candidates * region_size_bytes - (candidates_garbage + unfragmented);\n@@ -559,3 +556,3 @@\n-    size_t old_gen_capacity = _old_generation->max_capacity();\n-    size_t heap_capacity = heap->capacity();\n-    double percent = percent_of(old_gen_capacity, heap_capacity);\n+    const size_t old_gen_capacity = _old_generation->max_capacity();\n+    const size_t heap_capacity = heap->capacity();\n+    const double percent = percent_of(old_gen_capacity, heap_capacity);\n@@ -568,2 +565,2 @@\n-    size_t used = _old_generation->used();\n-    size_t used_regions_size = _old_generation->used_regions_size();\n+    const size_t used = _old_generation->used();\n+    const size_t used_regions_size = _old_generation->used_regions_size();\n@@ -572,1 +569,1 @@\n-    size_t used_regions = _old_generation->used_regions();\n+    const size_t used_regions = _old_generation->used_regions();\n@@ -578,2 +575,2 @@\n-    size_t span_of_old_regions = (last_old_region >= first_old_region)? last_old_region + 1 - first_old_region: 0;\n-    size_t fragmented_free = used_regions_size - used;\n+    const size_t span_of_old_regions = (last_old_region >= first_old_region)? last_old_region + 1 - first_old_region: 0;\n+    const size_t fragmented_free = used_regions_size - used;\n@@ -592,3 +589,4 @@\n-    size_t current_usage = _old_generation->used();\n-    size_t trigger_threshold = _old_generation->usage_trigger_threshold();\n-    size_t heap_size = heap->capacity();\n+    const size_t current_usage = _old_generation->used();\n+    const size_t trigger_threshold = _old_generation->usage_trigger_threshold();\n+    const size_t heap_size = heap->capacity();\n+    const size_t ignore_threshold = (ShenandoahIgnoreOldGrowthBelowPercentage * heap_size) \/ 100;\n@@ -596,1 +594,0 @@\n-    size_t ignore_threshold = (ShenandoahIgnoreOldGrowthBelowPercentage * heap_size) \/ 100;\n@@ -607,2 +604,2 @@\n-      size_t live_at_previous_old = _old_generation->get_live_bytes_after_last_mark();\n-      double percent_growth = percent_of(current_usage - live_at_previous_old, live_at_previous_old);\n+      const size_t live_at_previous_old = _old_generation->get_live_bytes_after_last_mark();\n+      const double percent_growth = percent_of(current_usage - live_at_previous_old, live_at_previous_old);\n@@ -658,4 +655,0 @@\n-\n-\n-#undef BYTES_FORMAT\n-#undef FORMAT_BYTES\n","filename":"src\/hotspot\/share\/gc\/shenandoah\/heuristics\/shenandoahOldHeuristics.cpp","additions":32,"deletions":39,"binary":false,"changes":71,"status":"modified"},{"patch":"@@ -30,1 +30,1 @@\n-#include \"gc\/shenandoah\/shenandoahHeap.inline.hpp\"\n+#include \"gc\/shenandoah\/shenandoahGenerationalHeap.hpp\"\n@@ -32,0 +32,1 @@\n+#include \"gc\/shenandoah\/shenandoahOldGeneration.hpp\"\n@@ -81,1 +82,1 @@\n-  ShenandoahHeap* heap = ShenandoahHeap::heap();\n+  auto heap = ShenandoahGenerationalHeap::heap();\n@@ -90,1 +91,1 @@\n-  size_t max_cset = (size_t) (heap->get_young_evac_reserve() \/ ShenandoahEvacWaste);\n+  size_t max_cset = (size_t) (heap->young_generation()->get_evacuation_reserve() \/ ShenandoahEvacWaste);\n@@ -126,1 +127,1 @@\n-  ShenandoahHeap* heap = ShenandoahHeap::heap();\n+  auto heap = ShenandoahGenerationalHeap::heap();\n@@ -130,1 +131,1 @@\n-  if (ShenandoahMinimumOldMarkTimeMs > 0 && ShenandoahHeap::heap()->is_concurrent_old_mark_in_progress()) {\n+  if (ShenandoahMinimumOldMarkTimeMs > 0 && heap->is_concurrent_old_mark_in_progress()) {\n@@ -148,1 +149,1 @@\n-  size_t promo_potential = heap->get_promotion_potential();\n+  size_t promo_potential = heap->old_generation()->get_promotion_potential();\n@@ -178,1 +179,1 @@\n-  size_t capacity = _space_info->soft_max_capacity();\n+  size_t capacity = _space_info->max_capacity();\n","filename":"src\/hotspot\/share\/gc\/shenandoah\/heuristics\/shenandoahYoungHeuristics.cpp","additions":8,"deletions":7,"binary":false,"changes":15,"status":"modified"},{"patch":"@@ -186,0 +186,1 @@\n+#ifndef PRODUCT\n@@ -215,0 +216,1 @@\n+#endif \/\/ !PRODUCT\n","filename":"src\/hotspot\/share\/gc\/shenandoah\/shenandoahAgeCensus.cpp","additions":2,"deletions":0,"binary":false,"changes":2,"status":"modified"},{"patch":"@@ -181,0 +181,1 @@\n+#ifndef PRODUCT\n@@ -184,0 +185,1 @@\n+#endif \/\/ !PRODUCT\n","filename":"src\/hotspot\/share\/gc\/shenandoah\/shenandoahAgeCensus.hpp","additions":2,"deletions":0,"binary":false,"changes":2,"status":"modified"},{"patch":"@@ -126,0 +126,10 @@\n+  \/\/ Disable support for dynamic number of GC threads. We do not let the runtime\n+  \/\/ heuristics to misjudge how many threads we need during the heavy concurrent phase\n+  \/\/ or a GC pause.\n+  if (UseDynamicNumberOfGCThreads) {\n+    if (FLAG_IS_CMDLINE(UseDynamicNumberOfGCThreads)) {\n+      warning(\"Shenandoah does not support UseDynamicNumberOfGCThreads, disabling\");\n+    }\n+    FLAG_SET_DEFAULT(UseDynamicNumberOfGCThreads, false);\n+  }\n+\n","filename":"src\/hotspot\/share\/gc\/shenandoah\/shenandoahArguments.cpp","additions":10,"deletions":0,"binary":false,"changes":10,"status":"modified"},{"patch":"@@ -405,1 +405,1 @@\n-        assert(obj != fwd || _heap->cancelled_gc(), \"must be forwarded\");\n+        shenandoah_assert_forwarded_except(elem_ptr, obj, _heap->cancelled_gc());\n","filename":"src\/hotspot\/share\/gc\/shenandoah\/shenandoahBarrierSet.inline.hpp","additions":1,"deletions":1,"binary":false,"changes":2,"status":"modified"},{"patch":"@@ -56,1 +56,1 @@\n-        assert(obj != fwd || _heap->cancelled_gc(), \"must be forwarded\");\n+        shenandoah_assert_forwarded_except(p, obj, _heap->cancelled_gc());\n","filename":"src\/hotspot\/share\/gc\/shenandoah\/shenandoahBarrierSetClone.inline.hpp","additions":1,"deletions":1,"binary":false,"changes":2,"status":"modified"},{"patch":"@@ -59,4 +59,0 @@\n-  CardValue* guard_card = &_byte_map[num_cards];\n-  assert(is_aligned(guard_card, _page_size), \"must be on its own OS page\");\n-  _guard_region = MemRegion((HeapWord*)guard_card, _page_size);\n-\n@@ -89,4 +85,0 @@\n-  \/\/ If we choose to modify the mutator write barrier so that we can swap _read_byte_map_base and\n-  \/\/ _write_byte_map_base pointers, we may also have to figure out certain details about how the\n-  \/\/ _guard_region is implemented so that we can replicate the read and write versions of this region.\n-  \/\/\n","filename":"src\/hotspot\/share\/gc\/shenandoah\/shenandoahCardTable.cpp","additions":0,"deletions":8,"binary":false,"changes":8,"status":"modified"},{"patch":"@@ -35,1 +35,0 @@\n-  _mixed_gcs(0),\n@@ -37,3 +36,0 @@\n-  _abbreviated_degenerated_gcs(0),\n-  _success_old_gcs(0),\n-  _interrupted_old_gcs(0),\n@@ -41,0 +37,1 @@\n+  _abbreviated_degenerated_gcs(0),\n@@ -42,1 +39,0 @@\n-  _consecutive_young_gcs(0),\n@@ -44,0 +40,4 @@\n+  _consecutive_young_gcs(0),\n+  _mixed_gcs(0),\n+  _success_old_gcs(0),\n+  _interrupted_old_gcs(0),\n@@ -46,6 +46,1 @@\n-  _alloc_failure_full(0),\n-  _explicit_concurrent(0),\n-  _explicit_full(0),\n-  _implicit_concurrent(0),\n-  _implicit_full(0),\n-  _cycle_counter(0) {\n+  _alloc_failure_full(0) {\n@@ -53,1 +48,2 @@\n-  Copy::zero_to_bytes(_degen_points, sizeof(size_t) * ShenandoahGC::_DEGENERATED_LIMIT);\n+  Copy::zero_to_bytes(_degen_point_counts, sizeof(size_t) * ShenandoahGC::_DEGENERATED_LIMIT);\n+  Copy::zero_to_bytes(_collection_cause_counts, sizeof(size_t) * GCCause::_last_gc_cause);\n@@ -56,1 +52,0 @@\n-\n@@ -59,14 +54,3 @@\n-void ShenandoahCollectorPolicy::record_explicit_to_concurrent() {\n-  _explicit_concurrent++;\n-}\n-\n-void ShenandoahCollectorPolicy::record_explicit_to_full() {\n-  _explicit_full++;\n-}\n-\n-void ShenandoahCollectorPolicy::record_implicit_to_concurrent() {\n-  _implicit_concurrent++;\n-}\n-\n-void ShenandoahCollectorPolicy::record_implicit_to_full() {\n-  _implicit_full++;\n+void ShenandoahCollectorPolicy::record_collection_cause(GCCause::Cause cause) {\n+  assert(cause < GCCause::_last_gc_cause, \"Invalid GCCause\");\n+  _collection_cause_counts[cause]++;\n@@ -82,1 +66,1 @@\n-  _degen_points[point]++;\n+  _degen_point_counts[point]++;\n@@ -138,8 +122,0 @@\n-size_t ShenandoahCollectorPolicy::cycle_counter() const {\n-  return _cycle_counter;\n-}\n-\n-void ShenandoahCollectorPolicy::record_cycle_start() {\n-  _cycle_counter++;\n-}\n-\n@@ -154,0 +130,43 @@\n+bool is_explicit_gc(GCCause::Cause cause) {\n+  return GCCause::is_user_requested_gc(cause)\n+      || GCCause::is_serviceability_requested_gc(cause);\n+}\n+\n+bool is_implicit_gc(GCCause::Cause cause) {\n+  return cause != GCCause::_no_gc\n+      && cause != GCCause::_shenandoah_concurrent_gc\n+      && cause != GCCause::_allocation_failure\n+      && !is_explicit_gc(cause);\n+}\n+\n+#ifdef ASSERT\n+bool is_valid_request(GCCause::Cause cause) {\n+  return is_explicit_gc(cause)\n+      || cause == GCCause::_metadata_GC_clear_soft_refs\n+      || cause == GCCause::_codecache_GC_aggressive\n+      || cause == GCCause::_codecache_GC_threshold\n+      || cause == GCCause::_full_gc_alot\n+      || cause == GCCause::_wb_young_gc\n+      || cause == GCCause::_wb_full_gc\n+      || cause == GCCause::_wb_breakpoint\n+      || cause == GCCause::_scavenge_alot;\n+}\n+#endif\n+\n+bool ShenandoahCollectorPolicy::is_requested_gc(GCCause::Cause cause) {\n+  return is_explicit_gc(cause) || is_implicit_gc(cause);\n+}\n+\n+bool ShenandoahCollectorPolicy::should_run_full_gc(GCCause::Cause cause) {\n+  return is_explicit_gc(cause) ? !ExplicitGCInvokesConcurrent : !ShenandoahImplicitGCInvokesConcurrent;\n+}\n+\n+bool ShenandoahCollectorPolicy::should_handle_requested_gc(GCCause::Cause cause) {\n+  assert(is_valid_request(cause), \"only requested GCs here: %s\", GCCause::to_string(cause));\n+\n+  if (DisableExplicitGC) {\n+    return !is_explicit_gc(cause);\n+  }\n+  return true;\n+}\n+\n@@ -164,4 +183,26 @@\n-  out->print_cr(SIZE_FORMAT_W(5) \" Successful Concurrent GCs (%.2f%%)\",  _success_concurrent_gcs, percent_of(_success_concurrent_gcs, completed_gcs));\n-  out->print_cr(\"  \" SIZE_FORMAT_W(5) \" invoked explicitly (%.2f%%)\",    _explicit_concurrent, percent_of(_explicit_concurrent, _success_concurrent_gcs));\n-  out->print_cr(\"  \" SIZE_FORMAT_W(5) \" invoked implicitly (%.2f%%)\",    _implicit_concurrent, percent_of(_implicit_concurrent, _success_concurrent_gcs));\n-  out->print_cr(\"  \" SIZE_FORMAT_W(5) \" abbreviated (%.2f%%)\",           _abbreviated_concurrent_gcs, percent_of(_abbreviated_concurrent_gcs, _success_concurrent_gcs));\n+\n+  size_t explicit_requests = 0;\n+  size_t implicit_requests = 0;\n+  for (int c = 0; c < GCCause::_last_gc_cause; c++) {\n+    size_t cause_count = _collection_cause_counts[c];\n+    if (cause_count > 0) {\n+      auto cause = (GCCause::Cause) c;\n+      if (is_explicit_gc(cause)) {\n+        explicit_requests += cause_count;\n+      } else if (is_implicit_gc(cause)) {\n+        implicit_requests += cause_count;\n+      }\n+      const char* desc = GCCause::to_string(cause);\n+      out->print_cr(\"  \" SIZE_FORMAT_W(5) \" caused by %s (%.2f%%)\", cause_count, desc, percent_of(cause_count, completed_gcs));\n+    }\n+  }\n+\n+  out->cr();\n+  out->print_cr(SIZE_FORMAT_W(5) \" Successful Concurrent GCs (%.2f%%)\", _success_concurrent_gcs, percent_of(_success_concurrent_gcs, completed_gcs));\n+  if (ExplicitGCInvokesConcurrent) {\n+    out->print_cr(\"  \" SIZE_FORMAT_W(5) \" invoked explicitly (%.2f%%)\", explicit_requests, percent_of(explicit_requests, _success_concurrent_gcs));\n+  }\n+  if (ShenandoahImplicitGCInvokesConcurrent) {\n+    out->print_cr(\"  \" SIZE_FORMAT_W(5) \" invoked implicitly (%.2f%%)\", implicit_requests, percent_of(implicit_requests, _success_concurrent_gcs));\n+  }\n+  out->print_cr(\"  \" SIZE_FORMAT_W(5) \" abbreviated (%.2f%%)\",  _abbreviated_concurrent_gcs, percent_of(_abbreviated_concurrent_gcs, _success_concurrent_gcs));\n@@ -183,1 +224,1 @@\n-    if (_degen_points[c] > 0) {\n+    if (_degen_point_counts[c] > 0) {\n@@ -185,1 +226,1 @@\n-      out->print_cr(\"    \" SIZE_FORMAT_W(5) \" happened at %s\",         _degen_points[c], desc);\n+      out->print_cr(\"    \" SIZE_FORMAT_W(5) \" happened at %s\", _degen_point_counts[c], desc);\n@@ -190,3 +231,7 @@\n-  out->print_cr(SIZE_FORMAT_W(5) \" Full GCs (%.2f%%)\",                          _success_full_gcs, percent_of(_success_full_gcs, completed_gcs));\n-  out->print_cr(\"  \" SIZE_FORMAT_W(5) \" invoked explicitly (%.2f%%)\",           _explicit_full, percent_of(_explicit_full, _success_full_gcs));\n-  out->print_cr(\"  \" SIZE_FORMAT_W(5) \" invoked implicitly (%.2f%%)\",           _implicit_full, percent_of(_implicit_full, _success_full_gcs));\n+  out->print_cr(SIZE_FORMAT_W(5) \" Full GCs (%.2f%%)\", _success_full_gcs, percent_of(_success_full_gcs, completed_gcs));\n+  if (!ExplicitGCInvokesConcurrent) {\n+    out->print_cr(\"  \" SIZE_FORMAT_W(5) \" invoked explicitly (%.2f%%)\", explicit_requests, percent_of(explicit_requests, _success_concurrent_gcs));\n+  }\n+  if (!ShenandoahImplicitGCInvokesConcurrent) {\n+    out->print_cr(\"  \" SIZE_FORMAT_W(5) \" invoked implicitly (%.2f%%)\", implicit_requests, percent_of(implicit_requests, _success_concurrent_gcs));\n+  }\n","filename":"src\/hotspot\/share\/gc\/shenandoah\/shenandoahCollectorPolicy.cpp","additions":90,"deletions":45,"binary":false,"changes":135,"status":"modified"},{"patch":"@@ -43,1 +43,0 @@\n-  size_t _mixed_gcs;\n@@ -45,3 +44,0 @@\n-  size_t _abbreviated_degenerated_gcs;\n-  size_t _success_old_gcs;\n-  size_t _interrupted_old_gcs;\n@@ -49,0 +45,1 @@\n+  size_t _abbreviated_degenerated_gcs;\n@@ -51,1 +48,0 @@\n-  volatile size_t _consecutive_young_gcs;\n@@ -53,0 +49,4 @@\n+  volatile size_t _consecutive_young_gcs;\n+  size_t _mixed_gcs;\n+  size_t _success_old_gcs;\n+  size_t _interrupted_old_gcs;\n@@ -56,6 +56,2 @@\n-  size_t _explicit_concurrent;\n-  size_t _explicit_full;\n-  size_t _implicit_concurrent;\n-  size_t _implicit_full;\n-  size_t _cycle_counter;\n-  size_t _degen_points[ShenandoahGC::_DEGENERATED_LIMIT];\n+  size_t _collection_cause_counts[GCCause::_last_gc_cause];\n+  size_t _degen_point_counts[ShenandoahGC::_DEGENERATED_LIMIT];\n@@ -70,4 +66,0 @@\n-  \/\/ TODO: This is different from gc_end: that one encompasses one VM operation.\n-  \/\/ These two encompass the entire cycle.\n-  void record_cycle_start();\n-\n@@ -75,2 +67,0 @@\n-\n-  void record_success_concurrent(bool is_young, bool is_abbreviated);\n@@ -79,0 +69,7 @@\n+\n+  \/\/ A collection cycle may be \"abbreviated\" if Shenandoah finds a sufficient percentage\n+  \/\/ of regions that contain no live objects (ShenandoahImmediateThreshold). These cycles\n+  \/\/ end after final mark, skipping the evacuation and reference-updating phases. Such\n+  \/\/ cycles are very efficient and are worth tracking. Note that both degenerated and\n+  \/\/ concurrent cycles can be abbreviated.\n+  void record_success_concurrent(bool is_young, bool is_abbreviated);\n@@ -84,4 +81,1 @@\n-  void record_explicit_to_concurrent();\n-  void record_explicit_to_full();\n-  void record_implicit_to_concurrent();\n-  void record_implicit_to_full();\n+  void record_collection_cause(GCCause::Cause cause);\n@@ -94,2 +88,0 @@\n-  size_t cycle_counter() const;\n-\n@@ -102,4 +94,3 @@\n-  inline size_t consecutive_young_gc_count() const {\n-    return _consecutive_young_gcs;\n-  }\n-\n+  \/\/ If the heuristics find that the number of consecutive degenerated cycles is above\n+  \/\/ ShenandoahFullGCThreshold, then they will initiate a Full GC upon an allocation\n+  \/\/ failure.\n@@ -110,0 +101,8 @@\n+  static bool is_requested_gc(GCCause::Cause cause);\n+  static bool should_run_full_gc(GCCause::Cause cause);\n+  static bool should_handle_requested_gc(GCCause::Cause cause);\n+\n+  inline size_t consecutive_young_gc_count() const {\n+    return _consecutive_young_gcs;\n+  }\n+\n","filename":"src\/hotspot\/share\/gc\/shenandoah\/shenandoahCollectorPolicy.hpp","additions":26,"deletions":27,"binary":false,"changes":53,"status":"modified"},{"patch":"@@ -36,0 +36,1 @@\n+#include \"gc\/shenandoah\/shenandoahGenerationalHeap.hpp\"\n@@ -233,5 +234,2 @@\n-    bool success;\n-    size_t region_xfer;\n-    const char* region_destination;\n-    ShenandoahYoungGeneration* young_gen = heap->young_generation();\n-    ShenandoahGeneration* old_gen = heap->old_generation();\n+\n+    ShenandoahGenerationalHeap::TransferResult result;\n@@ -239,25 +237,5 @@\n-      ShenandoahHeapLocker locker(heap->lock());\n-\n-      size_t old_region_surplus = heap->get_old_region_surplus();\n-      size_t old_region_deficit = heap->get_old_region_deficit();\n-      if (old_region_surplus) {\n-        success = heap->generation_sizer()->transfer_to_young(old_region_surplus);\n-        region_destination = \"young\";\n-        region_xfer = old_region_surplus;\n-      } else if (old_region_deficit) {\n-        success = heap->generation_sizer()->transfer_to_old(old_region_deficit);\n-        region_destination = \"old\";\n-        region_xfer = old_region_deficit;\n-        if (!success) {\n-          ((ShenandoahOldHeuristics *) old_gen->heuristics())->trigger_cannot_expand();\n-        }\n-      } else {\n-        region_destination = \"none\";\n-        region_xfer = 0;\n-        success = true;\n-      }\n-      heap->set_old_region_surplus(0);\n-      heap->set_old_region_deficit(0);\n-      heap->set_young_evac_reserve(0);\n-      heap->set_old_evac_reserve(0);\n-      heap->set_promoted_reserve(0);\n+      ShenandoahGenerationalHeap* gen_heap = ShenandoahGenerationalHeap::heap();\n+      ShenandoahHeapLocker locker(gen_heap->lock());\n+\n+      result = gen_heap->balance_generations();\n+      gen_heap->reset_generation_reserves();\n@@ -266,8 +244,5 @@\n-    \/\/ Report outside the heap lock\n-    size_t young_available = young_gen->available();\n-    size_t old_available = old_gen->available();\n-    log_info(gc, ergo)(\"After cleanup, %s \" SIZE_FORMAT \" regions to %s to prepare for next gc, old available: \"\n-                       SIZE_FORMAT \"%s, young_available: \" SIZE_FORMAT \"%s\",\n-                       success? \"successfully transferred\": \"failed to transfer\", region_xfer, region_destination,\n-                       byte_size_in_proper_unit(old_available), proper_unit_for_byte_size(old_available),\n-                       byte_size_in_proper_unit(young_available), proper_unit_for_byte_size(young_available));\n+    LogTarget(Info, gc, ergo) lt;\n+    if (lt.is_enabled()) {\n+      LogStream ls(lt);\n+      result.print_on(\"Concurrent GC\", &ls);\n+    }\n@@ -768,3 +743,1 @@\n-      size_t humongous_regions_promoted = heap->get_promotable_humongous_regions();\n-      size_t regular_regions_promoted_in_place = heap->get_regular_regions_promoted_in_place();\n-      if (!heap->collection_set()->is_empty() || (humongous_regions_promoted + regular_regions_promoted_in_place > 0)) {\n+      if (!heap->collection_set()->is_empty() || heap->old_generation()->has_in_place_promotions()) {\n@@ -822,1 +795,1 @@\n-        LogTarget(Info, gc, ergo) lt;\n+        LogTarget(Debug, gc, ergo) lt;\n@@ -962,0 +935,1 @@\n+      shenandoah_assert_not_in_cset_except(p, resolved, _heap->cancelled_gc());\n@@ -963,3 +937,0 @@\n-      assert(_heap->cancelled_gc() ||\n-             _mark_context->is_marked(resolved) && !_heap->in_collection_set(resolved),\n-             \"Sanity\");\n","filename":"src\/hotspot\/share\/gc\/shenandoah\/shenandoahConcurrentGC.cpp","additions":16,"deletions":45,"binary":false,"changes":61,"status":"modified"},{"patch":"@@ -63,0 +63,2 @@\n+\n+  \/\/ Return true if this cycle found enough immediate garbage to skip evacuation\n","filename":"src\/hotspot\/share\/gc\/shenandoah\/shenandoahConcurrentGC.hpp","additions":2,"deletions":0,"binary":false,"changes":2,"status":"modified"},{"patch":"@@ -67,1 +67,0 @@\n-\n@@ -70,2 +69,1 @@\n-    _cm->mark_loop(GENERATION, worker_id, _terminator, rp,\n-                   true \/*cancellable*\/,\n+    _cm->mark_loop(worker_id, _terminator, rp, GENERATION, true \/*cancellable*\/,\n@@ -99,1 +97,1 @@\n-template<ShenandoahGenerationType GENERATION>\n+template <ShenandoahGenerationType GENERATION>\n@@ -133,2 +131,1 @@\n-    _cm->mark_loop(GENERATION, worker_id, _terminator, rp,\n-                   false \/*not cancellable*\/,\n+    _cm->mark_loop(worker_id, _terminator, rp, GENERATION, false \/*not cancellable*\/,\n@@ -145,1 +142,1 @@\n-template<ShenandoahGenerationType GENERATION>\n+template <ShenandoahGenerationType GENERATION>\n@@ -163,1 +160,1 @@\n-template<ShenandoahGenerationType GENERATION>\n+template <ShenandoahGenerationType GENERATION>\n@@ -177,1 +174,1 @@\n-template<ShenandoahGenerationType GENERATION>\n+template <ShenandoahGenerationType GENERATION>\n@@ -201,1 +198,1 @@\n-    case GLOBAL_GEN: {\n+    case GLOBAL: {\n@@ -203,2 +200,2 @@\n-      ShenandoahMarkConcurrentRootsTask<GLOBAL_GEN> task(task_queues(), nullptr, rp,\n-                                                         ShenandoahPhaseTimings::conc_mark_roots, workers->active_workers());\n+      ShenandoahMarkConcurrentRootsTask<GLOBAL> task(task_queues(), nullptr, rp,\n+                                                     ShenandoahPhaseTimings::conc_mark_roots, workers->active_workers());\n@@ -208,1 +205,1 @@\n-    case GLOBAL_NON_GEN: {\n+    case NON_GEN: {\n@@ -210,2 +207,2 @@\n-      ShenandoahMarkConcurrentRootsTask<GLOBAL_NON_GEN> task(task_queues(), nullptr, rp,\n-                                                         ShenandoahPhaseTimings::conc_mark_roots, workers->active_workers());\n+      ShenandoahMarkConcurrentRootsTask<NON_GEN> task(task_queues(), nullptr, rp,\n+                                                      ShenandoahPhaseTimings::conc_mark_roots, workers->active_workers());\n@@ -263,1 +260,1 @@\n-      case GLOBAL_GEN: {\n+      case GLOBAL: {\n@@ -268,1 +265,1 @@\n-        ShenandoahConcurrentMarkingTask<GLOBAL_GEN> task(this, &terminator);\n+        ShenandoahConcurrentMarkingTask<GLOBAL> task(this, &terminator);\n@@ -272,1 +269,1 @@\n-      case GLOBAL_NON_GEN: {\n+      case NON_GEN: {\n@@ -274,1 +271,1 @@\n-        ShenandoahConcurrentMarkingTask<GLOBAL_NON_GEN> task(this, &terminator);\n+        ShenandoahConcurrentMarkingTask<NON_GEN> task(this, &terminator);\n@@ -340,2 +337,2 @@\n-    case GLOBAL_GEN:{\n-      ShenandoahFinalMarkingTask<GLOBAL_GEN> task(this, &terminator, ShenandoahStringDedup::is_enabled());\n+    case GLOBAL:{\n+      ShenandoahFinalMarkingTask<GLOBAL> task(this, &terminator, ShenandoahStringDedup::is_enabled());\n@@ -345,2 +342,2 @@\n-    case GLOBAL_NON_GEN:{\n-      ShenandoahFinalMarkingTask<GLOBAL_NON_GEN> task(this, &terminator, ShenandoahStringDedup::is_enabled());\n+    case NON_GEN:{\n+      ShenandoahFinalMarkingTask<NON_GEN> task(this, &terminator, ShenandoahStringDedup::is_enabled());\n","filename":"src\/hotspot\/share\/gc\/shenandoah\/shenandoahConcurrentMark.cpp","additions":20,"deletions":23,"binary":false,"changes":43,"status":"modified"},{"patch":"@@ -28,0 +28,1 @@\n+#include \"gc\/shenandoah\/shenandoahGenerationType.hpp\"\n","filename":"src\/hotspot\/share\/gc\/shenandoah\/shenandoahConcurrentMark.hpp","additions":1,"deletions":0,"binary":false,"changes":1,"status":"modified"},{"patch":"@@ -4,1 +4,0 @@\n- * Copyright Amazon.com Inc. or its affiliates. All Rights Reserved.\n@@ -32,1 +31,0 @@\n-#include \"gc\/shenandoah\/shenandoahEvacTracker.hpp\"\n@@ -36,4 +34,0 @@\n-#include \"gc\/shenandoah\/shenandoahGlobalGeneration.hpp\"\n-#include \"gc\/shenandoah\/shenandoahYoungGeneration.hpp\"\n-#include \"gc\/shenandoah\/shenandoahOldGeneration.hpp\"\n-#include \"gc\/shenandoah\/shenandoahPhaseTimings.hpp\"\n@@ -41,1 +35,0 @@\n-#include \"gc\/shenandoah\/shenandoahMark.inline.hpp\"\n@@ -43,3 +36,1 @@\n-#include \"gc\/shenandoah\/shenandoahOopClosures.inline.hpp\"\n-#include \"gc\/shenandoah\/shenandoahOldGC.hpp\"\n-#include \"gc\/shenandoah\/shenandoahRootProcessor.inline.hpp\"\n+#include \"gc\/shenandoah\/shenandoahPacer.inline.hpp\"\n@@ -47,2 +38,0 @@\n-#include \"gc\/shenandoah\/shenandoahVMOperations.hpp\"\n-#include \"gc\/shenandoah\/shenandoahWorkerPolicy.hpp\"\n@@ -52,1 +41,0 @@\n-#include \"memory\/iterator.hpp\"\n@@ -55,2 +43,0 @@\n-#include \"memory\/universe.hpp\"\n-#include \"runtime\/atomic.hpp\"\n@@ -59,12 +45,3 @@\n-  ConcurrentGCThread(),\n-  _alloc_failure_waiters_lock(Mutex::safepoint - 2, \"ShenandoahAllocFailureGC_lock\", true),\n-  _gc_waiters_lock(Mutex::safepoint - 2, \"ShenandoahRequestedGC_lock\", true),\n-  _control_lock(Mutex::nosafepoint - 2, \"ShenandoahControlGC_lock\", true),\n-  _regulator_lock(Mutex::nosafepoint - 2, \"ShenandoahRegulatorGC_lock\", true),\n-  _periodic_task(this),\n-  _requested_gc_cause(GCCause::_no_gc),\n-  _requested_generation(select_global_generation()),\n-  _degen_point(ShenandoahGC::_degenerated_outside_cycle),\n-  _degen_generation(nullptr),\n-  _allocs_seen(0),\n-  _mode(none) {\n+  ShenandoahController(),\n+  _requested_gc_cause(GCCause::_no_cause_specified),\n+  _degen_point(ShenandoahGC::_degenerated_outside_cycle) {\n@@ -72,1 +49,0 @@\n-  reset_gc_id();\n@@ -74,18 +50,0 @@\n-  _periodic_task.enroll();\n-  if (ShenandoahPacing) {\n-    _periodic_pacer_notify_task.enroll();\n-  }\n-}\n-\n-ShenandoahControlThread::~ShenandoahControlThread() {\n-  \/\/ This is here so that super is called.\n-}\n-\n-void ShenandoahPeriodicTask::task() {\n-  _thread->handle_force_counters_update();\n-  _thread->handle_counters_update();\n-}\n-\n-void ShenandoahPeriodicPacerNotify::task() {\n-  assert(ShenandoahPacing, \"Should not be here otherwise\");\n-  ShenandoahHeap::heap()->pacer()->notify_waiters();\n@@ -98,1 +56,2 @@\n-  ShenandoahGenerationType generation = select_global_generation();\n+  const GCCause::Cause default_cause = GCCause::_shenandoah_concurrent_gc;\n+  int sleep = ShenandoahControlIntervalMin;\n@@ -101,1 +60,1 @@\n-  uint age_period = 0;\n+  double last_sleep_adjust_time = os::elapsedTime();\n@@ -110,6 +69,1 @@\n-\n-  \/\/ Heuristics are notified of allocation failures here and other outcomes\n-  \/\/ of the cycle. They're also used here to control whether the Nth consecutive\n-  \/\/ degenerated cycle should be 'promoted' to a full cycle. The decision to\n-  \/\/ trigger a cycle or not is evaluated on the regulator thread.\n-  ShenandoahHeuristics* global_heuristics = heap->global_generation()->heuristics();\n+  ShenandoahHeuristics* const heuristics = heap->heuristics();\n@@ -119,1 +73,2 @@\n-    const bool humongous_alloc_failure_pending = _humongous_alloc_failure_gc.is_set();\n+    const bool is_gc_requested = _gc_requested.is_set();\n+    const GCCause::Cause requested_gc_cause = _requested_gc_cause;\n@@ -121,7 +76,2 @@\n-    GCCause::Cause cause = Atomic::xchg(&_requested_gc_cause, GCCause::_no_gc);\n-\n-    const bool explicit_gc_requested = is_explicit_gc(cause);\n-    const bool implicit_gc_requested = is_implicit_gc(cause);\n-\n-    \/\/ This control loop iteration have seen this much allocations.\n-    const size_t allocs_seen = Atomic::xchg(&_allocs_seen, (size_t)0, memory_order_relaxed);\n+    \/\/ This control loop iteration has seen this much allocation.\n+    const size_t allocs_seen = reset_allocs_seen();\n@@ -130,1 +80,1 @@\n-    const bool soft_max_changed = check_soft_max_changed();\n+    const bool soft_max_changed = heap->check_soft_max_changed();\n@@ -133,1 +83,2 @@\n-    set_gc_mode(none);\n+    GCMode mode = none;\n+    GCCause::Cause cause = GCCause::_last_gc_cause;\n@@ -146,14 +97,1 @@\n-      if (degen_point == ShenandoahGC::_degenerated_outside_cycle) {\n-        _degen_generation = heap->mode()->is_generational() ?\n-                heap->young_generation() : heap->global_generation();\n-      } else {\n-        assert(_degen_generation != nullptr, \"Need to know which generation to resume\");\n-      }\n-\n-      ShenandoahHeuristics* heuristics = _degen_generation->heuristics();\n-      generation = _degen_generation->type();\n-      bool old_gen_evacuation_failed = heap->clear_old_evacuation_failure();\n-\n-      \/\/ Do not bother with degenerated cycle if old generation evacuation failed or if humongous allocation failed\n-      if (ShenandoahDegeneratedGC && heuristics->should_degenerate_cycle() &&\n-          !old_gen_evacuation_failed && !humongous_alloc_failure_pending) {\n+      if (ShenandoahDegeneratedGC && heuristics->should_degenerate_cycle()) {\n@@ -162,1 +100,1 @@\n-        set_gc_mode(stw_degenerated);\n+        mode = stw_degenerated;\n@@ -164,7 +102,0 @@\n-        \/\/ TODO: if humongous_alloc_failure_pending, there might be value in trying a \"compacting\" degen before\n-        \/\/ going all the way to full.  But it's a lot of work to implement this, and it may not provide value.\n-        \/\/ A compacting degen can move young regions around without doing full old-gen mark (relying upon the\n-        \/\/ remembered set scan), so it might be faster than a full gc.\n-        \/\/\n-        \/\/ Longer term, think about how to defragment humongous memory concurrently.\n-\n@@ -173,2 +104,1 @@\n-        generation = select_global_generation();\n-        set_gc_mode(stw_full);\n+        mode = stw_full;\n@@ -176,3 +106,4 @@\n-    } else if (explicit_gc_requested) {\n-      generation = select_global_generation();\n-      log_info(gc)(\"Trigger: Explicit GC request (%s)\", GCCause::to_string(cause));\n+    } else if (is_gc_requested) {\n+      cause = requested_gc_cause;\n+      log_info(gc)(\"Trigger: GC request (%s)\", GCCause::to_string(cause));\n+      heuristics->record_requested_gc();\n@@ -180,7 +111,2 @@\n-      global_heuristics->record_requested_gc();\n-\n-      if (ExplicitGCInvokesConcurrent) {\n-        policy->record_explicit_to_concurrent();\n-        set_gc_mode(default_mode);\n-        \/\/ Unload and clean up everything\n-        heap->set_unload_classes(global_heuristics->can_unload_classes());\n+      if (ShenandoahCollectorPolicy::should_run_full_gc(cause)) {\n+        mode = stw_full;\n@@ -188,13 +114,1 @@\n-        policy->record_explicit_to_full();\n-        set_gc_mode(stw_full);\n-      }\n-    } else if (implicit_gc_requested) {\n-      generation = select_global_generation();\n-      log_info(gc)(\"Trigger: Implicit GC request (%s)\", GCCause::to_string(cause));\n-\n-      global_heuristics->record_requested_gc();\n-\n-      if (ShenandoahImplicitGCInvokesConcurrent) {\n-        policy->record_implicit_to_concurrent();\n-        set_gc_mode(default_mode);\n-\n+        mode = default_mode;\n@@ -202,4 +116,1 @@\n-        heap->set_unload_classes(global_heuristics->can_unload_classes());\n-      } else {\n-        policy->record_implicit_to_full();\n-        set_gc_mode(stw_full);\n+        heap->set_unload_classes(heuristics->can_unload_classes());\n@@ -208,20 +119,5 @@\n-      \/\/ We should only be here if the regulator requested a cycle or if\n-      \/\/ there is an old generation mark in progress.\n-      if (cause == GCCause::_shenandoah_concurrent_gc) {\n-        if (_requested_generation == OLD && heap->doing_mixed_evacuations()) {\n-          \/\/ If a request to start an old cycle arrived while an old cycle was running, but _before_\n-          \/\/ it chose any regions for evacuation we don't want to start a new old cycle. Rather, we want\n-          \/\/ the heuristic to run a young collection so that we can evacuate some old regions.\n-          assert(!heap->is_concurrent_old_mark_in_progress(), \"Should not be running mixed collections and concurrent marking\");\n-          generation = YOUNG;\n-        } else {\n-          generation = _requested_generation;\n-        }\n-\n-        \/\/ preemption was requested or this is a regular cycle\n-        set_gc_mode(default_mode);\n-\n-        \/\/ Don't start a new old marking if there is one already in progress\n-        if (generation == OLD && heap->is_concurrent_old_mark_in_progress()) {\n-          set_gc_mode(servicing_old);\n-        }\n+      \/\/ Potential normal cycle: ask heuristics if it wants to act\n+      if (heuristics->should_start_gc()) {\n+        mode = default_mode;\n+        cause = default_cause;\n+      }\n@@ -229,11 +125,3 @@\n-        if (generation == select_global_generation()) {\n-          heap->set_unload_classes(global_heuristics->should_unload_classes());\n-        } else {\n-          heap->set_unload_classes(false);\n-        }\n-      } else if (heap->is_concurrent_old_mark_in_progress() || heap->is_prepare_for_old_mark_in_progress()) {\n-        \/\/ Nobody asked us to do anything, but we have an old-generation mark or old-generation preparation for\n-        \/\/ mixed evacuation in progress, so resume working on that.\n-        log_info(gc)(\"Resume old GC: marking is%s in progress, preparing is%s in progress\",\n-                     heap->is_concurrent_old_mark_in_progress() ? \"\" : \" NOT\",\n-                     heap->is_prepare_for_old_mark_in_progress() ? \"\" : \" NOT\");\n+      \/\/ Ask policy if this cycle wants to process references or unload classes\n+      heap->set_unload_classes(heuristics->should_unload_classes());\n+    }\n@@ -241,5 +129,4 @@\n-        cause = GCCause::_shenandoah_concurrent_gc;\n-        generation = OLD;\n-        set_gc_mode(servicing_old);\n-        heap->set_unload_classes(false);\n-      }\n+    \/\/ Blow all soft references on this cycle, if handling allocation failure,\n+    \/\/ either implicit or explicit GC request,  or we are requested to do so unconditionally.\n+    if (alloc_failure_pending || is_gc_requested || ShenandoahAlwaysClearSoftRefs) {\n+      heap->soft_ref_policy()->set_should_clear_all_soft_refs(true);\n@@ -248,2 +135,2 @@\n-    const bool gc_requested = (gc_mode() != none);\n-    assert (!gc_requested || cause != GCCause::_no_gc, \"GC cause should be set\");\n+    const bool gc_requested = (mode != none);\n+    assert (!gc_requested || cause != GCCause::_last_gc_cause, \"GC cause should be set\");\n@@ -252,6 +139,0 @@\n-      \/\/ Blow away all soft references on this cycle, if handling allocation failure,\n-      \/\/ either implicit or explicit GC request, or we are requested to do so unconditionally.\n-      if (generation == select_global_generation() && (alloc_failure_pending || implicit_gc_requested || explicit_gc_requested || ShenandoahAlwaysClearSoftRefs)) {\n-        heap->soft_ref_policy()->set_should_clear_all_soft_refs(true);\n-      }\n-\n@@ -267,1 +148,1 @@\n-      set_forced_counters_update(true);\n+      heap->set_forced_counters_update(true);\n@@ -274,3 +155,0 @@\n-      \/\/ In case this is a degenerated cycle, remember whether original cycle was aging.\n-      const bool was_aging_cycle = heap->is_aging_cycle();\n-      heap->set_aging_cycle(false);\n@@ -278,12 +156,3 @@\n-      switch (gc_mode()) {\n-        case concurrent_normal: {\n-          \/\/ At this point:\n-          \/\/  if (generation == YOUNG), this is a normal YOUNG cycle\n-          \/\/  if (generation == OLD), this is a bootstrap OLD cycle\n-          \/\/  if (generation == GLOBAL), this is a GLOBAL cycle triggered by System.gc()\n-          \/\/ In all three cases, we want to age old objects if this is an aging cycle\n-          if (age_period-- == 0) {\n-             heap->set_aging_cycle(true);\n-             age_period = ShenandoahAgingCyclePeriod - 1;\n-          }\n-          service_concurrent_normal_cycle(heap, generation, cause);\n+      switch (mode) {\n+        case concurrent_normal:\n+          service_concurrent_normal_cycle(cause);\n@@ -291,3 +160,1 @@\n-        }\n-        case stw_degenerated: {\n-          heap->set_aging_cycle(was_aging_cycle);\n+        case stw_degenerated:\n@@ -296,6 +163,1 @@\n-        }\n-        case stw_full: {\n-          if (age_period-- == 0) {\n-            heap->set_aging_cycle(true);\n-            age_period = ShenandoahAgingCyclePeriod - 1;\n-          }\n+        case stw_full:\n@@ -304,7 +166,0 @@\n-        }\n-        case servicing_old: {\n-          assert(generation == OLD, \"Expected old generation here\");\n-          GCIdMark gc_id_mark;\n-          service_concurrent_old_cycle(heap, cause);\n-          break;\n-        }\n@@ -316,1 +171,1 @@\n-      if (explicit_gc_requested || implicit_gc_requested) {\n+      if (is_gc_requested) {\n@@ -334,1 +189,1 @@\n-        Universe::heap()->update_capacity_and_used_at_gc();\n+        heap->update_capacity_and_used_at_gc();\n@@ -337,1 +192,1 @@\n-        Universe::heap()->record_whole_heap_examined_timestamp();\n+        heap->record_whole_heap_examined_timestamp();\n@@ -342,2 +197,2 @@\n-      handle_force_counters_update();\n-      set_forced_counters_update(false);\n+      heap->handle_force_counters_update();\n+      heap->set_forced_counters_update(false);\n@@ -350,1 +205,20 @@\n-        global_heuristics->clear_metaspace_oom();\n+        heuristics->clear_metaspace_oom();\n+      }\n+\n+      \/\/ Commit worker statistics to cycle data\n+      heap->phase_timings()->flush_par_workers_to_cycle();\n+      if (ShenandoahPacing) {\n+        heap->pacer()->flush_stats_to_cycle();\n+      }\n+\n+      \/\/ Print GC stats for current cycle\n+      {\n+        LogTarget(Info, gc, stats) lt;\n+        if (lt.is_enabled()) {\n+          ResourceMark rm;\n+          LogStream ls(lt);\n+          heap->phase_timings()->print_cycle_on(&ls);\n+          if (ShenandoahPacing) {\n+            heap->pacer()->print_cycle_on(&ls);\n+          }\n+        }\n@@ -353,1 +227,2 @@\n-      process_phase_timings(heap);\n+      \/\/ Commit statistics to globals\n+      heap->phase_timings()->flush_cycle_to_global();\n@@ -363,1 +238,1 @@\n-      \/\/ Allow pacer to know we have seen this many allocations\n+      \/\/ Report to pacer that we have seen this many words allocated\n@@ -369,1 +244,1 @@\n-    double current = os::elapsedTime();\n+    const double current = os::elapsedTime();\n@@ -371,1 +246,1 @@\n-    if (ShenandoahUncommit && (explicit_gc_requested || soft_max_changed || (current - last_shrink_time > shrink_period))) {\n+    if (ShenandoahUncommit && (is_gc_requested || soft_max_changed || (current - last_shrink_time > shrink_period))) {\n@@ -376,1 +251,1 @@\n-      double shrink_before = (explicit_gc_requested || soft_max_changed) ?\n+      double shrink_before = (is_gc_requested || soft_max_changed) ?\n@@ -384,1 +259,1 @@\n-      service_uncommit(shrink_before, shrink_until);\n+      heap->maybe_uncommit(shrink_before, shrink_until);\n@@ -389,6 +264,8 @@\n-    \/\/ Wait for ShenandoahControlIntervalMax unless there was an allocation failure or another request was made mid-cycle.\n-    if (!is_alloc_failure_gc() && _requested_gc_cause == GCCause::_no_gc) {\n-      \/\/ The timed wait is necessary because this thread has a responsibility to send\n-      \/\/ 'alloc_words' to the pacer when it does not perform a GC.\n-      MonitorLocker lock(&_control_lock, Mutex::_no_safepoint_check_flag);\n-      lock.wait(ShenandoahControlIntervalMax);\n+    \/\/ Wait before performing the next action. If allocation happened during this wait,\n+    \/\/ we exit sooner, to let heuristics re-evaluate new conditions. If we are at idle,\n+    \/\/ back off exponentially.\n+    if (heap->has_changed()) {\n+      sleep = ShenandoahControlIntervalMin;\n+    } else if ((current - last_sleep_adjust_time) * 1000 > ShenandoahControlIntervalAdjustPeriod){\n+      sleep = MIN2<int>(ShenandoahControlIntervalMax, MAX2(1, sleep * 2));\n+      last_sleep_adjust_time = current;\n@@ -396,0 +273,1 @@\n+    os::naked_short_sleep(sleep);\n@@ -404,226 +282,1 @@\n-void ShenandoahControlThread::process_phase_timings(const ShenandoahHeap* heap) {\n-  \/\/ Commit worker statistics to cycle data\n-  heap->phase_timings()->flush_par_workers_to_cycle();\n-  if (ShenandoahPacing) {\n-    heap->pacer()->flush_stats_to_cycle();\n-  }\n-\n-  ShenandoahEvacuationTracker* evac_tracker = heap->evac_tracker();\n-  ShenandoahCycleStats         evac_stats   = evac_tracker->flush_cycle_to_global();\n-\n-  \/\/ Print GC stats for current cycle\n-  {\n-    LogTarget(Info, gc, stats) lt;\n-    if (lt.is_enabled()) {\n-      ResourceMark rm;\n-      LogStream ls(lt);\n-      heap->phase_timings()->print_cycle_on(&ls);\n-      evac_tracker->print_evacuations_on(&ls, &evac_stats.workers,\n-                                              &evac_stats.mutators);\n-      if (ShenandoahPacing) {\n-        heap->pacer()->print_cycle_on(&ls);\n-      }\n-    }\n-  }\n-\n-  \/\/ Commit statistics to globals\n-  heap->phase_timings()->flush_cycle_to_global();\n-}\n-\n-\/\/ Young and old concurrent cycles are initiated by the regulator. Implicit\n-\/\/ and explicit GC requests are handled by the controller thread and always\n-\/\/ run a global cycle (which is concurrent by default, but may be overridden\n-\/\/ by command line options). Old cycles always degenerate to a global cycle.\n-\/\/ Young cycles are degenerated to complete the young cycle.  Young\n-\/\/ and old degen may upgrade to Full GC.  Full GC may also be\n-\/\/ triggered directly by a System.gc() invocation.\n-\/\/\n-\/\/\n-\/\/      +-----+ Idle +-----+-----------+---------------------+\n-\/\/      |         +        |           |                     |\n-\/\/      |         |        |           |                     |\n-\/\/      |         |        v           |                     |\n-\/\/      |         |  Bootstrap Old +-- | ------------+       |\n-\/\/      |         |   +                |             |       |\n-\/\/      |         |   |                |             |       |\n-\/\/      |         v   v                v             v       |\n-\/\/      |    Resume Old <----------+ Young +--> Young Degen  |\n-\/\/      |     +  +   ^                            +  +       |\n-\/\/      v     |  |   |                            |  |       |\n-\/\/   Global <-+  |   +----------------------------+  |       |\n-\/\/      +        |                                   |       |\n-\/\/      |        v                                   v       |\n-\/\/      +--->  Global Degen +--------------------> Full <----+\n-\/\/\n-void ShenandoahControlThread::service_concurrent_normal_cycle(ShenandoahHeap* heap,\n-                                                              const ShenandoahGenerationType generation,\n-                                                              GCCause::Cause cause) {\n-  GCIdMark gc_id_mark;\n-  ShenandoahGeneration* the_generation = nullptr;\n-  switch (generation) {\n-    case YOUNG: {\n-      \/\/ Run a young cycle. This might or might not, have interrupted an ongoing\n-      \/\/ concurrent mark in the old generation. We need to think about promotions\n-      \/\/ in this case. Promoted objects should be above the TAMS in the old regions\n-      \/\/ they end up in, but we have to be sure we don't promote into any regions\n-      \/\/ that are in the cset.\n-      log_info(gc, ergo)(\"Start GC cycle (YOUNG)\");\n-      the_generation = heap->young_generation();\n-      service_concurrent_cycle(the_generation, cause, false);\n-      break;\n-    }\n-    case OLD: {\n-      log_info(gc, ergo)(\"Start GC cycle (OLD)\");\n-      the_generation = heap->old_generation();\n-      service_concurrent_old_cycle(heap, cause);\n-      break;\n-    }\n-    case GLOBAL_GEN: {\n-      log_info(gc, ergo)(\"Start GC cycle (GLOBAL)\");\n-      the_generation = heap->global_generation();\n-      service_concurrent_cycle(the_generation, cause, false);\n-      break;\n-    }\n-    case GLOBAL_NON_GEN: {\n-      log_info(gc, ergo)(\"Start GC cycle\");\n-      the_generation = heap->global_generation();\n-      service_concurrent_cycle(the_generation, cause, false);\n-      break;\n-    }\n-    default:\n-      ShouldNotReachHere();\n-  }\n-}\n-\n-void ShenandoahControlThread::service_concurrent_old_cycle(ShenandoahHeap* heap, GCCause::Cause &cause) {\n-  ShenandoahOldGeneration* old_generation = heap->old_generation();\n-  ShenandoahYoungGeneration* young_generation = heap->young_generation();\n-  ShenandoahOldGeneration::State original_state = old_generation->state();\n-\n-  TraceCollectorStats tcs(heap->monitoring_support()->concurrent_collection_counters());\n-\n-  switch (original_state) {\n-    case ShenandoahOldGeneration::FILLING: {\n-      _allow_old_preemption.set();\n-      old_generation->entry_coalesce_and_fill();\n-      _allow_old_preemption.unset();\n-\n-      \/\/ Before bootstrapping begins, we must acknowledge any cancellation request.\n-      \/\/ If the gc has not been cancelled, this does nothing. If it has been cancelled,\n-      \/\/ this will clear the cancellation request and exit before starting the bootstrap\n-      \/\/ phase. This will allow the young GC cycle to proceed normally. If we do not\n-      \/\/ acknowledge the cancellation request, the subsequent young cycle will observe\n-      \/\/ the request and essentially cancel itself.\n-      if (check_cancellation_or_degen(ShenandoahGC::_degenerated_outside_cycle)) {\n-        log_info(gc)(\"Preparation for old generation cycle was cancelled\");\n-        return;\n-      }\n-\n-      \/\/ Coalescing threads completed and nothing was cancelled. it is safe to transition from this state.\n-      old_generation->transition_to(ShenandoahOldGeneration::WAITING_FOR_BOOTSTRAP);\n-      return;\n-    }\n-    case ShenandoahOldGeneration::WAITING_FOR_BOOTSTRAP:\n-      old_generation->transition_to(ShenandoahOldGeneration::BOOTSTRAPPING);\n-    case ShenandoahOldGeneration::BOOTSTRAPPING: {\n-      \/\/ Configure the young generation's concurrent mark to put objects in\n-      \/\/ old regions into the concurrent mark queues associated with the old\n-      \/\/ generation. The young cycle will run as normal except that rather than\n-      \/\/ ignore old references it will mark and enqueue them in the old concurrent\n-      \/\/ task queues but it will not traverse them.\n-      set_gc_mode(bootstrapping_old);\n-      young_generation->set_old_gen_task_queues(old_generation->task_queues());\n-      ShenandoahGCSession session(cause, young_generation);\n-      service_concurrent_cycle(heap, young_generation, cause, true);\n-      process_phase_timings(heap);\n-      if (heap->cancelled_gc()) {\n-        \/\/ Young generation bootstrap cycle has failed. Concurrent mark for old generation\n-        \/\/ is going to resume after degenerated bootstrap cycle completes.\n-        log_info(gc)(\"Bootstrap cycle for old generation was cancelled\");\n-        return;\n-      }\n-\n-      \/\/ Reset the degenerated point. Normally this would happen at the top\n-      \/\/ of the control loop, but here we have just completed a young cycle\n-      \/\/ which has bootstrapped the old concurrent marking.\n-      _degen_point = ShenandoahGC::_degenerated_outside_cycle;\n-\n-      \/\/ From here we will 'resume' the old concurrent mark. This will skip reset\n-      \/\/ and init mark for the concurrent mark. All of that work will have been\n-      \/\/ done by the bootstrapping young cycle.\n-      set_gc_mode(servicing_old);\n-      old_generation->transition_to(ShenandoahOldGeneration::MARKING);\n-    }\n-    case ShenandoahOldGeneration::MARKING: {\n-      ShenandoahGCSession session(cause, old_generation);\n-      bool marking_complete = resume_concurrent_old_cycle(old_generation, cause);\n-      if (marking_complete) {\n-        assert(old_generation->state() != ShenandoahOldGeneration::MARKING, \"Should not still be marking\");\n-        if (original_state == ShenandoahOldGeneration::MARKING) {\n-          heap->mmu_tracker()->record_old_marking_increment(true);\n-          heap->log_heap_status(\"At end of Concurrent Old Marking finishing increment\");\n-        }\n-      } else if (original_state == ShenandoahOldGeneration::MARKING) {\n-        heap->mmu_tracker()->record_old_marking_increment(false);\n-        heap->log_heap_status(\"At end of Concurrent Old Marking increment\");\n-      }\n-      break;\n-    }\n-    default:\n-      fatal(\"Unexpected state for old GC: %s\", ShenandoahOldGeneration::state_name(old_generation->state()));\n-  }\n-}\n-\n-bool ShenandoahControlThread::resume_concurrent_old_cycle(ShenandoahGeneration* generation, GCCause::Cause cause) {\n-  assert(ShenandoahHeap::heap()->is_concurrent_old_mark_in_progress(), \"Old mark should be in progress\");\n-  log_debug(gc)(\"Resuming old generation with \" UINT32_FORMAT \" marking tasks queued\", generation->task_queues()->tasks());\n-\n-  ShenandoahHeap* heap = ShenandoahHeap::heap();\n-\n-  \/\/ We can only tolerate being cancelled during concurrent marking or during preparation for mixed\n-  \/\/ evacuation. This flag here (passed by reference) is used to control precisely where the regulator\n-  \/\/ is allowed to cancel a GC.\n-  ShenandoahOldGC gc(generation, _allow_old_preemption);\n-  if (gc.collect(cause)) {\n-    generation->record_success_concurrent(false);\n-  }\n-\n-  if (heap->cancelled_gc()) {\n-    \/\/ It's possible the gc cycle was cancelled after the last time\n-    \/\/ the collection checked for cancellation. In which case, the\n-    \/\/ old gc cycle is still completed, and we have to deal with this\n-    \/\/ cancellation. We set the degeneration point to be outside\n-    \/\/ the cycle because if this is an allocation failure, that is\n-    \/\/ what must be done (there is no degenerated old cycle). If the\n-    \/\/ cancellation was due to a heuristic wanting to start a young\n-    \/\/ cycle, then we are not actually going to a degenerated cycle,\n-    \/\/ so the degenerated point doesn't matter here.\n-    check_cancellation_or_degen(ShenandoahGC::_degenerated_outside_cycle);\n-    if (_requested_gc_cause == GCCause::_shenandoah_concurrent_gc) {\n-      heap->shenandoah_policy()->record_interrupted_old();\n-    }\n-    return false;\n-  }\n-  return true;\n-}\n-\n-bool ShenandoahControlThread::check_soft_max_changed() const {\n-  ShenandoahHeap* heap = ShenandoahHeap::heap();\n-  size_t new_soft_max = Atomic::load(&SoftMaxHeapSize);\n-  size_t old_soft_max = heap->soft_max_capacity();\n-  if (new_soft_max != old_soft_max) {\n-    new_soft_max = MAX2(heap->min_capacity(), new_soft_max);\n-    new_soft_max = MIN2(heap->max_capacity(), new_soft_max);\n-    if (new_soft_max != old_soft_max) {\n-      log_info(gc)(\"Soft Max Heap Size: \" SIZE_FORMAT \"%s -> \" SIZE_FORMAT \"%s\",\n-                   byte_size_in_proper_unit(old_soft_max), proper_unit_for_byte_size(old_soft_max),\n-                   byte_size_in_proper_unit(new_soft_max), proper_unit_for_byte_size(new_soft_max)\n-      );\n-      heap->set_soft_max_capacity(new_soft_max);\n-      return true;\n-    }\n-  }\n-  return false;\n-}\n-\n-void ShenandoahControlThread::service_concurrent_cycle(ShenandoahGeneration* generation, GCCause::Cause cause, bool do_old_gc_bootstrap) {\n+void ShenandoahControlThread::service_concurrent_normal_cycle(GCCause::Cause cause) {\n@@ -665,0 +318,1 @@\n+  ShenandoahHeap* heap = ShenandoahHeap::heap();\n@@ -667,3 +321,2 @@\n-  ShenandoahHeap* heap = ShenandoahHeap::heap();\n-  ShenandoahGCSession session(cause, generation);\n-  TraceCollectorStats tcs(heap->monitoring_support()->concurrent_collection_counters());\n+  GCIdMark gc_id_mark;\n+  ShenandoahGCSession session(cause, heap->global_generation());\n@@ -671,2 +324,1 @@\n-  service_concurrent_cycle(heap, generation, cause, do_old_gc_bootstrap);\n-}\n+  TraceCollectorStats tcs(heap->monitoring_support()->concurrent_collection_counters());\n@@ -674,5 +326,1 @@\n-void ShenandoahControlThread::service_concurrent_cycle(ShenandoahHeap* heap,\n-                                                       ShenandoahGeneration* generation,\n-                                                       GCCause::Cause& cause,\n-                                                       bool do_old_gc_bootstrap) {\n-  ShenandoahConcurrentGC gc(generation, do_old_gc_bootstrap);\n+  ShenandoahConcurrentGC gc(heap->global_generation(), false);\n@@ -681,1 +329,3 @@\n-    generation->record_success_concurrent(gc.abbreviated());\n+    heap->global_generation()->heuristics()->record_success_concurrent(gc.abbreviated());\n+    heap->shenandoah_policy()->record_success_concurrent(false, gc.abbreviated());\n+    heap->log_heap_status(\"At end of GC\");\n@@ -685,4 +335,1 @@\n-    assert(!generation->is_old(), \"Old GC takes a different control path\");\n-    \/\/ Concurrent young-gen collection degenerates to young\n-    \/\/ collection.  Same for global collections.\n-    _degen_generation = generation;\n+    heap->log_heap_status(\"At end of cancelled GC\");\n@@ -690,35 +337,0 @@\n-  const char* msg;\n-  if (heap->mode()->is_generational()) {\n-    ShenandoahMmuTracker* mmu_tracker = heap->mmu_tracker();\n-    if (generation->is_young()) {\n-      if (heap->cancelled_gc()) {\n-        msg = (do_old_gc_bootstrap) ? \"At end of Interrupted Concurrent Bootstrap GC\":\n-                                      \"At end of Interrupted Concurrent Young GC\";\n-      } else {\n-        \/\/ We only record GC results if GC was successful\n-        msg = (do_old_gc_bootstrap) ? \"At end of Concurrent Bootstrap GC\":\n-                                      \"At end of Concurrent Young GC\";\n-        if (heap->collection_set()->has_old_regions()) {\n-          mmu_tracker->record_mixed(get_gc_id());\n-        } else if (do_old_gc_bootstrap) {\n-          mmu_tracker->record_bootstrap(get_gc_id());\n-        } else {\n-          mmu_tracker->record_young(get_gc_id());\n-        }\n-      }\n-    } else {\n-      assert(generation->is_global(), \"If not young, must be GLOBAL\");\n-      assert(!do_old_gc_bootstrap, \"Do not bootstrap with GLOBAL GC\");\n-      if (heap->cancelled_gc()) {\n-        msg = \"At end of Interrupted Concurrent GLOBAL GC\";\n-      } else {\n-        \/\/ We only record GC results if GC was successful\n-        msg = \"At end of Concurrent Global GC\";\n-        mmu_tracker->record_global(get_gc_id());\n-      }\n-    }\n-  } else {\n-    msg = heap->cancelled_gc() ? \"At end of cancelled GC\" :\n-                                 \"At end of GC\";\n-  }\n-  heap->log_heap_status(msg);\n@@ -729,28 +341,7 @@\n-  if (!heap->cancelled_gc()) {\n-    return false;\n-  }\n-\n-  if (in_graceful_shutdown()) {\n-    return true;\n-  }\n-\n-  assert(_degen_point == ShenandoahGC::_degenerated_outside_cycle,\n-         \"Should not be set yet: %s\", ShenandoahGC::degen_point_to_string(_degen_point));\n-\n-  if (is_alloc_failure_gc()) {\n-    _degen_point = point;\n-    _preemption_requested.unset();\n-    return true;\n-  }\n-\n-  if (_preemption_requested.is_set()) {\n-    assert(_requested_generation == YOUNG, \"Only young GCs may preempt old.\");\n-    _preemption_requested.unset();\n-\n-    \/\/ Old generation marking is only cancellable during concurrent marking.\n-    \/\/ Once final mark is complete, the code does not check again for cancellation.\n-    \/\/ If old generation was cancelled for an allocation failure, we wouldn't\n-    \/\/ make it to this case. The calling code is responsible for forcing a\n-    \/\/ cancellation due to allocation failure into a degenerated cycle.\n-    _degen_point = point;\n-    heap->clear_cancelled_gc(false \/* clear oom handler *\/);\n+  if (heap->cancelled_gc()) {\n+    assert (is_alloc_failure_gc() || in_graceful_shutdown(), \"Cancel GC either for alloc failure GC, or gracefully exiting\");\n+    if (!in_graceful_shutdown()) {\n+      assert (_degen_point == ShenandoahGC::_degenerated_outside_cycle,\n+              \"Should not be set yet: %s\", ShenandoahGC::degen_point_to_string(_degen_point));\n+      _degen_point = point;\n+    }\n@@ -759,2 +350,0 @@\n-\n-  fatal(\"Cancel GC either for alloc failure GC, or gracefully exiting, or to pause old generation marking\");\n@@ -770,1 +359,0 @@\n-\n@@ -778,3 +366,2 @@\n-void ShenandoahControlThread::service_stw_degenerated_cycle(GCCause::Cause cause,\n-                                                            ShenandoahGC::ShenandoahDegenPoint point) {\n-  assert(point != ShenandoahGC::_degenerated_unset, \"Degenerated point should be set\");\n+void ShenandoahControlThread::service_stw_degenerated_cycle(GCCause::Cause cause, ShenandoahGC::ShenandoahDegenPoint point) {\n+  assert (point != ShenandoahGC::_degenerated_unset, \"Degenerated point should be set\");\n@@ -782,1 +369,0 @@\n-\n@@ -784,1 +370,1 @@\n-  ShenandoahGCSession session(cause, _degen_generation);\n+  ShenandoahGCSession session(cause, heap->global_generation());\n@@ -786,1 +372,1 @@\n-  ShenandoahDegenGC gc(point, _degen_generation);\n+  ShenandoahDegenGC gc(point, heap->global_generation());\n@@ -788,46 +374,0 @@\n-\n-  assert(heap->young_generation()->task_queues()->is_empty(), \"Unexpected young generation marking tasks\");\n-  if (_degen_generation->is_global()) {\n-    assert(heap->old_generation()->task_queues()->is_empty(), \"Unexpected old generation marking tasks\");\n-    assert(heap->global_generation()->task_queues()->is_empty(), \"Unexpected global generation marking tasks\");\n-  } else {\n-    assert(_degen_generation->is_young(), \"Expected degenerated young cycle, if not global.\");\n-    ShenandoahOldGeneration* old = heap->old_generation();\n-    if (old->state() == ShenandoahOldGeneration::BOOTSTRAPPING) {\n-      old->transition_to(ShenandoahOldGeneration::MARKING);\n-    }\n-  }\n-}\n-\n-void ShenandoahControlThread::service_uncommit(double shrink_before, size_t shrink_until) {\n-  ShenandoahHeap* heap = ShenandoahHeap::heap();\n-\n-  \/\/ Determine if there is work to do. This avoids taking heap lock if there is\n-  \/\/ no work available, avoids spamming logs with superfluous logging messages,\n-  \/\/ and minimises the amount of work while locks are taken.\n-\n-  if (heap->committed() <= shrink_until) return;\n-\n-  bool has_work = false;\n-  for (size_t i = 0; i < heap->num_regions(); i++) {\n-    ShenandoahHeapRegion *r = heap->get_region(i);\n-    if (r->is_empty_committed() && (r->empty_time() < shrink_before)) {\n-      has_work = true;\n-      break;\n-    }\n-  }\n-\n-  if (has_work) {\n-    heap->entry_uncommit(shrink_before, shrink_until);\n-  }\n-}\n-\n-bool ShenandoahControlThread::is_explicit_gc(GCCause::Cause cause) const {\n-  return GCCause::is_user_requested_gc(cause) ||\n-         GCCause::is_serviceability_requested_gc(cause);\n-}\n-\n-bool ShenandoahControlThread::is_implicit_gc(GCCause::Cause cause) const {\n-  return !is_explicit_gc(cause)\n-      && cause != GCCause::_shenandoah_concurrent_gc\n-      && cause != GCCause::_no_gc;\n@@ -837,17 +377,1 @@\n-  assert(GCCause::is_user_requested_gc(cause) ||\n-         GCCause::is_serviceability_requested_gc(cause) ||\n-         cause == GCCause::_metadata_GC_clear_soft_refs ||\n-         cause == GCCause::_codecache_GC_aggressive ||\n-         cause == GCCause::_codecache_GC_threshold ||\n-         cause == GCCause::_full_gc_alot ||\n-         cause == GCCause::_wb_young_gc ||\n-         cause == GCCause::_wb_full_gc ||\n-         cause == GCCause::_wb_breakpoint ||\n-         cause == GCCause::_scavenge_alot,\n-         \"only requested GCs here: %s\", GCCause::to_string(cause));\n-\n-  if (is_explicit_gc(cause)) {\n-    if (!DisableExplicitGC) {\n-      handle_requested_gc(cause);\n-    }\n-  } else {\n+  if (ShenandoahCollectorPolicy::should_handle_requested_gc(cause)) {\n@@ -858,63 +382,0 @@\n-bool ShenandoahControlThread::request_concurrent_gc(ShenandoahGenerationType generation) {\n-  if (_preemption_requested.is_set() || _requested_gc_cause != GCCause::_no_gc || ShenandoahHeap::heap()->cancelled_gc()) {\n-    \/\/ Ignore subsequent requests from the heuristics\n-    log_debug(gc, thread)(\"Reject request for concurrent gc: preemption_requested: %s, gc_requested: %s, gc_cancelled: %s\",\n-                          BOOL_TO_STR(_preemption_requested.is_set()),\n-                          GCCause::to_string(_requested_gc_cause),\n-                          BOOL_TO_STR(ShenandoahHeap::heap()->cancelled_gc()));\n-    return false;\n-  }\n-\n-  if (gc_mode() == none) {\n-    GCCause::Cause existing = Atomic::cmpxchg(&_requested_gc_cause, GCCause::_no_gc, GCCause::_shenandoah_concurrent_gc);\n-    if (existing != GCCause::_no_gc) {\n-      log_debug(gc, thread)(\"Reject request for concurrent gc because another gc is pending: %s\", GCCause::to_string(existing));\n-      return false;\n-    }\n-\n-    _requested_generation = generation;\n-    notify_control_thread();\n-\n-    MonitorLocker ml(&_regulator_lock, Mutex::_no_safepoint_check_flag);\n-    while (gc_mode() == none) {\n-      ml.wait();\n-    }\n-    return true;\n-  }\n-\n-  if (preempt_old_marking(generation)) {\n-    assert(gc_mode() == servicing_old, \"Expected to be servicing old, but was: %s.\", gc_mode_name(gc_mode()));\n-    GCCause::Cause existing = Atomic::cmpxchg(&_requested_gc_cause, GCCause::_no_gc, GCCause::_shenandoah_concurrent_gc);\n-    if (existing != GCCause::_no_gc) {\n-      log_debug(gc, thread)(\"Reject request to interrupt old gc because another gc is pending: %s\", GCCause::to_string(existing));\n-      return false;\n-    }\n-\n-    log_info(gc)(\"Preempting old generation mark to allow %s GC\", shenandoah_generation_name(generation));\n-    _requested_generation = generation;\n-    _preemption_requested.set();\n-    ShenandoahHeap::heap()->cancel_gc(GCCause::_shenandoah_concurrent_gc);\n-    notify_control_thread();\n-\n-    MonitorLocker ml(&_regulator_lock, Mutex::_no_safepoint_check_flag);\n-    while (gc_mode() == servicing_old) {\n-      ml.wait();\n-    }\n-    return true;\n-  }\n-\n-  log_debug(gc, thread)(\"Reject request for concurrent gc: mode: %s, allow_old_preemption: %s\",\n-                        gc_mode_name(gc_mode()),\n-                        BOOL_TO_STR(_allow_old_preemption.is_set()));\n-  return false;\n-}\n-\n-void ShenandoahControlThread::notify_control_thread() {\n-  MonitorLocker locker(&_control_lock, Mutex::_no_safepoint_check_flag);\n-  _control_lock.notify();\n-}\n-\n-bool ShenandoahControlThread::preempt_old_marking(ShenandoahGenerationType generation) {\n-  return (generation == YOUNG) && _allow_old_preemption.try_unset();\n-}\n-\n@@ -935,7 +396,5 @@\n-    \/\/ This races with the regulator thread to start a concurrent gc and the\n-    \/\/ control thread to clear it at the start of a cycle. Threads here are\n-    \/\/ allowed to escalate a heuristic's request for concurrent gc.\n-    GCCause::Cause existing = Atomic::xchg(&_requested_gc_cause, cause);\n-    if (existing != GCCause::_no_gc) {\n-      log_debug(gc, thread)(\"GC request supersedes existing request: %s\", GCCause::to_string(existing));\n-    }\n+    \/\/ Although setting gc request is under _gc_waiters_lock, but read side (run_service())\n+    \/\/ does not take the lock. We need to enforce following order, so that read side sees\n+    \/\/ latest requested gc cause when the flag is set.\n+    _requested_gc_cause = cause;\n+    _gc_requested.set();\n@@ -943,1 +402,0 @@\n-    notify_control_thread();\n@@ -951,53 +409,0 @@\n-void ShenandoahControlThread::handle_alloc_failure(ShenandoahAllocRequest& req) {\n-  ShenandoahHeap* heap = ShenandoahHeap::heap();\n-\n-  assert(current()->is_Java_thread(), \"expect Java thread here\");\n-  bool is_humongous = req.size() > ShenandoahHeapRegion::region_size_words();\n-\n-  if (try_set_alloc_failure_gc(is_humongous)) {\n-    \/\/ Only report the first allocation failure\n-    log_info(gc)(\"Failed to allocate %s, \" SIZE_FORMAT \"%s\",\n-                 req.type_string(),\n-                 byte_size_in_proper_unit(req.size() * HeapWordSize), proper_unit_for_byte_size(req.size() * HeapWordSize));\n-    \/\/ Now that alloc failure GC is scheduled, we can abort everything else\n-    heap->cancel_gc(GCCause::_allocation_failure);\n-  }\n-\n-  MonitorLocker ml(&_alloc_failure_waiters_lock);\n-  while (is_alloc_failure_gc()) {\n-    ml.wait();\n-  }\n-}\n-\n-void ShenandoahControlThread::handle_alloc_failure_evac(size_t words) {\n-  ShenandoahHeap* heap = ShenandoahHeap::heap();\n-  bool is_humongous = (words > ShenandoahHeapRegion::region_size_words());\n-\n-  if (try_set_alloc_failure_gc(is_humongous)) {\n-    \/\/ Only report the first allocation failure\n-    log_info(gc)(\"Failed to allocate \" SIZE_FORMAT \"%s for evacuation\",\n-                 byte_size_in_proper_unit(words * HeapWordSize), proper_unit_for_byte_size(words * HeapWordSize));\n-  }\n-\n-  \/\/ Forcefully report allocation failure\n-  heap->cancel_gc(GCCause::_shenandoah_allocation_failure_evac);\n-}\n-\n-void ShenandoahControlThread::notify_alloc_failure_waiters() {\n-  _alloc_failure_gc.unset();\n-  _humongous_alloc_failure_gc.unset();\n-  MonitorLocker ml(&_alloc_failure_waiters_lock);\n-  ml.notify_all();\n-}\n-\n-bool ShenandoahControlThread::try_set_alloc_failure_gc(bool is_humongous) {\n-  if (is_humongous) {\n-    _humongous_alloc_failure_gc.try_set();\n-  }\n-  return _alloc_failure_gc.try_set();\n-}\n-\n-bool ShenandoahControlThread::is_alloc_failure_gc() {\n-  return _alloc_failure_gc.is_set();\n-}\n-\n@@ -1005,0 +410,1 @@\n+  _gc_requested.unset();\n@@ -1008,86 +414,0 @@\n-\n-void ShenandoahControlThread::handle_counters_update() {\n-  if (_do_counters_update.is_set()) {\n-    _do_counters_update.unset();\n-    ShenandoahHeap::heap()->monitoring_support()->update_counters();\n-  }\n-}\n-\n-void ShenandoahControlThread::handle_force_counters_update() {\n-  if (_force_counters_update.is_set()) {\n-    _do_counters_update.unset(); \/\/ reset these too, we do update now!\n-    ShenandoahHeap::heap()->monitoring_support()->update_counters();\n-  }\n-}\n-\n-void ShenandoahControlThread::notify_heap_changed() {\n-  \/\/ This is called from allocation path, and thus should be fast.\n-\n-  \/\/ Update monitoring counters when we took a new region. This amortizes the\n-  \/\/ update costs on slow path.\n-  if (_do_counters_update.is_unset()) {\n-    _do_counters_update.set();\n-  }\n-}\n-\n-void ShenandoahControlThread::pacing_notify_alloc(size_t words) {\n-  assert(ShenandoahPacing, \"should only call when pacing is enabled\");\n-  Atomic::add(&_allocs_seen, words, memory_order_relaxed);\n-}\n-\n-void ShenandoahControlThread::set_forced_counters_update(bool value) {\n-  _force_counters_update.set_cond(value);\n-}\n-\n-void ShenandoahControlThread::reset_gc_id() {\n-  Atomic::store(&_gc_id, (size_t)0);\n-}\n-\n-void ShenandoahControlThread::update_gc_id() {\n-  Atomic::inc(&_gc_id);\n-}\n-\n-size_t ShenandoahControlThread::get_gc_id() {\n-  return Atomic::load(&_gc_id);\n-}\n-\n-void ShenandoahControlThread::start() {\n-  create_and_start();\n-}\n-\n-void ShenandoahControlThread::prepare_for_graceful_shutdown() {\n-  _graceful_shutdown.set();\n-}\n-\n-bool ShenandoahControlThread::in_graceful_shutdown() {\n-  return _graceful_shutdown.is_set();\n-}\n-\n-const char* ShenandoahControlThread::gc_mode_name(ShenandoahControlThread::GCMode mode) {\n-  switch (mode) {\n-    case none:              return \"idle\";\n-    case concurrent_normal: return \"normal\";\n-    case stw_degenerated:   return \"degenerated\";\n-    case stw_full:          return \"full\";\n-    case servicing_old:     return \"old\";\n-    case bootstrapping_old: return \"bootstrap\";\n-    default:                return \"unknown\";\n-  }\n-}\n-\n-void ShenandoahControlThread::set_gc_mode(ShenandoahControlThread::GCMode new_mode) {\n-  if (_mode != new_mode) {\n-    log_info(gc)(\"Transition from: %s to: %s\", gc_mode_name(_mode), gc_mode_name(new_mode));\n-    MonitorLocker ml(&_regulator_lock, Mutex::_no_safepoint_check_flag);\n-    _mode = new_mode;\n-    ml.notify_all();\n-  }\n-}\n-\n-ShenandoahGenerationType ShenandoahControlThread::select_global_generation() {\n-  if (ShenandoahHeap::heap()->mode()->is_generational()) {\n-    return GLOBAL_GEN;\n-  } else {\n-    return GLOBAL_NON_GEN;\n-  }\n-}\n","filename":"src\/hotspot\/share\/gc\/shenandoah\/shenandoahControlThread.cpp","additions":115,"deletions":795,"binary":false,"changes":910,"status":"modified"},{"patch":"@@ -3,1 +3,0 @@\n- * Copyright Amazon.com Inc. or its affiliates. All Rights Reserved.\n@@ -32,0 +31,1 @@\n+#include \"gc\/shenandoah\/shenandoahGenerationType.hpp\"\n@@ -35,2 +35,0 @@\n-#include \"runtime\/task.hpp\"\n-#include \"utilities\/ostream.hpp\"\n@@ -38,20 +36,1 @@\n-\/\/ Periodic task is useful for doing asynchronous things that do not require (heap) locks,\n-\/\/ or synchronization with other parts of collector. These could run even when ShenandoahConcurrentThread\n-\/\/ is busy driving the GC cycle.\n-class ShenandoahPeriodicTask : public PeriodicTask {\n-private:\n-  ShenandoahControlThread* _thread;\n-public:\n-  ShenandoahPeriodicTask(ShenandoahControlThread* thread) :\n-          PeriodicTask(100), _thread(thread) {}\n-  virtual void task();\n-};\n-\n-\/\/ Periodic task to notify blocked paced waiters.\n-class ShenandoahPeriodicPacerNotify : public PeriodicTask {\n-public:\n-  ShenandoahPeriodicPacerNotify() : PeriodicTask(PeriodicTask::min_interval) {}\n-  virtual void task();\n-};\n-\n-class ShenandoahControlThread: public ConcurrentGCThread {\n+class ShenandoahControlThread: public ShenandoahController {\n@@ -61,11 +40,0 @@\n-  \/\/ While we could have a single lock for these, it may risk unblocking\n-  \/\/ GC waiters when alloc failure GC cycle finishes. We want instead\n-  \/\/ to make complete explicit cycle for demanding customers.\n-  Monitor _alloc_failure_waiters_lock;\n-  Monitor _gc_waiters_lock;\n-  Monitor _control_lock;\n-  Monitor _regulator_lock;\n-  ShenandoahPeriodicTask _periodic_task;\n-  ShenandoahPeriodicPacerNotify _periodic_pacer_notify_task;\n-\n-public:\n@@ -76,3 +44,1 @@\n-    stw_full,\n-    bootstrapping_old,\n-    servicing_old\n+    stw_full\n@@ -81,2 +47,3 @@\n-  void run_service();\n-  void stop_service();\n+  ShenandoahSharedFlag _gc_requested;\n+  GCCause::Cause       _requested_gc_cause;\n+  ShenandoahGC::ShenandoahDegenPoint _degen_point;\n@@ -84,1 +51,2 @@\n-  size_t get_gc_id();\n+public:\n+  ShenandoahControlThread();\n@@ -86,8 +54,2 @@\n-private:\n-  ShenandoahSharedFlag _allow_old_preemption;\n-  ShenandoahSharedFlag _preemption_requested;\n-  ShenandoahSharedFlag _alloc_failure_gc;\n-  ShenandoahSharedFlag _humongous_alloc_failure_gc;\n-  ShenandoahSharedFlag _graceful_shutdown;\n-  ShenandoahSharedFlag _do_counters_update;\n-  ShenandoahSharedFlag _force_counters_update;\n+  void run_service() override;\n+  void stop_service() override;\n@@ -95,4 +57,1 @@\n-  GCCause::Cause  _requested_gc_cause;\n-  volatile ShenandoahGenerationType _requested_generation;\n-  ShenandoahGC::ShenandoahDegenPoint _degen_point;\n-  ShenandoahGeneration* _degen_generation;\n+  void request_gc(GCCause::Cause cause) override;\n@@ -100,7 +59,1 @@\n-  shenandoah_padding(0);\n-  volatile size_t _allocs_seen;\n-  shenandoah_padding(1);\n-  volatile size_t _gc_id;\n-  shenandoah_padding(2);\n-  volatile GCMode _mode;\n-  shenandoah_padding(3);\n+private:\n@@ -108,1 +61,0 @@\n-  \/\/ Returns true if the cycle has been cancelled or degenerated.\n@@ -110,4 +62,1 @@\n-\n-  \/\/ Returns true if the old generation marking completed (i.e., final mark executed for old generation).\n-  bool resume_concurrent_old_cycle(ShenandoahGeneration* generation, GCCause::Cause cause);\n-  void service_concurrent_cycle(ShenandoahGeneration* generation, GCCause::Cause cause, bool reset_old_bitmap_specially);\n+  void service_concurrent_normal_cycle(GCCause::Cause cause);\n@@ -116,13 +65,0 @@\n-  void service_uncommit(double shrink_before, size_t shrink_until);\n-\n-  \/\/ Return true if setting the flag which indicates allocation failure succeeds.\n-  bool try_set_alloc_failure_gc(bool is_humongous);\n-\n-  \/\/ Notify threads waiting for GC to complete.\n-  void notify_alloc_failure_waiters();\n-\n-  \/\/ True if allocation failure flag has been set.\n-  bool is_alloc_failure_gc();\n-\n-  void reset_gc_id();\n-  void update_gc_id();\n@@ -135,64 +71,0 @@\n-\n-  bool is_explicit_gc(GCCause::Cause cause) const;\n-  bool is_implicit_gc(GCCause::Cause cause) const;\n-\n-  \/\/ Returns true if the old generation marking was interrupted to allow a young cycle.\n-  bool preempt_old_marking(ShenandoahGenerationType generation);\n-\n-  \/\/ Returns true if the soft maximum heap has been changed using management APIs.\n-  bool check_soft_max_changed() const;\n-\n-  void process_phase_timings(const ShenandoahHeap* heap);\n-\n-public:\n-  \/\/ Constructor\n-  ShenandoahControlThread();\n-  ~ShenandoahControlThread();\n-\n-  \/\/ Handle allocation failure from normal allocation.\n-  \/\/ Blocks until memory is available.\n-  void handle_alloc_failure(ShenandoahAllocRequest& req);\n-\n-  \/\/ Handle allocation failure from evacuation path.\n-  \/\/ Optionally blocks while collector is handling the failure.\n-  void handle_alloc_failure_evac(size_t words);\n-\n-  void request_gc(GCCause::Cause cause);\n-  \/\/ Return true if the request to start a concurrent GC for the given generation succeeded.\n-  bool request_concurrent_gc(ShenandoahGenerationType generation);\n-\n-  void handle_counters_update();\n-  void handle_force_counters_update();\n-  void set_forced_counters_update(bool value);\n-\n-  void notify_heap_changed();\n-\n-  void pacing_notify_alloc(size_t words);\n-\n-  void start();\n-  void prepare_for_graceful_shutdown();\n-  bool in_graceful_shutdown();\n-\n-  void service_concurrent_normal_cycle(ShenandoahHeap* heap,\n-                                       const ShenandoahGenerationType generation,\n-                                       GCCause::Cause cause);\n-\n-  void service_concurrent_old_cycle(ShenandoahHeap* heap,\n-                                    GCCause::Cause &cause);\n-\n-  void set_gc_mode(GCMode new_mode);\n-  GCMode gc_mode() {\n-    return _mode;\n-  }\n-\n-  static ShenandoahGenerationType select_global_generation();\n-\n- private:\n-  static const char* gc_mode_name(GCMode mode);\n-  void notify_control_thread();\n-\n-  void service_concurrent_cycle(ShenandoahHeap* heap,\n-                                ShenandoahGeneration* generation,\n-                                GCCause::Cause &cause,\n-                                bool do_old_gc_bootstrap);\n-\n","filename":"src\/hotspot\/share\/gc\/shenandoah\/shenandoahControlThread.hpp","additions":13,"deletions":141,"binary":false,"changes":154,"status":"modified"},{"patch":"@@ -0,0 +1,112 @@\n+\/*\n+ * Copyright Amazon.com Inc. or its affiliates. All Rights Reserved.\n+ * DO NOT ALTER OR REMOVE COPYRIGHT NOTICES OR THIS FILE HEADER.\n+ *\n+ * This code is free software; you can redistribute it and\/or modify it\n+ * under the terms of the GNU General Public License version 2 only, as\n+ * published by the Free Software Foundation.\n+ *\n+ * This code is distributed in the hope that it will be useful, but WITHOUT\n+ * ANY WARRANTY; without even the implied warranty of MERCHANTABILITY or\n+ * FITNESS FOR A PARTICULAR PURPOSE.  See the GNU General Public License\n+ * version 2 for more details (a copy is included in the LICENSE file that\n+ * accompanied this code).\n+ *\n+ * You should have received a copy of the GNU General Public License version\n+ * 2 along with this work; if not, write to the Free Software Foundation,\n+ * Inc., 51 Franklin St, Fifth Floor, Boston, MA 02110-1301 USA.\n+ *\n+ * Please contact Oracle, 500 Oracle Parkway, Redwood Shores, CA 94065 USA\n+ * or visit www.oracle.com if you need additional information or have any\n+ * questions.\n+ *\n+ *\/\n+#include \"precompiled.hpp\"\n+\n+#include \"gc\/shared\/gc_globals.hpp\"\n+#include \"gc\/shenandoah\/shenandoahController.hpp\"\n+#include \"gc\/shenandoah\/shenandoahHeap.hpp\"\n+#include \"gc\/shenandoah\/shenandoahHeapRegion.inline.hpp\"\n+\n+void ShenandoahController::pacing_notify_alloc(size_t words) {\n+  assert(ShenandoahPacing, \"should only call when pacing is enabled\");\n+  Atomic::add(&_allocs_seen, words, memory_order_relaxed);\n+}\n+\n+size_t ShenandoahController::reset_allocs_seen() {\n+  return Atomic::xchg(&_allocs_seen, (size_t)0, memory_order_relaxed);\n+}\n+\n+void ShenandoahController::prepare_for_graceful_shutdown() {\n+  _graceful_shutdown.set();\n+}\n+\n+bool ShenandoahController::in_graceful_shutdown() {\n+  return _graceful_shutdown.is_set();\n+}\n+\n+void ShenandoahController::update_gc_id() {\n+  Atomic::inc(&_gc_id);\n+}\n+\n+size_t ShenandoahController::get_gc_id() {\n+  return Atomic::load(&_gc_id);\n+}\n+\n+void ShenandoahController::handle_alloc_failure(ShenandoahAllocRequest& req, bool block) {\n+  ShenandoahHeap* heap = ShenandoahHeap::heap();\n+\n+  assert(current()->is_Java_thread(), \"expect Java thread here\");\n+  bool is_humongous = req.size() > ShenandoahHeapRegion::humongous_threshold_words();\n+\n+  if (try_set_alloc_failure_gc(is_humongous)) {\n+    \/\/ Only report the first allocation failure\n+    log_info(gc)(\"Failed to allocate %s, \" SIZE_FORMAT \"%s\",\n+                 req.type_string(),\n+                 byte_size_in_proper_unit(req.size() * HeapWordSize), proper_unit_for_byte_size(req.size() * HeapWordSize));\n+\n+    \/\/ Now that alloc failure GC is scheduled, we can abort everything else\n+    heap->cancel_gc(GCCause::_allocation_failure);\n+  }\n+\n+\n+  if (block) {\n+    MonitorLocker ml(&_alloc_failure_waiters_lock);\n+    while (is_alloc_failure_gc()) {\n+      ml.wait();\n+    }\n+  }\n+}\n+\n+void ShenandoahController::handle_alloc_failure_evac(size_t words) {\n+  ShenandoahHeap* heap = ShenandoahHeap::heap();\n+  bool is_humongous = (words > ShenandoahHeapRegion::region_size_words());\n+\n+  if (try_set_alloc_failure_gc(is_humongous)) {\n+    \/\/ Only report the first allocation failure\n+    log_info(gc)(\"Failed to allocate \" SIZE_FORMAT \"%s for evacuation\",\n+                 byte_size_in_proper_unit(words * HeapWordSize), proper_unit_for_byte_size(words * HeapWordSize));\n+  }\n+\n+  \/\/ Forcefully report allocation failure\n+  heap->cancel_gc(GCCause::_shenandoah_allocation_failure_evac);\n+}\n+\n+void ShenandoahController::notify_alloc_failure_waiters() {\n+  _alloc_failure_gc.unset();\n+  _humongous_alloc_failure_gc.unset();\n+  MonitorLocker ml(&_alloc_failure_waiters_lock);\n+  ml.notify_all();\n+}\n+\n+bool ShenandoahController::try_set_alloc_failure_gc(bool is_humongous) {\n+  if (is_humongous) {\n+    _humongous_alloc_failure_gc.try_set();\n+  }\n+  return _alloc_failure_gc.try_set();\n+}\n+\n+bool ShenandoahController::is_alloc_failure_gc() {\n+  return _alloc_failure_gc.is_set();\n+}\n+\n","filename":"src\/hotspot\/share\/gc\/shenandoah\/shenandoahController.cpp","additions":112,"deletions":0,"binary":false,"changes":112,"status":"added"},{"patch":"@@ -0,0 +1,105 @@\n+\/*\n+ * Copyright Amazon.com Inc. or its affiliates. All Rights Reserved.\n+ * DO NOT ALTER OR REMOVE COPYRIGHT NOTICES OR THIS FILE HEADER.\n+ *\n+ * This code is free software; you can redistribute it and\/or modify it\n+ * under the terms of the GNU General Public License version 2 only, as\n+ * published by the Free Software Foundation.\n+ *\n+ * This code is distributed in the hope that it will be useful, but WITHOUT\n+ * ANY WARRANTY; without even the implied warranty of MERCHANTABILITY or\n+ * FITNESS FOR A PARTICULAR PURPOSE.  See the GNU General Public License\n+ * version 2 for more details (a copy is included in the LICENSE file that\n+ * accompanied this code).\n+ *\n+ * You should have received a copy of the GNU General Public License version\n+ * 2 along with this work; if not, write to the Free Software Foundation,\n+ * Inc., 51 Franklin St, Fifth Floor, Boston, MA 02110-1301 USA.\n+ *\n+ * Please contact Oracle, 500 Oracle Parkway, Redwood Shores, CA 94065 USA\n+ * or visit www.oracle.com if you need additional information or have any\n+ * questions.\n+ *\n+ *\/\n+\n+#ifndef LINUX_X86_64_SERVER_SLOWDEBUG_SHENANDOAHCONTROLLER_HPP\n+#define LINUX_X86_64_SERVER_SLOWDEBUG_SHENANDOAHCONTROLLER_HPP\n+\n+#include \"gc\/shared\/gcCause.hpp\"\n+#include \"gc\/shared\/concurrentGCThread.hpp\"\n+#include \"gc\/shenandoah\/shenandoahAllocRequest.hpp\"\n+#include \"gc\/shenandoah\/shenandoahSharedVariables.hpp\"\n+\n+\/**\n+ * This interface exposes methods necessary for the heap to interact\n+ * with the threads responsible for driving the collection cycle.\n+ *\/\n+class ShenandoahController: public ConcurrentGCThread {\n+private:\n+  ShenandoahSharedFlag _graceful_shutdown;\n+\n+  shenandoah_padding(0);\n+  volatile size_t _allocs_seen;\n+  shenandoah_padding(1);\n+  volatile size_t _gc_id;\n+  shenandoah_padding(2);\n+\n+protected:\n+  ShenandoahSharedFlag _alloc_failure_gc;\n+  ShenandoahSharedFlag _humongous_alloc_failure_gc;\n+\n+  \/\/ While we could have a single lock for these, it may risk unblocking\n+  \/\/ GC waiters when alloc failure GC cycle finishes. We want instead\n+  \/\/ to make complete explicit cycle for demanding customers.\n+  Monitor _alloc_failure_waiters_lock;\n+  Monitor _gc_waiters_lock;\n+\n+public:\n+  ShenandoahController():\n+    ConcurrentGCThread(),\n+    _allocs_seen(0),\n+    _gc_id(0),\n+    _alloc_failure_waiters_lock(Mutex::safepoint-2, \"ShenandoahAllocFailureGC_lock\", true),\n+    _gc_waiters_lock(Mutex::safepoint-2, \"ShenandoahRequestedGC_lock\", true)\n+  { }\n+\n+  \/\/ Request a collection cycle. This handles \"explicit\" gc requests\n+  \/\/ like System.gc and \"implicit\" gc requests, like metaspace oom.\n+  virtual void request_gc(GCCause::Cause cause) = 0;\n+\n+  \/\/ This cancels the collection cycle and has an option to block\n+  \/\/ until another cycle runs and clears the alloc failure gc flag.\n+  void handle_alloc_failure(ShenandoahAllocRequest& req, bool block);\n+\n+  \/\/ Invoked for allocation failures during evacuation. This cancels\n+  \/\/ the collection cycle without blocking.\n+  void handle_alloc_failure_evac(size_t words);\n+\n+  \/\/ Return true if setting the flag which indicates allocation failure succeeds.\n+  bool try_set_alloc_failure_gc(bool is_humongous);\n+\n+  \/\/ Notify threads waiting for GC to complete.\n+  void notify_alloc_failure_waiters();\n+\n+  \/\/ True if allocation failure flag has been set.\n+  bool is_alloc_failure_gc();\n+\n+  \/\/ This is called for every allocation. The control thread accumulates\n+  \/\/ this value when idle. During the gc cycle, the control resets it\n+  \/\/ and reports it to the pacer.\n+  void pacing_notify_alloc(size_t words);\n+  size_t reset_allocs_seen();\n+\n+  \/\/ These essentially allows to cancel a collection cycle for the\n+  \/\/ purpose of shutting down the JVM, without trying to start a degenerated\n+  \/\/ cycle.\n+  void prepare_for_graceful_shutdown();\n+  bool in_graceful_shutdown();\n+\n+\n+  \/\/ Returns the internal gc count used by the control thread. Probably\n+  \/\/ doesn't need to be exposed.\n+  size_t get_gc_id();\n+  void update_gc_id();\n+};\n+#endif \/\/LINUX_X86_64_SERVER_SLOWDEBUG_SHENANDOAHCONTROLLER_HPP\n","filename":"src\/hotspot\/share\/gc\/shenandoah\/shenandoahController.hpp","additions":105,"deletions":0,"binary":false,"changes":105,"status":"added"},{"patch":"@@ -34,0 +34,1 @@\n+#include \"gc\/shenandoah\/shenandoahGenerationalHeap.hpp\"\n@@ -293,20 +294,5 @@\n-        size_t old_region_surplus = heap->get_old_region_surplus();\n-        size_t old_region_deficit = heap->get_old_region_deficit();\n-        bool success;\n-        size_t region_xfer;\n-        const char* region_destination;\n-        if (old_region_surplus) {\n-          region_xfer = old_region_surplus;\n-          region_destination = \"young\";\n-          success = heap->generation_sizer()->transfer_to_young(old_region_surplus);\n-        } else if (old_region_deficit) {\n-          region_xfer = old_region_surplus;\n-          region_destination = \"old\";\n-          success = heap->generation_sizer()->transfer_to_old(old_region_deficit);\n-          if (!success) {\n-            heap->old_heuristics()->trigger_cannot_expand();\n-          }\n-        } else {\n-          region_destination = \"none\";\n-          region_xfer = 0;\n-          success = true;\n+        auto result = ShenandoahGenerationalHeap::heap()->balance_generations();\n+        LogTarget(Info, gc, ergo) lt;\n+        if (lt.is_enabled()) {\n+          LogStream ls(lt);\n+          result.print_on(\"Degenerated GC\", &ls);\n@@ -314,11 +300,0 @@\n-\n-        size_t young_available = heap->young_generation()->available();\n-        size_t old_available = heap->old_generation()->available();\n-        log_info(gc, ergo)(\"After cleanup, %s \" SIZE_FORMAT \" regions to %s to prepare for next gc, old available: \"\n-                           SIZE_FORMAT \"%s, young_available: \" SIZE_FORMAT \"%s\",\n-                           success? \"successfully transferred\": \"failed to transfer\", region_xfer, region_destination,\n-                           byte_size_in_proper_unit(old_available), proper_unit_for_byte_size(old_available),\n-                           byte_size_in_proper_unit(young_available), proper_unit_for_byte_size(young_available));\n-\n-        heap->set_old_region_surplus(0);\n-        heap->set_old_region_deficit(0);\n@@ -334,3 +309,1 @@\n-    heap->set_young_evac_reserve(0);\n-    heap->set_old_evac_reserve(0);\n-    heap->set_promoted_reserve(0);\n+    ShenandoahGenerationalHeap::heap()->reset_generation_reserves();\n@@ -400,3 +373,1 @@\n-  size_t humongous_regions_promoted = heap->get_promotable_humongous_regions();\n-  size_t regular_regions_promoted_in_place = heap->get_regular_regions_promoted_in_place();\n-  if (!heap->collection_set()->is_empty() || (humongous_regions_promoted + regular_regions_promoted_in_place > 0)) {\n+  if (!heap->collection_set()->is_empty() || heap->old_generation()->has_in_place_promotions()) {\n@@ -506,1 +477,1 @@\n-  log_info(gc)(\"Degenerate GC upgrading to Full GC\");\n+  log_info(gc)(\"Degenerated GC upgrading to Full GC\");\n","filename":"src\/hotspot\/share\/gc\/shenandoah\/shenandoahDegeneratedGC.cpp","additions":9,"deletions":38,"binary":false,"changes":47,"status":"modified"},{"patch":"@@ -65,0 +65,1 @@\n+  \/\/ Turns this degenerated cycle into a full gc without leaving the safepoint\n","filename":"src\/hotspot\/share\/gc\/shenandoah\/shenandoahDegeneratedGC.hpp","additions":1,"deletions":0,"binary":false,"changes":1,"status":"modified"},{"patch":"@@ -167,7 +167,3 @@\n-  assert (((region_capacity <= _region_size_bytes) &&\n-           ((orig_set == Mutator) && (new_set == Collector)) ||\n-           ((orig_set == Collector) && (new_set == Mutator))) ||\n-          ((region_capacity == _region_size_bytes) &&\n-           ((orig_set == Mutator) && (new_set == Collector)) ||\n-           ((orig_set == OldCollector) && (new_set == Mutator)) ||\n-           (new_set == OldCollector)), \"Unexpected movement between sets\");\n+  assert((region_capacity <= _region_size_bytes && ((orig_set == Mutator && new_set == Collector) || (orig_set == Collector && new_set == Mutator)))\n+      || (region_capacity == _region_size_bytes && ((orig_set == Mutator && new_set == Collector) || (orig_set == OldCollector && new_set == Mutator) || new_set == OldCollector)),\n+      \"Unexpected movement between sets\");\n@@ -471,1 +467,1 @@\n-    _heap->augment_promo_reserve(capacity);\n+    _heap->old_generation()->augment_promoted_reserve(capacity);\n@@ -1006,1 +1002,1 @@\n-  _heap->augment_old_evac_reserve(region_capacity);\n+  _heap->old_generation()->augment_evacuation_reserve(region_capacity);\n@@ -1168,1 +1164,1 @@\n-void ShenandoahFreeSet::rebuild(size_t young_cset_regions, size_t old_cset_regions) {\n+void ShenandoahFreeSet::rebuild(size_t young_cset_regions, size_t old_cset_regions, bool have_evacuation_reserves) {\n@@ -1173,6 +1169,8 @@\n-  size_t old_capacity = _heap->old_generation()->max_capacity();\n-  size_t old_available = _heap->old_generation()->available();\n-  size_t old_unaffiliated_regions = _heap->old_generation()->free_unaffiliated_regions();\n-  size_t young_capacity = _heap->young_generation()->max_capacity();\n-  size_t young_available = _heap->young_generation()->available();\n-  size_t young_unaffiliated_regions = _heap->young_generation()->free_unaffiliated_regions();\n+  ShenandoahOldGeneration* old_generation = _heap->old_generation();\n+  size_t old_capacity = old_generation->max_capacity();\n+  size_t old_available = old_generation->available();\n+  size_t old_unaffiliated_regions = old_generation->free_unaffiliated_regions();\n+  ShenandoahYoungGeneration* young_generation = _heap->young_generation();\n+  size_t young_capacity = young_generation->max_capacity();\n+  size_t young_available = young_generation->available();\n+  size_t young_unaffiliated_regions = young_generation->free_unaffiliated_regions();\n@@ -1187,2 +1185,2 @@\n-  size_t old_region_surplus = _heap->get_old_region_surplus();\n-  size_t old_region_deficit = _heap->get_old_region_deficit();\n+  size_t old_region_surplus = old_generation->get_region_surplus();\n+  size_t old_region_deficit = old_generation->get_region_deficit();\n@@ -1220,1 +1218,1 @@\n-    if (_heap->has_evacuation_reserve_quantities()) {\n+    if (have_evacuation_reserves) {\n@@ -1222,2 +1220,5 @@\n-      young_reserve = _heap->get_young_evac_reserve();\n-      old_reserve = _heap->get_promoted_reserve() + _heap->get_old_evac_reserve();\n+\n+      size_t promoted_reserve = old_generation->get_promoted_reserve();\n+      size_t old_evac_reserve = old_generation->get_evacuation_reserve();\n+      young_reserve = young_generation->get_evacuation_reserve();\n+      old_reserve = promoted_reserve + old_evac_reserve;\n@@ -1226,1 +1227,1 @@\n-             _heap->get_promoted_reserve(), _heap->get_old_evac_reserve(), old_available);\n+             promoted_reserve, old_evac_reserve, old_available);\n","filename":"src\/hotspot\/share\/gc\/shenandoah\/shenandoahFreeSet.cpp","additions":23,"deletions":22,"binary":false,"changes":45,"status":"modified"},{"patch":"@@ -195,1 +195,15 @@\n-  void rebuild(size_t young_cset_regions, size_t old_cset_regions);\n+\n+  \/\/ At the end of final mark, but before we begin evacuating, heuristics calculate how much memory is required to\n+  \/\/ hold the results of evacuating to young-gen and to old-gen.  These quantities, stored in reserves for their,\n+  \/\/ respective generations, are consulted prior to rebuilding the free set (ShenandoahFreeSet) in preparation for\n+  \/\/ evacuation.  When the free set is rebuilt, we make sure to reserve sufficient memory in the collector and\n+  \/\/ old_collector sets to hold evacuations, if have_evacuation_reserves is true.  The other time we rebuild the free\n+  \/\/ set is at the end of GC, as we prepare to idle GC until the next trigger.  In this case, have_evacuation_reserves\n+  \/\/ is false because we don't yet know how much memory will need to be evacuated in the next GC cycle.  When\n+  \/\/ have_evacuation_reserves is false, the free set rebuild operation reserves for the collector and old_collector sets\n+  \/\/ based on alternative mechanisms, such as ShenandoahEvacReserve, ShenandoahOldEvacReserve, and\n+  \/\/ ShenandoahOldCompactionReserve.  In a future planned enhancement, the reserve for old_collector set when the\n+  \/\/ evacuation reserves are unknown, is based in part on anticipated promotion as determined by analysis of live data\n+  \/\/ found during the previous GC pass which is one less than the current tenure age.\n+  void rebuild(size_t young_cset_regions, size_t old_cset_regions, bool have_evacuation_reserves = false);\n+\n","filename":"src\/hotspot\/share\/gc\/shenandoah\/shenandoahFreeSet.hpp","additions":15,"deletions":1,"binary":false,"changes":16,"status":"modified"},{"patch":"@@ -35,0 +35,1 @@\n+#include \"gc\/shenandoah\/shenandoahCollectorPolicy.hpp\"\n@@ -40,0 +41,1 @@\n+#include \"gc\/shenandoah\/shenandoahGenerationalFullGC.hpp\"\n@@ -49,1 +51,0 @@\n-#include \"gc\/shenandoah\/shenandoahOldGeneration.hpp\"\n@@ -58,1 +59,0 @@\n-#include \"gc\/shenandoah\/shenandoahYoungGeneration.hpp\"\n@@ -63,1 +63,0 @@\n-#include \"runtime\/javaThread.hpp\"\n@@ -70,63 +69,0 @@\n-\/\/ After Full GC is done, reconstruct the remembered set by iterating over OLD regions,\n-\/\/ registering all objects between bottom() and top(), and setting remembered set cards to\n-\/\/ DIRTY if they hold interesting pointers.\n-class ShenandoahReconstructRememberedSetTask : public WorkerTask {\n-private:\n-  ShenandoahRegionIterator _regions;\n-\n-public:\n-  ShenandoahReconstructRememberedSetTask() :\n-    WorkerTask(\"Shenandoah Reset Bitmap\") { }\n-\n-  void work(uint worker_id) {\n-    ShenandoahParallelWorkerSession worker_session(worker_id);\n-    ShenandoahHeapRegion* r = _regions.next();\n-    ShenandoahHeap* heap = ShenandoahHeap::heap();\n-    RememberedScanner* scanner = heap->card_scan();\n-    ShenandoahSetRememberedCardsToDirtyClosure dirty_cards_for_interesting_pointers;\n-\n-    while (r != nullptr) {\n-      if (r->is_old() && r->is_active()) {\n-        HeapWord* obj_addr = r->bottom();\n-        if (r->is_humongous_start()) {\n-          \/\/ First, clear the remembered set\n-          oop obj = cast_to_oop(obj_addr);\n-          size_t size = obj->size();\n-\n-          \/\/ First, clear the remembered set for all spanned humongous regions\n-          size_t num_regions = ShenandoahHeapRegion::required_regions(size * HeapWordSize);\n-          size_t region_span = num_regions * ShenandoahHeapRegion::region_size_words();\n-          scanner->reset_remset(r->bottom(), region_span);\n-          size_t region_index = r->index();\n-          ShenandoahHeapRegion* humongous_region = heap->get_region(region_index);\n-          while (num_regions-- != 0) {\n-            scanner->reset_object_range(humongous_region->bottom(), humongous_region->end());\n-            region_index++;\n-            humongous_region = heap->get_region(region_index);\n-          }\n-\n-          \/\/ Then register the humongous object and DIRTY relevant remembered set cards\n-          scanner->register_object_without_lock(obj_addr);\n-          obj->oop_iterate(&dirty_cards_for_interesting_pointers);\n-        } else if (!r->is_humongous()) {\n-          \/\/ First, clear the remembered set\n-          scanner->reset_remset(r->bottom(), ShenandoahHeapRegion::region_size_words());\n-          scanner->reset_object_range(r->bottom(), r->end());\n-\n-          \/\/ Then iterate over all objects, registering object and DIRTYing relevant remembered set cards\n-          HeapWord* t = r->top();\n-          while (obj_addr < t) {\n-            oop obj = cast_to_oop(obj_addr);\n-            size_t size = obj->size();\n-            scanner->register_object_without_lock(obj_addr);\n-            obj_addr += obj->oop_iterate_size(&dirty_cards_for_interesting_pointers);\n-          }\n-        } \/\/ else, ignore humongous continuation region\n-      }\n-      \/\/ else, this region is FREE or YOUNG or inactive and we can ignore it.\n-      \/\/ TODO: Assert this.\n-      r = _regions.next();\n-    }\n-  }\n-};\n-\n@@ -170,1 +106,0 @@\n-  ShenandoahHeap* const heap = ShenandoahHeap::heap();\n@@ -177,1 +112,2 @@\n-  metrics.snap_after();\n+  ShenandoahHeap* const heap = ShenandoahHeap::heap();\n+\n@@ -179,25 +115,1 @@\n-    \/\/ Full GC should reset time since last gc for young and old heuristics\n-    heap->young_generation()->heuristics()->record_cycle_end();\n-    heap->old_generation()->heuristics()->record_cycle_end();\n-\n-    heap->mmu_tracker()->record_full(GCId::current());\n-    heap->log_heap_status(\"At end of Full GC\");\n-\n-    assert(heap->old_generation()->state() == ShenandoahOldGeneration::WAITING_FOR_BOOTSTRAP,\n-           \"After full GC, old generation should be waiting for bootstrap.\");\n-\n-    \/\/ Since we allow temporary violation of these constraints during Full GC, we want to enforce that the assertions are\n-    \/\/ made valid by the time Full GC completes.\n-    assert(heap->old_generation()->used_regions_size() <= heap->old_generation()->max_capacity(),\n-           \"Old generation affiliated regions must be less than capacity\");\n-    assert(heap->young_generation()->used_regions_size() <= heap->young_generation()->max_capacity(),\n-           \"Young generation affiliated regions must be less than capacity\");\n-\n-    assert((heap->young_generation()->used() + heap->young_generation()->get_humongous_waste())\n-           <= heap->young_generation()->used_regions_size(), \"Young consumed can be no larger than span of affiliated regions\");\n-    assert((heap->old_generation()->used() + heap->old_generation()->get_humongous_waste())\n-           <= heap->old_generation()->used_regions_size(), \"Old consumed can be no larger than span of affiliated regions\");\n-\n-    \/\/ Establish baseline for next old-has-grown trigger.\n-    heap->old_generation()->set_live_bytes_after_last_mark(heap->old_generation()->used() +\n-                                                           heap->old_generation()->get_humongous_waste());\n+    ShenandoahGenerationalFullGC::handle_completion(heap);\n@@ -205,0 +117,3 @@\n+\n+  metrics.snap_after();\n+\n@@ -206,1 +121,1 @@\n-    ShenandoahHeap::heap()->notify_gc_progress();\n+    heap->notify_gc_progress();\n@@ -210,1 +125,1 @@\n-    ShenandoahHeap::heap()->notify_gc_no_progress();\n+    heap->notify_gc_no_progress();\n@@ -220,2 +135,0 @@\n-  \/\/ Since we may arrive here from degenerated GC failure of either young or old, establish generation as GLOBAL.\n-  heap->set_gc_generation(heap->global_generation());\n@@ -224,7 +137,1 @@\n-    \/\/ No need for old_gen->increase_used() as this was done when plabs were allocated.\n-    heap->set_young_evac_reserve(0);\n-    heap->set_old_evac_reserve(0);\n-    heap->set_promoted_reserve(0);\n-\n-    \/\/ Full GC supersedes any marking or coalescing in old generation.\n-    heap->cancel_old_gc();\n+    ShenandoahGenerationalFullGC::prepare();\n@@ -277,0 +184,1 @@\n+      \/\/ TODO: Send cancel_concurrent_mark upstream? Does it really not have it already?\n@@ -299,6 +207,1 @@\n-      for (size_t i = 0; i < heap->num_regions(); i++) {\n-        ShenandoahHeapRegion* r = heap->get_region(i);\n-        if (r->get_top_before_promote() != nullptr) {\n-          r->restore_top_before_promote();\n-        }\n-      }\n+      ShenandoahGenerationalFullGC::restore_top_before_promote(heap);\n@@ -355,13 +258,0 @@\n-  {\n-    \/\/ Epilogue\n-    \/\/ TODO: Merge with phase5_epilog?\n-    _preserved_marks->restore(heap->workers());\n-    _preserved_marks->reclaim();\n-\n-    if (heap->mode()->is_generational()) {\n-      ShenandoahGCPhase phase(ShenandoahPhaseTimings::full_gc_reconstruct_remembered_set);\n-      ShenandoahReconstructRememberedSetTask task;\n-      heap->workers()->run_task(&task);\n-    }\n-  }\n-\n@@ -403,0 +293,1 @@\n+    \/\/ TODO: Add API to heap to skip free regions\n@@ -431,6 +322,2 @@\n-  size_t live_bytes_in_old = 0;\n-  for (size_t i = 0; i < heap->num_regions(); i++) {\n-    ShenandoahHeapRegion* r = heap->get_region(i);\n-    if (r->is_old()) {\n-      live_bytes_in_old += r->get_live_data_bytes();\n-    }\n+  if (ShenandoahHeap::heap()->mode()->is_generational()) {\n+    ShenandoahGenerationalFullGC::log_live_in_old(heap);\n@@ -438,1 +325,0 @@\n-  log_info(gc)(\"Live bytes in old after STW mark: \" PROPERFMT, PROPERFMTARGS(live_bytes_in_old));\n@@ -441,235 +327,0 @@\n-class ShenandoahPrepareForCompactionTask : public WorkerTask {\n-private:\n-  PreservedMarksSet*        const _preserved_marks;\n-  ShenandoahHeap*           const _heap;\n-  ShenandoahHeapRegionSet** const _worker_slices;\n-  size_t                    const _num_workers;\n-\n-public:\n-  ShenandoahPrepareForCompactionTask(PreservedMarksSet *preserved_marks,\n-                                     ShenandoahHeapRegionSet **worker_slices,\n-                                     size_t num_workers);\n-\n-  static bool is_candidate_region(ShenandoahHeapRegion* r) {\n-    \/\/ Empty region: get it into the slice to defragment the slice itself.\n-    \/\/ We could have skipped this without violating correctness, but we really\n-    \/\/ want to compact all live regions to the start of the heap, which sometimes\n-    \/\/ means moving them into the fully empty regions.\n-    if (r->is_empty()) return true;\n-\n-    \/\/ Can move the region, and this is not the humongous region. Humongous\n-    \/\/ moves are special cased here, because their moves are handled separately.\n-    return r->is_stw_move_allowed() && !r->is_humongous();\n-  }\n-\n-  void work(uint worker_id);\n-};\n-\n-class ShenandoahPrepareForGenerationalCompactionObjectClosure : public ObjectClosure {\n-private:\n-  PreservedMarks*          const _preserved_marks;\n-  ShenandoahHeap*          const _heap;\n-  uint                           _tenuring_threshold;\n-\n-  \/\/ _empty_regions is a thread-local list of heap regions that have been completely emptied by this worker thread's\n-  \/\/ compaction efforts.  The worker thread that drives these efforts adds compacted regions to this list if the\n-  \/\/ region has not been compacted onto itself.\n-  GrowableArray<ShenandoahHeapRegion*>& _empty_regions;\n-  int _empty_regions_pos;\n-  ShenandoahHeapRegion*          _old_to_region;\n-  ShenandoahHeapRegion*          _young_to_region;\n-  ShenandoahHeapRegion*          _from_region;\n-  ShenandoahAffiliation          _from_affiliation;\n-  HeapWord*                      _old_compact_point;\n-  HeapWord*                      _young_compact_point;\n-  uint                           _worker_id;\n-\n-public:\n-  ShenandoahPrepareForGenerationalCompactionObjectClosure(PreservedMarks* preserved_marks,\n-                                                          GrowableArray<ShenandoahHeapRegion*>& empty_regions,\n-                                                          ShenandoahHeapRegion* old_to_region,\n-                                                          ShenandoahHeapRegion* young_to_region, uint worker_id) :\n-      _preserved_marks(preserved_marks),\n-      _heap(ShenandoahHeap::heap()),\n-      _tenuring_threshold(0),\n-      _empty_regions(empty_regions),\n-      _empty_regions_pos(0),\n-      _old_to_region(old_to_region),\n-      _young_to_region(young_to_region),\n-      _from_region(nullptr),\n-      _old_compact_point((old_to_region != nullptr)? old_to_region->bottom(): nullptr),\n-      _young_compact_point((young_to_region != nullptr)? young_to_region->bottom(): nullptr),\n-      _worker_id(worker_id) {\n-    if (_heap->mode()->is_generational()) {\n-      _tenuring_threshold = _heap->age_census()->tenuring_threshold();\n-    }\n-  }\n-\n-  void set_from_region(ShenandoahHeapRegion* from_region) {\n-    _from_region = from_region;\n-    _from_affiliation = from_region->affiliation();\n-    if (_from_region->has_live()) {\n-      if (_from_affiliation == ShenandoahAffiliation::OLD_GENERATION) {\n-        if (_old_to_region == nullptr) {\n-          _old_to_region = from_region;\n-          _old_compact_point = from_region->bottom();\n-        }\n-      } else {\n-        assert(_from_affiliation == ShenandoahAffiliation::YOUNG_GENERATION, \"from_region must be OLD or YOUNG\");\n-        if (_young_to_region == nullptr) {\n-          _young_to_region = from_region;\n-          _young_compact_point = from_region->bottom();\n-        }\n-      }\n-    } \/\/ else, we won't iterate over this _from_region so we don't need to set up to region to hold copies\n-  }\n-\n-  void finish() {\n-    finish_old_region();\n-    finish_young_region();\n-  }\n-\n-  void finish_old_region() {\n-    if (_old_to_region != nullptr) {\n-      log_debug(gc)(\"Planned compaction into Old Region \" SIZE_FORMAT \", used: \" SIZE_FORMAT \" tabulated by worker %u\",\n-                    _old_to_region->index(), _old_compact_point - _old_to_region->bottom(), _worker_id);\n-      _old_to_region->set_new_top(_old_compact_point);\n-      _old_to_region = nullptr;\n-    }\n-  }\n-\n-  void finish_young_region() {\n-    if (_young_to_region != nullptr) {\n-      log_debug(gc)(\"Worker %u planned compaction into Young Region \" SIZE_FORMAT \", used: \" SIZE_FORMAT,\n-                    _worker_id, _young_to_region->index(), _young_compact_point - _young_to_region->bottom());\n-      _young_to_region->set_new_top(_young_compact_point);\n-      _young_to_region = nullptr;\n-    }\n-  }\n-\n-  bool is_compact_same_region() {\n-    return (_from_region == _old_to_region) || (_from_region == _young_to_region);\n-  }\n-\n-  int empty_regions_pos() {\n-    return _empty_regions_pos;\n-  }\n-\n-  void do_object(oop p) {\n-    assert(_from_region != nullptr, \"must set before work\");\n-    assert((_from_region->bottom() <= cast_from_oop<HeapWord*>(p)) && (cast_from_oop<HeapWord*>(p) < _from_region->top()),\n-           \"Object must reside in _from_region\");\n-    assert(_heap->complete_marking_context()->is_marked(p), \"must be marked\");\n-    assert(!_heap->complete_marking_context()->allocated_after_mark_start(p), \"must be truly marked\");\n-\n-    size_t obj_size = p->size();\n-    uint from_region_age = _from_region->age();\n-    uint object_age = p->age();\n-\n-    bool promote_object = false;\n-    if ((_from_affiliation == ShenandoahAffiliation::YOUNG_GENERATION) &&\n-        (from_region_age + object_age >= _tenuring_threshold)) {\n-      if ((_old_to_region != nullptr) && (_old_compact_point + obj_size > _old_to_region->end())) {\n-        finish_old_region();\n-        _old_to_region = nullptr;\n-      }\n-      if (_old_to_region == nullptr) {\n-        if (_empty_regions_pos < _empty_regions.length()) {\n-          ShenandoahHeapRegion* new_to_region = _empty_regions.at(_empty_regions_pos);\n-          _empty_regions_pos++;\n-          new_to_region->set_affiliation(OLD_GENERATION);\n-          _old_to_region = new_to_region;\n-          _old_compact_point = _old_to_region->bottom();\n-          promote_object = true;\n-        }\n-        \/\/ Else this worker thread does not yet have any empty regions into which this aged object can be promoted so\n-        \/\/ we leave promote_object as false, deferring the promotion.\n-      } else {\n-        promote_object = true;\n-      }\n-    }\n-\n-    if (promote_object || (_from_affiliation == ShenandoahAffiliation::OLD_GENERATION)) {\n-      assert(_old_to_region != nullptr, \"_old_to_region should not be nullptr when evacuating to OLD region\");\n-      if (_old_compact_point + obj_size > _old_to_region->end()) {\n-        ShenandoahHeapRegion* new_to_region;\n-\n-        log_debug(gc)(\"Worker %u finishing old region \" SIZE_FORMAT \", compact_point: \" PTR_FORMAT \", obj_size: \" SIZE_FORMAT\n-                      \", &compact_point[obj_size]: \" PTR_FORMAT \", region end: \" PTR_FORMAT,  _worker_id, _old_to_region->index(),\n-                      p2i(_old_compact_point), obj_size, p2i(_old_compact_point + obj_size), p2i(_old_to_region->end()));\n-\n-        \/\/ Object does not fit.  Get a new _old_to_region.\n-        finish_old_region();\n-        if (_empty_regions_pos < _empty_regions.length()) {\n-          new_to_region = _empty_regions.at(_empty_regions_pos);\n-          _empty_regions_pos++;\n-          new_to_region->set_affiliation(OLD_GENERATION);\n-        } else {\n-          \/\/ If we've exhausted the previously selected _old_to_region, we know that the _old_to_region is distinct\n-          \/\/ from _from_region.  That's because there is always room for _from_region to be compacted into itself.\n-          \/\/ Since we're out of empty regions, let's use _from_region to hold the results of its own compaction.\n-          new_to_region = _from_region;\n-        }\n-\n-        assert(new_to_region != _old_to_region, \"must not reuse same OLD to-region\");\n-        assert(new_to_region != nullptr, \"must not be nullptr\");\n-        _old_to_region = new_to_region;\n-        _old_compact_point = _old_to_region->bottom();\n-      }\n-\n-      \/\/ Object fits into current region, record new location:\n-      assert(_old_compact_point + obj_size <= _old_to_region->end(), \"must fit\");\n-      shenandoah_assert_not_forwarded(nullptr, p);\n-      _preserved_marks->push_if_necessary(p, p->mark());\n-      p->forward_to(cast_to_oop(_old_compact_point));\n-      _old_compact_point += obj_size;\n-    } else {\n-      assert(_from_affiliation == ShenandoahAffiliation::YOUNG_GENERATION,\n-             \"_from_region must be OLD_GENERATION or YOUNG_GENERATION\");\n-      assert(_young_to_region != nullptr, \"_young_to_region should not be nullptr when compacting YOUNG _from_region\");\n-\n-      \/\/ After full gc compaction, all regions have age 0.  Embed the region's age into the object's age in order to preserve\n-      \/\/ tenuring progress.\n-      if (_heap->is_aging_cycle()) {\n-        _heap->increase_object_age(p, from_region_age + 1);\n-      } else {\n-        _heap->increase_object_age(p, from_region_age);\n-      }\n-\n-      if (_young_compact_point + obj_size > _young_to_region->end()) {\n-        ShenandoahHeapRegion* new_to_region;\n-\n-        log_debug(gc)(\"Worker %u finishing young region \" SIZE_FORMAT \", compact_point: \" PTR_FORMAT \", obj_size: \" SIZE_FORMAT\n-                      \", &compact_point[obj_size]: \" PTR_FORMAT \", region end: \" PTR_FORMAT,  _worker_id, _young_to_region->index(),\n-                      p2i(_young_compact_point), obj_size, p2i(_young_compact_point + obj_size), p2i(_young_to_region->end()));\n-\n-        \/\/ Object does not fit.  Get a new _young_to_region.\n-        finish_young_region();\n-        if (_empty_regions_pos < _empty_regions.length()) {\n-          new_to_region = _empty_regions.at(_empty_regions_pos);\n-          _empty_regions_pos++;\n-          new_to_region->set_affiliation(YOUNG_GENERATION);\n-        } else {\n-          \/\/ If we've exhausted the previously selected _young_to_region, we know that the _young_to_region is distinct\n-          \/\/ from _from_region.  That's because there is always room for _from_region to be compacted into itself.\n-          \/\/ Since we're out of empty regions, let's use _from_region to hold the results of its own compaction.\n-          new_to_region = _from_region;\n-        }\n-\n-        assert(new_to_region != _young_to_region, \"must not reuse same OLD to-region\");\n-        assert(new_to_region != nullptr, \"must not be nullptr\");\n-        _young_to_region = new_to_region;\n-        _young_compact_point = _young_to_region->bottom();\n-      }\n-\n-      \/\/ Object fits into current region, record new location:\n-      assert(_young_compact_point + obj_size <= _young_to_region->end(), \"must fit\");\n-      shenandoah_assert_not_forwarded(nullptr, p);\n-      _preserved_marks->push_if_necessary(p, p->mark());\n-      p->forward_to(cast_to_oop(_young_compact_point));\n-      _young_compact_point += obj_size;\n-    }\n-  }\n-};\n-\n-\n@@ -702,1 +353,1 @@\n-  void finish_region() {\n+  void finish() {\n@@ -704,1 +355,0 @@\n-    assert(!_heap->mode()->is_generational(), \"Generational GC should use different Closure\");\n@@ -723,1 +373,1 @@\n-      finish_region();\n+      finish();\n@@ -741,1 +391,1 @@\n-    \/\/ Object fits into current region, record new location:\n+    \/\/ Object fits into current region, record new location, if object does not move:\n@@ -744,2 +394,4 @@\n-    _preserved_marks->push_if_necessary(p, p->mark());\n-    p->forward_to(cast_to_oop(_compact_point));\n+    if (_compact_point != cast_from_oop<HeapWord*>(p)) {\n+      _preserved_marks->push_if_necessary(p, p->mark());\n+      p->forward_to(cast_to_oop(_compact_point));\n+    }\n@@ -750,0 +402,5 @@\n+class ShenandoahPrepareForCompactionTask : public WorkerTask {\n+private:\n+  PreservedMarksSet*        const _preserved_marks;\n+  ShenandoahHeap*           const _heap;\n+  ShenandoahHeapRegionSet** const _worker_slices;\n@@ -751,3 +408,2 @@\n-ShenandoahPrepareForCompactionTask::ShenandoahPrepareForCompactionTask(PreservedMarksSet *preserved_marks,\n-                                                                       ShenandoahHeapRegionSet **worker_slices,\n-                                                                       size_t num_workers) :\n+public:\n+  ShenandoahPrepareForCompactionTask(PreservedMarksSet *preserved_marks, ShenandoahHeapRegionSet **worker_slices) :\n@@ -755,2 +411,10 @@\n-    _preserved_marks(preserved_marks), _heap(ShenandoahHeap::heap()),\n-    _worker_slices(worker_slices), _num_workers(num_workers) { }\n+    _preserved_marks(preserved_marks),\n+    _heap(ShenandoahHeap::heap()), _worker_slices(worker_slices) {\n+  }\n+\n+  static bool is_candidate_region(ShenandoahHeapRegion* r) {\n+    \/\/ Empty region: get it into the slice to defragment the slice itself.\n+    \/\/ We could have skipped this without violating correctness, but we really\n+    \/\/ want to compact all live regions to the start of the heap, which sometimes\n+    \/\/ means moving them into the fully empty regions.\n+    if (r->is_empty()) return true;\n@@ -758,0 +422,13 @@\n+    \/\/ Can move the region, and this is not the humongous region. Humongous\n+    \/\/ moves are special cased here, because their moves are handled separately.\n+    return r->is_stw_move_allowed() && !r->is_humongous();\n+  }\n+\n+  void work(uint worker_id) override;\n+private:\n+  template<typename ClosureType>\n+  void prepare_for_compaction(ClosureType& cl,\n+                              GrowableArray<ShenandoahHeapRegion*>& empty_regions,\n+                              ShenandoahHeapRegionSetIterator& it,\n+                              ShenandoahHeapRegion* from_region);\n+};\n@@ -776,2 +453,0 @@\n-    ShenandoahHeapRegion* old_to_region = (from_region->is_old())? from_region: nullptr;\n-    ShenandoahHeapRegion* young_to_region = (from_region->is_young())? from_region: nullptr;\n@@ -779,25 +454,2 @@\n-                                                               empty_regions,\n-                                                               old_to_region, young_to_region,\n-                                                               worker_id);\n-    while (from_region != nullptr) {\n-      assert(is_candidate_region(from_region), \"Sanity\");\n-      log_debug(gc)(\"Worker %u compacting %s Region \" SIZE_FORMAT \" which had used \" SIZE_FORMAT \" and %s live\",\n-                    worker_id, from_region->affiliation_name(),\n-                    from_region->index(), from_region->used(), from_region->has_live()? \"has\": \"does not have\");\n-      cl.set_from_region(from_region);\n-      if (from_region->has_live()) {\n-        _heap->marked_object_iterate(from_region, &cl);\n-      }\n-      \/\/ Compacted the region to somewhere else? From-region is empty then.\n-      if (!cl.is_compact_same_region()) {\n-        empty_regions.append(from_region);\n-      }\n-      from_region = it.next();\n-    }\n-    cl.finish();\n-\n-    \/\/ Mark all remaining regions as empty\n-    for (int pos = cl.empty_regions_pos(); pos < empty_regions.length(); ++pos) {\n-      ShenandoahHeapRegion* r = empty_regions.at(pos);\n-      r->set_new_top(r->bottom());\n-    }\n+                                                               empty_regions, from_region, worker_id);\n+    prepare_for_compaction(cl, empty_regions, it, from_region);\n@@ -806,6 +458,3 @@\n-    while (from_region != nullptr) {\n-      assert(is_candidate_region(from_region), \"Sanity\");\n-      cl.set_from_region(from_region);\n-      if (from_region->has_live()) {\n-        _heap->marked_object_iterate(from_region, &cl);\n-      }\n+    prepare_for_compaction(cl, empty_regions, it, from_region);\n+  }\n+}\n@@ -813,5 +462,10 @@\n-      \/\/ Compacted the region to somewhere else? From-region is empty then.\n-      if (!cl.is_compact_same_region()) {\n-        empty_regions.append(from_region);\n-      }\n-      from_region = it.next();\n+template<typename ClosureType>\n+void ShenandoahPrepareForCompactionTask::prepare_for_compaction(ClosureType& cl,\n+                                                                GrowableArray<ShenandoahHeapRegion*>& empty_regions,\n+                                                                ShenandoahHeapRegionSetIterator& it,\n+                                                                ShenandoahHeapRegion* from_region) {\n+  while (from_region != nullptr) {\n+    assert(is_candidate_region(from_region), \"Sanity\");\n+    cl.set_from_region(from_region);\n+    if (from_region->has_live()) {\n+      _heap->marked_object_iterate(from_region, &cl);\n@@ -819,1 +473,0 @@\n-    cl.finish_region();\n@@ -821,4 +474,3 @@\n-    \/\/ Mark all remaining regions as empty\n-    for (int pos = cl.empty_regions_pos(); pos < empty_regions.length(); ++pos) {\n-      ShenandoahHeapRegion* r = empty_regions.at(pos);\n-      r->set_new_top(r->bottom());\n+    \/\/ Compacted the region to somewhere else? From-region is empty then.\n+    if (!cl.is_compact_same_region()) {\n+      empty_regions.append(from_region);\n@@ -826,0 +478,8 @@\n+    from_region = it.next();\n+  }\n+  cl.finish();\n+\n+  \/\/ Mark all remaining regions as empty\n+  for (int pos = cl.empty_regions_pos(); pos < empty_regions.length(); ++pos) {\n+    ShenandoahHeapRegion* r = empty_regions.at(pos);\n+    r->set_new_top(r->bottom());\n@@ -924,3 +584,1 @@\n-               \"Humongous Start %s Region \" SIZE_FORMAT \" is not marked, should not have live\",\n-               r->affiliation_name(),  r->index());\n-        log_debug(gc)(\"Trashing immediate humongous region \" SIZE_FORMAT \" because not marked\", r->index());\n+               \"Region \" SIZE_FORMAT \" is not marked, should not have live\", r->index());\n@@ -930,1 +588,1 @@\n-               \"Humongous Start %s Region \" SIZE_FORMAT \" should have live\", r->affiliation_name(),  r->index());\n+               \"Region \" SIZE_FORMAT \" should have live\", r->index());\n@@ -935,1 +593,1 @@\n-             \"Humongous Continuation %s Region \" SIZE_FORMAT \" should have live\", r->affiliation_name(),  r->index());\n+             \"Region \" SIZE_FORMAT \" should have live\", r->index());\n@@ -938,1 +596,0 @@\n-        log_debug(gc)(\"Trashing immediate regular region \" SIZE_FORMAT \" because has no live\", r->index());\n@@ -1119,2 +776,1 @@\n-    size_t num_workers = heap->max_workers();\n-\n+    \/\/ TODO: This is ResourceMark is missing upstream.\n@@ -1122,1 +778,1 @@\n-    ShenandoahPrepareForCompactionTask task(_preserved_marks, worker_slices, num_workers);\n+    ShenandoahPrepareForCompactionTask task(_preserved_marks, worker_slices);\n@@ -1196,6 +852,2 @@\n-      if (r->is_pinned() && r->is_old() && r->is_active() && !r->is_humongous()) {\n-        \/\/ Pinned regions are not compacted so they may still hold unmarked objects with\n-        \/\/ reference to reclaimed memory. Remembered set scanning will crash if it attempts\n-        \/\/ to iterate the oops in these objects.\n-        r->begin_preemptible_coalesce_and_fill();\n-        r->oop_fill_and_coalesce_without_cancel();\n+      if (_heap->mode()->is_generational()) {\n+        ShenandoahGenerationalFullGC::maybe_coalesce_and_fill_region(r);\n@@ -1265,0 +917,1 @@\n+      assert(compact_from != compact_to, \"Forwarded object should move\");\n@@ -1303,17 +956,0 @@\n-static void account_for_region(ShenandoahHeapRegion* r, size_t &region_count, size_t &region_usage, size_t &humongous_waste) {\n-  region_count++;\n-  region_usage += r->used();\n-  if (r->is_humongous_start()) {\n-    \/\/ For each humongous object, we take this path once regardless of how many regions it spans.\n-    HeapWord* obj_addr = r->bottom();\n-    oop obj = cast_to_oop(obj_addr);\n-    size_t word_size = obj->size();\n-    size_t region_size_words = ShenandoahHeapRegion::region_size_words();\n-    size_t overreach = word_size % region_size_words;\n-    if (overreach != 0) {\n-      humongous_waste += (region_size_words - overreach) * HeapWordSize;\n-    }\n-    \/\/ else, this humongous object aligns exactly on region size, so no waste.\n-  }\n-}\n-\n@@ -1377,1 +1013,1 @@\n-        account_for_region(r, _old_regions, _old_usage, _old_humongous_waste);\n+        ShenandoahGenerationalFullGC::account_for_region(r, _old_regions, _old_usage, _old_humongous_waste);\n@@ -1379,1 +1015,1 @@\n-        account_for_region(r, _young_regions, _young_usage, _young_humongous_waste);\n+        ShenandoahGenerationalFullGC::account_for_region(r, _young_regions, _young_usage, _young_humongous_waste);\n@@ -1431,7 +1067,3 @@\n-      ContinuationGCSupport::relativize_stack_chunk(cast_to_oop<HeapWord*>(heap->get_region(old_start)->bottom()));\n-      log_debug(gc)(\"Full GC compaction moves humongous object from region \" SIZE_FORMAT \" to region \" SIZE_FORMAT,\n-                    old_start, new_start);\n-\n-      Copy::aligned_conjoint_words(heap->get_region(old_start)->bottom(),\n-                                   heap->get_region(new_start)->bottom(),\n-                                   words_size);\n+      log_debug(gc)(\"Full GC compaction moves humongous object from region \" SIZE_FORMAT \" to region \" SIZE_FORMAT, old_start, new_start);\n+      Copy::aligned_conjoint_words(r->bottom(), heap->get_region(new_start)->bottom(), words_size);\n+      ContinuationGCSupport::relativize_stack_chunk(cast_to_oop<HeapWord*>(r->bottom()));\n@@ -1542,14 +1174,0 @@\n-    if (heap->mode()->is_generational()) {\n-      size_t old_usage = heap->old_generation()->used_regions_size();\n-      size_t old_capacity = heap->old_generation()->max_capacity();\n-\n-      assert(old_usage % ShenandoahHeapRegion::region_size_bytes() == 0, \"Old usage must aligh with region size\");\n-      assert(old_capacity % ShenandoahHeapRegion::region_size_bytes() == 0, \"Old capacity must aligh with region size\");\n-\n-      if (old_capacity > old_usage) {\n-        size_t excess_old_regions = (old_capacity - old_usage) \/ ShenandoahHeapRegion::region_size_bytes();\n-        heap->generation_sizer()->transfer_to_young(excess_old_regions);\n-      } else if (old_capacity < old_usage) {\n-        size_t old_regions_deficit = (old_usage - old_capacity) \/ ShenandoahHeapRegion::region_size_bytes();\n-        heap->generation_sizer()->force_transfer_to_old(old_regions_deficit);\n-      }\n@@ -1557,3 +1175,2 @@\n-      log_info(gc)(\"FullGC done: young usage: \" SIZE_FORMAT \"%s, old usage: \" SIZE_FORMAT \"%s\",\n-                   byte_size_in_proper_unit(heap->young_generation()->used()), proper_unit_for_byte_size(heap->young_generation()->used()),\n-                   byte_size_in_proper_unit(heap->old_generation()->used()),   proper_unit_for_byte_size(heap->old_generation()->used()));\n+    if (heap->mode()->is_generational()) {\n+      ShenandoahGenerationalFullGC::balance_generations_after_gc(heap);\n@@ -1561,0 +1178,1 @@\n+\n@@ -1574,2 +1192,0 @@\n-    \/\/ In case this Full GC resulted from degeneration, clear the tally on anticipated promotion.\n-    heap->clear_promotion_potential();\n@@ -1578,2 +1194,1 @@\n-      \/\/ Invoke this in case we are able to transfer memory from OLD to YOUNG.\n-      heap->adjust_generation_sizes_for_next_cycle(0, 0, 0);\n+      ShenandoahGenerationalFullGC::compute_balances();\n@@ -1581,0 +1196,1 @@\n+\n@@ -1586,33 +1202,2 @@\n-      bool success;\n-      size_t region_xfer;\n-      const char* region_destination;\n-      ShenandoahYoungGeneration* young_gen = heap->young_generation();\n-      ShenandoahGeneration* old_gen = heap->old_generation();\n-\n-      size_t old_region_surplus = heap->get_old_region_surplus();\n-      size_t old_region_deficit = heap->get_old_region_deficit();\n-      if (old_region_surplus) {\n-        success = heap->generation_sizer()->transfer_to_young(old_region_surplus);\n-        region_destination = \"young\";\n-        region_xfer = old_region_surplus;\n-      } else if (old_region_deficit) {\n-        success = heap->generation_sizer()->transfer_to_old(old_region_deficit);\n-        region_destination = \"old\";\n-        region_xfer = old_region_deficit;\n-        if (!success) {\n-          ((ShenandoahOldHeuristics *) old_gen->heuristics())->trigger_cannot_expand();\n-        }\n-      } else {\n-        region_destination = \"none\";\n-        region_xfer = 0;\n-        success = true;\n-      }\n-      heap->set_old_region_surplus(0);\n-      heap->set_old_region_deficit(0);\n-      size_t young_available = young_gen->available();\n-      size_t old_available = old_gen->available();\n-      log_info(gc, ergo)(\"After cleanup, %s \" SIZE_FORMAT \" regions to %s to prepare for next gc, old available: \"\n-                         SIZE_FORMAT \"%s, young_available: \" SIZE_FORMAT \"%s\",\n-                         success? \"successfully transferred\": \"failed to transfer\", region_xfer, region_destination,\n-                         byte_size_in_proper_unit(old_available), proper_unit_for_byte_size(old_available),\n-                         byte_size_in_proper_unit(young_available), proper_unit_for_byte_size(young_available));\n+      ShenandoahGenerationalFullGC::balance_generations_after_rebuilding_free_set();\n+      ShenandoahGenerationalFullGC::rebuild_remembered_set(heap);\n@@ -1622,0 +1207,3 @@\n+\n+  _preserved_marks->restore(heap->workers());\n+  _preserved_marks->reclaim();\n","filename":"src\/hotspot\/share\/gc\/shenandoah\/shenandoahFullGC.cpp","additions":102,"deletions":514,"binary":false,"changes":616,"status":"modified"},{"patch":"@@ -30,1 +30,1 @@\n-#include \"gc\/shenandoah\/shenandoahHeap.hpp\"\n+#include \"gc\/shenandoah\/shenandoahGenerationalHeap.hpp\"\n@@ -154,0 +154,12 @@\n+void ShenandoahGeneration::set_evacuation_reserve(size_t new_val) {\n+  _evacuation_reserve = new_val;\n+}\n+\n+size_t ShenandoahGeneration::get_evacuation_reserve() const {\n+  return _evacuation_reserve;\n+}\n+\n+void ShenandoahGeneration::augment_evacuation_reserve(size_t increment) {\n+  _evacuation_reserve += increment;\n+}\n+\n@@ -234,1 +246,1 @@\n-  ShenandoahGeneration* const old_generation = heap->old_generation();\n+  ShenandoahOldGeneration* const old_generation = heap->old_generation();\n@@ -338,4 +350,3 @@\n-\n-  heap->set_young_evac_reserve(young_evacuation_reserve);\n-  heap->set_old_evac_reserve(old_evacuation_reserve);\n-  heap->set_promoted_reserve(consumed_by_advance_promotion);\n+  young_generation->set_evacuation_reserve(young_evacuation_reserve);\n+  old_generation->set_evacuation_reserve(old_evacuation_reserve);\n+  old_generation->set_promoted_reserve(consumed_by_advance_promotion);\n@@ -367,2 +378,2 @@\n-  const ShenandoahOldGeneration* const old_generation = heap->old_generation();\n-  const ShenandoahYoungGeneration* const young_generation = heap->young_generation();\n+  ShenandoahOldGeneration* const old_generation = heap->old_generation();\n+  ShenandoahYoungGeneration* const young_generation = heap->young_generation();\n@@ -372,1 +383,1 @@\n-  size_t old_evacuation_reserve = heap->get_old_evac_reserve();\n+  size_t old_evacuation_reserve = old_generation->get_evacuation_reserve();\n@@ -384,1 +395,1 @@\n-    heap->set_old_evac_reserve(old_evacuation_reserve);\n+    old_generation->set_evacuation_reserve(old_evacuation_reserve);\n@@ -395,1 +406,1 @@\n-  heap->set_young_evac_reserve(young_evacuated_reserve_used);\n+  young_generation->set_evacuation_reserve(young_evacuated_reserve_used);\n@@ -460,2 +471,2 @@\n-  heap->set_promoted_reserve(total_promotion_reserve);\n-  heap->reset_promoted_expended();\n+  old_generation->set_promoted_reserve(total_promotion_reserve);\n+  old_generation->reset_promoted_expended();\n@@ -511,2 +522,1 @@\n-  ShenandoahHeap* const heap = ShenandoahHeap::heap();\n-  assert(heap->mode()->is_generational(), \"Only in generational mode\");\n+  auto const heap = ShenandoahGenerationalHeap::heap();\n@@ -637,2 +647,3 @@\n-  heap->set_pad_for_promote_in_place(promote_in_place_pad);\n-  heap->set_promotion_potential(promo_potential);\n+\n+  heap->old_generation()->set_pad_for_promote_in_place(promote_in_place_pad);\n+  heap->old_generation()->set_promotion_potential(promo_potential);\n@@ -735,2 +746,1 @@\n-  \/\/ Freeset construction uses reserve quantities if they are valid\n-  heap->set_evacuation_reserve_quantities(true);\n+\n@@ -746,1 +756,2 @@\n-    heap->free_set()->rebuild(young_cset_regions, old_cset_regions);\n+    \/\/ Free set construction uses reserve quantities, because they are known to be valid here\n+    heap->free_set()->rebuild(young_cset_regions, old_cset_regions, true);\n@@ -748,1 +759,0 @@\n-  heap->set_evacuation_reserve_quantities(false);\n","filename":"src\/hotspot\/share\/gc\/shenandoah\/shenandoahGeneration.cpp","additions":31,"deletions":21,"binary":false,"changes":52,"status":"modified"},{"patch":"@@ -62,0 +62,3 @@\n+  \/\/ Bytes reserved within this generation to hold evacuated objects from the collection set\n+  size_t _evacuation_reserve;\n+\n@@ -108,1 +111,6 @@\n-  bool is_global() const { return _type == GLOBAL_GEN || _type == GLOBAL_NON_GEN; }\n+  bool is_global() const { return _type == GLOBAL || _type == NON_GEN; }\n+\n+  \/\/ see description in field declaration\n+  void set_evacuation_reserve(size_t new_val);\n+  size_t get_evacuation_reserve() const;\n+  void augment_evacuation_reserve(size_t increment);\n@@ -126,0 +134,3 @@\n+  size_t used_including_humongous_waste() const {\n+    return used() + get_humongous_waste();\n+  }\n","filename":"src\/hotspot\/share\/gc\/shenandoah\/shenandoahGeneration.hpp","additions":12,"deletions":1,"binary":false,"changes":13,"status":"modified"},{"patch":"@@ -29,4 +29,4 @@\n-    GLOBAL_NON_GEN,  \/\/ Global, non-generational\n-    GLOBAL_GEN,      \/\/ Global, generational\n-    YOUNG,           \/\/ Young,  generational\n-    OLD              \/\/ Old,    generational\n+    NON_GEN,         \/\/ non-generational\n+    GLOBAL,          \/\/ generational: Global\n+    YOUNG,           \/\/ generational: Young\n+    OLD              \/\/ generational: Old\n@@ -37,3 +37,3 @@\n-    case GLOBAL_NON_GEN:\n-      return \"\";\n-    case GLOBAL_GEN:\n+    case NON_GEN:\n+      return \"Non-Generational\";\n+    case GLOBAL:\n@@ -47,1 +47,1 @@\n-      return \"?\";\n+      return \"Unknown\";\n","filename":"src\/hotspot\/share\/gc\/shenandoah\/shenandoahGenerationType.hpp","additions":8,"deletions":8,"binary":false,"changes":16,"status":"modified"},{"patch":"@@ -0,0 +1,841 @@\n+\/*\n+ * Copyright (c) 2013, 2021, Red Hat, Inc. All rights reserved.\n+ * Copyright (C) 2022 THL A29 Limited, a Tencent company. All rights reserved.\n+ * Copyright Amazon.com Inc. or its affiliates. All Rights Reserved.\n+ * DO NOT ALTER OR REMOVE COPYRIGHT NOTICES OR THIS FILE HEADER.\n+ *\n+ * This code is free software; you can redistribute it and\/or modify it\n+ * under the terms of the GNU General Public License version 2 only, as\n+ * published by the Free Software Foundation.\n+ *\n+ * This code is distributed in the hope that it will be useful, but WITHOUT\n+ * ANY WARRANTY; without even the implied warranty of MERCHANTABILITY or\n+ * FITNESS FOR A PARTICULAR PURPOSE.  See the GNU General Public License\n+ * version 2 for more details (a copy is included in the LICENSE file that\n+ * accompanied this code).\n+ *\n+ * You should have received a copy of the GNU General Public License version\n+ * 2 along with this work; if not, write to the Free Software Foundation,\n+ * Inc., 51 Franklin St, Fifth Floor, Boston, MA 02110-1301 USA.\n+ *\n+ * Please contact Oracle, 500 Oracle Parkway, Redwood Shores, CA 94065 USA\n+ * or visit www.oracle.com if you need additional information or have any\n+ * questions.\n+ *\n+ *\/\n+\n+#include \"precompiled.hpp\"\n+#include \"gc\/shenandoah\/mode\/shenandoahMode.hpp\"\n+#include \"gc\/shenandoah\/shenandoahAsserts.hpp\"\n+#include \"gc\/shenandoah\/shenandoahCollectorPolicy.hpp\"\n+#include \"gc\/shenandoah\/shenandoahConcurrentGC.hpp\"\n+#include \"gc\/shenandoah\/shenandoahGenerationalControlThread.hpp\"\n+#include \"gc\/shenandoah\/shenandoahDegeneratedGC.hpp\"\n+#include \"gc\/shenandoah\/shenandoahFreeSet.hpp\"\n+#include \"gc\/shenandoah\/shenandoahFullGC.hpp\"\n+#include \"gc\/shenandoah\/shenandoahGeneration.hpp\"\n+#include \"gc\/shenandoah\/shenandoahOldGC.hpp\"\n+#include \"gc\/shenandoah\/shenandoahOldGeneration.hpp\"\n+#include \"gc\/shenandoah\/shenandoahHeap.inline.hpp\"\n+#include \"gc\/shenandoah\/shenandoahMonitoringSupport.hpp\"\n+#include \"gc\/shenandoah\/shenandoahPacer.inline.hpp\"\n+#include \"gc\/shenandoah\/shenandoahUtils.hpp\"\n+#include \"gc\/shenandoah\/shenandoahYoungGeneration.hpp\"\n+#include \"logging\/log.hpp\"\n+#include \"memory\/metaspaceUtils.hpp\"\n+#include \"memory\/metaspaceStats.hpp\"\n+#include \"runtime\/atomic.hpp\"\n+\n+ShenandoahGenerationalControlThread::ShenandoahGenerationalControlThread() :\n+  ShenandoahController(),\n+  _control_lock(Mutex::nosafepoint - 2, \"ShenandoahControlGC_lock\", true),\n+  _regulator_lock(Mutex::nosafepoint - 2, \"ShenandoahRegulatorGC_lock\", true),\n+  _requested_gc_cause(GCCause::_no_gc),\n+  _requested_generation(GLOBAL),\n+  _degen_point(ShenandoahGC::_degenerated_outside_cycle),\n+  _degen_generation(nullptr),\n+  _mode(none) {\n+  shenandoah_assert_generational();\n+  set_name(\"Shenandoah Control Thread\");\n+  create_and_start();\n+}\n+\n+void ShenandoahGenerationalControlThread::run_service() {\n+  ShenandoahHeap* const heap = ShenandoahHeap::heap();\n+\n+  const GCMode default_mode = concurrent_normal;\n+  ShenandoahGenerationType generation = GLOBAL;\n+\n+  double last_shrink_time = os::elapsedTime();\n+  uint age_period = 0;\n+\n+  \/\/ Shrink period avoids constantly polling regions for shrinking.\n+  \/\/ Having a period 10x lower than the delay would mean we hit the\n+  \/\/ shrinking with lag of less than 1\/10-th of true delay.\n+  \/\/ ShenandoahUncommitDelay is in msecs, but shrink_period is in seconds.\n+  const double shrink_period = (double)ShenandoahUncommitDelay \/ 1000 \/ 10;\n+\n+  ShenandoahCollectorPolicy* const policy = heap->shenandoah_policy();\n+\n+  \/\/ Heuristics are notified of allocation failures here and other outcomes\n+  \/\/ of the cycle. They're also used here to control whether the Nth consecutive\n+  \/\/ degenerated cycle should be 'promoted' to a full cycle. The decision to\n+  \/\/ trigger a cycle or not is evaluated on the regulator thread.\n+  ShenandoahHeuristics* global_heuristics = heap->global_generation()->heuristics();\n+  while (!in_graceful_shutdown() && !should_terminate()) {\n+    \/\/ Figure out if we have pending requests.\n+    const bool alloc_failure_pending = _alloc_failure_gc.is_set();\n+    const bool humongous_alloc_failure_pending = _humongous_alloc_failure_gc.is_set();\n+\n+    GCCause::Cause cause = Atomic::xchg(&_requested_gc_cause, GCCause::_no_gc);\n+\n+    const bool is_gc_requested = ShenandoahCollectorPolicy::is_requested_gc(cause);\n+\n+    \/\/ This control loop iteration has seen this much allocation.\n+    const size_t allocs_seen = reset_allocs_seen();\n+\n+    \/\/ Check if we have seen a new target for soft max heap size.\n+    const bool soft_max_changed = heap->check_soft_max_changed();\n+\n+    \/\/ Choose which GC mode to run in. The block below should select a single mode.\n+    set_gc_mode(none);\n+    ShenandoahGC::ShenandoahDegenPoint degen_point = ShenandoahGC::_degenerated_unset;\n+\n+    if (alloc_failure_pending) {\n+      \/\/ Allocation failure takes precedence: we have to deal with it first thing\n+      log_info(gc)(\"Trigger: Handle Allocation Failure\");\n+\n+      cause = GCCause::_allocation_failure;\n+\n+      \/\/ Consume the degen point, and seed it with default value\n+      degen_point = _degen_point;\n+      _degen_point = ShenandoahGC::_degenerated_outside_cycle;\n+\n+      if (degen_point == ShenandoahGC::_degenerated_outside_cycle) {\n+        _degen_generation = heap->young_generation();\n+      } else {\n+        assert(_degen_generation != nullptr, \"Need to know which generation to resume\");\n+      }\n+\n+      ShenandoahHeuristics* heuristics = _degen_generation->heuristics();\n+      generation = _degen_generation->type();\n+      bool old_gen_evacuation_failed = heap->old_generation()->clear_failed_evacuation();\n+\n+      \/\/ Do not bother with degenerated cycle if old generation evacuation failed or if humongous allocation failed\n+      if (ShenandoahDegeneratedGC && heuristics->should_degenerate_cycle() &&\n+          !old_gen_evacuation_failed && !humongous_alloc_failure_pending) {\n+        heuristics->record_allocation_failure_gc();\n+        policy->record_alloc_failure_to_degenerated(degen_point);\n+        set_gc_mode(stw_degenerated);\n+      } else {\n+        \/\/ TODO: if humongous_alloc_failure_pending, there might be value in trying a \"compacting\" degen before\n+        \/\/ going all the way to full.  But it's a lot of work to implement this, and it may not provide value.\n+        \/\/ A compacting degen can move young regions around without doing full old-gen mark (relying upon the\n+        \/\/ remembered set scan), so it might be faster than a full gc.\n+        \/\/\n+        \/\/ Longer term, think about how to defragment humongous memory concurrently.\n+\n+        heuristics->record_allocation_failure_gc();\n+        policy->record_alloc_failure_to_full();\n+        generation = GLOBAL;\n+        set_gc_mode(stw_full);\n+      }\n+    } else if (is_gc_requested) {\n+      generation = GLOBAL;\n+      log_info(gc)(\"Trigger: GC request (%s)\", GCCause::to_string(cause));\n+      global_heuristics->record_requested_gc();\n+\n+      if (ShenandoahCollectorPolicy::should_run_full_gc(cause)) {\n+        set_gc_mode(stw_full);\n+      } else {\n+        set_gc_mode(default_mode);\n+        \/\/ Unload and clean up everything\n+        heap->set_unload_classes(global_heuristics->can_unload_classes());\n+      }\n+    } else {\n+      \/\/ We should only be here if the regulator requested a cycle or if\n+      \/\/ there is an old generation mark in progress.\n+      if (cause == GCCause::_shenandoah_concurrent_gc) {\n+        if (_requested_generation == OLD && heap->doing_mixed_evacuations()) {\n+          \/\/ If a request to start an old cycle arrived while an old cycle was running, but _before_\n+          \/\/ it chose any regions for evacuation we don't want to start a new old cycle. Rather, we want\n+          \/\/ the heuristic to run a young collection so that we can evacuate some old regions.\n+          assert(!heap->is_concurrent_old_mark_in_progress(), \"Should not be running mixed collections and concurrent marking\");\n+          generation = YOUNG;\n+        } else {\n+          generation = _requested_generation;\n+        }\n+\n+        \/\/ preemption was requested or this is a regular cycle\n+        set_gc_mode(default_mode);\n+\n+        \/\/ Don't start a new old marking if there is one already in progress\n+        if (generation == OLD && heap->is_concurrent_old_mark_in_progress()) {\n+          set_gc_mode(servicing_old);\n+        }\n+\n+        if (generation == GLOBAL) {\n+          heap->set_unload_classes(global_heuristics->should_unload_classes());\n+        } else {\n+          heap->set_unload_classes(false);\n+        }\n+      } else if (heap->is_concurrent_old_mark_in_progress() || heap->is_prepare_for_old_mark_in_progress()) {\n+        \/\/ Nobody asked us to do anything, but we have an old-generation mark or old-generation preparation for\n+        \/\/ mixed evacuation in progress, so resume working on that.\n+        log_info(gc)(\"Resume old GC: marking is%s in progress, preparing is%s in progress\",\n+                     heap->is_concurrent_old_mark_in_progress() ? \"\" : \" NOT\",\n+                     heap->is_prepare_for_old_mark_in_progress() ? \"\" : \" NOT\");\n+\n+        cause = GCCause::_shenandoah_concurrent_gc;\n+        generation = OLD;\n+        set_gc_mode(servicing_old);\n+        heap->set_unload_classes(false);\n+      }\n+    }\n+\n+    const bool gc_requested = (gc_mode() != none);\n+    assert (!gc_requested || cause != GCCause::_no_gc, \"GC cause should be set\");\n+\n+    if (gc_requested) {\n+      \/\/ Blow away all soft references on this cycle, if handling allocation failure,\n+      \/\/ either implicit or explicit GC request, or we are requested to do so unconditionally.\n+      if (generation == GLOBAL && (alloc_failure_pending || is_gc_requested || ShenandoahAlwaysClearSoftRefs)) {\n+        heap->soft_ref_policy()->set_should_clear_all_soft_refs(true);\n+      }\n+\n+      \/\/ GC is starting, bump the internal ID\n+      update_gc_id();\n+\n+      heap->reset_bytes_allocated_since_gc_start();\n+\n+      MetaspaceCombinedStats meta_sizes = MetaspaceUtils::get_combined_statistics();\n+\n+      \/\/ If GC was requested, we are sampling the counters even without actual triggers\n+      \/\/ from allocation machinery. This captures GC phases more accurately.\n+      heap->set_forced_counters_update(true);\n+\n+      \/\/ If GC was requested, we better dump freeset data for performance debugging\n+      {\n+        ShenandoahHeapLocker locker(heap->lock());\n+        heap->free_set()->log_status();\n+      }\n+      \/\/ In case this is a degenerated cycle, remember whether original cycle was aging.\n+      const bool was_aging_cycle = heap->is_aging_cycle();\n+      heap->set_aging_cycle(false);\n+\n+      switch (gc_mode()) {\n+        case concurrent_normal: {\n+          \/\/ At this point:\n+          \/\/  if (generation == YOUNG), this is a normal YOUNG cycle\n+          \/\/  if (generation == OLD), this is a bootstrap OLD cycle\n+          \/\/  if (generation == GLOBAL), this is a GLOBAL cycle triggered by System.gc()\n+          \/\/ In all three cases, we want to age old objects if this is an aging cycle\n+          if (age_period-- == 0) {\n+             heap->set_aging_cycle(true);\n+             age_period = ShenandoahAgingCyclePeriod - 1;\n+          }\n+          service_concurrent_normal_cycle(heap, generation, cause);\n+          break;\n+        }\n+        case stw_degenerated: {\n+          heap->set_aging_cycle(was_aging_cycle);\n+          service_stw_degenerated_cycle(cause, degen_point);\n+          break;\n+        }\n+        case stw_full: {\n+          if (age_period-- == 0) {\n+            heap->set_aging_cycle(true);\n+            age_period = ShenandoahAgingCyclePeriod - 1;\n+          }\n+          service_stw_full_cycle(cause);\n+          break;\n+        }\n+        case servicing_old: {\n+          assert(generation == OLD, \"Expected old generation here\");\n+          GCIdMark gc_id_mark;\n+          service_concurrent_old_cycle(heap, cause);\n+          break;\n+        }\n+        default:\n+          ShouldNotReachHere();\n+      }\n+\n+      \/\/ If this was the requested GC cycle, notify waiters about it\n+      if (is_gc_requested) {\n+        notify_gc_waiters();\n+      }\n+\n+      \/\/ If this was the allocation failure GC cycle, notify waiters about it\n+      if (alloc_failure_pending) {\n+        notify_alloc_failure_waiters();\n+      }\n+\n+      \/\/ Report current free set state at the end of cycle, whether\n+      \/\/ it is a normal completion, or the abort.\n+      {\n+        ShenandoahHeapLocker locker(heap->lock());\n+        heap->free_set()->log_status();\n+\n+        \/\/ Notify Universe about new heap usage. This has implications for\n+        \/\/ global soft refs policy, and we better report it every time heap\n+        \/\/ usage goes down.\n+        heap->update_capacity_and_used_at_gc();\n+\n+        \/\/ Signal that we have completed a visit to all live objects.\n+        heap->record_whole_heap_examined_timestamp();\n+      }\n+\n+      \/\/ Disable forced counters update, and update counters one more time\n+      \/\/ to capture the state at the end of GC session.\n+      heap->handle_force_counters_update();\n+      heap->set_forced_counters_update(false);\n+\n+      \/\/ Retract forceful part of soft refs policy\n+      heap->soft_ref_policy()->set_should_clear_all_soft_refs(false);\n+\n+      \/\/ Clear metaspace oom flag, if current cycle unloaded classes\n+      if (heap->unload_classes()) {\n+        global_heuristics->clear_metaspace_oom();\n+      }\n+\n+      process_phase_timings(heap);\n+\n+      \/\/ Print Metaspace change following GC (if logging is enabled).\n+      MetaspaceUtils::print_metaspace_change(meta_sizes);\n+\n+      \/\/ GC is over, we are at idle now\n+      if (ShenandoahPacing) {\n+        heap->pacer()->setup_for_idle();\n+      }\n+    } else {\n+      \/\/ Report to pacer that we have seen this many words allocated\n+      if (ShenandoahPacing && (allocs_seen > 0)) {\n+        heap->pacer()->report_alloc(allocs_seen);\n+      }\n+    }\n+\n+    const double current = os::elapsedTime();\n+\n+    if (ShenandoahUncommit && (is_gc_requested || soft_max_changed || (current - last_shrink_time > shrink_period))) {\n+      \/\/ Explicit GC tries to uncommit everything down to min capacity.\n+      \/\/ Soft max change tries to uncommit everything down to target capacity.\n+      \/\/ Periodic uncommit tries to uncommit suitable regions down to min capacity.\n+\n+      double shrink_before = (is_gc_requested || soft_max_changed) ?\n+                             current :\n+                             current - (ShenandoahUncommitDelay \/ 1000.0);\n+\n+      size_t shrink_until = soft_max_changed ?\n+                             heap->soft_max_capacity() :\n+                             heap->min_capacity();\n+\n+      heap->maybe_uncommit(shrink_before, shrink_until);\n+      heap->phase_timings()->flush_cycle_to_global();\n+      last_shrink_time = current;\n+    }\n+\n+    \/\/ Wait for ShenandoahControlIntervalMax unless there was an allocation failure or another request was made mid-cycle.\n+    if (!is_alloc_failure_gc() && _requested_gc_cause == GCCause::_no_gc) {\n+      \/\/ The timed wait is necessary because this thread has a responsibility to send\n+      \/\/ 'alloc_words' to the pacer when it does not perform a GC.\n+      MonitorLocker lock(&_control_lock, Mutex::_no_safepoint_check_flag);\n+      lock.wait(ShenandoahControlIntervalMax);\n+    }\n+  }\n+\n+  \/\/ Wait for the actual stop(), can't leave run_service() earlier.\n+  while (!should_terminate()) {\n+    os::naked_short_sleep(ShenandoahControlIntervalMin);\n+  }\n+}\n+\n+void ShenandoahGenerationalControlThread::process_phase_timings(const ShenandoahHeap* heap) {\n+  \/\/ Commit worker statistics to cycle data\n+  heap->phase_timings()->flush_par_workers_to_cycle();\n+  if (ShenandoahPacing) {\n+    heap->pacer()->flush_stats_to_cycle();\n+  }\n+\n+  ShenandoahEvacuationTracker* evac_tracker = heap->evac_tracker();\n+  ShenandoahCycleStats         evac_stats   = evac_tracker->flush_cycle_to_global();\n+\n+  \/\/ Print GC stats for current cycle\n+  {\n+    LogTarget(Info, gc, stats) lt;\n+    if (lt.is_enabled()) {\n+      ResourceMark rm;\n+      LogStream ls(lt);\n+      heap->phase_timings()->print_cycle_on(&ls);\n+      evac_tracker->print_evacuations_on(&ls, &evac_stats.workers,\n+                                              &evac_stats.mutators);\n+      if (ShenandoahPacing) {\n+        heap->pacer()->print_cycle_on(&ls);\n+      }\n+    }\n+  }\n+\n+  \/\/ Commit statistics to globals\n+  heap->phase_timings()->flush_cycle_to_global();\n+}\n+\n+\/\/ Young and old concurrent cycles are initiated by the regulator. Implicit\n+\/\/ and explicit GC requests are handled by the controller thread and always\n+\/\/ run a global cycle (which is concurrent by default, but may be overridden\n+\/\/ by command line options). Old cycles always degenerate to a global cycle.\n+\/\/ Young cycles are degenerated to complete the young cycle.  Young\n+\/\/ and old degen may upgrade to Full GC.  Full GC may also be\n+\/\/ triggered directly by a System.gc() invocation.\n+\/\/\n+\/\/\n+\/\/      +-----+ Idle +-----+-----------+---------------------+\n+\/\/      |         +        |           |                     |\n+\/\/      |         |        |           |                     |\n+\/\/      |         |        v           |                     |\n+\/\/      |         |  Bootstrap Old +-- | ------------+       |\n+\/\/      |         |   +                |             |       |\n+\/\/      |         |   |                |             |       |\n+\/\/      |         v   v                v             v       |\n+\/\/      |    Resume Old <----------+ Young +--> Young Degen  |\n+\/\/      |     +  +   ^                            +  +       |\n+\/\/      v     |  |   |                            |  |       |\n+\/\/   Global <-+  |   +----------------------------+  |       |\n+\/\/      +        |                                   |       |\n+\/\/      |        v                                   v       |\n+\/\/      +--->  Global Degen +--------------------> Full <----+\n+\/\/\n+void ShenandoahGenerationalControlThread::service_concurrent_normal_cycle(ShenandoahHeap* heap,\n+                                                              const ShenandoahGenerationType generation,\n+                                                              GCCause::Cause cause) {\n+  GCIdMark gc_id_mark;\n+  switch (generation) {\n+    case YOUNG: {\n+      \/\/ Run a young cycle. This might or might not, have interrupted an ongoing\n+      \/\/ concurrent mark in the old generation. We need to think about promotions\n+      \/\/ in this case. Promoted objects should be above the TAMS in the old regions\n+      \/\/ they end up in, but we have to be sure we don't promote into any regions\n+      \/\/ that are in the cset.\n+      log_info(gc, ergo)(\"Start GC cycle (YOUNG)\");\n+      service_concurrent_cycle(heap->young_generation(), cause, false);\n+      break;\n+    }\n+    case OLD: {\n+      log_info(gc, ergo)(\"Start GC cycle (OLD)\");\n+      service_concurrent_old_cycle(heap, cause);\n+      break;\n+    }\n+    case GLOBAL: {\n+      log_info(gc, ergo)(\"Start GC cycle (GLOBAL)\");\n+      service_concurrent_cycle(heap->global_generation(), cause, false);\n+      break;\n+    }\n+    default:\n+      ShouldNotReachHere();\n+  }\n+}\n+\n+void ShenandoahGenerationalControlThread::service_concurrent_old_cycle(ShenandoahHeap* heap, GCCause::Cause &cause) {\n+  ShenandoahOldGeneration* old_generation = heap->old_generation();\n+  ShenandoahYoungGeneration* young_generation = heap->young_generation();\n+  ShenandoahOldGeneration::State original_state = old_generation->state();\n+\n+  TraceCollectorStats tcs(heap->monitoring_support()->concurrent_collection_counters());\n+\n+  switch (original_state) {\n+    case ShenandoahOldGeneration::FILLING: {\n+      _allow_old_preemption.set();\n+      old_generation->entry_coalesce_and_fill();\n+      _allow_old_preemption.unset();\n+\n+      \/\/ Before bootstrapping begins, we must acknowledge any cancellation request.\n+      \/\/ If the gc has not been cancelled, this does nothing. If it has been cancelled,\n+      \/\/ this will clear the cancellation request and exit before starting the bootstrap\n+      \/\/ phase. This will allow the young GC cycle to proceed normally. If we do not\n+      \/\/ acknowledge the cancellation request, the subsequent young cycle will observe\n+      \/\/ the request and essentially cancel itself.\n+      if (check_cancellation_or_degen(ShenandoahGC::_degenerated_outside_cycle)) {\n+        log_info(gc)(\"Preparation for old generation cycle was cancelled\");\n+        return;\n+      }\n+\n+      \/\/ Coalescing threads completed and nothing was cancelled. it is safe to transition from this state.\n+      old_generation->transition_to(ShenandoahOldGeneration::WAITING_FOR_BOOTSTRAP);\n+      return;\n+    }\n+    case ShenandoahOldGeneration::WAITING_FOR_BOOTSTRAP:\n+      old_generation->transition_to(ShenandoahOldGeneration::BOOTSTRAPPING);\n+    case ShenandoahOldGeneration::BOOTSTRAPPING: {\n+      \/\/ Configure the young generation's concurrent mark to put objects in\n+      \/\/ old regions into the concurrent mark queues associated with the old\n+      \/\/ generation. The young cycle will run as normal except that rather than\n+      \/\/ ignore old references it will mark and enqueue them in the old concurrent\n+      \/\/ task queues but it will not traverse them.\n+      set_gc_mode(bootstrapping_old);\n+      young_generation->set_old_gen_task_queues(old_generation->task_queues());\n+      ShenandoahGCSession session(cause, young_generation);\n+      service_concurrent_cycle(heap, young_generation, cause, true);\n+      process_phase_timings(heap);\n+      if (heap->cancelled_gc()) {\n+        \/\/ Young generation bootstrap cycle has failed. Concurrent mark for old generation\n+        \/\/ is going to resume after degenerated bootstrap cycle completes.\n+        log_info(gc)(\"Bootstrap cycle for old generation was cancelled\");\n+        return;\n+      }\n+\n+      \/\/ Reset the degenerated point. Normally this would happen at the top\n+      \/\/ of the control loop, but here we have just completed a young cycle\n+      \/\/ which has bootstrapped the old concurrent marking.\n+      _degen_point = ShenandoahGC::_degenerated_outside_cycle;\n+\n+      \/\/ From here we will 'resume' the old concurrent mark. This will skip reset\n+      \/\/ and init mark for the concurrent mark. All of that work will have been\n+      \/\/ done by the bootstrapping young cycle.\n+      set_gc_mode(servicing_old);\n+      old_generation->transition_to(ShenandoahOldGeneration::MARKING);\n+    }\n+    case ShenandoahOldGeneration::MARKING: {\n+      ShenandoahGCSession session(cause, old_generation);\n+      bool marking_complete = resume_concurrent_old_cycle(old_generation, cause);\n+      if (marking_complete) {\n+        assert(old_generation->state() != ShenandoahOldGeneration::MARKING, \"Should not still be marking\");\n+        if (original_state == ShenandoahOldGeneration::MARKING) {\n+          heap->mmu_tracker()->record_old_marking_increment(true);\n+          heap->log_heap_status(\"At end of Concurrent Old Marking finishing increment\");\n+        }\n+      } else if (original_state == ShenandoahOldGeneration::MARKING) {\n+        heap->mmu_tracker()->record_old_marking_increment(false);\n+        heap->log_heap_status(\"At end of Concurrent Old Marking increment\");\n+      }\n+      break;\n+    }\n+    default:\n+      fatal(\"Unexpected state for old GC: %s\", ShenandoahOldGeneration::state_name(old_generation->state()));\n+  }\n+}\n+\n+bool ShenandoahGenerationalControlThread::resume_concurrent_old_cycle(ShenandoahGeneration* generation, GCCause::Cause cause) {\n+  assert(ShenandoahHeap::heap()->is_concurrent_old_mark_in_progress(), \"Old mark should be in progress\");\n+  log_debug(gc)(\"Resuming old generation with \" UINT32_FORMAT \" marking tasks queued\", generation->task_queues()->tasks());\n+\n+  ShenandoahHeap* heap = ShenandoahHeap::heap();\n+\n+  \/\/ We can only tolerate being cancelled during concurrent marking or during preparation for mixed\n+  \/\/ evacuation. This flag here (passed by reference) is used to control precisely where the regulator\n+  \/\/ is allowed to cancel a GC.\n+  ShenandoahOldGC gc(generation, _allow_old_preemption);\n+  if (gc.collect(cause)) {\n+    generation->record_success_concurrent(false);\n+  }\n+\n+  if (heap->cancelled_gc()) {\n+    \/\/ It's possible the gc cycle was cancelled after the last time\n+    \/\/ the collection checked for cancellation. In which case, the\n+    \/\/ old gc cycle is still completed, and we have to deal with this\n+    \/\/ cancellation. We set the degeneration point to be outside\n+    \/\/ the cycle because if this is an allocation failure, that is\n+    \/\/ what must be done (there is no degenerated old cycle). If the\n+    \/\/ cancellation was due to a heuristic wanting to start a young\n+    \/\/ cycle, then we are not actually going to a degenerated cycle,\n+    \/\/ so the degenerated point doesn't matter here.\n+    check_cancellation_or_degen(ShenandoahGC::_degenerated_outside_cycle);\n+    if (_requested_gc_cause == GCCause::_shenandoah_concurrent_gc) {\n+      heap->shenandoah_policy()->record_interrupted_old();\n+    }\n+    return false;\n+  }\n+  return true;\n+}\n+\n+void ShenandoahGenerationalControlThread::service_concurrent_cycle(ShenandoahGeneration* generation, GCCause::Cause cause, bool do_old_gc_bootstrap) {\n+  \/\/ Normal cycle goes via all concurrent phases. If allocation failure (af) happens during\n+  \/\/ any of the concurrent phases, it first degrades to Degenerated GC and completes GC there.\n+  \/\/ If second allocation failure happens during Degenerated GC cycle (for example, when GC\n+  \/\/ tries to evac something and no memory is available), cycle degrades to Full GC.\n+  \/\/\n+  \/\/ There are also a shortcut through the normal cycle: immediate garbage shortcut, when\n+  \/\/ heuristics says there are no regions to compact, and all the collection comes from immediately\n+  \/\/ reclaimable regions.\n+  \/\/\n+  \/\/ ................................................................................................\n+  \/\/\n+  \/\/                                    (immediate garbage shortcut)                Concurrent GC\n+  \/\/                             \/-------------------------------------------\\\n+  \/\/                             |                                           |\n+  \/\/                             |                                           |\n+  \/\/                             |                                           |\n+  \/\/                             |                                           v\n+  \/\/ [START] ----> Conc Mark ----o----> Conc Evac --o--> Conc Update-Refs ---o----> [END]\n+  \/\/                   |                    |                 |              ^\n+  \/\/                   | (af)               | (af)            | (af)         |\n+  \/\/ ..................|....................|.................|..............|.......................\n+  \/\/                   |                    |                 |              |\n+  \/\/                   |                    |                 |              |      Degenerated GC\n+  \/\/                   v                    v                 v              |\n+  \/\/               STW Mark ----------> STW Evac ----> STW Update-Refs ----->o\n+  \/\/                   |                    |                 |              ^\n+  \/\/                   | (af)               | (af)            | (af)         |\n+  \/\/ ..................|....................|.................|..............|.......................\n+  \/\/                   |                    |                 |              |\n+  \/\/                   |                    v                 |              |      Full GC\n+  \/\/                   \\------------------->o<----------------\/              |\n+  \/\/                                        |                                |\n+  \/\/                                        v                                |\n+  \/\/                                      Full GC  --------------------------\/\n+  \/\/\n+  if (check_cancellation_or_degen(ShenandoahGC::_degenerated_outside_cycle)) return;\n+\n+  ShenandoahHeap* heap = ShenandoahHeap::heap();\n+  ShenandoahGCSession session(cause, generation);\n+  TraceCollectorStats tcs(heap->monitoring_support()->concurrent_collection_counters());\n+\n+  service_concurrent_cycle(heap, generation, cause, do_old_gc_bootstrap);\n+}\n+\n+void ShenandoahGenerationalControlThread::service_concurrent_cycle(ShenandoahHeap* heap,\n+                                                       ShenandoahGeneration* generation,\n+                                                       GCCause::Cause& cause,\n+                                                       bool do_old_gc_bootstrap) {\n+  assert(!generation->is_old(), \"Old GC takes a different control path\");\n+\n+  ShenandoahConcurrentGC gc(generation, do_old_gc_bootstrap);\n+  if (gc.collect(cause)) {\n+    \/\/ Cycle is complete\n+    generation->record_success_concurrent(gc.abbreviated());\n+  } else {\n+    assert(heap->cancelled_gc(), \"Must have been cancelled\");\n+    check_cancellation_or_degen(gc.degen_point());\n+\n+    \/\/ Concurrent young-gen collection degenerates to young\n+    \/\/ collection.  Same for global collections.\n+    _degen_generation = generation;\n+  }\n+  const char* msg;\n+  ShenandoahMmuTracker* mmu_tracker = heap->mmu_tracker();\n+  if (generation->is_young()) {\n+    if (heap->cancelled_gc()) {\n+      msg = (do_old_gc_bootstrap) ? \"At end of Interrupted Concurrent Bootstrap GC\" :\n+            \"At end of Interrupted Concurrent Young GC\";\n+    } else {\n+      \/\/ We only record GC results if GC was successful\n+      msg = (do_old_gc_bootstrap) ? \"At end of Concurrent Bootstrap GC\" :\n+            \"At end of Concurrent Young GC\";\n+      if (heap->collection_set()->has_old_regions()) {\n+        mmu_tracker->record_mixed(get_gc_id());\n+      } else if (do_old_gc_bootstrap) {\n+        mmu_tracker->record_bootstrap(get_gc_id());\n+      } else {\n+        mmu_tracker->record_young(get_gc_id());\n+      }\n+    }\n+  } else {\n+    assert(generation->is_global(), \"If not young, must be GLOBAL\");\n+    assert(!do_old_gc_bootstrap, \"Do not bootstrap with GLOBAL GC\");\n+    if (heap->cancelled_gc()) {\n+      msg = \"At end of Interrupted Concurrent GLOBAL GC\";\n+    } else {\n+      \/\/ We only record GC results if GC was successful\n+      msg = \"At end of Concurrent Global GC\";\n+      mmu_tracker->record_global(get_gc_id());\n+    }\n+  }\n+  heap->log_heap_status(msg);\n+}\n+\n+bool ShenandoahGenerationalControlThread::check_cancellation_or_degen(ShenandoahGC::ShenandoahDegenPoint point) {\n+  ShenandoahHeap* heap = ShenandoahHeap::heap();\n+  if (!heap->cancelled_gc()) {\n+    return false;\n+  }\n+\n+  if (in_graceful_shutdown()) {\n+    return true;\n+  }\n+\n+  assert(_degen_point == ShenandoahGC::_degenerated_outside_cycle,\n+         \"Should not be set yet: %s\", ShenandoahGC::degen_point_to_string(_degen_point));\n+\n+  if (is_alloc_failure_gc()) {\n+    _degen_point = point;\n+    _preemption_requested.unset();\n+    return true;\n+  }\n+\n+  if (_preemption_requested.is_set()) {\n+    assert(_requested_generation == YOUNG, \"Only young GCs may preempt old.\");\n+    _preemption_requested.unset();\n+\n+    \/\/ Old generation marking is only cancellable during concurrent marking.\n+    \/\/ Once final mark is complete, the code does not check again for cancellation.\n+    \/\/ If old generation was cancelled for an allocation failure, we wouldn't\n+    \/\/ make it to this case. The calling code is responsible for forcing a\n+    \/\/ cancellation due to allocation failure into a degenerated cycle.\n+    _degen_point = point;\n+    heap->clear_cancelled_gc(false \/* clear oom handler *\/);\n+    return true;\n+  }\n+\n+  fatal(\"Cancel GC either for alloc failure GC, or gracefully exiting, or to pause old generation marking\");\n+  return false;\n+}\n+\n+void ShenandoahGenerationalControlThread::stop_service() {\n+  \/\/ Nothing to do here.\n+}\n+\n+void ShenandoahGenerationalControlThread::service_stw_full_cycle(GCCause::Cause cause) {\n+  ShenandoahHeap* const heap = ShenandoahHeap::heap();\n+\n+  GCIdMark gc_id_mark;\n+  ShenandoahGCSession session(cause, heap->global_generation());\n+\n+  ShenandoahFullGC gc;\n+  gc.collect(cause);\n+}\n+\n+void ShenandoahGenerationalControlThread::service_stw_degenerated_cycle(GCCause::Cause cause,\n+                                                            ShenandoahGC::ShenandoahDegenPoint point) {\n+  assert(point != ShenandoahGC::_degenerated_unset, \"Degenerated point should be set\");\n+  ShenandoahHeap* const heap = ShenandoahHeap::heap();\n+\n+  GCIdMark gc_id_mark;\n+  ShenandoahGCSession session(cause, _degen_generation);\n+\n+  ShenandoahDegenGC gc(point, _degen_generation);\n+  gc.collect(cause);\n+\n+  assert(heap->young_generation()->task_queues()->is_empty(), \"Unexpected young generation marking tasks\");\n+  if (_degen_generation->is_global()) {\n+    assert(heap->old_generation()->task_queues()->is_empty(), \"Unexpected old generation marking tasks\");\n+    assert(heap->global_generation()->task_queues()->is_empty(), \"Unexpected global generation marking tasks\");\n+  } else {\n+    assert(_degen_generation->is_young(), \"Expected degenerated young cycle, if not global.\");\n+    ShenandoahOldGeneration* old = heap->old_generation();\n+    if (old->state() == ShenandoahOldGeneration::BOOTSTRAPPING) {\n+      old->transition_to(ShenandoahOldGeneration::MARKING);\n+    }\n+  }\n+}\n+\n+void ShenandoahGenerationalControlThread::request_gc(GCCause::Cause cause) {\n+  if (ShenandoahCollectorPolicy::should_handle_requested_gc(cause)) {\n+    handle_requested_gc(cause);\n+  }\n+}\n+\n+bool ShenandoahGenerationalControlThread::request_concurrent_gc(ShenandoahGenerationType generation) {\n+  if (_preemption_requested.is_set() || _requested_gc_cause != GCCause::_no_gc || ShenandoahHeap::heap()->cancelled_gc()) {\n+    \/\/ Ignore subsequent requests from the heuristics\n+    log_debug(gc, thread)(\"Reject request for concurrent gc: preemption_requested: %s, gc_requested: %s, gc_cancelled: %s\",\n+                          BOOL_TO_STR(_preemption_requested.is_set()),\n+                          GCCause::to_string(_requested_gc_cause),\n+                          BOOL_TO_STR(ShenandoahHeap::heap()->cancelled_gc()));\n+    return false;\n+  }\n+\n+  if (gc_mode() == none) {\n+    GCCause::Cause existing = Atomic::cmpxchg(&_requested_gc_cause, GCCause::_no_gc, GCCause::_shenandoah_concurrent_gc);\n+    if (existing != GCCause::_no_gc) {\n+      log_debug(gc, thread)(\"Reject request for concurrent gc because another gc is pending: %s\", GCCause::to_string(existing));\n+      return false;\n+    }\n+\n+    _requested_generation = generation;\n+    notify_control_thread();\n+\n+    MonitorLocker ml(&_regulator_lock, Mutex::_no_safepoint_check_flag);\n+    while (gc_mode() == none) {\n+      ml.wait();\n+    }\n+    return true;\n+  }\n+\n+  if (preempt_old_marking(generation)) {\n+    assert(gc_mode() == servicing_old, \"Expected to be servicing old, but was: %s.\", gc_mode_name(gc_mode()));\n+    GCCause::Cause existing = Atomic::cmpxchg(&_requested_gc_cause, GCCause::_no_gc, GCCause::_shenandoah_concurrent_gc);\n+    if (existing != GCCause::_no_gc) {\n+      log_debug(gc, thread)(\"Reject request to interrupt old gc because another gc is pending: %s\", GCCause::to_string(existing));\n+      return false;\n+    }\n+\n+    log_info(gc)(\"Preempting old generation mark to allow %s GC\", shenandoah_generation_name(generation));\n+    _requested_generation = generation;\n+    _preemption_requested.set();\n+    ShenandoahHeap::heap()->cancel_gc(GCCause::_shenandoah_concurrent_gc);\n+    notify_control_thread();\n+\n+    MonitorLocker ml(&_regulator_lock, Mutex::_no_safepoint_check_flag);\n+    while (gc_mode() == servicing_old) {\n+      ml.wait();\n+    }\n+    return true;\n+  }\n+\n+  log_debug(gc, thread)(\"Reject request for concurrent gc: mode: %s, allow_old_preemption: %s\",\n+                        gc_mode_name(gc_mode()),\n+                        BOOL_TO_STR(_allow_old_preemption.is_set()));\n+  return false;\n+}\n+\n+void ShenandoahGenerationalControlThread::notify_control_thread() {\n+  MonitorLocker locker(&_control_lock, Mutex::_no_safepoint_check_flag);\n+  _control_lock.notify();\n+}\n+\n+bool ShenandoahGenerationalControlThread::preempt_old_marking(ShenandoahGenerationType generation) {\n+  return (generation == YOUNG) && _allow_old_preemption.try_unset();\n+}\n+\n+void ShenandoahGenerationalControlThread::handle_requested_gc(GCCause::Cause cause) {\n+  \/\/ Make sure we have at least one complete GC cycle before unblocking\n+  \/\/ from the explicit GC request.\n+  \/\/\n+  \/\/ This is especially important for weak references cleanup and\/or native\n+  \/\/ resources (e.g. DirectByteBuffers) machinery: when explicit GC request\n+  \/\/ comes very late in the already running cycle, it would miss lots of new\n+  \/\/ opportunities for cleanup that were made available before the caller\n+  \/\/ requested the GC.\n+\n+  MonitorLocker ml(&_gc_waiters_lock);\n+  size_t current_gc_id = get_gc_id();\n+  size_t required_gc_id = current_gc_id + 1;\n+  while (current_gc_id < required_gc_id) {\n+    \/\/ This races with the regulator thread to start a concurrent gc and the\n+    \/\/ control thread to clear it at the start of a cycle. Threads here are\n+    \/\/ allowed to escalate a heuristic's request for concurrent gc.\n+    GCCause::Cause existing = Atomic::xchg(&_requested_gc_cause, cause);\n+    if (existing != GCCause::_no_gc) {\n+      log_debug(gc, thread)(\"GC request supersedes existing request: %s\", GCCause::to_string(existing));\n+    }\n+\n+    notify_control_thread();\n+    if (cause != GCCause::_wb_breakpoint) {\n+      ml.wait();\n+    }\n+    current_gc_id = get_gc_id();\n+  }\n+}\n+\n+void ShenandoahGenerationalControlThread::notify_gc_waiters() {\n+  MonitorLocker ml(&_gc_waiters_lock);\n+  ml.notify_all();\n+}\n+\n+const char* ShenandoahGenerationalControlThread::gc_mode_name(ShenandoahGenerationalControlThread::GCMode mode) {\n+  switch (mode) {\n+    case none:              return \"idle\";\n+    case concurrent_normal: return \"normal\";\n+    case stw_degenerated:   return \"degenerated\";\n+    case stw_full:          return \"full\";\n+    case servicing_old:     return \"old\";\n+    case bootstrapping_old: return \"bootstrap\";\n+    default:                return \"unknown\";\n+  }\n+}\n+\n+void ShenandoahGenerationalControlThread::set_gc_mode(ShenandoahGenerationalControlThread::GCMode new_mode) {\n+  if (_mode != new_mode) {\n+    log_info(gc)(\"Transition from: %s to: %s\", gc_mode_name(_mode), gc_mode_name(new_mode));\n+    MonitorLocker ml(&_regulator_lock, Mutex::_no_safepoint_check_flag);\n+    _mode = new_mode;\n+    ml.notify_all();\n+  }\n+}\n","filename":"src\/hotspot\/share\/gc\/shenandoah\/shenandoahGenerationalControlThread.cpp","additions":841,"deletions":0,"binary":false,"changes":841,"status":"added"},{"patch":"@@ -0,0 +1,124 @@\n+\/*\n+ * Copyright (c) 2013, 2021, Red Hat, Inc. All rights reserved.\n+ * Copyright Amazon.com Inc. or its affiliates. All Rights Reserved.\n+ * DO NOT ALTER OR REMOVE COPYRIGHT NOTICES OR THIS FILE HEADER.\n+ *\n+ * This code is free software; you can redistribute it and\/or modify it\n+ * under the terms of the GNU General Public License version 2 only, as\n+ * published by the Free Software Foundation.\n+ *\n+ * This code is distributed in the hope that it will be useful, but WITHOUT\n+ * ANY WARRANTY; without even the implied warranty of MERCHANTABILITY or\n+ * FITNESS FOR A PARTICULAR PURPOSE.  See the GNU General Public License\n+ * version 2 for more details (a copy is included in the LICENSE file that\n+ * accompanied this code).\n+ *\n+ * You should have received a copy of the GNU General Public License version\n+ * 2 along with this work; if not, write to the Free Software Foundation,\n+ * Inc., 51 Franklin St, Fifth Floor, Boston, MA 02110-1301 USA.\n+ *\n+ * Please contact Oracle, 500 Oracle Parkway, Redwood Shores, CA 94065 USA\n+ * or visit www.oracle.com if you need additional information or have any\n+ * questions.\n+ *\n+ *\/\n+\n+#ifndef SHARE_GC_SHENANDOAH_SHENANDOAHGENERATIONALCONTROLTHREAD_HPP\n+#define SHARE_GC_SHENANDOAH_SHENANDOAHGENERATIONALCONTROLTHREAD_HPP\n+\n+#include \"gc\/shared\/gcCause.hpp\"\n+#include \"gc\/shenandoah\/shenandoahController.hpp\"\n+#include \"gc\/shenandoah\/shenandoahGC.hpp\"\n+#include \"gc\/shenandoah\/shenandoahHeap.hpp\"\n+#include \"gc\/shenandoah\/shenandoahPadding.hpp\"\n+#include \"gc\/shenandoah\/shenandoahSharedVariables.hpp\"\n+\n+class ShenandoahGenerationalControlThread: public ShenandoahController {\n+  friend class VMStructs;\n+\n+public:\n+  typedef enum {\n+    none,\n+    concurrent_normal,\n+    stw_degenerated,\n+    stw_full,\n+    bootstrapping_old,\n+    servicing_old\n+  } GCMode;\n+\n+private:\n+  Monitor _control_lock;\n+  Monitor _regulator_lock;\n+\n+  ShenandoahSharedFlag _allow_old_preemption;\n+  ShenandoahSharedFlag _preemption_requested;\n+\n+  GCCause::Cause  _requested_gc_cause;\n+  volatile ShenandoahGenerationType _requested_generation;\n+  ShenandoahGC::ShenandoahDegenPoint _degen_point;\n+  ShenandoahGeneration* _degen_generation;\n+\n+  shenandoah_padding(0);\n+  volatile GCMode _mode;\n+  shenandoah_padding(1);\n+\n+public:\n+  ShenandoahGenerationalControlThread();\n+\n+  void run_service() override;\n+  void stop_service() override;\n+\n+  void request_gc(GCCause::Cause cause) override;\n+\n+  \/\/ Return true if the request to start a concurrent GC for the given generation succeeded.\n+  bool request_concurrent_gc(ShenandoahGenerationType generation);\n+\n+  GCMode gc_mode() {\n+    return _mode;\n+  }\n+private:\n+\n+  \/\/ Returns true if the cycle has been cancelled or degenerated.\n+  bool check_cancellation_or_degen(ShenandoahGC::ShenandoahDegenPoint point);\n+\n+  \/\/ Returns true if the old generation marking completed (i.e., final mark executed for old generation).\n+  bool resume_concurrent_old_cycle(ShenandoahGeneration* generation, GCCause::Cause cause);\n+  void service_concurrent_cycle(ShenandoahGeneration* generation, GCCause::Cause cause, bool reset_old_bitmap_specially);\n+  void service_stw_full_cycle(GCCause::Cause cause);\n+  void service_stw_degenerated_cycle(GCCause::Cause cause, ShenandoahGC::ShenandoahDegenPoint point);\n+\n+  void notify_gc_waiters();\n+\n+  \/\/ Handle GC request.\n+  \/\/ Blocks until GC is over.\n+  void handle_requested_gc(GCCause::Cause cause);\n+\n+  bool is_explicit_gc(GCCause::Cause cause) const;\n+  bool is_implicit_gc(GCCause::Cause cause) const;\n+\n+  \/\/ Returns true if the old generation marking was interrupted to allow a young cycle.\n+  bool preempt_old_marking(ShenandoahGenerationType generation);\n+\n+  void process_phase_timings(const ShenandoahHeap* heap);\n+\n+  void service_concurrent_normal_cycle(ShenandoahHeap* heap,\n+                                       ShenandoahGenerationType generation,\n+                                       GCCause::Cause cause);\n+\n+  void service_concurrent_old_cycle(ShenandoahHeap* heap,\n+                                    GCCause::Cause &cause);\n+\n+  void set_gc_mode(GCMode new_mode);\n+\n+  static const char* gc_mode_name(GCMode mode);\n+\n+  void notify_control_thread();\n+\n+  void service_concurrent_cycle(ShenandoahHeap* heap,\n+                                ShenandoahGeneration* generation,\n+                                GCCause::Cause &cause,\n+                                bool do_old_gc_bootstrap);\n+\n+};\n+\n+#endif \/\/ SHARE_GC_SHENANDOAH_SHENANDOAHGENERATIONALCONTROLTHREAD_HPP\n","filename":"src\/hotspot\/share\/gc\/shenandoah\/shenandoahGenerationalControlThread.hpp","additions":124,"deletions":0,"binary":false,"changes":124,"status":"added"},{"patch":"@@ -0,0 +1,378 @@\n+\/*\n+ * Copyright Amazon.com Inc. or its affiliates. All Rights Reserved.\n+ * DO NOT ALTER OR REMOVE COPYRIGHT NOTICES OR THIS FILE HEADER.\n+ *\n+ * This code is free software; you can redistribute it and\/or modify it\n+ * under the terms of the GNU General Public License version 2 only, as\n+ * published by the Free Software Foundation.\n+ *\n+ * This code is distributed in the hope that it will be useful, but WITHOUT\n+ * ANY WARRANTY; without even the implied warranty of MERCHANTABILITY or\n+ * FITNESS FOR A PARTICULAR PURPOSE.  See the GNU General Public License\n+ * version 2 for more details (a copy is included in the LICENSE file that\n+ * accompanied this code).\n+ *\n+ * You should have received a copy of the GNU General Public License version\n+ * 2 along with this work; if not, write to the Free Software Foundation,\n+ * Inc., 51 Franklin St, Fifth Floor, Boston, MA 02110-1301 USA.\n+ *\n+ * Please contact Oracle, 500 Oracle Parkway, Redwood Shores, CA 94065 USA\n+ * or visit www.oracle.com if you need additional information or have any\n+ * questions.\n+ *\n+ *\/\n+\n+#include \"precompiled.hpp\"\n+\n+#include \"gc\/shared\/preservedMarks.inline.hpp\"\n+#include \"gc\/shenandoah\/shenandoahGenerationalFullGC.hpp\"\n+#include \"gc\/shenandoah\/shenandoahGenerationalHeap.hpp\"\n+#include \"gc\/shenandoah\/shenandoahGeneration.hpp\"\n+#include \"gc\/shenandoah\/shenandoahHeap.inline.hpp\"\n+#include \"gc\/shenandoah\/shenandoahHeapRegion.hpp\"\n+#include \"gc\/shenandoah\/shenandoahYoungGeneration.hpp\"\n+#include \"gc\/shenandoah\/shenandoahOldGeneration.hpp\"\n+#include \"gc\/shenandoah\/shenandoahUtils.hpp\"\n+\n+#ifdef ASSERT\n+void assert_regions_used_not_more_than_capacity(ShenandoahGeneration* generation) {\n+  assert(generation->used_regions_size() <= generation->max_capacity(),\n+         \"%s generation affiliated regions must be less than capacity\", generation->name());\n+}\n+\n+void assert_usage_not_more_than_regions_used(ShenandoahGeneration* generation) {\n+  assert(generation->used_including_humongous_waste() <= generation->used_regions_size(),\n+         \"%s consumed can be no larger than span of affiliated regions\", generation->name());\n+}\n+#else\n+void assert_regions_used_not_more_than_capacity(ShenandoahGeneration* generation) {}\n+void assert_usage_not_more_than_regions_used(ShenandoahGeneration* generation) {}\n+#endif\n+\n+\n+void ShenandoahGenerationalFullGC::prepare() {\n+  auto heap = ShenandoahGenerationalHeap::heap();\n+  \/\/ Since we may arrive here from degenerated GC failure of either young or old, establish generation as GLOBAL.\n+  heap->set_gc_generation(heap->global_generation());\n+\n+  \/\/ No need for old_gen->increase_used() as this was done when plabs were allocated.\n+  heap->reset_generation_reserves();\n+\n+  \/\/ Full GC supersedes any marking or coalescing in old generation.\n+  heap->cancel_old_gc();\n+}\n+\n+void ShenandoahGenerationalFullGC::handle_completion(ShenandoahHeap* heap) {\n+  \/\/ Full GC should reset time since last gc for young and old heuristics\n+  ShenandoahYoungGeneration* young = heap->young_generation();\n+  ShenandoahOldGeneration* old = heap->old_generation();\n+  young->heuristics()->record_cycle_end();\n+  old->heuristics()->record_cycle_end();\n+\n+  heap->mmu_tracker()->record_full(GCId::current());\n+  heap->log_heap_status(\"At end of Full GC\");\n+\n+  assert(old->state() == ShenandoahOldGeneration::WAITING_FOR_BOOTSTRAP,\n+         \"After full GC, old generation should be waiting for bootstrap.\");\n+\n+  \/\/ Since we allow temporary violation of these constraints during Full GC, we want to enforce that the assertions are\n+  \/\/ made valid by the time Full GC completes.\n+  assert_regions_used_not_more_than_capacity(old);\n+  assert_regions_used_not_more_than_capacity(young);\n+  assert_usage_not_more_than_regions_used(old);\n+  assert_usage_not_more_than_regions_used(young);\n+\n+  \/\/ Establish baseline for next old-has-grown trigger.\n+  old->set_live_bytes_after_last_mark(old->used_including_humongous_waste());\n+}\n+\n+void ShenandoahGenerationalFullGC::rebuild_remembered_set(ShenandoahHeap* heap) {\n+  ShenandoahGCPhase phase(ShenandoahPhaseTimings::full_gc_reconstruct_remembered_set);\n+  ShenandoahRegionIterator regions;\n+  ShenandoahReconstructRememberedSetTask task(&regions);\n+  heap->workers()->run_task(&task);\n+}\n+\n+void ShenandoahGenerationalFullGC::balance_generations_after_gc(ShenandoahHeap* heap) {\n+  size_t old_usage = heap->old_generation()->used_regions_size();\n+  size_t old_capacity = heap->old_generation()->max_capacity();\n+\n+  assert(old_usage % ShenandoahHeapRegion::region_size_bytes() == 0, \"Old usage must align with region size\");\n+  assert(old_capacity % ShenandoahHeapRegion::region_size_bytes() == 0, \"Old capacity must align with region size\");\n+\n+  if (old_capacity > old_usage) {\n+    size_t excess_old_regions = (old_capacity - old_usage) \/ ShenandoahHeapRegion::region_size_bytes();\n+    heap->generation_sizer()->transfer_to_young(excess_old_regions);\n+  } else if (old_capacity < old_usage) {\n+    size_t old_regions_deficit = (old_usage - old_capacity) \/ ShenandoahHeapRegion::region_size_bytes();\n+    heap->generation_sizer()->force_transfer_to_old(old_regions_deficit);\n+  }\n+\n+  log_info(gc)(\"FullGC done: young usage: \" PROPERFMT \", old usage: \" PROPERFMT,\n+               PROPERFMTARGS(heap->young_generation()->used()),\n+               PROPERFMTARGS(heap->old_generation()->used()));\n+}\n+\n+void ShenandoahGenerationalFullGC::balance_generations_after_rebuilding_free_set() {\n+  auto result = ShenandoahGenerationalHeap::heap()->balance_generations();\n+  LogTarget(Info, gc, ergo) lt;\n+  if (lt.is_enabled()) {\n+    LogStream ls(lt);\n+    result.print_on(\"Full GC\", &ls);\n+  }\n+}\n+\n+void ShenandoahGenerationalFullGC::log_live_in_old(ShenandoahHeap* heap) {\n+  LogTarget(Info, gc) lt;\n+  if (lt.is_enabled()) {\n+    size_t live_bytes_in_old = 0;\n+    for (size_t i = 0; i < heap->num_regions(); i++) {\n+      ShenandoahHeapRegion* r = heap->get_region(i);\n+      if (r->is_old()) {\n+        live_bytes_in_old += r->get_live_data_bytes();\n+      }\n+    }\n+    log_info(gc)(\"Live bytes in old after STW mark: \" PROPERFMT, PROPERFMTARGS(live_bytes_in_old));\n+  }\n+}\n+\n+void ShenandoahGenerationalFullGC::restore_top_before_promote(ShenandoahHeap* heap) {\n+  for (size_t i = 0; i < heap->num_regions(); i++) {\n+    ShenandoahHeapRegion* r = heap->get_region(i);\n+    if (r->get_top_before_promote() != nullptr) {\n+      r->restore_top_before_promote();\n+    }\n+  }\n+}\n+\n+void ShenandoahGenerationalFullGC::account_for_region(ShenandoahHeapRegion* r, size_t &region_count, size_t &region_usage, size_t &humongous_waste) {\n+  region_count++;\n+  region_usage += r->used();\n+  if (r->is_humongous_start()) {\n+    \/\/ For each humongous object, we take this path once regardless of how many regions it spans.\n+    HeapWord* obj_addr = r->bottom();\n+    oop obj = cast_to_oop(obj_addr);\n+    size_t word_size = obj->size();\n+    size_t region_size_words = ShenandoahHeapRegion::region_size_words();\n+    size_t overreach = word_size % region_size_words;\n+    if (overreach != 0) {\n+      humongous_waste += (region_size_words - overreach) * HeapWordSize;\n+    }\n+    \/\/ else, this humongous object aligns exactly on region size, so no waste.\n+  }\n+}\n+\n+void ShenandoahGenerationalFullGC::maybe_coalesce_and_fill_region(ShenandoahHeapRegion* r) {\n+  if (r->is_pinned() && r->is_old() && r->is_active() && !r->is_humongous()) {\n+    r->begin_preemptible_coalesce_and_fill();\n+    r->oop_fill_and_coalesce_without_cancel();\n+  }\n+}\n+\n+void ShenandoahGenerationalFullGC::compute_balances() {\n+  auto heap = ShenandoahGenerationalHeap::heap();\n+\n+  \/\/ In case this Full GC resulted from degeneration, clear the tally on anticipated promotion.\n+  heap->old_generation()->set_promotion_potential(0);\n+  \/\/ Invoke this in case we are able to transfer memory from OLD to YOUNG.\n+  heap->compute_old_generation_balance(0, 0);\n+}\n+\n+ShenandoahPrepareForGenerationalCompactionObjectClosure::ShenandoahPrepareForGenerationalCompactionObjectClosure(PreservedMarks* preserved_marks,\n+                                                          GrowableArray<ShenandoahHeapRegion*>& empty_regions,\n+                                                          ShenandoahHeapRegion* from_region, uint worker_id) :\n+        _preserved_marks(preserved_marks),\n+        _heap(ShenandoahHeap::heap()),\n+        _tenuring_threshold(0),\n+        _empty_regions(empty_regions),\n+        _empty_regions_pos(0),\n+        _old_to_region(nullptr),\n+        _young_to_region(nullptr),\n+        _from_region(nullptr),\n+        _from_affiliation(ShenandoahAffiliation::FREE),\n+        _old_compact_point(nullptr),\n+        _young_compact_point(nullptr),\n+        _worker_id(worker_id) {\n+  assert(from_region != nullptr, \"Worker needs from_region\");\n+  \/\/ assert from_region has live?\n+  if (from_region->is_old()) {\n+    _old_to_region = from_region;\n+    _old_compact_point = from_region->bottom();\n+  } else if (from_region->is_young()) {\n+    _young_to_region = from_region;\n+    _young_compact_point = from_region->bottom();\n+  }\n+\n+  _tenuring_threshold = _heap->age_census()->tenuring_threshold();\n+}\n+\n+void ShenandoahPrepareForGenerationalCompactionObjectClosure::set_from_region(ShenandoahHeapRegion* from_region) {\n+  log_debug(gc)(\"Worker %u compacting %s Region \" SIZE_FORMAT \" which had used \" SIZE_FORMAT \" and %s live\",\n+                _worker_id, from_region->affiliation_name(),\n+                from_region->index(), from_region->used(), from_region->has_live()? \"has\": \"does not have\");\n+\n+  _from_region = from_region;\n+  _from_affiliation = from_region->affiliation();\n+  if (_from_region->has_live()) {\n+    if (_from_affiliation == ShenandoahAffiliation::OLD_GENERATION) {\n+      if (_old_to_region == nullptr) {\n+        _old_to_region = from_region;\n+        _old_compact_point = from_region->bottom();\n+      }\n+    } else {\n+      assert(_from_affiliation == ShenandoahAffiliation::YOUNG_GENERATION, \"from_region must be OLD or YOUNG\");\n+      if (_young_to_region == nullptr) {\n+        _young_to_region = from_region;\n+        _young_compact_point = from_region->bottom();\n+      }\n+    }\n+  } \/\/ else, we won't iterate over this _from_region so we don't need to set up to region to hold copies\n+}\n+\n+void ShenandoahPrepareForGenerationalCompactionObjectClosure::finish() {\n+  finish_old_region();\n+  finish_young_region();\n+}\n+\n+void ShenandoahPrepareForGenerationalCompactionObjectClosure::finish_old_region() {\n+  if (_old_to_region != nullptr) {\n+    log_debug(gc)(\"Planned compaction into Old Region \" SIZE_FORMAT \", used: \" SIZE_FORMAT \" tabulated by worker %u\",\n+            _old_to_region->index(), _old_compact_point - _old_to_region->bottom(), _worker_id);\n+    _old_to_region->set_new_top(_old_compact_point);\n+    _old_to_region = nullptr;\n+  }\n+}\n+\n+void ShenandoahPrepareForGenerationalCompactionObjectClosure::finish_young_region() {\n+  if (_young_to_region != nullptr) {\n+    log_debug(gc)(\"Worker %u planned compaction into Young Region \" SIZE_FORMAT \", used: \" SIZE_FORMAT,\n+            _worker_id, _young_to_region->index(), _young_compact_point - _young_to_region->bottom());\n+    _young_to_region->set_new_top(_young_compact_point);\n+    _young_to_region = nullptr;\n+  }\n+}\n+\n+bool ShenandoahPrepareForGenerationalCompactionObjectClosure::is_compact_same_region() {\n+  return (_from_region == _old_to_region) || (_from_region == _young_to_region);\n+}\n+\n+void ShenandoahPrepareForGenerationalCompactionObjectClosure::do_object(oop p) {\n+  assert(_from_region != nullptr, \"must set before work\");\n+  assert((_from_region->bottom() <= cast_from_oop<HeapWord*>(p)) && (cast_from_oop<HeapWord*>(p) < _from_region->top()),\n+         \"Object must reside in _from_region\");\n+  assert(_heap->complete_marking_context()->is_marked(p), \"must be marked\");\n+  assert(!_heap->complete_marking_context()->allocated_after_mark_start(p), \"must be truly marked\");\n+\n+  size_t obj_size = p->size();\n+  uint from_region_age = _from_region->age();\n+  uint object_age = p->age();\n+\n+  bool promote_object = false;\n+  if ((_from_affiliation == ShenandoahAffiliation::YOUNG_GENERATION) &&\n+      (from_region_age + object_age >= _tenuring_threshold)) {\n+    if ((_old_to_region != nullptr) && (_old_compact_point + obj_size > _old_to_region->end())) {\n+      finish_old_region();\n+      _old_to_region = nullptr;\n+    }\n+    if (_old_to_region == nullptr) {\n+      if (_empty_regions_pos < _empty_regions.length()) {\n+        ShenandoahHeapRegion* new_to_region = _empty_regions.at(_empty_regions_pos);\n+        _empty_regions_pos++;\n+        new_to_region->set_affiliation(OLD_GENERATION);\n+        _old_to_region = new_to_region;\n+        _old_compact_point = _old_to_region->bottom();\n+        promote_object = true;\n+      }\n+      \/\/ Else this worker thread does not yet have any empty regions into which this aged object can be promoted so\n+      \/\/ we leave promote_object as false, deferring the promotion.\n+    } else {\n+      promote_object = true;\n+    }\n+  }\n+\n+  if (promote_object || (_from_affiliation == ShenandoahAffiliation::OLD_GENERATION)) {\n+    assert(_old_to_region != nullptr, \"_old_to_region should not be nullptr when evacuating to OLD region\");\n+    if (_old_compact_point + obj_size > _old_to_region->end()) {\n+      ShenandoahHeapRegion* new_to_region;\n+\n+      log_debug(gc)(\"Worker %u finishing old region \" SIZE_FORMAT \", compact_point: \" PTR_FORMAT \", obj_size: \" SIZE_FORMAT\n+      \", &compact_point[obj_size]: \" PTR_FORMAT \", region end: \" PTR_FORMAT,  _worker_id, _old_to_region->index(),\n+              p2i(_old_compact_point), obj_size, p2i(_old_compact_point + obj_size), p2i(_old_to_region->end()));\n+\n+      \/\/ Object does not fit.  Get a new _old_to_region.\n+      finish_old_region();\n+      if (_empty_regions_pos < _empty_regions.length()) {\n+        new_to_region = _empty_regions.at(_empty_regions_pos);\n+        _empty_regions_pos++;\n+        new_to_region->set_affiliation(OLD_GENERATION);\n+      } else {\n+        \/\/ If we've exhausted the previously selected _old_to_region, we know that the _old_to_region is distinct\n+        \/\/ from _from_region.  That's because there is always room for _from_region to be compacted into itself.\n+        \/\/ Since we're out of empty regions, let's use _from_region to hold the results of its own compaction.\n+        new_to_region = _from_region;\n+      }\n+\n+      assert(new_to_region != _old_to_region, \"must not reuse same OLD to-region\");\n+      assert(new_to_region != nullptr, \"must not be nullptr\");\n+      _old_to_region = new_to_region;\n+      _old_compact_point = _old_to_region->bottom();\n+    }\n+\n+    \/\/ Object fits into current region, record new location, if object does not move:\n+    assert(_old_compact_point + obj_size <= _old_to_region->end(), \"must fit\");\n+    shenandoah_assert_not_forwarded(nullptr, p);\n+    if (_old_compact_point != cast_from_oop<HeapWord*>(p)) {\n+      _preserved_marks->push_if_necessary(p, p->mark());\n+      p->forward_to(cast_to_oop(_old_compact_point));\n+    }\n+    _old_compact_point += obj_size;\n+  } else {\n+    assert(_from_affiliation == ShenandoahAffiliation::YOUNG_GENERATION,\n+           \"_from_region must be OLD_GENERATION or YOUNG_GENERATION\");\n+    assert(_young_to_region != nullptr, \"_young_to_region should not be nullptr when compacting YOUNG _from_region\");\n+\n+    \/\/ After full gc compaction, all regions have age 0.  Embed the region's age into the object's age in order to preserve\n+    \/\/ tenuring progress.\n+    if (_heap->is_aging_cycle()) {\n+      ShenandoahHeap::increase_object_age(p, from_region_age + 1);\n+    } else {\n+      ShenandoahHeap::increase_object_age(p, from_region_age);\n+    }\n+\n+    if (_young_compact_point + obj_size > _young_to_region->end()) {\n+      ShenandoahHeapRegion* new_to_region;\n+\n+      log_debug(gc)(\"Worker %u finishing young region \" SIZE_FORMAT \", compact_point: \" PTR_FORMAT \", obj_size: \" SIZE_FORMAT\n+      \", &compact_point[obj_size]: \" PTR_FORMAT \", region end: \" PTR_FORMAT,  _worker_id, _young_to_region->index(),\n+              p2i(_young_compact_point), obj_size, p2i(_young_compact_point + obj_size), p2i(_young_to_region->end()));\n+\n+      \/\/ Object does not fit.  Get a new _young_to_region.\n+      finish_young_region();\n+      if (_empty_regions_pos < _empty_regions.length()) {\n+        new_to_region = _empty_regions.at(_empty_regions_pos);\n+        _empty_regions_pos++;\n+        new_to_region->set_affiliation(YOUNG_GENERATION);\n+      } else {\n+        \/\/ If we've exhausted the previously selected _young_to_region, we know that the _young_to_region is distinct\n+        \/\/ from _from_region.  That's because there is always room for _from_region to be compacted into itself.\n+        \/\/ Since we're out of empty regions, let's use _from_region to hold the results of its own compaction.\n+        new_to_region = _from_region;\n+      }\n+\n+      assert(new_to_region != _young_to_region, \"must not reuse same OLD to-region\");\n+      assert(new_to_region != nullptr, \"must not be nullptr\");\n+      _young_to_region = new_to_region;\n+      _young_compact_point = _young_to_region->bottom();\n+    }\n+\n+    \/\/ Object fits into current region, record new location, if object does not move:\n+    assert(_young_compact_point + obj_size <= _young_to_region->end(), \"must fit\");\n+    shenandoah_assert_not_forwarded(nullptr, p);\n+\n+    if (_young_compact_point != cast_from_oop<HeapWord*>(p)) {\n+      _preserved_marks->push_if_necessary(p, p->mark());\n+      p->forward_to(cast_to_oop(_young_compact_point));\n+    }\n+    _young_compact_point += obj_size;\n+  }\n+}\n","filename":"src\/hotspot\/share\/gc\/shenandoah\/shenandoahGenerationalFullGC.cpp","additions":378,"deletions":0,"binary":false,"changes":378,"status":"added"},{"patch":"@@ -0,0 +1,122 @@\n+\/*\n+ * Copyright Amazon.com Inc. or its affiliates. All Rights Reserved.\n+ * DO NOT ALTER OR REMOVE COPYRIGHT NOTICES OR THIS FILE HEADER.\n+ *\n+ * This code is free software; you can redistribute it and\/or modify it\n+ * under the terms of the GNU General Public License version 2 only, as\n+ * published by the Free Software Foundation.\n+ *\n+ * This code is distributed in the hope that it will be useful, but WITHOUT\n+ * ANY WARRANTY; without even the implied warranty of MERCHANTABILITY or\n+ * FITNESS FOR A PARTICULAR PURPOSE.  See the GNU General Public License\n+ * version 2 for more details (a copy is included in the LICENSE file that\n+ * accompanied this code).\n+ *\n+ * You should have received a copy of the GNU General Public License version\n+ * 2 along with this work; if not, write to the Free Software Foundation,\n+ * Inc., 51 Franklin St, Fifth Floor, Boston, MA 02110-1301 USA.\n+ *\n+ * Please contact Oracle, 500 Oracle Parkway, Redwood Shores, CA 94065 USA\n+ * or visit www.oracle.com if you need additional information or have any\n+ * questions.\n+ *\n+ *\/\n+\n+#ifndef SHARE_GC_SHENANDOAH_SHENANDOAHGENERATIONALFULLGC_HPP\n+#define SHARE_GC_SHENANDOAH_SHENANDOAHGENERATIONALFULLGC_HPP\n+\n+#include \"gc\/shared\/preservedMarks.hpp\"\n+#include \"memory\/iterator.hpp\"\n+#include \"oops\/oop.inline.hpp\"\n+#include \"utilities\/growableArray.hpp\"\n+\n+class ShenandoahHeap;\n+class ShenandoahHeapRegion;\n+\n+class ShenandoahGenerationalFullGC {\n+public:\n+  \/\/ Prepares the generational mode heap for a full collection.\n+  static void prepare();\n+\n+  \/\/ Full GC may have compacted objects in the old generation, so we need to rebuild the card tables.\n+  static void rebuild_remembered_set(ShenandoahHeap* heap);\n+\n+  \/\/ Records end of cycle for young and old and establishes size of live bytes in old\n+  static void handle_completion(ShenandoahHeap* heap);\n+\n+  \/\/ Full GC may have promoted regions and may have temporarily violated constraints on the usage and\n+  \/\/ capacity of the old generation. This method will balance the accounting of regions between the\n+  \/\/ young and old generations. This is somewhat vestigial, but the outcome of this method is used\n+  \/\/ when rebuilding the free sets.\n+  static void balance_generations_after_gc(ShenandoahHeap* heap);\n+\n+  \/\/ This will compute the target size for the old generation. It will be expressed in terms of\n+  \/\/ a region surplus and deficit, which will be redistributed accordingly after rebuilding the\n+  \/\/ free set.\n+  static void compute_balances();\n+\n+  \/\/ Rebuilding the free set may have resulted in regions being pulled in to the old generation\n+  \/\/ evacuation reserve. For this reason, we must update the usage and capacity of the generations\n+  \/\/ again. In the distant past, the free set did not know anything about generations, so we had\n+  \/\/ a layer built above it to represent how much young\/old memory was available. This layer is\n+  \/\/ redundant and adds complexity. We would like to one day remove it. Until then, we must keep it\n+  \/\/ synchronized with the free set's view of things.\n+  static void balance_generations_after_rebuilding_free_set();\n+\n+  \/\/ Logs the number of live bytes marked in the old generation. This is _not_ the same\n+  \/\/ value used as the baseline for the old generation _after_ the full gc is complete.\n+  \/\/ The value reported in the logs does not include objects and regions that may be\n+  \/\/ promoted during the full gc.\n+  static void log_live_in_old(ShenandoahHeap* heap);\n+\n+  \/\/ This is used to tally the number, usage and space wasted by humongous objects for each generation.\n+  static void account_for_region(ShenandoahHeapRegion* r, size_t &region_count, size_t &region_usage, size_t &humongous_waste);\n+\n+  \/\/ Regions which are scheduled for in-place promotion during evacuation temporarily\n+  \/\/ have their top set to their end to prevent new objects from being allocated in them\n+  \/\/ before they are promoted. If the full GC encounters such a region, it means the\n+  \/\/ in-place promotion did not happen, and we must restore the original value of top.\n+  static void restore_top_before_promote(ShenandoahHeap* heap);\n+\n+  \/\/ Pinned regions are not compacted, so they may still hold unmarked objects with\n+  \/\/ references to reclaimed memory. Remembered set scanning will crash if it attempts\n+  \/\/ to iterate the oops in these objects. This method fills in dead objects for pinned,\n+  \/\/ old regions.\n+  static void maybe_coalesce_and_fill_region(ShenandoahHeapRegion* r);\n+};\n+\n+class ShenandoahPrepareForGenerationalCompactionObjectClosure : public ObjectClosure {\n+private:\n+  PreservedMarks*          const _preserved_marks;\n+  ShenandoahHeap*          const _heap;\n+  uint                           _tenuring_threshold;\n+\n+  \/\/ _empty_regions is a thread-local list of heap regions that have been completely emptied by this worker thread's\n+  \/\/ compaction efforts.  The worker thread that drives these efforts adds compacted regions to this list if the\n+  \/\/ region has not been compacted onto itself.\n+  GrowableArray<ShenandoahHeapRegion*>& _empty_regions;\n+  int _empty_regions_pos;\n+  ShenandoahHeapRegion*          _old_to_region;\n+  ShenandoahHeapRegion*          _young_to_region;\n+  ShenandoahHeapRegion*          _from_region;\n+  ShenandoahAffiliation          _from_affiliation;\n+  HeapWord*                      _old_compact_point;\n+  HeapWord*                      _young_compact_point;\n+  uint                           _worker_id;\n+\n+public:\n+  ShenandoahPrepareForGenerationalCompactionObjectClosure(PreservedMarks* preserved_marks,\n+                                                          GrowableArray<ShenandoahHeapRegion*>& empty_regions,\n+                                                          ShenandoahHeapRegion* from_region, uint worker_id);\n+\n+  void set_from_region(ShenandoahHeapRegion* from_region);\n+  void finish();\n+  void finish_old_region();\n+  void finish_young_region();\n+  bool is_compact_same_region();\n+  int empty_regions_pos() const { return _empty_regions_pos; }\n+\n+  void do_object(oop p) override;\n+};\n+\n+#endif \/\/SHARE_GC_SHENANDOAH_SHENANDOAHGENERATIONALFULLGC_HPP\n","filename":"src\/hotspot\/share\/gc\/shenandoah\/shenandoahGenerationalFullGC.hpp","additions":122,"deletions":0,"binary":false,"changes":122,"status":"added"},{"patch":"@@ -27,0 +27,1 @@\n+#include \"gc\/shenandoah\/shenandoahCollectorPolicy.hpp\"\n@@ -30,0 +31,1 @@\n+#include \"gc\/shenandoah\/shenandoahMemoryPool.hpp\"\n@@ -31,0 +33,1 @@\n+#include \"gc\/shenandoah\/shenandoahRegulatorThread.hpp\"\n@@ -48,2 +51,2 @@\n-    log_info(gc, init)(\"Young Generation Soft Size: \" PROPERFMT, PROPERFMTARGS(young->soft_max_capacity()));\n-    log_info(gc, init)(\"Young Generation Max: \" PROPERFMT, PROPERFMTARGS(young->max_capacity()));\n+    log_info(gc, init)(\"Young Generation Soft Size: \" EXACTFMT, EXACTFMTARGS(young->soft_max_capacity()));\n+    log_info(gc, init)(\"Young Generation Max: \" EXACTFMT, EXACTFMTARGS(young->max_capacity()));\n@@ -52,2 +55,2 @@\n-    log_info(gc, init)(\"Old Generation Soft Size: \" PROPERFMT, PROPERFMTARGS(old->soft_max_capacity()));\n-    log_info(gc, init)(\"Old Generation Max: \" PROPERFMT, PROPERFMTARGS(old->max_capacity()));\n+    log_info(gc, init)(\"Old Generation Soft Size: \" EXACTFMT, EXACTFMTARGS(old->soft_max_capacity()));\n+    log_info(gc, init)(\"Old Generation Max: \" EXACTFMT, EXACTFMTARGS(old->max_capacity()));\n@@ -67,0 +70,1 @@\n+  shenandoah_assert_generational();\n@@ -71,0 +75,4 @@\n+ShenandoahGenerationalHeap::ShenandoahGenerationalHeap(ShenandoahCollectorPolicy* policy) :\n+  ShenandoahHeap(policy),\n+  _regulator_thread(nullptr) { }\n+\n@@ -75,0 +83,186 @@\n+\n+void ShenandoahGenerationalHeap::initialize_serviceability() {\n+  assert(mode()->is_generational(), \"Only for the generational mode\");\n+  _young_gen_memory_pool = new ShenandoahYoungGenMemoryPool(this);\n+  _old_gen_memory_pool = new ShenandoahOldGenMemoryPool(this);\n+  cycle_memory_manager()->add_pool(_young_gen_memory_pool);\n+  cycle_memory_manager()->add_pool(_old_gen_memory_pool);\n+  stw_memory_manager()->add_pool(_young_gen_memory_pool);\n+  stw_memory_manager()->add_pool(_old_gen_memory_pool);\n+}\n+\n+GrowableArray<MemoryPool*> ShenandoahGenerationalHeap::memory_pools() {\n+  assert(mode()->is_generational(), \"Only for the generational mode\");\n+  GrowableArray<MemoryPool*> memory_pools(2);\n+  memory_pools.append(_young_gen_memory_pool);\n+  memory_pools.append(_old_gen_memory_pool);\n+  return memory_pools;\n+}\n+\n+void ShenandoahGenerationalHeap::initialize_controller() {\n+  auto control_thread = new ShenandoahGenerationalControlThread();\n+  _control_thread = control_thread;\n+  _regulator_thread = new ShenandoahRegulatorThread(control_thread);\n+}\n+\n+void ShenandoahGenerationalHeap::gc_threads_do(ThreadClosure* tcl) const {\n+  if (!shenandoah_policy()->is_at_shutdown()) {\n+    ShenandoahHeap::gc_threads_do(tcl);\n+    tcl->do_thread(regulator_thread());\n+  }\n+}\n+\n+void ShenandoahGenerationalHeap::stop() {\n+  regulator_thread()->stop();\n+  ShenandoahHeap::stop();\n+}\n+\n+ShenandoahGenerationalHeap::TransferResult ShenandoahGenerationalHeap::balance_generations() {\n+  shenandoah_assert_heaplocked_or_safepoint();\n+\n+  ShenandoahOldGeneration* old_gen = old_generation();\n+  const size_t old_region_surplus = old_gen->get_region_surplus();\n+  const size_t old_region_deficit = old_gen->get_region_deficit();\n+  old_gen->set_region_surplus(0);\n+  old_gen->set_region_deficit(0);\n+\n+  if (old_region_surplus) {\n+    bool success = generation_sizer()->transfer_to_young(old_region_surplus);\n+    return TransferResult {\n+      success, old_region_surplus, \"young\"\n+    };\n+  }\n+\n+  if (old_region_deficit) {\n+    const bool success = generation_sizer()->transfer_to_old(old_region_deficit);\n+    if (!success) {\n+      old_gen->handle_failed_transfer();\n+    }\n+    return TransferResult {\n+      success, old_region_deficit, \"old\"\n+    };\n+  }\n+\n+  return TransferResult {true, 0, \"none\"};\n+}\n+\n+\/\/ Make sure old-generation is large enough, but no larger than is necessary, to hold mixed evacuations\n+\/\/ and promotions, if we anticipate either. Any deficit is provided by the young generation, subject to\n+\/\/ xfer_limit, and any surplus is transferred to the young generation.\n+\/\/ xfer_limit is the maximum we're able to transfer from young to old.\n+void ShenandoahGenerationalHeap::compute_old_generation_balance(size_t old_xfer_limit, size_t old_cset_regions) {\n+\n+  \/\/ We can limit the old reserve to the size of anticipated promotions:\n+  \/\/ max_old_reserve is an upper bound on memory evacuated from old and promoted to old,\n+  \/\/ clamped by the old generation space available.\n+  \/\/\n+  \/\/ Here's the algebra.\n+  \/\/ Let SOEP = ShenandoahOldEvacRatioPercent,\n+  \/\/     OE = old evac,\n+  \/\/     YE = young evac, and\n+  \/\/     TE = total evac = OE + YE\n+  \/\/ By definition:\n+  \/\/            SOEP\/100 = OE\/TE\n+  \/\/                     = OE\/(OE+YE)\n+  \/\/  => SOEP\/(100-SOEP) = OE\/((OE+YE)-OE)      \/\/ componendo-dividendo: If a\/b = c\/d, then a\/(b-a) = c\/(d-c)\n+  \/\/                     = OE\/YE\n+  \/\/  =>              OE = YE*SOEP\/(100-SOEP)\n+\n+  \/\/ We have to be careful in the event that SOEP is set to 100 by the user.\n+  assert(ShenandoahOldEvacRatioPercent <= 100, \"Error\");\n+  const size_t old_available = old_generation()->available();\n+  \/\/ The free set will reserve this amount of memory to hold young evacuations\n+  const size_t young_reserve = (young_generation()->max_capacity() * ShenandoahEvacReserve) \/ 100;\n+\n+  \/\/ In the case that ShenandoahOldEvacRatioPercent equals 100, max_old_reserve is limited only by xfer_limit.\n+\n+  const size_t bound_on_old_reserve = old_available + old_xfer_limit + young_reserve;\n+  const size_t max_old_reserve = (ShenandoahOldEvacRatioPercent == 100)?\n+                                 bound_on_old_reserve: MIN2((young_reserve * ShenandoahOldEvacRatioPercent) \/ (100 - ShenandoahOldEvacRatioPercent),\n+                                                            bound_on_old_reserve);\n+\n+  const size_t region_size_bytes = ShenandoahHeapRegion::region_size_bytes();\n+\n+  \/\/ Decide how much old space we should reserve for a mixed collection\n+  size_t reserve_for_mixed = 0;\n+  const size_t mixed_candidates = old_heuristics()->unprocessed_old_collection_candidates();\n+  const bool doing_mixed = (mixed_candidates > 0);\n+  if (doing_mixed) {\n+    \/\/ We want this much memory to be unfragmented in order to reliably evacuate old.  This is conservative because we\n+    \/\/ may not evacuate the entirety of unprocessed candidates in a single mixed evacuation.\n+    const size_t max_evac_need = (size_t)\n+            (old_heuristics()->unprocessed_old_collection_candidates_live_memory() * ShenandoahOldEvacWaste);\n+    assert(old_available >= old_generation()->free_unaffiliated_regions() * region_size_bytes,\n+           \"Unaffiliated available must be less than total available\");\n+    const size_t old_fragmented_available =\n+            old_available - old_generation()->free_unaffiliated_regions() * region_size_bytes;\n+    reserve_for_mixed = max_evac_need + old_fragmented_available;\n+    if (reserve_for_mixed > max_old_reserve) {\n+      reserve_for_mixed = max_old_reserve;\n+    }\n+  }\n+\n+  \/\/ Decide how much space we should reserve for promotions from young\n+  size_t reserve_for_promo = 0;\n+  const size_t promo_load = old_generation()->get_promotion_potential();\n+  const bool doing_promotions = promo_load > 0;\n+  if (doing_promotions) {\n+    \/\/ We're promoting and have a bound on the maximum amount that can be promoted\n+    assert(max_old_reserve >= reserve_for_mixed, \"Sanity\");\n+    const size_t available_for_promotions = max_old_reserve - reserve_for_mixed;\n+    reserve_for_promo = MIN2((size_t)(promo_load * ShenandoahPromoEvacWaste), available_for_promotions);\n+  }\n+\n+  \/\/ This is the total old we want to ideally reserve\n+  const size_t old_reserve = reserve_for_mixed + reserve_for_promo;\n+  assert(old_reserve <= max_old_reserve, \"cannot reserve more than max for old evacuations\");\n+\n+  \/\/ We now check if the old generation is running a surplus or a deficit.\n+  size_t old_region_deficit = 0;\n+  size_t old_region_surplus = 0;\n+\n+  const size_t max_old_available = old_generation()->available() + old_cset_regions * region_size_bytes;\n+  if (max_old_available >= old_reserve) {\n+    \/\/ We are running a surplus, so the old region surplus can go to young\n+    const size_t old_surplus = max_old_available - old_reserve;\n+    old_region_surplus = old_surplus \/ region_size_bytes;\n+    const size_t unaffiliated_old_regions = old_generation()->free_unaffiliated_regions() + old_cset_regions;\n+    old_region_surplus = MIN2(old_region_surplus, unaffiliated_old_regions);\n+  } else {\n+    \/\/ We are running a deficit which we'd like to fill from young.\n+    \/\/ Ignore that this will directly impact young_generation()->max_capacity(),\n+    \/\/ indirectly impacting young_reserve and old_reserve.  These computations are conservative.\n+    const size_t old_need = old_reserve - max_old_available;\n+    \/\/ The old region deficit (rounded up) will come from young\n+    old_region_deficit = (old_need + region_size_bytes - 1) \/ region_size_bytes;\n+\n+    \/\/ Round down the regions we can transfer from young to old. If we're running short\n+    \/\/ on young-gen memory, we restrict the xfer. Old-gen collection activities will be\n+    \/\/ curtailed if the budget is restricted.\n+    const size_t max_old_region_xfer = old_xfer_limit \/ region_size_bytes;\n+    old_region_deficit = MIN2(old_region_deficit, max_old_region_xfer);\n+  }\n+  assert(old_region_deficit == 0 || old_region_surplus == 0, \"Only surplus or deficit, never both\");\n+\n+  old_generation()->set_region_surplus(old_region_surplus);\n+  old_generation()->set_region_deficit(old_region_deficit);\n+}\n+\n+void ShenandoahGenerationalHeap::reset_generation_reserves() {\n+  young_generation()->set_evacuation_reserve(0);\n+  old_generation()->set_evacuation_reserve(0);\n+  old_generation()->set_promoted_reserve(0);\n+}\n+\n+void ShenandoahGenerationalHeap::TransferResult::print_on(const char* when, outputStream* ss) const {\n+  auto heap = ShenandoahGenerationalHeap::heap();\n+  ShenandoahYoungGeneration* const young_gen = heap->young_generation();\n+  ShenandoahOldGeneration* const old_gen = heap->old_generation();\n+  const size_t young_available = young_gen->available();\n+  const size_t old_available = old_gen->available();\n+  ss->print_cr(\"After %s, %s \" SIZE_FORMAT \" regions to %s to prepare for next gc, old available: \"\n+                     PROPERFMT \", young_available: \" PROPERFMT,\n+                     when,\n+                     success? \"successfully transferred\": \"failed to transfer\", region_count, region_destination,\n+                     PROPERFMTARGS(old_available), PROPERFMTARGS(young_available));\n+}\n\\ No newline at end of file\n","filename":"src\/hotspot\/share\/gc\/shenandoah\/shenandoahGenerationalHeap.cpp","additions":198,"deletions":4,"binary":false,"changes":202,"status":"modified"},{"patch":"@@ -30,0 +30,3 @@\n+class ShenandoahRegulatorThread;\n+class ShenandoahGenerationalControlThread;\n+\n@@ -32,1 +35,1 @@\n-  explicit ShenandoahGenerationalHeap(ShenandoahCollectorPolicy* policy) : ShenandoahHeap(policy) {}\n+  explicit ShenandoahGenerationalHeap(ShenandoahCollectorPolicy* policy);\n@@ -37,0 +40,38 @@\n+\n+  \/\/ ---------- Serviceability\n+  \/\/\n+  void initialize_serviceability() override;\n+  GrowableArray<MemoryPool*> memory_pools() override;\n+\n+  ShenandoahRegulatorThread* regulator_thread() const { return _regulator_thread;  }\n+\n+  void gc_threads_do(ThreadClosure* tcl) const override;\n+\n+  void stop() override;\n+\n+  \/\/ Used for logging the result of a region transfer outside of the heap lock\n+  struct TransferResult {\n+    bool success;\n+    size_t region_count;\n+    const char* region_destination;\n+\n+    void print_on(const char* when, outputStream* ss) const;\n+  };\n+\n+  \/\/ Zeros out the evacuation and promotion reserves\n+  void reset_generation_reserves();\n+\n+  \/\/ Computes the optimal size for the old generation, represented as a surplus or deficit of old regions\n+  void compute_old_generation_balance(size_t old_xfer_limit, size_t old_cset_regions);\n+\n+  \/\/ Transfers surplus old regions to young, or takes regions from young to satisfy old region deficit\n+  TransferResult balance_generations();\n+\n+\n+private:\n+  void initialize_controller() override;\n+\n+  ShenandoahRegulatorThread* _regulator_thread;\n+\n+  MemoryPool* _young_gen_memory_pool;\n+  MemoryPool* _old_gen_memory_pool;\n","filename":"src\/hotspot\/share\/gc\/shenandoah\/shenandoahGenerationalHeap.hpp","additions":42,"deletions":1,"binary":false,"changes":43,"status":"modified"},{"patch":"@@ -36,1 +36,1 @@\n-  : ShenandoahGeneration(generational ? GLOBAL_GEN : GLOBAL_NON_GEN, max_queues, max_capacity, soft_max_capacity) { }\n+  : ShenandoahGeneration(generational ? GLOBAL : NON_GEN, max_queues, max_capacity, soft_max_capacity) { }\n","filename":"src\/hotspot\/share\/gc\/shenandoah\/shenandoahGlobalGeneration.hpp","additions":1,"deletions":1,"binary":false,"changes":2,"status":"modified"},{"patch":"@@ -2,1 +2,1 @@\n- * Copyright (c) 2023, Oracle and\/or its affiliates. All rights reserved.\n+ * Copyright (c) 2023, 2024, Oracle and\/or its affiliates. All rights reserved.\n@@ -49,0 +49,1 @@\n+#include \"gc\/shenandoah\/shenandoahGenerationalHeap.hpp\"\n@@ -51,1 +52,0 @@\n-#include \"gc\/shenandoah\/shenandoahRegulatorThread.hpp\"\n@@ -481,2 +481,0 @@\n-  } else {\n-    _pacer = nullptr;\n@@ -485,2 +483,1 @@\n-  _control_thread = new ShenandoahControlThread();\n-  _regulator_thread = new ShenandoahRegulatorThread(_control_thread);\n+  initialize_controller();\n@@ -493,0 +490,4 @@\n+void ShenandoahHeap::initialize_controller() {\n+  _control_thread = new ShenandoahControlThread();\n+}\n+\n@@ -503,2 +504,2 @@\n-    case GLOBAL_GEN:\n-    case GLOBAL_NON_GEN:\n+    case GLOBAL:\n+    case NON_GEN:\n@@ -518,2 +519,2 @@\n-    case GLOBAL_GEN:\n-    case GLOBAL_NON_GEN:\n+    case GLOBAL:\n+    case NON_GEN:\n@@ -585,1 +586,0 @@\n-  _promotion_potential(0),\n@@ -596,3 +596,1 @@\n-  _promoted_reserve(0),\n-  _old_evac_reserve(0),\n-  _young_evac_reserve(0),\n+  _gc_no_progress_count(0),\n@@ -600,1 +598,0 @@\n-  _has_evacuation_reserve_quantities(false),\n@@ -606,1 +603,0 @@\n-  _regulator_thread(nullptr),\n@@ -617,2 +613,0 @@\n-  _young_gen_memory_pool(nullptr),\n-  _old_gen_memory_pool(nullptr),\n@@ -624,2 +618,0 @@\n-  _old_regions_surplus(0),\n-  _old_regions_deficit(0),\n@@ -881,0 +873,27 @@\n+void ShenandoahHeap::maybe_uncommit(double shrink_before, size_t shrink_until) {\n+  assert (ShenandoahUncommit, \"should be enabled\");\n+\n+  \/\/ Determine if there is work to do. This avoids taking heap lock if there is\n+  \/\/ no work available, avoids spamming logs with superfluous logging messages,\n+  \/\/ and minimises the amount of work while locks are taken.\n+\n+  if (committed() <= shrink_until) return;\n+\n+  bool has_work = false;\n+  for (size_t i = 0; i < num_regions(); i++) {\n+    ShenandoahHeapRegion* r = get_region(i);\n+    if (r->is_empty_committed() && (r->empty_time() < shrink_before)) {\n+      has_work = true;\n+      break;\n+    }\n+  }\n+\n+  if (has_work) {\n+    static const char* msg = \"Concurrent uncommit\";\n+    ShenandoahConcurrentPhase gcPhase(msg, ShenandoahPhaseTimings::conc_uncommit, true \/* log_heap_usage *\/);\n+    EventMark em(\"%s\", msg);\n+\n+    op_uncommit(shrink_before, shrink_until);\n+  }\n+}\n+\n@@ -907,2 +926,18 @@\n-    control_thread()->notify_heap_changed();\n-    regulator_thread()->notify_heap_changed();\n+    notify_heap_changed();\n+  }\n+}\n+\n+bool ShenandoahHeap::check_soft_max_changed() {\n+  size_t new_soft_max = Atomic::load(&SoftMaxHeapSize);\n+  size_t old_soft_max = soft_max_capacity();\n+  if (new_soft_max != old_soft_max) {\n+    new_soft_max = MAX2(min_capacity(), new_soft_max);\n+    new_soft_max = MIN2(max_capacity(), new_soft_max);\n+    if (new_soft_max != old_soft_max) {\n+      log_info(gc)(\"Soft Max Heap Size: \" SIZE_FORMAT \"%s -> \" SIZE_FORMAT \"%s\",\n+                   byte_size_in_proper_unit(old_soft_max), proper_unit_for_byte_size(old_soft_max),\n+                   byte_size_in_proper_unit(new_soft_max), proper_unit_for_byte_size(new_soft_max)\n+      );\n+      set_soft_max_capacity(new_soft_max);\n+      return true;\n+    }\n@@ -910,0 +945,1 @@\n+  return false;\n@@ -912,14 +948,5 @@\n-void ShenandoahHeap::handle_old_evacuation(HeapWord* obj, size_t words, bool promotion) {\n-  \/\/ Only register the copy of the object that won the evacuation race.\n-  card_scan()->register_object_without_lock(obj);\n-\n-  \/\/ Mark the entire range of the evacuated object as dirty.  At next remembered set scan,\n-  \/\/ we will clear dirty bits that do not hold interesting pointers.  It's more efficient to\n-  \/\/ do this in batch, in a background GC thread than to try to carefully dirty only cards\n-  \/\/ that hold interesting pointers right now.\n-  card_scan()->mark_range_as_dirty(obj, words);\n-\n-  if (promotion) {\n-    \/\/ This evacuation was a promotion, track this as allocation against old gen\n-    old_generation()->increase_allocated(words * HeapWordSize);\n-  }\n+void ShenandoahHeap::notify_heap_changed() {\n+  \/\/ Update monitoring counters when we took a new region. This amortizes the\n+  \/\/ update costs on slow path.\n+  monitoring_support()->notify_heap_changed();\n+  _heap_changed.set();\n@@ -928,4 +955,2 @@\n-void ShenandoahHeap::handle_old_evacuation_failure() {\n-  if (_old_gen_oom_evac.try_set()) {\n-    log_info(gc)(\"Old gen evac failure.\");\n-  }\n+void ShenandoahHeap::set_forced_counters_update(bool value) {\n+  monitoring_support()->set_forced_counters_update(value);\n@@ -934,40 +959,2 @@\n-void ShenandoahHeap::report_promotion_failure(Thread* thread, size_t size) {\n-  \/\/ We squelch excessive reports to reduce noise in logs.\n-  const size_t MaxReportsPerEpoch = 4;\n-  static size_t last_report_epoch = 0;\n-  static size_t epoch_report_count = 0;\n-\n-  size_t promotion_reserve;\n-  size_t promotion_expended;\n-\n-  size_t gc_id = control_thread()->get_gc_id();\n-\n-  if ((gc_id != last_report_epoch) || (epoch_report_count++ < MaxReportsPerEpoch)) {\n-    {\n-      \/\/ Promotion failures should be very rare.  Invest in providing useful diagnostic info.\n-      ShenandoahHeapLocker locker(lock());\n-      promotion_reserve = get_promoted_reserve();\n-      promotion_expended = get_promoted_expended();\n-    }\n-    PLAB* plab = ShenandoahThreadLocalData::plab(thread);\n-    size_t words_remaining = (plab == nullptr)? 0: plab->words_remaining();\n-    const char* promote_enabled = ShenandoahThreadLocalData::allow_plab_promotions(thread)? \"enabled\": \"disabled\";\n-    ShenandoahGeneration* old_gen = old_generation();\n-    size_t old_capacity = old_gen->max_capacity();\n-    size_t old_usage = old_gen->used();\n-    size_t old_free_regions = old_gen->free_unaffiliated_regions();\n-\n-    log_info(gc, ergo)(\"Promotion failed, size \" SIZE_FORMAT \", has plab? %s, PLAB remaining: \" SIZE_FORMAT\n-                       \", plab promotions %s, promotion reserve: \" SIZE_FORMAT \", promotion expended: \" SIZE_FORMAT\n-                       \", old capacity: \" SIZE_FORMAT \", old_used: \" SIZE_FORMAT \", old unaffiliated regions: \" SIZE_FORMAT,\n-                       size * HeapWordSize, plab == nullptr? \"no\": \"yes\",\n-                       words_remaining * HeapWordSize, promote_enabled, promotion_reserve, promotion_expended,\n-                       old_capacity, old_usage, old_free_regions);\n-\n-    if ((gc_id == last_report_epoch) && (epoch_report_count >= MaxReportsPerEpoch)) {\n-      log_info(gc, ergo)(\"Squelching additional promotion failure reports for current epoch\");\n-    } else if (gc_id != last_report_epoch) {\n-      last_report_epoch = gc_id;\n-      epoch_report_count = 1;\n-    }\n-  }\n+void ShenandoahHeap::handle_force_counters_update() {\n+  monitoring_support()->handle_force_counters_update();\n@@ -1140,1 +1127,1 @@\n-    unexpend_promoted(not_promoted);\n+    old_generation()->unexpend_promoted(not_promoted);\n@@ -1181,103 +1168,0 @@\n-\/\/ Make sure old-generation is large enough, but no larger than is necessary, to hold mixed evacuations\n-\/\/ and promotions, if we anticipate either. Any deficit is provided by the young generation, subject to\n-\/\/ xfer_limit, and any excess is transferred to the young generation.\n-\/\/ xfer_limit is the maximum we're able to transfer from young to old.\n-void ShenandoahHeap::adjust_generation_sizes_for_next_cycle(\n-  size_t xfer_limit, size_t young_cset_regions, size_t old_cset_regions) {\n-\n-  \/\/ We can limit the old reserve to the size of anticipated promotions:\n-  \/\/ max_old_reserve is an upper bound on memory evacuated from old and promoted to old,\n-  \/\/ clamped by the old generation space available.\n-  \/\/\n-  \/\/ Here's the algebra.\n-  \/\/ Let SOEP = ShenandoahOldEvacRatioPercent,\n-  \/\/     OE = old evac,\n-  \/\/     YE = young evac, and\n-  \/\/     TE = total evac = OE + YE\n-  \/\/ By definition:\n-  \/\/            SOEP\/100 = OE\/TE\n-  \/\/                     = OE\/(OE+YE)\n-  \/\/  => SOEP\/(100-SOEP) = OE\/((OE+YE)-OE)      \/\/ componendo-dividendo: If a\/b = c\/d, then a\/(b-a) = c\/(d-c)\n-  \/\/                     = OE\/YE\n-  \/\/  =>              OE = YE*SOEP\/(100-SOEP)\n-\n-  \/\/ We have to be careful in the event that SOEP is set to 100 by the user.\n-  assert(ShenandoahOldEvacRatioPercent <= 100, \"Error\");\n-  const size_t old_available = old_generation()->available();\n-  \/\/ The free set will reserve this amount of memory to hold young evacuations\n-  const size_t young_reserve = (young_generation()->max_capacity() * ShenandoahEvacReserve) \/ 100;\n-\n-  \/\/ In the case that ShenandoahOldEvacRatioPercent equals 100, max_old_reserve is limited only by xfer_limit.\n-\n-  const size_t bound_on_old_reserve = old_available + xfer_limit + young_reserve;\n-  const size_t max_old_reserve = (ShenandoahOldEvacRatioPercent == 100)?\n-    bound_on_old_reserve: MIN2((young_reserve * ShenandoahOldEvacRatioPercent) \/ (100 - ShenandoahOldEvacRatioPercent),\n-                               bound_on_old_reserve);\n-\n-  const size_t region_size_bytes = ShenandoahHeapRegion::region_size_bytes();\n-\n-  \/\/ Decide how much old space we should reserve for a mixed collection\n-  size_t reserve_for_mixed = 0;\n-  const size_t mixed_candidates = old_heuristics()->unprocessed_old_collection_candidates();\n-  const bool doing_mixed = (mixed_candidates > 0);\n-  if (doing_mixed) {\n-    \/\/ We want this much memory to be unfragmented in order to reliably evacuate old.  This is conservative because we\n-    \/\/ may not evacuate the entirety of unprocessed candidates in a single mixed evacuation.\n-    size_t max_evac_need = (size_t)\n-      (old_heuristics()->unprocessed_old_collection_candidates_live_memory() * ShenandoahOldEvacWaste);\n-    assert(old_available >= old_generation()->free_unaffiliated_regions() * region_size_bytes,\n-           \"Unaffiliated available must be less than total available\");\n-    size_t old_fragmented_available =\n-      old_available - old_generation()->free_unaffiliated_regions() * region_size_bytes;\n-    reserve_for_mixed = max_evac_need + old_fragmented_available;\n-    if (reserve_for_mixed > max_old_reserve) {\n-      reserve_for_mixed = max_old_reserve;\n-    }\n-  }\n-\n-  \/\/ Decide how much space we should reserve for promotions from young\n-  size_t reserve_for_promo = 0;\n-  const size_t promo_load = get_promotion_potential();\n-  const bool doing_promotions = promo_load > 0;\n-  if (doing_promotions) {\n-    \/\/ We're promoting and have a bound on the maximum amount that can be promoted\n-    assert(max_old_reserve >= reserve_for_mixed, \"Sanity\");\n-    const size_t available_for_promotions = max_old_reserve - reserve_for_mixed;\n-    reserve_for_promo = MIN2((size_t)(promo_load * ShenandoahPromoEvacWaste), available_for_promotions);\n-  }\n-\n-  \/\/ This is the total old we want to ideally reserve\n-  const size_t old_reserve = reserve_for_mixed + reserve_for_promo;\n-  assert(old_reserve <= max_old_reserve, \"cannot reserve more than max for old evacuations\");\n-\n-  \/\/ We now check if the old generation is running a surplus or a deficit.\n-  size_t old_region_deficit = 0;\n-  size_t old_region_surplus = 0;\n-\n-  const size_t max_old_available = old_generation()->available() + old_cset_regions * region_size_bytes;\n-  if (max_old_available >= old_reserve) {\n-    \/\/ We are running a surplus, so the old region surplus can go to young\n-    const size_t old_surplus = max_old_available - old_reserve;\n-    old_region_surplus = old_surplus \/ region_size_bytes;\n-    const size_t unaffiliated_old_regions = old_generation()->free_unaffiliated_regions() + old_cset_regions;\n-    old_region_surplus = MIN2(old_region_surplus, unaffiliated_old_regions);\n-  } else {\n-    \/\/ We are running a deficit which we'd like to fill from young.\n-    \/\/ Ignore that this will directly impact young_generation()->max_capacity(),\n-    \/\/ indirectly impacting young_reserve and old_reserve.  These computations are conservative.\n-    const size_t old_need = old_reserve - max_old_available;\n-    \/\/ The old region deficit (rounded up) will come from young\n-    old_region_deficit = (old_need + region_size_bytes - 1) \/ region_size_bytes;\n-\n-    \/\/ Round down the regions we can transfer from young to old. If we're running short\n-    \/\/ on young-gen memory, we restrict the xfer. Old-gen collection activities will be\n-    \/\/ curtailed if the budget is restricted.\n-    const size_t max_old_region_xfer = xfer_limit \/ region_size_bytes;\n-    old_region_deficit = MIN2(old_region_deficit, max_old_region_xfer);\n-  }\n-  assert(old_region_deficit == 0 || old_region_surplus == 0, \"Only surplus or deficit, never both\");\n-\n-  set_old_region_surplus(old_region_surplus);\n-  set_old_region_deficit(old_region_deficit);\n-}\n-\n@@ -1348,1 +1232,13 @@\n-    \/\/ Allocation failed, block until control thread reacted, then retry allocation.\n+    \/\/ Check that gc overhead is not exceeded.\n+    \/\/\n+    \/\/ Shenandoah will grind along for quite a while allocating one\n+    \/\/ object at a time using shared (non-tlab) allocations. This check\n+    \/\/ is testing that the GC overhead limit has not been exceeded.\n+    \/\/ This will notify the collector to start a cycle, but will raise\n+    \/\/ an OOME to the mutator if the last Full GCs have not made progress.\n+    if (result == nullptr && !req.is_lab_alloc() && get_gc_no_progress_count() > ShenandoahNoProgressThreshold) {\n+      control_thread()->handle_alloc_failure(req, false);\n+      return nullptr;\n+    }\n+\n+    \/\/ Block until control thread reacted, then retry allocation.\n@@ -1357,2 +1253,2 @@\n-        && (_progress_last_gc.is_set() || original_count == shenandoah_policy()->full_gc_count())) {\n-      control_thread()->handle_alloc_failure(req);\n+        && (get_gc_no_progress_count() == 0 || original_count == shenandoah_policy()->full_gc_count())) {\n+      control_thread()->handle_alloc_failure(req, true);\n@@ -1362,0 +1258,6 @@\n+    if (log_is_enabled(Debug, gc, alloc)) {\n+      ResourceMark rm;\n+      log_debug(gc, alloc)(\"Thread: %s, Result: \" PTR_FORMAT \", Request: %s, Size: \" SIZE_FORMAT \", Original: \" SIZE_FORMAT \", Latest: \" SIZE_FORMAT,\n+                           Thread::current()->name(), p2i(result), req.type_string(), req.size(), original_count, get_gc_no_progress_count());\n+    }\n+\n@@ -1370,2 +1272,1 @@\n-    control_thread()->notify_heap_changed();\n-    regulator_thread()->notify_heap_changed();\n+    notify_heap_changed();\n@@ -1413,1 +1314,6 @@\n-    ShenandoahHeapLocker locker(lock());\n+\n+    \/\/ If we are dealing with mutator allocation, then we may need to block for safepoint.\n+    \/\/ We cannot block for safepoint for GC allocations, because there is a high chance\n+    \/\/ we are already running at safepoint or from stack watermark machinery, and we cannot\n+    \/\/ block again.\n+    ShenandoahHeapLocker locker(lock(), req.is_mutator_alloc());\n@@ -1437,2 +1343,2 @@\n-          size_t promotion_avail = get_promoted_reserve();\n-          size_t promotion_expended = get_promoted_expended();\n+          size_t promotion_avail = old_generation()->get_promoted_reserve();\n+          size_t promotion_expended = old_generation()->get_promoted_expended();\n@@ -1441,1 +1347,1 @@\n-            if (get_old_evac_reserve() == 0) {\n+            if (old_generation()->get_evacuation_reserve() == 0) {\n@@ -1452,2 +1358,2 @@\n-          size_t promotion_avail = get_promoted_reserve();\n-          size_t promotion_expended = get_promoted_expended();\n+          size_t promotion_avail = old_generation()->get_promoted_reserve();\n+          size_t promotion_expended = old_generation()->get_promoted_expended();\n@@ -1484,1 +1390,1 @@\n-              if (get_promoted_expended() + actual_size <= get_promoted_reserve()) {\n+              if (old_generation()->get_promoted_expended() + actual_size <= old_generation()->get_promoted_reserve()) {\n@@ -1488,2 +1394,1 @@\n-                expend_promoted(actual_size);\n-                assert(get_promoted_expended() <= get_promoted_reserve(), \"Do not expend more promotion than budgeted\");\n+                old_generation()->expend_promoted(actual_size);\n@@ -1504,2 +1409,1 @@\n-            expend_promoted(requested_bytes);\n-            assert(get_promoted_expended() <= get_promoted_reserve(), \"Do not expend more promotion than budgeted\");\n+            old_generation()->expend_promoted(requested_bytes);\n@@ -1794,0 +1698,177 @@\n+\/\/ try_evacuate_object registers the object and dirties the associated remembered set information when evacuating\n+\/\/ to OLD_GENERATION.\n+oop ShenandoahHeap::try_evacuate_object(oop p, Thread* thread, ShenandoahHeapRegion* from_region,\n+                                               ShenandoahAffiliation target_gen) {\n+  bool alloc_from_lab = true;\n+  bool has_plab = false;\n+  HeapWord* copy = nullptr;\n+  size_t size = p->size();\n+  bool is_promotion = (target_gen == OLD_GENERATION) && from_region->is_young();\n+\n+#ifdef ASSERT\n+  if (ShenandoahOOMDuringEvacALot &&\n+      (os::random() & 1) == 0) { \/\/ Simulate OOM every ~2nd slow-path call\n+    copy = nullptr;\n+  } else {\n+#endif\n+    if (UseTLAB) {\n+      switch (target_gen) {\n+        case YOUNG_GENERATION: {\n+          copy = allocate_from_gclab(thread, size);\n+          if ((copy == nullptr) && (size < ShenandoahThreadLocalData::gclab_size(thread))) {\n+            \/\/ GCLAB allocation failed because we are bumping up against the limit on young evacuation reserve.  Try resetting\n+            \/\/ the desired GCLAB size and retry GCLAB allocation to avoid cascading of shared memory allocations.\n+            ShenandoahThreadLocalData::set_gclab_size(thread, PLAB::min_size());\n+            copy = allocate_from_gclab(thread, size);\n+            \/\/ If we still get nullptr, we'll try a shared allocation below.\n+          }\n+          break;\n+        }\n+        case OLD_GENERATION: {\n+          PLAB* plab = ShenandoahThreadLocalData::plab(thread);\n+          if (plab != nullptr) {\n+            has_plab = true;\n+          }\n+          copy = allocate_from_plab(thread, size, is_promotion);\n+          if ((copy == nullptr) && (size < ShenandoahThreadLocalData::plab_size(thread)) &&\n+              ShenandoahThreadLocalData::plab_retries_enabled(thread)) {\n+            \/\/ PLAB allocation failed because we are bumping up against the limit on old evacuation reserve or because\n+            \/\/ the requested object does not fit within the current plab but the plab still has an \"abundance\" of memory,\n+            \/\/ where abundance is defined as >= PLAB::min_size().  In the former case, we try resetting the desired\n+            \/\/ PLAB size and retry PLAB allocation to avoid cascading of shared memory allocations.\n+\n+            \/\/ In this situation, PLAB memory is precious.  We'll try to preserve our existing PLAB by forcing\n+            \/\/ this particular allocation to be shared.\n+            if (plab->words_remaining() < PLAB::min_size()) {\n+              ShenandoahThreadLocalData::set_plab_size(thread, PLAB::min_size());\n+              copy = allocate_from_plab(thread, size, is_promotion);\n+              \/\/ If we still get nullptr, we'll try a shared allocation below.\n+              if (copy == nullptr) {\n+                \/\/ If retry fails, don't continue to retry until we have success (probably in next GC pass)\n+                ShenandoahThreadLocalData::disable_plab_retries(thread);\n+              }\n+            }\n+            \/\/ else, copy still equals nullptr.  this causes shared allocation below, preserving this plab for future needs.\n+          }\n+          break;\n+        }\n+        default: {\n+          ShouldNotReachHere();\n+          break;\n+        }\n+      }\n+    }\n+\n+    if (copy == nullptr) {\n+      \/\/ If we failed to allocate in LAB, we'll try a shared allocation.\n+      if (!is_promotion || !has_plab || (size > PLAB::min_size())) {\n+        ShenandoahAllocRequest req = ShenandoahAllocRequest::for_shared_gc(size, target_gen);\n+        copy = allocate_memory(req, is_promotion);\n+        alloc_from_lab = false;\n+      }\n+      \/\/ else, we leave copy equal to nullptr, signaling a promotion failure below if appropriate.\n+      \/\/ We choose not to promote objects smaller than PLAB::min_size() by way of shared allocations, as this is too\n+      \/\/ costly.  Instead, we'll simply \"evacuate\" to young-gen memory (using a GCLAB) and will promote in a future\n+      \/\/ evacuation pass.  This condition is denoted by: is_promotion && has_plab && (size <= PLAB::min_size())\n+    }\n+#ifdef ASSERT\n+  }\n+#endif\n+\n+  if (copy == nullptr) {\n+    if (target_gen == OLD_GENERATION) {\n+      assert(mode()->is_generational(), \"Should only be here in generational mode.\");\n+      if (from_region->is_young()) {\n+        \/\/ Signal that promotion failed. Will evacuate this old object somewhere in young gen.\n+        old_generation()->handle_failed_promotion(thread, size);\n+        return nullptr;\n+      } else {\n+        \/\/ Remember that evacuation to old gen failed. We'll want to trigger a full gc to recover from this\n+        \/\/ after the evacuation threads have finished.\n+        old_generation()->handle_failed_evacuation();\n+      }\n+    }\n+\n+    control_thread()->handle_alloc_failure_evac(size);\n+\n+    _oom_evac_handler.handle_out_of_memory_during_evacuation();\n+\n+    return ShenandoahBarrierSet::resolve_forwarded(p);\n+  }\n+\n+  \/\/ Copy the object:\n+  _evac_tracker->begin_evacuation(thread, size * HeapWordSize);\n+  Copy::aligned_disjoint_words(cast_from_oop<HeapWord*>(p), copy, size);\n+\n+  oop copy_val = cast_to_oop(copy);\n+\n+  if (mode()->is_generational() && target_gen == YOUNG_GENERATION && is_aging_cycle()) {\n+    ShenandoahHeap::increase_object_age(copy_val, from_region->age() + 1);\n+  }\n+\n+  \/\/ Try to install the new forwarding pointer.\n+  ContinuationGCSupport::relativize_stack_chunk(copy_val);\n+\n+  oop result = ShenandoahForwarding::try_update_forwardee(p, copy_val);\n+  if (result == copy_val) {\n+    \/\/ Successfully evacuated. Our copy is now the public one!\n+    _evac_tracker->end_evacuation(thread, size * HeapWordSize);\n+    if (mode()->is_generational()) {\n+      if (target_gen == OLD_GENERATION) {\n+        old_generation()->handle_evacuation(copy, size, from_region->is_young());\n+      } else {\n+        \/\/ When copying to the old generation above, we don't care\n+        \/\/ about recording object age in the census stats.\n+        assert(target_gen == YOUNG_GENERATION, \"Error\");\n+        \/\/ We record this census only when simulating pre-adaptive tenuring behavior, or\n+        \/\/ when we have been asked to record the census at evacuation rather than at mark\n+        if (ShenandoahGenerationalCensusAtEvac || !ShenandoahGenerationalAdaptiveTenuring) {\n+          _evac_tracker->record_age(thread, size * HeapWordSize, ShenandoahHeap::get_object_age(copy_val));\n+        }\n+      }\n+    }\n+    shenandoah_assert_correct(nullptr, copy_val);\n+    return copy_val;\n+  }  else {\n+    \/\/ Failed to evacuate. We need to deal with the object that is left behind. Since this\n+    \/\/ new allocation is certainly after TAMS, it will be considered live in the next cycle.\n+    \/\/ But if it happens to contain references to evacuated regions, those references would\n+    \/\/ not get updated for this stale copy during this cycle, and we will crash while scanning\n+    \/\/ it the next cycle.\n+    if (alloc_from_lab) {\n+      \/\/ For LAB allocations, it is enough to rollback the allocation ptr. Either the next\n+      \/\/ object will overwrite this stale copy, or the filler object on LAB retirement will\n+      \/\/ do this.\n+      switch (target_gen) {\n+        case YOUNG_GENERATION: {\n+          ShenandoahThreadLocalData::gclab(thread)->undo_allocation(copy, size);\n+          break;\n+        }\n+        case OLD_GENERATION: {\n+          ShenandoahThreadLocalData::plab(thread)->undo_allocation(copy, size);\n+          if (is_promotion) {\n+            ShenandoahThreadLocalData::subtract_from_plab_promoted(thread, size * HeapWordSize);\n+          } else {\n+            ShenandoahThreadLocalData::subtract_from_plab_evacuated(thread, size * HeapWordSize);\n+          }\n+          break;\n+        }\n+        default: {\n+          ShouldNotReachHere();\n+          break;\n+        }\n+      }\n+    } else {\n+      \/\/ For non-LAB allocations, we have no way to retract the allocation, and\n+      \/\/ have to explicitly overwrite the copy with the filler object. With that overwrite,\n+      \/\/ we have to keep the fwdptr initialized and pointing to our (stale) copy.\n+      assert(size >= ShenandoahHeap::min_fill_size(), \"previously allocated object known to be larger than min_size\");\n+      fill_with_object(copy, size);\n+      shenandoah_assert_correct(nullptr, copy_val);\n+      \/\/ For non-LAB allocations, the object has already been registered\n+    }\n+    shenandoah_assert_correct(nullptr, result);\n+    return result;\n+  }\n+}\n+\n@@ -1943,2 +2024,1 @@\n-    \/\/ Return the max allowed size, and let the allocation path\n-    \/\/ figure out the safe size for current allocation.\n+    \/\/ Return the max allowed size, and let the allocation path figure out the safe size for current allocation.\n@@ -1990,2 +2070,4 @@\n-  tcl->do_thread(_control_thread);\n-  tcl->do_thread(_regulator_thread);\n+  if (_control_thread != nullptr) {\n+    tcl->do_thread(_control_thread);\n+  }\n+\n@@ -2021,0 +2103,2 @@\n+  shenandoah_policy()->record_collection_cause(cause);\n+\n@@ -2024,1 +2108,0 @@\n-  shenandoah_policy()->record_cycle_start();\n@@ -2420,4 +2503,0 @@\n-void ShenandoahHeap::set_evacuation_reserve_quantities(bool is_valid) {\n-  _has_evacuation_reserve_quantities = is_valid;\n-}\n-\n@@ -2539,4 +2618,1 @@\n-  \/\/ Step 2. Stop requesting collections.\n-  regulator_thread()->stop();\n-\n-  \/\/ Step 3. Notify control thread that we are in shutdown.\n+  \/\/ Step 2. Notify control thread that we are in shutdown.\n@@ -2547,1 +2623,1 @@\n-  \/\/ Step 4. Notify GC workers that we are cancelling GC.\n+  \/\/ Step 3. Notify GC workers that we are cancelling GC.\n@@ -2550,1 +2626,1 @@\n-  \/\/ Step 5. Wait until GC worker exits normally.\n+  \/\/ Step 4. Wait until GC worker exits normally.\n@@ -2749,6 +2825,3 @@\n-    if (UseDynamicNumberOfGCThreads) {\n-      assert(nworkers <= ParallelGCThreads, \"Cannot use more than it has\");\n-    } else {\n-      \/\/ Use ParallelGCThreads inside safepoints\n-      assert(nworkers == ParallelGCThreads, \"Use ParallelGCThreads within safepoints\");\n-    }\n+    \/\/ Use ParallelGCThreads inside safepoints\n+    assert(nworkers == ParallelGCThreads, \"Use ParallelGCThreads (%u) within safepoint, not %u\",\n+           ParallelGCThreads, nworkers);\n@@ -2756,6 +2829,3 @@\n-    if (UseDynamicNumberOfGCThreads) {\n-      assert(nworkers <= ConcGCThreads, \"Cannot use more than it has\");\n-    } else {\n-      \/\/ Use ConcGCThreads outside safepoints\n-      assert(nworkers == ConcGCThreads, \"Use ConcGCThreads outside safepoints\");\n-    }\n+    \/\/ Use ConcGCThreads outside safepoints\n+    assert(nworkers == ConcGCThreads, \"Use ConcGCThreads (%u) outside safepoints, %u\",\n+           ConcGCThreads, nworkers);\n@@ -3102,1 +3172,1 @@\n-    adjust_generation_sizes_for_next_cycle(allocation_runway, young_cset_regions, old_cset_regions);\n+    ShenandoahGenerationalHeap::heap()->compute_old_generation_balance(allocation_runway, old_cset_regions);\n@@ -3248,8 +3318,0 @@\n-void ShenandoahHeap::entry_uncommit(double shrink_before, size_t shrink_until) {\n-  static const char *msg = \"Concurrent uncommit\";\n-  ShenandoahConcurrentPhase gc_phase(msg, ShenandoahPhaseTimings::conc_uncommit, true \/* log_heap_usage *\/);\n-  EventMark em(\"%s\", msg);\n-\n-  op_uncommit(shrink_before, shrink_until);\n-}\n-\n@@ -3271,12 +3333,3 @@\n-  if (mode()->is_generational()) {\n-    _young_gen_memory_pool = new ShenandoahYoungGenMemoryPool(this);\n-    _old_gen_memory_pool = new ShenandoahOldGenMemoryPool(this);\n-    _cycle_memory_manager.add_pool(_young_gen_memory_pool);\n-    _cycle_memory_manager.add_pool(_old_gen_memory_pool);\n-    _stw_memory_manager.add_pool(_young_gen_memory_pool);\n-    _stw_memory_manager.add_pool(_old_gen_memory_pool);\n-  } else {\n-    _memory_pool = new ShenandoahMemoryPool(this);\n-    _cycle_memory_manager.add_pool(_memory_pool);\n-    _stw_memory_manager.add_pool(_memory_pool);\n-  }\n+  _memory_pool = new ShenandoahMemoryPool(this);\n+  _cycle_memory_manager.add_pool(_memory_pool);\n+  _stw_memory_manager.add_pool(_memory_pool);\n@@ -3294,6 +3347,1 @@\n-  if (mode()->is_generational()) {\n-    memory_pools.append(_young_gen_memory_pool);\n-    memory_pools.append(_old_gen_memory_pool);\n-  } else {\n-    memory_pools.append(_memory_pool);\n-  }\n+  memory_pools.append(_memory_pool);\n@@ -3392,1 +3440,1 @@\n-void ShenandoahGenerationRegionClosure<GLOBAL_GEN>::heap_region_do(ShenandoahHeapRegion* region) {\n+void ShenandoahGenerationRegionClosure<GLOBAL>::heap_region_do(ShenandoahHeapRegion* region) {\n@@ -3397,1 +3445,1 @@\n-void ShenandoahGenerationRegionClosure<GLOBAL_NON_GEN>::heap_region_do(ShenandoahHeapRegion* region) {\n+void ShenandoahGenerationRegionClosure<NON_GEN>::heap_region_do(ShenandoahHeapRegion* region) {\n","filename":"src\/hotspot\/share\/gc\/shenandoah\/shenandoahHeap.cpp","additions":304,"deletions":256,"binary":false,"changes":560,"status":"modified"},{"patch":"@@ -39,0 +39,1 @@\n+#include \"gc\/shenandoah\/shenandoahController.hpp\"\n@@ -57,1 +58,0 @@\n-class ShenandoahControlThread;\n@@ -152,0 +152,1 @@\n+\n@@ -225,4 +226,0 @@\n-  size_t _promotion_potential;\n-  size_t _pad_for_promote_in_place;    \/\/ bytes of filler\n-  size_t _promotable_humongous_regions;\n-  size_t _regular_regions_promoted_in_place;\n@@ -258,0 +255,9 @@\n+\/\/ ---------- Periodic Tasks\n+\/\/\n+private:\n+  void notify_heap_changed();\n+\n+public:\n+  void set_forced_counters_update(bool value);\n+  void handle_force_counters_update();\n+\n@@ -265,0 +271,2 @@\n+  virtual void initialize_controller();\n+\n@@ -349,0 +357,1 @@\n+  ShenandoahSharedFlag   _heap_changed;\n@@ -352,1 +361,0 @@\n-  ShenandoahSharedFlag   _progress_last_gc;\n@@ -355,30 +363,1 @@\n-  \/\/ TODO: Revisit the following comment.  It may not accurately represent the true behavior when evacuations fail due to\n-  \/\/ difficulty finding memory to hold evacuated objects.\n-  \/\/\n-  \/\/ Note that the typical total expenditure on evacuation is less than the associated evacuation reserve because we generally\n-  \/\/ reserve ShenandoahEvacWaste (> 1.0) times the anticipated evacuation need.  In the case that there is an excessive amount\n-  \/\/ of waste, it may be that one thread fails to grab a new GCLAB, this does not necessarily doom the associated evacuation\n-  \/\/ effort.  If this happens, the requesting thread blocks until some other thread manages to evacuate the offending object.\n-  \/\/ Only after \"all\" threads fail to evacuate an object do we consider the evacuation effort to have failed.\n-\n-  size_t _promoted_reserve;            \/\/ Bytes reserved within old-gen to hold the results of promotion\n-  volatile size_t _promoted_expended;  \/\/ Bytes of old-gen memory expended on promotions\n-\n-  size_t _old_evac_reserve;            \/\/ Bytes reserved within old-gen to hold evacuated objects from old-gen collection set\n-  size_t _young_evac_reserve;          \/\/ Bytes reserved within young-gen to hold evacuated objects from young-gen collection set\n-\n-  ShenandoahAgeCensus* _age_census;    \/\/ Age census used for adapting tenuring threshold in generational mode\n-\n-  \/\/ At the end of final mark, but before we begin evacuating, heuristics calculate how much memory is required to\n-  \/\/ hold the results of evacuating to young-gen and to old-gen.  These quantitites, stored in _promoted_reserve,\n-  \/\/ _old_evac_reserve, and _young_evac_reserve, are consulted prior to rebuilding the free set (ShenandoahFreeSet)\n-  \/\/ in preparation for evacuation.  When the free set is rebuilt, we make sure to reserve sufficient memory in the\n-  \/\/ collector and old_collector sets to hold if _has_evacuation_reserve_quantities is true.  The other time we\n-  \/\/ rebuild the freeset is at the end of GC, as we prepare to idle GC until the next trigger.  In this case,\n-  \/\/ _has_evacuation_reserve_quantities is false because we don't yet know how much memory will need to be evacuated\n-  \/\/ in the next GC cycle.  When _has_evacuation_reserve_quantities is false, the free set rebuild operation reserves\n-  \/\/ for the collector and old_collector sets based on alternative mechanisms, such as ShenandoahEvacReserve,\n-  \/\/ ShenandoahOldEvacReserve, and ShenandoahOldCompactionReserve.  In a future planned enhancement, the reserve\n-  \/\/ for old_collector set when not _has_evacuation_reserve_quantities is based in part on anticipated promotion as\n-  \/\/ determined by analysis of live data found during the previous GC pass which is one less than the current tenure age.\n-  bool _has_evacuation_reserve_quantities;\n+  size_t _gc_no_progress_count;\n@@ -389,0 +368,2 @@\n+  ShenandoahAgeCensus* _age_census;    \/\/ Age census used for adapting tenuring threshold in generational mode\n+\n@@ -400,1 +381,6 @@\n-  void set_evacuation_reserve_quantities(bool is_valid);\n+  \/\/ Returns true if allocations have occurred in new regions or if regions have been\n+  \/\/ uncommitted since the previous calls. This call will reset the flag to false.\n+  bool has_changed() {\n+    return _heap_changed.try_unset();\n+  }\n+\n@@ -416,1 +402,1 @@\n-  inline bool has_evacuation_reserve_quantities() const;\n+\n@@ -433,32 +419,0 @@\n-  inline void clear_promotion_potential() { _promotion_potential = 0; };\n-  inline void set_promotion_potential(size_t val) { _promotion_potential = val; };\n-  inline size_t get_promotion_potential() { return _promotion_potential; };\n-\n-  inline void set_pad_for_promote_in_place(size_t pad) { _pad_for_promote_in_place = pad; }\n-  inline size_t get_pad_for_promote_in_place() { return _pad_for_promote_in_place; }\n-\n-  inline void reserve_promotable_humongous_regions(size_t region_count) { _promotable_humongous_regions = region_count; }\n-  inline void reserve_promotable_regular_regions(size_t region_count) { _regular_regions_promoted_in_place = region_count; }\n-\n-  inline size_t get_promotable_humongous_regions() { return _promotable_humongous_regions; }\n-  inline size_t get_regular_regions_promoted_in_place() { return _regular_regions_promoted_in_place; }\n-\n-  \/\/ Returns previous value\n-  inline size_t set_promoted_reserve(size_t new_val);\n-  inline size_t get_promoted_reserve() const;\n-  inline void augment_promo_reserve(size_t increment);\n-\n-  inline void reset_promoted_expended();\n-  inline size_t expend_promoted(size_t increment);\n-  inline size_t unexpend_promoted(size_t decrement);\n-  inline size_t get_promoted_expended();\n-\n-  \/\/ Returns previous value\n-  inline size_t set_old_evac_reserve(size_t new_val);\n-  inline size_t get_old_evac_reserve() const;\n-  inline void augment_old_evac_reserve(size_t increment);\n-\n-  \/\/ Returns previous value\n-  inline size_t set_young_evac_reserve(size_t new_val);\n-  inline size_t get_young_evac_reserve() const;\n-\n@@ -500,2 +454,3 @@\n-  \/\/ Elastic heap support\n-  void entry_uncommit(double shrink_before, size_t shrink_until);\n+  \/\/ These will uncommit empty regions if heap::committed > shrink_until\n+  \/\/ and there exists at least one region which was made empty before shrink_before.\n+  void maybe_uncommit(double shrink_before, size_t shrink_until);\n@@ -504,0 +459,3 @@\n+  \/\/ Returns true if the soft maximum heap has been changed using management APIs.\n+  bool check_soft_max_changed();\n+\n@@ -523,2 +481,3 @@\n-  void notify_gc_progress()    { _progress_last_gc.set();   }\n-  void notify_gc_no_progress() { _progress_last_gc.unset(); }\n+  void notify_gc_progress();\n+  void notify_gc_no_progress();\n+  size_t get_gc_no_progress_count() const;\n@@ -533,2 +492,4 @@\n-  ShenandoahControlThread*   _control_thread;\n-  ShenandoahRegulatorThread* _regulator_thread;\n+protected:\n+  ShenandoahController*  _control_thread;\n+\n+private:\n@@ -546,2 +507,0 @@\n-  ShenandoahRegulatorThread* regulator_thread()        { return _regulator_thread;  }\n-\n@@ -549,1 +508,2 @@\n-  ShenandoahControlThread*   control_thread()          { return _control_thread;    }\n+  ShenandoahController*   control_thread() { return _control_thread; }\n+\n@@ -565,1 +525,1 @@\n-  ShenandoahEvacuationTracker* evac_tracker()    const { return  _evac_tracker;     }\n+  ShenandoahEvacuationTracker* evac_tracker()    const { return _evac_tracker;      }\n@@ -577,3 +537,0 @@\n-  MemoryPool*                  _young_gen_memory_pool;\n-  MemoryPool*                  _old_gen_memory_pool;\n-\n@@ -698,4 +655,0 @@\n-  \/\/ How many bytes to transfer between old and young after we have finished recycling collection set regions?\n-  size_t _old_regions_surplus;\n-  size_t _old_regions_deficit;\n-\n@@ -733,6 +686,0 @@\n-  inline void set_old_region_surplus(size_t surplus) { _old_regions_surplus = surplus; };\n-  inline void set_old_region_deficit(size_t deficit) { _old_regions_deficit = deficit; };\n-\n-  inline size_t get_old_region_surplus() { return _old_regions_surplus; };\n-  inline size_t get_old_region_deficit() { return _old_regions_deficit; };\n-\n@@ -792,5 +739,0 @@\n-  ShenandoahSharedFlag _old_gen_oom_evac;\n-\n-  inline oop try_evacuate_object(oop src, Thread* thread, ShenandoahHeapRegion* from_region, ShenandoahAffiliation target_gen);\n-  void handle_old_evacuation(HeapWord* obj, size_t words, bool promotion);\n-  void handle_old_evacuation_failure();\n@@ -798,0 +740,1 @@\n+  oop try_evacuate_object(oop src, Thread* thread, ShenandoahHeapRegion* from_region, ShenandoahAffiliation target_gen);\n@@ -799,1 +742,0 @@\n-  void report_promotion_failure(Thread* thread, size_t size);\n@@ -819,2 +761,0 @@\n-  inline bool clear_old_evacuation_failure();\n-\n@@ -834,2 +774,0 @@\n-  void adjust_generation_sizes_for_next_cycle(size_t old_xfer_limit, size_t young_cset_regions, size_t old_cset_regions);\n-\n","filename":"src\/hotspot\/share\/gc\/shenandoah\/shenandoahHeap.hpp","additions":41,"deletions":103,"binary":false,"changes":144,"status":"modified"},{"patch":"@@ -44,1 +44,1 @@\n-#include \"gc\/shenandoah\/shenandoahControlThread.hpp\"\n+#include \"gc\/shenandoah\/shenandoahGenerationalControlThread.hpp\"\n@@ -46,1 +46,0 @@\n-#include \"gc\/shenandoah\/shenandoahScanRemembered.inline.hpp\"\n@@ -81,0 +80,12 @@\n+inline void ShenandoahHeap::notify_gc_progress() {\n+  Atomic::store(&_gc_no_progress_count, (size_t) 0);\n+\n+}\n+inline void ShenandoahHeap::notify_gc_no_progress() {\n+  Atomic::inc(&_gc_no_progress_count);\n+}\n+\n+inline size_t ShenandoahHeap::get_gc_no_progress_count() const {\n+  return Atomic::load(&_gc_no_progress_count);\n+}\n+\n@@ -363,177 +374,0 @@\n-\/\/ try_evacuate_object registers the object and dirties the associated remembered set information when evacuating\n-\/\/ to OLD_GENERATION.\n-inline oop ShenandoahHeap::try_evacuate_object(oop p, Thread* thread, ShenandoahHeapRegion* from_region,\n-                                               ShenandoahAffiliation target_gen) {\n-  bool alloc_from_lab = true;\n-  bool has_plab = false;\n-  HeapWord* copy = nullptr;\n-  size_t size = p->size();\n-  bool is_promotion = (target_gen == OLD_GENERATION) && from_region->is_young();\n-\n-#ifdef ASSERT\n-  if (ShenandoahOOMDuringEvacALot &&\n-      (os::random() & 1) == 0) { \/\/ Simulate OOM every ~2nd slow-path call\n-        copy = nullptr;\n-  } else {\n-#endif\n-    if (UseTLAB) {\n-      switch (target_gen) {\n-        case YOUNG_GENERATION: {\n-           copy = allocate_from_gclab(thread, size);\n-           if ((copy == nullptr) && (size < ShenandoahThreadLocalData::gclab_size(thread))) {\n-             \/\/ GCLAB allocation failed because we are bumping up against the limit on young evacuation reserve.  Try resetting\n-             \/\/ the desired GCLAB size and retry GCLAB allocation to avoid cascading of shared memory allocations.\n-             ShenandoahThreadLocalData::set_gclab_size(thread, PLAB::min_size());\n-             copy = allocate_from_gclab(thread, size);\n-             \/\/ If we still get nullptr, we'll try a shared allocation below.\n-           }\n-           break;\n-        }\n-        case OLD_GENERATION: {\n-           PLAB* plab = ShenandoahThreadLocalData::plab(thread);\n-           if (plab != nullptr) {\n-             has_plab = true;\n-           }\n-           copy = allocate_from_plab(thread, size, is_promotion);\n-           if ((copy == nullptr) && (size < ShenandoahThreadLocalData::plab_size(thread)) &&\n-               ShenandoahThreadLocalData::plab_retries_enabled(thread)) {\n-             \/\/ PLAB allocation failed because we are bumping up against the limit on old evacuation reserve or because\n-             \/\/ the requested object does not fit within the current plab but the plab still has an \"abundance\" of memory,\n-             \/\/ where abundance is defined as >= PLAB::min_size().  In the former case, we try resetting the desired\n-             \/\/ PLAB size and retry PLAB allocation to avoid cascading of shared memory allocations.\n-\n-             \/\/ In this situation, PLAB memory is precious.  We'll try to preserve our existing PLAB by forcing\n-             \/\/ this particular allocation to be shared.\n-             if (plab->words_remaining() < PLAB::min_size()) {\n-               ShenandoahThreadLocalData::set_plab_size(thread, PLAB::min_size());\n-               copy = allocate_from_plab(thread, size, is_promotion);\n-               \/\/ If we still get nullptr, we'll try a shared allocation below.\n-               if (copy == nullptr) {\n-                 \/\/ If retry fails, don't continue to retry until we have success (probably in next GC pass)\n-                 ShenandoahThreadLocalData::disable_plab_retries(thread);\n-               }\n-             }\n-             \/\/ else, copy still equals nullptr.  this causes shared allocation below, preserving this plab for future needs.\n-           }\n-           break;\n-        }\n-        default: {\n-          ShouldNotReachHere();\n-          break;\n-        }\n-      }\n-    }\n-\n-    if (copy == nullptr) {\n-      \/\/ If we failed to allocate in LAB, we'll try a shared allocation.\n-      if (!is_promotion || !has_plab || (size > PLAB::min_size())) {\n-        ShenandoahAllocRequest req = ShenandoahAllocRequest::for_shared_gc(size, target_gen);\n-        copy = allocate_memory(req, is_promotion);\n-        alloc_from_lab = false;\n-      }\n-      \/\/ else, we leave copy equal to nullptr, signaling a promotion failure below if appropriate.\n-      \/\/ We choose not to promote objects smaller than PLAB::min_size() by way of shared allocations, as this is too\n-      \/\/ costly.  Instead, we'll simply \"evacuate\" to young-gen memory (using a GCLAB) and will promote in a future\n-      \/\/ evacuation pass.  This condition is denoted by: is_promotion && has_plab && (size <= PLAB::min_size())\n-    }\n-#ifdef ASSERT\n-  }\n-#endif\n-\n-  if (copy == nullptr) {\n-    if (target_gen == OLD_GENERATION) {\n-      assert(mode()->is_generational(), \"Should only be here in generational mode.\");\n-      if (from_region->is_young()) {\n-        \/\/ Signal that promotion failed. Will evacuate this old object somewhere in young gen.\n-        report_promotion_failure(thread, size);\n-        return nullptr;\n-      } else {\n-        \/\/ Remember that evacuation to old gen failed. We'll want to trigger a full gc to recover from this\n-        \/\/ after the evacuation threads have finished.\n-        handle_old_evacuation_failure();\n-      }\n-    }\n-\n-    control_thread()->handle_alloc_failure_evac(size);\n-\n-    _oom_evac_handler.handle_out_of_memory_during_evacuation();\n-\n-    return ShenandoahBarrierSet::resolve_forwarded(p);\n-  }\n-\n-  \/\/ Copy the object:\n-  _evac_tracker->begin_evacuation(thread, size * HeapWordSize);\n-  Copy::aligned_disjoint_words(cast_from_oop<HeapWord*>(p), copy, size);\n-\n-  oop copy_val = cast_to_oop(copy);\n-\n-  if (mode()->is_generational() && target_gen == YOUNG_GENERATION && is_aging_cycle()) {\n-    ShenandoahHeap::increase_object_age(copy_val, from_region->age() + 1);\n-  }\n-\n-  \/\/ Try to install the new forwarding pointer.\n-  ContinuationGCSupport::relativize_stack_chunk(copy_val);\n-\n-  oop result = ShenandoahForwarding::try_update_forwardee(p, copy_val);\n-  if (result == copy_val) {\n-    \/\/ Successfully evacuated. Our copy is now the public one!\n-    _evac_tracker->end_evacuation(thread, size * HeapWordSize);\n-    if (mode()->is_generational()) {\n-      if (target_gen == OLD_GENERATION) {\n-        handle_old_evacuation(copy, size, from_region->is_young());\n-      } else {\n-        \/\/ When copying to the old generation above, we don't care\n-        \/\/ about recording object age in the census stats.\n-        assert(target_gen == YOUNG_GENERATION, \"Error\");\n-        \/\/ We record this census only when simulating pre-adaptive tenuring behavior, or\n-        \/\/ when we have been asked to record the census at evacuation rather than at mark\n-        if (ShenandoahGenerationalCensusAtEvac || !ShenandoahGenerationalAdaptiveTenuring) {\n-          _evac_tracker->record_age(thread, size * HeapWordSize, ShenandoahHeap::get_object_age(copy_val));\n-        }\n-      }\n-    }\n-    shenandoah_assert_correct(nullptr, copy_val);\n-    return copy_val;\n-  }  else {\n-    \/\/ Failed to evacuate. We need to deal with the object that is left behind. Since this\n-    \/\/ new allocation is certainly after TAMS, it will be considered live in the next cycle.\n-    \/\/ But if it happens to contain references to evacuated regions, those references would\n-    \/\/ not get updated for this stale copy during this cycle, and we will crash while scanning\n-    \/\/ it the next cycle.\n-    if (alloc_from_lab) {\n-       \/\/ For LAB allocations, it is enough to rollback the allocation ptr. Either the next\n-       \/\/ object will overwrite this stale copy, or the filler object on LAB retirement will\n-       \/\/ do this.\n-       switch (target_gen) {\n-         case YOUNG_GENERATION: {\n-             ShenandoahThreadLocalData::gclab(thread)->undo_allocation(copy, size);\n-            break;\n-         }\n-         case OLD_GENERATION: {\n-            ShenandoahThreadLocalData::plab(thread)->undo_allocation(copy, size);\n-            if (is_promotion) {\n-              ShenandoahThreadLocalData::subtract_from_plab_promoted(thread, size * HeapWordSize);\n-            } else {\n-              ShenandoahThreadLocalData::subtract_from_plab_evacuated(thread, size * HeapWordSize);\n-            }\n-            break;\n-         }\n-         default: {\n-           ShouldNotReachHere();\n-           break;\n-         }\n-       }\n-    } else {\n-      \/\/ For non-LAB allocations, we have no way to retract the allocation, and\n-      \/\/ have to explicitly overwrite the copy with the filler object. With that overwrite,\n-      \/\/ we have to keep the fwdptr initialized and pointing to our (stale) copy.\n-      assert(size >= ShenandoahHeap::min_fill_size(), \"previously allocated object known to be larger than min_size\");\n-      fill_with_object(copy, size);\n-      shenandoah_assert_correct(nullptr, copy_val);\n-      \/\/ For non-LAB allocations, the object has already been registered\n-    }\n-    shenandoah_assert_correct(nullptr, result);\n-    return result;\n-  }\n-}\n-\n@@ -588,4 +422,0 @@\n-inline bool ShenandoahHeap::clear_old_evacuation_failure() {\n-  return _old_gen_oom_evac.try_unset();\n-}\n-\n@@ -700,4 +530,0 @@\n-inline bool ShenandoahHeap::has_evacuation_reserve_quantities() const {\n-  return _has_evacuation_reserve_quantities;\n-}\n-\n@@ -756,54 +582,0 @@\n-inline size_t ShenandoahHeap::set_promoted_reserve(size_t new_val) {\n-  size_t orig = _promoted_reserve;\n-  _promoted_reserve = new_val;\n-  return orig;\n-}\n-\n-inline size_t ShenandoahHeap::get_promoted_reserve() const {\n-  return _promoted_reserve;\n-}\n-\n-inline size_t ShenandoahHeap::set_old_evac_reserve(size_t new_val) {\n-  size_t orig = _old_evac_reserve;\n-  _old_evac_reserve = new_val;\n-  return orig;\n-}\n-\n-inline size_t ShenandoahHeap::get_old_evac_reserve() const {\n-  return _old_evac_reserve;\n-}\n-\n-inline void ShenandoahHeap::augment_old_evac_reserve(size_t increment) {\n-  _old_evac_reserve += increment;\n-}\n-\n-inline void ShenandoahHeap::augment_promo_reserve(size_t increment) {\n-  _promoted_reserve += increment;\n-}\n-\n-inline void ShenandoahHeap::reset_promoted_expended() {\n-  Atomic::store(&_promoted_expended, (size_t) 0);\n-}\n-\n-inline size_t ShenandoahHeap::expend_promoted(size_t increment) {\n-  return Atomic::add(&_promoted_expended, increment);\n-}\n-\n-inline size_t ShenandoahHeap::unexpend_promoted(size_t decrement) {\n-  return Atomic::sub(&_promoted_expended, decrement);\n-}\n-\n-inline size_t ShenandoahHeap::get_promoted_expended() {\n-  return Atomic::load(&_promoted_expended);\n-}\n-\n-inline size_t ShenandoahHeap::set_young_evac_reserve(size_t new_val) {\n-  size_t orig = _young_evac_reserve;\n-  _young_evac_reserve = new_val;\n-  return orig;\n-}\n-\n-inline size_t ShenandoahHeap::get_young_evac_reserve() const {\n-  return _young_evac_reserve;\n-}\n-\n","filename":"src\/hotspot\/share\/gc\/shenandoah\/shenandoahHeap.inline.hpp","additions":13,"deletions":241,"binary":false,"changes":254,"status":"modified"},{"patch":"@@ -35,1 +35,0 @@\n-#include \"gc\/shenandoah\/shenandoahPacer.hpp\"\n","filename":"src\/hotspot\/share\/gc\/shenandoah\/shenandoahHeapRegion.hpp","additions":0,"deletions":1,"binary":false,"changes":1,"status":"modified"},{"patch":"@@ -160,2 +160,2 @@\n-    case GLOBAL_NON_GEN:\n-    case GLOBAL_GEN:\n+    case NON_GEN:\n+    case GLOBAL:\n","filename":"src\/hotspot\/share\/gc\/shenandoah\/shenandoahHeapRegionCounters.cpp","additions":2,"deletions":2,"binary":false,"changes":4,"status":"modified"},{"patch":"@@ -44,3 +44,3 @@\n-  log_info(gc, init)(\"Heap Region Size: \" PROPERFMT, PROPERFMTARGS(ShenandoahHeapRegion::region_size_bytes()));\n-  log_info(gc, init)(\"TLAB Size Max: \" PROPERFMT, PROPERFMTARGS(ShenandoahHeapRegion::max_tlab_size_bytes()));\n-  log_info(gc, init)(\"Humongous Object Threshold: \" PROPERFMT, PROPERFMTARGS(ShenandoahHeapRegion::humongous_threshold_bytes()));\n+  log_info(gc, init)(\"Heap Region Size: \" EXACTFMT, EXACTFMTARGS(ShenandoahHeapRegion::region_size_bytes()));\n+  log_info(gc, init)(\"TLAB Size Max: \" EXACTFMT, EXACTFMTARGS(ShenandoahHeapRegion::max_tlab_size_bytes()));\n+  log_info(gc, init)(\"Humongous Object Threshold: \" EXACTFMT, EXACTFMTARGS(ShenandoahHeapRegion::humongous_threshold_bytes()));\n","filename":"src\/hotspot\/share\/gc\/shenandoah\/shenandoahInitLogger.cpp","additions":3,"deletions":3,"binary":false,"changes":6,"status":"modified"},{"patch":"@@ -31,0 +31,1 @@\n+#include \"runtime\/interfaceSupport.inline.hpp\"\n@@ -34,0 +35,37 @@\n+\/\/ These are inline variants of Thread::SpinAcquire with optional blocking in VM.\n+\n+class ShenandoahNoBlockOp : public StackObj {\n+public:\n+  ShenandoahNoBlockOp(JavaThread* java_thread) {\n+    assert(java_thread == nullptr, \"Should not pass anything\");\n+  }\n+};\n+\n+void ShenandoahLock::contended_lock(bool allow_block_for_safepoint) {\n+  Thread* thread = Thread::current();\n+  if (allow_block_for_safepoint && thread->is_Java_thread()) {\n+    contended_lock_internal<ThreadBlockInVM>(JavaThread::cast(thread));\n+  } else {\n+    contended_lock_internal<ShenandoahNoBlockOp>(nullptr);\n+  }\n+}\n+\n+template<typename BlockOp>\n+void ShenandoahLock::contended_lock_internal(JavaThread* java_thread) {\n+  int ctr = 0;\n+  int yields = 0;\n+  while (Atomic::cmpxchg(&_state, unlocked, locked) != unlocked) {\n+    if ((++ctr & 0xFFF) == 0) {\n+      BlockOp block(java_thread);\n+      if (yields > 5) {\n+        os::naked_short_sleep(1);\n+      } else {\n+        os::naked_yield();\n+        yields++;\n+      }\n+    } else {\n+      SpinPause();\n+    }\n+  }\n+}\n+\n","filename":"src\/hotspot\/share\/gc\/shenandoah\/shenandoahLock.cpp","additions":38,"deletions":0,"binary":false,"changes":38,"status":"modified"},{"patch":"@@ -38,1 +38,1 @@\n-  volatile int _state;\n+  volatile LockState _state;\n@@ -43,0 +43,3 @@\n+  template<typename BlockOp>\n+  void contended_lock_internal(JavaThread* java_thread);\n+\n@@ -46,10 +49,11 @@\n-  void lock() {\n-#ifdef ASSERT\n-    assert(_owner != Thread::current(), \"reentrant locking attempt, would deadlock\");\n-#endif\n-    Thread::SpinAcquire(&_state, \"Shenandoah Heap Lock\");\n-#ifdef ASSERT\n-    assert(_state == locked, \"must be locked\");\n-    assert(_owner == nullptr, \"must not be owned\");\n-    _owner = Thread::current();\n-#endif\n+  void lock(bool allow_block_for_safepoint) {\n+    assert(Atomic::load(&_owner) != Thread::current(), \"reentrant locking attempt, would deadlock\");\n+\n+    \/\/ Try to lock fast, or dive into contended lock handling.\n+    if (Atomic::cmpxchg(&_state, unlocked, locked) != unlocked) {\n+      contended_lock(allow_block_for_safepoint);\n+    }\n+\n+    assert(Atomic::load(&_state) == locked, \"must be locked\");\n+    assert(Atomic::load(&_owner) == nullptr, \"must not be owned\");\n+    DEBUG_ONLY(Atomic::store(&_owner, Thread::current());)\n@@ -59,5 +63,4 @@\n-#ifdef ASSERT\n-    assert (_owner == Thread::current(), \"sanity\");\n-    _owner = nullptr;\n-#endif\n-    Thread::SpinRelease(&_state);\n+    assert(Atomic::load(&_owner) == Thread::current(), \"sanity\");\n+    DEBUG_ONLY(Atomic::store(&_owner, (Thread*)nullptr);)\n+    OrderAccess::fence();\n+    Atomic::store(&_state, unlocked);\n@@ -66,0 +69,2 @@\n+  void contended_lock(bool allow_block_for_safepoint);\n+\n@@ -80,1 +85,1 @@\n-  ShenandoahLocker(ShenandoahLock* lock) : _lock(lock) {\n+  ShenandoahLocker(ShenandoahLock* lock, bool allow_block_for_safepoint = false) : _lock(lock) {\n@@ -82,1 +87,1 @@\n-      _lock->lock();\n+      _lock->lock(allow_block_for_safepoint);\n","filename":"src\/hotspot\/share\/gc\/shenandoah\/shenandoahLock.hpp","additions":23,"deletions":18,"binary":false,"changes":41,"status":"modified"},{"patch":"@@ -91,1 +91,2 @@\n-void ShenandoahMark::mark_loop(ShenandoahGenerationType generation, uint worker_id, TaskTerminator* terminator, ShenandoahReferenceProcessor *rp, StringDedup::Requests* const req) {\n+void ShenandoahMark::mark_loop(uint worker_id, TaskTerminator* terminator, ShenandoahReferenceProcessor *rp,\n+                               ShenandoahGenerationType generation, StringDedup::Requests* const req) {\n@@ -101,2 +102,2 @@\n-    case GLOBAL_GEN:\n-      mark_loop_prework<GLOBAL_GEN, CANCELLABLE, STRING_DEDUP>(worker_id, terminator, rp, req, update_refs);\n+    case GLOBAL:\n+      mark_loop_prework<GLOBAL, CANCELLABLE, STRING_DEDUP>(worker_id, terminator, rp, req, update_refs);\n@@ -104,2 +105,2 @@\n-    case GLOBAL_NON_GEN:\n-      mark_loop_prework<GLOBAL_NON_GEN, CANCELLABLE, STRING_DEDUP>(worker_id, terminator, rp, req, update_refs);\n+    case NON_GEN:\n+      mark_loop_prework<NON_GEN, CANCELLABLE, STRING_DEDUP>(worker_id, terminator, rp, req, update_refs);\n@@ -113,2 +114,2 @@\n-void ShenandoahMark::mark_loop(ShenandoahGenerationType generation, uint worker_id, TaskTerminator* terminator, ShenandoahReferenceProcessor *rp,\n-                               bool cancellable, StringDedupMode dedup_mode, StringDedup::Requests* const req) {\n+void ShenandoahMark::mark_loop(uint worker_id, TaskTerminator* terminator, ShenandoahReferenceProcessor *rp,\n+                               ShenandoahGenerationType generation, bool cancellable, StringDedupMode dedup_mode, StringDedup::Requests* const req) {\n@@ -118,1 +119,1 @@\n-        mark_loop<true, NO_DEDUP>(generation, worker_id, terminator, rp, req);\n+        mark_loop<true, NO_DEDUP>(worker_id, terminator, rp, generation, req);\n@@ -121,1 +122,1 @@\n-        mark_loop<true, ENQUEUE_DEDUP>(generation, worker_id, terminator, rp, req);\n+        mark_loop<true, ENQUEUE_DEDUP>(worker_id, terminator, rp, generation, req);\n@@ -124,1 +125,1 @@\n-        mark_loop<true, ALWAYS_DEDUP>(generation, worker_id, terminator, rp, req);\n+        mark_loop<true, ALWAYS_DEDUP>(worker_id, terminator, rp, generation, req);\n@@ -130,1 +131,1 @@\n-        mark_loop<false, NO_DEDUP>(generation, worker_id, terminator, rp, req);\n+        mark_loop<false, NO_DEDUP>(worker_id, terminator, rp, generation, req);\n@@ -133,1 +134,1 @@\n-        mark_loop<false, ENQUEUE_DEDUP>(generation, worker_id, terminator, rp, req);\n+        mark_loop<false, ENQUEUE_DEDUP>(worker_id, terminator, rp, generation, req);\n@@ -136,1 +137,1 @@\n-        mark_loop<false, ALWAYS_DEDUP>(generation, worker_id, terminator, rp, req);\n+        mark_loop<false, ALWAYS_DEDUP>(worker_id, terminator, rp, generation, req);\n","filename":"src\/hotspot\/share\/gc\/shenandoah\/shenandoahMark.cpp","additions":14,"deletions":13,"binary":false,"changes":27,"status":"modified"},{"patch":"@@ -32,1 +32,3 @@\n-#include \"gc\/shenandoah\/shenandoahOopClosures.hpp\"\n+#include \"gc\/shenandoah\/shenandoahGenerationType.hpp\"\n+#include \"gc\/shenandoah\/shenandoahHeap.hpp\"\n+#include \"gc\/shenandoah\/shenandoahGeneration.hpp\"\n@@ -35,0 +37,9 @@\n+enum StringDedupMode {\n+  NO_DEDUP,      \/\/ Do not do anything for String deduplication\n+  ENQUEUE_DEDUP, \/\/ Enqueue candidate Strings for deduplication, if meet age threshold\n+  ALWAYS_DEDUP   \/\/ Enqueue Strings for deduplication\n+};\n+\n+class ShenandoahMarkingContext;\n+class ShenandoahReferenceProcessor;\n+\n@@ -98,2 +109,2 @@\n-  void mark_loop(ShenandoahGenerationType generation, uint worker_id, TaskTerminator* terminator, ShenandoahReferenceProcessor *rp,\n-                 StringDedup::Requests* const req);\n+  void mark_loop(uint worker_id, TaskTerminator* terminator, ShenandoahReferenceProcessor *rp,\n+                 ShenandoahGenerationType generation, StringDedup::Requests* const req);\n@@ -101,2 +112,2 @@\n-  void mark_loop(ShenandoahGenerationType generation, uint worker_id, TaskTerminator* terminator, ShenandoahReferenceProcessor *rp,\n-                 bool cancellable, StringDedupMode dedup_mode, StringDedup::Requests* const req);\n+  void mark_loop(uint worker_id, TaskTerminator* terminator, ShenandoahReferenceProcessor *rp,\n+                 ShenandoahGenerationType generation, bool cancellable, StringDedupMode dedup_mode, StringDedup::Requests* const req);\n","filename":"src\/hotspot\/share\/gc\/shenandoah\/shenandoahMark.hpp","additions":16,"deletions":5,"binary":false,"changes":21,"status":"modified"},{"patch":"@@ -36,0 +36,1 @@\n+#include \"gc\/shenandoah\/shenandoahOopClosures.inline.hpp\"\n@@ -118,1 +119,1 @@\n-  if (GENERATION == YOUNG || (GENERATION == GLOBAL_GEN && region->is_young())) {\n+  if (GENERATION == YOUNG || (GENERATION == GLOBAL && region->is_young())) {\n@@ -284,1 +285,1 @@\n-  } else if (GENERATION == GLOBAL_GEN || GENERATION == GLOBAL_NON_GEN) {\n+  } else if (GENERATION == GLOBAL || GENERATION == NON_GEN) {\n@@ -306,1 +307,1 @@\n-      \/\/ TODO: As implemented herein, GLOBAL_GEN collections reconstruct the card table during GLOBAL_GEN concurrent\n+      \/\/ TODO: As implemented herein, GLOBAL collections reconstruct the card table during GLOBAL concurrent\n@@ -314,1 +315,1 @@\n-      } else if (GENERATION == GLOBAL_GEN && heap->is_in_old(p) && heap->is_in_young(obj)) {\n+      } else if (GENERATION == GLOBAL && heap->is_in_old(p) && heap->is_in_young(obj)) {\n","filename":"src\/hotspot\/share\/gc\/shenandoah\/shenandoahMark.inline.hpp","additions":5,"deletions":4,"binary":false,"changes":9,"status":"modified"},{"patch":"@@ -40,1 +40,1 @@\n-  virtual void update_all() {\n+  void update_all() override {\n@@ -49,1 +49,1 @@\n-  ShenandoahGenerationCounters(ShenandoahHeap* heap) :\n+  explicit ShenandoahGenerationCounters(ShenandoahHeap* heap) :\n@@ -54,1 +54,1 @@\n-  virtual void update_all() {\n+  void update_all() override {\n@@ -61,1 +61,2 @@\n-        _full_counters(nullptr)\n+        _full_counters(nullptr),\n+        _counters_update_task(this)\n@@ -74,0 +75,2 @@\n+\n+  _counters_update_task.enroll();\n@@ -106,0 +109,41 @@\n+\n+void ShenandoahMonitoringSupport::notify_heap_changed() {\n+  _counters_update_task.notify_heap_changed();\n+}\n+\n+void ShenandoahMonitoringSupport::set_forced_counters_update(bool value) {\n+  _counters_update_task.set_forced_counters_update(value);\n+}\n+\n+void ShenandoahMonitoringSupport::handle_force_counters_update() {\n+  _counters_update_task.handle_force_counters_update();\n+}\n+\n+void ShenandoahPeriodicCountersUpdateTask::task() {\n+  handle_force_counters_update();\n+  handle_counters_update();\n+}\n+\n+void ShenandoahPeriodicCountersUpdateTask::handle_counters_update() {\n+  if (_do_counters_update.is_set()) {\n+    _do_counters_update.unset();\n+    _monitoring_support->update_counters();\n+  }\n+}\n+\n+void ShenandoahPeriodicCountersUpdateTask::handle_force_counters_update() {\n+  if (_force_counters_update.is_set()) {\n+    _do_counters_update.unset(); \/\/ reset these too, we do update now!\n+    _monitoring_support->update_counters();\n+  }\n+}\n+\n+void ShenandoahPeriodicCountersUpdateTask::notify_heap_changed() {\n+  if (_do_counters_update.is_unset()) {\n+    _do_counters_update.set();\n+  }\n+}\n+\n+void ShenandoahPeriodicCountersUpdateTask::set_forced_counters_update(bool value) {\n+  _force_counters_update.set_cond(value);\n+}\n","filename":"src\/hotspot\/share\/gc\/shenandoah\/shenandoahMonitoringSupport.cpp","additions":48,"deletions":4,"binary":false,"changes":52,"status":"modified"},{"patch":"@@ -28,0 +28,1 @@\n+#include \"gc\/shenandoah\/shenandoahSharedVariables.hpp\"\n@@ -29,0 +30,1 @@\n+#include \"runtime\/task.hpp\"\n@@ -35,0 +37,20 @@\n+class ShenandoahMonitoringSupport;\n+\n+class ShenandoahPeriodicCountersUpdateTask : public PeriodicTask {\n+private:\n+  ShenandoahSharedFlag _do_counters_update;\n+  ShenandoahSharedFlag _force_counters_update;\n+  ShenandoahMonitoringSupport* const _monitoring_support;\n+\n+public:\n+  explicit ShenandoahPeriodicCountersUpdateTask(ShenandoahMonitoringSupport* monitoring_support) :\n+    PeriodicTask(100),\n+    _monitoring_support(monitoring_support) { }\n+\n+  void task() override;\n+\n+  void handle_counters_update();\n+  void handle_force_counters_update();\n+  void set_forced_counters_update(bool value);\n+  void notify_heap_changed();\n+};\n@@ -47,0 +69,1 @@\n+  ShenandoahPeriodicCountersUpdateTask _counters_update_task;\n@@ -49,6 +72,11 @@\n- ShenandoahMonitoringSupport(ShenandoahHeap* heap);\n- CollectorCounters* stw_collection_counters();\n- CollectorCounters* full_stw_collection_counters();\n- CollectorCounters* concurrent_collection_counters();\n- CollectorCounters* partial_collection_counters();\n- void update_counters();\n+  explicit ShenandoahMonitoringSupport(ShenandoahHeap* heap);\n+  CollectorCounters* stw_collection_counters();\n+  CollectorCounters* full_stw_collection_counters();\n+  CollectorCounters* concurrent_collection_counters();\n+  CollectorCounters* partial_collection_counters();\n+\n+  void notify_heap_changed();\n+  void set_forced_counters_update(bool value);\n+  void handle_force_counters_update();\n+\n+  void update_counters();\n","filename":"src\/hotspot\/share\/gc\/shenandoah\/shenandoahMonitoringSupport.hpp","additions":34,"deletions":6,"binary":false,"changes":40,"status":"modified"},{"patch":"@@ -29,0 +29,1 @@\n+#include \"gc\/shenandoah\/shenandoahGenerationalHeap.hpp\"\n@@ -40,2 +41,0 @@\n-\n-\n@@ -88,1 +87,1 @@\n-  ShenandoahHeap* heap = ShenandoahHeap::heap();\n+  auto heap = ShenandoahGenerationalHeap::heap();\n@@ -151,1 +150,1 @@\n-  \/\/ We do not rebuild_free following increments of old marking because memory has not been reclaimed..  However, we may\n+  \/\/ We do not rebuild_free following increments of old marking because memory has not been reclaimed. However, we may\n@@ -154,1 +153,1 @@\n-  heap->adjust_generation_sizes_for_next_cycle(allocation_runway, 0, 0);\n+  heap->compute_old_generation_balance(allocation_runway, 0);\n@@ -156,5 +155,1 @@\n-  bool success;\n-  size_t region_xfer;\n-  const char* region_destination;\n-  ShenandoahYoungGeneration* young_gen = heap->young_generation();\n-  ShenandoahGeneration* old_gen = heap->old_generation();\n+  ShenandoahGenerationalHeap::TransferResult result;\n@@ -163,21 +158,1 @@\n-\n-    size_t old_region_surplus = heap->get_old_region_surplus();\n-    size_t old_region_deficit = heap->get_old_region_deficit();\n-    if (old_region_surplus) {\n-      success = heap->generation_sizer()->transfer_to_young(old_region_surplus);\n-      region_destination = \"young\";\n-      region_xfer = old_region_surplus;\n-    } else if (old_region_deficit) {\n-      success = heap->generation_sizer()->transfer_to_old(old_region_deficit);\n-      region_destination = \"old\";\n-      region_xfer = old_region_deficit;\n-      if (!success) {\n-        ((ShenandoahOldHeuristics *) old_gen->heuristics())->trigger_cannot_expand();\n-      }\n-    } else {\n-      region_destination = \"none\";\n-      region_xfer = 0;\n-      success = true;\n-    }\n-    heap->set_old_region_surplus(0);\n-    heap->set_old_region_deficit(0);\n+    result = heap->balance_generations();\n@@ -186,8 +161,5 @@\n-  \/\/ Report outside the heap lock\n-  size_t young_available = young_gen->available();\n-  size_t old_available = old_gen->available();\n-  log_info(gc, ergo)(\"After old marking finished, %s \" SIZE_FORMAT \" regions to %s to prepare for next gc, old available: \"\n-                     SIZE_FORMAT \"%s, young_available: \" SIZE_FORMAT \"%s\",\n-                     success? \"successfully transferred\": \"failed to transfer\", region_xfer, region_destination,\n-                     byte_size_in_proper_unit(old_available), proper_unit_for_byte_size(old_available),\n-                     byte_size_in_proper_unit(young_available), proper_unit_for_byte_size(young_available));\n+  LogTarget(Info, gc, ergo) lt;\n+  if (lt.is_enabled()) {\n+    LogStream ls(lt);\n+    result.print_on(\"Old Mark\", &ls);\n+  }\n","filename":"src\/hotspot\/share\/gc\/shenandoah\/shenandoahOldGC.cpp","additions":11,"deletions":39,"binary":false,"changes":50,"status":"modified"},{"patch":"@@ -30,3 +30,0 @@\n-#include \"gc\/shenandoah\/heuristics\/shenandoahAdaptiveHeuristics.hpp\"\n-#include \"gc\/shenandoah\/heuristics\/shenandoahAggressiveHeuristics.hpp\"\n-#include \"gc\/shenandoah\/heuristics\/shenandoahCompactHeuristics.hpp\"\n@@ -34,1 +31,0 @@\n-#include \"gc\/shenandoah\/heuristics\/shenandoahStaticHeuristics.hpp\"\n@@ -37,0 +33,1 @@\n+#include \"gc\/shenandoah\/shenandoahGenerationalHeap.hpp\"\n@@ -41,1 +38,0 @@\n-#include \"gc\/shenandoah\/shenandoahMark.inline.hpp\"\n@@ -46,1 +42,0 @@\n-#include \"gc\/shenandoah\/shenandoahStringDedup.hpp\"\n@@ -50,1 +45,0 @@\n-#include \"prims\/jvmtiTagMap.hpp\"\n@@ -177,0 +171,9 @@\n+    _old_heuristics(nullptr),\n+    _region_surplus(0),\n+    _region_deficit(0),\n+    _promoted_reserve(0),\n+    _promoted_expended(0),\n+    _promotion_potential(0),\n+    _pad_for_promote_in_place(0),\n+    _promotable_humongous_regions(0),\n+    _promotable_regular_regions(0),\n@@ -186,0 +189,33 @@\n+void ShenandoahOldGeneration::set_promoted_reserve(size_t new_val) {\n+  shenandoah_assert_heaplocked_or_safepoint();\n+  _promoted_reserve = new_val;\n+}\n+\n+size_t ShenandoahOldGeneration::get_promoted_reserve() const {\n+  return _promoted_reserve;\n+}\n+\n+void ShenandoahOldGeneration::augment_promoted_reserve(size_t increment) {\n+  shenandoah_assert_heaplocked_or_safepoint();\n+  _promoted_reserve += increment;\n+}\n+\n+void ShenandoahOldGeneration::reset_promoted_expended() {\n+  shenandoah_assert_heaplocked_or_safepoint();\n+  Atomic::store(&_promoted_expended, (size_t) 0);\n+}\n+\n+size_t ShenandoahOldGeneration::expend_promoted(size_t increment) {\n+  shenandoah_assert_heaplocked_or_safepoint();\n+  assert(get_promoted_expended() + increment <= get_promoted_reserve(), \"Do not expend more promotion than budgeted\");\n+  return Atomic::add(&_promoted_expended, increment);\n+}\n+\n+size_t ShenandoahOldGeneration::unexpend_promoted(size_t decrement) {\n+  return Atomic::sub(&_promoted_expended, decrement);\n+}\n+\n+size_t ShenandoahOldGeneration::get_promoted_expended() {\n+  return Atomic::load(&_promoted_expended);\n+}\n+\n@@ -198,0 +234,4 @@\n+void ShenandoahOldGeneration::handle_failed_transfer() {\n+  _old_heuristics->trigger_cannot_expand();\n+}\n+\n@@ -469,0 +509,64 @@\n+\n+void ShenandoahOldGeneration::handle_failed_evacuation() {\n+  if (_failed_evacuation.try_set()) {\n+    log_info(gc)(\"Old gen evac failure.\");\n+  }\n+}\n+\n+void ShenandoahOldGeneration::handle_failed_promotion(Thread* thread, size_t size) {\n+  \/\/ We squelch excessive reports to reduce noise in logs.\n+  const size_t MaxReportsPerEpoch = 4;\n+  static size_t last_report_epoch = 0;\n+  static size_t epoch_report_count = 0;\n+  auto heap = ShenandoahGenerationalHeap::heap();\n+\n+  size_t promotion_reserve;\n+  size_t promotion_expended;\n+\n+  const size_t gc_id = heap->control_thread()->get_gc_id();\n+\n+  if ((gc_id != last_report_epoch) || (epoch_report_count++ < MaxReportsPerEpoch)) {\n+    {\n+      \/\/ Promotion failures should be very rare.  Invest in providing useful diagnostic info.\n+      ShenandoahHeapLocker locker(heap->lock());\n+      promotion_reserve = get_promoted_reserve();\n+      promotion_expended = get_promoted_expended();\n+    }\n+    PLAB* const plab = ShenandoahThreadLocalData::plab(thread);\n+    const size_t words_remaining = (plab == nullptr)? 0: plab->words_remaining();\n+    const char* promote_enabled = ShenandoahThreadLocalData::allow_plab_promotions(thread)? \"enabled\": \"disabled\";\n+\n+    log_info(gc, ergo)(\"Promotion failed, size \" SIZE_FORMAT \", has plab? %s, PLAB remaining: \" SIZE_FORMAT\n+                       \", plab promotions %s, promotion reserve: \" SIZE_FORMAT \", promotion expended: \" SIZE_FORMAT\n+                       \", old capacity: \" SIZE_FORMAT \", old_used: \" SIZE_FORMAT \", old unaffiliated regions: \" SIZE_FORMAT,\n+                       size * HeapWordSize, plab == nullptr? \"no\": \"yes\",\n+                       words_remaining * HeapWordSize, promote_enabled, promotion_reserve, promotion_expended,\n+                       max_capacity(), used(), free_unaffiliated_regions());\n+\n+    if ((gc_id == last_report_epoch) && (epoch_report_count >= MaxReportsPerEpoch)) {\n+      log_info(gc, ergo)(\"Squelching additional promotion failure reports for current epoch\");\n+    } else if (gc_id != last_report_epoch) {\n+      last_report_epoch = gc_id;\n+      epoch_report_count = 1;\n+    }\n+  }\n+}\n+\n+void ShenandoahOldGeneration::handle_evacuation(HeapWord* obj, size_t words, bool promotion) {\n+  auto heap = ShenandoahGenerationalHeap::heap();\n+  auto card_scan = heap->card_scan();\n+\n+  \/\/ Only register the copy of the object that won the evacuation race.\n+  card_scan->register_object_without_lock(obj);\n+\n+  \/\/ Mark the entire range of the evacuated object as dirty.  At next remembered set scan,\n+  \/\/ we will clear dirty bits that do not hold interesting pointers.  It's more efficient to\n+  \/\/ do this in batch, in a background GC thread than to try to carefully dirty only cards\n+  \/\/ that hold interesting pointers right now.\n+  card_scan->mark_range_as_dirty(obj, words);\n+\n+  if (promotion) {\n+    \/\/ This evacuation was a promotion, track this as allocation against old gen\n+    increase_allocated(words * HeapWordSize);\n+  }\n+}\n\\ No newline at end of file\n","filename":"src\/hotspot\/share\/gc\/shenandoah\/shenandoahOldGeneration.cpp","additions":111,"deletions":7,"binary":false,"changes":118,"status":"modified"},{"patch":"@@ -29,0 +29,1 @@\n+#include \"gc\/shenandoah\/shenandoahSharedVariables.hpp\"\n@@ -39,0 +40,39 @@\n+  \/\/ After determining the desired size of the old generation (see compute_old_generation_balance), these\n+  \/\/ quantities represent the number of regions above (surplus) or below (deficit) that size.\n+  \/\/ These values are computed prior to the actual exchange of any regions. These may never both\n+  \/\/ be positive simultaneously.\n+  size_t _region_surplus;\n+  size_t _region_deficit;\n+\n+  \/\/ Set when evacuation in the old generation fails. When this is set, the control thread will initiate a\n+  \/\/ full GC instead of a futile degenerated cycle.\n+  ShenandoahSharedFlag _failed_evacuation;\n+\n+  \/\/ Bytes reserved within old-gen to hold the results of promotion. This is separate from\n+  \/\/ and in addition to the evacuation reserve for intra-generation evacuations (ShenandoahGeneration::_evacuation_reserve).\n+  size_t _promoted_reserve;\n+\n+  \/\/ Bytes of old-gen memory expended on promotions. This may be modified concurrently\n+  \/\/ by mutators and gc workers when promotion LABs are retired during evacuation. It\n+  \/\/ is therefore always accessed through atomic operations. This is increased when a\n+  \/\/ PLAB is allocated for promotions. The value is decreased by the amount of memory\n+  \/\/ remaining in a PLAB when it is retired.\n+  size_t _promoted_expended;\n+\n+  \/\/ Represents the quantity of live bytes we expect to promote in place during the next\n+  \/\/ evacuation cycle. This value is used by the young heuristic to trigger mixed collections.\n+  \/\/ It is also used when computing the optimum size for the old generation.\n+  size_t _promotion_potential;\n+\n+  \/\/ When a region is selected to be promoted in place, the remaining free memory is filled\n+  \/\/ in to prevent additional allocations (preventing premature promotion of newly allocated\n+  \/\/ objects. This field records the total amount of padding used for such regions.\n+  size_t _pad_for_promote_in_place;\n+\n+  \/\/ During construction of the collection set, we keep track of regions that are eligible\n+  \/\/ for promotion in place. These fields track the count of those humongous and regular regions.\n+  \/\/ This data is used to force the evacuation phase even when the collection set is otherwise\n+  \/\/ empty.\n+  size_t _promotable_humongous_regions;\n+  size_t _promotable_regular_regions;\n+\n@@ -50,0 +90,55 @@\n+  \/\/ See description in field declaration\n+  void set_promoted_reserve(size_t new_val);\n+  size_t get_promoted_reserve() const;\n+\n+  \/\/ The promotion reserve is increased when rebuilding the free set transfers a region to the old generation\n+  void augment_promoted_reserve(size_t increment);\n+\n+  \/\/ This zeros out the expended promotion count after the promotion reserve is computed\n+  void reset_promoted_expended();\n+\n+  \/\/ This is incremented when allocations are made to copy promotions into the old generation\n+  size_t expend_promoted(size_t increment);\n+\n+  \/\/ This is used to return unused memory from a retired promotion LAB\n+  size_t unexpend_promoted(size_t decrement);\n+\n+  \/\/ This is used on the allocation path to gate promotions that would exceed the reserve\n+  size_t get_promoted_expended();\n+\n+  \/\/ See description in field declaration\n+  void set_region_surplus(size_t surplus) { _region_surplus = surplus; };\n+  void set_region_deficit(size_t deficit) { _region_deficit = deficit; };\n+  size_t get_region_surplus() const { return _region_surplus; };\n+  size_t get_region_deficit() const { return _region_deficit; };\n+\n+  \/\/ See description in field declaration\n+  void set_promotion_potential(size_t val) { _promotion_potential = val; };\n+  size_t get_promotion_potential() const { return _promotion_potential; };\n+\n+  \/\/ See description in field declaration\n+  void set_pad_for_promote_in_place(size_t pad) { _pad_for_promote_in_place = pad; }\n+  size_t get_pad_for_promote_in_place() const { return _pad_for_promote_in_place; }\n+\n+  \/\/ See description in field declaration\n+  void set_expected_humongous_region_promotions(size_t region_count) { _promotable_humongous_regions = region_count; }\n+  void set_expected_regular_region_promotions(size_t region_count) { _promotable_regular_regions = region_count; }\n+  bool has_in_place_promotions() const { return (_promotable_humongous_regions + _promotable_regular_regions) > 0; }\n+\n+  \/\/ This will signal the heuristic to trigger an old generation collection\n+  void handle_failed_transfer();\n+\n+  \/\/ This will signal the control thread to run a full GC instead of a futile degenerated gc\n+  void handle_failed_evacuation();\n+\n+  \/\/ This logs that an evacuation to the old generation has failed\n+  void handle_failed_promotion(Thread* thread, size_t size);\n+\n+  \/\/ A successful evacuation re-dirties the cards and registers the object with the remembered set\n+  void handle_evacuation(HeapWord* obj, size_t words, bool promotion);\n+\n+  \/\/ Clear the flag after it is consumed by the control thread\n+  bool clear_failed_evacuation() {\n+    return _failed_evacuation.try_unset();\n+  }\n+\n","filename":"src\/hotspot\/share\/gc\/shenandoah\/shenandoahOldGeneration.hpp","additions":95,"deletions":0,"binary":false,"changes":95,"status":"modified"},{"patch":"@@ -31,0 +31,1 @@\n+#include \"gc\/shenandoah\/shenandoahGenerationType.hpp\"\n@@ -37,6 +38,0 @@\n-enum StringDedupMode {\n-  NO_DEDUP,      \/\/ Do not do anything for String deduplication\n-  ENQUEUE_DEDUP, \/\/ Enqueue candidate Strings for deduplication, if meet age threshold\n-  ALWAYS_DEDUP   \/\/ Enqueue Strings for deduplication\n-};\n-\n@@ -148,17 +143,0 @@\n-class ShenandoahSetRememberedCardsToDirtyClosure : public BasicOopIterateClosure {\n-protected:\n-  ShenandoahHeap*    const _heap;\n-  RememberedScanner* const _scanner;\n-\n-public:\n-  ShenandoahSetRememberedCardsToDirtyClosure() :\n-      _heap(ShenandoahHeap::heap()),\n-      _scanner(_heap->card_scan()) {}\n-\n-  template<class T>\n-  inline void work(T* p);\n-\n-  virtual void do_oop(narrowOop* p) { work(p); }\n-  virtual void do_oop(oop* p)       { work(p); }\n-};\n-\n","filename":"src\/hotspot\/share\/gc\/shenandoah\/shenandoahOopClosures.hpp","additions":1,"deletions":23,"binary":false,"changes":24,"status":"modified"},{"patch":"@@ -58,12 +58,0 @@\n-template<class T>\n-inline void ShenandoahSetRememberedCardsToDirtyClosure::work(T* p) {\n-  T o = RawAccess<>::oop_load(p);\n-  if (!CompressedOops::is_null(o)) {\n-    oop obj = CompressedOops::decode_not_null(o);\n-    if (_heap->is_in_young(obj)) {\n-      \/\/ Found interesting pointer.  Mark the containing card as dirty.\n-      _scanner->mark_card_as_dirty((HeapWord*) p);\n-    }\n-  }\n-}\n-\n","filename":"src\/hotspot\/share\/gc\/shenandoah\/shenandoahOopClosures.inline.hpp","additions":0,"deletions":12,"binary":false,"changes":12,"status":"modified"},{"patch":"@@ -341,0 +341,5 @@\n+\n+void ShenandoahPeriodicPacerNotifyTask::task() {\n+  assert(ShenandoahPacing, \"Should not be here otherwise\");\n+  _pacer->notify_waiters();\n+}\n","filename":"src\/hotspot\/share\/gc\/shenandoah\/shenandoahPacer.cpp","additions":5,"deletions":0,"binary":false,"changes":5,"status":"modified"},{"patch":"@@ -32,0 +32,1 @@\n+#include \"runtime\/task.hpp\"\n@@ -34,0 +35,15 @@\n+class ShenandoahPacer;\n+\n+\n+\/\/ Periodic task to notify blocked paced waiters.\n+class ShenandoahPeriodicPacerNotifyTask : public PeriodicTask {\n+private:\n+  ShenandoahPacer* const _pacer;\n+public:\n+  explicit ShenandoahPeriodicPacerNotifyTask(ShenandoahPacer* pacer) :\n+    PeriodicTask(PeriodicTask::min_interval),\n+    _pacer(pacer) { }\n+\n+  void task() override;\n+};\n+\n@@ -51,0 +67,1 @@\n+  ShenandoahPeriodicPacerNotifyTask _notify_waiters_task;\n@@ -67,1 +84,1 @@\n-  ShenandoahPacer(ShenandoahHeap* heap) :\n+  explicit ShenandoahPacer(ShenandoahHeap* heap) :\n@@ -72,0 +89,1 @@\n+          _notify_waiters_task(this),\n@@ -75,1 +93,3 @@\n-          _progress(PACING_PROGRESS_UNINIT) {}\n+          _progress(PACING_PROGRESS_UNINIT) {\n+    _notify_waiters_task.enroll();\n+  }\n","filename":"src\/hotspot\/share\/gc\/shenandoah\/shenandoahPacer.hpp","additions":22,"deletions":2,"binary":false,"changes":24,"status":"modified"},{"patch":"@@ -28,1 +28,2 @@\n-#include \"gc\/shenandoah\/shenandoahControlThread.hpp\"\n+#include \"gc\/shenandoah\/shenandoahAsserts.hpp\"\n+#include \"gc\/shenandoah\/shenandoahGenerationalControlThread.hpp\"\n@@ -35,5 +36,1 @@\n-static ShenandoahHeuristics* get_heuristics(ShenandoahGeneration* nullable) {\n-  return nullable != nullptr ? nullable->heuristics() : nullptr;\n-}\n-\n-ShenandoahRegulatorThread::ShenandoahRegulatorThread(ShenandoahControlThread* control_thread) :\n+ShenandoahRegulatorThread::ShenandoahRegulatorThread(ShenandoahGenerationalControlThread* control_thread) :\n@@ -44,1 +41,1 @@\n-\n+  shenandoah_assert_generational();\n@@ -46,3 +43,3 @@\n-  _old_heuristics = get_heuristics(heap->old_generation());\n-  _young_heuristics = get_heuristics(heap->young_generation());\n-  _global_heuristics = get_heuristics(heap->global_generation());\n+  _old_heuristics = heap->old_generation()->heuristics();\n+  _young_heuristics = heap->young_generation()->heuristics();\n+  _global_heuristics = heap->global_generation()->heuristics();\n@@ -50,0 +47,1 @@\n+  set_name(\"Shenandoah Regulator Thread\");\n@@ -54,6 +52,2 @@\n-  if (ShenandoahHeap::heap()->mode()->is_generational()) {\n-    if (ShenandoahAllowOldMarkingPreemption) {\n-      regulate_young_and_old_cycles();\n-    } else {\n-      regulate_young_and_global_cycles();\n-    }\n+  if (ShenandoahAllowOldMarkingPreemption) {\n+    regulate_young_and_old_cycles();\n@@ -61,1 +55,1 @@\n-    regulate_global_cycles();\n+    regulate_young_and_global_cycles();\n@@ -68,3 +62,0 @@\n-  assert(_young_heuristics != nullptr, \"Need young heuristics.\");\n-  assert(_old_heuristics != nullptr, \"Need old heuristics.\");\n-\n@@ -72,2 +63,2 @@\n-    ShenandoahControlThread::GCMode mode = _control_thread->gc_mode();\n-    if (mode == ShenandoahControlThread::none) {\n+    ShenandoahGenerationalControlThread::GCMode mode = _control_thread->gc_mode();\n+    if (mode == ShenandoahGenerationalControlThread::none) {\n@@ -75,1 +66,1 @@\n-        if (request_concurrent_gc(ShenandoahControlThread::select_global_generation())) {\n+        if (request_concurrent_gc(GLOBAL)) {\n@@ -89,1 +80,1 @@\n-    } else if (mode == ShenandoahControlThread::servicing_old) {\n+    } else if (mode == ShenandoahGenerationalControlThread::servicing_old) {\n@@ -101,3 +92,0 @@\n-  assert(_young_heuristics != nullptr, \"Need young heuristics.\");\n-  assert(_global_heuristics != nullptr, \"Need global heuristics.\");\n-\n@@ -105,1 +93,1 @@\n-    if (_control_thread->gc_mode() == ShenandoahControlThread::none) {\n+    if (_control_thread->gc_mode() == ShenandoahGenerationalControlThread::none) {\n@@ -117,14 +105,0 @@\n-void ShenandoahRegulatorThread::regulate_global_cycles() {\n-  assert(_global_heuristics != nullptr, \"Need global heuristics.\");\n-\n-  while (!should_terminate()) {\n-    if (_control_thread->gc_mode() == ShenandoahControlThread::none) {\n-      if (start_global_cycle()) {\n-        log_info(gc)(\"Heuristics request for global collection accepted.\");\n-      }\n-    }\n-\n-    regulator_sleep();\n-  }\n-}\n-\n@@ -137,1 +111,1 @@\n-  if (_heap_changed.try_unset()) {\n+  if (ShenandoahHeap::heap()->has_changed()) {\n@@ -140,1 +114,1 @@\n-    _sleep = MIN2<int>(ShenandoahControlIntervalMax, MAX2(1, _sleep * 2));\n+    _sleep = MIN2<uint>(ShenandoahControlIntervalMax, MAX2(1u, _sleep * 2));\n@@ -163,1 +137,1 @@\n-  return _global_heuristics->should_start_gc() && request_concurrent_gc(ShenandoahControlThread::select_global_generation());\n+  return _global_heuristics->should_start_gc() && request_concurrent_gc(GLOBAL);\n","filename":"src\/hotspot\/share\/gc\/shenandoah\/shenandoahRegulatorThread.cpp","additions":19,"deletions":45,"binary":false,"changes":64,"status":"modified"},{"patch":"@@ -28,3 +28,0 @@\n-#include \"gc\/shared\/gcCause.hpp\"\n-#include \"gc\/shenandoah\/shenandoahSharedVariables.hpp\"\n-#include \"runtime\/mutex.hpp\"\n@@ -33,1 +30,1 @@\n-class ShenandoahControlThread;\n+class ShenandoahGenerationalControlThread;\n@@ -51,11 +48,1 @@\n-  explicit ShenandoahRegulatorThread(ShenandoahControlThread* control_thread);\n-\n-  const char* name() const { return \"ShenandoahRegulatorThread\";}\n-\n-  \/\/ This is called from allocation path, and thus should be fast.\n-  void notify_heap_changed() {\n-    \/\/ Notify that something had changed.\n-    if (_heap_changed.is_unset()) {\n-      _heap_changed.set();\n-    }\n-  }\n+  explicit ShenandoahRegulatorThread(ShenandoahGenerationalControlThread* control_thread);\n@@ -64,2 +51,2 @@\n-  void run_service();\n-  void stop_service();\n+  void run_service() override;\n+  void stop_service() override;\n@@ -72,2 +59,0 @@\n-  \/\/ Default behavior for other modes (single generation).\n-  void regulate_global_cycles();\n@@ -90,2 +75,1 @@\n-  ShenandoahSharedFlag _heap_changed;\n-  ShenandoahControlThread* _control_thread;\n+  ShenandoahGenerationalControlThread* _control_thread;\n@@ -96,1 +80,1 @@\n-  int _sleep;\n+  uint _sleep;\n","filename":"src\/hotspot\/share\/gc\/shenandoah\/shenandoahRegulatorThread.hpp","additions":6,"deletions":22,"binary":false,"changes":28,"status":"modified"},{"patch":"@@ -29,1 +29,0 @@\n-#include \"gc\/shenandoah\/shenandoahHeap.hpp\"\n","filename":"src\/hotspot\/share\/gc\/shenandoah\/shenandoahSATBMarkQueueSet.hpp","additions":0,"deletions":1,"binary":false,"changes":1,"status":"modified"},{"patch":"@@ -34,0 +34,1 @@\n+#include \"gc\/shenandoah\/shenandoahGenerationType.hpp\"\n@@ -150,2 +151,2 @@\n-    case GLOBAL_NON_GEN: {\n-      ShenandoahInitMarkRootsClosure<GLOBAL_NON_GEN> init_mark(task_queues()->queue(worker_id));\n+    case NON_GEN: {\n+      ShenandoahInitMarkRootsClosure<NON_GEN> init_mark(task_queues()->queue(worker_id));\n@@ -155,2 +156,2 @@\n-    case GLOBAL_GEN: {\n-      ShenandoahInitMarkRootsClosure<GLOBAL_GEN> init_mark(task_queues()->queue(worker_id));\n+    case GLOBAL: {\n+      ShenandoahInitMarkRootsClosure<GLOBAL> init_mark(task_queues()->queue(worker_id));\n@@ -165,0 +166,1 @@\n+    case OLD:\n@@ -176,3 +178,2 @@\n-  mark_loop(_generation->type(),\n-            worker_id, &_terminator, rp,\n-            false \/* not cancellable *\/,\n+  mark_loop(worker_id, &_terminator, rp,\n+            _generation->type(), false \/* not cancellable *\/,\n","filename":"src\/hotspot\/share\/gc\/shenandoah\/shenandoahSTWMark.cpp","additions":8,"deletions":7,"binary":false,"changes":15,"status":"modified"},{"patch":"@@ -29,0 +29,1 @@\n+#include \"gc\/shenandoah\/shenandoahRootProcessor.hpp\"\n","filename":"src\/hotspot\/share\/gc\/shenandoah\/shenandoahSTWMark.hpp","additions":1,"deletions":0,"binary":false,"changes":1,"status":"modified"},{"patch":"@@ -34,0 +34,30 @@\n+\/\/ A closure that takes an oop in the old generation and, if it's pointing\n+\/\/ into the young generation, dirties the corresponding remembered set entry.\n+\/\/ This is only used to rebuild the remembered set after a full GC.\n+class ShenandoahDirtyRememberedSetClosure : public BasicOopIterateClosure {\n+protected:\n+  ShenandoahHeap*    const _heap;\n+  RememberedScanner* const _scanner;\n+\n+public:\n+  ShenandoahDirtyRememberedSetClosure() :\n+          _heap(ShenandoahHeap::heap()),\n+          _scanner(_heap->card_scan()) {}\n+\n+  template<class T>\n+  inline void work(T* p) {\n+    assert(_heap->is_in_old(p), \"Expecting to get an old gen address\");\n+    T o = RawAccess<>::oop_load(p);\n+    if (!CompressedOops::is_null(o)) {\n+      oop obj = CompressedOops::decode_not_null(o);\n+      if (_heap->is_in_young(obj)) {\n+        \/\/ Dirty the card containing the cross-generational pointer.\n+        _scanner->mark_card_as_dirty((HeapWord*) p);\n+      }\n+    }\n+  }\n+\n+  virtual void do_oop(narrowOop* p) { work(p); }\n+  virtual void do_oop(oop* p)       { work(p); }\n+};\n+\n@@ -374,0 +404,54 @@\n+\n+ShenandoahReconstructRememberedSetTask::ShenandoahReconstructRememberedSetTask(ShenandoahRegionIterator* regions)\n+  : WorkerTask(\"Shenandoah Reset Bitmap\")\n+  , _regions(regions) { }\n+\n+void ShenandoahReconstructRememberedSetTask::work(uint worker_id) {\n+  ShenandoahParallelWorkerSession worker_session(worker_id);\n+  ShenandoahHeapRegion* r = _regions->next();\n+  ShenandoahHeap* heap = ShenandoahHeap::heap();\n+  RememberedScanner* scanner = heap->card_scan();\n+  ShenandoahDirtyRememberedSetClosure dirty_cards_for_cross_generational_pointers;\n+\n+  while (r != nullptr) {\n+    if (r->is_old() && r->is_active()) {\n+      HeapWord* obj_addr = r->bottom();\n+      if (r->is_humongous_start()) {\n+        \/\/ First, clear the remembered set\n+        oop obj = cast_to_oop(obj_addr);\n+        size_t size = obj->size();\n+\n+        \/\/ First, clear the remembered set for all spanned humongous regions\n+        size_t num_regions = ShenandoahHeapRegion::required_regions(size * HeapWordSize);\n+        size_t region_span = num_regions * ShenandoahHeapRegion::region_size_words();\n+        scanner->reset_remset(r->bottom(), region_span);\n+        size_t region_index = r->index();\n+        ShenandoahHeapRegion* humongous_region = heap->get_region(region_index);\n+        while (num_regions-- != 0) {\n+          scanner->reset_object_range(humongous_region->bottom(), humongous_region->end());\n+          region_index++;\n+          humongous_region = heap->get_region(region_index);\n+        }\n+\n+        \/\/ Then register the humongous object and DIRTY relevant remembered set cards\n+        scanner->register_object_without_lock(obj_addr);\n+        obj->oop_iterate(&dirty_cards_for_cross_generational_pointers);\n+      } else if (!r->is_humongous()) {\n+        \/\/ First, clear the remembered set\n+        scanner->reset_remset(r->bottom(), ShenandoahHeapRegion::region_size_words());\n+        scanner->reset_object_range(r->bottom(), r->end());\n+\n+        \/\/ Then iterate over all objects, registering object and DIRTYing relevant remembered set cards\n+        HeapWord* t = r->top();\n+        while (obj_addr < t) {\n+          oop obj = cast_to_oop(obj_addr);\n+          size_t size = obj->size();\n+          scanner->register_object_without_lock(obj_addr);\n+          obj_addr += obj->oop_iterate_size(&dirty_cards_for_cross_generational_pointers);\n+        }\n+      } \/\/ else, ignore humongous continuation region\n+    }\n+    \/\/ else, this region is FREE or YOUNG or inactive and we can ignore it.\n+    r = _regions->next();\n+  }\n+}\n\\ No newline at end of file\n","filename":"src\/hotspot\/share\/gc\/shenandoah\/shenandoahScanRemembered.cpp","additions":84,"deletions":0,"binary":false,"changes":84,"status":"modified"},{"patch":"@@ -177,1 +177,0 @@\n-#include <stdint.h>\n@@ -1050,0 +1049,12 @@\n+\/\/ After Full GC is done, reconstruct the remembered set by iterating over OLD regions,\n+\/\/ registering all objects between bottom() and top(), and dirtying the cards containing\n+\/\/ cross-generational pointers.\n+class ShenandoahReconstructRememberedSetTask : public WorkerTask {\n+private:\n+  ShenandoahRegionIterator* _regions;\n+\n+public:\n+  explicit ShenandoahReconstructRememberedSetTask(ShenandoahRegionIterator* regions);\n+\n+  void work(uint worker_id) override;\n+};\n","filename":"src\/hotspot\/share\/gc\/shenandoah\/shenandoahScanRemembered.hpp","additions":12,"deletions":1,"binary":false,"changes":13,"status":"modified"},{"patch":"@@ -185,1 +185,1 @@\n-      ClassLoaderDataGraph::purge(\/*at_safepoint*\/false);\n+      ClassLoaderDataGraph::purge(false \/* at_safepoint *\/);\n","filename":"src\/hotspot\/share\/gc\/shenandoah\/shenandoahUnload.cpp","additions":1,"deletions":1,"binary":false,"changes":2,"status":"modified"},{"patch":"@@ -77,1 +77,0 @@\n-\n","filename":"src\/hotspot\/share\/gc\/shenandoah\/shenandoahUtils.cpp","additions":0,"deletions":1,"binary":false,"changes":1,"status":"modified"},{"patch":"@@ -49,3 +49,3 @@\n-    case GLOBAL_NON_GEN:                                                  \\\n-      return prefix \"\" postfix;                                           \\\n-    case GLOBAL_GEN:                                                      \\\n+    case NON_GEN:                                                         \\\n+      return prefix \" (NON-GENERATIONAL)\" postfix;                        \\\n+    case GLOBAL:                                                          \\\n@@ -59,1 +59,1 @@\n-      return prefix \" (?)\" postfix;                                       \\\n+      return prefix \" (UNKNOWN)\" postfix;                                 \\\n","filename":"src\/hotspot\/share\/gc\/shenandoah\/shenandoahUtils.hpp","additions":4,"deletions":4,"binary":false,"changes":8,"status":"modified"},{"patch":"@@ -414,1 +414,1 @@\n-      size_t pad = ShenandoahHeap::heap()->get_pad_for_promote_in_place();\n+      size_t pad = heap->old_generation()->get_pad_for_promote_in_place();\n@@ -864,1 +864,1 @@\n-      heap_used = _heap->used() + _heap->get_pad_for_promote_in_place();\n+      heap_used = _heap->used() + _heap->old_generation()->get_pad_for_promote_in_place();\n","filename":"src\/hotspot\/share\/gc\/shenandoah\/shenandoahVerifier.cpp","additions":2,"deletions":2,"binary":false,"changes":4,"status":"modified"},{"patch":"@@ -28,1 +28,0 @@\n-#include \"gc\/shared\/workerPolicy.hpp\"\n@@ -30,15 +29,0 @@\n-#include \"runtime\/javaThread.hpp\"\n-#include \"runtime\/threads.hpp\"\n-\n-uint ShenandoahWorkerPolicy::_prev_par_marking     = 0;\n-uint ShenandoahWorkerPolicy::_prev_conc_marking    = 0;\n-uint ShenandoahWorkerPolicy::_prev_conc_rs_scanning = 0;\n-uint ShenandoahWorkerPolicy::_prev_conc_evac       = 0;\n-uint ShenandoahWorkerPolicy::_prev_conc_root_proc  = 0;\n-uint ShenandoahWorkerPolicy::_prev_conc_refs_proc  = 0;\n-uint ShenandoahWorkerPolicy::_prev_fullgc          = 0;\n-uint ShenandoahWorkerPolicy::_prev_degengc         = 0;\n-uint ShenandoahWorkerPolicy::_prev_conc_update_ref = 0;\n-uint ShenandoahWorkerPolicy::_prev_par_update_ref  = 0;\n-uint ShenandoahWorkerPolicy::_prev_conc_cleanup    = 0;\n-uint ShenandoahWorkerPolicy::_prev_conc_reset      = 0;\n@@ -47,7 +31,1 @@\n-  uint active_workers = (_prev_par_marking == 0) ? ParallelGCThreads : _prev_par_marking;\n-\n-  _prev_par_marking =\n-    WorkerPolicy::calc_active_workers(ParallelGCThreads,\n-                                      active_workers,\n-                                      Threads::number_of_non_daemon_threads());\n-  return _prev_par_marking;\n+  return ParallelGCThreads;\n@@ -57,6 +35,1 @@\n-  uint active_workers = (_prev_conc_marking == 0) ?  ConcGCThreads : _prev_conc_marking;\n-  _prev_conc_marking =\n-    WorkerPolicy::calc_active_conc_workers(ConcGCThreads,\n-                                           active_workers,\n-                                           Threads::number_of_non_daemon_threads());\n-  return _prev_conc_marking;\n+  return ConcGCThreads;\n@@ -66,6 +39,1 @@\n-  uint active_workers = (_prev_conc_rs_scanning == 0) ? ConcGCThreads : _prev_conc_rs_scanning;\n-  _prev_conc_rs_scanning =\n-    WorkerPolicy::calc_active_conc_workers(ConcGCThreads,\n-                                           active_workers,\n-                                           Threads::number_of_non_daemon_threads());\n-  return _prev_conc_rs_scanning;\n+  return ConcGCThreads;\n@@ -74,1 +42,0 @@\n-\/\/ Reuse the calculation result from init marking\n@@ -76,1 +43,1 @@\n-  return _prev_par_marking;\n+  return ParallelGCThreads;\n@@ -79,1 +46,0 @@\n-\/\/ Calculate workers for concurrent refs processing\n@@ -81,6 +47,1 @@\n-  uint active_workers = (_prev_conc_refs_proc == 0) ? ConcGCThreads : _prev_conc_refs_proc;\n-  _prev_conc_refs_proc =\n-    WorkerPolicy::calc_active_conc_workers(ConcGCThreads,\n-                                           active_workers,\n-                                           Threads::number_of_non_daemon_threads());\n-  return _prev_conc_refs_proc;\n+  return ConcGCThreads;\n@@ -89,1 +50,0 @@\n-\/\/ Calculate workers for concurrent root processing\n@@ -91,6 +51,1 @@\n-  uint active_workers = (_prev_conc_root_proc == 0) ? ConcGCThreads : _prev_conc_root_proc;\n-  _prev_conc_root_proc =\n-          WorkerPolicy::calc_active_conc_workers(ConcGCThreads,\n-                                                 active_workers,\n-                                                 Threads::number_of_non_daemon_threads());\n-  return _prev_conc_root_proc;\n+  return ConcGCThreads;\n@@ -99,1 +54,0 @@\n-\/\/ Calculate workers for concurrent evacuation (concurrent GC)\n@@ -101,6 +55,1 @@\n-  uint active_workers = (_prev_conc_evac == 0) ? ConcGCThreads : _prev_conc_evac;\n-  _prev_conc_evac =\n-    WorkerPolicy::calc_active_conc_workers(ConcGCThreads,\n-                                           active_workers,\n-                                           Threads::number_of_non_daemon_threads());\n-  return _prev_conc_evac;\n+  return ConcGCThreads;\n@@ -109,1 +58,0 @@\n-\/\/ Calculate workers for parallel fullgc\n@@ -111,6 +59,1 @@\n-  uint active_workers = (_prev_fullgc == 0) ?  ParallelGCThreads : _prev_fullgc;\n-  _prev_fullgc =\n-    WorkerPolicy::calc_active_workers(ParallelGCThreads,\n-                                      active_workers,\n-                                      Threads::number_of_non_daemon_threads());\n-  return _prev_fullgc;\n+  return ParallelGCThreads;\n@@ -119,1 +62,0 @@\n-\/\/ Calculate workers for parallel degenerated gc\n@@ -121,6 +63,1 @@\n-  uint active_workers = (_prev_degengc == 0) ?  ParallelGCThreads : _prev_degengc;\n-  _prev_degengc =\n-    WorkerPolicy::calc_active_workers(ParallelGCThreads,\n-                                      active_workers,\n-                                      Threads::number_of_non_daemon_threads());\n-  return _prev_degengc;\n+  return ParallelGCThreads;\n@@ -129,1 +66,0 @@\n-\/\/ Calculate workers for concurrent reference update\n@@ -131,6 +67,1 @@\n-  uint active_workers = (_prev_conc_update_ref == 0) ? ConcGCThreads : _prev_conc_update_ref;\n-  _prev_conc_update_ref =\n-    WorkerPolicy::calc_active_conc_workers(ConcGCThreads,\n-                                           active_workers,\n-                                           Threads::number_of_non_daemon_threads());\n-  return _prev_conc_update_ref;\n+  return ConcGCThreads;\n@@ -139,1 +70,0 @@\n-\/\/ Calculate workers for parallel reference update\n@@ -141,15 +71,1 @@\n-  uint active_workers = (_prev_par_update_ref == 0) ? ParallelGCThreads : _prev_par_update_ref;\n-  _prev_par_update_ref =\n-    WorkerPolicy::calc_active_workers(ParallelGCThreads,\n-                                      active_workers,\n-                                      Threads::number_of_non_daemon_threads());\n-  return _prev_par_update_ref;\n-}\n-\n-uint ShenandoahWorkerPolicy::calc_workers_for_conc_cleanup() {\n-  uint active_workers = (_prev_conc_cleanup == 0) ? ConcGCThreads : _prev_conc_cleanup;\n-  _prev_conc_cleanup =\n-          WorkerPolicy::calc_active_conc_workers(ConcGCThreads,\n-                                                 active_workers,\n-                                                 Threads::number_of_non_daemon_threads());\n-  return _prev_conc_cleanup;\n+  return ParallelGCThreads;\n@@ -159,6 +75,1 @@\n-  uint active_workers = (_prev_conc_reset == 0) ? ConcGCThreads : _prev_conc_reset;\n-  _prev_conc_reset =\n-          WorkerPolicy::calc_active_conc_workers(ConcGCThreads,\n-                                                 active_workers,\n-                                                 Threads::number_of_non_daemon_threads());\n-  return _prev_conc_reset;\n+  return ConcGCThreads;\n","filename":"src\/hotspot\/share\/gc\/shenandoah\/shenandoahWorkerPolicy.cpp","additions":12,"deletions":101,"binary":false,"changes":113,"status":"modified"},{"patch":"@@ -31,14 +31,0 @@\n-private:\n-  static uint _prev_par_marking;\n-  static uint _prev_conc_marking;\n-  static uint _prev_conc_rs_scanning;\n-  static uint _prev_conc_root_proc;\n-  static uint _prev_conc_refs_proc;\n-  static uint _prev_conc_evac;\n-  static uint _prev_fullgc;\n-  static uint _prev_degengc;\n-  static uint _prev_conc_update_ref;\n-  static uint _prev_par_update_ref;\n-  static uint _prev_conc_cleanup;\n-  static uint _prev_conc_reset;\n-\n@@ -79,3 +65,0 @@\n-  \/\/ Calculate workers for concurrent cleanup\n-  static uint calc_workers_for_conc_cleanup();\n-\n","filename":"src\/hotspot\/share\/gc\/shenandoah\/shenandoahWorkerPolicy.hpp","additions":0,"deletions":17,"binary":false,"changes":17,"status":"modified"},{"patch":"@@ -499,3 +499,4 @@\n-  product(uintx, ShenandoahOOMGCRetries, 3, EXPERIMENTAL,                   \\\n-          \"How many GCs should happen before we throw OutOfMemoryException \"\\\n-          \"for allocation request, including at least one Full GC.\")        \\\n+  product(uintx, ShenandoahNoProgressThreshold, 5, EXPERIMENTAL,            \\\n+          \"After this number of consecutive Full GCs fail to make \"         \\\n+          \"progress, Shenandoah will raise out of memory errors. Note \"     \\\n+          \"that progress is determined by ShenandoahCriticalFreeThreshold\") \\\n","filename":"src\/hotspot\/share\/gc\/shenandoah\/shenandoah_globals.hpp","additions":4,"deletions":3,"binary":false,"changes":7,"status":"modified"},{"patch":"@@ -88,1 +88,1 @@\n-    _heap->set_old_evac_reserve(_heap->old_generation()->soft_max_capacity() \/ 4);\n+    _heap->old_generation()->set_evacuation_reserve(_heap->old_generation()->soft_max_capacity() \/ 4);\n","filename":"test\/hotspot\/gtest\/gc\/shenandoah\/test_shenandoahOldHeuristic.cpp","additions":1,"deletions":1,"binary":false,"changes":2,"status":"modified"},{"patch":"@@ -56,5 +56,0 @@\n-    if (GC.Shenandoah.isSupported()) {\n-      noneGCSupported = false;\n-      testDynamicNumberOfGCThreads(\"UseShenandoahGC\");\n-    }\n-\n@@ -62,1 +57,1 @@\n-      throw new SkippedException(\"Skipping test because none of G1\/Parallel\/Shenandoah is supported.\");\n+      throw new SkippedException(\"Skipping test because none of G1\/Parallel is supported.\");\n","filename":"test\/hotspot\/jtreg\/gc\/ergonomics\/TestDynamicNumberOfGCThreads.java","additions":1,"deletions":6,"binary":false,"changes":7,"status":"modified"},{"patch":"@@ -84,0 +84,1 @@\n+                    \"-XX:ShenandoahNoProgressThreshold=16\",\n","filename":"test\/hotspot\/jtreg\/gc\/shenandoah\/oom\/TestThreadFailure.java","additions":1,"deletions":0,"binary":false,"changes":1,"status":"modified"}]}