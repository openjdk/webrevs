{"files":[{"patch":"@@ -81,11 +81,0 @@\n-ShenandoahGenerationalHeap* ShenandoahGenerationalHeap::heap() {\n-  shenandoah_assert_generational();\n-  CollectedHeap* heap = Universe::heap();\n-  return cast(heap);\n-}\n-\n-ShenandoahGenerationalHeap* ShenandoahGenerationalHeap::cast(CollectedHeap* heap) {\n-  shenandoah_assert_generational();\n-  return checked_cast<ShenandoahGenerationalHeap*>(heap);\n-}\n-\n@@ -251,1 +240,0 @@\n-          assert(mode()->is_generational(), \"OLD Generation only exists in generational mode\");\n@@ -261,5 +249,2 @@\n-            \/\/ where abundance is defined as >= ShenGenHeap::plab_min_size().  In the former case, we try resetting the desired\n-            \/\/ PLAB size and retry PLAB allocation to avoid cascading of shared memory allocations.\n-\n-            \/\/ In this situation, PLAB memory is precious.  We'll try to preserve our existing PLAB by forcing\n-            \/\/ this particular allocation to be shared.\n+            \/\/ where abundance is defined as >= ShenGenHeap::plab_min_size().  In the former case, we try shrinking the\n+            \/\/ desired PLAB size to the minimum and retry PLAB allocation to avoid cascading of shared memory allocations.\n@@ -325,1 +310,0 @@\n-\n@@ -328,0 +312,1 @@\n+  \/\/ Update the age of the evacuated object\n@@ -333,2 +318,0 @@\n-  ContinuationGCSupport::relativize_stack_chunk(copy_val);\n-\n@@ -338,0 +321,9 @@\n+\n+    \/\/ This is necessary for virtual thread support. This uses the mark word without\n+    \/\/ considering that it may now be a forwarding pointer (and could therefore crash).\n+    \/\/ Secondarily, we do not want to spend cycles relativizing stack chunks for oops\n+    \/\/ that lost the evacuation race (and will therefore not become visible). It is\n+    \/\/ safe to do this on the public copy (this is also done during concurrent mark).\n+    ContinuationGCSupport::relativize_stack_chunk(copy_val);\n+\n+    \/\/ Record that the evacuation succeeded\n@@ -339,0 +331,1 @@\n+\n@@ -528,4 +521,0 @@\n-\/\/ TODO: It is probably most efficient to register all objects (both promotions and evacuations) that were allocated within\n-\/\/ this plab at the time we retire the plab.  A tight registration loop will run within both code and data caches.  This change\n-\/\/ would allow smaller and faster in-line implementation of alloc_from_plab().  Since plabs are aligned on card-table boundaries,\n-\/\/ this object registration loop can be performed without acquiring a lock.\n@@ -806,1 +795,1 @@\n-      assert (update_watermark >= r->bottom(), \"sanity\");\n+      assert(update_watermark >= r->bottom(), \"sanity\");\n@@ -816,4 +805,0 @@\n-            \/\/ Note that GLOBAL collection is not as effectively balanced as young and mixed cycles.  This is because\n-            \/\/ concurrent GC threads are parceled out entire heap regions of work at a time and there\n-            \/\/ is no \"catchup phase\" consisting of remembered set scanning, during which parcels of work are smaller\n-            \/\/ and more easily distributed more fairly across threads.\n@@ -821,1 +806,0 @@\n-            \/\/ TODO: Consider an improvement to load balance GLOBAL GC.\n@@ -843,0 +827,1 @@\n+\n@@ -846,0 +831,1 @@\n+\n@@ -849,0 +835,1 @@\n+\n@@ -856,15 +843,6 @@\n-      \/\/ After this thread has exhausted its traditional update-refs work, it continues with updating refs within remembered set.\n-      \/\/ The remembered set workload is better balanced between threads, so threads that are \"behind\" can catch up with other\n-      \/\/ threads during this phase, allowing all threads to work more effectively in parallel.\n-      struct ShenandoahRegionChunk assignment;\n-      ShenandoahScanRemembered* scanner = _heap->old_generation()->card_scan();\n-\n-      while (!_heap->check_cancelled_gc_and_yield(CONCURRENT) && _work_chunks->next(&assignment)) {\n-        \/\/ Keep grabbing next work chunk to process until finished, or asked to yield\n-        ShenandoahHeapRegion* r = assignment._r;\n-        if (r->is_active() && !r->is_cset() && r->is_old()) {\n-          HeapWord* start_of_range = r->bottom() + assignment._chunk_offset;\n-          HeapWord* end_of_range = r->get_update_watermark();\n-          if (end_of_range > start_of_range + assignment._chunk_size) {\n-            end_of_range = start_of_range + assignment._chunk_size;\n-          }\n+      \/\/ After this thread has exhausted its traditional update-refs work, it continues with updating refs within\n+      \/\/ remembered set. The remembered set workload is better balanced between threads, so threads that are \"behind\"\n+      \/\/ can catch up with other threads during this phase, allowing all threads to work more effectively in parallel.\n+      update_references_in_remembered_set(worker_id, cl, ctx, is_mixed);\n+    }\n+  }\n@@ -872,77 +850,25 @@\n-          \/\/ Old region in a young cycle or mixed cycle.\n-          if (is_mixed) {\n-            \/\/ TODO: For mixed evac, consider building an old-gen remembered set that allows restricted updating\n-            \/\/ within old-gen HeapRegions.  This remembered set can be constructed by old-gen concurrent marking\n-            \/\/ and augmented by card marking.  For example, old-gen concurrent marking can remember for each old-gen\n-            \/\/ card which other old-gen regions it refers to: none, one-other specifically, multiple-other non-specific.\n-            \/\/ Update-references when _mixed_evac processess each old-gen memory range that has a traditional DIRTY\n-            \/\/ card or if the \"old-gen remembered set\" indicates that this card holds pointers specifically to an\n-            \/\/ old-gen region in the most recent collection set, or if this card holds pointers to other non-specific\n-            \/\/ old-gen heap regions.\n-\n-            if (r->is_humongous()) {\n-              if (start_of_range < end_of_range) {\n-                \/\/ Need to examine both dirty and clean cards during mixed evac.\n-                r->oop_iterate_humongous_slice(&cl, false, start_of_range, assignment._chunk_size, true);\n-              }\n-            } else {\n-              \/\/ Since this is mixed evacuation, old regions that are candidates for collection have not been coalesced\n-              \/\/ and filled.  Use mark bits to find objects that need to be updated.\n-              \/\/\n-              \/\/ Future TODO: establish a second remembered set to identify which old-gen regions point to other old-gen\n-              \/\/ regions which are in the collection set for a particular mixed evacuation.\n-              if (start_of_range < end_of_range) {\n-                HeapWord* p = nullptr;\n-                size_t card_index = scanner->card_index_for_addr(start_of_range);\n-                \/\/ In case last object in my range spans boundary of my chunk, I may need to scan all the way to top()\n-                ShenandoahObjectToOopBoundedClosure<T> objs(&cl, start_of_range, r->top());\n-\n-                \/\/ Any object that begins in a previous range is part of a different scanning assignment.  Any object that\n-                \/\/ starts after end_of_range is also not my responsibility.  (Either allocated during evacuation, so does\n-                \/\/ not hold pointers to from-space, or is beyond the range of my assigned work chunk.)\n-\n-                \/\/ Find the first object that begins in my range, if there is one.\n-                p = start_of_range;\n-                oop obj = cast_to_oop(p);\n-                HeapWord* tams = ctx->top_at_mark_start(r);\n-                if (p >= tams) {\n-                  \/\/ We cannot use ctx->is_marked(obj) to test whether an object begins at this address.  Instead,\n-                  \/\/ we need to use the remembered set crossing map to advance p to the first object that starts\n-                  \/\/ within the enclosing card.\n-\n-                  while (true) {\n-                    HeapWord* first_object = scanner->first_object_in_card(card_index);\n-                    if (first_object != nullptr) {\n-                      p = first_object;\n-                      break;\n-                    } else if (scanner->addr_for_card_index(card_index + 1) < end_of_range) {\n-                      card_index++;\n-                    } else {\n-                      \/\/ Force the loop that follows to immediately terminate.\n-                      p = end_of_range;\n-                      break;\n-                    }\n-                  }\n-                  obj = cast_to_oop(p);\n-                  \/\/ Note: p may be >= end_of_range\n-                } else if (!ctx->is_marked(obj)) {\n-                  p = ctx->get_next_marked_addr(p, tams);\n-                  obj = cast_to_oop(p);\n-                  \/\/ If there are no more marked objects before tams, this returns tams.\n-                  \/\/ Note that tams is either >= end_of_range, or tams is the start of an object that is marked.\n-                }\n-                while (p < end_of_range) {\n-                  \/\/ p is known to point to the beginning of marked object obj\n-                  objs.do_object(obj);\n-                  HeapWord* prev_p = p;\n-                  p += obj->size();\n-                  if (p < tams) {\n-                    p = ctx->get_next_marked_addr(p, tams);\n-                    \/\/ If there are no more marked objects before tams, this returns tams.  Note that tams is\n-                    \/\/ either >= end_of_range, or tams is the start of an object that is marked.\n-                  }\n-                  assert(p != prev_p, \"Lack of forward progress\");\n-                  obj = cast_to_oop(p);\n-                }\n-              }\n-            }\n+  template<class T>\n+  void update_references_in_remembered_set(uint worker_id, T &cl, const ShenandoahMarkingContext* ctx, bool is_mixed) {\n+\n+    struct ShenandoahRegionChunk assignment;\n+    ShenandoahScanRemembered* scanner = _heap->old_generation()->card_scan();\n+\n+    while (!_heap->check_cancelled_gc_and_yield(CONCURRENT) && _work_chunks->next(&assignment)) {\n+      \/\/ Keep grabbing next work chunk to process until finished, or asked to yield\n+      ShenandoahHeapRegion* r = assignment._r;\n+      if (r->is_active() && !r->is_cset() && r->is_old()) {\n+        HeapWord* start_of_range = r->bottom() + assignment._chunk_offset;\n+        HeapWord* end_of_range = r->get_update_watermark();\n+        if (end_of_range > start_of_range + assignment._chunk_size) {\n+          end_of_range = start_of_range + assignment._chunk_size;\n+        }\n+\n+        if (start_of_range >= end_of_range) {\n+          continue;\n+        }\n+\n+        \/\/ Old region in a young cycle or mixed cycle.\n+        if (is_mixed) {\n+          if (r->is_humongous()) {\n+            \/\/ Need to examine both dirty and clean cards during mixed evac.\n+            r->oop_iterate_humongous_slice(&cl, false, start_of_range, assignment._chunk_size, true);\n@@ -950,11 +876,3 @@\n-            \/\/ This is a young evac..\n-            if (start_of_range < end_of_range) {\n-              size_t cluster_size =\n-                      CardTable::card_size_in_words() * ShenandoahCardCluster::CardsPerCluster;\n-              size_t clusters = assignment._chunk_size \/ cluster_size;\n-              assert(clusters * cluster_size == assignment._chunk_size, \"Chunk assignment must align on cluster boundaries\");\n-              scanner->process_region_slice(r, assignment._chunk_offset, clusters, end_of_range, &cl, true, worker_id);\n-            }\n-          }\n-          if (ShenandoahPacing && (start_of_range < end_of_range)) {\n-            _heap->pacer()->report_updaterefs(pointer_delta(end_of_range, start_of_range));\n+            \/\/ Since this is mixed evacuation, old regions that are candidates for collection have not been coalesced\n+            \/\/ and filled.  This will use mark bits to find objects that need to be updated.\n+            update_references_in_old_region(cl, ctx, scanner, r, start_of_range, end_of_range);\n@@ -962,0 +880,66 @@\n+        } else {\n+          \/\/ This is a young evacuation\n+          size_t cluster_size = CardTable::card_size_in_words() * ShenandoahCardCluster::CardsPerCluster;\n+          size_t clusters = assignment._chunk_size \/ cluster_size;\n+          assert(clusters * cluster_size == assignment._chunk_size, \"Chunk assignment must align on cluster boundaries\");\n+          scanner->process_region_slice(r, assignment._chunk_offset, clusters, end_of_range, &cl, true, worker_id);\n+        }\n+\n+        if (ShenandoahPacing) {\n+          _heap->pacer()->report_updaterefs(pointer_delta(end_of_range, start_of_range));\n+        }\n+      }\n+    }\n+  }\n+\n+  template<class T>\n+  void update_references_in_old_region(T &cl, const ShenandoahMarkingContext* ctx, ShenandoahScanRemembered* scanner,\n+                                    const ShenandoahHeapRegion* r, HeapWord* start_of_range,\n+                                    HeapWord* end_of_range) const {\n+    \/\/ In case last object in my range spans boundary of my chunk, I may need to scan all the way to top()\n+    ShenandoahObjectToOopBoundedClosure<T> objs(&cl, start_of_range, r->top());\n+\n+    \/\/ Any object that begins in a previous range is part of a different scanning assignment.  Any object that\n+    \/\/ starts after end_of_range is also not my responsibility.  (Either allocated during evacuation, so does\n+    \/\/ not hold pointers to from-space, or is beyond the range of my assigned work chunk.)\n+\n+    \/\/ Find the first object that begins in my range, if there is one. Note that `p` will be set to `end_of_range`\n+    \/\/ when no live object is found in the range.\n+    HeapWord* tams = ctx->top_at_mark_start(r);\n+    HeapWord* p = get_first_object_start_word(ctx, scanner, tams, start_of_range, end_of_range);\n+\n+    while (p < end_of_range) {\n+      \/\/ p is known to point to the beginning of marked object obj\n+      oop obj = cast_to_oop(p);\n+      objs.do_object(obj);\n+      HeapWord* prev_p = p;\n+      p += obj->size();\n+      if (p < tams) {\n+        p = ctx->get_next_marked_addr(p, tams);\n+        \/\/ If there are no more marked objects before tams, this returns tams.  Note that tams is\n+        \/\/ either >= end_of_range, or tams is the start of an object that is marked.\n+      }\n+      assert(p != prev_p, \"Lack of forward progress\");\n+    }\n+  }\n+\n+  HeapWord* get_first_object_start_word(const ShenandoahMarkingContext* ctx, ShenandoahScanRemembered* scanner, HeapWord* tams,\n+                                        HeapWord* start_of_range, HeapWord* end_of_range) const {\n+    HeapWord* p = start_of_range;\n+\n+    if (p >= tams) {\n+      \/\/ We cannot use ctx->is_marked(obj) to test whether an object begins at this address.  Instead,\n+      \/\/ we need to use the remembered set crossing map to advance p to the first object that starts\n+      \/\/ within the enclosing card.\n+      size_t card_index = scanner->card_index_for_addr(start_of_range);\n+      while (true) {\n+        HeapWord* first_object = scanner->first_object_in_card(card_index);\n+        if (first_object != nullptr) {\n+          p = first_object;\n+          break;\n+        } else if (scanner->addr_for_card_index(card_index + 1) < end_of_range) {\n+          card_index++;\n+        } else {\n+          \/\/ Signal that no object was found in range\n+          p = end_of_range;\n+          break;\n@@ -964,0 +948,4 @@\n+    } else if (!ctx->is_marked(cast_to_oop(p))) {\n+      p = ctx->get_next_marked_addr(p, tams);\n+      \/\/ If there are no more marked objects before tams, this returns tams.\n+      \/\/ Note that tams is either >= end_of_range, or tams is the start of an object that is marked.\n@@ -965,0 +953,1 @@\n+    return p;\n","filename":"src\/hotspot\/share\/gc\/shenandoah\/shenandoahGenerationalHeap.cpp","additions":122,"deletions":133,"binary":false,"changes":255,"status":"modified"},{"patch":"@@ -28,0 +28,1 @@\n+#include \"gc\/shenandoah\/shenandoahAsserts.hpp\"\n@@ -29,0 +30,2 @@\n+#include \"memory\/universe.hpp\"\n+#include \"utilities\/checkedCast.hpp\"\n@@ -41,2 +44,10 @@\n-  static ShenandoahGenerationalHeap* heap();\n-  static ShenandoahGenerationalHeap* cast(CollectedHeap* heap);\n+  static ShenandoahGenerationalHeap* heap() {\n+    shenandoah_assert_generational();\n+    CollectedHeap* heap = Universe::heap();\n+    return cast(heap);\n+  }\n+\n+  static ShenandoahGenerationalHeap* cast(CollectedHeap* heap) {\n+    shenandoah_assert_generational();\n+    return checked_cast<ShenandoahGenerationalHeap*>(heap);\n+  }\n","filename":"src\/hotspot\/share\/gc\/shenandoah\/shenandoahGenerationalHeap.hpp","additions":13,"deletions":2,"binary":false,"changes":15,"status":"modified"}]}