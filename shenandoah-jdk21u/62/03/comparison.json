{"files":[{"patch":"@@ -328,2 +328,3 @@\n-    \/\/ Cycle is complete\n-    heap->global_generation()->heuristics()->record_success_concurrent(gc.abbreviated());\n+    \/\/ Cycle is complete.  There were no failed allocation requests and no degeneration, so count this as good progress.\n+    heap->notify_gc_progress();\n+    heap->global_generation()->heuristics()->record_success_concurrent();\n","filename":"src\/hotspot\/share\/gc\/shenandoah\/shenandoahControlThread.cpp","additions":3,"deletions":2,"binary":false,"changes":5,"status":"modified"},{"patch":"@@ -307,1 +307,0 @@\n-    heap->notify_gc_no_progress();\n","filename":"src\/hotspot\/share\/gc\/shenandoah\/shenandoahDegeneratedGC.cpp","additions":0,"deletions":1,"binary":false,"changes":1,"status":"modified"},{"patch":"@@ -37,0 +37,2 @@\n+#include \"gc\/shenandoah\/shenandoahSimpleBitMap.hpp\"\n+#include \"gc\/shenandoah\/shenandoahSimpleBitMap.inline.hpp\"\n@@ -41,1 +43,62 @@\n-ShenandoahSetsOfFree::ShenandoahSetsOfFree(size_t max_regions, ShenandoahFreeSet* free_set) :\n+static const char* partition_name(ShenandoahFreeSetPartitionId t) {\n+  switch (t) {\n+    case ShenandoahFreeSetPartitionId::NotFree: return \"NotFree\";\n+    case ShenandoahFreeSetPartitionId::Mutator: return \"Mutator\";\n+    case ShenandoahFreeSetPartitionId::Collector: return \"Collector\";\n+    case ShenandoahFreeSetPartitionId::OldCollector: return \"OldCollector\";\n+    default:\n+      ShouldNotReachHere();\n+      return \"Unrecognized\";\n+  }\n+}\n+\n+#ifndef PRODUCT\n+void ShenandoahRegionPartitions::dump_bitmap() const {\n+  log_info(gc)(\"Mutator range [\" SSIZE_FORMAT \", \" SSIZE_FORMAT \"], Collector range [\" SSIZE_FORMAT \", \" SSIZE_FORMAT\n+               \"], Old Collector range [\" SSIZE_FORMAT \", \" SSIZE_FORMAT \"]\",\n+               _leftmosts[int(ShenandoahFreeSetPartitionId::Mutator)],\n+               _rightmosts[int(ShenandoahFreeSetPartitionId::Mutator)],\n+               _leftmosts[int(ShenandoahFreeSetPartitionId::Collector)],\n+               _rightmosts[int(ShenandoahFreeSetPartitionId::Collector)],\n+               _leftmosts[int(ShenandoahFreeSetPartitionId::OldCollector)],\n+               _rightmosts[int(ShenandoahFreeSetPartitionId::OldCollector)]);\n+  log_info(gc)(\"Empty Mutator range [\" SSIZE_FORMAT \", \" SSIZE_FORMAT\n+               \"], Empty Collector range [\" SSIZE_FORMAT \", \" SSIZE_FORMAT\n+               \"], Empty Old Collecto range [\" SSIZE_FORMAT \", \" SSIZE_FORMAT \"]\",\n+               _leftmosts_empty[int(ShenandoahFreeSetPartitionId::Mutator)],\n+               _rightmosts_empty[int(ShenandoahFreeSetPartitionId::Mutator)],\n+               _leftmosts_empty[int(ShenandoahFreeSetPartitionId::Collector)],\n+               _rightmosts_empty[int(ShenandoahFreeSetPartitionId::Collector)],\n+               _leftmosts_empty[int(ShenandoahFreeSetPartitionId::OldCollector)],\n+               _rightmosts_empty[int(ShenandoahFreeSetPartitionId::OldCollector)]);\n+\n+  log_info(gc)(\"%6s: %18s %18s %18s %18s\", \"index\", \"Mutator Bits\", \"Collector Bits\", \"Old Collector Bits\", \"NotFree Bits\");\n+  dump_bitmap_range(0, _max-1);\n+}\n+\n+void ShenandoahRegionPartitions::dump_bitmap_range(idx_t start_region_idx, idx_t end_region_idx) const {\n+  assert((start_region_idx >= 0) && (start_region_idx < (idx_t) _max), \"precondition\");\n+  assert((end_region_idx >= 0) && (end_region_idx < (idx_t) _max), \"precondition\");\n+  idx_t aligned_start = _membership[int(ShenandoahFreeSetPartitionId::Mutator)].aligned_index(start_region_idx);\n+  idx_t aligned_end = _membership[int(ShenandoahFreeSetPartitionId::Mutator)].aligned_index(end_region_idx);\n+  idx_t alignment = _membership[int(ShenandoahFreeSetPartitionId::Mutator)].alignment();\n+  while (aligned_start <= aligned_end) {\n+    dump_bitmap_row(aligned_start);\n+    aligned_start += alignment;\n+  }\n+}\n+\n+void ShenandoahRegionPartitions::dump_bitmap_row(idx_t region_idx) const {\n+  assert((region_idx >= 0) && (region_idx < (idx_t) _max), \"precondition\");\n+  idx_t aligned_idx = _membership[int(ShenandoahFreeSetPartitionId::Mutator)].aligned_index(region_idx);\n+  uintx mutator_bits = _membership[int(ShenandoahFreeSetPartitionId::Mutator)].bits_at(aligned_idx);\n+  uintx collector_bits = _membership[int(ShenandoahFreeSetPartitionId::Collector)].bits_at(aligned_idx);\n+  uintx old_collector_bits = _membership[int(ShenandoahFreeSetPartitionId::OldCollector)].bits_at(aligned_idx);\n+  uintx free_bits = mutator_bits | collector_bits | old_collector_bits;\n+  uintx notfree_bits =  ~free_bits;\n+  log_info(gc)(SSIZE_FORMAT_W(6) \": \" SIZE_FORMAT_X_0 \" 0x\" SIZE_FORMAT_X_0 \" 0x\" SIZE_FORMAT_X_0 \" 0x\" SIZE_FORMAT_X_0,\n+               aligned_idx, mutator_bits, collector_bits, old_collector_bits, notfree_bits);\n+}\n+#endif\n+\n+ShenandoahRegionPartitions::ShenandoahRegionPartitions(size_t max_regions, ShenandoahFreeSet* free_set) :\n@@ -43,0 +106,1 @@\n+    _region_size_bytes(ShenandoahHeapRegion::region_size_bytes()),\n@@ -44,1 +108,1 @@\n-    _region_size_bytes(ShenandoahHeapRegion::region_size_bytes())\n+    _membership{ ShenandoahSimpleBitMap(max_regions), ShenandoahSimpleBitMap(max_regions) , ShenandoahSimpleBitMap(max_regions) }\n@@ -46,2 +110,1 @@\n-  _membership = NEW_C_HEAP_ARRAY(ShenandoahFreeMemoryType, max_regions, mtGC);\n-  clear_internal();\n+  make_all_regions_unavailable();\n@@ -50,2 +113,2 @@\n-ShenandoahSetsOfFree::~ShenandoahSetsOfFree() {\n-  FREE_C_HEAP_ARRAY(ShenandoahFreeMemoryType, _membership);\n+inline bool ShenandoahFreeSet::can_allocate_from(ShenandoahHeapRegion *r) const {\n+  return r->is_empty() || (r->is_trash() && !_heap->is_concurrent_weak_root_in_progress());\n@@ -54,0 +117,4 @@\n+inline bool ShenandoahFreeSet::can_allocate_from(size_t idx) const {\n+  ShenandoahHeapRegion* r = _heap->get_region(idx);\n+  return can_allocate_from(r);\n+}\n@@ -55,3 +122,6 @@\n-void ShenandoahSetsOfFree::clear_internal() {\n-  for (size_t idx = 0; idx < _max; idx++) {\n-    _membership[idx] = NotFree;\n+inline size_t ShenandoahFreeSet::alloc_capacity(ShenandoahHeapRegion *r) const {\n+  if (r->is_trash()) {\n+    \/\/ This would be recycled on allocation path\n+    return ShenandoahHeapRegion::region_size_bytes();\n+  } else {\n+    return r->free();\n@@ -59,0 +129,10 @@\n+}\n+\n+inline size_t ShenandoahFreeSet::alloc_capacity(size_t idx) const {\n+  ShenandoahHeapRegion* r = _heap->get_region(idx);\n+  return alloc_capacity(r);\n+}\n+\n+inline bool ShenandoahFreeSet::has_alloc_capacity(ShenandoahHeapRegion *r) const {\n+  return alloc_capacity(r) > 0;\n+}\n@@ -60,7 +140,10 @@\n-  for (size_t idx = 0; idx < NumFreeSets; idx++) {\n-    _leftmosts[idx] = _max;\n-    _rightmosts[idx] = 0;\n-    _leftmosts_empty[idx] = _max;\n-    _rightmosts_empty[idx] = 0;\n-    _capacity_of[idx] = 0;\n-    _used_by[idx] = 0;\n+inline idx_t ShenandoahRegionPartitions::leftmost(ShenandoahFreeSetPartitionId which_partition) const {\n+  assert (which_partition < NumPartitions, \"selected free partition must be valid\");\n+  idx_t idx = _leftmosts[int(which_partition)];\n+  if (idx >= _max) {\n+    return _max;\n+  } else {\n+    \/\/ Cannot assert that membership[which_partition.is_set(idx) because this helper method may be used\n+    \/\/ to query the original value of leftmost when leftmost must be adjusted because the interval representing\n+    \/\/ which_partition is shrinking after the region that used to be leftmost is retired.\n+    return idx;\n@@ -68,0 +151,1 @@\n+}\n@@ -69,3 +153,8 @@\n-  _left_to_right_bias[Mutator] = true;\n-  _left_to_right_bias[Collector] = false;\n-  _left_to_right_bias[OldCollector] = false;\n+inline idx_t ShenandoahRegionPartitions::rightmost(ShenandoahFreeSetPartitionId which_partition) const {\n+  assert (which_partition < NumPartitions, \"selected free partition must be valid\");\n+  idx_t idx = _rightmosts[int(which_partition)];\n+  \/\/ Cannot assert that membership[which_partition.is_set(idx) because this helper method may be used\n+  \/\/ to query the original value of leftmost when leftmost must be adjusted because the interval representing\n+  \/\/ which_partition is shrinking after the region that used to be leftmost is retired.\n+  return idx;\n+}\n@@ -73,4 +162,11 @@\n-  _region_counts[Mutator] = 0;\n-  _region_counts[Collector] = 0;\n-  _region_counts[OldCollector] = 0;\n-  _region_counts[NotFree] = _max;\n+void ShenandoahRegionPartitions::make_all_regions_unavailable() {\n+  for (size_t partition_id = 0; partition_id < IntNumPartitions; partition_id++) {\n+    _membership[partition_id].clear_all();\n+    _leftmosts[partition_id] = _max;\n+    _rightmosts[partition_id] = -1;\n+    _leftmosts_empty[partition_id] = _max;\n+    _rightmosts_empty[partition_id] = -1;;\n+    _capacity[partition_id] = 0;\n+    _used[partition_id] = 0;\n+  }\n+  _region_counts[int(ShenandoahFreeSetPartitionId::Mutator)] = _region_counts[int(ShenandoahFreeSetPartitionId::Collector)] = 0;\n@@ -79,2 +175,20 @@\n-void ShenandoahSetsOfFree::clear_all() {\n-  clear_internal();\n+void ShenandoahRegionPartitions::establish_mutator_intervals(idx_t mutator_leftmost, idx_t mutator_rightmost,\n+                                                             idx_t mutator_leftmost_empty, idx_t mutator_rightmost_empty,\n+                                                             size_t mutator_region_count, size_t mutator_used) {\n+  _leftmosts[int(ShenandoahFreeSetPartitionId::Mutator)] = mutator_leftmost;\n+  _rightmosts[int(ShenandoahFreeSetPartitionId::Mutator)] = mutator_rightmost;\n+  _leftmosts_empty[int(ShenandoahFreeSetPartitionId::Mutator)] = mutator_leftmost_empty;\n+  _rightmosts_empty[int(ShenandoahFreeSetPartitionId::Mutator)] = mutator_rightmost_empty;\n+\n+  _region_counts[int(ShenandoahFreeSetPartitionId::Mutator)] = mutator_region_count;\n+  _used[int(ShenandoahFreeSetPartitionId::Mutator)] = mutator_used;\n+  _capacity[int(ShenandoahFreeSetPartitionId::Mutator)] = mutator_region_count * _region_size_bytes;\n+\n+  _leftmosts[int(ShenandoahFreeSetPartitionId::Collector)] = _max;\n+  _rightmosts[int(ShenandoahFreeSetPartitionId::Collector)] = -1;\n+  _leftmosts_empty[int(ShenandoahFreeSetPartitionId::Collector)] = _max;\n+  _rightmosts_empty[int(ShenandoahFreeSetPartitionId::Collector)] = -1;\n+\n+  _region_counts[int(ShenandoahFreeSetPartitionId::Collector)] = 0;\n+  _used[int(ShenandoahFreeSetPartitionId::Collector)] = 0;\n+  _capacity[int(ShenandoahFreeSetPartitionId::Collector)] = 0;\n@@ -83,4 +197,18 @@\n-void ShenandoahSetsOfFree::increase_used(ShenandoahFreeMemoryType which_set, size_t bytes) {\n-  assert (which_set > NotFree && which_set < NumFreeSets, \"Set must correspond to a valid freeset\");\n-  _used_by[which_set] += bytes;\n-  assert (_used_by[which_set] <= _capacity_of[which_set],\n+void ShenandoahRegionPartitions::establish_old_collector_intervals(idx_t old_collector_leftmost, idx_t old_collector_rightmost,\n+                                                                   idx_t old_collector_leftmost_empty,\n+                                                                   idx_t old_collector_rightmost_empty,\n+                                                                   size_t old_collector_region_count, size_t old_collector_used) {\n+  _leftmosts[int(ShenandoahFreeSetPartitionId::OldCollector)] = old_collector_leftmost;\n+  _rightmosts[int(ShenandoahFreeSetPartitionId::OldCollector)] = old_collector_rightmost;\n+  _leftmosts_empty[int(ShenandoahFreeSetPartitionId::OldCollector)] = old_collector_leftmost_empty;\n+  _rightmosts_empty[int(ShenandoahFreeSetPartitionId::OldCollector)] = old_collector_rightmost_empty;\n+\n+  _region_counts[int(ShenandoahFreeSetPartitionId::OldCollector)] = old_collector_region_count;\n+  _used[int(ShenandoahFreeSetPartitionId::OldCollector)] = old_collector_used;\n+  _capacity[int(ShenandoahFreeSetPartitionId::OldCollector)] = old_collector_region_count * _region_size_bytes;\n+}\n+\n+void ShenandoahRegionPartitions::increase_used(ShenandoahFreeSetPartitionId which_partition, size_t bytes) {\n+  assert (which_partition < NumPartitions, \"Partition must be valid\");\n+  _used[int(which_partition)] += bytes;\n+  assert (_used[int(which_partition)] <= _capacity[int(which_partition)],\n@@ -88,1 +216,1 @@\n-          _used_by[which_set], _capacity_of[which_set], bytes);\n+          _used[int(which_partition)], _capacity[int(which_partition)], bytes);\n@@ -91,4 +219,9 @@\n-inline void ShenandoahSetsOfFree::shrink_bounds_if_touched(ShenandoahFreeMemoryType set, size_t idx) {\n-  if (idx == _leftmosts[set]) {\n-    while ((_leftmosts[set] < _max) && !in_free_set(_leftmosts[set], set)) {\n-      _leftmosts[set]++;\n+inline void ShenandoahRegionPartitions::shrink_interval_if_range_modifies_either_boundary(\n+  ShenandoahFreeSetPartitionId partition, idx_t low_idx, idx_t high_idx) {\n+  assert((low_idx <= high_idx) && (low_idx >= 0) && (high_idx < _max), \"Range must span legal index values\");\n+  if (low_idx == leftmost(partition)) {\n+    assert (!_membership[int(partition)].is_set(low_idx), \"Do not shrink interval if region not removed\");\n+    if (high_idx + 1 == _max) {\n+      _leftmosts[int(partition)] = _max;\n+    } else {\n+      _leftmosts[int(partition)] = find_index_of_next_available_region(partition, high_idx + 1);\n@@ -96,1 +229,1 @@\n-    if (_leftmosts_empty[set] < _leftmosts[set]) {\n+    if (_leftmosts_empty[int(partition)] < _leftmosts[int(partition)]) {\n@@ -98,1 +231,1 @@\n-      _leftmosts_empty[set] = _leftmosts[set];\n+      _leftmosts_empty[int(partition)] = _leftmosts[int(partition)];\n@@ -101,3 +234,6 @@\n-  if (idx == _rightmosts[set]) {\n-    while (_rightmosts[set] > 0 && !in_free_set(_rightmosts[set], set)) {\n-      _rightmosts[set]--;\n+  if (high_idx == _rightmosts[int(partition)]) {\n+    assert (!_membership[int(partition)].is_set(high_idx), \"Do not shrink interval if region not removed\");\n+    if (low_idx == 0) {\n+      _rightmosts[int(partition)] = -1;\n+    } else {\n+      _rightmosts[int(partition)] = find_index_of_previous_available_region(partition, low_idx - 1);\n@@ -105,1 +241,1 @@\n-    if (_rightmosts_empty[set] > _rightmosts[set]) {\n+    if (_rightmosts_empty[int(partition)] > _rightmosts[int(partition)]) {\n@@ -107,1 +243,1 @@\n-      _rightmosts_empty[set] = _rightmosts[set];\n+      _rightmosts_empty[int(partition)] = _rightmosts[int(partition)];\n@@ -110,0 +246,10 @@\n+  if (_leftmosts[int(partition)] > _rightmosts[int(partition)]) {\n+    _leftmosts[int(partition)] = _max;\n+    _rightmosts[int(partition)] = -1;\n+    _leftmosts_empty[int(partition)] = _max;\n+    _rightmosts_empty[int(partition)] = -1;\n+  }\n+}\n+\n+inline void ShenandoahRegionPartitions::shrink_interval_if_boundary_modified(ShenandoahFreeSetPartitionId partition, idx_t idx) {\n+  shrink_interval_if_range_modifies_either_boundary(partition, idx, idx);\n@@ -112,4 +258,11 @@\n-inline void ShenandoahSetsOfFree::expand_bounds_maybe(ShenandoahFreeMemoryType set, size_t idx, size_t region_capacity) {\n-  if (region_capacity == _region_size_bytes) {\n-    if (_leftmosts_empty[set] > idx) {\n-      _leftmosts_empty[set] = idx;\n+inline void ShenandoahRegionPartitions::expand_interval_if_boundary_modified(ShenandoahFreeSetPartitionId partition,\n+                                                                             idx_t idx, size_t region_available) {\n+  if (_leftmosts[int(partition)] > idx) {\n+    _leftmosts[int(partition)] = idx;\n+  }\n+  if (_rightmosts[int(partition)] < idx) {\n+    _rightmosts[int(partition)] = idx;\n+  }\n+  if (region_available == _region_size_bytes) {\n+    if (_leftmosts_empty[int(partition)] > idx) {\n+      _leftmosts_empty[int(partition)] = idx;\n@@ -117,2 +270,2 @@\n-    if (_rightmosts_empty[set] < idx) {\n-      _rightmosts_empty[set] = idx;\n+    if (_rightmosts_empty[int(partition)] < idx) {\n+      _rightmosts_empty[int(partition)] = idx;\n@@ -121,2 +274,13 @@\n-  if (_leftmosts[set] > idx) {\n-    _leftmosts[set] = idx;\n+}\n+\n+void ShenandoahRegionPartitions::retire_range_from_partition(\n+  ShenandoahFreeSetPartitionId partition, idx_t low_idx, idx_t high_idx) {\n+\n+  \/\/ Note: we may remove from free partition even if region is not entirely full, such as when available < PLAB::min_size()\n+  assert ((low_idx < _max) && (high_idx < _max), \"Both indices are sane: \" SIZE_FORMAT \" and \" SIZE_FORMAT \" < \" SIZE_FORMAT,\n+          low_idx, high_idx, _max);\n+  assert (partition < NumPartitions, \"Cannot remove from free partitions if not already free\");\n+\n+  for (idx_t idx = low_idx; idx <= high_idx; idx++) {\n+    assert (in_free_set(partition, idx), \"Must be in partition to remove from partition\");\n+    _membership[int(partition)].clear_bit(idx);\n@@ -124,2 +288,14 @@\n-  if (_rightmosts[set] < idx) {\n-    _rightmosts[set] = idx;\n+  _region_counts[int(partition)] -= high_idx + 1 - low_idx;\n+  shrink_interval_if_range_modifies_either_boundary(partition, low_idx, high_idx);\n+}\n+\n+void ShenandoahRegionPartitions::retire_from_partition(ShenandoahFreeSetPartitionId partition, idx_t idx, size_t used_bytes) {\n+\n+  \/\/ Note: we may remove from free partition even if region is not entirely full, such as when available < PLAB::min_size()\n+  assert (idx < _max, \"index is sane: \" SIZE_FORMAT \" < \" SIZE_FORMAT, idx, _max);\n+  assert (partition < NumPartitions, \"Cannot remove from free partitions if not already free\");\n+  assert (in_free_set(partition, idx), \"Must be in partition to remove from partition\");\n+\n+  if (used_bytes < _region_size_bytes) {\n+    \/\/ Count the alignment pad remnant of memory as used when we retire this region\n+    increase_used(partition, _region_size_bytes - used_bytes);\n@@ -127,0 +303,3 @@\n+  _membership[int(partition)].clear_bit(idx);\n+  shrink_interval_if_boundary_modified(partition, idx);\n+  _region_counts[int(partition)]--;\n@@ -129,1 +308,1 @@\n-void ShenandoahSetsOfFree::remove_from_free_sets(size_t idx) {\n+void ShenandoahRegionPartitions::make_free(idx_t idx, ShenandoahFreeSetPartitionId which_partition, size_t available) {\n@@ -131,4 +310,10 @@\n-  ShenandoahFreeMemoryType orig_set = membership(idx);\n-  assert (orig_set > NotFree && orig_set < NumFreeSets, \"Cannot remove from free sets if not already free\");\n-  _membership[idx] = NotFree;\n-  shrink_bounds_if_touched(orig_set, idx);\n+  assert (membership(idx) == ShenandoahFreeSetPartitionId::NotFree, \"Cannot make free if already free\");\n+  assert (which_partition < NumPartitions, \"selected free partition must be valid\");\n+  assert (available <= _region_size_bytes, \"Available cannot exceed region size\");\n+\n+  _membership[int(which_partition)].set_bit(idx);\n+  _capacity[int(which_partition)] += _region_size_bytes;\n+  _used[int(which_partition)] += _region_size_bytes - available;\n+  expand_interval_if_boundary_modified(which_partition, idx, available);\n+  _region_counts[int(which_partition)]++;\n+}\n@@ -136,2 +321,2 @@\n-  _region_counts[orig_set]--;\n-  _region_counts[NotFree]++;\n+bool ShenandoahRegionPartitions::is_mutator_partition(ShenandoahFreeSetPartitionId p) {\n+  return (p == ShenandoahFreeSetPartitionId::Mutator);\n@@ -140,0 +325,3 @@\n+bool ShenandoahRegionPartitions::is_young_collector_partition(ShenandoahFreeSetPartitionId p) {\n+  return (p == ShenandoahFreeSetPartitionId::Collector);\n+}\n@@ -141,7 +329,3 @@\n-void ShenandoahSetsOfFree::make_free(size_t idx, ShenandoahFreeMemoryType which_set, size_t region_capacity) {\n-  assert (idx < _max, \"index is sane: \" SIZE_FORMAT \" < \" SIZE_FORMAT, idx, _max);\n-  assert (_membership[idx] == NotFree, \"Cannot make free if already free\");\n-  assert (which_set > NotFree && which_set < NumFreeSets, \"selected free set must be valid\");\n-  _membership[idx] = which_set;\n-  _capacity_of[which_set] += region_capacity;\n-  expand_bounds_maybe(which_set, idx, region_capacity);\n+bool ShenandoahRegionPartitions::is_old_collector_partition(ShenandoahFreeSetPartitionId p) {\n+  return (p == ShenandoahFreeSetPartitionId::OldCollector);\n+}\n@@ -149,2 +333,2 @@\n-  _region_counts[NotFree]--;\n-  _region_counts[which_set]++;\n+bool ShenandoahRegionPartitions::available_implies_empty(size_t available_in_region) {\n+  return (available_in_region == _region_size_bytes);\n@@ -153,1 +337,4 @@\n-void ShenandoahSetsOfFree::move_to_set(size_t idx, ShenandoahFreeMemoryType new_set, size_t region_capacity) {\n+\n+void ShenandoahRegionPartitions::move_from_partition_to_partition(idx_t idx, ShenandoahFreeSetPartitionId orig_partition,\n+                                                                  ShenandoahFreeSetPartitionId new_partition, size_t available) {\n+  ShenandoahHeapRegion* r = ShenandoahHeap::heap()->get_region(idx);\n@@ -155,3 +342,9 @@\n-  assert ((new_set > NotFree) && (new_set < NumFreeSets), \"New set must be valid\");\n-  ShenandoahFreeMemoryType orig_set = _membership[idx];\n-  assert ((orig_set > NotFree) && (orig_set < NumFreeSets), \"Cannot move free unless already free\");\n+  assert (orig_partition < NumPartitions, \"Original partition must be valid\");\n+  assert (new_partition < NumPartitions, \"New partition must be valid\");\n+  assert (available <= _region_size_bytes, \"Available cannot exceed region size\");\n+  assert (_membership[int(orig_partition)].is_set(idx), \"Cannot move from partition unless in partition\");\n+  assert ((r != nullptr) && ((r->is_trash() && (available == _region_size_bytes)) ||\n+                             (r->used() + available == _region_size_bytes)),\n+          \"Used: \" SIZE_FORMAT \" + available: \" SIZE_FORMAT \" should equal region size: \" SIZE_FORMAT,\n+          ShenandoahHeap::heap()->get_region(idx)->used(), available, _region_size_bytes);\n+\n@@ -159,26 +352,35 @@\n-  \/\/  During rebuild: Mutator => Collector\n-  \/\/                  Mutator empty => Collector\n-  \/\/  During flip_to_gc:\n-  \/\/                  Mutator empty => Collector\n-  \/\/                  Mutator empty => Old Collector\n-  \/\/ At start of update refs:\n-  \/\/                  Collector => Mutator\n-  \/\/                  OldCollector Empty => Mutator\n-  assert((region_capacity <= _region_size_bytes && ((orig_set == Mutator && new_set == Collector) || (orig_set == Collector && new_set == Mutator)))\n-      || (region_capacity == _region_size_bytes && ((orig_set == Mutator && new_set == Collector) || (orig_set == OldCollector && new_set == Mutator) || new_set == OldCollector)),\n-      \"Unexpected movement between sets\");\n-\n-  _membership[idx] = new_set;\n-  _capacity_of[orig_set] -= region_capacity;\n-  shrink_bounds_if_touched(orig_set, idx);\n-\n-  _capacity_of[new_set] += region_capacity;\n-  expand_bounds_maybe(new_set, idx, region_capacity);\n-\n-  _region_counts[orig_set]--;\n-  _region_counts[new_set]++;\n-}\n-\n-inline ShenandoahFreeMemoryType ShenandoahSetsOfFree::membership(size_t idx) const {\n-  assert (idx < _max, \"index is sane: \" SIZE_FORMAT \" < \" SIZE_FORMAT, idx, _max);\n-  return _membership[idx];\n+  \/\/  During rebuild:         Mutator => Collector\n+  \/\/                          Mutator empty => Collector\n+  \/\/                          Mutator empty => OldCollector\n+  \/\/  During flip_to_gc:      Mutator empty => Collector\n+  \/\/                          Mutator empty => OldCollector\n+  \/\/ At start of update refs: Collector => Mutator\n+  \/\/                          OldCollector Empty => Mutator\n+  assert ((is_mutator_partition(orig_partition) && is_young_collector_partition(new_partition)) ||\n+          (is_mutator_partition(orig_partition) &&\n+           available_implies_empty(available) && is_old_collector_partition(new_partition)) ||\n+          (is_young_collector_partition(orig_partition) && is_mutator_partition(new_partition)) ||\n+          (is_old_collector_partition(orig_partition)\n+           && available_implies_empty(available) && is_mutator_partition(new_partition)),\n+          \"Unexpected movement between partitions, available: \" SIZE_FORMAT \", _region_size_bytes: \" SIZE_FORMAT\n+          \", orig_partition: %s, new_partition: %s\",\n+          available, _region_size_bytes, partition_name(orig_partition), partition_name(new_partition));\n+\n+  size_t used = _region_size_bytes - available;\n+  assert (_used[int(orig_partition)] >= used,\n+          \"Orig partition used: \" SIZE_FORMAT \" must exceed moved used: \" SIZE_FORMAT \" within region \" SSIZE_FORMAT,\n+          _used[int(orig_partition)], used, idx);\n+\n+  _membership[int(orig_partition)].clear_bit(idx);\n+  _membership[int(new_partition)].set_bit(idx);\n+\n+  _capacity[int(orig_partition)] -= _region_size_bytes;\n+  _used[int(orig_partition)] -= used;\n+  shrink_interval_if_boundary_modified(orig_partition, idx);\n+\n+  _capacity[int(new_partition)] += _region_size_bytes;;\n+  _used[int(new_partition)] += used;\n+  expand_interval_if_boundary_modified(new_partition, idx, available);\n+\n+  _region_counts[int(orig_partition)]--;\n+  _region_counts[int(new_partition)]++;\n@@ -187,10 +389,2 @@\n-  \/\/ Returns true iff region idx is in the test_set free_set.  Before returning true, asserts that the free\n-  \/\/ set is not empty.  Requires that test_set != NotFree or NumFreeSets.\n-inline bool ShenandoahSetsOfFree::in_free_set(size_t idx, ShenandoahFreeMemoryType test_set) const {\n-  assert (idx < _max, \"index is sane: \" SIZE_FORMAT \" < \" SIZE_FORMAT, idx, _max);\n-  if (_membership[idx] == test_set) {\n-    assert (test_set == NotFree || _free_set->alloc_capacity(idx) > 0, \"Free regions must have alloc capacity\");\n-    return true;\n-  } else {\n-    return false;\n-  }\n+const char* ShenandoahRegionPartitions::partition_membership_name(idx_t idx) const {\n+  return partition_name(membership(idx));\n@@ -199,8 +393,8 @@\n-inline size_t ShenandoahSetsOfFree::leftmost(ShenandoahFreeMemoryType which_set) const {\n-  assert (which_set > NotFree && which_set < NumFreeSets, \"selected free set must be valid\");\n-  size_t idx = _leftmosts[which_set];\n-  if (idx >= _max) {\n-    return _max;\n-  } else {\n-    assert (in_free_set(idx, which_set), \"left-most region must be free\");\n-    return idx;\n+inline ShenandoahFreeSetPartitionId ShenandoahRegionPartitions::membership(idx_t idx) const {\n+  assert (idx < _max, \"index is sane: \" SIZE_FORMAT \" < \" SIZE_FORMAT, idx, _max);\n+  ShenandoahFreeSetPartitionId result = ShenandoahFreeSetPartitionId::NotFree;\n+  for (uint partition_id = 0; partition_id < UIntNumPartitions; partition_id++) {\n+    if (_membership[partition_id].is_set(idx)) {\n+      assert(result == ShenandoahFreeSetPartitionId::NotFree, \"Region should reside in only one partition\");\n+      result = (ShenandoahFreeSetPartitionId) partition_id;\n+    }\n@@ -208,0 +402,1 @@\n+  return result;\n@@ -210,5 +405,6 @@\n-inline size_t ShenandoahSetsOfFree::rightmost(ShenandoahFreeMemoryType which_set) const {\n-  assert (which_set > NotFree && which_set < NumFreeSets, \"selected free set must be valid\");\n-  size_t idx = _rightmosts[which_set];\n-  assert ((_leftmosts[which_set] == _max) || in_free_set(idx, which_set), \"right-most region must be free\");\n-  return idx;\n+#ifdef ASSERT\n+inline bool ShenandoahRegionPartitions::partition_id_matches(idx_t idx, ShenandoahFreeSetPartitionId test_partition) const {\n+  assert (idx < _max, \"index is sane: \" SIZE_FORMAT \" < \" SIZE_FORMAT, idx, _max);\n+  assert (test_partition < ShenandoahFreeSetPartitionId::NotFree, \"must be a valid partition\");\n+\n+  return membership(idx) == test_partition;\n@@ -216,0 +412,1 @@\n+#endif\n@@ -217,3 +414,3 @@\n-inline bool ShenandoahSetsOfFree::is_empty(ShenandoahFreeMemoryType which_set) const {\n-  assert (which_set > NotFree && which_set < NumFreeSets, \"selected free set must be valid\");\n-  return (leftmost(which_set) > rightmost(which_set));\n+inline bool ShenandoahRegionPartitions::is_empty(ShenandoahFreeSetPartitionId which_partition) const {\n+  assert (which_partition < NumPartitions, \"selected free partition must be valid\");\n+  return (leftmost(which_partition) > rightmost(which_partition));\n@@ -222,7 +419,7 @@\n-size_t ShenandoahSetsOfFree::leftmost_empty(ShenandoahFreeMemoryType which_set) {\n-  assert (which_set > NotFree && which_set < NumFreeSets, \"selected free set must be valid\");\n-  for (size_t idx = _leftmosts_empty[which_set]; idx < _max; idx++) {\n-    if ((membership(idx) == which_set) && (_free_set->alloc_capacity(idx) == _region_size_bytes)) {\n-      _leftmosts_empty[which_set] = idx;\n-      return idx;\n-    }\n+inline idx_t ShenandoahRegionPartitions::find_index_of_next_available_region(\n+  ShenandoahFreeSetPartitionId which_partition, idx_t start_index) const {\n+  idx_t rightmost_idx = rightmost(which_partition);\n+  idx_t leftmost_idx = leftmost(which_partition);\n+  if ((rightmost_idx < leftmost_idx) || (start_index > rightmost_idx)) return _max;\n+  if (start_index < leftmost_idx) {\n+    start_index = leftmost_idx;\n@@ -230,3 +427,6 @@\n-  _leftmosts_empty[which_set] = _max;\n-  _rightmosts_empty[which_set] = 0;\n-  return _max;\n+  idx_t result = _membership[int(which_partition)].find_first_set_bit(start_index, rightmost_idx + 1);\n+  if (result > rightmost_idx) {\n+    result = _max;\n+  }\n+  assert (result >= start_index, \"Requires progress\");\n+  return result;\n@@ -235,7 +435,8 @@\n-inline size_t ShenandoahSetsOfFree::rightmost_empty(ShenandoahFreeMemoryType which_set) {\n-  assert (which_set > NotFree && which_set < NumFreeSets, \"selected free set must be valid\");\n-  for (intptr_t idx = _rightmosts_empty[which_set]; idx >= 0; idx--) {\n-    if ((membership(idx) == which_set) && (_free_set->alloc_capacity(idx) == _region_size_bytes)) {\n-      _rightmosts_empty[which_set] = idx;\n-      return idx;\n-    }\n+inline idx_t ShenandoahRegionPartitions::find_index_of_previous_available_region(\n+  ShenandoahFreeSetPartitionId which_partition, idx_t last_index) const {\n+  idx_t rightmost_idx = rightmost(which_partition);\n+  idx_t leftmost_idx = leftmost(which_partition);\n+  \/\/ if (leftmost_idx == max) then (last_index < leftmost_idx)\n+  if (last_index < leftmost_idx) return -1;\n+  if (last_index > rightmost_idx) {\n+    last_index = rightmost_idx;\n@@ -243,3 +444,6 @@\n-  _leftmosts_empty[which_set] = _max;\n-  _rightmosts_empty[which_set] = 0;\n-  return 0;\n+  idx_t result = _membership[int(which_partition)].find_last_set_bit(-1, last_index);\n+  if (result < leftmost_idx) {\n+    result = -1;\n+  }\n+  assert (result <= last_index, \"Requires progress\");\n+  return result;\n@@ -248,3 +452,11 @@\n-inline bool ShenandoahSetsOfFree::alloc_from_left_bias(ShenandoahFreeMemoryType which_set) {\n-  assert (which_set > NotFree && which_set < NumFreeSets, \"selected free set must be valid\");\n-  return _left_to_right_bias[which_set];\n+inline idx_t ShenandoahRegionPartitions::find_index_of_next_available_cluster_of_regions(\n+  ShenandoahFreeSetPartitionId which_partition, idx_t start_index, size_t cluster_size) const {\n+  idx_t rightmost_idx = rightmost(which_partition);\n+  idx_t leftmost_idx = leftmost(which_partition);\n+  if ((rightmost_idx < leftmost_idx) || (start_index > rightmost_idx)) return _max;\n+  idx_t result = _membership[int(which_partition)].find_first_consecutive_set_bits(start_index, rightmost_idx + 1, cluster_size);\n+  if (result > rightmost_idx) {\n+    result = _max;\n+  }\n+  assert (result >= start_index, \"Requires progress\");\n+  return result;\n@@ -253,8 +465,12 @@\n-void ShenandoahSetsOfFree::establish_alloc_bias(ShenandoahFreeMemoryType which_set) {\n-  ShenandoahHeap* heap = ShenandoahHeap::heap();\n-  shenandoah_assert_heaplocked();\n-  assert (which_set > NotFree && which_set < NumFreeSets, \"selected free set must be valid\");\n-\n-  size_t middle = (_leftmosts[which_set] + _rightmosts[which_set]) \/ 2;\n-  size_t available_in_first_half = 0;\n-  size_t available_in_second_half = 0;\n+inline idx_t ShenandoahRegionPartitions::find_index_of_previous_available_cluster_of_regions(\n+  ShenandoahFreeSetPartitionId which_partition, idx_t last_index, size_t cluster_size) const {\n+  idx_t leftmost_idx = leftmost(which_partition);\n+  \/\/ if (leftmost_idx == max) then (last_index < leftmost_idx)\n+  if (last_index < leftmost_idx) return -1;\n+  idx_t result = _membership[int(which_partition)].find_last_consecutive_set_bits(leftmost_idx - 1, last_index, cluster_size);\n+  if (result <= leftmost_idx) {\n+    result = -1;\n+  }\n+  assert (result <= last_index, \"Requires progress\");\n+  return result;\n+}\n@@ -262,5 +478,5 @@\n-  for (size_t index = _leftmosts[which_set]; index < middle; index++) {\n-    if (in_free_set(index, which_set)) {\n-      ShenandoahHeapRegion* r = heap->get_region(index);\n-      available_in_first_half += r->free();\n-    }\n+idx_t ShenandoahRegionPartitions::leftmost_empty(ShenandoahFreeSetPartitionId which_partition) {\n+  assert (which_partition < NumPartitions, \"selected free partition must be valid\");\n+  idx_t max_regions = _max;\n+  if (_leftmosts_empty[int(which_partition)] == _max) {\n+    return _max;\n@@ -268,4 +484,6 @@\n-  for (size_t index = middle; index <= _rightmosts[which_set]; index++) {\n-    if (in_free_set(index, which_set)) {\n-      ShenandoahHeapRegion* r = heap->get_region(index);\n-      available_in_second_half += r->free();\n+  for (idx_t idx = find_index_of_next_available_region(which_partition, _leftmosts_empty[int(which_partition)]);\n+       idx < max_regions; ) {\n+    assert(in_free_set(which_partition, idx), \"Boundaries or find_last_set_bit failed: \" SSIZE_FORMAT, idx);\n+    if (_free_set->alloc_capacity(idx) == _region_size_bytes) {\n+      _leftmosts_empty[int(which_partition)] = idx;\n+      return idx;\n@@ -273,0 +491,1 @@\n+    idx = find_index_of_next_available_region(which_partition, idx + 1);\n@@ -274,0 +493,4 @@\n+  _leftmosts_empty[int(which_partition)] = _max;\n+  _rightmosts_empty[int(which_partition)] = -1;\n+  return _max;\n+}\n@@ -275,8 +498,17 @@\n-  \/\/ We desire to first consume the sparsely distributed regions in order that the remaining regions are densely packed.\n-  \/\/ Densely packing regions reduces the effort to search for a region that has sufficient memory to satisfy a new allocation\n-  \/\/ request.  Regions become sparsely distributed following a Full GC, which tends to slide all regions to the front of the\n-  \/\/ heap rather than allowing survivor regions to remain at the high end of the heap where we intend for them to congregate.\n-\n-  \/\/ TODO: In the future, we may modify Full GC so that it slides old objects to the end of the heap and young objects to the\n-  \/\/ front of the heap. If this is done, we can always search survivor Collector and OldCollector regions right to left.\n-  _left_to_right_bias[which_set] = (available_in_second_half > available_in_first_half);\n+idx_t ShenandoahRegionPartitions::rightmost_empty(ShenandoahFreeSetPartitionId which_partition) {\n+  assert (which_partition < NumPartitions, \"selected free partition must be valid\");\n+  if (_rightmosts_empty[int(which_partition)] < 0) {\n+    return -1;\n+  }\n+  for (idx_t idx = find_index_of_previous_available_region(which_partition, _rightmosts_empty[int(which_partition)]);\n+       idx >= 0; ) {\n+    assert(in_free_set(which_partition, idx), \"Boundaries or find_last_set_bit failed: \" SSIZE_FORMAT, idx);\n+    if (_free_set->alloc_capacity(idx) == _region_size_bytes) {\n+      _rightmosts_empty[int(which_partition)] = idx;\n+      return idx;\n+    }\n+    idx = find_index_of_previous_available_region(which_partition, idx - 1);\n+  }\n+  _leftmosts_empty[int(which_partition)] = _max;\n+  _rightmosts_empty[int(which_partition)] = -1;\n+  return -1;\n@@ -285,0 +517,1 @@\n+\n@@ -286,1 +519,1 @@\n-void ShenandoahSetsOfFree::assert_bounds() {\n+void ShenandoahRegionPartitions::assert_bounds() {\n@@ -288,4 +521,4 @@\n-  size_t leftmosts[NumFreeSets];\n-  size_t rightmosts[NumFreeSets];\n-  size_t empty_leftmosts[NumFreeSets];\n-  size_t empty_rightmosts[NumFreeSets];\n+  idx_t leftmosts[UIntNumPartitions];\n+  idx_t rightmosts[UIntNumPartitions];\n+  idx_t empty_leftmosts[UIntNumPartitions];\n+  idx_t empty_rightmosts[UIntNumPartitions];\n@@ -293,1 +526,1 @@\n-  for (int i = 0; i < NumFreeSets; i++) {\n+  for (uint i = 0; i < UIntNumPartitions; i++) {\n@@ -296,2 +529,2 @@\n-    rightmosts[i] = 0;\n-    empty_rightmosts[i] = 0;\n+    rightmosts[i] = -1;\n+    empty_rightmosts[i] = -1;\n@@ -300,4 +533,4 @@\n-  for (size_t i = 0; i < _max; i++) {\n-    ShenandoahFreeMemoryType set = membership(i);\n-    switch (set) {\n-      case NotFree:\n+  for (idx_t i = 0; i < _max; i++) {\n+    ShenandoahFreeSetPartitionId partition = membership(i);\n+    switch (partition) {\n+      case ShenandoahFreeSetPartitionId::NotFree:\n@@ -306,3 +539,3 @@\n-      case Mutator:\n-      case Collector:\n-      case OldCollector:\n+      case ShenandoahFreeSetPartitionId::Mutator:\n+      case ShenandoahFreeSetPartitionId::Collector:\n+      case ShenandoahFreeSetPartitionId::OldCollector:\n@@ -313,2 +546,2 @@\n-        if (i < leftmosts[set]) {\n-          leftmosts[set] = i;\n+        if (i < leftmosts[int(partition)]) {\n+          leftmosts[int(partition)] = i;\n@@ -316,2 +549,2 @@\n-        if (is_empty && (i < empty_leftmosts[set])) {\n-          empty_leftmosts[set] = i;\n+        if (is_empty && (i < empty_leftmosts[int(partition)])) {\n+          empty_leftmosts[int(partition)] = i;\n@@ -319,2 +552,2 @@\n-        if (i > rightmosts[set]) {\n-          rightmosts[set] = i;\n+        if (i > rightmosts[int(partition)]) {\n+          rightmosts[int(partition)] = i;\n@@ -322,2 +555,2 @@\n-        if (is_empty && (i > empty_rightmosts[set])) {\n-          empty_rightmosts[set] = i;\n+        if (is_empty && (i > empty_rightmosts[int(partition)])) {\n+          empty_rightmosts[int(partition)] = i;\n@@ -328,1 +561,0 @@\n-      case NumFreeSets:\n@@ -334,71 +566,100 @@\n-  \/\/ Performance invariants. Failing these would not break the free set, but performance would suffer.\n-  assert (leftmost(Mutator) <= _max, \"leftmost in bounds: \"  SIZE_FORMAT \" < \" SIZE_FORMAT, leftmost(Mutator),  _max);\n-  assert (rightmost(Mutator) < _max, \"rightmost in bounds: \"  SIZE_FORMAT \" < \" SIZE_FORMAT, rightmost(Mutator),  _max);\n-\n-  assert (leftmost(Mutator) == _max || in_free_set(leftmost(Mutator), Mutator),\n-          \"leftmost region should be free: \" SIZE_FORMAT,  leftmost(Mutator));\n-  assert (leftmost(Mutator) == _max || in_free_set(rightmost(Mutator), Mutator),\n-          \"rightmost region should be free: \" SIZE_FORMAT, rightmost(Mutator));\n-\n-  \/\/ If Mutator set is empty, leftmosts will both equal max, rightmosts will both equal zero.  Likewise for empty region sets.\n-  size_t beg_off = leftmosts[Mutator];\n-  size_t end_off = rightmosts[Mutator];\n-  assert (beg_off >= leftmost(Mutator),\n-          \"free regions before the leftmost: \" SIZE_FORMAT \", bound \" SIZE_FORMAT, beg_off, leftmost(Mutator));\n-  assert (end_off <= rightmost(Mutator),\n-          \"free regions past the rightmost: \" SIZE_FORMAT \", bound \" SIZE_FORMAT,  end_off, rightmost(Mutator));\n-\n-  beg_off = empty_leftmosts[Mutator];\n-  end_off = empty_rightmosts[Mutator];\n-  assert (beg_off >= leftmost_empty(Mutator),\n-          \"free empty regions before the leftmost: \" SIZE_FORMAT \", bound \" SIZE_FORMAT, beg_off, leftmost_empty(Mutator));\n-  assert (end_off <= rightmost_empty(Mutator),\n-          \"free empty regions past the rightmost: \" SIZE_FORMAT \", bound \" SIZE_FORMAT,  end_off, rightmost_empty(Mutator));\n-\n-  \/\/ Performance invariants. Failing these would not break the free set, but performance would suffer.\n-  assert (leftmost(Collector) <= _max, \"leftmost in bounds: \"  SIZE_FORMAT \" < \" SIZE_FORMAT, leftmost(Collector),  _max);\n-  assert (rightmost(Collector) < _max, \"rightmost in bounds: \"  SIZE_FORMAT \" < \" SIZE_FORMAT, rightmost(Collector),  _max);\n-\n-  assert (leftmost(Collector) == _max || in_free_set(leftmost(Collector), Collector),\n-          \"leftmost region should be free: \" SIZE_FORMAT,  leftmost(Collector));\n-  assert (leftmost(Collector) == _max || in_free_set(rightmost(Collector), Collector),\n-          \"rightmost region should be free: \" SIZE_FORMAT, rightmost(Collector));\n-\n-  \/\/ If Collector set is empty, leftmosts will both equal max, rightmosts will both equal zero.  Likewise for empty region sets.\n-  beg_off = leftmosts[Collector];\n-  end_off = rightmosts[Collector];\n-  assert (beg_off >= leftmost(Collector),\n-          \"free regions before the leftmost: \" SIZE_FORMAT \", bound \" SIZE_FORMAT, beg_off, leftmost(Collector));\n-  assert (end_off <= rightmost(Collector),\n-          \"free regions past the rightmost: \" SIZE_FORMAT \", bound \" SIZE_FORMAT,  end_off, rightmost(Collector));\n-\n-  beg_off = empty_leftmosts[Collector];\n-  end_off = empty_rightmosts[Collector];\n-  assert (beg_off >= leftmost_empty(Collector),\n-          \"free empty regions before the leftmost: \" SIZE_FORMAT \", bound \" SIZE_FORMAT, beg_off, leftmost_empty(Collector));\n-  assert (end_off <= rightmost_empty(Collector),\n-          \"free empty regions past the rightmost: \" SIZE_FORMAT \", bound \" SIZE_FORMAT,  end_off, rightmost_empty(Collector));\n-\n-  \/\/ Performance invariants. Failing these would not break the free set, but performance would suffer.\n-  assert (leftmost(OldCollector) <= _max, \"leftmost in bounds: \"  SIZE_FORMAT \" < \" SIZE_FORMAT, leftmost(OldCollector),  _max);\n-  assert (rightmost(OldCollector) < _max, \"rightmost in bounds: \"  SIZE_FORMAT \" < \" SIZE_FORMAT, rightmost(OldCollector),  _max);\n-\n-  assert (leftmost(OldCollector) == _max || in_free_set(leftmost(OldCollector), OldCollector),\n-          \"leftmost region should be free: \" SIZE_FORMAT,  leftmost(OldCollector));\n-  assert (leftmost(OldCollector) == _max || in_free_set(rightmost(OldCollector), OldCollector),\n-          \"rightmost region should be free: \" SIZE_FORMAT, rightmost(OldCollector));\n-\n-  \/\/ If OldCollector set is empty, leftmosts will both equal max, rightmosts will both equal zero.  Likewise for empty region sets.\n-  beg_off = leftmosts[OldCollector];\n-  end_off = rightmosts[OldCollector];\n-  assert (beg_off >= leftmost(OldCollector),\n-          \"free regions before the leftmost: \" SIZE_FORMAT \", bound \" SIZE_FORMAT, beg_off, leftmost(OldCollector));\n-  assert (end_off <= rightmost(OldCollector),\n-          \"free regions past the rightmost: \" SIZE_FORMAT \", bound \" SIZE_FORMAT,  end_off, rightmost(OldCollector));\n-\n-  beg_off = empty_leftmosts[OldCollector];\n-  end_off = empty_rightmosts[OldCollector];\n-  assert (beg_off >= leftmost_empty(OldCollector),\n-          \"free empty regions before the leftmost: \" SIZE_FORMAT \", bound \" SIZE_FORMAT, beg_off, leftmost_empty(OldCollector));\n-  assert (end_off <= rightmost_empty(OldCollector),\n-          \"free empty regions past the rightmost: \" SIZE_FORMAT \", bound \" SIZE_FORMAT,  end_off, rightmost_empty(OldCollector));\n+  \/\/ Performance invariants. Failing these would not break the free partition, but performance would suffer.\n+  assert (leftmost(ShenandoahFreeSetPartitionId::Mutator) <= _max,\n+          \"leftmost in bounds: \"  SSIZE_FORMAT \" < \" SSIZE_FORMAT, leftmost(ShenandoahFreeSetPartitionId::Mutator),  _max);\n+  assert (rightmost(ShenandoahFreeSetPartitionId::Mutator) < _max,\n+          \"rightmost in bounds: \"  SSIZE_FORMAT \" < \" SSIZE_FORMAT, rightmost(ShenandoahFreeSetPartitionId::Mutator),  _max);\n+\n+  assert (leftmost(ShenandoahFreeSetPartitionId::Mutator) == _max\n+          || partition_id_matches(leftmost(ShenandoahFreeSetPartitionId::Mutator), ShenandoahFreeSetPartitionId::Mutator),\n+          \"leftmost region should be free: \" SSIZE_FORMAT,  leftmost(ShenandoahFreeSetPartitionId::Mutator));\n+  assert (leftmost(ShenandoahFreeSetPartitionId::Mutator) == _max\n+          || partition_id_matches(rightmost(ShenandoahFreeSetPartitionId::Mutator), ShenandoahFreeSetPartitionId::Mutator),\n+          \"rightmost region should be free: \" SSIZE_FORMAT, rightmost(ShenandoahFreeSetPartitionId::Mutator));\n+\n+  \/\/ If Mutator partition is empty, leftmosts will both equal max, rightmosts will both equal zero.\n+  \/\/ Likewise for empty region partitions.\n+  idx_t beg_off = leftmosts[int(ShenandoahFreeSetPartitionId::Mutator)];\n+  idx_t end_off = rightmosts[int(ShenandoahFreeSetPartitionId::Mutator)];\n+  assert (beg_off >= leftmost(ShenandoahFreeSetPartitionId::Mutator),\n+          \"free regions before the leftmost: \" SSIZE_FORMAT \", bound \" SSIZE_FORMAT,\n+          beg_off, leftmost(ShenandoahFreeSetPartitionId::Mutator));\n+  assert (end_off <= rightmost(ShenandoahFreeSetPartitionId::Mutator),\n+          \"free regions past the rightmost: \" SSIZE_FORMAT \", bound \" SSIZE_FORMAT,\n+          end_off, rightmost(ShenandoahFreeSetPartitionId::Mutator));\n+\n+  beg_off = empty_leftmosts[int(ShenandoahFreeSetPartitionId::Mutator)];\n+  end_off = empty_rightmosts[int(ShenandoahFreeSetPartitionId::Mutator)];\n+  assert (beg_off >= leftmost_empty(ShenandoahFreeSetPartitionId::Mutator),\n+          \"free empty regions before the leftmost: \" SSIZE_FORMAT \", bound \" SSIZE_FORMAT,\n+          beg_off, leftmost_empty(ShenandoahFreeSetPartitionId::Mutator));\n+  assert (end_off <= rightmost_empty(ShenandoahFreeSetPartitionId::Mutator),\n+          \"free empty regions past the rightmost: \" SSIZE_FORMAT \", bound \" SSIZE_FORMAT,\n+          end_off, rightmost_empty(ShenandoahFreeSetPartitionId::Mutator));\n+\n+  \/\/ Performance invariants. Failing these would not break the free partition, but performance would suffer.\n+  assert (leftmost(ShenandoahFreeSetPartitionId::Collector) <= _max, \"leftmost in bounds: \"  SSIZE_FORMAT \" < \" SSIZE_FORMAT,\n+          leftmost(ShenandoahFreeSetPartitionId::Collector),  _max);\n+  assert (rightmost(ShenandoahFreeSetPartitionId::Collector) < _max, \"rightmost in bounds: \"  SSIZE_FORMAT \" < \" SSIZE_FORMAT,\n+          rightmost(ShenandoahFreeSetPartitionId::Collector),  _max);\n+\n+  assert (leftmost(ShenandoahFreeSetPartitionId::Collector) == _max\n+          || partition_id_matches(leftmost(ShenandoahFreeSetPartitionId::Collector), ShenandoahFreeSetPartitionId::Collector),\n+          \"leftmost region should be free: \" SSIZE_FORMAT,  leftmost(ShenandoahFreeSetPartitionId::Collector));\n+  assert (leftmost(ShenandoahFreeSetPartitionId::Collector) == _max\n+          || partition_id_matches(rightmost(ShenandoahFreeSetPartitionId::Collector), ShenandoahFreeSetPartitionId::Collector),\n+          \"rightmost region should be free: \" SSIZE_FORMAT, rightmost(ShenandoahFreeSetPartitionId::Collector));\n+\n+  \/\/ If Collector partition is empty, leftmosts will both equal max, rightmosts will both equal zero.\n+  \/\/ Likewise for empty region partitions.\n+  beg_off = leftmosts[int(ShenandoahFreeSetPartitionId::Collector)];\n+  end_off = rightmosts[int(ShenandoahFreeSetPartitionId::Collector)];\n+  assert (beg_off >= leftmost(ShenandoahFreeSetPartitionId::Collector),\n+          \"free regions before the leftmost: \" SSIZE_FORMAT \", bound \" SSIZE_FORMAT,\n+          beg_off, leftmost(ShenandoahFreeSetPartitionId::Collector));\n+  assert (end_off <= rightmost(ShenandoahFreeSetPartitionId::Collector),\n+          \"free regions past the rightmost: \" SSIZE_FORMAT \", bound \" SSIZE_FORMAT,\n+          end_off, rightmost(ShenandoahFreeSetPartitionId::Collector));\n+\n+  beg_off = empty_leftmosts[int(ShenandoahFreeSetPartitionId::Collector)];\n+  end_off = empty_rightmosts[int(ShenandoahFreeSetPartitionId::Collector)];\n+  assert (beg_off >= _leftmosts_empty[int(ShenandoahFreeSetPartitionId::Collector)],\n+          \"free empty regions before the leftmost: \" SSIZE_FORMAT \", bound \" SSIZE_FORMAT,\n+          beg_off, leftmost_empty(ShenandoahFreeSetPartitionId::Collector));\n+  assert (end_off <= _rightmosts_empty[int(ShenandoahFreeSetPartitionId::Collector)],\n+          \"free empty regions past the rightmost: \" SSIZE_FORMAT \", bound \" SSIZE_FORMAT,\n+          end_off, rightmost_empty(ShenandoahFreeSetPartitionId::Collector));\n+\n+  \/\/ Performance invariants. Failing these would not break the free partition, but performance would suffer.\n+  assert (leftmost(ShenandoahFreeSetPartitionId::OldCollector) <= _max, \"leftmost in bounds: \"  SSIZE_FORMAT \" < \" SSIZE_FORMAT,\n+          leftmost(ShenandoahFreeSetPartitionId::OldCollector),  _max);\n+  assert (rightmost(ShenandoahFreeSetPartitionId::OldCollector) < _max, \"rightmost in bounds: \"  SSIZE_FORMAT \" < \" SSIZE_FORMAT,\n+          rightmost(ShenandoahFreeSetPartitionId::OldCollector),  _max);\n+\n+  assert (leftmost(ShenandoahFreeSetPartitionId::OldCollector) == _max\n+          || partition_id_matches(leftmost(ShenandoahFreeSetPartitionId::OldCollector),\n+                                  ShenandoahFreeSetPartitionId::OldCollector),\n+          \"leftmost region should be free: \" SSIZE_FORMAT,  leftmost(ShenandoahFreeSetPartitionId::OldCollector));\n+  assert (leftmost(ShenandoahFreeSetPartitionId::OldCollector) == _max\n+          || partition_id_matches(rightmost(ShenandoahFreeSetPartitionId::OldCollector),\n+                                  ShenandoahFreeSetPartitionId::OldCollector),\n+          \"rightmost region should be free: \" SSIZE_FORMAT, rightmost(ShenandoahFreeSetPartitionId::OldCollector));\n+\n+  \/\/ If OldCollector partition is empty, leftmosts will both equal max, rightmosts will both equal zero.\n+  \/\/ Likewise for empty region partitions.\n+  beg_off = leftmosts[int(ShenandoahFreeSetPartitionId::OldCollector)];\n+  end_off = rightmosts[int(ShenandoahFreeSetPartitionId::OldCollector)];\n+  assert (beg_off >= leftmost(ShenandoahFreeSetPartitionId::OldCollector),\n+          \"free regions before the leftmost: \" SSIZE_FORMAT \", bound \" SSIZE_FORMAT,\n+          beg_off, leftmost(ShenandoahFreeSetPartitionId::OldCollector));\n+  assert (end_off <= rightmost(ShenandoahFreeSetPartitionId::OldCollector),\n+          \"free regions past the rightmost: \" SSIZE_FORMAT \", bound \" SSIZE_FORMAT,\n+          end_off, rightmost(ShenandoahFreeSetPartitionId::OldCollector));\n+\n+  beg_off = empty_leftmosts[int(ShenandoahFreeSetPartitionId::OldCollector)];\n+  end_off = empty_rightmosts[int(ShenandoahFreeSetPartitionId::OldCollector)];\n+  assert (beg_off >= _leftmosts_empty[int(ShenandoahFreeSetPartitionId::OldCollector)],\n+          \"free empty regions before the leftmost: \" SSIZE_FORMAT \", bound \" SSIZE_FORMAT,\n+          beg_off, leftmost_empty(ShenandoahFreeSetPartitionId::OldCollector));\n+  assert (end_off <= _rightmosts_empty[int(ShenandoahFreeSetPartitionId::OldCollector)],\n+          \"free empty regions past the rightmost: \" SSIZE_FORMAT \", bound \" SSIZE_FORMAT,\n+          end_off, rightmost_empty(ShenandoahFreeSetPartitionId::OldCollector));\n@@ -410,1 +671,2 @@\n-  _free_sets(max_regions, this)\n+  _partitions(max_regions, this),\n+  _alloc_bias_weight(0)\n@@ -415,46 +677,1 @@\n-\/\/ This allocates from a region within the old_collector_set.  If affiliation equals OLD, the allocation must be taken\n-\/\/ from a region that is_old().  Otherwise, affiliation should be FREE, in which case this will put a previously unaffiliated\n-\/\/ region into service.\n-HeapWord* ShenandoahFreeSet::allocate_old_with_affiliation(ShenandoahAffiliation affiliation,\n-                                                           ShenandoahAllocRequest& req, bool& in_new_region) {\n-  shenandoah_assert_heaplocked();\n-\n-  size_t rightmost =\n-    (affiliation == ShenandoahAffiliation::FREE)? _free_sets.rightmost_empty(OldCollector): _free_sets.rightmost(OldCollector);\n-  size_t leftmost =\n-    (affiliation == ShenandoahAffiliation::FREE)? _free_sets.leftmost_empty(OldCollector): _free_sets.leftmost(OldCollector);\n-  if (_free_sets.alloc_from_left_bias(OldCollector)) {\n-    \/\/ This mode picks up stragglers left by a full GC\n-    for (size_t idx = leftmost; idx <= rightmost; idx++) {\n-      if (_free_sets.in_free_set(idx, OldCollector)) {\n-        ShenandoahHeapRegion* r = _heap->get_region(idx);\n-        assert(r->is_trash() || !r->is_affiliated() || r->is_old(), \"old_collector_set region has bad affiliation\");\n-        if (r->affiliation() == affiliation) {\n-          HeapWord* result = try_allocate_in(r, req, in_new_region);\n-          if (result != nullptr) {\n-            return result;\n-          }\n-        }\n-      }\n-    }\n-  } else {\n-    \/\/ This mode picks up stragglers left by a previous concurrent GC\n-    for (size_t count = rightmost + 1; count > leftmost; count--) {\n-      \/\/ size_t is unsigned, need to dodge underflow when _leftmost = 0\n-      size_t idx = count - 1;\n-      if (_free_sets.in_free_set(idx, OldCollector)) {\n-        ShenandoahHeapRegion* r = _heap->get_region(idx);\n-        assert(r->is_trash() || !r->is_affiliated() || r->is_old(), \"old_collector_set region has bad affiliation\");\n-        if (r->affiliation() == affiliation) {\n-          HeapWord* result = try_allocate_in(r, req, in_new_region);\n-          if (result != nullptr) {\n-            return result;\n-          }\n-        }\n-      }\n-    }\n-  }\n-  return nullptr;\n-}\n-\n-void ShenandoahFreeSet::add_old_collector_free_region(ShenandoahHeapRegion* region) {\n+void ShenandoahFreeSet::add_promoted_in_place_region_to_old_collector(ShenandoahHeapRegion* region) {\n@@ -465,1 +682,2 @@\n-  assert(_free_sets.membership(idx) == NotFree, \"Regions promoted in place should not be in any free set\");\n+  assert(_partitions.membership(idx) == ShenandoahFreeSetPartitionId::NotFree,\n+         \"Regions promoted in place should have been excluded from Mutator partition\");\n@@ -467,1 +685,1 @@\n-    _free_sets.make_free(idx, OldCollector, capacity);\n+    _partitions.make_free(idx, ShenandoahFreeSetPartitionId::OldCollector, capacity);\n@@ -472,2 +690,3 @@\n-HeapWord* ShenandoahFreeSet::allocate_with_affiliation(ShenandoahAffiliation affiliation,\n-                                                       ShenandoahAllocRequest& req, bool& in_new_region) {\n+HeapWord* ShenandoahFreeSet::allocate_from_partition_with_affiliation(ShenandoahFreeSetPartitionId which_partition,\n+                                                                      ShenandoahAffiliation affiliation,\n+                                                                      ShenandoahAllocRequest& req, bool& in_new_region) {\n@@ -475,8 +694,7 @@\n-  size_t rightmost =\n-    (affiliation == ShenandoahAffiliation::FREE)? _free_sets.rightmost_empty(Collector): _free_sets.rightmost(Collector);\n-  size_t leftmost =\n-    (affiliation == ShenandoahAffiliation::FREE)? _free_sets.leftmost_empty(Collector): _free_sets.leftmost(Collector);\n-  for (size_t c = rightmost + 1; c > leftmost; c--) {\n-    \/\/ size_t is unsigned, need to dodge underflow when _leftmost = 0\n-    size_t idx = c - 1;\n-    if (_free_sets.in_free_set(idx, Collector)) {\n+  idx_t rightmost_collector = ((affiliation == ShenandoahAffiliation::FREE)?\n+                               _partitions.rightmost_empty(which_partition): _partitions.rightmost(which_partition));\n+  idx_t leftmost_collector = ((affiliation == ShenandoahAffiliation::FREE)?\n+                              _partitions.leftmost_empty(which_partition): _partitions.leftmost(which_partition));\n+  if (_partitions.alloc_from_left_bias(which_partition)) {\n+    for (idx_t idx = leftmost_collector; idx <= rightmost_collector; ) {\n+      assert(_partitions.in_free_set(which_partition, idx), \"Boundaries or find_prev_last_bit failed: \" SSIZE_FORMAT, idx);\n@@ -490,0 +708,14 @@\n+      idx = _partitions.find_index_of_next_available_region(which_partition, idx + 1);\n+    }\n+  } else {\n+    for (idx_t idx = rightmost_collector; idx >= leftmost_collector; ) {\n+      assert(_partitions.in_free_set(which_partition, idx),\n+             \"Boundaries or find_prev_last_bit failed: \" SSIZE_FORMAT, idx);\n+      ShenandoahHeapRegion* r = _heap->get_region(idx);\n+      if (r->affiliation() == affiliation) {\n+        HeapWord* result = try_allocate_in(r, req, in_new_region);\n+        if (result != nullptr) {\n+          return result;\n+        }\n+      }\n+      idx = _partitions.find_index_of_previous_available_region(which_partition, idx - 1);\n@@ -505,3 +737,3 @@\n-  \/\/ Allocations are biased: new application allocs go to beginning of the heap, and GC allocs\n-  \/\/ go to the end. This makes application allocation faster, because we would clear lots\n-  \/\/ of regions from the beginning most of the time.\n+  \/\/ Allocations are biased: GC allocations are taken from the high end of the heap.  Regular (and TLAB)\n+  \/\/ mutator allocations are taken from the middle of heap, below the memory reserved for Collector.\n+  \/\/ Humongous mutator allocations are taken from the bottom of the heap.\n@@ -509,2 +741,2 @@\n-  \/\/ Free set maintains mutator and collector views, and normally they allocate in their views only,\n-  \/\/ unless we special cases for stealing and mixed allocations.\n+  \/\/ Free set maintains mutator and collector partitions.  Normally, each allocates only from its partition,\n+  \/\/ except in special cases when the collector steals regions from the mutator partition.\n@@ -542,7 +774,32 @@\n-      \/\/ Allocate within mutator free from high memory to low so as to preserve low memory for humongous allocations\n-      if (!_free_sets.is_empty(Mutator)) {\n-        \/\/ Use signed idx.  Otherwise, loop will never terminate.\n-        int leftmost = (int) _free_sets.leftmost(Mutator);\n-        for (int idx = (int) _free_sets.rightmost(Mutator); idx >= leftmost; idx--) {\n-          ShenandoahHeapRegion* r = _heap->get_region(idx);\n-          if (_free_sets.in_free_set(idx, Mutator) && (allow_new_region || r->is_affiliated())) {\n+      if (_alloc_bias_weight-- <= 0) {\n+        \/\/ We have observed that regions not collected in previous GC cycle tend to congregate at one end or the other\n+        \/\/ of the heap.  Typically, these are the more recently engaged regions and the objects in these regions have not\n+        \/\/ yet had a chance to die (and\/or are treated as floating garbage).  If we use the same allocation bias on each\n+        \/\/ GC pass, these \"most recently\" engaged regions for GC pass N will also be the \"most recently\" engaged regions\n+        \/\/ for GC pass N+1, and the relatively large amount of live data and\/or floating garbage introduced\n+        \/\/ during the most recent GC pass may once again prevent the region from being collected.  We have found that\n+        \/\/ alternating the allocation behavior between GC passes improves evacuation performance by 3-7% on certain\n+        \/\/ benchmarks.  In the best case, this has the effect of consuming these partially consumed regions before\n+        \/\/ the start of the next mark cycle so all of their garbage can be efficiently reclaimed.\n+        \/\/\n+        \/\/ First, finish consuming regions that are already partially consumed so as to more tightly limit ranges of\n+        \/\/ available regions.  Other potential benefits:\n+        \/\/  1. Eventual collection set has fewer regions because we have packed newly allocated objects into fewer regions\n+        \/\/  2. We preserve the \"empty\" regions longer into the GC cycle, reducing likelihood of allocation failures\n+        \/\/     late in the GC cycle.\n+        idx_t non_empty_on_left = (_partitions.leftmost_empty(ShenandoahFreeSetPartitionId::Mutator)\n+                                     - _partitions.leftmost(ShenandoahFreeSetPartitionId::Mutator));\n+        idx_t non_empty_on_right = (_partitions.rightmost(ShenandoahFreeSetPartitionId::Mutator)\n+                                      - _partitions.rightmost_empty(ShenandoahFreeSetPartitionId::Mutator));\n+        _partitions.set_bias_from_left_to_right(ShenandoahFreeSetPartitionId::Mutator, (non_empty_on_right < non_empty_on_left));\n+        _alloc_bias_weight = _InitialAllocBiasWeight;\n+      }\n+      if (!_partitions.alloc_from_left_bias(ShenandoahFreeSetPartitionId::Mutator)) {\n+        \/\/ Allocate within mutator free from high memory to low so as to preserve low memory for humongous allocations\n+        if (!_partitions.is_empty(ShenandoahFreeSetPartitionId::Mutator)) {\n+          \/\/ Use signed idx.  Otherwise, loop will never terminate.\n+          idx_t leftmost = _partitions.leftmost(ShenandoahFreeSetPartitionId::Mutator);\n+          for (idx_t idx = _partitions.rightmost(ShenandoahFreeSetPartitionId::Mutator); idx >= leftmost; ) {\n+            assert(_partitions.in_free_set(ShenandoahFreeSetPartitionId::Mutator, idx),\n+                   \"Boundaries or find_last_set_bit failed: \" SSIZE_FORMAT, idx);\n+            ShenandoahHeapRegion* r = _heap->get_region(idx);\n@@ -555,0 +812,21 @@\n+            idx = _partitions.find_index_of_previous_available_region(ShenandoahFreeSetPartitionId::Mutator, idx - 1);\n+          }\n+        }\n+      } else {\n+        \/\/ Allocate from low to high memory.  This keeps the range of fully empty regions more tightly packed.\n+        \/\/ Note that the most recently allocated regions tend not to be evacuated in a given GC cycle.  So this\n+        \/\/ tends to accumulate \"fragmented\" uncollected regions in high memory.\n+        if (!_partitions.is_empty(ShenandoahFreeSetPartitionId::Mutator)) {\n+          \/\/ Use signed idx.  Otherwise, loop will never terminate.\n+          idx_t rightmost = _partitions.rightmost(ShenandoahFreeSetPartitionId::Mutator);\n+          for (idx_t idx = _partitions.leftmost(ShenandoahFreeSetPartitionId::Mutator); idx <= rightmost; ) {\n+            assert(_partitions.in_free_set(ShenandoahFreeSetPartitionId::Mutator, idx),\n+                   \"Boundaries or find_last_set_bit failed: \" SSIZE_FORMAT, idx);\n+            ShenandoahHeapRegion* r = _heap->get_region(idx);\n+            \/\/ try_allocate_in() increases used if the allocation is successful.\n+            HeapWord* result;\n+            size_t min_size = (req.type() == ShenandoahAllocRequest::_alloc_tlab)? req.min_size(): req.size();\n+            if ((alloc_capacity(r) >= min_size) && ((result = try_allocate_in(r, req, in_new_region)) != nullptr)) {\n+              return result;\n+            }\n+            idx = _partitions.find_index_of_next_available_region(ShenandoahFreeSetPartitionId::Mutator, idx + 1);\n@@ -562,2 +840,1 @@\n-      \/\/ GCLABs are for evacuation so we must be in evacuation phase.  If this allocation is successful, increment\n-      \/\/ the relevant evac_expended rather than used value.\n+      \/\/ GCLABs are for evacuation so we must be in evacuation phase.\n@@ -565,2 +842,3 @@\n-    case ShenandoahAllocRequest::_alloc_plab:\n-      \/\/ PLABs always reside in old-gen and are only allocated during evacuation phase.\n+    case ShenandoahAllocRequest::_alloc_plab: {\n+      \/\/ PLABs always reside in old-gen and are only allocated during\n+      \/\/ evacuation phase.\n@@ -569,20 +847,12 @@\n-      if (!_heap->mode()->is_generational()) {\n-        \/\/ size_t is unsigned, need to dodge underflow when _leftmost = 0\n-        \/\/ Fast-path: try to allocate in the collector view first\n-        for (size_t c = _free_sets.rightmost(Collector) + 1; c > _free_sets.leftmost(Collector); c--) {\n-          size_t idx = c - 1;\n-          if (_free_sets.in_free_set(idx, Collector)) {\n-            HeapWord* result = try_allocate_in(_heap->get_region(idx), req, in_new_region);\n-            if (result != nullptr) {\n-              return result;\n-            }\n-          }\n-        }\n-      } else {\n-        \/\/ First try to fit into a region that is already in use in the same generation.\n-        HeapWord* result;\n-        if (req.is_old()) {\n-          result = allocate_old_with_affiliation(req.affiliation(), req, in_new_region);\n-        } else {\n-          result = allocate_with_affiliation(req.affiliation(), req, in_new_region);\n-        }\n+      \/\/ Fast-path: try to allocate in the collector view first\n+      HeapWord* result;\n+      result = allocate_from_partition_with_affiliation(req.is_old()? ShenandoahFreeSetPartitionId::OldCollector:\n+                                                        ShenandoahFreeSetPartitionId::Collector,\n+                                                        req.affiliation(), req, in_new_region);\n+      if (result != nullptr) {\n+        return result;\n+      } else if (allow_new_region) {\n+        \/\/ Try a free region that is dedicated to GC allocations.\n+        result = allocate_from_partition_with_affiliation(req.is_old()? ShenandoahFreeSetPartitionId::OldCollector:\n+                                                          ShenandoahFreeSetPartitionId::Collector,\n+                                                          ShenandoahAffiliation::FREE, req, in_new_region);\n@@ -592,11 +862,0 @@\n-        if (allow_new_region) {\n-          \/\/ Then try a free region that is dedicated to GC allocations.\n-          if (req.is_old()) {\n-            result = allocate_old_with_affiliation(FREE, req, in_new_region);\n-          } else {\n-            result = allocate_with_affiliation(FREE, req, in_new_region);\n-          }\n-          if (result != nullptr) {\n-            return result;\n-          }\n-        }\n@@ -604,0 +863,1 @@\n+\n@@ -608,1 +868,0 @@\n-\n@@ -620,1 +879,1 @@\n-      \/\/ Also TODO:\n+      \/\/ TODO:\n@@ -629,15 +888,11 @@\n-        for (size_t c = _free_sets.rightmost_empty(Mutator) + 1; c > _free_sets.leftmost_empty(Mutator); c--) {\n-          size_t idx = c - 1;\n-          if (_free_sets.in_free_set(idx, Mutator)) {\n-            ShenandoahHeapRegion* r = _heap->get_region(idx);\n-            if (can_allocate_from(r)) {\n-              if (req.is_old()) {\n-                flip_to_old_gc(r);\n-              } else {\n-                flip_to_gc(r);\n-              }\n-              HeapWord *result = try_allocate_in(r, req, in_new_region);\n-              if (result != nullptr) {\n-                log_debug(gc, free)(\"Flipped region \" SIZE_FORMAT \" to gc for request: \" PTR_FORMAT, idx, p2i(&req));\n-                return result;\n-              }\n+        idx_t rightmost_mutator = _partitions.rightmost_empty(ShenandoahFreeSetPartitionId::Mutator);\n+        idx_t leftmost_mutator =  _partitions.leftmost_empty(ShenandoahFreeSetPartitionId::Mutator);\n+        for (idx_t idx = rightmost_mutator; idx >= leftmost_mutator; ) {\n+          assert(_partitions.in_free_set(ShenandoahFreeSetPartitionId::Mutator, idx),\n+                 \"Boundaries or find_prev_last_bit failed: \" SSIZE_FORMAT, idx);\n+          ShenandoahHeapRegion* r = _heap->get_region(idx);\n+          if (can_allocate_from(r)) {\n+            if (req.is_old()) {\n+              flip_to_old_gc(r);\n+            } else {\n+              flip_to_gc(r);\n@@ -645,0 +900,4 @@\n+            \/\/ Region r is entirely empty.  If try_allocat_in fails on region r, something else is really wrong.\n+            \/\/ Don't bother to retry with other regions.\n+            log_debug(gc, free)(\"Flipped region \" SIZE_FORMAT \" to gc for request: \" PTR_FORMAT, idx, p2i(&req));\n+            return try_allocate_in(r, req, in_new_region);\n@@ -646,0 +905,1 @@\n+          idx = _partitions.find_index_of_previous_available_region(ShenandoahFreeSetPartitionId::Mutator, idx - 1);\n@@ -648,4 +908,2 @@\n-\n-      \/\/ No dice. Do not try to mix mutator and GC allocations, because\n-      \/\/ URWM moves due to GC allocations would expose unparsable mutator\n-      \/\/ allocations.\n+      \/\/ No dice. Do not try to mix mutator and GC allocations, because adjusting region UWM\n+      \/\/ due to GC allocations would expose unparsable mutator allocations.\n@@ -654,0 +912,1 @@\n+    }\n@@ -672,1 +931,1 @@\n-size_t get_usable_free_words(size_t free_bytes) {\n+size_t ShenandoahFreeSet::get_usable_free_words(size_t free_bytes) const {\n@@ -706,1 +965,0 @@\n-  assert(req.actual_size() == size, \"Should not have needed to adjust size for PLAB.\");\n@@ -708,1 +966,0 @@\n-\n@@ -717,1 +974,1 @@\n-\n+  HeapWord* result = nullptr;\n@@ -719,2 +976,6 @@\n-  if (!r->is_affiliated()) {\n-    ShenandoahMarkingContext* const ctx = _heap->complete_marking_context();\n+  in_new_region = r->is_empty();\n+\n+  if (in_new_region) {\n+    log_debug(gc)(\"Using new region (\" SIZE_FORMAT \") for %s (\" PTR_FORMAT \").\",\n+                       r->index(), ShenandoahAllocRequest::alloc_type_to_string(req.type()), p2i(&req));\n+    assert(!r->is_affiliated(), \"New region \" SIZE_FORMAT \" should be unaffiliated\", r->index());\n@@ -734,0 +995,2 @@\n+#ifdef ASSERT\n+    ShenandoahMarkingContext* const ctx = _heap->complete_marking_context();\n@@ -736,11 +999,2 @@\n-  } else if (r->affiliation() != req.affiliation()) {\n-    assert(_heap->mode()->is_generational(), \"Request for %s from %s region should only happen in generational mode.\",\n-           req.affiliation_name(), r->affiliation_name());\n-    return nullptr;\n-  }\n-\n-  in_new_region = r->is_empty();\n-  HeapWord* result = nullptr;\n-\n-  if (in_new_region) {\n-    log_debug(gc, free)(\"Using new region (\" SIZE_FORMAT \") for %s (\" PTR_FORMAT \").\",\n+#endif\n+    log_debug(gc)(\"Using new region (\" SIZE_FORMAT \") for %s (\" PTR_FORMAT \").\",\n@@ -748,0 +1002,7 @@\n+  } else {\n+    assert(r->is_affiliated(), \"Region \" SIZE_FORMAT \" that is not new should be affiliated\", r->index());\n+    if (r->affiliation() != req.affiliation()) {\n+      assert(_heap->mode()->is_generational(), \"Request for %s from %s region should only happen in generational mode.\",\n+             req.affiliation_name(), r->affiliation_name());\n+      return nullptr;\n+    }\n@@ -752,0 +1013,2 @@\n+    size_t adjusted_size = req.size();\n+    size_t free = r->free();    \/\/ free represents bytes available within region r\n@@ -753,0 +1016,1 @@\n+      \/\/ This is a PLAB allocation\n@@ -754,7 +1018,2 @@\n-      assert(_free_sets.in_free_set(r->index(), OldCollector), \"PLABS must be allocated in old_collector_free regions\");\n-      \/\/ Need to assure that plabs are aligned on multiple of card region.\n-      \/\/ Since we have Elastic TLABs, align sizes up. They may be decreased to fit in the usable\n-      \/\/ memory remaining in the region (which will also be aligned to cards).\n-      size_t adjusted_size = align_up(req.size(), CardTable::card_size_in_words());\n-      size_t adjusted_min_size = align_up(req.min_size(), CardTable::card_size_in_words());\n-      size_t usable_free = get_usable_free_words(r->free());\n+      assert(_partitions.in_free_set(ShenandoahFreeSetPartitionId::OldCollector, r->index()),\n+             \"PLABS must be allocated in old_collector_free regions\");\n@@ -762,0 +1021,3 @@\n+      \/\/ Need to assure that plabs are aligned on multiple of card region\n+      \/\/ Convert free from unaligned bytes to aligned number of words\n+      size_t usable_free = get_usable_free_words(free);\n@@ -765,2 +1027,2 @@\n-\n-      if (adjusted_size >= adjusted_min_size) {\n+      adjusted_size = align_down(adjusted_size, CardTable::card_size_in_words());\n+      if (adjusted_size >= req.min_size()) {\n@@ -768,0 +1030,6 @@\n+        assert(result != nullptr, \"allocate must succeed\");\n+        req.set_actual_size(adjusted_size);\n+      } else {\n+        \/\/ Otherwise, leave result == nullptr because the adjusted size is smaller than min size.\n+        log_trace(gc, free)(\"Failed to shrink PLAB request (\" SIZE_FORMAT \") in region \" SIZE_FORMAT \" to \" SIZE_FORMAT\n+                            \" because min_size() is \" SIZE_FORMAT, req.size(), r->index(), adjusted_size, req.min_size());\n@@ -769,1 +1037,0 @@\n-      \/\/ Otherwise, leave result == nullptr because the adjusted size is smaller than min size.\n@@ -772,2 +1039,2 @@\n-      size_t adjusted_size = req.size();\n-      size_t free = align_down(r->free() >> LogHeapWordSize, MinObjAlignment);\n+      \/\/ Convert free from unaligned bytes to aligned number of words\n+      free = align_down(free >> LogHeapWordSize, MinObjAlignment);\n@@ -783,1 +1050,1 @@\n-                           \" because min_size() is \" SIZE_FORMAT, req.size(), r->index(), adjusted_size, req.min_size());\n+                            \" because min_size() is \" SIZE_FORMAT, req.size(), r->index(), adjusted_size, req.min_size());\n@@ -795,1 +1062,0 @@\n-  ShenandoahGeneration* generation = _heap->generation_for(req.affiliation());\n@@ -800,1 +1066,1 @@\n-      _free_sets.increase_used(Mutator, req.actual_size() * HeapWordSize);\n+      _partitions.increase_used(ShenandoahFreeSetPartitionId::Mutator, req.actual_size() * HeapWordSize);\n@@ -813,0 +1079,1 @@\n+        _partitions.increase_used(ShenandoahFreeSetPartitionId::OldCollector, req.actual_size() * HeapWordSize);\n@@ -815,0 +1082,2 @@\n+      } else {\n+        _partitions.increase_used(ShenandoahFreeSetPartitionId::Collector, req.actual_size() * HeapWordSize);\n@@ -819,6 +1088,9 @@\n-  if (result == nullptr || alloc_capacity(r) < PLAB::min_size() * HeapWordSize) {\n-    \/\/ Region cannot afford this and is likely to not afford future allocations. Retire it.\n-    \/\/\n-    \/\/ While this seems a bit harsh, especially in the case when this large allocation does not\n-    \/\/ fit but the next small one would, we are risking to inflate scan times when lots of\n-    \/\/ almost-full regions precede the fully-empty region where we want to allocate the entire TLAB.\n+  static const size_t min_capacity = (size_t) (ShenandoahHeapRegion::region_size_bytes() * (1.0 - 1.0 \/ ShenandoahEvacWaste));\n+  size_t ac = alloc_capacity(r);\n+\n+  if (((result == nullptr) && (ac < min_capacity)) || (alloc_capacity(r) < PLAB::min_size() * HeapWordSize)) {\n+    \/\/ Regardless of whether this allocation succeeded, if the remaining memory is less than PLAB:min_size(), retire this region.\n+    \/\/ Note that retire_from_partition() increases used to account for waste.\n+\n+    \/\/ Also, if this allocation request failed and the consumed within this region * ShenandoahEvacWaste > region size,\n+    \/\/ then retire the region so that subsequent searches can find available memory more quickly.\n@@ -826,1 +1098,0 @@\n-    \/\/ Record the remainder as allocation waste\n@@ -828,0 +1099,1 @@\n+    ShenandoahFreeSetPartitionId orig_partition;\n@@ -829,7 +1101,5 @@\n-      size_t waste = r->free();\n-      if (waste > 0) {\n-        _free_sets.increase_used(Mutator, waste);\n-        \/\/ This one request could cause several regions to be \"retired\", so we must accumulate the waste\n-        req.set_waste((waste >> LogHeapWordSize) + req.waste());\n-      }\n-      assert(_free_sets.membership(idx) == Mutator, \"Must be mutator free: \" SIZE_FORMAT, idx);\n+      orig_partition = ShenandoahFreeSetPartitionId::Mutator;\n+    } else if (req.type() == ShenandoahAllocRequest::_alloc_gclab) {\n+      orig_partition = ShenandoahFreeSetPartitionId::Collector;\n+    } else if (req.type() == ShenandoahAllocRequest::_alloc_plab) {\n+      orig_partition = ShenandoahFreeSetPartitionId::OldCollector;\n@@ -837,2 +1107,6 @@\n-      assert(_free_sets.membership(idx) == Collector || _free_sets.membership(idx) == OldCollector,\n-             \"Must be collector or old-collector free: \" SIZE_FORMAT, idx);\n+      assert(req.type() == ShenandoahAllocRequest::_alloc_shared_gc, \"Unexpected allocation type\");\n+      if (req.is_old()) {\n+        orig_partition = ShenandoahFreeSetPartitionId::OldCollector;\n+      } else {\n+        orig_partition = ShenandoahFreeSetPartitionId::Collector;\n+      }\n@@ -840,3 +1114,2 @@\n-    \/\/ This region is no longer considered free (in any set)\n-    _free_sets.remove_from_free_sets(idx);\n-    _free_sets.assert_bounds();\n+    _partitions.retire_from_partition(orig_partition, idx, r->used());\n+    _partitions.assert_bounds();\n@@ -848,0 +1121,1 @@\n+  assert(req.is_mutator_alloc(), \"All humongous allocations are performed by mutator\");\n@@ -851,1 +1125,1 @@\n-  size_t num = ShenandoahHeapRegion::required_regions(words_size * HeapWordSize);\n+  idx_t num = ShenandoahHeapRegion::required_regions(words_size * HeapWordSize);\n@@ -857,9 +1131,2 @@\n-  if (_heap->mode()->is_generational()) {\n-    size_t avail_young_regions = generation->free_unaffiliated_regions();\n-    if (num > _free_sets.count(Mutator) || (num > avail_young_regions)) {\n-      return nullptr;\n-    }\n-  } else {\n-    if (num > _free_sets.count(Mutator)) {\n-      return nullptr;\n-    }\n+  if (num > (idx_t) _partitions.count(ShenandoahFreeSetPartitionId::Mutator)) {\n+    return nullptr;\n@@ -868,0 +1135,4 @@\n+  idx_t start_range = _partitions.leftmost_empty(ShenandoahFreeSetPartitionId::Mutator);\n+  idx_t end_range = _partitions.rightmost_empty(ShenandoahFreeSetPartitionId::Mutator) + 1;\n+  idx_t last_possible_start = end_range - num;\n+\n@@ -870,3 +1141,7 @@\n-\n-  size_t beg = _free_sets.leftmost(Mutator);\n-  size_t end = beg;\n+  idx_t beg = _partitions.find_index_of_next_available_cluster_of_regions(ShenandoahFreeSetPartitionId::Mutator,\n+                                                                          start_range, num);\n+  if (beg > last_possible_start) {\n+    \/\/ Hit the end, goodbye\n+    return nullptr;\n+  }\n+  idx_t end = beg;\n@@ -875,11 +1150,26 @@\n-    if (end >= _free_sets.max()) {\n-      \/\/ Hit the end, goodbye\n-      return nullptr;\n-    }\n-\n-    \/\/ If regions are not adjacent, then current [beg; end] is useless, and we may fast-forward.\n-    \/\/ If region is not completely free, the current [beg; end] is useless, and we may fast-forward.\n-    if (!_free_sets.in_free_set(end, Mutator) || !can_allocate_from(_heap->get_region(end))) {\n-      end++;\n-      beg = end;\n-      continue;\n+    \/\/ We've confirmed num contiguous regions belonging to Mutator partition, so no need to confirm membership.\n+    \/\/ If region is not completely free, the current [beg; end] is useless, and we may fast-forward.  If we can extend\n+    \/\/ the existing range, we can exploit that certain regions are already known to be in the Mutator free set.\n+    while (!can_allocate_from(_heap->get_region(end))) {\n+      \/\/ region[end] is not empty, so we restart our search after region[end]\n+      idx_t slide_delta = end + 1 - beg;\n+      if (beg + slide_delta > last_possible_start) {\n+        \/\/ no room to slide\n+        return nullptr;\n+      }\n+      for (idx_t span_end = beg + num; slide_delta > 0; slide_delta--) {\n+        if (!_partitions.in_free_set(ShenandoahFreeSetPartitionId::Mutator, span_end)) {\n+          beg = _partitions.find_index_of_next_available_cluster_of_regions(ShenandoahFreeSetPartitionId::Mutator,\n+                                                                            span_end + 1, num);\n+          break;\n+        } else {\n+          beg++;\n+          span_end++;\n+        }\n+      }\n+      \/\/ Here, either beg identifies a range of num regions all of which are in the Mutator free set, or beg > last_possible_start\n+      if (beg > last_possible_start) {\n+        \/\/ Hit the end, goodbye\n+        return nullptr;\n+      }\n+      end = beg;\n@@ -897,2 +1187,1 @@\n-  ShenandoahMarkingContext* const ctx = _heap->complete_marking_context();\n-\n+  bool is_generational = _heap->mode()->is_generational();\n@@ -900,1 +1189,1 @@\n-  for (size_t i = beg; i <= end; i++) {\n+  for (idx_t i = beg; i <= end; i++) {\n@@ -924,3 +1213,0 @@\n-\n-    \/\/ While individual regions report their true use, all humongous regions are marked used in the free set.\n-    _free_sets.remove_from_free_sets(r->index());\n@@ -929,0 +1215,7 @@\n+  if (remainder != 0) {\n+    \/\/ Record this remainder as allocation waste\n+    _heap->notify_mutator_alloc_words(ShenandoahHeapRegion::region_size_words() - remainder, true);\n+  }\n+\n+  \/\/ retire_range_from_partition() will adjust bounds on Mutator free set if appropriate\n+  _partitions.retire_range_from_partition(ShenandoahFreeSetPartitionId::Mutator, beg, end);\n@@ -931,2 +1224,2 @@\n-  _free_sets.increase_used(Mutator, total_humongous_size);\n-  _free_sets.assert_bounds();\n+  _partitions.increase_used(ShenandoahFreeSetPartitionId::Mutator, total_humongous_size);\n+  _partitions.assert_bounds();\n@@ -940,30 +1233,0 @@\n-\/\/ Returns true iff this region is entirely available, either because it is empty() or because it has been found to represent\n-\/\/ immediate trash and we'll be able to immediately recycle it.  Note that we cannot recycle immediate trash if\n-\/\/ concurrent weak root processing is in progress.\n-bool ShenandoahFreeSet::can_allocate_from(ShenandoahHeapRegion *r) const {\n-  return r->is_empty() || (r->is_trash() && !_heap->is_concurrent_weak_root_in_progress());\n-}\n-\n-bool ShenandoahFreeSet::can_allocate_from(size_t idx) const {\n-  ShenandoahHeapRegion* r = _heap->get_region(idx);\n-  return can_allocate_from(r);\n-}\n-\n-size_t ShenandoahFreeSet::alloc_capacity(size_t idx) const {\n-  ShenandoahHeapRegion* r = _heap->get_region(idx);\n-  return alloc_capacity(r);\n-}\n-\n-size_t ShenandoahFreeSet::alloc_capacity(ShenandoahHeapRegion *r) const {\n-  if (r->is_trash()) {\n-    \/\/ This would be recycled on allocation path\n-    return ShenandoahHeapRegion::region_size_bytes();\n-  } else {\n-    return r->free();\n-  }\n-}\n-\n-bool ShenandoahFreeSet::has_alloc_capacity(ShenandoahHeapRegion *r) const {\n-  return alloc_capacity(r) > 0;\n-}\n-\n@@ -979,1 +1242,0 @@\n-\n@@ -993,2 +1255,1 @@\n-  assert(_free_sets.in_free_set(idx, Mutator), \"Should be in mutator view\");\n-  \/\/ Note: can_allocate_from(r) means r is entirely empty\n+  assert(_partitions.partition_id_matches(idx, ShenandoahFreeSetPartitionId::Mutator), \"Should be in mutator view\");\n@@ -997,1 +1258,1 @@\n-  ShenandoahGenerationalHeap* gen_heap = ShenandoahGenerationalHeap::cast(_heap);\n+  ShenandoahGenerationalHeap* gen_heap = ShenandoahGenerationalHeap::heap();\n@@ -999,2 +1260,3 @@\n-  _free_sets.move_to_set(idx, OldCollector, region_capacity);\n-  _free_sets.assert_bounds();\n+  _partitions.move_from_partition_to_partition(idx, ShenandoahFreeSetPartitionId::Mutator,\n+                                               ShenandoahFreeSetPartitionId::OldCollector, region_capacity);\n+  _partitions.assert_bounds();\n@@ -1014,1 +1276,1 @@\n-  assert(_free_sets.in_free_set(idx, Mutator), \"Should be in mutator view\");\n+  assert(_partitions.partition_id_matches(idx, ShenandoahFreeSetPartitionId::Mutator), \"Should be in mutator view\");\n@@ -1017,3 +1279,4 @@\n-  size_t region_capacity = alloc_capacity(r);\n-  _free_sets.move_to_set(idx, Collector, region_capacity);\n-  _free_sets.assert_bounds();\n+  size_t ac = alloc_capacity(r);\n+  _partitions.move_from_partition_to_partition(idx, ShenandoahFreeSetPartitionId::Mutator,\n+                                               ShenandoahFreeSetPartitionId::Collector, ac);\n+  _partitions.assert_bounds();\n@@ -1031,1 +1294,6 @@\n-  _free_sets.clear_all();\n+  _partitions.make_all_regions_unavailable();\n+\n+  _alloc_bias_weight = 0;\n+  _partitions.set_bias_from_left_to_right(ShenandoahFreeSetPartitionId::Mutator, true);\n+  _partitions.set_bias_from_left_to_right(ShenandoahFreeSetPartitionId::Collector, false);\n+  _partitions.set_bias_from_left_to_right(ShenandoahFreeSetPartitionId::OldCollector, false);\n@@ -1034,5 +1302,0 @@\n-\/\/ This function places all is_old() regions that have allocation capacity into the old_collector set.  It places\n-\/\/ all other regions (not is_old()) that have allocation capacity into the mutator_set.  Subsequently, we will\n-\/\/ move some of the mutator regions into the collector set or old_collector set with the intent of packing\n-\/\/ old_collector memory into the highest (rightmost) addresses of the heap and the collector memory into the\n-\/\/ next highest addresses of the heap, with mutator memory consuming the lowest addresses of the heap.\n@@ -1042,0 +1305,2 @@\n+  clear_internal();\n+\n@@ -1047,0 +1312,18 @@\n+\n+  size_t region_size_bytes = _partitions.region_size_bytes();\n+  size_t max_regions = _partitions.max_regions();\n+\n+  size_t mutator_leftmost = max_regions;\n+  size_t mutator_rightmost = 0;\n+  size_t mutator_leftmost_empty = max_regions;\n+  size_t mutator_rightmost_empty = 0;\n+  size_t mutator_regions = 0;\n+  size_t mutator_used = 0;\n+\n+  size_t old_collector_leftmost = max_regions;\n+  size_t old_collector_rightmost = 0;\n+  size_t old_collector_leftmost_empty = max_regions;\n+  size_t old_collector_rightmost_empty = 0;\n+  size_t old_collector_regions = 0;\n+  size_t old_collector_used = 0;\n+\n@@ -1050,1 +1333,2 @@\n-      \/\/ Trashed regions represent regions that had been in the collection set but have not yet been \"cleaned up\".\n+      \/\/ Trashed regions represent regions that had been in the collection partition but have not yet been \"cleaned up\".\n+      \/\/ The cset regions are not \"trashed\" until we have finished update refs.\n@@ -1057,1 +1341,2 @@\n-    } else if (region->is_old() && region->is_regular()) {\n+    } else if (region->is_old()) {\n+      \/\/ count both humongous and regular regions, but don't count trash (cset) regions.\n@@ -1066,1 +1351,0 @@\n-      assert(_free_sets.in_free_set(idx, NotFree), \"We are about to make region free; it should not be free already\");\n@@ -1068,15 +1352,42 @@\n-      \/\/ Do not add regions that would almost surely fail allocation.  Note that PLAB::min_size() is typically less than ShenandoahGenerationalHeap::plab_min_size()\n-      if (alloc_capacity(region) < PLAB::min_size() * HeapWordSize) continue;\n-\n-      if (region->is_old()) {\n-        _free_sets.make_free(idx, OldCollector, alloc_capacity(region));\n-        log_debug(gc, free)(\n-          \"  Adding Region \" SIZE_FORMAT  \" (Free: \" SIZE_FORMAT \"%s, Used: \" SIZE_FORMAT \"%s) to old collector set\",\n-          idx, byte_size_in_proper_unit(region->free()), proper_unit_for_byte_size(region->free()),\n-          byte_size_in_proper_unit(region->used()), proper_unit_for_byte_size(region->used()));\n-      } else {\n-        _free_sets.make_free(idx, Mutator, alloc_capacity(region));\n-        log_debug(gc, free)(\n-          \"  Adding Region \" SIZE_FORMAT \" (Free: \" SIZE_FORMAT \"%s, Used: \" SIZE_FORMAT \"%s) to mutator set\",\n-          idx, byte_size_in_proper_unit(region->free()), proper_unit_for_byte_size(region->free()),\n-          byte_size_in_proper_unit(region->used()), proper_unit_for_byte_size(region->used()));\n+      \/\/ Do not add regions that would almost surely fail allocation\n+      size_t ac = alloc_capacity(region);\n+      if (ac > PLAB::min_size() * HeapWordSize) {\n+        if (region->is_trash() || !region->is_old()) {\n+          \/\/ Both young and old collected regions (trashed) are placed into the Mutator set\n+          _partitions.raw_assign_membership(idx, ShenandoahFreeSetPartitionId::Mutator);\n+          if (idx < mutator_leftmost) {\n+            mutator_leftmost = idx;\n+          }\n+          if (idx > mutator_rightmost) {\n+            mutator_rightmost = idx;\n+          }\n+          if (ac == region_size_bytes) {\n+            if (idx < mutator_leftmost_empty) {\n+              mutator_leftmost_empty = idx;\n+            }\n+            if (idx > mutator_rightmost_empty) {\n+              mutator_rightmost_empty = idx;\n+            }\n+          }\n+          mutator_regions++;\n+          mutator_used += (region_size_bytes - ac);\n+        } else {\n+          \/\/ !region->is_trash() && region is_old()\n+          _partitions.raw_assign_membership(idx, ShenandoahFreeSetPartitionId::OldCollector);\n+          if (idx < old_collector_leftmost) {\n+            old_collector_leftmost = idx;\n+          }\n+          if (idx > old_collector_rightmost) {\n+            old_collector_rightmost = idx;\n+          }\n+          if (ac == region_size_bytes) {\n+            if (idx < old_collector_leftmost_empty) {\n+              old_collector_leftmost_empty = idx;\n+            }\n+            if (idx > old_collector_rightmost_empty) {\n+              old_collector_rightmost_empty = idx;\n+            }\n+          }\n+          old_collector_regions++;\n+          old_collector_used += (region_size_bytes - ac);\n+        }\n@@ -1086,0 +1397,28 @@\n+  log_debug(gc)(\"  At end of prep_to_rebuild, mutator_leftmost: \" SIZE_FORMAT\n+                \", mutator_rightmost: \" SIZE_FORMAT\n+                \", mutator_leftmost_empty: \" SIZE_FORMAT\n+                \", mutator_rightmost_empty: \" SIZE_FORMAT\n+                \", mutator_regions: \" SIZE_FORMAT\n+                \", mutator_used: \" SIZE_FORMAT,\n+                mutator_leftmost, mutator_rightmost, mutator_leftmost_empty, mutator_rightmost_empty,\n+                mutator_regions, mutator_used);\n+\n+  log_debug(gc)(\"  old_collector_leftmost: \" SIZE_FORMAT\n+                \", old_collector_rightmost: \" SIZE_FORMAT\n+                \", old_collector_leftmost_empty: \" SIZE_FORMAT\n+                \", old_collector_rightmost_empty: \" SIZE_FORMAT\n+                \", old_collector_regions: \" SIZE_FORMAT\n+                \", old_collector_used: \" SIZE_FORMAT,\n+                old_collector_leftmost, old_collector_rightmost, old_collector_leftmost_empty, old_collector_rightmost_empty,\n+                old_collector_regions, old_collector_used);\n+\n+  _partitions.establish_mutator_intervals(mutator_leftmost, mutator_rightmost, mutator_leftmost_empty, mutator_rightmost_empty,\n+                                          mutator_regions, mutator_used);\n+  _partitions.establish_old_collector_intervals(old_collector_leftmost, old_collector_rightmost, old_collector_leftmost_empty,\n+                                                old_collector_rightmost_empty, old_collector_regions, old_collector_used);\n+  log_debug(gc)(\"  After find_regions_with_alloc_capacity(), Mutator range [\" SSIZE_FORMAT \", \" SSIZE_FORMAT \"],\"\n+                \"  Old Collector range [\" SSIZE_FORMAT \", \" SSIZE_FORMAT \"]\",\n+                _partitions.leftmost(ShenandoahFreeSetPartitionId::Mutator),\n+                _partitions.rightmost(ShenandoahFreeSetPartitionId::Mutator),\n+                _partitions.leftmost(ShenandoahFreeSetPartitionId::OldCollector),\n+                _partitions.rightmost(ShenandoahFreeSetPartitionId::OldCollector));\n@@ -1088,3 +1427,5 @@\n-\/\/ Move no more than cset_regions from the existing Collector and OldCollector free sets to the Mutator free set.\n-\/\/ This is called from outside the heap lock.\n-void ShenandoahFreeSet::move_collector_sets_to_mutator(size_t max_xfer_regions) {\n+\/\/ Returns number of regions transferred, adds transferred bytes to var argument bytes_transferred\n+size_t ShenandoahFreeSet::transfer_empty_regions_from_collector_set_to_mutator_set(ShenandoahFreeSetPartitionId which_collector,\n+                                                                                   size_t max_xfer_regions,\n+                                                                                   size_t& bytes_transferred) {\n+  shenandoah_assert_heaplocked();\n@@ -1092,3 +1433,14 @@\n-  size_t collector_empty_xfer = 0;\n-  size_t collector_not_empty_xfer = 0;\n-  size_t old_collector_empty_xfer = 0;\n+  size_t transferred_regions = 0;\n+  idx_t rightmost = _partitions.rightmost_empty(which_collector);\n+  for (idx_t idx = _partitions.leftmost_empty(which_collector); (transferred_regions < max_xfer_regions) && (idx <= rightmost); ) {\n+    assert(_partitions.in_free_set(which_collector, idx), \"Boundaries or find_first_set_bit failed: \" SSIZE_FORMAT, idx);\n+    \/\/ Note: can_allocate_from() denotes that region is entirely empty\n+    if (can_allocate_from(idx)) {\n+      _partitions.move_from_partition_to_partition(idx, which_collector, ShenandoahFreeSetPartitionId::Mutator, region_size_bytes);\n+      transferred_regions++;\n+      bytes_transferred += region_size_bytes;\n+    }\n+    idx = _partitions.find_index_of_next_available_region(which_collector, idx + 1);\n+  }\n+  return transferred_regions;\n+}\n@@ -1096,10 +1448,14 @@\n-  \/\/ Process empty regions within the Collector free set\n-  if ((max_xfer_regions > 0) && (_free_sets.leftmost_empty(Collector) <= _free_sets.rightmost_empty(Collector))) {\n-    ShenandoahHeapLocker locker(_heap->lock());\n-    for (size_t idx = _free_sets.leftmost_empty(Collector);\n-         (max_xfer_regions > 0) && (idx <= _free_sets.rightmost_empty(Collector)); idx++) {\n-      if (_free_sets.in_free_set(idx, Collector) && can_allocate_from(idx)) {\n-        _free_sets.move_to_set(idx, Mutator, region_size_bytes);\n-        max_xfer_regions--;\n-        collector_empty_xfer += region_size_bytes;\n-      }\n+\/\/ Returns number of regions transferred, adds transferred bytes to var argument bytes_transferred\n+size_t ShenandoahFreeSet::transfer_non_empty_regions_from_collector_set_to_mutator_set(ShenandoahFreeSetPartitionId collector_id,\n+                                                                                       size_t max_xfer_regions,\n+                                                                                       size_t& bytes_transferred) {\n+  shenandoah_assert_heaplocked();\n+  size_t transferred_regions = 0;\n+  idx_t rightmost = _partitions.rightmost(collector_id);\n+  for (idx_t idx = _partitions.leftmost(collector_id); (transferred_regions < max_xfer_regions) && (idx <= rightmost); ) {\n+    assert(_partitions.in_free_set(collector_id, idx), \"Boundaries or find_first_set_bit failed: \" SSIZE_FORMAT, idx);\n+    size_t ac = alloc_capacity(idx);\n+    if (ac > 0) {\n+      _partitions.move_from_partition_to_partition(idx, collector_id, ShenandoahFreeSetPartitionId::Mutator, ac);\n+      transferred_regions++;\n+      bytes_transferred += ac;\n@@ -1107,0 +1463,1 @@\n+    idx = _partitions.find_index_of_next_available_region(ShenandoahFreeSetPartitionId::Collector, idx + 1);\n@@ -1108,0 +1465,2 @@\n+  return transferred_regions;\n+}\n@@ -1109,3 +1468,8 @@\n-  \/\/ Process empty regions within the OldCollector free set\n-  size_t old_collector_regions = 0;\n-  if ((max_xfer_regions > 0) && (_free_sets.leftmost_empty(OldCollector) <= _free_sets.rightmost_empty(OldCollector))) {\n+void ShenandoahFreeSet::move_regions_from_collector_to_mutator(size_t max_xfer_regions) {\n+  size_t collector_xfer = 0;\n+  size_t old_collector_xfer = 0;\n+\n+  \/\/ Process empty regions within the Collector free partition\n+  if ((max_xfer_regions > 0) &&\n+      (_partitions.leftmost_empty(ShenandoahFreeSetPartitionId::Collector)\n+       <= _partitions.rightmost_empty(ShenandoahFreeSetPartitionId::Collector))) {\n@@ -1113,9 +1477,14 @@\n-    for (size_t idx = _free_sets.leftmost_empty(OldCollector);\n-         (max_xfer_regions > 0) && (idx <= _free_sets.rightmost_empty(OldCollector)); idx++) {\n-      if (_free_sets.in_free_set(idx, OldCollector) && can_allocate_from(idx)) {\n-        _free_sets.move_to_set(idx, Mutator, region_size_bytes);\n-        max_xfer_regions--;\n-        old_collector_empty_xfer += region_size_bytes;\n-        old_collector_regions++;\n-      }\n-    }\n+    max_xfer_regions -=\n+      transfer_empty_regions_from_collector_set_to_mutator_set(ShenandoahFreeSetPartitionId::Collector, max_xfer_regions,\n+                                                               collector_xfer);\n+  }\n+\n+  \/\/ Process empty regions within the OldCollector free partition\n+  if ((max_xfer_regions > 0) &&\n+      (_partitions.leftmost_empty(ShenandoahFreeSetPartitionId::OldCollector)\n+       <= _partitions.rightmost_empty(ShenandoahFreeSetPartitionId::OldCollector))) {\n+    ShenandoahHeapLocker locker(_heap->lock());\n+    size_t old_collector_regions =\n+      transfer_empty_regions_from_collector_set_to_mutator_set(ShenandoahFreeSetPartitionId::OldCollector, max_xfer_regions,\n+                                                               old_collector_xfer);\n+    max_xfer_regions -= old_collector_regions;\n@@ -1127,2 +1496,3 @@\n-  \/\/ If there are any non-empty regions within Collector set, we can also move them to the Mutator free set\n-  if ((max_xfer_regions > 0) && (_free_sets.leftmost(Collector) <= _free_sets.rightmost(Collector))) {\n+  \/\/ If there are any non-empty regions within Collector partition, we can also move them to the Mutator free partition\n+  if ((max_xfer_regions > 0) && (_partitions.leftmost(ShenandoahFreeSetPartitionId::Collector)\n+                                 <= _partitions.rightmost(ShenandoahFreeSetPartitionId::Collector))) {\n@@ -1130,8 +1500,3 @@\n-    for (size_t idx = _free_sets.leftmost(Collector); (max_xfer_regions > 0) && (idx <= _free_sets.rightmost(Collector)); idx++) {\n-      size_t alloc_capacity = this->alloc_capacity(idx);\n-      if (_free_sets.in_free_set(idx, Collector) && (alloc_capacity > 0)) {\n-        _free_sets.move_to_set(idx, Mutator, alloc_capacity);\n-        max_xfer_regions--;\n-        collector_not_empty_xfer += alloc_capacity;\n-      }\n-    }\n+    max_xfer_regions -=\n+      transfer_non_empty_regions_from_collector_set_to_mutator_set(ShenandoahFreeSetPartitionId::Collector, max_xfer_regions,\n+                                                                   collector_xfer);\n@@ -1140,2 +1505,1 @@\n-  size_t collector_xfer = collector_empty_xfer + collector_not_empty_xfer;\n-  size_t total_xfer = collector_xfer + old_collector_empty_xfer;\n+  size_t total_xfer = collector_xfer + old_collector_xfer;\n@@ -1146,1 +1510,1 @@\n-                     byte_size_in_proper_unit(old_collector_empty_xfer), proper_unit_for_byte_size(old_collector_empty_xfer));\n+                     byte_size_in_proper_unit(old_collector_xfer), proper_unit_for_byte_size(old_collector_xfer));\n@@ -1159,1 +1523,1 @@\n-  \/\/ mutator set otherwise.\n+  \/\/ mutator set otherwise.  All trashed (cset) regions are affiliated young and placed in mutator set.\n@@ -1163,1 +1527,15 @@\n-void ShenandoahFreeSet::rebuild(size_t young_cset_regions, size_t old_cset_regions, bool have_evacuation_reserves) {\n+void ShenandoahFreeSet::establish_generation_sizes(size_t young_region_count, size_t old_region_count) {\n+  assert(young_region_count + old_region_count == ShenandoahHeap::heap()->num_regions(), \"Sanity\");\n+  if (ShenandoahHeap::heap()->mode()->is_generational()) {\n+    ShenandoahGenerationalHeap* heap = ShenandoahGenerationalHeap::heap();\n+    ShenandoahOldGeneration* old_gen = heap->old_generation();\n+    ShenandoahYoungGeneration* young_gen = heap->young_generation();\n+    size_t region_size_bytes = ShenandoahHeapRegion::region_size_bytes();\n+\n+    old_gen->set_capacity(old_region_count * region_size_bytes);\n+    young_gen->set_capacity(young_region_count * region_size_bytes);\n+  }\n+}\n+\n+void ShenandoahFreeSet::finish_rebuild(size_t young_cset_regions, size_t old_cset_regions, size_t old_region_count,\n+                                       bool have_evacuation_reserves) {\n@@ -1167,4 +1545,1 @@\n-  if (!_heap->mode()->is_generational()) {\n-    young_reserve = (_heap->max_capacity() \/ 100) * ShenandoahEvacReserve;\n-    old_reserve = 0;\n-  } else {\n+  if (_heap->mode()->is_generational()) {\n@@ -1173,1 +1548,3 @@\n-\n+  } else {\n+    young_reserve = (_heap->max_capacity() \/ 100) * ShenandoahEvacReserve;\n+    old_reserve = 0;\n@@ -1176,3 +1553,7 @@\n-  reserve_regions(young_reserve, old_reserve);\n-  _free_sets.establish_alloc_bias(OldCollector);\n-  _free_sets.assert_bounds();\n+  \/\/ Move some of the mutator regions in the Collector and OldCollector partitions in order to satisfy\n+  \/\/ young_reserve and old_reserve.\n+  reserve_regions(young_reserve, old_reserve, old_region_count);\n+  size_t young_region_count = _heap->num_regions() - old_region_count;\n+  establish_generation_sizes(young_region_count, old_region_count);\n+  establish_old_collector_alloc_bias();\n+  _partitions.assert_bounds();\n@@ -1182,1 +1563,2 @@\n-void ShenandoahFreeSet::compute_young_and_old_reserves(size_t young_cset_regions, size_t old_cset_regions, bool have_evacuation_reserves,\n+void ShenandoahFreeSet::compute_young_and_old_reserves(size_t young_cset_regions, size_t old_cset_regions,\n+                                                       bool have_evacuation_reserves,\n@@ -1196,1 +1578,0 @@\n-  old_available += old_cset_regions * region_size_bytes;\n@@ -1203,0 +1584,1 @@\n+#ifdef ASSERT\n@@ -1208,0 +1590,1 @@\n+#endif\n@@ -1242,2 +1625,4 @@\n-  if (old_reserve_result > _free_sets.capacity_of(OldCollector) + old_unaffiliated_regions * region_size_bytes) {\n-    old_reserve_result = _free_sets.capacity_of(OldCollector) + old_unaffiliated_regions * region_size_bytes;\n+  if (old_reserve_result >\n+      _partitions.capacity_of(ShenandoahFreeSetPartitionId::OldCollector) + old_unaffiliated_regions * region_size_bytes) {\n+    old_reserve_result =\n+      _partitions.capacity_of(ShenandoahFreeSetPartitionId::OldCollector) + old_unaffiliated_regions * region_size_bytes;\n@@ -1246,1 +1631,1 @@\n-  if (old_reserve_result > young_unaffiliated_regions * region_size_bytes) {\n+  if (young_reserve_result > young_unaffiliated_regions * region_size_bytes) {\n@@ -1254,1 +1639,1 @@\n-\/\/ the collector set is at least to_reserve, and the memory available for allocations within the old collector set\n+\/\/ the collector set is at least to_reserve and the memory available for allocations within the old collector set\n@@ -1256,1 +1641,1 @@\n-void ShenandoahFreeSet::reserve_regions(size_t to_reserve, size_t to_reserve_old) {\n+void ShenandoahFreeSet::reserve_regions(size_t to_reserve, size_t to_reserve_old, size_t &old_region_count) {\n@@ -1260,1 +1645,1 @@\n-    if (!_free_sets.in_free_set(idx, Mutator)) {\n+    if (!_partitions.in_free_set(ShenandoahFreeSetPartitionId::Mutator, idx)) {\n@@ -1266,1 +1651,1 @@\n-    assert (!r->is_old(), \"mutator_is_free regions should not be affiliated OLD\");\n+    assert (!r->is_old() || r->is_trash(), \"Except for trash, mutator_is_free regions should not be affiliated OLD\");\n@@ -1268,2 +1653,2 @@\n-    bool move_to_old = _free_sets.capacity_of(OldCollector) < to_reserve_old;\n-    bool move_to_young = _free_sets.capacity_of(Collector) < to_reserve;\n+    bool move_to_old_collector = _partitions.capacity_of(ShenandoahFreeSetPartitionId::OldCollector) < to_reserve_old;\n+    bool move_to_collector = _partitions.capacity_of(ShenandoahFreeSetPartitionId::Collector) < to_reserve;\n@@ -1271,1 +1656,1 @@\n-    if (!move_to_old && !move_to_young) {\n+    if (!move_to_collector && !move_to_old_collector) {\n@@ -1276,1 +1661,4 @@\n-    if (move_to_old) {\n+    if (move_to_old_collector) {\n+      \/\/ We give priority to OldCollector partition because we desire to pack OldCollector regions into higher\n+      \/\/ addresses than Collector regions.  Presumably, OldCollector regions are more \"stable\" and less likely to\n+      \/\/ be collected in the near future.\n@@ -1278,3 +1666,11 @@\n-        \/\/ OLD regions that have available memory are already in the old_collector free set\n-        _free_sets.move_to_set(idx, OldCollector, ac);\n-        log_debug(gc, free)(\"  Shifting region \" SIZE_FORMAT \" from mutator_free to old_collector_free\", idx);\n+        \/\/ OLD regions that have available memory are already in the old_collector free set.\n+        _partitions.move_from_partition_to_partition(idx, ShenandoahFreeSetPartitionId::Mutator,\n+                                                     ShenandoahFreeSetPartitionId::OldCollector, ac);\n+        log_debug(gc)(\"  Shifting region \" SIZE_FORMAT \" from mutator_free to old_collector_free\", idx);\n+        log_debug(gc)(\"  Shifted Mutator range [\" SSIZE_FORMAT \", \" SSIZE_FORMAT \"],\"\n+                      \"  Old Collector range [\" SSIZE_FORMAT \", \" SSIZE_FORMAT \"]\",\n+                      _partitions.leftmost(ShenandoahFreeSetPartitionId::Mutator),\n+                      _partitions.rightmost(ShenandoahFreeSetPartitionId::Mutator),\n+                      _partitions.leftmost(ShenandoahFreeSetPartitionId::OldCollector),\n+                      _partitions.rightmost(ShenandoahFreeSetPartitionId::OldCollector));\n+        old_region_count++;\n@@ -1285,1 +1681,1 @@\n-    if (move_to_young) {\n+    if (move_to_collector) {\n@@ -1287,4 +1683,9 @@\n-      \/\/ they were entirely empty.  I'm not sure I understand the rationale for that.  That alternative behavior would\n-      \/\/ tend to mix survivor objects with ephemeral objects, making it more difficult to reclaim the memory for the\n-      \/\/ ephemeral objects.  It also delays aging of regions, causing promotion in place to be delayed.\n-      _free_sets.move_to_set(idx, Collector, ac);\n+      \/\/ they were entirely empty.  This has the effect of causing new Mutator allocation to reside next to objects\n+      \/\/ that have already survived at least one GC, mixing ephemeral with longer-lived objects in the same region.\n+      \/\/ Any objects that have survived a GC are less likely to immediately become garbage, so a region that contains\n+      \/\/ survivor objects is less likely to be selected for the collection set.  This alternative implementation allows\n+      \/\/ survivor regions to continue accumulating other survivor objects, and makes it more likely that ephemeral objects\n+      \/\/ occupy regions comprised entirely of ephemeral objects.  These regions are highly likely to be included in the next\n+      \/\/ collection set, and they are easily evacuated because they have low density of live objects.\n+      _partitions.move_from_partition_to_partition(idx, ShenandoahFreeSetPartitionId::Mutator,\n+                                                   ShenandoahFreeSetPartitionId::Collector, ac);\n@@ -1292,0 +1693,6 @@\n+      log_debug(gc)(\"  Shifted Mutator range [\" SSIZE_FORMAT \", \" SSIZE_FORMAT \"],\"\n+                    \"  Collector range [\" SSIZE_FORMAT \", \" SSIZE_FORMAT \"]\",\n+                    _partitions.leftmost(ShenandoahFreeSetPartitionId::Mutator),\n+                    _partitions.rightmost(ShenandoahFreeSetPartitionId::Mutator),\n+                    _partitions.leftmost(ShenandoahFreeSetPartitionId::Collector),\n+                    _partitions.rightmost(ShenandoahFreeSetPartitionId::Collector));\n@@ -1296,1 +1703,1 @@\n-    size_t old_reserve = _free_sets.capacity_of(OldCollector);\n+    size_t old_reserve = _partitions.capacity_of(ShenandoahFreeSetPartitionId::OldCollector);\n@@ -1301,4 +1708,4 @@\n-    size_t young_reserve = _free_sets.capacity_of(Collector);\n-    if (young_reserve < to_reserve) {\n-      log_info(gc, free)(\"Wanted \" PROPERFMT \" for young reserve, but only reserved: \" PROPERFMT,\n-                         PROPERFMTARGS(to_reserve), PROPERFMTARGS(young_reserve));\n+    size_t reserve = _partitions.capacity_of(ShenandoahFreeSetPartitionId::Collector);\n+    if (reserve < to_reserve) {\n+      log_debug(gc)(\"Wanted \" PROPERFMT \" for young reserve, but only reserved: \" PROPERFMT,\n+                    PROPERFMTARGS(to_reserve), PROPERFMTARGS(reserve));\n@@ -1309,0 +1716,35 @@\n+void ShenandoahFreeSet::establish_old_collector_alloc_bias() {\n+  ShenandoahHeap* heap = ShenandoahHeap::heap();\n+  shenandoah_assert_heaplocked();\n+\n+  idx_t left_idx = _partitions.leftmost(ShenandoahFreeSetPartitionId::OldCollector);\n+  idx_t right_idx = _partitions.rightmost(ShenandoahFreeSetPartitionId::OldCollector);\n+  idx_t middle = (left_idx + right_idx) \/ 2;\n+  size_t available_in_first_half = 0;\n+  size_t available_in_second_half = 0;\n+\n+  for (idx_t index = left_idx; index < middle; index++) {\n+    if (_partitions.in_free_set(ShenandoahFreeSetPartitionId::OldCollector, index)) {\n+      ShenandoahHeapRegion* r = heap->get_region((size_t) index);\n+      available_in_first_half += r->free();\n+    }\n+  }\n+  for (idx_t index = middle; index <= right_idx; index++) {\n+    if (_partitions.in_free_set(ShenandoahFreeSetPartitionId::OldCollector, index)) {\n+      ShenandoahHeapRegion* r = heap->get_region(index);\n+      available_in_second_half += r->free();\n+    }\n+  }\n+\n+  \/\/ We desire to first consume the sparsely distributed regions in order that the remaining regions are densely packed.\n+  \/\/ Densely packing regions reduces the effort to search for a region that has sufficient memory to satisfy a new allocation\n+  \/\/ request.  Regions become sparsely distributed following a Full GC, which tends to slide all regions to the front of the\n+  \/\/ heap rather than allowing survivor regions to remain at the high end of the heap where we intend for them to congregate.\n+\n+  \/\/ TODO: In the future, we may modify Full GC so that it slides old objects to the end of the heap and young objects to the\n+  \/\/ front of the heap. If this is done, we can always search survivor Collector and OldCollector regions right to left.\n+  _partitions.set_bias_from_left_to_right(ShenandoahFreeSetPartitionId::OldCollector,\n+                                          (available_in_second_half > available_in_first_half));\n+}\n+\n+\n@@ -1336,1 +1778,2 @@\n-    log_debug(gc, free)(\"FreeSet map legend:\"\n+\n+    log_debug(gc)(\"FreeSet map legend:\"\n@@ -1339,7 +1782,11 @@\n-    log_debug(gc, free)(\" mutator free range [\" SIZE_FORMAT \"..\" SIZE_FORMAT \"], \"\n-                       \" collector free range [\" SIZE_FORMAT \"..\" SIZE_FORMAT \"], \"\n-                       \"old collector free range [\" SIZE_FORMAT \"..\" SIZE_FORMAT \"] allocates from %s\",\n-                       _free_sets.leftmost(Mutator), _free_sets.rightmost(Mutator),\n-                       _free_sets.leftmost(Collector), _free_sets.rightmost(Collector),\n-                       _free_sets.leftmost(OldCollector), _free_sets.rightmost(OldCollector),\n-                       _free_sets.alloc_from_left_bias(OldCollector)? \"left to right\": \"right to left\");\n+    log_debug(gc)(\" mutator free range [\" SIZE_FORMAT \"..\" SIZE_FORMAT \"] allocating from %s, \"\n+                  \" collector free range [\" SIZE_FORMAT \"..\" SIZE_FORMAT \"], \"\n+                  \"old collector free range [\" SIZE_FORMAT \"..\" SIZE_FORMAT \"] allocates from %s\",\n+                  _partitions.leftmost(ShenandoahFreeSetPartitionId::Mutator),\n+                  _partitions.rightmost(ShenandoahFreeSetPartitionId::Mutator),\n+                  _partitions.alloc_from_left_bias(ShenandoahFreeSetPartitionId::Mutator)? \"left to right\": \"right to left\",\n+                  _partitions.leftmost(ShenandoahFreeSetPartitionId::Collector),\n+                  _partitions.rightmost(ShenandoahFreeSetPartitionId::Collector),\n+                  _partitions.leftmost(ShenandoahFreeSetPartitionId::OldCollector),\n+                  _partitions.rightmost(ShenandoahFreeSetPartitionId::OldCollector),\n+                  _partitions.alloc_from_left_bias(ShenandoahFreeSetPartitionId::OldCollector)? \"left to right\": \"right to left\");\n@@ -1351,1 +1798,1 @@\n-        log_debug(gc, free)(\" %6u: %s\", i-64, buffer);\n+        log_debug(gc)(\" %6u: %s\", i-64, buffer);\n@@ -1353,2 +1800,1 @@\n-      if (_free_sets.in_free_set(i, Mutator)) {\n-        assert(!r->is_old(), \"Old regions should not be in mutator_free set\");\n+      if (_partitions.in_free_set(ShenandoahFreeSetPartitionId::Mutator, i)) {\n@@ -1356,0 +1802,1 @@\n+        assert(!r->is_old() || r->is_trash(), \"Old regions except trash regions should not be in mutator_free set\");\n@@ -1359,2 +1806,1 @@\n-      } else if (_free_sets.in_free_set(i, Collector)) {\n-        assert(!r->is_old(), \"Old regions should not be in collector_free set\");\n+      } else if (_partitions.in_free_set(ShenandoahFreeSetPartitionId::Collector, i)) {\n@@ -1362,0 +1808,1 @@\n+        assert(!r->is_old() || r->is_trash(), \"Old regions except trash regions should not be in collector_free set\");\n@@ -1365,1 +1812,1 @@\n-      } else if (_free_sets.in_free_set(i, OldCollector)) {\n+      } else if (_partitions.in_free_set(ShenandoahFreeSetPartitionId::OldCollector, i)) {\n@@ -1396,3 +1843,1 @@\n-    log_debug(gc, free)(\" %6u: %s\", (uint) (_heap->num_regions() - remnant), buffer);\n-    size_t total_young = retired_young + retired_young_humongous;\n-    size_t total_old = retired_old + retired_old_humongous;\n+    log_debug(gc)(\" %6u: %s\", (uint) (_heap->num_regions() - remnant), buffer);\n@@ -1408,1 +1853,1 @@\n-      size_t last_idx = 0;\n+      idx_t last_idx = 0;\n@@ -1417,2 +1862,3 @@\n-      for (size_t idx = _free_sets.leftmost(Mutator); idx <= _free_sets.rightmost(Mutator); idx++) {\n-        if (_free_sets.in_free_set(idx, Mutator)) {\n+      for (idx_t idx = _partitions.leftmost(ShenandoahFreeSetPartitionId::Mutator);\n+           idx <= _partitions.rightmost(ShenandoahFreeSetPartitionId::Mutator); idx++) {\n+        if (_partitions.in_free_set(ShenandoahFreeSetPartitionId::Mutator, idx)) {\n@@ -1442,4 +1888,4 @@\n-      assert(free == total_free, \"Sum of free within mutator regions (\" SIZE_FORMAT\n-             \") should match mutator capacity (\" SIZE_FORMAT \") minus mutator used (\" SIZE_FORMAT \")\",\n-             total_free, capacity(), used());\n-\n+      \/\/ Since certain regions that belonged to the Mutator free partition at the time of most recent rebuild may have been\n+      \/\/ retired, the sum of used and capacities within regions that are still in the Mutator free partition may not match\n+      \/\/ my internally tracked values of used() and free().\n+      assert(free == total_free, \"Free memory should match\");\n@@ -1462,2 +1908,3 @@\n-      if (_free_sets.count(Mutator) > 0) {\n-        frag_int = (100 * (total_used \/ _free_sets.count(Mutator)) \/ ShenandoahHeapRegion::region_size_bytes());\n+      if (_partitions.count(ShenandoahFreeSetPartitionId::Mutator) > 0) {\n+        frag_int = (100 * (total_used \/ _partitions.count(ShenandoahFreeSetPartitionId::Mutator))\n+                    \/ ShenandoahHeapRegion::region_size_bytes());\n@@ -1469,1 +1916,2 @@\n-               byte_size_in_proper_unit(total_used), proper_unit_for_byte_size(total_used), _free_sets.count(Mutator));\n+               byte_size_in_proper_unit(total_used), proper_unit_for_byte_size(total_used),\n+               _partitions.count(ShenandoahFreeSetPartitionId::Mutator));\n@@ -1477,2 +1925,3 @@\n-      for (size_t idx = _free_sets.leftmost(Collector); idx <= _free_sets.rightmost(Collector); idx++) {\n-        if (_free_sets.in_free_set(idx, Collector)) {\n+      for (idx_t idx = _partitions.leftmost(ShenandoahFreeSetPartitionId::Collector);\n+           idx <= _partitions.rightmost(ShenandoahFreeSetPartitionId::Collector); idx++) {\n+        if (_partitions.in_free_set(ShenandoahFreeSetPartitionId::Collector, idx)) {\n@@ -1497,2 +1946,3 @@\n-      for (size_t idx = _free_sets.leftmost(OldCollector); idx <= _free_sets.rightmost(OldCollector); idx++) {\n-        if (_free_sets.in_free_set(idx, OldCollector)) {\n+      for (idx_t idx = _partitions.leftmost(ShenandoahFreeSetPartitionId::OldCollector);\n+           idx <= _partitions.rightmost(ShenandoahFreeSetPartitionId::OldCollector); idx++) {\n+        if (_partitions.in_free_set(ShenandoahFreeSetPartitionId::OldCollector, idx)) {\n@@ -1516,2 +1966,0 @@\n-\n-  \/\/ Allocation request is known to satisfy all memory budgeting constraints.\n@@ -1540,16 +1988,0 @@\n-size_t ShenandoahFreeSet::unsafe_peek_free() const {\n-  \/\/ Deliberately not locked, this method is unsafe when free set is modified.\n-\n-  for (size_t index = _free_sets.leftmost(Mutator); index <= _free_sets.rightmost(Mutator); index++) {\n-    if (index < _free_sets.max() && _free_sets.in_free_set(index, Mutator)) {\n-      ShenandoahHeapRegion* r = _heap->get_region(index);\n-      if (r->free() >= MinTLABSize) {\n-        return r->free();\n-      }\n-    }\n-  }\n-\n-  \/\/ It appears that no regions left\n-  return 0;\n-}\n-\n@@ -1557,5 +1989,7 @@\n-  out->print_cr(\"Mutator Free Set: \" SIZE_FORMAT \"\", _free_sets.count(Mutator));\n-  for (size_t index = _free_sets.leftmost(Mutator); index <= _free_sets.rightmost(Mutator); index++) {\n-    if (_free_sets.in_free_set(index, Mutator)) {\n-      _heap->get_region(index)->print_on(out);\n-    }\n+  out->print_cr(\"Mutator Free Set: \" SIZE_FORMAT \"\", _partitions.count(ShenandoahFreeSetPartitionId::Mutator));\n+  idx_t rightmost = _partitions.rightmost(ShenandoahFreeSetPartitionId::Mutator);\n+  for (idx_t index = _partitions.leftmost(ShenandoahFreeSetPartitionId::Mutator); index <= rightmost; ) {\n+    assert(_partitions.in_free_set(ShenandoahFreeSetPartitionId::Mutator, index),\n+           \"Boundaries or find_first_set_bit failed: \" SSIZE_FORMAT, index);\n+    _heap->get_region(index)->print_on(out);\n+    index = _partitions.find_index_of_next_available_region(ShenandoahFreeSetPartitionId::Mutator, index + 1);\n@@ -1563,5 +1997,7 @@\n-  out->print_cr(\"Collector Free Set: \" SIZE_FORMAT \"\", _free_sets.count(Collector));\n-  for (size_t index = _free_sets.leftmost(Collector); index <= _free_sets.rightmost(Collector); index++) {\n-    if (_free_sets.in_free_set(index, Collector)) {\n-      _heap->get_region(index)->print_on(out);\n-    }\n+  out->print_cr(\"Collector Free Set: \" SIZE_FORMAT \"\", _partitions.count(ShenandoahFreeSetPartitionId::Collector));\n+  rightmost = _partitions.rightmost(ShenandoahFreeSetPartitionId::Collector);\n+  for (idx_t index = _partitions.leftmost(ShenandoahFreeSetPartitionId::Collector); index <= rightmost; ) {\n+    assert(_partitions.in_free_set(ShenandoahFreeSetPartitionId::Collector, index),\n+           \"Boundaries or find_first_set_bit failed: \" SSIZE_FORMAT, index);\n+    _heap->get_region(index)->print_on(out);\n+    index = _partitions.find_index_of_next_available_region(ShenandoahFreeSetPartitionId::Collector, index + 1);\n@@ -1570,3 +2006,4 @@\n-    out->print_cr(\"Old Collector Free Set: \" SIZE_FORMAT \"\", _free_sets.count(OldCollector));\n-    for (size_t index = _free_sets.leftmost(OldCollector); index <= _free_sets.rightmost(OldCollector); index++) {\n-      if (_free_sets.in_free_set(index, OldCollector)) {\n+    out->print_cr(\"Old Collector Free Set: \" SIZE_FORMAT \"\", _partitions.count(ShenandoahFreeSetPartitionId::OldCollector));\n+    for (idx_t index = _partitions.leftmost(ShenandoahFreeSetPartitionId::OldCollector);\n+         index <= _partitions.rightmost(ShenandoahFreeSetPartitionId::OldCollector); index++) {\n+      if (_partitions.in_free_set(ShenandoahFreeSetPartitionId::OldCollector, index)) {\n@@ -1579,21 +2016,0 @@\n-\/*\n- * Internal fragmentation metric: describes how fragmented the heap regions are.\n- *\n- * It is derived as:\n- *\n- *               sum(used[i]^2, i=0..k)\n- *   IF = 1 - ------------------------------\n- *              C * sum(used[i], i=0..k)\n- *\n- * ...where k is the number of regions in computation, C is the region capacity, and\n- * used[i] is the used space in the region.\n- *\n- * The non-linearity causes IF to be lower for the cases where the same total heap\n- * used is densely packed. For example:\n- *   a) Heap is completely full  => IF = 0\n- *   b) Heap is half full, first 50% regions are completely full => IF = 0\n- *   c) Heap is half full, each region is 50% full => IF = 1\/2\n- *   d) Heap is quarter full, first 50% regions are completely full => IF = 0\n- *   e) Heap is quarter full, each region is 25% full => IF = 3\/4\n- *   f) Heap has one small object per each region => IF =~ 1\n- *\/\n@@ -1605,8 +2021,10 @@\n-  for (size_t index = _free_sets.leftmost(Mutator); index <= _free_sets.rightmost(Mutator); index++) {\n-    if (_free_sets.in_free_set(index, Mutator)) {\n-      ShenandoahHeapRegion* r = _heap->get_region(index);\n-      size_t used = r->used();\n-      squared += used * used;\n-      linear += used;\n-      count++;\n-    }\n+  idx_t rightmost = _partitions.rightmost(ShenandoahFreeSetPartitionId::Mutator);\n+  for (idx_t index = _partitions.leftmost(ShenandoahFreeSetPartitionId::Mutator); index <= rightmost; ) {\n+    assert(_partitions.in_free_set(ShenandoahFreeSetPartitionId::Mutator, index),\n+           \"Boundaries or find_first_set_bit failed: \" SSIZE_FORMAT, index);\n+    ShenandoahHeapRegion* r = _heap->get_region(index);\n+    size_t used = r->used();\n+    squared += used * used;\n+    linear += used;\n+    count++;\n+    index = _partitions.find_index_of_next_available_region(ShenandoahFreeSetPartitionId::Mutator, index + 1);\n@@ -1623,13 +2041,0 @@\n-\/*\n- * External fragmentation metric: describes how fragmented the heap is.\n- *\n- * It is derived as:\n- *\n- *   EF = 1 - largest_contiguous_free \/ total_free\n- *\n- * For example:\n- *   a) Heap is completely empty => EF = 0\n- *   b) Heap is completely full => EF = 0\n- *   c) Heap is first-half full => EF = 1\/2\n- *   d) Heap is half full, full and empty regions interleave => EF =~ 1\n- *\/\n@@ -1637,1 +2042,1 @@\n-  size_t last_idx = 0;\n+  idx_t last_idx = 0;\n@@ -1643,10 +2048,9 @@\n-  for (size_t index = _free_sets.leftmost(Mutator); index <= _free_sets.rightmost(Mutator); index++) {\n-    if (_free_sets.in_free_set(index, Mutator)) {\n-      ShenandoahHeapRegion* r = _heap->get_region(index);\n-      if (r->is_empty()) {\n-        free += ShenandoahHeapRegion::region_size_bytes();\n-        if (last_idx + 1 == index) {\n-          empty_contig++;\n-        } else {\n-          empty_contig = 1;\n-        }\n+  idx_t rightmost = _partitions.rightmost(ShenandoahFreeSetPartitionId::Mutator);\n+  for (idx_t index = _partitions.leftmost(ShenandoahFreeSetPartitionId::Mutator); index <= rightmost; ) {\n+    assert(_partitions.in_free_set(ShenandoahFreeSetPartitionId::Mutator, index),\n+           \"Boundaries or find_first_set_bit failed: \" SSIZE_FORMAT, index);\n+    ShenandoahHeapRegion* r = _heap->get_region(index);\n+    if (r->is_empty()) {\n+      free += ShenandoahHeapRegion::region_size_bytes();\n+      if (last_idx + 1 == index) {\n+        empty_contig++;\n@@ -1654,1 +2058,1 @@\n-        empty_contig = 0;\n+        empty_contig = 1;\n@@ -1656,3 +2060,2 @@\n-\n-      max_contig = MAX2(max_contig, empty_contig);\n-      last_idx = index;\n+    } else {\n+      empty_contig = 0;\n@@ -1660,0 +2063,3 @@\n+    max_contig = MAX2(max_contig, empty_contig);\n+    last_idx = index;\n+    index = _partitions.find_index_of_next_available_region(ShenandoahFreeSetPartitionId::Mutator, index + 1);\n","filename":"src\/hotspot\/share\/gc\/shenandoah\/shenandoahFreeSet.cpp","additions":1142,"deletions":736,"binary":false,"changes":1878,"status":"modified"},{"patch":"@@ -32,7 +32,9 @@\n-\n-enum ShenandoahFreeMemoryType : uint8_t {\n-  NotFree,\n-  Mutator,\n-  Collector,\n-  OldCollector,\n-  NumFreeSets\n+#include \"gc\/shenandoah\/shenandoahSimpleBitMap.hpp\"\n+\n+\/\/ Each ShenandoahHeapRegion is associated with a ShenandoahFreeSetPartitionId.\n+enum class ShenandoahFreeSetPartitionId : uint8_t {\n+  Mutator,                      \/\/ Region is in the Mutator free set: available memory is available to mutators.\n+  Collector,                    \/\/ Region is in the Collector free set: available memory is reserved for evacuations.\n+  OldCollector,                 \/\/ Region is in the Old Collector free set:\n+                                \/\/    available memory is reserved for old evacuations and for promotions..\n+  NotFree                       \/\/ Region is in no free set: it has no available memory\n@@ -41,1 +43,5 @@\n-class ShenandoahSetsOfFree {\n+\/\/ We do not maintain counts, capacity, or used for regions that are not free.  Informally, if a region is NotFree, it is\n+\/\/ in no partition.  NumPartitions represents the size of an array that may be indexed by Mutator or Collector.\n+#define NumPartitions           (ShenandoahFreeSetPartitionId::NotFree)\n+#define IntNumPartitions     int(ShenandoahFreeSetPartitionId::NotFree)\n+#define UIntNumPartitions   uint(ShenandoahFreeSetPartitionId::NotFree)\n@@ -43,19 +49,5 @@\n-private:\n-  size_t _max;                  \/\/ The maximum number of heap regions\n-  ShenandoahFreeSet* _free_set;\n-  size_t _region_size_bytes;\n-  ShenandoahFreeMemoryType* _membership;\n-  size_t _leftmosts[NumFreeSets];\n-  size_t _rightmosts[NumFreeSets];\n-  size_t _leftmosts_empty[NumFreeSets];\n-  size_t _rightmosts_empty[NumFreeSets];\n-  size_t _capacity_of[NumFreeSets];\n-  size_t _used_by[NumFreeSets];\n-  bool _left_to_right_bias[NumFreeSets];\n-  size_t _region_counts[NumFreeSets];\n-\n-  inline void shrink_bounds_if_touched(ShenandoahFreeMemoryType set, size_t idx);\n-  inline void expand_bounds_maybe(ShenandoahFreeMemoryType set, size_t idx, size_t capacity);\n-\n-  \/\/ Restore all state variables to initial default state.\n-  void clear_internal();\n+\/\/ ShenandoahRegionPartitions provides an abstraction to help organize the implementation of ShenandoahFreeSet.  This\n+\/\/ class implements partitioning of regions into distinct sets.  Each ShenandoahHeapRegion is either in the Mutator free set,\n+\/\/ the Collector free set, or in neither free set (NotFree).  When we speak of a \"free partition\", we mean partitions that\n+\/\/ for which the ShenandoahFreeSetPartitionId is not equal to NotFree.\n+class ShenandoahRegionPartitions {\n@@ -63,0 +55,53 @@\n+private:\n+  const ssize_t _max;           \/\/ The maximum number of heap regions\n+  const size_t _region_size_bytes;\n+  const ShenandoahFreeSet* _free_set;\n+  \/\/ For each partition, we maintain a bitmap of which regions are affiliated with his partition.\n+  ShenandoahSimpleBitMap _membership[UIntNumPartitions];\n+\n+  \/\/ For each partition, we track an interval outside of which a region affiliated with that partition is guaranteed\n+  \/\/ not to be found. This makes searches for free space more efficient.  For each partition p, _leftmosts[p]\n+  \/\/ represents its least index, and its _rightmosts[p] its greatest index. Empty intervals are indicated by the\n+  \/\/ canonical [_max, -1].\n+  ssize_t _leftmosts[UIntNumPartitions];\n+  ssize_t _rightmosts[UIntNumPartitions];\n+\n+  \/\/ Allocation for humongous objects needs to find regions that are entirely empty.  For each partion p, _leftmosts_empty[p]\n+  \/\/ represents the first region belonging to this partition that is completely empty and _rightmosts_empty[p] represents the\n+  \/\/ last region that is completely empty.  If there is no completely empty region in this partition, this is represented\n+  \/\/ by the canonical [_max, -1].\n+  ssize_t _leftmosts_empty[UIntNumPartitions];\n+  ssize_t _rightmosts_empty[UIntNumPartitions];\n+\n+  \/\/ For each partition p, _capacity[p] represents the total amount of memory within the partition at the time\n+  \/\/ of the most recent rebuild, _used[p] represents the total amount of memory that has been allocated within this\n+  \/\/ partition (either already allocated as of the rebuild, or allocated since the rebuild).  _capacity[p] and _used[p]\n+  \/\/ are denoted in bytes.  Note that some regions that had been assigned to a particular partition at rebuild time\n+  \/\/ may have been retired following the rebuild.  The tallies for these regions are still reflected in _capacity[p]\n+  \/\/ and _used[p], even though the region may have been removed from the free set.\n+  size_t _capacity[UIntNumPartitions];\n+  size_t _used[UIntNumPartitions];\n+  size_t _region_counts[UIntNumPartitions];\n+\n+  \/\/ For each partition p, _left_to_right_bias is true iff allocations are normally made from lower indexed regions\n+  \/\/ before higher indexed regions.\n+  bool _left_to_right_bias[UIntNumPartitions];\n+\n+  \/\/ Shrink the intervals associated with partition when region idx is removed from this free set\n+  inline void shrink_interval_if_boundary_modified(ShenandoahFreeSetPartitionId partition, ssize_t idx);\n+\n+  \/\/ Shrink the intervals associated with partition when regions low_idx through high_idx inclusive are removed from this free set\n+  inline void shrink_interval_if_range_modifies_either_boundary(ShenandoahFreeSetPartitionId partition,\n+                                                                ssize_t low_idx, ssize_t high_idx);\n+  inline void expand_interval_if_boundary_modified(ShenandoahFreeSetPartitionId partition, ssize_t idx, size_t capacity);\n+\n+  inline bool is_mutator_partition(ShenandoahFreeSetPartitionId p);\n+  inline bool is_young_collector_partition(ShenandoahFreeSetPartitionId p);\n+  inline bool is_old_collector_partition(ShenandoahFreeSetPartitionId p);\n+  inline bool available_implies_empty(size_t available);\n+\n+#ifndef PRODUCT\n+  void dump_bitmap_row(ssize_t region_idx) const;\n+  void dump_bitmap_range(ssize_t start_region_idx, ssize_t end_region_idx) const;\n+  void dump_bitmap() const;\n+#endif\n@@ -64,2 +109,34 @@\n-  ShenandoahSetsOfFree(size_t max_regions, ShenandoahFreeSet* free_set);\n-  ~ShenandoahSetsOfFree();\n+  ShenandoahRegionPartitions(size_t max_regions, ShenandoahFreeSet* free_set);\n+  ~ShenandoahRegionPartitions() {}\n+\n+  \/\/ Remove all regions from all partitions and reset all bounds\n+  void make_all_regions_unavailable();\n+\n+  \/\/ Set the partition id for a particular region without adjusting interval bounds or usage\/capacity tallies\n+  inline void raw_assign_membership(size_t idx, ShenandoahFreeSetPartitionId p) {\n+    _membership[int(p)].set_bit(idx);\n+  }\n+\n+  \/\/ Set the Mutator intervals, usage, and capacity according to arguments.  Reset the Collector intervals, used, capacity\n+  \/\/ to represent empty Collector free set.  We use this at the end of rebuild_free_set() to avoid the overhead of making\n+  \/\/ many redundant incremental adjustments to the mutator intervals as the free set is being rebuilt.\n+  void establish_mutator_intervals(ssize_t mutator_leftmost, ssize_t mutator_rightmost,\n+                                   ssize_t mutator_leftmost_empty, ssize_t mutator_rightmost_empty,\n+                                   size_t mutator_region_count, size_t mutator_used);\n+\n+  \/\/ Set the OldCollector intervals, usage, and capacity according to arguments.  We use this at the end of rebuild_free_set()\n+  \/\/ to avoid the overhead of making many redundant incremental adjustments to the mutator intervals as the free set is being\n+  \/\/ rebuilt.\n+  void establish_old_collector_intervals(ssize_t old_collector_leftmost, ssize_t old_collector_rightmost,\n+                                         ssize_t old_collector_leftmost_empty, ssize_t old_collector_rightmost_empty,\n+                                         size_t old_collector_region_count, size_t old_collector_used);\n+\n+  \/\/ Retire region idx from within partition, , leaving its capacity and used as part of the original free partition's totals.\n+  \/\/ Requires that region idx is in in the Mutator or Collector partitions.  Hereafter, identifies this region as NotFree.\n+  \/\/ Any remnant of available memory at the time of retirement is added to the original partition's total of used bytes.\n+  void retire_from_partition(ShenandoahFreeSetPartitionId p, ssize_t idx, size_t used_bytes);\n+\n+  \/\/ Retire all regions between low_idx and high_idx inclusive from within partition.  Requires that each region idx is\n+  \/\/ in the same Mutator or Collector partition.  Hereafter, identifies each region as NotFree.   Assumes that each region\n+  \/\/ is now considered fully used, since the region is presumably used to represent a humongous object.\n+  void retire_range_from_partition(ShenandoahFreeSetPartitionId partition, ssize_t low_idx, ssize_t high_idx);\n@@ -67,2 +144,2 @@\n-  \/\/ Make all regions NotFree and reset all bounds\n-  void clear_all();\n+  \/\/ Place region idx into free set which_partition.  Requires that idx is currently NotFree.\n+  void make_free(ssize_t idx, ShenandoahFreeSetPartitionId which_partition, size_t region_capacity);\n@@ -70,2 +147,4 @@\n-  \/\/ Remove or retire region idx from all free sets.  Requires that idx is in a free set.  This does not affect capacity.\n-  void remove_from_free_sets(size_t idx);\n+  \/\/ Place region idx into free partition new_partition, adjusting used and capacity totals for the original and new partition\n+  \/\/ given that available bytes can still be allocated within this region.  Requires that idx is currently not NotFree.\n+  void move_from_partition_to_partition(ssize_t idx, ShenandoahFreeSetPartitionId orig_partition,\n+                                        ShenandoahFreeSetPartitionId new_partition, size_t available);\n@@ -73,2 +152,1 @@\n-  \/\/ Place region idx into free set which_set.  Requires that idx is currently NotFree.\n-  void make_free(size_t idx, ShenandoahFreeMemoryType which_set, size_t region_capacity);\n+  const char* partition_membership_name(ssize_t idx) const;\n@@ -76,2 +154,2 @@\n-  \/\/ Place region idx into free set new_set.  Requires that idx is currently not NotFree.\n-  void move_to_set(size_t idx, ShenandoahFreeMemoryType new_set, size_t region_capacity);\n+  \/\/ Return the index of the next available region >= start_index, or maximum_regions if not found.\n+  inline ssize_t find_index_of_next_available_region(ShenandoahFreeSetPartitionId which_partition, ssize_t start_index) const;\n@@ -79,3 +157,2 @@\n-  \/\/ Returns the ShenandoahFreeMemoryType affiliation of region idx, or NotFree if this region is not currently free.  This does\n-  \/\/ not enforce that free_set membership implies allocation capacity.\n-  inline ShenandoahFreeMemoryType membership(size_t idx) const;\n+  \/\/ Return the index of the previous available region <= last_index, or -1 if not found.\n+  inline ssize_t find_index_of_previous_available_region(ShenandoahFreeSetPartitionId which_partition, ssize_t last_index) const;\n@@ -83,3 +160,25 @@\n-  \/\/ Returns true iff region idx is in the test_set free_set.  Before returning true, asserts that the free\n-  \/\/ set is not empty.  Requires that test_set != NotFree or NumFreeSets.\n-  inline bool in_free_set(size_t idx, ShenandoahFreeMemoryType which_set) const;\n+  \/\/ Return the index of the next available cluster of cluster_size regions >= start_index, or maximum_regions if not found.\n+  inline ssize_t find_index_of_next_available_cluster_of_regions(ShenandoahFreeSetPartitionId which_partition,\n+                                                                 ssize_t start_index, size_t cluster_size) const;\n+\n+  \/\/ Return the index of the previous available cluster of cluster_size regions <= last_index, or -1 if not found.\n+  inline ssize_t find_index_of_previous_available_cluster_of_regions(ShenandoahFreeSetPartitionId which_partition,\n+                                                                     ssize_t last_index, size_t cluster_size) const;\n+\n+  inline bool in_free_set(ShenandoahFreeSetPartitionId which_partition, ssize_t idx) const {\n+    return _membership[int(which_partition)].is_set(idx);\n+  }\n+\n+  \/\/ Returns the ShenandoahFreeSetPartitionId affiliation of region idx, NotFree if this region is not currently in any partition.\n+  \/\/ This does not enforce that free_set membership implies allocation capacity.\n+  inline ShenandoahFreeSetPartitionId membership(ssize_t idx) const;\n+\n+#ifdef ASSERT\n+  \/\/ Returns true iff region idx's membership is which_partition.  If which_partition represents a free set, asserts\n+  \/\/ that the region has allocation capacity.\n+  inline bool partition_id_matches(ssize_t idx, ShenandoahFreeSetPartitionId which_partition) const;\n+#endif\n+\n+  inline size_t max_regions() const { return _max; }\n+\n+  inline size_t region_size_bytes() const { return _region_size_bytes; };\n@@ -89,4 +188,2 @@\n-  \/\/ regions, which are required for humongous allocations and desired for \"very large\" allocations.  A\n-  \/\/ return value of -1 from leftmost() or leftmost_empty() denotes that the corresponding set is empty.\n-  \/\/ In other words:\n-  \/\/   if the requested which_set is empty:\n+  \/\/ regions, which are required for humongous allocations and desired for \"very large\" allocations.\n+  \/\/   if the requested which_partition is empty:\n@@ -96,4 +193,4 @@\n-  inline size_t leftmost(ShenandoahFreeMemoryType which_set) const;\n-  inline size_t rightmost(ShenandoahFreeMemoryType which_set) const;\n-  size_t leftmost_empty(ShenandoahFreeMemoryType which_set);\n-  size_t rightmost_empty(ShenandoahFreeMemoryType which_set);\n+  inline ssize_t leftmost(ShenandoahFreeSetPartitionId which_partition) const;\n+  inline ssize_t rightmost(ShenandoahFreeSetPartitionId which_partition) const;\n+  ssize_t leftmost_empty(ShenandoahFreeSetPartitionId which_partition);\n+  ssize_t rightmost_empty(ShenandoahFreeSetPartitionId which_partition);\n@@ -101,1 +198,1 @@\n-  inline bool is_empty(ShenandoahFreeMemoryType which_set) const;\n+  inline bool is_empty(ShenandoahFreeSetPartitionId which_partition) const;\n@@ -103,1 +200,1 @@\n-  inline void increase_used(ShenandoahFreeMemoryType which_set, size_t bytes);\n+  inline void increase_used(ShenandoahFreeSetPartitionId which_partition, size_t bytes);\n@@ -105,3 +202,3 @@\n-  inline size_t capacity_of(ShenandoahFreeMemoryType which_set) const {\n-    assert (which_set > NotFree && which_set < NumFreeSets, \"selected free set must be valid\");\n-    return _capacity_of[which_set];\n+  inline void set_bias_from_left_to_right(ShenandoahFreeSetPartitionId which_partition, bool value) {\n+    assert (which_partition < NumPartitions, \"selected free set must be valid\");\n+    _left_to_right_bias[int(which_partition)] = value;\n@@ -110,3 +207,3 @@\n-  inline size_t used_by(ShenandoahFreeMemoryType which_set) const {\n-    assert (which_set > NotFree && which_set < NumFreeSets, \"selected free set must be valid\");\n-    return _used_by[which_set];\n+  inline bool alloc_from_left_bias(ShenandoahFreeSetPartitionId which_partition) const {\n+    assert (which_partition < NumPartitions, \"selected free set must be valid\");\n+    return _left_to_right_bias[int(which_partition)];\n@@ -115,1 +212,4 @@\n-  inline size_t max() const { return _max; }\n+  inline size_t capacity_of(ShenandoahFreeSetPartitionId which_partition) const {\n+    assert (which_partition < NumPartitions, \"selected free set must be valid\");\n+    return _capacity[int(which_partition)];\n+  }\n@@ -117,1 +217,4 @@\n-  inline size_t count(ShenandoahFreeMemoryType which_set) const { return _region_counts[which_set]; }\n+  inline size_t used_by(ShenandoahFreeSetPartitionId which_partition) const {\n+    assert (which_partition < NumPartitions, \"selected free set must be valid\");\n+    return _used[int(which_partition)];\n+  }\n@@ -119,3 +222,4 @@\n-  \/\/ Return true iff regions for allocation from this set should be peformed left to right.  Otherwise, allocate\n-  \/\/ from right to left.\n-  inline bool alloc_from_left_bias(ShenandoahFreeMemoryType which_set);\n+  inline size_t available_in(ShenandoahFreeSetPartitionId which_partition) const {\n+    assert (which_partition < NumPartitions, \"selected free set must be valid\");\n+    return _capacity[int(which_partition)] - _used[int(which_partition)];\n+  }\n@@ -123,2 +227,11 @@\n-  \/\/ Determine whether we prefer to allocate from left to right or from right to left for this free-set.\n-  void establish_alloc_bias(ShenandoahFreeMemoryType which_set);\n+  inline void set_capacity_of(ShenandoahFreeSetPartitionId which_partition, size_t value) {\n+    assert (which_partition < NumPartitions, \"selected free set must be valid\");\n+    _capacity[int(which_partition)] = value;\n+  }\n+\n+  inline void set_used_by(ShenandoahFreeSetPartitionId which_partition, size_t value) {\n+    assert (which_partition < NumPartitions, \"selected free set must be valid\");\n+    _used[int(which_partition)] = value;\n+  }\n+\n+  inline size_t count(ShenandoahFreeSetPartitionId which_partition) const { return _region_counts[int(which_partition)]; }\n@@ -140,1 +253,1 @@\n-  \/\/     0 <= lefmost_empty < max and 0 <= rightmost_empty < max\n+  \/\/     0 <= leftmost_empty < max and 0 <= rightmost_empty < max\n@@ -149,0 +262,24 @@\n+\/\/ Publicly, ShenandoahFreeSet represents memory that is available to mutator threads.  The public capacity(), used(),\n+\/\/ and available() methods represent this public notion of memory that is under control of the mutator.  Separately,\n+\/\/ ShenandoahFreeSet also represents memory available to garbage collection activities for compaction purposes.\n+\/\/\n+\/\/ The Shenandoah garbage collector evacuates live objects out of specific regions that are identified as members of the\n+\/\/ collection set (cset).\n+\/\/\n+\/\/ The ShenandoahFreeSet tries to colocate survivor objects (objects that have been evacuated at least once) at the\n+\/\/ high end of memory.  New mutator allocations are taken from the low end of memory.  Within the mutator's range of regions,\n+\/\/ humongous allocations are taken from the lowest addresses, and LAB (local allocation buffers) and regular shared allocations\n+\/\/ are taken from the higher address of the mutator's range of regions.  This approach allows longer lasting survivor regions\n+\/\/ to congregate at the top of the heap and longer lasting humongous regions to congregate at the bottom of the heap, with\n+\/\/ short-lived frequently evacuated regions occupying the middle of the heap.\n+\/\/\n+\/\/ Mutator and garbage collection activities tend to scramble the content of regions.  Twice, during each GC pass, we rebuild\n+\/\/ the free set in an effort to restore the efficient segregation of Collector and Mutator regions:\n+\/\/\n+\/\/  1. At the start of evacuation, we know exactly how much memory is going to be evacuated, and this guides our\n+\/\/     sizing of the Collector free set.\n+\/\/\n+\/\/  2. At the end of GC, we have reclaimed all of the memory that was spanned by the cset.  We rebuild here to make\n+\/\/     sure there is enough memory reserved at the high end of memory to hold the objects that might need to be evacuated\n+\/\/     during the next GC pass.\n+\n@@ -152,3 +289,2 @@\n-  ShenandoahSetsOfFree _free_sets;\n-\n-  HeapWord* try_allocate_in(ShenandoahHeapRegion* region, ShenandoahAllocRequest& req, bool& in_new_region);\n+  ShenandoahRegionPartitions _partitions;\n+  size_t _retired_old_regions;\n@@ -158,3 +294,5 @@\n-  \/\/ Satisfy young-generation or single-generation collector allocation request req by finding memory that matches\n-  \/\/ affiliation, which either equals req.affiliation or FREE.  We know req.is_young().\n-  HeapWord* allocate_with_affiliation(ShenandoahAffiliation affiliation, ShenandoahAllocRequest& req, bool& in_new_region);\n+  \/\/ Return the address of memory allocated, setting in_new_region to true iff the allocation is taken\n+  \/\/ from a region that was previously empty.  Return nullptr if memory could not be allocated.\n+  inline HeapWord* allocate_from_partition_with_affiliation(ShenandoahFreeSetPartitionId which_partition,\n+                                                            ShenandoahAffiliation affiliation,\n+                                                            ShenandoahAllocRequest& req, bool& in_new_region);\n@@ -162,3 +300,4 @@\n-  \/\/ Satisfy allocation request req by finding memory that matches affiliation, which either equals req.affiliation\n-  \/\/ or FREE. We know req.is_old().\n-  HeapWord* allocate_old_with_affiliation(ShenandoahAffiliation affiliation, ShenandoahAllocRequest& req, bool& in_new_region);\n+  \/\/ We re-evaluate the left-to-right allocation bias whenever _alloc_bias_weight is less than zero.  Each time\n+  \/\/ we allocate an object, we decrement the count of this value.  Each time we re-evaluate whether to allocate\n+  \/\/ from right-to-left or left-to-right, we reset the value of this counter to _InitialAllocBiasWeight.\n+  ssize_t _alloc_bias_weight;\n@@ -166,6 +305,8 @@\n-  \/\/ While holding the heap lock, allocate memory for a single object which is to be entirely contained\n-  \/\/ within a single HeapRegion as characterized by req.  The req.size() value is known to be less than or\n-  \/\/ equal to ShenandoahHeapRegion::humongous_threshold_words().  The caller of allocate_single is responsible\n-  \/\/ for registering the resulting object and setting the remembered set card values as appropriate.  The\n-  \/\/ most common case is that we are allocating a PLAB in which case object registering and card dirtying\n-  \/\/ is managed after the PLAB is divided into individual objects.\n+  const ssize_t _InitialAllocBiasWeight = 256;\n+\n+  HeapWord* try_allocate_in(ShenandoahHeapRegion* region, ShenandoahAllocRequest& req, bool& in_new_region);\n+\n+  \/\/ While holding the heap lock, allocate memory for a single object or LAB  which is to be entirely contained\n+  \/\/ within a single HeapRegion as characterized by req.\n+  \/\/\n+  \/\/ Precondition: req.size() <= ShenandoahHeapRegion::humongous_threshold_words().\n@@ -173,0 +314,6 @@\n+\n+  \/\/ While holding the heap lock, allocate memory for a humongous object which spans one or more regions that\n+  \/\/ were previously empty.  Regions that represent humongous objects are entirely dedicated to the humongous\n+  \/\/ object.  No other objects are packed into these regions.\n+  \/\/\n+  \/\/ Precondition: req.size() > ShenandoahHeapRegion::humongous_threshold_words().\n@@ -175,0 +322,6 @@\n+  \/\/ Change region r from the Mutator partition to the GC's Collector or OldCollector partition.  This requires that the\n+  \/\/ region is entirely empty.\n+  \/\/\n+  \/\/ Typical usage: During evacuation, the GC may find it needs more memory than had been reserved at the start of evacuation to\n+  \/\/ hold evacuated objects.  If this occurs and memory is still available in the Mutator's free set, we will flip a region from\n+  \/\/ the Mutator free set into the Collector or OldCollector free set.\n@@ -179,1 +332,0 @@\n-\n@@ -182,3 +334,22 @@\n-  bool can_allocate_from(ShenandoahHeapRegion *r) const;\n-  bool can_allocate_from(size_t idx) const;\n-  bool has_alloc_capacity(ShenandoahHeapRegion *r) const;\n+  \/\/ Returns true iff this region is entirely available, either because it is empty() or because it has been found to represent\n+  \/\/ immediate trash and we'll be able to immediately recycle it.  Note that we cannot recycle immediate trash if\n+  \/\/ concurrent weak root processing is in progress.\n+  inline bool can_allocate_from(ShenandoahHeapRegion *r) const;\n+  inline bool can_allocate_from(size_t idx) const;\n+\n+  inline bool has_alloc_capacity(ShenandoahHeapRegion *r) const;\n+\n+  size_t transfer_empty_regions_from_collector_set_to_mutator_set(ShenandoahFreeSetPartitionId which_collector,\n+                                                                  size_t max_xfer_regions,\n+                                                                  size_t& bytes_transferred);\n+  size_t transfer_non_empty_regions_from_collector_set_to_mutator_set(ShenandoahFreeSetPartitionId collector_id,\n+                                                                      size_t max_xfer_regions,\n+                                                                      size_t& bytes_transferred);\n+\n+\n+  \/\/ Determine whether we prefer to allocate from left to right or from right to left within the OldCollector free-set.\n+  void establish_old_collector_alloc_bias();\n+\n+  \/\/ Set max_capacity for young and old generations\n+  void establish_generation_sizes(size_t young_region_count, size_t old_region_count);\n+  size_t get_usable_free_words(size_t free_bytes) const;\n@@ -189,2 +360,3 @@\n-  size_t alloc_capacity(ShenandoahHeapRegion *r) const;\n-  size_t alloc_capacity(size_t idx) const;\n+  \/\/ Public because ShenandoahRegionPartitions assertions require access.\n+  inline size_t alloc_capacity(ShenandoahHeapRegion *r) const;\n+  inline size_t alloc_capacity(size_t idx) const;\n@@ -193,0 +365,8 @@\n+\n+  \/\/ Examine the existing free set representation, capturing the current state into var arguments:\n+  \/\/\n+  \/\/ young_cset_regions is the number of regions currently in the young cset if we are starting to evacuate, or zero\n+  \/\/   old_cset_regions is the number of regions currently in the old cset if we are starting a mixed evacuation, or zero\n+  \/\/   first_old_region is the index of the first region that is part of the OldCollector set\n+  \/\/    last_old_region is the index of the last region that is part of the OldCollector set\n+  \/\/   old_region_count is the number of regions in the OldCollector set that have memory available to be allocated\n@@ -197,8 +377,9 @@\n-  \/\/ hold the results of evacuating to young-gen and to old-gen.  These quantities, stored in reserves for their,\n-  \/\/ respective generations, are consulted prior to rebuilding the free set (ShenandoahFreeSet) in preparation for\n-  \/\/ evacuation.  When the free set is rebuilt, we make sure to reserve sufficient memory in the collector and\n-  \/\/ old_collector sets to hold evacuations, if have_evacuation_reserves is true.  The other time we rebuild the free\n-  \/\/ set is at the end of GC, as we prepare to idle GC until the next trigger.  In this case, have_evacuation_reserves\n-  \/\/ is false because we don't yet know how much memory will need to be evacuated in the next GC cycle.  When\n-  \/\/ have_evacuation_reserves is false, the free set rebuild operation reserves for the collector and old_collector sets\n-  \/\/ based on alternative mechanisms, such as ShenandoahEvacReserve, ShenandoahOldEvacReserve, and\n+  \/\/ hold the results of evacuating to young-gen and to old-gen, and have_evacuation_reserves should be true.\n+  \/\/ These quantities, stored as reserves for their respective generations, are consulted prior to rebuilding\n+  \/\/ the free set (ShenandoahFreeSet) in preparation for evacuation.  When the free set is rebuilt, we make sure\n+  \/\/ to reserve sufficient memory in the collector and old_collector sets to hold evacuations.\n+  \/\/\n+  \/\/ We also rebuild the free set at the end of GC, as we prepare to idle GC until the next trigger.  In this case,\n+  \/\/ have_evacuation_reserves is false because we don't yet know how much memory will need to be evacuated in the\n+  \/\/ next GC cycle.  When have_evacuation_reserves is false, the free set rebuild operation reserves for the collector\n+  \/\/ and old_collector sets based on alternative mechanisms, such as ShenandoahEvacReserve, ShenandoahOldEvacReserve, and\n@@ -208,5 +389,19 @@\n-  void rebuild(size_t young_cset_regions, size_t old_cset_regions, bool have_evacuation_reserves = false);\n-\n-  void move_collector_sets_to_mutator(size_t cset_regions);\n-\n-  void add_old_collector_free_region(ShenandoahHeapRegion* region);\n+  \/\/\n+  \/\/ young_cset_regions is the number of regions currently in the young cset if we are starting to evacuate, or zero\n+  \/\/   old_cset_regions is the number of regions currently in the old cset if we are starting a mixed evacuation, or zero\n+  \/\/    num_old_regions is the number of old-gen regions that have available memory for further allocations (excluding old cset)\n+  \/\/ have_evacuation_reserves is true iff the desired values of young-gen and old-gen evacuation reserves and old-gen\n+  \/\/                    promotion reserve have been precomputed (and can be obtained by invoking\n+  \/\/                    <generation>->get_evacuation_reserve() or old_gen->get_promoted_reserve()\n+  void finish_rebuild(size_t young_cset_regions, size_t old_cset_regions, size_t num_old_regions,\n+                      bool have_evacuation_reserves = false);\n+\n+  \/\/ When a region is promoted in place, we add the region's available memory if it is greater than plab_min_size()\n+  \/\/ into the old collector partition by invoking this method.\n+  void add_promoted_in_place_region_to_old_collector(ShenandoahHeapRegion* region);\n+\n+  \/\/ Move up to cset_regions number of regions from being available to the collector to being available to the mutator.\n+  \/\/\n+  \/\/ Typical usage: At the end of evacuation, when the collector no longer needs the regions that had been reserved\n+  \/\/ for evacuation, invoke this to make regions available for mutator allocations.\n+  void move_regions_from_collector_to_mutator(size_t cset_regions);\n@@ -218,2 +413,2 @@\n-  inline size_t capacity()  const { return _free_sets.capacity_of(Mutator); }\n-  inline size_t used()      const { return _free_sets.used_by(Mutator);     }\n+  inline size_t capacity()  const { return _partitions.capacity_of(ShenandoahFreeSetPartitionId::Mutator); }\n+  inline size_t used()      const { return _partitions.used_by(ShenandoahFreeSetPartitionId::Mutator);     }\n@@ -228,0 +423,21 @@\n+  \/*\n+   * Internal fragmentation metric: describes how fragmented the heap regions are.\n+   *\n+   * It is derived as:\n+   *\n+   *               sum(used[i]^2, i=0..k)\n+   *   IF = 1 - ------------------------------\n+   *              C * sum(used[i], i=0..k)\n+   *\n+   * ...where k is the number of regions in computation, C is the region capacity, and\n+   * used[i] is the used space in the region.\n+   *\n+   * The non-linearity causes IF to be lower for the cases where the same total heap\n+   * used is densely packed. For example:\n+   *   a) Heap is completely full  => IF = 0\n+   *   b) Heap is half full, first 50% regions are completely full => IF = 0\n+   *   c) Heap is half full, each region is 50% full => IF = 1\/2\n+   *   d) Heap is quarter full, first 50% regions are completely full => IF = 0\n+   *   e) Heap is quarter full, each region is 25% full => IF = 3\/4\n+   *   f) Heap has one small object per each region => IF =~ 1\n+   *\/\n@@ -229,0 +445,14 @@\n+\n+  \/*\n+   * External fragmentation metric: describes how fragmented the heap is.\n+   *\n+   * It is derived as:\n+   *\n+   *   EF = 1 - largest_contiguous_free \/ total_free\n+   *\n+   * For example:\n+   *   a) Heap is completely empty => EF = 0\n+   *   b) Heap is completely full => EF = 0\n+   *   c) Heap is first-half full => EF = 1\/2\n+   *   d) Heap is half full, full and empty regions interleave => EF =~ 1\n+   *\/\n@@ -233,0 +463,9 @@\n+  \/\/ This function places all regions that have allocation capacity into the mutator partition, or if the region\n+  \/\/ is already affiliated with old, into the old collector partition, identifying regions that have no allocation\n+  \/\/ capacity as NotFree.  Capture the modified state of the freeset into var arguments:\n+  \/\/\n+  \/\/ young_cset_regions is the number of regions currently in the young cset if we are starting to evacuate, or zero\n+  \/\/   old_cset_regions is the number of regions currently in the old cset if we are starting a mixed evacuation, or zero\n+  \/\/   first_old_region is the index of the first region that is part of the OldCollector set\n+  \/\/    last_old_region is the index of the last region that is part of the OldCollector set\n+  \/\/   old_region_count is the number of regions in the OldCollector set that have memory available to be allocated\n@@ -235,1 +474,5 @@\n-  void reserve_regions(size_t young_reserve, size_t old_reserve);\n+\n+  \/\/ Ensure that Collector has at least to_reserve bytes of available memory, and OldCollector has at least old_reserve\n+  \/\/ bytes of available memory.  On input, old_region_count holds the number of regions already present in the\n+  \/\/ OldCollector partition.  Upon return, old_region_count holds the updated number of regions in the OldCollector partition.\n+  void reserve_regions(size_t to_reserve, size_t old_reserve, size_t &old_region_count);\n","filename":"src\/hotspot\/share\/gc\/shenandoah\/shenandoahFreeSet.hpp","additions":347,"deletions":104,"binary":false,"changes":451,"status":"modified"},{"patch":"@@ -1197,1 +1197,1 @@\n-    heap->free_set()->rebuild(young_cset_regions, old_cset_regions);\n+    heap->free_set()->finish_rebuild(young_cset_regions, old_cset_regions, num_old);\n","filename":"src\/hotspot\/share\/gc\/shenandoah\/shenandoahFullGC.cpp","additions":1,"deletions":1,"binary":false,"changes":2,"status":"modified"},{"patch":"@@ -757,1 +757,1 @@\n-    heap->free_set()->rebuild(young_cset_regions, old_cset_regions, true);\n+    heap->free_set()->finish_rebuild(young_cset_regions, old_cset_regions, num_old, true);\n@@ -967,1 +967,1 @@\n-void ShenandoahGeneration::increase_capacity(size_t increment) {\n+size_t ShenandoahGeneration::increase_capacity(size_t increment) {\n@@ -984,0 +984,1 @@\n+  return _max_capacity;\n@@ -986,1 +987,7 @@\n-void ShenandoahGeneration::decrease_capacity(size_t decrement) {\n+size_t ShenandoahGeneration::set_capacity(size_t byte_size) {\n+  shenandoah_assert_heaplocked_or_safepoint();\n+  _max_capacity = byte_size;\n+  return _max_capacity;\n+}\n+\n+size_t ShenandoahGeneration::decrease_capacity(size_t decrement) {\n@@ -1009,0 +1016,1 @@\n+  return _max_capacity;\n","filename":"src\/hotspot\/share\/gc\/shenandoah\/shenandoahGeneration.cpp","additions":11,"deletions":3,"binary":false,"changes":14,"status":"modified"},{"patch":"@@ -150,4 +150,7 @@\n-  \/\/ These methods change the capacity of the region by adding or subtracting the given number of bytes from the current\n-  \/\/ capacity.\n-  void increase_capacity(size_t increment);\n-  void decrease_capacity(size_t decrement);\n+  \/\/ These methods change the capacity of the generation by adding or subtracting the given number of bytes from the current\n+  \/\/ capacity, returning the capacity of the generation following the change.\n+  size_t increase_capacity(size_t increment);\n+  size_t decrease_capacity(size_t decrement);\n+\n+  \/\/ Set the capacity of the generation, returning the value set\n+  size_t set_capacity(size_t byte_size);\n","filename":"src\/hotspot\/share\/gc\/shenandoah\/shenandoahGeneration.hpp","additions":7,"deletions":4,"binary":false,"changes":11,"status":"modified"},{"patch":"@@ -526,0 +526,1 @@\n+    heap->notify_gc_progress();\n@@ -602,0 +603,1 @@\n+    heap->notify_gc_progress();\n","filename":"src\/hotspot\/share\/gc\/shenandoah\/shenandoahGenerationalControlThread.cpp","additions":2,"deletions":0,"binary":false,"changes":2,"status":"modified"},{"patch":"@@ -212,1 +212,1 @@\n-    _heap->free_set()->add_old_collector_free_region(region);\n+    _heap->free_set()->add_promoted_in_place_region_to_old_collector(region);\n","filename":"src\/hotspot\/share\/gc\/shenandoah\/shenandoahGenerationalEvacuationTask.cpp","additions":1,"deletions":1,"binary":false,"changes":2,"status":"modified"},{"patch":"@@ -785,3 +785,6 @@\n-      \/\/ We cannot transfer any more regions than will be reclaimed when the existing collection set is recycled, because\n-      \/\/ we need the reclaimed collection set regions to replenish the collector reserves\n-      _heap->free_set()->move_collector_sets_to_mutator(cset_regions);\n+\n+      \/\/ Now that evacuation is done, we can reassign any regions that had been reserved to hold the results of evacuation\n+      \/\/ to the mutator free set.  At the end of GC, we will have cset_regions newly evacuated fully empty regions from\n+      \/\/ which we will be able to replenish the Collector free set and the OldCollector free set in preparation for the\n+      \/\/ next GC cycle.\n+      _heap->free_set()->move_regions_from_collector_to_mutator(cset_regions);\n","filename":"src\/hotspot\/share\/gc\/shenandoah\/shenandoahGenerationalHeap.cpp","additions":6,"deletions":3,"binary":false,"changes":9,"status":"modified"},{"patch":"@@ -414,1 +414,1 @@\n-    _free_set->rebuild(young_cset_regions, old_cset_regions);\n+    _free_set->finish_rebuild(young_cset_regions, old_cset_regions, num_old);\n@@ -990,1 +990,2 @@\n-    if (result == nullptr && !req.is_lab_alloc() && get_gc_no_progress_count() > ShenandoahNoProgressThreshold) {\n+    \/\/ gc_no_progress_count is incremented following each degen or full GC that fails to achieve is_good_progress().\n+    if ((result == nullptr) && !req.is_lab_alloc() && (get_gc_no_progress_count() > ShenandoahNoProgressThreshold)) {\n@@ -992,0 +993,1 @@\n+      req.set_actual_size(0);\n@@ -995,13 +997,13 @@\n-    \/\/ Block until control thread reacted, then retry allocation.\n-    \/\/\n-    \/\/ It might happen that one of the threads requesting allocation would unblock\n-    \/\/ way later after GC happened, only to fail the second allocation, because\n-    \/\/ other threads have already depleted the free storage. In this case, a better\n-    \/\/ strategy is to try again, as long as GC makes progress (or until at least\n-    \/\/ one full GC has completed).\n-    size_t original_count = shenandoah_policy()->full_gc_count();\n-    while (result == nullptr\n-        && (get_gc_no_progress_count() == 0 || original_count == shenandoah_policy()->full_gc_count())) {\n-      control_thread()->handle_alloc_failure(req, true);\n-      result = allocate_memory_under_lock(req, in_new_region);\n-    }\n+    if (result == nullptr) {\n+      \/\/ Block until control thread reacted, then retry allocation.\n+      \/\/\n+      \/\/ It might happen that one of the threads requesting allocation would unblock\n+      \/\/ way later after GC happened, only to fail the second allocation, because\n+      \/\/ other threads have already depleted the free storage. In this case, a better\n+      \/\/ strategy is to try again, until at least one full GC has completed.\n+      \/\/\n+      \/\/ Stop retrying and return nullptr to cause OOMError exception if our allocation failed even after:\n+      \/\/   a) We experienced a GC that had good progress, or\n+      \/\/   b) We experienced at least one Full GC (whether or not it had good progress)\n+      \/\/\n+      \/\/ TODO: Consider GLOBAL GC rather than Full GC to remediate OOM condition: https:\/\/bugs.openjdk.org\/browse\/JDK-8335910\n@@ -1009,4 +1011,16 @@\n-    if (log_is_enabled(Debug, gc, alloc)) {\n-      ResourceMark rm;\n-      log_debug(gc, alloc)(\"Thread: %s, Result: \" PTR_FORMAT \", Request: %s, Size: \" SIZE_FORMAT \", Original: \" SIZE_FORMAT \", Latest: \" SIZE_FORMAT,\n-                           Thread::current()->name(), p2i(result), req.type_string(), req.size(), original_count, get_gc_no_progress_count());\n+      size_t original_count = shenandoah_policy()->full_gc_count();\n+      while ((result == nullptr) && (original_count == shenandoah_policy()->full_gc_count())) {\n+        control_thread()->handle_alloc_failure(req, true);\n+        result = allocate_memory_under_lock(req, in_new_region);\n+      }\n+      if (result != nullptr) {\n+        \/\/ If our allocation request has been satisifed after it initially failed, we count this as good gc progress\n+        notify_gc_progress();\n+      }\n+      if (log_is_enabled(Debug, gc, alloc)) {\n+        ResourceMark rm;\n+        log_debug(gc, alloc)(\"Thread: %s, Result: \" PTR_FORMAT \", Request: %s, Size: \" SIZE_FORMAT\n+                             \", Original: \" SIZE_FORMAT \", Latest: \" SIZE_FORMAT,\n+                             Thread::current()->name(), p2i(result), req.type_string(), req.size(),\n+                             original_count, get_gc_no_progress_count());\n+      }\n@@ -2324,3 +2338,6 @@\n-      \/\/ We cannot transfer any more regions than will be reclaimed when the existing collection set is recycled, because\n-      \/\/ we need the reclaimed collection set regions to replenish the collector reserves\n-      _heap->free_set()->move_collector_sets_to_mutator(cset_regions);\n+\n+      \/\/ Now that evacuation is done, we can reassign any regions that had been reserved to hold the results of evacuation\n+      \/\/ to the mutator free set.  At the end of GC, we will have cset_regions newly evacuated fully empty regions from\n+      \/\/ which we will be able to replenish the Collector free set and the OldCollector free set in preparation for the\n+      \/\/ next GC cycle.\n+      _heap->free_set()->move_regions_from_collector_to_mutator(cset_regions);\n@@ -2444,1 +2461,1 @@\n-  _free_set->rebuild(young_cset_regions, old_cset_regions);\n+  _free_set->finish_rebuild(young_cset_regions, old_cset_regions, old_region_count);\n","filename":"src\/hotspot\/share\/gc\/shenandoah\/shenandoahHeap.cpp","additions":40,"deletions":23,"binary":false,"changes":63,"status":"modified"},{"patch":"@@ -121,1 +121,1 @@\n-\/\/ behavior previously performed as a side effect of make_regular_bypass().\n+\/\/ behavior previously performed as a side effect of make_regular_bypass().  This is used by Full GC\n@@ -124,0 +124,1 @@\n+  assert(!ShenandoahHeap::heap()->mode()->is_generational(), \"Only call if non-generational\");\n@@ -131,7 +132,0 @@\n-       ShenandoahHeap* heap = ShenandoahHeap::heap();\n-       if (heap->mode()->is_generational()) {\n-         if (is_old()) {\n-           heap->old_generation()->decrement_affiliated_region_count();\n-         }\n-         heap->young_generation()->increment_affiliated_region_count();\n-       }\n","filename":"src\/hotspot\/share\/gc\/shenandoah\/shenandoahHeapRegion.cpp","additions":2,"deletions":8,"binary":false,"changes":10,"status":"modified"},{"patch":"@@ -78,1 +78,1 @@\n-    req.set_actual_size(size);\n+    \/\/ We do not req.set_actual_size() here.  The caller sets it.\n","filename":"src\/hotspot\/share\/gc\/shenandoah\/shenandoahHeapRegion.inline.hpp","additions":1,"deletions":1,"binary":false,"changes":2,"status":"modified"},{"patch":"@@ -479,1 +479,1 @@\n-    heap->free_set()->rebuild(cset_young_regions, cset_old_regions);\n+    heap->free_set()->finish_rebuild(cset_young_regions, cset_old_regions, num_old);\n","filename":"src\/hotspot\/share\/gc\/shenandoah\/shenandoahOldGeneration.cpp","additions":1,"deletions":1,"binary":false,"changes":2,"status":"modified"},{"patch":"@@ -0,0 +1,291 @@\n+\/*\n+ * Copyright Amazon.com Inc. or its affiliates. All Rights Reserved.\n+ * DO NOT ALTER OR REMOVE COPYRIGHT NOTICES OR THIS FILE HEADER.\n+ *\n+ * This code is free software; you can redistribute it and\/or modify it\n+ * under the terms of the GNU General Public License version 2 only, as\n+ * published by the Free Software Foundation.\n+ *\n+ * This code is distributed in the hope that it will be useful, but WITHOUT\n+ * ANY WARRANTY; without even the implied warranty of MERCHANTABILITY or\n+ * FITNESS FOR A PARTICULAR PURPOSE.  See the GNU General Public License\n+ * version 2 for more details (a copy is included in the LICENSE file that\n+ * accompanied this code).\n+ *\n+ * You should have received a copy of the GNU General Public License version\n+ * 2 along with this work; if not, write to the Free Software Foundation,\n+ * Inc., 51 Franklin St, Fifth Floor, Boston, MA 02110-1301 USA.\n+ *\n+ * Please contact Oracle, 500 Oracle Parkway, Redwood Shores, CA 94065 USA\n+ * or visit www.oracle.com if you need additional information or have any\n+ * questions.\n+ *\n+ *\/\n+\n+#include \"precompiled.hpp\"\n+#include \"gc\/shenandoah\/shenandoahSimpleBitMap.hpp\"\n+\n+ShenandoahSimpleBitMap::ShenandoahSimpleBitMap(size_t num_bits) :\n+    _num_bits(num_bits),\n+    _num_words(align_up(num_bits, BitsPerWord) \/ BitsPerWord),\n+    _bitmap(NEW_C_HEAP_ARRAY(uintx, _num_words, mtGC))\n+{\n+  clear_all();\n+}\n+\n+ShenandoahSimpleBitMap::~ShenandoahSimpleBitMap() {\n+  if (_bitmap != nullptr) {\n+    FREE_C_HEAP_ARRAY(uintx, _bitmap);\n+  }\n+}\n+\n+size_t ShenandoahSimpleBitMap::count_leading_ones(idx_t start_idx) const {\n+  assert((start_idx >= 0) && (start_idx < _num_bits), \"precondition\");\n+  size_t array_idx = start_idx >> LogBitsPerWord;\n+  uintx element_bits = _bitmap[array_idx];\n+  uintx bit_number = start_idx & right_n_bits(LogBitsPerWord);\n+  uintx mask = ~right_n_bits(bit_number);\n+  size_t counted_ones = 0;\n+  while ((element_bits & mask) == mask) {\n+    \/\/ All bits numbered >= bit_number are set\n+    size_t found_ones = BitsPerWord - bit_number;\n+    counted_ones += found_ones;\n+    \/\/ Dead code: do not need to compute: start_idx += found_ones;\n+    \/\/ Strength reduction:                array_idx = (start_idx >> LogBitsPerWord)\n+    array_idx++;\n+    element_bits = _bitmap[array_idx];\n+    \/\/ Constant folding:                  bit_number = start_idx & right_n_bits(LogBitsPerWord);\n+    bit_number = 0;\n+    \/\/ Constant folding:                  mask = ~right_n_bits(bit_number);\n+    mask = ~0;\n+  }\n+\n+  \/\/ Add in number of consecutive ones starting with the_bit and including more significant bits and return result\n+  uintx aligned = element_bits >> bit_number;\n+  uintx complement = ~aligned;\n+  return counted_ones + count_trailing_zeros<uintx>(complement);\n+}\n+\n+size_t ShenandoahSimpleBitMap::count_trailing_ones(idx_t last_idx) const {\n+  assert((last_idx >= 0) && (last_idx < _num_bits), \"precondition\");\n+  size_t array_idx = last_idx >> LogBitsPerWord;\n+  uintx element_bits = _bitmap[array_idx];\n+  uintx bit_number = last_idx & right_n_bits(LogBitsPerWord);\n+  \/\/ All ones from bit 0 to the_bit\n+  uintx mask = right_n_bits(bit_number + 1);\n+  size_t counted_ones = 0;\n+  while ((element_bits & mask) == mask) {\n+    \/\/ All bits numbered <= bit_number are set\n+    size_t found_ones = bit_number + 1;\n+    counted_ones += found_ones;\n+    \/\/ Dead code: do not need to compute: last_idx -= found_ones;\n+    array_idx--;\n+    element_bits = _bitmap[array_idx];\n+    \/\/ Constant folding:                  bit_number = last_idx & right_n_bits(LogBitsPerWord);\n+    bit_number = BitsPerWord - 1;\n+    \/\/ Constant folding:                  mask = right_n_bits(bit_number + 1);\n+    mask = ~0;\n+  }\n+\n+  \/\/ Add in number of consecutive ones starting with the_bit and including less significant bits and return result\n+  uintx aligned = element_bits << (BitsPerWord - (bit_number + 1));\n+  uintx complement = ~aligned;\n+  return counted_ones + count_leading_zeros<uintx>(complement);\n+}\n+\n+bool ShenandoahSimpleBitMap::is_forward_consecutive_ones(idx_t start_idx, idx_t count) const {\n+  while (count > 0) {\n+    assert((start_idx >= 0) && (start_idx < _num_bits), \"precondition: start_idx: \" SSIZE_FORMAT \", count: \" SSIZE_FORMAT,\n+           start_idx, count);\n+    assert(start_idx + count <= (idx_t) _num_bits, \"precondition\");\n+    size_t array_idx = start_idx >> LogBitsPerWord;\n+    uintx bit_number = start_idx & right_n_bits(LogBitsPerWord);\n+    uintx element_bits = _bitmap[array_idx];\n+    uintx bits_to_examine  = BitsPerWord - bit_number;\n+    element_bits >>= bit_number;\n+    uintx complement = ~element_bits;\n+    uintx trailing_ones;\n+    if (complement != 0) {\n+      trailing_ones = count_trailing_zeros<uintx>(complement);\n+    } else {\n+      trailing_ones = bits_to_examine;\n+    }\n+    if (trailing_ones >= (uintx) count) {\n+      return true;\n+    } else if (trailing_ones == bits_to_examine) {\n+      start_idx += bits_to_examine;\n+      count -= bits_to_examine;\n+      \/\/ Repeat search with smaller goal\n+    } else {\n+      return false;\n+    }\n+  }\n+  return true;\n+}\n+\n+bool ShenandoahSimpleBitMap::is_backward_consecutive_ones(idx_t last_idx, idx_t count) const {\n+  while (count > 0) {\n+    assert((last_idx >= 0) && (last_idx < _num_bits), \"precondition\");\n+    assert(last_idx - count >= -1, \"precondition\");\n+    size_t array_idx = last_idx >> LogBitsPerWord;\n+    uintx bit_number = last_idx & right_n_bits(LogBitsPerWord);\n+    uintx element_bits = _bitmap[array_idx];\n+    uintx bits_to_examine = bit_number + 1;\n+    element_bits <<= (BitsPerWord - bits_to_examine);\n+    uintx complement = ~element_bits;\n+    uintx leading_ones;\n+    if (complement != 0) {\n+      leading_ones = count_leading_zeros<uintx>(complement);\n+    } else {\n+      leading_ones = bits_to_examine;\n+    }\n+    if (leading_ones >= (uintx) count) {\n+      return true;\n+    } else if (leading_ones == bits_to_examine) {\n+      last_idx -= leading_ones;\n+      count -= leading_ones;\n+      \/\/ Repeat search with smaller goal\n+    } else {\n+      return false;\n+    }\n+  }\n+  return true;\n+}\n+\n+idx_t ShenandoahSimpleBitMap::find_first_consecutive_set_bits(idx_t beg, idx_t end, size_t num_bits) const {\n+  assert((beg >= 0) && (beg < _num_bits), \"precondition\");\n+\n+  \/\/ Stop looking if there are not num_bits remaining in probe space.\n+  idx_t start_boundary = end - num_bits;\n+  if (beg > start_boundary) {\n+    return end;\n+  }\n+  uintx array_idx = beg >> LogBitsPerWord;\n+  uintx bit_number = beg & right_n_bits(LogBitsPerWord);\n+  uintx element_bits = _bitmap[array_idx];\n+  if (bit_number > 0) {\n+    uintx mask_out = right_n_bits(bit_number);\n+    element_bits &= ~mask_out;\n+  }\n+\n+  \/\/ The following loop minimizes the number of spans probed in order to find num_bits consecutive bits.\n+  \/\/ For example, if bit_number = beg = 0, num_bits = 8, and element bits equals 00111111_11000000_00000000_10011000B,\n+  \/\/ we need only 3 probes to find the match at bit offset 22.\n+  \/\/\n+  \/\/ Let beg = 0\n+  \/\/ element_bits = 00111111_11000000_00000000_10011000B;\n+  \/\/                                           ________   (the searched span)\n+  \/\/                                           ^   ^  ^- bit_number = beg = 0\n+  \/\/                                           |   +-- next_start_candidate_1 (where next 1 is found)\n+  \/\/                                           +------ next_start_candidate_2 (start of the trailing 1s within span)\n+  \/\/ Let beg = 7\n+  \/\/ element_bits = 00111111_11000000_00000000_10011000B;\n+  \/\/                          ^       ^_________   (the searched span)\n+  \/\/                          |       |        ^- bit_number = beg = 7\n+  \/\/                          |       +---------- next_start_candidate_2 (there are no trailing 1s within span)\n+  \/\/                          +------------------ next_start_candidate_1 (where next 1 is found)\n+  \/\/ Let beg = 22\n+  \/\/ Let beg = 22\n+  \/\/ element_bits = 00111111_11000001_11111100_10011000B;\n+  \/\/                  _________   (the searched span)\n+  \/\/                          ^- bit_number = beg = 18\n+  \/\/ Here, is_forward_consecutive_ones(22, 8) succeeds and we report the match\n+\n+  while (true) {\n+    if (element_bits == 0) {\n+      \/\/ move to the next element\n+      beg += BitsPerWord - bit_number;\n+      if (beg > start_boundary) {\n+        \/\/ No match found.\n+        return end;\n+      }\n+      array_idx++;\n+      bit_number = 0;\n+      element_bits = _bitmap[array_idx];\n+    } else if (is_forward_consecutive_ones(beg, num_bits)) {\n+      return beg;\n+    } else {\n+      \/\/ There is at least one non-zero bit within the masked element_bits. Arrange to skip over bits that\n+      \/\/ cannot be part of a consecutive-ones match.\n+      uintx next_set_bit = count_trailing_zeros<uintx>(element_bits);\n+      uintx next_start_candidate_1 = (array_idx << LogBitsPerWord) + next_set_bit;\n+\n+      \/\/ There is at least one zero bit in this span. Align the next probe at the start of trailing ones for probed span,\n+      \/\/ or align at end of span if this span has no trailing ones.\n+      size_t trailing_ones = count_trailing_ones(beg + num_bits - 1);\n+      uintx next_start_candidate_2 = beg + num_bits - trailing_ones;\n+\n+      beg = MAX2(next_start_candidate_1, next_start_candidate_2);\n+      if (beg > start_boundary) {\n+        \/\/ No match found.\n+        return end;\n+      }\n+      array_idx = beg >> LogBitsPerWord;\n+      element_bits = _bitmap[array_idx];\n+      bit_number = beg & right_n_bits(LogBitsPerWord);\n+      if (bit_number > 0) {\n+        size_t mask_out = right_n_bits(bit_number);\n+        element_bits &= ~mask_out;\n+      }\n+    }\n+  }\n+}\n+\n+idx_t ShenandoahSimpleBitMap::find_last_consecutive_set_bits(const idx_t beg, idx_t end, const size_t num_bits) const {\n+\n+  assert((end >= 0) && (end < _num_bits), \"precondition\");\n+\n+  \/\/ Stop looking if there are not num_bits remaining in probe space.\n+  idx_t last_boundary = beg + num_bits;\n+  if (end < last_boundary) {\n+    return beg;\n+  }\n+\n+  size_t array_idx = end >> LogBitsPerWord;\n+  uintx bit_number = end & right_n_bits(LogBitsPerWord);\n+  uintx element_bits = _bitmap[array_idx];\n+  if (bit_number < BitsPerWord - 1) {\n+    uintx mask_in = right_n_bits(bit_number + 1);\n+    element_bits &= mask_in;\n+  }\n+\n+  \/\/ See comment in find_first_consecutive_set_bits to understand how this loop works.\n+  while (true) {\n+    if (element_bits == 0) {\n+      \/\/ move to the previous element\n+      end -= bit_number + 1;\n+      if (end < last_boundary) {\n+        \/\/ No match found.\n+        return beg;\n+      }\n+      array_idx--;\n+      bit_number = BitsPerWord - 1;\n+      element_bits = _bitmap[array_idx];\n+    } else if (is_backward_consecutive_ones(end, num_bits)) {\n+      return end + 1 - num_bits;\n+    } else {\n+      \/\/ There is at least one non-zero bit within the masked element_bits. Arrange to skip over bits that\n+      \/\/ cannot be part of a consecutive-ones match.\n+      uintx next_set_bit = BitsPerWord - (1 + count_leading_zeros<uintx>(element_bits));\n+      uintx next_last_candidate_1 = (array_idx << LogBitsPerWord) + next_set_bit;\n+\n+      \/\/ There is at least one zero bit in this span.  Align the next probe at the end of leading ones for probed span,\n+      \/\/ or align before start of span if this span has no leading ones.\n+      size_t leading_ones = count_leading_ones(end - (num_bits - 1));\n+      uintx next_last_candidate_2 = end - (num_bits - leading_ones);\n+\n+      end = MIN2(next_last_candidate_1, next_last_candidate_2);\n+      if (end < last_boundary) {\n+        \/\/ No match found.\n+        return beg;\n+      }\n+      array_idx = end >> LogBitsPerWord;\n+      bit_number = end & right_n_bits(LogBitsPerWord);\n+      element_bits = _bitmap[array_idx];\n+      if (bit_number < BitsPerWord - 1){\n+        size_t mask_in = right_n_bits(bit_number + 1);\n+        element_bits &= mask_in;\n+      }\n+    }\n+  }\n+}\n","filename":"src\/hotspot\/share\/gc\/shenandoah\/shenandoahSimpleBitMap.cpp","additions":291,"deletions":0,"binary":false,"changes":291,"status":"added"},{"patch":"@@ -0,0 +1,170 @@\n+\/*\n+ * Copyright Amazon.com Inc. or its affiliates. All Rights Reserved.\n+ * DO NOT ALTER OR REMOVE COPYRIGHT NOTICES OR THIS FILE HEADER.\n+ *\n+ * This code is free software; you can redistribute it and\/or modify it\n+ * under the terms of the GNU General Public License version 2 only, as\n+ * published by the Free Software Foundation.\n+ *\n+ * This code is distributed in the hope that it will be useful, but WITHOUT\n+ * ANY WARRANTY; without even the implied warranty of MERCHANTABILITY or\n+ * FITNESS FOR A PARTICULAR PURPOSE.  See the GNU General Public License\n+ * version 2 for more details (a copy is included in the LICENSE file that\n+ * accompanied this code).\n+ *\n+ * You should have received a copy of the GNU General Public License version\n+ * 2 along with this work; if not, write to the Free Software Foundation,\n+ * Inc., 51 Franklin St, Fifth Floor, Boston, MA 02110-1301 USA.\n+ *\n+ * Please contact Oracle, 500 Oracle Parkway, Redwood Shores, CA 94065 USA\n+ * or visit www.oracle.com if you need additional information or have any\n+ * questions.\n+ *\n+ *\/\n+\n+#ifndef SHARE_GC_SHENANDOAH_SHENANDOAHSIMPLEBITMAP_HPP\n+#define SHARE_GC_SHENANDOAH_SHENANDOAHSIMPLEBITMAP_HPP\n+\n+#include <cstddef>\n+\n+#include \"gc\/shenandoah\/shenandoahAsserts.hpp\"\n+\n+\/\/ TODO: Merge the enhanced capabilities of ShenandoahSimpleBitMap into src\/hotspot\/share\/utilities\/bitMap.hpp\n+\/\/       and deprecate ShenandoahSimpleBitMap.  The key enhanced capabilities to be integrated include:\n+\/\/\n+\/\/   1. Allow searches from high to low memory (when biasing allocations towards the top of the heap)\n+\/\/   2. Allow searches for clusters of contiguous set bits (to expedite allocation for humongous objects)\n+\/\/\n+\/\/ idx_t is defined here as ssize_t.  In src\/hotspot\/share\/utiliities\/bitMap.hpp, idx is defined as size_t.\n+\/\/ This is a significant incompatibility.\n+\/\/\n+\/\/ The API and internal implementation of ShenandoahSimpleBitMap and ShenandoahRegionPartitions use idx_t to\n+\/\/ represent index, even though index is \"inherently\" unsigned.  There are two reasons for this choice:\n+\/\/  1. We use -1 as a sentinel value to represent empty partitions.  This same value may be used to represent\n+\/\/     failure to find a previous set bit or previous range of set bits.\n+\/\/  2. Certain loops are written most naturally if the iterator, which may hold the sentinel -1 value, can be\n+\/\/     declared as signed and the terminating condition can be < 0.\n+\n+typedef ssize_t idx_t;\n+\n+\/\/ ShenandoahSimpleBitMap resembles CHeapBitMap but adds missing support for find_first_consecutive_set_bits() and\n+\/\/ find_last_consecutive_set_bits.  An alternative refactoring of code would subclass CHeapBitMap, but this might\n+\/\/ break abstraction rules, because efficient implementation requires assumptions about superclass internals that\n+\/\/ might be violatee through future software maintenance.\n+class ShenandoahSimpleBitMap {\n+  const idx_t _num_bits;\n+  const size_t _num_words;\n+  uintx* const _bitmap;\n+\n+public:\n+  ShenandoahSimpleBitMap(size_t num_bits);\n+\n+  ~ShenandoahSimpleBitMap();\n+\n+  void clear_all() {\n+    for (size_t i = 0; i < _num_words; i++) {\n+      _bitmap[i] = 0;\n+    }\n+  }\n+\n+private:\n+\n+  \/\/ Count consecutive ones in forward order, starting from start_idx.  Requires that there is at least one zero\n+  \/\/ between start_idx and index value (_num_bits - 1), inclusive.\n+  size_t count_leading_ones(idx_t start_idx) const;\n+\n+  \/\/ Count consecutive ones in reverse order, starting from last_idx.  Requires that there is at least one zero\n+  \/\/ between last_idx and index value zero, inclusive.\n+  size_t count_trailing_ones(idx_t last_idx) const;\n+\n+  bool is_forward_consecutive_ones(idx_t start_idx, idx_t count) const;\n+  bool is_backward_consecutive_ones(idx_t last_idx, idx_t count) const;\n+\n+public:\n+\n+  inline idx_t aligned_index(idx_t idx) const {\n+    assert((idx >= 0) && (idx < _num_bits), \"precondition\");\n+    idx_t array_idx = idx & ~right_n_bits(LogBitsPerWord);\n+    return array_idx;\n+  }\n+\n+  inline constexpr idx_t alignment() const {\n+    return BitsPerWord;\n+  }\n+\n+  \/\/ For testing\n+  inline idx_t size() const {\n+    return _num_bits;\n+  }\n+\n+  \/\/ Return the word that holds idx bit and its neighboring bits.\n+  inline uintx bits_at(idx_t idx) const {\n+    assert((idx >= 0) && (idx < _num_bits), \"precondition\");\n+    idx_t array_idx = idx >> LogBitsPerWord;\n+    return _bitmap[array_idx];\n+  }\n+\n+  inline void set_bit(idx_t idx) {\n+    assert((idx >= 0) && (idx < _num_bits), \"precondition\");\n+    size_t array_idx = idx >> LogBitsPerWord;\n+    uintx bit_number = idx & right_n_bits(LogBitsPerWord);\n+    uintx the_bit = nth_bit(bit_number);\n+    _bitmap[array_idx] |= the_bit;\n+  }\n+\n+  inline void clear_bit(idx_t idx) {\n+    assert((idx >= 0) && (idx < _num_bits), \"precondition\");\n+    assert(idx >= 0, \"precondition\");\n+    size_t array_idx = idx >> LogBitsPerWord;\n+    uintx bit_number = idx & right_n_bits(LogBitsPerWord);\n+    uintx the_bit = nth_bit(bit_number);\n+    _bitmap[array_idx] &= ~the_bit;\n+  }\n+\n+  inline bool is_set(idx_t idx) const {\n+    assert((idx >= 0) && (idx < _num_bits), \"precondition\");\n+    assert(idx >= 0, \"precondition\");\n+    size_t array_idx = idx >> LogBitsPerWord;\n+    uintx bit_number = idx & right_n_bits(LogBitsPerWord);\n+    uintx the_bit = nth_bit(bit_number);\n+    return (_bitmap[array_idx] & the_bit)? true: false;\n+  }\n+\n+  \/\/ Return the index of the first set bit in the range [beg, size()), or size() if none found.\n+  \/\/ precondition: beg and end form a valid range for the bitmap.\n+  inline idx_t find_first_set_bit(idx_t beg) const;\n+\n+  \/\/ Return the index of the first set bit in the range [beg, end), or end if none found.\n+  \/\/ precondition: beg and end form a valid range for the bitmap.\n+  inline idx_t find_first_set_bit(idx_t beg, idx_t end) const;\n+\n+  \/\/ Return the index of the last set bit in the range (-1, end], or -1 if none found.\n+  \/\/ precondition: beg and end form a valid range for the bitmap.\n+  inline idx_t find_last_set_bit(idx_t end) const;\n+\n+  \/\/ Return the index of the last set bit in the range (beg, end], or beg if none found.\n+  \/\/ precondition: beg and end form a valid range for the bitmap.\n+  inline idx_t find_last_set_bit(idx_t beg, idx_t end) const;\n+\n+  \/\/ Return the start index of the first run of <num_bits> consecutive set bits for which the first set bit is within\n+  \/\/   the range [beg, size()), or size() if the run of <num_bits> is not found within this range.\n+  \/\/ precondition: beg is within the valid range for the bitmap.\n+  inline idx_t find_first_consecutive_set_bits(idx_t beg, size_t num_bits) const;\n+\n+  \/\/ Return the start index of the first run of <num_bits> consecutive set bits for which the first set bit is within\n+  \/\/   the range [beg, end), or end if the run of <num_bits> is not found within this range.\n+  \/\/ precondition: beg and end form a valid range for the bitmap.\n+  idx_t find_first_consecutive_set_bits(idx_t beg, idx_t end, size_t num_bits) const;\n+\n+  \/\/ Return the start index of the last run of <num_bits> consecutive set bits for which the entire run of set bits is within\n+  \/\/ the range (-1, end], or -1 if the run of <num_bits> is not found within this range.\n+  \/\/ precondition: end is within the valid range for the bitmap.\n+  inline idx_t find_last_consecutive_set_bits(idx_t end, size_t num_bits) const;\n+\n+  \/\/ Return the start index of the first run of <num_bits> consecutive set bits for which the entire run of set bits is within\n+  \/\/ the range (beg, end], or beg if the run of <num_bits> is not found within this range.\n+  \/\/ precondition: beg and end form a valid range for the bitmap.\n+  idx_t find_last_consecutive_set_bits(idx_t beg, idx_t end, size_t num_bits) const;\n+};\n+\n+#endif \/\/ SHARE_GC_SHENANDOAH_SHENANDOAHSIMPLEBITMAP_HPP\n","filename":"src\/hotspot\/share\/gc\/shenandoah\/shenandoahSimpleBitMap.hpp","additions":170,"deletions":0,"binary":false,"changes":170,"status":"added"},{"patch":"@@ -0,0 +1,100 @@\n+\/*\n+ * Copyright Amazon.com Inc. or its affiliates. All Rights Reserved.\n+ * DO NOT ALTER OR REMOVE COPYRIGHT NOTICES OR THIS FILE HEADER.\n+ *\n+ * This code is free software; you can redistribute it and\/or modify it\n+ * under the terms of the GNU General Public License version 2 only, as\n+ * published by the Free Software Foundation.\n+ *\n+ * This code is distributed in the hope that it will be useful, but WITHOUT\n+ * ANY WARRANTY; without even the implied warranty of MERCHANTABILITY or\n+ * FITNESS FOR A PARTICULAR PURPOSE.  See the GNU General Public License\n+ * version 2 for more details (a copy is included in the LICENSE file that\n+ * accompanied this code).\n+ *\n+ * You should have received a copy of the GNU General Public License version\n+ * 2 along with this work; if not, write to the Free Software Foundation,\n+ * Inc., 51 Franklin St, Fifth Floor, Boston, MA 02110-1301 USA.\n+ *\n+ * Please contact Oracle, 500 Oracle Parkway, Redwood Shores, CA 94065 USA\n+ * or visit www.oracle.com if you need additional information or have any\n+ * questions.\n+ *\n+ *\/\n+\n+#ifndef SHARE_GC_SHENANDOAH_SHENANDOAHSIMPLEBITMAP_INLINE_HPP\n+#define SHARE_GC_SHENANDOAH_SHENANDOAHSIMPLEBITMAP_INLINE_HPP\n+\n+#include \"gc\/shenandoah\/shenandoahSimpleBitMap.hpp\"\n+\n+inline idx_t ShenandoahSimpleBitMap::find_first_set_bit(idx_t beg, idx_t end) const {\n+  assert((beg >= 0) && (beg < _num_bits), \"precondition\");\n+  assert((end > beg) && (end <= _num_bits), \"precondition\");\n+  do {\n+    size_t array_idx = beg >> LogBitsPerWord;\n+    uintx bit_number = beg & right_n_bits(LogBitsPerWord);\n+    uintx element_bits = _bitmap[array_idx];\n+    if (bit_number > 0) {\n+      uintx mask_out = right_n_bits(bit_number);\n+      element_bits &= ~mask_out;\n+    }\n+    if (element_bits) {\n+      \/\/ The next set bit is here.  Find first set bit >= bit_number;\n+      uintx aligned = element_bits >> bit_number;\n+      uintx first_set_bit = count_trailing_zeros<uintx>(aligned);\n+      idx_t candidate_result = (array_idx * BitsPerWord) + bit_number + first_set_bit;\n+      return (candidate_result < end)? candidate_result: end;\n+    } else {\n+      \/\/ Next bit is not here.  Try the next array element\n+      beg += BitsPerWord - bit_number;\n+    }\n+  } while (beg < end);\n+  return end;\n+}\n+\n+inline idx_t ShenandoahSimpleBitMap::find_first_set_bit(idx_t beg) const {\n+  assert((beg >= 0) && (beg < size()), \"precondition\");\n+  return find_first_set_bit(beg, size());\n+}\n+\n+inline idx_t ShenandoahSimpleBitMap::find_last_set_bit(idx_t beg, idx_t end) const {\n+  assert((end >= 0) && (end < _num_bits), \"precondition\");\n+  assert((beg >= -1) && (beg < end), \"precondition\");\n+  do {\n+    idx_t array_idx = end >> LogBitsPerWord;\n+    uintx bit_number = end & right_n_bits(LogBitsPerWord);\n+    uintx element_bits = _bitmap[array_idx];\n+    if (bit_number < BitsPerWord - 1){\n+      uintx mask_in = right_n_bits(bit_number + 1);\n+      element_bits &= mask_in;\n+    }\n+    if (element_bits) {\n+      \/\/ The prev set bit is here.  Find the first set bit <= bit_number\n+      uintx aligned = element_bits << (BitsPerWord - (bit_number + 1));\n+      uintx first_set_bit = count_leading_zeros<uintx>(aligned);\n+      idx_t candidate_result = array_idx * BitsPerWord + (bit_number - first_set_bit);\n+      return (candidate_result > beg)? candidate_result: beg;\n+    } else {\n+      \/\/ Next bit is not here.  Try the previous array element\n+      end -= (bit_number + 1);\n+    }\n+  } while (end > beg);\n+  return beg;\n+}\n+\n+inline idx_t ShenandoahSimpleBitMap::find_last_set_bit(idx_t end) const {\n+  assert((end >= 0) && (end < _num_bits), \"precondition\");\n+  return find_last_set_bit(-1, end);\n+}\n+\n+inline idx_t ShenandoahSimpleBitMap::find_first_consecutive_set_bits(idx_t beg, size_t num_bits) const {\n+  assert((beg >= 0) && (beg < _num_bits), \"precondition\");\n+  return find_first_consecutive_set_bits(beg, size(), num_bits);\n+}\n+\n+inline idx_t ShenandoahSimpleBitMap::find_last_consecutive_set_bits(idx_t end, size_t num_bits) const {\n+  assert((end >= 0) && (end < _num_bits), \"precondition\");\n+  return find_last_consecutive_set_bits((idx_t) -1, end, num_bits);\n+}\n+\n+#endif \/\/ SHARE_GC_SHENANDOAH_SHENANDOAHSIMPLEBITMAP_INLINE_HPP\n","filename":"src\/hotspot\/share\/gc\/shenandoah\/shenandoahSimpleBitMap.inline.hpp","additions":100,"deletions":0,"binary":false,"changes":100,"status":"added"},{"patch":"@@ -353,1 +353,1 @@\n-  size_t _used, _committed, _garbage, _regions, _humongous_waste;\n+  size_t _used, _committed, _garbage, _regions, _humongous_waste, _trashed_regions;\n@@ -355,1 +355,2 @@\n-  ShenandoahCalculateRegionStatsClosure() : _used(0), _committed(0), _garbage(0), _regions(0), _humongous_waste(0) {};\n+  ShenandoahCalculateRegionStatsClosure() :\n+      _used(0), _committed(0), _garbage(0), _regions(0), _humongous_waste(0), _trashed_regions(0) {};\n@@ -364,0 +365,3 @@\n+    if (r->is_trash()) {\n+      _trashed_regions++;\n+    }\n@@ -377,0 +381,1 @@\n+  size_t non_trashed_span() const { return (_regions - _trashed_regions) * ShenandoahHeapRegion::region_size_bytes(); }\n@@ -428,3 +433,5 @@\n-    guarantee(stats.span() <= generation_capacity,\n-              \"%s: generation (%s) size spanned by regions (\" SIZE_FORMAT \") must not exceed current capacity (\" PROPERFMT \")\",\n-              label, generation->name(), stats.regions(), PROPERFMTARGS(generation_capacity));\n+    guarantee(stats.non_trashed_span() <= generation_capacity,\n+              \"%s: generation (%s) size spanned by regions (\" SIZE_FORMAT \") * region size (\" PROPERFMT\n+              \") must not exceed current capacity (\" PROPERFMT \")\",\n+              label, generation->name(), stats.regions(), PROPERFMTARGS(ShenandoahHeapRegion::region_size_bytes()),\n+              PROPERFMTARGS(generation_capacity));\n","filename":"src\/hotspot\/share\/gc\/shenandoah\/shenandoahVerifier.cpp","additions":12,"deletions":5,"binary":false,"changes":17,"status":"modified"},{"patch":"@@ -94,4 +94,2 @@\n-gc\/shenandoah\/TestHumongousThreshold.java#default 8327000 generic-all\n-gc\/shenandoah\/TestHumongousThreshold.java#16b 8327000 generic-all\n-gc\/shenandoah\/TestHumongousThreshold.java#generational 8327000 generic-all\n-gc\/shenandoah\/TestHumongousThreshold.java#generational-16b 8327000 generic-all\n+gc\/shenandoah\/TestAllocIntArrays.java#iu-aggressive 8289220 generic-all\n+gc\/shenandoah\/TestAllocIntArrays.java#aggressive 8289220 generic-all\n","filename":"test\/hotspot\/jtreg\/ProblemList.txt","additions":2,"deletions":4,"binary":false,"changes":6,"status":"modified"},{"patch":"@@ -210,0 +210,2 @@\n+        \/\/ Each allocated int array is assumed to consume 16 bytes for alignment and header, plus\n+        \/\/  an average of 4 * the average number of elements in the array.\n@@ -213,0 +215,3 @@\n+        \/\/ Repeatedly, allocate an array of int having between 0 and 384K elements, until we have\n+        \/\/ allocated approximately TARGET_MB.  The largest allocated array consumes 384K*4 + 16, which is 1.5 M,\n+        \/\/ which is well below the heap size of 1g.\n","filename":"test\/hotspot\/jtreg\/gc\/shenandoah\/TestAllocIntArrays.java","additions":5,"deletions":0,"binary":false,"changes":5,"status":"modified"},{"patch":"@@ -45,2 +45,4 @@\n-    final int array_size = 512 * 1024;   \/\/ 512K entries\n-    BigInteger array[] = new BigInteger[array_size];\n+    final int ArraySize = 512 * 1024;   \/\/ 512K entries\n+    final int BitsInBigInteger = 128;\n+    final int RefillIterations = 64;\n+    BigInteger array[] = new BigInteger[ArraySize];\n@@ -49,2 +51,2 @@\n-    for (int i = 0; i < array_size; i++) {\n-      array[i] = new BigInteger(128, r);\n+    for (int i = 0; i < ArraySize; i++) {\n+      array[i] = new BigInteger(BitsInBigInteger, r);\n@@ -53,5 +55,5 @@\n-    for (int refill_count = 0; refill_count < 192; refill_count++) {\n-      \/\/ Each refill repopulates array_size randomly selected elements within array\n-      for (int i = 0; i < array_size; i++) {\n-        int replace_index = r.nextInt(array_size);\n-        int derive_index = r.nextInt(array_size);\n+    for (int refill_count = 0; refill_count < RefillIterations; refill_count++) {\n+      \/\/ Each refill repopulates ArraySize randomly selected elements within array\n+      for (int i = 0; i < ArraySize; i++) {\n+        int replace_index = r.nextInt(ArraySize);\n+        int derive_index = r.nextInt(ArraySize);\n@@ -103,2 +105,2 @@\n-            \"-Xms256m\",\n-            \"-Xmx256m\",\n+            \"-Xms96m\",\n+            \"-Xmx96m\",\n","filename":"test\/hotspot\/jtreg\/gc\/shenandoah\/generational\/TestOldGrowthTriggers.java","additions":13,"deletions":11,"binary":false,"changes":24,"status":"modified"},{"patch":"@@ -57,0 +57,5 @@\n+                \/\/ If we experience OutOfMemoryError during our attempt to instantiate NastyThread, we'll abort\n+                \/\/ main and will not print \"All good\".  We'll also report a non-zero termination code.  In the\n+                \/\/ case that the previously instantiated NastyThread accumulated more than SheanndoahNoProgressThreshold\n+                \/\/ unproductive GC cycles before failing, the main thread may not try a Full GC before it experiences\n+                \/\/ OutOfMemoryError exception.\n@@ -60,0 +65,3 @@\n+                \/\/ Having joined thread, we know the memory consumed by thread is now garbage, and will eventually be\n+                \/\/ collected.  Some or all of that memory may have been promoted, so we may need to perform a Full GC\n+                \/\/ in order to reclaim it quickly.\n","filename":"test\/hotspot\/jtreg\/gc\/shenandoah\/oom\/TestThreadFailure.java","additions":8,"deletions":0,"binary":false,"changes":8,"status":"modified"}]}