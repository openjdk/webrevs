{"files":[{"patch":"@@ -174,1 +174,1 @@\n-      heap->old_heuristics()->prime_collection_set(collection_set);\n+      heap->old_generation()->heuristics()->prime_collection_set(collection_set);\n","filename":"src\/hotspot\/share\/gc\/shenandoah\/heuristics\/shenandoahGenerationalHeuristics.cpp","additions":1,"deletions":1,"binary":false,"changes":2,"status":"modified"},{"patch":"@@ -128,1 +128,1 @@\n-  ShenandoahOldHeuristics* old_heuristics = heap->old_heuristics();\n+  ShenandoahOldHeuristics* old_heuristics = heap->old_generation()->heuristics();\n","filename":"src\/hotspot\/share\/gc\/shenandoah\/heuristics\/shenandoahYoungHeuristics.cpp","additions":1,"deletions":1,"binary":false,"changes":2,"status":"modified"},{"patch":"@@ -83,0 +83,3 @@\n+  \/\/ True if this request is trying to copy any object from young to old (promote).\n+  bool _is_promotion;\n+\n@@ -88,1 +91,1 @@\n-  ShenandoahAllocRequest(size_t _min_size, size_t _requested_size, Type _alloc_type, ShenandoahAffiliation affiliation) :\n+  ShenandoahAllocRequest(size_t _min_size, size_t _requested_size, Type _alloc_type, ShenandoahAffiliation affiliation, bool is_promotion = false) :\n@@ -90,1 +93,1 @@\n-          _actual_size(0), _waste(0), _alloc_type(_alloc_type), _affiliation(affiliation)\n+          _actual_size(0), _waste(0), _alloc_type(_alloc_type), _affiliation(affiliation), _is_promotion(is_promotion)\n@@ -109,1 +112,5 @@\n-  static inline ShenandoahAllocRequest for_shared_gc(size_t requested_size, ShenandoahAffiliation affiliation) {\n+  static inline ShenandoahAllocRequest for_shared_gc(size_t requested_size, ShenandoahAffiliation affiliation, bool is_promotion = false) {\n+    if (is_promotion) {\n+      assert(affiliation == ShenandoahAffiliation::OLD_GENERATION, \"Should only promote to old generation\");\n+      return ShenandoahAllocRequest(0, requested_size, _alloc_shared_gc, affiliation, true);\n+    }\n@@ -215,0 +222,4 @@\n+\n+  bool is_promotion() const {\n+    return _is_promotion;\n+  }\n","filename":"src\/hotspot\/share\/gc\/shenandoah\/shenandoahAllocRequest.hpp","additions":14,"deletions":3,"binary":false,"changes":17,"status":"modified"},{"patch":"@@ -34,0 +34,1 @@\n+#include \"gc\/shenandoah\/shenandoahScanRemembered.inline.hpp\"\n@@ -138,1 +139,1 @@\n-      _heap->retire_plab(plab);\n+      ShenandoahGenerationalHeap::heap()->retire_plab(plab);\n","filename":"src\/hotspot\/share\/gc\/shenandoah\/shenandoahBarrierSet.cpp","additions":2,"deletions":1,"binary":false,"changes":3,"status":"modified"},{"patch":"@@ -37,0 +37,1 @@\n+#include \"gc\/shenandoah\/shenandoahGeneration.hpp\"\n@@ -261,1 +262,2 @@\n-                              !(ShenandoahHeap::heap()->is_gc_generation_young() && ShenandoahHeap::heap()->heap_region_containing(value)->is_old()));\n+                              !(ShenandoahHeap::heap()->active_generation()->is_young() &&\n+                              ShenandoahHeap::heap()->heap_region_containing(value)->is_old()));\n","filename":"src\/hotspot\/share\/gc\/shenandoah\/shenandoahBarrierSet.inline.hpp","additions":3,"deletions":1,"binary":false,"changes":4,"status":"modified"},{"patch":"@@ -294,1 +294,1 @@\n-    if (r->affiliation() != FREE) {\n+    if (r->is_affiliated()) {\n@@ -1199,6 +1199,0 @@\n-    \/\/ We defer generation resizing actions until after cset regions have been recycled.  We do this even following an\n-    \/\/ abbreviated cycle.\n-    if (heap->mode()->is_generational()) {\n-      ShenandoahGenerationalFullGC::balance_generations_after_rebuilding_free_set();\n-      ShenandoahGenerationalFullGC::rebuild_remembered_set(heap);\n-    }\n@@ -1210,0 +1204,7 @@\n+\n+  \/\/ We defer generation resizing actions until after cset regions have been recycled.  We do this even following an\n+  \/\/ abbreviated cycle.\n+  if (heap->mode()->is_generational()) {\n+    ShenandoahGenerationalFullGC::balance_generations_after_rebuilding_free_set();\n+    ShenandoahGenerationalFullGC::rebuild_remembered_set(heap);\n+  }\n","filename":"src\/hotspot\/share\/gc\/shenandoah\/shenandoahFullGC.cpp","additions":8,"deletions":7,"binary":false,"changes":15,"status":"modified"},{"patch":"@@ -35,0 +35,1 @@\n+#include \"gc\/shenandoah\/shenandoahScanRemembered.inline.hpp\"\n@@ -246,5 +247,0 @@\n-  size_t region_size_bytes = ShenandoahHeapRegion::region_size_bytes();\n-  size_t regions_available_to_loan = 0;\n-  size_t minimum_evacuation_reserve = ShenandoahOldCompactionReserve * region_size_bytes;\n-  size_t old_regions_loaned_for_young_evac = 0;\n-\n@@ -264,3 +260,0 @@\n-  \/\/ Do not fill up old-gen memory with promotions.  Reserve some amount of memory for compaction purposes.\n-  size_t young_evac_reserve_max = 0;\n-\n@@ -303,1 +296,0 @@\n-  ShenandoahOldHeuristics* const old_heuristics = heap->old_heuristics();\n@@ -321,1 +313,1 @@\n-  } else if (old_heuristics->unprocessed_old_collection_candidates() > 0) {\n+  } else if (old_generation->has_unprocessed_collection_candidates()) {\n@@ -337,1 +329,1 @@\n-  const size_t old_free_unfragmented = old_generation->free_unaffiliated_regions() * region_size_bytes;\n+  const size_t old_free_unfragmented = old_generation->free_unaffiliated_regions() * ShenandoahHeapRegion::region_size_bytes();\n@@ -349,1 +341,1 @@\n-  size_t consumed_by_advance_promotion = select_aged_regions(old_promo_reserve);\n+  const size_t consumed_by_advance_promotion = select_aged_regions(old_promo_reserve);\n@@ -740,5 +732,1 @@\n-        assert(heap->old_generation()->is_mark_complete(), \"Expected old generation mark to be complete after global cycle.\");\n-        heap->old_heuristics()->prepare_for_old_collections();\n-        log_info(gc)(\"After choosing global collection set, mixed candidates: \" UINT32_FORMAT \", coalescing candidates: \" SIZE_FORMAT,\n-                     heap->old_heuristics()->unprocessed_old_collection_candidates(),\n-                     heap->old_heuristics()->coalesce_and_fill_candidates_count());\n+        heap->old_generation()->prepare_for_mixed_collections_after_global_gc();\n","filename":"src\/hotspot\/share\/gc\/shenandoah\/shenandoahGeneration.cpp","additions":5,"deletions":17,"binary":false,"changes":22,"status":"modified"},{"patch":"@@ -29,1 +29,0 @@\n-#include \"gc\/shenandoah\/heuristics\/shenandoahOldHeuristics.hpp\"\n@@ -35,0 +34,2 @@\n+class ShenandoahCollectionSet;\n+class ShenandoahHeap;\n@@ -37,2 +38,1 @@\n-class ShenandoahReferenceProcessor;\n-class ShenandoahHeap;\n+class ShenandoahHeuristics;\n@@ -40,0 +40,2 @@\n+class ShenandoahReferenceProcessor;\n+\n@@ -120,1 +122,1 @@\n-  inline ShenandoahHeuristics* heuristics() const { return _heuristics; }\n+  virtual ShenandoahHeuristics* heuristics() const { return _heuristics; }\n","filename":"src\/hotspot\/share\/gc\/shenandoah\/shenandoahGeneration.hpp","additions":6,"deletions":4,"binary":false,"changes":10,"status":"modified"},{"patch":"@@ -159,1 +159,1 @@\n-        if (_requested_generation == OLD && heap->doing_mixed_evacuations()) {\n+        if (_requested_generation == OLD && heap->old_generation()->is_doing_mixed_evacuations()) {\n@@ -515,1 +515,1 @@\n-bool ShenandoahGenerationalControlThread::resume_concurrent_old_cycle(ShenandoahGeneration* generation, GCCause::Cause cause) {\n+bool ShenandoahGenerationalControlThread::resume_concurrent_old_cycle(ShenandoahOldGeneration* generation, GCCause::Cause cause) {\n","filename":"src\/hotspot\/share\/gc\/shenandoah\/shenandoahGenerationalControlThread.cpp","additions":2,"deletions":2,"binary":false,"changes":4,"status":"modified"},{"patch":"@@ -36,0 +36,2 @@\n+class ShenandoahOldGeneration;\n+\n@@ -85,1 +87,1 @@\n-  bool resume_concurrent_old_cycle(ShenandoahGeneration* generation, GCCause::Cause cause);\n+  bool resume_concurrent_old_cycle(ShenandoahOldGeneration* generation, GCCause::Cause cause);\n","filename":"src\/hotspot\/share\/gc\/shenandoah\/shenandoahGenerationalControlThread.hpp","additions":3,"deletions":1,"binary":false,"changes":4,"status":"modified"},{"patch":"@@ -0,0 +1,285 @@\n+\/*\n+ * Copyright Amazon.com Inc. or its affiliates. All Rights Reserved.\n+ * DO NOT ALTER OR REMOVE COPYRIGHT NOTICES OR THIS FILE HEADER.\n+ *\n+ * This code is free software; you can redistribute it and\/or modify it\n+ * under the terms of the GNU General Public License version 2 only, as\n+ * published by the Free Software Foundation.\n+ *\n+ * This code is distributed in the hope that it will be useful, but WITHOUT\n+ * ANY WARRANTY; without even the implied warranty of MERCHANTABILITY or\n+ * FITNESS FOR A PARTICULAR PURPOSE.  See the GNU General Public License\n+ * version 2 for more details (a copy is included in the LICENSE file that\n+ * accompanied this code).\n+ *\n+ * You should have received a copy of the GNU General Public License version\n+ * 2 along with this work; if not, write to the Free Software Foundation,\n+ * Inc., 51 Franklin St, Fifth Floor, Boston, MA 02110-1301 USA.\n+ *\n+ * Please contact Oracle, 500 Oracle Parkway, Redwood Shores, CA 94065 USA\n+ * or visit www.oracle.com if you need additional information or have any\n+ * questions.\n+ *\n+ *\/\n+\n+#include \"precompiled.hpp\"\n+\n+#include \"gc\/shenandoah\/shenandoahAsserts.hpp\"\n+#include \"gc\/shenandoah\/shenandoahFreeSet.hpp\"\n+#include \"gc\/shenandoah\/shenandoahGenerationalEvacuationTask.hpp\"\n+#include \"gc\/shenandoah\/shenandoahGenerationalHeap.hpp\"\n+#include \"gc\/shenandoah\/shenandoahHeap.inline.hpp\"\n+#include \"gc\/shenandoah\/shenandoahOldGeneration.hpp\"\n+#include \"gc\/shenandoah\/shenandoahPacer.hpp\"\n+#include \"gc\/shenandoah\/shenandoahScanRemembered.inline.hpp\"\n+#include \"gc\/shenandoah\/shenandoahYoungGeneration.hpp\"\n+#include \"gc\/shenandoah\/shenandoahUtils.hpp\"\n+\n+class ShenandoahConcurrentEvacuator : public ObjectClosure {\n+private:\n+  ShenandoahGenerationalHeap* const _heap;\n+  Thread* const _thread;\n+public:\n+  explicit ShenandoahConcurrentEvacuator(ShenandoahGenerationalHeap* heap) :\n+          _heap(heap), _thread(Thread::current()) {}\n+\n+  void do_object(oop p) override {\n+    shenandoah_assert_marked(nullptr, p);\n+    if (!p->is_forwarded()) {\n+      _heap->evacuate_object(p, _thread);\n+    }\n+  }\n+};\n+\n+ShenandoahGenerationalEvacuationTask::ShenandoahGenerationalEvacuationTask(ShenandoahGenerationalHeap* heap,\n+                                                                           ShenandoahRegionIterator* iterator,\n+                                                                           bool concurrent) :\n+  WorkerTask(\"Shenandoah Evacuation\"),\n+  _heap(heap),\n+  _regions(iterator),\n+  _concurrent(concurrent),\n+  _tenuring_threshold(0)\n+{\n+  shenandoah_assert_generational();\n+  _tenuring_threshold = _heap->age_census()->tenuring_threshold();\n+}\n+\n+void ShenandoahGenerationalEvacuationTask::work(uint worker_id) {\n+  if (_concurrent) {\n+    ShenandoahConcurrentWorkerSession worker_session(worker_id);\n+    ShenandoahSuspendibleThreadSetJoiner stsj;\n+    ShenandoahEvacOOMScope oom_evac_scope;\n+    do_work();\n+  } else {\n+    ShenandoahParallelWorkerSession worker_session(worker_id);\n+    ShenandoahEvacOOMScope oom_evac_scope;\n+    do_work();\n+  }\n+}\n+\n+void ShenandoahGenerationalEvacuationTask::do_work() {\n+  ShenandoahConcurrentEvacuator cl(_heap);\n+  ShenandoahHeapRegion* r;\n+  ShenandoahMarkingContext* const ctx = _heap->marking_context();\n+\n+  while ((r = _regions->next()) != nullptr) {\n+    log_debug(gc)(\"GenerationalEvacuationTask do_work(), looking at %s region \" SIZE_FORMAT \", (age: %d) [%s, %s, %s]\",\n+            r->is_old()? \"old\": r->is_young()? \"young\": \"free\", r->index(), r->age(),\n+            r->is_active()? \"active\": \"inactive\",\n+            r->is_humongous()? (r->is_humongous_start()? \"humongous_start\": \"humongous_continuation\"): \"regular\",\n+            r->is_cset()? \"cset\": \"not-cset\");\n+\n+    if (r->is_cset()) {\n+      assert(r->has_live(), \"Region \" SIZE_FORMAT \" should have been reclaimed early\", r->index());\n+      _heap->marked_object_iterate(r, &cl);\n+      if (ShenandoahPacing) {\n+        _heap->pacer()->report_evac(r->used() >> LogHeapWordSize);\n+      }\n+    } else if (r->is_young() && r->is_active() && (r->age() >= _tenuring_threshold)) {\n+      if (r->is_humongous_start()) {\n+        \/\/ We promote humongous_start regions along with their affiliated continuations during evacuation rather than\n+        \/\/ doing this work during a safepoint.  We cannot put humongous regions into the collection set because that\n+        \/\/ triggers the load-reference barrier (LRB) to copy on reference fetch.\n+        promote_humongous(r);\n+      } else if (r->is_regular() && (r->get_top_before_promote() != nullptr)) {\n+        \/\/ Likewise, we cannot put promote-in-place regions into the collection set because that would also trigger\n+        \/\/ the LRB to copy on reference fetch.\n+        promote_in_place(r);\n+      }\n+      \/\/ Aged humongous continuation regions are handled with their start region.  If an aged regular region has\n+      \/\/ more garbage than ShenandoahOldGarbageThreshold, we'll promote by evacuation.  If there is room for evacuation\n+      \/\/ in this cycle, the region will be in the collection set.  If there is not room, the region will be promoted\n+      \/\/ by evacuation in some future GC cycle.\n+\n+      \/\/ If an aged regular region has received allocations during the current cycle, we do not promote because the\n+      \/\/ newly allocated objects do not have appropriate age; this region's age will be reset to zero at end of cycle.\n+    }\n+    \/\/ else, region is free, or OLD, or not in collection set, or humongous_continuation,\n+    \/\/ or is young humongous_start that is too young to be promoted\n+    if (_heap->check_cancelled_gc_and_yield(_concurrent)) {\n+      break;\n+    }\n+  }\n+}\n+\n+\/\/ When we promote a region in place, we can continue to use the established marking context to guide subsequent remembered\n+\/\/ set scans of this region's content.  The region will be coalesced and filled prior to the next old-gen marking effort.\n+\/\/ We identify the entirety of the region as DIRTY to force the next remembered set scan to identify the \"interesting poitners\"\n+\/\/ contained herein.\n+void ShenandoahGenerationalEvacuationTask::promote_in_place(ShenandoahHeapRegion* region) {\n+  ShenandoahMarkingContext* const marking_context = _heap->marking_context();\n+  HeapWord* const tams = marking_context->top_at_mark_start(region);\n+\n+  {\n+    const size_t old_garbage_threshold = (ShenandoahHeapRegion::region_size_bytes() * ShenandoahOldGarbageThreshold) \/ 100;\n+    assert(_heap->active_generation()->is_mark_complete(), \"sanity\");\n+    assert(!_heap->is_concurrent_old_mark_in_progress(), \"Cannot promote in place during old marking\");\n+    assert(region->garbage_before_padded_for_promote() < old_garbage_threshold, \"Region \" SIZE_FORMAT \" has too much garbage for promotion\", region->index());\n+    assert(region->is_young(), \"Only young regions can be promoted\");\n+    assert(region->is_regular(), \"Use different service to promote humongous regions\");\n+    assert(region->age() >= _heap->age_census()->tenuring_threshold(), \"Only promote regions that are sufficiently aged\");\n+    assert(region->get_top_before_promote() == tams, \"Region \" SIZE_FORMAT \" has been used for allocations before promotion\", region->index());\n+  }\n+\n+  \/\/ Rebuild the remembered set information and mark the entire range as DIRTY.  We do NOT scan the content of this\n+  \/\/ range to determine which cards need to be DIRTY.  That would force us to scan the region twice, once now, and\n+  \/\/ once during the subsequent remembered set scan.  Instead, we blindly (conservatively) mark everything as DIRTY\n+  \/\/ now and then sort out the CLEAN pages during the next remembered set scan.\n+  \/\/\n+  \/\/ Rebuilding the remembered set consists of clearing all object registrations (reset_object_range()) here,\n+  \/\/ then registering every live object and every coalesced range of free objects in the loop that follows.\n+  _heap->card_scan()->reset_object_range(region->bottom(), region->end());\n+  _heap->card_scan()->mark_range_as_dirty(region->bottom(), region->get_top_before_promote() - region->bottom());\n+\n+  \/\/ TODO: use an existing coalesce-and-fill function rather than replicating the code here.\n+  HeapWord* obj_addr = region->bottom();\n+  while (obj_addr < tams) {\n+    oop obj = cast_to_oop(obj_addr);\n+    if (marking_context->is_marked(obj)) {\n+      assert(obj->klass() != nullptr, \"klass should not be NULL\");\n+      \/\/ This thread is responsible for registering all objects in this region.  No need for lock.\n+      _heap->card_scan()->register_object_without_lock(obj_addr);\n+      obj_addr += obj->size();\n+    } else {\n+      HeapWord* next_marked_obj = marking_context->get_next_marked_addr(obj_addr, tams);\n+      assert(next_marked_obj <= tams, \"next marked object cannot exceed tams\");\n+      size_t fill_size = next_marked_obj - obj_addr;\n+      assert(fill_size >= ShenandoahHeap::min_fill_size(), \"previously allocated objects known to be larger than min_size\");\n+      ShenandoahHeap::fill_with_object(obj_addr, fill_size);\n+      _heap->card_scan()->register_object_without_lock(obj_addr);\n+      obj_addr = next_marked_obj;\n+    }\n+  }\n+  \/\/ We do not need to scan above TAMS because restored top equals tams\n+  assert(obj_addr == tams, \"Expect loop to terminate when obj_addr equals tams\");\n+\n+  ShenandoahOldGeneration* const old_gen = _heap->old_generation();\n+  ShenandoahYoungGeneration* const young_gen = _heap->young_generation();\n+\n+  {\n+    ShenandoahHeapLocker locker(_heap->lock());\n+\n+    HeapWord* update_watermark = region->get_update_watermark();\n+\n+    \/\/ Now that this region is affiliated with old, we can allow it to receive allocations, though it may not be in the\n+    \/\/ is_collector_free range.\n+    region->restore_top_before_promote();\n+\n+    size_t region_used = region->used();\n+\n+    \/\/ The update_watermark was likely established while we had the artificially high value of top.  Make it sane now.\n+    assert(update_watermark >= region->top(), \"original top cannot exceed preserved update_watermark\");\n+    region->set_update_watermark(region->top());\n+\n+    \/\/ Unconditionally transfer one region from young to old. This represents the newly promoted region.\n+    \/\/ This expands old and shrinks new by the size of one region.  Strictly, we do not \"need\" to expand old\n+    \/\/ if there are already enough unaffiliated regions in old to account for this newly promoted region.\n+    \/\/ However, if we do not transfer the capacities, we end up reducing the amount of memory that would have\n+    \/\/ otherwise been available to hold old evacuations, because old available is max_capacity - used and now\n+    \/\/ we would be trading a fully empty region for a partially used region.\n+    young_gen->decrease_used(region_used);\n+    young_gen->decrement_affiliated_region_count();\n+\n+    \/\/ transfer_to_old() increases capacity of old and decreases capacity of young\n+    _heap->generation_sizer()->force_transfer_to_old(1);\n+    region->set_affiliation(OLD_GENERATION);\n+\n+    old_gen->increment_affiliated_region_count();\n+    old_gen->increase_used(region_used);\n+\n+    \/\/ add_old_collector_free_region() increases promoted_reserve() if available space exceeds plab_min_size()\n+    _heap->free_set()->add_old_collector_free_region(region);\n+  }\n+}\n+\n+void ShenandoahGenerationalEvacuationTask::promote_humongous(ShenandoahHeapRegion* region) {\n+  ShenandoahMarkingContext* marking_context = _heap->marking_context();\n+  oop obj = cast_to_oop(region->bottom());\n+  assert(_heap->active_generation()->is_mark_complete(), \"sanity\");\n+  assert(region->is_young(), \"Only young regions can be promoted\");\n+  assert(region->is_humongous_start(), \"Should not promote humongous continuation in isolation\");\n+  assert(region->age() >= _heap->age_census()->tenuring_threshold(), \"Only promote regions that are sufficiently aged\");\n+  assert(marking_context->is_marked(obj), \"promoted humongous object should be alive\");\n+\n+  \/\/ TODO: Consider not promoting humongous objects that represent primitive arrays.  Leaving a primitive array\n+  \/\/ (obj->is_typeArray()) in young-gen is harmless because these objects are never relocated and they are not\n+  \/\/ scanned.  Leaving primitive arrays in young-gen memory allows their memory to be reclaimed more quickly when\n+  \/\/ it becomes garbage.  Better to not make this change until sizes of young-gen and old-gen are completely\n+  \/\/ adaptive, as leaving primitive arrays in young-gen might be perceived as an \"astonishing result\" by someone\n+  \/\/ has carefully analyzed the required sizes of an application's young-gen and old-gen.\n+  const size_t used_bytes = obj->size() * HeapWordSize;\n+  const size_t spanned_regions = ShenandoahHeapRegion::required_regions(used_bytes);\n+  const size_t humongous_waste = spanned_regions * ShenandoahHeapRegion::region_size_bytes() - obj->size() * HeapWordSize;\n+  const size_t index_limit = region->index() + spanned_regions;\n+\n+  ShenandoahGeneration* const old_generation = _heap->old_generation();\n+  ShenandoahGeneration* const young_generation = _heap->young_generation();\n+  {\n+    \/\/ We need to grab the heap lock in order to avoid a race when changing the affiliations of spanned_regions from\n+    \/\/ young to old.\n+    ShenandoahHeapLocker locker(_heap->lock());\n+\n+    \/\/ We promote humongous objects unconditionally, without checking for availability.  We adjust\n+    \/\/ usage totals, including humongous waste, after evacuation is done.\n+    log_debug(gc)(\"promoting humongous region \" SIZE_FORMAT \", spanning \" SIZE_FORMAT, region->index(), spanned_regions);\n+\n+    young_generation->decrease_used(used_bytes);\n+    young_generation->decrease_humongous_waste(humongous_waste);\n+    young_generation->decrease_affiliated_region_count(spanned_regions);\n+\n+    \/\/ transfer_to_old() increases capacity of old and decreases capacity of young\n+    _heap->generation_sizer()->force_transfer_to_old(spanned_regions);\n+\n+    \/\/ For this region and each humongous continuation region spanned by this humongous object, change\n+    \/\/ affiliation to OLD_GENERATION and adjust the generation-use tallies.  The remnant of memory\n+    \/\/ in the last humongous region that is not spanned by obj is currently not used.\n+    for (size_t i = region->index(); i < index_limit; i++) {\n+      ShenandoahHeapRegion* r = _heap->get_region(i);\n+      log_debug(gc)(\"promoting humongous region \" SIZE_FORMAT \", from \" PTR_FORMAT \" to \" PTR_FORMAT,\n+              r->index(), p2i(r->bottom()), p2i(r->top()));\n+      \/\/ We mark the entire humongous object's range as dirty after loop terminates, so no need to dirty the range here\n+      r->set_affiliation(OLD_GENERATION);\n+    }\n+\n+    old_generation->increase_affiliated_region_count(spanned_regions);\n+    old_generation->increase_used(used_bytes);\n+    old_generation->increase_humongous_waste(humongous_waste);\n+  }\n+\n+  \/\/ Since this region may have served previously as OLD, it may hold obsolete object range info.\n+  HeapWord* const humongous_bottom = region->bottom();\n+  _heap->card_scan()->reset_object_range(humongous_bottom, humongous_bottom + spanned_regions * ShenandoahHeapRegion::region_size_words());\n+  \/\/ Since the humongous region holds only one object, no lock is necessary for this register_object() invocation.\n+  _heap->card_scan()->register_object_without_lock(humongous_bottom);\n+\n+  if (obj->is_typeArray()) {\n+    \/\/ Primitive arrays don't need to be scanned.\n+    log_debug(gc)(\"Clean cards for promoted humongous object (Region \" SIZE_FORMAT \") from \" PTR_FORMAT \" to \" PTR_FORMAT,\n+            region->index(), p2i(humongous_bottom), p2i(humongous_bottom + obj->size()));\n+    _heap->card_scan()->mark_range_as_clean(humongous_bottom, obj->size());\n+  } else {\n+    log_debug(gc)(\"Dirty cards for promoted humongous object (Region \" SIZE_FORMAT \") from \" PTR_FORMAT \" to \" PTR_FORMAT,\n+            region->index(), p2i(humongous_bottom), p2i(humongous_bottom + obj->size()));\n+    _heap->card_scan()->mark_range_as_dirty(humongous_bottom, obj->size());\n+  }\n+}\n","filename":"src\/hotspot\/share\/gc\/shenandoah\/shenandoahGenerationalEvacuationTask.cpp","additions":285,"deletions":0,"binary":false,"changes":285,"status":"added"},{"patch":"@@ -0,0 +1,55 @@\n+\/*\n+ * Copyright Amazon.com Inc. or its affiliates. All Rights Reserved.\n+ * DO NOT ALTER OR REMOVE COPYRIGHT NOTICES OR THIS FILE HEADER.\n+ *\n+ * This code is free software; you can redistribute it and\/or modify it\n+ * under the terms of the GNU General Public License version 2 only, as\n+ * published by the Free Software Foundation.\n+ *\n+ * This code is distributed in the hope that it will be useful, but WITHOUT\n+ * ANY WARRANTY; without even the implied warranty of MERCHANTABILITY or\n+ * FITNESS FOR A PARTICULAR PURPOSE.  See the GNU General Public License\n+ * version 2 for more details (a copy is included in the LICENSE file that\n+ * accompanied this code).\n+ *\n+ * You should have received a copy of the GNU General Public License version\n+ * 2 along with this work; if not, write to the Free Software Foundation,\n+ * Inc., 51 Franklin St, Fifth Floor, Boston, MA 02110-1301 USA.\n+ *\n+ * Please contact Oracle, 500 Oracle Parkway, Redwood Shores, CA 94065 USA\n+ * or visit www.oracle.com if you need additional information or have any\n+ * questions.\n+ *\n+ *\/\n+\n+#ifndef SHARE_GC_SHENANDOAH_SHENANDOAHGENERATIONALEVACUATIONTASK_HPP\n+#define SHARE_GC_SHENANDOAH_SHENANDOAHGENERATIONALEVACUATIONTASK_HPP\n+\n+#include \"gc\/shared\/workerThread.hpp\"\n+\n+class ShenandoahGenerationalHeap;\n+class ShenandoahHeapRegion;\n+class ShenandoahRegionIterator;\n+\n+\/\/ Unlike ShenandoahEvacuationTask, this iterates over all regions rather than just the collection set.\n+\/\/ This is needed in order to promote humongous start regions if age() >= tenure threshold.\n+class ShenandoahGenerationalEvacuationTask : public WorkerTask {\n+private:\n+  ShenandoahGenerationalHeap* const _heap;\n+  ShenandoahRegionIterator* _regions;\n+  bool _concurrent;\n+  uint _tenuring_threshold;\n+\n+public:\n+  ShenandoahGenerationalEvacuationTask(ShenandoahGenerationalHeap* sh,\n+                                       ShenandoahRegionIterator* iterator,\n+                                       bool concurrent);\n+  void work(uint worker_id) override;\n+private:\n+  void do_work();\n+\n+  void promote_in_place(ShenandoahHeapRegion* region);\n+  void promote_humongous(ShenandoahHeapRegion* region);\n+};\n+\n+#endif \/\/SHARE_GC_SHENANDOAH_SHENANDOAHGENERATIONALEVACUATIONTASK_HPP\n","filename":"src\/hotspot\/share\/gc\/shenandoah\/shenandoahGenerationalEvacuationTask.hpp","additions":55,"deletions":0,"binary":false,"changes":55,"status":"added"},{"patch":"@@ -28,0 +28,3 @@\n+#include \"gc\/shenandoah\/shenandoahFreeSet.hpp\"\n+#include \"gc\/shenandoah\/shenandoahGenerationalControlThread.hpp\"\n+#include \"gc\/shenandoah\/shenandoahGenerationalEvacuationTask.hpp\"\n@@ -30,0 +33,1 @@\n+#include \"gc\/shenandoah\/shenandoahHeapRegion.hpp\"\n@@ -34,0 +38,1 @@\n+#include \"gc\/shenandoah\/shenandoahScanRemembered.inline.hpp\"\n@@ -35,1 +40,1 @@\n-\n+#include \"gc\/shenandoah\/shenandoahUtils.hpp\"\n@@ -38,0 +43,1 @@\n+\n@@ -75,1 +81,1 @@\n-size_t ShenandoahGenerationalHeap::calculate_min_plab() const {\n+size_t ShenandoahGenerationalHeap::calculate_min_plab() {\n@@ -79,1 +85,1 @@\n-size_t ShenandoahGenerationalHeap::calculate_max_plab() const {\n+size_t ShenandoahGenerationalHeap::calculate_max_plab() {\n@@ -86,0 +92,5 @@\n+\/\/ Returns size in bytes\n+size_t ShenandoahGenerationalHeap::unsafe_max_tlab_alloc(Thread *thread) const {\n+  return MIN2(ShenandoahHeapRegion::max_tlab_size_bytes(), young_generation()->available());\n+}\n+\n@@ -90,1 +101,3 @@\n-  _regulator_thread(nullptr) {\n+  _regulator_thread(nullptr),\n+  _young_gen_memory_pool(nullptr),\n+  _old_gen_memory_pool(nullptr) {\n@@ -136,0 +149,389 @@\n+oop ShenandoahGenerationalHeap::evacuate_object(oop p, Thread* thread) {\n+  assert(thread == Thread::current(), \"Expected thread parameter to be current thread.\");\n+  if (ShenandoahThreadLocalData::is_oom_during_evac(thread)) {\n+    \/\/ This thread went through the OOM during evac protocol and it is safe to return\n+    \/\/ the forward pointer. It must not attempt to evacuate anymore.\n+    return ShenandoahBarrierSet::resolve_forwarded(p);\n+  }\n+\n+  assert(ShenandoahThreadLocalData::is_evac_allowed(thread), \"must be enclosed in oom-evac scope\");\n+\n+  ShenandoahHeapRegion* r = heap_region_containing(p);\n+  assert(!r->is_humongous(), \"never evacuate humongous objects\");\n+\n+  ShenandoahAffiliation target_gen = r->affiliation();\n+  if (active_generation()->is_young() && target_gen == YOUNG_GENERATION) {\n+    markWord mark = p->mark();\n+    if (mark.is_marked()) {\n+      \/\/ Already forwarded.\n+      return ShenandoahBarrierSet::resolve_forwarded(p);\n+    }\n+\n+    if (mark.has_displaced_mark_helper()) {\n+      \/\/ We don't want to deal with MT here just to ensure we read the right mark word.\n+      \/\/ Skip the potential promotion attempt for this one.\n+    } else if (r->age() + mark.age() >= age_census()->tenuring_threshold()) {\n+      oop result = try_evacuate_object(p, thread, r, OLD_GENERATION);\n+      if (result != nullptr) {\n+        return result;\n+      }\n+      \/\/ If we failed to promote this aged object, we'll fall through to code below and evacuate to young-gen.\n+    }\n+  }\n+  return try_evacuate_object(p, thread, r, target_gen);\n+}\n+\n+\/\/ try_evacuate_object registers the object and dirties the associated remembered set information when evacuating\n+\/\/ to OLD_GENERATION.\n+oop ShenandoahGenerationalHeap::try_evacuate_object(oop p, Thread* thread, ShenandoahHeapRegion* from_region,\n+                                        ShenandoahAffiliation target_gen) {\n+  bool alloc_from_lab = true;\n+  bool has_plab = false;\n+  HeapWord* copy = nullptr;\n+  size_t size = p->size();\n+  bool is_promotion = (target_gen == OLD_GENERATION) && from_region->is_young();\n+\n+#ifdef ASSERT\n+  if (ShenandoahOOMDuringEvacALot &&\n+      (os::random() & 1) == 0) { \/\/ Simulate OOM every ~2nd slow-path call\n+    copy = nullptr;\n+  } else {\n+#endif\n+    if (UseTLAB) {\n+      switch (target_gen) {\n+        case YOUNG_GENERATION: {\n+          copy = allocate_from_gclab(thread, size);\n+          if ((copy == nullptr) && (size < ShenandoahThreadLocalData::gclab_size(thread))) {\n+            \/\/ GCLAB allocation failed because we are bumping up against the limit on young evacuation reserve.  Try resetting\n+            \/\/ the desired GCLAB size and retry GCLAB allocation to avoid cascading of shared memory allocations.\n+            ShenandoahThreadLocalData::set_gclab_size(thread, PLAB::min_size());\n+            copy = allocate_from_gclab(thread, size);\n+            \/\/ If we still get nullptr, we'll try a shared allocation below.\n+          }\n+          break;\n+        }\n+        case OLD_GENERATION: {\n+          assert(mode()->is_generational(), \"OLD Generation only exists in generational mode\");\n+          PLAB* plab = ShenandoahThreadLocalData::plab(thread);\n+          if (plab != nullptr) {\n+            has_plab = true;\n+          }\n+          copy = allocate_from_plab(thread, size, is_promotion);\n+          if ((copy == nullptr) && (size < ShenandoahThreadLocalData::plab_size(thread)) &&\n+              ShenandoahThreadLocalData::plab_retries_enabled(thread)) {\n+            \/\/ PLAB allocation failed because we are bumping up against the limit on old evacuation reserve or because\n+            \/\/ the requested object does not fit within the current plab but the plab still has an \"abundance\" of memory,\n+            \/\/ where abundance is defined as >= ShenGenHeap::plab_min_size().  In the former case, we try resetting the desired\n+            \/\/ PLAB size and retry PLAB allocation to avoid cascading of shared memory allocations.\n+\n+            \/\/ In this situation, PLAB memory is precious.  We'll try to preserve our existing PLAB by forcing\n+            \/\/ this particular allocation to be shared.\n+            if (plab->words_remaining() < plab_min_size()) {\n+              ShenandoahThreadLocalData::set_plab_size(thread, plab_min_size());\n+              copy = allocate_from_plab(thread, size, is_promotion);\n+              \/\/ If we still get nullptr, we'll try a shared allocation below.\n+              if (copy == nullptr) {\n+                \/\/ If retry fails, don't continue to retry until we have success (probably in next GC pass)\n+                ShenandoahThreadLocalData::disable_plab_retries(thread);\n+              }\n+            }\n+            \/\/ else, copy still equals nullptr.  this causes shared allocation below, preserving this plab for future needs.\n+          }\n+          break;\n+        }\n+        default: {\n+          ShouldNotReachHere();\n+          break;\n+        }\n+      }\n+    }\n+\n+    if (copy == nullptr) {\n+      \/\/ If we failed to allocate in LAB, we'll try a shared allocation.\n+      if (!is_promotion || !has_plab || (size > PLAB::min_size())) {\n+        ShenandoahAllocRequest req = ShenandoahAllocRequest::for_shared_gc(size, target_gen, is_promotion);\n+        copy = allocate_memory(req);\n+        alloc_from_lab = false;\n+      }\n+      \/\/ else, we leave copy equal to nullptr, signaling a promotion failure below if appropriate.\n+      \/\/ We choose not to promote objects smaller than PLAB::min_size() by way of shared allocations, as this is too\n+      \/\/ costly.  Instead, we'll simply \"evacuate\" to young-gen memory (using a GCLAB) and will promote in a future\n+      \/\/ evacuation pass.  This condition is denoted by: is_promotion && has_plab && (size <= PLAB::min_size())\n+    }\n+#ifdef ASSERT\n+  }\n+#endif\n+\n+  if (copy == nullptr) {\n+    if (target_gen == OLD_GENERATION) {\n+      if (from_region->is_young()) {\n+        \/\/ Signal that promotion failed. Will evacuate this old object somewhere in young gen.\n+        old_generation()->handle_failed_promotion(thread, size);\n+        return nullptr;\n+      } else {\n+        \/\/ Remember that evacuation to old gen failed. We'll want to trigger a full gc to recover from this\n+        \/\/ after the evacuation threads have finished.\n+        old_generation()->handle_failed_evacuation();\n+      }\n+    }\n+\n+    control_thread()->handle_alloc_failure_evac(size);\n+\n+    oom_evac_handler()->handle_out_of_memory_during_evacuation();\n+\n+    return ShenandoahBarrierSet::resolve_forwarded(p);\n+  }\n+\n+  \/\/ Copy the object:\n+  evac_tracker()->begin_evacuation(thread, size * HeapWordSize);\n+  Copy::aligned_disjoint_words(cast_from_oop<HeapWord*>(p), copy, size);\n+\n+  oop copy_val = cast_to_oop(copy);\n+\n+  if (target_gen == YOUNG_GENERATION && is_aging_cycle()) {\n+    ShenandoahHeap::increase_object_age(copy_val, from_region->age() + 1);\n+  }\n+\n+  \/\/ Try to install the new forwarding pointer.\n+  ContinuationGCSupport::relativize_stack_chunk(copy_val);\n+\n+  oop result = ShenandoahForwarding::try_update_forwardee(p, copy_val);\n+  if (result == copy_val) {\n+    \/\/ Successfully evacuated. Our copy is now the public one!\n+    evac_tracker()->end_evacuation(thread, size * HeapWordSize);\n+    if (target_gen == OLD_GENERATION) {\n+      old_generation()->handle_evacuation(copy, size, from_region->is_young());\n+    } else {\n+      \/\/ When copying to the old generation above, we don't care\n+      \/\/ about recording object age in the census stats.\n+      assert(target_gen == YOUNG_GENERATION, \"Error\");\n+      \/\/ We record this census only when simulating pre-adaptive tenuring behavior, or\n+      \/\/ when we have been asked to record the census at evacuation rather than at mark\n+      if (ShenandoahGenerationalCensusAtEvac || !ShenandoahGenerationalAdaptiveTenuring) {\n+        evac_tracker()->record_age(thread, size * HeapWordSize, ShenandoahHeap::get_object_age(copy_val));\n+      }\n+    }\n+    shenandoah_assert_correct(nullptr, copy_val);\n+    return copy_val;\n+  }  else {\n+    \/\/ Failed to evacuate. We need to deal with the object that is left behind. Since this\n+    \/\/ new allocation is certainly after TAMS, it will be considered live in the next cycle.\n+    \/\/ But if it happens to contain references to evacuated regions, those references would\n+    \/\/ not get updated for this stale copy during this cycle, and we will crash while scanning\n+    \/\/ it the next cycle.\n+    if (alloc_from_lab) {\n+      \/\/ For LAB allocations, it is enough to rollback the allocation ptr. Either the next\n+      \/\/ object will overwrite this stale copy, or the filler object on LAB retirement will\n+      \/\/ do this.\n+      switch (target_gen) {\n+        case YOUNG_GENERATION: {\n+          ShenandoahThreadLocalData::gclab(thread)->undo_allocation(copy, size);\n+          break;\n+        }\n+        case OLD_GENERATION: {\n+          ShenandoahThreadLocalData::plab(thread)->undo_allocation(copy, size);\n+          if (is_promotion) {\n+            ShenandoahThreadLocalData::subtract_from_plab_promoted(thread, size * HeapWordSize);\n+          } else {\n+            ShenandoahThreadLocalData::subtract_from_plab_evacuated(thread, size * HeapWordSize);\n+          }\n+          break;\n+        }\n+        default: {\n+          ShouldNotReachHere();\n+          break;\n+        }\n+      }\n+    } else {\n+      \/\/ For non-LAB allocations, we have no way to retract the allocation, and\n+      \/\/ have to explicitly overwrite the copy with the filler object. With that overwrite,\n+      \/\/ we have to keep the fwdptr initialized and pointing to our (stale) copy.\n+      assert(size >= ShenandoahHeap::min_fill_size(), \"previously allocated object known to be larger than min_size\");\n+      fill_with_object(copy, size);\n+      shenandoah_assert_correct(nullptr, copy_val);\n+      \/\/ For non-LAB allocations, the object has already been registered\n+    }\n+    shenandoah_assert_correct(nullptr, result);\n+    return result;\n+  }\n+}\n+\n+inline HeapWord* ShenandoahGenerationalHeap::allocate_from_plab(Thread* thread, size_t size, bool is_promotion) {\n+  assert(UseTLAB, \"TLABs should be enabled\");\n+\n+  PLAB* plab = ShenandoahThreadLocalData::plab(thread);\n+  HeapWord* obj;\n+\n+  if (plab == nullptr) {\n+    assert(!thread->is_Java_thread() && !thread->is_Worker_thread(), \"Performance: thread should have PLAB: %s\", thread->name());\n+    \/\/ No PLABs in this thread, fallback to shared allocation\n+    return nullptr;\n+  } else if (is_promotion && !ShenandoahThreadLocalData::allow_plab_promotions(thread)) {\n+    return nullptr;\n+  }\n+  \/\/ if plab->word_size() <= 0, thread's plab not yet initialized for this pass, so allow_plab_promotions() is not trustworthy\n+  obj = plab->allocate(size);\n+  if ((obj == nullptr) && (plab->words_remaining() < plab_min_size())) {\n+    \/\/ allocate_from_plab_slow will establish allow_plab_promotions(thread) for future invocations\n+    obj = allocate_from_plab_slow(thread, size, is_promotion);\n+  }\n+  \/\/ if plab->words_remaining() >= ShenGenHeap::heap()->plab_min_size(), just return nullptr so we can use a shared allocation\n+  if (obj == nullptr) {\n+    return nullptr;\n+  }\n+\n+  if (is_promotion) {\n+    ShenandoahThreadLocalData::add_to_plab_promoted(thread, size * HeapWordSize);\n+  } else {\n+    ShenandoahThreadLocalData::add_to_plab_evacuated(thread, size * HeapWordSize);\n+  }\n+  return obj;\n+}\n+\n+\/\/ Establish a new PLAB and allocate size HeapWords within it.\n+HeapWord* ShenandoahGenerationalHeap::allocate_from_plab_slow(Thread* thread, size_t size, bool is_promotion) {\n+  \/\/ New object should fit the PLAB size\n+\n+  assert(mode()->is_generational(), \"PLABs only relevant to generational GC\");\n+  const size_t plab_min_size = this->plab_min_size();\n+  const size_t min_size = (size > plab_min_size)? align_up(size, CardTable::card_size_in_words()): plab_min_size;\n+\n+  \/\/ Figure out size of new PLAB, looking back at heuristics. Expand aggressively.  PLABs must align on size\n+  \/\/ of card table in order to avoid the need for synchronization when registering newly allocated objects within\n+  \/\/ the card table.\n+  size_t cur_size = ShenandoahThreadLocalData::plab_size(thread);\n+  if (cur_size == 0) {\n+    cur_size = plab_min_size;\n+  }\n+\n+  \/\/ Limit growth of PLABs to the smaller of ShenandoahMaxEvacLABRatio * the minimum size and ShenandoahHumongousThreshold.\n+  \/\/ This minimum value is represented by generational_heap->plab_max_size().  Enforcing this limit enables more equitable\n+  \/\/ distribution of available evacuation budget between the many threads that are coordinating in the evacuation effort.\n+  size_t future_size = MIN2(cur_size * 2, plab_max_size());\n+  assert(is_aligned(future_size, CardTable::card_size_in_words()), \"Align by design, future_size: \" SIZE_FORMAT\n+          \", alignment: \" SIZE_FORMAT \", cur_size: \" SIZE_FORMAT \", max: \" SIZE_FORMAT,\n+         future_size, (size_t) CardTable::card_size_in_words(), cur_size, plab_max_size());\n+\n+  \/\/ Record new heuristic value even if we take any shortcut. This captures\n+  \/\/ the case when moderately-sized objects always take a shortcut. At some point,\n+  \/\/ heuristics should catch up with them.  Note that the requested cur_size may\n+  \/\/ not be honored, but we remember that this is the preferred size.\n+  ShenandoahThreadLocalData::set_plab_size(thread, future_size);\n+  if (cur_size < size) {\n+    \/\/ The PLAB to be allocated is still not large enough to hold the object. Fall back to shared allocation.\n+    \/\/ This avoids retiring perfectly good PLABs in order to represent a single large object allocation.\n+    return nullptr;\n+  }\n+\n+  \/\/ Retire current PLAB, and allocate a new one.\n+  PLAB* plab = ShenandoahThreadLocalData::plab(thread);\n+  if (plab->words_remaining() < plab_min_size) {\n+    \/\/ Retire current PLAB, and allocate a new one.\n+    \/\/ CAUTION: retire_plab may register the remnant filler object with the remembered set scanner without a lock.  This\n+    \/\/ is safe iff it is assured that each PLAB is a whole-number multiple of card-mark memory size and each PLAB is\n+    \/\/ aligned with the start of a card's memory range.\n+    retire_plab(plab, thread);\n+\n+    size_t actual_size = 0;\n+    \/\/ allocate_new_plab resets plab_evacuated and plab_promoted and disables promotions if old-gen available is\n+    \/\/ less than the remaining evacuation need.  It also adjusts plab_preallocated and expend_promoted if appropriate.\n+    HeapWord* plab_buf = allocate_new_plab(min_size, cur_size, &actual_size);\n+    if (plab_buf == nullptr) {\n+      if (min_size == plab_min_size) {\n+        \/\/ Disable plab promotions for this thread because we cannot even allocate a plab of minimal size.  This allows us\n+        \/\/ to fail faster on subsequent promotion attempts.\n+        ShenandoahThreadLocalData::disable_plab_promotions(thread);\n+      }\n+      return nullptr;\n+    } else {\n+      ShenandoahThreadLocalData::enable_plab_retries(thread);\n+    }\n+    \/\/ Since the allocated PLAB may have been down-sized for alignment, plab->allocate(size) below may still fail.\n+    if (ZeroTLAB) {\n+      \/\/ ... and clear it.\n+      Copy::zero_to_words(plab_buf, actual_size);\n+    } else {\n+      \/\/ ...and zap just allocated object.\n+#ifdef ASSERT\n+      \/\/ Skip mangling the space corresponding to the object header to\n+      \/\/ ensure that the returned space is not considered parsable by\n+      \/\/ any concurrent GC thread.\n+      size_t hdr_size = oopDesc::header_size();\n+      Copy::fill_to_words(plab_buf + hdr_size, actual_size - hdr_size, badHeapWordVal);\n+#endif \/\/ ASSERT\n+    }\n+    assert(is_aligned(actual_size, CardTable::card_size_in_words()), \"Align by design\");\n+    plab->set_buf(plab_buf, actual_size);\n+    if (is_promotion && !ShenandoahThreadLocalData::allow_plab_promotions(thread)) {\n+      return nullptr;\n+    }\n+    return plab->allocate(size);\n+  } else {\n+    \/\/ If there's still at least min_size() words available within the current plab, don't retire it.  Let's gnaw\n+    \/\/ away on this plab as long as we can.  Meanwhile, return nullptr to force this particular allocation request\n+    \/\/ to be satisfied with a shared allocation.  By packing more promotions into the previously allocated PLAB, we\n+    \/\/ reduce the likelihood of evacuation failures, and we reduce the need for downsizing our PLABs.\n+    return nullptr;\n+  }\n+}\n+\n+HeapWord* ShenandoahGenerationalHeap::allocate_new_plab(size_t min_size, size_t word_size, size_t* actual_size) {\n+  \/\/ Align requested sizes to card-sized multiples.  Align down so that we don't violate max size of TLAB.\n+  assert(is_aligned(min_size, CardTable::card_size_in_words()), \"Align by design\");\n+  assert(word_size >= min_size, \"Requested PLAB is too small\");\n+\n+  ShenandoahAllocRequest req = ShenandoahAllocRequest::for_plab(min_size, word_size);\n+  \/\/ Note that allocate_memory() sets a thread-local flag to prohibit further promotions by this thread\n+  \/\/ if we are at risk of infringing on the old-gen evacuation budget.\n+  HeapWord* res = allocate_memory(req);\n+  if (res != nullptr) {\n+    *actual_size = req.actual_size();\n+  } else {\n+    *actual_size = 0;\n+  }\n+  assert(is_aligned(res, CardTable::card_size_in_words()), \"Align by design\");\n+  return res;\n+}\n+\n+\/\/ TODO: It is probably most efficient to register all objects (both promotions and evacuations) that were allocated within\n+\/\/ this plab at the time we retire the plab.  A tight registration loop will run within both code and data caches.  This change\n+\/\/ would allow smaller and faster in-line implementation of alloc_from_plab().  Since plabs are aligned on card-table boundaries,\n+\/\/ this object registration loop can be performed without acquiring a lock.\n+void ShenandoahGenerationalHeap::retire_plab(PLAB* plab, Thread* thread) {\n+  \/\/ We don't enforce limits on plab_evacuated.  We let it consume all available old-gen memory in order to reduce\n+  \/\/ probability of an evacuation failure.  We do enforce limits on promotion, to make sure that excessive promotion\n+  \/\/ does not result in an old-gen evacuation failure.  Note that a failed promotion is relatively harmless.  Any\n+  \/\/ object that fails to promote in the current cycle will be eligible for promotion in a subsequent cycle.\n+\n+  \/\/ When the plab was instantiated, its entirety was treated as if the entire buffer was going to be dedicated to\n+  \/\/ promotions.  Now that we are retiring the buffer, we adjust for the reality that the plab is not entirely promotions.\n+  \/\/  1. Some of the plab may have been dedicated to evacuations.\n+  \/\/  2. Some of the plab may have been abandoned due to waste (at the end of the plab).\n+  size_t not_promoted =\n+          ShenandoahThreadLocalData::get_plab_preallocated_promoted(thread) - ShenandoahThreadLocalData::get_plab_promoted(thread);\n+  ShenandoahThreadLocalData::reset_plab_promoted(thread);\n+  ShenandoahThreadLocalData::reset_plab_evacuated(thread);\n+  ShenandoahThreadLocalData::set_plab_preallocated_promoted(thread, 0);\n+  if (not_promoted > 0) {\n+    old_generation()->unexpend_promoted(not_promoted);\n+  }\n+  const size_t original_waste = plab->waste();\n+  HeapWord* const top = plab->top();\n+\n+  \/\/ plab->retire() overwrites unused memory between plab->top() and plab->hard_end() with a dummy object to make memory parsable.\n+  \/\/ It adds the size of this unused memory, in words, to plab->waste().\n+  plab->retire();\n+  if (top != nullptr && plab->waste() > original_waste && is_in_old(top)) {\n+    \/\/ If retiring the plab created a filler object, then we need to register it with our card scanner so it can\n+    \/\/ safely walk the region backing the plab.\n+    log_debug(gc)(\"retire_plab() is registering remnant of size \" SIZE_FORMAT \" at \" PTR_FORMAT,\n+                  plab->waste() - original_waste, p2i(top));\n+    card_scan()->register_object_without_lock(top);\n+  }\n+}\n+\n+void ShenandoahGenerationalHeap::retire_plab(PLAB* plab) {\n+  Thread* thread = Thread::current();\n+  retire_plab(plab, thread);\n+}\n+\n@@ -204,3 +606,1 @@\n-  const size_t mixed_candidates = old_heuristics()->unprocessed_old_collection_candidates();\n-  const bool doing_mixed = (mixed_candidates > 0);\n-  if (doing_mixed) {\n+  if (old_generation()->has_unprocessed_collection_candidates()) {\n@@ -210,1 +610,1 @@\n-            (old_heuristics()->unprocessed_old_collection_candidates_live_memory() * ShenandoahOldEvacWaste);\n+            (old_generation()->unprocessed_collection_candidates_live_memory() * ShenandoahOldEvacWaste);\n","filename":"src\/hotspot\/share\/gc\/shenandoah\/shenandoahGenerationalHeap.cpp","additions":408,"deletions":8,"binary":false,"changes":416,"status":"modified"},{"patch":"@@ -34,7 +34,0 @@\n-private:\n-  const size_t _min_plab_size;\n-  const size_t _max_plab_size;\n-\n-  size_t calculate_min_plab() const;\n-  size_t calculate_max_plab() const;\n-\n@@ -44,1 +37,0 @@\n-\n@@ -47,3 +39,0 @@\n-  inline size_t plab_min_size() const { return _min_plab_size; }\n-  inline size_t plab_max_size() const { return _max_plab_size; }\n-\n@@ -51,0 +40,6 @@\n+  size_t unsafe_max_tlab_alloc(Thread *thread) const override;\n+\n+  \/\/ ---------- Evacuations and Promotions\n+  \/\/\n+  oop evacuate_object(oop p, Thread* thread) override;\n+  oop try_evacuate_object(oop p, Thread* thread, ShenandoahHeapRegion* from_region, ShenandoahAffiliation target_gen);\n@@ -52,0 +47,18 @@\n+  size_t plab_min_size() const { return _min_plab_size; }\n+  size_t plab_max_size() const { return _max_plab_size; }\n+\n+  void retire_plab(PLAB* plab);\n+  void retire_plab(PLAB* plab, Thread* thread);\n+\n+private:\n+  HeapWord* allocate_from_plab(Thread* thread, size_t size, bool is_promotion);\n+  HeapWord* allocate_from_plab_slow(Thread* thread, size_t size, bool is_promotion);\n+  HeapWord* allocate_new_plab(size_t min_size, size_t word_size, size_t* actual_size);\n+\n+  const size_t _min_plab_size;\n+  const size_t _max_plab_size;\n+\n+  static size_t calculate_min_plab();\n+  static size_t calculate_max_plab();\n+\n+public:\n@@ -63,1 +76,1 @@\n-  \/\/ Used for logging the result of a region transfer outside of the heap lock\n+  \/\/ Used for logging the result of a region transfer outside the heap lock\n","filename":"src\/hotspot\/share\/gc\/shenandoah\/shenandoahGenerationalHeap.hpp","additions":25,"deletions":12,"binary":false,"changes":37,"status":"modified"},{"patch":"@@ -49,1 +49,0 @@\n-#include \"gc\/shenandoah\/shenandoahGenerationalHeap.hpp\"\n@@ -53,0 +52,1 @@\n+#include \"gc\/shenandoah\/shenandoahGenerationalEvacuationTask.hpp\"\n@@ -693,20 +693,0 @@\n-ShenandoahOldHeuristics* ShenandoahHeap::old_heuristics() {\n-  return (ShenandoahOldHeuristics*) _old_generation->heuristics();\n-}\n-\n-ShenandoahYoungHeuristics* ShenandoahHeap::young_heuristics() {\n-  return (ShenandoahYoungHeuristics*) _young_generation->heuristics();\n-}\n-\n-bool ShenandoahHeap::doing_mixed_evacuations() {\n-  return _old_generation->state() == ShenandoahOldGeneration::EVACUATING;\n-}\n-\n-bool ShenandoahHeap::is_old_bitmap_stable() const {\n-  return _old_generation->is_mark_complete();\n-}\n-\n-bool ShenandoahHeap::is_gc_generation_young() const {\n-  return _gc_generation != nullptr && _gc_generation->is_young();\n-}\n-\n@@ -992,130 +972,0 @@\n-\/\/ Establish a new PLAB and allocate size HeapWords within it.\n-HeapWord* ShenandoahHeap::allocate_from_plab_slow(Thread* thread, size_t size, bool is_promotion) {\n-  \/\/ New object should fit the PLAB size\n-\n-  assert(mode()->is_generational(), \"PLABs only relevant to generational GC\");\n-  ShenandoahGenerationalHeap* generational_heap = (ShenandoahGenerationalHeap*) this;\n-  const size_t plab_min_size = generational_heap->plab_min_size();\n-  const size_t min_size = (size > plab_min_size)? align_up(size, CardTable::card_size_in_words()): plab_min_size;\n-\n-  \/\/ Figure out size of new PLAB, looking back at heuristics. Expand aggressively.  PLABs must align on size\n-  \/\/ of card table in order to avoid the need for synchronization when registering newly allocated objects within\n-  \/\/ the card table.\n-  size_t cur_size = ShenandoahThreadLocalData::plab_size(thread);\n-  if (cur_size == 0) {\n-    cur_size = plab_min_size;\n-  }\n-\n-  \/\/ Limit growth of PLABs to the smaller of ShenandoahMaxEvacLABRatio * the minimum size and ShenandoahHumongousThreshold.\n-  \/\/ This minimum value is represented by generational_heap->plab_max_size().  Enforcing this limit enables more equitable\n-  \/\/ distribution of available evacuation budget between the many threads that are coordinating in the evacuation effort.\n-  size_t future_size = MIN2(cur_size * 2, generational_heap->plab_max_size());\n-  assert(is_aligned(future_size, CardTable::card_size_in_words()), \"Align by design, future_size: \" SIZE_FORMAT\n-         \", alignment: \" SIZE_FORMAT \", cur_size: \" SIZE_FORMAT \", max: \" SIZE_FORMAT,\n-         future_size, (size_t) CardTable::card_size_in_words(), cur_size, generational_heap->plab_max_size());\n-\n-  \/\/ Record new heuristic value even if we take any shortcut. This captures\n-  \/\/ the case when moderately-sized objects always take a shortcut. At some point,\n-  \/\/ heuristics should catch up with them.  Note that the requested cur_size may\n-  \/\/ not be honored, but we remember that this is the preferred size.\n-  ShenandoahThreadLocalData::set_plab_size(thread, future_size);\n-  if (cur_size < size) {\n-    \/\/ The PLAB to be allocated is still not large enough to hold the object. Fall back to shared allocation.\n-    \/\/ This avoids retiring perfectly good PLABs in order to represent a single large object allocation.\n-    return nullptr;\n-  }\n-\n-  \/\/ Retire current PLAB, and allocate a new one.\n-  PLAB* plab = ShenandoahThreadLocalData::plab(thread);\n-  if (plab->words_remaining() < plab_min_size) {\n-    \/\/ Retire current PLAB, and allocate a new one.\n-    \/\/ CAUTION: retire_plab may register the remnant filler object with the remembered set scanner without a lock.  This\n-    \/\/ is safe iff it is assured that each PLAB is a whole-number multiple of card-mark memory size and each PLAB is\n-    \/\/ aligned with the start of a card's memory range.\n-    retire_plab(plab, thread);\n-\n-    size_t actual_size = 0;\n-    \/\/ allocate_new_plab resets plab_evacuated and plab_promoted and disables promotions if old-gen available is\n-    \/\/ less than the remaining evacuation need.  It also adjusts plab_preallocated and expend_promoted if appropriate.\n-    HeapWord* plab_buf = allocate_new_plab(min_size, cur_size, &actual_size);\n-    if (plab_buf == nullptr) {\n-      if (min_size == plab_min_size) {\n-        \/\/ Disable plab promotions for this thread because we cannot even allocate a plab of minimal size.  This allows us\n-        \/\/ to fail faster on subsequent promotion attempts.\n-        ShenandoahThreadLocalData::disable_plab_promotions(thread);\n-      }\n-      return NULL;\n-    } else {\n-      ShenandoahThreadLocalData::enable_plab_retries(thread);\n-    }\n-    \/\/ Since the allocated PLAB may have been down-sized for alignment, plab->allocate(size) below may still fail.\n-    if (ZeroTLAB) {\n-      \/\/ ..and clear it.\n-      Copy::zero_to_words(plab_buf, actual_size);\n-    } else {\n-      \/\/ ...and zap just allocated object.\n-#ifdef ASSERT\n-      \/\/ Skip mangling the space corresponding to the object header to\n-      \/\/ ensure that the returned space is not considered parsable by\n-      \/\/ any concurrent GC thread.\n-      size_t hdr_size = oopDesc::header_size();\n-      Copy::fill_to_words(plab_buf + hdr_size, actual_size - hdr_size, badHeapWordVal);\n-#endif \/\/ ASSERT\n-    }\n-    assert(is_aligned(actual_size, CardTable::card_size_in_words()), \"Align by design\");\n-    plab->set_buf(plab_buf, actual_size);\n-    if (is_promotion && !ShenandoahThreadLocalData::allow_plab_promotions(thread)) {\n-      return nullptr;\n-    }\n-    return plab->allocate(size);\n-  } else {\n-    \/\/ If there's still at least min_size() words available within the current plab, don't retire it.  Let's gnaw\n-    \/\/ away on this plab as long as we can.  Meanwhile, return nullptr to force this particular allocation request\n-    \/\/ to be satisfied with a shared allocation.  By packing more promotions into the previously allocated PLAB, we\n-    \/\/ reduce the likelihood of evacuation failures, and we we reduce the need for downsizing our PLABs.\n-    return nullptr;\n-  }\n-}\n-\n-\/\/ TODO: It is probably most efficient to register all objects (both promotions and evacuations) that were allocated within\n-\/\/ this plab at the time we retire the plab.  A tight registration loop will run within both code and data caches.  This change\n-\/\/ would allow smaller and faster in-line implementation of alloc_from_plab().  Since plabs are aligned on card-table boundaries,\n-\/\/ this object registration loop can be performed without acquiring a lock.\n-void ShenandoahHeap::retire_plab(PLAB* plab, Thread* thread) {\n-  \/\/ We don't enforce limits on plab_evacuated.  We let it consume all available old-gen memory in order to reduce\n-  \/\/ probability of an evacuation failure.  We do enforce limits on promotion, to make sure that excessive promotion\n-  \/\/ does not result in an old-gen evacuation failure.  Note that a failed promotion is relatively harmless.  Any\n-  \/\/ object that fails to promote in the current cycle will be eligible for promotion in a subsequent cycle.\n-\n-  \/\/ When the plab was instantiated, its entirety was treated as if the entire buffer was going to be dedicated to\n-  \/\/ promotions.  Now that we are retiring the buffer, we adjust for the reality that the plab is not entirely promotions.\n-  \/\/  1. Some of the plab may have been dedicated to evacuations.\n-  \/\/  2. Some of the plab may have been abandoned due to waste (at the end of the plab).\n-  size_t not_promoted =\n-    ShenandoahThreadLocalData::get_plab_preallocated_promoted(thread) - ShenandoahThreadLocalData::get_plab_promoted(thread);\n-  ShenandoahThreadLocalData::reset_plab_promoted(thread);\n-  ShenandoahThreadLocalData::reset_plab_evacuated(thread);\n-  ShenandoahThreadLocalData::set_plab_preallocated_promoted(thread, 0);\n-  if (not_promoted > 0) {\n-    old_generation()->unexpend_promoted(not_promoted);\n-  }\n-  const size_t original_waste = plab->waste();\n-  HeapWord* const top = plab->top();\n-\n-  \/\/ plab->retire() overwrites unused memory between plab->top() and plab->hard_end() with a dummy object to make memory parsable.\n-  \/\/ It adds the size of this unused memory, in words, to plab->waste().\n-  plab->retire();\n-  if (top != nullptr && plab->waste() > original_waste && is_in_old(top)) {\n-    \/\/ If retiring the plab created a filler object, then we need to register it with our card scanner so it can\n-    \/\/ safely walk the region backing the plab.\n-    log_debug(gc)(\"retire_plab() is registering remnant of size \" SIZE_FORMAT \" at \" PTR_FORMAT,\n-                  plab->waste() - original_waste, p2i(top));\n-    card_scan()->register_object_without_lock(top);\n-  }\n-}\n-\n-void ShenandoahHeap::retire_plab(PLAB* plab) {\n-  Thread* thread = Thread::current();\n-  retire_plab(plab, thread);\n-}\n-\n@@ -1124,6 +974,5 @@\n-  assert(_old_generation != nullptr, \"Should only have mixed collections in generation mode.\");\n-  if (_old_generation->state() == ShenandoahOldGeneration::WAITING_FOR_BOOTSTRAP) {\n-    assert(!old_generation()->is_concurrent_mark_in_progress(), \"Cannot be marking in IDLE\");\n-    assert(!old_heuristics()->has_coalesce_and_fill_candidates(), \"Cannot have coalesce and fill candidates in IDLE\");\n-    assert(!old_heuristics()->unprocessed_old_collection_candidates(), \"Cannot have mixed collection candidates in IDLE\");\n-    assert(!young_generation()->is_bootstrap_cycle(), \"Cannot have old mark queues if IDLE\");\n+  assert(old_generation() != nullptr, \"Should only have mixed collections in generation mode.\");\n+  if (old_generation()->state() == ShenandoahOldGeneration::WAITING_FOR_BOOTSTRAP) {\n+#ifdef ASSERT\n+    old_generation()->validate_waiting_for_bootstrap();\n+#endif\n@@ -1135,1 +984,1 @@\n-    old_heuristics()->abandon_collection_candidates();\n+    old_generation()->abandon_collection_candidates();\n@@ -1139,1 +988,1 @@\n-    _old_generation->transition_to(ShenandoahOldGeneration::WAITING_FOR_BOOTSTRAP);\n+    old_generation()->transition_to(ShenandoahOldGeneration::WAITING_FOR_BOOTSTRAP);\n@@ -1148,1 +997,1 @@\n-  HeapWord* res = allocate_memory(req, false);\n+  HeapWord* res = allocate_memory(req);\n@@ -1161,1 +1010,1 @@\n-  HeapWord* res = allocate_memory(req, false);\n+  HeapWord* res = allocate_memory(req);\n@@ -1170,17 +1019,0 @@\n-HeapWord* ShenandoahHeap::allocate_new_plab(size_t min_size, size_t word_size, size_t* actual_size) {\n-  \/\/ Align requested sizes to card-sized multiples.  Align down so that we don't violate max size of TLAB.\n-  assert(is_aligned(min_size, CardTable::card_size_in_words()), \"Align by design\");\n-  assert(word_size >= min_size, \"Requested PLAB is too small\");\n-\n-  ShenandoahAllocRequest req = ShenandoahAllocRequest::for_plab(min_size, word_size);\n-  \/\/ Note that allocate_memory() sets a thread-local flag to prohibit further promotions by this thread\n-  \/\/ if we are at risk of infringing on the old-gen evacuation budget.\n-  HeapWord* res = allocate_memory(req, false);\n-  if (res != nullptr) {\n-    *actual_size = req.actual_size();\n-  } else {\n-    *actual_size = 0;\n-  }\n-  assert(is_aligned(res, CardTable::card_size_in_words()), \"Align by design\");\n-  return res;\n-}\n@@ -1190,1 +1022,1 @@\n-HeapWord* ShenandoahHeap::allocate_memory(ShenandoahAllocRequest& req, bool is_promotion) {\n+HeapWord* ShenandoahHeap::allocate_memory(ShenandoahAllocRequest& req) {\n@@ -1202,1 +1034,1 @@\n-      result = allocate_memory_under_lock(req, in_new_region, is_promotion);\n+      result = allocate_memory_under_lock(req, in_new_region);\n@@ -1228,1 +1060,1 @@\n-      result = allocate_memory_under_lock(req, in_new_region, is_promotion);\n+      result = allocate_memory_under_lock(req, in_new_region);\n@@ -1239,1 +1071,1 @@\n-    result = allocate_memory_under_lock(req, in_new_region, is_promotion);\n+    result = allocate_memory_under_lock(req, in_new_region);\n@@ -1277,1 +1109,1 @@\n-HeapWord* ShenandoahHeap::allocate_memory_under_lock(ShenandoahAllocRequest& req, bool& in_new_region, bool is_promotion) {\n+HeapWord* ShenandoahHeap::allocate_memory_under_lock(ShenandoahAllocRequest& req, bool& in_new_region) {\n@@ -1329,1 +1161,1 @@\n-        } else if (is_promotion) {\n+        } else if (req.is_promotion()) {\n@@ -1380,1 +1212,1 @@\n-          } else if (is_promotion) {\n+          } else if (req.is_promotion()) {\n@@ -1404,1 +1236,1 @@\n-        ShenandoahHeap::heap()->card_scan()->register_object(result);\n+        card_scan()->register_object(result);\n@@ -1457,1 +1289,1 @@\n-  HeapWord* result = allocate_memory_under_lock(smaller_req, in_new_region, is_promotion);\n+  HeapWord* result = allocate_memory_under_lock(smaller_req, in_new_region);\n@@ -1467,1 +1299,1 @@\n-  return allocate_memory(req, false);\n+  return allocate_memory(req);\n@@ -1556,1 +1388,0 @@\n-\n@@ -1562,90 +1393,0 @@\n-      if (_sh->check_cancelled_gc_and_yield(_concurrent)) {\n-        break;\n-      }\n-    }\n-  }\n-};\n-\n-\/\/ Unlike ShenandoahEvacuationTask, this iterates over all regions rather than just the collection set.\n-\/\/ This is needed in order to promote humongous start regions if age() >= tenure threshold.\n-class ShenandoahGenerationalEvacuationTask : public WorkerTask {\n-private:\n-  ShenandoahHeap* const _sh;\n-  ShenandoahRegionIterator *_regions;\n-  bool _concurrent;\n-  uint _tenuring_threshold;\n-\n-public:\n-  ShenandoahGenerationalEvacuationTask(ShenandoahHeap* sh,\n-                                       ShenandoahRegionIterator* iterator,\n-                                       bool concurrent) :\n-    WorkerTask(\"Shenandoah Evacuation\"),\n-    _sh(sh),\n-    _regions(iterator),\n-    _concurrent(concurrent),\n-    _tenuring_threshold(0)\n-  {\n-    if (_sh->mode()->is_generational()) {\n-      _tenuring_threshold = _sh->age_census()->tenuring_threshold();\n-    }\n-  }\n-\n-  void work(uint worker_id) {\n-    if (_concurrent) {\n-      ShenandoahConcurrentWorkerSession worker_session(worker_id);\n-      ShenandoahSuspendibleThreadSetJoiner stsj;\n-      ShenandoahEvacOOMScope oom_evac_scope;\n-      do_work();\n-    } else {\n-      ShenandoahParallelWorkerSession worker_session(worker_id);\n-      ShenandoahEvacOOMScope oom_evac_scope;\n-      do_work();\n-    }\n-  }\n-\n-private:\n-  void do_work() {\n-    ShenandoahConcurrentEvacuateRegionObjectClosure cl(_sh);\n-    ShenandoahHeapRegion* r;\n-    ShenandoahMarkingContext* const ctx = ShenandoahHeap::heap()->marking_context();\n-    size_t region_size_bytes = ShenandoahHeapRegion::region_size_bytes();\n-    size_t old_garbage_threshold = (region_size_bytes * ShenandoahOldGarbageThreshold) \/ 100;\n-    while ((r = _regions->next()) != nullptr) {\n-      log_debug(gc)(\"GenerationalEvacuationTask do_work(), looking at %s region \" SIZE_FORMAT \", (age: %d) [%s, %s, %s]\",\n-                    r->is_old()? \"old\": r->is_young()? \"young\": \"free\", r->index(), r->age(),\n-                    r->is_active()? \"active\": \"inactive\",\n-                    r->is_humongous()? (r->is_humongous_start()? \"humongous_start\": \"humongous_continuation\"): \"regular\",\n-                    r->is_cset()? \"cset\": \"not-cset\");\n-\n-      if (r->is_cset()) {\n-        assert(r->has_live(), \"Region \" SIZE_FORMAT \" should have been reclaimed early\", r->index());\n-        _sh->marked_object_iterate(r, &cl);\n-        if (ShenandoahPacing) {\n-          _sh->pacer()->report_evac(r->used() >> LogHeapWordSize);\n-        }\n-      } else if (r->is_young() && r->is_active() && (r->age() >= _tenuring_threshold)) {\n-        HeapWord* tams = ctx->top_at_mark_start(r);\n-        if (r->is_humongous_start()) {\n-          \/\/ We promote humongous_start regions along with their affiliated continuations during evacuation rather than\n-          \/\/ doing this work during a safepoint.  We cannot put humongous regions into the collection set because that\n-          \/\/ triggers the load-reference barrier (LRB) to copy on reference fetch.\n-          r->promote_humongous();\n-        } else if (r->is_regular() && (r->get_top_before_promote() != nullptr)) {\n-          assert(r->garbage_before_padded_for_promote() < old_garbage_threshold,\n-                 \"Region \" SIZE_FORMAT \" has too much garbage for promotion\", r->index());\n-          assert(r->get_top_before_promote() == tams,\n-                 \"Region \" SIZE_FORMAT \" has been used for allocations before promotion\", r->index());\n-          \/\/ Likewise, we cannot put promote-in-place regions into the collection set because that would also trigger\n-          \/\/ the LRB to copy on reference fetch.\n-          r->promote_in_place();\n-        }\n-        \/\/ Aged humongous continuation regions are handled with their start region.  If an aged regular region has\n-        \/\/ more garbage than ShenandoahOldGarbageTrheshold, we'll promote by evacuation.  If there is room for evacuation\n-        \/\/ in this cycle, the region will be in the collection set.  If there is not room, the region will be promoted\n-        \/\/ by evacuation in some future GC cycle.\n-\n-        \/\/ If an aged regular region has received allocations during the current cycle, we do not promote because the\n-        \/\/ newly allocated objects do not have appropriate age; this region's age will be reset to zero at end of cycle.\n-      }\n-      \/\/ else, region is free, or OLD, or not in collection set, or humongous_continuation,\n-      \/\/ or is young humongous_start that is too young to be promoted\n@@ -1661,1 +1402,1 @@\n-  if (ShenandoahHeap::heap()->mode()->is_generational()) {\n+  if (mode()->is_generational()) {\n@@ -1663,1 +1404,1 @@\n-    ShenandoahGenerationalEvacuationTask task(this, &regions, concurrent);\n+    ShenandoahGenerationalEvacuationTask task(ShenandoahGenerationalHeap::heap(), &regions, concurrent);\n@@ -1671,2 +1412,17 @@\n-\/\/ try_evacuate_object registers the object and dirties the associated remembered set information when evacuating\n-\/\/ to OLD_GENERATION.\n+oop ShenandoahHeap::evacuate_object(oop p, Thread* thread) {\n+  assert(thread == Thread::current(), \"Expected thread parameter to be current thread.\");\n+  if (ShenandoahThreadLocalData::is_oom_during_evac(thread)) {\n+    \/\/ This thread went through the OOM during evac protocol. It is safe to return\n+    \/\/ the forward pointer. It must not attempt to evacuate any other objects.\n+    return ShenandoahBarrierSet::resolve_forwarded(p);\n+  }\n+\n+  assert(ShenandoahThreadLocalData::is_evac_allowed(thread), \"must be enclosed in oom-evac scope\");\n+\n+  ShenandoahHeapRegion* r = heap_region_containing(p);\n+  assert(!r->is_humongous(), \"never evacuate humongous objects\");\n+\n+  ShenandoahAffiliation target_gen = r->affiliation();\n+  return try_evacuate_object(p, thread, r, target_gen);\n+}\n+\n@@ -1675,0 +1431,2 @@\n+  assert(target_gen == YOUNG_GENERATION, \"Only expect evacuations to young in this mode\");\n+  assert(from_region->is_young(), \"Only expect evacuations from young in this mode\");\n@@ -1676,1 +1434,0 @@\n-  bool has_plab = false;\n@@ -1679,1 +1436,0 @@\n-  bool is_promotion = (target_gen == OLD_GENERATION) && from_region->is_young();\n@@ -1688,46 +1444,8 @@\n-      switch (target_gen) {\n-        case YOUNG_GENERATION: {\n-          copy = allocate_from_gclab(thread, size);\n-          if ((copy == nullptr) && (size < ShenandoahThreadLocalData::gclab_size(thread))) {\n-            \/\/ GCLAB allocation failed because we are bumping up against the limit on young evacuation reserve.  Try resetting\n-            \/\/ the desired GCLAB size and retry GCLAB allocation to avoid cascading of shared memory allocations.\n-            ShenandoahThreadLocalData::set_gclab_size(thread, PLAB::min_size());\n-            copy = allocate_from_gclab(thread, size);\n-            \/\/ If we still get nullptr, we'll try a shared allocation below.\n-          }\n-          break;\n-        }\n-        case OLD_GENERATION: {\n-          assert(mode()->is_generational(), \"OLD Generation only exists in generational mode\");\n-          ShenandoahGenerationalHeap* gen_heap = (ShenandoahGenerationalHeap*) this;\n-          PLAB* plab = ShenandoahThreadLocalData::plab(thread);\n-          if (plab != nullptr) {\n-            has_plab = true;\n-          }\n-          copy = allocate_from_plab(thread, size, is_promotion);\n-          if ((copy == nullptr) && (size < ShenandoahThreadLocalData::plab_size(thread)) &&\n-              ShenandoahThreadLocalData::plab_retries_enabled(thread)) {\n-            \/\/ PLAB allocation failed because we are bumping up against the limit on old evacuation reserve or because\n-            \/\/ the requested object does not fit within the current plab but the plab still has an \"abundance\" of memory,\n-            \/\/ where abundance is defined as >= ShenGenHeap::plab_min_size().  In the former case, we try resetting the desired\n-            \/\/ PLAB size and retry PLAB allocation to avoid cascading of shared memory allocations.\n-\n-            \/\/ In this situation, PLAB memory is precious.  We'll try to preserve our existing PLAB by forcing\n-            \/\/ this particular allocation to be shared.\n-            if (plab->words_remaining() < gen_heap->plab_min_size()) {\n-              ShenandoahThreadLocalData::set_plab_size(thread, gen_heap->plab_min_size());\n-              copy = allocate_from_plab(thread, size, is_promotion);\n-              \/\/ If we still get nullptr, we'll try a shared allocation below.\n-              if (copy == nullptr) {\n-                \/\/ If retry fails, don't continue to retry until we have success (probably in next GC pass)\n-                ShenandoahThreadLocalData::disable_plab_retries(thread);\n-              }\n-            }\n-            \/\/ else, copy still equals nullptr.  this causes shared allocation below, preserving this plab for future needs.\n-          }\n-          break;\n-        }\n-        default: {\n-          ShouldNotReachHere();\n-          break;\n-        }\n+      copy = allocate_from_gclab(thread, size);\n+      if ((copy == nullptr) && (size < ShenandoahThreadLocalData::gclab_size(thread))) {\n+        \/\/ GCLAB allocation failed because we are bumping up against the limit on young evacuation reserve.  Try resetting\n+        \/\/ the desired GCLAB size and retry GCLAB allocation to avoid cascading of shared memory allocations.\n+        \/\/ TODO: is this right? using PLAB::min_size() here for gc lab size?\n+        ShenandoahThreadLocalData::set_gclab_size(thread, PLAB::min_size());\n+        copy = allocate_from_gclab(thread, size);\n+        \/\/ If we still get nullptr, we'll try a shared allocation below.\n@@ -1739,9 +1457,3 @@\n-      if (!is_promotion || !has_plab || (size > PLAB::min_size())) {\n-        ShenandoahAllocRequest req = ShenandoahAllocRequest::for_shared_gc(size, target_gen);\n-        copy = allocate_memory(req, is_promotion);\n-        alloc_from_lab = false;\n-      }\n-      \/\/ else, we leave copy equal to nullptr, signaling a promotion failure below if appropriate.\n-      \/\/ We choose not to promote objects smaller than PLAB::min_size() by way of shared allocations, as this is too\n-      \/\/ costly.  Instead, we'll simply \"evacuate\" to young-gen memory (using a GCLAB) and will promote in a future\n-      \/\/ evacuation pass.  This condition is denoted by: is_promotion && has_plab && (size <= PLAB::min_size())\n+      ShenandoahAllocRequest req = ShenandoahAllocRequest::for_shared_gc(size, target_gen);\n+      copy = allocate_memory(req);\n+      alloc_from_lab = false;\n@@ -1754,13 +1466,0 @@\n-    if (target_gen == OLD_GENERATION) {\n-      assert(mode()->is_generational(), \"Should only be here in generational mode.\");\n-      if (from_region->is_young()) {\n-        \/\/ Signal that promotion failed. Will evacuate this old object somewhere in young gen.\n-        old_generation()->handle_failed_promotion(thread, size);\n-        return nullptr;\n-      } else {\n-        \/\/ Remember that evacuation to old gen failed. We'll want to trigger a full gc to recover from this\n-        \/\/ after the evacuation threads have finished.\n-        old_generation()->handle_failed_evacuation();\n-      }\n-    }\n-\n@@ -1780,4 +1479,0 @@\n-  if (mode()->is_generational() && target_gen == YOUNG_GENERATION && is_aging_cycle()) {\n-    ShenandoahHeap::increase_object_age(copy_val, from_region->age() + 1);\n-  }\n-\n@@ -1791,14 +1486,0 @@\n-    if (mode()->is_generational()) {\n-      if (target_gen == OLD_GENERATION) {\n-        old_generation()->handle_evacuation(copy, size, from_region->is_young());\n-      } else {\n-        \/\/ When copying to the old generation above, we don't care\n-        \/\/ about recording object age in the census stats.\n-        assert(target_gen == YOUNG_GENERATION, \"Error\");\n-        \/\/ We record this census only when simulating pre-adaptive tenuring behavior, or\n-        \/\/ when we have been asked to record the census at evacuation rather than at mark\n-        if (ShenandoahGenerationalCensusAtEvac || !ShenandoahGenerationalAdaptiveTenuring) {\n-          _evac_tracker->record_age(thread, size * HeapWordSize, ShenandoahHeap::get_object_age(copy_val));\n-        }\n-      }\n-    }\n@@ -1817,19 +1498,1 @@\n-      switch (target_gen) {\n-        case YOUNG_GENERATION: {\n-          ShenandoahThreadLocalData::gclab(thread)->undo_allocation(copy, size);\n-          break;\n-        }\n-        case OLD_GENERATION: {\n-          ShenandoahThreadLocalData::plab(thread)->undo_allocation(copy, size);\n-          if (is_promotion) {\n-            ShenandoahThreadLocalData::subtract_from_plab_promoted(thread, size * HeapWordSize);\n-          } else {\n-            ShenandoahThreadLocalData::subtract_from_plab_evacuated(thread, size * HeapWordSize);\n-          }\n-          break;\n-        }\n-        default: {\n-          ShouldNotReachHere();\n-          break;\n-        }\n-      }\n+      ShenandoahThreadLocalData::gclab(thread)->undo_allocation(copy, size);\n@@ -1908,3 +1571,5 @@\n-    PLAB* plab = ShenandoahThreadLocalData::plab(thread);\n-    assert(plab != nullptr, \"PLAB should be initialized for %s\", thread->name());\n-    assert(plab->words_remaining() == 0, \"PLAB should not need retirement\");\n+    if (ShenandoahHeap::heap()->mode()->is_generational()) {\n+      PLAB* plab = ShenandoahThreadLocalData::plab(thread);\n+      assert(plab != nullptr, \"PLAB should be initialized for %s\", thread->name());\n+      assert(plab->words_remaining() == 0, \"PLAB should not need retirement\");\n+    }\n@@ -1927,2 +1592,3 @@\n-    PLAB* plab = ShenandoahThreadLocalData::plab(thread);\n-    assert(plab != nullptr, \"PLAB should be initialized for %s\", thread->name());\n+    if (ShenandoahHeap::heap()->mode()->is_generational()) {\n+      PLAB* plab = ShenandoahThreadLocalData::plab(thread);\n+      assert(plab != nullptr, \"PLAB should be initialized for %s\", thread->name());\n@@ -1930,6 +1596,7 @@\n-    \/\/ There are two reasons to retire all plabs between old-gen evacuation passes.\n-    \/\/  1. We need to make the plab memory parsable by remembered-set scanning.\n-    \/\/  2. We need to establish a trustworthy UpdateWaterMark value within each old-gen heap region\n-    ShenandoahHeap::heap()->retire_plab(plab, thread);\n-    if (_resize && ShenandoahThreadLocalData::plab_size(thread) > 0) {\n-      ShenandoahThreadLocalData::set_plab_size(thread, 0);\n+      \/\/ There are two reasons to retire all plabs between old-gen evacuation passes.\n+      \/\/  1. We need to make the plab memory parsable by remembered-set scanning.\n+      \/\/  2. We need to establish a trustworthy UpdateWaterMark value within each old-gen heap region\n+      ShenandoahGenerationalHeap::heap()->retire_plab(plab, thread);\n+      if (_resize && ShenandoahThreadLocalData::plab_size(thread) > 0) {\n+        ShenandoahThreadLocalData::set_plab_size(thread, 0);\n+      }\n@@ -1996,6 +1663,2 @@\n-  if (mode()->is_generational()) {\n-    return MIN2(ShenandoahHeapRegion::max_tlab_size_bytes(), young_generation()->available());\n-  } else {\n-    \/\/ Return the max allowed size, and let the allocation path figure out the safe size for current allocation.\n-    return ShenandoahHeapRegion::max_tlab_size_bytes();\n-  }\n+  \/\/ Return the max allowed size, and let the allocation path figure out the safe size for current allocation.\n+  return ShenandoahHeapRegion::max_tlab_size_bytes();\n@@ -2832,1 +2495,2 @@\n-    log_info(gc, remset)(\"Scan remembered set using bitmap: %s\", BOOL_TO_STR(_heap->is_old_bitmap_stable()));\n+    bool old_bitmap_stable = _heap->old_generation()->is_mark_complete();\n+    log_info(gc, remset)(\"Scan remembered set using bitmap: %s\", BOOL_TO_STR(old_bitmap_stable));\n@@ -3053,3 +2717,3 @@\n-  ShenandoahFinalUpdateRefsUpdateRegionStateClosure(\n-    ShenandoahMarkingContext* ctx) : _ctx(ctx), _lock(ShenandoahHeap::heap()->lock()),\n-                                     _is_generational(ShenandoahHeap::heap()->mode()->is_generational()) { }\n+  ShenandoahFinalUpdateRefsUpdateRegionStateClosure(ShenandoahMarkingContext* ctx) :\n+    _ctx(ctx), _lock(ShenandoahHeap::heap()->lock()),\n+    _is_generational(ShenandoahHeap::heap()->mode()->is_generational()) { }\n@@ -3057,1 +2721,1 @@\n-  void heap_region_do(ShenandoahHeapRegion* r) {\n+  void heap_region_do(ShenandoahHeapRegion* r) override {\n@@ -3066,1 +2730,1 @@\n-      \/\/ Allocations move the watermark when top moves.  However compacting\n+      \/\/ Allocations move the watermark when top moves.  However, compacting\n@@ -3096,1 +2760,1 @@\n-  bool is_thread_safe() { return true; }\n+  bool is_thread_safe() override { return true; }\n@@ -3125,1 +2789,0 @@\n-  size_t region_size_bytes = ShenandoahHeapRegion::region_size_bytes();\n@@ -3146,1 +2809,1 @@\n-    size_t allocation_runway = young_heuristics()->bytes_of_allocation_runway_before_gc_trigger(young_cset_regions);\n+    size_t allocation_runway = young_generation()->heuristics()->bytes_of_allocation_runway_before_gc_trigger(young_cset_regions);\n@@ -3161,45 +2824,1 @@\n-    size_t old_region_span = (first_old_region <= last_old_region)? (last_old_region + 1 - first_old_region): 0;\n-    size_t allowed_old_gen_span = num_regions() - (ShenandoahGenerationalHumongousReserve * num_regions() \/ 100);\n-\n-    \/\/ Tolerate lower density if total span is small.  Here's the implementation:\n-    \/\/   if old_gen spans more than 100% and density < 75%, trigger old-defrag\n-    \/\/   else if old_gen spans more than 87.5% and density < 62.5%, trigger old-defrag\n-    \/\/   else if old_gen spans more than 75% and density < 50%, trigger old-defrag\n-    \/\/   else if old_gen spans more than 62.5% and density < 37.5%, trigger old-defrag\n-    \/\/   else if old_gen spans more than 50% and density < 25%, trigger old-defrag\n-    \/\/\n-    \/\/ A previous implementation was more aggressive in triggering, resulting in degraded throughput when\n-    \/\/ humongous allocation was not required.\n-\n-    ShenandoahGeneration* old_gen = old_generation();\n-    size_t old_available = old_gen->available();\n-    size_t region_size_bytes = ShenandoahHeapRegion::region_size_bytes();\n-    size_t old_unaffiliated_available = old_gen->free_unaffiliated_regions() * region_size_bytes;\n-    assert(old_available >= old_unaffiliated_available, \"sanity\");\n-    size_t old_fragmented_available = old_available - old_unaffiliated_available;\n-\n-    size_t old_bytes_consumed = old_region_count * region_size_bytes - old_fragmented_available;\n-    size_t old_bytes_spanned = old_region_span * region_size_bytes;\n-    double old_density = ((double) old_bytes_consumed) \/ old_bytes_spanned;\n-\n-    uint eighths = 8;\n-    for (uint i = 0; i < 5; i++) {\n-      size_t span_threshold = eighths * allowed_old_gen_span \/ 8;\n-      double density_threshold = (eighths - 2) \/ 8.0;\n-      if ((old_region_span >= span_threshold) && (old_density < density_threshold)) {\n-        old_heuristics()->trigger_old_is_fragmented(old_density, first_old_region, last_old_region);\n-        break;\n-      }\n-      eighths--;\n-    }\n-\n-    size_t old_used = old_generation()->used() + old_generation()->get_humongous_waste();\n-    size_t trigger_threshold = old_generation()->usage_trigger_threshold();\n-    \/\/ Detects unsigned arithmetic underflow\n-    assert(old_used <= capacity(),\n-           \"Old used (\" SIZE_FORMAT \", \" SIZE_FORMAT\") must not be more than heap capacity (\" SIZE_FORMAT \")\",\n-           old_generation()->used(), old_generation()->get_humongous_waste(), capacity());\n-\n-    if (old_used > trigger_threshold) {\n-      old_heuristics()->trigger_old_has_grown();\n-    }\n+    old_generation()->maybe_trigger_collection(first_old_region, last_old_region, old_region_count);\n@@ -3470,0 +3089,12 @@\n+\n+void ShenandoahHeap::clear_cards_for(ShenandoahHeapRegion* region) {\n+  if (mode()->is_generational()) {\n+    _card_scan->mark_range_as_empty(region->bottom(), pointer_delta(region->end(), region->bottom()));\n+  }\n+}\n+\n+void ShenandoahHeap::mark_card_as_dirty(void* location) {\n+  if (mode()->is_generational()) {\n+    _card_scan->mark_card_as_dirty((HeapWord*)location);\n+  }\n+}\n","filename":"src\/hotspot\/share\/gc\/shenandoah\/shenandoahHeap.cpp","additions":92,"deletions":461,"binary":false,"changes":553,"status":"modified"},{"patch":"@@ -171,6 +171,0 @@\n-  ShenandoahOldHeuristics* old_heuristics();\n-  ShenandoahYoungHeuristics* young_heuristics();\n-\n-  bool doing_mixed_evacuations();\n-  bool is_old_bitmap_stable() const;\n-  bool is_gc_generation_young() const;\n@@ -514,0 +508,2 @@\n+  ShenandoahEvacOOMHandler* oom_evac_handler() { return &_oom_evac_handler; }\n+\n@@ -641,3 +637,1 @@\n-private:\n-  HeapWord* allocate_memory_under_lock(ShenandoahAllocRequest& request, bool& in_new_region, bool is_promotion);\n-\n+protected:\n@@ -645,0 +639,3 @@\n+\n+private:\n+  HeapWord* allocate_memory_under_lock(ShenandoahAllocRequest& request, bool& in_new_region);\n@@ -648,4 +645,0 @@\n-  inline HeapWord* allocate_from_plab(Thread* thread, size_t size, bool is_promotion);\n-  HeapWord* allocate_from_plab_slow(Thread* thread, size_t size, bool is_promotion);\n-  HeapWord* allocate_new_plab(size_t min_size, size_t word_size, size_t* actual_size);\n-\n@@ -653,1 +646,1 @@\n-  HeapWord* allocate_memory(ShenandoahAllocRequest& request, bool is_promotion);\n+  HeapWord* allocate_memory(ShenandoahAllocRequest& request);\n@@ -742,1 +735,1 @@\n-  inline oop evacuate_object(oop src, Thread* thread);\n+  virtual oop evacuate_object(oop src, Thread* thread);\n@@ -757,2 +750,0 @@\n-  void retire_plab(PLAB* plab);\n-  void retire_plab(PLAB* plab, Thread* thread);\n","filename":"src\/hotspot\/share\/gc\/shenandoah\/shenandoahHeap.hpp","additions":8,"deletions":17,"binary":false,"changes":25,"status":"modified"},{"patch":"@@ -44,1 +44,1 @@\n-#include \"gc\/shenandoah\/shenandoahGenerationalControlThread.hpp\"\n+#include \"gc\/shenandoah\/shenandoahGeneration.hpp\"\n@@ -47,1 +47,0 @@\n-#include \"gc\/shenandoah\/shenandoahScanRemembered.inline.hpp\"\n@@ -301,32 +300,0 @@\n-inline HeapWord* ShenandoahHeap::allocate_from_plab(Thread* thread, size_t size, bool is_promotion) {\n-  assert(UseTLAB, \"TLABs should be enabled\");\n-\n-  PLAB* plab = ShenandoahThreadLocalData::plab(thread);\n-  HeapWord* obj;\n-\n-  if (plab == nullptr) {\n-    assert(!thread->is_Java_thread() && !thread->is_Worker_thread(), \"Performance: thread should have PLAB: %s\", thread->name());\n-    \/\/ No PLABs in this thread, fallback to shared allocation\n-    return nullptr;\n-  } else if (is_promotion && !ShenandoahThreadLocalData::allow_plab_promotions(thread)) {\n-    return nullptr;\n-  }\n-  \/\/ if plab->word_size() <= 0, thread's plab not yet initialized for this pass, so allow_plab_promotions() is not trustworthy\n-  obj = plab->allocate(size);\n-  if ((obj == nullptr) && (plab->words_remaining() < ShenandoahGenerationalHeap::heap()->plab_min_size())) {\n-    \/\/ allocate_from_plab_slow will establish allow_plab_promotions(thread) for future invocations\n-    obj = allocate_from_plab_slow(thread, size, is_promotion);\n-  }\n-  \/\/ if plab->words_remaining() >= ShenGenHeap::heap()->plab_min_size(), just return nullptr so we can use a shared allocation\n-  if (obj == nullptr) {\n-    return nullptr;\n-  }\n-\n-  if (is_promotion) {\n-    ShenandoahThreadLocalData::add_to_plab_promoted(thread, size * HeapWordSize);\n-  } else {\n-    ShenandoahThreadLocalData::add_to_plab_evacuated(thread, size * HeapWordSize);\n-  }\n-  return obj;\n-}\n-\n@@ -339,35 +306,0 @@\n-inline oop ShenandoahHeap::evacuate_object(oop p, Thread* thread) {\n-  assert(thread == Thread::current(), \"Expected thread parameter to be current thread.\");\n-  if (ShenandoahThreadLocalData::is_oom_during_evac(thread)) {\n-    \/\/ This thread went through the OOM during evac protocol and it is safe to return\n-    \/\/ the forward pointer. It must not attempt to evacuate any more.\n-    return ShenandoahBarrierSet::resolve_forwarded(p);\n-  }\n-\n-  assert(ShenandoahThreadLocalData::is_evac_allowed(thread), \"must be enclosed in oom-evac scope\");\n-\n-  ShenandoahHeapRegion* r = heap_region_containing(p);\n-  assert(!r->is_humongous(), \"never evacuate humongous objects\");\n-\n-  ShenandoahAffiliation target_gen = r->affiliation();\n-  if (mode()->is_generational() && ShenandoahHeap::heap()->is_gc_generation_young() &&\n-      target_gen == YOUNG_GENERATION) {\n-    markWord mark = p->mark();\n-    if (mark.is_marked()) {\n-      \/\/ Already forwarded.\n-      return ShenandoahBarrierSet::resolve_forwarded(p);\n-    }\n-    if (mark.has_displaced_mark_helper()) {\n-      \/\/ We don't want to deal with MT here just to ensure we read the right mark word.\n-      \/\/ Skip the potential promotion attempt for this one.\n-    } else if (r->age() + mark.age() >= age_census()->tenuring_threshold()) {\n-      oop result = try_evacuate_object(p, thread, r, OLD_GENERATION);\n-      if (result != nullptr) {\n-        return result;\n-      }\n-      \/\/ If we failed to promote this aged object, we'll fall through to code below and evacuate to young-gen.\n-    }\n-  }\n-  return try_evacuate_object(p, thread, r, target_gen);\n-}\n-\n@@ -471,1 +403,1 @@\n-  return is_gc_generation_young() && is_in_old(obj);\n+  return active_generation()->is_young() && is_in_old(obj);\n@@ -731,12 +663,0 @@\n-inline void ShenandoahHeap::clear_cards_for(ShenandoahHeapRegion* region) {\n-  if (mode()->is_generational()) {\n-    _card_scan->mark_range_as_empty(region->bottom(), pointer_delta(region->end(), region->bottom()));\n-  }\n-}\n-\n-inline void ShenandoahHeap::mark_card_as_dirty(void* location) {\n-  if (mode()->is_generational()) {\n-    _card_scan->mark_card_as_dirty((HeapWord*)location);\n-  }\n-}\n-\n","filename":"src\/hotspot\/share\/gc\/shenandoah\/shenandoahHeap.inline.hpp","additions":2,"deletions":82,"binary":false,"changes":84,"status":"modified"},{"patch":"@@ -550,50 +550,0 @@\n-void ShenandoahHeapRegion::global_oop_iterate_and_fill_dead(OopIterateClosure* blk) {\n-  if (!is_active()) return;\n-  if (is_humongous()) {\n-    \/\/ No need to fill dead within humongous regions.  Either the entire region is dead, or the entire region is\n-    \/\/ unchanged.  A humongous region holds no more than one humongous object.\n-    oop_iterate_humongous(blk);\n-  } else {\n-    global_oop_iterate_objects_and_fill_dead(blk);\n-  }\n-}\n-\n-void ShenandoahHeapRegion::global_oop_iterate_objects_and_fill_dead(OopIterateClosure* blk) {\n-  assert(!is_humongous(), \"no humongous region here\");\n-  HeapWord* obj_addr = bottom();\n-\n-  ShenandoahHeap* heap = ShenandoahHeap::heap();\n-  ShenandoahMarkingContext* marking_context = heap->marking_context();\n-  RememberedScanner* rem_set_scanner = heap->card_scan();\n-  \/\/ Objects allocated above TAMS are not marked, but are considered live for purposes of current GC efforts.\n-  HeapWord* t = marking_context->top_at_mark_start(this);\n-\n-  assert(heap->active_generation()->is_mark_complete(), \"sanity\");\n-\n-  while (obj_addr < t) {\n-    oop obj = cast_to_oop(obj_addr);\n-    if (marking_context->is_marked(obj)) {\n-      assert(obj->klass() != nullptr, \"klass should not be nullptr\");\n-      \/\/ when promoting an entire region, we have to register the marked objects as well\n-      obj_addr += obj->oop_iterate_size(blk);\n-    } else {\n-      \/\/ Object is not marked.  Coalesce and fill dead object with dead neighbors.\n-      HeapWord* next_marked_obj = marking_context->get_next_marked_addr(obj_addr, t);\n-      assert(next_marked_obj <= t, \"next marked object cannot exceed top\");\n-      size_t fill_size = next_marked_obj - obj_addr;\n-      assert(fill_size >= ShenandoahHeap::min_fill_size(), \"previously allocated objects known to be larger than min_size\");\n-      ShenandoahHeap::fill_with_object(obj_addr, fill_size);\n-      \/\/ coalesce_objects() unregisters all but first object subsumed within coalesced range.\n-      rem_set_scanner->coalesce_objects(obj_addr, fill_size);\n-      obj_addr = next_marked_obj;\n-    }\n-  }\n-\n-  \/\/ Any object above TAMS and below top() is considered live.\n-  t = top();\n-  while (obj_addr < t) {\n-    oop obj = cast_to_oop(obj_addr);\n-    obj_addr += obj->oop_iterate_size(blk);\n-  }\n-}\n-\n@@ -641,18 +591,0 @@\n-void ShenandoahHeapRegion::oop_iterate_humongous(OopIterateClosure* blk, HeapWord* start, size_t words) {\n-  assert(is_humongous(), \"only humongous region here\");\n-  \/\/ Find head.\n-  ShenandoahHeapRegion* r = humongous_start_region();\n-  assert(r->is_humongous_start(), \"need humongous head here\");\n-  oop obj = cast_to_oop(r->bottom());\n-  obj->oop_iterate(blk, MemRegion(start, start + words));\n-}\n-\n-void ShenandoahHeapRegion::oop_iterate_humongous(OopIterateClosure* blk) {\n-  assert(is_humongous(), \"only humongous region here\");\n-  \/\/ Find head.\n-  ShenandoahHeapRegion* r = humongous_start_region();\n-  assert(r->is_humongous_start(), \"need humongous head here\");\n-  oop obj = cast_to_oop(r->bottom());\n-  obj->oop_iterate(blk, MemRegion(bottom(), top()));\n-}\n-\n@@ -984,163 +916,0 @@\n-\/\/ When we promote a region in place, we can continue to use the established marking context to guide subsequent remembered\n-\/\/ set scans of this region's content.  The region will be coalesced and filled prior to the next old-gen marking effort.\n-\/\/ We identify the entirety of the region as DIRTY to force the next remembered set scan to identify the \"interesting poitners\"\n-\/\/ contained herein.\n-void ShenandoahHeapRegion::promote_in_place() {\n-  ShenandoahHeap* heap = ShenandoahHeap::heap();\n-  ShenandoahMarkingContext* marking_context = heap->marking_context();\n-  HeapWord* tams = marking_context->top_at_mark_start(this);\n-  assert(heap->active_generation()->is_mark_complete(), \"sanity\");\n-  assert(!heap->is_concurrent_old_mark_in_progress(), \"Cannot promote in place during old marking\");\n-  assert(is_young(), \"Only young regions can be promoted\");\n-  assert(is_regular(), \"Use different service to promote humongous regions\");\n-  assert(age() >= heap->age_census()->tenuring_threshold(), \"Only promote regions that are sufficiently aged\");\n-\n-  ShenandoahOldGeneration* old_gen = heap->old_generation();\n-  ShenandoahYoungGeneration* young_gen = heap->young_generation();\n-  size_t region_size_bytes = ShenandoahHeapRegion::region_size_bytes();\n-\n-  assert(get_top_before_promote() == tams, \"Cannot promote regions in place if top has advanced beyond TAMS\");\n-\n-  \/\/ Rebuild the remembered set information and mark the entire range as DIRTY.  We do NOT scan the content of this\n-  \/\/ range to determine which cards need to be DIRTY.  That would force us to scan the region twice, once now, and\n-  \/\/ once during the subsequent remembered set scan.  Instead, we blindly (conservatively) mark everything as DIRTY\n-  \/\/ now and then sort out the CLEAN pages during the next remembered set scan.\n-  \/\/\n-  \/\/ Rebuilding the remembered set consists of clearing all object registrations (reset_object_range()) here,\n-  \/\/ then registering every live object and every coalesced range of free objects in the loop that follows.\n-  heap->card_scan()->reset_object_range(bottom(), end());\n-  heap->card_scan()->mark_range_as_dirty(bottom(), get_top_before_promote() - bottom());\n-\n-  \/\/ TODO: use an existing coalesce-and-fill function rather than replicating the code here.\n-  HeapWord* obj_addr = bottom();\n-  while (obj_addr < tams) {\n-    oop obj = cast_to_oop(obj_addr);\n-    if (marking_context->is_marked(obj)) {\n-      assert(obj->klass() != nullptr, \"klass should not be NULL\");\n-      \/\/ This thread is responsible for registering all objects in this region.  No need for lock.\n-      heap->card_scan()->register_object_without_lock(obj_addr);\n-      obj_addr += obj->size();\n-    } else {\n-      HeapWord* next_marked_obj = marking_context->get_next_marked_addr(obj_addr, tams);\n-      assert(next_marked_obj <= tams, \"next marked object cannot exceed tams\");\n-      size_t fill_size = next_marked_obj - obj_addr;\n-      assert(fill_size >= ShenandoahHeap::min_fill_size(), \"previously allocated objects known to be larger than min_size\");\n-      ShenandoahHeap::fill_with_object(obj_addr, fill_size);\n-      heap->card_scan()->register_object_without_lock(obj_addr);\n-      obj_addr = next_marked_obj;\n-    }\n-  }\n-  \/\/ We do not need to scan above TAMS because restored top equals tams\n-  assert(obj_addr == tams, \"Expect loop to terminate when obj_addr equals tams\");\n-\n-  {\n-    ShenandoahHeapLocker locker(heap->lock());\n-\n-    HeapWord* update_watermark = get_update_watermark();\n-\n-    \/\/ Now that this region is affiliated with old, we can allow it to receive allocations, though it may not be in the\n-    \/\/ is_collector_free range.\n-    restore_top_before_promote();\n-\n-    size_t region_capacity = free();\n-    size_t region_used = used();\n-\n-    \/\/ The update_watermark was likely established while we had the artificially high value of top.  Make it sane now.\n-    assert(update_watermark >= top(), \"original top cannot exceed preserved update_watermark\");\n-    set_update_watermark(top());\n-\n-    \/\/ Unconditionally transfer one region from young to old to represent the newly promoted region.\n-    \/\/ This expands old and shrinks new by the size of one region.  Strictly, we do not \"need\" to expand old\n-    \/\/ if there are already enough unaffiliated regions in old to account for this newly promoted region.\n-    \/\/ However, if we do not transfer the capacities, we end up reducing the amount of memory that would have\n-    \/\/ otherwise been available to hold old evacuations, because old available is max_capacity - used and now\n-    \/\/ we would be trading a fully empty region for a partially used region.\n-\n-    young_gen->decrease_used(region_used);\n-    young_gen->decrement_affiliated_region_count();\n-\n-    \/\/ transfer_to_old() increases capacity of old and decreases capacity of young\n-    heap->generation_sizer()->force_transfer_to_old(1);\n-    set_affiliation(OLD_GENERATION);\n-\n-    old_gen->increment_affiliated_region_count();\n-    old_gen->increase_used(region_used);\n-\n-    \/\/ add_old_collector_free_region() increases promoted_reserve() if available space exceeds plab_min_size()\n-    heap->free_set()->add_old_collector_free_region(this);\n-  }\n-}\n-\n-void ShenandoahHeapRegion::promote_humongous() {\n-  ShenandoahHeap* heap = ShenandoahHeap::heap();\n-  ShenandoahMarkingContext* marking_context = heap->marking_context();\n-  assert(heap->active_generation()->is_mark_complete(), \"sanity\");\n-  assert(is_young(), \"Only young regions can be promoted\");\n-  assert(is_humongous_start(), \"Should not promote humongous continuation in isolation\");\n-  assert(age() >= heap->age_census()->tenuring_threshold(), \"Only promote regions that are sufficiently aged\");\n-\n-  ShenandoahGeneration* old_generation = heap->old_generation();\n-  ShenandoahGeneration* young_generation = heap->young_generation();\n-\n-  oop obj = cast_to_oop(bottom());\n-  assert(marking_context->is_marked(obj), \"promoted humongous object should be alive\");\n-\n-  \/\/ TODO: Consider not promoting humongous objects that represent primitive arrays.  Leaving a primitive array\n-  \/\/ (obj->is_typeArray()) in young-gen is harmless because these objects are never relocated and they are not\n-  \/\/ scanned.  Leaving primitive arrays in young-gen memory allows their memory to be reclaimed more quickly when\n-  \/\/ it becomes garbage.  Better to not make this change until sizes of young-gen and old-gen are completely\n-  \/\/ adaptive, as leaving primitive arrays in young-gen might be perceived as an \"astonishing result\" by someone\n-  \/\/ has carefully analyzed the required sizes of an application's young-gen and old-gen.\n-  size_t used_bytes = obj->size() * HeapWordSize;\n-  size_t spanned_regions = ShenandoahHeapRegion::required_regions(used_bytes);\n-  size_t humongous_waste = spanned_regions * ShenandoahHeapRegion::region_size_bytes() - obj->size() * HeapWordSize;\n-  size_t index_limit = index() + spanned_regions;\n-  {\n-    \/\/ We need to grab the heap lock in order to avoid a race when changing the affiliations of spanned_regions from\n-    \/\/ young to old.\n-    ShenandoahHeapLocker locker(heap->lock());\n-\n-    \/\/ We promote humongous objects unconditionally, without checking for availability.  We adjust\n-    \/\/ usage totals, including humongous waste, after evacuation is done.\n-    log_debug(gc)(\"promoting humongous region \" SIZE_FORMAT \", spanning \" SIZE_FORMAT, index(), spanned_regions);\n-\n-    young_generation->decrease_used(used_bytes);\n-    young_generation->decrease_humongous_waste(humongous_waste);\n-    young_generation->decrease_affiliated_region_count(spanned_regions);\n-\n-    \/\/ transfer_to_old() increases capacity of old and decreases capacity of young\n-    heap->generation_sizer()->force_transfer_to_old(spanned_regions);\n-\n-    \/\/ For this region and each humongous continuation region spanned by this humongous object, change\n-    \/\/ affiliation to OLD_GENERATION and adjust the generation-use tallies.  The remnant of memory\n-    \/\/ in the last humongous region that is not spanned by obj is currently not used.\n-    for (size_t i = index(); i < index_limit; i++) {\n-      ShenandoahHeapRegion* r = heap->get_region(i);\n-      log_debug(gc)(\"promoting humongous region \" SIZE_FORMAT \", from \" PTR_FORMAT \" to \" PTR_FORMAT,\n-                    r->index(), p2i(r->bottom()), p2i(r->top()));\n-      \/\/ We mark the entire humongous object's range as dirty after loop terminates, so no need to dirty the range here\n-      r->set_affiliation(OLD_GENERATION);\n-    }\n-\n-    old_generation->increase_affiliated_region_count(spanned_regions);\n-    old_generation->increase_used(used_bytes);\n-    old_generation->increase_humongous_waste(humongous_waste);\n-  }\n-\n-  \/\/ Since this region may have served previously as OLD, it may hold obsolete object range info.\n-  heap->card_scan()->reset_object_range(bottom(), bottom() + spanned_regions * ShenandoahHeapRegion::region_size_words());\n-  \/\/ Since the humongous region holds only one object, no lock is necessary for this register_object() invocation.\n-  heap->card_scan()->register_object_without_lock(bottom());\n-\n-  if (obj->is_typeArray()) {\n-    \/\/ Primitive arrays don't need to be scanned.\n-    log_debug(gc)(\"Clean cards for promoted humongous object (Region \" SIZE_FORMAT \") from \" PTR_FORMAT \" to \" PTR_FORMAT,\n-                  index(), p2i(bottom()), p2i(bottom() + obj->size()));\n-    heap->card_scan()->mark_range_as_clean(bottom(), obj->size());\n-  } else {\n-    log_debug(gc)(\"Dirty cards for promoted humongous object (Region \" SIZE_FORMAT \") from \" PTR_FORMAT \" to \" PTR_FORMAT,\n-                  index(), p2i(bottom()), p2i(bottom() + obj->size()));\n-    heap->card_scan()->mark_range_as_dirty(bottom(), obj->size());\n-  }\n-}\n-\n","filename":"src\/hotspot\/share\/gc\/shenandoah\/shenandoahHeapRegion.cpp","additions":0,"deletions":231,"binary":false,"changes":231,"status":"modified"},{"patch":"@@ -406,7 +406,0 @@\n-  \/\/ During global collections, this service iterates through an old-gen heap region that is not part of collection\n-  \/\/ set to fill and register ranges of dead memory.  Note that live objects were previously registered.  Some dead objects\n-  \/\/ that are subsumed into coalesced ranges of dead memory need to be \"unregistered\".\n-  void global_oop_iterate_and_fill_dead(OopIterateClosure* cl);\n-  void oop_iterate_humongous(OopIterateClosure* cl);\n-  void oop_iterate_humongous(OopIterateClosure* cl, HeapWord* start, size_t words);\n-\n@@ -479,4 +472,0 @@\n-  \/\/ Register all objects.  Set all remembered set cards to dirty.\n-  void promote_humongous();\n-  void promote_in_place();\n-\n@@ -488,4 +477,0 @@\n-  \/\/ This is an old-region that was not part of the collection set during a GLOBAL collection.  We coalesce the dead\n-  \/\/ objects, but do not need to register the live objects as they are already registered.\n-  void global_oop_iterate_objects_and_fill_dead(OopIterateClosure* cl);\n-\n","filename":"src\/hotspot\/share\/gc\/shenandoah\/shenandoahHeapRegion.hpp","additions":0,"deletions":15,"binary":false,"changes":15,"status":"modified"},{"patch":"@@ -30,1 +30,0 @@\n-\n","filename":"src\/hotspot\/share\/gc\/shenandoah\/shenandoahHeapRegion.inline.hpp","additions":0,"deletions":1,"binary":false,"changes":1,"status":"modified"},{"patch":"@@ -309,3 +309,3 @@\n-      \/\/ future young-gen collections.  It might be better to reconstruct card table in\n-      \/\/ ShenandoahHeapRegion::global_oop_iterate_and_fill_dead.  We could either mark all live memory as dirty, or could\n-      \/\/ use the GLOBAL update-refs scanning of pointers to determine precisely which cards to flag as dirty.\n+      \/\/ future young-gen collections.  It might be better to reconstruct card table in a different phase.  We could\n+      \/\/ either mark all live memory as dirty, or could use the GLOBAL update-refs scanning of pointers to determine\n+      \/\/ precisely which cards to flag as dirty.\n","filename":"src\/hotspot\/share\/gc\/shenandoah\/shenandoahMark.inline.hpp","additions":3,"deletions":3,"binary":false,"changes":6,"status":"modified"},{"patch":"@@ -36,0 +36,1 @@\n+class ShenandoahHeapRegion;\n","filename":"src\/hotspot\/share\/gc\/shenandoah\/shenandoahMarkingContext.hpp","additions":1,"deletions":0,"binary":false,"changes":1,"status":"modified"},{"patch":"@@ -41,2 +41,2 @@\n-ShenandoahOldGC::ShenandoahOldGC(ShenandoahGeneration* generation, ShenandoahSharedFlag& allow_preemption) :\n-    ShenandoahConcurrentGC(generation, false), _allow_preemption(allow_preemption) {\n+ShenandoahOldGC::ShenandoahOldGC(ShenandoahOldGeneration* generation, ShenandoahSharedFlag& allow_preemption) :\n+    ShenandoahConcurrentGC(generation, false), _old_generation(generation), _allow_preemption(allow_preemption) {\n@@ -88,2 +88,2 @@\n-  assert(!heap->doing_mixed_evacuations(), \"Should not start an old gc with pending mixed evacuations\");\n-  assert(!heap->is_prepare_for_old_mark_in_progress(), \"Old regions need to be parsable during concurrent mark.\");\n+  assert(!_old_generation->is_doing_mixed_evacuations(), \"Should not start an old gc with pending mixed evacuations\");\n+  assert(!_old_generation->is_preparing_for_mark(), \"Old regions need to be parsable during concurrent mark.\");\n@@ -152,1 +152,1 @@\n-  size_t allocation_runway = heap->young_heuristics()->bytes_of_allocation_runway_before_gc_trigger(0);\n+  size_t allocation_runway = heap->young_generation()->heuristics()->bytes_of_allocation_runway_before_gc_trigger(0);\n","filename":"src\/hotspot\/share\/gc\/shenandoah\/shenandoahOldGC.cpp","additions":5,"deletions":5,"binary":false,"changes":10,"status":"modified"},{"patch":"@@ -32,1 +32,1 @@\n-class ShenandoahGeneration;\n+class ShenandoahOldGeneration;\n@@ -36,1 +36,1 @@\n-  ShenandoahOldGC(ShenandoahGeneration* generation, ShenandoahSharedFlag& allow_preemption);\n+  ShenandoahOldGC(ShenandoahOldGeneration* generation, ShenandoahSharedFlag& allow_preemption);\n@@ -43,1 +43,1 @@\n-\n+  ShenandoahOldGeneration* _old_generation;\n","filename":"src\/hotspot\/share\/gc\/shenandoah\/shenandoahOldGC.hpp","additions":3,"deletions":3,"binary":false,"changes":6,"status":"modified"},{"patch":"@@ -43,0 +43,1 @@\n+#include \"gc\/shenandoah\/shenandoahScanRemembered.inline.hpp\"\n@@ -304,1 +305,0 @@\n-  ShenandoahOldHeuristics* old_heuristics = heap->old_heuristics();\n@@ -313,1 +313,1 @@\n-  uint coalesce_and_fill_regions_count = old_heuristics->get_coalesce_and_fill_candidates(_coalesce_and_fill_region_array);\n+  uint coalesce_and_fill_regions_count = heuristics()->get_coalesce_and_fill_candidates(_coalesce_and_fill_region_array);\n@@ -319,1 +319,1 @@\n-    old_heuristics->abandon_collection_candidates();\n+    abandon_collection_candidates();\n@@ -434,1 +434,1 @@\n-\/\/               |   |            v                     |     may move the old generation\n+\/\/               |   |            v                     |     move the old generation\n@@ -468,1 +468,1 @@\n-      assert(heap->is_old_bitmap_stable(), \"Cannot begin filling without first completing marking, state is '%s'\", state_name(_state));\n+      assert(is_mark_complete(), \"Cannot begin filling without first completing marking, state is '%s'\", state_name(_state));\n@@ -473,3 +473,1 @@\n-      assert(!heap->is_concurrent_old_mark_in_progress(), \"Cannot become ready for bootstrap during old mark.\");\n-      assert(_old_heuristics->unprocessed_old_collection_candidates() == 0, \"Cannot become ready for bootstrap with collection candidates\");\n-      assert(heap->young_generation()->old_gen_task_queues() == nullptr, \"Cannot become ready for bootstrap when still setup for bootstrapping.\");\n+      validate_waiting_for_bootstrap();\n@@ -495,0 +493,11 @@\n+\n+bool ShenandoahOldGeneration::validate_waiting_for_bootstrap() {\n+  ShenandoahHeap* heap = ShenandoahHeap::heap();\n+  assert(!heap->is_concurrent_old_mark_in_progress(), \"Cannot become ready for bootstrap during old mark.\");\n+  assert(heap->young_generation()->old_gen_task_queues() == nullptr, \"Cannot become ready for bootstrap when still setup for bootstrapping.\");\n+  assert(!is_concurrent_mark_in_progress(), \"Cannot be marking in IDLE\");\n+  assert(!heap->young_generation()->is_bootstrap_cycle(), \"Cannot have old mark queues if IDLE\");\n+  assert(!heuristics()->has_coalesce_and_fill_candidates(), \"Cannot have coalesce and fill candidates in IDLE\");\n+  assert(heuristics()->unprocessed_old_collection_candidates() == 0, \"Cannot have mixed collection candidates in IDLE\");\n+  return true;\n+}\n@@ -573,0 +582,68 @@\n+bool ShenandoahOldGeneration::has_unprocessed_collection_candidates() {\n+  return _old_heuristics->unprocessed_old_collection_candidates() > 0;\n+}\n+\n+size_t ShenandoahOldGeneration::unprocessed_collection_candidates_live_memory() {\n+  return _old_heuristics->unprocessed_old_collection_candidates_live_memory();\n+}\n+\n+void ShenandoahOldGeneration::abandon_collection_candidates() {\n+  _old_heuristics->abandon_collection_candidates();\n+}\n+\n+void ShenandoahOldGeneration::prepare_for_mixed_collections_after_global_gc() {\n+  assert(is_mark_complete(), \"Expected old generation mark to be complete after global cycle.\");\n+  _old_heuristics->prepare_for_old_collections();\n+  log_info(gc)(\"After choosing global collection set, mixed candidates: \" UINT32_FORMAT \", coalescing candidates: \" SIZE_FORMAT,\n+               _old_heuristics->unprocessed_old_collection_candidates(),\n+               _old_heuristics->coalesce_and_fill_candidates_count());\n+}\n+\n+void ShenandoahOldGeneration::maybe_trigger_collection(size_t first_old_region, size_t last_old_region, size_t old_region_count) {\n+  ShenandoahHeap* heap = ShenandoahHeap::heap();\n+  const size_t old_region_span = (first_old_region <= last_old_region)? (last_old_region + 1 - first_old_region): 0;\n+  const size_t allowed_old_gen_span = heap->num_regions() - (ShenandoahGenerationalHumongousReserve * heap->num_regions() \/ 100);\n+\n+  \/\/ Tolerate lower density if total span is small.  Here's the implementation:\n+  \/\/   if old_gen spans more than 100% and density < 75%, trigger old-defrag\n+  \/\/   else if old_gen spans more than 87.5% and density < 62.5%, trigger old-defrag\n+  \/\/   else if old_gen spans more than 75% and density < 50%, trigger old-defrag\n+  \/\/   else if old_gen spans more than 62.5% and density < 37.5%, trigger old-defrag\n+  \/\/   else if old_gen spans more than 50% and density < 25%, trigger old-defrag\n+  \/\/\n+  \/\/ A previous implementation was more aggressive in triggering, resulting in degraded throughput when\n+  \/\/ humongous allocation was not required.\n+\n+  const size_t old_available = available();\n+  const size_t region_size_bytes = ShenandoahHeapRegion::region_size_bytes();\n+  const size_t old_unaffiliated_available = free_unaffiliated_regions() * region_size_bytes;\n+  assert(old_available >= old_unaffiliated_available, \"sanity\");\n+  const size_t old_fragmented_available = old_available - old_unaffiliated_available;\n+\n+  const size_t old_bytes_consumed = old_region_count * region_size_bytes - old_fragmented_available;\n+  const size_t old_bytes_spanned = old_region_span * region_size_bytes;\n+  const double old_density = ((double) old_bytes_consumed) \/ old_bytes_spanned;\n+\n+  uint eighths = 8;\n+  for (uint i = 0; i < 5; i++) {\n+    size_t span_threshold = eighths * allowed_old_gen_span \/ 8;\n+    double density_threshold = (eighths - 2) \/ 8.0;\n+    if ((old_region_span >= span_threshold) && (old_density < density_threshold)) {\n+      heuristics()->trigger_old_is_fragmented(old_density, first_old_region, last_old_region);\n+      break;\n+    }\n+    eighths--;\n+  }\n+\n+  const size_t old_used = used() + get_humongous_waste();\n+  const size_t trigger_threshold = usage_trigger_threshold();\n+  \/\/ Detects unsigned arithmetic underflow\n+  assert(old_used <= heap->free_set()->capacity(),\n+         \"Old used (\" SIZE_FORMAT \", \" SIZE_FORMAT\") must not be more than heap capacity (\" SIZE_FORMAT \")\",\n+         used(), get_humongous_waste(), heap->free_set()->capacity());\n+\n+  if (old_used > trigger_threshold) {\n+    heuristics()->trigger_old_has_grown();\n+  }\n+}\n+\n","filename":"src\/hotspot\/share\/gc\/shenandoah\/shenandoahOldGeneration.cpp","additions":85,"deletions":8,"binary":false,"changes":93,"status":"modified"},{"patch":"@@ -28,0 +28,1 @@\n+#include \"gc\/shenandoah\/heuristics\/shenandoahOldHeuristics.hpp\"\n@@ -33,1 +34,0 @@\n-class ShenandoahOldHeuristics;\n@@ -90,0 +90,4 @@\n+  ShenandoahOldHeuristics* heuristics() const override {\n+    return _old_heuristics;\n+  }\n+\n@@ -155,0 +159,1 @@\n+  void prepare_for_mixed_collections_after_global_gc();\n@@ -180,0 +185,18 @@\n+  \/\/ True if there are old regions waiting to be selected for a mixed collection\n+  bool has_unprocessed_collection_candidates();\n+\n+  bool is_doing_mixed_evacuations() const {\n+    return state() == EVACUATING;\n+  }\n+\n+  bool is_preparing_for_mark() const {\n+    return state() == FILLING;\n+  }\n+\n+  \/\/ Amount of live memory (bytes) in regions waiting for mixed collections\n+  size_t unprocessed_collection_candidates_live_memory();\n+\n+  \/\/ Abandon any regions waiting for mixed collections\n+  void abandon_collection_candidates();\n+\n+  void maybe_trigger_collection(size_t first_old_region, size_t last_old_region, size_t old_region_count);\n@@ -185,0 +208,4 @@\n+#ifdef ASSERT\n+  bool validate_waiting_for_bootstrap();\n+#endif\n+\n","filename":"src\/hotspot\/share\/gc\/shenandoah\/shenandoahOldGeneration.hpp","additions":28,"deletions":1,"binary":false,"changes":29,"status":"modified"},{"patch":"@@ -33,0 +33,1 @@\n+#include \"gc\/shenandoah\/shenandoahScanRemembered.inline.hpp\"\n","filename":"src\/hotspot\/share\/gc\/shenandoah\/shenandoahReferenceProcessor.cpp","additions":1,"deletions":0,"binary":false,"changes":1,"status":"modified"},{"patch":"@@ -34,0 +34,1 @@\n+#include \"gc\/shenandoah\/shenandoahGeneration.hpp\"\n@@ -73,1 +74,1 @@\n-  if (heap->mode()->is_generational() && heap->is_gc_generation_young()) {\n+  if (heap->mode()->is_generational() && heap->active_generation()->is_young()) {\n@@ -96,1 +97,1 @@\n-  if (heap->mode()->is_generational() && heap->is_gc_generation_young()) {\n+  if (heap->mode()->is_generational() && heap->active_generation()->is_young()) {\n","filename":"src\/hotspot\/share\/gc\/shenandoah\/shenandoahRootVerifier.cpp","additions":3,"deletions":2,"binary":false,"changes":5,"status":"modified"},{"patch":"@@ -28,0 +28,1 @@\n+#include \"gc\/shenandoah\/shenandoahOldGeneration.hpp\"\n@@ -140,1 +141,2 @@\n-  log_info(gc, remset)(\"Scan remembered set using bitmap: %s\", BOOL_TO_STR(ShenandoahHeap::heap()->is_old_bitmap_stable()));\n+  bool old_bitmap_stable = ShenandoahHeap::heap()->old_generation()->is_mark_complete();\n+  log_info(gc, remset)(\"Scan remembered set using bitmap: %s\", BOOL_TO_STR(old_bitmap_stable));\n@@ -448,1 +450,0 @@\n-          size_t size = obj->size();\n@@ -457,1 +458,1 @@\n-}\n\\ No newline at end of file\n+}\n","filename":"src\/hotspot\/share\/gc\/shenandoah\/shenandoahScanRemembered.cpp","additions":4,"deletions":3,"binary":false,"changes":7,"status":"modified"},{"patch":"@@ -36,0 +36,1 @@\n+#include \"gc\/shenandoah\/shenandoahOldGeneration.hpp\"\n@@ -38,0 +39,1 @@\n+#include \"logging\/log.hpp\"\n@@ -586,1 +588,1 @@\n-  const ShenandoahMarkingContext* ctx = heap->is_old_bitmap_stable() ?\n+  const ShenandoahMarkingContext* ctx = heap->old_generation()->is_mark_complete() ?\n@@ -897,1 +899,2 @@\n-  log_info(gc, remset)(\"Scan remembered set using bitmap: %s\", BOOL_TO_STR(heap->is_old_bitmap_stable()));\n+  bool old_bitmap_stable = heap->old_generation()->is_mark_complete();\n+  log_info(gc, remset)(\"Scan remembered set using bitmap: %s\", BOOL_TO_STR(old_bitmap_stable));\n","filename":"src\/hotspot\/share\/gc\/shenandoah\/shenandoahScanRemembered.inline.hpp","additions":5,"deletions":2,"binary":false,"changes":7,"status":"modified"},{"patch":"@@ -29,0 +29,2 @@\n+#include \"gc\/shenandoah\/shenandoahEvacTracker.hpp\"\n+#include \"gc\/shenandoah\/shenandoahGenerationalHeap.hpp\"\n@@ -57,1 +59,1 @@\n-    ShenandoahHeap::heap()->retire_plab(_plab);\n+    ShenandoahGenerationalHeap::heap()->retire_plab(_plab);\n","filename":"src\/hotspot\/share\/gc\/shenandoah\/shenandoahThreadLocalData.cpp","additions":3,"deletions":1,"binary":false,"changes":4,"status":"modified"},{"patch":"@@ -121,2 +121,4 @@\n-    data(thread)->_plab = new PLAB(align_up(PLAB::min_size(), CardTable::card_size_in_words()));\n-    data(thread)->_plab_size = 0;\n+    if (ShenandoahHeap::heap()->mode()->is_generational()) {\n+      data(thread)->_plab = new PLAB(align_up(PLAB::min_size(), CardTable::card_size_in_words()));\n+      data(thread)->_plab_size = 0;\n+    }\n","filename":"src\/hotspot\/share\/gc\/shenandoah\/shenandoahThreadLocalData.hpp","additions":4,"deletions":2,"binary":false,"changes":6,"status":"modified"},{"patch":"@@ -36,0 +36,1 @@\n+#include \"gc\/shenandoah\/shenandoahScanRemembered.inline.hpp\"\n@@ -191,1 +192,1 @@\n-                (obj_reg->is_old() && ShenandoahHeap::heap()->is_gc_generation_young()),\n+                (obj_reg->is_old() && _heap->active_generation()->is_young()),\n@@ -1359,1 +1360,2 @@\n-  log_debug(gc)(\"Verifying remembered set at %s mark\", _heap->doing_mixed_evacuations()? \"mixed\": \"young\");\n+  ShenandoahOldGeneration* old_generation = _heap->old_generation();\n+  log_debug(gc)(\"Verifying remembered set at %s mark\", old_generation->is_doing_mixed_evacuations() ? \"mixed\" : \"young\");\n@@ -1361,1 +1363,1 @@\n-  if (_heap->is_old_bitmap_stable() || _heap->active_generation()->is_global()) {\n+  if (old_generation->is_mark_complete() || _heap->active_generation()->is_global()) {\n@@ -1453,1 +1455,1 @@\n-  if (_heap->is_old_bitmap_stable() || _heap->active_generation()->is_global()) {\n+  if (_heap->old_generation()->is_mark_complete() || _heap->active_generation()->is_global()) {\n","filename":"src\/hotspot\/share\/gc\/shenandoah\/shenandoahVerifier.cpp","additions":6,"deletions":4,"binary":false,"changes":10,"status":"modified"},{"patch":"@@ -28,1 +28,0 @@\n-#include \"gc\/shenandoah\/shenandoahHeap.inline.hpp\"\n@@ -30,1 +29,0 @@\n-#include \"gc\/shenandoah\/shenandoahOldGeneration.hpp\"\n@@ -32,1 +30,0 @@\n-#include \"gc\/shenandoah\/shenandoahVerifier.hpp\"\n@@ -69,0 +66,5 @@\n+void ShenandoahYoungGeneration::parallel_region_iterate_free(ShenandoahHeapRegionClosure* cl) {\n+  ShenandoahExcludeRegionClosure<OLD_GENERATION> exclude_cl(cl);\n+  ShenandoahHeap::heap()->parallel_heap_region_iterate(&exclude_cl);\n+}\n+\n@@ -85,1 +87,2 @@\n-  _heuristics = new ShenandoahYoungHeuristics(this);\n+  _young_heuristics = new ShenandoahYoungHeuristics(this);\n+  _heuristics = _young_heuristics;\n@@ -103,4 +106,0 @@\n-void ShenandoahYoungGeneration::parallel_region_iterate_free(ShenandoahHeapRegionClosure* cl) {\n-  ShenandoahExcludeRegionClosure<OLD_GENERATION> exclude_cl(cl);\n-  ShenandoahHeap::heap()->parallel_heap_region_iterate(&exclude_cl);\n-}\n","filename":"src\/hotspot\/share\/gc\/shenandoah\/shenandoahYoungGeneration.cpp","additions":7,"deletions":8,"binary":false,"changes":15,"status":"modified"},{"patch":"@@ -29,0 +29,1 @@\n+#include \"gc\/shenandoah\/heuristics\/shenandoahYoungHeuristics.hpp\"\n@@ -33,0 +34,1 @@\n+  ShenandoahYoungHeuristics* _young_heuristics;\n@@ -43,0 +45,4 @@\n+  ShenandoahYoungHeuristics* heuristics() const override {\n+    return _young_heuristics;\n+  }\n+\n","filename":"src\/hotspot\/share\/gc\/shenandoah\/shenandoahYoungGeneration.hpp","additions":6,"deletions":0,"binary":false,"changes":6,"status":"modified"},{"patch":"@@ -83,1 +83,1 @@\n-    _heuristics = _heap->old_heuristics();\n+    _heuristics = _heap->old_generation()->heuristics();\n","filename":"test\/hotspot\/gtest\/gc\/shenandoah\/test_shenandoahOldHeuristic.cpp","additions":1,"deletions":1,"binary":false,"changes":2,"status":"modified"}]}