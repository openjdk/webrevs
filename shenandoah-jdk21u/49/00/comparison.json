{"files":[{"patch":"@@ -1203,1 +1203,0 @@\n-  assert(!heap->_update_refs_iterator.has_next(), \"Should have finished update references\");\n","filename":"src\/hotspot\/share\/gc\/shenandoah\/shenandoahConcurrentGC.cpp","additions":0,"deletions":1,"binary":false,"changes":1,"status":"modified"},{"patch":"@@ -37,0 +37,1 @@\n+#include \"gc\/shenandoah\/shenandoahOopClosures.inline.hpp\"\n@@ -713,0 +714,233 @@\n+\n+template<bool CONCURRENT>\n+class ShenandoahGenerationalUpdateHeapRefsTask : public WorkerTask {\n+private:\n+  ShenandoahHeap* _heap;\n+  ShenandoahRegionIterator* _regions;\n+  ShenandoahRegionChunkIterator* _work_chunks;\n+\n+public:\n+  explicit ShenandoahGenerationalUpdateHeapRefsTask(ShenandoahRegionIterator* regions,\n+                                                    ShenandoahRegionChunkIterator* work_chunks) :\n+          WorkerTask(\"Shenandoah Update References\"),\n+          _heap(ShenandoahHeap::heap()),\n+          _regions(regions),\n+          _work_chunks(work_chunks)\n+  {\n+    bool old_bitmap_stable = _heap->old_generation()->is_mark_complete();\n+    log_debug(gc, remset)(\"Update refs, scan remembered set using bitmap: %s\", BOOL_TO_STR(old_bitmap_stable));\n+  }\n+\n+  void work(uint worker_id) {\n+    if (CONCURRENT) {\n+      ShenandoahConcurrentWorkerSession worker_session(worker_id);\n+      ShenandoahSuspendibleThreadSetJoiner stsj;\n+      do_work<ShenandoahConcUpdateRefsClosure>(worker_id);\n+    } else {\n+      ShenandoahParallelWorkerSession worker_session(worker_id);\n+      do_work<ShenandoahSTWUpdateRefsClosure>(worker_id);\n+    }\n+  }\n+\n+private:\n+  template<class T>\n+  void do_work(uint worker_id) {\n+    T cl;\n+    if (CONCURRENT && (worker_id == 0)) {\n+      \/\/ We ask the first worker to replenish the Mutator free set by moving regions previously reserved to hold the\n+      \/\/ results of evacuation.  These reserves are no longer necessary because evacuation has completed.\n+      size_t cset_regions = _heap->collection_set()->count();\n+      \/\/ We cannot transfer any more regions than will be reclaimed when the existing collection set is recycled, because\n+      \/\/ we need the reclaimed collection set regions to replenish the collector reserves\n+      _heap->free_set()->move_collector_sets_to_mutator(cset_regions);\n+    }\n+    \/\/ If !CONCURRENT, there's no value in expanding Mutator free set\n+\n+    ShenandoahHeapRegion* r = _regions->next();\n+    \/\/ We update references for global, old, and young collections.\n+    assert(_heap->active_generation()->is_mark_complete(), \"Expected complete marking\");\n+    ShenandoahMarkingContext* const ctx = _heap->marking_context();\n+    bool is_mixed = _heap->collection_set()->has_old_regions();\n+    while (r != nullptr) {\n+      HeapWord* update_watermark = r->get_update_watermark();\n+      assert (update_watermark >= r->bottom(), \"sanity\");\n+\n+      log_debug(gc)(\"Update refs worker \" UINT32_FORMAT \", looking at region \" SIZE_FORMAT, worker_id, r->index());\n+      bool region_progress = false;\n+      if (r->is_active() && !r->is_cset()) {\n+        if (r->is_young()) {\n+          _heap->marked_object_oop_iterate(r, &cl, update_watermark);\n+          region_progress = true;\n+        } else if (r->is_old()) {\n+          if (_heap->active_generation()->is_global()) {\n+            \/\/ Note that GLOBAL collection is not as effectively balanced as young and mixed cycles.  This is because\n+            \/\/ concurrent GC threads are parceled out entire heap regions of work at a time and there\n+            \/\/ is no \"catchup phase\" consisting of remembered set scanning, during which parcels of work are smaller\n+            \/\/ and more easily distributed more fairly across threads.\n+\n+            \/\/ TODO: Consider an improvement to load balance GLOBAL GC.\n+            _heap->marked_object_oop_iterate(r, &cl, update_watermark);\n+            region_progress = true;\n+          }\n+          \/\/ Otherwise, this is an old region in a young or mixed cycle.  Process it during a second phase, below.\n+          \/\/ Don't bother to report pacing progress in this case.\n+        } else {\n+          \/\/ Because updating of references runs concurrently, it is possible that a FREE inactive region transitions\n+          \/\/ to a non-free active region while this loop is executing.  Whenever this happens, the changing of a region's\n+          \/\/ active status may propagate at a different speed than the changing of the region's affiliation.\n+\n+          \/\/ When we reach this control point, it is because a race has allowed a region's is_active() status to be seen\n+          \/\/ by this thread before the region's affiliation() is seen by this thread.\n+\n+          \/\/ It's ok for this race to occur because the newly transformed region does not have any references to be\n+          \/\/ updated.\n+\n+          assert(r->get_update_watermark() == r->bottom(),\n+                 \"%s Region \" SIZE_FORMAT \" is_active but not recognized as YOUNG or OLD so must be newly transitioned from FREE\",\n+                 r->affiliation_name(), r->index());\n+        }\n+      }\n+      if (region_progress && ShenandoahPacing) {\n+        _heap->pacer()->report_updaterefs(pointer_delta(update_watermark, r->bottom()));\n+      }\n+      if (_heap->check_cancelled_gc_and_yield(CONCURRENT)) {\n+        return;\n+      }\n+      r = _regions->next();\n+    }\n+\n+    if (!_heap->active_generation()->is_global()) {\n+      \/\/ Since this is generational and not GLOBAL, we have to process the remembered set.  There's no remembered\n+      \/\/ set processing if not in generational mode or if GLOBAL mode.\n+\n+      \/\/ After this thread has exhausted its traditional update-refs work, it continues with updating refs within remembered set.\n+      \/\/ The remembered set workload is better balanced between threads, so threads that are \"behind\" can catch up with other\n+      \/\/ threads during this phase, allowing all threads to work more effectively in parallel.\n+      struct ShenandoahRegionChunk assignment;\n+      RememberedScanner* scanner = _heap->card_scan();\n+\n+      while (!_heap->check_cancelled_gc_and_yield(CONCURRENT) && _work_chunks->next(&assignment)) {\n+        \/\/ Keep grabbing next work chunk to process until finished, or asked to yield\n+        ShenandoahHeapRegion* r = assignment._r;\n+        if (r->is_active() && !r->is_cset() && r->is_old()) {\n+          HeapWord* start_of_range = r->bottom() + assignment._chunk_offset;\n+          HeapWord* end_of_range = r->get_update_watermark();\n+          if (end_of_range > start_of_range + assignment._chunk_size) {\n+            end_of_range = start_of_range + assignment._chunk_size;\n+          }\n+\n+          \/\/ Old region in a young cycle or mixed cycle.\n+          if (is_mixed) {\n+            \/\/ TODO: For mixed evac, consider building an old-gen remembered set that allows restricted updating\n+            \/\/ within old-gen HeapRegions.  This remembered set can be constructed by old-gen concurrent marking\n+            \/\/ and augmented by card marking.  For example, old-gen concurrent marking can remember for each old-gen\n+            \/\/ card which other old-gen regions it refers to: none, one-other specifically, multiple-other non-specific.\n+            \/\/ Update-references when _mixed_evac processess each old-gen memory range that has a traditional DIRTY\n+            \/\/ card or if the \"old-gen remembered set\" indicates that this card holds pointers specifically to an\n+            \/\/ old-gen region in the most recent collection set, or if this card holds pointers to other non-specific\n+            \/\/ old-gen heap regions.\n+\n+            if (r->is_humongous()) {\n+              if (start_of_range < end_of_range) {\n+                \/\/ Need to examine both dirty and clean cards during mixed evac.\n+                r->oop_iterate_humongous_slice(&cl, false, start_of_range, assignment._chunk_size, true);\n+              }\n+            } else {\n+              \/\/ Since this is mixed evacuation, old regions that are candidates for collection have not been coalesced\n+              \/\/ and filled.  Use mark bits to find objects that need to be updated.\n+              \/\/\n+              \/\/ Future TODO: establish a second remembered set to identify which old-gen regions point to other old-gen\n+              \/\/ regions which are in the collection set for a particular mixed evacuation.\n+              if (start_of_range < end_of_range) {\n+                HeapWord* p = nullptr;\n+                size_t card_index = scanner->card_index_for_addr(start_of_range);\n+                \/\/ In case last object in my range spans boundary of my chunk, I may need to scan all the way to top()\n+                ShenandoahObjectToOopBoundedClosure<T> objs(&cl, start_of_range, r->top());\n+\n+                \/\/ Any object that begins in a previous range is part of a different scanning assignment.  Any object that\n+                \/\/ starts after end_of_range is also not my responsibility.  (Either allocated during evacuation, so does\n+                \/\/ not hold pointers to from-space, or is beyond the range of my assigned work chunk.)\n+\n+                \/\/ Find the first object that begins in my range, if there is one.\n+                p = start_of_range;\n+                oop obj = cast_to_oop(p);\n+                HeapWord* tams = ctx->top_at_mark_start(r);\n+                if (p >= tams) {\n+                  \/\/ We cannot use ctx->is_marked(obj) to test whether an object begins at this address.  Instead,\n+                  \/\/ we need to use the remembered set crossing map to advance p to the first object that starts\n+                  \/\/ within the enclosing card.\n+\n+                  while (true) {\n+                    HeapWord* first_object = scanner->first_object_in_card(card_index);\n+                    if (first_object != nullptr) {\n+                      p = first_object;\n+                      break;\n+                    } else if (scanner->addr_for_card_index(card_index + 1) < end_of_range) {\n+                      card_index++;\n+                    } else {\n+                      \/\/ Force the loop that follows to immediately terminate.\n+                      p = end_of_range;\n+                      break;\n+                    }\n+                  }\n+                  obj = cast_to_oop(p);\n+                  \/\/ Note: p may be >= end_of_range\n+                } else if (!ctx->is_marked(obj)) {\n+                  p = ctx->get_next_marked_addr(p, tams);\n+                  obj = cast_to_oop(p);\n+                  \/\/ If there are no more marked objects before tams, this returns tams.\n+                  \/\/ Note that tams is either >= end_of_range, or tams is the start of an object that is marked.\n+                }\n+                while (p < end_of_range) {\n+                  \/\/ p is known to point to the beginning of marked object obj\n+                  objs.do_object(obj);\n+                  HeapWord* prev_p = p;\n+                  p += obj->size();\n+                  if (p < tams) {\n+                    p = ctx->get_next_marked_addr(p, tams);\n+                    \/\/ If there are no more marked objects before tams, this returns tams.  Note that tams is\n+                    \/\/ either >= end_of_range, or tams is the start of an object that is marked.\n+                  }\n+                  assert(p != prev_p, \"Lack of forward progress\");\n+                  obj = cast_to_oop(p);\n+                }\n+              }\n+            }\n+          } else {\n+            \/\/ This is a young evac..\n+            if (start_of_range < end_of_range) {\n+              size_t cluster_size =\n+                      CardTable::card_size_in_words() * ShenandoahCardCluster<ShenandoahDirectCardMarkRememberedSet>::CardsPerCluster;\n+              size_t clusters = assignment._chunk_size \/ cluster_size;\n+              assert(clusters * cluster_size == assignment._chunk_size, \"Chunk assignment must align on cluster boundaries\");\n+              scanner->process_region_slice(r, assignment._chunk_offset, clusters, end_of_range, &cl, true, worker_id);\n+            }\n+          }\n+          if (ShenandoahPacing && (start_of_range < end_of_range)) {\n+            _heap->pacer()->report_updaterefs(pointer_delta(end_of_range, start_of_range));\n+          }\n+        }\n+      }\n+    }\n+  }\n+};\n+\n+void ShenandoahGenerationalHeap::update_heap_references(bool concurrent) {\n+  assert(!is_full_gc_in_progress(), \"Only for concurrent and degenerated GC\");\n+  uint nworkers = workers()->active_workers();\n+  ShenandoahRegionChunkIterator work_list(nworkers);\n+  ShenandoahRegionIterator update_refs_iterator(this);\n+  if (concurrent) {\n+    ShenandoahGenerationalUpdateHeapRefsTask<true> task(&update_refs_iterator, &work_list);\n+    workers()->run_task(&task);\n+  } else {\n+    ShenandoahGenerationalUpdateHeapRefsTask<false> task(&update_refs_iterator, &work_list);\n+    workers()->run_task(&task);\n+  }\n+  assert(cancelled_gc() || !update_refs_iterator.has_next(), \"Should have finished update references\");\n+\n+  if (ShenandoahEnableCardStats) { \/\/ generational check proxy\n+    assert(card_scan() != nullptr, \"Card table must exist when card stats are enabled\");\n+    card_scan()->log_card_stats(nworkers, CARD_STAT_UPDATE_REFS);\n+  }\n+}\n","filename":"src\/hotspot\/share\/gc\/shenandoah\/shenandoahGenerationalHeap.cpp","additions":234,"deletions":0,"binary":false,"changes":234,"status":"modified"},{"patch":"@@ -53,0 +53,3 @@\n+  \/\/ ---------- Update References\n+  \/\/\n+  void update_heap_references(bool concurrent) override;\n","filename":"src\/hotspot\/share\/gc\/shenandoah\/shenandoahGenerationalHeap.hpp","additions":3,"deletions":0,"binary":false,"changes":3,"status":"modified"},{"patch":"@@ -566,1 +566,0 @@\n-  _update_refs_iterator(this),\n@@ -1979,2 +1978,0 @@\n-\n-  _update_refs_iterator.reset();\n@@ -2352,2 +2349,0 @@\n-  ShenandoahRegionChunkIterator* _work_chunks;\n-\n@@ -2355,2 +2350,1 @@\n-  explicit ShenandoahUpdateHeapRefsTask(ShenandoahRegionIterator* regions,\n-                                        ShenandoahRegionChunkIterator* work_chunks) :\n+  explicit ShenandoahUpdateHeapRefsTask(ShenandoahRegionIterator* regions) :\n@@ -2359,5 +2353,1 @@\n-    _regions(regions),\n-    _work_chunks(work_chunks)\n-  {\n-    bool old_bitmap_stable = _heap->old_generation()->is_mark_complete();\n-    log_info(gc, remset)(\"Scan remembered set using bitmap: %s\", BOOL_TO_STR(old_bitmap_stable));\n+    _regions(regions) {\n@@ -2380,1 +2370,0 @@\n-    T cl;\n@@ -2390,1 +2379,1 @@\n-\n+    T cl;\n@@ -2392,4 +2381,0 @@\n-    \/\/ We update references for global, old, and young collections.\n-    assert(_heap->active_generation()->is_mark_complete(), \"Expected complete marking\");\n-    ShenandoahMarkingContext* const ctx = _heap->marking_context();\n-    bool is_mixed = _heap->collection_set()->has_old_regions();\n@@ -2399,3 +2384,0 @@\n-\n-      log_debug(gc)(\"ShenandoahUpdateHeapRefsTask::do_work(%u) looking at region \" SIZE_FORMAT, worker_id, r->index());\n-      bool region_progress = false;\n@@ -2403,30 +2385,3 @@\n-        if (!_heap->mode()->is_generational() || r->is_young()) {\n-          _heap->marked_object_oop_iterate(r, &cl, update_watermark);\n-          region_progress = true;\n-        } else if (r->is_old()) {\n-          if (_heap->active_generation()->is_global()) {\n-            \/\/ Note that GLOBAL collection is not as effectively balanced as young and mixed cycles.  This is because\n-            \/\/ concurrent GC threads are parceled out entire heap regions of work at a time and there\n-            \/\/ is no \"catchup phase\" consisting of remembered set scanning, during which parcels of work are smaller\n-            \/\/ and more easily distributed more fairly across threads.\n-\n-            \/\/ TODO: Consider an improvement to load balance GLOBAL GC.\n-            _heap->marked_object_oop_iterate(r, &cl, update_watermark);\n-            region_progress = true;\n-          }\n-          \/\/ Otherwise, this is an old region in a young or mixed cycle.  Process it during a second phase, below.\n-          \/\/ Don't bother to report pacing progress in this case.\n-        } else {\n-          \/\/ Because updating of references runs concurrently, it is possible that a FREE inactive region transitions\n-          \/\/ to a non-free active region while this loop is executing.  Whenever this happens, the changing of a region's\n-          \/\/ active status may propagate at a different speed than the changing of the region's affiliation.\n-\n-          \/\/ When we reach this control point, it is because a race has allowed a region's is_active() status to be seen\n-          \/\/ by this thread before the region's affiliation() is seen by this thread.\n-\n-          \/\/ It's ok for this race to occur because the newly transformed region does not have any references to be\n-          \/\/ updated.\n-\n-          assert(r->get_update_watermark() == r->bottom(),\n-                 \"%s Region \" SIZE_FORMAT \" is_active but not recognized as YOUNG or OLD so must be newly transitioned from FREE\",\n-                 r->affiliation_name(), r->index());\n+        _heap->marked_object_oop_iterate(r, &cl, update_watermark);\n+        if (ShenandoahPacing) {\n+          _heap->pacer()->report_updaterefs(pointer_delta(update_watermark, r->bottom()));\n@@ -2435,3 +2390,0 @@\n-      if (region_progress && ShenandoahPacing) {\n-        _heap->pacer()->report_updaterefs(pointer_delta(update_watermark, r->bottom()));\n-      }\n@@ -2443,114 +2395,0 @@\n-\n-    if (_heap->mode()->is_generational() && !_heap->active_generation()->is_global()) {\n-      \/\/ Since this is generational and not GLOBAL, we have to process the remembered set.  There's no remembered\n-      \/\/ set processing if not in generational mode or if GLOBAL mode.\n-\n-      \/\/ After this thread has exhausted its traditional update-refs work, it continues with updating refs within remembered set.\n-      \/\/ The remembered set workload is better balanced between threads, so threads that are \"behind\" can catch up with other\n-      \/\/ threads during this phase, allowing all threads to work more effectively in parallel.\n-      struct ShenandoahRegionChunk assignment;\n-      RememberedScanner* scanner = _heap->card_scan();\n-\n-      while (!_heap->check_cancelled_gc_and_yield(CONCURRENT) && _work_chunks->next(&assignment)) {\n-        \/\/ Keep grabbing next work chunk to process until finished, or asked to yield\n-        ShenandoahHeapRegion* r = assignment._r;\n-        if (r->is_active() && !r->is_cset() && r->is_old()) {\n-          HeapWord* start_of_range = r->bottom() + assignment._chunk_offset;\n-          HeapWord* end_of_range = r->get_update_watermark();\n-          if (end_of_range > start_of_range + assignment._chunk_size) {\n-            end_of_range = start_of_range + assignment._chunk_size;\n-          }\n-\n-          \/\/ Old region in a young cycle or mixed cycle.\n-          if (is_mixed) {\n-            \/\/ TODO: For mixed evac, consider building an old-gen remembered set that allows restricted updating\n-            \/\/ within old-gen HeapRegions.  This remembered set can be constructed by old-gen concurrent marking\n-            \/\/ and augmented by card marking.  For example, old-gen concurrent marking can remember for each old-gen\n-            \/\/ card which other old-gen regions it refers to: none, one-other specifically, multiple-other non-specific.\n-            \/\/ Update-references when _mixed_evac processess each old-gen memory range that has a traditional DIRTY\n-            \/\/ card or if the \"old-gen remembered set\" indicates that this card holds pointers specifically to an\n-            \/\/ old-gen region in the most recent collection set, or if this card holds pointers to other non-specific\n-            \/\/ old-gen heap regions.\n-\n-            if (r->is_humongous()) {\n-              if (start_of_range < end_of_range) {\n-                \/\/ Need to examine both dirty and clean cards during mixed evac.\n-                r->oop_iterate_humongous_slice(&cl, false, start_of_range, assignment._chunk_size, true);\n-              }\n-            } else {\n-              \/\/ Since this is mixed evacuation, old regions that are candidates for collection have not been coalesced\n-              \/\/ and filled.  Use mark bits to find objects that need to be updated.\n-              \/\/\n-              \/\/ Future TODO: establish a second remembered set to identify which old-gen regions point to other old-gen\n-              \/\/ regions which are in the collection set for a particular mixed evacuation.\n-              if (start_of_range < end_of_range) {\n-                HeapWord* p = nullptr;\n-                size_t card_index = scanner->card_index_for_addr(start_of_range);\n-                \/\/ In case last object in my range spans boundary of my chunk, I may need to scan all the way to top()\n-                ShenandoahObjectToOopBoundedClosure<T> objs(&cl, start_of_range, r->top());\n-\n-                \/\/ Any object that begins in a previous range is part of a different scanning assignment.  Any object that\n-                \/\/ starts after end_of_range is also not my responsibility.  (Either allocated during evacuation, so does\n-                \/\/ not hold pointers to from-space, or is beyond the range of my assigned work chunk.)\n-\n-                \/\/ Find the first object that begins in my range, if there is one.\n-                p = start_of_range;\n-                oop obj = cast_to_oop(p);\n-                HeapWord* tams = ctx->top_at_mark_start(r);\n-                if (p >= tams) {\n-                  \/\/ We cannot use ctx->is_marked(obj) to test whether an object begins at this address.  Instead,\n-                  \/\/ we need to use the remembered set crossing map to advance p to the first object that starts\n-                  \/\/ within the enclosing card.\n-\n-                  while (true) {\n-                    HeapWord* first_object = scanner->first_object_in_card(card_index);\n-                    if (first_object != nullptr) {\n-                      p = first_object;\n-                      break;\n-                    } else if (scanner->addr_for_card_index(card_index + 1) < end_of_range) {\n-                      card_index++;\n-                    } else {\n-                      \/\/ Force the loop that follows to immediately terminate.\n-                      p = end_of_range;\n-                      break;\n-                    }\n-                  }\n-                  obj = cast_to_oop(p);\n-                  \/\/ Note: p may be >= end_of_range\n-                } else if (!ctx->is_marked(obj)) {\n-                  p = ctx->get_next_marked_addr(p, tams);\n-                  obj = cast_to_oop(p);\n-                  \/\/ If there are no more marked objects before tams, this returns tams.\n-                  \/\/ Note that tams is either >= end_of_range, or tams is the start of an object that is marked.\n-                }\n-                while (p < end_of_range) {\n-                  \/\/ p is known to point to the beginning of marked object obj\n-                  objs.do_object(obj);\n-                  HeapWord* prev_p = p;\n-                  p += obj->size();\n-                  if (p < tams) {\n-                    p = ctx->get_next_marked_addr(p, tams);\n-                    \/\/ If there are no more marked objects before tams, this returns tams.  Note that tams is\n-                    \/\/ either >= end_of_range, or tams is the start of an object that is marked.\n-                  }\n-                  assert(p != prev_p, \"Lack of forward progress\");\n-                  obj = cast_to_oop(p);\n-                }\n-              }\n-            }\n-          } else {\n-            \/\/ This is a young evac..\n-            if (start_of_range < end_of_range) {\n-              size_t cluster_size =\n-                CardTable::card_size_in_words() * ShenandoahCardCluster<ShenandoahDirectCardMarkRememberedSet>::CardsPerCluster;\n-              size_t clusters = assignment._chunk_size \/ cluster_size;\n-              assert(clusters * cluster_size == assignment._chunk_size, \"Chunk assignment must align on cluster boundaries\");\n-              scanner->process_region_slice(r, assignment._chunk_offset, clusters, end_of_range, &cl, true, worker_id);\n-            }\n-          }\n-          if (ShenandoahPacing && (start_of_range < end_of_range)) {\n-            _heap->pacer()->report_updaterefs(pointer_delta(end_of_range, start_of_range));\n-          }\n-        }\n-      }\n-    }\n@@ -2562,3 +2400,1 @@\n-  uint nworkers = workers()->active_workers();\n-  ShenandoahRegionChunkIterator work_list(nworkers);\n-\n+  ShenandoahRegionIterator update_refs_iterator(this);\n@@ -2566,1 +2402,1 @@\n-    ShenandoahUpdateHeapRefsTask<true> task(&_update_refs_iterator, &work_list);\n+    ShenandoahUpdateHeapRefsTask<true> task(&update_refs_iterator);\n@@ -2569,1 +2405,1 @@\n-    ShenandoahUpdateHeapRefsTask<false> task(&_update_refs_iterator, &work_list);\n+    ShenandoahUpdateHeapRefsTask<false> task(&update_refs_iterator);\n@@ -2572,3 +2408,1 @@\n-  if (ShenandoahEnableCardStats && card_scan()!=nullptr) { \/\/ generational check proxy\n-    card_scan()->log_card_stats(nworkers, CARD_STAT_UPDATE_REFS);\n-  }\n+  assert(cancelled_gc() || !update_refs_iterator.has_next(), \"Should have finished update references\");\n","filename":"src\/hotspot\/share\/gc\/shenandoah\/shenandoahHeap.cpp","additions":10,"deletions":176,"binary":false,"changes":186,"status":"modified"},{"patch":"@@ -271,1 +271,0 @@\n-  ShenandoahRegionIterator _update_refs_iterator;\n@@ -454,1 +453,1 @@\n-  void update_heap_references(bool concurrent);\n+  virtual void update_heap_references(bool concurrent);\n","filename":"src\/hotspot\/share\/gc\/shenandoah\/shenandoahHeap.hpp","additions":1,"deletions":2,"binary":false,"changes":3,"status":"modified"},{"patch":"@@ -1349,1 +1349,0 @@\n-  ShenandoahRegionIterator iterator;\n@@ -1363,7 +1362,2 @@\n-  while (iterator.has_next()) {\n-    ShenandoahHeapRegion* r = iterator.next();\n-    if (r == nullptr) {\n-      \/\/ TODO: Can this really happen?\n-      break;\n-    }\n-\n+  for (size_t i = 0, n = _heap->num_regions(); i < n; ++i) {\n+    ShenandoahHeapRegion* r = _heap->get_region(i);\n@@ -1424,8 +1418,2 @@\n-  ShenandoahRegionIterator iterator;\n-\n-  while (iterator.has_next()) {\n-    ShenandoahHeapRegion* r = iterator.next();\n-    if (r == nullptr) {\n-      \/\/ TODO: Can this really happen?\n-      break;\n-    }\n+  for (size_t i = 0, n = _heap->num_regions(); i < n; ++i) {\n+    ShenandoahHeapRegion* r = _heap->get_region(i);\n@@ -1446,1 +1434,0 @@\n-  ShenandoahRegionIterator iterator;\n@@ -1455,6 +1442,2 @@\n-  while (iterator.has_next()) {\n-    ShenandoahHeapRegion* r = iterator.next();\n-    if (r == nullptr) {\n-      \/\/ TODO: Can this really happen?\n-      break;\n-    }\n+  for (size_t i = 0, n = _heap->num_regions(); i < n; ++i) {\n+    ShenandoahHeapRegion* r = _heap->get_region(i);\n","filename":"src\/hotspot\/share\/gc\/shenandoah\/shenandoahVerifier.cpp","additions":6,"deletions":23,"binary":false,"changes":29,"status":"modified"}]}