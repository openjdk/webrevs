{"files":[{"patch":"@@ -2,1 +2,1 @@\n-project=jdk-updates\n+project=shenandoah\n@@ -4,0 +4,1 @@\n+<<<<<<< HEAD\n@@ -5,0 +6,3 @@\n+=======\n+version=repo-shenandoah-21\n+>>>>>>> 82b11cb49a9f6509959a6476d0d1d91c81fcfd02\n@@ -26,1 +30,1 @@\n-reviewers=1\n+committers=1\n","filename":".jcheck\/conf","additions":6,"deletions":2,"binary":false,"changes":8,"status":"modified"},{"patch":"@@ -4,0 +4,1 @@\n+ * Copyright Amazon.com Inc. or its affiliates. All Rights Reserved.\n@@ -32,0 +33,1 @@\n+#include \"gc\/shenandoah\/shenandoahClosures.inline.hpp\"\n@@ -35,0 +37,4 @@\n+#include \"gc\/shenandoah\/shenandoahGeneration.hpp\"\n+#include \"gc\/shenandoah\/shenandoahGenerationalHeap.hpp\"\n+#include \"gc\/shenandoah\/shenandoahOldGeneration.hpp\"\n+#include \"gc\/shenandoah\/shenandoahYoungGeneration.hpp\"\n@@ -38,1 +44,0 @@\n-#include \"gc\/shenandoah\/shenandoahOopClosures.inline.hpp\"\n@@ -89,3 +94,6 @@\n-ShenandoahConcurrentGC::ShenandoahConcurrentGC() :\n-  _mark(),\n-  _degen_point(ShenandoahDegenPoint::_degenerated_unset) {\n+ShenandoahConcurrentGC::ShenandoahConcurrentGC(ShenandoahGeneration* generation, bool do_old_gc_bootstrap) :\n+  _mark(generation),\n+  _generation(generation),\n+  _degen_point(ShenandoahDegenPoint::_degenerated_unset),\n+  _abbreviated(false),\n+  _do_old_gc_bootstrap(do_old_gc_bootstrap) {\n@@ -98,2 +106,8 @@\n-void ShenandoahConcurrentGC::cancel() {\n-  ShenandoahConcurrentMark::cancel();\n+void ShenandoahConcurrentGC::entry_concurrent_update_refs_prepare(ShenandoahHeap* const heap) {\n+  TraceCollectorStats tcs(heap->monitoring_support()->concurrent_collection_counters());\n+  const char* msg = conc_init_update_refs_event_message();\n+  ShenandoahConcurrentPhase gc_phase(msg, ShenandoahPhaseTimings::conc_update_refs_prepare);\n+  EventMark em(\"%s\", msg);\n+\n+  \/\/ Evacuation is complete, retire gc labs and change gc state\n+  heap->concurrent_prepare_for_update_refs();\n@@ -104,0 +118,1 @@\n+\n@@ -114,0 +129,9 @@\n+\n+    \/\/ Reset task queue stats here, rather than in mark_concurrent_roots,\n+    \/\/ because remembered set scan will `push` oops into the queues and\n+    \/\/ resetting after this happens will lose those counts.\n+    TASKQUEUE_STATS_ONLY(_mark.task_queues()->reset_taskqueue_stats());\n+\n+    \/\/ Concurrent remembered set scanning\n+    entry_scan_remembered_set();\n+\n@@ -116,1 +140,3 @@\n-    if (check_cancellation_and_abort(ShenandoahDegenPoint::_degenerated_outside_cycle)) return false;\n+    if (check_cancellation_and_abort(ShenandoahDegenPoint::_degenerated_roots)) {\n+      return false;\n+    }\n@@ -120,1 +146,3 @@\n-    if (check_cancellation_and_abort(ShenandoahDegenPoint::_degenerated_mark)) return false;\n+    if (check_cancellation_and_abort(ShenandoahDegenPoint::_degenerated_mark)) {\n+      return false;\n+    }\n@@ -126,0 +154,12 @@\n+  \/\/ If the GC was cancelled before final mark, nothing happens on the safepoint. We are still\n+  \/\/ in the marking phase and must resume the degenerated cycle from there. If the GC was cancelled\n+  \/\/ after final mark, then we've entered the evacuation phase and must resume the degenerated cycle\n+  \/\/ from that phase.\n+  if (_generation->is_concurrent_mark_in_progress()) {\n+    bool cancelled = check_cancellation_and_abort(ShenandoahDegenPoint::_degenerated_mark);\n+    assert(cancelled, \"GC must have been cancelled between concurrent and final mark\");\n+    return false;\n+  }\n+\n+  assert(heap->is_concurrent_weak_root_in_progress(), \"Must be doing weak roots now\");\n+\n@@ -131,4 +171,9 @@\n-  \/\/ Process weak roots that might still point to regions that would be broken by cleanup\n-  if (heap->is_concurrent_weak_root_in_progress()) {\n-    entry_weak_refs();\n-    entry_weak_roots();\n+  \/\/ Process weak roots that might still point to regions that would be broken by cleanup.\n+  \/\/ We cannot recycle regions because weak roots need to know what is marked in trashed regions.\n+  entry_weak_refs();\n+  entry_weak_roots();\n+\n+  \/\/ Perform concurrent class unloading before any regions get recycled. Class unloading may\n+  \/\/ need to inspect unmarked objects in trashed regions.\n+  if (heap->unload_classes()) {\n+    entry_class_unloading();\n@@ -138,1 +183,2 @@\n-  \/\/ the space. This would be the last action if there is nothing to evacuate.\n+  \/\/ the space. This would be the last action if there is nothing to evacuate.  Note that\n+  \/\/ we will not age young-gen objects in the case that we skip evacuation.\n@@ -141,10 +187,1 @@\n-  {\n-    ShenandoahHeapLocker locker(heap->lock());\n-    heap->free_set()->log_status();\n-  }\n-\n-  \/\/ Perform concurrent class unloading\n-  if (heap->unload_classes() &&\n-      heap->is_concurrent_weak_root_in_progress()) {\n-    entry_class_unloading();\n-  }\n+  heap->free_set()->log_status_under_lock();\n@@ -165,1 +202,5 @@\n-    if (check_cancellation_and_abort(ShenandoahDegenPoint::_degenerated_evac)) return false;\n+    if (check_cancellation_and_abort(ShenandoahDegenPoint::_degenerated_evac)) {\n+      return false;\n+    }\n+\n+    entry_concurrent_update_refs_prepare(heap);\n@@ -168,3 +209,8 @@\n-    vmop_entry_init_updaterefs();\n-    entry_updaterefs();\n-    if (check_cancellation_and_abort(ShenandoahDegenPoint::_degenerated_updaterefs)) return false;\n+    if (ShenandoahVerify || ShenandoahPacing) {\n+      vmop_entry_init_update_refs();\n+    }\n+\n+    entry_update_refs();\n+    if (check_cancellation_and_abort(ShenandoahDegenPoint::_degenerated_update_refs)) {\n+      return false;\n+    }\n@@ -174,1 +220,3 @@\n-    if (check_cancellation_and_abort(ShenandoahDegenPoint::_degenerated_updaterefs)) return false;\n+    if (check_cancellation_and_abort(ShenandoahDegenPoint::_degenerated_update_refs)) {\n+      return false;\n+    }\n@@ -176,1 +224,1 @@\n-    vmop_entry_final_updaterefs();\n+    vmop_entry_final_update_refs();\n@@ -181,1 +229,15 @@\n-    vmop_entry_final_roots();\n+    if (!entry_final_roots()) {\n+      assert(_degen_point != _degenerated_unset, \"Need to know where to start degenerated cycle\");\n+      return false;\n+    }\n+\n+    if (VerifyAfterGC) {\n+      vmop_entry_verify_final_roots();\n+    }\n+    _abbreviated = true;\n+  }\n+\n+  \/\/ We defer generation resizing actions until after cset regions have been recycled.  We do this even following an\n+  \/\/ abbreviated cycle.\n+  if (heap->mode()->is_generational()) {\n+    ShenandoahGenerationalHeap::heap()->complete_concurrent_cycle();\n@@ -184,0 +246,5 @@\n+  \/\/ Instead of always resetting immediately before the start of a new GC, we can often reset at the end of the\n+  \/\/ previous GC. This allows us to start the next GC cycle more quickly after a trigger condition is detected,\n+  \/\/ reducing the likelihood that GC will degenerate.\n+  entry_reset_after_collect();\n+\n@@ -187,0 +254,46 @@\n+bool ShenandoahConcurrentGC::complete_abbreviated_cycle() {\n+  shenandoah_assert_generational();\n+\n+  ShenandoahGenerationalHeap* const heap = ShenandoahGenerationalHeap::heap();\n+\n+  \/\/ We chose not to evacuate because we found sufficient immediate garbage.\n+  \/\/ However, there may still be regions to promote in place, so do that now.\n+  if (heap->old_generation()->has_in_place_promotions()) {\n+    entry_promote_in_place();\n+\n+    \/\/ If the promote-in-place operation was cancelled, we can have the degenerated\n+    \/\/ cycle complete the operation. It will see that no evacuations are in progress,\n+    \/\/ and that there are regions wanting promotion. The risk with not handling the\n+    \/\/ cancellation would be failing to restore top for these regions and leaving\n+    \/\/ them unable to serve allocations for the old generation.This will leave the weak\n+    \/\/ roots flag set (the degenerated cycle will unset it).\n+    if (check_cancellation_and_abort(ShenandoahDegenPoint::_degenerated_evac)) {\n+      return false;\n+    }\n+  }\n+\n+  \/\/ At this point, the cycle is effectively complete. If the cycle has been cancelled here,\n+  \/\/ the control thread will detect it on its next iteration and run a degenerated young cycle.\n+  if (!_generation->is_old()) {\n+    heap->update_region_ages(_generation->complete_marking_context());\n+  }\n+\n+  if (!heap->is_concurrent_old_mark_in_progress()) {\n+    heap->concurrent_final_roots();\n+  } else {\n+    \/\/ Since the cycle was shortened for having enough immediate garbage, this will be\n+    \/\/ the last phase before concurrent marking of old resumes. We must be sure\n+    \/\/ that old mark threads don't see any pointers to garbage in the SATB queues. Even\n+    \/\/ though nothing was evacuated, overwriting unreachable weak roots with null may still\n+    \/\/ put pointers to regions that become trash in the SATB queues. The following will\n+    \/\/ piggyback flushing the thread local SATB queues on the same handshake that propagates\n+    \/\/ the gc state change.\n+    ShenandoahSATBMarkQueueSet& satb_queues = ShenandoahBarrierSet::satb_mark_queue_set();\n+    ShenandoahFlushSATBHandshakeClosure complete_thread_local_satb_buffers(satb_queues);\n+    heap->concurrent_final_roots(&complete_thread_local_satb_buffers);\n+    heap->old_generation()->concurrent_transfer_pointers_from_satb();\n+  }\n+  return true;\n+}\n+\n+\n@@ -207,1 +320,1 @@\n-void ShenandoahConcurrentGC::vmop_entry_init_updaterefs() {\n+void ShenandoahConcurrentGC::vmop_entry_init_update_refs() {\n@@ -217,1 +330,1 @@\n-void ShenandoahConcurrentGC::vmop_entry_final_updaterefs() {\n+void ShenandoahConcurrentGC::vmop_entry_final_update_refs() {\n@@ -227,1 +340,1 @@\n-void ShenandoahConcurrentGC::vmop_entry_final_roots() {\n+void ShenandoahConcurrentGC::vmop_entry_verify_final_roots() {\n@@ -262,1 +375,1 @@\n-void ShenandoahConcurrentGC::entry_init_updaterefs() {\n+void ShenandoahConcurrentGC::entry_init_update_refs() {\n@@ -268,1 +381,1 @@\n-  op_init_updaterefs();\n+  op_init_update_refs();\n@@ -271,1 +384,1 @@\n-void ShenandoahConcurrentGC::entry_final_updaterefs() {\n+void ShenandoahConcurrentGC::entry_final_update_refs() {\n@@ -280,1 +393,1 @@\n-  op_final_updaterefs();\n+  op_final_update_refs();\n@@ -283,2 +396,2 @@\n-void ShenandoahConcurrentGC::entry_final_roots() {\n-  static const char* msg = \"Pause Final Roots\";\n+void ShenandoahConcurrentGC::entry_verify_final_roots() {\n+  const char* msg = verify_final_roots_event_message();\n@@ -288,1 +401,1 @@\n-  op_final_roots();\n+  op_verify_final_roots();\n@@ -293,0 +406,2 @@\n+  heap->try_inject_alloc_failure();\n+\n@@ -294,3 +409,10 @@\n-  static const char* msg = \"Concurrent reset\";\n-  ShenandoahConcurrentPhase gc_phase(msg, ShenandoahPhaseTimings::conc_reset);\n-  EventMark em(\"%s\", msg);\n+  {\n+    const char* msg = conc_reset_event_message();\n+    ShenandoahConcurrentPhase gc_phase(msg, ShenandoahPhaseTimings::conc_reset);\n+    EventMark em(\"%s\", msg);\n+\n+    ShenandoahWorkerScope scope(heap->workers(),\n+                                ShenandoahWorkerPolicy::calc_workers_for_conc_reset(),\n+                                msg);\n+    op_reset();\n+  }\n@@ -298,3 +420,4 @@\n-  ShenandoahWorkerScope scope(heap->workers(),\n-                              ShenandoahWorkerPolicy::calc_workers_for_conc_reset(),\n-                              \"concurrent reset\");\n+  if (heap->mode()->is_generational()) {\n+    heap->old_generation()->card_scan()->mark_read_table_as_clean();\n+  }\n+}\n@@ -302,2 +425,15 @@\n-  heap->try_inject_alloc_failure();\n-  op_reset();\n+void ShenandoahConcurrentGC::entry_scan_remembered_set() {\n+  if (_generation->is_young()) {\n+    ShenandoahHeap* const heap = ShenandoahHeap::heap();\n+    TraceCollectorStats tcs(heap->monitoring_support()->concurrent_collection_counters());\n+    const char* msg = \"Concurrent remembered set scanning\";\n+    ShenandoahConcurrentPhase gc_phase(msg, ShenandoahPhaseTimings::init_scan_rset);\n+    EventMark em(\"%s\", msg);\n+\n+    ShenandoahWorkerScope scope(heap->workers(),\n+                                ShenandoahWorkerPolicy::calc_workers_for_rs_scanning(),\n+                                msg);\n+\n+    heap->try_inject_alloc_failure();\n+    _generation->scan_remembered_set(true \/* is_concurrent *\/);\n+  }\n@@ -352,1 +488,1 @@\n-  static const char* msg = \"Concurrent weak references\";\n+  const char* msg = conc_weak_refs_event_message();\n@@ -367,1 +503,1 @@\n-  static const char* msg = \"Concurrent weak roots\";\n+  const char* msg = conc_weak_roots_event_message();\n@@ -414,1 +550,1 @@\n-  static const char* msg = \"Concurrent cleanup\";\n+  const char* msg = conc_cleanup_event_message();\n@@ -439,0 +575,10 @@\n+void ShenandoahConcurrentGC::entry_promote_in_place() const {\n+  shenandoah_assert_generational();\n+\n+  ShenandoahTimingsTracker timing(ShenandoahPhaseTimings::promote_in_place);\n+  ShenandoahGCWorkerPhase worker_phase(ShenandoahPhaseTimings::promote_in_place);\n+  EventMark em(\"%s\", \"Promote in place\");\n+\n+  ShenandoahGenerationalHeap::heap()->promote_regions_in_place(true);\n+}\n+\n@@ -452,1 +598,1 @@\n-void ShenandoahConcurrentGC::entry_updaterefs() {\n+void ShenandoahConcurrentGC::entry_update_refs() {\n@@ -464,1 +610,1 @@\n-  op_updaterefs();\n+  op_update_refs();\n@@ -470,1 +616,1 @@\n-  static const char* msg = \"Concurrent cleanup\";\n+  const char* msg = conc_cleanup_event_message();\n@@ -479,0 +625,10 @@\n+void ShenandoahConcurrentGC::entry_reset_after_collect() {\n+  ShenandoahHeap* const heap = ShenandoahHeap::heap();\n+  TraceCollectorStats tcs(heap->monitoring_support()->concurrent_collection_counters());\n+  const char* msg = conc_reset_after_collect_event_message();\n+  ShenandoahConcurrentPhase gc_phase(msg, ShenandoahPhaseTimings::conc_reset_after_collect);\n+  EventMark em(\"%s\", msg);\n+\n+  op_reset_after_collect();\n+}\n+\n@@ -484,2 +640,8 @@\n-\n-  heap->prepare_gc();\n+  \/\/ If it is old GC bootstrap cycle, always clear bitmap for global gen\n+  \/\/ to ensure bitmap for old gen is clear for old GC cycle after this.\n+  if (_do_old_gc_bootstrap) {\n+    assert(!heap->is_prepare_for_old_mark_in_progress(), \"Cannot reset old without making it parsable\");\n+    heap->global_generation()->prepare_gc();\n+  } else {\n+    _generation->prepare_gc();\n+  }\n@@ -498,1 +660,2 @@\n-      \/\/ reset, so it is very likely we don't need to do another write here.\n+      \/\/ reset, so it is very likely we don't need to do another write here.  Since most regions\n+      \/\/ are not \"active\", this path is relatively rare.\n@@ -520,2 +683,2 @@\n-  assert(heap->marking_context()->is_bitmap_clear(), \"need clear marking bitmap\");\n-  assert(!heap->marking_context()->is_complete(), \"should not be complete\");\n+  assert(_generation->is_bitmap_clear(), \"need clear marking bitmap\");\n+  assert(!_generation->is_mark_complete(), \"should not be complete\");\n@@ -524,0 +687,21 @@\n+  if (heap->mode()->is_generational()) {\n+\n+    if (_generation->is_global()) {\n+      heap->old_generation()->cancel_gc();\n+    } else if (heap->is_concurrent_old_mark_in_progress()) {\n+      \/\/ Purge the SATB buffers, transferring any valid, old pointers to the\n+      \/\/ old generation mark queue. Any pointers in a young region will be\n+      \/\/ abandoned.\n+      ShenandoahGCPhase phase(ShenandoahPhaseTimings::init_transfer_satb);\n+      heap->old_generation()->transfer_pointers_from_satb();\n+    }\n+    {\n+      \/\/ After we swap card table below, the write-table is all clean, and the read table holds\n+      \/\/ cards dirty prior to the start of GC. Young and bootstrap collection will update\n+      \/\/ the write card table as a side effect of remembered set scanning. Global collection will\n+      \/\/ update the card table as a side effect of global marking of old objects.\n+      ShenandoahGCPhase phase(ShenandoahPhaseTimings::init_swap_rset);\n+      _generation->swap_card_tables();\n+    }\n+  }\n+\n@@ -525,0 +709,1 @@\n+    ShenandoahTimingsTracker v(ShenandoahPhaseTimings::init_mark_verify);\n@@ -532,1 +717,1 @@\n-  heap->set_concurrent_mark_in_progress(true);\n+  _generation->set_concurrent_mark_in_progress(true);\n@@ -536,1 +721,3 @@\n-  {\n+  if (_do_old_gc_bootstrap) {\n+    shenandoah_assert_generational();\n+    \/\/ Update region state for both young and old regions\n@@ -540,0 +727,6 @@\n+    heap->old_generation()->ref_processor()->reset_thread_locals();\n+  } else {\n+    \/\/ Update region state for only young regions\n+    ShenandoahGCPhase phase(ShenandoahPhaseTimings::init_update_region_states);\n+    ShenandoahInitMarkUpdateRegionStateClosure cl;\n+    _generation->parallel_heap_region_iterate(&cl);\n@@ -543,1 +736,1 @@\n-  ShenandoahReferenceProcessor* rp = heap->ref_processor();\n+  ShenandoahReferenceProcessor* rp = _generation->ref_processor();\n@@ -557,0 +750,5 @@\n+\n+  {\n+    ShenandoahTimingsTracker timing(ShenandoahPhaseTimings::init_propagate_gc_state);\n+    heap->propagate_gc_state_to_all_threads();\n+  }\n@@ -583,1 +781,4 @@\n-    heap->prepare_regions_and_collection_set(true \/*concurrent*\/);\n+    \/\/ The collection set is chosen by prepare_regions_and_collection_set(). Additionally, certain parameters have been\n+    \/\/ established to govern the evacuation efforts that are about to begin.  Refer to comments on reserve members in\n+    \/\/ ShenandoahGeneration and ShenandoahOldGeneration for more detail.\n+    _generation->prepare_regions_and_collection_set(true \/*concurrent*\/);\n@@ -589,0 +790,7 @@\n+      LogTarget(Debug, gc, cset) lt;\n+      if (lt.is_enabled()) {\n+        ResourceMark rm;\n+        LogStream ls(lt);\n+        heap->collection_set()->print_on(&ls);\n+      }\n+\n@@ -590,0 +798,1 @@\n+        ShenandoahTimingsTracker v(ShenandoahPhaseTimings::final_mark_verify);\n@@ -597,6 +806,0 @@\n-      \/\/ Verify before arming for concurrent processing.\n-      \/\/ Otherwise, verification can trigger stack processing.\n-      if (ShenandoahVerify) {\n-        heap->verifier()->verify_during_evacuation();\n-      }\n-\n@@ -612,5 +815,6 @@\n-        heap->verifier()->verify_after_concmark();\n-      }\n-\n-      if (VerifyAfterGC) {\n-        Universe::verify();\n+        ShenandoahTimingsTracker v(ShenandoahPhaseTimings::final_mark_verify);\n+        if (has_in_place_promotions(heap)) {\n+          heap->verifier()->verify_after_concmark_with_promotions();\n+        } else {\n+          heap->verifier()->verify_after_concmark();\n+        }\n@@ -620,0 +824,9 @@\n+\n+  {\n+    ShenandoahTimingsTracker timing(ShenandoahPhaseTimings::final_mark_propagate_gc_state);\n+    heap->propagate_gc_state_to_all_threads();\n+  }\n+}\n+\n+bool ShenandoahConcurrentGC::has_in_place_promotions(ShenandoahHeap* heap) {\n+  return heap->mode()->is_generational() && heap->old_generation()->has_in_place_promotions();\n@@ -622,0 +835,1 @@\n+template<bool GENERATIONAL>\n@@ -625,1 +839,0 @@\n-\n@@ -627,3 +840,1 @@\n-  ShenandoahConcurrentEvacThreadClosure(OopClosure* oops);\n-  void do_thread(Thread* thread);\n-};\n+  explicit ShenandoahConcurrentEvacThreadClosure(OopClosure* oops) : _oops(oops) {}\n@@ -631,8 +842,8 @@\n-ShenandoahConcurrentEvacThreadClosure::ShenandoahConcurrentEvacThreadClosure(OopClosure* oops) :\n-  _oops(oops) {\n-}\n-\n-void ShenandoahConcurrentEvacThreadClosure::do_thread(Thread* thread) {\n-  JavaThread* const jt = JavaThread::cast(thread);\n-  StackWatermarkSet::finish_processing(jt, _oops, StackWatermarkKind::gc);\n-}\n+  void do_thread(Thread* thread) override {\n+    JavaThread* const jt = JavaThread::cast(thread);\n+    StackWatermarkSet::finish_processing(jt, _oops, StackWatermarkKind::gc);\n+    if (GENERATIONAL) {\n+      ShenandoahThreadLocalData::enable_plab_promotions(thread);\n+    }\n+  }\n+};\n@@ -640,0 +851,1 @@\n+template<bool GENERATIONAL>\n@@ -645,1 +857,1 @@\n-  ShenandoahConcurrentEvacUpdateThreadTask(uint n_workers) :\n+  explicit ShenandoahConcurrentEvacUpdateThreadTask(uint n_workers) :\n@@ -650,1 +862,6 @@\n-  void work(uint worker_id) {\n+  void work(uint worker_id) override {\n+    if (GENERATIONAL) {\n+      Thread* worker_thread = Thread::current();\n+      ShenandoahThreadLocalData::enable_plab_promotions(worker_thread);\n+    }\n+\n@@ -654,1 +871,1 @@\n-    ShenandoahConcurrentEvacThreadClosure thr_cl(&oops_cl);\n+    ShenandoahConcurrentEvacThreadClosure<GENERATIONAL> thr_cl(&oops_cl);\n@@ -663,2 +880,7 @@\n-  ShenandoahConcurrentEvacUpdateThreadTask task(heap->workers()->active_workers());\n-  heap->workers()->run_task(&task);\n+  if (heap->mode()->is_generational()) {\n+    ShenandoahConcurrentEvacUpdateThreadTask<true> task(heap->workers()->active_workers());\n+    heap->workers()->run_task(&task);\n+  } else {\n+    ShenandoahConcurrentEvacUpdateThreadTask<false> task(heap->workers()->active_workers());\n+    heap->workers()->run_task(&task);\n+  }\n@@ -675,1 +897,1 @@\n-  heap->ref_processor()->process_references(ShenandoahPhaseTimings::conc_weak_refs, heap->workers(), true \/* concurrent *\/);\n+  _generation->ref_processor()->process_references(ShenandoahPhaseTimings::conc_weak_refs, heap->workers(), true \/* concurrent *\/);\n@@ -702,0 +924,1 @@\n+<<<<<<< HEAD\n@@ -704,0 +927,7 @@\n+=======\n+      shenandoah_assert_generations_reconciled();\n+      if (_heap->is_in_active_generation(obj)) {\n+        \/\/ Note: The obj is dead here. Do not touch it, just clear.\n+        ShenandoahHeap::atomic_clear_oop(p, obj);\n+      }\n+>>>>>>> 82b11cb49a9f6509959a6476d0d1d91c81fcfd02\n@@ -709,0 +939,1 @@\n+      shenandoah_assert_not_in_cset_except(p, resolved, _heap->cancelled_gc());\n@@ -710,3 +941,0 @@\n-      assert(_heap->cancelled_gc() ||\n-             _mark_context->is_marked(resolved) && !_heap->in_collection_set(resolved),\n-             \"Sanity\");\n@@ -772,2 +1000,2 @@\n-    \/\/ cleanup the weak oops in CLD and determinate nmethod's unloading state, so that we\n-    \/\/ can cleanup immediate garbage sooner.\n+    \/\/ clean up the weak oops in CLD and determine nmethod's unloading state, so that we\n+    \/\/ can clean up immediate garbage sooner.\n@@ -798,1 +1026,0 @@\n-  \/\/ Concurrent weak root processing\n@@ -800,0 +1027,1 @@\n+    \/\/ Concurrent weak root processing\n@@ -806,1 +1034,0 @@\n-  \/\/ Perform handshake to flush out dead oops\n@@ -808,0 +1035,7 @@\n+    \/\/ It is possible for mutators executing the load reference barrier to have\n+    \/\/ loaded an oop through a weak handle that has since been nulled out by\n+    \/\/ weak root processing. Handshaking here forces them to complete the\n+    \/\/ barrier before the GC cycle continues and does something that would\n+    \/\/ change the evaluation of the barrier (for example, resetting the TAMS\n+    \/\/ on trashed regions could make an oop appear to be marked _after_ the\n+    \/\/ region has been recycled).\n@@ -895,1 +1129,4 @@\n-  ShenandoahHeap::heap()->free_set()->recycle_trash();\n+  ShenandoahWorkerScope scope(ShenandoahHeap::heap()->workers(),\n+                              ShenandoahWorkerPolicy::calc_workers_for_conc_cleanup(),\n+                              \"cleanup early.\");\n+  ShenandoahHeap::heap()->recycle_trash();\n@@ -902,1 +1139,1 @@\n-void ShenandoahConcurrentGC::op_init_updaterefs() {\n+void ShenandoahConcurrentGC::op_init_update_refs() {\n@@ -904,5 +1141,4 @@\n-  heap->set_evacuation_in_progress(false);\n-  heap->set_concurrent_weak_root_in_progress(false);\n-  heap->prepare_update_heap_references(true \/*concurrent*\/);\n-  heap->set_update_refs_in_progress(true);\n-\n+  if (ShenandoahVerify) {\n+    ShenandoahTimingsTracker v(ShenandoahPhaseTimings::init_update_refs_verify);\n+    heap->verifier()->verify_before_update_refs();\n+  }\n@@ -910,1 +1146,1 @@\n-    heap->pacer()->setup_for_updaterefs();\n+    heap->pacer()->setup_for_update_refs();\n@@ -914,1 +1150,1 @@\n-void ShenandoahConcurrentGC::op_updaterefs() {\n+void ShenandoahConcurrentGC::op_update_refs() {\n@@ -943,1 +1179,1 @@\n-void ShenandoahConcurrentGC::op_final_updaterefs() {\n+void ShenandoahConcurrentGC::op_final_update_refs() {\n@@ -953,1 +1189,1 @@\n-    heap->clear_cancelled_gc();\n+    heap->clear_cancelled_gc(true \/* clear oom handler *\/);\n@@ -961,0 +1197,2 @@\n+  \/\/ If we are running in generational mode and this is an aging cycle, this will also age active\n+  \/\/ regions that haven't been used for allocation.\n@@ -966,0 +1204,22 @@\n+  if (heap->mode()->is_generational() && heap->is_concurrent_old_mark_in_progress()) {\n+    \/\/ When the SATB barrier is left on to support concurrent old gen mark, it may pick up writes to\n+    \/\/ objects in the collection set. After those objects are evacuated, the pointers in the\n+    \/\/ SATB are no longer safe. Once we have finished update references, we are guaranteed that\n+    \/\/ no more writes to the collection set are possible.\n+    \/\/\n+    \/\/ This will transfer any old pointers in _active_ regions from the SATB to the old gen\n+    \/\/ mark queues. All other pointers will be discarded. This would also discard any pointers\n+    \/\/ in old regions that were included in a mixed evacuation. We aren't using the SATB filter\n+    \/\/ methods here because we cannot control when they execute. If the SATB filter runs _after_\n+    \/\/ a region has been recycled, we will not be able to detect the bad pointer.\n+    \/\/\n+    \/\/ We are not concerned about skipping this step in abbreviated cycles because regions\n+    \/\/ with no live objects cannot have been written to and so cannot have entries in the SATB\n+    \/\/ buffers.\n+    heap->old_generation()->transfer_pointers_from_satb();\n+\n+    \/\/ Aging_cycle is only relevant during evacuation cycle for individual objects and during final mark for\n+    \/\/ entire regions.  Both of these relevant operations occur before final update refs.\n+    ShenandoahGenerationalHeap::heap()->set_aging_cycle(false);\n+  }\n+\n@@ -967,1 +1227,2 @@\n-    heap->verifier()->verify_after_updaterefs();\n+    ShenandoahTimingsTracker v(ShenandoahPhaseTimings::final_update_refs_verify);\n+    heap->verifier()->verify_after_update_refs();\n@@ -975,0 +1236,5 @@\n+\n+  {\n+    ShenandoahTimingsTracker timing(ShenandoahPhaseTimings::final_update_refs_propagate_gc_state);\n+    heap->propagate_gc_state_to_all_threads();\n+  }\n@@ -977,2 +1243,26 @@\n-void ShenandoahConcurrentGC::op_final_roots() {\n-  ShenandoahHeap::heap()->set_concurrent_weak_root_in_progress(false);\n+bool ShenandoahConcurrentGC::entry_final_roots() {\n+  ShenandoahHeap* const heap = ShenandoahHeap::heap();\n+  TraceCollectorStats tcs(heap->monitoring_support()->concurrent_collection_counters());\n+\n+\n+  const char* msg = conc_final_roots_event_message();\n+  ShenandoahConcurrentPhase gc_phase(msg, ShenandoahPhaseTimings::conc_final_roots);\n+  EventMark em(\"%s\", msg);\n+  ShenandoahWorkerScope scope(heap->workers(),\n+                              ShenandoahWorkerPolicy::calc_workers_for_conc_evac(),\n+                              msg);\n+\n+  if (!heap->mode()->is_generational()) {\n+    heap->concurrent_final_roots();\n+  } else {\n+    if (!complete_abbreviated_cycle()) {\n+      return false;\n+    }\n+  }\n+  return true;\n+}\n+\n+void ShenandoahConcurrentGC::op_verify_final_roots() {\n+  if (VerifyAfterGC) {\n+    Universe::verify();\n+  }\n@@ -982,1 +1272,22 @@\n-  ShenandoahHeap::heap()->free_set()->recycle_trash();\n+  ShenandoahWorkerScope scope(ShenandoahHeap::heap()->workers(),\n+                              ShenandoahWorkerPolicy::calc_workers_for_conc_cleanup(),\n+                              \"cleanup complete.\");\n+  ShenandoahHeap::heap()->recycle_trash();\n+}\n+\n+void ShenandoahConcurrentGC::op_reset_after_collect() {\n+  ShenandoahWorkerScope scope(ShenandoahHeap::heap()->workers(),\n+                          ShenandoahWorkerPolicy::calc_workers_for_conc_reset(),\n+                          \"reset after collection.\");\n+\n+  ShenandoahHeap* const heap = ShenandoahHeap::heap();\n+  if (heap->mode()->is_generational()) {\n+    \/\/ If we are in the midst of an old gc bootstrap or an old marking, we want to leave the mark bit map of\n+    \/\/ the young generation intact. In particular, reference processing in the old generation may potentially\n+    \/\/ need the reachability of a young generation referent of a Reference object in the old generation.\n+    if (!_do_old_gc_bootstrap && !heap->is_concurrent_old_mark_in_progress()) {\n+      heap->young_generation()->reset_mark_bitmap<false>();\n+    }\n+  } else {\n+    _generation->reset_mark_bitmap<false>();\n+  }\n@@ -997,1 +1308,1 @@\n-    return \"Pause Init Mark (unload classes)\";\n+    SHENANDOAH_RETURN_EVENT_MESSAGE(_generation->type(), \"Pause Init Mark\", \" (unload classes)\");\n@@ -999,1 +1310,1 @@\n-    return \"Pause Init Mark\";\n+    SHENANDOAH_RETURN_EVENT_MESSAGE(_generation->type(), \"Pause Init Mark\", \"\");\n@@ -1005,1 +1316,3 @@\n-  assert(!heap->has_forwarded_objects(), \"Should not have forwarded objects here\");\n+  assert(!heap->has_forwarded_objects() || heap->is_concurrent_old_mark_in_progress(),\n+         \"Should not have forwarded objects during final mark, unless old gen concurrent mark is running\");\n+\n@@ -1007,1 +1320,1 @@\n-    return \"Pause Final Mark (unload classes)\";\n+    SHENANDOAH_RETURN_EVENT_MESSAGE(_generation->type(), \"Pause Final Mark\", \" (unload classes)\");\n@@ -1009,1 +1322,1 @@\n-    return \"Pause Final Mark\";\n+    SHENANDOAH_RETURN_EVENT_MESSAGE(_generation->type(), \"Pause Final Mark\", \"\");\n@@ -1015,1 +1328,2 @@\n-  assert(!heap->has_forwarded_objects(), \"Should not have forwarded objects here\");\n+  assert(!heap->has_forwarded_objects() || heap->is_concurrent_old_mark_in_progress(),\n+         \"Should not have forwarded objects concurrent mark, unless old gen concurrent mark is running\");\n@@ -1017,1 +1331,65 @@\n-    return \"Concurrent marking (unload classes)\";\n+    SHENANDOAH_RETURN_EVENT_MESSAGE(_generation->type(), \"Concurrent marking\", \" (unload classes)\");\n+  } else {\n+    SHENANDOAH_RETURN_EVENT_MESSAGE(_generation->type(), \"Concurrent marking\", \"\");\n+  }\n+}\n+\n+const char* ShenandoahConcurrentGC::conc_reset_event_message() const {\n+  if (ShenandoahHeap::heap()->unload_classes()) {\n+    SHENANDOAH_RETURN_EVENT_MESSAGE(_generation->type(), \"Concurrent reset\", \" (unload classes)\");\n+  } else {\n+    SHENANDOAH_RETURN_EVENT_MESSAGE(_generation->type(), \"Concurrent reset\", \"\");\n+  }\n+}\n+\n+const char* ShenandoahConcurrentGC::conc_reset_after_collect_event_message() const {\n+  if (ShenandoahHeap::heap()->unload_classes()) {\n+    SHENANDOAH_RETURN_EVENT_MESSAGE(_generation->type(), \"Concurrent reset after collect\", \" (unload classes)\");\n+  } else {\n+    SHENANDOAH_RETURN_EVENT_MESSAGE(_generation->type(), \"Concurrent reset after collect\", \"\");\n+  }\n+}\n+\n+const char* ShenandoahConcurrentGC::verify_final_roots_event_message() const {\n+  if (ShenandoahHeap::heap()->unload_classes()) {\n+    SHENANDOAH_RETURN_EVENT_MESSAGE(_generation->type(), \"Pause Verify Final Roots\", \" (unload classes)\");\n+  } else {\n+    SHENANDOAH_RETURN_EVENT_MESSAGE(_generation->type(), \"Pause Verify Final Roots\", \"\");\n+  }\n+}\n+\n+const char* ShenandoahConcurrentGC::conc_final_roots_event_message() const {\n+  if (ShenandoahHeap::heap()->unload_classes()) {\n+    SHENANDOAH_RETURN_EVENT_MESSAGE(_generation->type(), \"Concurrent Final Roots\", \" (unload classes)\");\n+  } else {\n+    SHENANDOAH_RETURN_EVENT_MESSAGE(_generation->type(), \"Concurrent Final Roots\", \"\");\n+  }\n+}\n+\n+const char* ShenandoahConcurrentGC::conc_weak_refs_event_message() const {\n+  if (ShenandoahHeap::heap()->unload_classes()) {\n+    SHENANDOAH_RETURN_EVENT_MESSAGE(_generation->type(), \"Concurrent weak references\", \" (unload classes)\");\n+  } else {\n+    SHENANDOAH_RETURN_EVENT_MESSAGE(_generation->type(), \"Concurrent weak references\", \"\");\n+  }\n+}\n+\n+const char* ShenandoahConcurrentGC::conc_weak_roots_event_message() const {\n+  if (ShenandoahHeap::heap()->unload_classes()) {\n+    SHENANDOAH_RETURN_EVENT_MESSAGE(_generation->type(), \"Concurrent weak roots\", \" (unload classes)\");\n+  } else {\n+    SHENANDOAH_RETURN_EVENT_MESSAGE(_generation->type(), \"Concurrent weak roots\", \"\");\n+  }\n+}\n+\n+const char* ShenandoahConcurrentGC::conc_cleanup_event_message() const {\n+  if (ShenandoahHeap::heap()->unload_classes()) {\n+    SHENANDOAH_RETURN_EVENT_MESSAGE(_generation->type(), \"Concurrent cleanup\", \" (unload classes)\");\n+  } else {\n+    SHENANDOAH_RETURN_EVENT_MESSAGE(_generation->type(), \"Concurrent cleanup\", \"\");\n+  }\n+}\n+\n+const char* ShenandoahConcurrentGC::conc_init_update_refs_event_message() const {\n+  if (ShenandoahHeap::heap()->unload_classes()) {\n+    SHENANDOAH_RETURN_EVENT_MESSAGE(_generation->type(), \"Concurrent Init Update Refs\", \" (unload classes)\");\n@@ -1019,1 +1397,1 @@\n-    return \"Concurrent marking\";\n+    SHENANDOAH_RETURN_EVENT_MESSAGE(_generation->type(), \"Concurrent Init Update Refs\", \"\");\n","filename":"src\/hotspot\/share\/gc\/shenandoah\/shenandoahConcurrentGC.cpp","additions":500,"deletions":122,"binary":false,"changes":622,"status":"modified"},{"patch":"@@ -3,0 +3,1 @@\n+ * Copyright Amazon.com Inc. or its affiliates. All Rights Reserved.\n@@ -33,0 +34,1 @@\n+#include \"gc\/shenandoah\/shenandoahGeneration.hpp\"\n@@ -42,0 +44,1 @@\n+#include \"gc\/shenandoah\/shenandoahScanRemembered.inline.hpp\"\n@@ -47,0 +50,1 @@\n+template <ShenandoahGenerationType GENERATION>\n@@ -60,0 +64,1 @@\n+    ShenandoahWorkerTimingsTracker timer(ShenandoahPhaseTimings::conc_mark, ShenandoahPhaseTimings::ParallelMark, worker_id, true);\n@@ -61,1 +66,5 @@\n-    ShenandoahReferenceProcessor* rp = heap->ref_processor();\n+    \/\/ Do not use active_generation() : we must use the gc_generation() set by\n+    \/\/ ShenandoahGCScope on the ControllerThread's stack; no safepoint may\n+    \/\/ intervene to update active_generation, so we can't\n+    \/\/ shenandoah_assert_generations_reconciled() here.\n+    ShenandoahReferenceProcessor* rp = heap->gc_generation()->ref_processor();\n@@ -64,2 +73,1 @@\n-    _cm->mark_loop(worker_id, _terminator, rp,\n-                   true \/*cancellable*\/,\n+    _cm->mark_loop(worker_id, _terminator, rp, GENERATION, true \/*cancellable*\/,\n@@ -74,1 +82,0 @@\n-  OopClosure* const _cl;\n@@ -77,3 +84,2 @@\n-  ShenandoahSATBAndRemarkThreadsClosure(SATBMarkQueueSet& satb_qset, OopClosure* cl) :\n-    _satb_qset(satb_qset),\n-    _cl(cl)  {}\n+  explicit ShenandoahSATBAndRemarkThreadsClosure(SATBMarkQueueSet& satb_qset) :\n+    _satb_qset(satb_qset) {}\n@@ -81,1 +87,1 @@\n-  void do_thread(Thread* thread) {\n+  void do_thread(Thread* thread) override {\n@@ -84,6 +90,0 @@\n-    if (thread->is_Java_thread()) {\n-      if (_cl != nullptr) {\n-        ResourceMark rm;\n-        thread->oops_do(_cl, nullptr);\n-      }\n-    }\n@@ -93,0 +93,1 @@\n+template <ShenandoahGenerationType GENERATION>\n@@ -108,1 +109,0 @@\n-    ShenandoahReferenceProcessor* rp = heap->ref_processor();\n@@ -110,0 +110,2 @@\n+    ShenandoahReferenceProcessor* rp = heap->gc_generation()->ref_processor();\n+    shenandoah_assert_generations_reconciled();\n@@ -114,0 +116,1 @@\n+      ShenandoahObjToScanQueue* old_q = _cm->get_old_queue(worker_id);\n@@ -115,1 +118,1 @@\n-      ShenandoahSATBBufferClosure cl(q);\n+      ShenandoahSATBBufferClosure<GENERATION> cl(q, old_q);\n@@ -120,3 +123,1 @@\n-      ShenandoahMarkRefsClosure             mark_cl(q, rp);\n-      ShenandoahSATBAndRemarkThreadsClosure tc(satb_mq_set,\n-                                               ShenandoahIUBarrier ? &mark_cl : nullptr);\n+      ShenandoahSATBAndRemarkThreadsClosure tc(satb_mq_set);\n@@ -125,2 +126,1 @@\n-    _cm->mark_loop(worker_id, _terminator, rp,\n-                   false \/*not cancellable*\/,\n+    _cm->mark_loop(worker_id, _terminator, rp, GENERATION, false \/*not cancellable*\/,\n@@ -133,2 +133,2 @@\n-ShenandoahConcurrentMark::ShenandoahConcurrentMark() :\n-  ShenandoahMark() {}\n+ShenandoahConcurrentMark::ShenandoahConcurrentMark(ShenandoahGeneration* generation) :\n+  ShenandoahMark(generation) {}\n@@ -137,0 +137,1 @@\n+template <ShenandoahGenerationType GENERATION>\n@@ -142,0 +143,1 @@\n+  ShenandoahObjToScanQueueSet* const  _old_queue_set;\n@@ -146,0 +148,1 @@\n+                                    ShenandoahObjToScanQueueSet* old,\n@@ -152,4 +155,6 @@\n-ShenandoahMarkConcurrentRootsTask::ShenandoahMarkConcurrentRootsTask(ShenandoahObjToScanQueueSet* qs,\n-                                                                     ShenandoahReferenceProcessor* rp,\n-                                                                     ShenandoahPhaseTimings::Phase phase,\n-                                                                     uint nworkers) :\n+template <ShenandoahGenerationType GENERATION>\n+ShenandoahMarkConcurrentRootsTask<GENERATION>::ShenandoahMarkConcurrentRootsTask(ShenandoahObjToScanQueueSet* qs,\n+                                                                                 ShenandoahObjToScanQueueSet* old,\n+                                                                                 ShenandoahReferenceProcessor* rp,\n+                                                                                 ShenandoahPhaseTimings::Phase phase,\n+                                                                                 uint nworkers) :\n@@ -159,0 +164,1 @@\n+  _old_queue_set(old),\n@@ -163,1 +169,2 @@\n-void ShenandoahMarkConcurrentRootsTask::work(uint worker_id) {\n+template <ShenandoahGenerationType GENERATION>\n+void ShenandoahMarkConcurrentRootsTask<GENERATION>::work(uint worker_id) {\n@@ -166,1 +173,3 @@\n-  ShenandoahMarkRefsClosure cl(q, _rp);\n+  ShenandoahObjToScanQueue* old_q = (_old_queue_set == nullptr) ?\n+          nullptr : _old_queue_set->queue(worker_id);\n+  ShenandoahMarkRefsClosure<GENERATION> cl(q, _rp, old_q);\n@@ -174,2 +183,0 @@\n-  TASKQUEUE_STATS_ONLY(task_queues()->reset_taskqueue_stats());\n-\n@@ -177,0 +184,1 @@\n+<<<<<<< HEAD\n@@ -194,0 +202,32 @@\n+=======\n+  ShenandoahReferenceProcessor* rp = _generation->ref_processor();\n+  _generation->reserve_task_queues(workers->active_workers());\n+  switch (_generation->type()) {\n+    case YOUNG: {\n+      ShenandoahMarkConcurrentRootsTask<YOUNG> task(task_queues(), old_task_queues(), rp,\n+                                                    ShenandoahPhaseTimings::conc_mark_roots, workers->active_workers());\n+      workers->run_task(&task);\n+      break;\n+    }\n+    case GLOBAL: {\n+      assert(old_task_queues() == nullptr, \"Global mark should not have old gen mark queues\");\n+      ShenandoahMarkConcurrentRootsTask<GLOBAL> task(task_queues(), nullptr, rp,\n+                                                     ShenandoahPhaseTimings::conc_mark_roots, workers->active_workers());\n+      workers->run_task(&task);\n+      break;\n+    }\n+    case NON_GEN: {\n+      assert(old_task_queues() == nullptr, \"Non-generational mark should not have old gen mark queues\");\n+      ShenandoahMarkConcurrentRootsTask<NON_GEN> task(task_queues(), nullptr, rp,\n+                                                      ShenandoahPhaseTimings::conc_mark_roots, workers->active_workers());\n+      workers->run_task(&task);\n+      break;\n+    }\n+    case OLD: {\n+      \/\/ We use a YOUNG generation cycle to bootstrap concurrent old marking.\n+      ShouldNotReachHere();\n+      break;\n+    }\n+    default:\n+      ShouldNotReachHere();\n+>>>>>>> 82b11cb49a9f6509959a6476d0d1d91c81fcfd02\n@@ -195,1 +235,1 @@\n-};\n+}\n@@ -203,0 +243,1 @@\n+  ShenandoahGenerationType gen_type = _generation->type();\n@@ -206,3 +247,28 @@\n-    TaskTerminator terminator(nworkers, task_queues());\n-    ShenandoahConcurrentMarkingTask task(this, &terminator);\n-    workers->run_task(&task);\n+    switch (gen_type) {\n+      case YOUNG: {\n+        TaskTerminator terminator(nworkers, task_queues());\n+        ShenandoahConcurrentMarkingTask<YOUNG> task(this, &terminator);\n+        workers->run_task(&task);\n+        break;\n+      }\n+      case OLD: {\n+        TaskTerminator terminator(nworkers, task_queues());\n+        ShenandoahConcurrentMarkingTask<OLD> task(this, &terminator);\n+        workers->run_task(&task);\n+        break;\n+      }\n+      case GLOBAL: {\n+        TaskTerminator terminator(nworkers, task_queues());\n+        ShenandoahConcurrentMarkingTask<GLOBAL> task(this, &terminator);\n+        workers->run_task(&task);\n+        break;\n+      }\n+      case NON_GEN: {\n+        TaskTerminator terminator(nworkers, task_queues());\n+        ShenandoahConcurrentMarkingTask<NON_GEN> task(this, &terminator);\n+        workers->run_task(&task);\n+        break;\n+      }\n+      default:\n+        ShouldNotReachHere();\n+    }\n@@ -216,1 +282,4 @@\n-    Handshake::execute(&flush_satb);\n+    {\n+      ShenandoahTimingsTracker t(ShenandoahPhaseTimings::conc_mark_satb_flush, true);\n+      Handshake::execute(&flush_satb);\n+    }\n@@ -232,2 +301,1 @@\n-  TASKQUEUE_STATS_ONLY(task_queues()->print_taskqueue_stats());\n-  TASKQUEUE_STATS_ONLY(task_queues()->reset_taskqueue_stats());\n+  TASKQUEUE_STATS_ONLY(task_queues()->print_and_reset_taskqueue_stats(\"\"));\n@@ -235,3 +303,2 @@\n-  ShenandoahHeap* const heap = ShenandoahHeap::heap();\n-  heap->set_concurrent_mark_in_progress(false);\n-  heap->mark_complete_marking_context();\n+  _generation->set_concurrent_mark_in_progress(false);\n+  _generation->set_mark_complete();\n@@ -257,2 +324,0 @@\n-  ShenandoahFinalMarkingTask task(this, &terminator, ShenandoahStringDedup::is_enabled());\n-  heap->workers()->run_task(&task);\n@@ -260,2 +325,24 @@\n-  assert(task_queues()->is_empty(), \"Should be empty\");\n-}\n+  switch (_generation->type()) {\n+    case YOUNG:{\n+      ShenandoahFinalMarkingTask<YOUNG> task(this, &terminator, ShenandoahStringDedup::is_enabled());\n+      heap->workers()->run_task(&task);\n+      break;\n+    }\n+    case OLD:{\n+      ShenandoahFinalMarkingTask<OLD> task(this, &terminator, ShenandoahStringDedup::is_enabled());\n+      heap->workers()->run_task(&task);\n+      break;\n+    }\n+    case GLOBAL:{\n+      ShenandoahFinalMarkingTask<GLOBAL> task(this, &terminator, ShenandoahStringDedup::is_enabled());\n+      heap->workers()->run_task(&task);\n+      break;\n+    }\n+    case NON_GEN:{\n+      ShenandoahFinalMarkingTask<NON_GEN> task(this, &terminator, ShenandoahStringDedup::is_enabled());\n+      heap->workers()->run_task(&task);\n+      break;\n+    }\n+    default:\n+      ShouldNotReachHere();\n+  }\n@@ -264,4 +351,1 @@\n-void ShenandoahConcurrentMark::cancel() {\n-  clear();\n-  ShenandoahReferenceProcessor* rp = ShenandoahHeap::heap()->ref_processor();\n-  rp->abandon_partial_discovery();\n+  assert(task_queues()->is_empty(), \"Should be empty\");\n","filename":"src\/hotspot\/share\/gc\/shenandoah\/shenandoahConcurrentMark.cpp","additions":133,"deletions":49,"binary":false,"changes":182,"status":"modified"},{"patch":"@@ -2,1 +2,1 @@\n- * Copyright (c) 2023, Oracle and\/or its affiliates. All rights reserved.\n+ * Copyright (c) 2023, 2024, Oracle and\/or its affiliates. All rights reserved.\n@@ -4,0 +4,1 @@\n+ * Copyright Amazon.com Inc. or its affiliates. All Rights Reserved.\n@@ -39,0 +40,3 @@\n+#include \"gc\/shenandoah\/heuristics\/shenandoahOldHeuristics.hpp\"\n+#include \"gc\/shenandoah\/heuristics\/shenandoahYoungHeuristics.hpp\"\n+#include \"gc\/shenandoah\/shenandoahAllocRequest.hpp\"\n@@ -47,0 +51,3 @@\n+#include \"gc\/shenandoah\/shenandoahGenerationalEvacuationTask.hpp\"\n+#include \"gc\/shenandoah\/shenandoahGenerationalHeap.hpp\"\n+#include \"gc\/shenandoah\/shenandoahGlobalGeneration.hpp\"\n@@ -49,0 +56,1 @@\n+#include \"gc\/shenandoah\/shenandoahHeapRegionClosures.hpp\"\n@@ -54,1 +62,0 @@\n-#include \"gc\/shenandoah\/shenandoahMetrics.hpp\"\n@@ -56,0 +63,1 @@\n+#include \"gc\/shenandoah\/shenandoahOldGeneration.hpp\"\n@@ -62,0 +70,1 @@\n+#include \"gc\/shenandoah\/shenandoahScanRemembered.inline.hpp\"\n@@ -63,0 +72,1 @@\n+#include \"gc\/shenandoah\/shenandoahUncommitThread.hpp\"\n@@ -69,1 +79,2 @@\n-#include \"gc\/shenandoah\/mode\/shenandoahIUMode.hpp\"\n+#include \"gc\/shenandoah\/shenandoahYoungGeneration.hpp\"\n+#include \"gc\/shenandoah\/mode\/shenandoahGenerationalMode.hpp\"\n@@ -72,0 +83,2 @@\n+#include \"utilities\/globalDefinitions.hpp\"\n+\n@@ -88,0 +101,1 @@\n+#include \"runtime\/threads.hpp\"\n@@ -163,3 +177,0 @@\n-  \/\/ Now we know the number of regions, initialize the heuristics.\n-  initialize_heuristics();\n-\n@@ -196,0 +207,3 @@\n+  os::trace_page_sizes_for_requested_size(\"Heap\",\n+                                          max_byte_size, heap_rs.page_size(), heap_alignment,\n+                                          heap_rs.base(), heap_rs.size());\n@@ -215,0 +229,22 @@\n+  BarrierSet::set_barrier_set(new ShenandoahBarrierSet(this, _heap_region));\n+\n+  \/\/ Now we know the number of regions and heap sizes, initialize the heuristics.\n+  initialize_heuristics();\n+\n+  assert(_heap_region.byte_size() == heap_rs.size(), \"Need to know reserved size for card table\");\n+\n+  \/\/\n+  \/\/ Worker threads must be initialized after the barrier is configured\n+  \/\/\n+  _workers = new ShenandoahWorkerThreads(\"Shenandoah GC Threads\", _max_workers);\n+  if (_workers == nullptr) {\n+    vm_exit_during_initialization(\"Failed necessary allocation.\");\n+  } else {\n+    _workers->initialize_workers();\n+  }\n+\n+  if (ParallelGCThreads > 1) {\n+    _safepoint_workers = new ShenandoahWorkerThreads(\"Safepoint Cleanup Thread\", ParallelGCThreads);\n+    _safepoint_workers->initialize_workers();\n+  }\n+\n@@ -219,2 +255,2 @@\n-  _bitmap_size = ShenandoahMarkBitMap::compute_size(heap_rs.size());\n-  _bitmap_size = align_up(_bitmap_size, bitmap_page_size);\n+  size_t bitmap_size_orig = ShenandoahMarkBitMap::compute_size(heap_rs.size());\n+  _bitmap_size = align_up(bitmap_size_orig, bitmap_page_size);\n@@ -246,0 +282,4 @@\n+  os::trace_page_sizes_for_requested_size(\"Mark Bitmap\",\n+                                          bitmap_size_orig, bitmap.page_size(), bitmap_page_size,\n+                                          bitmap.base(),\n+                                          bitmap.size());\n@@ -251,1 +291,1 @@\n-                              align_up(num_committed_regions, _bitmap_regions_per_slice) \/ _bitmap_regions_per_slice;\n+    align_up(num_committed_regions, _bitmap_regions_per_slice) \/ _bitmap_regions_per_slice;\n@@ -258,1 +298,1 @@\n-  _marking_context = new ShenandoahMarkingContext(_heap_region, _bitmap_region, _num_regions, _max_workers);\n+  _marking_context = new ShenandoahMarkingContext(_heap_region, _bitmap_region, _num_regions);\n@@ -262,0 +302,4 @@\n+    os::trace_page_sizes_for_requested_size(\"Verify Bitmap\",\n+                                            bitmap_size_orig, verify_bitmap.page_size(), bitmap_page_size,\n+                                            verify_bitmap.base(),\n+                                            verify_bitmap.size());\n@@ -273,1 +317,6 @@\n-  ReservedSpace aux_bitmap(_bitmap_size, bitmap_page_size);\n+  size_t aux_bitmap_page_size = bitmap_page_size;\n+\n+  ReservedSpace aux_bitmap(_bitmap_size, aux_bitmap_page_size);\n+  os::trace_page_sizes_for_requested_size(\"Aux Bitmap\",\n+                                          bitmap_size_orig, aux_bitmap.page_size(), aux_bitmap_page_size,\n+                                          aux_bitmap.base(), aux_bitmap.size());\n@@ -283,2 +332,3 @@\n-  size_t region_storage_size = align_up(region_align * _num_regions, region_page_size);\n-  region_storage_size = align_up(region_storage_size, os::vm_allocation_granularity());\n+  size_t region_storage_size_orig = region_align * _num_regions;\n+  size_t region_storage_size = align_up(region_storage_size_orig,\n+                                        MAX2(region_page_size, os::vm_allocation_granularity()));\n@@ -287,0 +337,3 @@\n+  os::trace_page_sizes_for_requested_size(\"Region Storage\",\n+                                          region_storage_size_orig, region_storage.page_size(), region_page_size,\n+                                          region_storage.base(), region_storage.size());\n@@ -297,2 +350,3 @@\n-    size_t cset_align = MAX2<size_t>(os::vm_page_size(), os::vm_allocation_granularity());\n-    size_t cset_size = align_up(((size_t) sh_rs.base() + sh_rs.size()) >> ShenandoahHeapRegion::region_size_bytes_shift(), cset_align);\n+    const size_t cset_align = MAX2<size_t>(os::vm_page_size(), os::vm_allocation_granularity());\n+    const size_t cset_size = align_up(((size_t) sh_rs.base() + sh_rs.size()) >> ShenandoahHeapRegion::region_size_bytes_shift(), cset_align);\n+    const size_t cset_page_size = os::vm_page_size();\n@@ -302,0 +356,1 @@\n+    ReservedSpace cset_rs;\n@@ -306,1 +361,1 @@\n-      ReservedSpace cset_rs(cset_size, cset_align, os::vm_page_size(), req_addr);\n+      cset_rs = ReservedSpace(cset_size, cset_align, cset_page_size, req_addr);\n@@ -315,1 +370,1 @@\n-      ReservedSpace cset_rs(cset_size, cset_align, os::vm_page_size());\n+      cset_rs = ReservedSpace(cset_size, cset_align, os::vm_page_size());\n@@ -318,0 +373,4 @@\n+    os::trace_page_sizes_for_requested_size(\"Collection Set\",\n+                                            cset_size, cset_rs.page_size(), cset_page_size,\n+                                            cset_rs.base(),\n+                                            cset_rs.size());\n@@ -321,0 +380,1 @@\n+  _affiliations = NEW_C_HEAP_ARRAY(uint8_t, _num_regions, mtGC);\n@@ -337,0 +397,2 @@\n+\n+      _affiliations[i] = ShenandoahAffiliation::FREE;\n@@ -339,2 +401,1 @@\n-    \/\/ Initialize to complete\n-    _marking_context->mark_complete();\n+    size_t young_cset_regions, old_cset_regions;\n@@ -342,1 +403,4 @@\n-    _free_set->rebuild();\n+    \/\/ We are initializing free set.  We ignore cset region tallies.\n+    size_t first_old, last_old, num_old;\n+    _free_set->prepare_to_rebuild(young_cset_regions, old_cset_regions, first_old, last_old, num_old);\n+    _free_set->finish_rebuild(young_cset_regions, old_cset_regions, num_old);\n@@ -354,10 +418,0 @@\n-#ifdef LINUX\n-    \/\/ UseTransparentHugePages would madvise that backing memory can be coalesced into huge\n-    \/\/ pages. But, the kernel needs to know that every small page is used, in order to coalesce\n-    \/\/ them into huge one. Therefore, we need to pretouch with smaller pages.\n-    if (UseTransparentHugePages) {\n-      _pretouch_heap_page_size = (size_t)os::vm_page_size();\n-      _pretouch_bitmap_page_size = (size_t)os::vm_page_size();\n-    }\n-#endif\n-\n@@ -399,2 +453,0 @@\n-  } else {\n-    _pacer = nullptr;\n@@ -403,1 +455,1 @@\n-  _control_thread = new ShenandoahControlThread();\n+  initialize_controller();\n@@ -405,1 +457,5 @@\n-  ShenandoahInitLogger::print();\n+  if (ShenandoahUncommit) {\n+    _uncommit_thread = new ShenandoahUncommitThread(this);\n+  }\n+\n+  print_init_logger();\n@@ -410,0 +466,8 @@\n+void ShenandoahHeap::initialize_controller() {\n+  _control_thread = new ShenandoahControlThread();\n+}\n+\n+void ShenandoahHeap::print_init_logger() const {\n+  ShenandoahInitLogger::print();\n+}\n+\n@@ -414,2 +478,0 @@\n-    } else if (strcmp(ShenandoahGCMode, \"iu\") == 0) {\n-      _gc_mode = new ShenandoahIUMode();\n@@ -418,0 +480,2 @@\n+    } else if (strcmp(ShenandoahGCMode, \"generational\") == 0) {\n+      _gc_mode = new ShenandoahGenerationalMode();\n@@ -438,13 +502,2 @@\n-  assert(_gc_mode != nullptr, \"Must be initialized\");\n-  _heuristics = _gc_mode->initialize_heuristics();\n-\n-  if (_heuristics->is_diagnostic() && !UnlockDiagnosticVMOptions) {\n-    vm_exit_during_initialization(\n-            err_msg(\"Heuristics \\\"%s\\\" is diagnostic, and must be enabled via -XX:+UnlockDiagnosticVMOptions.\",\n-                    _heuristics->name()));\n-  }\n-  if (_heuristics->is_experimental() && !UnlockExperimentalVMOptions) {\n-    vm_exit_during_initialization(\n-            err_msg(\"Heuristics \\\"%s\\\" is experimental, and must be enabled via -XX:+UnlockExperimentalVMOptions.\",\n-                    _heuristics->name()));\n-  }\n+  _global_generation = new ShenandoahGlobalGeneration(mode()->is_generational(), max_workers(), max_capacity(), max_capacity());\n+  _global_generation->initialize_heuristics(mode());\n@@ -460,0 +513,2 @@\n+  _gc_generation(nullptr),\n+  _active_generation(nullptr),\n@@ -461,1 +516,0 @@\n-  _used(0),\n@@ -463,2 +517,1 @@\n-  _bytes_allocated_since_gc_start(0),\n-  _max_workers(MAX2(ConcGCThreads, ParallelGCThreads)),\n+  _max_workers(MAX3(ConcGCThreads, ParallelGCThreads, 1U)),\n@@ -470,1 +523,1 @@\n-  _update_refs_iterator(this),\n+  _affiliations(nullptr),\n@@ -472,0 +525,4 @@\n+  _gc_no_progress_count(0),\n+  _cancel_requested_time(0),\n+  _update_refs_iterator(this),\n+  _global_generation(nullptr),\n@@ -473,0 +530,3 @@\n+  _uncommit_thread(nullptr),\n+  _young_generation(nullptr),\n+  _old_generation(nullptr),\n@@ -475,1 +535,0 @@\n-  _heuristics(nullptr),\n@@ -487,1 +546,0 @@\n-  _ref_processor(new ShenandoahReferenceProcessor(MAX2(_max_workers, 1U))),\n@@ -497,1 +555,1 @@\n-  \/\/ Initialize GC mode early, so we can adjust barrier support\n+  \/\/ Initialize GC mode early, many subsequent initialization procedures depend on it\n@@ -499,15 +557,1 @@\n-  BarrierSet::set_barrier_set(new ShenandoahBarrierSet(this));\n-\n-  _max_workers = MAX2(_max_workers, 1U);\n-  _workers = new ShenandoahWorkerThreads(\"Shenandoah GC Threads\", _max_workers);\n-  if (_workers == nullptr) {\n-    vm_exit_during_initialization(\"Failed necessary allocation.\");\n-  } else {\n-    _workers->initialize_workers();\n-  }\n-\n-  if (ParallelGCThreads > 1) {\n-    _safepoint_workers = new ShenandoahWorkerThreads(\"Safepoint Cleanup Thread\",\n-                                                ParallelGCThreads);\n-    _safepoint_workers->initialize_workers();\n-  }\n+  _cancelled_gc.set(GCCause::_no_gc);\n@@ -520,29 +564,0 @@\n-class ShenandoahResetBitmapTask : public WorkerTask {\n-private:\n-  ShenandoahRegionIterator _regions;\n-\n-public:\n-  ShenandoahResetBitmapTask() :\n-    WorkerTask(\"Shenandoah Reset Bitmap\") {}\n-\n-  void work(uint worker_id) {\n-    ShenandoahHeapRegion* region = _regions.next();\n-    ShenandoahHeap* heap = ShenandoahHeap::heap();\n-    ShenandoahMarkingContext* const ctx = heap->marking_context();\n-    while (region != nullptr) {\n-      if (heap->is_bitmap_slice_committed(region)) {\n-        ctx->clear_bitmap(region);\n-      }\n-      region = _regions.next();\n-    }\n-  }\n-};\n-\n-void ShenandoahHeap::reset_mark_bitmap() {\n-  assert_gc_workers(_workers->active_workers());\n-  mark_incomplete_marking_context();\n-\n-  ShenandoahResetBitmapTask task;\n-  _workers->run_task(&task);\n-}\n-\n@@ -563,1 +578,6 @@\n-  if (is_concurrent_mark_in_progress())        st->print(\"marking, \");\n+  if (!mode()->is_generational()) {\n+    if (is_concurrent_mark_in_progress())      st->print(\"marking,\");\n+  } else {\n+    if (is_concurrent_old_mark_in_progress())    st->print(\"old marking, \");\n+    if (is_concurrent_young_mark_in_progress())  st->print(\"young marking, \");\n+  }\n@@ -607,1 +627,0 @@\n-    assert(thread->is_Worker_thread(), \"Only worker thread expected\");\n@@ -614,0 +633,4 @@\n+\n+  \/\/ Schedule periodic task to report on gc thread CPU utilization\n+  _mmu_tracker.initialize();\n+\n@@ -622,0 +645,3 @@\n+\n+  \/\/ Note that the safepoint workers may require gclabs if the threads are used to create a heap dump\n+  \/\/ during a concurrent evacuation phase.\n@@ -627,2 +653,0 @@\n-  _heuristics->initialize();\n-\n@@ -632,0 +656,4 @@\n+ShenandoahHeuristics* ShenandoahHeap::heuristics() {\n+  return _global_generation->heuristics();\n+}\n+\n@@ -633,1 +661,1 @@\n-  return Atomic::load(&_used);\n+  return global_generation()->used();\n@@ -650,2 +678,43 @@\n-void ShenandoahHeap::increase_used(size_t bytes) {\n-  Atomic::add(&_used, bytes, memory_order_relaxed);\n+\/\/ For tracking usage based on allocations, it should be the case that:\n+\/\/ * The sum of regions::used == heap::used\n+\/\/ * The sum of a generation's regions::used == generation::used\n+\/\/ * The sum of a generation's humongous regions::free == generation::humongous_waste\n+\/\/ These invariants are checked by the verifier on GC safepoints.\n+\/\/\n+\/\/ Additional notes:\n+\/\/ * When a mutator's allocation request causes a region to be retired, the\n+\/\/   free memory left in that region is considered waste. It does not contribute\n+\/\/   to the usage, but it _does_ contribute to allocation rate.\n+\/\/ * The bottom of a PLAB must be aligned on card size. In some cases this will\n+\/\/   require padding in front of the PLAB (a filler object). Because this padding\n+\/\/   is included in the region's used memory we include the padding in the usage\n+\/\/   accounting as waste.\n+\/\/ * Mutator allocations are used to compute an allocation rate. They are also\n+\/\/   sent to the Pacer for those purposes.\n+\/\/ * There are three sources of waste:\n+\/\/  1. The padding used to align a PLAB on card size\n+\/\/  2. Region's free is less than minimum TLAB size and is retired\n+\/\/  3. The unused portion of memory in the last region of a humongous object\n+void ShenandoahHeap::increase_used(const ShenandoahAllocRequest& req) {\n+  size_t actual_bytes = req.actual_size() * HeapWordSize;\n+  size_t wasted_bytes = req.waste() * HeapWordSize;\n+  ShenandoahGeneration* generation = generation_for(req.affiliation());\n+\n+  if (req.is_gc_alloc()) {\n+    assert(wasted_bytes == 0 || req.type() == ShenandoahAllocRequest::_alloc_plab, \"Only PLABs have waste\");\n+    increase_used(generation, actual_bytes + wasted_bytes);\n+  } else {\n+    assert(req.is_mutator_alloc(), \"Expected mutator alloc here\");\n+    \/\/ padding and actual size both count towards allocation counter\n+    generation->increase_allocated(actual_bytes + wasted_bytes);\n+\n+    \/\/ only actual size counts toward usage for mutator allocations\n+    increase_used(generation, actual_bytes);\n+\n+    \/\/ notify pacer of both actual size and waste\n+    notify_mutator_alloc_words(req.actual_size(), req.waste());\n+\n+    if (wasted_bytes > 0 && ShenandoahHeapRegion::requires_humongous(req.actual_size())) {\n+      increase_humongous_waste(generation,wasted_bytes);\n+    }\n+  }\n@@ -654,2 +723,5 @@\n-void ShenandoahHeap::set_used(size_t bytes) {\n-  Atomic::store(&_used, bytes);\n+void ShenandoahHeap::increase_humongous_waste(ShenandoahGeneration* generation, size_t bytes) {\n+  generation->increase_humongous_waste(bytes);\n+  if (!generation->is_global()) {\n+    global_generation()->increase_humongous_waste(bytes);\n+  }\n@@ -658,3 +730,5 @@\n-void ShenandoahHeap::decrease_used(size_t bytes) {\n-  assert(used() >= bytes, \"never decrease heap size by more than we've left\");\n-  Atomic::sub(&_used, bytes, memory_order_relaxed);\n+void ShenandoahHeap::decrease_humongous_waste(ShenandoahGeneration* generation, size_t bytes) {\n+  generation->decrease_humongous_waste(bytes);\n+  if (!generation->is_global()) {\n+    global_generation()->decrease_humongous_waste(bytes);\n+  }\n@@ -663,2 +737,5 @@\n-void ShenandoahHeap::increase_allocated(size_t bytes) {\n-  Atomic::add(&_bytes_allocated_since_gc_start, bytes, memory_order_relaxed);\n+void ShenandoahHeap::increase_used(ShenandoahGeneration* generation, size_t bytes) {\n+  generation->increase_used(bytes);\n+  if (!generation->is_global()) {\n+    global_generation()->increase_used(bytes);\n+  }\n@@ -667,4 +744,4 @@\n-void ShenandoahHeap::notify_mutator_alloc_words(size_t words, bool waste) {\n-  size_t bytes = words * HeapWordSize;\n-  if (!waste) {\n-    increase_used(bytes);\n+void ShenandoahHeap::decrease_used(ShenandoahGeneration* generation, size_t bytes) {\n+  generation->decrease_used(bytes);\n+  if (!generation->is_global()) {\n+    global_generation()->decrease_used(bytes);\n@@ -672,1 +749,3 @@\n-  increase_allocated(bytes);\n+}\n+\n+void ShenandoahHeap::notify_mutator_alloc_words(size_t words, size_t waste) {\n@@ -675,2 +754,2 @@\n-    if (waste) {\n-      pacer()->claim_for_alloc(words, true);\n+    if (waste > 0) {\n+      pacer()->claim_for_alloc<true>(waste);\n@@ -713,0 +792,1 @@\n+<<<<<<< HEAD\n@@ -725,1 +805,10 @@\n-}\n+=======\n+  if (!is_in_reserved(p)) {\n+    return false;\n+  }\n+\n+  if (is_full_gc_move_in_progress()) {\n+    \/\/ Full GC move is running, we do not have a consistent region\n+    \/\/ information yet. But we know the pointer is in heap.\n+    return true;\n+  }\n@@ -727,2 +816,5 @@\n-void ShenandoahHeap::op_uncommit(double shrink_before, size_t shrink_until) {\n-  assert (ShenandoahUncommit, \"should be enabled\");\n+  \/\/ Now check if we point to a live section in active region.\n+  const ShenandoahHeapRegion* r = heap_region_containing(p);\n+  if (p >= r->top()) {\n+    return false;\n+  }\n@@ -730,4 +822,3 @@\n-  \/\/ Application allocates from the beginning of the heap, and GC allocates at\n-  \/\/ the end of it. It is more efficient to uncommit from the end, so that applications\n-  \/\/ could enjoy the near committed regions. GC allocations are much less frequent,\n-  \/\/ and therefore can accept the committing costs.\n+  if (r->is_active()) {\n+    return true;\n+  }\n@@ -735,9 +826,7 @@\n-  size_t count = 0;\n-  for (size_t i = num_regions(); i > 0; i--) { \/\/ care about size_t underflow\n-    ShenandoahHeapRegion* r = get_region(i - 1);\n-    if (r->is_empty_committed() && (r->empty_time() < shrink_before)) {\n-      ShenandoahHeapLocker locker(lock());\n-      if (r->is_empty_committed()) {\n-        if (committed() < shrink_until + ShenandoahHeapRegion::region_size_bytes()) {\n-          break;\n-        }\n+  \/\/ The region is trash, but won't be recycled until after concurrent weak\n+  \/\/ roots. We also don't allow mutators to allocate from trash regions\n+  \/\/ during weak roots. Concurrent class unloading may access unmarked oops\n+  \/\/ in trash regions.\n+  return r->is_trash() && is_concurrent_weak_root_in_progress();\n+>>>>>>> 82b11cb49a9f6509959a6476d0d1d91c81fcfd02\n+}\n@@ -745,5 +834,9 @@\n-        r->make_uncommitted();\n-        count++;\n-      }\n-    }\n-    SpinPause(); \/\/ allow allocators to take the lock\n+void ShenandoahHeap::notify_soft_max_changed() {\n+  if (_uncommit_thread != nullptr) {\n+    _uncommit_thread->notify_soft_max_changed();\n+  }\n+}\n+\n+void ShenandoahHeap::notify_explicit_gc_requested() {\n+  if (_uncommit_thread != nullptr) {\n+    _uncommit_thread->notify_explicit_gc_requested();\n@@ -751,0 +844,1 @@\n+}\n@@ -752,2 +846,14 @@\n-  if (count > 0) {\n-    control_thread()->notify_heap_changed();\n+bool ShenandoahHeap::check_soft_max_changed() {\n+  size_t new_soft_max = Atomic::load(&SoftMaxHeapSize);\n+  size_t old_soft_max = soft_max_capacity();\n+  if (new_soft_max != old_soft_max) {\n+    new_soft_max = MAX2(min_capacity(), new_soft_max);\n+    new_soft_max = MIN2(max_capacity(), new_soft_max);\n+    if (new_soft_max != old_soft_max) {\n+      log_info(gc)(\"Soft Max Heap Size: \" SIZE_FORMAT \"%s -> \" SIZE_FORMAT \"%s\",\n+                   byte_size_in_proper_unit(old_soft_max), proper_unit_for_byte_size(old_soft_max),\n+                   byte_size_in_proper_unit(new_soft_max), proper_unit_for_byte_size(new_soft_max)\n+      );\n+      set_soft_max_capacity(new_soft_max);\n+      return true;\n+    }\n@@ -755,0 +861,16 @@\n+  return false;\n+}\n+\n+void ShenandoahHeap::notify_heap_changed() {\n+  \/\/ Update monitoring counters when we took a new region. This amortizes the\n+  \/\/ update costs on slow path.\n+  monitoring_support()->notify_heap_changed();\n+  _heap_changed.try_set();\n+}\n+\n+void ShenandoahHeap::set_forced_counters_update(bool value) {\n+  monitoring_support()->set_forced_counters_update(value);\n+}\n+\n+void ShenandoahHeap::handle_force_counters_update() {\n+  monitoring_support()->handle_force_counters_update();\n@@ -763,0 +885,1 @@\n+\n@@ -769,0 +892,1 @@\n+  log_debug(gc, free)(\"Set new GCLAB size: \" SIZE_FORMAT, new_size);\n@@ -774,0 +898,1 @@\n+    log_debug(gc, free)(\"New gclab size (\" SIZE_FORMAT \") is too small for \" SIZE_FORMAT, new_size, size);\n@@ -803,0 +928,1 @@\n+\/\/ Called from stubs in JIT code or interpreter\n@@ -844,1 +970,1 @@\n-    \/\/ Allocation failed, block until control thread reacted, then retry allocation.\n+    \/\/ Check that gc overhead is not exceeded.\n@@ -846,10 +972,40 @@\n-    \/\/ It might happen that one of the threads requesting allocation would unblock\n-    \/\/ way later after GC happened, only to fail the second allocation, because\n-    \/\/ other threads have already depleted the free storage. In this case, a better\n-    \/\/ strategy is to try again, as long as GC makes progress (or until at least\n-    \/\/ one full GC has completed).\n-    size_t original_count = shenandoah_policy()->full_gc_count();\n-    while (result == nullptr\n-        && (_progress_last_gc.is_set() || original_count == shenandoah_policy()->full_gc_count())) {\n-      control_thread()->handle_alloc_failure(req);\n-      result = allocate_memory_under_lock(req, in_new_region);\n+    \/\/ Shenandoah will grind along for quite a while allocating one\n+    \/\/ object at a time using shared (non-tlab) allocations. This check\n+    \/\/ is testing that the GC overhead limit has not been exceeded.\n+    \/\/ This will notify the collector to start a cycle, but will raise\n+    \/\/ an OOME to the mutator if the last Full GCs have not made progress.\n+    \/\/ gc_no_progress_count is incremented following each degen or full GC that fails to achieve is_good_progress().\n+    if ((result == nullptr) && !req.is_lab_alloc() && (get_gc_no_progress_count() > ShenandoahNoProgressThreshold)) {\n+      control_thread()->handle_alloc_failure(req, false);\n+      req.set_actual_size(0);\n+      return nullptr;\n+    }\n+\n+    if (result == nullptr) {\n+      \/\/ Block until control thread reacted, then retry allocation.\n+      \/\/\n+      \/\/ It might happen that one of the threads requesting allocation would unblock\n+      \/\/ way later after GC happened, only to fail the second allocation, because\n+      \/\/ other threads have already depleted the free storage. In this case, a better\n+      \/\/ strategy is to try again, until at least one full GC has completed.\n+      \/\/\n+      \/\/ Stop retrying and return nullptr to cause OOMError exception if our allocation failed even after:\n+      \/\/   a) We experienced a GC that had good progress, or\n+      \/\/   b) We experienced at least one Full GC (whether or not it had good progress)\n+\n+      const size_t original_count = shenandoah_policy()->full_gc_count();\n+      while (result == nullptr && should_retry_allocation(original_count)) {\n+        control_thread()->handle_alloc_failure(req, true);\n+        result = allocate_memory_under_lock(req, in_new_region);\n+      }\n+      if (result != nullptr) {\n+        \/\/ If our allocation request has been satisfied after it initially failed, we count this as good gc progress\n+        notify_gc_progress();\n+      }\n+      if (log_develop_is_enabled(Debug, gc, alloc)) {\n+        ResourceMark rm;\n+        log_debug(gc, alloc)(\"Thread: %s, Result: \" PTR_FORMAT \", Request: %s, Size: \" SIZE_FORMAT\n+                             \", Original: \" SIZE_FORMAT \", Latest: \" SIZE_FORMAT,\n+                             Thread::current()->name(), p2i(result), req.type_string(), req.size(),\n+                             original_count, get_gc_no_progress_count());\n+      }\n@@ -865,1 +1021,5 @@\n-    control_thread()->notify_heap_changed();\n+    notify_heap_changed();\n+  }\n+\n+  if (result == nullptr) {\n+    req.set_actual_size(0);\n@@ -868,0 +1028,4 @@\n+  \/\/ This is called regardless of the outcome of the allocation to account\n+  \/\/ for any waste created by retiring regions with this request.\n+  increase_used(req);\n+\n@@ -877,2 +1041,0 @@\n-      notify_mutator_alloc_words(actual, false);\n-\n@@ -885,2 +1047,0 @@\n-    } else {\n-      increase_used(actual*HeapWordSize);\n@@ -893,0 +1053,5 @@\n+inline bool ShenandoahHeap::should_retry_allocation(size_t original_full_gc_count) const {\n+  return shenandoah_policy()->full_gc_count() == original_full_gc_count\n+      && !shenandoah_policy()->is_at_shutdown();\n+}\n+\n@@ -899,1 +1064,37 @@\n-  return _free_set->allocate(req, in_new_region);\n+\n+  \/\/ Make sure the old generation has room for either evacuations or promotions before trying to allocate.\n+  if (req.is_old() && !old_generation()->can_allocate(req)) {\n+    return nullptr;\n+  }\n+\n+  \/\/ If TLAB request size is greater than available, allocate() will attempt to downsize request to fit within available\n+  \/\/ memory.\n+  HeapWord* result = _free_set->allocate(req, in_new_region);\n+\n+  \/\/ Record the plab configuration for this result and register the object.\n+  if (result != nullptr && req.is_old()) {\n+    old_generation()->configure_plab_for_current_thread(req);\n+    if (req.type() == ShenandoahAllocRequest::_alloc_shared_gc) {\n+      \/\/ Register the newly allocated object while we're holding the global lock since there's no synchronization\n+      \/\/ built in to the implementation of register_object().  There are potential races when multiple independent\n+      \/\/ threads are allocating objects, some of which might span the same card region.  For example, consider\n+      \/\/ a card table's memory region within which three objects are being allocated by three different threads:\n+      \/\/\n+      \/\/ objects being \"concurrently\" allocated:\n+      \/\/    [-----a------][-----b-----][--------------c------------------]\n+      \/\/            [---- card table memory range --------------]\n+      \/\/\n+      \/\/ Before any objects are allocated, this card's memory range holds no objects.  Note that allocation of object a\n+      \/\/ wants to set the starts-object, first-start, and last-start attributes of the preceding card region.\n+      \/\/ Allocation of object b wants to set the starts-object, first-start, and last-start attributes of this card region.\n+      \/\/ Allocation of object c also wants to set the starts-object, first-start, and last-start attributes of this\n+      \/\/ card region.\n+      \/\/\n+      \/\/ The thread allocating b and the thread allocating c can \"race\" in various ways, resulting in confusion, such as\n+      \/\/ last-start representing object b while first-start represents object c.  This is why we need to require all\n+      \/\/ register_object() invocations to be \"mutually exclusive\" with respect to each card's memory range.\n+      old_generation()->card_scan()->register_object(result);\n+    }\n+  }\n+\n+  return result;\n@@ -914,2 +1115,2 @@\n-  if (heuristics()->can_unload_classes()) {\n-    ShenandoahHeuristics* h = heuristics();\n+  ShenandoahHeuristics* h = global_generation()->heuristics();\n+  if (h->can_unload_classes()) {\n@@ -1007,0 +1208,58 @@\n+class ShenandoahRetireGCLABClosure : public ThreadClosure {\n+private:\n+  bool const _resize;\n+public:\n+  explicit ShenandoahRetireGCLABClosure(bool resize) : _resize(resize) {}\n+  void do_thread(Thread* thread) override {\n+    PLAB* gclab = ShenandoahThreadLocalData::gclab(thread);\n+    assert(gclab != nullptr, \"GCLAB should be initialized for %s\", thread->name());\n+    gclab->retire();\n+    if (_resize && ShenandoahThreadLocalData::gclab_size(thread) > 0) {\n+      ShenandoahThreadLocalData::set_gclab_size(thread, 0);\n+    }\n+\n+    if (ShenandoahHeap::heap()->mode()->is_generational()) {\n+      PLAB* plab = ShenandoahThreadLocalData::plab(thread);\n+      assert(plab != nullptr, \"PLAB should be initialized for %s\", thread->name());\n+\n+      \/\/ There are two reasons to retire all plabs between old-gen evacuation passes.\n+      \/\/  1. We need to make the plab memory parsable by remembered-set scanning.\n+      \/\/  2. We need to establish a trustworthy UpdateWaterMark value within each old-gen heap region\n+      ShenandoahGenerationalHeap::heap()->retire_plab(plab, thread);\n+      if (_resize && ShenandoahThreadLocalData::plab_size(thread) > 0) {\n+        ShenandoahThreadLocalData::set_plab_size(thread, 0);\n+      }\n+    }\n+  }\n+};\n+\n+class ShenandoahGCStatePropagator : public HandshakeClosure {\n+public:\n+  explicit ShenandoahGCStatePropagator(char gc_state) :\n+    HandshakeClosure(\"Shenandoah GC State Change\"),\n+    _gc_state(gc_state) {}\n+\n+  void do_thread(Thread* thread) override {\n+    ShenandoahThreadLocalData::set_gc_state(thread, _gc_state);\n+  }\n+private:\n+  char _gc_state;\n+};\n+\n+class ShenandoahPrepareForUpdateRefs : public HandshakeClosure {\n+public:\n+  explicit ShenandoahPrepareForUpdateRefs(char gc_state) :\n+    HandshakeClosure(\"Shenandoah Prepare for Update Refs\"),\n+    _retire(ResizeTLAB), _propagator(gc_state) {}\n+\n+  void do_thread(Thread* thread) override {\n+    _propagator.do_thread(thread);\n+    if (ShenandoahThreadLocalData::gclab(thread) != nullptr) {\n+      _retire.do_thread(thread);\n+    }\n+  }\n+private:\n+  ShenandoahRetireGCLABClosure _retire;\n+  ShenandoahGCStatePropagator _propagator;\n+};\n+\n@@ -1012,0 +1271,145 @@\n+void ShenandoahHeap::concurrent_prepare_for_update_refs() {\n+  {\n+    \/\/ Java threads take this lock while they are being attached and added to the list of thread.\n+    \/\/ If another thread holds this lock before we update the gc state, it will receive a stale\n+    \/\/ gc state, but they will have been added to the list of java threads and so will be corrected\n+    \/\/ by the following handshake.\n+    MutexLocker lock(Threads_lock);\n+\n+    \/\/ A cancellation at this point means the degenerated cycle must resume from update-refs.\n+    set_gc_state_concurrent(EVACUATION, false);\n+    set_gc_state_concurrent(WEAK_ROOTS, false);\n+    set_gc_state_concurrent(UPDATE_REFS, true);\n+  }\n+\n+  \/\/ This will propagate the gc state and retire gclabs and plabs for threads that require it.\n+  ShenandoahPrepareForUpdateRefs prepare_for_update_refs(_gc_state.raw_value());\n+\n+  \/\/ The handshake won't touch worker threads (or control thread, or VM thread), so do those separately.\n+  Threads::non_java_threads_do(&prepare_for_update_refs);\n+\n+  \/\/ Now retire gclabs and plabs and propagate gc_state for mutator threads\n+  Handshake::execute(&prepare_for_update_refs);\n+\n+  _update_refs_iterator.reset();\n+}\n+\n+class ShenandoahCompositeHandshakeClosure : public HandshakeClosure {\n+  HandshakeClosure* _handshake_1;\n+  HandshakeClosure* _handshake_2;\n+  public:\n+    ShenandoahCompositeHandshakeClosure(HandshakeClosure* handshake_1, HandshakeClosure* handshake_2) :\n+      HandshakeClosure(handshake_2->name()),\n+      _handshake_1(handshake_1), _handshake_2(handshake_2) {}\n+\n+  void do_thread(Thread* thread) override {\n+      _handshake_1->do_thread(thread);\n+      _handshake_2->do_thread(thread);\n+    }\n+};\n+\n+void ShenandoahHeap::concurrent_final_roots(HandshakeClosure* handshake_closure) {\n+  {\n+    assert(!is_evacuation_in_progress(), \"Should not evacuate for abbreviated or old cycles\");\n+    MutexLocker lock(Threads_lock);\n+    set_gc_state_concurrent(WEAK_ROOTS, false);\n+  }\n+\n+  ShenandoahGCStatePropagator propagator(_gc_state.raw_value());\n+  Threads::non_java_threads_do(&propagator);\n+  if (handshake_closure == nullptr) {\n+    Handshake::execute(&propagator);\n+  } else {\n+    ShenandoahCompositeHandshakeClosure composite(&propagator, handshake_closure);\n+    Handshake::execute(&composite);\n+  }\n+}\n+\n+oop ShenandoahHeap::evacuate_object(oop p, Thread* thread) {\n+  assert(thread == Thread::current(), \"Expected thread parameter to be current thread.\");\n+  if (ShenandoahThreadLocalData::is_oom_during_evac(thread)) {\n+    \/\/ This thread went through the OOM during evac protocol. It is safe to return\n+    \/\/ the forward pointer. It must not attempt to evacuate any other objects.\n+    return ShenandoahBarrierSet::resolve_forwarded(p);\n+  }\n+\n+  assert(ShenandoahThreadLocalData::is_evac_allowed(thread), \"must be enclosed in oom-evac scope\");\n+\n+  ShenandoahHeapRegion* r = heap_region_containing(p);\n+  assert(!r->is_humongous(), \"never evacuate humongous objects\");\n+\n+  ShenandoahAffiliation target_gen = r->affiliation();\n+  return try_evacuate_object(p, thread, r, target_gen);\n+}\n+\n+oop ShenandoahHeap::try_evacuate_object(oop p, Thread* thread, ShenandoahHeapRegion* from_region,\n+                                               ShenandoahAffiliation target_gen) {\n+  assert(target_gen == YOUNG_GENERATION, \"Only expect evacuations to young in this mode\");\n+  assert(from_region->is_young(), \"Only expect evacuations from young in this mode\");\n+  bool alloc_from_lab = true;\n+  HeapWord* copy = nullptr;\n+  size_t size = p->size();\n+\n+#ifdef ASSERT\n+  if (ShenandoahOOMDuringEvacALot &&\n+      (os::random() & 1) == 0) { \/\/ Simulate OOM every ~2nd slow-path call\n+    copy = nullptr;\n+  } else {\n+#endif\n+    if (UseTLAB) {\n+      copy = allocate_from_gclab(thread, size);\n+    }\n+    if (copy == nullptr) {\n+      \/\/ If we failed to allocate in LAB, we'll try a shared allocation.\n+      ShenandoahAllocRequest req = ShenandoahAllocRequest::for_shared_gc(size, target_gen);\n+      copy = allocate_memory(req);\n+      alloc_from_lab = false;\n+    }\n+#ifdef ASSERT\n+  }\n+#endif\n+\n+  if (copy == nullptr) {\n+    control_thread()->handle_alloc_failure_evac(size);\n+\n+    _oom_evac_handler.handle_out_of_memory_during_evacuation();\n+\n+    return ShenandoahBarrierSet::resolve_forwarded(p);\n+  }\n+\n+  \/\/ Copy the object:\n+  Copy::aligned_disjoint_words(cast_from_oop<HeapWord*>(p), copy, size);\n+\n+  \/\/ Try to install the new forwarding pointer.\n+  oop copy_val = cast_to_oop(copy);\n+  oop result = ShenandoahForwarding::try_update_forwardee(p, copy_val);\n+  if (result == copy_val) {\n+    \/\/ Successfully evacuated. Our copy is now the public one!\n+    ContinuationGCSupport::relativize_stack_chunk(copy_val);\n+    shenandoah_assert_correct(nullptr, copy_val);\n+    return copy_val;\n+  }  else {\n+    \/\/ Failed to evacuate. We need to deal with the object that is left behind. Since this\n+    \/\/ new allocation is certainly after TAMS, it will be considered live in the next cycle.\n+    \/\/ But if it happens to contain references to evacuated regions, those references would\n+    \/\/ not get updated for this stale copy during this cycle, and we will crash while scanning\n+    \/\/ it the next cycle.\n+    if (alloc_from_lab) {\n+      \/\/ For LAB allocations, it is enough to rollback the allocation ptr. Either the next\n+      \/\/ object will overwrite this stale copy, or the filler object on LAB retirement will\n+      \/\/ do this.\n+      ShenandoahThreadLocalData::gclab(thread)->undo_allocation(copy, size);\n+    } else {\n+      \/\/ For non-LAB allocations, we have no way to retract the allocation, and\n+      \/\/ have to explicitly overwrite the copy with the filler object. With that overwrite,\n+      \/\/ we have to keep the fwdptr initialized and pointing to our (stale) copy.\n+      assert(size >= ShenandoahHeap::min_fill_size(), \"previously allocated object known to be larger than min_size\");\n+      fill_with_object(copy, size);\n+      shenandoah_assert_correct(nullptr, copy_val);\n+      \/\/ For non-LAB allocations, the object has already been registered\n+    }\n+    shenandoah_assert_correct(nullptr, result);\n+    return result;\n+  }\n+}\n+\n@@ -1039,1 +1443,1 @@\n-void ShenandoahHeap::trash_humongous_region_at(ShenandoahHeapRegion* start) {\n+size_t ShenandoahHeap::trash_humongous_region_at(ShenandoahHeapRegion* start) {\n@@ -1059,0 +1463,1 @@\n+  return required_regions;\n@@ -1068,2 +1473,0 @@\n-  }\n-};\n@@ -1071,11 +1474,4 @@\n-class ShenandoahRetireGCLABClosure : public ThreadClosure {\n-private:\n-  bool const _resize;\n-public:\n-  ShenandoahRetireGCLABClosure(bool resize) : _resize(resize) {}\n-  void do_thread(Thread* thread) {\n-    PLAB* gclab = ShenandoahThreadLocalData::gclab(thread);\n-    assert(gclab != nullptr, \"GCLAB should be initialized for %s\", thread->name());\n-    gclab->retire();\n-    if (_resize && ShenandoahThreadLocalData::gclab_size(thread) > 0) {\n-      ShenandoahThreadLocalData::set_gclab_size(thread, 0);\n+    if (ShenandoahHeap::heap()->mode()->is_generational()) {\n+      PLAB* plab = ShenandoahThreadLocalData::plab(thread);\n+      assert(plab != nullptr, \"PLAB should be initialized for %s\", thread->name());\n+      assert(plab->words_remaining() == 0, \"PLAB should not need retirement\");\n@@ -1098,0 +1494,4 @@\n+\n+  if (safepoint_workers() != nullptr) {\n+    safepoint_workers()->threads_do(&cl);\n+  }\n@@ -1133,0 +1533,1 @@\n+\n@@ -1196,1 +1597,12 @@\n-  tcl->do_thread(_control_thread);\n+  if (_shenandoah_policy->is_at_shutdown()) {\n+    return;\n+  }\n+\n+  if (_control_thread != nullptr) {\n+    tcl->do_thread(_control_thread);\n+  }\n+\n+  if (_uncommit_thread != nullptr) {\n+    tcl->do_thread(_uncommit_thread);\n+  }\n+\n@@ -1221,0 +1633,42 @@\n+void ShenandoahHeap::set_gc_generation(ShenandoahGeneration* generation) {\n+  shenandoah_assert_control_or_vm_thread_at_safepoint();\n+  _gc_generation = generation;\n+}\n+\n+\/\/ Active generation may only be set by the VM thread at a safepoint.\n+void ShenandoahHeap::set_active_generation() {\n+  assert(Thread::current()->is_VM_thread(), \"Only the VM Thread\");\n+  assert(SafepointSynchronize::is_at_safepoint(), \"Only at a safepoint!\");\n+  assert(_gc_generation != nullptr, \"Will set _active_generation to nullptr\");\n+  _active_generation = _gc_generation;\n+}\n+\n+void ShenandoahHeap::on_cycle_start(GCCause::Cause cause, ShenandoahGeneration* generation) {\n+  shenandoah_policy()->record_collection_cause(cause);\n+\n+  const GCCause::Cause current = gc_cause();\n+  assert(current == GCCause::_no_gc, \"Over-writing cause: %s, with: %s\",\n+         GCCause::to_string(current), GCCause::to_string(cause));\n+  assert(_gc_generation == nullptr, \"Over-writing _gc_generation\");\n+\n+  set_gc_cause(cause);\n+  set_gc_generation(generation);\n+\n+  generation->heuristics()->record_cycle_start();\n+}\n+\n+void ShenandoahHeap::on_cycle_end(ShenandoahGeneration* generation) {\n+  assert(gc_cause() != GCCause::_no_gc, \"cause wasn't set\");\n+  assert(_gc_generation != nullptr, \"_gc_generation wasn't set\");\n+\n+  generation->heuristics()->record_cycle_end();\n+  if (mode()->is_generational() && generation->is_global()) {\n+    \/\/ If we just completed a GLOBAL GC, claim credit for completion of young-gen and old-gen GC as well\n+    young_generation()->heuristics()->record_cycle_end();\n+    old_generation()->heuristics()->record_cycle_end();\n+  }\n+\n+  set_gc_generation(nullptr);\n+  set_gc_cause(GCCause::_no_gc);\n+}\n+\n@@ -1555,23 +2009,0 @@\n-class ShenandoahInitMarkUpdateRegionStateClosure : public ShenandoahHeapRegionClosure {\n-private:\n-  ShenandoahMarkingContext* const _ctx;\n-public:\n-  ShenandoahInitMarkUpdateRegionStateClosure() : _ctx(ShenandoahHeap::heap()->marking_context()) {}\n-\n-  void heap_region_do(ShenandoahHeapRegion* r) {\n-    assert(!r->has_live(), \"Region \" SIZE_FORMAT \" should have no live data\", r->index());\n-    if (r->is_active()) {\n-      \/\/ Check if region needs updating its TAMS. We have updated it already during concurrent\n-      \/\/ reset, so it is very likely we don't need to do another write here.\n-      if (_ctx->top_at_mark_start(r) != r->top()) {\n-        _ctx->capture_top_at_mark_start(r);\n-      }\n-    } else {\n-      assert(_ctx->top_at_mark_start(r) == r->top(),\n-             \"Region \" SIZE_FORMAT \" should already have correct TAMS\", r->index());\n-    }\n-  }\n-\n-  bool is_thread_safe() { return true; }\n-};\n-\n@@ -1593,99 +2024,0 @@\n-class ShenandoahResetUpdateRegionStateClosure : public ShenandoahHeapRegionClosure {\n-private:\n-  ShenandoahMarkingContext* const _ctx;\n-public:\n-  ShenandoahResetUpdateRegionStateClosure() : _ctx(ShenandoahHeap::heap()->marking_context()) {}\n-\n-  void heap_region_do(ShenandoahHeapRegion* r) {\n-    if (r->is_active()) {\n-      \/\/ Reset live data and set TAMS optimistically. We would recheck these under the pause\n-      \/\/ anyway to capture any updates that happened since now.\n-      r->clear_live_data();\n-      _ctx->capture_top_at_mark_start(r);\n-    }\n-  }\n-\n-  bool is_thread_safe() { return true; }\n-};\n-\n-void ShenandoahHeap::prepare_gc() {\n-  reset_mark_bitmap();\n-\n-  ShenandoahResetUpdateRegionStateClosure cl;\n-  parallel_heap_region_iterate(&cl);\n-}\n-\n-class ShenandoahFinalMarkUpdateRegionStateClosure : public ShenandoahHeapRegionClosure {\n-private:\n-  ShenandoahMarkingContext* const _ctx;\n-  ShenandoahHeapLock* const _lock;\n-\n-public:\n-  ShenandoahFinalMarkUpdateRegionStateClosure() :\n-    _ctx(ShenandoahHeap::heap()->complete_marking_context()), _lock(ShenandoahHeap::heap()->lock()) {}\n-\n-  void heap_region_do(ShenandoahHeapRegion* r) {\n-    if (r->is_active()) {\n-      \/\/ All allocations past TAMS are implicitly live, adjust the region data.\n-      \/\/ Bitmaps\/TAMS are swapped at this point, so we need to poll complete bitmap.\n-      HeapWord *tams = _ctx->top_at_mark_start(r);\n-      HeapWord *top = r->top();\n-      if (top > tams) {\n-        r->increase_live_data_alloc_words(pointer_delta(top, tams));\n-      }\n-\n-      \/\/ We are about to select the collection set, make sure it knows about\n-      \/\/ current pinning status. Also, this allows trashing more regions that\n-      \/\/ now have their pinning status dropped.\n-      if (r->is_pinned()) {\n-        if (r->pin_count() == 0) {\n-          ShenandoahHeapLocker locker(_lock);\n-          r->make_unpinned();\n-        }\n-      } else {\n-        if (r->pin_count() > 0) {\n-          ShenandoahHeapLocker locker(_lock);\n-          r->make_pinned();\n-        }\n-      }\n-\n-      \/\/ Remember limit for updating refs. It's guaranteed that we get no\n-      \/\/ from-space-refs written from here on.\n-      r->set_update_watermark_at_safepoint(r->top());\n-    } else {\n-      assert(!r->has_live(), \"Region \" SIZE_FORMAT \" should have no live data\", r->index());\n-      assert(_ctx->top_at_mark_start(r) == r->top(),\n-             \"Region \" SIZE_FORMAT \" should have correct TAMS\", r->index());\n-    }\n-  }\n-\n-  bool is_thread_safe() { return true; }\n-};\n-\n-void ShenandoahHeap::prepare_regions_and_collection_set(bool concurrent) {\n-  assert(!is_full_gc_in_progress(), \"Only for concurrent and degenerated GC\");\n-  {\n-    ShenandoahGCPhase phase(concurrent ? ShenandoahPhaseTimings::final_update_region_states :\n-                                         ShenandoahPhaseTimings::degen_gc_final_update_region_states);\n-    ShenandoahFinalMarkUpdateRegionStateClosure cl;\n-    parallel_heap_region_iterate(&cl);\n-\n-    assert_pinned_region_status();\n-  }\n-\n-  {\n-    ShenandoahGCPhase phase(concurrent ? ShenandoahPhaseTimings::choose_cset :\n-                                         ShenandoahPhaseTimings::degen_gc_choose_cset);\n-    ShenandoahHeapLocker locker(lock());\n-    _collection_set->clear();\n-    heuristics()->choose_collection_set(_collection_set);\n-  }\n-\n-  {\n-    ShenandoahGCPhase phase(concurrent ? ShenandoahPhaseTimings::final_rebuild_freeset :\n-                                         ShenandoahPhaseTimings::degen_gc_final_rebuild_freeset);\n-    ShenandoahHeapLocker locker(lock());\n-    _free_set->rebuild();\n-  }\n-}\n-\n@@ -1694,0 +2026,3 @@\n+  if (mode()->is_generational()) {\n+    old_generation()->set_parsable(false);\n+  }\n@@ -1702,1 +2037,2 @@\n-  ref_processor()->process_references(phase, workers(), false \/* concurrent *\/);\n+  shenandoah_assert_generations_reconciled();\n+  gc_generation()->ref_processor()->process_references(phase, workers(), false \/* concurrent *\/);\n@@ -1705,1 +2041,1 @@\n-void ShenandoahHeap::prepare_update_heap_references(bool concurrent) {\n+void ShenandoahHeap::prepare_update_heap_references() {\n@@ -1712,3 +2048,1 @@\n-    ShenandoahGCPhase phase(concurrent ?\n-                            ShenandoahPhaseTimings::init_update_refs_manage_gclabs :\n-                            ShenandoahPhaseTimings::degen_gc_init_update_refs_manage_gclabs);\n+    ShenandoahGCPhase phase(ShenandoahPhaseTimings::degen_gc_init_update_refs_manage_gclabs);\n@@ -1721,1 +2055,1 @@\n-void ShenandoahHeap::propagate_gc_state_to_java_threads() {\n+void ShenandoahHeap::propagate_gc_state_to_all_threads() {\n@@ -1724,0 +2058,2 @@\n+    ShenandoahGCStatePropagator propagator(_gc_state.raw_value());\n+    Threads::threads_do(&propagator);\n@@ -1725,4 +2061,0 @@\n-    char state = gc_state();\n-    for (JavaThreadIteratorWithHandle jtiwh; JavaThread *t = jtiwh.next(); ) {\n-      ShenandoahThreadLocalData::set_gc_state(t, state);\n-    }\n@@ -1732,1 +2064,1 @@\n-void ShenandoahHeap::set_gc_state(uint mask, bool value) {\n+void ShenandoahHeap::set_gc_state_at_safepoint(uint mask, bool value) {\n@@ -1738,4 +2070,62 @@\n-void ShenandoahHeap::set_concurrent_mark_in_progress(bool in_progress) {\n-  assert(!has_forwarded_objects(), \"Not expected before\/after mark phase\");\n-  set_gc_state(MARKING, in_progress);\n-  ShenandoahBarrierSet::satb_mark_queue_set().set_active_all_threads(in_progress, !in_progress);\n+void ShenandoahHeap::set_gc_state_concurrent(uint mask, bool value) {\n+  \/\/ Holding the thread lock here assures that any thread created after we change the gc\n+  \/\/ state will have the correct state. It also prevents attaching threads from seeing\n+  \/\/ an inconsistent state. See ShenandoahBarrierSet::on_thread_attach for reference. Established\n+  \/\/ threads will use their thread local copy of the gc state (changed by a handshake, or on a\n+  \/\/ safepoint).\n+  assert(Threads_lock->is_locked(), \"Must hold thread lock for concurrent gc state change\");\n+  _gc_state.set_cond(mask, value);\n+}\n+\n+void ShenandoahHeap::set_concurrent_young_mark_in_progress(bool in_progress) {\n+  uint mask;\n+  assert(!has_forwarded_objects(), \"Young marking is not concurrent with evacuation\");\n+  if (!in_progress && is_concurrent_old_mark_in_progress()) {\n+    assert(mode()->is_generational(), \"Only generational GC has old marking\");\n+    assert(_gc_state.is_set(MARKING), \"concurrent_old_marking_in_progress implies MARKING\");\n+    \/\/ If old-marking is in progress when we turn off YOUNG_MARKING, leave MARKING (and OLD_MARKING) on\n+    mask = YOUNG_MARKING;\n+  } else {\n+    mask = MARKING | YOUNG_MARKING;\n+  }\n+  set_gc_state_at_safepoint(mask, in_progress);\n+  manage_satb_barrier(in_progress);\n+}\n+\n+void ShenandoahHeap::set_concurrent_old_mark_in_progress(bool in_progress) {\n+#ifdef ASSERT\n+  \/\/ has_forwarded_objects() iff UPDATE_REFS or EVACUATION\n+  bool has_forwarded = has_forwarded_objects();\n+  bool updating_or_evacuating = _gc_state.is_set(UPDATE_REFS | EVACUATION);\n+  bool evacuating = _gc_state.is_set(EVACUATION);\n+  assert ((has_forwarded == updating_or_evacuating) || (evacuating && !has_forwarded && collection_set()->is_empty()),\n+          \"Updating or evacuating iff has forwarded objects, or if evacuation phase is promoting in place without forwarding\");\n+#endif\n+  if (!in_progress && is_concurrent_young_mark_in_progress()) {\n+    \/\/ If young-marking is in progress when we turn off OLD_MARKING, leave MARKING (and YOUNG_MARKING) on\n+    assert(_gc_state.is_set(MARKING), \"concurrent_young_marking_in_progress implies MARKING\");\n+    set_gc_state_at_safepoint(OLD_MARKING, in_progress);\n+  } else {\n+    set_gc_state_at_safepoint(MARKING | OLD_MARKING, in_progress);\n+  }\n+  manage_satb_barrier(in_progress);\n+}\n+\n+bool ShenandoahHeap::is_prepare_for_old_mark_in_progress() const {\n+  return old_generation()->is_preparing_for_mark();\n+}\n+\n+void ShenandoahHeap::manage_satb_barrier(bool active) {\n+  if (is_concurrent_mark_in_progress()) {\n+    \/\/ Ignore request to deactivate barrier while concurrent mark is in progress.\n+    \/\/ Do not attempt to re-activate the barrier if it is already active.\n+    if (active && !ShenandoahBarrierSet::satb_mark_queue_set().is_active()) {\n+      ShenandoahBarrierSet::satb_mark_queue_set().set_active_all_threads(active, !active);\n+    }\n+  } else {\n+    \/\/ No concurrent marking is in progress so honor request to deactivate,\n+    \/\/ but only if the barrier is already active.\n+    if (!active && ShenandoahBarrierSet::satb_mark_queue_set().is_active()) {\n+      ShenandoahBarrierSet::satb_mark_queue_set().set_active_all_threads(active, !active);\n+    }\n+  }\n@@ -1746,1 +2136,1 @@\n-  set_gc_state(EVACUATION, in_progress);\n+  set_gc_state_at_safepoint(EVACUATION, in_progress);\n@@ -1758,1 +2148,1 @@\n-  set_gc_state(WEAK_ROOTS, cond);\n+  set_gc_state_at_safepoint(WEAK_ROOTS, cond);\n@@ -1769,3 +2159,14 @@\n-bool ShenandoahHeap::try_cancel_gc() {\n-  jbyte prev = _cancelled_gc.cmpxchg(CANCELLED, CANCELLABLE);\n-  return prev == CANCELLABLE;\n+bool ShenandoahHeap::try_cancel_gc(GCCause::Cause cause) {\n+  const GCCause::Cause prev = _cancelled_gc.xchg(cause);\n+  return prev == GCCause::_no_gc || prev == GCCause::_shenandoah_concurrent_gc;\n+}\n+\n+void ShenandoahHeap::cancel_concurrent_mark() {\n+  if (mode()->is_generational()) {\n+    young_generation()->cancel_marking();\n+    old_generation()->cancel_marking();\n+  }\n+\n+  global_generation()->cancel_marking();\n+\n+  ShenandoahBarrierSet::satb_mark_queue_set().abandon_partial_marking();\n@@ -1774,2 +2175,2 @@\n-void ShenandoahHeap::cancel_gc(GCCause::Cause cause) {\n-  if (try_cancel_gc()) {\n+bool ShenandoahHeap::cancel_gc(GCCause::Cause cause) {\n+  if (try_cancel_gc(cause)) {\n@@ -1777,1 +2178,1 @@\n-    log_info(gc)(\"%s\", msg.buffer());\n+    log_info(gc,thread)(\"%s\", msg.buffer());\n@@ -1779,0 +2180,2 @@\n+    _cancel_requested_time = os::elapsedTime();\n+    return true;\n@@ -1780,0 +2183,1 @@\n+  return false;\n@@ -1792,7 +2196,2 @@\n-  \/\/ Step 1. Notify control thread that we are in shutdown.\n-  \/\/ Note that we cannot do that with stop(), because stop() is blocking and waits for the actual shutdown.\n-  \/\/ Doing stop() here would wait for the normal GC cycle to complete, never falling through to cancel below.\n-  control_thread()->prepare_for_graceful_shutdown();\n-\n-  \/\/ Step 2. Notify GC workers that we are cancelling GC.\n-  cancel_gc(GCCause::_shenandoah_stop_vm);\n+  \/\/ Step 1. Stop reporting on gc thread cpu utilization\n+  mmu_tracker()->stop();\n@@ -1800,1 +2199,1 @@\n-  \/\/ Step 3. Wait until GC worker exits normally.\n+  \/\/ Step 2. Wait until GC worker exits normally (this will cancel any ongoing GC).\n@@ -1802,0 +2201,5 @@\n+\n+  \/\/ Stop 4. Shutdown uncommit thread.\n+  if (_uncommit_thread != nullptr) {\n+    _uncommit_thread->stop();\n+  }\n@@ -1841,1 +2245,1 @@\n-\/\/ Weak roots are either pre-evacuated (final mark) or updated (final updaterefs),\n+\/\/ Weak roots are either pre-evacuated (final mark) or updated (final update refs),\n@@ -1885,1 +2289,1 @@\n-  set_gc_state(HAS_FORWARDED, cond);\n+  set_gc_state_at_safepoint(HAS_FORWARDED, cond);\n@@ -1902,4 +2306,0 @@\n-size_t ShenandoahHeap::bytes_allocated_since_gc_start() {\n-  return Atomic::load(&_bytes_allocated_since_gc_start);\n-}\n-\n@@ -1907,1 +2307,6 @@\n-  Atomic::store(&_bytes_allocated_since_gc_start, (size_t)0);\n+  if (mode()->is_generational()) {\n+    young_generation()->reset_bytes_allocated_since_gc_start();\n+    old_generation()->reset_bytes_allocated_since_gc_start();\n+  }\n+\n+  global_generation()->reset_bytes_allocated_since_gc_start();\n@@ -1924,1 +2329,1 @@\n-  set_gc_state(UPDATEREFS, in_progress);\n+  set_gc_state_at_safepoint(UPDATE_REFS, in_progress);\n@@ -1971,2 +2376,5 @@\n-    assert((r->is_pinned() && r->pin_count() > 0) || (!r->is_pinned() && r->pin_count() == 0),\n-           \"Region \" SIZE_FORMAT \" pinning status is inconsistent\", i);\n+    shenandoah_assert_generations_reconciled();\n+    if (gc_generation()->contains(r)) {\n+      assert((r->is_pinned() && r->pin_count() > 0) || (!r->is_pinned() && r->pin_count() == 0),\n+             \"Region \" SIZE_FORMAT \" pinning status is inconsistent\", i);\n+    }\n@@ -2004,6 +2412,3 @@\n-    if (UseDynamicNumberOfGCThreads) {\n-      assert(nworkers <= ParallelGCThreads, \"Cannot use more than it has\");\n-    } else {\n-      \/\/ Use ParallelGCThreads inside safepoints\n-      assert(nworkers == ParallelGCThreads, \"Use ParallelGCThreads within safepoints\");\n-    }\n+    \/\/ Use ParallelGCThreads inside safepoints\n+    assert(nworkers == ParallelGCThreads, \"Use ParallelGCThreads (%u) within safepoint, not %u\",\n+           ParallelGCThreads, nworkers);\n@@ -2011,6 +2416,3 @@\n-    if (UseDynamicNumberOfGCThreads) {\n-      assert(nworkers <= ConcGCThreads, \"Cannot use more than it has\");\n-    } else {\n-      \/\/ Use ConcGCThreads outside safepoints\n-      assert(nworkers == ConcGCThreads, \"Use ConcGCThreads outside safepoints\");\n-    }\n+    \/\/ Use ConcGCThreads outside safepoints\n+    assert(nworkers == ConcGCThreads, \"Use ConcGCThreads (%u) outside safepoints, %u\",\n+           ConcGCThreads, nworkers);\n@@ -2033,1 +2435,1 @@\n-  ShenandoahUpdateHeapRefsTask(ShenandoahRegionIterator* regions) :\n+  explicit ShenandoahUpdateHeapRefsTask(ShenandoahRegionIterator* regions) :\n@@ -2043,1 +2445,1 @@\n-      do_work<ShenandoahConcUpdateRefsClosure>();\n+      do_work<ShenandoahConcUpdateRefsClosure>(worker_id);\n@@ -2046,1 +2448,1 @@\n-      do_work<ShenandoahSTWUpdateRefsClosure>();\n+      do_work<ShenandoahSTWUpdateRefsClosure>(worker_id);\n@@ -2052,1 +2454,13 @@\n-  void do_work() {\n+  void do_work(uint worker_id) {\n+    if (CONCURRENT && (worker_id == 0)) {\n+      \/\/ We ask the first worker to replenish the Mutator free set by moving regions previously reserved to hold the\n+      \/\/ results of evacuation.  These reserves are no longer necessary because evacuation has completed.\n+      size_t cset_regions = _heap->collection_set()->count();\n+\n+      \/\/ Now that evacuation is done, we can reassign any regions that had been reserved to hold the results of evacuation\n+      \/\/ to the mutator free set.  At the end of GC, we will have cset_regions newly evacuated fully empty regions from\n+      \/\/ which we will be able to replenish the Collector free set and the OldCollector free set in preparation for the\n+      \/\/ next GC cycle.\n+      _heap->free_set()->move_regions_from_collector_to_mutator(cset_regions);\n+    }\n+    \/\/ If !CONCURRENT, there's no value in expanding Mutator free set\n@@ -2055,1 +2469,0 @@\n-    ShenandoahMarkingContext* const ctx = _heap->complete_marking_context();\n@@ -2061,3 +2474,3 @@\n-      }\n-      if (ShenandoahPacing) {\n-        _heap->pacer()->report_updaterefs(pointer_delta(update_watermark, r->bottom()));\n+        if (ShenandoahPacing) {\n+          _heap->pacer()->report_update_refs(pointer_delta(update_watermark, r->bottom()));\n+        }\n@@ -2085,30 +2498,0 @@\n-\n-class ShenandoahFinalUpdateRefsUpdateRegionStateClosure : public ShenandoahHeapRegionClosure {\n-private:\n-  ShenandoahHeapLock* const _lock;\n-\n-public:\n-  ShenandoahFinalUpdateRefsUpdateRegionStateClosure() : _lock(ShenandoahHeap::heap()->lock()) {}\n-\n-  void heap_region_do(ShenandoahHeapRegion* r) {\n-    \/\/ Drop unnecessary \"pinned\" state from regions that does not have CP marks\n-    \/\/ anymore, as this would allow trashing them.\n-\n-    if (r->is_active()) {\n-      if (r->is_pinned()) {\n-        if (r->pin_count() == 0) {\n-          ShenandoahHeapLocker locker(_lock);\n-          r->make_unpinned();\n-        }\n-      } else {\n-        if (r->pin_count() > 0) {\n-          ShenandoahHeapLocker locker(_lock);\n-          r->make_pinned();\n-        }\n-      }\n-    }\n-  }\n-\n-  bool is_thread_safe() { return true; }\n-};\n-\n@@ -2123,2 +2506,2 @@\n-    ShenandoahFinalUpdateRefsUpdateRegionStateClosure cl;\n-    parallel_heap_region_iterate(&cl);\n+\n+    final_update_refs_update_region_states();\n@@ -2137,0 +2520,5 @@\n+void ShenandoahHeap::final_update_refs_update_region_states() {\n+  ShenandoahSynchronizePinnedRegionStates cl;\n+  parallel_heap_region_iterate(&cl);\n+}\n+\n@@ -2138,6 +2526,42 @@\n-  {\n-    ShenandoahGCPhase phase(concurrent ?\n-                            ShenandoahPhaseTimings::final_update_refs_rebuild_freeset :\n-                            ShenandoahPhaseTimings::degen_gc_final_update_refs_rebuild_freeset);\n-    ShenandoahHeapLocker locker(lock());\n-    _free_set->rebuild();\n+  ShenandoahGCPhase phase(concurrent ?\n+                          ShenandoahPhaseTimings::final_update_refs_rebuild_freeset :\n+                          ShenandoahPhaseTimings::degen_gc_final_update_refs_rebuild_freeset);\n+  ShenandoahHeapLocker locker(lock());\n+  size_t young_cset_regions, old_cset_regions;\n+  size_t first_old_region, last_old_region, old_region_count;\n+  _free_set->prepare_to_rebuild(young_cset_regions, old_cset_regions, first_old_region, last_old_region, old_region_count);\n+  \/\/ If there are no old regions, first_old_region will be greater than last_old_region\n+  assert((first_old_region > last_old_region) ||\n+         ((last_old_region + 1 - first_old_region >= old_region_count) &&\n+          get_region(first_old_region)->is_old() && get_region(last_old_region)->is_old()),\n+         \"sanity: old_region_count: \" SIZE_FORMAT \", first_old_region: \" SIZE_FORMAT \", last_old_region: \" SIZE_FORMAT,\n+         old_region_count, first_old_region, last_old_region);\n+\n+  if (mode()->is_generational()) {\n+#ifdef ASSERT\n+    if (ShenandoahVerify) {\n+      verifier()->verify_before_rebuilding_free_set();\n+    }\n+#endif\n+\n+    \/\/ The computation of bytes_of_allocation_runway_before_gc_trigger is quite conservative so consider all of this\n+    \/\/ available for transfer to old. Note that transfer of humongous regions does not impact available.\n+    ShenandoahGenerationalHeap* gen_heap = ShenandoahGenerationalHeap::heap();\n+    size_t allocation_runway = gen_heap->young_generation()->heuristics()->bytes_of_allocation_runway_before_gc_trigger(young_cset_regions);\n+    gen_heap->compute_old_generation_balance(allocation_runway, old_cset_regions);\n+\n+    \/\/ Total old_available may have been expanded to hold anticipated promotions.  We trigger if the fragmented available\n+    \/\/ memory represents more than 16 regions worth of data.  Note that fragmentation may increase when we promote regular\n+    \/\/ regions in place when many of these regular regions have an abundant amount of available memory within them.  Fragmentation\n+    \/\/ will decrease as promote-by-copy consumes the available memory within these partially consumed regions.\n+    \/\/\n+    \/\/ We consider old-gen to have excessive fragmentation if more than 12.5% of old-gen is free memory that resides\n+    \/\/ within partially consumed regions of memory.\n+  }\n+  \/\/ Rebuild free set based on adjusted generation sizes.\n+  _free_set->finish_rebuild(young_cset_regions, old_cset_regions, old_region_count);\n+\n+  if (mode()->is_generational()) {\n+    ShenandoahGenerationalHeap* gen_heap = ShenandoahGenerationalHeap::heap();\n+    ShenandoahOldGeneration* old_gen = gen_heap->old_generation();\n+    old_gen->heuristics()->evaluate_triggers(first_old_region, last_old_region, old_region_count, num_regions());\n@@ -2209,1 +2633,1 @@\n-    \/\/ slice is should stay committed, exit right away.\n+    \/\/ slice should stay committed, exit right away.\n@@ -2223,0 +2647,21 @@\n+void ShenandoahHeap::forbid_uncommit() {\n+  if (_uncommit_thread != nullptr) {\n+    _uncommit_thread->forbid_uncommit();\n+  }\n+}\n+\n+void ShenandoahHeap::allow_uncommit() {\n+  if (_uncommit_thread != nullptr) {\n+    _uncommit_thread->allow_uncommit();\n+  }\n+}\n+\n+#ifdef ASSERT\n+bool ShenandoahHeap::is_uncommit_in_progress() {\n+  if (_uncommit_thread != nullptr) {\n+    return _uncommit_thread->is_uncommit_in_progress();\n+  }\n+  return false;\n+}\n+#endif\n+\n@@ -2231,8 +2676,0 @@\n-void ShenandoahHeap::entry_uncommit(double shrink_before, size_t shrink_until) {\n-  static const char *msg = \"Concurrent uncommit\";\n-  ShenandoahConcurrentPhase gc_phase(msg, ShenandoahPhaseTimings::conc_uncommit, true \/* log_heap_usage *\/);\n-  EventMark em(\"%s\", msg);\n-\n-  op_uncommit(shrink_before, shrink_until);\n-}\n-\n@@ -2273,1 +2710,1 @@\n-  return _memory_pool->get_memory_usage();\n+  return MemoryUsage(_initial_size, used(), committed(), max_capacity());\n@@ -2296,0 +2733,8 @@\n+bool ShenandoahHeap::is_gc_state(GCState state) const {\n+  \/\/ If the global gc state has been changed, but hasn't yet been propagated to all threads, then\n+  \/\/ the global gc state is the correct value. Once the gc state has been synchronized with all threads,\n+  \/\/ _gc_state_changed will be toggled to false and we need to use the thread local state.\n+  return _gc_state_changed ? _gc_state.is_set(state) : ShenandoahThreadLocalData::is_gc_state(state);\n+}\n+\n+\n@@ -2338,0 +2783,22 @@\n+\n+ShenandoahGeneration* ShenandoahHeap::generation_for(ShenandoahAffiliation affiliation) const {\n+  if (!mode()->is_generational()) {\n+    return global_generation();\n+  } else if (affiliation == YOUNG_GENERATION) {\n+    return young_generation();\n+  } else if (affiliation == OLD_GENERATION) {\n+    return old_generation();\n+  }\n+\n+  ShouldNotReachHere();\n+  return nullptr;\n+}\n+\n+void ShenandoahHeap::log_heap_status(const char* msg) const {\n+  if (mode()->is_generational()) {\n+    young_generation()->log_status(msg);\n+    old_generation()->log_status(msg);\n+  } else {\n+    global_generation()->log_status(msg);\n+  }\n+}\n","filename":"src\/hotspot\/share\/gc\/shenandoah\/shenandoahHeap.cpp","additions":876,"deletions":409,"binary":false,"changes":1285,"status":"modified"},{"patch":"@@ -4,0 +4,1 @@\n+ * Copyright Amazon.com Inc. or its affiliates. All Rights Reserved.\n@@ -29,0 +30,1 @@\n+#include \"gc\/shenandoah\/shenandoahGeneration.hpp\"\n@@ -31,0 +33,1 @@\n+#include \"gc\/shenandoah\/shenandoahScanRemembered.inline.hpp\"\n@@ -60,0 +63,17 @@\n+template <typename T>\n+static void card_mark_barrier(T* field, oop value) {\n+  assert(ShenandoahCardBarrier, \"Card-mark barrier should be on\");\n+  ShenandoahGenerationalHeap* heap = ShenandoahGenerationalHeap::heap();\n+  assert(heap->is_in_or_null(value), \"Should be in heap\");\n+  if (heap->is_in_old(field) && heap->is_in_young(value)) {\n+    \/\/ For Shenandoah, each generation collects all the _referents_ that belong to the\n+    \/\/ collected generation. We can end up with discovered lists that contain a mixture\n+    \/\/ of old and young _references_. These references are linked together through the\n+    \/\/ discovered field in java.lang.Reference. In some cases, creating or editing this\n+    \/\/ list may result in the creation of _new_ old-to-young pointers which must dirty\n+    \/\/ the corresponding card. Failing to do this may cause heap verification errors and\n+    \/\/ lead to incorrect GC behavior.\n+    heap->old_generation()->mark_card_as_dirty(field);\n+  }\n+}\n+\n@@ -66,0 +86,3 @@\n+  if (ShenandoahCardBarrier) {\n+    card_mark_barrier(field, value);\n+  }\n@@ -71,0 +94,3 @@\n+  if (ShenandoahCardBarrier) {\n+    card_mark_barrier(field, value);\n+  }\n@@ -271,0 +297,1 @@\n+  ShenandoahHeap* heap = ShenandoahHeap::heap();\n@@ -287,0 +314,5 @@\n+  if (!heap->is_in_active_generation(referent)) {\n+    log_trace(gc,ref)(\"Referent outside of active generation: \" PTR_FORMAT, p2i(referent));\n+    return false;\n+  }\n+\n@@ -300,0 +332,1 @@\n+  ShenandoahHeap* heap = ShenandoahHeap::heap();\n@@ -303,0 +336,1 @@\n+<<<<<<< HEAD\n@@ -306,0 +340,5 @@\n+=======\n+    return heap->active_generation()->complete_marking_context()->is_marked(raw_referent);\n+  } else {\n+    return heap->active_generation()->complete_marking_context()->is_marked_strong(raw_referent);\n+>>>>>>> 82b11cb49a9f6509959a6476d0d1d91c81fcfd02\n@@ -317,0 +356,1 @@\n+<<<<<<< HEAD\n@@ -318,0 +358,3 @@\n+=======\n+    assert(ShenandoahHeap::heap()->active_generation()->complete_marking_context()->is_marked(reference_referent_raw<T>(reference)), \"only make inactive final refs with alive referents\");\n+>>>>>>> 82b11cb49a9f6509959a6476d0d1d91c81fcfd02\n@@ -352,0 +395,3 @@\n+  \/\/ Each worker thread has a private copy of refproc_data, which includes a private discovered list.  This means\n+  \/\/ there's no risk that a different worker thread will try to manipulate my discovered list head while I'm making\n+  \/\/ reference the head of my discovered list.\n@@ -360,0 +406,11 @@\n+    \/\/ We successfully set this reference object's next pointer to discovered_head.  This marks reference as discovered.\n+    \/\/ If reference_cas_discovered fails, that means some other worker thread took credit for discovery of this reference,\n+    \/\/ and that other thread will place reference on its discovered list, so I can ignore reference.\n+\n+    \/\/ In case we have created an interesting pointer, mark the remembered set card as dirty.\n+    if (ShenandoahCardBarrier) {\n+      T* addr = reinterpret_cast<T*>(java_lang_ref_Reference::discovered_addr_raw(reference));\n+      card_mark_barrier(addr, discovered_head);\n+    }\n+\n+    \/\/ Make the discovered_list_head point to reference.\n@@ -374,1 +431,2 @@\n-  log_trace(gc, ref)(\"Encountered Reference: \" PTR_FORMAT \" (%s)\", p2i(reference), reference_type_name(type));\n+  log_trace(gc, ref)(\"Encountered Reference: \" PTR_FORMAT \" (%s, %s)\",\n+          p2i(reference), reference_type_name(type), ShenandoahHeap::heap()->heap_region_containing(reference)->affiliation_name());\n@@ -389,0 +447,1 @@\n+<<<<<<< HEAD\n@@ -394,0 +453,5 @@\n+=======\n+  ShenandoahHeap* heap = ShenandoahHeap::heap();\n+  HeapWord* referent = reference_referent_raw<T>(reference);\n+  assert(referent == nullptr || heap->marking_context()->is_marked(referent), \"only drop references with alive referents\");\n+>>>>>>> 82b11cb49a9f6509959a6476d0d1d91c81fcfd02\n@@ -398,0 +462,7 @@\n+  \/\/ When this reference was discovered, it would not have been marked. If it ends up surviving\n+  \/\/ the cycle, we need to dirty the card if the reference is old and the referent is young.  Note\n+  \/\/ that if the reference is not dropped, then its pointer to the referent will be nulled before\n+  \/\/ evacuation begins so card does not need to be dirtied.\n+  if (ShenandoahCardBarrier) {\n+    card_mark_barrier(cast_from_oop<HeapWord*>(reference), cast_to_oop(referent));\n+  }\n@@ -449,0 +520,1 @@\n+  \/\/ set_oop_field maintains the card mark barrier as this list is constructed.\n@@ -453,1 +525,1 @@\n-    RawAccess<>::oop_store(p, prev);\n+    set_oop_field(p, prev);\n@@ -525,0 +597,13 @@\n+\n+  \/\/ During reference processing, we maintain a local list of references that are identified by\n+  \/\/   _pending_list and _pending_list_tail.  _pending_list_tail points to the next field of the last Reference object on\n+  \/\/   the local list.\n+  \/\/\n+  \/\/ There is also a global list of reference identified by Universe::_reference_pending_list\n+\n+  \/\/ The following code has the effect of:\n+  \/\/  1. Making the global Universe::_reference_pending_list point to my local list\n+  \/\/  2. Overwriting the next field of the last Reference on my local list to point at the previous head of the\n+  \/\/     global Universe::_reference_pending_list\n+\n+  oop former_head_of_global_list = Universe::swap_reference_pending_list(_pending_list);\n@@ -526,1 +611,1 @@\n-    *reinterpret_cast<narrowOop*>(_pending_list_tail) = CompressedOops::encode(Universe::swap_reference_pending_list(_pending_list));\n+    set_oop_field<narrowOop>(reinterpret_cast<narrowOop*>(_pending_list_tail), former_head_of_global_list);\n@@ -528,1 +613,1 @@\n-    *reinterpret_cast<oop*>(_pending_list_tail) = Universe::swap_reference_pending_list(_pending_list);\n+    set_oop_field<oop>(reinterpret_cast<oop*>(_pending_list_tail), former_head_of_global_list);\n@@ -537,1 +622,0 @@\n-\n@@ -615,1 +699,0 @@\n-\n","filename":"src\/hotspot\/share\/gc\/shenandoah\/shenandoahReferenceProcessor.cpp","additions":89,"deletions":6,"binary":false,"changes":95,"status":"modified"},{"patch":"@@ -3,0 +3,1 @@\n+ * Copyright Amazon.com Inc. or its affiliates. All Rights Reserved.\n@@ -30,0 +31,1 @@\n+#include \"gc\/shenandoah\/shenandoahGeneration.hpp\"\n@@ -32,0 +34,1 @@\n+#include \"gc\/shenandoah\/shenandoahOldGeneration.hpp\"\n@@ -33,0 +36,1 @@\n+#include \"gc\/shenandoah\/shenandoahScanRemembered.inline.hpp\"\n@@ -36,0 +40,1 @@\n+#include \"gc\/shenandoah\/shenandoahYoungGeneration.hpp\"\n@@ -65,0 +70,4 @@\n+<<<<<<< HEAD\n+=======\n+  ShenandoahGeneration* _generation;\n+>>>>>>> 82b11cb49a9f6509959a6476d0d1d91c81fcfd02\n@@ -76,1 +85,2 @@\n-    _loc(nullptr) {\n+    _loc(nullptr),\n+    _generation(nullptr) {\n@@ -78,0 +88,1 @@\n+        options._verify_marked == ShenandoahVerifier::_verify_marked_complete_satb_empty ||\n@@ -87,0 +98,10 @@\n+\n+    if (_heap->mode()->is_generational()) {\n+      _generation = _heap->gc_generation();\n+      assert(_generation != nullptr, \"Expected active generation in this mode\");\n+      shenandoah_assert_generations_reconciled();\n+    }\n+  }\n+\n+  ReferenceIterationMode reference_iteration_mode() override {\n+    return _ref_mode;\n@@ -113,2 +134,1 @@\n-\n-      if (_map->par_mark(obj)) {\n+      if (in_generation(obj) && _map->par_mark(obj)) {\n@@ -121,0 +141,9 @@\n+  bool in_generation(oop obj) {\n+    if (_generation == nullptr) {\n+      return true;\n+    }\n+\n+    ShenandoahHeapRegion* region = _heap->heap_region_containing(obj);\n+    return _generation->contains(region);\n+  }\n+\n@@ -171,1 +200,2 @@\n-          check(ShenandoahAsserts::_safe_oop, obj, obj_reg->has_live(),\n+          check(ShenandoahAsserts::_safe_oop, obj, obj_reg->has_live() ||\n+                (obj_reg->is_old() && _heap->gc_generation()->is_young()),\n@@ -173,0 +203,1 @@\n+          shenandoah_assert_generations_reconciled();\n@@ -222,1 +253,8 @@\n-    \/\/ ------------ obj and fwd are safe at this point --------------\n+    \/\/ Do additional checks for special objects: their fields can hold metadata as well.\n+    \/\/ We want to check class loading\/unloading did not corrupt them.\n+\n+    if (obj_klass == vmClasses::Class_klass()) {\n+      Metadata* klass = obj->metadata_field(java_lang_Class::klass_offset());\n+      check(ShenandoahAsserts::_safe_oop, obj,\n+            klass == nullptr || Metaspace::contains(klass),\n+            \"Instance class mirror should point to Metaspace\");\n@@ -224,0 +262,7 @@\n+      Metadata* array_klass = obj->metadata_field(java_lang_Class::array_klass_offset());\n+      check(ShenandoahAsserts::_safe_oop, obj,\n+            array_klass == nullptr || Metaspace::contains(array_klass),\n+            \"Array class mirror should point to Metaspace\");\n+    }\n+\n+    \/\/ ------------ obj and fwd are safe at this point --------------\n@@ -233,1 +278,1 @@\n-        check(ShenandoahAsserts::_safe_all, obj, _heap->complete_marking_context()->is_marked(obj),\n+        check(ShenandoahAsserts::_safe_all, obj, _heap->gc_generation()->complete_marking_context()->is_marked(obj),\n@@ -237,1 +282,2 @@\n-        check(ShenandoahAsserts::_safe_all, obj, _heap->complete_marking_context()->is_marked(obj),\n+      case ShenandoahVerifier::_verify_marked_complete_satb_empty:\n+        check(ShenandoahAsserts::_safe_all, obj, _heap->gc_generation()->complete_marking_context()->is_marked(obj),\n@@ -319,0 +365,1 @@\n+<<<<<<< HEAD\n@@ -321,0 +368,4 @@\n+=======\n+  void do_oop(oop* p) override { do_oop_work(p); }\n+  void do_oop(narrowOop* p) override { do_oop_work(p); }\n+>>>>>>> 82b11cb49a9f6509959a6476d0d1d91c81fcfd02\n@@ -323,0 +374,2 @@\n+\/\/ This closure computes the amounts of used, committed, and garbage memory and the number of regions contained within\n+\/\/ a subset (e.g. the young generation or old generation) of the total heap.\n@@ -325,1 +378,1 @@\n-  size_t _used, _committed, _garbage;\n+  size_t _used, _committed, _garbage, _regions, _humongous_waste, _trashed_regions;\n@@ -327,1 +380,2 @@\n-  ShenandoahCalculateRegionStatsClosure() : _used(0), _committed(0), _garbage(0) {};\n+  ShenandoahCalculateRegionStatsClosure() :\n+      _used(0), _committed(0), _garbage(0), _regions(0), _humongous_waste(0), _trashed_regions(0) {};\n@@ -329,1 +383,1 @@\n-  void heap_region_do(ShenandoahHeapRegion* r) {\n+  void heap_region_do(ShenandoahHeapRegion* r) override {\n@@ -333,0 +387,50 @@\n+    if (r->is_humongous()) {\n+      _humongous_waste += r->free();\n+    }\n+    if (r->is_trash()) {\n+      _trashed_regions++;\n+    }\n+    _regions++;\n+    log_debug(gc)(\"ShenandoahCalculateRegionStatsClosure: adding \" SIZE_FORMAT \" for %s Region \" SIZE_FORMAT \", yielding: \" SIZE_FORMAT,\n+            r->used(), (r->is_humongous() ? \"humongous\" : \"regular\"), r->index(), _used);\n+  }\n+\n+  size_t used() const { return _used; }\n+  size_t committed() const { return _committed; }\n+  size_t garbage() const { return _garbage; }\n+  size_t regions() const { return _regions; }\n+  size_t waste() const { return _humongous_waste; }\n+\n+  \/\/ span is the total memory affiliated with these stats (some of which is in use and other is available)\n+  size_t span() const { return _regions * ShenandoahHeapRegion::region_size_bytes(); }\n+  size_t non_trashed_span() const { return (_regions - _trashed_regions) * ShenandoahHeapRegion::region_size_bytes(); }\n+};\n+\n+class ShenandoahGenerationStatsClosure : public ShenandoahHeapRegionClosure {\n+ public:\n+  ShenandoahCalculateRegionStatsClosure old;\n+  ShenandoahCalculateRegionStatsClosure young;\n+  ShenandoahCalculateRegionStatsClosure global;\n+\n+  void heap_region_do(ShenandoahHeapRegion* r) override {\n+    switch (r->affiliation()) {\n+      case FREE:\n+        return;\n+      case YOUNG_GENERATION:\n+        young.heap_region_do(r);\n+        global.heap_region_do(r);\n+        break;\n+      case OLD_GENERATION:\n+        old.heap_region_do(r);\n+        global.heap_region_do(r);\n+        break;\n+      default:\n+        ShouldNotReachHere();\n+    }\n+  }\n+\n+  static void log_usage(ShenandoahGeneration* generation, ShenandoahCalculateRegionStatsClosure& stats) {\n+    log_debug(gc)(\"Safepoint verification: %s verified usage: \" SIZE_FORMAT \"%s, recorded usage: \" SIZE_FORMAT \"%s\",\n+                  generation->name(),\n+                  byte_size_in_proper_unit(generation->used()), proper_unit_for_byte_size(generation->used()),\n+                  byte_size_in_proper_unit(stats.used()),       proper_unit_for_byte_size(stats.used()));\n@@ -335,3 +439,30 @@\n-  size_t used() { return _used; }\n-  size_t committed() { return _committed; }\n-  size_t garbage() { return _garbage; }\n+  static void validate_usage(const bool adjust_for_padding,\n+                             const char* label, ShenandoahGeneration* generation, ShenandoahCalculateRegionStatsClosure& stats) {\n+    ShenandoahHeap* heap = ShenandoahHeap::heap();\n+    size_t generation_used = generation->used();\n+    size_t generation_used_regions = generation->used_regions();\n+    if (adjust_for_padding && (generation->is_young() || generation->is_global())) {\n+      size_t pad = heap->old_generation()->get_pad_for_promote_in_place();\n+      generation_used += pad;\n+    }\n+\n+    guarantee(stats.used() == generation_used,\n+              \"%s: generation (%s) used size must be consistent: generation-used: \" PROPERFMT \", regions-used: \" PROPERFMT,\n+              label, generation->name(), PROPERFMTARGS(generation_used), PROPERFMTARGS(stats.used()));\n+\n+    guarantee(stats.regions() == generation_used_regions,\n+              \"%s: generation (%s) used regions (\" SIZE_FORMAT \") must equal regions that are in use (\" SIZE_FORMAT \")\",\n+              label, generation->name(), generation->used_regions(), stats.regions());\n+\n+    size_t generation_capacity = generation->max_capacity();\n+    guarantee(stats.non_trashed_span() <= generation_capacity,\n+              \"%s: generation (%s) size spanned by regions (\" SIZE_FORMAT \") * region size (\" PROPERFMT\n+              \") must not exceed current capacity (\" PROPERFMT \")\",\n+              label, generation->name(), stats.regions(), PROPERFMTARGS(ShenandoahHeapRegion::region_size_bytes()),\n+              PROPERFMTARGS(generation_capacity));\n+\n+    size_t humongous_waste = generation->get_humongous_waste();\n+    guarantee(stats.waste() == humongous_waste,\n+              \"%s: generation (%s) humongous waste must be consistent: generation: \" PROPERFMT \", regions: \" PROPERFMT,\n+              label, generation->name(), PROPERFMTARGS(humongous_waste), PROPERFMTARGS(stats.waste()));\n+  }\n@@ -369,1 +500,1 @@\n-  void heap_region_do(ShenandoahHeapRegion* r) {\n+  void heap_region_do(ShenandoahHeapRegion* r) override {\n@@ -421,2 +552,5 @@\n-    verify(r, r->get_shared_allocs() + r->get_tlab_allocs() + r->get_gclab_allocs() == r->used(),\n-           \"Accurate accounting: shared + TLAB + GCLAB = used\");\n+    verify(r, r->get_plab_allocs() <= r->capacity(),\n+           \"PLAB alloc count should not be larger than capacity\");\n+\n+    verify(r, r->get_shared_allocs() + r->get_tlab_allocs() + r->get_gclab_allocs() + r->get_plab_allocs() == r->used(),\n+           \"Accurate accounting: shared + TLAB + GCLAB + PLAB = used\");\n@@ -454,1 +588,1 @@\n-  size_t processed() {\n+  size_t processed() const {\n@@ -458,1 +592,1 @@\n-  virtual void work(uint worker_id) {\n+  void work(uint worker_id) override {\n@@ -495,0 +629,14 @@\n+class ShenandoahVerifyNoIncompleteSatbBuffers : public ThreadClosure {\n+public:\n+  void do_thread(Thread* thread) override {\n+    SATBMarkQueue& queue = ShenandoahThreadLocalData::satb_mark_queue(thread);\n+    if (!is_empty(queue)) {\n+      fatal(\"All SATB buffers should have been flushed during mark\");\n+    }\n+  }\n+private:\n+  bool is_empty(SATBMarkQueue& queue) {\n+    return queue.buffer() == nullptr || queue.index() == queue.capacity();\n+  }\n+};\n+\n@@ -504,0 +652,1 @@\n+  ShenandoahGeneration* _generation;\n@@ -517,1 +666,8 @@\n-          _processed(0) {};\n+          _processed(0),\n+          _generation(nullptr) {\n+    if (_heap->mode()->is_generational()) {\n+      _generation = _heap->gc_generation();\n+      assert(_generation != nullptr, \"Expected active generation in this mode.\");\n+      shenandoah_assert_generations_reconciled();\n+    }\n+  };\n@@ -523,1 +679,6 @@\n-  virtual void work(uint worker_id) {\n+  void work(uint worker_id) override {\n+    if (_options._verify_marked == ShenandoahVerifier::_verify_marked_complete_satb_empty) {\n+      ShenandoahVerifyNoIncompleteSatbBuffers verify_satb;\n+      Threads::threads_do(&verify_satb);\n+    }\n+\n@@ -533,0 +694,4 @@\n+        if (!in_generation(r)) {\n+          continue;\n+        }\n+\n@@ -544,0 +709,4 @@\n+  bool in_generation(ShenandoahHeapRegion* r) {\n+    return _generation == nullptr || _generation->contains(r);\n+  }\n+\n@@ -547,1 +716,1 @@\n-    if (_heap->complete_marking_context()->is_marked(cast_to_oop(obj))) {\n+    if (_heap->gc_generation()->complete_marking_context()->is_marked(cast_to_oop(obj))) {\n@@ -555,1 +724,1 @@\n-    ShenandoahMarkingContext* ctx = _heap->complete_marking_context();\n+    ShenandoahMarkingContext* ctx = _heap->gc_generation()->complete_marking_context();\n@@ -614,1 +783,1 @@\n-  void do_thread(Thread* t) {\n+  void do_thread(Thread* t) override {\n@@ -616,1 +785,1 @@\n-    if (actual != _expected) {\n+    if (!verify_gc_state(actual, _expected)) {\n@@ -620,0 +789,10 @@\n+\n+  static bool verify_gc_state(char actual, char expected) {\n+    \/\/ Old generation marking is allowed in all states.\n+    if (ShenandoahHeap::heap()->mode()->is_generational()) {\n+      return ((actual & ~(ShenandoahHeap::OLD_MARKING | ShenandoahHeap::MARKING)) == expected);\n+    } else {\n+      assert((actual & ShenandoahHeap::OLD_MARKING) == 0, \"Should not mark old in non-generational mode\");\n+      return (actual == expected);\n+    }\n+  }\n@@ -622,2 +801,4 @@\n-void ShenandoahVerifier::verify_at_safepoint(const char *label,\n-                                             VerifyForwarded forwarded, VerifyMarked marked,\n+void ShenandoahVerifier::verify_at_safepoint(const char* label,\n+                                             VerifyRememberedSet remembered,\n+                                             VerifyForwarded forwarded,\n+                                             VerifyMarked marked,\n@@ -625,1 +806,3 @@\n-                                             VerifyLiveness liveness, VerifyRegions regions,\n+                                             VerifyLiveness liveness,\n+                                             VerifyRegions regions,\n+                                             VerifySize sizeness,\n@@ -630,1 +813,1 @@\n-  ShenandoahHeap::heap()->propagate_gc_state_to_java_threads();\n+  ShenandoahHeap::heap()->propagate_gc_state_to_all_threads();\n@@ -649,1 +832,1 @@\n-      case _verify_gcstate_evacuation:\n+      case _verify_gcstate_updating:\n@@ -651,5 +834,1 @@\n-        expected = ShenandoahHeap::HAS_FORWARDED | ShenandoahHeap::EVACUATION;\n-        if (!_heap->is_stw_gc_in_progress()) {\n-          \/\/ Only concurrent GC sets this.\n-          expected |= ShenandoahHeap::WEAK_ROOTS;\n-        }\n+        expected = ShenandoahHeap::HAS_FORWARDED | ShenandoahHeap::UPDATE_REFS;\n@@ -676,1 +855,7 @@\n-      if (actual != expected) {\n+\n+      bool is_marking = (actual & ShenandoahHeap::MARKING);\n+      bool is_marking_young_or_old = (actual & (ShenandoahHeap::YOUNG_MARKING | ShenandoahHeap::OLD_MARKING));\n+      assert(is_marking == is_marking_young_or_old, \"MARKING iff (YOUNG_MARKING or OLD_MARKING), gc_state is: %x\", actual);\n+\n+      \/\/ Old generation marking is allowed in all states.\n+      if (!VerifyThreadGCState::verify_gc_state(actual, expected)) {\n@@ -694,7 +879,14 @@\n-    size_t heap_used = _heap->used();\n-    guarantee(cl.used() == heap_used,\n-              \"%s: heap used size must be consistent: heap-used = \" SIZE_FORMAT \"%s, regions-used = \" SIZE_FORMAT \"%s\",\n-              label,\n-              byte_size_in_proper_unit(heap_used), proper_unit_for_byte_size(heap_used),\n-              byte_size_in_proper_unit(cl.used()), proper_unit_for_byte_size(cl.used()));\n-\n+    size_t heap_used;\n+    if (_heap->mode()->is_generational() && (sizeness == _verify_size_adjusted_for_padding)) {\n+      \/\/ Prior to evacuation, regular regions that are to be evacuated in place are padded to prevent further allocations\n+      heap_used = _heap->used() + _heap->old_generation()->get_pad_for_promote_in_place();\n+    } else if (sizeness != _verify_size_disable) {\n+      heap_used = _heap->used();\n+    }\n+    if (sizeness != _verify_size_disable) {\n+      guarantee(cl.used() == heap_used,\n+                \"%s: heap used size must be consistent: heap-used = \" SIZE_FORMAT \"%s, regions-used = \" SIZE_FORMAT \"%s\",\n+                label,\n+                byte_size_in_proper_unit(heap_used), proper_unit_for_byte_size(heap_used),\n+                byte_size_in_proper_unit(cl.used()), proper_unit_for_byte_size(cl.used()));\n+    }\n@@ -709,0 +901,55 @@\n+  log_debug(gc)(\"Safepoint verification finished heap usage verification\");\n+\n+  ShenandoahGeneration* generation;\n+  if (_heap->mode()->is_generational()) {\n+    generation = _heap->gc_generation();\n+    guarantee(generation != nullptr, \"Need to know which generation to verify.\");\n+    shenandoah_assert_generations_reconciled();\n+  } else {\n+    generation = nullptr;\n+  }\n+\n+  if (generation != nullptr) {\n+    ShenandoahHeapLocker lock(_heap->lock());\n+\n+    switch (remembered) {\n+      case _verify_remembered_disable:\n+        break;\n+      case _verify_remembered_before_marking:\n+        log_debug(gc)(\"Safepoint verification of remembered set at mark\");\n+        verify_rem_set_before_mark();\n+        break;\n+      case _verify_remembered_before_updating_references:\n+        log_debug(gc)(\"Safepoint verification of remembered set at update ref\");\n+        verify_rem_set_before_update_ref();\n+        break;\n+      case _verify_remembered_after_full_gc:\n+        log_debug(gc)(\"Safepoint verification of remembered set after full gc\");\n+        verify_rem_set_after_full_gc();\n+        break;\n+      default:\n+        fatal(\"Unhandled remembered set verification mode\");\n+    }\n+\n+    ShenandoahGenerationStatsClosure cl;\n+    _heap->heap_region_iterate(&cl);\n+\n+    if (LogTarget(Debug, gc)::is_enabled()) {\n+      ShenandoahGenerationStatsClosure::log_usage(_heap->old_generation(),    cl.old);\n+      ShenandoahGenerationStatsClosure::log_usage(_heap->young_generation(),  cl.young);\n+      ShenandoahGenerationStatsClosure::log_usage(_heap->global_generation(), cl.global);\n+    }\n+    if (sizeness == _verify_size_adjusted_for_padding) {\n+      ShenandoahGenerationStatsClosure::validate_usage(false, label, _heap->old_generation(), cl.old);\n+      ShenandoahGenerationStatsClosure::validate_usage(true, label, _heap->young_generation(), cl.young);\n+      ShenandoahGenerationStatsClosure::validate_usage(true, label, _heap->global_generation(), cl.global);\n+    } else if (sizeness == _verify_size_exact) {\n+      ShenandoahGenerationStatsClosure::validate_usage(false, label, _heap->old_generation(), cl.old);\n+      ShenandoahGenerationStatsClosure::validate_usage(false, label, _heap->young_generation(), cl.young);\n+      ShenandoahGenerationStatsClosure::validate_usage(false, label, _heap->global_generation(), cl.global);\n+    }\n+    \/\/ else: sizeness must equal _verify_size_disable\n+  }\n+\n+  log_debug(gc)(\"Safepoint verification finished remembered set verification\");\n+\n@@ -712,1 +959,5 @@\n-    _heap->heap_region_iterate(&cl);\n+    if (generation != nullptr) {\n+      generation->heap_region_iterate(&cl);\n+    } else {\n+      _heap->heap_region_iterate(&cl);\n+    }\n@@ -715,0 +966,2 @@\n+  log_debug(gc)(\"Safepoint verification finished heap region closure verification\");\n+\n@@ -739,0 +992,2 @@\n+  log_debug(gc)(\"Safepoint verification finished getting initial reachable set\");\n+\n@@ -747,2 +1002,5 @@\n-  if (ShenandoahVerifyLevel >= 4 && (marked == _verify_marked_complete || marked == _verify_marked_complete_except_references)) {\n-    guarantee(_heap->marking_context()->is_complete(), \"Marking context should be complete\");\n+  if (ShenandoahVerifyLevel >= 4 &&\n+        (marked == _verify_marked_complete ||\n+         marked == _verify_marked_complete_except_references ||\n+         marked == _verify_marked_complete_satb_empty)) {\n+    guarantee(_heap->gc_generation()->is_mark_complete(), \"Marking context should be complete\");\n@@ -756,0 +1014,2 @@\n+  log_debug(gc)(\"Safepoint verification finished walking marked objects\");\n+\n@@ -762,0 +1022,3 @@\n+      if (generation != nullptr && !generation->contains(r)) {\n+        continue;\n+      }\n@@ -785,0 +1048,3 @@\n+  log_debug(gc)(\"Safepoint verification finished accumulation of liveness data\");\n+\n+\n@@ -794,0 +1060,1 @@\n+          _verify_remembered_disable,  \/\/ do not verify remembered set\n@@ -799,0 +1066,1 @@\n+          _verify_size_exact,          \/\/ expect generation and heap sizes to match exactly\n@@ -804,1 +1072,7 @@\n-    verify_at_safepoint(\n+  VerifyRememberedSet verify_remembered_set = _verify_remembered_before_marking;\n+  if (_heap->mode()->is_generational() &&\n+      !_heap->old_generation()->is_mark_complete()) {\n+    \/\/ Before marking in generational mode, remembered set can't be verified w\/o complete old marking.\n+    verify_remembered_set = _verify_remembered_disable;\n+  }\n+  verify_at_safepoint(\n@@ -806,0 +1080,2 @@\n+          verify_remembered_set,\n+                                       \/\/ verify read-only remembered set from bottom() to top()\n@@ -811,0 +1087,1 @@\n+          _verify_size_exact,          \/\/ expect generation and heap sizes to match exactly\n@@ -818,6 +1095,23 @@\n-          _verify_forwarded_none,      \/\/ no forwarded references\n-          _verify_marked_complete_except_references, \/\/ bitmaps as precise as we can get, except dangling j.l.r.Refs\n-          _verify_cset_none,           \/\/ no references to cset anymore\n-          _verify_liveness_complete,   \/\/ liveness data must be complete here\n-          _verify_regions_disable,     \/\/ trash regions not yet recycled\n-          _verify_gcstate_stable_weakroots  \/\/ heap is still stable, weakroots are in progress\n+          _verify_remembered_disable,         \/\/ do not verify remembered set\n+          _verify_forwarded_none,             \/\/ no forwarded references\n+          _verify_marked_complete_satb_empty, \/\/ bitmaps as precise as we can get, except dangling j.l.r.Refs\n+          _verify_cset_none,                  \/\/ no references to cset anymore\n+          _verify_liveness_complete,          \/\/ liveness data must be complete here\n+          _verify_regions_disable,            \/\/ trash regions not yet recycled\n+          _verify_size_exact,                 \/\/ expect generation and heap sizes to match exactly\n+          _verify_gcstate_stable_weakroots    \/\/ heap is still stable, weakroots are in progress\n+  );\n+}\n+\n+void ShenandoahVerifier::verify_after_concmark_with_promotions() {\n+  verify_at_safepoint(\n+          \"After Mark\",\n+          _verify_remembered_disable,         \/\/ do not verify remembered set\n+          _verify_forwarded_none,             \/\/ no forwarded references\n+          _verify_marked_complete_satb_empty, \/\/ bitmaps as precise as we can get, except dangling j.l.r.Refs\n+          _verify_cset_none,                  \/\/ no references to cset anymore\n+          _verify_liveness_complete,          \/\/ liveness data must be complete here\n+          _verify_regions_disable,            \/\/ trash regions not yet recycled\n+          _verify_size_adjusted_for_padding,  \/\/ expect generation and heap sizes to match after adjustments\n+                                              \/\/ for promote in place padding\n+          _verify_gcstate_stable_weakroots    \/\/ heap is still stable, weakroots are in progress\n@@ -830,0 +1124,1 @@\n+          _verify_remembered_disable,                \/\/ do not verify remembered set\n@@ -835,0 +1130,2 @@\n+          _verify_size_adjusted_for_padding,         \/\/ expect generation and heap sizes to match after adjustments\n+                                                     \/\/  for promote in place padding\n@@ -839,25 +1136,6 @@\n-void ShenandoahVerifier::verify_during_evacuation() {\n-  verify_at_safepoint(\n-          \"During Evacuation\",\n-          _verify_forwarded_allow,    \/\/ some forwarded references are allowed\n-          _verify_marked_disable,     \/\/ walk only roots\n-          _verify_cset_disable,       \/\/ some cset references are not forwarded yet\n-          _verify_liveness_disable,   \/\/ liveness data might be already stale after pre-evacs\n-          _verify_regions_disable,    \/\/ trash regions not yet recycled\n-          _verify_gcstate_evacuation  \/\/ evacuation is in progress\n-  );\n-}\n-\n-void ShenandoahVerifier::verify_after_evacuation() {\n-  verify_at_safepoint(\n-          \"After Evacuation\",\n-          _verify_forwarded_allow,     \/\/ objects are still forwarded\n-          _verify_marked_complete,     \/\/ bitmaps might be stale, but alloc-after-mark should be well\n-          _verify_cset_forwarded,      \/\/ all cset refs are fully forwarded\n-          _verify_liveness_disable,    \/\/ no reliable liveness data anymore\n-          _verify_regions_notrash,     \/\/ trash regions have been recycled already\n-          _verify_gcstate_forwarded    \/\/ evacuation produced some forwarded objects\n-  );\n-}\n-\n-void ShenandoahVerifier::verify_before_updaterefs() {\n+void ShenandoahVerifier::verify_before_update_refs() {\n+  VerifyRememberedSet verify_remembered_set = _verify_remembered_before_updating_references;\n+  if (_heap->mode()->is_generational() &&\n+      !_heap->old_generation()->is_mark_complete()) {\n+    verify_remembered_set = _verify_remembered_disable;\n+  }\n@@ -866,0 +1144,1 @@\n+          verify_remembered_set,        \/\/ verify read-write remembered set\n@@ -871,1 +1150,2 @@\n-          _verify_gcstate_forwarded    \/\/ evacuation should have produced some forwarded objects\n+          _verify_size_exact,          \/\/ expect generation and heap sizes to match exactly\n+          _verify_gcstate_updating     \/\/ evacuation should have produced some forwarded objects\n@@ -875,1 +1155,2 @@\n-void ShenandoahVerifier::verify_after_updaterefs() {\n+\/\/ We have not yet cleanup (reclaimed) the collection set\n+void ShenandoahVerifier::verify_after_update_refs() {\n@@ -878,0 +1159,1 @@\n+          _verify_remembered_disable,  \/\/ do not verify remembered set\n@@ -883,0 +1165,1 @@\n+          _verify_size_exact,          \/\/ expect generation and heap sizes to match exactly\n@@ -890,0 +1173,1 @@\n+          _verify_remembered_disable,  \/\/ do not verify remembered set\n@@ -895,0 +1179,1 @@\n+          _verify_size_exact,          \/\/ expect generation and heap sizes to match exactly\n@@ -902,0 +1187,1 @@\n+          _verify_remembered_disable,  \/\/ do not verify remembered set\n@@ -907,0 +1193,1 @@\n+          _verify_size_disable,        \/\/ if we degenerate during evacuation, usage not valid: padding and deferred accounting\n@@ -914,0 +1201,1 @@\n+          _verify_remembered_after_full_gc,  \/\/ verify read-write remembered set\n@@ -915,1 +1203,1 @@\n-          _verify_marked_complete,     \/\/ all objects are marked in complete bitmap\n+          _verify_marked_incomplete,   \/\/ all objects are marked in incomplete bitmap\n@@ -919,0 +1207,1 @@\n+          _verify_size_exact,           \/\/ expect generation and heap sizes to match exactly\n@@ -923,1 +1212,1 @@\n-class ShenandoahVerifyNoForwared : public OopClosure {\n+class ShenandoahVerifyNoForwarded : public BasicOopIterateClosure {\n@@ -943,1 +1232,1 @@\n-class ShenandoahVerifyInToSpaceClosure : public OopClosure {\n+class ShenandoahVerifyInToSpaceClosure : public BasicOopIterateClosure {\n@@ -952,1 +1241,1 @@\n-      if (!heap->marking_context()->is_marked(obj)) {\n+      if (!heap->marking_context()->is_marked_or_old(obj)) {\n@@ -971,2 +1260,2 @@\n-  void do_oop(narrowOop* p) { do_oop_work(p); }\n-  void do_oop(oop* p)       { do_oop_work(p); }\n+  void do_oop(narrowOop* p) override { do_oop_work(p); }\n+  void do_oop(oop* p)       override { do_oop_work(p); }\n@@ -981,1 +1270,1 @@\n-  ShenandoahVerifyNoForwared cl;\n+  ShenandoahVerifyNoForwarded cl;\n@@ -984,0 +1273,160 @@\n+\n+template<typename Scanner>\n+class ShenandoahVerifyRemSetClosure : public BasicOopIterateClosure {\n+protected:\n+  ShenandoahGenerationalHeap* const _heap;\n+  Scanner*   const _scanner;\n+  const char* _message;\n+\n+public:\n+  \/\/ Argument distinguishes between initial mark or start of update refs verification.\n+  explicit ShenandoahVerifyRemSetClosure(Scanner* scanner, const char* message) :\n+            _heap(ShenandoahGenerationalHeap::heap()),\n+            _scanner(scanner),\n+            _message(message) {}\n+\n+  template<class T>\n+  inline void work(T* p) {\n+    T o = RawAccess<>::oop_load(p);\n+    if (!CompressedOops::is_null(o)) {\n+      oop obj = CompressedOops::decode_not_null(o);\n+      if (_heap->is_in_young(obj) && !_scanner->is_card_dirty((HeapWord*) p)) {\n+        ShenandoahAsserts::print_failure(ShenandoahAsserts::_safe_all, obj, p, nullptr,\n+                                         _message, \"clean card, it should be dirty.\", __FILE__, __LINE__);\n+      }\n+    }\n+  }\n+\n+  void do_oop(narrowOop* p) override { work(p); }\n+  void do_oop(oop* p)       override { work(p); }\n+};\n+\n+template<typename Scanner>\n+void ShenandoahVerifier::help_verify_region_rem_set(Scanner* scanner, ShenandoahHeapRegion* r,\n+                                                    HeapWord* registration_watermark, const char* message) {\n+  shenandoah_assert_generations_reconciled();\n+  ShenandoahOldGeneration* old_gen = _heap->old_generation();\n+  assert(old_gen->is_mark_complete() || old_gen->is_parsable(), \"Sanity\");\n+\n+  ShenandoahMarkingContext* ctx = old_gen->is_mark_complete() ? old_gen->complete_marking_context() : nullptr;\n+  ShenandoahVerifyRemSetClosure<Scanner> check_interesting_pointers(scanner, message);\n+  HeapWord* from = r->bottom();\n+  HeapWord* obj_addr = from;\n+  if (r->is_humongous_start()) {\n+    oop obj = cast_to_oop(obj_addr);\n+    if ((ctx == nullptr) || ctx->is_marked(obj)) {\n+      \/\/ For humongous objects, the typical object is an array, so the following checks may be overkill\n+      \/\/ For regular objects (not object arrays), if the card holding the start of the object is dirty,\n+      \/\/ we do not need to verify that cards spanning interesting pointers within this object are dirty.\n+      if (!scanner->is_card_dirty(obj_addr) || obj->is_objArray()) {\n+        obj->oop_iterate(&check_interesting_pointers);\n+      }\n+      \/\/ else, object's start is marked dirty and obj is not an objArray, so any interesting pointers are covered\n+    }\n+    \/\/ else, this humongous object is not live so no need to verify its internal pointers\n+\n+    if ((obj_addr < registration_watermark) && !scanner->verify_registration(obj_addr, ctx)) {\n+      ShenandoahAsserts::print_failure(ShenandoahAsserts::_safe_all, obj, obj_addr, nullptr, message,\n+                                       \"object not properly registered\", __FILE__, __LINE__);\n+    }\n+  } else if (!r->is_humongous()) {\n+    HeapWord* top = r->top();\n+    while (obj_addr < top) {\n+      oop obj = cast_to_oop(obj_addr);\n+      \/\/ ctx->is_marked() returns true if mark bit set or if obj above TAMS.\n+      if ((ctx == nullptr) || ctx->is_marked(obj)) {\n+        \/\/ For regular objects (not object arrays), if the card holding the start of the object is dirty,\n+        \/\/ we do not need to verify that cards spanning interesting pointers within this object are dirty.\n+        if (!scanner->is_card_dirty(obj_addr) || obj->is_objArray()) {\n+          obj->oop_iterate(&check_interesting_pointers);\n+        }\n+        \/\/ else, object's start is marked dirty and obj is not an objArray, so any interesting pointers are covered\n+\n+        if ((obj_addr < registration_watermark) && !scanner->verify_registration(obj_addr, ctx)) {\n+          ShenandoahAsserts::print_failure(ShenandoahAsserts::_safe_all, obj, obj_addr, nullptr, message,\n+                                           \"object not properly registered\", __FILE__, __LINE__);\n+        }\n+        obj_addr += obj->size();\n+      } else {\n+        \/\/ This object is not live so we don't verify dirty cards contained therein\n+        HeapWord* tams = ctx->top_at_mark_start(r);\n+        obj_addr = ctx->get_next_marked_addr(obj_addr, tams);\n+      }\n+    }\n+  }\n+}\n+\n+class ShenandoahWriteTableScanner {\n+private:\n+  ShenandoahScanRemembered* _scanner;\n+public:\n+  explicit ShenandoahWriteTableScanner(ShenandoahScanRemembered* scanner) : _scanner(scanner) {}\n+\n+  bool is_card_dirty(HeapWord* obj_addr) {\n+    return _scanner->is_write_card_dirty(obj_addr);\n+  }\n+\n+  bool verify_registration(HeapWord* obj_addr, ShenandoahMarkingContext* ctx) {\n+    return _scanner->verify_registration(obj_addr, ctx);\n+  }\n+};\n+\n+\/\/ Assure that the remember set has a dirty card everywhere there is an interesting pointer.\n+\/\/ This examines the read_card_table between bottom() and top() since all PLABS are retired\n+\/\/ before the safepoint for init_mark.  Actually, we retire them before update-references and don't\n+\/\/ restore them until the start of evacuation.\n+void ShenandoahVerifier::verify_rem_set_before_mark() {\n+  shenandoah_assert_safepoint();\n+  shenandoah_assert_generational();\n+\n+  ShenandoahOldGeneration* old_generation = _heap->old_generation();\n+\n+  log_debug(gc)(\"Verifying remembered set at %s mark\", old_generation->is_doing_mixed_evacuations() ? \"mixed\" : \"young\");\n+\n+  ShenandoahScanRemembered* scanner = old_generation->card_scan();\n+  for (size_t i = 0, n = _heap->num_regions(); i < n; ++i) {\n+    ShenandoahHeapRegion* r = _heap->get_region(i);\n+    if (r->is_old() && r->is_active()) {\n+      help_verify_region_rem_set(scanner, r, r->end(), \"Verify init-mark remembered set violation\");\n+    }\n+  }\n+}\n+\n+void ShenandoahVerifier::verify_rem_set_after_full_gc() {\n+  shenandoah_assert_safepoint();\n+  shenandoah_assert_generational();\n+\n+  ShenandoahWriteTableScanner scanner(ShenandoahGenerationalHeap::heap()->old_generation()->card_scan());\n+  for (size_t i = 0, n = _heap->num_regions(); i < n; ++i) {\n+    ShenandoahHeapRegion* r = _heap->get_region(i);\n+    if (r->is_old() && !r->is_cset()) {\n+      help_verify_region_rem_set(&scanner, r, r->top(), \"Remembered set violation at end of Full GC\");\n+    }\n+  }\n+}\n+\n+\/\/ Assure that the remember set has a dirty card everywhere there is an interesting pointer.  Even though\n+\/\/ the update-references scan of remembered set only examines cards up to update_watermark, the remembered\n+\/\/ set should be valid through top.  This examines the write_card_table between bottom() and top() because\n+\/\/ all PLABS are retired immediately before the start of update refs.\n+void ShenandoahVerifier::verify_rem_set_before_update_ref() {\n+  shenandoah_assert_safepoint();\n+  shenandoah_assert_generational();\n+\n+  ShenandoahWriteTableScanner scanner(_heap->old_generation()->card_scan());\n+  for (size_t i = 0, n = _heap->num_regions(); i < n; ++i) {\n+    ShenandoahHeapRegion* r = _heap->get_region(i);\n+    if (r->is_old() && !r->is_cset()) {\n+      help_verify_region_rem_set(&scanner, r, r->get_update_watermark(), \"Remembered set violation at init-update-references\");\n+    }\n+  }\n+}\n+\n+void ShenandoahVerifier::verify_before_rebuilding_free_set() {\n+  ShenandoahGenerationStatsClosure cl;\n+  _heap->heap_region_iterate(&cl);\n+\n+  ShenandoahGenerationStatsClosure::validate_usage(false, \"Before free set rebuild\", _heap->old_generation(), cl.old);\n+  ShenandoahGenerationStatsClosure::validate_usage(false, \"Before free set rebuild\", _heap->young_generation(), cl.young);\n+  ShenandoahGenerationStatsClosure::validate_usage(false, \"Before free set rebuild\", _heap->global_generation(), cl.global);\n+}\n","filename":"src\/hotspot\/share\/gc\/shenandoah\/shenandoahVerifier.cpp","additions":535,"deletions":86,"binary":false,"changes":621,"status":"modified"}]}