{"files":[{"patch":"@@ -71,0 +71,2 @@\n+          - 'hs\/tier2_gc_shenandoah shenandoah tier2'\n+          - 'hs\/tier3_gc_shenandoah shenandoah tier3'\n@@ -112,0 +114,8 @@\n+          - test-name: 'hs\/tier2_gc_shenandoah shenandoah tier2'\n+            test-suite: 'test\/hotspot\/jtreg\/:tier2_gc_shenandoah'\n+            debug-suffix: -debug\n+\n+          - test-name: 'hs\/tier3_gc_shenandoah shenandoah tier3'\n+            test-suite: 'test\/hotspot\/jtreg\/:tier3_gc_shenandoah'\n+            debug-suffix: -debug\n+\n","filename":".github\/workflows\/test.yml","additions":10,"deletions":0,"binary":false,"changes":10,"status":"modified"},{"patch":"@@ -2,1 +2,1 @@\n-project=jdk-updates\n+project=shenandoah\n@@ -4,1 +4,1 @@\n-version=21.0.10\n+version=repo-shenandoah-21\n@@ -26,1 +26,1 @@\n-reviewers=1\n+committers=1\n","filename":".jcheck\/conf","additions":3,"deletions":3,"binary":false,"changes":6,"status":"modified"},{"patch":"@@ -4692,0 +4692,1 @@\n+            SHENANDOAHGC_ONLY(!BarrierSet::barrier_set()->is_a(BarrierSet::ShenandoahBarrierSet) &&)\n","filename":"src\/hotspot\/cpu\/aarch64\/aarch64.ad","additions":1,"deletions":0,"binary":false,"changes":1,"status":"modified"},{"patch":"@@ -3,0 +3,1 @@\n+ * Copyright Amazon.com Inc. or its affiliates. All Rights Reserved.\n@@ -44,2 +45,0 @@\n-  ShenandoahBarrierSet::assembler()->iu_barrier(masm->masm(), newval, rscratch2);\n-\n@@ -91,0 +90,4 @@\n+\n+      if (ShenandoahCardBarrier) {\n+        post_barrier(access, access.resolved_addr(), new_value.result());\n+      }\n@@ -105,4 +108,0 @@\n-  if (access.is_oop()) {\n-    value_opr = iu_barrier(access.gen(), value_opr, access.access_emit_info(), access.decorators());\n-  }\n-\n@@ -122,0 +121,3 @@\n+    if (ShenandoahCardBarrier) {\n+      post_barrier(access, access.resolved_addr(), result);\n+    }\n","filename":"src\/hotspot\/cpu\/aarch64\/gc\/shenandoah\/c1\/shenandoahBarrierSetC1_aarch64.cpp","additions":8,"deletions":6,"binary":false,"changes":14,"status":"modified"},{"patch":"@@ -3,0 +3,1 @@\n+ * Copyright Amazon.com Inc. or its affiliates. All Rights Reserved.\n@@ -34,0 +35,1 @@\n+#include \"gc\/shenandoah\/mode\/shenandoahMode.hpp\"\n@@ -50,1 +52,1 @@\n-    if ((ShenandoahSATBBarrier && !dest_uninitialized) || ShenandoahIUBarrier || ShenandoahLoadRefBarrier) {\n+    if ((ShenandoahSATBBarrier && !dest_uninitialized) || ShenandoahLoadRefBarrier) {\n@@ -80,0 +82,7 @@\n+void ShenandoahBarrierSetAssembler::arraycopy_epilogue(MacroAssembler* masm, DecoratorSet decorators, bool is_oop,\n+                                                       Register start, Register count, Register tmp, RegSet saved_regs) {\n+  if (ShenandoahCardBarrier && is_oop) {\n+    gen_write_ref_array_post_barrier(masm, decorators, start, count, tmp, saved_regs);\n+  }\n+}\n+\n@@ -308,8 +317,0 @@\n-void ShenandoahBarrierSetAssembler::iu_barrier(MacroAssembler* masm, Register dst, Register tmp) {\n-  if (ShenandoahIUBarrier) {\n-    __ push_call_clobbered_registers();\n-    satb_write_barrier_pre(masm, noreg, dst, rthread, tmp, rscratch1, true, false);\n-    __ pop_call_clobbered_registers();\n-  }\n-}\n-\n@@ -378,0 +379,21 @@\n+void ShenandoahBarrierSetAssembler::store_check(MacroAssembler* masm, Register obj) {\n+  assert(ShenandoahCardBarrier, \"Should have been checked by caller\");\n+\n+  __ lsr(obj, obj, CardTable::card_shift());\n+\n+  assert(CardTable::dirty_card_val() == 0, \"must be\");\n+\n+  Address curr_ct_holder_addr(rthread, in_bytes(ShenandoahThreadLocalData::card_table_offset()));\n+  __ ldr(rscratch1, curr_ct_holder_addr);\n+\n+  if (UseCondCardMark) {\n+    Label L_already_dirty;\n+    __ ldrb(rscratch2, Address(obj, rscratch1));\n+    __ cbz(rscratch2, L_already_dirty);\n+    __ strb(zr, Address(obj, rscratch1));\n+    __ bind(L_already_dirty);\n+  } else {\n+    __ strb(zr, Address(obj, rscratch1));\n+  }\n+}\n+\n@@ -406,2 +428,1 @@\n-    iu_barrier(masm, val, tmp1);\n-    \/\/ G1 barrier needs uncompressed oop for region cross check.\n+    \/\/ Barrier needs uncompressed oop for region cross check.\n@@ -414,0 +435,3 @@\n+    if (ShenandoahCardBarrier) {\n+      store_check(masm, tmp3);\n+    }\n@@ -598,0 +622,30 @@\n+void ShenandoahBarrierSetAssembler::gen_write_ref_array_post_barrier(MacroAssembler* masm, DecoratorSet decorators,\n+                                                                     Register start, Register count, Register scratch, RegSet saved_regs) {\n+  assert(ShenandoahCardBarrier, \"Should have been checked by caller\");\n+\n+  Label L_loop, L_done;\n+  const Register end = count;\n+\n+  \/\/ Zero count? Nothing to do.\n+  __ cbz(count, L_done);\n+\n+  \/\/ end = start + count << LogBytesPerHeapOop\n+  \/\/ last element address to make inclusive\n+  __ lea(end, Address(start, count, Address::lsl(LogBytesPerHeapOop)));\n+  __ sub(end, end, BytesPerHeapOop);\n+  __ lsr(start, start, CardTable::card_shift());\n+  __ lsr(end, end, CardTable::card_shift());\n+\n+  \/\/ number of bytes to copy\n+  __ sub(count, end, start);\n+\n+  Address curr_ct_holder_addr(rthread, in_bytes(ShenandoahThreadLocalData::card_table_offset()));\n+  __ ldr(scratch, curr_ct_holder_addr);\n+  __ add(start, start, scratch);\n+  __ bind(L_loop);\n+  __ strb(zr, Address(start, count));\n+  __ subs(count, count, 1);\n+  __ br(Assembler::GE, L_loop);\n+  __ bind(L_done);\n+}\n+\n","filename":"src\/hotspot\/cpu\/aarch64\/gc\/shenandoah\/shenandoahBarrierSetAssembler_aarch64.cpp","additions":65,"deletions":11,"binary":false,"changes":76,"status":"modified"},{"patch":"@@ -3,0 +3,1 @@\n+ * Copyright Amazon.com Inc. or its affiliates. All Rights Reserved.\n@@ -58,0 +59,2 @@\n+  void store_check(MacroAssembler* masm, Register obj);\n+\n@@ -62,3 +65,3 @@\n-public:\n-\n-  void iu_barrier(MacroAssembler* masm, Register dst, Register tmp);\n+  void gen_write_ref_array_post_barrier(MacroAssembler* masm, DecoratorSet decorators,\n+                                        Register start, Register count,\n+                                        Register scratch, RegSet saved_regs);\n@@ -66,1 +69,2 @@\n-  virtual NMethodPatchingType nmethod_patching_type() { return NMethodPatchingType::conc_data_patch; }\n+public:\n+  virtual NMethodPatchingType nmethod_patching_type() { return NMethodPatchingType::conc_instruction_and_data_patch; }\n@@ -77,0 +81,2 @@\n+  virtual void arraycopy_epilogue(MacroAssembler* masm, DecoratorSet decorators, bool is_oop,\n+                                  Register start, Register count, Register tmp, RegSet saved_regs);\n","filename":"src\/hotspot\/cpu\/aarch64\/gc\/shenandoah\/shenandoahBarrierSetAssembler_aarch64.hpp","additions":10,"deletions":4,"binary":false,"changes":14,"status":"modified"},{"patch":"@@ -46,5 +46,0 @@\n-  if (ShenandoahIUBarrier) {\n-    ShenandoahBarrierSet::assembler()->iu_barrier(masm->masm(), new_val, tmp1, tmp2,\n-                                                  MacroAssembler::PRESERVATION_FRAME_LR_GP_FP_REGS);\n-  }\n-\n@@ -110,0 +105,4 @@\n+      if (ShenandoahCardBarrier) {\n+        post_barrier(access, access.resolved_addr(), new_value.result());\n+      }\n+\n@@ -125,4 +124,0 @@\n-  if (access.is_oop()) {\n-    value_opr = iu_barrier(access.gen(), value_opr, access.access_emit_info(), access.decorators());\n-  }\n-\n@@ -144,0 +139,4 @@\n+\n+    if (ShenandoahCardBarrier) {\n+      post_barrier(access, access.resolved_addr(), result);\n+    }\n","filename":"src\/hotspot\/cpu\/ppc\/gc\/shenandoah\/c1\/shenandoahBarrierSetC1_ppc.cpp","additions":8,"deletions":9,"binary":false,"changes":17,"status":"modified"},{"patch":"@@ -3,1 +3,1 @@\n- * Copyright (c) 2012, 2021 SAP SE. All rights reserved.\n+ * Copyright (c) 2012, 2022 SAP SE. All rights reserved.\n@@ -40,0 +40,1 @@\n+#include \"gc\/shenandoah\/mode\/shenandoahMode.hpp\"\n@@ -64,14 +65,0 @@\n-void ShenandoahBarrierSetAssembler::iu_barrier(MacroAssembler *masm,\n-                                               Register val,\n-                                               Register tmp1, Register tmp2,\n-                                               MacroAssembler::PreservationLevel preservation_level,\n-                                               DecoratorSet decorators) {\n-  \/\/ IU barriers are also employed to avoid resurrection of weak references,\n-  \/\/ even if Shenandoah does not operate in incremental update mode.\n-  if (ShenandoahIUBarrier || ShenandoahSATBBarrier) {\n-    __ block_comment(\"iu_barrier (shenandoahgc) {\");\n-    satb_write_barrier_impl(masm, decorators, noreg, noreg, val, tmp1, tmp2, preservation_level);\n-    __ block_comment(\"} iu_barrier (shenandoahgc)\");\n-  }\n-}\n-\n@@ -93,2 +80,0 @@\n-  __ block_comment(\"arraycopy_prologue (shenandoahgc) {\");\n-\n@@ -113,1 +98,1 @@\n-  if ((!ShenandoahSATBBarrier || dest_uninitialized) && !ShenandoahIUBarrier && !ShenandoahLoadRefBarrier) {\n+  if ((!ShenandoahSATBBarrier || dest_uninitialized) && !ShenandoahLoadRefBarrier) {\n@@ -117,0 +102,1 @@\n+  __ block_comment(\"arraycopy_prologue (shenandoahgc) {\");\n@@ -190,0 +176,10 @@\n+void ShenandoahBarrierSetAssembler::arraycopy_epilogue(MacroAssembler* masm, DecoratorSet decorators, BasicType type,\n+                                                       Register dst, Register count,\n+                                                       Register preserve) {\n+  if (ShenandoahCardBarrier && is_reference_type(type)) {\n+    __ block_comment(\"arraycopy_epilogue (shenandoahgc) {\");\n+    gen_write_ref_array_post_barrier(masm, decorators, dst, count, preserve);\n+    __ block_comment(\"} arraycopy_epilogue (shenandoahgc)\");\n+  }\n+}\n+\n@@ -585,1 +581,5 @@\n-    iu_barrier(masm, dst, tmp1, tmp2, preservation_level);\n+    if (ShenandoahSATBBarrier) {\n+      __ block_comment(\"keep_alive_barrier (shenandoahgc) {\");\n+      satb_write_barrier_impl(masm, 0, noreg, noreg, dst, tmp1, tmp2, preservation_level);\n+      __ block_comment(\"} keep_alive_barrier (shenandoahgc)\");\n+    }\n@@ -589,0 +589,16 @@\n+void ShenandoahBarrierSetAssembler::store_check(MacroAssembler* masm, Register base, RegisterOrConstant ind_or_offs, Register tmp) {\n+  assert(ShenandoahCardBarrier, \"Should have been checked by caller\");\n+  assert_different_registers(base, tmp, R0);\n+\n+  if (ind_or_offs.is_constant()) {\n+    __ add_const_optimized(base, base, ind_or_offs.as_constant(), tmp);\n+  } else {\n+    __ add(base, ind_or_offs.as_register(), base);\n+  }\n+\n+  __ ld(tmp, in_bytes(ShenandoahThreadLocalData::card_table_offset()), R16_thread); \/* tmp = *[R16_thread + card_table_offset] *\/\n+  __ srdi(base, base, CardTable::card_shift());\n+  __ li(R0, CardTable::dirty_card_val());\n+  __ stbx(R0, tmp, base);\n+}\n+\n@@ -600,4 +616,0 @@\n-\n-    if (ShenandoahIUBarrier && val != noreg) {\n-      iu_barrier(masm, val, tmp1, tmp2, preservation_level, decorators);\n-    }\n@@ -611,0 +623,5 @@\n+\n+  \/\/ No need for post barrier if storing NULL\n+  if (ShenandoahCardBarrier && is_reference_type(type) && val != noreg) {\n+    store_check(masm, base, ind_or_offs, tmp1);\n+  }\n@@ -760,0 +777,35 @@\n+void ShenandoahBarrierSetAssembler::gen_write_ref_array_post_barrier(MacroAssembler* masm, DecoratorSet decorators,\n+                                                                     Register addr, Register count, Register preserve) {\n+  assert(ShenandoahCardBarrier, \"Should have been checked by caller\");\n+\n+  ShenandoahBarrierSet* bs = ShenandoahBarrierSet::barrier_set();\n+  CardTable* ct = bs->card_table();\n+  assert_different_registers(addr, count, R0);\n+\n+  Label L_skip_loop, L_store_loop;\n+\n+  __ sldi_(count, count, LogBytesPerHeapOop);\n+\n+  \/\/ Zero length? Skip.\n+  __ beq(CCR0, L_skip_loop);\n+\n+  __ addi(count, count, -BytesPerHeapOop);\n+  __ add(count, addr, count);\n+  \/\/ Use two shifts to clear out those low order two bits! (Cannot opt. into 1.)\n+  __ srdi(addr, addr, CardTable::card_shift());\n+  __ srdi(count, count, CardTable::card_shift());\n+  __ subf(count, addr, count);\n+  __ ld(R0, in_bytes(ShenandoahThreadLocalData::card_table_offset()), R16_thread);\n+  __ add(addr, addr, R0);\n+  __ addi(count, count, 1);\n+  __ li(R0, 0);\n+  __ mtctr(count);\n+\n+  \/\/ Byte store loop\n+  __ bind(L_store_loop);\n+  __ stb(R0, 0, addr);\n+  __ addi(addr, addr, 1);\n+  __ bdnz(L_store_loop);\n+  __ bind(L_skip_loop);\n+}\n+\n","filename":"src\/hotspot\/cpu\/ppc\/gc\/shenandoah\/shenandoahBarrierSetAssembler_ppc.cpp","additions":75,"deletions":23,"binary":false,"changes":98,"status":"modified"},{"patch":"@@ -54,0 +54,4 @@\n+  void store_check(MacroAssembler* masm,\n+                   Register base, RegisterOrConstant ind_or_offs,\n+                   Register tmp);\n+\n@@ -63,0 +67,4 @@\n+  void gen_write_ref_array_post_barrier(MacroAssembler* masm, DecoratorSet decorators,\n+                                        Register addr, Register count,\n+                                        Register preserve);\n+\n@@ -64,1 +72,1 @@\n-  virtual NMethodPatchingType nmethod_patching_type() { return NMethodPatchingType::conc_data_patch; }\n+  virtual NMethodPatchingType nmethod_patching_type() { return NMethodPatchingType::conc_instruction_and_data_patch; }\n@@ -85,5 +93,0 @@\n-  void iu_barrier(MacroAssembler* masm,\n-                        Register val,\n-                        Register tmp1, Register tmp2,\n-                        MacroAssembler::PreservationLevel preservation_level, DecoratorSet decorators = 0);\n-\n@@ -103,1 +106,5 @@\n-                          Register src, Register dst, Register count, Register preserve1, Register preserve2);\n+                                  Register src, Register dst, Register count,\n+                                  Register preserve1, Register preserve2);\n+  virtual void arraycopy_epilogue(MacroAssembler* masm, DecoratorSet decorators, BasicType type,\n+                                  Register dst, Register count,\n+                                  Register preserve);\n","filename":"src\/hotspot\/cpu\/ppc\/gc\/shenandoah\/shenandoahBarrierSetAssembler_ppc.hpp","additions":14,"deletions":7,"binary":false,"changes":21,"status":"modified"},{"patch":"@@ -44,2 +44,0 @@\n-  ShenandoahBarrierSet::assembler()->iu_barrier(masm->masm(), newval, t1);\n-\n@@ -97,4 +95,0 @@\n-  if (access.is_oop()) {\n-    value_opr = iu_barrier(access.gen(), value_opr, access.access_emit_info(), access.decorators());\n-  }\n-\n","filename":"src\/hotspot\/cpu\/riscv\/gc\/shenandoah\/c1\/shenandoahBarrierSetC1_riscv.cpp","additions":0,"deletions":6,"binary":false,"changes":6,"status":"modified"},{"patch":"@@ -51,1 +51,1 @@\n-    if ((ShenandoahSATBBarrier && !dest_uninitialized) || ShenandoahIUBarrier || ShenandoahLoadRefBarrier) {\n+    if ((ShenandoahSATBBarrier && !dest_uninitialized) || ShenandoahLoadRefBarrier) {\n@@ -84,0 +84,7 @@\n+void ShenandoahBarrierSetAssembler::arraycopy_epilogue(MacroAssembler* masm, DecoratorSet decorators, bool is_oop,\n+                                                       Register start, Register count, Register tmp, RegSet saved_regs) {\n+  if (ShenandoahCardBarrier && is_oop) {\n+    gen_write_ref_array_post_barrier(masm, decorators, start, count, tmp, saved_regs);\n+  }\n+}\n+\n@@ -115,1 +122,0 @@\n-  Address in_progress(thread, in_bytes(ShenandoahThreadLocalData::satb_mark_queue_active_offset()));\n@@ -120,7 +126,4 @@\n-  if (in_bytes(SATBMarkQueue::byte_width_of_active()) == 4) {\n-    __ lwu(tmp1, in_progress);\n-  } else {\n-    assert(in_bytes(SATBMarkQueue::byte_width_of_active()) == 1, \"Assumption\");\n-    __ lbu(tmp1, in_progress);\n-  }\n-  __ beqz(tmp1, done);\n+  Address gc_state(xthread, in_bytes(ShenandoahThreadLocalData::gc_state_offset()));\n+  __ lbu(t1, gc_state);\n+  __ test_bit(t1, t1, ShenandoahHeap::MARKING_BITPOS);\n+  __ beqz(t1, done);\n@@ -142,1 +145,1 @@\n-  __ sub(tmp1, tmp1, wordSize);        \/\/ tmp := tmp - wordSize\n+  __ sub(tmp1, tmp1, wordSize);       \/\/ tmp := tmp - wordSize\n@@ -315,10 +318,0 @@\n-void ShenandoahBarrierSetAssembler::iu_barrier(MacroAssembler* masm, Register dst, Register tmp) {\n-  if (ShenandoahIUBarrier) {\n-    __ push_call_clobbered_registers();\n-\n-    satb_write_barrier_pre(masm, noreg, dst, xthread, tmp, t0, true, false);\n-\n-    __ pop_call_clobbered_registers();\n-  }\n-}\n-\n@@ -399,0 +392,22 @@\n+void ShenandoahBarrierSetAssembler::store_check(MacroAssembler* masm, Register obj) {\n+  assert(ShenandoahCardBarrier, \"Did you mean to enable ShenandoahCardBarrier?\");\n+\n+  __ srli(obj, obj, CardTable::card_shift());\n+\n+  assert(CardTable::dirty_card_val() == 0, \"must be\");\n+\n+  Address curr_ct_holder_addr(xthread, in_bytes(ShenandoahThreadLocalData::card_table_offset()));\n+  __ ld(t1, curr_ct_holder_addr);\n+  __ add(t1, obj, t1);\n+\n+  if (UseCondCardMark) {\n+    Label L_already_dirty;\n+    __ lbu(t0, Address(t1));\n+    __ beqz(t0, L_already_dirty);\n+    __ sb(zr, Address(t1));\n+    __ bind(L_already_dirty);\n+  } else {\n+    __ sb(zr, Address(t1));\n+  }\n+}\n+\n@@ -424,11 +439,6 @@\n-  if (val == noreg) {\n-    BarrierSetAssembler::store_at(masm, decorators, type, Address(tmp3, 0), noreg, noreg, noreg, noreg);\n-  } else {\n-    iu_barrier(masm, val, tmp1);\n-    \/\/ G1 barrier needs uncompressed oop for region cross check.\n-    Register new_val = val;\n-    if (UseCompressedOops) {\n-      new_val = t1;\n-      __ mv(new_val, val);\n-    }\n-    BarrierSetAssembler::store_at(masm, decorators, type, Address(tmp3, 0), val, noreg, noreg, noreg);\n+  BarrierSetAssembler::store_at(masm, decorators, type, Address(tmp3, 0), val, noreg, noreg, noreg);\n+\n+  bool in_heap = (decorators & IN_HEAP) != 0;\n+  bool needs_post_barrier = (val != noreg) && in_heap && ShenandoahCardBarrier;\n+  if (needs_post_barrier) {\n+    store_check(masm, tmp3);\n@@ -542,0 +552,32 @@\n+void ShenandoahBarrierSetAssembler::gen_write_ref_array_post_barrier(MacroAssembler* masm, DecoratorSet decorators,\n+                                                                     Register start, Register count, Register tmp, RegSet saved_regs) {\n+  assert(ShenandoahCardBarrier, \"Did you mean to enable ShenandoahCardBarrier?\");\n+\n+  Label L_loop, L_done;\n+  const Register end = count;\n+\n+  \/\/ Zero count? Nothing to do.\n+  __ beqz(count, L_done);\n+\n+  \/\/ end = start + count << LogBytesPerHeapOop\n+  \/\/ last element address to make inclusive\n+  __ shadd(end, count, start, tmp, LogBytesPerHeapOop);\n+  __ sub(end, end, BytesPerHeapOop);\n+  __ srli(start, start, CardTable::card_shift());\n+  __ srli(end, end, CardTable::card_shift());\n+\n+  \/\/ number of bytes to copy\n+  __ sub(count, end, start);\n+\n+  Address curr_ct_holder_addr(xthread, in_bytes(ShenandoahThreadLocalData::card_table_offset()));\n+  __ ld(tmp, curr_ct_holder_addr);\n+  __ add(start, start, tmp);\n+\n+  __ bind(L_loop);\n+  __ add(tmp, start, count);\n+  __ sb(zr, Address(tmp));\n+  __ sub(count, count, 1);\n+  __ bgez(count, L_loop);\n+  __ bind(L_done);\n+}\n+\n","filename":"src\/hotspot\/cpu\/riscv\/gc\/shenandoah\/shenandoahBarrierSetAssembler_riscv.cpp","additions":73,"deletions":31,"binary":false,"changes":104,"status":"modified"},{"patch":"@@ -60,0 +60,2 @@\n+  void store_check(MacroAssembler* masm, Register obj);\n+\n@@ -64,1 +66,3 @@\n-public:\n+  void gen_write_ref_array_post_barrier(MacroAssembler* masm, DecoratorSet decorators,\n+                                        Register start, Register count,\n+                                        Register tmp, RegSet saved_regs);\n@@ -66,1 +70,1 @@\n-  void iu_barrier(MacroAssembler* masm, Register dst, Register tmp);\n+public:\n@@ -68,1 +72,1 @@\n-  virtual NMethodPatchingType nmethod_patching_type() { return NMethodPatchingType::conc_data_patch; }\n+  virtual NMethodPatchingType nmethod_patching_type() { return NMethodPatchingType::conc_instruction_and_data_patch; }\n@@ -80,0 +84,3 @@\n+  virtual void arraycopy_epilogue(MacroAssembler* masm, DecoratorSet decorators, bool is_oop,\n+                                  Register start, Register count, Register tmp, RegSet saved_regs);\n+\n","filename":"src\/hotspot\/cpu\/riscv\/gc\/shenandoah\/shenandoahBarrierSetAssembler_riscv.hpp","additions":10,"deletions":3,"binary":false,"changes":13,"status":"modified"},{"patch":"@@ -51,0 +51,1 @@\n+  SHENANDOAHGC_ONLY(assert(!UseShenandoahGC, \"Shenandoah byte_map_base is not constant.\");)\n","filename":"src\/hotspot\/cpu\/x86\/gc\/shared\/cardTableBarrierSetAssembler_x86.cpp","additions":1,"deletions":0,"binary":false,"changes":1,"status":"modified"},{"patch":"@@ -3,0 +3,1 @@\n+ * Copyright Amazon.com Inc. or its affiliates. All Rights Reserved.\n@@ -49,3 +50,0 @@\n-  \/\/ Apply IU barrier to newval.\n-  ShenandoahBarrierSet::assembler()->iu_barrier(masm->masm(), newval, tmp1);\n-\n@@ -90,0 +88,4 @@\n+\n+      if (ShenandoahCardBarrier) {\n+        post_barrier(access, access.resolved_addr(), new_value.result());\n+      }\n@@ -104,4 +106,0 @@\n-  if (access.is_oop()) {\n-    value_opr = iu_barrier(access.gen(), value_opr, access.access_emit_info(), access.decorators());\n-  }\n-\n@@ -123,0 +121,3 @@\n+    if (ShenandoahCardBarrier) {\n+      post_barrier(access, access.resolved_addr(), result);\n+    }\n","filename":"src\/hotspot\/cpu\/x86\/gc\/shenandoah\/c1\/shenandoahBarrierSetC1_x86.cpp","additions":8,"deletions":7,"binary":false,"changes":15,"status":"modified"},{"patch":"@@ -3,0 +3,1 @@\n+ * Copyright Amazon.com Inc. or its affiliates. All Rights Reserved.\n@@ -34,0 +35,1 @@\n+#include \"gc\/shenandoah\/mode\/shenandoahMode.hpp\"\n@@ -123,0 +125,4 @@\n+    if (ShenandoahCardBarrier) {\n+      bool checkcast = (decorators & ARRAYCOPY_CHECKCAST) != 0;\n+      bool disjoint = (decorators & ARRAYCOPY_DISJOINT) != 0;\n+      bool obj_int = type == T_OBJECT LP64_ONLY(&& UseCompressedOops);\n@@ -124,1 +130,20 @@\n-    if ((ShenandoahSATBBarrier && !dest_uninitialized) || ShenandoahIUBarrier || ShenandoahLoadRefBarrier) {\n+      \/\/ We need to save the original element count because the array copy stub\n+      \/\/ will destroy the value and we need it for the card marking barrier.\n+#ifdef _LP64\n+      if (!checkcast) {\n+        if (!obj_int) {\n+          \/\/ Save count for barrier\n+          __ movptr(r11, count);\n+        } else if (disjoint) {\n+          \/\/ Save dst in r11 in the disjoint case\n+          __ movq(r11, dst);\n+        }\n+      }\n+#else\n+      if (disjoint) {\n+        __ mov(rdx, dst);          \/\/ save 'to'\n+      }\n+#endif\n+    }\n+\n+    if ((ShenandoahSATBBarrier && !dest_uninitialized) || ShenandoahLoadRefBarrier) {\n@@ -143,1 +168,1 @@\n-      Label done;\n+      Label L_done;\n@@ -146,1 +171,1 @@\n-      __ jcc(Assembler::zero, done);\n+      __ jcc(Assembler::zero, L_done);\n@@ -157,1 +182,1 @@\n-      __ jcc(Assembler::zero, done);\n+      __ jcc(Assembler::zero, L_done);\n@@ -177,1 +202,1 @@\n-      __ bind(done);\n+      __ bind(L_done);\n@@ -184,0 +209,30 @@\n+void ShenandoahBarrierSetAssembler::arraycopy_epilogue(MacroAssembler* masm, DecoratorSet decorators, BasicType type,\n+                                                       Register src, Register dst, Register count) {\n+\n+  if (ShenandoahCardBarrier && is_reference_type(type)) {\n+    bool checkcast = (decorators & ARRAYCOPY_CHECKCAST) != 0;\n+    bool disjoint = (decorators & ARRAYCOPY_DISJOINT) != 0;\n+    bool obj_int = type == T_OBJECT LP64_ONLY(&& UseCompressedOops);\n+    Register tmp = rax;\n+\n+#ifdef _LP64\n+    if (!checkcast) {\n+      if (!obj_int) {\n+        \/\/ Save count for barrier\n+        count = r11;\n+      } else if (disjoint) {\n+        \/\/ Use the saved dst in the disjoint case\n+        dst = r11;\n+      }\n+    } else {\n+      tmp = rscratch1;\n+    }\n+#else\n+    if (disjoint) {\n+      __ mov(dst, rdx); \/\/ restore 'to'\n+    }\n+#endif\n+    gen_write_ref_array_post_barrier(masm, decorators, dst, count, tmp);\n+  }\n+}\n+\n@@ -222,1 +277,0 @@\n-  Address in_progress(thread, in_bytes(ShenandoahThreadLocalData::satb_mark_queue_active_offset()));\n@@ -476,34 +530,0 @@\n-void ShenandoahBarrierSetAssembler::iu_barrier(MacroAssembler* masm, Register dst, Register tmp) {\n-  if (ShenandoahIUBarrier) {\n-    iu_barrier_impl(masm, dst, tmp);\n-  }\n-}\n-\n-void ShenandoahBarrierSetAssembler::iu_barrier_impl(MacroAssembler* masm, Register dst, Register tmp) {\n-  assert(ShenandoahIUBarrier, \"should be enabled\");\n-\n-  if (dst == noreg) return;\n-\n-  if (ShenandoahIUBarrier) {\n-    save_machine_state(masm, \/* handle_gpr = *\/ true, \/* handle_fp = *\/ true);\n-\n-#ifdef _LP64\n-    Register thread = r15_thread;\n-#else\n-    Register thread = rcx;\n-    if (thread == dst || thread == tmp) {\n-      thread = rdi;\n-    }\n-    if (thread == dst || thread == tmp) {\n-      thread = rbx;\n-    }\n-    __ get_thread(thread);\n-#endif\n-    assert_different_registers(dst, tmp, thread);\n-\n-    satb_write_barrier_pre(masm, noreg, dst, thread, tmp, true, false);\n-\n-    restore_machine_state(masm, \/* handle_gpr = *\/ true, \/* handle_fp = *\/ true);\n-  }\n-}\n-\n@@ -593,0 +613,48 @@\n+void ShenandoahBarrierSetAssembler::store_check(MacroAssembler* masm, Register obj) {\n+  assert(ShenandoahCardBarrier, \"Should have been checked by caller\");\n+\n+  \/\/ Does a store check for the oop in register obj. The content of\n+  \/\/ register obj is destroyed afterwards.\n+  __ shrptr(obj, CardTable::card_shift());\n+\n+  \/\/ We'll use this register as the TLS base address and also later on\n+  \/\/ to hold the byte_map_base.\n+  Register thread = LP64_ONLY(r15_thread) NOT_LP64(rcx);\n+  Register tmp = LP64_ONLY(rscratch1) NOT_LP64(rdx);\n+\n+#ifndef _LP64\n+  \/\/ The next two ifs are just to get temporary registers to use for TLS and card table base.\n+  if (thread == obj) {\n+    thread = rdx;\n+    tmp = rsi;\n+  }\n+  if (tmp == obj) {\n+    tmp = rsi;\n+  }\n+\n+  __ push(thread);\n+  __ push(tmp);\n+  __ get_thread(thread);\n+#endif\n+\n+  Address curr_ct_holder_addr(thread, in_bytes(ShenandoahThreadLocalData::card_table_offset()));\n+  __ movptr(tmp, curr_ct_holder_addr);\n+  Address card_addr(tmp, obj, Address::times_1);\n+\n+  int dirty = CardTable::dirty_card_val();\n+  if (UseCondCardMark) {\n+    Label L_already_dirty;\n+    __ cmpb(card_addr, dirty);\n+    __ jccb(Assembler::equal, L_already_dirty);\n+    __ movb(card_addr, dirty);\n+    __ bind(L_already_dirty);\n+  } else {\n+    __ movb(card_addr, dirty);\n+  }\n+\n+#ifndef _LP64\n+  __ pop(tmp);\n+  __ pop(thread);\n+#endif\n+}\n+\n@@ -630,5 +698,6 @@\n-    if (val == noreg) {\n-      BarrierSetAssembler::store_at(masm, decorators, type, Address(tmp1, 0), val, noreg, noreg, noreg);\n-    } else {\n-      iu_barrier(masm, val, tmp3);\n-      BarrierSetAssembler::store_at(masm, decorators, type, Address(tmp1, 0), val, noreg, noreg, noreg);\n+\n+    BarrierSetAssembler::store_at(masm, decorators, type, Address(tmp1, 0), val, noreg, noreg, noreg);\n+    if (val != noreg) {\n+      if (ShenandoahCardBarrier) {\n+        store_check(masm, tmp1);\n+      }\n@@ -830,0 +899,64 @@\n+#ifdef PRODUCT\n+#define BLOCK_COMMENT(str) \/* nothing *\/\n+#else\n+#define BLOCK_COMMENT(str) __ block_comment(str)\n+#endif\n+\n+#define BIND(label) bind(label); BLOCK_COMMENT(#label \":\")\n+\n+#define TIMES_OOP (UseCompressedOops ? Address::times_4 : Address::times_8)\n+\n+void ShenandoahBarrierSetAssembler::gen_write_ref_array_post_barrier(MacroAssembler* masm, DecoratorSet decorators,\n+                                                                     Register addr, Register count,\n+                                                                     Register tmp) {\n+  assert(ShenandoahCardBarrier, \"Should have been checked by caller\");\n+\n+  Label L_loop, L_done;\n+  const Register end = count;\n+  assert_different_registers(addr, end);\n+\n+  \/\/ Zero count? Nothing to do.\n+  __ testl(count, count);\n+  __ jccb(Assembler::zero, L_done);\n+\n+#ifdef _LP64\n+  const Register thread = r15_thread;\n+  Address curr_ct_holder_addr(thread, in_bytes(ShenandoahThreadLocalData::card_table_offset()));\n+  __ movptr(tmp, curr_ct_holder_addr);\n+\n+  __ leaq(end, Address(addr, count, TIMES_OOP, 0));  \/\/ end == addr+count*oop_size\n+  __ subptr(end, BytesPerHeapOop); \/\/ end - 1 to make inclusive\n+  __ shrptr(addr, CardTable::card_shift());\n+  __ shrptr(end, CardTable::card_shift());\n+  __ subptr(end, addr); \/\/ end --> cards count\n+\n+  __ addptr(addr, tmp);\n+\n+  __ BIND(L_loop);\n+  __ movb(Address(addr, count, Address::times_1), 0);\n+  __ decrement(count);\n+  __ jccb(Assembler::greaterEqual, L_loop);\n+#else\n+  const Register thread = tmp;\n+  __ get_thread(thread);\n+\n+  Address curr_ct_holder_addr(thread, in_bytes(ShenandoahThreadLocalData::card_table_offset()));\n+  __ movptr(tmp, curr_ct_holder_addr);\n+\n+  __ lea(end, Address(addr, count, Address::times_ptr, -wordSize));\n+  __ shrptr(addr, CardTable::card_shift());\n+  __ shrptr(end,  CardTable::card_shift());\n+  __ subptr(end, addr); \/\/ end --> count\n+\n+  __ addptr(addr, tmp);\n+\n+  __ BIND(L_loop);\n+  Address cardtable(addr, count, Address::times_1, 0);\n+  __ movb(cardtable, 0);\n+  __ decrement(count);\n+  __ jccb(Assembler::greaterEqual, L_loop);\n+#endif\n+\n+  __ BIND(L_done);\n+}\n+\n","filename":"src\/hotspot\/cpu\/x86\/gc\/shenandoah\/shenandoahBarrierSetAssembler_x86.cpp","additions":178,"deletions":45,"binary":false,"changes":223,"status":"modified"},{"patch":"@@ -3,0 +3,1 @@\n+ * Copyright Amazon.com Inc. or its affiliates. All Rights Reserved.\n@@ -59,1 +60,5 @@\n-  void iu_barrier_impl(MacroAssembler* masm, Register dst, Register tmp);\n+  void store_check(MacroAssembler* masm, Register obj);\n+\n+  void gen_write_ref_array_post_barrier(MacroAssembler* masm, DecoratorSet decorators,\n+                                        Register addr, Register count,\n+                                        Register tmp);\n@@ -62,1 +67,0 @@\n-  void iu_barrier(MacroAssembler* masm, Register dst, Register tmp);\n@@ -77,0 +81,2 @@\n+  virtual void arraycopy_epilogue(MacroAssembler* masm, DecoratorSet decorators, BasicType type,\n+                                  Register src, Register dst, Register count);\n","filename":"src\/hotspot\/cpu\/x86\/gc\/shenandoah\/shenandoahBarrierSetAssembler_x86.hpp","additions":8,"deletions":2,"binary":false,"changes":10,"status":"modified"},{"patch":"@@ -30,0 +30,1 @@\n+#include \"gc\/shared\/gc_globals.hpp\"\n@@ -49,0 +50,1 @@\n+  assert(!UseShenandoahGC, \"Shenandoah byte_map_base is not constant.\");\n","filename":"src\/hotspot\/share\/ci\/ciUtilities.cpp","additions":2,"deletions":0,"binary":false,"changes":2,"status":"modified"},{"patch":"@@ -74,0 +74,9 @@\n+#ifndef PRODUCT\n+bool AgeTable::is_clear() const {\n+  for (const size_t* p = sizes; p < sizes + table_size; ++p) {\n+    if (*p != 0) return false;\n+  }\n+  return true;\n+}\n+#endif \/\/ !PRODUCT\n+\n","filename":"src\/hotspot\/share\/gc\/shared\/ageTable.cpp","additions":9,"deletions":0,"binary":false,"changes":9,"status":"modified"},{"patch":"@@ -28,0 +28,1 @@\n+#include \"memory\/allocation.hpp\"\n@@ -39,1 +40,1 @@\n-class AgeTable {\n+class AgeTable: public CHeapObj<mtGC> {\n@@ -56,0 +57,5 @@\n+#ifndef PRODUCT\n+  \/\/ check whether it's clear\n+  bool is_clear() const;\n+#endif \/\/ !PRODUCT\n+\n@@ -60,1 +66,1 @@\n-    assert(age > 0 && age < table_size, \"invalid age of object\");\n+    assert(age < table_size, \"invalid age of object\");\n@@ -64,2 +70,1 @@\n-  \/\/ Merge another age table with the current one.  Used\n-  \/\/ for parallel young generation gc.\n+  \/\/ Merge another age table with the current one.\n","filename":"src\/hotspot\/share\/gc\/shared\/ageTable.hpp","additions":9,"deletions":4,"binary":false,"changes":13,"status":"modified"},{"patch":"@@ -50,0 +50,2 @@\n+  SHENANDOAHGC_ONLY(assert(!UseShenandoahGC, \"Shenandoah byte_map_base is not constant.\");)\n+\n","filename":"src\/hotspot\/share\/gc\/shared\/c1\/cardTableBarrierSetC1.cpp","additions":2,"deletions":0,"binary":false,"changes":2,"status":"modified"},{"patch":"@@ -47,1 +47,1 @@\n-  assert(UseG1GC || UseParallelGC || UseSerialGC,\n+  assert(UseG1GC || UseParallelGC || UseSerialGC || UseShenandoahGC,\n","filename":"src\/hotspot\/share\/gc\/shared\/cardTable.cpp","additions":1,"deletions":1,"binary":false,"changes":2,"status":"modified"},{"patch":"@@ -80,0 +80,1 @@\n+    _shenandoah_humongous_allocation_failure,\n","filename":"src\/hotspot\/share\/gc\/shared\/gcCause.hpp","additions":1,"deletions":0,"binary":false,"changes":1,"status":"modified"},{"patch":"@@ -54,0 +54,5 @@\n+#if INCLUDE_SHENANDOAHGC\n+    if (ShenandoahCardBarrier) {\n+      return ShenandoahYoung;\n+    }\n+#endif\n@@ -78,0 +83,5 @@\n+#if INCLUDE_SHENANDOAHGC\n+    if (ShenandoahCardBarrier) {\n+      return ShenandoahOld;\n+    }\n+#endif\n","filename":"src\/hotspot\/share\/gc\/shared\/gcConfiguration.cpp","additions":10,"deletions":0,"binary":false,"changes":10,"status":"modified"},{"patch":"@@ -42,0 +42,2 @@\n+  ShenandoahYoung,\n+  ShenandoahOld,\n@@ -61,0 +63,2 @@\n+      case ShenandoahYoung: return \"Shenandoah Young\";\n+      case ShenandoahOld: return \"Shenandoah Old\";\n","filename":"src\/hotspot\/share\/gc\/shared\/gcName.hpp","additions":4,"deletions":0,"binary":false,"changes":4,"status":"modified"},{"patch":"@@ -3,0 +3,1 @@\n+ * Copyright Amazon.com Inc. or its affiliates. All Rights Reserved.\n@@ -28,0 +29,1 @@\n+#include \"gc\/shenandoah\/mode\/shenandoahMode.hpp\"\n@@ -180,8 +182,0 @@\n-LIR_Opr ShenandoahBarrierSetC1::iu_barrier(LIRGenerator* gen, LIR_Opr obj, CodeEmitInfo* info, DecoratorSet decorators) {\n-  if (ShenandoahIUBarrier) {\n-    obj = ensure_in_register(gen, obj, T_OBJECT);\n-    pre_barrier(gen, info, decorators, LIR_OprFact::illegalOpr, obj);\n-  }\n-  return obj;\n-}\n-\n@@ -193,1 +187,0 @@\n-    value = iu_barrier(access.gen(), value, access.access_emit_info(), access.decorators());\n@@ -196,0 +189,10 @@\n+\n+  if (ShenandoahCardBarrier && access.is_oop()) {\n+    DecoratorSet decorators = access.decorators();\n+    bool is_array = (decorators & IS_ARRAY) != 0;\n+    bool on_anonymous = (decorators & ON_UNKNOWN_OOP_REF) != 0;\n+\n+    bool precise = is_array || on_anonymous;\n+    LIR_Opr post_addr = precise ? access.resolved_addr() : access.base().opr();\n+    post_barrier(access, post_addr, value);\n+  }\n@@ -294,0 +297,55 @@\n+\n+void ShenandoahBarrierSetC1::post_barrier(LIRAccess& access, LIR_Opr addr, LIR_Opr new_val) {\n+  assert(ShenandoahCardBarrier, \"Should have been checked by caller\");\n+\n+  DecoratorSet decorators = access.decorators();\n+  LIRGenerator* gen = access.gen();\n+  bool in_heap = (decorators & IN_HEAP) != 0;\n+  if (!in_heap) {\n+    return;\n+  }\n+\n+  LIR_Opr thrd = gen->getThreadPointer();\n+  const int curr_ct_holder_offset = in_bytes(ShenandoahThreadLocalData::card_table_offset());\n+  LIR_Address* curr_ct_holder_addr = new LIR_Address(thrd, curr_ct_holder_offset, T_ADDRESS);\n+  LIR_Opr curr_ct_holder_ptr_reg = gen->new_register(T_ADDRESS);\n+  __ move(curr_ct_holder_addr, curr_ct_holder_ptr_reg);\n+\n+  if (addr->is_address()) {\n+    LIR_Address* address = addr->as_address_ptr();\n+    \/\/ ptr cannot be an object because we use this barrier for array card marks\n+    \/\/ and addr can point in the middle of an array.\n+    LIR_Opr ptr = gen->new_pointer_register();\n+    if (!address->index()->is_valid() && address->disp() == 0) {\n+      __ move(address->base(), ptr);\n+    } else {\n+      assert(address->disp() != max_jint, \"lea doesn't support patched addresses!\");\n+      __ leal(addr, ptr);\n+    }\n+    addr = ptr;\n+  }\n+  assert(addr->is_register(), \"must be a register at this point\");\n+\n+  LIR_Opr tmp = gen->new_pointer_register();\n+  if (two_operand_lir_form) {\n+    __ move(addr, tmp);\n+    __ unsigned_shift_right(tmp, CardTable::card_shift(), tmp);\n+  } else {\n+    __ unsigned_shift_right(addr, CardTable::card_shift(), tmp);\n+  }\n+\n+  LIR_Address* card_addr = new LIR_Address(curr_ct_holder_ptr_reg, tmp, T_BYTE);\n+  LIR_Opr dirty = LIR_OprFact::intConst(CardTable::dirty_card_val());\n+  if (UseCondCardMark) {\n+    LIR_Opr cur_value = gen->new_register(T_INT);\n+    __ move(card_addr, cur_value);\n+\n+    LabelObj* L_already_dirty = new LabelObj();\n+    __ cmp(lir_cond_equal, cur_value, dirty);\n+    __ branch(lir_cond_equal, L_already_dirty->label());\n+    __ move(dirty, card_addr);\n+    __ branch_destination(L_already_dirty->label());\n+  } else {\n+    __ move(dirty, card_addr);\n+  }\n+}\n","filename":"src\/hotspot\/share\/gc\/shenandoah\/c1\/shenandoahBarrierSetC1.cpp","additions":67,"deletions":9,"binary":false,"changes":76,"status":"modified"},{"patch":"@@ -203,1 +203,0 @@\n-  LIR_Opr iu_barrier(LIRGenerator* gen, LIR_Opr obj, CodeEmitInfo* info, DecoratorSet decorators);\n@@ -247,0 +246,2 @@\n+  void post_barrier(LIRAccess& access, LIR_Opr addr, LIR_Opr new_val);\n+\n","filename":"src\/hotspot\/share\/gc\/shenandoah\/c1\/shenandoahBarrierSetC1.hpp","additions":2,"deletions":1,"binary":false,"changes":3,"status":"modified"},{"patch":"@@ -2,1 +2,2 @@\n- * Copyright (c) 2018, 2021, Red Hat, Inc. All rights reserved.\n+ * Copyright (c) 2018, 2023, Red Hat, Inc. All rights reserved.\n+ * Copyright Amazon.com Inc. or its affiliates. All Rights Reserved.\n@@ -29,0 +30,1 @@\n+#include \"gc\/shenandoah\/shenandoahCardTable.hpp\"\n@@ -51,19 +53,1 @@\n-  : _iu_barriers(new (comp_arena) GrowableArray<ShenandoahIUBarrierNode*>(comp_arena, 8,  0, nullptr)),\n-    _load_reference_barriers(new (comp_arena) GrowableArray<ShenandoahLoadReferenceBarrierNode*>(comp_arena, 8,  0, nullptr)) {\n-}\n-\n-int ShenandoahBarrierSetC2State::iu_barriers_count() const {\n-  return _iu_barriers->length();\n-}\n-\n-ShenandoahIUBarrierNode* ShenandoahBarrierSetC2State::iu_barrier(int idx) const {\n-  return _iu_barriers->at(idx);\n-}\n-\n-void ShenandoahBarrierSetC2State::add_iu_barrier(ShenandoahIUBarrierNode* n) {\n-  assert(!_iu_barriers->contains(n), \"duplicate entry in barrier list\");\n-  _iu_barriers->append(n);\n-}\n-\n-void ShenandoahBarrierSetC2State::remove_iu_barrier(ShenandoahIUBarrierNode* n) {\n-  _iu_barriers->remove_if_existing(n);\n+  : _load_reference_barriers(new (comp_arena) GrowableArray<ShenandoahLoadReferenceBarrierNode*>(comp_arena, 8,  0, nullptr)) {\n@@ -91,7 +75,0 @@\n-Node* ShenandoahBarrierSetC2::shenandoah_iu_barrier(GraphKit* kit, Node* obj) const {\n-  if (ShenandoahIUBarrier) {\n-    return kit->gvn().transform(new ShenandoahIUBarrierNode(obj));\n-  }\n-  return obj;\n-}\n-\n@@ -299,0 +276,5 @@\n+bool ShenandoahBarrierSetC2::is_shenandoah_clone_call(Node* call) {\n+  return call->is_CallLeaf() &&\n+         call->as_CallLeaf()->entry_point() == CAST_FROM_FN_PTR(address, ShenandoahRuntime::shenandoah_clone_barrier);\n+}\n+\n@@ -309,1 +291,2 @@\n-         (entry_point == CAST_FROM_FN_PTR(address, ShenandoahRuntime::load_reference_barrier_phantom));\n+         (entry_point == CAST_FROM_FN_PTR(address, ShenandoahRuntime::load_reference_barrier_phantom)) ||\n+         (entry_point == CAST_FROM_FN_PTR(address, ShenandoahRuntime::load_reference_barrier_phantom_narrow));\n@@ -453,0 +436,79 @@\n+void ShenandoahBarrierSetC2::post_barrier(GraphKit* kit,\n+                                          Node* ctl,\n+                                          Node* oop_store,\n+                                          Node* obj,\n+                                          Node* adr,\n+                                          uint  adr_idx,\n+                                          Node* val,\n+                                          BasicType bt,\n+                                          bool use_precise) const {\n+  assert(ShenandoahCardBarrier, \"Should have been checked by caller\");\n+\n+  \/\/ No store check needed if we're storing a null.\n+  if (val != nullptr && val->is_Con()) {\n+    \/\/ must be either an oop or NULL\n+    const Type* t = val->bottom_type();\n+    if (t == TypePtr::NULL_PTR || t == Type::TOP)\n+      return;\n+  }\n+\n+  if (ReduceInitialCardMarks && obj == kit->just_allocated_object(kit->control())) {\n+    \/\/ We can skip marks on a freshly-allocated object in Eden.\n+    \/\/ Keep this code in sync with new_deferred_store_barrier() in runtime.cpp.\n+    \/\/ That routine informs GC to take appropriate compensating steps,\n+    \/\/ upon a slow-path allocation, so as to make this card-mark\n+    \/\/ elision safe.\n+    return;\n+  }\n+\n+  if (!use_precise) {\n+    \/\/ All card marks for a (non-array) instance are in one place:\n+    adr = obj;\n+  }\n+  \/\/ (Else it's an array (or unknown), and we want more precise card marks.)\n+  assert(adr != nullptr, \"\");\n+\n+  IdealKit ideal(kit, true);\n+\n+  Node* tls = __ thread(); \/\/ ThreadLocalStorage\n+\n+  \/\/ Convert the pointer to an int prior to doing math on it\n+  Node* cast = __ CastPX(__ ctrl(), adr);\n+\n+  Node* curr_ct_holder_offset = __ ConX(in_bytes(ShenandoahThreadLocalData::card_table_offset()));\n+  Node* curr_ct_holder_addr  = __ AddP(__ top(), tls, curr_ct_holder_offset);\n+  Node* curr_ct_base_addr = __ load( __ ctrl(), curr_ct_holder_addr, TypeRawPtr::NOTNULL, T_ADDRESS, Compile::AliasIdxRaw);\n+\n+  \/\/ Divide by card size\n+  Node* card_offset = __ URShiftX( cast, __ ConI(CardTable::card_shift()) );\n+\n+  \/\/ Combine card table base and card offset\n+  Node* card_adr = __ AddP(__ top(), curr_ct_base_addr, card_offset);\n+\n+  \/\/ Get the alias_index for raw card-mark memory\n+  int adr_type = Compile::AliasIdxRaw;\n+  Node*   zero = __ ConI(0); \/\/ Dirty card value\n+\n+  if (UseCondCardMark) {\n+    \/\/ The classic GC reference write barrier is typically implemented\n+    \/\/ as a store into the global card mark table.  Unfortunately\n+    \/\/ unconditional stores can result in false sharing and excessive\n+    \/\/ coherence traffic as well as false transactional aborts.\n+    \/\/ UseCondCardMark enables MP \"polite\" conditional card mark\n+    \/\/ stores.  In theory we could relax the load from ctrl() to\n+    \/\/ no_ctrl, but that doesn't buy much latitude.\n+    Node* card_val = __ load( __ ctrl(), card_adr, TypeInt::BYTE, T_BYTE, adr_type);\n+    __ if_then(card_val, BoolTest::ne, zero);\n+  }\n+\n+  \/\/ Smash zero into card\n+  __ store(__ ctrl(), card_adr, zero, T_BYTE, adr_type, MemNode::unordered);\n+\n+  if (UseCondCardMark) {\n+    __ end_if();\n+  }\n+\n+  \/\/ Final sync IdealKit and GraphKit.\n+  kit->final_sync(ideal);\n+}\n+\n@@ -511,3 +573,0 @@\n-    Node* value = val.node();\n-    value = shenandoah_iu_barrier(kit, value);\n-    val.set_node(value);\n@@ -516,0 +575,11 @@\n+\n+    Node* result = BarrierSetC2::store_at_resolved(access, val);\n+\n+    if (ShenandoahCardBarrier) {\n+      const bool anonymous = (decorators & ON_UNKNOWN_OOP_REF) != 0;\n+      const bool is_array = (decorators & IS_ARRAY) != 0;\n+      const bool use_precise = is_array || anonymous;\n+      post_barrier(kit, kit->control(), access.raw_access(), access.base(),\n+                   adr, adr_idx, val.node(), access.type(), use_precise);\n+    }\n+    return result;\n@@ -519,7 +589,1 @@\n-    C2OptAccess& opt_access = static_cast<C2OptAccess&>(access);\n-    PhaseGVN& gvn =  opt_access.gvn();\n-\n-    if (ShenandoahIUBarrier) {\n-      Node* enqueue = gvn.transform(new ShenandoahIUBarrierNode(val.node()));\n-      val.set_node(enqueue);\n-    }\n+    return BarrierSetC2::store_at_resolved(access, val);\n@@ -527,1 +591,0 @@\n-  return BarrierSetC2::store_at_resolved(access, val);\n@@ -598,1 +661,1 @@\n-                                                   Node* new_val, const Type* value_type) const {\n+                                                             Node* new_val, const Type* value_type) const {\n@@ -601,1 +664,0 @@\n-    new_val = shenandoah_iu_barrier(kit, new_val);\n@@ -640,0 +702,4 @@\n+    if (ShenandoahCardBarrier) {\n+      post_barrier(kit, kit->control(), access.raw_access(), access.base(),\n+                   access.addr().node(), access.alias_idx(), new_val, T_OBJECT, true);\n+    }\n@@ -649,1 +715,0 @@\n-    new_val = shenandoah_iu_barrier(kit, new_val);\n@@ -695,0 +760,4 @@\n+    if (ShenandoahCardBarrier) {\n+      post_barrier(kit, kit->control(), access.raw_access(), access.base(),\n+                   access.addr().node(), access.alias_idx(), new_val, T_OBJECT, true);\n+    }\n@@ -702,3 +771,0 @@\n-  if (access.is_oop()) {\n-    val = shenandoah_iu_barrier(kit, val);\n-  }\n@@ -711,0 +777,4 @@\n+    if (ShenandoahCardBarrier) {\n+      post_barrier(kit, kit->control(), access.raw_access(), access.base(),\n+                   access.addr().node(), access.alias_idx(), val, T_OBJECT, true);\n+    }\n@@ -720,1 +790,0 @@\n-\/\/ Support for GC barriers emitted during parsing\n@@ -722,12 +791,4 @@\n-  if (node->Opcode() == Op_ShenandoahLoadReferenceBarrier || node->Opcode() == Op_ShenandoahIUBarrier) return true;\n-  if (node->Opcode() != Op_CallLeaf && node->Opcode() != Op_CallLeafNoFP) {\n-    return false;\n-  }\n-  CallLeafNode *call = node->as_CallLeaf();\n-  if (call->_name == nullptr) {\n-    return false;\n-  }\n-\n-  return strcmp(call->_name, \"shenandoah_clone_barrier\") == 0 ||\n-         strcmp(call->_name, \"shenandoah_cas_obj\") == 0 ||\n-         strcmp(call->_name, \"shenandoah_wb_pre\") == 0;\n+  return (node->Opcode() == Op_ShenandoahLoadReferenceBarrier) ||\n+         is_shenandoah_lrb_call(node) ||\n+         is_shenandoah_wb_pre_call(node) ||\n+         is_shenandoah_clone_call(node);\n@@ -743,3 +804,0 @@\n-  if (c->Opcode() == Op_ShenandoahIUBarrier) {\n-    c = c->in(1);\n-  }\n@@ -758,5 +816,0 @@\n-  } else if (mode == LoopOptsShenandoahPostExpand) {\n-    assert(UseShenandoahGC, \"only for shenandoah\");\n-    visited.clear();\n-    ShenandoahBarrierC2Support::optimize_after_expansion(visited, nstack, worklist, phase);\n-    return true;\n@@ -778,3 +831,0 @@\n-  if (phase == Optimization) {\n-    return !ShenandoahIUBarrier;\n-  }\n@@ -839,5 +889,1 @@\n-    int flags = ShenandoahHeap::HAS_FORWARDED;\n-    if (ShenandoahIUBarrier) {\n-      flags |= ShenandoahHeap::MARKING;\n-    }\n-    Node* stable_and  = phase->transform_later(new AndINode(gc_state, phase->igvn().intcon(flags)));\n+    Node* stable_and  = phase->transform_later(new AndINode(gc_state, phase->igvn().intcon(ShenandoahHeap::HAS_FORWARDED)));\n@@ -892,3 +938,0 @@\n-  if (node->Opcode() == Op_ShenandoahIUBarrier) {\n-    state()->add_iu_barrier((ShenandoahIUBarrierNode*) node);\n-  }\n@@ -901,3 +944,0 @@\n-  if (node->Opcode() == Op_ShenandoahIUBarrier) {\n-    state()->remove_iu_barrier((ShenandoahIUBarrierNode*) node);\n-  }\n@@ -909,3 +949,19 @@\n-void ShenandoahBarrierSetC2::eliminate_gc_barrier(PhaseMacroExpand* macro, Node* n) const {\n-  if (is_shenandoah_wb_pre_call(n)) {\n-    shenandoah_eliminate_wb_pre(n, &macro->igvn());\n+void ShenandoahBarrierSetC2::eliminate_gc_barrier(PhaseMacroExpand* macro, Node* node) const {\n+  if (is_shenandoah_wb_pre_call(node)) {\n+    shenandoah_eliminate_wb_pre(node, &macro->igvn());\n+  }\n+  if (ShenandoahCardBarrier && node->Opcode() == Op_CastP2X) {\n+    Node* shift = node->unique_out();\n+    Node* addp = shift->unique_out();\n+    for (DUIterator_Last jmin, j = addp->last_outs(jmin); j >= jmin; --j) {\n+      Node* mem = addp->last_out(j);\n+      if (UseCondCardMark && mem->is_Load()) {\n+        assert(mem->Opcode() == Op_LoadB, \"unexpected code shape\");\n+        \/\/ The load is checking if the card has been written so\n+        \/\/ replace it with zero to fold the test.\n+        macro->replace_node(mem, macro->intcon(0));\n+        continue;\n+      }\n+      assert(mem->is_Store(), \"store required\");\n+      macro->replace_node(mem, mem->in(MemNode::Memory));\n+    }\n@@ -951,6 +1007,1 @@\n-  for (int i = state()->iu_barriers_count() - 1; i >= 0; i--) {\n-    ShenandoahIUBarrierNode* n = state()->iu_barrier(i);\n-    if (!useful.member(n)) {\n-      state()->remove_iu_barrier(n);\n-    }\n-  }\n+\n@@ -1190,3 +1241,0 @@\n-    case Op_ShenandoahIUBarrier:\n-      conn_graph->add_local_var_and_edge(n, PointsToNode::NoEscape, n->in(1), delayed_worklist);\n-      break;\n@@ -1216,3 +1264,0 @@\n-    case Op_ShenandoahIUBarrier:\n-      conn_graph->add_local_var_and_edge(n, PointsToNode::NoEscape, n->in(1), nullptr);\n-      return true;\n","filename":"src\/hotspot\/share\/gc\/shenandoah\/c2\/shenandoahBarrierSetC2.cpp","additions":140,"deletions":95,"binary":false,"changes":235,"status":"modified"},{"patch":"@@ -34,1 +34,0 @@\n-  GrowableArray<ShenandoahIUBarrierNode*>* _iu_barriers;\n@@ -40,5 +39,0 @@\n-  int iu_barriers_count() const;\n-  ShenandoahIUBarrierNode* iu_barrier(int idx) const;\n-  void add_iu_barrier(ShenandoahIUBarrierNode* n);\n-  void remove_iu_barrier(ShenandoahIUBarrierNode * n);\n-\n@@ -76,1 +70,9 @@\n-  Node* shenandoah_iu_barrier(GraphKit* kit, Node* obj) const;\n+  void post_barrier(GraphKit* kit,\n+                    Node* ctl,\n+                    Node* store,\n+                    Node* obj,\n+                    Node* adr,\n+                    uint adr_idx,\n+                    Node* val,\n+                    BasicType bt,\n+                    bool use_precise) const;\n@@ -96,0 +98,1 @@\n+  static bool is_shenandoah_clone_call(Node* call);\n@@ -120,2 +123,2 @@\n-  virtual bool strip_mined_loops_expanded(LoopOptsMode mode) const { return mode == LoopOptsShenandoahExpand || mode == LoopOptsShenandoahPostExpand; }\n-  virtual bool is_gc_specific_loop_opts_pass(LoopOptsMode mode) const { return mode == LoopOptsShenandoahExpand || mode == LoopOptsShenandoahPostExpand; }\n+  virtual bool strip_mined_loops_expanded(LoopOptsMode mode) const { return mode == LoopOptsShenandoahExpand; }\n+  virtual bool is_gc_specific_loop_opts_pass(LoopOptsMode mode) const { return mode == LoopOptsShenandoahExpand; }\n","filename":"src\/hotspot\/share\/gc\/shenandoah\/c2\/shenandoahBarrierSetC2.hpp","additions":12,"deletions":9,"binary":false,"changes":21,"status":"modified"},{"patch":"@@ -49,2 +49,1 @@\n-  if ((state->iu_barriers_count() +\n-       state->load_reference_barriers_count()) > 0) {\n+  if (state->load_reference_barriers_count() > 0) {\n@@ -56,6 +55,0 @@\n-\n-    C->set_major_progress();\n-    if (!C->optimize_loops(igvn, LoopOptsShenandoahPostExpand)) {\n-      return false;\n-    }\n-    C->clear_major_progress();\n@@ -189,10 +182,1 @@\n-          uint i = 0;\n-          for (; i < phis.size(); i++) {\n-            Node* n = phis.node_at(i);\n-            if (n->Opcode() == Op_ShenandoahIUBarrier) {\n-              break;\n-            }\n-          }\n-          if (i == phis.size()) {\n-            return false;\n-          }\n+          return false;\n@@ -202,9 +186,0 @@\n-      } else if (in->Opcode() == Op_ShenandoahIUBarrier) {\n-        if (t != ShenandoahOopStore) {\n-          in = in->in(1);\n-          continue;\n-        }\n-        if (trace) {tty->print(\"Found enqueue barrier\"); in->dump();}\n-        phis.push(in, in->req());\n-        in = in->in(1);\n-        continue;\n@@ -329,1 +304,1 @@\n-        if (verify && !verify_helper(n->in(MemNode::ValueIn), phis, visited, ShenandoahIUBarrier ? ShenandoahOopStore : ShenandoahValue, trace, barriers_used)) {\n+        if (verify && !verify_helper(n->in(MemNode::ValueIn), phis, visited, ShenandoahValue, trace, barriers_used)) {\n@@ -371,1 +346,1 @@\n-          !verify_helper(n->in(MemNode::ValueIn), phis, visited, ShenandoahIUBarrier ? ShenandoahOopStore : ShenandoahValue, trace, barriers_used)) {\n+          !verify_helper(n->in(MemNode::ValueIn), phis, visited, ShenandoahValue, trace, barriers_used)) {\n@@ -388,0 +363,6 @@\n+        \"array_partition_stub\",\n+        { { TypeFunc::Parms, ShenandoahStore }, { TypeFunc::Parms+4, ShenandoahStore },   { -1, ShenandoahNone },\n+          { -1, ShenandoahNone },                { -1, ShenandoahNone },                  { -1, ShenandoahNone } },\n+        \"arraysort_stub\",\n+        { { TypeFunc::Parms, ShenandoahStore },  { -1, ShenandoahNone },                  { -1, ShenandoahNone },\n+          { -1,  ShenandoahNone},                 { -1,  ShenandoahNone},                 { -1,  ShenandoahNone} },\n@@ -430,1 +411,1 @@\n-        \"shenandoah_clone_barrier\",\n+        \"shenandoah_clone\",\n@@ -523,1 +504,1 @@\n-    } else if (n->Opcode() == Op_ShenandoahIUBarrier || n->Opcode() == Op_ShenandoahLoadReferenceBarrier) {\n+    } else if (n->Opcode() == Op_ShenandoahLoadReferenceBarrier) {\n@@ -664,1 +645,1 @@\n-    if (in != nullptr && (ShenandoahIUBarrier ? (phase->ctrl_or_self(in) == ctrl) : (phase->has_ctrl(in) && phase->get_ctrl(in) == ctrl))) {\n+    if (in != nullptr && phase->has_ctrl(in) && phase->get_ctrl(in) == ctrl) {\n@@ -1120,14 +1101,0 @@\n-  for (int i = 0; i < state->iu_barriers_count(); i++) {\n-    Node* barrier = state->iu_barrier(i);\n-    Node* ctrl = phase->get_ctrl(barrier);\n-    IdealLoopTree* loop = phase->get_loop(ctrl);\n-    Node* head = loop->head();\n-    if (head->is_OuterStripMinedLoop()) {\n-      \/\/ Expanding a barrier here will break loop strip mining\n-      \/\/ verification. Transform the loop so the loop nest doesn't\n-      \/\/ appear as strip mined.\n-      OuterStripMinedLoopNode* outer = head->as_OuterStripMinedLoop();\n-      hide_strip_mined_loop(outer, outer->unique_ctrl_out()->as_CountedLoop(), phase);\n-    }\n-  }\n-\n@@ -1475,151 +1442,0 @@\n-\n-  for (int i = state->iu_barriers_count() - 1; i >= 0; i--) {\n-    Node* barrier = state->iu_barrier(i);\n-    Node* pre_val = barrier->in(1);\n-\n-    if (phase->igvn().type(pre_val)->higher_equal(TypePtr::NULL_PTR)) {\n-      ShouldNotReachHere();\n-      continue;\n-    }\n-\n-    Node* ctrl = phase->get_ctrl(barrier);\n-\n-    if (ctrl->is_Proj() && ctrl->in(0)->is_CallJava()) {\n-      assert(is_dominator(phase->get_ctrl(pre_val), ctrl->in(0)->in(0), pre_val, ctrl->in(0), phase), \"can't move\");\n-      ctrl = ctrl->in(0)->in(0);\n-      phase->set_ctrl(barrier, ctrl);\n-    } else if (ctrl->is_CallRuntime()) {\n-      assert(is_dominator(phase->get_ctrl(pre_val), ctrl->in(0), pre_val, ctrl, phase), \"can't move\");\n-      ctrl = ctrl->in(0);\n-      phase->set_ctrl(barrier, ctrl);\n-    }\n-\n-    Node* init_ctrl = ctrl;\n-    IdealLoopTree* loop = phase->get_loop(ctrl);\n-    Node* raw_mem = fixer.find_mem(ctrl, barrier);\n-    Node* init_raw_mem = raw_mem;\n-    Node* raw_mem_for_ctrl = fixer.find_mem(ctrl, nullptr);\n-    Node* heap_stable_ctrl = nullptr;\n-    Node* null_ctrl = nullptr;\n-    uint last = phase->C->unique();\n-\n-    enum { _heap_stable = 1, _heap_unstable, PATH_LIMIT };\n-    Node* region = new RegionNode(PATH_LIMIT);\n-    Node* phi = PhiNode::make(region, raw_mem, Type::MEMORY, TypeRawPtr::BOTTOM);\n-\n-    enum { _fast_path = 1, _slow_path, _null_path, PATH_LIMIT2 };\n-    Node* region2 = new RegionNode(PATH_LIMIT2);\n-    Node* phi2 = PhiNode::make(region2, raw_mem, Type::MEMORY, TypeRawPtr::BOTTOM);\n-\n-    \/\/ Stable path.\n-    test_gc_state(ctrl, raw_mem, heap_stable_ctrl, phase, ShenandoahHeap::MARKING);\n-    region->init_req(_heap_stable, heap_stable_ctrl);\n-    phi->init_req(_heap_stable, raw_mem);\n-\n-    \/\/ Null path\n-    Node* reg2_ctrl = nullptr;\n-    test_null(ctrl, pre_val, null_ctrl, phase);\n-    if (null_ctrl != nullptr) {\n-      reg2_ctrl = null_ctrl->in(0);\n-      region2->init_req(_null_path, null_ctrl);\n-      phi2->init_req(_null_path, raw_mem);\n-    } else {\n-      region2->del_req(_null_path);\n-      phi2->del_req(_null_path);\n-    }\n-\n-    const int index_offset = in_bytes(ShenandoahThreadLocalData::satb_mark_queue_index_offset());\n-    const int buffer_offset = in_bytes(ShenandoahThreadLocalData::satb_mark_queue_buffer_offset());\n-    Node* thread = new ThreadLocalNode();\n-    phase->register_new_node(thread, ctrl);\n-    Node* buffer_adr = new AddPNode(phase->C->top(), thread, phase->igvn().MakeConX(buffer_offset));\n-    phase->register_new_node(buffer_adr, ctrl);\n-    Node* index_adr = new AddPNode(phase->C->top(), thread, phase->igvn().MakeConX(index_offset));\n-    phase->register_new_node(index_adr, ctrl);\n-\n-    BasicType index_bt = TypeX_X->basic_type();\n-    assert(sizeof(size_t) == type2aelembytes(index_bt), \"Loading Shenandoah SATBMarkQueue::_index with wrong size.\");\n-    const TypePtr* adr_type = TypeRawPtr::BOTTOM;\n-    Node* index = new LoadXNode(ctrl, raw_mem, index_adr, adr_type, TypeX_X, MemNode::unordered);\n-    phase->register_new_node(index, ctrl);\n-    Node* index_cmp = new CmpXNode(index, phase->igvn().MakeConX(0));\n-    phase->register_new_node(index_cmp, ctrl);\n-    Node* index_test = new BoolNode(index_cmp, BoolTest::ne);\n-    phase->register_new_node(index_test, ctrl);\n-    IfNode* queue_full_iff = new IfNode(ctrl, index_test, PROB_LIKELY(0.999), COUNT_UNKNOWN);\n-    if (reg2_ctrl == nullptr) reg2_ctrl = queue_full_iff;\n-    phase->register_control(queue_full_iff, loop, ctrl);\n-    Node* not_full = new IfTrueNode(queue_full_iff);\n-    phase->register_control(not_full, loop, queue_full_iff);\n-    Node* full = new IfFalseNode(queue_full_iff);\n-    phase->register_control(full, loop, queue_full_iff);\n-\n-    ctrl = not_full;\n-\n-    Node* next_index = new SubXNode(index, phase->igvn().MakeConX(sizeof(intptr_t)));\n-    phase->register_new_node(next_index, ctrl);\n-\n-    Node* buffer  = new LoadPNode(ctrl, raw_mem, buffer_adr, adr_type, TypeRawPtr::NOTNULL, MemNode::unordered);\n-    phase->register_new_node(buffer, ctrl);\n-    Node *log_addr = new AddPNode(phase->C->top(), buffer, next_index);\n-    phase->register_new_node(log_addr, ctrl);\n-    Node* log_store = new StorePNode(ctrl, raw_mem, log_addr, adr_type, pre_val, MemNode::unordered);\n-    phase->register_new_node(log_store, ctrl);\n-    \/\/ update the index\n-    Node* index_update = new StoreXNode(ctrl, log_store, index_adr, adr_type, next_index, MemNode::unordered);\n-    phase->register_new_node(index_update, ctrl);\n-\n-    \/\/ Fast-path case\n-    region2->init_req(_fast_path, ctrl);\n-    phi2->init_req(_fast_path, index_update);\n-\n-    ctrl = full;\n-\n-    Node* base = find_bottom_mem(ctrl, phase);\n-\n-    MergeMemNode* mm = MergeMemNode::make(base);\n-    mm->set_memory_at(Compile::AliasIdxRaw, raw_mem);\n-    phase->register_new_node(mm, ctrl);\n-\n-    Node* call = new CallLeafNode(ShenandoahBarrierSetC2::write_ref_field_pre_entry_Type(), CAST_FROM_FN_PTR(address, ShenandoahRuntime::write_ref_field_pre_entry), \"shenandoah_wb_pre\", TypeRawPtr::BOTTOM);\n-    call->init_req(TypeFunc::Control, ctrl);\n-    call->init_req(TypeFunc::I_O, phase->C->top());\n-    call->init_req(TypeFunc::Memory, mm);\n-    call->init_req(TypeFunc::FramePtr, phase->C->top());\n-    call->init_req(TypeFunc::ReturnAdr, phase->C->top());\n-    call->init_req(TypeFunc::Parms, pre_val);\n-    call->init_req(TypeFunc::Parms+1, thread);\n-    phase->register_control(call, loop, ctrl);\n-\n-    Node* ctrl_proj = new ProjNode(call, TypeFunc::Control);\n-    phase->register_control(ctrl_proj, loop, call);\n-    Node* mem_proj = new ProjNode(call, TypeFunc::Memory);\n-    phase->register_new_node(mem_proj, call);\n-\n-    \/\/ Slow-path case\n-    region2->init_req(_slow_path, ctrl_proj);\n-    phi2->init_req(_slow_path, mem_proj);\n-\n-    phase->register_control(region2, loop, reg2_ctrl);\n-    phase->register_new_node(phi2, region2);\n-\n-    region->init_req(_heap_unstable, region2);\n-    phi->init_req(_heap_unstable, phi2);\n-\n-    phase->register_control(region, loop, heap_stable_ctrl->in(0));\n-    phase->register_new_node(phi, region);\n-\n-    fix_ctrl(barrier, region, fixer, uses, nodes_above_barriers, last, phase);\n-    for(uint next = 0; next < uses.size(); next++ ) {\n-      Node *n = uses.at(next);\n-      assert(phase->get_ctrl(n) == init_ctrl, \"bad control\");\n-      assert(n != init_raw_mem, \"should leave input raw mem above the barrier\");\n-      phase->set_ctrl(n, region);\n-      follow_barrier_uses(n, init_ctrl, uses, phase);\n-    }\n-    fixer.fix_mem(init_ctrl, region, init_raw_mem, raw_mem_for_ctrl, phi, uses);\n-\n-    phase->igvn().replace_node(barrier, pre_val);\n-  }\n-  assert(state->iu_barriers_count() == 0, \"all enqueue barrier nodes should have been replaced\");\n-\n@@ -1678,2 +1494,0 @@\n-    case Op_ShenandoahIUBarrier:\n-      return get_load_addr(phase, visited, in->in(1));\n@@ -1697,350 +1511,0 @@\n-void ShenandoahBarrierC2Support::move_gc_state_test_out_of_loop(IfNode* iff, PhaseIdealLoop* phase) {\n-  IdealLoopTree *loop = phase->get_loop(iff);\n-  Node* loop_head = loop->_head;\n-  Node* entry_c = loop_head->in(LoopNode::EntryControl);\n-\n-  Node* bol = iff->in(1);\n-  Node* cmp = bol->in(1);\n-  Node* andi = cmp->in(1);\n-  Node* load = andi->in(1);\n-\n-  assert(is_gc_state_load(load), \"broken\");\n-  if (!phase->is_dominator(load->in(0), entry_c)) {\n-    Node* mem_ctrl = nullptr;\n-    Node* mem = dom_mem(load->in(MemNode::Memory), loop_head, Compile::AliasIdxRaw, mem_ctrl, phase);\n-    load = load->clone();\n-    load->set_req(MemNode::Memory, mem);\n-    load->set_req(0, entry_c);\n-    phase->register_new_node(load, entry_c);\n-    andi = andi->clone();\n-    andi->set_req(1, load);\n-    phase->register_new_node(andi, entry_c);\n-    cmp = cmp->clone();\n-    cmp->set_req(1, andi);\n-    phase->register_new_node(cmp, entry_c);\n-    bol = bol->clone();\n-    bol->set_req(1, cmp);\n-    phase->register_new_node(bol, entry_c);\n-\n-    phase->igvn().replace_input_of(iff, 1, bol);\n-  }\n-}\n-\n-bool ShenandoahBarrierC2Support::identical_backtoback_ifs(Node* n, PhaseIdealLoop* phase) {\n-  if (!n->is_If() || n->is_CountedLoopEnd()) {\n-    return false;\n-  }\n-  Node* region = n->in(0);\n-\n-  if (!region->is_Region()) {\n-    return false;\n-  }\n-  Node* dom = phase->idom(region);\n-  if (!dom->is_If()) {\n-    return false;\n-  }\n-\n-  if (!is_heap_stable_test(n) || !is_heap_stable_test(dom)) {\n-    return false;\n-  }\n-\n-  IfNode* dom_if = dom->as_If();\n-  Node* proj_true = dom_if->proj_out(1);\n-  Node* proj_false = dom_if->proj_out(0);\n-\n-  for (uint i = 1; i < region->req(); i++) {\n-    if (phase->is_dominator(proj_true, region->in(i))) {\n-      continue;\n-    }\n-    if (phase->is_dominator(proj_false, region->in(i))) {\n-      continue;\n-    }\n-    return false;\n-  }\n-\n-  return true;\n-}\n-\n-bool ShenandoahBarrierC2Support::merge_point_safe(Node* region) {\n-  for (DUIterator_Fast imax, i = region->fast_outs(imax); i < imax; i++) {\n-    Node* n = region->fast_out(i);\n-    if (n->is_LoadStore()) {\n-      \/\/ Splitting a LoadStore node through phi, causes it to lose its SCMemProj: the split if code doesn't have support\n-      \/\/ for a LoadStore at the region the if is split through because that's not expected to happen (LoadStore nodes\n-      \/\/ should be between barrier nodes). It does however happen with Shenandoah though because barriers can get\n-      \/\/ expanded around a LoadStore node.\n-      return false;\n-    }\n-  }\n-  return true;\n-}\n-\n-\n-void ShenandoahBarrierC2Support::merge_back_to_back_tests(Node* n, PhaseIdealLoop* phase) {\n-  assert(is_heap_stable_test(n), \"no other tests\");\n-  if (identical_backtoback_ifs(n, phase)) {\n-    Node* n_ctrl = n->in(0);\n-    if (phase->can_split_if(n_ctrl) && merge_point_safe(n_ctrl)) {\n-      IfNode* dom_if = phase->idom(n_ctrl)->as_If();\n-      if (is_heap_stable_test(n)) {\n-        Node* gc_state_load = n->in(1)->in(1)->in(1)->in(1);\n-        assert(is_gc_state_load(gc_state_load), \"broken\");\n-        Node* dom_gc_state_load = dom_if->in(1)->in(1)->in(1)->in(1);\n-        assert(is_gc_state_load(dom_gc_state_load), \"broken\");\n-        if (gc_state_load != dom_gc_state_load) {\n-          phase->igvn().replace_node(gc_state_load, dom_gc_state_load);\n-        }\n-      }\n-      PhiNode* bolphi = PhiNode::make_blank(n_ctrl, n->in(1));\n-      Node* proj_true = dom_if->proj_out(1);\n-      Node* proj_false = dom_if->proj_out(0);\n-      Node* con_true = phase->igvn().makecon(TypeInt::ONE);\n-      Node* con_false = phase->igvn().makecon(TypeInt::ZERO);\n-\n-      for (uint i = 1; i < n_ctrl->req(); i++) {\n-        if (phase->is_dominator(proj_true, n_ctrl->in(i))) {\n-          bolphi->init_req(i, con_true);\n-        } else {\n-          assert(phase->is_dominator(proj_false, n_ctrl->in(i)), \"bad if\");\n-          bolphi->init_req(i, con_false);\n-        }\n-      }\n-      phase->register_new_node(bolphi, n_ctrl);\n-      phase->igvn().replace_input_of(n, 1, bolphi);\n-      phase->do_split_if(n);\n-    }\n-  }\n-}\n-\n-IfNode* ShenandoahBarrierC2Support::find_unswitching_candidate(const IdealLoopTree* loop, PhaseIdealLoop* phase) {\n-  \/\/ Find first invariant test that doesn't exit the loop\n-  LoopNode *head = loop->_head->as_Loop();\n-  IfNode* unswitch_iff = nullptr;\n-  Node* n = head->in(LoopNode::LoopBackControl);\n-  int loop_has_sfpts = -1;\n-  while (n != head) {\n-    Node* n_dom = phase->idom(n);\n-    if (n->is_Region()) {\n-      if (n_dom->is_If()) {\n-        IfNode* iff = n_dom->as_If();\n-        if (iff->in(1)->is_Bool()) {\n-          BoolNode* bol = iff->in(1)->as_Bool();\n-          if (bol->in(1)->is_Cmp()) {\n-            \/\/ If condition is invariant and not a loop exit,\n-            \/\/ then found reason to unswitch.\n-            if (is_heap_stable_test(iff) &&\n-                (loop_has_sfpts == -1 || loop_has_sfpts == 0)) {\n-              assert(!loop->is_loop_exit(iff), \"both branches should be in the loop\");\n-              if (loop_has_sfpts == -1) {\n-                for(uint i = 0; i < loop->_body.size(); i++) {\n-                  Node *m = loop->_body[i];\n-                  if (m->is_SafePoint() && !m->is_CallLeaf()) {\n-                    loop_has_sfpts = 1;\n-                    break;\n-                  }\n-                }\n-                if (loop_has_sfpts == -1) {\n-                  loop_has_sfpts = 0;\n-                }\n-              }\n-              if (!loop_has_sfpts) {\n-                unswitch_iff = iff;\n-              }\n-            }\n-          }\n-        }\n-      }\n-    }\n-    n = n_dom;\n-  }\n-  return unswitch_iff;\n-}\n-\n-\n-void ShenandoahBarrierC2Support::optimize_after_expansion(VectorSet &visited, Node_Stack &stack, Node_List &old_new, PhaseIdealLoop* phase) {\n-  Node_List heap_stable_tests;\n-  stack.push(phase->C->start(), 0);\n-  do {\n-    Node* n = stack.node();\n-    uint i = stack.index();\n-\n-    if (i < n->outcnt()) {\n-      Node* u = n->raw_out(i);\n-      stack.set_index(i+1);\n-      if (!visited.test_set(u->_idx)) {\n-        stack.push(u, 0);\n-      }\n-    } else {\n-      stack.pop();\n-      if (n->is_If() && is_heap_stable_test(n)) {\n-        heap_stable_tests.push(n);\n-      }\n-    }\n-  } while (stack.size() > 0);\n-\n-  for (uint i = 0; i < heap_stable_tests.size(); i++) {\n-    Node* n = heap_stable_tests.at(i);\n-    assert(is_heap_stable_test(n), \"only evacuation test\");\n-    merge_back_to_back_tests(n, phase);\n-  }\n-\n-  if (!phase->C->major_progress()) {\n-    VectorSet seen;\n-    for (uint i = 0; i < heap_stable_tests.size(); i++) {\n-      Node* n = heap_stable_tests.at(i);\n-      IdealLoopTree* loop = phase->get_loop(n);\n-      if (loop != phase->ltree_root() &&\n-          loop->_child == nullptr &&\n-          !loop->_irreducible) {\n-        Node* head = loop->_head;\n-        if (head->is_Loop() &&\n-            (!head->is_CountedLoop() || head->as_CountedLoop()->is_main_loop() || head->as_CountedLoop()->is_normal_loop()) &&\n-            !seen.test_set(head->_idx)) {\n-          IfNode* iff = find_unswitching_candidate(loop, phase);\n-          if (iff != nullptr) {\n-            Node* bol = iff->in(1);\n-            if (head->as_Loop()->is_strip_mined()) {\n-              head->as_Loop()->verify_strip_mined(0);\n-            }\n-            move_gc_state_test_out_of_loop(iff, phase);\n-\n-            AutoNodeBudget node_budget(phase);\n-\n-            if (loop->policy_unswitching(phase)) {\n-              if (head->as_Loop()->is_strip_mined()) {\n-                OuterStripMinedLoopNode* outer = head->as_CountedLoop()->outer_loop();\n-                hide_strip_mined_loop(outer, head->as_CountedLoop(), phase);\n-              }\n-              phase->do_unswitching(loop, old_new);\n-            } else {\n-              \/\/ Not proceeding with unswitching. Move load back in\n-              \/\/ the loop.\n-              phase->igvn().replace_input_of(iff, 1, bol);\n-            }\n-          }\n-        }\n-      }\n-    }\n-  }\n-}\n-\n-ShenandoahIUBarrierNode::ShenandoahIUBarrierNode(Node* val) : Node(nullptr, val) {\n-  ShenandoahBarrierSetC2::bsc2()->state()->add_iu_barrier(this);\n-}\n-\n-const Type* ShenandoahIUBarrierNode::bottom_type() const {\n-  if (in(1) == nullptr || in(1)->is_top()) {\n-    return Type::TOP;\n-  }\n-  const Type* t = in(1)->bottom_type();\n-  if (t == TypePtr::NULL_PTR) {\n-    return t;\n-  }\n-  return t->is_oopptr();\n-}\n-\n-const Type* ShenandoahIUBarrierNode::Value(PhaseGVN* phase) const {\n-  if (in(1) == nullptr) {\n-    return Type::TOP;\n-  }\n-  const Type* t = phase->type(in(1));\n-  if (t == Type::TOP) {\n-    return Type::TOP;\n-  }\n-  if (t == TypePtr::NULL_PTR) {\n-    return t;\n-  }\n-  return t->is_oopptr();\n-}\n-\n-int ShenandoahIUBarrierNode::needed(Node* n) {\n-  if (n == nullptr ||\n-      n->is_Allocate() ||\n-      n->Opcode() == Op_ShenandoahIUBarrier ||\n-      n->bottom_type() == TypePtr::NULL_PTR ||\n-      (n->bottom_type()->make_oopptr() != nullptr && n->bottom_type()->make_oopptr()->const_oop() != nullptr)) {\n-    return NotNeeded;\n-  }\n-  if (n->is_Phi() ||\n-      n->is_CMove()) {\n-    return MaybeNeeded;\n-  }\n-  return Needed;\n-}\n-\n-Node* ShenandoahIUBarrierNode::next(Node* n) {\n-  for (;;) {\n-    if (n == nullptr) {\n-      return n;\n-    } else if (n->bottom_type() == TypePtr::NULL_PTR) {\n-      return n;\n-    } else if (n->bottom_type()->make_oopptr() != nullptr && n->bottom_type()->make_oopptr()->const_oop() != nullptr) {\n-      return n;\n-    } else if (n->is_ConstraintCast() ||\n-               n->Opcode() == Op_DecodeN ||\n-               n->Opcode() == Op_EncodeP) {\n-      n = n->in(1);\n-    } else if (n->is_Proj()) {\n-      n = n->in(0);\n-    } else {\n-      return n;\n-    }\n-  }\n-  ShouldNotReachHere();\n-  return nullptr;\n-}\n-\n-Node* ShenandoahIUBarrierNode::Identity(PhaseGVN* phase) {\n-  PhaseIterGVN* igvn = phase->is_IterGVN();\n-\n-  Node* n = next(in(1));\n-\n-  int cont = needed(n);\n-\n-  if (cont == NotNeeded) {\n-    return in(1);\n-  } else if (cont == MaybeNeeded) {\n-    if (igvn == nullptr) {\n-      phase->record_for_igvn(this);\n-      return this;\n-    } else {\n-      ResourceMark rm;\n-      Unique_Node_List wq;\n-      uint wq_i = 0;\n-\n-      for (;;) {\n-        if (n->is_Phi()) {\n-          for (uint i = 1; i < n->req(); i++) {\n-            Node* m = n->in(i);\n-            if (m != nullptr) {\n-              wq.push(m);\n-            }\n-          }\n-        } else {\n-          assert(n->is_CMove(), \"nothing else here\");\n-          Node* m = n->in(CMoveNode::IfFalse);\n-          wq.push(m);\n-          m = n->in(CMoveNode::IfTrue);\n-          wq.push(m);\n-        }\n-        Node* orig_n = nullptr;\n-        do {\n-          if (wq_i >= wq.size()) {\n-            return in(1);\n-          }\n-          n = wq.at(wq_i);\n-          wq_i++;\n-          orig_n = n;\n-          n = next(n);\n-          cont = needed(n);\n-          if (cont == Needed) {\n-            return this;\n-          }\n-        } while (cont != MaybeNeeded || (orig_n != n && wq.member(n)));\n-      }\n-    }\n-  }\n-\n-  return this;\n-}\n-\n@@ -2203,1 +1667,1 @@\n-              assert(c->is_Loop() && j == LoopNode::LoopBackControl || _phase->C->has_irreducible_loop() || has_never_branch(_phase->C->root()), \"\");\n+              assert((c->is_Loop() && j == LoopNode::LoopBackControl) || _phase->C->has_irreducible_loop() || has_never_branch(_phase->C->root()), \"\");\n@@ -3030,2 +2494,0 @@\n-    case Op_ShenandoahIUBarrier:\n-      return needs_barrier_impl(phase, n->in(1), visited);\n","filename":"src\/hotspot\/share\/gc\/shenandoah\/c2\/shenandoahSupport.cpp","additions":14,"deletions":552,"binary":false,"changes":566,"status":"modified"},{"patch":"@@ -70,4 +70,0 @@\n-  static void move_gc_state_test_out_of_loop(IfNode* iff, PhaseIdealLoop* phase);\n-  static void merge_back_to_back_tests(Node* n, PhaseIdealLoop* phase);\n-  static bool merge_point_safe(Node* region);\n-  static bool identical_backtoback_ifs(Node *n, PhaseIdealLoop* phase);\n@@ -75,1 +71,0 @@\n-  static IfNode* find_unswitching_candidate(const IdealLoopTree *loop, PhaseIdealLoop* phase);\n@@ -87,1 +82,0 @@\n-  static void optimize_after_expansion(VectorSet& visited, Node_Stack& nstack, Node_List& old_new, PhaseIdealLoop* phase);\n@@ -99,17 +93,0 @@\n-class ShenandoahIUBarrierNode : public Node {\n-public:\n-  ShenandoahIUBarrierNode(Node* val);\n-\n-  const Type *bottom_type() const;\n-  const Type* Value(PhaseGVN* phase) const;\n-  Node* Identity(PhaseGVN* phase);\n-\n-  int Opcode() const;\n-\n-private:\n-  enum { Needed, NotNeeded, MaybeNeeded };\n-\n-  static int needed(Node* n);\n-  static Node* next(Node* n);\n-};\n-\n","filename":"src\/hotspot\/share\/gc\/shenandoah\/c2\/shenandoahSupport.hpp","additions":0,"deletions":23,"binary":false,"changes":23,"status":"modified"},{"patch":"@@ -3,0 +3,1 @@\n+ * Copyright Amazon.com Inc. or its affiliates. All Rights Reserved.\n@@ -27,0 +28,3 @@\n+#include \"gc\/shared\/gcCause.hpp\"\n+#include \"gc\/shenandoah\/heuristics\/shenandoahHeuristics.hpp\"\n+#include \"gc\/shenandoah\/heuristics\/shenandoahSpaceInfo.hpp\"\n@@ -29,0 +33,1 @@\n+#include \"gc\/shenandoah\/shenandoahCollectorPolicy.hpp\"\n@@ -34,0 +39,1 @@\n+#include \"runtime\/globals.hpp\"\n@@ -57,2 +63,2 @@\n-ShenandoahAdaptiveHeuristics::ShenandoahAdaptiveHeuristics() :\n-  ShenandoahHeuristics(),\n+ShenandoahAdaptiveHeuristics::ShenandoahAdaptiveHeuristics(ShenandoahSpaceInfo* space_info) :\n+  ShenandoahHeuristics(space_info),\n@@ -61,1 +67,2 @@\n-  _last_trigger(OTHER) { }\n+  _last_trigger(OTHER),\n+  _available(Moving_Average_Samples, ShenandoahAdaptiveDecayFactor) { }\n@@ -93,1 +100,1 @@\n-                     SIZE_FORMAT \"%s, Max CSet: \" SIZE_FORMAT \"%s, Min Garbage: \" SIZE_FORMAT \"%s\",\n+                     SIZE_FORMAT \"%s, Max Evacuation: \" SIZE_FORMAT \"%s, Min Garbage: \" SIZE_FORMAT \"%s\",\n@@ -106,1 +113,1 @@\n-    ShenandoahHeapRegion* r = data[idx]._region;\n+    ShenandoahHeapRegion* r = data[idx].get_region();\n@@ -131,1 +138,1 @@\n-  size_t available = ShenandoahHeap::heap()->free_set()->available();\n+  size_t available = _space_info->available();\n@@ -133,1 +140,0 @@\n-  _available.add(available);\n@@ -135,2 +141,9 @@\n-  if (_available.sd() > 0) {\n-    z_score = (available - _available.avg()) \/ _available.sd();\n+  double available_sd = _available.sd();\n+  if (available_sd > 0) {\n+    double available_avg = _available.avg();\n+    z_score = (double(available) - available_avg) \/ available_sd;\n+    log_debug(gc, ergo)(\"Available: \" SIZE_FORMAT \" %sB, z-score=%.3f. Average available: %.1f %sB +\/- %.1f %sB.\",\n+                        byte_size_in_proper_unit(available), proper_unit_for_byte_size(available),\n+                        z_score,\n+                        byte_size_in_proper_unit(available_avg), proper_unit_for_byte_size(available_avg),\n+                        byte_size_in_proper_unit(available_sd), proper_unit_for_byte_size(available_sd));\n@@ -139,5 +152,1 @@\n-  log_debug(gc, ergo)(\"Available: \" SIZE_FORMAT \" %sB, z-score=%.3f. Average available: %.1f %sB +\/- %.1f %sB.\",\n-                      byte_size_in_proper_unit(available), proper_unit_for_byte_size(available),\n-                      z_score,\n-                      byte_size_in_proper_unit(_available.avg()), proper_unit_for_byte_size(_available.avg()),\n-                      byte_size_in_proper_unit(_available.sd()), proper_unit_for_byte_size(_available.sd()));\n+  _available.add(double(available));\n@@ -198,0 +207,28 @@\n+\/\/  Rationale:\n+\/\/    The idea is that there is an average allocation rate and there are occasional abnormal bursts (or spikes) of\n+\/\/    allocations that exceed the average allocation rate.  What do these spikes look like?\n+\/\/\n+\/\/    1. At certain phase changes, we may discard large amounts of data and replace it with large numbers of newly\n+\/\/       allocated objects.  This \"spike\" looks more like a phase change.  We were in steady state at M bytes\/sec\n+\/\/       allocation rate and now we're in a \"reinitialization phase\" that looks like N bytes\/sec.  We need the \"spike\"\n+\/\/       accommodation to give us enough runway to recalibrate our \"average allocation rate\".\n+\/\/\n+\/\/   2. The typical workload changes.  \"Suddenly\", our typical workload of N TPS increases to N+delta TPS.  This means\n+\/\/       our average allocation rate needs to be adjusted.  Once again, we need the \"spike\" accomodation to give us\n+\/\/       enough runway to recalibrate our \"average allocation rate\".\n+\/\/\n+\/\/    3. Though there is an \"average\" allocation rate, a given workload's demand for allocation may be very bursty.  We\n+\/\/       allocate a bunch of LABs during the 5 ms that follow completion of a GC, then we perform no more allocations for\n+\/\/       the next 150 ms.  It seems we want the \"spike\" to represent the maximum divergence from average within the\n+\/\/       period of time between consecutive evaluation of the should_start_gc() service.  Here's the thinking:\n+\/\/\n+\/\/       a) Between now and the next time I ask whether should_start_gc(), we might experience a spike representing\n+\/\/          the anticipated burst of allocations.  If that would put us over budget, then we should start GC immediately.\n+\/\/       b) Between now and the anticipated depletion of allocation pool, there may be two or more bursts of allocations.\n+\/\/          If there are more than one of these bursts, we can \"approximate\" that these will be separated by spans of\n+\/\/          time with very little or no allocations so the \"average\" allocation rate should be a suitable approximation\n+\/\/          of how this will behave.\n+\/\/\n+\/\/    For cases 1 and 2, we need to \"quickly\" recalibrate the average allocation rate whenever we detect a change\n+\/\/    in operation mode.  We want some way to decide that the average rate has changed, while keeping average\n+\/\/    allocation rate computation independent.\n@@ -199,5 +236,3 @@\n-  ShenandoahHeap* heap = ShenandoahHeap::heap();\n-  size_t max_capacity = heap->max_capacity();\n-  size_t capacity = heap->soft_max_capacity();\n-  size_t available = heap->free_set()->available();\n-  size_t allocated = heap->bytes_allocated_since_gc_start();\n+  size_t capacity = ShenandoahHeap::heap()->soft_max_capacity();\n+  size_t available = _space_info->soft_available();\n+  size_t allocated = _space_info->bytes_allocated_since_gc_start();\n@@ -205,3 +240,7 @@\n-  \/\/ Make sure the code below treats available without the soft tail.\n-  size_t soft_tail = max_capacity - capacity;\n-  available = (available > soft_tail) ? (available - soft_tail) : 0;\n+  log_debug(gc)(\"should_start_gc? available: \" SIZE_FORMAT \", soft_max_capacity: \" SIZE_FORMAT\n+                \", allocated: \" SIZE_FORMAT, available, capacity, allocated);\n+\n+  if (_start_gc_is_pending) {\n+    log_trigger(\"GC start is already pending\");\n+    return true;\n+  }\n@@ -213,1 +252,1 @@\n-  size_t min_threshold = capacity \/ 100 * ShenandoahMinFreeThreshold;\n+  size_t min_threshold = min_free_threshold();\n@@ -215,2 +254,2 @@\n-    log_info(gc)(\"Trigger: Free (\" SIZE_FORMAT \"%s) is below minimum threshold (\" SIZE_FORMAT \"%s)\",\n-                 byte_size_in_proper_unit(available),     proper_unit_for_byte_size(available),\n+    log_trigger(\"Free (\" SIZE_FORMAT \"%s) is below minimum threshold (\" SIZE_FORMAT \"%s)\",\n+                 byte_size_in_proper_unit(available), proper_unit_for_byte_size(available),\n@@ -218,0 +257,1 @@\n+    accept_trigger_with_type(OTHER);\n@@ -221,0 +261,1 @@\n+  \/\/ Check if we need to learn a bit about the application\n@@ -225,1 +266,1 @@\n-      log_info(gc)(\"Trigger: Learning \" SIZE_FORMAT \" of \" SIZE_FORMAT \". Free (\" SIZE_FORMAT \"%s) is below initial threshold (\" SIZE_FORMAT \"%s)\",\n+      log_trigger(\"Learning \" SIZE_FORMAT \" of \" SIZE_FORMAT \". Free (\" SIZE_FORMAT \"%s) is below initial threshold (\" SIZE_FORMAT \"%s)\",\n@@ -227,1 +268,1 @@\n-                   byte_size_in_proper_unit(available),      proper_unit_for_byte_size(available),\n+                   byte_size_in_proper_unit(available), proper_unit_for_byte_size(available),\n@@ -229,0 +270,1 @@\n+      accept_trigger_with_type(OTHER);\n@@ -232,1 +274,0 @@\n-\n@@ -234,1 +275,1 @@\n-  \/\/   1. Some space to absorb allocation spikes\n+  \/\/   1. Some space to absorb allocation spikes (ShenandoahAllocSpikeFactor)\n@@ -244,1 +285,1 @@\n-  double avg_cycle_time = _gc_time_history->davg() + (_margin_of_error_sd * _gc_time_history->dsd());\n+  double avg_cycle_time = _gc_cycle_time_history->davg() + (_margin_of_error_sd * _gc_cycle_time_history->dsd());\n@@ -246,0 +287,2 @@\n+  log_debug(gc)(\"average GC time: %.2f ms, allocation rate: %.0f %s\/s\",\n+          avg_cycle_time * 1000, byte_size_in_proper_unit(avg_alloc_rate), proper_unit_for_byte_size(avg_alloc_rate));\n@@ -247,1 +290,2 @@\n-    log_info(gc)(\"Trigger: Average GC time (%.2f ms) is above the time for average allocation rate (%.0f %sB\/s) to deplete free headroom (\" SIZE_FORMAT \"%s) (margin of error = %.2f)\",\n+    log_trigger(\"Average GC time (%.2f ms) is above the time for average allocation rate (%.0f %sB\/s)\"\n+                 \" to deplete free headroom (\" SIZE_FORMAT \"%s) (margin of error = %.2f)\",\n@@ -252,1 +296,0 @@\n-\n@@ -258,2 +301,1 @@\n-\n-    _last_trigger = RATE;\n+    accept_trigger_with_type(RATE);\n@@ -265,1 +307,1 @@\n-    log_info(gc)(\"Trigger: Average GC time (%.2f ms) is above the time for instantaneous allocation rate (%.0f %sB\/s) to deplete free headroom (\" SIZE_FORMAT \"%s) (spike threshold = %.2f)\",\n+    log_trigger(\"Average GC time (%.2f ms) is above the time for instantaneous allocation rate (%.0f %sB\/s) to deplete free headroom (\" SIZE_FORMAT \"%s) (spike threshold = %.2f)\",\n@@ -270,1 +312,1 @@\n-    _last_trigger = SPIKE;\n+    accept_trigger_with_type(SPIKE);\n@@ -274,1 +316,6 @@\n-  return ShenandoahHeuristics::should_start_gc();\n+  if (ShenandoahHeuristics::should_start_gc()) {\n+    _start_gc_is_pending = true;\n+    return true;\n+  } else {\n+    return false;\n+  }\n@@ -303,0 +350,4 @@\n+size_t ShenandoahAdaptiveHeuristics::min_free_threshold() {\n+  return ShenandoahHeap::heap()->soft_max_capacity() \/ 100 * ShenandoahMinFreeThreshold;\n+}\n+\n","filename":"src\/hotspot\/share\/gc\/shenandoah\/heuristics\/shenandoahAdaptiveHeuristics.cpp","additions":88,"deletions":37,"binary":false,"changes":125,"status":"modified"},{"patch":"@@ -3,0 +3,1 @@\n+ * Copyright Amazon.com Inc. or its affiliates. All Rights Reserved.\n@@ -28,0 +29,1 @@\n+#include \"memory\/allocation.hpp\"\n@@ -30,0 +32,1 @@\n+#include \"gc\/shenandoah\/shenandoahSharedVariables.hpp\"\n@@ -41,1 +44,0 @@\n-\n@@ -53,0 +55,11 @@\n+\/*\n+ * The adaptive heuristic tracks the allocation behavior and average cycle\n+ * time of the application. It attempts to start a cycle with enough time\n+ * to complete before the available memory is exhausted. It errors on the\n+ * side of starting cycles early to avoid allocation failures (degenerated\n+ * cycles).\n+ *\n+ * This heuristic limits the number of regions for evacuation such that the\n+ * evacuation reserve is respected. This helps it avoid allocation failures\n+ * during evacuation. It preferentially selects regions with the most garbage.\n+ *\/\n@@ -55,1 +68,1 @@\n-  ShenandoahAdaptiveHeuristics();\n+  ShenandoahAdaptiveHeuristics(ShenandoahSpaceInfo* space_info);\n@@ -101,0 +114,1 @@\n+protected:\n@@ -105,1 +119,1 @@\n-  \/\/ tend to over estimate the rate at which mutators will deplete the\n+  \/\/ tend to overestimate the rate at which mutators will deplete the\n@@ -128,0 +142,11 @@\n+\n+  \/\/ A conservative minimum threshold of free space that we'll try to maintain when possible.\n+  \/\/ For example, we might trigger a concurrent gc if we are likely to drop below\n+  \/\/ this threshold, or we might consider this when dynamically resizing generations\n+  \/\/ in the generational case. Controlled by global flag ShenandoahMinFreeThreshold.\n+  size_t min_free_threshold();\n+\n+  inline void accept_trigger_with_type(Trigger trigger_type) {\n+    _last_trigger = trigger_type;\n+    ShenandoahHeuristics::accept_trigger();\n+  }\n","filename":"src\/hotspot\/share\/gc\/shenandoah\/heuristics\/shenandoahAdaptiveHeuristics.hpp","additions":28,"deletions":3,"binary":false,"changes":31,"status":"modified"},{"patch":"@@ -34,1 +34,2 @@\n-ShenandoahAggressiveHeuristics::ShenandoahAggressiveHeuristics() : ShenandoahHeuristics() {\n+ShenandoahAggressiveHeuristics::ShenandoahAggressiveHeuristics(ShenandoahSpaceInfo* space_info) :\n+  ShenandoahHeuristics(space_info) {\n@@ -46,1 +47,1 @@\n-    ShenandoahHeapRegion* r = data[idx]._region;\n+    ShenandoahHeapRegion* r = data[idx].get_region();\n@@ -54,1 +55,2 @@\n-  log_info(gc)(\"Trigger: Start next cycle immediately\");\n+  log_trigger(\"Start next cycle immediately\");\n+  accept_trigger();\n","filename":"src\/hotspot\/share\/gc\/shenandoah\/heuristics\/shenandoahAggressiveHeuristics.cpp","additions":5,"deletions":3,"binary":false,"changes":8,"status":"modified"},{"patch":"@@ -30,0 +30,4 @@\n+\/*\n+ * This is a diagnostic heuristic that continuously runs collections\n+ * cycles and adds every region with any garbage to the collection set.\n+ *\/\n@@ -32,1 +36,1 @@\n-  ShenandoahAggressiveHeuristics();\n+  ShenandoahAggressiveHeuristics(ShenandoahSpaceInfo* space_info);\n","filename":"src\/hotspot\/share\/gc\/shenandoah\/heuristics\/shenandoahAggressiveHeuristics.hpp","additions":5,"deletions":1,"binary":false,"changes":6,"status":"modified"},{"patch":"@@ -35,1 +35,2 @@\n-ShenandoahCompactHeuristics::ShenandoahCompactHeuristics() : ShenandoahHeuristics() {\n+ShenandoahCompactHeuristics::ShenandoahCompactHeuristics(ShenandoahSpaceInfo* space_info) :\n+  ShenandoahHeuristics(space_info) {\n@@ -48,5 +49,3 @@\n-  ShenandoahHeap* heap = ShenandoahHeap::heap();\n-\n-  size_t max_capacity = heap->max_capacity();\n-  size_t capacity = heap->soft_max_capacity();\n-  size_t available = heap->free_set()->available();\n+  size_t max_capacity = _space_info->max_capacity();\n+  size_t capacity = ShenandoahHeap::heap()->soft_max_capacity();\n+  size_t available = _space_info->available();\n@@ -62,3 +61,4 @@\n-    log_info(gc)(\"Trigger: Free (\" SIZE_FORMAT \"%s) is below minimum threshold (\" SIZE_FORMAT \"%s)\",\n-                 byte_size_in_proper_unit(available),     proper_unit_for_byte_size(available),\n-                 byte_size_in_proper_unit(min_threshold), proper_unit_for_byte_size(min_threshold));\n+    log_trigger(\"Free (\" SIZE_FORMAT \"%s) is below minimum threshold (\" SIZE_FORMAT \"%s)\",\n+                byte_size_in_proper_unit(available),     proper_unit_for_byte_size(available),\n+                byte_size_in_proper_unit(min_threshold), proper_unit_for_byte_size(min_threshold));\n+    accept_trigger();\n@@ -68,1 +68,1 @@\n-  size_t bytes_allocated = heap->bytes_allocated_since_gc_start();\n+  size_t bytes_allocated = _space_info->bytes_allocated_since_gc_start();\n@@ -70,3 +70,4 @@\n-    log_info(gc)(\"Trigger: Allocated since last cycle (\" SIZE_FORMAT \"%s) is larger than allocation threshold (\" SIZE_FORMAT \"%s)\",\n-                 byte_size_in_proper_unit(bytes_allocated),           proper_unit_for_byte_size(bytes_allocated),\n-                 byte_size_in_proper_unit(threshold_bytes_allocated), proper_unit_for_byte_size(threshold_bytes_allocated));\n+    log_trigger(\"Allocated since last cycle (\" SIZE_FORMAT \"%s) is larger than allocation threshold (\" SIZE_FORMAT \"%s)\",\n+                byte_size_in_proper_unit(bytes_allocated),           proper_unit_for_byte_size(bytes_allocated),\n+                byte_size_in_proper_unit(threshold_bytes_allocated), proper_unit_for_byte_size(threshold_bytes_allocated));\n+    accept_trigger();\n@@ -93,1 +94,1 @@\n-    ShenandoahHeapRegion* r = data[idx]._region;\n+    ShenandoahHeapRegion* r = data[idx].get_region();\n","filename":"src\/hotspot\/share\/gc\/shenandoah\/heuristics\/shenandoahCompactHeuristics.cpp","additions":15,"deletions":14,"binary":false,"changes":29,"status":"modified"},{"patch":"@@ -30,0 +30,4 @@\n+\/*\n+ * This heuristic has simpler triggers than the adaptive heuristic. The\n+ * size of the collection set is limited to 3\/4 of available memory.\n+ *\/\n@@ -32,1 +36,1 @@\n-  ShenandoahCompactHeuristics();\n+  ShenandoahCompactHeuristics(ShenandoahSpaceInfo* space_info);\n","filename":"src\/hotspot\/share\/gc\/shenandoah\/heuristics\/shenandoahCompactHeuristics.hpp","additions":5,"deletions":1,"binary":false,"changes":6,"status":"modified"},{"patch":"@@ -0,0 +1,284 @@\n+\/*\n+ * Copyright Amazon.com Inc. or its affiliates. All Rights Reserved.\n+ * DO NOT ALTER OR REMOVE COPYRIGHT NOTICES OR THIS FILE HEADER.\n+ *\n+ * This code is free software; you can redistribute it and\/or modify it\n+ * under the terms of the GNU General Public License version 2 only, as\n+ * published by the Free Software Foundation.\n+ *\n+ * This code is distributed in the hope that it will be useful, but WITHOUT\n+ * ANY WARRANTY; without even the implied warranty of MERCHANTABILITY or\n+ * FITNESS FOR A PARTICULAR PURPOSE.  See the GNU General Public License\n+ * version 2 for more details (a copy is included in the LICENSE file that\n+ * accompanied this code).\n+ *\n+ * You should have received a copy of the GNU General Public License version\n+ * 2 along with this work; if not, write to the Free Software Foundation,\n+ * Inc., 51 Franklin St, Fifth Floor, Boston, MA 02110-1301 USA.\n+ *\n+ * Please contact Oracle, 500 Oracle Parkway, Redwood Shores, CA 94065 USA\n+ * or visit www.oracle.com if you need additional information or have any\n+ * questions.\n+ *\n+ *\/\n+\n+#include \"precompiled.hpp\"\n+\n+#include \"gc\/shenandoah\/heuristics\/shenandoahGenerationalHeuristics.hpp\"\n+#include \"gc\/shenandoah\/shenandoahCollectionSet.hpp\"\n+#include \"gc\/shenandoah\/shenandoahCollectorPolicy.hpp\"\n+#include \"gc\/shenandoah\/shenandoahGeneration.hpp\"\n+#include \"gc\/shenandoah\/shenandoahGenerationalHeap.inline.hpp\"\n+#include \"gc\/shenandoah\/shenandoahHeapRegion.inline.hpp\"\n+#include \"gc\/shenandoah\/shenandoahOldGeneration.hpp\"\n+#include \"gc\/shenandoah\/shenandoahEvacInfo.hpp\"\n+#include \"gc\/shenandoah\/shenandoahTrace.hpp\"\n+\n+#include \"logging\/log.hpp\"\n+\n+ShenandoahGenerationalHeuristics::ShenandoahGenerationalHeuristics(ShenandoahGeneration* generation)\n+        : ShenandoahAdaptiveHeuristics(generation), _generation(generation) {\n+}\n+\n+void ShenandoahGenerationalHeuristics::choose_collection_set(ShenandoahCollectionSet* collection_set) {\n+  assert(collection_set->is_empty(), \"Must be empty\");\n+\n+  auto heap = ShenandoahGenerationalHeap::heap();\n+  size_t region_size_bytes = ShenandoahHeapRegion::region_size_bytes();\n+\n+\n+  \/\/ Check all pinned regions have updated status before choosing the collection set.\n+  heap->assert_pinned_region_status();\n+\n+  \/\/ Step 1. Build up the region candidates we care about, rejecting losers and accepting winners right away.\n+\n+  size_t num_regions = heap->num_regions();\n+\n+  RegionData* candidates = _region_data;\n+\n+  size_t cand_idx = 0;\n+  size_t preselected_candidates = 0;\n+\n+  size_t total_garbage = 0;\n+\n+  size_t immediate_garbage = 0;\n+  size_t immediate_regions = 0;\n+\n+  size_t free = 0;\n+  size_t free_regions = 0;\n+\n+  \/\/ This counts number of humongous regions that we intend to promote in this cycle.\n+  size_t humongous_regions_promoted = 0;\n+  \/\/ This counts number of regular regions that will be promoted in place.\n+  size_t regular_regions_promoted_in_place = 0;\n+  \/\/ This counts bytes of memory used by regular regions to be promoted in place.\n+  size_t regular_regions_promoted_usage = 0;\n+  \/\/ This counts bytes of memory free in regular regions to be promoted in place.\n+  size_t regular_regions_promoted_free = 0;\n+  \/\/ This counts bytes of garbage memory in regular regions to be promoted in place.\n+  size_t regular_regions_promoted_garbage = 0;\n+\n+  for (size_t i = 0; i < num_regions; i++) {\n+    ShenandoahHeapRegion* region = heap->get_region(i);\n+    if (!_generation->contains(region)) {\n+      continue;\n+    }\n+    size_t garbage = region->garbage();\n+    total_garbage += garbage;\n+    if (region->is_empty()) {\n+      free_regions++;\n+      free += region_size_bytes;\n+    } else if (region->is_regular()) {\n+      if (!region->has_live()) {\n+        \/\/ We can recycle it right away and put it in the free set.\n+        immediate_regions++;\n+        immediate_garbage += garbage;\n+        region->make_trash_immediate();\n+      } else {\n+        bool is_candidate;\n+        \/\/ This is our candidate for later consideration.\n+        if (collection_set->is_preselected(i)) {\n+          assert(heap->is_tenurable(region), \"Preselection filter\");\n+          is_candidate = true;\n+          preselected_candidates++;\n+          \/\/ Set garbage value to maximum value to force this into the sorted collection set.\n+          garbage = region_size_bytes;\n+        } else if (region->is_young() && heap->is_tenurable(region)) {\n+          \/\/ Note that for GLOBAL GC, region may be OLD, and OLD regions do not qualify for pre-selection\n+\n+          \/\/ This region is old enough to be promoted but it was not preselected, either because its garbage is below\n+          \/\/ ShenandoahOldGarbageThreshold so it will be promoted in place, or because there is not sufficient room\n+          \/\/ in old gen to hold the evacuated copies of this region's live data.  In both cases, we choose not to\n+          \/\/ place this region into the collection set.\n+          if (region->get_top_before_promote() != nullptr) {\n+            \/\/ Region was included for promotion-in-place\n+            regular_regions_promoted_in_place++;\n+            regular_regions_promoted_usage += region->used_before_promote();\n+            regular_regions_promoted_free += region->free();\n+            regular_regions_promoted_garbage += region->garbage();\n+          }\n+          is_candidate = false;\n+        } else {\n+          is_candidate = true;\n+        }\n+        if (is_candidate) {\n+          candidates[cand_idx].set_region_and_garbage(region, garbage);\n+          cand_idx++;\n+        }\n+      }\n+    } else if (region->is_humongous_start()) {\n+      \/\/ Reclaim humongous regions here, and count them as the immediate garbage\n+#ifdef ASSERT\n+      bool reg_live = region->has_live();\n+      bool bm_live = heap->active_generation()->complete_marking_context()->is_marked(cast_to_oop(region->bottom()));\n+      assert(reg_live == bm_live,\n+             \"Humongous liveness and marks should agree. Region live: %s; Bitmap live: %s; Region Live Words: \" SIZE_FORMAT,\n+             BOOL_TO_STR(reg_live), BOOL_TO_STR(bm_live), region->get_live_data_words());\n+#endif\n+      if (!region->has_live()) {\n+        heap->trash_humongous_region_at(region);\n+\n+        \/\/ Count only the start. Continuations would be counted on \"trash\" path\n+        immediate_regions++;\n+        immediate_garbage += garbage;\n+      } else {\n+        if (region->is_young() && heap->is_tenurable(region)) {\n+          oop obj = cast_to_oop(region->bottom());\n+          size_t humongous_regions = ShenandoahHeapRegion::required_regions(obj->size() * HeapWordSize);\n+          humongous_regions_promoted += humongous_regions;\n+        }\n+      }\n+    } else if (region->is_trash()) {\n+      \/\/ Count in just trashed collection set, during coalesced CM-with-UR\n+      immediate_regions++;\n+      immediate_garbage += garbage;\n+    }\n+  }\n+  heap->old_generation()->set_expected_humongous_region_promotions(humongous_regions_promoted);\n+  heap->old_generation()->set_expected_regular_region_promotions(regular_regions_promoted_in_place);\n+  log_info(gc, ergo)(\"Planning to promote in place \" SIZE_FORMAT \" humongous regions and \" SIZE_FORMAT\n+                     \" regular regions, spanning a total of \" SIZE_FORMAT \" used bytes\",\n+                     humongous_regions_promoted, regular_regions_promoted_in_place,\n+                     humongous_regions_promoted * ShenandoahHeapRegion::region_size_bytes() +\n+                     regular_regions_promoted_usage);\n+\n+  \/\/ Step 2. Look back at garbage statistics, and decide if we want to collect anything,\n+  \/\/ given the amount of immediately reclaimable garbage. If we do, figure out the collection set.\n+\n+  assert (immediate_garbage <= total_garbage,\n+          \"Cannot have more immediate garbage than total garbage: \" SIZE_FORMAT \"%s vs \" SIZE_FORMAT \"%s\",\n+          byte_size_in_proper_unit(immediate_garbage), proper_unit_for_byte_size(immediate_garbage),\n+          byte_size_in_proper_unit(total_garbage), proper_unit_for_byte_size(total_garbage));\n+\n+  size_t immediate_percent = (total_garbage == 0) ? 0 : (immediate_garbage * 100 \/ total_garbage);\n+\n+  bool doing_promote_in_place = (humongous_regions_promoted + regular_regions_promoted_in_place > 0);\n+  if (doing_promote_in_place || (preselected_candidates > 0) || (immediate_percent <= ShenandoahImmediateThreshold)) {\n+    \/\/ Only young collections need to prime the collection set.\n+    if (_generation->is_young()) {\n+      heap->old_generation()->heuristics()->prime_collection_set(collection_set);\n+    }\n+\n+    \/\/ Call the subclasses to add young-gen regions into the collection set.\n+    choose_collection_set_from_regiondata(collection_set, candidates, cand_idx, immediate_garbage + free);\n+  }\n+\n+  if (collection_set->has_old_regions()) {\n+    heap->shenandoah_policy()->record_mixed_cycle();\n+  }\n+\n+  size_t cset_percent = (total_garbage == 0) ? 0 : (collection_set->garbage() * 100 \/ total_garbage);\n+  size_t collectable_garbage = collection_set->garbage() + immediate_garbage;\n+  size_t collectable_garbage_percent = (total_garbage == 0) ? 0 : (collectable_garbage * 100 \/ total_garbage);\n+\n+  log_info(gc, ergo)(\"Collectable Garbage: \" SIZE_FORMAT \"%s (\" SIZE_FORMAT \"%%), \"\n+                     \"Immediate: \" SIZE_FORMAT \"%s (\" SIZE_FORMAT \"%%), \" SIZE_FORMAT \" regions, \"\n+                     \"CSet: \" SIZE_FORMAT \"%s (\" SIZE_FORMAT \"%%), \" SIZE_FORMAT \" regions\",\n+\n+                     byte_size_in_proper_unit(collectable_garbage),\n+                     proper_unit_for_byte_size(collectable_garbage),\n+                     collectable_garbage_percent,\n+\n+                     byte_size_in_proper_unit(immediate_garbage),\n+                     proper_unit_for_byte_size(immediate_garbage),\n+                     immediate_percent,\n+                     immediate_regions,\n+\n+                     byte_size_in_proper_unit(collection_set->garbage()),\n+                     proper_unit_for_byte_size(collection_set->garbage()),\n+                     cset_percent,\n+                     collection_set->count());\n+\n+  if (collection_set->garbage() > 0) {\n+    size_t young_evac_bytes = collection_set->get_young_bytes_reserved_for_evacuation();\n+    size_t promote_evac_bytes = collection_set->get_young_bytes_to_be_promoted();\n+    size_t old_evac_bytes = collection_set->get_old_bytes_reserved_for_evacuation();\n+    size_t total_evac_bytes = young_evac_bytes + promote_evac_bytes + old_evac_bytes;\n+    log_info(gc, ergo)(\"Evacuation Targets: YOUNG: \" SIZE_FORMAT \"%s, \"\n+                       \"PROMOTE: \" SIZE_FORMAT \"%s, \"\n+                       \"OLD: \" SIZE_FORMAT \"%s, \"\n+                       \"TOTAL: \" SIZE_FORMAT \"%s\",\n+                       byte_size_in_proper_unit(young_evac_bytes), proper_unit_for_byte_size(young_evac_bytes),\n+                       byte_size_in_proper_unit(promote_evac_bytes), proper_unit_for_byte_size(promote_evac_bytes),\n+                       byte_size_in_proper_unit(old_evac_bytes), proper_unit_for_byte_size(old_evac_bytes),\n+                       byte_size_in_proper_unit(total_evac_bytes), proper_unit_for_byte_size(total_evac_bytes));\n+\n+    ShenandoahEvacuationInformation evacInfo;\n+    evacInfo.set_collection_set_regions(collection_set->count());\n+    evacInfo.set_collection_set_used_before(collection_set->used());\n+    evacInfo.set_collection_set_used_after(collection_set->live());\n+    evacInfo.set_collected_old(old_evac_bytes);\n+    evacInfo.set_collected_promoted(promote_evac_bytes);\n+    evacInfo.set_collected_young(young_evac_bytes);\n+    evacInfo.set_regions_promoted_humongous(humongous_regions_promoted);\n+    evacInfo.set_regions_promoted_regular(regular_regions_promoted_in_place);\n+    evacInfo.set_regular_promoted_garbage(regular_regions_promoted_garbage);\n+    evacInfo.set_regular_promoted_free(regular_regions_promoted_free);\n+    evacInfo.set_regions_immediate(immediate_regions);\n+    evacInfo.set_immediate_size(immediate_garbage);\n+    evacInfo.set_free_regions(free_regions);\n+\n+    ShenandoahTracer().report_evacuation_info(&evacInfo);\n+  }\n+}\n+\n+\n+size_t ShenandoahGenerationalHeuristics::add_preselected_regions_to_collection_set(ShenandoahCollectionSet* cset,\n+                                                                                   const RegionData* data,\n+                                                                                   size_t size) const {\n+  \/\/ cur_young_garbage represents the amount of memory to be reclaimed from young-gen.  In the case that live objects\n+  \/\/ are known to be promoted out of young-gen, we count this as cur_young_garbage because this memory is reclaimed\n+  \/\/ from young-gen and becomes available to serve future young-gen allocation requests.\n+  size_t cur_young_garbage = 0;\n+  for (size_t idx = 0; idx < size; idx++) {\n+    ShenandoahHeapRegion* r = data[idx].get_region();\n+    if (cset->is_preselected(r->index())) {\n+      assert(ShenandoahGenerationalHeap::heap()->is_tenurable(r), \"Preselected regions must have tenure age\");\n+      \/\/ Entire region will be promoted, This region does not impact young-gen or old-gen evacuation reserve.\n+      \/\/ This region has been pre-selected and its impact on promotion reserve is already accounted for.\n+\n+      \/\/ r->used() is r->garbage() + r->get_live_data_bytes()\n+      \/\/ Since all live data in this region is being evacuated from young-gen, it is as if this memory\n+      \/\/ is garbage insofar as young-gen is concerned.  Counting this as garbage reduces the need to\n+      \/\/ reclaim highly utilized young-gen regions just for the sake of finding min_garbage to reclaim\n+      \/\/ within young-gen memory.\n+\n+      cur_young_garbage += r->garbage();\n+      cset->add_region(r);\n+    }\n+  }\n+  return cur_young_garbage;\n+}\n+\n+void ShenandoahGenerationalHeuristics::log_cset_composition(ShenandoahCollectionSet* cset) const {\n+  size_t collected_old = cset->get_old_bytes_reserved_for_evacuation();\n+  size_t collected_promoted = cset->get_young_bytes_to_be_promoted();\n+  size_t collected_young = cset->get_young_bytes_reserved_for_evacuation();\n+\n+  log_info(gc, ergo)(\n+          \"Chosen CSet evacuates young: \" SIZE_FORMAT \"%s (of which at least: \" SIZE_FORMAT \"%s are to be promoted), \"\n+          \"old: \" SIZE_FORMAT \"%s\",\n+          byte_size_in_proper_unit(collected_young), proper_unit_for_byte_size(collected_young),\n+          byte_size_in_proper_unit(collected_promoted), proper_unit_for_byte_size(collected_promoted),\n+          byte_size_in_proper_unit(collected_old), proper_unit_for_byte_size(collected_old));\n+}\n","filename":"src\/hotspot\/share\/gc\/shenandoah\/heuristics\/shenandoahGenerationalHeuristics.cpp","additions":284,"deletions":0,"binary":false,"changes":284,"status":"added"},{"patch":"@@ -0,0 +1,59 @@\n+\/*\n+ * Copyright Amazon.com Inc. or its affiliates. All Rights Reserved.\n+ * DO NOT ALTER OR REMOVE COPYRIGHT NOTICES OR THIS FILE HEADER.\n+ *\n+ * This code is free software; you can redistribute it and\/or modify it\n+ * under the terms of the GNU General Public License version 2 only, as\n+ * published by the Free Software Foundation.\n+ *\n+ * This code is distributed in the hope that it will be useful, but WITHOUT\n+ * ANY WARRANTY; without even the implied warranty of MERCHANTABILITY or\n+ * FITNESS FOR A PARTICULAR PURPOSE.  See the GNU General Public License\n+ * version 2 for more details (a copy is included in the LICENSE file that\n+ * accompanied this code).\n+ *\n+ * You should have received a copy of the GNU General Public License version\n+ * 2 along with this work; if not, write to the Free Software Foundation,\n+ * Inc., 51 Franklin St, Fifth Floor, Boston, MA 02110-1301 USA.\n+ *\n+ * Please contact Oracle, 500 Oracle Parkway, Redwood Shores, CA 94065 USA\n+ * or visit www.oracle.com if you need additional information or have any\n+ * questions.\n+ *\n+ *\/\n+\n+#ifndef SHARE_GC_SHENANDOAH_HEURISTICS_SHENANDOAHGENERATIONALHEURISTICS_HPP\n+#define SHARE_GC_SHENANDOAH_HEURISTICS_SHENANDOAHGENERATIONALHEURISTICS_HPP\n+\n+\n+#include \"gc\/shenandoah\/heuristics\/shenandoahAdaptiveHeuristics.hpp\"\n+\n+class ShenandoahGeneration;\n+\n+\/*\n+ * This class serves as the base class for heuristics used to trigger and\n+ * choose the collection sets for young and global collections. It leans\n+ * heavily on the existing functionality of ShenandoahAdaptiveHeuristics.\n+ *\n+ * It differs from the base class primarily in that choosing the collection\n+ * set is responsible for mixed collections and in-place promotions of tenured\n+ * regions.\n+ *\/\n+class ShenandoahGenerationalHeuristics : public ShenandoahAdaptiveHeuristics {\n+\n+public:\n+  explicit ShenandoahGenerationalHeuristics(ShenandoahGeneration* generation);\n+\n+  void choose_collection_set(ShenandoahCollectionSet* collection_set) override;\n+protected:\n+  ShenandoahGeneration* _generation;\n+\n+  size_t add_preselected_regions_to_collection_set(ShenandoahCollectionSet* cset,\n+                                                   const RegionData* data,\n+                                                   size_t size) const;\n+\n+  void log_cset_composition(ShenandoahCollectionSet* cset) const;\n+};\n+\n+\n+#endif \/\/SHARE_GC_SHENANDOAH_HEURISTICS_SHENANDOAHGENERATIONALHEURISTICS_HPP\n","filename":"src\/hotspot\/share\/gc\/shenandoah\/heuristics\/shenandoahGenerationalHeuristics.hpp","additions":59,"deletions":0,"binary":false,"changes":59,"status":"added"},{"patch":"@@ -0,0 +1,146 @@\n+\/*\n+ * Copyright Amazon.com Inc. or its affiliates. All Rights Reserved.\n+ * DO NOT ALTER OR REMOVE COPYRIGHT NOTICES OR THIS FILE HEADER.\n+ *\n+ * This code is free software; you can redistribute it and\/or modify it\n+ * under the terms of the GNU General Public License version 2 only, as\n+ * published by the Free Software Foundation.\n+ *\n+ * This code is distributed in the hope that it will be useful, but WITHOUT\n+ * ANY WARRANTY; without even the implied warranty of MERCHANTABILITY or\n+ * FITNESS FOR A PARTICULAR PURPOSE.  See the GNU General Public License\n+ * version 2 for more details (a copy is included in the LICENSE file that\n+ * accompanied this code).\n+ *\n+ * You should have received a copy of the GNU General Public License version\n+ * 2 along with this work; if not, write to the Free Software Foundation,\n+ * Inc., 51 Franklin St, Fifth Floor, Boston, MA 02110-1301 USA.\n+ *\n+ * Please contact Oracle, 500 Oracle Parkway, Redwood Shores, CA 94065 USA\n+ * or visit www.oracle.com if you need additional information or have any\n+ * questions.\n+ *\n+ *\/\n+\n+#include \"precompiled.hpp\"\n+\n+#include \"gc\/shenandoah\/heuristics\/shenandoahGlobalHeuristics.hpp\"\n+#include \"gc\/shenandoah\/shenandoahCollectorPolicy.hpp\"\n+#include \"gc\/shenandoah\/shenandoahGenerationalHeap.inline.hpp\"\n+#include \"gc\/shenandoah\/shenandoahGlobalGeneration.hpp\"\n+#include \"gc\/shenandoah\/shenandoahHeapRegion.inline.hpp\"\n+\n+#include \"utilities\/quickSort.hpp\"\n+\n+ShenandoahGlobalHeuristics::ShenandoahGlobalHeuristics(ShenandoahGlobalGeneration* generation)\n+        : ShenandoahGenerationalHeuristics(generation) {\n+}\n+\n+\n+void ShenandoahGlobalHeuristics::choose_collection_set_from_regiondata(ShenandoahCollectionSet* cset,\n+                                                                       RegionData* data, size_t size,\n+                                                                       size_t actual_free) {\n+  \/\/ Better select garbage-first regions\n+  QuickSort::sort<RegionData>(data, (int) size, compare_by_garbage, false);\n+\n+  choose_global_collection_set(cset, data, size, actual_free, 0 \/* cur_young_garbage *\/);\n+\n+  log_cset_composition(cset);\n+}\n+\n+\n+void ShenandoahGlobalHeuristics::choose_global_collection_set(ShenandoahCollectionSet* cset,\n+                                                              const ShenandoahHeuristics::RegionData* data,\n+                                                              size_t size, size_t actual_free,\n+                                                              size_t cur_young_garbage) const {\n+  auto heap = ShenandoahGenerationalHeap::heap();\n+  size_t region_size_bytes = ShenandoahHeapRegion::region_size_bytes();\n+  size_t capacity = heap->soft_max_capacity();\n+  size_t garbage_threshold = region_size_bytes * ShenandoahGarbageThreshold \/ 100;\n+  size_t ignore_threshold = region_size_bytes * ShenandoahIgnoreGarbageThreshold \/ 100;\n+\n+  size_t young_evac_reserve = heap->young_generation()->get_evacuation_reserve();\n+  size_t old_evac_reserve = heap->old_generation()->get_evacuation_reserve();\n+  size_t max_young_cset = (size_t) (young_evac_reserve \/ ShenandoahEvacWaste);\n+  size_t young_cur_cset = 0;\n+  size_t max_old_cset = (size_t) (old_evac_reserve \/ ShenandoahOldEvacWaste);\n+  size_t old_cur_cset = 0;\n+\n+  \/\/ Figure out how many unaffiliated young regions are dedicated to mutator and to evacuator.  Allow the young\n+  \/\/ collector's unaffiliated regions to be transferred to old-gen if old-gen has more easily reclaimed garbage\n+  \/\/ than young-gen.  At the end of this cycle, any excess regions remaining in old-gen will be transferred back\n+  \/\/ to young.  Do not transfer the mutator's unaffiliated regions to old-gen.  Those must remain available\n+  \/\/ to the mutator as it needs to be able to consume this memory during concurrent GC.\n+\n+  size_t unaffiliated_young_regions = heap->young_generation()->free_unaffiliated_regions();\n+  size_t unaffiliated_young_memory = unaffiliated_young_regions * region_size_bytes;\n+\n+  if (unaffiliated_young_memory > max_young_cset) {\n+    size_t unaffiliated_mutator_memory = unaffiliated_young_memory - max_young_cset;\n+    unaffiliated_young_memory -= unaffiliated_mutator_memory;\n+    unaffiliated_young_regions = unaffiliated_young_memory \/ region_size_bytes; \/\/ round down\n+    unaffiliated_young_memory = unaffiliated_young_regions * region_size_bytes;\n+  }\n+\n+  \/\/ We'll affiliate these unaffiliated regions with either old or young, depending on need.\n+  max_young_cset -= unaffiliated_young_memory;\n+\n+  \/\/ Keep track of how many regions we plan to transfer from young to old.\n+  size_t regions_transferred_to_old = 0;\n+\n+  size_t free_target = (capacity * ShenandoahMinFreeThreshold) \/ 100 + max_young_cset;\n+  size_t min_garbage = (free_target > actual_free) ? (free_target - actual_free) : 0;\n+\n+  log_info(gc, ergo)(\"Adaptive CSet Selection for GLOBAL. Max Young Evacuation: \" SIZE_FORMAT\n+                     \"%s, Max Old Evacuation: \" SIZE_FORMAT \"%s, Actual Free: \" SIZE_FORMAT \"%s.\",\n+                     byte_size_in_proper_unit(max_young_cset), proper_unit_for_byte_size(max_young_cset),\n+                     byte_size_in_proper_unit(max_old_cset), proper_unit_for_byte_size(max_old_cset),\n+                     byte_size_in_proper_unit(actual_free), proper_unit_for_byte_size(actual_free));\n+\n+  for (size_t idx = 0; idx < size; idx++) {\n+    ShenandoahHeapRegion* r = data[idx].get_region();\n+    assert(!cset->is_preselected(r->index()), \"There should be no preselected regions during GLOBAL GC\");\n+    bool add_region = false;\n+    if (r->is_old() || heap->is_tenurable(r)) {\n+      size_t new_cset = old_cur_cset + r->get_live_data_bytes();\n+      if ((r->garbage() > garbage_threshold)) {\n+        while ((new_cset > max_old_cset) && (unaffiliated_young_regions > 0)) {\n+          unaffiliated_young_regions--;\n+          regions_transferred_to_old++;\n+          max_old_cset += region_size_bytes \/ ShenandoahOldEvacWaste;\n+        }\n+      }\n+      if ((new_cset <= max_old_cset) && (r->garbage() > garbage_threshold)) {\n+        add_region = true;\n+        old_cur_cset = new_cset;\n+      }\n+    } else {\n+      assert(r->is_young() && !heap->is_tenurable(r), \"DeMorgan's law (assuming r->is_affiliated)\");\n+      size_t new_cset = young_cur_cset + r->get_live_data_bytes();\n+      size_t region_garbage = r->garbage();\n+      size_t new_garbage = cur_young_garbage + region_garbage;\n+      bool add_regardless = (region_garbage > ignore_threshold) && (new_garbage < min_garbage);\n+\n+      if (add_regardless || (r->garbage() > garbage_threshold)) {\n+        while ((new_cset > max_young_cset) && (unaffiliated_young_regions > 0)) {\n+          unaffiliated_young_regions--;\n+          max_young_cset += region_size_bytes \/ ShenandoahEvacWaste;\n+        }\n+      }\n+      if ((new_cset <= max_young_cset) && (add_regardless || (region_garbage > garbage_threshold))) {\n+        add_region = true;\n+        young_cur_cset = new_cset;\n+        cur_young_garbage = new_garbage;\n+      }\n+    }\n+    if (add_region) {\n+      cset->add_region(r);\n+    }\n+  }\n+\n+  if (regions_transferred_to_old > 0) {\n+    heap->generation_sizer()->force_transfer_to_old(regions_transferred_to_old);\n+    heap->young_generation()->set_evacuation_reserve(young_evac_reserve - regions_transferred_to_old * region_size_bytes);\n+    heap->old_generation()->set_evacuation_reserve(old_evac_reserve + regions_transferred_to_old * region_size_bytes);\n+  }\n+}\n","filename":"src\/hotspot\/share\/gc\/shenandoah\/heuristics\/shenandoahGlobalHeuristics.cpp","additions":146,"deletions":0,"binary":false,"changes":146,"status":"added"},{"patch":"@@ -0,0 +1,54 @@\n+\/*\n+ * Copyright Amazon.com Inc. or its affiliates. All Rights Reserved.\n+ * DO NOT ALTER OR REMOVE COPYRIGHT NOTICES OR THIS FILE HEADER.\n+ *\n+ * This code is free software; you can redistribute it and\/or modify it\n+ * under the terms of the GNU General Public License version 2 only, as\n+ * published by the Free Software Foundation.\n+ *\n+ * This code is distributed in the hope that it will be useful, but WITHOUT\n+ * ANY WARRANTY; without even the implied warranty of MERCHANTABILITY or\n+ * FITNESS FOR A PARTICULAR PURPOSE.  See the GNU General Public License\n+ * version 2 for more details (a copy is included in the LICENSE file that\n+ * accompanied this code).\n+ *\n+ * You should have received a copy of the GNU General Public License version\n+ * 2 along with this work; if not, write to the Free Software Foundation,\n+ * Inc., 51 Franklin St, Fifth Floor, Boston, MA 02110-1301 USA.\n+ *\n+ * Please contact Oracle, 500 Oracle Parkway, Redwood Shores, CA 94065 USA\n+ * or visit www.oracle.com if you need additional information or have any\n+ * questions.\n+ *\n+ *\/\n+\n+#ifndef SHARE_GC_SHENANDOAH_HEURISTICS_SHENANDOAHGLOBALHEURISTICS_HPP\n+#define SHARE_GC_SHENANDOAH_HEURISTICS_SHENANDOAHGLOBALHEURISTICS_HPP\n+\n+\n+#include \"gc\/shenandoah\/heuristics\/shenandoahGenerationalHeuristics.hpp\"\n+\n+class ShenandoahGlobalGeneration;\n+\n+\/*\n+ * This is a specialization of the generational heuristics which is aware\n+ * of old and young regions and respects the configured evacuation parameters\n+ * for such regions during a global collection of a generational heap.\n+ *\/\n+class ShenandoahGlobalHeuristics : public ShenandoahGenerationalHeuristics {\n+public:\n+  ShenandoahGlobalHeuristics(ShenandoahGlobalGeneration* generation);\n+\n+  void choose_collection_set_from_regiondata(ShenandoahCollectionSet* cset,\n+                                             RegionData* data, size_t size,\n+                                             size_t actual_free) override;\n+\n+private:\n+  void choose_global_collection_set(ShenandoahCollectionSet* cset,\n+                                    const ShenandoahHeuristics::RegionData* data,\n+                                    size_t size, size_t actual_free,\n+                                    size_t cur_young_garbage) const;\n+};\n+\n+\n+#endif \/\/ SHARE_GC_SHENANDOAH_HEURISTICS_SHENANDOAHGLOBALHEURISTICS_HPP\n","filename":"src\/hotspot\/share\/gc\/shenandoah\/heuristics\/shenandoahGlobalHeuristics.hpp","additions":54,"deletions":0,"binary":false,"changes":54,"status":"added"},{"patch":"@@ -3,0 +3,1 @@\n+ * Copyright Amazon.com Inc. or its affiliates. All Rights Reserved.\n@@ -27,1 +28,0 @@\n-#include \"gc\/shenandoah\/shenandoahCollectionSet.inline.hpp\"\n@@ -29,1 +29,0 @@\n-#include \"gc\/shenandoah\/shenandoahHeap.inline.hpp\"\n@@ -36,0 +35,1 @@\n+#include \"utilities\/quickSort.hpp\"\n@@ -37,0 +37,1 @@\n+\/\/ sort by decreasing garbage (so most garbage comes first)\n@@ -38,1 +39,1 @@\n-  if (a._garbage > b._garbage)\n+  if (a.get_garbage() > b.get_garbage()) {\n@@ -40,1 +41,1 @@\n-  else if (a._garbage < b._garbage)\n+  } else if (a.get_garbage() < b.get_garbage()) {\n@@ -42,1 +43,3 @@\n-  else return 0;\n+  } else {\n+    return 0;\n+  }\n@@ -45,1 +48,5 @@\n-ShenandoahHeuristics::ShenandoahHeuristics() :\n+ShenandoahHeuristics::ShenandoahHeuristics(ShenandoahSpaceInfo* space_info) :\n+  _start_gc_is_pending(false),\n+  _declined_trigger_count(0),\n+  _most_recent_declined_trigger_count(0),\n+  _space_info(space_info),\n@@ -47,2 +54,1 @@\n-  _degenerated_cycles_in_a_row(0),\n-  _successful_cycles_in_a_row(0),\n+  _guaranteed_gc_interval(0),\n@@ -53,1 +59,1 @@\n-  _gc_time_history(new TruncatedSeq(10, ShenandoahAdaptiveDecayFactor)),\n+  _gc_cycle_time_history(new TruncatedSeq(Moving_Average_Samples, ShenandoahAdaptiveDecayFactor)),\n@@ -60,0 +66,3 @@\n+  for (size_t i = 0; i < num_regions; i++) {\n+    _region_data[i].clear();\n+  }\n@@ -67,1 +76,1 @@\n-  assert(collection_set->count() == 0, \"Must be empty\");\n+  assert(collection_set->is_empty(), \"Must be empty\");\n@@ -90,2 +99,0 @@\n-  ShenandoahMarkingContext* const ctx = heap->complete_marking_context();\n-\n@@ -109,2 +116,1 @@\n-        candidates[cand_idx]._region = region;\n-        candidates[cand_idx]._garbage = garbage;\n+        candidates[cand_idx].set_region_and_garbage(region, garbage);\n@@ -117,1 +123,1 @@\n-      bool bm_live = ctx->is_marked(cast_to_oop(region->bottom()));\n+      bool bm_live = heap->gc_generation()->complete_marking_context()->is_marked(cast_to_oop(region->bottom()));\n@@ -151,1 +157,0 @@\n-\n@@ -156,2 +161,2 @@\n-                     \"Immediate: \" SIZE_FORMAT \"%s (\" SIZE_FORMAT \"%%), \"\n-                     \"CSet: \" SIZE_FORMAT \"%s (\" SIZE_FORMAT \"%%)\",\n+                     \"Immediate: \" SIZE_FORMAT \"%s (\" SIZE_FORMAT \"%%), \" SIZE_FORMAT \" regions, \"\n+                     \"CSet: \" SIZE_FORMAT \"%s (\" SIZE_FORMAT \"%%), \" SIZE_FORMAT \" regions\",\n@@ -166,0 +171,1 @@\n+                     immediate_regions,\n@@ -169,1 +175,2 @@\n-                     cset_percent);\n+                     cset_percent,\n+                     collection_set->count());\n@@ -181,0 +188,4 @@\n+  if (_start_gc_is_pending) {\n+    log_trigger(\"GC start is already pending\");\n+    return true;\n+  }\n@@ -184,1 +195,2 @@\n-    log_info(gc)(\"Trigger: %s\", GCCause::to_string(GCCause::_metadata_GC_threshold));\n+    log_trigger(\"%s\", GCCause::to_string(GCCause::_metadata_GC_threshold));\n+    accept_trigger();\n@@ -188,1 +200,1 @@\n-  if (ShenandoahGuaranteedGCInterval > 0) {\n+  if (_guaranteed_gc_interval > 0) {\n@@ -190,3 +202,4 @@\n-    if (last_time_ms > ShenandoahGuaranteedGCInterval) {\n-      log_info(gc)(\"Trigger: Time since last GC (%.0f ms) is larger than guaranteed interval (\" UINTX_FORMAT \" ms)\",\n-                   last_time_ms, ShenandoahGuaranteedGCInterval);\n+    if (last_time_ms > _guaranteed_gc_interval) {\n+      log_trigger(\"Time since last GC (%.0f ms) is larger than guaranteed interval (\" UINTX_FORMAT \" ms)\",\n+                   last_time_ms, _guaranteed_gc_interval);\n+      accept_trigger();\n@@ -196,1 +209,1 @@\n-\n+  decline_trigger();\n@@ -201,1 +214,1 @@\n-  return _degenerated_cycles_in_a_row <= ShenandoahFullGCThreshold;\n+  return ShenandoahHeap::heap()->shenandoah_policy()->consecutive_degenerated_gc_count() <= ShenandoahFullGCThreshold;\n@@ -206,1 +219,7 @@\n-          \"In range before adjustment: \" INTX_FORMAT, _gc_time_penalties);\n+         \"In range before adjustment: \" INTX_FORMAT, _gc_time_penalties);\n+\n+  if ((_most_recent_declined_trigger_count <= Penalty_Free_Declinations) && (step > 0)) {\n+    \/\/ Don't penalize if heuristics are not responsible for a negative outcome.  Allow Penalty_Free_Declinations following\n+    \/\/ previous GC for self calibration without penalty.\n+    step = 0;\n+  }\n@@ -218,1 +237,1 @@\n-          \"In range after adjustment: \" INTX_FORMAT, _gc_time_penalties);\n+         \"In range after adjustment: \" INTX_FORMAT, _gc_time_penalties);\n@@ -221,3 +240,17 @@\n-void ShenandoahHeuristics::record_success_concurrent() {\n-  _degenerated_cycles_in_a_row = 0;\n-  _successful_cycles_in_a_row++;\n+void ShenandoahHeuristics::log_trigger(const char* fmt, ...) {\n+  LogTarget(Info, gc) lt;\n+  if (lt.is_enabled()) {\n+    ResourceMark rm;\n+    LogStream ls(lt);\n+    ls.print_raw(\"Trigger\", 7);\n+    if (ShenandoahHeap::heap()->mode()->is_generational()) {\n+      ls.print(\" (%s)\", _space_info->name());\n+    }\n+    ls.print_raw(\": \", 2);\n+    va_list va;\n+    va_start(va, fmt);\n+    ls.vprint(fmt, va);\n+    va_end(va);\n+    ls.cr();\n+  }\n+}\n@@ -225,1 +258,2 @@\n-  _gc_time_history->add(time_since_last_gc());\n+void ShenandoahHeuristics::record_success_concurrent() {\n+  _gc_cycle_time_history->add(elapsed_cycle_time());\n@@ -232,3 +266,0 @@\n-  _degenerated_cycles_in_a_row++;\n-  _successful_cycles_in_a_row = 0;\n-\n@@ -239,3 +270,0 @@\n-  _degenerated_cycles_in_a_row = 0;\n-  _successful_cycles_in_a_row++;\n-\n@@ -256,2 +284,1 @@\n-  if (!ClassUnloading) return false;\n-  return true;\n+  return ClassUnloading;\n@@ -270,1 +297,1 @@\n-double ShenandoahHeuristics::time_since_last_gc() const {\n+double ShenandoahHeuristics::elapsed_cycle_time() const {\n","filename":"src\/hotspot\/share\/gc\/shenandoah\/heuristics\/shenandoahHeuristics.cpp","additions":68,"deletions":41,"binary":false,"changes":109,"status":"modified"},{"patch":"@@ -3,0 +3,1 @@\n+ * Copyright Amazon.com Inc. or its affiliates. All Rights Reserved.\n@@ -28,2 +29,1 @@\n-#include \"gc\/shenandoah\/shenandoahHeap.hpp\"\n-#include \"gc\/shenandoah\/shenandoahPhaseTimings.hpp\"\n+#include \"gc\/shenandoah\/heuristics\/shenandoahSpaceInfo.hpp\"\n@@ -33,0 +33,1 @@\n+#include \"utilities\/numberSeq.hpp\"\n@@ -61,0 +62,5 @@\n+\/*\n+ * Shenandoah heuristics are primarily responsible for deciding when to start\n+ * a collection cycle and choosing which regions will be evacuated during the\n+ * cycle.\n+ *\/\n@@ -66,0 +72,9 @@\n+  \/\/ How many times can I decline a trigger opportunity without being penalized for excessive idle span before trigger?\n+  static const size_t Penalty_Free_Declinations = 16;\n+\n+#ifdef ASSERT\n+  enum UnionTag {\n+    is_uninitialized, is_garbage, is_live_data\n+  };\n+#endif\n+\n@@ -67,1 +82,16 @@\n-  typedef struct {\n+  static const uint Moving_Average_Samples = 10; \/\/ Number of samples to store in moving averages\n+\n+  bool _start_gc_is_pending;              \/\/ True denotes that GC has been triggered, so no need to trigger again.\n+  size_t _declined_trigger_count;         \/\/ This counts how many times since previous GC finished that this\n+                                          \/\/  heuristic has answered false to should_start_gc().\n+  size_t _most_recent_declined_trigger_count;\n+                                       ;  \/\/ This represents the value of _declined_trigger_count as captured at the\n+                                          \/\/  moment the most recent GC effort was triggered.  In case the most recent\n+                                          \/\/  concurrent GC effort degenerates, the value of this variable allows us to\n+                                          \/\/  differentiate between degeneration because heuristic was overly optimistic\n+                                          \/\/  in delaying the trigger vs. degeneration for other reasons (such as the\n+                                          \/\/  most recent GC triggered \"immediately\" after previous GC finished, but the\n+                                          \/\/  free headroom has already been depleted).\n+\n+  class RegionData {\n+    private:\n@@ -69,3 +99,64 @@\n-    size_t _garbage;\n-  } RegionData;\n-\n+    union {\n+      size_t _garbage;          \/\/ Not used by old-gen heuristics.\n+      size_t _live_data;        \/\/ Only used for old-gen heuristics, which prioritizes retention of _live_data over garbage reclaim\n+    } _region_union;\n+#ifdef ASSERT\n+    UnionTag _union_tag;\n+#endif\n+    public:\n+\n+    inline void clear() {\n+      _region = nullptr;\n+      _region_union._garbage = 0;\n+#ifdef ASSERT\n+      _union_tag = is_uninitialized;\n+#endif\n+    }\n+\n+    inline void set_region_and_garbage(ShenandoahHeapRegion* region, size_t garbage) {\n+      _region = region;\n+      _region_union._garbage = garbage;\n+#ifdef ASSERT\n+      _union_tag = is_garbage;\n+#endif\n+    }\n+\n+    inline void set_region_and_livedata(ShenandoahHeapRegion* region, size_t live) {\n+      _region = region;\n+      _region_union._live_data = live;\n+#ifdef ASSERT\n+      _union_tag = is_live_data;\n+#endif\n+    }\n+\n+    inline ShenandoahHeapRegion* get_region() const {\n+      assert(_union_tag != is_uninitialized, \"Cannot fetch region from uninitialized RegionData\");\n+      return _region;\n+    }\n+\n+    inline size_t get_garbage() const {\n+      assert(_union_tag == is_garbage, \"Invalid union fetch\");\n+      return _region_union._garbage;\n+    }\n+\n+    inline size_t get_livedata() const {\n+      assert(_union_tag == is_live_data, \"Invalid union fetch\");\n+      return _region_union._live_data;\n+    }\n+  };\n+\n+  \/\/ Source of information about the memory space managed by this heuristic\n+  ShenandoahSpaceInfo* _space_info;\n+\n+  \/\/ Depending on generation mode, region data represents the results of the relevant\n+  \/\/ most recently completed marking pass:\n+  \/\/   - in GLOBAL mode, global marking pass\n+  \/\/   - in OLD mode,    old-gen marking pass\n+  \/\/   - in YOUNG mode,  young-gen marking pass\n+  \/\/\n+  \/\/ Note that there is some redundancy represented in region data because\n+  \/\/ each instance is an array large enough to hold all regions. However,\n+  \/\/ any region in young-gen is not in old-gen. And any time we are\n+  \/\/ making use of the GLOBAL data, there is no need to maintain the\n+  \/\/ YOUNG or OLD data. Consider this redundancy of data structure to\n+  \/\/ have negligible cost unless proven otherwise.\n@@ -74,2 +165,1 @@\n-  uint _degenerated_cycles_in_a_row;\n-  uint _successful_cycles_in_a_row;\n+  size_t _guaranteed_gc_interval;\n@@ -82,1 +172,1 @@\n-  TruncatedSeq* _gc_time_history;\n+  TruncatedSeq* _gc_cycle_time_history;\n@@ -95,0 +185,10 @@\n+  inline void accept_trigger() {\n+    _most_recent_declined_trigger_count = _declined_trigger_count;\n+    _declined_trigger_count = 0;\n+    _start_gc_is_pending = true;\n+  }\n+\n+  inline void decline_trigger() {\n+    _declined_trigger_count++;\n+  }\n+\n@@ -96,1 +196,1 @@\n-  ShenandoahHeuristics();\n+  ShenandoahHeuristics(ShenandoahSpaceInfo* space_info);\n@@ -103,0 +203,4 @@\n+  void set_guaranteed_gc_interval(size_t guaranteed_gc_interval) {\n+    _guaranteed_gc_interval = guaranteed_gc_interval;\n+  }\n+\n@@ -109,0 +213,4 @@\n+  inline void cancel_trigger_request() {\n+    _start_gc_is_pending = false;\n+  }\n+\n@@ -124,0 +232,3 @@\n+\n+  \/\/ This indicates whether or not the current cycle should unload classes.\n+  \/\/ It does NOT indicate that a cycle should be started.\n@@ -131,1 +242,4 @@\n-  double time_since_last_gc() const;\n+  double elapsed_cycle_time() const;\n+\n+  \/\/ Format prefix and emit log message indicating a GC cycle hs been triggered\n+  void log_trigger(const char* fmt, ...) ATTRIBUTE_PRINTF(2, 3);\n","filename":"src\/hotspot\/share\/gc\/shenandoah\/heuristics\/shenandoahHeuristics.hpp","additions":125,"deletions":11,"binary":false,"changes":136,"status":"modified"},{"patch":"@@ -0,0 +1,753 @@\n+\/*\n+ * Copyright Amazon.com Inc. or its affiliates. All Rights Reserved.\n+ * DO NOT ALTER OR REMOVE COPYRIGHT NOTICES OR THIS FILE HEADER.\n+ *\n+ * This code is free software; you can redistribute it and\/or modify it\n+ * under the terms of the GNU General Public License version 2 only, as\n+ * published by the Free Software Foundation.\n+ *\n+ * This code is distributed in the hope that it will be useful, but WITHOUT\n+ * ANY WARRANTY; without even the implied warranty of MERCHANTABILITY or\n+ * FITNESS FOR A PARTICULAR PURPOSE.  See the GNU General Public License\n+ * version 2 for more details (a copy is included in the LICENSE file that\n+ * accompanied this code).\n+ *\n+ * You should have received a copy of the GNU General Public License version\n+ * 2 along with this work; if not, write to the Free Software Foundation,\n+ * Inc., 51 Franklin St, Fifth Floor, Boston, MA 02110-1301 USA.\n+ *\n+ * Please contact Oracle, 500 Oracle Parkway, Redwood Shores, CA 94065 USA\n+ * or visit www.oracle.com if you need additional information or have any\n+ * questions.\n+ *\n+ *\/\n+\n+#include \"precompiled.hpp\"\n+\n+#include \"gc\/shenandoah\/heuristics\/shenandoahOldHeuristics.hpp\"\n+#include \"gc\/shenandoah\/shenandoahCollectionSet.hpp\"\n+#include \"gc\/shenandoah\/shenandoahCollectorPolicy.hpp\"\n+#include \"gc\/shenandoah\/shenandoahGenerationalHeap.hpp\"\n+#include \"gc\/shenandoah\/shenandoahOldGeneration.hpp\"\n+#include \"gc\/shenandoah\/shenandoahHeap.inline.hpp\"\n+#include \"gc\/shenandoah\/shenandoahHeapRegion.inline.hpp\"\n+#include \"logging\/log.hpp\"\n+#include \"utilities\/quickSort.hpp\"\n+\n+uint ShenandoahOldHeuristics::NOT_FOUND = -1U;\n+\n+\/\/ sort by increasing live (so least live comes first)\n+int ShenandoahOldHeuristics::compare_by_live(RegionData a, RegionData b) {\n+  if (a.get_livedata() < b.get_livedata()) {\n+    return -1;\n+  } else if (a.get_livedata() > b.get_livedata()) {\n+    return 1;\n+  } else {\n+    return 0;\n+  }\n+}\n+\n+\/\/ sort by increasing index\n+int ShenandoahOldHeuristics::compare_by_index(RegionData a, RegionData b) {\n+  if (a.get_region()->index() < b.get_region()->index()) {\n+    return -1;\n+  } else if (a.get_region()->index() > b.get_region()->index()) {\n+    return 1;\n+  } else {\n+    \/\/ quicksort may compare to self during search for pivot\n+    return 0;\n+  }\n+}\n+\n+ShenandoahOldHeuristics::ShenandoahOldHeuristics(ShenandoahOldGeneration* generation, ShenandoahGenerationalHeap* gen_heap) :\n+  ShenandoahHeuristics(generation),\n+  _heap(gen_heap),\n+  _first_pinned_candidate(NOT_FOUND),\n+  _last_old_collection_candidate(0),\n+  _next_old_collection_candidate(0),\n+  _last_old_region(0),\n+  _live_bytes_in_unprocessed_candidates(0),\n+  _old_generation(generation),\n+  _cannot_expand_trigger(false),\n+  _fragmentation_trigger(false),\n+  _growth_trigger(false),\n+  _fragmentation_density(0.0),\n+  _fragmentation_first_old_region(0),\n+  _fragmentation_last_old_region(0)\n+{\n+}\n+\n+bool ShenandoahOldHeuristics::prime_collection_set(ShenandoahCollectionSet* collection_set) {\n+  if (unprocessed_old_collection_candidates() == 0) {\n+    return false;\n+  }\n+\n+  if (_old_generation->is_preparing_for_mark()) {\n+    \/\/ We have unprocessed old collection candidates, but the heuristic has given up on evacuating them.\n+    \/\/ This is most likely because they were _all_ pinned at the time of the last mixed evacuation (and\n+    \/\/ this in turn is most likely because there are just one or two candidate regions remaining).\n+    log_info(gc, ergo)(\"Remaining \" UINT32_FORMAT \" old regions are being coalesced and filled\", unprocessed_old_collection_candidates());\n+    return false;\n+  }\n+\n+  _first_pinned_candidate = NOT_FOUND;\n+\n+  uint included_old_regions = 0;\n+  size_t evacuated_old_bytes = 0;\n+  size_t collected_old_bytes = 0;\n+\n+  \/\/ If a region is put into the collection set, then this region's free (not yet used) bytes are no longer\n+  \/\/ \"available\" to hold the results of other evacuations.  This may cause a decrease in the remaining amount\n+  \/\/ of memory that can still be evacuated.  We address this by reducing the evacuation budget by the amount\n+  \/\/ of live memory in that region and by the amount of unallocated memory in that region if the evacuation\n+  \/\/ budget is constrained by availability of free memory.\n+  const size_t old_evacuation_reserve = _old_generation->get_evacuation_reserve();\n+  const size_t old_evacuation_budget = (size_t) ((double) old_evacuation_reserve \/ ShenandoahOldEvacWaste);\n+  size_t unfragmented_available = _old_generation->free_unaffiliated_regions() * ShenandoahHeapRegion::region_size_bytes();\n+  size_t fragmented_available;\n+  size_t excess_fragmented_available;\n+\n+  if (unfragmented_available > old_evacuation_budget) {\n+    unfragmented_available = old_evacuation_budget;\n+    fragmented_available = 0;\n+    excess_fragmented_available = 0;\n+  } else {\n+    assert(_old_generation->available() >= old_evacuation_budget, \"Cannot budget more than is available\");\n+    fragmented_available = _old_generation->available() - unfragmented_available;\n+    assert(fragmented_available + unfragmented_available >= old_evacuation_budget, \"Budgets do not add up\");\n+    if (fragmented_available + unfragmented_available > old_evacuation_budget) {\n+      excess_fragmented_available = (fragmented_available + unfragmented_available) - old_evacuation_budget;\n+      fragmented_available -= excess_fragmented_available;\n+    }\n+  }\n+\n+  size_t remaining_old_evacuation_budget = old_evacuation_budget;\n+  log_debug(gc)(\"Choose old regions for mixed collection: old evacuation budget: \" SIZE_FORMAT \"%s, candidates: %u\",\n+                byte_size_in_proper_unit(old_evacuation_budget), proper_unit_for_byte_size(old_evacuation_budget),\n+                unprocessed_old_collection_candidates());\n+\n+  size_t lost_evacuation_capacity = 0;\n+\n+  \/\/ The number of old-gen regions that were selected as candidates for collection at the end of the most recent old-gen\n+  \/\/ concurrent marking phase and have not yet been collected is represented by unprocessed_old_collection_candidates().\n+  \/\/ Candidate regions are ordered according to increasing amount of live data.  If there is not sufficient room to\n+  \/\/ evacuate region N, then there is no need to even consider evacuating region N+1.\n+  while (unprocessed_old_collection_candidates() > 0) {\n+    \/\/ Old collection candidates are sorted in order of decreasing garbage contained therein.\n+    ShenandoahHeapRegion* r = next_old_collection_candidate();\n+    if (r == nullptr) {\n+      break;\n+    }\n+    assert(r->is_regular(), \"There should be no humongous regions in the set of mixed-evac candidates\");\n+\n+    \/\/ If region r is evacuated to fragmented memory (to free memory within a partially used region), then we need\n+    \/\/ to decrease the capacity of the fragmented memory by the scaled loss.\n+\n+    size_t live_data_for_evacuation = r->get_live_data_bytes();\n+    size_t lost_available = r->free();\n+\n+    if ((lost_available > 0) && (excess_fragmented_available > 0)) {\n+      if (lost_available < excess_fragmented_available) {\n+        excess_fragmented_available -= lost_available;\n+        lost_evacuation_capacity -= lost_available;\n+        lost_available  = 0;\n+      } else {\n+        lost_available -= excess_fragmented_available;\n+        lost_evacuation_capacity -= excess_fragmented_available;\n+        excess_fragmented_available = 0;\n+      }\n+    }\n+    size_t scaled_loss = (size_t) ((double) lost_available \/ ShenandoahOldEvacWaste);\n+    if ((lost_available > 0) && (fragmented_available > 0)) {\n+      if (scaled_loss + live_data_for_evacuation < fragmented_available) {\n+        fragmented_available -= scaled_loss;\n+        scaled_loss = 0;\n+      } else {\n+        \/\/ We will have to allocate this region's evacuation memory from unfragmented memory, so don't bother\n+        \/\/ to decrement scaled_loss\n+      }\n+    }\n+    if (scaled_loss > 0) {\n+      \/\/ We were not able to account for the lost free memory within fragmented memory, so we need to take this\n+      \/\/ allocation out of unfragmented memory.  Unfragmented memory does not need to account for loss of free.\n+      if (live_data_for_evacuation > unfragmented_available) {\n+        \/\/ There is not room to evacuate this region or any that come after it in within the candidates array.\n+        break;\n+      } else {\n+        unfragmented_available -= live_data_for_evacuation;\n+      }\n+    } else {\n+      \/\/ Since scaled_loss == 0, we have accounted for the loss of free memory, so we can allocate from either\n+      \/\/ fragmented or unfragmented available memory.  Use up the fragmented memory budget first.\n+      size_t evacuation_need = live_data_for_evacuation;\n+\n+      if (evacuation_need > fragmented_available) {\n+        evacuation_need -= fragmented_available;\n+        fragmented_available = 0;\n+      } else {\n+        fragmented_available -= evacuation_need;\n+        evacuation_need = 0;\n+      }\n+      if (evacuation_need > unfragmented_available) {\n+        \/\/ There is not room to evacuate this region or any that come after it in within the candidates array.\n+        break;\n+      } else {\n+        unfragmented_available -= evacuation_need;\n+        \/\/ dead code: evacuation_need == 0;\n+      }\n+    }\n+    collection_set->add_region(r);\n+    included_old_regions++;\n+    evacuated_old_bytes += live_data_for_evacuation;\n+    collected_old_bytes += r->garbage();\n+    consume_old_collection_candidate();\n+  }\n+\n+  if (_first_pinned_candidate != NOT_FOUND) {\n+    \/\/ Need to deal with pinned regions\n+    slide_pinned_regions_to_front();\n+  }\n+  decrease_unprocessed_old_collection_candidates_live_memory(evacuated_old_bytes);\n+  if (included_old_regions > 0) {\n+    log_info(gc, ergo)(\"Old-gen piggyback evac (\" UINT32_FORMAT \" regions, evacuating \" PROPERFMT \", reclaiming: \" PROPERFMT \")\",\n+                  included_old_regions, PROPERFMTARGS(evacuated_old_bytes), PROPERFMTARGS(collected_old_bytes));\n+  }\n+\n+  if (unprocessed_old_collection_candidates() == 0) {\n+    \/\/ We have added the last of our collection candidates to a mixed collection.\n+    \/\/ Any triggers that occurred during mixed evacuations may no longer be valid.  They can retrigger if appropriate.\n+    clear_triggers();\n+\n+    _old_generation->complete_mixed_evacuations();\n+  } else if (included_old_regions == 0) {\n+    \/\/ We have candidates, but none were included for evacuation - are they all pinned?\n+    \/\/ or did we just not have enough room for any of them in this collection set?\n+    \/\/ We don't want a region with a stuck pin to prevent subsequent old collections, so\n+    \/\/ if they are all pinned we transition to a state that will allow us to make these uncollected\n+    \/\/ (pinned) regions parsable.\n+    if (all_candidates_are_pinned()) {\n+      log_info(gc, ergo)(\"All candidate regions \" UINT32_FORMAT \" are pinned\", unprocessed_old_collection_candidates());\n+      _old_generation->abandon_mixed_evacuations();\n+    } else {\n+      log_info(gc, ergo)(\"No regions selected for mixed collection. \"\n+                         \"Old evacuation budget: \" PROPERFMT \", Remaining evacuation budget: \" PROPERFMT\n+                         \", Lost capacity: \" PROPERFMT\n+                         \", Next candidate: \" UINT32_FORMAT \", Last candidate: \" UINT32_FORMAT,\n+                         PROPERFMTARGS(old_evacuation_reserve),\n+                         PROPERFMTARGS(remaining_old_evacuation_budget),\n+                         PROPERFMTARGS(lost_evacuation_capacity),\n+                         _next_old_collection_candidate, _last_old_collection_candidate);\n+    }\n+  }\n+\n+  return (included_old_regions > 0);\n+}\n+\n+bool ShenandoahOldHeuristics::all_candidates_are_pinned() {\n+#ifdef ASSERT\n+  if (uint(os::random()) % 100 < ShenandoahCoalesceChance) {\n+    return true;\n+  }\n+#endif\n+\n+  for (uint i = _next_old_collection_candidate; i < _last_old_collection_candidate; ++i) {\n+    ShenandoahHeapRegion* region = _region_data[i].get_region();\n+    if (!region->is_pinned()) {\n+      return false;\n+    }\n+  }\n+  return true;\n+}\n+\n+void ShenandoahOldHeuristics::slide_pinned_regions_to_front() {\n+  \/\/ Find the first unpinned region to the left of the next region that\n+  \/\/ will be added to the collection set. These regions will have been\n+  \/\/ added to the cset, so we can use them to hold pointers to regions\n+  \/\/ that were pinned when the cset was chosen.\n+  \/\/ [ r p r p p p r r ]\n+  \/\/     ^         ^ ^\n+  \/\/     |         | | pointer to next region to add to a mixed collection is here.\n+  \/\/     |         | first r to the left should be in the collection set now.\n+  \/\/     | first pinned region, we don't need to look past this\n+  uint write_index = NOT_FOUND;\n+  for (uint search = _next_old_collection_candidate - 1; search > _first_pinned_candidate; --search) {\n+    ShenandoahHeapRegion* region = _region_data[search].get_region();\n+    if (!region->is_pinned()) {\n+      write_index = search;\n+      assert(region->is_cset(), \"Expected unpinned region to be added to the collection set.\");\n+      break;\n+    }\n+  }\n+\n+  \/\/ If we could not find an unpinned region, it means there are no slots available\n+  \/\/ to move up the pinned regions. In this case, we just reset our next index in the\n+  \/\/ hopes that some of these regions will become unpinned before the next mixed\n+  \/\/ collection. We may want to bailout of here instead, as it should be quite\n+  \/\/ rare to have so many pinned regions and may indicate something is wrong.\n+  if (write_index == NOT_FOUND) {\n+    assert(_first_pinned_candidate != NOT_FOUND, \"Should only be here if there are pinned regions.\");\n+    _next_old_collection_candidate = _first_pinned_candidate;\n+    return;\n+  }\n+\n+  \/\/ Find pinned regions to the left and move their pointer into a slot\n+  \/\/ that was pointing at a region that has been added to the cset (or was pointing\n+  \/\/ to a pinned region that we've already moved up). We are done when the leftmost\n+  \/\/ pinned region has been slid up.\n+  \/\/ [ r p r x p p p r ]\n+  \/\/         ^       ^\n+  \/\/         |       | next region for mixed collections\n+  \/\/         | Write pointer is here. We know this region is already in the cset\n+  \/\/         | so we can clobber it with the next pinned region we find.\n+  for (int32_t search = (int32_t)write_index - 1; search >= (int32_t)_first_pinned_candidate; --search) {\n+    RegionData& skipped = _region_data[search];\n+    if (skipped.get_region()->is_pinned()) {\n+      RegionData& available_slot = _region_data[write_index];\n+      available_slot.set_region_and_livedata(skipped.get_region(), skipped.get_livedata());\n+      --write_index;\n+    }\n+  }\n+\n+  \/\/ Update to read from the leftmost pinned region. Plus one here because we decremented\n+  \/\/ the write index to hold the next found pinned region. We are just moving it back now\n+  \/\/ to point to the first pinned region.\n+  _next_old_collection_candidate = write_index + 1;\n+}\n+\n+void ShenandoahOldHeuristics::prepare_for_old_collections() {\n+  ShenandoahHeap* heap = ShenandoahHeap::heap();\n+\n+  const size_t num_regions = heap->num_regions();\n+  size_t cand_idx = 0;\n+  size_t immediate_garbage = 0;\n+  size_t immediate_regions = 0;\n+  size_t live_data = 0;\n+\n+  RegionData* candidates = _region_data;\n+  for (size_t i = 0; i < num_regions; i++) {\n+    ShenandoahHeapRegion* region = heap->get_region(i);\n+    if (!region->is_old()) {\n+      continue;\n+    }\n+\n+    size_t garbage = region->garbage();\n+    size_t live_bytes = region->get_live_data_bytes();\n+    live_data += live_bytes;\n+\n+    if (region->is_regular() || region->is_regular_pinned()) {\n+        \/\/ Only place regular or pinned regions with live data into the candidate set.\n+        \/\/ Pinned regions cannot be evacuated, but we are not actually choosing candidates\n+        \/\/ for the collection set here. That happens later during the next young GC cycle,\n+        \/\/ by which time, the pinned region may no longer be pinned.\n+      if (!region->has_live()) {\n+        assert(!region->is_pinned(), \"Pinned region should have live (pinned) objects.\");\n+        region->make_trash_immediate();\n+        immediate_regions++;\n+        immediate_garbage += garbage;\n+      } else {\n+        region->begin_preemptible_coalesce_and_fill();\n+        candidates[cand_idx].set_region_and_livedata(region, live_bytes);\n+        cand_idx++;\n+      }\n+    } else if (region->is_humongous_start()) {\n+      \/\/ This will handle humongous start regions whether they are also pinned, or not.\n+      \/\/ If they are pinned, we expect them to hold live data, so they will not be\n+      \/\/ turned into immediate garbage.\n+      if (!region->has_live()) {\n+        assert(!region->is_pinned(), \"Pinned region should have live (pinned) objects.\");\n+        \/\/ The humongous object is dead, we can just return this region and the continuations\n+        \/\/ immediately to the freeset - no evacuations are necessary here. The continuations\n+        \/\/ will be made into trash by this method, so they'll be skipped by the 'is_regular'\n+        \/\/ check above, but we still need to count the start region.\n+        immediate_regions++;\n+        immediate_garbage += garbage;\n+        size_t region_count = heap->trash_humongous_region_at(region);\n+        log_debug(gc)(\"Trashed \" SIZE_FORMAT \" regions for humongous object.\", region_count);\n+      }\n+    } else if (region->is_trash()) {\n+      \/\/ Count humongous objects made into trash here.\n+      immediate_regions++;\n+      immediate_garbage += garbage;\n+    }\n+  }\n+\n+  _old_generation->set_live_bytes_after_last_mark(live_data);\n+\n+  \/\/ Unlike young, we are more interested in efficiently packing OLD-gen than in reclaiming garbage first.  We sort by live-data.\n+  \/\/ Some regular regions may have been promoted in place with no garbage but also with very little live data.  When we \"compact\"\n+  \/\/ old-gen, we want to pack these underutilized regions together so we can have more unaffiliated (unfragmented) free regions\n+  \/\/ in old-gen.\n+\n+  QuickSort::sort<RegionData>(candidates, cand_idx, compare_by_live, false);\n+\n+  const size_t region_size_bytes = ShenandoahHeapRegion::region_size_bytes();\n+\n+  \/\/ The convention is to collect regions that have more than this amount of garbage.\n+  const size_t garbage_threshold = region_size_bytes * ShenandoahOldGarbageThreshold \/ 100;\n+\n+  \/\/ Enlightened interpretation: collect regions that have less than this amount of live.\n+  const size_t live_threshold = region_size_bytes - garbage_threshold;\n+\n+  _last_old_region = (uint)cand_idx;\n+  _last_old_collection_candidate = (uint)cand_idx;\n+  _next_old_collection_candidate = 0;\n+\n+  size_t unfragmented = 0;\n+  size_t candidates_garbage = 0;\n+\n+  for (size_t i = 0; i < cand_idx; i++) {\n+    size_t live = candidates[i].get_livedata();\n+    if (live > live_threshold) {\n+      \/\/ Candidates are sorted in increasing order of live data, so no regions after this will be below the threshold.\n+      _last_old_collection_candidate = (uint)i;\n+      break;\n+    }\n+    ShenandoahHeapRegion* r = candidates[i].get_region();\n+    size_t region_garbage = r->garbage();\n+    size_t region_free = r->free();\n+    candidates_garbage += region_garbage;\n+    unfragmented += region_free;\n+  }\n+\n+  \/\/ defrag_count represents regions that are placed into the old collection set in order to defragment the memory\n+  \/\/ that we try to \"reserve\" for humongous allocations.\n+  size_t defrag_count = 0;\n+  size_t total_uncollected_old_regions = _last_old_region - _last_old_collection_candidate;\n+\n+  if (cand_idx > _last_old_collection_candidate) {\n+    \/\/ Above, we have added into the set of mixed-evacuation candidates all old-gen regions for which the live memory\n+    \/\/ that they contain is below a particular old-garbage threshold.  Regions that were not selected for the collection\n+    \/\/ set hold enough live memory that it is not considered efficient (by \"garbage-first standards\") to compact these\n+    \/\/ at the current time.\n+    \/\/\n+    \/\/ However, if any of these regions that were rejected from the collection set reside within areas of memory that\n+    \/\/ might interfere with future humongous allocation requests, we will prioritize them for evacuation at this time.\n+    \/\/ Humongous allocations target the bottom of the heap.  We want old-gen regions to congregate at the top of the\n+    \/\/ heap.\n+    \/\/\n+    \/\/ Sort the regions that were initially rejected from the collection set in order of index.  This allows us to\n+    \/\/ focus our attention on the regions that have low index value (i.e. the old-gen regions at the bottom of the heap).\n+    QuickSort::sort<RegionData>(candidates + _last_old_collection_candidate, cand_idx - _last_old_collection_candidate,\n+                                compare_by_index, false);\n+\n+    const size_t first_unselected_old_region = candidates[_last_old_collection_candidate].get_region()->index();\n+    const size_t last_unselected_old_region = candidates[cand_idx - 1].get_region()->index();\n+    size_t span_of_uncollected_regions = 1 + last_unselected_old_region - first_unselected_old_region;\n+\n+    \/\/ Add no more than 1\/8 of the existing old-gen regions to the set of mixed evacuation candidates.\n+    const int MAX_FRACTION_OF_HUMONGOUS_DEFRAG_REGIONS = 8;\n+    const size_t bound_on_additional_regions = cand_idx \/ MAX_FRACTION_OF_HUMONGOUS_DEFRAG_REGIONS;\n+\n+    \/\/ The heuristic old_is_fragmented trigger may be seeking to achieve up to 75% density.  Allow ourselves to overshoot\n+    \/\/ that target (at 7\/8) so we will not have to do another defragmenting old collection right away.\n+    while ((defrag_count < bound_on_additional_regions) &&\n+           (total_uncollected_old_regions < 7 * span_of_uncollected_regions \/ 8)) {\n+      ShenandoahHeapRegion* r = candidates[_last_old_collection_candidate].get_region();\n+      assert(r->is_regular() || r->is_regular_pinned(), \"Region \" SIZE_FORMAT \" has wrong state for collection: %s\",\n+             r->index(), ShenandoahHeapRegion::region_state_to_string(r->state()));\n+      const size_t region_garbage = r->garbage();\n+      const size_t region_free = r->free();\n+      candidates_garbage += region_garbage;\n+      unfragmented += region_free;\n+      defrag_count++;\n+      _last_old_collection_candidate++;\n+\n+      \/\/ We now have one fewer uncollected regions, and our uncollected span shrinks because we have removed its first region.\n+      total_uncollected_old_regions--;\n+      span_of_uncollected_regions =\n+        1 + last_unselected_old_region - candidates[_last_old_collection_candidate].get_region()->index();\n+    }\n+  }\n+\n+  \/\/ Note that we do not coalesce and fill occupied humongous regions\n+  \/\/ HR: humongous regions, RR: regular regions, CF: coalesce and fill regions\n+  const size_t collectable_garbage = immediate_garbage + candidates_garbage;\n+  const size_t old_candidates = _last_old_collection_candidate;\n+  const size_t mixed_evac_live = old_candidates * region_size_bytes - (candidates_garbage + unfragmented);\n+  set_unprocessed_old_collection_candidates_live_memory(mixed_evac_live);\n+\n+  log_info(gc, ergo)(\"Old-Gen Collectable Garbage: \" PROPERFMT \" consolidated with free: \" PROPERFMT \", over \" SIZE_FORMAT \" regions\",\n+                     PROPERFMTARGS(collectable_garbage), PROPERFMTARGS(unfragmented), old_candidates);\n+  log_info(gc, ergo)(\"Old-Gen Immediate Garbage: \" PROPERFMT \" over \" SIZE_FORMAT \" regions\",\n+                     PROPERFMTARGS(immediate_garbage), immediate_regions);\n+  log_info(gc, ergo)(\"Old regions selected for defragmentation: \" SIZE_FORMAT, defrag_count);\n+  log_info(gc, ergo)(\"Old regions not selected: \" SIZE_FORMAT, total_uncollected_old_regions);\n+\n+  if (unprocessed_old_collection_candidates() > 0) {\n+    _old_generation->transition_to(ShenandoahOldGeneration::EVACUATING);\n+  } else if (has_coalesce_and_fill_candidates()) {\n+    _old_generation->transition_to(ShenandoahOldGeneration::FILLING);\n+  } else {\n+    _old_generation->transition_to(ShenandoahOldGeneration::WAITING_FOR_BOOTSTRAP);\n+  }\n+}\n+\n+size_t ShenandoahOldHeuristics::unprocessed_old_collection_candidates_live_memory() const {\n+  return _live_bytes_in_unprocessed_candidates;\n+}\n+\n+void ShenandoahOldHeuristics::set_unprocessed_old_collection_candidates_live_memory(size_t initial_live) {\n+  _live_bytes_in_unprocessed_candidates = initial_live;\n+}\n+\n+void ShenandoahOldHeuristics::decrease_unprocessed_old_collection_candidates_live_memory(size_t evacuated_live) {\n+  assert(evacuated_live <= _live_bytes_in_unprocessed_candidates, \"Cannot evacuate more than was present\");\n+  _live_bytes_in_unprocessed_candidates -= evacuated_live;\n+}\n+\n+\/\/ Used by unit test: test_shenandoahOldHeuristic.cpp\n+uint ShenandoahOldHeuristics::last_old_collection_candidate_index() const {\n+  return _last_old_collection_candidate;\n+}\n+\n+uint ShenandoahOldHeuristics::unprocessed_old_collection_candidates() const {\n+  return _last_old_collection_candidate - _next_old_collection_candidate;\n+}\n+\n+ShenandoahHeapRegion* ShenandoahOldHeuristics::next_old_collection_candidate() {\n+  while (_next_old_collection_candidate < _last_old_collection_candidate) {\n+    ShenandoahHeapRegion* next = _region_data[_next_old_collection_candidate].get_region();\n+    if (!next->is_pinned()) {\n+      return next;\n+    } else {\n+      assert(next->is_pinned(), \"sanity\");\n+      if (_first_pinned_candidate == NOT_FOUND) {\n+        _first_pinned_candidate = _next_old_collection_candidate;\n+      }\n+    }\n+\n+    _next_old_collection_candidate++;\n+  }\n+  return nullptr;\n+}\n+\n+void ShenandoahOldHeuristics::consume_old_collection_candidate() {\n+  _next_old_collection_candidate++;\n+}\n+\n+unsigned int ShenandoahOldHeuristics::get_coalesce_and_fill_candidates(ShenandoahHeapRegion** buffer) {\n+  uint end = _last_old_region;\n+  uint index = _next_old_collection_candidate;\n+  while (index < end) {\n+    *buffer++ = _region_data[index++].get_region();\n+  }\n+  return (_last_old_region - _next_old_collection_candidate);\n+}\n+\n+void ShenandoahOldHeuristics::abandon_collection_candidates() {\n+  _last_old_collection_candidate = 0;\n+  _next_old_collection_candidate = 0;\n+  _last_old_region = 0;\n+}\n+\n+void ShenandoahOldHeuristics::record_cycle_end() {\n+  this->ShenandoahHeuristics::record_cycle_end();\n+  clear_triggers();\n+}\n+\n+void ShenandoahOldHeuristics::clear_triggers() {\n+  \/\/ Clear any triggers that were set during mixed evacuations.  Conditions may be different now that this phase has finished.\n+  _cannot_expand_trigger = false;\n+  _fragmentation_trigger = false;\n+  _growth_trigger = false;\n+}\n+\n+\/\/ This triggers old-gen collection if the number of regions \"dedicated\" to old generation is much larger than\n+\/\/ is required to represent the memory currently used within the old generation.  This trigger looks specifically\n+\/\/ at density of the old-gen spanned region.  A different mechanism triggers old-gen GC if the total number of\n+\/\/ old-gen regions (regardless of how close the regions are to one another) grows beyond an anticipated growth target.\n+void ShenandoahOldHeuristics::set_trigger_if_old_is_fragmented(size_t first_old_region, size_t last_old_region,\n+                                                               size_t old_region_count, size_t num_regions) {\n+  if (ShenandoahGenerationalHumongousReserve > 0) {\n+    \/\/ Our intent is to pack old-gen memory into the highest-numbered regions of the heap.  Count all memory\n+    \/\/ above first_old_region as the \"span\" of old generation.\n+    size_t old_region_span = (first_old_region <= last_old_region)? (num_regions - first_old_region): 0;\n+    \/\/ Given that memory at the bottom of the heap is reserved to represent humongous objects, the number of\n+    \/\/ regions that old_gen is \"allowed\" to consume is less than the total heap size.  The restriction on allowed\n+    \/\/ span is not strictly enforced.  This is a heuristic designed to reduce the likelihood that a humongous\n+    \/\/ allocation request will require a STW full GC.\n+    size_t allowed_old_gen_span = num_regions - (ShenandoahGenerationalHumongousReserve * num_regions) \/ 100;\n+\n+    size_t old_available = _old_generation->available() \/ HeapWordSize;\n+    size_t region_size_words = ShenandoahHeapRegion::region_size_words();\n+    size_t old_unaffiliated_available = _old_generation->free_unaffiliated_regions() * region_size_words;\n+    assert(old_available >= old_unaffiliated_available, \"sanity\");\n+    size_t old_fragmented_available = old_available - old_unaffiliated_available;\n+\n+    size_t old_words_consumed = old_region_count * region_size_words - old_fragmented_available;\n+    size_t old_words_spanned = old_region_span * region_size_words;\n+    double old_density = ((double) old_words_consumed) \/ old_words_spanned;\n+\n+    double old_span_percent = ((double) old_region_span) \/ allowed_old_gen_span;\n+    if (old_span_percent > 0.50) {\n+      \/\/ Squaring old_span_percent in the denominator below allows more aggressive triggering when we are\n+      \/\/ above desired maximum span and less aggressive triggering when we are far below the desired maximum span.\n+      double old_span_percent_squared = old_span_percent * old_span_percent;\n+      if (old_density \/ old_span_percent_squared < 0.75) {\n+        \/\/ We trigger old defragmentation, for example, if:\n+        \/\/  old_span_percent is 110% and old_density is below 90.8%, or\n+        \/\/  old_span_percent is 100% and old_density is below 75.0%, or\n+        \/\/  old_span_percent is  90% and old_density is below 60.8%, or\n+        \/\/  old_span_percent is  80% and old_density is below 48.0%, or\n+        \/\/  old_span_percent is  70% and old_density is below 36.8%, or\n+        \/\/  old_span_percent is  60% and old_density is below 27.0%, or\n+        \/\/  old_span_percent is  50% and old_density is below 18.8%.\n+\n+        \/\/ Set the fragmentation trigger and related attributes\n+        _fragmentation_trigger = true;\n+        _fragmentation_density = old_density;\n+        _fragmentation_first_old_region = first_old_region;\n+        _fragmentation_last_old_region = last_old_region;\n+      }\n+    }\n+  }\n+}\n+\n+void ShenandoahOldHeuristics::set_trigger_if_old_is_overgrown() {\n+  size_t old_used = _old_generation->used() + _old_generation->get_humongous_waste();\n+  size_t trigger_threshold = _old_generation->usage_trigger_threshold();\n+  \/\/ Detects unsigned arithmetic underflow\n+  assert(old_used <= _heap->capacity(),\n+         \"Old used (\" SIZE_FORMAT \", \" SIZE_FORMAT\") must not be more than heap capacity (\" SIZE_FORMAT \")\",\n+         _old_generation->used(), _old_generation->get_humongous_waste(), _heap->capacity());\n+  if (old_used > trigger_threshold) {\n+    _growth_trigger = true;\n+  }\n+}\n+\n+void ShenandoahOldHeuristics::evaluate_triggers(size_t first_old_region, size_t last_old_region,\n+                                                size_t old_region_count, size_t num_regions) {\n+  set_trigger_if_old_is_fragmented(first_old_region, last_old_region, old_region_count, num_regions);\n+  set_trigger_if_old_is_overgrown();\n+}\n+\n+bool ShenandoahOldHeuristics::should_resume_old_cycle() {\n+  \/\/ If we are preparing to mark old, or if we are already marking old, then try to continue that work.\n+  if (_old_generation->is_concurrent_mark_in_progress()) {\n+    assert(_old_generation->state() == ShenandoahOldGeneration::MARKING, \"Unexpected old gen state: %s\", _old_generation->state_name());\n+    log_trigger(\"Resume marking old\");\n+    return true;\n+  }\n+\n+  if (_old_generation->is_preparing_for_mark()) {\n+    assert(_old_generation->state() == ShenandoahOldGeneration::FILLING, \"Unexpected old gen state: %s\", _old_generation->state_name());\n+    log_trigger(\"Resume preparing to mark old\");\n+    return true;\n+  }\n+\n+  return false;\n+}\n+\n+bool ShenandoahOldHeuristics::should_start_gc() {\n+\n+  const ShenandoahHeap* heap = ShenandoahHeap::heap();\n+  if (_old_generation->is_doing_mixed_evacuations()) {\n+    \/\/ Do not try to start an old cycle if we are waiting for old regions to be evacuated (we need\n+    \/\/ a young cycle for this). Note that the young heuristic has a feature to expedite old evacuations.\n+    \/\/ Future refinement: under certain circumstances, we might be more sophisticated about this choice.\n+    \/\/ For example, we could choose to abandon the previous old collection before it has completed evacuations.\n+    log_debug(gc)(\"Not starting an old cycle because we are waiting for mixed evacuations\");\n+    return false;\n+  }\n+\n+  if (_cannot_expand_trigger) {\n+    const size_t old_gen_capacity = _old_generation->max_capacity();\n+    const size_t heap_capacity = heap->capacity();\n+    const double percent = percent_of(old_gen_capacity, heap_capacity);\n+    log_trigger(\"Expansion failure, current size: \" SIZE_FORMAT \"%s which is %.1f%% of total heap size\",\n+                 byte_size_in_proper_unit(old_gen_capacity), proper_unit_for_byte_size(old_gen_capacity), percent);\n+    return true;\n+  }\n+\n+  if (_fragmentation_trigger) {\n+    const size_t used = _old_generation->used();\n+    const size_t used_regions_size = _old_generation->used_regions_size();\n+\n+    \/\/ used_regions includes humongous regions\n+    const size_t used_regions = _old_generation->used_regions();\n+    assert(used_regions_size > used_regions, \"Cannot have more used than used regions\");\n+\n+    size_t first_old_region, last_old_region;\n+    double density;\n+    get_fragmentation_trigger_reason_for_log_message(density, first_old_region, last_old_region);\n+    const size_t span_of_old_regions = (last_old_region >= first_old_region)? last_old_region + 1 - first_old_region: 0;\n+    const size_t fragmented_free = used_regions_size - used;\n+\n+    log_trigger(\"Old has become fragmented: \"\n+                SIZE_FORMAT \"%s available bytes spread between range spanned from \"\n+                SIZE_FORMAT \" to \" SIZE_FORMAT \" (\" SIZE_FORMAT \"), density: %.1f%%\",\n+                byte_size_in_proper_unit(fragmented_free), proper_unit_for_byte_size(fragmented_free),\n+                first_old_region, last_old_region, span_of_old_regions, density * 100);\n+    return true;\n+  }\n+\n+  if (_growth_trigger) {\n+    \/\/ Growth may be falsely triggered during mixed evacuations, before the mixed-evacuation candidates have been\n+    \/\/ evacuated.  Before acting on a false trigger, we check to confirm the trigger condition is still satisfied.\n+    const size_t current_usage = _old_generation->used() + _old_generation->get_humongous_waste();\n+    const size_t trigger_threshold = _old_generation->usage_trigger_threshold();\n+    const size_t heap_size = heap->capacity();\n+    const size_t ignore_threshold = (ShenandoahIgnoreOldGrowthBelowPercentage * heap_size) \/ 100;\n+    size_t consecutive_young_cycles;\n+    if ((current_usage < ignore_threshold) &&\n+        ((consecutive_young_cycles = heap->shenandoah_policy()->consecutive_young_gc_count())\n+         < ShenandoahDoNotIgnoreGrowthAfterYoungCycles)) {\n+      log_debug(gc)(\"Ignoring Trigger: Old has overgrown: usage (\" SIZE_FORMAT \"%s) is below threshold (\"\n+                    SIZE_FORMAT \"%s) after \" SIZE_FORMAT \" consecutive completed young GCs\",\n+                    byte_size_in_proper_unit(current_usage), proper_unit_for_byte_size(current_usage),\n+                    byte_size_in_proper_unit(ignore_threshold), proper_unit_for_byte_size(ignore_threshold),\n+                    consecutive_young_cycles);\n+      _growth_trigger = false;\n+    } else if (current_usage > trigger_threshold) {\n+      const size_t live_at_previous_old = _old_generation->get_live_bytes_after_last_mark();\n+      const double percent_growth = percent_of(current_usage - live_at_previous_old, live_at_previous_old);\n+      log_trigger(\"Old has overgrown, live at end of previous OLD marking: \"\n+                  SIZE_FORMAT \"%s, current usage: \" SIZE_FORMAT \"%s, percent growth: %.1f%%\",\n+                  byte_size_in_proper_unit(live_at_previous_old), proper_unit_for_byte_size(live_at_previous_old),\n+                  byte_size_in_proper_unit(current_usage), proper_unit_for_byte_size(current_usage), percent_growth);\n+      return true;\n+    } else {\n+      \/\/ Mixed evacuations have decreased current_usage such that old-growth trigger is no longer relevant.\n+      _growth_trigger = false;\n+    }\n+  }\n+\n+  \/\/ Otherwise, defer to inherited heuristic for gc trigger.\n+  return this->ShenandoahHeuristics::should_start_gc();\n+}\n+\n+void ShenandoahOldHeuristics::record_success_concurrent() {\n+  \/\/ Forget any triggers that occurred while OLD GC was ongoing.  If we really need to start another, it will retrigger.\n+  clear_triggers();\n+  this->ShenandoahHeuristics::record_success_concurrent();\n+}\n+\n+void ShenandoahOldHeuristics::record_success_degenerated() {\n+  \/\/ Forget any triggers that occurred while OLD GC was ongoing.  If we really need to start another, it will retrigger.\n+  clear_triggers();\n+  this->ShenandoahHeuristics::record_success_degenerated();\n+}\n+\n+void ShenandoahOldHeuristics::record_success_full() {\n+  \/\/ Forget any triggers that occurred while OLD GC was ongoing.  If we really need to start another, it will retrigger.\n+  clear_triggers();\n+  this->ShenandoahHeuristics::record_success_full();\n+}\n+\n+const char* ShenandoahOldHeuristics::name() {\n+  return \"Old\";\n+}\n+\n+bool ShenandoahOldHeuristics::is_diagnostic() {\n+  return false;\n+}\n+\n+bool ShenandoahOldHeuristics::is_experimental() {\n+  return true;\n+}\n+\n+void ShenandoahOldHeuristics::choose_collection_set_from_regiondata(ShenandoahCollectionSet* set,\n+                                                                    ShenandoahHeuristics::RegionData* data,\n+                                                                    size_t data_size, size_t free) {\n+  ShouldNotReachHere();\n+}\n","filename":"src\/hotspot\/share\/gc\/shenandoah\/heuristics\/shenandoahOldHeuristics.cpp","additions":753,"deletions":0,"binary":false,"changes":753,"status":"added"},{"patch":"@@ -0,0 +1,208 @@\n+\/*\n+ * Copyright Amazon.com Inc. or its affiliates. All Rights Reserved.\n+ * DO NOT ALTER OR REMOVE COPYRIGHT NOTICES OR THIS FILE HEADER.\n+ *\n+ * This code is free software; you can redistribute it and\/or modify it\n+ * under the terms of the GNU General Public License version 2 only, as\n+ * published by the Free Software Foundation.\n+ *\n+ * This code is distributed in the hope that it will be useful, but WITHOUT\n+ * ANY WARRANTY; without even the implied warranty of MERCHANTABILITY or\n+ * FITNESS FOR A PARTICULAR PURPOSE.  See the GNU General Public License\n+ * version 2 for more details (a copy is included in the LICENSE file that\n+ * accompanied this code).\n+ *\n+ * You should have received a copy of the GNU General Public License version\n+ * 2 along with this work; if not, write to the Free Software Foundation,\n+ * Inc., 51 Franklin St, Fifth Floor, Boston, MA 02110-1301 USA.\n+ *\n+ * Please contact Oracle, 500 Oracle Parkway, Redwood Shores, CA 94065 USA\n+ * or visit www.oracle.com if you need additional information or have any\n+ * questions.\n+ *\n+ *\/\n+\n+#ifndef SHARE_GC_SHENANDOAH_HEURISTICS_SHENANDOAHOLDHEURISTICS_HPP\n+#define SHARE_GC_SHENANDOAH_HEURISTICS_SHENANDOAHOLDHEURISTICS_HPP\n+\n+\n+#include \"gc\/shenandoah\/heuristics\/shenandoahHeuristics.hpp\"\n+#include \"gc\/shenandoah\/shenandoahGenerationalHeap.hpp\"\n+\n+class ShenandoahCollectionSet;\n+class ShenandoahHeapRegion;\n+class ShenandoahOldGeneration;\n+\n+\/*\n+ * This heuristic is responsible for choosing a set of candidates for inclusion\n+ * in mixed collections. These candidates are chosen when marking of the old\n+ * generation is complete. Note that this list of candidates may live through\n+ * several mixed collections.\n+ *\n+ * This heuristic is also responsible for triggering old collections. It has its\n+ * own collection of triggers to decide whether to start an old collection. It does\n+ * _not_ use any of the functionality from the adaptive heuristics for triggers.\n+ * It also does not use any of the functionality from the heuristics base classes\n+ * to choose the collection set. For these reasons, it does not extend from\n+ * ShenandoahGenerationalHeuristics.\n+ *\/\n+class ShenandoahOldHeuristics : public ShenandoahHeuristics {\n+\n+private:\n+\n+  static uint NOT_FOUND;\n+\n+  ShenandoahGenerationalHeap* _heap;\n+\n+  \/\/ After final marking of the old generation, this heuristic will select\n+  \/\/ a set of candidate regions to be included in subsequent mixed collections.\n+  \/\/ The regions are sorted into a `_region_data` array (declared in base\n+  \/\/ class) in decreasing order of garbage. The heuristic will give priority\n+  \/\/ to regions containing more garbage.\n+\n+  \/\/ The following members are used to keep track of which candidate regions\n+  \/\/ have yet to be added to a mixed collection. There is also some special\n+  \/\/ handling for pinned regions, described further below.\n+\n+  \/\/ Pinned regions may not be included in the collection set. Any old regions\n+  \/\/ which were pinned at the time when old regions were added to the mixed\n+  \/\/ collection will have been skipped. These regions are still contain garbage,\n+  \/\/ so we want to include them at the start of the list of candidates for the\n+  \/\/ _next_ mixed collection cycle. This variable is the index of the _first_\n+  \/\/ old region which is pinned when the mixed collection set is formed.\n+  uint _first_pinned_candidate;\n+\n+  \/\/ This is the index of the last region which is above the garbage threshold.\n+  \/\/ No regions after this will be considered for inclusion in a mixed collection\n+  \/\/ set.\n+  uint _last_old_collection_candidate;\n+\n+  \/\/ This index points to the first candidate in line to be added to the mixed\n+  \/\/ collection set. It is updated as regions are added to the collection set.\n+  uint _next_old_collection_candidate;\n+\n+  \/\/ This is the last index in the array of old regions which were active at\n+  \/\/ the end of old final mark.\n+  uint _last_old_region;\n+\n+  \/\/ How much live data must be evacuated from within the unprocessed mixed evacuation candidates?\n+  size_t _live_bytes_in_unprocessed_candidates;\n+\n+  \/\/ Keep a pointer to our generation that we can use without down casting a protected member from the base class.\n+  ShenandoahOldGeneration* _old_generation;\n+\n+  \/\/ Flags are set when promotion failure is detected (by gc thread), and cleared when\n+  \/\/ old generation collection begins (by control thread).  Flags are set and cleared at safepoints.\n+  bool _cannot_expand_trigger;\n+  bool _fragmentation_trigger;\n+  bool _growth_trigger;\n+\n+  \/\/ Motivation for a fragmentation_trigger\n+  double _fragmentation_density;\n+  size_t _fragmentation_first_old_region;\n+  size_t _fragmentation_last_old_region;\n+\n+  \/\/ Compare by live is used to prioritize compaction of old-gen regions.  With old-gen compaction, the goal is\n+  \/\/ to tightly pack long-lived objects into available regions.  In most cases, there has not been an accumulation\n+  \/\/ of garbage within old-gen regions.  The more likely opportunity will be to combine multiple sparsely populated\n+  \/\/ old-gen regions which may have been promoted in place into a smaller number of densely packed old-gen regions.\n+  \/\/ This improves subsequent allocation efficiency and reduces the likelihood of allocation failure (including\n+  \/\/ humongous allocation failure) due to fragmentation of the available old-gen allocation pool\n+  static int compare_by_live(RegionData a, RegionData b);\n+\n+  static int compare_by_index(RegionData a, RegionData b);\n+\n+  \/\/ Set the fragmentation trigger if old-gen memory has become fragmented.\n+  void set_trigger_if_old_is_fragmented(size_t first_old_region, size_t last_old_region,\n+                                        size_t old_region_count, size_t num_regions);\n+\n+  \/\/ Set the overgrowth trigger if old-gen memory has grown beyond a particular threshold.\n+  void set_trigger_if_old_is_overgrown();\n+\n+ protected:\n+  void choose_collection_set_from_regiondata(ShenandoahCollectionSet* set, RegionData* data, size_t data_size, size_t free) override;\n+\n+public:\n+  explicit ShenandoahOldHeuristics(ShenandoahOldGeneration* generation, ShenandoahGenerationalHeap* gen_heap);\n+\n+  \/\/ Prepare for evacuation of old-gen regions by capturing the mark results of a recently completed concurrent mark pass.\n+  void prepare_for_old_collections();\n+\n+  \/\/ Return true iff the collection set is primed with at least one old-gen region.\n+  bool prime_collection_set(ShenandoahCollectionSet* set);\n+\n+  \/\/ How many old-collection candidates have not yet been processed?\n+  uint unprocessed_old_collection_candidates() const;\n+\n+  \/\/ How much live memory must be evacuated from within old-collection candidates that have not yet been processed?\n+  size_t unprocessed_old_collection_candidates_live_memory() const;\n+\n+  void set_unprocessed_old_collection_candidates_live_memory(size_t initial_live);\n+\n+  void decrease_unprocessed_old_collection_candidates_live_memory(size_t evacuated_live);\n+\n+  \/\/ How many old or hidden collection candidates have not yet been processed?\n+  uint last_old_collection_candidate_index() const;\n+\n+  \/\/ Return the next old-collection candidate in order of decreasing amounts of garbage.  (We process most-garbage regions\n+  \/\/ first.)  This does not consume the candidate.  If the candidate is selected for inclusion in a collection set, then\n+  \/\/ the candidate is consumed by invoking consume_old_collection_candidate().\n+  ShenandoahHeapRegion* next_old_collection_candidate();\n+\n+  \/\/ Adjust internal state to reflect that one fewer old-collection candidate remains to be processed.\n+  void consume_old_collection_candidate();\n+\n+  \/\/ Fill in buffer with all the old-collection regions that were identified at the end of the most recent old-gen\n+  \/\/ mark to require their unmarked objects to be coalesced and filled.  The buffer array must have at least\n+  \/\/ last_old_region_index() entries, or memory may be corrupted when this function overwrites the\n+  \/\/ end of the array.\n+  unsigned int get_coalesce_and_fill_candidates(ShenandoahHeapRegion** buffer);\n+\n+  \/\/ True if there are old regions that need to be filled.\n+  bool has_coalesce_and_fill_candidates() const { return coalesce_and_fill_candidates_count() > 0; }\n+\n+  \/\/ Return the number of old regions that need to be filled.\n+  size_t coalesce_and_fill_candidates_count() const { return _last_old_region - _next_old_collection_candidate; }\n+\n+  \/\/ If a GLOBAL gc occurs, it will collect the entire heap which invalidates any collection candidates being\n+  \/\/ held by this heuristic for supplying mixed collections.\n+  void abandon_collection_candidates();\n+\n+  void trigger_cannot_expand() { _cannot_expand_trigger = true; };\n+\n+  inline void get_fragmentation_trigger_reason_for_log_message(double &density, size_t &first_index, size_t &last_index) {\n+    density = _fragmentation_density;\n+    first_index = _fragmentation_first_old_region;\n+    last_index = _fragmentation_last_old_region;\n+  }\n+\n+  void clear_triggers();\n+\n+  \/\/ Check whether conditions merit the start of old GC.  Set appropriate trigger if so.\n+  void evaluate_triggers(size_t first_old_region, size_t last_old_region, size_t old_region_count, size_t num_regions);\n+\n+  void record_cycle_end() override;\n+\n+  bool should_start_gc() override;\n+\n+  \/\/ Returns true if the old generation needs to prepare for marking, or continue marking.\n+  bool should_resume_old_cycle();\n+\n+  void record_success_concurrent() override;\n+\n+  void record_success_degenerated() override;\n+\n+  void record_success_full() override;\n+\n+  const char* name() override;\n+\n+  bool is_diagnostic() override;\n+\n+  bool is_experimental() override;\n+\n+private:\n+  void slide_pinned_regions_to_front();\n+  bool all_candidates_are_pinned();\n+};\n+\n+#endif \/\/ SHARE_GC_SHENANDOAH_HEURISTICS_SHENANDOAHOLDHEURISTICS_HPP\n","filename":"src\/hotspot\/share\/gc\/shenandoah\/heuristics\/shenandoahOldHeuristics.hpp","additions":208,"deletions":0,"binary":false,"changes":208,"status":"added"},{"patch":"@@ -34,0 +34,3 @@\n+ShenandoahPassiveHeuristics::ShenandoahPassiveHeuristics(ShenandoahSpaceInfo* space_info) :\n+  ShenandoahHeuristics(space_info) {}\n+\n@@ -36,0 +39,1 @@\n+  decline_trigger();\n@@ -56,1 +60,1 @@\n-  size_t max_capacity = ShenandoahHeap::heap()->max_capacity();\n+  size_t max_capacity = _space_info->max_capacity();\n@@ -68,1 +72,1 @@\n-    ShenandoahHeapRegion* r = data[idx]._region;\n+    ShenandoahHeapRegion* r = data[idx].get_region();\n","filename":"src\/hotspot\/share\/gc\/shenandoah\/heuristics\/shenandoahPassiveHeuristics.cpp","additions":6,"deletions":2,"binary":false,"changes":8,"status":"modified"},{"patch":"@@ -30,0 +30,9 @@\n+\/*\n+ * The passive heuristic is for use only with the passive mode. In\n+ * the passive mode, Shenandoah only performs STW (i.e., degenerated)\n+ * collections. All the barriers are disabled and there are no concurrent\n+ * activities. Therefore, this heuristic _never_ triggers a cycle. It\n+ * will select regions for evacuation based on ShenandoahEvacReserve,\n+ * ShenandoahEvacWaste and ShenandoahGarbageThreshold. Note that it does\n+ * not attempt to evacuate regions with more garbage.\n+ *\/\n@@ -32,0 +41,2 @@\n+  ShenandoahPassiveHeuristics(ShenandoahSpaceInfo* space_info);\n+\n","filename":"src\/hotspot\/share\/gc\/shenandoah\/heuristics\/shenandoahPassiveHeuristics.hpp","additions":11,"deletions":0,"binary":false,"changes":11,"status":"modified"},{"patch":"@@ -0,0 +1,47 @@\n+\/*\n+ * Copyright Amazon.com Inc. or its affiliates. All Rights Reserved.\n+ * DO NOT ALTER OR REMOVE COPYRIGHT NOTICES OR THIS FILE HEADER.\n+ *\n+ * This code is free software; you can redistribute it and\/or modify it\n+ * under the terms of the GNU General Public License version 2 only, as\n+ * published by the Free Software Foundation.\n+ *\n+ * This code is distributed in the hope that it will be useful, but WITHOUT\n+ * ANY WARRANTY; without even the implied warranty of MERCHANTABILITY or\n+ * FITNESS FOR A PARTICULAR PURPOSE.  See the GNU General Public License\n+ * version 2 for more details (a copy is included in the LICENSE file that\n+ * accompanied this code).\n+ *\n+ * You should have received a copy of the GNU General Public License version\n+ * 2 along with this work; if not, write to the Free Software Foundation,\n+ * Inc., 51 Franklin St, Fifth Floor, Boston, MA 02110-1301 USA.\n+ *\n+ * Please contact Oracle, 500 Oracle Parkway, Redwood Shores, CA 94065 USA\n+ * or visit www.oracle.com if you need additional information or have any\n+ * questions.\n+ *\n+ *\/\n+\n+#ifndef SHARE_GC_SHENANDOAH_HEURISTICS_SHENANDOAHSPACEINFO_HPP\n+#define SHARE_GC_SHENANDOAH_HEURISTICS_SHENANDOAHSPACEINFO_HPP\n+\n+#include \"utilities\/globalDefinitions.hpp\"\n+\n+\/*\n+ * The purpose of this interface is to decouple the heuristics from a\n+ * direct dependency on the ShenandoahHeap singleton instance. This is\n+ * done to facilitate future unit testing of the heuristics and to support\n+ * future operational modes of Shenandoah in which the heap may be split\n+ * into generations.\n+ *\/\n+class ShenandoahSpaceInfo {\n+public:\n+  virtual const char* name() const = 0;\n+  virtual size_t max_capacity() const = 0;\n+  virtual size_t soft_available() const = 0;\n+  virtual size_t available() const = 0;\n+  virtual size_t used() const = 0;\n+  virtual size_t bytes_allocated_since_gc_start() const = 0;\n+};\n+\n+#endif \/\/SHARE_GC_SHENANDOAH_HEURISTICS_SHENANDOAHSPACEINFO_HPP\n","filename":"src\/hotspot\/share\/gc\/shenandoah\/heuristics\/shenandoahSpaceInfo.hpp","additions":47,"deletions":0,"binary":false,"changes":47,"status":"added"},{"patch":"@@ -35,1 +35,2 @@\n-ShenandoahStaticHeuristics::ShenandoahStaticHeuristics() : ShenandoahHeuristics() {\n+ShenandoahStaticHeuristics::ShenandoahStaticHeuristics(ShenandoahSpaceInfo* space_info) :\n+  ShenandoahHeuristics(space_info) {\n@@ -43,5 +44,3 @@\n-  ShenandoahHeap* heap = ShenandoahHeap::heap();\n-\n-  size_t max_capacity = heap->max_capacity();\n-  size_t capacity = heap->soft_max_capacity();\n-  size_t available = heap->free_set()->available();\n+  size_t max_capacity = _space_info->max_capacity();\n+  size_t capacity = ShenandoahHeap::heap()->soft_max_capacity();\n+  size_t available = _space_info->available();\n@@ -56,1 +55,1 @@\n-    log_info(gc)(\"Trigger: Free (\" SIZE_FORMAT \"%s) is below minimum threshold (\" SIZE_FORMAT \"%s)\",\n+    log_trigger(\"Free (\" SIZE_FORMAT \"%s) is below minimum threshold (\" SIZE_FORMAT \"%s)\",\n@@ -59,0 +58,1 @@\n+    accept_trigger();\n@@ -70,1 +70,1 @@\n-    ShenandoahHeapRegion* r = data[idx]._region;\n+    ShenandoahHeapRegion* r = data[idx].get_region();\n","filename":"src\/hotspot\/share\/gc\/shenandoah\/heuristics\/shenandoahStaticHeuristics.cpp","additions":8,"deletions":8,"binary":false,"changes":16,"status":"modified"},{"patch":"@@ -30,0 +30,5 @@\n+\/*\n+ * The static heuristic will trigger cycles if the available memory falls\n+ * below ShenandoahMinFreeThreshold percentage of total capacity. This\n+ * heuristic will attempt to evacuation any region with any garbage.\n+ *\/\n@@ -32,1 +37,1 @@\n-  ShenandoahStaticHeuristics();\n+  ShenandoahStaticHeuristics(ShenandoahSpaceInfo* space_info);\n","filename":"src\/hotspot\/share\/gc\/shenandoah\/heuristics\/shenandoahStaticHeuristics.hpp","additions":6,"deletions":1,"binary":false,"changes":7,"status":"modified"},{"patch":"@@ -0,0 +1,229 @@\n+\/*\n+ * Copyright Amazon.com Inc. or its affiliates. All Rights Reserved.\n+ * DO NOT ALTER OR REMOVE COPYRIGHT NOTICES OR THIS FILE HEADER.\n+ *\n+ * This code is free software; you can redistribute it and\/or modify it\n+ * under the terms of the GNU General Public License version 2 only, as\n+ * published by the Free Software Foundation.\n+ *\n+ * This code is distributed in the hope that it will be useful, but WITHOUT\n+ * ANY WARRANTY; without even the implied warranty of MERCHANTABILITY or\n+ * FITNESS FOR A PARTICULAR PURPOSE.  See the GNU General Public License\n+ * version 2 for more details (a copy is included in the LICENSE file that\n+ * accompanied this code).\n+ *\n+ * You should have received a copy of the GNU General Public License version\n+ * 2 along with this work; if not, write to the Free Software Foundation,\n+ * Inc., 51 Franklin St, Fifth Floor, Boston, MA 02110-1301 USA.\n+ *\n+ * Please contact Oracle, 500 Oracle Parkway, Redwood Shores, CA 94065 USA\n+ * or visit www.oracle.com if you need additional information or have any\n+ * questions.\n+ *\n+ *\/\n+\n+#include \"precompiled.hpp\"\n+\n+#include \"gc\/shenandoah\/heuristics\/shenandoahOldHeuristics.hpp\"\n+#include \"gc\/shenandoah\/heuristics\/shenandoahYoungHeuristics.hpp\"\n+#include \"gc\/shenandoah\/shenandoahCollectorPolicy.hpp\"\n+#include \"gc\/shenandoah\/shenandoahGenerationalHeap.inline.hpp\"\n+#include \"gc\/shenandoah\/shenandoahHeapRegion.inline.hpp\"\n+#include \"gc\/shenandoah\/shenandoahOldGeneration.hpp\"\n+#include \"gc\/shenandoah\/shenandoahYoungGeneration.hpp\"\n+\n+#include \"utilities\/quickSort.hpp\"\n+\n+ShenandoahYoungHeuristics::ShenandoahYoungHeuristics(ShenandoahYoungGeneration* generation)\n+        : ShenandoahGenerationalHeuristics(generation) {\n+}\n+\n+\n+void ShenandoahYoungHeuristics::choose_collection_set_from_regiondata(ShenandoahCollectionSet* cset,\n+                                                                      RegionData* data, size_t size,\n+                                                                      size_t actual_free) {\n+  \/\/ See comments in ShenandoahAdaptiveHeuristics::choose_collection_set_from_regiondata():\n+  \/\/ we do the same here, but with the following adjustments for generational mode:\n+  \/\/\n+  \/\/ In generational mode, the sort order within the data array is not strictly descending amounts\n+  \/\/ of garbage. In particular, regions that have reached tenure age will be sorted into this\n+  \/\/ array before younger regions that typically contain more garbage. This is one reason why,\n+  \/\/ for example, we continue examining regions even after rejecting a region that has\n+  \/\/ more live data than we can evacuate.\n+\n+  \/\/ Better select garbage-first regions\n+  QuickSort::sort<RegionData>(data, (int) size, compare_by_garbage, false);\n+\n+  size_t cur_young_garbage = add_preselected_regions_to_collection_set(cset, data, size);\n+\n+  choose_young_collection_set(cset, data, size, actual_free, cur_young_garbage);\n+\n+  log_cset_composition(cset);\n+}\n+\n+void ShenandoahYoungHeuristics::choose_young_collection_set(ShenandoahCollectionSet* cset,\n+                                                            const RegionData* data,\n+                                                            size_t size, size_t actual_free,\n+                                                            size_t cur_young_garbage) const {\n+\n+  const auto heap = ShenandoahGenerationalHeap::heap();\n+\n+  const size_t capacity = heap->soft_max_capacity();\n+  const size_t garbage_threshold = ShenandoahHeapRegion::region_size_bytes() * ShenandoahGarbageThreshold \/ 100;\n+  const size_t ignore_threshold = ShenandoahHeapRegion::region_size_bytes() * ShenandoahIgnoreGarbageThreshold \/ 100;\n+\n+  \/\/ This is young-gen collection or a mixed evacuation.\n+  \/\/ If this is mixed evacuation, the old-gen candidate regions have already been added.\n+  size_t cur_cset = 0;\n+  const size_t max_cset = (size_t) (heap->young_generation()->get_evacuation_reserve() \/ ShenandoahEvacWaste);\n+  const size_t free_target = (capacity * ShenandoahMinFreeThreshold) \/ 100 + max_cset;\n+  const size_t min_garbage = (free_target > actual_free) ? (free_target - actual_free) : 0;\n+\n+\n+  log_info(gc, ergo)(\n+          \"Adaptive CSet Selection for YOUNG. Max Evacuation: \" SIZE_FORMAT \"%s, Actual Free: \" SIZE_FORMAT \"%s.\",\n+          byte_size_in_proper_unit(max_cset), proper_unit_for_byte_size(max_cset),\n+          byte_size_in_proper_unit(actual_free), proper_unit_for_byte_size(actual_free));\n+\n+  for (size_t idx = 0; idx < size; idx++) {\n+    ShenandoahHeapRegion* r = data[idx].get_region();\n+    if (cset->is_preselected(r->index())) {\n+      continue;\n+    }\n+\n+    \/\/ Note that we do not add tenurable regions if they were not pre-selected.  They were not preselected\n+    \/\/ because there is insufficient room in old-gen to hold their to-be-promoted live objects or because\n+    \/\/ they are to be promoted in place.\n+    if (!heap->is_tenurable(r)) {\n+      const size_t new_cset = cur_cset + r->get_live_data_bytes();\n+      const size_t region_garbage = r->garbage();\n+      const size_t new_garbage = cur_young_garbage + region_garbage;\n+      const bool add_regardless = (region_garbage > ignore_threshold) && (new_garbage < min_garbage);\n+      assert(r->is_young(), \"Only young candidates expected in the data array\");\n+      if ((new_cset <= max_cset) && (add_regardless || (region_garbage > garbage_threshold))) {\n+        cur_cset = new_cset;\n+        cur_young_garbage = new_garbage;\n+        cset->add_region(r);\n+      }\n+    }\n+  }\n+}\n+\n+\n+bool ShenandoahYoungHeuristics::should_start_gc() {\n+  auto heap = ShenandoahGenerationalHeap::heap();\n+  ShenandoahOldGeneration* old_generation = heap->old_generation();\n+  ShenandoahOldHeuristics* old_heuristics = old_generation->heuristics();\n+\n+  \/\/ Checks that an old cycle has run for at least ShenandoahMinimumOldTimeMs before allowing a young cycle.\n+  if (ShenandoahMinimumOldTimeMs > 0) {\n+    if (old_generation->is_preparing_for_mark() || old_generation->is_concurrent_mark_in_progress()) {\n+      size_t old_time_elapsed = size_t(old_heuristics->elapsed_cycle_time() * 1000);\n+      if (old_time_elapsed < ShenandoahMinimumOldTimeMs) {\n+        \/\/ Do not decline_trigger() when waiting for minimum quantum of Old-gen marking.  It is not at our discretion\n+        \/\/ to trigger at this time.\n+        return false;\n+      }\n+    }\n+  }\n+\n+  \/\/ inherited triggers have already decided to start a cycle, so no further evaluation is required\n+  if (ShenandoahAdaptiveHeuristics::should_start_gc()) {\n+    return true;\n+  }\n+\n+  \/\/ Get through promotions and mixed evacuations as quickly as possible.  These cycles sometimes require significantly\n+  \/\/ more time than traditional young-generation cycles so start them up as soon as possible.  This is a \"mitigation\"\n+  \/\/ for the reality that old-gen and young-gen activities are not truly \"concurrent\".  If there is old-gen work to\n+  \/\/ be done, we start up the young-gen GC threads so they can do some of this old-gen work.  As implemented, promotion\n+  \/\/ gets priority over old-gen marking.\n+  size_t promo_expedite_threshold = percent_of(heap->young_generation()->max_capacity(), ShenandoahExpeditePromotionsThreshold);\n+  size_t promo_potential = old_generation->get_promotion_potential();\n+  if (promo_potential > promo_expedite_threshold) {\n+    \/\/ Detect unsigned arithmetic underflow\n+    assert(promo_potential < heap->capacity(), \"Sanity\");\n+    log_trigger(\"Expedite promotion of \" PROPERFMT, PROPERFMTARGS(promo_potential));\n+    accept_trigger();\n+    return true;\n+  }\n+\n+  size_t mixed_candidates = old_heuristics->unprocessed_old_collection_candidates();\n+  if (mixed_candidates > ShenandoahExpediteMixedThreshold && !heap->is_concurrent_weak_root_in_progress()) {\n+    \/\/ We need to run young GC in order to open up some free heap regions so we can finish mixed evacuations.\n+    \/\/ If concurrent weak root processing is in progress, it means the old cycle has chosen mixed collection\n+    \/\/ candidates, but has not completed. There is no point in trying to start the young cycle before the old\n+    \/\/ cycle completes.\n+    log_trigger(\"Expedite mixed evacuation of \" SIZE_FORMAT \" regions\", mixed_candidates);\n+    accept_trigger();\n+    return true;\n+  }\n+\n+  \/\/ Don't decline_trigger() here  That was done in ShenandoahAdaptiveHeuristics::should_start_gc()\n+  return false;\n+}\n+\n+\/\/ Return a conservative estimate of how much memory can be allocated before we need to start GC. The estimate is based\n+\/\/ on memory that is currently available within young generation plus all of the memory that will be added to the young\n+\/\/ generation at the end of the current cycle (as represented by young_regions_to_be_reclaimed) and on the anticipated\n+\/\/ amount of time required to perform a GC.\n+size_t ShenandoahYoungHeuristics::bytes_of_allocation_runway_before_gc_trigger(size_t young_regions_to_be_reclaimed) {\n+  size_t capacity = _space_info->max_capacity();\n+  size_t usage = _space_info->used();\n+  size_t available = (capacity > usage)? capacity - usage: 0;\n+  size_t allocated = _space_info->bytes_allocated_since_gc_start();\n+\n+  size_t available_young_collected = ShenandoahHeap::heap()->collection_set()->get_young_available_bytes_collected();\n+  size_t anticipated_available =\n+          available + young_regions_to_be_reclaimed * ShenandoahHeapRegion::region_size_bytes() - available_young_collected;\n+  size_t spike_headroom = capacity * ShenandoahAllocSpikeFactor \/ 100;\n+  size_t penalties      = capacity * _gc_time_penalties \/ 100;\n+\n+  double rate = _allocation_rate.sample(allocated);\n+\n+  \/\/ At what value of available, would avg and spike triggers occur?\n+  \/\/  if allocation_headroom < avg_cycle_time * avg_alloc_rate, then we experience avg trigger\n+  \/\/  if allocation_headroom < avg_cycle_time * rate, then we experience spike trigger if is_spiking\n+  \/\/\n+  \/\/ allocation_headroom =\n+  \/\/     0, if penalties > available or if penalties + spike_headroom > available\n+  \/\/     available - penalties - spike_headroom, otherwise\n+  \/\/\n+  \/\/ so we trigger if available - penalties - spike_headroom < avg_cycle_time * avg_alloc_rate, which is to say\n+  \/\/                  available < avg_cycle_time * avg_alloc_rate + penalties + spike_headroom\n+  \/\/            or if available < penalties + spike_headroom\n+  \/\/\n+  \/\/ since avg_cycle_time * avg_alloc_rate > 0, the first test is sufficient to test both conditions\n+  \/\/\n+  \/\/ thus, evac_slack_avg is MIN2(0,  available - avg_cycle_time * avg_alloc_rate + penalties + spike_headroom)\n+  \/\/\n+  \/\/ similarly, evac_slack_spiking is MIN2(0, available - avg_cycle_time * rate + penalties + spike_headroom)\n+  \/\/ but evac_slack_spiking is only relevant if is_spiking, as defined below.\n+\n+  double avg_cycle_time = _gc_cycle_time_history->davg() + (_margin_of_error_sd * _gc_cycle_time_history->dsd());\n+  double avg_alloc_rate = _allocation_rate.upper_bound(_margin_of_error_sd);\n+  size_t evac_slack_avg;\n+  if (anticipated_available > avg_cycle_time * avg_alloc_rate + penalties + spike_headroom) {\n+    evac_slack_avg = anticipated_available - (avg_cycle_time * avg_alloc_rate + penalties + spike_headroom);\n+  } else {\n+    \/\/ we have no slack because it's already time to trigger\n+    evac_slack_avg = 0;\n+  }\n+\n+  bool is_spiking = _allocation_rate.is_spiking(rate, _spike_threshold_sd);\n+  size_t evac_slack_spiking;\n+  if (is_spiking) {\n+    if (anticipated_available > avg_cycle_time * rate + penalties + spike_headroom) {\n+      evac_slack_spiking = anticipated_available - (avg_cycle_time * rate + penalties + spike_headroom);\n+    } else {\n+      \/\/ we have no slack because it's already time to trigger\n+      evac_slack_spiking = 0;\n+    }\n+  } else {\n+    evac_slack_spiking = evac_slack_avg;\n+  }\n+\n+  size_t threshold = min_free_threshold();\n+  size_t evac_min_threshold = (anticipated_available > threshold)? anticipated_available - threshold: 0;\n+  return MIN3(evac_slack_spiking, evac_slack_avg, evac_min_threshold);\n+}\n+\n","filename":"src\/hotspot\/share\/gc\/shenandoah\/heuristics\/shenandoahYoungHeuristics.cpp","additions":229,"deletions":0,"binary":false,"changes":229,"status":"added"},{"patch":"@@ -0,0 +1,57 @@\n+\/*\n+ * Copyright Amazon.com Inc. or its affiliates. All Rights Reserved.\n+ * DO NOT ALTER OR REMOVE COPYRIGHT NOTICES OR THIS FILE HEADER.\n+ *\n+ * This code is free software; you can redistribute it and\/or modify it\n+ * under the terms of the GNU General Public License version 2 only, as\n+ * published by the Free Software Foundation.\n+ *\n+ * This code is distributed in the hope that it will be useful, but WITHOUT\n+ * ANY WARRANTY; without even the implied warranty of MERCHANTABILITY or\n+ * FITNESS FOR A PARTICULAR PURPOSE.  See the GNU General Public License\n+ * version 2 for more details (a copy is included in the LICENSE file that\n+ * accompanied this code).\n+ *\n+ * You should have received a copy of the GNU General Public License version\n+ * 2 along with this work; if not, write to the Free Software Foundation,\n+ * Inc., 51 Franklin St, Fifth Floor, Boston, MA 02110-1301 USA.\n+ *\n+ * Please contact Oracle, 500 Oracle Parkway, Redwood Shores, CA 94065 USA\n+ * or visit www.oracle.com if you need additional information or have any\n+ * questions.\n+ *\n+ *\/\n+#ifndef SHARE_GC_SHENANDOAH_HEURISTICS_SHENANDOAHYOUNGHEURISTICS_HPP\n+#define SHARE_GC_SHENANDOAH_HEURISTICS_SHENANDOAHYOUNGHEURISTICS_HPP\n+\n+#include \"gc\/shenandoah\/heuristics\/shenandoahGenerationalHeuristics.hpp\"\n+\n+class ShenandoahYoungGeneration;\n+\n+\/*\n+ * This is a specialization of the generational heuristic which chooses\n+ * young regions for evacuation. This heuristic also has additional triggers\n+ * designed to expedite mixed collections and promotions.\n+ *\/\n+class ShenandoahYoungHeuristics : public ShenandoahGenerationalHeuristics {\n+public:\n+  explicit ShenandoahYoungHeuristics(ShenandoahYoungGeneration* generation);\n+\n+\n+  void choose_collection_set_from_regiondata(ShenandoahCollectionSet* cset,\n+                                             RegionData* data, size_t size,\n+                                             size_t actual_free) override;\n+\n+  bool should_start_gc() override;\n+\n+  size_t bytes_of_allocation_runway_before_gc_trigger(size_t young_regions_to_be_reclaimed);\n+\n+private:\n+  void choose_young_collection_set(ShenandoahCollectionSet* cset,\n+                                   const RegionData* data,\n+                                   size_t size, size_t actual_free,\n+                                   size_t cur_young_garbage) const;\n+\n+};\n+\n+#endif \/\/ SHARE_GC_SHENANDOAH_HEURISTICS_SHENANDOAHYOUNGHEURISTICS_HPP\n","filename":"src\/hotspot\/share\/gc\/shenandoah\/heuristics\/shenandoahYoungHeuristics.hpp","additions":57,"deletions":0,"binary":false,"changes":57,"status":"added"},{"patch":"@@ -0,0 +1,64 @@\n+\/*\n+ * Copyright Amazon.com Inc. or its affiliates. All Rights Reserved.\n+ * DO NOT ALTER OR REMOVE COPYRIGHT NOTICES OR THIS FILE HEADER.\n+ *\n+ * This code is free software; you can redistribute it and\/or modify it\n+ * under the terms of the GNU General Public License version 2 only, as\n+ * published by the Free Software Foundation.\n+ *\n+ * This code is distributed in the hope that it will be useful, but WITHOUT\n+ * ANY WARRANTY; without even the implied warranty of MERCHANTABILITY or\n+ * FITNESS FOR A PARTICULAR PURPOSE.  See the GNU General Public License\n+ * version 2 for more details (a copy is included in the LICENSE file that\n+ * accompanied this code).\n+ *\n+ * You should have received a copy of the GNU General Public License version\n+ * 2 along with this work; if not, write to the Free Software Foundation,\n+ * Inc., 51 Franklin St, Fifth Floor, Boston, MA 02110-1301 USA.\n+ *\n+ * Please contact Oracle, 500 Oracle Parkway, Redwood Shores, CA 94065 USA\n+ * or visit www.oracle.com if you need additional information or have any\n+ * questions.\n+ *\n+ *\/\n+\n+#include \"precompiled.hpp\"\n+#include \"gc\/shenandoah\/heuristics\/shenandoahYoungHeuristics.hpp\"\n+#include \"gc\/shenandoah\/heuristics\/shenandoahHeuristics.hpp\"\n+#include \"gc\/shenandoah\/mode\/shenandoahGenerationalMode.hpp\"\n+#include \"logging\/log.hpp\"\n+#include \"logging\/logTag.hpp\"\n+#include \"runtime\/globals_extension.hpp\"\n+\n+void ShenandoahGenerationalMode::initialize_flags() const {\n+\n+#if !(defined AARCH64 || defined AMD64 || defined IA32 || defined PPC64)\n+  vm_exit_during_initialization(\"Shenandoah Generational GC is not supported on this platform.\");\n+#endif\n+\n+  \/\/ Exit if the user has asked ShenandoahCardBarrier to be disabled\n+  if (!FLAG_IS_DEFAULT(ShenandoahCardBarrier)) {\n+    SHENANDOAH_CHECK_FLAG_SET(ShenandoahCardBarrier);\n+  }\n+\n+  \/\/ Enable card-marking post-write barrier for tracking old to young pointers\n+  FLAG_SET_DEFAULT(ShenandoahCardBarrier, true);\n+\n+  if (ClassUnloading) {\n+    FLAG_SET_DEFAULT(VerifyBeforeExit, false);\n+  }\n+\n+  SHENANDOAH_ERGO_OVERRIDE_DEFAULT(GCTimeRatio, 70);\n+  SHENANDOAH_ERGO_ENABLE_FLAG(ExplicitGCInvokesConcurrent);\n+  SHENANDOAH_ERGO_ENABLE_FLAG(ShenandoahImplicitGCInvokesConcurrent);\n+\n+  \/\/ This helps most multi-core hardware hosts, enable by default\n+  SHENANDOAH_ERGO_ENABLE_FLAG(UseCondCardMark);\n+\n+  \/\/ Final configuration checks\n+  SHENANDOAH_CHECK_FLAG_SET(ShenandoahLoadRefBarrier);\n+  SHENANDOAH_CHECK_FLAG_SET(ShenandoahSATBBarrier);\n+  SHENANDOAH_CHECK_FLAG_SET(ShenandoahCASBarrier);\n+  SHENANDOAH_CHECK_FLAG_SET(ShenandoahCloneBarrier);\n+  SHENANDOAH_CHECK_FLAG_SET(ShenandoahCardBarrier);\n+}\n","filename":"src\/hotspot\/share\/gc\/shenandoah\/mode\/shenandoahGenerationalMode.cpp","additions":64,"deletions":0,"binary":false,"changes":64,"status":"added"},{"patch":"@@ -0,0 +1,39 @@\n+\/*\n+ * Copyright Amazon.com Inc. or its affiliates. All Rights Reserved.\n+ * DO NOT ALTER OR REMOVE COPYRIGHT NOTICES OR THIS FILE HEADER.\n+ *\n+ * This code is free software; you can redistribute it and\/or modify it\n+ * under the terms of the GNU General Public License version 2 only, as\n+ * published by the Free Software Foundation.\n+ *\n+ * This code is distributed in the hope that it will be useful, but WITHOUT\n+ * ANY WARRANTY; without even the implied warranty of MERCHANTABILITY or\n+ * FITNESS FOR A PARTICULAR PURPOSE.  See the GNU General Public License\n+ * version 2 for more details (a copy is included in the LICENSE file that\n+ * accompanied this code).\n+ *\n+ * You should have received a copy of the GNU General Public License version\n+ * 2 along with this work; if not, write to the Free Software Foundation,\n+ * Inc., 51 Franklin St, Fifth Floor, Boston, MA 02110-1301 USA.\n+ *\n+ * Please contact Oracle, 500 Oracle Parkway, Redwood Shores, CA 94065 USA\n+ * or visit www.oracle.com if you need additional information or have any\n+ * questions.\n+ *\n+ *\/\n+\n+#ifndef SHARE_GC_SHENANDOAH_MODE_SHENANDOAHGENERATIONALMODE_HPP\n+#define SHARE_GC_SHENANDOAH_MODE_SHENANDOAHGENERATIONALMODE_HPP\n+\n+#include \"gc\/shenandoah\/mode\/shenandoahMode.hpp\"\n+\n+class ShenandoahGenerationalMode : public ShenandoahMode {\n+public:\n+  virtual void initialize_flags() const;\n+  virtual const char* name()     { return \"Generational\"; }\n+  virtual bool is_diagnostic()   { return false; }\n+  virtual bool is_experimental() { return true; }\n+  virtual bool is_generational() { return true; }\n+};\n+\n+#endif \/\/ SHARE_GC_SHENANDOAH_MODE_SHENANDOAHGENERATIONALMODE_HPP\n","filename":"src\/hotspot\/share\/gc\/shenandoah\/mode\/shenandoahGenerationalMode.hpp","additions":39,"deletions":0,"binary":false,"changes":39,"status":"added"},{"patch":"@@ -1,80 +0,0 @@\n-\/*\n- * Copyright (c) 2020, 2022, Red Hat, Inc. All rights reserved.\n- * DO NOT ALTER OR REMOVE COPYRIGHT NOTICES OR THIS FILE HEADER.\n- *\n- * This code is free software; you can redistribute it and\/or modify it\n- * under the terms of the GNU General Public License version 2 only, as\n- * published by the Free Software Foundation.\n- *\n- * This code is distributed in the hope that it will be useful, but WITHOUT\n- * ANY WARRANTY; without even the implied warranty of MERCHANTABILITY or\n- * FITNESS FOR A PARTICULAR PURPOSE.  See the GNU General Public License\n- * version 2 for more details (a copy is included in the LICENSE file that\n- * accompanied this code).\n- *\n- * You should have received a copy of the GNU General Public License version\n- * 2 along with this work; if not, write to the Free Software Foundation,\n- * Inc., 51 Franklin St, Fifth Floor, Boston, MA 02110-1301 USA.\n- *\n- * Please contact Oracle, 500 Oracle Parkway, Redwood Shores, CA 94065 USA\n- * or visit www.oracle.com if you need additional information or have any\n- * questions.\n- *\n- *\/\n-\n-#include \"precompiled.hpp\"\n-#include \"gc\/shenandoah\/heuristics\/shenandoahAdaptiveHeuristics.hpp\"\n-#include \"gc\/shenandoah\/heuristics\/shenandoahAggressiveHeuristics.hpp\"\n-#include \"gc\/shenandoah\/heuristics\/shenandoahCompactHeuristics.hpp\"\n-#include \"gc\/shenandoah\/heuristics\/shenandoahStaticHeuristics.hpp\"\n-#include \"gc\/shenandoah\/mode\/shenandoahIUMode.hpp\"\n-#include \"logging\/log.hpp\"\n-#include \"logging\/logTag.hpp\"\n-#include \"runtime\/globals_extension.hpp\"\n-#include \"runtime\/java.hpp\"\n-\n-void ShenandoahIUMode::initialize_flags() const {\n-  if (FLAG_IS_CMDLINE(ClassUnloadingWithConcurrentMark) && ClassUnloading) {\n-    log_warning(gc)(\"Shenandoah I-U mode sets -XX:-ClassUnloadingWithConcurrentMark; see JDK-8261341 for details\");\n-  }\n-  FLAG_SET_DEFAULT(ClassUnloadingWithConcurrentMark, false);\n-\n-  if (ClassUnloading) {\n-    FLAG_SET_DEFAULT(VerifyBeforeExit, false);\n-  }\n-\n-  if (FLAG_IS_DEFAULT(ShenandoahIUBarrier)) {\n-    FLAG_SET_DEFAULT(ShenandoahIUBarrier, true);\n-  }\n-  if (FLAG_IS_DEFAULT(ShenandoahSATBBarrier)) {\n-    FLAG_SET_DEFAULT(ShenandoahSATBBarrier, false);\n-  }\n-\n-  SHENANDOAH_ERGO_ENABLE_FLAG(ExplicitGCInvokesConcurrent);\n-  SHENANDOAH_ERGO_ENABLE_FLAG(ShenandoahImplicitGCInvokesConcurrent);\n-\n-  \/\/ Final configuration checks\n-  SHENANDOAH_CHECK_FLAG_SET(ShenandoahLoadRefBarrier);\n-  SHENANDOAH_CHECK_FLAG_UNSET(ShenandoahSATBBarrier);\n-  SHENANDOAH_CHECK_FLAG_SET(ShenandoahIUBarrier);\n-  SHENANDOAH_CHECK_FLAG_SET(ShenandoahCASBarrier);\n-  SHENANDOAH_CHECK_FLAG_SET(ShenandoahCloneBarrier);\n-  SHENANDOAH_CHECK_FLAG_SET(ShenandoahStackWatermarkBarrier);\n-}\n-\n-ShenandoahHeuristics* ShenandoahIUMode::initialize_heuristics() const {\n-  if (ShenandoahGCHeuristics == nullptr) {\n-    vm_exit_during_initialization(\"Unknown -XX:ShenandoahGCHeuristics option (null)\");\n-  }\n-  if (strcmp(ShenandoahGCHeuristics, \"aggressive\") == 0) {\n-    return new ShenandoahAggressiveHeuristics();\n-  } else if (strcmp(ShenandoahGCHeuristics, \"static\") == 0) {\n-    return new ShenandoahStaticHeuristics();\n-  } else if (strcmp(ShenandoahGCHeuristics, \"adaptive\") == 0) {\n-    return new ShenandoahAdaptiveHeuristics();\n-  } else if (strcmp(ShenandoahGCHeuristics, \"compact\") == 0) {\n-    return new ShenandoahCompactHeuristics();\n-  }\n-  vm_exit_during_initialization(\"Unknown -XX:ShenandoahGCHeuristics option\");\n-  return nullptr;\n-}\n","filename":"src\/hotspot\/share\/gc\/shenandoah\/mode\/shenandoahIUMode.cpp","additions":0,"deletions":80,"binary":false,"changes":80,"status":"deleted"},{"patch":"@@ -1,42 +0,0 @@\n-\/*\n- * Copyright (c) 2020, Red Hat, Inc. All rights reserved.\n- * DO NOT ALTER OR REMOVE COPYRIGHT NOTICES OR THIS FILE HEADER.\n- *\n- * This code is free software; you can redistribute it and\/or modify it\n- * under the terms of the GNU General Public License version 2 only, as\n- * published by the Free Software Foundation.\n- *\n- * This code is distributed in the hope that it will be useful, but WITHOUT\n- * ANY WARRANTY; without even the implied warranty of MERCHANTABILITY or\n- * FITNESS FOR A PARTICULAR PURPOSE.  See the GNU General Public License\n- * version 2 for more details (a copy is included in the LICENSE file that\n- * accompanied this code).\n- *\n- * You should have received a copy of the GNU General Public License version\n- * 2 along with this work; if not, write to the Free Software Foundation,\n- * Inc., 51 Franklin St, Fifth Floor, Boston, MA 02110-1301 USA.\n- *\n- * Please contact Oracle, 500 Oracle Parkway, Redwood Shores, CA 94065 USA\n- * or visit www.oracle.com if you need additional information or have any\n- * questions.\n- *\n- *\/\n-\n-#ifndef SHARE_GC_SHENANDOAH_MODE_SHENANDOAHIUMODE_HPP\n-#define SHARE_GC_SHENANDOAH_MODE_SHENANDOAHIUMODE_HPP\n-\n-#include \"gc\/shenandoah\/mode\/shenandoahMode.hpp\"\n-\n-class ShenandoahHeuristics;\n-\n-class ShenandoahIUMode : public ShenandoahMode {\n-public:\n-  virtual void initialize_flags() const;\n-  virtual ShenandoahHeuristics* initialize_heuristics() const;\n-\n-  virtual const char* name()     { return \"Incremental-Update (IU)\"; }\n-  virtual bool is_diagnostic()   { return false; }\n-  virtual bool is_experimental() { return true; }\n-};\n-\n-#endif \/\/ SHARE_GC_SHENANDOAH_MODE_SHENANDOAHIUMODE_HPP\n","filename":"src\/hotspot\/share\/gc\/shenandoah\/mode\/shenandoahIUMode.hpp","additions":0,"deletions":42,"binary":false,"changes":42,"status":"deleted"},{"patch":"@@ -0,0 +1,54 @@\n+\/*\n+ * Copyright (c) 2020, Red Hat, Inc. All rights reserved.\n+ * Copyright Amazon.com Inc. or its affiliates. All Rights Reserved.\n+ * DO NOT ALTER OR REMOVE COPYRIGHT NOTICES OR THIS FILE HEADER.\n+ *\n+ * This code is free software; you can redistribute it and\/or modify it\n+ * under the terms of the GNU General Public License version 2 only, as\n+ * published by the Free Software Foundation.\n+ *\n+ * This code is distributed in the hope that it will be useful, but WITHOUT\n+ * ANY WARRANTY; without even the implied warranty of MERCHANTABILITY or\n+ * FITNESS FOR A PARTICULAR PURPOSE.  See the GNU General Public License\n+ * version 2 for more details (a copy is included in the LICENSE file that\n+ * accompanied this code).\n+ *\n+ * You should have received a copy of the GNU General Public License version\n+ * 2 along with this work; if not, write to the Free Software Foundation,\n+ * Inc., 51 Franklin St, Fifth Floor, Boston, MA 02110-1301 USA.\n+ *\n+ * Please contact Oracle, 500 Oracle Parkway, Redwood Shores, CA 94065 USA\n+ * or visit www.oracle.com if you need additional information or have any\n+ * questions.\n+ *\n+ *\/\n+\n+#include \"precompiled.hpp\"\n+#include \"gc\/shenandoah\/heuristics\/shenandoahAdaptiveHeuristics.hpp\"\n+#include \"gc\/shenandoah\/heuristics\/shenandoahAggressiveHeuristics.hpp\"\n+#include \"gc\/shenandoah\/heuristics\/shenandoahCompactHeuristics.hpp\"\n+#include \"gc\/shenandoah\/heuristics\/shenandoahSpaceInfo.hpp\"\n+#include \"gc\/shenandoah\/heuristics\/shenandoahStaticHeuristics.hpp\"\n+#include \"gc\/shenandoah\/mode\/shenandoahMode.hpp\"\n+\n+ShenandoahHeuristics* ShenandoahMode::initialize_heuristics(ShenandoahSpaceInfo* space_info) const {\n+  if (ShenandoahGCHeuristics == nullptr) {\n+    vm_exit_during_initialization(\"Unknown -XX:ShenandoahGCHeuristics option (null)\");\n+  }\n+\n+  if (strcmp(ShenandoahGCHeuristics, \"aggressive\") == 0) {\n+    return new ShenandoahAggressiveHeuristics(space_info);\n+  } else if (strcmp(ShenandoahGCHeuristics, \"static\") == 0) {\n+    return new ShenandoahStaticHeuristics(space_info);\n+  } else if (strcmp(ShenandoahGCHeuristics, \"adaptive\") == 0) {\n+    return new ShenandoahAdaptiveHeuristics(space_info);\n+  } else if (strcmp(ShenandoahGCHeuristics, \"compact\") == 0) {\n+    return new ShenandoahCompactHeuristics(space_info);\n+  } else {\n+    vm_exit_during_initialization(\"Unknown -XX:ShenandoahGCHeuristics option\");\n+  }\n+\n+  ShouldNotReachHere();\n+  return nullptr;\n+}\n+\n","filename":"src\/hotspot\/share\/gc\/shenandoah\/mode\/shenandoahMode.cpp","additions":54,"deletions":0,"binary":false,"changes":54,"status":"added"},{"patch":"@@ -3,0 +3,1 @@\n+ * Copyright Amazon.com Inc. or its affiliates. All Rights Reserved.\n@@ -28,0 +29,1 @@\n+#include \"gc\/shared\/gc_globals.hpp\"\n@@ -29,0 +31,2 @@\n+#include \"runtime\/java.hpp\"\n+#include \"utilities\/formatBuffer.hpp\"\n@@ -30,0 +34,1 @@\n+class ShenandoahSpaceInfo;\n@@ -51,1 +56,1 @@\n-  virtual ShenandoahHeuristics* initialize_heuristics() const = 0;\n+  virtual ShenandoahHeuristics* initialize_heuristics(ShenandoahSpaceInfo* space_info) const;\n@@ -55,0 +60,1 @@\n+  virtual bool is_generational() { return false; }\n","filename":"src\/hotspot\/share\/gc\/shenandoah\/mode\/shenandoahMode.hpp","additions":7,"deletions":1,"binary":false,"changes":8,"status":"modified"},{"patch":"@@ -26,0 +26,2 @@\n+#include \"gc\/shenandoah\/heuristics\/shenandoahHeuristics.hpp\"\n+#include \"gc\/shenandoah\/heuristics\/shenandoahSpaceInfo.hpp\"\n@@ -28,0 +30,1 @@\n+#include \"gc\/shenandoah\/shenandoahHeap.inline.hpp\"\n@@ -30,1 +33,0 @@\n-#include \"runtime\/globals_extension.hpp\"\n@@ -49,1 +51,0 @@\n-  SHENANDOAH_ERGO_DISABLE_FLAG(ShenandoahIUBarrier);\n@@ -53,0 +54,1 @@\n+  SHENANDOAH_ERGO_DISABLE_FLAG(ShenandoahCardBarrier);\n@@ -55,1 +57,3 @@\n-  \/\/ No barriers are required to run.\n+  \/\/ Passive mode does not instantiate the machinery to support the card table.\n+  \/\/ Exit if the flag has been explicitly set.\n+  SHENANDOAH_CHECK_FLAG_UNSET(ShenandoahCardBarrier);\n@@ -57,1 +61,2 @@\n-ShenandoahHeuristics* ShenandoahPassiveMode::initialize_heuristics() const {\n+\n+ShenandoahHeuristics* ShenandoahPassiveMode::initialize_heuristics(ShenandoahSpaceInfo* space_info) const {\n@@ -61,1 +66,1 @@\n-  return new ShenandoahPassiveHeuristics();\n+  return new ShenandoahPassiveHeuristics(space_info);\n","filename":"src\/hotspot\/share\/gc\/shenandoah\/mode\/shenandoahPassiveMode.cpp","additions":10,"deletions":5,"binary":false,"changes":15,"status":"modified"},{"patch":"@@ -33,2 +33,1 @@\n-  virtual ShenandoahHeuristics* initialize_heuristics() const;\n-\n+  virtual ShenandoahHeuristics* initialize_heuristics(ShenandoahSpaceInfo* space_info) const;\n","filename":"src\/hotspot\/share\/gc\/shenandoah\/mode\/shenandoahPassiveMode.hpp","additions":1,"deletions":2,"binary":false,"changes":3,"status":"modified"},{"patch":"@@ -26,4 +26,1 @@\n-#include \"gc\/shenandoah\/heuristics\/shenandoahAdaptiveHeuristics.hpp\"\n-#include \"gc\/shenandoah\/heuristics\/shenandoahAggressiveHeuristics.hpp\"\n-#include \"gc\/shenandoah\/heuristics\/shenandoahCompactHeuristics.hpp\"\n-#include \"gc\/shenandoah\/heuristics\/shenandoahStaticHeuristics.hpp\"\n+#include \"gc\/shenandoah\/heuristics\/shenandoahHeuristics.hpp\"\n@@ -46,1 +43,0 @@\n-  SHENANDOAH_CHECK_FLAG_UNSET(ShenandoahIUBarrier);\n@@ -51,17 +47,1 @@\n-}\n-\n-ShenandoahHeuristics* ShenandoahSATBMode::initialize_heuristics() const {\n-  if (ShenandoahGCHeuristics == nullptr) {\n-    vm_exit_during_initialization(\"Unknown -XX:ShenandoahGCHeuristics option (null)\");\n-  }\n-  if (strcmp(ShenandoahGCHeuristics, \"aggressive\") == 0) {\n-    return new ShenandoahAggressiveHeuristics();\n-  } else if (strcmp(ShenandoahGCHeuristics, \"static\") == 0) {\n-    return new ShenandoahStaticHeuristics();\n-  } else if (strcmp(ShenandoahGCHeuristics, \"adaptive\") == 0) {\n-    return new ShenandoahAdaptiveHeuristics();\n-  } else if (strcmp(ShenandoahGCHeuristics, \"compact\") == 0) {\n-    return new ShenandoahCompactHeuristics();\n-  }\n-  vm_exit_during_initialization(\"Unknown -XX:ShenandoahGCHeuristics option\");\n-  return nullptr;\n+  SHENANDOAH_CHECK_FLAG_UNSET(ShenandoahCardBarrier);\n","filename":"src\/hotspot\/share\/gc\/shenandoah\/mode\/shenandoahSATBMode.cpp","additions":2,"deletions":22,"binary":false,"changes":24,"status":"modified"},{"patch":"@@ -35,1 +35,0 @@\n-  virtual ShenandoahHeuristics* initialize_heuristics() const;\n","filename":"src\/hotspot\/share\/gc\/shenandoah\/mode\/shenandoahSATBMode.hpp","additions":0,"deletions":1,"binary":false,"changes":1,"status":"modified"},{"patch":"@@ -0,0 +1,60 @@\n+\/*\n+ * Copyright Amazon.com Inc. or its affiliates. All Rights Reserved.\n+ * DO NOT ALTER OR REMOVE COPYRIGHT NOTICES OR THIS FILE HEADER.\n+ *\n+ * This code is free software; you can redistribute it and\/or modify it\n+ * under the terms of the GNU General Public License version 2 only, as\n+ * published by the Free Software Foundation.\n+ *\n+ * This code is distributed in the hope that it will be useful, but WITHOUT\n+ * ANY WARRANTY; without even the implied warranty of MERCHANTABILITY or\n+ * FITNESS FOR A PARTICULAR PURPOSE.  See the GNU General Public License\n+ * version 2 for more details (a copy is included in the LICENSE file that\n+ * accompanied this code).\n+ *\n+ * You should have received a copy of the GNU General Public License version\n+ * 2 along with this work; if not, write to the Free Software Foundation,\n+ * Inc., 51 Franklin St, Fifth Floor, Boston, MA 02110-1301 USA.\n+ *\n+ * Please contact Oracle, 500 Oracle Parkway, Redwood Shores, CA 94065 USA\n+ * or visit www.oracle.com if you need additional information or have any\n+ * questions.\n+ *\n+ *\/\n+\n+#ifndef SHARE_GC_SHENANDOAH_SHENANDOAHAFFILIATION_HPP\n+#define SHARE_GC_SHENANDOAH_SHENANDOAHAFFILIATION_HPP\n+\n+enum ShenandoahAffiliation {\n+  FREE,\n+  YOUNG_GENERATION,\n+  OLD_GENERATION,\n+};\n+\n+inline const char* shenandoah_affiliation_code(ShenandoahAffiliation type) {\n+  switch(type) {\n+    case FREE:\n+      return \"F\";\n+    case YOUNG_GENERATION:\n+      return \"Y\";\n+    case OLD_GENERATION:\n+      return \"O\";\n+    default:\n+      ShouldNotReachHere();\n+  }\n+}\n+\n+inline const char* shenandoah_affiliation_name(ShenandoahAffiliation type) {\n+  switch (type) {\n+    case FREE:\n+      return \"FREE\";\n+    case YOUNG_GENERATION:\n+      return \"YOUNG\";\n+    case OLD_GENERATION:\n+      return \"OLD\";\n+    default:\n+      ShouldNotReachHere();\n+  }\n+}\n+\n+#endif \/\/ SHARE_GC_SHENANDOAH_SHENANDOAHAFFILIATION_HPP\n","filename":"src\/hotspot\/share\/gc\/shenandoah\/shenandoahAffiliation.hpp","additions":60,"deletions":0,"binary":false,"changes":60,"status":"added"},{"patch":"@@ -0,0 +1,413 @@\n+\/*\n+ * Copyright Amazon.com Inc. or its affiliates. All Rights Reserved.\n+ * DO NOT ALTER OR REMOVE COPYRIGHT NOTICES OR THIS FILE HEADER.\n+ *\n+ * This code is free software; you can redistribute it and\/or modify it\n+ * under the terms of the GNU General Public License version 2 only, as\n+ * published by the Free Software Foundation.\n+ *\n+ * This code is distributed in the hope that it will be useful, but WITHOUT\n+ * ANY WARRANTY; without even the implied warranty of MERCHANTABILITY or\n+ * FITNESS FOR A PARTICULAR PURPOSE.  See the GNU General Public License\n+ * version 2 for more details (a copy is included in the LICENSE file that\n+ * accompanied this code).\n+ *\n+ * You should have received a copy of the GNU General Public License version\n+ * 2 along with this work; if not, write to the Free Software Foundation,\n+ * Inc., 51 Franklin St, Fifth Floor, Boston, MA 02110-1301 USA.\n+ *\n+ * Please contact Oracle, 500 Oracle Parkway, Redwood Shores, CA 94065 USA\n+ * or visit www.oracle.com if you need additional information or have any\n+ * questions.\n+ *\n+ *\/\n+\n+#include \"precompiled.hpp\"\n+\n+#include \"gc\/shenandoah\/mode\/shenandoahGenerationalMode.hpp\"\n+#include \"gc\/shenandoah\/shenandoahAgeCensus.hpp\"\n+#include \"gc\/shenandoah\/shenandoahHeap.inline.hpp\"\n+\n+ShenandoahAgeCensus::ShenandoahAgeCensus()\n+  : ShenandoahAgeCensus(ShenandoahHeap::heap()->max_workers())\n+{\n+  assert(ShenandoahHeap::heap()->mode()->is_generational(), \"Only in generational mode\");\n+}\n+\n+ShenandoahAgeCensus::ShenandoahAgeCensus(uint max_workers)\n+  : _max_workers(max_workers)\n+{\n+  if (ShenandoahGenerationalMinTenuringAge > ShenandoahGenerationalMaxTenuringAge) {\n+    vm_exit_during_initialization(\n+      err_msg(\"ShenandoahGenerationalMinTenuringAge=\" SIZE_FORMAT\n+              \" should be no more than ShenandoahGenerationalMaxTenuringAge=\" SIZE_FORMAT,\n+              ShenandoahGenerationalMinTenuringAge, ShenandoahGenerationalMaxTenuringAge));\n+  }\n+\n+  _global_age_table = NEW_C_HEAP_ARRAY(AgeTable*, MAX_SNAPSHOTS, mtGC);\n+  CENSUS_NOISE(_global_noise = NEW_C_HEAP_ARRAY(ShenandoahNoiseStats, MAX_SNAPSHOTS, mtGC);)\n+  _tenuring_threshold = NEW_C_HEAP_ARRAY(uint, MAX_SNAPSHOTS, mtGC);\n+  CENSUS_NOISE(_skipped = 0);\n+  NOT_PRODUCT(_counted = 0);\n+  NOT_PRODUCT(_total = 0);\n+\n+  for (int i = 0; i < MAX_SNAPSHOTS; i++) {\n+    \/\/ Note that we don't now get perfdata from age_table\n+    _global_age_table[i] = new AgeTable(false);\n+    CENSUS_NOISE(_global_noise[i].clear();)\n+    \/\/ Sentinel value\n+    _tenuring_threshold[i] = MAX_COHORTS;\n+  }\n+  if (ShenandoahGenerationalAdaptiveTenuring && !ShenandoahGenerationalCensusAtEvac) {\n+    _local_age_table = NEW_C_HEAP_ARRAY(AgeTable*, _max_workers, mtGC);\n+    CENSUS_NOISE(_local_noise = NEW_C_HEAP_ARRAY(ShenandoahNoiseStats, max_workers, mtGC);)\n+    for (uint i = 0; i < _max_workers; i++) {\n+      _local_age_table[i] = new AgeTable(false);\n+      CENSUS_NOISE(_local_noise[i].clear();)\n+    }\n+  } else {\n+    _local_age_table = nullptr;\n+  }\n+  _epoch = MAX_SNAPSHOTS - 1;  \/\/ see update_epoch()\n+}\n+\n+ShenandoahAgeCensus::~ShenandoahAgeCensus() {\n+  for (uint i = 0; i < MAX_SNAPSHOTS; i++) {\n+    delete _global_age_table[i];\n+  }\n+  FREE_C_HEAP_ARRAY(AgeTable*, _global_age_table);\n+  FREE_C_HEAP_ARRAY(uint, _tenuring_threshold);\n+  CENSUS_NOISE(FREE_C_HEAP_ARRAY(ShenandoahNoiseStats, _global_noise));\n+  if (_local_age_table) {\n+    for (uint i = 0; i < _max_workers; i++) {\n+      delete _local_age_table[i];\n+    }\n+    FREE_C_HEAP_ARRAY(AgeTable*, _local_age_table);\n+    CENSUS_NOISE(FREE_C_HEAP_ARRAY(ShenandoahNoiseStats, _local_noise));\n+  }\n+}\n+\n+CENSUS_NOISE(void ShenandoahAgeCensus::add(uint obj_age, uint region_age, uint region_youth, size_t size, uint worker_id) {)\n+NO_CENSUS_NOISE(void ShenandoahAgeCensus::add(uint obj_age, uint region_age, size_t size, uint worker_id) {)\n+  if (obj_age <= markWord::max_age) {\n+    assert(obj_age < MAX_COHORTS && region_age < MAX_COHORTS, \"Should have been tenured\");\n+#ifdef SHENANDOAH_CENSUS_NOISE\n+    \/\/ Region ageing is stochastic and non-monotonic; this vitiates mortality\n+    \/\/ demographics in ways that might defeat our algorithms. Marking may be a\n+    \/\/ time when we might be able to correct this, but we currently do not do\n+    \/\/ this. Like skipped statistics further below, we want to track the\n+    \/\/ impact of this noise to see if this may be worthwhile. JDK-<TBD>.\n+    uint age = obj_age;\n+    if (region_age > 0) {\n+      add_aged(size, worker_id);   \/\/ this tracking is coarse for now\n+      age += region_age;\n+      if (age >= MAX_COHORTS) {\n+        age = (uint)(MAX_COHORTS - 1);  \/\/ clamp\n+        add_clamped(size, worker_id);\n+      }\n+    }\n+    if (region_youth > 0) {   \/\/ track object volume with retrograde age\n+      add_young(size, worker_id);\n+    }\n+#else   \/\/ SHENANDOAH_CENSUS_NOISE\n+    uint age = MIN2(obj_age + region_age, (uint)(MAX_COHORTS - 1));  \/\/ clamp\n+#endif  \/\/ SHENANDOAH_CENSUS_NOISE\n+    get_local_age_table(worker_id)->add(age, size);\n+  } else {\n+    \/\/ update skipped statistics\n+    CENSUS_NOISE(add_skipped(size, worker_id);)\n+  }\n+}\n+\n+#ifdef SHENANDOAH_CENSUS_NOISE\n+void ShenandoahAgeCensus::add_skipped(size_t size, uint worker_id) {\n+  _local_noise[worker_id].skipped += size;\n+}\n+\n+void ShenandoahAgeCensus::add_aged(size_t size, uint worker_id) {\n+  _local_noise[worker_id].aged += size;\n+}\n+\n+void ShenandoahAgeCensus::add_clamped(size_t size, uint worker_id) {\n+  _local_noise[worker_id].clamped += size;\n+}\n+\n+void ShenandoahAgeCensus::add_young(size_t size, uint worker_id) {\n+  _local_noise[worker_id].young += size;\n+}\n+#endif \/\/ SHENANDOAH_CENSUS_NOISE\n+\n+\/\/ Prepare for a new census update, by clearing appropriate global slots.\n+void ShenandoahAgeCensus::prepare_for_census_update() {\n+  assert(_epoch < MAX_SNAPSHOTS, \"Out of bounds\");\n+  if (++_epoch >= MAX_SNAPSHOTS) {\n+    _epoch=0;\n+  }\n+  _global_age_table[_epoch]->clear();\n+  CENSUS_NOISE(_global_noise[_epoch].clear();)\n+}\n+\n+\/\/ Update the census data from appropriate sources,\n+\/\/ and compute the new tenuring threshold.\n+void ShenandoahAgeCensus::update_census(size_t age0_pop, AgeTable* pv1, AgeTable* pv2) {\n+  prepare_for_census_update();\n+  assert(_global_age_table[_epoch]->is_clear(), \"Dirty decks\");\n+  CENSUS_NOISE(assert(_global_noise[_epoch].is_clear(), \"Dirty decks\");)\n+  if (ShenandoahGenerationalAdaptiveTenuring && !ShenandoahGenerationalCensusAtEvac) {\n+    assert(pv1 == nullptr && pv2 == nullptr, \"Error, check caller\");\n+    \/\/ Seed cohort 0 with population that may have been missed during\n+    \/\/ regular census.\n+    _global_age_table[_epoch]->add(0u, age0_pop);\n+\n+    \/\/ Merge data from local age tables into the global age table for the epoch,\n+    \/\/ clearing the local tables.\n+    for (uint i = 0; i < _max_workers; i++) {\n+      \/\/ age stats\n+      _global_age_table[_epoch]->merge(_local_age_table[i]);\n+      _local_age_table[i]->clear();   \/\/ clear for next census\n+      \/\/ Merge noise stats\n+      CENSUS_NOISE(_global_noise[_epoch].merge(_local_noise[i]);)\n+      CENSUS_NOISE(_local_noise[i].clear();)\n+    }\n+  } else {\n+    \/\/ census during evac\n+    assert(pv1 != nullptr && pv2 != nullptr, \"Error, check caller\");\n+    _global_age_table[_epoch]->merge(pv1);\n+    _global_age_table[_epoch]->merge(pv2);\n+  }\n+\n+  update_tenuring_threshold();\n+\n+  \/\/ used for checking reasonableness of census coverage, non-product\n+  \/\/ only.\n+  NOT_PRODUCT(update_total();)\n+}\n+\n+\n+\/\/ Reset the epoch for the global age tables,\n+\/\/ clearing all history.\n+void ShenandoahAgeCensus::reset_global() {\n+  assert(_epoch < MAX_SNAPSHOTS, \"Out of bounds\");\n+  for (uint i = 0; i < MAX_SNAPSHOTS; i++) {\n+    _global_age_table[i]->clear();\n+    CENSUS_NOISE(_global_noise[i].clear();)\n+  }\n+  _epoch = MAX_SNAPSHOTS;\n+  assert(_epoch < MAX_SNAPSHOTS, \"Error\");\n+}\n+\n+\/\/ Reset the local age tables, clearing any partial census.\n+void ShenandoahAgeCensus::reset_local() {\n+  if (!ShenandoahGenerationalAdaptiveTenuring || ShenandoahGenerationalCensusAtEvac) {\n+    assert(_local_age_table == nullptr, \"Error\");\n+    return;\n+  }\n+  for (uint i = 0; i < _max_workers; i++) {\n+    _local_age_table[i]->clear();\n+    CENSUS_NOISE(_local_noise[i].clear();)\n+  }\n+}\n+\n+#ifndef PRODUCT\n+\/\/ Is global census information clear?\n+bool ShenandoahAgeCensus::is_clear_global() {\n+  assert(_epoch < MAX_SNAPSHOTS, \"Out of bounds\");\n+  for (uint i = 0; i < MAX_SNAPSHOTS; i++) {\n+    bool clear = _global_age_table[i]->is_clear();\n+    CENSUS_NOISE(clear |= _global_noise[i].is_clear();)\n+    if (!clear) {\n+      return false;\n+    }\n+  }\n+  return true;\n+}\n+\n+\/\/ Is local census information clear?\n+bool ShenandoahAgeCensus::is_clear_local() {\n+  if (!ShenandoahGenerationalAdaptiveTenuring || ShenandoahGenerationalCensusAtEvac) {\n+    assert(_local_age_table == nullptr, \"Error\");\n+    return true;\n+  }\n+  for (uint i = 0; i < _max_workers; i++) {\n+    bool clear = _local_age_table[i]->is_clear();\n+    CENSUS_NOISE(clear |= _local_noise[i].is_clear();)\n+    if (!clear) {\n+      return false;\n+    }\n+  }\n+  return true;\n+}\n+\n+size_t ShenandoahAgeCensus::get_all_ages(uint snap) {\n+  assert(snap < MAX_SNAPSHOTS, \"Out of bounds\");\n+  size_t pop = 0;\n+  const AgeTable* pv = _global_age_table[snap];\n+  for (uint i = 0; i < MAX_COHORTS; i++) {\n+    pop += pv->sizes[i];\n+  }\n+  return pop;\n+}\n+\n+size_t ShenandoahAgeCensus::get_skipped(uint snap) {\n+  assert(snap < MAX_SNAPSHOTS, \"Out of bounds\");\n+  return _global_noise[snap].skipped;\n+}\n+\n+void ShenandoahAgeCensus::update_total() {\n+  _counted = get_all_ages(_epoch);\n+  _skipped = get_skipped(_epoch);\n+  _total   = _counted + _skipped;\n+}\n+#endif \/\/ !PRODUCT\n+\n+void ShenandoahAgeCensus::update_tenuring_threshold() {\n+  if (!ShenandoahGenerationalAdaptiveTenuring) {\n+    _tenuring_threshold[_epoch] = InitialTenuringThreshold;\n+  } else {\n+    uint tt = compute_tenuring_threshold();\n+    assert(tt <= MAX_COHORTS, \"Out of bounds\");\n+    _tenuring_threshold[_epoch] = tt;\n+  }\n+  print();\n+  log_info(gc, age)(\"New tenuring threshold \" UINTX_FORMAT \" (min \" UINTX_FORMAT \", max \" UINTX_FORMAT\")\",\n+    (uintx) _tenuring_threshold[_epoch], ShenandoahGenerationalMinTenuringAge, ShenandoahGenerationalMaxTenuringAge);\n+}\n+\n+\/\/ Currently Shenandoah{Min,Max}TenuringAge have a floor of 1 because we\n+\/\/ aren't set up to promote age 0 objects.\n+uint ShenandoahAgeCensus::compute_tenuring_threshold() {\n+  \/\/ Dispose of the extremal cases early so the loop below\n+  \/\/ is less fragile.\n+  if (ShenandoahGenerationalMaxTenuringAge == ShenandoahGenerationalMinTenuringAge) {\n+    return ShenandoahGenerationalMaxTenuringAge; \/\/ Any value in [1,16]\n+  }\n+  assert(ShenandoahGenerationalMinTenuringAge < ShenandoahGenerationalMaxTenuringAge, \"Error\");\n+\n+  \/\/ Starting with the oldest cohort with a non-trivial population\n+  \/\/ (as specified by ShenandoahGenerationalTenuringCohortPopulationThreshold) in the\n+  \/\/ previous epoch, and working down the cohorts by age, find the\n+  \/\/ oldest age that has a significant mortality rate (as specified by\n+  \/\/ ShenandoahGenerationalTenuringMortalityRateThreshold). We use this as\n+  \/\/ tenuring age to be used for the evacuation cycle to follow.\n+  \/\/ Results are clamped between user-specified min & max guardrails,\n+  \/\/ so we ignore any cohorts outside ShenandoahGenerational[Min,Max]Age.\n+\n+  \/\/ Current and previous epoch in ring\n+  const uint cur_epoch = _epoch;\n+  const uint prev_epoch = cur_epoch > 0  ? cur_epoch - 1 : markWord::max_age;\n+\n+  \/\/ Current and previous population vectors in ring\n+  const AgeTable* cur_pv = _global_age_table[cur_epoch];\n+  const AgeTable* prev_pv = _global_age_table[prev_epoch];\n+  uint upper_bound = ShenandoahGenerationalMaxTenuringAge;\n+  const uint prev_tt = previous_tenuring_threshold();\n+  if (ShenandoahGenerationalCensusIgnoreOlderCohorts && prev_tt > 0) {\n+     \/\/ We stay below the computed tenuring threshold for the last cycle,\n+     \/\/ ignoring the mortality rates of any older cohorts (which may see\n+     \/\/ higher mortality rates due to promotions).\n+     upper_bound = MIN2(upper_bound, prev_tt);\n+  }\n+  upper_bound = MIN2(upper_bound, markWord::max_age);\n+\n+  const uint lower_bound = MAX2((uint)ShenandoahGenerationalMinTenuringAge, 1u);\n+\n+  uint tenuring_threshold = upper_bound;\n+  for (uint i = upper_bound; i >= lower_bound; i--) {\n+    assert(i > 0, \"Index (i-1) would underflow\/wrap\");\n+    assert(i <= markWord::max_age, \"Index i would overflow\");\n+    \/\/ Cohort of current age i\n+    const size_t cur_pop = cur_pv->sizes[i];\n+    const size_t prev_pop = prev_pv->sizes[i-1];\n+    const double mr = mortality_rate(prev_pop, cur_pop);\n+    if (prev_pop > ShenandoahGenerationalTenuringCohortPopulationThreshold &&\n+        mr > ShenandoahGenerationalTenuringMortalityRateThreshold) {\n+      \/\/ This is the oldest cohort that has high mortality.\n+      \/\/ We ignore any cohorts that had a very low population count, or\n+      \/\/ that have a lower mortality rate than we care to age in young; these\n+      \/\/ cohorts are considered eligible for tenuring when all older\n+      \/\/ cohorts are. We return the next higher age as the tenuring threshold\n+      \/\/ so that we do not prematurely promote objects of this age.\n+      assert(tenuring_threshold == i + 1 || tenuring_threshold == upper_bound, \"Error\");\n+      assert(tenuring_threshold >= lower_bound && tenuring_threshold <= upper_bound, \"Error\");\n+      return i + 1;\n+    }\n+    \/\/ Remember that we passed over this cohort, looking for younger cohorts\n+    \/\/ showing high mortality. We want to tenure cohorts of this age.\n+    tenuring_threshold = i;\n+  }\n+  assert(tenuring_threshold >= lower_bound && tenuring_threshold <= upper_bound, \"Error\");\n+  return tenuring_threshold;\n+}\n+\n+\/\/ Mortality rate of a cohort, given its previous and current population\n+double ShenandoahAgeCensus::mortality_rate(size_t prev_pop, size_t cur_pop) {\n+  \/\/ The following also covers the case where both entries are 0\n+  if (prev_pop <= cur_pop) {\n+    \/\/ adjust for inaccurate censuses by finessing the\n+    \/\/ reappearance of dark matter as normal matter;\n+    \/\/ mortality rate is 0 if population remained the same\n+    \/\/ or increased.\n+    if (cur_pop > prev_pop) {\n+      log_trace(gc, age)\n+        (\" (dark matter) Cohort population \" SIZE_FORMAT_W(10) \" to \" SIZE_FORMAT_W(10),\n+        prev_pop*oopSize, cur_pop*oopSize);\n+    }\n+    return 0.0;\n+  }\n+  assert(prev_pop > 0 && prev_pop > cur_pop, \"Error\");\n+  return 1.0 - (((double)cur_pop)\/((double)prev_pop));\n+}\n+\n+void ShenandoahAgeCensus::print() {\n+\n+  const LogTarget(Debug, gc, age) lt;\n+  if (!lt.is_enabled()) {\n+    return;\n+  }\n+\n+  LogStream ls(lt);\n+\n+  \/\/ Print the population vector for the current epoch, and\n+  \/\/ for the previous epoch, as well as the computed mortality\n+  \/\/ ratio for each extant cohort.\n+  const uint cur_epoch = _epoch;\n+  const uint prev_epoch = cur_epoch > 0 ? cur_epoch - 1: markWord::max_age;\n+\n+  const AgeTable* cur_pv = _global_age_table[cur_epoch];\n+  const AgeTable* prev_pv = _global_age_table[prev_epoch];\n+\n+  const uint tt = tenuring_threshold();\n+\n+  size_t total= 0;\n+  for (uint i = 1; i < MAX_COHORTS; i++) {\n+    const size_t prev_pop = prev_pv->sizes[i-1];  \/\/ (i-1) OK because i >= 1\n+    const size_t cur_pop  = cur_pv->sizes[i];\n+    const double mr = mortality_rate(prev_pop, cur_pop);\n+    \/\/ Suppress printing when everything is zero\n+    if (prev_pop + cur_pop > 0) {\n+      ls.print_cr(\" - age %3u: prev \" SIZE_FORMAT_W(10) \" bytes, curr \" SIZE_FORMAT_W(10) \" bytes, mortality %.2f \",\n+         i, prev_pop*oopSize, cur_pop*oopSize, mr);\n+    }\n+    total += cur_pop;\n+    if (i == tt) {\n+      \/\/ Underline the cohort for tenuring threshold (if < MAX_COHORTS)\n+      ls.print_cr(\"----------------------------------------------------------------------------\");\n+    }\n+  }\n+  CENSUS_NOISE(_global_noise[cur_epoch].print(ls, total);)\n+}\n+\n+#ifdef SHENANDOAH_CENSUS_NOISE\n+void ShenandoahNoiseStats::print(LogStream& ls, const size_t total) {\n+  if (total > 0) {\n+    const float f_skipped = (float)skipped\/(float)total;\n+    const float f_aged    = (float)aged\/(float)total;\n+    const float f_clamped = (float)clamped\/(float)total;\n+    const float f_young   = (float)young\/(float)total;\n+    ls.print_cr(\"Skipped: \" SIZE_FORMAT_W(10) \" (%.2f),  R-Aged: \" SIZE_FORMAT_W(10) \" (%.2f),  \"\n+                \"Clamped: \" SIZE_FORMAT_W(10) \" (%.2f),  R-Young: \" SIZE_FORMAT_W(10) \" (%.2f)\",\n+                skipped*oopSize, f_skipped, aged*oopSize, f_aged,\n+                clamped*oopSize, f_clamped, young*oopSize, f_young);\n+  }\n+}\n+#endif \/\/ SHENANDOAH_CENSUS_NOISE\n","filename":"src\/hotspot\/share\/gc\/shenandoah\/shenandoahAgeCensus.cpp","additions":413,"deletions":0,"binary":false,"changes":413,"status":"added"},{"patch":"@@ -0,0 +1,241 @@\n+\/*\n+ * Copyright Amazon.com Inc. or its affiliates. All Rights Reserved.\n+ * DO NOT ALTER OR REMOVE COPYRIGHT NOTICES OR THIS FILE HEADER.\n+ *\n+ * This code is free software; you can redistribute it and\/or modify it\n+ * under the terms of the GNU General Public License version 2 only, as\n+ * published by the Free Software Foundation.\n+ *\n+ * This code is distributed in the hope that it will be useful, but WITHOUT\n+ * ANY WARRANTY; without even the implied warranty of MERCHANTABILITY or\n+ * FITNESS FOR A PARTICULAR PURPOSE.  See the GNU General Public License\n+ * version 2 for more details (a copy is included in the LICENSE file that\n+ * accompanied this code).\n+ *\n+ * You should have received a copy of the GNU General Public License version\n+ * 2 along with this work; if not, write to the Free Software Foundation,\n+ * Inc., 51 Franklin St, Fifth Floor, Boston, MA 02110-1301 USA.\n+ *\n+ * Please contact Oracle, 500 Oracle Parkway, Redwood Shores, CA 94065 USA\n+ * or visit www.oracle.com if you need additional information or have any\n+ * questions.\n+ *\n+ *\/\n+\n+#ifndef SHARE_GC_SHENANDOAH_SHENANDOAHAGECENSUS_HPP\n+#define SHARE_GC_SHENANDOAH_SHENANDOAHAGECENSUS_HPP\n+\n+#include \"gc\/shared\/ageTable.hpp\"\n+\n+#ifndef PRODUCT\n+\/\/ Enable noise instrumentation\n+#define SHENANDOAH_CENSUS_NOISE 1\n+#endif  \/\/ PRODUCT\n+\n+#ifdef SHENANDOAH_CENSUS_NOISE\n+\n+#define CENSUS_NOISE(x) x\n+#define NO_CENSUS_NOISE(x)\n+\n+class LogStream;\n+\n+struct ShenandoahNoiseStats {\n+  size_t skipped;   \/\/ Volume of objects skipped\n+  size_t aged;      \/\/ Volume of objects from aged regions\n+  size_t clamped;   \/\/ Volume of objects whose ages were clamped\n+  size_t young;     \/\/ Volume of (rejuvenated) objects of retrograde age\n+\n+  ShenandoahNoiseStats() {\n+    clear();\n+  }\n+\n+  void clear() {\n+    skipped = 0;\n+    aged = 0;\n+    clamped = 0;\n+    young = 0;\n+  }\n+\n+#ifndef PRODUCT\n+  bool is_clear() {\n+    return (skipped + aged + clamped + young) == 0;\n+  }\n+#endif \/\/ !PRODUCT\n+\n+  void merge(ShenandoahNoiseStats& other) {\n+    skipped += other.skipped;\n+    aged    += other.aged;\n+    clamped += other.clamped;\n+    young   += other.young;\n+  }\n+\n+  void print(LogStream& ls, size_t total);\n+};\n+#else  \/\/ SHENANDOAH_CENSUS_NOISE\n+#define CENSUS_NOISE(x)\n+#define NO_CENSUS_NOISE(x) x\n+#endif \/\/ SHENANDOAH_CENSUS_NOISE\n+\n+\/\/ A class for tracking a sequence of cohort population vectors (or,\n+\/\/ interchangeably, age tables) for up to C=MAX_COHORTS age cohorts, where a cohort\n+\/\/ represents the set of objects allocated during a specific inter-GC epoch.\n+\/\/ Epochs are demarcated by GC cycles, with those surviving a cycle aging by\n+\/\/ an epoch. The census tracks the historical variation of cohort demographics\n+\/\/ across N=MAX_SNAPSHOTS recent epochs. Since there are at most C age cohorts in\n+\/\/ the population, we need only track at most N=C epochal snapshots to track a\n+\/\/ maximal longitudinal demographics of every object's longitudinal cohort in\n+\/\/ the young generation. The _global_age_table is thus, currently, a C x N (row-major)\n+\/\/ matrix, with C=16, and, for now N=C=16, currently.\n+\/\/ In theory, we might decide to track even longer (N=MAX_SNAPSHOTS) demographic\n+\/\/ histories, but that isn't the case today. In particular, the current tenuring\n+\/\/ threshold algorithm uses only 2 most recent snapshots, with the remaining\n+\/\/ MAX_SNAPSHOTS-2=14 reserved for research purposes.\n+\/\/\n+\/\/ In addition, this class also maintains per worker population vectors into which\n+\/\/ census for the current minor GC is accumulated (during marking or, optionally, during\n+\/\/ evacuation). These are cleared after each marking (respectively, evacuation) cycle,\n+\/\/ once the per-worker data is consolidated into the appropriate population vector\n+\/\/ per minor collection. The _local_age_table is thus C x N, for N GC workers.\n+class ShenandoahAgeCensus: public CHeapObj<mtGC> {\n+  AgeTable** _global_age_table;      \/\/ Global age table used for adapting tenuring threshold, one per snapshot\n+  AgeTable** _local_age_table;       \/\/ Local scratch age tables to track object ages, one per worker\n+\n+#ifdef SHENANDOAH_CENSUS_NOISE\n+  ShenandoahNoiseStats* _global_noise; \/\/ Noise stats, one per snapshot\n+  ShenandoahNoiseStats* _local_noise;  \/\/ Local scratch table for noise stats, one per worker\n+\n+  size_t _skipped;                   \/\/ net size of objects encountered, but skipped during census,\n+                                     \/\/ because their age was indeterminate\n+#endif \/\/ SHENANDOAH_CENSUS_NOISE\n+\n+#ifndef PRODUCT\n+  size_t _counted;                   \/\/ net size of objects counted in census\n+  size_t _total;                     \/\/ net size of objects encountered (counted or skipped) in census\n+#endif\n+\n+  uint  _epoch;                      \/\/ Current epoch (modulo max age)\n+  uint* _tenuring_threshold;         \/\/ An array of the last N tenuring threshold values we\n+                                     \/\/ computed.\n+\n+  uint _max_workers;                 \/\/ Maximum number of workers for parallel tasks\n+\n+  \/\/ Mortality rate of a cohort, given its population in\n+  \/\/ previous and current epochs\n+  double mortality_rate(size_t prev_pop, size_t cur_pop);\n+\n+  \/\/ Update to a new epoch, creating a slot for new census.\n+  void prepare_for_census_update();\n+\n+  \/\/ Update the tenuring threshold, calling\n+  \/\/ compute_tenuring_threshold() to calculate the new\n+  \/\/ value\n+  void update_tenuring_threshold();\n+\n+  \/\/ Use _global_age_table and the current _epoch to compute a new tenuring\n+  \/\/ threshold, which will be remembered until the next invocation of\n+  \/\/ compute_tenuring_threshold.\n+  uint compute_tenuring_threshold();\n+\n+  \/\/ Return the tenuring threshold computed for the previous epoch\n+  uint previous_tenuring_threshold() const {\n+    assert(_epoch < MAX_SNAPSHOTS, \"Error\");\n+    uint prev = _epoch - 1;\n+    if (prev >= MAX_SNAPSHOTS) {\n+      \/\/ _epoch is 0\n+      assert(_epoch == 0, \"Error\");\n+      prev = MAX_SNAPSHOTS - 1;\n+    }\n+    return _tenuring_threshold[prev];\n+  }\n+\n+#ifndef PRODUCT\n+  \/\/ Return the sum of size of objects of all ages recorded in the\n+  \/\/ census at snapshot indexed by snap.\n+  size_t get_all_ages(uint snap);\n+\n+  \/\/ Return the size of all objects that were encountered, but skipped,\n+  \/\/ during the census, because their age was indeterminate.\n+  size_t get_skipped(uint snap);\n+\n+  \/\/ Update the total size of objects counted or skipped at the census for\n+  \/\/ the most recent epoch.\n+  void update_total();\n+#endif \/\/ !PRODUCT\n+\n+ public:\n+  enum {\n+    MAX_COHORTS = AgeTable::table_size,    \/\/ = markWord::max_age + 1\n+    MAX_SNAPSHOTS = MAX_COHORTS            \/\/ May change in the future\n+  };\n+\n+  ShenandoahAgeCensus();\n+  ShenandoahAgeCensus(uint max_workers);\n+  ~ShenandoahAgeCensus();\n+\n+  \/\/ Return the local age table (population vector) for worker_id.\n+  \/\/ Only used in the case of (ShenandoahGenerationalAdaptiveTenuring && !ShenandoahGenerationalCensusAtEvac)\n+  AgeTable* get_local_age_table(uint worker_id) const {\n+    return _local_age_table[worker_id];\n+  }\n+\n+  \/\/ Return the most recently computed tenuring threshold.\n+  \/\/ Visible for testing. Use is_tenurable for consistent tenuring comparisons.\n+  uint tenuring_threshold() const { return _tenuring_threshold[_epoch]; }\n+\n+  \/\/ Return true if this age is at or above the tenuring threshold.\n+  bool is_tenurable(uint age) const {\n+    return age >= tenuring_threshold();\n+  }\n+\n+  \/\/ Update the local age table for worker_id by size for\n+  \/\/ given obj_age, region_age, and region_youth\n+  CENSUS_NOISE(void add(uint obj_age, uint region_age, uint region_youth, size_t size, uint worker_id);)\n+  NO_CENSUS_NOISE(void add(uint obj_age, uint region_age, size_t size, uint worker_id);)\n+\n+#ifdef SHENANDOAH_CENSUS_NOISE\n+  \/\/ Update the local skip table for worker_id by size\n+  void add_skipped(size_t size, uint worker_id);\n+  \/\/ Update the local aged region volume table for worker_id by size\n+  void add_aged(size_t size, uint worker_id);\n+  \/\/ Update the local clamped object volume table for worker_id by size\n+  void add_clamped(size_t size, uint worker_id);\n+  \/\/ Update the local (rejuvenated) object volume (retrograde age) for worker_id by size\n+  void add_young(size_t size, uint worker_id);\n+#endif \/\/ SHENANDOAH_CENSUS_NOISE\n+\n+  \/\/ Update the census data, and compute the new tenuring threshold.\n+  \/\/ This method should be called at the end of each marking (or optionally\n+  \/\/ evacuation) cycle to update the tenuring threshold to be used in\n+  \/\/ the next cycle.\n+  \/\/ age0_pop is the population of Cohort 0 that may have been missed in\n+  \/\/ the regular census during the marking cycle, corresponding to objects\n+  \/\/ allocated when the concurrent marking was in progress.\n+  \/\/ Optional parameters, pv1 and pv2 are population vectors that together\n+  \/\/ provide object census data (only) for the case when\n+  \/\/ ShenandoahGenerationalCensusAtEvac. In this case, the age0_pop\n+  \/\/ is 0, because the evacuated objects have all had their ages incremented.\n+  void update_census(size_t age0_pop, AgeTable* pv1 = nullptr, AgeTable* pv2 = nullptr);\n+\n+  \/\/ Reset the epoch, clearing accumulated census history\n+  \/\/ Note: this isn't currently used, but reserved for planned\n+  \/\/ future usage.\n+  void reset_global();\n+\n+  \/\/ Reset any (potentially partial) census information in worker-local age tables\n+  void reset_local();\n+\n+#ifndef PRODUCT\n+  \/\/ Check whether census information is clear\n+  bool is_clear_global();\n+  bool is_clear_local();\n+\n+  \/\/ Return the net size of objects encountered (counted or skipped) in census\n+  \/\/ at most recent epoch.\n+  size_t get_total() { return _total; }\n+#endif \/\/ !PRODUCT\n+\n+  \/\/ Print the age census information\n+  void print();\n+};\n+\n+#endif \/\/ SHARE_GC_SHENANDOAH_SHENANDOAHAGECENSUS_HPP\n","filename":"src\/hotspot\/share\/gc\/shenandoah\/shenandoahAgeCensus.hpp","additions":241,"deletions":0,"binary":false,"changes":241,"status":"added"},{"patch":"@@ -3,0 +3,1 @@\n+ * Copyright Amazon.com Inc. or its affiliates. All Rights Reserved.\n@@ -28,0 +29,1 @@\n+#include \"gc\/shenandoah\/shenandoahAffiliation.hpp\"\n@@ -34,1 +36,1 @@\n-    _alloc_shared_gc,   \/\/ Allocate common, outside of GCLAB\n+    _alloc_shared_gc,   \/\/ Allocate common, outside of GCLAB\/PLAB\n@@ -37,0 +39,1 @@\n+    _alloc_plab,        \/\/ Allocate PLAB\n@@ -50,0 +53,2 @@\n+      case _alloc_plab:\n+        return \"PLAB\";\n@@ -57,0 +62,1 @@\n+  \/\/ When ShenandoahElasticTLAB is enabled, the request cannot be made smaller than _min_size.\n@@ -58,0 +64,2 @@\n+\n+  \/\/ The size of the request in words.\n@@ -59,0 +67,2 @@\n+\n+  \/\/ The allocation may be increased for padding or decreased to fit in the remaining space of a region.\n@@ -60,0 +70,8 @@\n+\n+  \/\/ For a humongous object, the _waste is the amount of free memory in the last region.\n+  \/\/ For other requests, the _waste will be non-zero if the request enountered one or more regions\n+  \/\/ with less memory than _min_size. This waste does not contribute to the used memory for\n+  \/\/ the heap, but it does contribute to the allocation rate for heuristics.\n+  size_t _waste;\n+\n+  \/\/ This is the type of the request.\n@@ -61,0 +79,7 @@\n+\n+  \/\/ This is the generation which the request is targeting.\n+  ShenandoahAffiliation const _affiliation;\n+\n+  \/\/ True if this request is trying to copy any object from young to old (promote).\n+  bool _is_promotion;\n+\n@@ -62,0 +87,1 @@\n+  \/\/ Check that this is set before being read.\n@@ -65,1 +91,1 @@\n-  ShenandoahAllocRequest(size_t _min_size, size_t _requested_size, Type _alloc_type) :\n+  ShenandoahAllocRequest(size_t _min_size, size_t _requested_size, Type _alloc_type, ShenandoahAffiliation affiliation, bool is_promotion = false) :\n@@ -67,1 +93,1 @@\n-          _actual_size(0), _alloc_type(_alloc_type)\n+          _actual_size(0), _waste(0), _alloc_type(_alloc_type), _affiliation(affiliation), _is_promotion(is_promotion)\n@@ -75,1 +101,1 @@\n-    return ShenandoahAllocRequest(min_size, requested_size, _alloc_tlab);\n+    return ShenandoahAllocRequest(min_size, requested_size, _alloc_tlab, ShenandoahAffiliation::YOUNG_GENERATION);\n@@ -79,1 +105,5 @@\n-    return ShenandoahAllocRequest(min_size, requested_size, _alloc_gclab);\n+    return ShenandoahAllocRequest(min_size, requested_size, _alloc_gclab, ShenandoahAffiliation::YOUNG_GENERATION);\n+  }\n+\n+  static inline ShenandoahAllocRequest for_plab(size_t min_size, size_t requested_size) {\n+    return ShenandoahAllocRequest(min_size, requested_size, _alloc_plab, ShenandoahAffiliation::OLD_GENERATION);\n@@ -82,2 +112,6 @@\n-  static inline ShenandoahAllocRequest for_shared_gc(size_t requested_size) {\n-    return ShenandoahAllocRequest(0, requested_size, _alloc_shared_gc);\n+  static inline ShenandoahAllocRequest for_shared_gc(size_t requested_size, ShenandoahAffiliation affiliation, bool is_promotion = false) {\n+    if (is_promotion) {\n+      assert(affiliation == ShenandoahAffiliation::OLD_GENERATION, \"Should only promote to old generation\");\n+      return ShenandoahAllocRequest(0, requested_size, _alloc_shared_gc, affiliation, true);\n+    }\n+    return ShenandoahAllocRequest(0, requested_size, _alloc_shared_gc, affiliation);\n@@ -87,1 +121,1 @@\n-    return ShenandoahAllocRequest(0, requested_size, _alloc_shared);\n+    return ShenandoahAllocRequest(0, requested_size, _alloc_shared, ShenandoahAffiliation::YOUNG_GENERATION);\n@@ -90,1 +124,1 @@\n-  inline size_t size() {\n+  inline size_t size() const {\n@@ -94,1 +128,1 @@\n-  inline Type type() {\n+  inline Type type() const {\n@@ -98,1 +132,1 @@\n-  inline const char* type_string() {\n+  inline const char* type_string() const {\n@@ -102,1 +136,1 @@\n-  inline size_t min_size() {\n+  inline size_t min_size() const {\n@@ -107,1 +141,1 @@\n-  inline size_t actual_size() {\n+  inline size_t actual_size() const {\n@@ -120,1 +154,9 @@\n-  inline bool is_mutator_alloc() {\n+  inline size_t waste() const {\n+    return _waste;\n+  }\n+\n+  inline void set_waste(size_t v) {\n+    _waste = v;\n+  }\n+\n+  inline bool is_mutator_alloc() const {\n@@ -126,0 +168,1 @@\n+      case _alloc_plab:\n@@ -134,1 +177,1 @@\n-  inline bool is_gc_alloc() {\n+  inline bool is_gc_alloc() const {\n@@ -140,0 +183,1 @@\n+      case _alloc_plab:\n@@ -148,1 +192,1 @@\n-  inline bool is_lab_alloc() {\n+  inline bool is_lab_alloc() const {\n@@ -152,0 +196,1 @@\n+      case _alloc_plab:\n@@ -161,0 +206,20 @@\n+\n+  bool is_old() const {\n+    return _affiliation == OLD_GENERATION;\n+  }\n+\n+  bool is_young() const {\n+    return _affiliation == YOUNG_GENERATION;\n+  }\n+\n+  ShenandoahAffiliation affiliation() const {\n+    return _affiliation;\n+  }\n+\n+  const char* affiliation_name() const {\n+    return shenandoah_affiliation_name(_affiliation);\n+  }\n+\n+  bool is_promotion() const {\n+    return _is_promotion;\n+  }\n","filename":"src\/hotspot\/share\/gc\/shenandoah\/shenandoahAllocRequest.hpp","additions":81,"deletions":16,"binary":false,"changes":97,"status":"modified"},{"patch":"@@ -3,0 +3,1 @@\n+ * Copyright Amazon.com Inc. or its affiliates. All Rights Reserved.\n@@ -30,0 +31,1 @@\n+#include \"gc\/shenandoah\/shenandoahCardTable.hpp\"\n@@ -31,0 +33,1 @@\n+#include \"gc\/shenandoah\/shenandoahGenerationalHeap.hpp\"\n@@ -51,1 +54,0 @@\n-  FLAG_SET_DEFAULT(ShenandoahIUBarrier,              false);\n@@ -53,0 +55,1 @@\n+  FLAG_SET_DEFAULT(ShenandoahCardBarrier,            false);\n@@ -72,0 +75,7 @@\n+  \/\/ We use this as the time period for tracking minimum mutator utilization (MMU).\n+  \/\/ In generational mode, the MMU is used as a signal to adjust the size of the\n+  \/\/ young generation.\n+  if (FLAG_IS_DEFAULT(GCPauseIntervalMillis)) {\n+    FLAG_SET_DEFAULT(GCPauseIntervalMillis, 5000);\n+  }\n+\n@@ -116,0 +126,10 @@\n+  \/\/ Disable support for dynamic number of GC threads. We do not let the runtime\n+  \/\/ heuristics to misjudge how many threads we need during the heavy concurrent phase\n+  \/\/ or a GC pause.\n+  if (UseDynamicNumberOfGCThreads) {\n+    if (FLAG_IS_CMDLINE(UseDynamicNumberOfGCThreads)) {\n+      warning(\"Shenandoah does not support UseDynamicNumberOfGCThreads, disabling\");\n+    }\n+    FLAG_SET_DEFAULT(UseDynamicNumberOfGCThreads, false);\n+  }\n+\n@@ -135,1 +155,0 @@\n-           !FLAG_IS_DEFAULT(ShenandoahIUBarrier)              ||\n@@ -170,0 +189,5 @@\n+\n+  if (GCCardSizeInBytes < ShenandoahMinCardSizeInBytes) {\n+    vm_exit_during_initialization(\n+      err_msg(\"GCCardSizeInBytes ( %u ) must be >= %u\\n\", GCCardSizeInBytes, (unsigned int) ShenandoahMinCardSizeInBytes));\n+  }\n@@ -181,0 +205,2 @@\n+  CardTable::initialize_card_size();\n+\n@@ -194,1 +220,6 @@\n-  return new ShenandoahHeap(new ShenandoahCollectorPolicy());\n+  if (strcmp(ShenandoahGCMode, \"generational\") != 0) {\n+    \/\/ Not generational\n+    return new ShenandoahHeap(new ShenandoahCollectorPolicy());\n+  } else {\n+    return new ShenandoahGenerationalHeap(new ShenandoahCollectorPolicy());\n+  }\n","filename":"src\/hotspot\/share\/gc\/shenandoah\/shenandoahArguments.cpp","additions":34,"deletions":3,"binary":false,"changes":37,"status":"modified"},{"patch":"@@ -74,0 +74,3 @@\n+  if (heap->mode()->is_generational() && !obj->is_forwarded()) {\n+    msg.append(\"  age: %d\\n\", obj->age());\n+  }\n@@ -265,0 +268,19 @@\n+\n+  \/\/ Do additional checks for special objects: their fields can hold metadata as well.\n+  \/\/ We want to check class loading\/unloading did not corrupt them.\n+\n+  if (obj_klass == vmClasses::Class_klass()) {\n+    Metadata* klass = obj->metadata_field(java_lang_Class::klass_offset());\n+    if (klass != nullptr && !Metaspace::contains(klass)) {\n+      print_failure(_safe_all, obj, interior_loc, nullptr, \"Shenandoah assert_correct failed\",\n+                    \"Instance class mirror should point to Metaspace\",\n+                    file, line);\n+    }\n+\n+    Metadata* array_klass = obj->metadata_field(java_lang_Class::array_klass_offset());\n+    if (array_klass != nullptr && !Metaspace::contains(array_klass)) {\n+      print_failure(_safe_all, obj, interior_loc, nullptr, \"Shenandoah assert_correct failed\",\n+                    \"Array class mirror should point to Metaspace\",\n+                    file, line);\n+    }\n+  }\n@@ -279,1 +301,1 @@\n-  if (alloc_size > ShenandoahHeapRegion::humongous_threshold_words()) {\n+  if (ShenandoahHeapRegion::requires_humongous(alloc_size)) {\n@@ -400,1 +422,1 @@\n-  ShenandoahMessageBuffer msg(\"Must ba at a Shenandoah safepoint or held %s lock\", lock->name());\n+  ShenandoahMessageBuffer msg(\"Must be at a Shenandoah safepoint or held %s lock\", lock->name());\n@@ -433,1 +455,1 @@\n-  if (ShenandoahSafepoint::is_at_shenandoah_safepoint() && Thread::current()->is_VM_thread()) {\n+  if (ShenandoahSafepoint::is_at_shenandoah_safepoint()) {\n@@ -440,0 +462,45 @@\n+\n+void ShenandoahAsserts::assert_generational(const char* file, int line) {\n+  if (ShenandoahHeap::heap()->mode()->is_generational()) {\n+    return;\n+  }\n+\n+  ShenandoahMessageBuffer msg(\"Must be in generational mode\");\n+  report_vm_error(file, line, msg.buffer());\n+}\n+\n+void ShenandoahAsserts::assert_control_or_vm_thread_at_safepoint(bool at_safepoint, const char* file, int line) {\n+  Thread* thr = Thread::current();\n+  if (thr == ShenandoahHeap::heap()->control_thread()) {\n+    return;\n+  }\n+  if (thr->is_VM_thread()) {\n+    if (!at_safepoint) {\n+      return;\n+    } else if (SafepointSynchronize::is_at_safepoint()) {\n+      return;\n+    }\n+  }\n+\n+  ShenandoahMessageBuffer msg(\"Must be either control thread, or vm thread\");\n+  if (at_safepoint) {\n+    msg.append(\" at a safepoint\");\n+  }\n+  report_vm_error(file, line, msg.buffer());\n+}\n+\n+void ShenandoahAsserts::assert_generations_reconciled(const char* file, int line) {\n+  if (!SafepointSynchronize::is_at_safepoint()) {\n+    return;\n+  }\n+\n+  ShenandoahHeap* heap = ShenandoahHeap::heap();\n+  ShenandoahGeneration* ggen = heap->gc_generation();\n+  ShenandoahGeneration* agen = heap->active_generation();\n+  if (agen == ggen) {\n+    return;\n+  }\n+\n+  ShenandoahMessageBuffer msg(\"Active(%d) & GC(%d) Generations aren't reconciled\", agen->type(), ggen->type());\n+  report_vm_error(file, line, msg.buffer());\n+}\n","filename":"src\/hotspot\/share\/gc\/shenandoah\/shenandoahAsserts.cpp","additions":70,"deletions":3,"binary":false,"changes":73,"status":"modified"},{"patch":"@@ -3,0 +3,1 @@\n+ * Copyright Amazon.com Inc. or its affiliates. All Rights Reserved.\n@@ -75,0 +76,3 @@\n+  static void assert_control_or_vm_thread_at_safepoint(bool at_safepoint, const char* file, int line);\n+  static void assert_generational(const char* file, int line);\n+  static void assert_generations_reconciled(const char* file, int line);\n@@ -166,0 +170,15 @@\n+\n+#define shenandoah_assert_control_or_vm_thread() \\\n+                    ShenandoahAsserts::assert_control_or_vm_thread(false \/* at_safepoint *\/, __FILE__, __LINE__)\n+\n+\/\/ A stronger version of the above that checks that we are at a safepoint if the vm thread\n+#define shenandoah_assert_control_or_vm_thread_at_safepoint()                                                                                                               \\\n+                    ShenandoahAsserts::assert_control_or_vm_thread_at_safepoint(true \/* at_safepoint *\/, __FILE__, __LINE__)\n+\n+#define shenandoah_assert_generational() \\\n+                    ShenandoahAsserts::assert_generational(__FILE__, __LINE__)\n+\n+\/\/ Some limited sanity checking of the _gc_generation and _active_generation fields of ShenandoahHeap\n+#define shenandoah_assert_generations_reconciled()                                                             \\\n+                    ShenandoahAsserts::assert_generations_reconciled(__FILE__, __LINE__)\n+\n@@ -216,0 +235,4 @@\n+#define shenandoah_assert_control_or_vm_thread()\n+#define shenandoah_assert_control_or_vm_thread_at_safepoint()\n+#define shenandoah_assert_generational()\n+#define shenandoah_assert_generations_reconciled()\n","filename":"src\/hotspot\/share\/gc\/shenandoah\/shenandoahAsserts.hpp","additions":23,"deletions":0,"binary":false,"changes":23,"status":"modified"},{"patch":"@@ -3,0 +3,1 @@\n+ * Copyright Amazon.com Inc. or its affiliates. All Rights Reserved.\n@@ -31,0 +32,1 @@\n+#include \"gc\/shenandoah\/shenandoahCardTable.hpp\"\n@@ -33,0 +35,1 @@\n+#include \"gc\/shenandoah\/shenandoahScanRemembered.inline.hpp\"\n@@ -44,1 +47,1 @@\n-ShenandoahBarrierSet::ShenandoahBarrierSet(ShenandoahHeap* heap) :\n+ShenandoahBarrierSet::ShenandoahBarrierSet(ShenandoahHeap* heap, MemRegion heap_region) :\n@@ -52,0 +55,1 @@\n+  _card_table(nullptr),\n@@ -55,0 +59,4 @@\n+  if (ShenandoahCardBarrier) {\n+    _card_table = new ShenandoahCardTable(heap_region);\n+    _card_table->initialize();\n+  }\n@@ -101,0 +109,8 @@\n+\n+  if (ShenandoahCardBarrier) {\n+    \/\/ Every thread always have a pointer to the _current_ _write_ version of the card table.\n+    \/\/ The JIT'ed code will use this address (+card entry offset) to mark the card as dirty.\n+    ShenandoahThreadLocalData::set_card_table(thread, _card_table->write_byte_map_base());\n+  }\n+  ShenandoahThreadLocalData::set_gc_state(thread, _heap->gc_state());\n+\n@@ -102,1 +118,0 @@\n-    ShenandoahThreadLocalData::set_gc_state(thread, _heap->gc_state());\n@@ -127,0 +142,6 @@\n+    PLAB* plab = ShenandoahThreadLocalData::plab(thread);\n+    if (plab != nullptr) {\n+      \/\/ This will assert if plab is not null in non-generational mode\n+      ShenandoahGenerationalHeap::heap()->retire_plab(plab);\n+    }\n+\n@@ -141,1 +162,1 @@\n-  if (_heap->has_forwarded_objects() || (ShenandoahIUBarrier && _heap->is_concurrent_mark_in_progress())) {\n+  if (_heap->has_forwarded_objects()) {\n@@ -145,0 +166,19 @@\n+\n+void ShenandoahBarrierSet::write_ref_array(HeapWord* start, size_t count) {\n+  assert(ShenandoahCardBarrier, \"Should have been checked by caller\");\n+\n+  HeapWord* end = (HeapWord*)((char*) start + (count * heapOopSize));\n+  \/\/ In the case of compressed oops, start and end may potentially be misaligned;\n+  \/\/ so we need to conservatively align the first downward (this is not\n+  \/\/ strictly necessary for current uses, but a case of good hygiene and,\n+  \/\/ if you will, aesthetics) and the second upward (this is essential for\n+  \/\/ current uses) to a HeapWord boundary, so we mark all cards overlapping\n+  \/\/ this write.\n+  HeapWord* aligned_start = align_down(start, HeapWordSize);\n+  HeapWord* aligned_end   = align_up  (end,   HeapWordSize);\n+  \/\/ If compressed oops were not being used, these should already be aligned\n+  assert(UseCompressedOops || (aligned_start == start && aligned_end == end),\n+         \"Expected heap word alignment of start and end\");\n+  _heap->old_generation()->card_scan()->mark_range_as_dirty(aligned_start, (aligned_end - aligned_start));\n+}\n+\n","filename":"src\/hotspot\/share\/gc\/shenandoah\/shenandoahBarrierSet.cpp","additions":43,"deletions":3,"binary":false,"changes":46,"status":"modified"},{"patch":"@@ -3,0 +3,1 @@\n+ * Copyright Amazon.com Inc. or its affiliates. All Rights Reserved.\n@@ -34,0 +35,1 @@\n+class ShenandoahCardTable;\n@@ -38,0 +40,1 @@\n+  ShenandoahCardTable* _card_table;\n@@ -42,1 +45,1 @@\n-  ShenandoahBarrierSet(ShenandoahHeap* heap);\n+  ShenandoahBarrierSet(ShenandoahHeap* heap, MemRegion heap_region);\n@@ -50,0 +53,4 @@\n+  inline ShenandoahCardTable* card_table() {\n+    return _card_table;\n+  }\n+\n@@ -92,1 +99,0 @@\n-  inline void iu_barrier(oop obj);\n@@ -115,0 +121,5 @@\n+  template <DecoratorSet decorators, typename T>\n+  void write_ref_field_post(T* field);\n+\n+  void write_ref_array(HeapWord* start, size_t count);\n+\n@@ -117,1 +128,1 @@\n-  inline void arraycopy_marking(T* src, T* dst, size_t count);\n+  inline void arraycopy_marking(T* src, T* dst, size_t count, bool is_old_marking);\n@@ -123,1 +134,0 @@\n-  inline void clone_marking(oop src);\n","filename":"src\/hotspot\/share\/gc\/shenandoah\/shenandoahBarrierSet.hpp","additions":14,"deletions":4,"binary":false,"changes":18,"status":"modified"},{"patch":"@@ -3,0 +3,1 @@\n+ * Copyright Amazon.com Inc. or its affiliates. All Rights Reserved.\n@@ -31,0 +32,1 @@\n+#include \"gc\/shared\/cardTable.hpp\"\n@@ -32,0 +34,1 @@\n+#include \"gc\/shenandoah\/shenandoahCardTable.hpp\"\n@@ -35,0 +38,1 @@\n+#include \"gc\/shenandoah\/shenandoahGeneration.hpp\"\n@@ -39,0 +43,2 @@\n+#include \"gc\/shenandoah\/mode\/shenandoahMode.hpp\"\n+#include \"memory\/iterator.inline.hpp\"\n@@ -64,2 +70,1 @@\n-    assert(_heap->is_evacuation_in_progress(),\n-           \"evac should be in progress\");\n+    assert(_heap->is_evacuation_in_progress(), \"evac should be in progress\");\n@@ -83,2 +88,2 @@\n-  if (_heap->has_forwarded_objects() &&\n-      _heap->in_collection_set(obj)) { \/\/ Subsumes null-check\n+  if (_heap->has_forwarded_objects() && _heap->in_collection_set(obj)) {\n+    \/\/ Subsumes null-check\n@@ -106,0 +111,1 @@\n+      _heap->is_in_active_generation(obj) &&\n@@ -113,0 +119,1 @@\n+      _heap->is_in_active_generation(obj) &&\n@@ -117,2 +124,1 @@\n-  \/\/ Prevent resurrection of unreachable objects that are visited during\n-  \/\/ concurrent class-unloading.\n+  \/\/ Allow runtime to see unreachable objects that are visited during concurrent class-unloading.\n@@ -120,1 +126,1 @@\n-      _heap->is_evacuation_in_progress() &&\n+      _heap->is_concurrent_weak_root_in_progress() &&\n@@ -149,0 +155,1 @@\n+  \/\/ Uninitialized and no-keepalive stores do not need barrier.\n@@ -153,0 +160,8 @@\n+\n+  \/\/ Stores to weak\/phantom require no barrier. The original references would\n+  \/\/ have been enqueued in the SATB buffer by the load barrier if they were needed.\n+  if (HasDecorator<decorators, ON_WEAK_OOP_REF>::value ||\n+      HasDecorator<decorators, ON_PHANTOM_OOP_REF>::value) {\n+    return;\n+  }\n+\n@@ -167,6 +182,0 @@\n-inline void ShenandoahBarrierSet::iu_barrier(oop obj) {\n-  if (ShenandoahIUBarrier && obj != nullptr && _heap->is_concurrent_mark_in_progress()) {\n-    enqueue(obj);\n-  }\n-}\n-\n@@ -182,0 +191,7 @@\n+template <DecoratorSet decorators, typename T>\n+inline void ShenandoahBarrierSet::write_ref_field_post(T* field) {\n+  assert(ShenandoahCardBarrier, \"Should have been checked by caller\");\n+  volatile CardTable::CardValue* byte = card_table()->byte_for(field);\n+  *byte = CardTable::dirty_card_val();\n+}\n+\n@@ -192,1 +208,0 @@\n-  iu_barrier(new_value);\n@@ -210,1 +225,0 @@\n-  iu_barrier(new_value);\n@@ -245,1 +259,4 @@\n-  shenandoah_assert_marked_if(nullptr, value, !CompressedOops::is_null(value) && ShenandoahHeap::heap()->is_evacuation_in_progress());\n+  shenandoah_assert_marked_if(nullptr, value,\n+                              !CompressedOops::is_null(value) && ShenandoahHeap::heap()->is_evacuation_in_progress()\n+                              && !(ShenandoahHeap::heap()->active_generation()->is_young()\n+                                   && ShenandoahHeap::heap()->heap_region_containing(value)->is_old()));\n@@ -248,1 +265,0 @@\n-  bs->iu_barrier(value);\n@@ -256,0 +272,1 @@\n+  assert((decorators & ON_UNKNOWN_OOP_REF) == 0, \"Reference strength must be known\");\n@@ -266,0 +283,4 @@\n+  if (ShenandoahCardBarrier) {\n+    ShenandoahBarrierSet* bs = ShenandoahBarrierSet::barrier_set();\n+    bs->write_ref_field_post<decorators>(addr);\n+  }\n@@ -286,1 +307,5 @@\n-  return bs->oop_cmpxchg(decorators, addr, compare_value, new_value);\n+  oop result = bs->oop_cmpxchg(decorators, addr, compare_value, new_value);\n+  if (ShenandoahCardBarrier) {\n+    bs->write_ref_field_post<decorators>(addr);\n+  }\n+  return result;\n@@ -294,1 +319,6 @@\n-  return bs->oop_cmpxchg(resolved_decorators, AccessInternal::oop_field_addr<decorators>(base, offset), compare_value, new_value);\n+  auto addr = AccessInternal::oop_field_addr<decorators>(base, offset);\n+  oop result = bs->oop_cmpxchg(resolved_decorators, addr, compare_value, new_value);\n+  if (ShenandoahCardBarrier) {\n+    bs->write_ref_field_post<decorators>(addr);\n+  }\n+  return result;\n@@ -310,1 +340,5 @@\n-  return bs->oop_xchg(decorators, addr, new_value);\n+  oop result = bs->oop_xchg(decorators, addr, new_value);\n+  if (ShenandoahCardBarrier) {\n+    bs->write_ref_field_post<decorators>(addr);\n+  }\n+  return result;\n@@ -318,1 +352,6 @@\n-  return bs->oop_xchg(resolved_decorators, AccessInternal::oop_field_addr<decorators>(base, offset), new_value);\n+  auto addr = AccessInternal::oop_field_addr<decorators>(base, offset);\n+  oop result = bs->oop_xchg(resolved_decorators, addr, new_value);\n+  if (ShenandoahCardBarrier) {\n+    bs->write_ref_field_post<decorators>(addr);\n+  }\n+  return result;\n@@ -335,0 +374,3 @@\n+  T* src = arrayOopDesc::obj_offset_to_raw(src_obj, src_offset_in_bytes, src_raw);\n+  T* dst = arrayOopDesc::obj_offset_to_raw(dst_obj, dst_offset_in_bytes, dst_raw);\n+\n@@ -336,4 +378,6 @@\n-  bs->arraycopy_barrier(arrayOopDesc::obj_offset_to_raw(src_obj, src_offset_in_bytes, src_raw),\n-                        arrayOopDesc::obj_offset_to_raw(dst_obj, dst_offset_in_bytes, dst_raw),\n-                        length);\n-  return Raw::oop_arraycopy_in_heap(src_obj, src_offset_in_bytes, src_raw, dst_obj, dst_offset_in_bytes, dst_raw, length);\n+  bs->arraycopy_barrier(src, dst, length);\n+  bool result = Raw::oop_arraycopy_in_heap(src_obj, src_offset_in_bytes, src_raw, dst_obj, dst_offset_in_bytes, dst_raw, length);\n+  if (ShenandoahCardBarrier) {\n+    bs->write_ref_array((HeapWord*) dst, length);\n+  }\n+  return result;\n@@ -344,1 +388,9 @@\n-  assert(HAS_FWD == _heap->has_forwarded_objects(), \"Forwarded object status is sane\");\n+  \/\/ Young cycles are allowed to run when old marking is in progress. When old marking is in progress,\n+  \/\/ this barrier will be called with ENQUEUE=true and HAS_FWD=false, even though the young generation\n+  \/\/ may have forwarded objects. In this case, the `arraycopy_work` is first called with HAS_FWD=true and\n+  \/\/ ENQUEUE=false.\n+  assert(HAS_FWD == _heap->has_forwarded_objects() || _heap->is_concurrent_old_mark_in_progress(),\n+         \"Forwarded object status is sane\");\n+  \/\/ This function cannot be called to handle marking and evacuation at the same time (they operate on\n+  \/\/ different sides of the copy).\n+  assert((HAS_FWD || EVAC) != ENQUEUE, \"Cannot evacuate and mark both sides of copy.\");\n@@ -360,1 +412,1 @@\n-        assert(obj != fwd || _heap->cancelled_gc(), \"must be forwarded\");\n+        shenandoah_assert_forwarded_except(elem_ptr, obj, _heap->cancelled_gc());\n@@ -362,1 +414,0 @@\n-        obj = fwd;\n@@ -364,1 +415,1 @@\n-      if (ENQUEUE && !ctx->is_marked_strong(obj)) {\n+      if (ENQUEUE && !ctx->is_marked_strong_or_old(obj)) {\n@@ -374,0 +425,1 @@\n+    \/\/ No elements to copy, no need for barrier\n@@ -376,4 +428,3 @@\n-  int gc_state = _heap->gc_state();\n-  if ((gc_state & ShenandoahHeap::MARKING) != 0) {\n-    arraycopy_marking(src, dst, count);\n-  } else if ((gc_state & ShenandoahHeap::EVACUATION) != 0) {\n+\n+  char gc_state = ShenandoahThreadLocalData::gc_state(Thread::current());\n+  if ((gc_state & ShenandoahHeap::EVACUATION) != 0) {\n@@ -381,1 +432,1 @@\n-  } else if ((gc_state & ShenandoahHeap::UPDATEREFS) != 0) {\n+  } else if ((gc_state & ShenandoahHeap::UPDATE_REFS) != 0) {\n@@ -384,0 +435,12 @@\n+\n+  if (_heap->mode()->is_generational()) {\n+    assert(ShenandoahSATBBarrier, \"Generational mode assumes SATB mode\");\n+    if ((gc_state & ShenandoahHeap::YOUNG_MARKING) != 0) {\n+      arraycopy_marking(src, dst, count, false);\n+    }\n+    if ((gc_state & ShenandoahHeap::OLD_MARKING) != 0) {\n+      arraycopy_marking(src, dst, count, true);\n+    }\n+  } else if ((gc_state & ShenandoahHeap::MARKING) != 0) {\n+    arraycopy_marking(src, dst, count, false);\n+  }\n@@ -387,1 +450,1 @@\n-void ShenandoahBarrierSet::arraycopy_marking(T* src, T* dst, size_t count) {\n+void ShenandoahBarrierSet::arraycopy_marking(T* src, T* dst, size_t count, bool is_old_marking) {\n@@ -389,3 +452,40 @@\n-  T* array = ShenandoahSATBBarrier ? dst : src;\n-  if (!_heap->marking_context()->allocated_after_mark_start(reinterpret_cast<HeapWord*>(array))) {\n-    arraycopy_work<T, false, false, true>(array, count);\n+  \/*\n+   * Note that an old-gen object is considered live if it is live at the start of OLD marking or if it is promoted\n+   * following the start of OLD marking.\n+   *\n+   * 1. Every object promoted following the start of OLD marking will be above TAMS within its old-gen region\n+   * 2. Every object live at the start of OLD marking will be referenced from a \"root\" or it will be referenced from\n+   *    another live OLD-gen object.  With regards to old-gen, roots include stack locations and all of live young-gen.\n+   *    All root references to old-gen are identified during a bootstrap young collection.  All references from other\n+   *    old-gen objects will be marked during the traversal of all old objects, or will be marked by the SATB barrier.\n+   *\n+   * During old-gen marking (which is interleaved with young-gen collections), call arraycopy_work() if:\n+   *\n+   * 1. The overwritten array resides in old-gen and it is below TAMS within its old-gen region\n+   * 2. Do not call arraycopy_work for any array residing in young-gen because young-gen collection is idle at this time\n+   *\n+   * During young-gen marking, call arraycopy_work() if:\n+   *\n+   * 1. The overwritten array resides in young-gen and is below TAMS within its young-gen region\n+   * 2. Additionally, if array resides in old-gen, regardless of its relationship to TAMS because this old-gen array\n+   *    may hold references to young-gen\n+   *\/\n+  if (ShenandoahSATBBarrier) {\n+    T* array = dst;\n+    HeapWord* array_addr = reinterpret_cast<HeapWord*>(array);\n+    ShenandoahHeapRegion* r = _heap->heap_region_containing(array_addr);\n+    if (is_old_marking) {\n+      \/\/ Generational, old marking\n+      assert(_heap->mode()->is_generational(), \"Invariant\");\n+      if (r->is_old() && (array_addr < _heap->marking_context()->top_at_mark_start(r))) {\n+        arraycopy_work<T, false, false, true>(array, count);\n+      }\n+    } else if (_heap->mode()->is_generational()) {\n+      \/\/ Generational, young marking\n+      if (r->is_old() || (array_addr < _heap->marking_context()->top_at_mark_start(r))) {\n+        arraycopy_work<T, false, false, true>(array, count);\n+      }\n+    } else if (array_addr < _heap->marking_context()->top_at_mark_start(r)) {\n+      \/\/ Non-generational, marking\n+      arraycopy_work<T, false, false, true>(array, count);\n+    }\n","filename":"src\/hotspot\/share\/gc\/shenandoah\/shenandoahBarrierSet.inline.hpp","additions":138,"deletions":38,"binary":false,"changes":176,"status":"modified"},{"patch":"@@ -56,1 +56,1 @@\n-        assert(obj != fwd || _heap->cancelled_gc(), \"must be forwarded\");\n+        shenandoah_assert_forwarded_except(p, obj, _heap->cancelled_gc());\n@@ -77,9 +77,0 @@\n-void ShenandoahBarrierSet::clone_marking(oop obj) {\n-  assert(_heap->is_concurrent_mark_in_progress(), \"only during marking\");\n-  assert(ShenandoahIUBarrier, \"only with incremental-update\");\n-  if (!_heap->marking_context()->allocated_after_mark_start(obj)) {\n-    ShenandoahUpdateRefsForOopClosure<\/* has_fwd = *\/ false, \/* evac = *\/ false, \/* enqueue *\/ true> cl;\n-    obj->oop_iterate(&cl);\n-  }\n-}\n-\n@@ -107,4 +98,1 @@\n-  int gc_state = _heap->gc_state();\n-  if ((gc_state & ShenandoahHeap::MARKING) != 0) {\n-    clone_marking(obj);\n-  } else if ((gc_state & ShenandoahHeap::EVACUATION) != 0) {\n+  if (_heap->is_evacuation_in_progress()) {\n","filename":"src\/hotspot\/share\/gc\/shenandoah\/shenandoahBarrierSetClone.inline.hpp","additions":2,"deletions":14,"binary":false,"changes":16,"status":"modified"},{"patch":"@@ -0,0 +1,43 @@\n+\/*\n+ * Copyright Amazon.com Inc. or its affiliates. All Rights Reserved. *\n+ * DO NOT ALTER OR REMOVE COPYRIGHT NOTICES OR THIS FILE HEADER.\n+ *\n+ * This code is free software; you can redistribute it and\/or modify it\n+ * under the terms of the GNU General Public License version 2 only, as\n+ * published by the Free Software Foundation.\n+ *\n+ * This code is distributed in the hope that it will be useful, but WITHOUT\n+ * ANY WARRANTY; without even the implied warranty of MERCHANTABILITY or\n+ * FITNESS FOR A PARTICULAR PURPOSE.  See the GNU General Public License\n+ * version 2 for more details (a copy is included in the LICENSE file that\n+ * accompanied this code).\n+ *\n+ * You should have received a copy of the GNU General Public License version\n+ * 2 along with this work; if not, write to the Free Software Foundation,\n+ * Inc., 51 Franklin St, Fifth Floor, Boston, MA 02110-1301 USA.\n+ *\n+ * Please contact Oracle, 500 Oracle Parkway, Redwood Shores, CA 94065 USA\n+ * or visit www.oracle.com if you need additional information or have any\n+ * questions.\n+ *\n+ *\/\n+\n+\n+#include \"precompiled.hpp\"\n+\n+#include \"gc\/shenandoah\/shenandoahCardStats.hpp\"\n+#include \"logging\/log.hpp\"\n+\n+#ifndef PRODUCT\n+void ShenandoahCardStats::log() const {\n+  if (ShenandoahEnableCardStats) {\n+    log_info(gc,remset)(\"Card stats: dirty \" SIZE_FORMAT \" (max run: \" SIZE_FORMAT \"),\"\n+      \" clean \" SIZE_FORMAT \" (max run: \" SIZE_FORMAT \"),\"\n+      \" dirty scans\/objs \" SIZE_FORMAT,\n+      _dirty_card_cnt, _max_dirty_run,\n+      _clean_card_cnt, _max_clean_run,\n+      _dirty_scan_obj_cnt);\n+  }\n+}\n+#endif \/\/ !PRODUCT\n+\n","filename":"src\/hotspot\/share\/gc\/shenandoah\/shenandoahCardStats.cpp","additions":43,"deletions":0,"binary":false,"changes":43,"status":"added"},{"patch":"@@ -0,0 +1,132 @@\n+\/*\n+ * Copyright Amazon.com Inc. or its affiliates. All Rights Reserved.\n+ * DO NOT ALTER OR REMOVE COPYRIGHT NOTICES OR THIS FILE HEADER.\n+ *\n+ * This code is free software; you can redistribute it and\/or modify it\n+ * under the terms of the GNU General Public License version 2 only, as\n+ * published by the Free Software Foundation.\n+ *\n+ * This code is distributed in the hope that it will be useful, but WITHOUT\n+ * ANY WARRANTY; without even the implied warranty of MERCHANTABILITY or\n+ * FITNESS FOR A PARTICULAR PURPOSE.  See the GNU General Public License\n+ * version 2 for more details (a copy is included in the LICENSE file that\n+ * accompanied this code).\n+ *\n+ * You should have received a copy of the GNU General Public License version\n+ * 2 along with this work; if not, write to the Free Software Foundation,\n+ * Inc., 51 Franklin St, Fifth Floor, Boston, MA 02110-1301 USA.\n+ *\n+ * Please contact Oracle, 500 Oracle Parkway, Redwood Shores, CA 94065 USA\n+ * or visit www.oracle.com if you need additional information or have any\n+ * questions.\n+ *\n+ *\/\n+\n+#ifndef SHARE_GC_SHENANDOAH_SHENANDOAHCARDSTATS_HPP\n+#define SHARE_GC_SHENANDOAH_SHENANDOAHCARDSTATS_HPP\n+\n+#include \"gc\/shared\/gc_globals.hpp\"\n+#include \"gc\/shenandoah\/shenandoahNumberSeq.hpp\"\n+\n+enum CardStatType {\n+  DIRTY_RUN,\n+  CLEAN_RUN,\n+  DIRTY_CARDS,\n+  CLEAN_CARDS,\n+  MAX_DIRTY_RUN,\n+  MAX_CLEAN_RUN,\n+  DIRTY_SCAN_OBJS,\n+  ALTERNATIONS,\n+  MAX_CARD_STAT_TYPE\n+};\n+\n+enum CardStatLogType {\n+  CARD_STAT_SCAN_RS,\n+  CARD_STAT_UPDATE_REFS,\n+  MAX_CARD_STAT_LOG_TYPE\n+};\n+\n+class ShenandoahCardStats: public CHeapObj<mtGC> {\n+private:\n+  size_t _cards_in_cluster;\n+  HdrSeq* _local_card_stats;\n+\n+  size_t _dirty_card_cnt;\n+  size_t _clean_card_cnt;\n+\n+  size_t _max_dirty_run;\n+  size_t _max_clean_run;\n+\n+  size_t _dirty_scan_obj_cnt;\n+\n+  size_t _alternation_cnt;\n+\n+public:\n+  ShenandoahCardStats(size_t cards_in_cluster, HdrSeq* card_stats) :\n+    _cards_in_cluster(cards_in_cluster),\n+    _local_card_stats(card_stats),\n+    _dirty_card_cnt(0),\n+    _clean_card_cnt(0),\n+    _max_dirty_run(0),\n+    _max_clean_run(0),\n+    _dirty_scan_obj_cnt(0),\n+    _alternation_cnt(0)\n+  { }\n+\n+  ~ShenandoahCardStats() {\n+    record();\n+   }\n+\n+   void record() {\n+    if (ShenandoahEnableCardStats) {\n+      \/\/ Update global stats for distribution of dirty\/clean cards as a percentage of chunk\n+      _local_card_stats[DIRTY_CARDS].add(percent_of(_dirty_card_cnt, _cards_in_cluster));\n+      _local_card_stats[CLEAN_CARDS].add(percent_of(_clean_card_cnt, _cards_in_cluster));\n+\n+      \/\/ Update global stats for max dirty\/clean run distribution as a percentage of chunk\n+      _local_card_stats[MAX_DIRTY_RUN].add(percent_of(_max_dirty_run, _cards_in_cluster));\n+      _local_card_stats[MAX_CLEAN_RUN].add(percent_of(_max_clean_run, _cards_in_cluster));\n+\n+      \/\/ Update global stats for dirty obj scan counts\n+      _local_card_stats[DIRTY_SCAN_OBJS].add(_dirty_scan_obj_cnt);\n+\n+      \/\/ Update global stats for alternation counts\n+      _local_card_stats[ALTERNATIONS].add(_alternation_cnt);\n+    }\n+  }\n+\n+public:\n+  inline void record_dirty_run(size_t len) {\n+    if (ShenandoahEnableCardStats) {\n+      _alternation_cnt++;\n+      if (len > _max_dirty_run) {\n+        _max_dirty_run = len;\n+      }\n+      _dirty_card_cnt += len;\n+      assert(len <= _cards_in_cluster, \"Error\");\n+      _local_card_stats[DIRTY_RUN].add(percent_of(len, _cards_in_cluster));\n+    }\n+  }\n+\n+  inline void record_clean_run(size_t len) {\n+    if (ShenandoahEnableCardStats) {\n+      _alternation_cnt++;\n+      if (len > _max_clean_run) {\n+        _max_clean_run = len;\n+      }\n+      _clean_card_cnt += len;\n+      assert(len <= _cards_in_cluster, \"Error\");\n+      _local_card_stats[CLEAN_RUN].add(percent_of(len, _cards_in_cluster));\n+    }\n+  }\n+\n+  inline void record_scan_obj_cnt(size_t i) {\n+    if (ShenandoahEnableCardStats) {\n+      _dirty_scan_obj_cnt += i;\n+    }\n+  }\n+\n+  void log() const PRODUCT_RETURN;\n+};\n+\n+#endif \/\/ SHARE_GC_SHENANDOAH_SHENANDOAHCARDSTATS_HPP\n","filename":"src\/hotspot\/share\/gc\/shenandoah\/shenandoahCardStats.hpp","additions":132,"deletions":0,"binary":false,"changes":132,"status":"added"},{"patch":"@@ -0,0 +1,105 @@\n+\/*\n+ * Copyright Amazon.com Inc. or its affiliates. All Rights Reserved.\n+ * DO NOT ALTER OR REMOVE COPYRIGHT NOTICES OR THIS FILE HEADER.\n+ *\n+ * This code is free software; you can redistribute it and\/or modify it\n+ * under the terms of the GNU General Public License version 2 only, as\n+ * published by the Free Software Foundation.\n+ *\n+ * This code is distributed in the hope that it will be useful, but WITHOUT\n+ * ANY WARRANTY; without even the implied warranty of MERCHANTABILITY or\n+ * FITNESS FOR A PARTICULAR PURPOSE.  See the GNU General Public License\n+ * version 2 for more details (a copy is included in the LICENSE file that\n+ * accompanied this code).\n+ *\n+ * You should have received a copy of the GNU General Public License version\n+ * 2 along with this work; if not, write to the Free Software Foundation,\n+ * Inc., 51 Franklin St, Fifth Floor, Boston, MA 02110-1301 USA.\n+ *\n+ * Please contact Oracle, 500 Oracle Parkway, Redwood Shores, CA 94065 USA\n+ * or visit www.oracle.com if you need additional information or have any\n+ * questions.\n+ *\n+ *\/\n+\n+#include \"precompiled.hpp\"\n+#include \"gc\/shenandoah\/shenandoahCardTable.hpp\"\n+#include \"gc\/shenandoah\/shenandoahHeap.inline.hpp\"\n+#include \"gc\/shenandoah\/shenandoahUtils.hpp\"\n+#include \"runtime\/init.hpp\"\n+#include \"services\/memTracker.hpp\"\n+\n+void ShenandoahCardTable::initialize() {\n+  size_t num_cards = cards_required(_whole_heap.word_size());\n+\n+  \/\/ each card takes 1 byte; + 1 for the guard card\n+  size_t num_bytes = num_cards + 1;\n+  const size_t granularity = os::vm_allocation_granularity();\n+  _byte_map_size = align_up(num_bytes, MAX2(_page_size, granularity));\n+\n+  HeapWord* low_bound  = _whole_heap.start();\n+  HeapWord* high_bound = _whole_heap.end();\n+\n+  \/\/ ReservedSpace constructor would assert rs_align >= os::vm_page_size().\n+  const size_t rs_align = _page_size == os::vm_page_size() ? 0 : MAX2(_page_size, granularity);\n+\n+  ReservedSpace write_space(_byte_map_size, rs_align, _page_size);\n+  initialize(write_space);\n+\n+  \/\/ The assembler store_check code will do an unsigned shift of the oop,\n+  \/\/ then add it to _byte_map_base, i.e.\n+  \/\/\n+  \/\/   _byte_map = _byte_map_base + (uintptr_t(low_bound) >> card_shift)\n+  _byte_map = (CardValue*) write_space.base();\n+  _byte_map_base = _byte_map - (uintptr_t(low_bound) >> _card_shift);\n+  assert(byte_for(low_bound) == &_byte_map[0], \"Checking start of map\");\n+  assert(byte_for(high_bound-1) <= &_byte_map[last_valid_index()], \"Checking end of map\");\n+\n+  _write_byte_map = _byte_map;\n+  _write_byte_map_base = _byte_map_base;\n+\n+  ReservedSpace read_space(_byte_map_size, rs_align, _page_size);\n+  initialize(read_space);\n+\n+  _read_byte_map = (CardValue*) read_space.base();\n+  _read_byte_map_base = _read_byte_map - (uintptr_t(low_bound) >> card_shift());\n+  assert(read_byte_for(low_bound) == &_read_byte_map[0], \"Checking start of map\");\n+  assert(read_byte_for(high_bound-1) <= &_read_byte_map[last_valid_index()], \"Checking end of map\");\n+\n+  _covered[0] = _whole_heap;\n+\n+  log_trace(gc, barrier)(\"ShenandoahCardTable::ShenandoahCardTable:\");\n+  log_trace(gc, barrier)(\"    &_write_byte_map[0]: \" INTPTR_FORMAT \"  &_write_byte_map[_last_valid_index]: \" INTPTR_FORMAT,\n+                         p2i(&_write_byte_map[0]), p2i(&_write_byte_map[last_valid_index()]));\n+  log_trace(gc, barrier)(\"    _write_byte_map_base: \" INTPTR_FORMAT, p2i(_write_byte_map_base));\n+  log_trace(gc, barrier)(\"    &_read_byte_map[0]: \" INTPTR_FORMAT \"  &_read_byte_map[_last_valid_index]: \" INTPTR_FORMAT,\n+                  p2i(&_read_byte_map[0]), p2i(&_read_byte_map[last_valid_index()]));\n+  log_trace(gc, barrier)(\"    _read_byte_map_base: \" INTPTR_FORMAT, p2i(_read_byte_map_base));\n+}\n+\n+void ShenandoahCardTable::initialize(const ReservedSpace& card_table) {\n+  MemTracker::record_virtual_memory_type((address)card_table.base(), mtGC);\n+\n+  os::trace_page_sizes(\"Card Table\", _byte_map_size, _byte_map_size,\n+                       _page_size, card_table.base(), card_table.size());\n+  if (!card_table.is_reserved()) {\n+    vm_exit_during_initialization(\"Could not reserve enough space for the card marking array\");\n+  }\n+  os::commit_memory_or_exit(card_table.base(), _byte_map_size, card_table.alignment(), false,\n+                            \"Cannot commit memory for card table\");\n+}\n+\n+bool ShenandoahCardTable::is_in_young(const void* obj) const {\n+  return ShenandoahHeap::heap()->is_in_young(obj);\n+}\n+\n+CardValue* ShenandoahCardTable::read_byte_for(const void* p) {\n+    CardValue* result = &_read_byte_map_base[uintptr_t(p) >> _card_shift];\n+    assert(result >= _read_byte_map && result < _read_byte_map + _byte_map_size,\n+           \"out of bounds accessor for card marking array\");\n+    return result;\n+}\n+\n+size_t ShenandoahCardTable::last_valid_index() {\n+  return CardTable::last_valid_index();\n+}\n","filename":"src\/hotspot\/share\/gc\/shenandoah\/shenandoahCardTable.cpp","additions":105,"deletions":0,"binary":false,"changes":105,"status":"added"},{"patch":"@@ -0,0 +1,103 @@\n+\/*\n+ * Copyright Amazon.com Inc. or its affiliates. All Rights Reserved.\n+ * DO NOT ALTER OR REMOVE COPYRIGHT NOTICES OR THIS FILE HEADER.\n+ *\n+ * This code is free software; you can redistribute it and\/or modify it\n+ * under the terms of the GNU General Public License version 2 only, as\n+ * published by the Free Software Foundation.\n+ *\n+ * This code is distributed in the hope that it will be useful, but WITHOUT\n+ * ANY WARRANTY; without even the implied warranty of MERCHANTABILITY or\n+ * FITNESS FOR A PARTICULAR PURPOSE.  See the GNU General Public License\n+ * version 2 for more details (a copy is included in the LICENSE file that\n+ * accompanied this code).\n+ *\n+ * You should have received a copy of the GNU General Public License version\n+ * 2 along with this work; if not, write to the Free Software Foundation,\n+ * Inc., 51 Franklin St, Fifth Floor, Boston, MA 02110-1301 USA.\n+ *\n+ * Please contact Oracle, 500 Oracle Parkway, Redwood Shores, CA 94065 USA\n+ * or visit www.oracle.com if you need additional information or have any\n+ * questions.\n+ *\n+ *\/\n+\n+#ifndef SHARE_GC_SHENANDOAH_SHENANDOAHCARDTABLE_HPP\n+#define SHARE_GC_SHENANDOAH_SHENANDOAHCARDTABLE_HPP\n+\n+#include \"gc\/shared\/cardTable.hpp\"\n+#include \"memory\/virtualspace.hpp\"\n+#include \"oops\/oopsHierarchy.hpp\"\n+#include \"utilities\/macros.hpp\"\n+\n+#define ShenandoahMinCardSizeInBytes 128\n+\n+class ShenandoahCardTable: public CardTable {\n+  friend class VMStructs;\n+\n+private:\n+  \/\/ We maintain two copies of the card table to facilitate concurrent remembered set scanning\n+  \/\/ and concurrent clearing of stale remembered set information.  During the init_mark safepoint,\n+  \/\/ we copy the contents of _write_byte_map to _read_byte_map and clear _write_byte_map.\n+  \/\/\n+  \/\/ Concurrent remembered set scanning reads from _read_byte_map while concurrent mutator write\n+  \/\/ barriers are overwriting cards of the _write_byte_map with DIRTY codes.  Concurrent remembered\n+  \/\/ set scanning also overwrites cards of the _write_byte_map with DIRTY codes whenever it discovers\n+  \/\/ interesting pointers.\n+  \/\/\n+  \/\/ During a concurrent update-references phase, we scan the _write_byte_map concurrently to find\n+  \/\/ all old-gen references that may need to be updated.\n+  \/\/\n+  \/\/ In a future implementation, we may swap the values of _read_byte_map and _write_byte_map during\n+  \/\/ the init-mark safepoint to avoid the need for bulk STW copying and initialization.  Doing so\n+  \/\/ requires a change to the implementation of mutator write barriers as the address of the card\n+  \/\/ table is currently in-lined and hard-coded.\n+  CardValue* _read_byte_map;\n+  CardValue* _write_byte_map;\n+  CardValue* _read_byte_map_base;\n+  CardValue* _write_byte_map_base;\n+\n+public:\n+  explicit ShenandoahCardTable(MemRegion whole_heap) : CardTable(whole_heap),\n+    _read_byte_map(nullptr), _write_byte_map(nullptr),\n+    _read_byte_map_base(nullptr), _write_byte_map_base(nullptr) {}\n+\n+  void initialize();\n+\n+  bool is_in_young(const void* obj) const override;\n+\n+  CardValue* read_byte_for(const void* p);\n+\n+  size_t last_valid_index();\n+\n+  CardValue* swap_read_and_write_tables() {\n+    swap(_read_byte_map, _write_byte_map);\n+    swap(_read_byte_map_base, _write_byte_map_base);\n+\n+    _byte_map = _write_byte_map;\n+    _byte_map_base = _write_byte_map_base;\n+\n+    return _byte_map_base;\n+  }\n+\n+  CardValue* read_byte_map() {\n+    return _read_byte_map;\n+  }\n+\n+  CardValue* read_byte_map_base() {\n+    return _read_byte_map_base;\n+  }\n+\n+  CardValue* write_byte_map() {\n+    return _write_byte_map;\n+  }\n+\n+  CardValue* write_byte_map_base() {\n+    return _write_byte_map_base;\n+  }\n+\n+private:\n+  void initialize(const ReservedSpace& card_table);\n+};\n+\n+#endif \/\/ SHARE_GC_SHENANDOAH_SHENANDOAHCARDTABLE_HPP\n","filename":"src\/hotspot\/share\/gc\/shenandoah\/shenandoahCardTable.hpp","additions":103,"deletions":0,"binary":false,"changes":103,"status":"added"},{"patch":"@@ -38,0 +38,9 @@\n+class SATBMarkQueueSet;\n+\n+class ShenandoahFlushSATBHandshakeClosure : public HandshakeClosure {\n+private:\n+  SATBMarkQueueSet& _qset;\n+public:\n+  inline explicit ShenandoahFlushSATBHandshakeClosure(SATBMarkQueueSet& qset);\n+  inline void do_thread(Thread* thread) override;\n+};\n","filename":"src\/hotspot\/share\/gc\/shenandoah\/shenandoahClosures.hpp","additions":9,"deletions":0,"binary":false,"changes":9,"status":"modified"},{"patch":"@@ -3,0 +3,1 @@\n+ * Copyright Amazon.com Inc. or its affiliates. All Rights Reserved.\n@@ -41,0 +42,8 @@\n+ShenandoahFlushSATBHandshakeClosure::ShenandoahFlushSATBHandshakeClosure(SATBMarkQueueSet& qset) :\n+  HandshakeClosure(\"Shenandoah Flush SATB\"),\n+  _qset(qset) {}\n+\n+void ShenandoahFlushSATBHandshakeClosure::do_thread(Thread* thread) {\n+  _qset.flush_queue(ShenandoahThreadLocalData::satb_mark_queue(thread));\n+}\n+\n@@ -51,1 +60,1 @@\n-  return _mark_context->is_marked(obj);\n+  return _mark_context->is_marked_or_old(obj);\n@@ -63,1 +72,1 @@\n-  return _mark_context->is_marked(obj);\n+  return _mark_context->is_marked_or_old(obj);\n@@ -91,1 +100,1 @@\n-  assert(!ShenandoahHeap::heap()->has_forwarded_objects(), \"Not expected\");\n+  assert(ShenandoahHeap::heap()->is_concurrent_old_mark_in_progress() || !ShenandoahHeap::heap()->has_forwarded_objects(), \"Not expected\");\n","filename":"src\/hotspot\/share\/gc\/shenandoah\/shenandoahClosures.inline.hpp","additions":12,"deletions":3,"binary":false,"changes":15,"status":"modified"},{"patch":"@@ -3,0 +3,1 @@\n+ * Copyright Amazon.com Inc. or its affiliates. All Rights Reserved.\n@@ -27,0 +28,1 @@\n+#include \"gc\/shenandoah\/shenandoahAgeCensus.hpp\"\n@@ -28,0 +30,1 @@\n+#include \"gc\/shenandoah\/shenandoahGenerationalHeap.inline.hpp\"\n@@ -43,0 +46,1 @@\n+  _has_old_regions(false),\n@@ -45,0 +49,1 @@\n+  _live(0),\n@@ -46,0 +51,2 @@\n+  _old_garbage(0),\n+  _preselected_regions(nullptr),\n@@ -86,0 +93,2 @@\n+  assert(!r->is_humongous(), \"Only add regular regions to the collection set\");\n+\n@@ -87,0 +96,14 @@\n+  size_t live    = r->get_live_data_bytes();\n+  size_t garbage = r->garbage();\n+  size_t free    = r->free();\n+  if (r->is_young()) {\n+    _young_bytes_to_evacuate += live;\n+    _young_available_bytes_collected += free;\n+    if (ShenandoahHeap::heap()->mode()->is_generational() && ShenandoahGenerationalHeap::heap()->is_tenurable(r)) {\n+      _young_bytes_to_promote += live;\n+    }\n+  } else if (r->is_old()) {\n+    _old_bytes_to_evacuate += live;\n+    _old_garbage += garbage;\n+  }\n+\n@@ -88,1 +111,2 @@\n-  _garbage += r->garbage();\n+  _has_old_regions |= r->is_old();\n+  _garbage += garbage;\n@@ -90,1 +114,1 @@\n-\n+  _live += live;\n@@ -97,0 +121,1 @@\n+\n@@ -106,0 +131,1 @@\n+  _old_garbage = 0;\n@@ -107,0 +133,1 @@\n+  _live = 0;\n@@ -110,0 +137,8 @@\n+\n+  _young_bytes_to_evacuate = 0;\n+  _young_bytes_to_promote = 0;\n+  _old_bytes_to_evacuate = 0;\n+\n+  _young_available_bytes_collected = 0;\n+\n+  _has_old_regions = false;\n@@ -153,1 +188,5 @@\n-  out->print_cr(\"Collection Set : \" SIZE_FORMAT \"\", count());\n+  out->print_cr(\"Collection Set: Regions: \"\n+                SIZE_FORMAT \", Garbage: \" SIZE_FORMAT \"%s, Live: \" SIZE_FORMAT \"%s, Used: \" SIZE_FORMAT \"%s\", count(),\n+                byte_size_in_proper_unit(garbage()), proper_unit_for_byte_size(garbage()),\n+                byte_size_in_proper_unit(live()),    proper_unit_for_byte_size(live()),\n+                byte_size_in_proper_unit(used()),    proper_unit_for_byte_size(used()));\n","filename":"src\/hotspot\/share\/gc\/shenandoah\/shenandoahCollectionSet.cpp","additions":42,"deletions":3,"binary":false,"changes":45,"status":"modified"},{"patch":"@@ -3,0 +3,1 @@\n+ * Copyright Amazon.com Inc. or its affiliates. All Rights Reserved.\n@@ -36,0 +37,8 @@\n+  friend class ShenandoahCollectionSetPreselector;\n+\n+  void establish_preselected(bool *preselected) {\n+   assert(_preselected_regions == nullptr, \"Over-writing\");\n+   _preselected_regions = preselected;\n+  }\n+  void abandon_preselected() { _preselected_regions = nullptr; }\n+\n@@ -46,0 +55,1 @@\n+  bool                  _has_old_regions;\n@@ -48,0 +58,1 @@\n+  size_t                _live;\n@@ -50,0 +61,16 @@\n+  size_t                _young_bytes_to_evacuate;\n+  size_t                _young_bytes_to_promote;\n+  size_t                _old_bytes_to_evacuate;\n+\n+  \/\/ How many bytes of old garbage are present in a mixed collection set?\n+  size_t                _old_garbage;\n+\n+  \/\/ Points to array identifying which tenure-age regions have been preselected\n+  \/\/ for inclusion in collection set. This field is only valid during brief\n+  \/\/ spans of time while collection set is being constructed.\n+  bool*                 _preselected_regions;\n+\n+  \/\/ When a region having memory available to be allocated is added to the collection set, the region's available memory\n+  \/\/ should be subtracted from what's available.\n+  size_t                _young_available_bytes_collected;\n+\n@@ -80,2 +107,25 @@\n-  size_t used()      const { return _used; }\n-  size_t garbage()   const { return _garbage;   }\n+  \/\/ It is not known how many of these bytes will be promoted.\n+  inline size_t get_young_bytes_reserved_for_evacuation();\n+  inline size_t get_old_bytes_reserved_for_evacuation();\n+\n+  inline size_t get_young_bytes_to_be_promoted();\n+\n+  size_t get_young_available_bytes_collected() { return _young_available_bytes_collected; }\n+\n+  inline size_t get_old_garbage();\n+\n+  bool is_preselected(size_t region_idx) {\n+    assert(_preselected_regions != nullptr, \"Missing etsablish after abandon\");\n+    return _preselected_regions[region_idx];\n+  }\n+\n+  bool* preselected_regions() {\n+    assert(_preselected_regions != nullptr, \"Null ptr\");\n+    return _preselected_regions;\n+  }\n+\n+  bool has_old_regions() const { return _has_old_regions; }\n+  size_t used()          const { return _used; }\n+  size_t live()          const { return _live; }\n+  size_t garbage()       const { return _garbage; }\n+\n","filename":"src\/hotspot\/share\/gc\/shenandoah\/shenandoahCollectionSet.hpp","additions":52,"deletions":2,"binary":false,"changes":54,"status":"modified"},{"patch":"@@ -3,0 +3,1 @@\n+ * Copyright Amazon.com Inc. or its affiliates. All Rights Reserved.\n@@ -56,0 +57,16 @@\n+size_t ShenandoahCollectionSet::get_old_bytes_reserved_for_evacuation() {\n+  return _old_bytes_to_evacuate;\n+}\n+\n+size_t ShenandoahCollectionSet::get_young_bytes_reserved_for_evacuation() {\n+  return _young_bytes_to_evacuate - _young_bytes_to_promote;\n+}\n+\n+size_t ShenandoahCollectionSet::get_young_bytes_to_be_promoted() {\n+  return _young_bytes_to_promote;\n+}\n+\n+size_t ShenandoahCollectionSet::get_old_garbage() {\n+  return _old_garbage;\n+}\n+\n","filename":"src\/hotspot\/share\/gc\/shenandoah\/shenandoahCollectionSet.inline.hpp","additions":17,"deletions":0,"binary":false,"changes":17,"status":"modified"},{"patch":"@@ -0,0 +1,51 @@\n+\/*\n+ * Copyright Amazon.com Inc. or its affiliates. All Rights Reserved.\n+ * DO NOT ALTER OR REMOVE COPYRIGHT NOTICES OR THIS FILE HEADER.\n+ *\n+ * This code is free software; you can redistribute it and\/or modify it\n+ * under the terms of the GNU General Public License version 2 only, as\n+ * published by the Free Software Foundation.\n+ *\n+ * This code is distributed in the hope that it will be useful, but WITHOUT\n+ * ANY WARRANTY; without even the implied warranty of MERCHANTABILITY or\n+ * FITNESS FOR A PARTICULAR PURPOSE.  See the GNU General Public License\n+ * version 2 for more details (a copy is included in the LICENSE file that\n+ * accompanied this code).\n+ *\n+ * You should have received a copy of the GNU General Public License version\n+ * 2 along with this work; if not, write to the Free Software Foundation,\n+ * Inc., 51 Franklin St, Fifth Floor, Boston, MA 02110-1301 USA.\n+ *\n+ * Please contact Oracle, 500 Oracle Parkway, Redwood Shores, CA 94065 USA\n+ * or visit www.oracle.com if you need additional information or have any\n+ * questions.\n+ *\n+ *\/\n+\n+#ifndef SHARE_GC_SHENANDOAH_SHENANDOAHCOLLECTIONSETPRESELECTOR_HPP\n+#define SHARE_GC_SHENANDOAH_SHENANDOAHCOLLECTIONSETPRESELECTOR_HPP\n+\n+#include \"gc\/shenandoah\/shenandoahCollectionSet.hpp\"\n+#include \"memory\/resourceArea.hpp\"\n+\n+class ShenandoahCollectionSetPreselector : public StackObj {\n+  ShenandoahCollectionSet* _cset;\n+  bool* _pset;\n+  ResourceMark _rm;\n+\n+public:\n+  ShenandoahCollectionSetPreselector(ShenandoahCollectionSet* cset, size_t num_regions):\n+    _cset(cset) {\n+    _pset = NEW_RESOURCE_ARRAY(bool, num_regions);\n+    for (unsigned int i = 0; i < num_regions; i++) {\n+        _pset[i] = false;\n+    }\n+    _cset->establish_preselected(_pset);\n+  }\n+\n+  ~ShenandoahCollectionSetPreselector() {\n+    _cset->abandon_preselected();\n+  }\n+};\n+\n+#endif \/\/ SHARE_GC_SHENANDOAH_SHENANDOAHCOLLECTIONSETPRESELECTOR_HPP\n","filename":"src\/hotspot\/share\/gc\/shenandoah\/shenandoahCollectionSetPreselector.hpp","additions":51,"deletions":0,"binary":false,"changes":51,"status":"added"},{"patch":"@@ -3,0 +3,1 @@\n+ * Copyright Amazon.com Inc. or its affiliates. All Rights Reserved.\n@@ -34,0 +35,1 @@\n+  _abbreviated_concurrent_gcs(0),\n@@ -35,0 +37,1 @@\n+  _abbreviated_degenerated_gcs(0),\n@@ -36,0 +39,6 @@\n+  _consecutive_degenerated_gcs(0),\n+  _consecutive_degenerated_gcs_without_progress(0),\n+  _consecutive_young_gcs(0),\n+  _mixed_gcs(0),\n+  _success_old_gcs(0),\n+  _interrupted_old_gcs(0),\n@@ -38,6 +47,1 @@\n-  _alloc_failure_full(0),\n-  _explicit_concurrent(0),\n-  _explicit_full(0),\n-  _implicit_concurrent(0),\n-  _implicit_full(0),\n-  _cycle_counter(0) {\n+  _alloc_failure_full(0) {\n@@ -45,1 +49,2 @@\n-  Copy::zero_to_bytes(_degen_points, sizeof(size_t) * ShenandoahGC::_DEGENERATED_LIMIT);\n+  Copy::zero_to_bytes(_degen_point_counts, sizeof(size_t) * ShenandoahGC::_DEGENERATED_LIMIT);\n+  Copy::zero_to_bytes(_collection_cause_counts, sizeof(size_t) * GCCause::_last_gc_cause);\n@@ -48,13 +53,0 @@\n-\n-}\n-\n-void ShenandoahCollectorPolicy::record_explicit_to_concurrent() {\n-  _explicit_concurrent++;\n-}\n-\n-void ShenandoahCollectorPolicy::record_explicit_to_full() {\n-  _explicit_full++;\n-}\n-\n-void ShenandoahCollectorPolicy::record_implicit_to_concurrent() {\n-  _implicit_concurrent++;\n@@ -63,2 +55,3 @@\n-void ShenandoahCollectorPolicy::record_implicit_to_full() {\n-  _implicit_full++;\n+void ShenandoahCollectorPolicy::record_collection_cause(GCCause::Cause cause) {\n+  assert(cause < GCCause::_last_gc_cause, \"Invalid GCCause\");\n+  _collection_cause_counts[cause]++;\n@@ -74,1 +67,1 @@\n-  _degen_points[point]++;\n+  _degen_point_counts[point]++;\n@@ -78,0 +71,1 @@\n+  reset_consecutive_degenerated_gcs();\n@@ -81,1 +75,4 @@\n-void ShenandoahCollectorPolicy::record_success_concurrent() {\n+void ShenandoahCollectorPolicy::record_success_concurrent(bool is_young, bool is_abbreviated) {\n+  update_young(is_young);\n+\n+  reset_consecutive_degenerated_gcs();\n@@ -83,0 +80,3 @@\n+  if (is_abbreviated) {\n+    _abbreviated_concurrent_gcs++;\n+  }\n@@ -85,2 +85,2 @@\n-void ShenandoahCollectorPolicy::record_success_degenerated() {\n-  _success_degenerated_gcs++;\n+void ShenandoahCollectorPolicy::record_mixed_cycle() {\n+  _mixed_gcs++;\n@@ -89,2 +89,8 @@\n-void ShenandoahCollectorPolicy::record_success_full() {\n-  _success_full_gcs++;\n+void ShenandoahCollectorPolicy::record_success_old() {\n+  _consecutive_young_gcs = 0;\n+  _success_old_gcs++;\n+}\n+\n+void ShenandoahCollectorPolicy::record_interrupted_old() {\n+  _consecutive_young_gcs = 0;\n+  _interrupted_old_gcs++;\n@@ -93,2 +99,23 @@\n-size_t ShenandoahCollectorPolicy::cycle_counter() const {\n-  return _cycle_counter;\n+void ShenandoahCollectorPolicy::record_degenerated(bool is_young, bool is_abbreviated, bool progress) {\n+  update_young(is_young);\n+\n+  _success_degenerated_gcs++;\n+  _consecutive_degenerated_gcs++;\n+\n+  if (progress) {\n+    _consecutive_degenerated_gcs_without_progress = 0;\n+  } else {\n+    _consecutive_degenerated_gcs_without_progress++;\n+  }\n+\n+  if (is_abbreviated) {\n+    _abbreviated_degenerated_gcs++;\n+  }\n+}\n+\n+void ShenandoahCollectorPolicy::update_young(bool is_young) {\n+  if (is_young) {\n+    _consecutive_young_gcs++;\n+  } else {\n+    _consecutive_young_gcs = 0;\n+  }\n@@ -97,2 +124,4 @@\n-void ShenandoahCollectorPolicy::record_cycle_start() {\n-  _cycle_counter++;\n+void ShenandoahCollectorPolicy::record_success_full() {\n+  reset_consecutive_degenerated_gcs();\n+  _consecutive_young_gcs = 0;\n+  _success_full_gcs++;\n@@ -105,1 +134,1 @@\n-bool ShenandoahCollectorPolicy::is_at_shutdown() {\n+bool ShenandoahCollectorPolicy::is_at_shutdown() const {\n@@ -109,0 +138,62 @@\n+bool ShenandoahCollectorPolicy::is_explicit_gc(GCCause::Cause cause) {\n+  return GCCause::is_user_requested_gc(cause)\n+      || GCCause::is_serviceability_requested_gc(cause)\n+      || cause == GCCause::_wb_full_gc\n+      || cause == GCCause::_wb_young_gc;\n+}\n+\n+bool is_implicit_gc(GCCause::Cause cause) {\n+  return cause != GCCause::_no_gc\n+      && cause != GCCause::_shenandoah_concurrent_gc\n+      && cause != GCCause::_allocation_failure\n+      && !ShenandoahCollectorPolicy::is_explicit_gc(cause);\n+}\n+\n+#ifdef ASSERT\n+bool is_valid_request(GCCause::Cause cause) {\n+  return ShenandoahCollectorPolicy::is_explicit_gc(cause)\n+      || ShenandoahCollectorPolicy::is_shenandoah_gc(cause)\n+      || cause == GCCause::_metadata_GC_clear_soft_refs\n+      || cause == GCCause::_codecache_GC_aggressive\n+      || cause == GCCause::_codecache_GC_threshold\n+      || cause == GCCause::_full_gc_alot\n+      || cause == GCCause::_wb_young_gc\n+      || cause == GCCause::_wb_full_gc\n+      || cause == GCCause::_wb_breakpoint\n+      || cause == GCCause::_scavenge_alot;\n+}\n+#endif\n+\n+bool ShenandoahCollectorPolicy::is_shenandoah_gc(GCCause::Cause cause) {\n+  return cause == GCCause::_allocation_failure\n+      || cause == GCCause::_shenandoah_stop_vm\n+      || cause == GCCause::_shenandoah_allocation_failure_evac\n+      || cause == GCCause::_shenandoah_humongous_allocation_failure\n+      || cause == GCCause::_shenandoah_concurrent_gc\n+      || cause == GCCause::_shenandoah_upgrade_to_full_gc;\n+}\n+\n+\n+bool ShenandoahCollectorPolicy::is_allocation_failure(GCCause::Cause cause) {\n+  return cause == GCCause::_allocation_failure\n+      || cause == GCCause::_shenandoah_allocation_failure_evac\n+      || cause == GCCause::_shenandoah_humongous_allocation_failure;\n+}\n+\n+bool ShenandoahCollectorPolicy::is_requested_gc(GCCause::Cause cause) {\n+  return is_explicit_gc(cause) || is_implicit_gc(cause);\n+}\n+\n+bool ShenandoahCollectorPolicy::should_run_full_gc(GCCause::Cause cause) {\n+  return is_explicit_gc(cause) ? !ExplicitGCInvokesConcurrent : !ShenandoahImplicitGCInvokesConcurrent;\n+}\n+\n+bool ShenandoahCollectorPolicy::should_handle_requested_gc(GCCause::Cause cause) {\n+  assert(is_valid_request(cause), \"only requested GCs here: %s\", GCCause::to_string(cause));\n+\n+  if (DisableExplicitGC) {\n+    return !is_explicit_gc(cause);\n+  }\n+  return true;\n+}\n+\n@@ -113,1 +204,2 @@\n-  out->print_cr(\"to avoid Degenerated and Full GC cycles.\");\n+  out->print_cr(\"to avoid Degenerated and Full GC cycles. Abbreviated cycles are those which found\");\n+  out->print_cr(\"enough regions with no live objects to skip evacuation.\");\n@@ -116,3 +208,19 @@\n-  out->print_cr(SIZE_FORMAT_W(5) \" successful concurrent GCs\",         _success_concurrent_gcs);\n-  out->print_cr(\"  \" SIZE_FORMAT_W(5) \" invoked explicitly\",           _explicit_concurrent);\n-  out->print_cr(\"  \" SIZE_FORMAT_W(5) \" invoked implicitly\",           _implicit_concurrent);\n+  size_t completed_gcs = _success_full_gcs + _success_degenerated_gcs + _success_concurrent_gcs + _success_old_gcs;\n+  out->print_cr(SIZE_FORMAT_W(5) \" Completed GCs\", completed_gcs);\n+\n+  size_t explicit_requests = 0;\n+  size_t implicit_requests = 0;\n+  for (int c = 0; c < GCCause::_last_gc_cause; c++) {\n+    size_t cause_count = _collection_cause_counts[c];\n+    if (cause_count > 0) {\n+      auto cause = (GCCause::Cause) c;\n+      if (is_explicit_gc(cause)) {\n+        explicit_requests += cause_count;\n+      } else if (is_implicit_gc(cause)) {\n+        implicit_requests += cause_count;\n+      }\n+      const char* desc = GCCause::to_string(cause);\n+      out->print_cr(\"  \" SIZE_FORMAT_W(5) \" caused by %s (%.2f%%)\", cause_count, desc, percent_of(cause_count, completed_gcs));\n+    }\n+  }\n+\n@@ -120,0 +228,16 @@\n+  out->print_cr(SIZE_FORMAT_W(5) \" Successful Concurrent GCs (%.2f%%)\", _success_concurrent_gcs, percent_of(_success_concurrent_gcs, completed_gcs));\n+  if (ExplicitGCInvokesConcurrent) {\n+    out->print_cr(\"  \" SIZE_FORMAT_W(5) \" invoked explicitly (%.2f%%)\", explicit_requests, percent_of(explicit_requests, _success_concurrent_gcs));\n+  }\n+  if (ShenandoahImplicitGCInvokesConcurrent) {\n+    out->print_cr(\"  \" SIZE_FORMAT_W(5) \" invoked implicitly (%.2f%%)\", implicit_requests, percent_of(implicit_requests, _success_concurrent_gcs));\n+  }\n+  out->print_cr(\"  \" SIZE_FORMAT_W(5) \" abbreviated (%.2f%%)\",  _abbreviated_concurrent_gcs, percent_of(_abbreviated_concurrent_gcs, _success_concurrent_gcs));\n+  out->cr();\n+\n+  if (ShenandoahHeap::heap()->mode()->is_generational()) {\n+    out->print_cr(SIZE_FORMAT_W(5) \" Completed Old GCs (%.2f%%)\",        _success_old_gcs, percent_of(_success_old_gcs, completed_gcs));\n+    out->print_cr(\"  \" SIZE_FORMAT_W(5) \" mixed\",                        _mixed_gcs);\n+    out->print_cr(\"  \" SIZE_FORMAT_W(5) \" interruptions\",                _interrupted_old_gcs);\n+    out->cr();\n+  }\n@@ -121,2 +245,5 @@\n-  out->print_cr(SIZE_FORMAT_W(5) \" Degenerated GCs\",                   _success_degenerated_gcs);\n-  out->print_cr(\"  \" SIZE_FORMAT_W(5) \" caused by allocation failure\", _alloc_failure_degenerated);\n+  size_t degenerated_gcs = _alloc_failure_degenerated_upgrade_to_full + _success_degenerated_gcs;\n+  out->print_cr(SIZE_FORMAT_W(5) \" Degenerated GCs (%.2f%%)\", degenerated_gcs, percent_of(degenerated_gcs, completed_gcs));\n+  out->print_cr(\"  \" SIZE_FORMAT_W(5) \" upgraded to Full GC (%.2f%%)\",          _alloc_failure_degenerated_upgrade_to_full, percent_of(_alloc_failure_degenerated_upgrade_to_full, degenerated_gcs));\n+  out->print_cr(\"  \" SIZE_FORMAT_W(5) \" caused by allocation failure (%.2f%%)\", _alloc_failure_degenerated, percent_of(_alloc_failure_degenerated, degenerated_gcs));\n+  out->print_cr(\"  \" SIZE_FORMAT_W(5) \" abbreviated (%.2f%%)\",                  _abbreviated_degenerated_gcs, percent_of(_abbreviated_degenerated_gcs, degenerated_gcs));\n@@ -124,1 +251,1 @@\n-    if (_degen_points[c] > 0) {\n+    if (_degen_point_counts[c] > 0) {\n@@ -126,1 +253,1 @@\n-      out->print_cr(\"    \" SIZE_FORMAT_W(5) \" happened at %s\",         _degen_points[c], desc);\n+      out->print_cr(\"    \" SIZE_FORMAT_W(5) \" happened at %s\", _degen_point_counts[c], desc);\n@@ -129,1 +256,0 @@\n-  out->print_cr(\"  \" SIZE_FORMAT_W(5) \" upgraded to Full GC\",          _alloc_failure_degenerated_upgrade_to_full);\n@@ -132,5 +258,9 @@\n-  out->print_cr(SIZE_FORMAT_W(5) \" Full GCs\",                          _success_full_gcs + _alloc_failure_degenerated_upgrade_to_full);\n-  out->print_cr(\"  \" SIZE_FORMAT_W(5) \" invoked explicitly\",           _explicit_full);\n-  out->print_cr(\"  \" SIZE_FORMAT_W(5) \" invoked implicitly\",           _implicit_full);\n-  out->print_cr(\"  \" SIZE_FORMAT_W(5) \" caused by allocation failure\", _alloc_failure_full);\n-  out->print_cr(\"  \" SIZE_FORMAT_W(5) \" upgraded from Degenerated GC\", _alloc_failure_degenerated_upgrade_to_full);\n+  out->print_cr(SIZE_FORMAT_W(5) \" Full GCs (%.2f%%)\", _success_full_gcs, percent_of(_success_full_gcs, completed_gcs));\n+  if (!ExplicitGCInvokesConcurrent) {\n+    out->print_cr(\"  \" SIZE_FORMAT_W(5) \" invoked explicitly (%.2f%%)\", explicit_requests, percent_of(explicit_requests, _success_concurrent_gcs));\n+  }\n+  if (!ShenandoahImplicitGCInvokesConcurrent) {\n+    out->print_cr(\"  \" SIZE_FORMAT_W(5) \" invoked implicitly (%.2f%%)\", implicit_requests, percent_of(implicit_requests, _success_concurrent_gcs));\n+  }\n+  out->print_cr(\"  \" SIZE_FORMAT_W(5) \" caused by allocation failure (%.2f%%)\", _alloc_failure_full, percent_of(_alloc_failure_full, _success_full_gcs));\n+  out->print_cr(\"  \" SIZE_FORMAT_W(5) \" upgraded from Degenerated GC (%.2f%%)\", _alloc_failure_degenerated_upgrade_to_full, percent_of(_alloc_failure_degenerated_upgrade_to_full, _success_full_gcs));\n","filename":"src\/hotspot\/share\/gc\/shenandoah\/shenandoahCollectorPolicy.cpp","additions":177,"deletions":47,"binary":false,"changes":224,"status":"modified"},{"patch":"@@ -3,0 +3,1 @@\n+ * Copyright Amazon.com Inc. or its affiliates. All Rights Reserved.\n@@ -31,0 +32,1 @@\n+#include \"gc\/shenandoah\/shenandoahTrace.hpp\"\n@@ -34,5 +36,0 @@\n-class ShenandoahTracer : public GCTracer, public CHeapObj<mtGC> {\n-public:\n-  ShenandoahTracer() : GCTracer(Shenandoah) {}\n-};\n-\n@@ -42,0 +39,1 @@\n+  size_t _abbreviated_concurrent_gcs;\n@@ -43,0 +41,1 @@\n+  size_t _abbreviated_degenerated_gcs;\n@@ -45,0 +44,6 @@\n+  uint _consecutive_degenerated_gcs;\n+  uint _consecutive_degenerated_gcs_without_progress;\n+  volatile size_t _consecutive_young_gcs;\n+  size_t _mixed_gcs;\n+  size_t _success_old_gcs;\n+  size_t _interrupted_old_gcs;\n@@ -48,5 +53,2 @@\n-  size_t _explicit_concurrent;\n-  size_t _explicit_full;\n-  size_t _implicit_concurrent;\n-  size_t _implicit_full;\n-  size_t _degen_points[ShenandoahGC::_DEGENERATED_LIMIT];\n+  size_t _collection_cause_counts[GCCause::_last_gc_cause];\n+  size_t _degen_point_counts[ShenandoahGC::_DEGENERATED_LIMIT];\n@@ -55,1 +57,0 @@\n-\n@@ -58,1 +59,4 @@\n-  size_t _cycle_counter;\n+  void reset_consecutive_degenerated_gcs() {\n+    _consecutive_degenerated_gcs = 0;\n+    _consecutive_degenerated_gcs_without_progress = 0;\n+  }\n@@ -61,1 +65,12 @@\n-  ShenandoahCollectorPolicy();\n+  \/\/ The most common scenario for lack of good progress following a degenerated GC is an accumulation of floating\n+  \/\/ garbage during the most recently aborted concurrent GC effort.  With generational GC, it is far more effective to\n+  \/\/ reclaim this floating garbage with another degenerated cycle (which focuses on young generation and might require\n+  \/\/ a pause of 200 ms) rather than a full GC cycle (which may require over 2 seconds with a 10 GB old generation).\n+  \/\/\n+  \/\/ In generational mode, we'll only upgrade to full GC if we've done two degen cycles in a row and both indicated\n+  \/\/ bad progress.  In non-generational mode, we'll preserve the original behavior, which is to upgrade to full\n+  \/\/ immediately following a degenerated cycle with bad progress.  This preserves original behavior of non-generational\n+  \/\/ Shenandoah to avoid introducing \"surprising new behavior.\"  It also makes less sense with non-generational\n+  \/\/ Shenandoah to replace a full GC with a degenerated GC, because both have similar pause times in non-generational\n+  \/\/ mode.\n+  static constexpr size_t GENERATIONAL_CONSECUTIVE_BAD_DEGEN_PROGRESS_THRESHOLD = 2;\n@@ -63,3 +78,1 @@\n-  \/\/ TODO: This is different from gc_end: that one encompasses one VM operation.\n-  \/\/ These two encompass the entire cycle.\n-  void record_cycle_start();\n+  ShenandoahCollectorPolicy();\n@@ -67,2 +80,16 @@\n-  void record_success_concurrent();\n-  void record_success_degenerated();\n+  void record_mixed_cycle();\n+  void record_success_old();\n+  void record_interrupted_old();\n+\n+  \/\/ A collection cycle may be \"abbreviated\" if Shenandoah finds a sufficient percentage\n+  \/\/ of regions that contain no live objects (ShenandoahImmediateThreshold). These cycles\n+  \/\/ end after final mark, skipping the evacuation and reference-updating phases. Such\n+  \/\/ cycles are very efficient and are worth tracking. Note that both degenerated and\n+  \/\/ concurrent cycles can be abbreviated.\n+  void record_success_concurrent(bool is_young, bool is_abbreviated);\n+\n+  \/\/ Record that a degenerated cycle has been completed. Note that such a cycle may or\n+  \/\/ may not make \"progress\". We separately track the total number of degenerated cycles,\n+  \/\/ the number of consecutive degenerated cycles and the number of consecutive cycles that\n+  \/\/ fail to make good progress.\n+  void record_degenerated(bool is_young, bool is_abbreviated, bool progress);\n@@ -73,4 +100,1 @@\n-  void record_explicit_to_concurrent();\n-  void record_explicit_to_full();\n-  void record_implicit_to_concurrent();\n-  void record_implicit_to_full();\n+  void record_collection_cause(GCCause::Cause cause);\n@@ -79,1 +103,1 @@\n-  bool is_at_shutdown();\n+  bool is_at_shutdown() const;\n@@ -81,3 +105,1 @@\n-  ShenandoahTracer* tracer() {return _tracer;}\n-\n-  size_t cycle_counter() const;\n+  ShenandoahTracer* tracer() const {return _tracer;}\n@@ -90,0 +112,26 @@\n+\n+  \/\/ If the heuristics find that the number of consecutive degenerated cycles is above\n+  \/\/ ShenandoahFullGCThreshold, then they will initiate a Full GC upon an allocation\n+  \/\/ failure.\n+  size_t consecutive_degenerated_gc_count() const {\n+    return _consecutive_degenerated_gcs;\n+  }\n+\n+  \/\/ Genshen will only upgrade to a full gc after the configured number of futile degenerated cycles.\n+  bool generational_should_upgrade_degenerated_gc() const {\n+    return _consecutive_degenerated_gcs_without_progress >= GENERATIONAL_CONSECUTIVE_BAD_DEGEN_PROGRESS_THRESHOLD;\n+  }\n+\n+  static bool is_allocation_failure(GCCause::Cause cause);\n+  static bool is_shenandoah_gc(GCCause::Cause cause);\n+  static bool is_requested_gc(GCCause::Cause cause);\n+  static bool is_explicit_gc(GCCause::Cause cause);\n+  static bool should_run_full_gc(GCCause::Cause cause);\n+  static bool should_handle_requested_gc(GCCause::Cause cause);\n+\n+  size_t consecutive_young_gc_count() const {\n+    return _consecutive_young_gcs;\n+  }\n+\n+private:\n+  void update_young(bool is_young);\n","filename":"src\/hotspot\/share\/gc\/shenandoah\/shenandoahCollectorPolicy.hpp","additions":74,"deletions":26,"binary":false,"changes":100,"status":"modified"},{"patch":"@@ -4,0 +4,1 @@\n+ * Copyright Amazon.com Inc. or its affiliates. All Rights Reserved.\n@@ -32,0 +33,1 @@\n+#include \"gc\/shenandoah\/shenandoahClosures.inline.hpp\"\n@@ -35,0 +37,4 @@\n+#include \"gc\/shenandoah\/shenandoahGeneration.hpp\"\n+#include \"gc\/shenandoah\/shenandoahGenerationalHeap.hpp\"\n+#include \"gc\/shenandoah\/shenandoahOldGeneration.hpp\"\n+#include \"gc\/shenandoah\/shenandoahYoungGeneration.hpp\"\n@@ -38,1 +44,0 @@\n-#include \"gc\/shenandoah\/shenandoahOopClosures.inline.hpp\"\n@@ -89,3 +94,6 @@\n-ShenandoahConcurrentGC::ShenandoahConcurrentGC() :\n-  _mark(),\n-  _degen_point(ShenandoahDegenPoint::_degenerated_unset) {\n+ShenandoahConcurrentGC::ShenandoahConcurrentGC(ShenandoahGeneration* generation, bool do_old_gc_bootstrap) :\n+  _mark(generation),\n+  _generation(generation),\n+  _degen_point(ShenandoahDegenPoint::_degenerated_unset),\n+  _abbreviated(false),\n+  _do_old_gc_bootstrap(do_old_gc_bootstrap) {\n@@ -98,2 +106,8 @@\n-void ShenandoahConcurrentGC::cancel() {\n-  ShenandoahConcurrentMark::cancel();\n+void ShenandoahConcurrentGC::entry_concurrent_update_refs_prepare(ShenandoahHeap* const heap) {\n+  TraceCollectorStats tcs(heap->monitoring_support()->concurrent_collection_counters());\n+  const char* msg = conc_init_update_refs_event_message();\n+  ShenandoahConcurrentPhase gc_phase(msg, ShenandoahPhaseTimings::conc_update_refs_prepare);\n+  EventMark em(\"%s\", msg);\n+\n+  \/\/ Evacuation is complete, retire gc labs and change gc state\n+  heap->concurrent_prepare_for_update_refs();\n@@ -104,0 +118,1 @@\n+\n@@ -114,0 +129,9 @@\n+\n+    \/\/ Reset task queue stats here, rather than in mark_concurrent_roots,\n+    \/\/ because remembered set scan will `push` oops into the queues and\n+    \/\/ resetting after this happens will lose those counts.\n+    TASKQUEUE_STATS_ONLY(_mark.task_queues()->reset_taskqueue_stats());\n+\n+    \/\/ Concurrent remembered set scanning\n+    entry_scan_remembered_set();\n+\n@@ -116,1 +140,3 @@\n-    if (check_cancellation_and_abort(ShenandoahDegenPoint::_degenerated_outside_cycle)) return false;\n+    if (check_cancellation_and_abort(ShenandoahDegenPoint::_degenerated_roots)) {\n+      return false;\n+    }\n@@ -120,1 +146,3 @@\n-    if (check_cancellation_and_abort(ShenandoahDegenPoint::_degenerated_mark)) return false;\n+    if (check_cancellation_and_abort(ShenandoahDegenPoint::_degenerated_mark)) {\n+      return false;\n+    }\n@@ -126,0 +154,12 @@\n+  \/\/ If the GC was cancelled before final mark, nothing happens on the safepoint. We are still\n+  \/\/ in the marking phase and must resume the degenerated cycle from there. If the GC was cancelled\n+  \/\/ after final mark, then we've entered the evacuation phase and must resume the degenerated cycle\n+  \/\/ from that phase.\n+  if (_generation->is_concurrent_mark_in_progress()) {\n+    bool cancelled = check_cancellation_and_abort(ShenandoahDegenPoint::_degenerated_mark);\n+    assert(cancelled, \"GC must have been cancelled between concurrent and final mark\");\n+    return false;\n+  }\n+\n+  assert(heap->is_concurrent_weak_root_in_progress(), \"Must be doing weak roots now\");\n+\n@@ -131,4 +171,9 @@\n-  \/\/ Process weak roots that might still point to regions that would be broken by cleanup\n-  if (heap->is_concurrent_weak_root_in_progress()) {\n-    entry_weak_refs();\n-    entry_weak_roots();\n+  \/\/ Process weak roots that might still point to regions that would be broken by cleanup.\n+  \/\/ We cannot recycle regions because weak roots need to know what is marked in trashed regions.\n+  entry_weak_refs();\n+  entry_weak_roots();\n+\n+  \/\/ Perform concurrent class unloading before any regions get recycled. Class unloading may\n+  \/\/ need to inspect unmarked objects in trashed regions.\n+  if (heap->unload_classes()) {\n+    entry_class_unloading();\n@@ -138,1 +183,2 @@\n-  \/\/ the space. This would be the last action if there is nothing to evacuate.\n+  \/\/ the space. This would be the last action if there is nothing to evacuate.  Note that\n+  \/\/ we will not age young-gen objects in the case that we skip evacuation.\n@@ -141,10 +187,1 @@\n-  {\n-    ShenandoahHeapLocker locker(heap->lock());\n-    heap->free_set()->log_status();\n-  }\n-\n-  \/\/ Perform concurrent class unloading\n-  if (heap->unload_classes() &&\n-      heap->is_concurrent_weak_root_in_progress()) {\n-    entry_class_unloading();\n-  }\n+  heap->free_set()->log_status_under_lock();\n@@ -165,1 +202,5 @@\n-    if (check_cancellation_and_abort(ShenandoahDegenPoint::_degenerated_evac)) return false;\n+    if (check_cancellation_and_abort(ShenandoahDegenPoint::_degenerated_evac)) {\n+      return false;\n+    }\n+\n+    entry_concurrent_update_refs_prepare(heap);\n@@ -168,3 +209,8 @@\n-    vmop_entry_init_updaterefs();\n-    entry_updaterefs();\n-    if (check_cancellation_and_abort(ShenandoahDegenPoint::_degenerated_updaterefs)) return false;\n+    if (ShenandoahVerify || ShenandoahPacing) {\n+      vmop_entry_init_update_refs();\n+    }\n+\n+    entry_update_refs();\n+    if (check_cancellation_and_abort(ShenandoahDegenPoint::_degenerated_update_refs)) {\n+      return false;\n+    }\n@@ -174,1 +220,3 @@\n-    if (check_cancellation_and_abort(ShenandoahDegenPoint::_degenerated_updaterefs)) return false;\n+    if (check_cancellation_and_abort(ShenandoahDegenPoint::_degenerated_update_refs)) {\n+      return false;\n+    }\n@@ -176,1 +224,1 @@\n-    vmop_entry_final_updaterefs();\n+    vmop_entry_final_update_refs();\n@@ -181,1 +229,44 @@\n-    vmop_entry_final_roots();\n+    if (!entry_final_roots()) {\n+      assert(_degen_point != _degenerated_unset, \"Need to know where to start degenerated cycle\");\n+      return false;\n+    }\n+\n+    if (VerifyAfterGC) {\n+      vmop_entry_verify_final_roots();\n+    }\n+    _abbreviated = true;\n+  }\n+\n+  \/\/ We defer generation resizing actions until after cset regions have been recycled.  We do this even following an\n+  \/\/ abbreviated cycle.\n+  if (heap->mode()->is_generational()) {\n+    ShenandoahGenerationalHeap::heap()->complete_concurrent_cycle();\n+  }\n+\n+  \/\/ Instead of always resetting immediately before the start of a new GC, we can often reset at the end of the\n+  \/\/ previous GC. This allows us to start the next GC cycle more quickly after a trigger condition is detected,\n+  \/\/ reducing the likelihood that GC will degenerate.\n+  entry_reset_after_collect();\n+\n+  return true;\n+}\n+\n+bool ShenandoahConcurrentGC::complete_abbreviated_cycle() {\n+  shenandoah_assert_generational();\n+\n+  ShenandoahGenerationalHeap* const heap = ShenandoahGenerationalHeap::heap();\n+\n+  \/\/ We chose not to evacuate because we found sufficient immediate garbage.\n+  \/\/ However, there may still be regions to promote in place, so do that now.\n+  if (heap->old_generation()->has_in_place_promotions()) {\n+    entry_promote_in_place();\n+\n+    \/\/ If the promote-in-place operation was cancelled, we can have the degenerated\n+    \/\/ cycle complete the operation. It will see that no evacuations are in progress,\n+    \/\/ and that there are regions wanting promotion. The risk with not handling the\n+    \/\/ cancellation would be failing to restore top for these regions and leaving\n+    \/\/ them unable to serve allocations for the old generation.This will leave the weak\n+    \/\/ roots flag set (the degenerated cycle will unset it).\n+    if (check_cancellation_and_abort(ShenandoahDegenPoint::_degenerated_evac)) {\n+      return false;\n+    }\n@@ -184,0 +275,21 @@\n+  \/\/ At this point, the cycle is effectively complete. If the cycle has been cancelled here,\n+  \/\/ the control thread will detect it on its next iteration and run a degenerated young cycle.\n+  if (!_generation->is_old()) {\n+    heap->update_region_ages(_generation->complete_marking_context());\n+  }\n+\n+  if (!heap->is_concurrent_old_mark_in_progress()) {\n+    heap->concurrent_final_roots();\n+  } else {\n+    \/\/ Since the cycle was shortened for having enough immediate garbage, this will be\n+    \/\/ the last phase before concurrent marking of old resumes. We must be sure\n+    \/\/ that old mark threads don't see any pointers to garbage in the SATB queues. Even\n+    \/\/ though nothing was evacuated, overwriting unreachable weak roots with null may still\n+    \/\/ put pointers to regions that become trash in the SATB queues. The following will\n+    \/\/ piggyback flushing the thread local SATB queues on the same handshake that propagates\n+    \/\/ the gc state change.\n+    ShenandoahSATBMarkQueueSet& satb_queues = ShenandoahBarrierSet::satb_mark_queue_set();\n+    ShenandoahFlushSATBHandshakeClosure complete_thread_local_satb_buffers(satb_queues);\n+    heap->concurrent_final_roots(&complete_thread_local_satb_buffers);\n+    heap->old_generation()->concurrent_transfer_pointers_from_satb();\n+  }\n@@ -187,0 +299,1 @@\n+\n@@ -207,1 +320,1 @@\n-void ShenandoahConcurrentGC::vmop_entry_init_updaterefs() {\n+void ShenandoahConcurrentGC::vmop_entry_init_update_refs() {\n@@ -217,1 +330,1 @@\n-void ShenandoahConcurrentGC::vmop_entry_final_updaterefs() {\n+void ShenandoahConcurrentGC::vmop_entry_final_update_refs() {\n@@ -227,1 +340,1 @@\n-void ShenandoahConcurrentGC::vmop_entry_final_roots() {\n+void ShenandoahConcurrentGC::vmop_entry_verify_final_roots() {\n@@ -262,1 +375,1 @@\n-void ShenandoahConcurrentGC::entry_init_updaterefs() {\n+void ShenandoahConcurrentGC::entry_init_update_refs() {\n@@ -268,1 +381,1 @@\n-  op_init_updaterefs();\n+  op_init_update_refs();\n@@ -271,1 +384,1 @@\n-void ShenandoahConcurrentGC::entry_final_updaterefs() {\n+void ShenandoahConcurrentGC::entry_final_update_refs() {\n@@ -280,1 +393,1 @@\n-  op_final_updaterefs();\n+  op_final_update_refs();\n@@ -283,2 +396,2 @@\n-void ShenandoahConcurrentGC::entry_final_roots() {\n-  static const char* msg = \"Pause Final Roots\";\n+void ShenandoahConcurrentGC::entry_verify_final_roots() {\n+  const char* msg = verify_final_roots_event_message();\n@@ -288,1 +401,1 @@\n-  op_final_roots();\n+  op_verify_final_roots();\n@@ -293,0 +406,2 @@\n+  heap->try_inject_alloc_failure();\n+\n@@ -294,3 +409,11 @@\n-  static const char* msg = \"Concurrent reset\";\n-  ShenandoahConcurrentPhase gc_phase(msg, ShenandoahPhaseTimings::conc_reset);\n-  EventMark em(\"%s\", msg);\n+  {\n+    const char* msg = conc_reset_event_message();\n+    ShenandoahConcurrentPhase gc_phase(msg, ShenandoahPhaseTimings::conc_reset);\n+    EventMark em(\"%s\", msg);\n+\n+    ShenandoahWorkerScope scope(heap->workers(),\n+                                ShenandoahWorkerPolicy::calc_workers_for_conc_reset(),\n+                                msg);\n+    op_reset();\n+  }\n+}\n@@ -298,3 +421,7 @@\n-  ShenandoahWorkerScope scope(heap->workers(),\n-                              ShenandoahWorkerPolicy::calc_workers_for_conc_reset(),\n-                              \"concurrent reset\");\n+void ShenandoahConcurrentGC::entry_scan_remembered_set() {\n+  if (_generation->is_young()) {\n+    ShenandoahHeap* const heap = ShenandoahHeap::heap();\n+    TraceCollectorStats tcs(heap->monitoring_support()->concurrent_collection_counters());\n+    const char* msg = \"Concurrent remembered set scanning\";\n+    ShenandoahConcurrentPhase gc_phase(msg, ShenandoahPhaseTimings::init_scan_rset);\n+    EventMark em(\"%s\", msg);\n@@ -302,2 +429,7 @@\n-  heap->try_inject_alloc_failure();\n-  op_reset();\n+    ShenandoahWorkerScope scope(heap->workers(),\n+                                ShenandoahWorkerPolicy::calc_workers_for_rs_scanning(),\n+                                msg);\n+\n+    heap->try_inject_alloc_failure();\n+    _generation->scan_remembered_set(true \/* is_concurrent *\/);\n+  }\n@@ -352,1 +484,1 @@\n-  static const char* msg = \"Concurrent weak references\";\n+  const char* msg = conc_weak_refs_event_message();\n@@ -367,1 +499,1 @@\n-  static const char* msg = \"Concurrent weak roots\";\n+  const char* msg = conc_weak_roots_event_message();\n@@ -414,1 +546,1 @@\n-  static const char* msg = \"Concurrent cleanup\";\n+  const char* msg = conc_cleanup_event_message();\n@@ -439,0 +571,10 @@\n+void ShenandoahConcurrentGC::entry_promote_in_place() const {\n+  shenandoah_assert_generational();\n+\n+  ShenandoahTimingsTracker timing(ShenandoahPhaseTimings::promote_in_place);\n+  ShenandoahGCWorkerPhase worker_phase(ShenandoahPhaseTimings::promote_in_place);\n+  EventMark em(\"%s\", \"Promote in place\");\n+\n+  ShenandoahGenerationalHeap::heap()->promote_regions_in_place(true);\n+}\n+\n@@ -452,1 +594,1 @@\n-void ShenandoahConcurrentGC::entry_updaterefs() {\n+void ShenandoahConcurrentGC::entry_update_refs() {\n@@ -464,1 +606,1 @@\n-  op_updaterefs();\n+  op_update_refs();\n@@ -470,1 +612,1 @@\n-  static const char* msg = \"Concurrent cleanup\";\n+  const char* msg = conc_cleanup_event_message();\n@@ -479,0 +621,10 @@\n+void ShenandoahConcurrentGC::entry_reset_after_collect() {\n+  ShenandoahHeap* const heap = ShenandoahHeap::heap();\n+  TraceCollectorStats tcs(heap->monitoring_support()->concurrent_collection_counters());\n+  const char* msg = conc_reset_after_collect_event_message();\n+  ShenandoahConcurrentPhase gc_phase(msg, ShenandoahPhaseTimings::conc_reset_after_collect);\n+  EventMark em(\"%s\", msg);\n+\n+  op_reset_after_collect();\n+}\n+\n@@ -484,0 +636,8 @@\n+  \/\/ If it is old GC bootstrap cycle, always clear bitmap for global gen\n+  \/\/ to ensure bitmap for old gen is clear for old GC cycle after this.\n+  if (_do_old_gc_bootstrap) {\n+    assert(!heap->is_prepare_for_old_mark_in_progress(), \"Cannot reset old without making it parsable\");\n+    heap->global_generation()->prepare_gc();\n+  } else {\n+    _generation->prepare_gc();\n+  }\n@@ -485,1 +645,3 @@\n-  heap->prepare_gc();\n+  if (heap->mode()->is_generational()) {\n+    heap->old_generation()->card_scan()->mark_read_table_as_clean();\n+  }\n@@ -498,1 +660,2 @@\n-      \/\/ reset, so it is very likely we don't need to do another write here.\n+      \/\/ reset, so it is very likely we don't need to do another write here.  Since most regions\n+      \/\/ are not \"active\", this path is relatively rare.\n@@ -520,2 +683,2 @@\n-  assert(heap->marking_context()->is_bitmap_clear(), \"need clear marking bitmap\");\n-  assert(!heap->marking_context()->is_complete(), \"should not be complete\");\n+  assert(_generation->is_bitmap_clear(), \"need clear marking bitmap\");\n+  assert(!_generation->is_mark_complete(), \"should not be complete\");\n@@ -524,0 +687,21 @@\n+  if (heap->mode()->is_generational()) {\n+\n+    if (_generation->is_global()) {\n+      heap->old_generation()->cancel_gc();\n+    } else if (heap->is_concurrent_old_mark_in_progress()) {\n+      \/\/ Purge the SATB buffers, transferring any valid, old pointers to the\n+      \/\/ old generation mark queue. Any pointers in a young region will be\n+      \/\/ abandoned.\n+      ShenandoahGCPhase phase(ShenandoahPhaseTimings::init_transfer_satb);\n+      heap->old_generation()->transfer_pointers_from_satb();\n+    }\n+    {\n+      \/\/ After we swap card table below, the write-table is all clean, and the read table holds\n+      \/\/ cards dirty prior to the start of GC. Young and bootstrap collection will update\n+      \/\/ the write card table as a side effect of remembered set scanning. Global collection will\n+      \/\/ update the card table as a side effect of global marking of old objects.\n+      ShenandoahGCPhase phase(ShenandoahPhaseTimings::init_swap_rset);\n+      _generation->swap_card_tables();\n+    }\n+  }\n+\n@@ -525,0 +709,1 @@\n+    ShenandoahTimingsTracker v(ShenandoahPhaseTimings::init_mark_verify);\n@@ -532,1 +717,1 @@\n-  heap->set_concurrent_mark_in_progress(true);\n+  _generation->set_concurrent_mark_in_progress(true);\n@@ -536,1 +721,3 @@\n-  {\n+  if (_do_old_gc_bootstrap) {\n+    shenandoah_assert_generational();\n+    \/\/ Update region state for both young and old regions\n@@ -540,0 +727,6 @@\n+    heap->old_generation()->ref_processor()->reset_thread_locals();\n+  } else {\n+    \/\/ Update region state for only young regions\n+    ShenandoahGCPhase phase(ShenandoahPhaseTimings::init_update_region_states);\n+    ShenandoahInitMarkUpdateRegionStateClosure cl;\n+    _generation->parallel_heap_region_iterate(&cl);\n@@ -543,1 +736,1 @@\n-  ShenandoahReferenceProcessor* rp = heap->ref_processor();\n+  ShenandoahReferenceProcessor* rp = _generation->ref_processor();\n@@ -557,0 +750,5 @@\n+\n+  {\n+    ShenandoahTimingsTracker timing(ShenandoahPhaseTimings::init_propagate_gc_state);\n+    heap->propagate_gc_state_to_all_threads();\n+  }\n@@ -583,1 +781,4 @@\n-    heap->prepare_regions_and_collection_set(true \/*concurrent*\/);\n+    \/\/ The collection set is chosen by prepare_regions_and_collection_set(). Additionally, certain parameters have been\n+    \/\/ established to govern the evacuation efforts that are about to begin.  Refer to comments on reserve members in\n+    \/\/ ShenandoahGeneration and ShenandoahOldGeneration for more detail.\n+    _generation->prepare_regions_and_collection_set(true \/*concurrent*\/);\n@@ -589,0 +790,7 @@\n+      LogTarget(Debug, gc, cset) lt;\n+      if (lt.is_enabled()) {\n+        ResourceMark rm;\n+        LogStream ls(lt);\n+        heap->collection_set()->print_on(&ls);\n+      }\n+\n@@ -590,0 +798,1 @@\n+        ShenandoahTimingsTracker v(ShenandoahPhaseTimings::final_mark_verify);\n@@ -597,6 +806,0 @@\n-      \/\/ Verify before arming for concurrent processing.\n-      \/\/ Otherwise, verification can trigger stack processing.\n-      if (ShenandoahVerify) {\n-        heap->verifier()->verify_during_evacuation();\n-      }\n-\n@@ -612,5 +815,6 @@\n-        heap->verifier()->verify_after_concmark();\n-      }\n-\n-      if (VerifyAfterGC) {\n-        Universe::verify();\n+        ShenandoahTimingsTracker v(ShenandoahPhaseTimings::final_mark_verify);\n+        if (has_in_place_promotions(heap)) {\n+          heap->verifier()->verify_after_concmark_with_promotions();\n+        } else {\n+          heap->verifier()->verify_after_concmark();\n+        }\n@@ -620,0 +824,9 @@\n+\n+  {\n+    ShenandoahTimingsTracker timing(ShenandoahPhaseTimings::final_mark_propagate_gc_state);\n+    heap->propagate_gc_state_to_all_threads();\n+  }\n+}\n+\n+bool ShenandoahConcurrentGC::has_in_place_promotions(ShenandoahHeap* heap) {\n+  return heap->mode()->is_generational() && heap->old_generation()->has_in_place_promotions();\n@@ -625,1 +838,0 @@\n-\n@@ -627,7 +839,1 @@\n-  ShenandoahConcurrentEvacThreadClosure(OopClosure* oops);\n-  void do_thread(Thread* thread);\n-};\n-\n-ShenandoahConcurrentEvacThreadClosure::ShenandoahConcurrentEvacThreadClosure(OopClosure* oops) :\n-  _oops(oops) {\n-}\n+  explicit ShenandoahConcurrentEvacThreadClosure(OopClosure* oops) : _oops(oops) {}\n@@ -635,4 +841,5 @@\n-void ShenandoahConcurrentEvacThreadClosure::do_thread(Thread* thread) {\n-  JavaThread* const jt = JavaThread::cast(thread);\n-  StackWatermarkSet::finish_processing(jt, _oops, StackWatermarkKind::gc);\n-}\n+  void do_thread(Thread* thread) override {\n+    JavaThread* const jt = JavaThread::cast(thread);\n+    StackWatermarkSet::finish_processing(jt, _oops, StackWatermarkKind::gc);\n+  }\n+};\n@@ -645,1 +852,1 @@\n-  ShenandoahConcurrentEvacUpdateThreadTask(uint n_workers) :\n+  explicit ShenandoahConcurrentEvacUpdateThreadTask(uint n_workers) :\n@@ -650,1 +857,1 @@\n-  void work(uint worker_id) {\n+  void work(uint worker_id) override {\n@@ -660,1 +867,1 @@\n-  ShenandoahHeap* const heap = ShenandoahHeap::heap();\n+  const ShenandoahHeap* const heap = ShenandoahHeap::heap();\n@@ -675,1 +882,1 @@\n-  heap->ref_processor()->process_references(ShenandoahPhaseTimings::conc_weak_refs, heap->workers(), true \/* concurrent *\/);\n+  _generation->ref_processor()->process_references(ShenandoahPhaseTimings::conc_weak_refs, heap->workers(), true \/* concurrent *\/);\n@@ -702,2 +909,5 @@\n-      \/\/ Note: The obj is dead here. Do not touch it, just clear.\n-      ShenandoahHeap::atomic_clear_oop(p, obj);\n+      shenandoah_assert_generations_reconciled();\n+      if (_heap->is_in_active_generation(obj)) {\n+        \/\/ Note: The obj is dead here. Do not touch it, just clear.\n+        ShenandoahHeap::atomic_clear_oop(p, obj);\n+      }\n@@ -709,0 +919,1 @@\n+      shenandoah_assert_not_in_cset_except(p, resolved, _heap->cancelled_gc());\n@@ -710,3 +921,0 @@\n-      assert(_heap->cancelled_gc() ||\n-             _mark_context->is_marked(resolved) && !_heap->in_collection_set(resolved),\n-             \"Sanity\");\n@@ -772,2 +980,2 @@\n-    \/\/ cleanup the weak oops in CLD and determinate nmethod's unloading state, so that we\n-    \/\/ can cleanup immediate garbage sooner.\n+    \/\/ clean up the weak oops in CLD and determine nmethod's unloading state, so that we\n+    \/\/ can clean up immediate garbage sooner.\n@@ -798,1 +1006,0 @@\n-  \/\/ Concurrent weak root processing\n@@ -800,0 +1007,1 @@\n+    \/\/ Concurrent weak root processing\n@@ -806,1 +1014,0 @@\n-  \/\/ Perform handshake to flush out dead oops\n@@ -808,0 +1015,7 @@\n+    \/\/ It is possible for mutators executing the load reference barrier to have\n+    \/\/ loaded an oop through a weak handle that has since been nulled out by\n+    \/\/ weak root processing. Handshaking here forces them to complete the\n+    \/\/ barrier before the GC cycle continues and does something that would\n+    \/\/ change the evaluation of the barrier (for example, resetting the TAMS\n+    \/\/ on trashed regions could make an oop appear to be marked _after_ the\n+    \/\/ region has been recycled).\n@@ -895,1 +1109,4 @@\n-  ShenandoahHeap::heap()->free_set()->recycle_trash();\n+  ShenandoahWorkerScope scope(ShenandoahHeap::heap()->workers(),\n+                              ShenandoahWorkerPolicy::calc_workers_for_conc_cleanup(),\n+                              \"cleanup early.\");\n+  ShenandoahHeap::heap()->recycle_trash();\n@@ -902,1 +1119,1 @@\n-void ShenandoahConcurrentGC::op_init_updaterefs() {\n+void ShenandoahConcurrentGC::op_init_update_refs() {\n@@ -904,5 +1121,4 @@\n-  heap->set_evacuation_in_progress(false);\n-  heap->set_concurrent_weak_root_in_progress(false);\n-  heap->prepare_update_heap_references(true \/*concurrent*\/);\n-  heap->set_update_refs_in_progress(true);\n-\n+  if (ShenandoahVerify) {\n+    ShenandoahTimingsTracker v(ShenandoahPhaseTimings::init_update_refs_verify);\n+    heap->verifier()->verify_before_update_refs();\n+  }\n@@ -910,1 +1126,1 @@\n-    heap->pacer()->setup_for_updaterefs();\n+    heap->pacer()->setup_for_update_refs();\n@@ -914,1 +1130,1 @@\n-void ShenandoahConcurrentGC::op_updaterefs() {\n+void ShenandoahConcurrentGC::op_update_refs() {\n@@ -943,1 +1159,1 @@\n-void ShenandoahConcurrentGC::op_final_updaterefs() {\n+void ShenandoahConcurrentGC::op_final_update_refs() {\n@@ -953,1 +1169,1 @@\n-    heap->clear_cancelled_gc();\n+    heap->clear_cancelled_gc(true \/* clear oom handler *\/);\n@@ -961,0 +1177,2 @@\n+  \/\/ If we are running in generational mode and this is an aging cycle, this will also age active\n+  \/\/ regions that haven't been used for allocation.\n@@ -966,0 +1184,23 @@\n+  if (heap->mode()->is_generational() && heap->is_concurrent_old_mark_in_progress()) {\n+    \/\/ When the SATB barrier is left on to support concurrent old gen mark, it may pick up writes to\n+    \/\/ objects in the collection set. After those objects are evacuated, the pointers in the\n+    \/\/ SATB are no longer safe. Once we have finished update references, we are guaranteed that\n+    \/\/ no more writes to the collection set are possible.\n+    \/\/\n+    \/\/ This will transfer any old pointers in _active_ regions from the SATB to the old gen\n+    \/\/ mark queues. All other pointers will be discarded. This would also discard any pointers\n+    \/\/ in old regions that were included in a mixed evacuation. We aren't using the SATB filter\n+    \/\/ methods here because we cannot control when they execute. If the SATB filter runs _after_\n+    \/\/ a region has been recycled, we will not be able to detect the bad pointer.\n+    \/\/\n+    \/\/ We are not concerned about skipping this step in abbreviated cycles because regions\n+    \/\/ with no live objects cannot have been written to and so cannot have entries in the SATB\n+    \/\/ buffers.\n+    ShenandoahGCPhase phase(ShenandoahPhaseTimings::final_update_refs_transfer_satb);\n+    heap->old_generation()->transfer_pointers_from_satb();\n+\n+    \/\/ Aging_cycle is only relevant during evacuation cycle for individual objects and during final mark for\n+    \/\/ entire regions.  Both of these relevant operations occur before final update refs.\n+    ShenandoahGenerationalHeap::heap()->set_aging_cycle(false);\n+  }\n+\n@@ -967,1 +1208,2 @@\n-    heap->verifier()->verify_after_updaterefs();\n+    ShenandoahTimingsTracker v(ShenandoahPhaseTimings::final_update_refs_verify);\n+    heap->verifier()->verify_after_update_refs();\n@@ -975,0 +1217,27 @@\n+\n+  {\n+    ShenandoahTimingsTracker timing(ShenandoahPhaseTimings::final_update_refs_propagate_gc_state);\n+    heap->propagate_gc_state_to_all_threads();\n+  }\n+}\n+\n+bool ShenandoahConcurrentGC::entry_final_roots() {\n+  ShenandoahHeap* const heap = ShenandoahHeap::heap();\n+  TraceCollectorStats tcs(heap->monitoring_support()->concurrent_collection_counters());\n+\n+\n+  const char* msg = conc_final_roots_event_message();\n+  ShenandoahConcurrentPhase gc_phase(msg, ShenandoahPhaseTimings::conc_final_roots);\n+  EventMark em(\"%s\", msg);\n+  ShenandoahWorkerScope scope(heap->workers(),\n+                              ShenandoahWorkerPolicy::calc_workers_for_conc_evac(),\n+                              msg);\n+\n+  if (!heap->mode()->is_generational()) {\n+    heap->concurrent_final_roots();\n+  } else {\n+    if (!complete_abbreviated_cycle()) {\n+      return false;\n+    }\n+  }\n+  return true;\n@@ -977,2 +1246,4 @@\n-void ShenandoahConcurrentGC::op_final_roots() {\n-  ShenandoahHeap::heap()->set_concurrent_weak_root_in_progress(false);\n+void ShenandoahConcurrentGC::op_verify_final_roots() {\n+  if (VerifyAfterGC) {\n+    Universe::verify();\n+  }\n@@ -982,1 +1253,22 @@\n-  ShenandoahHeap::heap()->free_set()->recycle_trash();\n+  ShenandoahWorkerScope scope(ShenandoahHeap::heap()->workers(),\n+                              ShenandoahWorkerPolicy::calc_workers_for_conc_cleanup(),\n+                              \"cleanup complete.\");\n+  ShenandoahHeap::heap()->recycle_trash();\n+}\n+\n+void ShenandoahConcurrentGC::op_reset_after_collect() {\n+  ShenandoahWorkerScope scope(ShenandoahHeap::heap()->workers(),\n+                          ShenandoahWorkerPolicy::calc_workers_for_conc_reset(),\n+                          \"reset after collection.\");\n+\n+  ShenandoahHeap* const heap = ShenandoahHeap::heap();\n+  if (heap->mode()->is_generational()) {\n+    \/\/ If we are in the midst of an old gc bootstrap or an old marking, we want to leave the mark bit map of\n+    \/\/ the young generation intact. In particular, reference processing in the old generation may potentially\n+    \/\/ need the reachability of a young generation referent of a Reference object in the old generation.\n+    if (!_do_old_gc_bootstrap && !heap->is_concurrent_old_mark_in_progress()) {\n+      heap->young_generation()->reset_mark_bitmap<false>();\n+    }\n+  } else {\n+    _generation->reset_mark_bitmap<false>();\n+  }\n@@ -997,1 +1289,1 @@\n-    return \"Pause Init Mark (unload classes)\";\n+    SHENANDOAH_RETURN_EVENT_MESSAGE(_generation->type(), \"Pause Init Mark\", \" (unload classes)\");\n@@ -999,1 +1291,1 @@\n-    return \"Pause Init Mark\";\n+    SHENANDOAH_RETURN_EVENT_MESSAGE(_generation->type(), \"Pause Init Mark\", \"\");\n@@ -1005,1 +1297,3 @@\n-  assert(!heap->has_forwarded_objects(), \"Should not have forwarded objects here\");\n+  assert(!heap->has_forwarded_objects() || heap->is_concurrent_old_mark_in_progress(),\n+         \"Should not have forwarded objects during final mark, unless old gen concurrent mark is running\");\n+\n@@ -1007,1 +1301,1 @@\n-    return \"Pause Final Mark (unload classes)\";\n+    SHENANDOAH_RETURN_EVENT_MESSAGE(_generation->type(), \"Pause Final Mark\", \" (unload classes)\");\n@@ -1009,1 +1303,1 @@\n-    return \"Pause Final Mark\";\n+    SHENANDOAH_RETURN_EVENT_MESSAGE(_generation->type(), \"Pause Final Mark\", \"\");\n@@ -1015,1 +1309,2 @@\n-  assert(!heap->has_forwarded_objects(), \"Should not have forwarded objects here\");\n+  assert(!heap->has_forwarded_objects() || heap->is_concurrent_old_mark_in_progress(),\n+         \"Should not have forwarded objects concurrent mark, unless old gen concurrent mark is running\");\n@@ -1017,1 +1312,65 @@\n-    return \"Concurrent marking (unload classes)\";\n+    SHENANDOAH_RETURN_EVENT_MESSAGE(_generation->type(), \"Concurrent marking\", \" (unload classes)\");\n+  } else {\n+    SHENANDOAH_RETURN_EVENT_MESSAGE(_generation->type(), \"Concurrent marking\", \"\");\n+  }\n+}\n+\n+const char* ShenandoahConcurrentGC::conc_reset_event_message() const {\n+  if (ShenandoahHeap::heap()->unload_classes()) {\n+    SHENANDOAH_RETURN_EVENT_MESSAGE(_generation->type(), \"Concurrent reset\", \" (unload classes)\");\n+  } else {\n+    SHENANDOAH_RETURN_EVENT_MESSAGE(_generation->type(), \"Concurrent reset\", \"\");\n+  }\n+}\n+\n+const char* ShenandoahConcurrentGC::conc_reset_after_collect_event_message() const {\n+  if (ShenandoahHeap::heap()->unload_classes()) {\n+    SHENANDOAH_RETURN_EVENT_MESSAGE(_generation->type(), \"Concurrent reset after collect\", \" (unload classes)\");\n+  } else {\n+    SHENANDOAH_RETURN_EVENT_MESSAGE(_generation->type(), \"Concurrent reset after collect\", \"\");\n+  }\n+}\n+\n+const char* ShenandoahConcurrentGC::verify_final_roots_event_message() const {\n+  if (ShenandoahHeap::heap()->unload_classes()) {\n+    SHENANDOAH_RETURN_EVENT_MESSAGE(_generation->type(), \"Pause Verify Final Roots\", \" (unload classes)\");\n+  } else {\n+    SHENANDOAH_RETURN_EVENT_MESSAGE(_generation->type(), \"Pause Verify Final Roots\", \"\");\n+  }\n+}\n+\n+const char* ShenandoahConcurrentGC::conc_final_roots_event_message() const {\n+  if (ShenandoahHeap::heap()->unload_classes()) {\n+    SHENANDOAH_RETURN_EVENT_MESSAGE(_generation->type(), \"Concurrent Final Roots\", \" (unload classes)\");\n+  } else {\n+    SHENANDOAH_RETURN_EVENT_MESSAGE(_generation->type(), \"Concurrent Final Roots\", \"\");\n+  }\n+}\n+\n+const char* ShenandoahConcurrentGC::conc_weak_refs_event_message() const {\n+  if (ShenandoahHeap::heap()->unload_classes()) {\n+    SHENANDOAH_RETURN_EVENT_MESSAGE(_generation->type(), \"Concurrent weak references\", \" (unload classes)\");\n+  } else {\n+    SHENANDOAH_RETURN_EVENT_MESSAGE(_generation->type(), \"Concurrent weak references\", \"\");\n+  }\n+}\n+\n+const char* ShenandoahConcurrentGC::conc_weak_roots_event_message() const {\n+  if (ShenandoahHeap::heap()->unload_classes()) {\n+    SHENANDOAH_RETURN_EVENT_MESSAGE(_generation->type(), \"Concurrent weak roots\", \" (unload classes)\");\n+  } else {\n+    SHENANDOAH_RETURN_EVENT_MESSAGE(_generation->type(), \"Concurrent weak roots\", \"\");\n+  }\n+}\n+\n+const char* ShenandoahConcurrentGC::conc_cleanup_event_message() const {\n+  if (ShenandoahHeap::heap()->unload_classes()) {\n+    SHENANDOAH_RETURN_EVENT_MESSAGE(_generation->type(), \"Concurrent cleanup\", \" (unload classes)\");\n+  } else {\n+    SHENANDOAH_RETURN_EVENT_MESSAGE(_generation->type(), \"Concurrent cleanup\", \"\");\n+  }\n+}\n+\n+const char* ShenandoahConcurrentGC::conc_init_update_refs_event_message() const {\n+  if (ShenandoahHeap::heap()->unload_classes()) {\n+    SHENANDOAH_RETURN_EVENT_MESSAGE(_generation->type(), \"Concurrent Init Update Refs\", \" (unload classes)\");\n@@ -1019,1 +1378,1 @@\n-    return \"Concurrent marking\";\n+    SHENANDOAH_RETURN_EVENT_MESSAGE(_generation->type(), \"Concurrent Init Update Refs\", \"\");\n","filename":"src\/hotspot\/share\/gc\/shenandoah\/shenandoahConcurrentGC.cpp","additions":480,"deletions":121,"binary":false,"changes":601,"status":"modified"},{"patch":"@@ -3,0 +3,1 @@\n+ * Copyright Amazon.com Inc. or its affiliates. All Rights Reserved.\n@@ -33,0 +34,2 @@\n+class ShenandoahGeneration;\n+\n@@ -45,0 +48,4 @@\n+protected:\n+  ShenandoahConcurrentMark    _mark;\n+  ShenandoahGeneration* const _generation;\n+\n@@ -46,2 +53,3 @@\n-  ShenandoahConcurrentMark  _mark;\n-  ShenandoahDegenPoint      _degen_point;\n+  ShenandoahDegenPoint        _degen_point;\n+  bool                        _abbreviated;\n+  const bool                  _do_old_gc_bootstrap;\n@@ -50,2 +58,3 @@\n-  ShenandoahConcurrentGC();\n-  bool collect(GCCause::Cause cause);\n+  ShenandoahConcurrentGC(ShenandoahGeneration* generation, bool do_old_gc_bootstrap);\n+\n+  bool collect(GCCause::Cause cause) override;\n@@ -54,3 +63,6 @@\n-  \/\/ Cancel ongoing concurrent GC\n-  static void cancel();\n-private:\n+  void entry_concurrent_update_refs_prepare(ShenandoahHeap* heap);\n+\n+  \/\/ Return true if this cycle found enough immediate garbage to skip evacuation\n+  bool abbreviated() const { return _abbreviated; }\n+\n+protected:\n@@ -61,3 +73,3 @@\n-  void vmop_entry_init_updaterefs();\n-  void vmop_entry_final_updaterefs();\n-  void vmop_entry_final_roots();\n+  void vmop_entry_init_update_refs();\n+  void vmop_entry_final_update_refs();\n+  void vmop_entry_verify_final_roots();\n@@ -66,1 +78,1 @@\n-  \/\/ and workers for net VM operation\n+  \/\/ and workers for next VM operation\n@@ -69,3 +81,3 @@\n-  void entry_init_updaterefs();\n-  void entry_final_updaterefs();\n-  void entry_final_roots();\n+  void entry_init_update_refs();\n+  void entry_final_update_refs();\n+  void entry_verify_final_roots();\n@@ -77,0 +89,1 @@\n+  void entry_scan_remembered_set();\n@@ -86,1 +99,1 @@\n-  void entry_updaterefs();\n+  void entry_update_refs();\n@@ -89,0 +102,6 @@\n+  \/\/ This is the last phase of a cycle which performs no evacuations\n+  bool entry_final_roots();\n+\n+  \/\/ Called when the collection set is empty, but the generational mode has regions to promote in place\n+  void entry_promote_in_place() const;\n+\n@@ -94,1 +113,1 @@\n-  void op_final_mark();\n+  virtual void op_final_mark();\n@@ -102,2 +121,2 @@\n-  void op_init_updaterefs();\n-  void op_updaterefs();\n+  void op_init_update_refs();\n+  void op_update_refs();\n@@ -105,2 +124,3 @@\n-  void op_final_updaterefs();\n-  void op_final_roots();\n+  void op_final_update_refs();\n+\n+  void op_verify_final_roots();\n@@ -108,0 +128,1 @@\n+  void op_reset_after_collect();\n@@ -109,0 +130,7 @@\n+  \/\/ Check GC cancellation and abort concurrent GC\n+  bool check_cancellation_and_abort(ShenandoahDegenPoint point);\n+\n+  \/\/ Called when concurrent GC succeeds.\n+  void entry_reset_after_collect();\n+\n+private:\n@@ -111,0 +139,4 @@\n+  bool complete_abbreviated_cycle();\n+\n+  static bool has_in_place_promotions(ShenandoahHeap* heap);\n+\n@@ -115,0 +147,2 @@\n+  const char* verify_final_roots_event_message() const;\n+  const char* conc_final_roots_event_message() const;\n@@ -116,3 +150,6 @@\n-\n-  \/\/ Check GC cancellation and abort concurrent GC\n-  bool check_cancellation_and_abort(ShenandoahDegenPoint point);\n+  const char* conc_reset_event_message() const;\n+  const char* conc_reset_after_collect_event_message() const;\n+  const char* conc_weak_refs_event_message() const;\n+  const char* conc_weak_roots_event_message() const;\n+  const char* conc_cleanup_event_message() const;\n+  const char* conc_init_update_refs_event_message() const;\n","filename":"src\/hotspot\/share\/gc\/shenandoah\/shenandoahConcurrentGC.hpp","additions":60,"deletions":23,"binary":false,"changes":83,"status":"modified"},{"patch":"@@ -3,0 +3,1 @@\n+ * Copyright Amazon.com Inc. or its affiliates. All Rights Reserved.\n@@ -33,0 +34,1 @@\n+#include \"gc\/shenandoah\/shenandoahGeneration.hpp\"\n@@ -42,0 +44,1 @@\n+#include \"gc\/shenandoah\/shenandoahScanRemembered.inline.hpp\"\n@@ -47,0 +50,1 @@\n+template <ShenandoahGenerationType GENERATION>\n@@ -60,0 +64,1 @@\n+    ShenandoahWorkerTimingsTracker timer(ShenandoahPhaseTimings::conc_mark, ShenandoahPhaseTimings::ParallelMark, worker_id, true);\n@@ -61,1 +66,5 @@\n-    ShenandoahReferenceProcessor* rp = heap->ref_processor();\n+    \/\/ Do not use active_generation() : we must use the gc_generation() set by\n+    \/\/ ShenandoahGCScope on the ControllerThread's stack; no safepoint may\n+    \/\/ intervene to update active_generation, so we can't\n+    \/\/ shenandoah_assert_generations_reconciled() here.\n+    ShenandoahReferenceProcessor* rp = heap->gc_generation()->ref_processor();\n@@ -64,2 +73,1 @@\n-    _cm->mark_loop(worker_id, _terminator, rp,\n-                   true \/*cancellable*\/,\n+    _cm->mark_loop(worker_id, _terminator, rp, GENERATION, true \/*cancellable*\/,\n@@ -74,1 +82,0 @@\n-  OopClosure* const _cl;\n@@ -77,3 +84,2 @@\n-  ShenandoahSATBAndRemarkThreadsClosure(SATBMarkQueueSet& satb_qset, OopClosure* cl) :\n-    _satb_qset(satb_qset),\n-    _cl(cl)  {}\n+  explicit ShenandoahSATBAndRemarkThreadsClosure(SATBMarkQueueSet& satb_qset) :\n+    _satb_qset(satb_qset) {}\n@@ -81,1 +87,1 @@\n-  void do_thread(Thread* thread) {\n+  void do_thread(Thread* thread) override {\n@@ -84,6 +90,0 @@\n-    if (thread->is_Java_thread()) {\n-      if (_cl != nullptr) {\n-        ResourceMark rm;\n-        thread->oops_do(_cl, nullptr);\n-      }\n-    }\n@@ -93,0 +93,1 @@\n+template <ShenandoahGenerationType GENERATION>\n@@ -108,1 +109,0 @@\n-    ShenandoahReferenceProcessor* rp = heap->ref_processor();\n@@ -110,0 +110,2 @@\n+    ShenandoahReferenceProcessor* rp = heap->gc_generation()->ref_processor();\n+    shenandoah_assert_generations_reconciled();\n@@ -114,0 +116,1 @@\n+      ShenandoahObjToScanQueue* old_q = _cm->get_old_queue(worker_id);\n@@ -115,1 +118,1 @@\n-      ShenandoahSATBBufferClosure cl(q);\n+      ShenandoahSATBBufferClosure<GENERATION> cl(q, old_q);\n@@ -120,3 +123,1 @@\n-      ShenandoahMarkRefsClosure             mark_cl(q, rp);\n-      ShenandoahSATBAndRemarkThreadsClosure tc(satb_mq_set,\n-                                               ShenandoahIUBarrier ? &mark_cl : nullptr);\n+      ShenandoahSATBAndRemarkThreadsClosure tc(satb_mq_set);\n@@ -125,2 +126,1 @@\n-    _cm->mark_loop(worker_id, _terminator, rp,\n-                   false \/*not cancellable*\/,\n+    _cm->mark_loop(worker_id, _terminator, rp, GENERATION, false \/*not cancellable*\/,\n@@ -133,2 +133,2 @@\n-ShenandoahConcurrentMark::ShenandoahConcurrentMark() :\n-  ShenandoahMark() {}\n+ShenandoahConcurrentMark::ShenandoahConcurrentMark(ShenandoahGeneration* generation) :\n+  ShenandoahMark(generation) {}\n@@ -137,0 +137,1 @@\n+template <ShenandoahGenerationType GENERATION>\n@@ -142,0 +143,1 @@\n+  ShenandoahObjToScanQueueSet* const  _old_queue_set;\n@@ -146,0 +148,1 @@\n+                                    ShenandoahObjToScanQueueSet* old,\n@@ -152,4 +155,6 @@\n-ShenandoahMarkConcurrentRootsTask::ShenandoahMarkConcurrentRootsTask(ShenandoahObjToScanQueueSet* qs,\n-                                                                     ShenandoahReferenceProcessor* rp,\n-                                                                     ShenandoahPhaseTimings::Phase phase,\n-                                                                     uint nworkers) :\n+template <ShenandoahGenerationType GENERATION>\n+ShenandoahMarkConcurrentRootsTask<GENERATION>::ShenandoahMarkConcurrentRootsTask(ShenandoahObjToScanQueueSet* qs,\n+                                                                                 ShenandoahObjToScanQueueSet* old,\n+                                                                                 ShenandoahReferenceProcessor* rp,\n+                                                                                 ShenandoahPhaseTimings::Phase phase,\n+                                                                                 uint nworkers) :\n@@ -159,0 +164,1 @@\n+  _old_queue_set(old),\n@@ -163,1 +169,2 @@\n-void ShenandoahMarkConcurrentRootsTask::work(uint worker_id) {\n+template <ShenandoahGenerationType GENERATION>\n+void ShenandoahMarkConcurrentRootsTask<GENERATION>::work(uint worker_id) {\n@@ -166,1 +173,3 @@\n-  ShenandoahMarkRefsClosure cl(q, _rp);\n+  ShenandoahObjToScanQueue* old_q = (_old_queue_set == nullptr) ?\n+          nullptr : _old_queue_set->queue(worker_id);\n+  ShenandoahMarkRefsClosure<GENERATION> cl(q, _rp, old_q);\n@@ -174,2 +183,0 @@\n-  TASKQUEUE_STATS_ONLY(task_queues()->reset_taskqueue_stats());\n-\n@@ -177,17 +184,30 @@\n-  ShenandoahReferenceProcessor* rp = heap->ref_processor();\n-  task_queues()->reserve(workers->active_workers());\n-  ShenandoahMarkConcurrentRootsTask task(task_queues(), rp, ShenandoahPhaseTimings::conc_mark_roots, workers->active_workers());\n-\n-  workers->run_task(&task);\n-}\n-\n-class ShenandoahFlushSATBHandshakeClosure : public HandshakeClosure {\n-private:\n-  SATBMarkQueueSet& _qset;\n-public:\n-  ShenandoahFlushSATBHandshakeClosure(SATBMarkQueueSet& qset) :\n-    HandshakeClosure(\"Shenandoah Flush SATB\"),\n-    _qset(qset) {}\n-\n-  void do_thread(Thread* thread) {\n-    _qset.flush_queue(ShenandoahThreadLocalData::satb_mark_queue(thread));\n+  ShenandoahReferenceProcessor* rp = _generation->ref_processor();\n+  _generation->reserve_task_queues(workers->active_workers());\n+  switch (_generation->type()) {\n+    case YOUNG: {\n+      ShenandoahMarkConcurrentRootsTask<YOUNG> task(task_queues(), old_task_queues(), rp,\n+                                                    ShenandoahPhaseTimings::conc_mark_roots, workers->active_workers());\n+      workers->run_task(&task);\n+      break;\n+    }\n+    case GLOBAL: {\n+      assert(old_task_queues() == nullptr, \"Global mark should not have old gen mark queues\");\n+      ShenandoahMarkConcurrentRootsTask<GLOBAL> task(task_queues(), nullptr, rp,\n+                                                     ShenandoahPhaseTimings::conc_mark_roots, workers->active_workers());\n+      workers->run_task(&task);\n+      break;\n+    }\n+    case NON_GEN: {\n+      assert(old_task_queues() == nullptr, \"Non-generational mark should not have old gen mark queues\");\n+      ShenandoahMarkConcurrentRootsTask<NON_GEN> task(task_queues(), nullptr, rp,\n+                                                      ShenandoahPhaseTimings::conc_mark_roots, workers->active_workers());\n+      workers->run_task(&task);\n+      break;\n+    }\n+    case OLD: {\n+      \/\/ We use a YOUNG generation cycle to bootstrap concurrent old marking.\n+      ShouldNotReachHere();\n+      break;\n+    }\n+    default:\n+      ShouldNotReachHere();\n@@ -195,1 +215,1 @@\n-};\n+}\n@@ -203,0 +223,1 @@\n+  ShenandoahGenerationType gen_type = _generation->type();\n@@ -206,3 +227,28 @@\n-    TaskTerminator terminator(nworkers, task_queues());\n-    ShenandoahConcurrentMarkingTask task(this, &terminator);\n-    workers->run_task(&task);\n+    switch (gen_type) {\n+      case YOUNG: {\n+        TaskTerminator terminator(nworkers, task_queues());\n+        ShenandoahConcurrentMarkingTask<YOUNG> task(this, &terminator);\n+        workers->run_task(&task);\n+        break;\n+      }\n+      case OLD: {\n+        TaskTerminator terminator(nworkers, task_queues());\n+        ShenandoahConcurrentMarkingTask<OLD> task(this, &terminator);\n+        workers->run_task(&task);\n+        break;\n+      }\n+      case GLOBAL: {\n+        TaskTerminator terminator(nworkers, task_queues());\n+        ShenandoahConcurrentMarkingTask<GLOBAL> task(this, &terminator);\n+        workers->run_task(&task);\n+        break;\n+      }\n+      case NON_GEN: {\n+        TaskTerminator terminator(nworkers, task_queues());\n+        ShenandoahConcurrentMarkingTask<NON_GEN> task(this, &terminator);\n+        workers->run_task(&task);\n+        break;\n+      }\n+      default:\n+        ShouldNotReachHere();\n+    }\n@@ -216,1 +262,4 @@\n-    Handshake::execute(&flush_satb);\n+    {\n+      ShenandoahTimingsTracker t(ShenandoahPhaseTimings::conc_mark_satb_flush, true);\n+      Handshake::execute(&flush_satb);\n+    }\n@@ -232,2 +281,1 @@\n-  TASKQUEUE_STATS_ONLY(task_queues()->print_taskqueue_stats());\n-  TASKQUEUE_STATS_ONLY(task_queues()->reset_taskqueue_stats());\n+  TASKQUEUE_STATS_ONLY(task_queues()->print_and_reset_taskqueue_stats(\"\"));\n@@ -235,3 +283,2 @@\n-  ShenandoahHeap* const heap = ShenandoahHeap::heap();\n-  heap->set_concurrent_mark_in_progress(false);\n-  heap->mark_complete_marking_context();\n+  _generation->set_concurrent_mark_in_progress(false);\n+  _generation->set_mark_complete();\n@@ -257,2 +304,0 @@\n-  ShenandoahFinalMarkingTask task(this, &terminator, ShenandoahStringDedup::is_enabled());\n-  heap->workers()->run_task(&task);\n@@ -260,2 +305,24 @@\n-  assert(task_queues()->is_empty(), \"Should be empty\");\n-}\n+  switch (_generation->type()) {\n+    case YOUNG:{\n+      ShenandoahFinalMarkingTask<YOUNG> task(this, &terminator, ShenandoahStringDedup::is_enabled());\n+      heap->workers()->run_task(&task);\n+      break;\n+    }\n+    case OLD:{\n+      ShenandoahFinalMarkingTask<OLD> task(this, &terminator, ShenandoahStringDedup::is_enabled());\n+      heap->workers()->run_task(&task);\n+      break;\n+    }\n+    case GLOBAL:{\n+      ShenandoahFinalMarkingTask<GLOBAL> task(this, &terminator, ShenandoahStringDedup::is_enabled());\n+      heap->workers()->run_task(&task);\n+      break;\n+    }\n+    case NON_GEN:{\n+      ShenandoahFinalMarkingTask<NON_GEN> task(this, &terminator, ShenandoahStringDedup::is_enabled());\n+      heap->workers()->run_task(&task);\n+      break;\n+    }\n+    default:\n+      ShouldNotReachHere();\n+  }\n@@ -264,4 +331,1 @@\n-void ShenandoahConcurrentMark::cancel() {\n-  clear();\n-  ShenandoahReferenceProcessor* rp = ShenandoahHeap::heap()->ref_processor();\n-  rp->abandon_partial_discovery();\n+  assert(task_queues()->is_empty(), \"Should be empty\");\n","filename":"src\/hotspot\/share\/gc\/shenandoah\/shenandoahConcurrentMark.cpp","additions":130,"deletions":66,"binary":false,"changes":196,"status":"modified"},{"patch":"@@ -28,0 +28,1 @@\n+#include \"gc\/shenandoah\/shenandoahGenerationType.hpp\"\n@@ -30,0 +31,1 @@\n+template <ShenandoahGenerationType GENERATION>\n@@ -31,0 +33,1 @@\n+template <ShenandoahGenerationType GENERATION>\n@@ -32,0 +35,1 @@\n+class ShenandoahGeneration;\n@@ -34,2 +38,2 @@\n-  friend class ShenandoahConcurrentMarkingTask;\n-  friend class ShenandoahFinalMarkingTask;\n+  template <ShenandoahGenerationType GENERATION> friend class ShenandoahConcurrentMarkingTask;\n+  template <ShenandoahGenerationType GENERATION> friend class ShenandoahFinalMarkingTask;\n@@ -38,1 +42,2 @@\n-  ShenandoahConcurrentMark();\n+  ShenandoahConcurrentMark(ShenandoahGeneration* generation);\n+\n@@ -41,0 +46,1 @@\n+\n@@ -43,0 +49,1 @@\n+\n@@ -46,2 +53,0 @@\n-  static void cancel();\n-\n","filename":"src\/hotspot\/share\/gc\/shenandoah\/shenandoahConcurrentMark.hpp","additions":10,"deletions":5,"binary":false,"changes":15,"status":"modified"},{"patch":"@@ -33,1 +33,1 @@\n-#include \"gc\/shenandoah\/shenandoahPhaseTimings.hpp\"\n+#include \"gc\/shenandoah\/shenandoahGeneration.hpp\"\n@@ -35,1 +35,0 @@\n-#include \"gc\/shenandoah\/shenandoahMark.inline.hpp\"\n@@ -37,2 +36,1 @@\n-#include \"gc\/shenandoah\/shenandoahOopClosures.inline.hpp\"\n-#include \"gc\/shenandoah\/shenandoahRootProcessor.inline.hpp\"\n+#include \"gc\/shenandoah\/shenandoahPacer.inline.hpp\"\n@@ -40,2 +38,0 @@\n-#include \"gc\/shenandoah\/shenandoahVMOperations.hpp\"\n-#include \"gc\/shenandoah\/shenandoahWorkerPolicy.hpp\"\n@@ -43,1 +39,2 @@\n-#include \"memory\/iterator.hpp\"\n+#include \"gc\/shenandoah\/mode\/shenandoahMode.hpp\"\n+#include \"logging\/log.hpp\"\n@@ -46,2 +43,0 @@\n-#include \"memory\/universe.hpp\"\n-#include \"runtime\/atomic.hpp\"\n@@ -50,4 +45,1 @@\n-  ConcurrentGCThread(),\n-  _alloc_failure_waiters_lock(Mutex::safepoint-2, \"ShenandoahAllocFailureGC_lock\", true),\n-  _gc_waiters_lock(Mutex::safepoint-2, \"ShenandoahRequestedGC_lock\", true),\n-  _periodic_task(this),\n+  ShenandoahController(),\n@@ -55,2 +47,1 @@\n-  _degen_point(ShenandoahGC::_degenerated_outside_cycle),\n-  _allocs_seen(0) {\n+  _degen_point(ShenandoahGC::_degenerated_outside_cycle) {\n@@ -58,1 +49,0 @@\n-  reset_gc_id();\n@@ -60,18 +50,0 @@\n-  _periodic_task.enroll();\n-  if (ShenandoahPacing) {\n-    _periodic_pacer_notify_task.enroll();\n-  }\n-}\n-\n-ShenandoahControlThread::~ShenandoahControlThread() {\n-  \/\/ This is here so that super is called.\n-}\n-\n-void ShenandoahPeriodicTask::task() {\n-  _thread->handle_force_counters_update();\n-  _thread->handle_counters_update();\n-}\n-\n-void ShenandoahPeriodicPacerNotify::task() {\n-  assert(ShenandoahPacing, \"Should not be here otherwise\");\n-  ShenandoahHeap::heap()->pacer()->notify_waiters();\n@@ -81,4 +53,3 @@\n-  ShenandoahHeap* heap = ShenandoahHeap::heap();\n-\n-  GCMode default_mode = concurrent_normal;\n-  GCCause::Cause default_cause = GCCause::_shenandoah_concurrent_gc;\n+  ShenandoahHeap* const heap = ShenandoahHeap::heap();\n+  const GCMode default_mode = concurrent_normal;\n+  const GCCause::Cause default_cause = GCCause::_shenandoah_concurrent_gc;\n@@ -87,1 +58,0 @@\n-  double last_shrink_time = os::elapsedTime();\n@@ -90,5 +60,7 @@\n-  \/\/ Shrink period avoids constantly polling regions for shrinking.\n-  \/\/ Having a period 10x lower than the delay would mean we hit the\n-  \/\/ shrinking with lag of less than 1\/10-th of true delay.\n-  \/\/ ShenandoahUncommitDelay is in msecs, but shrink_period is in seconds.\n-  double shrink_period = (double)ShenandoahUncommitDelay \/ 1000 \/ 10;\n+  ShenandoahCollectorPolicy* const policy = heap->shenandoah_policy();\n+  ShenandoahHeuristics* const heuristics = heap->heuristics();\n+  while (!should_terminate()) {\n+    const GCCause::Cause cancelled_cause = heap->cancelled_cause();\n+    if (cancelled_cause == GCCause::_shenandoah_stop_vm) {\n+      break;\n+    }\n@@ -96,3 +68,0 @@\n-  ShenandoahCollectorPolicy* policy = heap->shenandoah_policy();\n-  ShenandoahHeuristics* heuristics = heap->heuristics();\n-  while (!in_graceful_shutdown() && !should_terminate()) {\n@@ -100,8 +69,3 @@\n-    bool alloc_failure_pending = _alloc_failure_gc.is_set();\n-    bool is_gc_requested = _gc_requested.is_set();\n-    GCCause::Cause requested_gc_cause = _requested_gc_cause;\n-    bool explicit_gc_requested = is_gc_requested && is_explicit_gc(requested_gc_cause);\n-    bool implicit_gc_requested = is_gc_requested && !is_explicit_gc(requested_gc_cause);\n-\n-    \/\/ This control loop iteration have seen this much allocations.\n-    size_t allocs_seen = Atomic::xchg(&_allocs_seen, (size_t)0, memory_order_relaxed);\n+    const bool alloc_failure_pending = ShenandoahCollectorPolicy::is_allocation_failure(cancelled_cause);\n+    const bool is_gc_requested = _gc_requested.is_set();\n+    const GCCause::Cause requested_gc_cause = _requested_gc_cause;\n@@ -109,2 +73,2 @@\n-    \/\/ Check if we have seen a new target for soft max heap size.\n-    bool soft_max_changed = check_soft_max_changed();\n+    \/\/ This control loop iteration has seen this much allocation.\n+    const size_t allocs_seen = reset_allocs_seen();\n@@ -119,1 +83,1 @@\n-      log_info(gc)(\"Trigger: Handle Allocation Failure\");\n+      heuristics->log_trigger(\"Handle Allocation Failure\");\n@@ -136,2 +100,1 @@\n-\n-    } else if (explicit_gc_requested) {\n+    } else if (is_gc_requested) {\n@@ -139,2 +102,1 @@\n-      log_info(gc)(\"Trigger: Explicit GC request (%s)\", GCCause::to_string(cause));\n-\n+      heuristics->log_trigger(\"GC request (%s)\", GCCause::to_string(cause));\n@@ -143,7 +105,1 @@\n-      if (ExplicitGCInvokesConcurrent) {\n-        policy->record_explicit_to_concurrent();\n-        mode = default_mode;\n-        \/\/ Unload and clean up everything\n-        heap->set_unload_classes(heuristics->can_unload_classes());\n-      } else {\n-        policy->record_explicit_to_full();\n+      if (ShenandoahCollectorPolicy::should_run_full_gc(cause)) {\n@@ -151,9 +107,1 @@\n-      }\n-    } else if (implicit_gc_requested) {\n-      cause = requested_gc_cause;\n-      log_info(gc)(\"Trigger: Implicit GC request (%s)\", GCCause::to_string(cause));\n-\n-      heuristics->record_requested_gc();\n-\n-      if (ShenandoahImplicitGCInvokesConcurrent) {\n-        policy->record_implicit_to_concurrent();\n+      } else {\n@@ -161,1 +109,0 @@\n-\n@@ -164,3 +111,0 @@\n-      } else {\n-        policy->record_implicit_to_full();\n-        mode = stw_full;\n@@ -181,1 +125,1 @@\n-    if (alloc_failure_pending || implicit_gc_requested || explicit_gc_requested || ShenandoahAlwaysClearSoftRefs) {\n+    if (alloc_failure_pending || is_gc_requested || ShenandoahAlwaysClearSoftRefs) {\n@@ -185,1 +129,1 @@\n-    bool gc_requested = (mode != none);\n+    const bool gc_requested = (mode != none);\n@@ -189,0 +133,3 @@\n+      \/\/ Cannot uncommit bitmap slices during concurrent reset\n+      ShenandoahNoUncommitMark forbid_region_uncommit(heap);\n+\n@@ -192,0 +139,4 @@\n+      GCIdMark gc_id_mark;\n+\n+      heuristics->cancel_trigger_request();\n+\n@@ -198,1 +149,1 @@\n-      set_forced_counters_update(true);\n+      heap->set_forced_counters_update(true);\n@@ -201,4 +152,1 @@\n-      {\n-        ShenandoahHeapLocker locker(heap->lock());\n-        heap->free_set()->log_status();\n-      }\n+      heap->free_set()->log_status_under_lock();\n@@ -221,1 +169,1 @@\n-      if (explicit_gc_requested || implicit_gc_requested) {\n+      if (is_gc_requested) {\n@@ -225,2 +173,2 @@\n-      \/\/ If this was the allocation failure GC cycle, notify waiters about it\n-      if (alloc_failure_pending) {\n+      \/\/ If this cycle completed without being cancelled, notify waiters about it\n+      if (!heap->cancelled_gc()) {\n@@ -232,3 +180,1 @@\n-      {\n-        ShenandoahHeapLocker locker(heap->lock());\n-        heap->free_set()->log_status();\n+      heap->free_set()->log_status_under_lock();\n@@ -236,0 +182,1 @@\n+      {\n@@ -239,4 +186,2 @@\n-        Universe::heap()->update_capacity_and_used_at_gc();\n-\n-        \/\/ Signal that we have completed a visit to all live objects.\n-        Universe::heap()->record_whole_heap_examined_timestamp();\n+        ShenandoahHeapLocker locker(heap->lock());\n+        heap->update_capacity_and_used_at_gc();\n@@ -245,0 +190,3 @@\n+      \/\/ Signal that we have completed a visit to all live objects.\n+      heap->record_whole_heap_examined_timestamp();\n+\n@@ -247,2 +195,2 @@\n-      handle_force_counters_update();\n-      set_forced_counters_update(false);\n+      heap->handle_force_counters_update();\n+      heap->set_forced_counters_update(false);\n@@ -288,1 +236,1 @@\n-      \/\/ Allow allocators to know we have seen this much regions\n+      \/\/ Report to pacer that we have seen this many words allocated\n@@ -294,18 +242,8 @@\n-    double current = os::elapsedTime();\n-\n-    if (ShenandoahUncommit && (explicit_gc_requested || soft_max_changed || (current - last_shrink_time > shrink_period))) {\n-      \/\/ Explicit GC tries to uncommit everything down to min capacity.\n-      \/\/ Soft max change tries to uncommit everything down to target capacity.\n-      \/\/ Periodic uncommit tries to uncommit suitable regions down to min capacity.\n-\n-      double shrink_before = (explicit_gc_requested || soft_max_changed) ?\n-                             current :\n-                             current - (ShenandoahUncommitDelay \/ 1000.0);\n-\n-      size_t shrink_until = soft_max_changed ?\n-                             heap->soft_max_capacity() :\n-                             heap->min_capacity();\n-\n-      service_uncommit(shrink_before, shrink_until);\n-      heap->phase_timings()->flush_cycle_to_global();\n-      last_shrink_time = current;\n+    \/\/ Check if we have seen a new target for soft max heap size or if a gc was requested.\n+    \/\/ Either of these conditions will attempt to uncommit regions.\n+    if (ShenandoahUncommit) {\n+      if (heap->check_soft_max_changed()) {\n+        heap->notify_soft_max_changed();\n+      } else if (is_gc_requested) {\n+        heap->notify_explicit_gc_requested();\n+      }\n@@ -317,1 +255,2 @@\n-    if (_heap_changed.try_unset()) {\n+    const double current = os::elapsedTime();\n+    if (heap->has_changed()) {\n@@ -325,24 +264,0 @@\n-\n-  \/\/ Wait for the actual stop(), can't leave run_service() earlier.\n-  while (!should_terminate()) {\n-    os::naked_short_sleep(ShenandoahControlIntervalMin);\n-  }\n-}\n-\n-bool ShenandoahControlThread::check_soft_max_changed() const {\n-  ShenandoahHeap* heap = ShenandoahHeap::heap();\n-  size_t new_soft_max = Atomic::load(&SoftMaxHeapSize);\n-  size_t old_soft_max = heap->soft_max_capacity();\n-  if (new_soft_max != old_soft_max) {\n-    new_soft_max = MAX2(heap->min_capacity(), new_soft_max);\n-    new_soft_max = MIN2(heap->max_capacity(), new_soft_max);\n-    if (new_soft_max != old_soft_max) {\n-      log_info(gc)(\"Soft Max Heap Size: \" SIZE_FORMAT \"%s -> \" SIZE_FORMAT \"%s\",\n-                   byte_size_in_proper_unit(old_soft_max), proper_unit_for_byte_size(old_soft_max),\n-                   byte_size_in_proper_unit(new_soft_max), proper_unit_for_byte_size(new_soft_max)\n-      );\n-      heap->set_soft_max_capacity(new_soft_max);\n-      return true;\n-    }\n-  }\n-  return false;\n@@ -390,2 +305,1 @@\n-  GCIdMark gc_id_mark;\n-  ShenandoahGCSession session(cause);\n+  ShenandoahGCSession session(cause, heap->global_generation());\n@@ -395,1 +309,1 @@\n-  ShenandoahConcurrentGC gc;\n+  ShenandoahConcurrentGC gc(heap->global_generation(), false);\n@@ -397,3 +311,5 @@\n-    \/\/ Cycle is complete\n-    heap->heuristics()->record_success_concurrent();\n-    heap->shenandoah_policy()->record_success_concurrent();\n+    \/\/ Cycle is complete.  There were no failed allocation requests and no degeneration, so count this as good progress.\n+    heap->notify_gc_progress();\n+    heap->global_generation()->heuristics()->record_success_concurrent();\n+    heap->shenandoah_policy()->record_success_concurrent(false, gc.abbreviated());\n+    heap->log_heap_status(\"At end of GC\");\n@@ -403,0 +319,1 @@\n+    heap->log_heap_status(\"At end of cancelled GC\");\n@@ -409,2 +326,5 @@\n-    assert (is_alloc_failure_gc() || in_graceful_shutdown(), \"Cancel GC either for alloc failure GC, or gracefully exiting\");\n-    if (!in_graceful_shutdown()) {\n+    if (heap->cancelled_cause() == GCCause::_shenandoah_stop_vm) {\n+      return true;\n+    }\n+\n+    if (ShenandoahCollectorPolicy::is_allocation_failure(heap->cancelled_cause())) {\n@@ -414,0 +334,1 @@\n+      return true;\n@@ -415,1 +336,2 @@\n-    return true;\n+\n+    fatal(\"Unexpected reason for cancellation: %s\", GCCause::to_string(heap->cancelled_cause()));\n@@ -421,1 +343,1 @@\n-  \/\/ Nothing to do here.\n+  ShenandoahHeap::heap()->cancel_gc(GCCause::_shenandoah_stop_vm);\n@@ -425,2 +347,2 @@\n-  GCIdMark gc_id_mark;\n-  ShenandoahGCSession session(cause);\n+  ShenandoahHeap* const heap = ShenandoahHeap::heap();\n+  ShenandoahGCSession session(cause, heap->global_generation());\n@@ -430,4 +352,0 @@\n-\n-  ShenandoahHeap* const heap = ShenandoahHeap::heap();\n-  heap->heuristics()->record_success_full();\n-  heap->shenandoah_policy()->record_success_full();\n@@ -438,7 +356,0 @@\n-\n-  GCIdMark gc_id_mark;\n-  ShenandoahGCSession session(cause);\n-\n-  ShenandoahDegenGC gc(point);\n-  gc.collect(cause);\n-\n@@ -446,6 +357,1 @@\n-  heap->heuristics()->record_success_degenerated();\n-  heap->shenandoah_policy()->record_success_degenerated();\n-}\n-\n-void ShenandoahControlThread::service_uncommit(double shrink_before, size_t shrink_until) {\n-  ShenandoahHeap* heap = ShenandoahHeap::heap();\n+  ShenandoahGCSession session(cause, heap->global_generation());\n@@ -453,23 +359,2 @@\n-  \/\/ Determine if there is work to do. This avoids taking heap lock if there is\n-  \/\/ no work available, avoids spamming logs with superfluous logging messages,\n-  \/\/ and minimises the amount of work while locks are taken.\n-\n-  if (heap->committed() <= shrink_until) return;\n-\n-  bool has_work = false;\n-  for (size_t i = 0; i < heap->num_regions(); i++) {\n-    ShenandoahHeapRegion *r = heap->get_region(i);\n-    if (r->is_empty_committed() && (r->empty_time() < shrink_before)) {\n-      has_work = true;\n-      break;\n-    }\n-  }\n-\n-  if (has_work) {\n-    heap->entry_uncommit(shrink_before, shrink_until);\n-  }\n-}\n-\n-bool ShenandoahControlThread::is_explicit_gc(GCCause::Cause cause) const {\n-  return GCCause::is_user_requested_gc(cause) ||\n-         GCCause::is_serviceability_requested_gc(cause);\n+  ShenandoahDegenGC gc(point, heap->global_generation());\n+  gc.collect(cause);\n@@ -479,17 +364,1 @@\n-  assert(GCCause::is_user_requested_gc(cause) ||\n-         GCCause::is_serviceability_requested_gc(cause) ||\n-         cause == GCCause::_metadata_GC_clear_soft_refs ||\n-         cause == GCCause::_codecache_GC_aggressive ||\n-         cause == GCCause::_codecache_GC_threshold ||\n-         cause == GCCause::_full_gc_alot ||\n-         cause == GCCause::_wb_young_gc ||\n-         cause == GCCause::_wb_full_gc ||\n-         cause == GCCause::_wb_breakpoint ||\n-         cause == GCCause::_scavenge_alot,\n-         \"only requested GCs here: %s\", GCCause::to_string(cause));\n-\n-  if (is_explicit_gc(cause)) {\n-    if (!DisableExplicitGC) {\n-      handle_requested_gc(cause);\n-    }\n-  } else {\n+  if (ShenandoahCollectorPolicy::should_handle_requested_gc(cause)) {\n@@ -501,0 +370,15 @@\n+  if (should_terminate()) {\n+    log_info(gc)(\"Control thread is terminating, no more GCs\");\n+    return;\n+  }\n+\n+  \/\/ For normal requested GCs (System.gc) we want to block the caller. However,\n+  \/\/ for whitebox requested GC, we want to initiate the GC and return immediately.\n+  \/\/ The whitebox caller thread will arrange for itself to wait until the GC notifies\n+  \/\/ it that has reached the requested breakpoint (phase in the GC).\n+  if (cause == GCCause::_wb_breakpoint) {\n+    _requested_gc_cause = cause;\n+    _gc_requested.set();\n+    return;\n+  }\n+\n@@ -513,1 +397,1 @@\n-  while (current_gc_id < required_gc_id) {\n+  while (current_gc_id < required_gc_id && !should_terminate()) {\n@@ -520,24 +404,0 @@\n-    if (cause != GCCause::_wb_breakpoint) {\n-      ml.wait();\n-    }\n-    current_gc_id = get_gc_id();\n-  }\n-}\n-\n-void ShenandoahControlThread::handle_alloc_failure(ShenandoahAllocRequest& req) {\n-  ShenandoahHeap* heap = ShenandoahHeap::heap();\n-\n-  assert(current()->is_Java_thread(), \"expect Java thread here\");\n-\n-  if (try_set_alloc_failure_gc()) {\n-    \/\/ Only report the first allocation failure\n-    log_info(gc)(\"Failed to allocate %s, \" SIZE_FORMAT \"%s\",\n-                 req.type_string(),\n-                 byte_size_in_proper_unit(req.size() * HeapWordSize), proper_unit_for_byte_size(req.size() * HeapWordSize));\n-\n-    \/\/ Now that alloc failure GC is scheduled, we can abort everything else\n-    heap->cancel_gc(GCCause::_allocation_failure);\n-  }\n-\n-  MonitorLocker ml(&_alloc_failure_waiters_lock);\n-  while (is_alloc_failure_gc()) {\n@@ -545,0 +405,1 @@\n+    current_gc_id = get_gc_id();\n@@ -548,27 +409,0 @@\n-void ShenandoahControlThread::handle_alloc_failure_evac(size_t words) {\n-  ShenandoahHeap* heap = ShenandoahHeap::heap();\n-\n-  if (try_set_alloc_failure_gc()) {\n-    \/\/ Only report the first allocation failure\n-    log_info(gc)(\"Failed to allocate \" SIZE_FORMAT \"%s for evacuation\",\n-                 byte_size_in_proper_unit(words * HeapWordSize), proper_unit_for_byte_size(words * HeapWordSize));\n-  }\n-\n-  \/\/ Forcefully report allocation failure\n-  heap->cancel_gc(GCCause::_shenandoah_allocation_failure_evac);\n-}\n-\n-void ShenandoahControlThread::notify_alloc_failure_waiters() {\n-  _alloc_failure_gc.unset();\n-  MonitorLocker ml(&_alloc_failure_waiters_lock);\n-  ml.notify_all();\n-}\n-\n-bool ShenandoahControlThread::try_set_alloc_failure_gc() {\n-  return _alloc_failure_gc.try_set();\n-}\n-\n-bool ShenandoahControlThread::is_alloc_failure_gc() {\n-  return _alloc_failure_gc.is_set();\n-}\n-\n@@ -580,61 +414,0 @@\n-\n-void ShenandoahControlThread::handle_counters_update() {\n-  if (_do_counters_update.is_set()) {\n-    _do_counters_update.unset();\n-    ShenandoahHeap::heap()->monitoring_support()->update_counters();\n-  }\n-}\n-\n-void ShenandoahControlThread::handle_force_counters_update() {\n-  if (_force_counters_update.is_set()) {\n-    _do_counters_update.unset(); \/\/ reset these too, we do update now!\n-    ShenandoahHeap::heap()->monitoring_support()->update_counters();\n-  }\n-}\n-\n-void ShenandoahControlThread::notify_heap_changed() {\n-  \/\/ This is called from allocation path, and thus should be fast.\n-\n-  \/\/ Update monitoring counters when we took a new region. This amortizes the\n-  \/\/ update costs on slow path.\n-  if (_do_counters_update.is_unset()) {\n-    _do_counters_update.set();\n-  }\n-  \/\/ Notify that something had changed.\n-  if (_heap_changed.is_unset()) {\n-    _heap_changed.set();\n-  }\n-}\n-\n-void ShenandoahControlThread::pacing_notify_alloc(size_t words) {\n-  assert(ShenandoahPacing, \"should only call when pacing is enabled\");\n-  Atomic::add(&_allocs_seen, words, memory_order_relaxed);\n-}\n-\n-void ShenandoahControlThread::set_forced_counters_update(bool value) {\n-  _force_counters_update.set_cond(value);\n-}\n-\n-void ShenandoahControlThread::reset_gc_id() {\n-  Atomic::store(&_gc_id, (size_t)0);\n-}\n-\n-void ShenandoahControlThread::update_gc_id() {\n-  Atomic::inc(&_gc_id);\n-}\n-\n-size_t ShenandoahControlThread::get_gc_id() {\n-  return Atomic::load(&_gc_id);\n-}\n-\n-void ShenandoahControlThread::start() {\n-  create_and_start();\n-}\n-\n-void ShenandoahControlThread::prepare_for_graceful_shutdown() {\n-  _graceful_shutdown.set();\n-}\n-\n-bool ShenandoahControlThread::in_graceful_shutdown() {\n-  return _graceful_shutdown.is_set();\n-}\n","filename":"src\/hotspot\/share\/gc\/shenandoah\/shenandoahControlThread.cpp","additions":100,"deletions":327,"binary":false,"changes":427,"status":"modified"},{"patch":"@@ -31,0 +31,1 @@\n+#include \"gc\/shenandoah\/shenandoahGenerationType.hpp\"\n@@ -34,2 +35,0 @@\n-#include \"runtime\/task.hpp\"\n-#include \"utilities\/ostream.hpp\"\n@@ -37,20 +36,1 @@\n-\/\/ Periodic task is useful for doing asynchronous things that do not require (heap) locks,\n-\/\/ or synchronization with other parts of collector. These could run even when ShenandoahConcurrentThread\n-\/\/ is busy driving the GC cycle.\n-class ShenandoahPeriodicTask : public PeriodicTask {\n-private:\n-  ShenandoahControlThread* _thread;\n-public:\n-  ShenandoahPeriodicTask(ShenandoahControlThread* thread) :\n-          PeriodicTask(100), _thread(thread) {}\n-  virtual void task();\n-};\n-\n-\/\/ Periodic task to notify blocked paced waiters.\n-class ShenandoahPeriodicPacerNotify : public PeriodicTask {\n-public:\n-  ShenandoahPeriodicPacerNotify() : PeriodicTask(PeriodicTask::min_interval) {}\n-  virtual void task();\n-};\n-\n-class ShenandoahControlThread: public ConcurrentGCThread {\n+class ShenandoahControlThread: public ShenandoahController {\n@@ -67,13 +47,0 @@\n-  \/\/ While we could have a single lock for these, it may risk unblocking\n-  \/\/ GC waiters when alloc failure GC cycle finishes. We want instead\n-  \/\/ to make complete explicit cycle for for demanding customers.\n-  Monitor _alloc_failure_waiters_lock;\n-  Monitor _gc_waiters_lock;\n-  ShenandoahPeriodicTask _periodic_task;\n-  ShenandoahPeriodicPacerNotify _periodic_pacer_notify_task;\n-\n-public:\n-  void run_service();\n-  void stop_service();\n-\n-private:\n@@ -81,5 +48,0 @@\n-  ShenandoahSharedFlag _alloc_failure_gc;\n-  ShenandoahSharedFlag _graceful_shutdown;\n-  ShenandoahSharedFlag _heap_changed;\n-  ShenandoahSharedFlag _do_counters_update;\n-  ShenandoahSharedFlag _force_counters_update;\n@@ -89,5 +51,9 @@\n-  shenandoah_padding(0);\n-  volatile size_t _allocs_seen;\n-  shenandoah_padding(1);\n-  volatile size_t _gc_id;\n-  shenandoah_padding(2);\n+public:\n+  ShenandoahControlThread();\n+\n+  void run_service() override;\n+  void stop_service() override;\n+\n+  void request_gc(GCCause::Cause cause) override;\n+\n+private:\n@@ -99,9 +65,0 @@\n-  void service_uncommit(double shrink_before, size_t shrink_until);\n-\n-  bool try_set_alloc_failure_gc();\n-  void notify_alloc_failure_waiters();\n-  bool is_alloc_failure_gc();\n-\n-  void reset_gc_id();\n-  void update_gc_id();\n-  size_t get_gc_id();\n@@ -114,31 +71,0 @@\n-\n-  bool is_explicit_gc(GCCause::Cause cause) const;\n-\n-  bool check_soft_max_changed() const;\n-\n-public:\n-  \/\/ Constructor\n-  ShenandoahControlThread();\n-  ~ShenandoahControlThread();\n-\n-  \/\/ Handle allocation failure from normal allocation.\n-  \/\/ Blocks until memory is available.\n-  void handle_alloc_failure(ShenandoahAllocRequest& req);\n-\n-  \/\/ Handle allocation failure from evacuation path.\n-  \/\/ Optionally blocks while collector is handling the failure.\n-  void handle_alloc_failure_evac(size_t words);\n-\n-  void request_gc(GCCause::Cause cause);\n-\n-  void handle_counters_update();\n-  void handle_force_counters_update();\n-  void set_forced_counters_update(bool value);\n-\n-  void notify_heap_changed();\n-\n-  void pacing_notify_alloc(size_t words);\n-\n-  void start();\n-  void prepare_for_graceful_shutdown();\n-  bool in_graceful_shutdown();\n","filename":"src\/hotspot\/share\/gc\/shenandoah\/shenandoahControlThread.hpp","additions":11,"deletions":85,"binary":false,"changes":96,"status":"modified"},{"patch":"@@ -0,0 +1,84 @@\n+\/*\n+ * Copyright Amazon.com Inc. or its affiliates. All Rights Reserved.\n+ * DO NOT ALTER OR REMOVE COPYRIGHT NOTICES OR THIS FILE HEADER.\n+ *\n+ * This code is free software; you can redistribute it and\/or modify it\n+ * under the terms of the GNU General Public License version 2 only, as\n+ * published by the Free Software Foundation.\n+ *\n+ * This code is distributed in the hope that it will be useful, but WITHOUT\n+ * ANY WARRANTY; without even the implied warranty of MERCHANTABILITY or\n+ * FITNESS FOR A PARTICULAR PURPOSE.  See the GNU General Public License\n+ * version 2 for more details (a copy is included in the LICENSE file that\n+ * accompanied this code).\n+ *\n+ * You should have received a copy of the GNU General Public License version\n+ * 2 along with this work; if not, write to the Free Software Foundation,\n+ * Inc., 51 Franklin St, Fifth Floor, Boston, MA 02110-1301 USA.\n+ *\n+ * Please contact Oracle, 500 Oracle Parkway, Redwood Shores, CA 94065 USA\n+ * or visit www.oracle.com if you need additional information or have any\n+ * questions.\n+ *\n+ *\/\n+#include \"precompiled.hpp\"\n+\n+#include \"gc\/shared\/gc_globals.hpp\"\n+#include \"gc\/shenandoah\/shenandoahCollectorPolicy.hpp\"\n+#include \"gc\/shenandoah\/shenandoahController.hpp\"\n+#include \"gc\/shenandoah\/shenandoahHeap.hpp\"\n+#include \"gc\/shenandoah\/shenandoahHeapRegion.inline.hpp\"\n+\n+void ShenandoahController::pacing_notify_alloc(size_t words) {\n+  assert(ShenandoahPacing, \"should only call when pacing is enabled\");\n+  Atomic::add(&_allocs_seen, words, memory_order_relaxed);\n+}\n+\n+size_t ShenandoahController::reset_allocs_seen() {\n+  return Atomic::xchg(&_allocs_seen, (size_t)0, memory_order_relaxed);\n+}\n+\n+void ShenandoahController::update_gc_id() {\n+  Atomic::inc(&_gc_id);\n+}\n+\n+size_t ShenandoahController::get_gc_id() {\n+  return Atomic::load(&_gc_id);\n+}\n+\n+void ShenandoahController::handle_alloc_failure(const ShenandoahAllocRequest& req, bool block) {\n+  assert(current()->is_Java_thread(), \"expect Java thread here\");\n+\n+  const bool is_humongous = ShenandoahHeapRegion::requires_humongous(req.size());\n+  const GCCause::Cause cause = is_humongous ? GCCause::_shenandoah_humongous_allocation_failure : GCCause::_allocation_failure;\n+\n+  ShenandoahHeap* const heap = ShenandoahHeap::heap();\n+  if (heap->cancel_gc(cause)) {\n+    log_info(gc)(\"Failed to allocate %s, \" PROPERFMT, req.type_string(), PROPERFMTARGS(req.size() * HeapWordSize));\n+    request_gc(cause);\n+  }\n+\n+  if (block) {\n+    MonitorLocker ml(&_alloc_failure_waiters_lock);\n+    while (!should_terminate() && ShenandoahCollectorPolicy::is_allocation_failure(heap->cancelled_cause())) {\n+      ml.wait();\n+    }\n+  }\n+}\n+\n+void ShenandoahController::handle_alloc_failure_evac(size_t words) {\n+\n+  ShenandoahHeap* const heap = ShenandoahHeap::heap();\n+  const bool is_humongous = ShenandoahHeapRegion::requires_humongous(words);\n+  const GCCause::Cause cause = is_humongous ? GCCause::_shenandoah_humongous_allocation_failure : GCCause::_shenandoah_allocation_failure_evac;\n+\n+  if (heap->cancel_gc(cause)) {\n+    log_info(gc)(\"Failed to allocate \" PROPERFMT \" for evacuation\", PROPERFMTARGS(words * HeapWordSize));\n+  }\n+}\n+\n+void ShenandoahController::notify_alloc_failure_waiters() {\n+  MonitorLocker ml(&_alloc_failure_waiters_lock);\n+  ml.notify_all();\n+}\n+\n","filename":"src\/hotspot\/share\/gc\/shenandoah\/shenandoahController.cpp","additions":84,"deletions":0,"binary":false,"changes":84,"status":"added"},{"patch":"@@ -0,0 +1,90 @@\n+\/*\n+ * Copyright Amazon.com Inc. or its affiliates. All Rights Reserved.\n+ * DO NOT ALTER OR REMOVE COPYRIGHT NOTICES OR THIS FILE HEADER.\n+ *\n+ * This code is free software; you can redistribute it and\/or modify it\n+ * under the terms of the GNU General Public License version 2 only, as\n+ * published by the Free Software Foundation.\n+ *\n+ * This code is distributed in the hope that it will be useful, but WITHOUT\n+ * ANY WARRANTY; without even the implied warranty of MERCHANTABILITY or\n+ * FITNESS FOR A PARTICULAR PURPOSE.  See the GNU General Public License\n+ * version 2 for more details (a copy is included in the LICENSE file that\n+ * accompanied this code).\n+ *\n+ * You should have received a copy of the GNU General Public License version\n+ * 2 along with this work; if not, write to the Free Software Foundation,\n+ * Inc., 51 Franklin St, Fifth Floor, Boston, MA 02110-1301 USA.\n+ *\n+ * Please contact Oracle, 500 Oracle Parkway, Redwood Shores, CA 94065 USA\n+ * or visit www.oracle.com if you need additional information or have any\n+ * questions.\n+ *\n+ *\/\n+\n+#ifndef SHARE_GC_SHENANDOAH_SHENANDOAHCONTROLLER_HPP\n+#define SHARE_GC_SHENANDOAH_SHENANDOAHCONTROLLER_HPP\n+\n+#include \"gc\/shared\/gcCause.hpp\"\n+#include \"gc\/shared\/concurrentGCThread.hpp\"\n+#include \"gc\/shenandoah\/shenandoahAllocRequest.hpp\"\n+#include \"gc\/shenandoah\/shenandoahSharedVariables.hpp\"\n+\n+\/**\n+ * This interface exposes methods necessary for the heap to interact\n+ * with the threads responsible for driving the collection cycle.\n+ *\/\n+class ShenandoahController: public ConcurrentGCThread {\n+private:\n+  shenandoah_padding(0);\n+  volatile size_t _allocs_seen;\n+  shenandoah_padding(1);\n+  \/\/ A monotonically increasing GC count.\n+  volatile size_t _gc_id;\n+  shenandoah_padding(2);\n+\n+protected:\n+  \/\/ While we could have a single lock for these, it may risk unblocking\n+  \/\/ GC waiters when alloc failure GC cycle finishes. We want instead\n+  \/\/ to make complete explicit cycle for demanding customers.\n+  Monitor _alloc_failure_waiters_lock;\n+  Monitor _gc_waiters_lock;\n+\n+  \/\/ Increments the internal GC count.\n+  void update_gc_id();\n+\n+public:\n+  ShenandoahController():\n+    _allocs_seen(0),\n+    _gc_id(0),\n+    _alloc_failure_waiters_lock(Mutex::safepoint-2, \"ShenandoahAllocFailureGC_lock\", true),\n+    _gc_waiters_lock(Mutex::safepoint-2, \"ShenandoahRequestedGC_lock\", true)\n+  { }\n+\n+  \/\/ Request a collection cycle. This handles \"explicit\" gc requests\n+  \/\/ like System.gc and \"implicit\" gc requests, like metaspace oom.\n+  virtual void request_gc(GCCause::Cause cause) = 0;\n+\n+  \/\/ This cancels the collection cycle and has an option to block\n+  \/\/ until another cycle completes successfully.\n+  void handle_alloc_failure(const ShenandoahAllocRequest& req, bool block);\n+\n+  \/\/ Invoked for allocation failures during evacuation. This cancels\n+  \/\/ the collection cycle without blocking.\n+  void handle_alloc_failure_evac(size_t words);\n+\n+  \/\/ Notify threads waiting for GC to complete.\n+  void notify_alloc_failure_waiters();\n+\n+  \/\/ This is called for every allocation. The control thread accumulates\n+  \/\/ this value when idle. During the gc cycle, the control resets it\n+  \/\/ and reports it to the pacer.\n+  void pacing_notify_alloc(size_t words);\n+\n+  \/\/ Zeros out the number of allocations seen since the last GC cycle.\n+  size_t reset_allocs_seen();\n+\n+  \/\/ Return the value of a monotonic increasing GC count, maintained by the control thread.\n+  size_t get_gc_id();\n+};\n+#endif \/\/ SHARE_GC_SHENANDOAH_SHENANDOAHCONTROLLER_HPP\n","filename":"src\/hotspot\/share\/gc\/shenandoah\/shenandoahController.hpp","additions":90,"deletions":0,"binary":false,"changes":90,"status":"added"},{"patch":"@@ -3,0 +3,1 @@\n+ * Copyright Amazon.com Inc. or its affiliates. All Rights Reserved.\n@@ -32,0 +33,2 @@\n+#include \"gc\/shenandoah\/shenandoahGeneration.hpp\"\n+#include \"gc\/shenandoah\/shenandoahGenerationalHeap.hpp\"\n@@ -35,0 +38,1 @@\n+#include \"gc\/shenandoah\/shenandoahOldGeneration.hpp\"\n@@ -40,0 +44,1 @@\n+#include \"gc\/shenandoah\/shenandoahYoungGeneration.hpp\"\n@@ -45,1 +50,1 @@\n-ShenandoahDegenGC::ShenandoahDegenGC(ShenandoahDegenPoint degen_point) :\n+ShenandoahDegenGC::ShenandoahDegenGC(ShenandoahDegenPoint degen_point, ShenandoahGeneration* generation) :\n@@ -47,1 +52,3 @@\n-  _degen_point(degen_point) {\n+  _degen_point(degen_point),\n+  _generation(generation),\n+  _abbreviated(false) {\n@@ -52,0 +59,7 @@\n+  ShenandoahHeap* heap = ShenandoahHeap::heap();\n+  if (heap->mode()->is_generational()) {\n+    bool is_bootstrap_gc = heap->old_generation()->is_bootstrapping();\n+    heap->mmu_tracker()->record_degenerated(GCId::current(), is_bootstrap_gc);\n+    const char* msg = is_bootstrap_gc? \"At end of Degenerated Bootstrap Old GC\": \"At end of Degenerated Young GC\";\n+    heap->log_heap_status(msg);\n+  }\n@@ -67,1 +81,0 @@\n-\n@@ -75,0 +88,4 @@\n+  {\n+    ShenandoahTimingsTracker timing(ShenandoahPhaseTimings::degen_gc_propagate_gc_state);\n+    heap->propagate_gc_state_to_all_threads();\n+  }\n@@ -82,1 +99,20 @@\n-  heap->clear_cancelled_gc();\n+  heap->clear_cancelled_gc(true \/* clear oom handler *\/);\n+\n+#ifdef ASSERT\n+  if (heap->mode()->is_generational()) {\n+    ShenandoahOldGeneration* old_generation = heap->old_generation();\n+    if (!heap->is_concurrent_old_mark_in_progress()) {\n+      \/\/ If we are not marking the old generation, there should be nothing in the old mark queues\n+      assert(old_generation->task_queues()->is_empty(), \"Old gen task queues should be empty\");\n+    }\n+\n+    if (_generation->is_global()) {\n+      \/\/ If we are in a global cycle, the old generation should not be marking. It is, however,\n+      \/\/ allowed to be holding regions for evacuation or coalescing.\n+      assert(old_generation->is_idle()\n+             || old_generation->is_doing_mixed_evacuations()\n+             || old_generation->is_preparing_for_mark(),\n+             \"Old generation cannot be in state: %s\", old_generation->state_name());\n+    }\n+  }\n+#endif\n@@ -98,7 +134,0 @@\n-      \/\/\n-\n-      \/\/ Degenerated from concurrent root mark, reset the flag for STW mark\n-      if (heap->is_concurrent_mark_in_progress()) {\n-        ShenandoahConcurrentMark::cancel();\n-        heap->set_concurrent_mark_in_progress(false);\n-      }\n@@ -108,1 +137,48 @@\n-      heap->set_unload_classes(heap->heuristics()->can_unload_classes());\n+      heap->set_unload_classes(_generation->heuristics()->can_unload_classes() &&\n+                                (!heap->mode()->is_generational() || _generation->is_global()));\n+\n+      if (heap->mode()->is_generational()) {\n+        \/\/ Clean the read table before swapping it. The end goal here is to have a clean\n+        \/\/ write table, and to have the read table updated with the previous write table.\n+        heap->old_generation()->card_scan()->mark_read_table_as_clean();\n+\n+        if (_generation->is_young()) {\n+          \/\/ Swap remembered sets for young\n+          _generation->swap_card_tables();\n+        }\n+      }\n+\n+    case _degenerated_roots:\n+      \/\/ Degenerated from concurrent root mark, reset the flag for STW mark\n+      if (!heap->mode()->is_generational()) {\n+        if (heap->is_concurrent_mark_in_progress()) {\n+          heap->cancel_concurrent_mark();\n+        }\n+      } else {\n+        if (_generation->is_concurrent_mark_in_progress()) {\n+          \/\/ We want to allow old generation marking to be punctuated by young collections\n+          \/\/ (even if they have degenerated). If this is a global cycle, we'd have cancelled\n+          \/\/ the entire old gc before coming into this switch. Note that cancel_marking on\n+          \/\/ the generation does NOT abandon incomplete SATB buffers as cancel_concurrent_mark does.\n+          \/\/ We need to separate out the old pointers which is done below.\n+          _generation->cancel_marking();\n+        }\n+\n+        if (heap->is_concurrent_mark_in_progress()) {\n+          \/\/ If either old or young marking is in progress, the SATB barrier will be enabled.\n+          \/\/ The SATB buffer may hold a mix of old and young pointers. The old pointers need to be\n+          \/\/ transferred to the old generation mark queues and the young pointers are NOT part\n+          \/\/ of this snapshot, so they must be dropped here. It is safe to drop them here because\n+          \/\/ we will rescan the roots on this safepoint.\n+          heap->old_generation()->transfer_pointers_from_satb();\n+        }\n+\n+        if (_degen_point == ShenandoahDegenPoint::_degenerated_roots) {\n+          \/\/ We only need this if the concurrent cycle has already swapped the card tables.\n+          \/\/ Marking will use the 'read' table, but interesting pointers may have been\n+          \/\/ recorded in the 'write' table in the time between the cancelled concurrent cycle\n+          \/\/ and this degenerated cycle. These pointers need to be included in the 'read' table\n+          \/\/ used to scan the remembered set during the STW mark which follows here.\n+          _generation->merge_write_table();\n+        }\n+      }\n@@ -172,1 +248,0 @@\n-\n@@ -176,1 +251,0 @@\n-              heap->cancel_gc(GCCause::_shenandoah_upgrade_to_full_gc);\n@@ -189,0 +263,4 @@\n+      } else if (has_in_place_promotions(heap)) {\n+        \/\/ We have nothing to evacuate, but there are still regions to promote in place.\n+        ShenandoahGCPhase phase(ShenandoahPhaseTimings::degen_gc_promote_regions);\n+        ShenandoahGenerationalHeap::heap()->promote_regions_in_place(false \/* concurrent*\/);\n@@ -191,0 +269,5 @@\n+      \/\/ Update collector state regardless of whether there are forwarded objects\n+      heap->set_evacuation_in_progress(false);\n+      heap->set_concurrent_weak_root_in_progress(false);\n+      heap->set_concurrent_strong_root_in_progress(false);\n+\n@@ -194,1 +277,1 @@\n-        op_init_updaterefs();\n+        op_init_update_refs();\n@@ -196,0 +279,2 @@\n+      } else {\n+        _abbreviated = true;\n@@ -198,1 +283,1 @@\n-    case _degenerated_updaterefs:\n+    case _degenerated_update_refs:\n@@ -200,1 +285,1 @@\n-        op_updaterefs();\n+        op_update_refs();\n@@ -210,0 +295,5 @@\n+\n+      if (heap->mode()->is_generational()) {\n+        ShenandoahGenerationalHeap::heap()->complete_degenerated_cycle();\n+      }\n+\n@@ -225,7 +315,5 @@\n-  \/\/ Check for futility and fail. There is no reason to do several back-to-back Degenerated cycles,\n-  \/\/ because that probably means the heap is overloaded and\/or fragmented.\n-  if (!metrics.is_good_progress()) {\n-    heap->notify_gc_no_progress();\n-    heap->cancel_gc(GCCause::_shenandoah_upgrade_to_full_gc);\n-    op_degenerated_futile();\n-  } else {\n+  \/\/ Decide if this cycle made good progress, and, if not, should it upgrade to a full GC.\n+  const bool progress = metrics.is_good_progress(_generation);\n+  ShenandoahCollectorPolicy* policy = heap->shenandoah_policy();\n+  policy->record_degenerated(_generation->is_young(), _abbreviated, progress);\n+  if (progress) {\n@@ -233,0 +321,2 @@\n+  } else if (!heap->mode()->is_generational() || policy->generational_should_upgrade_degenerated_gc()) {\n+    op_degenerated_futile();\n@@ -237,1 +327,1 @@\n-  ShenandoahHeap::heap()->prepare_gc();\n+  _generation->prepare_gc();\n@@ -241,1 +331,1 @@\n-  assert(!ShenandoahHeap::heap()->is_concurrent_mark_in_progress(), \"Should be reset\");\n+  assert(!_generation->is_concurrent_mark_in_progress(), \"Should be reset\");\n@@ -243,2 +333,1 @@\n-  ShenandoahSTWMark mark(false \/*full gc*\/);\n-  mark.clear();\n+  ShenandoahSTWMark mark(_generation, false \/*full gc*\/);\n@@ -249,1 +338,1 @@\n-  ShenandoahConcurrentMark mark;\n+  ShenandoahConcurrentMark mark(_generation);\n@@ -261,0 +350,1 @@\n+\n@@ -262,1 +352,1 @@\n-  heap->prepare_regions_and_collection_set(false \/*concurrent*\/);\n+  _generation->prepare_regions_and_collection_set(false \/*concurrent*\/);\n@@ -275,0 +365,4 @@\n+    if (ShenandoahVerify) {\n+      heap->verifier()->verify_before_evacuation();\n+    }\n+\n@@ -276,1 +370,0 @@\n-    heap->set_has_forwarded_objects(true);\n@@ -278,3 +371,1 @@\n-    if(ShenandoahVerify) {\n-      heap->verifier()->verify_during_evacuation();\n-    }\n+    heap->set_has_forwarded_objects(true);\n@@ -283,1 +374,5 @@\n-      heap->verifier()->verify_after_concmark();\n+      if (has_in_place_promotions(heap)) {\n+        heap->verifier()->verify_after_concmark_with_promotions();\n+      } else {\n+        heap->verifier()->verify_after_concmark();\n+      }\n@@ -292,0 +387,4 @@\n+bool ShenandoahDegenGC::has_in_place_promotions(const ShenandoahHeap* heap) const {\n+  return heap->mode()->is_generational() && heap->old_generation()->has_in_place_promotions();\n+}\n+\n@@ -301,1 +400,1 @@\n-void ShenandoahDegenGC::op_init_updaterefs() {\n+void ShenandoahDegenGC::op_init_update_refs() {\n@@ -304,5 +403,1 @@\n-  heap->set_evacuation_in_progress(false);\n-  heap->set_concurrent_weak_root_in_progress(false);\n-  heap->set_concurrent_strong_root_in_progress(false);\n-\n-  heap->prepare_update_heap_references(false \/*concurrent*\/);\n+  heap->prepare_update_heap_references();\n@@ -312,1 +407,1 @@\n-void ShenandoahDegenGC::op_updaterefs() {\n+void ShenandoahDegenGC::op_update_refs() {\n@@ -314,1 +409,1 @@\n-  ShenandoahGCPhase phase(ShenandoahPhaseTimings::degen_gc_updaterefs);\n+  ShenandoahGCPhase phase(ShenandoahPhaseTimings::degen_gc_update_refs);\n@@ -330,1 +425,1 @@\n-    heap->verifier()->verify_after_updaterefs();\n+    heap->verifier()->verify_after_update_refs();\n@@ -346,5 +441,1 @@\n-  log_info(gc)(\"Cannot finish degeneration, upgrading to Full GC\");\n-  ShenandoahHeap::heap()->shenandoah_policy()->record_degenerated_upgrade_to_full();\n-\n-  ShenandoahFullGC full_gc;\n-  full_gc.op_full(GCCause::_shenandoah_upgrade_to_full_gc);\n+  upgrade_to_full();\n@@ -354,3 +445,1 @@\n-  ShenandoahHeap::heap()->shenandoah_policy()->record_degenerated_upgrade_to_full();\n-  ShenandoahFullGC full_gc;\n-  full_gc.op_full(GCCause::_shenandoah_upgrade_to_full_gc);\n+  upgrade_to_full();\n@@ -362,1 +451,1 @@\n-      return \"Pause Degenerated GC (<UNSET>)\";\n+      SHENANDOAH_RETURN_EVENT_MESSAGE(_generation->type(), \"Pause Degenerated GC\", \" (<UNSET>)\");\n@@ -364,1 +453,3 @@\n-      return \"Pause Degenerated GC (Outside of Cycle)\";\n+      SHENANDOAH_RETURN_EVENT_MESSAGE(_generation->type(), \"Pause Degenerated GC\", \" (Outside of Cycle)\");\n+    case _degenerated_roots:\n+      SHENANDOAH_RETURN_EVENT_MESSAGE(_generation->type(), \"Pause Degenerated GC\", \" (Roots)\");\n@@ -366,1 +457,1 @@\n-      return \"Pause Degenerated GC (Mark)\";\n+      SHENANDOAH_RETURN_EVENT_MESSAGE(_generation->type(), \"Pause Degenerated GC\", \" (Mark)\");\n@@ -368,3 +459,3 @@\n-      return \"Pause Degenerated GC (Evacuation)\";\n-    case _degenerated_updaterefs:\n-      return \"Pause Degenerated GC (Update Refs)\";\n+      SHENANDOAH_RETURN_EVENT_MESSAGE(_generation->type(), \"Pause Degenerated GC\", \" (Evacuation)\");\n+    case _degenerated_update_refs:\n+      SHENANDOAH_RETURN_EVENT_MESSAGE(_generation->type(), \"Pause Degenerated GC\", \" (Update Refs)\");\n@@ -373,1 +464,1 @@\n-      return \"ERROR\";\n+      SHENANDOAH_RETURN_EVENT_MESSAGE(_generation->type(), \"Pause Degenerated GC\", \" (?)\");\n@@ -376,0 +467,9 @@\n+\n+void ShenandoahDegenGC::upgrade_to_full() {\n+  log_info(gc)(\"Degenerated GC upgrading to Full GC\");\n+  ShenandoahHeap* heap = ShenandoahHeap::heap();\n+  heap->cancel_gc(GCCause::_shenandoah_upgrade_to_full_gc);\n+  heap->shenandoah_policy()->record_degenerated_upgrade_to_full();\n+  ShenandoahFullGC full_gc;\n+  full_gc.op_full(GCCause::_shenandoah_upgrade_to_full_gc);\n+}\n","filename":"src\/hotspot\/share\/gc\/shenandoah\/shenandoahDegeneratedGC.cpp","additions":159,"deletions":59,"binary":false,"changes":218,"status":"modified"},{"patch":"@@ -31,0 +31,1 @@\n+class ShenandoahGeneration;\n@@ -36,0 +37,2 @@\n+  ShenandoahGeneration* _generation;\n+  bool _abbreviated;\n@@ -38,1 +41,1 @@\n-  ShenandoahDegenGC(ShenandoahDegenPoint degen_point);\n+  ShenandoahDegenGC(ShenandoahDegenPoint degen_point, ShenandoahGeneration* generation);\n@@ -51,0 +54,1 @@\n+\n@@ -52,2 +56,2 @@\n-  void op_init_updaterefs();\n-  void op_updaterefs();\n+  void op_init_update_refs();\n+  void op_update_refs();\n@@ -61,0 +65,3 @@\n+  \/\/ Turns this degenerated cycle into a full gc without leaving the safepoint\n+  void upgrade_to_full();\n+\n@@ -62,0 +69,2 @@\n+\n+  bool has_in_place_promotions(const ShenandoahHeap* heap) const;\n","filename":"src\/hotspot\/share\/gc\/shenandoah\/shenandoahDegeneratedGC.hpp","additions":12,"deletions":3,"binary":false,"changes":15,"status":"modified"},{"patch":"@@ -0,0 +1,120 @@\n+\/*\n+ * Copyright Amazon.com Inc. or its affiliates. All Rights Reserved.\n+ * DO NOT ALTER OR REMOVE COPYRIGHT NOTICES OR THIS FILE HEADER.\n+ *\n+ * This code is free software; you can redistribute it and\/or modify it\n+ * under the terms of the GNU General Public License version 2 only, as\n+ * published by the Free Software Foundation.\n+ *\n+ * This code is distributed in the hope that it will be useful, but WITHOUT\n+ * ANY WARRANTY; without even the implied warranty of MERCHANTABILITY or\n+ * FITNESS FOR A PARTICULAR PURPOSE.  See the GNU General Public License\n+ * version 2 for more details (a copy is included in the LICENSE file that\n+ * accompanied this code).\n+ *\n+ * You should have received a copy of the GNU General Public License version\n+ * 2 along with this work; if not, write to the Free Software Foundation,\n+ * Inc., 51 Franklin St, Fifth Floor, Boston, MA 02110-1301 USA.\n+ *\n+ * Please contact Oracle, 500 Oracle Parkway, Redwood Shores, CA 94065 USA\n+ * or visit www.oracle.com if you need additional information or have any\n+ * questions.\n+ *\n+ *\/\n+\n+#ifndef SHARE_GC_SHENANDOAH_SHENANDOAHEVACINFO_HPP\n+#define SHARE_GC_SHENANDOAH_SHENANDOAHEVACINFO_HPP\n+\n+#include \"memory\/allocation.hpp\"\n+\n+class ShenandoahEvacuationInformation : public StackObj {\n+  \/\/ Values for ShenandoahEvacuationInformation jfr event, sizes stored as bytes\n+  size_t _collection_set_regions;\n+  size_t _collection_set_used_before;\n+  size_t _collection_set_used_after;\n+  size_t _collected_old;\n+  size_t _collected_promoted;\n+  size_t _collected_young;\n+  size_t _free_regions;\n+  size_t _regions_promoted_humongous;\n+  size_t _regions_promoted_regular;\n+  size_t _regular_promoted_garbage;\n+  size_t _regular_promoted_free;\n+  size_t _regions_immediate;\n+  size_t _immediate_size;\n+\n+public:\n+  ShenandoahEvacuationInformation() :\n+    _collection_set_regions(0), _collection_set_used_before(0), _collection_set_used_after(0),\n+    _collected_old(0), _collected_promoted(0), _collected_young(0), _free_regions(0),\n+    _regions_promoted_humongous(0), _regions_promoted_regular(0), _regular_promoted_garbage(0),\n+    _regular_promoted_free(0), _regions_immediate(0), _immediate_size(0) { }\n+\n+  void set_collection_set_regions(size_t collection_set_regions) {\n+    _collection_set_regions = collection_set_regions;\n+  }\n+\n+  void set_collection_set_used_before(size_t used) {\n+    _collection_set_used_before = used;\n+  }\n+\n+  void set_collection_set_used_after(size_t used) {\n+    _collection_set_used_after = used;\n+  }\n+\n+  void set_collected_old(size_t collected) {\n+    _collected_old = collected;\n+  }\n+\n+  void set_collected_promoted(size_t collected) {\n+    _collected_promoted = collected;\n+  }\n+\n+  void set_collected_young(size_t collected) {\n+    _collected_young = collected;\n+  }\n+\n+  void set_free_regions(size_t freed) {\n+    _free_regions = freed;\n+  }\n+\n+  void set_regions_promoted_humongous(size_t humongous) {\n+    _regions_promoted_humongous = humongous;\n+  }\n+\n+  void set_regions_promoted_regular(size_t regular) {\n+    _regions_promoted_regular = regular;\n+  }\n+\n+  void set_regular_promoted_garbage(size_t garbage) {\n+    _regular_promoted_garbage = garbage;\n+  }\n+\n+  void set_regular_promoted_free(size_t free) {\n+    _regular_promoted_free = free;\n+  }\n+\n+  void set_regions_immediate(size_t immediate) {\n+    _regions_immediate = immediate;\n+  }\n+\n+  void set_immediate_size(size_t size) {\n+    _immediate_size = size;\n+  }\n+\n+  size_t collection_set_regions()     { return _collection_set_regions; }\n+  size_t collection_set_used_before() { return _collection_set_used_before; }\n+  size_t collection_set_used_after()  { return _collection_set_used_after; }\n+  size_t collected_old()              { return _collected_old; }\n+  size_t collected_promoted()         { return _collected_promoted; }\n+  size_t collected_young()            { return _collected_young; }\n+  size_t regions_promoted_humongous() { return _regions_promoted_humongous; }\n+  size_t regions_promoted_regular()   { return _regions_promoted_regular; }\n+  size_t regular_promoted_garbage()   { return _regular_promoted_garbage; }\n+  size_t regular_promoted_free()      { return _regular_promoted_free; }\n+  size_t free_regions()               { return _free_regions; }\n+  size_t regions_immediate()          { return _regions_immediate; }\n+  size_t immediate_size()             { return _immediate_size; }\n+};\n+\n+#endif \/\/ SHARE_GC_SHENANDOAH_SHENANDOAHEVACINFO_HPP\n","filename":"src\/hotspot\/share\/gc\/shenandoah\/shenandoahEvacInfo.hpp","additions":120,"deletions":0,"binary":false,"changes":120,"status":"added"},{"patch":"@@ -0,0 +1,174 @@\n+\/*\n+ * Copyright Amazon.com Inc. or its affiliates. All Rights Reserved.\n+ * DO NOT ALTER OR REMOVE COPYRIGHT NOTICES OR THIS FILE HEADER.\n+ *\n+ * This code is free software; you can redistribute it and\/or modify it\n+ * under the terms of the GNU General Public License version 2 only, as\n+ * published by the Free Software Foundation.\n+ *\n+ * This code is distributed in the hope that it will be useful, but WITHOUT\n+ * ANY WARRANTY; without even the implied warranty of MERCHANTABILITY or\n+ * FITNESS FOR A PARTICULAR PURPOSE.  See the GNU General Public License\n+ * version 2 for more details (a copy is included in the LICENSE file that\n+ * accompanied this code).\n+ *\n+ * You should have received a copy of the GNU General Public License version\n+ * 2 along with this work; if not, write to the Free Software Foundation,\n+ * Inc., 51 Franklin St, Fifth Floor, Boston, MA 02110-1301 USA.\n+ *\n+ * Please contact Oracle, 500 Oracle Parkway, Redwood Shores, CA 94065 USA\n+ * or visit www.oracle.com if you need additional information or have any\n+ * questions.\n+ *\n+ *\/\n+#include \"precompiled.hpp\"\n+\n+#include \"gc\/shenandoah\/shenandoahAgeCensus.hpp\"\n+#include \"gc\/shenandoah\/shenandoahHeap.inline.hpp\"\n+#include \"gc\/shenandoah\/shenandoahEvacTracker.hpp\"\n+#include \"gc\/shenandoah\/shenandoahThreadLocalData.hpp\"\n+#include \"runtime\/threadSMR.inline.hpp\"\n+#include \"runtime\/thread.hpp\"\n+\n+ShenandoahEvacuationStats::ShenandoahEvacuationStats()\n+  : _evacuations_completed(0), _bytes_completed(0),\n+    _evacuations_attempted(0), _bytes_attempted(0),\n+    _use_age_table(ShenandoahGenerationalCensusAtEvac || !ShenandoahGenerationalAdaptiveTenuring),\n+    _age_table(nullptr) {\n+  if (_use_age_table) {\n+    _age_table = new AgeTable(false);\n+  }\n+}\n+\n+AgeTable* ShenandoahEvacuationStats::age_table() const {\n+  assert(_use_age_table, \"Don't call\");\n+  return _age_table;\n+}\n+\n+void ShenandoahEvacuationStats::begin_evacuation(size_t bytes) {\n+  ++_evacuations_attempted;\n+  _bytes_attempted += bytes;\n+}\n+\n+void ShenandoahEvacuationStats::end_evacuation(size_t bytes) {\n+  ++_evacuations_completed;\n+  _bytes_completed += bytes;\n+}\n+\n+void ShenandoahEvacuationStats::record_age(size_t bytes, uint age) {\n+  assert(_use_age_table, \"Don't call!\");\n+  if (age <= markWord::max_age) { \/\/ Filter age sentinel.\n+    _age_table->add(age, bytes >> LogBytesPerWord);\n+  }\n+}\n+\n+void ShenandoahEvacuationStats::accumulate(const ShenandoahEvacuationStats* other) {\n+  _evacuations_completed += other->_evacuations_completed;\n+  _bytes_completed += other->_bytes_completed;\n+  _evacuations_attempted += other->_evacuations_attempted;\n+  _bytes_attempted += other->_bytes_attempted;\n+  if (_use_age_table) {\n+    _age_table->merge(other->age_table());\n+  }\n+}\n+\n+void ShenandoahEvacuationStats::reset() {\n+  _evacuations_completed = _evacuations_attempted = 0;\n+  _bytes_completed = _bytes_attempted = 0;\n+  if (_use_age_table) {\n+    _age_table->clear();\n+  }\n+}\n+\n+void ShenandoahEvacuationStats::print_on(outputStream* st) {\n+#ifndef PRODUCT\n+  size_t abandoned_size = _bytes_attempted - _bytes_completed;\n+  size_t abandoned_count = _evacuations_attempted - _evacuations_completed;\n+  st->print_cr(\"Evacuated \" SIZE_FORMAT \"%s across \" SIZE_FORMAT \" objects, \"\n+            \"abandoned \" SIZE_FORMAT \"%s across \" SIZE_FORMAT \" objects.\",\n+            byte_size_in_proper_unit(_bytes_completed), proper_unit_for_byte_size(_bytes_completed),\n+            _evacuations_completed,\n+            byte_size_in_proper_unit(abandoned_size),   proper_unit_for_byte_size(abandoned_size),\n+            abandoned_count);\n+#endif\n+  if (_use_age_table) {\n+    shenandoah_assert_generational();\n+    _age_table->print_on(st, ShenandoahGenerationalHeap::heap()->age_census()->tenuring_threshold());\n+  }\n+}\n+\n+void ShenandoahEvacuationTracker::print_global_on(outputStream* st) {\n+  print_evacuations_on(st, &_workers_global, &_mutators_global);\n+}\n+\n+void ShenandoahEvacuationTracker::print_evacuations_on(outputStream* st,\n+                                                       ShenandoahEvacuationStats* workers,\n+                                                       ShenandoahEvacuationStats* mutators) {\n+  st->print(\"Workers: \");\n+  workers->print_on(st);\n+  st->cr();\n+  st->print(\"Mutators: \");\n+  mutators->print_on(st);\n+  st->cr();\n+\n+  ShenandoahGenerationalHeap* heap = ShenandoahGenerationalHeap::heap();\n+  AgeTable young_region_ages(false);\n+  for (uint i = 0; i < heap->num_regions(); ++i) {\n+    ShenandoahHeapRegion* r = heap->get_region(i);\n+    if (r->is_young()) {\n+      young_region_ages.add(r->age(), r->get_live_data_words());\n+    }\n+  }\n+  uint tenuring_threshold = heap->age_census()->tenuring_threshold();\n+  st->print(\"Young regions: \");\n+  young_region_ages.print_on(st, tenuring_threshold);\n+  st->cr();\n+}\n+\n+class ShenandoahStatAggregator : public ThreadClosure {\n+public:\n+  ShenandoahEvacuationStats* _target;\n+  explicit ShenandoahStatAggregator(ShenandoahEvacuationStats* target) : _target(target) {}\n+  void do_thread(Thread* thread) override {\n+    ShenandoahEvacuationStats* local = ShenandoahThreadLocalData::evacuation_stats(thread);\n+    _target->accumulate(local);\n+    local->reset();\n+  }\n+};\n+\n+ShenandoahCycleStats ShenandoahEvacuationTracker::flush_cycle_to_global() {\n+  ShenandoahEvacuationStats mutators, workers;\n+\n+  ThreadsListHandle java_threads_iterator;\n+  ShenandoahStatAggregator aggregate_mutators(&mutators);\n+  java_threads_iterator.list()->threads_do(&aggregate_mutators);\n+\n+  ShenandoahStatAggregator aggregate_workers(&workers);\n+  ShenandoahHeap::heap()->gc_threads_do(&aggregate_workers);\n+\n+  _mutators_global.accumulate(&mutators);\n+  _workers_global.accumulate(&workers);\n+\n+  if (ShenandoahGenerationalCensusAtEvac || !ShenandoahGenerationalAdaptiveTenuring) {\n+    \/\/ Ingest mutator & worker collected population vectors into the heap's\n+    \/\/ global census data, and use it to compute an appropriate tenuring threshold\n+    \/\/ for use in the next cycle.\n+    \/\/ The first argument is used for any age 0 cohort population that we may otherwise have\n+    \/\/ missed during the census. This is non-zero only when census happens at marking.\n+    ShenandoahGenerationalHeap::heap()->age_census()->update_census(0, mutators.age_table(), workers.age_table());\n+  }\n+\n+  return {workers, mutators};\n+}\n+\n+void ShenandoahEvacuationTracker::begin_evacuation(Thread* thread, size_t bytes) {\n+  ShenandoahThreadLocalData::begin_evacuation(thread, bytes);\n+}\n+\n+void ShenandoahEvacuationTracker::end_evacuation(Thread* thread, size_t bytes) {\n+  ShenandoahThreadLocalData::end_evacuation(thread, bytes);\n+}\n+\n+void ShenandoahEvacuationTracker::record_age(Thread* thread, size_t bytes, uint age) {\n+  ShenandoahThreadLocalData::record_age(thread, bytes, age);\n+}\n","filename":"src\/hotspot\/share\/gc\/shenandoah\/shenandoahEvacTracker.cpp","additions":174,"deletions":0,"binary":false,"changes":174,"status":"added"},{"patch":"@@ -0,0 +1,81 @@\n+\/*\n+ * Copyright Amazon.com Inc. or its affiliates. All Rights Reserved.\n+ * DO NOT ALTER OR REMOVE COPYRIGHT NOTICES OR THIS FILE HEADER.\n+ *\n+ * This code is free software; you can redistribute it and\/or modify it\n+ * under the terms of the GNU General Public License version 2 only, as\n+ * published by the Free Software Foundation.\n+ *\n+ * This code is distributed in the hope that it will be useful, but WITHOUT\n+ * ANY WARRANTY; without even the implied warranty of MERCHANTABILITY or\n+ * FITNESS FOR A PARTICULAR PURPOSE.  See the GNU General Public License\n+ * version 2 for more details (a copy is included in the LICENSE file that\n+ * accompanied this code).\n+ *\n+ * You should have received a copy of the GNU General Public License version\n+ * 2 along with this work; if not, write to the Free Software Foundation,\n+ * Inc., 51 Franklin St, Fifth Floor, Boston, MA 02110-1301 USA.\n+ *\n+ * Please contact Oracle, 500 Oracle Parkway, Redwood Shores, CA 94065 USA\n+ * or visit www.oracle.com if you need additional information or have any\n+ * questions.\n+ *\n+ *\/\n+\n+#ifndef SHARE_GC_SHENANDOAH_SHENANDOAHEVACTRACKER_HPP\n+#define SHARE_GC_SHENANDOAH_SHENANDOAHEVACTRACKER_HPP\n+\n+#include \"gc\/shared\/ageTable.hpp\"\n+#include \"utilities\/ostream.hpp\"\n+\n+class ShenandoahEvacuationStats : public CHeapObj<mtGC> {\n+private:\n+  size_t _evacuations_completed;\n+  size_t _bytes_completed;\n+  size_t _evacuations_attempted;\n+  size_t _bytes_attempted;\n+\n+  bool      _use_age_table;\n+  AgeTable* _age_table;\n+\n+ public:\n+  ShenandoahEvacuationStats();\n+\n+  AgeTable* age_table() const;\n+\n+  void begin_evacuation(size_t bytes);\n+  void end_evacuation(size_t bytes);\n+  void record_age(size_t bytes, uint age);\n+\n+  void print_on(outputStream* st);\n+  void accumulate(const ShenandoahEvacuationStats* other);\n+  void reset();\n+};\n+\n+struct ShenandoahCycleStats {\n+  ShenandoahEvacuationStats workers;\n+  ShenandoahEvacuationStats mutators;\n+};\n+\n+class ShenandoahEvacuationTracker : public CHeapObj<mtGC> {\n+private:\n+\n+  ShenandoahEvacuationStats _workers_global;\n+  ShenandoahEvacuationStats _mutators_global;\n+\n+public:\n+  ShenandoahEvacuationTracker() = default;\n+\n+  void begin_evacuation(Thread* thread, size_t bytes);\n+  void end_evacuation(Thread* thread, size_t bytes);\n+  void record_age(Thread* thread, size_t bytes, uint age);\n+\n+  void print_global_on(outputStream* st);\n+  void print_evacuations_on(outputStream* st,\n+                            ShenandoahEvacuationStats* workers,\n+                            ShenandoahEvacuationStats* mutators);\n+\n+  ShenandoahCycleStats flush_cycle_to_global();\n+};\n+\n+#endif \/\/SHARE_GC_SHENANDOAH_SHENANDOAHEVACTRACKER_HPP\n","filename":"src\/hotspot\/share\/gc\/shenandoah\/shenandoahEvacTracker.hpp","additions":81,"deletions":0,"binary":false,"changes":81,"status":"added"},{"patch":"@@ -3,0 +3,1 @@\n+ * Copyright Amazon.com Inc. or its affiliates. All Rights Reserved.\n@@ -27,0 +28,1 @@\n+#include \"gc\/shenandoah\/shenandoahAffiliation.hpp\"\n@@ -31,0 +33,4 @@\n+#include \"gc\/shenandoah\/shenandoahOldGeneration.hpp\"\n+#include \"gc\/shenandoah\/shenandoahYoungGeneration.hpp\"\n+#include \"gc\/shenandoah\/shenandoahSimpleBitMap.hpp\"\n+#include \"gc\/shenandoah\/shenandoahSimpleBitMap.inline.hpp\"\n@@ -35,0 +41,688 @@\n+static const char* partition_name(ShenandoahFreeSetPartitionId t) {\n+  switch (t) {\n+    case ShenandoahFreeSetPartitionId::NotFree: return \"NotFree\";\n+    case ShenandoahFreeSetPartitionId::Mutator: return \"Mutator\";\n+    case ShenandoahFreeSetPartitionId::Collector: return \"Collector\";\n+    case ShenandoahFreeSetPartitionId::OldCollector: return \"OldCollector\";\n+    default:\n+      ShouldNotReachHere();\n+      return \"Unrecognized\";\n+  }\n+}\n+\n+class ShenandoahLeftRightIterator {\n+private:\n+  idx_t _idx;\n+  idx_t _end;\n+  ShenandoahRegionPartitions* _partitions;\n+  ShenandoahFreeSetPartitionId _partition;\n+public:\n+  explicit ShenandoahLeftRightIterator(ShenandoahRegionPartitions* partitions, ShenandoahFreeSetPartitionId partition, bool use_empty = false)\n+    : _idx(0), _end(0), _partitions(partitions), _partition(partition) {\n+    _idx = use_empty ? _partitions->leftmost_empty(_partition) : _partitions->leftmost(_partition);\n+    _end = use_empty ? _partitions->rightmost_empty(_partition) : _partitions->rightmost(_partition);\n+  }\n+\n+  bool has_next() const {\n+    if (_idx <= _end) {\n+      assert(_partitions->in_free_set(_partition, _idx), \"Boundaries or find_last_set_bit failed: \" SSIZE_FORMAT, _idx);\n+      return true;\n+    }\n+    return false;\n+  }\n+\n+  idx_t current() const {\n+    return _idx;\n+  }\n+\n+  idx_t next() {\n+    _idx = _partitions->find_index_of_next_available_region(_partition, _idx + 1);\n+    return current();\n+  }\n+};\n+\n+class ShenandoahRightLeftIterator {\n+private:\n+  idx_t _idx;\n+  idx_t _end;\n+  ShenandoahRegionPartitions* _partitions;\n+  ShenandoahFreeSetPartitionId _partition;\n+public:\n+  explicit ShenandoahRightLeftIterator(ShenandoahRegionPartitions* partitions, ShenandoahFreeSetPartitionId partition, bool use_empty = false)\n+    : _idx(0), _end(0), _partitions(partitions), _partition(partition) {\n+    _idx = use_empty ? _partitions->rightmost_empty(_partition) : _partitions->rightmost(_partition);\n+    _end = use_empty ? _partitions->leftmost_empty(_partition) : _partitions->leftmost(_partition);\n+  }\n+\n+  bool has_next() const {\n+    if (_idx >= _end) {\n+      assert(_partitions->in_free_set(_partition, _idx), \"Boundaries or find_last_set_bit failed: \" SSIZE_FORMAT, _idx);\n+      return true;\n+    }\n+    return false;\n+  }\n+\n+  idx_t current() const {\n+    return _idx;\n+  }\n+\n+  idx_t next() {\n+    _idx = _partitions->find_index_of_previous_available_region(_partition, _idx - 1);\n+    return current();\n+  }\n+};\n+\n+#ifndef PRODUCT\n+void ShenandoahRegionPartitions::dump_bitmap() const {\n+  log_debug(gc)(\"Mutator range [\" SSIZE_FORMAT \", \" SSIZE_FORMAT \"], Collector range [\" SSIZE_FORMAT \", \" SSIZE_FORMAT\n+                \"], Old Collector range [\" SSIZE_FORMAT \", \" SSIZE_FORMAT \"]\",\n+                _leftmosts[int(ShenandoahFreeSetPartitionId::Mutator)],\n+                _rightmosts[int(ShenandoahFreeSetPartitionId::Mutator)],\n+                _leftmosts[int(ShenandoahFreeSetPartitionId::Collector)],\n+                _rightmosts[int(ShenandoahFreeSetPartitionId::Collector)],\n+                _leftmosts[int(ShenandoahFreeSetPartitionId::OldCollector)],\n+                _rightmosts[int(ShenandoahFreeSetPartitionId::OldCollector)]);\n+  log_debug(gc)(\"Empty Mutator range [\" SSIZE_FORMAT \", \" SSIZE_FORMAT\n+                \"], Empty Collector range [\" SSIZE_FORMAT \", \" SSIZE_FORMAT\n+                \"], Empty Old Collecto range [\" SSIZE_FORMAT \", \" SSIZE_FORMAT \"]\",\n+                _leftmosts_empty[int(ShenandoahFreeSetPartitionId::Mutator)],\n+                _rightmosts_empty[int(ShenandoahFreeSetPartitionId::Mutator)],\n+                _leftmosts_empty[int(ShenandoahFreeSetPartitionId::Collector)],\n+                _rightmosts_empty[int(ShenandoahFreeSetPartitionId::Collector)],\n+                _leftmosts_empty[int(ShenandoahFreeSetPartitionId::OldCollector)],\n+                _rightmosts_empty[int(ShenandoahFreeSetPartitionId::OldCollector)]);\n+\n+  log_debug(gc)(\"%6s: %18s %18s %18s %18s\", \"index\", \"Mutator Bits\", \"Collector Bits\", \"Old Collector Bits\", \"NotFree Bits\");\n+  dump_bitmap_range(0, _max-1);\n+}\n+\n+void ShenandoahRegionPartitions::dump_bitmap_range(idx_t start_region_idx, idx_t end_region_idx) const {\n+  assert((start_region_idx >= 0) && (start_region_idx < (idx_t) _max), \"precondition\");\n+  assert((end_region_idx >= 0) && (end_region_idx < (idx_t) _max), \"precondition\");\n+  idx_t aligned_start = _membership[int(ShenandoahFreeSetPartitionId::Mutator)].aligned_index(start_region_idx);\n+  idx_t aligned_end = _membership[int(ShenandoahFreeSetPartitionId::Mutator)].aligned_index(end_region_idx);\n+  idx_t alignment = _membership[int(ShenandoahFreeSetPartitionId::Mutator)].alignment();\n+  while (aligned_start <= aligned_end) {\n+    dump_bitmap_row(aligned_start);\n+    aligned_start += alignment;\n+  }\n+}\n+\n+void ShenandoahRegionPartitions::dump_bitmap_row(idx_t region_idx) const {\n+  assert((region_idx >= 0) && (region_idx < (idx_t) _max), \"precondition\");\n+  idx_t aligned_idx = _membership[int(ShenandoahFreeSetPartitionId::Mutator)].aligned_index(region_idx);\n+  uintx mutator_bits = _membership[int(ShenandoahFreeSetPartitionId::Mutator)].bits_at(aligned_idx);\n+  uintx collector_bits = _membership[int(ShenandoahFreeSetPartitionId::Collector)].bits_at(aligned_idx);\n+  uintx old_collector_bits = _membership[int(ShenandoahFreeSetPartitionId::OldCollector)].bits_at(aligned_idx);\n+  uintx free_bits = mutator_bits | collector_bits | old_collector_bits;\n+  uintx notfree_bits =  ~free_bits;\n+  log_debug(gc)(SSIZE_FORMAT_W(6) \": \" SIZE_FORMAT_X_0 \" 0x\" SIZE_FORMAT_X_0 \" 0x\" SIZE_FORMAT_X_0 \" 0x\" SIZE_FORMAT_X_0,\n+                aligned_idx, mutator_bits, collector_bits, old_collector_bits, notfree_bits);\n+}\n+#endif\n+\n+ShenandoahRegionPartitions::ShenandoahRegionPartitions(size_t max_regions, ShenandoahFreeSet* free_set) :\n+    _max(max_regions),\n+    _region_size_bytes(ShenandoahHeapRegion::region_size_bytes()),\n+    _free_set(free_set),\n+    _membership{ ShenandoahSimpleBitMap(max_regions), ShenandoahSimpleBitMap(max_regions) , ShenandoahSimpleBitMap(max_regions) }\n+{\n+  make_all_regions_unavailable();\n+}\n+\n+inline bool ShenandoahFreeSet::can_allocate_from(ShenandoahHeapRegion *r) const {\n+  return r->is_empty() || (r->is_trash() && !_heap->is_concurrent_weak_root_in_progress());\n+}\n+\n+inline bool ShenandoahFreeSet::can_allocate_from(size_t idx) const {\n+  ShenandoahHeapRegion* r = _heap->get_region(idx);\n+  return can_allocate_from(r);\n+}\n+\n+inline size_t ShenandoahFreeSet::alloc_capacity(ShenandoahHeapRegion *r) const {\n+  if (r->is_trash()) {\n+    \/\/ This would be recycled on allocation path\n+    return ShenandoahHeapRegion::region_size_bytes();\n+  } else {\n+    return r->free();\n+  }\n+}\n+\n+inline size_t ShenandoahFreeSet::alloc_capacity(size_t idx) const {\n+  ShenandoahHeapRegion* r = _heap->get_region(idx);\n+  return alloc_capacity(r);\n+}\n+\n+inline bool ShenandoahFreeSet::has_alloc_capacity(ShenandoahHeapRegion *r) const {\n+  return alloc_capacity(r) > 0;\n+}\n+\n+inline idx_t ShenandoahRegionPartitions::leftmost(ShenandoahFreeSetPartitionId which_partition) const {\n+  assert (which_partition < NumPartitions, \"selected free partition must be valid\");\n+  idx_t idx = _leftmosts[int(which_partition)];\n+  if (idx >= _max) {\n+    return _max;\n+  } else {\n+    \/\/ Cannot assert that membership[which_partition.is_set(idx) because this helper method may be used\n+    \/\/ to query the original value of leftmost when leftmost must be adjusted because the interval representing\n+    \/\/ which_partition is shrinking after the region that used to be leftmost is retired.\n+    return idx;\n+  }\n+}\n+\n+inline idx_t ShenandoahRegionPartitions::rightmost(ShenandoahFreeSetPartitionId which_partition) const {\n+  assert (which_partition < NumPartitions, \"selected free partition must be valid\");\n+  idx_t idx = _rightmosts[int(which_partition)];\n+  \/\/ Cannot assert that membership[which_partition.is_set(idx) because this helper method may be used\n+  \/\/ to query the original value of leftmost when leftmost must be adjusted because the interval representing\n+  \/\/ which_partition is shrinking after the region that used to be leftmost is retired.\n+  return idx;\n+}\n+\n+void ShenandoahRegionPartitions::make_all_regions_unavailable() {\n+  for (size_t partition_id = 0; partition_id < IntNumPartitions; partition_id++) {\n+    _membership[partition_id].clear_all();\n+    _leftmosts[partition_id] = _max;\n+    _rightmosts[partition_id] = -1;\n+    _leftmosts_empty[partition_id] = _max;\n+    _rightmosts_empty[partition_id] = -1;;\n+    _capacity[partition_id] = 0;\n+    _used[partition_id] = 0;\n+  }\n+  _region_counts[int(ShenandoahFreeSetPartitionId::Mutator)] = _region_counts[int(ShenandoahFreeSetPartitionId::Collector)] = 0;\n+}\n+\n+void ShenandoahRegionPartitions::establish_mutator_intervals(idx_t mutator_leftmost, idx_t mutator_rightmost,\n+                                                             idx_t mutator_leftmost_empty, idx_t mutator_rightmost_empty,\n+                                                             size_t mutator_region_count, size_t mutator_used) {\n+  _leftmosts[int(ShenandoahFreeSetPartitionId::Mutator)] = mutator_leftmost;\n+  _rightmosts[int(ShenandoahFreeSetPartitionId::Mutator)] = mutator_rightmost;\n+  _leftmosts_empty[int(ShenandoahFreeSetPartitionId::Mutator)] = mutator_leftmost_empty;\n+  _rightmosts_empty[int(ShenandoahFreeSetPartitionId::Mutator)] = mutator_rightmost_empty;\n+\n+  _region_counts[int(ShenandoahFreeSetPartitionId::Mutator)] = mutator_region_count;\n+  _used[int(ShenandoahFreeSetPartitionId::Mutator)] = mutator_used;\n+  _capacity[int(ShenandoahFreeSetPartitionId::Mutator)] = mutator_region_count * _region_size_bytes;\n+\n+  _leftmosts[int(ShenandoahFreeSetPartitionId::Collector)] = _max;\n+  _rightmosts[int(ShenandoahFreeSetPartitionId::Collector)] = -1;\n+  _leftmosts_empty[int(ShenandoahFreeSetPartitionId::Collector)] = _max;\n+  _rightmosts_empty[int(ShenandoahFreeSetPartitionId::Collector)] = -1;\n+\n+  _region_counts[int(ShenandoahFreeSetPartitionId::Collector)] = 0;\n+  _used[int(ShenandoahFreeSetPartitionId::Collector)] = 0;\n+  _capacity[int(ShenandoahFreeSetPartitionId::Collector)] = 0;\n+}\n+\n+void ShenandoahRegionPartitions::establish_old_collector_intervals(idx_t old_collector_leftmost, idx_t old_collector_rightmost,\n+                                                                   idx_t old_collector_leftmost_empty,\n+                                                                   idx_t old_collector_rightmost_empty,\n+                                                                   size_t old_collector_region_count, size_t old_collector_used) {\n+  _leftmosts[int(ShenandoahFreeSetPartitionId::OldCollector)] = old_collector_leftmost;\n+  _rightmosts[int(ShenandoahFreeSetPartitionId::OldCollector)] = old_collector_rightmost;\n+  _leftmosts_empty[int(ShenandoahFreeSetPartitionId::OldCollector)] = old_collector_leftmost_empty;\n+  _rightmosts_empty[int(ShenandoahFreeSetPartitionId::OldCollector)] = old_collector_rightmost_empty;\n+\n+  _region_counts[int(ShenandoahFreeSetPartitionId::OldCollector)] = old_collector_region_count;\n+  _used[int(ShenandoahFreeSetPartitionId::OldCollector)] = old_collector_used;\n+  _capacity[int(ShenandoahFreeSetPartitionId::OldCollector)] = old_collector_region_count * _region_size_bytes;\n+}\n+\n+void ShenandoahRegionPartitions::increase_used(ShenandoahFreeSetPartitionId which_partition, size_t bytes) {\n+  assert (which_partition < NumPartitions, \"Partition must be valid\");\n+  _used[int(which_partition)] += bytes;\n+  assert (_used[int(which_partition)] <= _capacity[int(which_partition)],\n+          \"Must not use (\" SIZE_FORMAT \") more than capacity (\" SIZE_FORMAT \") after increase by \" SIZE_FORMAT,\n+          _used[int(which_partition)], _capacity[int(which_partition)], bytes);\n+}\n+\n+inline void ShenandoahRegionPartitions::shrink_interval_if_range_modifies_either_boundary(\n+  ShenandoahFreeSetPartitionId partition, idx_t low_idx, idx_t high_idx) {\n+  assert((low_idx <= high_idx) && (low_idx >= 0) && (high_idx < _max), \"Range must span legal index values\");\n+  if (low_idx == leftmost(partition)) {\n+    assert (!_membership[int(partition)].is_set(low_idx), \"Do not shrink interval if region not removed\");\n+    if (high_idx + 1 == _max) {\n+      _leftmosts[int(partition)] = _max;\n+    } else {\n+      _leftmosts[int(partition)] = find_index_of_next_available_region(partition, high_idx + 1);\n+    }\n+    if (_leftmosts_empty[int(partition)] < _leftmosts[int(partition)]) {\n+      \/\/ This gets us closer to where we need to be; we'll scan further when leftmosts_empty is requested.\n+      _leftmosts_empty[int(partition)] = _leftmosts[int(partition)];\n+    }\n+  }\n+  if (high_idx == _rightmosts[int(partition)]) {\n+    assert (!_membership[int(partition)].is_set(high_idx), \"Do not shrink interval if region not removed\");\n+    if (low_idx == 0) {\n+      _rightmosts[int(partition)] = -1;\n+    } else {\n+      _rightmosts[int(partition)] = find_index_of_previous_available_region(partition, low_idx - 1);\n+    }\n+    if (_rightmosts_empty[int(partition)] > _rightmosts[int(partition)]) {\n+      \/\/ This gets us closer to where we need to be; we'll scan further when rightmosts_empty is requested.\n+      _rightmosts_empty[int(partition)] = _rightmosts[int(partition)];\n+    }\n+  }\n+  if (_leftmosts[int(partition)] > _rightmosts[int(partition)]) {\n+    _leftmosts[int(partition)] = _max;\n+    _rightmosts[int(partition)] = -1;\n+    _leftmosts_empty[int(partition)] = _max;\n+    _rightmosts_empty[int(partition)] = -1;\n+  }\n+}\n+\n+inline void ShenandoahRegionPartitions::shrink_interval_if_boundary_modified(ShenandoahFreeSetPartitionId partition, idx_t idx) {\n+  shrink_interval_if_range_modifies_either_boundary(partition, idx, idx);\n+}\n+\n+inline void ShenandoahRegionPartitions::expand_interval_if_boundary_modified(ShenandoahFreeSetPartitionId partition,\n+                                                                             idx_t idx, size_t region_available) {\n+  if (_leftmosts[int(partition)] > idx) {\n+    _leftmosts[int(partition)] = idx;\n+  }\n+  if (_rightmosts[int(partition)] < idx) {\n+    _rightmosts[int(partition)] = idx;\n+  }\n+  if (region_available == _region_size_bytes) {\n+    if (_leftmosts_empty[int(partition)] > idx) {\n+      _leftmosts_empty[int(partition)] = idx;\n+    }\n+    if (_rightmosts_empty[int(partition)] < idx) {\n+      _rightmosts_empty[int(partition)] = idx;\n+    }\n+  }\n+}\n+\n+void ShenandoahRegionPartitions::retire_range_from_partition(\n+  ShenandoahFreeSetPartitionId partition, idx_t low_idx, idx_t high_idx) {\n+\n+  \/\/ Note: we may remove from free partition even if region is not entirely full, such as when available < PLAB::min_size()\n+  assert ((low_idx < _max) && (high_idx < _max), \"Both indices are sane: \" SIZE_FORMAT \" and \" SIZE_FORMAT \" < \" SIZE_FORMAT,\n+          low_idx, high_idx, _max);\n+  assert (partition < NumPartitions, \"Cannot remove from free partitions if not already free\");\n+\n+  for (idx_t idx = low_idx; idx <= high_idx; idx++) {\n+    assert (in_free_set(partition, idx), \"Must be in partition to remove from partition\");\n+    _membership[int(partition)].clear_bit(idx);\n+  }\n+  _region_counts[int(partition)] -= high_idx + 1 - low_idx;\n+  shrink_interval_if_range_modifies_either_boundary(partition, low_idx, high_idx);\n+}\n+\n+void ShenandoahRegionPartitions::retire_from_partition(ShenandoahFreeSetPartitionId partition, idx_t idx, size_t used_bytes) {\n+\n+  \/\/ Note: we may remove from free partition even if region is not entirely full, such as when available < PLAB::min_size()\n+  assert (idx < _max, \"index is sane: \" SIZE_FORMAT \" < \" SIZE_FORMAT, idx, _max);\n+  assert (partition < NumPartitions, \"Cannot remove from free partitions if not already free\");\n+  assert (in_free_set(partition, idx), \"Must be in partition to remove from partition\");\n+\n+  if (used_bytes < _region_size_bytes) {\n+    \/\/ Count the alignment pad remnant of memory as used when we retire this region\n+    increase_used(partition, _region_size_bytes - used_bytes);\n+  }\n+  _membership[int(partition)].clear_bit(idx);\n+  shrink_interval_if_boundary_modified(partition, idx);\n+  _region_counts[int(partition)]--;\n+}\n+\n+void ShenandoahRegionPartitions::make_free(idx_t idx, ShenandoahFreeSetPartitionId which_partition, size_t available) {\n+  assert (idx < _max, \"index is sane: \" SIZE_FORMAT \" < \" SIZE_FORMAT, idx, _max);\n+  assert (membership(idx) == ShenandoahFreeSetPartitionId::NotFree, \"Cannot make free if already free\");\n+  assert (which_partition < NumPartitions, \"selected free partition must be valid\");\n+  assert (available <= _region_size_bytes, \"Available cannot exceed region size\");\n+\n+  _membership[int(which_partition)].set_bit(idx);\n+  _capacity[int(which_partition)] += _region_size_bytes;\n+  _used[int(which_partition)] += _region_size_bytes - available;\n+  expand_interval_if_boundary_modified(which_partition, idx, available);\n+  _region_counts[int(which_partition)]++;\n+}\n+\n+bool ShenandoahRegionPartitions::is_mutator_partition(ShenandoahFreeSetPartitionId p) {\n+  return (p == ShenandoahFreeSetPartitionId::Mutator);\n+}\n+\n+bool ShenandoahRegionPartitions::is_young_collector_partition(ShenandoahFreeSetPartitionId p) {\n+  return (p == ShenandoahFreeSetPartitionId::Collector);\n+}\n+\n+bool ShenandoahRegionPartitions::is_old_collector_partition(ShenandoahFreeSetPartitionId p) {\n+  return (p == ShenandoahFreeSetPartitionId::OldCollector);\n+}\n+\n+bool ShenandoahRegionPartitions::available_implies_empty(size_t available_in_region) {\n+  return (available_in_region == _region_size_bytes);\n+}\n+\n+\n+void ShenandoahRegionPartitions::move_from_partition_to_partition(idx_t idx, ShenandoahFreeSetPartitionId orig_partition,\n+                                                                  ShenandoahFreeSetPartitionId new_partition, size_t available) {\n+  ShenandoahHeapRegion* r = ShenandoahHeap::heap()->get_region(idx);\n+  assert (idx < _max, \"index is sane: \" SIZE_FORMAT \" < \" SIZE_FORMAT, idx, _max);\n+  assert (orig_partition < NumPartitions, \"Original partition must be valid\");\n+  assert (new_partition < NumPartitions, \"New partition must be valid\");\n+  assert (available <= _region_size_bytes, \"Available cannot exceed region size\");\n+  assert (_membership[int(orig_partition)].is_set(idx), \"Cannot move from partition unless in partition\");\n+  assert ((r != nullptr) && ((r->is_trash() && (available == _region_size_bytes)) ||\n+                             (r->used() + available == _region_size_bytes)),\n+          \"Used: \" SIZE_FORMAT \" + available: \" SIZE_FORMAT \" should equal region size: \" SIZE_FORMAT,\n+          ShenandoahHeap::heap()->get_region(idx)->used(), available, _region_size_bytes);\n+\n+  \/\/ Expected transitions:\n+  \/\/  During rebuild:         Mutator => Collector\n+  \/\/                          Mutator empty => Collector\n+  \/\/                          Mutator empty => OldCollector\n+  \/\/  During flip_to_gc:      Mutator empty => Collector\n+  \/\/                          Mutator empty => OldCollector\n+  \/\/ At start of update refs: Collector => Mutator\n+  \/\/                          OldCollector Empty => Mutator\n+  assert ((is_mutator_partition(orig_partition) && is_young_collector_partition(new_partition)) ||\n+          (is_mutator_partition(orig_partition) &&\n+           available_implies_empty(available) && is_old_collector_partition(new_partition)) ||\n+          (is_young_collector_partition(orig_partition) && is_mutator_partition(new_partition)) ||\n+          (is_old_collector_partition(orig_partition)\n+           && available_implies_empty(available) && is_mutator_partition(new_partition)),\n+          \"Unexpected movement between partitions, available: \" SIZE_FORMAT \", _region_size_bytes: \" SIZE_FORMAT\n+          \", orig_partition: %s, new_partition: %s\",\n+          available, _region_size_bytes, partition_name(orig_partition), partition_name(new_partition));\n+\n+  size_t used = _region_size_bytes - available;\n+  assert (_used[int(orig_partition)] >= used,\n+          \"Orig partition used: \" SIZE_FORMAT \" must exceed moved used: \" SIZE_FORMAT \" within region \" SSIZE_FORMAT,\n+          _used[int(orig_partition)], used, idx);\n+\n+  _membership[int(orig_partition)].clear_bit(idx);\n+  _membership[int(new_partition)].set_bit(idx);\n+\n+  _capacity[int(orig_partition)] -= _region_size_bytes;\n+  _used[int(orig_partition)] -= used;\n+  shrink_interval_if_boundary_modified(orig_partition, idx);\n+\n+  _capacity[int(new_partition)] += _region_size_bytes;;\n+  _used[int(new_partition)] += used;\n+  expand_interval_if_boundary_modified(new_partition, idx, available);\n+\n+  _region_counts[int(orig_partition)]--;\n+  _region_counts[int(new_partition)]++;\n+}\n+\n+const char* ShenandoahRegionPartitions::partition_membership_name(idx_t idx) const {\n+  return partition_name(membership(idx));\n+}\n+\n+inline ShenandoahFreeSetPartitionId ShenandoahRegionPartitions::membership(idx_t idx) const {\n+  assert (idx < _max, \"index is sane: \" SIZE_FORMAT \" < \" SIZE_FORMAT, idx, _max);\n+  ShenandoahFreeSetPartitionId result = ShenandoahFreeSetPartitionId::NotFree;\n+  for (uint partition_id = 0; partition_id < UIntNumPartitions; partition_id++) {\n+    if (_membership[partition_id].is_set(idx)) {\n+      assert(result == ShenandoahFreeSetPartitionId::NotFree, \"Region should reside in only one partition\");\n+      result = (ShenandoahFreeSetPartitionId) partition_id;\n+    }\n+  }\n+  return result;\n+}\n+\n+#ifdef ASSERT\n+inline bool ShenandoahRegionPartitions::partition_id_matches(idx_t idx, ShenandoahFreeSetPartitionId test_partition) const {\n+  assert (idx < _max, \"index is sane: \" SIZE_FORMAT \" < \" SIZE_FORMAT, idx, _max);\n+  assert (test_partition < ShenandoahFreeSetPartitionId::NotFree, \"must be a valid partition\");\n+\n+  return membership(idx) == test_partition;\n+}\n+#endif\n+\n+inline bool ShenandoahRegionPartitions::is_empty(ShenandoahFreeSetPartitionId which_partition) const {\n+  assert (which_partition < NumPartitions, \"selected free partition must be valid\");\n+  return (leftmost(which_partition) > rightmost(which_partition));\n+}\n+\n+inline idx_t ShenandoahRegionPartitions::find_index_of_next_available_region(\n+  ShenandoahFreeSetPartitionId which_partition, idx_t start_index) const {\n+  idx_t rightmost_idx = rightmost(which_partition);\n+  idx_t leftmost_idx = leftmost(which_partition);\n+  if ((rightmost_idx < leftmost_idx) || (start_index > rightmost_idx)) return _max;\n+  if (start_index < leftmost_idx) {\n+    start_index = leftmost_idx;\n+  }\n+  idx_t result = _membership[int(which_partition)].find_first_set_bit(start_index, rightmost_idx + 1);\n+  if (result > rightmost_idx) {\n+    result = _max;\n+  }\n+  assert (result >= start_index, \"Requires progress\");\n+  return result;\n+}\n+\n+inline idx_t ShenandoahRegionPartitions::find_index_of_previous_available_region(\n+  ShenandoahFreeSetPartitionId which_partition, idx_t last_index) const {\n+  idx_t rightmost_idx = rightmost(which_partition);\n+  idx_t leftmost_idx = leftmost(which_partition);\n+  \/\/ if (leftmost_idx == max) then (last_index < leftmost_idx)\n+  if (last_index < leftmost_idx) return -1;\n+  if (last_index > rightmost_idx) {\n+    last_index = rightmost_idx;\n+  }\n+  idx_t result = _membership[int(which_partition)].find_last_set_bit(-1, last_index);\n+  if (result < leftmost_idx) {\n+    result = -1;\n+  }\n+  assert (result <= last_index, \"Requires progress\");\n+  return result;\n+}\n+\n+inline idx_t ShenandoahRegionPartitions::find_index_of_next_available_cluster_of_regions(\n+  ShenandoahFreeSetPartitionId which_partition, idx_t start_index, size_t cluster_size) const {\n+  idx_t rightmost_idx = rightmost(which_partition);\n+  idx_t leftmost_idx = leftmost(which_partition);\n+  if ((rightmost_idx < leftmost_idx) || (start_index > rightmost_idx)) return _max;\n+  idx_t result = _membership[int(which_partition)].find_first_consecutive_set_bits(start_index, rightmost_idx + 1, cluster_size);\n+  if (result > rightmost_idx) {\n+    result = _max;\n+  }\n+  assert (result >= start_index, \"Requires progress\");\n+  return result;\n+}\n+\n+inline idx_t ShenandoahRegionPartitions::find_index_of_previous_available_cluster_of_regions(\n+  ShenandoahFreeSetPartitionId which_partition, idx_t last_index, size_t cluster_size) const {\n+  idx_t leftmost_idx = leftmost(which_partition);\n+  \/\/ if (leftmost_idx == max) then (last_index < leftmost_idx)\n+  if (last_index < leftmost_idx) return -1;\n+  idx_t result = _membership[int(which_partition)].find_last_consecutive_set_bits(leftmost_idx - 1, last_index, cluster_size);\n+  if (result <= leftmost_idx) {\n+    result = -1;\n+  }\n+  assert (result <= last_index, \"Requires progress\");\n+  return result;\n+}\n+\n+idx_t ShenandoahRegionPartitions::leftmost_empty(ShenandoahFreeSetPartitionId which_partition) {\n+  assert (which_partition < NumPartitions, \"selected free partition must be valid\");\n+  idx_t max_regions = _max;\n+  if (_leftmosts_empty[int(which_partition)] == _max) {\n+    return _max;\n+  }\n+  for (idx_t idx = find_index_of_next_available_region(which_partition, _leftmosts_empty[int(which_partition)]);\n+       idx < max_regions; ) {\n+    assert(in_free_set(which_partition, idx), \"Boundaries or find_last_set_bit failed: \" SSIZE_FORMAT, idx);\n+    if (_free_set->alloc_capacity(idx) == _region_size_bytes) {\n+      _leftmosts_empty[int(which_partition)] = idx;\n+      return idx;\n+    }\n+    idx = find_index_of_next_available_region(which_partition, idx + 1);\n+  }\n+  _leftmosts_empty[int(which_partition)] = _max;\n+  _rightmosts_empty[int(which_partition)] = -1;\n+  return _max;\n+}\n+\n+idx_t ShenandoahRegionPartitions::rightmost_empty(ShenandoahFreeSetPartitionId which_partition) {\n+  assert (which_partition < NumPartitions, \"selected free partition must be valid\");\n+  if (_rightmosts_empty[int(which_partition)] < 0) {\n+    return -1;\n+  }\n+  for (idx_t idx = find_index_of_previous_available_region(which_partition, _rightmosts_empty[int(which_partition)]);\n+       idx >= 0; ) {\n+    assert(in_free_set(which_partition, idx), \"Boundaries or find_last_set_bit failed: \" SSIZE_FORMAT, idx);\n+    if (_free_set->alloc_capacity(idx) == _region_size_bytes) {\n+      _rightmosts_empty[int(which_partition)] = idx;\n+      return idx;\n+    }\n+    idx = find_index_of_previous_available_region(which_partition, idx - 1);\n+  }\n+  _leftmosts_empty[int(which_partition)] = _max;\n+  _rightmosts_empty[int(which_partition)] = -1;\n+  return -1;\n+}\n+\n+\n+#ifdef ASSERT\n+void ShenandoahRegionPartitions::assert_bounds() {\n+\n+  idx_t leftmosts[UIntNumPartitions];\n+  idx_t rightmosts[UIntNumPartitions];\n+  idx_t empty_leftmosts[UIntNumPartitions];\n+  idx_t empty_rightmosts[UIntNumPartitions];\n+\n+  for (uint i = 0; i < UIntNumPartitions; i++) {\n+    leftmosts[i] = _max;\n+    empty_leftmosts[i] = _max;\n+    rightmosts[i] = -1;\n+    empty_rightmosts[i] = -1;\n+  }\n+\n+  for (idx_t i = 0; i < _max; i++) {\n+    ShenandoahFreeSetPartitionId partition = membership(i);\n+    switch (partition) {\n+      case ShenandoahFreeSetPartitionId::NotFree:\n+        break;\n+\n+      case ShenandoahFreeSetPartitionId::Mutator:\n+      case ShenandoahFreeSetPartitionId::Collector:\n+      case ShenandoahFreeSetPartitionId::OldCollector:\n+      {\n+        size_t capacity = _free_set->alloc_capacity(i);\n+        bool is_empty = (capacity == _region_size_bytes);\n+        assert(capacity > 0, \"free regions must have allocation capacity\");\n+        if (i < leftmosts[int(partition)]) {\n+          leftmosts[int(partition)] = i;\n+        }\n+        if (is_empty && (i < empty_leftmosts[int(partition)])) {\n+          empty_leftmosts[int(partition)] = i;\n+        }\n+        if (i > rightmosts[int(partition)]) {\n+          rightmosts[int(partition)] = i;\n+        }\n+        if (is_empty && (i > empty_rightmosts[int(partition)])) {\n+          empty_rightmosts[int(partition)] = i;\n+        }\n+        break;\n+      }\n+\n+      default:\n+        ShouldNotReachHere();\n+    }\n+  }\n+\n+  \/\/ Performance invariants. Failing these would not break the free partition, but performance would suffer.\n+  assert (leftmost(ShenandoahFreeSetPartitionId::Mutator) <= _max,\n+          \"leftmost in bounds: \"  SSIZE_FORMAT \" < \" SSIZE_FORMAT, leftmost(ShenandoahFreeSetPartitionId::Mutator),  _max);\n+  assert (rightmost(ShenandoahFreeSetPartitionId::Mutator) < _max,\n+          \"rightmost in bounds: \"  SSIZE_FORMAT \" < \" SSIZE_FORMAT, rightmost(ShenandoahFreeSetPartitionId::Mutator),  _max);\n+\n+  assert (leftmost(ShenandoahFreeSetPartitionId::Mutator) == _max\n+          || partition_id_matches(leftmost(ShenandoahFreeSetPartitionId::Mutator), ShenandoahFreeSetPartitionId::Mutator),\n+          \"leftmost region should be free: \" SSIZE_FORMAT,  leftmost(ShenandoahFreeSetPartitionId::Mutator));\n+  assert (leftmost(ShenandoahFreeSetPartitionId::Mutator) == _max\n+          || partition_id_matches(rightmost(ShenandoahFreeSetPartitionId::Mutator), ShenandoahFreeSetPartitionId::Mutator),\n+          \"rightmost region should be free: \" SSIZE_FORMAT, rightmost(ShenandoahFreeSetPartitionId::Mutator));\n+\n+  \/\/ If Mutator partition is empty, leftmosts will both equal max, rightmosts will both equal zero.\n+  \/\/ Likewise for empty region partitions.\n+  idx_t beg_off = leftmosts[int(ShenandoahFreeSetPartitionId::Mutator)];\n+  idx_t end_off = rightmosts[int(ShenandoahFreeSetPartitionId::Mutator)];\n+  assert (beg_off >= leftmost(ShenandoahFreeSetPartitionId::Mutator),\n+          \"free regions before the leftmost: \" SSIZE_FORMAT \", bound \" SSIZE_FORMAT,\n+          beg_off, leftmost(ShenandoahFreeSetPartitionId::Mutator));\n+  assert (end_off <= rightmost(ShenandoahFreeSetPartitionId::Mutator),\n+          \"free regions past the rightmost: \" SSIZE_FORMAT \", bound \" SSIZE_FORMAT,\n+          end_off, rightmost(ShenandoahFreeSetPartitionId::Mutator));\n+\n+  beg_off = empty_leftmosts[int(ShenandoahFreeSetPartitionId::Mutator)];\n+  end_off = empty_rightmosts[int(ShenandoahFreeSetPartitionId::Mutator)];\n+  assert (beg_off >= leftmost_empty(ShenandoahFreeSetPartitionId::Mutator),\n+          \"free empty regions before the leftmost: \" SSIZE_FORMAT \", bound \" SSIZE_FORMAT,\n+          beg_off, leftmost_empty(ShenandoahFreeSetPartitionId::Mutator));\n+  assert (end_off <= rightmost_empty(ShenandoahFreeSetPartitionId::Mutator),\n+          \"free empty regions past the rightmost: \" SSIZE_FORMAT \", bound \" SSIZE_FORMAT,\n+          end_off, rightmost_empty(ShenandoahFreeSetPartitionId::Mutator));\n+\n+  \/\/ Performance invariants. Failing these would not break the free partition, but performance would suffer.\n+  assert (leftmost(ShenandoahFreeSetPartitionId::Collector) <= _max, \"leftmost in bounds: \"  SSIZE_FORMAT \" < \" SSIZE_FORMAT,\n+          leftmost(ShenandoahFreeSetPartitionId::Collector),  _max);\n+  assert (rightmost(ShenandoahFreeSetPartitionId::Collector) < _max, \"rightmost in bounds: \"  SSIZE_FORMAT \" < \" SSIZE_FORMAT,\n+          rightmost(ShenandoahFreeSetPartitionId::Collector),  _max);\n+\n+  assert (leftmost(ShenandoahFreeSetPartitionId::Collector) == _max\n+          || partition_id_matches(leftmost(ShenandoahFreeSetPartitionId::Collector), ShenandoahFreeSetPartitionId::Collector),\n+          \"leftmost region should be free: \" SSIZE_FORMAT,  leftmost(ShenandoahFreeSetPartitionId::Collector));\n+  assert (leftmost(ShenandoahFreeSetPartitionId::Collector) == _max\n+          || partition_id_matches(rightmost(ShenandoahFreeSetPartitionId::Collector), ShenandoahFreeSetPartitionId::Collector),\n+          \"rightmost region should be free: \" SSIZE_FORMAT, rightmost(ShenandoahFreeSetPartitionId::Collector));\n+\n+  \/\/ If Collector partition is empty, leftmosts will both equal max, rightmosts will both equal zero.\n+  \/\/ Likewise for empty region partitions.\n+  beg_off = leftmosts[int(ShenandoahFreeSetPartitionId::Collector)];\n+  end_off = rightmosts[int(ShenandoahFreeSetPartitionId::Collector)];\n+  assert (beg_off >= leftmost(ShenandoahFreeSetPartitionId::Collector),\n+          \"free regions before the leftmost: \" SSIZE_FORMAT \", bound \" SSIZE_FORMAT,\n+          beg_off, leftmost(ShenandoahFreeSetPartitionId::Collector));\n+  assert (end_off <= rightmost(ShenandoahFreeSetPartitionId::Collector),\n+          \"free regions past the rightmost: \" SSIZE_FORMAT \", bound \" SSIZE_FORMAT,\n+          end_off, rightmost(ShenandoahFreeSetPartitionId::Collector));\n+\n+  beg_off = empty_leftmosts[int(ShenandoahFreeSetPartitionId::Collector)];\n+  end_off = empty_rightmosts[int(ShenandoahFreeSetPartitionId::Collector)];\n+  assert (beg_off >= _leftmosts_empty[int(ShenandoahFreeSetPartitionId::Collector)],\n+          \"free empty regions before the leftmost: \" SSIZE_FORMAT \", bound \" SSIZE_FORMAT,\n+          beg_off, leftmost_empty(ShenandoahFreeSetPartitionId::Collector));\n+  assert (end_off <= _rightmosts_empty[int(ShenandoahFreeSetPartitionId::Collector)],\n+          \"free empty regions past the rightmost: \" SSIZE_FORMAT \", bound \" SSIZE_FORMAT,\n+          end_off, rightmost_empty(ShenandoahFreeSetPartitionId::Collector));\n+\n+  \/\/ Performance invariants. Failing these would not break the free partition, but performance would suffer.\n+  assert (leftmost(ShenandoahFreeSetPartitionId::OldCollector) <= _max, \"leftmost in bounds: \"  SSIZE_FORMAT \" < \" SSIZE_FORMAT,\n+          leftmost(ShenandoahFreeSetPartitionId::OldCollector),  _max);\n+  assert (rightmost(ShenandoahFreeSetPartitionId::OldCollector) < _max, \"rightmost in bounds: \"  SSIZE_FORMAT \" < \" SSIZE_FORMAT,\n+          rightmost(ShenandoahFreeSetPartitionId::OldCollector),  _max);\n+\n+  assert (leftmost(ShenandoahFreeSetPartitionId::OldCollector) == _max\n+          || partition_id_matches(leftmost(ShenandoahFreeSetPartitionId::OldCollector),\n+                                  ShenandoahFreeSetPartitionId::OldCollector),\n+          \"leftmost region should be free: \" SSIZE_FORMAT,  leftmost(ShenandoahFreeSetPartitionId::OldCollector));\n+  assert (leftmost(ShenandoahFreeSetPartitionId::OldCollector) == _max\n+          || partition_id_matches(rightmost(ShenandoahFreeSetPartitionId::OldCollector),\n+                                  ShenandoahFreeSetPartitionId::OldCollector),\n+          \"rightmost region should be free: \" SSIZE_FORMAT, rightmost(ShenandoahFreeSetPartitionId::OldCollector));\n+\n+  \/\/ If OldCollector partition is empty, leftmosts will both equal max, rightmosts will both equal zero.\n+  \/\/ Likewise for empty region partitions.\n+  beg_off = leftmosts[int(ShenandoahFreeSetPartitionId::OldCollector)];\n+  end_off = rightmosts[int(ShenandoahFreeSetPartitionId::OldCollector)];\n+  assert (beg_off >= leftmost(ShenandoahFreeSetPartitionId::OldCollector),\n+          \"free regions before the leftmost: \" SSIZE_FORMAT \", bound \" SSIZE_FORMAT,\n+          beg_off, leftmost(ShenandoahFreeSetPartitionId::OldCollector));\n+  assert (end_off <= rightmost(ShenandoahFreeSetPartitionId::OldCollector),\n+          \"free regions past the rightmost: \" SSIZE_FORMAT \", bound \" SSIZE_FORMAT,\n+          end_off, rightmost(ShenandoahFreeSetPartitionId::OldCollector));\n+\n+  beg_off = empty_leftmosts[int(ShenandoahFreeSetPartitionId::OldCollector)];\n+  end_off = empty_rightmosts[int(ShenandoahFreeSetPartitionId::OldCollector)];\n+  assert (beg_off >= _leftmosts_empty[int(ShenandoahFreeSetPartitionId::OldCollector)],\n+          \"free empty regions before the leftmost: \" SSIZE_FORMAT \", bound \" SSIZE_FORMAT,\n+          beg_off, leftmost_empty(ShenandoahFreeSetPartitionId::OldCollector));\n+  assert (end_off <= _rightmosts_empty[int(ShenandoahFreeSetPartitionId::OldCollector)],\n+          \"free empty regions past the rightmost: \" SSIZE_FORMAT \", bound \" SSIZE_FORMAT,\n+          end_off, rightmost_empty(ShenandoahFreeSetPartitionId::OldCollector));\n+}\n+#endif\n+\n@@ -37,3 +731,2 @@\n-  _mutator_free_bitmap(max_regions, mtGC),\n-  _collector_free_bitmap(max_regions, mtGC),\n-  _max(max_regions)\n+  _partitions(max_regions, this),\n+  _alloc_bias_weight(0)\n@@ -44,1 +737,1 @@\n-void ShenandoahFreeSet::increase_used(size_t num_bytes) {\n+void ShenandoahFreeSet::add_promoted_in_place_region_to_old_collector(ShenandoahHeapRegion* region) {\n@@ -46,4 +739,9 @@\n-  _used += num_bytes;\n-\n-  assert(_used <= _capacity, \"must not use more than we have: used: \" SIZE_FORMAT\n-         \", capacity: \" SIZE_FORMAT \", num_bytes: \" SIZE_FORMAT, _used, _capacity, num_bytes);\n+  size_t plab_min_size_in_bytes = ShenandoahGenerationalHeap::heap()->plab_min_size() * HeapWordSize;\n+  size_t idx = region->index();\n+  size_t capacity = alloc_capacity(region);\n+  assert(_partitions.membership(idx) == ShenandoahFreeSetPartitionId::NotFree,\n+         \"Regions promoted in place should have been excluded from Mutator partition\");\n+  if (capacity >= plab_min_size_in_bytes) {\n+    _partitions.make_free(idx, ShenandoahFreeSetPartitionId::OldCollector, capacity);\n+    _heap->old_generation()->augment_promoted_reserve(capacity);\n+  }\n@@ -52,4 +750,12 @@\n-bool ShenandoahFreeSet::is_mutator_free(size_t idx) const {\n-  assert (idx < _max, \"index is sane: \" SIZE_FORMAT \" < \" SIZE_FORMAT \" (left: \" SIZE_FORMAT \", right: \" SIZE_FORMAT \")\",\n-          idx, _max, _mutator_leftmost, _mutator_rightmost);\n-  return _mutator_free_bitmap.at(idx);\n+HeapWord* ShenandoahFreeSet::allocate_from_partition_with_affiliation(ShenandoahAffiliation affiliation,\n+                                                                      ShenandoahAllocRequest& req, bool& in_new_region) {\n+\n+  shenandoah_assert_heaplocked();\n+  ShenandoahFreeSetPartitionId which_partition = req.is_old()? ShenandoahFreeSetPartitionId::OldCollector: ShenandoahFreeSetPartitionId::Collector;\n+  if (_partitions.alloc_from_left_bias(which_partition)) {\n+    ShenandoahLeftRightIterator iterator(&_partitions, which_partition, affiliation == ShenandoahAffiliation::FREE);\n+    return allocate_with_affiliation(iterator, affiliation, req, in_new_region);\n+  } else {\n+    ShenandoahRightLeftIterator iterator(&_partitions, which_partition, affiliation == ShenandoahAffiliation::FREE);\n+    return allocate_with_affiliation(iterator, affiliation, req, in_new_region);\n+  }\n@@ -58,4 +764,14 @@\n-bool ShenandoahFreeSet::is_collector_free(size_t idx) const {\n-  assert (idx < _max, \"index is sane: \" SIZE_FORMAT \" < \" SIZE_FORMAT \" (left: \" SIZE_FORMAT \", right: \" SIZE_FORMAT \")\",\n-          idx, _max, _collector_leftmost, _collector_rightmost);\n-  return _collector_free_bitmap.at(idx);\n+template<typename Iter>\n+HeapWord* ShenandoahFreeSet::allocate_with_affiliation(Iter& iterator, ShenandoahAffiliation affiliation, ShenandoahAllocRequest& req, bool& in_new_region) {\n+  for (idx_t idx = iterator.current(); iterator.has_next(); idx = iterator.next()) {\n+    ShenandoahHeapRegion* r = _heap->get_region(idx);\n+    if (r->affiliation() == affiliation) {\n+      HeapWord* result = try_allocate_in(r, req, in_new_region);\n+      if (result != nullptr) {\n+        return result;\n+      }\n+    }\n+  }\n+  log_debug(gc, free)(\"Could not allocate collector region with affiliation: %s for request \" PTR_FORMAT,\n+                      shenandoah_affiliation_name(affiliation), p2i(&req));\n+  return nullptr;\n@@ -65,0 +781,2 @@\n+  shenandoah_assert_heaplocked();\n+\n@@ -70,3 +788,3 @@\n-  \/\/ Allocations are biased: new application allocs go to beginning of the heap, and GC allocs\n-  \/\/ go to the end. This makes application allocation faster, because we would clear lots\n-  \/\/ of regions from the beginning most of the time.\n+  \/\/ Allocations are biased: GC allocations are taken from the high end of the heap.  Regular (and TLAB)\n+  \/\/ mutator allocations are taken from the middle of heap, below the memory reserved for Collector.\n+  \/\/ Humongous mutator allocations are taken from the bottom of the heap.\n@@ -74,2 +792,4 @@\n-  \/\/ Free set maintains mutator and collector views, and normally they allocate in their views only,\n-  \/\/ unless we special cases for stealing and mixed allocations.\n+  \/\/ Free set maintains mutator and collector partitions.  Normally, each allocates only from its partition,\n+  \/\/ except in special cases when the collector steals regions from the mutator partition.\n+\n+  \/\/ Overwrite with non-zero (non-NULL) values only if necessary for allocation bookkeeping.\n@@ -79,15 +799,2 @@\n-    case ShenandoahAllocRequest::_alloc_shared: {\n-\n-      \/\/ Try to allocate in the mutator view\n-      for (size_t idx = _mutator_leftmost; idx <= _mutator_rightmost; idx++) {\n-        if (is_mutator_free(idx)) {\n-          HeapWord* result = try_allocate_in(_heap->get_region(idx), req, in_new_region);\n-          if (result != nullptr) {\n-            return result;\n-          }\n-        }\n-      }\n-\n-      \/\/ There is no recovery. Mutator does not touch collector view at all.\n-      break;\n-    }\n+    case ShenandoahAllocRequest::_alloc_shared:\n+      return allocate_for_mutator(req, in_new_region);\n@@ -95,40 +802,3 @@\n-    case ShenandoahAllocRequest::_alloc_shared_gc: {\n-      \/\/ size_t is unsigned, need to dodge underflow when _leftmost = 0\n-\n-      \/\/ Fast-path: try to allocate in the collector view first\n-      for (size_t c = _collector_rightmost + 1; c > _collector_leftmost; c--) {\n-        size_t idx = c - 1;\n-        if (is_collector_free(idx)) {\n-          HeapWord* result = try_allocate_in(_heap->get_region(idx), req, in_new_region);\n-          if (result != nullptr) {\n-            return result;\n-          }\n-        }\n-      }\n-\n-      \/\/ No dice. Can we borrow space from mutator view?\n-      if (!ShenandoahEvacReserveOverflow) {\n-        return nullptr;\n-      }\n-\n-      \/\/ Try to steal the empty region from the mutator view\n-      for (size_t c = _mutator_rightmost + 1; c > _mutator_leftmost; c--) {\n-        size_t idx = c - 1;\n-        if (is_mutator_free(idx)) {\n-          ShenandoahHeapRegion* r = _heap->get_region(idx);\n-          if (can_allocate_from(r)) {\n-            flip_to_gc(r);\n-            HeapWord *result = try_allocate_in(r, req, in_new_region);\n-            if (result != nullptr) {\n-              return result;\n-            }\n-          }\n-        }\n-      }\n-\n-      \/\/ No dice. Do not try to mix mutator and GC allocations, because\n-      \/\/ URWM moves due to GC allocations would expose unparsable mutator\n-      \/\/ allocations.\n-\n-      break;\n-    }\n+    case ShenandoahAllocRequest::_alloc_plab:\n+    case ShenandoahAllocRequest::_alloc_shared_gc:\n+      return allocate_for_collector(req, in_new_region);\n@@ -138,1 +808,0 @@\n-\n@@ -142,2 +811,2 @@\n-HeapWord* ShenandoahFreeSet::try_allocate_in(ShenandoahHeapRegion* r, ShenandoahAllocRequest& req, bool& in_new_region) {\n-  assert (!has_no_alloc_capacity(r), \"Performance: should avoid full regions on this path: \" SIZE_FORMAT, r->index());\n+HeapWord* ShenandoahFreeSet::allocate_for_mutator(ShenandoahAllocRequest &req, bool &in_new_region) {\n+  update_allocation_bias();\n@@ -145,2 +814,2 @@\n-  if (_heap->is_concurrent_weak_root_in_progress() &&\n-      r->is_trash()) {\n+  if (_partitions.is_empty(ShenandoahFreeSetPartitionId::Mutator)) {\n+    \/\/ There is no recovery. Mutator does not touch collector view at all.\n@@ -150,1 +819,8 @@\n-  try_recycle_trashed(r);\n+  \/\/ Try to allocate in the mutator view\n+  if (_partitions.alloc_from_left_bias(ShenandoahFreeSetPartitionId::Mutator)) {\n+    \/\/ Allocate from low to high memory.  This keeps the range of fully empty regions more tightly packed.\n+    \/\/ Note that the most recently allocated regions tend not to be evacuated in a given GC cycle.  So this\n+    \/\/ tends to accumulate \"fragmented\" uncollected regions in high memory.\n+    ShenandoahLeftRightIterator iterator(&_partitions, ShenandoahFreeSetPartitionId::Mutator);\n+    return allocate_from_regions(iterator, req, in_new_region);\n+  }\n@@ -152,1 +828,4 @@\n-  in_new_region = r->is_empty();\n+  \/\/ Allocate from high to low memory. This preserves low memory for humongous allocations.\n+  ShenandoahRightLeftIterator iterator(&_partitions, ShenandoahFreeSetPartitionId::Mutator);\n+  return allocate_from_regions(iterator, req, in_new_region);\n+}\n@@ -154,2 +833,25 @@\n-  HeapWord* result = nullptr;\n-  size_t size = req.size();\n+void ShenandoahFreeSet::update_allocation_bias() {\n+  if (_alloc_bias_weight-- <= 0) {\n+    \/\/ We have observed that regions not collected in previous GC cycle tend to congregate at one end or the other\n+    \/\/ of the heap.  Typically, these are the more recently engaged regions and the objects in these regions have not\n+    \/\/ yet had a chance to die (and\/or are treated as floating garbage).  If we use the same allocation bias on each\n+    \/\/ GC pass, these \"most recently\" engaged regions for GC pass N will also be the \"most recently\" engaged regions\n+    \/\/ for GC pass N+1, and the relatively large amount of live data and\/or floating garbage introduced\n+    \/\/ during the most recent GC pass may once again prevent the region from being collected.  We have found that\n+    \/\/ alternating the allocation behavior between GC passes improves evacuation performance by 3-7% on certain\n+    \/\/ benchmarks.  In the best case, this has the effect of consuming these partially consumed regions before\n+    \/\/ the start of the next mark cycle so all of their garbage can be efficiently reclaimed.\n+    \/\/\n+    \/\/ First, finish consuming regions that are already partially consumed so as to more tightly limit ranges of\n+    \/\/ available regions.  Other potential benefits:\n+    \/\/  1. Eventual collection set has fewer regions because we have packed newly allocated objects into fewer regions\n+    \/\/  2. We preserve the \"empty\" regions longer into the GC cycle, reducing likelihood of allocation failures\n+    \/\/     late in the GC cycle.\n+    idx_t non_empty_on_left = (_partitions.leftmost_empty(ShenandoahFreeSetPartitionId::Mutator)\n+                               - _partitions.leftmost(ShenandoahFreeSetPartitionId::Mutator));\n+    idx_t non_empty_on_right = (_partitions.rightmost(ShenandoahFreeSetPartitionId::Mutator)\n+                                - _partitions.rightmost_empty(ShenandoahFreeSetPartitionId::Mutator));\n+    _partitions.set_bias_from_left_to_right(ShenandoahFreeSetPartitionId::Mutator, (non_empty_on_right < non_empty_on_left));\n+    _alloc_bias_weight = INITIAL_ALLOC_BIAS_WEIGHT;\n+  }\n+}\n@@ -157,8 +859,10 @@\n-  if (req.is_lab_alloc()) {\n-    size_t free = align_down(r->free() >> LogHeapWordSize, MinObjAlignment);\n-    if (size > free) {\n-      size = free;\n-    }\n-    if (size >= req.min_size()) {\n-      result = r->allocate(size, req.type());\n-      assert (result != nullptr, \"Allocation must succeed: free \" SIZE_FORMAT \", actual \" SIZE_FORMAT, free, size);\n+template<typename Iter>\n+HeapWord* ShenandoahFreeSet::allocate_from_regions(Iter& iterator, ShenandoahAllocRequest &req, bool &in_new_region) {\n+  for (idx_t idx = iterator.current(); iterator.has_next(); idx = iterator.next()) {\n+    ShenandoahHeapRegion* r = _heap->get_region(idx);\n+    size_t min_size = (req.type() == ShenandoahAllocRequest::_alloc_tlab) ? req.min_size() : req.size();\n+    if (alloc_capacity(r) >= min_size * HeapWordSize) {\n+      HeapWord* result = try_allocate_in(r, req, in_new_region);\n+      if (result != nullptr) {\n+        return result;\n+      }\n@@ -166,2 +870,0 @@\n-  } else {\n-    result = r->allocate(size, req.type());\n@@ -169,0 +871,2 @@\n+  return nullptr;\n+}\n@@ -170,0 +874,4 @@\n+HeapWord* ShenandoahFreeSet::allocate_for_collector(ShenandoahAllocRequest &req, bool &in_new_region) {\n+  \/\/ Fast-path: try to allocate in the collector view first\n+  HeapWord* result;\n+  result = allocate_from_partition_with_affiliation(req.affiliation(), req, in_new_region);\n@@ -171,3 +879,9 @@\n-    \/\/ Allocation successful, bump stats:\n-    if (req.is_mutator_alloc()) {\n-      increase_used(size * HeapWordSize);\n+    return result;\n+  }\n+\n+  bool allow_new_region = can_allocate_in_new_region(req);\n+  if (allow_new_region) {\n+    \/\/ Try a free region that is dedicated to GC allocations.\n+    result = allocate_from_partition_with_affiliation(ShenandoahAffiliation::FREE, req, in_new_region);\n+    if (result != nullptr) {\n+      return result;\n@@ -175,0 +889,1 @@\n+  }\n@@ -176,2 +891,4 @@\n-    \/\/ Record actual allocation size\n-    req.set_actual_size(size);\n+  \/\/ No dice. Can we borrow space from mutator view?\n+  if (!ShenandoahEvacReserveOverflow) {\n+    return nullptr;\n+  }\n@@ -179,3 +896,3 @@\n-    if (req.is_gc_alloc()) {\n-      r->set_update_watermark(r->top());\n-    }\n+  if (!allow_new_region && req.is_old() && (_heap->young_generation()->free_unaffiliated_regions() > 0)) {\n+    \/\/ This allows us to flip a mutator region to old_collector\n+    allow_new_region = true;\n@@ -184,7 +901,9 @@\n-  if (result == nullptr || has_no_alloc_capacity(r)) {\n-    \/\/ Region cannot afford this or future allocations. Retire it.\n-    \/\/\n-    \/\/ While this seems a bit harsh, especially in the case when this large allocation does not\n-    \/\/ fit, but the next small one would, we are risking to inflate scan times when lots of\n-    \/\/ almost-full regions precede the fully-empty region where we want allocate the entire TLAB.\n-    \/\/ TODO: Record first fully-empty region, and use that for large allocations\n+  \/\/ We should expand old-gen if this can prevent an old-gen evacuation failure.  We don't care so much about\n+  \/\/ promotion failures since they can be mitigated in a subsequent GC pass.  Would be nice to know if this\n+  \/\/ allocation request is for evacuation or promotion.  Individual threads limit their use of PLAB memory for\n+  \/\/ promotions, so we already have an assurance that any additional memory set aside for old-gen will be used\n+  \/\/ only for old-gen evacuations.\n+  if (allow_new_region) {\n+    \/\/ Try to steal an empty region from the mutator view.\n+    result = try_allocate_from_mutator(req, in_new_region);\n+  }\n@@ -192,6 +911,28 @@\n-    \/\/ Record the remainder as allocation waste\n-    if (req.is_mutator_alloc()) {\n-      size_t waste = r->free();\n-      if (waste > 0) {\n-        increase_used(waste);\n-        _heap->notify_mutator_alloc_words(waste >> LogHeapWordSize, true);\n+  \/\/ This is it. Do not try to mix mutator and GC allocations, because adjusting region UWM\n+  \/\/ due to GC allocations would expose unparsable mutator allocations.\n+  return result;\n+}\n+\n+bool ShenandoahFreeSet::can_allocate_in_new_region(const ShenandoahAllocRequest& req) {\n+  if (!_heap->mode()->is_generational()) {\n+    return true;\n+  }\n+\n+  assert(req.is_old() || req.is_young(), \"Should request affiliation\");\n+  return (req.is_old() && _heap->old_generation()->free_unaffiliated_regions() > 0)\n+         || (req.is_young() && _heap->young_generation()->free_unaffiliated_regions() > 0);\n+}\n+\n+HeapWord* ShenandoahFreeSet::try_allocate_from_mutator(ShenandoahAllocRequest& req, bool& in_new_region) {\n+  \/\/ The collector prefers to keep longer lived regions toward the right side of the heap, so it always\n+  \/\/ searches for regions from right to left here.\n+  ShenandoahRightLeftIterator iterator(&_partitions, ShenandoahFreeSetPartitionId::Mutator, true);\n+  for (idx_t idx = iterator.current(); iterator.has_next(); idx = iterator.next()) {\n+    ShenandoahHeapRegion* r = _heap->get_region(idx);\n+    if (can_allocate_from(r)) {\n+      if (req.is_old()) {\n+        if (!flip_to_old_gc(r)) {\n+          continue;\n+        }\n+      } else {\n+        flip_to_gc(r);\n@@ -199,0 +940,4 @@\n+      \/\/ Region r is entirely empty.  If try_allocate_in fails on region r, something else is really wrong.\n+      \/\/ Don't bother to retry with other regions.\n+      log_debug(gc, free)(\"Flipped region \" SIZE_FORMAT \" to gc for request: \" PTR_FORMAT, idx, p2i(&req));\n+      return try_allocate_in(r, req, in_new_region);\n@@ -200,0 +945,1 @@\n+  }\n@@ -201,6 +947,29 @@\n-    size_t num = r->index();\n-    _collector_free_bitmap.clear_bit(num);\n-    _mutator_free_bitmap.clear_bit(num);\n-    \/\/ Touched the bounds? Need to update:\n-    if (touches_bounds(num)) {\n-      adjust_bounds();\n+  return nullptr;\n+}\n+\n+\/\/ This work method takes an argument corresponding to the number of bytes\n+\/\/ free in a region, and returns the largest amount in heapwords that can be allocated\n+\/\/ such that both of the following conditions are satisfied:\n+\/\/\n+\/\/ 1. it is a multiple of card size\n+\/\/ 2. any remaining shard may be filled with a filler object\n+\/\/\n+\/\/ The idea is that the allocation starts and ends at card boundaries. Because\n+\/\/ a region ('s end) is card-aligned, the remainder shard that must be filled is\n+\/\/ at the start of the free space.\n+\/\/\n+\/\/ This is merely a helper method to use for the purpose of such a calculation.\n+size_t ShenandoahFreeSet::get_usable_free_words(size_t free_bytes) const {\n+  \/\/ e.g. card_size is 512, card_shift is 9, min_fill_size() is 8\n+  \/\/      free is 514\n+  \/\/      usable_free is 512, which is decreased to 0\n+  size_t usable_free = (free_bytes \/ CardTable::card_size()) << CardTable::card_shift();\n+  assert(usable_free <= free_bytes, \"Sanity check\");\n+  if ((free_bytes != usable_free) && (free_bytes - usable_free < ShenandoahHeap::min_fill_size() * HeapWordSize)) {\n+    \/\/ After aligning to card multiples, the remainder would be smaller than\n+    \/\/ the minimum filler object, so we'll need to take away another card's\n+    \/\/ worth to construct a filler object.\n+    if (usable_free >= CardTable::card_size()) {\n+      usable_free -= CardTable::card_size();\n+    } else {\n+      assert(usable_free == 0, \"usable_free is a multiple of card_size and card_size > min_fill_size\");\n@@ -208,1 +977,0 @@\n-    assert_bounds();\n@@ -210,1 +978,2 @@\n-  return result;\n+\n+  return usable_free \/ HeapWordSize;\n@@ -213,2 +982,16 @@\n-bool ShenandoahFreeSet::touches_bounds(size_t num) const {\n-  return num == _collector_leftmost || num == _collector_rightmost || num == _mutator_leftmost || num == _mutator_rightmost;\n+\/\/ Given a size argument, which is a multiple of card size, a request struct\n+\/\/ for a PLAB, and an old region, return a pointer to the allocated space for\n+\/\/ a PLAB which is card-aligned and where any remaining shard in the region\n+\/\/ has been suitably filled by a filler object.\n+\/\/ It is assumed (and assertion-checked) that such an allocation is always possible.\n+HeapWord* ShenandoahFreeSet::allocate_aligned_plab(size_t size, ShenandoahAllocRequest& req, ShenandoahHeapRegion* r) {\n+  assert(_heap->mode()->is_generational(), \"PLABs are only for generational mode\");\n+  assert(r->is_old(), \"All PLABs reside in old-gen\");\n+  assert(!req.is_mutator_alloc(), \"PLABs should not be allocated by mutators.\");\n+  assert(is_aligned(size, CardTable::card_size_in_words()), \"Align by design\");\n+\n+  HeapWord* result = r->allocate_aligned(size, req, CardTable::card_size());\n+  assert(result != nullptr, \"Allocation cannot fail\");\n+  assert(r->top() <= r->end(), \"Allocation cannot span end of region\");\n+  assert(is_aligned(result, CardTable::card_size_in_words()), \"Align by design\");\n+  return result;\n@@ -217,6 +1000,13 @@\n-void ShenandoahFreeSet::recompute_bounds() {\n-  \/\/ Reset to the most pessimistic case:\n-  _mutator_rightmost = _max - 1;\n-  _mutator_leftmost = 0;\n-  _collector_rightmost = _max - 1;\n-  _collector_leftmost = 0;\n+HeapWord* ShenandoahFreeSet::try_allocate_in(ShenandoahHeapRegion* r, ShenandoahAllocRequest& req, bool& in_new_region) {\n+  assert (has_alloc_capacity(r), \"Performance: should avoid full regions on this path: \" SIZE_FORMAT, r->index());\n+  if (_heap->is_concurrent_weak_root_in_progress() && r->is_trash()) {\n+    \/\/ We cannot use this region for allocation when weak roots are in progress because the collector may need\n+    \/\/ to reference unmarked oops during concurrent classunloading. The collector also needs accurate marking\n+    \/\/ information to determine which weak handles need to be null'd out. If the region is recycled before weak\n+    \/\/ roots processing has finished, weak root processing may fail to null out a handle into a trashed region.\n+    \/\/ This turns the handle into a dangling pointer and will crash or corrupt the heap.\n+    return nullptr;\n+  }\n+  HeapWord* result = nullptr;\n+  r->try_recycle_under_lock();\n+  in_new_region = r->is_empty();\n@@ -224,3 +1014,16 @@\n-  \/\/ ...and adjust from there\n-  adjust_bounds();\n-}\n+  if (in_new_region) {\n+    log_debug(gc, free)(\"Using new region (\" SIZE_FORMAT \") for %s (\" PTR_FORMAT \").\",\n+                        r->index(), ShenandoahAllocRequest::alloc_type_to_string(req.type()), p2i(&req));\n+    assert(!r->is_affiliated(), \"New region \" SIZE_FORMAT \" should be unaffiliated\", r->index());\n+    r->set_affiliation(req.affiliation());\n+    if (r->is_old()) {\n+      \/\/ Any OLD region allocated during concurrent coalesce-and-fill does not need to be coalesced and filled because\n+      \/\/ all objects allocated within this region are above TAMS (and thus are implicitly marked).  In case this is an\n+      \/\/ OLD region and concurrent preparation for mixed evacuations visits this region before the start of the next\n+      \/\/ old-gen concurrent mark (i.e. this region is allocated following the start of old-gen concurrent mark but before\n+      \/\/ concurrent preparations for mixed evacuations are completed), we mark this region as not requiring any\n+      \/\/ coalesce-and-fill processing.\n+      r->end_preemptible_coalesce_and_fill();\n+      _heap->old_generation()->clear_cards_for(r);\n+    }\n+    _heap->generation_for(r->affiliation())->increment_affiliated_region_count();\n@@ -228,4 +1031,14 @@\n-void ShenandoahFreeSet::adjust_bounds() {\n-  \/\/ Rewind both mutator bounds until the next bit.\n-  while (_mutator_leftmost < _max && !is_mutator_free(_mutator_leftmost)) {\n-    _mutator_leftmost++;\n+#ifdef ASSERT\n+    ShenandoahMarkingContext* const ctx = _heap->marking_context();\n+    assert(ctx->top_at_mark_start(r) == r->bottom(), \"Newly established allocation region starts with TAMS equal to bottom\");\n+    assert(ctx->is_bitmap_range_within_region_clear(ctx->top_bitmap(r), r->end()), \"Bitmap above top_bitmap() must be clear\");\n+#endif\n+    log_debug(gc, free)(\"Using new region (\" SIZE_FORMAT \") for %s (\" PTR_FORMAT \").\",\n+                        r->index(), ShenandoahAllocRequest::alloc_type_to_string(req.type()), p2i(&req));\n+  } else {\n+    assert(r->is_affiliated(), \"Region \" SIZE_FORMAT \" that is not new should be affiliated\", r->index());\n+    if (r->affiliation() != req.affiliation()) {\n+      assert(_heap->mode()->is_generational(), \"Request for %s from %s region should only happen in generational mode.\",\n+             req.affiliation_name(), r->affiliation_name());\n+      return nullptr;\n+    }\n@@ -233,2 +1046,50 @@\n-  while (_mutator_rightmost > 0 && !is_mutator_free(_mutator_rightmost)) {\n-    _mutator_rightmost--;\n+\n+  \/\/ req.size() is in words, r->free() is in bytes.\n+  if (req.is_lab_alloc()) {\n+    size_t adjusted_size = req.size();\n+    size_t free = r->free();    \/\/ free represents bytes available within region r\n+    if (req.type() == ShenandoahAllocRequest::_alloc_plab) {\n+      \/\/ This is a PLAB allocation\n+      assert(_heap->mode()->is_generational(), \"PLABs are only for generational mode\");\n+      assert(_partitions.in_free_set(ShenandoahFreeSetPartitionId::OldCollector, r->index()),\n+             \"PLABS must be allocated in old_collector_free regions\");\n+\n+      \/\/ Need to assure that plabs are aligned on multiple of card region\n+      \/\/ Convert free from unaligned bytes to aligned number of words\n+      size_t usable_free = get_usable_free_words(free);\n+      if (adjusted_size > usable_free) {\n+        adjusted_size = usable_free;\n+      }\n+      adjusted_size = align_down(adjusted_size, CardTable::card_size_in_words());\n+      if (adjusted_size >= req.min_size()) {\n+        result = allocate_aligned_plab(adjusted_size, req, r);\n+        assert(result != nullptr, \"allocate must succeed\");\n+        req.set_actual_size(adjusted_size);\n+      } else {\n+        \/\/ Otherwise, leave result == nullptr because the adjusted size is smaller than min size.\n+        log_trace(gc, free)(\"Failed to shrink PLAB request (\" SIZE_FORMAT \") in region \" SIZE_FORMAT \" to \" SIZE_FORMAT\n+                            \" because min_size() is \" SIZE_FORMAT, req.size(), r->index(), adjusted_size, req.min_size());\n+      }\n+    } else {\n+      \/\/ This is a GCLAB or a TLAB allocation\n+      \/\/ Convert free from unaligned bytes to aligned number of words\n+      free = align_down(free >> LogHeapWordSize, MinObjAlignment);\n+      if (adjusted_size > free) {\n+        adjusted_size = free;\n+      }\n+      if (adjusted_size >= req.min_size()) {\n+        result = r->allocate(adjusted_size, req);\n+        assert (result != nullptr, \"Allocation must succeed: free \" SIZE_FORMAT \", actual \" SIZE_FORMAT, free, adjusted_size);\n+        req.set_actual_size(adjusted_size);\n+      } else {\n+        log_trace(gc, free)(\"Failed to shrink TLAB or GCLAB request (\" SIZE_FORMAT \") in region \" SIZE_FORMAT \" to \" SIZE_FORMAT\n+                            \" because min_size() is \" SIZE_FORMAT, req.size(), r->index(), adjusted_size, req.min_size());\n+      }\n+    }\n+  } else {\n+    size_t size = req.size();\n+    result = r->allocate(size, req);\n+    if (result != nullptr) {\n+      \/\/ Record actual allocation size\n+      req.set_actual_size(size);\n+    }\n@@ -236,3 +1097,21 @@\n-  \/\/ Rewind both collector bounds until the next bit.\n-  while (_collector_leftmost < _max && !is_collector_free(_collector_leftmost)) {\n-    _collector_leftmost++;\n+\n+  if (result != nullptr) {\n+    \/\/ Allocation successful, bump stats:\n+    if (req.is_mutator_alloc()) {\n+      assert(req.is_young(), \"Mutator allocations always come from young generation.\");\n+      _partitions.increase_used(ShenandoahFreeSetPartitionId::Mutator, req.actual_size() * HeapWordSize);\n+    } else {\n+      assert(req.is_gc_alloc(), \"Should be gc_alloc since req wasn't mutator alloc\");\n+\n+      \/\/ For GC allocations, we advance update_watermark because the objects relocated into this memory during\n+      \/\/ evacuation are not updated during evacuation.  For both young and old regions r, it is essential that all\n+      \/\/ PLABs be made parsable at the end of evacuation.  This is enabled by retiring all plabs at end of evacuation.\n+      r->set_update_watermark(r->top());\n+      if (r->is_old()) {\n+        _partitions.increase_used(ShenandoahFreeSetPartitionId::OldCollector, req.actual_size() * HeapWordSize);\n+        assert(req.type() != ShenandoahAllocRequest::_alloc_gclab, \"old-gen allocations use PLAB or shared allocation\");\n+        \/\/ for plabs, we'll sort the difference between evac and promotion usage when we retire the plab\n+      } else {\n+        _partitions.increase_used(ShenandoahFreeSetPartitionId::Collector, req.actual_size() * HeapWordSize);\n+      }\n+    }\n@@ -240,2 +1119,29 @@\n-  while (_collector_rightmost > 0 && !is_collector_free(_collector_rightmost)) {\n-    _collector_rightmost--;\n+\n+  static const size_t min_capacity = (size_t) (ShenandoahHeapRegion::region_size_bytes() * (1.0 - 1.0 \/ ShenandoahEvacWaste));\n+  size_t ac = alloc_capacity(r);\n+\n+  if (((result == nullptr) && (ac < min_capacity)) || (alloc_capacity(r) < PLAB::min_size() * HeapWordSize)) {\n+    \/\/ Regardless of whether this allocation succeeded, if the remaining memory is less than PLAB:min_size(), retire this region.\n+    \/\/ Note that retire_from_partition() increases used to account for waste.\n+\n+    \/\/ Also, if this allocation request failed and the consumed within this region * ShenandoahEvacWaste > region size,\n+    \/\/ then retire the region so that subsequent searches can find available memory more quickly.\n+\n+    size_t idx = r->index();\n+    ShenandoahFreeSetPartitionId orig_partition;\n+    if (req.is_mutator_alloc()) {\n+      orig_partition = ShenandoahFreeSetPartitionId::Mutator;\n+    } else if (req.type() == ShenandoahAllocRequest::_alloc_gclab) {\n+      orig_partition = ShenandoahFreeSetPartitionId::Collector;\n+    } else if (req.type() == ShenandoahAllocRequest::_alloc_plab) {\n+      orig_partition = ShenandoahFreeSetPartitionId::OldCollector;\n+    } else {\n+      assert(req.type() == ShenandoahAllocRequest::_alloc_shared_gc, \"Unexpected allocation type\");\n+      if (req.is_old()) {\n+        orig_partition = ShenandoahFreeSetPartitionId::OldCollector;\n+      } else {\n+        orig_partition = ShenandoahFreeSetPartitionId::Collector;\n+      }\n+    }\n+    _partitions.retire_from_partition(orig_partition, idx, r->used());\n+    _partitions.assert_bounds();\n@@ -243,0 +1149,1 @@\n+  return result;\n@@ -246,0 +1153,1 @@\n+  assert(req.is_mutator_alloc(), \"All humongous allocations are performed by mutator\");\n@@ -249,1 +1157,1 @@\n-  size_t num = ShenandoahHeapRegion::required_regions(words_size * HeapWordSize);\n+  idx_t num = ShenandoahHeapRegion::required_regions(words_size * HeapWordSize);\n@@ -251,2 +1159,5 @@\n-  \/\/ No regions left to satisfy allocation, bye.\n-  if (num > mutator_count()) {\n+  assert(req.is_young(), \"Humongous regions always allocated in YOUNG\");\n+  ShenandoahGeneration* generation = _heap->generation_for(req.affiliation());\n+\n+  \/\/ Check if there are enough regions left to satisfy allocation.\n+  if (num > (idx_t) _partitions.count(ShenandoahFreeSetPartitionId::Mutator)) {\n@@ -256,0 +1167,4 @@\n+  idx_t start_range = _partitions.leftmost_empty(ShenandoahFreeSetPartitionId::Mutator);\n+  idx_t end_range = _partitions.rightmost_empty(ShenandoahFreeSetPartitionId::Mutator) + 1;\n+  idx_t last_possible_start = end_range - num;\n+\n@@ -258,3 +1173,7 @@\n-\n-  size_t beg = _mutator_leftmost;\n-  size_t end = beg;\n+  idx_t beg = _partitions.find_index_of_next_available_cluster_of_regions(ShenandoahFreeSetPartitionId::Mutator,\n+                                                                          start_range, num);\n+  if (beg > last_possible_start) {\n+    \/\/ Hit the end, goodbye\n+    return nullptr;\n+  }\n+  idx_t end = beg;\n@@ -263,11 +1182,26 @@\n-    if (end >= _max) {\n-      \/\/ Hit the end, goodbye\n-      return nullptr;\n-    }\n-\n-    \/\/ If regions are not adjacent, then current [beg; end] is useless, and we may fast-forward.\n-    \/\/ If region is not completely free, the current [beg; end] is useless, and we may fast-forward.\n-    if (!is_mutator_free(end) || !can_allocate_from(_heap->get_region(end))) {\n-      end++;\n-      beg = end;\n-      continue;\n+    \/\/ We've confirmed num contiguous regions belonging to Mutator partition, so no need to confirm membership.\n+    \/\/ If region is not completely free, the current [beg; end] is useless, and we may fast-forward.  If we can extend\n+    \/\/ the existing range, we can exploit that certain regions are already known to be in the Mutator free set.\n+    while (!can_allocate_from(_heap->get_region(end))) {\n+      \/\/ region[end] is not empty, so we restart our search after region[end]\n+      idx_t slide_delta = end + 1 - beg;\n+      if (beg + slide_delta > last_possible_start) {\n+        \/\/ no room to slide\n+        return nullptr;\n+      }\n+      for (idx_t span_end = beg + num; slide_delta > 0; slide_delta--) {\n+        if (!_partitions.in_free_set(ShenandoahFreeSetPartitionId::Mutator, span_end)) {\n+          beg = _partitions.find_index_of_next_available_cluster_of_regions(ShenandoahFreeSetPartitionId::Mutator,\n+                                                                            span_end + 1, num);\n+          break;\n+        } else {\n+          beg++;\n+          span_end++;\n+        }\n+      }\n+      \/\/ Here, either beg identifies a range of num regions all of which are in the Mutator free set, or beg > last_possible_start\n+      if (beg > last_possible_start) {\n+        \/\/ Hit the end, goodbye\n+        return nullptr;\n+      }\n+      end = beg;\n@@ -285,1 +1219,0 @@\n-\n@@ -287,1 +1220,1 @@\n-  for (size_t i = beg; i <= end; i++) {\n+  for (idx_t i = beg; i <= end; i++) {\n@@ -289,1 +1222,1 @@\n-    try_recycle_trashed(r);\n+    r->try_recycle_under_lock();\n@@ -308,0 +1241,2 @@\n+    r->set_affiliation(req.affiliation());\n+    r->set_update_watermark(r->bottom());\n@@ -309,2 +1244,0 @@\n-\n-    _mutator_free_bitmap.clear_bit(r->index());\n@@ -312,5 +1245,1 @@\n-\n-  \/\/ While individual regions report their true use, all humongous regions are\n-  \/\/ marked used in the free set.\n-  increase_used(ShenandoahHeapRegion::region_size_bytes() * num);\n-\n+  generation->increase_affiliated_region_count(num);\n@@ -322,5 +1251,2 @@\n-  \/\/ Allocated at left\/rightmost? Move the bounds appropriately.\n-  if (beg == _mutator_leftmost || end == _mutator_rightmost) {\n-    adjust_bounds();\n-  }\n-  assert_bounds();\n+  \/\/ retire_range_from_partition() will adjust bounds on Mutator free set if appropriate\n+  _partitions.retire_range_from_partition(ShenandoahFreeSetPartitionId::Mutator, beg, end);\n@@ -328,0 +1254,3 @@\n+  size_t total_humongous_size = ShenandoahHeapRegion::region_size_bytes() * num;\n+  _partitions.increase_used(ShenandoahFreeSetPartitionId::Mutator, total_humongous_size);\n+  _partitions.assert_bounds();\n@@ -329,0 +1258,3 @@\n+  if (remainder != 0) {\n+    req.set_waste(ShenandoahHeapRegion::region_size_words() - remainder);\n+  }\n@@ -332,3 +1264,3 @@\n-bool ShenandoahFreeSet::can_allocate_from(ShenandoahHeapRegion *r) {\n-  return r->is_empty() || (r->is_trash() && !_heap->is_concurrent_weak_root_in_progress());\n-}\n+class ShenandoahRecycleTrashedRegionClosure final : public ShenandoahHeapRegionClosure {\n+public:\n+  ShenandoahRecycleTrashedRegionClosure(): ShenandoahHeapRegionClosure() {}\n@@ -336,6 +1268,2 @@\n-size_t ShenandoahFreeSet::alloc_capacity(ShenandoahHeapRegion *r) {\n-  if (r->is_trash()) {\n-    \/\/ This would be recycled on allocation path\n-    return ShenandoahHeapRegion::region_size_bytes();\n-  } else {\n-    return r->free();\n+  void heap_region_do(ShenandoahHeapRegion* r) {\n+    r->try_recycle();\n@@ -343,5 +1271,0 @@\n-}\n-\n-bool ShenandoahFreeSet::has_no_alloc_capacity(ShenandoahHeapRegion *r) {\n-  return alloc_capacity(r) == 0;\n-}\n@@ -349,4 +1272,2 @@\n-void ShenandoahFreeSet::try_recycle_trashed(ShenandoahHeapRegion *r) {\n-  if (r->is_trash()) {\n-    _heap->decrease_used(r->used());\n-    r->recycle();\n+  bool is_thread_safe() {\n+    return true;\n@@ -354,1 +1275,1 @@\n-}\n+};\n@@ -357,1 +1278,1 @@\n-  \/\/ lock is not reentrable, check we don't have it\n+  \/\/ lock is not non-reentrant, check we don't have it\n@@ -360,5 +1281,61 @@\n-  for (size_t i = 0; i < _heap->num_regions(); i++) {\n-    ShenandoahHeapRegion* r = _heap->get_region(i);\n-    if (r->is_trash()) {\n-      ShenandoahHeapLocker locker(_heap->lock());\n-      try_recycle_trashed(r);\n+  ShenandoahHeap* heap = ShenandoahHeap::heap();\n+  heap->assert_gc_workers(heap->workers()->active_workers());\n+\n+  ShenandoahRecycleTrashedRegionClosure closure;\n+  heap->parallel_heap_region_iterate(&closure);\n+}\n+\n+bool ShenandoahFreeSet::flip_to_old_gc(ShenandoahHeapRegion* r) {\n+  const size_t idx = r->index();\n+\n+  assert(_partitions.partition_id_matches(idx, ShenandoahFreeSetPartitionId::Mutator), \"Should be in mutator view\");\n+  assert(can_allocate_from(r), \"Should not be allocated\");\n+\n+  ShenandoahGenerationalHeap* gen_heap = ShenandoahGenerationalHeap::heap();\n+  const size_t region_capacity = alloc_capacity(r);\n+\n+  bool transferred = gen_heap->generation_sizer()->transfer_to_old(1);\n+  if (transferred) {\n+    _partitions.move_from_partition_to_partition(idx, ShenandoahFreeSetPartitionId::Mutator,\n+                                                 ShenandoahFreeSetPartitionId::OldCollector, region_capacity);\n+    _partitions.assert_bounds();\n+    _heap->old_generation()->augment_evacuation_reserve(region_capacity);\n+    return true;\n+  }\n+\n+  if (_heap->young_generation()->free_unaffiliated_regions() == 0 && _heap->old_generation()->free_unaffiliated_regions() > 0) {\n+    \/\/ Old has free unaffiliated regions, but it couldn't use them for allocation (likely because they\n+    \/\/ are trash and weak roots are in process). In this scenario, we aren't really stealing from the\n+    \/\/ mutator (they have nothing to steal), but they do have a usable region in their partition. What\n+    \/\/ we want to do here is swap that region from the mutator partition with one from the old collector\n+    \/\/ partition.\n+    \/\/ 1. Find a temporarily unusable trash region in the old collector partition\n+    ShenandoahRightLeftIterator iterator(&_partitions, ShenandoahFreeSetPartitionId::OldCollector, true);\n+    idx_t unusable_trash = -1;\n+    for (unusable_trash = iterator.current(); iterator.has_next(); unusable_trash = iterator.next()) {\n+      const ShenandoahHeapRegion* region = _heap->get_region(unusable_trash);\n+      if (region->is_trash() && _heap->is_concurrent_weak_root_in_progress()) {\n+        break;\n+      }\n+    }\n+\n+    if (unusable_trash != -1) {\n+      const size_t unusable_capacity = alloc_capacity(unusable_trash);\n+      \/\/ 2. Move the (temporarily) unusable trash region we found to the mutator partition\n+      _partitions.move_from_partition_to_partition(unusable_trash,\n+                                                   ShenandoahFreeSetPartitionId::OldCollector,\n+                                                   ShenandoahFreeSetPartitionId::Mutator, unusable_capacity);\n+\n+      \/\/ 3. Move this usable region from the mutator partition to the old collector partition\n+      _partitions.move_from_partition_to_partition(idx,\n+                                                   ShenandoahFreeSetPartitionId::Mutator,\n+                                                   ShenandoahFreeSetPartitionId::OldCollector, region_capacity);\n+\n+      _partitions.assert_bounds();\n+\n+      \/\/ 4. Do not adjust capacities for generations, we just swapped the regions that have already\n+      \/\/ been accounted for. However, we should adjust the evacuation reserves as those may have changed.\n+      shenandoah_assert_heaplocked();\n+      const size_t reserve = _heap->old_generation()->get_evacuation_reserve();\n+      _heap->old_generation()->set_evacuation_reserve(reserve - unusable_capacity + region_capacity);\n+      return true;\n@@ -366,1 +1343,0 @@\n-    SpinPause(); \/\/ allow allocators to take the lock\n@@ -368,0 +1344,3 @@\n+\n+  \/\/ We can't take this region young because it has no free unaffiliated regions (transfer failed).\n+  return false;\n@@ -373,1 +1352,1 @@\n-  assert(_mutator_free_bitmap.at(idx), \"Should be in mutator view\");\n+  assert(_partitions.partition_id_matches(idx, ShenandoahFreeSetPartitionId::Mutator), \"Should be in mutator view\");\n@@ -376,6 +1355,4 @@\n-  _mutator_free_bitmap.clear_bit(idx);\n-  _collector_free_bitmap.set_bit(idx);\n-  _collector_leftmost = MIN2(idx, _collector_leftmost);\n-  _collector_rightmost = MAX2(idx, _collector_rightmost);\n-\n-  _capacity -= alloc_capacity(r);\n+  size_t ac = alloc_capacity(r);\n+  _partitions.move_from_partition_to_partition(idx, ShenandoahFreeSetPartitionId::Mutator,\n+                                               ShenandoahFreeSetPartitionId::Collector, ac);\n+  _partitions.assert_bounds();\n@@ -383,4 +1360,2 @@\n-  if (touches_bounds(idx)) {\n-    adjust_bounds();\n-  }\n-  assert_bounds();\n+  \/\/ We do not ensure that the region is no longer trash, relying on try_allocate_in(), which always comes next,\n+  \/\/ to recycle trash before attempting to allocate anything in the region.\n@@ -395,8 +1370,6 @@\n-  _mutator_free_bitmap.clear();\n-  _collector_free_bitmap.clear();\n-  _mutator_leftmost = _max;\n-  _mutator_rightmost = 0;\n-  _collector_leftmost = _max;\n-  _collector_rightmost = 0;\n-  _capacity = 0;\n-  _used = 0;\n+  _partitions.make_all_regions_unavailable();\n+\n+  _alloc_bias_weight = 0;\n+  _partitions.set_bias_from_left_to_right(ShenandoahFreeSetPartitionId::Mutator, true);\n+  _partitions.set_bias_from_left_to_right(ShenandoahFreeSetPartitionId::Collector, false);\n+  _partitions.set_bias_from_left_to_right(ShenandoahFreeSetPartitionId::OldCollector, false);\n@@ -405,3 +1378,4 @@\n-void ShenandoahFreeSet::rebuild() {\n-  shenandoah_assert_heaplocked();\n-  clear();\n+void ShenandoahFreeSet::find_regions_with_alloc_capacity(size_t &young_cset_regions, size_t &old_cset_regions,\n+                                                         size_t &first_old_region, size_t &last_old_region,\n+                                                         size_t &old_region_count) {\n+  clear_internal();\n@@ -409,1 +1383,25 @@\n-  for (size_t idx = 0; idx < _heap->num_regions(); idx++) {\n+  first_old_region = _heap->num_regions();\n+  last_old_region = 0;\n+  old_region_count = 0;\n+  old_cset_regions = 0;\n+  young_cset_regions = 0;\n+\n+  size_t region_size_bytes = _partitions.region_size_bytes();\n+  size_t max_regions = _partitions.max_regions();\n+\n+  size_t mutator_leftmost = max_regions;\n+  size_t mutator_rightmost = 0;\n+  size_t mutator_leftmost_empty = max_regions;\n+  size_t mutator_rightmost_empty = 0;\n+  size_t mutator_regions = 0;\n+  size_t mutator_used = 0;\n+\n+  size_t old_collector_leftmost = max_regions;\n+  size_t old_collector_rightmost = 0;\n+  size_t old_collector_leftmost_empty = max_regions;\n+  size_t old_collector_rightmost_empty = 0;\n+  size_t old_collector_regions = 0;\n+  size_t old_collector_used = 0;\n+\n+  size_t num_regions = _heap->num_regions();\n+  for (size_t idx = 0; idx < num_regions; idx++) {\n@@ -411,0 +1409,17 @@\n+    if (region->is_trash()) {\n+      \/\/ Trashed regions represent regions that had been in the collection partition but have not yet been \"cleaned up\".\n+      \/\/ The cset regions are not \"trashed\" until we have finished update refs.\n+      if (region->is_old()) {\n+        old_cset_regions++;\n+      } else {\n+        assert(region->is_young(), \"Trashed region should be old or young\");\n+        young_cset_regions++;\n+      }\n+    } else if (region->is_old()) {\n+      \/\/ count both humongous and regular regions, but don't count trash (cset) regions.\n+      old_region_count++;\n+      if (first_old_region > idx) {\n+        first_old_region = idx;\n+      }\n+      last_old_region = idx;\n+    }\n@@ -412,1 +1427,1 @@\n-      assert(!region->is_cset(), \"Shouldn't be adding those to the free set\");\n+      assert(!region->is_cset(), \"Shouldn't be adding cset regions to the free set\");\n@@ -414,2 +1429,78 @@\n-      \/\/ Do not add regions that would surely fail allocation\n-      if (has_no_alloc_capacity(region)) continue;\n+      \/\/ Do not add regions that would almost surely fail allocation\n+      size_t ac = alloc_capacity(region);\n+      if (ac > PLAB::min_size() * HeapWordSize) {\n+        if (region->is_trash() || !region->is_old()) {\n+          \/\/ Both young and old collected regions (trashed) are placed into the Mutator set\n+          _partitions.raw_assign_membership(idx, ShenandoahFreeSetPartitionId::Mutator);\n+          if (idx < mutator_leftmost) {\n+            mutator_leftmost = idx;\n+          }\n+          if (idx > mutator_rightmost) {\n+            mutator_rightmost = idx;\n+          }\n+          if (ac == region_size_bytes) {\n+            if (idx < mutator_leftmost_empty) {\n+              mutator_leftmost_empty = idx;\n+            }\n+            if (idx > mutator_rightmost_empty) {\n+              mutator_rightmost_empty = idx;\n+            }\n+          }\n+          mutator_regions++;\n+          mutator_used += (region_size_bytes - ac);\n+        } else {\n+          \/\/ !region->is_trash() && region is_old()\n+          _partitions.raw_assign_membership(idx, ShenandoahFreeSetPartitionId::OldCollector);\n+          if (idx < old_collector_leftmost) {\n+            old_collector_leftmost = idx;\n+          }\n+          if (idx > old_collector_rightmost) {\n+            old_collector_rightmost = idx;\n+          }\n+          if (ac == region_size_bytes) {\n+            if (idx < old_collector_leftmost_empty) {\n+              old_collector_leftmost_empty = idx;\n+            }\n+            if (idx > old_collector_rightmost_empty) {\n+              old_collector_rightmost_empty = idx;\n+            }\n+          }\n+          old_collector_regions++;\n+          old_collector_used += (region_size_bytes - ac);\n+        }\n+      }\n+    }\n+  }\n+  log_debug(gc, free)(\"  At end of prep_to_rebuild, mutator_leftmost: \" SIZE_FORMAT\n+                      \", mutator_rightmost: \" SIZE_FORMAT\n+                      \", mutator_leftmost_empty: \" SIZE_FORMAT\n+                      \", mutator_rightmost_empty: \" SIZE_FORMAT\n+                      \", mutator_regions: \" SIZE_FORMAT\n+                      \", mutator_used: \" SIZE_FORMAT,\n+                      mutator_leftmost, mutator_rightmost, mutator_leftmost_empty, mutator_rightmost_empty,\n+                      mutator_regions, mutator_used);\n+\n+  log_debug(gc, free)(\"  old_collector_leftmost: \" SIZE_FORMAT\n+                      \", old_collector_rightmost: \" SIZE_FORMAT\n+                      \", old_collector_leftmost_empty: \" SIZE_FORMAT\n+                      \", old_collector_rightmost_empty: \" SIZE_FORMAT\n+                      \", old_collector_regions: \" SIZE_FORMAT\n+                      \", old_collector_used: \" SIZE_FORMAT,\n+                      old_collector_leftmost, old_collector_rightmost, old_collector_leftmost_empty, old_collector_rightmost_empty,\n+                      old_collector_regions, old_collector_used);\n+\n+  idx_t rightmost_idx = (mutator_leftmost == max_regions)? -1: (idx_t) mutator_rightmost;\n+  idx_t rightmost_empty_idx = (mutator_leftmost_empty == max_regions)? -1: (idx_t) mutator_rightmost_empty;\n+  _partitions.establish_mutator_intervals(mutator_leftmost, rightmost_idx, mutator_leftmost_empty, rightmost_empty_idx,\n+                                          mutator_regions, mutator_used);\n+  rightmost_idx = (old_collector_leftmost == max_regions)? -1: (idx_t) old_collector_rightmost;\n+  rightmost_empty_idx = (old_collector_leftmost_empty == max_regions)? -1: (idx_t) old_collector_rightmost_empty;\n+  _partitions.establish_old_collector_intervals(old_collector_leftmost, rightmost_idx, old_collector_leftmost_empty,\n+                                                rightmost_empty_idx, old_collector_regions, old_collector_used);\n+  log_debug(gc, free)(\"  After find_regions_with_alloc_capacity(), Mutator range [%zd, %zd],\"\n+                      \"  Old Collector range [%zd, %zd]\",\n+                      _partitions.leftmost(ShenandoahFreeSetPartitionId::Mutator),\n+                      _partitions.rightmost(ShenandoahFreeSetPartitionId::Mutator),\n+                      _partitions.leftmost(ShenandoahFreeSetPartitionId::OldCollector),\n+                      _partitions.rightmost(ShenandoahFreeSetPartitionId::OldCollector));\n+}\n@@ -417,2 +1508,18 @@\n-      _capacity += alloc_capacity(region);\n-      assert(_used <= _capacity, \"must not use more than we have\");\n+\/\/ Returns number of regions transferred, adds transferred bytes to var argument bytes_transferred\n+size_t ShenandoahFreeSet::transfer_empty_regions_from_collector_set_to_mutator_set(ShenandoahFreeSetPartitionId which_collector,\n+                                                                                   size_t max_xfer_regions,\n+                                                                                   size_t& bytes_transferred) {\n+  shenandoah_assert_heaplocked();\n+  const size_t region_size_bytes = ShenandoahHeapRegion::region_size_bytes();\n+  size_t transferred_regions = 0;\n+  ShenandoahLeftRightIterator iterator(&_partitions, which_collector, true);\n+  for (idx_t idx = iterator.current(); transferred_regions < max_xfer_regions && iterator.has_next(); idx = iterator.next()) {\n+    \/\/ Note: can_allocate_from() denotes that region is entirely empty\n+    if (can_allocate_from(idx)) {\n+      _partitions.move_from_partition_to_partition(idx, which_collector, ShenandoahFreeSetPartitionId::Mutator, region_size_bytes);\n+      transferred_regions++;\n+      bytes_transferred += region_size_bytes;\n+    }\n+  }\n+  return transferred_regions;\n+}\n@@ -420,2 +1527,13 @@\n-      assert(!is_mutator_free(idx), \"We are about to add it, it shouldn't be there already\");\n-      _mutator_free_bitmap.set_bit(idx);\n+\/\/ Returns number of regions transferred, adds transferred bytes to var argument bytes_transferred\n+size_t ShenandoahFreeSet::transfer_non_empty_regions_from_collector_set_to_mutator_set(ShenandoahFreeSetPartitionId which_collector,\n+                                                                                       size_t max_xfer_regions,\n+                                                                                       size_t& bytes_transferred) {\n+  shenandoah_assert_heaplocked();\n+  size_t transferred_regions = 0;\n+  ShenandoahLeftRightIterator iterator(&_partitions, which_collector, false);\n+  for (idx_t idx = iterator.current(); transferred_regions < max_xfer_regions && iterator.has_next(); idx = iterator.next()) {\n+    size_t ac = alloc_capacity(idx);\n+    if (ac > 0) {\n+      _partitions.move_from_partition_to_partition(idx, which_collector, ShenandoahFreeSetPartitionId::Mutator, ac);\n+      transferred_regions++;\n+      bytes_transferred += ac;\n@@ -424,0 +1542,2 @@\n+  return transferred_regions;\n+}\n@@ -425,3 +1545,13 @@\n-  \/\/ Evac reserve: reserve trailing space for evacuations\n-  size_t to_reserve = _heap->max_capacity() \/ 100 * ShenandoahEvacReserve;\n-  size_t reserved = 0;\n+void ShenandoahFreeSet::move_regions_from_collector_to_mutator(size_t max_xfer_regions) {\n+  size_t collector_xfer = 0;\n+  size_t old_collector_xfer = 0;\n+\n+  \/\/ Process empty regions within the Collector free partition\n+  if ((max_xfer_regions > 0) &&\n+      (_partitions.leftmost_empty(ShenandoahFreeSetPartitionId::Collector)\n+       <= _partitions.rightmost_empty(ShenandoahFreeSetPartitionId::Collector))) {\n+    ShenandoahHeapLocker locker(_heap->lock());\n+    max_xfer_regions -=\n+      transfer_empty_regions_from_collector_set_to_mutator_set(ShenandoahFreeSetPartitionId::Collector, max_xfer_regions,\n+                                                               collector_xfer);\n+  }\n@@ -429,2 +1559,13 @@\n-  for (size_t idx = _heap->num_regions() - 1; idx > 0; idx--) {\n-    if (reserved >= to_reserve) break;\n+  \/\/ Process empty regions within the OldCollector free partition\n+  if ((max_xfer_regions > 0) &&\n+      (_partitions.leftmost_empty(ShenandoahFreeSetPartitionId::OldCollector)\n+       <= _partitions.rightmost_empty(ShenandoahFreeSetPartitionId::OldCollector))) {\n+    ShenandoahHeapLocker locker(_heap->lock());\n+    size_t old_collector_regions =\n+      transfer_empty_regions_from_collector_set_to_mutator_set(ShenandoahFreeSetPartitionId::OldCollector, max_xfer_regions,\n+                                                               old_collector_xfer);\n+    max_xfer_regions -= old_collector_regions;\n+    if (old_collector_regions > 0) {\n+      ShenandoahGenerationalHeap::cast(_heap)->generation_sizer()->transfer_to_young(old_collector_regions);\n+    }\n+  }\n@@ -432,7 +1573,108 @@\n-    ShenandoahHeapRegion* region = _heap->get_region(idx);\n-    if (_mutator_free_bitmap.at(idx) && can_allocate_from(region)) {\n-      _mutator_free_bitmap.clear_bit(idx);\n-      _collector_free_bitmap.set_bit(idx);\n-      size_t ac = alloc_capacity(region);\n-      _capacity -= ac;\n-      reserved += ac;\n+  \/\/ If there are any non-empty regions within Collector partition, we can also move them to the Mutator free partition\n+  if ((max_xfer_regions > 0) && (_partitions.leftmost(ShenandoahFreeSetPartitionId::Collector)\n+                                 <= _partitions.rightmost(ShenandoahFreeSetPartitionId::Collector))) {\n+    ShenandoahHeapLocker locker(_heap->lock());\n+    max_xfer_regions -=\n+      transfer_non_empty_regions_from_collector_set_to_mutator_set(ShenandoahFreeSetPartitionId::Collector, max_xfer_regions,\n+                                                                   collector_xfer);\n+  }\n+\n+  size_t total_xfer = collector_xfer + old_collector_xfer;\n+  log_info(gc, ergo)(\"At start of update refs, moving \" SIZE_FORMAT \"%s to Mutator free set from Collector Reserve (\"\n+                     SIZE_FORMAT \"%s) and from Old Collector Reserve (\" SIZE_FORMAT \"%s)\",\n+                     byte_size_in_proper_unit(total_xfer), proper_unit_for_byte_size(total_xfer),\n+                     byte_size_in_proper_unit(collector_xfer), proper_unit_for_byte_size(collector_xfer),\n+                     byte_size_in_proper_unit(old_collector_xfer), proper_unit_for_byte_size(old_collector_xfer));\n+}\n+\n+\n+\/\/ Overwrite arguments to represent the amount of memory in each generation that is about to be recycled\n+void ShenandoahFreeSet::prepare_to_rebuild(size_t &young_cset_regions, size_t &old_cset_regions,\n+                                           size_t &first_old_region, size_t &last_old_region, size_t &old_region_count) {\n+  shenandoah_assert_heaplocked();\n+  \/\/ This resets all state information, removing all regions from all sets.\n+  clear();\n+  log_debug(gc, free)(\"Rebuilding FreeSet\");\n+\n+  \/\/ This places regions that have alloc_capacity into the old_collector set if they identify as is_old() or the\n+  \/\/ mutator set otherwise.  All trashed (cset) regions are affiliated young and placed in mutator set.\n+  find_regions_with_alloc_capacity(young_cset_regions, old_cset_regions, first_old_region, last_old_region, old_region_count);\n+}\n+\n+void ShenandoahFreeSet::establish_generation_sizes(size_t young_region_count, size_t old_region_count) {\n+  assert(young_region_count + old_region_count == ShenandoahHeap::heap()->num_regions(), \"Sanity\");\n+  if (ShenandoahHeap::heap()->mode()->is_generational()) {\n+    ShenandoahGenerationalHeap* heap = ShenandoahGenerationalHeap::heap();\n+    ShenandoahOldGeneration* old_gen = heap->old_generation();\n+    ShenandoahYoungGeneration* young_gen = heap->young_generation();\n+    size_t region_size_bytes = ShenandoahHeapRegion::region_size_bytes();\n+\n+    size_t original_old_capacity = old_gen->max_capacity();\n+    size_t new_old_capacity = old_region_count * region_size_bytes;\n+    size_t new_young_capacity = young_region_count * region_size_bytes;\n+    old_gen->set_capacity(new_old_capacity);\n+    young_gen->set_capacity(new_young_capacity);\n+\n+    if (new_old_capacity > original_old_capacity) {\n+      size_t region_count = (new_old_capacity - original_old_capacity) \/ region_size_bytes;\n+      log_info(gc, ergo)(\"Transfer \" SIZE_FORMAT \" region(s) from %s to %s, yielding increased size: \" PROPERFMT,\n+                         region_count, young_gen->name(), old_gen->name(), PROPERFMTARGS(new_old_capacity));\n+    } else if (new_old_capacity < original_old_capacity) {\n+      size_t region_count = (original_old_capacity - new_old_capacity) \/ region_size_bytes;\n+      log_info(gc, ergo)(\"Transfer \" SIZE_FORMAT \" region(s) from %s to %s, yielding increased size: \" PROPERFMT,\n+                         region_count, old_gen->name(), young_gen->name(), PROPERFMTARGS(new_young_capacity));\n+    }\n+    \/\/ This balances generations, so clear any pending request to balance.\n+    old_gen->set_region_balance(0);\n+  }\n+}\n+\n+void ShenandoahFreeSet::finish_rebuild(size_t young_cset_regions, size_t old_cset_regions, size_t old_region_count,\n+                                       bool have_evacuation_reserves) {\n+  shenandoah_assert_heaplocked();\n+  size_t young_reserve(0), old_reserve(0);\n+\n+  if (_heap->mode()->is_generational()) {\n+    compute_young_and_old_reserves(young_cset_regions, old_cset_regions, have_evacuation_reserves,\n+                                   young_reserve, old_reserve);\n+  } else {\n+    young_reserve = (_heap->max_capacity() \/ 100) * ShenandoahEvacReserve;\n+    old_reserve = 0;\n+  }\n+\n+  \/\/ Move some of the mutator regions in the Collector and OldCollector partitions in order to satisfy\n+  \/\/ young_reserve and old_reserve.\n+  reserve_regions(young_reserve, old_reserve, old_region_count);\n+  size_t young_region_count = _heap->num_regions() - old_region_count;\n+  establish_generation_sizes(young_region_count, old_region_count);\n+  establish_old_collector_alloc_bias();\n+  _partitions.assert_bounds();\n+  log_status();\n+}\n+\n+void ShenandoahFreeSet::compute_young_and_old_reserves(size_t young_cset_regions, size_t old_cset_regions,\n+                                                       bool have_evacuation_reserves,\n+                                                       size_t& young_reserve_result, size_t& old_reserve_result) const {\n+  shenandoah_assert_generational();\n+  const size_t region_size_bytes = ShenandoahHeapRegion::region_size_bytes();\n+\n+  ShenandoahOldGeneration* const old_generation = _heap->old_generation();\n+  size_t old_available = old_generation->available();\n+  size_t old_unaffiliated_regions = old_generation->free_unaffiliated_regions();\n+  ShenandoahYoungGeneration* const young_generation = _heap->young_generation();\n+  size_t young_capacity = young_generation->max_capacity();\n+  size_t young_unaffiliated_regions = young_generation->free_unaffiliated_regions();\n+\n+  \/\/ Add in the regions we anticipate to be freed by evacuation of the collection set\n+  old_unaffiliated_regions += old_cset_regions;\n+  young_unaffiliated_regions += young_cset_regions;\n+\n+  \/\/ Consult old-region balance to make adjustments to current generation capacities and availability.\n+  \/\/ The generation region transfers take place after we rebuild.\n+  const ssize_t old_region_balance = old_generation->get_region_balance();\n+  if (old_region_balance != 0) {\n+#ifdef ASSERT\n+    if (old_region_balance > 0) {\n+      assert(old_region_balance <= checked_cast<ssize_t>(old_unaffiliated_regions), \"Cannot transfer regions that are affiliated\");\n+    } else {\n+      assert(0 - old_region_balance <= checked_cast<ssize_t>(young_unaffiliated_regions), \"Cannot transfer regions that are affiliated\");\n@@ -440,0 +1682,29 @@\n+#endif\n+\n+    ssize_t xfer_bytes = old_region_balance * checked_cast<ssize_t>(region_size_bytes);\n+    old_available -= xfer_bytes;\n+    old_unaffiliated_regions -= old_region_balance;\n+    young_capacity += xfer_bytes;\n+    young_unaffiliated_regions += old_region_balance;\n+  }\n+\n+  \/\/ All allocations taken from the old collector set are performed by GC, generally using PLABs for both\n+  \/\/ promotions and evacuations.  The partition between which old memory is reserved for evacuation and\n+  \/\/ which is reserved for promotion is enforced using thread-local variables that prescribe intentions for\n+  \/\/ each PLAB's available memory.\n+  if (have_evacuation_reserves) {\n+    \/\/ We are rebuilding at the end of final mark, having already established evacuation budgets for this GC pass.\n+    const size_t promoted_reserve = old_generation->get_promoted_reserve();\n+    const size_t old_evac_reserve = old_generation->get_evacuation_reserve();\n+    young_reserve_result = young_generation->get_evacuation_reserve();\n+    old_reserve_result = promoted_reserve + old_evac_reserve;\n+    assert(old_reserve_result <= old_available,\n+           \"Cannot reserve (\" SIZE_FORMAT \" + \" SIZE_FORMAT\") more OLD than is available: \" SIZE_FORMAT,\n+           promoted_reserve, old_evac_reserve, old_available);\n+  } else {\n+    \/\/ We are rebuilding at end of GC, so we set aside budgets specified on command line (or defaults)\n+    young_reserve_result = (young_capacity * ShenandoahEvacReserve) \/ 100;\n+    \/\/ The auto-sizer has already made old-gen large enough to hold all anticipated evacuations and promotions.\n+    \/\/ Affiliated old-gen regions are already in the OldCollector free set.  Add in the relevant number of\n+    \/\/ unaffiliated regions.\n+    old_reserve_result = old_available;\n@@ -442,2 +1713,134 @@\n-  recompute_bounds();\n-  assert_bounds();\n+  \/\/ Old available regions that have less than PLAB::min_size() of available memory are not placed into the OldCollector\n+  \/\/ free set.  Because of this, old_available may not have enough memory to represent the intended reserve.  Adjust\n+  \/\/ the reserve downward to account for this possibility. This loss is part of the reason why the original budget\n+  \/\/ was adjusted with ShenandoahOldEvacWaste and ShenandoahOldPromoWaste multipliers.\n+  if (old_reserve_result >\n+      _partitions.capacity_of(ShenandoahFreeSetPartitionId::OldCollector) + old_unaffiliated_regions * region_size_bytes) {\n+    old_reserve_result =\n+      _partitions.capacity_of(ShenandoahFreeSetPartitionId::OldCollector) + old_unaffiliated_regions * region_size_bytes;\n+  }\n+\n+  if (young_reserve_result > young_unaffiliated_regions * region_size_bytes) {\n+    young_reserve_result = young_unaffiliated_regions * region_size_bytes;\n+  }\n+}\n+\n+\/\/ Having placed all regions that have allocation capacity into the mutator set if they identify as is_young()\n+\/\/ or into the old collector set if they identify as is_old(), move some of these regions from the mutator set\n+\/\/ into the collector set or old collector set in order to assure that the memory available for allocations within\n+\/\/ the collector set is at least to_reserve and the memory available for allocations within the old collector set\n+\/\/ is at least to_reserve_old.\n+void ShenandoahFreeSet::reserve_regions(size_t to_reserve, size_t to_reserve_old, size_t &old_region_count) {\n+  for (size_t i = _heap->num_regions(); i > 0; i--) {\n+    size_t idx = i - 1;\n+    ShenandoahHeapRegion* r = _heap->get_region(idx);\n+    if (!_partitions.in_free_set(ShenandoahFreeSetPartitionId::Mutator, idx)) {\n+      continue;\n+    }\n+\n+    size_t ac = alloc_capacity(r);\n+    assert (ac > 0, \"Membership in free set implies has capacity\");\n+    assert (!r->is_old() || r->is_trash(), \"Except for trash, mutator_is_free regions should not be affiliated OLD\");\n+\n+    bool move_to_old_collector = _partitions.available_in(ShenandoahFreeSetPartitionId::OldCollector) < to_reserve_old;\n+    bool move_to_collector = _partitions.available_in(ShenandoahFreeSetPartitionId::Collector) < to_reserve;\n+\n+    if (!move_to_collector && !move_to_old_collector) {\n+      \/\/ We've satisfied both to_reserve and to_reserved_old\n+      break;\n+    }\n+\n+    if (move_to_old_collector) {\n+      \/\/ We give priority to OldCollector partition because we desire to pack OldCollector regions into higher\n+      \/\/ addresses than Collector regions.  Presumably, OldCollector regions are more \"stable\" and less likely to\n+      \/\/ be collected in the near future.\n+      if (r->is_trash() || !r->is_affiliated()) {\n+        \/\/ OLD regions that have available memory are already in the old_collector free set.\n+        _partitions.move_from_partition_to_partition(idx, ShenandoahFreeSetPartitionId::Mutator,\n+                                                     ShenandoahFreeSetPartitionId::OldCollector, ac);\n+        log_trace(gc, free)(\"  Shifting region \" SIZE_FORMAT \" from mutator_free to old_collector_free\", idx);\n+        log_trace(gc, free)(\"  Shifted Mutator range [%zd, %zd],\"\n+                            \"  Old Collector range [%zd, %zd]\",\n+                            _partitions.leftmost(ShenandoahFreeSetPartitionId::Mutator),\n+                            _partitions.rightmost(ShenandoahFreeSetPartitionId::Mutator),\n+                            _partitions.leftmost(ShenandoahFreeSetPartitionId::OldCollector),\n+                            _partitions.rightmost(ShenandoahFreeSetPartitionId::OldCollector));\n+        old_region_count++;\n+        continue;\n+      }\n+    }\n+\n+    if (move_to_collector) {\n+      \/\/ Note: In a previous implementation, regions were only placed into the survivor space (collector_is_free) if\n+      \/\/ they were entirely empty.  This has the effect of causing new Mutator allocation to reside next to objects\n+      \/\/ that have already survived at least one GC, mixing ephemeral with longer-lived objects in the same region.\n+      \/\/ Any objects that have survived a GC are less likely to immediately become garbage, so a region that contains\n+      \/\/ survivor objects is less likely to be selected for the collection set.  This alternative implementation allows\n+      \/\/ survivor regions to continue accumulating other survivor objects, and makes it more likely that ephemeral objects\n+      \/\/ occupy regions comprised entirely of ephemeral objects.  These regions are highly likely to be included in the next\n+      \/\/ collection set, and they are easily evacuated because they have low density of live objects.\n+      _partitions.move_from_partition_to_partition(idx, ShenandoahFreeSetPartitionId::Mutator,\n+                                                   ShenandoahFreeSetPartitionId::Collector, ac);\n+      log_trace(gc, free)(\"  Shifting region \" SIZE_FORMAT \" from mutator_free to collector_free\", idx);\n+      log_trace(gc, free)(\"  Shifted Mutator range [%zd, %zd],\"\n+                          \"  Collector range [%zd, %zd]\",\n+                          _partitions.leftmost(ShenandoahFreeSetPartitionId::Mutator),\n+                          _partitions.rightmost(ShenandoahFreeSetPartitionId::Mutator),\n+                          _partitions.leftmost(ShenandoahFreeSetPartitionId::Collector),\n+                          _partitions.rightmost(ShenandoahFreeSetPartitionId::Collector));\n+    }\n+  }\n+\n+  if (LogTarget(Info, gc, free)::is_enabled()) {\n+    size_t old_reserve = _partitions.available_in(ShenandoahFreeSetPartitionId::OldCollector);\n+    if (old_reserve < to_reserve_old) {\n+      log_info(gc, free)(\"Wanted \" PROPERFMT \" for old reserve, but only reserved: \" PROPERFMT,\n+                         PROPERFMTARGS(to_reserve_old), PROPERFMTARGS(old_reserve));\n+    }\n+    size_t reserve = _partitions.available_in(ShenandoahFreeSetPartitionId::Collector);\n+    if (reserve < to_reserve) {\n+      log_info(gc, free)(\"Wanted \" PROPERFMT \" for young reserve, but only reserved: \" PROPERFMT,\n+                          PROPERFMTARGS(to_reserve), PROPERFMTARGS(reserve));\n+    }\n+  }\n+}\n+\n+void ShenandoahFreeSet::establish_old_collector_alloc_bias() {\n+  ShenandoahHeap* heap = ShenandoahHeap::heap();\n+  shenandoah_assert_heaplocked();\n+\n+  idx_t left_idx = _partitions.leftmost(ShenandoahFreeSetPartitionId::OldCollector);\n+  idx_t right_idx = _partitions.rightmost(ShenandoahFreeSetPartitionId::OldCollector);\n+  idx_t middle = (left_idx + right_idx) \/ 2;\n+  size_t available_in_first_half = 0;\n+  size_t available_in_second_half = 0;\n+\n+  for (idx_t index = left_idx; index < middle; index++) {\n+    if (_partitions.in_free_set(ShenandoahFreeSetPartitionId::OldCollector, index)) {\n+      ShenandoahHeapRegion* r = heap->get_region((size_t) index);\n+      available_in_first_half += r->free();\n+    }\n+  }\n+  for (idx_t index = middle; index <= right_idx; index++) {\n+    if (_partitions.in_free_set(ShenandoahFreeSetPartitionId::OldCollector, index)) {\n+      ShenandoahHeapRegion* r = heap->get_region(index);\n+      available_in_second_half += r->free();\n+    }\n+  }\n+\n+  \/\/ We desire to first consume the sparsely distributed regions in order that the remaining regions are densely packed.\n+  \/\/ Densely packing regions reduces the effort to search for a region that has sufficient memory to satisfy a new allocation\n+  \/\/ request.  Regions become sparsely distributed following a Full GC, which tends to slide all regions to the front of the\n+  \/\/ heap rather than allowing survivor regions to remain at the high end of the heap where we intend for them to congregate.\n+  _partitions.set_bias_from_left_to_right(ShenandoahFreeSetPartitionId::OldCollector,\n+                                          (available_in_second_half > available_in_first_half));\n+}\n+\n+void ShenandoahFreeSet::log_status_under_lock() {\n+  \/\/ Must not be heap locked, it acquires heap lock only when log is enabled\n+  shenandoah_assert_not_heaplocked();\n+  if (LogTarget(Info, gc, free)::is_enabled()\n+      DEBUG_ONLY(|| LogTarget(Debug, gc, free)::is_enabled())) {\n+    ShenandoahHeapLocker locker(_heap->lock());\n+    log_status();\n+  }\n@@ -449,1 +1852,68 @@\n-  LogTarget(Info, gc, ergo) lt;\n+#ifdef ASSERT\n+  \/\/ Dump of the FreeSet details is only enabled if assertions are enabled\n+  LogTarget(Debug, gc, free) debug_free;\n+  if (debug_free.is_enabled()) {\n+#define BUFFER_SIZE 80\n+    LogStream ls(debug_free);\n+\n+    char buffer[BUFFER_SIZE];\n+    for (uint i = 0; i < BUFFER_SIZE; i++) {\n+      buffer[i] = '\\0';\n+    }\n+\n+    ls.cr();\n+    ls.print_cr(\"Mutator free range [%zd..%zd] allocating from %s\",\n+                _partitions.leftmost(ShenandoahFreeSetPartitionId::Mutator),\n+                _partitions.rightmost(ShenandoahFreeSetPartitionId::Mutator),\n+                _partitions.alloc_from_left_bias(ShenandoahFreeSetPartitionId::Mutator)? \"left to right\": \"right to left\");\n+\n+    ls.print_cr(\"Collector free range [%zd..%zd] allocating from %s\",\n+                _partitions.leftmost(ShenandoahFreeSetPartitionId::Collector),\n+                _partitions.rightmost(ShenandoahFreeSetPartitionId::Collector),\n+                _partitions.alloc_from_left_bias(ShenandoahFreeSetPartitionId::Collector)? \"left to right\": \"right to left\");\n+\n+    ls.print_cr(\"Old collector free range [%zd..%zd] allocates from %s\",\n+                _partitions.leftmost(ShenandoahFreeSetPartitionId::OldCollector),\n+                _partitions.rightmost(ShenandoahFreeSetPartitionId::OldCollector),\n+                _partitions.alloc_from_left_bias(ShenandoahFreeSetPartitionId::OldCollector)? \"left to right\": \"right to left\");\n+    ls.cr();\n+    ls.print_cr(\"FreeSet map legend:\");\n+    ls.print_cr(\" M\/m:mutator, C\/c:collector O\/o:old_collector (Empty\/Occupied)\");\n+    ls.print_cr(\" H\/h:humongous, X\/x:no alloc capacity, ~\/_:retired (Old\/Young)\");\n+\n+    for (uint i = 0; i < _heap->num_regions(); i++) {\n+      ShenandoahHeapRegion *r = _heap->get_region(i);\n+      uint idx = i % 64;\n+      if ((i != 0) && (idx == 0)) {\n+        ls.print_cr(\" %6u: %s\", i-64, buffer);\n+      }\n+      if (_partitions.in_free_set(ShenandoahFreeSetPartitionId::Mutator, i)) {\n+        size_t capacity = alloc_capacity(r);\n+        assert(!r->is_old() || r->is_trash(), \"Old regions except trash regions should not be in mutator_free set\");\n+        buffer[idx] = (capacity == ShenandoahHeapRegion::region_size_bytes()) ? 'M' : 'm';\n+      } else if (_partitions.in_free_set(ShenandoahFreeSetPartitionId::Collector, i)) {\n+        size_t capacity = alloc_capacity(r);\n+        assert(!r->is_old() || r->is_trash(), \"Old regions except trash regions should not be in collector_free set\");\n+        buffer[idx] = (capacity == ShenandoahHeapRegion::region_size_bytes()) ? 'C' : 'c';\n+      } else if (_partitions.in_free_set(ShenandoahFreeSetPartitionId::OldCollector, i)) {\n+        size_t capacity = alloc_capacity(r);\n+        buffer[idx] = (capacity == ShenandoahHeapRegion::region_size_bytes()) ? 'O' : 'o';\n+      } else if (r->is_humongous()) {\n+        buffer[idx] = (r->is_old() ? 'H' : 'h');\n+      } else if (alloc_capacity(r) == 0) {\n+        buffer[idx] = (r->is_old() ? 'X' : 'x');\n+      } else {\n+        buffer[idx] = (r->is_old() ? '~' : '_');\n+      }\n+    }\n+    uint remnant = _heap->num_regions() % 64;\n+    if (remnant > 0) {\n+      buffer[remnant] = '\\0';\n+    } else {\n+      remnant = 64;\n+    }\n+    ls.print_cr(\" %6u: %s\", (uint) (_heap->num_regions() - remnant), buffer);\n+  }\n+#endif\n+\n+  LogTarget(Info, gc, free) lt;\n@@ -455,1 +1925,1 @@\n-      size_t last_idx = 0;\n+      idx_t last_idx = 0;\n@@ -464,2 +1934,3 @@\n-      for (size_t idx = _mutator_leftmost; idx <= _mutator_rightmost; idx++) {\n-        if (is_mutator_free(idx)) {\n+      for (idx_t idx = _partitions.leftmost(ShenandoahFreeSetPartitionId::Mutator);\n+           idx <= _partitions.rightmost(ShenandoahFreeSetPartitionId::Mutator); idx++) {\n+        if (_partitions.in_free_set(ShenandoahFreeSetPartitionId::Mutator, idx)) {\n@@ -468,1 +1939,0 @@\n-\n@@ -470,1 +1940,0 @@\n-\n@@ -481,1 +1950,0 @@\n-\n@@ -484,1 +1952,0 @@\n-\n@@ -493,0 +1960,4 @@\n+      \/\/ Since certain regions that belonged to the Mutator free partition at the time of most recent rebuild may have been\n+      \/\/ retired, the sum of used and capacities within regions that are still in the Mutator free partition may not match\n+      \/\/ my internally tracked values of used() and free().\n+      assert(free == total_free, \"Free memory should match\");\n@@ -509,2 +1980,3 @@\n-      if (mutator_count() > 0) {\n-        frag_int = (100 * (total_used \/ mutator_count()) \/ ShenandoahHeapRegion::region_size_bytes());\n+      if (_partitions.count(ShenandoahFreeSetPartitionId::Mutator) > 0) {\n+        frag_int = (100 * (total_used \/ _partitions.count(ShenandoahFreeSetPartitionId::Mutator))\n+                    \/ ShenandoahHeapRegion::region_size_bytes());\n@@ -515,0 +1987,3 @@\n+      ls.print(\"Used: \" SIZE_FORMAT \"%s, Mutator Free: \" SIZE_FORMAT,\n+               byte_size_in_proper_unit(total_used), proper_unit_for_byte_size(total_used),\n+               _partitions.count(ShenandoahFreeSetPartitionId::Mutator));\n@@ -520,0 +1995,1 @@\n+      size_t total_used = 0;\n@@ -521,2 +1997,3 @@\n-      for (size_t idx = _collector_leftmost; idx <= _collector_rightmost; idx++) {\n-        if (is_collector_free(idx)) {\n+      for (idx_t idx = _partitions.leftmost(ShenandoahFreeSetPartitionId::Collector);\n+           idx <= _partitions.rightmost(ShenandoahFreeSetPartitionId::Collector); idx++) {\n+        if (_partitions.in_free_set(ShenandoahFreeSetPartitionId::Collector, idx)) {\n@@ -527,0 +2004,1 @@\n+          total_used += r->used();\n@@ -529,0 +2007,5 @@\n+      ls.print(\" Collector Reserve: \" SIZE_FORMAT \"%s, Max: \" SIZE_FORMAT \"%s; Used: \" SIZE_FORMAT \"%s\",\n+               byte_size_in_proper_unit(total_free), proper_unit_for_byte_size(total_free),\n+               byte_size_in_proper_unit(max),        proper_unit_for_byte_size(max),\n+               byte_size_in_proper_unit(total_used), proper_unit_for_byte_size(total_used));\n+    }\n@@ -530,1 +2013,16 @@\n-      ls.print_cr(\"Reserve: \" SIZE_FORMAT \"%s, Max: \" SIZE_FORMAT \"%s\",\n+    if (_heap->mode()->is_generational()) {\n+      size_t max = 0;\n+      size_t total_free = 0;\n+      size_t total_used = 0;\n+\n+      for (idx_t idx = _partitions.leftmost(ShenandoahFreeSetPartitionId::OldCollector);\n+           idx <= _partitions.rightmost(ShenandoahFreeSetPartitionId::OldCollector); idx++) {\n+        if (_partitions.in_free_set(ShenandoahFreeSetPartitionId::OldCollector, idx)) {\n+          ShenandoahHeapRegion *r = _heap->get_region(idx);\n+          size_t free = alloc_capacity(r);\n+          max = MAX2(max, free);\n+          total_free += free;\n+          total_used += r->used();\n+        }\n+      }\n+      ls.print_cr(\" Old Collector Reserve: \" SIZE_FORMAT \"%s, Max: \" SIZE_FORMAT \"%s; Used: \" SIZE_FORMAT \"%s\",\n@@ -532,1 +2030,2 @@\n-                  byte_size_in_proper_unit(max),        proper_unit_for_byte_size(max));\n+                  byte_size_in_proper_unit(max),        proper_unit_for_byte_size(max),\n+                  byte_size_in_proper_unit(total_used), proper_unit_for_byte_size(total_used));\n@@ -539,3 +2038,1 @@\n-  assert_bounds();\n-\n-  if (req.size() > ShenandoahHeapRegion::humongous_threshold_words()) {\n+  if (ShenandoahHeapRegion::requires_humongous(req.size())) {\n@@ -547,0 +2044,1 @@\n+      case ShenandoahAllocRequest::_alloc_plab:\n@@ -550,2 +2048,1 @@\n-        assert(false, \"Trying to allocate TLAB larger than the humongous threshold: \" SIZE_FORMAT \" > \" SIZE_FORMAT,\n-               req.size(), ShenandoahHeapRegion::humongous_threshold_words());\n+        assert(false, \"Trying to allocate TLAB in humongous region: \" SIZE_FORMAT, req.size());\n@@ -562,10 +2059,5 @@\n-size_t ShenandoahFreeSet::unsafe_peek_free() const {\n-  \/\/ Deliberately not locked, this method is unsafe when free set is modified.\n-\n-  for (size_t index = _mutator_leftmost; index <= _mutator_rightmost; index++) {\n-    if (index < _max && is_mutator_free(index)) {\n-      ShenandoahHeapRegion* r = _heap->get_region(index);\n-      if (r->free() >= MinTLABSize) {\n-        return r->free();\n-      }\n-    }\n+void ShenandoahFreeSet::print_on(outputStream* out) const {\n+  out->print_cr(\"Mutator Free Set: \" SIZE_FORMAT \"\", _partitions.count(ShenandoahFreeSetPartitionId::Mutator));\n+  ShenandoahLeftRightIterator mutator(const_cast<ShenandoahRegionPartitions*>(&_partitions), ShenandoahFreeSetPartitionId::Mutator);\n+  for (idx_t index = mutator.current(); mutator.has_next(); index = mutator.next()) {\n+    _heap->get_region(index)->print_on(out);\n@@ -574,10 +2066,4 @@\n-  \/\/ It appears that no regions left\n-  return 0;\n-}\n-\n-void ShenandoahFreeSet::print_on(outputStream* out) const {\n-  out->print_cr(\"Mutator Free Set: \" SIZE_FORMAT \"\", mutator_count());\n-  for (size_t index = _mutator_leftmost; index <= _mutator_rightmost; index++) {\n-    if (is_mutator_free(index)) {\n-      _heap->get_region(index)->print_on(out);\n-    }\n+  out->print_cr(\"Collector Free Set: \" SIZE_FORMAT \"\", _partitions.count(ShenandoahFreeSetPartitionId::Collector));\n+  ShenandoahLeftRightIterator collector(const_cast<ShenandoahRegionPartitions*>(&_partitions), ShenandoahFreeSetPartitionId::Collector);\n+  for (idx_t index = collector.current(); collector.has_next(); index = collector.next()) {\n+    _heap->get_region(index)->print_on(out);\n@@ -585,4 +2071,8 @@\n-  out->print_cr(\"Collector Free Set: \" SIZE_FORMAT \"\", collector_count());\n-  for (size_t index = _collector_leftmost; index <= _collector_rightmost; index++) {\n-    if (is_collector_free(index)) {\n-      _heap->get_region(index)->print_on(out);\n+\n+  if (_heap->mode()->is_generational()) {\n+    out->print_cr(\"Old Collector Free Set: \" SIZE_FORMAT \"\", _partitions.count(ShenandoahFreeSetPartitionId::OldCollector));\n+    for (idx_t index = _partitions.leftmost(ShenandoahFreeSetPartitionId::OldCollector);\n+         index <= _partitions.rightmost(ShenandoahFreeSetPartitionId::OldCollector); index++) {\n+      if (_partitions.in_free_set(ShenandoahFreeSetPartitionId::OldCollector, index)) {\n+        _heap->get_region(index)->print_on(out);\n+      }\n@@ -593,21 +2083,0 @@\n-\/*\n- * Internal fragmentation metric: describes how fragmented the heap regions are.\n- *\n- * It is derived as:\n- *\n- *               sum(used[i]^2, i=0..k)\n- *   IF = 1 - ------------------------------\n- *              C * sum(used[i], i=0..k)\n- *\n- * ...where k is the number of regions in computation, C is the region capacity, and\n- * used[i] is the used space in the region.\n- *\n- * The non-linearity causes IF to be lower for the cases where the same total heap\n- * used is densely packed. For example:\n- *   a) Heap is completely full  => IF = 0\n- *   b) Heap is half full, first 50% regions are completely full => IF = 0\n- *   c) Heap is half full, each region is 50% full => IF = 1\/2\n- *   d) Heap is quarter full, first 50% regions are completely full => IF = 0\n- *   e) Heap is quarter full, each region is 25% full => IF = 3\/4\n- *   f) Heap has one small object per each region => IF =~ 1\n- *\/\n@@ -618,7 +2087,6 @@\n-  for (size_t index = _mutator_leftmost; index <= _mutator_rightmost; index++) {\n-    if (is_mutator_free(index)) {\n-      ShenandoahHeapRegion* r = _heap->get_region(index);\n-      size_t used = r->used();\n-      squared += used * used;\n-      linear += used;\n-    }\n+  ShenandoahLeftRightIterator iterator(&_partitions, ShenandoahFreeSetPartitionId::Mutator);\n+  for (idx_t index = iterator.current(); iterator.has_next(); index = iterator.next()) {\n+    ShenandoahHeapRegion* r = _heap->get_region(index);\n+    size_t used = r->used();\n+    squared += used * used;\n+    linear += used;\n@@ -635,13 +2103,0 @@\n-\/*\n- * External fragmentation metric: describes how fragmented the heap is.\n- *\n- * It is derived as:\n- *\n- *   EF = 1 - largest_contiguous_free \/ total_free\n- *\n- * For example:\n- *   a) Heap is completely empty => EF = 0\n- *   b) Heap is completely full => EF = 0\n- *   c) Heap is first-half full => EF = 1\/2\n- *   d) Heap is half full, full and empty regions interleave => EF =~ 1\n- *\/\n@@ -649,1 +2104,1 @@\n-  size_t last_idx = 0;\n+  idx_t last_idx = 0;\n@@ -652,1 +2107,0 @@\n-\n@@ -655,10 +2109,7 @@\n-  for (size_t index = _mutator_leftmost; index <= _mutator_rightmost; index++) {\n-    if (is_mutator_free(index)) {\n-      ShenandoahHeapRegion* r = _heap->get_region(index);\n-      if (r->is_empty()) {\n-        free += ShenandoahHeapRegion::region_size_bytes();\n-        if (last_idx + 1 == index) {\n-          empty_contig++;\n-        } else {\n-          empty_contig = 1;\n-        }\n+  ShenandoahLeftRightIterator iterator(&_partitions, ShenandoahFreeSetPartitionId::Mutator);\n+  for (idx_t index = iterator.current(); iterator.has_next(); index = iterator.next()) {\n+    ShenandoahHeapRegion* r = _heap->get_region(index);\n+    if (r->is_empty()) {\n+      free += ShenandoahHeapRegion::region_size_bytes();\n+      if (last_idx + 1 == index) {\n+        empty_contig++;\n@@ -666,1 +2117,1 @@\n-        empty_contig = 0;\n+        empty_contig = 1;\n@@ -668,3 +2119,2 @@\n-\n-      max_contig = MAX2(max_contig, empty_contig);\n-      last_idx = index;\n+    } else {\n+      empty_contig = 0;\n@@ -672,0 +2122,2 @@\n+    max_contig = MAX2(max_contig, empty_contig);\n+    last_idx = index;\n@@ -681,27 +2133,0 @@\n-#ifdef ASSERT\n-void ShenandoahFreeSet::assert_bounds() const {\n-  \/\/ Performance invariants. Failing these would not break the free set, but performance\n-  \/\/ would suffer.\n-  assert (_mutator_leftmost <= _max, \"leftmost in bounds: \"  SIZE_FORMAT \" < \" SIZE_FORMAT, _mutator_leftmost,  _max);\n-  assert (_mutator_rightmost < _max, \"rightmost in bounds: \" SIZE_FORMAT \" < \" SIZE_FORMAT, _mutator_rightmost, _max);\n-\n-  assert (_mutator_leftmost == _max || is_mutator_free(_mutator_leftmost),  \"leftmost region should be free: \" SIZE_FORMAT,  _mutator_leftmost);\n-  assert (_mutator_rightmost == 0   || is_mutator_free(_mutator_rightmost), \"rightmost region should be free: \" SIZE_FORMAT, _mutator_rightmost);\n-\n-  size_t beg_off = _mutator_free_bitmap.find_first_set_bit(0);\n-  size_t end_off = _mutator_free_bitmap.find_first_set_bit(_mutator_rightmost + 1);\n-  assert (beg_off >= _mutator_leftmost, \"free regions before the leftmost: \" SIZE_FORMAT \", bound \" SIZE_FORMAT, beg_off, _mutator_leftmost);\n-  assert (end_off == _max,      \"free regions past the rightmost: \" SIZE_FORMAT \", bound \" SIZE_FORMAT,  end_off, _mutator_rightmost);\n-\n-  assert (_collector_leftmost <= _max, \"leftmost in bounds: \"  SIZE_FORMAT \" < \" SIZE_FORMAT, _collector_leftmost,  _max);\n-  assert (_collector_rightmost < _max, \"rightmost in bounds: \" SIZE_FORMAT \" < \" SIZE_FORMAT, _collector_rightmost, _max);\n-\n-  assert (_collector_leftmost == _max || is_collector_free(_collector_leftmost),  \"leftmost region should be free: \" SIZE_FORMAT,  _collector_leftmost);\n-  assert (_collector_rightmost == 0   || is_collector_free(_collector_rightmost), \"rightmost region should be free: \" SIZE_FORMAT, _collector_rightmost);\n-\n-  beg_off = _collector_free_bitmap.find_first_set_bit(0);\n-  end_off = _collector_free_bitmap.find_first_set_bit(_collector_rightmost + 1);\n-  assert (beg_off >= _collector_leftmost, \"free regions before the leftmost: \" SIZE_FORMAT \", bound \" SIZE_FORMAT, beg_off, _collector_leftmost);\n-  assert (end_off == _max,      \"free regions past the rightmost: \" SIZE_FORMAT \", bound \" SIZE_FORMAT,  end_off, _collector_rightmost);\n-}\n-#endif\n","filename":"src\/hotspot\/share\/gc\/shenandoah\/shenandoahFreeSet.cpp","additions":1800,"deletions":375,"binary":false,"changes":2175,"status":"modified"},{"patch":"@@ -3,0 +3,1 @@\n+ * Copyright Amazon.com Inc. or its affiliates. All Rights Reserved.\n@@ -30,0 +31,253 @@\n+#include \"gc\/shenandoah\/shenandoahSimpleBitMap.hpp\"\n+\n+\/\/ Each ShenandoahHeapRegion is associated with a ShenandoahFreeSetPartitionId.\n+enum class ShenandoahFreeSetPartitionId : uint8_t {\n+  Mutator,                      \/\/ Region is in the Mutator free set: available memory is available to mutators.\n+  Collector,                    \/\/ Region is in the Collector free set: available memory is reserved for evacuations.\n+  OldCollector,                 \/\/ Region is in the Old Collector free set:\n+                                \/\/    available memory is reserved for old evacuations and for promotions..\n+  NotFree                       \/\/ Region is in no free set: it has no available memory\n+};\n+\n+\/\/ We do not maintain counts, capacity, or used for regions that are not free.  Informally, if a region is NotFree, it is\n+\/\/ in no partition.  NumPartitions represents the size of an array that may be indexed by Mutator or Collector.\n+#define NumPartitions           (ShenandoahFreeSetPartitionId::NotFree)\n+#define IntNumPartitions     int(ShenandoahFreeSetPartitionId::NotFree)\n+#define UIntNumPartitions   uint(ShenandoahFreeSetPartitionId::NotFree)\n+\n+\/\/ ShenandoahRegionPartitions provides an abstraction to help organize the implementation of ShenandoahFreeSet.  This\n+\/\/ class implements partitioning of regions into distinct sets.  Each ShenandoahHeapRegion is either in the Mutator free set,\n+\/\/ the Collector free set, or in neither free set (NotFree).  When we speak of a \"free partition\", we mean partitions that\n+\/\/ for which the ShenandoahFreeSetPartitionId is not equal to NotFree.\n+class ShenandoahRegionPartitions {\n+\n+private:\n+  const ssize_t _max;           \/\/ The maximum number of heap regions\n+  const size_t _region_size_bytes;\n+  const ShenandoahFreeSet* _free_set;\n+  \/\/ For each partition, we maintain a bitmap of which regions are affiliated with his partition.\n+  ShenandoahSimpleBitMap _membership[UIntNumPartitions];\n+\n+  \/\/ For each partition, we track an interval outside of which a region affiliated with that partition is guaranteed\n+  \/\/ not to be found. This makes searches for free space more efficient.  For each partition p, _leftmosts[p]\n+  \/\/ represents its least index, and its _rightmosts[p] its greatest index. Empty intervals are indicated by the\n+  \/\/ canonical [_max, -1].\n+  ssize_t _leftmosts[UIntNumPartitions];\n+  ssize_t _rightmosts[UIntNumPartitions];\n+\n+  \/\/ Allocation for humongous objects needs to find regions that are entirely empty.  For each partion p, _leftmosts_empty[p]\n+  \/\/ represents the first region belonging to this partition that is completely empty and _rightmosts_empty[p] represents the\n+  \/\/ last region that is completely empty.  If there is no completely empty region in this partition, this is represented\n+  \/\/ by the canonical [_max, -1].\n+  ssize_t _leftmosts_empty[UIntNumPartitions];\n+  ssize_t _rightmosts_empty[UIntNumPartitions];\n+\n+  \/\/ For each partition p, _capacity[p] represents the total amount of memory within the partition at the time\n+  \/\/ of the most recent rebuild, _used[p] represents the total amount of memory that has been allocated within this\n+  \/\/ partition (either already allocated as of the rebuild, or allocated since the rebuild).  _capacity[p] and _used[p]\n+  \/\/ are denoted in bytes.  Note that some regions that had been assigned to a particular partition at rebuild time\n+  \/\/ may have been retired following the rebuild.  The tallies for these regions are still reflected in _capacity[p]\n+  \/\/ and _used[p], even though the region may have been removed from the free set.\n+  size_t _capacity[UIntNumPartitions];\n+  size_t _used[UIntNumPartitions];\n+  size_t _region_counts[UIntNumPartitions];\n+\n+  \/\/ For each partition p, _left_to_right_bias is true iff allocations are normally made from lower indexed regions\n+  \/\/ before higher indexed regions.\n+  bool _left_to_right_bias[UIntNumPartitions];\n+\n+  \/\/ Shrink the intervals associated with partition when region idx is removed from this free set\n+  inline void shrink_interval_if_boundary_modified(ShenandoahFreeSetPartitionId partition, ssize_t idx);\n+\n+  \/\/ Shrink the intervals associated with partition when regions low_idx through high_idx inclusive are removed from this free set\n+  inline void shrink_interval_if_range_modifies_either_boundary(ShenandoahFreeSetPartitionId partition,\n+                                                                ssize_t low_idx, ssize_t high_idx);\n+  inline void expand_interval_if_boundary_modified(ShenandoahFreeSetPartitionId partition, ssize_t idx, size_t capacity);\n+\n+  inline bool is_mutator_partition(ShenandoahFreeSetPartitionId p);\n+  inline bool is_young_collector_partition(ShenandoahFreeSetPartitionId p);\n+  inline bool is_old_collector_partition(ShenandoahFreeSetPartitionId p);\n+  inline bool available_implies_empty(size_t available);\n+\n+#ifndef PRODUCT\n+  void dump_bitmap_row(ssize_t region_idx) const;\n+  void dump_bitmap_range(ssize_t start_region_idx, ssize_t end_region_idx) const;\n+  void dump_bitmap() const;\n+#endif\n+public:\n+  ShenandoahRegionPartitions(size_t max_regions, ShenandoahFreeSet* free_set);\n+  ~ShenandoahRegionPartitions() {}\n+\n+  \/\/ Remove all regions from all partitions and reset all bounds\n+  void make_all_regions_unavailable();\n+\n+  \/\/ Set the partition id for a particular region without adjusting interval bounds or usage\/capacity tallies\n+  inline void raw_assign_membership(size_t idx, ShenandoahFreeSetPartitionId p) {\n+    _membership[int(p)].set_bit(idx);\n+  }\n+\n+  \/\/ Set the Mutator intervals, usage, and capacity according to arguments.  Reset the Collector intervals, used, capacity\n+  \/\/ to represent empty Collector free set.  We use this at the end of rebuild_free_set() to avoid the overhead of making\n+  \/\/ many redundant incremental adjustments to the mutator intervals as the free set is being rebuilt.\n+  void establish_mutator_intervals(ssize_t mutator_leftmost, ssize_t mutator_rightmost,\n+                                   ssize_t mutator_leftmost_empty, ssize_t mutator_rightmost_empty,\n+                                   size_t mutator_region_count, size_t mutator_used);\n+\n+  \/\/ Set the OldCollector intervals, usage, and capacity according to arguments.  We use this at the end of rebuild_free_set()\n+  \/\/ to avoid the overhead of making many redundant incremental adjustments to the mutator intervals as the free set is being\n+  \/\/ rebuilt.\n+  void establish_old_collector_intervals(ssize_t old_collector_leftmost, ssize_t old_collector_rightmost,\n+                                         ssize_t old_collector_leftmost_empty, ssize_t old_collector_rightmost_empty,\n+                                         size_t old_collector_region_count, size_t old_collector_used);\n+\n+  \/\/ Retire region idx from within partition, , leaving its capacity and used as part of the original free partition's totals.\n+  \/\/ Requires that region idx is in in the Mutator or Collector partitions.  Hereafter, identifies this region as NotFree.\n+  \/\/ Any remnant of available memory at the time of retirement is added to the original partition's total of used bytes.\n+  void retire_from_partition(ShenandoahFreeSetPartitionId p, ssize_t idx, size_t used_bytes);\n+\n+  \/\/ Retire all regions between low_idx and high_idx inclusive from within partition.  Requires that each region idx is\n+  \/\/ in the same Mutator or Collector partition.  Hereafter, identifies each region as NotFree.   Assumes that each region\n+  \/\/ is now considered fully used, since the region is presumably used to represent a humongous object.\n+  void retire_range_from_partition(ShenandoahFreeSetPartitionId partition, ssize_t low_idx, ssize_t high_idx);\n+\n+  \/\/ Place region idx into free set which_partition.  Requires that idx is currently NotFree.\n+  void make_free(ssize_t idx, ShenandoahFreeSetPartitionId which_partition, size_t region_capacity);\n+\n+  \/\/ Place region idx into free partition new_partition, adjusting used and capacity totals for the original and new partition\n+  \/\/ given that available bytes can still be allocated within this region.  Requires that idx is currently not NotFree.\n+  void move_from_partition_to_partition(ssize_t idx, ShenandoahFreeSetPartitionId orig_partition,\n+                                        ShenandoahFreeSetPartitionId new_partition, size_t available);\n+\n+  const char* partition_membership_name(ssize_t idx) const;\n+\n+  \/\/ Return the index of the next available region >= start_index, or maximum_regions if not found.\n+  inline ssize_t find_index_of_next_available_region(ShenandoahFreeSetPartitionId which_partition, ssize_t start_index) const;\n+\n+  \/\/ Return the index of the previous available region <= last_index, or -1 if not found.\n+  inline ssize_t find_index_of_previous_available_region(ShenandoahFreeSetPartitionId which_partition, ssize_t last_index) const;\n+\n+  \/\/ Return the index of the next available cluster of cluster_size regions >= start_index, or maximum_regions if not found.\n+  inline ssize_t find_index_of_next_available_cluster_of_regions(ShenandoahFreeSetPartitionId which_partition,\n+                                                                 ssize_t start_index, size_t cluster_size) const;\n+\n+  \/\/ Return the index of the previous available cluster of cluster_size regions <= last_index, or -1 if not found.\n+  inline ssize_t find_index_of_previous_available_cluster_of_regions(ShenandoahFreeSetPartitionId which_partition,\n+                                                                     ssize_t last_index, size_t cluster_size) const;\n+\n+  inline bool in_free_set(ShenandoahFreeSetPartitionId which_partition, ssize_t idx) const {\n+    return _membership[int(which_partition)].is_set(idx);\n+  }\n+\n+  \/\/ Returns the ShenandoahFreeSetPartitionId affiliation of region idx, NotFree if this region is not currently in any partition.\n+  \/\/ This does not enforce that free_set membership implies allocation capacity.\n+  inline ShenandoahFreeSetPartitionId membership(ssize_t idx) const;\n+\n+#ifdef ASSERT\n+  \/\/ Returns true iff region idx's membership is which_partition.  If which_partition represents a free set, asserts\n+  \/\/ that the region has allocation capacity.\n+  inline bool partition_id_matches(ssize_t idx, ShenandoahFreeSetPartitionId which_partition) const;\n+#endif\n+\n+  inline size_t max_regions() const { return _max; }\n+\n+  inline size_t region_size_bytes() const { return _region_size_bytes; };\n+\n+  \/\/ The following four methods return the left-most and right-most bounds on ranges of regions representing\n+  \/\/ the requested set.  The _empty variants represent bounds on the range that holds completely empty\n+  \/\/ regions, which are required for humongous allocations and desired for \"very large\" allocations.\n+  \/\/   if the requested which_partition is empty:\n+  \/\/     leftmost() and leftmost_empty() return _max, rightmost() and rightmost_empty() return 0\n+  \/\/   otherwise, expect the following:\n+  \/\/     0 <= leftmost <= leftmost_empty <= rightmost_empty <= rightmost < _max\n+  inline ssize_t leftmost(ShenandoahFreeSetPartitionId which_partition) const;\n+  inline ssize_t rightmost(ShenandoahFreeSetPartitionId which_partition) const;\n+  ssize_t leftmost_empty(ShenandoahFreeSetPartitionId which_partition);\n+  ssize_t rightmost_empty(ShenandoahFreeSetPartitionId which_partition);\n+\n+  inline bool is_empty(ShenandoahFreeSetPartitionId which_partition) const;\n+\n+  inline void increase_used(ShenandoahFreeSetPartitionId which_partition, size_t bytes);\n+\n+  inline void set_bias_from_left_to_right(ShenandoahFreeSetPartitionId which_partition, bool value) {\n+    assert (which_partition < NumPartitions, \"selected free set must be valid\");\n+    _left_to_right_bias[int(which_partition)] = value;\n+  }\n+\n+  inline bool alloc_from_left_bias(ShenandoahFreeSetPartitionId which_partition) const {\n+    assert (which_partition < NumPartitions, \"selected free set must be valid\");\n+    return _left_to_right_bias[int(which_partition)];\n+  }\n+\n+  inline size_t capacity_of(ShenandoahFreeSetPartitionId which_partition) const {\n+    assert (which_partition < NumPartitions, \"selected free set must be valid\");\n+    return _capacity[int(which_partition)];\n+  }\n+\n+  inline size_t used_by(ShenandoahFreeSetPartitionId which_partition) const {\n+    assert (which_partition < NumPartitions, \"selected free set must be valid\");\n+    return _used[int(which_partition)];\n+  }\n+\n+  inline size_t available_in(ShenandoahFreeSetPartitionId which_partition) const {\n+    assert (which_partition < NumPartitions, \"selected free set must be valid\");\n+    return _capacity[int(which_partition)] - _used[int(which_partition)];\n+  }\n+\n+  inline void set_capacity_of(ShenandoahFreeSetPartitionId which_partition, size_t value) {\n+    assert (which_partition < NumPartitions, \"selected free set must be valid\");\n+    _capacity[int(which_partition)] = value;\n+  }\n+\n+  inline void set_used_by(ShenandoahFreeSetPartitionId which_partition, size_t value) {\n+    assert (which_partition < NumPartitions, \"selected free set must be valid\");\n+    _used[int(which_partition)] = value;\n+  }\n+\n+  inline size_t count(ShenandoahFreeSetPartitionId which_partition) const { return _region_counts[int(which_partition)]; }\n+\n+  \/\/ Assure leftmost, rightmost, leftmost_empty, and rightmost_empty bounds are valid for all free sets.\n+  \/\/ Valid bounds honor all of the following (where max is the number of heap regions):\n+  \/\/   if the set is empty, leftmost equals max and rightmost equals 0\n+  \/\/   Otherwise (the set is not empty):\n+  \/\/     0 <= leftmost < max and 0 <= rightmost < max\n+  \/\/     the region at leftmost is in the set\n+  \/\/     the region at rightmost is in the set\n+  \/\/     rightmost >= leftmost\n+  \/\/     for every idx that is in the set {\n+  \/\/       idx >= leftmost &&\n+  \/\/       idx <= rightmost\n+  \/\/     }\n+  \/\/   if the set has no empty regions, leftmost_empty equals max and rightmost_empty equals 0\n+  \/\/   Otherwise (the region has empty regions):\n+  \/\/     0 <= leftmost_empty < max and 0 <= rightmost_empty < max\n+  \/\/     rightmost_empty >= leftmost_empty\n+  \/\/     for every idx that is in the set and is empty {\n+  \/\/       idx >= leftmost &&\n+  \/\/       idx <= rightmost\n+  \/\/     }\n+  void assert_bounds() NOT_DEBUG_RETURN;\n+};\n+\n+\/\/ Publicly, ShenandoahFreeSet represents memory that is available to mutator threads.  The public capacity(), used(),\n+\/\/ and available() methods represent this public notion of memory that is under control of the mutator.  Separately,\n+\/\/ ShenandoahFreeSet also represents memory available to garbage collection activities for compaction purposes.\n+\/\/\n+\/\/ The Shenandoah garbage collector evacuates live objects out of specific regions that are identified as members of the\n+\/\/ collection set (cset).\n+\/\/\n+\/\/ The ShenandoahFreeSet tries to colocate survivor objects (objects that have been evacuated at least once) at the\n+\/\/ high end of memory.  New mutator allocations are taken from the low end of memory.  Within the mutator's range of regions,\n+\/\/ humongous allocations are taken from the lowest addresses, and LAB (local allocation buffers) and regular shared allocations\n+\/\/ are taken from the higher address of the mutator's range of regions.  This approach allows longer lasting survivor regions\n+\/\/ to congregate at the top of the heap and longer lasting humongous regions to congregate at the bottom of the heap, with\n+\/\/ short-lived frequently evacuated regions occupying the middle of the heap.\n+\/\/\n+\/\/ Mutator and garbage collection activities tend to scramble the content of regions.  Twice, during each GC pass, we rebuild\n+\/\/ the free set in an effort to restore the efficient segregation of Collector and Mutator regions:\n+\/\/\n+\/\/  1. At the start of evacuation, we know exactly how much memory is going to be evacuated, and this guides our\n+\/\/     sizing of the Collector free set.\n+\/\/\n+\/\/  2. At the end of GC, we have reclaimed all of the memory that was spanned by the cset.  We rebuild here to make\n+\/\/     sure there is enough memory reserved at the high end of memory to hold the objects that might need to be evacuated\n+\/\/     during the next GC pass.\n@@ -34,3 +288,1 @@\n-  CHeapBitMap _mutator_free_bitmap;\n-  CHeapBitMap _collector_free_bitmap;\n-  size_t _max;\n+  ShenandoahRegionPartitions _partitions;\n@@ -38,4 +290,1 @@\n-  \/\/ Left-most and right-most region indexes. There are no free regions outside\n-  \/\/ of [left-most; right-most] index intervals\n-  size_t _mutator_leftmost, _mutator_rightmost;\n-  size_t _collector_leftmost, _collector_rightmost;\n+  HeapWord* allocate_aligned_plab(size_t size, ShenandoahAllocRequest& req, ShenandoahHeapRegion* r);\n@@ -43,2 +292,4 @@\n-  size_t _capacity;\n-  size_t _used;\n+  \/\/ Return the address of memory allocated, setting in_new_region to true iff the allocation is taken\n+  \/\/ from a region that was previously empty.  Return nullptr if memory could not be allocated.\n+  inline HeapWord* allocate_from_partition_with_affiliation(ShenandoahAffiliation affiliation,\n+                                                            ShenandoahAllocRequest& req, bool& in_new_region);\n@@ -46,1 +297,4 @@\n-  void assert_bounds() const NOT_DEBUG_RETURN;\n+  \/\/ We re-evaluate the left-to-right allocation bias whenever _alloc_bias_weight is less than zero.  Each time\n+  \/\/ we allocate an object, we decrement the count of this value.  Each time we re-evaluate whether to allocate\n+  \/\/ from right-to-left or left-to-right, we reset the value of this counter to _InitialAllocBiasWeight.\n+  ssize_t _alloc_bias_weight;\n@@ -48,2 +302,1 @@\n-  bool is_mutator_free(size_t idx) const;\n-  bool is_collector_free(size_t idx) const;\n+  const ssize_t INITIAL_ALLOC_BIAS_WEIGHT = 256;\n@@ -51,0 +304,2 @@\n+  \/\/ Increases used memory for the partition if the allocation is successful. `in_new_region` will be set\n+  \/\/ if this is the first allocation in the region.\n@@ -52,0 +307,5 @@\n+\n+  \/\/ While holding the heap lock, allocate memory for a single object or LAB  which is to be entirely contained\n+  \/\/ within a single HeapRegion as characterized by req.\n+  \/\/\n+  \/\/ Precondition: !ShenandoahHeapRegion::requires_humongous(req.size())\n@@ -53,0 +313,6 @@\n+\n+  \/\/ While holding the heap lock, allocate memory for a humongous object which spans one or more regions that\n+  \/\/ were previously empty.  Regions that represent humongous objects are entirely dedicated to the humongous\n+  \/\/ object.  No other objects are packed into these regions.\n+  \/\/\n+  \/\/ Precondition: ShenandoahHeapRegion::requires_humongous(req.size())\n@@ -55,0 +321,7 @@\n+  \/\/ Change region r from the Mutator partition to the GC's Collector or OldCollector partition.  This requires that the\n+  \/\/ region is entirely empty.\n+  \/\/\n+  \/\/ Typical usage: During evacuation, the GC may find it needs more memory than had been reserved at the start of evacuation to\n+  \/\/ hold evacuated objects.  If this occurs and memory is still available in the Mutator's free set, we will flip a region from\n+  \/\/ the Mutator free set into the Collector or OldCollector free set. The conditions to move this region are checked by\n+  \/\/ the caller, so the given region is always moved.\n@@ -57,3 +330,25 @@\n-  void recompute_bounds();\n-  void adjust_bounds();\n-  bool touches_bounds(size_t num) const;\n+  \/\/ Return true if and only if the given region is successfully flipped to the old partition\n+  bool flip_to_old_gc(ShenandoahHeapRegion* r);\n+\n+  \/\/ Handle allocation for mutator.\n+  HeapWord* allocate_for_mutator(ShenandoahAllocRequest &req, bool &in_new_region);\n+\n+  \/\/ Update allocation bias and decided whether to allocate from the left or right side of the heap.\n+  void update_allocation_bias();\n+\n+  \/\/ Search for regions to satisfy allocation request using iterator.\n+  template<typename Iter>\n+  HeapWord* allocate_from_regions(Iter& iterator, ShenandoahAllocRequest &req, bool &in_new_region);\n+\n+  \/\/ Handle allocation for collector (for evacuation).\n+  HeapWord* allocate_for_collector(ShenandoahAllocRequest& req, bool& in_new_region);\n+\n+  \/\/ Search for allocation in region with same affiliation as request, using given iterator.\n+  template<typename Iter>\n+  HeapWord* allocate_with_affiliation(Iter& iterator, ShenandoahAffiliation affiliation, ShenandoahAllocRequest& req, bool& in_new_region);\n+\n+  \/\/ Return true if the respective generation for this request has free regions.\n+  bool can_allocate_in_new_region(const ShenandoahAllocRequest& req);\n+\n+  \/\/ Attempt to allocate memory for an evacuation from the mutator's partition.\n+  HeapWord* try_allocate_from_mutator(ShenandoahAllocRequest& req, bool& in_new_region);\n@@ -61,1 +356,0 @@\n-  void increase_used(size_t amount);\n@@ -64,2 +358,7 @@\n-  size_t collector_count() const { return _collector_free_bitmap.count_one_bits(); }\n-  size_t mutator_count()   const { return _mutator_free_bitmap.count_one_bits();   }\n+  \/\/ Returns true iff this region is entirely available, either because it is empty() or because it has been found to represent\n+  \/\/ immediate trash and we'll be able to immediately recycle it.  Note that we cannot recycle immediate trash if\n+  \/\/ concurrent weak root processing is in progress.\n+  inline bool can_allocate_from(ShenandoahHeapRegion *r) const;\n+  inline bool can_allocate_from(size_t idx) const;\n+\n+  inline bool has_alloc_capacity(ShenandoahHeapRegion *r) const;\n@@ -67,1 +366,6 @@\n-  void try_recycle_trashed(ShenandoahHeapRegion *r);\n+  size_t transfer_empty_regions_from_collector_set_to_mutator_set(ShenandoahFreeSetPartitionId which_collector,\n+                                                                  size_t max_xfer_regions,\n+                                                                  size_t& bytes_transferred);\n+  size_t transfer_non_empty_regions_from_collector_set_to_mutator_set(ShenandoahFreeSetPartitionId which_collector,\n+                                                                      size_t max_xfer_regions,\n+                                                                      size_t& bytes_transferred);\n@@ -69,3 +373,10 @@\n-  bool can_allocate_from(ShenandoahHeapRegion *r);\n-  size_t alloc_capacity(ShenandoahHeapRegion *r);\n-  bool has_no_alloc_capacity(ShenandoahHeapRegion *r);\n+\n+  \/\/ Determine whether we prefer to allocate from left to right or from right to left within the OldCollector free-set.\n+  void establish_old_collector_alloc_bias();\n+\n+  \/\/ Set max_capacity for young and old generations\n+  void establish_generation_sizes(size_t young_region_count, size_t old_region_count);\n+  size_t get_usable_free_words(size_t free_bytes) const;\n+\n+  \/\/ log status, assuming lock has already been acquired by the caller.\n+  void log_status();\n@@ -76,0 +387,4 @@\n+  \/\/ Public because ShenandoahRegionPartitions assertions require access.\n+  inline size_t alloc_capacity(ShenandoahHeapRegion *r) const;\n+  inline size_t alloc_capacity(size_t idx) const;\n+\n@@ -77,1 +392,43 @@\n-  void rebuild();\n+\n+  \/\/ Examine the existing free set representation, capturing the current state into var arguments:\n+  \/\/\n+  \/\/ young_cset_regions is the number of regions currently in the young cset if we are starting to evacuate, or zero\n+  \/\/   old_cset_regions is the number of regions currently in the old cset if we are starting a mixed evacuation, or zero\n+  \/\/   first_old_region is the index of the first region that is part of the OldCollector set\n+  \/\/    last_old_region is the index of the last region that is part of the OldCollector set\n+  \/\/   old_region_count is the number of regions in the OldCollector set that have memory available to be allocated\n+  void prepare_to_rebuild(size_t &young_cset_regions, size_t &old_cset_regions,\n+                          size_t &first_old_region, size_t &last_old_region, size_t &old_region_count);\n+\n+  \/\/ At the end of final mark, but before we begin evacuating, heuristics calculate how much memory is required to\n+  \/\/ hold the results of evacuating to young-gen and to old-gen, and have_evacuation_reserves should be true.\n+  \/\/ These quantities, stored as reserves for their respective generations, are consulted prior to rebuilding\n+  \/\/ the free set (ShenandoahFreeSet) in preparation for evacuation.  When the free set is rebuilt, we make sure\n+  \/\/ to reserve sufficient memory in the collector and old_collector sets to hold evacuations.\n+  \/\/\n+  \/\/ We also rebuild the free set at the end of GC, as we prepare to idle GC until the next trigger.  In this case,\n+  \/\/ have_evacuation_reserves is false because we don't yet know how much memory will need to be evacuated in the\n+  \/\/ next GC cycle.  When have_evacuation_reserves is false, the free set rebuild operation reserves for the collector\n+  \/\/ and old_collector sets based on alternative mechanisms, such as ShenandoahEvacReserve, ShenandoahOldEvacReserve, and\n+  \/\/ ShenandoahOldCompactionReserve.  In a future planned enhancement, the reserve for old_collector set when the\n+  \/\/ evacuation reserves are unknown, is based in part on anticipated promotion as determined by analysis of live data\n+  \/\/ found during the previous GC pass which is one less than the current tenure age.\n+  \/\/\n+  \/\/ young_cset_regions is the number of regions currently in the young cset if we are starting to evacuate, or zero\n+  \/\/   old_cset_regions is the number of regions currently in the old cset if we are starting a mixed evacuation, or zero\n+  \/\/    num_old_regions is the number of old-gen regions that have available memory for further allocations (excluding old cset)\n+  \/\/ have_evacuation_reserves is true iff the desired values of young-gen and old-gen evacuation reserves and old-gen\n+  \/\/                    promotion reserve have been precomputed (and can be obtained by invoking\n+  \/\/                    <generation>->get_evacuation_reserve() or old_gen->get_promoted_reserve()\n+  void finish_rebuild(size_t young_cset_regions, size_t old_cset_regions, size_t num_old_regions,\n+                      bool have_evacuation_reserves = false);\n+\n+  \/\/ When a region is promoted in place, we add the region's available memory if it is greater than plab_min_size()\n+  \/\/ into the old collector partition by invoking this method.\n+  void add_promoted_in_place_region_to_old_collector(ShenandoahHeapRegion* region);\n+\n+  \/\/ Move up to cset_regions number of regions from being available to the collector to being available to the mutator.\n+  \/\/\n+  \/\/ Typical usage: At the end of evacuation, when the collector no longer needs the regions that had been reserved\n+  \/\/ for evacuation, invoke this to make regions available for mutator allocations.\n+  void move_regions_from_collector_to_mutator(size_t cset_regions);\n@@ -81,1 +438,2 @@\n-  void log_status();\n+  \/\/ Acquire heap lock and log status, assuming heap lock is not acquired by the caller.\n+  void log_status_under_lock();\n@@ -83,5 +441,9 @@\n-  size_t capacity()  const { return _capacity; }\n-  size_t used()      const { return _used;     }\n-  size_t available() const {\n-    assert(_used <= _capacity, \"must use less than capacity\");\n-    return _capacity - _used;\n+  \/\/ Note that capacity is the number of regions that had available memory at most recent rebuild.  It is not the\n+  \/\/ entire size of the young or global generation.  (Regions within the generation that were fully utilized at time of\n+  \/\/ rebuild are not counted as part of capacity.)\n+  inline size_t capacity()  const { return _partitions.capacity_of(ShenandoahFreeSetPartitionId::Mutator); }\n+  inline size_t used()      const { return _partitions.used_by(ShenandoahFreeSetPartitionId::Mutator);     }\n+\n+  inline size_t available() const {\n+    assert(used() <= capacity(), \"must use less than capacity\");\n+    return capacity() - used();\n@@ -91,1 +453,0 @@\n-  size_t unsafe_peek_free() const;\n@@ -93,0 +454,21 @@\n+  \/*\n+   * Internal fragmentation metric: describes how fragmented the heap regions are.\n+   *\n+   * It is derived as:\n+   *\n+   *               sum(used[i]^2, i=0..k)\n+   *   IF = 1 - ------------------------------\n+   *              C * sum(used[i], i=0..k)\n+   *\n+   * ...where k is the number of regions in computation, C is the region capacity, and\n+   * used[i] is the used space in the region.\n+   *\n+   * The non-linearity causes IF to be lower for the cases where the same total heap\n+   * used is densely packed. For example:\n+   *   a) Heap is completely full  => IF = 0\n+   *   b) Heap is half full, first 50% regions are completely full => IF = 0\n+   *   c) Heap is half full, each region is 50% full => IF = 1\/2\n+   *   d) Heap is quarter full, first 50% regions are completely full => IF = 0\n+   *   e) Heap is quarter full, each region is 25% full => IF = 3\/4\n+   *   f) Heap has one small object per each region => IF =~ 1\n+   *\/\n@@ -94,0 +476,14 @@\n+\n+  \/*\n+   * External fragmentation metric: describes how fragmented the heap is.\n+   *\n+   * It is derived as:\n+   *\n+   *   EF = 1 - largest_contiguous_free \/ total_free\n+   *\n+   * For example:\n+   *   a) Heap is completely empty => EF = 0\n+   *   b) Heap is completely full => EF = 0\n+   *   c) Heap is first-half full => EF = 1\/2\n+   *   d) Heap is half full, full and empty regions interleave => EF =~ 1\n+   *\/\n@@ -97,0 +493,22 @@\n+\n+  \/\/ This function places all regions that have allocation capacity into the mutator partition, or if the region\n+  \/\/ is already affiliated with old, into the old collector partition, identifying regions that have no allocation\n+  \/\/ capacity as NotFree.  Capture the modified state of the freeset into var arguments:\n+  \/\/\n+  \/\/ young_cset_regions is the number of regions currently in the young cset if we are starting to evacuate, or zero\n+  \/\/   old_cset_regions is the number of regions currently in the old cset if we are starting a mixed evacuation, or zero\n+  \/\/   first_old_region is the index of the first region that is part of the OldCollector set\n+  \/\/    last_old_region is the index of the last region that is part of the OldCollector set\n+  \/\/   old_region_count is the number of regions in the OldCollector set that have memory available to be allocated\n+  void find_regions_with_alloc_capacity(size_t &young_cset_regions, size_t &old_cset_regions,\n+                                        size_t &first_old_region, size_t &last_old_region, size_t &old_region_count);\n+\n+  \/\/ Ensure that Collector has at least to_reserve bytes of available memory, and OldCollector has at least old_reserve\n+  \/\/ bytes of available memory.  On input, old_region_count holds the number of regions already present in the\n+  \/\/ OldCollector partition.  Upon return, old_region_count holds the updated number of regions in the OldCollector partition.\n+  void reserve_regions(size_t to_reserve, size_t old_reserve, size_t &old_region_count);\n+\n+  \/\/ Reserve space for evacuations, with regions reserved for old evacuations placed to the right\n+  \/\/ of regions reserved of young evacuations.\n+  void compute_young_and_old_reserves(size_t young_cset_regions, size_t old_cset_regions, bool have_evacuation_reserves,\n+                                      size_t &young_reserve_result, size_t &old_reserve_result) const;\n","filename":"src\/hotspot\/share\/gc\/shenandoah\/shenandoahFreeSet.hpp","additions":448,"deletions":30,"binary":false,"changes":478,"status":"modified"},{"patch":"@@ -3,0 +3,1 @@\n+ * Copyright Amazon.com Inc. or its affiliates. All Rights Reserved.\n@@ -34,0 +35,1 @@\n+#include \"gc\/shenandoah\/shenandoahCollectorPolicy.hpp\"\n@@ -36,0 +38,1 @@\n+#include \"gc\/shenandoah\/shenandoahCollectorPolicy.hpp\"\n@@ -38,0 +41,2 @@\n+#include \"gc\/shenandoah\/shenandoahGenerationalFullGC.hpp\"\n+#include \"gc\/shenandoah\/shenandoahGlobalGeneration.hpp\"\n@@ -41,0 +46,1 @@\n+#include \"gc\/shenandoah\/shenandoahHeapRegionClosures.hpp\"\n@@ -58,1 +64,0 @@\n-#include \"runtime\/javaThread.hpp\"\n@@ -108,0 +113,6 @@\n+  ShenandoahHeap* const heap = ShenandoahHeap::heap();\n+\n+  if (heap->mode()->is_generational()) {\n+    ShenandoahGenerationalFullGC::handle_completion(heap);\n+  }\n+\n@@ -110,2 +121,2 @@\n-  if (metrics.is_good_progress()) {\n-    ShenandoahHeap::heap()->notify_gc_progress();\n+  if (metrics.is_good_progress(heap->global_generation())) {\n+    heap->notify_gc_progress();\n@@ -115,1 +126,10 @@\n-    ShenandoahHeap::heap()->notify_gc_no_progress();\n+    heap->notify_gc_no_progress();\n+  }\n+\n+  \/\/ Regardless if progress was made, we record that we completed a \"successful\" full GC.\n+  heap->global_generation()->heuristics()->record_success_full();\n+  heap->shenandoah_policy()->record_success_full();\n+\n+  {\n+    ShenandoahTimingsTracker timing(ShenandoahPhaseTimings::full_gc_propagate_gc_state);\n+    heap->propagate_gc_state_to_all_threads();\n@@ -122,0 +142,4 @@\n+  if (heap->mode()->is_generational()) {\n+    ShenandoahGenerationalFullGC::prepare();\n+  }\n+\n@@ -164,1 +188,1 @@\n-    \/\/ b. Cancel concurrent mark, if in progress\n+    \/\/ b. Cancel all concurrent marks, if in progress\n@@ -166,2 +190,1 @@\n-      ShenandoahConcurrentGC::cancel();\n-      heap->set_concurrent_mark_in_progress(false);\n+      heap->cancel_concurrent_mark();\n@@ -176,7 +199,2 @@\n-    \/\/ d. Reset the bitmaps for new marking\n-    heap->reset_mark_bitmap();\n-    assert(heap->marking_context()->is_bitmap_clear(), \"sanity\");\n-    assert(!heap->marking_context()->is_complete(), \"sanity\");\n-\n-    \/\/ e. Abandon reference discovery and clear all discovered references.\n-    ShenandoahReferenceProcessor* rp = heap->ref_processor();\n+    \/\/ d. Abandon reference discovery and clear all discovered references.\n+    ShenandoahReferenceProcessor* rp = heap->global_generation()->ref_processor();\n@@ -185,1 +203,1 @@\n-    \/\/ f. Sync pinned region status from the CP marks\n+    \/\/ e. Sync pinned region status from the CP marks\n@@ -188,0 +206,4 @@\n+    if (heap->mode()->is_generational()) {\n+      ShenandoahGenerationalFullGC::restore_top_before_promote(heap);\n+    }\n+\n@@ -195,0 +217,1 @@\n+    \/\/ Note: PLABs are also retired with GCLABs in generational mode.\n@@ -231,1 +254,0 @@\n-  }\n@@ -233,4 +255,1 @@\n-  {\n-    \/\/ Epilogue\n-    _preserved_marks->restore(heap->workers());\n-    _preserved_marks->reclaim();\n+    phase5_epilog();\n@@ -265,13 +284,0 @@\n-class ShenandoahPrepareForMarkClosure: public ShenandoahHeapRegionClosure {\n-private:\n-  ShenandoahMarkingContext* const _ctx;\n-\n-public:\n-  ShenandoahPrepareForMarkClosure() : _ctx(ShenandoahHeap::heap()->marking_context()) {}\n-\n-  void heap_region_do(ShenandoahHeapRegion *r) {\n-    _ctx->capture_top_at_mark_start(r);\n-    r->clear_live_data();\n-  }\n-};\n-\n@@ -284,2 +290,3 @@\n-  ShenandoahPrepareForMarkClosure cl;\n-  heap->heap_region_iterate(&cl);\n+  heap->global_generation()->reset_mark_bitmap<true, true>();\n+  assert(heap->marking_context()->is_bitmap_clear(), \"sanity\");\n+  assert(!heap->global_generation()->is_mark_complete(), \"sanity\");\n@@ -287,1 +294,1 @@\n-  heap->set_unload_classes(heap->heuristics()->can_unload_classes());\n+  heap->set_unload_classes(heap->global_generation()->heuristics()->can_unload_classes());\n@@ -289,1 +296,1 @@\n-  ShenandoahReferenceProcessor* rp = heap->ref_processor();\n+  ShenandoahReferenceProcessor* rp = heap->global_generation()->ref_processor();\n@@ -293,1 +300,1 @@\n-  ShenandoahSTWMark mark(true \/*full_gc*\/);\n+  ShenandoahSTWMark mark(heap->global_generation(), true \/*full_gc*\/);\n@@ -296,0 +303,4 @@\n+\n+  if (ShenandoahHeap::heap()->mode()->is_generational()) {\n+    ShenandoahGenerationalFullGC::log_live_in_old(heap);\n+  }\n@@ -324,1 +335,1 @@\n-  void finish_region() {\n+  void finish() {\n@@ -339,2 +350,2 @@\n-    assert(_heap->complete_marking_context()->is_marked(p), \"must be marked\");\n-    assert(!_heap->complete_marking_context()->allocated_after_mark_start(p), \"must be truly marked\");\n+    assert(_heap->gc_generation()->complete_marking_context()->is_marked(p), \"must be marked\");\n+    assert(!_heap->gc_generation()->complete_marking_context()->allocated_after_mark_start(p), \"must be truly marked\");\n@@ -344,1 +355,1 @@\n-      finish_region();\n+      finish();\n@@ -398,9 +409,8 @@\n-  void work(uint worker_id) {\n-    ShenandoahParallelWorkerSession worker_session(worker_id);\n-    ShenandoahHeapRegionSet* slice = _worker_slices[worker_id];\n-    ShenandoahHeapRegionSetIterator it(slice);\n-    ShenandoahHeapRegion* from_region = it.next();\n-    \/\/ No work?\n-    if (from_region == nullptr) {\n-       return;\n-    }\n+  void work(uint worker_id) override;\n+private:\n+  template<typename ClosureType>\n+  void prepare_for_compaction(ClosureType& cl,\n+                              GrowableArray<ShenandoahHeapRegion*>& empty_regions,\n+                              ShenandoahHeapRegionSetIterator& it,\n+                              ShenandoahHeapRegion* from_region);\n+};\n@@ -408,3 +418,9 @@\n-    \/\/ Sliding compaction. Walk all regions in the slice, and compact them.\n-    \/\/ Remember empty regions and reuse them as needed.\n-    ResourceMark rm;\n+void ShenandoahPrepareForCompactionTask::work(uint worker_id) {\n+  ShenandoahParallelWorkerSession worker_session(worker_id);\n+  ShenandoahHeapRegionSet* slice = _worker_slices[worker_id];\n+  ShenandoahHeapRegionSetIterator it(slice);\n+  ShenandoahHeapRegion* from_region = it.next();\n+  \/\/ No work?\n+  if (from_region == nullptr) {\n+    return;\n+  }\n@@ -412,1 +428,3 @@\n-    GrowableArray<ShenandoahHeapRegion*> empty_regions((int)_heap->num_regions());\n+  \/\/ Sliding compaction. Walk all regions in the slice, and compact them.\n+  \/\/ Remember empty regions and reuse them as needed.\n+  ResourceMark rm;\n@@ -414,1 +432,1 @@\n-    ShenandoahPrepareForCompactionObjectClosure cl(_preserved_marks->get(worker_id), empty_regions, from_region);\n+  GrowableArray<ShenandoahHeapRegion*> empty_regions((int)_heap->num_regions());\n@@ -416,7 +434,9 @@\n-    while (from_region != nullptr) {\n-      assert(is_candidate_region(from_region), \"Sanity\");\n-\n-      cl.set_from_region(from_region);\n-      if (from_region->has_live()) {\n-        _heap->marked_object_iterate(from_region, &cl);\n-      }\n+  if (_heap->mode()->is_generational()) {\n+    ShenandoahPrepareForGenerationalCompactionObjectClosure cl(_preserved_marks->get(worker_id),\n+                                                               empty_regions, from_region, worker_id);\n+    prepare_for_compaction(cl, empty_regions, it, from_region);\n+  } else {\n+    ShenandoahPrepareForCompactionObjectClosure cl(_preserved_marks->get(worker_id), empty_regions, from_region);\n+    prepare_for_compaction(cl, empty_regions, it, from_region);\n+  }\n+}\n@@ -424,5 +444,10 @@\n-      \/\/ Compacted the region to somewhere else? From-region is empty then.\n-      if (!cl.is_compact_same_region()) {\n-        empty_regions.append(from_region);\n-      }\n-      from_region = it.next();\n+template<typename ClosureType>\n+void ShenandoahPrepareForCompactionTask::prepare_for_compaction(ClosureType& cl,\n+                                                                GrowableArray<ShenandoahHeapRegion*>& empty_regions,\n+                                                                ShenandoahHeapRegionSetIterator& it,\n+                                                                ShenandoahHeapRegion* from_region) {\n+  while (from_region != nullptr) {\n+    assert(is_candidate_region(from_region), \"Sanity\");\n+    cl.set_from_region(from_region);\n+    if (from_region->has_live()) {\n+      _heap->marked_object_iterate(from_region, &cl);\n@@ -430,1 +455,0 @@\n-    cl.finish_region();\n@@ -432,4 +456,3 @@\n-    \/\/ Mark all remaining regions as empty\n-    for (int pos = cl.empty_regions_pos(); pos < empty_regions.length(); ++pos) {\n-      ShenandoahHeapRegion* r = empty_regions.at(pos);\n-      r->set_new_top(r->bottom());\n+    \/\/ Compacted the region to somewhere else? From-region is empty then.\n+    if (!cl.is_compact_same_region()) {\n+      empty_regions.append(from_region);\n@@ -437,0 +460,1 @@\n+    from_region = it.next();\n@@ -438,1 +462,8 @@\n-};\n+  cl.finish();\n+\n+  \/\/ Mark all remaining regions as empty\n+  for (int pos = cl.empty_regions_pos(); pos < empty_regions.length(); ++pos) {\n+    ShenandoahHeapRegion* r = empty_regions.at(pos);\n+    r->set_new_top(r->bottom());\n+  }\n+}\n@@ -457,0 +488,1 @@\n+  log_debug(gc)(\"Full GC calculating target humongous objects from end \" SIZE_FORMAT, to_end);\n@@ -496,1 +528,1 @@\n-      r->recycle();\n+      r->try_recycle_under_lock();\n@@ -499,0 +531,1 @@\n+      \/\/ Leave affiliation unchanged\n@@ -520,1 +553,1 @@\n-    _ctx(ShenandoahHeap::heap()->complete_marking_context()) {}\n+    _ctx(ShenandoahHeap::heap()->global_generation()->complete_marking_context()) {}\n@@ -522,1 +555,1 @@\n-  void heap_region_do(ShenandoahHeapRegion* r) {\n+  void heap_region_do(ShenandoahHeapRegion* r) override {\n@@ -526,2 +559,1 @@\n-        assert(!r->has_live(),\n-               \"Region \" SIZE_FORMAT \" is not marked, should not have live\", r->index());\n+        assert(!r->has_live(), \"Region \" SIZE_FORMAT \" is not marked, should not have live\", r->index());\n@@ -530,2 +562,1 @@\n-        assert(r->has_live(),\n-               \"Region \" SIZE_FORMAT \" should have live\", r->index());\n+        assert(r->has_live(), \"Region \" SIZE_FORMAT \" should have live\", r->index());\n@@ -535,2 +566,1 @@\n-      assert(r->humongous_start_region()->has_live(),\n-             \"Region \" SIZE_FORMAT \" should have live\", r->index());\n+      assert(r->humongous_start_region()->has_live(), \"Region \" SIZE_FORMAT \" should have live\", r->index());\n@@ -699,2 +729,3 @@\n-    ShenandoahTrashImmediateGarbageClosure tigcl;\n-    heap->heap_region_iterate(&tigcl);\n+    ShenandoahTrashImmediateGarbageClosure trash_immediate_garbage;\n+    ShenandoahExcludeRegionClosure<FREE> cl(&trash_immediate_garbage);\n+    heap->heap_region_iterate(&cl);\n@@ -746,1 +777,1 @@\n-    _ctx(ShenandoahHeap::heap()->complete_marking_context()) {}\n+    _ctx(ShenandoahHeap::heap()->gc_generation()->complete_marking_context()) {}\n@@ -764,1 +795,1 @@\n-    assert(_heap->complete_marking_context()->is_marked(p), \"must be marked\");\n+    assert(_heap->gc_generation()->complete_marking_context()->is_marked(p), \"must be marked\");\n@@ -788,0 +819,3 @@\n+      if (_heap->mode()->is_generational()) {\n+        ShenandoahGenerationalFullGC::maybe_coalesce_and_fill_region(r);\n+      }\n@@ -845,1 +879,1 @@\n-    assert(_heap->complete_marking_context()->is_marked(p), \"must be marked\");\n+    assert(_heap->gc_generation()->complete_marking_context()->is_marked(p), \"must be marked\");\n@@ -892,1 +926,3 @@\n-  size_t _live;\n+  bool _is_generational;\n+  size_t _young_regions, _young_usage, _young_humongous_waste;\n+  size_t _old_regions, _old_usage, _old_humongous_waste;\n@@ -895,1 +931,9 @@\n-  ShenandoahPostCompactClosure() : _heap(ShenandoahHeap::heap()), _live(0) {\n+  ShenandoahPostCompactClosure() : _heap(ShenandoahHeap::heap()),\n+                                   _is_generational(_heap->mode()->is_generational()),\n+                                   _young_regions(0),\n+                                   _young_usage(0),\n+                                   _young_humongous_waste(0),\n+                                   _old_regions(0),\n+                                   _old_usage(0),\n+                                   _old_humongous_waste(0)\n+  {\n@@ -908,1 +952,1 @@\n-      _heap->complete_marking_context()->reset_top_at_mark_start(r);\n+      _heap->gc_generation()->complete_marking_context()->reset_top_at_mark_start(r);\n@@ -915,0 +959,4 @@\n+      if (!_is_generational) {\n+        r->make_affiliated_maybe();\n+      }\n+      \/\/ else, generational mode compaction has already established affiliation.\n@@ -929,1 +977,7 @@\n-      r->recycle();\n+      r->try_recycle_under_lock();\n+    } else {\n+      if (r->is_old()) {\n+        ShenandoahGenerationalFullGC::account_for_region(r, _old_regions, _old_usage, _old_humongous_waste);\n+      } else if (r->is_young()) {\n+        ShenandoahGenerationalFullGC::account_for_region(r, _young_regions, _young_usage, _young_humongous_waste);\n+      }\n@@ -931,1 +985,0 @@\n-\n@@ -934,1 +987,0 @@\n-    _live += live;\n@@ -937,2 +989,15 @@\n-  size_t get_live() {\n-    return _live;\n+  void update_generation_usage() {\n+    if (_is_generational) {\n+      _heap->old_generation()->establish_usage(_old_regions, _old_usage, _old_humongous_waste);\n+      _heap->young_generation()->establish_usage(_young_regions, _young_usage, _young_humongous_waste);\n+    } else {\n+      assert(_old_regions == 0, \"Old regions only expected in generational mode\");\n+      assert(_old_usage == 0, \"Old usage only expected in generational mode\");\n+      assert(_old_humongous_waste == 0, \"Old humongous waste only expected in generational mode\");\n+    }\n+\n+    \/\/ In generational mode, global usage should be the sum of young and old. This is also true\n+    \/\/ for non-generational modes except that there are no old regions.\n+    _heap->global_generation()->establish_usage(_old_regions + _young_regions,\n+                                                _old_usage + _young_usage,\n+                                                _old_humongous_waste + _young_humongous_waste);\n@@ -947,1 +1012,1 @@\n-  \/\/ sliding costs. We may consider doing this in parallel in future.\n+  \/\/ sliding costs. We may consider doing this in parallel in the future.\n@@ -969,0 +1034,1 @@\n+      log_debug(gc)(\"Full GC compaction moves humongous object from region \" SIZE_FORMAT \" to region \" SIZE_FORMAT, old_start, new_start);\n@@ -976,0 +1042,1 @@\n+        ShenandoahAffiliation original_affiliation = r->affiliation();\n@@ -978,0 +1045,1 @@\n+          \/\/ Leave humongous region affiliation unchanged.\n@@ -985,1 +1053,1 @@\n-            r->make_humongous_start_bypass();\n+            r->make_humongous_start_bypass(original_affiliation);\n@@ -987,1 +1055,1 @@\n-            r->make_humongous_cont_bypass();\n+            r->make_humongous_cont_bypass(original_affiliation);\n@@ -1025,1 +1093,1 @@\n-    ShenandoahMarkingContext* const ctx = heap->complete_marking_context();\n+    ShenandoahMarkingContext* const ctx = heap->gc_generation()->complete_marking_context();\n@@ -1053,0 +1121,5 @@\n+}\n+\n+void ShenandoahFullGC::phase5_epilog() {\n+  GCTraceTime(Info, gc, phases) time(\"Phase 5: Full GC epilog\", _gc_timer);\n+  ShenandoahHeap* heap = ShenandoahHeap::heap();\n@@ -1065,1 +1138,0 @@\n-\n@@ -1068,1 +1140,5 @@\n-    heap->set_used(post_compact.get_live());\n+    post_compact.update_generation_usage();\n+\n+    if (heap->mode()->is_generational()) {\n+      ShenandoahGenerationalFullGC::balance_generations_after_gc(heap);\n+    }\n@@ -1071,1 +1147,16 @@\n-    heap->free_set()->rebuild();\n+    size_t young_cset_regions, old_cset_regions;\n+    size_t first_old, last_old, num_old;\n+    heap->free_set()->prepare_to_rebuild(young_cset_regions, old_cset_regions, first_old, last_old, num_old);\n+\n+    \/\/ We also do not expand old generation size following Full GC because we have scrambled age populations and\n+    \/\/ no longer have objects separated by age into distinct regions.\n+    if (heap->mode()->is_generational()) {\n+      ShenandoahGenerationalFullGC::compute_balances();\n+    }\n+\n+    heap->free_set()->finish_rebuild(young_cset_regions, old_cset_regions, num_old);\n+\n+    \/\/ Set mark incomplete because the marking bitmaps have been reset except pinned regions.\n+    heap->global_generation()->set_mark_incomplete();\n+\n+    heap->clear_cancelled_gc(true \/* clear oom handler *\/);\n@@ -1074,1 +1165,9 @@\n-  heap->clear_cancelled_gc();\n+  _preserved_marks->restore(heap->workers());\n+  _preserved_marks->reclaim();\n+\n+  \/\/ We defer generation resizing actions until after cset regions have been recycled.  We do this even following an\n+  \/\/ abbreviated cycle.\n+  if (heap->mode()->is_generational()) {\n+    ShenandoahGenerationalFullGC::balance_generations_after_rebuilding_free_set();\n+    ShenandoahGenerationalFullGC::rebuild_remembered_set(heap);\n+  }\n","filename":"src\/hotspot\/share\/gc\/shenandoah\/shenandoahFullGC.cpp","additions":203,"deletions":104,"binary":false,"changes":307,"status":"modified"},{"patch":"@@ -84,0 +84,1 @@\n+  void phase5_epilog();\n","filename":"src\/hotspot\/share\/gc\/shenandoah\/shenandoahFullGC.hpp","additions":1,"deletions":0,"binary":false,"changes":1,"status":"modified"},{"patch":"@@ -42,0 +42,2 @@\n+    case _degenerated_roots:\n+      return \"Roots\";\n@@ -46,1 +48,1 @@\n-    case _degenerated_updaterefs:\n+    case _degenerated_update_refs:\n","filename":"src\/hotspot\/share\/gc\/shenandoah\/shenandoahGC.cpp","additions":3,"deletions":1,"binary":false,"changes":4,"status":"modified"},{"patch":"@@ -53,0 +53,1 @@\n+    _degenerated_roots,\n@@ -55,1 +56,1 @@\n-    _degenerated_updaterefs,\n+    _degenerated_update_refs,\n","filename":"src\/hotspot\/share\/gc\/shenandoah\/shenandoahGC.hpp","additions":2,"deletions":1,"binary":false,"changes":3,"status":"modified"},{"patch":"@@ -0,0 +1,1011 @@\n+\/*\n+ * Copyright Amazon.com Inc. or its affiliates. All Rights Reserved.\n+ * DO NOT ALTER OR REMOVE COPYRIGHT NOTICES OR THIS FILE HEADER.\n+ *\n+ * This code is free software; you can redistribute it and\/or modify it\n+ * under the terms of the GNU General Public License version 2 only, as\n+ * published by the Free Software Foundation.\n+ *\n+ * This code is distributed in the hope that it will be useful, but WITHOUT\n+ * ANY WARRANTY; without even the implied warranty of MERCHANTABILITY or\n+ * FITNESS FOR A PARTICULAR PURPOSE.  See the GNU General Public License\n+ * version 2 for more details (a copy is included in the LICENSE file that\n+ * accompanied this code).\n+ *\n+ * You should have received a copy of the GNU General Public License version\n+ * 2 along with this work; if not, write to the Free Software Foundation,\n+ * Inc., 51 Franklin St, Fifth Floor, Boston, MA 02110-1301 USA.\n+ *\n+ * Please contact Oracle, 500 Oracle Parkway, Redwood Shores, CA 94065 USA\n+ * or visit www.oracle.com if you need additional information or have any\n+ * questions.\n+ *\n+ *\/\n+\n+#include \"precompiled.hpp\"\n+#include \"gc\/shenandoah\/shenandoahCollectorPolicy.hpp\"\n+#include \"gc\/shenandoah\/shenandoahCollectionSetPreselector.hpp\"\n+#include \"gc\/shenandoah\/shenandoahFreeSet.hpp\"\n+#include \"gc\/shenandoah\/shenandoahGeneration.hpp\"\n+#include \"gc\/shenandoah\/shenandoahGenerationalHeap.inline.hpp\"\n+#include \"gc\/shenandoah\/shenandoahHeapRegionClosures.hpp\"\n+#include \"gc\/shenandoah\/shenandoahOldGeneration.hpp\"\n+#include \"gc\/shenandoah\/shenandoahReferenceProcessor.hpp\"\n+#include \"gc\/shenandoah\/shenandoahScanRemembered.inline.hpp\"\n+#include \"gc\/shenandoah\/shenandoahTaskqueue.inline.hpp\"\n+#include \"gc\/shenandoah\/shenandoahUtils.hpp\"\n+#include \"gc\/shenandoah\/shenandoahVerifier.hpp\"\n+#include \"gc\/shenandoah\/shenandoahYoungGeneration.hpp\"\n+#include \"gc\/shenandoah\/heuristics\/shenandoahHeuristics.hpp\"\n+\n+#include \"utilities\/quickSort.hpp\"\n+\n+template <bool PREPARE_FOR_CURRENT_CYCLE, bool FULL_GC = false>\n+class ShenandoahResetBitmapClosure final : public ShenandoahHeapRegionClosure {\n+private:\n+  ShenandoahHeap*           _heap;\n+  ShenandoahMarkingContext* _ctx;\n+\n+public:\n+  explicit ShenandoahResetBitmapClosure() :\n+    ShenandoahHeapRegionClosure(), _heap(ShenandoahHeap::heap()), _ctx(_heap->marking_context()) {}\n+\n+  void heap_region_do(ShenandoahHeapRegion* region) override {\n+    assert(!_heap->is_uncommit_in_progress(), \"Cannot uncommit bitmaps while resetting them.\");\n+    if (PREPARE_FOR_CURRENT_CYCLE) {\n+      if (region->need_bitmap_reset() && _heap->is_bitmap_slice_committed(region)) {\n+        _ctx->clear_bitmap(region);\n+      } else {\n+        region->set_needs_bitmap_reset();\n+      }\n+      \/\/ Capture Top At Mark Start for this generation.\n+      if (FULL_GC || region->is_active()) {\n+        \/\/ Reset live data and set TAMS optimistically. We would recheck these under the pause\n+        \/\/ anyway to capture any updates that happened since now.\n+        _ctx->capture_top_at_mark_start(region);\n+        region->clear_live_data();\n+      }\n+    } else {\n+      if (_heap->is_bitmap_slice_committed(region)) {\n+        _ctx->clear_bitmap(region);\n+        region->unset_needs_bitmap_reset();\n+      } else {\n+        region->set_needs_bitmap_reset();\n+      }\n+    }\n+  }\n+\n+  bool is_thread_safe() override { return true; }\n+};\n+\n+\/\/ Copy the write-version of the card-table into the read-version, clearing the\n+\/\/ write-copy.\n+class ShenandoahMergeWriteTable: public ShenandoahHeapRegionClosure {\n+private:\n+  ShenandoahScanRemembered* _scanner;\n+public:\n+  ShenandoahMergeWriteTable(ShenandoahScanRemembered* scanner) : _scanner(scanner) {}\n+\n+  void heap_region_do(ShenandoahHeapRegion* r) override {\n+    assert(r->is_old(), \"Don't waste time doing this for non-old regions\");\n+    _scanner->merge_write_table(r->bottom(), ShenandoahHeapRegion::region_size_words());\n+  }\n+\n+  bool is_thread_safe() override {\n+    return true;\n+  }\n+};\n+\n+\/\/ Add [TAMS, top) volume over young regions. Used to correct age 0 cohort census\n+\/\/ for adaptive tenuring when census is taken during marking.\n+\/\/ In non-product builds, for the purposes of verification, we also collect the total\n+\/\/ live objects in young regions as well.\n+class ShenandoahUpdateCensusZeroCohortClosure : public ShenandoahHeapRegionClosure {\n+private:\n+  ShenandoahMarkingContext* const _ctx;\n+  \/\/ Population size units are words (not bytes)\n+  size_t _age0_pop;                \/\/ running tally of age0 population size\n+  size_t _total_pop;               \/\/ total live population size\n+public:\n+  explicit ShenandoahUpdateCensusZeroCohortClosure(ShenandoahMarkingContext* ctx)\n+    : _ctx(ctx), _age0_pop(0), _total_pop(0) {}\n+\n+  void heap_region_do(ShenandoahHeapRegion* r) override {\n+    if (_ctx != nullptr && r->is_active()) {\n+      assert(r->is_young(), \"Young regions only\");\n+      HeapWord* tams = _ctx->top_at_mark_start(r);\n+      HeapWord* top  = r->top();\n+      if (top > tams) {\n+        _age0_pop += pointer_delta(top, tams);\n+      }\n+      \/\/ TODO: check significance of _ctx != nullptr above, can that\n+      \/\/ spoof _total_pop in some corner cases?\n+      NOT_PRODUCT(_total_pop += r->get_live_data_words();)\n+    }\n+  }\n+\n+  size_t get_age0_population()  const { return _age0_pop; }\n+  size_t get_total_population() const { return _total_pop; }\n+};\n+\n+void ShenandoahGeneration::confirm_heuristics_mode() {\n+  if (_heuristics->is_diagnostic() && !UnlockDiagnosticVMOptions) {\n+    vm_exit_during_initialization(\n+            err_msg(\"Heuristics \\\"%s\\\" is diagnostic, and must be enabled via -XX:+UnlockDiagnosticVMOptions.\",\n+                    _heuristics->name()));\n+  }\n+  if (_heuristics->is_experimental() && !UnlockExperimentalVMOptions) {\n+    vm_exit_during_initialization(\n+            err_msg(\"Heuristics \\\"%s\\\" is experimental, and must be enabled via -XX:+UnlockExperimentalVMOptions.\",\n+                    _heuristics->name()));\n+  }\n+}\n+\n+ShenandoahHeuristics* ShenandoahGeneration::initialize_heuristics(ShenandoahMode* gc_mode) {\n+  _heuristics = gc_mode->initialize_heuristics(this);\n+  _heuristics->set_guaranteed_gc_interval(ShenandoahGuaranteedGCInterval);\n+  confirm_heuristics_mode();\n+  return _heuristics;\n+}\n+\n+size_t ShenandoahGeneration::bytes_allocated_since_gc_start() const {\n+  return Atomic::load(&_bytes_allocated_since_gc_start);\n+}\n+\n+void ShenandoahGeneration::reset_bytes_allocated_since_gc_start() {\n+  Atomic::store(&_bytes_allocated_since_gc_start, (size_t)0);\n+}\n+\n+void ShenandoahGeneration::increase_allocated(size_t bytes) {\n+  Atomic::add(&_bytes_allocated_since_gc_start, bytes, memory_order_relaxed);\n+}\n+\n+void ShenandoahGeneration::set_evacuation_reserve(size_t new_val) {\n+  _evacuation_reserve = new_val;\n+}\n+\n+size_t ShenandoahGeneration::get_evacuation_reserve() const {\n+  return _evacuation_reserve;\n+}\n+\n+void ShenandoahGeneration::augment_evacuation_reserve(size_t increment) {\n+  _evacuation_reserve += increment;\n+}\n+\n+void ShenandoahGeneration::log_status(const char *msg) const {\n+  typedef LogTarget(Info, gc, ergo) LogGcInfo;\n+\n+  if (!LogGcInfo::is_enabled()) {\n+    return;\n+  }\n+\n+  \/\/ Not under a lock here, so read each of these once to make sure\n+  \/\/ byte size in proper unit and proper unit for byte size are consistent.\n+  const size_t v_used = used();\n+  const size_t v_used_regions = used_regions_size();\n+  const size_t v_soft_max_capacity = ShenandoahHeap::heap()->soft_max_capacity();\n+  const size_t v_max_capacity = max_capacity();\n+  const size_t v_available = available();\n+  const size_t v_humongous_waste = get_humongous_waste();\n+\n+  const LogGcInfo target;\n+  LogStream ls(target);\n+  ls.print(\"%s: \", msg);\n+  if (_type != NON_GEN) {\n+    ls.print(\"%s generation \", name());\n+  }\n+\n+  ls.print_cr(\"used: \" PROPERFMT \", used regions: \" PROPERFMT \", humongous waste: \" PROPERFMT\n+              \", soft capacity: \" PROPERFMT \", max capacity: \" PROPERFMT \", available: \" PROPERFMT,\n+              PROPERFMTARGS(v_used), PROPERFMTARGS(v_used_regions), PROPERFMTARGS(v_humongous_waste),\n+              PROPERFMTARGS(v_soft_max_capacity), PROPERFMTARGS(v_max_capacity), PROPERFMTARGS(v_available));\n+}\n+\n+template <bool PREPARE_FOR_CURRENT_CYCLE, bool FULL_GC>\n+void ShenandoahGeneration::reset_mark_bitmap() {\n+  ShenandoahHeap* heap = ShenandoahHeap::heap();\n+  heap->assert_gc_workers(heap->workers()->active_workers());\n+\n+  set_mark_incomplete();\n+\n+  ShenandoahResetBitmapClosure<PREPARE_FOR_CURRENT_CYCLE, FULL_GC> closure;\n+  parallel_heap_region_iterate_free(&closure);\n+}\n+\/\/ Explicit specializations\n+template void ShenandoahGeneration::reset_mark_bitmap<true, false>();\n+template void ShenandoahGeneration::reset_mark_bitmap<true, true>();\n+template void ShenandoahGeneration::reset_mark_bitmap<false, false>();\n+\n+\/\/ Swap the read and write card table pointers prior to the next remset scan.\n+\/\/ This avoids the need to synchronize reads of the table by the GC workers\n+\/\/ doing remset scanning, on the one hand, with the dirtying of the table by\n+\/\/ mutators on the other.\n+void ShenandoahGeneration::swap_card_tables() {\n+  \/\/ Must be sure that marking is complete before we swap remembered set.\n+  ShenandoahGenerationalHeap* heap = ShenandoahGenerationalHeap::heap();\n+  heap->assert_gc_workers(heap->workers()->active_workers());\n+  shenandoah_assert_safepoint();\n+\n+  ShenandoahOldGeneration* old_generation = heap->old_generation();\n+  old_generation->card_scan()->swap_card_tables();\n+}\n+\n+\/\/ Copy the write-version of the card-table into the read-version, clearing the\n+\/\/ write-version. The work is done at a safepoint and in parallel by the GC\n+\/\/ worker threads.\n+void ShenandoahGeneration::merge_write_table() {\n+  \/\/ This should only happen for degenerated cycles\n+  ShenandoahGenerationalHeap* heap = ShenandoahGenerationalHeap::heap();\n+  heap->assert_gc_workers(heap->workers()->active_workers());\n+  shenandoah_assert_safepoint();\n+\n+  ShenandoahOldGeneration* old_generation = heap->old_generation();\n+  ShenandoahMergeWriteTable task(old_generation->card_scan());\n+  old_generation->parallel_heap_region_iterate(&task);\n+}\n+\n+void ShenandoahGeneration::prepare_gc() {\n+  reset_mark_bitmap<true>();\n+}\n+\n+void ShenandoahGeneration::parallel_heap_region_iterate_free(ShenandoahHeapRegionClosure* cl) {\n+  ShenandoahHeap::heap()->parallel_heap_region_iterate(cl);\n+}\n+\n+void ShenandoahGeneration::compute_evacuation_budgets(ShenandoahHeap* const heap) {\n+  shenandoah_assert_generational();\n+\n+  ShenandoahOldGeneration* const old_generation = heap->old_generation();\n+  ShenandoahYoungGeneration* const young_generation = heap->young_generation();\n+\n+  \/\/ During initialization and phase changes, it is more likely that fewer objects die young and old-gen\n+  \/\/ memory is not yet full (or is in the process of being replaced).  During these times especially, it\n+  \/\/ is beneficial to loan memory from old-gen to young-gen during the evacuation and update-refs phases\n+  \/\/ of execution.\n+\n+  \/\/ Calculate EvacuationReserve before PromotionReserve.  Evacuation is more critical than promotion.\n+  \/\/ If we cannot evacuate old-gen, we will not be able to reclaim old-gen memory.  Promotions are less\n+  \/\/ critical.  If we cannot promote, there may be degradation of young-gen memory because old objects\n+  \/\/ accumulate there until they can be promoted.  This increases the young-gen marking and evacuation work.\n+\n+  \/\/ First priority is to reclaim the easy garbage out of young-gen.\n+\n+  \/\/ maximum_young_evacuation_reserve is upper bound on memory to be evacuated out of young\n+  const size_t maximum_young_evacuation_reserve = (young_generation->max_capacity() * ShenandoahEvacReserve) \/ 100;\n+  const size_t young_evacuation_reserve = MIN2(maximum_young_evacuation_reserve, young_generation->available_with_reserve());\n+\n+  \/\/ maximum_old_evacuation_reserve is an upper bound on memory evacuated from old and evacuated to old (promoted),\n+  \/\/ clamped by the old generation space available.\n+  \/\/\n+  \/\/ Here's the algebra.\n+  \/\/ Let SOEP = ShenandoahOldEvacRatioPercent,\n+  \/\/     OE = old evac,\n+  \/\/     YE = young evac, and\n+  \/\/     TE = total evac = OE + YE\n+  \/\/ By definition:\n+  \/\/            SOEP\/100 = OE\/TE\n+  \/\/                     = OE\/(OE+YE)\n+  \/\/  => SOEP\/(100-SOEP) = OE\/((OE+YE)-OE)         \/\/ componendo-dividendo: If a\/b = c\/d, then a\/(b-a) = c\/(d-c)\n+  \/\/                     = OE\/YE\n+  \/\/  =>              OE = YE*SOEP\/(100-SOEP)\n+\n+  \/\/ We have to be careful in the event that SOEP is set to 100 by the user.\n+  assert(ShenandoahOldEvacRatioPercent <= 100, \"Error\");\n+  const size_t old_available = old_generation->available();\n+  const size_t maximum_old_evacuation_reserve = (ShenandoahOldEvacRatioPercent == 100) ?\n+    old_available : MIN2((maximum_young_evacuation_reserve * ShenandoahOldEvacRatioPercent) \/ (100 - ShenandoahOldEvacRatioPercent),\n+                          old_available);\n+\n+\n+  \/\/ Second priority is to reclaim garbage out of old-gen if there are old-gen collection candidates.  Third priority\n+  \/\/ is to promote as much as we have room to promote.  However, if old-gen memory is in short supply, this means young\n+  \/\/ GC is operating under \"duress\" and was unable to transfer the memory that we would normally expect.  In this case,\n+  \/\/ old-gen will refrain from compacting itself in order to allow a quicker young-gen cycle (by avoiding the update-refs\n+  \/\/ through ALL of old-gen).  If there is some memory available in old-gen, we will use this for promotions as promotions\n+  \/\/ do not add to the update-refs burden of GC.\n+\n+  size_t old_evacuation_reserve, old_promo_reserve;\n+  if (is_global()) {\n+    \/\/ Global GC is typically triggered by user invocation of System.gc(), and typically indicates that there is lots\n+    \/\/ of garbage to be reclaimed because we are starting a new phase of execution.  Marking for global GC may take\n+    \/\/ significantly longer than typical young marking because we must mark through all old objects.  To expedite\n+    \/\/ evacuation and update-refs, we give emphasis to reclaiming garbage first, wherever that garbage is found.\n+    \/\/ Global GC will adjust generation sizes to accommodate the collection set it chooses.\n+\n+    \/\/ Set old_promo_reserve to enforce that no regions are preselected for promotion.  Such regions typically\n+    \/\/ have relatively high memory utilization.  We still call select_aged_regions() because this will prepare for\n+    \/\/ promotions in place, if relevant.\n+    old_promo_reserve = 0;\n+\n+    \/\/ Dedicate all available old memory to old_evacuation reserve.  This may be small, because old-gen is only\n+    \/\/ expanded based on an existing mixed evacuation workload at the end of the previous GC cycle.  We'll expand\n+    \/\/ the budget for evacuation of old during GLOBAL cset selection.\n+    old_evacuation_reserve = maximum_old_evacuation_reserve;\n+  } else if (old_generation->has_unprocessed_collection_candidates()) {\n+    \/\/ We reserved all old-gen memory at end of previous GC to hold anticipated evacuations to old-gen.  If this is\n+    \/\/ mixed evacuation, reserve all of this memory for compaction of old-gen and do not promote.  Prioritize compaction\n+    \/\/ over promotion in order to defragment OLD so that it will be better prepared to efficiently receive promoted memory.\n+    old_evacuation_reserve = maximum_old_evacuation_reserve;\n+    old_promo_reserve = 0;\n+  } else {\n+    \/\/ Make all old-evacuation memory for promotion, but if we can't use it all for promotion, we'll allow some evacuation.\n+    old_evacuation_reserve = 0;\n+    old_promo_reserve = maximum_old_evacuation_reserve;\n+  }\n+  assert(old_evacuation_reserve <= old_available, \"Error\");\n+\n+  \/\/ We see too many old-evacuation failures if we force ourselves to evacuate into regions that are not initially empty.\n+  \/\/ So we limit the old-evacuation reserve to unfragmented memory.  Even so, old-evacuation is free to fill in nooks and\n+  \/\/ crannies within existing partially used regions and it generally tries to do so.\n+  const size_t old_free_unfragmented = old_generation->free_unaffiliated_regions() * ShenandoahHeapRegion::region_size_bytes();\n+  if (old_evacuation_reserve > old_free_unfragmented) {\n+    const size_t delta = old_evacuation_reserve - old_free_unfragmented;\n+    old_evacuation_reserve -= delta;\n+    \/\/ Let promo consume fragments of old-gen memory if not global\n+    if (!is_global()) {\n+      old_promo_reserve += delta;\n+    }\n+  }\n+\n+  \/\/ Preselect regions for promotion by evacuation (obtaining the live data to seed promoted_reserve),\n+  \/\/ and identify regions that will promote in place. These use the tenuring threshold.\n+  const size_t consumed_by_advance_promotion = select_aged_regions(old_promo_reserve);\n+  assert(consumed_by_advance_promotion <= maximum_old_evacuation_reserve, \"Cannot promote more than available old-gen memory\");\n+\n+  \/\/ Note that unused old_promo_reserve might not be entirely consumed_by_advance_promotion.  Do not transfer this\n+  \/\/ to old_evacuation_reserve because this memory is likely very fragmented, and we do not want to increase the likelihood\n+  \/\/ of old evacuation failure.\n+  young_generation->set_evacuation_reserve(young_evacuation_reserve);\n+  old_generation->set_evacuation_reserve(old_evacuation_reserve);\n+  old_generation->set_promoted_reserve(consumed_by_advance_promotion);\n+\n+  \/\/ There is no need to expand OLD because all memory used here was set aside at end of previous GC, except in the\n+  \/\/ case of a GLOBAL gc.  During choose_collection_set() of GLOBAL, old will be expanded on demand.\n+}\n+\n+\/\/ Having chosen the collection set, adjust the budgets for generational mode based on its composition.  Note\n+\/\/ that young_generation->available() now knows about recently discovered immediate garbage.\n+\/\/\n+void ShenandoahGeneration::adjust_evacuation_budgets(ShenandoahHeap* const heap, ShenandoahCollectionSet* const collection_set) {\n+  shenandoah_assert_generational();\n+  \/\/ We may find that old_evacuation_reserve and\/or loaned_for_young_evacuation are not fully consumed, in which case we may\n+  \/\/  be able to increase regions_available_to_loan\n+\n+  \/\/ The role of adjust_evacuation_budgets() is to compute the correct value of regions_available_to_loan and to make\n+  \/\/ effective use of this memory, including the remnant memory within these regions that may result from rounding loan to\n+  \/\/ integral number of regions.  Excess memory that is available to be loaned is applied to an allocation supplement,\n+  \/\/ which allows mutators to allocate memory beyond the current capacity of young-gen on the promise that the loan\n+  \/\/ will be repaid as soon as we finish updating references for the recently evacuated collection set.\n+\n+  \/\/ We cannot recalculate regions_available_to_loan by simply dividing old_generation->available() by region_size_bytes\n+  \/\/ because the available memory may be distributed between many partially occupied regions that are already holding old-gen\n+  \/\/ objects.  Memory in partially occupied regions is not \"available\" to be loaned.  Note that an increase in old-gen\n+  \/\/ available that results from a decrease in memory consumed by old evacuation is not necessarily available to be loaned\n+  \/\/ to young-gen.\n+\n+  size_t region_size_bytes = ShenandoahHeapRegion::region_size_bytes();\n+  ShenandoahOldGeneration* const old_generation = heap->old_generation();\n+  ShenandoahYoungGeneration* const young_generation = heap->young_generation();\n+\n+  size_t old_evacuated = collection_set->get_old_bytes_reserved_for_evacuation();\n+  size_t old_evacuated_committed = (size_t) (ShenandoahOldEvacWaste * double(old_evacuated));\n+  size_t old_evacuation_reserve = old_generation->get_evacuation_reserve();\n+\n+  if (old_evacuated_committed > old_evacuation_reserve) {\n+    \/\/ This should only happen due to round-off errors when enforcing ShenandoahOldEvacWaste\n+    assert(old_evacuated_committed <= (33 * old_evacuation_reserve) \/ 32,\n+           \"Round-off errors should be less than 3.125%%, committed: \" SIZE_FORMAT \", reserved: \" SIZE_FORMAT,\n+           old_evacuated_committed, old_evacuation_reserve);\n+    old_evacuated_committed = old_evacuation_reserve;\n+    \/\/ Leave old_evac_reserve as previously configured\n+  } else if (old_evacuated_committed < old_evacuation_reserve) {\n+    \/\/ This happens if the old-gen collection consumes less than full budget.\n+    old_evacuation_reserve = old_evacuated_committed;\n+    old_generation->set_evacuation_reserve(old_evacuation_reserve);\n+  }\n+\n+  size_t young_advance_promoted = collection_set->get_young_bytes_to_be_promoted();\n+  size_t young_advance_promoted_reserve_used = (size_t) (ShenandoahPromoEvacWaste * double(young_advance_promoted));\n+\n+  size_t young_evacuated = collection_set->get_young_bytes_reserved_for_evacuation();\n+  size_t young_evacuated_reserve_used = (size_t) (ShenandoahEvacWaste * double(young_evacuated));\n+\n+  size_t total_young_available = young_generation->available_with_reserve();\n+  assert(young_evacuated_reserve_used <= total_young_available, \"Cannot evacuate more than is available in young\");\n+  young_generation->set_evacuation_reserve(young_evacuated_reserve_used);\n+\n+  size_t old_available = old_generation->available();\n+  \/\/ Now that we've established the collection set, we know how much memory is really required by old-gen for evacuation\n+  \/\/ and promotion reserves.  Try shrinking OLD now in case that gives us a bit more runway for mutator allocations during\n+  \/\/ evac and update phases.\n+  size_t old_consumed = old_evacuated_committed + young_advance_promoted_reserve_used;\n+\n+  if (old_available < old_consumed) {\n+    \/\/ This can happen due to round-off errors when adding the results of truncated integer arithmetic.\n+    \/\/ We've already truncated old_evacuated_committed.  Truncate young_advance_promoted_reserve_used here.\n+    assert(young_advance_promoted_reserve_used <= (33 * (old_available - old_evacuated_committed)) \/ 32,\n+           \"Round-off errors should be less than 3.125%%, committed: \" SIZE_FORMAT \", reserved: \" SIZE_FORMAT,\n+           young_advance_promoted_reserve_used, old_available - old_evacuated_committed);\n+    young_advance_promoted_reserve_used = old_available - old_evacuated_committed;\n+    old_consumed = old_evacuated_committed + young_advance_promoted_reserve_used;\n+  }\n+\n+  assert(old_available >= old_consumed, \"Cannot consume (\" SIZE_FORMAT \") more than is available (\" SIZE_FORMAT \")\",\n+         old_consumed, old_available);\n+  size_t excess_old = old_available - old_consumed;\n+  size_t unaffiliated_old_regions = old_generation->free_unaffiliated_regions();\n+  size_t unaffiliated_old = unaffiliated_old_regions * region_size_bytes;\n+  assert(old_available >= unaffiliated_old, \"Unaffiliated old is a subset of old available\");\n+\n+  \/\/ Make sure old_evac_committed is unaffiliated\n+  if (old_evacuated_committed > 0) {\n+    if (unaffiliated_old > old_evacuated_committed) {\n+      size_t giveaway = unaffiliated_old - old_evacuated_committed;\n+      size_t giveaway_regions = giveaway \/ region_size_bytes;  \/\/ round down\n+      if (giveaway_regions > 0) {\n+        excess_old = MIN2(excess_old, giveaway_regions * region_size_bytes);\n+      } else {\n+        excess_old = 0;\n+      }\n+    } else {\n+      excess_old = 0;\n+    }\n+  }\n+\n+  \/\/ If we find that OLD has excess regions, give them back to YOUNG now to reduce likelihood we run out of allocation\n+  \/\/ runway during evacuation and update-refs.\n+  size_t regions_to_xfer = 0;\n+  if (excess_old > unaffiliated_old) {\n+    \/\/ we can give back unaffiliated_old (all of unaffiliated is excess)\n+    if (unaffiliated_old_regions > 0) {\n+      regions_to_xfer = unaffiliated_old_regions;\n+    }\n+  } else if (unaffiliated_old_regions > 0) {\n+    \/\/ excess_old < unaffiliated old: we can give back MIN(excess_old\/region_size_bytes, unaffiliated_old_regions)\n+    size_t excess_regions = excess_old \/ region_size_bytes;\n+    regions_to_xfer = MIN2(excess_regions, unaffiliated_old_regions);\n+  }\n+\n+  if (regions_to_xfer > 0) {\n+    bool result = ShenandoahGenerationalHeap::cast(heap)->generation_sizer()->transfer_to_young(regions_to_xfer);\n+    assert(excess_old >= regions_to_xfer * region_size_bytes,\n+           \"Cannot transfer (\" SIZE_FORMAT \", \" SIZE_FORMAT \") more than excess old (\" SIZE_FORMAT \")\",\n+           regions_to_xfer, region_size_bytes, excess_old);\n+    excess_old -= regions_to_xfer * region_size_bytes;\n+    log_debug(gc, ergo)(\"%s transferred \" SIZE_FORMAT \" excess regions to young before start of evacuation\",\n+                       result? \"Successfully\": \"Unsuccessfully\", regions_to_xfer);\n+  }\n+\n+  \/\/ Add in the excess_old memory to hold unanticipated promotions, if any.  If there are more unanticipated\n+  \/\/ promotions than fit in reserved memory, they will be deferred until a future GC pass.\n+  size_t total_promotion_reserve = young_advance_promoted_reserve_used + excess_old;\n+  old_generation->set_promoted_reserve(total_promotion_reserve);\n+  old_generation->reset_promoted_expended();\n+}\n+\n+typedef struct {\n+  ShenandoahHeapRegion* _region;\n+  size_t _live_data;\n+} AgedRegionData;\n+\n+static int compare_by_aged_live(AgedRegionData a, AgedRegionData b) {\n+  if (a._live_data < b._live_data)\n+    return -1;\n+  else if (a._live_data > b._live_data)\n+    return 1;\n+  else return 0;\n+}\n+\n+inline void assert_no_in_place_promotions() {\n+#ifdef ASSERT\n+  class ShenandoahNoInPlacePromotions : public ShenandoahHeapRegionClosure {\n+  public:\n+    void heap_region_do(ShenandoahHeapRegion *r) override {\n+      assert(r->get_top_before_promote() == nullptr,\n+             \"Region \" SIZE_FORMAT \" should not be ready for in-place promotion\", r->index());\n+    }\n+  } cl;\n+  ShenandoahHeap::heap()->heap_region_iterate(&cl);\n+#endif\n+}\n+\n+\/\/ Preselect for inclusion into the collection set regions whose age is at or above tenure age which contain more than\n+\/\/ ShenandoahOldGarbageThreshold amounts of garbage.  We identify these regions by setting the appropriate entry of\n+\/\/ the collection set's preselected regions array to true.  All entries are initialized to false before calling this\n+\/\/ function.\n+\/\/\n+\/\/ During the subsequent selection of the collection set, we give priority to these promotion set candidates.\n+\/\/ Without this prioritization, we found that the aged regions tend to be ignored because they typically have\n+\/\/ much less garbage and much more live data than the recently allocated \"eden\" regions.  When aged regions are\n+\/\/ repeatedly excluded from the collection set, the amount of live memory within the young generation tends to\n+\/\/ accumulate and this has the undesirable side effect of causing young-generation collections to require much more\n+\/\/ CPU and wall-clock time.\n+\/\/\n+\/\/ A second benefit of treating aged regions differently than other regions during collection set selection is\n+\/\/ that this allows us to more accurately budget memory to hold the results of evacuation.  Memory for evacuation\n+\/\/ of aged regions must be reserved in the old generation.  Memory for evacuation of all other regions must be\n+\/\/ reserved in the young generation.\n+size_t ShenandoahGeneration::select_aged_regions(size_t old_available) {\n+\n+  \/\/ There should be no regions configured for subsequent in-place-promotions carried over from the previous cycle.\n+  assert_no_in_place_promotions();\n+\n+  auto const heap = ShenandoahGenerationalHeap::heap();\n+  bool* const candidate_regions_for_promotion_by_copy = heap->collection_set()->preselected_regions();\n+  ShenandoahMarkingContext* const ctx = heap->marking_context();\n+\n+  const size_t old_garbage_threshold = (ShenandoahHeapRegion::region_size_bytes() * ShenandoahOldGarbageThreshold) \/ 100;\n+\n+  size_t old_consumed = 0;\n+  size_t promo_potential = 0;\n+  size_t candidates = 0;\n+\n+  \/\/ Tracks the padding of space above top in regions eligible for promotion in place\n+  size_t promote_in_place_pad = 0;\n+\n+  \/\/ Sort the promotion-eligible regions in order of increasing live-data-bytes so that we can first reclaim regions that require\n+  \/\/ less evacuation effort.  This prioritizes garbage first, expanding the allocation pool early before we reclaim regions that\n+  \/\/ have more live data.\n+  const size_t num_regions = heap->num_regions();\n+\n+  ResourceMark rm;\n+  AgedRegionData* sorted_regions = NEW_RESOURCE_ARRAY(AgedRegionData, num_regions);\n+\n+  for (size_t i = 0; i < num_regions; i++) {\n+    ShenandoahHeapRegion* const r = heap->get_region(i);\n+    if (r->is_empty() || !r->has_live() || !r->is_young() || !r->is_regular()) {\n+      \/\/ skip over regions that aren't regular young with some live data\n+      continue;\n+    }\n+    if (heap->is_tenurable(r)) {\n+      if ((r->garbage() < old_garbage_threshold)) {\n+        \/\/ This tenure-worthy region has too little garbage, so we do not want to expend the copying effort to\n+        \/\/ reclaim the garbage; instead this region may be eligible for promotion-in-place to the\n+        \/\/ old generation.\n+        HeapWord* tams = ctx->top_at_mark_start(r);\n+        HeapWord* original_top = r->top();\n+        if (!heap->is_concurrent_old_mark_in_progress() && tams == original_top) {\n+          \/\/ No allocations from this region have been made during concurrent mark. It meets all the criteria\n+          \/\/ for in-place-promotion. Though we only need the value of top when we fill the end of the region,\n+          \/\/ we use this field to indicate that this region should be promoted in place during the evacuation\n+          \/\/ phase.\n+          r->save_top_before_promote();\n+\n+          size_t remnant_size = r->free() \/ HeapWordSize;\n+          if (remnant_size > ShenandoahHeap::min_fill_size()) {\n+            ShenandoahHeap::fill_with_object(original_top, remnant_size);\n+            \/\/ Fill the remnant memory within this region to assure no allocations prior to promote in place.  Otherwise,\n+            \/\/ newly allocated objects will not be parsable when promote in place tries to register them.  Furthermore, any\n+            \/\/ new allocations would not necessarily be eligible for promotion.  This addresses both issues.\n+            r->set_top(r->end());\n+            promote_in_place_pad += remnant_size * HeapWordSize;\n+          } else {\n+            \/\/ Since the remnant is so small that it cannot be filled, we don't have to worry about any accidental\n+            \/\/ allocations occurring within this region before the region is promoted in place.\n+          }\n+        }\n+        \/\/ Else, we do not promote this region (either in place or by copy) because it has received new allocations.\n+\n+        \/\/ During evacuation, we exclude from promotion regions for which age > tenure threshold, garbage < garbage-threshold,\n+        \/\/  and get_top_before_promote() != tams\n+      } else {\n+        \/\/ Record this promotion-eligible candidate region. After sorting and selecting the best candidates below,\n+        \/\/ we may still decide to exclude this promotion-eligible region from the current collection set.  If this\n+        \/\/ happens, we will consider this region as part of the anticipated promotion potential for the next GC\n+        \/\/ pass; see further below.\n+        sorted_regions[candidates]._region = r;\n+        sorted_regions[candidates++]._live_data = r->get_live_data_bytes();\n+      }\n+    } else {\n+      \/\/ We only evacuate & promote objects from regular regions whose garbage() is above old-garbage-threshold.\n+      \/\/ Objects in tenure-worthy regions with less garbage are promoted in place. These take a different path to\n+      \/\/ old-gen.  Regions excluded from promotion because their garbage content is too low (causing us to anticipate that\n+      \/\/ the region would be promoted in place) may be eligible for evacuation promotion by the time promotion takes\n+      \/\/ place during a subsequent GC pass because more garbage is found within the region between now and then.  This\n+      \/\/ should not happen if we are properly adapting the tenure age.  The theory behind adaptive tenuring threshold\n+      \/\/ is to choose the youngest age that demonstrates no \"significant\" further loss of population since the previous\n+      \/\/ age.  If not this, we expect the tenure age to demonstrate linear population decay for at least two population\n+      \/\/ samples, whereas we expect to observe exponential population decay for ages younger than the tenure age.\n+      \/\/\n+      \/\/ In the case that certain regions which were anticipated to be promoted in place need to be promoted by\n+      \/\/ evacuation, it may be the case that there is not sufficient reserve within old-gen to hold evacuation of\n+      \/\/ these regions.  The likely outcome is that these regions will not be selected for evacuation or promotion\n+      \/\/ in the current cycle and we will anticipate that they will be promoted in the next cycle.  This will cause\n+      \/\/ us to reserve more old-gen memory so that these objects can be promoted in the subsequent cycle.\n+      if (heap->is_aging_cycle() && heap->age_census()->is_tenurable(r->age() + 1)) {\n+        if (r->garbage() >= old_garbage_threshold) {\n+          promo_potential += r->get_live_data_bytes();\n+        }\n+      }\n+    }\n+    \/\/ Note that we keep going even if one region is excluded from selection.\n+    \/\/ Subsequent regions may be selected if they have smaller live data.\n+  }\n+  \/\/ Sort in increasing order according to live data bytes.  Note that candidates represents the number of regions\n+  \/\/ that qualify to be promoted by evacuation.\n+  if (candidates > 0) {\n+    size_t selected_regions = 0;\n+    size_t selected_live = 0;\n+    QuickSort::sort<AgedRegionData>(sorted_regions, candidates, compare_by_aged_live, false);\n+    for (size_t i = 0; i < candidates; i++) {\n+      ShenandoahHeapRegion* const region = sorted_regions[i]._region;\n+      size_t region_live_data = sorted_regions[i]._live_data;\n+      size_t promotion_need = (size_t) (region_live_data * ShenandoahPromoEvacWaste);\n+      if (old_consumed + promotion_need <= old_available) {\n+        old_consumed += promotion_need;\n+        candidate_regions_for_promotion_by_copy[region->index()] = true;\n+        selected_regions++;\n+        selected_live += region_live_data;\n+      } else {\n+        \/\/ We rejected this promotable region from the collection set because we had no room to hold its copy.\n+        \/\/ Add this region to promo potential for next GC.\n+        promo_potential += region_live_data;\n+        assert(!candidate_regions_for_promotion_by_copy[region->index()], \"Shouldn't be selected\");\n+      }\n+      \/\/ We keep going even if one region is excluded from selection because we need to accumulate all eligible\n+      \/\/ regions that are not preselected into promo_potential\n+    }\n+    log_debug(gc)(\"Preselected \" SIZE_FORMAT \" regions containing \" SIZE_FORMAT \" live bytes,\"\n+                 \" consuming: \" SIZE_FORMAT \" of budgeted: \" SIZE_FORMAT,\n+                 selected_regions, selected_live, old_consumed, old_available);\n+  }\n+\n+  heap->old_generation()->set_pad_for_promote_in_place(promote_in_place_pad);\n+  heap->old_generation()->set_promotion_potential(promo_potential);\n+  return old_consumed;\n+}\n+\n+void ShenandoahGeneration::prepare_regions_and_collection_set(bool concurrent) {\n+  ShenandoahHeap* heap = ShenandoahHeap::heap();\n+  ShenandoahCollectionSet* collection_set = heap->collection_set();\n+  bool is_generational = heap->mode()->is_generational();\n+\n+  assert(!heap->is_full_gc_in_progress(), \"Only for concurrent and degenerated GC\");\n+  assert(!is_old(), \"Only YOUNG and GLOBAL GC perform evacuations\");\n+  {\n+    ShenandoahGCPhase phase(concurrent ? ShenandoahPhaseTimings::final_update_region_states :\n+                            ShenandoahPhaseTimings::degen_gc_final_update_region_states);\n+    ShenandoahFinalMarkUpdateRegionStateClosure cl(complete_marking_context());\n+    parallel_heap_region_iterate(&cl);\n+\n+    if (is_young()) {\n+      \/\/ We always need to update the watermark for old regions. If there\n+      \/\/ are mixed collections pending, we also need to synchronize the\n+      \/\/ pinned status for old regions. Since we are already visiting every\n+      \/\/ old region here, go ahead and sync the pin status too.\n+      ShenandoahFinalMarkUpdateRegionStateClosure old_cl(nullptr);\n+      heap->old_generation()->parallel_heap_region_iterate(&old_cl);\n+    }\n+  }\n+\n+  \/\/ Tally the census counts and compute the adaptive tenuring threshold\n+  if (is_generational && ShenandoahGenerationalAdaptiveTenuring && !ShenandoahGenerationalCensusAtEvac) {\n+    \/\/ Objects above TAMS weren't included in the age census. Since they were all\n+    \/\/ allocated in this cycle they belong in the age 0 cohort. We walk over all\n+    \/\/ young regions and sum the volume of objects between TAMS and top.\n+    ShenandoahUpdateCensusZeroCohortClosure age0_cl(complete_marking_context());\n+    heap->young_generation()->heap_region_iterate(&age0_cl);\n+    size_t age0_pop = age0_cl.get_age0_population();\n+\n+    \/\/ Update the global census, including the missed age 0 cohort above,\n+    \/\/ along with the census done during marking, and compute the tenuring threshold.\n+    ShenandoahAgeCensus* census = ShenandoahGenerationalHeap::heap()->age_census();\n+    census->update_census(age0_pop);\n+#ifndef PRODUCT\n+    size_t total_pop = age0_cl.get_total_population();\n+    size_t total_census = census->get_total();\n+    \/\/ Usually total_pop > total_census, but not by too much.\n+    \/\/ We use integer division so anything up to just less than 2 is considered\n+    \/\/ reasonable, and the \"+1\" is to avoid divide-by-zero.\n+    assert((total_pop+1)\/(total_census+1) ==  1, \"Extreme divergence: \"\n+           SIZE_FORMAT \"\/\" SIZE_FORMAT, total_pop, total_census);\n+#endif\n+  }\n+\n+  {\n+    ShenandoahGCPhase phase(concurrent ? ShenandoahPhaseTimings::choose_cset :\n+                            ShenandoahPhaseTimings::degen_gc_choose_cset);\n+\n+    collection_set->clear();\n+    ShenandoahHeapLocker locker(heap->lock());\n+    if (is_generational) {\n+      \/\/ Seed the collection set with resource area-allocated\n+      \/\/ preselected regions, which are removed when we exit this scope.\n+      ShenandoahCollectionSetPreselector preselector(collection_set, heap->num_regions());\n+\n+      \/\/ Find the amount that will be promoted, regions that will be promoted in\n+      \/\/ place, and preselect older regions that will be promoted by evacuation.\n+      compute_evacuation_budgets(heap);\n+\n+      \/\/ Choose the collection set, including the regions preselected above for\n+      \/\/ promotion into the old generation.\n+      _heuristics->choose_collection_set(collection_set);\n+      if (!collection_set->is_empty()) {\n+        \/\/ only make use of evacuation budgets when we are evacuating\n+        adjust_evacuation_budgets(heap, collection_set);\n+      }\n+\n+      if (is_global()) {\n+        \/\/ We have just chosen a collection set for a global cycle. The mark bitmap covering old regions is complete, so\n+        \/\/ the remembered set scan can use that to avoid walking into garbage. When the next old mark begins, we will\n+        \/\/ use the mark bitmap to make the old regions parsable by coalescing and filling any unmarked objects. Thus,\n+        \/\/ we prepare for old collections by remembering which regions are old at this time. Note that any objects\n+        \/\/ promoted into old regions will be above TAMS, and so will be considered marked. However, free regions that\n+        \/\/ become old after this point will not be covered correctly by the mark bitmap, so we must be careful not to\n+        \/\/ coalesce those regions. Only the old regions which are not part of the collection set at this point are\n+        \/\/ eligible for coalescing. As implemented now, this has the side effect of possibly initiating mixed-evacuations\n+        \/\/ after a global cycle for old regions that were not included in this collection set.\n+        heap->old_generation()->prepare_for_mixed_collections_after_global_gc();\n+      }\n+    } else {\n+      _heuristics->choose_collection_set(collection_set);\n+    }\n+  }\n+\n+\n+  {\n+    ShenandoahGCPhase phase(concurrent ? ShenandoahPhaseTimings::final_rebuild_freeset :\n+                            ShenandoahPhaseTimings::degen_gc_final_rebuild_freeset);\n+    ShenandoahHeapLocker locker(heap->lock());\n+    size_t young_cset_regions, old_cset_regions;\n+\n+    \/\/ We are preparing for evacuation.  At this time, we ignore cset region tallies.\n+    size_t first_old, last_old, num_old;\n+    heap->free_set()->prepare_to_rebuild(young_cset_regions, old_cset_regions, first_old, last_old, num_old);\n+    \/\/ Free set construction uses reserve quantities, because they are known to be valid here\n+    heap->free_set()->finish_rebuild(young_cset_regions, old_cset_regions, num_old, true);\n+  }\n+}\n+\n+bool ShenandoahGeneration::is_bitmap_clear() {\n+  ShenandoahHeap* heap = ShenandoahHeap::heap();\n+  ShenandoahMarkingContext* context = heap->marking_context();\n+  const size_t num_regions = heap->num_regions();\n+  for (size_t idx = 0; idx < num_regions; idx++) {\n+    ShenandoahHeapRegion* r = heap->get_region(idx);\n+    if (contains(r) && r->is_affiliated()) {\n+      if (heap->is_bitmap_slice_committed(r) && (context->top_at_mark_start(r) > r->bottom()) &&\n+          !context->is_bitmap_range_within_region_clear(r->bottom(), r->end())) {\n+        return false;\n+      }\n+    }\n+  }\n+  return true;\n+}\n+\n+void ShenandoahGeneration::set_mark_complete() {\n+  _is_marking_complete.set();\n+}\n+\n+void ShenandoahGeneration::set_mark_incomplete() {\n+  _is_marking_complete.unset();\n+}\n+\n+ShenandoahMarkingContext* ShenandoahGeneration::complete_marking_context() {\n+  assert(is_mark_complete(), \"Marking must be completed.\");\n+  return ShenandoahHeap::heap()->marking_context();\n+}\n+\n+void ShenandoahGeneration::cancel_marking() {\n+  log_info(gc)(\"Cancel marking: %s\", name());\n+  if (is_concurrent_mark_in_progress()) {\n+    set_mark_incomplete();\n+  }\n+  _task_queues->clear();\n+  ref_processor()->abandon_partial_discovery();\n+  set_concurrent_mark_in_progress(false);\n+}\n+\n+ShenandoahGeneration::ShenandoahGeneration(ShenandoahGenerationType type,\n+                                           uint max_workers,\n+                                           size_t max_capacity) :\n+  _type(type),\n+  _task_queues(new ShenandoahObjToScanQueueSet(max_workers)),\n+  _ref_processor(new ShenandoahReferenceProcessor(MAX2(max_workers, 1U))),\n+  _affiliated_region_count(0), _humongous_waste(0), _evacuation_reserve(0),\n+  _used(0), _bytes_allocated_since_gc_start(0),\n+  _max_capacity(max_capacity),\n+  _heuristics(nullptr)\n+{\n+  _is_marking_complete.set();\n+  assert(max_workers > 0, \"At least one queue\");\n+  for (uint i = 0; i < max_workers; ++i) {\n+    ShenandoahObjToScanQueue* task_queue = new ShenandoahObjToScanQueue();\n+    _task_queues->register_queue(i, task_queue);\n+  }\n+}\n+\n+ShenandoahGeneration::~ShenandoahGeneration() {\n+  for (uint i = 0; i < _task_queues->size(); ++i) {\n+    ShenandoahObjToScanQueue* q = _task_queues->queue(i);\n+    delete q;\n+  }\n+  delete _task_queues;\n+}\n+\n+void ShenandoahGeneration::reserve_task_queues(uint workers) {\n+  _task_queues->reserve(workers);\n+}\n+\n+ShenandoahObjToScanQueueSet* ShenandoahGeneration::old_gen_task_queues() const {\n+  return nullptr;\n+}\n+\n+void ShenandoahGeneration::scan_remembered_set(bool is_concurrent) {\n+  assert(is_young(), \"Should only scan remembered set for young generation.\");\n+\n+  ShenandoahGenerationalHeap* const heap = ShenandoahGenerationalHeap::heap();\n+  uint nworkers = heap->workers()->active_workers();\n+  reserve_task_queues(nworkers);\n+\n+  ShenandoahReferenceProcessor* rp = ref_processor();\n+  ShenandoahRegionChunkIterator work_list(nworkers);\n+  ShenandoahScanRememberedTask task(task_queues(), old_gen_task_queues(), rp, &work_list, is_concurrent);\n+  heap->assert_gc_workers(nworkers);\n+  heap->workers()->run_task(&task);\n+  if (ShenandoahEnableCardStats) {\n+    ShenandoahScanRemembered* scanner = heap->old_generation()->card_scan();\n+    assert(scanner != nullptr, \"Not generational\");\n+    scanner->log_card_stats(nworkers, CARD_STAT_SCAN_RS);\n+  }\n+}\n+\n+size_t ShenandoahGeneration::increment_affiliated_region_count() {\n+  shenandoah_assert_heaplocked_or_safepoint();\n+  \/\/ During full gc, multiple GC worker threads may change region affiliations without a lock.  No lock is enforced\n+  \/\/ on read and write of _affiliated_region_count.  At the end of full gc, a single thread overwrites the count with\n+  \/\/ a coherent value.\n+  return Atomic::add(&_affiliated_region_count, (size_t) 1);\n+}\n+\n+size_t ShenandoahGeneration::decrement_affiliated_region_count() {\n+  shenandoah_assert_heaplocked_or_safepoint();\n+  \/\/ During full gc, multiple GC worker threads may change region affiliations without a lock.  No lock is enforced\n+  \/\/ on read and write of _affiliated_region_count.  At the end of full gc, a single thread overwrites the count with\n+  \/\/ a coherent value.\n+  auto affiliated_region_count = Atomic::sub(&_affiliated_region_count, (size_t) 1);\n+  assert(ShenandoahHeap::heap()->is_full_gc_in_progress() ||\n+         (used() + _humongous_waste <= affiliated_region_count * ShenandoahHeapRegion::region_size_bytes()),\n+         \"used + humongous cannot exceed regions\");\n+  return affiliated_region_count;\n+}\n+\n+size_t ShenandoahGeneration::decrement_affiliated_region_count_without_lock() {\n+  return Atomic::sub(&_affiliated_region_count, (size_t) 1);\n+}\n+\n+size_t ShenandoahGeneration::increase_affiliated_region_count(size_t delta) {\n+  shenandoah_assert_heaplocked_or_safepoint();\n+  return Atomic::add(&_affiliated_region_count, delta);\n+}\n+\n+size_t ShenandoahGeneration::decrease_affiliated_region_count(size_t delta) {\n+  shenandoah_assert_heaplocked_or_safepoint();\n+  assert(Atomic::load(&_affiliated_region_count) >= delta, \"Affiliated region count cannot be negative\");\n+\n+  auto const affiliated_region_count = Atomic::sub(&_affiliated_region_count, delta);\n+  assert(ShenandoahHeap::heap()->is_full_gc_in_progress() ||\n+         (_used + _humongous_waste <= affiliated_region_count * ShenandoahHeapRegion::region_size_bytes()),\n+         \"used + humongous cannot exceed regions\");\n+  return affiliated_region_count;\n+}\n+\n+void ShenandoahGeneration::establish_usage(size_t num_regions, size_t num_bytes, size_t humongous_waste) {\n+  assert(ShenandoahSafepoint::is_at_shenandoah_safepoint(), \"must be at a safepoint\");\n+  Atomic::store(&_affiliated_region_count, num_regions);\n+  Atomic::store(&_used, num_bytes);\n+  _humongous_waste = humongous_waste;\n+}\n+\n+void ShenandoahGeneration::increase_used(size_t bytes) {\n+  Atomic::add(&_used, bytes);\n+}\n+\n+void ShenandoahGeneration::increase_humongous_waste(size_t bytes) {\n+  if (bytes > 0) {\n+    Atomic::add(&_humongous_waste, bytes);\n+  }\n+}\n+\n+void ShenandoahGeneration::decrease_humongous_waste(size_t bytes) {\n+  if (bytes > 0) {\n+    assert(ShenandoahHeap::heap()->is_full_gc_in_progress() || (_humongous_waste >= bytes),\n+           \"Waste (\" SIZE_FORMAT \") cannot be negative (after subtracting \" SIZE_FORMAT \")\", _humongous_waste, bytes);\n+    Atomic::sub(&_humongous_waste, bytes);\n+  }\n+}\n+\n+void ShenandoahGeneration::decrease_used(size_t bytes) {\n+  assert(ShenandoahHeap::heap()->is_full_gc_in_progress() ||\n+         (_used >= bytes), \"cannot reduce bytes used by generation below zero\");\n+  Atomic::sub(&_used, bytes);\n+}\n+\n+size_t ShenandoahGeneration::used_regions() const {\n+  return Atomic::load(&_affiliated_region_count);\n+}\n+\n+size_t ShenandoahGeneration::free_unaffiliated_regions() const {\n+  size_t result = max_capacity() \/ ShenandoahHeapRegion::region_size_bytes();\n+  auto const used_regions = this->used_regions();\n+  if (used_regions > result) {\n+    result = 0;\n+  } else {\n+    result -= used_regions;\n+  }\n+  return result;\n+}\n+\n+size_t ShenandoahGeneration::used_regions_size() const {\n+  return used_regions() * ShenandoahHeapRegion::region_size_bytes();\n+}\n+\n+size_t ShenandoahGeneration::available() const {\n+  return available(max_capacity());\n+}\n+\n+\/\/ For ShenandoahYoungGeneration, Include the young available that may have been reserved for the Collector.\n+size_t ShenandoahGeneration::available_with_reserve() const {\n+  return available(max_capacity());\n+}\n+\n+size_t ShenandoahGeneration::soft_available() const {\n+  return available(ShenandoahHeap::heap()->soft_max_capacity());\n+}\n+\n+size_t ShenandoahGeneration::available(size_t capacity) const {\n+  size_t in_use = used() + get_humongous_waste();\n+  return in_use > capacity ? 0 : capacity - in_use;\n+}\n+\n+size_t ShenandoahGeneration::increase_capacity(size_t increment) {\n+  shenandoah_assert_heaplocked_or_safepoint();\n+\n+  \/\/ We do not enforce that new capacity >= heap->max_size_for(this).  The maximum generation size is treated as a rule of thumb\n+  \/\/ which may be violated during certain transitions, such as when we are forcing transfers for the purpose of promoting regions\n+  \/\/ in place.\n+  assert(ShenandoahHeap::heap()->is_full_gc_in_progress() ||\n+         (_max_capacity + increment <= ShenandoahHeap::heap()->max_capacity()), \"Generation cannot be larger than heap size\");\n+  assert(increment % ShenandoahHeapRegion::region_size_bytes() == 0, \"Generation capacity must be multiple of region size\");\n+  _max_capacity += increment;\n+\n+  \/\/ This detects arithmetic wraparound on _used\n+  assert(ShenandoahHeap::heap()->is_full_gc_in_progress() ||\n+         (used_regions_size() >= used()),\n+         \"Affiliated regions must hold more than what is currently used\");\n+  return _max_capacity;\n+}\n+\n+size_t ShenandoahGeneration::set_capacity(size_t byte_size) {\n+  shenandoah_assert_heaplocked_or_safepoint();\n+  _max_capacity = byte_size;\n+  return _max_capacity;\n+}\n+\n+size_t ShenandoahGeneration::decrease_capacity(size_t decrement) {\n+  shenandoah_assert_heaplocked_or_safepoint();\n+\n+  \/\/ We do not enforce that new capacity >= heap->min_size_for(this).  The minimum generation size is treated as a rule of thumb\n+  \/\/ which may be violated during certain transitions, such as when we are forcing transfers for the purpose of promoting regions\n+  \/\/ in place.\n+  assert(decrement % ShenandoahHeapRegion::region_size_bytes() == 0, \"Generation capacity must be multiple of region size\");\n+  assert(_max_capacity >= decrement, \"Generation capacity cannot be negative\");\n+\n+  _max_capacity -= decrement;\n+\n+  \/\/ This detects arithmetic wraparound on _used\n+  assert(ShenandoahHeap::heap()->is_full_gc_in_progress() ||\n+         (used_regions_size() >= used()),\n+         \"Affiliated regions must hold more than what is currently used\");\n+  assert(ShenandoahHeap::heap()->is_full_gc_in_progress() ||\n+         (_used <= _max_capacity), \"Cannot use more than capacity\");\n+  assert(ShenandoahHeap::heap()->is_full_gc_in_progress() ||\n+         (used_regions_size() <= _max_capacity),\n+         \"Cannot use more than capacity\");\n+  return _max_capacity;\n+}\n+\n+void ShenandoahGeneration::record_success_concurrent(bool abbreviated) {\n+  heuristics()->record_success_concurrent();\n+  ShenandoahHeap::heap()->shenandoah_policy()->record_success_concurrent(is_young(), abbreviated);\n+}\n","filename":"src\/hotspot\/share\/gc\/shenandoah\/shenandoahGeneration.cpp","additions":1011,"deletions":0,"binary":false,"changes":1011,"status":"added"},{"patch":"@@ -0,0 +1,244 @@\n+\/*\n+ * Copyright Amazon.com Inc. or its affiliates. All Rights Reserved.\n+ * DO NOT ALTER OR REMOVE COPYRIGHT NOTICES OR THIS FILE HEADER.\n+ *\n+ * This code is free software; you can redistribute it and\/or modify it\n+ * under the terms of the GNU General Public License version 2 only, as\n+ * published by the Free Software Foundation.\n+ *\n+ * This code is distributed in the hope that it will be useful, but WITHOUT\n+ * ANY WARRANTY; without even the implied warranty of MERCHANTABILITY or\n+ * FITNESS FOR A PARTICULAR PURPOSE.  See the GNU General Public License\n+ * version 2 for more details (a copy is included in the LICENSE file that\n+ * accompanied this code).\n+ *\n+ * You should have received a copy of the GNU General Public License version\n+ * 2 along with this work; if not, write to the Free Software Foundation,\n+ * Inc., 51 Franklin St, Fifth Floor, Boston, MA 02110-1301 USA.\n+ *\n+ * Please contact Oracle, 500 Oracle Parkway, Redwood Shores, CA 94065 USA\n+ * or visit www.oracle.com if you need additional information or have any\n+ * questions.\n+ *\n+ *\/\n+\n+#ifndef SHARE_VM_GC_SHENANDOAH_SHENANDOAHGENERATION_HPP\n+#define SHARE_VM_GC_SHENANDOAH_SHENANDOAHGENERATION_HPP\n+\n+#include \"memory\/allocation.hpp\"\n+#include \"gc\/shenandoah\/heuristics\/shenandoahSpaceInfo.hpp\"\n+#include \"gc\/shenandoah\/shenandoahAffiliation.hpp\"\n+#include \"gc\/shenandoah\/shenandoahGenerationType.hpp\"\n+#include \"gc\/shenandoah\/shenandoahLock.hpp\"\n+#include \"gc\/shenandoah\/shenandoahMarkingContext.hpp\"\n+\n+class ShenandoahCollectionSet;\n+class ShenandoahHeap;\n+class ShenandoahHeapRegion;\n+class ShenandoahHeapRegionClosure;\n+class ShenandoahHeuristics;\n+class ShenandoahMode;\n+class ShenandoahReferenceProcessor;\n+\n+\n+class ShenandoahGeneration : public CHeapObj<mtGC>, public ShenandoahSpaceInfo {\n+  friend class VMStructs;\n+private:\n+  ShenandoahGenerationType const _type;\n+\n+  \/\/ Marking task queues and completeness\n+  ShenandoahObjToScanQueueSet* _task_queues;\n+  ShenandoahSharedFlag _is_marking_complete;\n+\n+  ShenandoahReferenceProcessor* const _ref_processor;\n+\n+  volatile size_t _affiliated_region_count;\n+\n+  \/\/ How much free memory is left in the last region of humongous objects.\n+  \/\/ This is _not_ included in used, but it _is_ deducted from available,\n+  \/\/ which gives the heuristics a more accurate view of how much memory remains\n+  \/\/ for allocation. This figure is also included the heap status logging.\n+  \/\/ The units are bytes. The value is only changed on a safepoint or under the\n+  \/\/ heap lock.\n+  size_t _humongous_waste;\n+\n+  \/\/ Bytes reserved within this generation to hold evacuated objects from the collection set\n+  size_t _evacuation_reserve;\n+\n+protected:\n+  \/\/ Usage\n+\n+  volatile size_t _used;\n+  volatile size_t _bytes_allocated_since_gc_start;\n+  size_t _max_capacity;\n+\n+  ShenandoahHeuristics* _heuristics;\n+\n+private:\n+  \/\/ Compute evacuation budgets prior to choosing collection set.\n+  void compute_evacuation_budgets(ShenandoahHeap* heap);\n+\n+  \/\/ Adjust evacuation budgets after choosing collection set.\n+  void adjust_evacuation_budgets(ShenandoahHeap* heap,\n+                                 ShenandoahCollectionSet* collection_set);\n+\n+  \/\/ Preselect for possible inclusion into the collection set exactly the most\n+  \/\/ garbage-dense regions, including those that satisfy criteria 1 & 2 below,\n+  \/\/ and whose live bytes will fit within old_available budget:\n+  \/\/ Criterion 1. region age >= tenuring threshold\n+  \/\/ Criterion 2. region garbage percentage > ShenandoahOldGarbageThreshold\n+  \/\/\n+  \/\/ Identifies regions eligible for promotion in place,\n+  \/\/ being those of at least tenuring_threshold age that have lower garbage\n+  \/\/ density.\n+  \/\/\n+  \/\/ Updates promotion_potential and pad_for_promote_in_place fields\n+  \/\/ of the heap. Returns bytes of live object memory in the preselected\n+  \/\/ regions, which are marked in the preselected_regions() indicator\n+  \/\/ array of the heap's collection set, which should be initialized\n+  \/\/ to false.\n+  size_t select_aged_regions(size_t old_available);\n+\n+  size_t available(size_t capacity) const;\n+\n+ public:\n+  ShenandoahGeneration(ShenandoahGenerationType type,\n+                       uint max_workers,\n+                       size_t max_capacity);\n+  ~ShenandoahGeneration();\n+\n+  bool is_young() const  { return _type == YOUNG; }\n+  bool is_old() const    { return _type == OLD; }\n+  bool is_global() const { return _type == GLOBAL || _type == NON_GEN; }\n+\n+  \/\/ see description in field declaration\n+  void set_evacuation_reserve(size_t new_val);\n+  size_t get_evacuation_reserve() const;\n+  void augment_evacuation_reserve(size_t increment);\n+\n+  inline ShenandoahGenerationType type() const { return _type; }\n+\n+  virtual ShenandoahHeuristics* heuristics() const { return _heuristics; }\n+\n+  ShenandoahReferenceProcessor* ref_processor() { return _ref_processor; }\n+\n+  virtual ShenandoahHeuristics* initialize_heuristics(ShenandoahMode* gc_mode);\n+\n+  size_t max_capacity() const override      { return _max_capacity; }\n+  virtual size_t used_regions() const;\n+  virtual size_t used_regions_size() const;\n+  virtual size_t free_unaffiliated_regions() const;\n+  size_t used() const override { return Atomic::load(&_used); }\n+  size_t available() const override;\n+  size_t available_with_reserve() const;\n+  size_t used_including_humongous_waste() const {\n+    return used() + get_humongous_waste();\n+  }\n+\n+  \/\/ Returns the memory available based on the _soft_ max heap capacity (soft_max_heap - used).\n+  \/\/ The soft max heap size may be adjusted lower than the max heap size to cause the trigger\n+  \/\/ to believe it has less memory available than is _really_ available. Lowering the soft\n+  \/\/ max heap size will cause the adaptive heuristic to run more frequent cycles.\n+  size_t soft_available() const override;\n+\n+  size_t bytes_allocated_since_gc_start() const override;\n+  void reset_bytes_allocated_since_gc_start();\n+  void increase_allocated(size_t bytes);\n+\n+  \/\/ These methods change the capacity of the generation by adding or subtracting the given number of bytes from the current\n+  \/\/ capacity, returning the capacity of the generation following the change.\n+  size_t increase_capacity(size_t increment);\n+  size_t decrease_capacity(size_t decrement);\n+\n+  \/\/ Set the capacity of the generation, returning the value set\n+  size_t set_capacity(size_t byte_size);\n+\n+  void log_status(const char* msg) const;\n+\n+  \/\/ Used directly by FullGC\n+  template <bool FOR_CURRENT_CYCLE, bool FULL_GC = false>\n+  void reset_mark_bitmap();\n+\n+  \/\/ Used by concurrent and degenerated GC to reset remembered set.\n+  void swap_card_tables();\n+\n+  \/\/ Update the read cards with the state of the write table (write table is not cleared).\n+  void merge_write_table();\n+\n+  \/\/ Called before init mark, expected to prepare regions for marking.\n+  virtual void prepare_gc();\n+\n+  \/\/ Called during final mark, chooses collection set, rebuilds free set.\n+  virtual void prepare_regions_and_collection_set(bool concurrent);\n+\n+  \/\/ Cancel marking (used by Full collect and when cancelling cycle).\n+  virtual void cancel_marking();\n+\n+  virtual bool contains(ShenandoahAffiliation affiliation) const = 0;\n+\n+  \/\/ Return true if this region is affiliated with this generation.\n+  virtual bool contains(ShenandoahHeapRegion* region) const = 0;\n+\n+  \/\/ Return true if this object is affiliated with this generation.\n+  virtual bool contains(oop obj) const = 0;\n+\n+  \/\/ Apply closure to all regions affiliated with this generation.\n+  virtual void parallel_heap_region_iterate(ShenandoahHeapRegionClosure* cl) = 0;\n+\n+  \/\/ Apply closure to all regions affiliated with this generation (include free regions);\n+  virtual void parallel_heap_region_iterate_free(ShenandoahHeapRegionClosure* cl);\n+\n+  \/\/ Apply closure to all regions affiliated with this generation (single threaded).\n+  virtual void heap_region_iterate(ShenandoahHeapRegionClosure* cl) = 0;\n+\n+  \/\/ This is public to support cancellation of marking when a Full cycle is started.\n+  virtual void set_concurrent_mark_in_progress(bool in_progress) = 0;\n+\n+  \/\/ Check the bitmap only for regions belong to this generation.\n+  bool is_bitmap_clear();\n+\n+  \/\/ We need to track the status of marking for different generations.\n+  bool is_mark_complete() { return _is_marking_complete.is_set(); }\n+  virtual void set_mark_complete();\n+  virtual void set_mark_incomplete();\n+\n+  ShenandoahMarkingContext* complete_marking_context();\n+\n+  \/\/ Task queues\n+  ShenandoahObjToScanQueueSet* task_queues() const { return _task_queues; }\n+  virtual void reserve_task_queues(uint workers);\n+  virtual ShenandoahObjToScanQueueSet* old_gen_task_queues() const;\n+\n+  \/\/ Scan remembered set at start of concurrent young-gen marking.\n+  void scan_remembered_set(bool is_concurrent);\n+\n+  \/\/ Return the updated value of affiliated_region_count\n+  size_t increment_affiliated_region_count();\n+\n+  \/\/ Return the updated value of affiliated_region_count\n+  size_t decrement_affiliated_region_count();\n+  \/\/ Same as decrement_affiliated_region_count, but w\/o the need to hold heap lock before being called.\n+  size_t decrement_affiliated_region_count_without_lock();\n+\n+  \/\/ Return the updated value of affiliated_region_count\n+  size_t increase_affiliated_region_count(size_t delta);\n+\n+  \/\/ Return the updated value of affiliated_region_count\n+  size_t decrease_affiliated_region_count(size_t delta);\n+\n+  void establish_usage(size_t num_regions, size_t num_bytes, size_t humongous_waste);\n+\n+  void increase_used(size_t bytes);\n+  void decrease_used(size_t bytes);\n+\n+  void increase_humongous_waste(size_t bytes);\n+  void decrease_humongous_waste(size_t bytes);\n+  size_t get_humongous_waste() const { return _humongous_waste; }\n+\n+  virtual bool is_concurrent_mark_in_progress() = 0;\n+  void confirm_heuristics_mode();\n+\n+  virtual void record_success_concurrent(bool abbreviated);\n+};\n+\n+#endif \/\/ SHARE_VM_GC_SHENANDOAH_SHENANDOAHGENERATION_HPP\n","filename":"src\/hotspot\/share\/gc\/shenandoah\/shenandoahGeneration.hpp","additions":244,"deletions":0,"binary":false,"changes":244,"status":"added"},{"patch":"@@ -0,0 +1,209 @@\n+\/*\n+ * Copyright Amazon.com Inc. or its affiliates. All Rights Reserved.\n+ * DO NOT ALTER OR REMOVE COPYRIGHT NOTICES OR THIS FILE HEADER.\n+ *\n+ * This code is free software; you can redistribute it and\/or modify it\n+ * under the terms of the GNU General Public License version 2 only, as\n+ * published by the Free Software Foundation.\n+ *\n+ * This code is distributed in the hope that it will be useful, but WITHOUT\n+ * ANY WARRANTY; without even the implied warranty of MERCHANTABILITY or\n+ * FITNESS FOR A PARTICULAR PURPOSE.  See the GNU General Public License\n+ * version 2 for more details (a copy is included in the LICENSE file that\n+ * accompanied this code).\n+ *\n+ * You should have received a copy of the GNU General Public License version\n+ * 2 along with this work; if not, write to the Free Software Foundation,\n+ * Inc., 51 Franklin St, Fifth Floor, Boston, MA 02110-1301 USA.\n+ *\n+ * Please contact Oracle, 500 Oracle Parkway, Redwood Shores, CA 94065 USA\n+ * or visit www.oracle.com if you need additional information or have any\n+ * questions.\n+ *\n+ *\/\n+\n+#include \"precompiled.hpp\"\n+\n+#include \"gc\/shenandoah\/shenandoahGeneration.hpp\"\n+#include \"gc\/shenandoah\/shenandoahGenerationSizer.hpp\"\n+#include \"gc\/shenandoah\/shenandoahHeap.inline.hpp\"\n+#include \"gc\/shenandoah\/shenandoahHeapRegion.hpp\"\n+#include \"gc\/shenandoah\/shenandoahOldGeneration.hpp\"\n+#include \"gc\/shenandoah\/shenandoahYoungGeneration.hpp\"\n+#include \"gc\/shared\/gc_globals.hpp\"\n+#include \"logging\/log.hpp\"\n+#include \"runtime\/globals_extension.hpp\"\n+\n+\n+ShenandoahGenerationSizer::ShenandoahGenerationSizer()\n+        : _sizer_kind(SizerDefaults),\n+          _min_desired_young_regions(0),\n+          _max_desired_young_regions(0) {\n+\n+  if (FLAG_IS_CMDLINE(NewRatio)) {\n+    if (FLAG_IS_CMDLINE(NewSize) || FLAG_IS_CMDLINE(MaxNewSize)) {\n+      log_warning(gc, ergo)(\"-XX:NewSize and -XX:MaxNewSize override -XX:NewRatio\");\n+    } else {\n+      _sizer_kind = SizerNewRatio;\n+      return;\n+    }\n+  }\n+\n+  if (NewSize > MaxNewSize) {\n+    if (FLAG_IS_CMDLINE(MaxNewSize)) {\n+      log_warning(gc, ergo)(\"NewSize (\" SIZE_FORMAT \"k) is greater than the MaxNewSize (\" SIZE_FORMAT \"k). \"\n+                            \"A new max generation size of \" SIZE_FORMAT \"k will be used.\",\n+              NewSize\/K, MaxNewSize\/K, NewSize\/K);\n+    }\n+    FLAG_SET_ERGO(MaxNewSize, NewSize);\n+  }\n+\n+  if (FLAG_IS_CMDLINE(NewSize)) {\n+    _min_desired_young_regions = MAX2(uint(NewSize \/ ShenandoahHeapRegion::region_size_bytes()), 1U);\n+    if (FLAG_IS_CMDLINE(MaxNewSize)) {\n+      _max_desired_young_regions = MAX2(uint(MaxNewSize \/ ShenandoahHeapRegion::region_size_bytes()), 1U);\n+      _sizer_kind = SizerMaxAndNewSize;\n+    } else {\n+      _sizer_kind = SizerNewSizeOnly;\n+    }\n+  } else if (FLAG_IS_CMDLINE(MaxNewSize)) {\n+    _max_desired_young_regions = MAX2(uint(MaxNewSize \/ ShenandoahHeapRegion::region_size_bytes()), 1U);\n+    _sizer_kind = SizerMaxNewSizeOnly;\n+  }\n+}\n+\n+size_t ShenandoahGenerationSizer::calculate_min_young_regions(size_t heap_region_count) {\n+  size_t min_young_regions = (heap_region_count * ShenandoahMinYoungPercentage) \/ 100;\n+  return MAX2(min_young_regions, (size_t) 1U);\n+}\n+\n+size_t ShenandoahGenerationSizer::calculate_max_young_regions(size_t heap_region_count) {\n+  size_t max_young_regions = (heap_region_count * ShenandoahMaxYoungPercentage) \/ 100;\n+  return MAX2(max_young_regions, (size_t) 1U);\n+}\n+\n+void ShenandoahGenerationSizer::recalculate_min_max_young_length(size_t heap_region_count) {\n+  assert(heap_region_count > 0, \"Heap must be initialized\");\n+\n+  switch (_sizer_kind) {\n+    case SizerDefaults:\n+      _min_desired_young_regions = calculate_min_young_regions(heap_region_count);\n+      _max_desired_young_regions = calculate_max_young_regions(heap_region_count);\n+      break;\n+    case SizerNewSizeOnly:\n+      _max_desired_young_regions = calculate_max_young_regions(heap_region_count);\n+      _max_desired_young_regions = MAX2(_min_desired_young_regions, _max_desired_young_regions);\n+      break;\n+    case SizerMaxNewSizeOnly:\n+      _min_desired_young_regions = calculate_min_young_regions(heap_region_count);\n+      _min_desired_young_regions = MIN2(_min_desired_young_regions, _max_desired_young_regions);\n+      break;\n+    case SizerMaxAndNewSize:\n+      \/\/ Do nothing. Values set on the command line, don't update them at runtime.\n+      break;\n+    case SizerNewRatio:\n+      _min_desired_young_regions = MAX2(uint(heap_region_count \/ (NewRatio + 1)), 1U);\n+      _max_desired_young_regions = _min_desired_young_regions;\n+      break;\n+    default:\n+      ShouldNotReachHere();\n+  }\n+\n+  assert(_min_desired_young_regions <= _max_desired_young_regions, \"Invalid min\/max young gen size values\");\n+}\n+\n+void ShenandoahGenerationSizer::heap_size_changed(size_t heap_size) {\n+  recalculate_min_max_young_length(heap_size \/ ShenandoahHeapRegion::region_size_bytes());\n+}\n+\n+bool ShenandoahGenerationSizer::transfer_regions(ShenandoahGeneration* src, ShenandoahGeneration* dst, size_t regions) const {\n+  const size_t bytes_to_transfer = regions * ShenandoahHeapRegion::region_size_bytes();\n+\n+  if (src->free_unaffiliated_regions() < regions) {\n+    \/\/ Source does not have enough free regions for this transfer. The caller should have\n+    \/\/ already capped the transfer based on available unaffiliated regions.\n+    return false;\n+  }\n+\n+  if (dst->max_capacity() + bytes_to_transfer > max_size_for(dst)) {\n+    \/\/ This transfer would cause the destination generation to grow above its configured maximum size.\n+    return false;\n+  }\n+\n+  if (src->max_capacity() - bytes_to_transfer < min_size_for(src)) {\n+    \/\/ This transfer would cause the source generation to shrink below its configured minimum size.\n+    return false;\n+  }\n+\n+  src->decrease_capacity(bytes_to_transfer);\n+  dst->increase_capacity(bytes_to_transfer);\n+  const size_t new_size = dst->max_capacity();\n+  log_info(gc, ergo)(\"Transfer \" SIZE_FORMAT \" region(s) from %s to %s, yielding increased size: \" PROPERFMT,\n+                     regions, src->name(), dst->name(), PROPERFMTARGS(new_size));\n+  return true;\n+}\n+\n+\n+size_t ShenandoahGenerationSizer::max_size_for(ShenandoahGeneration* generation) const {\n+  switch (generation->type()) {\n+    case YOUNG:\n+      return max_young_size();\n+    case OLD:\n+      \/\/ On the command line, max size of OLD is specified indirectly, by setting a minimum size of young.\n+      \/\/ OLD is what remains within the heap after YOUNG has been sized.\n+      return ShenandoahHeap::heap()->max_capacity() - min_young_size();\n+    default:\n+      ShouldNotReachHere();\n+      return 0;\n+  }\n+}\n+\n+size_t ShenandoahGenerationSizer::min_size_for(ShenandoahGeneration* generation) const {\n+  switch (generation->type()) {\n+    case YOUNG:\n+      return min_young_size();\n+    case OLD:\n+      \/\/ On the command line, min size of OLD is specified indirectly, by setting a maximum size of young.\n+      \/\/ OLD is what remains within the heap after YOUNG has been sized.\n+      return ShenandoahHeap::heap()->max_capacity() - max_young_size();\n+    default:\n+      ShouldNotReachHere();\n+      return 0;\n+  }\n+}\n+\n+\n+\/\/ Returns true iff transfer is successful\n+bool ShenandoahGenerationSizer::transfer_to_old(size_t regions) const {\n+  ShenandoahGenerationalHeap* heap = ShenandoahGenerationalHeap::heap();\n+  return transfer_regions(heap->young_generation(), heap->old_generation(), regions);\n+}\n+\n+\/\/ This is used when promoting humongous or highly utilized regular regions in place.  It is not required in this situation\n+\/\/ that the transferred regions be unaffiliated.\n+void ShenandoahGenerationSizer::force_transfer_to_old(size_t regions) const {\n+  ShenandoahGenerationalHeap* heap = ShenandoahGenerationalHeap::heap();\n+  ShenandoahGeneration* old_gen = heap->old_generation();\n+  ShenandoahGeneration* young_gen = heap->young_generation();\n+  const size_t bytes_to_transfer = regions * ShenandoahHeapRegion::region_size_bytes();\n+\n+  young_gen->decrease_capacity(bytes_to_transfer);\n+  old_gen->increase_capacity(bytes_to_transfer);\n+  const size_t new_size = old_gen->max_capacity();\n+  log_info(gc, ergo)(\"Forcing transfer of \" SIZE_FORMAT \" region(s) from %s to %s, yielding increased size: \" PROPERFMT,\n+                     regions, young_gen->name(), old_gen->name(), PROPERFMTARGS(new_size));\n+}\n+\n+\n+bool ShenandoahGenerationSizer::transfer_to_young(size_t regions) const {\n+  ShenandoahGenerationalHeap* heap = ShenandoahGenerationalHeap::heap();\n+  return transfer_regions(heap->old_generation(), heap->young_generation(), regions);\n+}\n+\n+size_t ShenandoahGenerationSizer::min_young_size() const {\n+  return min_young_regions() * ShenandoahHeapRegion::region_size_bytes();\n+}\n+\n+size_t ShenandoahGenerationSizer::max_young_size() const {\n+  return max_young_regions() * ShenandoahHeapRegion::region_size_bytes();\n+}\n","filename":"src\/hotspot\/share\/gc\/shenandoah\/shenandoahGenerationSizer.cpp","additions":209,"deletions":0,"binary":false,"changes":209,"status":"added"},{"patch":"@@ -0,0 +1,93 @@\n+\/*\n+ * Copyright Amazon.com Inc. or its affiliates. All Rights Reserved.\n+ * DO NOT ALTER OR REMOVE COPYRIGHT NOTICES OR THIS FILE HEADER.\n+ *\n+ * This code is free software; you can redistribute it and\/or modify it\n+ * under the terms of the GNU General Public License version 2 only, as\n+ * published by the Free Software Foundation.\n+ *\n+ * This code is distributed in the hope that it will be useful, but WITHOUT\n+ * ANY WARRANTY; without even the implied warranty of MERCHANTABILITY or\n+ * FITNESS FOR A PARTICULAR PURPOSE.  See the GNU General Public License\n+ * version 2 for more details (a copy is included in the LICENSE file that\n+ * accompanied this code).\n+ *\n+ * You should have received a copy of the GNU General Public License version\n+ * 2 along with this work; if not, write to the Free Software Foundation,\n+ * Inc., 51 Franklin St, Fifth Floor, Boston, MA 02110-1301 USA.\n+ *\n+ * Please contact Oracle, 500 Oracle Parkway, Redwood Shores, CA 94065 USA\n+ * or visit www.oracle.com if you need additional information or have any\n+ * questions.\n+ *\n+ *\/\n+\n+#ifndef SHARE_GC_SHENANDOAH_SHENANDOAHGENERATIONSIZER_HPP\n+#define SHARE_GC_SHENANDOAH_SHENANDOAHGENERATIONSIZER_HPP\n+\n+#include \"utilities\/globalDefinitions.hpp\"\n+\n+class ShenandoahGeneration;\n+class ShenandoahGenerationalHeap;\n+\n+class ShenandoahGenerationSizer {\n+private:\n+  enum SizerKind {\n+    SizerDefaults,\n+    SizerNewSizeOnly,\n+    SizerMaxNewSizeOnly,\n+    SizerMaxAndNewSize,\n+    SizerNewRatio\n+  };\n+  SizerKind _sizer_kind;\n+\n+  size_t _min_desired_young_regions;\n+  size_t _max_desired_young_regions;\n+\n+  static size_t calculate_min_young_regions(size_t heap_region_count);\n+  static size_t calculate_max_young_regions(size_t heap_region_count);\n+\n+  \/\/ Update the given values for minimum and maximum young gen length in regions\n+  \/\/ given the number of heap regions depending on the kind of sizing algorithm.\n+  void recalculate_min_max_young_length(size_t heap_region_count);\n+\n+  \/\/ This will attempt to transfer regions from the `src` generation to `dst` generation.\n+  \/\/ If the transfer would violate the configured minimum size for the source or the configured\n+  \/\/ maximum size of the destination, it will not perform the transfer and will return false.\n+  \/\/ Returns true if the transfer is performed.\n+  bool transfer_regions(ShenandoahGeneration* src, ShenandoahGeneration* dst, size_t regions) const;\n+\n+  \/\/ Return the configured maximum size in bytes for the given generation.\n+  size_t max_size_for(ShenandoahGeneration* generation) const;\n+\n+  \/\/ Return the configured minimum size in bytes for the given generation.\n+  size_t min_size_for(ShenandoahGeneration* generation) const;\n+\n+public:\n+  ShenandoahGenerationSizer();\n+\n+  \/\/ Calculate the maximum length of the young gen given the number of regions\n+  \/\/ depending on the sizing algorithm.\n+  void heap_size_changed(size_t heap_size);\n+\n+  \/\/ Minimum size of young generation in bytes as multiple of region size.\n+  size_t min_young_size() const;\n+  size_t min_young_regions() const {\n+    return _min_desired_young_regions;\n+  }\n+\n+  \/\/ Maximum size of young generation in bytes as multiple of region size.\n+  size_t max_young_size() const;\n+  size_t max_young_regions() const {\n+    return _max_desired_young_regions;\n+  }\n+\n+  \/\/ True if transfer succeeds, else false. See transfer_regions.\n+  bool transfer_to_young(size_t regions) const;\n+  bool transfer_to_old(size_t regions) const;\n+\n+  \/\/ force transfer is used when we promote humongous objects.  May violate min\/max limits on generation sizes\n+  void force_transfer_to_old(size_t regions) const;\n+};\n+\n+#endif \/\/SHARE_GC_SHENANDOAH_SHENANDOAHGENERATIONSIZER_HPP\n","filename":"src\/hotspot\/share\/gc\/shenandoah\/shenandoahGenerationSizer.hpp","additions":93,"deletions":0,"binary":false,"changes":93,"status":"added"},{"patch":"@@ -0,0 +1,51 @@\n+\/*\n+ * Copyright Amazon.com Inc. or its affiliates. All Rights Reserved.\n+ * DO NOT ALTER OR REMOVE COPYRIGHT NOTICES OR THIS FILE HEADER.\n+ *\n+ * This code is free software; you can redistribute it and\/or modify it\n+ * under the terms of the GNU General Public License version 2 only, as\n+ * published by the Free Software Foundation.\n+ *\n+ * This code is distributed in the hope that it will be useful, but WITHOUT\n+ * ANY WARRANTY; without even the implied warranty of MERCHANTABILITY or\n+ * FITNESS FOR A PARTICULAR PURPOSE.  See the GNU General Public License\n+ * version 2 for more details (a copy is included in the LICENSE file that\n+ * accompanied this code).\n+ *\n+ * You should have received a copy of the GNU General Public License version\n+ * 2 along with this work; if not, write to the Free Software Foundation,\n+ * Inc., 51 Franklin St, Fifth Floor, Boston, MA 02110-1301 USA.\n+ *\n+ * Please contact Oracle, 500 Oracle Parkway, Redwood Shores, CA 94065 USA\n+ * or visit www.oracle.com if you need additional information or have any\n+ * questions.\n+ *\n+ *\/\n+\n+#ifndef SHARE_GC_SHENANDOAH_SHENANDOAHGENERATIONTYPE_HPP\n+#define SHARE_GC_SHENANDOAH_SHENANDOAHGENERATIONTYPE_HPP\n+\n+enum ShenandoahGenerationType {\n+    NON_GEN,         \/\/ non-generational\n+    GLOBAL,          \/\/ generational: Global\n+    YOUNG,           \/\/ generational: Young\n+    OLD              \/\/ generational: Old\n+};\n+\n+inline const char* shenandoah_generation_name(ShenandoahGenerationType mode) {\n+  switch (mode) {\n+    case NON_GEN:\n+      return \"Non-Generational\";\n+    case GLOBAL:\n+      return \"Global\";\n+    case OLD:\n+      return \"Old\";\n+    case YOUNG:\n+      return \"Young\";\n+    default:\n+      ShouldNotReachHere();\n+      return \"Unknown\";\n+  }\n+}\n+\n+#endif \/\/ SHARE_GC_SHENANDOAH_SHENANDOAHGENERATIONTYPE_HPP\n","filename":"src\/hotspot\/share\/gc\/shenandoah\/shenandoahGenerationType.hpp","additions":51,"deletions":0,"binary":false,"changes":51,"status":"added"},{"patch":"@@ -0,0 +1,795 @@\n+\/*\n+ * Copyright (c) 2013, 2021, Red Hat, Inc. All rights reserved.\n+ * Copyright (C) 2022 THL A29 Limited, a Tencent company. All rights reserved.\n+ * Copyright Amazon.com Inc. or its affiliates. All Rights Reserved.\n+ * DO NOT ALTER OR REMOVE COPYRIGHT NOTICES OR THIS FILE HEADER.\n+ *\n+ * This code is free software; you can redistribute it and\/or modify it\n+ * under the terms of the GNU General Public License version 2 only, as\n+ * published by the Free Software Foundation.\n+ *\n+ * This code is distributed in the hope that it will be useful, but WITHOUT\n+ * ANY WARRANTY; without even the implied warranty of MERCHANTABILITY or\n+ * FITNESS FOR A PARTICULAR PURPOSE.  See the GNU General Public License\n+ * version 2 for more details (a copy is included in the LICENSE file that\n+ * accompanied this code).\n+ *\n+ * You should have received a copy of the GNU General Public License version\n+ * 2 along with this work; if not, write to the Free Software Foundation,\n+ * Inc., 51 Franklin St, Fifth Floor, Boston, MA 02110-1301 USA.\n+ *\n+ * Please contact Oracle, 500 Oracle Parkway, Redwood Shores, CA 94065 USA\n+ * or visit www.oracle.com if you need additional information or have any\n+ * questions.\n+ *\n+ *\/\n+\n+#include \"precompiled.hpp\"\n+#include \"gc\/shenandoah\/shenandoahAsserts.hpp\"\n+#include \"gc\/shenandoah\/shenandoahCollectorPolicy.hpp\"\n+#include \"gc\/shenandoah\/shenandoahConcurrentGC.hpp\"\n+#include \"gc\/shenandoah\/shenandoahGenerationalControlThread.hpp\"\n+#include \"gc\/shenandoah\/shenandoahDegeneratedGC.hpp\"\n+#include \"gc\/shenandoah\/shenandoahFreeSet.hpp\"\n+#include \"gc\/shenandoah\/shenandoahFullGC.hpp\"\n+#include \"gc\/shenandoah\/shenandoahGeneration.hpp\"\n+#include \"gc\/shenandoah\/shenandoahGenerationalHeap.hpp\"\n+#include \"gc\/shenandoah\/shenandoahOldGC.hpp\"\n+#include \"gc\/shenandoah\/shenandoahOldGeneration.hpp\"\n+#include \"gc\/shenandoah\/shenandoahHeap.inline.hpp\"\n+#include \"gc\/shenandoah\/shenandoahMonitoringSupport.hpp\"\n+#include \"gc\/shenandoah\/shenandoahPacer.inline.hpp\"\n+#include \"gc\/shenandoah\/shenandoahUtils.hpp\"\n+#include \"gc\/shenandoah\/shenandoahYoungGeneration.hpp\"\n+#include \"logging\/log.hpp\"\n+#include \"memory\/metaspaceUtils.hpp\"\n+#include \"memory\/metaspaceStats.hpp\"\n+#include \"runtime\/atomic.hpp\"\n+#include \"utilities\/events.hpp\"\n+\n+ShenandoahGenerationalControlThread::ShenandoahGenerationalControlThread() :\n+  _control_lock(Mutex::nosafepoint - 2, \"ShenandoahGCRequest_lock\", true),\n+  _requested_gc_cause(GCCause::_no_gc),\n+  _requested_generation(nullptr),\n+  _gc_mode(none),\n+  _degen_point(ShenandoahGC::_degenerated_unset),\n+  _heap(ShenandoahGenerationalHeap::heap()),\n+  _age_period(0) {\n+  shenandoah_assert_generational();\n+  set_name(\"Shenandoah Control Thread\");\n+  create_and_start();\n+}\n+\n+void ShenandoahGenerationalControlThread::run_service() {\n+\n+  const int64_t wait_ms = ShenandoahPacing ? ShenandoahControlIntervalMin : 0;\n+  ShenandoahGCRequest request;\n+  while (!should_terminate()) {\n+\n+    \/\/ This control loop iteration has seen this much allocation.\n+    const size_t allocs_seen = reset_allocs_seen();\n+\n+    \/\/ Figure out if we have pending requests.\n+    check_for_request(request);\n+\n+    if (request.cause == GCCause::_shenandoah_stop_vm) {\n+      break;\n+    }\n+\n+    if (request.cause != GCCause::_no_gc) {\n+      run_gc_cycle(request);\n+    } else {\n+      \/\/ Report to pacer that we have seen this many words allocated\n+      if (ShenandoahPacing && (allocs_seen > 0)) {\n+        _heap->pacer()->report_alloc(allocs_seen);\n+      }\n+    }\n+\n+    \/\/ If the cycle was cancelled, continue the next iteration to deal with it. Otherwise,\n+    \/\/ if there was no other cycle requested, cleanup and wait for the next request.\n+    if (!_heap->cancelled_gc()) {\n+      MonitorLocker ml(&_control_lock, Mutex::_no_safepoint_check_flag);\n+      if (_requested_gc_cause == GCCause::_no_gc) {\n+        set_gc_mode(ml, none);\n+        ml.wait(wait_ms);\n+      }\n+    }\n+  }\n+\n+  \/\/ In case any threads are waiting for a cycle to happen, notify them so they observe the shutdown.\n+  notify_gc_waiters();\n+  notify_alloc_failure_waiters();\n+  set_gc_mode(stopped);\n+}\n+\n+void ShenandoahGenerationalControlThread::stop_service() {\n+  log_debug(gc, thread)(\"Stopping control thread\");\n+  MonitorLocker ml(&_control_lock, Mutex::_no_safepoint_check_flag);\n+  _heap->cancel_gc(GCCause::_shenandoah_stop_vm);\n+  _requested_gc_cause = GCCause::_shenandoah_stop_vm;\n+  notify_cancellation(ml, GCCause::_shenandoah_stop_vm);\n+  \/\/ We can't wait here because it may interfere with the active cycle's ability\n+  \/\/ to reach a safepoint (this runs on a java thread).\n+}\n+\n+void ShenandoahGenerationalControlThread::check_for_request(ShenandoahGCRequest& request) {\n+  \/\/ Hold the lock while we read request cause and generation\n+  MonitorLocker ml(&_control_lock, Mutex::_no_safepoint_check_flag);\n+  if (_heap->cancelled_gc()) {\n+    \/\/ The previous request was cancelled. Either it was cancelled for an allocation\n+    \/\/ failure (degenerated cycle), or old marking was cancelled to run a young collection.\n+    \/\/ In either case, the correct generation for the next cycle can be determined by\n+    \/\/ the cancellation cause.\n+    request.cause = _heap->cancelled_cause();\n+    if (request.cause == GCCause::_shenandoah_concurrent_gc) {\n+      request.generation = _heap->young_generation();\n+      _heap->clear_cancelled_gc(false);\n+    }\n+  } else {\n+    request.cause = _requested_gc_cause;\n+    request.generation = _requested_generation;\n+\n+    \/\/ Only clear these if we made a request from them. In the case of a cancelled gc,\n+    \/\/ we do not want to inadvertently lose this pending request.\n+    _requested_gc_cause = GCCause::_no_gc;\n+    _requested_generation = nullptr;\n+  }\n+\n+  if (request.cause == GCCause::_no_gc || request.cause == GCCause::_shenandoah_stop_vm) {\n+    return;\n+  }\n+\n+  GCMode mode;\n+  if (ShenandoahCollectorPolicy::is_allocation_failure(request.cause)) {\n+    mode = prepare_for_allocation_failure_gc(request);\n+  } else if (ShenandoahCollectorPolicy::is_explicit_gc(request.cause)) {\n+    mode = prepare_for_explicit_gc(request);\n+  } else {\n+    mode = prepare_for_concurrent_gc(request);\n+  }\n+  set_gc_mode(ml, mode);\n+}\n+\n+ShenandoahGenerationalControlThread::GCMode ShenandoahGenerationalControlThread::prepare_for_allocation_failure_gc(ShenandoahGCRequest &request) {\n+\n+  if (_degen_point == ShenandoahGC::_degenerated_unset) {\n+    _degen_point = ShenandoahGC::_degenerated_outside_cycle;\n+    request.generation = _heap->young_generation();\n+  } else if (request.generation->is_old()) {\n+    \/\/ This means we degenerated during the young bootstrap for the old generation\n+    \/\/ cycle. The following degenerated cycle should therefore also be young.\n+    request.generation = _heap->young_generation();\n+  }\n+\n+  ShenandoahHeuristics* heuristics = request.generation->heuristics();\n+  bool old_gen_evacuation_failed = _heap->old_generation()->clear_failed_evacuation();\n+\n+  heuristics->log_trigger(\"Handle Allocation Failure\");\n+\n+  \/\/ Do not bother with degenerated cycle if old generation evacuation failed or if humongous allocation failed\n+  if (ShenandoahDegeneratedGC && heuristics->should_degenerate_cycle() &&\n+      !old_gen_evacuation_failed && request.cause != GCCause::_shenandoah_humongous_allocation_failure) {\n+    heuristics->record_allocation_failure_gc();\n+    _heap->shenandoah_policy()->record_alloc_failure_to_degenerated(_degen_point);\n+    return stw_degenerated;\n+  } else {\n+    heuristics->record_allocation_failure_gc();\n+    _heap->shenandoah_policy()->record_alloc_failure_to_full();\n+    request.generation = _heap->global_generation();\n+    return stw_full;\n+  }\n+}\n+\n+ShenandoahGenerationalControlThread::GCMode ShenandoahGenerationalControlThread::prepare_for_explicit_gc(ShenandoahGCRequest &request) const {\n+  ShenandoahHeuristics* global_heuristics = _heap->global_generation()->heuristics();\n+  request.generation = _heap->global_generation();\n+  global_heuristics->log_trigger(\"GC request (%s)\", GCCause::to_string(request.cause));\n+  global_heuristics->record_requested_gc();\n+\n+  if (ShenandoahCollectorPolicy::should_run_full_gc(request.cause)) {\n+    return stw_full;;\n+  } else {\n+    \/\/ Unload and clean up everything. Note that this is an _explicit_ request and so does not use\n+    \/\/ the same `should_unload_classes` call as the regulator's concurrent gc request.\n+    _heap->set_unload_classes(global_heuristics->can_unload_classes());\n+    return concurrent_normal;\n+  }\n+}\n+\n+ShenandoahGenerationalControlThread::GCMode ShenandoahGenerationalControlThread::prepare_for_concurrent_gc(const ShenandoahGCRequest &request) const {\n+  assert(!(request.generation->is_old() && _heap->old_generation()->is_doing_mixed_evacuations()),\n+             \"Old heuristic should not request cycles while it waits for mixed evacuations\");\n+\n+  if (request.generation->is_global()) {\n+    ShenandoahHeuristics* global_heuristics = _heap->global_generation()->heuristics();\n+    _heap->set_unload_classes(global_heuristics->should_unload_classes());\n+  } else {\n+    _heap->set_unload_classes(false);\n+  }\n+\n+  \/\/ preemption was requested or this is a regular cycle\n+  return request.generation->is_old() ? servicing_old : concurrent_normal;\n+}\n+\n+void ShenandoahGenerationalControlThread::maybe_set_aging_cycle() {\n+  if (_age_period-- == 0) {\n+    _heap->set_aging_cycle(true);\n+    _age_period = ShenandoahAgingCyclePeriod - 1;\n+  } else {\n+    _heap->set_aging_cycle(false);\n+  }\n+}\n+\n+void ShenandoahGenerationalControlThread::run_gc_cycle(const ShenandoahGCRequest& request) {\n+\n+  log_debug(gc, thread)(\"Starting GC (%s): %s, %s\", gc_mode_name(gc_mode()), GCCause::to_string(request.cause), request.generation->name());\n+  assert(gc_mode() != none, \"GC mode cannot be none here\");\n+\n+  \/\/ Blow away all soft references on this cycle, if handling allocation failure,\n+  \/\/ either implicit or explicit GC request, or we are requested to do so unconditionally.\n+  if (request.generation->is_global() && (ShenandoahCollectorPolicy::is_allocation_failure(request.cause) || ShenandoahCollectorPolicy::is_explicit_gc(request.cause) || ShenandoahAlwaysClearSoftRefs)) {\n+    _heap->soft_ref_policy()->set_should_clear_all_soft_refs(true);\n+  }\n+\n+  \/\/ GC is starting, bump the internal ID\n+  update_gc_id();\n+\n+  GCIdMark gc_id_mark;\n+\n+  _heap->reset_bytes_allocated_since_gc_start();\n+\n+  MetaspaceCombinedStats meta_sizes = MetaspaceUtils::get_combined_statistics();\n+\n+  \/\/ If GC was requested, we are sampling the counters even without actual triggers\n+  \/\/ from allocation machinery. This captures GC phases more accurately.\n+  _heap->set_forced_counters_update(true);\n+\n+  \/\/ If GC was requested, we better dump freeset data for performance debugging\n+  _heap->free_set()->log_status_under_lock();\n+\n+  {\n+    \/\/ Cannot uncommit bitmap slices during concurrent reset\n+    ShenandoahNoUncommitMark forbid_region_uncommit(_heap);\n+\n+    switch (gc_mode()) {\n+      case concurrent_normal: {\n+        service_concurrent_normal_cycle(request);\n+        break;\n+      }\n+      case stw_degenerated: {\n+        service_stw_degenerated_cycle(request);\n+        break;\n+      }\n+      case stw_full: {\n+        service_stw_full_cycle(request.cause);\n+        break;\n+      }\n+      case servicing_old: {\n+        assert(request.generation->is_old(), \"Expected old generation here\");\n+        service_concurrent_old_cycle(request);\n+        break;\n+      }\n+      default:\n+        ShouldNotReachHere();\n+    }\n+  }\n+\n+  \/\/ If this cycle completed successfully, notify threads waiting for gc\n+  if (!_heap->cancelled_gc()) {\n+    notify_gc_waiters();\n+    notify_alloc_failure_waiters();\n+  }\n+\n+  \/\/ Report current free set state at the end of cycle, whether\n+  \/\/ it is a normal completion, or the abort.\n+  _heap->free_set()->log_status_under_lock();\n+\n+  \/\/ Notify Universe about new heap usage. This has implications for\n+  \/\/ global soft refs policy, and we better report it every time heap\n+  \/\/ usage goes down.\n+  _heap->update_capacity_and_used_at_gc();\n+\n+  \/\/ Signal that we have completed a visit to all live objects.\n+  _heap->record_whole_heap_examined_timestamp();\n+\n+  \/\/ Disable forced counters update, and update counters one more time\n+  \/\/ to capture the state at the end of GC session.\n+  _heap->handle_force_counters_update();\n+  _heap->set_forced_counters_update(false);\n+\n+  \/\/ Retract forceful part of soft refs policy\n+  _heap->soft_ref_policy()->set_should_clear_all_soft_refs(false);\n+\n+  \/\/ Clear metaspace oom flag, if current cycle unloaded classes\n+  if (_heap->unload_classes()) {\n+    _heap->global_generation()->heuristics()->clear_metaspace_oom();\n+  }\n+\n+  process_phase_timings();\n+\n+  \/\/ Print Metaspace change following GC (if logging is enabled).\n+  MetaspaceUtils::print_metaspace_change(meta_sizes);\n+\n+  \/\/ GC is over, we are at idle now\n+  if (ShenandoahPacing) {\n+    _heap->pacer()->setup_for_idle();\n+  }\n+\n+  \/\/ Check if we have seen a new target for soft max heap size or if a gc was requested.\n+  \/\/ Either of these conditions will attempt to uncommit regions.\n+  if (ShenandoahUncommit) {\n+    if (_heap->check_soft_max_changed()) {\n+      _heap->notify_soft_max_changed();\n+    } else if (ShenandoahCollectorPolicy::is_explicit_gc(request.cause)) {\n+      _heap->notify_explicit_gc_requested();\n+    }\n+  }\n+\n+  log_debug(gc, thread)(\"Completed GC (%s): %s, %s, cancelled: %s\",\n+    gc_mode_name(gc_mode()), GCCause::to_string(request.cause), request.generation->name(), GCCause::to_string(_heap->cancelled_cause()));\n+}\n+\n+void ShenandoahGenerationalControlThread::process_phase_timings() const {\n+  \/\/ Commit worker statistics to cycle data\n+  _heap->phase_timings()->flush_par_workers_to_cycle();\n+  if (ShenandoahPacing) {\n+    _heap->pacer()->flush_stats_to_cycle();\n+  }\n+\n+  ShenandoahEvacuationTracker* evac_tracker = _heap->evac_tracker();\n+  ShenandoahCycleStats         evac_stats   = evac_tracker->flush_cycle_to_global();\n+\n+  \/\/ Print GC stats for current cycle\n+  {\n+    LogTarget(Info, gc, stats) lt;\n+    if (lt.is_enabled()) {\n+      ResourceMark rm;\n+      LogStream ls(lt);\n+      _heap->phase_timings()->print_cycle_on(&ls);\n+      evac_tracker->print_evacuations_on(&ls, &evac_stats.workers,\n+                                              &evac_stats.mutators);\n+      if (ShenandoahPacing) {\n+        _heap->pacer()->print_cycle_on(&ls);\n+      }\n+    }\n+  }\n+\n+  \/\/ Commit statistics to globals\n+  _heap->phase_timings()->flush_cycle_to_global();\n+}\n+\n+\/\/ Young and old concurrent cycles are initiated by the regulator. Implicit\n+\/\/ and explicit GC requests are handled by the controller thread and always\n+\/\/ run a global cycle (which is concurrent by default, but may be overridden\n+\/\/ by command line options). Old cycles always degenerate to a global cycle.\n+\/\/ Young cycles are degenerated to complete the young cycle.  Young\n+\/\/ and old degen may upgrade to Full GC.  Full GC may also be\n+\/\/ triggered directly by a System.gc() invocation.\n+\/\/\n+\/\/\n+\/\/      +-----+ Idle +-----+-----------+---------------------+\n+\/\/      |         +        |           |                     |\n+\/\/      |         |        |           |                     |\n+\/\/      |         |        v           |                     |\n+\/\/      |         |  Bootstrap Old +-- | ------------+       |\n+\/\/      |         |   +                |             |       |\n+\/\/      |         |   |                |             |       |\n+\/\/      |         v   v                v             v       |\n+\/\/      |    Resume Old <----------+ Young +--> Young Degen  |\n+\/\/      |     +  +   ^                            +  +       |\n+\/\/      v     |  |   |                            |  |       |\n+\/\/   Global <-+  |   +----------------------------+  |       |\n+\/\/      +        |                                   |       |\n+\/\/      |        v                                   v       |\n+\/\/      +--->  Global Degen +--------------------> Full <----+\n+\/\/\n+void ShenandoahGenerationalControlThread::service_concurrent_normal_cycle(const ShenandoahGCRequest& request) {\n+  log_info(gc, ergo)(\"Start GC cycle (%s)\", request.generation->name());\n+  if (request.generation->is_old()) {\n+    service_concurrent_old_cycle(request);\n+  } else {\n+    service_concurrent_cycle(request.generation, request.cause, false);\n+  }\n+}\n+\n+void ShenandoahGenerationalControlThread::service_concurrent_old_cycle(const ShenandoahGCRequest& request) {\n+  ShenandoahOldGeneration* old_generation = _heap->old_generation();\n+  ShenandoahYoungGeneration* young_generation = _heap->young_generation();\n+  ShenandoahOldGeneration::State original_state = old_generation->state();\n+\n+  TraceCollectorStats tcs(_heap->monitoring_support()->concurrent_collection_counters());\n+\n+  switch (original_state) {\n+    case ShenandoahOldGeneration::FILLING: {\n+      ShenandoahGCSession session(request.cause, old_generation);\n+      assert(gc_mode() == servicing_old, \"Filling should be servicing old\");\n+      _allow_old_preemption.set();\n+      old_generation->entry_coalesce_and_fill();\n+      _allow_old_preemption.unset();\n+\n+      \/\/ Before bootstrapping begins, we must acknowledge any cancellation request.\n+      \/\/ If the gc has not been cancelled, this does nothing. If it has been cancelled,\n+      \/\/ this will clear the cancellation request and exit before starting the bootstrap\n+      \/\/ phase. This will allow the young GC cycle to proceed normally. If we do not\n+      \/\/ acknowledge the cancellation request, the subsequent young cycle will observe\n+      \/\/ the request and essentially cancel itself.\n+      if (check_cancellation_or_degen(ShenandoahGC::_degenerated_outside_cycle)) {\n+        log_info(gc, thread)(\"Preparation for old generation cycle was cancelled\");\n+        return;\n+      }\n+\n+      \/\/ Coalescing threads completed and nothing was cancelled. it is safe to transition from this state.\n+      old_generation->transition_to(ShenandoahOldGeneration::WAITING_FOR_BOOTSTRAP);\n+      return;\n+    }\n+    case ShenandoahOldGeneration::WAITING_FOR_BOOTSTRAP:\n+      old_generation->transition_to(ShenandoahOldGeneration::BOOTSTRAPPING);\n+    case ShenandoahOldGeneration::BOOTSTRAPPING: {\n+      \/\/ Configure the young generation's concurrent mark to put objects in\n+      \/\/ old regions into the concurrent mark queues associated with the old\n+      \/\/ generation. The young cycle will run as normal except that rather than\n+      \/\/ ignore old references it will mark and enqueue them in the old concurrent\n+      \/\/ task queues but it will not traverse them.\n+      set_gc_mode(bootstrapping_old);\n+      young_generation->set_old_gen_task_queues(old_generation->task_queues());\n+      service_concurrent_cycle(young_generation, request.cause, true);\n+      process_phase_timings();\n+      if (_heap->cancelled_gc()) {\n+        \/\/ Young generation bootstrap cycle has failed. Concurrent mark for old generation\n+        \/\/ is going to resume after degenerated bootstrap cycle completes.\n+        log_info(gc)(\"Bootstrap cycle for old generation was cancelled\");\n+        return;\n+      }\n+\n+      assert(_degen_point == ShenandoahGC::_degenerated_unset, \"Degen point should not be set if gc wasn't cancelled\");\n+\n+      \/\/ From here we will 'resume' the old concurrent mark. This will skip reset\n+      \/\/ and init mark for the concurrent mark. All of that work will have been\n+      \/\/ done by the bootstrapping young cycle.\n+      set_gc_mode(servicing_old);\n+      old_generation->transition_to(ShenandoahOldGeneration::MARKING);\n+    }\n+    case ShenandoahOldGeneration::MARKING: {\n+      ShenandoahGCSession session(request.cause, old_generation);\n+      bool marking_complete = resume_concurrent_old_cycle(old_generation, request.cause);\n+      if (marking_complete) {\n+        assert(old_generation->state() != ShenandoahOldGeneration::MARKING, \"Should not still be marking\");\n+        if (original_state == ShenandoahOldGeneration::MARKING) {\n+          _heap->mmu_tracker()->record_old_marking_increment(true);\n+          _heap->log_heap_status(\"At end of Concurrent Old Marking finishing increment\");\n+        }\n+      } else if (original_state == ShenandoahOldGeneration::MARKING) {\n+        _heap->mmu_tracker()->record_old_marking_increment(false);\n+        _heap->log_heap_status(\"At end of Concurrent Old Marking increment\");\n+      }\n+      break;\n+    }\n+    default:\n+      fatal(\"Unexpected state for old GC: %s\", ShenandoahOldGeneration::state_name(old_generation->state()));\n+  }\n+}\n+\n+bool ShenandoahGenerationalControlThread::resume_concurrent_old_cycle(ShenandoahOldGeneration* generation, GCCause::Cause cause) {\n+  assert(_heap->is_concurrent_old_mark_in_progress(), \"Old mark should be in progress\");\n+  log_debug(gc)(\"Resuming old generation with \" UINT32_FORMAT \" marking tasks queued\", generation->task_queues()->tasks());\n+\n+  \/\/ We can only tolerate being cancelled during concurrent marking or during preparation for mixed\n+  \/\/ evacuation. This flag here (passed by reference) is used to control precisely where the regulator\n+  \/\/ is allowed to cancel a GC.\n+  ShenandoahOldGC gc(generation, _allow_old_preemption);\n+  if (gc.collect(cause)) {\n+    _heap->notify_gc_progress();\n+    generation->record_success_concurrent(false);\n+  }\n+\n+  if (_heap->cancelled_gc()) {\n+    \/\/ It's possible the gc cycle was cancelled after the last time\n+    \/\/ the collection checked for cancellation. In which case, the\n+    \/\/ old gc cycle is still completed, and we have to deal with this\n+    \/\/ cancellation. We set the degeneration point to be outside\n+    \/\/ the cycle because if this is an allocation failure, that is\n+    \/\/ what must be done (there is no degenerated old cycle). If the\n+    \/\/ cancellation was due to a heuristic wanting to start a young\n+    \/\/ cycle, then we are not actually going to a degenerated cycle,\n+    \/\/ so the degenerated point doesn't matter here.\n+    check_cancellation_or_degen(ShenandoahGC::_degenerated_outside_cycle);\n+    if (cause == GCCause::_shenandoah_concurrent_gc) {\n+      _heap->shenandoah_policy()->record_interrupted_old();\n+    }\n+    return false;\n+  }\n+  return true;\n+}\n+\n+\/\/ Normal cycle goes via all concurrent phases. If allocation failure (af) happens during\n+\/\/ any of the concurrent phases, it first degrades to Degenerated GC and completes GC there.\n+\/\/ If second allocation failure happens during Degenerated GC cycle (for example, when GC\n+\/\/ tries to evac something and no memory is available), cycle degrades to Full GC.\n+\/\/\n+\/\/ There are also a shortcut through the normal cycle: immediate garbage shortcut, when\n+\/\/ heuristics says there are no regions to compact, and all the collection comes from immediately\n+\/\/ reclaimable regions.\n+\/\/\n+\/\/ ................................................................................................\n+\/\/\n+\/\/                                    (immediate garbage shortcut)                Concurrent GC\n+\/\/                             \/-------------------------------------------\\\n+\/\/                             |                                           |\n+\/\/                             |                                           |\n+\/\/                             |                                           |\n+\/\/                             |                                           v\n+\/\/ [START] ----> Conc Mark ----o----> Conc Evac --o--> Conc Update-Refs ---o----> [END]\n+\/\/                   |                    |                 |              ^\n+\/\/                   | (af)               | (af)            | (af)         |\n+\/\/ ..................|....................|.................|..............|.......................\n+\/\/                   |                    |                 |              |\n+\/\/                   |                    |                 |              |      Degenerated GC\n+\/\/                   v                    v                 v              |\n+\/\/               STW Mark ----------> STW Evac ----> STW Update-Refs ----->o\n+\/\/                   |                    |                 |              ^\n+\/\/                   | (af)               | (af)            | (af)         |\n+\/\/ ..................|....................|.................|..............|.......................\n+\/\/                   |                    |                 |              |\n+\/\/                   |                    v                 |              |      Full GC\n+\/\/                   \\------------------->o<----------------\/              |\n+\/\/                                        |                                |\n+\/\/                                        v                                |\n+\/\/                                      Full GC  --------------------------\/\n+\/\/\n+void ShenandoahGenerationalControlThread::service_concurrent_cycle(ShenandoahGeneration* generation,\n+                                                                   GCCause::Cause cause,\n+                                                                   bool do_old_gc_bootstrap) {\n+  \/\/ At this point:\n+  \/\/  if (generation == YOUNG), this is a normal young cycle or a bootstrap cycle\n+  \/\/  if (generation == GLOBAL), this is a GLOBAL cycle\n+  \/\/ In either case, we want to age old objects if this is an aging cycle\n+  maybe_set_aging_cycle();\n+\n+  ShenandoahGCSession session(cause, generation);\n+  TraceCollectorStats tcs(_heap->monitoring_support()->concurrent_collection_counters());\n+\n+  assert(!generation->is_old(), \"Old GC takes a different control path\");\n+\n+  ShenandoahConcurrentGC gc(generation, do_old_gc_bootstrap);\n+  if (gc.collect(cause)) {\n+    \/\/ Cycle is complete\n+    _heap->notify_gc_progress();\n+    generation->record_success_concurrent(gc.abbreviated());\n+  } else {\n+    assert(_heap->cancelled_gc(), \"Must have been cancelled\");\n+    check_cancellation_or_degen(gc.degen_point());\n+  }\n+\n+  const char* msg;\n+  ShenandoahMmuTracker* mmu_tracker = _heap->mmu_tracker();\n+  if (generation->is_young()) {\n+    if (_heap->cancelled_gc()) {\n+      msg = (do_old_gc_bootstrap) ? \"At end of Interrupted Concurrent Bootstrap GC\" :\n+            \"At end of Interrupted Concurrent Young GC\";\n+    } else {\n+      \/\/ We only record GC results if GC was successful\n+      msg = (do_old_gc_bootstrap) ? \"At end of Concurrent Bootstrap GC\" :\n+            \"At end of Concurrent Young GC\";\n+      if (_heap->collection_set()->has_old_regions()) {\n+        mmu_tracker->record_mixed(get_gc_id());\n+      } else if (do_old_gc_bootstrap) {\n+        mmu_tracker->record_bootstrap(get_gc_id());\n+      } else {\n+        mmu_tracker->record_young(get_gc_id());\n+      }\n+    }\n+  } else {\n+    assert(generation->is_global(), \"If not young, must be GLOBAL\");\n+    assert(!do_old_gc_bootstrap, \"Do not bootstrap with GLOBAL GC\");\n+    if (_heap->cancelled_gc()) {\n+      msg = \"At end of Interrupted Concurrent GLOBAL GC\";\n+    } else {\n+      \/\/ We only record GC results if GC was successful\n+      msg = \"At end of Concurrent Global GC\";\n+      mmu_tracker->record_global(get_gc_id());\n+    }\n+  }\n+  _heap->log_heap_status(msg);\n+}\n+\n+bool ShenandoahGenerationalControlThread::check_cancellation_or_degen(ShenandoahGC::ShenandoahDegenPoint point) {\n+  if (!_heap->cancelled_gc()) {\n+    return false;\n+  }\n+\n+  if (_heap->cancelled_cause() == GCCause::_shenandoah_stop_vm\n+    || _heap->cancelled_cause() == GCCause::_shenandoah_concurrent_gc) {\n+    log_debug(gc, thread)(\"Cancellation detected, reason: %s\", GCCause::to_string(_heap->cancelled_cause()));\n+    return true;\n+  }\n+\n+  if (ShenandoahCollectorPolicy::is_allocation_failure(_heap->cancelled_cause())) {\n+    assert(_degen_point == ShenandoahGC::_degenerated_unset,\n+           \"Should not be set yet: %s\", ShenandoahGC::degen_point_to_string(_degen_point));\n+    _degen_point = point;\n+    log_debug(gc, thread)(\"Cancellation detected:, reason: %s, degen point: %s\",\n+                          GCCause::to_string(_heap->cancelled_cause()),\n+                          ShenandoahGC::degen_point_to_string(_degen_point));\n+    return true;\n+  }\n+\n+  fatal(\"Cancel GC either for alloc failure GC, or gracefully exiting, or to pause old generation marking\");\n+  return false;\n+}\n+\n+void ShenandoahGenerationalControlThread::service_stw_full_cycle(GCCause::Cause cause) {\n+  ShenandoahGCSession session(cause, _heap->global_generation());\n+  maybe_set_aging_cycle();\n+  ShenandoahFullGC gc;\n+  gc.collect(cause);\n+  _degen_point = ShenandoahGC::_degenerated_unset;\n+}\n+\n+void ShenandoahGenerationalControlThread::service_stw_degenerated_cycle(const ShenandoahGCRequest& request) {\n+  assert(_degen_point != ShenandoahGC::_degenerated_unset, \"Degenerated point should be set\");\n+\n+  ShenandoahGCSession session(request.cause, request.generation);\n+\n+  ShenandoahDegenGC gc(_degen_point, request.generation);\n+  gc.collect(request.cause);\n+  _degen_point = ShenandoahGC::_degenerated_unset;\n+\n+  assert(_heap->young_generation()->task_queues()->is_empty(), \"Unexpected young generation marking tasks\");\n+  if (request.generation->is_global()) {\n+    assert(_heap->old_generation()->task_queues()->is_empty(), \"Unexpected old generation marking tasks\");\n+    assert(_heap->global_generation()->task_queues()->is_empty(), \"Unexpected global generation marking tasks\");\n+  } else {\n+    assert(request.generation->is_young(), \"Expected degenerated young cycle, if not global.\");\n+    ShenandoahOldGeneration* old = _heap->old_generation();\n+    if (old->is_bootstrapping()) {\n+      old->transition_to(ShenandoahOldGeneration::MARKING);\n+    }\n+  }\n+}\n+\n+void ShenandoahGenerationalControlThread::request_gc(GCCause::Cause cause) {\n+  if (ShenandoahCollectorPolicy::is_allocation_failure(cause)) {\n+    \/\/ GC should already be cancelled. Here we are just notifying the control thread to\n+    \/\/ wake up and handle the cancellation request, so we don't need to set _requested_gc_cause.\n+    notify_cancellation(cause);\n+  } else if (ShenandoahCollectorPolicy::should_handle_requested_gc(cause)) {\n+    handle_requested_gc(cause);\n+  }\n+}\n+\n+bool ShenandoahGenerationalControlThread::request_concurrent_gc(ShenandoahGeneration* generation) {\n+  if (_heap->cancelled_gc()) {\n+    \/\/ Ignore subsequent requests from the heuristics\n+    log_debug(gc, thread)(\"Reject request for concurrent gc: gc_requested: %s, gc_cancelled: %s\",\n+                          GCCause::to_string(_requested_gc_cause),\n+                          BOOL_TO_STR(_heap->cancelled_gc()));\n+    return false;\n+  }\n+\n+  MonitorLocker ml(&_control_lock, Mutex::_no_safepoint_check_flag);\n+  if (gc_mode() == servicing_old) {\n+    if (!preempt_old_marking(generation)) {\n+      log_debug(gc, thread)(\"Cannot start young, old collection is not preemptible\");\n+      return false;\n+    }\n+\n+    \/\/ Cancel the old GC and wait for the control thread to start servicing the new request.\n+    log_info(gc)(\"Preempting old generation mark to allow %s GC\", generation->name());\n+    while (gc_mode() == servicing_old) {\n+      ShenandoahHeap::heap()->cancel_gc(GCCause::_shenandoah_concurrent_gc);\n+      notify_cancellation(ml, GCCause::_shenandoah_concurrent_gc);\n+      ml.wait();\n+    }\n+    return true;\n+  }\n+\n+  if (gc_mode() == none) {\n+    const size_t current_gc_id = get_gc_id();\n+    while (gc_mode() == none && current_gc_id == get_gc_id()) {\n+      if (_requested_gc_cause != GCCause::_no_gc) {\n+        log_debug(gc, thread)(\"Reject request for concurrent gc because another gc is pending: %s\", GCCause::to_string(_requested_gc_cause));\n+        return false;\n+      }\n+\n+      notify_control_thread(ml, GCCause::_shenandoah_concurrent_gc, generation);\n+      ml.wait();\n+    }\n+    return true;\n+  }\n+\n+\n+  log_debug(gc, thread)(\"Reject request for concurrent gc: mode: %s, allow_old_preemption: %s\",\n+                        gc_mode_name(gc_mode()),\n+                        BOOL_TO_STR(_allow_old_preemption.is_set()));\n+  return false;\n+}\n+\n+void ShenandoahGenerationalControlThread::notify_control_thread(GCCause::Cause cause, ShenandoahGeneration* generation) {\n+  MonitorLocker ml(&_control_lock, Mutex::_no_safepoint_check_flag);\n+  notify_control_thread(ml, cause, generation);\n+}\n+\n+void ShenandoahGenerationalControlThread::notify_control_thread(MonitorLocker& ml, GCCause::Cause cause, ShenandoahGeneration* generation) {\n+  assert(_control_lock.is_locked(), \"Request lock must be held here\");\n+  log_debug(gc, thread)(\"Notify control (%s): %s, %s\", gc_mode_name(gc_mode()), GCCause::to_string(cause), generation->name());\n+  _requested_gc_cause = cause;\n+  _requested_generation = generation;\n+  ml.notify();\n+}\n+\n+void ShenandoahGenerationalControlThread::notify_cancellation(GCCause::Cause cause) {\n+  MonitorLocker ml(&_control_lock, Mutex::_no_safepoint_check_flag);\n+  notify_cancellation(ml, cause);\n+}\n+\n+void ShenandoahGenerationalControlThread::notify_cancellation(MonitorLocker& ml, GCCause::Cause cause) {\n+  assert(_heap->cancelled_gc(), \"GC should already be cancelled\");\n+  log_debug(gc,thread)(\"Notify control (%s): %s\", gc_mode_name(gc_mode()), GCCause::to_string(cause));\n+  ml.notify();\n+}\n+\n+bool ShenandoahGenerationalControlThread::preempt_old_marking(ShenandoahGeneration* generation) {\n+  return generation->is_young() && _allow_old_preemption.try_unset();\n+}\n+\n+void ShenandoahGenerationalControlThread::handle_requested_gc(GCCause::Cause cause) {\n+  \/\/ For normal requested GCs (System.gc) we want to block the caller. However,\n+  \/\/ for whitebox requested GC, we want to initiate the GC and return immediately.\n+  \/\/ The whitebox caller thread will arrange for itself to wait until the GC notifies\n+  \/\/ it that has reached the requested breakpoint (phase in the GC).\n+  if (cause == GCCause::_wb_breakpoint) {\n+    notify_control_thread(cause, ShenandoahHeap::heap()->global_generation());\n+    return;\n+  }\n+\n+  \/\/ Make sure we have at least one complete GC cycle before unblocking\n+  \/\/ from the explicit GC request.\n+  \/\/\n+  \/\/ This is especially important for weak references cleanup and\/or native\n+  \/\/ resources (e.g. DirectByteBuffers) machinery: when explicit GC request\n+  \/\/ comes very late in the already running cycle, it would miss lots of new\n+  \/\/ opportunities for cleanup that were made available before the caller\n+  \/\/ requested the GC.\n+\n+  MonitorLocker ml(&_gc_waiters_lock);\n+  size_t current_gc_id = get_gc_id();\n+  const size_t required_gc_id = current_gc_id + 1;\n+  while (current_gc_id < required_gc_id && !should_terminate()) {\n+    \/\/ Make requests to run a global cycle until at least one is completed\n+    notify_control_thread(cause, ShenandoahHeap::heap()->global_generation());\n+    ml.wait();\n+    current_gc_id = get_gc_id();\n+  }\n+}\n+\n+void ShenandoahGenerationalControlThread::notify_gc_waiters() {\n+  MonitorLocker ml(&_gc_waiters_lock);\n+  ml.notify_all();\n+}\n+\n+const char* ShenandoahGenerationalControlThread::gc_mode_name(GCMode mode) {\n+  switch (mode) {\n+    case none:              return \"idle\";\n+    case concurrent_normal: return \"normal\";\n+    case stw_degenerated:   return \"degenerated\";\n+    case stw_full:          return \"full\";\n+    case servicing_old:     return \"old\";\n+    case bootstrapping_old: return \"bootstrap\";\n+    case stopped:           return \"stopped\";\n+    default:                return \"unknown\";\n+  }\n+}\n+\n+void ShenandoahGenerationalControlThread::set_gc_mode(GCMode new_mode) {\n+  MonitorLocker ml(&_control_lock, Mutex::_no_safepoint_check_flag);\n+  set_gc_mode(ml, new_mode);\n+}\n+\n+void ShenandoahGenerationalControlThread::set_gc_mode(MonitorLocker& ml, GCMode new_mode) {\n+  if (_gc_mode != new_mode) {\n+    log_debug(gc, thread)(\"Transition from: %s to: %s\", gc_mode_name(_gc_mode), gc_mode_name(new_mode));\n+    EventMark event(\"Control thread transition from: %s, to %s\", gc_mode_name(_gc_mode), gc_mode_name(new_mode));\n+    _gc_mode = new_mode;\n+    ml.notify_all();\n+  }\n+}\n","filename":"src\/hotspot\/share\/gc\/shenandoah\/shenandoahGenerationalControlThread.cpp","additions":795,"deletions":0,"binary":false,"changes":795,"status":"added"},{"patch":"@@ -0,0 +1,165 @@\n+\/*\n+ * Copyright (c) 2013, 2021, Red Hat, Inc. All rights reserved.\n+ * Copyright Amazon.com Inc. or its affiliates. All Rights Reserved.\n+ * DO NOT ALTER OR REMOVE COPYRIGHT NOTICES OR THIS FILE HEADER.\n+ *\n+ * This code is free software; you can redistribute it and\/or modify it\n+ * under the terms of the GNU General Public License version 2 only, as\n+ * published by the Free Software Foundation.\n+ *\n+ * This code is distributed in the hope that it will be useful, but WITHOUT\n+ * ANY WARRANTY; without even the implied warranty of MERCHANTABILITY or\n+ * FITNESS FOR A PARTICULAR PURPOSE.  See the GNU General Public License\n+ * version 2 for more details (a copy is included in the LICENSE file that\n+ * accompanied this code).\n+ *\n+ * You should have received a copy of the GNU General Public License version\n+ * 2 along with this work; if not, write to the Free Software Foundation,\n+ * Inc., 51 Franklin St, Fifth Floor, Boston, MA 02110-1301 USA.\n+ *\n+ * Please contact Oracle, 500 Oracle Parkway, Redwood Shores, CA 94065 USA\n+ * or visit www.oracle.com if you need additional information or have any\n+ * questions.\n+ *\n+ *\/\n+\n+#ifndef SHARE_GC_SHENANDOAH_SHENANDOAHGENERATIONALCONTROLTHREAD_HPP\n+#define SHARE_GC_SHENANDOAH_SHENANDOAHGENERATIONALCONTROLTHREAD_HPP\n+\n+#include \"gc\/shared\/gcCause.hpp\"\n+#include \"gc\/shenandoah\/shenandoahController.hpp\"\n+#include \"gc\/shenandoah\/shenandoahGC.hpp\"\n+#include \"gc\/shenandoah\/shenandoahSharedVariables.hpp\"\n+#include \"runtime\/mutexLocker.hpp\"\n+\n+class ShenandoahOldGeneration;\n+class ShenandoahGeneration;\n+class ShenandoahGenerationalHeap;\n+class ShenandoahHeap;\n+\n+class ShenandoahGenerationalControlThread: public ShenandoahController {\n+  friend class VMStructs;\n+\n+public:\n+  typedef enum {\n+    none,\n+    concurrent_normal,\n+    stw_degenerated,\n+    stw_full,\n+    bootstrapping_old,\n+    servicing_old,\n+    stopped\n+  } GCMode;\n+\n+  class ShenandoahGCRequest {\n+  public:\n+    ShenandoahGCRequest() : generation(nullptr), cause(GCCause::_no_gc) {}\n+    ShenandoahGeneration* generation;\n+    GCCause::Cause cause;\n+  };\n+\n+private:\n+  \/\/ This lock is used to coordinate setting the _requested_gc_cause, _requested generation\n+  \/\/ and _gc_mode. It is important that these be changed together and have a consistent view.\n+  Monitor _control_lock;\n+\n+  \/\/ Represents a normal (non cancellation) gc request. This can be set by mutators (System.gc,\n+  \/\/ whitebox gc, etc.) or by the regulator thread when the heuristics want to start a cycle.\n+  GCCause::Cause  _requested_gc_cause;\n+\n+  \/\/ This is the generation the request should operate on.\n+  ShenandoahGeneration* _requested_generation;\n+\n+  \/\/ The mode is read frequently by requesting threads and only ever written by the control thread.\n+  \/\/ This may be read without taking the _control_lock, but should be read again under the lock\n+  \/\/ before making any state changes (double-checked locking idiom).\n+  volatile GCMode _gc_mode;\n+\n+  \/\/ Only the control thread knows the correct degeneration point. This is used to have the\n+  \/\/ control thread resume a STW cycle from the point where the concurrent cycle was cancelled.\n+  ShenandoahGC::ShenandoahDegenPoint _degen_point;\n+\n+  \/\/ A reference to the heap\n+  ShenandoahGenerationalHeap* _heap;\n+\n+  \/\/ This is used to keep track of whether to age objects during the current cycle.\n+  uint _age_period;\n+\n+  \/\/ This is true when the old generation cycle is in an interruptible phase (i.e., marking or\n+  \/\/ preparing for mark).\n+  ShenandoahSharedFlag _allow_old_preemption;\n+\n+public:\n+  ShenandoahGenerationalControlThread();\n+\n+  void run_service() override;\n+  void stop_service() override;\n+\n+  void request_gc(GCCause::Cause cause) override;\n+\n+  \/\/ Return true if the request to start a concurrent GC for the given generation succeeded.\n+  bool request_concurrent_gc(ShenandoahGeneration* generation);\n+\n+  \/\/ Returns the current state of the control thread\n+  GCMode gc_mode() const {\n+    return _gc_mode;\n+  }\n+private:\n+  \/\/ Returns true if the cycle has been cancelled or degenerated.\n+  bool check_cancellation_or_degen(ShenandoahGC::ShenandoahDegenPoint point);\n+\n+  \/\/ Executes one GC cycle\n+  void run_gc_cycle(const ShenandoahGCRequest& request);\n+\n+  \/\/ Returns true if the old generation marking completed (i.e., final mark executed for old generation).\n+  bool resume_concurrent_old_cycle(ShenandoahOldGeneration* generation, GCCause::Cause cause);\n+\n+  \/\/ Various service methods handle different gc cycle types\n+  void service_concurrent_cycle(ShenandoahGeneration* generation, GCCause::Cause cause, bool reset_old_bitmap_specially);\n+  void service_stw_full_cycle(GCCause::Cause cause);\n+  void service_stw_degenerated_cycle(const ShenandoahGCRequest& request);\n+  void service_concurrent_normal_cycle(const ShenandoahGCRequest& request);\n+  void service_concurrent_old_cycle(const ShenandoahGCRequest& request);\n+\n+  void notify_gc_waiters();\n+\n+  \/\/ Blocks until at least one global GC cycle is complete.\n+  void handle_requested_gc(GCCause::Cause cause);\n+\n+  \/\/ Returns true if the old generation marking was interrupted to allow a young cycle.\n+  bool preempt_old_marking(ShenandoahGeneration* generation);\n+\n+  \/\/ Flushes cycle timings to global timings and prints the phase timings for the last completed cycle.\n+  void process_phase_timings() const;\n+\n+  \/\/ Set the gc mode and post a notification if it has changed. The overloaded variant should be used\n+  \/\/ when the _control_lock is already held.\n+  void set_gc_mode(GCMode new_mode);\n+  void set_gc_mode(MonitorLocker& ml, GCMode new_mode);\n+\n+  \/\/ Return printable name for the given gc mode.\n+  static const char* gc_mode_name(GCMode mode);\n+\n+  \/\/ Takes the request lock and updates the requested cause and generation, then notifies the control thread.\n+  \/\/ The overloaded variant should be used when the _control_lock is already held.\n+  void notify_control_thread(GCCause::Cause cause, ShenandoahGeneration* generation);\n+  void notify_control_thread(MonitorLocker& ml, GCCause::Cause cause, ShenandoahGeneration* generation);\n+\n+  \/\/ Notifies the control thread, but does not update the requested cause or generation.\n+  \/\/ The overloaded variant should be used when the _control_lock is already held.\n+  void notify_cancellation(GCCause::Cause cause);\n+  void notify_cancellation(MonitorLocker& ml, GCCause::Cause cause);\n+\n+  \/\/ Configure the heap to age objects and regions if the aging period has elapsed.\n+  void maybe_set_aging_cycle();\n+\n+  \/\/ Take the _control_lock and check for a request to run a gc cycle. If a request is found,\n+  \/\/ the `prepare` methods are used to configure the heap and update heuristics accordingly.\n+  void check_for_request(ShenandoahGCRequest& request);\n+\n+  GCMode prepare_for_allocation_failure_gc(ShenandoahGCRequest &request);\n+  GCMode prepare_for_explicit_gc(ShenandoahGCRequest &request) const;\n+  GCMode prepare_for_concurrent_gc(const ShenandoahGCRequest &request) const;\n+};\n+\n+#endif \/\/ SHARE_GC_SHENANDOAH_SHENANDOAHGENERATIONALCONTROLTHREAD_HPP\n","filename":"src\/hotspot\/share\/gc\/shenandoah\/shenandoahGenerationalControlThread.hpp","additions":165,"deletions":0,"binary":false,"changes":165,"status":"added"},{"patch":"@@ -0,0 +1,325 @@\n+\/*\n+ * Copyright Amazon.com Inc. or its affiliates. All Rights Reserved.\n+ * DO NOT ALTER OR REMOVE COPYRIGHT NOTICES OR THIS FILE HEADER.\n+ *\n+ * This code is free software; you can redistribute it and\/or modify it\n+ * under the terms of the GNU General Public License version 2 only, as\n+ * published by the Free Software Foundation.\n+ *\n+ * This code is distributed in the hope that it will be useful, but WITHOUT\n+ * ANY WARRANTY; without even the implied warranty of MERCHANTABILITY or\n+ * FITNESS FOR A PARTICULAR PURPOSE.  See the GNU General Public License\n+ * version 2 for more details (a copy is included in the LICENSE file that\n+ * accompanied this code).\n+ *\n+ * You should have received a copy of the GNU General Public License version\n+ * 2 along with this work; if not, write to the Free Software Foundation,\n+ * Inc., 51 Franklin St, Fifth Floor, Boston, MA 02110-1301 USA.\n+ *\n+ * Please contact Oracle, 500 Oracle Parkway, Redwood Shores, CA 94065 USA\n+ * or visit www.oracle.com if you need additional information or have any\n+ * questions.\n+ *\n+ *\/\n+\n+#include \"precompiled.hpp\"\n+\n+#include \"gc\/shenandoah\/shenandoahAsserts.hpp\"\n+#include \"gc\/shenandoah\/shenandoahFreeSet.hpp\"\n+#include \"gc\/shenandoah\/shenandoahGenerationalEvacuationTask.hpp\"\n+#include \"gc\/shenandoah\/shenandoahGenerationalHeap.inline.hpp\"\n+#include \"gc\/shenandoah\/shenandoahHeap.inline.hpp\"\n+#include \"gc\/shenandoah\/shenandoahOldGeneration.hpp\"\n+#include \"gc\/shenandoah\/shenandoahPacer.hpp\"\n+#include \"gc\/shenandoah\/shenandoahScanRemembered.inline.hpp\"\n+#include \"gc\/shenandoah\/shenandoahYoungGeneration.hpp\"\n+#include \"gc\/shenandoah\/shenandoahUtils.hpp\"\n+\n+class ShenandoahConcurrentEvacuator : public ObjectClosure {\n+private:\n+  ShenandoahGenerationalHeap* const _heap;\n+  Thread* const _thread;\n+public:\n+  explicit ShenandoahConcurrentEvacuator(ShenandoahGenerationalHeap* heap) :\n+          _heap(heap), _thread(Thread::current()) {}\n+\n+  void do_object(oop p) override {\n+    shenandoah_assert_marked(nullptr, p);\n+    if (!p->is_forwarded()) {\n+      _heap->evacuate_object(p, _thread);\n+    }\n+  }\n+};\n+\n+ShenandoahGenerationalEvacuationTask::ShenandoahGenerationalEvacuationTask(ShenandoahGenerationalHeap* heap,\n+                                                                           ShenandoahRegionIterator* iterator,\n+                                                                           bool concurrent, bool only_promote_regions) :\n+  WorkerTask(\"Shenandoah Evacuation\"),\n+  _heap(heap),\n+  _regions(iterator),\n+  _concurrent(concurrent),\n+  _only_promote_regions(only_promote_regions)\n+{\n+  shenandoah_assert_generational();\n+}\n+\n+void ShenandoahGenerationalEvacuationTask::work(uint worker_id) {\n+  if (_concurrent) {\n+    ShenandoahConcurrentWorkerSession worker_session(worker_id);\n+    ShenandoahSuspendibleThreadSetJoiner stsj;\n+    do_work();\n+  } else {\n+    ShenandoahParallelWorkerSession worker_session(worker_id);\n+    do_work();\n+  }\n+}\n+\n+void ShenandoahGenerationalEvacuationTask::do_work() {\n+  if (_only_promote_regions) {\n+    \/\/ No allocations will be made, do not enter oom-during-evac protocol.\n+    assert(ShenandoahHeap::heap()->collection_set()->is_empty(), \"Should not have a collection set here\");\n+    promote_regions();\n+  } else {\n+    assert(!ShenandoahHeap::heap()->collection_set()->is_empty(), \"Should have a collection set here\");\n+    ShenandoahEvacOOMScope oom_evac_scope;\n+    evacuate_and_promote_regions();\n+  }\n+}\n+\n+void log_region(const ShenandoahHeapRegion* r, LogStream* ls) {\n+  ls->print_cr(\"GenerationalEvacuationTask, looking at %s region \" SIZE_FORMAT \", (age: %d) [%s, %s, %s]\",\n+              r->is_old()? \"old\": r->is_young()? \"young\": \"free\", r->index(), r->age(),\n+              r->is_active()? \"active\": \"inactive\",\n+              r->is_humongous()? (r->is_humongous_start()? \"humongous_start\": \"humongous_continuation\"): \"regular\",\n+              r->is_cset()? \"cset\": \"not-cset\");\n+}\n+\n+void ShenandoahGenerationalEvacuationTask::promote_regions() {\n+  ShenandoahHeapRegion* r;\n+  LogTarget(Debug, gc) lt;\n+\n+  while ((r = _regions->next()) != nullptr) {\n+    if (lt.is_enabled()) {\n+      LogStream ls(lt);\n+      log_region(r, &ls);\n+    }\n+\n+    maybe_promote_region(r);\n+\n+    if (_heap->check_cancelled_gc_and_yield(_concurrent)) {\n+      break;\n+    }\n+  }\n+}\n+\n+void ShenandoahGenerationalEvacuationTask::evacuate_and_promote_regions() {\n+  LogTarget(Debug, gc) lt;\n+  ShenandoahConcurrentEvacuator cl(_heap);\n+  ShenandoahHeapRegion* r;\n+\n+  while ((r = _regions->next()) != nullptr) {\n+    if (lt.is_enabled()) {\n+      LogStream ls(lt);\n+      log_region(r, &ls);\n+    }\n+\n+    if (r->is_cset()) {\n+      assert(r->has_live(), \"Region \" SIZE_FORMAT \" should have been reclaimed early\", r->index());\n+      _heap->marked_object_iterate(r, &cl);\n+      if (ShenandoahPacing) {\n+        _heap->pacer()->report_evac(r->used() >> LogHeapWordSize);\n+      }\n+    } else {\n+      maybe_promote_region(r);\n+    }\n+\n+    if (_heap->check_cancelled_gc_and_yield(_concurrent)) {\n+      break;\n+    }\n+  }\n+}\n+\n+\n+void ShenandoahGenerationalEvacuationTask::maybe_promote_region(ShenandoahHeapRegion* r) {\n+  if (r->is_young() && r->is_active() && _heap->is_tenurable(r)) {\n+    if (r->is_humongous_start()) {\n+      \/\/ We promote humongous_start regions along with their affiliated continuations during evacuation rather than\n+      \/\/ doing this work during a safepoint.  We cannot put humongous regions into the collection set because that\n+      \/\/ triggers the load-reference barrier (LRB) to copy on reference fetch.\n+      \/\/\n+      \/\/ Aged humongous continuation regions are handled with their start region.  If an aged regular region has\n+      \/\/ more garbage than ShenandoahOldGarbageThreshold, we'll promote by evacuation.  If there is room for evacuation\n+      \/\/ in this cycle, the region will be in the collection set.  If there is not room, the region will be promoted\n+      \/\/ by evacuation in some future GC cycle.\n+      promote_humongous(r);\n+    } else if (r->is_regular() && (r->get_top_before_promote() != nullptr)) {\n+      \/\/ Likewise, we cannot put promote-in-place regions into the collection set because that would also trigger\n+      \/\/ the LRB to copy on reference fetch.\n+      \/\/\n+      \/\/ If an aged regular region has received allocations during the current cycle, we do not promote because the\n+      \/\/ newly allocated objects do not have appropriate age; this region's age will be reset to zero at end of cycle.\n+      promote_in_place(r);\n+    }\n+  }\n+}\n+\n+\/\/ When we promote a region in place, we can continue to use the established marking context to guide subsequent remembered\n+\/\/ set scans of this region's content.  The region will be coalesced and filled prior to the next old-gen marking effort.\n+\/\/ We identify the entirety of the region as DIRTY to force the next remembered set scan to identify the \"interesting pointers\"\n+\/\/ contained herein.\n+void ShenandoahGenerationalEvacuationTask::promote_in_place(ShenandoahHeapRegion* region) {\n+  assert(!_heap->gc_generation()->is_old(), \"Sanity check\");\n+  ShenandoahMarkingContext* const marking_context = _heap->young_generation()->complete_marking_context();\n+  HeapWord* const tams = marking_context->top_at_mark_start(region);\n+\n+  {\n+    const size_t old_garbage_threshold = (ShenandoahHeapRegion::region_size_bytes() * ShenandoahOldGarbageThreshold) \/ 100;\n+    shenandoah_assert_generations_reconciled();\n+    assert(!_heap->is_concurrent_old_mark_in_progress(), \"Cannot promote in place during old marking\");\n+    assert(region->garbage_before_padded_for_promote() < old_garbage_threshold, \"Region \" SIZE_FORMAT \" has too much garbage for promotion\", region->index());\n+    assert(region->is_young(), \"Only young regions can be promoted\");\n+    assert(region->is_regular(), \"Use different service to promote humongous regions\");\n+    assert(_heap->is_tenurable(region), \"Only promote regions that are sufficiently aged\");\n+    assert(region->get_top_before_promote() == tams, \"Region \" SIZE_FORMAT \" has been used for allocations before promotion\", region->index());\n+  }\n+\n+  ShenandoahOldGeneration* const old_gen = _heap->old_generation();\n+  ShenandoahYoungGeneration* const young_gen = _heap->young_generation();\n+\n+  \/\/ Rebuild the remembered set information and mark the entire range as DIRTY.  We do NOT scan the content of this\n+  \/\/ range to determine which cards need to be DIRTY.  That would force us to scan the region twice, once now, and\n+  \/\/ once during the subsequent remembered set scan.  Instead, we blindly (conservatively) mark everything as DIRTY\n+  \/\/ now and then sort out the CLEAN pages during the next remembered set scan.\n+  \/\/\n+  \/\/ Rebuilding the remembered set consists of clearing all object registrations (reset_object_range()) here,\n+  \/\/ then registering every live object and every coalesced range of free objects in the loop that follows.\n+  ShenandoahScanRemembered* const scanner = old_gen->card_scan();\n+  scanner->reset_object_range(region->bottom(), region->end());\n+  scanner->mark_range_as_dirty(region->bottom(), region->get_top_before_promote() - region->bottom());\n+\n+  HeapWord* obj_addr = region->bottom();\n+  while (obj_addr < tams) {\n+    oop obj = cast_to_oop(obj_addr);\n+    if (marking_context->is_marked(obj)) {\n+      assert(obj->klass() != nullptr, \"klass should not be NULL\");\n+      \/\/ This thread is responsible for registering all objects in this region.  No need for lock.\n+      scanner->register_object_without_lock(obj_addr);\n+      obj_addr += obj->size();\n+    } else {\n+      HeapWord* next_marked_obj = marking_context->get_next_marked_addr(obj_addr, tams);\n+      assert(next_marked_obj <= tams, \"next marked object cannot exceed tams\");\n+      size_t fill_size = next_marked_obj - obj_addr;\n+      assert(fill_size >= ShenandoahHeap::min_fill_size(), \"previously allocated objects known to be larger than min_size\");\n+      ShenandoahHeap::fill_with_object(obj_addr, fill_size);\n+      scanner->register_object_without_lock(obj_addr);\n+      obj_addr = next_marked_obj;\n+    }\n+  }\n+  \/\/ We do not need to scan above TAMS because restored top equals tams\n+  assert(obj_addr == tams, \"Expect loop to terminate when obj_addr equals tams\");\n+\n+\n+  {\n+    ShenandoahHeapLocker locker(_heap->lock());\n+\n+    HeapWord* update_watermark = region->get_update_watermark();\n+\n+    \/\/ Now that this region is affiliated with old, we can allow it to receive allocations, though it may not be in the\n+    \/\/ is_collector_free range.\n+    region->restore_top_before_promote();\n+\n+    size_t region_used = region->used();\n+\n+    \/\/ The update_watermark was likely established while we had the artificially high value of top.  Make it sane now.\n+    assert(update_watermark >= region->top(), \"original top cannot exceed preserved update_watermark\");\n+    region->set_update_watermark(region->top());\n+\n+    \/\/ Unconditionally transfer one region from young to old. This represents the newly promoted region.\n+    \/\/ This expands old and shrinks new by the size of one region.  Strictly, we do not \"need\" to expand old\n+    \/\/ if there are already enough unaffiliated regions in old to account for this newly promoted region.\n+    \/\/ However, if we do not transfer the capacities, we end up reducing the amount of memory that would have\n+    \/\/ otherwise been available to hold old evacuations, because old available is max_capacity - used and now\n+    \/\/ we would be trading a fully empty region for a partially used region.\n+    young_gen->decrease_used(region_used);\n+    young_gen->decrement_affiliated_region_count();\n+\n+    \/\/ transfer_to_old() increases capacity of old and decreases capacity of young\n+    _heap->generation_sizer()->force_transfer_to_old(1);\n+    region->set_affiliation(OLD_GENERATION);\n+\n+    old_gen->increment_affiliated_region_count();\n+    old_gen->increase_used(region_used);\n+\n+    \/\/ add_old_collector_free_region() increases promoted_reserve() if available space exceeds plab_min_size()\n+    _heap->free_set()->add_promoted_in_place_region_to_old_collector(region);\n+  }\n+}\n+\n+void ShenandoahGenerationalEvacuationTask::promote_humongous(ShenandoahHeapRegion* region) {\n+  ShenandoahMarkingContext* marking_context = _heap->marking_context();\n+  oop obj = cast_to_oop(region->bottom());\n+  assert(_heap->gc_generation()->is_mark_complete(), \"sanity\");\n+  shenandoah_assert_generations_reconciled();\n+  assert(region->is_young(), \"Only young regions can be promoted\");\n+  assert(region->is_humongous_start(), \"Should not promote humongous continuation in isolation\");\n+  assert(_heap->is_tenurable(region), \"Only promote regions that are sufficiently aged\");\n+  assert(marking_context->is_marked(obj), \"promoted humongous object should be alive\");\n+\n+  const size_t used_bytes = obj->size() * HeapWordSize;\n+  const size_t spanned_regions = ShenandoahHeapRegion::required_regions(used_bytes);\n+  const size_t humongous_waste = spanned_regions * ShenandoahHeapRegion::region_size_bytes() - obj->size() * HeapWordSize;\n+  const size_t index_limit = region->index() + spanned_regions;\n+\n+  ShenandoahOldGeneration* const old_gen = _heap->old_generation();\n+  ShenandoahGeneration* const young_gen = _heap->young_generation();\n+  {\n+    \/\/ We need to grab the heap lock in order to avoid a race when changing the affiliations of spanned_regions from\n+    \/\/ young to old.\n+    ShenandoahHeapLocker locker(_heap->lock());\n+\n+    \/\/ We promote humongous objects unconditionally, without checking for availability.  We adjust\n+    \/\/ usage totals, including humongous waste, after evacuation is done.\n+    log_debug(gc)(\"promoting humongous region \" SIZE_FORMAT \", spanning \" SIZE_FORMAT, region->index(), spanned_regions);\n+\n+    young_gen->decrease_used(used_bytes);\n+    young_gen->decrease_humongous_waste(humongous_waste);\n+    young_gen->decrease_affiliated_region_count(spanned_regions);\n+\n+    \/\/ transfer_to_old() increases capacity of old and decreases capacity of young\n+    _heap->generation_sizer()->force_transfer_to_old(spanned_regions);\n+\n+    \/\/ For this region and each humongous continuation region spanned by this humongous object, change\n+    \/\/ affiliation to OLD_GENERATION and adjust the generation-use tallies.  The remnant of memory\n+    \/\/ in the last humongous region that is not spanned by obj is currently not used.\n+    for (size_t i = region->index(); i < index_limit; i++) {\n+      ShenandoahHeapRegion* r = _heap->get_region(i);\n+      log_debug(gc)(\"promoting humongous region \" SIZE_FORMAT \", from \" PTR_FORMAT \" to \" PTR_FORMAT,\n+              r->index(), p2i(r->bottom()), p2i(r->top()));\n+      \/\/ We mark the entire humongous object's range as dirty after loop terminates, so no need to dirty the range here\n+      r->set_affiliation(OLD_GENERATION);\n+    }\n+\n+    old_gen->increase_affiliated_region_count(spanned_regions);\n+    old_gen->increase_used(used_bytes);\n+    old_gen->increase_humongous_waste(humongous_waste);\n+  }\n+\n+  \/\/ Since this region may have served previously as OLD, it may hold obsolete object range info.\n+  HeapWord* const humongous_bottom = region->bottom();\n+  ShenandoahScanRemembered* const scanner = old_gen->card_scan();\n+  scanner->reset_object_range(humongous_bottom, humongous_bottom + spanned_regions * ShenandoahHeapRegion::region_size_words());\n+  \/\/ Since the humongous region holds only one object, no lock is necessary for this register_object() invocation.\n+  scanner->register_object_without_lock(humongous_bottom);\n+\n+  if (obj->is_typeArray()) {\n+    \/\/ Primitive arrays don't need to be scanned.\n+    log_debug(gc)(\"Clean cards for promoted humongous object (Region \" SIZE_FORMAT \") from \" PTR_FORMAT \" to \" PTR_FORMAT,\n+            region->index(), p2i(humongous_bottom), p2i(humongous_bottom + obj->size()));\n+    scanner->mark_range_as_clean(humongous_bottom, obj->size());\n+  } else {\n+    log_debug(gc)(\"Dirty cards for promoted humongous object (Region \" SIZE_FORMAT \") from \" PTR_FORMAT \" to \" PTR_FORMAT,\n+            region->index(), p2i(humongous_bottom), p2i(humongous_bottom + obj->size()));\n+    scanner->mark_range_as_dirty(humongous_bottom, obj->size());\n+  }\n+}\n+\n","filename":"src\/hotspot\/share\/gc\/shenandoah\/shenandoahGenerationalEvacuationTask.cpp","additions":325,"deletions":0,"binary":false,"changes":325,"status":"added"},{"patch":"@@ -0,0 +1,57 @@\n+\/*\n+ * Copyright Amazon.com Inc. or its affiliates. All Rights Reserved.\n+ * DO NOT ALTER OR REMOVE COPYRIGHT NOTICES OR THIS FILE HEADER.\n+ *\n+ * This code is free software; you can redistribute it and\/or modify it\n+ * under the terms of the GNU General Public License version 2 only, as\n+ * published by the Free Software Foundation.\n+ *\n+ * This code is distributed in the hope that it will be useful, but WITHOUT\n+ * ANY WARRANTY; without even the implied warranty of MERCHANTABILITY or\n+ * FITNESS FOR A PARTICULAR PURPOSE.  See the GNU General Public License\n+ * version 2 for more details (a copy is included in the LICENSE file that\n+ * accompanied this code).\n+ *\n+ * You should have received a copy of the GNU General Public License version\n+ * 2 along with this work; if not, write to the Free Software Foundation,\n+ * Inc., 51 Franklin St, Fifth Floor, Boston, MA 02110-1301 USA.\n+ *\n+ * Please contact Oracle, 500 Oracle Parkway, Redwood Shores, CA 94065 USA\n+ * or visit www.oracle.com if you need additional information or have any\n+ * questions.\n+ *\n+ *\/\n+\n+#ifndef SHARE_GC_SHENANDOAH_SHENANDOAHGENERATIONALEVACUATIONTASK_HPP\n+#define SHARE_GC_SHENANDOAH_SHENANDOAHGENERATIONALEVACUATIONTASK_HPP\n+\n+#include \"gc\/shared\/workerThread.hpp\"\n+\n+class ShenandoahGenerationalHeap;\n+class ShenandoahHeapRegion;\n+class ShenandoahRegionIterator;\n+\n+\/\/ Unlike ShenandoahEvacuationTask, this iterates over all regions rather than just the collection set.\n+\/\/ This is needed in order to promote humongous start regions if age() >= tenure threshold.\n+class ShenandoahGenerationalEvacuationTask : public WorkerTask {\n+private:\n+  ShenandoahGenerationalHeap* const _heap;\n+  ShenandoahRegionIterator* _regions;\n+  bool _concurrent;\n+  bool _only_promote_regions;\n+\n+public:\n+  ShenandoahGenerationalEvacuationTask(ShenandoahGenerationalHeap* sh,\n+                                       ShenandoahRegionIterator* iterator,\n+                                       bool concurrent, bool only_promote_regions);\n+  void work(uint worker_id) override;\n+private:\n+  void do_work();\n+  void promote_regions();\n+  void evacuate_and_promote_regions();\n+  void maybe_promote_region(ShenandoahHeapRegion* region);\n+  void promote_in_place(ShenandoahHeapRegion* region);\n+  void promote_humongous(ShenandoahHeapRegion* region);\n+};\n+\n+#endif \/\/SHARE_GC_SHENANDOAH_SHENANDOAHGENERATIONALEVACUATIONTASK_HPP\n","filename":"src\/hotspot\/share\/gc\/shenandoah\/shenandoahGenerationalEvacuationTask.hpp","additions":57,"deletions":0,"binary":false,"changes":57,"status":"added"},{"patch":"@@ -0,0 +1,390 @@\n+\/*\n+ * Copyright Amazon.com Inc. or its affiliates. All Rights Reserved.\n+ * DO NOT ALTER OR REMOVE COPYRIGHT NOTICES OR THIS FILE HEADER.\n+ *\n+ * This code is free software; you can redistribute it and\/or modify it\n+ * under the terms of the GNU General Public License version 2 only, as\n+ * published by the Free Software Foundation.\n+ *\n+ * This code is distributed in the hope that it will be useful, but WITHOUT\n+ * ANY WARRANTY; without even the implied warranty of MERCHANTABILITY or\n+ * FITNESS FOR A PARTICULAR PURPOSE.  See the GNU General Public License\n+ * version 2 for more details (a copy is included in the LICENSE file that\n+ * accompanied this code).\n+ *\n+ * You should have received a copy of the GNU General Public License version\n+ * 2 along with this work; if not, write to the Free Software Foundation,\n+ * Inc., 51 Franklin St, Fifth Floor, Boston, MA 02110-1301 USA.\n+ *\n+ * Please contact Oracle, 500 Oracle Parkway, Redwood Shores, CA 94065 USA\n+ * or visit www.oracle.com if you need additional information or have any\n+ * questions.\n+ *\n+ *\/\n+\n+#include \"precompiled.hpp\"\n+\n+#include \"gc\/shared\/preservedMarks.inline.hpp\"\n+#include \"gc\/shenandoah\/shenandoahGenerationalFullGC.hpp\"\n+#include \"gc\/shenandoah\/shenandoahGenerationalHeap.hpp\"\n+#include \"gc\/shenandoah\/shenandoahGeneration.hpp\"\n+#include \"gc\/shenandoah\/shenandoahHeap.inline.hpp\"\n+#include \"gc\/shenandoah\/shenandoahHeapRegion.hpp\"\n+#include \"gc\/shenandoah\/shenandoahYoungGeneration.hpp\"\n+#include \"gc\/shenandoah\/shenandoahOldGeneration.hpp\"\n+#include \"gc\/shenandoah\/shenandoahUtils.hpp\"\n+\n+#ifdef ASSERT\n+void assert_regions_used_not_more_than_capacity(ShenandoahGeneration* generation) {\n+  assert(generation->used_regions_size() <= generation->max_capacity(),\n+         \"%s generation affiliated regions must be less than capacity\", generation->name());\n+}\n+\n+void assert_usage_not_more_than_regions_used(ShenandoahGeneration* generation) {\n+  assert(generation->used_including_humongous_waste() <= generation->used_regions_size(),\n+         \"%s consumed can be no larger than span of affiliated regions\", generation->name());\n+}\n+#else\n+void assert_regions_used_not_more_than_capacity(ShenandoahGeneration* generation) {}\n+void assert_usage_not_more_than_regions_used(ShenandoahGeneration* generation) {}\n+#endif\n+\n+\n+void ShenandoahGenerationalFullGC::prepare() {\n+  auto heap = ShenandoahGenerationalHeap::heap();\n+  \/\/ Since we may arrive here from degenerated GC failure of either young or old, establish generation as GLOBAL.\n+  heap->set_gc_generation(heap->global_generation());\n+  heap->set_active_generation();\n+\n+  \/\/ No need for old_gen->increase_used() as this was done when plabs were allocated.\n+  heap->reset_generation_reserves();\n+\n+  \/\/ Full GC supersedes any marking or coalescing in old generation.\n+  heap->old_generation()->cancel_gc();\n+}\n+\n+void ShenandoahGenerationalFullGC::handle_completion(ShenandoahHeap* heap) {\n+  \/\/ Full GC should reset time since last gc for young and old heuristics\n+  ShenandoahGenerationalHeap* gen_heap = ShenandoahGenerationalHeap::cast(heap);\n+  ShenandoahYoungGeneration* young = gen_heap->young_generation();\n+  ShenandoahOldGeneration* old = gen_heap->old_generation();\n+  young->heuristics()->record_cycle_end();\n+  old->heuristics()->record_cycle_end();\n+\n+  gen_heap->mmu_tracker()->record_full(GCId::current());\n+  gen_heap->log_heap_status(\"At end of Full GC\");\n+\n+  assert(old->is_idle(), \"After full GC, old generation should be idle.\");\n+\n+  \/\/ Since we allow temporary violation of these constraints during Full GC, we want to enforce that the assertions are\n+  \/\/ made valid by the time Full GC completes.\n+  assert_regions_used_not_more_than_capacity(old);\n+  assert_regions_used_not_more_than_capacity(young);\n+  assert_usage_not_more_than_regions_used(old);\n+  assert_usage_not_more_than_regions_used(young);\n+\n+  \/\/ Establish baseline for next old-has-grown trigger.\n+  old->set_live_bytes_after_last_mark(old->used_including_humongous_waste());\n+}\n+\n+void ShenandoahGenerationalFullGC::rebuild_remembered_set(ShenandoahHeap* heap) {\n+  ShenandoahGCPhase phase(ShenandoahPhaseTimings::full_gc_reconstruct_remembered_set);\n+\n+  ShenandoahScanRemembered* scanner = heap->old_generation()->card_scan();\n+  scanner->mark_read_table_as_clean();\n+  scanner->swap_card_tables();\n+\n+  ShenandoahRegionIterator regions;\n+  ShenandoahReconstructRememberedSetTask task(&regions);\n+  heap->workers()->run_task(&task);\n+\n+  \/\/ Rebuilding the remembered set recomputes all the card offsets for objects.\n+  \/\/ The adjust pointers phase coalesces and fills all necessary regions. In case\n+  \/\/ we came to the full GC from an incomplete global cycle, we need to indicate\n+  \/\/ that the old regions are parsable.\n+  heap->old_generation()->set_parsable(true);\n+}\n+\n+void ShenandoahGenerationalFullGC::balance_generations_after_gc(ShenandoahHeap* heap) {\n+  ShenandoahGenerationalHeap* gen_heap = ShenandoahGenerationalHeap::cast(heap);\n+  ShenandoahOldGeneration* const old_gen = gen_heap->old_generation();\n+\n+  size_t old_usage = old_gen->used_regions_size();\n+  size_t old_capacity = old_gen->max_capacity();\n+\n+  assert(old_usage % ShenandoahHeapRegion::region_size_bytes() == 0, \"Old usage must align with region size\");\n+  assert(old_capacity % ShenandoahHeapRegion::region_size_bytes() == 0, \"Old capacity must align with region size\");\n+\n+  if (old_capacity > old_usage) {\n+    size_t excess_old_regions = (old_capacity - old_usage) \/ ShenandoahHeapRegion::region_size_bytes();\n+    gen_heap->generation_sizer()->transfer_to_young(excess_old_regions);\n+  } else if (old_capacity < old_usage) {\n+    size_t old_regions_deficit = (old_usage - old_capacity) \/ ShenandoahHeapRegion::region_size_bytes();\n+    gen_heap->generation_sizer()->force_transfer_to_old(old_regions_deficit);\n+  }\n+\n+  log_info(gc, ergo)(\"FullGC done: young usage: \" PROPERFMT \", old usage: \" PROPERFMT,\n+               PROPERFMTARGS(gen_heap->young_generation()->used()),\n+               PROPERFMTARGS(old_gen->used()));\n+}\n+\n+void ShenandoahGenerationalFullGC::balance_generations_after_rebuilding_free_set() {\n+  auto result = ShenandoahGenerationalHeap::heap()->balance_generations();\n+  LogTarget(Info, gc, ergo) lt;\n+  if (lt.is_enabled()) {\n+    LogStream ls(lt);\n+    result.print_on(\"Full GC\", &ls);\n+  }\n+}\n+\n+void ShenandoahGenerationalFullGC::log_live_in_old(ShenandoahHeap* heap) {\n+  LogTarget(Debug, gc) lt;\n+  if (lt.is_enabled()) {\n+    size_t live_bytes_in_old = 0;\n+    for (size_t i = 0; i < heap->num_regions(); i++) {\n+      ShenandoahHeapRegion* r = heap->get_region(i);\n+      if (r->is_old()) {\n+        live_bytes_in_old += r->get_live_data_bytes();\n+      }\n+    }\n+    log_debug(gc)(\"Live bytes in old after STW mark: \" PROPERFMT, PROPERFMTARGS(live_bytes_in_old));\n+  }\n+}\n+\n+void ShenandoahGenerationalFullGC::restore_top_before_promote(ShenandoahHeap* heap) {\n+  for (size_t i = 0; i < heap->num_regions(); i++) {\n+    ShenandoahHeapRegion* r = heap->get_region(i);\n+    if (r->get_top_before_promote() != nullptr) {\n+      r->restore_top_before_promote();\n+    }\n+  }\n+}\n+\n+void ShenandoahGenerationalFullGC::account_for_region(ShenandoahHeapRegion* r, size_t &region_count, size_t &region_usage, size_t &humongous_waste) {\n+  region_count++;\n+  region_usage += r->used();\n+  if (r->is_humongous_start()) {\n+    \/\/ For each humongous object, we take this path once regardless of how many regions it spans.\n+    HeapWord* obj_addr = r->bottom();\n+    oop obj = cast_to_oop(obj_addr);\n+    size_t word_size = obj->size();\n+    size_t region_size_words = ShenandoahHeapRegion::region_size_words();\n+    size_t overreach = word_size % region_size_words;\n+    if (overreach != 0) {\n+      humongous_waste += (region_size_words - overreach) * HeapWordSize;\n+    }\n+    \/\/ else, this humongous object aligns exactly on region size, so no waste.\n+  }\n+}\n+\n+void ShenandoahGenerationalFullGC::maybe_coalesce_and_fill_region(ShenandoahHeapRegion* r) {\n+  if (r->is_pinned() && r->is_old() && r->is_active() && !r->is_humongous()) {\n+    r->begin_preemptible_coalesce_and_fill();\n+    r->oop_coalesce_and_fill(false);\n+  }\n+}\n+\n+void ShenandoahGenerationalFullGC::compute_balances() {\n+  auto heap = ShenandoahGenerationalHeap::heap();\n+\n+  \/\/ In case this Full GC resulted from degeneration, clear the tally on anticipated promotion.\n+  heap->old_generation()->set_promotion_potential(0);\n+  \/\/ Invoke this in case we are able to transfer memory from OLD to YOUNG.\n+  heap->compute_old_generation_balance(0, 0);\n+}\n+\n+ShenandoahPrepareForGenerationalCompactionObjectClosure::ShenandoahPrepareForGenerationalCompactionObjectClosure(PreservedMarks* preserved_marks,\n+                                                          GrowableArray<ShenandoahHeapRegion*>& empty_regions,\n+                                                          ShenandoahHeapRegion* from_region, uint worker_id) :\n+        _preserved_marks(preserved_marks),\n+        _heap(ShenandoahGenerationalHeap::heap()),\n+        _empty_regions(empty_regions),\n+        _empty_regions_pos(0),\n+        _old_to_region(nullptr),\n+        _young_to_region(nullptr),\n+        _from_region(nullptr),\n+        _from_affiliation(ShenandoahAffiliation::FREE),\n+        _old_compact_point(nullptr),\n+        _young_compact_point(nullptr),\n+        _worker_id(worker_id) {\n+  assert(from_region != nullptr, \"Worker needs from_region\");\n+  \/\/ assert from_region has live?\n+  if (from_region->is_old()) {\n+    _old_to_region = from_region;\n+    _old_compact_point = from_region->bottom();\n+  } else if (from_region->is_young()) {\n+    _young_to_region = from_region;\n+    _young_compact_point = from_region->bottom();\n+  }\n+}\n+\n+void ShenandoahPrepareForGenerationalCompactionObjectClosure::set_from_region(ShenandoahHeapRegion* from_region) {\n+  log_debug(gc)(\"Worker %u compacting %s Region \" SIZE_FORMAT \" which had used \" SIZE_FORMAT \" and %s live\",\n+                _worker_id, from_region->affiliation_name(),\n+                from_region->index(), from_region->used(), from_region->has_live()? \"has\": \"does not have\");\n+\n+  _from_region = from_region;\n+  _from_affiliation = from_region->affiliation();\n+  if (_from_region->has_live()) {\n+    if (_from_affiliation == ShenandoahAffiliation::OLD_GENERATION) {\n+      if (_old_to_region == nullptr) {\n+        _old_to_region = from_region;\n+        _old_compact_point = from_region->bottom();\n+      }\n+    } else {\n+      assert(_from_affiliation == ShenandoahAffiliation::YOUNG_GENERATION, \"from_region must be OLD or YOUNG\");\n+      if (_young_to_region == nullptr) {\n+        _young_to_region = from_region;\n+        _young_compact_point = from_region->bottom();\n+      }\n+    }\n+  } \/\/ else, we won't iterate over this _from_region so we don't need to set up to region to hold copies\n+}\n+\n+void ShenandoahPrepareForGenerationalCompactionObjectClosure::finish() {\n+  finish_old_region();\n+  finish_young_region();\n+}\n+\n+void ShenandoahPrepareForGenerationalCompactionObjectClosure::finish_old_region() {\n+  if (_old_to_region != nullptr) {\n+    log_debug(gc)(\"Planned compaction into Old Region \" SIZE_FORMAT \", used: \" SIZE_FORMAT \" tabulated by worker %u\",\n+            _old_to_region->index(), _old_compact_point - _old_to_region->bottom(), _worker_id);\n+    _old_to_region->set_new_top(_old_compact_point);\n+    _old_to_region = nullptr;\n+  }\n+}\n+\n+void ShenandoahPrepareForGenerationalCompactionObjectClosure::finish_young_region() {\n+  if (_young_to_region != nullptr) {\n+    log_debug(gc)(\"Worker %u planned compaction into Young Region \" SIZE_FORMAT \", used: \" SIZE_FORMAT,\n+            _worker_id, _young_to_region->index(), _young_compact_point - _young_to_region->bottom());\n+    _young_to_region->set_new_top(_young_compact_point);\n+    _young_to_region = nullptr;\n+  }\n+}\n+\n+bool ShenandoahPrepareForGenerationalCompactionObjectClosure::is_compact_same_region() {\n+  return (_from_region == _old_to_region) || (_from_region == _young_to_region);\n+}\n+\n+void ShenandoahPrepareForGenerationalCompactionObjectClosure::do_object(oop p) {\n+  assert(_from_region != nullptr, \"must set before work\");\n+  assert((_from_region->bottom() <= cast_from_oop<HeapWord*>(p)) && (cast_from_oop<HeapWord*>(p) < _from_region->top()),\n+         \"Object must reside in _from_region\");\n+  assert(_heap->global_generation()->complete_marking_context()->is_marked(p), \"must be marked\");\n+  assert(!_heap->global_generation()->complete_marking_context()->allocated_after_mark_start(p), \"must be truly marked\");\n+\n+  size_t obj_size = p->size();\n+  uint from_region_age = _from_region->age();\n+  uint object_age = p->age();\n+\n+  bool promote_object = false;\n+  if ((_from_affiliation == ShenandoahAffiliation::YOUNG_GENERATION) &&\n+      _heap->age_census()->is_tenurable(from_region_age + object_age)) {\n+    if ((_old_to_region != nullptr) && (_old_compact_point + obj_size > _old_to_region->end())) {\n+      finish_old_region();\n+      _old_to_region = nullptr;\n+    }\n+    if (_old_to_region == nullptr) {\n+      if (_empty_regions_pos < _empty_regions.length()) {\n+        ShenandoahHeapRegion* new_to_region = _empty_regions.at(_empty_regions_pos);\n+        _empty_regions_pos++;\n+        new_to_region->set_affiliation(OLD_GENERATION);\n+        _old_to_region = new_to_region;\n+        _old_compact_point = _old_to_region->bottom();\n+        promote_object = true;\n+      }\n+      \/\/ Else this worker thread does not yet have any empty regions into which this aged object can be promoted so\n+      \/\/ we leave promote_object as false, deferring the promotion.\n+    } else {\n+      promote_object = true;\n+    }\n+  }\n+\n+  if (promote_object || (_from_affiliation == ShenandoahAffiliation::OLD_GENERATION)) {\n+    assert(_old_to_region != nullptr, \"_old_to_region should not be nullptr when evacuating to OLD region\");\n+    if (_old_compact_point + obj_size > _old_to_region->end()) {\n+      ShenandoahHeapRegion* new_to_region;\n+\n+      log_debug(gc)(\"Worker %u finishing old region \" SIZE_FORMAT \", compact_point: \" PTR_FORMAT \", obj_size: \" SIZE_FORMAT\n+      \", &compact_point[obj_size]: \" PTR_FORMAT \", region end: \" PTR_FORMAT,  _worker_id, _old_to_region->index(),\n+              p2i(_old_compact_point), obj_size, p2i(_old_compact_point + obj_size), p2i(_old_to_region->end()));\n+\n+      \/\/ Object does not fit.  Get a new _old_to_region.\n+      finish_old_region();\n+      if (_empty_regions_pos < _empty_regions.length()) {\n+        new_to_region = _empty_regions.at(_empty_regions_pos);\n+        _empty_regions_pos++;\n+        new_to_region->set_affiliation(OLD_GENERATION);\n+      } else {\n+        \/\/ If we've exhausted the previously selected _old_to_region, we know that the _old_to_region is distinct\n+        \/\/ from _from_region.  That's because there is always room for _from_region to be compacted into itself.\n+        \/\/ Since we're out of empty regions, let's use _from_region to hold the results of its own compaction.\n+        new_to_region = _from_region;\n+      }\n+\n+      assert(new_to_region != _old_to_region, \"must not reuse same OLD to-region\");\n+      assert(new_to_region != nullptr, \"must not be nullptr\");\n+      _old_to_region = new_to_region;\n+      _old_compact_point = _old_to_region->bottom();\n+    }\n+\n+    \/\/ Object fits into current region, record new location, if object does not move:\n+    assert(_old_compact_point + obj_size <= _old_to_region->end(), \"must fit\");\n+    shenandoah_assert_not_forwarded(nullptr, p);\n+    if (_old_compact_point != cast_from_oop<HeapWord*>(p)) {\n+      _preserved_marks->push_if_necessary(p, p->mark());\n+      p->forward_to(cast_to_oop(_old_compact_point));\n+    }\n+    _old_compact_point += obj_size;\n+  } else {\n+    assert(_from_affiliation == ShenandoahAffiliation::YOUNG_GENERATION,\n+           \"_from_region must be OLD_GENERATION or YOUNG_GENERATION\");\n+    assert(_young_to_region != nullptr, \"_young_to_region should not be nullptr when compacting YOUNG _from_region\");\n+\n+    \/\/ After full gc compaction, all regions have age 0.  Embed the region's age into the object's age in order to preserve\n+    \/\/ tenuring progress.\n+    if (_heap->is_aging_cycle()) {\n+      ShenandoahHeap::increase_object_age(p, from_region_age + 1);\n+    } else {\n+      ShenandoahHeap::increase_object_age(p, from_region_age);\n+    }\n+\n+    if (_young_compact_point + obj_size > _young_to_region->end()) {\n+      ShenandoahHeapRegion* new_to_region;\n+\n+      log_debug(gc)(\"Worker %u finishing young region \" SIZE_FORMAT \", compact_point: \" PTR_FORMAT \", obj_size: \" SIZE_FORMAT\n+      \", &compact_point[obj_size]: \" PTR_FORMAT \", region end: \" PTR_FORMAT,  _worker_id, _young_to_region->index(),\n+              p2i(_young_compact_point), obj_size, p2i(_young_compact_point + obj_size), p2i(_young_to_region->end()));\n+\n+      \/\/ Object does not fit.  Get a new _young_to_region.\n+      finish_young_region();\n+      if (_empty_regions_pos < _empty_regions.length()) {\n+        new_to_region = _empty_regions.at(_empty_regions_pos);\n+        _empty_regions_pos++;\n+        new_to_region->set_affiliation(YOUNG_GENERATION);\n+      } else {\n+        \/\/ If we've exhausted the previously selected _young_to_region, we know that the _young_to_region is distinct\n+        \/\/ from _from_region.  That's because there is always room for _from_region to be compacted into itself.\n+        \/\/ Since we're out of empty regions, let's use _from_region to hold the results of its own compaction.\n+        new_to_region = _from_region;\n+      }\n+\n+      assert(new_to_region != _young_to_region, \"must not reuse same OLD to-region\");\n+      assert(new_to_region != nullptr, \"must not be nullptr\");\n+      _young_to_region = new_to_region;\n+      _young_compact_point = _young_to_region->bottom();\n+    }\n+\n+    \/\/ Object fits into current region, record new location, if object does not move:\n+    assert(_young_compact_point + obj_size <= _young_to_region->end(), \"must fit\");\n+    shenandoah_assert_not_forwarded(nullptr, p);\n+\n+    if (_young_compact_point != cast_from_oop<HeapWord*>(p)) {\n+      _preserved_marks->push_if_necessary(p, p->mark());\n+      p->forward_to(cast_to_oop(_young_compact_point));\n+    }\n+    _young_compact_point += obj_size;\n+  }\n+}\n","filename":"src\/hotspot\/share\/gc\/shenandoah\/shenandoahGenerationalFullGC.cpp","additions":390,"deletions":0,"binary":false,"changes":390,"status":"added"},{"patch":"@@ -0,0 +1,121 @@\n+\/*\n+ * Copyright Amazon.com Inc. or its affiliates. All Rights Reserved.\n+ * DO NOT ALTER OR REMOVE COPYRIGHT NOTICES OR THIS FILE HEADER.\n+ *\n+ * This code is free software; you can redistribute it and\/or modify it\n+ * under the terms of the GNU General Public License version 2 only, as\n+ * published by the Free Software Foundation.\n+ *\n+ * This code is distributed in the hope that it will be useful, but WITHOUT\n+ * ANY WARRANTY; without even the implied warranty of MERCHANTABILITY or\n+ * FITNESS FOR A PARTICULAR PURPOSE.  See the GNU General Public License\n+ * version 2 for more details (a copy is included in the LICENSE file that\n+ * accompanied this code).\n+ *\n+ * You should have received a copy of the GNU General Public License version\n+ * 2 along with this work; if not, write to the Free Software Foundation,\n+ * Inc., 51 Franklin St, Fifth Floor, Boston, MA 02110-1301 USA.\n+ *\n+ * Please contact Oracle, 500 Oracle Parkway, Redwood Shores, CA 94065 USA\n+ * or visit www.oracle.com if you need additional information or have any\n+ * questions.\n+ *\n+ *\/\n+\n+#ifndef SHARE_GC_SHENANDOAH_SHENANDOAHGENERATIONALFULLGC_HPP\n+#define SHARE_GC_SHENANDOAH_SHENANDOAHGENERATIONALFULLGC_HPP\n+\n+#include \"gc\/shared\/preservedMarks.hpp\"\n+#include \"memory\/iterator.hpp\"\n+#include \"oops\/oop.inline.hpp\"\n+#include \"utilities\/growableArray.hpp\"\n+\n+class ShenandoahHeap;\n+class ShenandoahHeapRegion;\n+\n+class ShenandoahGenerationalFullGC {\n+public:\n+  \/\/ Prepares the generational mode heap for a full collection.\n+  static void prepare();\n+\n+  \/\/ Full GC may have compacted objects in the old generation, so we need to rebuild the card tables.\n+  static void rebuild_remembered_set(ShenandoahHeap* heap);\n+\n+  \/\/ Records end of cycle for young and old and establishes size of live bytes in old\n+  static void handle_completion(ShenandoahHeap* heap);\n+\n+  \/\/ Full GC may have promoted regions and may have temporarily violated constraints on the usage and\n+  \/\/ capacity of the old generation. This method will balance the accounting of regions between the\n+  \/\/ young and old generations. This is somewhat vestigial, but the outcome of this method is used\n+  \/\/ when rebuilding the free sets.\n+  static void balance_generations_after_gc(ShenandoahHeap* heap);\n+\n+  \/\/ This will compute the target size for the old generation. It will be expressed in terms of\n+  \/\/ a region surplus and deficit, which will be redistributed accordingly after rebuilding the\n+  \/\/ free set.\n+  static void compute_balances();\n+\n+  \/\/ Rebuilding the free set may have resulted in regions being pulled in to the old generation\n+  \/\/ evacuation reserve. For this reason, we must update the usage and capacity of the generations\n+  \/\/ again. In the distant past, the free set did not know anything about generations, so we had\n+  \/\/ a layer built above it to represent how much young\/old memory was available. This layer is\n+  \/\/ redundant and adds complexity. We would like to one day remove it. Until then, we must keep it\n+  \/\/ synchronized with the free set's view of things.\n+  static void balance_generations_after_rebuilding_free_set();\n+\n+  \/\/ Logs the number of live bytes marked in the old generation. This is _not_ the same\n+  \/\/ value used as the baseline for the old generation _after_ the full gc is complete.\n+  \/\/ The value reported in the logs does not include objects and regions that may be\n+  \/\/ promoted during the full gc.\n+  static void log_live_in_old(ShenandoahHeap* heap);\n+\n+  \/\/ This is used to tally the number, usage and space wasted by humongous objects for each generation.\n+  static void account_for_region(ShenandoahHeapRegion* r, size_t &region_count, size_t &region_usage, size_t &humongous_waste);\n+\n+  \/\/ Regions which are scheduled for in-place promotion during evacuation temporarily\n+  \/\/ have their top set to their end to prevent new objects from being allocated in them\n+  \/\/ before they are promoted. If the full GC encounters such a region, it means the\n+  \/\/ in-place promotion did not happen, and we must restore the original value of top.\n+  static void restore_top_before_promote(ShenandoahHeap* heap);\n+\n+  \/\/ Pinned regions are not compacted, so they may still hold unmarked objects with\n+  \/\/ references to reclaimed memory. Remembered set scanning will crash if it attempts\n+  \/\/ to iterate the oops in these objects. This method fills in dead objects for pinned,\n+  \/\/ old regions.\n+  static void maybe_coalesce_and_fill_region(ShenandoahHeapRegion* r);\n+};\n+\n+class ShenandoahPrepareForGenerationalCompactionObjectClosure : public ObjectClosure {\n+private:\n+  PreservedMarks*             const _preserved_marks;\n+  ShenandoahGenerationalHeap* const _heap;\n+\n+  \/\/ _empty_regions is a thread-local list of heap regions that have been completely emptied by this worker thread's\n+  \/\/ compaction efforts.  The worker thread that drives these efforts adds compacted regions to this list if the\n+  \/\/ region has not been compacted onto itself.\n+  GrowableArray<ShenandoahHeapRegion*>& _empty_regions;\n+  int _empty_regions_pos;\n+  ShenandoahHeapRegion*          _old_to_region;\n+  ShenandoahHeapRegion*          _young_to_region;\n+  ShenandoahHeapRegion*          _from_region;\n+  ShenandoahAffiliation          _from_affiliation;\n+  HeapWord*                      _old_compact_point;\n+  HeapWord*                      _young_compact_point;\n+  uint                           _worker_id;\n+\n+public:\n+  ShenandoahPrepareForGenerationalCompactionObjectClosure(PreservedMarks* preserved_marks,\n+                                                          GrowableArray<ShenandoahHeapRegion*>& empty_regions,\n+                                                          ShenandoahHeapRegion* from_region, uint worker_id);\n+\n+  void set_from_region(ShenandoahHeapRegion* from_region);\n+  void finish();\n+  void finish_old_region();\n+  void finish_young_region();\n+  bool is_compact_same_region();\n+  int empty_regions_pos() const { return _empty_regions_pos; }\n+\n+  void do_object(oop p) override;\n+};\n+\n+#endif \/\/SHARE_GC_SHENANDOAH_SHENANDOAHGENERATIONALFULLGC_HPP\n","filename":"src\/hotspot\/share\/gc\/shenandoah\/shenandoahGenerationalFullGC.hpp","additions":121,"deletions":0,"binary":false,"changes":121,"status":"added"},{"patch":"@@ -0,0 +1,1148 @@\n+\/*\n+ * Copyright Amazon.com Inc. or its affiliates. All Rights Reserved.\n+ * DO NOT ALTER OR REMOVE COPYRIGHT NOTICES OR THIS FILE HEADER.\n+ *\n+ * This code is free software; you can redistribute it and\/or modify it\n+ * under the terms of the GNU General Public License version 2 only, as\n+ * published by the Free Software Foundation.\n+ *\n+ * This code is distributed in the hope that it will be useful, but WITHOUT\n+ * ANY WARRANTY; without even the implied warranty of MERCHANTABILITY or\n+ * FITNESS FOR A PARTICULAR PURPOSE.  See the GNU General Public License\n+ * version 2 for more details (a copy is included in the LICENSE file that\n+ * accompanied this code).\n+ *\n+ * You should have received a copy of the GNU General Public License version\n+ * 2 along with this work; if not, write to the Free Software Foundation,\n+ * Inc., 51 Franklin St, Fifth Floor, Boston, MA 02110-1301 USA.\n+ *\n+ * Please contact Oracle, 500 Oracle Parkway, Redwood Shores, CA 94065 USA\n+ * or visit www.oracle.com if you need additional information or have any\n+ * questions.\n+ *\n+ *\/\n+\n+#include \"precompiled.hpp\"\n+\n+#include \"gc\/shenandoah\/shenandoahAgeCensus.hpp\"\n+#include \"gc\/shenandoah\/shenandoahCollectorPolicy.hpp\"\n+#include \"gc\/shenandoah\/shenandoahFreeSet.hpp\"\n+#include \"gc\/shenandoah\/shenandoahGenerationalControlThread.hpp\"\n+#include \"gc\/shenandoah\/shenandoahGenerationalEvacuationTask.hpp\"\n+#include \"gc\/shenandoah\/shenandoahGenerationalHeap.hpp\"\n+#include \"gc\/shenandoah\/shenandoahHeap.inline.hpp\"\n+#include \"gc\/shenandoah\/shenandoahHeapRegion.hpp\"\n+#include \"gc\/shenandoah\/shenandoahHeapRegionClosures.hpp\"\n+#include \"gc\/shenandoah\/shenandoahInitLogger.hpp\"\n+#include \"gc\/shenandoah\/shenandoahMemoryPool.hpp\"\n+#include \"gc\/shenandoah\/shenandoahMonitoringSupport.hpp\"\n+#include \"gc\/shenandoah\/shenandoahOldGeneration.hpp\"\n+#include \"gc\/shenandoah\/shenandoahOopClosures.inline.hpp\"\n+#include \"gc\/shenandoah\/shenandoahPhaseTimings.hpp\"\n+#include \"gc\/shenandoah\/shenandoahRegulatorThread.hpp\"\n+#include \"gc\/shenandoah\/shenandoahScanRemembered.inline.hpp\"\n+#include \"gc\/shenandoah\/shenandoahWorkerPolicy.hpp\"\n+#include \"gc\/shenandoah\/shenandoahYoungGeneration.hpp\"\n+#include \"gc\/shenandoah\/shenandoahUtils.hpp\"\n+#include \"logging\/log.hpp\"\n+#include \"utilities\/events.hpp\"\n+\n+\n+class ShenandoahGenerationalInitLogger : public ShenandoahInitLogger {\n+public:\n+  static void print() {\n+    ShenandoahGenerationalInitLogger logger;\n+    logger.print_all();\n+  }\n+protected:\n+  void print_gc_specific() override {\n+    ShenandoahInitLogger::print_gc_specific();\n+\n+    ShenandoahGenerationalHeap* heap = ShenandoahGenerationalHeap::heap();\n+    log_info(gc, init)(\"Young Heuristics: %s\", heap->young_generation()->heuristics()->name());\n+    log_info(gc, init)(\"Old Heuristics: %s\", heap->old_generation()->heuristics()->name());\n+  }\n+};\n+\n+size_t ShenandoahGenerationalHeap::calculate_min_plab() {\n+  return align_up(PLAB::min_size(), CardTable::card_size_in_words());\n+}\n+\n+size_t ShenandoahGenerationalHeap::calculate_max_plab() {\n+  size_t MaxTLABSizeWords = ShenandoahHeapRegion::max_tlab_size_words();\n+  return align_down(MaxTLABSizeWords, CardTable::card_size_in_words());\n+}\n+\n+\/\/ Returns size in bytes\n+size_t ShenandoahGenerationalHeap::unsafe_max_tlab_alloc(Thread *thread) const {\n+  return MIN2(ShenandoahHeapRegion::max_tlab_size_bytes(), young_generation()->available());\n+}\n+\n+ShenandoahGenerationalHeap::ShenandoahGenerationalHeap(ShenandoahCollectorPolicy* policy) :\n+  ShenandoahHeap(policy),\n+  _age_census(nullptr),\n+  _evac_tracker(new ShenandoahEvacuationTracker()),\n+  _min_plab_size(calculate_min_plab()),\n+  _max_plab_size(calculate_max_plab()),\n+  _regulator_thread(nullptr),\n+  _young_gen_memory_pool(nullptr),\n+  _old_gen_memory_pool(nullptr) {\n+  assert(is_aligned(_min_plab_size, CardTable::card_size_in_words()), \"min_plab_size must be aligned\");\n+  assert(is_aligned(_max_plab_size, CardTable::card_size_in_words()), \"max_plab_size must be aligned\");\n+}\n+\n+void ShenandoahGenerationalHeap::post_initialize() {\n+  ShenandoahHeap::post_initialize();\n+  _age_census = new ShenandoahAgeCensus();\n+}\n+\n+void ShenandoahGenerationalHeap::print_init_logger() const {\n+  ShenandoahGenerationalInitLogger logger;\n+  logger.print_all();\n+}\n+\n+void ShenandoahGenerationalHeap::print_tracing_info() const {\n+  ShenandoahHeap::print_tracing_info();\n+\n+  LogTarget(Info, gc, stats) lt;\n+  if (lt.is_enabled()) {\n+    LogStream ls(lt);\n+    ls.cr();\n+    ls.cr();\n+    evac_tracker()->print_global_on(&ls);\n+  }\n+}\n+\n+void ShenandoahGenerationalHeap::initialize_heuristics() {\n+  \/\/ Initialize global generation and heuristics even in generational mode.\n+  ShenandoahHeap::initialize_heuristics();\n+\n+  \/\/ Max capacity is the maximum _allowed_ capacity. That is, the maximum allowed capacity\n+  \/\/ for old would be total heap - minimum capacity of young. This means the sum of the maximum\n+  \/\/ allowed for old and young could exceed the total heap size. It remains the case that the\n+  \/\/ _actual_ capacity of young + old = total.\n+  _generation_sizer.heap_size_changed(max_capacity());\n+  size_t initial_capacity_young = _generation_sizer.max_young_size();\n+  size_t max_capacity_young = _generation_sizer.max_young_size();\n+  size_t initial_capacity_old = max_capacity() - max_capacity_young;\n+  size_t max_capacity_old = max_capacity() - initial_capacity_young;\n+\n+  _young_generation = new ShenandoahYoungGeneration(max_workers(), max_capacity_young);\n+  _old_generation = new ShenandoahOldGeneration(max_workers(), max_capacity_old);\n+  _young_generation->initialize_heuristics(mode());\n+  _old_generation->initialize_heuristics(mode());\n+}\n+\n+void ShenandoahGenerationalHeap::initialize_serviceability() {\n+  assert(mode()->is_generational(), \"Only for the generational mode\");\n+  _young_gen_memory_pool = new ShenandoahYoungGenMemoryPool(this);\n+  _old_gen_memory_pool = new ShenandoahOldGenMemoryPool(this);\n+  cycle_memory_manager()->add_pool(_young_gen_memory_pool);\n+  cycle_memory_manager()->add_pool(_old_gen_memory_pool);\n+  stw_memory_manager()->add_pool(_young_gen_memory_pool);\n+  stw_memory_manager()->add_pool(_old_gen_memory_pool);\n+}\n+\n+GrowableArray<MemoryPool*> ShenandoahGenerationalHeap::memory_pools() {\n+  assert(mode()->is_generational(), \"Only for the generational mode\");\n+  GrowableArray<MemoryPool*> memory_pools(2);\n+  memory_pools.append(_young_gen_memory_pool);\n+  memory_pools.append(_old_gen_memory_pool);\n+  return memory_pools;\n+}\n+\n+void ShenandoahGenerationalHeap::initialize_controller() {\n+  auto control_thread = new ShenandoahGenerationalControlThread();\n+  _control_thread = control_thread;\n+  _regulator_thread = new ShenandoahRegulatorThread(control_thread);\n+}\n+\n+void ShenandoahGenerationalHeap::gc_threads_do(ThreadClosure* tcl) const {\n+  if (!shenandoah_policy()->is_at_shutdown()) {\n+    ShenandoahHeap::gc_threads_do(tcl);\n+    tcl->do_thread(regulator_thread());\n+  }\n+}\n+\n+void ShenandoahGenerationalHeap::stop() {\n+  ShenandoahHeap::stop();\n+  regulator_thread()->stop();\n+}\n+\n+bool ShenandoahGenerationalHeap::requires_barriers(stackChunkOop obj) const {\n+  if (is_idle()) {\n+    return false;\n+  }\n+\n+  if (is_concurrent_young_mark_in_progress() && is_in_young(obj) && !marking_context()->allocated_after_mark_start(obj)) {\n+    \/\/ We are marking young, this object is in young, and it is below the TAMS\n+    return true;\n+  }\n+\n+  if (is_in_old(obj)) {\n+    \/\/ Card marking barriers are required for objects in the old generation\n+    return true;\n+  }\n+\n+  if (has_forwarded_objects()) {\n+    \/\/ Object may have pointers that need to be updated\n+    return true;\n+  }\n+\n+  return false;\n+}\n+\n+void ShenandoahGenerationalHeap::evacuate_collection_set(bool concurrent) {\n+  ShenandoahRegionIterator regions;\n+  ShenandoahGenerationalEvacuationTask task(this, &regions, concurrent, false \/* only promote regions *\/);\n+  workers()->run_task(&task);\n+}\n+\n+void ShenandoahGenerationalHeap::promote_regions_in_place(bool concurrent) {\n+  ShenandoahRegionIterator regions;\n+  ShenandoahGenerationalEvacuationTask task(this, &regions, concurrent, true \/* only promote regions *\/);\n+  workers()->run_task(&task);\n+}\n+\n+oop ShenandoahGenerationalHeap::evacuate_object(oop p, Thread* thread) {\n+  assert(thread == Thread::current(), \"Expected thread parameter to be current thread.\");\n+  if (ShenandoahThreadLocalData::is_oom_during_evac(thread)) {\n+    \/\/ This thread went through the OOM during evac protocol and it is safe to return\n+    \/\/ the forward pointer. It must not attempt to evacuate anymore.\n+    return ShenandoahBarrierSet::resolve_forwarded(p);\n+  }\n+\n+  assert(ShenandoahThreadLocalData::is_evac_allowed(thread), \"must be enclosed in oom-evac scope\");\n+\n+  ShenandoahHeapRegion* r = heap_region_containing(p);\n+  assert(!r->is_humongous(), \"never evacuate humongous objects\");\n+\n+  ShenandoahAffiliation target_gen = r->affiliation();\n+  \/\/ gc_generation() can change asynchronously and should not be used here.\n+  assert(active_generation() != nullptr, \"Error\");\n+  if (active_generation()->is_young() && target_gen == YOUNG_GENERATION) {\n+    markWord mark = p->mark();\n+    if (mark.is_marked()) {\n+      \/\/ Already forwarded.\n+      return ShenandoahBarrierSet::resolve_forwarded(p);\n+    }\n+\n+    if (mark.has_displaced_mark_helper()) {\n+      \/\/ We don't want to deal with MT here just to ensure we read the right mark word.\n+      \/\/ Skip the potential promotion attempt for this one.\n+    } else if (age_census()->is_tenurable(r->age() + mark.age())) {\n+      oop result = try_evacuate_object(p, thread, r, OLD_GENERATION);\n+      if (result != nullptr) {\n+        return result;\n+      }\n+      \/\/ If we failed to promote this aged object, we'll fall through to code below and evacuate to young-gen.\n+    }\n+  }\n+  return try_evacuate_object(p, thread, r, target_gen);\n+}\n+\n+\/\/ try_evacuate_object registers the object and dirties the associated remembered set information when evacuating\n+\/\/ to OLD_GENERATION.\n+oop ShenandoahGenerationalHeap::try_evacuate_object(oop p, Thread* thread, ShenandoahHeapRegion* from_region,\n+                                        ShenandoahAffiliation target_gen) {\n+  bool alloc_from_lab = true;\n+  bool has_plab = false;\n+  HeapWord* copy = nullptr;\n+  size_t size = p->size();\n+  bool is_promotion = (target_gen == OLD_GENERATION) && from_region->is_young();\n+\n+#ifdef ASSERT\n+  if (ShenandoahOOMDuringEvacALot &&\n+      (os::random() & 1) == 0) { \/\/ Simulate OOM every ~2nd slow-path call\n+    copy = nullptr;\n+  } else {\n+#endif\n+    if (UseTLAB) {\n+      switch (target_gen) {\n+        case YOUNG_GENERATION: {\n+          copy = allocate_from_gclab(thread, size);\n+          if ((copy == nullptr) && (size < ShenandoahThreadLocalData::gclab_size(thread))) {\n+            \/\/ GCLAB allocation failed because we are bumping up against the limit on young evacuation reserve.  Try resetting\n+            \/\/ the desired GCLAB size and retry GCLAB allocation to avoid cascading of shared memory allocations.\n+            ShenandoahThreadLocalData::set_gclab_size(thread, PLAB::min_size());\n+            copy = allocate_from_gclab(thread, size);\n+            \/\/ If we still get nullptr, we'll try a shared allocation below.\n+          }\n+          break;\n+        }\n+        case OLD_GENERATION: {\n+          PLAB* plab = ShenandoahThreadLocalData::plab(thread);\n+          if (plab != nullptr) {\n+            has_plab = true;\n+            copy = allocate_from_plab(thread, size, is_promotion);\n+            if ((copy == nullptr) && (size < ShenandoahThreadLocalData::plab_size(thread)) &&\n+                ShenandoahThreadLocalData::plab_retries_enabled(thread)) {\n+              \/\/ PLAB allocation failed because we are bumping up against the limit on old evacuation reserve or because\n+              \/\/ the requested object does not fit within the current plab but the plab still has an \"abundance\" of memory,\n+              \/\/ where abundance is defined as >= ShenGenHeap::plab_min_size().  In the former case, we try shrinking the\n+              \/\/ desired PLAB size to the minimum and retry PLAB allocation to avoid cascading of shared memory allocations.\n+              if (plab->words_remaining() < plab_min_size()) {\n+                ShenandoahThreadLocalData::set_plab_size(thread, plab_min_size());\n+                copy = allocate_from_plab(thread, size, is_promotion);\n+                \/\/ If we still get nullptr, we'll try a shared allocation below.\n+                if (copy == nullptr) {\n+                  \/\/ If retry fails, don't continue to retry until we have success (probably in next GC pass)\n+                  ShenandoahThreadLocalData::disable_plab_retries(thread);\n+                }\n+              }\n+              \/\/ else, copy still equals nullptr.  this causes shared allocation below, preserving this plab for future needs.\n+            }\n+          }\n+          break;\n+        }\n+        default: {\n+          ShouldNotReachHere();\n+          break;\n+        }\n+      }\n+    }\n+\n+    if (copy == nullptr) {\n+      \/\/ If we failed to allocate in LAB, we'll try a shared allocation.\n+      if (!is_promotion || !has_plab || (size > PLAB::min_size())) {\n+        ShenandoahAllocRequest req = ShenandoahAllocRequest::for_shared_gc(size, target_gen, is_promotion);\n+        copy = allocate_memory(req);\n+        alloc_from_lab = false;\n+      }\n+      \/\/ else, we leave copy equal to nullptr, signaling a promotion failure below if appropriate.\n+      \/\/ We choose not to promote objects smaller than PLAB::min_size() by way of shared allocations, as this is too\n+      \/\/ costly.  Instead, we'll simply \"evacuate\" to young-gen memory (using a GCLAB) and will promote in a future\n+      \/\/ evacuation pass.  This condition is denoted by: is_promotion && has_plab && (size <= PLAB::min_size())\n+    }\n+#ifdef ASSERT\n+  }\n+#endif\n+\n+  if (copy == nullptr) {\n+    if (target_gen == OLD_GENERATION) {\n+      if (from_region->is_young()) {\n+        \/\/ Signal that promotion failed. Will evacuate this old object somewhere in young gen.\n+        old_generation()->handle_failed_promotion(thread, size);\n+        return nullptr;\n+      } else {\n+        \/\/ Remember that evacuation to old gen failed. We'll want to trigger a full gc to recover from this\n+        \/\/ after the evacuation threads have finished.\n+        old_generation()->handle_failed_evacuation();\n+      }\n+    }\n+\n+    control_thread()->handle_alloc_failure_evac(size);\n+\n+    oom_evac_handler()->handle_out_of_memory_during_evacuation();\n+\n+    return ShenandoahBarrierSet::resolve_forwarded(p);\n+  }\n+\n+  \/\/ Copy the object:\n+  NOT_PRODUCT(evac_tracker()->begin_evacuation(thread, size * HeapWordSize));\n+  Copy::aligned_disjoint_words(cast_from_oop<HeapWord*>(p), copy, size);\n+  oop copy_val = cast_to_oop(copy);\n+\n+  \/\/ Update the age of the evacuated object\n+  if (target_gen == YOUNG_GENERATION && is_aging_cycle()) {\n+    ShenandoahHeap::increase_object_age(copy_val, from_region->age() + 1);\n+  }\n+\n+  \/\/ Try to install the new forwarding pointer.\n+  oop result = ShenandoahForwarding::try_update_forwardee(p, copy_val);\n+  if (result == copy_val) {\n+    \/\/ Successfully evacuated. Our copy is now the public one!\n+\n+    \/\/ This is necessary for virtual thread support. This uses the mark word without\n+    \/\/ considering that it may now be a forwarding pointer (and could therefore crash).\n+    \/\/ Secondarily, we do not want to spend cycles relativizing stack chunks for oops\n+    \/\/ that lost the evacuation race (and will therefore not become visible). It is\n+    \/\/ safe to do this on the public copy (this is also done during concurrent mark).\n+    ContinuationGCSupport::relativize_stack_chunk(copy_val);\n+\n+    \/\/ Record that the evacuation succeeded\n+    NOT_PRODUCT(evac_tracker()->end_evacuation(thread, size * HeapWordSize));\n+\n+    if (target_gen == OLD_GENERATION) {\n+      old_generation()->handle_evacuation(copy, size, from_region->is_young());\n+    } else {\n+      \/\/ When copying to the old generation above, we don't care\n+      \/\/ about recording object age in the census stats.\n+      assert(target_gen == YOUNG_GENERATION, \"Error\");\n+      \/\/ We record this census only when simulating pre-adaptive tenuring behavior, or\n+      \/\/ when we have been asked to record the census at evacuation rather than at mark\n+      if (ShenandoahGenerationalCensusAtEvac || !ShenandoahGenerationalAdaptiveTenuring) {\n+        evac_tracker()->record_age(thread, size * HeapWordSize, ShenandoahHeap::get_object_age(copy_val));\n+      }\n+    }\n+    shenandoah_assert_correct(nullptr, copy_val);\n+    return copy_val;\n+  }  else {\n+    \/\/ Failed to evacuate. We need to deal with the object that is left behind. Since this\n+    \/\/ new allocation is certainly after TAMS, it will be considered live in the next cycle.\n+    \/\/ But if it happens to contain references to evacuated regions, those references would\n+    \/\/ not get updated for this stale copy during this cycle, and we will crash while scanning\n+    \/\/ it the next cycle.\n+    if (alloc_from_lab) {\n+      \/\/ For LAB allocations, it is enough to rollback the allocation ptr. Either the next\n+      \/\/ object will overwrite this stale copy, or the filler object on LAB retirement will\n+      \/\/ do this.\n+      switch (target_gen) {\n+        case YOUNG_GENERATION: {\n+          ShenandoahThreadLocalData::gclab(thread)->undo_allocation(copy, size);\n+          break;\n+        }\n+        case OLD_GENERATION: {\n+          ShenandoahThreadLocalData::plab(thread)->undo_allocation(copy, size);\n+          if (is_promotion) {\n+            ShenandoahThreadLocalData::subtract_from_plab_promoted(thread, size * HeapWordSize);\n+          }\n+          break;\n+        }\n+        default: {\n+          ShouldNotReachHere();\n+          break;\n+        }\n+      }\n+    } else {\n+      \/\/ For non-LAB allocations, we have no way to retract the allocation, and\n+      \/\/ have to explicitly overwrite the copy with the filler object. With that overwrite,\n+      \/\/ we have to keep the fwdptr initialized and pointing to our (stale) copy.\n+      assert(size >= ShenandoahHeap::min_fill_size(), \"previously allocated object known to be larger than min_size\");\n+      fill_with_object(copy, size);\n+      shenandoah_assert_correct(nullptr, copy_val);\n+      \/\/ For non-LAB allocations, the object has already been registered\n+    }\n+    shenandoah_assert_correct(nullptr, result);\n+    return result;\n+  }\n+}\n+\n+inline HeapWord* ShenandoahGenerationalHeap::allocate_from_plab(Thread* thread, size_t size, bool is_promotion) {\n+  assert(UseTLAB, \"TLABs should be enabled\");\n+\n+  PLAB* plab = ShenandoahThreadLocalData::plab(thread);\n+  HeapWord* obj;\n+\n+  if (plab == nullptr) {\n+    assert(!thread->is_Java_thread() && !thread->is_Worker_thread(), \"Performance: thread should have PLAB: %s\", thread->name());\n+    \/\/ No PLABs in this thread, fallback to shared allocation\n+    return nullptr;\n+  } else if (is_promotion && !ShenandoahThreadLocalData::allow_plab_promotions(thread)) {\n+    return nullptr;\n+  }\n+  \/\/ if plab->word_size() <= 0, thread's plab not yet initialized for this pass, so allow_plab_promotions() is not trustworthy\n+  obj = plab->allocate(size);\n+  if ((obj == nullptr) && (plab->words_remaining() < plab_min_size())) {\n+    \/\/ allocate_from_plab_slow will establish allow_plab_promotions(thread) for future invocations\n+    obj = allocate_from_plab_slow(thread, size, is_promotion);\n+  }\n+  \/\/ if plab->words_remaining() >= ShenGenHeap::heap()->plab_min_size(), just return nullptr so we can use a shared allocation\n+  if (obj == nullptr) {\n+    return nullptr;\n+  }\n+\n+  if (is_promotion) {\n+    ShenandoahThreadLocalData::add_to_plab_promoted(thread, size * HeapWordSize);\n+  }\n+  return obj;\n+}\n+\n+\/\/ Establish a new PLAB and allocate size HeapWords within it.\n+HeapWord* ShenandoahGenerationalHeap::allocate_from_plab_slow(Thread* thread, size_t size, bool is_promotion) {\n+  \/\/ New object should fit the PLAB size\n+\n+  assert(mode()->is_generational(), \"PLABs only relevant to generational GC\");\n+  const size_t plab_min_size = this->plab_min_size();\n+  \/\/ PLABs are aligned to card boundaries to avoid synchronization with concurrent\n+  \/\/ allocations in other PLABs.\n+  const size_t min_size = (size > plab_min_size)? align_up(size, CardTable::card_size_in_words()): plab_min_size;\n+\n+  \/\/ Figure out size of new PLAB, using value determined at last refill.\n+  size_t cur_size = ShenandoahThreadLocalData::plab_size(thread);\n+  if (cur_size == 0) {\n+    cur_size = plab_min_size;\n+  }\n+\n+  \/\/ Expand aggressively, doubling at each refill in this epoch, ceiling at plab_max_size()\n+  size_t future_size = MIN2(cur_size * 2, plab_max_size());\n+  \/\/ Doubling, starting at a card-multiple, should give us a card-multiple. (Ceiling and floor\n+  \/\/ are card multiples.)\n+  assert(is_aligned(future_size, CardTable::card_size_in_words()), \"Card multiple by construction, future_size: \" SIZE_FORMAT\n+          \", card_size: \" SIZE_FORMAT \", cur_size: \" SIZE_FORMAT \", max: \" SIZE_FORMAT,\n+         future_size, (size_t) CardTable::card_size_in_words(), cur_size, plab_max_size());\n+\n+  \/\/ Record new heuristic value even if we take any shortcut. This captures\n+  \/\/ the case when moderately-sized objects always take a shortcut. At some point,\n+  \/\/ heuristics should catch up with them.  Note that the requested cur_size may\n+  \/\/ not be honored, but we remember that this is the preferred size.\n+  log_debug(gc, free)(\"Set new PLAB size: \" SIZE_FORMAT, future_size);\n+  ShenandoahThreadLocalData::set_plab_size(thread, future_size);\n+  if (cur_size < size) {\n+    \/\/ The PLAB to be allocated is still not large enough to hold the object. Fall back to shared allocation.\n+    \/\/ This avoids retiring perfectly good PLABs in order to represent a single large object allocation.\n+    log_debug(gc, free)(\"Current PLAB size (\" SIZE_FORMAT \") is too small for \" SIZE_FORMAT, cur_size, size);\n+    return nullptr;\n+  }\n+\n+  \/\/ Retire current PLAB, and allocate a new one.\n+  PLAB* plab = ShenandoahThreadLocalData::plab(thread);\n+  if (plab->words_remaining() < plab_min_size) {\n+    \/\/ Retire current PLAB. This takes care of any PLAB book-keeping.\n+    \/\/ retire_plab() registers the remnant filler object with the remembered set scanner without a lock.\n+    \/\/ Since PLABs are card-aligned, concurrent registrations in other PLABs don't interfere.\n+    retire_plab(plab, thread);\n+\n+    size_t actual_size = 0;\n+    HeapWord* plab_buf = allocate_new_plab(min_size, cur_size, &actual_size);\n+    if (plab_buf == nullptr) {\n+      if (min_size == plab_min_size) {\n+        \/\/ Disable PLAB promotions for this thread because we cannot even allocate a minimal PLAB. This allows us\n+        \/\/ to fail faster on subsequent promotion attempts.\n+        ShenandoahThreadLocalData::disable_plab_promotions(thread);\n+      }\n+      return nullptr;\n+    } else {\n+      ShenandoahThreadLocalData::enable_plab_retries(thread);\n+    }\n+    \/\/ Since the allocated PLAB may have been down-sized for alignment, plab->allocate(size) below may still fail.\n+    if (ZeroTLAB) {\n+      \/\/ ... and clear it.\n+      Copy::zero_to_words(plab_buf, actual_size);\n+    } else {\n+      \/\/ ...and zap just allocated object.\n+#ifdef ASSERT\n+      \/\/ Skip mangling the space corresponding to the object header to\n+      \/\/ ensure that the returned space is not considered parsable by\n+      \/\/ any concurrent GC thread.\n+      size_t hdr_size = oopDesc::header_size();\n+      Copy::fill_to_words(plab_buf + hdr_size, actual_size - hdr_size, badHeapWordVal);\n+#endif \/\/ ASSERT\n+    }\n+    assert(is_aligned(actual_size, CardTable::card_size_in_words()), \"Align by design\");\n+    plab->set_buf(plab_buf, actual_size);\n+    if (is_promotion && !ShenandoahThreadLocalData::allow_plab_promotions(thread)) {\n+      return nullptr;\n+    }\n+    return plab->allocate(size);\n+  } else {\n+    \/\/ If there's still at least min_size() words available within the current plab, don't retire it.  Let's nibble\n+    \/\/ away on this plab as long as we can.  Meanwhile, return nullptr to force this particular allocation request\n+    \/\/ to be satisfied with a shared allocation.  By packing more promotions into the previously allocated PLAB, we\n+    \/\/ reduce the likelihood of evacuation failures, and we reduce the need for downsizing our PLABs.\n+    return nullptr;\n+  }\n+}\n+\n+HeapWord* ShenandoahGenerationalHeap::allocate_new_plab(size_t min_size, size_t word_size, size_t* actual_size) {\n+  \/\/ Align requested sizes to card-sized multiples.  Align down so that we don't violate max size of TLAB.\n+  assert(is_aligned(min_size, CardTable::card_size_in_words()), \"Align by design\");\n+  assert(word_size >= min_size, \"Requested PLAB is too small\");\n+\n+  ShenandoahAllocRequest req = ShenandoahAllocRequest::for_plab(min_size, word_size);\n+  \/\/ Note that allocate_memory() sets a thread-local flag to prohibit further promotions by this thread\n+  \/\/ if we are at risk of infringing on the old-gen evacuation budget.\n+  HeapWord* res = allocate_memory(req);\n+  if (res != nullptr) {\n+    *actual_size = req.actual_size();\n+  } else {\n+    *actual_size = 0;\n+  }\n+  assert(is_aligned(res, CardTable::card_size_in_words()), \"Align by design\");\n+  return res;\n+}\n+\n+void ShenandoahGenerationalHeap::retire_plab(PLAB* plab, Thread* thread) {\n+  \/\/ We don't enforce limits on plab evacuations.  We let it consume all available old-gen memory in order to reduce\n+  \/\/ probability of an evacuation failure.  We do enforce limits on promotion, to make sure that excessive promotion\n+  \/\/ does not result in an old-gen evacuation failure.  Note that a failed promotion is relatively harmless.  Any\n+  \/\/ object that fails to promote in the current cycle will be eligible for promotion in a subsequent cycle.\n+\n+  \/\/ When the plab was instantiated, its entirety was treated as if the entire buffer was going to be dedicated to\n+  \/\/ promotions.  Now that we are retiring the buffer, we adjust for the reality that the plab is not entirely promotions.\n+  \/\/  1. Some of the plab may have been dedicated to evacuations.\n+  \/\/  2. Some of the plab may have been abandoned due to waste (at the end of the plab).\n+  size_t not_promoted =\n+          ShenandoahThreadLocalData::get_plab_actual_size(thread) - ShenandoahThreadLocalData::get_plab_promoted(thread);\n+  ShenandoahThreadLocalData::reset_plab_promoted(thread);\n+  ShenandoahThreadLocalData::set_plab_actual_size(thread, 0);\n+  if (not_promoted > 0) {\n+    old_generation()->unexpend_promoted(not_promoted);\n+  }\n+  const size_t original_waste = plab->waste();\n+  HeapWord* const top = plab->top();\n+\n+  \/\/ plab->retire() overwrites unused memory between plab->top() and plab->hard_end() with a dummy object to make memory parsable.\n+  \/\/ It adds the size of this unused memory, in words, to plab->waste().\n+  plab->retire();\n+  if (top != nullptr && plab->waste() > original_waste && is_in_old(top)) {\n+    \/\/ If retiring the plab created a filler object, then we need to register it with our card scanner so it can\n+    \/\/ safely walk the region backing the plab.\n+    log_debug(gc)(\"retire_plab() is registering remnant of size \" SIZE_FORMAT \" at \" PTR_FORMAT,\n+                  plab->waste() - original_waste, p2i(top));\n+    \/\/ No lock is necessary because the PLAB memory is aligned on card boundaries.\n+    old_generation()->card_scan()->register_object_without_lock(top);\n+  }\n+}\n+\n+void ShenandoahGenerationalHeap::retire_plab(PLAB* plab) {\n+  Thread* thread = Thread::current();\n+  retire_plab(plab, thread);\n+}\n+\n+ShenandoahGenerationalHeap::TransferResult ShenandoahGenerationalHeap::balance_generations() {\n+  shenandoah_assert_heaplocked_or_safepoint();\n+\n+  ShenandoahOldGeneration* old_gen = old_generation();\n+  const ssize_t old_region_balance = old_gen->get_region_balance();\n+  old_gen->set_region_balance(0);\n+\n+  if (old_region_balance > 0) {\n+    const auto old_region_surplus = checked_cast<size_t>(old_region_balance);\n+    const bool success = generation_sizer()->transfer_to_young(old_region_surplus);\n+    return TransferResult {\n+      success, old_region_surplus, \"young\"\n+    };\n+  }\n+\n+  if (old_region_balance < 0) {\n+    const auto old_region_deficit = checked_cast<size_t>(-old_region_balance);\n+    const bool success = generation_sizer()->transfer_to_old(old_region_deficit);\n+    if (!success) {\n+      old_gen->handle_failed_transfer();\n+    }\n+    return TransferResult {\n+      success, old_region_deficit, \"old\"\n+    };\n+  }\n+\n+  return TransferResult {true, 0, \"none\"};\n+}\n+\n+\/\/ Make sure old-generation is large enough, but no larger than is necessary, to hold mixed evacuations\n+\/\/ and promotions, if we anticipate either. Any deficit is provided by the young generation, subject to\n+\/\/ xfer_limit, and any surplus is transferred to the young generation.\n+\/\/ xfer_limit is the maximum we're able to transfer from young to old.\n+void ShenandoahGenerationalHeap::compute_old_generation_balance(size_t old_xfer_limit, size_t old_cset_regions) {\n+\n+  \/\/ We can limit the old reserve to the size of anticipated promotions:\n+  \/\/ max_old_reserve is an upper bound on memory evacuated from old and promoted to old,\n+  \/\/ clamped by the old generation space available.\n+  \/\/\n+  \/\/ Here's the algebra.\n+  \/\/ Let SOEP = ShenandoahOldEvacRatioPercent,\n+  \/\/     OE = old evac,\n+  \/\/     YE = young evac, and\n+  \/\/     TE = total evac = OE + YE\n+  \/\/ By definition:\n+  \/\/            SOEP\/100 = OE\/TE\n+  \/\/                     = OE\/(OE+YE)\n+  \/\/  => SOEP\/(100-SOEP) = OE\/((OE+YE)-OE)      \/\/ componendo-dividendo: If a\/b = c\/d, then a\/(b-a) = c\/(d-c)\n+  \/\/                     = OE\/YE\n+  \/\/  =>              OE = YE*SOEP\/(100-SOEP)\n+\n+  \/\/ We have to be careful in the event that SOEP is set to 100 by the user.\n+  assert(ShenandoahOldEvacRatioPercent <= 100, \"Error\");\n+  const size_t old_available = old_generation()->available();\n+  \/\/ The free set will reserve this amount of memory to hold young evacuations\n+  const size_t young_reserve = (young_generation()->max_capacity() * ShenandoahEvacReserve) \/ 100;\n+\n+  \/\/ In the case that ShenandoahOldEvacRatioPercent equals 100, max_old_reserve is limited only by xfer_limit.\n+\n+  const double bound_on_old_reserve = old_available + old_xfer_limit + young_reserve;\n+  const double max_old_reserve = (ShenandoahOldEvacRatioPercent == 100)?\n+                                 bound_on_old_reserve: MIN2(double(young_reserve * ShenandoahOldEvacRatioPercent) \/ double(100 - ShenandoahOldEvacRatioPercent),\n+                                                            bound_on_old_reserve);\n+\n+  const size_t region_size_bytes = ShenandoahHeapRegion::region_size_bytes();\n+\n+  \/\/ Decide how much old space we should reserve for a mixed collection\n+  double reserve_for_mixed = 0;\n+  if (old_generation()->has_unprocessed_collection_candidates()) {\n+    \/\/ We want this much memory to be unfragmented in order to reliably evacuate old.  This is conservative because we\n+    \/\/ may not evacuate the entirety of unprocessed candidates in a single mixed evacuation.\n+    const double max_evac_need = (double(old_generation()->unprocessed_collection_candidates_live_memory()) * ShenandoahOldEvacWaste);\n+    assert(old_available >= old_generation()->free_unaffiliated_regions() * region_size_bytes,\n+           \"Unaffiliated available must be less than total available\");\n+    const double old_fragmented_available = double(old_available - old_generation()->free_unaffiliated_regions() * region_size_bytes);\n+    reserve_for_mixed = max_evac_need + old_fragmented_available;\n+    if (reserve_for_mixed > max_old_reserve) {\n+      reserve_for_mixed = max_old_reserve;\n+    }\n+  }\n+\n+  \/\/ Decide how much space we should reserve for promotions from young\n+  size_t reserve_for_promo = 0;\n+  const size_t promo_load = old_generation()->get_promotion_potential();\n+  const bool doing_promotions = promo_load > 0;\n+  if (doing_promotions) {\n+    \/\/ We're promoting and have a bound on the maximum amount that can be promoted\n+    assert(max_old_reserve >= reserve_for_mixed, \"Sanity\");\n+    const size_t available_for_promotions = max_old_reserve - reserve_for_mixed;\n+    reserve_for_promo = MIN2((size_t)(promo_load * ShenandoahPromoEvacWaste), available_for_promotions);\n+  }\n+\n+  \/\/ This is the total old we want to ideally reserve\n+  const size_t old_reserve = reserve_for_mixed + reserve_for_promo;\n+  assert(old_reserve <= max_old_reserve, \"cannot reserve more than max for old evacuations\");\n+\n+  \/\/ We now check if the old generation is running a surplus or a deficit.\n+  const size_t max_old_available = old_generation()->available() + old_cset_regions * region_size_bytes;\n+  if (max_old_available >= old_reserve) {\n+    \/\/ We are running a surplus, so the old region surplus can go to young\n+    const size_t old_surplus = (max_old_available - old_reserve) \/ region_size_bytes;\n+    const size_t unaffiliated_old_regions = old_generation()->free_unaffiliated_regions() + old_cset_regions;\n+    const size_t old_region_surplus = MIN2(old_surplus, unaffiliated_old_regions);\n+    old_generation()->set_region_balance(checked_cast<ssize_t>(old_region_surplus));\n+  } else {\n+    \/\/ We are running a deficit which we'd like to fill from young.\n+    \/\/ Ignore that this will directly impact young_generation()->max_capacity(),\n+    \/\/ indirectly impacting young_reserve and old_reserve.  These computations are conservative.\n+    \/\/ Note that deficit is rounded up by one region.\n+    const size_t old_need = (old_reserve - max_old_available + region_size_bytes - 1) \/ region_size_bytes;\n+    const size_t max_old_region_xfer = old_xfer_limit \/ region_size_bytes;\n+\n+    \/\/ Round down the regions we can transfer from young to old. If we're running short\n+    \/\/ on young-gen memory, we restrict the xfer. Old-gen collection activities will be\n+    \/\/ curtailed if the budget is restricted.\n+    const size_t old_region_deficit = MIN2(old_need, max_old_region_xfer);\n+    old_generation()->set_region_balance(0 - checked_cast<ssize_t>(old_region_deficit));\n+  }\n+}\n+\n+void ShenandoahGenerationalHeap::reset_generation_reserves() {\n+  young_generation()->set_evacuation_reserve(0);\n+  old_generation()->set_evacuation_reserve(0);\n+  old_generation()->set_promoted_reserve(0);\n+}\n+\n+void ShenandoahGenerationalHeap::TransferResult::print_on(const char* when, outputStream* ss) const {\n+  auto heap = ShenandoahGenerationalHeap::heap();\n+  ShenandoahYoungGeneration* const young_gen = heap->young_generation();\n+  ShenandoahOldGeneration* const old_gen = heap->old_generation();\n+  const size_t young_available = young_gen->available();\n+  const size_t old_available = old_gen->available();\n+  ss->print_cr(\"After %s, %s \" SIZE_FORMAT \" regions to %s to prepare for next gc, old available: \"\n+                     PROPERFMT \", young_available: \" PROPERFMT,\n+                     when,\n+                     success? \"successfully transferred\": \"failed to transfer\", region_count, region_destination,\n+                     PROPERFMTARGS(old_available), PROPERFMTARGS(young_available));\n+}\n+\n+void ShenandoahGenerationalHeap::coalesce_and_fill_old_regions(bool concurrent) {\n+  class ShenandoahGlobalCoalesceAndFill : public WorkerTask {\n+  private:\n+      ShenandoahPhaseTimings::Phase _phase;\n+      ShenandoahRegionIterator _regions;\n+  public:\n+    explicit ShenandoahGlobalCoalesceAndFill(ShenandoahPhaseTimings::Phase phase) :\n+      WorkerTask(\"Shenandoah Global Coalesce\"),\n+      _phase(phase) {}\n+\n+    void work(uint worker_id) override {\n+      ShenandoahWorkerTimingsTracker timer(_phase,\n+                                           ShenandoahPhaseTimings::ScanClusters,\n+                                           worker_id, true);\n+      ShenandoahHeapRegion* region;\n+      while ((region = _regions.next()) != nullptr) {\n+        \/\/ old region is not in the collection set and was not immediately trashed\n+        if (region->is_old() && region->is_active() && !region->is_humongous()) {\n+          \/\/ Reset the coalesce and fill boundary because this is a global collect\n+          \/\/ and cannot be preempted by young collects. We want to be sure the entire\n+          \/\/ region is coalesced here and does not resume from a previously interrupted\n+          \/\/ or completed coalescing.\n+          region->begin_preemptible_coalesce_and_fill();\n+          region->oop_coalesce_and_fill(false);\n+        }\n+      }\n+    }\n+  };\n+\n+  ShenandoahPhaseTimings::Phase phase = concurrent ?\n+          ShenandoahPhaseTimings::conc_coalesce_and_fill :\n+          ShenandoahPhaseTimings::degen_gc_coalesce_and_fill;\n+\n+  \/\/ This is not cancellable\n+  ShenandoahGlobalCoalesceAndFill coalesce(phase);\n+  workers()->run_task(&coalesce);\n+  old_generation()->set_parsable(true);\n+}\n+\n+template<bool CONCURRENT>\n+class ShenandoahGenerationalUpdateHeapRefsTask : public WorkerTask {\n+private:\n+  ShenandoahGenerationalHeap* _heap;\n+  ShenandoahRegionIterator* _regions;\n+  ShenandoahRegionChunkIterator* _work_chunks;\n+\n+public:\n+  explicit ShenandoahGenerationalUpdateHeapRefsTask(ShenandoahRegionIterator* regions,\n+                                                    ShenandoahRegionChunkIterator* work_chunks) :\n+          WorkerTask(\"Shenandoah Update References\"),\n+          _heap(ShenandoahGenerationalHeap::heap()),\n+          _regions(regions),\n+          _work_chunks(work_chunks)\n+  {\n+    bool old_bitmap_stable = _heap->old_generation()->is_mark_complete();\n+    log_debug(gc, remset)(\"Update refs, scan remembered set using bitmap: %s\", BOOL_TO_STR(old_bitmap_stable));\n+  }\n+\n+  void work(uint worker_id) {\n+    if (CONCURRENT) {\n+      ShenandoahConcurrentWorkerSession worker_session(worker_id);\n+      ShenandoahSuspendibleThreadSetJoiner stsj;\n+      do_work<ShenandoahConcUpdateRefsClosure>(worker_id);\n+    } else {\n+      ShenandoahParallelWorkerSession worker_session(worker_id);\n+      do_work<ShenandoahSTWUpdateRefsClosure>(worker_id);\n+    }\n+  }\n+\n+private:\n+  template<class T>\n+  void do_work(uint worker_id) {\n+    T cl;\n+\n+    if (CONCURRENT && (worker_id == 0)) {\n+      \/\/ We ask the first worker to replenish the Mutator free set by moving regions previously reserved to hold the\n+      \/\/ results of evacuation.  These reserves are no longer necessary because evacuation has completed.\n+      size_t cset_regions = _heap->collection_set()->count();\n+\n+      \/\/ Now that evacuation is done, we can reassign any regions that had been reserved to hold the results of evacuation\n+      \/\/ to the mutator free set.  At the end of GC, we will have cset_regions newly evacuated fully empty regions from\n+      \/\/ which we will be able to replenish the Collector free set and the OldCollector free set in preparation for the\n+      \/\/ next GC cycle.\n+      _heap->free_set()->move_regions_from_collector_to_mutator(cset_regions);\n+    }\n+    \/\/ If !CONCURRENT, there's no value in expanding Mutator free set\n+\n+    ShenandoahHeapRegion* r = _regions->next();\n+    \/\/ We update references for global, old, and young collections.\n+    ShenandoahGeneration* const gc_generation = _heap->gc_generation();\n+    shenandoah_assert_generations_reconciled();\n+    assert(gc_generation->is_mark_complete(), \"Expected complete marking\");\n+    ShenandoahMarkingContext* const ctx = _heap->marking_context();\n+    bool is_mixed = _heap->collection_set()->has_old_regions();\n+    while (r != nullptr) {\n+      HeapWord* update_watermark = r->get_update_watermark();\n+      assert(update_watermark >= r->bottom(), \"sanity\");\n+\n+      log_debug(gc)(\"Update refs worker \" UINT32_FORMAT \", looking at region \" SIZE_FORMAT, worker_id, r->index());\n+      bool region_progress = false;\n+      if (r->is_active() && !r->is_cset()) {\n+        if (r->is_young()) {\n+          _heap->marked_object_oop_iterate(r, &cl, update_watermark);\n+          region_progress = true;\n+        } else if (r->is_old()) {\n+          if (gc_generation->is_global()) {\n+\n+            _heap->marked_object_oop_iterate(r, &cl, update_watermark);\n+            region_progress = true;\n+          }\n+          \/\/ Otherwise, this is an old region in a young or mixed cycle.  Process it during a second phase, below.\n+          \/\/ Don't bother to report pacing progress in this case.\n+        } else {\n+          \/\/ Because updating of references runs concurrently, it is possible that a FREE inactive region transitions\n+          \/\/ to a non-free active region while this loop is executing.  Whenever this happens, the changing of a region's\n+          \/\/ active status may propagate at a different speed than the changing of the region's affiliation.\n+\n+          \/\/ When we reach this control point, it is because a race has allowed a region's is_active() status to be seen\n+          \/\/ by this thread before the region's affiliation() is seen by this thread.\n+\n+          \/\/ It's ok for this race to occur because the newly transformed region does not have any references to be\n+          \/\/ updated.\n+\n+          assert(r->get_update_watermark() == r->bottom(),\n+                 \"%s Region \" SIZE_FORMAT \" is_active but not recognized as YOUNG or OLD so must be newly transitioned from FREE\",\n+                 r->affiliation_name(), r->index());\n+        }\n+      }\n+\n+      if (region_progress && ShenandoahPacing) {\n+        _heap->pacer()->report_update_refs(pointer_delta(update_watermark, r->bottom()));\n+      }\n+\n+      if (_heap->check_cancelled_gc_and_yield(CONCURRENT)) {\n+        return;\n+      }\n+\n+      r = _regions->next();\n+    }\n+\n+    if (!gc_generation->is_global()) {\n+      \/\/ Since this is generational and not GLOBAL, we have to process the remembered set.  There's no remembered\n+      \/\/ set processing if not in generational mode or if GLOBAL mode.\n+\n+      \/\/ After this thread has exhausted its traditional update-refs work, it continues with updating refs within\n+      \/\/ remembered set. The remembered set workload is better balanced between threads, so threads that are \"behind\"\n+      \/\/ can catch up with other threads during this phase, allowing all threads to work more effectively in parallel.\n+      update_references_in_remembered_set(worker_id, cl, ctx, is_mixed);\n+    }\n+  }\n+\n+  template<class T>\n+  void update_references_in_remembered_set(uint worker_id, T &cl, const ShenandoahMarkingContext* ctx, bool is_mixed) {\n+\n+    struct ShenandoahRegionChunk assignment;\n+    ShenandoahScanRemembered* scanner = _heap->old_generation()->card_scan();\n+\n+    while (!_heap->check_cancelled_gc_and_yield(CONCURRENT) && _work_chunks->next(&assignment)) {\n+      \/\/ Keep grabbing next work chunk to process until finished, or asked to yield\n+      ShenandoahHeapRegion* r = assignment._r;\n+      if (r->is_active() && !r->is_cset() && r->is_old()) {\n+        HeapWord* start_of_range = r->bottom() + assignment._chunk_offset;\n+        HeapWord* end_of_range = r->get_update_watermark();\n+        if (end_of_range > start_of_range + assignment._chunk_size) {\n+          end_of_range = start_of_range + assignment._chunk_size;\n+        }\n+\n+        if (start_of_range >= end_of_range) {\n+          continue;\n+        }\n+\n+        \/\/ Old region in a young cycle or mixed cycle.\n+        if (is_mixed) {\n+          if (r->is_humongous()) {\n+            \/\/ Need to examine both dirty and clean cards during mixed evac.\n+            r->oop_iterate_humongous_slice_all(&cl,start_of_range, assignment._chunk_size);\n+          } else {\n+            \/\/ Since this is mixed evacuation, old regions that are candidates for collection have not been coalesced\n+            \/\/ and filled.  This will use mark bits to find objects that need to be updated.\n+            update_references_in_old_region(cl, ctx, scanner, r, start_of_range, end_of_range);\n+          }\n+        } else {\n+          \/\/ This is a young evacuation\n+          size_t cluster_size = CardTable::card_size_in_words() * ShenandoahCardCluster::CardsPerCluster;\n+          size_t clusters = assignment._chunk_size \/ cluster_size;\n+          assert(clusters * cluster_size == assignment._chunk_size, \"Chunk assignment must align on cluster boundaries\");\n+          scanner->process_region_slice(r, assignment._chunk_offset, clusters, end_of_range, &cl, true, worker_id);\n+        }\n+\n+        if (ShenandoahPacing) {\n+          _heap->pacer()->report_update_refs(pointer_delta(end_of_range, start_of_range));\n+        }\n+      }\n+    }\n+  }\n+\n+  template<class T>\n+  void update_references_in_old_region(T &cl, const ShenandoahMarkingContext* ctx, ShenandoahScanRemembered* scanner,\n+                                    const ShenandoahHeapRegion* r, HeapWord* start_of_range,\n+                                    HeapWord* end_of_range) const {\n+    \/\/ In case last object in my range spans boundary of my chunk, I may need to scan all the way to top()\n+    ShenandoahObjectToOopBoundedClosure<T> objs(&cl, start_of_range, r->top());\n+\n+    \/\/ Any object that begins in a previous range is part of a different scanning assignment.  Any object that\n+    \/\/ starts after end_of_range is also not my responsibility.  (Either allocated during evacuation, so does\n+    \/\/ not hold pointers to from-space, or is beyond the range of my assigned work chunk.)\n+\n+    \/\/ Find the first object that begins in my range, if there is one. Note that `p` will be set to `end_of_range`\n+    \/\/ when no live object is found in the range.\n+    HeapWord* tams = ctx->top_at_mark_start(r);\n+    HeapWord* p = get_first_object_start_word(ctx, scanner, tams, start_of_range, end_of_range);\n+\n+    while (p < end_of_range) {\n+      \/\/ p is known to point to the beginning of marked object obj\n+      oop obj = cast_to_oop(p);\n+      objs.do_object(obj);\n+      HeapWord* prev_p = p;\n+      p += obj->size();\n+      if (p < tams) {\n+        p = ctx->get_next_marked_addr(p, tams);\n+        \/\/ If there are no more marked objects before tams, this returns tams.  Note that tams is\n+        \/\/ either >= end_of_range, or tams is the start of an object that is marked.\n+      }\n+      assert(p != prev_p, \"Lack of forward progress\");\n+    }\n+  }\n+\n+  HeapWord* get_first_object_start_word(const ShenandoahMarkingContext* ctx, ShenandoahScanRemembered* scanner, HeapWord* tams,\n+                                        HeapWord* start_of_range, HeapWord* end_of_range) const {\n+    HeapWord* p = start_of_range;\n+\n+    if (p >= tams) {\n+      \/\/ We cannot use ctx->is_marked(obj) to test whether an object begins at this address.  Instead,\n+      \/\/ we need to use the remembered set crossing map to advance p to the first object that starts\n+      \/\/ within the enclosing card.\n+      size_t card_index = scanner->card_index_for_addr(start_of_range);\n+      while (true) {\n+        HeapWord* first_object = scanner->first_object_in_card(card_index);\n+        if (first_object != nullptr) {\n+          p = first_object;\n+          break;\n+        } else if (scanner->addr_for_card_index(card_index + 1) < end_of_range) {\n+          card_index++;\n+        } else {\n+          \/\/ Signal that no object was found in range\n+          p = end_of_range;\n+          break;\n+        }\n+      }\n+    } else if (!ctx->is_marked(cast_to_oop(p))) {\n+      p = ctx->get_next_marked_addr(p, tams);\n+      \/\/ If there are no more marked objects before tams, this returns tams.\n+      \/\/ Note that tams is either >= end_of_range, or tams is the start of an object that is marked.\n+    }\n+    return p;\n+  }\n+};\n+\n+void ShenandoahGenerationalHeap::update_heap_references(bool concurrent) {\n+  assert(!is_full_gc_in_progress(), \"Only for concurrent and degenerated GC\");\n+  const uint nworkers = workers()->active_workers();\n+  ShenandoahRegionChunkIterator work_list(nworkers);\n+  if (concurrent) {\n+    ShenandoahGenerationalUpdateHeapRefsTask<true> task(&_update_refs_iterator, &work_list);\n+    workers()->run_task(&task);\n+  } else {\n+    ShenandoahGenerationalUpdateHeapRefsTask<false> task(&_update_refs_iterator, &work_list);\n+    workers()->run_task(&task);\n+  }\n+\n+  if (ShenandoahEnableCardStats) {\n+    \/\/ Only do this if we are collecting card stats\n+    ShenandoahScanRemembered* card_scan = old_generation()->card_scan();\n+    assert(card_scan != nullptr, \"Card table must exist when card stats are enabled\");\n+    card_scan->log_card_stats(nworkers, CARD_STAT_UPDATE_REFS);\n+  }\n+}\n+\n+struct ShenandoahCompositeRegionClosure {\n+  template<typename C1, typename C2>\n+  class Closure : public ShenandoahHeapRegionClosure {\n+  private:\n+    C1 &_c1;\n+    C2 &_c2;\n+\n+  public:\n+    Closure(C1 &c1, C2 &c2) : ShenandoahHeapRegionClosure(), _c1(c1), _c2(c2) {}\n+\n+    void heap_region_do(ShenandoahHeapRegion* r) override {\n+      _c1.heap_region_do(r);\n+      _c2.heap_region_do(r);\n+    }\n+\n+    bool is_thread_safe() override {\n+      return _c1.is_thread_safe() && _c2.is_thread_safe();\n+    }\n+  };\n+\n+  template<typename C1, typename C2>\n+  static Closure<C1, C2> of(C1 &c1, C2 &c2) {\n+    return Closure<C1, C2>(c1, c2);\n+  }\n+};\n+\n+class ShenandoahUpdateRegionAges : public ShenandoahHeapRegionClosure {\n+private:\n+  ShenandoahMarkingContext* _ctx;\n+\n+public:\n+  explicit ShenandoahUpdateRegionAges(ShenandoahMarkingContext* ctx) : _ctx(ctx) { }\n+\n+  void heap_region_do(ShenandoahHeapRegion* r) override {\n+    \/\/ Maintenance of region age must follow evacuation in order to account for\n+    \/\/ evacuation allocations within survivor regions.  We consult region age during\n+    \/\/ the subsequent evacuation to determine whether certain objects need to\n+    \/\/ be promoted.\n+    if (r->is_young() && r->is_active()) {\n+      HeapWord *tams = _ctx->top_at_mark_start(r);\n+      HeapWord *top = r->top();\n+\n+      \/\/ Allocations move the watermark when top moves.  However, compacting\n+      \/\/ objects will sometimes lower top beneath the watermark, after which,\n+      \/\/ attempts to read the watermark will assert out (watermark should not be\n+      \/\/ higher than top).\n+      if (top > tams) {\n+        \/\/ There have been allocations in this region since the start of the cycle.\n+        \/\/ Any objects new to this region must not assimilate elevated age.\n+        r->reset_age();\n+      } else if (ShenandoahGenerationalHeap::heap()->is_aging_cycle()) {\n+        r->increment_age();\n+      }\n+    }\n+  }\n+\n+  bool is_thread_safe() override {\n+    return true;\n+  }\n+};\n+\n+void ShenandoahGenerationalHeap::final_update_refs_update_region_states() {\n+  ShenandoahSynchronizePinnedRegionStates pins;\n+  ShenandoahUpdateRegionAges ages(active_generation()->complete_marking_context());\n+  auto cl = ShenandoahCompositeRegionClosure::of(pins, ages);\n+  parallel_heap_region_iterate(&cl);\n+}\n+\n+void ShenandoahGenerationalHeap::complete_degenerated_cycle() {\n+  shenandoah_assert_heaplocked_or_safepoint();\n+  if (is_concurrent_old_mark_in_progress()) {\n+    \/\/ This is still necessary for degenerated cycles because the degeneration point may occur\n+    \/\/ after final mark of the young generation. See ShenandoahConcurrentGC::op_final_update_refs for\n+    \/\/ a more detailed explanation.\n+    old_generation()->transfer_pointers_from_satb();\n+  }\n+\n+  \/\/ We defer generation resizing actions until after cset regions have been recycled.\n+  TransferResult result = balance_generations();\n+  LogTarget(Info, gc, ergo) lt;\n+  if (lt.is_enabled()) {\n+    LogStream ls(lt);\n+    result.print_on(\"Degenerated GC\", &ls);\n+  }\n+\n+  \/\/ In case degeneration interrupted concurrent evacuation or update references, we need to clean up\n+  \/\/ transient state. Otherwise, these actions have no effect.\n+  reset_generation_reserves();\n+\n+  if (!old_generation()->is_parsable()) {\n+    ShenandoahGCPhase phase(ShenandoahPhaseTimings::degen_gc_coalesce_and_fill);\n+    coalesce_and_fill_old_regions(false);\n+  }\n+}\n+\n+void ShenandoahGenerationalHeap::complete_concurrent_cycle() {\n+  if (!old_generation()->is_parsable()) {\n+    \/\/ Class unloading may render the card offsets unusable, so we must rebuild them before\n+    \/\/ the next remembered set scan. We _could_ let the control thread do this sometime after\n+    \/\/ the global cycle has completed and before the next young collection, but under memory\n+    \/\/ pressure the control thread may not have the time (that is, because it's running back\n+    \/\/ to back GCs). In that scenario, we would have to make the old regions parsable before\n+    \/\/ we could start a young collection. This could delay the start of the young cycle and\n+    \/\/ throw off the heuristics.\n+    entry_global_coalesce_and_fill();\n+  }\n+\n+  TransferResult result;\n+  {\n+    ShenandoahHeapLocker locker(lock());\n+\n+    result = balance_generations();\n+    reset_generation_reserves();\n+  }\n+\n+  LogTarget(Info, gc, ergo) lt;\n+  if (lt.is_enabled()) {\n+    LogStream ls(lt);\n+    result.print_on(\"Concurrent GC\", &ls);\n+  }\n+}\n+\n+void ShenandoahGenerationalHeap::entry_global_coalesce_and_fill() {\n+  const char* msg = \"Coalescing and filling old regions\";\n+  ShenandoahConcurrentPhase gc_phase(msg, ShenandoahPhaseTimings::conc_coalesce_and_fill);\n+\n+  TraceCollectorStats tcs(monitoring_support()->concurrent_collection_counters());\n+  EventMark em(\"%s\", msg);\n+  ShenandoahWorkerScope scope(workers(),\n+                              ShenandoahWorkerPolicy::calc_workers_for_conc_marking(),\n+                              \"concurrent coalesce and fill\");\n+\n+  coalesce_and_fill_old_regions(true);\n+}\n+\n+void ShenandoahGenerationalHeap::update_region_ages(ShenandoahMarkingContext* ctx) {\n+  ShenandoahUpdateRegionAges cl(ctx);\n+  parallel_heap_region_iterate(&cl);\n+}\n","filename":"src\/hotspot\/share\/gc\/shenandoah\/shenandoahGenerationalHeap.cpp","additions":1148,"deletions":0,"binary":false,"changes":1148,"status":"added"},{"patch":"@@ -0,0 +1,172 @@\n+\/*\n+ * Copyright Amazon.com Inc. or its affiliates. All Rights Reserved.\n+ * DO NOT ALTER OR REMOVE COPYRIGHT NOTICES OR THIS FILE HEADER.\n+ *\n+ * This code is free software; you can redistribute it and\/or modify it\n+ * under the terms of the GNU General Public License version 2 only, as\n+ * published by the Free Software Foundation.\n+ *\n+ * This code is distributed in the hope that it will be useful, but WITHOUT\n+ * ANY WARRANTY; without even the implied warranty of MERCHANTABILITY or\n+ * FITNESS FOR A PARTICULAR PURPOSE.  See the GNU General Public License\n+ * version 2 for more details (a copy is included in the LICENSE file that\n+ * accompanied this code).\n+ *\n+ * You should have received a copy of the GNU General Public License version\n+ * 2 along with this work; if not, write to the Free Software Foundation,\n+ * Inc., 51 Franklin St, Fifth Floor, Boston, MA 02110-1301 USA.\n+ *\n+ * Please contact Oracle, 500 Oracle Parkway, Redwood Shores, CA 94065 USA\n+ * or visit www.oracle.com if you need additional information or have any\n+ * questions.\n+ *\n+ *\/\n+\n+#ifndef SHARE_GC_SHENANDOAH_SHENANDOAHGENERATIONALHEAP\n+#define SHARE_GC_SHENANDOAH_SHENANDOAHGENERATIONALHEAP\n+\n+#include \"gc\/shenandoah\/shenandoahAsserts.hpp\"\n+#include \"gc\/shenandoah\/shenandoahHeap.hpp\"\n+#include \"memory\/universe.hpp\"\n+\n+class PLAB;\n+class ShenandoahRegulatorThread;\n+class ShenandoahGenerationalControlThread;\n+class ShenandoahAgeCensus;\n+\n+class ShenandoahGenerationalHeap : public ShenandoahHeap {\n+public:\n+  explicit ShenandoahGenerationalHeap(ShenandoahCollectorPolicy* policy);\n+  void post_initialize() override;\n+  void initialize_heuristics() override;\n+\n+  static ShenandoahGenerationalHeap* heap() {\n+    shenandoah_assert_generational();\n+    CollectedHeap* heap = Universe::heap();\n+    return cast(heap);\n+  }\n+\n+  static ShenandoahGenerationalHeap* cast(CollectedHeap* heap) {\n+    shenandoah_assert_generational();\n+    return checked_cast<ShenandoahGenerationalHeap*>(heap);\n+  }\n+\n+  void print_init_logger() const override;\n+  void print_tracing_info() const override;\n+\n+  size_t unsafe_max_tlab_alloc(Thread *thread) const override;\n+\n+private:\n+  \/\/ ---------- Evacuations and Promotions\n+  \/\/\n+  \/\/ True when regions and objects should be aged during the current cycle\n+  ShenandoahSharedFlag  _is_aging_cycle;\n+  \/\/ Age census used for adapting tenuring threshold\n+  ShenandoahAgeCensus* _age_census;\n+  \/\/ Used primarily to look for failed evacuation attempts.\n+  ShenandoahEvacuationTracker*  _evac_tracker;\n+\n+public:\n+  void set_aging_cycle(bool cond) {\n+    _is_aging_cycle.set_cond(cond);\n+  }\n+\n+  inline bool is_aging_cycle() const {\n+    return _is_aging_cycle.is_set();\n+  }\n+\n+  \/\/ Return the age census object for young gen\n+  ShenandoahAgeCensus* age_census() const {\n+    return _age_census;\n+  }\n+\n+  inline bool is_tenurable(const ShenandoahHeapRegion* r) const;\n+\n+  ShenandoahEvacuationTracker* evac_tracker() const {\n+    return _evac_tracker;\n+  }\n+\n+  \/\/ Ages regions that haven't been used for allocations in the current cycle.\n+  \/\/ Resets ages for regions that have been used for allocations.\n+  void update_region_ages(ShenandoahMarkingContext* ctx);\n+\n+  oop evacuate_object(oop p, Thread* thread) override;\n+  oop try_evacuate_object(oop p, Thread* thread, ShenandoahHeapRegion* from_region, ShenandoahAffiliation target_gen);\n+  void evacuate_collection_set(bool concurrent) override;\n+  void promote_regions_in_place(bool concurrent);\n+\n+  size_t plab_min_size() const { return _min_plab_size; }\n+  size_t plab_max_size() const { return _max_plab_size; }\n+\n+  void retire_plab(PLAB* plab);\n+  void retire_plab(PLAB* plab, Thread* thread);\n+\n+  \/\/ ---------- Update References\n+  \/\/\n+  void update_heap_references(bool concurrent) override;\n+  void final_update_refs_update_region_states() override;\n+\n+private:\n+  HeapWord* allocate_from_plab(Thread* thread, size_t size, bool is_promotion);\n+  HeapWord* allocate_from_plab_slow(Thread* thread, size_t size, bool is_promotion);\n+  HeapWord* allocate_new_plab(size_t min_size, size_t word_size, size_t* actual_size);\n+\n+  const size_t _min_plab_size;\n+  const size_t _max_plab_size;\n+\n+  static size_t calculate_min_plab();\n+  static size_t calculate_max_plab();\n+\n+public:\n+  \/\/ ---------- Serviceability\n+  \/\/\n+  void initialize_serviceability() override;\n+  GrowableArray<MemoryPool*> memory_pools() override;\n+\n+  ShenandoahRegulatorThread* regulator_thread() const { return _regulator_thread;  }\n+\n+  void gc_threads_do(ThreadClosure* tcl) const override;\n+\n+  void stop() override;\n+\n+  bool requires_barriers(stackChunkOop obj) const override;\n+\n+  \/\/ Used for logging the result of a region transfer outside the heap lock\n+  struct TransferResult {\n+    bool success;\n+    size_t region_count;\n+    const char* region_destination;\n+\n+    void print_on(const char* when, outputStream* ss) const;\n+  };\n+\n+  const ShenandoahGenerationSizer* generation_sizer()  const { return &_generation_sizer;  }\n+\n+  \/\/ Zeros out the evacuation and promotion reserves\n+  void reset_generation_reserves();\n+\n+  \/\/ Computes the optimal size for the old generation, represented as a surplus or deficit of old regions\n+  void compute_old_generation_balance(size_t old_xfer_limit, size_t old_cset_regions);\n+\n+  \/\/ Transfers surplus old regions to young, or takes regions from young to satisfy old region deficit\n+  TransferResult balance_generations();\n+\n+  \/\/ Balances generations, coalesces and fills old regions if necessary\n+  void complete_degenerated_cycle();\n+  void complete_concurrent_cycle();\n+private:\n+  void initialize_controller() override;\n+  void entry_global_coalesce_and_fill();\n+\n+  \/\/ Makes old regions parsable. This will also rebuild card offsets, which is necessary if classes were unloaded\n+  void coalesce_and_fill_old_regions(bool concurrent);\n+\n+  ShenandoahRegulatorThread* _regulator_thread;\n+\n+  MemoryPool* _young_gen_memory_pool;\n+  MemoryPool* _old_gen_memory_pool;\n+\n+  ShenandoahGenerationSizer     _generation_sizer;\n+};\n+\n+#endif \/\/SHARE_GC_SHENANDOAH_SHENANDOAHGENERATIONALHEAP\n","filename":"src\/hotspot\/share\/gc\/shenandoah\/shenandoahGenerationalHeap.hpp","additions":172,"deletions":0,"binary":false,"changes":172,"status":"added"},{"patch":"@@ -0,0 +1,37 @@\n+\/*\n+ * Copyright Amazon.com Inc. or its affiliates. All Rights Reserved.\n+ * DO NOT ALTER OR REMOVE COPYRIGHT NOTICES OR THIS FILE HEADER.\n+ *\n+ * This code is free software; you can redistribute it and\/or modify it\n+ * under the terms of the GNU General Public License version 2 only, as\n+ * published by the Free Software Foundation.\n+ *\n+ * This code is distributed in the hope that it will be useful, but WITHOUT\n+ * ANY WARRANTY; without even the implied warranty of MERCHANTABILITY or\n+ * FITNESS FOR A PARTICULAR PURPOSE.  See the GNU General Public License\n+ * version 2 for more details (a copy is included in the LICENSE file that\n+ * accompanied this code).\n+ *\n+ * You should have received a copy of the GNU General Public License version\n+ * 2 along with this work; if not, write to the Free Software Foundation,\n+ * Inc., 51 Franklin St, Fifth Floor, Boston, MA 02110-1301 USA.\n+ *\n+ * Please contact Oracle, 500 Oracle Parkway, Redwood Shores, CA 94065 USA\n+ * or visit www.oracle.com if you need additional information or have any\n+ * questions.\n+ *\n+ *\/\n+\n+#ifndef SHARE_GC_SHENANDOAH_SHENANDOAHGENERATIONALHEAP_INLINE_HPP\n+#define SHARE_GC_SHENANDOAH_SHENANDOAHGENERATIONALHEAP_INLINE_HPP\n+\n+#include \"gc\/shenandoah\/shenandoahGenerationalHeap.hpp\"\n+\n+#include \"gc\/shenandoah\/shenandoahAgeCensus.hpp\"\n+#include \"gc\/shenandoah\/shenandoahHeapRegion.hpp\"\n+\n+inline bool ShenandoahGenerationalHeap::is_tenurable(const ShenandoahHeapRegion* r) const {\n+  return _age_census->is_tenurable(r->age());\n+}\n+\n+#endif \/\/ SHARE_GC_SHENANDOAH_SHENANDOAHGENERATIONALHEAP_INLINE_HPP\n","filename":"src\/hotspot\/share\/gc\/shenandoah\/shenandoahGenerationalHeap.inline.hpp","additions":37,"deletions":0,"binary":false,"changes":37,"status":"added"},{"patch":"@@ -0,0 +1,142 @@\n+\/*\n+ * Copyright Amazon.com Inc. or its affiliates. All Rights Reserved.\n+ * DO NOT ALTER OR REMOVE COPYRIGHT NOTICES OR THIS FILE HEADER.\n+ *\n+ * This code is free software; you can redistribute it and\/or modify it\n+ * under the terms of the GNU General Public License version 2 only, as\n+ * published by the Free Software Foundation.\n+ *\n+ * This code is distributed in the hope that it will be useful, but WITHOUT\n+ * ANY WARRANTY; without even the implied warranty of MERCHANTABILITY or\n+ * FITNESS FOR A PARTICULAR PURPOSE.  See the GNU General Public License\n+ * version 2 for more details (a copy is included in the LICENSE file that\n+ * accompanied this code).\n+ *\n+ * You should have received a copy of the GNU General Public License version\n+ * 2 along with this work; if not, write to the Free Software Foundation,\n+ * Inc., 51 Franklin St, Fifth Floor, Boston, MA 02110-1301 USA.\n+ *\n+ * Please contact Oracle, 500 Oracle Parkway, Redwood Shores, CA 94065 USA\n+ * or visit www.oracle.com if you need additional information or have any\n+ * questions.\n+ *\n+ *\/\n+#include \"precompiled.hpp\"\n+\n+#include \"gc\/shenandoah\/shenandoahAgeCensus.hpp\"\n+#include \"gc\/shenandoah\/heuristics\/shenandoahGlobalHeuristics.hpp\"\n+#include \"gc\/shenandoah\/shenandoahFreeSet.hpp\"\n+#include \"gc\/shenandoah\/shenandoahGlobalGeneration.hpp\"\n+#include \"gc\/shenandoah\/shenandoahHeap.hpp\"\n+#include \"gc\/shenandoah\/shenandoahHeapRegion.inline.hpp\"\n+#include \"gc\/shenandoah\/shenandoahUtils.hpp\"\n+#include \"gc\/shenandoah\/shenandoahVerifier.hpp\"\n+\n+\n+const char* ShenandoahGlobalGeneration::name() const {\n+  return type() == NON_GEN ? \"\" : \"Global\";\n+}\n+\n+size_t ShenandoahGlobalGeneration::max_capacity() const {\n+  return ShenandoahHeap::heap()->max_capacity();\n+}\n+\n+size_t ShenandoahGlobalGeneration::used_regions() const {\n+  ShenandoahGenerationalHeap* heap = ShenandoahGenerationalHeap::heap();\n+  assert(heap->mode()->is_generational(), \"Region usage accounting is only for generational mode\");\n+  return heap->old_generation()->used_regions() + heap->young_generation()->used_regions();\n+}\n+\n+size_t ShenandoahGlobalGeneration::used_regions_size() const {\n+  return ShenandoahHeap::heap()->capacity();\n+}\n+\n+size_t ShenandoahGlobalGeneration::available() const {\n+  return ShenandoahHeap::heap()->free_set()->available();\n+}\n+\n+size_t ShenandoahGlobalGeneration::soft_available() const {\n+  size_t available = this->available();\n+\n+  \/\/ Make sure the code below treats available without the soft tail.\n+  assert(max_capacity() >= ShenandoahHeap::heap()->soft_max_capacity(), \"Max capacity must be greater than soft max capacity.\");\n+  size_t soft_tail = max_capacity() - ShenandoahHeap::heap()->soft_max_capacity();\n+  return (available > soft_tail) ? (available - soft_tail) : 0;\n+}\n+\n+void ShenandoahGlobalGeneration::set_concurrent_mark_in_progress(bool in_progress) {\n+  ShenandoahHeap* heap = ShenandoahHeap::heap();\n+  if (in_progress && heap->mode()->is_generational()) {\n+    \/\/ Global collection has preempted an old generation mark. This is fine\n+    \/\/ because the global generation includes the old generation, but we\n+    \/\/ want the global collect to start from a clean slate and we don't want\n+    \/\/ any stale state in the old generation.\n+    assert(!heap->is_concurrent_old_mark_in_progress(), \"Old cycle should not be running.\");\n+  }\n+\n+  heap->set_concurrent_young_mark_in_progress(in_progress);\n+}\n+\n+bool ShenandoahGlobalGeneration::contains(ShenandoahAffiliation affiliation) const {\n+  return true;\n+}\n+\n+bool ShenandoahGlobalGeneration::contains(ShenandoahHeapRegion* region) const {\n+  return true;\n+}\n+\n+void ShenandoahGlobalGeneration::parallel_heap_region_iterate(ShenandoahHeapRegionClosure* cl) {\n+  ShenandoahHeap::heap()->parallel_heap_region_iterate(cl);\n+}\n+\n+void ShenandoahGlobalGeneration::heap_region_iterate(ShenandoahHeapRegionClosure* cl) {\n+  ShenandoahHeap::heap()->heap_region_iterate(cl);\n+}\n+\n+bool ShenandoahGlobalGeneration::is_concurrent_mark_in_progress() {\n+  ShenandoahHeap* heap = ShenandoahHeap::heap();\n+  return heap->is_concurrent_mark_in_progress();\n+}\n+\n+ShenandoahHeuristics* ShenandoahGlobalGeneration::initialize_heuristics(ShenandoahMode* gc_mode) {\n+  if (gc_mode->is_generational()) {\n+    _heuristics = new ShenandoahGlobalHeuristics(this);\n+  } else {\n+    _heuristics = gc_mode->initialize_heuristics(this);\n+  }\n+\n+  _heuristics->set_guaranteed_gc_interval(ShenandoahGuaranteedGCInterval);\n+  confirm_heuristics_mode();\n+  return _heuristics;\n+}\n+\n+void ShenandoahGlobalGeneration::set_mark_complete() {\n+  ShenandoahGeneration::set_mark_complete();\n+  if (ShenandoahHeap::heap()->mode()->is_generational()) {\n+    ShenandoahGenerationalHeap* heap = ShenandoahGenerationalHeap::heap();\n+    heap->young_generation()->set_mark_complete();\n+    heap->old_generation()->set_mark_complete();\n+  }\n+}\n+\n+void ShenandoahGlobalGeneration::set_mark_incomplete() {\n+  ShenandoahGeneration::set_mark_incomplete();\n+  if (ShenandoahHeap::heap()->mode()->is_generational()) {\n+    ShenandoahGenerationalHeap* heap = ShenandoahGenerationalHeap::heap();\n+    heap->young_generation()->set_mark_incomplete();\n+    heap->old_generation()->set_mark_incomplete();\n+  }\n+}\n+\n+void ShenandoahGlobalGeneration::prepare_gc() {\n+  ShenandoahGeneration::prepare_gc();\n+\n+  if (ShenandoahHeap::heap()->mode()->is_generational()) {\n+    assert(type() == GLOBAL, \"Unexpected generation type\");\n+    \/\/ Clear any stale\/partial local census data before the start of a\n+    \/\/ new marking cycle\n+    ShenandoahGenerationalHeap::heap()->age_census()->reset_local();\n+  } else {\n+    assert(type() == NON_GEN, \"Unexpected generation type\");\n+  }\n+}\n","filename":"src\/hotspot\/share\/gc\/shenandoah\/shenandoahGlobalGeneration.cpp","additions":142,"deletions":0,"binary":false,"changes":142,"status":"added"},{"patch":"@@ -0,0 +1,72 @@\n+\/*\n+ * Copyright Amazon.com Inc. or its affiliates. All Rights Reserved.\n+ * DO NOT ALTER OR REMOVE COPYRIGHT NOTICES OR THIS FILE HEADER.\n+ *\n+ * This code is free software; you can redistribute it and\/or modify it\n+ * under the terms of the GNU General Public License version 2 only, as\n+ * published by the Free Software Foundation.\n+ *\n+ * This code is distributed in the hope that it will be useful, but WITHOUT\n+ * ANY WARRANTY; without even the implied warranty of MERCHANTABILITY or\n+ * FITNESS FOR A PARTICULAR PURPOSE.  See the GNU General Public License\n+ * version 2 for more details (a copy is included in the LICENSE file that\n+ * accompanied this code).\n+ *\n+ * You should have received a copy of the GNU General Public License version\n+ * 2 along with this work; if not, write to the Free Software Foundation,\n+ * Inc., 51 Franklin St, Fifth Floor, Boston, MA 02110-1301 USA.\n+ *\n+ * Please contact Oracle, 500 Oracle Parkway, Redwood Shores, CA 94065 USA\n+ * or visit www.oracle.com if you need additional information or have any\n+ * questions.\n+ *\n+ *\/\n+\n+#ifndef SHARE_VM_GC_SHENANDOAH_SHENANDOAHGLOBALGENERATION_HPP\n+#define SHARE_VM_GC_SHENANDOAH_SHENANDOAHGLOBALGENERATION_HPP\n+\n+#include \"gc\/shenandoah\/shenandoahGeneration.hpp\"\n+#include \"gc\/shenandoah\/shenandoahYoungGeneration.hpp\"\n+#include \"gc\/shenandoah\/shenandoahOldGeneration.hpp\"\n+\n+\/\/ A \"generation\" that represents the whole heap.\n+class ShenandoahGlobalGeneration : public ShenandoahGeneration {\n+public:\n+  ShenandoahGlobalGeneration(bool generational, uint max_queues, size_t max_capacity)\n+  : ShenandoahGeneration(generational ? GLOBAL : NON_GEN, max_queues, max_capacity) { }\n+\n+public:\n+  const char* name() const override;\n+\n+  size_t max_capacity() const override;\n+  size_t used_regions() const override;\n+  size_t used_regions_size() const override;\n+  size_t available() const override;\n+  size_t soft_available() const override;\n+\n+  void set_concurrent_mark_in_progress(bool in_progress) override;\n+\n+  bool contains(ShenandoahAffiliation affiliation) const override;\n+  bool contains(ShenandoahHeapRegion* region) const override;\n+\n+  bool contains(oop obj) const override {\n+    return ShenandoahHeap::heap()->is_in_reserved(obj);\n+  }\n+\n+  void parallel_heap_region_iterate(ShenandoahHeapRegionClosure* cl) override;\n+\n+  void heap_region_iterate(ShenandoahHeapRegionClosure* cl) override;\n+\n+  bool is_concurrent_mark_in_progress() override;\n+\n+  void set_mark_complete() override;\n+\n+  void set_mark_incomplete() override;\n+\n+  ShenandoahHeuristics* initialize_heuristics(ShenandoahMode* gc_mode) override;\n+\n+  virtual void prepare_gc() override;\n+};\n+\n+#endif \/\/ SHARE_VM_GC_SHENANDOAH_SHENANDOAHGLOBALGENERATION_HPP\n+\n","filename":"src\/hotspot\/share\/gc\/shenandoah\/shenandoahGlobalGeneration.hpp","additions":72,"deletions":0,"binary":false,"changes":72,"status":"added"},{"patch":"@@ -2,1 +2,1 @@\n- * Copyright (c) 2023, Oracle and\/or its affiliates. All rights reserved.\n+ * Copyright (c) 2023, 2024, Oracle and\/or its affiliates. All rights reserved.\n@@ -4,0 +4,1 @@\n+ * Copyright Amazon.com Inc. or its affiliates. All Rights Reserved.\n@@ -34,0 +35,1 @@\n+#include \"gc\/shared\/gc_globals.hpp\"\n@@ -39,0 +41,3 @@\n+#include \"gc\/shenandoah\/heuristics\/shenandoahOldHeuristics.hpp\"\n+#include \"gc\/shenandoah\/heuristics\/shenandoahYoungHeuristics.hpp\"\n+#include \"gc\/shenandoah\/shenandoahAllocRequest.hpp\"\n@@ -47,0 +52,3 @@\n+#include \"gc\/shenandoah\/shenandoahGenerationalEvacuationTask.hpp\"\n+#include \"gc\/shenandoah\/shenandoahGenerationalHeap.hpp\"\n+#include \"gc\/shenandoah\/shenandoahGlobalGeneration.hpp\"\n@@ -49,0 +57,1 @@\n+#include \"gc\/shenandoah\/shenandoahHeapRegionClosures.hpp\"\n@@ -54,1 +63,0 @@\n-#include \"gc\/shenandoah\/shenandoahMetrics.hpp\"\n@@ -56,0 +64,1 @@\n+#include \"gc\/shenandoah\/shenandoahOldGeneration.hpp\"\n@@ -62,0 +71,1 @@\n+#include \"gc\/shenandoah\/shenandoahScanRemembered.inline.hpp\"\n@@ -63,0 +73,1 @@\n+#include \"gc\/shenandoah\/shenandoahUncommitThread.hpp\"\n@@ -69,1 +80,2 @@\n-#include \"gc\/shenandoah\/mode\/shenandoahIUMode.hpp\"\n+#include \"gc\/shenandoah\/shenandoahYoungGeneration.hpp\"\n+#include \"gc\/shenandoah\/mode\/shenandoahGenerationalMode.hpp\"\n@@ -72,0 +84,2 @@\n+#include \"utilities\/globalDefinitions.hpp\"\n+\n@@ -88,0 +102,1 @@\n+#include \"runtime\/threads.hpp\"\n@@ -163,3 +178,0 @@\n-  \/\/ Now we know the number of regions, initialize the heuristics.\n-  initialize_heuristics();\n-\n@@ -176,2 +188,1 @@\n-  \/\/ Default to max heap size.\n-  _soft_max_size = _num_regions * reg_size_bytes;\n+  _soft_max_size = SoftMaxHeapSize;\n@@ -196,0 +207,3 @@\n+  os::trace_page_sizes_for_requested_size(\"Heap\",\n+                                          max_byte_size, heap_rs.page_size(), heap_alignment,\n+                                          heap_rs.base(), heap_rs.size());\n@@ -215,0 +229,22 @@\n+  BarrierSet::set_barrier_set(new ShenandoahBarrierSet(this, _heap_region));\n+\n+  \/\/ Now we know the number of regions and heap sizes, initialize the heuristics.\n+  initialize_heuristics();\n+\n+  assert(_heap_region.byte_size() == heap_rs.size(), \"Need to know reserved size for card table\");\n+\n+  \/\/\n+  \/\/ Worker threads must be initialized after the barrier is configured\n+  \/\/\n+  _workers = new ShenandoahWorkerThreads(\"Shenandoah GC Threads\", _max_workers);\n+  if (_workers == nullptr) {\n+    vm_exit_during_initialization(\"Failed necessary allocation.\");\n+  } else {\n+    _workers->initialize_workers();\n+  }\n+\n+  if (ParallelGCThreads > 1) {\n+    _safepoint_workers = new ShenandoahWorkerThreads(\"Safepoint Cleanup Thread\", ParallelGCThreads);\n+    _safepoint_workers->initialize_workers();\n+  }\n+\n@@ -219,2 +255,2 @@\n-  _bitmap_size = ShenandoahMarkBitMap::compute_size(heap_rs.size());\n-  _bitmap_size = align_up(_bitmap_size, bitmap_page_size);\n+  size_t bitmap_size_orig = ShenandoahMarkBitMap::compute_size(heap_rs.size());\n+  _bitmap_size = align_up(bitmap_size_orig, bitmap_page_size);\n@@ -246,0 +282,4 @@\n+  os::trace_page_sizes_for_requested_size(\"Mark Bitmap\",\n+                                          bitmap_size_orig, bitmap.page_size(), bitmap_page_size,\n+                                          bitmap.base(),\n+                                          bitmap.size());\n@@ -251,1 +291,1 @@\n-                              align_up(num_committed_regions, _bitmap_regions_per_slice) \/ _bitmap_regions_per_slice;\n+    align_up(num_committed_regions, _bitmap_regions_per_slice) \/ _bitmap_regions_per_slice;\n@@ -258,1 +298,1 @@\n-  _marking_context = new ShenandoahMarkingContext(_heap_region, _bitmap_region, _num_regions, _max_workers);\n+  _marking_context = new ShenandoahMarkingContext(_heap_region, _bitmap_region, _num_regions);\n@@ -262,0 +302,4 @@\n+    os::trace_page_sizes_for_requested_size(\"Verify Bitmap\",\n+                                            bitmap_size_orig, verify_bitmap.page_size(), bitmap_page_size,\n+                                            verify_bitmap.base(),\n+                                            verify_bitmap.size());\n@@ -273,1 +317,6 @@\n-  ReservedSpace aux_bitmap(_bitmap_size, bitmap_page_size);\n+  size_t aux_bitmap_page_size = bitmap_page_size;\n+\n+  ReservedSpace aux_bitmap(_bitmap_size, aux_bitmap_page_size);\n+  os::trace_page_sizes_for_requested_size(\"Aux Bitmap\",\n+                                          bitmap_size_orig, aux_bitmap.page_size(), aux_bitmap_page_size,\n+                                          aux_bitmap.base(), aux_bitmap.size());\n@@ -283,2 +332,3 @@\n-  size_t region_storage_size = align_up(region_align * _num_regions, region_page_size);\n-  region_storage_size = align_up(region_storage_size, os::vm_allocation_granularity());\n+  size_t region_storage_size_orig = region_align * _num_regions;\n+  size_t region_storage_size = align_up(region_storage_size_orig,\n+                                        MAX2(region_page_size, os::vm_allocation_granularity()));\n@@ -287,0 +337,3 @@\n+  os::trace_page_sizes_for_requested_size(\"Region Storage\",\n+                                          region_storage_size_orig, region_storage.page_size(), region_page_size,\n+                                          region_storage.base(), region_storage.size());\n@@ -297,2 +350,3 @@\n-    size_t cset_align = MAX2<size_t>(os::vm_page_size(), os::vm_allocation_granularity());\n-    size_t cset_size = align_up(((size_t) sh_rs.base() + sh_rs.size()) >> ShenandoahHeapRegion::region_size_bytes_shift(), cset_align);\n+    const size_t cset_align = MAX2<size_t>(os::vm_page_size(), os::vm_allocation_granularity());\n+    const size_t cset_size = align_up(((size_t) sh_rs.base() + sh_rs.size()) >> ShenandoahHeapRegion::region_size_bytes_shift(), cset_align);\n+    const size_t cset_page_size = os::vm_page_size();\n@@ -302,0 +356,1 @@\n+    ReservedSpace cset_rs;\n@@ -306,1 +361,1 @@\n-      ReservedSpace cset_rs(cset_size, cset_align, os::vm_page_size(), req_addr);\n+      cset_rs = ReservedSpace(cset_size, cset_align, cset_page_size, req_addr);\n@@ -315,1 +370,1 @@\n-      ReservedSpace cset_rs(cset_size, cset_align, os::vm_page_size());\n+      cset_rs = ReservedSpace(cset_size, cset_align, os::vm_page_size());\n@@ -318,0 +373,4 @@\n+    os::trace_page_sizes_for_requested_size(\"Collection Set\",\n+                                            cset_size, cset_rs.page_size(), cset_page_size,\n+                                            cset_rs.base(),\n+                                            cset_rs.size());\n@@ -321,0 +380,1 @@\n+  _affiliations = NEW_C_HEAP_ARRAY(uint8_t, _num_regions, mtGC);\n@@ -337,0 +397,2 @@\n+\n+      _affiliations[i] = ShenandoahAffiliation::FREE;\n@@ -339,2 +401,1 @@\n-    \/\/ Initialize to complete\n-    _marking_context->mark_complete();\n+    size_t young_cset_regions, old_cset_regions;\n@@ -342,1 +403,4 @@\n-    _free_set->rebuild();\n+    \/\/ We are initializing free set.  We ignore cset region tallies.\n+    size_t first_old, last_old, num_old;\n+    _free_set->prepare_to_rebuild(young_cset_regions, old_cset_regions, first_old, last_old, num_old);\n+    _free_set->finish_rebuild(young_cset_regions, old_cset_regions, num_old);\n@@ -354,10 +418,0 @@\n-#ifdef LINUX\n-    \/\/ UseTransparentHugePages would madvise that backing memory can be coalesced into huge\n-    \/\/ pages. But, the kernel needs to know that every small page is used, in order to coalesce\n-    \/\/ them into huge one. Therefore, we need to pretouch with smaller pages.\n-    if (UseTransparentHugePages) {\n-      _pretouch_heap_page_size = (size_t)os::vm_page_size();\n-      _pretouch_bitmap_page_size = (size_t)os::vm_page_size();\n-    }\n-#endif\n-\n@@ -399,2 +453,0 @@\n-  } else {\n-    _pacer = nullptr;\n@@ -403,1 +455,1 @@\n-  _control_thread = new ShenandoahControlThread();\n+  initialize_controller();\n@@ -405,1 +457,5 @@\n-  ShenandoahInitLogger::print();\n+  if (ShenandoahUncommit) {\n+    _uncommit_thread = new ShenandoahUncommitThread(this);\n+  }\n+\n+  print_init_logger();\n@@ -410,0 +466,8 @@\n+void ShenandoahHeap::initialize_controller() {\n+  _control_thread = new ShenandoahControlThread();\n+}\n+\n+void ShenandoahHeap::print_init_logger() const {\n+  ShenandoahInitLogger::print();\n+}\n+\n@@ -414,2 +478,0 @@\n-    } else if (strcmp(ShenandoahGCMode, \"iu\") == 0) {\n-      _gc_mode = new ShenandoahIUMode();\n@@ -418,0 +480,2 @@\n+    } else if (strcmp(ShenandoahGCMode, \"generational\") == 0) {\n+      _gc_mode = new ShenandoahGenerationalMode();\n@@ -438,13 +502,2 @@\n-  assert(_gc_mode != nullptr, \"Must be initialized\");\n-  _heuristics = _gc_mode->initialize_heuristics();\n-\n-  if (_heuristics->is_diagnostic() && !UnlockDiagnosticVMOptions) {\n-    vm_exit_during_initialization(\n-            err_msg(\"Heuristics \\\"%s\\\" is diagnostic, and must be enabled via -XX:+UnlockDiagnosticVMOptions.\",\n-                    _heuristics->name()));\n-  }\n-  if (_heuristics->is_experimental() && !UnlockExperimentalVMOptions) {\n-    vm_exit_during_initialization(\n-            err_msg(\"Heuristics \\\"%s\\\" is experimental, and must be enabled via -XX:+UnlockExperimentalVMOptions.\",\n-                    _heuristics->name()));\n-  }\n+  _global_generation = new ShenandoahGlobalGeneration(mode()->is_generational(), max_workers(), max_capacity());\n+  _global_generation->initialize_heuristics(mode());\n@@ -460,0 +513,2 @@\n+  _gc_generation(nullptr),\n+  _active_generation(nullptr),\n@@ -461,1 +516,0 @@\n-  _used(0),\n@@ -463,2 +517,1 @@\n-  _bytes_allocated_since_gc_start(0),\n-  _max_workers(MAX2(ConcGCThreads, ParallelGCThreads)),\n+  _max_workers(MAX3(ConcGCThreads, ParallelGCThreads, 1U)),\n@@ -470,1 +523,1 @@\n-  _update_refs_iterator(this),\n+  _affiliations(nullptr),\n@@ -472,0 +525,4 @@\n+  _gc_no_progress_count(0),\n+  _cancel_requested_time(0),\n+  _update_refs_iterator(this),\n+  _global_generation(nullptr),\n@@ -473,0 +530,3 @@\n+  _uncommit_thread(nullptr),\n+  _young_generation(nullptr),\n+  _old_generation(nullptr),\n@@ -475,1 +535,0 @@\n-  _heuristics(nullptr),\n@@ -487,1 +546,0 @@\n-  _ref_processor(new ShenandoahReferenceProcessor(MAX2(_max_workers, 1U))),\n@@ -497,1 +555,1 @@\n-  \/\/ Initialize GC mode early, so we can adjust barrier support\n+  \/\/ Initialize GC mode early, many subsequent initialization procedures depend on it\n@@ -499,15 +557,1 @@\n-  BarrierSet::set_barrier_set(new ShenandoahBarrierSet(this));\n-\n-  _max_workers = MAX2(_max_workers, 1U);\n-  _workers = new ShenandoahWorkerThreads(\"Shenandoah GC Threads\", _max_workers);\n-  if (_workers == nullptr) {\n-    vm_exit_during_initialization(\"Failed necessary allocation.\");\n-  } else {\n-    _workers->initialize_workers();\n-  }\n-\n-  if (ParallelGCThreads > 1) {\n-    _safepoint_workers = new ShenandoahWorkerThreads(\"Safepoint Cleanup Thread\",\n-                                                ParallelGCThreads);\n-    _safepoint_workers->initialize_workers();\n-  }\n+  _cancelled_gc.set(GCCause::_no_gc);\n@@ -520,29 +564,0 @@\n-class ShenandoahResetBitmapTask : public WorkerTask {\n-private:\n-  ShenandoahRegionIterator _regions;\n-\n-public:\n-  ShenandoahResetBitmapTask() :\n-    WorkerTask(\"Shenandoah Reset Bitmap\") {}\n-\n-  void work(uint worker_id) {\n-    ShenandoahHeapRegion* region = _regions.next();\n-    ShenandoahHeap* heap = ShenandoahHeap::heap();\n-    ShenandoahMarkingContext* const ctx = heap->marking_context();\n-    while (region != nullptr) {\n-      if (heap->is_bitmap_slice_committed(region)) {\n-        ctx->clear_bitmap(region);\n-      }\n-      region = _regions.next();\n-    }\n-  }\n-};\n-\n-void ShenandoahHeap::reset_mark_bitmap() {\n-  assert_gc_workers(_workers->active_workers());\n-  mark_incomplete_marking_context();\n-\n-  ShenandoahResetBitmapTask task;\n-  _workers->run_task(&task);\n-}\n-\n@@ -563,1 +578,6 @@\n-  if (is_concurrent_mark_in_progress())        st->print(\"marking, \");\n+  if (!mode()->is_generational()) {\n+    if (is_concurrent_mark_in_progress())      st->print(\"marking,\");\n+  } else {\n+    if (is_concurrent_old_mark_in_progress())    st->print(\"old marking, \");\n+    if (is_concurrent_young_mark_in_progress())  st->print(\"young marking, \");\n+  }\n@@ -607,1 +627,0 @@\n-    assert(thread->is_Worker_thread(), \"Only worker thread expected\");\n@@ -614,0 +633,4 @@\n+\n+  \/\/ Schedule periodic task to report on gc thread CPU utilization\n+  _mmu_tracker.initialize();\n+\n@@ -622,0 +645,3 @@\n+\n+  \/\/ Note that the safepoint workers may require gclabs if the threads are used to create a heap dump\n+  \/\/ during a concurrent evacuation phase.\n@@ -627,2 +653,0 @@\n-  _heuristics->initialize();\n-\n@@ -632,0 +656,4 @@\n+ShenandoahHeuristics* ShenandoahHeap::heuristics() {\n+  return _global_generation->heuristics();\n+}\n+\n@@ -633,1 +661,1 @@\n-  return Atomic::load(&_used);\n+  return global_generation()->used();\n@@ -650,2 +678,43 @@\n-void ShenandoahHeap::increase_used(size_t bytes) {\n-  Atomic::add(&_used, bytes, memory_order_relaxed);\n+\/\/ For tracking usage based on allocations, it should be the case that:\n+\/\/ * The sum of regions::used == heap::used\n+\/\/ * The sum of a generation's regions::used == generation::used\n+\/\/ * The sum of a generation's humongous regions::free == generation::humongous_waste\n+\/\/ These invariants are checked by the verifier on GC safepoints.\n+\/\/\n+\/\/ Additional notes:\n+\/\/ * When a mutator's allocation request causes a region to be retired, the\n+\/\/   free memory left in that region is considered waste. It does not contribute\n+\/\/   to the usage, but it _does_ contribute to allocation rate.\n+\/\/ * The bottom of a PLAB must be aligned on card size. In some cases this will\n+\/\/   require padding in front of the PLAB (a filler object). Because this padding\n+\/\/   is included in the region's used memory we include the padding in the usage\n+\/\/   accounting as waste.\n+\/\/ * Mutator allocations are used to compute an allocation rate. They are also\n+\/\/   sent to the Pacer for those purposes.\n+\/\/ * There are three sources of waste:\n+\/\/  1. The padding used to align a PLAB on card size\n+\/\/  2. Region's free is less than minimum TLAB size and is retired\n+\/\/  3. The unused portion of memory in the last region of a humongous object\n+void ShenandoahHeap::increase_used(const ShenandoahAllocRequest& req) {\n+  size_t actual_bytes = req.actual_size() * HeapWordSize;\n+  size_t wasted_bytes = req.waste() * HeapWordSize;\n+  ShenandoahGeneration* generation = generation_for(req.affiliation());\n+\n+  if (req.is_gc_alloc()) {\n+    assert(wasted_bytes == 0 || req.type() == ShenandoahAllocRequest::_alloc_plab, \"Only PLABs have waste\");\n+    increase_used(generation, actual_bytes + wasted_bytes);\n+  } else {\n+    assert(req.is_mutator_alloc(), \"Expected mutator alloc here\");\n+    \/\/ padding and actual size both count towards allocation counter\n+    generation->increase_allocated(actual_bytes + wasted_bytes);\n+\n+    \/\/ only actual size counts toward usage for mutator allocations\n+    increase_used(generation, actual_bytes);\n+\n+    \/\/ notify pacer of both actual size and waste\n+    notify_mutator_alloc_words(req.actual_size(), req.waste());\n+\n+    if (wasted_bytes > 0 && ShenandoahHeapRegion::requires_humongous(req.actual_size())) {\n+      increase_humongous_waste(generation,wasted_bytes);\n+    }\n+  }\n@@ -654,2 +723,5 @@\n-void ShenandoahHeap::set_used(size_t bytes) {\n-  Atomic::store(&_used, bytes);\n+void ShenandoahHeap::increase_humongous_waste(ShenandoahGeneration* generation, size_t bytes) {\n+  generation->increase_humongous_waste(bytes);\n+  if (!generation->is_global()) {\n+    global_generation()->increase_humongous_waste(bytes);\n+  }\n@@ -658,3 +730,5 @@\n-void ShenandoahHeap::decrease_used(size_t bytes) {\n-  assert(used() >= bytes, \"never decrease heap size by more than we've left\");\n-  Atomic::sub(&_used, bytes, memory_order_relaxed);\n+void ShenandoahHeap::decrease_humongous_waste(ShenandoahGeneration* generation, size_t bytes) {\n+  generation->decrease_humongous_waste(bytes);\n+  if (!generation->is_global()) {\n+    global_generation()->decrease_humongous_waste(bytes);\n+  }\n@@ -663,2 +737,5 @@\n-void ShenandoahHeap::increase_allocated(size_t bytes) {\n-  Atomic::add(&_bytes_allocated_since_gc_start, bytes, memory_order_relaxed);\n+void ShenandoahHeap::increase_used(ShenandoahGeneration* generation, size_t bytes) {\n+  generation->increase_used(bytes);\n+  if (!generation->is_global()) {\n+    global_generation()->increase_used(bytes);\n+  }\n@@ -667,4 +744,4 @@\n-void ShenandoahHeap::notify_mutator_alloc_words(size_t words, bool waste) {\n-  size_t bytes = words * HeapWordSize;\n-  if (!waste) {\n-    increase_used(bytes);\n+void ShenandoahHeap::decrease_used(ShenandoahGeneration* generation, size_t bytes) {\n+  generation->decrease_used(bytes);\n+  if (!generation->is_global()) {\n+    global_generation()->decrease_used(bytes);\n@@ -672,1 +749,3 @@\n-  increase_allocated(bytes);\n+}\n+\n+void ShenandoahHeap::notify_mutator_alloc_words(size_t words, size_t waste) {\n@@ -675,2 +754,2 @@\n-    if (waste) {\n-      pacer()->claim_for_alloc(words, true);\n+    if (waste > 0) {\n+      pacer()->claim_for_alloc<true>(waste);\n@@ -713,10 +792,1 @@\n-  if (is_in_reserved(p)) {\n-    if (is_full_gc_move_in_progress()) {\n-      \/\/ Full GC move is running, we do not have a consistent region\n-      \/\/ information yet. But we know the pointer is in heap.\n-      return true;\n-    }\n-    \/\/ Now check if we point to a live section in active region.\n-    ShenandoahHeapRegion* r = heap_region_containing(p);\n-    return (r->is_active() && p < r->top());\n-  } else {\n+  if (!is_in_reserved(p)) {\n@@ -725,1 +795,0 @@\n-}\n@@ -727,2 +796,11 @@\n-void ShenandoahHeap::op_uncommit(double shrink_before, size_t shrink_until) {\n-  assert (ShenandoahUncommit, \"should be enabled\");\n+  if (is_full_gc_move_in_progress()) {\n+    \/\/ Full GC move is running, we do not have a consistent region\n+    \/\/ information yet. But we know the pointer is in heap.\n+    return true;\n+  }\n+\n+  \/\/ Now check if we point to a live section in active region.\n+  const ShenandoahHeapRegion* r = heap_region_containing(p);\n+  if (p >= r->top()) {\n+    return false;\n+  }\n@@ -730,4 +808,3 @@\n-  \/\/ Application allocates from the beginning of the heap, and GC allocates at\n-  \/\/ the end of it. It is more efficient to uncommit from the end, so that applications\n-  \/\/ could enjoy the near committed regions. GC allocations are much less frequent,\n-  \/\/ and therefore can accept the committing costs.\n+  if (r->is_active()) {\n+    return true;\n+  }\n@@ -735,9 +812,6 @@\n-  size_t count = 0;\n-  for (size_t i = num_regions(); i > 0; i--) { \/\/ care about size_t underflow\n-    ShenandoahHeapRegion* r = get_region(i - 1);\n-    if (r->is_empty_committed() && (r->empty_time() < shrink_before)) {\n-      ShenandoahHeapLocker locker(lock());\n-      if (r->is_empty_committed()) {\n-        if (committed() < shrink_until + ShenandoahHeapRegion::region_size_bytes()) {\n-          break;\n-        }\n+  \/\/ The region is trash, but won't be recycled until after concurrent weak\n+  \/\/ roots. We also don't allow mutators to allocate from trash regions\n+  \/\/ during weak roots. Concurrent class unloading may access unmarked oops\n+  \/\/ in trash regions.\n+  return r->is_trash() && is_concurrent_weak_root_in_progress();\n+}\n@@ -745,5 +819,3 @@\n-        r->make_uncommitted();\n-        count++;\n-      }\n-    }\n-    SpinPause(); \/\/ allow allocators to take the lock\n+void ShenandoahHeap::notify_soft_max_changed() {\n+  if (_uncommit_thread != nullptr) {\n+    _uncommit_thread->notify_soft_max_changed();\n@@ -751,0 +823,1 @@\n+}\n@@ -752,2 +825,3 @@\n-  if (count > 0) {\n-    control_thread()->notify_heap_changed();\n+void ShenandoahHeap::notify_explicit_gc_requested() {\n+  if (_uncommit_thread != nullptr) {\n+    _uncommit_thread->notify_explicit_gc_requested();\n@@ -757,0 +831,33 @@\n+bool ShenandoahHeap::check_soft_max_changed() {\n+  size_t new_soft_max = Atomic::load(&SoftMaxHeapSize);\n+  size_t old_soft_max = soft_max_capacity();\n+  if (new_soft_max != old_soft_max) {\n+    new_soft_max = MAX2(min_capacity(), new_soft_max);\n+    new_soft_max = MIN2(max_capacity(), new_soft_max);\n+    if (new_soft_max != old_soft_max) {\n+      log_info(gc)(\"Soft Max Heap Size: \" SIZE_FORMAT \"%s -> \" SIZE_FORMAT \"%s\",\n+                   byte_size_in_proper_unit(old_soft_max), proper_unit_for_byte_size(old_soft_max),\n+                   byte_size_in_proper_unit(new_soft_max), proper_unit_for_byte_size(new_soft_max)\n+      );\n+      set_soft_max_capacity(new_soft_max);\n+      return true;\n+    }\n+  }\n+  return false;\n+}\n+\n+void ShenandoahHeap::notify_heap_changed() {\n+  \/\/ Update monitoring counters when we took a new region. This amortizes the\n+  \/\/ update costs on slow path.\n+  monitoring_support()->notify_heap_changed();\n+  _heap_changed.try_set();\n+}\n+\n+void ShenandoahHeap::set_forced_counters_update(bool value) {\n+  monitoring_support()->set_forced_counters_update(value);\n+}\n+\n+void ShenandoahHeap::handle_force_counters_update() {\n+  monitoring_support()->handle_force_counters_update();\n+}\n+\n@@ -763,0 +870,1 @@\n+\n@@ -769,0 +877,1 @@\n+  log_debug(gc, free)(\"Set new GCLAB size: \" SIZE_FORMAT, new_size);\n@@ -774,0 +883,1 @@\n+    log_debug(gc, free)(\"New gclab size (\" SIZE_FORMAT \") is too small for \" SIZE_FORMAT, new_size, size);\n@@ -803,0 +913,1 @@\n+\/\/ Called from stubs in JIT code or interpreter\n@@ -844,1 +955,1 @@\n-    \/\/ Allocation failed, block until control thread reacted, then retry allocation.\n+    \/\/ Check that gc overhead is not exceeded.\n@@ -846,10 +957,40 @@\n-    \/\/ It might happen that one of the threads requesting allocation would unblock\n-    \/\/ way later after GC happened, only to fail the second allocation, because\n-    \/\/ other threads have already depleted the free storage. In this case, a better\n-    \/\/ strategy is to try again, as long as GC makes progress (or until at least\n-    \/\/ one full GC has completed).\n-    size_t original_count = shenandoah_policy()->full_gc_count();\n-    while (result == nullptr\n-        && (_progress_last_gc.is_set() || original_count == shenandoah_policy()->full_gc_count())) {\n-      control_thread()->handle_alloc_failure(req);\n-      result = allocate_memory_under_lock(req, in_new_region);\n+    \/\/ Shenandoah will grind along for quite a while allocating one\n+    \/\/ object at a time using shared (non-tlab) allocations. This check\n+    \/\/ is testing that the GC overhead limit has not been exceeded.\n+    \/\/ This will notify the collector to start a cycle, but will raise\n+    \/\/ an OOME to the mutator if the last Full GCs have not made progress.\n+    \/\/ gc_no_progress_count is incremented following each degen or full GC that fails to achieve is_good_progress().\n+    if ((result == nullptr) && !req.is_lab_alloc() && (get_gc_no_progress_count() > ShenandoahNoProgressThreshold)) {\n+      control_thread()->handle_alloc_failure(req, false);\n+      req.set_actual_size(0);\n+      return nullptr;\n+    }\n+\n+    if (result == nullptr) {\n+      \/\/ Block until control thread reacted, then retry allocation.\n+      \/\/\n+      \/\/ It might happen that one of the threads requesting allocation would unblock\n+      \/\/ way later after GC happened, only to fail the second allocation, because\n+      \/\/ other threads have already depleted the free storage. In this case, a better\n+      \/\/ strategy is to try again, until at least one full GC has completed.\n+      \/\/\n+      \/\/ Stop retrying and return nullptr to cause OOMError exception if our allocation failed even after:\n+      \/\/   a) We experienced a GC that had good progress, or\n+      \/\/   b) We experienced at least one Full GC (whether or not it had good progress)\n+\n+      const size_t original_count = shenandoah_policy()->full_gc_count();\n+      while (result == nullptr && should_retry_allocation(original_count)) {\n+        control_thread()->handle_alloc_failure(req, true);\n+        result = allocate_memory_under_lock(req, in_new_region);\n+      }\n+      if (result != nullptr) {\n+        \/\/ If our allocation request has been satisfied after it initially failed, we count this as good gc progress\n+        notify_gc_progress();\n+      }\n+      if (log_develop_is_enabled(Debug, gc, alloc)) {\n+        ResourceMark rm;\n+        log_debug(gc, alloc)(\"Thread: %s, Result: \" PTR_FORMAT \", Request: %s, Size: \" SIZE_FORMAT\n+                             \", Original: \" SIZE_FORMAT \", Latest: \" SIZE_FORMAT,\n+                             Thread::current()->name(), p2i(result), req.type_string(), req.size(),\n+                             original_count, get_gc_no_progress_count());\n+      }\n@@ -865,1 +1006,5 @@\n-    control_thread()->notify_heap_changed();\n+    notify_heap_changed();\n+  }\n+\n+  if (result == nullptr) {\n+    req.set_actual_size(0);\n@@ -868,0 +1013,4 @@\n+  \/\/ This is called regardless of the outcome of the allocation to account\n+  \/\/ for any waste created by retiring regions with this request.\n+  increase_used(req);\n+\n@@ -877,2 +1026,0 @@\n-      notify_mutator_alloc_words(actual, false);\n-\n@@ -885,2 +1032,0 @@\n-    } else {\n-      increase_used(actual*HeapWordSize);\n@@ -893,0 +1038,5 @@\n+inline bool ShenandoahHeap::should_retry_allocation(size_t original_full_gc_count) const {\n+  return shenandoah_policy()->full_gc_count() == original_full_gc_count\n+      && !shenandoah_policy()->is_at_shutdown();\n+}\n+\n@@ -899,1 +1049,37 @@\n-  return _free_set->allocate(req, in_new_region);\n+\n+  \/\/ Make sure the old generation has room for either evacuations or promotions before trying to allocate.\n+  if (req.is_old() && !old_generation()->can_allocate(req)) {\n+    return nullptr;\n+  }\n+\n+  \/\/ If TLAB request size is greater than available, allocate() will attempt to downsize request to fit within available\n+  \/\/ memory.\n+  HeapWord* result = _free_set->allocate(req, in_new_region);\n+\n+  \/\/ Record the plab configuration for this result and register the object.\n+  if (result != nullptr && req.is_old()) {\n+    old_generation()->configure_plab_for_current_thread(req);\n+    if (req.type() == ShenandoahAllocRequest::_alloc_shared_gc) {\n+      \/\/ Register the newly allocated object while we're holding the global lock since there's no synchronization\n+      \/\/ built in to the implementation of register_object().  There are potential races when multiple independent\n+      \/\/ threads are allocating objects, some of which might span the same card region.  For example, consider\n+      \/\/ a card table's memory region within which three objects are being allocated by three different threads:\n+      \/\/\n+      \/\/ objects being \"concurrently\" allocated:\n+      \/\/    [-----a------][-----b-----][--------------c------------------]\n+      \/\/            [---- card table memory range --------------]\n+      \/\/\n+      \/\/ Before any objects are allocated, this card's memory range holds no objects.  Note that allocation of object a\n+      \/\/ wants to set the starts-object, first-start, and last-start attributes of the preceding card region.\n+      \/\/ Allocation of object b wants to set the starts-object, first-start, and last-start attributes of this card region.\n+      \/\/ Allocation of object c also wants to set the starts-object, first-start, and last-start attributes of this\n+      \/\/ card region.\n+      \/\/\n+      \/\/ The thread allocating b and the thread allocating c can \"race\" in various ways, resulting in confusion, such as\n+      \/\/ last-start representing object b while first-start represents object c.  This is why we need to require all\n+      \/\/ register_object() invocations to be \"mutually exclusive\" with respect to each card's memory range.\n+      old_generation()->card_scan()->register_object(result);\n+    }\n+  }\n+\n+  return result;\n@@ -914,2 +1100,2 @@\n-  if (heuristics()->can_unload_classes()) {\n-    ShenandoahHeuristics* h = heuristics();\n+  ShenandoahHeuristics* h = global_generation()->heuristics();\n+  if (h->can_unload_classes()) {\n@@ -1007,0 +1193,63 @@\n+class ShenandoahRetireGCLABClosure : public ThreadClosure {\n+private:\n+  bool const _resize;\n+public:\n+  explicit ShenandoahRetireGCLABClosure(bool resize) : _resize(resize) {}\n+  void do_thread(Thread* thread) override {\n+    PLAB* gclab = ShenandoahThreadLocalData::gclab(thread);\n+    assert(gclab != nullptr, \"GCLAB should be initialized for %s\", thread->name());\n+    gclab->retire();\n+    if (_resize && ShenandoahThreadLocalData::gclab_size(thread) > 0) {\n+      ShenandoahThreadLocalData::set_gclab_size(thread, 0);\n+    }\n+\n+    if (ShenandoahHeap::heap()->mode()->is_generational()) {\n+      PLAB* plab = ShenandoahThreadLocalData::plab(thread);\n+      assert(plab != nullptr, \"PLAB should be initialized for %s\", thread->name());\n+\n+      \/\/ There are two reasons to retire all plabs between old-gen evacuation passes.\n+      \/\/  1. We need to make the plab memory parsable by remembered-set scanning.\n+      \/\/  2. We need to establish a trustworthy UpdateWaterMark value within each old-gen heap region\n+      ShenandoahGenerationalHeap::heap()->retire_plab(plab, thread);\n+\n+      \/\/ Re-enable promotions for the next evacuation phase.\n+      ShenandoahThreadLocalData::enable_plab_promotions(thread);\n+\n+      \/\/ Reset the fill size for next evacuation phase.\n+      if (_resize && ShenandoahThreadLocalData::plab_size(thread) > 0) {\n+        ShenandoahThreadLocalData::set_plab_size(thread, 0);\n+      }\n+    }\n+  }\n+};\n+\n+class ShenandoahGCStatePropagator : public HandshakeClosure {\n+public:\n+  explicit ShenandoahGCStatePropagator(char gc_state) :\n+    HandshakeClosure(\"Shenandoah GC State Change\"),\n+    _gc_state(gc_state) {}\n+\n+  void do_thread(Thread* thread) override {\n+    ShenandoahThreadLocalData::set_gc_state(thread, _gc_state);\n+  }\n+private:\n+  char _gc_state;\n+};\n+\n+class ShenandoahPrepareForUpdateRefs : public HandshakeClosure {\n+public:\n+  explicit ShenandoahPrepareForUpdateRefs(char gc_state) :\n+    HandshakeClosure(\"Shenandoah Prepare for Update Refs\"),\n+    _retire(ResizeTLAB), _propagator(gc_state) {}\n+\n+  void do_thread(Thread* thread) override {\n+    _propagator.do_thread(thread);\n+    if (ShenandoahThreadLocalData::gclab(thread) != nullptr) {\n+      _retire.do_thread(thread);\n+    }\n+  }\n+private:\n+  ShenandoahRetireGCLABClosure _retire;\n+  ShenandoahGCStatePropagator _propagator;\n+};\n+\n@@ -1012,0 +1261,145 @@\n+void ShenandoahHeap::concurrent_prepare_for_update_refs() {\n+  {\n+    \/\/ Java threads take this lock while they are being attached and added to the list of thread.\n+    \/\/ If another thread holds this lock before we update the gc state, it will receive a stale\n+    \/\/ gc state, but they will have been added to the list of java threads and so will be corrected\n+    \/\/ by the following handshake.\n+    MutexLocker lock(Threads_lock);\n+\n+    \/\/ A cancellation at this point means the degenerated cycle must resume from update-refs.\n+    set_gc_state_concurrent(EVACUATION, false);\n+    set_gc_state_concurrent(WEAK_ROOTS, false);\n+    set_gc_state_concurrent(UPDATE_REFS, true);\n+  }\n+\n+  \/\/ This will propagate the gc state and retire gclabs and plabs for threads that require it.\n+  ShenandoahPrepareForUpdateRefs prepare_for_update_refs(_gc_state.raw_value());\n+\n+  \/\/ The handshake won't touch worker threads (or control thread, or VM thread), so do those separately.\n+  Threads::non_java_threads_do(&prepare_for_update_refs);\n+\n+  \/\/ Now retire gclabs and plabs and propagate gc_state for mutator threads\n+  Handshake::execute(&prepare_for_update_refs);\n+\n+  _update_refs_iterator.reset();\n+}\n+\n+class ShenandoahCompositeHandshakeClosure : public HandshakeClosure {\n+  HandshakeClosure* _handshake_1;\n+  HandshakeClosure* _handshake_2;\n+  public:\n+    ShenandoahCompositeHandshakeClosure(HandshakeClosure* handshake_1, HandshakeClosure* handshake_2) :\n+      HandshakeClosure(handshake_2->name()),\n+      _handshake_1(handshake_1), _handshake_2(handshake_2) {}\n+\n+  void do_thread(Thread* thread) override {\n+      _handshake_1->do_thread(thread);\n+      _handshake_2->do_thread(thread);\n+    }\n+};\n+\n+void ShenandoahHeap::concurrent_final_roots(HandshakeClosure* handshake_closure) {\n+  {\n+    assert(!is_evacuation_in_progress(), \"Should not evacuate for abbreviated or old cycles\");\n+    MutexLocker lock(Threads_lock);\n+    set_gc_state_concurrent(WEAK_ROOTS, false);\n+  }\n+\n+  ShenandoahGCStatePropagator propagator(_gc_state.raw_value());\n+  Threads::non_java_threads_do(&propagator);\n+  if (handshake_closure == nullptr) {\n+    Handshake::execute(&propagator);\n+  } else {\n+    ShenandoahCompositeHandshakeClosure composite(&propagator, handshake_closure);\n+    Handshake::execute(&composite);\n+  }\n+}\n+\n+oop ShenandoahHeap::evacuate_object(oop p, Thread* thread) {\n+  assert(thread == Thread::current(), \"Expected thread parameter to be current thread.\");\n+  if (ShenandoahThreadLocalData::is_oom_during_evac(thread)) {\n+    \/\/ This thread went through the OOM during evac protocol. It is safe to return\n+    \/\/ the forward pointer. It must not attempt to evacuate any other objects.\n+    return ShenandoahBarrierSet::resolve_forwarded(p);\n+  }\n+\n+  assert(ShenandoahThreadLocalData::is_evac_allowed(thread), \"must be enclosed in oom-evac scope\");\n+\n+  ShenandoahHeapRegion* r = heap_region_containing(p);\n+  assert(!r->is_humongous(), \"never evacuate humongous objects\");\n+\n+  ShenandoahAffiliation target_gen = r->affiliation();\n+  return try_evacuate_object(p, thread, r, target_gen);\n+}\n+\n+oop ShenandoahHeap::try_evacuate_object(oop p, Thread* thread, ShenandoahHeapRegion* from_region,\n+                                               ShenandoahAffiliation target_gen) {\n+  assert(target_gen == YOUNG_GENERATION, \"Only expect evacuations to young in this mode\");\n+  assert(from_region->is_young(), \"Only expect evacuations from young in this mode\");\n+  bool alloc_from_lab = true;\n+  HeapWord* copy = nullptr;\n+  size_t size = p->size();\n+\n+#ifdef ASSERT\n+  if (ShenandoahOOMDuringEvacALot &&\n+      (os::random() & 1) == 0) { \/\/ Simulate OOM every ~2nd slow-path call\n+    copy = nullptr;\n+  } else {\n+#endif\n+    if (UseTLAB) {\n+      copy = allocate_from_gclab(thread, size);\n+    }\n+    if (copy == nullptr) {\n+      \/\/ If we failed to allocate in LAB, we'll try a shared allocation.\n+      ShenandoahAllocRequest req = ShenandoahAllocRequest::for_shared_gc(size, target_gen);\n+      copy = allocate_memory(req);\n+      alloc_from_lab = false;\n+    }\n+#ifdef ASSERT\n+  }\n+#endif\n+\n+  if (copy == nullptr) {\n+    control_thread()->handle_alloc_failure_evac(size);\n+\n+    _oom_evac_handler.handle_out_of_memory_during_evacuation();\n+\n+    return ShenandoahBarrierSet::resolve_forwarded(p);\n+  }\n+\n+  \/\/ Copy the object:\n+  Copy::aligned_disjoint_words(cast_from_oop<HeapWord*>(p), copy, size);\n+\n+  \/\/ Try to install the new forwarding pointer.\n+  oop copy_val = cast_to_oop(copy);\n+  oop result = ShenandoahForwarding::try_update_forwardee(p, copy_val);\n+  if (result == copy_val) {\n+    \/\/ Successfully evacuated. Our copy is now the public one!\n+    ContinuationGCSupport::relativize_stack_chunk(copy_val);\n+    shenandoah_assert_correct(nullptr, copy_val);\n+    return copy_val;\n+  }  else {\n+    \/\/ Failed to evacuate. We need to deal with the object that is left behind. Since this\n+    \/\/ new allocation is certainly after TAMS, it will be considered live in the next cycle.\n+    \/\/ But if it happens to contain references to evacuated regions, those references would\n+    \/\/ not get updated for this stale copy during this cycle, and we will crash while scanning\n+    \/\/ it the next cycle.\n+    if (alloc_from_lab) {\n+      \/\/ For LAB allocations, it is enough to rollback the allocation ptr. Either the next\n+      \/\/ object will overwrite this stale copy, or the filler object on LAB retirement will\n+      \/\/ do this.\n+      ShenandoahThreadLocalData::gclab(thread)->undo_allocation(copy, size);\n+    } else {\n+      \/\/ For non-LAB allocations, we have no way to retract the allocation, and\n+      \/\/ have to explicitly overwrite the copy with the filler object. With that overwrite,\n+      \/\/ we have to keep the fwdptr initialized and pointing to our (stale) copy.\n+      assert(size >= ShenandoahHeap::min_fill_size(), \"previously allocated object known to be larger than min_size\");\n+      fill_with_object(copy, size);\n+      shenandoah_assert_correct(nullptr, copy_val);\n+      \/\/ For non-LAB allocations, the object has already been registered\n+    }\n+    shenandoah_assert_correct(nullptr, result);\n+    return result;\n+  }\n+}\n+\n@@ -1065,2 +1459,0 @@\n-  }\n-};\n@@ -1068,11 +1460,4 @@\n-class ShenandoahRetireGCLABClosure : public ThreadClosure {\n-private:\n-  bool const _resize;\n-public:\n-  ShenandoahRetireGCLABClosure(bool resize) : _resize(resize) {}\n-  void do_thread(Thread* thread) {\n-    PLAB* gclab = ShenandoahThreadLocalData::gclab(thread);\n-    assert(gclab != nullptr, \"GCLAB should be initialized for %s\", thread->name());\n-    gclab->retire();\n-    if (_resize && ShenandoahThreadLocalData::gclab_size(thread) > 0) {\n-      ShenandoahThreadLocalData::set_gclab_size(thread, 0);\n+    if (ShenandoahHeap::heap()->mode()->is_generational()) {\n+      PLAB* plab = ShenandoahThreadLocalData::plab(thread);\n+      assert(plab != nullptr, \"PLAB should be initialized for %s\", thread->name());\n+      assert(plab->words_remaining() == 0, \"PLAB should not need retirement\");\n@@ -1095,0 +1480,4 @@\n+\n+  if (safepoint_workers() != nullptr) {\n+    safepoint_workers()->threads_do(&cl);\n+  }\n@@ -1130,0 +1519,1 @@\n+\n@@ -1193,1 +1583,12 @@\n-  tcl->do_thread(_control_thread);\n+  if (_shenandoah_policy->is_at_shutdown()) {\n+    return;\n+  }\n+\n+  if (_control_thread != nullptr) {\n+    tcl->do_thread(_control_thread);\n+  }\n+\n+  if (_uncommit_thread != nullptr) {\n+    tcl->do_thread(_uncommit_thread);\n+  }\n+\n@@ -1218,0 +1619,42 @@\n+void ShenandoahHeap::set_gc_generation(ShenandoahGeneration* generation) {\n+  shenandoah_assert_control_or_vm_thread_at_safepoint();\n+  _gc_generation = generation;\n+}\n+\n+\/\/ Active generation may only be set by the VM thread at a safepoint.\n+void ShenandoahHeap::set_active_generation() {\n+  assert(Thread::current()->is_VM_thread(), \"Only the VM Thread\");\n+  assert(SafepointSynchronize::is_at_safepoint(), \"Only at a safepoint!\");\n+  assert(_gc_generation != nullptr, \"Will set _active_generation to nullptr\");\n+  _active_generation = _gc_generation;\n+}\n+\n+void ShenandoahHeap::on_cycle_start(GCCause::Cause cause, ShenandoahGeneration* generation) {\n+  shenandoah_policy()->record_collection_cause(cause);\n+\n+  const GCCause::Cause current = gc_cause();\n+  assert(current == GCCause::_no_gc, \"Over-writing cause: %s, with: %s\",\n+         GCCause::to_string(current), GCCause::to_string(cause));\n+  assert(_gc_generation == nullptr, \"Over-writing _gc_generation\");\n+\n+  set_gc_cause(cause);\n+  set_gc_generation(generation);\n+\n+  generation->heuristics()->record_cycle_start();\n+}\n+\n+void ShenandoahHeap::on_cycle_end(ShenandoahGeneration* generation) {\n+  assert(gc_cause() != GCCause::_no_gc, \"cause wasn't set\");\n+  assert(_gc_generation != nullptr, \"_gc_generation wasn't set\");\n+\n+  generation->heuristics()->record_cycle_end();\n+  if (mode()->is_generational() && generation->is_global()) {\n+    \/\/ If we just completed a GLOBAL GC, claim credit for completion of young-gen and old-gen GC as well\n+    young_generation()->heuristics()->record_cycle_end();\n+    old_generation()->heuristics()->record_cycle_end();\n+  }\n+\n+  set_gc_generation(nullptr);\n+  set_gc_cause(GCCause::_no_gc);\n+}\n+\n@@ -1552,23 +1995,0 @@\n-class ShenandoahInitMarkUpdateRegionStateClosure : public ShenandoahHeapRegionClosure {\n-private:\n-  ShenandoahMarkingContext* const _ctx;\n-public:\n-  ShenandoahInitMarkUpdateRegionStateClosure() : _ctx(ShenandoahHeap::heap()->marking_context()) {}\n-\n-  void heap_region_do(ShenandoahHeapRegion* r) {\n-    assert(!r->has_live(), \"Region \" SIZE_FORMAT \" should have no live data\", r->index());\n-    if (r->is_active()) {\n-      \/\/ Check if region needs updating its TAMS. We have updated it already during concurrent\n-      \/\/ reset, so it is very likely we don't need to do another write here.\n-      if (_ctx->top_at_mark_start(r) != r->top()) {\n-        _ctx->capture_top_at_mark_start(r);\n-      }\n-    } else {\n-      assert(_ctx->top_at_mark_start(r) == r->top(),\n-             \"Region \" SIZE_FORMAT \" should already have correct TAMS\", r->index());\n-    }\n-  }\n-\n-  bool is_thread_safe() { return true; }\n-};\n-\n@@ -1590,99 +2010,0 @@\n-class ShenandoahResetUpdateRegionStateClosure : public ShenandoahHeapRegionClosure {\n-private:\n-  ShenandoahMarkingContext* const _ctx;\n-public:\n-  ShenandoahResetUpdateRegionStateClosure() : _ctx(ShenandoahHeap::heap()->marking_context()) {}\n-\n-  void heap_region_do(ShenandoahHeapRegion* r) {\n-    if (r->is_active()) {\n-      \/\/ Reset live data and set TAMS optimistically. We would recheck these under the pause\n-      \/\/ anyway to capture any updates that happened since now.\n-      r->clear_live_data();\n-      _ctx->capture_top_at_mark_start(r);\n-    }\n-  }\n-\n-  bool is_thread_safe() { return true; }\n-};\n-\n-void ShenandoahHeap::prepare_gc() {\n-  reset_mark_bitmap();\n-\n-  ShenandoahResetUpdateRegionStateClosure cl;\n-  parallel_heap_region_iterate(&cl);\n-}\n-\n-class ShenandoahFinalMarkUpdateRegionStateClosure : public ShenandoahHeapRegionClosure {\n-private:\n-  ShenandoahMarkingContext* const _ctx;\n-  ShenandoahHeapLock* const _lock;\n-\n-public:\n-  ShenandoahFinalMarkUpdateRegionStateClosure() :\n-    _ctx(ShenandoahHeap::heap()->complete_marking_context()), _lock(ShenandoahHeap::heap()->lock()) {}\n-\n-  void heap_region_do(ShenandoahHeapRegion* r) {\n-    if (r->is_active()) {\n-      \/\/ All allocations past TAMS are implicitly live, adjust the region data.\n-      \/\/ Bitmaps\/TAMS are swapped at this point, so we need to poll complete bitmap.\n-      HeapWord *tams = _ctx->top_at_mark_start(r);\n-      HeapWord *top = r->top();\n-      if (top > tams) {\n-        r->increase_live_data_alloc_words(pointer_delta(top, tams));\n-      }\n-\n-      \/\/ We are about to select the collection set, make sure it knows about\n-      \/\/ current pinning status. Also, this allows trashing more regions that\n-      \/\/ now have their pinning status dropped.\n-      if (r->is_pinned()) {\n-        if (r->pin_count() == 0) {\n-          ShenandoahHeapLocker locker(_lock);\n-          r->make_unpinned();\n-        }\n-      } else {\n-        if (r->pin_count() > 0) {\n-          ShenandoahHeapLocker locker(_lock);\n-          r->make_pinned();\n-        }\n-      }\n-\n-      \/\/ Remember limit for updating refs. It's guaranteed that we get no\n-      \/\/ from-space-refs written from here on.\n-      r->set_update_watermark_at_safepoint(r->top());\n-    } else {\n-      assert(!r->has_live(), \"Region \" SIZE_FORMAT \" should have no live data\", r->index());\n-      assert(_ctx->top_at_mark_start(r) == r->top(),\n-             \"Region \" SIZE_FORMAT \" should have correct TAMS\", r->index());\n-    }\n-  }\n-\n-  bool is_thread_safe() { return true; }\n-};\n-\n-void ShenandoahHeap::prepare_regions_and_collection_set(bool concurrent) {\n-  assert(!is_full_gc_in_progress(), \"Only for concurrent and degenerated GC\");\n-  {\n-    ShenandoahGCPhase phase(concurrent ? ShenandoahPhaseTimings::final_update_region_states :\n-                                         ShenandoahPhaseTimings::degen_gc_final_update_region_states);\n-    ShenandoahFinalMarkUpdateRegionStateClosure cl;\n-    parallel_heap_region_iterate(&cl);\n-\n-    assert_pinned_region_status();\n-  }\n-\n-  {\n-    ShenandoahGCPhase phase(concurrent ? ShenandoahPhaseTimings::choose_cset :\n-                                         ShenandoahPhaseTimings::degen_gc_choose_cset);\n-    ShenandoahHeapLocker locker(lock());\n-    _collection_set->clear();\n-    heuristics()->choose_collection_set(_collection_set);\n-  }\n-\n-  {\n-    ShenandoahGCPhase phase(concurrent ? ShenandoahPhaseTimings::final_rebuild_freeset :\n-                                         ShenandoahPhaseTimings::degen_gc_final_rebuild_freeset);\n-    ShenandoahHeapLocker locker(lock());\n-    _free_set->rebuild();\n-  }\n-}\n-\n@@ -1691,0 +2012,3 @@\n+  if (mode()->is_generational()) {\n+    old_generation()->set_parsable(false);\n+  }\n@@ -1699,1 +2023,2 @@\n-  ref_processor()->process_references(phase, workers(), false \/* concurrent *\/);\n+  shenandoah_assert_generations_reconciled();\n+  gc_generation()->ref_processor()->process_references(phase, workers(), false \/* concurrent *\/);\n@@ -1702,1 +2027,1 @@\n-void ShenandoahHeap::prepare_update_heap_references(bool concurrent) {\n+void ShenandoahHeap::prepare_update_heap_references() {\n@@ -1709,3 +2034,1 @@\n-    ShenandoahGCPhase phase(concurrent ?\n-                            ShenandoahPhaseTimings::init_update_refs_manage_gclabs :\n-                            ShenandoahPhaseTimings::degen_gc_init_update_refs_manage_gclabs);\n+    ShenandoahGCPhase phase(ShenandoahPhaseTimings::degen_gc_init_update_refs_manage_gclabs);\n@@ -1718,1 +2041,1 @@\n-void ShenandoahHeap::propagate_gc_state_to_java_threads() {\n+void ShenandoahHeap::propagate_gc_state_to_all_threads() {\n@@ -1721,0 +2044,2 @@\n+    ShenandoahGCStatePropagator propagator(_gc_state.raw_value());\n+    Threads::threads_do(&propagator);\n@@ -1722,4 +2047,0 @@\n-    char state = gc_state();\n-    for (JavaThreadIteratorWithHandle jtiwh; JavaThread *t = jtiwh.next(); ) {\n-      ShenandoahThreadLocalData::set_gc_state(t, state);\n-    }\n@@ -1729,1 +2050,1 @@\n-void ShenandoahHeap::set_gc_state(uint mask, bool value) {\n+void ShenandoahHeap::set_gc_state_at_safepoint(uint mask, bool value) {\n@@ -1735,4 +2056,62 @@\n-void ShenandoahHeap::set_concurrent_mark_in_progress(bool in_progress) {\n-  assert(!has_forwarded_objects(), \"Not expected before\/after mark phase\");\n-  set_gc_state(MARKING, in_progress);\n-  ShenandoahBarrierSet::satb_mark_queue_set().set_active_all_threads(in_progress, !in_progress);\n+void ShenandoahHeap::set_gc_state_concurrent(uint mask, bool value) {\n+  \/\/ Holding the thread lock here assures that any thread created after we change the gc\n+  \/\/ state will have the correct state. It also prevents attaching threads from seeing\n+  \/\/ an inconsistent state. See ShenandoahBarrierSet::on_thread_attach for reference. Established\n+  \/\/ threads will use their thread local copy of the gc state (changed by a handshake, or on a\n+  \/\/ safepoint).\n+  assert(Threads_lock->is_locked(), \"Must hold thread lock for concurrent gc state change\");\n+  _gc_state.set_cond(mask, value);\n+}\n+\n+void ShenandoahHeap::set_concurrent_young_mark_in_progress(bool in_progress) {\n+  uint mask;\n+  assert(!has_forwarded_objects(), \"Young marking is not concurrent with evacuation\");\n+  if (!in_progress && is_concurrent_old_mark_in_progress()) {\n+    assert(mode()->is_generational(), \"Only generational GC has old marking\");\n+    assert(_gc_state.is_set(MARKING), \"concurrent_old_marking_in_progress implies MARKING\");\n+    \/\/ If old-marking is in progress when we turn off YOUNG_MARKING, leave MARKING (and OLD_MARKING) on\n+    mask = YOUNG_MARKING;\n+  } else {\n+    mask = MARKING | YOUNG_MARKING;\n+  }\n+  set_gc_state_at_safepoint(mask, in_progress);\n+  manage_satb_barrier(in_progress);\n+}\n+\n+void ShenandoahHeap::set_concurrent_old_mark_in_progress(bool in_progress) {\n+#ifdef ASSERT\n+  \/\/ has_forwarded_objects() iff UPDATE_REFS or EVACUATION\n+  bool has_forwarded = has_forwarded_objects();\n+  bool updating_or_evacuating = _gc_state.is_set(UPDATE_REFS | EVACUATION);\n+  bool evacuating = _gc_state.is_set(EVACUATION);\n+  assert ((has_forwarded == updating_or_evacuating) || (evacuating && !has_forwarded && collection_set()->is_empty()),\n+          \"Updating or evacuating iff has forwarded objects, or if evacuation phase is promoting in place without forwarding\");\n+#endif\n+  if (!in_progress && is_concurrent_young_mark_in_progress()) {\n+    \/\/ If young-marking is in progress when we turn off OLD_MARKING, leave MARKING (and YOUNG_MARKING) on\n+    assert(_gc_state.is_set(MARKING), \"concurrent_young_marking_in_progress implies MARKING\");\n+    set_gc_state_at_safepoint(OLD_MARKING, in_progress);\n+  } else {\n+    set_gc_state_at_safepoint(MARKING | OLD_MARKING, in_progress);\n+  }\n+  manage_satb_barrier(in_progress);\n+}\n+\n+bool ShenandoahHeap::is_prepare_for_old_mark_in_progress() const {\n+  return old_generation()->is_preparing_for_mark();\n+}\n+\n+void ShenandoahHeap::manage_satb_barrier(bool active) {\n+  if (is_concurrent_mark_in_progress()) {\n+    \/\/ Ignore request to deactivate barrier while concurrent mark is in progress.\n+    \/\/ Do not attempt to re-activate the barrier if it is already active.\n+    if (active && !ShenandoahBarrierSet::satb_mark_queue_set().is_active()) {\n+      ShenandoahBarrierSet::satb_mark_queue_set().set_active_all_threads(active, !active);\n+    }\n+  } else {\n+    \/\/ No concurrent marking is in progress so honor request to deactivate,\n+    \/\/ but only if the barrier is already active.\n+    if (!active && ShenandoahBarrierSet::satb_mark_queue_set().is_active()) {\n+      ShenandoahBarrierSet::satb_mark_queue_set().set_active_all_threads(active, !active);\n+    }\n+  }\n@@ -1743,1 +2122,1 @@\n-  set_gc_state(EVACUATION, in_progress);\n+  set_gc_state_at_safepoint(EVACUATION, in_progress);\n@@ -1755,1 +2134,1 @@\n-  set_gc_state(WEAK_ROOTS, cond);\n+  set_gc_state_at_safepoint(WEAK_ROOTS, cond);\n@@ -1766,3 +2145,14 @@\n-bool ShenandoahHeap::try_cancel_gc() {\n-  jbyte prev = _cancelled_gc.cmpxchg(CANCELLED, CANCELLABLE);\n-  return prev == CANCELLABLE;\n+bool ShenandoahHeap::try_cancel_gc(GCCause::Cause cause) {\n+  const GCCause::Cause prev = _cancelled_gc.xchg(cause);\n+  return prev == GCCause::_no_gc || prev == GCCause::_shenandoah_concurrent_gc;\n+}\n+\n+void ShenandoahHeap::cancel_concurrent_mark() {\n+  if (mode()->is_generational()) {\n+    young_generation()->cancel_marking();\n+    old_generation()->cancel_marking();\n+  }\n+\n+  global_generation()->cancel_marking();\n+\n+  ShenandoahBarrierSet::satb_mark_queue_set().abandon_partial_marking();\n@@ -1771,2 +2161,2 @@\n-void ShenandoahHeap::cancel_gc(GCCause::Cause cause) {\n-  if (try_cancel_gc()) {\n+bool ShenandoahHeap::cancel_gc(GCCause::Cause cause) {\n+  if (try_cancel_gc(cause)) {\n@@ -1774,1 +2164,1 @@\n-    log_info(gc)(\"%s\", msg.buffer());\n+    log_info(gc,thread)(\"%s\", msg.buffer());\n@@ -1776,0 +2166,2 @@\n+    _cancel_requested_time = os::elapsedTime();\n+    return true;\n@@ -1777,0 +2169,1 @@\n+  return false;\n@@ -1789,7 +2182,2 @@\n-  \/\/ Step 1. Notify control thread that we are in shutdown.\n-  \/\/ Note that we cannot do that with stop(), because stop() is blocking and waits for the actual shutdown.\n-  \/\/ Doing stop() here would wait for the normal GC cycle to complete, never falling through to cancel below.\n-  control_thread()->prepare_for_graceful_shutdown();\n-\n-  \/\/ Step 2. Notify GC workers that we are cancelling GC.\n-  cancel_gc(GCCause::_shenandoah_stop_vm);\n+  \/\/ Step 1. Stop reporting on gc thread cpu utilization\n+  mmu_tracker()->stop();\n@@ -1797,1 +2185,1 @@\n-  \/\/ Step 3. Wait until GC worker exits normally.\n+  \/\/ Step 2. Wait until GC worker exits normally (this will cancel any ongoing GC).\n@@ -1799,0 +2187,5 @@\n+\n+  \/\/ Stop 4. Shutdown uncommit thread.\n+  if (_uncommit_thread != nullptr) {\n+    _uncommit_thread->stop();\n+  }\n@@ -1838,1 +2231,1 @@\n-\/\/ Weak roots are either pre-evacuated (final mark) or updated (final updaterefs),\n+\/\/ Weak roots are either pre-evacuated (final mark) or updated (final update refs),\n@@ -1882,1 +2275,1 @@\n-  set_gc_state(HAS_FORWARDED, cond);\n+  set_gc_state_at_safepoint(HAS_FORWARDED, cond);\n@@ -1899,4 +2292,0 @@\n-size_t ShenandoahHeap::bytes_allocated_since_gc_start() {\n-  return Atomic::load(&_bytes_allocated_since_gc_start);\n-}\n-\n@@ -1904,1 +2293,6 @@\n-  Atomic::store(&_bytes_allocated_since_gc_start, (size_t)0);\n+  if (mode()->is_generational()) {\n+    young_generation()->reset_bytes_allocated_since_gc_start();\n+    old_generation()->reset_bytes_allocated_since_gc_start();\n+  }\n+\n+  global_generation()->reset_bytes_allocated_since_gc_start();\n@@ -1921,1 +2315,1 @@\n-  set_gc_state(UPDATEREFS, in_progress);\n+  set_gc_state_at_safepoint(UPDATE_REFS, in_progress);\n@@ -1968,2 +2362,5 @@\n-    assert((r->is_pinned() && r->pin_count() > 0) || (!r->is_pinned() && r->pin_count() == 0),\n-           \"Region \" SIZE_FORMAT \" pinning status is inconsistent\", i);\n+    shenandoah_assert_generations_reconciled();\n+    if (gc_generation()->contains(r)) {\n+      assert((r->is_pinned() && r->pin_count() > 0) || (!r->is_pinned() && r->pin_count() == 0),\n+             \"Region \" SIZE_FORMAT \" pinning status is inconsistent\", i);\n+    }\n@@ -2001,6 +2398,3 @@\n-    if (UseDynamicNumberOfGCThreads) {\n-      assert(nworkers <= ParallelGCThreads, \"Cannot use more than it has\");\n-    } else {\n-      \/\/ Use ParallelGCThreads inside safepoints\n-      assert(nworkers == ParallelGCThreads, \"Use ParallelGCThreads within safepoints\");\n-    }\n+    \/\/ Use ParallelGCThreads inside safepoints\n+    assert(nworkers == ParallelGCThreads, \"Use ParallelGCThreads (%u) within safepoint, not %u\",\n+           ParallelGCThreads, nworkers);\n@@ -2008,6 +2402,3 @@\n-    if (UseDynamicNumberOfGCThreads) {\n-      assert(nworkers <= ConcGCThreads, \"Cannot use more than it has\");\n-    } else {\n-      \/\/ Use ConcGCThreads outside safepoints\n-      assert(nworkers == ConcGCThreads, \"Use ConcGCThreads outside safepoints\");\n-    }\n+    \/\/ Use ConcGCThreads outside safepoints\n+    assert(nworkers == ConcGCThreads, \"Use ConcGCThreads (%u) outside safepoints, %u\",\n+           ConcGCThreads, nworkers);\n@@ -2030,1 +2421,1 @@\n-  ShenandoahUpdateHeapRefsTask(ShenandoahRegionIterator* regions) :\n+  explicit ShenandoahUpdateHeapRefsTask(ShenandoahRegionIterator* regions) :\n@@ -2040,1 +2431,1 @@\n-      do_work<ShenandoahConcUpdateRefsClosure>();\n+      do_work<ShenandoahConcUpdateRefsClosure>(worker_id);\n@@ -2043,1 +2434,1 @@\n-      do_work<ShenandoahSTWUpdateRefsClosure>();\n+      do_work<ShenandoahSTWUpdateRefsClosure>(worker_id);\n@@ -2049,1 +2440,13 @@\n-  void do_work() {\n+  void do_work(uint worker_id) {\n+    if (CONCURRENT && (worker_id == 0)) {\n+      \/\/ We ask the first worker to replenish the Mutator free set by moving regions previously reserved to hold the\n+      \/\/ results of evacuation.  These reserves are no longer necessary because evacuation has completed.\n+      size_t cset_regions = _heap->collection_set()->count();\n+\n+      \/\/ Now that evacuation is done, we can reassign any regions that had been reserved to hold the results of evacuation\n+      \/\/ to the mutator free set.  At the end of GC, we will have cset_regions newly evacuated fully empty regions from\n+      \/\/ which we will be able to replenish the Collector free set and the OldCollector free set in preparation for the\n+      \/\/ next GC cycle.\n+      _heap->free_set()->move_regions_from_collector_to_mutator(cset_regions);\n+    }\n+    \/\/ If !CONCURRENT, there's no value in expanding Mutator free set\n@@ -2052,1 +2455,0 @@\n-    ShenandoahMarkingContext* const ctx = _heap->complete_marking_context();\n@@ -2058,3 +2460,3 @@\n-      }\n-      if (ShenandoahPacing) {\n-        _heap->pacer()->report_updaterefs(pointer_delta(update_watermark, r->bottom()));\n+        if (ShenandoahPacing) {\n+          _heap->pacer()->report_update_refs(pointer_delta(update_watermark, r->bottom()));\n+        }\n@@ -2082,30 +2484,0 @@\n-\n-class ShenandoahFinalUpdateRefsUpdateRegionStateClosure : public ShenandoahHeapRegionClosure {\n-private:\n-  ShenandoahHeapLock* const _lock;\n-\n-public:\n-  ShenandoahFinalUpdateRefsUpdateRegionStateClosure() : _lock(ShenandoahHeap::heap()->lock()) {}\n-\n-  void heap_region_do(ShenandoahHeapRegion* r) {\n-    \/\/ Drop unnecessary \"pinned\" state from regions that does not have CP marks\n-    \/\/ anymore, as this would allow trashing them.\n-\n-    if (r->is_active()) {\n-      if (r->is_pinned()) {\n-        if (r->pin_count() == 0) {\n-          ShenandoahHeapLocker locker(_lock);\n-          r->make_unpinned();\n-        }\n-      } else {\n-        if (r->pin_count() > 0) {\n-          ShenandoahHeapLocker locker(_lock);\n-          r->make_pinned();\n-        }\n-      }\n-    }\n-  }\n-\n-  bool is_thread_safe() { return true; }\n-};\n-\n@@ -2120,2 +2492,2 @@\n-    ShenandoahFinalUpdateRefsUpdateRegionStateClosure cl;\n-    parallel_heap_region_iterate(&cl);\n+\n+    final_update_refs_update_region_states();\n@@ -2134,0 +2506,5 @@\n+void ShenandoahHeap::final_update_refs_update_region_states() {\n+  ShenandoahSynchronizePinnedRegionStates cl;\n+  parallel_heap_region_iterate(&cl);\n+}\n+\n@@ -2135,6 +2512,42 @@\n-  {\n-    ShenandoahGCPhase phase(concurrent ?\n-                            ShenandoahPhaseTimings::final_update_refs_rebuild_freeset :\n-                            ShenandoahPhaseTimings::degen_gc_final_update_refs_rebuild_freeset);\n-    ShenandoahHeapLocker locker(lock());\n-    _free_set->rebuild();\n+  ShenandoahGCPhase phase(concurrent ?\n+                          ShenandoahPhaseTimings::final_update_refs_rebuild_freeset :\n+                          ShenandoahPhaseTimings::degen_gc_final_update_refs_rebuild_freeset);\n+  ShenandoahHeapLocker locker(lock());\n+  size_t young_cset_regions, old_cset_regions;\n+  size_t first_old_region, last_old_region, old_region_count;\n+  _free_set->prepare_to_rebuild(young_cset_regions, old_cset_regions, first_old_region, last_old_region, old_region_count);\n+  \/\/ If there are no old regions, first_old_region will be greater than last_old_region\n+  assert((first_old_region > last_old_region) ||\n+         ((last_old_region + 1 - first_old_region >= old_region_count) &&\n+          get_region(first_old_region)->is_old() && get_region(last_old_region)->is_old()),\n+         \"sanity: old_region_count: \" SIZE_FORMAT \", first_old_region: \" SIZE_FORMAT \", last_old_region: \" SIZE_FORMAT,\n+         old_region_count, first_old_region, last_old_region);\n+\n+  if (mode()->is_generational()) {\n+#ifdef ASSERT\n+    if (ShenandoahVerify) {\n+      verifier()->verify_before_rebuilding_free_set();\n+    }\n+#endif\n+\n+    \/\/ The computation of bytes_of_allocation_runway_before_gc_trigger is quite conservative so consider all of this\n+    \/\/ available for transfer to old. Note that transfer of humongous regions does not impact available.\n+    ShenandoahGenerationalHeap* gen_heap = ShenandoahGenerationalHeap::heap();\n+    size_t allocation_runway = gen_heap->young_generation()->heuristics()->bytes_of_allocation_runway_before_gc_trigger(young_cset_regions);\n+    gen_heap->compute_old_generation_balance(allocation_runway, old_cset_regions);\n+\n+    \/\/ Total old_available may have been expanded to hold anticipated promotions.  We trigger if the fragmented available\n+    \/\/ memory represents more than 16 regions worth of data.  Note that fragmentation may increase when we promote regular\n+    \/\/ regions in place when many of these regular regions have an abundant amount of available memory within them.  Fragmentation\n+    \/\/ will decrease as promote-by-copy consumes the available memory within these partially consumed regions.\n+    \/\/\n+    \/\/ We consider old-gen to have excessive fragmentation if more than 12.5% of old-gen is free memory that resides\n+    \/\/ within partially consumed regions of memory.\n+  }\n+  \/\/ Rebuild free set based on adjusted generation sizes.\n+  _free_set->finish_rebuild(young_cset_regions, old_cset_regions, old_region_count);\n+\n+  if (mode()->is_generational()) {\n+    ShenandoahGenerationalHeap* gen_heap = ShenandoahGenerationalHeap::heap();\n+    ShenandoahOldGeneration* old_gen = gen_heap->old_generation();\n+    old_gen->heuristics()->evaluate_triggers(first_old_region, last_old_region, old_region_count, num_regions());\n@@ -2206,1 +2619,1 @@\n-    \/\/ slice is should stay committed, exit right away.\n+    \/\/ slice should stay committed, exit right away.\n@@ -2220,0 +2633,21 @@\n+void ShenandoahHeap::forbid_uncommit() {\n+  if (_uncommit_thread != nullptr) {\n+    _uncommit_thread->forbid_uncommit();\n+  }\n+}\n+\n+void ShenandoahHeap::allow_uncommit() {\n+  if (_uncommit_thread != nullptr) {\n+    _uncommit_thread->allow_uncommit();\n+  }\n+}\n+\n+#ifdef ASSERT\n+bool ShenandoahHeap::is_uncommit_in_progress() {\n+  if (_uncommit_thread != nullptr) {\n+    return _uncommit_thread->is_uncommit_in_progress();\n+  }\n+  return false;\n+}\n+#endif\n+\n@@ -2228,8 +2662,0 @@\n-void ShenandoahHeap::entry_uncommit(double shrink_before, size_t shrink_until) {\n-  static const char *msg = \"Concurrent uncommit\";\n-  ShenandoahConcurrentPhase gc_phase(msg, ShenandoahPhaseTimings::conc_uncommit, true \/* log_heap_usage *\/);\n-  EventMark em(\"%s\", msg);\n-\n-  op_uncommit(shrink_before, shrink_until);\n-}\n-\n@@ -2270,1 +2696,1 @@\n-  return _memory_pool->get_memory_usage();\n+  return MemoryUsage(_initial_size, used(), committed(), max_capacity());\n@@ -2293,0 +2719,8 @@\n+bool ShenandoahHeap::is_gc_state(GCState state) const {\n+  \/\/ If the global gc state has been changed, but hasn't yet been propagated to all threads, then\n+  \/\/ the global gc state is the correct value. Once the gc state has been synchronized with all threads,\n+  \/\/ _gc_state_changed will be toggled to false and we need to use the thread local state.\n+  return _gc_state_changed ? _gc_state.is_set(state) : ShenandoahThreadLocalData::is_gc_state(state);\n+}\n+\n+\n@@ -2335,0 +2769,22 @@\n+\n+ShenandoahGeneration* ShenandoahHeap::generation_for(ShenandoahAffiliation affiliation) const {\n+  if (!mode()->is_generational()) {\n+    return global_generation();\n+  } else if (affiliation == YOUNG_GENERATION) {\n+    return young_generation();\n+  } else if (affiliation == OLD_GENERATION) {\n+    return old_generation();\n+  }\n+\n+  ShouldNotReachHere();\n+  return nullptr;\n+}\n+\n+void ShenandoahHeap::log_heap_status(const char* msg) const {\n+  if (mode()->is_generational()) {\n+    young_generation()->log_status(msg);\n+    old_generation()->log_status(msg);\n+  } else {\n+    global_generation()->log_status(msg);\n+  }\n+}\n","filename":"src\/hotspot\/share\/gc\/shenandoah\/shenandoahHeap.cpp","additions":876,"deletions":420,"binary":false,"changes":1296,"status":"modified"},{"patch":"@@ -4,0 +4,1 @@\n+ * Copyright Amazon.com Inc. or its affiliates. All Rights Reserved.\n@@ -32,1 +33,0 @@\n-#include \"gc\/shenandoah\/shenandoahAsserts.hpp\"\n@@ -34,0 +34,2 @@\n+#include \"gc\/shenandoah\/shenandoahAsserts.hpp\"\n+#include \"gc\/shenandoah\/shenandoahController.hpp\"\n@@ -36,0 +38,5 @@\n+#include \"gc\/shenandoah\/shenandoahEvacTracker.hpp\"\n+#include \"gc\/shenandoah\/shenandoahGenerationType.hpp\"\n+#include \"gc\/shenandoah\/shenandoahGenerationSizer.hpp\"\n+#include \"gc\/shenandoah\/shenandoahMmuTracker.hpp\"\n+#include \"gc\/shenandoah\/mode\/shenandoahMode.hpp\"\n@@ -47,1 +54,0 @@\n-class ShenandoahControlThread;\n@@ -50,0 +56,3 @@\n+class ShenandoahGeneration;\n+class ShenandoahYoungGeneration;\n+class ShenandoahOldGeneration;\n@@ -64,0 +73,1 @@\n+class ShenandoahUncommitThread;\n@@ -115,3 +125,3 @@\n-\/\/ Shenandoah GC is low-pause concurrent GC that uses Brooks forwarding pointers\n-\/\/ to encode forwarding data. See BrooksPointer for details on forwarding data encoding.\n-\/\/ See ShenandoahControlThread for GC cycle structure.\n+\/\/ Shenandoah GC is low-pause concurrent GC that uses a load reference barrier\n+\/\/ for concurent evacuation and a snapshot-at-the-beginning write barrier for\n+\/\/ concurrent marking. See ShenandoahControlThread for GC cycle structure.\n@@ -126,0 +136,1 @@\n+\n@@ -128,0 +139,1 @@\n+  friend class ShenandoahOldGC;\n@@ -137,0 +149,13 @@\n+  \/\/ Indicates the generation whose collection is in\n+  \/\/ progress. Mutator threads aren't allowed to read\n+  \/\/ this field.\n+  ShenandoahGeneration* _gc_generation;\n+\n+  \/\/ This is set and cleared by only the VMThread\n+  \/\/ at each STW pause (safepoint) to the value seen in\n+  \/\/ _gc_generation. This allows the value to be always consistently\n+  \/\/ seen by all mutators as well as all GC worker threads.\n+  \/\/ In that sense, it's a stable snapshot of _gc_generation that is\n+  \/\/ updated at each STW pause associated with a ShenandoahVMOp.\n+  ShenandoahGeneration* _active_generation;\n+\n@@ -142,0 +167,22 @@\n+  ShenandoahGeneration* gc_generation() const {\n+    \/\/ We don't want this field read by a mutator thread\n+    assert(!Thread::current()->is_Java_thread(), \"Not allowed\");\n+    \/\/ value of _gc_generation field, see above\n+    return _gc_generation;\n+  }\n+\n+  ShenandoahGeneration* active_generation() const {\n+    \/\/ value of _active_generation field, see above\n+    return _active_generation;\n+  }\n+\n+  \/\/ Set the _gc_generation field\n+  void set_gc_generation(ShenandoahGeneration* generation);\n+\n+  \/\/ Copy the value in the _gc_generation field into\n+  \/\/ the _active_generation field: can only be called at\n+  \/\/ a safepoint by the VMThread.\n+  void set_active_generation();\n+\n+  ShenandoahHeuristics* heuristics();\n+\n@@ -154,2 +201,2 @@\n-  void initialize_heuristics();\n-\n+  virtual void initialize_heuristics();\n+  virtual void print_init_logger() const;\n@@ -176,2 +223,3 @@\n-           size_t _initial_size;\n-           size_t _minimum_size;\n+  size_t _initial_size;\n+  size_t _minimum_size;\n+\n@@ -180,1 +228,0 @@\n-  volatile size_t _used;\n@@ -182,1 +229,0 @@\n-  volatile size_t _bytes_allocated_since_gc_start;\n@@ -185,0 +231,2 @@\n+  void increase_used(const ShenandoahAllocRequest& req);\n+\n@@ -186,3 +234,4 @@\n-  void increase_used(size_t bytes);\n-  void decrease_used(size_t bytes);\n-  void set_used(size_t bytes);\n+  void increase_used(ShenandoahGeneration* generation, size_t bytes);\n+  void decrease_used(ShenandoahGeneration* generation, size_t bytes);\n+  void increase_humongous_waste(ShenandoahGeneration* generation, size_t bytes);\n+  void decrease_humongous_waste(ShenandoahGeneration* generation, size_t bytes);\n@@ -192,1 +241,0 @@\n-  void increase_allocated(size_t bytes);\n@@ -194,1 +242,0 @@\n-  size_t bytes_allocated_since_gc_start();\n@@ -207,0 +254,12 @@\n+\/\/ ---------- Periodic Tasks\n+\/\/\n+public:\n+  \/\/ Notify heuristics and region state change logger that the state of the heap has changed\n+  void notify_heap_changed();\n+\n+  \/\/ Force counters to update\n+  void set_forced_counters_update(bool value);\n+\n+  \/\/ Update counters if forced flag is set\n+  void handle_force_counters_update();\n+\n@@ -214,0 +273,2 @@\n+  virtual void initialize_controller();\n+\n@@ -230,1 +291,1 @@\n-  ShenandoahRegionIterator _update_refs_iterator;\n+  uint8_t* _affiliations;       \/\/ Holds array of enum ShenandoahAffiliation, including FREE status in non-generational mode\n@@ -239,1 +300,1 @@\n-  inline ShenandoahHeapRegion* const heap_region_containing(const void* addr) const;\n+  inline ShenandoahHeapRegion* heap_region_containing(const void* addr) const;\n@@ -242,1 +303,1 @@\n-  inline ShenandoahHeapRegion* const get_region(size_t region_idx) const;\n+  inline ShenandoahHeapRegion* get_region(size_t region_idx) const;\n@@ -247,0 +308,2 @@\n+  inline ShenandoahMmuTracker* mmu_tracker() { return &_mmu_tracker; };\n+\n@@ -262,0 +325,1 @@\n+    \/\/ For generational mode, it means either young or old marking, or both.\n@@ -268,1 +332,1 @@\n-    UPDATEREFS_BITPOS = 3,\n+    UPDATE_REFS_BITPOS = 3,\n@@ -272,0 +336,6 @@\n+\n+    \/\/ Young regions are under marking, need SATB barriers.\n+    YOUNG_MARKING_BITPOS = 5,\n+\n+    \/\/ Old regions are under marking, need SATB barriers.\n+    OLD_MARKING_BITPOS = 6\n@@ -279,1 +349,1 @@\n-    UPDATEREFS    = 1 << UPDATEREFS_BITPOS,\n+    UPDATE_REFS   = 1 << UPDATE_REFS_BITPOS,\n@@ -281,0 +351,2 @@\n+    YOUNG_MARKING = 1 << YOUNG_MARKING_BITPOS,\n+    OLD_MARKING   = 1 << OLD_MARKING_BITPOS\n@@ -286,0 +358,1 @@\n+  ShenandoahSharedFlag   _heap_changed;\n@@ -289,1 +362,0 @@\n-  ShenandoahSharedFlag   _progress_last_gc;\n@@ -292,2 +364,10 @@\n-  \/\/ This updates the singlular, global gc state. This must happen on a safepoint.\n-  void set_gc_state(uint mask, bool value);\n+  size_t _gc_no_progress_count;\n+\n+  \/\/ This updates the singular, global gc state. This call must happen on a safepoint.\n+  void set_gc_state_at_safepoint(uint mask, bool value);\n+\n+  \/\/ This also updates the global gc state, but does not need to be called on a safepoint.\n+  \/\/ Critically, this method will _not_ flag that the global gc state has changed and threads\n+  \/\/ will continue to use their thread local copy. This is expected to be used in conjunction\n+  \/\/ with a handshake operation to propagate the new gc state.\n+  void set_gc_state_concurrent(uint mask, bool value);\n@@ -296,0 +376,1 @@\n+  \/\/ This returns the raw value of the singular, global gc state.\n@@ -298,3 +379,13 @@\n-  \/\/ This copies the global gc state into a thread local variable for java threads.\n-  \/\/ It is primarily intended to support quick access at barriers.\n-  void propagate_gc_state_to_java_threads();\n+  \/\/ Compares the given state against either the global gc state, or the thread local state.\n+  \/\/ The global gc state may change on a safepoint and is the correct value to use until\n+  \/\/ the global gc state has been propagated to all threads (after which, this method will\n+  \/\/ compare against the thread local state). The thread local gc state may also be changed\n+  \/\/ by a handshake operation, in which case, this function continues using the updated thread\n+  \/\/ local value.\n+  bool is_gc_state(GCState state) const;\n+\n+  \/\/ This copies the global gc state into a thread local variable for all threads.\n+  \/\/ The thread local gc state is primarily intended to support quick access at barriers.\n+  \/\/ All threads are updated because in some cases the control thread or the vm thread may\n+  \/\/ need to execute the load reference barrier.\n+  void propagate_gc_state_to_all_threads();\n@@ -303,1 +394,1 @@\n-  \/\/ a safepoint and that any changes were propagated to java threads after the safepoint.\n+  \/\/ a safepoint and that any changes were propagated to threads after the safepoint.\n@@ -306,1 +397,8 @@\n-  void set_concurrent_mark_in_progress(bool in_progress);\n+  \/\/ Returns true if allocations have occurred in new regions or if regions have been\n+  \/\/ uncommitted since the previous calls. This call will reset the flag to false.\n+  bool has_changed() {\n+    return _heap_changed.try_unset();\n+  }\n+\n+  void set_concurrent_young_mark_in_progress(bool in_progress);\n+  void set_concurrent_old_mark_in_progress(bool in_progress);\n@@ -316,1 +414,0 @@\n-  inline bool is_stable() const;\n@@ -319,0 +416,2 @@\n+  inline bool is_concurrent_young_mark_in_progress() const;\n+  inline bool is_concurrent_old_mark_in_progress() const;\n@@ -329,0 +428,1 @@\n+  bool is_prepare_for_old_mark_in_progress() const;\n@@ -331,9 +431,1 @@\n-  enum CancelState {\n-    \/\/ Normal state. GC has not been cancelled and is open for cancellation.\n-    \/\/ Worker threads can suspend for safepoint.\n-    CANCELLABLE,\n-\n-    \/\/ GC has been cancelled. Worker threads can not suspend for\n-    \/\/ safepoint but must finish their work as soon as possible.\n-    CANCELLED\n-  };\n+  void manage_satb_barrier(bool active);\n@@ -341,2 +433,3 @@\n-  ShenandoahSharedEnumFlag<CancelState> _cancelled_gc;\n-  bool try_cancel_gc();\n+  \/\/ Records the time of the first successful cancellation request. This is used to measure\n+  \/\/ the responsiveness of the heuristic when starting a cycle.\n+  double _cancel_requested_time;\n@@ -344,1 +437,2 @@\n-public:\n+  \/\/ Indicates the reason the current GC has been cancelled (GCCause::_no_gc means the gc is not cancelled).\n+  ShenandoahSharedEnumFlag<GCCause::Cause> _cancelled_gc;\n@@ -346,0 +440,8 @@\n+  \/\/ Returns true if cancel request was successfully communicated.\n+  \/\/ Returns false if some other thread already communicated cancel\n+  \/\/ request.  A true return value does not mean GC has been\n+  \/\/ cancelled, only that the process of cancelling GC has begun.\n+  bool try_cancel_gc(GCCause::Cause cause);\n+\n+public:\n+  \/\/ True if gc has been cancelled\n@@ -347,0 +449,2 @@\n+\n+  \/\/ Used by workers in the GC cycle to detect cancellation and honor STS requirements\n@@ -349,1 +453,2 @@\n-  inline void clear_cancelled_gc();\n+  \/\/ This indicates the reason the last GC cycle was cancelled.\n+  inline GCCause::Cause cancelled_cause() const;\n@@ -351,1 +456,3 @@\n-  void cancel_gc(GCCause::Cause cause);\n+  \/\/ Clears the cancellation cause and optionally resets the oom handler (cancelling an\n+  \/\/ old mark does _not_ touch the oom handler).\n+  inline void clear_cancelled_gc(bool clear_oom_handler = true);\n@@ -353,4 +460,13 @@\n-public:\n-  \/\/ Elastic heap support\n-  void entry_uncommit(double shrink_before, size_t shrink_until);\n-  void op_uncommit(double shrink_before, size_t shrink_until);\n+  void cancel_concurrent_mark();\n+\n+  \/\/ Returns true if and only if this call caused a gc to be cancelled.\n+  bool cancel_gc(GCCause::Cause cause);\n+\n+  \/\/ Returns true if the soft maximum heap has been changed using management APIs.\n+  bool check_soft_max_changed();\n+\n+protected:\n+  \/\/ This is shared between shConcurrentGC and shDegenerateGC so that degenerated\n+  \/\/ GC can resume update refs from where the concurrent GC was cancelled. It is\n+  \/\/ also used in shGenerationalHeap, which uses a different closure for update refs.\n+  ShenandoahRegionIterator _update_refs_iterator;\n@@ -360,3 +476,0 @@\n-  \/\/ Reset bitmap, prepare regions for new GC cycle\n-  void prepare_gc();\n-  void prepare_regions_and_collection_set(bool concurrent);\n@@ -364,1 +477,1 @@\n-  void evacuate_collection_set(bool concurrent);\n+  virtual void evacuate_collection_set(bool concurrent);\n@@ -371,2 +484,9 @@\n-  void prepare_update_heap_references(bool concurrent);\n-  void update_heap_references(bool concurrent);\n+  void prepare_update_heap_references();\n+\n+  \/\/ Retires LABs used for evacuation\n+  void concurrent_prepare_for_update_refs();\n+\n+  \/\/ Turn off weak roots flag, purge old satb buffers in generational mode\n+  void concurrent_final_roots(HandshakeClosure* handshake_closure = nullptr);\n+\n+  virtual void update_heap_references(bool concurrent);\n@@ -375,1 +495,1 @@\n-  void rebuild_free_set(bool concurrent);\n+  virtual void final_update_refs_update_region_states();\n@@ -380,2 +500,23 @@\n-  void notify_gc_progress()    { _progress_last_gc.set();   }\n-  void notify_gc_no_progress() { _progress_last_gc.unset(); }\n+  void rebuild_free_set(bool concurrent);\n+  void notify_gc_progress();\n+  void notify_gc_no_progress();\n+  size_t get_gc_no_progress_count() const;\n+\n+  \/\/ The uncommit thread targets soft max heap, notify this thread when that value has changed.\n+  void notify_soft_max_changed();\n+\n+  \/\/ An explicit GC request may have freed regions, notify the uncommit thread.\n+  void notify_explicit_gc_requested();\n+\n+private:\n+  ShenandoahGeneration*  _global_generation;\n+\n+protected:\n+  \/\/ The control thread presides over concurrent collection cycles\n+  ShenandoahController*  _control_thread;\n+\n+  \/\/ The uncommit thread periodically attempts to uncommit regions that have been empty for longer than ShenandoahUncommitDelay\n+  ShenandoahUncommitThread*  _uncommit_thread;\n+\n+  ShenandoahYoungGeneration* _young_generation;\n+  ShenandoahOldGeneration*   _old_generation;\n@@ -383,2 +524,0 @@\n-\/\/\n-\/\/ Mark support\n@@ -386,1 +525,0 @@\n-  ShenandoahControlThread*   _control_thread;\n@@ -389,1 +527,0 @@\n-  ShenandoahHeuristics*      _heuristics;\n@@ -394,3 +531,2 @@\n-  ShenandoahPhaseTimings*    _phase_timings;\n-\n-  ShenandoahControlThread*   control_thread()          { return _control_thread;    }\n+  ShenandoahPhaseTimings*       _phase_timings;\n+  ShenandoahMmuTracker          _mmu_tracker;\n@@ -399,0 +535,15 @@\n+  ShenandoahController*   control_thread() const { return _control_thread; }\n+\n+  ShenandoahGeneration*      global_generation() const { return _global_generation; }\n+  ShenandoahYoungGeneration* young_generation()  const {\n+    assert(mode()->is_generational(), \"Young generation requires generational mode\");\n+    return _young_generation;\n+  }\n+\n+  ShenandoahOldGeneration*   old_generation()    const {\n+    assert(mode()->is_generational(), \"Old generation requires generational mode\");\n+    return _old_generation;\n+  }\n+\n+  ShenandoahGeneration*      generation_for(ShenandoahAffiliation affiliation) const;\n+\n@@ -401,1 +552,0 @@\n-  ShenandoahHeuristics*      heuristics()        const { return _heuristics;        }\n@@ -407,0 +557,5 @@\n+  ShenandoahEvacOOMHandler*  oom_evac_handler()        { return &_oom_evac_handler; }\n+\n+  void on_cycle_start(GCCause::Cause cause, ShenandoahGeneration* generation);\n+  void on_cycle_end(ShenandoahGeneration* generation);\n+\n@@ -422,1 +577,1 @@\n-  ShenandoahMonitoringSupport* monitoring_support()          { return _monitoring_support;    }\n+  ShenandoahMonitoringSupport* monitoring_support() const    { return _monitoring_support;    }\n@@ -433,8 +588,0 @@\n-\/\/ ---------- Reference processing\n-\/\/\n-private:\n-  ShenandoahReferenceProcessor* const _ref_processor;\n-\n-public:\n-  ShenandoahReferenceProcessor* ref_processor() { return _ref_processor; }\n-\n@@ -459,0 +606,3 @@\n+  inline void assert_lock_for_affiliation(ShenandoahAffiliation orig_affiliation,\n+                                          ShenandoahAffiliation new_affiliation);\n+\n@@ -476,0 +626,15 @@\n+  \/\/ Returns true if the given oop belongs to a generation that is actively being collected.\n+  inline bool is_in_active_generation(oop obj) const;\n+  inline bool is_in_young(const void* p) const;\n+  inline bool is_in_old(const void* p) const;\n+\n+  \/\/ Returns true iff the young generation is being collected and the given pointer\n+  \/\/ is in the old generation. This is used to prevent the young collection from treating\n+  \/\/ such an object as unreachable.\n+  inline bool is_in_old_during_young_collection(oop obj) const;\n+\n+  inline ShenandoahAffiliation region_affiliation(const ShenandoahHeapRegion* r) const;\n+  inline void set_affiliation(ShenandoahHeapRegion* r, ShenandoahAffiliation new_affiliation);\n+\n+  inline ShenandoahAffiliation region_affiliation(size_t index) const;\n+\n@@ -528,0 +693,3 @@\n+protected:\n+  inline HeapWord* allocate_from_gclab(Thread* thread, size_t size);\n+\n@@ -530,1 +698,0 @@\n-  inline HeapWord* allocate_from_gclab(Thread* thread, size_t size);\n@@ -534,0 +701,3 @@\n+  \/\/ We want to retry an unsuccessful attempt at allocation until at least a full gc.\n+  bool should_retry_allocation(size_t original_full_gc_count) const;\n+\n@@ -541,1 +711,1 @@\n-  void notify_mutator_alloc_words(size_t words, bool waste);\n+  void notify_mutator_alloc_words(size_t words, size_t waste);\n@@ -577,1 +747,1 @@\n-  inline ShenandoahMarkingContext* complete_marking_context() const;\n+  \/\/ Return the marking context regardless of the completeness status.\n@@ -579,2 +749,0 @@\n-  inline void mark_complete_marking_context();\n-  inline void mark_incomplete_marking_context();\n@@ -591,2 +759,0 @@\n-  void reset_mark_bitmap();\n-\n@@ -601,0 +767,14 @@\n+  \/\/ During concurrent reset, the control thread will zero out the mark bitmaps for committed regions.\n+  \/\/ This cannot happen when the uncommit thread is simultaneously trying to uncommit regions and their bitmaps.\n+  \/\/ To prevent these threads from working at the same time, we provide these methods for the control thread to\n+  \/\/ prevent the uncommit thread from working while a collection cycle is in progress.\n+\n+  \/\/ Forbid uncommits (will stop and wait if regions are being uncommitted)\n+  void forbid_uncommit();\n+\n+  \/\/ Allow the uncommit thread to process regions\n+  void allow_uncommit();\n+#ifdef ASSERT\n+  bool is_uncommit_in_progress();\n+#endif\n+\n@@ -613,0 +793,2 @@\n+  oop try_evacuate_object(oop src, Thread* thread, ShenandoahHeapRegion* from_region, ShenandoahAffiliation target_gen);\n+\n@@ -624,1 +806,1 @@\n-  \/\/ Evacuates object src. Returns the evacuated object, either evacuated\n+  \/\/ Evacuates or promotes object src. Returns the evacuated object, either evacuated\n@@ -626,1 +808,1 @@\n-  inline oop evacuate_object(oop src, Thread* thread);\n+  virtual oop evacuate_object(oop src, Thread* thread);\n@@ -655,0 +837,9 @@\n+  static inline void increase_object_age(oop obj, uint additional_age);\n+\n+  \/\/ Return the object's age, or a sentinel value when the age can't\n+  \/\/ necessarily be determined because of concurrent locking by the\n+  \/\/ mutator\n+  static inline uint get_object_age(oop obj);\n+\n+  void log_heap_status(const char *msg) const;\n+\n","filename":"src\/hotspot\/share\/gc\/shenandoah\/shenandoahHeap.hpp","additions":274,"deletions":83,"binary":false,"changes":357,"status":"modified"},{"patch":"@@ -3,0 +3,1 @@\n+ * Copyright Amazon.com Inc. or its affiliates. All Rights Reserved.\n@@ -43,1 +44,1 @@\n-#include \"gc\/shenandoah\/shenandoahControlThread.hpp\"\n+#include \"gc\/shenandoah\/shenandoahGeneration.hpp\"\n@@ -46,0 +47,1 @@\n+#include \"gc\/shenandoah\/mode\/shenandoahMode.hpp\"\n@@ -51,0 +53,1 @@\n+#include \"runtime\/objectMonitor.inline.hpp\"\n@@ -64,4 +67,0 @@\n-inline bool ShenandoahHeap::has_forwarded_objects() const {\n-  return _gc_state.is_set(HAS_FORWARDED);\n-}\n-\n@@ -76,0 +75,12 @@\n+inline void ShenandoahHeap::notify_gc_progress() {\n+  Atomic::store(&_gc_no_progress_count, (size_t) 0);\n+\n+}\n+inline void ShenandoahHeap::notify_gc_no_progress() {\n+  Atomic::inc(&_gc_no_progress_count);\n+}\n+\n+inline size_t ShenandoahHeap::get_gc_no_progress_count() const {\n+  return Atomic::load(&_gc_no_progress_count);\n+}\n+\n@@ -83,1 +94,1 @@\n-inline ShenandoahHeapRegion* const ShenandoahHeap::heap_region_containing(const void* addr) const {\n+inline ShenandoahHeapRegion* ShenandoahHeap::heap_region_containing(const void* addr) const {\n@@ -243,1 +254,1 @@\n-  return _cancelled_gc.get() == CANCELLED;\n+  return _cancelled_gc.get() != GCCause::_no_gc;\n@@ -255,3 +266,14 @@\n-inline void ShenandoahHeap::clear_cancelled_gc() {\n-  _cancelled_gc.set(CANCELLABLE);\n-  _oom_evac_handler.clear();\n+inline GCCause::Cause ShenandoahHeap::cancelled_cause() const {\n+  return _cancelled_gc.get();\n+}\n+\n+inline void ShenandoahHeap::clear_cancelled_gc(bool clear_oom_handler) {\n+  _cancelled_gc.set(GCCause::_no_gc);\n+  if (_cancel_requested_time > 0) {\n+    log_debug(gc)(\"GC cancellation took %.3fs\", (os::elapsedTime() - _cancel_requested_time));\n+    _cancel_requested_time = 0;\n+  }\n+\n+  if (clear_oom_handler) {\n+    _oom_evac_handler.clear();\n+  }\n@@ -274,1 +296,0 @@\n-  \/\/ Otherwise...\n@@ -278,5 +299,27 @@\n-inline oop ShenandoahHeap::evacuate_object(oop p, Thread* thread) {\n-  if (ShenandoahThreadLocalData::is_oom_during_evac(Thread::current())) {\n-    \/\/ This thread went through the OOM during evac protocol and it is safe to return\n-    \/\/ the forward pointer. It must not attempt to evacuate any more.\n-    return ShenandoahBarrierSet::resolve_forwarded(p);\n+void ShenandoahHeap::increase_object_age(oop obj, uint additional_age) {\n+  \/\/ This operates on new copy of an object. This means that the object's mark-word\n+  \/\/ is thread-local and therefore safe to access. However, when the mark is\n+  \/\/ displaced (i.e. stack-locked or monitor-locked), then it must be considered\n+  \/\/ a shared memory location. It can be accessed by other threads.\n+  \/\/ In particular, a competing evacuating thread can succeed to install its copy\n+  \/\/ as the forwardee and continue to unlock the object, at which point 'our'\n+  \/\/ write to the foreign stack-location would potentially over-write random\n+  \/\/ information on that stack. Writing to a monitor is less problematic,\n+  \/\/ but still not safe: while the ObjectMonitor would not randomly disappear,\n+  \/\/ the other thread would also write to the same displaced header location,\n+  \/\/ possibly leading to increase the age twice.\n+  \/\/ For all these reasons, we take the conservative approach and not attempt\n+  \/\/ to increase the age when the header is displaced.\n+  markWord w = obj->mark();\n+  \/\/ The mark-word has been copied from the original object. It can not be\n+  \/\/ inflating, because inflation can not be interrupted by a safepoint,\n+  \/\/ and after a safepoint, a Java thread would first have to successfully\n+  \/\/ evacuate the object before it could inflate the monitor.\n+  assert(!w.is_being_inflated() || LockingMode == LM_LIGHTWEIGHT, \"must not inflate monitor before evacuation of object succeeds\");\n+  \/\/ It is possible that we have copied the object after another thread has\n+  \/\/ already successfully completed evacuation. While harmless (we would never\n+  \/\/ publish our copy), don't even attempt to modify the age when that\n+  \/\/ happens.\n+  if (!w.has_displaced_mark_helper() && !w.is_marked()) {\n+    w = w.set_age(MIN2(markWord::max_age, w.age() + additional_age));\n+    obj->set_mark(w);\n@@ -284,0 +327,1 @@\n+}\n@@ -285,1 +329,15 @@\n-  assert(ShenandoahThreadLocalData::is_evac_allowed(thread), \"must be enclosed in oom-evac scope\");\n+\/\/ Return the object's age, or a sentinel value when the age can't\n+\/\/ necessarily be determined because of concurrent locking by the\n+\/\/ mutator\n+uint ShenandoahHeap::get_object_age(oop obj) {\n+  markWord w = obj->mark();\n+  assert(!w.is_marked(), \"must not be forwarded\");\n+  if (w.has_monitor()) {\n+    w = w.monitor()->header();\n+  } else if (w.is_being_inflated() || w.has_displaced_mark_helper()) {\n+    \/\/ Informs caller that we aren't able to determine the age\n+    return markWord::max_age + 1; \/\/ sentinel\n+  }\n+  assert(w.age() <= markWord::max_age, \"Impossible!\");\n+  return w.age();\n+}\n@@ -287,1 +345,6 @@\n-  size_t size = p->size();\n+inline bool ShenandoahHeap::is_in_active_generation(oop obj) const {\n+  if (!mode()->is_generational()) {\n+    \/\/ everything is the same single generation\n+    assert(is_in(obj), \"Otherwise shouldn't return true below\");\n+    return true;\n+  }\n@@ -289,1 +352,1 @@\n-  assert(!heap_region_containing(p)->is_humongous(), \"never evacuate humongous objects\");\n+  ShenandoahGeneration* const gen = active_generation();\n@@ -291,2 +354,5 @@\n-  bool alloc_from_gclab = true;\n-  HeapWord* copy = nullptr;\n+  if (gen == nullptr) {\n+    \/\/ no collection is happening: only expect this to be called\n+    \/\/ when concurrent processing is active, but that could change\n+    return false;\n+  }\n@@ -294,15 +360,22 @@\n-#ifdef ASSERT\n-  if (ShenandoahOOMDuringEvacALot &&\n-      (os::random() & 1) == 0) { \/\/ Simulate OOM every ~2nd slow-path call\n-        copy = nullptr;\n-  } else {\n-#endif\n-    if (UseTLAB) {\n-      copy = allocate_from_gclab(thread, size);\n-    }\n-    if (copy == nullptr) {\n-      ShenandoahAllocRequest req = ShenandoahAllocRequest::for_shared_gc(size);\n-      copy = allocate_memory(req);\n-      alloc_from_gclab = false;\n-    }\n-#ifdef ASSERT\n+  assert(is_in(obj), \"only check if is in active generation for objects (\" PTR_FORMAT \") in heap\", p2i(obj));\n+  assert(gen->is_old() || gen->is_young() || gen->is_global(),\n+         \"Active generation must be old, young, or global\");\n+\n+  size_t index = heap_region_containing(obj)->index();\n+\n+  \/\/ No flickering!\n+  assert(gen == active_generation(), \"Race?\");\n+\n+  switch (region_affiliation(index)) {\n+  case ShenandoahAffiliation::FREE:\n+    \/\/ Free regions are in old, young, and global collections\n+    return true;\n+  case ShenandoahAffiliation::YOUNG_GENERATION:\n+    \/\/ Young regions are in young and global collections, not in old collections\n+    return !gen->is_old();\n+  case ShenandoahAffiliation::OLD_GENERATION:\n+    \/\/ Old regions are in old and global collections, not in young collections\n+    return !gen->is_young();\n+  default:\n+    assert(false, \"Bad affiliation (%d) for region \" SIZE_FORMAT, region_affiliation(index), index);\n+    return false;\n@@ -310,1 +383,1 @@\n-#endif\n+}\n@@ -312,2 +385,3 @@\n-  if (copy == nullptr) {\n-    control_thread()->handle_alloc_failure_evac(size);\n+inline bool ShenandoahHeap::is_in_young(const void* p) const {\n+  return is_in(p) && (_affiliations[heap_region_index_containing(p)] == ShenandoahAffiliation::YOUNG_GENERATION);\n+}\n@@ -315,1 +389,3 @@\n-    _oom_evac_handler.handle_out_of_memory_during_evacuation();\n+inline bool ShenandoahHeap::is_in_old(const void* p) const {\n+  return is_in(p) && (_affiliations[heap_region_index_containing(p)] == ShenandoahAffiliation::OLD_GENERATION);\n+}\n@@ -317,2 +393,3 @@\n-    return ShenandoahBarrierSet::resolve_forwarded(p);\n-  }\n+inline bool ShenandoahHeap::is_in_old_during_young_collection(oop obj) const {\n+  return active_generation()->is_young() && is_in_old(obj);\n+}\n@@ -320,32 +397,22 @@\n-  \/\/ Copy the object:\n-  Copy::aligned_disjoint_words(cast_from_oop<HeapWord*>(p), copy, size);\n-\n-  \/\/ Try to install the new forwarding pointer.\n-  oop copy_val = cast_to_oop(copy);\n-  ContinuationGCSupport::relativize_stack_chunk(copy_val);\n-\n-  oop result = ShenandoahForwarding::try_update_forwardee(p, copy_val);\n-  if (result == copy_val) {\n-    \/\/ Successfully evacuated. Our copy is now the public one!\n-    shenandoah_assert_correct(nullptr, copy_val);\n-    return copy_val;\n-  }  else {\n-    \/\/ Failed to evacuate. We need to deal with the object that is left behind. Since this\n-    \/\/ new allocation is certainly after TAMS, it will be considered live in the next cycle.\n-    \/\/ But if it happens to contain references to evacuated regions, those references would\n-    \/\/ not get updated for this stale copy during this cycle, and we will crash while scanning\n-    \/\/ it the next cycle.\n-    \/\/\n-    \/\/ For GCLAB allocations, it is enough to rollback the allocation ptr. Either the next\n-    \/\/ object will overwrite this stale copy, or the filler object on LAB retirement will\n-    \/\/ do this. For non-GCLAB allocations, we have no way to retract the allocation, and\n-    \/\/ have to explicitly overwrite the copy with the filler object. With that overwrite,\n-    \/\/ we have to keep the fwdptr initialized and pointing to our (stale) copy.\n-    if (alloc_from_gclab) {\n-      ShenandoahThreadLocalData::gclab(thread)->undo_allocation(copy, size);\n-    } else {\n-      fill_with_object(copy, size);\n-      shenandoah_assert_correct(nullptr, copy_val);\n-    }\n-    shenandoah_assert_correct(nullptr, result);\n-    return result;\n+inline ShenandoahAffiliation ShenandoahHeap::region_affiliation(const ShenandoahHeapRegion *r) const {\n+  return region_affiliation(r->index());\n+}\n+\n+inline void ShenandoahHeap::assert_lock_for_affiliation(ShenandoahAffiliation orig_affiliation,\n+                                                        ShenandoahAffiliation new_affiliation) {\n+  \/\/ A lock is required when changing from FREE to NON-FREE.  Though it may be possible to elide the lock when\n+  \/\/ transitioning from in-use to FREE, the current implementation uses a lock for this transition.  A lock is\n+  \/\/ not required to change from YOUNG to OLD (i.e. when promoting humongous region).\n+  \/\/\n+  \/\/         new_affiliation is:     FREE   YOUNG   OLD\n+  \/\/  orig_affiliation is:  FREE      X       L      L\n+  \/\/                       YOUNG      L       X\n+  \/\/                         OLD      L       X      X\n+  \/\/  X means state transition won't happen (so don't care)\n+  \/\/  L means lock should be held\n+  \/\/  Blank means no lock required because affiliation visibility will not be required until subsequent safepoint\n+  \/\/\n+  \/\/ Note: during full GC, all transitions between states are possible.  During Full GC, we should be in a safepoint.\n+\n+  if (orig_affiliation == ShenandoahAffiliation::FREE) {\n+    shenandoah_assert_heaplocked_or_safepoint();\n@@ -355,0 +422,11 @@\n+inline void ShenandoahHeap::set_affiliation(ShenandoahHeapRegion* r, ShenandoahAffiliation new_affiliation) {\n+#ifdef ASSERT\n+  assert_lock_for_affiliation(region_affiliation(r), new_affiliation);\n+#endif\n+  Atomic::store(_affiliations + r->index(), (uint8_t) new_affiliation);\n+}\n+\n+inline ShenandoahAffiliation ShenandoahHeap::region_affiliation(size_t index) const {\n+  return (ShenandoahAffiliation) Atomic::load(_affiliations + index);\n+}\n+\n@@ -370,2 +448,2 @@\n-inline bool ShenandoahHeap::is_stable() const {\n-  return _gc_state.is_clear();\n+inline bool ShenandoahHeap::is_idle() const {\n+  return _gc_state_changed ? _gc_state.is_clear() : ShenandoahThreadLocalData::gc_state(Thread::current()) == 0;\n@@ -374,2 +452,2 @@\n-inline bool ShenandoahHeap::is_idle() const {\n-  return _gc_state.is_unset(MARKING | EVACUATION | UPDATEREFS);\n+inline bool ShenandoahHeap::has_forwarded_objects() const {\n+  return is_gc_state(HAS_FORWARDED);\n@@ -379,1 +457,9 @@\n-  return _gc_state.is_set(MARKING);\n+  return is_gc_state(MARKING);\n+}\n+\n+inline bool ShenandoahHeap::is_concurrent_young_mark_in_progress() const {\n+  return is_gc_state(YOUNG_MARKING);\n+}\n+\n+inline bool ShenandoahHeap::is_concurrent_old_mark_in_progress() const {\n+  return is_gc_state(OLD_MARKING);\n@@ -383,1 +469,9 @@\n-  return _gc_state.is_set(EVACUATION);\n+  return is_gc_state(EVACUATION);\n+}\n+\n+inline bool ShenandoahHeap::is_update_refs_in_progress() const {\n+  return is_gc_state(UPDATE_REFS);\n+}\n+\n+inline bool ShenandoahHeap::is_concurrent_weak_root_in_progress() const {\n+  return is_gc_state(WEAK_ROOTS);\n@@ -398,4 +492,0 @@\n-inline bool ShenandoahHeap::is_update_refs_in_progress() const {\n-  return _gc_state.is_set(UPDATEREFS);\n-}\n-\n@@ -410,4 +500,0 @@\n-inline bool ShenandoahHeap::is_concurrent_weak_root_in_progress() const {\n-  return _gc_state.is_set(WEAK_ROOTS);\n-}\n-\n@@ -423,2 +509,1 @@\n-  ShenandoahMarkingContext* const ctx = complete_marking_context();\n-  assert(ctx->is_complete(), \"sanity\");\n+  ShenandoahMarkingContext* const ctx = marking_context();\n@@ -547,1 +632,1 @@\n-inline ShenandoahHeapRegion* const ShenandoahHeap::get_region(size_t region_idx) const {\n+inline ShenandoahHeapRegion* ShenandoahHeap::get_region(size_t region_idx) const {\n@@ -555,13 +640,0 @@\n-inline void ShenandoahHeap::mark_complete_marking_context() {\n-  _marking_context->mark_complete();\n-}\n-\n-inline void ShenandoahHeap::mark_incomplete_marking_context() {\n-  _marking_context->mark_incomplete();\n-}\n-\n-inline ShenandoahMarkingContext* ShenandoahHeap::complete_marking_context() const {\n-  assert (_marking_context->is_complete(),\" sanity\");\n-  return _marking_context;\n-}\n-\n","filename":"src\/hotspot\/share\/gc\/shenandoah\/shenandoahHeap.inline.hpp","additions":176,"deletions":104,"binary":false,"changes":280,"status":"modified"},{"patch":"@@ -3,1 +3,2 @@\n- * Copyright (c) 2013, 2019, Red Hat, Inc. All rights reserved.\n+ * Copyright (c) 2013, 2020, Red Hat, Inc. All rights reserved.\n+ * Copyright Amazon.com Inc. or its affiliates. All Rights Reserved.\n@@ -27,0 +28,1 @@\n+#include \"gc\/shared\/cardTable.hpp\"\n@@ -29,0 +31,2 @@\n+#include \"gc\/shenandoah\/shenandoahCardTable.hpp\"\n+#include \"gc\/shenandoah\/shenandoahFreeSet.hpp\"\n@@ -33,0 +37,4 @@\n+#include \"gc\/shenandoah\/shenandoahOldGeneration.hpp\"\n+#include \"gc\/shenandoah\/shenandoahGeneration.hpp\"\n+#include \"gc\/shenandoah\/shenandoahYoungGeneration.hpp\"\n+#include \"gc\/shenandoah\/shenandoahScanRemembered.inline.hpp\"\n@@ -47,0 +55,1 @@\n+\n@@ -54,2 +63,0 @@\n-size_t ShenandoahHeapRegion::HumongousThresholdBytes = 0;\n-size_t ShenandoahHeapRegion::HumongousThresholdWords = 0;\n@@ -65,0 +72,1 @@\n+  _top_before_promoted(nullptr),\n@@ -69,0 +77,1 @@\n+  _plab_allocs(0),\n@@ -71,1 +80,7 @@\n-  _update_watermark(start) {\n+  _update_watermark(start),\n+  _age(0),\n+#ifdef SHENANDOAH_CENSUS_NOISE\n+  _youth(0),\n+#endif \/\/ SHENANDOAH_CENSUS_NOISE\n+  _needs_bitmap_reset(false)\n+  {\n@@ -78,0 +93,1 @@\n+  _recycling.unset();\n@@ -82,1 +98,1 @@\n-  ss.print(\"Illegal region state transition from \\\"%s\\\", at %s\\n  \", region_state_to_string(_state), method);\n+  ss.print(\"Illegal region state transition from \\\"%s\\\", at %s\\n  \", region_state_to_string(state()), method);\n@@ -87,1 +103,1 @@\n-void ShenandoahHeapRegion::make_regular_allocation() {\n+void ShenandoahHeapRegion::make_regular_allocation(ShenandoahAffiliation affiliation) {\n@@ -89,2 +105,2 @@\n-\n-  switch (_state) {\n+  reset_age();\n+  switch (state()) {\n@@ -94,0 +110,1 @@\n+      assert(this->affiliation() == affiliation, \"Region affiliation should already be established\");\n@@ -103,0 +120,25 @@\n+\/\/ Change affiliation to YOUNG_GENERATION if _state is not _pinned_cset, _regular, or _pinned.  This implements\n+\/\/ behavior previously performed as a side effect of make_regular_bypass().  This is used by Full GC in non-generational\n+\/\/ modes to transition regions from FREE. Note that all non-free regions in single-generational modes are young.\n+void ShenandoahHeapRegion::make_affiliated_maybe() {\n+  shenandoah_assert_heaplocked();\n+  assert(!ShenandoahHeap::heap()->mode()->is_generational(), \"Only call if non-generational\");\n+  switch (state()) {\n+   case _empty_uncommitted:\n+   case _empty_committed:\n+   case _cset:\n+   case _humongous_start:\n+   case _humongous_cont:\n+     if (affiliation() != YOUNG_GENERATION) {\n+       set_affiliation(YOUNG_GENERATION);\n+     }\n+     return;\n+   case _pinned_cset:\n+   case _regular:\n+   case _pinned:\n+     return;\n+   default:\n+     assert(false, \"Unexpected _state in make_affiliated_maybe\");\n+  }\n+}\n+\n@@ -107,2 +149,3 @@\n-\n-  switch (_state) {\n+  reset_age();\n+  auto cur_state = state();\n+  switch (cur_state) {\n@@ -115,0 +158,8 @@\n+      if (cur_state == _humongous_start || cur_state == _humongous_cont) {\n+        \/\/ CDS allocates chunks of the heap to fill with regular objects. The allocator\n+        \/\/ will dutifully track any waste in the unused portion of the last region. Once\n+        \/\/ CDS has finished initializing the objects, it will convert these regions to\n+        \/\/ regular regions. The 'waste' in the last region is no longer wasted at this point,\n+        \/\/ so we must stop treating it as such.\n+        decrement_humongous_waste();\n+      }\n@@ -130,1 +181,2 @@\n-  switch (_state) {\n+  reset_age();\n+  switch (state()) {\n@@ -141,1 +193,1 @@\n-void ShenandoahHeapRegion::make_humongous_start_bypass() {\n+void ShenandoahHeapRegion::make_humongous_start_bypass(ShenandoahAffiliation affiliation) {\n@@ -144,2 +196,4 @@\n-\n-  switch (_state) {\n+  \/\/ Don't bother to account for affiliated regions during Full GC.  We recompute totals at end.\n+  set_affiliation(affiliation);\n+  reset_age();\n+  switch (state()) {\n@@ -159,1 +213,2 @@\n-  switch (_state) {\n+  reset_age();\n+  switch (state()) {\n@@ -170,1 +225,1 @@\n-void ShenandoahHeapRegion::make_humongous_cont_bypass() {\n+void ShenandoahHeapRegion::make_humongous_cont_bypass(ShenandoahAffiliation affiliation) {\n@@ -173,2 +228,4 @@\n-\n-  switch (_state) {\n+  set_affiliation(affiliation);\n+  \/\/ Don't bother to account for affiliated regions during Full GC.  We recompute totals at end.\n+  reset_age();\n+  switch (state()) {\n@@ -190,1 +247,1 @@\n-  switch (_state) {\n+  switch (state()) {\n@@ -201,1 +258,1 @@\n-      _state = _pinned_cset;\n+      set_state(_pinned_cset);\n@@ -212,1 +269,1 @@\n-  switch (_state) {\n+  switch (state()) {\n@@ -214,0 +271,1 @@\n+      assert(is_affiliated(), \"Pinned region should be affiliated\");\n@@ -232,1 +290,2 @@\n-  switch (_state) {\n+  \/\/ Leave age untouched.  We need to consult the age when we are deciding whether to promote evacuated objects.\n+  switch (state()) {\n@@ -244,3 +303,2 @@\n-  switch (_state) {\n-    case _cset:\n-      \/\/ Reclaiming cset regions\n+  reset_age();\n+  switch (state()) {\n@@ -249,1 +307,7 @@\n-      \/\/ Reclaiming humongous regions\n+    {\n+      \/\/ Reclaiming humongous regions and reclaim humongous waste.  When this region is eventually recycled, we'll reclaim\n+      \/\/ its used memory.  At recycle time, we no longer recognize this as a humongous region.\n+      decrement_humongous_waste();\n+    }\n+    case _cset:\n+      \/\/ Reclaiming cset regions\n@@ -264,1 +328,3 @@\n-  ShenandoahHeap::heap()->complete_marking_context()->reset_top_bitmap(this);\n+  assert(ShenandoahHeap::heap()->gc_generation()->is_mark_complete(), \"Marking should be complete here.\");\n+  shenandoah_assert_generations_reconciled();\n+  ShenandoahHeap::heap()->marking_context()->reset_top_bitmap(this);\n@@ -268,2 +334,3 @@\n-  shenandoah_assert_heaplocked();\n-  switch (_state) {\n+  reset_age();\n+  CENSUS_NOISE(clear_youth();)\n+  switch (state()) {\n@@ -281,1 +348,1 @@\n-  switch (_state) {\n+  switch (state()) {\n@@ -295,1 +362,1 @@\n-  switch (_state) {\n+  switch (state()) {\n@@ -308,0 +375,1 @@\n+  _plab_allocs = 0;\n@@ -311,1 +379,1 @@\n-  return used() - (_tlab_allocs + _gclab_allocs) * HeapWordSize;\n+  return used() - (_tlab_allocs + _gclab_allocs + _plab_allocs) * HeapWordSize;\n@@ -322,0 +390,4 @@\n+size_t ShenandoahHeapRegion::get_plab_allocs() const {\n+  return _plab_allocs * HeapWordSize;\n+}\n+\n@@ -331,1 +403,1 @@\n-  switch (_state) {\n+  switch (state()) {\n@@ -366,0 +438,2 @@\n+  st->print(\"|%s\", shenandoah_affiliation_code(affiliation()));\n+\n@@ -377,0 +451,3 @@\n+  if (ShenandoahHeap::heap()->mode()->is_generational()) {\n+    st->print(\"|P \" SIZE_FORMAT_W(5) \"%1s\", byte_size_in_proper_unit(get_plab_allocs()),   proper_unit_for_byte_size(get_plab_allocs()));\n+  }\n@@ -385,6 +462,7 @@\n-void ShenandoahHeapRegion::oop_iterate(OopIterateClosure* blk) {\n-  if (!is_active()) return;\n-  if (is_humongous()) {\n-    oop_iterate_humongous(blk);\n-  } else {\n-    oop_iterate_objects(blk);\n+\/\/ oop_iterate without closure, return true if completed without cancellation\n+bool ShenandoahHeapRegion::oop_coalesce_and_fill(bool cancellable) {\n+\n+  assert(!is_humongous(), \"No need to fill or coalesce humongous regions\");\n+  if (!is_active()) {\n+    end_preemptible_coalesce_and_fill();\n+    return true;\n@@ -392,1 +470,0 @@\n-}\n@@ -394,5 +471,17 @@\n-void ShenandoahHeapRegion::oop_iterate_objects(OopIterateClosure* blk) {\n-  assert(! is_humongous(), \"no humongous region here\");\n-  HeapWord* obj_addr = bottom();\n-  HeapWord* t = top();\n-  \/\/ Could call objects iterate, but this is easier.\n+  ShenandoahGenerationalHeap* heap = ShenandoahGenerationalHeap::heap();\n+  ShenandoahMarkingContext* marking_context = heap->marking_context();\n+\n+  \/\/ Expect marking to be completed before these threads invoke this service.\n+  assert(heap->gc_generation()->is_mark_complete(), \"sanity\");\n+  shenandoah_assert_generations_reconciled();\n+\n+  \/\/ All objects above TAMS are considered live even though their mark bits will not be set.  Note that young-\n+  \/\/ gen evacuations that interrupt a long-running old-gen concurrent mark may promote objects into old-gen\n+  \/\/ while the old-gen concurrent marking is ongoing.  These newly promoted objects will reside above TAMS\n+  \/\/ and will be treated as live during the current old-gen marking pass, even though they will not be\n+  \/\/ explicitly marked.\n+  HeapWord* t = marking_context->top_at_mark_start(this);\n+\n+  \/\/ Resume coalesce and fill from this address\n+  HeapWord* obj_addr = resume_coalesce_and_fill();\n+\n@@ -401,1 +490,17 @@\n-    obj_addr += obj->oop_iterate_size(blk);\n+    if (marking_context->is_marked(obj)) {\n+      assert(obj->klass() != nullptr, \"klass should not be nullptr\");\n+      obj_addr += obj->size();\n+    } else {\n+      \/\/ Object is not marked.  Coalesce and fill dead object with dead neighbors.\n+      HeapWord* next_marked_obj = marking_context->get_next_marked_addr(obj_addr, t);\n+      assert(next_marked_obj <= t, \"next marked object cannot exceed top\");\n+      size_t fill_size = next_marked_obj - obj_addr;\n+      assert(fill_size >= ShenandoahHeap::min_fill_size(), \"previously allocated object known to be larger than min_size\");\n+      ShenandoahHeap::fill_with_object(obj_addr, fill_size);\n+      heap->old_generation()->card_scan()->coalesce_objects(obj_addr, fill_size);\n+      obj_addr = next_marked_obj;\n+    }\n+    if (cancellable && heap->cancelled_gc()) {\n+      suspend_coalesce_and_fill(obj_addr);\n+      return false;\n+    }\n@@ -403,0 +508,3 @@\n+  \/\/ Mark that this region has been coalesced and filled\n+  end_preemptible_coalesce_and_fill();\n+  return true;\n@@ -405,1 +513,9 @@\n-void ShenandoahHeapRegion::oop_iterate_humongous(OopIterateClosure* blk) {\n+size_t get_card_count(size_t words) {\n+  assert(words % CardTable::card_size_in_words() == 0, \"Humongous iteration must span whole number of cards\");\n+  assert(CardTable::card_size_in_words() * (words \/ CardTable::card_size_in_words()) == words,\n+         \"slice must be integral number of cards\");\n+  return words \/ CardTable::card_size_in_words();\n+}\n+\n+void ShenandoahHeapRegion::oop_iterate_humongous_slice_dirty(OopIterateClosure* blk,\n+                                                             HeapWord* start, size_t words, bool write_table) const {\n@@ -407,1 +523,1 @@\n-  \/\/ Find head.\n+\n@@ -409,1 +525,0 @@\n-  assert(r->is_humongous_start(), \"need humongous head here\");\n@@ -411,1 +526,30 @@\n-  obj->oop_iterate(blk, MemRegion(bottom(), top()));\n+  size_t num_cards = get_card_count(words);\n+\n+  ShenandoahGenerationalHeap* heap = ShenandoahGenerationalHeap::heap();\n+  ShenandoahScanRemembered* scanner = heap->old_generation()->card_scan();\n+  size_t card_index = scanner->card_index_for_addr(start);\n+  if (write_table) {\n+    while (num_cards-- > 0) {\n+      if (scanner->is_write_card_dirty(card_index++)) {\n+        obj->oop_iterate(blk, MemRegion(start, start + CardTable::card_size_in_words()));\n+      }\n+      start += CardTable::card_size_in_words();\n+    }\n+  } else {\n+    while (num_cards-- > 0) {\n+      if (scanner->is_card_dirty(card_index++)) {\n+        obj->oop_iterate(blk, MemRegion(start, start + CardTable::card_size_in_words()));\n+      }\n+      start += CardTable::card_size_in_words();\n+    }\n+  }\n+}\n+\n+void ShenandoahHeapRegion::oop_iterate_humongous_slice_all(OopIterateClosure* cl, HeapWord* start, size_t words) const {\n+  assert(is_humongous(), \"only humongous region here\");\n+\n+  ShenandoahHeapRegion* r = humongous_start_region();\n+  oop obj = cast_to_oop(r->bottom());\n+\n+  \/\/ Scan all data, regardless of whether cards are dirty\n+  obj->oop_iterate(cl, MemRegion(start, start + words));\n@@ -429,1 +573,5 @@\n-void ShenandoahHeapRegion::recycle() {\n+\n+void ShenandoahHeapRegion::recycle_internal() {\n+  assert(_recycling.is_set() && is_trash(), \"Wrong state\");\n+  ShenandoahHeap* heap = ShenandoahHeap::heap();\n+\n@@ -432,1 +580,0 @@\n-\n@@ -434,2 +581,1 @@\n-\n-  ShenandoahHeap::heap()->marking_context()->reset_top_at_mark_start(this);\n+  heap->marking_context()->reset_top_at_mark_start(this);\n@@ -437,0 +583,3 @@\n+  if (ZapUnusedHeapArea) {\n+    SpaceMangler::mangle_region(MemRegion(bottom(), end()));\n+  }\n@@ -439,0 +588,2 @@\n+  set_affiliation(FREE);\n+}\n@@ -440,2 +591,38 @@\n-  if (ZapUnusedHeapArea) {\n-    SpaceMangler::mangle_region(MemRegion(bottom(), end()));\n+void ShenandoahHeapRegion::try_recycle_under_lock() {\n+  shenandoah_assert_heaplocked();\n+  if (is_trash() && _recycling.try_set()) {\n+    if (is_trash()) {\n+      ShenandoahHeap* heap = ShenandoahHeap::heap();\n+      ShenandoahGeneration* generation = heap->generation_for(affiliation());\n+\n+      heap->decrease_used(generation, used());\n+      generation->decrement_affiliated_region_count();\n+\n+      recycle_internal();\n+    }\n+    _recycling.unset();\n+  } else {\n+    \/\/ Ensure recycling is unset before returning to mutator to continue memory allocation.\n+    while (_recycling.is_set()) {\n+      if (os::is_MP()) {\n+        SpinPause();\n+      } else {\n+        os::naked_yield();\n+      }\n+    }\n+  }\n+}\n+\n+void ShenandoahHeapRegion::try_recycle() {\n+  shenandoah_assert_not_heaplocked();\n+  if (is_trash() && _recycling.try_set()) {\n+    \/\/ Double check region state after win the race to set recycling flag\n+    if (is_trash()) {\n+      ShenandoahHeap* heap = ShenandoahHeap::heap();\n+      ShenandoahGeneration* generation = heap->generation_for(affiliation());\n+      heap->decrease_used(generation, used());\n+      generation->decrement_affiliated_region_count_without_lock();\n+\n+      recycle_internal();\n+    }\n+    _recycling.unset();\n@@ -483,0 +670,5 @@\n+  \/\/ Generational Shenandoah needs this alignment for card tables.\n+  if (strcmp(ShenandoahGCMode, \"generational\") == 0) {\n+    max_heap_size = align_up(max_heap_size , CardTable::ct_max_alignment_constraint());\n+  }\n+\n@@ -601,9 +793,0 @@\n-  guarantee(HumongousThresholdWords == 0, \"we should only set it once\");\n-  HumongousThresholdWords = RegionSizeWords * ShenandoahHumongousThreshold \/ 100;\n-  HumongousThresholdWords = align_down(HumongousThresholdWords, MinObjAlignment);\n-  assert (HumongousThresholdWords <= RegionSizeWords, \"sanity\");\n-\n-  guarantee(HumongousThresholdBytes == 0, \"we should only set it once\");\n-  HumongousThresholdBytes = HumongousThresholdWords * HeapWordSize;\n-  assert (HumongousThresholdBytes <= RegionSizeBytes, \"sanity\");\n-\n@@ -611,2 +794,1 @@\n-  MaxTLABSizeWords = MIN2(RegionSizeWords, HumongousThresholdWords);\n-  MaxTLABSizeWords = align_down(MaxTLABSizeWords, MinObjAlignment);\n+  MaxTLABSizeWords = align_down(RegionSizeWords, MinObjAlignment);\n@@ -652,1 +834,1 @@\n-    evt.set_from(_state);\n+    evt.set_from(state());\n@@ -656,1 +838,1 @@\n-  _state = to;\n+  Atomic::store(&_state, to);\n@@ -671,0 +853,60 @@\n+\n+void ShenandoahHeapRegion::set_affiliation(ShenandoahAffiliation new_affiliation) {\n+  ShenandoahHeap* heap = ShenandoahHeap::heap();\n+\n+  ShenandoahAffiliation region_affiliation = heap->region_affiliation(this);\n+  ShenandoahMarkingContext* const ctx = heap->marking_context();\n+  {\n+    log_debug(gc)(\"Setting affiliation of Region \" SIZE_FORMAT \" from %s to %s, top: \" PTR_FORMAT \", TAMS: \" PTR_FORMAT\n+                  \", watermark: \" PTR_FORMAT \", top_bitmap: \" PTR_FORMAT,\n+                  index(), shenandoah_affiliation_name(region_affiliation), shenandoah_affiliation_name(new_affiliation),\n+                  p2i(top()), p2i(ctx->top_at_mark_start(this)), p2i(_update_watermark), p2i(ctx->top_bitmap(this)));\n+  }\n+\n+#ifdef ASSERT\n+  {\n+    size_t idx = this->index();\n+    HeapWord* top_bitmap = ctx->top_bitmap(this);\n+\n+    assert(ctx->is_bitmap_range_within_region_clear(top_bitmap, _end),\n+           \"Region \" SIZE_FORMAT \", bitmap should be clear between top_bitmap: \" PTR_FORMAT \" and end: \" PTR_FORMAT, idx,\n+           p2i(top_bitmap), p2i(_end));\n+  }\n+#endif\n+\n+  if (region_affiliation == new_affiliation) {\n+    return;\n+  }\n+\n+  if (!heap->mode()->is_generational()) {\n+    log_trace(gc)(\"Changing affiliation of region %zu from %s to %s\",\n+                  index(), affiliation_name(), shenandoah_affiliation_name(new_affiliation));\n+    heap->set_affiliation(this, new_affiliation);\n+    return;\n+  }\n+\n+  switch (new_affiliation) {\n+    case FREE:\n+      assert(!has_live(), \"Free region should not have live data\");\n+      break;\n+    case YOUNG_GENERATION:\n+      reset_age();\n+      break;\n+    case OLD_GENERATION:\n+      break;\n+    default:\n+      ShouldNotReachHere();\n+      return;\n+  }\n+  heap->set_affiliation(this, new_affiliation);\n+}\n+\n+void ShenandoahHeapRegion::decrement_humongous_waste() const {\n+  assert(is_humongous(), \"Should only use this for humongous regions\");\n+  size_t waste_bytes = free();\n+  if (waste_bytes > 0) {\n+    ShenandoahHeap* heap = ShenandoahHeap::heap();\n+    ShenandoahGeneration* generation = heap->generation_for(affiliation());\n+    heap->decrease_humongous_waste(generation, waste_bytes);\n+  }\n+}\n","filename":"src\/hotspot\/share\/gc\/shenandoah\/shenandoahHeapRegion.cpp","additions":311,"deletions":69,"binary":false,"changes":380,"status":"modified"},{"patch":"@@ -3,0 +3,1 @@\n+ * Copyright Amazon.com Inc. or its affiliates. All Rights Reserved.\n@@ -30,0 +31,2 @@\n+#include \"gc\/shenandoah\/shenandoahAffiliation.hpp\"\n+#include \"gc\/shenandoah\/shenandoahAgeCensus.hpp\"\n@@ -33,1 +36,0 @@\n-#include \"gc\/shenandoah\/shenandoahPacer.hpp\"\n@@ -126,0 +128,1 @@\n+public:\n@@ -144,0 +147,1 @@\n+private:\n@@ -164,0 +168,1 @@\n+  void recycle_internal();\n@@ -166,1 +171,1 @@\n-  static const int region_states_num() {\n+  static int region_states_num() {\n@@ -171,1 +176,2 @@\n-  void make_regular_allocation();\n+  void make_regular_allocation(ShenandoahAffiliation affiliation);\n+  void make_affiliated_maybe();\n@@ -175,2 +181,2 @@\n-  void make_humongous_start_bypass();\n-  void make_humongous_cont_bypass();\n+  void make_humongous_start_bypass(ShenandoahAffiliation affiliation);\n+  void make_humongous_cont_bypass(ShenandoahAffiliation affiliation);\n@@ -186,12 +192,15 @@\n-  \/\/ Individual states:\n-  bool is_empty_uncommitted()      const { return _state == _empty_uncommitted; }\n-  bool is_empty_committed()        const { return _state == _empty_committed; }\n-  bool is_regular()                const { return _state == _regular; }\n-  bool is_humongous_continuation() const { return _state == _humongous_cont; }\n-\n-  \/\/ Participation in logical groups:\n-  bool is_empty()                  const { return is_empty_committed() || is_empty_uncommitted(); }\n-  bool is_active()                 const { return !is_empty() && !is_trash(); }\n-  bool is_trash()                  const { return _state == _trash; }\n-  bool is_humongous_start()        const { return _state == _humongous_start || _state == _pinned_humongous_start; }\n-  bool is_humongous()              const { return is_humongous_start() || is_humongous_continuation(); }\n+  \/\/ Primitive state predicates\n+  bool is_empty_uncommitted()      const { return state() == _empty_uncommitted; }\n+  bool is_empty_committed()        const { return state() == _empty_committed; }\n+  bool is_regular()                const { return state() == _regular; }\n+  bool is_humongous_continuation() const { return state() == _humongous_cont; }\n+  bool is_regular_pinned()         const { return state() == _pinned; }\n+  bool is_trash()                  const { return state() == _trash; }\n+\n+  \/\/ Derived state predicates (boolean combinations of individual states)\n+  bool static is_empty_state(RegionState state) { return state == _empty_committed || state == _empty_uncommitted; }\n+  bool static is_humongous_start_state(RegionState state) { return state == _humongous_start || state == _pinned_humongous_start; }\n+  bool is_empty()                  const { return is_empty_state(this->state()); }\n+  bool is_active()                 const { auto cur_state = state(); return !is_empty_state(cur_state) && cur_state != _trash; }\n+  bool is_humongous_start()        const { return is_humongous_start_state(state()); }\n+  bool is_humongous()              const { auto cur_state = state(); return is_humongous_start_state(cur_state) || cur_state == _humongous_cont; }\n@@ -199,2 +208,6 @@\n-  bool is_cset()                   const { return _state == _cset   || _state == _pinned_cset; }\n-  bool is_pinned()                 const { return _state == _pinned || _state == _pinned_cset || _state == _pinned_humongous_start; }\n+  bool is_cset()                   const { auto cur_state = state(); return cur_state == _cset || cur_state == _pinned_cset; }\n+  bool is_pinned()                 const { auto cur_state = state(); return cur_state == _pinned || cur_state == _pinned_cset || cur_state == _pinned_humongous_start; }\n+\n+  inline bool is_young() const;\n+  inline bool is_old() const;\n+  inline bool is_affiliated() const;\n@@ -203,2 +216,2 @@\n-  bool is_alloc_allowed()          const { return is_empty() || is_regular() || _state == _pinned; }\n-  bool is_stw_move_allowed()       const { return is_regular() || _state == _cset || (ShenandoahHumongousMoves && _state == _humongous_start); }\n+  bool is_alloc_allowed()          const { auto cur_state = state(); return is_empty_state(cur_state) || cur_state == _regular || cur_state == _pinned; }\n+  bool is_stw_move_allowed()       const { auto cur_state = state(); return cur_state == _regular || cur_state == _cset || (ShenandoahHumongousMoves && cur_state == _humongous_start); }\n@@ -206,2 +219,2 @@\n-  RegionState state()              const { return _state; }\n-  int  state_ordinal()             const { return region_state_to_ordinal(_state); }\n+  RegionState state()              const { return Atomic::load(&_state); }\n+  int  state_ordinal()             const { return region_state_to_ordinal(state()); }\n@@ -221,2 +234,0 @@\n-  static size_t HumongousThresholdBytes;\n-  static size_t HumongousThresholdWords;\n@@ -235,0 +246,2 @@\n+  HeapWord* _top_before_promoted;\n+\n@@ -236,1 +249,2 @@\n-  RegionState _state;\n+  volatile RegionState _state;\n+  HeapWord* _coalesce_and_fill_boundary; \/\/ for old regions not selected as collection set candidates.\n@@ -243,0 +257,1 @@\n+  size_t _plab_allocs;\n@@ -249,0 +264,7 @@\n+  uint _age;\n+  CENSUS_NOISE(uint _youth;)   \/\/ tracks epochs of retrograde ageing (rejuvenation)\n+\n+  ShenandoahSharedFlag _recycling; \/\/ Used to indicate that the region is being recycled; see try_recycle*().\n+\n+  bool _needs_bitmap_reset;\n+\n@@ -265,0 +287,4 @@\n+  inline static bool requires_humongous(size_t words) {\n+    return words > ShenandoahHeapRegion::RegionSizeWords;\n+  }\n+\n@@ -317,8 +343,0 @@\n-  inline static size_t humongous_threshold_bytes() {\n-    return ShenandoahHeapRegion::HumongousThresholdBytes;\n-  }\n-\n-  inline static size_t humongous_threshold_words() {\n-    return ShenandoahHeapRegion::HumongousThresholdWords;\n-  }\n-\n@@ -337,2 +355,13 @@\n-  \/\/ Allocation (return null if full)\n-  inline HeapWord* allocate(size_t word_size, ShenandoahAllocRequest::Type type);\n+  inline void save_top_before_promote();\n+  inline HeapWord* get_top_before_promote() const { return _top_before_promoted; }\n+  inline void restore_top_before_promote();\n+  inline size_t garbage_before_padded_for_promote() const;\n+\n+  \/\/ If next available memory is not aligned on address that is multiple of alignment, fill the empty space\n+  \/\/ so that returned object is aligned on an address that is a multiple of alignment_in_bytes.  Requested\n+  \/\/ size is in words.  It is assumed that this->is_old().  A pad object is allocated, filled, and registered\n+  \/\/ if necessary to assure the new allocation is properly aligned.  Return nullptr if memory is not available.\n+  inline HeapWord* allocate_aligned(size_t word_size, ShenandoahAllocRequest &req, size_t alignment_in_bytes);\n+\n+  \/\/ Allocation (return nullptr if full)\n+  inline HeapWord* allocate(size_t word_size, const ShenandoahAllocRequest& req);\n@@ -357,1 +386,25 @@\n-  void recycle();\n+  void try_recycle_under_lock();\n+\n+  void try_recycle();\n+\n+  inline void begin_preemptible_coalesce_and_fill() {\n+    _coalesce_and_fill_boundary = _bottom;\n+  }\n+\n+  inline void end_preemptible_coalesce_and_fill() {\n+    _coalesce_and_fill_boundary = _end;\n+  }\n+\n+  inline void suspend_coalesce_and_fill(HeapWord* next_focus) {\n+    _coalesce_and_fill_boundary = next_focus;\n+  }\n+\n+  inline HeapWord* resume_coalesce_and_fill() {\n+    return _coalesce_and_fill_boundary;\n+  }\n+\n+  \/\/ Coalesce contiguous spans of garbage objects by filling header and registering start locations with remembered set.\n+  \/\/ This is used by old-gen GC following concurrent marking to make old-gen HeapRegions parsable. Old regions must be\n+  \/\/ parsable because the mark bitmap is not reliable during the concurrent old mark.\n+  \/\/ Return true iff region is completely coalesced and filled.  Returns false if cancelled before task is complete.\n+  bool oop_coalesce_and_fill(bool cancellable);\n@@ -359,1 +412,8 @@\n-  void oop_iterate(OopIterateClosure* cl);\n+  \/\/ Invoke closure on every reference contained within the humongous object that spans this humongous\n+  \/\/ region if the reference is contained within a DIRTY card and the reference is no more than words following\n+  \/\/ start within the humongous object.\n+  void oop_iterate_humongous_slice_dirty(OopIterateClosure* cl, HeapWord* start, size_t words, bool write_table) const;\n+\n+  \/\/ Invoke closure on every reference contained within the humongous object starting from start and\n+  \/\/ ending at start + words.\n+  void oop_iterate_humongous_slice_all(OopIterateClosure* cl, HeapWord* start, size_t words) const;\n@@ -379,0 +439,1 @@\n+  size_t used_before_promote() const { return byte_size(bottom(), get_top_before_promote()); }\n@@ -381,0 +442,5 @@\n+  \/\/ Does this region contain this address?\n+  bool contains(HeapWord* p) const {\n+    return (bottom() <= p) && (p < top());\n+  }\n+\n@@ -386,0 +452,1 @@\n+  size_t get_plab_allocs() const;\n@@ -391,0 +458,36 @@\n+  inline ShenandoahAffiliation affiliation() const;\n+  inline const char* affiliation_name() const;\n+\n+  void set_affiliation(ShenandoahAffiliation new_affiliation);\n+\n+  \/\/ Region ageing and rejuvenation\n+  uint age() const { return _age; }\n+  CENSUS_NOISE(uint youth() const { return _youth; })\n+\n+  void increment_age() {\n+    const uint max_age = markWord::max_age;\n+    assert(_age <= max_age, \"Error\");\n+    if (_age++ >= max_age) {\n+      _age = max_age;   \/\/ clamp\n+    }\n+  }\n+\n+  void reset_age() {\n+    CENSUS_NOISE(_youth += _age;)\n+    _age = 0;\n+  }\n+\n+  CENSUS_NOISE(void clear_youth() { _youth = 0; })\n+\n+  inline bool need_bitmap_reset() const {\n+    return _needs_bitmap_reset;\n+  }\n+\n+  inline void set_needs_bitmap_reset() {\n+    _needs_bitmap_reset = true;\n+  }\n+\n+  inline void unset_needs_bitmap_reset() {\n+    _needs_bitmap_reset = false;\n+  }\n+\n@@ -392,0 +495,1 @@\n+  void decrement_humongous_waste() const;\n@@ -395,3 +499,0 @@\n-  void oop_iterate_objects(OopIterateClosure* cl);\n-  void oop_iterate_humongous(OopIterateClosure* cl);\n-\n","filename":"src\/hotspot\/share\/gc\/shenandoah\/shenandoahHeapRegion.hpp","additions":142,"deletions":41,"binary":false,"changes":183,"status":"modified"},{"patch":"@@ -3,0 +3,1 @@\n+ * Copyright Amazon.com Inc. or its affiliates. All Rights Reserved.\n@@ -28,0 +29,1 @@\n+#include \"gc\/shenandoah\/shenandoahGenerationalHeap.hpp\"\n@@ -29,1 +31,0 @@\n-\n@@ -31,0 +32,1 @@\n+#include \"gc\/shenandoah\/shenandoahOldGeneration.hpp\"\n@@ -34,1 +36,54 @@\n-HeapWord* ShenandoahHeapRegion::allocate(size_t size, ShenandoahAllocRequest::Type type) {\n+HeapWord* ShenandoahHeapRegion::allocate_aligned(size_t size, ShenandoahAllocRequest &req, size_t alignment_in_bytes) {\n+  shenandoah_assert_heaplocked_or_safepoint();\n+  assert(req.is_lab_alloc(), \"allocate_aligned() only applies to LAB allocations\");\n+  assert(is_object_aligned(size), \"alloc size breaks alignment: \" SIZE_FORMAT, size);\n+  assert(is_old(), \"aligned allocations are only taken from OLD regions to support PLABs\");\n+  assert(is_aligned(alignment_in_bytes, HeapWordSize), \"Expect heap word alignment\");\n+\n+  HeapWord* orig_top = top();\n+  size_t alignment_in_words = alignment_in_bytes \/ HeapWordSize;\n+\n+  \/\/ unalignment_words is the amount by which current top() exceeds the desired alignment point.  We subtract this amount\n+  \/\/ from alignment_in_words to determine padding required to next alignment point.\n+\n+  HeapWord* aligned_obj = (HeapWord*) align_up(orig_top, alignment_in_bytes);\n+  size_t pad_words = aligned_obj - orig_top;\n+  if ((pad_words > 0) && (pad_words < ShenandoahHeap::min_fill_size())) {\n+    pad_words += alignment_in_words;\n+    aligned_obj += alignment_in_words;\n+  }\n+\n+  if (pointer_delta(end(), aligned_obj) < size) {\n+    \/\/ Shrink size to fit within available space and align it\n+    size = pointer_delta(end(), aligned_obj);\n+    size = align_down(size, alignment_in_words);\n+  }\n+\n+  \/\/ Both originally requested size and adjusted size must be properly aligned\n+  assert (is_aligned(size, alignment_in_words), \"Size must be multiple of alignment constraint\");\n+  if (size >= req.min_size()) {\n+    \/\/ Even if req.min_size() may not be a multiple of card size, we know that size is.\n+    if (pad_words > 0) {\n+      assert(pad_words >= ShenandoahHeap::min_fill_size(), \"pad_words expanded above to meet size constraint\");\n+      ShenandoahHeap::fill_with_object(orig_top, pad_words);\n+      ShenandoahGenerationalHeap::heap()->old_generation()->card_scan()->register_object(orig_top);\n+    }\n+\n+    make_regular_allocation(req.affiliation());\n+    adjust_alloc_metadata(req.type(), size);\n+\n+    HeapWord* new_top = aligned_obj + size;\n+    assert(new_top <= end(), \"PLAB cannot span end of heap region\");\n+    set_top(new_top);\n+    \/\/ We do not req.set_actual_size() here.  The caller sets it.\n+    req.set_waste(pad_words);\n+    assert(is_object_aligned(new_top), \"new top breaks alignment: \" PTR_FORMAT, p2i(new_top));\n+    assert(is_aligned(aligned_obj, alignment_in_bytes), \"obj is not aligned: \" PTR_FORMAT, p2i(aligned_obj));\n+    return aligned_obj;\n+  } else {\n+    \/\/ The aligned size that fits in this region is smaller than min_size, so don't align top and don't allocate.  Return failure.\n+    return nullptr;\n+  }\n+}\n+\n+HeapWord* ShenandoahHeapRegion::allocate(size_t size, const ShenandoahAllocRequest& req) {\n@@ -40,2 +95,2 @@\n-    make_regular_allocation();\n-    adjust_alloc_metadata(type, size);\n+    make_regular_allocation(req.affiliation());\n+    adjust_alloc_metadata(req.type(), size);\n@@ -67,0 +122,3 @@\n+    case ShenandoahAllocRequest::_alloc_plab:\n+      _plab_allocs += size;\n+      break;\n@@ -85,6 +143,0 @@\n-#ifdef ASSERT\n-  size_t live_bytes = new_live_data * HeapWordSize;\n-  size_t used_bytes = used();\n-  assert(live_bytes <= used_bytes,\n-         \"can't have more live data than used: \" SIZE_FORMAT \", \" SIZE_FORMAT, live_bytes, used_bytes);\n-#endif\n@@ -118,0 +170,11 @@\n+inline size_t ShenandoahHeapRegion::garbage_before_padded_for_promote() const {\n+  assert(get_top_before_promote() != nullptr, \"top before promote should not equal null\");\n+  size_t used_before_promote = byte_size(bottom(), get_top_before_promote());\n+  assert(used_before_promote >= get_live_data_bytes(),\n+         \"Live Data must be a subset of used before promotion live: \" SIZE_FORMAT \" used: \" SIZE_FORMAT,\n+         get_live_data_bytes(), used_before_promote);\n+  size_t result = used_before_promote - get_live_data_bytes();\n+  return result;\n+\n+}\n+\n@@ -136,0 +199,30 @@\n+inline ShenandoahAffiliation ShenandoahHeapRegion::affiliation() const {\n+  return ShenandoahHeap::heap()->region_affiliation(this);\n+}\n+\n+inline const char* ShenandoahHeapRegion::affiliation_name() const {\n+  return shenandoah_affiliation_name(affiliation());\n+}\n+\n+inline bool ShenandoahHeapRegion::is_young() const {\n+  return affiliation() == YOUNG_GENERATION;\n+}\n+\n+inline bool ShenandoahHeapRegion::is_old() const {\n+  return affiliation() == OLD_GENERATION;\n+}\n+\n+inline bool ShenandoahHeapRegion::is_affiliated() const {\n+  return affiliation() != FREE;\n+}\n+\n+inline void ShenandoahHeapRegion::save_top_before_promote() {\n+  _top_before_promoted = _top;\n+}\n+\n+inline void ShenandoahHeapRegion::restore_top_before_promote() {\n+  _top = _top_before_promoted;\n+  _top_before_promoted = nullptr;\n+ }\n+\n+\n","filename":"src\/hotspot\/share\/gc\/shenandoah\/shenandoahHeapRegion.inline.hpp","additions":103,"deletions":10,"binary":false,"changes":113,"status":"modified"},{"patch":"@@ -0,0 +1,89 @@\n+\/*\n+ * Copyright Amazon.com Inc. or its affiliates. All Rights Reserved.\n+ * DO NOT ALTER OR REMOVE COPYRIGHT NOTICES OR THIS FILE HEADER.\n+ *\n+ * This code is free software; you can redistribute it and\/or modify it\n+ * under the terms of the GNU General Public License version 2 only, as\n+ * published by the Free Software Foundation.\n+ *\n+ * This code is distributed in the hope that it will be useful, but WITHOUT\n+ * ANY WARRANTY; without even the implied warranty of MERCHANTABILITY or\n+ * FITNESS FOR A PARTICULAR PURPOSE.  See the GNU General Public License\n+ * version 2 for more details (a copy is included in the LICENSE file that\n+ * accompanied this code).\n+ *\n+ * You should have received a copy of the GNU General Public License version\n+ * 2 along with this work; if not, write to the Free Software Foundation,\n+ * Inc., 51 Franklin St, Fifth Floor, Boston, MA 02110-1301 USA.\n+ *\n+ * Please contact Oracle, 500 Oracle Parkway, Redwood Shores, CA 94065 USA\n+ * or visit www.oracle.com if you need additional information or have any\n+ * questions.\n+ *\n+ *\/\n+\n+#include \"precompiled.hpp\"\n+\n+#include \"gc\/shenandoah\/shenandoahHeapRegionClosures.hpp\"\n+#include \"gc\/shenandoah\/shenandoahMarkingContext.hpp\"\n+#include \"gc\/shenandoah\/shenandoahSharedVariables.hpp\"\n+\n+ShenandoahSynchronizePinnedRegionStates::ShenandoahSynchronizePinnedRegionStates() :\n+  _lock(ShenandoahHeap::heap()->lock()) { }\n+\n+void ShenandoahSynchronizePinnedRegionStates::heap_region_do(ShenandoahHeapRegion* r) {\n+  \/\/ Drop \"pinned\" state from regions that no longer have a pinned count. Put\n+  \/\/ regions with a pinned count into the \"pinned\" state.\n+  if (r->is_active()) {\n+    synchronize_pin_count(r);\n+  }\n+}\n+\n+void ShenandoahSynchronizePinnedRegionStates::synchronize_pin_count(ShenandoahHeapRegion* r) {\n+  if (r->is_pinned()) {\n+    if (r->pin_count() == 0) {\n+      ShenandoahHeapLocker locker(_lock);\n+      r->make_unpinned();\n+    }\n+  } else {\n+    if (r->pin_count() > 0) {\n+      ShenandoahHeapLocker locker(_lock);\n+      r->make_pinned();\n+    }\n+  }\n+}\n+\n+ShenandoahFinalMarkUpdateRegionStateClosure::ShenandoahFinalMarkUpdateRegionStateClosure(ShenandoahMarkingContext *ctx) :\n+        _ctx(ctx) { }\n+\n+void ShenandoahFinalMarkUpdateRegionStateClosure::heap_region_do(ShenandoahHeapRegion* r) {\n+  if (r->is_active()) {\n+    if (_ctx != nullptr) {\n+      \/\/ _ctx may be null when this closure is used to sync only the pin status\n+      \/\/ update the watermark of old regions. For old regions we cannot reset\n+      \/\/ the TAMS because we rely on that to keep promoted objects alive after\n+      \/\/ old marking is complete.\n+\n+      \/\/ All allocations past TAMS are implicitly live, adjust the region data.\n+      \/\/ Bitmaps\/TAMS are swapped at this point, so we need to poll complete bitmap.\n+      HeapWord *tams = _ctx->top_at_mark_start(r);\n+      HeapWord *top = r->top();\n+      if (top > tams) {\n+        r->increase_live_data_alloc_words(pointer_delta(top, tams));\n+      }\n+    }\n+\n+    \/\/ We are about to select the collection set, make sure it knows about\n+    \/\/ current pinning status. Also, this allows trashing more regions that\n+    \/\/ now have their pinning status dropped.\n+    _pins.synchronize_pin_count(r);\n+\n+    \/\/ Remember limit for updating refs. It's guaranteed that we get no\n+    \/\/ from-space-refs written from here on.\n+    r->set_update_watermark_at_safepoint(r->top());\n+  } else {\n+    assert(!r->has_live(), \"Region \" SIZE_FORMAT \" should have no live data\", r->index());\n+    assert(_ctx == nullptr || _ctx->top_at_mark_start(r) == r->top(),\n+           \"Region \" SIZE_FORMAT \" should have correct TAMS\", r->index());\n+  }\n+}\n","filename":"src\/hotspot\/share\/gc\/shenandoah\/shenandoahHeapRegionClosures.cpp","additions":89,"deletions":0,"binary":false,"changes":89,"status":"added"},{"patch":"@@ -0,0 +1,100 @@\n+\/*\n+ * Copyright Amazon.com Inc. or its affiliates. All Rights Reserved.\n+ * DO NOT ALTER OR REMOVE COPYRIGHT NOTICES OR THIS FILE HEADER.\n+ *\n+ * This code is free software; you can redistribute it and\/or modify it\n+ * under the terms of the GNU General Public License version 2 only, as\n+ * published by the Free Software Foundation.\n+ *\n+ * This code is distributed in the hope that it will be useful, but WITHOUT\n+ * ANY WARRANTY; without even the implied warranty of MERCHANTABILITY or\n+ * FITNESS FOR A PARTICULAR PURPOSE.  See the GNU General Public License\n+ * version 2 for more details (a copy is included in the LICENSE file that\n+ * accompanied this code).\n+ *\n+ * You should have received a copy of the GNU General Public License version\n+ * 2 along with this work; if not, write to the Free Software Foundation,\n+ * Inc., 51 Franklin St, Fifth Floor, Boston, MA 02110-1301 USA.\n+ *\n+ * Please contact Oracle, 500 Oracle Parkway, Redwood Shores, CA 94065 USA\n+ * or visit www.oracle.com if you need additional information or have any\n+ * questions.\n+ *\n+ *\/\n+\n+#ifndef SHARE_GC_SHENANDOAH_SHENANDOAHHEAPREGIONCLOSURES_HPP\n+#define SHARE_GC_SHENANDOAH_SHENANDOAHHEAPREGIONCLOSURES_HPP\n+\n+\n+#include \"gc\/shenandoah\/shenandoahHeap.hpp\"\n+#include \"gc\/shenandoah\/shenandoahHeapRegion.inline.hpp\"\n+\n+\/\/ Applies the given closure to all regions with the given affiliation\n+template<ShenandoahAffiliation AFFILIATION>\n+class ShenandoahIncludeRegionClosure : public ShenandoahHeapRegionClosure {\n+private:\n+  ShenandoahHeapRegionClosure* _closure;\n+\n+public:\n+  explicit ShenandoahIncludeRegionClosure(ShenandoahHeapRegionClosure* closure): _closure(closure) {}\n+\n+  void heap_region_do(ShenandoahHeapRegion* r) override {\n+    if (r->affiliation() == AFFILIATION) {\n+      _closure->heap_region_do(r);\n+    }\n+  }\n+\n+  bool is_thread_safe() override {\n+    return _closure->is_thread_safe();\n+  }\n+};\n+\n+\/\/ Applies the given closure to all regions without the given affiliation\n+template<ShenandoahAffiliation AFFILIATION>\n+class ShenandoahExcludeRegionClosure : public ShenandoahHeapRegionClosure {\n+private:\n+  ShenandoahHeapRegionClosure* _closure;\n+\n+public:\n+  explicit ShenandoahExcludeRegionClosure(ShenandoahHeapRegionClosure* closure): _closure(closure) {}\n+\n+  void heap_region_do(ShenandoahHeapRegion* r) override {\n+    if (r->affiliation() != AFFILIATION) {\n+      _closure->heap_region_do(r);\n+    }\n+  }\n+\n+  bool is_thread_safe() override {\n+    return _closure->is_thread_safe();\n+  }\n+};\n+\n+\/\/ Makes regions pinned or unpinned according to the region's pin count\n+class ShenandoahSynchronizePinnedRegionStates : public ShenandoahHeapRegionClosure {\n+private:\n+  ShenandoahHeapLock* const _lock;\n+\n+public:\n+  ShenandoahSynchronizePinnedRegionStates();\n+\n+  void heap_region_do(ShenandoahHeapRegion* r) override;\n+  bool is_thread_safe() override { return true; }\n+\n+  void synchronize_pin_count(ShenandoahHeapRegion* r);\n+};\n+\n+class ShenandoahMarkingContext;\n+\n+\/\/ Synchronizes region pinned status, sets update watermark and adjust live data tally for regions\n+class ShenandoahFinalMarkUpdateRegionStateClosure : public ShenandoahHeapRegionClosure {\n+private:\n+  ShenandoahMarkingContext* const _ctx;\n+  ShenandoahSynchronizePinnedRegionStates _pins;\n+public:\n+  explicit ShenandoahFinalMarkUpdateRegionStateClosure(ShenandoahMarkingContext* ctx);\n+\n+  void heap_region_do(ShenandoahHeapRegion* r) override;\n+  bool is_thread_safe() override { return true; }\n+};\n+\n+#endif \/\/ SHARE_GC_SHENANDOAH_SHENANDOAHHEAPREGIONCLOSURES_HPP\n","filename":"src\/hotspot\/share\/gc\/shenandoah\/shenandoahHeapRegionClosures.hpp","additions":100,"deletions":0,"binary":false,"changes":100,"status":"added"},{"patch":"@@ -3,0 +3,1 @@\n+ * Copyright Amazon.com Inc. or its affiliates. All Rights Reserved.\n@@ -26,0 +27,2 @@\n+\n+#include \"gc\/shenandoah\/shenandoahGeneration.hpp\"\n@@ -30,0 +33,1 @@\n+#include \"logging\/logStream.hpp\"\n@@ -33,0 +37,1 @@\n+#include \"utilities\/defaultStream.hpp\"\n@@ -52,0 +57,3 @@\n+    cname = PerfDataManager::counter_name(_name_space, \"protocol_version\");\n+    PerfDataManager::create_constant(SUN_GC, cname, PerfData::U_None, VERSION_NUMBER, CHECK);\n+\n@@ -60,0 +68,1 @@\n+    \/\/ Initializing performance data resources for each region\n@@ -66,2 +75,1 @@\n-      _regions_data[i] = PerfDataManager::create_long_variable(SUN_GC, data_name,\n-                                                               PerfData::U_None, CHECK);\n+      _regions_data[i] = PerfDataManager::create_long_variable(SUN_GC, data_name, PerfData::U_None, CHECK);\n@@ -76,0 +84,22 @@\n+void ShenandoahHeapRegionCounters::write_snapshot(PerfLongVariable** regions,\n+                                             PerfLongVariable* ts,\n+                                             PerfLongVariable* status,\n+                                             size_t num_regions,\n+                                             size_t region_size, size_t protocol_version) {\n+  LogTarget(Trace, gc, region) lt;\n+  if (lt.is_enabled()) {\n+    ResourceMark rm;\n+    LogStream ls(lt);\n+\n+    ls.print_cr(JLONG_FORMAT \" \" JLONG_FORMAT \" \" SIZE_FORMAT \" \" SIZE_FORMAT \" \" SIZE_FORMAT,\n+            ts->get_value(), status->get_value(), num_regions, region_size, protocol_version);\n+    if (num_regions > 0) {\n+      ls.print(JLONG_FORMAT, regions[0]->get_value());\n+    }\n+    for (uint i = 1; i < num_regions; ++i) {\n+      ls.print(\" \" JLONG_FORMAT, regions[i]->get_value());\n+    }\n+    ls.cr();\n+  }\n+}\n+\n@@ -80,2 +110,1 @@\n-    if (current - last > ShenandoahRegionSamplingRate &&\n-            Atomic::cmpxchg(&_last_sample_millis, last, current) == last) {\n+    if (current - last > ShenandoahRegionSamplingRate && Atomic::cmpxchg(&_last_sample_millis, last, current) == last) {\n@@ -84,6 +113,1 @@\n-      jlong status = 0;\n-      if (heap->is_concurrent_mark_in_progress())      status |= 1 << 0;\n-      if (heap->is_evacuation_in_progress())           status |= 1 << 1;\n-      if (heap->is_update_refs_in_progress())          status |= 1 << 2;\n-      _status->set_value(status);\n-\n+      _status->set_value(encode_heap_status(heap));\n@@ -92,2 +116,0 @@\n-      size_t num_regions = heap->num_regions();\n-\n@@ -97,0 +119,1 @@\n+        size_t num_regions = heap->num_regions();\n@@ -104,0 +127,1 @@\n+          data |= ((100 * r->get_plab_allocs() \/ rs)     & PERCENT_MASK) << PLAB_SHIFT;\n@@ -105,0 +129,3 @@\n+\n+          data |= (r->age() & AGE_MASK) << AGE_SHIFT;\n+          data |= (r->affiliation() & AFFILIATION_MASK) << AFFILIATION_SHIFT;\n@@ -108,0 +135,3 @@\n+\n+        \/\/ If logging enabled, dump current region snapshot to log file\n+        write_snapshot(_regions_data, _timestamp, _status, num_regions, rs >> 10, VERSION_NUMBER);\n@@ -109,0 +139,3 @@\n+    }\n+  }\n+}\n@@ -110,0 +143,46 @@\n+static int encode_phase(ShenandoahHeap* heap) {\n+  if (heap->is_evacuation_in_progress() || heap->is_full_gc_move_in_progress()) {\n+    return 2;\n+  }\n+  if (heap->is_update_refs_in_progress() || heap->is_full_gc_move_in_progress()) {\n+    return 3;\n+  }\n+  if (heap->is_concurrent_mark_in_progress() || heap->is_concurrent_weak_root_in_progress() || heap->is_full_gc_in_progress()) {\n+    return 1;\n+  }\n+  assert(heap->is_idle(), \"Unexpected gc_state: %d\", heap->gc_state());\n+  return 0;\n+}\n+\n+static int get_generation_shift(ShenandoahGeneration* generation) {\n+  switch (generation->type()) {\n+    case NON_GEN:\n+    case GLOBAL:\n+      return 0;\n+    case OLD:\n+      return 2;\n+    case YOUNG:\n+      return 4;\n+    default:\n+      ShouldNotReachHere();\n+      return -1;\n+  }\n+}\n+\n+jlong ShenandoahHeapRegionCounters::encode_heap_status(ShenandoahHeap* heap) {\n+\n+  if (heap->is_idle() && !heap->is_full_gc_in_progress()) {\n+    return 0;\n+  }\n+\n+  jlong status = 0;\n+  if (!heap->mode()->is_generational()) {\n+    status = encode_phase(heap);\n+  } else {\n+    int phase = encode_phase(heap);\n+    ShenandoahGeneration* generation = heap->active_generation();\n+    assert(generation != nullptr, \"Expected active generation in this mode.\");\n+    int shift = get_generation_shift(generation);\n+    status |= ((phase & 0x3) << shift);\n+    if (heap->is_concurrent_old_mark_in_progress()) {\n+      status |= (1 << 2);\n@@ -111,0 +190,2 @@\n+    log_develop_trace(gc)(\"%s, phase=%u, old_mark=%s, status=\" JLONG_FORMAT,\n+                          generation->name(), phase, BOOL_TO_STR(heap->is_concurrent_old_mark_in_progress()), status);\n@@ -112,0 +193,9 @@\n+\n+  if (heap->is_degenerated_gc_in_progress()) {\n+    status |= (1 << 6);\n+  }\n+  if (heap->is_full_gc_in_progress()) {\n+    status |= (1 << 7);\n+  }\n+\n+  return status;\n","filename":"src\/hotspot\/share\/gc\/shenandoah\/shenandoahHeapRegionCounters.cpp","additions":102,"deletions":12,"binary":false,"changes":114,"status":"modified"},{"patch":"@@ -3,0 +3,1 @@\n+ * Copyright Amazon.com Inc. or its affiliates. All Rights Reserved.\n@@ -29,0 +30,1 @@\n+#include \"logging\/logFileStreamOutput.hpp\"\n@@ -40,3 +42,8 @@\n- *     - bit 0 set when marking in progress\n- *     - bit 1 set when evacuation in progress\n- *     - bit 2 set when update refs in progress\n+ *   | global | old   | young | mode |\n+ *   |  0..1  | 2..3  | 4..5  | 6..7 |\n+ *\n+ *   For each generation:\n+ *   0 = idle, 1 = marking, 2 = evacuating, 3 = updating refs\n+ *\n+ *   For mode:\n+ *   0 = concurrent, 1 = degenerated, 2 = full\n@@ -54,1 +61,1 @@\n- * - bits 35-41  <reserved>\n+ * - bits 35-41  plab allocated memory in percent\n@@ -56,1 +63,2 @@\n- * - bits 51-57  <reserved>\n+ * - bits 51-55  age\n+ * - bits 56-57  affiliation: 0 = free, young = 1, old = 2\n@@ -62,2 +70,4 @@\n-  static const jlong PERCENT_MASK = 0x7f;\n-  static const jlong STATUS_MASK  = 0x3f;\n+  static const jlong PERCENT_MASK      = 0x7f;\n+  static const jlong AGE_MASK          = 0x1f;\n+  static const jlong AFFILIATION_MASK  = 0x03;\n+  static const jlong STATUS_MASK       = 0x3f;\n@@ -65,5 +75,9 @@\n-  static const jlong USED_SHIFT   = 0;\n-  static const jlong LIVE_SHIFT   = 7;\n-  static const jlong TLAB_SHIFT   = 14;\n-  static const jlong GCLAB_SHIFT  = 21;\n-  static const jlong SHARED_SHIFT = 28;\n+  static const jlong USED_SHIFT        = 0;\n+  static const jlong LIVE_SHIFT        = 7;\n+  static const jlong TLAB_SHIFT        = 14;\n+  static const jlong GCLAB_SHIFT       = 21;\n+  static const jlong SHARED_SHIFT      = 28;\n+  static const jlong PLAB_SHIFT        = 35;\n+  static const jlong AGE_SHIFT         = 51;\n+  static const jlong AFFILIATION_SHIFT = 56;\n+  static const jlong STATUS_SHIFT      = 58;\n@@ -71,1 +85,1 @@\n-  static const jlong STATUS_SHIFT = 58;\n+  static const jlong VERSION_NUMBER    = 2;\n@@ -79,0 +93,6 @@\n+  void write_snapshot(PerfLongVariable** regions,\n+                      PerfLongVariable* ts,\n+                      PerfLongVariable* status,\n+                      size_t num_regions,\n+                      size_t region_size, size_t protocolVersion);\n+\n@@ -83,0 +103,3 @@\n+\n+private:\n+  static jlong encode_heap_status(ShenandoahHeap* heap) ;\n","filename":"src\/hotspot\/share\/gc\/shenandoah\/shenandoahHeapRegionCounters.hpp","additions":36,"deletions":13,"binary":false,"changes":49,"status":"modified"},{"patch":"@@ -3,0 +3,1 @@\n+ * Copyright Amazon.com Inc. or its affiliates. All Rights Reserved.\n@@ -32,1 +33,0 @@\n-#include \"runtime\/globals.hpp\"\n@@ -35,0 +35,5 @@\n+void ShenandoahInitLogger::print() {\n+  ShenandoahInitLogger init_log;\n+  init_log.print_all();\n+}\n+\n@@ -38,22 +43,4 @@\n-  ShenandoahHeap* heap = ShenandoahHeap::heap();\n-\n-  log_info(gc, init)(\"Mode: %s\",\n-                     heap->mode()->name());\n-\n-  log_info(gc, init)(\"Heuristics: %s\",\n-                     heap->heuristics()->name());\n-\n-  log_info(gc, init)(\"Heap Region Count: \" SIZE_FORMAT,\n-                     ShenandoahHeapRegion::region_count());\n-\n-  log_info(gc, init)(\"Heap Region Size: \" SIZE_FORMAT \"%s\",\n-                     byte_size_in_exact_unit(ShenandoahHeapRegion::region_size_bytes()),\n-                     exact_unit_for_byte_size(ShenandoahHeapRegion::region_size_bytes()));\n-\n-  log_info(gc, init)(\"TLAB Size Max: \" SIZE_FORMAT \"%s\",\n-                     byte_size_in_exact_unit(ShenandoahHeapRegion::max_tlab_size_bytes()),\n-                     exact_unit_for_byte_size(ShenandoahHeapRegion::max_tlab_size_bytes()));\n-\n-  log_info(gc, init)(\"Humongous Object Threshold: \" SIZE_FORMAT \"%s\",\n-          byte_size_in_exact_unit(ShenandoahHeapRegion::humongous_threshold_bytes()),\n-          exact_unit_for_byte_size(ShenandoahHeapRegion::humongous_threshold_bytes()));\n+  log_info(gc, init)(\"Heap Region Count: \" SIZE_FORMAT, ShenandoahHeapRegion::region_count());\n+  log_info(gc, init)(\"Heap Region Size: \" EXACTFMT, EXACTFMTARGS(ShenandoahHeapRegion::region_size_bytes()));\n+  log_info(gc, init)(\"TLAB Size Max: \" EXACTFMT, EXACTFMTARGS(ShenandoahHeapRegion::max_tlab_size_bytes()));\n+  log_info(gc, init)(\"Soft Max Heap Size: \" EXACTFMT, EXACTFMTARGS(ShenandoahHeap::heap()->soft_max_capacity()));\n@@ -62,3 +49,6 @@\n-void ShenandoahInitLogger::print() {\n-  ShenandoahInitLogger init_log;\n-  init_log.print_all();\n+void ShenandoahInitLogger::print_gc_specific() {\n+  GCInitLogger::print_gc_specific();\n+\n+  ShenandoahHeap* heap = ShenandoahHeap::heap();\n+  log_info(gc, init)(\"Mode: %s\", heap->mode()->name());\n+  log_info(gc, init)(\"Heuristics: %s\", heap->heuristics()->name());\n","filename":"src\/hotspot\/share\/gc\/shenandoah\/shenandoahInitLogger.cpp","additions":16,"deletions":26,"binary":false,"changes":42,"status":"modified"},{"patch":"@@ -32,1 +32,2 @@\n-  virtual void print_heap();\n+  void print_heap() override;\n+  void print_gc_specific() override;\n","filename":"src\/hotspot\/share\/gc\/shenandoah\/shenandoahInitLogger.hpp","additions":2,"deletions":1,"binary":false,"changes":3,"status":"modified"},{"patch":"@@ -49,0 +49,1 @@\n+  int yields = 0;\n@@ -71,1 +72,1 @@\n-          os::naked_yield();\n+          yield_or_sleep(yields);\n@@ -74,1 +75,1 @@\n-        os::naked_yield();\n+        yield_or_sleep(yields);\n@@ -77,1 +78,1 @@\n-      os::naked_yield();\n+      yield_or_sleep(yields);\n@@ -82,0 +83,12 @@\n+void ShenandoahLock::yield_or_sleep(int &yields) {\n+  \/\/ Simple yield-sleep policy: do one 100us sleep after every N yields.\n+  \/\/ Tested with different values of N, and chose 3 for best performance.\n+  if (yields < 3) {\n+    os::naked_yield();\n+    yields++;\n+  } else {\n+    os::naked_short_nanosleep(100000);\n+    yields = 0;\n+  }\n+}\n+\n","filename":"src\/hotspot\/share\/gc\/shenandoah\/shenandoahLock.cpp","additions":16,"deletions":3,"binary":false,"changes":19,"status":"modified"},{"patch":"@@ -45,0 +45,2 @@\n+  static void yield_or_sleep(int &yields);\n+\n","filename":"src\/hotspot\/share\/gc\/shenandoah\/shenandoahLock.hpp","additions":2,"deletions":0,"binary":false,"changes":2,"status":"modified"},{"patch":"@@ -3,0 +3,1 @@\n+ * Copyright Amazon.com Inc. or its affiliates. All Rights Reserved.\n@@ -30,0 +31,1 @@\n+#include \"gc\/shenandoah\/shenandoahGeneration.hpp\"\n@@ -37,11 +39,0 @@\n-ShenandoahMarkRefsSuperClosure::ShenandoahMarkRefsSuperClosure(ShenandoahObjToScanQueue* q,  ShenandoahReferenceProcessor* rp) :\n-  MetadataVisitingOopIterateClosure(rp),\n-  _queue(q),\n-  _mark_context(ShenandoahHeap::heap()->marking_context()),\n-  _weak(false)\n-{ }\n-\n-ShenandoahMark::ShenandoahMark() :\n-  _task_queues(ShenandoahHeap::heap()->marking_context()->task_queues()) {\n-}\n-\n@@ -57,1 +48,3 @@\n-  CodeCache::on_gc_marking_cycle_finish();\n+  if (!ShenandoahHeap::heap()->is_concurrent_old_mark_in_progress()) {\n+    CodeCache::on_gc_marking_cycle_finish();\n+  }\n@@ -60,4 +53,7 @@\n-void ShenandoahMark::clear() {\n-  \/\/ Clean up marking stacks.\n-  ShenandoahObjToScanQueueSet* queues = ShenandoahHeap::heap()->marking_context()->task_queues();\n-  queues->clear();\n+ShenandoahMarkRefsSuperClosure::ShenandoahMarkRefsSuperClosure(ShenandoahObjToScanQueue* q,  ShenandoahReferenceProcessor* rp, ShenandoahObjToScanQueue* old_q) :\n+  MetadataVisitingOopIterateClosure(rp),\n+  _queue(q),\n+  _old_queue(old_q),\n+  _mark_context(ShenandoahHeap::heap()->marking_context()),\n+  _weak(false)\n+{ }\n@@ -65,2 +61,4 @@\n-  \/\/ Cancel SATB buffers.\n-  ShenandoahBarrierSet::satb_mark_queue_set().abandon_partial_marking();\n+ShenandoahMark::ShenandoahMark(ShenandoahGeneration* generation) :\n+  _generation(generation),\n+  _task_queues(generation->task_queues()),\n+  _old_gen_task_queues(generation->old_gen_task_queues()) {\n@@ -69,2 +67,2 @@\n-template <bool CANCELLABLE, StringDedupMode STRING_DEDUP>\n-void ShenandoahMark::mark_loop_prework(uint w, TaskTerminator *t, ShenandoahReferenceProcessor *rp, StringDedup::Requests* const req) {\n+template <ShenandoahGenerationType GENERATION, bool CANCELLABLE, StringDedupMode STRING_DEDUP>\n+void ShenandoahMark::mark_loop_prework(uint w, TaskTerminator *t, ShenandoahReferenceProcessor *rp, StringDedup::Requests* const req, bool update_refs) {\n@@ -72,0 +70,1 @@\n+  ShenandoahObjToScanQueue* old_q = get_old_queue(w);\n@@ -78,4 +77,4 @@\n-  if (heap->has_forwarded_objects()) {\n-    using Closure = ShenandoahMarkUpdateRefsClosure;\n-    Closure cl(q, rp);\n-    mark_loop_work<Closure, CANCELLABLE, STRING_DEDUP>(&cl, ld, w, t, req);\n+  if (update_refs) {\n+    using Closure = ShenandoahMarkUpdateRefsClosure<GENERATION>;\n+    Closure cl(q, rp, old_q);\n+    mark_loop_work<Closure, GENERATION, CANCELLABLE, STRING_DEDUP>(&cl, ld, w, t, req);\n@@ -83,3 +82,3 @@\n-    using Closure = ShenandoahMarkRefsClosure;\n-    Closure cl(q, rp);\n-    mark_loop_work<Closure, CANCELLABLE, STRING_DEDUP>(&cl, ld, w, t, req);\n+    using Closure = ShenandoahMarkRefsClosure<GENERATION>;\n+    Closure cl(q, rp, old_q);\n+    mark_loop_work<Closure, GENERATION, CANCELLABLE, STRING_DEDUP>(&cl, ld, w, t, req);\n@@ -91,0 +90,1 @@\n+template<bool CANCELLABLE, StringDedupMode STRING_DEDUP>\n@@ -92,1 +92,24 @@\n-               bool cancellable, StringDedupMode dedup_mode, StringDedup::Requests* const req) {\n+                               ShenandoahGenerationType generation, StringDedup::Requests* const req) {\n+  bool update_refs = ShenandoahHeap::heap()->has_forwarded_objects();\n+  switch (generation) {\n+    case YOUNG:\n+      mark_loop_prework<YOUNG, CANCELLABLE, STRING_DEDUP>(worker_id, terminator, rp, req, update_refs);\n+      break;\n+    case OLD:\n+      \/\/ Old generation collection only performs marking, it should not update references.\n+      mark_loop_prework<OLD, CANCELLABLE, STRING_DEDUP>(worker_id, terminator, rp, req, false);\n+      break;\n+    case GLOBAL:\n+      mark_loop_prework<GLOBAL, CANCELLABLE, STRING_DEDUP>(worker_id, terminator, rp, req, update_refs);\n+      break;\n+    case NON_GEN:\n+      mark_loop_prework<NON_GEN, CANCELLABLE, STRING_DEDUP>(worker_id, terminator, rp, req, update_refs);\n+      break;\n+    default:\n+      ShouldNotReachHere();\n+      break;\n+  }\n+}\n+\n+void ShenandoahMark::mark_loop(uint worker_id, TaskTerminator* terminator, ShenandoahReferenceProcessor *rp,\n+                               ShenandoahGenerationType generation, bool cancellable, StringDedupMode dedup_mode, StringDedup::Requests* const req) {\n@@ -96,1 +119,1 @@\n-        mark_loop_prework<true, NO_DEDUP>(worker_id, terminator, rp, req);\n+        mark_loop<true, NO_DEDUP>(worker_id, terminator, rp, generation, req);\n@@ -99,1 +122,1 @@\n-        mark_loop_prework<true, ENQUEUE_DEDUP>(worker_id, terminator, rp, req);\n+        mark_loop<true, ENQUEUE_DEDUP>(worker_id, terminator, rp, generation, req);\n@@ -102,1 +125,1 @@\n-        mark_loop_prework<true, ALWAYS_DEDUP>(worker_id, terminator, rp, req);\n+        mark_loop<true, ALWAYS_DEDUP>(worker_id, terminator, rp, generation, req);\n@@ -108,1 +131,1 @@\n-        mark_loop_prework<false, NO_DEDUP>(worker_id, terminator, rp, req);\n+        mark_loop<false, NO_DEDUP>(worker_id, terminator, rp, generation, req);\n@@ -111,1 +134,1 @@\n-        mark_loop_prework<false, ENQUEUE_DEDUP>(worker_id, terminator, rp, req);\n+        mark_loop<false, ENQUEUE_DEDUP>(worker_id, terminator, rp, generation, req);\n@@ -114,1 +137,1 @@\n-        mark_loop_prework<false, ALWAYS_DEDUP>(worker_id, terminator, rp, req);\n+        mark_loop<false, ALWAYS_DEDUP>(worker_id, terminator, rp, generation, req);\n@@ -120,1 +143,1 @@\n-template <class T, bool CANCELLABLE, StringDedupMode STRING_DEDUP>\n+template <class T, ShenandoahGenerationType GENERATION, bool CANCELLABLE, StringDedupMode STRING_DEDUP>\n@@ -129,1 +152,6 @@\n-  heap->ref_processor()->set_mark_closure(worker_id, cl);\n+  \/\/ Do not use active_generation() : we must use the gc_generation() set by\n+  \/\/ ShenandoahGCScope on the ControllerThread's stack; no safepoint may\n+  \/\/ intervene to update active_generation, so we can't\n+  \/\/ shenandoah_assert_generations_reconciled() here.\n+  assert(heap->gc_generation()->type() == GENERATION, \"Sanity: %d != %d\", heap->gc_generation()->type(), GENERATION);\n+  heap->gc_generation()->ref_processor()->set_mark_closure(worker_id, cl);\n@@ -149,1 +177,1 @@\n-        do_task<T, STRING_DEDUP>(q, cl, live_data, req, &t);\n+        do_task<T, GENERATION, STRING_DEDUP>(q, cl, live_data, req, &t, worker_id);\n@@ -158,0 +186,1 @@\n+  ShenandoahObjToScanQueue* old_q = get_old_queue(worker_id);\n@@ -159,1 +188,1 @@\n-  ShenandoahSATBBufferClosure drain_satb(q);\n+  ShenandoahSATBBufferClosure<GENERATION> drain_satb(q, old_q);\n@@ -169,1 +198,0 @@\n-\n@@ -178,1 +206,1 @@\n-        do_task<T, STRING_DEDUP>(q, cl, live_data, req, &t);\n+        do_task<T, GENERATION, STRING_DEDUP>(q, cl, live_data, req, &t, worker_id);\n","filename":"src\/hotspot\/share\/gc\/shenandoah\/shenandoahMark.cpp","additions":68,"deletions":40,"binary":false,"changes":108,"status":"modified"},{"patch":"@@ -3,0 +3,1 @@\n+ * Copyright Amazon.com Inc. or its affiliates. All Rights Reserved.\n@@ -28,0 +29,1 @@\n+#include \"gc\/shared\/ageTable.hpp\"\n@@ -30,1 +32,3 @@\n-#include \"gc\/shenandoah\/shenandoahOopClosures.hpp\"\n+#include \"gc\/shenandoah\/shenandoahGenerationType.hpp\"\n+#include \"gc\/shenandoah\/shenandoahHeap.hpp\"\n+#include \"gc\/shenandoah\/shenandoahGeneration.hpp\"\n@@ -33,0 +37,9 @@\n+enum StringDedupMode {\n+  NO_DEDUP,      \/\/ Do not do anything for String deduplication\n+  ENQUEUE_DEDUP, \/\/ Enqueue candidate Strings for deduplication, if meet age threshold\n+  ALWAYS_DEDUP   \/\/ Enqueue Strings for deduplication\n+};\n+\n+class ShenandoahMarkingContext;\n+class ShenandoahReferenceProcessor;\n+\n@@ -38,0 +51,1 @@\n+  ShenandoahGeneration* const _generation;\n@@ -39,0 +53,1 @@\n+  ShenandoahObjToScanQueueSet* const _old_gen_task_queues;\n@@ -41,1 +56,1 @@\n-  ShenandoahMark();\n+  ShenandoahMark(ShenandoahGeneration* generation);\n@@ -44,4 +59,2 @@\n-  template<class T>\n-  static inline void mark_through_ref(T* p, ShenandoahObjToScanQueue* q, ShenandoahMarkingContext* const mark_context, bool weak);\n-\n-  static void clear();\n+  template<class T, ShenandoahGenerationType GENERATION>\n+  static inline void mark_through_ref(T* p, ShenandoahObjToScanQueue* q, ShenandoahObjToScanQueue* old_q, ShenandoahMarkingContext* const mark_context, bool weak);\n@@ -55,0 +68,4 @@\n+  ShenandoahObjToScanQueueSet* old_task_queues() {\n+    return _old_gen_task_queues;\n+  }\n+\n@@ -56,0 +73,3 @@\n+  inline ShenandoahObjToScanQueue* get_old_queue(uint index) const;\n+\n+  inline ShenandoahGeneration* generation() { return _generation; };\n@@ -57,1 +77,0 @@\n-\/\/ ---------- Marking loop and tasks\n@@ -59,2 +78,4 @@\n-  template <class T, StringDedupMode STRING_DEDUP>\n-  inline void do_task(ShenandoahObjToScanQueue* q, T* cl, ShenandoahLiveData* live_data, StringDedup::Requests* const req, ShenandoahMarkTask* task);\n+\/\/ ---------- Marking loop and tasks\n+\n+  template <class T, ShenandoahGenerationType GENERATION, StringDedupMode STRING_DEDUP>\n+  inline void do_task(ShenandoahObjToScanQueue* q, T* cl, ShenandoahLiveData* live_data, StringDedup::Requests* const req, ShenandoahMarkTask* task, uint worker_id);\n@@ -68,1 +89,2 @@\n-  inline void count_liveness(ShenandoahLiveData* live_data, oop obj);\n+  template <ShenandoahGenerationType GENERATION>\n+  inline void count_liveness(ShenandoahLiveData* live_data, oop obj, uint worker_id);\n@@ -70,1 +92,1 @@\n-  template <class T, bool CANCELLABLE,StringDedupMode STRING_DEDUP>\n+  template <class T, ShenandoahGenerationType GENERATION, bool CANCELLABLE, StringDedupMode STRING_DEDUP>\n@@ -73,2 +95,12 @@\n-  template <bool CANCELLABLE, StringDedupMode STRING_DEDUP>\n-  void mark_loop_prework(uint worker_id, TaskTerminator *terminator, ShenandoahReferenceProcessor *rp, StringDedup::Requests* const req);\n+  template <ShenandoahGenerationType GENERATION, bool CANCELLABLE, StringDedupMode STRING_DEDUP>\n+  void mark_loop_prework(uint worker_id, TaskTerminator *terminator, ShenandoahReferenceProcessor *rp, StringDedup::Requests* const req, bool update_refs);\n+\n+  template <ShenandoahGenerationType GENERATION>\n+  static bool in_generation(ShenandoahHeap* const heap, oop obj);\n+\n+  template <class T>\n+  static void mark_non_generational_ref(T *p, ShenandoahObjToScanQueue* q, ShenandoahMarkingContext* const mark_context, bool weak);\n+\n+  static void mark_ref(ShenandoahObjToScanQueue* q,\n+                       ShenandoahMarkingContext* const mark_context,\n+                       bool weak, oop obj);\n@@ -79,0 +111,4 @@\n+  template<bool CANCELLABLE, StringDedupMode STRING_DEDUP>\n+  void mark_loop(uint worker_id, TaskTerminator* terminator, ShenandoahReferenceProcessor *rp,\n+                 ShenandoahGenerationType generation, StringDedup::Requests* const req);\n+\n@@ -80,1 +116,1 @@\n-                 bool cancellable, StringDedupMode dedup_mode, StringDedup::Requests* const req);\n+                 ShenandoahGenerationType generation, bool cancellable, StringDedupMode dedup_mode, StringDedup::Requests* const req);\n@@ -84,1 +120,0 @@\n-\n","filename":"src\/hotspot\/share\/gc\/shenandoah\/shenandoahMark.hpp","additions":50,"deletions":15,"binary":false,"changes":65,"status":"modified"},{"patch":"@@ -3,0 +3,1 @@\n+ * Copyright Amazon.com Inc. or its affiliates. All Rights Reserved.\n@@ -31,0 +32,1 @@\n+#include \"gc\/shenandoah\/shenandoahAgeCensus.hpp\"\n@@ -35,0 +37,3 @@\n+#include \"gc\/shenandoah\/shenandoahOldGeneration.hpp\"\n+#include \"gc\/shenandoah\/shenandoahOopClosures.inline.hpp\"\n+#include \"gc\/shenandoah\/shenandoahScanRemembered.inline.hpp\"\n@@ -59,2 +64,2 @@\n-template <class T, StringDedupMode STRING_DEDUP>\n-void ShenandoahMark::do_task(ShenandoahObjToScanQueue* q, T* cl, ShenandoahLiveData* live_data, StringDedup::Requests* const req, ShenandoahMarkTask* task) {\n+template <class T, ShenandoahGenerationType GENERATION, StringDedupMode STRING_DEDUP>\n+void ShenandoahMark::do_task(ShenandoahObjToScanQueue* q, T* cl, ShenandoahLiveData* live_data, StringDedup::Requests* const req, ShenandoahMarkTask* task, uint worker_id) {\n@@ -97,1 +102,1 @@\n-      count_liveness(live_data, obj);\n+      count_liveness<GENERATION>(live_data, obj, worker_id);\n@@ -105,5 +110,18 @@\n-inline void ShenandoahMark::count_liveness(ShenandoahLiveData* live_data, oop obj) {\n-  ShenandoahHeap* const heap = ShenandoahHeap::heap();\n-  size_t region_idx = heap->heap_region_index_containing(obj);\n-  ShenandoahHeapRegion* region = heap->get_region(region_idx);\n-  size_t size = obj->size();\n+template <ShenandoahGenerationType GENERATION>\n+inline void ShenandoahMark::count_liveness(ShenandoahLiveData* live_data, oop obj, uint worker_id) {\n+  const ShenandoahHeap* const heap = ShenandoahHeap::heap();\n+  const size_t region_idx = heap->heap_region_index_containing(obj);\n+  ShenandoahHeapRegion* const region = heap->get_region(region_idx);\n+  const size_t size = obj->size();\n+\n+  \/\/ Age census for objects in the young generation\n+  if (GENERATION == YOUNG || (GENERATION == GLOBAL && region->is_young())) {\n+    assert(heap->mode()->is_generational(), \"Only if generational\");\n+    if (ShenandoahGenerationalAdaptiveTenuring && !ShenandoahGenerationalCensusAtEvac) {\n+      assert(region->is_young(), \"Only for young objects\");\n+      uint age = ShenandoahHeap::get_object_age(obj);\n+      ShenandoahAgeCensus* const census = ShenandoahGenerationalHeap::heap()->age_census();\n+      CENSUS_NOISE(census->add(age, region->age(), region->youth(), size, worker_id);)\n+      NO_CENSUS_NOISE(census->add(age, region->age(), size, worker_id);)\n+    }\n+  }\n@@ -113,0 +131,1 @@\n+    assert(region->is_affiliated(), \"Do not count live data within Free Regular Region \" SIZE_FORMAT, region_idx);\n@@ -127,0 +146,1 @@\n+    assert(region->is_affiliated(), \"Do not count live data within FREE Humongous Start Region \" SIZE_FORMAT, region_idx);\n@@ -130,0 +150,1 @@\n+      assert(chain_reg->is_affiliated(), \"Do not count live data within FREE Humongous Continuation Region \" SIZE_FORMAT, i);\n@@ -232,0 +253,1 @@\n+template <ShenandoahGenerationType GENERATION>\n@@ -235,0 +257,1 @@\n+  ShenandoahObjToScanQueue* _old_queue;\n@@ -238,1 +261,1 @@\n-  ShenandoahSATBBufferClosure(ShenandoahObjToScanQueue* q) :\n+  ShenandoahSATBBufferClosure(ShenandoahObjToScanQueue* q, ShenandoahObjToScanQueue* old_q) :\n@@ -240,0 +263,1 @@\n+    _old_queue(old_q),\n@@ -246,1 +270,1 @@\n-    assert(size == 0 || !_heap->has_forwarded_objects(), \"Forwarded objects are not expected here\");\n+    assert(size == 0 || !_heap->has_forwarded_objects() || _heap->is_concurrent_old_mark_in_progress(), \"Forwarded objects are not expected here\");\n@@ -249,1 +273,1 @@\n-      ShenandoahMark::mark_through_ref<oop>(p, _queue, _mark_context, false);\n+      ShenandoahMark::mark_through_ref<oop, GENERATION>(p, _queue, _old_queue, _mark_context, false);\n@@ -254,2 +278,21 @@\n-template<class T>\n-inline void ShenandoahMark::mark_through_ref(T* p, ShenandoahObjToScanQueue* q, ShenandoahMarkingContext* const mark_context, bool weak) {\n+template<ShenandoahGenerationType GENERATION>\n+bool ShenandoahMark::in_generation(ShenandoahHeap* const heap, oop obj) {\n+  \/\/ Each in-line expansion of in_generation() resolves GENERATION at compile time.\n+  if (GENERATION == YOUNG) {\n+    return heap->is_in_young(obj);\n+  }\n+\n+  if (GENERATION == OLD) {\n+    return heap->is_in_old(obj);\n+  }\n+\n+  assert((GENERATION == GLOBAL || GENERATION == NON_GEN), \"Unexpected generation type\");\n+  assert(heap->is_in(obj), \"Object must be in heap\");\n+  return true;\n+}\n+\n+template<class T, ShenandoahGenerationType GENERATION>\n+inline void ShenandoahMark::mark_through_ref(T *p, ShenandoahObjToScanQueue* q, ShenandoahObjToScanQueue* old_q, ShenandoahMarkingContext* const mark_context, bool weak) {\n+  \/\/ Note: This is a very hot code path, so the code should be conditional on GENERATION template\n+  \/\/ parameter where possible, in order to generate the most efficient code.\n+\n@@ -260,0 +303,44 @@\n+    ShenandoahGenerationalHeap* heap = ShenandoahGenerationalHeap::heap();\n+    shenandoah_assert_not_forwarded(p, obj);\n+    shenandoah_assert_not_in_cset_except(p, obj, heap->cancelled_gc());\n+    if (in_generation<GENERATION>(heap, obj)) {\n+      mark_ref(q, mark_context, weak, obj);\n+      shenandoah_assert_marked(p, obj);\n+      if (GENERATION == YOUNG && heap->is_in_old(p)) {\n+        \/\/ Mark card as dirty because remembered set scanning still finds interesting pointer.\n+        heap->old_generation()->mark_card_as_dirty((HeapWord*)p);\n+      } else if (GENERATION == GLOBAL && heap->is_in_old(p) && heap->is_in_young(obj)) {\n+        \/\/ Mark card as dirty because GLOBAL marking finds interesting pointer.\n+        heap->old_generation()->mark_card_as_dirty((HeapWord*)p);\n+      }\n+    } else if (old_q != nullptr) {\n+      \/\/ Young mark, bootstrapping old_q or concurrent with old_q marking.\n+      mark_ref(old_q, mark_context, weak, obj);\n+      shenandoah_assert_marked(p, obj);\n+    } else if (GENERATION == OLD) {\n+      \/\/ Old mark, found a young pointer.\n+      if (heap->is_in(p)) {\n+        assert(heap->is_in_young(obj), \"Expected young object.\");\n+        heap->old_generation()->mark_card_as_dirty(p);\n+      }\n+    }\n+  }\n+}\n+\n+template<>\n+inline void ShenandoahMark::mark_through_ref<oop, ShenandoahGenerationType::NON_GEN>(oop *p, ShenandoahObjToScanQueue* q, ShenandoahObjToScanQueue* old_q, ShenandoahMarkingContext* const mark_context, bool weak) {\n+  mark_non_generational_ref(p, q, mark_context, weak);\n+}\n+\n+template<>\n+inline void ShenandoahMark::mark_through_ref<narrowOop, ShenandoahGenerationType::NON_GEN>(narrowOop *p, ShenandoahObjToScanQueue* q, ShenandoahObjToScanQueue* old_q, ShenandoahMarkingContext* const mark_context, bool weak) {\n+  mark_non_generational_ref(p, q, mark_context, weak);\n+}\n+\n+template<class T>\n+inline void ShenandoahMark::mark_non_generational_ref(T* p, ShenandoahObjToScanQueue* q,\n+                                                      ShenandoahMarkingContext* const mark_context, bool weak) {\n+  oop o = RawAccess<>::oop_load(p);\n+  if (!CompressedOops::is_null(o)) {\n+    oop obj = CompressedOops::decode_not_null(o);\n+\n@@ -263,11 +350,1 @@\n-    bool skip_live = false;\n-    bool marked;\n-    if (weak) {\n-      marked = mark_context->mark_weak(obj);\n-    } else {\n-      marked = mark_context->mark_strong(obj, \/* was_upgraded = *\/ skip_live);\n-    }\n-    if (marked) {\n-      bool pushed = q->push(ShenandoahMarkTask(obj, skip_live, weak));\n-      assert(pushed, \"overflow queue should always succeed pushing\");\n-    }\n+    mark_ref(q, mark_context, weak, obj);\n@@ -279,0 +356,16 @@\n+inline void ShenandoahMark::mark_ref(ShenandoahObjToScanQueue* q,\n+                              ShenandoahMarkingContext* const mark_context,\n+                              bool weak, oop obj) {\n+  bool skip_live = false;\n+  bool marked;\n+  if (weak) {\n+    marked = mark_context->mark_weak(obj);\n+  } else {\n+    marked = mark_context->mark_strong(obj, \/* was_upgraded = *\/ skip_live);\n+  }\n+  if (marked) {\n+    bool pushed = q->push(ShenandoahMarkTask(obj, skip_live, weak));\n+    assert(pushed, \"overflow queue should always succeed pushing\");\n+  }\n+}\n+\n@@ -286,0 +379,8 @@\n+\n+ShenandoahObjToScanQueue* ShenandoahMark::get_old_queue(uint index) const {\n+  if (_old_gen_task_queues != nullptr) {\n+    return _old_gen_task_queues->queue(index);\n+  }\n+  return nullptr;\n+}\n+\n","filename":"src\/hotspot\/share\/gc\/shenandoah\/shenandoahMark.inline.hpp","additions":125,"deletions":24,"binary":false,"changes":149,"status":"modified"},{"patch":"@@ -4,0 +4,1 @@\n+ * Copyright Amazon.com Inc. or its affiliates. All Rights Reserved.\n@@ -46,0 +47,14 @@\n+bool ShenandoahMarkBitMap::is_bitmap_clear_range(const HeapWord* start, const HeapWord* end) const {\n+  \/\/ Similar to get_next_marked_addr(), without assertion.\n+  \/\/ Round addr up to a possible object boundary to be safe.\n+  if (start == end) {\n+    return true;\n+  }\n+  size_t const addr_offset = address_to_index(align_up(start, HeapWordSize << LogMinObjAlignment));\n+  size_t const limit_offset = address_to_index(end);\n+  size_t const next_offset = get_next_one_offset(addr_offset, limit_offset);\n+  HeapWord* result = index_to_address(next_offset);\n+  return (result == end);\n+}\n+\n+\n@@ -48,0 +63,5 @@\n+#ifdef ASSERT\n+  ShenandoahHeap* heap = ShenandoahHeap::heap();\n+  ShenandoahHeapRegion* r = heap->heap_region_containing(addr);\n+  ShenandoahMarkingContext* ctx = heap->marking_context();\n+  HeapWord* tams = ctx->top_at_mark_start(r);\n@@ -49,0 +69,4 @@\n+  assert(limit <= r->top(), \"limit must be less than top\");\n+  assert(addr <= tams, \"addr must be less than TAMS\");\n+#endif\n+\n","filename":"src\/hotspot\/share\/gc\/shenandoah\/shenandoahMarkBitMap.cpp","additions":24,"deletions":0,"binary":false,"changes":24,"status":"modified"},{"patch":"@@ -4,0 +4,1 @@\n+ * Copyright Amazon.com Inc. or its affiliates. All Rights Reserved.\n@@ -26,2 +27,2 @@\n-#ifndef SHARE_VM_GC_SHENANDOAH_SHENANDOAHMARKBITMAP_HPP\n-#define SHARE_VM_GC_SHENANDOAH_SHENANDOAHMARKBITMAP_HPP\n+#ifndef SHARE_GC_SHENANDOAH_SHENANDOAHMARKBITMAP_HPP\n+#define SHARE_GC_SHENANDOAH_SHENANDOAHMARKBITMAP_HPP\n@@ -162,0 +163,2 @@\n+  bool is_bitmap_clear_range(const HeapWord* start, const HeapWord* end) const;\n+\n@@ -179,1 +182,1 @@\n-#endif \/\/ SHARE_VM_GC_SHENANDOAH_SHENANDOAHMARKBITMAP_HPP\n+#endif \/\/ SHARE_GC_SHENANDOAH_SHENANDOAHMARKBITMAP_HPP\n","filename":"src\/hotspot\/share\/gc\/shenandoah\/shenandoahMarkBitMap.hpp","additions":6,"deletions":3,"binary":false,"changes":9,"status":"modified"},{"patch":"@@ -26,2 +26,2 @@\n-#ifndef SHARE_VM_GC_SHENANDOAH_SHENANDOAHMARKBITMAP_INLINE_HPP\n-#define SHARE_VM_GC_SHENANDOAH_SHENANDOAHMARKBITMAP_INLINE_HPP\n+#ifndef SHARE_GC_SHENANDOAH_SHENANDOAHMARKBITMAP_INLINE_HPP\n+#define SHARE_GC_SHENANDOAH_SHENANDOAHMARKBITMAP_INLINE_HPP\n@@ -208,1 +208,1 @@\n-#endif \/\/ SHARE_VM_GC_SHENANDOAH_SHENANDOAHMARKBITMAP_INLINE_HPP\n+#endif \/\/ SHARE_GC_SHENANDOAH_SHENANDOAHMARKBITMAP_INLINE_HPP\n","filename":"src\/hotspot\/share\/gc\/shenandoah\/shenandoahMarkBitMap.inline.hpp","additions":3,"deletions":3,"binary":false,"changes":6,"status":"modified"},{"patch":"@@ -3,0 +3,1 @@\n+ * Copyright Amazon.com Inc. or its affiliates. All Rights Reserved.\n@@ -28,1 +29,0 @@\n-#include \"gc\/shenandoah\/shenandoahHeapRegion.inline.hpp\"\n@@ -30,2 +30,0 @@\n-#include \"gc\/shenandoah\/shenandoahTaskqueue.inline.hpp\"\n-#include \"utilities\/stack.inline.hpp\"\n@@ -33,1 +31,3 @@\n-ShenandoahMarkingContext::ShenandoahMarkingContext(MemRegion heap_region, MemRegion bitmap_region, size_t num_regions, uint max_queues) :\n+#include \"shenandoahGlobalGeneration.hpp\"\n+\n+ShenandoahMarkingContext::ShenandoahMarkingContext(MemRegion heap_region, MemRegion bitmap_region, size_t num_regions) :\n@@ -38,15 +38,1 @@\n-                      ((uintx) heap_region.start() >> ShenandoahHeapRegion::region_size_bytes_shift())),\n-  _task_queues(new ShenandoahObjToScanQueueSet(max_queues)) {\n-  assert(max_queues > 0, \"At least one queue\");\n-  for (uint i = 0; i < max_queues; ++i) {\n-    ShenandoahObjToScanQueue* task_queue = new ShenandoahObjToScanQueue();\n-    _task_queues->register_queue(i, task_queue);\n-  }\n-}\n-\n-ShenandoahMarkingContext::~ShenandoahMarkingContext() {\n-  for (uint i = 0; i < _task_queues->size(); ++i) {\n-    ShenandoahObjToScanQueue* q = _task_queues->queue(i);\n-    delete q;\n-  }\n-  delete _task_queues;\n+                      ((uintx) heap_region.start() >> ShenandoahHeapRegion::region_size_bytes_shift())) {\n@@ -60,1 +46,2 @@\n-    if (heap->is_bitmap_slice_committed(r) && !is_bitmap_clear_range(r->bottom(), r->end())) {\n+    if (r->is_affiliated() && heap->is_bitmap_slice_committed(r)\n+        && !is_bitmap_range_within_region_clear(r->bottom(), r->end())) {\n@@ -67,2 +54,15 @@\n-bool ShenandoahMarkingContext::is_bitmap_clear_range(HeapWord* start, HeapWord* end) const {\n-  return _mark_bit_map.get_next_marked_addr(start, end) == end;\n+bool ShenandoahMarkingContext::is_bitmap_range_within_region_clear(const HeapWord* start, const HeapWord* end) const {\n+  assert(start <= end, \"Invalid start \" PTR_FORMAT \" and end \" PTR_FORMAT, p2i(start), p2i(end));\n+  if (start < end) {\n+    ShenandoahHeap* heap = ShenandoahHeap::heap();\n+    size_t start_idx = heap->heap_region_index_containing(start);\n+#ifdef ASSERT\n+    size_t end_idx = heap->heap_region_index_containing(end - 1);\n+    assert(start_idx == end_idx, \"Expected range to be within same region (\" SIZE_FORMAT \", \" SIZE_FORMAT \")\", start_idx, end_idx);\n+#endif\n+    ShenandoahHeapRegion* r = heap->get_region(start_idx);\n+    if (!heap->is_bitmap_slice_committed(r)) {\n+      return true;\n+    }\n+  }\n+  return _mark_bit_map.is_bitmap_clear_range(start, end);\n@@ -74,0 +74,1 @@\n+\n@@ -76,0 +77,7 @@\n+\n+  log_debug(gc)(\"SMC:initialize_top_at_mark_start for Region \" SIZE_FORMAT \", TAMS: \" PTR_FORMAT \", TopOfBitMap: \" PTR_FORMAT,\n+                r->index(), p2i(bottom), p2i(r->end()));\n+}\n+\n+HeapWord* ShenandoahMarkingContext::top_bitmap(ShenandoahHeapRegion* r) {\n+  return _top_bitmaps[r->index()];\n@@ -81,0 +89,4 @@\n+\n+  log_debug(gc)(\"SMC:clear_bitmap for %s Region \" SIZE_FORMAT \", top_bitmap: \" PTR_FORMAT,\n+                r->affiliation_name(), r->index(), p2i(top_bitmap));\n+\n@@ -85,3 +97,0 @@\n-  assert(is_bitmap_clear_range(bottom, r->end()),\n-         \"Region \" SIZE_FORMAT \" should have no marks in bitmap\", r->index());\n-}\n@@ -89,10 +98,2 @@\n-bool ShenandoahMarkingContext::is_complete() {\n-  return _is_complete.is_set();\n-}\n-\n-void ShenandoahMarkingContext::mark_complete() {\n-  _is_complete.set();\n-}\n-\n-void ShenandoahMarkingContext::mark_incomplete() {\n-  _is_complete.unset();\n+  assert(is_bitmap_range_within_region_clear(bottom, r->end()),\n+         \"Region \" SIZE_FORMAT \" should have no marks in bitmap\", r->index());\n","filename":"src\/hotspot\/share\/gc\/shenandoah\/shenandoahMarkingContext.cpp","additions":36,"deletions":35,"binary":false,"changes":71,"status":"modified"},{"patch":"@@ -3,0 +3,1 @@\n+ * Copyright Amazon.com Inc. or its affiliates. All Rights Reserved.\n@@ -35,0 +36,1 @@\n+class ShenandoahHeapRegion;\n@@ -48,5 +50,0 @@\n-  ShenandoahSharedFlag _is_complete;\n-\n-  \/\/ Marking task queues\n-  ShenandoahObjToScanQueueSet* _task_queues;\n-\n@@ -54,2 +51,1 @@\n-  ShenandoahMarkingContext(MemRegion heap_region, MemRegion bitmap_region, size_t num_regions, uint max_queues);\n-  ~ShenandoahMarkingContext();\n+  ShenandoahMarkingContext(MemRegion heap_region, MemRegion bitmap_region, size_t num_regions);\n@@ -71,0 +67,2 @@\n+  inline bool is_marked_or_old(oop obj) const;\n+  inline bool is_marked_strong_or_old(oop obj) const;\n@@ -72,1 +70,1 @@\n-  inline HeapWord* get_next_marked_addr(HeapWord* addr, HeapWord* limit) const;\n+  inline HeapWord* get_next_marked_addr(const HeapWord* addr, const HeapWord* limit) const;\n@@ -75,1 +73,1 @@\n-  inline bool allocated_after_mark_start(HeapWord* addr) const;\n+  inline bool allocated_after_mark_start(const HeapWord* addr) const;\n@@ -77,1 +75,1 @@\n-  inline HeapWord* top_at_mark_start(ShenandoahHeapRegion* r) const;\n+  inline HeapWord* top_at_mark_start(const ShenandoahHeapRegion* r) const;\n@@ -82,0 +80,2 @@\n+  HeapWord* top_bitmap(ShenandoahHeapRegion* r);\n+\n@@ -86,8 +86,1 @@\n-  bool is_bitmap_clear_range(HeapWord* start, HeapWord* end) const;\n-\n-  bool is_complete();\n-  void mark_complete();\n-  void mark_incomplete();\n-\n-  \/\/ Task queues\n-  ShenandoahObjToScanQueueSet* task_queues() const { return _task_queues; }\n+  bool is_bitmap_range_within_region_clear(const HeapWord* start, const HeapWord* end) const;\n","filename":"src\/hotspot\/share\/gc\/shenandoah\/shenandoahMarkingContext.hpp","additions":11,"deletions":18,"binary":false,"changes":29,"status":"modified"},{"patch":"@@ -3,0 +3,1 @@\n+ * Copyright Amazon.com Inc. or its affiliates. All Rights Reserved.\n@@ -29,1 +30,0 @@\n-\n@@ -31,0 +31,1 @@\n+#include \"logging\/log.hpp\"\n@@ -60,1 +61,9 @@\n-inline HeapWord* ShenandoahMarkingContext::get_next_marked_addr(HeapWord* start, HeapWord* limit) const {\n+inline bool ShenandoahMarkingContext::is_marked_or_old(oop obj) const {\n+  return is_marked(obj) || ShenandoahHeap::heap()->is_in_old_during_young_collection(obj);\n+}\n+\n+inline bool ShenandoahMarkingContext::is_marked_strong_or_old(oop obj) const {\n+  return is_marked_strong(obj) || ShenandoahHeap::heap()->is_in_old_during_young_collection(obj);\n+}\n+\n+inline HeapWord* ShenandoahMarkingContext::get_next_marked_addr(const HeapWord* start, const HeapWord* limit) const {\n@@ -65,1 +74,1 @@\n-  HeapWord* addr = cast_from_oop<HeapWord*>(obj);\n+  const HeapWord* addr = cast_from_oop<HeapWord*>(obj);\n@@ -69,1 +78,1 @@\n-inline bool ShenandoahMarkingContext::allocated_after_mark_start(HeapWord* addr) const {\n+inline bool ShenandoahMarkingContext::allocated_after_mark_start(const HeapWord* addr) const {\n@@ -72,1 +81,1 @@\n-  bool alloc_after_mark_start = addr >= top_at_mark_start;\n+  const bool alloc_after_mark_start = addr >= top_at_mark_start;\n@@ -77,0 +86,5 @@\n+  if (!r->is_affiliated()) {\n+    \/\/ Non-affiliated regions do not need their TAMS updated\n+    return;\n+  }\n+\n@@ -84,1 +98,4 @@\n-  assert(is_bitmap_clear_range(old_tams, new_tams),\n+  assert((new_tams == r->bottom()) || (old_tams == r->bottom()) || (new_tams >= _top_bitmaps[idx]),\n+         \"Region \" SIZE_FORMAT\", top_bitmaps updates should be monotonic: \" PTR_FORMAT \" -> \" PTR_FORMAT,\n+         idx, p2i(_top_bitmaps[idx]), p2i(new_tams));\n+  assert(old_tams == r->bottom() || is_bitmap_range_within_region_clear(old_tams, new_tams),\n@@ -88,0 +105,3 @@\n+  log_debug(gc)(\"Capturing TAMS for %s Region \" SIZE_FORMAT \", was: \" PTR_FORMAT \", now: \" PTR_FORMAT,\n+                r->affiliation_name(), idx, p2i(old_tams), p2i(new_tams));\n+\n@@ -96,1 +116,1 @@\n-inline HeapWord* ShenandoahMarkingContext::top_at_mark_start(ShenandoahHeapRegion* r) const {\n+inline HeapWord* ShenandoahMarkingContext::top_at_mark_start(const ShenandoahHeapRegion* r) const {\n@@ -101,1 +121,1 @@\n-  assert(is_bitmap_clear_range(r->bottom(), r->end()),\n+  assert(is_bitmap_range_within_region_clear(r->bottom(), r->end()),\n","filename":"src\/hotspot\/share\/gc\/shenandoah\/shenandoahMarkingContext.inline.hpp","additions":28,"deletions":8,"binary":false,"changes":36,"status":"modified"},{"patch":"@@ -3,0 +3,1 @@\n+ * Copyright Amazon.com Inc. or its affiliates. All Rights Reserved.\n@@ -27,0 +28,2 @@\n+#include \"gc\/shenandoah\/shenandoahYoungGeneration.hpp\"\n+#include \"gc\/shenandoah\/shenandoahOldGeneration.hpp\"\n@@ -28,2 +31,3 @@\n-ShenandoahMemoryPool::ShenandoahMemoryPool(ShenandoahHeap* heap) :\n-        CollectedMemoryPool(\"Shenandoah\",\n+ShenandoahMemoryPool::ShenandoahMemoryPool(ShenandoahHeap* heap,\n+                                           const char* name) :\n+        CollectedMemoryPool(name,\n@@ -35,0 +39,11 @@\n+ShenandoahMemoryPool::ShenandoahMemoryPool(ShenandoahHeap* heap,\n+                                           const char* name,\n+                                           size_t initial_capacity,\n+                                           size_t max_capacity) :\n+        CollectedMemoryPool(name,\n+                            initial_capacity,\n+                            max_capacity,\n+                            true \/* support_usage_threshold *\/),\n+                            _heap(heap) {}\n+\n+\n@@ -54,0 +69,36 @@\n+\n+size_t ShenandoahMemoryPool::used_in_bytes() {\n+  return _heap->used();\n+}\n+\n+size_t ShenandoahMemoryPool::max_size() const {\n+  return _heap->max_capacity();\n+}\n+\n+ShenandoahGenerationalMemoryPool::ShenandoahGenerationalMemoryPool(ShenandoahHeap* heap, const char* name,\n+                                                                   ShenandoahGeneration* generation) :\n+        ShenandoahMemoryPool(heap, name, 0, heap->max_capacity()),\n+        _generation(generation) { }\n+\n+MemoryUsage ShenandoahGenerationalMemoryPool::get_memory_usage() {\n+  size_t initial   = initial_size();\n+  size_t max       = max_size();\n+  size_t used      = used_in_bytes();\n+  size_t committed = _generation->used_regions_size();\n+\n+  return MemoryUsage(initial, used, committed, max);\n+}\n+\n+size_t ShenandoahGenerationalMemoryPool::used_in_bytes() {\n+  return _generation->used();\n+}\n+\n+ShenandoahYoungGenMemoryPool::ShenandoahYoungGenMemoryPool(ShenandoahHeap* heap) :\n+        ShenandoahGenerationalMemoryPool(heap,\n+                             \"Shenandoah Young Gen\",\n+                             heap->young_generation()) { }\n+\n+ShenandoahOldGenMemoryPool::ShenandoahOldGenMemoryPool(ShenandoahHeap* heap) :\n+        ShenandoahGenerationalMemoryPool(heap,\n+                             \"Shenandoah Old Gen\",\n+                             heap->old_generation()) { }\n","filename":"src\/hotspot\/share\/gc\/shenandoah\/shenandoahMemoryPool.cpp","additions":53,"deletions":2,"binary":false,"changes":55,"status":"modified"},{"patch":"@@ -3,0 +3,1 @@\n+ * Copyright Amazon.com Inc. or its affiliates. All Rights Reserved.\n@@ -28,1 +29,0 @@\n-#ifndef SERIALGC\n@@ -32,1 +32,0 @@\n-#endif\n@@ -35,1 +34,1 @@\n-private:\n+protected:\n@@ -39,4 +38,30 @@\n-  ShenandoahMemoryPool(ShenandoahHeap* pool);\n-  MemoryUsage get_memory_usage();\n-  size_t used_in_bytes()              { return _heap->used(); }\n-  size_t max_size() const             { return _heap->max_capacity(); }\n+  explicit ShenandoahMemoryPool(ShenandoahHeap* heap,\n+                       const char* name = \"Shenandoah\");\n+  MemoryUsage get_memory_usage() override;\n+  size_t used_in_bytes() override;\n+  size_t max_size() const override;\n+\n+protected:\n+  ShenandoahMemoryPool(ShenandoahHeap* heap,\n+                       const char* name,\n+                       size_t initial_capacity,\n+                       size_t max_capacity);\n+};\n+\n+class ShenandoahGenerationalMemoryPool: public ShenandoahMemoryPool {\n+private:\n+  ShenandoahGeneration* _generation;\n+public:\n+  explicit ShenandoahGenerationalMemoryPool(ShenandoahHeap* heap, const char* name, ShenandoahGeneration* generation);\n+  MemoryUsage get_memory_usage() override;\n+  size_t used_in_bytes() override;\n+};\n+\n+class ShenandoahYoungGenMemoryPool : public ShenandoahGenerationalMemoryPool {\n+public:\n+  explicit ShenandoahYoungGenMemoryPool(ShenandoahHeap* heap);\n+};\n+\n+class ShenandoahOldGenMemoryPool : public ShenandoahGenerationalMemoryPool {\n+public:\n+  explicit ShenandoahOldGenMemoryPool(ShenandoahHeap* heap);\n","filename":"src\/hotspot\/share\/gc\/shenandoah\/shenandoahMemoryPool.hpp","additions":32,"deletions":7,"binary":false,"changes":39,"status":"modified"},{"patch":"@@ -46,1 +46,13 @@\n-bool ShenandoahMetricsSnapshot::is_good_progress() {\n+\/\/ For degenerated GC, generation is Young in generational mode, Global in non-generational mode.\n+\/\/ For full GC, generation is always Global.\n+\/\/\n+\/\/ Note that the size of the chosen collection set is proportional to the relevant generation's collection set.\n+\/\/ Note also that the generation size may change following selection of the collection set, as a side effect\n+\/\/ of evacuation.  Evacuation may promote objects, causing old to grow and young to shrink.  Or this may be a\n+\/\/ mixed evacuation.  When old regions are evacuated, this typically allows young to expand.  In all of these\n+\/\/ various scenarios, the purpose of asking is_good_progress() is to determine if there is enough memory available\n+\/\/ within young generation to justify making an attempt to perform a concurrent collection.  For this reason, we'll\n+\/\/ use the current size of the generation (which may not be different than when the collection set was chosen) to\n+\/\/ assess how much free memory we require in order to consider the most recent GC to have had good progress.\n+\n+bool ShenandoahMetricsSnapshot::is_good_progress(ShenandoahGeneration* generation) {\n@@ -48,2 +60,8 @@\n-  size_t free_actual   = _heap->free_set()->available();\n-  size_t free_expected = _heap->max_capacity() \/ 100 * ShenandoahCriticalFreeThreshold;\n+  ShenandoahFreeSet* free_set = _heap->free_set();\n+  size_t free_actual   = free_set->available();\n+\n+  \/\/ ShenandoahCriticalFreeThreshold is expressed as a percentage.  We multiple this percentage by 1\/100th\n+  \/\/ of the generation capacity to determine whether the available memory within the generation exceeds the\n+  \/\/ critical threshold.\n+  size_t free_expected = (ShenandoahHeap::heap()->soft_max_capacity() \/ 100) * ShenandoahCriticalFreeThreshold;\n+\n","filename":"src\/hotspot\/share\/gc\/shenandoah\/shenandoahMetrics.cpp","additions":21,"deletions":3,"binary":false,"changes":24,"status":"modified"},{"patch":"@@ -43,1 +43,1 @@\n-  bool is_good_progress();\n+  bool is_good_progress(ShenandoahGeneration *generation);\n","filename":"src\/hotspot\/share\/gc\/shenandoah\/shenandoahMetrics.hpp","additions":1,"deletions":1,"binary":false,"changes":2,"status":"modified"},{"patch":"@@ -0,0 +1,189 @@\n+\/*\n+ * Copyright Amazon.com Inc. or its affiliates. All Rights Reserved.\n+ * DO NOT ALTER OR REMOVE COPYRIGHT NOTICES OR THIS FILE HEADER.\n+ *\n+ * This code is free software; you can redistribute it and\/or modify it\n+ * under the terms of the GNU General Public License version 2 only, as\n+ * published by the Free Software Foundation.\n+ *\n+ * This code is distributed in the hope that it will be useful, but WITHOUT\n+ * ANY WARRANTY; without even the implied warranty of MERCHANTABILITY or\n+ * FITNESS FOR A PARTICULAR PURPOSE.  See the GNU General Public License\n+ * version 2 for more details (a copy is included in the LICENSE file that\n+ * accompanied this code).\n+ *\n+ * You should have received a copy of the GNU General Public License version\n+ * 2 along with this work; if not, write to the Free Software Foundation,\n+ * Inc., 51 Franklin St, Fifth Floor, Boston, MA 02110-1301 USA.\n+ *\n+ * Please contact Oracle, 500 Oracle Parkway, Redwood Shores, CA 94065 USA\n+ * or visit www.oracle.com if you need additional information or have any\n+ * questions.\n+ *\n+ *\/\n+#include \"precompiled.hpp\"\n+\n+#include \"gc\/shenandoah\/shenandoahAsserts.hpp\"\n+#include \"gc\/shenandoah\/shenandoahMmuTracker.hpp\"\n+#include \"gc\/shenandoah\/shenandoahHeap.inline.hpp\"\n+#include \"gc\/shenandoah\/shenandoahOldGeneration.hpp\"\n+#include \"gc\/shenandoah\/shenandoahYoungGeneration.hpp\"\n+#include \"logging\/log.hpp\"\n+#include \"runtime\/os.hpp\"\n+#include \"runtime\/task.hpp\"\n+\n+class ShenandoahMmuTask : public PeriodicTask {\n+  ShenandoahMmuTracker* _mmu_tracker;\n+public:\n+  explicit ShenandoahMmuTask(ShenandoahMmuTracker* mmu_tracker) :\n+    PeriodicTask(GCPauseIntervalMillis), _mmu_tracker(mmu_tracker) {}\n+\n+  void task() override {\n+    _mmu_tracker->report();\n+  }\n+};\n+\n+class ThreadTimeAccumulator : public ThreadClosure {\n+ public:\n+  size_t total_time;\n+  ThreadTimeAccumulator() : total_time(0) {}\n+  void do_thread(Thread* thread) override {\n+    assert(!thread->has_terminated(), \"Cannot get cpu time for terminated thread\");\n+    total_time += os::thread_cpu_time(thread);\n+  }\n+};\n+\n+ShenandoahMmuTracker::ShenandoahMmuTracker() :\n+    _most_recent_timestamp(0.0),\n+    _most_recent_gc_time(0.0),\n+    _most_recent_gcu(0.0),\n+    _most_recent_mutator_time(0.0),\n+    _most_recent_mu(0.0),\n+    _most_recent_periodic_time_stamp(0.0),\n+    _most_recent_periodic_gc_time(0.0),\n+    _most_recent_periodic_mutator_time(0.0),\n+    _mmu_periodic_task(new ShenandoahMmuTask(this)) {\n+}\n+\n+ShenandoahMmuTracker::~ShenandoahMmuTracker() {\n+  delete _mmu_periodic_task;\n+}\n+\n+void ShenandoahMmuTracker::fetch_cpu_times(double &gc_time, double &mutator_time) {\n+  ThreadTimeAccumulator cl;\n+  \/\/ We include only the gc threads because those are the only threads\n+  \/\/ we are responsible for.\n+  ShenandoahHeap::heap()->gc_threads_do(&cl);\n+  double most_recent_gc_thread_time = double(cl.total_time) \/ NANOSECS_PER_SEC;\n+  gc_time = most_recent_gc_thread_time;\n+\n+  double process_real_time(0.0), process_user_time(0.0), process_system_time(0.0);\n+  bool valid = os::getTimesSecs(&process_real_time, &process_user_time, &process_system_time);\n+  assert(valid, \"don't know why this would not be valid\");\n+  mutator_time =(process_user_time + process_system_time) - most_recent_gc_thread_time;\n+}\n+\n+void ShenandoahMmuTracker::update_utilization(size_t gcid, const char* msg) {\n+  double current = os::elapsedTime();\n+  _most_recent_gcid = gcid;\n+  _most_recent_is_full = false;\n+\n+  if (gcid == 0) {\n+    fetch_cpu_times(_most_recent_gc_time, _most_recent_mutator_time);\n+\n+    _most_recent_timestamp = current;\n+  } else {\n+    double gc_cycle_period = current - _most_recent_timestamp;\n+    _most_recent_timestamp = current;\n+\n+    double gc_thread_time, mutator_thread_time;\n+    fetch_cpu_times(gc_thread_time, mutator_thread_time);\n+    double gc_time = gc_thread_time - _most_recent_gc_time;\n+    _most_recent_gc_time = gc_thread_time;\n+    _most_recent_gcu = gc_time \/ (_active_processors * gc_cycle_period);\n+    double mutator_time = mutator_thread_time - _most_recent_mutator_time;\n+    _most_recent_mutator_time = mutator_thread_time;\n+    _most_recent_mu = mutator_time \/ (_active_processors * gc_cycle_period);\n+    log_info(gc, ergo)(\"At end of %s: GCU: %.1f%%, MU: %.1f%% during period of %.3fs\",\n+                       msg, _most_recent_gcu * 100, _most_recent_mu * 100, gc_cycle_period);\n+  }\n+}\n+\n+void ShenandoahMmuTracker::record_young(size_t gcid) {\n+  update_utilization(gcid, \"Concurrent Young GC\");\n+}\n+\n+void ShenandoahMmuTracker::record_global(size_t gcid) {\n+  update_utilization(gcid, \"Concurrent Global GC\");\n+}\n+\n+void ShenandoahMmuTracker::record_bootstrap(size_t gcid) {\n+  \/\/ Not likely that this will represent an \"ideal\" GCU, but doesn't hurt to try\n+  update_utilization(gcid, \"Concurrent Bootstrap GC\");\n+}\n+\n+void ShenandoahMmuTracker::record_old_marking_increment(bool old_marking_done) {\n+  \/\/ No special processing for old marking\n+  double now = os::elapsedTime();\n+  double duration = now - _most_recent_timestamp;\n+\n+  double gc_time, mutator_time;\n+  fetch_cpu_times(gc_time, mutator_time);\n+  double gcu = (gc_time - _most_recent_gc_time) \/ duration;\n+  double mu = (mutator_time - _most_recent_mutator_time) \/ duration;\n+  log_info(gc, ergo)(\"At end of %s: GCU: %.1f%%, MU: %.1f%% for duration %.3fs (totals to be subsumed in next gc report)\",\n+                     old_marking_done? \"last OLD marking increment\": \"OLD marking increment\",\n+                     gcu * 100, mu * 100, duration);\n+}\n+\n+void ShenandoahMmuTracker::record_mixed(size_t gcid) {\n+  update_utilization(gcid, \"Mixed Concurrent GC\");\n+}\n+\n+void ShenandoahMmuTracker::record_degenerated(size_t gcid, bool is_old_bootstrap) {\n+  if ((gcid == _most_recent_gcid) && _most_recent_is_full) {\n+    \/\/ Do nothing.  This is a redundant recording for the full gc that just completed.\n+  } else if (is_old_bootstrap) {\n+    update_utilization(gcid, \"Degenerated Bootstrap Old GC\");\n+  } else {\n+    update_utilization(gcid, \"Degenerated Young GC\");\n+  }\n+}\n+\n+void ShenandoahMmuTracker::record_full(size_t gcid) {\n+  update_utilization(gcid, \"Full GC\");\n+  _most_recent_is_full = true;\n+}\n+\n+void ShenandoahMmuTracker::report() {\n+  \/\/ This is only called by the periodic thread.\n+  double current = os::elapsedTime();\n+  double time_delta = current - _most_recent_periodic_time_stamp;\n+  _most_recent_periodic_time_stamp = current;\n+\n+  double gc_time, mutator_time;\n+  fetch_cpu_times(gc_time, mutator_time);\n+\n+  double gc_delta = gc_time - _most_recent_periodic_gc_time;\n+  _most_recent_periodic_gc_time = gc_time;\n+\n+  double mutator_delta = mutator_time - _most_recent_periodic_mutator_time;\n+  _most_recent_periodic_mutator_time = mutator_time;\n+\n+  double mu = mutator_delta \/ (_active_processors * time_delta);\n+  double gcu = gc_delta \/ (_active_processors * time_delta);\n+  log_debug(gc)(\"Periodic Sample: GCU = %.3f%%, MU = %.3f%% during most recent %.1fs\", gcu * 100, mu * 100, time_delta);\n+}\n+\n+void ShenandoahMmuTracker::stop() const {\n+  _mmu_periodic_task->disenroll();\n+}\n+\n+void ShenandoahMmuTracker::initialize() {\n+  \/\/ initialize static data\n+  _active_processors = os::initial_active_processor_count();\n+\n+  _most_recent_periodic_time_stamp = os::elapsedTime();\n+  fetch_cpu_times(_most_recent_periodic_gc_time, _most_recent_periodic_mutator_time);\n+  _mmu_periodic_task->enroll();\n+}\n","filename":"src\/hotspot\/share\/gc\/shenandoah\/shenandoahMmuTracker.cpp","additions":189,"deletions":0,"binary":false,"changes":189,"status":"added"},{"patch":"@@ -0,0 +1,110 @@\n+\/*\n+ * Copyright Amazon.com Inc. or its affiliates. All Rights Reserved.\n+ * DO NOT ALTER OR REMOVE COPYRIGHT NOTICES OR THIS FILE HEADER.\n+ *\n+ * This code is free software; you can redistribute it and\/or modify it\n+ * under the terms of the GNU General Public License version 2 only, as\n+ * published by the Free Software Foundation.\n+ *\n+ * This code is distributed in the hope that it will be useful, but WITHOUT\n+ * ANY WARRANTY; without even the implied warranty of MERCHANTABILITY or\n+ * FITNESS FOR A PARTICULAR PURPOSE.  See the GNU General Public License\n+ * version 2 for more details (a copy is included in the LICENSE file that\n+ * accompanied this code).\n+ *\n+ * You should have received a copy of the GNU General Public License version\n+ * 2 along with this work; if not, write to the Free Software Foundation,\n+ * Inc., 51 Franklin St, Fifth Floor, Boston, MA 02110-1301 USA.\n+ *\n+ * Please contact Oracle, 500 Oracle Parkway, Redwood Shores, CA 94065 USA\n+ * or visit www.oracle.com if you need additional information or have any\n+ * questions.\n+ *\n+ *\/\n+\n+#ifndef SHARE_GC_SHENANDOAH_SHENANDOAHMMUTRACKER_HPP\n+#define SHARE_GC_SHENANDOAH_SHENANDOAHMMUTRACKER_HPP\n+\n+#include \"utilities\/numberSeq.hpp\"\n+\n+class ShenandoahGeneration;\n+class ShenandoahMmuTask;\n+\n+\/**\n+ * This class is responsible for tracking and adjusting the minimum mutator\n+ * utilization (MMU). MMU is defined as the percentage of CPU time available\n+ * to mutator threads over an arbitrary, fixed interval of time. This interval\n+ * defaults to 5 seconds and is configured by GCPauseIntervalMillis. The class\n+ * maintains a decaying average of the last 10 values. The MMU is measured\n+ * by summing all of the time given to the GC threads and comparing this to\n+ * the total CPU time for the process. There are OS APIs to support this on\n+ * all major platforms.\n+ *\n+ * The time spent by GC threads is attributed to the young or old generation.\n+ * The time given to the controller and regulator threads is attributed to the\n+ * global generation. At the end of every collection, the average MMU is inspected.\n+ * If it is below `GCTimeRatio`, this class will attempt to increase the capacity\n+ * of the generation that is consuming the most CPU time. The assumption being\n+ * that increasing memory will reduce the collection frequency and raise the\n+ * MMU.\n+ *\/\n+class ShenandoahMmuTracker {\n+private:\n+  \/\/ These variables hold recent snapshots of cumulative quantities that are used for calculating\n+  \/\/ CPU time consumed by GC and mutator threads during each GC cycle.\n+  double _most_recent_timestamp;\n+  double _most_recent_gc_time;\n+  double _most_recent_gcu;\n+  double _most_recent_mutator_time;\n+  double _most_recent_mu;\n+\n+  \/\/ These variables hold recent snapshots of cumulative quantities that are used for reporting\n+  \/\/ periodic consumption of CPU time by GC and mutator threads.\n+  double _most_recent_periodic_time_stamp;\n+  double _most_recent_periodic_gc_time;\n+  double _most_recent_periodic_mutator_time;\n+\n+  size_t _most_recent_gcid;\n+  uint _active_processors;\n+\n+  bool _most_recent_is_full;\n+\n+  ShenandoahMmuTask* _mmu_periodic_task;\n+  TruncatedSeq _mmu_average;\n+\n+  void update_utilization(size_t gcid, const char* msg);\n+  static void fetch_cpu_times(double &gc_time, double &mutator_time);\n+\n+public:\n+  explicit ShenandoahMmuTracker();\n+  ~ShenandoahMmuTracker();\n+\n+  \/\/ This enrolls the periodic task after everything is initialized.\n+  void initialize();\n+\n+  \/\/ At completion of each GC cycle (not including interrupted cycles), we invoke one of the following to record the\n+  \/\/ GC utilization during this cycle.  Incremental efforts spent in an interrupted GC cycle will be accumulated into\n+  \/\/ the CPU time reports for the subsequent completed [degenerated or full] GC cycle.\n+  \/\/\n+  \/\/ We may redundantly record degen and full in the case that a degen upgrades to full.  When this happens, we will invoke\n+  \/\/ both record_full() and record_degenerated() with the same value of gcid.  record_full() is called first and the log\n+  \/\/ reports such a cycle as a FULL cycle.\n+  void record_young(size_t gcid);\n+  void record_global(size_t gcid);\n+  void record_bootstrap(size_t gcid);\n+  void record_old_marking_increment(bool old_marking_done);\n+  void record_mixed(size_t gcid);\n+  void record_full(size_t gcid);\n+  void record_degenerated(size_t gcid, bool is_old_boostrap);\n+\n+  \/\/ This is called by the periodic task timer. The interval is defined by\n+  \/\/ GCPauseIntervalMillis and defaults to 5 seconds. This method computes\n+  \/\/ the MMU over the elapsed interval and records it in a running average.\n+  void report();\n+\n+  \/\/ Unenrolls the periodic task that collects CPU utilization for GC threads. This must happen _before_ the\n+  \/\/ gc threads are stopped and terminated.\n+  void stop() const;\n+};\n+\n+#endif \/\/SHARE_GC_SHENANDOAH_SHENANDOAHMMUTRACKER_HPP\n","filename":"src\/hotspot\/share\/gc\/shenandoah\/shenandoahMmuTracker.hpp","additions":110,"deletions":0,"binary":false,"changes":110,"status":"added"},{"patch":"@@ -40,1 +40,1 @@\n-  virtual void update_all() {\n+  void update_all() override {\n@@ -49,1 +49,1 @@\n-  ShenandoahGenerationCounters(ShenandoahHeap* heap) :\n+  explicit ShenandoahGenerationCounters(ShenandoahHeap* heap) :\n@@ -54,1 +54,1 @@\n-  virtual void update_all() {\n+  void update_all() override {\n@@ -61,1 +61,2 @@\n-        _full_counters(nullptr)\n+        _full_counters(nullptr),\n+        _counters_update_task(this)\n@@ -74,0 +75,2 @@\n+\n+  _counters_update_task.enroll();\n@@ -106,0 +109,41 @@\n+\n+void ShenandoahMonitoringSupport::notify_heap_changed() {\n+  _counters_update_task.notify_heap_changed();\n+}\n+\n+void ShenandoahMonitoringSupport::set_forced_counters_update(bool value) {\n+  _counters_update_task.set_forced_counters_update(value);\n+}\n+\n+void ShenandoahMonitoringSupport::handle_force_counters_update() {\n+  _counters_update_task.handle_force_counters_update();\n+}\n+\n+void ShenandoahPeriodicCountersUpdateTask::task() {\n+  handle_force_counters_update();\n+  handle_counters_update();\n+}\n+\n+void ShenandoahPeriodicCountersUpdateTask::handle_counters_update() {\n+  if (_do_counters_update.is_set()) {\n+    _do_counters_update.unset();\n+    _monitoring_support->update_counters();\n+  }\n+}\n+\n+void ShenandoahPeriodicCountersUpdateTask::handle_force_counters_update() {\n+  if (_force_counters_update.is_set()) {\n+    _do_counters_update.unset(); \/\/ reset these too, we do update now!\n+    _monitoring_support->update_counters();\n+  }\n+}\n+\n+void ShenandoahPeriodicCountersUpdateTask::notify_heap_changed() {\n+  if (_do_counters_update.is_unset()) {\n+    _do_counters_update.set();\n+  }\n+}\n+\n+void ShenandoahPeriodicCountersUpdateTask::set_forced_counters_update(bool value) {\n+  _force_counters_update.set_cond(value);\n+}\n","filename":"src\/hotspot\/share\/gc\/shenandoah\/shenandoahMonitoringSupport.cpp","additions":48,"deletions":4,"binary":false,"changes":52,"status":"modified"},{"patch":"@@ -28,0 +28,1 @@\n+#include \"gc\/shenandoah\/shenandoahSharedVariables.hpp\"\n@@ -29,0 +30,1 @@\n+#include \"runtime\/task.hpp\"\n@@ -35,0 +37,20 @@\n+class ShenandoahMonitoringSupport;\n+\n+class ShenandoahPeriodicCountersUpdateTask : public PeriodicTask {\n+private:\n+  ShenandoahSharedFlag _do_counters_update;\n+  ShenandoahSharedFlag _force_counters_update;\n+  ShenandoahMonitoringSupport* const _monitoring_support;\n+\n+public:\n+  explicit ShenandoahPeriodicCountersUpdateTask(ShenandoahMonitoringSupport* monitoring_support) :\n+    PeriodicTask(100),\n+    _monitoring_support(monitoring_support) { }\n+\n+  void task() override;\n+\n+  void handle_counters_update();\n+  void handle_force_counters_update();\n+  void set_forced_counters_update(bool value);\n+  void notify_heap_changed();\n+};\n@@ -47,0 +69,1 @@\n+  ShenandoahPeriodicCountersUpdateTask _counters_update_task;\n@@ -49,6 +72,11 @@\n- ShenandoahMonitoringSupport(ShenandoahHeap* heap);\n- CollectorCounters* stw_collection_counters();\n- CollectorCounters* full_stw_collection_counters();\n- CollectorCounters* concurrent_collection_counters();\n- CollectorCounters* partial_collection_counters();\n- void update_counters();\n+  explicit ShenandoahMonitoringSupport(ShenandoahHeap* heap);\n+  CollectorCounters* stw_collection_counters();\n+  CollectorCounters* full_stw_collection_counters();\n+  CollectorCounters* concurrent_collection_counters();\n+  CollectorCounters* partial_collection_counters();\n+\n+  void notify_heap_changed();\n+  void set_forced_counters_update(bool value);\n+  void handle_force_counters_update();\n+\n+  void update_counters();\n","filename":"src\/hotspot\/share\/gc\/shenandoah\/shenandoahMonitoringSupport.hpp","additions":34,"deletions":6,"binary":false,"changes":40,"status":"modified"},{"patch":"@@ -129,5 +129,2 @@\n-  if (heap->is_concurrent_mark_in_progress()) {\n-    ShenandoahKeepAliveClosure cl;\n-    data->oops_do(&cl);\n-  } else if (heap->is_concurrent_weak_root_in_progress() ||\n-             heap->is_concurrent_strong_root_in_progress() ) {\n+  if (heap->is_concurrent_weak_root_in_progress() ||\n+      heap->is_concurrent_strong_root_in_progress()) {\n@@ -136,0 +133,3 @@\n+  } else if (heap->is_concurrent_mark_in_progress()) {\n+    ShenandoahKeepAliveClosure cl;\n+    data->oops_do(&cl);\n","filename":"src\/hotspot\/share\/gc\/shenandoah\/shenandoahNMethod.cpp","additions":5,"deletions":5,"binary":false,"changes":10,"status":"modified"},{"patch":"@@ -3,0 +3,1 @@\n+ * Copyright Amazon.com Inc. or its affiliates. All Rights Reserved.\n@@ -123,0 +124,64 @@\n+void HdrSeq::add(const HdrSeq& other) {\n+  if (other.num() == 0) {\n+    \/\/ Other sequence is empty, return\n+    return;\n+  }\n+\n+  for (int mag = 0; mag < MagBuckets; mag++) {\n+    int* other_bucket = other._hdr[mag];\n+    if (other_bucket == nullptr) {\n+      \/\/ Nothing to do\n+      continue;\n+    }\n+    int* bucket = _hdr[mag];\n+    if (bucket != nullptr) {\n+      \/\/ Add into our bucket\n+      for (int val = 0; val < ValBuckets; val++) {\n+        bucket[val] += other_bucket[val];\n+      }\n+    } else {\n+      \/\/ Create our bucket and copy the contents over\n+      bucket = NEW_C_HEAP_ARRAY(int, ValBuckets, mtInternal);\n+      for (int val = 0; val < ValBuckets; val++) {\n+        bucket[val] = other_bucket[val];\n+      }\n+      _hdr[mag] = bucket;\n+    }\n+  }\n+\n+  \/\/ This is a hacky way to only update the fields we want.\n+  \/\/ This inlines NumberSeq code without going into AbsSeq and\n+  \/\/ dealing with decayed average\/variance, which we do not\n+  \/\/ know how to compute yet.\n+  _last = other._last;\n+  _maximum = MAX2(_maximum, other._maximum);\n+  _sum += other._sum;\n+  _sum_of_squares += other._sum_of_squares;\n+  _num += other._num;\n+\n+  \/\/ Until JDK-8298902 is fixed, we taint the decaying statistics\n+  _davg = NAN;\n+  _dvariance = NAN;\n+}\n+\n+void HdrSeq::clear() {\n+  \/\/ Clear the storage\n+  for (int mag = 0; mag < MagBuckets; mag++) {\n+    int* bucket = _hdr[mag];\n+    if (bucket != nullptr) {\n+      for (int c = 0; c < ValBuckets; c++) {\n+        bucket[c] = 0;\n+      }\n+    }\n+  }\n+\n+  \/\/ Clear other fields too\n+  _last = 0;\n+  _maximum = 0;\n+  _sum = 0;\n+  _sum_of_squares = 0;\n+  _num = 0;\n+  _davg = 0;\n+  _dvariance = 0;\n+}\n+\n","filename":"src\/hotspot\/share\/gc\/shenandoah\/shenandoahNumberSeq.cpp","additions":65,"deletions":0,"binary":false,"changes":65,"status":"modified"},{"patch":"@@ -52,0 +52,1 @@\n+  void add(const HdrSeq& other);\n@@ -53,0 +54,1 @@\n+  void clear();\n","filename":"src\/hotspot\/share\/gc\/shenandoah\/shenandoahNumberSeq.hpp","additions":2,"deletions":0,"binary":false,"changes":2,"status":"modified"},{"patch":"@@ -0,0 +1,166 @@\n+\/*\n+ * Copyright Amazon.com Inc. or its affiliates. All Rights Reserved.\n+ * DO NOT ALTER OR REMOVE COPYRIGHT NOTICES OR THIS FILE HEADER.\n+ *\n+ * This code is free software; you can redistribute it and\/or modify it\n+ * under the terms of the GNU General Public License version 2 only, as\n+ * published by the Free Software Foundation.\n+ *\n+ * This code is distributed in the hope that it will be useful, but WITHOUT\n+ * ANY WARRANTY; without even the implied warranty of MERCHANTABILITY or\n+ * FITNESS FOR A PARTICULAR PURPOSE.  See the GNU General Public License\n+ * version 2 for more details (a copy is included in the LICENSE file that\n+ * accompanied this code).\n+ *\n+ * You should have received a copy of the GNU General Public License version\n+ * 2 along with this work; if not, write to the Free Software Foundation,\n+ * Inc., 51 Franklin St, Fifth Floor, Boston, MA 02110-1301 USA.\n+ *\n+ * Please contact Oracle, 500 Oracle Parkway, Redwood Shores, CA 94065 USA\n+ * or visit www.oracle.com if you need additional information or have any\n+ * questions.\n+ *\n+ *\/\n+\n+#include \"precompiled.hpp\"\n+\n+#include \"gc\/shenandoah\/heuristics\/shenandoahYoungHeuristics.hpp\"\n+#include \"gc\/shenandoah\/shenandoahFreeSet.hpp\"\n+#include \"gc\/shenandoah\/shenandoahGenerationalHeap.hpp\"\n+#include \"gc\/shenandoah\/shenandoahHeap.inline.hpp\"\n+#include \"gc\/shenandoah\/shenandoahMonitoringSupport.hpp\"\n+#include \"gc\/shenandoah\/shenandoahOldGC.hpp\"\n+#include \"gc\/shenandoah\/shenandoahOopClosures.inline.hpp\"\n+#include \"gc\/shenandoah\/shenandoahGeneration.hpp\"\n+#include \"gc\/shenandoah\/shenandoahOldGeneration.hpp\"\n+#include \"gc\/shenandoah\/shenandoahYoungGeneration.hpp\"\n+#include \"prims\/jvmtiTagMap.hpp\"\n+#include \"utilities\/events.hpp\"\n+\n+\n+ShenandoahOldGC::ShenandoahOldGC(ShenandoahOldGeneration* generation, ShenandoahSharedFlag& allow_preemption) :\n+    ShenandoahConcurrentGC(generation, false), _old_generation(generation), _allow_preemption(allow_preemption) {\n+}\n+\n+\/\/ Final mark for old-gen is different than for young or old, so we\n+\/\/ override the implementation.\n+void ShenandoahOldGC::op_final_mark() {\n+\n+  ShenandoahGenerationalHeap* const heap = ShenandoahGenerationalHeap::heap();\n+  assert(ShenandoahSafepoint::is_at_shenandoah_safepoint(), \"Should be at safepoint\");\n+  assert(!heap->has_forwarded_objects(), \"No forwarded objects on this path\");\n+\n+  if (ShenandoahVerify) {\n+    heap->verifier()->verify_roots_no_forwarded();\n+  }\n+\n+  if (!heap->cancelled_gc()) {\n+    assert(_mark.generation()->is_old(), \"Generation of Old-Gen GC should be OLD\");\n+    _mark.finish_mark();\n+    assert(!heap->cancelled_gc(), \"STW mark cannot OOM\");\n+\n+    \/\/ Old collection is complete, the young generation no longer needs this\n+    \/\/ reference to the old concurrent mark so clean it up.\n+    heap->young_generation()->set_old_gen_task_queues(nullptr);\n+\n+    \/\/ We need to do this because weak root cleaning reports the number of dead handles\n+    JvmtiTagMap::set_needs_cleaning();\n+\n+    _generation->prepare_regions_and_collection_set(true);\n+\n+    heap->set_unload_classes(false);\n+    heap->prepare_concurrent_roots();\n+\n+    \/\/ Believe verification following old-gen concurrent mark needs to be different than verification following\n+    \/\/ young-gen concurrent mark, so am commenting this out for now:\n+    \/\/   if (ShenandoahVerify) {\n+    \/\/     heap->verifier()->verify_after_concmark();\n+    \/\/   }\n+\n+    if (VerifyAfterGC) {\n+      Universe::verify();\n+    }\n+\n+    {\n+      ShenandoahTimingsTracker timing(ShenandoahPhaseTimings::final_mark_propagate_gc_state);\n+      heap->propagate_gc_state_to_all_threads();\n+    }\n+  }\n+}\n+\n+bool ShenandoahOldGC::collect(GCCause::Cause cause) {\n+  auto heap = ShenandoahGenerationalHeap::heap();\n+  assert(!_old_generation->is_doing_mixed_evacuations(), \"Should not start an old gc with pending mixed evacuations\");\n+  assert(!_old_generation->is_preparing_for_mark(), \"Old regions need to be parsable during concurrent mark.\");\n+\n+  \/\/ Enable preemption of old generation mark.\n+  _allow_preemption.set();\n+\n+  \/\/ Continue concurrent mark, do not reset regions, do not mark roots, do not collect $200.\n+  entry_mark();\n+\n+  \/\/ If we failed to unset the preemption flag, it means another thread has already unset it.\n+  if (!_allow_preemption.try_unset()) {\n+    \/\/ The regulator thread has unset the preemption guard. That thread will shortly cancel\n+    \/\/ the gc, but the control thread is now racing it. Wait until this thread sees the\n+    \/\/ cancellation.\n+    while (!heap->cancelled_gc()) {\n+      SpinPause();\n+    }\n+  }\n+\n+  if (heap->cancelled_gc()) {\n+    return false;\n+  }\n+\n+  \/\/ Complete marking under STW\n+  vmop_entry_final_mark();\n+\n+  if (_generation->is_concurrent_mark_in_progress()) {\n+    assert(heap->cancelled_gc(), \"Safepoint operation observed gc cancellation\");\n+    \/\/ GC may have been cancelled before final mark, but after the preceding cancellation check.\n+    return false;\n+  }\n+\n+  \/\/ We aren't dealing with old generation evacuation yet. Our heuristic\n+  \/\/ should not have built a cset in final mark.\n+  assert(!heap->is_evacuation_in_progress(), \"Old gen evacuations are not supported\");\n+\n+  \/\/ Process weak roots that might still point to regions that would be broken by cleanup\n+  if (heap->is_concurrent_weak_root_in_progress()) {\n+    entry_weak_refs();\n+    entry_weak_roots();\n+  }\n+\n+  \/\/ Final mark might have reclaimed some immediate garbage, kick cleanup to reclaim\n+  \/\/ the space. This would be the last action if there is nothing to evacuate.\n+  entry_cleanup_early();\n+\n+  heap->free_set()->log_status_under_lock();\n+\n+  assert(!heap->is_concurrent_strong_root_in_progress(), \"No evacuations during old gc.\");\n+\n+  \/\/ We must execute this vm operation if we completed final mark. We cannot\n+  \/\/ return from here with weak roots in progress. This is not a valid gc state\n+  \/\/ for any young collections (or allocation failures) that interrupt the old\n+  \/\/ collection.\n+  heap->concurrent_final_roots();\n+\n+  \/\/ We do not rebuild_free following increments of old marking because memory has not been reclaimed. However, we may\n+  \/\/ need to transfer memory to OLD in order to efficiently support the mixed evacuations that might immediately follow.\n+  size_t allocation_runway = heap->young_generation()->heuristics()->bytes_of_allocation_runway_before_gc_trigger(0);\n+  heap->compute_old_generation_balance(allocation_runway, 0);\n+\n+  ShenandoahGenerationalHeap::TransferResult result;\n+  {\n+    ShenandoahHeapLocker locker(heap->lock());\n+    result = heap->balance_generations();\n+  }\n+\n+  LogTarget(Info, gc, ergo) lt;\n+  if (lt.is_enabled()) {\n+    LogStream ls(lt);\n+    result.print_on(\"Old Mark\", &ls);\n+  }\n+  return true;\n+}\n","filename":"src\/hotspot\/share\/gc\/shenandoah\/shenandoahOldGC.cpp","additions":166,"deletions":0,"binary":false,"changes":166,"status":"added"},{"patch":"@@ -0,0 +1,48 @@\n+\/*\n+ * Copyright Amazon.com Inc. or its affiliates. All Rights Reserved.\n+ * DO NOT ALTER OR REMOVE COPYRIGHT NOTICES OR THIS FILE HEADER.\n+ *\n+ * This code is free software; you can redistribute it and\/or modify it\n+ * under the terms of the GNU General Public License version 2 only, as\n+ * published by the Free Software Foundation.\n+ *\n+ * This code is distributed in the hope that it will be useful, but WITHOUT\n+ * ANY WARRANTY; without even the implied warranty of MERCHANTABILITY or\n+ * FITNESS FOR A PARTICULAR PURPOSE.  See the GNU General Public License\n+ * version 2 for more details (a copy is included in the LICENSE file that\n+ * accompanied this code).\n+ *\n+ * You should have received a copy of the GNU General Public License version\n+ * 2 along with this work; if not, write to the Free Software Foundation,\n+ * Inc., 51 Franklin St, Fifth Floor, Boston, MA 02110-1301 USA.\n+ *\n+ * Please contact Oracle, 500 Oracle Parkway, Redwood Shores, CA 94065 USA\n+ * or visit www.oracle.com if you need additional information or have any\n+ * questions.\n+ *\n+ *\/\n+\n+#ifndef SHARE_GC_SHENANDOAH_SHENANDOAHOLDGC_HPP\n+#define SHARE_GC_SHENANDOAH_SHENANDOAHOLDGC_HPP\n+\n+#include \"gc\/shared\/gcCause.hpp\"\n+#include \"gc\/shenandoah\/shenandoahConcurrentGC.hpp\"\n+#include \"gc\/shenandoah\/shenandoahVerifier.hpp\"\n+\n+class ShenandoahOldGeneration;\n+\n+class ShenandoahOldGC : public ShenandoahConcurrentGC {\n+ public:\n+  ShenandoahOldGC(ShenandoahOldGeneration* generation, ShenandoahSharedFlag& allow_preemption);\n+  bool collect(GCCause::Cause cause) override;\n+\n+ protected:\n+  void op_final_mark() override;\n+\n+ private:\n+  ShenandoahOldGeneration* _old_generation;\n+  ShenandoahSharedFlag& _allow_preemption;\n+};\n+\n+\n+#endif \/\/SHARE_GC_SHENANDOAH_SHENANDOAHOLDGC_HPP\n","filename":"src\/hotspot\/share\/gc\/shenandoah\/shenandoahOldGC.hpp","additions":48,"deletions":0,"binary":false,"changes":48,"status":"added"},{"patch":"@@ -0,0 +1,831 @@\n+\/*\n+ * Copyright Amazon.com Inc. or its affiliates. All Rights Reserved.\n+ * DO NOT ALTER OR REMOVE COPYRIGHT NOTICES OR THIS FILE HEADER.\n+ *\n+ * This code is free software; you can redistribute it and\/or modify it\n+ * under the terms of the GNU General Public License version 2 only, as\n+ * published by the Free Software Foundation.\n+ *\n+ * This code is distributed in the hope that it will be useful, but WITHOUT\n+ * ANY WARRANTY; without even the implied warranty of MERCHANTABILITY or\n+ * FITNESS FOR A PARTICULAR PURPOSE.  See the GNU General Public License\n+ * version 2 for more details (a copy is included in the LICENSE file that\n+ * accompanied this code).\n+ *\n+ * You should have received a copy of the GNU General Public License version\n+ * 2 along with this work; if not, write to the Free Software Foundation,\n+ * Inc., 51 Franklin St, Fifth Floor, Boston, MA 02110-1301 USA.\n+ *\n+ * Please contact Oracle, 500 Oracle Parkway, Redwood Shores, CA 94065 USA\n+ * or visit www.oracle.com if you need additional information or have any\n+ * questions.\n+ *\n+ *\/\n+\n+\n+#include \"precompiled.hpp\"\n+\n+#include \"gc\/shenandoah\/heuristics\/shenandoahOldHeuristics.hpp\"\n+#include \"gc\/shenandoah\/shenandoahAsserts.hpp\"\n+#include \"gc\/shenandoah\/shenandoahCardTable.hpp\"\n+#include \"gc\/shenandoah\/shenandoahCollectorPolicy.hpp\"\n+#include \"gc\/shenandoah\/shenandoahFreeSet.hpp\"\n+#include \"gc\/shenandoah\/shenandoahGenerationalHeap.hpp\"\n+#include \"gc\/shenandoah\/shenandoahHeap.hpp\"\n+#include \"gc\/shenandoah\/shenandoahHeap.inline.hpp\"\n+#include \"gc\/shenandoah\/shenandoahHeapRegion.hpp\"\n+#include \"gc\/shenandoah\/shenandoahHeapRegionClosures.hpp\"\n+#include \"gc\/shenandoah\/shenandoahMonitoringSupport.hpp\"\n+#include \"gc\/shenandoah\/shenandoahOldGeneration.hpp\"\n+#include \"gc\/shenandoah\/shenandoahOopClosures.inline.hpp\"\n+#include \"gc\/shenandoah\/shenandoahReferenceProcessor.hpp\"\n+#include \"gc\/shenandoah\/shenandoahScanRemembered.inline.hpp\"\n+#include \"gc\/shenandoah\/shenandoahUtils.hpp\"\n+#include \"gc\/shenandoah\/shenandoahWorkerPolicy.hpp\"\n+#include \"gc\/shenandoah\/shenandoahYoungGeneration.hpp\"\n+#include \"runtime\/threads.hpp\"\n+#include \"utilities\/events.hpp\"\n+\n+class ShenandoahFlushAllSATB : public ThreadClosure {\n+private:\n+  SATBMarkQueueSet& _satb_qset;\n+\n+public:\n+  explicit ShenandoahFlushAllSATB(SATBMarkQueueSet& satb_qset) :\n+    _satb_qset(satb_qset) {}\n+\n+  void do_thread(Thread* thread) override {\n+    \/\/ Transfer any partial buffer to the qset for completed buffer processing.\n+    _satb_qset.flush_queue(ShenandoahThreadLocalData::satb_mark_queue(thread));\n+  }\n+};\n+\n+class ShenandoahProcessOldSATB : public SATBBufferClosure {\n+private:\n+  ShenandoahObjToScanQueue*       _queue;\n+  ShenandoahHeap*                 _heap;\n+  ShenandoahMarkingContext* const _mark_context;\n+  size_t                          _trashed_oops;\n+\n+public:\n+  explicit ShenandoahProcessOldSATB(ShenandoahObjToScanQueue* q) :\n+    _queue(q),\n+    _heap(ShenandoahHeap::heap()),\n+    _mark_context(_heap->marking_context()),\n+    _trashed_oops(0) {}\n+\n+  void do_buffer(void** buffer, size_t size) override {\n+    assert(size == 0 || !_heap->has_forwarded_objects() || _heap->is_concurrent_old_mark_in_progress(), \"Forwarded objects are not expected here\");\n+    for (size_t i = 0; i < size; ++i) {\n+      oop *p = (oop *) &buffer[i];\n+      ShenandoahHeapRegion* region = _heap->heap_region_containing(*p);\n+      if (region->is_old() && region->is_active()) {\n+          ShenandoahMark::mark_through_ref<oop, OLD>(p, _queue, nullptr, _mark_context, false);\n+      } else {\n+        _trashed_oops++;\n+      }\n+    }\n+  }\n+\n+  size_t trashed_oops() const {\n+    return _trashed_oops;\n+  }\n+};\n+\n+class ShenandoahPurgeSATBTask : public WorkerTask {\n+private:\n+  ShenandoahObjToScanQueueSet* _mark_queues;\n+  \/\/ Keep track of the number of oops that are not transferred to mark queues.\n+  \/\/ This is volatile because workers update it, but the vm thread reads it.\n+  volatile size_t             _trashed_oops;\n+\n+public:\n+  explicit ShenandoahPurgeSATBTask(ShenandoahObjToScanQueueSet* queues) :\n+    WorkerTask(\"Purge SATB\"),\n+    _mark_queues(queues),\n+    _trashed_oops(0) {\n+    Threads::change_thread_claim_token();\n+  }\n+\n+  ~ShenandoahPurgeSATBTask() {\n+    if (_trashed_oops > 0) {\n+      log_debug(gc)(\"Purged \" SIZE_FORMAT \" oops from old generation SATB buffers\", _trashed_oops);\n+    }\n+  }\n+\n+  void work(uint worker_id) override {\n+    ShenandoahParallelWorkerSession worker_session(worker_id);\n+    ShenandoahSATBMarkQueueSet &satb_queues = ShenandoahBarrierSet::satb_mark_queue_set();\n+    ShenandoahFlushAllSATB flusher(satb_queues);\n+    Threads::possibly_parallel_threads_do(true \/* is_par *\/, &flusher);\n+\n+    ShenandoahObjToScanQueue* mark_queue = _mark_queues->queue(worker_id);\n+    ShenandoahProcessOldSATB processor(mark_queue);\n+    while (satb_queues.apply_closure_to_completed_buffer(&processor)) {}\n+\n+    Atomic::add(&_trashed_oops, processor.trashed_oops());\n+  }\n+};\n+\n+class ShenandoahTransferOldSATBTask : public WorkerTask {\n+  ShenandoahSATBMarkQueueSet&  _satb_queues;\n+  ShenandoahObjToScanQueueSet* _mark_queues;\n+  \/\/ Keep track of the number of oops that are not transferred to mark queues.\n+  \/\/ This is volatile because workers update it, but the control thread reads it.\n+  volatile size_t              _trashed_oops;\n+\n+public:\n+  explicit ShenandoahTransferOldSATBTask(ShenandoahSATBMarkQueueSet& satb_queues, ShenandoahObjToScanQueueSet* mark_queues) :\n+    WorkerTask(\"Transfer SATB\"),\n+    _satb_queues(satb_queues),\n+    _mark_queues(mark_queues),\n+    _trashed_oops(0) {}\n+\n+  ~ShenandoahTransferOldSATBTask() {\n+    if (_trashed_oops > 0) {\n+      log_debug(gc)(\"Purged %zu oops from old generation SATB buffers\", _trashed_oops);\n+    }\n+  }\n+\n+  void work(uint worker_id) override {\n+    ShenandoahObjToScanQueue* mark_queue = _mark_queues->queue(worker_id);\n+    ShenandoahProcessOldSATB processor(mark_queue);\n+    while (_satb_queues.apply_closure_to_completed_buffer(&processor)) {}\n+\n+    Atomic::add(&_trashed_oops, processor.trashed_oops());\n+  }\n+};\n+\n+class ShenandoahConcurrentCoalesceAndFillTask : public WorkerTask {\n+private:\n+  uint                    _nworkers;\n+  ShenandoahHeapRegion**  _coalesce_and_fill_region_array;\n+  uint                    _coalesce_and_fill_region_count;\n+  volatile bool           _is_preempted;\n+\n+public:\n+  ShenandoahConcurrentCoalesceAndFillTask(uint nworkers,\n+                                          ShenandoahHeapRegion** coalesce_and_fill_region_array,\n+                                          uint region_count) :\n+    WorkerTask(\"Shenandoah Concurrent Coalesce and Fill\"),\n+    _nworkers(nworkers),\n+    _coalesce_and_fill_region_array(coalesce_and_fill_region_array),\n+    _coalesce_and_fill_region_count(region_count),\n+    _is_preempted(false) {\n+  }\n+\n+  void work(uint worker_id) override {\n+    ShenandoahWorkerTimingsTracker timer(ShenandoahPhaseTimings::conc_coalesce_and_fill, ShenandoahPhaseTimings::ScanClusters, worker_id);\n+    for (uint region_idx = worker_id; region_idx < _coalesce_and_fill_region_count; region_idx += _nworkers) {\n+      ShenandoahHeapRegion* r = _coalesce_and_fill_region_array[region_idx];\n+      if (r->is_humongous()) {\n+        \/\/ There is only one object in this region and it is not garbage,\n+        \/\/ so no need to coalesce or fill.\n+        continue;\n+      }\n+\n+      if (!r->oop_coalesce_and_fill(true)) {\n+        \/\/ Coalesce and fill has been preempted\n+        Atomic::store(&_is_preempted, true);\n+        return;\n+      }\n+    }\n+  }\n+\n+  \/\/ Value returned from is_completed() is only valid after all worker thread have terminated.\n+  bool is_completed() {\n+    return !Atomic::load(&_is_preempted);\n+  }\n+};\n+\n+ShenandoahOldGeneration::ShenandoahOldGeneration(uint max_queues, size_t max_capacity)\n+  : ShenandoahGeneration(OLD, max_queues, max_capacity),\n+    _coalesce_and_fill_region_array(NEW_C_HEAP_ARRAY(ShenandoahHeapRegion*, ShenandoahHeap::heap()->num_regions(), mtGC)),\n+    _old_heuristics(nullptr),\n+    _region_balance(0),\n+    _promoted_reserve(0),\n+    _promoted_expended(0),\n+    _promotion_potential(0),\n+    _pad_for_promote_in_place(0),\n+    _promotable_humongous_regions(0),\n+    _promotable_regular_regions(0),\n+    _is_parsable(true),\n+    _card_scan(nullptr),\n+    _state(WAITING_FOR_BOOTSTRAP),\n+    _growth_before_compaction(INITIAL_GROWTH_BEFORE_COMPACTION),\n+    _min_growth_before_compaction ((ShenandoahMinOldGenGrowthPercent * FRACTIONAL_DENOMINATOR) \/ 100)\n+{\n+  _live_bytes_after_last_mark = ShenandoahHeap::heap()->capacity() * INITIAL_LIVE_FRACTION \/ FRACTIONAL_DENOMINATOR;\n+  \/\/ Always clear references for old generation\n+  ref_processor()->set_soft_reference_policy(true);\n+\n+  if (ShenandoahCardBarrier) {\n+    ShenandoahCardTable* card_table = ShenandoahBarrierSet::barrier_set()->card_table();\n+    size_t card_count = card_table->cards_required(ShenandoahHeap::heap()->reserved_region().word_size());\n+    auto rs = new ShenandoahDirectCardMarkRememberedSet(card_table, card_count);\n+    _card_scan = new ShenandoahScanRemembered(rs);\n+  }\n+}\n+\n+void ShenandoahOldGeneration::set_promoted_reserve(size_t new_val) {\n+  shenandoah_assert_heaplocked_or_safepoint();\n+  _promoted_reserve = new_val;\n+}\n+\n+size_t ShenandoahOldGeneration::get_promoted_reserve() const {\n+  return _promoted_reserve;\n+}\n+\n+void ShenandoahOldGeneration::augment_promoted_reserve(size_t increment) {\n+  shenandoah_assert_heaplocked_or_safepoint();\n+  _promoted_reserve += increment;\n+}\n+\n+void ShenandoahOldGeneration::reset_promoted_expended() {\n+  shenandoah_assert_heaplocked_or_safepoint();\n+  Atomic::store(&_promoted_expended, (size_t) 0);\n+}\n+\n+size_t ShenandoahOldGeneration::expend_promoted(size_t increment) {\n+  shenandoah_assert_heaplocked_or_safepoint();\n+  assert(get_promoted_expended() + increment <= get_promoted_reserve(), \"Do not expend more promotion than budgeted\");\n+  return Atomic::add(&_promoted_expended, increment);\n+}\n+\n+size_t ShenandoahOldGeneration::unexpend_promoted(size_t decrement) {\n+  return Atomic::sub(&_promoted_expended, decrement);\n+}\n+\n+size_t ShenandoahOldGeneration::get_promoted_expended() const {\n+  return Atomic::load(&_promoted_expended);\n+}\n+\n+bool ShenandoahOldGeneration::can_allocate(const ShenandoahAllocRequest &req) const {\n+  assert(req.type() != ShenandoahAllocRequest::_alloc_gclab, \"GCLAB pertains only to young-gen memory\");\n+\n+  const size_t requested_bytes = req.size() * HeapWordSize;\n+  \/\/ The promotion reserve may also be used for evacuations. If we can promote this object,\n+  \/\/ then we can also evacuate it.\n+  if (can_promote(requested_bytes)) {\n+    \/\/ The promotion reserve should be able to accommodate this request. The request\n+    \/\/ might still fail if alignment with the card table increases the size. The request\n+    \/\/ may also fail if the heap is badly fragmented and the free set cannot find room for it.\n+    return true;\n+  }\n+\n+  if (req.type() == ShenandoahAllocRequest::_alloc_plab) {\n+    \/\/ The promotion reserve cannot accommodate this plab request. Check if we still have room for\n+    \/\/ evacuations. Note that we cannot really know how much of the plab will be used for evacuations,\n+    \/\/ so here we only check that some evacuation reserve still exists.\n+    return get_evacuation_reserve() > 0;\n+  }\n+\n+  \/\/ This is a shared allocation request. We've already checked that it can't be promoted, so if\n+  \/\/ it is a promotion, we return false. Otherwise, it is a shared evacuation request, and we allow\n+  \/\/ the allocation to proceed.\n+  return !req.is_promotion();\n+}\n+\n+void\n+ShenandoahOldGeneration::configure_plab_for_current_thread(const ShenandoahAllocRequest &req) {\n+  \/\/ Note: Even when a mutator is performing a promotion outside a LAB, we use a 'shared_gc' request.\n+  if (req.is_gc_alloc()) {\n+    const size_t actual_size = req.actual_size() * HeapWordSize;\n+    if (req.type() ==  ShenandoahAllocRequest::_alloc_plab) {\n+      \/\/ We've created a new plab. Now we configure it whether it will be used for promotions\n+      \/\/ and evacuations - or just evacuations.\n+      Thread* thread = Thread::current();\n+      ShenandoahThreadLocalData::reset_plab_promoted(thread);\n+\n+      \/\/ The actual size of the allocation may be larger than the requested bytes (due to alignment on card boundaries).\n+      \/\/ If this puts us over our promotion budget, we need to disable future PLAB promotions for this thread.\n+      if (can_promote(actual_size)) {\n+        \/\/ Assume the entirety of this PLAB will be used for promotion.  This prevents promotion from overreach.\n+        \/\/ When we retire this plab, we'll unexpend what we don't really use.\n+        expend_promoted(actual_size);\n+        ShenandoahThreadLocalData::enable_plab_promotions(thread);\n+        ShenandoahThreadLocalData::set_plab_actual_size(thread, actual_size);\n+      } else {\n+        \/\/ Disable promotions in this thread because entirety of this PLAB must be available to hold old-gen evacuations.\n+        ShenandoahThreadLocalData::disable_plab_promotions(thread);\n+        ShenandoahThreadLocalData::set_plab_actual_size(thread, 0);\n+      }\n+    } else if (req.is_promotion()) {\n+      \/\/ Shared promotion.\n+      expend_promoted(actual_size);\n+    }\n+  }\n+}\n+\n+size_t ShenandoahOldGeneration::get_live_bytes_after_last_mark() const {\n+  return _live_bytes_after_last_mark;\n+}\n+\n+void ShenandoahOldGeneration::set_live_bytes_after_last_mark(size_t bytes) {\n+  if (bytes == 0) {\n+    \/\/ Restart search for best old-gen size to the initial state\n+    _live_bytes_after_last_mark = ShenandoahHeap::heap()->capacity() * INITIAL_LIVE_FRACTION \/ FRACTIONAL_DENOMINATOR;\n+    _growth_before_compaction = INITIAL_GROWTH_BEFORE_COMPACTION;\n+  } else {\n+    _live_bytes_after_last_mark = bytes;\n+    _growth_before_compaction \/= 2;\n+    if (_growth_before_compaction < _min_growth_before_compaction) {\n+      _growth_before_compaction = _min_growth_before_compaction;\n+    }\n+  }\n+}\n+\n+void ShenandoahOldGeneration::handle_failed_transfer() {\n+  _old_heuristics->trigger_cannot_expand();\n+}\n+\n+size_t ShenandoahOldGeneration::usage_trigger_threshold() const {\n+  size_t result = _live_bytes_after_last_mark + (_live_bytes_after_last_mark * _growth_before_compaction) \/ FRACTIONAL_DENOMINATOR;\n+  return result;\n+}\n+\n+bool ShenandoahOldGeneration::contains(ShenandoahAffiliation affiliation) const {\n+  return affiliation == OLD_GENERATION;\n+}\n+bool ShenandoahOldGeneration::contains(ShenandoahHeapRegion* region) const {\n+  return region->is_old();\n+}\n+\n+void ShenandoahOldGeneration::parallel_heap_region_iterate(ShenandoahHeapRegionClosure* cl) {\n+  ShenandoahIncludeRegionClosure<OLD_GENERATION> old_regions_cl(cl);\n+  ShenandoahHeap::heap()->parallel_heap_region_iterate(&old_regions_cl);\n+}\n+\n+void ShenandoahOldGeneration::heap_region_iterate(ShenandoahHeapRegionClosure* cl) {\n+  ShenandoahIncludeRegionClosure<OLD_GENERATION> old_regions_cl(cl);\n+  ShenandoahHeap::heap()->heap_region_iterate(&old_regions_cl);\n+}\n+\n+void ShenandoahOldGeneration::set_concurrent_mark_in_progress(bool in_progress) {\n+  ShenandoahHeap::heap()->set_concurrent_old_mark_in_progress(in_progress);\n+}\n+\n+bool ShenandoahOldGeneration::is_concurrent_mark_in_progress() {\n+  return ShenandoahHeap::heap()->is_concurrent_old_mark_in_progress();\n+}\n+\n+void ShenandoahOldGeneration::cancel_marking() {\n+  if (is_concurrent_mark_in_progress()) {\n+    log_debug(gc)(\"Abandon SATB buffers\");\n+    ShenandoahBarrierSet::satb_mark_queue_set().abandon_partial_marking();\n+  }\n+\n+  ShenandoahGeneration::cancel_marking();\n+}\n+\n+void ShenandoahOldGeneration::cancel_gc() {\n+  shenandoah_assert_safepoint();\n+  if (is_idle()) {\n+#ifdef ASSERT\n+    validate_waiting_for_bootstrap();\n+#endif\n+  } else {\n+    log_info(gc)(\"Terminating old gc cycle.\");\n+    \/\/ Stop marking\n+    cancel_marking();\n+    \/\/ Stop tracking old regions\n+    abandon_collection_candidates();\n+    \/\/ Remove old generation access to young generation mark queues\n+    ShenandoahHeap::heap()->young_generation()->set_old_gen_task_queues(nullptr);\n+    \/\/ Transition to IDLE now.\n+    transition_to(ShenandoahOldGeneration::WAITING_FOR_BOOTSTRAP);\n+  }\n+}\n+\n+void ShenandoahOldGeneration::prepare_gc() {\n+  \/\/ Now that we have made the old generation parsable, it is safe to reset the mark bitmap.\n+  assert(state() != FILLING, \"Cannot reset old without making it parsable\");\n+\n+  ShenandoahGeneration::prepare_gc();\n+}\n+\n+bool ShenandoahOldGeneration::entry_coalesce_and_fill() {\n+  ShenandoahHeap* const heap = ShenandoahHeap::heap();\n+\n+  static const char* msg = \"Coalescing and filling (Old)\";\n+  ShenandoahConcurrentPhase gc_phase(msg, ShenandoahPhaseTimings::conc_coalesce_and_fill);\n+\n+  TraceCollectorStats tcs(heap->monitoring_support()->concurrent_collection_counters());\n+  EventMark em(\"%s\", msg);\n+  ShenandoahWorkerScope scope(heap->workers(),\n+                              ShenandoahWorkerPolicy::calc_workers_for_conc_marking(),\n+                              msg);\n+\n+  return coalesce_and_fill();\n+}\n+\n+\/\/ Make the old generation regions parsable, so they can be safely\n+\/\/ scanned when looking for objects in memory indicated by dirty cards.\n+bool ShenandoahOldGeneration::coalesce_and_fill() {\n+  transition_to(FILLING);\n+\n+  \/\/ This code will see the same set of regions to fill on each resumption as it did\n+  \/\/ on the initial run. That's okay because each region keeps track of its own coalesce\n+  \/\/ and fill state. Regions that were filled on a prior attempt will not try to fill again.\n+  uint coalesce_and_fill_regions_count = _old_heuristics->get_coalesce_and_fill_candidates(_coalesce_and_fill_region_array);\n+  assert(coalesce_and_fill_regions_count <= ShenandoahHeap::heap()->num_regions(), \"Sanity\");\n+  if (coalesce_and_fill_regions_count == 0) {\n+    \/\/ No regions need to be filled.\n+    abandon_collection_candidates();\n+    return true;\n+  }\n+\n+  ShenandoahHeap* const heap = ShenandoahHeap::heap();\n+  WorkerThreads* workers = heap->workers();\n+  uint nworkers = workers->active_workers();\n+  ShenandoahConcurrentCoalesceAndFillTask task(nworkers, _coalesce_and_fill_region_array, coalesce_and_fill_regions_count);\n+\n+  log_debug(gc)(\"Starting (or resuming) coalesce-and-fill of \" UINT32_FORMAT \" old heap regions\", coalesce_and_fill_regions_count);\n+  workers->run_task(&task);\n+  if (task.is_completed()) {\n+    \/\/ We no longer need to track regions that need to be coalesced and filled.\n+    abandon_collection_candidates();\n+    return true;\n+  } else {\n+    \/\/ Coalesce-and-fill has been preempted. We'll finish that effort in the future.  Do not invoke\n+    \/\/ ShenandoahGeneration::prepare_gc() until coalesce-and-fill is done because it resets the mark bitmap\n+    \/\/ and invokes set_mark_incomplete().  Coalesce-and-fill depends on the mark bitmap.\n+    log_debug(gc)(\"Suspending coalesce-and-fill of old heap regions\");\n+    return false;\n+  }\n+}\n+\n+void ShenandoahOldGeneration::concurrent_transfer_pointers_from_satb() const {\n+  const ShenandoahHeap* heap = ShenandoahHeap::heap();\n+  assert(heap->is_concurrent_old_mark_in_progress(), \"Only necessary during old marking.\");\n+  log_debug(gc)(\"Transfer SATB buffers\");\n+\n+  \/\/ Step 1. All threads need to 'complete' partially filled, thread local SATB buffers. This\n+  \/\/ is accomplished in ShenandoahConcurrentGC::complete_abbreviated_cycle using a Handshake\n+  \/\/ operation.\n+  \/\/ Step 2. Use worker threads to transfer oops from old, active regions in the completed\n+  \/\/ SATB buffers to old generation mark queues.\n+  ShenandoahSATBMarkQueueSet& satb_queues = ShenandoahBarrierSet::satb_mark_queue_set();\n+  ShenandoahTransferOldSATBTask transfer_task(satb_queues, task_queues());\n+  heap->workers()->run_task(&transfer_task);\n+}\n+\n+void ShenandoahOldGeneration::transfer_pointers_from_satb() const {\n+  const ShenandoahHeap* heap = ShenandoahHeap::heap();\n+  assert(heap->is_concurrent_old_mark_in_progress(), \"Only necessary during old marking.\");\n+  log_debug(gc)(\"Transfer SATB buffers\");\n+  ShenandoahPurgeSATBTask purge_satb_task(task_queues());\n+  heap->workers()->run_task(&purge_satb_task);\n+}\n+\n+bool ShenandoahOldGeneration::contains(oop obj) const {\n+  return ShenandoahHeap::heap()->is_in_old(obj);\n+}\n+\n+void ShenandoahOldGeneration::prepare_regions_and_collection_set(bool concurrent) {\n+  ShenandoahHeap* heap = ShenandoahHeap::heap();\n+  assert(!heap->is_full_gc_in_progress(), \"Only for concurrent and degenerated GC\");\n+\n+  {\n+    ShenandoahGCPhase phase(concurrent ?\n+        ShenandoahPhaseTimings::final_update_region_states :\n+        ShenandoahPhaseTimings::degen_gc_final_update_region_states);\n+    ShenandoahFinalMarkUpdateRegionStateClosure cl(complete_marking_context());\n+\n+    parallel_heap_region_iterate(&cl);\n+    heap->assert_pinned_region_status();\n+  }\n+\n+  {\n+    \/\/ This doesn't actually choose a collection set, but prepares a list of\n+    \/\/ regions as 'candidates' for inclusion in a mixed collection.\n+    ShenandoahGCPhase phase(concurrent ?\n+        ShenandoahPhaseTimings::choose_cset :\n+        ShenandoahPhaseTimings::degen_gc_choose_cset);\n+    ShenandoahHeapLocker locker(heap->lock());\n+    _old_heuristics->prepare_for_old_collections();\n+  }\n+\n+  {\n+    \/\/ Though we did not choose a collection set above, we still may have\n+    \/\/ freed up immediate garbage regions so proceed with rebuilding the free set.\n+    ShenandoahGCPhase phase(concurrent ?\n+        ShenandoahPhaseTimings::final_rebuild_freeset :\n+        ShenandoahPhaseTimings::degen_gc_final_rebuild_freeset);\n+    ShenandoahHeapLocker locker(heap->lock());\n+    size_t cset_young_regions, cset_old_regions;\n+    size_t first_old, last_old, num_old;\n+    heap->free_set()->prepare_to_rebuild(cset_young_regions, cset_old_regions, first_old, last_old, num_old);\n+    \/\/ This is just old-gen completion.  No future budgeting required here.  The only reason to rebuild the freeset here\n+    \/\/ is in case there was any immediate old garbage identified.\n+    heap->free_set()->finish_rebuild(cset_young_regions, cset_old_regions, num_old);\n+  }\n+}\n+\n+const char* ShenandoahOldGeneration::state_name(State state) {\n+  switch (state) {\n+    case WAITING_FOR_BOOTSTRAP:   return \"Waiting for Bootstrap\";\n+    case FILLING:                 return \"Coalescing\";\n+    case BOOTSTRAPPING:           return \"Bootstrapping\";\n+    case MARKING:                 return \"Marking\";\n+    case EVACUATING:              return \"Evacuating\";\n+    case EVACUATING_AFTER_GLOBAL: return \"Evacuating (G)\";\n+    default:\n+      ShouldNotReachHere();\n+      return \"Unknown\";\n+  }\n+}\n+\n+void ShenandoahOldGeneration::transition_to(State new_state) {\n+  if (_state != new_state) {\n+    log_debug(gc, thread)(\"Old generation transition from %s to %s\", state_name(_state), state_name(new_state));\n+    EventMark event(\"Old was %s, now is %s\", state_name(_state), state_name(new_state));\n+    validate_transition(new_state);\n+    _state = new_state;\n+  }\n+}\n+\n+#ifdef ASSERT\n+\/\/ This diagram depicts the expected state transitions for marking the old generation\n+\/\/ and preparing for old collections. When a young generation cycle executes, the\n+\/\/ remembered set scan must visit objects in old regions. Visiting an object which\n+\/\/ has become dead on previous old cycles will result in crashes. To avoid visiting\n+\/\/ such objects, the remembered set scan will use the old generation mark bitmap when\n+\/\/ possible. It is _not_ possible to use the old generation bitmap when old marking\n+\/\/ is active (bitmap is not complete). For this reason, the old regions are made\n+\/\/ parsable _before_ the old generation bitmap is reset. The diagram does not depict\n+\/\/ cancellation of old collections by global or full collections.\n+\/\/\n+\/\/ When a global collection supersedes an old collection, the global mark still\n+\/\/ \"completes\" the old mark bitmap. Subsequent remembered set scans may use the\n+\/\/ old generation mark bitmap, but any uncollected old regions must still be made parsable\n+\/\/ before the next old generation cycle begins. For this reason, a global collection may\n+\/\/ create mixed collection candidates and coalesce and fill candidates and will put\n+\/\/ the old generation in the respective states (EVACUATING or FILLING). After a Full GC,\n+\/\/ the mark bitmaps are all reset, all regions are parsable and the mark context will\n+\/\/ not be \"complete\". After a Full GC, remembered set scans will _not_ use the mark bitmap\n+\/\/ and we expect the old generation to be waiting for bootstrap.\n+\/\/\n+\/\/                              +-----------------+\n+\/\/               +------------> |     FILLING     | <---+\n+\/\/               |   +--------> |                 |     |\n+\/\/               |   |          +-----------------+     |\n+\/\/               |   |            |                     |\n+\/\/               |   |            | Filling Complete    | <-> A global collection may\n+\/\/               |   |            v                     |     move the old generation\n+\/\/               |   |          +-----------------+     |     directly from waiting for\n+\/\/           +-- |-- |--------> |     WAITING     |     |     bootstrap to filling or\n+\/\/           |   |   |    +---- |  FOR BOOTSTRAP  | ----+     evacuating. It may also\n+\/\/           |   |   |    |     +-----------------+           move from filling to waiting\n+\/\/           |   |   |    |       |                           for bootstrap.\n+\/\/           |   |   |    |       | Reset Bitmap\n+\/\/           |   |   |    |       v\n+\/\/           |   |   |    |     +-----------------+     +----------------------+\n+\/\/           |   |   |    |     |    BOOTSTRAP    | <-> |       YOUNG GC       |\n+\/\/           |   |   |    |     |                 |     | (RSet Parses Region) |\n+\/\/           |   |   |    |     +-----------------+     +----------------------+\n+\/\/           |   |   |    |       |\n+\/\/           |   |   |    |       | Old Marking\n+\/\/           |   |   |    |       v\n+\/\/           |   |   |    |     +-----------------+     +----------------------+\n+\/\/           |   |   |    |     |     MARKING     | <-> |       YOUNG GC       |\n+\/\/           |   |   +--------- |                 |     | (RSet Parses Region) |\n+\/\/           |   |        |     +-----------------+     +----------------------+\n+\/\/           |   |        |       |\n+\/\/           |   |        |       | Has Evacuation Candidates\n+\/\/           |   |        |       v\n+\/\/           |   |        |     +-----------------+     +--------------------+\n+\/\/           |   |        +---> |    EVACUATING   | <-> |      YOUNG GC      |\n+\/\/           |   +------------- |                 |     | (RSet Uses Bitmap) |\n+\/\/           |                  +-----------------+     +--------------------+\n+\/\/           |                    |\n+\/\/           |                    | Global Cycle Coalesces and Fills Old Regions\n+\/\/           |                    v\n+\/\/           |                  +-----------------+     +--------------------+\n+\/\/           +----------------- |    EVACUATING   | <-> |      YOUNG GC      |\n+\/\/                              |   AFTER GLOBAL  |     | (RSet Uses Bitmap) |\n+\/\/                              +-----------------+     +--------------------+\n+\/\/\n+\/\/\n+void ShenandoahOldGeneration::validate_transition(State new_state) {\n+  ShenandoahGenerationalHeap* heap = ShenandoahGenerationalHeap::heap();\n+  switch (new_state) {\n+    case FILLING:\n+      assert(_state != BOOTSTRAPPING, \"Cannot begin making old regions parsable after bootstrapping\");\n+      assert(is_mark_complete(), \"Cannot begin filling without first completing marking, state is '%s'\", state_name(_state));\n+      assert(_old_heuristics->has_coalesce_and_fill_candidates(), \"Cannot begin filling without something to fill.\");\n+      break;\n+    case WAITING_FOR_BOOTSTRAP:\n+      \/\/ GC cancellation can send us back here from any state.\n+      validate_waiting_for_bootstrap();\n+      break;\n+    case BOOTSTRAPPING:\n+      assert(_state == WAITING_FOR_BOOTSTRAP, \"Cannot reset bitmap without making old regions parsable, state is '%s'\", state_name(_state));\n+      assert(_old_heuristics->unprocessed_old_collection_candidates() == 0, \"Cannot bootstrap with mixed collection candidates\");\n+      assert(!heap->is_prepare_for_old_mark_in_progress(), \"Cannot still be making old regions parsable.\");\n+      break;\n+    case MARKING:\n+      assert(_state == BOOTSTRAPPING, \"Must have finished bootstrapping before marking, state is '%s'\", state_name(_state));\n+      assert(heap->young_generation()->old_gen_task_queues() != nullptr, \"Young generation needs old mark queues.\");\n+      assert(heap->is_concurrent_old_mark_in_progress(), \"Should be marking old now.\");\n+      break;\n+    case EVACUATING_AFTER_GLOBAL:\n+      assert(_state == EVACUATING, \"Must have been evacuating, state is '%s'\", state_name(_state));\n+      break;\n+    case EVACUATING:\n+      assert(_state == WAITING_FOR_BOOTSTRAP || _state == MARKING, \"Cannot have old collection candidates without first marking, state is '%s'\", state_name(_state));\n+      assert(_old_heuristics->unprocessed_old_collection_candidates() > 0, \"Must have collection candidates here.\");\n+      break;\n+    default:\n+      fatal(\"Unknown new state\");\n+  }\n+}\n+\n+bool ShenandoahOldGeneration::validate_waiting_for_bootstrap() {\n+  ShenandoahHeap* heap = ShenandoahHeap::heap();\n+  assert(!heap->is_concurrent_old_mark_in_progress(), \"Cannot become ready for bootstrap during old mark.\");\n+  assert(heap->young_generation()->old_gen_task_queues() == nullptr, \"Cannot become ready for bootstrap when still setup for bootstrapping.\");\n+  assert(!is_concurrent_mark_in_progress(), \"Cannot be marking in IDLE\");\n+  assert(!heap->young_generation()->is_bootstrap_cycle(), \"Cannot have old mark queues if IDLE\");\n+  assert(!_old_heuristics->has_coalesce_and_fill_candidates(), \"Cannot have coalesce and fill candidates in IDLE\");\n+  assert(_old_heuristics->unprocessed_old_collection_candidates() == 0, \"Cannot have mixed collection candidates in IDLE\");\n+  return true;\n+}\n+#endif\n+\n+ShenandoahHeuristics* ShenandoahOldGeneration::initialize_heuristics(ShenandoahMode* gc_mode) {\n+  _old_heuristics = new ShenandoahOldHeuristics(this, ShenandoahGenerationalHeap::heap());\n+  _old_heuristics->set_guaranteed_gc_interval(ShenandoahGuaranteedOldGCInterval);\n+  _heuristics = _old_heuristics;\n+  return _heuristics;\n+}\n+\n+void ShenandoahOldGeneration::record_success_concurrent(bool abbreviated) {\n+  heuristics()->record_success_concurrent();\n+  ShenandoahHeap::heap()->shenandoah_policy()->record_success_old();\n+}\n+\n+void ShenandoahOldGeneration::handle_failed_evacuation() {\n+  if (_failed_evacuation.try_set()) {\n+    log_debug(gc)(\"Old gen evac failure.\");\n+  }\n+}\n+\n+void ShenandoahOldGeneration::handle_failed_promotion(Thread* thread, size_t size) {\n+  \/\/ We squelch excessive reports to reduce noise in logs.\n+  const size_t MaxReportsPerEpoch = 4;\n+  static size_t last_report_epoch = 0;\n+  static size_t epoch_report_count = 0;\n+  auto heap = ShenandoahGenerationalHeap::heap();\n+\n+  size_t promotion_reserve;\n+  size_t promotion_expended;\n+\n+  const size_t gc_id = heap->control_thread()->get_gc_id();\n+\n+  if ((gc_id != last_report_epoch) || (epoch_report_count++ < MaxReportsPerEpoch)) {\n+    {\n+      \/\/ Promotion failures should be very rare.  Invest in providing useful diagnostic info.\n+      ShenandoahHeapLocker locker(heap->lock());\n+      promotion_reserve = get_promoted_reserve();\n+      promotion_expended = get_promoted_expended();\n+    }\n+    PLAB* const plab = ShenandoahThreadLocalData::plab(thread);\n+    const size_t words_remaining = (plab == nullptr)? 0: plab->words_remaining();\n+    const char* promote_enabled = ShenandoahThreadLocalData::allow_plab_promotions(thread)? \"enabled\": \"disabled\";\n+\n+    log_info(gc, ergo)(\"Promotion failed, size \" SIZE_FORMAT \", has plab? %s, PLAB remaining: \" SIZE_FORMAT\n+                       \", plab promotions %s, promotion reserve: \" SIZE_FORMAT \", promotion expended: \" SIZE_FORMAT\n+                       \", old capacity: \" SIZE_FORMAT \", old_used: \" SIZE_FORMAT \", old unaffiliated regions: \" SIZE_FORMAT,\n+                       size * HeapWordSize, plab == nullptr? \"no\": \"yes\",\n+                       words_remaining * HeapWordSize, promote_enabled, promotion_reserve, promotion_expended,\n+                       max_capacity(), used(), free_unaffiliated_regions());\n+\n+    if ((gc_id == last_report_epoch) && (epoch_report_count >= MaxReportsPerEpoch)) {\n+      log_debug(gc, ergo)(\"Squelching additional promotion failure reports for current epoch\");\n+    } else if (gc_id != last_report_epoch) {\n+      last_report_epoch = gc_id;\n+      epoch_report_count = 1;\n+    }\n+  }\n+}\n+\n+void ShenandoahOldGeneration::handle_evacuation(HeapWord* obj, size_t words, bool promotion) {\n+  \/\/ Only register the copy of the object that won the evacuation race.\n+  _card_scan->register_object_without_lock(obj);\n+\n+  \/\/ Mark the entire range of the evacuated object as dirty.  At next remembered set scan,\n+  \/\/ we will clear dirty bits that do not hold interesting pointers.  It's more efficient to\n+  \/\/ do this in batch, in a background GC thread than to try to carefully dirty only cards\n+  \/\/ that hold interesting pointers right now.\n+  _card_scan->mark_range_as_dirty(obj, words);\n+\n+  if (promotion) {\n+    \/\/ This evacuation was a promotion, track this as allocation against old gen\n+    increase_allocated(words * HeapWordSize);\n+  }\n+}\n+\n+bool ShenandoahOldGeneration::has_unprocessed_collection_candidates() {\n+  return _old_heuristics->unprocessed_old_collection_candidates() > 0;\n+}\n+\n+size_t ShenandoahOldGeneration::unprocessed_collection_candidates_live_memory() {\n+  return _old_heuristics->unprocessed_old_collection_candidates_live_memory();\n+}\n+\n+void ShenandoahOldGeneration::abandon_collection_candidates() {\n+  _old_heuristics->abandon_collection_candidates();\n+}\n+\n+void ShenandoahOldGeneration::prepare_for_mixed_collections_after_global_gc() {\n+  assert(is_mark_complete(), \"Expected old generation mark to be complete after global cycle.\");\n+  _old_heuristics->prepare_for_old_collections();\n+  log_info(gc, ergo)(\"After choosing global collection set, mixed candidates: \" UINT32_FORMAT \", coalescing candidates: \" SIZE_FORMAT,\n+               _old_heuristics->unprocessed_old_collection_candidates(),\n+               _old_heuristics->coalesce_and_fill_candidates_count());\n+}\n+\n+void ShenandoahOldGeneration::parallel_heap_region_iterate_free(ShenandoahHeapRegionClosure* cl) {\n+  \/\/ Iterate over old and free regions (exclude young).\n+  ShenandoahExcludeRegionClosure<YOUNG_GENERATION> exclude_cl(cl);\n+  ShenandoahGeneration::parallel_heap_region_iterate_free(&exclude_cl);\n+}\n+\n+void ShenandoahOldGeneration::set_parsable(bool parsable) {\n+  _is_parsable = parsable;\n+  if (_is_parsable) {\n+    \/\/ The current state would have been chosen during final mark of the global\n+    \/\/ collection, _before_ any decisions about class unloading have been made.\n+    \/\/\n+    \/\/ After unloading classes, we have made the old generation regions parsable.\n+    \/\/ We can skip filling or transition to a state that knows everything has\n+    \/\/ already been filled.\n+    switch (state()) {\n+      case ShenandoahOldGeneration::EVACUATING:\n+        transition_to(ShenandoahOldGeneration::EVACUATING_AFTER_GLOBAL);\n+        break;\n+      case ShenandoahOldGeneration::FILLING:\n+        assert(_old_heuristics->unprocessed_old_collection_candidates() == 0, \"Expected no mixed collection candidates\");\n+        assert(_old_heuristics->coalesce_and_fill_candidates_count() > 0, \"Expected coalesce and fill candidates\");\n+        \/\/ When the heuristic put the old generation in this state, it didn't know\n+        \/\/ that we would unload classes and make everything parsable. But, we know\n+        \/\/ that now so we can override this state.\n+        abandon_collection_candidates();\n+        transition_to(ShenandoahOldGeneration::WAITING_FOR_BOOTSTRAP);\n+        break;\n+      default:\n+        \/\/ We can get here during a full GC. The full GC will cancel anything\n+        \/\/ happening in the old generation and return it to the waiting for bootstrap\n+        \/\/ state. The full GC will then record that the old regions are parsable\n+        \/\/ after rebuilding the remembered set.\n+        assert(is_idle(), \"Unexpected state %s at end of global GC\", state_name());\n+        break;\n+    }\n+  }\n+}\n+\n+void ShenandoahOldGeneration::complete_mixed_evacuations() {\n+  assert(is_doing_mixed_evacuations(), \"Mixed evacuations should be in progress\");\n+  if (!_old_heuristics->has_coalesce_and_fill_candidates()) {\n+    \/\/ No candidate regions to coalesce and fill\n+    transition_to(ShenandoahOldGeneration::WAITING_FOR_BOOTSTRAP);\n+    return;\n+  }\n+\n+  if (state() == ShenandoahOldGeneration::EVACUATING) {\n+    transition_to(ShenandoahOldGeneration::FILLING);\n+    return;\n+  }\n+\n+  \/\/ Here, we have no more candidates for mixed collections. The candidates for coalescing\n+  \/\/ and filling have already been processed during the global cycle, so there is nothing\n+  \/\/ more to do.\n+  assert(state() == ShenandoahOldGeneration::EVACUATING_AFTER_GLOBAL, \"Should be evacuating after a global cycle\");\n+  abandon_collection_candidates();\n+  transition_to(ShenandoahOldGeneration::WAITING_FOR_BOOTSTRAP);\n+}\n+\n+void ShenandoahOldGeneration::abandon_mixed_evacuations() {\n+  switch(state()) {\n+    case ShenandoahOldGeneration::EVACUATING:\n+      transition_to(ShenandoahOldGeneration::FILLING);\n+      break;\n+    case ShenandoahOldGeneration::EVACUATING_AFTER_GLOBAL:\n+      abandon_collection_candidates();\n+      transition_to(ShenandoahOldGeneration::WAITING_FOR_BOOTSTRAP);\n+      break;\n+    default:\n+      log_warning(gc)(\"Abandon mixed evacuations in unexpected state: %s\", state_name(state()));\n+      ShouldNotReachHere();\n+      break;\n+  }\n+}\n+\n+void ShenandoahOldGeneration::clear_cards_for(ShenandoahHeapRegion* region) {\n+  _card_scan->mark_range_as_empty(region->bottom(), pointer_delta(region->end(), region->bottom()));\n+}\n+\n+void ShenandoahOldGeneration::mark_card_as_dirty(void* location) {\n+  _card_scan->mark_card_as_dirty((HeapWord*)location);\n+}\n","filename":"src\/hotspot\/share\/gc\/shenandoah\/shenandoahOldGeneration.cpp","additions":831,"deletions":0,"binary":false,"changes":831,"status":"added"},{"patch":"@@ -0,0 +1,325 @@\n+\/*\n+ * Copyright Amazon.com Inc. or its affiliates. All Rights Reserved.\n+ * DO NOT ALTER OR REMOVE COPYRIGHT NOTICES OR THIS FILE HEADER.\n+ *\n+ * This code is free software; you can redistribute it and\/or modify it\n+ * under the terms of the GNU General Public License version 2 only, as\n+ * published by the Free Software Foundation.\n+ *\n+ * This code is distributed in the hope that it will be useful, but WITHOUT\n+ * ANY WARRANTY; without even the implied warranty of MERCHANTABILITY or\n+ * FITNESS FOR A PARTICULAR PURPOSE.  See the GNU General Public License\n+ * version 2 for more details (a copy is included in the LICENSE file that\n+ * accompanied this code).\n+ *\n+ * You should have received a copy of the GNU General Public License version\n+ * 2 along with this work; if not, write to the Free Software Foundation,\n+ * Inc., 51 Franklin St, Fifth Floor, Boston, MA 02110-1301 USA.\n+ *\n+ * Please contact Oracle, 500 Oracle Parkway, Redwood Shores, CA 94065 USA\n+ * or visit www.oracle.com if you need additional information or have any\n+ * questions.\n+ *\n+ *\/\n+\n+#ifndef SHARE_VM_GC_SHENANDOAH_SHENANDOAHOLDGENERATION_HPP\n+#define SHARE_VM_GC_SHENANDOAH_SHENANDOAHOLDGENERATION_HPP\n+\n+#include \"gc\/shenandoah\/heuristics\/shenandoahOldHeuristics.hpp\"\n+#include \"gc\/shenandoah\/shenandoahAllocRequest.hpp\"\n+#include \"gc\/shenandoah\/shenandoahGeneration.hpp\"\n+#include \"gc\/shenandoah\/shenandoahGenerationalHeap.hpp\"\n+#include \"gc\/shenandoah\/shenandoahScanRemembered.hpp\"\n+#include \"gc\/shenandoah\/shenandoahSharedVariables.hpp\"\n+\n+class ShenandoahHeapRegion;\n+class ShenandoahHeapRegionClosure;\n+class ShenandoahOldHeuristics;\n+\n+class ShenandoahOldGeneration : public ShenandoahGeneration {\n+private:\n+  ShenandoahHeapRegion** _coalesce_and_fill_region_array;\n+  ShenandoahOldHeuristics* _old_heuristics;\n+\n+  \/\/ After determining the desired size of the old generation (see compute_old_generation_balance), this\n+  \/\/ quantity represents the number of regions above (surplus) or below (deficit) that size.\n+  \/\/ This value is computed prior to the actual exchange of any regions. A positive value represents\n+  \/\/ a surplus of old regions which will be transferred from old _to_ young. A negative value represents\n+  \/\/ a deficit of regions that will be replenished by a transfer _from_ young to old.\n+  ssize_t _region_balance;\n+\n+  \/\/ Set when evacuation in the old generation fails. When this is set, the control thread will initiate a\n+  \/\/ full GC instead of a futile degenerated cycle.\n+  ShenandoahSharedFlag _failed_evacuation;\n+\n+  \/\/ Bytes reserved within old-gen to hold the results of promotion. This is separate from\n+  \/\/ and in addition to the evacuation reserve for intra-generation evacuations (ShenandoahGeneration::_evacuation_reserve).\n+  \/\/ If there is more data ready to be promoted than can fit within this reserve, the promotion of some objects will be\n+  \/\/ deferred until a subsequent evacuation pass.\n+  size_t _promoted_reserve;\n+\n+  \/\/ Bytes of old-gen memory expended on promotions. This may be modified concurrently\n+  \/\/ by mutators and gc workers when promotion LABs are retired during evacuation. It\n+  \/\/ is therefore always accessed through atomic operations. This is increased when a\n+  \/\/ PLAB is allocated for promotions. The value is decreased by the amount of memory\n+  \/\/ remaining in a PLAB when it is retired.\n+  size_t _promoted_expended;\n+\n+  \/\/ Represents the quantity of live bytes we expect to promote in place during the next\n+  \/\/ evacuation cycle. This value is used by the young heuristic to trigger mixed collections.\n+  \/\/ It is also used when computing the optimum size for the old generation.\n+  size_t _promotion_potential;\n+\n+  \/\/ When a region is selected to be promoted in place, the remaining free memory is filled\n+  \/\/ in to prevent additional allocations (preventing premature promotion of newly allocated\n+  \/\/ objects. This field records the total amount of padding used for such regions.\n+  size_t _pad_for_promote_in_place;\n+\n+  \/\/ During construction of the collection set, we keep track of regions that are eligible\n+  \/\/ for promotion in place. These fields track the count of those humongous and regular regions.\n+  \/\/ This data is used to force the evacuation phase even when the collection set is otherwise\n+  \/\/ empty.\n+  size_t _promotable_humongous_regions;\n+  size_t _promotable_regular_regions;\n+\n+  \/\/ True if old regions may be safely traversed by the remembered set scan.\n+  bool _is_parsable;\n+\n+  bool coalesce_and_fill();\n+\n+public:\n+  ShenandoahOldGeneration(uint max_queues, size_t max_capacity);\n+\n+  ShenandoahHeuristics* initialize_heuristics(ShenandoahMode* gc_mode) override;\n+\n+  const char* name() const override {\n+    return \"Old\";\n+  }\n+\n+  ShenandoahOldHeuristics* heuristics() const override {\n+    return _old_heuristics;\n+  }\n+\n+  \/\/ See description in field declaration\n+  void set_promoted_reserve(size_t new_val);\n+  size_t get_promoted_reserve() const;\n+\n+  \/\/ The promotion reserve is increased when rebuilding the free set transfers a region to the old generation\n+  void augment_promoted_reserve(size_t increment);\n+\n+  \/\/ This zeros out the expended promotion count after the promotion reserve is computed\n+  void reset_promoted_expended();\n+\n+  \/\/ This is incremented when allocations are made to copy promotions into the old generation\n+  size_t expend_promoted(size_t increment);\n+\n+  \/\/ This is used to return unused memory from a retired promotion LAB\n+  size_t unexpend_promoted(size_t decrement);\n+\n+  \/\/ This is used on the allocation path to gate promotions that would exceed the reserve\n+  size_t get_promoted_expended() const;\n+\n+  \/\/ Test if there is enough memory reserved for this promotion\n+  bool can_promote(size_t requested_bytes) const {\n+    size_t promotion_avail = get_promoted_reserve();\n+    size_t promotion_expended = get_promoted_expended();\n+    return promotion_expended + requested_bytes <= promotion_avail;\n+  }\n+\n+  \/\/ Test if there is enough memory available in the old generation to accommodate this request.\n+  \/\/ The request will be subject to constraints on promotion and evacuation reserves.\n+  bool can_allocate(const ShenandoahAllocRequest& req) const;\n+\n+  \/\/ Updates the promotion expenditure tracking and configures whether the plab may be used\n+  \/\/ for promotions and evacuations, or just evacuations.\n+  void configure_plab_for_current_thread(const ShenandoahAllocRequest &req);\n+\n+  \/\/ See description in field declaration\n+  void set_region_balance(ssize_t balance) { _region_balance = balance; }\n+  ssize_t get_region_balance() const { return _region_balance; }\n+  \/\/ See description in field declaration\n+  void set_promotion_potential(size_t val) { _promotion_potential = val; };\n+  size_t get_promotion_potential() const { return _promotion_potential; };\n+\n+  \/\/ See description in field declaration\n+  void set_pad_for_promote_in_place(size_t pad) { _pad_for_promote_in_place = pad; }\n+  size_t get_pad_for_promote_in_place() const { return _pad_for_promote_in_place; }\n+\n+  \/\/ See description in field declaration\n+  void set_expected_humongous_region_promotions(size_t region_count) { _promotable_humongous_regions = region_count; }\n+  void set_expected_regular_region_promotions(size_t region_count) { _promotable_regular_regions = region_count; }\n+  size_t get_expected_in_place_promotions() const { return _promotable_humongous_regions + _promotable_regular_regions; }\n+  bool has_in_place_promotions() const { return get_expected_in_place_promotions() > 0; }\n+\n+  \/\/ Class unloading may render the card table offsets unusable, if they refer to unmarked objects\n+  bool is_parsable() const   { return _is_parsable; }\n+  void set_parsable(bool parsable);\n+\n+  \/\/ This will signal the heuristic to trigger an old generation collection\n+  void handle_failed_transfer();\n+\n+  \/\/ This will signal the control thread to run a full GC instead of a futile degenerated gc\n+  void handle_failed_evacuation();\n+\n+  \/\/ This logs that an evacuation to the old generation has failed\n+  void handle_failed_promotion(Thread* thread, size_t size);\n+\n+  \/\/ A successful evacuation re-dirties the cards and registers the object with the remembered set\n+  void handle_evacuation(HeapWord* obj, size_t words, bool promotion);\n+\n+  \/\/ Clear the flag after it is consumed by the control thread\n+  bool clear_failed_evacuation() {\n+    return _failed_evacuation.try_unset();\n+  }\n+\n+  \/\/ Transition to the next state after mixed evacuations have completed\n+  void complete_mixed_evacuations();\n+\n+  \/\/ Abandon any future mixed collections. This is invoked when all old regions eligible for\n+  \/\/ inclusion in a mixed evacuation are pinned. This should be rare.\n+  void abandon_mixed_evacuations();\n+\n+private:\n+  ShenandoahScanRemembered* _card_scan;\n+\n+public:\n+  ShenandoahScanRemembered* card_scan() { return _card_scan; }\n+\n+  \/\/ Clear cards for given region\n+  void clear_cards_for(ShenandoahHeapRegion* region);\n+\n+  \/\/ Mark card for this location as dirty\n+  void mark_card_as_dirty(void* location);\n+\n+  void parallel_heap_region_iterate(ShenandoahHeapRegionClosure* cl) override;\n+\n+  void parallel_heap_region_iterate_free(ShenandoahHeapRegionClosure* cl) override;\n+\n+  void heap_region_iterate(ShenandoahHeapRegionClosure* cl) override;\n+\n+  bool contains(ShenandoahAffiliation affiliation) const override;\n+  bool contains(ShenandoahHeapRegion* region) const override;\n+  bool contains(oop obj) const override;\n+\n+  void set_concurrent_mark_in_progress(bool in_progress) override;\n+  bool is_concurrent_mark_in_progress() override;\n+\n+  bool entry_coalesce_and_fill();\n+  void prepare_for_mixed_collections_after_global_gc();\n+  void prepare_gc() override;\n+  void prepare_regions_and_collection_set(bool concurrent) override;\n+  void record_success_concurrent(bool abbreviated) override;\n+  void cancel_marking() override;\n+\n+  \/\/ Cancels old gc and transitions to the idle state\n+  void cancel_gc();\n+\n+  \/\/ We leave the SATB barrier on for the entirety of the old generation\n+  \/\/ marking phase. In some cases, this can cause a write to a perfectly\n+  \/\/ reachable oop to enqueue a pointer that later becomes garbage (because\n+  \/\/ it points at an object that is later chosen for the collection set). There are\n+  \/\/ also cases where the referent of a weak reference ends up in the SATB\n+  \/\/ and is later collected. In these cases the oop in the SATB buffer becomes\n+  \/\/ invalid and the _next_ cycle will crash during its marking phase. To\n+  \/\/ avoid this problem, we \"purge\" the SATB buffers during the final update\n+  \/\/ references phase if (and only if) an old generation mark is in progress.\n+  \/\/ At this stage we can safely determine if any of the oops in the SATB\n+  \/\/ buffer belong to trashed regions (before they are recycled). As it\n+  \/\/ happens, flushing a SATB queue also filters out oops which have already\n+  \/\/ been marked - which is the case for anything that is being evacuated\n+  \/\/ from the collection set.\n+  \/\/\n+  \/\/ Alternatively, we could inspect the state of the heap and the age of the\n+  \/\/ object at the barrier, but we reject this approach because it is likely\n+  \/\/ the performance impact would be too severe.\n+  void transfer_pointers_from_satb() const;\n+  void concurrent_transfer_pointers_from_satb() const;\n+\n+  \/\/ True if there are old regions waiting to be selected for a mixed collection\n+  bool has_unprocessed_collection_candidates();\n+\n+  bool is_doing_mixed_evacuations() const {\n+    return state() == EVACUATING || state() == EVACUATING_AFTER_GLOBAL;\n+  }\n+\n+  bool is_preparing_for_mark() const {\n+    return state() == FILLING;\n+  }\n+\n+  bool is_idle() const {\n+    return state() == WAITING_FOR_BOOTSTRAP;\n+  }\n+\n+  bool is_bootstrapping() const {\n+    return state() == BOOTSTRAPPING;\n+  }\n+\n+  \/\/ Amount of live memory (bytes) in regions waiting for mixed collections\n+  size_t unprocessed_collection_candidates_live_memory();\n+\n+  \/\/ Abandon any regions waiting for mixed collections\n+  void abandon_collection_candidates();\n+\n+public:\n+  enum State {\n+    FILLING, WAITING_FOR_BOOTSTRAP, BOOTSTRAPPING, MARKING, EVACUATING, EVACUATING_AFTER_GLOBAL\n+  };\n+\n+#ifdef ASSERT\n+  bool validate_waiting_for_bootstrap();\n+#endif\n+\n+private:\n+  State _state;\n+\n+  static const size_t FRACTIONAL_DENOMINATOR = 65536;\n+\n+  \/\/ During initialization of the JVM, we search for the correct old-gen size by initially performing old-gen\n+  \/\/ collection when old-gen usage is 50% more (INITIAL_GROWTH_BEFORE_COMPACTION) than the initial old-gen size\n+  \/\/ estimate (3.125% of heap).  The next old-gen trigger occurs when old-gen grows 25% larger than its live\n+  \/\/ memory at the end of the first old-gen collection.  Then we trigger again when old-gen grows 12.5%\n+  \/\/ more than its live memory at the end of the previous old-gen collection.  Thereafter, we trigger each time\n+  \/\/ old-gen grows more than 12.5% following the end of its previous old-gen collection.\n+  static const size_t INITIAL_GROWTH_BEFORE_COMPACTION = FRACTIONAL_DENOMINATOR \/ 2;        \/\/  50.0%\n+\n+  \/\/ INITIAL_LIVE_FRACTION represents the initial guess of how large old-gen should be.  We estimate that old-gen\n+  \/\/ needs to consume 6.25% of the total heap size.  And we \"pretend\" that we start out with this amount of live\n+  \/\/ old-gen memory.  The first old-collection trigger will occur when old-gen occupies 50% more than this initial\n+  \/\/ approximation of the old-gen memory requirement, in other words when old-gen usage is 150% of 6.25%, which\n+  \/\/ is 9.375% of the total heap size.\n+  static const uint16_t INITIAL_LIVE_FRACTION = FRACTIONAL_DENOMINATOR \/ 16;                \/\/   6.25%\n+\n+  size_t _live_bytes_after_last_mark;\n+\n+  \/\/ How much growth in usage before we trigger old collection, per FRACTIONAL_DENOMINATOR (65_536)\n+  size_t _growth_before_compaction;\n+  const size_t _min_growth_before_compaction;                                               \/\/ Default is 12.5%\n+\n+  void validate_transition(State new_state) NOT_DEBUG_RETURN;\n+\n+public:\n+  State state() const {\n+    return _state;\n+  }\n+\n+  const char* state_name() const {\n+    return state_name(_state);\n+  }\n+\n+  void transition_to(State new_state);\n+\n+  size_t get_live_bytes_after_last_mark() const;\n+  void set_live_bytes_after_last_mark(size_t new_live);\n+\n+  size_t usage_trigger_threshold() const;\n+\n+  bool can_start_gc() {\n+    return _state == WAITING_FOR_BOOTSTRAP;\n+  }\n+\n+  static const char* state_name(State state);\n+\n+};\n+\n+\n+#endif \/\/SHARE_VM_GC_SHENANDOAH_SHENANDOAHOLDGENERATION_HPP\n","filename":"src\/hotspot\/share\/gc\/shenandoah\/shenandoahOldGeneration.hpp","additions":325,"deletions":0,"binary":false,"changes":325,"status":"added"},{"patch":"@@ -3,0 +3,1 @@\n+ * Copyright Amazon.com Inc. or its affiliates. All Rights Reserved.\n@@ -30,0 +31,1 @@\n+#include \"gc\/shenandoah\/shenandoahGenerationType.hpp\"\n@@ -36,6 +38,0 @@\n-enum StringDedupMode {\n-  NO_DEDUP,      \/\/ Do not do anything for String deduplication\n-  ENQUEUE_DEDUP, \/\/ Enqueue candidate Strings for deduplication, if meet age threshold\n-  ALWAYS_DEDUP   \/\/ Enqueue Strings for deduplication\n-};\n-\n@@ -45,0 +41,1 @@\n+  ShenandoahObjToScanQueue* _old_queue;\n@@ -49,1 +46,1 @@\n-  template <class T>\n+  template <class T, ShenandoahGenerationType GENERATION>\n@@ -53,1 +50,1 @@\n-  ShenandoahMarkRefsSuperClosure(ShenandoahObjToScanQueue* q, ShenandoahReferenceProcessor* rp);\n+  ShenandoahMarkRefsSuperClosure(ShenandoahObjToScanQueue* q, ShenandoahReferenceProcessor* rp, ShenandoahObjToScanQueue* old_q);\n@@ -73,1 +70,1 @@\n-  template <class T>\n+  template <class T, ShenandoahGenerationType GENERATION>\n@@ -77,2 +74,2 @@\n-  ShenandoahMarkUpdateRefsSuperClosure(ShenandoahObjToScanQueue* q, ShenandoahReferenceProcessor* rp) :\n-    ShenandoahMarkRefsSuperClosure(q, rp),\n+  ShenandoahMarkUpdateRefsSuperClosure(ShenandoahObjToScanQueue* q, ShenandoahReferenceProcessor* rp, ShenandoahObjToScanQueue* old_q) :\n+    ShenandoahMarkRefsSuperClosure(q, rp, old_q),\n@@ -84,0 +81,1 @@\n+template <ShenandoahGenerationType GENERATION>\n@@ -87,1 +85,1 @@\n-  inline void do_oop_work(T* p)     { work<T>(p); }\n+  inline void do_oop_work(T* p)     { work<T, GENERATION>(p); }\n@@ -90,2 +88,2 @@\n-  ShenandoahMarkUpdateRefsClosure(ShenandoahObjToScanQueue* q, ShenandoahReferenceProcessor* rp) :\n-    ShenandoahMarkUpdateRefsSuperClosure(q, rp) {}\n+  ShenandoahMarkUpdateRefsClosure(ShenandoahObjToScanQueue* q, ShenandoahReferenceProcessor* rp, ShenandoahObjToScanQueue* old_q) :\n+    ShenandoahMarkUpdateRefsSuperClosure(q, rp, old_q) {}\n@@ -97,0 +95,1 @@\n+template <ShenandoahGenerationType GENERATION>\n@@ -100,1 +99,1 @@\n-  inline void do_oop_work(T* p)     { work<T>(p); }\n+  inline void do_oop_work(T* p)     { work<T, GENERATION>(p); }\n@@ -103,2 +102,2 @@\n-  ShenandoahMarkRefsClosure(ShenandoahObjToScanQueue* q, ShenandoahReferenceProcessor* rp) :\n-    ShenandoahMarkRefsSuperClosure(q, rp) {};\n+  ShenandoahMarkRefsClosure(ShenandoahObjToScanQueue* q, ShenandoahReferenceProcessor* rp, ShenandoahObjToScanQueue* old_q) :\n+    ShenandoahMarkRefsSuperClosure(q, rp, old_q) {};\n@@ -110,1 +109,0 @@\n-\n","filename":"src\/hotspot\/share\/gc\/shenandoah\/shenandoahOopClosures.hpp","additions":16,"deletions":18,"binary":false,"changes":34,"status":"modified"},{"patch":"@@ -3,0 +3,1 @@\n+ * Copyright Amazon.com Inc. or its affiliates. All Rights Reserved.\n@@ -33,1 +34,1 @@\n-template<class T>\n+template<class T, ShenandoahGenerationType GENERATION>\n@@ -35,1 +36,1 @@\n-  ShenandoahMark::mark_through_ref<T>(p, _queue, _mark_context, _weak);\n+  ShenandoahMark::mark_through_ref<T, GENERATION>(p, _queue, _old_queue, _mark_context, _weak);\n@@ -38,1 +39,1 @@\n-template<class T>\n+template<class T, ShenandoahGenerationType GENERATION>\n@@ -44,1 +45,1 @@\n-  ShenandoahMarkRefsSuperClosure::work<T>(p);\n+  ShenandoahMarkRefsSuperClosure::work<T, GENERATION>(p);\n","filename":"src\/hotspot\/share\/gc\/shenandoah\/shenandoahOopClosures.inline.hpp","additions":5,"deletions":4,"binary":false,"changes":9,"status":"modified"},{"patch":"@@ -107,1 +107,1 @@\n-void ShenandoahPacer::setup_for_updaterefs() {\n+void ShenandoahPacer::setup_for_update_refs() {\n@@ -192,1 +192,2 @@\n-bool ShenandoahPacer::claim_for_alloc(size_t words, bool force) {\n+template<bool FORCE>\n+bool ShenandoahPacer::claim_for_alloc(size_t words) {\n@@ -201,1 +202,1 @@\n-    if (cur < tax && !force) {\n+    if (cur < tax && !FORCE) {\n@@ -210,0 +211,3 @@\n+template bool ShenandoahPacer::claim_for_alloc<true>(size_t words);\n+template bool ShenandoahPacer::claim_for_alloc<false>(size_t words);\n+\n@@ -230,1 +234,1 @@\n-  bool claimed = claim_for_alloc(words, false);\n+  bool claimed = claim_for_alloc<false>(words);\n@@ -235,7 +239,0 @@\n-  \/\/ Forcefully claim the budget: it may go negative at this point, and\n-  \/\/ GC should replenish for this and subsequent allocations. After this claim,\n-  \/\/ we would wait a bit until our claim is matched by additional progress,\n-  \/\/ or the time budget depletes.\n-  claimed = claim_for_alloc(words, true);\n-  assert(claimed, \"Should always succeed\");\n-\n@@ -252,0 +249,1 @@\n+    claim_for_alloc<true>(words);\n@@ -255,6 +253,3 @@\n-  double start = os::elapsedTime();\n-\n-  size_t max_ms = ShenandoahPacingMaxDelay;\n-  size_t total_ms = 0;\n-\n-  while (true) {\n+  jlong const max_delay = ShenandoahPacingMaxDelay * NANOSECS_PER_MILLISEC;\n+  jlong const start_time = os::elapsed_counter();\n+  while (!claimed && (os::elapsed_counter() - start_time) < max_delay) {\n@@ -262,15 +257,9 @@\n-    size_t cur_ms = (max_ms > total_ms) ? (max_ms - total_ms) : 1;\n-    wait(cur_ms);\n-\n-    double end = os::elapsedTime();\n-    total_ms = (size_t)((end - start) * 1000);\n-\n-    if (total_ms > max_ms || Atomic::load(&_budget) >= 0) {\n-      \/\/ Exiting if either:\n-      \/\/  a) Spent local time budget to wait for enough GC progress.\n-      \/\/     Breaking out and allocating anyway, which may mean we outpace GC,\n-      \/\/     and start Degenerated GC cycle.\n-      \/\/  b) The budget had been replenished, which means our claim is satisfied.\n-      ShenandoahThreadLocalData::add_paced_time(JavaThread::current(), end - start);\n-      break;\n-    }\n+    wait(1);\n+    claimed = claim_for_alloc<false>(words);\n+  }\n+  if (!claimed) {\n+    \/\/ Spent local time budget to wait for enough GC progress.\n+    \/\/ Force allocating anyway, which may mean we outpace GC,\n+    \/\/ and start Degenerated GC cycle.\n+    claimed = claim_for_alloc<true>(words);\n+    assert(claimed, \"Should always succeed\");\n@@ -278,0 +267,1 @@\n+  ShenandoahThreadLocalData::add_paced_time(current, (double)(os::elapsed_counter() - start_time) \/ NANOSECS_PER_SEC);\n@@ -341,0 +331,5 @@\n+\n+void ShenandoahPeriodicPacerNotifyTask::task() {\n+  assert(ShenandoahPacing, \"Should not be here otherwise\");\n+  _pacer->notify_waiters();\n+}\n","filename":"src\/hotspot\/share\/gc\/shenandoah\/shenandoahPacer.cpp","additions":27,"deletions":32,"binary":false,"changes":59,"status":"modified"},{"patch":"@@ -30,0 +30,1 @@\n+#include \"gc\/shenandoah\/shenandoahSharedVariables.hpp\"\n@@ -31,0 +32,1 @@\n+#include \"runtime\/task.hpp\"\n@@ -33,0 +35,15 @@\n+class ShenandoahPacer;\n+\n+\n+\/\/ Periodic task to notify blocked paced waiters.\n+class ShenandoahPeriodicPacerNotifyTask : public PeriodicTask {\n+private:\n+  ShenandoahPacer* const _pacer;\n+public:\n+  explicit ShenandoahPeriodicPacerNotifyTask(ShenandoahPacer* pacer) :\n+    PeriodicTask(PeriodicTask::min_interval),\n+    _pacer(pacer) { }\n+\n+  void task() override;\n+};\n+\n@@ -50,0 +67,1 @@\n+  ShenandoahPeriodicPacerNotifyTask _notify_waiters_task;\n@@ -66,1 +84,1 @@\n-  ShenandoahPacer(ShenandoahHeap* heap) :\n+  explicit ShenandoahPacer(ShenandoahHeap* heap) :\n@@ -71,0 +89,1 @@\n+          _notify_waiters_task(this),\n@@ -74,1 +93,3 @@\n-          _progress(PACING_PROGRESS_UNINIT) {}\n+          _progress(PACING_PROGRESS_UNINIT) {\n+    _notify_waiters_task.enroll();\n+  }\n@@ -79,1 +100,1 @@\n-  void setup_for_updaterefs();\n+  void setup_for_update_refs();\n@@ -85,1 +106,1 @@\n-  inline void report_updaterefs(size_t words);\n+  inline void report_update_refs(size_t words);\n@@ -89,1 +110,3 @@\n-  bool claim_for_alloc(size_t words, bool force);\n+  template<bool FORCE>\n+  bool claim_for_alloc(size_t words);\n+\n","filename":"src\/hotspot\/share\/gc\/shenandoah\/shenandoahPacer.hpp","additions":28,"deletions":5,"binary":false,"changes":33,"status":"modified"},{"patch":"@@ -41,1 +41,1 @@\n-inline void ShenandoahPacer::report_updaterefs(size_t words) {\n+inline void ShenandoahPacer::report_update_refs(size_t words) {\n","filename":"src\/hotspot\/share\/gc\/shenandoah\/shenandoahPacer.inline.hpp","additions":1,"deletions":1,"binary":false,"changes":2,"status":"modified"},{"patch":"@@ -3,0 +3,1 @@\n+ * Copyright Amazon.com Inc. or its affiliates. All Rights Reserved.\n@@ -100,0 +101,1 @@\n+    case init_scan_rset:\n@@ -111,0 +113,1 @@\n+    case degen_gc_coalesce_and_fill:\n@@ -115,0 +118,1 @@\n+    case conc_mark:\n@@ -120,0 +124,2 @@\n+    case conc_coalesce_and_fill:\n+    case promote_in_place:\n@@ -140,1 +146,5 @@\n-void ShenandoahPhaseTimings::set_cycle_data(Phase phase, double time) {\n+void ShenandoahPhaseTimings::set_cycle_data(Phase phase, double time, bool should_aggregate) {\n+  const double cycle_data = _cycle_data[phase];\n+  if (should_aggregate) {\n+    _cycle_data[phase] = (cycle_data == uninitialized()) ? time :  (cycle_data + time);\n+  } else {\n@@ -142,2 +152,1 @@\n-  double d = _cycle_data[phase];\n-  assert(d == uninitialized(), \"Should not be set yet: %s, current value: %lf\", phase_name(phase), d);\n+    assert(cycle_data == uninitialized(), \"Should not be set yet: %s, current value: %lf\", phase_name(phase), cycle_data);\n@@ -145,1 +154,2 @@\n-  _cycle_data[phase] = time;\n+    _cycle_data[phase] = time;\n+  }\n@@ -148,1 +158,1 @@\n-void ShenandoahPhaseTimings::record_phase_time(Phase phase, double time) {\n+void ShenandoahPhaseTimings::record_phase_time(Phase phase, double time, bool should_aggregate) {\n@@ -150,1 +160,1 @@\n-    set_cycle_data(phase, time);\n+    set_cycle_data(phase, time, should_aggregate);\n@@ -311,1 +321,1 @@\n-        ShenandoahPhaseTimings::ParPhase par_phase, uint worker_id) :\n+        ShenandoahPhaseTimings::ParPhase par_phase, uint worker_id, bool cumulative) :\n@@ -315,1 +325,1 @@\n-  assert(_timings->worker_data(_phase, _par_phase)->get(_worker_id) == ShenandoahWorkerData::uninitialized(),\n+  assert(_timings->worker_data(_phase, _par_phase)->get(_worker_id) == ShenandoahWorkerData::uninitialized() || cumulative,\n@@ -321,1 +331,1 @@\n-  _timings->worker_data(_phase, _par_phase)->set(_worker_id, os::elapsedTime() - _start_time);\n+  _timings->worker_data(_phase, _par_phase)->set_or_add(_worker_id, os::elapsedTime() - _start_time);\n@@ -329,1 +339,0 @@\n-\n","filename":"src\/hotspot\/share\/gc\/shenandoah\/shenandoahPhaseTimings.cpp","additions":19,"deletions":10,"binary":false,"changes":29,"status":"modified"},{"patch":"@@ -3,0 +3,1 @@\n+ * Copyright Amazon.com Inc. or its affiliates. All Rights Reserved.\n@@ -47,0 +48,1 @@\n+  f(CNT_PREFIX ## ScanClusters,             DESC_PREFIX \"Scan Clusters\")               \\\n@@ -51,1 +53,2 @@\n-                                                                                       \\\n+  f(conc_reset_after_collect,                       \"Concurrent Reset After Collect\")  \\\n+  f(conc_reset_old,                                 \"Concurrent Reset (OLD)\")          \\\n@@ -54,0 +57,1 @@\n+  f(init_mark_verify,                               \"  Verify\")                        \\\n@@ -55,0 +59,2 @@\n+  f(init_swap_rset,                                 \"  Swap Remembered Set\")           \\\n+  f(init_transfer_satb,                             \"  Transfer Old From SATB\")        \\\n@@ -56,0 +62,4 @@\n+  f(init_propagate_gc_state,                        \"  Propagate GC State\")            \\\n+                                                                                       \\\n+  f(init_scan_rset,                                 \"Concurrent Scan Remembered Set\")  \\\n+  SHENANDOAH_PAR_PHASE_DO(init_scan_rset_,          \"  RS: \", f)                       \\\n@@ -60,0 +70,2 @@\n+  SHENANDOAH_PAR_PHASE_DO(conc_mark,                \"  CM: \", f)                       \\\n+  f(conc_mark_satb_flush,                           \"  Flush SATB\")                    \\\n@@ -63,0 +75,1 @@\n+  f(final_mark_verify,                              \"  Verify\")                        \\\n@@ -64,0 +77,1 @@\n+  f(final_mark_propagate_gc_state,                  \"  Propagate GC State\")            \\\n@@ -98,3 +112,4 @@\n-                                                                                       \\\n-  f(final_roots_gross,                              \"Pause Final Roots (G)\")           \\\n-  f(final_roots,                                    \"Pause Final Roots (N)\")           \\\n+  f(conc_final_roots,                               \"Concurrent Final Roots\")          \\\n+  f(promote_in_place,                               \"  Promote Regions\")               \\\n+  f(final_roots_gross,                              \"Pause Verify Final Roots (G)\")    \\\n+  f(final_roots,                                    \"Pause Verify Final Roots (N)\")    \\\n@@ -104,1 +119,1 @@\n-  f(init_update_refs_manage_gclabs,                 \"  Manage GCLABs\")                 \\\n+  f(init_update_refs_verify,                        \"  Verify\")                        \\\n@@ -106,0 +121,1 @@\n+  f(conc_update_refs_prepare,                       \"Concurrent Update Refs Prepare\")  \\\n@@ -111,1 +127,1 @@\n-  f(final_update_refs_finish_work,                  \"  Finish Work\")                   \\\n+  f(final_update_refs_verify,                       \"  Verify\")                        \\\n@@ -113,0 +129,1 @@\n+  f(final_update_refs_transfer_satb,                \"  Transfer Old From SATB\")        \\\n@@ -115,0 +132,1 @@\n+  f(final_update_refs_propagate_gc_state,           \"  Propagate GC State\")            \\\n@@ -117,0 +135,2 @@\n+  f(conc_coalesce_and_fill,                         \"Concurrent Coalesce and Fill\")    \\\n+  SHENANDOAH_PAR_PHASE_DO(conc_coalesce_,           \"  CC&F: \", f)                     \\\n@@ -138,2 +158,1 @@\n-  f(degen_gc_updaterefs,                            \"  Update References\")             \\\n-  f(degen_gc_final_update_refs_finish_work,         \"  Finish Work\")                   \\\n+  f(degen_gc_update_refs,                           \"  Update References\")             \\\n@@ -146,0 +165,4 @@\n+  f(degen_gc_promote_regions,                       \"  Degen Promote Regions\")         \\\n+  f(degen_gc_coalesce_and_fill,                     \"  Degen Coalesce and Fill\")       \\\n+  SHENANDOAH_PAR_PHASE_DO(degen_coalesce_,          \"    DC&F\", f)                     \\\n+  f(degen_gc_propagate_gc_state,                    \"  Propagate GC State\")            \\\n@@ -172,0 +195,1 @@\n+  f(full_gc_recompute_generation_usage,             \"    Recompute generation usage\")  \\\n@@ -174,0 +198,1 @@\n+  f(full_gc_reconstruct_remembered_set,             \"    Reconstruct Remembered Set\")  \\\n@@ -175,0 +200,1 @@\n+  f(full_gc_propagate_gc_state,                     \"  Propagate GC State\")            \\\n@@ -176,1 +202,0 @@\n-  f(conc_uncommit,                                  \"Concurrent Uncommit\")             \\\n@@ -219,1 +244,1 @@\n-  void set_cycle_data(Phase phase, double time);\n+  void set_cycle_data(Phase phase, double time, bool should_aggregate = false);\n@@ -225,1 +250,1 @@\n-  void record_phase_time(Phase phase, double time);\n+  void record_phase_time(Phase phase, double time, bool should_aggregate = false);\n@@ -252,1 +277,4 @@\n-  ShenandoahWorkerTimingsTracker(ShenandoahPhaseTimings::Phase phase, ShenandoahPhaseTimings::ParPhase par_phase, uint worker_id);\n+  ShenandoahWorkerTimingsTracker(ShenandoahPhaseTimings::Phase phase,\n+                                 ShenandoahPhaseTimings::ParPhase par_phase,\n+                                 uint worker_id,\n+                                 bool cumulative = false);\n","filename":"src\/hotspot\/share\/gc\/shenandoah\/shenandoahPhaseTimings.hpp","additions":40,"deletions":12,"binary":false,"changes":52,"status":"modified"},{"patch":"@@ -4,0 +4,1 @@\n+ * Copyright Amazon.com Inc. or its affiliates. All Rights Reserved.\n@@ -29,0 +30,1 @@\n+#include \"gc\/shenandoah\/shenandoahGeneration.hpp\"\n@@ -31,0 +33,1 @@\n+#include \"gc\/shenandoah\/shenandoahScanRemembered.inline.hpp\"\n@@ -60,0 +63,17 @@\n+template <typename T>\n+static void card_mark_barrier(T* field, oop value) {\n+  assert(ShenandoahCardBarrier, \"Card-mark barrier should be on\");\n+  ShenandoahGenerationalHeap* heap = ShenandoahGenerationalHeap::heap();\n+  assert(heap->is_in_or_null(value), \"Should be in heap\");\n+  if (heap->is_in_old(field) && heap->is_in_young(value)) {\n+    \/\/ For Shenandoah, each generation collects all the _referents_ that belong to the\n+    \/\/ collected generation. We can end up with discovered lists that contain a mixture\n+    \/\/ of old and young _references_. These references are linked together through the\n+    \/\/ discovered field in java.lang.Reference. In some cases, creating or editing this\n+    \/\/ list may result in the creation of _new_ old-to-young pointers which must dirty\n+    \/\/ the corresponding card. Failing to do this may cause heap verification errors and\n+    \/\/ lead to incorrect GC behavior.\n+    heap->old_generation()->mark_card_as_dirty(field);\n+  }\n+}\n+\n@@ -66,0 +86,3 @@\n+  if (ShenandoahCardBarrier) {\n+    card_mark_barrier(field, value);\n+  }\n@@ -71,0 +94,3 @@\n+  if (ShenandoahCardBarrier) {\n+    card_mark_barrier(field, value);\n+  }\n@@ -271,0 +297,1 @@\n+  ShenandoahHeap* heap = ShenandoahHeap::heap();\n@@ -287,0 +314,5 @@\n+  if (!heap->is_in_active_generation(referent)) {\n+    log_trace(gc,ref)(\"Referent outside of active generation: \" PTR_FORMAT, p2i(referent));\n+    return false;\n+  }\n+\n@@ -300,0 +332,1 @@\n+  ShenandoahHeap* heap = ShenandoahHeap::heap();\n@@ -303,1 +336,1 @@\n-    return ShenandoahHeap::heap()->complete_marking_context()->is_marked(raw_referent);\n+    return heap->active_generation()->complete_marking_context()->is_marked(raw_referent);\n@@ -305,1 +338,1 @@\n-    return ShenandoahHeap::heap()->complete_marking_context()->is_marked_strong(raw_referent);\n+    return heap->active_generation()->complete_marking_context()->is_marked_strong(raw_referent);\n@@ -317,1 +350,1 @@\n-    assert(ShenandoahHeap::heap()->marking_context()->is_marked(reference_referent_raw<T>(reference)), \"only make inactive final refs with alive referents\");\n+    assert(ShenandoahHeap::heap()->active_generation()->complete_marking_context()->is_marked(reference_referent_raw<T>(reference)), \"only make inactive final refs with alive referents\");\n@@ -352,0 +385,3 @@\n+  \/\/ Each worker thread has a private copy of refproc_data, which includes a private discovered list.  This means\n+  \/\/ there's no risk that a different worker thread will try to manipulate my discovered list head while I'm making\n+  \/\/ reference the head of my discovered list.\n@@ -360,0 +396,11 @@\n+    \/\/ We successfully set this reference object's next pointer to discovered_head.  This marks reference as discovered.\n+    \/\/ If reference_cas_discovered fails, that means some other worker thread took credit for discovery of this reference,\n+    \/\/ and that other thread will place reference on its discovered list, so I can ignore reference.\n+\n+    \/\/ In case we have created an interesting pointer, mark the remembered set card as dirty.\n+    if (ShenandoahCardBarrier) {\n+      T* addr = reinterpret_cast<T*>(java_lang_ref_Reference::discovered_addr_raw(reference));\n+      card_mark_barrier(addr, discovered_head);\n+    }\n+\n+    \/\/ Make the discovered_list_head point to reference.\n@@ -374,1 +421,2 @@\n-  log_trace(gc, ref)(\"Encountered Reference: \" PTR_FORMAT \" (%s)\", p2i(reference), reference_type_name(type));\n+  log_trace(gc, ref)(\"Encountered Reference: \" PTR_FORMAT \" (%s, %s)\",\n+          p2i(reference), reference_type_name(type), ShenandoahHeap::heap()->heap_region_containing(reference)->affiliation_name());\n@@ -389,5 +437,3 @@\n-#ifdef ASSERT\n-  HeapWord* raw_referent = reference_referent_raw<T>(reference);\n-  assert(raw_referent == nullptr || ShenandoahHeap::heap()->marking_context()->is_marked(raw_referent),\n-         \"only drop references with alive referents\");\n-#endif\n+  ShenandoahHeap* heap = ShenandoahHeap::heap();\n+  HeapWord* referent = reference_referent_raw<T>(reference);\n+  assert(referent == nullptr || heap->marking_context()->is_marked(referent), \"only drop references with alive referents\");\n@@ -398,0 +444,7 @@\n+  \/\/ When this reference was discovered, it would not have been marked. If it ends up surviving\n+  \/\/ the cycle, we need to dirty the card if the reference is old and the referent is young.  Note\n+  \/\/ that if the reference is not dropped, then its pointer to the referent will be nulled before\n+  \/\/ evacuation begins so card does not need to be dirtied.\n+  if (ShenandoahCardBarrier) {\n+    card_mark_barrier(cast_from_oop<HeapWord*>(reference), cast_to_oop(referent));\n+  }\n@@ -449,0 +502,1 @@\n+  \/\/ set_oop_field maintains the card mark barrier as this list is constructed.\n@@ -453,1 +507,1 @@\n-    RawAccess<>::oop_store(p, prev);\n+    set_oop_field(p, prev);\n@@ -525,0 +579,13 @@\n+\n+  \/\/ During reference processing, we maintain a local list of references that are identified by\n+  \/\/   _pending_list and _pending_list_tail.  _pending_list_tail points to the next field of the last Reference object on\n+  \/\/   the local list.\n+  \/\/\n+  \/\/ There is also a global list of reference identified by Universe::_reference_pending_list\n+\n+  \/\/ The following code has the effect of:\n+  \/\/  1. Making the global Universe::_reference_pending_list point to my local list\n+  \/\/  2. Overwriting the next field of the last Reference on my local list to point at the previous head of the\n+  \/\/     global Universe::_reference_pending_list\n+\n+  oop former_head_of_global_list = Universe::swap_reference_pending_list(_pending_list);\n@@ -526,1 +593,1 @@\n-    *reinterpret_cast<narrowOop*>(_pending_list_tail) = CompressedOops::encode(Universe::swap_reference_pending_list(_pending_list));\n+    set_oop_field<narrowOop>(reinterpret_cast<narrowOop*>(_pending_list_tail), former_head_of_global_list);\n@@ -528,1 +595,1 @@\n-    *reinterpret_cast<oop*>(_pending_list_tail) = Universe::swap_reference_pending_list(_pending_list);\n+    set_oop_field<oop>(reinterpret_cast<oop*>(_pending_list_tail), former_head_of_global_list);\n@@ -537,1 +604,0 @@\n-\n@@ -615,1 +681,0 @@\n-\n","filename":"src\/hotspot\/share\/gc\/shenandoah\/shenandoahReferenceProcessor.cpp","additions":79,"deletions":14,"binary":false,"changes":93,"status":"modified"},{"patch":"@@ -26,2 +26,2 @@\n-#ifndef SHARE_VM_GC_SHENANDOAH_SHENANDOAHREFERENCEPROCESSOR_HPP\n-#define SHARE_VM_GC_SHENANDOAH_SHENANDOAHREFERENCEPROCESSOR_HPP\n+#ifndef SHARE_GC_SHENANDOAH_SHENANDOAHREFERENCEPROCESSOR_HPP\n+#define SHARE_GC_SHENANDOAH_SHENANDOAHREFERENCEPROCESSOR_HPP\n@@ -191,1 +191,1 @@\n-#endif \/\/ SHARE_VM_GC_SHENANDOAH_SHENANDOAHREFERENCEPROCESSOR_HPP\n+#endif \/\/ SHARE_GC_SHENANDOAH_SHENANDOAHREFERENCEPROCESSOR_HPP\n","filename":"src\/hotspot\/share\/gc\/shenandoah\/shenandoahReferenceProcessor.hpp","additions":3,"deletions":3,"binary":false,"changes":6,"status":"modified"},{"patch":"@@ -0,0 +1,176 @@\n+\/*\n+ * Copyright Amazon.com Inc. or its affiliates. All Rights Reserved.\n+ * DO NOT ALTER OR REMOVE COPYRIGHT NOTICES OR THIS FILE HEADER.\n+ *\n+ * This code is free software; you can redistribute it and\/or modify it\n+ * under the terms of the GNU General Public License version 2 only, as\n+ * published by the Free Software Foundation.\n+ *\n+ * This code is distributed in the hope that it will be useful, but WITHOUT\n+ * ANY WARRANTY; without even the implied warranty of MERCHANTABILITY or\n+ * FITNESS FOR A PARTICULAR PURPOSE.  See the GNU General Public License\n+ * version 2 for more details (a copy is included in the LICENSE file that\n+ * accompanied this code).\n+ *\n+ * You should have received a copy of the GNU General Public License version\n+ * 2 along with this work; if not, write to the Free Software Foundation,\n+ * Inc., 51 Franklin St, Fifth Floor, Boston, MA 02110-1301 USA.\n+ *\n+ * Please contact Oracle, 500 Oracle Parkway, Redwood Shores, CA 94065 USA\n+ * or visit www.oracle.com if you need additional information or have any\n+ * questions.\n+ *\n+ *\/\n+#include \"precompiled.hpp\"\n+\n+#include \"gc\/shenandoah\/heuristics\/shenandoahHeuristics.hpp\"\n+#include \"gc\/shenandoah\/mode\/shenandoahMode.hpp\"\n+#include \"gc\/shenandoah\/shenandoahAsserts.hpp\"\n+#include \"gc\/shenandoah\/shenandoahGenerationalControlThread.hpp\"\n+#include \"gc\/shenandoah\/shenandoahHeap.inline.hpp\"\n+#include \"gc\/shenandoah\/shenandoahOldGeneration.hpp\"\n+#include \"gc\/shenandoah\/shenandoahRegulatorThread.hpp\"\n+#include \"gc\/shenandoah\/shenandoahYoungGeneration.hpp\"\n+#include \"logging\/log.hpp\"\n+\n+ShenandoahRegulatorThread::ShenandoahRegulatorThread(ShenandoahGenerationalControlThread* control_thread) :\n+  _heap(ShenandoahHeap::heap()),\n+  _control_thread(control_thread),\n+  _sleep(ShenandoahControlIntervalMin),\n+  _last_sleep_adjust_time(os::elapsedTime()) {\n+  shenandoah_assert_generational();\n+  _old_heuristics = _heap->old_generation()->heuristics();\n+  _young_heuristics = _heap->young_generation()->heuristics();\n+  _global_heuristics = _heap->global_generation()->heuristics();\n+\n+  set_name(\"Shenandoah Regulator Thread\");\n+  create_and_start();\n+}\n+\n+void ShenandoahRegulatorThread::run_service() {\n+  if (ShenandoahAllowOldMarkingPreemption) {\n+    regulate_young_and_old_cycles();\n+  } else {\n+    regulate_young_and_global_cycles();\n+  }\n+\n+  log_debug(gc)(\"%s: Done.\", name());\n+}\n+\n+void ShenandoahRegulatorThread::regulate_young_and_old_cycles() {\n+  while (!should_terminate()) {\n+    ShenandoahGenerationalControlThread::GCMode mode = _control_thread->gc_mode();\n+    if (mode == ShenandoahGenerationalControlThread::none) {\n+      if (should_start_metaspace_gc()) {\n+        if (request_concurrent_gc(_heap->global_generation())) {\n+          \/\/ Some of vmTestbase\/metaspace tests depend on following line to count GC cycles\n+          _global_heuristics->log_trigger(\"%s\", GCCause::to_string(GCCause::_metadata_GC_threshold));\n+          _global_heuristics->cancel_trigger_request();\n+        }\n+      } else {\n+        if (_young_heuristics->should_start_gc()) {\n+          \/\/ Give the old generation a chance to run. The old generation cycle\n+          \/\/ begins with a 'bootstrap' cycle that will also collect young.\n+          if (start_old_cycle()) {\n+            log_debug(gc)(\"Heuristics request for old collection accepted\");\n+            _young_heuristics->cancel_trigger_request();\n+            _old_heuristics->cancel_trigger_request();\n+          } else if (request_concurrent_gc(_heap->young_generation())) {\n+            log_debug(gc)(\"Heuristics request for young collection accepted\");\n+            _young_heuristics->cancel_trigger_request();\n+          }\n+        } else if (_old_heuristics->should_resume_old_cycle() || _old_heuristics->should_start_gc()) {\n+          if (request_concurrent_gc(_heap->old_generation())) {\n+            _old_heuristics->cancel_trigger_request();\n+            log_debug(gc)(\"Heuristics request to resume old collection accepted\");\n+          }\n+        }\n+      }\n+    } else if (mode == ShenandoahGenerationalControlThread::servicing_old) {\n+      if (start_young_cycle()) {\n+        log_debug(gc)(\"Heuristics request to interrupt old for young collection accepted\");\n+        _young_heuristics->cancel_trigger_request();\n+      }\n+    }\n+\n+    regulator_sleep();\n+  }\n+}\n+\n+\n+void ShenandoahRegulatorThread::regulate_young_and_global_cycles() {\n+  while (!should_terminate()) {\n+    if (_control_thread->gc_mode() == ShenandoahGenerationalControlThread::none) {\n+      if (start_global_cycle()) {\n+        log_debug(gc)(\"Heuristics request for global collection accepted.\");\n+        _global_heuristics->cancel_trigger_request();\n+      } else if (start_young_cycle()) {\n+        log_debug(gc)(\"Heuristics request for young collection accepted.\");\n+        _young_heuristics->cancel_trigger_request();\n+      }\n+    }\n+\n+    regulator_sleep();\n+  }\n+}\n+\n+void ShenandoahRegulatorThread::regulator_sleep() {\n+  \/\/ Wait before performing the next action. If allocation happened during this wait,\n+  \/\/ we exit sooner, to let heuristics re-evaluate new conditions. If we are at idle,\n+  \/\/ back off exponentially.\n+  double current = os::elapsedTime();\n+\n+  if (ShenandoahHeap::heap()->has_changed()) {\n+    _sleep = ShenandoahControlIntervalMin;\n+  } else if ((current - _last_sleep_adjust_time) * 1000 > ShenandoahControlIntervalAdjustPeriod){\n+    _sleep = MIN2<uint>(ShenandoahControlIntervalMax, MAX2(1u, _sleep * 2));\n+    _last_sleep_adjust_time = current;\n+  }\n+\n+  os::naked_short_sleep(_sleep);\n+  if (LogTarget(Debug, gc, thread)::is_enabled()) {\n+    double elapsed = os::elapsedTime() - current;\n+    double hiccup = elapsed - double(_sleep);\n+    if (hiccup > 0.001) {\n+      log_debug(gc, thread)(\"Regulator hiccup time: %.3fs\", hiccup);\n+    }\n+  }\n+}\n+\n+bool ShenandoahRegulatorThread::start_old_cycle() const {\n+  return _old_heuristics->should_start_gc() && request_concurrent_gc(_heap->old_generation());\n+}\n+\n+bool ShenandoahRegulatorThread::start_young_cycle() const {\n+  return _young_heuristics->should_start_gc() && request_concurrent_gc(_heap->young_generation());\n+}\n+\n+bool ShenandoahRegulatorThread::start_global_cycle() const {\n+  return _global_heuristics->should_start_gc() && request_concurrent_gc(_heap->global_generation());\n+}\n+\n+bool ShenandoahRegulatorThread::request_concurrent_gc(ShenandoahGeneration* generation) const {\n+  double now = os::elapsedTime();\n+  bool accepted = _control_thread->request_concurrent_gc(generation);\n+  if (LogTarget(Debug, gc, thread)::is_enabled() && accepted) {\n+    double wait_time = os::elapsedTime() - now;\n+    if (wait_time > 0.001) {\n+      log_debug(gc, thread)(\"Regulator waited %.3fs for control thread to acknowledge request.\", wait_time);\n+    }\n+  }\n+  return accepted;\n+}\n+\n+void ShenandoahRegulatorThread::stop_service() {\n+  log_debug(gc)(\"%s: Stop requested.\", name());\n+}\n+\n+bool ShenandoahRegulatorThread::should_start_metaspace_gc() {\n+  \/\/ The generational mode can, at present, only unload classes during a global\n+  \/\/ cycle. For this reason, we treat an oom in metaspace as a _trigger_ for a\n+  \/\/ global cycle. But, we check other prerequisites before starting a gc that won't\n+  \/\/ unload anything.\n+  return ClassUnloadingWithConcurrentMark\n+      && _global_heuristics->can_unload_classes()\n+      && _global_heuristics->has_metaspace_oom();\n+}\n","filename":"src\/hotspot\/share\/gc\/shenandoah\/shenandoahRegulatorThread.cpp","additions":176,"deletions":0,"binary":false,"changes":176,"status":"added"},{"patch":"@@ -0,0 +1,90 @@\n+\/*\n+ * Copyright Amazon.com Inc. or its affiliates. All Rights Reserved.\n+ * DO NOT ALTER OR REMOVE COPYRIGHT NOTICES OR THIS FILE HEADER.\n+ *\n+ * This code is free software; you can redistribute it and\/or modify it\n+ * under the terms of the GNU General Public License version 2 only, as\n+ * published by the Free Software Foundation.\n+ *\n+ * This code is distributed in the hope that it will be useful, but WITHOUT\n+ * ANY WARRANTY; without even the implied warranty of MERCHANTABILITY or\n+ * FITNESS FOR A PARTICULAR PURPOSE.  See the GNU General Public License\n+ * version 2 for more details (a copy is included in the LICENSE file that\n+ * accompanied this code).\n+ *\n+ * You should have received a copy of the GNU General Public License version\n+ * 2 along with this work; if not, write to the Free Software Foundation,\n+ * Inc., 51 Franklin St, Fifth Floor, Boston, MA 02110-1301 USA.\n+ *\n+ * Please contact Oracle, 500 Oracle Parkway, Redwood Shores, CA 94065 USA\n+ * or visit www.oracle.com if you need additional information or have any\n+ * questions.\n+ *\n+ *\/\n+#ifndef SHARE_GC_SHENANDOAH_SHENANDOAHREGULATORTHREAD_HPP\n+#define SHARE_GC_SHENANDOAH_SHENANDOAHREGULATORTHREAD_HPP\n+\n+#include \"gc\/shared\/concurrentGCThread.hpp\"\n+\n+class ShenandoahHeap;\n+class ShenandoahHeuristics;\n+class ShenandoahGeneration;\n+class ShenandoahGenerationalControlThread;\n+class ShenandoahOldHeuristics;\n+\n+\/*\n+ * The purpose of this class (and thread) is to allow us to continue\n+ * to evaluate heuristics during a garbage collection. This is necessary\n+ * to allow young generation collections to interrupt an old generation\n+ * collection which is in-progress. This puts heuristic triggers on the\n+ * same footing as other gc requests (alloc failure, System.gc, etc.).\n+ * However, this regulator does not block after submitting a gc request.\n+ *\n+ * We could use a PeriodicTask for this, but this thread will sleep longer\n+ * when the allocation rate is lower and PeriodicTasks cannot adjust their\n+ * sleep time.\n+ *\/\n+class ShenandoahRegulatorThread: public ConcurrentGCThread {\n+  friend class VMStructs;\n+\n+ public:\n+  explicit ShenandoahRegulatorThread(ShenandoahGenerationalControlThread* control_thread);\n+\n+ protected:\n+  void run_service() override;\n+  void stop_service() override;\n+\n+ private:\n+  \/\/ When mode is generational\n+  void regulate_young_and_old_cycles();\n+  \/\/ When mode is generational, but ShenandoahAllowOldMarkingPreemption is false\n+  void regulate_young_and_global_cycles();\n+\n+  \/\/ These return true if a cycle was started.\n+  bool start_old_cycle() const;\n+  bool start_young_cycle() const;\n+  bool start_global_cycle() const;\n+  bool resume_old_cycle();\n+\n+  \/\/ The generational mode can only unload classes in a global cycle. The regulator\n+  \/\/ thread itself will trigger a global cycle if metaspace is out of memory.\n+  bool should_start_metaspace_gc();\n+\n+  \/\/ Regulator will sleep longer when the allocation rate is lower.\n+  void regulator_sleep();\n+\n+  \/\/ Provides instrumentation to track how long it takes to acknowledge a request.\n+  bool request_concurrent_gc(ShenandoahGeneration* generation) const;\n+\n+  ShenandoahHeap* _heap;\n+  ShenandoahGenerationalControlThread* _control_thread;\n+  ShenandoahHeuristics* _young_heuristics;\n+  ShenandoahOldHeuristics* _old_heuristics;\n+  ShenandoahHeuristics* _global_heuristics;\n+\n+  uint _sleep;\n+  double _last_sleep_adjust_time;\n+};\n+\n+\n+#endif \/\/ SHARE_GC_SHENANDOAH_SHENANDOAHREGULATORTHREAD_HPP\n","filename":"src\/hotspot\/share\/gc\/shenandoah\/shenandoahRegulatorThread.hpp","additions":90,"deletions":0,"binary":false,"changes":90,"status":"added"},{"patch":"@@ -3,0 +3,1 @@\n+ * Copyright Amazon.com Inc. or its affiliates. All Rights Reserved.\n@@ -33,0 +34,1 @@\n+#include \"gc\/shenandoah\/shenandoahGeneration.hpp\"\n@@ -35,0 +37,1 @@\n+#include \"gc\/shenandoah\/shenandoahScanRemembered.inline.hpp\"\n@@ -47,1 +50,5 @@\n-  _gc_state(_heap->gc_state()) {\n+  _gc_state(_heap->gc_state()),\n+  _gc_state_changed(_heap->_gc_state_changed) {\n+  \/\/ Clear state to deactivate barriers. Indicate that state has changed\n+  \/\/ so that verifier threads will use this value, rather than thread local\n+  \/\/ values (which we are _not_ changing here).\n@@ -49,0 +56,1 @@\n+  _heap->_gc_state_changed = true;\n@@ -53,0 +61,1 @@\n+  _heap->_gc_state_changed = _gc_state_changed;\n@@ -56,1 +65,1 @@\n-void ShenandoahRootVerifier::roots_do(OopClosure* oops) {\n+void ShenandoahRootVerifier::roots_do(OopIterateClosure* oops) {\n@@ -70,0 +79,6 @@\n+  ShenandoahHeap* heap = ShenandoahHeap::heap();\n+  if (heap->mode()->is_generational() && heap->active_generation()->is_young()) {\n+    shenandoah_assert_safepoint();\n+    ShenandoahGenerationalHeap::heap()->old_generation()->card_scan()->roots_do(oops);\n+  }\n+\n@@ -76,1 +91,1 @@\n-void ShenandoahRootVerifier::strong_roots_do(OopClosure* oops) {\n+void ShenandoahRootVerifier::strong_roots_do(OopIterateClosure* oops) {\n@@ -86,0 +101,6 @@\n+\n+  ShenandoahHeap* heap = ShenandoahHeap::heap();\n+  if (heap->mode()->is_generational() && heap->active_generation()->is_young()) {\n+    ShenandoahGenerationalHeap::heap()->old_generation()->card_scan()->roots_do(oops);\n+  }\n+\n","filename":"src\/hotspot\/share\/gc\/shenandoah\/shenandoahRootVerifier.cpp","additions":24,"deletions":3,"binary":false,"changes":27,"status":"modified"},{"patch":"@@ -3,0 +3,1 @@\n+ * Copyright Amazon.com Inc. or its affiliates. All Rights Reserved.\n@@ -35,0 +36,1 @@\n+  const bool _gc_state_changed;\n@@ -44,2 +46,2 @@\n-  static void roots_do(OopClosure* cl);\n-  static void strong_roots_do(OopClosure* cl);\n+  static void roots_do(OopIterateClosure* cl);\n+  static void strong_roots_do(OopIterateClosure* cl);\n","filename":"src\/hotspot\/share\/gc\/shenandoah\/shenandoahRootVerifier.hpp","additions":4,"deletions":2,"binary":false,"changes":6,"status":"modified"},{"patch":"@@ -30,1 +30,0 @@\n-#include \"gc\/shenandoah\/shenandoahHeap.hpp\"\n","filename":"src\/hotspot\/share\/gc\/shenandoah\/shenandoahSATBMarkQueueSet.hpp","additions":0,"deletions":1,"binary":false,"changes":1,"status":"modified"},{"patch":"@@ -3,0 +3,1 @@\n+ * Copyright Amazon.com Inc. or its affiliates. All Rights Reserved.\n@@ -32,0 +33,2 @@\n+#include \"gc\/shenandoah\/shenandoahGeneration.hpp\"\n+#include \"gc\/shenandoah\/shenandoahGenerationType.hpp\"\n@@ -39,0 +42,1 @@\n+template<ShenandoahGenerationType GENERATION>\n@@ -46,0 +50,1 @@\n+\n@@ -53,1 +58,2 @@\n-ShenandoahInitMarkRootsClosure::ShenandoahInitMarkRootsClosure(ShenandoahObjToScanQueue* q) :\n+template <ShenandoahGenerationType GENERATION>\n+ShenandoahInitMarkRootsClosure<GENERATION>::ShenandoahInitMarkRootsClosure(ShenandoahObjToScanQueue* q) :\n@@ -58,0 +64,1 @@\n+template <ShenandoahGenerationType GENERATION>\n@@ -59,2 +66,3 @@\n-void ShenandoahInitMarkRootsClosure::do_oop_work(T* p) {\n-  ShenandoahMark::mark_through_ref<T>(p, _queue, _mark_context, false);\n+void ShenandoahInitMarkRootsClosure<GENERATION>::do_oop_work(T* p) {\n+  \/\/ Only called from STW mark, should not be used to bootstrap old generation marking.\n+  ShenandoahMark::mark_through_ref<T, GENERATION>(p, _queue, nullptr, _mark_context, false);\n@@ -83,2 +91,2 @@\n-ShenandoahSTWMark::ShenandoahSTWMark(bool full_gc) :\n-  ShenandoahMark(),\n+ShenandoahSTWMark::ShenandoahSTWMark(ShenandoahGeneration* generation, bool full_gc) :\n+  ShenandoahMark(generation),\n@@ -86,1 +94,1 @@\n-  _terminator(ShenandoahHeap::heap()->workers()->active_workers(), ShenandoahHeap::heap()->marking_context()->task_queues()),\n+  _terminator(ShenandoahHeap::heap()->workers()->active_workers(), task_queues()),\n@@ -99,1 +107,2 @@\n-  ShenandoahReferenceProcessor* rp = heap->ref_processor();\n+  ShenandoahReferenceProcessor* rp = heap->gc_generation()->ref_processor();\n+  shenandoah_assert_generations_reconciled();\n@@ -118,0 +127,5 @@\n+    if (_generation->is_young()) {\n+      \/\/ But only scan the remembered set for young generation.\n+      _generation->scan_remembered_set(false \/* is_concurrent *\/);\n+    }\n+\n@@ -125,1 +139,1 @@\n-  heap->mark_complete_marking_context();\n+  _generation->set_mark_complete();\n@@ -132,2 +146,1 @@\n-  TASKQUEUE_STATS_ONLY(task_queues()->print_taskqueue_stats());\n-  TASKQUEUE_STATS_ONLY(task_queues()->reset_taskqueue_stats());\n+  TASKQUEUE_STATS_ONLY(task_queues()->print_and_reset_taskqueue_stats(\"\"));\n@@ -137,2 +150,23 @@\n-  ShenandoahInitMarkRootsClosure  init_mark(task_queues()->queue(worker_id));\n-  _root_scanner.roots_do(&init_mark, worker_id);\n+  switch (_generation->type()) {\n+    case NON_GEN: {\n+      ShenandoahInitMarkRootsClosure<NON_GEN> init_mark(task_queues()->queue(worker_id));\n+      _root_scanner.roots_do(&init_mark, worker_id);\n+      break;\n+    }\n+    case GLOBAL: {\n+      ShenandoahInitMarkRootsClosure<GLOBAL> init_mark(task_queues()->queue(worker_id));\n+      _root_scanner.roots_do(&init_mark, worker_id);\n+      break;\n+    }\n+    case YOUNG: {\n+      ShenandoahInitMarkRootsClosure<YOUNG> init_mark(task_queues()->queue(worker_id));\n+      _root_scanner.roots_do(&init_mark, worker_id);\n+      break;\n+    }\n+    case OLD:\n+      \/\/ We never exclusively mark the old generation on a safepoint. This would be encompassed\n+      \/\/ by a 'global' collection. Note that both GLOBAL and NON_GEN mark the entire heap, but\n+      \/\/ the GLOBAL closure is specialized for the generational mode.\n+    default:\n+      ShouldNotReachHere();\n+  }\n@@ -144,1 +178,2 @@\n-  ShenandoahReferenceProcessor* rp = ShenandoahHeap::heap()->ref_processor();\n+  ShenandoahReferenceProcessor* rp = ShenandoahHeap::heap()->gc_generation()->ref_processor();\n+  shenandoah_assert_generations_reconciled();\n@@ -148,1 +183,1 @@\n-            false \/* not cancellable *\/,\n+            _generation->type(), false \/* not cancellable *\/,\n@@ -151,1 +186,0 @@\n-\n","filename":"src\/hotspot\/share\/gc\/shenandoah\/shenandoahSTWMark.cpp","additions":49,"deletions":15,"binary":false,"changes":64,"status":"modified"},{"patch":"@@ -29,0 +29,1 @@\n+#include \"gc\/shenandoah\/shenandoahRootProcessor.hpp\"\n@@ -31,0 +32,1 @@\n+class ShenandoahGeneration;\n@@ -40,1 +42,1 @@\n- ShenandoahSTWMark(bool full_gc);\n+ ShenandoahSTWMark(ShenandoahGeneration* generation, bool full_gc);\n@@ -49,1 +51,0 @@\n-\n","filename":"src\/hotspot\/share\/gc\/shenandoah\/shenandoahSTWMark.hpp","additions":3,"deletions":2,"binary":false,"changes":5,"status":"modified"},{"patch":"@@ -0,0 +1,979 @@\n+\/*\n+ * Copyright Amazon.com Inc. or its affiliates. All Rights Reserved.\n+ * DO NOT ALTER OR REMOVE COPYRIGHT NOTICES OR THIS FILE HEADER.\n+ *\n+ * This code is free software; you can redistribute it and\/or modify it\n+ * under the terms of the GNU General Public License version 2 only, as\n+ * published by the Free Software Foundation.\n+ *\n+ * This code is distributed in the hope that it will be useful, but WITHOUT\n+ * ANY WARRANTY; without even the implied warranty of MERCHANTABILITY or\n+ * FITNESS FOR A PARTICULAR PURPOSE.  See the GNU General Public License\n+ * version 2 for more details (a copy is included in the LICENSE file that\n+ * accompanied this code).\n+ *\n+ * You should have received a copy of the GNU General Public License version\n+ * 2 along with this work; if not, write to the Free Software Foundation,\n+ * Inc., 51 Franklin St, Fifth Floor, Boston, MA 02110-1301 USA.\n+ *\n+ * Please contact Oracle, 500 Oracle Parkway, Redwood Shores, CA 94065 USA\n+ * or visit www.oracle.com if you need additional information or have any\n+ * questions.\n+ *\n+ *\/\n+\n+\n+#include \"precompiled.hpp\"\n+\n+#include \"gc\/shenandoah\/shenandoahOldGeneration.hpp\"\n+#include \"gc\/shenandoah\/shenandoahHeap.inline.hpp\"\n+#include \"gc\/shenandoah\/shenandoahOopClosures.inline.hpp\"\n+#include \"gc\/shenandoah\/shenandoahReferenceProcessor.hpp\"\n+#include \"gc\/shenandoah\/shenandoahScanRemembered.inline.hpp\"\n+#include \"logging\/log.hpp\"\n+#include \"runtime\/threads.hpp\"\n+\n+size_t ShenandoahDirectCardMarkRememberedSet::last_valid_index() const {\n+  return _card_table->last_valid_index();\n+}\n+\n+size_t ShenandoahDirectCardMarkRememberedSet::total_cards() const {\n+  return _total_card_count;\n+}\n+\n+size_t ShenandoahDirectCardMarkRememberedSet::card_index_for_addr(HeapWord *p) const {\n+  return _card_table->index_for(p);\n+}\n+\n+HeapWord* ShenandoahDirectCardMarkRememberedSet::addr_for_card_index(size_t card_index) const {\n+  return _whole_heap_base + CardTable::card_size_in_words() * card_index;\n+}\n+\n+bool ShenandoahDirectCardMarkRememberedSet::is_write_card_dirty(size_t card_index) const {\n+  CardValue* bp = &(_card_table->write_byte_map())[card_index];\n+  return (bp[0] == CardTable::dirty_card_val());\n+}\n+\n+bool ShenandoahDirectCardMarkRememberedSet::is_card_dirty(size_t card_index) const {\n+  CardValue* bp = &(_card_table->read_byte_map())[card_index];\n+  return (bp[0] == CardTable::dirty_card_val());\n+}\n+\n+void ShenandoahDirectCardMarkRememberedSet::mark_card_as_dirty(size_t card_index) {\n+  CardValue* bp = &(_card_table->write_byte_map())[card_index];\n+  bp[0] = CardTable::dirty_card_val();\n+}\n+\n+void ShenandoahDirectCardMarkRememberedSet::mark_range_as_dirty(size_t card_index, size_t num_cards) {\n+  CardValue* bp = &(_card_table->write_byte_map())[card_index];\n+  while (num_cards-- > 0) {\n+    *bp++ = CardTable::dirty_card_val();\n+  }\n+}\n+\n+bool ShenandoahDirectCardMarkRememberedSet::is_card_dirty(HeapWord* p) const {\n+  size_t index = card_index_for_addr(p);\n+  CardValue* bp = &(_card_table->read_byte_map())[index];\n+  return (bp[0] == CardTable::dirty_card_val());\n+}\n+\n+bool ShenandoahDirectCardMarkRememberedSet::is_write_card_dirty(HeapWord* p) const {\n+  size_t index = card_index_for_addr(p);\n+  CardValue* bp = &(_card_table->write_byte_map())[index];\n+  return (bp[0] == CardTable::dirty_card_val());\n+}\n+\n+void ShenandoahDirectCardMarkRememberedSet::mark_card_as_dirty(HeapWord* p) {\n+  size_t index = card_index_for_addr(p);\n+  CardValue* bp = &(_card_table->write_byte_map())[index];\n+  bp[0] = CardTable::dirty_card_val();\n+}\n+\n+void ShenandoahDirectCardMarkRememberedSet::mark_range_as_dirty(HeapWord* p, size_t num_heap_words) {\n+  CardValue* bp = &(_card_table->write_byte_map_base())[uintptr_t(p) >> _card_shift];\n+  CardValue* end_bp = &(_card_table->write_byte_map_base())[uintptr_t(p + num_heap_words) >> _card_shift];\n+  \/\/ If (p + num_heap_words) is not aligned on card boundary, we also need to dirty last card.\n+  if (((unsigned long long) (p + num_heap_words)) & (CardTable::card_size() - 1)) {\n+    end_bp++;\n+  }\n+  while (bp < end_bp) {\n+    *bp++ = CardTable::dirty_card_val();\n+  }\n+}\n+\n+void ShenandoahDirectCardMarkRememberedSet::mark_range_as_clean(HeapWord* p, size_t num_heap_words) {\n+  CardValue* bp = &(_card_table->write_byte_map_base())[uintptr_t(p) >> _card_shift];\n+  CardValue* end_bp = &(_card_table->write_byte_map_base())[uintptr_t(p + num_heap_words) >> _card_shift];\n+  \/\/ If (p + num_heap_words) is not aligned on card boundary, we also need to clean last card.\n+  if (((unsigned long long) (p + num_heap_words)) & (CardTable::card_size() - 1)) {\n+    end_bp++;\n+  }\n+  while (bp < end_bp) {\n+    *bp++ = CardTable::clean_card_val();\n+  }\n+}\n+\n+void ShenandoahDirectCardMarkRememberedSet::mark_read_table_as_clean() {\n+  CardValue* read_table = _card_table->read_byte_map();\n+  CardValue* bp = &(read_table)[0];\n+  CardValue* end_bp = &(read_table)[_card_table->last_valid_index()];\n+\n+  while (bp <= end_bp) {\n+    *bp++ = CardTable::clean_card_val();\n+  }\n+\n+  log_info(gc, barrier)(\"Cleaned read_table from \" PTR_FORMAT \" to \" PTR_FORMAT, p2i(&(read_table)[0]), p2i(end_bp));\n+}\n+\n+\/\/ No lock required because arguments align with card boundaries.\n+void ShenandoahCardCluster::reset_object_range(HeapWord* from, HeapWord* to) {\n+  assert(((((unsigned long long) from) & (CardTable::card_size() - 1)) == 0) &&\n+         ((((unsigned long long) to) & (CardTable::card_size() - 1)) == 0),\n+         \"reset_object_range bounds must align with card boundaries\");\n+  size_t card_at_start = _rs->card_index_for_addr(from);\n+  size_t num_cards = (to - from) \/ CardTable::card_size_in_words();\n+\n+  for (size_t i = 0; i < num_cards; i++) {\n+    _object_starts[card_at_start + i].short_word = 0;\n+  }\n+}\n+\n+\/\/ Assume only one thread at a time registers objects pertaining to\n+\/\/ each card-table entry's range of memory.\n+void ShenandoahCardCluster::register_object(HeapWord* address) {\n+  shenandoah_assert_heaplocked();\n+\n+  register_object_without_lock(address);\n+}\n+\n+void ShenandoahCardCluster::register_object_without_lock(HeapWord* address) {\n+  size_t card_at_start = _rs->card_index_for_addr(address);\n+  HeapWord* card_start_address = _rs->addr_for_card_index(card_at_start);\n+  uint8_t offset_in_card = checked_cast<uint8_t>(pointer_delta(address, card_start_address));\n+\n+  if (!starts_object(card_at_start)) {\n+    set_starts_object_bit(card_at_start);\n+    set_first_start(card_at_start, offset_in_card);\n+    set_last_start(card_at_start, offset_in_card);\n+  } else {\n+    if (offset_in_card < get_first_start(card_at_start))\n+      set_first_start(card_at_start, offset_in_card);\n+    if (offset_in_card > get_last_start(card_at_start))\n+      set_last_start(card_at_start, offset_in_card);\n+  }\n+}\n+\n+void ShenandoahCardCluster::coalesce_objects(HeapWord* address, size_t length_in_words) {\n+\n+  size_t card_at_start = _rs->card_index_for_addr(address);\n+  HeapWord* card_start_address = _rs->addr_for_card_index(card_at_start);\n+  size_t card_at_end = card_at_start + ((address + length_in_words) - card_start_address) \/ CardTable::card_size_in_words();\n+\n+  if (card_at_start == card_at_end) {\n+    \/\/ There are no changes to the get_first_start array.  Either get_first_start(card_at_start) returns this coalesced object,\n+    \/\/ or it returns an object that precedes the coalesced object.\n+    if (card_start_address + get_last_start(card_at_start) < address + length_in_words) {\n+      uint8_t coalesced_offset = checked_cast<uint8_t>(pointer_delta(address, card_start_address));\n+      \/\/ The object that used to be the last object starting within this card is being subsumed within the coalesced\n+      \/\/ object.  Since we always coalesce entire objects, this condition only occurs if the last object ends before or at\n+      \/\/ the end of the card's memory range and there is no object following this object.  In this case, adjust last_start\n+      \/\/ to represent the start of the coalesced range.\n+      set_last_start(card_at_start, coalesced_offset);\n+    }\n+    \/\/ Else, no changes to last_starts information.  Either get_last_start(card_at_start) returns the object that immediately\n+    \/\/ follows the coalesced object, or it returns an object that follows the object immediately following the coalesced object.\n+  } else {\n+    uint8_t coalesced_offset = checked_cast<uint8_t>(pointer_delta(address, card_start_address));\n+    if (get_last_start(card_at_start) > coalesced_offset) {\n+      \/\/ Existing last start is being coalesced, create new last start\n+      set_last_start(card_at_start, coalesced_offset);\n+    }\n+    \/\/ otherwise, get_last_start(card_at_start) must equal coalesced_offset\n+\n+    \/\/ All the cards between first and last get cleared.\n+    for (size_t i = card_at_start + 1; i < card_at_end; i++) {\n+      clear_starts_object_bit(i);\n+    }\n+\n+    uint8_t follow_offset = checked_cast<uint8_t>((address + length_in_words) - _rs->addr_for_card_index(card_at_end));\n+    if (starts_object(card_at_end) && (get_first_start(card_at_end) < follow_offset)) {\n+      \/\/ It may be that after coalescing within this last card's memory range, the last card\n+      \/\/ no longer holds an object.\n+      if (get_last_start(card_at_end) >= follow_offset) {\n+        set_first_start(card_at_end, follow_offset);\n+      } else {\n+        \/\/ last_start is being coalesced so this card no longer has any objects.\n+        clear_starts_object_bit(card_at_end);\n+      }\n+    }\n+    \/\/ else\n+    \/\/  card_at_end did not have an object, so it still does not have an object, or\n+    \/\/  card_at_end had an object that starts after the coalesced object, so no changes required for card_at_end\n+\n+  }\n+}\n+\n+\n+size_t ShenandoahCardCluster::get_first_start(size_t card_index) const {\n+  assert(starts_object(card_index), \"Can't get first start because no object starts here\");\n+  return _object_starts[card_index].offsets.first & FirstStartBits;\n+}\n+\n+size_t ShenandoahCardCluster::get_last_start(size_t card_index) const {\n+  assert(starts_object(card_index), \"Can't get last start because no object starts here\");\n+  return _object_starts[card_index].offsets.last;\n+}\n+\n+\/\/ Given a card_index, return the starting address of the first block in the heap\n+\/\/ that straddles into this card. If this card is co-initial with an object, then\n+\/\/ this would return the first address of the range that this card covers, which is\n+\/\/ where the card's first object also begins.\n+HeapWord* ShenandoahCardCluster::block_start(const size_t card_index) const {\n+\n+  HeapWord* left = _rs->addr_for_card_index(card_index);\n+\n+#ifdef ASSERT\n+  assert(ShenandoahHeap::heap()->mode()->is_generational(), \"Do not use in non-generational mode\");\n+  ShenandoahHeapRegion* region = ShenandoahHeap::heap()->heap_region_containing(left);\n+  assert(region->is_old(), \"Do not use for young regions\");\n+  \/\/ For HumongousRegion:s it's more efficient to jump directly to the\n+  \/\/ start region.\n+  assert(!region->is_humongous(), \"Use region->humongous_start_region() instead\");\n+#endif\n+  if (starts_object(card_index) && get_first_start(card_index) == 0) {\n+    \/\/ This card contains a co-initial object; a fortiori, it covers\n+    \/\/ also the case of a card being the first in a region.\n+    assert(oopDesc::is_oop(cast_to_oop(left)), \"Should be an object\");\n+    return left;\n+  }\n+\n+  HeapWord* p = nullptr;\n+  oop obj = cast_to_oop(p);\n+  ssize_t cur_index = (ssize_t)card_index;\n+  assert(cur_index >= 0, \"Overflow\");\n+  assert(cur_index > 0, \"Should have returned above\");\n+  \/\/ Walk backwards over the cards...\n+  while (--cur_index > 0 && !starts_object(cur_index)) {\n+   \/\/ ... to the one that starts the object\n+  }\n+  \/\/ cur_index should start an object: we should not have walked\n+  \/\/ past the left end of the region.\n+  assert(cur_index >= 0 && (cur_index <= (ssize_t)card_index), \"Error\");\n+  assert(region->bottom() <= _rs->addr_for_card_index(cur_index),\n+         \"Fell off the bottom of containing region\");\n+  assert(starts_object(cur_index), \"Error\");\n+  size_t offset = get_last_start(cur_index);\n+  \/\/ can avoid call via card size arithmetic below instead\n+  p = _rs->addr_for_card_index(cur_index) + offset;\n+  \/\/ Recall that we already dealt with the co-initial object case above\n+  assert(p < left, \"obj should start before left\");\n+  \/\/ While it is safe to ask an object its size in the loop that\n+  \/\/ follows, the (ifdef'd out) loop should never be needed.\n+  \/\/ 1. we ask this question only for regions in the old generation\n+  \/\/ 2. there is no direct allocation ever by mutators in old generation\n+  \/\/    regions. Only GC will ever allocate in old regions, and then\n+  \/\/    too only during promotion\/evacuation phases. Thus there is no danger\n+  \/\/    of races between reading from and writing to the object start array,\n+  \/\/    or of asking partially initialized objects their size (in the loop below).\n+  \/\/ 3. only GC asks this question during phases when it is not concurrently\n+  \/\/    evacuating\/promoting, viz. during concurrent root scanning (before\n+  \/\/    the evacuation phase) and during concurrent update refs (after the\n+  \/\/    evacuation phase) of young collections. This is never called\n+  \/\/    during old or global collections.\n+  \/\/ 4. Every allocation under TAMS updates the object start array.\n+  NOT_PRODUCT(obj = cast_to_oop(p);)\n+  assert(oopDesc::is_oop(obj), \"Should be an object\");\n+#define WALK_FORWARD_IN_BLOCK_START false\n+  while (WALK_FORWARD_IN_BLOCK_START && p + obj->size() < left) {\n+    p += obj->size();\n+  }\n+#undef WALK_FORWARD_IN_BLOCK_START \/\/ false\n+  assert(p + obj->size() > left, \"obj should end after left\");\n+  return p;\n+}\n+\n+size_t ShenandoahScanRemembered::card_index_for_addr(HeapWord* p) {\n+  return _rs->card_index_for_addr(p);\n+}\n+\n+HeapWord* ShenandoahScanRemembered::addr_for_card_index(size_t card_index) {\n+  return _rs->addr_for_card_index(card_index);\n+}\n+\n+bool ShenandoahScanRemembered::is_card_dirty(size_t card_index) {\n+  return _rs->is_card_dirty(card_index);\n+}\n+\n+bool ShenandoahScanRemembered::is_write_card_dirty(size_t card_index) {\n+  return _rs->is_write_card_dirty(card_index);\n+}\n+\n+bool ShenandoahScanRemembered::is_card_dirty(HeapWord* p) {\n+  return _rs->is_card_dirty(p);\n+}\n+\n+void ShenandoahScanRemembered::mark_card_as_dirty(HeapWord* p) {\n+  _rs->mark_card_as_dirty(p);\n+}\n+\n+bool ShenandoahScanRemembered::is_write_card_dirty(HeapWord* p) {\n+  return _rs->is_write_card_dirty(p);\n+}\n+\n+void ShenandoahScanRemembered::mark_range_as_dirty(HeapWord* p, size_t num_heap_words) {\n+  _rs->mark_range_as_dirty(p, num_heap_words);\n+}\n+\n+void ShenandoahScanRemembered::mark_range_as_clean(HeapWord* p, size_t num_heap_words) {\n+  _rs->mark_range_as_clean(p, num_heap_words);\n+}\n+\n+void ShenandoahScanRemembered::mark_read_table_as_clean() {\n+  _rs->mark_read_table_as_clean();\n+}\n+\n+void ShenandoahScanRemembered::reset_object_range(HeapWord* from, HeapWord* to) {\n+  _scc->reset_object_range(from, to);\n+}\n+\n+void ShenandoahScanRemembered::register_object(HeapWord* addr) {\n+  _scc->register_object(addr);\n+}\n+\n+void ShenandoahScanRemembered::register_object_without_lock(HeapWord* addr) {\n+  _scc->register_object_without_lock(addr);\n+}\n+\n+bool ShenandoahScanRemembered::verify_registration(HeapWord* address, ShenandoahMarkingContext* ctx) {\n+\n+  size_t index = card_index_for_addr(address);\n+  if (!_scc->starts_object(index)) {\n+    return false;\n+  }\n+  HeapWord* base_addr = addr_for_card_index(index);\n+  size_t offset = _scc->get_first_start(index);\n+  ShenandoahHeap* heap = ShenandoahHeap::heap();\n+\n+  \/\/ Verify that I can find this object within its enclosing card by scanning forward from first_start.\n+  while (base_addr + offset < address) {\n+    oop obj = cast_to_oop(base_addr + offset);\n+    if (!ctx || ctx->is_marked(obj)) {\n+      offset += obj->size();\n+    } else {\n+      \/\/ If this object is not live, don't trust its size(); all objects above tams are live.\n+      ShenandoahHeapRegion* r = heap->heap_region_containing(obj);\n+      HeapWord* tams = ctx->top_at_mark_start(r);\n+      offset = ctx->get_next_marked_addr(base_addr + offset, tams) - base_addr;\n+    }\n+  }\n+  if (base_addr + offset != address){\n+    return false;\n+  }\n+\n+  \/\/ At this point, offset represents object whose registration we are verifying.  We know that at least this object resides\n+  \/\/ within this card's memory.\n+\n+  \/\/ Make sure that last_offset is properly set for the enclosing card, but we can't verify this for\n+  \/\/ candidate collection-set regions during mixed evacuations, so disable this check in general\n+  \/\/ during mixed evacuations.\n+\n+  ShenandoahHeapRegion* r = heap->heap_region_containing(base_addr + offset);\n+  size_t max_offset = r->top() - base_addr;\n+  if (max_offset > CardTable::card_size_in_words()) {\n+    max_offset = CardTable::card_size_in_words();\n+  }\n+  size_t prev_offset;\n+  if (!ctx) {\n+    do {\n+      oop obj = cast_to_oop(base_addr + offset);\n+      prev_offset = offset;\n+      offset += obj->size();\n+    } while (offset < max_offset);\n+    if (_scc->get_last_start(index) != prev_offset) {\n+      return false;\n+    }\n+\n+    \/\/ base + offset represents address of first object that starts on following card, if there is one.\n+\n+    \/\/ Notes: base_addr is addr_for_card_index(index)\n+    \/\/        base_addr + offset is end of the object we are verifying\n+    \/\/        cannot use card_index_for_addr(base_addr + offset) because it asserts arg < end of whole heap\n+    size_t end_card_index = index + offset \/ CardTable::card_size_in_words();\n+\n+    if (end_card_index > index && end_card_index <= _rs->last_valid_index()) {\n+      \/\/ If there is a following object registered on the next card, it should begin where this object ends.\n+      if (_scc->starts_object(end_card_index) &&\n+          ((addr_for_card_index(end_card_index) + _scc->get_first_start(end_card_index)) != (base_addr + offset))) {\n+        return false;\n+      }\n+    }\n+\n+    \/\/ Assure that no other objects are registered \"inside\" of this one.\n+    for (index++; index < end_card_index; index++) {\n+      if (_scc->starts_object(index)) {\n+        return false;\n+      }\n+    }\n+  } else {\n+    \/\/ This is a mixed evacuation or a global collect: rely on mark bits to identify which objects need to be properly registered\n+    assert(!ShenandoahHeap::heap()->is_concurrent_old_mark_in_progress(), \"Cannot rely on mark context here.\");\n+    \/\/ If the object reaching or spanning the end of this card's memory is marked, then last_offset for this card\n+    \/\/ should represent this object.  Otherwise, last_offset is a don't care.\n+    ShenandoahHeapRegion* region = heap->heap_region_containing(base_addr + offset);\n+    HeapWord* tams = ctx->top_at_mark_start(region);\n+    oop last_obj = nullptr;\n+    do {\n+      oop obj = cast_to_oop(base_addr + offset);\n+      if (ctx->is_marked(obj)) {\n+        prev_offset = offset;\n+        offset += obj->size();\n+        last_obj = obj;\n+      } else {\n+        offset = ctx->get_next_marked_addr(base_addr + offset, tams) - base_addr;\n+        \/\/ If there are no marked objects remaining in this region, offset equals tams - base_addr.  If this offset is\n+        \/\/ greater than max_offset, we will immediately exit this loop.  Otherwise, the next iteration of the loop will\n+        \/\/ treat the object at offset as marked and live (because address >= tams) and we will continue iterating object\n+        \/\/ by consulting the size() fields of each.\n+      }\n+    } while (offset < max_offset);\n+    if (last_obj != nullptr && prev_offset + last_obj->size() >= max_offset) {\n+      \/\/ last marked object extends beyond end of card\n+      if (_scc->get_last_start(index) != prev_offset) {\n+        return false;\n+      }\n+      \/\/ otherwise, the value of _scc->get_last_start(index) is a don't care because it represents a dead object and we\n+      \/\/ cannot verify its context\n+    }\n+  }\n+  return true;\n+}\n+\n+void ShenandoahScanRemembered::coalesce_objects(HeapWord* addr, size_t length_in_words) {\n+  _scc->coalesce_objects(addr, length_in_words);\n+}\n+\n+void ShenandoahScanRemembered::mark_range_as_empty(HeapWord* addr, size_t length_in_words) {\n+  _rs->mark_range_as_clean(addr, length_in_words);\n+  _scc->clear_objects_in_range(addr, length_in_words);\n+}\n+\n+size_t ShenandoahScanRemembered::cluster_for_addr(HeapWordImpl **addr) {\n+  size_t card_index = _rs->card_index_for_addr(addr);\n+  size_t result = card_index \/ ShenandoahCardCluster::CardsPerCluster;\n+  return result;\n+}\n+\n+HeapWord* ShenandoahScanRemembered::addr_for_cluster(size_t cluster_no) {\n+  size_t card_index = cluster_no * ShenandoahCardCluster::CardsPerCluster;\n+  return addr_for_card_index(card_index);\n+}\n+\n+\/\/ This is used only for debug verification so don't worry about making the scan parallel.\n+void ShenandoahScanRemembered::roots_do(OopIterateClosure* cl) {\n+  ShenandoahHeap* heap = ShenandoahHeap::heap();\n+  bool old_bitmap_stable = heap->old_generation()->is_mark_complete();\n+  log_debug(gc, remset)(\"Scan remembered set using bitmap: %s\", BOOL_TO_STR(old_bitmap_stable));\n+  for (size_t i = 0, n = heap->num_regions(); i < n; ++i) {\n+    ShenandoahHeapRegion* region = heap->get_region(i);\n+    if (region->is_old() && region->is_active() && !region->is_cset()) {\n+      HeapWord* start_of_range = region->bottom();\n+      HeapWord* end_of_range = region->top();\n+      size_t start_cluster_no = cluster_for_addr(start_of_range);\n+      size_t num_heapwords = end_of_range - start_of_range;\n+      unsigned int cluster_size = CardTable::card_size_in_words() * ShenandoahCardCluster::CardsPerCluster;\n+      size_t num_clusters = (size_t) ((num_heapwords - 1 + cluster_size) \/ cluster_size);\n+\n+      \/\/ Remembered set scanner\n+      if (region->is_humongous()) {\n+        process_humongous_clusters(region->humongous_start_region(), start_cluster_no, num_clusters, end_of_range, cl,\n+                                   false \/* use_write_table *\/);\n+      } else {\n+        process_clusters(start_cluster_no, num_clusters, end_of_range, cl,\n+                         false \/* use_write_table *\/, 0 \/* fake worker id *\/);\n+      }\n+    }\n+  }\n+}\n+\n+#ifndef PRODUCT\n+\/\/ Log given card stats\n+void ShenandoahScanRemembered::log_card_stats(HdrSeq* stats) {\n+  for (int i = 0; i < MAX_CARD_STAT_TYPE; i++) {\n+    log_info(gc, remset)(\"%18s: [ %8.2f %8.2f %8.2f %8.2f %8.2f ]\",\n+      _card_stats_name[i],\n+      stats[i].percentile(0), stats[i].percentile(25),\n+      stats[i].percentile(50), stats[i].percentile(75),\n+      stats[i].maximum());\n+  }\n+}\n+\n+\/\/ Log card stats for all nworkers for a specific phase t\n+void ShenandoahScanRemembered::log_card_stats(uint nworkers, CardStatLogType t) {\n+  assert(ShenandoahEnableCardStats, \"Do not call\");\n+  HdrSeq* sum_stats = card_stats_for_phase(t);\n+  log_info(gc, remset)(\"%s\", _card_stat_log_type[t]);\n+  for (uint i = 0; i < nworkers; i++) {\n+    log_worker_card_stats(i, sum_stats);\n+  }\n+\n+  \/\/ Every so often, log the cumulative global stats\n+  if (++_card_stats_log_counter[t] >= ShenandoahCardStatsLogInterval) {\n+    _card_stats_log_counter[t] = 0;\n+    log_info(gc, remset)(\"Cumulative stats\");\n+    log_card_stats(sum_stats);\n+  }\n+}\n+\n+\/\/ Log card stats for given worker_id, & clear them after merging into given cumulative stats\n+void ShenandoahScanRemembered::log_worker_card_stats(uint worker_id, HdrSeq* sum_stats) {\n+  assert(ShenandoahEnableCardStats, \"Do not call\");\n+\n+  HdrSeq* worker_card_stats = card_stats(worker_id);\n+  log_info(gc, remset)(\"Worker %u Card Stats: \", worker_id);\n+  log_card_stats(worker_card_stats);\n+  \/\/ Merge worker stats into the cumulative stats & clear worker stats\n+  merge_worker_card_stats_cumulative(worker_card_stats, sum_stats);\n+}\n+\n+void ShenandoahScanRemembered::merge_worker_card_stats_cumulative(\n+  HdrSeq* worker_stats, HdrSeq* sum_stats) {\n+  for (int i = 0; i < MAX_CARD_STAT_TYPE; i++) {\n+    sum_stats[i].add(worker_stats[i]);\n+    worker_stats[i].clear();\n+  }\n+}\n+#endif\n+\n+\/\/ A closure that takes an oop in the old generation and, if it's pointing\n+\/\/ into the young generation, dirties the corresponding remembered set entry.\n+\/\/ This is only used to rebuild the remembered set after a full GC.\n+class ShenandoahDirtyRememberedSetClosure : public BasicOopIterateClosure {\n+protected:\n+  ShenandoahGenerationalHeap* const _heap;\n+  ShenandoahScanRemembered*   const _scanner;\n+\n+public:\n+  ShenandoahDirtyRememberedSetClosure() :\n+          _heap(ShenandoahGenerationalHeap::heap()),\n+          _scanner(_heap->old_generation()->card_scan()) {}\n+\n+  template<class T>\n+  inline void work(T* p) {\n+    assert(_heap->is_in_old(p), \"Expecting to get an old gen address\");\n+    T o = RawAccess<>::oop_load(p);\n+    if (!CompressedOops::is_null(o)) {\n+      oop obj = CompressedOops::decode_not_null(o);\n+      if (_heap->is_in_young(obj)) {\n+        \/\/ Dirty the card containing the cross-generational pointer.\n+        _scanner->mark_card_as_dirty((HeapWord*) p);\n+      }\n+    }\n+  }\n+\n+  virtual void do_oop(narrowOop* p) { work(p); }\n+  virtual void do_oop(oop* p)       { work(p); }\n+};\n+\n+ShenandoahDirectCardMarkRememberedSet::ShenandoahDirectCardMarkRememberedSet(ShenandoahCardTable* card_table, size_t total_card_count) :\n+  LogCardValsPerIntPtr(log2i_exact(sizeof(intptr_t)) - log2i_exact(sizeof(CardValue))),\n+  LogCardSizeInWords(log2i_exact(CardTable::card_size_in_words())) {\n+\n+  \/\/ Paranoid assert for LogCardsPerIntPtr calculation above\n+  assert(sizeof(intptr_t) > sizeof(CardValue), \"LogsCardValsPerIntPtr would underflow\");\n+\n+  _heap = ShenandoahHeap::heap();\n+  _card_table = card_table;\n+  _total_card_count = total_card_count;\n+  _card_shift = CardTable::card_shift();\n+\n+  _byte_map = _card_table->byte_for_index(0);\n+\n+  _whole_heap_base = _card_table->addr_for(_byte_map);\n+  _byte_map_base = _byte_map - (uintptr_t(_whole_heap_base) >> _card_shift);\n+\n+  assert(total_card_count % ShenandoahCardCluster::CardsPerCluster == 0, \"Invalid card count.\");\n+  assert(total_card_count > 0, \"Card count cannot be zero.\");\n+}\n+\n+\/\/ Merge any dirty values from write table into the read table, while leaving\n+\/\/ the write table unchanged.\n+void ShenandoahDirectCardMarkRememberedSet::merge_write_table(HeapWord* start, size_t word_count) {\n+  size_t start_index = card_index_for_addr(start);\n+#ifdef ASSERT\n+  \/\/ avoid querying card_index_for_addr() for an address past end of heap\n+  size_t end_index = card_index_for_addr(start + word_count - 1) + 1;\n+#endif\n+  assert(start_index % ((size_t)1 << LogCardValsPerIntPtr) == 0, \"Expected a multiple of CardValsPerIntPtr\");\n+  assert(end_index % ((size_t)1 << LogCardValsPerIntPtr) == 0, \"Expected a multiple of CardValsPerIntPtr\");\n+\n+  \/\/ We'll access in groups of intptr_t worth of card entries\n+  intptr_t* const read_table  = (intptr_t*) &(_card_table->read_byte_map())[start_index];\n+  intptr_t* const write_table = (intptr_t*) &(_card_table->write_byte_map())[start_index];\n+\n+  \/\/ Avoid division, use shift instead\n+  assert(word_count % ((size_t)1 << (LogCardSizeInWords + LogCardValsPerIntPtr)) == 0, \"Expected a multiple of CardSizeInWords*CardValsPerIntPtr\");\n+  size_t const num = word_count >> (LogCardSizeInWords + LogCardValsPerIntPtr);\n+\n+  for (size_t i = 0; i < num; i++) {\n+    read_table[i] &= write_table[i];\n+  }\n+\n+  log_info(gc, remset)(\"Finished merging write_table into read_table.\");\n+}\n+\n+void ShenandoahDirectCardMarkRememberedSet::swap_card_tables() {\n+  CardTable::CardValue* new_ptr = _card_table->swap_read_and_write_tables();\n+\n+#ifdef ASSERT\n+  CardValue* start_bp = &(_card_table->write_byte_map())[0];\n+  CardValue* end_bp = &(start_bp[_card_table->last_valid_index()]);\n+\n+  while (start_bp <= end_bp) {\n+    assert(*start_bp == CardTable::clean_card_val(), \"Should be clean: \" PTR_FORMAT, p2i(start_bp));\n+    start_bp++;\n+  }\n+#endif\n+\n+  struct SwapTLSCardTable : public ThreadClosure {\n+    CardTable::CardValue* _new_ptr;\n+    SwapTLSCardTable(CardTable::CardValue* np) : _new_ptr(np) {}\n+    virtual void do_thread(Thread* t) {\n+      ShenandoahThreadLocalData::set_card_table(t, _new_ptr);\n+    }\n+  } swap_it(new_ptr);\n+\n+  \/\/ Iterate on threads and adjust thread local data\n+  Threads::threads_do(&swap_it);\n+\n+  log_info(gc, barrier)(\"Current write_card_table: \" PTR_FORMAT, p2i(swap_it._new_ptr));\n+}\n+\n+ShenandoahScanRememberedTask::ShenandoahScanRememberedTask(ShenandoahObjToScanQueueSet* queue_set,\n+                                                           ShenandoahObjToScanQueueSet* old_queue_set,\n+                                                           ShenandoahReferenceProcessor* rp,\n+                                                           ShenandoahRegionChunkIterator* work_list, bool is_concurrent) :\n+  WorkerTask(\"Scan Remembered Set\"),\n+  _queue_set(queue_set), _old_queue_set(old_queue_set), _rp(rp), _work_list(work_list), _is_concurrent(is_concurrent) {\n+  bool old_bitmap_stable = ShenandoahHeap::heap()->old_generation()->is_mark_complete();\n+  log_debug(gc, remset)(\"Scan remembered set using bitmap: %s\", BOOL_TO_STR(old_bitmap_stable));\n+}\n+\n+void ShenandoahScanRememberedTask::work(uint worker_id) {\n+  if (_is_concurrent) {\n+    \/\/ This sets up a thread local reference to the worker_id which is needed by the weak reference processor.\n+    ShenandoahConcurrentWorkerSession worker_session(worker_id);\n+    ShenandoahSuspendibleThreadSetJoiner stsj;\n+    do_work(worker_id);\n+  } else {\n+    \/\/ This sets up a thread local reference to the worker_id which is needed by the weak reference processor.\n+    ShenandoahParallelWorkerSession worker_session(worker_id);\n+    do_work(worker_id);\n+  }\n+}\n+\n+void ShenandoahScanRememberedTask::do_work(uint worker_id) {\n+  ShenandoahWorkerTimingsTracker x(ShenandoahPhaseTimings::init_scan_rset, ShenandoahPhaseTimings::ScanClusters, worker_id);\n+\n+  ShenandoahObjToScanQueue* q = _queue_set->queue(worker_id);\n+  ShenandoahObjToScanQueue* old = _old_queue_set == nullptr ? nullptr : _old_queue_set->queue(worker_id);\n+  ShenandoahMarkRefsClosure<YOUNG> cl(q, _rp, old);\n+  ShenandoahGenerationalHeap* heap = ShenandoahGenerationalHeap::heap();\n+  ShenandoahScanRemembered* scanner = heap->old_generation()->card_scan();\n+\n+  \/\/ set up thread local closure for shen ref processor\n+  _rp->set_mark_closure(worker_id, &cl);\n+  struct ShenandoahRegionChunk assignment;\n+  while (_work_list->next(&assignment)) {\n+    ShenandoahHeapRegion* region = assignment._r;\n+    log_debug(gc)(\"ShenandoahScanRememberedTask::do_work(%u), processing slice of region \"\n+                  SIZE_FORMAT \" at offset \" SIZE_FORMAT \", size: \" SIZE_FORMAT,\n+                  worker_id, region->index(), assignment._chunk_offset, assignment._chunk_size);\n+    if (region->is_old()) {\n+      size_t cluster_size =\n+        CardTable::card_size_in_words() * ShenandoahCardCluster::CardsPerCluster;\n+      size_t clusters = assignment._chunk_size \/ cluster_size;\n+      assert(clusters * cluster_size == assignment._chunk_size, \"Chunk assignments must align on cluster boundaries\");\n+      HeapWord* end_of_range = region->bottom() + assignment._chunk_offset + assignment._chunk_size;\n+\n+      \/\/ During concurrent mark, region->top() equals TAMS with respect to the current young-gen pass.\n+      if (end_of_range > region->top()) {\n+        end_of_range = region->top();\n+      }\n+      scanner->process_region_slice(region, assignment._chunk_offset, clusters, end_of_range, &cl, false, worker_id);\n+    }\n+#ifdef ENABLE_REMEMBERED_SET_CANCELLATION\n+    \/\/ This check is currently disabled to avoid crashes that occur\n+    \/\/ when we try to cancel remembered set scanning; it should be re-enabled\n+    \/\/ after the issues are fixed, as it would allow more prompt cancellation and\n+    \/\/ transition to degenerated \/ full GCs. Note that work that has been assigned\/\n+    \/\/ claimed above must be completed before we return here upon cancellation.\n+    if (heap->check_cancelled_gc_and_yield(_is_concurrent)) {\n+      return;\n+    }\n+#endif\n+  }\n+}\n+\n+size_t ShenandoahRegionChunkIterator::calc_regular_group_size() {\n+  \/\/ The group size is calculated from the number of regions.  Suppose the heap has N regions.  The first group processes\n+  \/\/ N\/2 regions.  The second group processes N\/4 regions, the third group N\/8 regions and so on.\n+  \/\/ Note that infinite series N\/2 + N\/4 + N\/8 + N\/16 + ...  sums to N.\n+  \/\/\n+  \/\/ The normal group size is the number of regions \/ 2.\n+  \/\/\n+  \/\/ In the case that the region_size_words is greater than _maximum_chunk_size_words, the first group_size is\n+  \/\/ larger than the normal group size because each chunk in the group will be smaller than the region size.\n+  \/\/\n+  \/\/ The last group also has more than the normal entries because it finishes the total scanning effort.  The chunk sizes are\n+  \/\/ different for each group.  The intention is that the first group processes roughly half of the heap, the second processes\n+  \/\/ half of the remaining heap, the third processes half of what remains and so on.  The smallest chunk size\n+  \/\/ is represented by _smallest_chunk_size_words.  We do not divide work any smaller than this.\n+  \/\/\n+  size_t group_size = _heap->num_regions() \/ 2;\n+  return group_size;\n+}\n+\n+size_t ShenandoahRegionChunkIterator::calc_first_group_chunk_size_b4_rebalance() {\n+  size_t words_in_first_chunk = ShenandoahHeapRegion::region_size_words();\n+  return words_in_first_chunk;\n+}\n+\n+size_t ShenandoahRegionChunkIterator::calc_num_groups() {\n+  size_t total_heap_size = _heap->num_regions() * ShenandoahHeapRegion::region_size_words();\n+  size_t num_groups = 0;\n+  size_t cumulative_group_span = 0;\n+  size_t current_group_span = _first_group_chunk_size_b4_rebalance * _regular_group_size;\n+  size_t smallest_group_span = smallest_chunk_size_words() * _regular_group_size;\n+  while ((num_groups < _maximum_groups) && (cumulative_group_span + current_group_span <= total_heap_size)) {\n+    num_groups++;\n+    cumulative_group_span += current_group_span;\n+    if (current_group_span <= smallest_group_span) {\n+      break;\n+    } else {\n+      current_group_span \/= 2;    \/\/ Each group spans half of what the preceding group spanned.\n+    }\n+  }\n+  \/\/ Loop post condition:\n+  \/\/   num_groups <= _maximum_groups\n+  \/\/   cumulative_group_span is the memory spanned by num_groups\n+  \/\/   current_group_span is the span of the last fully populated group (assuming loop iterates at least once)\n+  \/\/   each of num_groups is fully populated with _regular_group_size chunks in each\n+  \/\/ Non post conditions:\n+  \/\/   cumulative_group_span may be less than total_heap size for one or more of the folowing reasons\n+  \/\/   a) The number of regions remaining to be spanned is smaller than a complete group, or\n+  \/\/   b) We have filled up all groups through _maximum_groups and still have not spanned all regions\n+\n+  if (cumulative_group_span < total_heap_size) {\n+    \/\/ We've got more regions to span\n+    if ((num_groups < _maximum_groups) && (current_group_span > smallest_group_span)) {\n+      num_groups++;             \/\/ Place all remaining regions into a new not-full group (chunk_size half that of previous group)\n+    }\n+    \/\/ Else we are unable to create a new group because we've exceed the number of allowed groups or have reached the\n+    \/\/ minimum chunk size.\n+\n+    \/\/ Any remaining regions will be treated as if they are part of the most recently created group.  This group will\n+    \/\/ have more than _regular_group_size chunks within it.\n+  }\n+  assert (num_groups <= _maximum_groups, \"Cannot have more than %zu groups\", _maximum_groups);\n+  return num_groups;\n+}\n+\n+size_t ShenandoahRegionChunkIterator::calc_total_chunks() {\n+  size_t region_size_words = ShenandoahHeapRegion::region_size_words();\n+  size_t unspanned_heap_size = _heap->num_regions() * region_size_words;\n+  size_t num_chunks = 0;\n+  size_t cumulative_group_span = 0;\n+  size_t current_group_span = _first_group_chunk_size_b4_rebalance * _regular_group_size;\n+  size_t smallest_group_span = smallest_chunk_size_words() * _regular_group_size;\n+\n+  \/\/ The first group gets special handling because the first chunk size can be no larger than _maximum_chunk_size_words\n+  if (region_size_words > _maximum_chunk_size_words) {\n+    \/\/ In the case that we shrink the first group's chunk size, certain other groups will also be subsumed within the first group\n+    size_t effective_chunk_size = _first_group_chunk_size_b4_rebalance;\n+    uint coalesced_groups = 0;\n+    while (effective_chunk_size >= _maximum_chunk_size_words) {\n+      \/\/ Each iteration of this loop subsumes one original group into a new rebalanced initial group.\n+      num_chunks += current_group_span \/ _maximum_chunk_size_words;\n+      unspanned_heap_size -= current_group_span;\n+      effective_chunk_size \/= 2;\n+      current_group_span \/= 2;\n+      coalesced_groups++;\n+    }\n+    assert(effective_chunk_size * 2 == _maximum_chunk_size_words,\n+           \"We assume _first_group_chunk_size_b4_rebalance is _maximum_chunk_size_words * a power of two\");\n+    _largest_chunk_size_words = _maximum_chunk_size_words;\n+    _adjusted_num_groups = _num_groups - (coalesced_groups - 1);\n+  } else {\n+    num_chunks = _regular_group_size;\n+    unspanned_heap_size -= current_group_span;\n+    _largest_chunk_size_words = current_group_span \/ num_chunks;\n+    _adjusted_num_groups = _num_groups;\n+    current_group_span \/= 2;\n+  }\n+\n+  size_t spanned_groups = 1;\n+  while (unspanned_heap_size > 0) {\n+    if (current_group_span <= unspanned_heap_size) {\n+      unspanned_heap_size -= current_group_span;\n+      num_chunks += _regular_group_size;\n+      spanned_groups++;\n+\n+      \/\/ _num_groups is the number of groups required to span the configured heap size.  We are not allowed\n+      \/\/ to change the number of groups.  The last group is responsible for spanning all chunks not spanned\n+      \/\/ by previously processed groups.\n+      if (spanned_groups >= _num_groups) {\n+        \/\/ The last group has more than _regular_group_size entries.\n+        size_t chunk_span = current_group_span \/ _regular_group_size;\n+        size_t extra_chunks = unspanned_heap_size \/ chunk_span;\n+        assert (extra_chunks * chunk_span == unspanned_heap_size, \"Chunks must precisely span regions\");\n+        num_chunks += extra_chunks;\n+        return num_chunks;\n+      } else if (current_group_span <= smallest_group_span) {\n+        \/\/ We cannot introduce new groups because we've reached the lower bound on group size.  So this last\n+        \/\/ group may hold extra chunks.\n+        size_t chunk_span = smallest_chunk_size_words();\n+        size_t extra_chunks = unspanned_heap_size \/ chunk_span;\n+        assert (extra_chunks * chunk_span == unspanned_heap_size, \"Chunks must precisely span regions\");\n+        num_chunks += extra_chunks;\n+        return num_chunks;\n+      } else {\n+        current_group_span \/= 2;\n+      }\n+    } else {\n+      \/\/ This last group has fewer than _regular_group_size entries.\n+      size_t chunk_span = current_group_span \/ _regular_group_size;\n+      size_t last_group_size = unspanned_heap_size \/ chunk_span;\n+      assert (last_group_size * chunk_span == unspanned_heap_size, \"Chunks must precisely span regions\");\n+      num_chunks += last_group_size;\n+      return num_chunks;\n+    }\n+  }\n+  return num_chunks;\n+}\n+\n+ShenandoahRegionChunkIterator::ShenandoahRegionChunkIterator(size_t worker_count) :\n+    ShenandoahRegionChunkIterator(ShenandoahHeap::heap(), worker_count)\n+{\n+}\n+\n+ShenandoahRegionChunkIterator::ShenandoahRegionChunkIterator(ShenandoahHeap* heap, size_t worker_count) :\n+    _heap(heap),\n+    _regular_group_size(calc_regular_group_size()),\n+    _first_group_chunk_size_b4_rebalance(calc_first_group_chunk_size_b4_rebalance()),\n+    _num_groups(calc_num_groups()),\n+    _total_chunks(calc_total_chunks()),\n+    _index(0)\n+{\n+#ifdef ASSERT\n+  size_t expected_chunk_size_words = _clusters_in_smallest_chunk * CardTable::card_size_in_words() * ShenandoahCardCluster::CardsPerCluster;\n+  assert(smallest_chunk_size_words() == expected_chunk_size_words, \"_smallest_chunk_size (\" SIZE_FORMAT\") is not valid because it does not equal (\" SIZE_FORMAT \")\",\n+         smallest_chunk_size_words(), expected_chunk_size_words);\n+  assert(_num_groups <= _maximum_groups,\n+         \"The number of remembered set scanning groups must be less than or equal to maximum groups\");\n+  assert(smallest_chunk_size_words() << (_adjusted_num_groups - 1) == _largest_chunk_size_words,\n+         \"The number of groups (%zu) needs to span smallest chunk size (%zu) to largest chunk size (%zu)\",\n+         _adjusted_num_groups, smallest_chunk_size_words(), _largest_chunk_size_words);\n+#endif\n+\n+  size_t words_in_region = ShenandoahHeapRegion::region_size_words();\n+  _region_index[0] = 0;\n+  _group_offset[0] = 0;\n+  if (words_in_region > _maximum_chunk_size_words) {\n+    \/\/ In the case that we shrink the first group's chunk size, certain other groups will also be subsumed within the first group\n+    size_t num_chunks = 0;\n+    size_t effective_chunk_size = _first_group_chunk_size_b4_rebalance;\n+    size_t  current_group_span = effective_chunk_size * _regular_group_size;\n+    while (effective_chunk_size >= _maximum_chunk_size_words) {\n+      num_chunks += current_group_span \/ _maximum_chunk_size_words;\n+      effective_chunk_size \/= 2;\n+      current_group_span \/= 2;\n+    }\n+    _group_entries[0] = num_chunks;\n+    _group_chunk_size[0] = _maximum_chunk_size_words;\n+  } else {\n+    _group_entries[0] = _regular_group_size;\n+    _group_chunk_size[0] = _first_group_chunk_size_b4_rebalance;\n+  }\n+\n+  size_t previous_group_span = _group_entries[0] * _group_chunk_size[0];\n+  for (size_t i = 1; i < _adjusted_num_groups; i++) {\n+    _group_chunk_size[i] = _group_chunk_size[i-1] \/ 2;\n+    size_t chunks_in_group = _regular_group_size;\n+    size_t this_group_span = _group_chunk_size[i] * chunks_in_group;\n+    size_t total_span_of_groups = previous_group_span + this_group_span;\n+    _region_index[i] = previous_group_span \/ words_in_region;\n+    _group_offset[i] = previous_group_span % words_in_region;\n+    _group_entries[i] = _group_entries[i-1] + _regular_group_size;\n+    previous_group_span = total_span_of_groups;\n+  }\n+  if (_group_entries[_adjusted_num_groups-1] < _total_chunks) {\n+    assert((_total_chunks - _group_entries[_adjusted_num_groups-1]) * _group_chunk_size[_adjusted_num_groups-1] + previous_group_span ==\n+           heap->num_regions() * words_in_region, \"Total region chunks (\" SIZE_FORMAT\n+           \") do not span total heap regions (\" SIZE_FORMAT \")\", _total_chunks, _heap->num_regions());\n+    previous_group_span += (_total_chunks - _group_entries[_adjusted_num_groups-1]) * _group_chunk_size[_adjusted_num_groups-1];\n+    _group_entries[_adjusted_num_groups-1] = _total_chunks;\n+  }\n+  assert(previous_group_span == heap->num_regions() * words_in_region, \"Total region chunks (\" SIZE_FORMAT\n+         \") do not span total heap regions (\" SIZE_FORMAT \"): \" SIZE_FORMAT \" does not equal \" SIZE_FORMAT,\n+         _total_chunks, _heap->num_regions(), previous_group_span, heap->num_regions() * words_in_region);\n+\n+  \/\/ Not necessary, but keeps things tidy\n+  for (size_t i = _adjusted_num_groups; i < _maximum_groups; i++) {\n+    _region_index[i] = 0;\n+    _group_offset[i] = 0;\n+    _group_entries[i] = _group_entries[i-1];\n+    _group_chunk_size[i] = 0;\n+  }\n+}\n+\n+void ShenandoahRegionChunkIterator::reset() {\n+  _index = 0;\n+}\n+\n+ShenandoahReconstructRememberedSetTask::ShenandoahReconstructRememberedSetTask(ShenandoahRegionIterator* regions)\n+  : WorkerTask(\"Shenandoah Reset Bitmap\")\n+  , _regions(regions) { }\n+\n+void ShenandoahReconstructRememberedSetTask::work(uint worker_id) {\n+  ShenandoahParallelWorkerSession worker_session(worker_id);\n+  ShenandoahHeapRegion* r = _regions->next();\n+  ShenandoahGenerationalHeap* heap = ShenandoahGenerationalHeap::heap();\n+  ShenandoahScanRemembered* scanner = heap->old_generation()->card_scan();\n+  ShenandoahDirtyRememberedSetClosure dirty_cards_for_cross_generational_pointers;\n+\n+  while (r != nullptr) {\n+    if (r->is_old() && r->is_active()) {\n+      HeapWord* obj_addr = r->bottom();\n+      if (r->is_humongous_start()) {\n+        \/\/ First, clear the remembered set\n+        oop obj = cast_to_oop(obj_addr);\n+        size_t size = obj->size();\n+\n+        size_t num_regions = ShenandoahHeapRegion::required_regions(size * HeapWordSize);\n+        size_t region_index = r->index();\n+        ShenandoahHeapRegion* humongous_region = heap->get_region(region_index);\n+        while (num_regions-- != 0) {\n+          scanner->reset_object_range(humongous_region->bottom(), humongous_region->end());\n+          region_index++;\n+          humongous_region = heap->get_region(region_index);\n+        }\n+\n+        \/\/ Then register the humongous object and DIRTY relevant remembered set cards\n+        scanner->register_object_without_lock(obj_addr);\n+        obj->oop_iterate(&dirty_cards_for_cross_generational_pointers);\n+      } else if (!r->is_humongous()) {\n+        scanner->reset_object_range(r->bottom(), r->end());\n+\n+        \/\/ Then iterate over all objects, registering object and DIRTYing relevant remembered set cards\n+        HeapWord* t = r->top();\n+        while (obj_addr < t) {\n+          oop obj = cast_to_oop(obj_addr);\n+          scanner->register_object_without_lock(obj_addr);\n+          obj_addr += obj->oop_iterate_size(&dirty_cards_for_cross_generational_pointers);\n+        }\n+      } \/\/ else, ignore humongous continuation region\n+    }\n+    \/\/ else, this region is FREE or YOUNG or inactive and we can ignore it.\n+    r = _regions->next();\n+  }\n+}\n","filename":"src\/hotspot\/share\/gc\/shenandoah\/shenandoahScanRemembered.cpp","additions":979,"deletions":0,"binary":false,"changes":979,"status":"added"},{"patch":"@@ -0,0 +1,1025 @@\n+\/*\n+ * Copyright Amazon.com Inc. or its affiliates. All Rights Reserved.\n+ * DO NOT ALTER OR REMOVE COPYRIGHT NOTICES OR THIS FILE HEADER.\n+ *\n+ * This code is free software; you can redistribute it and\/or modify it\n+ * under the terms of the GNU General Public License version 2 only, as\n+ * published by the Free Software Foundation.\n+ *\n+ * This code is distributed in the hope that it will be useful, but WITHOUT\n+ * ANY WARRANTY; without even the implied warranty of MERCHANTABILITY or\n+ * FITNESS FOR A PARTICULAR PURPOSE.  See the GNU General Public License\n+ * version 2 for more details (a copy is included in the LICENSE file that\n+ * accompanied this code).\n+ *\n+ * You should have received a copy of the GNU General Public License version\n+ * 2 along with this work; if not, write to the Free Software Foundation,\n+ * Inc., 51 Franklin St, Fifth Floor, Boston, MA 02110-1301 USA.\n+ *\n+ * Please contact Oracle, 500 Oracle Parkway, Redwood Shores, CA 94065 USA\n+ * or visit www.oracle.com if you need additional information or have any\n+ * questions.\n+ *\n+ *\/\n+\n+#ifndef SHARE_GC_SHENANDOAH_SHENANDOAHSCANREMEMBERED_HPP\n+#define SHARE_GC_SHENANDOAH_SHENANDOAHSCANREMEMBERED_HPP\n+\n+\/\/ Terminology used within this source file:\n+\/\/\n+\/\/ Card Entry:   This is the information that identifies whether a\n+\/\/               particular card-table entry is Clean or Dirty.  A clean\n+\/\/               card entry denotes that the associated memory does not\n+\/\/               hold references to young-gen memory.\n+\/\/\n+\/\/ Card Region, aka\n+\/\/ Card Memory:  This is the region of memory that is assocated with a\n+\/\/               particular card entry.\n+\/\/\n+\/\/ Card Cluster: A card cluster represents 64 card entries.  A card\n+\/\/               cluster is the minimal amount of work performed at a\n+\/\/               time by a parallel thread.  Note that the work required\n+\/\/               to scan a card cluster is somewhat variable in that the\n+\/\/               required effort depends on how many cards are dirty, how\n+\/\/               many references are held within the objects that span a\n+\/\/               DIRTY card's memory, and on the size of the object\n+\/\/               that spans the end of a DIRTY card's memory (because\n+\/\/               that object, if it's not an array, may need to be scanned in\n+\/\/               its entirety, when the object is imprecisely dirtied. Imprecise\n+\/\/               dirtying is when the card corresponding to the object header\n+\/\/               is dirtied, rather than the card on which the updated field lives).\n+\/\/               To better balance work amongst them, parallel worker threads dynamically\n+\/\/               claim clusters and are flexible in the number of clusters they\n+\/\/               process.\n+\/\/\n+\/\/ A cluster represents a \"natural\" quantum of work to be performed by\n+\/\/ a parallel GC thread's background remembered set scanning efforts.\n+\/\/ The notion of cluster is similar to the notion of stripe in the\n+\/\/ implementation of parallel GC card scanning.  However, a cluster is\n+\/\/ typically smaller than a stripe, enabling finer grain division of\n+\/\/ labor between multiple threads, and potentially better load balancing\n+\/\/ when dirty cards are not uniformly distributed in the heap, as is often\n+\/\/ the case with generational workloads where more recently promoted objects\n+\/\/ may be dirtied more frequently that older objects.\n+\/\/\n+\/\/ For illustration, consider the following possible JVM configurations:\n+\/\/\n+\/\/   Scenario 1:\n+\/\/     RegionSize is 128 MB\n+\/\/     Span of a card entry is 512 B\n+\/\/     Each card table entry consumes 1 B\n+\/\/     Assume one long word (8 B)of the card table represents a cluster.\n+\/\/       This long word holds 8 card table entries, spanning a\n+\/\/       total of 8*512 B = 4 KB of the heap\n+\/\/     The number of clusters per region is 128 MB \/ 4 KB = 32 K\n+\/\/\n+\/\/   Scenario 2:\n+\/\/     RegionSize is 128 MB\n+\/\/     Span of each card entry is 128 B\n+\/\/     Each card table entry consumes 1 bit\n+\/\/     Assume one int word (4 B) of the card table represents a cluster.\n+\/\/       This int word holds 32 b\/1 b = 32 card table entries, spanning a\n+\/\/       total of 32 * 128 B = 4 KB of the heap\n+\/\/     The number of clusters per region is 128 MB \/ 4 KB = 32 K\n+\/\/\n+\/\/   Scenario 3:\n+\/\/     RegionSize is 128 MB\n+\/\/     Span of each card entry is 512 B\n+\/\/     Each card table entry consumes 1 bit\n+\/\/     Assume one long word (8 B) of card table represents a cluster.\n+\/\/       This long word holds 64 b\/ 1 b = 64 card table entries, spanning a\n+\/\/       total of 64 * 512 B = 32 KB of the heap\n+\/\/     The number of clusters per region is 128 MB \/ 32 KB = 4 K\n+\/\/\n+\/\/ At the start of a new young-gen concurrent mark pass, the gang of\n+\/\/ Shenandoah worker threads collaborate in performing the following\n+\/\/ actions:\n+\/\/\n+\/\/  Let old_regions = number of ShenandoahHeapRegion comprising\n+\/\/    old-gen memory\n+\/\/  Let region_size = ShenandoahHeapRegion::region_size_bytes()\n+\/\/    represent the number of bytes in each region\n+\/\/  Let clusters_per_region = region_size \/ 512\n+\/\/  Let rs represent the ShenandoahDirectCardMarkRememberedSet\n+\/\/\n+\/\/  for each ShenandoahHeapRegion old_region in the whole heap\n+\/\/    determine the cluster number of the first cluster belonging\n+\/\/      to that region\n+\/\/    for each cluster contained within that region\n+\/\/      Assure that exactly one worker thread processes each\n+\/\/      cluster, each thread making a series of invocations of the\n+\/\/      following:\n+\/\/\n+\/\/        rs->process_clusters(worker_id, ReferenceProcessor *,\n+\/\/                             ShenandoahConcurrentMark *, cluster_no, cluster_count,\n+\/\/                             HeapWord *end_of_range, OopClosure *oops);\n+\/\/\n+\/\/  For efficiency, divide up the clusters so that different threads\n+\/\/  are responsible for processing different clusters.  Processing costs\n+\/\/  may vary greatly between clusters for the following reasons:\n+\/\/\n+\/\/        a) some clusters contain mostly dirty cards and other\n+\/\/           clusters contain mostly clean cards\n+\/\/        b) some clusters contain mostly primitive data and other\n+\/\/           clusters contain mostly reference data\n+\/\/        c) some clusters are spanned by very large non-array objects that\n+\/\/           begin in some other cluster.  When a large non-array object\n+\/\/           beginning in a preceding cluster spans large portions of\n+\/\/           this cluster, then because of imprecise dirtying, the\n+\/\/           portion of the object in this cluster may be clean, but\n+\/\/           will need to be processed by the worker responsible for\n+\/\/           this cluster, potentially increasing its work.\n+\/\/        d) in the case that the end of this cluster is spanned by a\n+\/\/           very large non-array object, the worker for this cluster will\n+\/\/           be responsible for processing the portion of the object\n+\/\/           in this cluster.\n+\/\/\n+\/\/ Though an initial division of labor between marking threads may\n+\/\/ assign equal numbers of clusters to be scanned by each thread, it\n+\/\/ should be expected that some threads will finish their assigned\n+\/\/ work before others.  Therefore, some amount of the full remembered\n+\/\/ set scanning effort should be held back and assigned incrementally\n+\/\/ to the threads that end up with excess capacity.  Consider the\n+\/\/ following strategy for dividing labor:\n+\/\/\n+\/\/        1. Assume there are 8 marking threads and 1024 remembered\n+\/\/           set clusters to be scanned.\n+\/\/        2. Assign each thread to scan 64 clusters.  This leaves\n+\/\/           512 (1024 - (8*64)) clusters to still be scanned.\n+\/\/        3. As the 8 server threads complete previous cluster\n+\/\/           scanning assignments, issue each of the next 8 scanning\n+\/\/           assignments as units of 32 additional cluster each.\n+\/\/           In the case that there is high variance in effort\n+\/\/           associated with previous cluster scanning assignments,\n+\/\/           multiples of these next assignments may be serviced by\n+\/\/           the server threads that were previously assigned lighter\n+\/\/           workloads.\n+\/\/        4. Make subsequent scanning assignments as follows:\n+\/\/             a) 8 assignments of size 16 clusters\n+\/\/             b) 8 assignments of size 8 clusters\n+\/\/             c) 16 assignments of size 4 clusters\n+\/\/\n+\/\/    When there is no more remembered set processing work to be\n+\/\/    assigned to a newly idled worker thread, that thread can move\n+\/\/    on to work on other tasks associated with root scanning until such\n+\/\/    time as all clusters have been examined.\n+\/\/\n+\/\/ Remembered set scanning is designed to run concurrently with\n+\/\/ mutator threads, with multiple concurrent workers. Furthermore, the\n+\/\/ current implementation of remembered set scanning never clears a\n+\/\/ card once it has been marked.\n+\/\/\n+\/\/ These limitations will be addressed in future enhancements to the\n+\/\/ existing implementation.\n+\n+#include \"gc\/shared\/workerThread.hpp\"\n+#include \"gc\/shenandoah\/shenandoahCardStats.hpp\"\n+#include \"gc\/shenandoah\/shenandoahCardTable.hpp\"\n+#include \"gc\/shenandoah\/shenandoahNumberSeq.hpp\"\n+#include \"gc\/shenandoah\/shenandoahTaskqueue.hpp\"\n+#include \"memory\/iterator.hpp\"\n+#include \"utilities\/globalDefinitions.hpp\"\n+\n+class ShenandoahReferenceProcessor;\n+class ShenandoahConcurrentMark;\n+class ShenandoahHeap;\n+class ShenandoahHeapRegion;\n+class ShenandoahRegionIterator;\n+class ShenandoahMarkingContext;\n+\n+class CardTable;\n+typedef CardTable::CardValue CardValue;\n+\n+class ShenandoahDirectCardMarkRememberedSet: public CHeapObj<mtGC> {\n+\n+private:\n+\n+  \/\/ Use symbolic constants defined in cardTable.hpp\n+  \/\/  CardTable::card_shift = 9;\n+  \/\/  CardTable::card_size = 512;  (default value 512, a power of 2 >= 128)\n+  \/\/  CardTable::card_size_in_words = 64; (default value 64, a power of 2 >= 16)\n+  \/\/  CardTable::clean_card_val()\n+  \/\/  CardTable::dirty_card_val()\n+\n+  \/\/ See shenandoahCardTable.hpp\n+  \/\/  ShenandoahMinCardSizeInBytes 128\n+\n+  const size_t LogCardValsPerIntPtr;    \/\/ the number of card values (entries) in an intptr_t\n+  const size_t LogCardSizeInWords;      \/\/ the size of a card in heap word units\n+\n+  ShenandoahHeap* _heap;\n+  ShenandoahCardTable* _card_table;\n+  size_t _card_shift;\n+  size_t _total_card_count;\n+  HeapWord* _whole_heap_base;   \/\/ Points to first HeapWord of data contained within heap memory\n+  CardValue* _byte_map;         \/\/ Points to first entry within the card table\n+  CardValue* _byte_map_base;    \/\/ Points to byte_map minus the bias computed from address of heap memory\n+\n+public:\n+\n+  \/\/ count is the number of cards represented by the card table.\n+  ShenandoahDirectCardMarkRememberedSet(ShenandoahCardTable* card_table, size_t total_card_count);\n+\n+  \/\/ Card index is zero-based relative to _byte_map.\n+  size_t last_valid_index() const;\n+  size_t total_cards() const;\n+  size_t card_index_for_addr(HeapWord* p) const;\n+  HeapWord* addr_for_card_index(size_t card_index) const;\n+  inline const CardValue* get_card_table_byte_map(bool use_write_table) const {\n+    return use_write_table ? _card_table->write_byte_map() : _card_table->read_byte_map();\n+  }\n+\n+  inline bool is_card_dirty(size_t card_index) const;\n+  inline bool is_write_card_dirty(size_t card_index) const;\n+  inline void mark_card_as_dirty(size_t card_index);\n+  inline void mark_range_as_dirty(size_t card_index, size_t num_cards);\n+  inline void mark_card_as_clean(size_t card_index);\n+  inline void mark_range_as_clean(size_t card_index, size_t num_cards);\n+  inline bool is_card_dirty(HeapWord* p) const;\n+  inline bool is_write_card_dirty(HeapWord* p) const;\n+  inline void mark_card_as_dirty(HeapWord* p);\n+  inline void mark_range_as_dirty(HeapWord* p, size_t num_heap_words);\n+  inline void mark_range_as_clean(HeapWord* p, size_t num_heap_words);\n+\n+  \/\/ See comment in ShenandoahScanRemembered\n+  inline void mark_read_table_as_clean();\n+\n+  \/\/ Merge any dirty values from write table into the read table, while leaving\n+  \/\/ the write table unchanged.\n+  void merge_write_table(HeapWord* start, size_t word_count);\n+\n+  \/\/ See comment in ShenandoahScanRemembered\n+  void swap_card_tables();\n+};\n+\n+\/\/ A ShenandoahCardCluster represents the minimal unit of work\n+\/\/ performed by independent parallel GC threads during scanning of\n+\/\/ remembered sets.\n+\/\/\n+\/\/ The GC threads that perform card-table remembered set scanning may\n+\/\/ overwrite card-table entries to mark them as clean in the case that\n+\/\/ the associated memory no longer holds references to young-gen\n+\/\/ memory.  Rather than access the card-table entries directly, all GC\n+\/\/ thread access to card-table information is made by way of the\n+\/\/ ShenandoahCardCluster data abstraction.  This abstraction\n+\/\/ effectively manages access to multiple possible underlying\n+\/\/ remembered set implementations, including a traditional card-table\n+\/\/ approach and a SATB-based approach.\n+\/\/\n+\/\/ The API services represent a compromise between efficiency and\n+\/\/ convenience.\n+\/\/\n+\/\/ Multiple GC threads that scan the remembered set\n+\/\/ in parallel.  The desire is to divide the complete scanning effort\n+\/\/ into multiple clusters of work that can be independently processed\n+\/\/ by individual threads without need for synchronizing efforts\n+\/\/ between the work performed by each task.  The term \"cluster\" of\n+\/\/ work is similar to the term \"stripe\" as used in the implementation\n+\/\/ of Parallel GC.\n+\/\/\n+\/\/ Complexity arises when an object to be scanned crosses the boundary\n+\/\/ between adjacent cluster regions.  Here is the protocol that we currently\n+\/\/ follow:\n+\/\/\n+\/\/  1. The thread responsible for scanning the cards in a cluster modifies\n+\/\/     the associated card-table entries. Only cards that are dirty are\n+\/\/     processed, except as described below for the case of objects that\n+\/\/     straddle more than one card.\n+\/\/  2. Object Arrays are precisely dirtied, so only the portion of the obj-array\n+\/\/     that overlaps the range of dirty cards in its cluster are scanned\n+\/\/     by each worker thread. This holds for portions of obj-arrays that extend\n+\/\/     over clusters processed by different workers, with each worked responsible\n+\/\/     for scanning the portion of the obj-array overlapping the dirty cards in\n+\/\/     its cluster.\n+\/\/  3. Non-array objects are precisely dirtied by the interpreter and the compilers\n+\/\/     For such objects that extend over multiple cards, or even multiple clusters,\n+\/\/     the entire object is scanned by the worker that processes the (dirty) card on\n+\/\/     which the object's header lies. (However, GC workers should precisely dirty the\n+\/\/     cards with inter-regional\/inter-generational pointers in the body of this object,\n+\/\/     thus making subsequent scans potentially less expensive.) Such larger non-array\n+\/\/     objects are relatively rare.\n+\/\/\n+\/\/  A possible criticism:\n+\/\/  C. The representation of pointer location descriptive information\n+\/\/     within Klass representations is not designed for efficient\n+\/\/     \"random access\".  An alternative approach to this design would\n+\/\/     be to scan very large objects multiple times, once for each\n+\/\/     cluster that is spanned by the object's range.  This reduces\n+\/\/     unnecessary overscan, but it introduces different sorts of\n+\/\/     overhead effort:\n+\/\/       i) For each spanned cluster, we have to look up the start of\n+\/\/          the crossing object.\n+\/\/      ii) Each time we scan the very large object, we have to\n+\/\/          sequentially walk through its pointer location\n+\/\/          descriptors, skipping over all of the pointers that\n+\/\/          precede the start of the range of addresses that we\n+\/\/          consider relevant.\n+\n+\n+\/\/ Because old-gen heap memory is not necessarily contiguous, and\n+\/\/ because cards are not necessarily maintained for young-gen memory,\n+\/\/ consecutive card numbers do not necessarily correspond to consecutive\n+\/\/ address ranges.  For the traditional direct-card-marking\n+\/\/ implementation of this interface, consecutive card numbers are\n+\/\/ likely to correspond to contiguous regions of memory, but this\n+\/\/ should not be assumed.  Instead, rely only upon the following:\n+\/\/\n+\/\/  1. All card numbers for cards pertaining to the same\n+\/\/     ShenandoahHeapRegion are consecutively numbered.\n+\/\/  2. In the case that neighboring ShenandoahHeapRegions both\n+\/\/     represent old-gen memory, the card regions that span the\n+\/\/     boundary between these neighboring heap regions will be\n+\/\/     consecutively numbered.\n+\/\/  3. (A corollary) In the case that an old-gen object straddles the\n+\/\/     boundary between two heap regions, the card regions that\n+\/\/     correspond to the span of this object will be consecutively\n+\/\/     numbered.\n+\/\/\n+\/\/ ShenandoahCardCluster abstracts access to the remembered set\n+\/\/ and also keeps track of crossing map information to allow efficient\n+\/\/ resolution of object start addresses.\n+\/\/\n+\/\/ ShenandoahCardCluster supports all of the services of\n+\/\/ DirectCardMarkRememberedSet, plus it supports register_object() and lookup_object().\n+\/\/ Note that we only need to register the start addresses of the object that\n+\/\/ overlays the first address of a card; we need to do this for every card.\n+\/\/ In other words, register_object() checks if the object crosses a card boundary,\n+\/\/ and updates the offset value for each card that the object crosses into.\n+\/\/ For objects that don't straddle cards, nothing needs to be done.\n+\/\/\n+class ShenandoahCardCluster: public CHeapObj<mtGC> {\n+\n+private:\n+  ShenandoahDirectCardMarkRememberedSet* _rs;\n+\n+public:\n+  static const size_t CardsPerCluster = 64;\n+\n+private:\n+  typedef struct cross_map { uint8_t first; uint8_t last; } xmap;\n+  typedef union crossing_info { uint16_t short_word; xmap offsets; } crossing_info;\n+\n+  \/\/ ObjectStartsInCardRegion bit is set within a crossing_info.offsets.start iff at least one object starts within\n+  \/\/ a particular card region.  We pack this bit into start byte under assumption that start byte is accessed less\n+  \/\/ frequently than last byte.  This is true when number of clean cards is greater than number of dirty cards.\n+  static const uint8_t ObjectStartsInCardRegion = 0x80;\n+  static const uint8_t FirstStartBits           = 0x7f;\n+\n+  \/\/ Check that we have enough bits to store the largest possible offset into a card for an object start.\n+  \/\/ The value for maximum card size is based on the constraints for GCCardSizeInBytes in gc_globals.hpp.\n+  static const int MaxCardSize = NOT_LP64(512) LP64_ONLY(1024);\n+  STATIC_ASSERT((MaxCardSize \/ HeapWordSize) - 1 <= FirstStartBits);\n+\n+  crossing_info* _object_starts;\n+\n+public:\n+  \/\/ If we're setting first_start, assume the card has an object.\n+  inline void set_first_start(size_t card_index, uint8_t value) {\n+    _object_starts[card_index].offsets.first = ObjectStartsInCardRegion | value;\n+  }\n+\n+  inline void set_last_start(size_t card_index, uint8_t value) {\n+    _object_starts[card_index].offsets.last = value;\n+  }\n+\n+  inline void set_starts_object_bit(size_t card_index) {\n+    _object_starts[card_index].offsets.first |= ObjectStartsInCardRegion;\n+  }\n+\n+  inline void clear_starts_object_bit(size_t card_index) {\n+    _object_starts[card_index].offsets.first &= ~ObjectStartsInCardRegion;\n+  }\n+\n+  \/\/ Returns true iff an object is known to start within the card memory associated with card card_index.\n+  inline bool starts_object(size_t card_index) const {\n+    return (_object_starts[card_index].offsets.first & ObjectStartsInCardRegion) != 0;\n+  }\n+\n+  inline void clear_objects_in_range(HeapWord* addr, size_t num_words) {\n+    size_t card_index = _rs->card_index_for_addr(addr);\n+    size_t last_card_index = _rs->card_index_for_addr(addr + num_words - 1);\n+    while (card_index <= last_card_index)\n+      _object_starts[card_index++].short_word = 0;\n+  }\n+\n+  ShenandoahCardCluster(ShenandoahDirectCardMarkRememberedSet* rs) {\n+    _rs = rs;\n+    _object_starts = NEW_C_HEAP_ARRAY(crossing_info, rs->total_cards() + 1, mtGC); \/\/ the +1 is to account for card table guarding entry\n+    for (size_t i = 0; i < rs->total_cards(); i++) {\n+      _object_starts[i].short_word = 0;\n+    }\n+  }\n+\n+  ~ShenandoahCardCluster() {\n+    FREE_C_HEAP_ARRAY(crossing_info, _object_starts);\n+    _object_starts = nullptr;\n+  }\n+\n+  \/\/ There is one entry within the object_starts array for each card entry.\n+  \/\/\n+  \/\/  Suppose multiple garbage objects are coalesced during GC sweep\n+  \/\/  into a single larger \"free segment\".  As each two objects are\n+  \/\/  coalesced together, the start information pertaining to the second\n+  \/\/  object must be removed from the objects_starts array.  If the\n+  \/\/  second object had been the first object within card memory,\n+  \/\/  the new first object is the object that follows that object if\n+  \/\/  that starts within the same card memory, or NoObject if the\n+  \/\/  following object starts within the following cluster.  If the\n+  \/\/  second object had been the last object in the card memory,\n+  \/\/  replace this entry with the newly coalesced object if it starts\n+  \/\/  within the same card memory, or with NoObject if it starts in a\n+  \/\/  preceding card's memory.\n+  \/\/\n+  \/\/  Suppose a large free segment is divided into a smaller free\n+  \/\/  segment and a new object.  The second part of the newly divided\n+  \/\/  memory must be registered as a new object, overwriting at most\n+  \/\/  one first_start and one last_start entry.  Note that one of the\n+  \/\/  newly divided two objects might be a new GCLAB.\n+  \/\/\n+  \/\/  Suppose postprocessing of a GCLAB finds that the original GCLAB\n+  \/\/  has been divided into N objects.  Each of the N newly allocated\n+  \/\/  objects will be registered, overwriting at most one first_start\n+  \/\/  and one last_start entries.\n+  \/\/\n+  \/\/  No object registration operations are linear in the length of\n+  \/\/  the registered objects.\n+  \/\/\n+  \/\/ Consider further the following observations regarding object\n+  \/\/ registration costs:\n+  \/\/\n+  \/\/   1. The cost is paid once for each old-gen object (Except when\n+  \/\/      an object is demoted and repromoted, in which case we would\n+  \/\/      pay the cost again).\n+  \/\/   2. The cost can be deferred so that there is no urgency during\n+  \/\/      mutator copy-on-first-access promotion.  Background GC\n+  \/\/      threads will update the object_starts array by post-\n+  \/\/      processing the contents of retired PLAB buffers.\n+  \/\/   3. The bet is that these costs are paid relatively rarely\n+  \/\/      because:\n+  \/\/      a) Most objects die young and objects that die in young-gen\n+  \/\/         memory never need to be registered with the object_starts\n+  \/\/         array.\n+  \/\/      b) Most objects that are promoted into old-gen memory live\n+  \/\/         there without further relocation for a relatively long\n+  \/\/         time, so we get a lot of benefit from each investment\n+  \/\/         in registering an object.\n+\n+public:\n+\n+  \/\/ The starting locations of objects contained within old-gen memory\n+  \/\/ are registered as part of the remembered set implementation.  This\n+  \/\/ information is required when scanning dirty card regions that are\n+  \/\/ spanned by objects beginning within preceding card regions.  It\n+  \/\/ is necessary to find the first and last objects that begin within\n+  \/\/ this card region.  Starting addresses of objects are required to\n+  \/\/ find the object headers, and object headers provide information\n+  \/\/ about which fields within the object hold addresses.\n+  \/\/\n+  \/\/ The old-gen memory allocator invokes register_object() for any\n+  \/\/ object that is allocated within old-gen memory.  This identifies\n+  \/\/ the starting addresses of objects that span boundaries between\n+  \/\/ card regions.\n+  \/\/\n+  \/\/ It is not necessary to invoke register_object at the very instant\n+  \/\/ an object is allocated.  It is only necessary to invoke it\n+  \/\/ prior to the next start of a garbage collection concurrent mark\n+  \/\/ or concurrent update-references phase.  An \"ideal\" time to register\n+  \/\/ objects is during post-processing of a GCLAB after the GCLAB is\n+  \/\/ retired due to depletion of its memory.\n+  \/\/\n+  \/\/ register_object() does not perform synchronization.  In the case\n+  \/\/ that multiple threads are registering objects whose starting\n+  \/\/ addresses are within the same cluster, races between these\n+  \/\/ threads may result in corruption of the object-start data\n+  \/\/ structures.  Parallel GC threads should avoid registering objects\n+  \/\/ residing within the same cluster by adhering to the following\n+  \/\/ coordination protocols:\n+  \/\/\n+  \/\/  1. Align thread-local GCLAB buffers with some TBD multiple of\n+  \/\/     card clusters.  The card cluster size is 32 KB.  If the\n+  \/\/     desired GCLAB size is 128 KB, align the buffer on a multiple\n+  \/\/     of 4 card clusters.\n+  \/\/  2. Post-process the contents of GCLAB buffers to register the\n+  \/\/     objects allocated therein.  Allow one GC thread at a\n+  \/\/     time to do the post-processing of each GCLAB.\n+  \/\/  3. Since only one GC thread at a time is registering objects\n+  \/\/     belonging to a particular allocation buffer, no locking\n+  \/\/     is performed when registering these objects.\n+  \/\/  4. Any remnant of unallocated memory within an expended GC\n+  \/\/     allocation buffer is not returned to the old-gen allocation\n+  \/\/     pool until after the GC allocation buffer has been post\n+  \/\/     processed.  Before any remnant memory is returned to the\n+  \/\/     old-gen allocation pool, the GC thread that scanned this GC\n+  \/\/     allocation buffer performs a write-commit memory barrier.\n+  \/\/  5. Background GC threads that perform tenuring of young-gen\n+  \/\/     objects without a GCLAB use a CAS lock before registering\n+  \/\/     each tenured object.  The CAS lock assures both mutual\n+  \/\/     exclusion and memory coherency\/visibility.  Note that an\n+  \/\/     object tenured by a background GC thread will not overlap\n+  \/\/     with any of the clusters that are receiving tenured objects\n+  \/\/     by way of GCLAB buffers.  Multiple independent GC threads may\n+  \/\/     attempt to tenure objects into a shared cluster.  This is why\n+  \/\/     sychronization may be necessary.  Consider the following\n+  \/\/     scenarios:\n+  \/\/\n+  \/\/     a) If two objects are tenured into the same card region, each\n+  \/\/        registration may attempt to modify the first-start or\n+  \/\/        last-start information associated with that card region.\n+  \/\/        Furthermore, because the representations of first-start\n+  \/\/        and last-start information within the object_starts array\n+  \/\/        entry uses different bits of a shared uint_16 to represent\n+  \/\/        each, it is necessary to lock the entire card entry\n+  \/\/        before modifying either the first-start or last-start\n+  \/\/        information within the entry.\n+  \/\/     b) Suppose GC thread X promotes a tenured object into\n+  \/\/        card region A and this tenured object spans into\n+  \/\/        neighboring card region B.  Suppose GC thread Y (not equal\n+  \/\/        to X) promotes a tenured object into cluster B.  GC thread X\n+  \/\/        will update the object_starts information for card A.  No\n+  \/\/        synchronization is required.\n+  \/\/     c) In summary, when background GC threads register objects\n+  \/\/        newly tenured into old-gen memory, they must acquire a\n+  \/\/        mutual exclusion lock on the card that holds the starting\n+  \/\/        address of the newly tenured object.  This can be achieved\n+  \/\/        by using a CAS instruction to assure that the previous\n+  \/\/        values of first-offset and last-offset have not been\n+  \/\/        changed since the same thread inquired as to their most\n+  \/\/        current values.\n+  \/\/\n+  \/\/     One way to minimize the need for synchronization between\n+  \/\/     background tenuring GC threads is for each tenuring GC thread\n+  \/\/     to promote young-gen objects into distinct dedicated cluster\n+  \/\/     ranges.\n+  \/\/  6. The object_starts information is only required during the\n+  \/\/     starting of concurrent marking and concurrent evacuation\n+  \/\/     phases of GC.  Before we start either of these GC phases, the\n+  \/\/     JVM enters a safe point and all GC threads perform\n+  \/\/     commit-write barriers to assure that access to the\n+  \/\/     object_starts information is coherent.\n+\n+\n+  \/\/ Notes on synchronization of register_object():\n+  \/\/\n+  \/\/  1. For efficiency, there is no locking in the implementation of register_object()\n+  \/\/  2. Thus, it is required that users of this service assure that concurrent\/parallel invocations of\n+  \/\/     register_object() do pertain to the same card's memory range.  See discussion below to understand\n+  \/\/     the risks.\n+  \/\/  3. When allocating from a TLAB or GCLAB, the mutual exclusion can be guaranteed by assuring that each\n+  \/\/     LAB's start and end are aligned on card memory boundaries.\n+  \/\/  4. Use the same lock that guarantees exclusivity when performing free-list allocation within heap regions.\n+  \/\/\n+  \/\/ Register the newly allocated object while we're holding the global lock since there's no synchronization\n+  \/\/ built in to the implementation of register_object().  There are potential races when multiple independent\n+  \/\/ threads are allocating objects, some of which might span the same card region.  For example, consider\n+  \/\/ a card table's memory region within which three objects are being allocated by three different threads:\n+  \/\/\n+  \/\/ objects being \"concurrently\" allocated:\n+  \/\/    [-----a------][-----b-----][--------------c------------------]\n+  \/\/            [---- card table memory range --------------]\n+  \/\/\n+  \/\/ Before any objects are allocated, this card's memory range holds no objects.  Note that:\n+  \/\/   allocation of object a wants to set the has-object, first-start, and last-start attributes of the preceding card region.\n+  \/\/   allocation of object b wants to set the has-object, first-start, and last-start attributes of this card region.\n+  \/\/   allocation of object c also wants to set the has-object, first-start, and last-start attributes of this card region.\n+  \/\/\n+  \/\/ The thread allocating b and the thread allocating c can \"race\" in various ways, resulting in confusion, such as last-start\n+  \/\/ representing object b while first-start represents object c.  This is why we need to require all register_object()\n+  \/\/ invocations associated with objects that are allocated from \"free lists\" to provide their own mutual exclusion locking\n+  \/\/ mechanism.\n+\n+  \/\/ Reset the starts_object() information to false for all cards in the range between from and to.\n+  void reset_object_range(HeapWord* from, HeapWord* to);\n+\n+  \/\/ register_object() requires that the caller hold the heap lock\n+  \/\/ before calling it.\n+  void register_object(HeapWord* address);\n+\n+  \/\/ register_object_without_lock() does not require that the caller hold\n+  \/\/ the heap lock before calling it, under the assumption that the\n+  \/\/ caller has assured no other thread will endeavor to concurrently\n+  \/\/ register objects that start within the same card's memory region\n+  \/\/ as address.\n+  void register_object_without_lock(HeapWord* address);\n+\n+  \/\/ During the reference updates phase of GC, we walk through each old-gen memory region that was\n+  \/\/ not part of the collection set and we invalidate all unmarked objects.  As part of this effort,\n+  \/\/ we coalesce neighboring dead objects in order to make future remembered set scanning more\n+  \/\/ efficient (since future remembered set scanning of any card region containing consecutive\n+  \/\/ dead objects can skip over all of them at once by reading only a single dead object header\n+  \/\/ instead of having to read the header of each of the coalesced dead objects.\n+  \/\/\n+  \/\/ At some future time, we may implement a further optimization: satisfy future allocation requests\n+  \/\/ by carving new objects out of the range of memory that represents the coalesced dead objects.\n+  \/\/\n+  \/\/ Suppose we want to combine several dead objects into a single coalesced object.  How does this\n+  \/\/ impact our representation of crossing map information?\n+  \/\/  1. If the newly coalesced range is contained entirely within a card range, that card's last\n+  \/\/     start entry either remains the same or it is changed to the start of the coalesced region.\n+  \/\/  2. For the card that holds the start of the coalesced object, it will not impact the first start\n+  \/\/     but it may impact the last start.\n+  \/\/  3. For following cards spanned entirely by the newly coalesced object, it will change starts_object\n+  \/\/     to false (and make first-start and last-start \"undefined\").\n+  \/\/  4. For a following card that is spanned patially by the newly coalesced object, it may change\n+  \/\/     first-start value, but it will not change the last-start value.\n+  \/\/\n+  \/\/ The range of addresses represented by the arguments to coalesce_objects() must represent a range\n+  \/\/ of memory that was previously occupied exactly by one or more previously registered objects.  For\n+  \/\/ convenience, it is legal to invoke coalesce_objects() with arguments that span a single previously\n+  \/\/ registered object.\n+  \/\/\n+  \/\/ The role of coalesce_objects is to change the crossing map information associated with all of the coalesced\n+  \/\/ objects.\n+  void coalesce_objects(HeapWord* address, size_t length_in_words);\n+\n+  \/\/ The typical use case is going to look something like this:\n+  \/\/   for each heapregion that comprises old-gen memory\n+  \/\/     for each card number that corresponds to this heap region\n+  \/\/       scan the objects contained therein if the card is dirty\n+  \/\/ To avoid excessive lookups in a sparse array, the API queries\n+  \/\/ the card number pertaining to a particular address and then uses the\n+  \/\/ card number for subsequent information lookups and stores.\n+\n+  \/\/ If starts_object(card_index), this returns the word offset within this card\n+  \/\/ memory at which the first object begins.  If !starts_object(card_index), the\n+  \/\/ result is a don't care value -- asserts in a debug build.\n+  size_t get_first_start(size_t card_index) const;\n+\n+  \/\/ If starts_object(card_index), this returns the word offset within this card\n+  \/\/ memory at which the last object begins.  If !starts_object(card_index), the\n+  \/\/ result is a don't care value.\n+  size_t get_last_start(size_t card_index) const;\n+\n+\n+  \/\/ Given a card_index, return the starting address of the first block in the heap\n+  \/\/ that straddles into the card. If the card is co-initial with an object, then\n+  \/\/ this would return the starting address of the heap that this card covers.\n+  \/\/ Expects to be called for a card affiliated with the old generation in\n+  \/\/ generational mode.\n+  HeapWord* block_start(size_t card_index) const;\n+};\n+\n+\/\/ ShenandoahScanRemembered is a concrete class representing the\n+\/\/ ability to scan the old-gen remembered set for references to\n+\/\/ objects residing in young-gen memory.\n+\/\/\n+\/\/ Scanning normally begins with an invocation of numRegions and ends\n+\/\/ after all clusters of all regions have been scanned.\n+\/\/\n+\/\/ Throughout the scanning effort, the number of regions does not\n+\/\/ change.\n+\/\/\n+\/\/ Even though the regions that comprise old-gen memory are not\n+\/\/ necessarily contiguous, the abstraction represented by this class\n+\/\/ identifies each of the old-gen regions with an integer value\n+\/\/ in the range from 0 to (numRegions() - 1) inclusive.\n+\/\/\n+\n+class ShenandoahScanRemembered: public CHeapObj<mtGC> {\n+\n+private:\n+  ShenandoahDirectCardMarkRememberedSet* _rs;\n+  ShenandoahCardCluster* _scc;\n+\n+  \/\/ Global card stats (cumulative)\n+  HdrSeq _card_stats_scan_rs[MAX_CARD_STAT_TYPE];\n+  HdrSeq _card_stats_update_refs[MAX_CARD_STAT_TYPE];\n+  \/\/ Per worker card stats (multiplexed by phase)\n+  HdrSeq** _card_stats;\n+\n+  \/\/ The types of card metrics that we gather\n+  const char* _card_stats_name[MAX_CARD_STAT_TYPE] = {\n+   \"dirty_run\", \"clean_run\",\n+   \"dirty_cards\", \"clean_cards\",\n+   \"max_dirty_run\", \"max_clean_run\",\n+   \"dirty_scan_objs\",\n+   \"alternations\"\n+  };\n+\n+  \/\/ The statistics are collected and logged separately for\n+  \/\/ card-scans for initial marking, and for updating refs.\n+  const char* _card_stat_log_type[MAX_CARD_STAT_LOG_TYPE] = {\n+   \"Scan Remembered Set\", \"Update Refs\"\n+  };\n+\n+  int _card_stats_log_counter[2] = {0, 0};\n+\n+public:\n+  ShenandoahScanRemembered(ShenandoahDirectCardMarkRememberedSet* rs) {\n+    _rs = rs;\n+    _scc = new ShenandoahCardCluster(rs);\n+\n+    \/\/ We allocate ParallelGCThreads worth even though we usually only\n+    \/\/ use up to ConcGCThreads, because degenerate collections may employ\n+    \/\/ ParallelGCThreads for remembered set scanning.\n+    if (ShenandoahEnableCardStats) {\n+      _card_stats = NEW_C_HEAP_ARRAY(HdrSeq*, ParallelGCThreads, mtGC);\n+      for (uint i = 0; i < ParallelGCThreads; i++) {\n+        _card_stats[i] = new HdrSeq[MAX_CARD_STAT_TYPE];\n+      }\n+    } else {\n+      _card_stats = nullptr;\n+    }\n+  }\n+\n+  ~ShenandoahScanRemembered() {\n+    delete _scc;\n+    if (ShenandoahEnableCardStats) {\n+      for (uint i = 0; i < ParallelGCThreads; i++) {\n+        delete _card_stats[i];\n+      }\n+      FREE_C_HEAP_ARRAY(HdrSeq*, _card_stats);\n+      _card_stats = nullptr;\n+    }\n+    assert(_card_stats == nullptr, \"Error\");\n+  }\n+\n+  HdrSeq* card_stats(uint worker_id) {\n+    assert(worker_id < ParallelGCThreads, \"Error\");\n+    assert(ShenandoahEnableCardStats == (_card_stats != nullptr), \"Error\");\n+    return ShenandoahEnableCardStats ? _card_stats[worker_id] : nullptr;\n+  }\n+\n+  HdrSeq* card_stats_for_phase(CardStatLogType t) {\n+    switch (t) {\n+      case CARD_STAT_SCAN_RS:\n+        return _card_stats_scan_rs;\n+      case CARD_STAT_UPDATE_REFS:\n+        return _card_stats_update_refs;\n+      default:\n+        guarantee(false, \"No such CardStatLogType\");\n+    }\n+    return nullptr; \/\/ Quiet compiler\n+  }\n+\n+  \/\/ Card index is zero-based relative to first spanned card region.\n+  size_t card_index_for_addr(HeapWord* p);\n+  HeapWord* addr_for_card_index(size_t card_index);\n+  bool is_card_dirty(size_t card_index);\n+  bool is_write_card_dirty(size_t card_index);\n+  bool is_card_dirty(HeapWord* p);\n+  bool is_write_card_dirty(HeapWord* p);\n+  void mark_card_as_dirty(HeapWord* p);\n+  void mark_range_as_dirty(HeapWord* p, size_t num_heap_words);\n+  void mark_range_as_clean(HeapWord* p, size_t num_heap_words);\n+\n+  \/\/ This method is used to concurrently clean the \"read\" card table -\n+  \/\/ currently, as part of the reset phase. Later on the pointers to the \"read\"\n+  \/\/ and \"write\" card tables are swapped everywhere to enable the GC to\n+  \/\/ concurrently operate on the \"read\" table while mutators effect changes on\n+  \/\/ the \"write\" table.\n+  void mark_read_table_as_clean();\n+\n+  \/\/ Swaps read and write card tables pointers in effect setting a clean card\n+  \/\/ table for the next GC cycle.\n+  void swap_card_tables() { _rs->swap_card_tables(); }\n+\n+  void merge_write_table(HeapWord* start, size_t word_count) { _rs->merge_write_table(start, word_count); }\n+\n+  size_t cluster_for_addr(HeapWord* addr);\n+  HeapWord* addr_for_cluster(size_t cluster_no);\n+\n+  void reset_object_range(HeapWord* from, HeapWord* to);\n+  void register_object(HeapWord* addr);\n+  void register_object_without_lock(HeapWord* addr);\n+  void coalesce_objects(HeapWord* addr, size_t length_in_words);\n+\n+  HeapWord* first_object_in_card(size_t card_index) {\n+    if (_scc->starts_object(card_index)) {\n+      return addr_for_card_index(card_index) + _scc->get_first_start(card_index);\n+    } else {\n+      return nullptr;\n+    }\n+  }\n+\n+  \/\/ Return true iff this object is \"properly\" registered.\n+  bool verify_registration(HeapWord* address, ShenandoahMarkingContext* ctx);\n+\n+  \/\/ clear the cards to clean, and clear the object_starts info to no objects\n+  void mark_range_as_empty(HeapWord* addr, size_t length_in_words);\n+\n+  \/\/ process_clusters() scans a portion of the remembered set\n+  \/\/ for references from old gen into young. Several worker threads\n+  \/\/ scan different portions of the remembered set by making parallel invocations\n+  \/\/ of process_clusters() with each invocation scanning different\n+  \/\/ \"clusters\" of the remembered set.\n+  \/\/\n+  \/\/ An invocation of process_clusters() examines all of the\n+  \/\/ intergenerational references spanned by `count` clusters starting\n+  \/\/ with `first_cluster`.  The `oops` argument is a worker-thread-local\n+  \/\/ OopClosure that is applied to all \"valid\" references in the remembered set.\n+  \/\/\n+  \/\/ A side-effect of executing process_clusters() is to update the remembered\n+  \/\/ set entries (e.g. marking dirty cards clean if they no longer\n+  \/\/ hold references to young-gen memory).\n+  \/\/\n+  \/\/ An implementation of process_clusters() may choose to efficiently\n+  \/\/ address more typical scenarios in the structure of remembered sets. E.g.\n+  \/\/ in the generational setting, one might expect remembered sets to be very sparse\n+  \/\/ (low mutation rates in the old generation leading to sparse dirty cards,\n+  \/\/ each with very few intergenerational pointers). Specific implementations\n+  \/\/ may choose to degrade gracefully as the sparsity assumption fails to hold,\n+  \/\/ such as when there are sudden spikes in (premature) promotion or in the\n+  \/\/ case of an underprovisioned, poorly-tuned, or poorly-shaped heap.\n+  \/\/\n+  \/\/ At the start of a concurrent young generation marking cycle, we invoke process_clusters\n+  \/\/ with ClosureType ShenandoahInitMarkRootsClosure.\n+  \/\/\n+  \/\/ At the start of a concurrent evacuation phase, we invoke process_clusters with\n+  \/\/ ClosureType ShenandoahEvacuateUpdateRootsClosure.\n+\n+  template <typename ClosureType>\n+  void process_clusters(size_t first_cluster, size_t count, HeapWord* end_of_range, ClosureType* oops,\n+                        bool use_write_table, uint worker_id);\n+\n+  template <typename ClosureType>\n+  void process_humongous_clusters(ShenandoahHeapRegion* r, size_t first_cluster, size_t count,\n+                                  HeapWord* end_of_range, ClosureType* oops, bool use_write_table);\n+\n+  template <typename ClosureType>\n+  void process_region_slice(ShenandoahHeapRegion* region, size_t offset, size_t clusters, HeapWord* end_of_range,\n+                            ClosureType* cl, bool use_write_table, uint worker_id);\n+\n+  \/\/ To Do:\n+  \/\/  Create subclasses of ShenandoahInitMarkRootsClosure and\n+  \/\/  ShenandoahEvacuateUpdateRootsClosure and any other closures\n+  \/\/  that need to participate in remembered set scanning.  Within the\n+  \/\/  subclasses, add a (probably templated) instance variable that\n+  \/\/  refers to the associated ShenandoahCardCluster object.  Use this\n+  \/\/  ShenandoahCardCluster instance to \"enhance\" the do_oops\n+  \/\/  processing so that we can:\n+  \/\/\n+  \/\/   1. Avoid processing references that correspond to clean card\n+  \/\/      regions, and\n+  \/\/   2. Set card status to CLEAN when the associated card region no\n+  \/\/      longer holds inter-generatioanal references.\n+  \/\/\n+  \/\/  To enable efficient implementation of these behaviors, we\n+  \/\/  probably also want to add a few fields into the\n+  \/\/  ShenandoahCardCluster object that allow us to precompute and\n+  \/\/  remember the addresses at which card status is going to change\n+  \/\/  from dirty to clean and clean to dirty.  The do_oops\n+  \/\/  implementations will want to update this value each time they\n+  \/\/  cross one of these boundaries.\n+  void roots_do(OopIterateClosure* cl);\n+\n+  \/\/ Log stats related to card\/RS stats for given phase t\n+  void log_card_stats(uint nworkers, CardStatLogType t) PRODUCT_RETURN;\n+private:\n+  \/\/ Log stats for given worker id related into given summary card\/RS stats\n+  void log_worker_card_stats(uint worker_id, HdrSeq* sum_stats) PRODUCT_RETURN;\n+\n+  \/\/ Log given stats\n+  void log_card_stats(HdrSeq* stats) PRODUCT_RETURN;\n+\n+  \/\/ Merge the stats from worked_id into the given summary stats, and clear the worker_id's stats.\n+  void merge_worker_card_stats_cumulative(HdrSeq* worker_stats, HdrSeq* sum_stats) PRODUCT_RETURN;\n+};\n+\n+\n+\/\/ A ShenandoahRegionChunk represents a contiguous interval of a ShenandoahHeapRegion, typically representing\n+\/\/ work to be done by a worker thread.\n+struct ShenandoahRegionChunk {\n+  ShenandoahHeapRegion* _r;      \/\/ The region of which this represents a chunk\n+  size_t _chunk_offset;          \/\/ HeapWordSize offset\n+  size_t _chunk_size;            \/\/ HeapWordSize qty\n+};\n+\n+\/\/ ShenandoahRegionChunkIterator divides the total remembered set scanning effort into ShenandoahRegionChunks\n+\/\/ that are assigned one at a time to worker threads. (Here, we use the terms `assignments` and `chunks`\n+\/\/ interchangeably.) Note that the effort required to scan a range of memory is not necessarily a linear\n+\/\/ function of the size of the range.  Some memory ranges hold only a small number of live objects.\n+\/\/ Some ranges hold primarily primitive (non-pointer) data.  We start with larger chunk sizes because larger chunks\n+\/\/ reduce coordination overhead.  We expect that the GC worker threads that receive more difficult assignments\n+\/\/ will work longer on those chunks.  Meanwhile, other worker threads will repeatedly accept and complete multiple\n+\/\/ easier chunks.  As the total amount of work remaining to be completed decreases, we decrease the size of chunks\n+\/\/ given to individual threads.  This reduces the likelihood of significant imbalance between worker thread assignments\n+\/\/ when there is less meaningful work to be performed by the remaining worker threads while they wait for\n+\/\/ worker threads with difficult assignments to finish, reducing the overall duration of the phase.\n+\n+class ShenandoahRegionChunkIterator : public StackObj {\n+private:\n+  \/\/ The largest chunk size is 4 MiB, measured in words.  Otherwise, remembered set scanning may become too unbalanced.\n+  \/\/ If the largest chunk size is too small, there is too much overhead sifting out assignments to individual worker threads.\n+  static const size_t _maximum_chunk_size_words = (4 * 1024 * 1024) \/ HeapWordSize;\n+  static const size_t _clusters_in_smallest_chunk = 4;\n+\n+  size_t _largest_chunk_size_words;\n+\n+  \/\/ smallest_chunk_size is 4 clusters.  Each cluster spans 128 KiB.\n+  \/\/ This is computed from CardTable::card_size_in_words() * ShenandoahCardCluster::CardsPerCluster;\n+  static size_t smallest_chunk_size_words() {\n+      return _clusters_in_smallest_chunk * CardTable::card_size_in_words() * ShenandoahCardCluster::CardsPerCluster;\n+  }\n+\n+  \/\/ The total remembered set scanning effort is divided into chunks of work that are assigned to individual worker tasks.\n+  \/\/ The chunks of assigned work are divided into groups, where the size of the typical group (_regular_group_size) is half the\n+  \/\/ total number of regions.  The first group may be larger than\n+  \/\/ _regular_group_size in the case that the first group's chunk\n+  \/\/ size is less than the region size.  The last group may be larger\n+  \/\/ than _regular_group_size because no group is allowed to\n+  \/\/ have smaller assignments than _smallest_chunk_size, which is 128 KB.\n+\n+  \/\/ Under normal circumstances, no configuration needs more than _maximum_groups (default value of 16).\n+  \/\/ The first group \"effectively\" processes chunks of size 1 MiB (or smaller for smaller region sizes).\n+  \/\/ The last group processes chunks of size 128 KiB.  There are four groups total.\n+\n+  \/\/ group[ 0] is 4 MiB chunk size (_maximum_chunk_size_words)\n+  \/\/ group[ 1] is 2 MiB chunk size\n+  \/\/ group[ 2] is 1 MiB chunk size\n+  \/\/ group[ 3] is 512 KiB chunk size\n+  \/\/ group[ 4] is 256 KiB chunk size\n+  \/\/ group[ 5] is 128 KiB chunk size\n+  \/\/ group[ 6] is  64 KiB chunk size\n+  \/\/ group[ 7] is  32 KiB chunk size\n+  \/\/ group[ 8] is  16 KiB chunk size\n+  \/\/ group[ 9] is   8 KiB chunk size\n+  \/\/ group[10] is   4 KiB chunk size\n+  \/\/   Note: 4 KiB is smallest possible chunk_size, computed from:\n+  \/\/         _clusters_in_smallest_chunk * MinimumCardSizeInWords * ShenandoahCardCluster::CardsPerCluster, which is\n+  \/\/         4 * 16 * 64 = 4096\n+\n+  \/\/ We set aside arrays to represent the maximum number of groups that may be required for any heap configuration\n+  static const size_t _maximum_groups = 11;\n+\n+  const ShenandoahHeap* _heap;\n+\n+  const size_t _regular_group_size;                        \/\/ Number of chunks in each group\n+  const size_t _first_group_chunk_size_b4_rebalance;\n+  const size_t _num_groups;                        \/\/ Number of groups in this configuration\n+  size_t _adjusted_num_groups;                     \/\/ Rebalancing may coalesce groups\n+  const size_t _total_chunks;\n+\n+  shenandoah_padding(0);\n+  volatile size_t _index;\n+  shenandoah_padding(1);\n+\n+  size_t _region_index[_maximum_groups];           \/\/ The region index for the first region spanned by this group\n+  size_t _group_offset[_maximum_groups];           \/\/ The offset at which group begins within first region spanned by this group\n+  size_t _group_chunk_size[_maximum_groups];       \/\/ The size of each chunk within this group\n+  size_t _group_entries[_maximum_groups];          \/\/ Total chunks spanned by this group and the ones before it.\n+\n+  \/\/ No implicit copying: iterators should be passed by reference to capture the state\n+  NONCOPYABLE(ShenandoahRegionChunkIterator);\n+\n+  \/\/ Makes use of _heap.\n+  size_t calc_regular_group_size();\n+\n+  \/\/ Makes use of _regular_group_size, which must be initialized before call.\n+  size_t calc_first_group_chunk_size_b4_rebalance();\n+\n+  \/\/ Makes use of _regular_group_size and _first_group_chunk_size_b4_rebalance, both of which must be initialized before call.\n+  size_t calc_num_groups();\n+\n+  \/\/ Makes use of _regular_group_size, _first_group_chunk_size_b4_rebalance, which must be initialized before call.\n+  size_t calc_total_chunks();\n+\n+public:\n+  ShenandoahRegionChunkIterator(size_t worker_count);\n+  ShenandoahRegionChunkIterator(ShenandoahHeap* heap, size_t worker_count);\n+\n+  \/\/ Reset iterator to default state\n+  void reset();\n+\n+  \/\/ Fills in assignment with next chunk of work and returns true iff there is more work.\n+  \/\/ Otherwise, returns false.  This is multi-thread-safe.\n+  inline bool next(struct ShenandoahRegionChunk* assignment);\n+\n+  \/\/ This is *not* MT safe. However, in the absence of multithreaded access, it\n+  \/\/ can be used to determine if there is more work to do.\n+  inline bool has_next() const;\n+};\n+\n+\n+class ShenandoahScanRememberedTask : public WorkerTask {\n+ private:\n+  ShenandoahObjToScanQueueSet* _queue_set;\n+  ShenandoahObjToScanQueueSet* _old_queue_set;\n+  ShenandoahReferenceProcessor* _rp;\n+  ShenandoahRegionChunkIterator* _work_list;\n+  bool _is_concurrent;\n+\n+ public:\n+  ShenandoahScanRememberedTask(ShenandoahObjToScanQueueSet* queue_set,\n+                               ShenandoahObjToScanQueueSet* old_queue_set,\n+                               ShenandoahReferenceProcessor* rp,\n+                               ShenandoahRegionChunkIterator* work_list,\n+                               bool is_concurrent);\n+\n+  void work(uint worker_id);\n+  void do_work(uint worker_id);\n+};\n+\n+\/\/ After Full GC is done, reconstruct the remembered set by iterating over OLD regions,\n+\/\/ registering all objects between bottom() and top(), and dirtying the cards containing\n+\/\/ cross-generational pointers.\n+class ShenandoahReconstructRememberedSetTask : public WorkerTask {\n+private:\n+  ShenandoahRegionIterator* _regions;\n+\n+public:\n+  explicit ShenandoahReconstructRememberedSetTask(ShenandoahRegionIterator* regions);\n+\n+  void work(uint worker_id) override;\n+};\n+\n+#endif \/\/ SHARE_GC_SHENANDOAH_SHENANDOAHSCANREMEMBERED_HPP\n","filename":"src\/hotspot\/share\/gc\/shenandoah\/shenandoahScanRemembered.hpp","additions":1025,"deletions":0,"binary":false,"changes":1025,"status":"added"},{"patch":"@@ -0,0 +1,405 @@\n+\/*\n+ * Copyright Amazon.com Inc. or its affiliates. All Rights Reserved.\n+ * DO NOT ALTER OR REMOVE COPYRIGHT NOTICES OR THIS FILE HEADER.\n+ *\n+ * This code is free software; you can redistribute it and\/or modify it\n+ * under the terms of the GNU General Public License version 2 only, as\n+ * published by the Free Software Foundation.\n+ *\n+ * This code is distributed in the hope that it will be useful, but WITHOUT\n+ * ANY WARRANTY; without even the implied warranty of MERCHANTABILITY or\n+ * FITNESS FOR A PARTICULAR PURPOSE.  See the GNU General Public License\n+ * version 2 for more details (a copy is included in the LICENSE file that\n+ * accompanied this code).\n+ *\n+ * You should have received a copy of the GNU General Public License version\n+ * 2 along with this work; if not, write to the Free Software Foundation,\n+ * Inc., 51 Franklin St, Fifth Floor, Boston, MA 02110-1301 USA.\n+ *\n+ * Please contact Oracle, 500 Oracle Parkway, Redwood Shores, CA 94065 USA\n+ * or visit www.oracle.com if you need additional information or have any\n+ * questions.\n+ *\n+ *\/\n+\n+#ifndef SHARE_GC_SHENANDOAH_SHENANDOAHSCANREMEMBEREDINLINE_HPP\n+#define SHARE_GC_SHENANDOAH_SHENANDOAHSCANREMEMBEREDINLINE_HPP\n+\n+#include \"memory\/iterator.hpp\"\n+#include \"oops\/oop.hpp\"\n+#include \"oops\/objArrayOop.hpp\"\n+#include \"gc\/shared\/collectorCounters.hpp\"\n+#include \"gc\/shenandoah\/shenandoahCardStats.hpp\"\n+#include \"gc\/shenandoah\/shenandoahCardTable.hpp\"\n+#include \"gc\/shenandoah\/shenandoahHeap.hpp\"\n+#include \"gc\/shenandoah\/shenandoahHeapRegion.hpp\"\n+#include \"gc\/shenandoah\/shenandoahOldGeneration.hpp\"\n+#include \"gc\/shenandoah\/shenandoahScanRemembered.hpp\"\n+#include \"gc\/shenandoah\/mode\/shenandoahMode.hpp\"\n+#include \"logging\/log.hpp\"\n+\n+\/\/ Process all objects starting within count clusters beginning with first_cluster and for which the start address is\n+\/\/ less than end_of_range.  For any non-array object whose header lies on a dirty card, scan the entire object,\n+\/\/ even if its end reaches beyond end_of_range. Object arrays, on the other hand, are precisely dirtied and\n+\/\/ only the portions of the array on dirty cards need to be scanned.\n+\/\/\n+\/\/ Do not CANCEL within process_clusters.  It is assumed that if a worker thread accepts responsibility for processing\n+\/\/ a chunk of work, it will finish the work it starts.  Otherwise, the chunk of work will be lost in the transition to\n+\/\/ degenerated execution, leading to dangling references.\n+template <typename ClosureType>\n+void ShenandoahScanRemembered::process_clusters(size_t first_cluster, size_t count, HeapWord* end_of_range,\n+                                                               ClosureType* cl, bool use_write_table, uint worker_id) {\n+\n+  assert(ShenandoahHeap::heap()->old_generation()->is_parsable(), \"Old generation regions must be parsable for remembered set scan\");\n+  \/\/ If old-gen evacuation is active, then MarkingContext for old-gen heap regions is valid.  We use the MarkingContext\n+  \/\/ bits to determine which objects within a DIRTY card need to be scanned.  This is necessary because old-gen heap\n+  \/\/ regions that are in the candidate collection set have not been coalesced and filled.  Thus, these heap regions\n+  \/\/ may contain zombie objects.  Zombie objects are known to be dead, but have not yet been \"collected\".  Scanning\n+  \/\/ zombie objects is unsafe because the Klass pointer is not reliable, objects referenced from a zombie may have been\n+  \/\/ collected (if dead), or relocated (if live), or if dead but not yet collected, we don't want to \"revive\" them\n+  \/\/ by marking them (when marking) or evacuating them (when updating references).\n+\n+  \/\/ start and end addresses of range of objects to be scanned, clipped to end_of_range\n+  const size_t start_card_index = first_cluster * ShenandoahCardCluster::CardsPerCluster;\n+  const HeapWord* start_addr = _rs->addr_for_card_index(start_card_index);\n+  \/\/ clip at end_of_range (exclusive)\n+  HeapWord* end_addr = MIN2(end_of_range, (HeapWord*)start_addr + (count * ShenandoahCardCluster::CardsPerCluster\n+                                                                   * CardTable::card_size_in_words()));\n+  assert(start_addr < end_addr, \"Empty region?\");\n+\n+  const size_t whole_cards = (end_addr - start_addr + CardTable::card_size_in_words() - 1)\/CardTable::card_size_in_words();\n+  const size_t end_card_index = start_card_index + whole_cards - 1;\n+  log_debug(gc, remset)(\"Worker %u: cluster = \" SIZE_FORMAT \" count = \" SIZE_FORMAT \" eor = \" INTPTR_FORMAT\n+                        \" start_addr = \" INTPTR_FORMAT \" end_addr = \" INTPTR_FORMAT \" cards = \" SIZE_FORMAT,\n+                        worker_id, first_cluster, count, p2i(end_of_range), p2i(start_addr), p2i(end_addr), whole_cards);\n+\n+  \/\/ use_write_table states whether we are using the card table that is being\n+  \/\/ marked by the mutators. If false, we are using a snapshot of the card table\n+  \/\/ that is not subject to modifications. Even when this arg is true, and\n+  \/\/ the card table is being actively marked, SATB marking ensures that we need not\n+  \/\/ worry about cards marked after the processing here has passed them.\n+  const CardValue* const ctbm = _rs->get_card_table_byte_map(use_write_table);\n+\n+  \/\/ If old gen evacuation is active, ctx will hold the completed marking of\n+  \/\/ old generation objects. We'll only scan objects that are marked live by\n+  \/\/ the old generation marking. These include objects allocated since the\n+  \/\/ start of old generation marking (being those above TAMS).\n+  const ShenandoahHeap* heap = ShenandoahHeap::heap();\n+  const ShenandoahMarkingContext* ctx = heap->old_generation()->is_mark_complete() ?\n+                                        heap->marking_context() : nullptr;\n+\n+  \/\/ The region we will scan is the half-open interval [start_addr, end_addr),\n+  \/\/ and lies entirely within a single region.\n+  const ShenandoahHeapRegion* region = ShenandoahHeap::heap()->heap_region_containing(start_addr);\n+  assert(region->contains(end_addr - 1), \"Slice shouldn't cross regions\");\n+\n+  \/\/ This code may have implicit assumptions of examining only old gen regions.\n+  assert(region->is_old(), \"We only expect to be processing old regions\");\n+  assert(!region->is_humongous(), \"Humongous regions can be processed more efficiently;\"\n+                                  \"see process_humongous_clusters()\");\n+  \/\/ tams and ctx below are for old generation marking. As such, young gen roots must\n+  \/\/ consider everything above tams, since it doesn't represent a TAMS for young gen's\n+  \/\/ SATB marking.\n+  const HeapWord* tams = (ctx == nullptr ? region->bottom() : ctx->top_at_mark_start(region));\n+\n+  NOT_PRODUCT(ShenandoahCardStats stats(whole_cards, card_stats(worker_id));)\n+\n+  \/\/ In the case of imprecise marking, we remember the lowest address\n+  \/\/ scanned in a range of dirty cards, as we work our way left from the\n+  \/\/ highest end_addr. This serves as another upper bound on the address we will\n+  \/\/ scan as we move left over each contiguous range of dirty cards.\n+  HeapWord* upper_bound = nullptr;\n+\n+  \/\/ Starting at the right end of the address range, walk backwards accumulating\n+  \/\/ a maximal dirty range of cards, then process those cards.\n+  ssize_t cur_index = (ssize_t) end_card_index;\n+  assert(cur_index >= 0, \"Overflow\");\n+  assert(((ssize_t)start_card_index) >= 0, \"Overflow\");\n+  while (cur_index >= (ssize_t)start_card_index) {\n+\n+    \/\/ We'll continue the search starting with the card for the upper bound\n+    \/\/ address identified by the last dirty range that we processed, if any,\n+    \/\/ skipping any cards at higher addresses.\n+    if (upper_bound != nullptr) {\n+      ssize_t right_index = _rs->card_index_for_addr(upper_bound);\n+      assert(right_index >= 0, \"Overflow\");\n+      cur_index = MIN2(cur_index, right_index);\n+      assert(upper_bound < end_addr, \"Program logic\");\n+      end_addr  = upper_bound;   \/\/ lower end_addr\n+      upper_bound = nullptr;     \/\/ and clear upper_bound\n+      if (end_addr <= start_addr) {\n+        assert(right_index <= (ssize_t)start_card_index, \"Program logic\");\n+        \/\/ We are done with our cluster\n+        return;\n+      }\n+    }\n+\n+    if (ctbm[cur_index] == CardTable::dirty_card_val()) {\n+      \/\/ ==== BEGIN DIRTY card range processing ====\n+\n+      const size_t dirty_r = cur_index;  \/\/ record right end of dirty range (inclusive)\n+      while (--cur_index >= (ssize_t)start_card_index && ctbm[cur_index] == CardTable::dirty_card_val()) {\n+        \/\/ walk back over contiguous dirty cards to find left end of dirty range (inclusive)\n+      }\n+      \/\/ [dirty_l, dirty_r] is a \"maximal\" closed interval range of dirty card indices:\n+      \/\/ it may not be maximal if we are using the write_table, because of concurrent\n+      \/\/ mutations dirtying the card-table. It may also not be maximal if an upper bound\n+      \/\/ was established by the scan of the previous chunk.\n+      const size_t dirty_l = cur_index + 1;   \/\/ record left end of dirty range (inclusive)\n+      \/\/ Check that we identified a boundary on our left\n+      assert(ctbm[dirty_l] == CardTable::dirty_card_val(), \"First card in range should be dirty\");\n+      assert(dirty_l == start_card_index || use_write_table\n+             || ctbm[dirty_l - 1] == CardTable::clean_card_val(),\n+             \"Interval isn't maximal on the left\");\n+      assert(dirty_r >= dirty_l, \"Error\");\n+      assert(ctbm[dirty_r] == CardTable::dirty_card_val(), \"Last card in range should be dirty\");\n+      \/\/ Record alternations, dirty run length, and dirty card count\n+      NOT_PRODUCT(stats.record_dirty_run(dirty_r - dirty_l + 1);)\n+\n+      \/\/ Find first object that starts this range:\n+      \/\/ [left, right) is a maximal right-open interval of dirty cards\n+      HeapWord* left = _rs->addr_for_card_index(dirty_l);        \/\/ inclusive\n+      HeapWord* right = _rs->addr_for_card_index(dirty_r + 1);   \/\/ exclusive\n+      \/\/ Clip right to end_addr established above (still exclusive)\n+      right = MIN2(right, end_addr);\n+      assert(right <= region->top() && end_addr <= region->top(), \"Busted bounds\");\n+      const MemRegion mr(left, right);\n+\n+      \/\/ NOTE: We'll not call block_start() repeatedly\n+      \/\/ on a very large object if its head card is dirty. If not,\n+      \/\/ (i.e. the head card is clean) we'll call it each time we\n+      \/\/ process a new dirty range on the object. This is always\n+      \/\/ the case for large object arrays, which are typically more\n+      \/\/ common.\n+      HeapWord* p = _scc->block_start(dirty_l);\n+      oop obj = cast_to_oop(p);\n+\n+      \/\/ PREFIX: The object that straddles into this range of dirty cards\n+      \/\/ from the left may be subject to special treatment unless\n+      \/\/ it is an object array.\n+      if (p < left && !obj->is_objArray()) {\n+        \/\/ The mutator (both compiler and interpreter, but not JNI?)\n+        \/\/ typically dirty imprecisely (i.e. only the head of an object),\n+        \/\/ but GC closures typically dirty the object precisely. (It would\n+        \/\/ be nice to have everything be precise for maximum efficiency.)\n+        \/\/\n+        \/\/ To handle this, we check the head card of the object here and,\n+        \/\/ if dirty, (arrange to) scan the object in its entirety. If we\n+        \/\/ find the head card clean, we'll scan only the portion of the\n+        \/\/ object lying in the dirty card range below, assuming this was\n+        \/\/ the result of precise marking by GC closures.\n+\n+        \/\/ index of the \"head card\" for p\n+        const size_t hc_index = _rs->card_index_for_addr(p);\n+        if (ctbm[hc_index] == CardTable::dirty_card_val()) {\n+          \/\/ Scan or skip the object, depending on location of its\n+          \/\/ head card, and remember that we'll have processed all\n+          \/\/ the objects back up to p, which is thus an upper bound\n+          \/\/ for the next iteration of a dirty card loop.\n+          upper_bound = p;   \/\/ remember upper bound for next chunk\n+          if (p < start_addr) {\n+            \/\/ if object starts in a previous slice, it'll be handled\n+            \/\/ in its entirety by the thread processing that slice; we can\n+            \/\/ skip over it and avoid an unnecessary extra scan.\n+            assert(obj == cast_to_oop(p), \"Inconsistency detected\");\n+            p += obj->size();\n+          } else {\n+            \/\/ the object starts in our slice, we scan it in its entirety\n+            assert(obj == cast_to_oop(p), \"Inconsistency detected\");\n+            if (ctx == nullptr || ctx->is_marked(obj)) {\n+              \/\/ Scan the object in its entirety\n+              p += obj->oop_iterate_size(cl);\n+            } else {\n+              assert(p < tams, \"Error 1 in ctx\/marking\/tams logic\");\n+              \/\/ Skip over any intermediate dead objects\n+              p = ctx->get_next_marked_addr(p, tams);\n+              assert(p <= tams, \"Error 2 in ctx\/marking\/tams logic\");\n+            }\n+          }\n+          assert(p > left, \"Should have processed into interior of dirty range\");\n+        }\n+      }\n+\n+      size_t i = 0;\n+      HeapWord* last_p = nullptr;\n+\n+      \/\/ BODY: Deal with (other) objects in this dirty card range\n+      while (p < right) {\n+        obj = cast_to_oop(p);\n+        \/\/ walk right scanning eligible objects\n+        if (ctx == nullptr || ctx->is_marked(obj)) {\n+          \/\/ we need to remember the last object ptr we scanned, in case we need to\n+          \/\/ complete a partial suffix scan after mr, see below\n+          last_p = p;\n+          \/\/ apply the closure to the oops in the portion of\n+          \/\/ the object within mr.\n+          p += obj->oop_iterate_size(cl, mr);\n+          NOT_PRODUCT(i++);\n+        } else {\n+          \/\/ forget the last object pointer we remembered\n+          last_p = nullptr;\n+          assert(p < tams, \"Tams and above are implicitly marked in ctx\");\n+          \/\/ object under tams isn't marked: skip to next live object\n+          p = ctx->get_next_marked_addr(p, tams);\n+          assert(p <= tams, \"Error 3 in ctx\/marking\/tams logic\");\n+        }\n+      }\n+\n+      \/\/ SUFFIX: Fix up a possible incomplete scan at right end of window\n+      \/\/ by scanning the portion of a non-objArray that wasn't done.\n+      if (p > right && last_p != nullptr) {\n+        assert(last_p < right, \"Error\");\n+        \/\/ check if last_p suffix needs scanning\n+        const oop last_obj = cast_to_oop(last_p);\n+        if (!last_obj->is_objArray()) {\n+          \/\/ scan the remaining suffix of the object\n+          const MemRegion last_mr(right, p);\n+          assert(p == last_p + last_obj->size(), \"Would miss portion of last_obj\");\n+          last_obj->oop_iterate(cl, last_mr);\n+          log_develop_debug(gc, remset)(\"Fixed up non-objArray suffix scan in [\" INTPTR_FORMAT \", \" INTPTR_FORMAT \")\",\n+                                        p2i(last_mr.start()), p2i(last_mr.end()));\n+        } else {\n+          log_develop_debug(gc, remset)(\"Skipped suffix scan of objArray in [\" INTPTR_FORMAT \", \" INTPTR_FORMAT \")\",\n+                                        p2i(right), p2i(p));\n+        }\n+      }\n+      NOT_PRODUCT(stats.record_scan_obj_cnt(i);)\n+\n+      \/\/ ==== END   DIRTY card range processing ====\n+    } else {\n+      \/\/ ==== BEGIN CLEAN card range processing ====\n+\n+      \/\/ If we are using the write table (during update refs, e.g.), a mutator may dirty\n+      \/\/ a card at any time. This is fine for the algorithm below because it is only\n+      \/\/ counting contiguous runs of clean cards (and only for non-product builds).\n+      assert(use_write_table || ctbm[cur_index] == CardTable::clean_card_val(), \"Error\");\n+\n+      \/\/ walk back over contiguous clean cards\n+      size_t i = 0;\n+      while (--cur_index >= (ssize_t)start_card_index && ctbm[cur_index] == CardTable::clean_card_val()) {\n+        NOT_PRODUCT(i++);\n+      }\n+      \/\/ Record alternations, clean run length, and clean card count\n+      NOT_PRODUCT(stats.record_clean_run(i);)\n+\n+      \/\/ ==== END CLEAN card range processing ====\n+    }\n+  }\n+}\n+\n+\/\/ Given that this range of clusters is known to span a humongous object spanned by region r, scan the\n+\/\/ portion of the humongous object that corresponds to the specified range.\n+template <typename ClosureType>\n+inline void\n+ShenandoahScanRemembered::process_humongous_clusters(ShenandoahHeapRegion* r, size_t first_cluster, size_t count,\n+                                                                    HeapWord *end_of_range, ClosureType *cl, bool use_write_table) {\n+  ShenandoahHeapRegion* start_region = r->humongous_start_region();\n+  HeapWord* p = start_region->bottom();\n+  oop obj = cast_to_oop(p);\n+  assert(r->is_humongous(), \"Only process humongous regions here\");\n+  assert(start_region->is_humongous_start(), \"Should be start of humongous region\");\n+  assert(p + obj->size() >= end_of_range, \"Humongous object ends before range ends\");\n+\n+  size_t first_card_index = first_cluster * ShenandoahCardCluster::CardsPerCluster;\n+  HeapWord* first_cluster_addr = _rs->addr_for_card_index(first_card_index);\n+  size_t spanned_words = count * ShenandoahCardCluster::CardsPerCluster * CardTable::card_size_in_words();\n+  start_region->oop_iterate_humongous_slice_dirty(cl, first_cluster_addr, spanned_words, use_write_table);\n+}\n+\n+\n+\/\/ This method takes a region & determines the end of the region that the worker can scan.\n+template <typename ClosureType>\n+inline void\n+ShenandoahScanRemembered::process_region_slice(ShenandoahHeapRegion *region, size_t start_offset, size_t clusters,\n+                                                              HeapWord *end_of_range, ClosureType *cl, bool use_write_table,\n+                                                              uint worker_id) {\n+\n+  \/\/ This is called only for young gen collection, when we scan old gen regions\n+  assert(region->is_old(), \"Expecting an old region\");\n+  HeapWord *start_of_range = region->bottom() + start_offset;\n+  size_t start_cluster_no = cluster_for_addr(start_of_range);\n+  assert(addr_for_cluster(start_cluster_no) == start_of_range, \"process_region_slice range must align on cluster boundary\");\n+\n+  \/\/ region->end() represents the end of memory spanned by this region, but not all of this\n+  \/\/   memory is eligible to be scanned because some of this memory has not yet been allocated.\n+  \/\/\n+  \/\/ region->top() represents the end of allocated memory within this region.  Any addresses\n+  \/\/   beyond region->top() should not be scanned as that memory does not hold valid objects.\n+\n+  if (use_write_table) {\n+    \/\/ This is update-refs servicing.\n+    if (end_of_range > region->get_update_watermark()) {\n+      end_of_range = region->get_update_watermark();\n+    }\n+  } else {\n+    \/\/ This is concurrent mark servicing.  Note that TAMS for this region is TAMS at start of old-gen\n+    \/\/ collection.  Here, we need to scan up to TAMS for most recently initiated young-gen collection.\n+    \/\/ Since all LABs are retired at init mark, and since replacement LABs are allocated lazily, and since no\n+    \/\/ promotions occur until evacuation phase, TAMS for most recent young-gen is same as top().\n+    if (end_of_range > region->top()) {\n+      end_of_range = region->top();\n+    }\n+  }\n+\n+  log_debug(gc)(\"Remembered set scan processing Region \" SIZE_FORMAT \", from \" PTR_FORMAT \" to \" PTR_FORMAT \", using %s table\",\n+                region->index(), p2i(start_of_range), p2i(end_of_range),\n+                use_write_table? \"read\/write (updating)\": \"read (marking)\");\n+\n+  \/\/ Note that end_of_range may point to the middle of a cluster because we limit scanning to\n+  \/\/ region->top() or region->get_update_watermark(). We avoid processing past end_of_range.\n+  \/\/ Objects that start between start_of_range and end_of_range, including humongous objects, will\n+  \/\/ be fully processed by process_clusters. In no case should we need to scan past end_of_range.\n+  if (start_of_range < end_of_range) {\n+    if (region->is_humongous()) {\n+      ShenandoahHeapRegion* start_region = region->humongous_start_region();\n+      process_humongous_clusters(start_region, start_cluster_no, clusters, end_of_range, cl, use_write_table);\n+    } else {\n+      process_clusters(start_cluster_no, clusters, end_of_range, cl, use_write_table, worker_id);\n+    }\n+  }\n+}\n+\n+inline bool ShenandoahRegionChunkIterator::has_next() const {\n+  return _index < _total_chunks;\n+}\n+\n+inline bool ShenandoahRegionChunkIterator::next(struct ShenandoahRegionChunk *assignment) {\n+  if (_index >= _total_chunks) {\n+    return false;\n+  }\n+  size_t new_index = Atomic::add(&_index, (size_t) 1, memory_order_relaxed);\n+  if (new_index > _total_chunks) {\n+    \/\/ First worker that hits new_index == _total_chunks continues, other\n+    \/\/ contending workers return false.\n+    return false;\n+  }\n+  \/\/ convert to zero-based indexing\n+  new_index--;\n+  assert(new_index < _total_chunks, \"Error\");\n+\n+  \/\/ Find the group number for the assigned chunk index\n+  size_t group_no;\n+  for (group_no = 0; new_index >= _group_entries[group_no]; group_no++)\n+    ;\n+  assert(group_no < _num_groups, \"Cannot have group no greater or equal to _num_groups\");\n+\n+  \/\/ All size computations measured in HeapWord\n+  size_t region_size_words = ShenandoahHeapRegion::region_size_words();\n+  size_t group_region_index = _region_index[group_no];\n+  size_t group_region_offset = _group_offset[group_no];\n+\n+  size_t index_within_group = (group_no == 0)? new_index: new_index - _group_entries[group_no - 1];\n+  size_t group_chunk_size = _group_chunk_size[group_no];\n+  size_t offset_of_this_chunk = group_region_offset + index_within_group * group_chunk_size;\n+  size_t regions_spanned_by_chunk_offset = offset_of_this_chunk \/ region_size_words;\n+  size_t offset_within_region = offset_of_this_chunk % region_size_words;\n+\n+  size_t region_index = group_region_index + regions_spanned_by_chunk_offset;\n+\n+  assignment->_r = _heap->get_region(region_index);\n+  assignment->_chunk_offset = offset_within_region;\n+  assignment->_chunk_size = group_chunk_size;\n+  return true;\n+}\n+\n+#endif   \/\/ SHARE_GC_SHENANDOAH_SHENANDOAHSCANREMEMBEREDINLINE_HPP\n","filename":"src\/hotspot\/share\/gc\/shenandoah\/shenandoahScanRemembered.inline.hpp","additions":405,"deletions":0,"binary":false,"changes":405,"status":"added"},{"patch":"@@ -124,1 +124,2 @@\n-      if ((ov & mask_val) != 0) {\n+      \/\/ We require all bits of mask_val to be set\n+      if ((ov & mask_val) == mask_val) {\n@@ -131,1 +132,1 @@\n-        \/\/ successfully set\n+        \/\/ successfully set: if value returned from cmpxchg equals ov, then nv has overwritten value.\n@@ -159,0 +160,1 @@\n+  \/\/ Returns true iff any bit set in mask is set in this.value.\n@@ -163,0 +165,8 @@\n+  \/\/ Returns true iff all bits set in mask are set in this.value.\n+  bool is_set_exactly(uint mask) const {\n+    assert (mask < (sizeof(ShenandoahSharedValue) * CHAR_MAX), \"sanity\");\n+    uint uvalue = Atomic::load_acquire(&value);\n+    return (uvalue & mask) == mask;\n+  }\n+\n+  \/\/ Returns true iff all bits set in mask are unset in this.value.\n@@ -205,0 +215,1 @@\n+  typedef uint32_t EnumValueType;\n@@ -206,1 +217,1 @@\n-  volatile ShenandoahSharedValue value;\n+  volatile EnumValueType value;\n@@ -215,2 +226,2 @@\n-    assert (v < (sizeof(ShenandoahSharedValue) * CHAR_MAX), \"sanity\");\n-    Atomic::release_store_fence(&value, (ShenandoahSharedValue)v);\n+    assert (v < (sizeof(EnumValueType) * CHAR_MAX), \"sanity\");\n+    Atomic::release_store_fence(&value, (EnumValueType)v);\n@@ -225,2 +236,2 @@\n-    assert (new_value < (sizeof(ShenandoahSharedValue) * CHAR_MAX), \"sanity\");\n-    return (T)Atomic::cmpxchg(&value, (ShenandoahSharedValue)expected, (ShenandoahSharedValue)new_value);\n+    assert (new_value < (sizeof(EnumValueType) * CHAR_MAX), \"sanity\");\n+    return (T)Atomic::cmpxchg(&value, (EnumValueType)expected, (EnumValueType)new_value);\n@@ -229,1 +240,7 @@\n-  volatile ShenandoahSharedValue* addr_of() {\n+  T xchg(T new_value) {\n+    assert (new_value >= 0, \"sanity\");\n+    assert (new_value < (sizeof(EnumValueType) * CHAR_MAX), \"sanity\");\n+    return (T)Atomic::xchg(&value, (EnumValueType)new_value);\n+  }\n+\n+  volatile EnumValueType* addr_of() {\n","filename":"src\/hotspot\/share\/gc\/shenandoah\/shenandoahSharedVariables.hpp","additions":25,"deletions":8,"binary":false,"changes":33,"status":"modified"},{"patch":"@@ -0,0 +1,291 @@\n+\/*\n+ * Copyright Amazon.com Inc. or its affiliates. All Rights Reserved.\n+ * DO NOT ALTER OR REMOVE COPYRIGHT NOTICES OR THIS FILE HEADER.\n+ *\n+ * This code is free software; you can redistribute it and\/or modify it\n+ * under the terms of the GNU General Public License version 2 only, as\n+ * published by the Free Software Foundation.\n+ *\n+ * This code is distributed in the hope that it will be useful, but WITHOUT\n+ * ANY WARRANTY; without even the implied warranty of MERCHANTABILITY or\n+ * FITNESS FOR A PARTICULAR PURPOSE.  See the GNU General Public License\n+ * version 2 for more details (a copy is included in the LICENSE file that\n+ * accompanied this code).\n+ *\n+ * You should have received a copy of the GNU General Public License version\n+ * 2 along with this work; if not, write to the Free Software Foundation,\n+ * Inc., 51 Franklin St, Fifth Floor, Boston, MA 02110-1301 USA.\n+ *\n+ * Please contact Oracle, 500 Oracle Parkway, Redwood Shores, CA 94065 USA\n+ * or visit www.oracle.com if you need additional information or have any\n+ * questions.\n+ *\n+ *\/\n+\n+#include \"precompiled.hpp\"\n+#include \"gc\/shenandoah\/shenandoahSimpleBitMap.inline.hpp\"\n+\n+ShenandoahSimpleBitMap::ShenandoahSimpleBitMap(size_t num_bits) :\n+    _num_bits(num_bits),\n+    _num_words(align_up(num_bits, BitsPerWord) \/ BitsPerWord),\n+    _bitmap(NEW_C_HEAP_ARRAY(uintx, _num_words, mtGC))\n+{\n+  clear_all();\n+}\n+\n+ShenandoahSimpleBitMap::~ShenandoahSimpleBitMap() {\n+  if (_bitmap != nullptr) {\n+    FREE_C_HEAP_ARRAY(uintx, _bitmap);\n+  }\n+}\n+\n+size_t ShenandoahSimpleBitMap::count_leading_ones(idx_t start_idx) const {\n+  assert((start_idx >= 0) && (start_idx < _num_bits), \"precondition\");\n+  size_t array_idx = start_idx >> LogBitsPerWord;\n+  uintx element_bits = _bitmap[array_idx];\n+  uintx bit_number = start_idx & (BitsPerWord - 1);\n+  uintx mask = ~tail_mask(bit_number);\n+  size_t counted_ones = 0;\n+  while ((element_bits & mask) == mask) {\n+    \/\/ All bits numbered >= bit_number are set\n+    size_t found_ones = BitsPerWord - bit_number;\n+    counted_ones += found_ones;\n+    \/\/ Dead code: do not need to compute: start_idx += found_ones;\n+    \/\/ Strength reduction:                array_idx = (start_idx >> LogBitsPerWord)\n+    array_idx++;\n+    element_bits = _bitmap[array_idx];\n+    \/\/ Constant folding:                  bit_number = start_idx & (BitsPerWord - 1);\n+    bit_number = 0;\n+    \/\/ Constant folding:                  mask = ~right_n_bits(bit_number);\n+    mask = ~0;\n+  }\n+\n+  \/\/ Add in number of consecutive ones starting with the_bit and including more significant bits and return result\n+  uintx aligned = element_bits >> bit_number;\n+  uintx complement = ~aligned;\n+  return counted_ones + count_trailing_zeros<uintx>(complement);\n+}\n+\n+size_t ShenandoahSimpleBitMap::count_trailing_ones(idx_t last_idx) const {\n+  assert((last_idx >= 0) && (last_idx < _num_bits), \"precondition\");\n+  size_t array_idx = last_idx >> LogBitsPerWord;\n+  uintx element_bits = _bitmap[array_idx];\n+  uintx bit_number = last_idx & (BitsPerWord - 1);\n+  \/\/ All ones from bit 0 to the_bit\n+  uintx mask = tail_mask(bit_number + 1);\n+  size_t counted_ones = 0;\n+  while ((element_bits & mask) == mask) {\n+    \/\/ All bits numbered <= bit_number are set\n+    size_t found_ones = bit_number + 1;\n+    counted_ones += found_ones;\n+    \/\/ Dead code: do not need to compute: last_idx -= found_ones;\n+    array_idx--;\n+    element_bits = _bitmap[array_idx];\n+    \/\/ Constant folding:                  bit_number = last_idx & (BitsPerWord - 1);\n+    bit_number = BitsPerWord - 1;\n+    \/\/ Constant folding:                  mask = right_n_bits(bit_number + 1);\n+    mask = ~0;\n+  }\n+\n+  \/\/ Add in number of consecutive ones starting with the_bit and including less significant bits and return result\n+  uintx aligned = element_bits << (BitsPerWord - (bit_number + 1));\n+  uintx complement = ~aligned;\n+  return counted_ones + count_leading_zeros<uintx>(complement);\n+}\n+\n+bool ShenandoahSimpleBitMap::is_forward_consecutive_ones(idx_t start_idx, idx_t count) const {\n+  while (count > 0) {\n+    assert((start_idx >= 0) && (start_idx < _num_bits), \"precondition: start_idx: \" SSIZE_FORMAT \", count: \" SSIZE_FORMAT,\n+           start_idx, count);\n+    assert(start_idx + count <= (idx_t) _num_bits, \"precondition\");\n+    size_t array_idx = start_idx >> LogBitsPerWord;\n+    uintx bit_number = start_idx & (BitsPerWord - 1);\n+    uintx element_bits = _bitmap[array_idx];\n+    uintx bits_to_examine  = BitsPerWord - bit_number;\n+    element_bits >>= bit_number;\n+    uintx complement = ~element_bits;\n+    uintx trailing_ones;\n+    if (complement != 0) {\n+      trailing_ones = count_trailing_zeros<uintx>(complement);\n+    } else {\n+      trailing_ones = bits_to_examine;\n+    }\n+    if (trailing_ones >= (uintx) count) {\n+      return true;\n+    } else if (trailing_ones == bits_to_examine) {\n+      start_idx += bits_to_examine;\n+      count -= bits_to_examine;\n+      \/\/ Repeat search with smaller goal\n+    } else {\n+      return false;\n+    }\n+  }\n+  return true;\n+}\n+\n+bool ShenandoahSimpleBitMap::is_backward_consecutive_ones(idx_t last_idx, idx_t count) const {\n+  while (count > 0) {\n+    assert((last_idx >= 0) && (last_idx < _num_bits), \"precondition\");\n+    assert(last_idx - count >= -1, \"precondition\");\n+    size_t array_idx = last_idx >> LogBitsPerWord;\n+    uintx bit_number = last_idx & (BitsPerWord - 1);\n+    uintx element_bits = _bitmap[array_idx];\n+    uintx bits_to_examine = bit_number + 1;\n+    element_bits <<= (BitsPerWord - bits_to_examine);\n+    uintx complement = ~element_bits;\n+    uintx leading_ones;\n+    if (complement != 0) {\n+      leading_ones = count_leading_zeros<uintx>(complement);\n+    } else {\n+      leading_ones = bits_to_examine;\n+    }\n+    if (leading_ones >= (uintx) count) {\n+      return true;\n+    } else if (leading_ones == bits_to_examine) {\n+      last_idx -= leading_ones;\n+      count -= leading_ones;\n+      \/\/ Repeat search with smaller goal\n+    } else {\n+      return false;\n+    }\n+  }\n+  return true;\n+}\n+\n+idx_t ShenandoahSimpleBitMap::find_first_consecutive_set_bits(idx_t beg, idx_t end, size_t num_bits) const {\n+  assert((beg >= 0) && (beg < _num_bits), \"precondition\");\n+\n+  \/\/ Stop looking if there are not num_bits remaining in probe space.\n+  idx_t start_boundary = end - num_bits;\n+  if (beg > start_boundary) {\n+    return end;\n+  }\n+  uintx array_idx = beg >> LogBitsPerWord;\n+  uintx bit_number = beg & (BitsPerWord - 1);\n+  uintx element_bits = _bitmap[array_idx];\n+  if (bit_number > 0) {\n+    uintx mask_out = tail_mask(bit_number);\n+    element_bits &= ~mask_out;\n+  }\n+\n+  \/\/ The following loop minimizes the number of spans probed in order to find num_bits consecutive bits.\n+  \/\/ For example, if bit_number = beg = 0, num_bits = 8, and element bits equals 00111111_11000000_00000000_10011000B,\n+  \/\/ we need only 3 probes to find the match at bit offset 22.\n+  \/\/\n+  \/\/ Let beg = 0\n+  \/\/ element_bits = 00111111_11000000_00000000_10011000B;\n+  \/\/                                           ________   (the searched span)\n+  \/\/                                           ^   ^  ^- bit_number = beg = 0\n+  \/\/                                           |   +-- next_start_candidate_1 (where next 1 is found)\n+  \/\/                                           +------ next_start_candidate_2 (start of the trailing 1s within span)\n+  \/\/ Let beg = 7\n+  \/\/ element_bits = 00111111_11000000_00000000_10011000B;\n+  \/\/                          ^       ^_________   (the searched span)\n+  \/\/                          |       |        ^- bit_number = beg = 7\n+  \/\/                          |       +---------- next_start_candidate_2 (there are no trailing 1s within span)\n+  \/\/                          +------------------ next_start_candidate_1 (where next 1 is found)\n+  \/\/ Let beg = 22\n+  \/\/ Let beg = 22\n+  \/\/ element_bits = 00111111_11000001_11111100_10011000B;\n+  \/\/                  _________   (the searched span)\n+  \/\/                          ^- bit_number = beg = 18\n+  \/\/ Here, is_forward_consecutive_ones(22, 8) succeeds and we report the match\n+\n+  while (true) {\n+    if (element_bits == 0) {\n+      \/\/ move to the next element\n+      beg += BitsPerWord - bit_number;\n+      if (beg > start_boundary) {\n+        \/\/ No match found.\n+        return end;\n+      }\n+      array_idx++;\n+      bit_number = 0;\n+      element_bits = _bitmap[array_idx];\n+    } else if (is_forward_consecutive_ones(beg, num_bits)) {\n+      return beg;\n+    } else {\n+      \/\/ There is at least one non-zero bit within the masked element_bits. Arrange to skip over bits that\n+      \/\/ cannot be part of a consecutive-ones match.\n+      uintx next_set_bit = count_trailing_zeros<uintx>(element_bits);\n+      uintx next_start_candidate_1 = (array_idx << LogBitsPerWord) + next_set_bit;\n+\n+      \/\/ There is at least one zero bit in this span. Align the next probe at the start of trailing ones for probed span,\n+      \/\/ or align at end of span if this span has no trailing ones.\n+      size_t trailing_ones = count_trailing_ones(beg + num_bits - 1);\n+      uintx next_start_candidate_2 = beg + num_bits - trailing_ones;\n+\n+      beg = MAX2(next_start_candidate_1, next_start_candidate_2);\n+      if (beg > start_boundary) {\n+        \/\/ No match found.\n+        return end;\n+      }\n+      array_idx = beg >> LogBitsPerWord;\n+      element_bits = _bitmap[array_idx];\n+      bit_number = beg & (BitsPerWord - 1);\n+      if (bit_number > 0) {\n+        size_t mask_out = tail_mask(bit_number);\n+        element_bits &= ~mask_out;\n+      }\n+    }\n+  }\n+}\n+\n+idx_t ShenandoahSimpleBitMap::find_last_consecutive_set_bits(const idx_t beg, idx_t end, const size_t num_bits) const {\n+\n+  assert((end >= 0) && (end < _num_bits), \"precondition\");\n+\n+  \/\/ Stop looking if there are not num_bits remaining in probe space.\n+  idx_t last_boundary = beg + num_bits;\n+  if (end < last_boundary) {\n+    return beg;\n+  }\n+\n+  size_t array_idx = end >> LogBitsPerWord;\n+  uintx bit_number = end & (BitsPerWord - 1);\n+  uintx element_bits = _bitmap[array_idx];\n+  if (bit_number < BitsPerWord - 1) {\n+    uintx mask_in = tail_mask(bit_number + 1);\n+    element_bits &= mask_in;\n+  }\n+\n+  \/\/ See comment in find_first_consecutive_set_bits to understand how this loop works.\n+  while (true) {\n+    if (element_bits == 0) {\n+      \/\/ move to the previous element\n+      end -= bit_number + 1;\n+      if (end < last_boundary) {\n+        \/\/ No match found.\n+        return beg;\n+      }\n+      array_idx--;\n+      bit_number = BitsPerWord - 1;\n+      element_bits = _bitmap[array_idx];\n+    } else if (is_backward_consecutive_ones(end, num_bits)) {\n+      return end + 1 - num_bits;\n+    } else {\n+      \/\/ There is at least one non-zero bit within the masked element_bits. Arrange to skip over bits that\n+      \/\/ cannot be part of a consecutive-ones match.\n+      uintx next_set_bit = BitsPerWord - (1 + count_leading_zeros<uintx>(element_bits));\n+      uintx next_last_candidate_1 = (array_idx << LogBitsPerWord) + next_set_bit;\n+\n+      \/\/ There is at least one zero bit in this span.  Align the next probe at the end of leading ones for probed span,\n+      \/\/ or align before start of span if this span has no leading ones.\n+      size_t leading_ones = count_leading_ones(end - (num_bits - 1));\n+      uintx next_last_candidate_2 = end - (num_bits - leading_ones);\n+\n+      end = MIN2(next_last_candidate_1, next_last_candidate_2);\n+      if (end < last_boundary) {\n+        \/\/ No match found.\n+        return beg;\n+      }\n+      array_idx = end >> LogBitsPerWord;\n+      bit_number = end & (BitsPerWord - 1);\n+      element_bits = _bitmap[array_idx];\n+      if (bit_number < BitsPerWord - 1){\n+        size_t mask_in = tail_mask(bit_number + 1);\n+        element_bits &= mask_in;\n+      }\n+    }\n+  }\n+}\n","filename":"src\/hotspot\/share\/gc\/shenandoah\/shenandoahSimpleBitMap.cpp","additions":291,"deletions":0,"binary":false,"changes":291,"status":"added"},{"patch":"@@ -0,0 +1,172 @@\n+\/*\n+ * Copyright Amazon.com Inc. or its affiliates. All Rights Reserved.\n+ * DO NOT ALTER OR REMOVE COPYRIGHT NOTICES OR THIS FILE HEADER.\n+ *\n+ * This code is free software; you can redistribute it and\/or modify it\n+ * under the terms of the GNU General Public License version 2 only, as\n+ * published by the Free Software Foundation.\n+ *\n+ * This code is distributed in the hope that it will be useful, but WITHOUT\n+ * ANY WARRANTY; without even the implied warranty of MERCHANTABILITY or\n+ * FITNESS FOR A PARTICULAR PURPOSE.  See the GNU General Public License\n+ * version 2 for more details (a copy is included in the LICENSE file that\n+ * accompanied this code).\n+ *\n+ * You should have received a copy of the GNU General Public License version\n+ * 2 along with this work; if not, write to the Free Software Foundation,\n+ * Inc., 51 Franklin St, Fifth Floor, Boston, MA 02110-1301 USA.\n+ *\n+ * Please contact Oracle, 500 Oracle Parkway, Redwood Shores, CA 94065 USA\n+ * or visit www.oracle.com if you need additional information or have any\n+ * questions.\n+ *\n+ *\/\n+\n+#ifndef SHARE_GC_SHENANDOAH_SHENANDOAHSIMPLEBITMAP_HPP\n+#define SHARE_GC_SHENANDOAH_SHENANDOAHSIMPLEBITMAP_HPP\n+\n+#include <cstddef>\n+\n+#include \"gc\/shenandoah\/shenandoahAsserts.hpp\"\n+\n+\/\/ TODO: Merge the enhanced capabilities of ShenandoahSimpleBitMap into src\/hotspot\/share\/utilities\/bitMap.hpp\n+\/\/       and deprecate ShenandoahSimpleBitMap.  The key enhanced capabilities to be integrated include:\n+\/\/\n+\/\/   1. Allow searches from high to low memory (when biasing allocations towards the top of the heap)\n+\/\/   2. Allow searches for clusters of contiguous set bits (to expedite allocation for humongous objects)\n+\/\/\n+\/\/ idx_t is defined here as ssize_t.  In src\/hotspot\/share\/utiliities\/bitMap.hpp, idx is defined as size_t.\n+\/\/ This is a significant incompatibility.\n+\/\/\n+\/\/ The API and internal implementation of ShenandoahSimpleBitMap and ShenandoahRegionPartitions use idx_t to\n+\/\/ represent index, even though index is \"inherently\" unsigned.  There are two reasons for this choice:\n+\/\/  1. We use -1 as a sentinel value to represent empty partitions.  This same value may be used to represent\n+\/\/     failure to find a previous set bit or previous range of set bits.\n+\/\/  2. Certain loops are written most naturally if the iterator, which may hold the sentinel -1 value, can be\n+\/\/     declared as signed and the terminating condition can be < 0.\n+\n+typedef ssize_t idx_t;\n+\n+\/\/ ShenandoahSimpleBitMap resembles CHeapBitMap but adds missing support for find_first_consecutive_set_bits() and\n+\/\/ find_last_consecutive_set_bits.  An alternative refactoring of code would subclass CHeapBitMap, but this might\n+\/\/ break abstraction rules, because efficient implementation requires assumptions about superclass internals that\n+\/\/ might be violated through future software maintenance.\n+class ShenandoahSimpleBitMap {\n+  const idx_t _num_bits;\n+  const size_t _num_words;\n+  uintx* const _bitmap;\n+\n+public:\n+  ShenandoahSimpleBitMap(size_t num_bits);\n+\n+  ~ShenandoahSimpleBitMap();\n+\n+  void clear_all() {\n+    for (size_t i = 0; i < _num_words; i++) {\n+      _bitmap[i] = 0;\n+    }\n+  }\n+\n+private:\n+\n+  \/\/ Count consecutive ones in forward order, starting from start_idx.  Requires that there is at least one zero\n+  \/\/ between start_idx and index value (_num_bits - 1), inclusive.\n+  size_t count_leading_ones(idx_t start_idx) const;\n+\n+  \/\/ Count consecutive ones in reverse order, starting from last_idx.  Requires that there is at least one zero\n+  \/\/ between last_idx and index value zero, inclusive.\n+  size_t count_trailing_ones(idx_t last_idx) const;\n+\n+  bool is_forward_consecutive_ones(idx_t start_idx, idx_t count) const;\n+  bool is_backward_consecutive_ones(idx_t last_idx, idx_t count) const;\n+\n+  static inline uintx tail_mask(uintx bit_number);\n+\n+public:\n+\n+  inline idx_t aligned_index(idx_t idx) const {\n+    assert((idx >= 0) && (idx < _num_bits), \"precondition\");\n+    idx_t array_idx = idx & ~(BitsPerWord - 1);\n+    return array_idx;\n+  }\n+\n+  inline constexpr idx_t alignment() const {\n+    return BitsPerWord;\n+  }\n+\n+  \/\/ For testing\n+  inline idx_t size() const {\n+    return _num_bits;\n+  }\n+\n+  \/\/ Return the word that holds idx bit and its neighboring bits.\n+  inline uintx bits_at(idx_t idx) const {\n+    assert((idx >= 0) && (idx < _num_bits), \"precondition\");\n+    idx_t array_idx = idx >> LogBitsPerWord;\n+    return _bitmap[array_idx];\n+  }\n+\n+  inline void set_bit(idx_t idx) {\n+    assert((idx >= 0) && (idx < _num_bits), \"precondition\");\n+    size_t array_idx = idx >> LogBitsPerWord;\n+    uintx bit_number = idx & (BitsPerWord - 1);\n+    uintx the_bit = nth_bit(bit_number);\n+    _bitmap[array_idx] |= the_bit;\n+  }\n+\n+  inline void clear_bit(idx_t idx) {\n+    assert((idx >= 0) && (idx < _num_bits), \"precondition\");\n+    assert(idx >= 0, \"precondition\");\n+    size_t array_idx = idx >> LogBitsPerWord;\n+    uintx bit_number = idx & (BitsPerWord - 1);\n+    uintx the_bit = nth_bit(bit_number);\n+    _bitmap[array_idx] &= ~the_bit;\n+  }\n+\n+  inline bool is_set(idx_t idx) const {\n+    assert((idx >= 0) && (idx < _num_bits), \"precondition\");\n+    assert(idx >= 0, \"precondition\");\n+    size_t array_idx = idx >> LogBitsPerWord;\n+    uintx bit_number = idx & (BitsPerWord - 1);\n+    uintx the_bit = nth_bit(bit_number);\n+    return (_bitmap[array_idx] & the_bit) != 0;\n+  }\n+\n+  \/\/ Return the index of the first set bit in the range [beg, size()), or size() if none found.\n+  \/\/ precondition: beg and end form a valid range for the bitmap.\n+  inline idx_t find_first_set_bit(idx_t beg) const;\n+\n+  \/\/ Return the index of the first set bit in the range [beg, end), or end if none found.\n+  \/\/ precondition: beg and end form a valid range for the bitmap.\n+  inline idx_t find_first_set_bit(idx_t beg, idx_t end) const;\n+\n+  \/\/ Return the index of the last set bit in the range (-1, end], or -1 if none found.\n+  \/\/ precondition: beg and end form a valid range for the bitmap.\n+  inline idx_t find_last_set_bit(idx_t end) const;\n+\n+  \/\/ Return the index of the last set bit in the range (beg, end], or beg if none found.\n+  \/\/ precondition: beg and end form a valid range for the bitmap.\n+  inline idx_t find_last_set_bit(idx_t beg, idx_t end) const;\n+\n+  \/\/ Return the start index of the first run of <num_bits> consecutive set bits for which the first set bit is within\n+  \/\/   the range [beg, size()), or size() if the run of <num_bits> is not found within this range.\n+  \/\/ precondition: beg is within the valid range for the bitmap.\n+  inline idx_t find_first_consecutive_set_bits(idx_t beg, size_t num_bits) const;\n+\n+  \/\/ Return the start index of the first run of <num_bits> consecutive set bits for which the first set bit is within\n+  \/\/   the range [beg, end), or end if the run of <num_bits> is not found within this range.\n+  \/\/ precondition: beg and end form a valid range for the bitmap.\n+  idx_t find_first_consecutive_set_bits(idx_t beg, idx_t end, size_t num_bits) const;\n+\n+  \/\/ Return the start index of the last run of <num_bits> consecutive set bits for which the entire run of set bits is within\n+  \/\/ the range (-1, end], or -1 if the run of <num_bits> is not found within this range.\n+  \/\/ precondition: end is within the valid range for the bitmap.\n+  inline idx_t find_last_consecutive_set_bits(idx_t end, size_t num_bits) const;\n+\n+  \/\/ Return the start index of the first run of <num_bits> consecutive set bits for which the entire run of set bits is within\n+  \/\/ the range (beg, end], or beg if the run of <num_bits> is not found within this range.\n+  \/\/ precondition: beg and end form a valid range for the bitmap.\n+  idx_t find_last_consecutive_set_bits(idx_t beg, idx_t end, size_t num_bits) const;\n+};\n+\n+#endif \/\/ SHARE_GC_SHENANDOAH_SHENANDOAHSIMPLEBITMAP_HPP\n","filename":"src\/hotspot\/share\/gc\/shenandoah\/shenandoahSimpleBitMap.hpp","additions":172,"deletions":0,"binary":false,"changes":172,"status":"added"},{"patch":"@@ -0,0 +1,107 @@\n+\/*\n+ * Copyright Amazon.com Inc. or its affiliates. All Rights Reserved.\n+ * DO NOT ALTER OR REMOVE COPYRIGHT NOTICES OR THIS FILE HEADER.\n+ *\n+ * This code is free software; you can redistribute it and\/or modify it\n+ * under the terms of the GNU General Public License version 2 only, as\n+ * published by the Free Software Foundation.\n+ *\n+ * This code is distributed in the hope that it will be useful, but WITHOUT\n+ * ANY WARRANTY; without even the implied warranty of MERCHANTABILITY or\n+ * FITNESS FOR A PARTICULAR PURPOSE.  See the GNU General Public License\n+ * version 2 for more details (a copy is included in the LICENSE file that\n+ * accompanied this code).\n+ *\n+ * You should have received a copy of the GNU General Public License version\n+ * 2 along with this work; if not, write to the Free Software Foundation,\n+ * Inc., 51 Franklin St, Fifth Floor, Boston, MA 02110-1301 USA.\n+ *\n+ * Please contact Oracle, 500 Oracle Parkway, Redwood Shores, CA 94065 USA\n+ * or visit www.oracle.com if you need additional information or have any\n+ * questions.\n+ *\n+ *\/\n+\n+#ifndef SHARE_GC_SHENANDOAH_SHENANDOAHSIMPLEBITMAP_INLINE_HPP\n+#define SHARE_GC_SHENANDOAH_SHENANDOAHSIMPLEBITMAP_INLINE_HPP\n+\n+#include \"gc\/shenandoah\/shenandoahSimpleBitMap.hpp\"\n+\n+inline uintx ShenandoahSimpleBitMap::tail_mask(uintx bit_number) {\n+  if (bit_number >= BitsPerWord) {\n+    return -1;\n+  }\n+  return (uintx(1) << bit_number) - 1;\n+}\n+\n+inline idx_t ShenandoahSimpleBitMap::find_first_set_bit(idx_t beg, idx_t end) const {\n+  assert((beg >= 0) && (beg < _num_bits), \"precondition\");\n+  assert((end > beg) && (end <= _num_bits), \"precondition\");\n+  do {\n+    size_t array_idx = beg >> LogBitsPerWord;\n+    uintx bit_number = beg & (BitsPerWord - 1);\n+    uintx element_bits = _bitmap[array_idx];\n+    if (bit_number > 0) {\n+      uintx mask_out = tail_mask(bit_number);\n+      element_bits &= ~mask_out;\n+    }\n+    if (element_bits) {\n+      \/\/ The next set bit is here.  Find first set bit >= bit_number;\n+      uintx aligned = element_bits >> bit_number;\n+      uintx first_set_bit = count_trailing_zeros<uintx>(aligned);\n+      idx_t candidate_result = (array_idx * BitsPerWord) + bit_number + first_set_bit;\n+      return (candidate_result < end)? candidate_result: end;\n+    } else {\n+      \/\/ Next bit is not here.  Try the next array element\n+      beg += BitsPerWord - bit_number;\n+    }\n+  } while (beg < end);\n+  return end;\n+}\n+\n+inline idx_t ShenandoahSimpleBitMap::find_first_set_bit(idx_t beg) const {\n+  assert((beg >= 0) && (beg < size()), \"precondition\");\n+  return find_first_set_bit(beg, size());\n+}\n+\n+inline idx_t ShenandoahSimpleBitMap::find_last_set_bit(idx_t beg, idx_t end) const {\n+  assert((end >= 0) && (end < _num_bits), \"precondition\");\n+  assert((beg >= -1) && (beg < end), \"precondition\");\n+  do {\n+    idx_t array_idx = end >> LogBitsPerWord;\n+    uint8_t bit_number = end & (BitsPerWord - 1);\n+    uintx element_bits = _bitmap[array_idx];\n+    if (bit_number < BitsPerWord - 1){\n+      uintx mask_in = tail_mask(bit_number + 1);\n+      element_bits &= mask_in;\n+    }\n+    if (element_bits) {\n+      \/\/ The prev set bit is here.  Find the first set bit <= bit_number\n+      uintx aligned = element_bits << (BitsPerWord - (bit_number + 1));\n+      uintx first_set_bit = count_leading_zeros<uintx>(aligned);\n+      idx_t candidate_result = array_idx * BitsPerWord + (bit_number - first_set_bit);\n+      return (candidate_result > beg)? candidate_result: beg;\n+    } else {\n+      \/\/ Next bit is not here.  Try the previous array element\n+      end -= (bit_number + 1);\n+    }\n+  } while (end > beg);\n+  return beg;\n+}\n+\n+inline idx_t ShenandoahSimpleBitMap::find_last_set_bit(idx_t end) const {\n+  assert((end >= 0) && (end < _num_bits), \"precondition\");\n+  return find_last_set_bit(-1, end);\n+}\n+\n+inline idx_t ShenandoahSimpleBitMap::find_first_consecutive_set_bits(idx_t beg, size_t num_bits) const {\n+  assert((beg >= 0) && (beg < _num_bits), \"precondition\");\n+  return find_first_consecutive_set_bits(beg, size(), num_bits);\n+}\n+\n+inline idx_t ShenandoahSimpleBitMap::find_last_consecutive_set_bits(idx_t end, size_t num_bits) const {\n+  assert((end >= 0) && (end < _num_bits), \"precondition\");\n+  return find_last_consecutive_set_bits((idx_t) -1, end, num_bits);\n+}\n+\n+#endif \/\/ SHARE_GC_SHENANDOAH_SHENANDOAHSIMPLEBITMAP_INLINE_HPP\n","filename":"src\/hotspot\/share\/gc\/shenandoah\/shenandoahSimpleBitMap.inline.hpp","additions":107,"deletions":0,"binary":false,"changes":107,"status":"added"},{"patch":"@@ -4,0 +4,1 @@\n+ * Copyright Amazon.com Inc. or its affiliates. All Rights Reserved.\n@@ -78,3 +79,1 @@\n-    if (_heap->is_concurrent_mark_in_progress()) {\n-      return &_keep_alive_cl;\n-    } else if (_heap->is_concurrent_weak_root_in_progress()) {\n+    if (_heap->is_concurrent_weak_root_in_progress()) {\n@@ -83,0 +82,2 @@\n+    } else if (_heap->is_concurrent_mark_in_progress()) {\n+      return &_keep_alive_cl;\n@@ -95,8 +96,1 @@\n-  if (heap->is_concurrent_mark_in_progress()) {\n-    \/\/ We need to reset all TLABs because they might be below the TAMS, and we need to mark\n-    \/\/ the objects in them. Do not let mutators allocate any new objects in their current TLABs.\n-    \/\/ It is also a good place to resize the TLAB sizes for future allocations.\n-    retire_tlab();\n-\n-    _jt->oops_do_no_frames(closure_from_context(context), &_cb_cl);\n-  } else if (heap->is_concurrent_weak_root_in_progress()) {\n+  if (heap->is_concurrent_weak_root_in_progress()) {\n@@ -111,0 +105,7 @@\n+    _jt->oops_do_no_frames(closure_from_context(context), &_cb_cl);\n+  } else if (heap->is_concurrent_mark_in_progress()) {\n+    \/\/ We need to reset all TLABs because they might be below the TAMS, and we need to mark\n+    \/\/ the objects in them. Do not let mutators allocate any new objects in their current TLABs.\n+    \/\/ It is also a good place to resize the TLAB sizes for future allocations.\n+    retire_tlab();\n+\n","filename":"src\/hotspot\/share\/gc\/shenandoah\/shenandoahStackWatermark.cpp","additions":12,"deletions":11,"binary":false,"changes":23,"status":"modified"},{"patch":"@@ -28,2 +28,0 @@\n-#include \"gc\/shenandoah\/shenandoahStringDedup.hpp\"\n-\n@@ -31,0 +29,3 @@\n+#include \"gc\/shenandoah\/shenandoahHeap.inline.hpp\"\n+#include \"gc\/shenandoah\/shenandoahStringDedup.hpp\"\n+#include \"oops\/markWord.hpp\"\n@@ -48,16 +49,4 @@\n-  const markWord mark = obj->mark();\n-\n-  \/\/ Having\/had displaced header, too risky to deal with them, skip\n-  if (mark == markWord::INFLATING() || mark.has_displaced_mark_helper()) {\n-    return false;\n-  }\n-\n-  if (StringDedup::is_below_threshold_age(mark.age())) {\n-    \/\/ Increase string age and enqueue it when it reaches age threshold\n-    markWord new_mark = mark.incr_age();\n-    if (mark == obj->cas_set_mark(new_mark, mark)) {\n-      return StringDedup::is_threshold_age(new_mark.age()) &&\n-             !dedup_requested(obj);\n-    }\n-  }\n-  return false;\n+  uint age = ShenandoahHeap::get_object_age(obj);\n+  return (age <= markWord::max_age) &&\n+         StringDedup::is_below_threshold_age(age) &&\n+         !dedup_requested(obj);\n","filename":"src\/hotspot\/share\/gc\/shenandoah\/shenandoahStringDedup.inline.hpp","additions":7,"deletions":18,"binary":false,"changes":25,"status":"modified"},{"patch":"@@ -54,39 +54,0 @@\n-#if TASKQUEUE_STATS\n-void ShenandoahObjToScanQueueSet::print_taskqueue_stats_hdr(outputStream* const st) {\n-  st->print_raw_cr(\"GC Task Stats\");\n-  st->print_raw(\"thr \"); TaskQueueStats::print_header(1, st); st->cr();\n-  st->print_raw(\"--- \"); TaskQueueStats::print_header(2, st); st->cr();\n-}\n-\n-void ShenandoahObjToScanQueueSet::print_taskqueue_stats() const {\n-  if (!log_develop_is_enabled(Trace, gc, task, stats)) {\n-    return;\n-  }\n-  Log(gc, task, stats) log;\n-  ResourceMark rm;\n-  LogStream ls(log.trace());\n-  outputStream* st = &ls;\n-  print_taskqueue_stats_hdr(st);\n-\n-  ShenandoahObjToScanQueueSet* queues = const_cast<ShenandoahObjToScanQueueSet*>(this);\n-  TaskQueueStats totals;\n-  const uint n = size();\n-  for (uint i = 0; i < n; ++i) {\n-    st->print(UINT32_FORMAT_W(3), i);\n-    queues->queue(i)->stats.print(st);\n-    st->cr();\n-    totals += queues->queue(i)->stats;\n-  }\n-  st->print(\"tot \"); totals.print(st); st->cr();\n-  DEBUG_ONLY(totals.verify());\n-\n-}\n-\n-void ShenandoahObjToScanQueueSet::reset_taskqueue_stats() {\n-  const uint n = size();\n-  for (uint i = 0; i < n; ++i) {\n-    queue(i)->stats.reset();\n-  }\n-}\n-#endif \/\/ TASKQUEUE_STATS\n-\n","filename":"src\/hotspot\/share\/gc\/shenandoah\/shenandoahTaskqueue.cpp","additions":0,"deletions":39,"binary":false,"changes":39,"status":"modified"},{"patch":"@@ -37,0 +37,2 @@\n+class ShenandoahHeap;\n+\n@@ -355,6 +357,0 @@\n-\n-#if TASKQUEUE_STATS\n-  static void print_taskqueue_stats_hdr(outputStream* const st);\n-  void print_taskqueue_stats() const;\n-  void reset_taskqueue_stats();\n-#endif \/\/ TASKQUEUE_STATS\n","filename":"src\/hotspot\/share\/gc\/shenandoah\/shenandoahTaskqueue.hpp","additions":2,"deletions":6,"binary":false,"changes":8,"status":"modified"},{"patch":"@@ -0,0 +1,65 @@\n+\/*\n+ * Copyright (c) 2018, 2023, Red Hat, Inc. All rights reserved.\n+ * Copyright Amazon.com Inc. or its affiliates. All Rights Reserved.\n+ * DO NOT ALTER OR REMOVE COPYRIGHT NOTICES OR THIS FILE HEADER.\n+ *\n+ * This code is free software; you can redistribute it and\/or modify it\n+ * under the terms of the GNU General Public License version 2 only, as\n+ * published by the Free Software Foundation.\n+ *\n+ * This code is distributed in the hope that it will be useful, but WITHOUT\n+ * ANY WARRANTY; without even the implied warranty of MERCHANTABILITY or\n+ * FITNESS FOR A PARTICULAR PURPOSE.  See the GNU General Public License\n+ * version 2 for more details (a copy is included in the LICENSE file that\n+ * accompanied this code).\n+ *\n+ * You should have received a copy of the GNU General Public License version\n+ * 2 along with this work; if not, write to the Free Software Foundation,\n+ * Inc., 51 Franklin St, Fifth Floor, Boston, MA 02110-1301 USA.\n+ *\n+ * Please contact Oracle, 500 Oracle Parkway, Redwood Shores, CA 94065 USA\n+ * or visit www.oracle.com if you need additional information or have any\n+ * questions.\n+ *\n+ *\/\n+\n+#include \"precompiled.hpp\"\n+\n+#include \"gc\/shenandoah\/mode\/shenandoahMode.hpp\"\n+#include \"gc\/shenandoah\/shenandoahEvacTracker.hpp\"\n+#include \"gc\/shenandoah\/shenandoahGenerationalHeap.hpp\"\n+#include \"gc\/shenandoah\/shenandoahHeap.inline.hpp\"\n+#include \"gc\/shenandoah\/shenandoahThreadLocalData.hpp\"\n+\n+ShenandoahThreadLocalData::ShenandoahThreadLocalData() :\n+  _gc_state(0),\n+  _oom_scope_nesting_level(0),\n+  _oom_during_evac(false),\n+  _satb_mark_queue(&ShenandoahBarrierSet::satb_mark_queue_set()),\n+  _card_table(nullptr),\n+  _gclab(nullptr),\n+  _gclab_size(0),\n+  _paced_time(0),\n+  _plab(nullptr),\n+  _plab_desired_size(0),\n+  _plab_actual_size(0),\n+  _plab_promoted(0),\n+  _plab_allows_promotion(true),\n+  _plab_retries_enabled(true),\n+  _evacuation_stats(nullptr) {\n+  if (ShenandoahHeap::heap()->mode()->is_generational()) {\n+    _evacuation_stats = new ShenandoahEvacuationStats();\n+  }\n+}\n+\n+ShenandoahThreadLocalData::~ShenandoahThreadLocalData() {\n+  if (_gclab != nullptr) {\n+    delete _gclab;\n+  }\n+  if (_plab != nullptr) {\n+    ShenandoahGenerationalHeap::heap()->retire_plab(_plab);\n+    delete _plab;\n+  }\n+\n+  delete _evacuation_stats;\n+}\n","filename":"src\/hotspot\/share\/gc\/shenandoah\/shenandoahThreadLocalData.cpp","additions":65,"deletions":0,"binary":false,"changes":65,"status":"added"},{"patch":"@@ -3,0 +3,1 @@\n+ * Copyright Amazon.com Inc. or its affiliates. All Rights Reserved.\n@@ -32,0 +33,1 @@\n+#include \"gc\/shenandoah\/shenandoahCardTable.hpp\"\n@@ -33,0 +35,2 @@\n+#include \"gc\/shenandoah\/shenandoahGenerationalHeap.hpp\"\n+#include \"gc\/shenandoah\/shenandoahEvacTracker.hpp\"\n@@ -34,0 +38,1 @@\n+#include \"gc\/shenandoah\/mode\/shenandoahMode.hpp\"\n@@ -44,0 +49,1 @@\n+\n@@ -45,0 +51,6 @@\n+\n+  \/\/ Current active CardTable's byte_map_base for this thread.\n+  CardTable::CardValue*   _card_table;\n+\n+  \/\/ Thread-local allocation buffer for object evacuations.\n+  \/\/ In generational mode, it is exclusive to the young generation.\n@@ -47,0 +59,1 @@\n+\n@@ -49,9 +62,5 @@\n-  ShenandoahThreadLocalData() :\n-    _gc_state(0),\n-    _oom_scope_nesting_level(0),\n-    _oom_during_evac(false),\n-    _satb_mark_queue(&ShenandoahBarrierSet::satb_mark_queue_set()),\n-    _gclab(nullptr),\n-    _gclab_size(0),\n-    _paced_time(0) {\n-  }\n+  \/\/ Thread-local allocation buffer only used in generational mode.\n+  \/\/ Used both by mutator threads and by GC worker threads\n+  \/\/ for evacuations within the old generation and\n+  \/\/ for promotions from the young generation into the old generation.\n+  PLAB* _plab;\n@@ -59,5 +68,21 @@\n-  ~ShenandoahThreadLocalData() {\n-    if (_gclab != nullptr) {\n-      delete _gclab;\n-    }\n-  }\n+  \/\/ Heuristics will grow the desired size of plabs.\n+  size_t _plab_desired_size;\n+\n+  \/\/ Once the plab has been allocated, and we know the actual size, we record it here.\n+  size_t _plab_actual_size;\n+\n+  \/\/ As the plab is used for promotions, this value is incremented. When the plab is\n+  \/\/ retired, the difference between 'actual_size' and 'promoted' will be returned to\n+  \/\/ the old generation's promotion reserve (i.e., it will be 'unexpended').\n+  size_t _plab_promoted;\n+\n+  \/\/ If false, no more promotion by this thread during this evacuation phase.\n+  bool   _plab_allows_promotion;\n+\n+  \/\/ If true, evacuations may attempt to allocate a smaller plab if the original size fails.\n+  bool   _plab_retries_enabled;\n+\n+  ShenandoahEvacuationStats* _evacuation_stats;\n+\n+  ShenandoahThreadLocalData();\n+  ~ShenandoahThreadLocalData();\n@@ -92,1 +117,0 @@\n-    assert(thread->is_Java_thread(), \"GC state is only synchronized to java threads\");\n@@ -96,0 +120,19 @@\n+  static bool is_gc_state(Thread* thread, ShenandoahHeap::GCState state) {\n+    return (gc_state(thread) & state) != 0;\n+  }\n+\n+  static bool is_gc_state(ShenandoahHeap::GCState state) {\n+    return is_gc_state(Thread::current(), state);\n+  }\n+\n+  static void set_card_table(Thread* thread, CardTable::CardValue* ct) {\n+    assert(ct != nullptr, \"trying to set thread local card_table pointer to nullptr.\");\n+    data(thread)->_card_table = ct;\n+  }\n+\n+  static CardTable::CardValue* card_table(Thread* thread) {\n+    CardTable::CardValue* ct = data(thread)->_card_table;\n+    assert(ct != nullptr, \"returning a null thread local card_table pointer.\");\n+    return ct;\n+  }\n+\n@@ -97,1 +140,0 @@\n-    assert (thread->is_Java_thread() || thread->is_Worker_thread(), \"Only Java and GC worker threads are allowed to get GCLABs\");\n@@ -101,0 +143,5 @@\n+\n+    if (ShenandoahHeap::heap()->mode()->is_generational()) {\n+      data(thread)->_plab = new PLAB(align_up(PLAB::min_size(), CardTable::card_size_in_words()));\n+      data(thread)->_plab_desired_size = 0;\n+    }\n@@ -115,0 +162,78 @@\n+  static void begin_evacuation(Thread* thread, size_t bytes) {\n+    data(thread)->_evacuation_stats->begin_evacuation(bytes);\n+  }\n+\n+  static void end_evacuation(Thread* thread, size_t bytes) {\n+    data(thread)->_evacuation_stats->end_evacuation(bytes);\n+  }\n+\n+  static void record_age(Thread* thread, size_t bytes, uint age) {\n+    data(thread)->_evacuation_stats->record_age(bytes, age);\n+  }\n+\n+  static ShenandoahEvacuationStats* evacuation_stats(Thread* thread) {\n+    shenandoah_assert_generational();\n+    return data(thread)->_evacuation_stats;\n+  }\n+\n+  static PLAB* plab(Thread* thread) {\n+    return data(thread)->_plab;\n+  }\n+\n+  static size_t plab_size(Thread* thread) {\n+    return data(thread)->_plab_desired_size;\n+  }\n+\n+  static void set_plab_size(Thread* thread, size_t v) {\n+    data(thread)->_plab_desired_size = v;\n+  }\n+\n+  static void enable_plab_retries(Thread* thread) {\n+    data(thread)->_plab_retries_enabled = true;\n+  }\n+\n+  static void disable_plab_retries(Thread* thread) {\n+    data(thread)->_plab_retries_enabled = false;\n+  }\n+\n+  static bool plab_retries_enabled(Thread* thread) {\n+    return data(thread)->_plab_retries_enabled;\n+  }\n+\n+  static void enable_plab_promotions(Thread* thread) {\n+    data(thread)->_plab_allows_promotion = true;\n+  }\n+\n+  static void disable_plab_promotions(Thread* thread) {\n+    data(thread)->_plab_allows_promotion = false;\n+  }\n+\n+  static bool allow_plab_promotions(Thread* thread) {\n+    return data(thread)->_plab_allows_promotion;\n+  }\n+\n+  static void reset_plab_promoted(Thread* thread) {\n+    data(thread)->_plab_promoted = 0;\n+  }\n+\n+  static void add_to_plab_promoted(Thread* thread, size_t increment) {\n+    data(thread)->_plab_promoted += increment;\n+  }\n+\n+  static void subtract_from_plab_promoted(Thread* thread, size_t increment) {\n+    assert(data(thread)->_plab_promoted >= increment, \"Cannot subtract more than remaining promoted\");\n+    data(thread)->_plab_promoted -= increment;\n+  }\n+\n+  static size_t get_plab_promoted(Thread* thread) {\n+    return data(thread)->_plab_promoted;\n+  }\n+\n+  static void set_plab_actual_size(Thread* thread, size_t value) {\n+    data(thread)->_plab_actual_size = value;\n+  }\n+\n+  static size_t get_plab_actual_size(Thread* thread) {\n+    return data(thread)->_plab_actual_size;\n+  }\n+\n@@ -176,0 +301,4 @@\n+\n+  static ByteSize card_table_offset() {\n+    return Thread::gc_data_offset() + byte_offset_of(ShenandoahThreadLocalData, _card_table);\n+  }\n","filename":"src\/hotspot\/share\/gc\/shenandoah\/shenandoahThreadLocalData.hpp","additions":145,"deletions":16,"binary":false,"changes":161,"status":"modified"},{"patch":"@@ -0,0 +1,54 @@\n+\/*\n+ * Copyright Amazon.com Inc. or its affiliates. All Rights Reserved.\n+ * DO NOT ALTER OR REMOVE COPYRIGHT NOTICES OR THIS FILE HEADER.\n+ *\n+ * This code is free software; you can redistribute it and\/or modify it\n+ * under the terms of the GNU General Public License version 2 only, as\n+ * published by the Free Software Foundation.\n+ *\n+ * This code is distributed in the hope that it will be useful, but WITHOUT\n+ * ANY WARRANTY; without even the implied warranty of MERCHANTABILITY or\n+ * FITNESS FOR A PARTICULAR PURPOSE.  See the GNU General Public License\n+ * version 2 for more details (a copy is included in the LICENSE file that\n+ * accompanied this code).\n+ *\n+ * You should have received a copy of the GNU General Public License version\n+ * 2 along with this work; if not, write to the Free Software Foundation,\n+ * Inc., 51 Franklin St, Fifth Floor, Boston, MA 02110-1301 USA.\n+ *\n+ * Please contact Oracle, 500 Oracle Parkway, Redwood Shores, CA 94065 USA\n+ * or visit www.oracle.com if you need additional information or have any\n+ * questions.\n+ *\n+ *\/\n+\n+#include \"precompiled.hpp\"\n+#include \"gc\/shenandoah\/shenandoahEvacInfo.hpp\"\n+#include \"gc\/shenandoah\/shenandoahTrace.hpp\"\n+#include \"jfr\/jfrEvents.hpp\"\n+\n+void ShenandoahTracer::report_evacuation_info(ShenandoahEvacuationInformation* info) {\n+  send_evacuation_info_event(info);\n+}\n+\n+void ShenandoahTracer::send_evacuation_info_event(ShenandoahEvacuationInformation* info) {\n+  EventShenandoahEvacuationInformation e;\n+  if (e.should_commit()) {\n+    e.set_gcId(GCId::current());\n+    e.set_cSetRegions(info->collection_set_regions());\n+    e.set_cSetUsedBefore(info->collection_set_used_before());\n+    e.set_cSetUsedAfter(info->collection_set_used_after());\n+    e.set_collectedOld(info->collected_old());\n+    e.set_collectedPromoted(info->collected_promoted());\n+    e.set_collectedYoung(info->collected_young());\n+    e.set_regionsPromotedHumongous(info->regions_promoted_humongous());\n+    e.set_regionsPromotedRegular(info->regions_promoted_regular());\n+    e.set_regularPromotedGarbage(info->regular_promoted_garbage());\n+    e.set_regularPromotedFree(info->regular_promoted_free());\n+    e.set_freeRegions(info->free_regions());\n+    e.set_regionsImmediate(info->regions_immediate());\n+    e.set_immediateBytes(info->immediate_size());\n+\n+    e.commit();\n+  }\n+}\n","filename":"src\/hotspot\/share\/gc\/shenandoah\/shenandoahTrace.cpp","additions":54,"deletions":0,"binary":false,"changes":54,"status":"added"},{"patch":"@@ -0,0 +1,42 @@\n+\/*\n+ * Copyright Amazon.com Inc. or its affiliates. All Rights Reserved.\n+ * DO NOT ALTER OR REMOVE COPYRIGHT NOTICES OR THIS FILE HEADER.\n+ *\n+ * This code is free software; you can redistribute it and\/or modify it\n+ * under the terms of the GNU General Public License version 2 only, as\n+ * published by the Free Software Foundation.\n+ *\n+ * This code is distributed in the hope that it will be useful, but WITHOUT\n+ * ANY WARRANTY; without even the implied warranty of MERCHANTABILITY or\n+ * FITNESS FOR A PARTICULAR PURPOSE.  See the GNU General Public License\n+ * version 2 for more details (a copy is included in the LICENSE file that\n+ * accompanied this code).\n+ *\n+ * You should have received a copy of the GNU General Public License version\n+ * 2 along with this work; if not, write to the Free Software Foundation,\n+ * Inc., 51 Franklin St, Fifth Floor, Boston, MA 02110-1301 USA.\n+ *\n+ * Please contact Oracle, 500 Oracle Parkway, Redwood Shores, CA 94065 USA\n+ * or visit www.oracle.com if you need additional information or have any\n+ * questions.\n+ *\n+ *\/\n+\n+#ifndef SHARE_GC_SHENANDOAH_SHENANDOAHTRACE_HPP\n+#define SHARE_GC_SHENANDOAH_SHENANDOAHTRACE_HPP\n+\n+#include \"gc\/shared\/gcTrace.hpp\"\n+#include \"memory\/allocation.hpp\"\n+\n+class ShenandoahEvacuationInformation;\n+\n+class ShenandoahTracer : public GCTracer, public CHeapObj<mtGC> {\n+public:\n+  ShenandoahTracer() : GCTracer(Shenandoah) {}\n+  void report_evacuation_info(ShenandoahEvacuationInformation* info);\n+\n+private:\n+  void send_evacuation_info_event(ShenandoahEvacuationInformation* info);\n+};\n+\n+#endif\n","filename":"src\/hotspot\/share\/gc\/shenandoah\/shenandoahTrace.hpp","additions":42,"deletions":0,"binary":false,"changes":42,"status":"added"},{"patch":"@@ -0,0 +1,215 @@\n+\/*\n+ * Copyright Amazon.com Inc. or its affiliates. All Rights Reserved.\n+ * DO NOT ALTER OR REMOVE COPYRIGHT NOTICES OR THIS FILE HEADER.\n+ *\n+ * This code is free software; you can redistribute it and\/or modify it\n+ * under the terms of the GNU General Public License version 2 only, as\n+ * published by the Free Software Foundation.\n+ *\n+ * This code is distributed in the hope that it will be useful, but WITHOUT\n+ * ANY WARRANTY; without even the implied warranty of MERCHANTABILITY or\n+ * FITNESS FOR A PARTICULAR PURPOSE.  See the GNU General Public License\n+ * version 2 for more details (a copy is included in the LICENSE file that\n+ * accompanied this code).\n+ *\n+ * You should have received a copy of the GNU General Public License version\n+ * 2 along with this work; if not, write to the Free Software Foundation,\n+ * Inc., 51 Franklin St, Fifth Floor, Boston, MA 02110-1301 USA.\n+ *\n+ * Please contact Oracle, 500 Oracle Parkway, Redwood Shores, CA 94065 USA\n+ * or visit www.oracle.com if you need additional information or have any\n+ * questions.\n+ *\n+ *\/\n+\n+#include \"precompiled.hpp\"\n+#include \"gc\/shenandoah\/shenandoahHeap.inline.hpp\"\n+#include \"gc\/shenandoah\/shenandoahHeapRegion.hpp\"\n+#include \"gc\/shenandoah\/shenandoahUncommitThread.hpp\"\n+#include \"logging\/log.hpp\"\n+#include \"runtime\/mutexLocker.hpp\"\n+#include \"utilities\/events.hpp\"\n+\n+ShenandoahUncommitThread::ShenandoahUncommitThread(ShenandoahHeap* heap)\n+  : _heap(heap),\n+    _uncommit_lock(Mutex::safepoint - 2, \"ShenandoahUncommit_lock\", true) {\n+  set_name(\"Shenandoah Uncommit Thread\");\n+  create_and_start();\n+\n+  \/\/ Allow uncommits. This is managed by the control thread during a GC.\n+  _uncommit_allowed.set();\n+}\n+\n+void ShenandoahUncommitThread::run_service() {\n+  assert(ShenandoahUncommit, \"Thread should only run when uncommit is enabled\");\n+\n+  \/\/ poll_interval avoids constantly polling regions for shrinking.\n+  \/\/ Having an interval 10x lower than the delay would mean we hit the\n+  \/\/ shrinking with lag of less than 1\/10-th of true delay.\n+  \/\/ ShenandoahUncommitDelay is in millis, but shrink_period is in seconds.\n+  const int64_t poll_interval = int64_t(ShenandoahUncommitDelay) \/ 10;\n+  const double shrink_period = double(ShenandoahUncommitDelay) \/ 1000;\n+  bool timed_out = false;\n+  while (!should_terminate()) {\n+    bool soft_max_changed = _soft_max_changed.try_unset();\n+    bool explicit_gc_requested = _explicit_gc_requested.try_unset();\n+\n+    if (soft_max_changed || explicit_gc_requested || timed_out) {\n+      double current = os::elapsedTime();\n+      size_t shrink_until = soft_max_changed ? _heap->soft_max_capacity() : _heap->min_capacity();\n+      double shrink_before = (soft_max_changed || explicit_gc_requested) ?\n+              current :\n+              current - shrink_period;\n+\n+      \/\/ Explicit GC tries to uncommit everything down to min capacity.\n+      \/\/ Soft max change tries to uncommit everything down to target capacity.\n+      \/\/ Periodic uncommit tries to uncommit suitable regions down to min capacity.\n+      if (should_uncommit(shrink_before, shrink_until)) {\n+        uncommit(shrink_before, shrink_until);\n+      }\n+    }\n+\n+    if (!should_terminate()) {\n+      MonitorLocker locker(&_uncommit_lock, Mutex::_no_safepoint_check_flag);\n+      timed_out = locker.wait(poll_interval);\n+    }\n+  }\n+}\n+\n+bool ShenandoahUncommitThread::should_uncommit(double shrink_before, size_t shrink_until) const {\n+  \/\/ Only start uncommit if the GC is idle, is not trying to run and there is work to do.\n+  return _heap->is_idle() && is_uncommit_allowed() && has_work(shrink_before, shrink_until);\n+}\n+\n+bool ShenandoahUncommitThread::has_work(double shrink_before, size_t shrink_until) const {\n+  \/\/ Determine if there is work to do. This avoids locking the heap if there is\n+  \/\/ no work available, avoids spamming logs with superfluous logging messages,\n+  \/\/ and minimises the amount of work while locks are held.\n+\n+  if (_heap->committed() <= shrink_until) {\n+    return false;\n+  }\n+\n+  for (size_t i = 0; i < _heap->num_regions(); i++) {\n+    ShenandoahHeapRegion *r = _heap->get_region(i);\n+    if (r->is_empty_committed() && (r->empty_time() < shrink_before)) {\n+      return true;\n+    }\n+  }\n+\n+  return false;\n+}\n+\n+void ShenandoahUncommitThread::notify_soft_max_changed() {\n+  assert(is_uncommit_allowed(), \"Only notify if uncommit is allowed\");\n+  if (_soft_max_changed.try_set()) {\n+    MonitorLocker locker(&_uncommit_lock, Mutex::_no_safepoint_check_flag);\n+    locker.notify_all();\n+  }\n+}\n+\n+void ShenandoahUncommitThread::notify_explicit_gc_requested() {\n+  assert(is_uncommit_allowed(), \"Only notify if uncommit is allowed\");\n+  if (_explicit_gc_requested.try_set()) {\n+    MonitorLocker locker(&_uncommit_lock, Mutex::_no_safepoint_check_flag);\n+    locker.notify_all();\n+  }\n+}\n+\n+bool ShenandoahUncommitThread::is_uncommit_allowed() const {\n+  return _uncommit_allowed.is_set();\n+}\n+\n+void ShenandoahUncommitThread::uncommit(double shrink_before, size_t shrink_until) {\n+  assert(ShenandoahUncommit, \"should be enabled\");\n+  assert(_uncommit_in_progress.is_unset(), \"Uncommit should not be in progress\");\n+\n+  {\n+    \/\/ Final check, under the lock, if uncommit is allowed.\n+    MonitorLocker locker(&_uncommit_lock, Mutex::_no_safepoint_check_flag);\n+    if (is_uncommit_allowed()) {\n+      _uncommit_in_progress.set();\n+    }\n+  }\n+\n+  \/\/ If not allowed to start, do nothing.\n+  if (!_uncommit_in_progress.is_set()) {\n+    return;\n+  }\n+\n+  \/\/ From here on, uncommit is in progress. Attempts to stop the uncommit must wait\n+  \/\/ until the cancellation request is acknowledged and uncommit is no longer in progress.\n+  const char* msg = \"Concurrent uncommit\";\n+  const double start = os::elapsedTime();\n+  EventMark em(\"%s\", msg);\n+  log_info(gc, start)(\"%s\", msg);\n+\n+  \/\/ This is the number of regions uncommitted during this increment of uncommit work.\n+  const size_t uncommitted_region_count = do_uncommit_work(shrink_before, shrink_until);\n+\n+  {\n+    MonitorLocker locker(&_uncommit_lock, Mutex::_no_safepoint_check_flag);\n+    _uncommit_in_progress.unset();\n+    locker.notify_all();\n+  }\n+\n+  if (uncommitted_region_count > 0) {\n+    _heap->notify_heap_changed();\n+  }\n+\n+  const double elapsed = os::elapsedTime() - start;\n+  log_info(gc)(\"%s \" PROPERFMT \" (\" PROPERFMT \") %.3fms\",\n+               msg, PROPERFMTARGS(uncommitted_region_count * ShenandoahHeapRegion::region_size_bytes()), PROPERFMTARGS(_heap->capacity()),\n+               elapsed * MILLIUNITS);\n+}\n+\n+size_t ShenandoahUncommitThread::do_uncommit_work(double shrink_before, size_t shrink_until) const {\n+  size_t count = 0;\n+  \/\/ Application allocates from the beginning of the heap, and GC allocates at\n+  \/\/ the end of it. It is more efficient to uncommit from the end, so that applications\n+  \/\/ could enjoy the near committed regions. GC allocations are much less frequent,\n+  \/\/ and therefore can accept the committing costs.\n+  for (size_t i = _heap->num_regions(); i > 0; i--) {\n+    if (!is_uncommit_allowed()) {\n+      \/\/ GC wants to start, so the uncommit operation must stop\n+      break;\n+    }\n+\n+    ShenandoahHeapRegion* r = _heap->get_region(i - 1);\n+    if (r->is_empty_committed() && (r->empty_time() < shrink_before)) {\n+      SuspendibleThreadSetJoiner sts_joiner;\n+      ShenandoahHeapLocker heap_locker(_heap->lock());\n+      if (r->is_empty_committed()) {\n+        if (_heap->committed() < shrink_until + ShenandoahHeapRegion::region_size_bytes()) {\n+          \/\/ We have uncommitted enough regions to hit the target heap committed size\n+          break;\n+        }\n+\n+        r->make_uncommitted();\n+        count++;\n+      }\n+    }\n+    SpinPause(); \/\/ allow allocators to take the lock\n+  }\n+  return count;\n+}\n+\n+\n+void ShenandoahUncommitThread::stop_service() {\n+  MonitorLocker locker(&_uncommit_lock, Mutex::_safepoint_check_flag);\n+  _uncommit_allowed.unset();\n+  locker.notify_all();\n+}\n+\n+void ShenandoahUncommitThread::forbid_uncommit() {\n+  MonitorLocker locker(&_uncommit_lock, Mutex::_no_safepoint_check_flag);\n+  _uncommit_allowed.unset();\n+  while (_uncommit_in_progress.is_set()) {\n+    locker.wait();\n+  }\n+}\n+\n+void ShenandoahUncommitThread::allow_uncommit() {\n+  MonitorLocker locker(&_uncommit_lock, Mutex::_no_safepoint_check_flag);\n+  _uncommit_allowed.set();\n+}\n","filename":"src\/hotspot\/share\/gc\/shenandoah\/shenandoahUncommitThread.cpp","additions":215,"deletions":0,"binary":false,"changes":215,"status":"added"},{"patch":"@@ -0,0 +1,97 @@\n+\/*\n+ * Copyright Amazon.com Inc. or its affiliates. All Rights Reserved.\n+ * DO NOT ALTER OR REMOVE COPYRIGHT NOTICES OR THIS FILE HEADER.\n+ *\n+ * This code is free software; you can redistribute it and\/or modify it\n+ * under the terms of the GNU General Public License version 2 only, as\n+ * published by the Free Software Foundation.\n+ *\n+ * This code is distributed in the hope that it will be useful, but WITHOUT\n+ * ANY WARRANTY; without even the implied warranty of MERCHANTABILITY or\n+ * FITNESS FOR A PARTICULAR PURPOSE.  See the GNU General Public License\n+ * version 2 for more details (a copy is included in the LICENSE file that\n+ * accompanied this code).\n+ *\n+ * You should have received a copy of the GNU General Public License version\n+ * 2 along with this work; if not, write to the Free Software Foundation,\n+ * Inc., 51 Franklin St, Fifth Floor, Boston, MA 02110-1301 USA.\n+ *\n+ * Please contact Oracle, 500 Oracle Parkway, Redwood Shores, CA 94065 USA\n+ * or visit www.oracle.com if you need additional information or have any\n+ * questions.\n+ *\n+ *\/\n+\n+#ifndef SHARE_GC_SHENANDOAH_SHENANDOAHUNCOMMITTHREAD\n+#define SHARE_GC_SHENANDOAH_SHENANDOAHUNCOMMITTHREAD\n+\n+#include \"gc\/shared\/concurrentGCThread.hpp\"\n+\n+class ShenandoahHeap;\n+\n+class ShenandoahUncommitThread : public ConcurrentGCThread {\n+  ShenandoahHeap* const _heap;\n+\n+  \/\/ Indicates that `SoftMaxHeapSize` has changed\n+  ShenandoahSharedFlag _soft_max_changed;\n+\n+  \/\/ Indicates that an explicit gc has been requested\n+  ShenandoahSharedFlag _explicit_gc_requested;\n+\n+  \/\/ Indicates whether it is safe to uncommit regions\n+  ShenandoahSharedFlag _uncommit_allowed;\n+\n+  \/\/ Indicates that regions are being actively uncommitted\n+  ShenandoahSharedFlag _uncommit_in_progress;\n+\n+  \/\/ This lock is used to coordinate allowing or forbidding regions to be uncommitted\n+  Monitor _uncommit_lock;\n+\n+  \/\/ True if there are regions to uncommit and uncommits are allowed\n+  bool should_uncommit(double shrink_before, size_t shrink_until) const;\n+\n+  \/\/ True if there are regions that have been empty for longer than ShenandoahUncommitDelay and the committed\n+  \/\/ memory is higher than soft max capacity or minimum capacity\n+  bool has_work(double shrink_before, size_t shrink_until) const;\n+\n+  \/\/ Perform the work of uncommitting empty regions\n+  void uncommit(double shrink_before, size_t shrink_until);\n+\n+  \/\/ True if the control thread has allowed this thread to uncommit regions\n+  bool is_uncommit_allowed() const;\n+\n+  \/\/ Iterate over and uncommit eligible regions until committed heap falls below\n+  \/\/ `shrink_until` bytes. A region is eligible for uncommit if the timestamp at which\n+  \/\/ it was last made empty is before `shrink_before` seconds since jvm start.\n+  \/\/ Returns the number of regions uncommitted. May be interrupted by `forbid_uncommit`.\n+  size_t do_uncommit_work(double shrink_before, size_t shrink_until) const;\n+\n+public:\n+  explicit ShenandoahUncommitThread(ShenandoahHeap* heap);\n+\n+  \/\/ Periodically check for regions to uncommit\n+  void run_service() override;\n+\n+  \/\/ Wake up this thread and try to uncommit for changed soft max size\n+  void notify_soft_max_changed();\n+\n+  \/\/ Wake up this thread and try to uncommit for min heap size\n+  void notify_explicit_gc_requested();\n+\n+  \/\/ Wait for uncommit operations to stop, returns immediately if uncommit thread is idle\n+  void forbid_uncommit();\n+\n+  \/\/ Allows uncommit operations to happen, does not block\n+  void allow_uncommit();\n+\n+  \/\/ True if uncommit is in progress\n+  bool is_uncommit_in_progress() const {\n+    return _uncommit_in_progress.is_set();\n+  }\n+protected:\n+  \/\/ Interrupt and stop this thread\n+  void stop_service() override;\n+};\n+\n+\n+#endif \/\/SHARE_GC_SHENANDOAH_SHENANDOAHUNCOMMITTHREAD\n","filename":"src\/hotspot\/share\/gc\/shenandoah\/shenandoahUncommitThread.hpp","additions":97,"deletions":0,"binary":false,"changes":97,"status":"added"},{"patch":"@@ -53,1 +53,1 @@\n-    _marking_context(ShenandoahHeap::heap()->complete_marking_context()),\n+    _marking_context(ShenandoahHeap::heap()->global_generation()->complete_marking_context()),\n","filename":"src\/hotspot\/share\/gc\/shenandoah\/shenandoahUnload.cpp","additions":1,"deletions":1,"binary":false,"changes":2,"status":"modified"},{"patch":"@@ -3,0 +3,1 @@\n+ * Copyright Amazon.com Inc. or its affiliates. All Rights Reserved.\n@@ -35,0 +36,1 @@\n+#include \"gc\/shenandoah\/shenandoahOldGeneration.hpp\"\n@@ -37,0 +39,1 @@\n+#include \"gc\/shenandoah\/shenandoahYoungGeneration.hpp\"\n@@ -41,1 +44,1 @@\n-ShenandoahGCSession::ShenandoahGCSession(GCCause::Cause cause) :\n+ShenandoahGCSession::ShenandoahGCSession(GCCause::Cause cause, ShenandoahGeneration* generation) :\n@@ -43,0 +46,1 @@\n+  _generation(generation),\n@@ -47,1 +51,2 @@\n-  _heap->set_gc_cause(cause);\n+  _heap->on_cycle_start(cause, _generation);\n+\n@@ -52,2 +57,0 @@\n-  _heap->shenandoah_policy()->record_cycle_start();\n-  _heap->heuristics()->record_cycle_start();\n@@ -68,1 +71,1 @@\n-  _heap->heuristics()->record_cycle_end();\n+  _heap->on_cycle_end(_generation);\n@@ -71,1 +74,1 @@\n-  _tracer->report_gc_reference_stats(_heap->ref_processor()->reference_process_stats());\n+  _tracer->report_gc_reference_stats(_generation->ref_processor()->reference_process_stats());\n@@ -74,1 +77,0 @@\n-  _heap->set_gc_cause(GCCause::_no_gc);\n@@ -114,2 +116,2 @@\n-ShenandoahTimingsTracker::ShenandoahTimingsTracker(ShenandoahPhaseTimings::Phase phase) :\n-  _timings(ShenandoahHeap::heap()->phase_timings()), _phase(phase) {\n+ShenandoahTimingsTracker::ShenandoahTimingsTracker(ShenandoahPhaseTimings::Phase phase, bool should_aggregate) :\n+  _timings(ShenandoahHeap::heap()->phase_timings()), _phase(phase), _should_aggregate(should_aggregate) {\n@@ -124,1 +126,1 @@\n-  _timings->record_phase_time(_phase, os::elapsedTime() - _start);\n+  _timings->record_phase_time(_phase, os::elapsedTime() - _start, _should_aggregate);\n","filename":"src\/hotspot\/share\/gc\/shenandoah\/shenandoahUtils.cpp","additions":12,"deletions":10,"binary":false,"changes":22,"status":"modified"},{"patch":"@@ -3,0 +3,1 @@\n+ * Copyright Amazon.com Inc. or its affiliates. All Rights Reserved.\n@@ -44,0 +45,16 @@\n+class ShenandoahGeneration;\n+\n+#define SHENANDOAH_RETURN_EVENT_MESSAGE(generation_type, prefix, postfix) \\\n+  switch (generation_type) {                                              \\\n+    case NON_GEN:                                                         \\\n+      return prefix postfix;                                              \\\n+    case GLOBAL:                                                          \\\n+      return prefix \" (Global)\" postfix;                                  \\\n+    case YOUNG:                                                           \\\n+      return prefix \" (Young)\" postfix;                                   \\\n+    case OLD:                                                             \\\n+      return prefix \" (Old)\" postfix;                                     \\\n+    default:                                                              \\\n+      ShouldNotReachHere();                                               \\\n+      return prefix \" (Unknown)\" postfix;                                 \\\n+  }                                                                       \\\n@@ -48,0 +65,1 @@\n+  ShenandoahGeneration* const _generation;\n@@ -53,1 +71,1 @@\n-  ShenandoahGCSession(GCCause::Cause cause);\n+  ShenandoahGCSession(GCCause::Cause cause, ShenandoahGeneration* generation);\n@@ -67,0 +85,1 @@\n+  const bool                            _should_aggregate;\n@@ -71,1 +90,1 @@\n-  ShenandoahTimingsTracker(ShenandoahPhaseTimings::Phase phase);\n+  ShenandoahTimingsTracker(ShenandoahPhaseTimings::Phase phase, bool should_aggregate = false);\n@@ -226,0 +245,15 @@\n+\/\/ Regions cannot be uncommitted when concurrent reset is zeroing out the bitmaps.\n+\/\/ This CADR class enforces this by forbidding region uncommits while it is in scope.\n+class ShenandoahNoUncommitMark : public StackObj {\n+  ShenandoahHeap* const _heap;\n+public:\n+  explicit ShenandoahNoUncommitMark(ShenandoahHeap* heap) : _heap(heap) {\n+    _heap->forbid_uncommit();\n+  }\n+\n+  ~ShenandoahNoUncommitMark() {\n+    _heap->allow_uncommit();\n+  }\n+};\n+\n+\n","filename":"src\/hotspot\/share\/gc\/shenandoah\/shenandoahUtils.hpp","additions":36,"deletions":2,"binary":false,"changes":38,"status":"modified"},{"patch":"@@ -30,0 +30,1 @@\n+#include \"gc\/shenandoah\/shenandoahGeneration.hpp\"\n@@ -36,0 +37,1 @@\n+#include \"logging\/log.hpp\"\n@@ -39,0 +41,1 @@\n+  log_active_generation(\"Prologue\");\n@@ -44,0 +47,1 @@\n+  log_active_generation(\"Epilogue\");\n@@ -50,0 +54,15 @@\n+void VM_ShenandoahOperation::log_active_generation(const char* prefix) {\n+  ShenandoahGeneration* agen = ShenandoahHeap::heap()->active_generation();\n+  ShenandoahGeneration* ggen = ShenandoahHeap::heap()->gc_generation();\n+  log_debug(gc, heap)(\"%s: active_generation is %s, gc_generation is %s\", prefix,\n+                      agen == nullptr ? \"nullptr\" : shenandoah_generation_name(agen->type()),\n+                      ggen == nullptr ? \"nullptr\" : shenandoah_generation_name(ggen->type()));\n+}\n+\n+void VM_ShenandoahOperation::set_active_generation() {\n+  if (evaluate_at_safepoint()) {\n+    assert(SafepointSynchronize::is_at_safepoint(), \"Error??\");\n+    ShenandoahHeap::heap()->set_active_generation();\n+  }\n+}\n+\n@@ -66,0 +85,1 @@\n+  set_active_generation();\n@@ -67,1 +87,0 @@\n-  ShenandoahHeap::heap()->propagate_gc_state_to_java_threads();\n@@ -72,0 +91,1 @@\n+  set_active_generation();\n@@ -73,1 +93,0 @@\n-  ShenandoahHeap::heap()->propagate_gc_state_to_java_threads();\n@@ -78,0 +97,1 @@\n+  set_active_generation();\n@@ -79,1 +99,0 @@\n-  ShenandoahHeap::heap()->propagate_gc_state_to_java_threads();\n@@ -84,0 +103,1 @@\n+  set_active_generation();\n@@ -85,1 +105,0 @@\n-  ShenandoahHeap::heap()->propagate_gc_state_to_java_threads();\n@@ -90,2 +109,2 @@\n-  _gc->entry_init_updaterefs();\n-  ShenandoahHeap::heap()->propagate_gc_state_to_java_threads();\n+  set_active_generation();\n+  _gc->entry_init_update_refs();\n@@ -96,2 +115,2 @@\n-  _gc->entry_final_updaterefs();\n-  ShenandoahHeap::heap()->propagate_gc_state_to_java_threads();\n+  set_active_generation();\n+  _gc->entry_final_update_refs();\n@@ -102,2 +121,2 @@\n-  _gc->entry_final_roots();\n-  ShenandoahHeap::heap()->propagate_gc_state_to_java_threads();\n+  set_active_generation();\n+  _gc->entry_verify_final_roots();\n","filename":"src\/hotspot\/share\/gc\/shenandoah\/shenandoahVMOperations.cpp","additions":29,"deletions":10,"binary":false,"changes":39,"status":"modified"},{"patch":"@@ -41,0 +41,1 @@\n+\/\/   - VM_ShenandoahFinalRoots: finish up roots on a non-evacuating cycle\n@@ -47,1 +48,3 @@\n-  uint         _gc_id;\n+  uint _gc_id;\n+\n+  void set_active_generation();\n@@ -51,0 +54,2 @@\n+\n+  void log_active_generation(const char* prefix);\n","filename":"src\/hotspot\/share\/gc\/shenandoah\/shenandoahVMOperations.hpp","additions":6,"deletions":1,"binary":false,"changes":7,"status":"modified"},{"patch":"@@ -3,0 +3,1 @@\n+ * Copyright Amazon.com Inc. or its affiliates. All Rights Reserved.\n@@ -30,0 +31,1 @@\n+#include \"gc\/shenandoah\/shenandoahGeneration.hpp\"\n@@ -32,0 +34,1 @@\n+#include \"gc\/shenandoah\/shenandoahOldGeneration.hpp\"\n@@ -33,0 +36,1 @@\n+#include \"gc\/shenandoah\/shenandoahScanRemembered.inline.hpp\"\n@@ -36,0 +40,1 @@\n+#include \"gc\/shenandoah\/shenandoahYoungGeneration.hpp\"\n@@ -65,0 +70,1 @@\n+  ShenandoahGeneration* _generation;\n@@ -76,1 +82,2 @@\n-    _loc(nullptr) {\n+    _loc(nullptr),\n+    _generation(nullptr) {\n@@ -78,0 +85,1 @@\n+        options._verify_marked == ShenandoahVerifier::_verify_marked_complete_satb_empty ||\n@@ -87,0 +95,6 @@\n+\n+    if (_heap->mode()->is_generational()) {\n+      _generation = _heap->gc_generation();\n+      assert(_generation != nullptr, \"Expected active generation in this mode\");\n+      shenandoah_assert_generations_reconciled();\n+    }\n@@ -113,2 +127,1 @@\n-\n-      if (_map->par_mark(obj)) {\n+      if (in_generation(obj) && _map->par_mark(obj)) {\n@@ -121,0 +134,9 @@\n+  bool in_generation(oop obj) {\n+    if (_generation == nullptr) {\n+      return true;\n+    }\n+\n+    ShenandoahHeapRegion* region = _heap->heap_region_containing(obj);\n+    return _generation->contains(region);\n+  }\n+\n@@ -171,1 +193,2 @@\n-          check(ShenandoahAsserts::_safe_oop, obj, obj_reg->has_live(),\n+          check(ShenandoahAsserts::_safe_oop, obj, obj_reg->has_live() ||\n+                (obj_reg->is_old() && _heap->gc_generation()->is_young()),\n@@ -173,0 +196,1 @@\n+          shenandoah_assert_generations_reconciled();\n@@ -222,1 +246,14 @@\n-    \/\/ ------------ obj and fwd are safe at this point --------------\n+    \/\/ Do additional checks for special objects: their fields can hold metadata as well.\n+    \/\/ We want to check class loading\/unloading did not corrupt them.\n+\n+    if (obj_klass == vmClasses::Class_klass()) {\n+      Metadata* klass = obj->metadata_field(java_lang_Class::klass_offset());\n+      check(ShenandoahAsserts::_safe_oop, obj,\n+            klass == nullptr || Metaspace::contains(klass),\n+            \"Instance class mirror should point to Metaspace\");\n+\n+      Metadata* array_klass = obj->metadata_field(java_lang_Class::array_klass_offset());\n+      check(ShenandoahAsserts::_safe_oop, obj,\n+            array_klass == nullptr || Metaspace::contains(array_klass),\n+            \"Array class mirror should point to Metaspace\");\n+    }\n@@ -224,0 +261,1 @@\n+    \/\/ ------------ obj and fwd are safe at this point --------------\n@@ -233,1 +271,1 @@\n-        check(ShenandoahAsserts::_safe_all, obj, _heap->complete_marking_context()->is_marked(obj),\n+        check(ShenandoahAsserts::_safe_all, obj, _heap->gc_generation()->complete_marking_context()->is_marked(obj),\n@@ -237,1 +275,2 @@\n-        check(ShenandoahAsserts::_safe_all, obj, _heap->complete_marking_context()->is_marked(obj),\n+      case ShenandoahVerifier::_verify_marked_complete_satb_empty:\n+        check(ShenandoahAsserts::_safe_all, obj, _heap->gc_generation()->complete_marking_context()->is_marked(obj),\n@@ -319,2 +358,2 @@\n-  virtual void do_oop(oop* p) override { do_oop_work(p); }\n-  virtual void do_oop(narrowOop* p) override { do_oop_work(p); }\n+  void do_oop(oop* p) override { do_oop_work(p); }\n+  void do_oop(narrowOop* p) override { do_oop_work(p); }\n@@ -323,0 +362,2 @@\n+\/\/ This closure computes the amounts of used, committed, and garbage memory and the number of regions contained within\n+\/\/ a subset (e.g. the young generation or old generation) of the total heap.\n@@ -325,1 +366,1 @@\n-  size_t _used, _committed, _garbage;\n+  size_t _used, _committed, _garbage, _regions, _humongous_waste, _trashed_regions;\n@@ -327,1 +368,2 @@\n-  ShenandoahCalculateRegionStatsClosure() : _used(0), _committed(0), _garbage(0) {};\n+  ShenandoahCalculateRegionStatsClosure() :\n+      _used(0), _committed(0), _garbage(0), _regions(0), _humongous_waste(0), _trashed_regions(0) {};\n@@ -329,1 +371,1 @@\n-  void heap_region_do(ShenandoahHeapRegion* r) {\n+  void heap_region_do(ShenandoahHeapRegion* r) override {\n@@ -333,0 +375,9 @@\n+    if (r->is_humongous()) {\n+      _humongous_waste += r->free();\n+    }\n+    if (r->is_trash()) {\n+      _trashed_regions++;\n+    }\n+    _regions++;\n+    log_debug(gc)(\"ShenandoahCalculateRegionStatsClosure: adding \" SIZE_FORMAT \" for %s Region \" SIZE_FORMAT \", yielding: \" SIZE_FORMAT,\n+            r->used(), (r->is_humongous() ? \"humongous\" : \"regular\"), r->index(), _used);\n@@ -335,3 +386,71 @@\n-  size_t used() { return _used; }\n-  size_t committed() { return _committed; }\n-  size_t garbage() { return _garbage; }\n+  size_t used() const { return _used; }\n+  size_t committed() const { return _committed; }\n+  size_t garbage() const { return _garbage; }\n+  size_t regions() const { return _regions; }\n+  size_t waste() const { return _humongous_waste; }\n+\n+  \/\/ span is the total memory affiliated with these stats (some of which is in use and other is available)\n+  size_t span() const { return _regions * ShenandoahHeapRegion::region_size_bytes(); }\n+  size_t non_trashed_span() const { return (_regions - _trashed_regions) * ShenandoahHeapRegion::region_size_bytes(); }\n+};\n+\n+class ShenandoahGenerationStatsClosure : public ShenandoahHeapRegionClosure {\n+ public:\n+  ShenandoahCalculateRegionStatsClosure old;\n+  ShenandoahCalculateRegionStatsClosure young;\n+  ShenandoahCalculateRegionStatsClosure global;\n+\n+  void heap_region_do(ShenandoahHeapRegion* r) override {\n+    switch (r->affiliation()) {\n+      case FREE:\n+        return;\n+      case YOUNG_GENERATION:\n+        young.heap_region_do(r);\n+        global.heap_region_do(r);\n+        break;\n+      case OLD_GENERATION:\n+        old.heap_region_do(r);\n+        global.heap_region_do(r);\n+        break;\n+      default:\n+        ShouldNotReachHere();\n+    }\n+  }\n+\n+  static void log_usage(ShenandoahGeneration* generation, ShenandoahCalculateRegionStatsClosure& stats) {\n+    log_debug(gc)(\"Safepoint verification: %s verified usage: \" SIZE_FORMAT \"%s, recorded usage: \" SIZE_FORMAT \"%s\",\n+                  generation->name(),\n+                  byte_size_in_proper_unit(generation->used()), proper_unit_for_byte_size(generation->used()),\n+                  byte_size_in_proper_unit(stats.used()),       proper_unit_for_byte_size(stats.used()));\n+  }\n+\n+  static void validate_usage(const bool adjust_for_padding,\n+                             const char* label, ShenandoahGeneration* generation, ShenandoahCalculateRegionStatsClosure& stats) {\n+    ShenandoahHeap* heap = ShenandoahHeap::heap();\n+    size_t generation_used = generation->used();\n+    size_t generation_used_regions = generation->used_regions();\n+    if (adjust_for_padding && (generation->is_young() || generation->is_global())) {\n+      size_t pad = heap->old_generation()->get_pad_for_promote_in_place();\n+      generation_used += pad;\n+    }\n+\n+    guarantee(stats.used() == generation_used,\n+              \"%s: generation (%s) used size must be consistent: generation-used: \" PROPERFMT \", regions-used: \" PROPERFMT,\n+              label, generation->name(), PROPERFMTARGS(generation_used), PROPERFMTARGS(stats.used()));\n+\n+    guarantee(stats.regions() == generation_used_regions,\n+              \"%s: generation (%s) used regions (\" SIZE_FORMAT \") must equal regions that are in use (\" SIZE_FORMAT \")\",\n+              label, generation->name(), generation->used_regions(), stats.regions());\n+\n+    size_t generation_capacity = generation->max_capacity();\n+    guarantee(stats.non_trashed_span() <= generation_capacity,\n+              \"%s: generation (%s) size spanned by regions (\" SIZE_FORMAT \") * region size (\" PROPERFMT\n+              \") must not exceed current capacity (\" PROPERFMT \")\",\n+              label, generation->name(), stats.regions(), PROPERFMTARGS(ShenandoahHeapRegion::region_size_bytes()),\n+              PROPERFMTARGS(generation_capacity));\n+\n+    size_t humongous_waste = generation->get_humongous_waste();\n+    guarantee(stats.waste() == humongous_waste,\n+              \"%s: generation (%s) humongous waste must be consistent: generation: \" PROPERFMT \", regions: \" PROPERFMT,\n+              label, generation->name(), PROPERFMTARGS(humongous_waste), PROPERFMTARGS(stats.waste()));\n+  }\n@@ -369,1 +488,1 @@\n-  void heap_region_do(ShenandoahHeapRegion* r) {\n+  void heap_region_do(ShenandoahHeapRegion* r) override {\n@@ -421,2 +540,5 @@\n-    verify(r, r->get_shared_allocs() + r->get_tlab_allocs() + r->get_gclab_allocs() == r->used(),\n-           \"Accurate accounting: shared + TLAB + GCLAB = used\");\n+    verify(r, r->get_plab_allocs() <= r->capacity(),\n+           \"PLAB alloc count should not be larger than capacity\");\n+\n+    verify(r, r->get_shared_allocs() + r->get_tlab_allocs() + r->get_gclab_allocs() + r->get_plab_allocs() == r->used(),\n+           \"Accurate accounting: shared + TLAB + GCLAB + PLAB = used\");\n@@ -454,1 +576,1 @@\n-  size_t processed() {\n+  size_t processed() const {\n@@ -458,1 +580,1 @@\n-  virtual void work(uint worker_id) {\n+  void work(uint worker_id) override {\n@@ -495,0 +617,14 @@\n+class ShenandoahVerifyNoIncompleteSatbBuffers : public ThreadClosure {\n+public:\n+  void do_thread(Thread* thread) override {\n+    SATBMarkQueue& queue = ShenandoahThreadLocalData::satb_mark_queue(thread);\n+    if (!is_empty(queue)) {\n+      fatal(\"All SATB buffers should have been flushed during mark\");\n+    }\n+  }\n+private:\n+  bool is_empty(SATBMarkQueue& queue) {\n+    return queue.buffer() == nullptr || queue.index() == queue.capacity();\n+  }\n+};\n+\n@@ -504,0 +640,1 @@\n+  ShenandoahGeneration* _generation;\n@@ -517,1 +654,8 @@\n-          _processed(0) {};\n+          _processed(0),\n+          _generation(nullptr) {\n+    if (_heap->mode()->is_generational()) {\n+      _generation = _heap->gc_generation();\n+      assert(_generation != nullptr, \"Expected active generation in this mode.\");\n+      shenandoah_assert_generations_reconciled();\n+    }\n+  };\n@@ -523,1 +667,6 @@\n-  virtual void work(uint worker_id) {\n+  void work(uint worker_id) override {\n+    if (_options._verify_marked == ShenandoahVerifier::_verify_marked_complete_satb_empty) {\n+      ShenandoahVerifyNoIncompleteSatbBuffers verify_satb;\n+      Threads::threads_do(&verify_satb);\n+    }\n+\n@@ -533,0 +682,4 @@\n+        if (!in_generation(r)) {\n+          continue;\n+        }\n+\n@@ -544,0 +697,4 @@\n+  bool in_generation(ShenandoahHeapRegion* r) {\n+    return _generation == nullptr || _generation->contains(r);\n+  }\n+\n@@ -547,1 +704,1 @@\n-    if (_heap->complete_marking_context()->is_marked(cast_to_oop(obj))) {\n+    if (_heap->gc_generation()->complete_marking_context()->is_marked(cast_to_oop(obj))) {\n@@ -555,1 +712,1 @@\n-    ShenandoahMarkingContext* ctx = _heap->complete_marking_context();\n+    ShenandoahMarkingContext* ctx = _heap->gc_generation()->complete_marking_context();\n@@ -614,1 +771,1 @@\n-  void do_thread(Thread* t) {\n+  void do_thread(Thread* t) override {\n@@ -616,1 +773,1 @@\n-    if (actual != _expected) {\n+    if (!verify_gc_state(actual, _expected)) {\n@@ -620,0 +777,10 @@\n+\n+  static bool verify_gc_state(char actual, char expected) {\n+    \/\/ Old generation marking is allowed in all states.\n+    if (ShenandoahHeap::heap()->mode()->is_generational()) {\n+      return ((actual & ~(ShenandoahHeap::OLD_MARKING | ShenandoahHeap::MARKING)) == expected);\n+    } else {\n+      assert((actual & ShenandoahHeap::OLD_MARKING) == 0, \"Should not mark old in non-generational mode\");\n+      return (actual == expected);\n+    }\n+  }\n@@ -622,2 +789,4 @@\n-void ShenandoahVerifier::verify_at_safepoint(const char *label,\n-                                             VerifyForwarded forwarded, VerifyMarked marked,\n+void ShenandoahVerifier::verify_at_safepoint(const char* label,\n+                                             VerifyRememberedSet remembered,\n+                                             VerifyForwarded forwarded,\n+                                             VerifyMarked marked,\n@@ -625,1 +794,3 @@\n-                                             VerifyLiveness liveness, VerifyRegions regions,\n+                                             VerifyLiveness liveness,\n+                                             VerifyRegions regions,\n+                                             VerifySize sizeness,\n@@ -630,1 +801,1 @@\n-  ShenandoahHeap::heap()->propagate_gc_state_to_java_threads();\n+  ShenandoahHeap::heap()->propagate_gc_state_to_all_threads();\n@@ -649,1 +820,1 @@\n-      case _verify_gcstate_evacuation:\n+      case _verify_gcstate_updating:\n@@ -651,5 +822,1 @@\n-        expected = ShenandoahHeap::HAS_FORWARDED | ShenandoahHeap::EVACUATION;\n-        if (!_heap->is_stw_gc_in_progress()) {\n-          \/\/ Only concurrent GC sets this.\n-          expected |= ShenandoahHeap::WEAK_ROOTS;\n-        }\n+        expected = ShenandoahHeap::HAS_FORWARDED | ShenandoahHeap::UPDATE_REFS;\n@@ -676,1 +843,7 @@\n-      if (actual != expected) {\n+\n+      bool is_marking = (actual & ShenandoahHeap::MARKING);\n+      bool is_marking_young_or_old = (actual & (ShenandoahHeap::YOUNG_MARKING | ShenandoahHeap::OLD_MARKING));\n+      assert(is_marking == is_marking_young_or_old, \"MARKING iff (YOUNG_MARKING or OLD_MARKING), gc_state is: %x\", actual);\n+\n+      \/\/ Old generation marking is allowed in all states.\n+      if (!VerifyThreadGCState::verify_gc_state(actual, expected)) {\n@@ -694,7 +867,14 @@\n-    size_t heap_used = _heap->used();\n-    guarantee(cl.used() == heap_used,\n-              \"%s: heap used size must be consistent: heap-used = \" SIZE_FORMAT \"%s, regions-used = \" SIZE_FORMAT \"%s\",\n-              label,\n-              byte_size_in_proper_unit(heap_used), proper_unit_for_byte_size(heap_used),\n-              byte_size_in_proper_unit(cl.used()), proper_unit_for_byte_size(cl.used()));\n-\n+    size_t heap_used;\n+    if (_heap->mode()->is_generational() && (sizeness == _verify_size_adjusted_for_padding)) {\n+      \/\/ Prior to evacuation, regular regions that are to be evacuated in place are padded to prevent further allocations\n+      heap_used = _heap->used() + _heap->old_generation()->get_pad_for_promote_in_place();\n+    } else if (sizeness != _verify_size_disable) {\n+      heap_used = _heap->used();\n+    }\n+    if (sizeness != _verify_size_disable) {\n+      guarantee(cl.used() == heap_used,\n+                \"%s: heap used size must be consistent: heap-used = \" SIZE_FORMAT \"%s, regions-used = \" SIZE_FORMAT \"%s\",\n+                label,\n+                byte_size_in_proper_unit(heap_used), proper_unit_for_byte_size(heap_used),\n+                byte_size_in_proper_unit(cl.used()), proper_unit_for_byte_size(cl.used()));\n+    }\n@@ -709,0 +889,55 @@\n+  log_debug(gc)(\"Safepoint verification finished heap usage verification\");\n+\n+  ShenandoahGeneration* generation;\n+  if (_heap->mode()->is_generational()) {\n+    generation = _heap->gc_generation();\n+    guarantee(generation != nullptr, \"Need to know which generation to verify.\");\n+    shenandoah_assert_generations_reconciled();\n+  } else {\n+    generation = nullptr;\n+  }\n+\n+  if (generation != nullptr) {\n+    ShenandoahHeapLocker lock(_heap->lock());\n+\n+    switch (remembered) {\n+      case _verify_remembered_disable:\n+        break;\n+      case _verify_remembered_before_marking:\n+        log_debug(gc)(\"Safepoint verification of remembered set at mark\");\n+        verify_rem_set_before_mark();\n+        break;\n+      case _verify_remembered_before_updating_references:\n+        log_debug(gc)(\"Safepoint verification of remembered set at update ref\");\n+        verify_rem_set_before_update_ref();\n+        break;\n+      case _verify_remembered_after_full_gc:\n+        log_debug(gc)(\"Safepoint verification of remembered set after full gc\");\n+        verify_rem_set_after_full_gc();\n+        break;\n+      default:\n+        fatal(\"Unhandled remembered set verification mode\");\n+    }\n+\n+    ShenandoahGenerationStatsClosure cl;\n+    _heap->heap_region_iterate(&cl);\n+\n+    if (LogTarget(Debug, gc)::is_enabled()) {\n+      ShenandoahGenerationStatsClosure::log_usage(_heap->old_generation(),    cl.old);\n+      ShenandoahGenerationStatsClosure::log_usage(_heap->young_generation(),  cl.young);\n+      ShenandoahGenerationStatsClosure::log_usage(_heap->global_generation(), cl.global);\n+    }\n+    if (sizeness == _verify_size_adjusted_for_padding) {\n+      ShenandoahGenerationStatsClosure::validate_usage(false, label, _heap->old_generation(), cl.old);\n+      ShenandoahGenerationStatsClosure::validate_usage(true, label, _heap->young_generation(), cl.young);\n+      ShenandoahGenerationStatsClosure::validate_usage(true, label, _heap->global_generation(), cl.global);\n+    } else if (sizeness == _verify_size_exact) {\n+      ShenandoahGenerationStatsClosure::validate_usage(false, label, _heap->old_generation(), cl.old);\n+      ShenandoahGenerationStatsClosure::validate_usage(false, label, _heap->young_generation(), cl.young);\n+      ShenandoahGenerationStatsClosure::validate_usage(false, label, _heap->global_generation(), cl.global);\n+    }\n+    \/\/ else: sizeness must equal _verify_size_disable\n+  }\n+\n+  log_debug(gc)(\"Safepoint verification finished remembered set verification\");\n+\n@@ -712,1 +947,5 @@\n-    _heap->heap_region_iterate(&cl);\n+    if (generation != nullptr) {\n+      generation->heap_region_iterate(&cl);\n+    } else {\n+      _heap->heap_region_iterate(&cl);\n+    }\n@@ -715,0 +954,2 @@\n+  log_debug(gc)(\"Safepoint verification finished heap region closure verification\");\n+\n@@ -739,0 +980,2 @@\n+  log_debug(gc)(\"Safepoint verification finished getting initial reachable set\");\n+\n@@ -747,2 +990,5 @@\n-  if (ShenandoahVerifyLevel >= 4 && (marked == _verify_marked_complete || marked == _verify_marked_complete_except_references)) {\n-    guarantee(_heap->marking_context()->is_complete(), \"Marking context should be complete\");\n+  if (ShenandoahVerifyLevel >= 4 &&\n+        (marked == _verify_marked_complete ||\n+         marked == _verify_marked_complete_except_references ||\n+         marked == _verify_marked_complete_satb_empty)) {\n+    guarantee(_heap->gc_generation()->is_mark_complete(), \"Marking context should be complete\");\n@@ -756,0 +1002,2 @@\n+  log_debug(gc)(\"Safepoint verification finished walking marked objects\");\n+\n@@ -762,0 +1010,3 @@\n+      if (generation != nullptr && !generation->contains(r)) {\n+        continue;\n+      }\n@@ -785,0 +1036,3 @@\n+  log_debug(gc)(\"Safepoint verification finished accumulation of liveness data\");\n+\n+\n@@ -794,0 +1048,1 @@\n+          _verify_remembered_disable,  \/\/ do not verify remembered set\n@@ -799,0 +1054,1 @@\n+          _verify_size_exact,          \/\/ expect generation and heap sizes to match exactly\n@@ -804,1 +1060,7 @@\n-    verify_at_safepoint(\n+  VerifyRememberedSet verify_remembered_set = _verify_remembered_before_marking;\n+  if (_heap->mode()->is_generational() &&\n+      !_heap->old_generation()->is_mark_complete()) {\n+    \/\/ Before marking in generational mode, remembered set can't be verified w\/o complete old marking.\n+    verify_remembered_set = _verify_remembered_disable;\n+  }\n+  verify_at_safepoint(\n@@ -806,0 +1068,2 @@\n+          verify_remembered_set,\n+                                       \/\/ verify read-only remembered set from bottom() to top()\n@@ -811,0 +1075,1 @@\n+          _verify_size_exact,          \/\/ expect generation and heap sizes to match exactly\n@@ -818,6 +1083,23 @@\n-          _verify_forwarded_none,      \/\/ no forwarded references\n-          _verify_marked_complete_except_references, \/\/ bitmaps as precise as we can get, except dangling j.l.r.Refs\n-          _verify_cset_none,           \/\/ no references to cset anymore\n-          _verify_liveness_complete,   \/\/ liveness data must be complete here\n-          _verify_regions_disable,     \/\/ trash regions not yet recycled\n-          _verify_gcstate_stable_weakroots  \/\/ heap is still stable, weakroots are in progress\n+          _verify_remembered_disable,         \/\/ do not verify remembered set\n+          _verify_forwarded_none,             \/\/ no forwarded references\n+          _verify_marked_complete_satb_empty, \/\/ bitmaps as precise as we can get, except dangling j.l.r.Refs\n+          _verify_cset_none,                  \/\/ no references to cset anymore\n+          _verify_liveness_complete,          \/\/ liveness data must be complete here\n+          _verify_regions_disable,            \/\/ trash regions not yet recycled\n+          _verify_size_exact,                 \/\/ expect generation and heap sizes to match exactly\n+          _verify_gcstate_stable_weakroots    \/\/ heap is still stable, weakroots are in progress\n+  );\n+}\n+\n+void ShenandoahVerifier::verify_after_concmark_with_promotions() {\n+  verify_at_safepoint(\n+          \"After Mark\",\n+          _verify_remembered_disable,         \/\/ do not verify remembered set\n+          _verify_forwarded_none,             \/\/ no forwarded references\n+          _verify_marked_complete_satb_empty, \/\/ bitmaps as precise as we can get, except dangling j.l.r.Refs\n+          _verify_cset_none,                  \/\/ no references to cset anymore\n+          _verify_liveness_complete,          \/\/ liveness data must be complete here\n+          _verify_regions_disable,            \/\/ trash regions not yet recycled\n+          _verify_size_adjusted_for_padding,  \/\/ expect generation and heap sizes to match after adjustments\n+                                              \/\/ for promote in place padding\n+          _verify_gcstate_stable_weakroots    \/\/ heap is still stable, weakroots are in progress\n@@ -830,0 +1112,1 @@\n+          _verify_remembered_disable,                \/\/ do not verify remembered set\n@@ -835,0 +1118,2 @@\n+          _verify_size_adjusted_for_padding,         \/\/ expect generation and heap sizes to match after adjustments\n+                                                     \/\/  for promote in place padding\n@@ -839,25 +1124,6 @@\n-void ShenandoahVerifier::verify_during_evacuation() {\n-  verify_at_safepoint(\n-          \"During Evacuation\",\n-          _verify_forwarded_allow,    \/\/ some forwarded references are allowed\n-          _verify_marked_disable,     \/\/ walk only roots\n-          _verify_cset_disable,       \/\/ some cset references are not forwarded yet\n-          _verify_liveness_disable,   \/\/ liveness data might be already stale after pre-evacs\n-          _verify_regions_disable,    \/\/ trash regions not yet recycled\n-          _verify_gcstate_evacuation  \/\/ evacuation is in progress\n-  );\n-}\n-\n-void ShenandoahVerifier::verify_after_evacuation() {\n-  verify_at_safepoint(\n-          \"After Evacuation\",\n-          _verify_forwarded_allow,     \/\/ objects are still forwarded\n-          _verify_marked_complete,     \/\/ bitmaps might be stale, but alloc-after-mark should be well\n-          _verify_cset_forwarded,      \/\/ all cset refs are fully forwarded\n-          _verify_liveness_disable,    \/\/ no reliable liveness data anymore\n-          _verify_regions_notrash,     \/\/ trash regions have been recycled already\n-          _verify_gcstate_forwarded    \/\/ evacuation produced some forwarded objects\n-  );\n-}\n-\n-void ShenandoahVerifier::verify_before_updaterefs() {\n+void ShenandoahVerifier::verify_before_update_refs() {\n+  VerifyRememberedSet verify_remembered_set = _verify_remembered_before_updating_references;\n+  if (_heap->mode()->is_generational() &&\n+      !_heap->old_generation()->is_mark_complete()) {\n+    verify_remembered_set = _verify_remembered_disable;\n+  }\n@@ -866,0 +1132,1 @@\n+          verify_remembered_set,        \/\/ verify read-write remembered set\n@@ -871,1 +1138,2 @@\n-          _verify_gcstate_forwarded    \/\/ evacuation should have produced some forwarded objects\n+          _verify_size_exact,          \/\/ expect generation and heap sizes to match exactly\n+          _verify_gcstate_updating     \/\/ evacuation should have produced some forwarded objects\n@@ -875,1 +1143,2 @@\n-void ShenandoahVerifier::verify_after_updaterefs() {\n+\/\/ We have not yet cleanup (reclaimed) the collection set\n+void ShenandoahVerifier::verify_after_update_refs() {\n@@ -878,0 +1147,1 @@\n+          _verify_remembered_disable,  \/\/ do not verify remembered set\n@@ -883,0 +1153,1 @@\n+          _verify_size_exact,          \/\/ expect generation and heap sizes to match exactly\n@@ -890,0 +1161,1 @@\n+          _verify_remembered_disable,  \/\/ do not verify remembered set\n@@ -895,0 +1167,1 @@\n+          _verify_size_exact,          \/\/ expect generation and heap sizes to match exactly\n@@ -902,0 +1175,1 @@\n+          _verify_remembered_disable,  \/\/ do not verify remembered set\n@@ -907,0 +1181,1 @@\n+          _verify_size_disable,        \/\/ if we degenerate during evacuation, usage not valid: padding and deferred accounting\n@@ -914,0 +1189,1 @@\n+          _verify_remembered_after_full_gc,  \/\/ verify read-write remembered set\n@@ -915,1 +1191,1 @@\n-          _verify_marked_complete,     \/\/ all objects are marked in complete bitmap\n+          _verify_marked_incomplete,   \/\/ all objects are marked in incomplete bitmap\n@@ -919,0 +1195,1 @@\n+          _verify_size_exact,           \/\/ expect generation and heap sizes to match exactly\n@@ -923,1 +1200,1 @@\n-class ShenandoahVerifyNoForwared : public OopClosure {\n+class ShenandoahVerifyNoForwarded : public BasicOopIterateClosure {\n@@ -943,1 +1220,1 @@\n-class ShenandoahVerifyInToSpaceClosure : public OopClosure {\n+class ShenandoahVerifyInToSpaceClosure : public BasicOopIterateClosure {\n@@ -952,1 +1229,1 @@\n-      if (!heap->marking_context()->is_marked(obj)) {\n+      if (!heap->marking_context()->is_marked_or_old(obj)) {\n@@ -971,2 +1248,2 @@\n-  void do_oop(narrowOop* p) { do_oop_work(p); }\n-  void do_oop(oop* p)       { do_oop_work(p); }\n+  void do_oop(narrowOop* p) override { do_oop_work(p); }\n+  void do_oop(oop* p)       override { do_oop_work(p); }\n@@ -981,1 +1258,1 @@\n-  ShenandoahVerifyNoForwared cl;\n+  ShenandoahVerifyNoForwarded cl;\n@@ -984,0 +1261,160 @@\n+\n+template<typename Scanner>\n+class ShenandoahVerifyRemSetClosure : public BasicOopIterateClosure {\n+protected:\n+  ShenandoahGenerationalHeap* const _heap;\n+  Scanner*   const _scanner;\n+  const char* _message;\n+\n+public:\n+  \/\/ Argument distinguishes between initial mark or start of update refs verification.\n+  explicit ShenandoahVerifyRemSetClosure(Scanner* scanner, const char* message) :\n+            _heap(ShenandoahGenerationalHeap::heap()),\n+            _scanner(scanner),\n+            _message(message) {}\n+\n+  template<class T>\n+  inline void work(T* p) {\n+    T o = RawAccess<>::oop_load(p);\n+    if (!CompressedOops::is_null(o)) {\n+      oop obj = CompressedOops::decode_not_null(o);\n+      if (_heap->is_in_young(obj) && !_scanner->is_card_dirty((HeapWord*) p)) {\n+        ShenandoahAsserts::print_failure(ShenandoahAsserts::_safe_all, obj, p, nullptr,\n+                                         _message, \"clean card, it should be dirty.\", __FILE__, __LINE__);\n+      }\n+    }\n+  }\n+\n+  void do_oop(narrowOop* p) override { work(p); }\n+  void do_oop(oop* p)       override { work(p); }\n+};\n+\n+template<typename Scanner>\n+void ShenandoahVerifier::help_verify_region_rem_set(Scanner* scanner, ShenandoahHeapRegion* r,\n+                                                    HeapWord* registration_watermark, const char* message) {\n+  shenandoah_assert_generations_reconciled();\n+  ShenandoahOldGeneration* old_gen = _heap->old_generation();\n+  assert(old_gen->is_mark_complete() || old_gen->is_parsable(), \"Sanity\");\n+\n+  ShenandoahMarkingContext* ctx = old_gen->is_mark_complete() ? old_gen->complete_marking_context() : nullptr;\n+  ShenandoahVerifyRemSetClosure<Scanner> check_interesting_pointers(scanner, message);\n+  HeapWord* from = r->bottom();\n+  HeapWord* obj_addr = from;\n+  if (r->is_humongous_start()) {\n+    oop obj = cast_to_oop(obj_addr);\n+    if ((ctx == nullptr) || ctx->is_marked(obj)) {\n+      \/\/ For humongous objects, the typical object is an array, so the following checks may be overkill\n+      \/\/ For regular objects (not object arrays), if the card holding the start of the object is dirty,\n+      \/\/ we do not need to verify that cards spanning interesting pointers within this object are dirty.\n+      if (!scanner->is_card_dirty(obj_addr) || obj->is_objArray()) {\n+        obj->oop_iterate(&check_interesting_pointers);\n+      }\n+      \/\/ else, object's start is marked dirty and obj is not an objArray, so any interesting pointers are covered\n+    }\n+    \/\/ else, this humongous object is not live so no need to verify its internal pointers\n+\n+    if ((obj_addr < registration_watermark) && !scanner->verify_registration(obj_addr, ctx)) {\n+      ShenandoahAsserts::print_failure(ShenandoahAsserts::_safe_all, obj, obj_addr, nullptr, message,\n+                                       \"object not properly registered\", __FILE__, __LINE__);\n+    }\n+  } else if (!r->is_humongous()) {\n+    HeapWord* top = r->top();\n+    while (obj_addr < top) {\n+      oop obj = cast_to_oop(obj_addr);\n+      \/\/ ctx->is_marked() returns true if mark bit set or if obj above TAMS.\n+      if ((ctx == nullptr) || ctx->is_marked(obj)) {\n+        \/\/ For regular objects (not object arrays), if the card holding the start of the object is dirty,\n+        \/\/ we do not need to verify that cards spanning interesting pointers within this object are dirty.\n+        if (!scanner->is_card_dirty(obj_addr) || obj->is_objArray()) {\n+          obj->oop_iterate(&check_interesting_pointers);\n+        }\n+        \/\/ else, object's start is marked dirty and obj is not an objArray, so any interesting pointers are covered\n+\n+        if ((obj_addr < registration_watermark) && !scanner->verify_registration(obj_addr, ctx)) {\n+          ShenandoahAsserts::print_failure(ShenandoahAsserts::_safe_all, obj, obj_addr, nullptr, message,\n+                                           \"object not properly registered\", __FILE__, __LINE__);\n+        }\n+        obj_addr += obj->size();\n+      } else {\n+        \/\/ This object is not live so we don't verify dirty cards contained therein\n+        HeapWord* tams = ctx->top_at_mark_start(r);\n+        obj_addr = ctx->get_next_marked_addr(obj_addr, tams);\n+      }\n+    }\n+  }\n+}\n+\n+class ShenandoahWriteTableScanner {\n+private:\n+  ShenandoahScanRemembered* _scanner;\n+public:\n+  explicit ShenandoahWriteTableScanner(ShenandoahScanRemembered* scanner) : _scanner(scanner) {}\n+\n+  bool is_card_dirty(HeapWord* obj_addr) {\n+    return _scanner->is_write_card_dirty(obj_addr);\n+  }\n+\n+  bool verify_registration(HeapWord* obj_addr, ShenandoahMarkingContext* ctx) {\n+    return _scanner->verify_registration(obj_addr, ctx);\n+  }\n+};\n+\n+\/\/ Assure that the remember set has a dirty card everywhere there is an interesting pointer.\n+\/\/ This examines the read_card_table between bottom() and top() since all PLABS are retired\n+\/\/ before the safepoint for init_mark.  Actually, we retire them before update-references and don't\n+\/\/ restore them until the start of evacuation.\n+void ShenandoahVerifier::verify_rem_set_before_mark() {\n+  shenandoah_assert_safepoint();\n+  shenandoah_assert_generational();\n+\n+  ShenandoahOldGeneration* old_generation = _heap->old_generation();\n+\n+  log_debug(gc)(\"Verifying remembered set at %s mark\", old_generation->is_doing_mixed_evacuations() ? \"mixed\" : \"young\");\n+\n+  ShenandoahScanRemembered* scanner = old_generation->card_scan();\n+  for (size_t i = 0, n = _heap->num_regions(); i < n; ++i) {\n+    ShenandoahHeapRegion* r = _heap->get_region(i);\n+    if (r->is_old() && r->is_active()) {\n+      help_verify_region_rem_set(scanner, r, r->end(), \"Verify init-mark remembered set violation\");\n+    }\n+  }\n+}\n+\n+void ShenandoahVerifier::verify_rem_set_after_full_gc() {\n+  shenandoah_assert_safepoint();\n+  shenandoah_assert_generational();\n+\n+  ShenandoahWriteTableScanner scanner(ShenandoahGenerationalHeap::heap()->old_generation()->card_scan());\n+  for (size_t i = 0, n = _heap->num_regions(); i < n; ++i) {\n+    ShenandoahHeapRegion* r = _heap->get_region(i);\n+    if (r->is_old() && !r->is_cset()) {\n+      help_verify_region_rem_set(&scanner, r, r->top(), \"Remembered set violation at end of Full GC\");\n+    }\n+  }\n+}\n+\n+\/\/ Assure that the remember set has a dirty card everywhere there is an interesting pointer.  Even though\n+\/\/ the update-references scan of remembered set only examines cards up to update_watermark, the remembered\n+\/\/ set should be valid through top.  This examines the write_card_table between bottom() and top() because\n+\/\/ all PLABS are retired immediately before the start of update refs.\n+void ShenandoahVerifier::verify_rem_set_before_update_ref() {\n+  shenandoah_assert_safepoint();\n+  shenandoah_assert_generational();\n+\n+  ShenandoahWriteTableScanner scanner(_heap->old_generation()->card_scan());\n+  for (size_t i = 0, n = _heap->num_regions(); i < n; ++i) {\n+    ShenandoahHeapRegion* r = _heap->get_region(i);\n+    if (r->is_old() && !r->is_cset()) {\n+      help_verify_region_rem_set(&scanner, r, r->get_update_watermark(), \"Remembered set violation at init-update-references\");\n+    }\n+  }\n+}\n+\n+void ShenandoahVerifier::verify_before_rebuilding_free_set() {\n+  ShenandoahGenerationStatsClosure cl;\n+  _heap->heap_region_iterate(&cl);\n+\n+  ShenandoahGenerationStatsClosure::validate_usage(false, \"Before free set rebuild\", _heap->old_generation(), cl.old);\n+  ShenandoahGenerationStatsClosure::validate_usage(false, \"Before free set rebuild\", _heap->young_generation(), cl.young);\n+  ShenandoahGenerationStatsClosure::validate_usage(false, \"Before free set rebuild\", _heap->global_generation(), cl.global);\n+}\n","filename":"src\/hotspot\/share\/gc\/shenandoah\/shenandoahVerifier.cpp","additions":525,"deletions":88,"binary":false,"changes":613,"status":"modified"},{"patch":"@@ -3,0 +3,1 @@\n+ * Copyright Amazon.com Inc. or its affiliates. All Rights Reserved.\n@@ -35,0 +36,1 @@\n+class ShenandoahMarkingContext;\n@@ -60,0 +62,18 @@\n+  typedef enum {\n+    \/\/ Disable remembered set verification.\n+    _verify_remembered_disable,\n+\n+    \/\/ Old objects should be registered and RS cards within *read-only* RS are dirty for all\n+    \/\/ inter-generational pointers.\n+    _verify_remembered_before_marking,\n+\n+    \/\/ Old objects should be registered and RS cards within *read-write* RS are dirty for all\n+    \/\/ inter-generational pointers.\n+    _verify_remembered_before_updating_references,\n+\n+    \/\/ Old objects should be registered and RS cards within *read-write* RS are dirty for all\n+    \/\/ inter-generational pointers. Differs from previous verification modes by using top instead\n+    \/\/ of update watermark and not using the marking context.\n+    _verify_remembered_after_full_gc\n+  } VerifyRememberedSet;\n+\n@@ -72,1 +92,6 @@\n-    _verify_marked_complete_except_references\n+    _verify_marked_complete_except_references,\n+\n+    \/\/ Objects should be marked in \"complete\" bitmap, except j.l.r.Reference referents, which\n+    \/\/ may be dangling after marking but before conc-weakrefs-processing. All SATB buffers must\n+    \/\/ be empty.\n+    _verify_marked_complete_satb_empty,\n@@ -125,0 +150,11 @@\n+  typedef enum {\n+    \/\/ Disable size verification\n+    _verify_size_disable,\n+\n+    \/\/ Enforce exact consistency\n+    _verify_size_exact,\n+\n+    \/\/ Expect promote-in-place adjustments: padding inserted to temporarily prevent further allocation in regular regions\n+    _verify_size_adjusted_for_padding\n+  } VerifySize;\n+\n@@ -138,2 +174,2 @@\n-    \/\/ Evacuation is in progress, some objects are forwarded\n-    _verify_gcstate_evacuation\n+    \/\/ Evacuation is done, some objects are forwarded, updating is in progress\n+    _verify_gcstate_updating\n@@ -163,1 +199,2 @@\n-  void verify_at_safepoint(const char *label,\n+  void verify_at_safepoint(const char* label,\n+                           VerifyRememberedSet remembered,\n@@ -169,0 +206,1 @@\n+                           VerifySize sizeness,\n@@ -177,0 +215,1 @@\n+  void verify_after_concmark_with_promotions();\n@@ -178,4 +217,2 @@\n-  void verify_during_evacuation();\n-  void verify_after_evacuation();\n-  void verify_before_updaterefs();\n-  void verify_after_updaterefs();\n+  void verify_before_update_refs();\n+  void verify_after_update_refs();\n@@ -189,1 +226,0 @@\n-\n@@ -191,0 +227,11 @@\n+\n+  \/\/ Check that generation usages are accurate before rebuilding free set\n+  void verify_before_rebuilding_free_set();\n+private:\n+  template<typename Scanner>\n+  void help_verify_region_rem_set(Scanner* scanner, ShenandoahHeapRegion* r,\n+                                  HeapWord* update_watermark, const char* message);\n+\n+  void verify_rem_set_before_mark();\n+  void verify_rem_set_before_update_ref();\n+  void verify_rem_set_after_full_gc();\n","filename":"src\/hotspot\/share\/gc\/shenandoah\/shenandoahVerifier.hpp","additions":56,"deletions":9,"binary":false,"changes":65,"status":"modified"},{"patch":"@@ -3,0 +3,1 @@\n+ * Copyright (c) 2014, Oracle and\/or its affiliates. All rights reserved.\n@@ -76,1 +77,0 @@\n-  ShenandoahThreadLocalData::create(worker);\n","filename":"src\/hotspot\/share\/gc\/shenandoah\/shenandoahWorkGroup.cpp","additions":1,"deletions":1,"binary":false,"changes":2,"status":"modified"},{"patch":"@@ -28,1 +28,0 @@\n-#include \"gc\/shared\/workerPolicy.hpp\"\n@@ -30,14 +29,0 @@\n-#include \"runtime\/javaThread.hpp\"\n-#include \"runtime\/threads.hpp\"\n-\n-uint ShenandoahWorkerPolicy::_prev_par_marking     = 0;\n-uint ShenandoahWorkerPolicy::_prev_conc_marking    = 0;\n-uint ShenandoahWorkerPolicy::_prev_conc_evac       = 0;\n-uint ShenandoahWorkerPolicy::_prev_conc_root_proc  = 0;\n-uint ShenandoahWorkerPolicy::_prev_conc_refs_proc  = 0;\n-uint ShenandoahWorkerPolicy::_prev_fullgc          = 0;\n-uint ShenandoahWorkerPolicy::_prev_degengc         = 0;\n-uint ShenandoahWorkerPolicy::_prev_conc_update_ref = 0;\n-uint ShenandoahWorkerPolicy::_prev_par_update_ref  = 0;\n-uint ShenandoahWorkerPolicy::_prev_conc_cleanup    = 0;\n-uint ShenandoahWorkerPolicy::_prev_conc_reset      = 0;\n@@ -46,7 +31,1 @@\n-  uint active_workers = (_prev_par_marking == 0) ? ParallelGCThreads : _prev_par_marking;\n-\n-  _prev_par_marking =\n-    WorkerPolicy::calc_active_workers(ParallelGCThreads,\n-                                      active_workers,\n-                                      Threads::number_of_non_daemon_threads());\n-  return _prev_par_marking;\n+  return ParallelGCThreads;\n@@ -56,6 +35,5 @@\n-  uint active_workers = (_prev_conc_marking == 0) ?  ConcGCThreads : _prev_conc_marking;\n-  _prev_conc_marking =\n-    WorkerPolicy::calc_active_conc_workers(ConcGCThreads,\n-                                           active_workers,\n-                                           Threads::number_of_non_daemon_threads());\n-  return _prev_conc_marking;\n+  return ConcGCThreads;\n+}\n+\n+uint ShenandoahWorkerPolicy::calc_workers_for_rs_scanning() {\n+  return ConcGCThreads;\n@@ -64,1 +42,0 @@\n-\/\/ Reuse the calculation result from init marking\n@@ -66,1 +43,1 @@\n-  return _prev_par_marking;\n+  return ParallelGCThreads;\n@@ -69,1 +46,0 @@\n-\/\/ Calculate workers for concurrent refs processing\n@@ -71,6 +47,1 @@\n-  uint active_workers = (_prev_conc_refs_proc == 0) ? ConcGCThreads : _prev_conc_refs_proc;\n-  _prev_conc_refs_proc =\n-    WorkerPolicy::calc_active_conc_workers(ConcGCThreads,\n-                                           active_workers,\n-                                           Threads::number_of_non_daemon_threads());\n-  return _prev_conc_refs_proc;\n+  return ConcGCThreads;\n@@ -79,1 +50,0 @@\n-\/\/ Calculate workers for concurrent root processing\n@@ -81,6 +51,1 @@\n-  uint active_workers = (_prev_conc_root_proc == 0) ? ConcGCThreads : _prev_conc_root_proc;\n-  _prev_conc_root_proc =\n-          WorkerPolicy::calc_active_conc_workers(ConcGCThreads,\n-                                                 active_workers,\n-                                                 Threads::number_of_non_daemon_threads());\n-  return _prev_conc_root_proc;\n+  return ConcGCThreads;\n@@ -89,1 +54,0 @@\n-\/\/ Calculate workers for concurrent evacuation (concurrent GC)\n@@ -91,6 +55,1 @@\n-  uint active_workers = (_prev_conc_evac == 0) ? ConcGCThreads : _prev_conc_evac;\n-  _prev_conc_evac =\n-    WorkerPolicy::calc_active_conc_workers(ConcGCThreads,\n-                                           active_workers,\n-                                           Threads::number_of_non_daemon_threads());\n-  return _prev_conc_evac;\n+  return ConcGCThreads;\n@@ -99,1 +58,0 @@\n-\/\/ Calculate workers for parallel fullgc\n@@ -101,6 +59,1 @@\n-  uint active_workers = (_prev_fullgc == 0) ?  ParallelGCThreads : _prev_fullgc;\n-  _prev_fullgc =\n-    WorkerPolicy::calc_active_workers(ParallelGCThreads,\n-                                      active_workers,\n-                                      Threads::number_of_non_daemon_threads());\n-  return _prev_fullgc;\n+  return ParallelGCThreads;\n@@ -109,1 +62,0 @@\n-\/\/ Calculate workers for parallel degenerated gc\n@@ -111,6 +63,1 @@\n-  uint active_workers = (_prev_degengc == 0) ?  ParallelGCThreads : _prev_degengc;\n-  _prev_degengc =\n-    WorkerPolicy::calc_active_workers(ParallelGCThreads,\n-                                      active_workers,\n-                                      Threads::number_of_non_daemon_threads());\n-  return _prev_degengc;\n+  return ParallelGCThreads;\n@@ -119,1 +66,0 @@\n-\/\/ Calculate workers for concurrent reference update\n@@ -121,6 +67,1 @@\n-  uint active_workers = (_prev_conc_update_ref == 0) ? ConcGCThreads : _prev_conc_update_ref;\n-  _prev_conc_update_ref =\n-    WorkerPolicy::calc_active_conc_workers(ConcGCThreads,\n-                                           active_workers,\n-                                           Threads::number_of_non_daemon_threads());\n-  return _prev_conc_update_ref;\n+  return ConcGCThreads;\n@@ -129,1 +70,0 @@\n-\/\/ Calculate workers for parallel reference update\n@@ -131,6 +71,1 @@\n-  uint active_workers = (_prev_par_update_ref == 0) ? ParallelGCThreads : _prev_par_update_ref;\n-  _prev_par_update_ref =\n-    WorkerPolicy::calc_active_workers(ParallelGCThreads,\n-                                      active_workers,\n-                                      Threads::number_of_non_daemon_threads());\n-  return _prev_par_update_ref;\n+  return ParallelGCThreads;\n@@ -139,7 +74,2 @@\n-uint ShenandoahWorkerPolicy::calc_workers_for_conc_cleanup() {\n-  uint active_workers = (_prev_conc_cleanup == 0) ? ConcGCThreads : _prev_conc_cleanup;\n-  _prev_conc_cleanup =\n-          WorkerPolicy::calc_active_conc_workers(ConcGCThreads,\n-                                                 active_workers,\n-                                                 Threads::number_of_non_daemon_threads());\n-  return _prev_conc_cleanup;\n+uint ShenandoahWorkerPolicy::calc_workers_for_conc_reset() {\n+  return ConcGCThreads;\n@@ -148,7 +78,2 @@\n-uint ShenandoahWorkerPolicy::calc_workers_for_conc_reset() {\n-  uint active_workers = (_prev_conc_reset == 0) ? ConcGCThreads : _prev_conc_reset;\n-  _prev_conc_reset =\n-          WorkerPolicy::calc_active_conc_workers(ConcGCThreads,\n-                                                 active_workers,\n-                                                 Threads::number_of_non_daemon_threads());\n-  return _prev_conc_reset;\n+uint ShenandoahWorkerPolicy::calc_workers_for_conc_cleanup() {\n+  return ConcGCThreads;\n","filename":"src\/hotspot\/share\/gc\/shenandoah\/shenandoahWorkerPolicy.cpp","additions":18,"deletions":93,"binary":false,"changes":111,"status":"modified"},{"patch":"@@ -31,13 +31,0 @@\n-private:\n-  static uint _prev_par_marking;\n-  static uint _prev_conc_marking;\n-  static uint _prev_conc_root_proc;\n-  static uint _prev_conc_refs_proc;\n-  static uint _prev_conc_evac;\n-  static uint _prev_fullgc;\n-  static uint _prev_degengc;\n-  static uint _prev_conc_update_ref;\n-  static uint _prev_par_update_ref;\n-  static uint _prev_conc_cleanup;\n-  static uint _prev_conc_reset;\n-\n@@ -51,0 +38,3 @@\n+  \/\/ Calculate the number of workers for remembered set scanning\n+  static uint calc_workers_for_rs_scanning();\n+\n@@ -75,3 +65,0 @@\n-  \/\/ Calculate workers for concurrent cleanup\n-  static uint calc_workers_for_conc_cleanup();\n-\n@@ -80,0 +67,3 @@\n+\n+  \/\/ Calculate workers for concurrent cleanup\n+  static uint calc_workers_for_conc_cleanup();\n","filename":"src\/hotspot\/share\/gc\/shenandoah\/shenandoahWorkerPolicy.hpp","additions":6,"deletions":16,"binary":false,"changes":22,"status":"modified"},{"patch":"@@ -0,0 +1,119 @@\n+\/*\n+ * Copyright Amazon.com Inc. or its affiliates. All Rights Reserved.\n+ * DO NOT ALTER OR REMOVE COPYRIGHT NOTICES OR THIS FILE HEADER.\n+ *\n+ * This code is free software; you can redistribute it and\/or modify it\n+ * under the terms of the GNU General Public License version 2 only, as\n+ * published by the Free Software Foundation.\n+ *\n+ * This code is distributed in the hope that it will be useful, but WITHOUT\n+ * ANY WARRANTY; without even the implied warranty of MERCHANTABILITY or\n+ * FITNESS FOR A PARTICULAR PURPOSE.  See the GNU General Public License\n+ * version 2 for more details (a copy is included in the LICENSE file that\n+ * accompanied this code).\n+ *\n+ * You should have received a copy of the GNU General Public License version\n+ * 2 along with this work; if not, write to the Free Software Foundation,\n+ * Inc., 51 Franklin St, Fifth Floor, Boston, MA 02110-1301 USA.\n+ *\n+ * Please contact Oracle, 500 Oracle Parkway, Redwood Shores, CA 94065 USA\n+ * or visit www.oracle.com if you need additional information or have any\n+ * questions.\n+ *\n+ *\/\n+#include \"precompiled.hpp\"\n+\n+#include \"gc\/shenandoah\/shenandoahAgeCensus.hpp\"\n+#include \"gc\/shenandoah\/shenandoahFreeSet.hpp\"\n+#include \"gc\/shenandoah\/shenandoahHeap.hpp\"\n+#include \"gc\/shenandoah\/shenandoahHeapRegionClosures.hpp\"\n+#include \"gc\/shenandoah\/shenandoahUtils.hpp\"\n+#include \"gc\/shenandoah\/shenandoahYoungGeneration.hpp\"\n+#include \"gc\/shenandoah\/heuristics\/shenandoahYoungHeuristics.hpp\"\n+\n+ShenandoahYoungGeneration::ShenandoahYoungGeneration(uint max_queues, size_t max_capacity) :\n+  ShenandoahGeneration(YOUNG, max_queues, max_capacity),\n+  _old_gen_task_queues(nullptr) {\n+}\n+\n+void ShenandoahYoungGeneration::set_concurrent_mark_in_progress(bool in_progress) {\n+  ShenandoahHeap* heap = ShenandoahHeap::heap();\n+  heap->set_concurrent_young_mark_in_progress(in_progress);\n+  if (is_bootstrap_cycle() && in_progress && !heap->is_prepare_for_old_mark_in_progress()) {\n+    \/\/ This is not a bug. When the bootstrapping marking phase is complete,\n+    \/\/ the old generation marking is still in progress, unless it's not.\n+    \/\/ In the case that old-gen preparation for mixed evacuation has been\n+    \/\/ preempted, we do not want to set concurrent old mark to be in progress.\n+    heap->set_concurrent_old_mark_in_progress(in_progress);\n+  }\n+}\n+\n+bool ShenandoahYoungGeneration::contains(ShenandoahAffiliation affiliation) const {\n+  return affiliation == YOUNG_GENERATION;\n+}\n+\n+bool ShenandoahYoungGeneration::contains(ShenandoahHeapRegion* region) const {\n+  return region->is_young();\n+}\n+\n+void ShenandoahYoungGeneration::parallel_heap_region_iterate(ShenandoahHeapRegionClosure* cl) {\n+  \/\/ Just iterate over the young generation here.\n+  ShenandoahIncludeRegionClosure<YOUNG_GENERATION> young_regions_cl(cl);\n+  ShenandoahHeap::heap()->parallel_heap_region_iterate(&young_regions_cl);\n+}\n+\n+void ShenandoahYoungGeneration::heap_region_iterate(ShenandoahHeapRegionClosure* cl) {\n+  ShenandoahIncludeRegionClosure<YOUNG_GENERATION> young_regions_cl(cl);\n+  ShenandoahHeap::heap()->heap_region_iterate(&young_regions_cl);\n+}\n+\n+void ShenandoahYoungGeneration::parallel_heap_region_iterate_free(ShenandoahHeapRegionClosure* cl) {\n+  \/\/ Iterate over everything that is not old.\n+  ShenandoahExcludeRegionClosure<OLD_GENERATION> exclude_cl(cl);\n+  ShenandoahHeap::heap()->parallel_heap_region_iterate(&exclude_cl);\n+}\n+\n+bool ShenandoahYoungGeneration::is_concurrent_mark_in_progress() {\n+  return ShenandoahHeap::heap()->is_concurrent_young_mark_in_progress();\n+}\n+\n+void ShenandoahYoungGeneration::reserve_task_queues(uint workers) {\n+  ShenandoahGeneration::reserve_task_queues(workers);\n+  if (is_bootstrap_cycle()) {\n+    _old_gen_task_queues->reserve(workers);\n+  }\n+}\n+\n+bool ShenandoahYoungGeneration::contains(oop obj) const {\n+  return ShenandoahHeap::heap()->is_in_young(obj);\n+}\n+\n+ShenandoahHeuristics* ShenandoahYoungGeneration::initialize_heuristics(ShenandoahMode* gc_mode) {\n+  _young_heuristics = new ShenandoahYoungHeuristics(this);\n+  _heuristics = _young_heuristics;\n+  _heuristics->set_guaranteed_gc_interval(ShenandoahGuaranteedYoungGCInterval);\n+  confirm_heuristics_mode();\n+  return _heuristics;\n+}\n+\n+size_t ShenandoahYoungGeneration::available() const {\n+  \/\/ The collector reserve may eat into what the mutator is allowed to use. Make sure we are looking\n+  \/\/ at what is available to the mutator when reporting how much memory is available.\n+  size_t available = this->ShenandoahGeneration::available();\n+  return MIN2(available, ShenandoahHeap::heap()->free_set()->available());\n+}\n+\n+size_t ShenandoahYoungGeneration::soft_available() const {\n+  size_t available = this->ShenandoahGeneration::soft_available();\n+  return MIN2(available, ShenandoahHeap::heap()->free_set()->available());\n+}\n+\n+void ShenandoahYoungGeneration::prepare_gc() {\n+\n+  ShenandoahGeneration::prepare_gc();\n+\n+  assert(type() == YOUNG, \"Error?\");\n+  \/\/ Clear any stale\/partial local census data before the start of a\n+  \/\/ new marking cycle\n+  ShenandoahGenerationalHeap::heap()->age_census()->reset_local();\n+}\n","filename":"src\/hotspot\/share\/gc\/shenandoah\/shenandoahYoungGeneration.cpp","additions":119,"deletions":0,"binary":false,"changes":119,"status":"added"},{"patch":"@@ -0,0 +1,85 @@\n+\/*\n+ * Copyright Amazon.com Inc. or its affiliates. All Rights Reserved.\n+ * DO NOT ALTER OR REMOVE COPYRIGHT NOTICES OR THIS FILE HEADER.\n+ *\n+ * This code is free software; you can redistribute it and\/or modify it\n+ * under the terms of the GNU General Public License version 2 only, as\n+ * published by the Free Software Foundation.\n+ *\n+ * This code is distributed in the hope that it will be useful, but WITHOUT\n+ * ANY WARRANTY; without even the implied warranty of MERCHANTABILITY or\n+ * FITNESS FOR A PARTICULAR PURPOSE.  See the GNU General Public License\n+ * version 2 for more details (a copy is included in the LICENSE file that\n+ * accompanied this code).\n+ *\n+ * You should have received a copy of the GNU General Public License version\n+ * 2 along with this work; if not, write to the Free Software Foundation,\n+ * Inc., 51 Franklin St, Fifth Floor, Boston, MA 02110-1301 USA.\n+ *\n+ * Please contact Oracle, 500 Oracle Parkway, Redwood Shores, CA 94065 USA\n+ * or visit www.oracle.com if you need additional information or have any\n+ * questions.\n+ *\n+ *\/\n+\n+#ifndef SHARE_VM_GC_SHENANDOAH_SHENANDOAHYOUNGGENERATION_HPP\n+#define SHARE_VM_GC_SHENANDOAH_SHENANDOAHYOUNGGENERATION_HPP\n+\n+#include \"gc\/shenandoah\/shenandoahGeneration.hpp\"\n+#include \"gc\/shenandoah\/heuristics\/shenandoahYoungHeuristics.hpp\"\n+\n+class ShenandoahYoungGeneration : public ShenandoahGeneration {\n+private:\n+  ShenandoahObjToScanQueueSet* _old_gen_task_queues;\n+  ShenandoahYoungHeuristics* _young_heuristics;\n+\n+public:\n+  ShenandoahYoungGeneration(uint max_queues, size_t max_capacity);\n+\n+  ShenandoahHeuristics* initialize_heuristics(ShenandoahMode* gc_mode) override;\n+\n+  const char* name() const override {\n+    return \"Young\";\n+  }\n+\n+  ShenandoahYoungHeuristics* heuristics() const override {\n+    return _young_heuristics;\n+  }\n+\n+  void set_concurrent_mark_in_progress(bool in_progress) override;\n+  bool is_concurrent_mark_in_progress() override;\n+\n+  void parallel_heap_region_iterate(ShenandoahHeapRegionClosure* cl) override;\n+\n+  void parallel_heap_region_iterate_free(ShenandoahHeapRegionClosure* cl) override;\n+\n+  void heap_region_iterate(ShenandoahHeapRegionClosure* cl) override;\n+\n+  bool contains(ShenandoahAffiliation affiliation) const override;\n+  bool contains(ShenandoahHeapRegion* region) const override;\n+  bool contains(oop obj) const override;\n+\n+  void reserve_task_queues(uint workers) override;\n+  void set_old_gen_task_queues(ShenandoahObjToScanQueueSet* old_gen_queues) {\n+    _old_gen_task_queues = old_gen_queues;\n+  }\n+  ShenandoahObjToScanQueueSet* old_gen_task_queues() const override {\n+    return _old_gen_task_queues;\n+  }\n+\n+  \/\/ Returns true if the young generation is configured to enqueue old\n+  \/\/ oops for the old generation mark queues.\n+  bool is_bootstrap_cycle() {\n+    return _old_gen_task_queues != nullptr;\n+  }\n+\n+  size_t available() const override;\n+\n+  \/\/ Do not override available_with_reserve() because that needs to see memory reserved for Collector\n+\n+  size_t soft_available() const override;\n+\n+  void prepare_gc() override;\n+};\n+\n+#endif \/\/ SHARE_VM_GC_SHENANDOAH_SHENANDOAHYOUNGGENERATION_HPP\n","filename":"src\/hotspot\/share\/gc\/shenandoah\/shenandoahYoungGeneration.hpp","additions":85,"deletions":0,"binary":false,"changes":85,"status":"added"},{"patch":"@@ -2,1 +2,1 @@\n- * Copyright (c) 2020, Oracle and\/or its affiliates. All rights reserved.\n+ * Copyright (c) 2020, 2023, Oracle and\/or its affiliates. All rights reserved.\n@@ -4,0 +4,1 @@\n+ * Copyright Amazon.com Inc. or its affiliates. All Rights Reserved.\n@@ -37,0 +38,80 @@\n+  product(uintx, ShenandoahGenerationalHumongousReserve, 0, EXPERIMENTAL,   \\\n+          \"(Generational mode only) What percent of the heap should be \"    \\\n+          \"reserved for humongous objects if possible.  Old-generation \"    \\\n+          \"collections will endeavor to evacuate old-gen regions within \"   \\\n+          \"this reserved area even if these regions do not contain high \"   \\\n+          \"percentage of garbage.  Setting a larger value will cause \"      \\\n+          \"more frequent old-gen collections.  A smaller value will \"       \\\n+          \"increase the likelihood that humongous object allocations \"      \\\n+          \"fail, resulting in stop-the-world full GCs.\")                    \\\n+          range(0,100)                                                      \\\n+                                                                            \\\n+  product(double, ShenandoahMinOldGenGrowthPercent, 12.5, EXPERIMENTAL,     \\\n+          \"(Generational mode only) If the usage within old generation \"    \\\n+          \"has grown by at least this percent of its live memory size \"     \\\n+          \"at completion of the most recent old-generation marking \"        \\\n+          \"effort, heuristics may trigger the start of a new old-gen \"      \\\n+          \"collection.\")                                                    \\\n+          range(0.0,100.0)                                                  \\\n+                                                                            \\\n+  product(uintx, ShenandoahIgnoreOldGrowthBelowPercentage,10, EXPERIMENTAL, \\\n+          \"(Generational mode only) If the total usage of the old \"         \\\n+          \"generation is smaller than this percent, we do not trigger \"     \\\n+          \"old gen collections even if old has grown, except when \"         \\\n+          \"ShenandoahGenerationalDoNotIgnoreGrowthAfterYoungCycles \"        \\\n+          \"consecutive cycles have been completed following the \"           \\\n+          \"preceding old-gen collection.\")                                  \\\n+          range(0,100)                                                      \\\n+                                                                            \\\n+  product(uintx, ShenandoahDoNotIgnoreGrowthAfterYoungCycles,               \\\n+          50, EXPERIMENTAL,                                                 \\\n+          \"(Generational mode only) Even if the usage of old generation \"   \\\n+          \"is below ShenandoahIgnoreOldGrowthBelowPercentage, \"             \\\n+          \"trigger an old-generation mark if old has grown and this \"       \\\n+          \"many consecutive young-gen collections have been \"               \\\n+          \"completed following the preceding old-gen collection.\")          \\\n+                                                                            \\\n+  product(bool, ShenandoahGenerationalCensusAtEvac, false, EXPERIMENTAL,    \\\n+          \"(Generational mode only) Object age census at evacuation, \"      \\\n+          \"rather than during marking.\")                                    \\\n+                                                                            \\\n+  product(bool, ShenandoahGenerationalAdaptiveTenuring, true, EXPERIMENTAL, \\\n+          \"(Generational mode only) Dynamically adapt tenuring age.\")       \\\n+                                                                            \\\n+  product(bool, ShenandoahGenerationalCensusIgnoreOlderCohorts, true,       \\\n+                                                               EXPERIMENTAL,\\\n+          \"(Generational mode only) Ignore mortality rates older than the \" \\\n+          \"oldest cohort under the tenuring age for the last cycle.\" )      \\\n+                                                                            \\\n+  product(uintx, ShenandoahGenerationalMinTenuringAge, 1, EXPERIMENTAL,     \\\n+          \"(Generational mode only) Floor for adaptive tenuring age. \"      \\\n+          \"Setting floor and ceiling to the same value fixes the tenuring \" \\\n+          \"age; setting both to 1 simulates a poor approximation to \"       \\\n+          \"AlwaysTenure, and setting both to 16 simulates NeverTenure.\")    \\\n+          range(1,16)                                                       \\\n+                                                                            \\\n+  product(uintx, ShenandoahGenerationalMaxTenuringAge, 15, EXPERIMENTAL,    \\\n+          \"(Generational mode only) Ceiling for adaptive tenuring age. \"    \\\n+          \"Setting floor and ceiling to the same value fixes the tenuring \" \\\n+          \"age; setting both to 1 simulates a poor approximation to \"       \\\n+          \"AlwaysTenure, and setting both to 16 simulates NeverTenure.\")    \\\n+          range(1,16)                                                       \\\n+                                                                            \\\n+  product(double, ShenandoahGenerationalTenuringMortalityRateThreshold,     \\\n+                                                         0.1, EXPERIMENTAL, \\\n+          \"(Generational mode only) Cohort mortality rates below this \"     \\\n+          \"value will be treated as indicative of longevity, leading to \"   \\\n+          \"tenuring. A lower value delays tenuring, a higher value hastens \"\\\n+          \"it. Used only when ShenandoahGenerationalhenAdaptiveTenuring is \"\\\n+          \"enabled.\")                                                       \\\n+          range(0.001,0.999)                                                \\\n+                                                                            \\\n+  product(size_t, ShenandoahGenerationalTenuringCohortPopulationThreshold,  \\\n+                                                         4*K, EXPERIMENTAL, \\\n+          \"(Generational mode only) Cohorts whose population is lower than \"\\\n+          \"this value in the previous census are ignored wrt tenuring \"     \\\n+          \"decisions. Effectively this makes then tenurable as soon as all \"\\\n+          \"older cohorts are. Set this value to the largest cohort \"        \\\n+          \"population volume that you are comfortable ignoring when making \"\\\n+          \"tenuring decisions.\")                                            \\\n+                                                                            \\\n@@ -53,7 +134,0 @@\n-  product(intx, ShenandoahHumongousThreshold, 100, EXPERIMENTAL,            \\\n-          \"Humongous objects are allocated in separate regions. \"           \\\n-          \"This setting defines how large the object should be to be \"      \\\n-          \"deemed humongous. Value is in  percents of heap region size. \"   \\\n-          \"This also caps the maximum TLAB size.\")                          \\\n-          range(1, 100)                                                     \\\n-                                                                            \\\n@@ -65,1 +139,2 @@\n-          \" passive - stop the world GC only (either degenerated or full)\") \\\n+          \" passive - stop the world GC only (either degenerated or full);\" \\\n+          \" generational - generational concurrent GC\")                     \\\n@@ -79,0 +154,10 @@\n+  product(uintx, ShenandoahExpeditePromotionsThreshold, 5, EXPERIMENTAL,    \\\n+          \"When Shenandoah expects to promote at least this percentage \"    \\\n+          \"of the young generation, trigger a young collection to \"         \\\n+          \"expedite these promotions.\")                                     \\\n+          range(0,100)                                                      \\\n+                                                                            \\\n+  product(uintx, ShenandoahExpediteMixedThreshold, 10, EXPERIMENTAL,        \\\n+          \"When there are this many old regions waiting to be collected, \"  \\\n+          \"trigger a mixed collection immediately.\")                        \\\n+                                                                            \\\n@@ -87,0 +172,14 @@\n+  product(uintx, ShenandoahOldGarbageThreshold, 15, EXPERIMENTAL,           \\\n+          \"How much garbage an old region has to contain before it would \"  \\\n+          \"be taken for collection.\")                                       \\\n+          range(0,100)                                                      \\\n+                                                                            \\\n+  product(uintx, ShenandoahIgnoreGarbageThreshold, 5, EXPERIMENTAL,         \\\n+          \"When less than this amount of garbage (as a percentage of \"      \\\n+          \"region size) exists within a region, the region will not be \"    \\\n+          \"added to the collection set, even when the heuristic has \"       \\\n+          \"chosen to aggressively add regions with less than \"              \\\n+          \"ShenandoahGarbageThreshold amount of garbage into the \"          \\\n+          \"collection set.\")                                                \\\n+          range(0,100)                                                      \\\n+                                                                            \\\n@@ -88,4 +187,6 @@\n-          \"How much heap should be free before some heuristics trigger the \"\\\n-          \"initial (learning) cycles. Affects cycle frequency on startup \"  \\\n-          \"and after drastic state changes, e.g. after degenerated\/full \"   \\\n-          \"GC cycles. In percents of (soft) max heap size.\")                \\\n+          \"When less than this amount of memory is free within the\"         \\\n+          \"heap or generation, trigger a learning cycle if we are \"         \\\n+          \"in learning mode.  Learning mode happens during initialization \" \\\n+          \"and following a drastic state change, such as following a \"      \\\n+          \"degenerated or Full GC cycle.  In percents of soft max \"         \\\n+          \"heap size.\")                                                     \\\n@@ -95,3 +196,5 @@\n-          \"How much heap should be free before most heuristics trigger the \"\\\n-          \"collection, even without other triggers. Provides the safety \"   \\\n-          \"margin for many heuristics. In percents of (soft) max heap size.\")\\\n+          \"Percentage of free heap memory (or young generation, in \"        \\\n+          \"generational mode) below which most heuristics trigger \"         \\\n+          \"collection independent of other triggers. Provides a safety \"    \\\n+          \"margin for many heuristics. In percents of (soft) max heap \"     \\\n+          \"size.\")                                                          \\\n@@ -118,1 +221,1 @@\n-  product(uintx, ShenandoahImmediateThreshold, 90, EXPERIMENTAL,            \\\n+  product(uintx, ShenandoahImmediateThreshold, 70, EXPERIMENTAL,            \\\n@@ -160,0 +263,10 @@\n+  product(uintx, ShenandoahGuaranteedOldGCInterval, 10*60*1000, EXPERIMENTAL, \\\n+          \"Run a collection of the old generation at least this often. \"    \\\n+          \"Heuristics may trigger collections more frequently. Time is in \" \\\n+          \"milliseconds. Setting this to 0 disables the feature.\")          \\\n+                                                                            \\\n+  product(uintx, ShenandoahGuaranteedYoungGCInterval, 5*60*1000,  EXPERIMENTAL,  \\\n+          \"Run a collection of the young generation at least this often. \"  \\\n+          \"Heuristics may trigger collections more frequently. Time is in \" \\\n+          \"milliseconds. Setting this to 0 disables the feature.\")          \\\n+                                                                            \\\n@@ -192,0 +305,1 @@\n+          range(1, 999)                                                     \\\n@@ -198,0 +312,1 @@\n+          range(1, 999)                                                     \\\n@@ -219,4 +334,12 @@\n-          \"How much of heap to reserve for evacuations. Larger values make \"\\\n-          \"GC evacuate more live objects on every cycle, while leaving \"    \\\n-          \"less headroom for application to allocate in. In percents of \"   \\\n-          \"total heap size.\")                                               \\\n+          \"How much of (young-generation) heap to reserve for \"             \\\n+          \"(young-generation) evacuations.  Larger values allow GC to \"     \\\n+          \"evacuate more live objects on every cycle, while leaving \"       \\\n+          \"less headroom for application to allocate while GC is \"          \\\n+          \"evacuating and updating references. This parameter is \"          \\\n+          \"consulted at the end of marking, before selecting the \"          \\\n+          \"collection set.  If available memory at this time is smaller \"   \\\n+          \"than the indicated reserve, the bound on collection set size is \"\\\n+          \"adjusted downward.  The size of a generational mixed \"           \\\n+          \"evacuation collection set (comprised of both young and old \"     \\\n+          \"regions) is also bounded by this parameter.  In percents of \"    \\\n+          \"total (young-generation) heap size.\")                            \\\n@@ -229,1 +352,18 @@\n-          \"GC cycle.\")                                                      \\\n+          \"GC cycle.  Smaller values increase the risk of evacuation \"      \\\n+          \"failures, which will trigger stop-the-world Full GC passes.\")    \\\n+          range(1.0,100.0)                                                  \\\n+                                                                            \\\n+  product(double, ShenandoahOldEvacWaste, 1.4, EXPERIMENTAL,                \\\n+          \"How much waste evacuations produce within the reserved space. \"  \\\n+          \"Larger values make evacuations more resilient against \"          \\\n+          \"evacuation conflicts, at expense of evacuating less on each \"    \\\n+          \"GC cycle.  Smaller values increase the risk of evacuation \"      \\\n+          \"failures, which will trigger stop-the-world Full GC passes.\")    \\\n+          range(1.0,100.0)                                                  \\\n+                                                                            \\\n+  product(double, ShenandoahPromoEvacWaste, 1.2, EXPERIMENTAL,              \\\n+          \"How much waste promotions produce within the reserved space. \"   \\\n+          \"Larger values make evacuations more resilient against \"          \\\n+          \"evacuation conflicts, at expense of promoting less on each \"     \\\n+          \"GC cycle.  Smaller values increase the risk of evacuation \"      \\\n+          \"failures, which will trigger stop-the-world Full GC passes.\")    \\\n@@ -238,0 +378,35 @@\n+  product(uintx, ShenandoahOldEvacRatioPercent, 75, EXPERIMENTAL,           \\\n+          \"The maximum proportion of evacuation from old-gen memory, \"      \\\n+          \"expressed as a percentage. The default value 75 denotes that no\" \\\n+          \"more than 75% of the collection set evacuation workload may be \" \\\n+          \"towards evacuation of old-gen heap regions. This limits both the\"\\\n+          \"promotion of aged regions and the compaction of existing old \"   \\\n+          \"regions.  A value of 75 denotes that the total evacuation work\"  \\\n+          \"may increase to up to four times the young gen evacuation work.\" \\\n+          \"A larger value allows quicker promotion and allows\"              \\\n+          \"a smaller number of mixed evacuations to process \"               \\\n+          \"the entire list of old-gen collection candidates at the cost \"   \\\n+          \"of an increased disruption of the normal cadence of young-gen \"  \\\n+          \"collections.  A value of 100 allows a mixed evacuation to \"      \\\n+          \"focus entirely on old-gen memory, allowing no young-gen \"        \\\n+          \"regions to be collected, likely resulting in subsequent \"        \\\n+          \"allocation failures because the allocation pool is not \"         \\\n+          \"replenished.  A value of 0 allows a mixed evacuation to\"         \\\n+          \"focus entirely on young-gen memory, allowing no old-gen \"        \\\n+          \"regions to be collected, likely resulting in subsequent \"        \\\n+          \"promotion failures and triggering of stop-the-world full GC \"    \\\n+          \"events.\")                                                        \\\n+          range(0,100)                                                      \\\n+                                                                            \\\n+  product(uintx, ShenandoahMinYoungPercentage, 20, EXPERIMENTAL,            \\\n+          \"The minimum percentage of the heap to use for the young \"        \\\n+          \"generation. Heuristics will not adjust the young generation \"    \\\n+          \"to be less than this.\")                                          \\\n+          range(0, 100)                                                     \\\n+                                                                            \\\n+  product(uintx, ShenandoahMaxYoungPercentage, 100, EXPERIMENTAL,           \\\n+          \"The maximum percentage of the heap to use for the young \"        \\\n+          \"generation. Heuristics will not adjust the young generation \"    \\\n+          \"to be more than this.\")                                          \\\n+          range(0, 100)                                                     \\\n+                                                                            \\\n@@ -289,0 +464,5 @@\n+  product(uintx, ShenandoahNoProgressThreshold, 5, EXPERIMENTAL,            \\\n+          \"After this number of consecutive Full GCs fail to make \"         \\\n+          \"progress, Shenandoah will raise out of memory errors. Note \"     \\\n+          \"that progress is determined by ShenandoahCriticalFreeThreshold\") \\\n+                                                                            \\\n@@ -307,0 +487,9 @@\n+  product(uintx, ShenandoahCoalesceChance, 0, DIAGNOSTIC,                   \\\n+          \"Testing: Abandon remaining mixed collections with this \"         \\\n+          \"likelihood. Following each mixed collection, abandon all \"       \\\n+          \"remaining mixed collection candidate regions with likelihood \"   \\\n+          \"ShenandoahCoalesceChance. Abandoning a mixed collection will \"   \\\n+          \"cause the old regions to be made parsable, rather than being \"   \\\n+          \"evacuated.\")                                                     \\\n+          range(0, 100)                                                     \\\n+                                                                            \\\n@@ -333,2 +522,3 @@\n-  product(bool, ShenandoahIUBarrier, false, DIAGNOSTIC,                     \\\n-          \"Turn on\/off I-U barriers barriers in Shenandoah\")                \\\n+  product(bool, ShenandoahCardBarrier, false, DIAGNOSTIC,                   \\\n+          \"Turn on\/off card-marking post-write barrier in Shenandoah: \"     \\\n+          \" true when ShenandoahGCMode is generational, false otherwise\")   \\\n@@ -351,2 +541,29 @@\n-\n-\/\/ end of GC_SHENANDOAH_FLAGS\n+  product(uintx, ShenandoahOldCompactionReserve, 8, EXPERIMENTAL,           \\\n+          \"During generational GC, prevent promotions from filling \"        \\\n+          \"this number of heap regions.  These regions are reserved \"       \\\n+          \"for the purpose of supporting compaction of old-gen \"            \\\n+          \"memory.  Otherwise, old-gen memory cannot be compacted.\")        \\\n+          range(0, 128)                                                     \\\n+                                                                            \\\n+  product(bool, ShenandoahAllowOldMarkingPreemption, true, DIAGNOSTIC,      \\\n+          \"Allow young generation collections to suspend concurrent\"        \\\n+          \" marking in the old generation.\")                                \\\n+                                                                            \\\n+  product(uintx, ShenandoahAgingCyclePeriod, 1, EXPERIMENTAL,               \\\n+          \"With generational mode, increment the age of objects and\"        \\\n+          \"regions each time this many young-gen GC cycles are completed.\") \\\n+                                                                            \\\n+  notproduct(bool, ShenandoahEnableCardStats, false,                        \\\n+          \"Enable statistics collection related to clean & dirty cards\")    \\\n+                                                                            \\\n+  notproduct(int, ShenandoahCardStatsLogInterval, 50,                       \\\n+          \"Log cumulative card stats every so many remembered set or \"      \\\n+          \"update refs scans\")                                              \\\n+                                                                            \\\n+  product(uintx, ShenandoahMinimumOldTimeMs, 100, EXPERIMENTAL,             \\\n+         \"Minimum amount of time in milliseconds to run old collections \"   \\\n+         \"before a young collection is allowed to run. This is intended \"   \\\n+         \"to prevent starvation of the old collector. Setting this to \"     \\\n+         \"0 will allow back to back young collections to run during old \"   \\\n+         \"collections.\")                                                    \\\n+  \/\/ end of GC_SHENANDOAH_FLAGS\n","filename":"src\/hotspot\/share\/gc\/shenandoah\/shenandoah_globals.hpp","additions":243,"deletions":26,"binary":false,"changes":269,"status":"modified"},{"patch":"@@ -28,0 +28,2 @@\n+#include \"gc\/shenandoah\/shenandoahGeneration.hpp\"\n+#include \"gc\/shenandoah\/shenandoahGenerationalHeap.hpp\"\n@@ -35,1 +37,1 @@\n-  volatile_nonstatic_field(ShenandoahHeap, _used,                  size_t)                            \\\n+  nonstatic_field(ShenandoahHeap, _global_generation,              ShenandoahGeneration*)             \\\n@@ -37,0 +39,1 @@\n+  volatile_nonstatic_field(ShenandoahGeneration, _used,            size_t)                            \\\n@@ -39,1 +42,1 @@\n-  nonstatic_field(ShenandoahHeapRegion, _state,                    ShenandoahHeapRegion::RegionState) \\\n+  volatile_nonstatic_field(ShenandoahHeapRegion, _state,           ShenandoahHeapRegion::RegionState) \\\n@@ -61,0 +64,1 @@\n+  declare_type(ShenandoahGenerationalHeap, ShenandoahHeap)                    \\\n@@ -65,0 +69,2 @@\n+  declare_toplevel_type(ShenandoahGeneration)                                 \\\n+  declare_toplevel_type(ShenandoahGeneration*)                                \\\n","filename":"src\/hotspot\/share\/gc\/shenandoah\/vmStructs_shenandoah.hpp","additions":8,"deletions":2,"binary":false,"changes":10,"status":"modified"},{"patch":"@@ -1181,0 +1181,17 @@\n+  <Event name=\"ShenandoahEvacuationInformation\" category=\"Java Virtual Machine, GC, Detailed\" label=\"Shenandoah Evacuation Information\" startTime=\"false\">\n+    <Field type=\"uint\" name=\"gcId\" label=\"GC Identifier\" relation=\"GcId\" \/>\n+    <Field type=\"ulong\" name=\"cSetRegions\" label=\"Collection Set Regions\" \/>\n+    <Field type=\"ulong\" contentType=\"bytes\" name=\"cSetUsedBefore\" label=\"Collection Set Before\" description=\"Memory usage before GC in the collection set regions\" \/>\n+    <Field type=\"ulong\" contentType=\"bytes\" name=\"cSetUsedAfter\" label=\"Collection Set After\" description=\"Memory usage after GC in the collection set regions\" \/>\n+    <Field type=\"ulong\" contentType=\"bytes\" name=\"collectedOld\" label=\"Collected Old\" description=\"Memory collected from old generation\" \/>\n+    <Field type=\"ulong\" contentType=\"bytes\" name=\"collectedPromoted\" label=\"Collected Promoted\" description=\"Memory collected from generation promotion\" \/>\n+    <Field type=\"ulong\" contentType=\"bytes\" name=\"collectedYoung\" label=\"Collection Young\" description=\"Memory collected from young generation\" \/>\n+    <Field type=\"ulong\" name=\"regionsPromotedHumongous\" label=\"Regions Promoted Humongous\" \/>\n+    <Field type=\"ulong\" name=\"regionsPromotedRegular\" label=\"Regions Promoted Regular\" \/>\n+    <Field type=\"ulong\" contentType=\"bytes\" name=\"regularPromotedGarbage\" label=\"Regular Promoted Garbage\" description=\"Garbage memory of in place promoted regular regions\" \/>\n+    <Field type=\"ulong\" contentType=\"bytes\" name=\"regularPromotedFree\" label=\"Regular Promoted Free\" description=\"Free memory of in place promoted regular regions\" \/>\n+    <Field type=\"ulong\" name=\"freeRegions\" label=\"Free Regions\" description=\"Free regions prior to collection\" \/>\n+    <Field type=\"ulong\" name=\"regionsImmediate\" label=\"Regions Immediate\" \/>\n+    <Field type=\"ulong\" contentType=\"bytes\" name=\"immediateBytes\" label=\"Immediate Bytes\" \/>\n+  <\/Event>\n+  \n","filename":"src\/hotspot\/share\/jfr\/metadata\/metadata.xml","additions":17,"deletions":0,"binary":false,"changes":17,"status":"modified"},{"patch":"@@ -326,1 +326,0 @@\n-shmacro(ShenandoahIUBarrier)\n","filename":"src\/hotspot\/share\/opto\/classes.hpp","additions":0,"deletions":1,"binary":false,"changes":1,"status":"modified"},{"patch":"@@ -105,1 +105,0 @@\n-  LoopOptsShenandoahPostExpand,\n","filename":"src\/hotspot\/share\/opto\/compile.hpp","additions":0,"deletions":1,"binary":false,"changes":1,"status":"modified"},{"patch":"@@ -413,1 +413,2 @@\n-    return Universe::heap()->is_in(p);\n+    ShenandoahHeap* sh = ShenandoahHeap::heap();\n+    return sh->mode()->is_generational() ?  sh->is_in_old(p) : sh->is_in(p);\n","filename":"src\/hotspot\/share\/prims\/whitebox.cpp","additions":2,"deletions":1,"binary":false,"changes":3,"status":"modified"},{"patch":"@@ -0,0 +1,59 @@\n+\/*\n+ * * Copyright Amazon.com Inc. or its affiliates. All Rights Reserved.\n+ * DO NOT ALTER OR REMOVE COPYRIGHT NOTICES OR THIS FILE HEADER.\n+ *\n+ * This code is free software; you can redistribute it and\/or modify it\n+ * under the terms of the GNU General Public License version 2 only, as\n+ * published by the Free Software Foundation.\n+ *\n+ * This code is distributed in the hope that it will be useful, but WITHOUT\n+ * ANY WARRANTY; without even the implied warranty of MERCHANTABILITY or\n+ * FITNESS FOR A PARTICULAR PURPOSE.  See the GNU General Public License\n+ * version 2 for more details (a copy is included in the LICENSE file that\n+ * accompanied this code).\n+ *\n+ * You should have received a copy of the GNU General Public License version\n+ * 2 along with this work; if not, write to the Free Software Foundation,\n+ * Inc., 51 Franklin St, Fifth Floor, Boston, MA 02110-1301 USA.\n+ *\n+ * Please contact Oracle, 500 Oracle Parkway, Redwood Shores, CA 94065 USA\n+ * or visit www.oracle.com if you need additional information or have any\n+ * questions.\n+ *\n+ *\/\n+\n+package sun.jvm.hotspot.gc.shenandoah;\n+\n+import sun.jvm.hotspot.utilities.Observable;\n+import sun.jvm.hotspot.utilities.Observer;\n+\n+import sun.jvm.hotspot.debugger.Address;\n+import sun.jvm.hotspot.runtime.VM;\n+import sun.jvm.hotspot.runtime.VMObject;\n+import sun.jvm.hotspot.types.CIntegerField;\n+import sun.jvm.hotspot.types.Type;\n+import sun.jvm.hotspot.types.TypeDataBase;\n+\n+public class ShenandoahGeneration extends VMObject {\n+    private static CIntegerField used;\n+    static {\n+        VM.registerVMInitializedObserver(new Observer() {\n+            public void update(Observable o, Object data) {\n+                initialize(VM.getVM().getTypeDataBase());\n+            }\n+        });\n+    }\n+\n+    private static synchronized void initialize(TypeDataBase db) {\n+        Type type = db.lookupType(\"ShenandoahGeneration\");\n+        used = type.getCIntegerField(\"_used\");\n+    }\n+\n+    public ShenandoahGeneration(Address addr) {\n+        super(addr);\n+    }\n+\n+    public long used() {\n+        return used.getValue(addr);\n+    }\n+}\n","filename":"src\/jdk.hotspot.agent\/share\/classes\/sun\/jvm\/hotspot\/gc\/shenandoah\/ShenandoahGeneration.java","additions":59,"deletions":0,"binary":false,"changes":59,"status":"added"},{"patch":"@@ -0,0 +1,33 @@\n+\/*\n+ * Copyright Amazon.com Inc. or its affiliates. All Rights Reserved.\n+ * DO NOT ALTER OR REMOVE COPYRIGHT NOTICES OR THIS FILE HEADER.\n+ *\n+ * This code is free software; you can redistribute it and\/or modify it\n+ * under the terms of the GNU General Public License version 2 only, as\n+ * published by the Free Software Foundation.\n+ *\n+ * This code is distributed in the hope that it will be useful, but WITHOUT\n+ * ANY WARRANTY; without even the implied warranty of MERCHANTABILITY or\n+ * FITNESS FOR A PARTICULAR PURPOSE.  See the GNU General Public License\n+ * version 2 for more details (a copy is included in the LICENSE file that\n+ * accompanied this code).\n+ *\n+ * You should have received a copy of the GNU General Public License version\n+ * 2 along with this work; if not, write to the Free Software Foundation,\n+ * Inc., 51 Franklin St, Fifth Floor, Boston, MA 02110-1301 USA.\n+ *\n+ * Please contact Oracle, 500 Oracle Parkway, Redwood Shores, CA 94065 USA\n+ * or visit www.oracle.com if you need additional information or have any\n+ * questions.\n+ *\n+ *\/\n+\n+package sun.jvm.hotspot.gc.shenandoah;\n+\n+import sun.jvm.hotspot.debugger.Address;\n+\n+public class ShenandoahGenerationalHeap extends ShenandoahHeap {\n+    public ShenandoahGenerationalHeap(Address addr) {\n+        super(addr);\n+    }\n+}\n","filename":"src\/jdk.hotspot.agent\/share\/classes\/sun\/jvm\/hotspot\/gc\/shenandoah\/ShenandoahGenerationalHeap.java","additions":33,"deletions":0,"binary":false,"changes":33,"status":"added"},{"patch":"@@ -46,1 +46,1 @@\n-    private static CIntegerField used;\n+    private static AddressField  globalGeneration;\n@@ -63,1 +63,1 @@\n-        used = type.getCIntegerField(\"_used\");\n+        globalGeneration = type.getAddressField(\"_global_generation\");\n@@ -92,1 +92,3 @@\n-        return used.getValue(addr);\n+        Address globalGenerationAddress = globalGeneration.getValue(addr);\n+        ShenandoahGeneration global = VMObjectFactory.newObject(ShenandoahGeneration.class, globalGenerationAddress);\n+        return global.used();\n","filename":"src\/jdk.hotspot.agent\/share\/classes\/sun\/jvm\/hotspot\/gc\/shenandoah\/ShenandoahHeap.java","additions":5,"deletions":3,"binary":false,"changes":8,"status":"modified"},{"patch":"@@ -38,0 +38,1 @@\n+import sun.jvm.hotspot.gc.shenandoah.ShenandoahGenerationalHeap;\n@@ -93,0 +94,1 @@\n+    addHeapTypeIfInDB(db, ShenandoahGenerationalHeap.class);\n","filename":"src\/jdk.hotspot.agent\/share\/classes\/sun\/jvm\/hotspot\/memory\/Universe.java","additions":2,"deletions":0,"binary":false,"changes":2,"status":"modified"},{"patch":"@@ -529,0 +529,4 @@\n+    <event name=\"jdk.ShenandoahEvacuationInformation\">\n+      <setting name=\"enabled\" control=\"gc-enabled-high\">false<\/setting>\n+    <\/event>\n+\n","filename":"src\/jdk.jfr\/share\/conf\/jfr\/default.jfc","additions":4,"deletions":0,"binary":false,"changes":4,"status":"modified"},{"patch":"@@ -529,0 +529,4 @@\n+    <event name=\"jdk.ShenandoahEvacuationInformation\">\n+      <setting name=\"enabled\" control=\"gc-enabled-high\">false<\/setting>\n+    <\/event>\n+\n","filename":"src\/jdk.jfr\/share\/conf\/jfr\/profile.jfc","additions":4,"deletions":0,"binary":false,"changes":4,"status":"modified"},{"patch":"@@ -0,0 +1,160 @@\n+\/*\n+ * Copyright Amazon.com Inc. or its affiliates. All Rights Reserved.\n+ * DO NOT ALTER OR REMOVE COPYRIGHT NOTICES OR THIS FILE HEADER.\n+ *\n+ * This code is free software; you can redistribute it and\/or modify it\n+ * under the terms of the GNU General Public License version 2 only, as\n+ * published by the Free Software Foundation.\n+ *\n+ * This code is distributed in the hope that it will be useful, but WITHOUT\n+ * ANY WARRANTY; without even the implied warranty of MERCHANTABILITY or\n+ * FITNESS FOR A PARTICULAR PURPOSE.  See the GNU General Public License\n+ * version 2 for more details (a copy is included in the LICENSE file that\n+ * accompanied this code).\n+ *\n+ * You should have received a copy of the GNU General Public License version\n+ * 2 along with this work; if not, write to the Free Software Foundation,\n+ * Inc., 51 Franklin St, Fifth Floor, Boston, MA 02110-1301 USA.\n+ *\n+ * Please contact Oracle, 500 Oracle Parkway, Redwood Shores, CA 94065 USA\n+ * or visit www.oracle.com if you need additional information or have any\n+ * questions.\n+ *\n+ *\/\n+#include \"precompiled.hpp\"\n+\n+#include \"gc\/shenandoah\/shenandoahAgeCensus.hpp\"\n+#include \"unittest.hpp\"\n+\n+class ShenandoahAgeCensusTest : public ::testing::Test {\n+protected:\n+  static constexpr size_t MinimumPopulationSize = 4*K;\n+  static constexpr size_t InitialPopulationSize = MinimumPopulationSize * 1000;\n+\n+  size_t _cohorts_count = ShenandoahAgeCensus::MAX_COHORTS;\n+  double _mortality_rates[ShenandoahAgeCensus::MAX_COHORTS];\n+  size_t _cohort_populations[ShenandoahAgeCensus::MAX_COHORTS];\n+\n+  ShenandoahAgeCensusTest()\n+  : _mortality_rates{0.9, 0.7, 0.5, 0.3, 0.09, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0}\n+  {\n+    build_cohort_populations(_mortality_rates, _cohort_populations, _cohorts_count);\n+  }\n+\n+  static void add_population(ShenandoahAgeCensus& census, const uint age, const size_t population_words) {\n+    CENSUS_NOISE(census.add(age, 0, 0, population_words, 0));\n+    NO_CENSUS_NOISE(census.add(age, 0, population_words, 0));\n+  }\n+\n+  void update(ShenandoahAgeCensus& census, size_t cohorts) const {\n+    for (uint i = 1; i < cohorts; i++) {\n+      add_population(census, i, _cohort_populations[i]);\n+    }\n+    census.update_census(_cohort_populations[0]);\n+  }\n+\n+  void update(ShenandoahAgeCensus& census) const {\n+    update(census, _cohorts_count);\n+  }\n+\n+  size_t get_total_population_older_than(const size_t min_cohort_age) const {\n+    size_t total = 0;\n+    for (size_t i = 0; i < _cohorts_count; i++) {\n+      if (i >= min_cohort_age) {\n+        total += _cohort_populations[i];\n+      }\n+    }\n+    return total;\n+  }\n+\n+  void promote_all_tenurable(const size_t tenuring_threshold) {\n+    for (size_t i = 0; i < _cohorts_count; i++) {\n+      if (i > tenuring_threshold) {\n+        _cohort_populations[i] = 0;\n+      }\n+    }\n+  }\n+\n+  static void build_cohort_populations(const double mortality_rates[], size_t cohort_populations[], const size_t cohorts) {\n+    cohort_populations[0] = InitialPopulationSize;\n+    for (size_t i = 1; i < cohorts; i++) {\n+      cohort_populations[i] = cohort_populations[i - 1] * (1.0 - mortality_rates[i - 1]);\n+    }\n+  }\n+};\n+\n+TEST_F(ShenandoahAgeCensusTest, initialize) {\n+  const ShenandoahAgeCensus census(1);\n+  EXPECT_EQ(census.tenuring_threshold(), ShenandoahAgeCensus::MAX_COHORTS);\n+}\n+\n+TEST_F(ShenandoahAgeCensusTest, ignore_small_populations) {\n+  \/\/ Small populations are ignored so we do not return early before reaching the youngest cohort.\n+  ShenandoahAgeCensus census(1);\n+  add_population(census,1, 32);\n+  add_population(census,1, 32);\n+  census.update_census(64);\n+  EXPECT_EQ(1u, census.tenuring_threshold());\n+}\n+\n+TEST_F(ShenandoahAgeCensusTest, find_high_mortality_rate) {\n+  ShenandoahAgeCensus census(1);\n+\n+  \/\/ Initial threshold, no data\n+  EXPECT_EQ(16u, census.tenuring_threshold());\n+\n+  \/\/ Provide population data for 1st cohort. Previous epoch has no population data so our\n+  \/\/ algorithm skips over all cohorts, leaving tenuring threshold at 1.\n+  update(census, 1);\n+  EXPECT_EQ(1u, census.tenuring_threshold());\n+\n+  \/\/ Mortality rate of 1st cohort at age 1 is 0.9, we don't want to promote here. Move threshold to 2.\n+  update(census, 2);\n+  EXPECT_EQ(2u, census.tenuring_threshold());\n+\n+  \/\/ Mortality rate of 1st cohort at age 2 is 0.7, we don't want to promote here. Move threshold to 3.\n+  update(census, 3);\n+  EXPECT_EQ(3u, census.tenuring_threshold());\n+\n+  \/\/ Mortality rate of 1st cohort at age 3 is 0.5, we don't want to promote here. Move threshold to 4.\n+  update(census, 4);\n+  EXPECT_EQ(4u, census.tenuring_threshold());\n+\n+  \/\/ Mortality rate of 1st cohort at age 4 is 0.3, we don't want to promote here. Move threshold to 5.\n+  update(census, 5);\n+  EXPECT_EQ(5u, census.tenuring_threshold());\n+\n+  \/\/ Mortality rate of 1st cohort at age 5 is 0.09, this is less than the mortality rate threshold. It\n+  \/\/ is okay to tenure objects older than 5 now. Keep threshold at 5.\n+  update(census, 6);\n+  EXPECT_EQ(5u, census.tenuring_threshold());\n+\n+  \/\/ Mortality rate at this age is 0. Keep tenuring threshold at 5.\n+  update(census, 7);\n+  EXPECT_EQ(5u, census.tenuring_threshold());\n+}\n+\n+TEST_F(ShenandoahAgeCensusTest, ignore_mortality_caused_by_promotions) {\n+  ShenandoahAgeCensus census(1);\n+\n+  \/\/ Simulate a sequence of censuses with the same mortality rate. Each one will see a\n+  \/\/ mortality rate above the tenuring threshold and raise the tenuring threshold by one.\n+  update(census, 1);\n+  update(census, 2);\n+  update(census, 3);\n+  update(census, 4);\n+  update(census, 5);\n+\n+  EXPECT_EQ(5u, census.tenuring_threshold());\n+\n+  \/\/ Simulate the effect of promoting all objects above the tenuring threshold\n+  \/\/ out of the young generation. This will look like a very high (100%) mortality\n+  \/\/ rate for these cohorts. However, we do _not_ want to raise the threshold in\n+  \/\/ this case because these objects haven't really \"died\", they have just been\n+  \/\/ tenured.\n+  promote_all_tenurable(census.tenuring_threshold());\n+  update(census);\n+\n+  \/\/ We want this to stay at 5 - the mortality in 1st cohort at age 6 was caused by expected promotions.\n+  EXPECT_EQ(5u, census.tenuring_threshold());\n+}\n","filename":"test\/hotspot\/gtest\/gc\/shenandoah\/test_shenandoahAgeCensus.cpp","additions":160,"deletions":0,"binary":false,"changes":160,"status":"added"},{"patch":"@@ -0,0 +1,74 @@\n+\/*\n+ * Copyright Amazon.com Inc. or its affiliates. All Rights Reserved.\n+ * DO NOT ALTER OR REMOVE COPYRIGHT NOTICES OR THIS FILE HEADER.\n+ *\n+ * This code is free software; you can redistribute it and\/or modify it\n+ * under the terms of the GNU General Public License version 2 only, as\n+ * published by the Free Software Foundation.\n+ *\n+ * This code is distributed in the hope that it will be useful, but WITHOUT\n+ * ANY WARRANTY; without even the implied warranty of MERCHANTABILITY or\n+ * FITNESS FOR A PARTICULAR PURPOSE.  See the GNU General Public License\n+ * version 2 for more details (a copy is included in the LICENSE file that\n+ * accompanied this code).\n+ *\n+ * You should have received a copy of the GNU General Public License version\n+ * 2 along with this work; if not, write to the Free Software Foundation,\n+ * Inc., 51 Franklin St, Fifth Floor, Boston, MA 02110-1301 USA.\n+ *\n+ * Please contact Oracle, 500 Oracle Parkway, Redwood Shores, CA 94065 USA\n+ * or visit www.oracle.com if you need additional information or have any\n+ * questions.\n+ *\n+ *\/\n+\n+#include \"precompiled.hpp\"\n+\n+#include \"gc\/shenandoah\/shenandoahCollectorPolicy.hpp\"\n+#include \"unittest.hpp\"\n+\n+TEST(ShenandoahCollectorPolicyTest, track_degen_cycles_sanity) {\n+  ShenandoahCollectorPolicy policy;\n+  EXPECT_EQ(policy.consecutive_degenerated_gc_count(), 0UL);\n+  EXPECT_EQ(policy.generational_should_upgrade_degenerated_gc(), false);\n+}\n+\n+TEST(ShenandoahCollectorPolicyTest, track_degen_cycles_no_upgrade) {\n+  ShenandoahCollectorPolicy policy;\n+  policy.record_degenerated(true, true, true);\n+  policy.record_degenerated(true, true, true);\n+  EXPECT_EQ(policy.consecutive_degenerated_gc_count(), 2UL);\n+  EXPECT_EQ(policy.generational_should_upgrade_degenerated_gc(), false);\n+}\n+\n+TEST(ShenandoahCollectorPolicyTest, track_degen_cycles_upgrade) {\n+  ShenandoahCollectorPolicy policy;\n+  policy.record_degenerated(true, true, false);\n+  policy.record_degenerated(true, true, false);\n+  EXPECT_EQ(policy.consecutive_degenerated_gc_count(), 2UL);\n+  EXPECT_EQ(policy.generational_should_upgrade_degenerated_gc(), true);\n+}\n+\n+TEST(ShenandoahCollectorPolicyTest, track_degen_cycles_reset_progress) {\n+  ShenandoahCollectorPolicy policy;\n+  policy.record_degenerated(true, true, false);\n+  policy.record_degenerated(true, true, true);\n+  EXPECT_EQ(policy.consecutive_degenerated_gc_count(), 2UL);\n+  EXPECT_EQ(policy.generational_should_upgrade_degenerated_gc(), false);\n+}\n+\n+TEST(ShenandoahCollectorPolicyTest, track_degen_cycles_full_reset) {\n+  ShenandoahCollectorPolicy policy;\n+  policy.record_degenerated(true, true, false);\n+  policy.record_success_full();\n+  EXPECT_EQ(policy.consecutive_degenerated_gc_count(), 0UL);\n+  EXPECT_EQ(policy.generational_should_upgrade_degenerated_gc(), false);\n+}\n+\n+TEST(ShenandoahCollectorPolicyTest, track_degen_cycles_reset) {\n+  ShenandoahCollectorPolicy policy;\n+  policy.record_degenerated(true, true, false);\n+  policy.record_success_concurrent(true, true);\n+  EXPECT_EQ(policy.consecutive_degenerated_gc_count(), 0UL);\n+  EXPECT_EQ(policy.generational_should_upgrade_degenerated_gc(), false);\n+}\n","filename":"test\/hotspot\/gtest\/gc\/shenandoah\/test_shenandoahCollectorPolicy.cpp","additions":74,"deletions":0,"binary":false,"changes":74,"status":"added"},{"patch":"@@ -2,0 +2,1 @@\n+ * Copyright (c) 2022, 2023, Oracle and\/or its affiliates. All rights reserved.\n@@ -3,1 +4,0 @@\n- * Copyright (c) 2022, Oracle and\/or its affiliates. All rights reserved.\n@@ -37,1 +37,25 @@\n-  HdrSeq seq;\n+  const double err = 0.5;\n+\n+  HdrSeq seq1;\n+  HdrSeq seq2;\n+  HdrSeq seq3;\n+\n+  void print() {\n+    if (seq1.num() > 0) {\n+      print(seq1, \"seq1\");\n+    }\n+    if (seq2.num() > 0) {\n+      print(seq2, \"seq2\");\n+    }\n+    if (seq3.num() > 0) {\n+      print(seq3, \"seq3\");\n+    }\n+  }\n+\n+  void print(HdrSeq& seq, const char* msg) {\n+    std::cout << \"[\";\n+    for (int i = 0; i <= 100; i += 10) {\n+      std::cout << \"\\t p\" << i << \":\" << seq.percentile(i);\n+    }\n+    std::cout << \"\\t] : \" << msg << \"\\n\";\n+  }\n@@ -41,2 +65,1 @@\n- protected:\n-  const double err = 0.5;\n+ public:\n@@ -44,3 +67,3 @@\n-    seq.add(0);\n-    seq.add(1);\n-    seq.add(10);\n+    seq1.add(0);\n+    seq1.add(1);\n+    seq1.add(10);\n@@ -48,1 +71,12 @@\n-      seq.add(100);\n+      seq1.add(100);\n+    }\n+    ShenandoahNumberSeqTest::print();\n+  }\n+};\n+\n+class ShenandoahNumberSeqMergeTest: public ShenandoahNumberSeqTest {\n+ public:\n+  ShenandoahNumberSeqMergeTest() {\n+    for (int i = 0; i < 80; i++) {\n+      seq1.add(1);\n+      seq3.add(1);\n@@ -50,8 +84,6 @@\n-    std::cout << \" p0 = \" << seq.percentile(0);\n-    std::cout << \" p10 = \" << seq.percentile(10);\n-    std::cout << \" p20 = \" << seq.percentile(20);\n-    std::cout << \" p30 = \" << seq.percentile(30);\n-    std::cout << \" p50 = \" << seq.percentile(50);\n-    std::cout << \" p80 = \" << seq.percentile(80);\n-    std::cout << \" p90 = \" << seq.percentile(90);\n-    std::cout << \" p100 = \" << seq.percentile(100);\n+\n+    for (int i = 0; i < 20; i++) {\n+      seq2.add(100);\n+      seq3.add(100);\n+    }\n+    ShenandoahNumberSeqTest::print();\n@@ -62,1 +94,1 @@\n-  EXPECT_EQ(seq.maximum(), 100);\n+  EXPECT_EQ(seq1.maximum(), 100);\n@@ -66,1 +98,1 @@\n-  EXPECT_EQ(0, seq.percentile(0));\n+  EXPECT_EQ(0, seq1.percentile(0));\n@@ -70,8 +102,61 @@\n-  EXPECT_NEAR(0, seq.percentile(10), err);\n-  EXPECT_NEAR(1, seq.percentile(20), err);\n-  EXPECT_NEAR(10, seq.percentile(30), err);\n-  EXPECT_NEAR(100, seq.percentile(40), err);\n-  EXPECT_NEAR(100, seq.percentile(50), err);\n-  EXPECT_NEAR(100, seq.percentile(75), err);\n-  EXPECT_NEAR(100, seq.percentile(90), err);\n-  EXPECT_NEAR(100, seq.percentile(100), err);\n+  EXPECT_NEAR(0, seq1.percentile(10), err);\n+  EXPECT_NEAR(1, seq1.percentile(20), err);\n+  EXPECT_NEAR(10, seq1.percentile(30), err);\n+  EXPECT_NEAR(100, seq1.percentile(40), err);\n+  EXPECT_NEAR(100, seq1.percentile(50), err);\n+  EXPECT_NEAR(100, seq1.percentile(75), err);\n+  EXPECT_NEAR(100, seq1.percentile(90), err);\n+  EXPECT_NEAR(100, seq1.percentile(100), err);\n+}\n+\n+TEST_VM_F(BasicShenandoahNumberSeqTest, clear_test) {\n+  HdrSeq test;\n+  test.add(1);\n+\n+  EXPECT_NE(test.num(), 0);\n+  EXPECT_NE(test.sum(), 0);\n+  EXPECT_NE(test.maximum(), 0);\n+  EXPECT_NE(test.avg(), 0);\n+  EXPECT_EQ(test.sd(), 0);\n+  EXPECT_NE(test.davg(), 0);\n+  EXPECT_EQ(test.dvariance(), 0);\n+  for (int i = 0; i <= 100; i += 10) {\n+    EXPECT_NE(test.percentile(i), 0);\n+  }\n+\n+  test.clear();\n+\n+  EXPECT_EQ(test.num(), 0);\n+  EXPECT_EQ(test.sum(), 0);\n+  EXPECT_EQ(test.maximum(), 0);\n+  EXPECT_EQ(test.avg(), 0);\n+  EXPECT_EQ(test.sd(), 0);\n+  EXPECT_EQ(test.davg(), 0);\n+  EXPECT_EQ(test.dvariance(), 0);\n+  for (int i = 0; i <= 100; i += 10) {\n+    EXPECT_EQ(test.percentile(i), 0);\n+  }\n+}\n+\n+TEST_VM_F(ShenandoahNumberSeqMergeTest, merge_test) {\n+  EXPECT_EQ(seq1.num(), 80);\n+  EXPECT_EQ(seq2.num(), 20);\n+  EXPECT_EQ(seq3.num(), 100);\n+\n+  HdrSeq merged;\n+  merged.add(seq1);\n+  merged.add(seq2);\n+\n+  EXPECT_EQ(merged.num(), seq3.num());\n+\n+  EXPECT_EQ(merged.maximum(), seq3.maximum());\n+  EXPECT_EQ(merged.percentile(0), seq3.percentile(0));\n+  for (int i = 0; i <= 100; i += 10) {\n+    EXPECT_NEAR(merged.percentile(i), seq3.percentile(i), err);\n+  }\n+  EXPECT_NEAR(merged.avg(), seq3.avg(), err);\n+  EXPECT_NEAR(merged.sd(),  seq3.sd(),  err);\n+\n+  \/\/ These are not implemented\n+  EXPECT_TRUE(isnan(merged.davg()));\n+  EXPECT_TRUE(isnan(merged.dvariance()));\n","filename":"test\/hotspot\/gtest\/gc\/shenandoah\/test_shenandoahNumberSeq.cpp","additions":111,"deletions":26,"binary":false,"changes":137,"status":"modified"},{"patch":"@@ -0,0 +1,200 @@\n+\/*\n+ * Copyright Amazon.com Inc. or its affiliates. All Rights Reserved.\n+ * DO NOT ALTER OR REMOVE COPYRIGHT NOTICES OR THIS FILE HEADER.\n+ *\n+ * This code is free software; you can redistribute it and\/or modify it\n+ * under the terms of the GNU General Public License version 2 only, as\n+ * published by the Free Software Foundation.\n+ *\n+ * This code is distributed in the hope that it will be useful, but WITHOUT\n+ * ANY WARRANTY; without even the implied warranty of MERCHANTABILITY or\n+ * FITNESS FOR A PARTICULAR PURPOSE.  See the GNU General Public License\n+ * version 2 for more details (a copy is included in the LICENSE file that\n+ * accompanied this code).\n+ *\n+ * You should have received a copy of the GNU General Public License version\n+ * 2 along with this work; if not, write to the Free Software Foundation,\n+ * Inc., 51 Franklin St, Fifth Floor, Boston, MA 02110-1301 USA.\n+ *\n+ * Please contact Oracle, 500 Oracle Parkway, Redwood Shores, CA 94065 USA\n+ * or visit www.oracle.com if you need additional information or have any\n+ * questions.\n+ *\/\n+\n+#include \"precompiled.hpp\"\n+#include \"unittest.hpp\"\n+\n+#include \"gc\/shenandoah\/shenandoahHeap.inline.hpp\"\n+#include \"gc\/shenandoah\/mode\/shenandoahMode.hpp\"\n+#include \"gc\/shenandoah\/shenandoahOldGeneration.hpp\"\n+#include \"gc\/shenandoah\/shenandoahThreadLocalData.hpp\"\n+\n+#define SKIP_IF_NOT_SHENANDOAH() \\\n+  if (!(UseShenandoahGC && ShenandoahHeap::heap()->mode()->is_generational())) {                 \\\n+    tty->print_cr(\"skipped (run with -XX:+UseShenandoahGC -XX:ShenandoahGCMode=generational)\");  \\\n+    return;                                                                                      \\\n+  }\n+\n+\n+class ShenandoahOldGenerationTest : public ::testing::Test {\n+protected:\n+  static const size_t INITIAL_PLAB_SIZE;\n+  static const size_t INITIAL_PLAB_PROMOTED;\n+\n+  ShenandoahOldGeneration* old;\n+\n+  ShenandoahOldGenerationTest()\n+    : old(nullptr)\n+  {\n+  }\n+\n+  void SetUp() override {\n+    SKIP_IF_NOT_SHENANDOAH();\n+\n+    ShenandoahHeap::heap()->lock()->lock(false);\n+\n+    old = new ShenandoahOldGeneration(8, 1024 * 1024);\n+    old->set_promoted_reserve(512 * HeapWordSize);\n+    old->expend_promoted(256 * HeapWordSize);\n+    old->set_evacuation_reserve(512 * HeapWordSize);\n+\n+    Thread* thread = Thread::current();\n+    ShenandoahThreadLocalData::reset_plab_promoted(thread);\n+    ShenandoahThreadLocalData::disable_plab_promotions(thread);\n+    ShenandoahThreadLocalData::set_plab_actual_size(thread, INITIAL_PLAB_SIZE);\n+    ShenandoahThreadLocalData::add_to_plab_promoted(thread, INITIAL_PLAB_PROMOTED);\n+  }\n+\n+  void TearDown() override {\n+    if (UseShenandoahGC) {\n+      ShenandoahHeap::heap()->lock()->unlock();\n+      delete old;\n+    }\n+  }\n+\n+  static bool promotions_enabled() {\n+    return ShenandoahThreadLocalData::allow_plab_promotions(Thread::current());\n+  }\n+\n+  static size_t plab_size() {\n+    return ShenandoahThreadLocalData::get_plab_actual_size(Thread::current());\n+  }\n+\n+  static size_t plab_promoted() {\n+    return ShenandoahThreadLocalData::get_plab_promoted(Thread::current());\n+  }\n+};\n+\n+const size_t ShenandoahOldGenerationTest::INITIAL_PLAB_SIZE = 42;\n+const size_t ShenandoahOldGenerationTest::INITIAL_PLAB_PROMOTED = 128;\n+\n+TEST_VM_F(ShenandoahOldGenerationTest, test_can_promote) {\n+  SKIP_IF_NOT_SHENANDOAH();\n+  EXPECT_TRUE(old->can_promote(128 * HeapWordSize)) << \"Should have room to promote\";\n+  EXPECT_FALSE(old->can_promote(384 * HeapWordSize)) << \"Should not have room to promote\";\n+}\n+\n+TEST_VM_F(ShenandoahOldGenerationTest, test_can_allocate_plab_for_promotion) {\n+  SKIP_IF_NOT_SHENANDOAH();\n+  ShenandoahAllocRequest req = ShenandoahAllocRequest::for_plab(128, 128);\n+  EXPECT_TRUE(old->can_allocate(req)) << \"Should have room to promote\";\n+}\n+\n+TEST_VM_F(ShenandoahOldGenerationTest, test_can_allocate_plab_for_evacuation) {\n+  SKIP_IF_NOT_SHENANDOAH();\n+  ShenandoahAllocRequest req = ShenandoahAllocRequest::for_plab(384, 384);\n+  EXPECT_FALSE(old->can_promote(req.size() * HeapWordSize)) << \"No room for promotions\";\n+  EXPECT_TRUE(old->can_allocate(req)) << \"Should have room to evacuate\";\n+}\n+\n+TEST_VM_F(ShenandoahOldGenerationTest, test_cannot_allocate_plab) {\n+  SKIP_IF_NOT_SHENANDOAH();\n+  \/\/ Simulate having exhausted the evacuation reserve when request is too big to be promoted\n+  old->set_evacuation_reserve(0);\n+  ShenandoahAllocRequest req = ShenandoahAllocRequest::for_plab(384, 384);\n+  EXPECT_FALSE(old->can_allocate(req)) << \"No room for promotions or evacuations\";\n+}\n+\n+TEST_VM_F(ShenandoahOldGenerationTest, test_can_allocate_for_shared_evacuation) {\n+  SKIP_IF_NOT_SHENANDOAH();\n+  ShenandoahAllocRequest req = ShenandoahAllocRequest::for_shared_gc(768, ShenandoahAffiliation::OLD_GENERATION, false);\n+  EXPECT_FALSE(old->can_promote(req.size() * HeapWordSize)) << \"No room for promotion\";\n+  EXPECT_TRUE(old->can_allocate(req)) << \"Should have room to evacuate shared (even though evacuation reserve is smaller than request)\";\n+}\n+\n+TEST_VM_F(ShenandoahOldGenerationTest, test_cannot_allocate_for_shared_promotion) {\n+  SKIP_IF_NOT_SHENANDOAH();\n+  ShenandoahAllocRequest req = ShenandoahAllocRequest::for_shared_gc(768, ShenandoahAffiliation::OLD_GENERATION, true);\n+  EXPECT_FALSE(old->can_promote(req.size() * HeapWordSize)) << \"No room for promotion\";\n+  EXPECT_FALSE(old->can_allocate(req)) << \"No room to promote, should fall back to evacuation in young gen\";\n+}\n+\n+TEST_VM_F(ShenandoahOldGenerationTest, test_expend_promoted) {\n+  SKIP_IF_NOT_SHENANDOAH();\n+  ShenandoahAllocRequest req = ShenandoahAllocRequest::for_plab(128, 128);\n+\n+  \/\/ simulate the allocation\n+  req.set_actual_size(128);\n+\n+  size_t actual_size = req.actual_size() * HeapWordSize;\n+  EXPECT_TRUE(old->can_promote(actual_size)) << \"Should have room for promotion\";\n+\n+  size_t expended_before = old->get_promoted_expended();\n+  old->configure_plab_for_current_thread(req);\n+  size_t expended_after = old->get_promoted_expended();\n+  EXPECT_EQ(expended_before + actual_size, expended_after) << \"Should expend promotion reserve\";\n+  EXPECT_EQ(plab_promoted(), 0UL) << \"Nothing promoted yet\";\n+  EXPECT_EQ(plab_size(), actual_size) << \"New plab should be able to hold this much promotion\";\n+  EXPECT_TRUE(promotions_enabled()) << \"Plab should be available for promotions\";\n+}\n+\n+TEST_VM_F(ShenandoahOldGenerationTest, test_actual_size_exceeds_promotion_reserve) {\n+  SKIP_IF_NOT_SHENANDOAH();\n+  ShenandoahAllocRequest req = ShenandoahAllocRequest::for_plab(128, 128);\n+\n+  \/\/ simulate an allocation that exceeds the promotion reserve after allocation\n+  req.set_actual_size(384);\n+  EXPECT_FALSE(old->can_promote(req.actual_size() * HeapWordSize)) << \"Should have room for promotion\";\n+\n+  size_t expended_before = old->get_promoted_expended();\n+  old->configure_plab_for_current_thread(req);\n+  size_t expended_after = old->get_promoted_expended();\n+\n+  EXPECT_EQ(expended_before, expended_after) << \"Did not promote, should not expend promotion\";\n+  EXPECT_EQ(plab_promoted(), 0UL) << \"Cannot promote in new plab\";\n+  EXPECT_EQ(plab_size(), 0UL) << \"Should not have space for promotions\";\n+  EXPECT_FALSE(promotions_enabled()) << \"New plab can only be used for evacuations\";\n+}\n+\n+TEST_VM_F(ShenandoahOldGenerationTest, test_shared_expends_promoted_but_does_not_change_plab) {\n+  SKIP_IF_NOT_SHENANDOAH();\n+  ShenandoahAllocRequest req = ShenandoahAllocRequest::for_shared_gc(128, ShenandoahAffiliation::OLD_GENERATION, true);\n+  req.set_actual_size(128);\n+  size_t actual_size = req.actual_size() * HeapWordSize;\n+\n+  size_t expended_before = old->get_promoted_expended();\n+  old->configure_plab_for_current_thread(req);\n+  size_t expended_after = old->get_promoted_expended();\n+\n+  EXPECT_EQ(expended_before + actual_size, expended_after) << \"Shared promotion still expends promotion\";\n+  EXPECT_EQ(plab_promoted(), INITIAL_PLAB_PROMOTED) << \"Shared promotion should not count in plab\";\n+  EXPECT_EQ(plab_size(), INITIAL_PLAB_SIZE) << \"Shared promotion should not change size of plab\";\n+  EXPECT_FALSE(promotions_enabled());\n+}\n+\n+TEST_VM_F(ShenandoahOldGenerationTest, test_shared_evacuation_has_no_side_effects) {\n+  SKIP_IF_NOT_SHENANDOAH();\n+  ShenandoahAllocRequest req = ShenandoahAllocRequest::for_shared_gc(128, ShenandoahAffiliation::OLD_GENERATION, false);\n+  req.set_actual_size(128);\n+\n+  size_t expended_before = old->get_promoted_expended();\n+  old->configure_plab_for_current_thread(req);\n+  size_t expended_after = old->get_promoted_expended();\n+\n+  EXPECT_EQ(expended_before, expended_after) << \"Not a promotion, should not expend promotion reserve\";\n+  EXPECT_EQ(plab_promoted(), INITIAL_PLAB_PROMOTED) << \"Not a plab, should not have touched plab\";\n+  EXPECT_EQ(plab_size(), INITIAL_PLAB_SIZE) << \"Not a plab, should not have touched plab\";\n+  EXPECT_FALSE(promotions_enabled());\n+}\n+\n+#undef SKIP_IF_NOT_SHENANDOAH\n","filename":"test\/hotspot\/gtest\/gc\/shenandoah\/test_shenandoahOldGeneration.cpp","additions":200,"deletions":0,"binary":false,"changes":200,"status":"added"},{"patch":"@@ -0,0 +1,366 @@\n+\/*\n+ * Copyright Amazon.com Inc. or its affiliates. All Rights Reserved.\n+ * DO NOT ALTER OR REMOVE COPYRIGHT NOTICES OR THIS FILE HEADER.\n+ *\n+ * This code is free software; you can redistribute it and\/or modify it\n+ * under the terms of the GNU General Public License version 2 only, as\n+ * published by the Free Software Foundation.\n+ *\n+ * This code is distributed in the hope that it will be useful, but WITHOUT\n+ * ANY WARRANTY; without even the implied warranty of MERCHANTABILITY or\n+ * FITNESS FOR A PARTICULAR PURPOSE.  See the GNU General Public License\n+ * version 2 for more details (a copy is included in the LICENSE file that\n+ * accompanied this code).\n+ *\n+ * You should have received a copy of the GNU General Public License version\n+ * 2 along with this work; if not, write to the Free Software Foundation,\n+ * Inc., 51 Franklin St, Fifth Floor, Boston, MA 02110-1301 USA.\n+ *\n+ * Please contact Oracle, 500 Oracle Parkway, Redwood Shores, CA 94065 USA\n+ * or visit www.oracle.com if you need additional information or have any\n+ * questions.\n+ *\/\n+\n+#include \"precompiled.hpp\"\n+#include \"unittest.hpp\"\n+#include \"gc\/shenandoah\/shenandoahHeap.inline.hpp\"\n+#include \"gc\/shenandoah\/shenandoahHeapRegion.hpp\"\n+#include \"gc\/shenandoah\/shenandoahGeneration.hpp\"\n+#include \"gc\/shenandoah\/shenandoahOldGeneration.hpp\"\n+#include \"gc\/shenandoah\/heuristics\/shenandoahOldHeuristics.hpp\"\n+#include <cstdarg>\n+\n+\/\/ These tests will all be skipped (unless Shenandoah becomes the default\n+\/\/ collector). To execute these tests, you must enable Shenandoah, which\n+\/\/ is done with:\n+\/\/\n+\/\/ % make exploded-test TEST=\"gtest:ShenandoahOld*\" CONF=release TEST_OPTS=\"JAVA_OPTIONS=-XX:+UseShenandoahGC -XX:+UnlockExperimentalVMOptions -XX:ShenandoahGCMode=generational\"\n+\/\/\n+\/\/ Please note that these 'unit' tests are really integration tests and rely\n+\/\/ on the JVM being initialized. These tests manipulate the state of the\n+\/\/ collector in ways that are not compatible with a normal collection run.\n+\/\/ If these tests take longer than the minimum time between gc intervals -\n+\/\/ or, more likely, if you have them paused in a debugger longer than this\n+\/\/ interval - you can expect trouble. These tests will also not run in a build\n+\/\/ with asserts enabled because they use APIs that expect to run on a safepoint.\n+#ifdef ASSERT\n+#define SKIP_IF_NOT_SHENANDOAH()           \\\n+  tty->print_cr(\"skipped (debug build)\" ); \\\n+  return;\n+#else\n+#define SKIP_IF_NOT_SHENANDOAH() \\\n+    if (!UseShenandoahGC) {      \\\n+      tty->print_cr(\"skipped\");  \\\n+      return;                    \\\n+    }\n+#endif\n+\n+class ShenandoahResetRegions : public ShenandoahHeapRegionClosure {\n+ public:\n+  virtual void heap_region_do(ShenandoahHeapRegion* region) override {\n+    if (!region->is_empty()) {\n+      region->make_trash();\n+      region->make_empty();\n+    }\n+    region->set_affiliation(FREE);\n+    region->clear_live_data();\n+    region->set_top(region->bottom());\n+  }\n+};\n+\n+class ShenandoahOldHeuristicTest : public ::testing::Test {\n+ protected:\n+  ShenandoahHeap* _heap;\n+  ShenandoahOldHeuristics* _heuristics;\n+  ShenandoahCollectionSet* _collection_set;\n+\n+  ShenandoahOldHeuristicTest()\n+    : _heap(nullptr),\n+      _heuristics(nullptr),\n+      _collection_set(nullptr) {\n+    SKIP_IF_NOT_SHENANDOAH();\n+    _heap = ShenandoahHeap::heap();\n+    _heuristics = _heap->old_generation()->heuristics();\n+    _collection_set = _heap->collection_set();\n+    _heap->lock()->lock(false);\n+    ShenandoahResetRegions reset;\n+    _heap->heap_region_iterate(&reset);\n+    _heap->old_generation()->increase_capacity(ShenandoahHeapRegion::region_size_bytes() * 10);\n+    _heap->old_generation()->set_evacuation_reserve(ShenandoahHeapRegion::region_size_bytes() * 4);\n+    _heuristics->abandon_collection_candidates();\n+    _collection_set->clear();\n+  }\n+\n+  ~ShenandoahOldHeuristicTest() override {\n+    SKIP_IF_NOT_SHENANDOAH();\n+    _heap->lock()->unlock();\n+  }\n+\n+  ShenandoahOldGeneration::State old_generation_state() {\n+    return _heap->old_generation()->state();\n+  }\n+\n+  size_t make_garbage(size_t region_idx, size_t garbage_bytes) {\n+    ShenandoahHeapRegion* region = _heap->get_region(region_idx);\n+    region->set_affiliation(OLD_GENERATION);\n+    region->make_regular_allocation(OLD_GENERATION);\n+    size_t live_bytes = ShenandoahHeapRegion::region_size_bytes() - garbage_bytes;\n+    region->increase_live_data_alloc_words(live_bytes \/ HeapWordSize);\n+    region->set_top(region->end());\n+    return region->garbage();\n+  }\n+\n+  size_t create_too_much_garbage_for_one_mixed_evacuation() {\n+    size_t garbage_target = _heap->old_generation()->max_capacity() \/ 2;\n+    size_t garbage_total = 0;\n+    size_t region_idx = 0;\n+    while (garbage_total < garbage_target && region_idx < _heap->num_regions()) {\n+      garbage_total += make_garbage_above_collection_threshold(region_idx++);\n+    }\n+    return garbage_total;\n+  }\n+\n+  void make_pinned(size_t region_idx) {\n+    ShenandoahHeapRegion* region = _heap->get_region(region_idx);\n+    region->record_pin();\n+    region->make_pinned();\n+  }\n+\n+  void make_unpinned(size_t region_idx) {\n+    ShenandoahHeapRegion* region = _heap->get_region(region_idx);\n+    region->record_unpin();\n+    region->make_unpinned();\n+  }\n+\n+  size_t make_garbage_below_collection_threshold(size_t region_idx) {\n+    return make_garbage(region_idx, collection_threshold() - 100);\n+  }\n+\n+  size_t make_garbage_above_collection_threshold(size_t region_idx) {\n+    return make_garbage(region_idx, collection_threshold() + 100);\n+  }\n+\n+  size_t collection_threshold() const {\n+    return ShenandoahHeapRegion::region_size_bytes() * ShenandoahOldGarbageThreshold \/ 100;\n+  }\n+\n+  bool collection_set_is(size_t r1) { return _collection_set_is(1, r1); }\n+  bool collection_set_is(size_t r1, size_t r2) { return _collection_set_is(2, r1, r2); }\n+  bool collection_set_is(size_t r1, size_t r2, size_t r3) { return _collection_set_is(3, r1, r2, r3); }\n+\n+  bool _collection_set_is(size_t count, ...) {\n+    va_list args;\n+    va_start(args, count);\n+    EXPECT_EQ(count, _collection_set->count());\n+    bool result = true;\n+    for (size_t i = 0; i < count; ++i) {\n+      size_t index = va_arg(args, size_t);\n+      if (!_collection_set->is_in(index)) {\n+        result = false;\n+        break;\n+      }\n+    }\n+    va_end(args);\n+    return result;\n+  }\n+};\n+\n+TEST_VM_F(ShenandoahOldHeuristicTest, select_no_old_regions) {\n+  SKIP_IF_NOT_SHENANDOAH();\n+\n+  _heuristics->prepare_for_old_collections();\n+  EXPECT_EQ(0U, _heuristics->coalesce_and_fill_candidates_count());\n+  EXPECT_EQ(0U, _heuristics->last_old_collection_candidate_index());\n+  EXPECT_EQ(0U, _heuristics->unprocessed_old_collection_candidates());\n+}\n+\n+TEST_VM_F(ShenandoahOldHeuristicTest, select_no_old_region_above_threshold) {\n+  SKIP_IF_NOT_SHENANDOAH();\n+\n+  \/\/ In this case, we have zero regions to add to the collection set,\n+  \/\/ but we will have one region that must still be made parseable.\n+  make_garbage_below_collection_threshold(10);\n+  _heuristics->prepare_for_old_collections();\n+  EXPECT_EQ(1U, _heuristics->coalesce_and_fill_candidates_count());\n+  EXPECT_EQ(0U, _heuristics->last_old_collection_candidate_index());\n+  EXPECT_EQ(0U, _heuristics->unprocessed_old_collection_candidates());\n+}\n+\n+TEST_VM_F(ShenandoahOldHeuristicTest, select_one_old_region_above_threshold) {\n+  SKIP_IF_NOT_SHENANDOAH();\n+\n+  make_garbage_above_collection_threshold(10);\n+  _heuristics->prepare_for_old_collections();\n+  EXPECT_EQ(1U, _heuristics->coalesce_and_fill_candidates_count());\n+  EXPECT_EQ(1U, _heuristics->last_old_collection_candidate_index());\n+  EXPECT_EQ(1U, _heuristics->unprocessed_old_collection_candidates());\n+}\n+\n+TEST_VM_F(ShenandoahOldHeuristicTest, prime_one_old_region) {\n+  SKIP_IF_NOT_SHENANDOAH();\n+\n+  size_t garbage = make_garbage_above_collection_threshold(10);\n+  _heuristics->prepare_for_old_collections();\n+  _heuristics->prime_collection_set(_collection_set);\n+\n+  EXPECT_TRUE(collection_set_is(10UL));\n+  EXPECT_EQ(garbage, _collection_set->get_old_garbage());\n+  EXPECT_EQ(0U, _heuristics->unprocessed_old_collection_candidates());\n+}\n+\n+TEST_VM_F(ShenandoahOldHeuristicTest, prime_many_old_regions) {\n+  SKIP_IF_NOT_SHENANDOAH();\n+\n+  size_t g1 = make_garbage_above_collection_threshold(100);\n+  size_t g2 = make_garbage_above_collection_threshold(101);\n+  _heuristics->prepare_for_old_collections();\n+  _heuristics->prime_collection_set(_collection_set);\n+\n+  EXPECT_TRUE(collection_set_is(100UL, 101UL));\n+  EXPECT_EQ(g1 + g2, _collection_set->get_old_garbage());\n+  EXPECT_EQ(0U, _heuristics->unprocessed_old_collection_candidates());\n+}\n+\n+TEST_VM_F(ShenandoahOldHeuristicTest, require_multiple_mixed_evacuations) {\n+  SKIP_IF_NOT_SHENANDOAH();\n+\n+  size_t garbage = create_too_much_garbage_for_one_mixed_evacuation();\n+  _heuristics->prepare_for_old_collections();\n+  _heuristics->prime_collection_set(_collection_set);\n+\n+  EXPECT_LT(_collection_set->get_old_garbage(), garbage);\n+  EXPECT_GT(_heuristics->unprocessed_old_collection_candidates(), 0UL);\n+}\n+\n+TEST_VM_F(ShenandoahOldHeuristicTest, skip_pinned_regions) {\n+  SKIP_IF_NOT_SHENANDOAH();\n+\n+  \/\/ Create three old regions with enough garbage to be collected.\n+  size_t g1 = make_garbage_above_collection_threshold(0);\n+  size_t g2 = make_garbage_above_collection_threshold(1);\n+  size_t g3 = make_garbage_above_collection_threshold(2);\n+\n+  \/\/ A region can be pinned when we chose collection set candidates.\n+  make_pinned(1);\n+  _heuristics->prepare_for_old_collections();\n+\n+  \/\/ We only exclude pinned regions when we actually add regions to the collection set.\n+  ASSERT_EQ(3UL, _heuristics->unprocessed_old_collection_candidates());\n+\n+  \/\/ Here the region is still pinned, so it cannot be added to the collection set.\n+  _heuristics->prime_collection_set(_collection_set);\n+\n+  \/\/ The two unpinned regions should be added to the collection set and the pinned\n+  \/\/ region should be retained at the front of the list of candidates as it would be\n+  \/\/ likely to become unpinned by the next mixed collection cycle.\n+  EXPECT_TRUE(collection_set_is(0UL, 2UL));\n+  EXPECT_EQ(_collection_set->get_old_garbage(), g1 + g3);\n+  EXPECT_EQ(_heuristics->unprocessed_old_collection_candidates(), 1UL);\n+\n+  \/\/ Simulate another mixed collection after making region 1 unpinned. This time,\n+  \/\/ the now unpinned region should be added to the collection set.\n+  make_unpinned(1);\n+  _collection_set->clear();\n+  _heuristics->prime_collection_set(_collection_set);\n+\n+  EXPECT_EQ(_collection_set->get_old_garbage(), g2);\n+  EXPECT_TRUE(collection_set_is(1UL));\n+  EXPECT_EQ(_heuristics->unprocessed_old_collection_candidates(), 0UL);\n+}\n+\n+TEST_VM_F(ShenandoahOldHeuristicTest, pinned_region_is_first) {\n+  SKIP_IF_NOT_SHENANDOAH();\n+\n+  \/\/ Create three old regions with enough garbage to be collected.\n+  size_t g1 = make_garbage_above_collection_threshold(0);\n+  size_t g2 = make_garbage_above_collection_threshold(1);\n+  size_t g3 = make_garbage_above_collection_threshold(2);\n+\n+  make_pinned(0);\n+  _heuristics->prepare_for_old_collections();\n+  _heuristics->prime_collection_set(_collection_set);\n+\n+  EXPECT_TRUE(collection_set_is(1UL, 2UL));\n+  EXPECT_EQ(_heuristics->unprocessed_old_collection_candidates(), 1UL);\n+\n+  make_unpinned(0);\n+  _collection_set->clear();\n+  _heuristics->prime_collection_set(_collection_set);\n+\n+  EXPECT_TRUE(collection_set_is(0UL));\n+  EXPECT_EQ(_heuristics->unprocessed_old_collection_candidates(), 0UL);\n+}\n+\n+TEST_VM_F(ShenandoahOldHeuristicTest, pinned_region_is_last) {\n+  SKIP_IF_NOT_SHENANDOAH();\n+\n+  \/\/ Create three old regions with enough garbage to be collected.\n+  size_t g1 = make_garbage_above_collection_threshold(0);\n+  size_t g2 = make_garbage_above_collection_threshold(1);\n+  size_t g3 = make_garbage_above_collection_threshold(2);\n+\n+  make_pinned(2);\n+  _heuristics->prepare_for_old_collections();\n+  _heuristics->prime_collection_set(_collection_set);\n+\n+  EXPECT_TRUE(collection_set_is(0UL, 1UL));\n+  EXPECT_EQ(_collection_set->get_old_garbage(), g1 + g2);\n+  EXPECT_EQ(_heuristics->unprocessed_old_collection_candidates(), 1UL);\n+\n+  make_unpinned(2);\n+  _collection_set->clear();\n+  _heuristics->prime_collection_set(_collection_set);\n+\n+  EXPECT_TRUE(collection_set_is(2UL));\n+  EXPECT_EQ(_collection_set->get_old_garbage(), g3);\n+  EXPECT_EQ(_heuristics->unprocessed_old_collection_candidates(), 0UL);\n+}\n+\n+TEST_VM_F(ShenandoahOldHeuristicTest, unpinned_region_is_middle) {\n+  SKIP_IF_NOT_SHENANDOAH();\n+\n+  \/\/ Create three old regions with enough garbage to be collected.\n+  size_t g1 = make_garbage_above_collection_threshold(0);\n+  size_t g2 = make_garbage_above_collection_threshold(1);\n+  size_t g3 = make_garbage_above_collection_threshold(2);\n+\n+  make_pinned(0);\n+  make_pinned(2);\n+  _heuristics->prepare_for_old_collections();\n+  _heuristics->prime_collection_set(_collection_set);\n+\n+  EXPECT_TRUE(collection_set_is(1UL));\n+  EXPECT_EQ(_collection_set->get_old_garbage(), g2);\n+  EXPECT_EQ(_heuristics->unprocessed_old_collection_candidates(), 2UL);\n+\n+  make_unpinned(0);\n+  make_unpinned(2);\n+  _collection_set->clear();\n+  _heuristics->prime_collection_set(_collection_set);\n+\n+  EXPECT_TRUE(collection_set_is(0UL, 2UL));\n+  EXPECT_EQ(_collection_set->get_old_garbage(), g1 + g3);\n+  EXPECT_EQ(_heuristics->unprocessed_old_collection_candidates(), 0UL);\n+}\n+\n+TEST_VM_F(ShenandoahOldHeuristicTest, all_candidates_are_pinned) {\n+  SKIP_IF_NOT_SHENANDOAH();\n+\n+  size_t g1 = make_garbage_above_collection_threshold(0);\n+  size_t g2 = make_garbage_above_collection_threshold(1);\n+  size_t g3 = make_garbage_above_collection_threshold(2);\n+\n+  make_pinned(0);\n+  make_pinned(1);\n+  make_pinned(2);\n+  _heuristics->prepare_for_old_collections();\n+  _heuristics->prime_collection_set(_collection_set);\n+\n+  \/\/ In the case when all candidates are pinned, we want to abandon\n+  \/\/ this set of mixed collection candidates so that another old collection\n+  \/\/ can run. This is meant to defend against \"bad\" JNI code that permanently\n+  \/\/ leaves an old region in the pinned state.\n+  EXPECT_EQ(_collection_set->count(), 0UL);\n+  EXPECT_EQ(old_generation_state(), ShenandoahOldGeneration::FILLING);\n+}\n+#undef SKIP_IF_NOT_SHENANDOAH\n","filename":"test\/hotspot\/gtest\/gc\/shenandoah\/test_shenandoahOldHeuristic.cpp","additions":366,"deletions":0,"binary":false,"changes":366,"status":"added"},{"patch":"@@ -92,1 +92,0 @@\n-gc\/TestAllocHumongousFragment.java#iu-aggressive 8298781 generic-all\n@@ -110,0 +109,2 @@\n+gc\/shenandoah\/TestAllocIntArrays.java#aggressive 8289220 generic-all\n+gc\/shenandoah\/TestSieveObjects.java#aggressive 8289220 generic-all\n","filename":"test\/hotspot\/jtreg\/ProblemList.txt","additions":2,"deletions":1,"binary":false,"changes":3,"status":"modified"},{"patch":"@@ -42,2 +42,1 @@\n- *                    Shenandoah,\n- *                    ShenandoahIU}\n+ *                    Shenandoah}\n@@ -98,7 +97,0 @@\n-        case \"ShenandoahIU\":\n-            argcount = 11;\n-            procArgs = new String[argcount];\n-            procArgs[argcount - 4] = \"-XX:+UnlockExperimentalVMOptions\";\n-            procArgs[argcount - 3] = \"-XX:+UseShenandoahGC\";\n-            procArgs[argcount - 2] = \"-XX:ShenandoahGCMode=iu\";\n-            break;\n@@ -289,1 +281,0 @@\n-        case \"ShenandoahIU\":\n@@ -361,1 +352,0 @@\n-        case \"ShenandoahIU\":\n@@ -448,1 +438,0 @@\n-        case \"ShenandoahIU\":\n@@ -515,1 +504,0 @@\n-        case \"ShenandoahIU\":\n","filename":"test\/hotspot\/jtreg\/compiler\/c2\/aarch64\/TestVolatiles.java","additions":1,"deletions":13,"binary":false,"changes":14,"status":"modified"},{"patch":"@@ -74,27 +74,0 @@\n- * @run driver compiler.c2.aarch64.TestVolatilesShenandoah\n- *      TestVolatileLoad ShenandoahIU\n- *\n- * @run driver compiler.c2.aarch64.TestVolatilesShenandoah\n- *      TestVolatileStore ShenandoahIU\n- *\n- * @run driver compiler.c2.aarch64.TestVolatilesShenandoah\n- *      TestUnsafeVolatileLoad ShenandoahIU\n- *\n- * @run driver compiler.c2.aarch64.TestVolatilesShenandoah\n- *      TestUnsafeVolatileStore ShenandoahIU\n- *\n- * @run driver compiler.c2.aarch64.TestVolatilesShenandoah\n- *      TestUnsafeVolatileCAS ShenandoahIU\n- *\n- * @run driver compiler.c2.aarch64.TestVolatilesShenandoah\n- *      TestUnsafeVolatileWeakCAS ShenandoahIU\n- *\n- * @run driver compiler.c2.aarch64.TestVolatilesShenandoah\n- *      TestUnsafeVolatileCAE ShenandoahIU\n- *\n- * @run driver compiler.c2.aarch64.TestVolatilesShenandoah\n- *      TestUnsafeVolatileGAS ShenandoahIU\n- *\n- * @run driver compiler.c2.aarch64.TestVolatilesShenandoah\n- *      TestUnsafeVolatileGAA ShenandoahIU\n- *\n","filename":"test\/hotspot\/jtreg\/compiler\/c2\/aarch64\/TestVolatilesShenandoah.java","additions":0,"deletions":27,"binary":false,"changes":27,"status":"modified"},{"patch":"@@ -123,45 +123,0 @@\n-\/*\n- * @test id=iu-aggressive\n- * @summary Make sure Shenandoah can recover from humongous allocation fragmentation\n- * @key randomness\n- * @requires vm.gc.Shenandoah\n- * @library \/test\/lib\n- *\n- * @run main\/othervm -Xlog:gc -XX:+UnlockDiagnosticVMOptions -XX:+UnlockExperimentalVMOptions -Xmx1g -Xms1g -XX:ShenandoahTargetNumRegions=2048\n- *      -XX:+UseShenandoahGC -XX:ShenandoahGCMode=iu -XX:ShenandoahGCHeuristics=aggressive\n- *      -XX:+ShenandoahOOMDuringEvacALot -XX:+ShenandoahVerify\n- *      TestAllocHumongousFragment\n- *\n- * @run main\/othervm -Xlog:gc -XX:+UnlockDiagnosticVMOptions -XX:+UnlockExperimentalVMOptions -Xmx1g -Xms1g -XX:ShenandoahTargetNumRegions=2048\n- *      -XX:+UseShenandoahGC -XX:ShenandoahGCMode=iu -XX:ShenandoahGCHeuristics=aggressive\n- *      -XX:+ShenandoahAllocFailureALot -XX:+ShenandoahVerify\n- *      TestAllocHumongousFragment\n- *\n- * @run main\/othervm -Xlog:gc -XX:+UnlockDiagnosticVMOptions -XX:+UnlockExperimentalVMOptions -Xmx1g -Xms1g -XX:ShenandoahTargetNumRegions=2048\n- *      -XX:+UseShenandoahGC -XX:ShenandoahGCMode=iu -XX:ShenandoahGCHeuristics=aggressive\n- *      -XX:+ShenandoahOOMDuringEvacALot\n- *      TestAllocHumongousFragment\n- *\n- * @run main\/othervm -Xlog:gc -XX:+UnlockDiagnosticVMOptions -XX:+UnlockExperimentalVMOptions -Xmx1g -Xms1g -XX:ShenandoahTargetNumRegions=2048\n- *      -XX:+UseShenandoahGC -XX:ShenandoahGCMode=iu -XX:ShenandoahGCHeuristics=aggressive\n- *      -XX:+ShenandoahAllocFailureALot\n- *      TestAllocHumongousFragment\n- *\/\n-\n-\/*\n- * @test id=iu\n- * @summary Make sure Shenandoah can recover from humongous allocation fragmentation\n- * @key randomness\n- * @requires vm.gc.Shenandoah\n- * @library \/test\/lib\n- *\n- * @run main\/othervm -Xlog:gc -XX:+UnlockDiagnosticVMOptions -XX:+UnlockExperimentalVMOptions -Xmx1g -Xms1g -XX:ShenandoahTargetNumRegions=2048\n- *      -XX:+UseShenandoahGC -XX:ShenandoahGCMode=iu\n- *      -XX:+ShenandoahVerify\n- *      TestAllocHumongousFragment\n- *\n- * @run main\/othervm -Xlog:gc -XX:+UnlockDiagnosticVMOptions -XX:+UnlockExperimentalVMOptions -Xmx1g -Xms1g -XX:ShenandoahTargetNumRegions=2048\n- *      -XX:+UseShenandoahGC -XX:ShenandoahGCMode=iu\n- *      TestAllocHumongousFragment\n- *\/\n-\n","filename":"test\/hotspot\/jtreg\/gc\/TestAllocHumongousFragment.java","additions":0,"deletions":45,"binary":false,"changes":45,"status":"modified"},{"patch":"@@ -56,5 +56,0 @@\n-    if (GC.Shenandoah.isSupported()) {\n-      noneGCSupported = false;\n-      testDynamicNumberOfGCThreads(\"UseShenandoahGC\");\n-    }\n-\n@@ -62,1 +57,1 @@\n-      throw new SkippedException(\"Skipping test because none of G1\/Parallel\/Shenandoah is supported.\");\n+      throw new SkippedException(\"Skipping test because none of G1\/Parallel is supported.\");\n","filename":"test\/hotspot\/jtreg\/gc\/ergonomics\/TestDynamicNumberOfGCThreads.java","additions":1,"deletions":6,"binary":false,"changes":7,"status":"modified"},{"patch":"@@ -3,0 +3,1 @@\n+ * Copyright Amazon.com Inc. or its affiliates. All Rights Reserved.\n@@ -103,1 +104,1 @@\n- * @test id=static\n+ * @test id=generational\n@@ -110,1 +111,2 @@\n- *      -XX:+UseShenandoahGC -XX:ShenandoahGCHeuristics=static\n+ *      -XX:+UseShenandoahGC -XX:ShenandoahGCHeuristics=adaptive -XX:ShenandoahGCMode=generational\n+ *      -XX:+ShenandoahVerify\n@@ -112,8 +114,0 @@\n- *\/\n-\n-\/*\n- * @test id=compact\n- * @summary Acceptance tests: collector can withstand allocation\n- * @key randomness\n- * @requires vm.gc.Shenandoah\n- * @library \/test\/lib\n@@ -122,1 +116,1 @@\n- *      -XX:+UseShenandoahGC -XX:ShenandoahGCHeuristics=compact\n+ *      -XX:+UseShenandoahGC -XX:ShenandoahGCHeuristics=adaptive -XX:ShenandoahGCMode=generational\n@@ -127,1 +121,1 @@\n- * @test id=no-tlab\n+ * @test id=static\n@@ -134,2 +128,1 @@\n- *      -XX:+UseShenandoahGC\n- *      -XX:-UseTLAB -XX:+ShenandoahVerify\n+ *      -XX:+UseShenandoahGC -XX:ShenandoahGCHeuristics=static\n@@ -140,1 +133,1 @@\n- * @test id=iu-aggressive\n+ * @test id=compact\n@@ -147,21 +140,1 @@\n- *      -XX:+UseShenandoahGC -XX:ShenandoahGCMode=iu -XX:ShenandoahGCHeuristics=aggressive\n- *      -XX:+ShenandoahOOMDuringEvacALot -XX:+ShenandoahVerify\n- *      TestAllocIntArrays\n- *\n- * @run main\/othervm -XX:+UnlockDiagnosticVMOptions -XX:+UnlockExperimentalVMOptions -Xmx1g -Xms1g\n- *      -XX:+UseShenandoahGC -XX:ShenandoahGCMode=iu -XX:ShenandoahGCHeuristics=aggressive\n- *      -XX:+ShenandoahAllocFailureALot -XX:+ShenandoahVerify\n- *      TestAllocIntArrays\n- *\n- * @run main\/othervm -XX:+UnlockDiagnosticVMOptions -XX:+UnlockExperimentalVMOptions -Xmx1g -Xms1g\n- *      -XX:+UseShenandoahGC -XX:ShenandoahGCMode=iu -XX:ShenandoahGCHeuristics=aggressive\n- *      -XX:+ShenandoahOOMDuringEvacALot\n- *      TestAllocIntArrays\n- *\n- * @run main\/othervm -XX:+UnlockDiagnosticVMOptions -XX:+UnlockExperimentalVMOptions -Xmx1g -Xms1g\n- *      -XX:+UseShenandoahGC -XX:ShenandoahGCMode=iu -XX:ShenandoahGCHeuristics=aggressive\n- *      -XX:+ShenandoahAllocFailureALot\n- *      TestAllocIntArrays\n- *\n- * @run main\/othervm -XX:+UnlockDiagnosticVMOptions -XX:+UnlockExperimentalVMOptions -Xmx1g -Xms1g\n- *      -XX:+UseShenandoahGC -XX:ShenandoahGCMode=iu -XX:ShenandoahGCHeuristics=aggressive\n+ *      -XX:+UseShenandoahGC -XX:ShenandoahGCHeuristics=compact\n@@ -172,1 +145,1 @@\n- * @test id=iu\n+ * @test id=no-tlab\n@@ -179,6 +152,2 @@\n- *      -XX:+UseShenandoahGC -XX:ShenandoahGCMode=iu\n- *      -XX:+ShenandoahVerify\n- *      TestAllocIntArrays\n- *\n- * @run main\/othervm -XX:+UnlockDiagnosticVMOptions -XX:+UnlockExperimentalVMOptions -Xmx1g -Xms1g\n- *      -XX:+UseShenandoahGC -XX:ShenandoahGCMode=iu\n+ *      -XX:+UseShenandoahGC\n+ *      -XX:-UseTLAB -XX:+ShenandoahVerify\n@@ -187,1 +156,0 @@\n-\n@@ -200,0 +168,2 @@\n+        \/\/ Each allocated int array is assumed to consume 16 bytes for alignment and header, plus\n+        \/\/  an average of 4 * the average number of elements in the array.\n@@ -203,0 +173,3 @@\n+        \/\/ Repeatedly, allocate an array of int having between 0 and 384K elements, until we have\n+        \/\/ allocated approximately TARGET_MB.  The largest allocated array consumes 384K*4 + 16, which is 1.5 M,\n+        \/\/ which is well below the heap size of 1g.\n","filename":"test\/hotspot\/jtreg\/gc\/shenandoah\/TestAllocIntArrays.java","additions":17,"deletions":44,"binary":false,"changes":61,"status":"modified"},{"patch":"@@ -3,0 +3,1 @@\n+ * Copyright Amazon.com Inc. or its affiliates. All Rights Reserved.\n@@ -103,1 +104,1 @@\n- * @test id=static\n+ * @test id=generational\n@@ -110,1 +111,2 @@\n- *      -XX:+UseShenandoahGC -XX:ShenandoahGCHeuristics=static\n+ *      -XX:+UseShenandoahGC -XX:ShenandoahGCHeuristics=adaptive -XX:ShenandoahGCMode=generational\n+ *      -XX:+ShenandoahVerify\n@@ -112,8 +114,0 @@\n- *\/\n-\n-\/*\n- * @test id=compact\n- * @summary Acceptance tests: collector can withstand allocation\n- * @key randomness\n- * @requires vm.gc.Shenandoah\n- * @library \/test\/lib\n@@ -122,1 +116,13 @@\n- *      -XX:+UseShenandoahGC -XX:ShenandoahGCHeuristics=compact\n+ *      -XX:+UseShenandoahGC -XX:ShenandoahGCHeuristics=adaptive -XX:ShenandoahGCMode=generational\n+ *      -XX:+ShenandoahOOMDuringEvacALot\n+ *      -XX:+ShenandoahVerify\n+ *      TestAllocObjectArrays\n+ *\n+ * @run main\/othervm -XX:+UnlockDiagnosticVMOptions -XX:+UnlockExperimentalVMOptions -Xmx1g -Xms1g\n+ *      -XX:+UseShenandoahGC -XX:ShenandoahGCHeuristics=adaptive -XX:ShenandoahGCMode=generational\n+ *      -XX:+ShenandoahAllocFailureALot\n+ *      -XX:+ShenandoahVerify\n+ *      TestAllocObjectArrays\n+ *\n+ * @run main\/othervm -XX:+UnlockDiagnosticVMOptions -XX:+UnlockExperimentalVMOptions -Xmx1g -Xms1g\n+ *      -XX:+UseShenandoahGC -XX:ShenandoahGCHeuristics=adaptive -XX:ShenandoahGCMode=generational\n@@ -127,1 +133,1 @@\n- * @test id=no-tlab\n+ * @test id=static\n@@ -134,2 +140,1 @@\n- *      -XX:+UseShenandoahGC\n- *      -XX:-UseTLAB -XX:+ShenandoahVerify\n+ *      -XX:+UseShenandoahGC -XX:ShenandoahGCHeuristics=static\n@@ -140,1 +145,1 @@\n- * @test id=iu-aggressive\n+ * @test id=compact\n@@ -147,21 +152,1 @@\n- *      -XX:+UseShenandoahGC -XX:ShenandoahGCMode=iu -XX:ShenandoahGCHeuristics=aggressive\n- *      -XX:+ShenandoahOOMDuringEvacALot -XX:+ShenandoahVerify\n- *      TestAllocObjectArrays\n- *\n- * @run main\/othervm -XX:+UnlockDiagnosticVMOptions -XX:+UnlockExperimentalVMOptions -Xmx1g -Xms1g\n- *      -XX:+UseShenandoahGC -XX:ShenandoahGCMode=iu -XX:ShenandoahGCHeuristics=aggressive\n- *      -XX:+ShenandoahAllocFailureALot -XX:+ShenandoahVerify\n- *      TestAllocObjectArrays\n- *\n- * @run main\/othervm -XX:+UnlockDiagnosticVMOptions -XX:+UnlockExperimentalVMOptions -Xmx1g -Xms1g\n- *      -XX:+UseShenandoahGC -XX:ShenandoahGCMode=iu -XX:ShenandoahGCHeuristics=aggressive\n- *      -XX:+ShenandoahOOMDuringEvacALot\n- *      TestAllocObjectArrays\n- *\n- * @run main\/othervm -XX:+UnlockDiagnosticVMOptions -XX:+UnlockExperimentalVMOptions -Xmx1g -Xms1g\n- *      -XX:+UseShenandoahGC -XX:ShenandoahGCMode=iu -XX:ShenandoahGCHeuristics=aggressive\n- *      -XX:+ShenandoahAllocFailureALot\n- *      TestAllocObjectArrays\n- *\n- * @run main\/othervm -XX:+UnlockDiagnosticVMOptions -XX:+UnlockExperimentalVMOptions -Xmx1g -Xms1g\n- *      -XX:+UseShenandoahGC -XX:ShenandoahGCMode=iu -XX:ShenandoahGCHeuristics=aggressive\n+ *      -XX:+UseShenandoahGC -XX:ShenandoahGCHeuristics=compact\n@@ -172,1 +157,1 @@\n- * @test id=iu\n+ * @test id=no-tlab\n@@ -179,2 +164,2 @@\n- *      -XX:+UseShenandoahGC -XX:ShenandoahGCMode=iu\n- *      -XX:+ShenandoahVerify\n+ *      -XX:+UseShenandoahGC\n+ *      -XX:-UseTLAB -XX:+ShenandoahVerify\n@@ -184,1 +169,2 @@\n- *      -XX:+UseShenandoahGC -XX:ShenandoahGCMode=iu\n+ *      -XX:+UseShenandoahGC -XX:ShenandoahGCHeuristics=adaptive -XX:ShenandoahGCMode=generational\n+ *      -XX:-UseTLAB -XX:+ShenandoahVerify\n@@ -187,1 +173,0 @@\n-\n","filename":"test\/hotspot\/jtreg\/gc\/shenandoah\/TestAllocObjectArrays.java","additions":26,"deletions":41,"binary":false,"changes":67,"status":"modified"},{"patch":"@@ -3,0 +3,1 @@\n+ * Copyright Amazon.com Inc. or its affiliates. All Rights Reserved.\n@@ -97,1 +98,1 @@\n- * @test id=static\n+ * @test id=generational\n@@ -102,1 +103,2 @@\n- *      -XX:+UseShenandoahGC -XX:ShenandoahGCHeuristics=static\n+ *      -XX:+UseShenandoahGC -XX:ShenandoahGCHeuristics=adaptive -XX:ShenandoahGCMode=generational\n+ *      -XX:+ShenandoahVerify\n@@ -104,6 +106,0 @@\n- *\/\n-\n-\/*\n- * @test id=compact\n- * @summary Acceptance tests: collector can withstand allocation\n- * @requires vm.gc.Shenandoah\n@@ -112,1 +108,1 @@\n- *      -XX:+UseShenandoahGC -XX:ShenandoahGCHeuristics=compact\n+ *      -XX:+UseShenandoahGC -XX:ShenandoahGCHeuristics=adaptive -XX:ShenandoahGCMode=generational\n@@ -117,1 +113,1 @@\n- * @test id=iu-aggressive\n+ * @test id=static\n@@ -122,21 +118,1 @@\n- *      -XX:+UseShenandoahGC -XX:ShenandoahGCMode=iu -XX:ShenandoahGCHeuristics=aggressive\n- *      -XX:+ShenandoahOOMDuringEvacALot -XX:+ShenandoahVerify\n- *      TestAllocObjects\n- *\n- * @run main\/othervm -Xmx1g -Xms1g -XX:+UnlockDiagnosticVMOptions -XX:+UnlockExperimentalVMOptions\n- *      -XX:+UseShenandoahGC -XX:ShenandoahGCMode=iu -XX:ShenandoahGCHeuristics=aggressive\n- *      -XX:+ShenandoahAllocFailureALot -XX:+ShenandoahVerify\n- *      TestAllocObjects\n- *\n- * @run main\/othervm -Xmx1g -Xms1g -XX:+UnlockDiagnosticVMOptions -XX:+UnlockExperimentalVMOptions\n- *      -XX:+UseShenandoahGC -XX:ShenandoahGCMode=iu -XX:ShenandoahGCHeuristics=aggressive\n- *      -XX:+ShenandoahOOMDuringEvacALot\n- *      TestAllocObjects\n- *\n- * @run main\/othervm -Xmx1g -Xms1g -XX:+UnlockDiagnosticVMOptions -XX:+UnlockExperimentalVMOptions\n- *      -XX:+UseShenandoahGC -XX:ShenandoahGCMode=iu -XX:ShenandoahGCHeuristics=aggressive\n- *      -XX:+ShenandoahAllocFailureALot\n- *      TestAllocObjects\n- *\n- * @run main\/othervm -Xmx1g -Xms1g -XX:+UnlockDiagnosticVMOptions -XX:+UnlockExperimentalVMOptions\n- *      -XX:+UseShenandoahGC -XX:ShenandoahGCMode=iu -XX:ShenandoahGCHeuristics=aggressive\n+ *      -XX:+UseShenandoahGC -XX:ShenandoahGCHeuristics=static\n@@ -147,1 +123,1 @@\n- * @test id=iu\n+ * @test id=compact\n@@ -152,6 +128,1 @@\n- *      -XX:+UseShenandoahGC -XX:ShenandoahGCMode=iu\n- *      -XX:+ShenandoahVerify\n- *      TestAllocObjects\n- *\n- * @run main\/othervm -Xmx1g -Xms1g -XX:+UnlockDiagnosticVMOptions -XX:+UnlockExperimentalVMOptions\n- *      -XX:+UseShenandoahGC -XX:ShenandoahGCMode=iu\n+ *      -XX:+UseShenandoahGC -XX:ShenandoahGCHeuristics=compact\n@@ -160,1 +131,0 @@\n-\n","filename":"test\/hotspot\/jtreg\/gc\/shenandoah\/TestAllocObjects.java","additions":9,"deletions":39,"binary":false,"changes":48,"status":"modified"},{"patch":"@@ -3,0 +3,1 @@\n+ * Copyright Amazon.com Inc. or its affiliates. All Rights Reserved.\n@@ -26,1 +27,1 @@\n- * @test\n+ * @test id=default\n@@ -31,0 +32,7 @@\n+\n+\/*\n+ * @test id=generational\n+ * @requires vm.gc.Shenandoah\n+ *\n+ * @run main\/othervm -XX:+UnlockExperimentalVMOptions -XX:+UseShenandoahGC -XX:TieredStopAtLevel=0 -Xmx16m TestArrayCopyCheckCast -XX:ShenandoahGCMode=generational\n+ *\/\n","filename":"test\/hotspot\/jtreg\/gc\/shenandoah\/TestArrayCopyCheckCast.java","additions":9,"deletions":1,"binary":false,"changes":10,"status":"modified"},{"patch":"@@ -3,0 +3,1 @@\n+ * Copyright Amazon.com Inc. or its affiliates. All Rights Reserved.\n@@ -29,1 +30,1 @@\n- * @test\n+ * @test id=default\n@@ -36,0 +37,9 @@\n+\n+\/*\n+ * @test id=generational\n+ * @key randomness\n+ * @requires vm.gc.Shenandoah\n+ * @library \/test\/lib\n+ *\n+ * @run main\/othervm -XX:+UnlockExperimentalVMOptions -XX:+UseShenandoahGC -XX:ShenandoahGCMode=generational -XX:TieredStopAtLevel=0 -Xmx16m TestArrayCopyStress\n+ *\/\n","filename":"test\/hotspot\/jtreg\/gc\/shenandoah\/TestArrayCopyStress.java","additions":11,"deletions":1,"binary":false,"changes":12,"status":"modified"},{"patch":"@@ -3,0 +3,1 @@\n+ * Copyright Amazon.com Inc. or its affiliates. All Rights Reserved.\n@@ -25,2 +26,2 @@\n-\/*\n- * @test id=passive\n+\/**\n+ * @test id=satb-adaptive\n@@ -30,2 +31,3 @@\n- * @run main\/othervm -Xms16m -Xmx512m -XX:+UnlockDiagnosticVMOptions -XX:+UnlockExperimentalVMOptions\n- *      -XX:+UseShenandoahGC -XX:ShenandoahGCMode=passive\n+ * @run main\/othervm -Xms100m -Xmx512m -XX:+UnlockDiagnosticVMOptions -XX:+UnlockExperimentalVMOptions\n+ *      -XX:+UseShenandoahGC -Xlog:gc=info -Dtarget=10000\n+ *      -XX:ShenandoahGCMode=satb\n@@ -33,1 +35,1 @@\n- *      -Dtarget=10000\n+ *      -XX:ShenandoahGCHeuristics=adaptive\n@@ -36,5 +38,0 @@\n- * @run main\/othervm -Xms16m -Xmx512m -XX:+UnlockDiagnosticVMOptions -XX:+UnlockExperimentalVMOptions\n- *      -XX:+UseShenandoahGC -XX:ShenandoahGCMode=passive\n- *      -XX:-ShenandoahDegeneratedGC\n- *      -Dtarget=10000\n- *      TestDynamicSoftMaxHeapSize\n@@ -43,2 +40,2 @@\n-\/*\n- * @test id=aggressive\n+\/**\n+ * @test id=satb-aggressive\n@@ -48,3 +45,5 @@\n- * @run main\/othervm -Xms16m -Xmx512m -XX:+UnlockDiagnosticVMOptions -XX:+UnlockExperimentalVMOptions\n- *      -XX:+UseShenandoahGC -XX:ShenandoahGCHeuristics=aggressive\n- *      -Dtarget=1000\n+ * @run main\/othervm -Xms100m -Xmx512m -XX:+UnlockDiagnosticVMOptions -XX:+UnlockExperimentalVMOptions\n+ *      -XX:+UseShenandoahGC -Xlog:gc=info -Dtarget=10000\n+ *      -XX:ShenandoahGCMode=satb\n+ *      -XX:+ShenandoahDegeneratedGC\n+ *      -XX:ShenandoahGCHeuristics=aggressive\n@@ -52,0 +51,1 @@\n+ *\n@@ -54,2 +54,2 @@\n-\/*\n- * @test id=adaptive\n+\/**\n+ * @test id=satb-compact\n@@ -59,3 +59,5 @@\n- * @run main\/othervm -Xms16m -Xmx512m -XX:+UnlockDiagnosticVMOptions -XX:+UnlockExperimentalVMOptions\n- *      -XX:+UseShenandoahGC -XX:ShenandoahGCHeuristics=adaptive\n- *      -Dtarget=10000\n+ * @run main\/othervm -Xms100m -Xmx512m -XX:+UnlockDiagnosticVMOptions -XX:+UnlockExperimentalVMOptions\n+ *      -XX:+UseShenandoahGC -Xlog:gc=info -Dtarget=10000\n+ *      -XX:ShenandoahGCMode=satb\n+ *      -XX:+ShenandoahDegeneratedGC\n+ *      -XX:ShenandoahGCHeuristics=compact\n@@ -63,0 +65,1 @@\n+ *\n@@ -65,2 +68,2 @@\n-\/*\n- * @test id=static\n+\/**\n+ * @test id=satb-static\n@@ -70,3 +73,5 @@\n- * @run main\/othervm -Xms16m -Xmx512m -XX:+UnlockDiagnosticVMOptions -XX:+UnlockExperimentalVMOptions\n- *      -XX:+UseShenandoahGC -XX:ShenandoahGCHeuristics=static\n- *      -Dtarget=10000\n+ * @run main\/othervm -Xms100m -Xmx512m -XX:+UnlockDiagnosticVMOptions -XX:+UnlockExperimentalVMOptions\n+ *      -XX:+UseShenandoahGC -Xlog:gc=info -Dtarget=10000\n+ *      -XX:ShenandoahGCMode=satb\n+ *      -XX:+ShenandoahDegeneratedGC\n+ *      -XX:ShenandoahGCHeuristics=static\n@@ -74,0 +79,1 @@\n+ *\n@@ -76,2 +82,2 @@\n-\/*\n- * @test id=compact\n+\/**\n+ * @test id=passive\n@@ -82,3 +88,10 @@\n- *      -XX:+UseShenandoahGC -XX:ShenandoahGCHeuristics=compact\n- *      -Dtarget=1000\n- *     TestDynamicSoftMaxHeapSize\n+ *      -XX:+UseShenandoahGC -XX:ShenandoahGCMode=passive\n+ *      -XX:+ShenandoahDegeneratedGC\n+ *      -Dtarget=10000\n+ *      TestDynamicSoftMaxHeapSize\n+ *\n+ * @run main\/othervm -Xms16m -Xmx512m -XX:+UnlockDiagnosticVMOptions -XX:+UnlockExperimentalVMOptions\n+ *      -XX:+UseShenandoahGC -XX:ShenandoahGCMode=passive\n+ *      -XX:-ShenandoahDegeneratedGC\n+ *      -Dtarget=10000\n+ *      TestDynamicSoftMaxHeapSize\n@@ -87,2 +100,2 @@\n-\/*\n- * @test id=iu-aggressive\n+\/**\n+ * @test id=generational\n@@ -92,3 +105,4 @@\n- * @run main\/othervm -Xms16m -Xmx512m -XX:+UnlockDiagnosticVMOptions -XX:+UnlockExperimentalVMOptions\n- *      -XX:+UseShenandoahGC -XX:ShenandoahGCMode=iu -XX:ShenandoahGCHeuristics=aggressive\n- *      -Dtarget=1000\n+ * @run main\/othervm -Xms100m -Xmx512m -XX:+UnlockDiagnosticVMOptions -XX:+UnlockExperimentalVMOptions\n+ *      -XX:+UseShenandoahGC -Xlog:gc=info -Dtarget=10000\n+ *      -XX:ShenandoahGCMode=generational\n+ *      -XX:ShenandoahGCHeuristics=adaptive\n@@ -96,0 +110,1 @@\n+ *\n@@ -98,2 +113,2 @@\n-\/*\n- * @test id=iu\n+\/**\n+ * @test id=generational-softMaxHeapSizeValidation\n@@ -103,3 +118,1 @@\n- * @run main\/othervm -Xms16m -Xmx512m -XX:+UnlockDiagnosticVMOptions -XX:+UnlockExperimentalVMOptions\n- *      -XX:+UseShenandoahGC -XX:ShenandoahGCMode=iu\n- *      -Dtarget=10000\n+ * @run main\/othervm -DvalidateSoftMaxHeap=true\n@@ -107,0 +120,4 @@\n+ *      -Xms100m -Xmx512m -XX:+UnlockDiagnosticVMOptions -XX:+UnlockExperimentalVMOptions\n+ *      -XX:+UseShenandoahGC -Xlog:gc=info -Dtarget=10000 -DverifySoftMaxHeapValue=true\n+ *      -XX:ShenandoahGCMode=generational\n+ *      -XX:ShenandoahGCHeuristics=adaptive\n@@ -108,2 +125,0 @@\n-\n-import java.util.Random;\n@@ -115,0 +130,5 @@\n+import java.util.ArrayList;\n+import java.util.Arrays;\n+import java.util.List;\n+import java.util.Random;\n+\n@@ -116,0 +136,3 @@\n+    static final int K = 1024;\n+    static final int XMS_MB = 100;\n+    static final int XMX_MB = 512;\n@@ -117,2 +140,3 @@\n-    static final long TARGET_MB = Long.getLong(\"target\", 10_000); \/\/ 10 Gb allocation\n-    static final long STRIDE = 10_000_000;\n+    public static void main(String[] args) throws Exception {\n+        if (\"true\".equals(System.getProperty(\"validateSoftMaxHeap\"))) {\n+            List<String> flagArgs = new ArrayList<>(Arrays.asList(args));\n@@ -120,1 +144,4 @@\n-    static volatile Object sink;\n+            int softMaxInMb = Utils.getRandomInstance().nextInt(XMS_MB, XMX_MB);\n+            flagArgs.add(\"-DsoftMaxCapacity=\" + softMaxInMb * K * K);\n+            flagArgs.add(\"-Dtest.jdk=\" + System.getProperty(\"test.jdk\"));\n+            flagArgs.add(\"-Dcompile.jdk=\" + System.getProperty(\"compile.jdk\"));\n@@ -122,10 +149,29 @@\n-    public static void main(String[] args) throws Exception {\n-        long count = TARGET_MB * 1024 * 1024 \/ 16;\n-        Random r = Utils.getRandomInstance();\n-        PidJcmdExecutor jcmd = new PidJcmdExecutor();\n-\n-        for (long c = 0; c < count; c += STRIDE) {\n-            \/\/ Sizes specifically include heaps below Xms and above Xmx to test saturation code.\n-            jcmd.execute(\"VM.set_flag SoftMaxHeapSize \" + r.nextInt(768*1024*1024), true);\n-            for (long s = 0; s < STRIDE; s++) {\n-                sink = new Object();\n+            flagArgs.add(SoftMaxWithExpectationTest.class.getName());\n+\n+            ProcessBuilder genShenPbValidateFlag = ProcessTools.createLimitedTestJavaProcessBuilder(flagArgs);\n+            OutputAnalyzer output = new OutputAnalyzer(genShenPbValidateFlag.start());\n+            output.shouldHaveExitValue(0);\n+            output.shouldContain(String.format(\"Soft Max Heap Size: %dM -> %dM\", XMX_MB, softMaxInMb)); \/\/ By default, the soft max heap size is Xmx\n+        } else {\n+            SoftMaxSetFlagOnlyTest.test();\n+        }\n+    }\n+\n+    public static class SoftMaxSetFlagOnlyTest {\n+        static final long TARGET_MB = Long.getLong(\"target\", 10_000); \/\/ 10 Gb allocation\n+        static final long STRIDE = 10_000_000;\n+\n+        static volatile Object sink;\n+\n+        public static void test() throws Exception {\n+            long count = TARGET_MB * 1024 * 1024 \/ 16;\n+            Random r = Utils.getRandomInstance();\n+            PidJcmdExecutor jcmd = new PidJcmdExecutor();\n+\n+            for (long c = 0; c < count; c += STRIDE) {\n+                \/\/ Sizes specifically include heaps below Xms and above Xmx to test saturation code.\n+                jcmd.execute(\"VM.set_flag SoftMaxHeapSize \" + r.nextInt(768*1024*1024), true);\n+                for (long s = 0; s < STRIDE; s++) {\n+                    sink = new Object();\n+                }\n+                Thread.sleep(1);\n@@ -133,1 +179,0 @@\n-            Thread.sleep(1);\n@@ -137,0 +182,15 @@\n+    public static class SoftMaxWithExpectationTest {\n+        static final long TOTAL = 100_000_000;\n+\n+        static volatile Object sink;\n+\n+        public static void main(String[] args) throws Exception {\n+            int expectedSoftMaxHeapSize = Integer.getInteger(\"softMaxCapacity\", 0);\n+            PidJcmdExecutor jcmd = new PidJcmdExecutor();\n+            jcmd.execute(\"VM.set_flag SoftMaxHeapSize \" + expectedSoftMaxHeapSize, false);\n+\n+            for (long s = 0; s < TOTAL; s++) {\n+                sink = new Object();\n+            }\n+        }\n+    }\n","filename":"test\/hotspot\/jtreg\/gc\/shenandoah\/TestDynamicSoftMaxHeapSize.java","additions":116,"deletions":56,"binary":false,"changes":172,"status":"modified"},{"patch":"@@ -3,0 +3,1 @@\n+ * Copyright Amazon.com Inc. or its affiliates. All Rights Reserved.\n@@ -26,1 +27,1 @@\n- * @test\n+ * @test id=default\n@@ -32,1 +33,11 @@\n- * @run driver\/timeout=480 TestEvilSyncBug\n+ * @run driver\/timeout=480 TestEvilSyncBug -XX:ShenandoahGCHeuristics=aggressive\n+ *\/\n+\n+\/*\n+ * @test id=generational\n+ * @summary Tests for crash\/assert when attaching init thread during shutdown\n+ * @requires vm.gc.Shenandoah\n+ * @library \/test\/lib\n+ * @modules java.base\/jdk.internal.misc\n+ *          java.management\n+ * @run driver\/timeout=480 TestEvilSyncBug -XX:ShenandoahGCMode=generational\n@@ -49,1 +60,1 @@\n-        if (args.length > 0) {\n+        if (\"test\".equals(args[0])) {\n@@ -52,0 +63,1 @@\n+            String options = args[0];\n@@ -64,1 +76,1 @@\n-                            \"-XX:ShenandoahGCHeuristics=aggressive\",\n+                            options,\n","filename":"test\/hotspot\/jtreg\/gc\/shenandoah\/TestEvilSyncBug.java","additions":16,"deletions":4,"binary":false,"changes":20,"status":"modified"},{"patch":"@@ -3,0 +3,1 @@\n+ * Copyright Amazon.com Inc. or its affiliates. All Rights Reserved.\n@@ -80,1 +81,1 @@\n- * @test id=iu\n+ * @test id=generational\n@@ -85,1 +86,1 @@\n- *      -XX:+UseShenandoahGC -XX:ShenandoahGCMode=iu\n+ *      -XX:+UseShenandoahGC\n@@ -87,1 +88,1 @@\n- *      -Dtarget=1000\n+ *      -Dtarget=1000 -XX:ShenandoahGCMode=generational\n@@ -91,3 +92,3 @@\n- *      -XX:+UseShenandoahGC -XX:ShenandoahGCMode=iu -XX:ShenandoahGCHeuristics=aggressive\n- *      -XX:ConcGCThreads=2 -XX:ParallelGCThreads=4\n- *      -Dtarget=1000\n+ *      -XX:+UseShenandoahGC\n+ *      -XX:-UseDynamicNumberOfGCThreads\n+ *      -Dtarget=1000 -XX:ShenandoahGCMode=generational\n@@ -95,1 +96,1 @@\n-*\/\n+ *\/\n","filename":"test\/hotspot\/jtreg\/gc\/shenandoah\/TestGCThreadGroups.java","additions":8,"deletions":7,"binary":false,"changes":15,"status":"modified"},{"patch":"@@ -3,0 +3,1 @@\n+ * Copyright Amazon.com Inc. or its affiliates. All Rights Reserved.\n@@ -66,0 +67,5 @@\n+ *      -XX:+UseShenandoahGC\n+ *      -XX:-UseTLAB -XX:+ShenandoahVerify\n+ *      TestHeapUncommit\n+ *\n+ * @run main\/othervm -Xmx1g -XX:+UnlockDiagnosticVMOptions -XX:+UnlockExperimentalVMOptions -XX:+ShenandoahUncommit -XX:ShenandoahUncommitDelay=0\n@@ -80,5 +86,0 @@\n- *\n- * @run main\/othervm -Xmx1g -XX:+UnlockDiagnosticVMOptions -XX:+UnlockExperimentalVMOptions -XX:+ShenandoahUncommit -XX:ShenandoahUncommitDelay=0\n- *      -XX:+UseShenandoahGC\n- *      -XX:-UseTLAB -XX:+ShenandoahVerify\n- *      TestHeapUncommit\n@@ -88,1 +89,1 @@\n- * @test id=iu\n+ * @test id=generational\n@@ -95,1 +96,1 @@\n- *      -XX:+UseShenandoahGC -XX:ShenandoahGCMode=iu\n+ *      -XX:+UseShenandoahGC -XX:ShenandoahGCHeuristics=adaptive -XX:ShenandoahGCMode=generational\n@@ -100,1 +101,2 @@\n- *      -XX:+UseShenandoahGC -XX:ShenandoahGCMode=iu\n+ *      -XX:+UseShenandoahGC -XX:ShenandoahGCHeuristics=adaptive -XX:ShenandoahGCMode=generational\n+ *      -XX:-UseTLAB -XX:+ShenandoahVerify\n@@ -104,1 +106,1 @@\n- *      -XX:+UseShenandoahGC -XX:ShenandoahGCMode=iu -XX:ShenandoahGCHeuristics=aggressive\n+ *      -XX:+UseShenandoahGC -XX:ShenandoahGCHeuristics=adaptive -XX:ShenandoahGCMode=generational\n@@ -122,0 +124,1 @@\n+ *      -XX:-UseTLAB -XX:+ShenandoahVerify\n@@ -126,1 +129,0 @@\n- *      -XX:-UseTLAB -XX:+ShenandoahVerify\n","filename":"test\/hotspot\/jtreg\/gc\/shenandoah\/TestHeapUncommit.java","additions":12,"deletions":10,"binary":false,"changes":22,"status":"modified"},{"patch":"@@ -1,131 +0,0 @@\n-\/*\n- * Copyright (c) 2017, 2018, Red Hat, Inc. All rights reserved.\n- * DO NOT ALTER OR REMOVE COPYRIGHT NOTICES OR THIS FILE HEADER.\n- *\n- * This code is free software; you can redistribute it and\/or modify it\n- * under the terms of the GNU General Public License version 2 only, as\n- * published by the Free Software Foundation.\n- *\n- * This code is distributed in the hope that it will be useful, but WITHOUT\n- * ANY WARRANTY; without even the implied warranty of MERCHANTABILITY or\n- * FITNESS FOR A PARTICULAR PURPOSE.  See the GNU General Public License\n- * version 2 for more details (a copy is included in the LICENSE file that\n- * accompanied this code).\n- *\n- * You should have received a copy of the GNU General Public License version\n- * 2 along with this work; if not, write to the Free Software Foundation,\n- * Inc., 51 Franklin St, Fifth Floor, Boston, MA 02110-1301 USA.\n- *\n- * Please contact Oracle, 500 Oracle Parkway, Redwood Shores, CA 94065 USA\n- * or visit www.oracle.com if you need additional information or have any\n- * questions.\n- *\n- *\/\n-\n-\/*\n- * @test id=default\n- * @key randomness\n- * @requires vm.gc.Shenandoah\n- * @library \/test\/lib\n- *\n- * @run main\/othervm -XX:+UnlockDiagnosticVMOptions -XX:+UnlockExperimentalVMOptions -XX:+UseShenandoahGC -Xmx1g\n- *                   -XX:+ShenandoahVerify\n- *                   TestHumongousThreshold\n- * @run main\/othervm -XX:+UnlockDiagnosticVMOptions -XX:+UnlockExperimentalVMOptions -XX:+UseShenandoahGC -Xmx1g\n- *                   -XX:+ShenandoahVerify -XX:ShenandoahHumongousThreshold=50\n- *                   TestHumongousThreshold\n- * @run main\/othervm -XX:+UnlockDiagnosticVMOptions -XX:+UnlockExperimentalVMOptions -XX:+UseShenandoahGC -Xmx1g\n- *                   -XX:+ShenandoahVerify -XX:ShenandoahHumongousThreshold=90\n- *                   TestHumongousThreshold\n- * @run main\/othervm -XX:+UnlockDiagnosticVMOptions -XX:+UnlockExperimentalVMOptions -XX:+UseShenandoahGC -Xmx1g\n- *                   -XX:+ShenandoahVerify -XX:ShenandoahHumongousThreshold=99\n- *                   TestHumongousThreshold\n- * @run main\/othervm -XX:+UnlockDiagnosticVMOptions -XX:+UnlockExperimentalVMOptions -XX:+UseShenandoahGC -Xmx1g\n- *                   -XX:+ShenandoahVerify -XX:ShenandoahHumongousThreshold=100\n- *                   TestHumongousThreshold\n- *\n- * @run main\/othervm -XX:+UnlockDiagnosticVMOptions -XX:+UnlockExperimentalVMOptions -XX:+UseShenandoahGC -Xmx1g\n- *                   -XX:-UseTLAB -XX:+ShenandoahVerify\n- *                   TestHumongousThreshold\n- * @run main\/othervm -XX:+UnlockDiagnosticVMOptions -XX:+UnlockExperimentalVMOptions -XX:+UseShenandoahGC -Xmx1g\n- *                   -XX:-UseTLAB -XX:+ShenandoahVerify -XX:ShenandoahHumongousThreshold=50\n- *                   TestHumongousThreshold\n- * @run main\/othervm -XX:+UnlockDiagnosticVMOptions -XX:+UnlockExperimentalVMOptions -XX:+UseShenandoahGC -Xmx1g\n- *                   -XX:-UseTLAB -XX:+ShenandoahVerify -XX:ShenandoahHumongousThreshold=90\n- *                   TestHumongousThreshold\n- * @run main\/othervm -XX:+UnlockDiagnosticVMOptions -XX:+UnlockExperimentalVMOptions -XX:+UseShenandoahGC -Xmx1g\n- *                   -XX:-UseTLAB -XX:+ShenandoahVerify -XX:ShenandoahHumongousThreshold=99\n- *                   TestHumongousThreshold\n- * @run main\/othervm -XX:+UnlockDiagnosticVMOptions -XX:+UnlockExperimentalVMOptions -XX:+UseShenandoahGC -Xmx1g\n- *                   -XX:-UseTLAB -XX:+ShenandoahVerify -XX:ShenandoahHumongousThreshold=100\n- *                   TestHumongousThreshold\n- *\n- * @run main\/othervm -XX:+UnlockDiagnosticVMOptions -XX:+UnlockExperimentalVMOptions -XX:+UseShenandoahGC -Xmx1g\n- *                   -XX:ShenandoahHumongousThreshold=90 -XX:ShenandoahGCHeuristics=aggressive\n- *                   TestHumongousThreshold\n- * @run main\/othervm -XX:+UnlockDiagnosticVMOptions -XX:+UnlockExperimentalVMOptions -XX:+UseShenandoahGC -Xmx1g\n- *                   -XX:-UseTLAB -XX:ShenandoahHumongousThreshold=90 -XX:ShenandoahGCHeuristics=aggressive\n- *                   TestHumongousThreshold\n- *\/\n-\n-\/*\n- * @test id=16b\n- * @key randomness\n- * @requires vm.gc.Shenandoah\n- * @requires vm.bits == \"64\"\n- * @library \/test\/lib\n- *\n- * @run main\/othervm -XX:+UnlockDiagnosticVMOptions -XX:+UnlockExperimentalVMOptions -XX:+UseShenandoahGC -Xmx1g\n- *                   -XX:ObjectAlignmentInBytes=16 -XX:+ShenandoahVerify\n- *                   TestHumongousThreshold\n- * @run main\/othervm -XX:+UnlockDiagnosticVMOptions -XX:+UnlockExperimentalVMOptions -XX:+UseShenandoahGC -Xmx1g\n- *                   -XX:ObjectAlignmentInBytes=16 -XX:+ShenandoahVerify -XX:ShenandoahHumongousThreshold=50\n- *                   TestHumongousThreshold\n- * @run main\/othervm -XX:+UnlockDiagnosticVMOptions -XX:+UnlockExperimentalVMOptions -XX:+UseShenandoahGC -Xmx1g\n- *                   -XX:ObjectAlignmentInBytes=16 -XX:+ShenandoahVerify -XX:ShenandoahHumongousThreshold=90\n- *                   TestHumongousThreshold\n- * @run main\/othervm -XX:+UnlockDiagnosticVMOptions -XX:+UnlockExperimentalVMOptions -XX:+UseShenandoahGC -Xmx1g\n- *                   -XX:ObjectAlignmentInBytes=16 -XX:+ShenandoahVerify -XX:ShenandoahHumongousThreshold=99\n- *                   TestHumongousThreshold\n- * @run main\/othervm -XX:+UnlockDiagnosticVMOptions -XX:+UnlockExperimentalVMOptions -XX:+UseShenandoahGC -Xmx1g\n- *                   -XX:ObjectAlignmentInBytes=16 -XX:+ShenandoahVerify -XX:ShenandoahHumongousThreshold=100\n- *                   TestHumongousThreshold\n- *\n- * @run main\/othervm -XX:+UnlockDiagnosticVMOptions -XX:+UnlockExperimentalVMOptions -XX:+UseShenandoahGC -Xmx1g\n- *                   -XX:-UseTLAB -XX:ObjectAlignmentInBytes=16 -XX:+ShenandoahVerify\n- *                   TestHumongousThreshold\n- * @run main\/othervm -XX:+UnlockDiagnosticVMOptions -XX:+UnlockExperimentalVMOptions -XX:+UseShenandoahGC -Xmx1g\n- *                   -XX:-UseTLAB -XX:ObjectAlignmentInBytes=16 -XX:+ShenandoahVerify -XX:ShenandoahHumongousThreshold=50\n- *                   TestHumongousThreshold\n- * @run main\/othervm -XX:+UnlockDiagnosticVMOptions -XX:+UnlockExperimentalVMOptions -XX:+UseShenandoahGC -Xmx1g\n- *                   -XX:-UseTLAB -XX:ObjectAlignmentInBytes=16 -XX:+ShenandoahVerify -XX:ShenandoahHumongousThreshold=90\n- *                   TestHumongousThreshold\n- * @run main\/othervm -XX:+UnlockDiagnosticVMOptions -XX:+UnlockExperimentalVMOptions -XX:+UseShenandoahGC -Xmx1g\n- *                   -XX:-UseTLAB -XX:ObjectAlignmentInBytes=16 -XX:+ShenandoahVerify -XX:ShenandoahHumongousThreshold=99\n- *                   TestHumongousThreshold\n- * @run main\/othervm -XX:+UnlockDiagnosticVMOptions -XX:+UnlockExperimentalVMOptions -XX:+UseShenandoahGC -Xmx1g\n- *                   -XX:-UseTLAB -XX:ObjectAlignmentInBytes=16 -XX:+ShenandoahVerify -XX:ShenandoahHumongousThreshold=100\n- *                   TestHumongousThreshold\n- *\/\n-\n-import java.util.Random;\n-import jdk.test.lib.Utils;\n-\n-public class TestHumongousThreshold {\n-\n-    static final long TARGET_MB = Long.getLong(\"target\", 20_000); \/\/ 20 Gb allocation\n-\n-    static volatile Object sink;\n-\n-    public static void main(String[] args) throws Exception {\n-        final int min = 0;\n-        final int max = 384 * 1024;\n-        long count = TARGET_MB * 1024 * 1024 \/ (16 + 4 * (min + (max - min) \/ 2));\n-\n-        Random r = Utils.getRandomInstance();\n-        for (long c = 0; c < count; c++) {\n-            sink = new int[min + r.nextInt(max - min)];\n-        }\n-    }\n-\n-}\n","filename":"test\/hotspot\/jtreg\/gc\/shenandoah\/TestHumongousThreshold.java","additions":0,"deletions":131,"binary":false,"changes":131,"status":"deleted"},{"patch":"@@ -3,0 +3,1 @@\n+ * Copyright Amazon.com Inc. or its affiliates. All Rights Reserved.\n@@ -76,1 +77,1 @@\n- * @test id=static\n+ * @test id=generational\n@@ -82,1 +83,2 @@\n- *      -XX:+UseShenandoahGC -XX:ShenandoahGCHeuristics=static\n+ *      -XX:+UseShenandoahGC -XX:ShenandoahGCHeuristics=adaptive -XX:ShenandoahGCMode=generational\n+ *      -Dtarget=10000\n@@ -87,12 +89,1 @@\n- * @test id=compact\n- * @library \/test\/lib\n- * @modules jdk.attach\/com.sun.tools.attach\n- * @requires vm.gc.Shenandoah\n- *\n- * @run main\/othervm\/timeout=480 -Xmx16m -XX:+UnlockDiagnosticVMOptions -XX:+UnlockExperimentalVMOptions\n- *      -XX:+UseShenandoahGC -XX:ShenandoahGCHeuristics=compact\n- *     TestJcmdHeapDump\n- *\/\n-\n-\/*\n- * @test id=iu-aggressive\n+ * @test id=static\n@@ -104,11 +95,1 @@\n- *      -XX:+UseShenandoahGC -XX:ShenandoahGCMode=iu -XX:ShenandoahGCHeuristics=aggressive\n- *      -XX:+ShenandoahOOMDuringEvacALot\n- *      TestJcmdHeapDump\n- *\n- * @run main\/othervm\/timeout=480 -Xmx16m -XX:+UnlockDiagnosticVMOptions -XX:+UnlockExperimentalVMOptions\n- *      -XX:+UseShenandoahGC -XX:ShenandoahGCMode=iu -XX:ShenandoahGCHeuristics=aggressive\n- *      -XX:+ShenandoahAllocFailureALot\n- *      TestJcmdHeapDump\n- *\n- * @run main\/othervm\/timeout=480 -Xmx16m -XX:+UnlockDiagnosticVMOptions -XX:+UnlockExperimentalVMOptions\n- *      -XX:+UseShenandoahGC -XX:ShenandoahGCMode=iu -XX:ShenandoahGCHeuristics=aggressive\n+ *      -XX:+UseShenandoahGC -XX:ShenandoahGCHeuristics=static\n@@ -119,2 +100,1 @@\n- * @test id=iu\n- * @requires vm.gc.Shenandoah\n+ * @test id=compact\n@@ -123,0 +103,1 @@\n+ * @requires vm.gc.Shenandoah\n@@ -125,2 +106,2 @@\n- *      -XX:+UseShenandoahGC -XX:ShenandoahGCMode=iu\n- *      TestJcmdHeapDump\n+ *      -XX:+UseShenandoahGC -XX:ShenandoahGCHeuristics=compact\n+ *     TestJcmdHeapDump\n","filename":"test\/hotspot\/jtreg\/gc\/shenandoah\/TestJcmdHeapDump.java","additions":10,"deletions":29,"binary":false,"changes":39,"status":"modified"},{"patch":"@@ -3,0 +3,1 @@\n+ * Copyright Amazon.com Inc. or its affiliates. All Rights Reserved.\n@@ -26,1 +27,1 @@\n- * @test\n+ * @test id=default\n@@ -37,0 +38,14 @@\n+ *\/\n+\n+\/*\n+ * @test id=generational\n+ * @summary Shenandoah crashes with -XX:ObjectAlignmentInBytes=16\n+ * @key randomness\n+ * @requires vm.gc.Shenandoah\n+ * @requires vm.bits == \"64\"\n+ * @library \/test\/lib\n+ *\n+ * @run main\/othervm -XX:+UnlockExperimentalVMOptions -XX:+UseShenandoahGC -XX:ShenandoahGCMode=generational -XX:ObjectAlignmentInBytes=16 -Xint                   TestLargeObjectAlignment\n+ * @run main\/othervm -XX:+UnlockExperimentalVMOptions -XX:+UseShenandoahGC -XX:ShenandoahGCMode=generational -XX:ObjectAlignmentInBytes=16 -XX:-TieredCompilation  TestLargeObjectAlignment\n+ * @run main\/othervm -XX:+UnlockExperimentalVMOptions -XX:+UseShenandoahGC -XX:ShenandoahGCMode=generational -XX:ObjectAlignmentInBytes=16 -XX:TieredStopAtLevel=1 TestLargeObjectAlignment\n+ * @run main\/othervm -XX:+UnlockExperimentalVMOptions -XX:+UseShenandoahGC -XX:ShenandoahGCMode=generational -XX:ObjectAlignmentInBytes=16 -XX:TieredStopAtLevel=4 TestLargeObjectAlignment\n","filename":"test\/hotspot\/jtreg\/gc\/shenandoah\/TestLargeObjectAlignment.java","additions":16,"deletions":1,"binary":false,"changes":17,"status":"modified"},{"patch":"@@ -3,0 +3,1 @@\n+ * Copyright Amazon.com Inc. or its affiliates. All Rights Reserved.\n@@ -75,1 +76,1 @@\n- * @test id=static\n+ * @test id=generational\n@@ -79,1 +80,1 @@\n- *      -XX:+UseShenandoahGC -XX:ShenandoahGCHeuristics=static\n+ *      -XX:+UseShenandoahGC -XX:ShenandoahGCHeuristics=adaptive -XX:ShenandoahGCMode=generational\n@@ -85,11 +86,1 @@\n- * @test id=compact\n- * @requires vm.gc.Shenandoah\n- *\n- * @run main\/othervm\/timeout=480 -Xmx16m -XX:+UnlockDiagnosticVMOptions -XX:+UnlockExperimentalVMOptions\n- *      -XX:+UseShenandoahGC -XX:ShenandoahGCHeuristics=compact\n- *      -Dtarget=1000\n- *     TestLotsOfCycles\n- *\/\n-\n-\/*\n- * @test id=iu-aggressive\n+ * @test id=static\n@@ -99,14 +90,2 @@\n- *      -XX:+UseShenandoahGC -XX:ShenandoahGCMode=iu -XX:ShenandoahGCHeuristics=aggressive\n- *      -XX:+ShenandoahOOMDuringEvacALot\n- *      -Dtarget=1000\n- *      TestLotsOfCycles\n- *\n- * @run main\/othervm\/timeout=480 -Xmx16m -XX:+UnlockDiagnosticVMOptions -XX:+UnlockExperimentalVMOptions\n- *      -XX:+UseShenandoahGC -XX:ShenandoahGCMode=iu -XX:ShenandoahGCHeuristics=aggressive\n- *      -XX:+ShenandoahAllocFailureALot\n- *      -Dtarget=1000\n- *      TestLotsOfCycles\n- *\n- * @run main\/othervm\/timeout=480 -Xmx16m -XX:+UnlockDiagnosticVMOptions -XX:+UnlockExperimentalVMOptions\n- *      -XX:+UseShenandoahGC -XX:ShenandoahGCMode=iu -XX:ShenandoahGCHeuristics=aggressive\n- *      -Dtarget=1000\n+ *      -XX:+UseShenandoahGC -XX:ShenandoahGCHeuristics=static\n+ *      -Dtarget=10000\n@@ -117,1 +96,1 @@\n- * @test id=iu\n+ * @test id=compact\n@@ -121,3 +100,3 @@\n- *      -XX:+UseShenandoahGC -XX:ShenandoahGCMode=iu\n- *      -Dtarget=10000\n- *      TestLotsOfCycles\n+ *      -XX:+UseShenandoahGC -XX:ShenandoahGCHeuristics=compact\n+ *      -Dtarget=1000\n+ *     TestLotsOfCycles\n","filename":"test\/hotspot\/jtreg\/gc\/shenandoah\/TestLotsOfCycles.java","additions":10,"deletions":31,"binary":false,"changes":41,"status":"modified"},{"patch":"@@ -58,3 +58,3 @@\n-             {{\"satb\"},    {\"adaptive\", \"compact\", \"static\", \"aggressive\"}},\n-             {{\"iu\"},      {\"adaptive\", \"aggressive\"}},\n-             {{\"passive\"}, {\"passive\"}}\n+             {{\"satb\"},         {\"adaptive\", \"compact\", \"static\", \"aggressive\"}},\n+             {{\"generational\"}, {\"adaptive\"}},\n+             {{\"passive\"},      {\"passive\"}}\n","filename":"test\/hotspot\/jtreg\/gc\/shenandoah\/TestObjItrWithHeapDump.java","additions":3,"deletions":3,"binary":false,"changes":6,"status":"modified"},{"patch":"@@ -1,50 +0,0 @@\n-\/*\n- * Copyright (c) 2017, 2018, Red Hat, Inc. All rights reserved.\n- * DO NOT ALTER OR REMOVE COPYRIGHT NOTICES OR THIS FILE HEADER.\n- *\n- * This code is free software; you can redistribute it and\/or modify it\n- * under the terms of the GNU General Public License version 2 only, as\n- * published by the Free Software Foundation.\n- *\n- * This code is distributed in the hope that it will be useful, but WITHOUT\n- * ANY WARRANTY; without even the implied warranty of MERCHANTABILITY or\n- * FITNESS FOR A PARTICULAR PURPOSE.  See the GNU General Public License\n- * version 2 for more details (a copy is included in the LICENSE file that\n- * accompanied this code).\n- *\n- * You should have received a copy of the GNU General Public License version\n- * 2 along with this work; if not, write to the Free Software Foundation,\n- * Inc., 51 Franklin St, Fifth Floor, Boston, MA 02110-1301 USA.\n- *\n- * Please contact Oracle, 500 Oracle Parkway, Redwood Shores, CA 94065 USA\n- * or visit www.oracle.com if you need additional information or have any\n- * questions.\n- *\n- *\/\n-\n-\/*\n- * @test\n- * @summary Test that reference processing works with both parallel and non-parallel variants.\n- * @requires vm.gc.Shenandoah\n- *\n- * @run main\/othervm -XX:+UnlockExperimentalVMOptions -XX:+UseShenandoahGC -Xmx1g -Xms1g                              TestParallelRefprocSanity\n- * @run main\/othervm -XX:+UnlockExperimentalVMOptions -XX:+UseShenandoahGC -Xmx1g -Xms1g  -XX:-ParallelRefProcEnabled TestParallelRefprocSanity\n- * @run main\/othervm -XX:+UnlockExperimentalVMOptions -XX:+UseShenandoahGC -Xmx1g -Xms1g  -XX:+ParallelRefProcEnabled TestParallelRefprocSanity\n- *\/\n-\n-import java.lang.ref.*;\n-\n-public class TestParallelRefprocSanity {\n-\n-    static final long TARGET_MB = Long.getLong(\"target\", 10_000); \/\/ 10 Gb allocation\n-\n-    static volatile Object sink;\n-\n-    public static void main(String[] args) throws Exception {\n-        long count = TARGET_MB * 1024 * 1024 \/ 32;\n-        for (long c = 0; c < count; c++) {\n-            sink = new WeakReference<Object>(new Object());\n-        }\n-    }\n-\n-}\n","filename":"test\/hotspot\/jtreg\/gc\/shenandoah\/TestParallelRefprocSanity.java","additions":0,"deletions":50,"binary":false,"changes":50,"status":"deleted"},{"patch":"@@ -3,0 +3,1 @@\n+ * Copyright Amazon.com Inc. or its affiliates. All Rights Reserved.\n@@ -48,2 +49,2 @@\n-        if (periodic && !output.getOutput().contains(\"Trigger: Time since last GC\")) {\n-            throw new AssertionError(msg + \": Should have periodic GC in logs\");\n+        if (periodic) {\n+            output.shouldContain(\"Trigger: Time since last GC\");\n@@ -51,2 +52,19 @@\n-        if (!periodic && output.getOutput().contains(\"Trigger: Time since last GC\")) {\n-            throw new AssertionError(msg + \": Should not have periodic GC in logs\");\n+        if (!periodic) {\n+            output.shouldNotContain(\"Trigger: Time since last GC\");\n+        }\n+    }\n+\n+    public static void testGenerational(boolean periodic, String... args) throws Exception {\n+        String[] cmds = Arrays.copyOf(args, args.length + 2);\n+        cmds[args.length] = TestPeriodicGC.class.getName();\n+        cmds[args.length + 1] = \"test\";\n+        ProcessBuilder pb = ProcessTools.createLimitedTestJavaProcessBuilder(cmds);\n+\n+        OutputAnalyzer output = new OutputAnalyzer(pb.start());\n+        output.shouldHaveExitValue(0);\n+        if (periodic) {\n+            output.shouldContain(\"Trigger (Young): Time since last GC\");\n+            output.shouldContain(\"Trigger (Old): Time since last GC\");\n+        } else {\n+            output.shouldNotContain(\"Trigger (Young): Time since last GC\");\n+            output.shouldNotContain(\"Trigger (Old): Time since last GC\");\n@@ -100,30 +118,0 @@\n-        testWith(\"Zero interval with iu mode\",\n-                 false,\n-                 \"-Xlog:gc\",\n-                 \"-XX:+UnlockDiagnosticVMOptions\",\n-                 \"-XX:+UnlockExperimentalVMOptions\",\n-                 \"-XX:+UseShenandoahGC\",\n-                 \"-XX:ShenandoahGCMode=iu\",\n-                 \"-XX:ShenandoahGuaranteedGCInterval=0\"\n-        );\n-\n-        testWith(\"Short interval with iu mode\",\n-                 true,\n-                 \"-Xlog:gc\",\n-                 \"-XX:+UnlockDiagnosticVMOptions\",\n-                 \"-XX:+UnlockExperimentalVMOptions\",\n-                 \"-XX:+UseShenandoahGC\",\n-                 \"-XX:ShenandoahGCMode=iu\",\n-                 \"-XX:ShenandoahGuaranteedGCInterval=1000\"\n-        );\n-\n-        testWith(\"Long interval with iu mode\",\n-                 false,\n-                 \"-Xlog:gc\",\n-                 \"-XX:+UnlockDiagnosticVMOptions\",\n-                 \"-XX:+UnlockExperimentalVMOptions\",\n-                 \"-XX:+UseShenandoahGC\",\n-                 \"-XX:ShenandoahGCMode=iu\",\n-                 \"-XX:ShenandoahGuaranteedGCInterval=100000\" \/\/ deliberately too long\n-        );\n-\n@@ -159,0 +147,20 @@\n+\n+        testGenerational(true,\n+                         \"-Xlog:gc\",\n+                         \"-XX:+UnlockDiagnosticVMOptions\",\n+                         \"-XX:+UnlockExperimentalVMOptions\",\n+                         \"-XX:+UseShenandoahGC\",\n+                         \"-XX:ShenandoahGCMode=generational\",\n+                         \"-XX:ShenandoahGuaranteedYoungGCInterval=1000\",\n+                         \"-XX:ShenandoahGuaranteedOldGCInterval=1500\"\n+        );\n+\n+        testGenerational(false,\n+                         \"-Xlog:gc\",\n+                         \"-XX:+UnlockDiagnosticVMOptions\",\n+                         \"-XX:+UnlockExperimentalVMOptions\",\n+                         \"-XX:+UseShenandoahGC\",\n+                         \"-XX:ShenandoahGCMode=generational\",\n+                         \"-XX:ShenandoahGuaranteedYoungGCInterval=0\",\n+                         \"-XX:ShenandoahGuaranteedOldGCInterval=0\"\n+        );\n","filename":"test\/hotspot\/jtreg\/gc\/shenandoah\/TestPeriodicGC.java","additions":42,"deletions":34,"binary":false,"changes":76,"status":"modified"},{"patch":"@@ -2,1 +2,1 @@\n- * Copyright (c) 2020, 2022, Oracle and\/or its affiliates. All rights reserved.\n+ * Copyright (c) 2020, 2023, Oracle and\/or its affiliates. All rights reserved.\n@@ -38,1 +38,1 @@\n-\/* @test id=iu\n+\/* @test id=satb-100\n@@ -42,1 +42,2 @@\n- * @run driver jdk.test.lib.helpers.ClassFileInstaller jdk.test.whitebox.WhiteBox\n+ * @modules java.base\n+ * @run main jdk.test.lib.helpers.ClassFileInstaller jdk.test.whitebox.WhiteBox\n@@ -46,1 +47,1 @@\n- *      -XX:+UnlockExperimentalVMOptions -XX:+UseShenandoahGC -XX:ShenandoahGCMode=iu\n+ *      -XX:+UnlockExperimentalVMOptions -XX:+UseShenandoahGC -XX:ShenandoahGCMode=satb -XX:ShenandoahGarbageThreshold=100 -Xmx100m\n@@ -50,1 +51,1 @@\n-\/* @test id=satb-100\n+\/* @test id=generational\n@@ -54,2 +55,1 @@\n- * @modules java.base\n- * @run main jdk.test.lib.helpers.ClassFileInstaller jdk.test.whitebox.WhiteBox\n+ * @run driver jdk.test.lib.helpers.ClassFileInstaller jdk.test.whitebox.WhiteBox\n@@ -59,1 +59,1 @@\n- *      -XX:+UnlockExperimentalVMOptions -XX:+UseShenandoahGC -XX:ShenandoahGCMode=satb -XX:ShenandoahGarbageThreshold=100 -Xmx100m\n+ *      -XX:+UnlockExperimentalVMOptions -XX:+UseShenandoahGC -XX:ShenandoahGCMode=generational\n@@ -63,1 +63,1 @@\n-\/* @test id=iu-100\n+\/* @test id=generational-100\n@@ -72,1 +72,1 @@\n- *      -XX:+UnlockExperimentalVMOptions -XX:+UseShenandoahGC -XX:ShenandoahGCMode=iu -XX:ShenandoahGarbageThreshold=100 -Xmx100m\n+ *      -XX:+UnlockExperimentalVMOptions -XX:+UseShenandoahGC -XX:ShenandoahGCMode=generational -XX:ShenandoahGarbageThreshold=100 -Xmx100m\n@@ -127,1 +127,5 @@\n-                fail(\"object not promoted by full gc\");\n+                \/\/ This is just a warning, because failing would\n+                \/\/ be overspecifying for generational shenandoah,\n+                \/\/ which need not necessarily promote objects upon\n+                \/\/ a full GC.\n+                warn(\"object not promoted by full gc\");\n@@ -154,0 +158,4 @@\n+    private static void warn(String msg) {\n+        System.out.println(\"Warning: \" + msg);\n+    }\n+\n@@ -203,4 +211,0 @@\n-    private static boolean isShenandoahIUMode() {\n-        return \"iu\".equals(WB.getStringVMFlag(\"ShenandoahGCMode\"));\n-    }\n-\n@@ -242,8 +246,1 @@\n-            \/\/ This is true for all currently supported concurrent collectors,\n-            \/\/ except Shenandoah+IU, which allows clearing refs even when\n-            \/\/ accessed during concurrent marking.\n-            if (isShenandoahIUMode()) {\n-              expectCleared(testWeak4, \"testWeak4\");\n-            } else {\n-              expectNotCleared(testWeak4, \"testWeak4\");\n-            }\n+            expectNotCleared(testWeak4, \"testWeak4\");\n@@ -264,6 +261,4 @@\n-            if (!isShenandoahIUMode()) {\n-                if (obj4 == null) {\n-                    fail(\"testWeak4.get() returned null\");\n-                } else if (obj4.value != 4) {\n-                    fail(\"testWeak4.get().value is \" + obj4.value);\n-                }\n+            if (obj4 == null) {\n+                fail(\"testWeak4.get() returned null\");\n+            } else if (obj4.value != 4) {\n+                fail(\"testWeak4.get().value is \" + obj4.value);\n","filename":"test\/hotspot\/jtreg\/gc\/shenandoah\/TestReferenceRefersToShenandoah.java","additions":24,"deletions":29,"binary":false,"changes":53,"status":"modified"},{"patch":"@@ -2,1 +2,1 @@\n- * Copyright (c) 2020, 2022, Oracle and\/or its affiliates. All rights reserved.\n+ * Copyright (c) 2020, 2023, Oracle and\/or its affiliates. All rights reserved.\n@@ -39,1 +39,1 @@\n-\/* @test id=iu-100\n+\/* @test id=generational-100\n@@ -48,1 +48,1 @@\n- *      -XX:+UnlockExperimentalVMOptions -XX:+UseShenandoahGC -XX:ShenandoahGCMode=iu -XX:ShenandoahGarbageThreshold=100 -Xmx100m\n+ *      -XX:+UnlockExperimentalVMOptions -XX:+UseShenandoahGC -XX:ShenandoahGCMode=generational -XX:ShenandoahGarbageThreshold=100 -Xmx100m\n","filename":"test\/hotspot\/jtreg\/gc\/shenandoah\/TestReferenceShortcutCycle.java","additions":3,"deletions":3,"binary":false,"changes":6,"status":"modified"},{"patch":"@@ -3,0 +3,1 @@\n+ * Copyright Amazon.com Inc. or its affiliates. All Rights Reserved.\n@@ -45,1 +46,1 @@\n- * @test id=iu\n+ * @test id=generational\n@@ -50,1 +51,1 @@\n- *      -XX:+UseShenandoahGC -XX:ShenandoahGCMode=iu\n+ *      -XX:+UseShenandoahGC -XX:ShenandoahGCMode=generational\n@@ -55,5 +56,1 @@\n- *      -XX:+UseShenandoahGC -XX:ShenandoahGCMode=iu\n- *      TestRefprocSanity\n- *\n- * @run main\/othervm -Xmx128m -Xms128m -XX:+UnlockDiagnosticVMOptions -XX:+UnlockExperimentalVMOptions\n- *      -XX:+UseShenandoahGC -XX:ShenandoahGCMode=iu -XX:ShenandoahGCHeuristics=aggressive\n+ *      -XX:+UseShenandoahGC -XX:ShenandoahGCMode=generational\n","filename":"test\/hotspot\/jtreg\/gc\/shenandoah\/TestRefprocSanity.java","additions":4,"deletions":7,"binary":false,"changes":11,"status":"modified"},{"patch":"@@ -3,0 +3,1 @@\n+ * Copyright Amazon.com Inc. or its affiliates. All Rights Reserved.\n@@ -50,10 +51,1 @@\n- * @test id=static\n- * @requires vm.gc.Shenandoah\n- *\n- * @run main\/othervm -Xmx1g -Xms1g -XX:+UnlockDiagnosticVMOptions -XX:+UnlockExperimentalVMOptions -XX:+ShenandoahRegionSampling\n- *      -XX:+UseShenandoahGC -XX:ShenandoahGCHeuristics=static\n- *      TestRegionSampling\n- *\/\n-\n-\/*\n- * @test id=compact\n+ * @test id=generational\n@@ -63,1 +55,1 @@\n- *      -XX:+UseShenandoahGC -XX:ShenandoahGCHeuristics=compact\n+ *      -XX:+UseShenandoahGC -XX:ShenandoahGCHeuristics=adaptive -XX:ShenandoahGCMode=generational\n@@ -68,1 +60,1 @@\n- * @test id=aggressive\n+ * @test id=static\n@@ -72,1 +64,1 @@\n- *      -XX:+UseShenandoahGC -XX:ShenandoahGCHeuristics=aggressive\n+ *      -XX:+UseShenandoahGC -XX:ShenandoahGCHeuristics=static\n@@ -77,1 +69,1 @@\n- * @test id=iu-aggressive\n+ * @test id=compact\n@@ -81,1 +73,1 @@\n- *      -XX:+UseShenandoahGC -XX:ShenandoahGCMode=iu -XX:ShenandoahGCHeuristics=aggressive\n+ *      -XX:+UseShenandoahGC -XX:ShenandoahGCHeuristics=compact\n@@ -86,1 +78,1 @@\n- * @test id=iu\n+ * @test id=aggressive\n@@ -90,1 +82,1 @@\n- *      -XX:+UseShenandoahGC -XX:ShenandoahGCMode=iu\n+ *      -XX:+UseShenandoahGC -XX:ShenandoahGCHeuristics=aggressive\n@@ -92,1 +84,0 @@\n- *\n","filename":"test\/hotspot\/jtreg\/gc\/shenandoah\/TestRegionSampling.java","additions":9,"deletions":18,"binary":false,"changes":27,"status":"modified"},{"patch":"@@ -0,0 +1,68 @@\n+\/*\n+ * Copyright Amazon.com Inc. or its affiliates. All Rights Reserved.\n+ * DO NOT ALTER OR REMOVE COPYRIGHT NOTICES OR THIS FILE HEADER.\n+ *\n+ * This code is free software; you can redistribute it and\/or modify it\n+ * under the terms of the GNU General Public License version 2 only, as\n+ * published by the Free Software Foundation.\n+ *\n+ * This code is distributed in the hope that it will be useful, but WITHOUT\n+ * ANY WARRANTY; without even the implied warranty of MERCHANTABILITY or\n+ * FITNESS FOR A PARTICULAR PURPOSE.  See the GNU General Public License\n+ * version 2 for more details (a copy is included in the LICENSE file that\n+ * accompanied this code).\n+ *\n+ * You should have received a copy of the GNU General Public License version\n+ * 2 along with this work; if not, write to the Free Software Foundation,\n+ * Inc., 51 Franklin St, Fifth Floor, Boston, MA 02110-1301 USA.\n+ *\n+ * Please contact Oracle, 500 Oracle Parkway, Redwood Shores, CA 94065 USA\n+ * or visit www.oracle.com if you need additional information or have any\n+ * questions.\n+ *\n+ *\/\n+\n+\/*\n+ * @test id=default-rotation\n+ * @requires vm.gc.Shenandoah\n+ *\n+ * @run main\/othervm -Xmx1g -Xms1g -XX:+UnlockDiagnosticVMOptions -XX:+UnlockExperimentalVMOptions\n+ *      -XX:+ShenandoahRegionSampling -XX:+ShenandoahRegionSampling\n+ *      -Xlog:gc+region=trace:region-snapshots-%p.log::filesize=100,filecount=3\n+ *      -XX:+UseShenandoahGC -XX:ShenandoahGCHeuristics=adaptive\n+ *      TestRegionSamplingLogging\n+ *\/\n+\n+\/*\n+ * @test id=generational-rotation\n+ * @requires vm.gc.Shenandoah\n+ *\n+ * @run main\/othervm -Xmx1g -Xms1g -XX:+UnlockDiagnosticVMOptions -XX:+UnlockExperimentalVMOptions\n+ *      -XX:+ShenandoahRegionSampling -XX:+ShenandoahRegionSampling\n+ *      -Xlog:gc+region=trace:region-snapshots-%p.log::filesize=100,filecount=3\n+ *      -XX:+UseShenandoahGC -XX:ShenandoahGCHeuristics=adaptive -XX:ShenandoahGCMode=generational\n+ *      TestRegionSamplingLogging\n+ *\/\n+import java.io.File;\n+import java.util.Arrays;\n+\n+public class TestRegionSamplingLogging {\n+\n+    static final long TARGET_MB = Long.getLong(\"target\", 2_000); \/\/ 2 Gb allocation\n+\n+    static volatile Object sink;\n+\n+    public static void main(String[] args) throws Exception {\n+        long count = TARGET_MB * 1024 * 1024 \/ 16;\n+        for (long c = 0; c < count; c++) {\n+            sink = new Object();\n+        }\n+\n+        File directory = new File(\".\");\n+        File[] files = directory.listFiles((dir, name) -> name.startsWith(\"region-snapshots\") && name.endsWith(\".log\"));\n+        System.out.println(Arrays.toString(files));\n+        if (files == null || files.length == 0) {\n+            throw new IllegalStateException(\"Did not find expected snapshot log file.\");\n+        }\n+    }\n+}\n","filename":"test\/hotspot\/jtreg\/gc\/shenandoah\/TestRegionSamplingLogging.java","additions":68,"deletions":0,"binary":false,"changes":68,"status":"added"},{"patch":"@@ -3,0 +3,1 @@\n+ * Copyright Amazon.com Inc. or its affiliates. All Rights Reserved.\n@@ -102,1 +103,1 @@\n- * @test id=static\n+ * @test id=generational\n@@ -109,1 +110,1 @@\n- *      -XX:+UseShenandoahGC -XX:ShenandoahGCHeuristics=static\n+ *      -XX:+UseShenandoahGC -XX:ShenandoahGCHeuristics=adaptive -XX:ShenandoahGCMode=generational\n@@ -115,1 +116,1 @@\n- *      -XX:+UseShenandoahGC -XX:ShenandoahGCHeuristics=static\n+ *      -XX:+UseShenandoahGC -XX:ShenandoahGCHeuristics=adaptive -XX:ShenandoahGCMode=generational\n@@ -122,21 +123,1 @@\n- * @test id=compact\n- * @key randomness\n- * @summary Test that Shenandoah is able to work with(out) resizeable TLABs\n- * @requires vm.gc.Shenandoah\n- * @library \/test\/lib\n- *\n- * @run main\/othervm -Xmx1g -Xms1g -XX:+UnlockDiagnosticVMOptions -XX:+UnlockExperimentalVMOptions\n- *      -XX:+UseShenandoahGC -XX:ShenandoahGCHeuristics=compact\n- *      -XX:+ShenandoahVerify\n- *      -XX:+ResizeTLAB\n- *      TestResizeTLAB\n- *\n- * @run main\/othervm -Xmx1g -Xms1g -XX:+UnlockDiagnosticVMOptions -XX:+UnlockExperimentalVMOptions\n- *      -XX:+UseShenandoahGC -XX:ShenandoahGCHeuristics=compact\n- *      -XX:+ShenandoahVerify\n- *      -XX:-ResizeTLAB\n- *      TestResizeTLAB\n- *\/\n-\n-\/*\n- * @test id=iu-aggressive\n+ * @test id=static\n@@ -149,1 +130,1 @@\n- *      -XX:+UseShenandoahGC -XX:ShenandoahGCMode=iu -XX:ShenandoahGCHeuristics=aggressive\n+ *      -XX:+UseShenandoahGC -XX:ShenandoahGCHeuristics=static\n@@ -155,1 +136,1 @@\n- *      -XX:+UseShenandoahGC -XX:ShenandoahGCMode=iu -XX:ShenandoahGCHeuristics=aggressive\n+ *      -XX:+UseShenandoahGC -XX:ShenandoahGCHeuristics=static\n@@ -162,1 +143,1 @@\n- * @test id=iu\n+ * @test id=compact\n@@ -169,1 +150,1 @@\n- *      -XX:+UseShenandoahGC -XX:ShenandoahGCMode=iu\n+ *      -XX:+UseShenandoahGC -XX:ShenandoahGCHeuristics=compact\n@@ -175,1 +156,1 @@\n- *      -XX:+UseShenandoahGC -XX:ShenandoahGCMode=iu\n+ *      -XX:+UseShenandoahGC -XX:ShenandoahGCHeuristics=compact\n","filename":"test\/hotspot\/jtreg\/gc\/shenandoah\/TestResizeTLAB.java","additions":10,"deletions":29,"binary":false,"changes":39,"status":"modified"},{"patch":"@@ -3,0 +3,1 @@\n+ * Copyright Amazon.com Inc. or its affiliates. All Rights Reserved.\n@@ -87,1 +88,1 @@\n- * @test id=static\n+ * @test id=generational\n@@ -92,1 +93,2 @@\n- *      -XX:+UseShenandoahGC -XX:ShenandoahGCHeuristics=static\n+ *      -XX:+UseShenandoahGC -XX:ShenandoahGCHeuristics=adaptive -XX:ShenandoahGCMode=generational\n+ *      -XX:+ShenandoahVerify\n@@ -94,6 +96,0 @@\n- *\/\n-\n-\/*\n- * @test id=compact\n- * @summary Acceptance tests: collector can deal with retained objects\n- * @requires vm.gc.Shenandoah\n@@ -102,1 +98,11 @@\n- *      -XX:+UseShenandoahGC -XX:ShenandoahGCHeuristics=compact\n+ *      -XX:+UseShenandoahGC -XX:ShenandoahGCHeuristics=adaptive -XX:ShenandoahGCMode=generational\n+ *      -XX:+ShenandoahOOMDuringEvacALot -XX:+ShenandoahVerify\n+ *      TestRetainObjects\n+ *\n+ * @run main\/othervm -Xmx1g -Xms1g -XX:+UnlockDiagnosticVMOptions -XX:+UnlockExperimentalVMOptions\n+ *      -XX:+UseShenandoahGC -XX:ShenandoahGCHeuristics=adaptive -XX:ShenandoahGCMode=generational\n+ *      -XX:+ShenandoahAllocFailureALot -XX:+ShenandoahVerify\n+ *      TestRetainObjects\n+ *\n+ * @run main\/othervm -Xmx1g -Xms1g -XX:+UnlockDiagnosticVMOptions -XX:+UnlockExperimentalVMOptions\n+ *      -XX:+UseShenandoahGC -XX:ShenandoahGCHeuristics=adaptive -XX:ShenandoahGCMode=generational\n@@ -107,1 +113,1 @@\n- * @test id=no-tlab\n+ * @test id=static\n@@ -112,2 +118,1 @@\n- *      -XX:+UseShenandoahGC\n- *      -XX:-UseTLAB -XX:+ShenandoahVerify\n+ *      -XX:+UseShenandoahGC -XX:ShenandoahGCHeuristics=static\n@@ -118,1 +123,1 @@\n- * @test id=iu-aggressive\n+ * @test id=compact\n@@ -123,11 +128,1 @@\n- *      -XX:+UseShenandoahGC -XX:ShenandoahGCMode=iu -XX:ShenandoahGCHeuristics=aggressive\n- *      -XX:+ShenandoahOOMDuringEvacALot\n- *      TestRetainObjects\n- *\n- * @run main\/othervm -Xmx1g -Xms1g -XX:+UnlockDiagnosticVMOptions -XX:+UnlockExperimentalVMOptions\n- *      -XX:+UseShenandoahGC -XX:ShenandoahGCMode=iu -XX:ShenandoahGCHeuristics=aggressive\n- *      -XX:+ShenandoahAllocFailureALot\n- *      TestRetainObjects\n- *\n- * @run main\/othervm -Xmx1g -Xms1g -XX:+UnlockDiagnosticVMOptions -XX:+UnlockExperimentalVMOptions\n- *      -XX:+UseShenandoahGC -XX:ShenandoahGCMode=iu -XX:ShenandoahGCHeuristics=aggressive\n+ *      -XX:+UseShenandoahGC -XX:ShenandoahGCHeuristics=compact\n@@ -138,1 +133,1 @@\n- * @test id=iu\n+ * @test id=no-tlab\n@@ -143,6 +138,2 @@\n- *      -XX:+UseShenandoahGC -XX:ShenandoahGCMode=iu\n- *      -XX:+ShenandoahVerify\n- *      TestRetainObjects\n- *\n- * @run main\/othervm -Xmx1g -Xms1g -XX:+UnlockDiagnosticVMOptions -XX:+UnlockExperimentalVMOptions\n- *      -XX:+UseShenandoahGC -XX:ShenandoahGCMode=iu\n+ *      -XX:+UseShenandoahGC\n+ *      -XX:-UseTLAB -XX:+ShenandoahVerify\n","filename":"test\/hotspot\/jtreg\/gc\/shenandoah\/TestRetainObjects.java","additions":22,"deletions":31,"binary":false,"changes":53,"status":"modified"},{"patch":"@@ -0,0 +1,49 @@\n+\/*\n+ * Copyright Amazon.com Inc. or its affiliates. All Rights Reserved.\n+ * DO NOT ALTER OR REMOVE COPYRIGHT NOTICES OR THIS FILE HEADER.\n+ *\n+ * This code is free software; you can redistribute it and\/or modify it\n+ * under the terms of the GNU General Public License version 2 only, as\n+ * published by the Free Software Foundation.\n+ *\n+ * This code is distributed in the hope that it will be useful, but WITHOUT\n+ * ANY WARRANTY; without even the implied warranty of MERCHANTABILITY or\n+ * FITNESS FOR A PARTICULAR PURPOSE.  See the GNU General Public License\n+ * version 2 for more details (a copy is included in the LICENSE file that\n+ * accompanied this code).\n+ *\n+ * You should have received a copy of the GNU General Public License version\n+ * 2 along with this work; if not, write to the Free Software Foundation,\n+ * Inc., 51 Franklin St, Fifth Floor, Boston, MA 02110-1301 USA.\n+ *\n+ * Please contact Oracle, 500 Oracle Parkway, Redwood Shores, CA 94065 USA\n+ * or visit www.oracle.com if you need additional information or have any\n+ * questions.\n+ *\n+ *\/\n+\n+\/*\n+ * @test id=rotation\n+ * @requires vm.gc.Shenandoah\n+ *\n+ * @run main\/othervm -Xmx1g -Xms1g -XX:+UnlockDiagnosticVMOptions -XX:+UnlockExperimentalVMOptions\n+ *      -XX:+ShenandoahRegionSampling\n+ *      -Xlog:gc+region=trace:region-snapshots-%p.log::filesize=100,filecount=3\n+ *      -XX:+UseShenandoahGC\n+ *      TestShenandoahRegionLogging\n+ *\/\n+import java.io.File;\n+\n+public class TestShenandoahRegionLogging {\n+    public static void main(String[] args) throws Exception {\n+        System.gc();\n+\n+        File directory = new File(\".\");\n+        File[] files = directory.listFiles((dir, name) -> name.startsWith(\"region-snapshots\"));\n+\n+        \/\/ Expect one or more log files when region logging is enabled\n+        if (files.length == 0) {\n+            throw new Error(\"Expected at least one log file for region sampling data.\");\n+        }\n+    }\n+}\n","filename":"test\/hotspot\/jtreg\/gc\/shenandoah\/TestShenandoahRegionLogging.java","additions":49,"deletions":0,"binary":false,"changes":49,"status":"added"},{"patch":"@@ -3,0 +3,1 @@\n+ * Copyright Amazon.com Inc. or its affiliates. All Rights Reserved.\n@@ -94,1 +95,1 @@\n- * @test id=static\n+ * @test id=generational\n@@ -101,1 +102,2 @@\n- *      -XX:+UseShenandoahGC -XX:ShenandoahGCHeuristics=static\n+ *      -XX:+UseShenandoahGC -XX:ShenandoahGCHeuristics=adaptive -XX:ShenandoahGCMode=generational\n+ *      -XX:+ShenandoahOOMDuringEvacALot -XX:+ShenandoahVerify\n@@ -103,8 +105,0 @@\n- *\/\n-\n-\/*\n- * @test id=compact\n- * @summary Acceptance tests: collector can deal with retained objects\n- * @key randomness\n- * @requires vm.gc.Shenandoah\n- * @library \/test\/lib\n@@ -113,1 +107,6 @@\n- *      -XX:+UseShenandoahGC -XX:ShenandoahGCHeuristics=compact\n+ *      -XX:+UseShenandoahGC -XX:ShenandoahGCHeuristics=adaptive -XX:ShenandoahGCMode=generational\n+ *      -XX:+ShenandoahAllocFailureALot -XX:+ShenandoahVerify\n+ *      TestSieveObjects\n+ *\n+ * @run main\/othervm -Xmx1g -Xms1g -XX:+UnlockDiagnosticVMOptions -XX:+UnlockExperimentalVMOptions\n+ *      -XX:+UseShenandoahGC -XX:ShenandoahGCHeuristics=adaptive -XX:ShenandoahGCMode=generational\n@@ -118,1 +117,1 @@\n- * @test id=no-tlab\n+ * @test id=static\n@@ -124,3 +123,2 @@\n- * @run main\/othervm\/timeout=240 -Xmx1g -Xms1g -XX:+UnlockDiagnosticVMOptions -XX:+UnlockExperimentalVMOptions\n- *      -XX:+UseShenandoahGC\n- *      -XX:-UseTLAB -XX:+ShenandoahVerify\n+ * @run main\/othervm -Xmx1g -Xms1g -XX:+UnlockDiagnosticVMOptions -XX:+UnlockExperimentalVMOptions\n+ *      -XX:+UseShenandoahGC -XX:ShenandoahGCHeuristics=static\n@@ -131,1 +129,1 @@\n- * @test id=iu-aggressive\n+ * @test id=compact\n@@ -138,11 +136,1 @@\n- *      -XX:+UseShenandoahGC -XX:ShenandoahGCMode=iu -XX:ShenandoahGCHeuristics=aggressive\n- *      -XX:+ShenandoahOOMDuringEvacALot\n- *      TestSieveObjects\n- *\n- * @run main\/othervm -Xmx1g -Xms1g -XX:+UnlockDiagnosticVMOptions -XX:+UnlockExperimentalVMOptions\n- *      -XX:+UseShenandoahGC -XX:ShenandoahGCMode=iu -XX:ShenandoahGCHeuristics=aggressive\n- *      -XX:+ShenandoahAllocFailureALot\n- *      TestSieveObjects\n- *\n- * @run main\/othervm -Xmx1g -Xms1g -XX:+UnlockDiagnosticVMOptions -XX:+UnlockExperimentalVMOptions\n- *      -XX:+UseShenandoahGC -XX:ShenandoahGCMode=iu -XX:ShenandoahGCHeuristics=aggressive\n+ *      -XX:+UseShenandoahGC -XX:ShenandoahGCHeuristics=compact\n@@ -153,1 +141,1 @@\n- * @test id=iu\n+ * @test id=no-tlab\n@@ -159,7 +147,3 @@\n- * @run main\/othervm -Xmx1g -Xms1g -XX:+UnlockDiagnosticVMOptions -XX:+UnlockExperimentalVMOptions\n- *      -XX:+UseShenandoahGC -XX:ShenandoahGCMode=iu\n- *      -XX:+ShenandoahVerify\n- *      TestSieveObjects\n- *\n- * @run main\/othervm -Xmx1g -Xms1g -XX:+UnlockDiagnosticVMOptions -XX:+UnlockExperimentalVMOptions\n- *      -XX:+UseShenandoahGC -XX:ShenandoahGCMode=iu\n+ * @run main\/othervm\/timeout=300 -Xmx1g -Xms1g -XX:+UnlockDiagnosticVMOptions -XX:+UnlockExperimentalVMOptions\n+ *      -XX:+UseShenandoahGC\n+ *      -XX:-UseTLAB -XX:+ShenandoahVerify\n","filename":"test\/hotspot\/jtreg\/gc\/shenandoah\/TestSieveObjects.java","additions":19,"deletions":35,"binary":false,"changes":54,"status":"modified"},{"patch":"@@ -3,0 +3,1 @@\n+ * Copyright Amazon.com Inc. or its affiliates. All Rights Reserved.\n@@ -26,1 +27,1 @@\n- * @test\n+ * @test id=default\n@@ -37,0 +38,11 @@\n+\/*\n+ * @test id=generational\n+ * @requires vm.gc.Shenandoah\n+ *\n+ * @run main\/othervm -XX:+UnlockExperimentalVMOptions -XX:+UseShenandoahGC -XX:ShenandoahGCMode=generational         TestSmallHeap\n+ * @run main\/othervm -XX:+UnlockExperimentalVMOptions -XX:+UseShenandoahGC -XX:ShenandoahGCMode=generational -Xmx64m TestSmallHeap\n+ * @run main\/othervm -XX:+UnlockExperimentalVMOptions -XX:+UseShenandoahGC -XX:ShenandoahGCMode=generational -Xmx32m TestSmallHeap\n+ * @run main\/othervm -XX:+UnlockExperimentalVMOptions -XX:+UseShenandoahGC -XX:ShenandoahGCMode=generational -Xmx16m TestSmallHeap\n+ * @run main\/othervm -XX:+UnlockExperimentalVMOptions -XX:+UseShenandoahGC -XX:ShenandoahGCMode=generational -Xmx8m  TestSmallHeap\n+ * @run main\/othervm -XX:+UnlockExperimentalVMOptions -XX:+UseShenandoahGC -XX:ShenandoahGCMode=generational -Xmx4m  TestSmallHeap\n+ *\/\n","filename":"test\/hotspot\/jtreg\/gc\/shenandoah\/TestSmallHeap.java","additions":13,"deletions":1,"binary":false,"changes":14,"status":"modified"},{"patch":"@@ -69,1 +69,1 @@\n- * @test id=iu\n+ * @test id=generational\n@@ -78,5 +78,1 @@\n- *      -XX:+UseShenandoahGC -XX:ShenandoahGCMode=iu -XX:StringDeduplicationAgeThreshold=3\n- *      TestStringDedup\n- *\n- * @run main\/othervm -Xmx256m -Xlog:gc+stats -XX:+UnlockDiagnosticVMOptions -XX:+UnlockExperimentalVMOptions -XX:+UseStringDeduplication\n- *      -XX:+UseShenandoahGC -XX:ShenandoahGCMode=iu -XX:ShenandoahGCHeuristics=aggressive -XX:StringDeduplicationAgeThreshold=3\n+ *      -XX:+UseShenandoahGC -XX:ShenandoahGCMode=generational -XX:StringDeduplicationAgeThreshold=3\n","filename":"test\/hotspot\/jtreg\/gc\/shenandoah\/TestStringDedup.java","additions":2,"deletions":6,"binary":false,"changes":8,"status":"modified"},{"patch":"@@ -3,0 +3,1 @@\n+ * Copyright Amazon.com Inc. or its affiliates. All Rights Reserved.\n@@ -46,1 +47,1 @@\n- * @test id=default\n+ * @test id=generational\n@@ -55,1 +56,2 @@\n- *      -XX:+UseShenandoahGC\n+ *      -XX:+UseShenandoahGC -XX:ShenandoahGCMode=generational\n+ *      -XX:+ShenandoahDegeneratedGC\n@@ -58,15 +60,0 @@\n- *\n- * @run main\/othervm -Xmx1g -Xlog:gc+stats -Xlog:gc -XX:+UnlockDiagnosticVMOptions -XX:+UnlockExperimentalVMOptions -XX:+UseStringDeduplication\n- *      -XX:+UseShenandoahGC -XX:ShenandoahGCHeuristics=aggressive\n- *      -DtargetStrings=2000000\n- *      TestStringDedupStress\n- *\n- * @run main\/othervm -Xmx1g -Xlog:gc+stats -Xlog:gc -XX:+UnlockDiagnosticVMOptions -XX:+UnlockExperimentalVMOptions -XX:+UseStringDeduplication\n- *      -XX:+UseShenandoahGC -XX:ShenandoahGCHeuristics=aggressive\n- *      -XX:+ShenandoahOOMDuringEvacALot\n- *      -DtargetStrings=2000000\n- *      TestStringDedupStress\n- *\n- * @run main\/othervm -Xmx1g -Xlog:gc+stats -Xlog:gc -XX:+UnlockDiagnosticVMOptions -XX:+UnlockExperimentalVMOptions -XX:+UseStringDeduplication\n- *      -XX:+UseShenandoahGC -XX:ShenandoahGCHeuristics=compact\n- *      TestStringDedupStress\n@@ -75,2 +62,2 @@\n- \/*\n- * @test id=iu\n+\/*\n+ * @test id=default\n@@ -85,1 +72,2 @@\n- *      -XX:+UseShenandoahGC -XX:ShenandoahGCMode=iu\n+ *      -XX:+UseShenandoahGC\n+ *      -DtargetStrings=3000000\n@@ -89,1 +77,1 @@\n- *      -XX:+UseShenandoahGC -XX:ShenandoahGCMode=iu -XX:ShenandoahGCHeuristics=aggressive\n+ *      -XX:+UseShenandoahGC -XX:ShenandoahGCHeuristics=aggressive\n@@ -94,1 +82,1 @@\n- *      -XX:+UseShenandoahGC -XX:ShenandoahGCMode=iu\n+ *      -XX:+UseShenandoahGC -XX:ShenandoahGCHeuristics=aggressive\n@@ -100,3 +88,1 @@\n- *      -XX:+UseShenandoahGC -XX:ShenandoahGCMode=iu -XX:ShenandoahGCHeuristics=aggressive\n- *      -XX:+ShenandoahOOMDuringEvacALot\n- *      -DtargetStrings=2000000\n+ *      -XX:+UseShenandoahGC -XX:ShenandoahGCHeuristics=compact\n@@ -116,1 +102,1 @@\n-    private static final long MAX_REWRITE_TIME = 30*1000; \/\/ ms\n+    private static final long MAX_REWRITE_TIME_NS = 30L * 1_000_000_000L; \/\/ 30s in ns\n@@ -214,1 +200,1 @@\n-        long timeBeforeRewrite = System.currentTimeMillis();\n+        long timeBeforeRewriteNanos = System.nanoTime();\n@@ -232,1 +218,1 @@\n-                if (System.currentTimeMillis() - timeBeforeRewrite >= MAX_REWRITE_TIME) {\n+                if (System.nanoTime() - timeBeforeRewriteNanos >= MAX_REWRITE_TIME_NS) {\n","filename":"test\/hotspot\/jtreg\/gc\/shenandoah\/TestStringDedupStress.java","additions":14,"deletions":28,"binary":false,"changes":42,"status":"modified"},{"patch":"@@ -3,0 +3,1 @@\n+ * Copyright Amazon.com Inc. or its affiliates. All Rights Reserved.\n@@ -79,1 +80,1 @@\n- * @test id=iu\n+ * @test id=generational\n@@ -84,1 +85,1 @@\n- *      -XX:+UseShenandoahGC -XX:ShenandoahGCMode=iu\n+ *      -XX:+UseShenandoahGC -XX:ShenandoahGCHeuristics=adaptive -XX:ShenandoahGCMode=generational\n@@ -89,6 +90,1 @@\n- *      -XX:+UseShenandoahGC -XX:ShenandoahGCMode=iu -XX:ShenandoahGCHeuristics=aggressive\n- *      -XX:+ShenandoahVerify\n- *      TestStringInternCleanup\n- *\n- * @run main\/othervm -Xmx64m -XX:+UnlockDiagnosticVMOptions -XX:+UnlockExperimentalVMOptions -XX:+ClassUnloadingWithConcurrentMark\n- *      -XX:+UseShenandoahGC -XX:ShenandoahGCMode=iu\n+ *      -XX:+UseShenandoahGC -XX:ShenandoahGCHeuristics=adaptive -XX:ShenandoahGCMode=generational\n@@ -98,0 +94,1 @@\n+\n","filename":"test\/hotspot\/jtreg\/gc\/shenandoah\/TestStringInternCleanup.java","additions":5,"deletions":8,"binary":false,"changes":13,"status":"modified"},{"patch":"@@ -47,0 +47,1 @@\n+ * @requires !vm.debug\n@@ -61,27 +62,0 @@\n-\/*\n- * @test id=iu\n- * @summary Tests that we pass at least one jcstress-like test with all verification turned on\n- * @requires vm.gc.Shenandoah\n- * @modules java.base\/jdk.internal.misc\n- *          java.management\n- *\n- * @run main\/othervm -Xmx1g -Xms1g -XX:+UnlockExperimentalVMOptions -XX:+UnlockDiagnosticVMOptions\n- *      -XX:+UseShenandoahGC -XX:ShenandoahGCMode=iu\n- *      -XX:+ShenandoahVerify\n- *      TestVerifyJCStress\n- *\/\n-\n-\/*\n- * @test id=iu-c1\n- * @summary Tests that we pass at least one jcstress-like test with all verification turned on\n- * @requires vm.gc.Shenandoah\n- * @modules java.base\/jdk.internal.misc\n- *          java.management\n- *\n- * @run main\/othervm -Xmx1g -Xms1g -XX:+UnlockExperimentalVMOptions -XX:+UnlockDiagnosticVMOptions\n- *      -XX:+UseShenandoahGC -XX:ShenandoahGCMode=iu\n- *      -XX:+ShenandoahVerify -XX:TieredStopAtLevel=1\n- *      TestVerifyJCStress\n- *\/\n-\n-\n","filename":"test\/hotspot\/jtreg\/gc\/shenandoah\/TestVerifyJCStress.java","additions":1,"deletions":27,"binary":false,"changes":28,"status":"modified"},{"patch":"@@ -3,0 +3,1 @@\n+ * Copyright Amazon.com Inc. or its affiliates. All Rights Reserved.\n@@ -26,1 +27,1 @@\n- * @test\n+ * @test id=default\n@@ -36,0 +37,10 @@\n+\/*\n+ * @test id=generational\n+ * @requires vm.gc.Shenandoah\n+ *\n+ * @run main\/othervm -XX:+UnlockExperimentalVMOptions -XX:+UseShenandoahGC -XX:ShenandoahGCMode=generational -XX:+UnlockDiagnosticVMOptions -Xmx128m -XX:+ShenandoahVerify -XX:ShenandoahVerifyLevel=0 TestVerifyLevels\n+ * @run main\/othervm -XX:+UnlockExperimentalVMOptions -XX:+UseShenandoahGC -XX:ShenandoahGCMode=generational -XX:+UnlockDiagnosticVMOptions -Xmx128m -XX:+ShenandoahVerify -XX:ShenandoahVerifyLevel=1 TestVerifyLevels\n+ * @run main\/othervm -XX:+UnlockExperimentalVMOptions -XX:+UseShenandoahGC -XX:ShenandoahGCMode=generational -XX:+UnlockDiagnosticVMOptions -Xmx128m -XX:+ShenandoahVerify -XX:ShenandoahVerifyLevel=2 TestVerifyLevels\n+ * @run main\/othervm -XX:+UnlockExperimentalVMOptions -XX:+UseShenandoahGC -XX:ShenandoahGCMode=generational -XX:+UnlockDiagnosticVMOptions -Xmx128m -XX:+ShenandoahVerify -XX:ShenandoahVerifyLevel=3 TestVerifyLevels\n+ * @run main\/othervm -XX:+UnlockExperimentalVMOptions -XX:+UseShenandoahGC -XX:ShenandoahGCMode=generational -XX:+UnlockDiagnosticVMOptions -Xmx128m -XX:+ShenandoahVerify -XX:ShenandoahVerifyLevel=4 TestVerifyLevels\n+ *\/\n","filename":"test\/hotspot\/jtreg\/gc\/shenandoah\/TestVerifyLevels.java","additions":12,"deletions":1,"binary":false,"changes":13,"status":"modified"},{"patch":"@@ -3,0 +3,1 @@\n+ * Copyright Amazon.com Inc. or its affiliates. All Rights Reserved.\n@@ -26,1 +27,1 @@\n- * @test\n+ * @test id=default\n@@ -37,0 +38,11 @@\n+\/*\n+ * @test id=generational\n+ * @summary Test Shenandoah with different log levels\n+ * @requires vm.gc.Shenandoah\n+ *\n+ * @run main\/othervm -XX:+UnlockExperimentalVMOptions -XX:+UseShenandoahGC -XX:ShenandoahGCMode=generational -Xms256M -Xmx1G -Xlog:gc*=error   TestWithLogLevel\n+ * @run main\/othervm -XX:+UnlockExperimentalVMOptions -XX:+UseShenandoahGC -XX:ShenandoahGCMode=generational -Xms256M -Xmx1G -Xlog:gc*=warning TestWithLogLevel\n+ * @run main\/othervm -XX:+UnlockExperimentalVMOptions -XX:+UseShenandoahGC -XX:ShenandoahGCMode=generational -Xms256M -Xmx1G -Xlog:gc*=info    TestWithLogLevel\n+ * @run main\/othervm -XX:+UnlockExperimentalVMOptions -XX:+UseShenandoahGC -XX:ShenandoahGCMode=generational -Xms256M -Xmx1G -Xlog:gc*=debug   TestWithLogLevel\n+ * @run main\/othervm -XX:+UnlockExperimentalVMOptions -XX:+UseShenandoahGC -XX:ShenandoahGCMode=generational -Xms256M -Xmx1G -Xlog:gc*=trace   TestWithLogLevel\n+ *\/\n","filename":"test\/hotspot\/jtreg\/gc\/shenandoah\/TestWithLogLevel.java","additions":13,"deletions":1,"binary":false,"changes":14,"status":"modified"},{"patch":"@@ -29,2 +29,2 @@\n- * @run main\/othervm -Xmx128m -XX:+UnlockExperimentalVMOptions -XX:+UseShenandoahGC                         TestWrongArrayMember\n- * @run main\/othervm -Xmx128m -XX:+UnlockExperimentalVMOptions -XX:+UseShenandoahGC -XX:ShenandoahGCMode=iu TestWrongArrayMember\n+ * @run main\/othervm -Xmx128m -XX:+UnlockExperimentalVMOptions -XX:+UseShenandoahGC                                   TestWrongArrayMember\n+ * @run main\/othervm -Xmx128m -XX:+UnlockExperimentalVMOptions -XX:+UseShenandoahGC -XX:ShenandoahGCMode=generational TestWrongArrayMember\n@@ -57,1 +57,0 @@\n-\n","filename":"test\/hotspot\/jtreg\/gc\/shenandoah\/TestWrongArrayMember.java","additions":2,"deletions":3,"binary":false,"changes":5,"status":"modified"},{"patch":"@@ -33,2 +33,0 @@\n- * @run main\/othervm -XX:+UnlockExperimentalVMOptions -XX:+UseShenandoahGC -XX:ShenandoahGCMode=iu -Xcomp -XX:CompileOnly=BarrierInInfiniteLoop::test1\n- *                   -XX:CompileOnly=BarrierInInfiniteLoop::test2 -XX:CompileOnly=BarrierInInfiniteLoop::test3 -XX:CompileCommand=quiet BarrierInInfiniteLoop\n","filename":"test\/hotspot\/jtreg\/gc\/shenandoah\/compiler\/BarrierInInfiniteLoop.java","additions":0,"deletions":2,"binary":false,"changes":2,"status":"modified"},{"patch":"@@ -3,0 +3,1 @@\n+ * Copyright Amazon.com Inc. or its affiliates. All Rights Reserved.\n@@ -208,0 +209,114 @@\n+\/*\n+ * @test id=generational\n+ * @summary Test clone barriers work correctly\n+ * @requires vm.gc.Shenandoah\n+ *\n+ * @run main\/othervm -XX:+UnlockDiagnosticVMOptions -XX:+UnlockExperimentalVMOptions -Xms1g -Xmx1g\n+ *                   -XX:+UseShenandoahGC -XX:ShenandoahGCMode=generational\n+ *                   TestClone\n+ * @run main\/othervm -XX:+UnlockDiagnosticVMOptions -XX:+UnlockExperimentalVMOptions -Xms1g -Xmx1g\n+ *                   -XX:+UseShenandoahGC -XX:ShenandoahGCMode=generational\n+ *                   -Xint\n+ *                   TestClone\n+ * @run main\/othervm -XX:+UnlockDiagnosticVMOptions -XX:+UnlockExperimentalVMOptions -Xms1g -Xmx1g\n+ *                   -XX:+UseShenandoahGC -XX:ShenandoahGCMode=generational\n+ *                   -XX:-TieredCompilation\n+ *                   TestClone\n+ * @run main\/othervm -XX:+UnlockDiagnosticVMOptions -XX:+UnlockExperimentalVMOptions -Xms1g -Xmx1g\n+ *                   -XX:+UseShenandoahGC -XX:ShenandoahGCMode=generational\n+ *                   -XX:TieredStopAtLevel=1\n+ *                   TestClone\n+ * @run main\/othervm -XX:+UnlockDiagnosticVMOptions -XX:+UnlockExperimentalVMOptions -Xms1g -Xmx1g\n+ *                   -XX:+UseShenandoahGC -XX:ShenandoahGCMode=generational\n+ *                   -XX:TieredStopAtLevel=4\n+ *                   TestClone\n+ *\/\n+\n+\/*\n+ * @test id=generational-small-card-size\n+ * @summary Test clone barriers work correctly\n+ * @requires vm.gc.Shenandoah\n+ *\n+ * @run main\/othervm -XX:+UnlockDiagnosticVMOptions -XX:+UnlockExperimentalVMOptions -Xms1g -Xmx1g\n+ *                   -XX:+UseShenandoahGC -XX:ShenandoahGCMode=generational -XX:GCCardSizeInBytes=128\n+ *                   TestClone\n+ * @run main\/othervm -XX:+UnlockDiagnosticVMOptions -XX:+UnlockExperimentalVMOptions -Xms1g -Xmx1g\n+ *                   -XX:+UseShenandoahGC -XX:ShenandoahGCMode=generational -XX:GCCardSizeInBytes=128\n+ *                   -Xint\n+ *                   TestClone\n+ * @run main\/othervm -XX:+UnlockDiagnosticVMOptions -XX:+UnlockExperimentalVMOptions -Xms1g -Xmx1g\n+ *                   -XX:+UseShenandoahGC -XX:ShenandoahGCMode=generational -XX:GCCardSizeInBytes=128\n+ *                   -XX:-TieredCompilation\n+ *                   TestClone\n+ * @run main\/othervm -XX:+UnlockDiagnosticVMOptions -XX:+UnlockExperimentalVMOptions -Xms1g -Xmx1g\n+ *                   -XX:+UseShenandoahGC -XX:ShenandoahGCMode=generational -XX:GCCardSizeInBytes=128\n+ *                   -XX:TieredStopAtLevel=1\n+ *                   TestClone\n+ * @run main\/othervm -XX:+UnlockDiagnosticVMOptions -XX:+UnlockExperimentalVMOptions -Xms1g -Xmx1g\n+ *                   -XX:+UseShenandoahGC -XX:ShenandoahGCMode=generational -XX:GCCardSizeInBytes=128\n+ *                   -XX:TieredStopAtLevel=4\n+ *                   TestClone\n+ *\/\n+\n+\/*\n+ * @test id=generational-verify\n+ * @summary Test clone barriers work correctly\n+ * @requires vm.gc.Shenandoah\n+ *\n+ * @run main\/othervm -XX:+UnlockDiagnosticVMOptions -XX:+UnlockExperimentalVMOptions -Xms1g -Xmx1g\n+ *                   -XX:+UseShenandoahGC -XX:ShenandoahGCMode=generational\n+ *                   -XX:+ShenandoahVerify\n+ *                   TestClone\n+ * @run main\/othervm -XX:+UnlockDiagnosticVMOptions -XX:+UnlockExperimentalVMOptions -Xms1g -Xmx1g\n+ *                   -XX:+UseShenandoahGC -XX:ShenandoahGCMode=generational\n+ *                   -XX:+ShenandoahVerify\n+ *                   -Xint\n+ *                   TestClone\n+ * @run main\/othervm -XX:+UnlockDiagnosticVMOptions -XX:+UnlockExperimentalVMOptions -Xms1g -Xmx1g\n+ *                   -XX:+UseShenandoahGC -XX:ShenandoahGCMode=generational\n+ *                   -XX:+ShenandoahVerify\n+ *                   -XX:-TieredCompilation\n+ *                   TestClone\n+ * @run main\/othervm -XX:+UnlockDiagnosticVMOptions -XX:+UnlockExperimentalVMOptions -Xms1g -Xmx1g\n+ *                   -XX:+UseShenandoahGC -XX:ShenandoahGCMode=generational\n+ *                   -XX:+ShenandoahVerify\n+ *                   -XX:TieredStopAtLevel=1\n+ *                   TestClone\n+ * @run main\/othervm -XX:+UnlockDiagnosticVMOptions -XX:+UnlockExperimentalVMOptions -Xms1g -Xmx1g\n+ *                   -XX:+UseShenandoahGC -XX:ShenandoahGCMode=generational\n+ *                   -XX:+ShenandoahVerify\n+ *                   -XX:TieredStopAtLevel=4\n+ *                   TestClone\n+ *\/\n+\n+ \/*\n+  * @test id=generational-no-coops\n+  * @summary Test clone barriers work correctly\n+  * @requires vm.gc.Shenandoah\n+  * @requires vm.bits == \"64\"\n+  *\n+  * @run main\/othervm -XX:+UnlockDiagnosticVMOptions -XX:+UnlockExperimentalVMOptions -Xms1g -Xmx1g\n+  *                   -XX:-UseCompressedOops\n+  *                   -XX:+UseShenandoahGC -XX:ShenandoahGCMode=generational\n+  *                   TestClone\n+  * @run main\/othervm -XX:+UnlockDiagnosticVMOptions -XX:+UnlockExperimentalVMOptions -Xms1g -Xmx1g\n+  *                   -XX:-UseCompressedOops\n+  *                   -XX:+UseShenandoahGC -XX:ShenandoahGCMode=generational\n+  *                   -Xint\n+  *                   TestClone\n+  * @run main\/othervm -XX:+UnlockDiagnosticVMOptions -XX:+UnlockExperimentalVMOptions -Xms1g -Xmx1g\n+  *                   -XX:-UseCompressedOops\n+  *                   -XX:+UseShenandoahGC -XX:ShenandoahGCMode=generational\n+  *                   -XX:-TieredCompilation\n+  *                   TestClone\n+  * @run main\/othervm -XX:+UnlockDiagnosticVMOptions -XX:+UnlockExperimentalVMOptions -Xms1g -Xmx1g\n+  *                   -XX:-UseCompressedOops\n+  *                   -XX:+UseShenandoahGC -XX:ShenandoahGCMode=generational\n+  *                   -XX:TieredStopAtLevel=1\n+  *                   TestClone\n+  * @run main\/othervm -XX:+UnlockDiagnosticVMOptions -XX:+UnlockExperimentalVMOptions -Xms1g -Xmx1g\n+  *                   -XX:-UseCompressedOops\n+  *                   -XX:+UseShenandoahGC -XX:ShenandoahGCMode=generational\n+  *                   -XX:TieredStopAtLevel=4\n+  *                   TestClone\n+  *\/\n@@ -209,0 +324,36 @@\n+ \/*\n+  * @test id=generational-no-coops-verify\n+  * @summary Test clone barriers work correctly\n+  * @requires vm.gc.Shenandoah\n+  * @requires vm.bits == \"64\"\n+  *\n+  * @run main\/othervm -XX:+UnlockDiagnosticVMOptions -XX:+UnlockExperimentalVMOptions -Xms1g -Xmx1g\n+  *                   -XX:-UseCompressedOops\n+  *                   -XX:+UseShenandoahGC -XX:ShenandoahGCMode=generational\n+  *                   -XX:+ShenandoahVerify\n+  *                   TestClone\n+  * @run main\/othervm -XX:+UnlockDiagnosticVMOptions -XX:+UnlockExperimentalVMOptions -Xms1g -Xmx1g\n+  *                   -XX:-UseCompressedOops\n+  *                   -XX:+UseShenandoahGC -XX:ShenandoahGCMode=generational\n+  *                   -XX:+ShenandoahVerify\n+  *                   -Xint\n+  *                   TestClone\n+  * @run main\/othervm -XX:+UnlockDiagnosticVMOptions -XX:+UnlockExperimentalVMOptions -Xms1g -Xmx1g\n+  *                   -XX:-UseCompressedOops\n+  *                   -XX:+UseShenandoahGC -XX:ShenandoahGCMode=generational\n+  *                   -XX:+ShenandoahVerify\n+  *                   -XX:-TieredCompilation\n+  *                   TestClone\n+  * @run main\/othervm -XX:+UnlockDiagnosticVMOptions -XX:+UnlockExperimentalVMOptions -Xms1g -Xmx1g\n+  *                   -XX:-UseCompressedOops\n+  *                   -XX:+UseShenandoahGC -XX:ShenandoahGCMode=generational\n+  *                   -XX:+ShenandoahVerify\n+  *                   -XX:TieredStopAtLevel=1\n+  *                   TestClone\n+  * @run main\/othervm -XX:+UnlockDiagnosticVMOptions -XX:+UnlockExperimentalVMOptions -Xms1g -Xmx1g\n+  *                   -XX:-UseCompressedOops\n+  *                   -XX:+UseShenandoahGC -XX:ShenandoahGCMode=generational\n+  *                   -XX:+ShenandoahVerify\n+  *                   -XX:TieredStopAtLevel=4\n+  *                   TestClone\n+  *\/\n","filename":"test\/hotspot\/jtreg\/gc\/shenandoah\/compiler\/TestClone.java","additions":151,"deletions":0,"binary":false,"changes":151,"status":"modified"},{"patch":"@@ -3,0 +3,1 @@\n+ * Copyright Amazon.com Inc. or its affiliates. All Rights Reserved.\n@@ -56,0 +57,26 @@\n+\/*\n+ * @test id=generational\n+ * @summary Shenandoah reference CAS test\n+ * @requires vm.gc.Shenandoah\n+ * @modules java.base\/jdk.internal.misc:+open\n+ *\n+ * @run main\/othervm -Diters=20000 -XX:+UnlockDiagnosticVMOptions -XX:+UnlockExperimentalVMOptions -XX:+UseShenandoahGC -XX:ShenandoahGCMode=generational                                                 TestReferenceCAS\n+ * @run main\/othervm -Diters=100   -XX:+UnlockDiagnosticVMOptions -XX:+UnlockExperimentalVMOptions -XX:+UseShenandoahGC -XX:ShenandoahGCMode=generational -Xint                                           TestReferenceCAS\n+ * @run main\/othervm -Diters=20000 -XX:+UnlockDiagnosticVMOptions -XX:+UnlockExperimentalVMOptions -XX:+UseShenandoahGC -XX:ShenandoahGCMode=generational -XX:-TieredCompilation                          TestReferenceCAS\n+ * @run main\/othervm -Diters=20000 -XX:+UnlockDiagnosticVMOptions -XX:+UnlockExperimentalVMOptions -XX:+UseShenandoahGC -XX:ShenandoahGCMode=generational -XX:TieredStopAtLevel=1                         TestReferenceCAS\n+ * @run main\/othervm -Diters=20000 -XX:+UnlockDiagnosticVMOptions -XX:+UnlockExperimentalVMOptions -XX:+UseShenandoahGC -XX:ShenandoahGCMode=generational -XX:TieredStopAtLevel=4                         TestReferenceCAS\n+ *\/\n+\n+\/*\n+ * @test id=generational-no-coops\n+ * @summary Shenandoah reference CAS test\n+ * @requires vm.gc.Shenandoah\n+ * @requires vm.bits == \"64\"\n+ * @modules java.base\/jdk.internal.misc:+open\n+ *\n+ * @run main\/othervm -Diters=20000 -XX:+UnlockDiagnosticVMOptions -XX:+UnlockExperimentalVMOptions -XX:+UseShenandoahGC -XX:ShenandoahGCMode=generational -XX:-UseCompressedOops                          TestReferenceCAS\n+ * @run main\/othervm -Diters=100   -XX:+UnlockDiagnosticVMOptions -XX:+UnlockExperimentalVMOptions -XX:+UseShenandoahGC -XX:ShenandoahGCMode=generational -XX:-UseCompressedOops -Xint                    TestReferenceCAS\n+ * @run main\/othervm -Diters=20000 -XX:+UnlockDiagnosticVMOptions -XX:+UnlockExperimentalVMOptions -XX:+UseShenandoahGC -XX:ShenandoahGCMode=generational -XX:-UseCompressedOops -XX:-TieredCompilation   TestReferenceCAS\n+ * @run main\/othervm -Diters=20000 -XX:+UnlockDiagnosticVMOptions -XX:+UnlockExperimentalVMOptions -XX:+UseShenandoahGC -XX:ShenandoahGCMode=generational -XX:-UseCompressedOops -XX:TieredStopAtLevel=1  TestReferenceCAS\n+ * @run main\/othervm -Diters=20000 -XX:+UnlockDiagnosticVMOptions -XX:+UnlockExperimentalVMOptions -XX:+UseShenandoahGC -XX:ShenandoahGCMode=generational -XX:-UseCompressedOops -XX:TieredStopAtLevel=4  TestReferenceCAS\n+ *\/\n","filename":"test\/hotspot\/jtreg\/gc\/shenandoah\/compiler\/TestReferenceCAS.java","additions":27,"deletions":0,"binary":false,"changes":27,"status":"modified"},{"patch":"@@ -1,74 +0,0 @@\n-\/*\n- * Copyright (c) 2022, Red Hat, Inc. All rights reserved.\n- * DO NOT ALTER OR REMOVE COPYRIGHT NOTICES OR THIS FILE HEADER.\n- *\n- * This code is free software; you can redistribute it and\/or modify it\n- * under the terms of the GNU General Public License version 2 only, as\n- * published by the Free Software Foundation.\n- *\n- * This code is distributed in the hope that it will be useful, but WITHOUT\n- * ANY WARRANTY; without even the implied warranty of MERCHANTABILITY or\n- * FITNESS FOR A PARTICULAR PURPOSE.  See the GNU General Public License\n- * version 2 for more details (a copy is included in the LICENSE file that\n- * accompanied this code).\n- *\n- * You should have received a copy of the GNU General Public License version\n- * 2 along with this work; if not, write to the Free Software Foundation,\n- * Inc., 51 Franklin St, Fifth Floor, Boston, MA 02110-1301 USA.\n- *\n- * Please contact Oracle, 500 Oracle Parkway, Redwood Shores, CA 94065 USA\n- * or visit www.oracle.com if you need additional information or have any\n- * questions.\n- *\/\n-\n-\/*\n- * @test\n- * bug 8280885\n- * @summary Shenandoah: Some tests failed with \"EA: missing allocation reference path\"\n- * @requires vm.gc.Shenandoah\n- *\n- * @run main\/othervm -XX:-BackgroundCompilation -XX:+UseShenandoahGC -XX:+UnlockExperimentalVMOptions -XX:ShenandoahGCMode=iu\n- *                   -XX:CompileCommand=dontinline,TestUnexpectedIUBarrierEA::notInlined TestUnexpectedIUBarrierEA\n- *\/\n-\n-public class TestUnexpectedIUBarrierEA {\n-\n-    private static Object field;\n-\n-    public static void main(String[] args) {\n-        for (int i = 0; i < 20_000; i++) {\n-            test(false);\n-        }\n-    }\n-\n-    private static void test(boolean flag) {\n-        A a = new A();\n-        B b = new B();\n-        b.field = a;\n-        notInlined();\n-        Object o = b.field;\n-        if (!(o instanceof A)) {\n-\n-        }\n-        C c = new C();\n-        c.field = o;\n-        if (flag) {\n-            field = c.field;\n-        }\n-    }\n-\n-    private static void notInlined() {\n-\n-    }\n-\n-    private static class A {\n-    }\n-\n-    private static class B {\n-        public Object field;\n-    }\n-\n-    private static class C {\n-        public Object field;\n-    }\n-}\n","filename":"test\/hotspot\/jtreg\/gc\/shenandoah\/compiler\/TestUnexpectedIUBarrierEA.java","additions":0,"deletions":74,"binary":false,"changes":74,"status":"deleted"},{"patch":"@@ -0,0 +1,185 @@\n+\/*\n+ * Copyright Amazon.com Inc. or its affiliates. All Rights Reserved.\n+ * DO NOT ALTER OR REMOVE COPYRIGHT NOTICES OR THIS FILE HEADER.\n+ *\n+ * This code is free software; you can redistribute it and\/or modify it\n+ * under the terms of the GNU General Public License version 2 only, as\n+ * published by the Free Software Foundation.\n+ *\n+ * This code is distributed in the hope that it will be useful, but WITHOUT\n+ * ANY WARRANTY; without even the implied warranty of MERCHANTABILITY or\n+ * FITNESS FOR A PARTICULAR PURPOSE.  See the GNU General Public License\n+ * version 2 for more details (a copy is included in the LICENSE file that\n+ * accompanied this code).\n+ *\n+ * You should have received a copy of the GNU General Public License version\n+ * 2 along with this work; if not, write to the Free Software Foundation,\n+ * Inc., 51 Franklin St, Fifth Floor, Boston, MA 02110-1301 USA.\n+ *\n+ * Please contact Oracle, 500 Oracle Parkway, Redwood Shores, CA 94065 USA\n+ * or visit www.oracle.com if you need additional information or have any\n+ * questions.\n+ *\n+ *\/\n+\n+package gc.shenandoah.generational;\n+\n+import jdk.test.whitebox.WhiteBox;\n+import java.util.Random;\n+import java.util.HashMap;\n+\n+\/*\n+ *  To avoid the risk of false regressions identified by this test, the heap\n+ *  size is set artificially high.  Though this test is known to run reliably\n+ *  in 66 MB heap, the heap size for this test run is currently set to 256 MB.\n+ *\/\n+\n+\/*\n+ * @test id=generational\n+ * @requires vm.gc.Shenandoah\n+ * @summary Confirm that card marking and remembered set scanning do not crash.\n+ * @library \/testlibrary \/test\/lib \/\n+ * @build jdk.test.whitebox.WhiteBox\n+ * @run driver jdk.test.lib.helpers.ClassFileInstaller jdk.test.whitebox.WhiteBox\n+ * @run main\/othervm -Xbootclasspath\/a:.\n+ *      -Xms256m -Xmx256m\n+ *      -XX:+IgnoreUnrecognizedVMOptions\n+ *      -XX:+UnlockDiagnosticVMOptions -XX:+WhiteBoxAPI\n+ *      -XX:+UseShenandoahGC -XX:ShenandoahGCMode=generational\n+ *      -XX:NewRatio=1 -XX:+UnlockExperimentalVMOptions\n+ *      -XX:ShenandoahGuaranteedGCInterval=3000\n+ *      -XX:-UseDynamicNumberOfGCThreads -XX:-ShenandoahPacing\n+ *      gc.shenandoah.generational.TestConcurrentEvac\n+ *\/\n+\n+public class TestConcurrentEvac {\n+    private static WhiteBox wb = WhiteBox.getWhiteBox();\n+\n+    private static final int RANDOM_SEED = 46;\n+\n+    \/\/ Smaller table will cause creation of more old-gen garbage\n+    \/\/ as previous entries in table are overwritten with new values.\n+    private static final int TABLE_SIZE = 53;\n+    private static final int MAX_STRING_LENGTH = 47;\n+    private static final int SENTENCE_LENGTH = 5;\n+\n+    private static Random random = new Random(RANDOM_SEED);\n+\n+    public static class Node {\n+\n+        private String name;\n+\n+        \/\/ Each Node instance holds an array containing all substrings of its name\n+\n+        \/\/ This array has entries from 0 .. (name.length() - 1).\n+        \/\/ numSubstrings[i] represents the number of substrings that\n+        \/\/ correspond to a name of length i+1.\n+        private static int [] numSubstrings;\n+\n+        static {\n+            \/\/ Initialize numSubstrings.\n+            \/\/ For a name of length N, there are\n+            \/\/  N substrings of length 1\n+            \/\/  N-1 substrings of length 2\n+            \/\/  N-2 substrings of length 3\n+            \/\/  ...\n+            \/\/  1 substring of length N\n+            \/\/ Note that:\n+            \/\/   numSubstrings[0] = 1\n+            \/\/   numSubstrings[1] = 3\n+            \/\/   numSubstrings[i] = (i + 1) + numSubstrings[i - 1]\n+            numSubstrings = new int[MAX_STRING_LENGTH];\n+            numSubstrings[0] = 1;\n+            for (int i = 1; i < MAX_STRING_LENGTH; i++) {\n+                numSubstrings[i] = (i + 1) + numSubstrings[i - 1];\n+            }\n+        }\n+\n+        private String [] substrings;\n+        private Node [] neighbors;\n+\n+        public Node(String name) {\n+            this.name = name;\n+            this.substrings = new String[numSubstrings[name.length() - 1]];\n+\n+            int index = 0;\n+            for (int substringLength = 1; substringLength <= name.length(); substringLength++) {\n+                for (int offset = 0; offset + substringLength <= name.length(); offset++) {\n+                    this.substrings[index++] = name.substring(offset, offset + substringLength);\n+                }\n+            }\n+        }\n+\n+        public String value() {\n+            return name;\n+        }\n+\n+        public String arbitrarySubstring() {\n+            int index = TestConcurrentEvac.randomUnsignedInt(substrings.length);\n+            return substrings[index];\n+        }\n+    }\n+\n+\n+    \/\/ Return random int between 1 and MAX_STRING_LENGTH inclusive\n+    static int randomStringLength() {\n+        return randomUnsignedInt(MAX_STRING_LENGTH - 1) + 1;\n+    }\n+\n+    static String randomCharacter() {\n+        int index = randomUnsignedInt(52);\n+        return \"abcdefghijklmnopqrstuvwxyzABCDEFGHIJKLMNOPQRSTUVWXYZ\".substring(index, index + 1);\n+    }\n+\n+    static String randomString() {\n+        int length = randomStringLength();\n+        String result = new String(); \/\/ make the compiler work for this garbage...\n+        for (int i = 0; i < length; i++) {\n+            result += randomCharacter();\n+        }\n+        return result;\n+    }\n+\n+    static int randomUnsignedInt(int max) {\n+        return random.nextInt(max);\n+    }\n+\n+    static int randomIndex() {\n+        return randomUnsignedInt(TABLE_SIZE);\n+    }\n+\n+    public static void main(String args[]) throws Exception {\n+        HashMap<Integer, Node> table = new HashMap<Integer, Node>(TABLE_SIZE);\n+\n+        if (!wb.getBooleanVMFlag(\"UseShenandoahGC\") || !wb.getStringVMFlag(\"ShenandoahGCMode\").equals(\"generational\")) {\n+            throw new IllegalStateException(\"Command-line options not honored!\");\n+        }\n+\n+        for (int count = java.lang.Integer.MAX_VALUE\/1024; count >= 0; count--) {\n+            int index = randomIndex();\n+            String name = randomString();\n+            table.put(index, new Node(name));\n+        }\n+\n+        String conclusion = \"\";\n+\n+        for (int i = 0; i < SENTENCE_LENGTH; i++) {\n+            Node node = table.get(randomIndex());\n+            if (node == null) {\n+                i--;\n+            } else {\n+                String s = node.arbitrarySubstring();\n+                conclusion += s;\n+                conclusion += \" \";\n+            }\n+        }\n+\n+        conclusion = conclusion.substring(0, conclusion.length() - 1);\n+\n+        System.out.println(\"Conclusion is [\" + conclusion + \"]\");\n+\n+        if (!conclusion.equals(\"HN TInkzoLSDFVJYM mQAirHXbbgCJmUWozx DeispxWF MYFKBh\")) {\n+            throw new IllegalStateException(\"Random sequence of words did not end well!\");\n+        }\n+    }\n+}\n","filename":"test\/hotspot\/jtreg\/gc\/shenandoah\/generational\/TestConcurrentEvac.java","additions":185,"deletions":0,"binary":false,"changes":185,"status":"added"},{"patch":"@@ -0,0 +1,110 @@\n+\/*\n+ * Copyright Amazon.com Inc. or its affiliates. All Rights Reserved.\n+ * DO NOT ALTER OR REMOVE COPYRIGHT NOTICES OR THIS FILE HEADER.\n+ *\n+ * This code is free software; you can redistribute it and\/or modify it\n+ * under the terms of the GNU General Public License version 2 only, as\n+ * published by the Free Software Foundation.\n+ *\n+ * This code is distributed in the hope that it will be useful, but WITHOUT\n+ * ANY WARRANTY; without even the implied warranty of MERCHANTABILITY or\n+ * FITNESS FOR A PARTICULAR PURPOSE.  See the GNU General Public License\n+ * version 2 for more details (a copy is included in the LICENSE file that\n+ * accompanied this code).\n+ *\n+ * You should have received a copy of the GNU General Public License version\n+ * 2 along with this work; if not, write to the Free Software Foundation,\n+ * Inc., 51 Franklin St, Fifth Floor, Boston, MA 02110-1301 USA.\n+ *\n+ * Please contact Oracle, 500 Oracle Parkway, Redwood Shores, CA 94065 USA\n+ * or visit www.oracle.com if you need additional information or have any\n+ * questions.\n+ *\n+ *\/\n+\n+\/*\n+ * @test id=generational\n+ * @summary Test that growth of old-gen triggers old-gen marking\n+ * @requires vm.gc.Shenandoah\n+ * @library \/test\/lib\n+ * @run driver TestOldGrowthTriggers\n+ *\/\n+\n+import java.util.*;\n+import java.math.BigInteger;\n+\n+import jdk.test.lib.Asserts;\n+import jdk.test.lib.process.ProcessTools;\n+import jdk.test.lib.process.OutputAnalyzer;\n+\n+public class TestOldGrowthTriggers {\n+\n+    public static void makeOldAllocations() {\n+        \/\/ Expect most of the BigInteger entries placed into array to be promoted, and most will eventually become garbage within old\n+\n+        final int ArraySize = 512 * 1024;   \/\/ 512K entries\n+        final int BitsInBigInteger = 128;\n+        final int RefillIterations = 64;\n+        BigInteger array[] = new BigInteger[ArraySize];\n+        Random r = new Random(46);\n+\n+        for (int i = 0; i < ArraySize; i++) {\n+            array[i] = new BigInteger(BitsInBigInteger, r);\n+        }\n+\n+        for (int refillCount = 0; refillCount < RefillIterations; refillCount++) {\n+            \/\/ Each refill repopulates ArraySize randomly selected elements within array\n+            for (int i = 0; i < ArraySize; i++) {\n+                int replaceIndex = r.nextInt(ArraySize);\n+                int deriveIndex = r.nextInt(ArraySize);\n+                switch (i & 0x3) {\n+                    case 0:\n+                        \/\/ 50% chance of creating garbage\n+                        array[replaceIndex] = array[replaceIndex].max(array[deriveIndex]);\n+                        break;\n+                    case 1:\n+                        \/\/ 50% chance of creating garbage\n+                        array[replaceIndex] = array[replaceIndex].min(array[deriveIndex]);\n+                        break;\n+                    case 2:\n+                        \/\/ creates new old BigInteger, releases old BigInteger,\n+                        \/\/ may create ephemeral data while computing gcd\n+                        array[replaceIndex] = array[replaceIndex].gcd(array[deriveIndex]);\n+                        break;\n+                    case 3:\n+                        \/\/ creates new old BigInteger, releases old BigInteger\n+                        array[replaceIndex] = array[replaceIndex].multiply(array[deriveIndex]);\n+                        break;\n+                }\n+            }\n+        }\n+    }\n+\n+    public static void testOld(String... args) throws Exception {\n+        String[] cmds = Arrays.copyOf(args, args.length + 2);\n+        cmds[args.length] = TestOldGrowthTriggers.class.getName();\n+        cmds[args.length + 1] = \"test\";\n+        ProcessBuilder pb = ProcessTools.createLimitedTestJavaProcessBuilder(cmds);\n+        OutputAnalyzer output = new OutputAnalyzer(pb.start());\n+        output.shouldHaveExitValue(0);\n+        output.shouldContain(\"Trigger (Old): Old has overgrown\");\n+    }\n+\n+    public static void main(String[] args) throws Exception {\n+        if (args.length > 0 && args[0].equals(\"test\")) {\n+            makeOldAllocations();\n+            return;\n+        }\n+\n+        testOld(\"-Xlog:gc\",\n+                \"-Xms96m\",\n+                \"-Xmx96m\",\n+                \"-XX:+UnlockDiagnosticVMOptions\",\n+                \"-XX:+UnlockExperimentalVMOptions\",\n+                \"-XX:+UseShenandoahGC\",\n+                \"-XX:ShenandoahGCMode=generational\",\n+                \"-XX:ShenandoahGuaranteedYoungGCInterval=0\",\n+                \"-XX:ShenandoahGuaranteedOldGCInterval=0\"\n+        );\n+    }\n+}\n","filename":"test\/hotspot\/jtreg\/gc\/shenandoah\/generational\/TestOldGrowthTriggers.java","additions":110,"deletions":0,"binary":false,"changes":110,"status":"added"},{"patch":"@@ -0,0 +1,120 @@\n+\/*\n+ * Copyright Amazon.com Inc. or its affiliates. All Rights Reserved.\n+ * DO NOT ALTER OR REMOVE COPYRIGHT NOTICES OR THIS FILE HEADER.\n+ *\n+ * This code is free software; you can redistribute it and\/or modify it\n+ * under the terms of the GNU General Public License version 2 only, as\n+ * published by the Free Software Foundation.\n+ *\n+ * This code is distributed in the hope that it will be useful, but WITHOUT\n+ * ANY WARRANTY; without even the implied warranty of MERCHANTABILITY or\n+ * FITNESS FOR A PARTICULAR PURPOSE.  See the GNU General Public License\n+ * version 2 for more details (a copy is included in the LICENSE file that\n+ * accompanied this code).\n+ *\n+ * You should have received a copy of the GNU General Public License version\n+ * 2 along with this work; if not, write to the Free Software Foundation,\n+ * Inc., 51 Franklin St, Fifth Floor, Boston, MA 02110-1301 USA.\n+ *\n+ * Please contact Oracle, 500 Oracle Parkway, Redwood Shores, CA 94065 USA\n+ * or visit www.oracle.com if you need additional information or have any\n+ * questions.\n+ *\n+ *\/\n+\n+package gc.shenandoah.generational;\n+\n+import jdk.test.whitebox.WhiteBox;\n+import java.util.Random;\n+\n+\/*\n+ * @test id=generational\n+ * @requires vm.gc.Shenandoah\n+ * @summary Confirm that card marking and remembered set scanning do not crash.\n+ * @library \/testlibrary \/test\/lib \/\n+ * @build jdk.test.whitebox.WhiteBox\n+ * @run driver jdk.test.lib.helpers.ClassFileInstaller jdk.test.whitebox.WhiteBox\n+ * @run main\/othervm -Xbootclasspath\/a:.\n+ *      -XX:+IgnoreUnrecognizedVMOptions\n+ *      -XX:+UnlockDiagnosticVMOptions -XX:+WhiteBoxAPI\n+ *      -XX:+UnlockExperimentalVMOptions\n+ *      -XX:+UseShenandoahGC -XX:ShenandoahGCMode=generational\n+ *      gc.shenandoah.generational.TestSimpleGenerational\n+ *\/\n+public class TestSimpleGenerational {\n+    private static WhiteBox WB = WhiteBox.getWhiteBox();\n+    private static final int RANDOM_SEED = 46;\n+    \/\/ Sequence of random numbers should end with same value\n+    private static final int EXPECTED_LAST_RANDOM = 136227050;\n+\n+\n+    public static class Node {\n+        private static final int NEIGHBOR_COUNT = 5;\n+        private static final int INT_ARRAY_SIZE = 8;\n+        private static final Random RANDOM = new Random(RANDOM_SEED);\n+\n+        private int val;\n+        private Object objectField;\n+\n+        \/\/ Each Node instance holds references to two \"private\" arrays.\n+        \/\/ One array holds raw seething bits (primitive integers) and the other\n+        \/\/ holds references.\n+\n+        private int[] intsField;\n+        private Node [] neighbors;\n+\n+        public Node(int val) {\n+            this.val = val;\n+            this.objectField = new Object();\n+            this.intsField = new int[INT_ARRAY_SIZE];\n+            this.intsField[0] = 0xca;\n+            this.intsField[1] = 0xfe;\n+            this.intsField[2] = 0xba;\n+            this.intsField[3] = 0xbe;\n+            this.intsField[4] = 0xba;\n+            this.intsField[5] = 0xad;\n+            this.intsField[6] = 0xba;\n+            this.intsField[7] = 0xbe;\n+\n+            this.neighbors = new Node[NEIGHBOR_COUNT];\n+        }\n+\n+        public int value() {\n+            return val;\n+        }\n+\n+        \/\/ Copy each neighbor of n into a new node's neighbor array.\n+        \/\/ Then overwrite arbitrarily selected neighbor with newly allocated\n+        \/\/ leaf node.\n+        public static Node upheaval(Node n) {\n+            int firstValue = RANDOM.nextInt(Integer.MAX_VALUE);\n+            Node result = new Node(firstValue);\n+            if (n != null) {\n+                for (int i = 0; i < NEIGHBOR_COUNT; i++) {\n+                    result.neighbors[i] = n.neighbors[i];\n+                }\n+            }\n+            int secondValue = RANDOM.nextInt(Integer.MAX_VALUE);\n+            int overwriteIndex = firstValue % NEIGHBOR_COUNT;\n+            result.neighbors[overwriteIndex] = new Node(secondValue);\n+            return result;\n+        }\n+    }\n+\n+    public static void main(String args[]) throws Exception {\n+        Node n = null;\n+\n+        if (!WB.getBooleanVMFlag(\"UseShenandoahGC\") || !WB.getStringVMFlag(\"ShenandoahGCMode\").equals(\"generational\")) {\n+            throw new IllegalStateException(\"Command-line options not honored!\");\n+        }\n+\n+        for (int count = 10000; count > 0; count--) {\n+            n = Node.upheaval(n);\n+        }\n+\n+        System.out.println(\"Expected Last Random: [\" + n.value() + \"]\");\n+        if (n.value() != EXPECTED_LAST_RANDOM) {\n+            throw new IllegalStateException(\"Random number sequence ended badly!\");\n+        }\n+    }\n+}\n","filename":"test\/hotspot\/jtreg\/gc\/shenandoah\/generational\/TestSimpleGenerational.java","additions":120,"deletions":0,"binary":false,"changes":120,"status":"added"},{"patch":"@@ -3,0 +3,1 @@\n+ * Copyright Amazon.com Inc. or its affiliates. All Rights Reserved.\n@@ -25,1 +26,1 @@\n-\/* @test\n+\/* @test id=default\n@@ -35,0 +36,10 @@\n+ \/* @test id=generational\n+  * @summary test JNI critical arrays support in Shenandoah\n+  * @key randomness\n+  * @requires vm.gc.Shenandoah\n+  * @library \/test\/lib\n+  *\n+  * @run main\/othervm\/native -XX:+UnlockDiagnosticVMOptions -XX:+UnlockExperimentalVMOptions -XX:+UseShenandoahGC -XX:ShenandoahGCMode=generational -XX:+ShenandoahVerify TestJNICritical\n+  * @run main\/othervm\/native -XX:+UnlockDiagnosticVMOptions -XX:+UnlockExperimentalVMOptions -XX:+UseShenandoahGC -XX:ShenandoahGCMode=generational                       TestJNICritical\n+  *\/\n+\n","filename":"test\/hotspot\/jtreg\/gc\/shenandoah\/jni\/TestJNICritical.java","additions":12,"deletions":1,"binary":false,"changes":13,"status":"modified"},{"patch":"@@ -3,0 +3,1 @@\n+ * Copyright Amazon.com Inc. or its affiliates. All Rights Reserved.\n@@ -44,0 +45,19 @@\n+\/* @test id=generational-verify\n+ * @summary Test JNI Global Refs with Shenandoah\n+ * @requires vm.gc.Shenandoah\n+ *\n+ * @run main\/othervm\/native -Xmx1g -Xlog:gc -XX:+UnlockDiagnosticVMOptions -XX:+UnlockExperimentalVMOptions\n+ *      -XX:+UseShenandoahGC -XX:ShenandoahGCMode=generational\n+ *      -XX:+ShenandoahVerify\n+ *      TestJNIGlobalRefs\n+ *\/\n+\n+\/* @test id=generational\n+ * @summary Test JNI Global Refs with Shenandoah\n+ * @requires vm.gc.Shenandoah\n+ *\n+ * @run main\/othervm\/native -Xmx1g -Xlog:gc -XX:+UnlockDiagnosticVMOptions -XX:+UnlockExperimentalVMOptions\n+ *      -XX:+UseShenandoahGC -XX:ShenandoahGCMode=generational\n+ *      TestJNIGlobalRefs\n+ *\/\n+\n@@ -52,1 +72,1 @@\n-    private static final int TIME_MSEC = 120000;\n+    private static final long TIME_NSEC = 120L * 1_000_000_000L;\n@@ -63,3 +83,3 @@\n-        long start = System.currentTimeMillis();\n-        long current = start;\n-        while (current - start < TIME_MSEC) {\n+        long startNanos = System.nanoTime();\n+        long currentNanos = startNanos;\n+        while (currentNanos - startNanos < TIME_NSEC) {\n@@ -69,1 +89,1 @@\n-            current = System.currentTimeMillis();\n+            currentNanos = System.nanoTime();\n","filename":"test\/hotspot\/jtreg\/gc\/shenandoah\/jni\/TestJNIGlobalRefs.java","additions":25,"deletions":5,"binary":false,"changes":30,"status":"modified"},{"patch":"@@ -3,0 +3,1 @@\n+ * Copyright Amazon.com Inc. or its affiliates. All Rights Reserved.\n@@ -64,0 +65,16 @@\n+\/* @test id=generational\n+ * @summary Test that garbage in the pinned region does not crash VM\n+ * @key randomness\n+ * @requires vm.gc.Shenandoah\n+ * @library \/test\/lib\n+ *\n+ * @run main\/othervm\/native -XX:+UnlockDiagnosticVMOptions -XX:+UnlockExperimentalVMOptions -Xmx128m\n+ *      -XX:+UseShenandoahGC -XX:ShenandoahGCMode=generational\n+ *      -XX:+ShenandoahVerify\n+ *      TestPinnedGarbage\n+ *\n+ * @run main\/othervm\/native -XX:+UnlockDiagnosticVMOptions -XX:+UnlockExperimentalVMOptions -Xmx128m\n+ *      -XX:+UseShenandoahGC -XX:ShenandoahGCMode=generational\n+ *      TestPinnedGarbage\n+ *\/\n+\n","filename":"test\/hotspot\/jtreg\/gc\/shenandoah\/jni\/TestPinnedGarbage.java","additions":17,"deletions":0,"binary":false,"changes":17,"status":"modified"},{"patch":"@@ -3,0 +3,1 @@\n+ * Copyright Amazon.com Inc. or its affiliates. All Rights Reserved.\n@@ -66,0 +67,40 @@\n+\/**\n+ * @test id=generational\n+ * @summary Tests JVMTI heap dumps\n+ * @requires vm.gc.Shenandoah\n+ * @requires vm.jvmti\n+ * @compile TestHeapDump.java\n+ * @run main\/othervm\/native\/timeout=300 -agentlib:TestHeapDump\n+ *      -XX:+UnlockDiagnosticVMOptions -XX:+UnlockExperimentalVMOptions\n+ *      -XX:+UseShenandoahGC -Xmx128m\n+ *      -XX:ShenandoahGCMode=generational\n+ *      TestHeapDump\n+ *\n+ *\/\n+\n+\/**\n+ * @test id=no-coops-generational\n+ * @summary Tests JVMTI heap dumps\n+ * @requires vm.gc.Shenandoah\n+ * @requires vm.jvmti\n+ * @requires vm.bits == \"64\"\n+ * @compile TestHeapDump.java\n+ * @run main\/othervm\/native\/timeout=300 -agentlib:TestHeapDump\n+ *      -XX:+UnlockDiagnosticVMOptions -XX:+UnlockExperimentalVMOptions\n+ *      -XX:+UseShenandoahGC -Xmx128m\n+ *      -XX:ShenandoahGCMode=generational\n+ *      -XX:-UseCompressedOops TestHeapDump\n+ *\/\n+\n+\/**\n+ * @test id=generational-strdedup\n+ * @summary Tests JVMTI heap dumps\n+ * @requires vm.gc.Shenandoah\n+ * @requires vm.jvmti\n+ * @compile TestHeapDump.java\n+ * @run main\/othervm\/native\/timeout=300 -agentlib:TestHeapDump\n+ *      -XX:+UnlockDiagnosticVMOptions -XX:+UnlockExperimentalVMOptions\n+ *      -XX:+UseShenandoahGC -Xmx128m\n+ *      -XX:ShenandoahGCMode=generational\n+ *      -XX:+UseStringDeduplication TestHeapDump\n+ *\/\n","filename":"test\/hotspot\/jtreg\/gc\/shenandoah\/jvmti\/TestHeapDump.java","additions":41,"deletions":0,"binary":false,"changes":41,"status":"modified"},{"patch":"@@ -3,0 +3,1 @@\n+ * Copyright Amazon.com Inc. or its affiliates. All Rights Reserved.\n@@ -29,1 +30,1 @@\n- * @requires vm.gc.Shenandoah\n+ * @requires vm.gc.Shenandoah & vm.opt.ShenandoahGCMode != \"generational\"\n@@ -46,1 +47,1 @@\n- * @requires vm.gc.Shenandoah\n+ * @requires vm.gc.Shenandoah & vm.opt.ShenandoahGCMode != \"generational\"\n@@ -58,1 +59,1 @@\n- * @requires vm.gc.Shenandoah\n+ * @requires vm.gc.Shenandoah & vm.opt.ShenandoahGCMode != \"generational\"\n@@ -70,1 +71,1 @@\n- * @requires vm.gc.Shenandoah\n+ * @requires vm.gc.Shenandoah & vm.opt.ShenandoahGCMode != \"generational\"\n@@ -82,1 +83,1 @@\n- * @requires vm.gc.Shenandoah\n+ * @requires vm.gc.Shenandoah & vm.opt.ShenandoahGCMode != \"generational\"\n@@ -91,1 +92,1 @@\n- * @test id=iu\n+ * @test id=generational\n@@ -97,7 +98,2 @@\n- *      -XX:+UseShenandoahGC -XX:ShenandoahGCMode=iu -XX:ShenandoahGCHeuristics=aggressive\n- *      -Dprecise=false\n- *      TestChurnNotifications\n- *\n- * @run main\/othervm -Xmx128m -XX:+UnlockDiagnosticVMOptions -XX:+UnlockExperimentalVMOptions\n- *      -XX:+UseShenandoahGC -XX:ShenandoahGCMode=iu\n- *      -Dprecise=false\n+ *      -XX:+UseShenandoahGC -XX:ShenandoahGCMode=generational\n+ *      -Dprecise=false -Dmem.pool=Young\n@@ -131,0 +127,2 @@\n+    private static final String POOL_NAME = \"Young\".equals(System.getProperty(\"mem.pool\")) ? \"Shenandoah Young Gen\" : \"Shenandoah\";\n+\n@@ -132,1 +130,1 @@\n-        final long startTime = System.currentTimeMillis();\n+        final long startTimeNanos = System.nanoTime();\n@@ -144,2 +142,2 @@\n-                    MemoryUsage before = mapBefore.get(\"Shenandoah\");\n-                    MemoryUsage after = mapAfter.get(\"Shenandoah\");\n+                    MemoryUsage before = mapBefore.get(POOL_NAME);\n+                    MemoryUsage after = mapAfter.get(POOL_NAME);\n@@ -179,2 +177,2 @@\n-        long spentTime = System.currentTimeMillis() - startTime;\n-        long maxTries = (Utils.adjustTimeout(Utils.DEFAULT_TEST_TIMEOUT) - spentTime) \/ STEP_MS \/ 4;\n+        long spentTimeNanos = System.nanoTime() - startTimeNanos;\n+        long maxTries = (Utils.adjustTimeout(Utils.DEFAULT_TEST_TIMEOUT) - (spentTimeNanos \/ 1_000_000L)) \/ STEP_MS \/ 4;\n","filename":"test\/hotspot\/jtreg\/gc\/shenandoah\/mxbeans\/TestChurnNotifications.java","additions":16,"deletions":18,"binary":false,"changes":34,"status":"modified"},{"patch":"@@ -3,0 +3,1 @@\n+ * Copyright Amazon.com Inc. or its affiliates. All Rights Reserved.\n@@ -26,1 +27,1 @@\n- * @test\n+ * @test id=default\n@@ -38,0 +39,13 @@\n+\/**\n+ * @test id=generational\n+ * @summary Test JMX memory beans\n+ * @requires vm.gc.Shenandoah\n+ * @modules java.base\/jdk.internal.misc\n+ *          java.management\n+ * @run main\/othervm -XX:+UnlockExperimentalVMOptions -XX:+UseShenandoahGC -XX:ShenandoahGCMode=generational          -Xmx1g TestMemoryMXBeans   -1 1024\n+ * @run main\/othervm -XX:+UnlockExperimentalVMOptions -XX:+UseShenandoahGC -XX:ShenandoahGCMode=generational -Xms1g   -Xmx1g TestMemoryMXBeans 1024 1024\n+ * @run main\/othervm -XX:+UnlockExperimentalVMOptions -XX:+UseShenandoahGC -XX:ShenandoahGCMode=generational -Xms128m -Xmx1g TestMemoryMXBeans  128 1024\n+ * @run main\/othervm -XX:+UnlockExperimentalVMOptions -XX:+UseShenandoahGC -XX:ShenandoahGCMode=generational -Xms1g   -Xmx1g -XX:ShenandoahUncommitDelay=0 TestMemoryMXBeans 1024 1024\n+ * @run main\/othervm -XX:+UnlockExperimentalVMOptions -XX:+UseShenandoahGC -XX:ShenandoahGCMode=generational -Xms128m -Xmx1g -XX:ShenandoahUncommitDelay=0 TestMemoryMXBeans  128 1024\n+ *\/\n+\n","filename":"test\/hotspot\/jtreg\/gc\/shenandoah\/mxbeans\/TestMemoryMXBeans.java","additions":15,"deletions":1,"binary":false,"changes":16,"status":"modified"},{"patch":"@@ -3,0 +3,1 @@\n+ * Copyright Amazon.com Inc. or its affiliates. All Rights Reserved.\n@@ -26,1 +27,1 @@\n- * @test\n+ * @test id=default\n@@ -34,0 +35,9 @@\n+\/**\n+ * @test id=generational\n+ * @summary Test JMX memory pools\n+ * @requires vm.gc.Shenandoah\n+ * @modules java.base\/jdk.internal.misc\n+ *          java.management\n+ * @run main\/othervm -XX:+UnlockExperimentalVMOptions -XX:+UseShenandoahGC -XX:ShenandoahGCMode=generational -Xmx1g -Xms1g TestMemoryPools\n+ *\/\n+\n","filename":"test\/hotspot\/jtreg\/gc\/shenandoah\/mxbeans\/TestMemoryPools.java","additions":11,"deletions":1,"binary":false,"changes":12,"status":"modified"},{"patch":"@@ -3,0 +3,1 @@\n+ * Copyright Amazon.com Inc. or its affiliates. All Rights Reserved.\n@@ -87,1 +88,1 @@\n- * @test id=iu\n+ * @test id=generational\n@@ -93,5 +94,1 @@\n- *      -XX:+UseShenandoahGC -XX:ShenandoahGCMode=iu -XX:ShenandoahGCHeuristics=aggressive\n- *      TestPauseNotifications\n- *\n- * @run main\/othervm -Xmx128m -XX:+UnlockDiagnosticVMOptions -XX:+UnlockExperimentalVMOptions\n- *      -XX:+UseShenandoahGC -XX:ShenandoahGCMode=iu\n+ *      -XX:+UseShenandoahGC -XX:ShenandoahGCMode=generational\n@@ -125,1 +122,1 @@\n-        final long startTime = System.currentTimeMillis();\n+        final long startTimeNanos = System.nanoTime();\n@@ -176,2 +173,2 @@\n-        long spentTime = System.currentTimeMillis() - startTime;\n-        long maxTries = (Utils.adjustTimeout(Utils.DEFAULT_TEST_TIMEOUT) - spentTime) \/ STEP_MS \/ 4;\n+        long spentTimeNanos = System.nanoTime() - startTimeNanos;\n+        long maxTries = (Utils.adjustTimeout(Utils.DEFAULT_TEST_TIMEOUT) - (spentTimeNanos \/ 1_000_000L)) \/ STEP_MS \/ 4;\n","filename":"test\/hotspot\/jtreg\/gc\/shenandoah\/mxbeans\/TestPauseNotifications.java","additions":6,"deletions":9,"binary":false,"changes":15,"status":"modified"},{"patch":"@@ -1,81 +0,0 @@\n-\/*\n- * Copyright (c) 2018, Red Hat, Inc. All rights reserved.\n- * DO NOT ALTER OR REMOVE COPYRIGHT NOTICES OR THIS FILE HEADER.\n- *\n- * This code is free software; you can redistribute it and\/or modify it\n- * under the terms of the GNU General Public License version 2 only, as\n- * published by the Free Software Foundation.\n- *\n- * This code is distributed in the hope that it will be useful, but WITHOUT\n- * ANY WARRANTY; without even the implied warranty of MERCHANTABILITY or\n- * FITNESS FOR A PARTICULAR PURPOSE.  See the GNU General Public License\n- * version 2 for more details (a copy is included in the LICENSE file that\n- * accompanied this code).\n- *\n- * You should have received a copy of the GNU General Public License version\n- * 2 along with this work; if not, write to the Free Software Foundation,\n- * Inc., 51 Franklin St, Fifth Floor, Boston, MA 02110-1301 USA.\n- *\n- * Please contact Oracle, 500 Oracle Parkway, Redwood Shores, CA 94065 USA\n- * or visit www.oracle.com if you need additional information or have any\n- * questions.\n- *\n- *\/\n-\n-\/**\n- * @test\n- * @summary Test allocation of small object to result OOM, but not to crash JVM\n- * @requires vm.gc.Shenandoah\n- * @library \/test\/lib\n- * @run driver TestAllocLargeObj\n- *\/\n-\n-import jdk.test.lib.process.OutputAnalyzer;\n-import jdk.test.lib.process.ProcessTools;\n-\n-public class TestAllocLargeObj {\n-\n-    static final int SIZE = 1 * 1024 * 1024;\n-    static final int COUNT = 16;\n-\n-    static volatile Object sink;\n-\n-    public static void work() throws Exception {\n-        Object[] root = new Object[COUNT];\n-        sink = root;\n-        for (int c = 0; c < COUNT; c++) {\n-            root[c] = new Object[SIZE];\n-        }\n-    }\n-\n-    public static void main(String[] args) throws Exception {\n-        if (args.length > 0) {\n-            work();\n-            return;\n-        }\n-\n-        {\n-            OutputAnalyzer analyzer = ProcessTools.executeLimitedTestJava(\n-                    \"-Xmx16m\",\n-                    \"-XX:+UnlockExperimentalVMOptions\",\n-                    \"-XX:+UseShenandoahGC\",\n-                    TestAllocLargeObj.class.getName(),\n-                    \"test\");\n-\n-            analyzer.shouldHaveExitValue(1);\n-            analyzer.shouldContain(\"java.lang.OutOfMemoryError: Java heap space\");\n-        }\n-\n-        {\n-            OutputAnalyzer analyzer = ProcessTools.executeLimitedTestJava(\n-                    \"-Xmx1g\",\n-                    \"-XX:+UnlockExperimentalVMOptions\",\n-                    \"-XX:+UseShenandoahGC\",\n-                    TestAllocLargeObj.class.getName(),\n-                    \"test\");\n-\n-            analyzer.shouldHaveExitValue(0);\n-            analyzer.shouldNotContain(\"java.lang.OutOfMemoryError: Java heap space\");\n-        }\n-    }\n-}\n","filename":"test\/hotspot\/jtreg\/gc\/shenandoah\/oom\/TestAllocLargeObj.java","additions":0,"deletions":81,"binary":false,"changes":81,"status":"deleted"},{"patch":"@@ -1,76 +0,0 @@\n-\/*\n- * Copyright (c) 2018, Red Hat, Inc. All rights reserved.\n- * DO NOT ALTER OR REMOVE COPYRIGHT NOTICES OR THIS FILE HEADER.\n- *\n- * This code is free software; you can redistribute it and\/or modify it\n- * under the terms of the GNU General Public License version 2 only, as\n- * published by the Free Software Foundation.\n- *\n- * This code is distributed in the hope that it will be useful, but WITHOUT\n- * ANY WARRANTY; without even the implied warranty of MERCHANTABILITY or\n- * FITNESS FOR A PARTICULAR PURPOSE.  See the GNU General Public License\n- * version 2 for more details (a copy is included in the LICENSE file that\n- * accompanied this code).\n- *\n- * You should have received a copy of the GNU General Public License version\n- * 2 along with this work; if not, write to the Free Software Foundation,\n- * Inc., 51 Franklin St, Fifth Floor, Boston, MA 02110-1301 USA.\n- *\n- * Please contact Oracle, 500 Oracle Parkway, Redwood Shores, CA 94065 USA\n- * or visit www.oracle.com if you need additional information or have any\n- * questions.\n- *\n- *\/\n-\n-\/**\n- * @test\n- * @summary Test that allocation of the object larger than heap fails predictably\n- * @requires vm.gc.Shenandoah\n- * @library \/test\/lib\n- * @run driver TestAllocLargerThanHeap\n- *\/\n-\n-import jdk.test.lib.process.OutputAnalyzer;\n-import jdk.test.lib.process.ProcessTools;\n-\n-public class TestAllocLargerThanHeap {\n-\n-    static final int SIZE = 16 * 1024 * 1024;\n-\n-    static volatile Object sink;\n-\n-    public static void work() throws Exception {\n-        sink = new Object[SIZE];\n-    }\n-\n-    public static void main(String[] args) throws Exception {\n-        if (args.length > 0) {\n-            work();\n-            return;\n-        }\n-\n-        {\n-            OutputAnalyzer analyzer = ProcessTools.executeLimitedTestJava(\n-                    \"-Xmx16m\",\n-                    \"-XX:+UnlockExperimentalVMOptions\",\n-                    \"-XX:+UseShenandoahGC\",\n-                    TestAllocLargerThanHeap.class.getName(),\n-                    \"test\");\n-\n-            analyzer.shouldHaveExitValue(1);\n-            analyzer.shouldContain(\"java.lang.OutOfMemoryError: Java heap space\");\n-        }\n-\n-        {\n-            OutputAnalyzer analyzer = ProcessTools.executeLimitedTestJava(\n-                    \"-Xmx1g\",\n-                    \"-XX:+UnlockExperimentalVMOptions\",\n-                    \"-XX:+UseShenandoahGC\",\n-                    TestAllocLargerThanHeap.class.getName(),\n-                    \"test\");\n-\n-            analyzer.shouldHaveExitValue(0);\n-            analyzer.shouldNotContain(\"java.lang.OutOfMemoryError: Java heap space\");\n-        }\n-    }\n-}\n","filename":"test\/hotspot\/jtreg\/gc\/shenandoah\/oom\/TestAllocLargerThanHeap.java","additions":0,"deletions":76,"binary":false,"changes":76,"status":"deleted"},{"patch":"@@ -0,0 +1,147 @@\n+\/*\n+ * Copyright (c) 2018, Red Hat, Inc. All rights reserved.\n+ * DO NOT ALTER OR REMOVE COPYRIGHT NOTICES OR THIS FILE HEADER.\n+ *\n+ * This code is free software; you can redistribute it and\/or modify it\n+ * under the terms of the GNU General Public License version 2 only, as\n+ * published by the Free Software Foundation.\n+ *\n+ * This code is distributed in the hope that it will be useful, but WITHOUT\n+ * ANY WARRANTY; without even the implied warranty of MERCHANTABILITY or\n+ * FITNESS FOR A PARTICULAR PURPOSE.  See the GNU General Public License\n+ * version 2 for more details (a copy is included in the LICENSE file that\n+ * accompanied this code).\n+ *\n+ * You should have received a copy of the GNU General Public License version\n+ * 2 along with this work; if not, write to the Free Software Foundation,\n+ * Inc., 51 Franklin St, Fifth Floor, Boston, MA 02110-1301 USA.\n+ *\n+ * Please contact Oracle, 500 Oracle Parkway, Redwood Shores, CA 94065 USA\n+ * or visit www.oracle.com if you need additional information or have any\n+ * questions.\n+ *\n+ *\/\n+\n+\/**\n+ * @test id=large\n+ * @summary Test allocation of large objects results in OOM, but will not crash the JVM\n+ * @requires vm.gc.Shenandoah\n+ * @library \/test\/lib\n+ * @run driver TestAllocOutOfMemory large\n+ *\/\n+\n+\/**\n+ * @test id=heap\n+ * @summary Test allocation of a heap-sized object results in OOM, but will not crash the JVM\n+ * @requires vm.gc.Shenandoah\n+ * @library \/test\/lib\n+ * @run driver TestAllocOutOfMemory heap\n+ *\/\n+\n+\/**\n+ * @test id=small\n+ * @summary Test allocation of small objects results in OOM, but will not crash the JVM\n+ * @requires vm.gc.Shenandoah\n+ * @library \/test\/lib\n+ * @run driver TestAllocOutOfMemory small\n+ *\/\n+\n+import jdk.test.lib.process.OutputAnalyzer;\n+import jdk.test.lib.process.ProcessTools;\n+\n+public class TestAllocOutOfMemory {\n+\n+    static volatile Object sink;\n+\n+    public static void work(int size, int count) throws Exception {\n+        Object[] root = new Object[count];\n+        sink = root;\n+        for (int c = 0; c < count; c++) {\n+            root[c] = new Object[size];\n+        }\n+    }\n+\n+    private static void allocate(String size, int multiplier) throws Exception {\n+        switch (size) {\n+            case \"large\":\n+                work(1024 * 1024, 16 * multiplier);\n+                break;\n+            case \"heap\":\n+                work(16 * 1024 * 1024, multiplier);\n+                break;\n+            case \"small\":\n+                work(1, 16 * 1024 * 1024 * multiplier);\n+                break;\n+            default:\n+                throw new IllegalArgumentException(\"Usage: test [large|small|heap]\");\n+        }\n+    }\n+\n+    public static void main(String[] args) throws Exception {\n+        if (args.length > 2) {\n+            \/\/ Called from test, size is second argument, heap requested is third\n+            String size = args[1];\n+            long spec_heap = Integer.parseInt(args[2]);\n+\n+            \/\/ The actual heap we get may be larger than the one we asked for\n+            \/\/ (particularly in the generational case)\n+            final long actual_heap = Runtime.getRuntime().maxMemory();\n+            int multiplier = 1;\n+            if (actual_heap > spec_heap) {\n+                \/\/ A suitable multiplier is used, so as to allocate an\n+                \/\/ amount appropriate to the larger actual heap size than what\n+                \/\/ was specified.\n+                multiplier = (int)((actual_heap + spec_heap - 1)\/spec_heap);\n+            }\n+\n+            allocate(size, multiplier);\n+            return;\n+        }\n+\n+        \/\/ Called from jtreg, size is first argument\n+        String size = args[0];\n+        {\n+            int heap = 16*1024*1024;      \/\/ -Xmx16m\n+            expectFailure(\"-Xmx16m\",\n+                          \"-XX:+UnlockExperimentalVMOptions\",\n+                          \"-XX:+UseShenandoahGC\",\n+                          TestAllocOutOfMemory.class.getName(),\n+                          \"test\", size, Integer.toString(heap));\n+\n+            expectFailure(\"-Xmx16m\",\n+                          \"-XX:+UnlockExperimentalVMOptions\",\n+                          \"-XX:+UseShenandoahGC\", \"-XX:ShenandoahGCMode=generational\",\n+                          TestAllocOutOfMemory.class.getName(),\n+                          \"test\", size, Integer.toString(heap));\n+        }\n+\n+        {\n+            int heap = 1*1024*1024*1024;  \/\/ -Xmx1g\n+            expectSuccess(\"-Xmx1g\",\n+                          \"-XX:+UnlockExperimentalVMOptions\",\n+                          \"-XX:+UseShenandoahGC\",\n+                          TestAllocOutOfMemory.class.getName(),\n+                          \"test\", size, Integer.toString(heap));\n+\n+            expectSuccess(\"-Xmx1g\",\n+                          \"-XX:+UnlockExperimentalVMOptions\",\n+                          \"-XX:+UseShenandoahGC\", \"-XX:ShenandoahGCMode=generational\",\n+                          TestAllocOutOfMemory.class.getName(),\n+                          \"test\", size, Integer.toString(heap));\n+        }\n+    }\n+\n+    private static void expectSuccess(String... args) throws Exception {\n+        ProcessBuilder pb = ProcessTools.createLimitedTestJavaProcessBuilder(args);\n+        OutputAnalyzer analyzer = new OutputAnalyzer(pb.start());\n+        analyzer.shouldHaveExitValue(0);\n+        analyzer.shouldNotContain(\"java.lang.OutOfMemoryError: Java heap space\");\n+    }\n+\n+    private static void expectFailure(String... args) throws Exception {\n+        ProcessBuilder pb = ProcessTools.createLimitedTestJavaProcessBuilder(args);\n+        OutputAnalyzer analyzer = new OutputAnalyzer(pb.start());\n+        analyzer.shouldHaveExitValue(1);\n+        analyzer.shouldContain(\"java.lang.OutOfMemoryError: Java heap space\");\n+    }\n+}\n","filename":"test\/hotspot\/jtreg\/gc\/shenandoah\/oom\/TestAllocOutOfMemory.java","additions":147,"deletions":0,"binary":false,"changes":147,"status":"added"},{"patch":"@@ -1,80 +0,0 @@\n-\/*\n- * Copyright (c) 2018, Red Hat, Inc. All rights reserved.\n- * DO NOT ALTER OR REMOVE COPYRIGHT NOTICES OR THIS FILE HEADER.\n- *\n- * This code is free software; you can redistribute it and\/or modify it\n- * under the terms of the GNU General Public License version 2 only, as\n- * published by the Free Software Foundation.\n- *\n- * This code is distributed in the hope that it will be useful, but WITHOUT\n- * ANY WARRANTY; without even the implied warranty of MERCHANTABILITY or\n- * FITNESS FOR A PARTICULAR PURPOSE.  See the GNU General Public License\n- * version 2 for more details (a copy is included in the LICENSE file that\n- * accompanied this code).\n- *\n- * You should have received a copy of the GNU General Public License version\n- * 2 along with this work; if not, write to the Free Software Foundation,\n- * Inc., 51 Franklin St, Fifth Floor, Boston, MA 02110-1301 USA.\n- *\n- * Please contact Oracle, 500 Oracle Parkway, Redwood Shores, CA 94065 USA\n- * or visit www.oracle.com if you need additional information or have any\n- * questions.\n- *\n- *\/\n-\n-\/**\n- * @test\n- * @summary Test allocation of small object to result OOM, but not to crash JVM\n- * @requires vm.gc.Shenandoah\n- * @library \/test\/lib\n- * @run driver TestAllocSmallObj\n- *\/\n-\n-import jdk.test.lib.process.OutputAnalyzer;\n-import jdk.test.lib.process.ProcessTools;\n-\n-public class TestAllocSmallObj {\n-\n-    static final int COUNT = 16 * 1024 * 1024;\n-\n-    static volatile Object sink;\n-\n-    public static void work() throws Exception {\n-        Object[] root = new Object[COUNT];\n-        sink = root;\n-        for (int c = 0; c < COUNT; c++) {\n-            root[c] = new Object();\n-        }\n-    }\n-\n-    public static void main(String[] args) throws Exception {\n-        if (args.length > 0) {\n-            work();\n-            return;\n-        }\n-\n-        {\n-            OutputAnalyzer analyzer = ProcessTools.executeLimitedTestJava(\n-                    \"-Xmx16m\",\n-                    \"-XX:+UnlockExperimentalVMOptions\",\n-                    \"-XX:+UseShenandoahGC\",\n-                    TestAllocSmallObj.class.getName(),\n-                    \"test\");\n-\n-            analyzer.shouldHaveExitValue(1);\n-            analyzer.shouldContain(\"java.lang.OutOfMemoryError: Java heap space\");\n-        }\n-\n-        {\n-            OutputAnalyzer analyzer = ProcessTools.executeLimitedTestJava(\n-                    \"-Xmx1g\",\n-                    \"-XX:+UnlockExperimentalVMOptions\",\n-                    \"-XX:+UseShenandoahGC\",\n-                    TestAllocSmallObj.class.getName(),\n-                    \"test\");\n-\n-            analyzer.shouldHaveExitValue(0);\n-            analyzer.shouldNotContain(\"java.lang.OutOfMemoryError: Java heap space\");\n-        }\n-    }\n-}\n","filename":"test\/hotspot\/jtreg\/gc\/shenandoah\/oom\/TestAllocSmallObj.java","additions":0,"deletions":80,"binary":false,"changes":80,"status":"deleted"},{"patch":"@@ -3,0 +3,1 @@\n+ * Copyright Amazon.com Inc. or its affiliates. All Rights Reserved.\n@@ -30,1 +31,1 @@\n- * @run driver TestClassLoaderLeak\n+ * @run driver\/timeout=600 TestClassLoaderLeak\n@@ -126,3 +127,3 @@\n-             {{\"satb\"},    {\"adaptive\", \"compact\", \"static\", \"aggressive\"}},\n-             {{\"iu\"},      {\"adaptive\", \"aggressive\"}},\n-             {{\"passive\"}, {\"passive\"}}\n+             {{\"satb\"},         {\"adaptive\", \"compact\", \"static\", \"aggressive\"}},\n+             {{\"passive\"},      {\"passive\"}},\n+             {{\"generational\"}, {\"adaptive\"}}\n","filename":"test\/hotspot\/jtreg\/gc\/shenandoah\/oom\/TestClassLoaderLeak.java","additions":5,"deletions":4,"binary":false,"changes":9,"status":"modified"},{"patch":"@@ -3,0 +3,1 @@\n+ * Copyright Amazon.com Inc. or its affiliates. All Rights Reserved.\n@@ -56,0 +57,5 @@\n+                \/\/ If we experience OutOfMemoryError during our attempt to instantiate NastyThread, we'll abort\n+                \/\/ main and will not print \"All good\".  We'll also report a non-zero termination code.  In the\n+                \/\/ case that the previously instantiated NastyThread accumulated more than SheanndoahNoProgressThreshold\n+                \/\/ unproductive GC cycles before failing, the main thread may not try a Full GC before it experiences\n+                \/\/ OutOfMemoryError exception.\n@@ -59,0 +65,3 @@\n+                \/\/ Having joined thread, we know the memory consumed by thread is now garbage, and will eventually be\n+                \/\/ collected.  Some or all of that memory may have been promoted, so we may need to perform a Full GC\n+                \/\/ in order to reclaim it quickly.\n@@ -76,0 +85,15 @@\n+\n+        {\n+            ProcessBuilder pb = ProcessTools.createLimitedTestJavaProcessBuilder(\n+                    \"-Xmx32m\",\n+                    \"-XX:+UnlockExperimentalVMOptions\",\n+                    \"-XX:+UseShenandoahGC\", \"-XX:ShenandoahGCMode=generational\",\n+                    \"-XX:ShenandoahNoProgressThreshold=16\",\n+                    TestThreadFailure.class.getName(),\n+                    \"test\");\n+\n+            OutputAnalyzer analyzer = new OutputAnalyzer(pb.start());\n+            analyzer.shouldHaveExitValue(0);\n+            analyzer.shouldContain(\"java.lang.OutOfMemoryError\");\n+            analyzer.shouldContain(\"All good\");\n+        }\n","filename":"test\/hotspot\/jtreg\/gc\/shenandoah\/oom\/TestThreadFailure.java","additions":24,"deletions":0,"binary":false,"changes":24,"status":"modified"},{"patch":"@@ -128,18 +128,0 @@\n-\n-        {\n-            OutputAnalyzer output = ProcessTools.executeLimitedTestJava(\n-                    \"-Xmx128m\",\n-                    \"-XX:+UnlockExperimentalVMOptions\",\n-                    \"-XX:+UseShenandoahGC\",\n-                    \"-Xlog:gc\",\n-                    \"-XX:+ExplicitGCInvokesConcurrent\",\n-                    \"-XX:ShenandoahGCMode=iu\",\n-                    TestExplicitGC.class.getName(),\n-                    \"test\");\n-            for (String p : full) {\n-                output.shouldNotContain(p);\n-            }\n-            for (String p : concNormal) {\n-                output.shouldContain(p);\n-            }\n-         }\n","filename":"test\/hotspot\/jtreg\/gc\/shenandoah\/options\/TestExplicitGC.java","additions":0,"deletions":18,"binary":false,"changes":18,"status":"modified"},{"patch":"@@ -1,72 +0,0 @@\n-\/*\n- * Copyright (c) 2017, 2018, Red Hat, Inc. All rights reserved.\n- * DO NOT ALTER OR REMOVE COPYRIGHT NOTICES OR THIS FILE HEADER.\n- *\n- * This code is free software; you can redistribute it and\/or modify it\n- * under the terms of the GNU General Public License version 2 only, as\n- * published by the Free Software Foundation.\n- *\n- * This code is distributed in the hope that it will be useful, but WITHOUT\n- * ANY WARRANTY; without even the implied warranty of MERCHANTABILITY or\n- * FITNESS FOR A PARTICULAR PURPOSE.  See the GNU General Public License\n- * version 2 for more details (a copy is included in the LICENSE file that\n- * accompanied this code).\n- *\n- * You should have received a copy of the GNU General Public License version\n- * 2 along with this work; if not, write to the Free Software Foundation,\n- * Inc., 51 Franklin St, Fifth Floor, Boston, MA 02110-1301 USA.\n- *\n- * Please contact Oracle, 500 Oracle Parkway, Redwood Shores, CA 94065 USA\n- * or visit www.oracle.com if you need additional information or have any\n- * questions.\n- *\n- *\/\n-\n-\/*\n- * @test\n- * @summary Test that Shenandoah humongous threshold args are checked\n- * @requires vm.gc.Shenandoah\n- * @library \/test\/lib\n- * @modules java.base\/jdk.internal.misc\n- *          java.management\n- * @run driver TestHumongousThresholdArgs\n- *\/\n-\n-import jdk.test.lib.process.ProcessTools;\n-import jdk.test.lib.process.OutputAnalyzer;\n-\n-public class TestHumongousThresholdArgs {\n-    public static void main(String[] args) throws Exception {\n-        {\n-            OutputAnalyzer output = ProcessTools.executeLimitedTestJava(\n-                    \"-Xmx128m\",\n-                    \"-XX:+UnlockExperimentalVMOptions\",\n-                    \"-XX:+UseShenandoahGC\",\n-                    \"-version\");\n-            output.shouldHaveExitValue(0);\n-        }\n-\n-        int[] valid = new int[] {1, 10, 50, 90, 100};\n-        int[] invalid = new int[] {-100, -1, 0, 101, 1000};\n-\n-        for (int v : valid) {\n-            OutputAnalyzer output = ProcessTools.executeLimitedTestJava(\n-                    \"-Xmx128m\",\n-                    \"-XX:+UnlockExperimentalVMOptions\",\n-                    \"-XX:+UseShenandoahGC\",\n-                    \"-XX:ShenandoahHumongousThreshold=\" + v,\n-                    \"-version\");\n-            output.shouldHaveExitValue(0);\n-        }\n-\n-        for (int v : invalid) {\n-            OutputAnalyzer output = ProcessTools.executeLimitedTestJava(\n-                    \"-Xmx128m\",\n-                    \"-XX:+UnlockExperimentalVMOptions\",\n-                    \"-XX:+UseShenandoahGC\",\n-                    \"-XX:ShenandoahHumongousThreshold=\" + v,\n-                    \"-version\");\n-            output.shouldHaveExitValue(1);\n-        }\n-    }\n-}\n","filename":"test\/hotspot\/jtreg\/gc\/shenandoah\/options\/TestHumongousThresholdArgs.java","additions":0,"deletions":72,"binary":false,"changes":72,"status":"deleted"},{"patch":"@@ -47,3 +47,3 @@\n-        testWith(\"-XX:ShenandoahGCMode=satb\",    Mode.PRODUCT);\n-        testWith(\"-XX:ShenandoahGCMode=iu\",      Mode.EXPERIMENTAL);\n-        testWith(\"-XX:ShenandoahGCMode=passive\", Mode.DIAGNOSTIC);\n+        testWith(\"-XX:ShenandoahGCMode=satb\",         Mode.PRODUCT);\n+        testWith(\"-XX:ShenandoahGCMode=passive\",      Mode.DIAGNOSTIC);\n+        testWith(\"-XX:ShenandoahGCMode=generational\", Mode.EXPERIMENTAL);\n","filename":"test\/hotspot\/jtreg\/gc\/shenandoah\/options\/TestModeUnlock.java","additions":3,"deletions":3,"binary":false,"changes":6,"status":"modified"},{"patch":"@@ -53,1 +53,1 @@\n-                new String[] { \"ShenandoahSATBBarrier\", \"ShenandoahIUBarrier\" },\n+                new String[] { \"ShenandoahSATBBarrier\" },\n","filename":"test\/hotspot\/jtreg\/gc\/shenandoah\/options\/TestSelectiveBarrierFlags.java","additions":1,"deletions":1,"binary":false,"changes":2,"status":"modified"},{"patch":"@@ -47,6 +47,3 @@\n-        String[] iu = {\n-                \"ShenandoahLoadRefBarrier\",\n-                \"ShenandoahIUBarrier\",\n-                \"ShenandoahCASBarrier\",\n-                \"ShenandoahCloneBarrier\",\n-                \"ShenandoahStackWatermarkBarrier\",\n+\n+        String[] generational = {\n+                \"ShenandoahCardBarrier\"\n@@ -59,1 +56,0 @@\n-        shouldFailAll(\"-XX:ShenandoahGCMode=iu\",               iu);\n@@ -61,1 +57,3 @@\n-        shouldPassAll(\"-XX:ShenandoahGCMode=passive\",          iu);\n+        shouldPassAll(\"-XX:ShenandoahGCMode=passive\",          generational);\n+        shouldPassAll(\"-XX:ShenandoahGCMode=satb\",             generational);\n+        shouldFailAll(\"-XX:ShenandoahGCMode=generational\",     generational);\n","filename":"test\/hotspot\/jtreg\/gc\/shenandoah\/options\/TestWrongBarrierDisable.java","additions":6,"deletions":8,"binary":false,"changes":14,"status":"modified"},{"patch":"@@ -26,1 +26,1 @@\n- * @summary Test that disabling wrong barriers fails early\n+ * @summary Test that SATB barrier may be enabled for all modes\n@@ -41,3 +41,0 @@\n-                \"ShenandoahIUBarrier\",\n-        };\n-        String[] iu = {\n@@ -46,6 +43,10 @@\n-\n-        shouldFailAll(\"-XX:ShenandoahGCHeuristics=adaptive\",   concurrent);\n-        shouldFailAll(\"-XX:ShenandoahGCHeuristics=static\",     concurrent);\n-        shouldFailAll(\"-XX:ShenandoahGCHeuristics=compact\",    concurrent);\n-        shouldFailAll(\"-XX:ShenandoahGCHeuristics=aggressive\", concurrent);\n-        shouldFailAll(\"-XX:ShenandoahGCMode=iu\",               iu);\n+        String[] generational = {\n+                \"ShenandoahCardBarrier\"\n+        };\n+        String[] all = {\n+                \"ShenandoahSATBBarrier\", \"ShenandoahCardBarrier\"\n+        };\n+        shouldPassAll(\"-XX:ShenandoahGCHeuristics=adaptive\",   concurrent);\n+        shouldPassAll(\"-XX:ShenandoahGCHeuristics=static\",     concurrent);\n+        shouldPassAll(\"-XX:ShenandoahGCHeuristics=compact\",    concurrent);\n+        shouldPassAll(\"-XX:ShenandoahGCHeuristics=aggressive\", concurrent);\n@@ -53,1 +54,3 @@\n-        shouldPassAll(\"-XX:ShenandoahGCMode=passive\",          iu);\n+        shouldPassAll(\"-XX:ShenandoahGCMode=generational\",     all);\n+        shouldFailAll(\"-XX:ShenandoahGCMode=satb\",             generational);\n+        shouldFailAll(\"-XX:ShenandoahGCMode=passive\",          generational);\n@@ -87,1 +90,0 @@\n-\n","filename":"test\/hotspot\/jtreg\/gc\/shenandoah\/options\/TestWrongBarrierEnable.java","additions":14,"deletions":12,"binary":false,"changes":26,"status":"modified"},{"patch":"@@ -102,41 +102,0 @@\n-\/*\n- * @test id=iu-aggressive\n- * @key stress\n- * @library \/\n- * @requires vm.gc.Shenandoah\n- * @requires vm.flavor == \"server\" & !vm.emulatedClient\n- * @summary Stress the Shenandoah GC by trying to make old objects more likely to be garbage than young objects.\n- *\n- * @run main\/othervm\/timeout=200 -Xlog:gc*=info -Xmx1g -XX:+UnlockDiagnosticVMOptions -XX:+UnlockExperimentalVMOptions\n- *      -XX:+UseShenandoahGC -XX:ShenandoahGCMode=iu -XX:ShenandoahGCHeuristics=aggressive\n- *      -XX:+ShenandoahOOMDuringEvacALot\n- *      gc.stress.gcbasher.TestGCBasherWithShenandoah 120000\n- *\n- * @run main\/othervm\/timeout=200 -Xlog:gc*=info -Xmx1g -XX:+UnlockDiagnosticVMOptions -XX:+UnlockExperimentalVMOptions\n- *      -XX:+UseShenandoahGC -XX:ShenandoahGCMode=iu -XX:ShenandoahGCHeuristics=aggressive\n- *      -XX:+ShenandoahAllocFailureALot\n- *      gc.stress.gcbasher.TestGCBasherWithShenandoah 120000\n- *\n- * @run main\/othervm\/timeout=200 -Xlog:gc*=info -Xmx1g -XX:+UnlockDiagnosticVMOptions -XX:+UnlockExperimentalVMOptions\n- *      -XX:+UseShenandoahGC -XX:ShenandoahGCMode=iu -XX:ShenandoahGCHeuristics=aggressive\n- *      gc.stress.gcbasher.TestGCBasherWithShenandoah 120000\n- *\/\n-\n-\/*\n- * @test id=iu\n- * @key stress\n- * @library \/\n- * @requires vm.gc.Shenandoah\n- * @requires vm.flavor == \"server\" & !vm.emulatedClient\n- * @summary Stress the Shenandoah GC by trying to make old objects more likely to be garbage than young objects.\n- *\n- * @run main\/othervm\/timeout=200 -Xlog:gc*=info -Xmx1g -XX:+UnlockDiagnosticVMOptions -XX:+UnlockExperimentalVMOptions\n- *      -XX:+UseShenandoahGC -XX:ShenandoahGCMode=iu\n- *      -XX:+ShenandoahVerify\n- *      gc.stress.gcbasher.TestGCBasherWithShenandoah 120000\n- *\n- * @run main\/othervm\/timeout=200 -Xlog:gc*=info -Xmx1g -XX:+UnlockDiagnosticVMOptions -XX:+UnlockExperimentalVMOptions\n- *      -XX:+UseShenandoahGC -XX:ShenandoahGCMode=iu\n- *      gc.stress.gcbasher.TestGCBasherWithShenandoah 120000\n- *\/\n-\n@@ -224,47 +183,0 @@\n-\/*\n- * @test id=iu-aggressive-deopt-nmethod\n- * @key stress\n- * @library \/\n- * @requires vm.gc.Shenandoah\n- * @requires vm.flavor == \"server\" & !vm.emulatedClient & vm.opt.ClassUnloading != false\n- * @summary Stress Shenandoah GC with nmethod barrier forced deoptimization enabled.\n- *\n- * @run main\/othervm\/timeout=200 -Xlog:gc*=info,nmethod+barrier=trace -Xmx1g -XX:+UnlockDiagnosticVMOptions -XX:+UnlockExperimentalVMOptions\n- *      -XX:+UseShenandoahGC -XX:ShenandoahGCMode=iu -XX:ShenandoahGCHeuristics=aggressive\n- *      -XX:+DeoptimizeNMethodBarriersALot -XX:-Inline\n- *      -XX:+ShenandoahOOMDuringEvacALot\n- *      gc.stress.gcbasher.TestGCBasherWithShenandoah 120000\n- *\n- * @run main\/othervm\/timeout=200 -Xlog:gc*=info,nmethod+barrier=trace -Xmx1g -XX:+UnlockDiagnosticVMOptions -XX:+UnlockExperimentalVMOptions\n- *      -XX:+UseShenandoahGC -XX:ShenandoahGCMode=iu -XX:ShenandoahGCHeuristics=aggressive\n- *      -XX:+DeoptimizeNMethodBarriersALot -XX:-Inline\n- *      -XX:+ShenandoahAllocFailureALot\n- *      gc.stress.gcbasher.TestGCBasherWithShenandoah 120000\n- *\n- * @run main\/othervm\/timeout=200 -Xlog:gc*=info,nmethod+barrier=trace -Xmx1g -XX:+UnlockDiagnosticVMOptions -XX:+UnlockExperimentalVMOptions\n- *      -XX:+UseShenandoahGC -XX:ShenandoahGCMode=iu -XX:ShenandoahGCHeuristics=aggressive\n- *      -XX:+DeoptimizeNMethodBarriersALot -XX:-Inline\n- *      gc.stress.gcbasher.TestGCBasherWithShenandoah 120000\n- *\/\n-\n-\/*\n- * @test id=iu-deopt-nmethod\n- * @key stress\n- * @library \/\n- * @requires vm.gc.Shenandoah\n- * @requires vm.flavor == \"server\" & !vm.emulatedClient & vm.opt.ClassUnloading != false\n- * @summary Stress Shenandoah GC with nmethod barrier forced deoptimization enabled.\n- *\n- * @run main\/othervm\/timeout=200 -Xlog:gc*=info,nmethod+barrier=trace -Xmx1g -XX:+UnlockDiagnosticVMOptions -XX:+UnlockExperimentalVMOptions\n- *      -XX:+UseShenandoahGC -XX:ShenandoahGCMode=iu\n- *      -XX:+DeoptimizeNMethodBarriersALot -XX:-Inline\n- *      -XX:+ShenandoahVerify\n- *      gc.stress.gcbasher.TestGCBasherWithShenandoah 120000\n- *\n- * @run main\/othervm\/timeout=200 -Xlog:gc*=info,nmethod+barrier=trace -Xmx1g -XX:+UnlockDiagnosticVMOptions -XX:+UnlockExperimentalVMOptions\n- *      -XX:+UseShenandoahGC -XX:ShenandoahGCMode=iu\n- *      -XX:+DeoptimizeNMethodBarriersALot -XX:-Inline\n- *      gc.stress.gcbasher.TestGCBasherWithShenandoah 120000\n- *\/\n-\n-\n","filename":"test\/hotspot\/jtreg\/gc\/stress\/gcbasher\/TestGCBasherWithShenandoah.java","additions":0,"deletions":88,"binary":false,"changes":88,"status":"modified"},{"patch":"@@ -105,39 +105,0 @@\n-\/*\n- * @test id=iu-aggressive\n- * @key stress randomness\n- * @library \/ \/test\/lib\n- * @requires vm.gc.Shenandoah\n- * @summary Stress the GC by trying to make old objects more likely to be garbage than young objects.\n- *\n- * @run main\/othervm -Xmx384M -XX:+UnlockDiagnosticVMOptions -XX:+UnlockExperimentalVMOptions\n- *      -XX:+UseShenandoahGC -XX:ShenandoahGCMode=iu -XX:ShenandoahGCHeuristics=aggressive\n- *      -XX:+ShenandoahOOMDuringEvacALot\n- *      gc.stress.gcold.TestGCOld 50 1 20 10 10000\n- *\n- * @run main\/othervm -Xmx384M -XX:+UnlockDiagnosticVMOptions -XX:+UnlockExperimentalVMOptions\n- *      -XX:+UseShenandoahGC -XX:ShenandoahGCMode=iu -XX:ShenandoahGCHeuristics=aggressive\n- *      -XX:+ShenandoahAllocFailureALot\n- *      gc.stress.gcold.TestGCOld 50 1 20 10 10000\n- *\n- * @run main\/othervm -Xmx384M -XX:+UnlockDiagnosticVMOptions -XX:+UnlockExperimentalVMOptions\n- *      -XX:+UseShenandoahGC -XX:ShenandoahGCMode=iu -XX:ShenandoahGCHeuristics=aggressive\n- *      gc.stress.gcold.TestGCOld 50 1 20 10 10000\n- *\/\n-\n-\/*\n- * @test id=iu\n- * @key stress randomness\n- * @library \/ \/test\/lib\n- * @requires vm.gc.Shenandoah\n- * @summary Stress the GC by trying to make old objects more likely to be garbage than young objects.\n- *\n- * @run main\/othervm\/timeout=600 -Xmx384M -XX:+UnlockDiagnosticVMOptions -XX:+UnlockExperimentalVMOptions\n- *      -XX:+UseShenandoahGC -XX:ShenandoahGCMode=iu\n- *      -XX:+ShenandoahVerify\n- *      gc.stress.gcold.TestGCOld 50 1 20 10 10000\n- *\n- * @run main\/othervm -Xmx384M -XX:+UnlockDiagnosticVMOptions -XX:+UnlockExperimentalVMOptions\n- *      -XX:+UseShenandoahGC -XX:ShenandoahGCMode=iu\n- *      gc.stress.gcold.TestGCOld 50 1 20 10 10000\n- *\/\n-\n","filename":"test\/hotspot\/jtreg\/gc\/stress\/gcold\/TestGCOldWithShenandoah.java","additions":0,"deletions":39,"binary":false,"changes":39,"status":"modified"},{"patch":"@@ -43,14 +43,0 @@\n-\n-\/*\n- * @test id=iu\n- * @key stress\n- * @library \/\n- * @requires vm.gc.Shenandoah\n- * @summary Stress the Shenandoah GC full GC by allocating objects of different lifetimes concurrently with System.gc().\n- *\n- * @run main\/othervm\/timeout=300 -Xlog:gc*=info -Xmx512m -XX:+UnlockExperimentalVMOptions -XX:+UnlockDiagnosticVMOptions\n- *      -XX:+UseShenandoahGC -XX:ShenandoahGCMode=iu\n- *      -XX:+ShenandoahVerify\n- *      gc.stress.systemgc.TestSystemGCWithShenandoah 270\n- *\n- *\/\n","filename":"test\/hotspot\/jtreg\/gc\/stress\/systemgc\/TestSystemGCWithShenandoah.java","additions":0,"deletions":14,"binary":false,"changes":14,"status":"modified"},{"patch":"@@ -319,0 +319,1 @@\n+        public final boolean ShenandoahGCIsSelected;\n@@ -334,0 +335,2 @@\n+            val = testCaseBaseTargetClass.getValue(testCaseBaseTargetClass.fieldByName(\"ShenandoahGCIsSelected\"));\n+            ShenandoahGCIsSelected = ((PrimitiveValue) val).booleanValue();\n@@ -823,0 +826,1 @@\n+    public static final boolean ShenandoahGCIsSelected = GC.Shenandoah.isSelected();\n@@ -2687,1 +2691,1 @@\n-                \/\/ With ZGC the OOME is not always thrown as expected\n+                \/\/ With ZGC or Shenandoah the OOME is not always thrown as expected\n@@ -2689,0 +2693,1 @@\n+                env.targetVMOptions.ShenandoahGCIsSelected ||\n@@ -2732,1 +2737,1 @@\n-                \/\/ With ZGC the OOME is not always thrown as expected\n+                \/\/ With ZGC or Shenandoah the OOME is not always thrown as expected\n@@ -2734,0 +2739,1 @@\n+                ShenandoahGCIsSelected ||\n@@ -2785,1 +2791,1 @@\n-                \/\/ With ZGC the OOME is not always thrown as expected\n+                \/\/ With ZGC or Shenandoah the OOME is not always thrown as expected\n@@ -2787,0 +2793,1 @@\n+                env.targetVMOptions.ShenandoahGCIsSelected ||\n@@ -2846,1 +2853,1 @@\n-                \/\/ With ZGC the OOME is not always thrown as expected\n+                \/\/ With ZGC or Shenandoah the OOME is not always thrown as expected\n@@ -2848,0 +2855,1 @@\n+                ShenandoahGCIsSelected ||\n@@ -3052,1 +3060,1 @@\n-                \/\/ With ZGC the OOME is not always thrown as expected\n+                \/\/ With ZGC or Shenandoah the OOME is not always thrown as expected\n@@ -3054,0 +3062,1 @@\n+                env.targetVMOptions.ShenandoahGCIsSelected ||\n@@ -3114,1 +3123,1 @@\n-                \/\/ With ZGC the OOME is not always thrown as expected\n+                \/\/ With ZGC or Shenandoah the OOME is not always thrown as expected\n@@ -3116,0 +3125,1 @@\n+                ShenandoahGCIsSelected ||\n","filename":"test\/jdk\/com\/sun\/jdi\/EATests.java","additions":16,"deletions":6,"binary":false,"changes":22,"status":"modified"},{"patch":"@@ -61,1 +61,1 @@\n- * @test\n+ * @test id=Shenandoah\n@@ -63,3 +63,3 @@\n- * @summary Basic unit test of MemoryMXBean.getMemoryPools() and\n- *          MemoryMXBean.getMemoryManager().\n- * @requires vm.gc == \"Shenandoah\"\n+ * @summary Shenandoah has a gc mgr bean for cycles and another\n+ *          for pauses, they both have one pool.\n+ * @requires vm.gc == \"Shenandoah\" & vm.opt.ShenandoahGCMode != \"generational\"\n@@ -72,0 +72,12 @@\n+\/*\n+ * @test id=Genshen\n+ * @bug     4530538\n+ * @summary Shenandoah's generational mode has a gc mgr bean for cycles\n+ *          and another for pauses. They both reference the young and old pools.\n+ * @requires vm.gc == \"Shenandoah\" & vm.opt.ShenandoahGCMode == \"generational\"\n+ * @author  Mandy Chung\n+ *\n+ * @modules jdk.management\n+ * @run main MemoryTest 2 2\n+ *\/\n+\n","filename":"test\/jdk\/java\/lang\/management\/MemoryMXBean\/MemoryTest.java","additions":16,"deletions":4,"binary":false,"changes":20,"status":"modified"},{"patch":"@@ -0,0 +1,114 @@\n+\/*\n+ * Copyright Amazon.com Inc. All rights reserved.\n+ * DO NOT ALTER OR REMOVE COPYRIGHT NOTICES OR THIS FILE HEADER.\n+ *\n+ * This code is free software; you can redistribute it and\/or modify it\n+ * under the terms of the GNU General Public License version 2 only, as\n+ * published by the Free Software Foundation.\n+ *\n+ * This code is distributed in the hope that it will be useful, but WITHOUT\n+ * ANY WARRANTY; without even the implied warranty of MERCHANTABILITY or\n+ * FITNESS FOR A PARTICULAR PURPOSE.  See the GNU General Public License\n+ * version 2 for more details (a copy is included in the LICENSE file that\n+ * accompanied this code).\n+ *\n+ * You should have received a copy of the GNU General Public License version\n+ * 2 along with this work; if not, write to the Free Software Foundation,\n+ * Inc., 51 Franklin St, Fifth Floor, Boston, MA 02110-1301 USA.\n+ *\n+ * Please contact Oracle, 500 Oracle Parkway, Redwood Shores, CA 94065 USA\n+ * or visit www.oracle.com if you need additional information or have any\n+ * questions.\n+ *\n+ *\/\n+\n+package jdk.jfr.event.gc.detailed;\n+\n+import java.time.Duration;\n+import java.util.List;\n+import java.util.Random;\n+\n+import jdk.jfr.Recording;\n+import jdk.jfr.consumer.RecordedEvent;\n+import jdk.test.lib.Asserts;\n+import jdk.test.lib.jfr.EventNames;\n+import jdk.test.lib.jfr.Events;\n+import jdk.test.lib.jfr.GCHelper;\n+\n+\/**\n+ * @test\n+ * @bug 8221507\n+ * @requires vm.hasJFR & vm.gc.Shenandoah\n+ * @key jfr\n+ * @library \/test\/lib \/test\/jdk\n+ * @run main\/othervm -Xmx64m -XX:+UnlockExperimentalVMOptions -XX:ShenandoahRegionSize=1m -XX:+UseShenandoahGC -XX:ShenandoahGCMode=generational jdk.jfr.event.gc.detailed.TestShenandoahEvacuationInformationEvent\n+ *\/\n+\n+public class TestShenandoahEvacuationInformationEvent {\n+    private final static String EVENT_NAME = EventNames.ShenandoahEvacuationInformation;\n+\n+    public static void main(String[] args) throws Exception {\n+        final long shenandoahHeapRegionSize = 1024 * 1024;\n+        final long shenandoahMaxHeapRegionCount = 64;\n+        Recording recording = new Recording();\n+        recording.enable(EVENT_NAME).withThreshold(Duration.ofMillis(0));\n+        recording.start();\n+        allocate();\n+        recording.stop();\n+\n+        List<RecordedEvent> events = Events.fromRecording(recording);\n+        Asserts.assertFalse(events.isEmpty(), \"No events found\");\n+        for (RecordedEvent event : events) {\n+            if (!Events.isEventType(event, EVENT_NAME)) {\n+                continue;\n+            }\n+            System.out.println(\"Event: \" + event);\n+\n+            long cSetRegions = Events.assertField(event, \"cSetRegions\").atLeast(0L).getValue();\n+            long setUsedAfter = Events.assertField(event, \"cSetUsedAfter\").atLeast(0L).getValue();\n+            long setUsedBefore = Events.assertField(event, \"cSetUsedBefore\").atLeast(setUsedAfter).getValue();\n+            long freeRegions = Events.assertField(event, \"freeRegions\").atLeast(0L).getValue();\n+            Events.assertField(event, \"collectedOld\").atLeast(0L).getValue();\n+            Events.assertField(event, \"collectedYoung\").atLeast(0L).getValue();\n+\n+            Asserts.assertGreaterThanOrEqual(shenandoahMaxHeapRegionCount, freeRegions + cSetRegions, \"numRegions >= freeRegions + cSetRegions\");\n+            Asserts.assertGreaterThanOrEqual(shenandoahHeapRegionSize * cSetRegions, setUsedAfter, \"ShenandoahHeapRegionSize * cSetRegions >= setUsedAfter\");\n+            Asserts.assertGreaterThanOrEqual(shenandoahHeapRegionSize * cSetRegions, setUsedBefore, \"ShenandoahHeapRegionSize * cSetRegions >= setUsedBefore\");\n+\n+            int gcId = Events.assertField(event, \"gcId\").getValue();\n+        }\n+    }\n+\n+    \/**\n+     * Allocate memory to trigger garbage collections.\n+     * We want the allocated objects to have different life time, because we want both \"young\" and \"old\" objects.\n+     * This is done by keeping the objects in an array and step the current index by a small random number in the loop.\n+     * The loop will continue until we have allocated a fixed number of bytes.\n+     *\/\n+    private static void allocate() {\n+        DummyObject[] dummys = new DummyObject[6000];\n+\n+        Random r = new Random(0);\n+        long bytesToAllocate = 256 * 1024 * 1024;\n+        int currPos = 0;\n+        while (bytesToAllocate > 0) {\n+            int allocSize = 1000 + (r.nextInt(4000));\n+            bytesToAllocate -= allocSize;\n+            dummys[currPos] = new DummyObject(allocSize);\n+\n+            \/\/ Skip a few positions to get different duration on the objects.\n+            currPos = (currPos + r.nextInt(20)) % dummys.length;\n+        }\n+        for (int c=0; c<dummys.length; c++) {\n+            dummys[c] = null;\n+        }\n+        System.gc();\n+    }\n+\n+    public static class DummyObject {\n+        public byte[] payload;\n+        DummyObject(int size) {\n+            payload = new byte[size];\n+        }\n+    }\n+}\n","filename":"test\/jdk\/jdk\/jfr\/event\/gc\/detailed\/TestShenandoahEvacuationInformationEvent.java","additions":114,"deletions":0,"binary":false,"changes":114,"status":"added"},{"patch":"@@ -110,0 +110,1 @@\n+    public static final String ShenandoahEvacuationInformation = PREFIX + \"ShenandoahEvacuationInformation\";\n","filename":"test\/lib\/jdk\/test\/lib\/jfr\/EventNames.java","additions":1,"deletions":0,"binary":false,"changes":1,"status":"modified"}]}