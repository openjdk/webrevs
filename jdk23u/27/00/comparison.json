{"files":[{"patch":"@@ -35,9 +35,0 @@\n-\/\/ These are inline variants of Thread::SpinAcquire with optional blocking in VM.\n-\n-class ShenandoahNoBlockOp : public StackObj {\n-public:\n-  ShenandoahNoBlockOp(JavaThread* java_thread) {\n-    assert(java_thread == nullptr, \"Should not pass anything\");\n-  }\n-};\n-\n@@ -47,1 +38,1 @@\n-    contended_lock_internal<ThreadBlockInVM>(JavaThread::cast(thread));\n+    contended_lock_internal<true>(JavaThread::cast(thread));\n@@ -49,1 +40,1 @@\n-    contended_lock_internal<ShenandoahNoBlockOp>(nullptr);\n+    contended_lock_internal<false>(nullptr);\n@@ -53,1 +44,1 @@\n-template<typename BlockOp>\n+template<bool ALLOW_BLOCK>\n@@ -55,2 +46,4 @@\n-  int ctr = 0;\n-  int yields = 0;\n+  assert(!ALLOW_BLOCK || java_thread != nullptr, \"Must have a Java thread when allowing block.\");\n+  \/\/ Spin this much on multi-processor, do not spin on multi-processor.\n+  int ctr = os::is_MP() ? 0xFF : 0;\n+  \/\/ Apply TTAS to avoid more expensive CAS calls if the lock is still held by other thread.\n@@ -59,4 +52,21 @@\n-    if ((++ctr & 0xFFF) == 0) {\n-      BlockOp block(java_thread);\n-      if (yields > 5) {\n-        os::naked_short_sleep(1);\n+    if (ctr > 0 && !SafepointSynchronize::is_synchronizing()) {\n+      \/\/ Lightly contended, spin a little if no safepoint is pending.\n+      SpinPause();\n+      ctr--;\n+    } else if (ALLOW_BLOCK) {\n+      ThreadBlockInVM block(java_thread);\n+      if (SafepointSynchronize::is_synchronizing()) {\n+        \/\/ If safepoint is pending, we want to block and allow safepoint to proceed.\n+        \/\/ Normally, TBIVM above would block us in its destructor.\n+        \/\/\n+        \/\/ But that blocking only happens when TBIVM knows the thread poll is armed.\n+        \/\/ There is a window between announcing a safepoint and arming the thread poll\n+        \/\/ during which trying to continuously enter TBIVM is counter-productive.\n+        \/\/ Under high contention, we may end up going in circles thousands of times.\n+        \/\/ To avoid it, we wait here until local poll is armed and then proceed\n+        \/\/ to TBVIM exit for blocking. We do not SpinPause, but yield to let\n+        \/\/ VM thread to arm the poll sooner.\n+        while (SafepointSynchronize::is_synchronizing() &&\n+               !SafepointMechanism::local_poll_armed(java_thread)) {\n+          os::naked_yield();\n+        }\n@@ -65,1 +75,0 @@\n-        yields++;\n@@ -68,1 +77,1 @@\n-      SpinPause();\n+      os::naked_yield();\n","filename":"src\/hotspot\/share\/gc\/shenandoah\/shenandoahLock.cpp","additions":29,"deletions":20,"binary":false,"changes":49,"status":"modified"},{"patch":"@@ -40,1 +40,1 @@\n-  volatile Thread* _owner;\n+  Thread* volatile _owner;\n@@ -43,1 +43,1 @@\n-  template<typename BlockOp>\n+  template<bool ALLOW_BLOCK>\n@@ -45,1 +45,0 @@\n-\n@@ -52,2 +51,5 @@\n-    \/\/ Try to lock fast, or dive into contended lock handling.\n-    if (Atomic::cmpxchg(&_state, unlocked, locked) != unlocked) {\n+    if ((allow_block_for_safepoint && SafepointSynchronize::is_synchronizing()) ||\n+        (Atomic::cmpxchg(&_state, unlocked, locked) != unlocked)) {\n+      \/\/ 1. Java thread, and there is a pending safepoint. Dive into contended locking\n+      \/\/    immediately without trying anything else, and block.\n+      \/\/ 2. Fast lock fails, dive into contended lock handling.\n","filename":"src\/hotspot\/share\/gc\/shenandoah\/shenandoahLock.hpp","additions":7,"deletions":5,"binary":false,"changes":12,"status":"modified"}]}