{"files":[{"patch":"@@ -3228,6 +3228,0 @@\n-#ifndef PRODUCT\n-void TestReserveMemorySpecial_test() {\n-  \/\/ No tests available for this platform\n-}\n-#endif\n-\n","filename":"src\/hotspot\/os\/aix\/os_aix.cpp","additions":0,"deletions":6,"binary":false,"changes":6,"status":"modified"},{"patch":"@@ -2722,6 +2722,0 @@\n-#ifndef PRODUCT\n-void TestReserveMemorySpecial_test() {\n-  \/\/ No tests available for this platform\n-}\n-#endif\n-\n","filename":"src\/hotspot\/os\/bsd\/os_bsd.cpp","additions":0,"deletions":6,"binary":false,"changes":6,"status":"modified"},{"patch":"@@ -5465,169 +5465,0 @@\n-\n-\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/ Unit tests \/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\n-\n-#ifndef PRODUCT\n-\n-class TestReserveMemorySpecial : AllStatic {\n- public:\n-  static void small_page_write(void* addr, size_t size) {\n-    size_t page_size = os::vm_page_size();\n-\n-    char* end = (char*)addr + size;\n-    for (char* p = (char*)addr; p < end; p += page_size) {\n-      *p = 1;\n-    }\n-  }\n-\n-  static void test_reserve_memory_special_huge_tlbfs_only(size_t size) {\n-    if (!UseHugeTLBFS) {\n-      return;\n-    }\n-\n-    char* addr = os::Linux::reserve_memory_special_huge_tlbfs_only(size, NULL, false);\n-\n-    if (addr != NULL) {\n-      small_page_write(addr, size);\n-\n-      os::Linux::release_memory_special_huge_tlbfs(addr, size);\n-    }\n-  }\n-\n-  static void test_reserve_memory_special_huge_tlbfs_only() {\n-    if (!UseHugeTLBFS) {\n-      return;\n-    }\n-\n-    size_t lp = os::large_page_size();\n-\n-    for (size_t size = lp; size <= lp * 10; size += lp) {\n-      test_reserve_memory_special_huge_tlbfs_only(size);\n-    }\n-  }\n-\n-  static void test_reserve_memory_special_huge_tlbfs_mixed() {\n-    size_t lp = os::large_page_size();\n-    size_t ag = os::vm_allocation_granularity();\n-\n-    \/\/ sizes to test\n-    const size_t sizes[] = {\n-      lp, lp + ag, lp + lp \/ 2, lp * 2,\n-      lp * 2 + ag, lp * 2 - ag, lp * 2 + lp \/ 2,\n-      lp * 10, lp * 10 + lp \/ 2\n-    };\n-    const int num_sizes = sizeof(sizes) \/ sizeof(size_t);\n-\n-    \/\/ For each size\/alignment combination, we test three scenarios:\n-    \/\/ 1) with req_addr == NULL\n-    \/\/ 2) with a non-null req_addr at which we expect to successfully allocate\n-    \/\/ 3) with a non-null req_addr which contains a pre-existing mapping, at which we\n-    \/\/    expect the allocation to either fail or to ignore req_addr\n-\n-    \/\/ Pre-allocate two areas; they shall be as large as the largest allocation\n-    \/\/  and aligned to the largest alignment we will be testing.\n-    const size_t mapping_size = sizes[num_sizes - 1] * 2;\n-    char* const mapping1 = (char*) ::mmap(NULL, mapping_size,\n-      PROT_NONE, MAP_PRIVATE|MAP_ANONYMOUS|MAP_NORESERVE,\n-      -1, 0);\n-    assert(mapping1 != MAP_FAILED, \"should work\");\n-\n-    char* const mapping2 = (char*) ::mmap(NULL, mapping_size,\n-      PROT_NONE, MAP_PRIVATE|MAP_ANONYMOUS|MAP_NORESERVE,\n-      -1, 0);\n-    assert(mapping2 != MAP_FAILED, \"should work\");\n-\n-    \/\/ Unmap the first mapping, but leave the second mapping intact: the first\n-    \/\/ mapping will serve as a value for a \"good\" req_addr (case 2). The second\n-    \/\/ mapping, still intact, as \"bad\" req_addr (case 3).\n-    ::munmap(mapping1, mapping_size);\n-\n-    \/\/ Case 1\n-    for (int i = 0; i < num_sizes; i++) {\n-      const size_t size = sizes[i];\n-      for (size_t alignment = ag; is_aligned(size, alignment); alignment *= 2) {\n-        char* p = os::Linux::reserve_memory_special_huge_tlbfs_mixed(size, alignment, NULL, false);\n-        if (p != NULL) {\n-          assert(is_aligned(p, alignment), \"must be\");\n-          small_page_write(p, size);\n-          os::Linux::release_memory_special_huge_tlbfs(p, size);\n-        }\n-      }\n-    }\n-\n-    \/\/ Case 2\n-    for (int i = 0; i < num_sizes; i++) {\n-      const size_t size = sizes[i];\n-      for (size_t alignment = ag; is_aligned(size, alignment); alignment *= 2) {\n-        char* const req_addr = align_up(mapping1, alignment);\n-        char* p = os::Linux::reserve_memory_special_huge_tlbfs_mixed(size, alignment, req_addr, false);\n-        if (p != NULL) {\n-          assert(p == req_addr, \"must be\");\n-          small_page_write(p, size);\n-          os::Linux::release_memory_special_huge_tlbfs(p, size);\n-        }\n-      }\n-    }\n-\n-    \/\/ Case 3\n-    for (int i = 0; i < num_sizes; i++) {\n-      const size_t size = sizes[i];\n-      for (size_t alignment = ag; is_aligned(size, alignment); alignment *= 2) {\n-        char* const req_addr = align_up(mapping2, alignment);\n-        char* p = os::Linux::reserve_memory_special_huge_tlbfs_mixed(size, alignment, req_addr, false);\n-        \/\/ as the area around req_addr contains already existing mappings, the API should always\n-        \/\/ return NULL (as per contract, it cannot return another address)\n-        assert(p == NULL, \"must be\");\n-      }\n-    }\n-\n-    ::munmap(mapping2, mapping_size);\n-\n-  }\n-\n-  static void test_reserve_memory_special_huge_tlbfs() {\n-    if (!UseHugeTLBFS) {\n-      return;\n-    }\n-\n-    test_reserve_memory_special_huge_tlbfs_only();\n-    test_reserve_memory_special_huge_tlbfs_mixed();\n-  }\n-\n-  static void test_reserve_memory_special_shm(size_t size, size_t alignment) {\n-    if (!UseSHM) {\n-      return;\n-    }\n-\n-    char* addr = os::Linux::reserve_memory_special_shm(size, alignment, NULL, false);\n-\n-    if (addr != NULL) {\n-      assert(is_aligned(addr, alignment), \"Check\");\n-      assert(is_aligned(addr, os::large_page_size()), \"Check\");\n-\n-      small_page_write(addr, size);\n-\n-      os::Linux::release_memory_special_shm(addr, size);\n-    }\n-  }\n-\n-  static void test_reserve_memory_special_shm() {\n-    size_t lp = os::large_page_size();\n-    size_t ag = os::vm_allocation_granularity();\n-\n-    for (size_t size = ag; size < lp * 3; size += ag) {\n-      for (size_t alignment = ag; is_aligned(size, alignment); alignment *= 2) {\n-        test_reserve_memory_special_shm(size, alignment);\n-      }\n-    }\n-  }\n-\n-  static void test() {\n-    test_reserve_memory_special_huge_tlbfs();\n-    test_reserve_memory_special_shm();\n-  }\n-};\n-\n-void TestReserveMemorySpecial_test() {\n-  TestReserveMemorySpecial::test();\n-}\n-\n-#endif\n","filename":"src\/hotspot\/os\/linux\/os_linux.cpp","additions":0,"deletions":169,"binary":false,"changes":169,"status":"modified"},{"patch":"@@ -5781,52 +5781,0 @@\n-#ifndef PRODUCT\n-\n-\/\/ test the code path in reserve_memory_special() that tries to allocate memory in a single\n-\/\/ contiguous memory block at a particular address.\n-\/\/ The test first tries to find a good approximate address to allocate at by using the same\n-\/\/ method to allocate some memory at any address. The test then tries to allocate memory in\n-\/\/ the vicinity (not directly after it to avoid possible by-chance use of that location)\n-\/\/ This is of course only some dodgy assumption, there is no guarantee that the vicinity of\n-\/\/ the previously allocated memory is available for allocation. The only actual failure\n-\/\/ that is reported is when the test tries to allocate at a particular location but gets a\n-\/\/ different valid one. A NULL return value at this point is not considered an error but may\n-\/\/ be legitimate.\n-void TestReserveMemorySpecial_test() {\n-  if (!UseLargePages) {\n-    return;\n-  }\n-  \/\/ save current value of globals\n-  bool old_use_large_pages_individual_allocation = UseLargePagesIndividualAllocation;\n-  bool old_use_numa_interleaving = UseNUMAInterleaving;\n-\n-  \/\/ set globals to make sure we hit the correct code path\n-  UseLargePagesIndividualAllocation = UseNUMAInterleaving = false;\n-\n-  \/\/ do an allocation at an address selected by the OS to get a good one.\n-  const size_t large_allocation_size = os::large_page_size() * 4;\n-  char* result = os::reserve_memory_special(large_allocation_size, os::large_page_size(), NULL, false);\n-  if (result == NULL) {\n-  } else {\n-    os::release_memory_special(result, large_allocation_size);\n-\n-    \/\/ allocate another page within the recently allocated memory area which seems to be a good location. At least\n-    \/\/ we managed to get it once.\n-    const size_t expected_allocation_size = os::large_page_size();\n-    char* expected_location = result + os::large_page_size();\n-    char* actual_location = os::reserve_memory_special(expected_allocation_size, os::large_page_size(), expected_location, false);\n-    if (actual_location == NULL) {\n-    } else {\n-      \/\/ release memory\n-      os::release_memory_special(actual_location, expected_allocation_size);\n-      \/\/ only now check, after releasing any memory to avoid any leaks.\n-      assert(actual_location == expected_location,\n-             \"Failed to allocate memory at requested location \" PTR_FORMAT \" of size \" SIZE_FORMAT \", is \" PTR_FORMAT \" instead\",\n-             expected_location, expected_allocation_size, actual_location);\n-    }\n-  }\n-\n-  \/\/ restore globals\n-  UseLargePagesIndividualAllocation = old_use_large_pages_individual_allocation;\n-  UseNUMAInterleaving = old_use_numa_interleaving;\n-}\n-#endif \/\/ PRODUCT\n-\n","filename":"src\/hotspot\/os\/windows\/os_windows.cpp","additions":0,"deletions":52,"binary":false,"changes":52,"status":"modified"},{"patch":"@@ -2,1 +2,1 @@\n- * Copyright (c) 1997, 2020, Oracle and\/or its affiliates. All rights reserved.\n+ * Copyright (c) 1997, 2021, Oracle and\/or its affiliates. All rights reserved.\n@@ -1088,339 +1088,0 @@\n-\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/ Unit tests \/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\n-\n-#ifndef PRODUCT\n-\n-class TestReservedSpace : AllStatic {\n- public:\n-  static void small_page_write(void* addr, size_t size) {\n-    size_t page_size = os::vm_page_size();\n-\n-    char* end = (char*)addr + size;\n-    for (char* p = (char*)addr; p < end; p += page_size) {\n-      *p = 1;\n-    }\n-  }\n-\n-  static void release_memory_for_test(ReservedSpace rs) {\n-    if (rs.special()) {\n-      guarantee(os::release_memory_special(rs.base(), rs.size()), \"Shouldn't fail\");\n-    } else {\n-      guarantee(os::release_memory(rs.base(), rs.size()), \"Shouldn't fail\");\n-    }\n-  }\n-\n-  static void test_reserved_space1(size_t size, size_t alignment) {\n-    assert(is_aligned(size, alignment), \"Incorrect input parameters\");\n-\n-    ReservedSpace rs(size,          \/\/ size\n-                     alignment,     \/\/ alignment\n-                     UseLargePages, \/\/ large\n-                     (char *)NULL); \/\/ requested_address\n-\n-    assert(rs.base() != NULL, \"Must be\");\n-    assert(rs.size() == size, \"Must be\");\n-\n-    assert(is_aligned(rs.base(), alignment), \"aligned sizes should always give aligned addresses\");\n-    assert(is_aligned(rs.size(), alignment), \"aligned sizes should always give aligned addresses\");\n-\n-    if (rs.special()) {\n-      small_page_write(rs.base(), size);\n-    }\n-\n-    release_memory_for_test(rs);\n-  }\n-\n-  static void test_reserved_space2(size_t size) {\n-    assert(is_aligned(size, os::vm_allocation_granularity()), \"Must be at least AG aligned\");\n-\n-    ReservedSpace rs(size);\n-\n-    assert(rs.base() != NULL, \"Must be\");\n-    assert(rs.size() == size, \"Must be\");\n-\n-    if (rs.special()) {\n-      small_page_write(rs.base(), size);\n-    }\n-\n-    release_memory_for_test(rs);\n-  }\n-\n-  static void test_reserved_space3(size_t size, size_t alignment, bool maybe_large) {\n-    if (size < alignment) {\n-      \/\/ Tests might set -XX:LargePageSizeInBytes=<small pages> and cause unexpected input arguments for this test.\n-      assert((size_t)os::vm_page_size() == os::large_page_size(), \"Test needs further refinement\");\n-      return;\n-    }\n-\n-    assert(is_aligned(size, os::vm_allocation_granularity()), \"Must be at least AG aligned\");\n-    assert(is_aligned(size, alignment), \"Must be at least aligned against alignment\");\n-\n-    bool large = maybe_large && UseLargePages && size >= os::large_page_size();\n-\n-    ReservedSpace rs(size, alignment, large);\n-\n-    assert(rs.base() != NULL, \"Must be\");\n-    assert(rs.size() == size, \"Must be\");\n-\n-    if (rs.special()) {\n-      small_page_write(rs.base(), size);\n-    }\n-\n-    release_memory_for_test(rs);\n-  }\n-\n-\n-  static void test_reserved_space1() {\n-    size_t size = 2 * 1024 * 1024;\n-    size_t ag   = os::vm_allocation_granularity();\n-\n-    test_reserved_space1(size,      ag);\n-    test_reserved_space1(size * 2,  ag);\n-    test_reserved_space1(size * 10, ag);\n-  }\n-\n-  static void test_reserved_space2() {\n-    size_t size = 2 * 1024 * 1024;\n-    size_t ag = os::vm_allocation_granularity();\n-\n-    test_reserved_space2(size * 1);\n-    test_reserved_space2(size * 2);\n-    test_reserved_space2(size * 10);\n-    test_reserved_space2(ag);\n-    test_reserved_space2(size - ag);\n-    test_reserved_space2(size);\n-    test_reserved_space2(size + ag);\n-    test_reserved_space2(size * 2);\n-    test_reserved_space2(size * 2 - ag);\n-    test_reserved_space2(size * 2 + ag);\n-    test_reserved_space2(size * 3);\n-    test_reserved_space2(size * 3 - ag);\n-    test_reserved_space2(size * 3 + ag);\n-    test_reserved_space2(size * 10);\n-    test_reserved_space2(size * 10 + size \/ 2);\n-  }\n-\n-  static void test_reserved_space3() {\n-    size_t ag = os::vm_allocation_granularity();\n-\n-    test_reserved_space3(ag,      ag    , false);\n-    test_reserved_space3(ag * 2,  ag    , false);\n-    test_reserved_space3(ag * 3,  ag    , false);\n-    test_reserved_space3(ag * 2,  ag * 2, false);\n-    test_reserved_space3(ag * 4,  ag * 2, false);\n-    test_reserved_space3(ag * 8,  ag * 2, false);\n-    test_reserved_space3(ag * 4,  ag * 4, false);\n-    test_reserved_space3(ag * 8,  ag * 4, false);\n-    test_reserved_space3(ag * 16, ag * 4, false);\n-\n-    if (UseLargePages) {\n-      size_t lp = os::large_page_size();\n-\n-      \/\/ Without large pages\n-      test_reserved_space3(lp,     ag * 4, false);\n-      test_reserved_space3(lp * 2, ag * 4, false);\n-      test_reserved_space3(lp * 4, ag * 4, false);\n-      test_reserved_space3(lp,     lp    , false);\n-      test_reserved_space3(lp * 2, lp    , false);\n-      test_reserved_space3(lp * 3, lp    , false);\n-      test_reserved_space3(lp * 2, lp * 2, false);\n-      test_reserved_space3(lp * 4, lp * 2, false);\n-      test_reserved_space3(lp * 8, lp * 2, false);\n-\n-      \/\/ With large pages\n-      test_reserved_space3(lp, ag * 4    , true);\n-      test_reserved_space3(lp * 2, ag * 4, true);\n-      test_reserved_space3(lp * 4, ag * 4, true);\n-      test_reserved_space3(lp, lp        , true);\n-      test_reserved_space3(lp * 2, lp    , true);\n-      test_reserved_space3(lp * 3, lp    , true);\n-      test_reserved_space3(lp * 2, lp * 2, true);\n-      test_reserved_space3(lp * 4, lp * 2, true);\n-      test_reserved_space3(lp * 8, lp * 2, true);\n-    }\n-  }\n-\n-  static void test_reserved_space() {\n-    test_reserved_space1();\n-    test_reserved_space2();\n-    test_reserved_space3();\n-  }\n-};\n-\n-void TestReservedSpace_test() {\n-  TestReservedSpace::test_reserved_space();\n-}\n-\n-#define assert_equals(actual, expected)  \\\n-  assert(actual == expected,             \\\n-         \"Got \" SIZE_FORMAT \" expected \" \\\n-         SIZE_FORMAT, actual, expected);\n-\n-#define assert_ge(value1, value2)                  \\\n-  assert(value1 >= value2,                         \\\n-         \"'\" #value1 \"': \" SIZE_FORMAT \" '\"        \\\n-         #value2 \"': \" SIZE_FORMAT, value1, value2);\n-\n-#define assert_lt(value1, value2)                  \\\n-  assert(value1 < value2,                          \\\n-         \"'\" #value1 \"': \" SIZE_FORMAT \" '\"        \\\n-         #value2 \"': \" SIZE_FORMAT, value1, value2);\n-\n-\n-class TestVirtualSpace : AllStatic {\n-  enum TestLargePages {\n-    Default,\n-    Disable,\n-    Reserve,\n-    Commit\n-  };\n-\n-  static ReservedSpace reserve_memory(size_t reserve_size_aligned, TestLargePages mode) {\n-    switch(mode) {\n-    default:\n-    case Default:\n-    case Reserve:\n-      return ReservedSpace(reserve_size_aligned);\n-    case Disable:\n-    case Commit:\n-      return ReservedSpace(reserve_size_aligned,\n-                           os::vm_allocation_granularity(),\n-                           \/* large *\/ false);\n-    }\n-  }\n-\n-  static bool initialize_virtual_space(VirtualSpace& vs, ReservedSpace rs, TestLargePages mode) {\n-    switch(mode) {\n-    default:\n-    case Default:\n-    case Reserve:\n-      return vs.initialize(rs, 0);\n-    case Disable:\n-      return vs.initialize_with_granularity(rs, 0, os::vm_page_size());\n-    case Commit:\n-      return vs.initialize_with_granularity(rs, 0, os::page_size_for_region_unaligned(rs.size(), 1));\n-    }\n-  }\n-\n- public:\n-  static void test_virtual_space_actual_committed_space(size_t reserve_size, size_t commit_size,\n-                                                        TestLargePages mode = Default) {\n-    size_t granularity = os::vm_allocation_granularity();\n-    size_t reserve_size_aligned = align_up(reserve_size, granularity);\n-\n-    ReservedSpace reserved = reserve_memory(reserve_size_aligned, mode);\n-\n-    assert(reserved.is_reserved(), \"Must be\");\n-\n-    VirtualSpace vs;\n-    bool initialized = initialize_virtual_space(vs, reserved, mode);\n-    assert(initialized, \"Failed to initialize VirtualSpace\");\n-\n-    vs.expand_by(commit_size, false);\n-\n-    if (vs.special()) {\n-      assert_equals(vs.actual_committed_size(), reserve_size_aligned);\n-    } else {\n-      assert_ge(vs.actual_committed_size(), commit_size);\n-      \/\/ Approximate the commit granularity.\n-      \/\/ Make sure that we don't commit using large pages\n-      \/\/ if large pages has been disabled for this VirtualSpace.\n-      size_t commit_granularity = (mode == Disable || !UseLargePages) ?\n-                                   os::vm_page_size() : os::large_page_size();\n-      assert_lt(vs.actual_committed_size(), commit_size + commit_granularity);\n-    }\n-\n-    reserved.release();\n-  }\n-\n-  static void test_virtual_space_actual_committed_space_one_large_page() {\n-    if (!UseLargePages) {\n-      return;\n-    }\n-\n-    size_t large_page_size = os::large_page_size();\n-\n-    ReservedSpace reserved(large_page_size, large_page_size, true);\n-\n-    assert(reserved.is_reserved(), \"Must be\");\n-\n-    VirtualSpace vs;\n-    bool initialized = vs.initialize(reserved, 0);\n-    assert(initialized, \"Failed to initialize VirtualSpace\");\n-\n-    vs.expand_by(large_page_size, false);\n-\n-    assert_equals(vs.actual_committed_size(), large_page_size);\n-\n-    reserved.release();\n-  }\n-\n-  static void test_virtual_space_actual_committed_space() {\n-    test_virtual_space_actual_committed_space(4 * K, 0);\n-    test_virtual_space_actual_committed_space(4 * K, 4 * K);\n-    test_virtual_space_actual_committed_space(8 * K, 0);\n-    test_virtual_space_actual_committed_space(8 * K, 4 * K);\n-    test_virtual_space_actual_committed_space(8 * K, 8 * K);\n-    test_virtual_space_actual_committed_space(12 * K, 0);\n-    test_virtual_space_actual_committed_space(12 * K, 4 * K);\n-    test_virtual_space_actual_committed_space(12 * K, 8 * K);\n-    test_virtual_space_actual_committed_space(12 * K, 12 * K);\n-    test_virtual_space_actual_committed_space(64 * K, 0);\n-    test_virtual_space_actual_committed_space(64 * K, 32 * K);\n-    test_virtual_space_actual_committed_space(64 * K, 64 * K);\n-    test_virtual_space_actual_committed_space(2 * M, 0);\n-    test_virtual_space_actual_committed_space(2 * M, 4 * K);\n-    test_virtual_space_actual_committed_space(2 * M, 64 * K);\n-    test_virtual_space_actual_committed_space(2 * M, 1 * M);\n-    test_virtual_space_actual_committed_space(2 * M, 2 * M);\n-    test_virtual_space_actual_committed_space(10 * M, 0);\n-    test_virtual_space_actual_committed_space(10 * M, 4 * K);\n-    test_virtual_space_actual_committed_space(10 * M, 8 * K);\n-    test_virtual_space_actual_committed_space(10 * M, 1 * M);\n-    test_virtual_space_actual_committed_space(10 * M, 2 * M);\n-    test_virtual_space_actual_committed_space(10 * M, 5 * M);\n-    test_virtual_space_actual_committed_space(10 * M, 10 * M);\n-  }\n-\n-  static void test_virtual_space_disable_large_pages() {\n-    if (!UseLargePages) {\n-      return;\n-    }\n-    \/\/ These test cases verify that if we force VirtualSpace to disable large pages\n-    test_virtual_space_actual_committed_space(10 * M, 0, Disable);\n-    test_virtual_space_actual_committed_space(10 * M, 4 * K, Disable);\n-    test_virtual_space_actual_committed_space(10 * M, 8 * K, Disable);\n-    test_virtual_space_actual_committed_space(10 * M, 1 * M, Disable);\n-    test_virtual_space_actual_committed_space(10 * M, 2 * M, Disable);\n-    test_virtual_space_actual_committed_space(10 * M, 5 * M, Disable);\n-    test_virtual_space_actual_committed_space(10 * M, 10 * M, Disable);\n-\n-    test_virtual_space_actual_committed_space(10 * M, 0, Reserve);\n-    test_virtual_space_actual_committed_space(10 * M, 4 * K, Reserve);\n-    test_virtual_space_actual_committed_space(10 * M, 8 * K, Reserve);\n-    test_virtual_space_actual_committed_space(10 * M, 1 * M, Reserve);\n-    test_virtual_space_actual_committed_space(10 * M, 2 * M, Reserve);\n-    test_virtual_space_actual_committed_space(10 * M, 5 * M, Reserve);\n-    test_virtual_space_actual_committed_space(10 * M, 10 * M, Reserve);\n-\n-    test_virtual_space_actual_committed_space(10 * M, 0, Commit);\n-    test_virtual_space_actual_committed_space(10 * M, 4 * K, Commit);\n-    test_virtual_space_actual_committed_space(10 * M, 8 * K, Commit);\n-    test_virtual_space_actual_committed_space(10 * M, 1 * M, Commit);\n-    test_virtual_space_actual_committed_space(10 * M, 2 * M, Commit);\n-    test_virtual_space_actual_committed_space(10 * M, 5 * M, Commit);\n-    test_virtual_space_actual_committed_space(10 * M, 10 * M, Commit);\n-  }\n-\n-  static void test_virtual_space() {\n-    test_virtual_space_actual_committed_space();\n-    test_virtual_space_actual_committed_space_one_large_page();\n-    test_virtual_space_disable_large_pages();\n-  }\n-};\n-\n-void TestVirtualSpace_test() {\n-  TestVirtualSpace::test_virtual_space();\n-}\n-\n-#endif \/\/ PRODUCT\n-\n","filename":"src\/hotspot\/share\/memory\/virtualspace.cpp","additions":1,"deletions":340,"binary":false,"changes":341,"status":"modified"},{"patch":"@@ -257,15 +257,0 @@\n-#ifndef PRODUCT\n-\/\/ Forward declaration\n-void TestReservedSpace_test();\n-void TestReserveMemorySpecial_test();\n-void TestVirtualSpace_test();\n-#endif\n-\n-WB_ENTRY(void, WB_RunMemoryUnitTests(JNIEnv* env, jobject o))\n-#ifndef PRODUCT\n-  TestReservedSpace_test();\n-  TestReserveMemorySpecial_test();\n-  TestVirtualSpace_test();\n-#endif\n-WB_END\n-\n@@ -2352,1 +2337,0 @@\n-  {CC\"runMemoryUnitTests\", CC\"()V\",                   (void*)&WB_RunMemoryUnitTests},\n","filename":"src\/hotspot\/share\/prims\/whitebox.cpp","additions":0,"deletions":16,"binary":false,"changes":16,"status":"modified"},{"patch":"@@ -0,0 +1,98 @@\n+\/*\n+ * Copyright (c) 2021, Oracle and\/or its affiliates. All rights reserved.\n+ * DO NOT ALTER OR REMOVE COPYRIGHT NOTICES OR THIS FILE HEADER.\n+ *\n+ * This code is free software; you can redistribute it and\/or modify it\n+ * under the terms of the GNU General Public License version 2 only, as\n+ * published by the Free Software Foundation.\n+ *\n+ * This code is distributed in the hope that it will be useful, but WITHOUT\n+ * ANY WARRANTY; without even the implied warranty of MERCHANTABILITY or\n+ * FITNESS FOR A PARTICULAR PURPOSE.  See the GNU General Public License\n+ * version 2 for more details (a copy is included in the LICENSE file that\n+ * accompanied this code).\n+ *\n+ * You should have received a copy of the GNU General Public License version\n+ * 2 along with this work; if not, write to the Free Software Foundation,\n+ * Inc., 51 Franklin St, Fifth Floor, Boston, MA 02110-1301 USA.\n+ *\n+ * Please contact Oracle, 500 Oracle Parkway, Redwood Shores, CA 94065 USA\n+ * or visit www.oracle.com if you need additional information or have any\n+ * questions.\n+ *\/\n+\n+#ifndef GTEST_CONCURRENT_TEST_RUNNER_INLINE_HPP\n+#define GTEST_CONCURRENT_TEST_RUNNER_INLINE_HPP\n+\n+#include \"memory\/allocation.hpp\"\n+#include \"runtime\/semaphore.hpp\"\n+#include \"runtime\/thread.inline.hpp\"\n+#include \"threadHelper.inline.hpp\"\n+\n+\/\/ This file contains helper classes to run unit tests concurrently in multiple threads.\n+\n+\/\/ Base class for test runnable. Override runUnitTest() to specify what to run.\n+class TestRunnable {\n+public:\n+  virtual void runUnitTest() const = 0;\n+};\n+\n+\/\/ This class represents a thread for a unit test.\n+class UnitTestThread : public JavaTestThread {\n+public:\n+  \/\/ runnableArg - what to run\n+  \/\/ doneArg - a semaphore to notify when the thread is done running\n+  \/\/ testDurationArg - how long to run (in milliseconds)\n+  UnitTestThread(TestRunnable* const runnableArg, Semaphore* doneArg, const long testDurationArg) :\n+    JavaTestThread(doneArg), runnable(runnableArg), testDuration(testDurationArg) {}\n+\n+  \/\/ from JavaTestThread\n+  void main_run() {\n+    long stopTime = os::javaTimeMillis() + testDuration;\n+    while (os::javaTimeMillis() < stopTime) {\n+      runnable->runUnitTest();\n+    }\n+  }\n+private:\n+  TestRunnable* const runnable;\n+  const long testDuration;\n+};\n+\n+\/\/ Helper class for running a given unit test concurrently in multiple threads.\n+class ConcurrentTestRunner {\n+public:\n+  \/\/ runnableArg - what to run\n+  \/\/ nrOfThreadsArg - how many threads to use concurrently\n+  \/\/ testDurationMillisArg - duration for each test run\n+  ConcurrentTestRunner(TestRunnable* const runnableArg, int nrOfThreadsArg, long testDurationMillisArg) :\n+    unitTestRunnable(runnableArg),\n+    nrOfThreads(nrOfThreadsArg),\n+    testDurationMillis(testDurationMillisArg) {}\n+\n+  void run() {\n+    Semaphore done(0);\n+\n+    UnitTestThread** t = NEW_C_HEAP_ARRAY(UnitTestThread*, nrOfThreads, mtInternal);\n+\n+    for (int i = 0; i < nrOfThreads; i++) {\n+      t[i] = new UnitTestThread(unitTestRunnable, &done, testDurationMillis);\n+    }\n+\n+    for (int i = 0; i < nrOfThreads; i++) {\n+      t[i]->doit();\n+    }\n+\n+    for (int i = 0; i < nrOfThreads; i++) {\n+      done.wait();\n+    }\n+\n+    FREE_C_HEAP_ARRAY(UnitTestThread**, t);\n+  }\n+\n+private:\n+  TestRunnable* const unitTestRunnable;\n+  const int nrOfThreads;\n+  const long testDurationMillis;\n+};\n+\n+#endif \/\/ GTEST_CONCURRENT_TEST_RUNNER_INLINE_HPP\n","filename":"test\/hotspot\/gtest\/concurrentTestRunner.inline.hpp","additions":98,"deletions":0,"binary":false,"changes":98,"status":"added"},{"patch":"@@ -2,1 +2,1 @@\n- * Copyright (c) 2018, 2020, Oracle and\/or its affiliates. All rights reserved.\n+ * Copyright (c) 2018, 2021, Oracle and\/or its affiliates. All rights reserved.\n@@ -29,0 +29,1 @@\n+#include \"concurrentTestRunner.inline.hpp\"\n@@ -340,0 +341,340 @@\n+\n+\n+\/\/ ========================= concurrent virtual space memory tests\n+\/\/ This class have been imported from the original \"internal VM test\" with minor modification,\n+\/\/ specifically using GTest asserts instead of native HotSpot asserts.\n+class TestReservedSpace : AllStatic {\n+ public:\n+  static void small_page_write(void* addr, size_t size) {\n+    size_t page_size = os::vm_page_size();\n+\n+    char* end = (char*)addr + size;\n+    for (char* p = (char*)addr; p < end; p += page_size) {\n+      *p = 1;\n+    }\n+  }\n+\n+  static void release_memory_for_test(ReservedSpace rs) {\n+    if (rs.special()) {\n+      EXPECT_TRUE(os::release_memory_special(rs.base(), rs.size()));\n+    } else {\n+      EXPECT_TRUE(os::release_memory(rs.base(), rs.size()));\n+    }\n+  }\n+\n+  static void test_reserved_space1(size_t size, size_t alignment) {\n+    ASSERT_TRUE(is_aligned(size, alignment)) << \"Incorrect input parameters\";\n+\n+    ReservedSpace rs(size,          \/\/ size\n+                     alignment,     \/\/ alignment\n+                     UseLargePages, \/\/ large\n+                     (char *)NULL); \/\/ requested_address\n+\n+    EXPECT_TRUE(rs.base() != NULL);\n+    EXPECT_EQ(rs.size(), size) <<  \"rs.size: \" << rs.size();\n+\n+    EXPECT_TRUE(is_aligned(rs.base(), alignment)) << \"aligned sizes should always give aligned addresses\";\n+    EXPECT_TRUE(is_aligned(rs.size(), alignment)) <<  \"aligned sizes should always give aligned addresses\";\n+\n+    if (rs.special()) {\n+      small_page_write(rs.base(), size);\n+    }\n+\n+    release_memory_for_test(rs);\n+  }\n+\n+  static void test_reserved_space2(size_t size) {\n+    ASSERT_TRUE(is_aligned(size, os::vm_allocation_granularity())) << \"Must be at least AG aligned\";\n+\n+    ReservedSpace rs(size);\n+\n+    EXPECT_TRUE(rs.base() != NULL);\n+    EXPECT_EQ(rs.size(), size) <<  \"rs.size: \" << rs.size();\n+\n+    if (rs.special()) {\n+      small_page_write(rs.base(), size);\n+    }\n+\n+    release_memory_for_test(rs);\n+  }\n+\n+  static void test_reserved_space3(size_t size, size_t alignment, bool maybe_large) {\n+    if (size < alignment) {\n+      \/\/ Tests might set -XX:LargePageSizeInBytes=<small pages> and cause unexpected input arguments for this test.\n+      ASSERT_EQ((size_t)os::vm_page_size(), os::large_page_size()) << \"Test needs further refinement\";\n+      return;\n+    }\n+\n+    EXPECT_TRUE(is_aligned(size, os::vm_allocation_granularity())) << \"Must be at least AG aligned\";\n+    EXPECT_TRUE(is_aligned(size, alignment)) << \"Must be at least aligned against alignment\";\n+\n+    bool large = maybe_large && UseLargePages && size >= os::large_page_size();\n+\n+    ReservedSpace rs(size, alignment, large);\n+\n+    EXPECT_TRUE(rs.base() != NULL);\n+    EXPECT_EQ(rs.size(), size) <<  \"rs.size: \" << rs.size();\n+\n+    if (rs.special()) {\n+      small_page_write(rs.base(), size);\n+    }\n+\n+    release_memory_for_test(rs);\n+  }\n+\n+\n+  static void test_reserved_space1() {\n+    size_t size = 2 * 1024 * 1024;\n+    size_t ag   = os::vm_allocation_granularity();\n+\n+    test_reserved_space1(size,      ag);\n+    test_reserved_space1(size * 2,  ag);\n+    test_reserved_space1(size * 10, ag);\n+  }\n+\n+  static void test_reserved_space2() {\n+    size_t size = 2 * 1024 * 1024;\n+    size_t ag = os::vm_allocation_granularity();\n+\n+    test_reserved_space2(size * 1);\n+    test_reserved_space2(size * 2);\n+    test_reserved_space2(size * 10);\n+    test_reserved_space2(ag);\n+    test_reserved_space2(size - ag);\n+    test_reserved_space2(size);\n+    test_reserved_space2(size + ag);\n+    test_reserved_space2(size * 2);\n+    test_reserved_space2(size * 2 - ag);\n+    test_reserved_space2(size * 2 + ag);\n+    test_reserved_space2(size * 3);\n+    test_reserved_space2(size * 3 - ag);\n+    test_reserved_space2(size * 3 + ag);\n+    test_reserved_space2(size * 10);\n+    test_reserved_space2(size * 10 + size \/ 2);\n+  }\n+\n+  static void test_reserved_space3() {\n+    size_t ag = os::vm_allocation_granularity();\n+\n+    test_reserved_space3(ag,      ag    , false);\n+    test_reserved_space3(ag * 2,  ag    , false);\n+    test_reserved_space3(ag * 3,  ag    , false);\n+    test_reserved_space3(ag * 2,  ag * 2, false);\n+    test_reserved_space3(ag * 4,  ag * 2, false);\n+    test_reserved_space3(ag * 8,  ag * 2, false);\n+    test_reserved_space3(ag * 4,  ag * 4, false);\n+    test_reserved_space3(ag * 8,  ag * 4, false);\n+    test_reserved_space3(ag * 16, ag * 4, false);\n+\n+    if (UseLargePages) {\n+      size_t lp = os::large_page_size();\n+\n+      \/\/ Without large pages\n+      test_reserved_space3(lp,     ag * 4, false);\n+      test_reserved_space3(lp * 2, ag * 4, false);\n+      test_reserved_space3(lp * 4, ag * 4, false);\n+      test_reserved_space3(lp,     lp    , false);\n+      test_reserved_space3(lp * 2, lp    , false);\n+      test_reserved_space3(lp * 3, lp    , false);\n+      test_reserved_space3(lp * 2, lp * 2, false);\n+      test_reserved_space3(lp * 4, lp * 2, false);\n+      test_reserved_space3(lp * 8, lp * 2, false);\n+\n+      \/\/ With large pages\n+      test_reserved_space3(lp, ag * 4    , true);\n+      test_reserved_space3(lp * 2, ag * 4, true);\n+      test_reserved_space3(lp * 4, ag * 4, true);\n+      test_reserved_space3(lp, lp        , true);\n+      test_reserved_space3(lp * 2, lp    , true);\n+      test_reserved_space3(lp * 3, lp    , true);\n+      test_reserved_space3(lp * 2, lp * 2, true);\n+      test_reserved_space3(lp * 4, lp * 2, true);\n+      test_reserved_space3(lp * 8, lp * 2, true);\n+    }\n+  }\n+\n+  static void test_reserved_space() {\n+    test_reserved_space1();\n+    test_reserved_space2();\n+    test_reserved_space3();\n+  }\n+};\n+\n+\n+class TestVirtualSpace : AllStatic {\n+  enum TestLargePages {\n+    Default,\n+    Disable,\n+    Reserve,\n+    Commit\n+  };\n+\n+  static ReservedSpace reserve_memory(size_t reserve_size_aligned, TestLargePages mode) {\n+    switch(mode) {\n+    default:\n+    case Default:\n+    case Reserve:\n+      return ReservedSpace(reserve_size_aligned);\n+    case Disable:\n+    case Commit:\n+      return ReservedSpace(reserve_size_aligned,\n+                           os::vm_allocation_granularity(),\n+                           \/* large *\/ false);\n+    }\n+  }\n+\n+  static bool initialize_virtual_space(VirtualSpace& vs, ReservedSpace rs, TestLargePages mode) {\n+    switch(mode) {\n+    default:\n+    case Default:\n+    case Reserve:\n+      return vs.initialize(rs, 0);\n+    case Disable:\n+      return vs.initialize_with_granularity(rs, 0, os::vm_page_size());\n+    case Commit:\n+      return vs.initialize_with_granularity(rs, 0, os::page_size_for_region_unaligned(rs.size(), 1));\n+    }\n+  }\n+\n+ public:\n+  static void test_virtual_space_actual_committed_space(size_t reserve_size, size_t commit_size,\n+                                                        TestLargePages mode = Default) {\n+    size_t granularity = os::vm_allocation_granularity();\n+    size_t reserve_size_aligned = align_up(reserve_size, granularity);\n+\n+    ReservedSpace reserved = reserve_memory(reserve_size_aligned, mode);\n+\n+    EXPECT_TRUE(reserved.is_reserved());\n+\n+    VirtualSpace vs;\n+    bool initialized = initialize_virtual_space(vs, reserved, mode);\n+    EXPECT_TRUE(initialized) << \"Failed to initialize VirtualSpace\";\n+\n+    vs.expand_by(commit_size, false);\n+\n+    if (vs.special()) {\n+      EXPECT_EQ(vs.actual_committed_size(), reserve_size_aligned);\n+    } else {\n+      EXPECT_GE(vs.actual_committed_size(), commit_size);\n+      \/\/ Approximate the commit granularity.\n+      \/\/ Make sure that we don't commit using large pages\n+      \/\/ if large pages has been disabled for this VirtualSpace.\n+      size_t commit_granularity = (mode == Disable || !UseLargePages) ?\n+                                   os::vm_page_size() : os::large_page_size();\n+      EXPECT_LT(vs.actual_committed_size(), commit_size + commit_granularity);\n+    }\n+\n+    reserved.release();\n+  }\n+\n+  static void test_virtual_space_actual_committed_space_one_large_page() {\n+    if (!UseLargePages) {\n+      return;\n+    }\n+\n+    size_t large_page_size = os::large_page_size();\n+\n+    ReservedSpace reserved(large_page_size, large_page_size, true);\n+\n+    EXPECT_TRUE(reserved.is_reserved());\n+\n+    VirtualSpace vs;\n+    bool initialized = vs.initialize(reserved, 0);\n+    EXPECT_TRUE(initialized) << \"Failed to initialize VirtualSpace\";\n+\n+    vs.expand_by(large_page_size, false);\n+\n+    EXPECT_EQ(vs.actual_committed_size(), large_page_size);\n+\n+    reserved.release();\n+  }\n+\n+  static void test_virtual_space_actual_committed_space() {\n+    test_virtual_space_actual_committed_space(4 * K, 0);\n+    test_virtual_space_actual_committed_space(4 * K, 4 * K);\n+    test_virtual_space_actual_committed_space(8 * K, 0);\n+    test_virtual_space_actual_committed_space(8 * K, 4 * K);\n+    test_virtual_space_actual_committed_space(8 * K, 8 * K);\n+    test_virtual_space_actual_committed_space(12 * K, 0);\n+    test_virtual_space_actual_committed_space(12 * K, 4 * K);\n+    test_virtual_space_actual_committed_space(12 * K, 8 * K);\n+    test_virtual_space_actual_committed_space(12 * K, 12 * K);\n+    test_virtual_space_actual_committed_space(64 * K, 0);\n+    test_virtual_space_actual_committed_space(64 * K, 32 * K);\n+    test_virtual_space_actual_committed_space(64 * K, 64 * K);\n+    test_virtual_space_actual_committed_space(2 * M, 0);\n+    test_virtual_space_actual_committed_space(2 * M, 4 * K);\n+    test_virtual_space_actual_committed_space(2 * M, 64 * K);\n+    test_virtual_space_actual_committed_space(2 * M, 1 * M);\n+    test_virtual_space_actual_committed_space(2 * M, 2 * M);\n+    test_virtual_space_actual_committed_space(10 * M, 0);\n+    test_virtual_space_actual_committed_space(10 * M, 4 * K);\n+    test_virtual_space_actual_committed_space(10 * M, 8 * K);\n+    test_virtual_space_actual_committed_space(10 * M, 1 * M);\n+    test_virtual_space_actual_committed_space(10 * M, 2 * M);\n+    test_virtual_space_actual_committed_space(10 * M, 5 * M);\n+    test_virtual_space_actual_committed_space(10 * M, 10 * M);\n+  }\n+\n+  static void test_virtual_space_disable_large_pages() {\n+    if (!UseLargePages) {\n+      return;\n+    }\n+    \/\/ These test cases verify that if we force VirtualSpace to disable large pages\n+    test_virtual_space_actual_committed_space(10 * M, 0, Disable);\n+    test_virtual_space_actual_committed_space(10 * M, 4 * K, Disable);\n+    test_virtual_space_actual_committed_space(10 * M, 8 * K, Disable);\n+    test_virtual_space_actual_committed_space(10 * M, 1 * M, Disable);\n+    test_virtual_space_actual_committed_space(10 * M, 2 * M, Disable);\n+    test_virtual_space_actual_committed_space(10 * M, 5 * M, Disable);\n+    test_virtual_space_actual_committed_space(10 * M, 10 * M, Disable);\n+\n+    test_virtual_space_actual_committed_space(10 * M, 0, Reserve);\n+    test_virtual_space_actual_committed_space(10 * M, 4 * K, Reserve);\n+    test_virtual_space_actual_committed_space(10 * M, 8 * K, Reserve);\n+    test_virtual_space_actual_committed_space(10 * M, 1 * M, Reserve);\n+    test_virtual_space_actual_committed_space(10 * M, 2 * M, Reserve);\n+    test_virtual_space_actual_committed_space(10 * M, 5 * M, Reserve);\n+    test_virtual_space_actual_committed_space(10 * M, 10 * M, Reserve);\n+\n+    test_virtual_space_actual_committed_space(10 * M, 0, Commit);\n+    test_virtual_space_actual_committed_space(10 * M, 4 * K, Commit);\n+    test_virtual_space_actual_committed_space(10 * M, 8 * K, Commit);\n+    test_virtual_space_actual_committed_space(10 * M, 1 * M, Commit);\n+    test_virtual_space_actual_committed_space(10 * M, 2 * M, Commit);\n+    test_virtual_space_actual_committed_space(10 * M, 5 * M, Commit);\n+    test_virtual_space_actual_committed_space(10 * M, 10 * M, Commit);\n+  }\n+\n+  static void test_virtual_space() {\n+    test_virtual_space_actual_committed_space();\n+    test_virtual_space_actual_committed_space_one_large_page();\n+    test_virtual_space_disable_large_pages();\n+  }\n+};\n+\n+class ReservedSpaceRunnable : public TestRunnable {\n+public:\n+  void runUnitTest() const {\n+    TestReservedSpace::test_reserved_space();\n+  }\n+};\n+\n+TEST_VM(VirtualSpace, os_reserve_space_concurrent) {\n+  ReservedSpaceRunnable runnable;\n+  ConcurrentTestRunner testRunner(&runnable, 30, 15000);\n+  testRunner.run();\n+}\n+\n+class VirtualSpaceRunnable : public TestRunnable {\n+public:\n+  void runUnitTest() const {\n+    TestVirtualSpace::test_virtual_space();\n+  }\n+};\n+\n+TEST_VM(VirtualSpace, os_virtual_space_concurrent) {\n+  VirtualSpaceRunnable runnable;\n+  ConcurrentTestRunner testRunner(&runnable, 30, 15000);\n+  testRunner.run();\n+}\n","filename":"test\/hotspot\/gtest\/memory\/test_virtualspace.cpp","additions":342,"deletions":1,"binary":false,"changes":343,"status":"modified"},{"patch":"@@ -2,1 +2,1 @@\n- * Copyright (c) 2018, Oracle and\/or its affiliates. All rights reserved.\n+ * Copyright (c) 2018, 2021, Oracle and\/or its affiliates. All rights reserved.\n@@ -33,0 +33,1 @@\n+#include \"concurrentTestRunner.inline.hpp\"\n@@ -245,0 +246,176 @@\n+class TestReserveMemorySpecial : AllStatic {\n+ public:\n+  static void small_page_write(void* addr, size_t size) {\n+    size_t page_size = os::vm_page_size();\n+\n+    char* end = (char*)addr + size;\n+    for (char* p = (char*)addr; p < end; p += page_size) {\n+      *p = 1;\n+    }\n+  }\n+\n+  static void test_reserve_memory_special_huge_tlbfs_only(size_t size) {\n+    if (!UseHugeTLBFS) {\n+      return;\n+    }\n+\n+    char* addr = os::Linux::reserve_memory_special_huge_tlbfs_only(size, NULL, false);\n+\n+    if (addr != NULL) {\n+      small_page_write(addr, size);\n+\n+      os::Linux::release_memory_special_huge_tlbfs(addr, size);\n+    }\n+  }\n+\n+  static void test_reserve_memory_special_huge_tlbfs_only() {\n+    if (!UseHugeTLBFS) {\n+      return;\n+    }\n+\n+    size_t lp = os::large_page_size();\n+\n+    for (size_t size = lp; size <= lp * 10; size += lp) {\n+      test_reserve_memory_special_huge_tlbfs_only(size);\n+    }\n+  }\n+\n+  static void test_reserve_memory_special_huge_tlbfs_mixed() {\n+    size_t lp = os::large_page_size();\n+    size_t ag = os::vm_allocation_granularity();\n+\n+    \/\/ sizes to test\n+    const size_t sizes[] = {\n+      lp, lp + ag, lp + lp \/ 2, lp * 2,\n+      lp * 2 + ag, lp * 2 - ag, lp * 2 + lp \/ 2,\n+      lp * 10, lp * 10 + lp \/ 2\n+    };\n+    const int num_sizes = sizeof(sizes) \/ sizeof(size_t);\n+\n+    \/\/ For each size\/alignment combination, we test three scenarios:\n+    \/\/ 1) with req_addr == NULL\n+    \/\/ 2) with a non-null req_addr at which we expect to successfully allocate\n+    \/\/ 3) with a non-null req_addr which contains a pre-existing mapping, at which we\n+    \/\/    expect the allocation to either fail or to ignore req_addr\n+\n+    \/\/ Pre-allocate two areas; they shall be as large as the largest allocation\n+    \/\/  and aligned to the largest alignment we will be testing.\n+    const size_t mapping_size = sizes[num_sizes - 1] * 2;\n+    char* const mapping1 = (char*) ::mmap(NULL, mapping_size,\n+      PROT_NONE, MAP_PRIVATE|MAP_ANONYMOUS|MAP_NORESERVE,\n+      -1, 0);\n+    EXPECT_NE(mapping1, MAP_FAILED);\n+\n+    char* const mapping2 = (char*) ::mmap(NULL, mapping_size,\n+      PROT_NONE, MAP_PRIVATE|MAP_ANONYMOUS|MAP_NORESERVE,\n+      -1, 0);\n+    EXPECT_NE(mapping2, MAP_FAILED);\n+\n+    \/\/ Unmap the first mapping, but leave the second mapping intact: the first\n+    \/\/ mapping will serve as a value for a \"good\" req_addr (case 2). The second\n+    \/\/ mapping, still intact, as \"bad\" req_addr (case 3).\n+    ::munmap(mapping1, mapping_size);\n+\n+    \/\/ Case 1\n+    for (int i = 0; i < num_sizes; i++) {\n+      const size_t size = sizes[i];\n+      for (size_t alignment = ag; is_aligned(size, alignment); alignment *= 2) {\n+        char* p = os::Linux::reserve_memory_special_huge_tlbfs_mixed(size, alignment, NULL, false);\n+        if (p != NULL) {\n+          EXPECT_TRUE(is_aligned(p, alignment));\n+          small_page_write(p, size);\n+          os::Linux::release_memory_special_huge_tlbfs(p, size);\n+        }\n+      }\n+    }\n+\n+    \/\/ Case 2\n+    for (int i = 0; i < num_sizes; i++) {\n+      const size_t size = sizes[i];\n+      for (size_t alignment = ag; is_aligned(size, alignment); alignment *= 2) {\n+        char* const req_addr = align_up(mapping1, alignment);\n+        char* p = os::Linux::reserve_memory_special_huge_tlbfs_mixed(size, alignment, req_addr, false);\n+        if (p != NULL) {\n+          EXPECT_EQ(p, req_addr);\n+          small_page_write(p, size);\n+          os::Linux::release_memory_special_huge_tlbfs(p, size);\n+        }\n+      }\n+    }\n+\n+    \/\/ Case 3\n+    for (int i = 0; i < num_sizes; i++) {\n+      const size_t size = sizes[i];\n+      for (size_t alignment = ag; is_aligned(size, alignment); alignment *= 2) {\n+        char* const req_addr = align_up(mapping2, alignment);\n+        char* p = os::Linux::reserve_memory_special_huge_tlbfs_mixed(size, alignment, req_addr, false);\n+        \/\/ as the area around req_addr contains already existing mappings, the API should always\n+        \/\/ return NULL (as per contract, it cannot return another address)\n+        EXPECT_TRUE(p == NULL);\n+      }\n+    }\n+\n+    ::munmap(mapping2, mapping_size);\n+\n+  }\n+\n+  static void test_reserve_memory_special_huge_tlbfs() {\n+    if (!UseHugeTLBFS) {\n+      return;\n+    }\n+\n+    test_reserve_memory_special_huge_tlbfs_only();\n+    test_reserve_memory_special_huge_tlbfs_mixed();\n+  }\n+\n+  static void test_reserve_memory_special_shm(size_t size, size_t alignment) {\n+    if (!UseSHM) {\n+      return;\n+    }\n+\n+    char* addr = os::Linux::reserve_memory_special_shm(size, alignment, NULL, false);\n+\n+    if (addr != NULL) {\n+      EXPECT_TRUE(is_aligned(addr, alignment));\n+      EXPECT_TRUE(is_aligned(addr, os::large_page_size()));\n+\n+      small_page_write(addr, size);\n+\n+      os::Linux::release_memory_special_shm(addr, size);\n+    }\n+  }\n+\n+  static void test_reserve_memory_special_shm() {\n+    size_t lp = os::large_page_size();\n+    size_t ag = os::vm_allocation_granularity();\n+\n+    for (size_t size = ag; size < lp * 3; size += ag) {\n+      for (size_t alignment = ag; is_aligned(size, alignment); alignment *= 2) {\n+        test_reserve_memory_special_shm(size, alignment);\n+      }\n+    }\n+  }\n+\n+  static void test() {\n+    test_reserve_memory_special_huge_tlbfs();\n+    test_reserve_memory_special_shm();\n+  }\n+};\n+\n+TEST_VM(os_linux, reserve_memory_special) {\n+  TestReserveMemorySpecial::test();\n+}\n+\n+class ReserveMemorySpecialRunnable : public TestRunnable {\n+public:\n+  void runUnitTest() const {\n+    TestReserveMemorySpecial::test();\n+  }\n+};\n+\n+TEST_VM(os_linux, reserve_memory_special_concurrent) {\n+  ReserveMemorySpecialRunnable runnable;\n+  ConcurrentTestRunner testRunner(&runnable, 30, 15000);\n+  testRunner.run();\n+}\n+\n","filename":"test\/hotspot\/gtest\/runtime\/test_os_linux.cpp","additions":178,"deletions":1,"binary":false,"changes":179,"status":"modified"},{"patch":"@@ -2,1 +2,1 @@\n- * Copyright (c) 2018, 2020, Oracle and\/or its affiliates. All rights reserved.\n+ * Copyright (c) 2018, 2021, Oracle and\/or its affiliates. All rights reserved.\n@@ -31,0 +31,1 @@\n+#include \"concurrentTestRunner.inline.hpp\"\n@@ -54,1 +55,1 @@\n-TEST_VM(os_windows, reserve_memory_special) {\n+void TestReserveMemorySpecial_test() {\n@@ -691,0 +692,17 @@\n+TEST_VM(os_windows, reserve_memory_special) {\n+  TestReserveMemorySpecial_test();\n+}\n+\n+class ReserveMemorySpecialRunnable : public TestRunnable {\n+public:\n+  void runUnitTest() const {\n+    TestReserveMemorySpecial_test();\n+  }\n+};\n+\n+TEST_VM(os_windows, reserve_memory_special_concurrent) {\n+  ReserveMemorySpecialRunnable runnable;\n+  ConcurrentTestRunner testRunner(&runnable, 30, 15000);\n+  testRunner.run();\n+}\n+\n","filename":"test\/hotspot\/gtest\/runtime\/test_os_windows.cpp","additions":20,"deletions":2,"binary":false,"changes":22,"status":"modified"},{"patch":"@@ -284,1 +284,0 @@\n- -runtime\/memory\/RunUnitTestsConcurrently.java \\\n","filename":"test\/hotspot\/jtreg\/TEST.groups","additions":0,"deletions":1,"binary":false,"changes":1,"status":"modified"},{"patch":"@@ -1,74 +0,0 @@\n-\/*\n- * Copyright (c) 2013, 2020, Oracle and\/or its affiliates. All rights reserved.\n- * DO NOT ALTER OR REMOVE COPYRIGHT NOTICES OR THIS FILE HEADER.\n- *\n- * This code is free software; you can redistribute it and\/or modify it\n- * under the terms of the GNU General Public License version 2 only, as\n- * published by the Free Software Foundation.\n- *\n- * This code is distributed in the hope that it will be useful, but WITHOUT\n- * ANY WARRANTY; without even the implied warranty of MERCHANTABILITY or\n- * FITNESS FOR A PARTICULAR PURPOSE.  See the GNU General Public License\n- * version 2 for more details (a copy is included in the LICENSE file that\n- * accompanied this code).\n- *\n- * You should have received a copy of the GNU General Public License version\n- * 2 along with this work; if not, write to the Free Software Foundation,\n- * Inc., 51 Franklin St, Fifth Floor, Boston, MA 02110-1301 USA.\n- *\n- * Please contact Oracle, 500 Oracle Parkway, Redwood Shores, CA 94065 USA\n- * or visit www.oracle.com if you need additional information or have any\n- * questions.\n- *\/\n-\n-\/*\n- * @test\n- * @summary Test launches unit tests inside vm concurrently\n- * @requires vm.debug\n- * @requires vm.bits == 64\n- * @library \/test\/lib\n- * @modules java.base\/jdk.internal.misc\n- *          java.management\n- * @build sun.hotspot.WhiteBox\n- * @run driver ClassFileInstaller sun.hotspot.WhiteBox\n- * @run main\/othervm -Xbootclasspath\/a:. -XX:+UnlockDiagnosticVMOptions -XX:+WhiteBoxAPI RunUnitTestsConcurrently 30 15000\n- *\/\n-\n-import sun.hotspot.WhiteBox;\n-\n-public class RunUnitTestsConcurrently {\n-\n-  private static WhiteBox wb;\n-  private static long timeout;\n-  private static long timeStamp;\n-\n-  public static class Worker implements Runnable {\n-    @Override\n-    public void run() {\n-      while (System.currentTimeMillis() - timeStamp < timeout) {\n-        wb.runMemoryUnitTests();\n-      }\n-    }\n-  }\n-\n-  public static void main(String[] args) throws InterruptedException {\n-    wb = WhiteBox.getWhiteBox();\n-    System.out.println(\"Starting threads\");\n-\n-    int threads = Integer.valueOf(args[0]);\n-    timeout = Long.valueOf(args[1]);\n-\n-    timeStamp = System.currentTimeMillis();\n-\n-    Thread[] threadsArray = new Thread[threads];\n-    for (int i = 0; i < threads; i++) {\n-      threadsArray[i] = new Thread(new Worker());\n-      threadsArray[i].start();\n-    }\n-    for (int i = 0; i < threads; i++) {\n-      threadsArray[i].join();\n-    }\n-\n-    System.out.println(\"Quitting test.\");\n-  }\n-}\n","filename":"test\/hotspot\/jtreg\/runtime\/memory\/RunUnitTestsConcurrently.java","additions":0,"deletions":74,"binary":false,"changes":74,"status":"deleted"},{"patch":"@@ -512,1 +512,0 @@\n-  public native void runMemoryUnitTests();\n","filename":"test\/lib\/sun\/hotspot\/WhiteBox.java","additions":0,"deletions":1,"binary":false,"changes":1,"status":"modified"}]}