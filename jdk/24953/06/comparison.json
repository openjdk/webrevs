{"files":[{"patch":"@@ -4207,0 +4207,2 @@\n+  generate_kyber_stubs();\n+\n","filename":"src\/hotspot\/cpu\/x86\/stubGenerator_x86_64.cpp","additions":2,"deletions":0,"binary":false,"changes":2,"status":"modified"},{"patch":"@@ -493,1 +493,4 @@\n-  \/\/ Dilithium stubs and helper functions\n+  \/\/ Kyber stubs\n+  void generate_kyber_stubs();\n+\n+  \/\/ Dilithium stubs\n@@ -495,1 +498,0 @@\n-  \/\/ BASE64 stubs\n@@ -497,0 +499,1 @@\n+  \/\/ BASE64 stubs\n","filename":"src\/hotspot\/cpu\/x86\/stubGenerator_x86_64.hpp","additions":5,"deletions":2,"binary":false,"changes":7,"status":"modified"},{"patch":"@@ -0,0 +1,952 @@\n+\/*\n+ * Copyright (c) 2025, Oracle and\/or its affiliates. All rights reserved.\n+ * DO NOT ALTER OR REMOVE COPYRIGHT NOTICES OR THIS FILE HEADER.\n+ *\n+ * This code is free software; you can redistribute it and\/or modify it\n+ * under the terms of the GNU General Public License version 2 only, as\n+ * published by the Free Software Foundation.\n+ *\n+ * This code is distributed in the hope that it will be useful, but WITHOUT\n+ * ANY WARRANTY; without even the implied warranty of MERCHANTABILITY or\n+ * FITNESS FOR A PARTICULAR PURPOSE.  See the GNU General Public License\n+ * version 2 for more details (a copy is included in the LICENSE file that\n+ * accompanied this code).\n+ *\n+ * You should have received a copy of the GNU General Public License version\n+ * 2 along with this work; if not, write to the Free Software Foundation,\n+ * Inc., 51 Franklin St, Fifth Floor, Boston, MA 02110-1301 USA.\n+ *\n+ * Please contact Oracle, 500 Oracle Parkway, Redwood Shores, CA 94065 USA\n+ * or visit www.oracle.com if you need additional information or have any\n+ * questions.\n+ *\n+ *\/\n+\n+#include \"asm\/assembler.hpp\"\n+#include \"asm\/assembler.inline.hpp\"\n+#include \"runtime\/stubRoutines.hpp\"\n+#include \"macroAssembler_x86.hpp\"\n+#include \"stubGenerator_x86_64.hpp\"\n+\n+#define __ _masm->\n+\n+#define xmm(i) as_XMMRegister(i)\n+\n+#ifdef PRODUCT\n+#define BLOCK_COMMENT(str) \/* nothing *\/\n+#else\n+#define BLOCK_COMMENT(str) __ block_comment(str)\n+#endif \/\/ PRODUCT\n+\n+#define BIND(label) bind(label); BLOCK_COMMENT(#label \":\")\n+\n+\/\/ Constants\n+\/\/\n+ATTRIBUTE_ALIGNED(64) static const uint16_t kyberAvx512Consts[] = {\n+    0xF301, 0xF301, 0xF301, 0xF301, \/\/ q^-1 mod montR\n+    0x0D01, 0x0D01, 0x0D01, 0x0D01, \/\/ q\n+    0x4EBF, 0x4EBF, 0x4EBF, 0x4EBF, \/\/ Barrett multiplier\n+    0x0200, 0x0200, 0x0200, 0x0200, \/\/(dim\/2)^-1 mod q\n+    0x0549, 0x0549, 0x0549, 0x0549, \/\/ montR^2 mod q\n+    0x0F00, 0x0F00, 0x0F00, 0x0F00  \/\/ mask for kyber12to16\n+  };\n+\n+static int qInvModROffset = 0;\n+static int qOffset = 8;\n+static int barretMultiplierOffset = 16;\n+static int dimHalfInverseOffset = 24;\n+static int montRSquareModqOffset = 32;\n+static int f00Offset = 40;\n+\n+static address kyberAvx512ConstsAddr(int offset) {\n+  return ((address) kyberAvx512Consts) + offset;\n+}\n+\n+const Register scratch = r10;\n+\n+ATTRIBUTE_ALIGNED(64) static const uint16_t kyberAvx512NttPerms[] = {\n+\/\/ 0\n+    0x10, 0x11, 0x12, 0x13, 0x14, 0x15, 0x16, 0x17,\n+    0x18, 0x19, 0x1A, 0x1B, 0x1C, 0x1D, 0x1E, 0x1F,\n+    0x30, 0x31, 0x32, 0x33, 0x34, 0x35, 0x36, 0x37,\n+    0x38, 0x39, 0x3A, 0x3B, 0x3C, 0x3D, 0x3E, 0x3F,\n+    0x00, 0x01, 0x02, 0x03, 0x04, 0x05, 0x06, 0x07,\n+    0x08, 0x09, 0x0A, 0x0B, 0x0C, 0x0D, 0x0E, 0x0F,\n+    0x20, 0x21, 0x22, 0x23, 0x24, 0x25, 0x26, 0x27,\n+    0x28, 0x29, 0x2A, 0x2B, 0x2C, 0x2D, 0x2E, 0x2F,\n+\/\/ 128\n+    0x00, 0x01, 0x02, 0x03, 0x04, 0x05, 0x06, 0x07,\n+    0x20, 0x21, 0x22, 0x23, 0x24, 0x25, 0x26, 0x27,\n+    0x10, 0x11, 0x12, 0x13, 0x14, 0x15, 0x16, 0x17,\n+    0x30, 0x31, 0x32, 0x33, 0x34, 0x35, 0x36, 0x37,\n+    0x08, 0x09, 0x0A, 0x0B, 0x0C, 0x0D, 0x0E, 0x0F,\n+    0x28, 0x29, 0x2A, 0x2B, 0x2C, 0x2D, 0x2E, 0x2F,\n+    0x18, 0x19, 0x1A, 0x1B, 0x1C, 0x1D, 0x1E, 0x1F,\n+    0x38, 0x39, 0x3A, 0x3B, 0x3C, 0x3D, 0x3E, 0x3F,\n+\/\/ 256\n+    0x00, 0x01, 0x02, 0x03, 0x20, 0x21, 0x22, 0x23,\n+    0x08, 0x09, 0x0A, 0x0B, 0x28, 0x29, 0x2A, 0x2B,\n+    0x10, 0x11, 0x12, 0x13, 0x30, 0x31, 0x32, 0x33,\n+    0x18, 0x19, 0x1A, 0x1B, 0x38, 0x39, 0x3A, 0x3B,\n+    0x04, 0x05, 0x06, 0x07, 0x24, 0x25, 0x26, 0x27,\n+    0x0C, 0x0D, 0x0E, 0x0F, 0x2C, 0x2D, 0x2E, 0x2F,\n+    0x14, 0x15, 0x16, 0x17, 0x34, 0x35, 0x36, 0x37,\n+    0x1C, 0x1D, 0x1E, 0x1F, 0x3C, 0x3D, 0x3E, 0x3F,\n+\/\/ 384\n+    0x00, 0x01, 0x20, 0x21, 0x04, 0x05, 0x24, 0x25,\n+    0x08, 0x09, 0x28, 0x29, 0x0C, 0x0D, 0x2C, 0x2D,\n+    0x10, 0x11, 0x30, 0x31, 0x14, 0x15, 0x34, 0x35,\n+    0x18, 0x19, 0x38, 0x39, 0x1C, 0x1D, 0x3C, 0x3D,\n+    0x02, 0x03, 0x22, 0x23, 0x06, 0x07, 0x26, 0x27,\n+    0x0A, 0x0B, 0x2A, 0x2B, 0x0E, 0x0F, 0x2E, 0x2F,\n+    0x12, 0x13, 0x32, 0x33, 0x16, 0x17, 0x36, 0x37,\n+    0x1A, 0x1B, 0x3A, 0x3B, 0x1E, 0x1F, 0x3E, 0x3F,\n+\/\/ 512\n+    0x10, 0x11, 0x30, 0x31, 0x12, 0x13, 0x32, 0x33,\n+    0x14, 0x15, 0x34, 0x35, 0x16, 0x17, 0x36, 0x37,\n+    0x18, 0x19, 0x38, 0x39, 0x1A, 0x1B, 0x3A, 0x3B,\n+    0x1C, 0x1D, 0x3C, 0x3D, 0x1E, 0x1F, 0x3E, 0x3F,\n+    0x00, 0x01, 0x20, 0x21, 0x02, 0x03, 0x22, 0x23,\n+    0x04, 0x05, 0x24, 0x25, 0x06, 0x07, 0x26, 0x27,\n+    0x08, 0x09, 0x28, 0x29, 0x0A, 0x0B, 0x2A, 0x2B,\n+    0x0C, 0x0D, 0x2C, 0x2D, 0x0E, 0x0F, 0x2E, 0x2F\n+  };\n+\n+static address kyberAvx512NttPermsAddr() {\n+  return (address) kyberAvx512NttPerms;\n+}\n+\n+ATTRIBUTE_ALIGNED(64) static const uint16_t kyberAvx512InverseNttPerms[] = {\n+\/\/ 0\n+    0x02, 0x03, 0x06, 0x07, 0x0A, 0x0B, 0x0E, 0x0F,\n+    0x12, 0x13, 0x16, 0x17, 0x1A, 0x1B, 0x1E, 0x1F,\n+    0x22, 0x23, 0x26, 0x27, 0x2A, 0x2B, 0x2E, 0x2F,\n+    0x32, 0x33, 0x36, 0x37, 0x3A, 0x3B, 0x3E, 0x3F,\n+    0x00, 0x01, 0x04, 0x05, 0x08, 0x09, 0x0C, 0x0D,\n+    0x10, 0x11, 0x14, 0x15, 0x18, 0x19, 0x1C, 0x1D,\n+    0x20, 0x21, 0x24, 0x25, 0x28, 0x29, 0x2C, 0x2D,\n+    0x30, 0x31, 0x34, 0x35, 0x38, 0x39, 0x3C, 0x3D,\n+\/\/ 128\n+    0x00, 0x01, 0x20, 0x21, 0x04, 0x05, 0x24, 0x25,\n+    0x08, 0x09, 0x28, 0x29, 0x0C, 0x0D, 0x2C, 0x2D,\n+    0x10, 0x11, 0x30, 0x31, 0x14, 0x15, 0x34, 0x35,\n+    0x18, 0x19, 0x38, 0x39, 0x1C, 0x1D, 0x3C, 0x3D,\n+    0x02, 0x03, 0x22, 0x23, 0x06, 0x07, 0x26, 0x27,\n+    0x0A, 0x0B, 0x2A, 0x2B, 0x0E, 0x0F, 0x2E, 0x2F,\n+    0x12, 0x13, 0x32, 0x33, 0x16, 0x17, 0x36, 0x37,\n+    0x1A, 0x1B, 0x3A, 0x3B, 0x1E, 0x1F, 0x3E, 0x3F,\n+\/\/ 256\n+    0x00, 0x01, 0x02, 0x03, 0x20, 0x21, 0x22, 0x23,\n+    0x08, 0x09, 0x0A, 0x0B, 0x28, 0x29, 0x2A, 0x2B,\n+    0x10, 0x11, 0x12, 0x13, 0x30, 0x31, 0x32, 0x33,\n+    0x18, 0x19, 0x1A, 0x1B, 0x38, 0x39, 0x3A, 0x3B,\n+    0x04, 0x05, 0x06, 0x07, 0x24, 0x25, 0x26, 0x27,\n+    0x0C, 0x0D, 0x0E, 0x0F, 0x2C, 0x2D, 0x2E, 0x2F,\n+    0x14, 0x15, 0x16, 0x17, 0x34, 0x35, 0x36, 0x37,\n+    0x1C, 0x1D, 0x1E, 0x1F, 0x3C, 0x3D, 0x3E, 0x3F,\n+\/\/ 384\n+    0x00, 0x01, 0x02, 0x03, 0x04, 0x05, 0x06, 0x07,\n+    0x20, 0x21, 0x22, 0x23, 0x24, 0x25, 0x26, 0x27,\n+    0x10, 0x11, 0x12, 0x13, 0x14, 0x15, 0x16, 0x17,\n+    0x30, 0x31, 0x32, 0x33, 0x34, 0x35, 0x36, 0x37,\n+    0x08, 0x09, 0x0A, 0x0B, 0x0C, 0x0D, 0x0E, 0x0F,\n+    0x28, 0x29, 0x2A, 0x2B, 0x2C, 0x2D, 0x2E, 0x2F,\n+    0x18, 0x19, 0x1A, 0x1B, 0x1C, 0x1D, 0x1E, 0x1F,\n+    0x38, 0x39, 0x3A, 0x3B, 0x3C, 0x3D, 0x3E, 0x3F,\n+\/\/ 512\n+    0x10, 0x11, 0x12, 0x13, 0x14, 0x15, 0x16, 0x17,\n+    0x18, 0x19, 0x1A, 0x1B, 0x1C, 0x1D, 0x1E, 0x1F,\n+    0x30, 0x31, 0x32, 0x33, 0x34, 0x35, 0x36, 0x37,\n+    0x38, 0x39, 0x3A, 0x3B, 0x3C, 0x3D, 0x3E, 0x3F,\n+    0x00, 0x01, 0x02, 0x03, 0x04, 0x05, 0x06, 0x07,\n+    0x08, 0x09, 0x0A, 0x0B, 0x0C, 0x0D, 0x0E, 0x0F,\n+    0x20, 0x21, 0x22, 0x23, 0x24, 0x25, 0x26, 0x27,\n+    0x28, 0x29, 0x2A, 0x2B, 0x2C, 0x2D, 0x2E, 0x2F\n+  };\n+\n+static address kyberAvx512InverseNttPermsAddr() {\n+  return (address) kyberAvx512InverseNttPerms;\n+}\n+\n+ATTRIBUTE_ALIGNED(64) static const uint16_t kyberAvx512_nttMultPerms[] = {\n+    0x00, 0x02, 0x04, 0x06, 0x08, 0x0A, 0x0C, 0x0E,\n+    0x10, 0x12, 0x14, 0x16, 0x18, 0x1A, 0x1C, 0x1E,\n+    0x20, 0x22, 0x24, 0x26, 0x28, 0x2A, 0x2C, 0x2E,\n+    0x30, 0x32, 0x34, 0x36, 0x38, 0x3A, 0x3C, 0x3E,\n+\n+    0x01, 0x03, 0x05, 0x07, 0x09, 0x0B, 0x0D, 0x0F,\n+    0x11, 0x13, 0x15, 0x17, 0x19, 0x1B, 0x1D, 0x1F,\n+    0x21, 0x23, 0x25, 0x27, 0x29, 0x2B, 0x2D, 0x2F,\n+    0x31, 0x33, 0x35, 0x37, 0x39, 0x3B, 0x3D, 0x3F,\n+\n+    0x00, 0x20, 0x01, 0x21, 0x02, 0x22, 0x03, 0x23,\n+    0x04, 0x24, 0x05, 0x25, 0x06, 0x26, 0x07, 0x27,\n+    0x08, 0x28, 0x09, 0x29, 0x0A, 0x2A, 0x0B, 0x2B,\n+    0x0C, 0x2C, 0x0D, 0x2D, 0x0E, 0x2E, 0x0F, 0x2F,\n+\n+    0x10, 0x30, 0x11, 0x31, 0x12, 0x32, 0x13, 0x33,\n+    0x14, 0x34, 0x15, 0x35, 0x16, 0x36, 0x17, 0x37,\n+    0x18, 0x38, 0x19, 0x39, 0x1A, 0x3A, 0x1B, 0x3B,\n+    0x1C, 0x3C, 0x1D, 0x3D, 0x1E, 0x3E, 0x1F, 0x3F\n+  };\n+\n+static address kyberAvx512_nttMultPermsAddr() {\n+  return (address) kyberAvx512_nttMultPerms;\n+}\n+\n+  ATTRIBUTE_ALIGNED(64) static const uint16_t kyberAvx512_12To16Perms[] = {\n+\/\/ 0\n+    0x00, 0x03, 0x06, 0x09, 0x0C, 0x0F, 0x12, 0x15,\n+    0x18, 0x1B, 0x1E, 0x21, 0x24, 0x27, 0x2A, 0x2D,\n+    0x30, 0x33, 0x36, 0x39, 0x3C, 0x3F, 0x00, 0x00,\n+    0x00, 0x00, 0x00, 0x00, 0x00, 0x00, 0x00, 0x00,\n+    0x01, 0x04, 0x07, 0x0A, 0x0D, 0x10, 0x13, 0x16,\n+    0x19, 0x1C, 0x1F, 0x22, 0x25, 0x28, 0x2B, 0x2E,\n+    0x31, 0x34, 0x37, 0x3A, 0x3D, 0x00, 0x00, 0x00,\n+    0x00, 0x00, 0x00, 0x00, 0x00, 0x00, 0x00, 0x00,\n+\/\/ 128\n+    0x02, 0x05, 0x08, 0x0B, 0x0E, 0x11, 0x14, 0x17,\n+    0x1A, 0x1D, 0x20, 0x23, 0x26, 0x29, 0x2C, 0x2F,\n+    0x32, 0x35, 0x38, 0x3B, 0x3E, 0x00, 0x00, 0x00,\n+    0x00, 0x00, 0x00, 0x00, 0x00, 0x00, 0x00, 0x00,\n+    0x00, 0x01, 0x02, 0x03, 0x04, 0x05, 0x06, 0x07,\n+    0x08, 0x09, 0x0A, 0x0B, 0x0C, 0x0D, 0x0E, 0x0F,\n+    0x10, 0x11, 0x12, 0x13, 0x14, 0x15, 0x22, 0x25,\n+    0x28, 0x2B, 0x2E, 0x31, 0x34, 0x37, 0x3A, 0x3D,\n+\/\/ 256\n+    0x00, 0x01, 0x02, 0x03, 0x04, 0x05, 0x06, 0x07,\n+    0x08, 0x09, 0x0A, 0x0B, 0x0C, 0x0D, 0x0E, 0x0F,\n+    0x10, 0x11, 0x12, 0x13, 0x14, 0x20, 0x23, 0x26,\n+    0x29, 0x2C, 0x2F, 0x32, 0x35, 0x38, 0x3B, 0x3E,\n+    0x00, 0x01, 0x02, 0x03, 0x04, 0x05, 0x06, 0x07,\n+    0x08, 0x09, 0x0A, 0x0B, 0x0C, 0x0D, 0x0E, 0x0F,\n+    0x10, 0x11, 0x12, 0x13, 0x14, 0x21, 0x24, 0x27,\n+    0x2A, 0x2D, 0x30, 0x33, 0x36, 0x39, 0x3C, 0x3F,\n+\/\/ 384\n+    0x00, 0x20, 0x01, 0x21, 0x02, 0x22, 0x03, 0x23,\n+    0x04, 0x24, 0x05, 0x25, 0x06, 0x26, 0x07, 0x27,\n+    0x08, 0x28, 0x09, 0x29, 0x0A, 0x2A, 0x0B, 0x2B,\n+    0x0C, 0x2C, 0x0D, 0x2D, 0x0E, 0x2E, 0x0F, 0x2F,\n+    0x10, 0x30, 0x11, 0x31, 0x12, 0x32, 0x13, 0x33,\n+    0x14, 0x34, 0x15, 0x35, 0x16, 0x36, 0x17, 0x37,\n+    0x18, 0x38, 0x19, 0x39, 0x1A, 0x3A, 0x1B, 0x3B,\n+    0x1C, 0x3C, 0x1D, 0x3D, 0x1E, 0x3E, 0x1F, 0x3F\n+  };\n+\n+static address kyberAvx512_12To16PermsAddr() {\n+  return (address) kyberAvx512_12To16Perms;\n+}\n+\n+static void load4regs(int destRegs[], Register address, int offset,\n+                      MacroAssembler *_masm) {\n+  for (int i = 0; i < 4; i++) {\n+    __ evmovdquw(xmm(destRegs[i]), Address(address, offset + i * 64),\n+                 Assembler::AVX_512bit);\n+  }\n+}\n+\n+\/\/ For z = montmul(a,b), z will be  between -q and q and congruent\n+\/\/ to a * b * R^-1 mod q, where R > 2 * q, R is a power of 2,\n+\/\/ -R\/2 * q <= a * b < R\/2 * q.\n+\/\/ (See e.g. Algorithm 3 in https:\/\/eprint.iacr.org\/2018\/039.pdf)\n+\/\/ For the Java code, we use R = 2^20 and for the intrinsic, R = 2^16.\n+\/\/ In our computations, b is always c * R mod q, so the montmul() really\n+\/\/ computes a * c mod q. In the Java code, we use 32-bit numbers for the\n+\/\/ computations, and we use R = 2^20 because that way the a * b numbers\n+\/\/ that occur during all computations stay in the required range.\n+\/\/ For the intrinsics, we use R = 2^16, because this way we can do twice\n+\/\/ as much work in parallel, the only drawback is that we should do some Barrett\n+\/\/ reductions in kyberInverseNtt so that the numbers stay in the required range.\n+static void montmul(int outputRegs[], int inputRegs1[], int inputRegs2[],\n+             int scratchRegs1[], int scratchRegs2[], MacroAssembler *_masm) {\n+   for (int i = 0; i < 4; i++) {\n+     __ evpmullw(xmm(scratchRegs1[i]), k0, xmm(inputRegs1[i]),\n+                 xmm(inputRegs2[i]), false, Assembler::AVX_512bit);\n+   }\n+   for (int i = 0; i < 4; i++) {\n+     __ evpmulhw(xmm(scratchRegs2[i]), k0, xmm(inputRegs1[i]),\n+                 xmm(inputRegs2[i]), false, Assembler::AVX_512bit);\n+   }\n+   for (int i = 0; i < 4; i++) {\n+     __ evpmullw(xmm(scratchRegs1[i]), k0, xmm(scratchRegs1[i]),\n+                 xmm31, false, Assembler::AVX_512bit);\n+   }\n+   for (int i = 0; i < 4; i++) {\n+     __ evpmulhw(xmm(scratchRegs1[i]), k0, xmm(scratchRegs1[i]),\n+                 xmm30, false, Assembler::AVX_512bit);\n+   }\n+   for (int i = 0; i < 4; i++) {\n+     __ evpsubw(xmm(outputRegs[i]), k0, xmm(scratchRegs2[i]),\n+                xmm(scratchRegs1[i]), false, Assembler::AVX_512bit);\n+   }\n+}\n+\n+static void sub_add(int subResult[], int addResult[], int input1[], int input2[],\n+                    MacroAssembler *_masm) {\n+  for (int i = 0; i < 4; i++) {\n+    __ evpsubw(xmm(subResult[i]), k0, xmm(input1[i]), xmm(input2[i]),\n+               false, Assembler::AVX_512bit);\n+    __ evpaddw(xmm(addResult[i]), k0, xmm(input1[i]), xmm(input2[i]),\n+               false, Assembler::AVX_512bit);\n+  }\n+}\n+\n+\/\/ result2 also acts as input1\n+\/\/ result1 also acts as perm1\n+static void permute(int result1[], int result2[], int input2[], int perm2,\n+                    MacroAssembler *_masm) {\n+\n+  for (int i = 1; i < 4; i++) {\n+    __ evmovdquw(xmm(result1[i]), xmm(result1[0]), Assembler::AVX_512bit);\n+  }\n+\n+  for (int i = 0; i < 4; i++) {\n+    __ evpermi2w(xmm(result1[i]), xmm(result2[i]), xmm(input2[i]),\n+                 Assembler::AVX_512bit);\n+    __ evpermt2w(xmm(result2[i]), xmm(perm2), xmm(input2[i]),\n+                 Assembler::AVX_512bit);\n+  }\n+}\n+\n+static void store4regs(Register address, int offset, int sourceRegs[],\n+                       MacroAssembler *_masm) {\n+  for (int i = 0; i < 4; i++) {\n+    __ evmovdquw(Address(address, offset + i * 64), xmm(sourceRegs[i]),\n+                 Assembler::AVX_512bit);\n+  }\n+}\n+\n+\/\/ In all 3 invocations of this function we use the same registers:\n+\/\/ xmm0-xmm7 for the input and the result,\n+\/\/ xmm8-xmm15 as scratch registers and\n+\/\/ xmm16-xmm17 for the constants,\n+\/\/ so we don't pass register arguments.\n+static void barrettReduce(MacroAssembler *_masm) {\n+  for (int i = 0; i < 8; i++) {\n+    __ evpmulhw(xmm(i + 8), k0, xmm(i), xmm16, false, Assembler::AVX_512bit);\n+  }\n+\n+  for (int i = 0; i < 8; i++) {\n+    __ evpsraw(xmm(i + 8), k0, xmm(i + 8), 10, false, Assembler::AVX_512bit);\n+  }\n+\n+  for (int i = 0; i < 8; i++) {\n+    __ evpmullw(xmm(i + 8), k0, xmm(i + 8), xmm17, false, Assembler::AVX_512bit);\n+  }\n+\n+  for (int i = 0; i < 8; i++) {\n+    __ evpsubw(xmm(i), k0, xmm(i), xmm(i + 8), false, Assembler::AVX_512bit);\n+  }\n+}\n+\n+static int xmm0_3[] = {0, 1, 2, 3};\n+static int xmm0145[] = {0, 1, 4, 5};\n+static int xmm0246[] = {0, 2, 4, 6};\n+static int xmm0829[] = {0, 8, 2, 9};\n+static int xmm1001[] = {1, 0, 0, 1};\n+static int xmm1357[] = {1, 3, 5, 7};\n+static int xmm2367[] = {2, 3, 6, 7};\n+static int xmm2_0_10_8[] = {2, 0, 10, 8};\n+static int xmm3223[] = {3, 2, 2, 3};\n+static int xmm4_7[] = {4, 5, 6, 7};\n+static int xmm5454[] = {5, 4, 5, 4};\n+static int xmm7676[] = {7, 6, 7, 6};\n+static int xmm8_11[] = {8, 9, 10, 11};\n+static int xmm12_15[] = {12, 13, 14, 15};\n+static int xmm16_19[] = {16, 17, 18, 19};\n+static int xmm20_23[] = {20, 21, 22, 23};\n+static int xmm23_23[] = {23, 23, 23, 23};\n+static int xmm24_27[] = {24, 25, 26, 27};\n+static int xmm26_29[] = {26, 27, 28, 29};\n+static int xmm28_31[] = {28, 29, 30, 31};\n+static int xmm29_29[] = {29, 29, 29, 29};\n+\n+\/\/ Kyber NTT function.\n+\/\/\n+\/\/ coeffs (short[256]) = c_rarg0\n+\/\/ ntt_zetas (short[256]) = c_rarg1\n+address generate_kyberNtt_avx512(StubGenerator *stubgen,\n+                                 MacroAssembler *_masm) {\n+  __ align(CodeEntryAlignment);\n+  StubGenStubId stub_id = kyberNtt_id;\n+  StubCodeMark mark(stubgen, stub_id);\n+  address start = __ pc();\n+  __ enter();\n+\n+  const Register coeffs = c_rarg0;\n+  const Register zetas = c_rarg1;\n+\n+  const Register perms = r11;\n+\n+  __ lea(perms, ExternalAddress(kyberAvx512NttPermsAddr()));\n+\n+  load4regs(xmm4_7, coeffs, 256, _masm);\n+  load4regs(xmm20_23, zetas, 0, _masm);\n+\n+  __ vpbroadcastq(xmm30,\n+                  ExternalAddress(kyberAvx512ConstsAddr(qOffset)),\n+                  Assembler::AVX_512bit, scratch); \/\/ q\n+  __ vpbroadcastq(xmm31,\n+                  ExternalAddress(kyberAvx512ConstsAddr(qInvModROffset)),\n+                  Assembler::AVX_512bit, scratch); \/\/ q^-1 mod montR\n+\n+  load4regs(xmm0_3, coeffs, 0, _masm);\n+\n+  \/\/ Each level represents one iteration of the outer for loop of the Java version.\n+  \/\/ level 0\n+  montmul(xmm8_11, xmm4_7, xmm20_23, xmm8_11, xmm4_7, _masm);\n+  load4regs(xmm20_23, zetas, 256, _masm);\n+  sub_add(xmm4_7, xmm0_3, xmm0_3, xmm8_11, _masm);\n+\n+  \/\/level 1\n+  montmul(xmm12_15, xmm2367, xmm20_23, xmm12_15, xmm8_11, _masm);\n+  load4regs(xmm20_23, zetas, 512, _masm);\n+  sub_add(xmm2367, xmm0145, xmm0145, xmm12_15, _masm);\n+\n+  \/\/ level 2\n+  montmul(xmm8_11, xmm1357, xmm20_23, xmm12_15, xmm8_11, _masm);\n+  __ evmovdquw(xmm12, Address(perms, 0), Assembler::AVX_512bit);\n+  __ evmovdquw(xmm16, Address(perms, 64), Assembler::AVX_512bit);\n+  load4regs(xmm20_23, zetas, 768, _masm);\n+  sub_add(xmm1357, xmm0246, xmm0246, xmm8_11, _masm);\n+\n+  \/\/level 3\n+  permute(xmm12_15, xmm0246, xmm1357, 16, _masm);\n+  montmul(xmm8_11, xmm12_15, xmm20_23, xmm16_19, xmm8_11, _masm);\n+  __ evmovdquw(xmm16, Address(perms, 128), Assembler::AVX_512bit);\n+  __ evmovdquw(xmm24, Address(perms, 192), Assembler::AVX_512bit);\n+  load4regs(xmm20_23, zetas, 1024, _masm);\n+  sub_add(xmm1357, xmm0246, xmm0246, xmm8_11, _masm);\n+\n+  \/\/ level 4\n+  permute(xmm16_19, xmm0246, xmm1357, 24, _masm);\n+  montmul(xmm8_11, xmm0246, xmm20_23, xmm24_27, xmm8_11, _masm);\n+  __ evmovdquw(xmm1, Address(perms, 256), Assembler::AVX_512bit);\n+  __ evmovdquw(xmm24, Address(perms, 320), Assembler::AVX_512bit);\n+  load4regs(xmm20_23, zetas, 1280, _masm);\n+  sub_add(xmm12_15, xmm0246, xmm16_19, xmm8_11, _masm);\n+\n+  \/\/ level 5\n+  permute(xmm1357, xmm0246, xmm12_15, 24, _masm);\n+  montmul(xmm16_19, xmm0246, xmm20_23, xmm16_19, xmm8_11, _masm);\n+\n+  __ evmovdquw(xmm12, Address(perms, 384), Assembler::AVX_512bit);\n+  __ evmovdquw(xmm8, Address(perms, 448), Assembler::AVX_512bit);\n+\n+  load4regs(xmm20_23, zetas, 1536, _masm);\n+  sub_add(xmm24_27, xmm0246, xmm1357, xmm16_19, _masm);\n+\n+  \/\/ level 6\n+  permute(xmm12_15, xmm0246, xmm24_27, 8, _masm);\n+\n+  __ evmovdquw(xmm1, Address(perms, 512), Assembler::AVX_512bit);\n+  __ evmovdquw(xmm24, Address(perms, 576), Assembler::AVX_512bit);\n+\n+  montmul(xmm16_19, xmm0246, xmm20_23, xmm16_19, xmm8_11, _masm);\n+  sub_add(xmm20_23, xmm0246, xmm12_15, xmm16_19, _masm);\n+\n+  permute(xmm1357, xmm0246, xmm20_23, 24, _masm);\n+\n+  store4regs(coeffs, 0, xmm0_3, _masm);\n+  store4regs(coeffs, 256, xmm4_7, _masm);\n+\n+  __ leave(); \/\/ required for proper stackwalking of RuntimeStub frame\n+  __ mov64(rax, 0); \/\/ return 0\n+  __ ret(0);\n+\n+  return start;\n+}\n+\n+\/\/ Kyber Inverse NTT function\n+\/\/\n+\/\/ coeffs (short[256]) = c_rarg0\n+\/\/ ntt_zetas (short[256]) = c_rarg1\n+address generate_kyberInverseNtt_avx512(StubGenerator *stubgen,\n+                                        MacroAssembler *_masm) {\n+\n+  __ align(CodeEntryAlignment);\n+  StubGenStubId stub_id = kyberInverseNtt_id;\n+  StubCodeMark mark(stubgen, stub_id);\n+  address start = __ pc();\n+  __ enter();\n+\n+  const Register coeffs = c_rarg0;\n+  const Register zetas = c_rarg1;\n+\n+  const Register perms = r11;\n+\n+  __ lea(perms, ExternalAddress(kyberAvx512InverseNttPermsAddr()));\n+  __ evmovdquw(xmm12, Address(perms, 0), Assembler::AVX_512bit);\n+  __ evmovdquw(xmm16, Address(perms, 64), Assembler::AVX_512bit);\n+\n+  __ vpbroadcastq(xmm31,\n+                  ExternalAddress(kyberAvx512ConstsAddr(qInvModROffset)),\n+                  Assembler::AVX_512bit, scratch); \/\/ q^-1 mod montR\n+  __ vpbroadcastq(xmm30,\n+                  ExternalAddress(kyberAvx512ConstsAddr(qOffset)),\n+                  Assembler::AVX_512bit, scratch); \/\/ q\n+  __ vpbroadcastq(xmm29,\n+                  ExternalAddress(kyberAvx512ConstsAddr(dimHalfInverseOffset)),\n+                  Assembler::AVX_512bit, scratch); \/\/ (dim\/2)^-1 mod q\n+\n+  load4regs(xmm0_3, coeffs, 0, _masm);\n+  load4regs(xmm4_7, coeffs, 256, _masm);\n+\n+  \/\/ Each level represents one iteration of the outer for loop of the Java version.\n+  \/\/ level 0\n+  load4regs(xmm8_11, zetas, 0, _masm);\n+  permute(xmm12_15, xmm0246, xmm1357, 16, _masm);\n+\n+  __ evmovdquw(xmm1, Address(perms, 128), Assembler::AVX_512bit);\n+  __ evmovdquw(xmm20, Address(perms, 192), Assembler::AVX_512bit);\n+\n+  sub_add(xmm16_19, xmm0246, xmm0246, xmm12_15, _masm);\n+  montmul(xmm12_15, xmm16_19, xmm8_11, xmm12_15, xmm8_11, _masm);\n+\n+  \/\/ level 1\n+  load4regs(xmm8_11, zetas, 256, _masm);\n+  permute(xmm1357, xmm0246, xmm12_15, 20, _masm);\n+  sub_add(xmm16_19, xmm0246, xmm1357, xmm0246, _masm);\n+\n+  __ evmovdquw(xmm1, Address(perms, 256), Assembler::AVX_512bit);\n+  __ evmovdquw(xmm20, Address(perms, 320), Assembler::AVX_512bit);\n+\n+  montmul(xmm12_15, xmm16_19, xmm8_11, xmm12_15, xmm8_11, _masm);\n+\n+  \/\/ level2\n+  load4regs(xmm8_11, zetas, 512, _masm);\n+  permute(xmm1357, xmm0246, xmm12_15, 20, _masm);\n+  sub_add(xmm16_19, xmm0246, xmm1357,  xmm0246,_masm);\n+\n+  __ evmovdquw(xmm1, Address(perms, 384), Assembler::AVX_512bit);\n+  __ evmovdquw(xmm20, Address(perms, 448), Assembler::AVX_512bit);\n+\n+  montmul(xmm12_15, xmm16_19, xmm8_11, xmm12_15, xmm8_11, _masm);\n+\n+  __ vpbroadcastq(xmm16,\n+                  ExternalAddress(kyberAvx512ConstsAddr(barretMultiplierOffset)),\n+                  Assembler::AVX_512bit, scratch); \/\/ Barrett multiplier\n+  __ vpbroadcastq(xmm17,\n+                  ExternalAddress(kyberAvx512ConstsAddr(qOffset)),\n+                  Assembler::AVX_512bit, scratch); \/\/ q\n+\n+  permute(xmm1357, xmm0246, xmm12_15, 20, _masm);\n+  barrettReduce(_masm);\n+\n+\/\/ level 3\n+  load4regs(xmm8_11, zetas, 768, _masm);\n+  sub_add(xmm16_19, xmm0246, xmm1357, xmm0246, _masm);\n+\n+  __ evmovdquw(xmm1, Address(perms, 512), Assembler::AVX_512bit);\n+  __ evmovdquw(xmm20, Address(perms, 576), Assembler::AVX_512bit);\n+\n+  montmul(xmm12_15, xmm16_19, xmm8_11, xmm12_15, xmm8_11, _masm);\n+  permute(xmm1357, xmm0246, xmm12_15, 20, _masm);\n+\n+  \/\/ level 4\n+  load4regs(xmm8_11, zetas, 1024, _masm);\n+\n+  __ vpbroadcastq(xmm16,\n+                  ExternalAddress(kyberAvx512ConstsAddr(barretMultiplierOffset)),\n+                  Assembler::AVX_512bit, scratch); \/\/ Barrett multiplier\n+  __ vpbroadcastq(xmm17,\n+                  ExternalAddress(kyberAvx512ConstsAddr(qOffset)),\n+                  Assembler::AVX_512bit, scratch); \/\/ q\n+\n+  sub_add(xmm12_15, xmm0246, xmm0246, xmm1357, _masm);\n+  montmul(xmm1357, xmm12_15, xmm8_11, xmm1357, xmm8_11, _masm);\n+  barrettReduce(_masm);\n+\n+  \/\/ level 5\n+  load4regs(xmm8_11, zetas, 1280, _masm);\n+  sub_add(xmm12_15, xmm0145, xmm0145, xmm2367, _masm);\n+  montmul(xmm2367, xmm12_15, xmm8_11, xmm2367, xmm8_11, _masm);\n+\n+  \/\/ level 6\n+  load4regs(xmm8_11, zetas, 1536, _masm);\n+  sub_add(xmm12_15, xmm0_3, xmm0_3, xmm4_7, _masm);\n+  montmul(xmm4_7, xmm12_15, xmm8_11, xmm4_7, xmm8_11, _masm);\n+\n+  montmul(xmm8_11, xmm29_29, xmm0_3, xmm8_11, xmm0_3, _masm);\n+  montmul(xmm12_15, xmm29_29, xmm4_7, xmm12_15, xmm4_7, _masm);\n+\n+  store4regs(coeffs, 0, xmm8_11, _masm);\n+  store4regs(coeffs, 256, xmm12_15, _masm);\n+\n+  __ leave(); \/\/ required for proper stackwalking of RuntimeStub frame\n+  __ mov64(rax, 0); \/\/ return 0\n+  __ ret(0);\n+\n+  return start;\n+}\n+\n+\/\/ Kyber multiply polynomials in the NTT domain.\n+\/\/\n+\/\/ result (short[256]) = c_rarg0\n+\/\/ ntta (short[256]) = c_rarg1\n+\/\/ nttb (short[256]) = c_rarg2\n+\/\/ zetas (short[128]) = c_rarg3\n+address generate_kyberNttMult_avx512(StubGenerator *stubgen,\n+                                     MacroAssembler *_masm) {\n+\n+  __ align(CodeEntryAlignment);\n+  StubGenStubId stub_id = kyberNttMult_id;\n+  StubCodeMark mark(stubgen, stub_id);\n+  address start = __ pc();\n+  __ enter();\n+\n+  const Register result = c_rarg0;\n+  const Register ntta = c_rarg1;\n+  const Register nttb = c_rarg2;\n+  const Register zetas = c_rarg3;\n+\n+  const Register perms = r11;\n+  const Register loopCnt = r12;\n+\n+  __ push(r12);\n+  __ movl(loopCnt, 2);\n+\n+  Label Loop;\n+\n+  __ lea(perms, ExternalAddress(kyberAvx512_nttMultPermsAddr()));\n+\n+\n+  load4regs(xmm26_29, perms, 0, _masm);\n+  __ vpbroadcastq(xmm31,\n+                  ExternalAddress(kyberAvx512ConstsAddr(qInvModROffset)),\n+                  Assembler::AVX_512bit, scratch); \/\/ q^-1 mod montR\n+  __ vpbroadcastq(xmm30,\n+                  ExternalAddress(kyberAvx512ConstsAddr(qOffset)),\n+                  Assembler::AVX_512bit, scratch); \/\/ q\n+  __ vpbroadcastq(xmm23,\n+                  ExternalAddress(kyberAvx512ConstsAddr(montRSquareModqOffset)),\n+                  Assembler::AVX_512bit, scratch); \/\/ montR^2 mod q\n+\n+  __ BIND(Loop);\n+\n+    __ evmovdquw(xmm1, Address(ntta, 0), Assembler::AVX_512bit);\n+    __ evmovdquw(xmm8, Address(ntta, 64), Assembler::AVX_512bit);\n+    __ evmovdquw(xmm3, Address(ntta, 128), Assembler::AVX_512bit);\n+    __ evmovdquw(xmm9, Address(ntta, 192), Assembler::AVX_512bit);\n+\n+    __ evmovdquw(xmm5, Address(nttb, 0), Assembler::AVX_512bit);\n+    __ evmovdquw(xmm10, Address(nttb, 64), Assembler::AVX_512bit);\n+    __ evmovdquw(xmm7, Address(nttb, 128), Assembler::AVX_512bit);\n+    __ evmovdquw(xmm11, Address(nttb, 192), Assembler::AVX_512bit);\n+\n+    __ evmovdquw(xmm0, xmm26, Assembler::AVX_512bit);\n+    __ evmovdquw(xmm2, xmm26, Assembler::AVX_512bit);\n+    __ evmovdquw(xmm4, xmm26, Assembler::AVX_512bit);\n+    __ evmovdquw(xmm6, xmm26, Assembler::AVX_512bit);\n+\n+    __ evpermi2w(xmm0, xmm1, xmm8, Assembler::AVX_512bit);\n+    __ evpermt2w(xmm1, xmm27, xmm8, Assembler::AVX_512bit);\n+    __ evpermi2w(xmm2, xmm3, xmm9, Assembler::AVX_512bit);\n+    __ evpermt2w(xmm3, xmm27, xmm9, Assembler::AVX_512bit);\n+\n+    __ evpermi2w(xmm4, xmm5, xmm10, Assembler::AVX_512bit);\n+    __ evpermt2w(xmm5, xmm27, xmm10, Assembler::AVX_512bit);\n+    __ evpermi2w(xmm6, xmm7, xmm11, Assembler::AVX_512bit);\n+    __ evpermt2w(xmm7, xmm27, xmm11, Assembler::AVX_512bit);\n+\n+    __ evmovdquw(xmm24, Address(zetas, 0), Assembler::AVX_512bit);\n+    __ evmovdquw(xmm25, Address(zetas, 64), Assembler::AVX_512bit);\n+\n+    montmul(xmm16_19, xmm1001, xmm5454, xmm16_19, xmm12_15, _masm);\n+\n+    montmul(xmm0145, xmm3223, xmm7676, xmm0145, xmm12_15, _masm);\n+\n+    __ evpmullw(xmm2, k0, xmm16, xmm24, false, Assembler::AVX_512bit);\n+    __ evpmullw(xmm3, k0, xmm0, xmm25, false, Assembler::AVX_512bit);\n+    __ evpmulhw(xmm12, k0, xmm16, xmm24, false, Assembler::AVX_512bit);\n+    __ evpmulhw(xmm13, k0, xmm0, xmm25, false, Assembler::AVX_512bit);\n+\n+    __ evpmullw(xmm2, k0, xmm2, xmm31, false, Assembler::AVX_512bit);\n+    __ evpmullw(xmm3, k0, xmm3, xmm31, false, Assembler::AVX_512bit);\n+    __ evpmulhw(xmm2, k0, xmm30, xmm2, false, Assembler::AVX_512bit);\n+    __ evpmulhw(xmm3, k0, xmm30, xmm3, false, Assembler::AVX_512bit);\n+\n+    __ evpsubw(xmm2, k0, xmm12, xmm2, false, Assembler::AVX_512bit);\n+    __ evpsubw(xmm3, k0, xmm13, xmm3, false, Assembler::AVX_512bit);\n+\n+    __ evpaddw(xmm0, k0, xmm2, xmm17, false, Assembler::AVX_512bit);\n+    __ evpaddw(xmm8, k0, xmm3, xmm1, false, Assembler::AVX_512bit);\n+    __ evpaddw(xmm2, k0, xmm18, xmm19, false, Assembler::AVX_512bit);\n+    __ evpaddw(xmm9, k0, xmm4, xmm5, false, Assembler::AVX_512bit);\n+\n+    montmul(xmm1357, xmm0829, xmm23_23, xmm1357, xmm0829, _masm);\n+\n+    __ evmovdquw(xmm0, xmm28, Assembler::AVX_512bit);\n+    __ evmovdquw(xmm2, xmm28, Assembler::AVX_512bit);\n+    __ evpermi2w(xmm0, xmm1, xmm5, Assembler::AVX_512bit);\n+    __ evpermt2w(xmm1, xmm29, xmm5, Assembler::AVX_512bit);\n+    __ evpermi2w(xmm2, xmm3, xmm7, Assembler::AVX_512bit);\n+    __ evpermt2w(xmm3, xmm29, xmm7, Assembler::AVX_512bit);\n+\n+    store4regs(result, 0, xmm0_3, _masm);\n+\n+    __ addptr(ntta, 256);\n+    __ addptr(nttb, 256);\n+    __ addptr(result, 256);\n+    __ addptr(zetas, 128);\n+    __ subl(loopCnt, 1);\n+    __ jcc(Assembler::greater, Loop);\n+\n+  __ pop(r12);\n+\n+  __ leave(); \/\/ required for proper stackwalking of RuntimeStub frame\n+  __ mov64(rax, 0); \/\/ return 0\n+  __ ret(0);\n+\n+  return start;\n+}\n+\n+\/\/ Kyber add 2 polynomials.\n+\/\/\n+\/\/ result (short[256]) = c_rarg0\n+\/\/ a (short[256]) = c_rarg1\n+\/\/ b (short[256]) = c_rarg2\n+address generate_kyberAddPoly_2_avx512(StubGenerator *stubgen,\n+                                       MacroAssembler *_masm) {\n+\n+  __ align(CodeEntryAlignment);\n+  StubGenStubId stub_id = kyberAddPoly_2_id;\n+  StubCodeMark mark(stubgen, stub_id);\n+  address start = __ pc();\n+  __ enter();\n+\n+  const Register result = c_rarg0;\n+  const Register a = c_rarg1;\n+  const Register b = c_rarg2;\n+\n+  __ vpbroadcastq(xmm31,\n+                  ExternalAddress(kyberAvx512ConstsAddr(qOffset)),\n+                  Assembler::AVX_512bit, scratch); \/\/ q\n+\n+  for (int i = 0; i < 8; i++) {\n+    __ evmovdquw(xmm(i), Address(a, 64 * i), Assembler::AVX_512bit);\n+    __ evmovdquw(xmm(i + 8), Address(b, 64 * i), Assembler::AVX_512bit);\n+  }\n+\n+  for (int i = 0; i < 8; i++) {\n+    __ evpaddw(xmm(i), k0, xmm(i), xmm(i + 8), false, Assembler::AVX_512bit);\n+  }\n+\n+  for (int i = 0; i < 8; i++) {\n+    __ evpaddw(xmm(i), k0, xmm(i), xmm31, false, Assembler::AVX_512bit);\n+  }\n+\n+  store4regs(result, 0, xmm0_3, _masm);\n+  store4regs(result, 256, xmm4_7, _masm);\n+\n+  __ leave(); \/\/ required for proper stackwalking of RuntimeStub frame\n+  __ mov64(rax, 0); \/\/ return 0\n+  __ ret(0);\n+\n+  return start;\n+}\n+\n+\/\/ Kyber add 3 polynomials.\n+\/\/\n+\/\/ result (short[256]) = c_rarg0\n+\/\/ a (short[256]) = c_rarg1\n+\/\/ b (short[256]) = c_rarg2\n+\/\/ c (short[256]) = c_rarg3\n+address generate_kyberAddPoly_3_avx512(StubGenerator *stubgen,\n+                                       MacroAssembler *_masm) {\n+\n+  __ align(CodeEntryAlignment);\n+  StubGenStubId stub_id = kyberAddPoly_3_id;\n+  StubCodeMark mark(stubgen, stub_id);\n+  address start = __ pc();\n+  __ enter();\n+\n+  const Register result = c_rarg0;\n+  const Register a = c_rarg1;\n+  const Register b = c_rarg2;\n+  const Register c = c_rarg3;\n+\n+  __ vpbroadcastq(xmm31,\n+                  ExternalAddress(kyberAvx512ConstsAddr(qOffset)),\n+                  Assembler::AVX_512bit, scratch); \/\/ q\n+\n+  for (int i = 0; i < 8; i++) {\n+    __ evmovdquw(xmm(i), Address(a, 64 * i), Assembler::AVX_512bit);\n+    __ evmovdquw(xmm(i + 8), Address(b, 64 * i), Assembler::AVX_512bit);\n+    __ evmovdquw(xmm(i + 16), Address(c, 64 * i), Assembler::AVX_512bit);\n+  }\n+\n+  __ evpaddw(xmm31, k0, xmm31, xmm31, false, Assembler::AVX_512bit);\n+\n+  for (int i = 0; i < 8; i++) {\n+    __ evpaddw(xmm(i), k0, xmm(i), xmm(i + 8), false, Assembler::AVX_512bit);\n+  }\n+\n+  for (int i = 0; i < 8; i++) {\n+    __ evpaddw(xmm(i), k0, xmm(i), xmm(i + 16), false, Assembler::AVX_512bit);\n+  }\n+\n+  for (int i = 0; i < 8; i++) {\n+    __ evpaddw(xmm(i), k0, xmm(i), xmm31, false, Assembler::AVX_512bit);\n+  }\n+\n+  store4regs(result, 0, xmm0_3, _masm);\n+  store4regs(result, 256, xmm4_7, _masm);\n+\n+  __ leave(); \/\/ required for proper stackwalking of RuntimeStub frame\n+  __ mov64(rax, 0); \/\/ return 0\n+  __ ret(0);\n+\n+  return start;\n+}\n+\n+\/\/ Kyber parse XOF output to polynomial coefficient candidates.\n+\/\/\n+\/\/ condensed (byte[168]) = c_rarg0\n+\/\/ condensedOffs (int) = c_rarg1\n+\/\/ parsed (short[112]) = c_rarg2\n+\/\/ parsedLength (int) = c_rarg3\n+address generate_kyber12To16_avx512(StubGenerator *stubgen,\n+                                    MacroAssembler *_masm) {\n+\n+  __ align(CodeEntryAlignment);\n+  StubGenStubId stub_id = kyber12To16_id;\n+  StubCodeMark mark(stubgen, stub_id);\n+  address start = __ pc();\n+  __ enter();\n+\n+  const Register condensed = c_rarg0;\n+  const Register condensedOffs = c_rarg1;\n+  const Register parsed = c_rarg2;\n+  const Register parsedLength = c_rarg3;\n+\n+  const Register perms = r11;\n+\n+  Label Loop;\n+\n+  __ addptr(condensed, condensedOffs);\n+\n+  __ lea(perms, ExternalAddress(kyberAvx512_12To16PermsAddr()));\n+\n+  load4regs(xmm24_27, perms, 0, _masm);\n+  load4regs(xmm28_31, perms, 256, _masm);\n+  __ vpbroadcastq(xmm23,\n+                  ExternalAddress(kyberAvx512ConstsAddr(f00Offset)),\n+                  Assembler::AVX_512bit, scratch); \/\/ 0xF00\n+\n+  __ BIND(Loop);\n+    __ evmovdqub(xmm0, Address(condensed, 0),Assembler::AVX_256bit);\n+    __ evmovdqub(xmm1, Address(condensed, 32),Assembler::AVX_256bit);\n+    __ evmovdqub(xmm2, Address(condensed, 64),Assembler::AVX_256bit);\n+    __ evmovdqub(xmm8, Address(condensed, 96),Assembler::AVX_256bit);\n+    __ evmovdqub(xmm9, Address(condensed, 128),Assembler::AVX_256bit);\n+    __ evmovdqub(xmm10, Address(condensed, 160),Assembler::AVX_256bit);\n+    __ vpmovzxbw(xmm0, xmm0, Assembler::AVX_512bit);\n+    __ vpmovzxbw(xmm1, xmm1, Assembler::AVX_512bit);\n+    __ vpmovzxbw(xmm2, xmm2, Assembler::AVX_512bit);\n+    __ vpmovzxbw(xmm8, xmm8, Assembler::AVX_512bit);\n+    __ vpmovzxbw(xmm9, xmm9, Assembler::AVX_512bit);\n+    __ vpmovzxbw(xmm10, xmm10, Assembler::AVX_512bit);\n+    __ evmovdquw(xmm3, xmm24, Assembler::AVX_512bit);\n+    __ evmovdquw(xmm4, xmm25, Assembler::AVX_512bit);\n+    __ evmovdquw(xmm5, xmm26, Assembler::AVX_512bit);\n+    __ evmovdquw(xmm11, xmm24, Assembler::AVX_512bit);\n+    __ evmovdquw(xmm12, xmm25, Assembler::AVX_512bit);\n+    __ evmovdquw(xmm13, xmm26, Assembler::AVX_512bit);\n+    __ evpermi2w(xmm3, xmm0, xmm1, Assembler::AVX_512bit);\n+    __ evpermi2w(xmm4, xmm0, xmm1, Assembler::AVX_512bit);\n+    __ evpermi2w(xmm5, xmm0, xmm1, Assembler::AVX_512bit);\n+    __ evpermi2w(xmm11, xmm8, xmm9, Assembler::AVX_512bit);\n+    __ evpermi2w(xmm12, xmm8, xmm9, Assembler::AVX_512bit);\n+    __ evpermi2w(xmm13, xmm8, xmm9, Assembler::AVX_512bit);\n+    __ evpermt2w(xmm3, xmm27, xmm2, Assembler::AVX_512bit);\n+    __ evpermt2w(xmm4, xmm28, xmm2, Assembler::AVX_512bit);\n+    __ evpermt2w(xmm5, xmm29, xmm2, Assembler::AVX_512bit);\n+    __ evpermt2w(xmm11, xmm27, xmm10, Assembler::AVX_512bit);\n+    __ evpermt2w(xmm12, xmm28, xmm10, Assembler::AVX_512bit);\n+    __ evpermt2w(xmm13, xmm29, xmm10, Assembler::AVX_512bit);\n+\n+    __ evpsraw(xmm2, k0, xmm4, 4, false, Assembler::AVX_512bit);\n+    __ evpsllw(xmm0, k0, xmm4, 8, false, Assembler::AVX_512bit);\n+    __ evpsllw(xmm1, k0, xmm5, 4, false, Assembler::AVX_512bit);\n+    __ evpsllw(xmm8, k0, xmm12, 8, false, Assembler::AVX_512bit);\n+    __ evpsraw(xmm10, k0, xmm12, 4, false, Assembler::AVX_512bit);\n+    __ evpsllw(xmm9, k0, xmm13, 4, false, Assembler::AVX_512bit);\n+    __ evpandq(xmm0, k0, xmm0, xmm23, false, Assembler::AVX_512bit);\n+    __ evpandq(xmm8, k0, xmm8, xmm23, false, Assembler::AVX_512bit);\n+    __ evpaddw(xmm1, k0, xmm1, xmm2, false, Assembler::AVX_512bit);\n+    __ evpaddw(xmm0, k0, xmm0, xmm3, false, Assembler::AVX_512bit);\n+    __ evmovdquw(xmm2, xmm30, Assembler::AVX_512bit);\n+    __ evpaddw(xmm9, k0, xmm9, xmm10, false, Assembler::AVX_512bit);\n+    __ evpaddw(xmm8, k0, xmm8, xmm11, false, Assembler::AVX_512bit);\n+    __ evmovdquw(xmm10, xmm30, Assembler::AVX_512bit);\n+    __ evpermi2w(xmm2, xmm0, xmm1, Assembler::AVX_512bit);\n+    __ evpermt2w(xmm0, xmm31, xmm1, Assembler::AVX_512bit);\n+    __ evpermi2w(xmm10, xmm8, xmm9, Assembler::AVX_512bit);\n+    __ evpermt2w(xmm8, xmm31, xmm9, Assembler::AVX_512bit);\n+\n+    store4regs(parsed, 0, xmm2_0_10_8, _masm);\n+\n+    __ addptr(condensed, 192);\n+    __ addptr(parsed, 256);\n+    __ subl(parsedLength, 128);\n+    __ jcc(Assembler::greater, Loop);\n+\n+  __ leave(); \/\/ required for proper stackwalking of RuntimeStub frame\n+  __ mov64(rax, 0); \/\/ return 0\n+  __ ret(0);\n+\n+  return start;\n+}\n+\n+\n+\/\/ Kyber barrett reduce function.\n+\/\/\n+\/\/ coeffs (short[256]) = c_rarg0\n+address generate_kyberBarrettReduce_avx512(StubGenerator *stubgen,\n+                                           MacroAssembler *_masm) {\n+\n+  __ align(CodeEntryAlignment);\n+  StubGenStubId stub_id = kyberBarrettReduce_id;\n+  StubCodeMark mark(stubgen, stub_id);\n+  address start = __ pc();\n+  __ enter();\n+\n+  const Register coeffs = c_rarg0;\n+\n+  __ vpbroadcastq(xmm16,\n+                  ExternalAddress(kyberAvx512ConstsAddr(barretMultiplierOffset)),\n+                  Assembler::AVX_512bit, scratch); \/\/ Barrett multiplier\n+  __ vpbroadcastq(xmm17,\n+                  ExternalAddress(kyberAvx512ConstsAddr(qOffset)),\n+                  Assembler::AVX_512bit, scratch); \/\/ q\n+\n+  load4regs(xmm0_3, coeffs, 0, _masm);\n+  load4regs(xmm4_7, coeffs, 256, _masm);\n+\n+  barrettReduce(_masm);\n+\n+  store4regs(coeffs, 0, xmm0_3, _masm);\n+  store4regs(coeffs, 256, xmm4_7, _masm);\n+\n+  __ leave(); \/\/ required for proper stackwalking of RuntimeStub frame\n+  __ mov64(rax, 0); \/\/ return 0\n+  __ ret(0);\n+\n+  return start;\n+}\n+\n+void StubGenerator::generate_kyber_stubs() {\n+  \/\/ Generate Kyber intrinsics code\n+  if (UseKyberIntrinsics) {\n+    if (VM_Version::supports_evex()) {\n+      StubRoutines::_kyberNtt = generate_kyberNtt_avx512(this, _masm);\n+      StubRoutines::_kyberInverseNtt = generate_kyberInverseNtt_avx512(this, _masm);\n+      StubRoutines::_kyberNttMult = generate_kyberNttMult_avx512(this, _masm);\n+      StubRoutines::_kyberAddPoly_2 = generate_kyberAddPoly_2_avx512(this, _masm);\n+      StubRoutines::_kyberAddPoly_3 = generate_kyberAddPoly_3_avx512(this, _masm);\n+      StubRoutines::_kyber12To16 = generate_kyber12To16_avx512(this, _masm);\n+      StubRoutines::_kyberBarrettReduce = generate_kyberBarrettReduce_avx512(this, _masm);\n+    }\n+  }\n+}\n","filename":"src\/hotspot\/cpu\/x86\/stubGenerator_x86_64_kyber.cpp","additions":952,"deletions":0,"binary":false,"changes":952,"status":"added"},{"patch":"@@ -1228,0 +1228,14 @@\n+  \/\/ Kyber Intrinsics\n+  \/\/ Currently we only have them for AVX512\n+#ifdef _LP64\n+  if (supports_evex() && supports_avx512bw()) {\n+      if (FLAG_IS_DEFAULT(UseKyberIntrinsics)) {\n+          UseKyberIntrinsics = true;\n+      }\n+  } else\n+#endif\n+  if (UseKyberIntrinsics) {\n+     warning(\"Intrinsics for ML-KEM are not available on this CPU.\");\n+     FLAG_SET_DEFAULT(UseKyberIntrinsics, false);\n+  }\n+\n","filename":"src\/hotspot\/cpu\/x86\/vm_version_x86.cpp","additions":14,"deletions":0,"binary":false,"changes":14,"status":"modified"},{"patch":"@@ -499,0 +499,1 @@\n+    break;\n","filename":"src\/hotspot\/share\/classfile\/vmIntrinsics.cpp","additions":1,"deletions":0,"binary":false,"changes":1,"status":"modified"},{"patch":"@@ -7744,0 +7744,1 @@\n+\n","filename":"src\/hotspot\/share\/opto\/library_call.cpp","additions":1,"deletions":0,"binary":false,"changes":1,"status":"modified"},{"patch":"@@ -1475,0 +1475,1 @@\n+\n@@ -1547,0 +1548,1 @@\n+\n","filename":"src\/hotspot\/share\/opto\/runtime.cpp","additions":2,"deletions":0,"binary":false,"changes":2,"status":"modified"},{"patch":"@@ -598,1 +598,1 @@\n-  static const TypeFunc* kyberNtt_Type() {\n+  static inline const TypeFunc* kyberNtt_Type() {\n@@ -603,1 +603,1 @@\n-  static const TypeFunc* kyberInverseNtt_Type() {\n+  static inline const TypeFunc* kyberInverseNtt_Type() {\n@@ -608,1 +608,1 @@\n-  static const TypeFunc* kyberNttMult_Type() {\n+  static inline const TypeFunc* kyberNttMult_Type() {\n@@ -613,1 +613,1 @@\n-  static const TypeFunc* kyberAddPoly_2_Type() {\n+  static inline const TypeFunc* kyberAddPoly_2_Type() {\n@@ -618,1 +618,1 @@\n-  static const TypeFunc* kyberAddPoly_3_Type() {\n+  static inline const TypeFunc* kyberAddPoly_3_Type() {\n@@ -623,1 +623,1 @@\n-  static const TypeFunc* kyber12To16_Type() {\n+  static inline const TypeFunc* kyber12To16_Type() {\n@@ -628,1 +628,1 @@\n-  static const TypeFunc* kyberBarrettReduce_Type() {\n+  static inline const TypeFunc* kyberBarrettReduce_Type() {\n","filename":"src\/hotspot\/share\/opto\/runtime.hpp","additions":7,"deletions":7,"binary":false,"changes":14,"status":"modified"},{"patch":"@@ -330,0 +330,1 @@\n+                                                                            \\\n","filename":"src\/hotspot\/share\/runtime\/globals.hpp","additions":1,"deletions":0,"binary":false,"changes":1,"status":"modified"},{"patch":"@@ -2,1 +2,1 @@\n- * Copyright (c) 2024, Oracle and\/or its affiliates. All rights reserved.\n+ * Copyright (c) 2024, 2025, Oracle and\/or its affiliates. All rights reserved.\n@@ -1184,1 +1184,0 @@\n-        mlKemBarrettReduce(a);\n","filename":"src\/java.base\/share\/classes\/com\/sun\/crypto\/provider\/ML_KEM.java","additions":1,"deletions":2,"binary":false,"changes":3,"status":"modified"}]}