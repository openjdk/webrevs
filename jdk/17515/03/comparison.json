{"files":[{"patch":"@@ -1848,0 +1848,5 @@\n+void Assembler::cmpb(Register dst, int imm8) {\n+  prefix(dst);\n+  emit_arith_b(0x80, 0xF8, dst, imm8);\n+}\n+\n@@ -8976,0 +8981,9 @@\n+void Assembler::evinserti64x2(XMMRegister dst, XMMRegister nds, XMMRegister src, uint8_t imm8, int vector_len) {\n+   assert(VM_Version::supports_avx512dq(), \"\");\n+   assert(vector_len == AVX_512bit || VM_Version::supports_avx512vl(), \"\");\n+   InstructionAttr attributes(vector_len, \/* vex_w *\/ true, \/* legacy_mode *\/ false, \/* no_mask_reg *\/ true, \/* uses_vl *\/ true);\n+   attributes.set_is_evex_instruction();\n+   int encode = vex_prefix_and_encode(dst->encoding(), nds->encoding(), src->encoding(), VEX_SIMD_66, VEX_OPCODE_0F_3A, &attributes);\n+   emit_int24(0x38, (0xC0 | encode), imm8 & 0x03);\n+}\n+\n@@ -11040,0 +11054,15 @@\n+void Assembler::evbroadcastf64x2(XMMRegister dst, Address src, int vector_len) {\n+  assert(VM_Version::supports_avx512dq(), \"\");\n+  assert(vector_len == AVX_512bit || VM_Version::supports_avx512vl(), \"\");\n+  assert(dst != xnoreg, \"sanity\");\n+  InstructionMark im(this);\n+  InstructionAttr attributes(vector_len, \/* vex_w *\/ true, \/* legacy_mode *\/ false, \/* no_mask_reg *\/ true, \/* uses_vl *\/ true);\n+  attributes.set_address_attributes(\/* tuple_type *\/ EVEX_T2, \/* input_size_in_bits *\/ EVEX_64bit);\n+  attributes.set_is_evex_instruction();\n+  \/\/ swap src<->dst for encoding\n+  vex_prefix(src, 0, dst->encoding(), VEX_SIMD_66, VEX_OPCODE_0F_38, &attributes);\n+  emit_int8(0x1A);\n+  emit_operand(dst, src, 0);\n+}\n+\n+\n","filename":"src\/hotspot\/cpu\/x86\/assembler_x86.cpp","additions":29,"deletions":0,"binary":false,"changes":29,"status":"modified"},{"patch":"@@ -1196,0 +1196,1 @@\n+  void cmpb(Register reg, int imm8);\n@@ -2821,0 +2822,1 @@\n+  void evinserti64x2(XMMRegister dst, XMMRegister nds, XMMRegister src, uint8_t imm8, int vector_len);\n@@ -2870,0 +2872,1 @@\n+  void evbroadcastf64x2(XMMRegister dst, Address src, int vector_len);\n","filename":"src\/hotspot\/cpu\/x86\/assembler_x86.hpp","additions":3,"deletions":0,"binary":false,"changes":3,"status":"modified"},{"patch":"@@ -379,5 +379,16 @@\n-  void generateHtbl_48_block_zmm(Register htbl, Register avx512_subkeyHtbl, Register rscratch);\n-  void ghash16_encrypt16_parallel(Register key, Register subkeyHtbl, XMMRegister ctr_blockx,\n-                                  XMMRegister aad_hashx, Register in, Register out, Register data, Register pos, bool reduction,\n-                                  XMMRegister addmask, bool no_ghash_input, Register rounds, Register ghash_pos,\n-                                  bool final_reduction, int index, XMMRegister counter_inc_mask);\n+  void ghash16_encrypt_parallel16_avx512(Register in, Register out, Register ct, Register pos, Register avx512_subkeyHtbl,\n+                                         Register CTR_CHECK, Register NROUNDS, Register key, XMMRegister CTR, XMMRegister GHASH,\n+                                         XMMRegister ADDBE_4x4, XMMRegister ADDBE_1234, XMMRegister ADD_1234, XMMRegister SHUF_MASK,\n+                                         bool hk_broadcast, bool is_hash_start, bool do_hash_reduction, bool do_hash_hxor,\n+                                         bool no_ghash_in, int ghashin_offset, int aesout_offset, int hashkey_offset);\n+  void generateHtbl_32_blocks_avx512(Register htbl, Register avx512_htbl);\n+  void initial_blocks_16_avx512(Register in, Register out, Register ct, Register pos, Register key, Register avx512_subkeyHtbl,\n+                                Register CTR_CHECK, Register rounds, XMMRegister CTR, XMMRegister GHASH,  XMMRegister ADDBE_4x4,\n+                                XMMRegister ADDBE_1234, XMMRegister ADD_1234, XMMRegister SHUF_MASK, int stack_offset);\n+  void gcm_enc_dec_last_avx512(Register len, Register in, Register pos, XMMRegister HASH, XMMRegister SHUFM, Register subkeyHtbl,\n+                               int ghashin_offset, int hashkey_offset, bool start_ghash, bool do_reduction);\n+  void ghash16_avx512(bool start_ghash, bool do_reduction, bool uload_shuffle, bool hk_broadcast, bool do_hxor,\n+                      Register in, Register pos, Register subkeyHtbl, XMMRegister HASH, XMMRegister SHUFM, int in_offset,\n+                      int in_disp, int displacement, int hashkey_offset);\n+  void aesgcm_avx512(Register in, Register len, Register ct, Register out, Register key,\n+                     Register state, Register subkeyHtbl, Register avx512_subkeyHtbl, Register counter);\n","filename":"src\/hotspot\/cpu\/x86\/stubGenerator_x86_64.hpp","additions":16,"deletions":5,"binary":false,"changes":21,"status":"modified"},{"patch":"@@ -2,1 +2,1 @@\n-* Copyright (c) 2019, 2023, Intel Corporation. All rights reserved.\n+* Copyright (c) 2019, 2024, Intel Corporation. All rights reserved.\n@@ -175,0 +175,32 @@\n+\/\/ This mask is used for incrementing counter value\n+ATTRIBUTE_ALIGNED(64) static const uint64_t COUNTER_MASK_ADDBE_4444[] = {\n+    0x0000000000000000ULL, 0x0400000000000000ULL,\n+    0x0000000000000000ULL, 0x0400000000000000ULL,\n+    0x0000000000000000ULL, 0x0400000000000000ULL,\n+    0x0000000000000000ULL, 0x0400000000000000ULL,\n+};\n+static address counter_mask_addbe_4444_addr() {\n+    return (address)COUNTER_MASK_ADDBE_4444;\n+}\n+\n+\/\/ This mask is used for incrementing counter value\n+ATTRIBUTE_ALIGNED(64) static const uint64_t COUNTER_MASK_ADDBE_1234[] = {\n+    0x0000000000000000ULL, 0x0100000000000000ULL,\n+    0x0000000000000000ULL, 0x0200000000000000ULL,\n+    0x0000000000000000ULL, 0x0300000000000000ULL,\n+    0x0000000000000000ULL, 0x0400000000000000ULL,\n+};\n+static address counter_mask_addbe_1234_addr() {\n+    return (address)COUNTER_MASK_ADDBE_1234;\n+}\n+\n+\/\/ This mask is used for incrementing counter value\n+ATTRIBUTE_ALIGNED(64) static const uint64_t COUNTER_MASK_ADD_1234[] = {\n+    0x0000000000000001ULL, 0x0000000000000000ULL,\n+    0x0000000000000002ULL, 0x0000000000000000ULL,\n+    0x0000000000000003ULL, 0x0000000000000000ULL,\n+    0x0000000000000004ULL, 0x0000000000000000ULL,\n+};\n+static address counter_mask_add_1234_addr() {\n+    return (address)COUNTER_MASK_ADD_1234;\n+}\n@@ -212,4 +244,4 @@\n-\/\/   key        = r10           | r8  (c_rarg4)\n-\/\/   state      = r13           | r9  (c_rarg5)\n-\/\/   subkeyHtbl = r14           | r11\n-\/\/   counter    = rsi           | r12\n+\/\/   key        = rsi           | r8  (c_rarg4)\n+\/\/   state      = rdi           | r9  (c_rarg5)\n+\/\/   subkeyHtbl = r10           | r10\n+\/\/   counter    = r11           | r11\n@@ -233,2 +265,2 @@\n-  const Register subkeyHtbl = r11;\n-  const Register avx512_subkeyHtbl = r13;\n+  const Register subkeyHtbl = r10;\n+  const Register avx512_subkeyHtbl = r12;\n@@ -236,1 +268,1 @@\n-  const Register counter = r12;\n+  const Register counter = r11;\n@@ -239,1 +271,1 @@\n-  const Register key = r10;\n+  const Register key = rsi;\n@@ -241,1 +273,1 @@\n-  const Register state = r13;\n+  const Register state = rdi;\n@@ -243,1 +275,1 @@\n-  const Register subkeyHtbl = r14;\n+  const Register subkeyHtbl = r10;\n@@ -246,1 +278,1 @@\n-  const Register counter = rsi;\n+  const Register counter = r11;\n@@ -250,5 +282,4 @@\n-  __ push(r12);\n-  __ push(r13);\n-  __ push(r14);\n-  __ push(r15);\n-  __ push(rbx);\n+  __ push(r12);\/\/holds pointer to avx512_subkeyHtbl\n+  __ push(r14);\/\/holds CTR_CHECK value to check for overflow\n+  __ push(r15);\/\/holds number of rounds\n+  __ push(rbx);\/\/scratch register\n@@ -258,0 +289,1 @@\n+  __ push(rdi);\n@@ -265,1 +297,1 @@\n-  __ subptr(rsp, 96 * longSize); \/\/ Create space on the stack for htbl entries\n+  __ subptr(rsp, 200 * longSize); \/\/ Create space on the stack for 64 htbl entries and 8 zmm AES entries\n@@ -268,1 +300,1 @@\n-  aesgcm_encrypt(in, len, ct, out, key, state, subkeyHtbl, avx512_subkeyHtbl, counter);\n+  aesgcm_avx512(in, len, ct, out, key, state, subkeyHtbl, avx512_subkeyHtbl, counter);\n@@ -275,0 +307,1 @@\n+  __ pop(rdi);\n@@ -277,1 +310,1 @@\n-  __ lea(rsp, Address(rbp, -5 * wordSize));\n+  __ lea(rsp, Address(rbp, -4 * wordSize));\n@@ -282,1 +315,0 @@\n-  __ pop(r13);\n@@ -2711,1 +2743,2 @@\n-void StubGenerator::generateHtbl_48_block_zmm(Register htbl, Register avx512_htbl, Register rscratch) {\n+\/\/ Holds 64 Htbl entries, 32 HKey and 32 HkKey (derived from HKey)\n+void StubGenerator::generateHtbl_32_blocks_avx512(Register htbl, Register avx512_htbl) {\n@@ -2713,5 +2746,3 @@\n-  const XMMRegister ZT5 = xmm4;\n-  const XMMRegister ZT7 = xmm7;\n-  const XMMRegister ZT8 = xmm8;\n-\n-  Label GFMUL_AVX512;\n+  const XMMRegister ZT1 = xmm0, ZT2 = xmm1, ZT3 = xmm2, ZT4 = xmm3;\n+  const XMMRegister ZT5 = xmm4, ZT6 = xmm5, ZT7 = xmm7, ZT8 = xmm8;\n+  const XMMRegister ZT10 = xmm10, ZT11 = xmm11, ZT12 = xmm12;\n@@ -2720,5 +2751,4 @@\n-  __ movdqu(xmm10, ExternalAddress(ghash_long_swap_mask_addr()), rscratch);\n-  __ vpshufb(HK, HK, xmm10, Assembler::AVX_128bit);\n-\n-  __ movdqu(xmm11, ExternalAddress(ghash_polynomial_addr()), rscratch);\n-  __ movdqu(xmm12, ExternalAddress(ghash_polynomial_two_one_addr()), rscratch);\n+  __ movdqu(ZT10, ExternalAddress(ghash_long_swap_mask_addr()), r15);\n+  __ vpshufb(HK, HK, ZT10, Assembler::AVX_128bit);\n+  __ movdqu(ZT11, ExternalAddress(ghash_polynomial_addr()), r15);\n+  __ movdqu(ZT12, ExternalAddress(ghash_polynomial_two_one_addr()), r15);\n@@ -2726,14 +2756,13 @@\n-  __ movdqu(xmm2, xmm6);\n-  __ vpsllq(xmm6, xmm6, 1, Assembler::AVX_128bit);\n-  __ vpsrlq(xmm2, xmm2, 63, Assembler::AVX_128bit);\n-  __ movdqu(xmm1, xmm2);\n-  __ vpslldq(xmm2, xmm2, 8, Assembler::AVX_128bit);\n-  __ vpsrldq(xmm1, xmm1, 8, Assembler::AVX_128bit);\n-  __ vpor(xmm6, xmm6, xmm2, Assembler::AVX_128bit);\n-\n-  __ vpshufd(xmm2, xmm1, 0x24, Assembler::AVX_128bit);\n-  __ vpcmpeqd(xmm2, xmm2, xmm12, Assembler::AVX_128bit);\n-  __ vpand(xmm2, xmm2, xmm11, Assembler::AVX_128bit);\n-  __ vpxor(xmm6, xmm6, xmm2, Assembler::AVX_128bit);\n-  __ movdqu(Address(avx512_htbl, 16 * 47), xmm6); \/\/ H ^ 2\n-  \/\/ Compute the remaining three powers of H using XMM registers and all following powers using ZMM\n+  __ movdqu(ZT3, HK);\n+  __ vpsllq(HK, HK, 1, Assembler::AVX_128bit);\n+  __ vpsrlq(ZT3, ZT3, 63, Assembler::AVX_128bit);\n+  __ movdqu(ZT2, ZT3);\n+  __ vpslldq(ZT3, ZT3, 8, Assembler::AVX_128bit);\n+  __ vpsrldq(ZT2, ZT2, 8, Assembler::AVX_128bit);\n+  __ vpor(HK, HK, ZT3, Assembler::AVX_128bit);\n+  __ vpshufd(ZT3, ZT2, 0x24, Assembler::AVX_128bit);\n+  __ vpcmpeqd(ZT3, ZT3, ZT12, Assembler::AVX_128bit);\n+  __ vpand(ZT3, ZT3, ZT11, Assembler::AVX_128bit);\n+  __ vpxor(HK, HK, ZT3, Assembler::AVX_128bit);\n+  __ movdqu(Address(avx512_htbl, 16 * 31), HK); \/\/ H ^ 2\n+\n@@ -2741,1 +2770,1 @@\n-  __ vinserti32x4(ZT7, ZT7, HK, 3);\n+  __ evinserti64x2(ZT7, ZT7, HK, 3, Assembler::AVX_512bit);\n@@ -2743,0 +2772,1 @@\n+  \/\/calculate HashKey ^ 2 << 1 mod poly\n@@ -2744,2 +2774,2 @@\n-  __ movdqu(Address(avx512_htbl, 16 * 46), ZT5); \/\/ H ^ 2 * 2\n-  __ vinserti32x4(ZT7, ZT7, ZT5, 2);\n+  __ movdqu(Address(avx512_htbl, 16 * 30), ZT5);\n+  __ evinserti64x2(ZT7, ZT7, ZT5, 2, Assembler::AVX_512bit);\n@@ -2747,0 +2777,1 @@\n+  \/\/calculate HashKey ^ 3 << 1 mod poly\n@@ -2748,2 +2779,2 @@\n-  __ movdqu(Address(avx512_htbl, 16 * 45), ZT5); \/\/ H ^ 2 * 3\n-  __ vinserti32x4(ZT7, ZT7, ZT5, 1);\n+  __ movdqu(Address(avx512_htbl, 16 * 29), ZT5);\n+  __ evinserti64x2(ZT7, ZT7, ZT5, 1, Assembler::AVX_512bit);\n@@ -2751,0 +2782,1 @@\n+  \/\/calculate HashKey ^ 4 << 1 mod poly\n@@ -2752,14 +2784,17 @@\n-  __ movdqu(Address(avx512_htbl, 16 * 44), ZT5); \/\/ H ^ 2 * 4\n-  __ vinserti32x4(ZT7, ZT7, ZT5, 0);\n-\n-  __ evshufi64x2(ZT5, ZT5, ZT5, 0x00, Assembler::AVX_512bit);\n-  __ evmovdquq(ZT8, ZT7, Assembler::AVX_512bit);\n-  gfmul_avx512(ZT7, ZT5);\n-  __ evmovdquq(Address(avx512_htbl, 16 * 40), ZT7, Assembler::AVX_512bit);\n-  __ evshufi64x2(ZT5, ZT7, ZT7, 0x00, Assembler::AVX_512bit);\n-  gfmul_avx512(ZT8, ZT5);\n-  __ evmovdquq(Address(avx512_htbl, 16 * 36), ZT8, Assembler::AVX_512bit);\n-  gfmul_avx512(ZT7, ZT5);\n-  __ evmovdquq(Address(avx512_htbl, 16 * 32), ZT7, Assembler::AVX_512bit);\n-  gfmul_avx512(ZT8, ZT5);\n-  __ evmovdquq(Address(avx512_htbl, 16 * 28), ZT8, Assembler::AVX_512bit);\n+  __ movdqu(Address(avx512_htbl, 16 * 28), ZT5);\n+  __ evinserti64x2(ZT7, ZT7, ZT5, 0, Assembler::AVX_512bit);\n+  \/\/ ZT5 amd ZT7 to be cleared(hash key)\n+  \/\/calculate HashKeyK = HashKey x POLY\n+  __ evmovdquq(xmm11, ExternalAddress(ghash_polynomial_addr()), Assembler::AVX_512bit, r15);\n+  __ evpclmulqdq(ZT1, ZT7, xmm11, 0x10, Assembler::AVX_512bit);\n+  __ vpshufd(ZT2, ZT7, 78, Assembler::AVX_512bit);\n+  __ evpxorq(ZT1, ZT1, ZT2, Assembler::AVX_512bit);\n+  __ evmovdquq(Address(avx512_htbl, 16 * 60), ZT1, Assembler::AVX_512bit);\n+  \/\/**ZT1 amd ZT2 to be cleared(hash key)\n+\n+  \/\/switch to 4x128 - bit computations now\n+  __ evshufi64x2(ZT5, ZT5, ZT5, 0x00, Assembler::AVX_512bit); \/\/;; broadcast HashKey ^ 4 across all ZT5\n+  __ evmovdquq(ZT8, ZT7, Assembler::AVX_512bit);\/\/; save HashKey ^ 4 to HashKey ^ 1 in ZT8\n+  \/\/**ZT8 to be cleared(hash key)\n+\n+  \/\/calculate HashKey ^ 5 << 1 mod poly, HashKey ^ 6 << 1 mod poly, ... HashKey ^ 8 << 1 mod poly\n@@ -2767,25 +2802,35 @@\n-  __ evmovdquq(Address(avx512_htbl, 16 * 24), ZT7, Assembler::AVX_512bit);\n-  gfmul_avx512(ZT8, ZT5);\n-  __ evmovdquq(Address(avx512_htbl, 16 * 20), ZT8, Assembler::AVX_512bit);\n-  gfmul_avx512(ZT7, ZT5);\n-  __ evmovdquq(Address(avx512_htbl, 16 * 16), ZT7, Assembler::AVX_512bit);\n-  gfmul_avx512(ZT8, ZT5);\n-  __ evmovdquq(Address(avx512_htbl, 16 * 12), ZT8, Assembler::AVX_512bit);\n-  gfmul_avx512(ZT7, ZT5);\n-  __ evmovdquq(Address(avx512_htbl, 16 * 8), ZT7, Assembler::AVX_512bit);\n-  gfmul_avx512(ZT8, ZT5);\n-  __ evmovdquq(Address(avx512_htbl, 16 * 4), ZT8, Assembler::AVX_512bit);\n-  gfmul_avx512(ZT7, ZT5);\n-  __ evmovdquq(Address(avx512_htbl, 16 * 0), ZT7, Assembler::AVX_512bit);\n-  __ ret(0);\n-}\n-\n-#define vclmul_reduce(out, poly, hi128, lo128, tmp0, tmp1)      \\\n-__ evpclmulqdq(tmp0, poly, lo128, 0x01, Assembler::AVX_512bit); \\\n-__ vpslldq(tmp0, tmp0, 8, Assembler::AVX_512bit);               \\\n-__ evpxorq(tmp0, lo128, tmp0, Assembler::AVX_512bit);           \\\n-__ evpclmulqdq(tmp1, poly, tmp0, 0x00, Assembler::AVX_512bit);  \\\n-__ vpsrldq(tmp1, tmp1, 4, Assembler::AVX_512bit);               \\\n-__ evpclmulqdq(out, poly, tmp0, 0x10, Assembler::AVX_512bit);   \\\n-__ vpslldq(out, out, 4, Assembler::AVX_512bit);                 \\\n-__ vpternlogq(out, 0x96, tmp1, hi128, Assembler::AVX_512bit);   \\\n+  __ evmovdquq(Address(avx512_htbl, 16 * 24), ZT7, Assembler::AVX_512bit);\/\/; HashKey ^ 8 to HashKey ^ 5 in ZT7 now\n+\n+  \/\/calculate HashKeyX = HashKey x POLY\n+  __ evpclmulqdq(ZT1, ZT7, xmm11, 0x10, Assembler::AVX_512bit);\n+  __ vpshufd(ZT2, ZT7, 78, Assembler::AVX_512bit);\n+  __ evpxorq(ZT1, ZT1, ZT2, Assembler::AVX_512bit);\n+  __ evmovdquq(Address(avx512_htbl, 16 * 56), ZT1, Assembler::AVX_512bit);\n+\n+  __ evshufi64x2(ZT5, ZT7, ZT7, 0x00, Assembler::AVX_512bit);\/\/;; broadcast HashKey ^ 8 across all ZT5\n+\n+  for (int i = 20, j = 52; i > 0;) {\n+    gfmul_avx512(ZT8, ZT5);\n+    __ evmovdquq(Address(avx512_htbl, 16 * i), ZT8, Assembler::AVX_512bit);\n+    \/\/calculate HashKeyK = HashKey x POLY\n+    __ evpclmulqdq(ZT1, ZT8, xmm11, 0x10, Assembler::AVX_512bit);\n+    __ vpshufd(ZT2, ZT8, 78, Assembler::AVX_512bit);\n+    __ evpxorq(ZT1, ZT1, ZT2, Assembler::AVX_512bit);\n+    __ evmovdquq(Address(avx512_htbl, 16 * j), ZT1, Assembler::AVX_512bit);\n+\n+    i -= 4;\n+    j -= 4;\n+    \/\/compute HashKey ^ (8 + n), HashKey ^ (7 + n), ... HashKey ^ (5 + n)\n+    gfmul_avx512(ZT7, ZT5);\n+    __ evmovdquq(Address(avx512_htbl, 16 * i), ZT7, Assembler::AVX_512bit);\n+\n+    \/\/calculate HashKeyK = HashKey x POLY\n+    __ evpclmulqdq(ZT1, ZT7, xmm11, 0x10, Assembler::AVX_512bit);\n+    __ vpshufd(ZT2, ZT7, 78, Assembler::AVX_512bit);\n+    __ evpxorq(ZT1, ZT1, ZT2, Assembler::AVX_512bit);\n+    __ evmovdquq(Address(avx512_htbl, 16 * j), ZT1, Assembler::AVX_512bit);\n+\n+    i -= 4;\n+    j -= 4;\n+  }\n+ }\n@@ -2823,15 +2868,11 @@\n-#define carrylessMultiply(dst00, dst01, dst10, dst11, ghdata, hkey) \\\n-__ evpclmulqdq(dst00, ghdata, hkey, 0x00, Assembler::AVX_512bit); \\\n-__ evpclmulqdq(dst01, ghdata, hkey, 0x01, Assembler::AVX_512bit); \\\n-__ evpclmulqdq(dst10, ghdata, hkey, 0x10, Assembler::AVX_512bit); \\\n-__ evpclmulqdq(dst11, ghdata, hkey, 0x11, Assembler::AVX_512bit); \\\n-\n-#define shuffleExorRnd1Key(dst0, dst1, dst2, dst3, shufmask, rndkey) \\\n-__ vpshufb(dst0, dst0, shufmask, Assembler::AVX_512bit); \\\n-__ evpxorq(dst0, dst0, rndkey, Assembler::AVX_512bit); \\\n-__ vpshufb(dst1, dst1, shufmask, Assembler::AVX_512bit); \\\n-__ evpxorq(dst1, dst1, rndkey, Assembler::AVX_512bit); \\\n-__ vpshufb(dst2, dst2, shufmask, Assembler::AVX_512bit); \\\n-__ evpxorq(dst2, dst2, rndkey, Assembler::AVX_512bit); \\\n-__ vpshufb(dst3, dst3, shufmask, Assembler::AVX_512bit); \\\n-__ evpxorq(dst3, dst3, rndkey, Assembler::AVX_512bit); \\\n+#define carrylessMultiply(dst00, dst01, dst10, dst11, ghdata, hkey2, hkey1) \\\n+__ evpclmulqdq(dst00, ghdata, hkey2, 0x00, Assembler::AVX_512bit); \\\n+__ evpclmulqdq(dst01, ghdata, hkey2, 0x10, Assembler::AVX_512bit); \\\n+__ evpclmulqdq(dst10, ghdata, hkey1, 0x01, Assembler::AVX_512bit); \\\n+__ evpclmulqdq(dst11, ghdata, hkey1, 0x11, Assembler::AVX_512bit); \\\n+\n+#define shuffle(dst0, dst1, dst2, dst3, src0, src1, src2, src3, shufmask) \\\n+__ vpshufb(dst0, src0, shufmask, Assembler::AVX_512bit); \\\n+__ vpshufb(dst1, src1, shufmask, Assembler::AVX_512bit); \\\n+__ vpshufb(dst2, src2, shufmask, Assembler::AVX_512bit); \\\n+__ vpshufb(dst3, src3, shufmask, Assembler::AVX_512bit); \\\n@@ -2851,4 +2892,5 @@\n-void StubGenerator::ghash16_encrypt16_parallel(Register key, Register subkeyHtbl, XMMRegister ctr_blockx, XMMRegister aad_hashx,\n-                                               Register in, Register out, Register data, Register pos, bool first_time_reduction, XMMRegister addmask, bool ghash_input, Register rounds,\n-                                               Register ghash_pos, bool final_reduction, int i, XMMRegister counter_inc_mask) {\n-  Label AES_192, AES_256, LAST_AES_RND;\n+\/\/schoolbook multiply of 16 blocks(8 x 16 bytes)\n+\/\/it is assumed that data read is already shuffledand\n+void StubGenerator::ghash16_avx512(bool start_ghash, bool do_reduction, bool uload_shuffle, bool hk_broadcast, bool do_hxor,\n+                                   Register in, Register pos, Register subkeyHtbl, XMMRegister HASH, XMMRegister SHUFM, int in_offset,\n+                                   int in_disp, int displacement, int hashkey_offset) {\n@@ -2859,0 +2901,1 @@\n+  const XMMRegister ZTMP4 = xmm6;\n@@ -2864,3 +2907,12 @@\n-  const XMMRegister ZTMP10 = xmm15;\n-  const XMMRegister ZTMP11 = xmm16;\n-  const XMMRegister ZTMP12 = xmm17;\n+  const XMMRegister ZTMPA = xmm26;\n+  const XMMRegister ZTMPB = xmm23;\n+  const XMMRegister GH = xmm24;\n+  const XMMRegister GL = xmm25;\n+  const int hkey_gap = 16 * 32;\n+\n+  if (uload_shuffle) {\n+    __ evmovdquq(ZTMP9, Address(subkeyHtbl,  in_offset * 16 + in_disp), Assembler::AVX_512bit);\n+    __ vpshufb(ZTMP9, ZTMP9, SHUFM, Assembler::AVX_512bit);\n+  } else {\n+    __ evmovdquq(ZTMP9, Address(subkeyHtbl, in_offset * 16 + in_disp), Assembler::AVX_512bit);\n+  }\n@@ -2868,10 +2920,12 @@\n-  const XMMRegister ZTMP13 = xmm19;\n-  const XMMRegister ZTMP14 = xmm20;\n-  const XMMRegister ZTMP15 = xmm21;\n-  const XMMRegister ZTMP16 = xmm30;\n-  const XMMRegister ZTMP17 = xmm31;\n-  const XMMRegister ZTMP18 = xmm1;\n-  const XMMRegister ZTMP19 = xmm2;\n-  const XMMRegister ZTMP20 = xmm8;\n-  const XMMRegister ZTMP21 = xmm22;\n-  const XMMRegister ZTMP22 = xmm23;\n+  if (start_ghash) {\n+    __ evpxorq(ZTMP9, ZTMP9, HASH, Assembler::AVX_512bit);\n+  }\n+  if (hk_broadcast) {\n+    __ evbroadcastf64x2(ZTMP8, Address(subkeyHtbl, hashkey_offset + displacement + 0 * 64), Assembler::AVX_512bit);\n+    __ evbroadcastf64x2(ZTMPA, Address(subkeyHtbl, hashkey_offset + displacement + hkey_gap + 0 * 64), Assembler::AVX_512bit);\n+  } else {\n+    __ evmovdquq(ZTMP8, Address(subkeyHtbl, hashkey_offset + displacement + 0 * 64), Assembler::AVX_512bit);\n+    __ evmovdquq(ZTMPA, Address(subkeyHtbl, hashkey_offset + displacement + hkey_gap + 0 * 64), Assembler::AVX_512bit);\n+  }\n+\n+  carrylessMultiply(ZTMP0, ZTMP1, ZTMP2, ZTMP3, ZTMP9, ZTMPA, ZTMP8);\n@@ -2879,24 +2933,6 @@\n-  \/\/ Pre increment counters\n-  __ vpaddd(ZTMP0, ctr_blockx, counter_inc_mask, Assembler::AVX_512bit);\n-  __ vpaddd(ZTMP1, ZTMP0, counter_inc_mask, Assembler::AVX_512bit);\n-  __ vpaddd(ZTMP2, ZTMP1, counter_inc_mask, Assembler::AVX_512bit);\n-  __ vpaddd(ZTMP3, ZTMP2, counter_inc_mask, Assembler::AVX_512bit);\n-  \/\/ Save counter value\n-  __ evmovdquq(ctr_blockx, ZTMP3, Assembler::AVX_512bit);\n-\n-  \/\/ Reuse ZTMP17 \/ ZTMP18 for loading AES Keys\n-  \/\/ Pre-load AES round keys\n-  ev_load_key(ZTMP17, key, 0, xmm29);\n-  ev_load_key(ZTMP18, key, 1 * 16, xmm29);\n-\n-  \/\/ ZTMP19 & ZTMP20 used for loading hash key\n-  \/\/ Pre-load hash key\n-  __ evmovdquq(ZTMP19, Address(subkeyHtbl, i * 64), Assembler::AVX_512bit);\n-  __ evmovdquq(ZTMP20, Address(subkeyHtbl, ++i * 64), Assembler::AVX_512bit);\n-  \/\/ Load data for computing ghash\n-  __ evmovdquq(ZTMP21, Address(data, ghash_pos, Address::times_1, 0 * 64), Assembler::AVX_512bit);\n-  __ vpshufb(ZTMP21, ZTMP21, xmm24, Assembler::AVX_512bit);\n-\n-  \/\/ Xor cipher block 0 with input ghash, if available\n-  if (ghash_input) {\n-    __ evpxorq(ZTMP21, ZTMP21, aad_hashx, Assembler::AVX_512bit);\n+  \/\/ghash blocks 4 - 7\n+  if (uload_shuffle) {\n+    __ evmovdquq(ZTMP9, Address(subkeyHtbl, in_offset * 16 + in_disp + 64), Assembler::AVX_512bit);\n+    __ vpshufb(ZTMP9, ZTMP9, SHUFM, Assembler::AVX_512bit);\n+  } else {\n+    __ evmovdquq(ZTMP9, Address(subkeyHtbl, in_offset * 16 + in_disp + 64), Assembler::AVX_512bit);\n@@ -2904,65 +2940,7 @@\n-  \/\/ Load data for computing ghash\n-  __ evmovdquq(ZTMP22, Address(data, ghash_pos, Address::times_1, 1 * 64), Assembler::AVX_512bit);\n-  __ vpshufb(ZTMP22, ZTMP22, xmm24, Assembler::AVX_512bit);\n-\n-  \/\/ stitch AES rounds with GHASH\n-  \/\/ AES round 0, xmm24 has shuffle mask\n-  shuffleExorRnd1Key(ZTMP0, ZTMP1, ZTMP2, ZTMP3, xmm24, ZTMP17);\n-  \/\/ Reuse ZTMP17 \/ ZTMP18 for loading remaining AES Keys\n-  ev_load_key(ZTMP17, key, 2 * 16, xmm29);\n-  \/\/ GHASH 4 blocks\n-  carrylessMultiply(ZTMP6, ZTMP7, ZTMP8, ZTMP5, ZTMP21, ZTMP19);\n-  \/\/ Load the next hkey and Ghash data\n-  __ evmovdquq(ZTMP19, Address(subkeyHtbl, ++i * 64), Assembler::AVX_512bit);\n-  __ evmovdquq(ZTMP21, Address(data, ghash_pos, Address::times_1, 2 * 64), Assembler::AVX_512bit);\n-  __ vpshufb(ZTMP21, ZTMP21, xmm24, Assembler::AVX_512bit);\n-\n-  \/\/ AES round 1\n-  roundEncode(ZTMP18, ZTMP0, ZTMP1, ZTMP2, ZTMP3);\n-  ev_load_key(ZTMP18, key, 3 * 16, xmm29);\n-\n-  \/\/ GHASH 4 blocks(11 to 8)\n-  carrylessMultiply(ZTMP10, ZTMP12, ZTMP11, ZTMP9, ZTMP22, ZTMP20);\n-  \/\/ Load the next hkey and GDATA\n-  __ evmovdquq(ZTMP20, Address(subkeyHtbl, ++i * 64), Assembler::AVX_512bit);\n-  __ evmovdquq(ZTMP22, Address(data, ghash_pos, Address::times_1, 3 * 64), Assembler::AVX_512bit);\n-  __ vpshufb(ZTMP22, ZTMP22, xmm24, Assembler::AVX_512bit);\n-\n-  \/\/ AES round 2\n-  roundEncode(ZTMP17, ZTMP0, ZTMP1, ZTMP2, ZTMP3);\n-  ev_load_key(ZTMP17, key, 4 * 16, xmm29);\n-\n-  \/\/ GHASH 4 blocks(7 to 4)\n-  carrylessMultiply(ZTMP14, ZTMP16, ZTMP15, ZTMP13, ZTMP21, ZTMP19);\n-  \/\/ AES rounds 3\n-  roundEncode(ZTMP18, ZTMP0, ZTMP1, ZTMP2, ZTMP3);\n-  ev_load_key(ZTMP18, key, 5 * 16, xmm29);\n-\n-  \/\/ Gather(XOR) GHASH for 12 blocks\n-  xorGHASH(ZTMP5, ZTMP6, ZTMP8, ZTMP7, ZTMP9, ZTMP13, ZTMP10, ZTMP14, ZTMP12, ZTMP16, ZTMP11, ZTMP15);\n-\n-  \/\/ AES rounds 4\n-  roundEncode(ZTMP17, ZTMP0, ZTMP1, ZTMP2, ZTMP3);\n-  ev_load_key(ZTMP17, key, 6 * 16, xmm29);\n-\n-  \/\/ load plain \/ cipher text(recycle registers)\n-  loadData(in, pos, ZTMP13, ZTMP14, ZTMP15, ZTMP16);\n-\n-  \/\/ AES rounds 5\n-  roundEncode(ZTMP18, ZTMP0, ZTMP1, ZTMP2, ZTMP3);\n-  ev_load_key(ZTMP18, key, 7 * 16, xmm29);\n-  \/\/ GHASH 4 blocks(3 to 0)\n-  carrylessMultiply(ZTMP10, ZTMP12, ZTMP11, ZTMP9, ZTMP22, ZTMP20);\n-\n-  \/\/  AES round 6\n-  roundEncode(ZTMP17, ZTMP0, ZTMP1, ZTMP2, ZTMP3);\n-  ev_load_key(ZTMP17, key, 8 * 16, xmm29);\n-\n-  \/\/ gather GHASH in ZTMP6(low) and ZTMP5(high)\n-  if (first_time_reduction) {\n-    __ vpternlogq(ZTMP7, 0x96, ZTMP8, ZTMP12, Assembler::AVX_512bit);\n-    __ evpxorq(xmm25, ZTMP7, ZTMP11, Assembler::AVX_512bit);\n-    __ evpxorq(xmm27, ZTMP5, ZTMP9, Assembler::AVX_512bit);\n-    __ evpxorq(xmm26, ZTMP6, ZTMP10, Assembler::AVX_512bit);\n-  } else if (!first_time_reduction && !final_reduction) {\n-    xorGHASH(ZTMP7, xmm25, xmm27, xmm26, ZTMP8, ZTMP12, ZTMP7, ZTMP11, ZTMP5, ZTMP9, ZTMP6, ZTMP10);\n+\n+  if (hk_broadcast) {\n+    __ evbroadcastf64x2(ZTMP8, Address(subkeyHtbl, hashkey_offset + displacement + 1 * 64), Assembler::AVX_512bit);;\n+    __ evbroadcastf64x2(ZTMPA, Address(subkeyHtbl, hashkey_offset + displacement + hkey_gap + 1 * 64), Assembler::AVX_512bit);\n+  } else {\n+    __ evmovdquq(ZTMP8, Address(subkeyHtbl, hashkey_offset + displacement + 1 * 64), Assembler::AVX_512bit);\n+    __ evmovdquq(ZTMPA, Address(subkeyHtbl, hashkey_offset + displacement + hkey_gap + 1 * 64), Assembler::AVX_512bit);\n@@ -2971,8 +2949,9 @@\n-  if (final_reduction) {\n-    \/\/ Phase one: Add mid products together\n-    \/\/ Also load polynomial constant for reduction\n-    __ vpternlogq(ZTMP7, 0x96, ZTMP8, ZTMP12, Assembler::AVX_512bit);\n-    __ vpternlogq(ZTMP7, 0x96, xmm25, ZTMP11, Assembler::AVX_512bit);\n-    __ vpsrldq(ZTMP11, ZTMP7, 8, Assembler::AVX_512bit);\n-    __ vpslldq(ZTMP7, ZTMP7, 8, Assembler::AVX_512bit);\n-    __ evmovdquq(ZTMP12, ExternalAddress(ghash_polynomial_reduction_addr()), Assembler::AVX_512bit, rbx \/*rscratch*\/);\n+  carrylessMultiply(ZTMP4, ZTMP5, ZTMP6, ZTMP7, ZTMP9, ZTMPA, ZTMP8);\n+\n+  \/\/update sums\n+  if (start_ghash) {\n+    __ evpxorq(GL, ZTMP0, ZTMP2, Assembler::AVX_512bit);\/\/T2 = THL + TLL\n+    __ evpxorq(GH, ZTMP1, ZTMP3, Assembler::AVX_512bit);\/\/T1 = THH + TLH\n+  } else { \/\/mid, end, end_reduce\n+    __ vpternlogq(GL, 0x96, ZTMP0, ZTMP2, Assembler::AVX_512bit);\/\/T2 = THL + TLL\n+    __ vpternlogq(GH, 0x96, ZTMP1, ZTMP3, Assembler::AVX_512bit);\/\/T1 = THH + TLH\n@@ -2980,8 +2959,6 @@\n-  \/\/ AES round 7\n-  roundEncode(ZTMP18, ZTMP0, ZTMP1, ZTMP2, ZTMP3);\n-  ev_load_key(ZTMP18, key, 9 * 16, xmm29);\n-  if (final_reduction) {\n-    __ vpternlogq(ZTMP5, 0x96, ZTMP9, ZTMP11, Assembler::AVX_512bit);\n-    __ evpxorq(ZTMP5, ZTMP5, xmm27, Assembler::AVX_512bit);\n-    __ vpternlogq(ZTMP6, 0x96, ZTMP10, ZTMP7, Assembler::AVX_512bit);\n-    __ evpxorq(ZTMP6, ZTMP6, xmm26, Assembler::AVX_512bit);\n+  \/\/ghash blocks 8 - 11\n+  if (uload_shuffle) {\n+    __ evmovdquq(ZTMP9, Address(subkeyHtbl, in_offset * 16 + in_disp + 128), Assembler::AVX_512bit);\n+    __ vpshufb(ZTMP9, ZTMP9, SHUFM, Assembler::AVX_512bit);\n+  } else {\n+    __ evmovdquq(ZTMP9, Address(subkeyHtbl, in_offset * 16 + in_disp + 128), Assembler::AVX_512bit);\n@@ -2989,8 +2966,6 @@\n-  \/\/ AES round 8\n-  roundEncode(ZTMP17, ZTMP0, ZTMP1, ZTMP2, ZTMP3);\n-  ev_load_key(ZTMP17, key, 10 * 16, xmm29);\n-\n-  \/\/ Horizontal xor of low and high 4*128\n-  if (final_reduction) {\n-    vhpxori4x128(ZTMP5, ZTMP9);\n-    vhpxori4x128(ZTMP6, ZTMP10);\n+  if (hk_broadcast) {\n+    __ evbroadcastf64x2(ZTMP8, Address(subkeyHtbl, hashkey_offset + displacement + 2 * 64), Assembler::AVX_512bit);\n+    __ evbroadcastf64x2(ZTMPA, Address(subkeyHtbl, hashkey_offset + displacement + hkey_gap + 2 * 64), Assembler::AVX_512bit);\n+  } else {\n+    __ evmovdquq(ZTMP8, Address(subkeyHtbl, hashkey_offset + displacement + 2 * 64), Assembler::AVX_512bit);\n+    __ evmovdquq(ZTMPA, Address(subkeyHtbl, hashkey_offset + displacement + hkey_gap + 2 * 64), Assembler::AVX_512bit);\n@@ -2998,7 +2973,20 @@\n-  \/\/ AES round 9\n-  roundEncode(ZTMP18, ZTMP0, ZTMP1, ZTMP2, ZTMP3);\n-  \/\/ First phase of reduction\n-  if (final_reduction) {\n-      __ evpclmulqdq(ZTMP10, ZTMP12, ZTMP6, 0x01, Assembler::AVX_128bit);\n-      __ vpslldq(ZTMP10, ZTMP10, 8, Assembler::AVX_128bit);\n-      __ evpxorq(ZTMP10, ZTMP6, ZTMP10, Assembler::AVX_128bit);\n+\n+  carrylessMultiply(ZTMP0, ZTMP1, ZTMP2, ZTMP3, ZTMP9, ZTMPA, ZTMP8);\n+\n+  \/\/update sums\n+  __ vpternlogq(GL, 0x96, ZTMP6, ZTMP4, Assembler::AVX_512bit);\/\/T2 = THL + TLL\n+  __ vpternlogq(GH, 0x96, ZTMP7, ZTMP5, Assembler::AVX_512bit);\/\/T1 = THH + TLH\n+  \/\/ghash blocks 12 - 15\n+  if (uload_shuffle) {\n+    __ evmovdquq(ZTMP9, Address(subkeyHtbl, in_offset * 16 + in_disp + 192), Assembler::AVX_512bit);\n+    __ vpshufb(ZTMP9, ZTMP9, SHUFM, Assembler::AVX_512bit);\n+  } else {\n+    __ evmovdquq(ZTMP9, Address(subkeyHtbl, in_offset * 16 + in_disp + 192), Assembler::AVX_512bit);\n+  }\n+\n+  if (hk_broadcast) {\n+    __ evbroadcastf64x2(ZTMP8, Address(subkeyHtbl, hashkey_offset + displacement + 3 * 64), Assembler::AVX_512bit);\n+    __ evbroadcastf64x2(ZTMPA, Address(subkeyHtbl, hashkey_offset + displacement + hkey_gap + 3 * 64), Assembler::AVX_512bit);\n+  } else {\n+    __ evmovdquq(ZTMP8, Address(subkeyHtbl, hashkey_offset + displacement + 3 * 64), Assembler::AVX_512bit);\n+    __ evmovdquq(ZTMPA, Address(subkeyHtbl, hashkey_offset + displacement + hkey_gap + 3 * 64), Assembler::AVX_512bit);\n@@ -3006,0 +2994,304 @@\n+  carrylessMultiply(ZTMP4, ZTMP5, ZTMP6, ZTMP7, ZTMP9, ZTMPA, ZTMP8);\n+\n+  \/\/update sums\n+  xorGHASH(GL, GH, GL, GH, ZTMP0, ZTMP2, ZTMP1, ZTMP3, ZTMP6, ZTMP4, ZTMP7, ZTMP5);\n+\n+  if (do_reduction) {\n+  \/\/new reduction\n+    __ evmovdquq(ZTMPB, ExternalAddress(ghash_polynomial_addr()), Assembler::AVX_512bit, rbx \/*rscratch*\/);\n+    __ evpclmulqdq(HASH, GL, ZTMPB, 0x10, Assembler::AVX_512bit);\n+    __ vpshufd(ZTMP0, GL, 78, Assembler::AVX_512bit);\n+    __ vpternlogq(HASH, 0x96, GH, ZTMP0, Assembler::AVX_512bit);\n+    if (do_hxor) {\n+      vhpxori4x128(HASH, ZTMP0);\n+    }\n+  }\n+}\n+\n+\/\/Stitched GHASH of 16 blocks(with reduction) with encryption of 0 blocks\n+void StubGenerator::gcm_enc_dec_last_avx512(Register len, Register in, Register pos, XMMRegister HASH, XMMRegister SHUFM, Register subkeyHtbl,\n+                                            int ghashin_offset, int hashkey_offset, bool start_ghash, bool do_reduction) {\n+  \/\/there is 0 blocks to cipher so there are only 16 blocks for ghash and reduction\n+  ghash16_avx512(start_ghash, do_reduction, false, false, true, in, pos, subkeyHtbl, HASH, SHUFM, ghashin_offset, 0, 0, hashkey_offset);\n+}\n+\n+\/\/Main GCM macro stitching cipher with GHASH\n+\/\/encrypts 16 blocks at a time\n+\/\/ghash the 16 previously encrypted ciphertext blocks\n+void StubGenerator::ghash16_encrypt_parallel16_avx512(Register in, Register out, Register ct, Register pos, Register avx512_subkeyHtbl,\n+                                                      Register CTR_CHECK, Register NROUNDS, Register key, XMMRegister CTR_BE, XMMRegister GHASH_IN,\n+                                                      XMMRegister ADDBE_4x4, XMMRegister ADDBE_1234, XMMRegister ADD_1234, XMMRegister SHFMSK,\n+                                                      bool hk_broadcast, bool is_hash_start, bool do_hash_reduction, bool do_hash_hxor,\n+                                                      bool no_ghash_in, int ghashin_offset, int aesout_offset, int hashkey_offset) {\n+  const XMMRegister B00_03 = xmm0;\n+  const XMMRegister B04_07 = xmm3;\n+  const XMMRegister B08_11 = xmm4;\n+  const XMMRegister B12_15 = xmm5;\n+  const XMMRegister THH1 = xmm6;\n+  const XMMRegister THL1 = xmm7;\n+  const XMMRegister TLH1 = xmm10;\n+  const XMMRegister TLL1 = xmm11, THH2 = xmm12, THL2 = xmm13, TLH2 = xmm15;\n+  const XMMRegister TLL2 = xmm16, THH3 = xmm17, THL3 = xmm19, TLH3 = xmm20;\n+  const XMMRegister TLL3 = xmm21, DATA1 = xmm17, DATA2 = xmm19, DATA3 = xmm20, DATA4 = xmm21;\n+  const XMMRegister AESKEY1 = xmm30, AESKEY2 = xmm31;\n+  const XMMRegister GHKEY1 = xmm1, GHKEY2 = xmm18, GHDAT1 = xmm8, GHDAT2 = xmm22;\n+  const XMMRegister ZT = xmm23, TO_REDUCE_L = xmm25, TO_REDUCE_H = xmm24;\n+  const int hkey_gap = 16 * 32;\n+\n+  Label blocks_overflow, blocks_ok, skip_shuffle, cont, aes_256, aes_192, last_aes_rnd;\n+\n+  __ cmpb(CTR_CHECK, (256 - 16));\n+  __ jcc(Assembler::aboveEqual, blocks_overflow);\n+  __ vpaddd(B00_03, CTR_BE, ADDBE_1234, Assembler::AVX_512bit);\n+  __ vpaddd(B04_07, B00_03, ADDBE_4x4, Assembler::AVX_512bit);\n+  __ vpaddd(B08_11, B04_07, ADDBE_4x4, Assembler::AVX_512bit);\n+  __ vpaddd(B12_15, B08_11, ADDBE_4x4, Assembler::AVX_512bit);\n+  __ jmp(blocks_ok);\n+  __ bind(blocks_overflow);\n+  __ vpshufb(CTR_BE, CTR_BE, SHFMSK, Assembler::AVX_512bit);\n+  __ evmovdquq(B12_15, ExternalAddress(counter_mask_linc4_addr()), Assembler::AVX_512bit, rbx \/*rscratch*\/);\n+  __ vpaddd(B00_03, CTR_BE, ADD_1234, Assembler::AVX_512bit);\n+  __ vpaddd(B04_07, B00_03, B12_15, Assembler::AVX_512bit);\n+  __ vpaddd(B08_11, B04_07, B12_15, Assembler::AVX_512bit);\n+  __ vpaddd(B12_15, B08_11, B12_15, Assembler::AVX_512bit);\n+  shuffle(B00_03, B04_07, B08_11, B12_15, B00_03, B04_07, B08_11, B12_15, SHFMSK);\n+\n+  __ bind(blocks_ok);\n+\n+  \/\/pre - load constants\n+  ev_load_key(AESKEY1, key, 0, rbx);\n+  if (!no_ghash_in) {\n+    __ evpxorq(GHDAT1, GHASH_IN, Address(avx512_subkeyHtbl, 16 * ghashin_offset), Assembler::AVX_512bit);\n+  } else {\n+    __ evmovdquq(GHDAT1, Address(avx512_subkeyHtbl, 16 * ghashin_offset), Assembler::AVX_512bit);\n+  }\n+\n+  if (hk_broadcast) {\n+    __ evbroadcastf64x2(GHKEY1, Address(avx512_subkeyHtbl, hashkey_offset + 0 * 64), Assembler::AVX_512bit);\n+    __ evbroadcastf64x2(GHKEY2, Address(avx512_subkeyHtbl, hashkey_offset + hkey_gap + 0 * 64), Assembler::AVX_512bit);\n+  } else {\n+    __ evmovdquq(GHKEY1, Address(avx512_subkeyHtbl, hashkey_offset + 0 * 64), Assembler::AVX_512bit);\n+    __ evmovdquq(GHKEY2, Address(avx512_subkeyHtbl, hashkey_offset + hkey_gap + 0 * 64), Assembler::AVX_512bit);\n+  }\n+\n+  \/\/save counter for the next round\n+  \/\/increment counter overflow check register\n+  __ evshufi64x2(CTR_BE, B12_15, B12_15, 255, Assembler::AVX_512bit);\n+  __ addb(CTR_CHECK, 16);\n+\n+  \/\/pre - load constants\n+  ev_load_key(AESKEY2, key, 1 * 16, rbx);\n+  __ evmovdquq(GHDAT2, Address(avx512_subkeyHtbl, 16 * (ghashin_offset +4)), Assembler::AVX_512bit);\n+\n+  \/\/stitch AES rounds with GHASH\n+  \/\/AES round 0\n+  __ evpxorq(B00_03, B00_03, AESKEY1, Assembler::AVX_512bit);\n+  __ evpxorq(B04_07, B04_07, AESKEY1, Assembler::AVX_512bit);\n+  __ evpxorq(B08_11, B08_11, AESKEY1, Assembler::AVX_512bit);\n+  __ evpxorq(B12_15, B12_15, AESKEY1, Assembler::AVX_512bit);\n+  ev_load_key(AESKEY1, key, 2 * 16, rbx);\n+\n+  \/\/GHASH 4 blocks(15 to 12)\n+  carrylessMultiply(TLL1, TLH1, THL1, THH1, GHDAT1, GHKEY2, GHKEY1);\n+\n+  if (hk_broadcast) {\n+    __ evbroadcastf64x2(GHKEY1, Address(avx512_subkeyHtbl, hashkey_offset + 1 * 64), Assembler::AVX_512bit);\n+    __ evbroadcastf64x2(GHKEY2, Address(avx512_subkeyHtbl, hashkey_offset + hkey_gap + 1 * 64), Assembler::AVX_512bit);\n+  } else {\n+    __ evmovdquq(GHKEY1, Address(avx512_subkeyHtbl, hashkey_offset + 1 * 64), Assembler::AVX_512bit);\n+    __ evmovdquq(GHKEY2, Address(avx512_subkeyHtbl, hashkey_offset + hkey_gap + 1 * 64), Assembler::AVX_512bit);\n+  }\n+\n+  __ evmovdquq(GHDAT1, Address(avx512_subkeyHtbl, 16 * (ghashin_offset + 8)), Assembler::AVX_512bit);\n+\n+  \/\/AES round 1\n+  roundEncode(AESKEY2, B00_03, B04_07, B08_11, B12_15);\n+\n+  ev_load_key(AESKEY2, key, 3 * 16, rbx);\n+\n+  \/\/GHASH 4 blocks(11 to 8)\n+  carrylessMultiply(TLL2, TLH2, THL2, THH2, GHDAT2, GHKEY2, GHKEY1);\n+\n+  if (hk_broadcast) {\n+    __ evbroadcastf64x2(GHKEY1, Address(avx512_subkeyHtbl, hashkey_offset + 2 * 64), Assembler::AVX_512bit);\n+    __ evbroadcastf64x2(GHKEY2, Address(avx512_subkeyHtbl, hashkey_offset + hkey_gap + 2 * 64), Assembler::AVX_512bit);\n+  } else {\n+    __ evmovdquq(GHKEY1, Address(avx512_subkeyHtbl, hashkey_offset + 2 * 64 ), Assembler::AVX_512bit);\n+    __ evmovdquq(GHKEY2, Address(avx512_subkeyHtbl, hashkey_offset + hkey_gap + 2 * 64), Assembler::AVX_512bit);\n+  }\n+  __ evmovdquq(GHDAT2, Address(avx512_subkeyHtbl, 16 * (ghashin_offset + 12)), Assembler::AVX_512bit);\n+\n+  \/\/AES round 2\n+  roundEncode(AESKEY1, B00_03, B04_07, B08_11, B12_15);\n+  ev_load_key(AESKEY1, key, 4 * 16, rbx);\n+\n+  \/\/GHASH 4 blocks(7 to 4)\n+  carrylessMultiply(TLL3, TLH3, THL3, THH3, GHDAT1, GHKEY2, GHKEY1);\n+\n+  if (hk_broadcast) {\n+    __ evbroadcastf64x2(GHKEY1, Address(avx512_subkeyHtbl, hashkey_offset + 3 * 64), Assembler::AVX_512bit);\n+    __ evbroadcastf64x2(GHKEY2, Address(avx512_subkeyHtbl, hashkey_offset + hkey_gap + 3 * 64), Assembler::AVX_512bit);\n+  } else {\n+    __ evmovdquq(GHKEY1, Address(avx512_subkeyHtbl, hashkey_offset + 3 * 64), Assembler::AVX_512bit);\n+    __ evmovdquq(GHKEY2, Address(avx512_subkeyHtbl, hashkey_offset + hkey_gap + 3 * 64), Assembler::AVX_512bit);\n+  }\n+\n+  \/\/AES rounds 3\n+  roundEncode(AESKEY2, B00_03, B04_07, B08_11, B12_15);\n+  ev_load_key(AESKEY2, key, 5 * 16, rbx);\n+\n+  \/\/Gather(XOR) GHASH for 12 blocks\n+  xorGHASH(TLL1, TLH1, THL1, THH1, TLL2, TLL3, TLH2, TLH3, THL2, THL3, THH2, THH3);\n+\n+  \/\/AES rounds 4\n+  roundEncode(AESKEY1, B00_03, B04_07, B08_11, B12_15);\n+  ev_load_key(AESKEY1, key, 6 * 16, rbx);\n+\n+  \/\/load plain \/ cipher text(recycle GH3xx registers)\n+  loadData(in, pos, DATA1, DATA2, DATA3, DATA4);\n+\n+  \/\/AES rounds 5\n+  roundEncode(AESKEY2, B00_03, B04_07, B08_11, B12_15);\n+  ev_load_key(AESKEY2, key, 7 * 16, rbx);\n+\n+  \/\/GHASH 4 blocks(3 to 0)\n+  carrylessMultiply(TLL2, TLH2, THL2, THH2, GHDAT2, GHKEY2, GHKEY1);\n+\n+  \/\/AES round 6\n+  roundEncode(AESKEY1, B00_03, B04_07, B08_11, B12_15);\n+  ev_load_key(AESKEY1, key, 8 * 16, rbx);\n+\n+  \/\/gather GHASH in TO_REDUCE_H \/ L\n+  if (is_hash_start) {\n+    __ evpxorq(TO_REDUCE_L, TLL2, THL2, Assembler::AVX_512bit);\n+    __ evpxorq(TO_REDUCE_H, THH2, TLH2, Assembler::AVX_512bit);\n+    __ vpternlogq(TO_REDUCE_L, 0x96, TLL1, THL1, Assembler::AVX_512bit);\n+    __ vpternlogq(TO_REDUCE_H, 0x96, THH1, TLH1, Assembler::AVX_512bit);\n+  } else {\n+    \/\/not the first round so sums need to be updated\n+    xorGHASH(TO_REDUCE_L, TO_REDUCE_H, TO_REDUCE_L, TO_REDUCE_H, TLL2, THL2, THH2, TLH2, TLL1, THL1, THH1, TLH1);\n+  }\n+\n+  \/\/AES round 7\n+  roundEncode(AESKEY2, B00_03, B04_07, B08_11, B12_15);\n+  ev_load_key(AESKEY2, key, 9 * 16, rbx);\n+\n+  \/\/new reduction\n+  if (do_hash_reduction) {\n+    __ evmovdquq(ZT, ExternalAddress(ghash_polynomial_reduction_addr()), Assembler::AVX_512bit, rbx \/*rscratch*\/);\n+    __ evpclmulqdq(THH1, TO_REDUCE_L, ZT, 0x10, Assembler::AVX_512bit);\n+    __ vpshufd(TO_REDUCE_L, TO_REDUCE_L, 78, Assembler::AVX_512bit);\n+    __ vpternlogq(THH1, 0x96, TO_REDUCE_H, TO_REDUCE_L, Assembler::AVX_512bit);\n+  }\n+\n+  \/\/AES round 8\n+  roundEncode(AESKEY1, B00_03, B04_07, B08_11, B12_15);\n+  ev_load_key(AESKEY1, key, 10 * 16, rbx);\n+\n+  \/\/horizontalxor of 4 reduced hashes\n+  if (do_hash_hxor) {\n+    vhpxori4x128(THH1, TLL1);\n+  }\n+\n+  \/\/AES round 9\n+  roundEncode(AESKEY2, B00_03, B04_07, B08_11, B12_15);\n+  ev_load_key(AESKEY2, key, 11 * 16, rbx);\n+  \/\/AES rounds up to 11 (AES192) or 13 (AES256)\n+  \/\/AES128 is done\n+  __ cmpl(NROUNDS, 52);\n+  __ jcc(Assembler::less, last_aes_rnd);\n+  __ bind(aes_192);\n+  roundEncode(AESKEY1, B00_03, B04_07, B08_11, B12_15);\n+  ev_load_key(AESKEY1, key, 12 * 16, rbx);\n+  roundEncode(AESKEY2, B00_03, B04_07, B08_11, B12_15);\n+  __ cmpl(NROUNDS, 60);\n+  __ jcc(Assembler::less, last_aes_rnd);\n+  __ bind(aes_256);\n+  ev_load_key(AESKEY2, key, 13 * 16, rbx);\n+  roundEncode(AESKEY1, B00_03, B04_07, B08_11, B12_15);\n+  ev_load_key(AESKEY1, key, 14 * 16, rbx);\n+  roundEncode(AESKEY2, B00_03, B04_07, B08_11, B12_15);\n+\n+  __ bind(last_aes_rnd);\n+  \/\/the last AES round\n+  lastroundEncode(AESKEY1, B00_03, B04_07, B08_11, B12_15);\n+  \/\/AESKEY1and AESKEY2 contain AES round keys\n+\n+  \/\/XOR against plain \/ cipher text\n+  xorBeforeStore(B00_03, B04_07, B08_11, B12_15, DATA1, DATA2, DATA3, DATA4);\n+\n+  \/\/store cipher \/ plain text\n+  storeData(out, pos, B00_03, B04_07, B08_11, B12_15);\n+  \/\/**B00_03, B04_07, B08_011, B12_B15 may contain sensitive data\n+\n+  \/\/shuffle cipher text blocks for GHASH computation\n+  __ cmpptr(ct, out);\n+  __ jcc(Assembler::notEqual, skip_shuffle);\n+  shuffle(B00_03, B04_07, B08_11, B12_15, B00_03, B04_07, B08_11, B12_15, SHFMSK);\n+  __ jmp(cont);\n+  __ bind(skip_shuffle);\n+  shuffle(B00_03, B04_07, B08_11, B12_15, DATA1, DATA2, DATA3, DATA4, SHFMSK);\n+\n+  \/\/**B00_03, B04_07, B08_011, B12_B15 overwritten with shuffled cipher text\n+  __ bind(cont);\n+  \/\/store shuffled cipher text for ghashing\n+  __ evmovdquq(Address(avx512_subkeyHtbl, 16 * aesout_offset), B00_03, Assembler::AVX_512bit);\n+  __ evmovdquq(Address(avx512_subkeyHtbl, 16 * (aesout_offset + 4)), B04_07, Assembler::AVX_512bit);\n+  __ evmovdquq(Address(avx512_subkeyHtbl, 16 * (aesout_offset + 8)), B08_11, Assembler::AVX_512bit);\n+  __ evmovdquq(Address(avx512_subkeyHtbl, 16 * (aesout_offset + 12)), B12_15, Assembler::AVX_512bit);\n+}\n+\n+\n+\/\/Encrypt \/ decrypt the initial 16 blocks\n+void StubGenerator::initial_blocks_16_avx512(Register in, Register out, Register ct, Register pos, Register key, Register avx512_subkeyHtbl,\n+                                             Register CTR_CHECK, Register rounds, XMMRegister CTR, XMMRegister GHASH,  XMMRegister ADDBE_4x4,\n+                                             XMMRegister ADDBE_1234, XMMRegister ADD_1234, XMMRegister SHUF_MASK, int stack_offset) {\n+  const XMMRegister B00_03 = xmm7;\n+  const XMMRegister B04_07 = xmm10;\n+  const XMMRegister B08_11 = xmm11;\n+  const XMMRegister B12_15 = xmm12;\n+  const XMMRegister T0 = xmm0;\n+  const XMMRegister T1 = xmm3;\n+  const XMMRegister T2 = xmm4;\n+  const XMMRegister T3 = xmm5;\n+  const XMMRegister T4 = xmm6;\n+  const XMMRegister T5 = xmm30;\n+\n+  Label next_16_overflow, next_16_ok, cont, skip_shuffle, aes_256, aes_192, last_aes_rnd;\n+  \/\/prepare counter blocks\n+  __ cmpb(CTR_CHECK, (256 - 16));\n+  __ jcc(Assembler::aboveEqual, next_16_overflow);\n+  __ vpaddd(B00_03, CTR, ADDBE_1234, Assembler::AVX_512bit);\n+  __ vpaddd(B04_07, B00_03, ADDBE_4x4, Assembler::AVX_512bit);\n+  __ vpaddd(B08_11, B04_07, ADDBE_4x4, Assembler::AVX_512bit);\n+  __ vpaddd(B12_15, B08_11, ADDBE_4x4, Assembler::AVX_512bit);\n+  __ jmp(next_16_ok);\n+  __ bind(next_16_overflow);\n+  __ vpshufb(CTR, CTR, SHUF_MASK, Assembler::AVX_512bit);\n+  __ evmovdquq(B12_15, ExternalAddress(counter_mask_linc4_addr()), Assembler::AVX_512bit, rbx);\n+  __ vpaddd(B00_03, CTR, ADD_1234, Assembler::AVX_512bit);\n+  __ vpaddd(B04_07, B00_03, B12_15, Assembler::AVX_512bit);\n+  __ vpaddd(B08_11, B04_07, B12_15, Assembler::AVX_512bit);\n+  __ vpaddd(B12_15, B08_11, B12_15, Assembler::AVX_512bit);\n+  shuffle(B00_03, B04_07, B08_11, B12_15, B00_03, B04_07, B08_11, B12_15, SHUF_MASK);\n+  __ bind(next_16_ok);\n+  __ evshufi64x2(CTR, B12_15, B12_15, 255, Assembler::AVX_512bit);\n+  __ addb(CTR_CHECK, 16);\n+\n+  \/\/load 16 blocks of data\n+  loadData(in, pos, T0, T1, T2, T3);\n+\n+  \/\/move to AES encryption rounds\n+  __ movdqu(T5, ExternalAddress(key_shuffle_mask_addr()), rbx \/*rscratch*\/);\n+  ev_load_key(T4, key, 0, T5);\n+  __ evpxorq(B00_03, B00_03, T4, Assembler::AVX_512bit);\n+  __ evpxorq(B04_07, B04_07, T4, Assembler::AVX_512bit);\n+  __ evpxorq(B08_11, B08_11, T4, Assembler::AVX_512bit);\n+  __ evpxorq(B12_15, B12_15, T4, Assembler::AVX_512bit);\n+\n+  for (int i = 1; i < 10; i++) {\n+    ev_load_key(T4, key, i * 16, T5);\n+    roundEncode(T4, B00_03, B04_07, B08_11, B12_15);\n+  }\n+\n+  ev_load_key(T4, key, 10 * 16, T5);\n@@ -3007,8 +3299,6 @@\n-  __ jcc(Assembler::greaterEqual, AES_192);\n-  __ jmp(LAST_AES_RND);\n-  \/\/ AES rounds up to 11 (AES192) or 13 (AES256)\n-  __ bind(AES_192);\n-  roundEncode(ZTMP17, ZTMP0, ZTMP1, ZTMP2, ZTMP3);\n-  ev_load_key(ZTMP18, key, 11 * 16, xmm29);\n-  roundEncode(ZTMP18, ZTMP0, ZTMP1, ZTMP2, ZTMP3);\n-  ev_load_key(ZTMP17, key, 12 * 16, xmm29);\n+  __ jcc(Assembler::less, last_aes_rnd);\n+  __ bind(aes_192);\n+  roundEncode(T4, B00_03, B04_07, B08_11, B12_15);\n+  ev_load_key(T4, key, 16 * 11, T5);\n+  roundEncode(T4, B00_03, B04_07, B08_11, B12_15);\n+  ev_load_key(T4, key, 16 * 12, T5);\n@@ -3016,25 +3306,30 @@\n-  __ jcc(Assembler::aboveEqual, AES_256);\n-  __ jmp(LAST_AES_RND);\n-\n-  __ bind(AES_256);\n-  roundEncode(ZTMP17, ZTMP0, ZTMP1, ZTMP2, ZTMP3);\n-  ev_load_key(ZTMP18, key, 13 * 16, xmm29);\n-  roundEncode(ZTMP18, ZTMP0, ZTMP1, ZTMP2, ZTMP3);\n-  ev_load_key(ZTMP17, key, 14 * 16, xmm29);\n-\n-  __ bind(LAST_AES_RND);\n-  \/\/ Second phase of reduction\n-  if (final_reduction) {\n-    __ evpclmulqdq(ZTMP9, ZTMP12, ZTMP10, 0x00, Assembler::AVX_128bit);\n-    __ vpsrldq(ZTMP9, ZTMP9, 4, Assembler::AVX_128bit); \/\/ Shift-R 1-DW to obtain 2-DWs shift-R\n-    __ evpclmulqdq(ZTMP11, ZTMP12, ZTMP10, 0x10, Assembler::AVX_128bit);\n-    __ vpslldq(ZTMP11, ZTMP11, 4, Assembler::AVX_128bit); \/\/ Shift-L 1-DW for result\n-    \/\/ ZTMP5 = ZTMP5 X ZTMP11 X ZTMP9\n-    __ vpternlogq(ZTMP5, 0x96, ZTMP11, ZTMP9, Assembler::AVX_128bit);\n-  }\n-  \/\/ Last AES round\n-  lastroundEncode(ZTMP17, ZTMP0, ZTMP1, ZTMP2, ZTMP3);\n-  \/\/ XOR against plain \/ cipher text\n-  xorBeforeStore(ZTMP0, ZTMP1, ZTMP2, ZTMP3, ZTMP13, ZTMP14, ZTMP15, ZTMP16);\n-  \/\/ store cipher \/ plain text\n-  storeData(out, pos, ZTMP0, ZTMP1, ZTMP2, ZTMP3);\n+  __ jcc(Assembler::less, last_aes_rnd);\n+  __ bind(aes_256);\n+  roundEncode(T4, B00_03, B04_07, B08_11, B12_15);\n+  ev_load_key(T4, key, 16 * 13, T5);\n+  roundEncode(T4, B00_03, B04_07, B08_11, B12_15);\n+  ev_load_key(T4, key, 16 * 14, T5);\n+\n+  __ bind(last_aes_rnd);\n+  lastroundEncode(T4, B00_03, B04_07, B08_11, B12_15);\n+\n+  \/\/xor against text\n+  xorBeforeStore(B00_03, B04_07, B08_11, B12_15, T0, T1, T2, T3);\n+\n+  \/\/store\n+  storeData(out, pos, B00_03, B04_07, B08_11, B12_15);\n+\n+  __ cmpptr(ct, out);\n+  __ jcc(Assembler::equal, skip_shuffle);\n+  \/\/decryption - cipher text needs to go to GHASH phase\n+  shuffle(B00_03, B04_07, B08_11, B12_15, T0, T1, T2, T3, SHUF_MASK);\n+  __ jmp(cont);\n+  __ bind(skip_shuffle);\n+  shuffle(B00_03, B04_07, B08_11, B12_15, B00_03, B04_07, B08_11, B12_15, SHUF_MASK);\n+\n+  \/\/B00_03, B04_07, B08_11, B12_15 overwritten with shuffled cipher text\n+  __ bind(cont);\n+  __ evmovdquq(Address(avx512_subkeyHtbl, 16 * stack_offset), B00_03, Assembler::AVX_512bit);\n+  __ evmovdquq(Address(avx512_subkeyHtbl, 16 * (stack_offset + 4)), B04_07, Assembler::AVX_512bit);\n+  __ evmovdquq(Address(avx512_subkeyHtbl, 16 * (stack_offset + 8)), B08_11, Assembler::AVX_512bit);\n+  __ evmovdquq(Address(avx512_subkeyHtbl, 16 * (stack_offset + 12)), B12_15, Assembler::AVX_512bit);\n@@ -3043,5 +3338,5 @@\n-void StubGenerator::aesgcm_encrypt(Register in, Register len, Register ct, Register out, Register key,\n-                                   Register state, Register subkeyHtbl, Register avx512_subkeyHtbl, Register counter) {\n-  Label ENC_DEC_DONE, GENERATE_HTBL_48_BLKS, AES_192, AES_256, STORE_CT, GHASH_LAST_32,\n-        AES_32_BLOCKS, GHASH_AES_PARALLEL, LOOP, ACCUMULATE, GHASH_16_AES_16;\n-  const XMMRegister CTR_BLOCKx = xmm9;\n+void StubGenerator::aesgcm_avx512(Register in, Register len, Register ct, Register out, Register key, Register state,\n+                                  Register subkeyHtbl, Register avx512_subkeyHtbl, Register counter) {\n+  Label ENC_DEC_DONE, MESG_BELOW_32_BLKS, NO_BIG_BLKS, ENCRYPT_BIG_BLKS_NO_HXOR,\n+        ENCRYPT_BIG_NBLKS, ENCRYPT_16_BLKS, ENCRYPT_N_GHASH_32_N_BLKS, GHASH_DONE;\n+  const XMMRegister CTR_BLOCKx = xmm2;\n@@ -3049,3 +3344,0 @@\n-  const Register pos = rax;\n-  const Register rounds = r15;\n-  const Register ghash_pos = NOT_WIN64( r14) WIN64_ONLY( r11 );\n@@ -3053,3 +3345,3 @@\n-  const XMMRegister ZTMP1 = xmm3;\n-  const XMMRegister ZTMP2 = xmm4;\n-  const XMMRegister ZTMP3 = xmm5;\n+  const XMMRegister ZTMP1 = xmm3; \/\/**sensitive\n+  const XMMRegister ZTMP2 = xmm4; \/\/**sensitive(small data)\n+  const XMMRegister ZTMP3 = xmm5; \/\/**sensitive(small data)\n@@ -3069,6 +3361,17 @@\n-  const XMMRegister COUNTER_INC_MASK = xmm18;\n-\n-  __ movl(pos, 0); \/\/ Total length processed\n-  \/\/ Min data size processed = 768 bytes\n-  __ cmpl(len, 768);\n-  __ jcc(Assembler::less, ENC_DEC_DONE);\n+  const XMMRegister ZTMP17 = xmm31;\n+  const XMMRegister ZTMP18 = xmm1;\n+  const XMMRegister ZTMP19 = xmm18;\n+  const XMMRegister ZTMP20 = xmm8;\n+  const XMMRegister ZTMP21 = xmm22;\n+  const XMMRegister ZTMP22 = xmm23;\n+  const XMMRegister ZTMP23 = xmm26;\n+  const XMMRegister GH = xmm24;\n+  const XMMRegister GL = xmm25;\n+  const XMMRegister SHUF_MASK = xmm29;\n+  const XMMRegister ADDBE_4x4 = xmm27;\n+  const XMMRegister ADDBE_1234 = xmm28;\n+  const XMMRegister ADD_1234 = xmm9;\n+  const KRegister MASKREG = k1;\n+  const Register pos = rax;\n+  const Register rounds = r15;\n+  const Register CTR_CHECK = r14;\n@@ -3076,4 +3379,7 @@\n-  \/\/ Generate 48 constants for htbl\n-  __ call(GENERATE_HTBL_48_BLKS, relocInfo::none);\n-  int index = 0; \/\/ Index for choosing subkeyHtbl entry\n-  __ movl(ghash_pos, 0); \/\/ Pointer for ghash read and store operations\n+  const int stack_offset = 64;\n+  const int ghashin_offset = 64;\n+  const int aesout_offset = 64;\n+  const int hashkey_offset = 0;\n+  const int hashkey_gap = 16 * 32;\n+  const int HashKey_32 = 0;\n+  const int HashKey_16 = 16 * 16;\n@@ -3081,1 +3387,34 @@\n-  \/\/ Move initial counter value and STATE value into variables\n+  __ movl(pos, 0);\n+  __ cmpl(len, 256);\n+  __ jcc(Assembler::lessEqual, ENC_DEC_DONE);\n+\n+  \/* Structure of the Htbl is as follows:\n+  *   Where 0 - 31 we have 32 Hashkey's and 32-63 we have 32 HashKeyK (derived from HashKey)\n+  *   Rest 8 entries are for storing CTR values post AES rounds\n+  * ----------------------------------------------------------------------------------------\n+      Hashkey32 -> 16 * 0\n+      Hashkey31 -> 16 * 1\n+      Hashkey30 -> 16 * 2\n+      ........\n+      Hashkey1 -> 16 * 31\n+      ---------------------\n+      HaskeyK32 -> 16 * 32\n+      HashkeyK31 -> 16 * 33\n+      .........\n+      HashkeyK1 -> 16 * 63\n+      ---------------------\n+      1st set of AES Entries\n+      B00_03 -> 16 * 64\n+      B04_07 -> 16 * 68\n+      B08_11 -> 16 * 72\n+      B12_15 -> 16 * 80\n+      ---------------------\n+      2nd set of AES Entries\n+      B00_03 -> 16 * 84\n+      B04_07 -> 16 * 88\n+      B08_11 -> 16 * 92\n+      B12_15 -> 16 * 96\n+      ---------------------*\/\n+  generateHtbl_32_blocks_avx512(subkeyHtbl, avx512_subkeyHtbl);\n+\n+  \/\/Move initial counter value and STATE value into variables\n@@ -3084,1 +3423,2 @@\n-  \/\/ Load lswap mask for ghash\n+\n+  \/\/Load lswap mask for ghash\n@@ -3086,1 +3426,1 @@\n-  \/\/ Shuffle input state using lswap mask\n+  \/\/Shuffle input state using lswap mask\n@@ -3092,1 +3432,8 @@\n-  \/\/ Broadcast counter value to 512 bit register\n+  __ evmovdquq(ADDBE_4x4, ExternalAddress(counter_mask_addbe_4444_addr()), Assembler::AVX_512bit, rbx \/*rscratch*\/);\n+  __ evmovdquq(ADDBE_1234, ExternalAddress(counter_mask_addbe_1234_addr()), Assembler::AVX_512bit, rbx \/*rscratch*\/);\n+  __ evmovdquq(SHUF_MASK, ExternalAddress(counter_shuffle_mask_addr()), Assembler::AVX_512bit, rbx \/*rscratch*\/);\n+  __ evmovdquq(ADD_1234, ExternalAddress(counter_mask_add_1234_addr()), Assembler::AVX_512bit, rbx \/*rscratch*\/);\n+\n+  \/\/Shuffle counter, subtract 1 from the pre-incremented counter value and broadcast counter value to 512 bit register\n+  __ vpshufb(CTR_BLOCKx, CTR_BLOCKx, SHUF_MASK, Assembler::AVX_128bit);\n+  __ vpsubd(CTR_BLOCKx, CTR_BLOCKx, ADD_1234, Assembler::AVX_128bit);\n@@ -3094,171 +3441,0 @@\n-  \/\/ Load counter shuffle mask\n-  __ evmovdquq(xmm24, ExternalAddress(counter_shuffle_mask_addr()), Assembler::AVX_512bit, rbx \/*rscratch*\/);\n-  \/\/ Shuffle counter\n-  __ vpshufb(CTR_BLOCKx, CTR_BLOCKx, xmm24, Assembler::AVX_512bit);\n-\n-  \/\/ Load mask for incrementing counter\n-  __ evmovdquq(COUNTER_INC_MASK, ExternalAddress(counter_mask_linc4_addr()), Assembler::AVX_512bit, rbx \/*rscratch*\/);\n-  \/\/ Pre-increment counter\n-  __ vpaddd(ZTMP5, CTR_BLOCKx, ExternalAddress(counter_mask_linc0_addr()), Assembler::AVX_512bit, rbx \/*rscratch*\/);\n-  __ vpaddd(ZTMP6, ZTMP5, COUNTER_INC_MASK, Assembler::AVX_512bit);\n-  __ vpaddd(ZTMP7, ZTMP6, COUNTER_INC_MASK, Assembler::AVX_512bit);\n-  __ vpaddd(ZTMP8, ZTMP7, COUNTER_INC_MASK, Assembler::AVX_512bit);\n-\n-  \/\/ Begin 32 blocks of AES processing\n-  __ bind(AES_32_BLOCKS);\n-  \/\/ Save incremented counter before overwriting it with AES data\n-  __ evmovdquq(CTR_BLOCKx, ZTMP8, Assembler::AVX_512bit);\n-\n-  \/\/ Move 256 bytes of data\n-  loadData(in, pos, ZTMP0, ZTMP1, ZTMP2, ZTMP3);\n-  \/\/ Load key shuffle mask\n-  __ movdqu(xmm29, ExternalAddress(key_shuffle_mask_addr()), rbx \/*rscratch*\/);\n-  \/\/ Load 0th AES round key\n-  ev_load_key(ZTMP4, key, 0, xmm29);\n-  \/\/ AES-ROUND0, xmm24 has the shuffle mask\n-  shuffleExorRnd1Key(ZTMP5, ZTMP6, ZTMP7, ZTMP8, xmm24, ZTMP4);\n-\n-  for (int j = 1; j < 10; j++) {\n-      ev_load_key(ZTMP4, key, j * 16, xmm29);\n-      roundEncode(ZTMP4, ZTMP5, ZTMP6, ZTMP7, ZTMP8);\n-  }\n-  ev_load_key(ZTMP4, key, 10 * 16, xmm29);\n-  \/\/ AES rounds up to 11 (AES192) or 13 (AES256)\n-  __ cmpl(rounds, 52);\n-  __ jcc(Assembler::greaterEqual, AES_192);\n-  lastroundEncode(ZTMP4, ZTMP5, ZTMP6, ZTMP7, ZTMP8);\n-  __ jmp(STORE_CT);\n-\n-  __ bind(AES_192);\n-  roundEncode(ZTMP4, ZTMP5, ZTMP6, ZTMP7, ZTMP8);\n-  ev_load_key(ZTMP4, key, 11 * 16, xmm29);\n-  roundEncode(ZTMP4, ZTMP5, ZTMP6, ZTMP7, ZTMP8);\n-  __ cmpl(rounds, 60);\n-  __ jcc(Assembler::aboveEqual, AES_256);\n-  ev_load_key(ZTMP4, key, 12 * 16, xmm29);\n-  lastroundEncode(ZTMP4, ZTMP5, ZTMP6, ZTMP7, ZTMP8);\n-  __ jmp(STORE_CT);\n-\n-  __ bind(AES_256);\n-  ev_load_key(ZTMP4, key, 12 * 16, xmm29);\n-  roundEncode(ZTMP4, ZTMP5, ZTMP6, ZTMP7, ZTMP8);\n-  ev_load_key(ZTMP4, key, 13 * 16, xmm29);\n-  roundEncode(ZTMP4, ZTMP5, ZTMP6, ZTMP7, ZTMP8);\n-  ev_load_key(ZTMP4, key, 14 * 16, xmm29);\n-  \/\/ Last AES round\n-  lastroundEncode(ZTMP4, ZTMP5, ZTMP6, ZTMP7, ZTMP8);\n-\n-  __ bind(STORE_CT);\n-  \/\/ Xor the encrypted key with PT to obtain CT\n-  xorBeforeStore(ZTMP5, ZTMP6, ZTMP7, ZTMP8, ZTMP0, ZTMP1, ZTMP2, ZTMP3);\n-  storeData(out, pos, ZTMP5, ZTMP6, ZTMP7, ZTMP8);\n-  \/\/ 16 blocks encryption completed\n-  __ addl(pos, 256);\n-  __ cmpl(pos, 512);\n-  __ jcc(Assembler::aboveEqual, GHASH_AES_PARALLEL);\n-  __ vpaddd(ZTMP5, CTR_BLOCKx, COUNTER_INC_MASK, Assembler::AVX_512bit);\n-  __ vpaddd(ZTMP6, ZTMP5, COUNTER_INC_MASK, Assembler::AVX_512bit);\n-  __ vpaddd(ZTMP7, ZTMP6, COUNTER_INC_MASK, Assembler::AVX_512bit);\n-  __ vpaddd(ZTMP8, ZTMP7, COUNTER_INC_MASK, Assembler::AVX_512bit);\n-  __ jmp(AES_32_BLOCKS);\n-\n-  __ bind(GHASH_AES_PARALLEL);\n-  \/\/ Ghash16_encrypt16_parallel takes place in the order with three reduction values:\n-  \/\/ 1) First time -> cipher xor input ghash\n-  \/\/ 2) No reduction -> accumulate multiplication values\n-  \/\/ 3) Final reduction post 48 blocks -> new ghash value is computed for the next round\n-  \/\/ Reduction value = first time\n-  ghash16_encrypt16_parallel(key, avx512_subkeyHtbl, CTR_BLOCKx, AAD_HASHx, in, out, ct, pos, true, xmm24, true, rounds, ghash_pos, false, index, COUNTER_INC_MASK);\n-  __ addl(pos, 256);\n-  __ addl(ghash_pos, 256);\n-  index += 4;\n-\n-  \/\/ At this point we have processed 768 bytes of AES and 256 bytes of GHASH.\n-  \/\/ If the remaining length is less than 768, process remaining 512 bytes of ghash in GHASH_LAST_32 code\n-  __ subl(len, 768);\n-  __ cmpl(len, 768);\n-  __ jcc(Assembler::less, GHASH_LAST_32);\n-\n-  \/\/ AES 16 blocks and GHASH 16 blocks in parallel\n-  \/\/ For multiples of 48 blocks we will do ghash16_encrypt16 interleaved multiple times\n-  \/\/ Reduction value = no reduction means that the carryless multiplication values are accumulated for further calculations\n-  \/\/ Each call uses 4 subkeyHtbl values, so increment the index by 4.\n-  __ bind(GHASH_16_AES_16);\n-  \/\/ Reduction value = no reduction\n-  ghash16_encrypt16_parallel(key, avx512_subkeyHtbl, CTR_BLOCKx, AAD_HASHx, in, out, ct, pos, false, xmm24, false, rounds, ghash_pos, false, index, COUNTER_INC_MASK);\n-  __ addl(pos, 256);\n-  __ addl(ghash_pos, 256);\n-  index += 4;\n-  \/\/ Reduction value = final reduction means that the accumulated values have to be reduced as we have completed 48 blocks of ghash\n-  ghash16_encrypt16_parallel(key, avx512_subkeyHtbl, CTR_BLOCKx, AAD_HASHx, in, out, ct, pos, false, xmm24, false, rounds, ghash_pos, true, index, COUNTER_INC_MASK);\n-  __ addl(pos, 256);\n-  __ addl(ghash_pos, 256);\n-  \/\/ Calculated ghash value needs to be __ moved to AAD_HASHX so that we can restart the ghash16-aes16 pipeline\n-  __ movdqu(AAD_HASHx, ZTMP5);\n-  index = 0; \/\/ Reset subkeyHtbl index\n-\n-  \/\/ Restart the pipeline\n-  \/\/ Reduction value = first time\n-  ghash16_encrypt16_parallel(key, avx512_subkeyHtbl, CTR_BLOCKx, AAD_HASHx, in, out, ct, pos, true, xmm24, true, rounds, ghash_pos, false, index, COUNTER_INC_MASK);\n-  __ addl(pos, 256);\n-  __ addl(ghash_pos, 256);\n-  index += 4;\n-\n-  __ subl(len, 768);\n-  __ cmpl(len, 768);\n-  __ jcc(Assembler::greaterEqual, GHASH_16_AES_16);\n-\n-  \/\/ GHASH last 32 blocks processed here\n-  \/\/ GHASH products accumulated in ZMM27, ZMM25 and ZMM26 during GHASH16-AES16 operation is used\n-  __ bind(GHASH_LAST_32);\n-  \/\/ Use rbx as a pointer to the htbl; For last 32 blocks of GHASH, use key# 4-11 entry in subkeyHtbl\n-  __ movl(rbx, 256);\n-  \/\/ Load cipher blocks\n-  __ evmovdquq(ZTMP13, Address(ct, ghash_pos, Address::times_1, 0 * 64), Assembler::AVX_512bit);\n-  __ evmovdquq(ZTMP14, Address(ct, ghash_pos, Address::times_1, 1 * 64), Assembler::AVX_512bit);\n-  __ vpshufb(ZTMP13, ZTMP13, xmm24, Assembler::AVX_512bit);\n-  __ vpshufb(ZTMP14, ZTMP14, xmm24, Assembler::AVX_512bit);\n-  \/\/ Load ghash keys\n-  __ evmovdquq(ZTMP15, Address(avx512_subkeyHtbl, rbx, Address::times_1, 0 * 64), Assembler::AVX_512bit);\n-  __ evmovdquq(ZTMP16, Address(avx512_subkeyHtbl, rbx, Address::times_1, 1 * 64), Assembler::AVX_512bit);\n-\n-  \/\/ Ghash blocks 0 - 3\n-  carrylessMultiply(ZTMP2, ZTMP3, ZTMP4, ZTMP1, ZTMP13, ZTMP15);\n-  \/\/ Ghash blocks 4 - 7\n-  carrylessMultiply(ZTMP6, ZTMP7, ZTMP8, ZTMP5, ZTMP14, ZTMP16);\n-\n-  __ vpternlogq(ZTMP1, 0x96, ZTMP5, xmm27, Assembler::AVX_512bit); \/\/ ZTMP1 = ZTMP1 + ZTMP5 + zmm27\n-  __ vpternlogq(ZTMP2, 0x96, ZTMP6, xmm26, Assembler::AVX_512bit); \/\/ ZTMP2 = ZTMP2 + ZTMP6 + zmm26\n-  __ vpternlogq(ZTMP3, 0x96, ZTMP7, xmm25, Assembler::AVX_512bit); \/\/ ZTMP3 = ZTMP3 + ZTMP7 + zmm25\n-  __ evpxorq(ZTMP4, ZTMP4, ZTMP8, Assembler::AVX_512bit);          \/\/ ZTMP4 = ZTMP4 + ZTMP8\n-\n-  __ addl(ghash_pos, 128);\n-  __ addl(rbx, 128);\n-\n-  \/\/ Ghash remaining blocks\n-  __ bind(LOOP);\n-  __ cmpl(ghash_pos, pos);\n-  __ jcc(Assembler::aboveEqual, ACCUMULATE);\n-  \/\/ Load next cipher blocks and corresponding ghash keys\n-  __ evmovdquq(ZTMP13, Address(ct, ghash_pos, Address::times_1, 0 * 64), Assembler::AVX_512bit);\n-  __ evmovdquq(ZTMP14, Address(ct, ghash_pos, Address::times_1, 1 * 64), Assembler::AVX_512bit);\n-  __ vpshufb(ZTMP13, ZTMP13, xmm24, Assembler::AVX_512bit);\n-  __ vpshufb(ZTMP14, ZTMP14, xmm24, Assembler::AVX_512bit);\n-  __ evmovdquq(ZTMP15, Address(avx512_subkeyHtbl, rbx, Address::times_1, 0 * 64), Assembler::AVX_512bit);\n-  __ evmovdquq(ZTMP16, Address(avx512_subkeyHtbl, rbx, Address::times_1, 1 * 64), Assembler::AVX_512bit);\n-\n-  \/\/ ghash blocks 0 - 3\n-  carrylessMultiply(ZTMP6, ZTMP7, ZTMP8, ZTMP5, ZTMP13, ZTMP15);\n-\n-  \/\/ ghash blocks 4 - 7\n-  carrylessMultiply(ZTMP10, ZTMP11, ZTMP12, ZTMP9, ZTMP14, ZTMP16);\n-\n-  \/\/ update sums\n-  \/\/ ZTMP1 = ZTMP1 + ZTMP5 + ZTMP9\n-  \/\/ ZTMP2 = ZTMP2 + ZTMP6 + ZTMP10\n-  \/\/ ZTMP3 = ZTMP3 + ZTMP7 xor ZTMP11\n-  \/\/ ZTMP4 = ZTMP4 + ZTMP8 xor ZTMP12\n-  xorGHASH(ZTMP1, ZTMP2, ZTMP3, ZTMP4, ZTMP5, ZTMP9, ZTMP6, ZTMP10, ZTMP7, ZTMP11, ZTMP8, ZTMP12);\n-  __ addl(ghash_pos, 128);\n-  __ addl(rbx, 128);\n-  __ jmp(LOOP);\n@@ -3266,19 +3442,68 @@\n-  \/\/ Integrate ZTMP3\/ZTMP4 into ZTMP1 and ZTMP2\n-  __ bind(ACCUMULATE);\n-  __ evpxorq(ZTMP3, ZTMP3, ZTMP4, Assembler::AVX_512bit);\n-  __ vpsrldq(ZTMP7, ZTMP3, 8, Assembler::AVX_512bit);\n-  __ vpslldq(ZTMP8, ZTMP3, 8, Assembler::AVX_512bit);\n-  __ evpxorq(ZTMP1, ZTMP1, ZTMP7, Assembler::AVX_512bit);\n-  __ evpxorq(ZTMP2, ZTMP2, ZTMP8, Assembler::AVX_512bit);\n-\n-  \/\/ Add ZTMP1 and ZTMP2 128 - bit words horizontally\n-  vhpxori4x128(ZTMP1, ZTMP11);\n-  vhpxori4x128(ZTMP2, ZTMP12);\n-  \/\/ Load reduction polynomial and compute final reduction\n-  __ evmovdquq(ZTMP15, ExternalAddress(ghash_polynomial_reduction_addr()), Assembler::AVX_512bit, rbx \/*rscratch*\/);\n-  vclmul_reduce(AAD_HASHx, ZTMP15, ZTMP1, ZTMP2, ZTMP3, ZTMP4);\n-\n-  \/\/ Pre-increment counter for next operation\n-  __ vpaddd(CTR_BLOCKx, CTR_BLOCKx, xmm18, Assembler::AVX_128bit);\n-  \/\/ Shuffle counter and save the updated value\n-  __ vpshufb(CTR_BLOCKx, CTR_BLOCKx, xmm24, Assembler::AVX_512bit);\n+  __ movdl(CTR_CHECK, CTR_BLOCKx);\n+  __ andl(CTR_CHECK, 255);\n+\n+  \/\/ Reshuffle counter\n+  __ vpshufb(CTR_BLOCKx, CTR_BLOCKx, SHUF_MASK, Assembler::AVX_512bit);\n+\n+  initial_blocks_16_avx512(in, out, ct, pos, key, avx512_subkeyHtbl, CTR_CHECK, rounds, CTR_BLOCKx, AAD_HASHx,  ADDBE_4x4, ADDBE_1234, ADD_1234, SHUF_MASK, stack_offset);\n+  __ addl(pos, 16 * 16);\n+  __ cmpl(len, 32 * 16);\n+  __ jcc(Assembler::below, MESG_BELOW_32_BLKS);\n+\n+  initial_blocks_16_avx512(in, out, ct, pos, key, avx512_subkeyHtbl, CTR_CHECK, rounds, CTR_BLOCKx, AAD_HASHx, ADDBE_4x4, ADDBE_1234, ADD_1234, SHUF_MASK, stack_offset + 16);\n+  __ addl(pos, 16 * 16);\n+  __ subl(len, 32 * 16);\n+\n+  __ cmpl(len, 32 * 16);\n+  __ jcc(Assembler::below, NO_BIG_BLKS);\n+\n+  __ bind(ENCRYPT_BIG_BLKS_NO_HXOR);\n+  __ cmpl(len, 2 * 32 * 16);\n+  __ jcc(Assembler::below, ENCRYPT_BIG_NBLKS);\n+  ghash16_encrypt_parallel16_avx512(in, out, ct, pos, avx512_subkeyHtbl, CTR_CHECK, rounds, key, CTR_BLOCKx, AAD_HASHx, ADDBE_4x4, ADDBE_1234, ADD_1234, SHUF_MASK,\n+                                    true, true, false, false, false, ghashin_offset, aesout_offset, HashKey_32);\n+  __ addl(pos, 16 * 16);\n+\n+  ghash16_encrypt_parallel16_avx512(in, out, ct, pos, avx512_subkeyHtbl, CTR_CHECK, rounds, key, CTR_BLOCKx, AAD_HASHx, ADDBE_4x4, ADDBE_1234, ADD_1234, SHUF_MASK,\n+                                    true, false, true, false, true, ghashin_offset + 16, aesout_offset + 16, HashKey_16);\n+  __ evmovdquq(AAD_HASHx, ZTMP4, Assembler::AVX_512bit);\n+  __ addl(pos, 16 * 16);\n+  __ subl(len, 32 * 16);\n+  __ jmp(ENCRYPT_BIG_BLKS_NO_HXOR);\n+\n+  __ bind(ENCRYPT_BIG_NBLKS);\n+  ghash16_encrypt_parallel16_avx512(in, out, ct, pos, avx512_subkeyHtbl, CTR_CHECK, rounds, key, CTR_BLOCKx, AAD_HASHx, ADDBE_4x4, ADDBE_1234, ADD_1234, SHUF_MASK,\n+                                    false, true, false, false, false, ghashin_offset, aesout_offset, HashKey_32);\n+  __ addl(pos, 16 * 16);\n+  ghash16_encrypt_parallel16_avx512(in, out, ct, pos, avx512_subkeyHtbl, CTR_CHECK, rounds, key, CTR_BLOCKx, AAD_HASHx, ADDBE_4x4, ADDBE_1234, ADD_1234, SHUF_MASK,\n+                                    false, false, true, true, true, ghashin_offset + 16, aesout_offset + 16, HashKey_16);\n+\n+  __ movdqu(AAD_HASHx, ZTMP4);\n+  __ addl(pos, 16 * 16);\n+  __ subl(len, 32 * 16);\n+\n+  __ bind(NO_BIG_BLKS);\n+  __ cmpl(len, 16 * 16);\n+  __ jcc(Assembler::aboveEqual, ENCRYPT_16_BLKS);\n+\n+  __ bind(ENCRYPT_N_GHASH_32_N_BLKS);\n+  ghash16_avx512(true, false, false, false, true, in, pos, avx512_subkeyHtbl, AAD_HASHx, SHUF_MASK, stack_offset, 0, 0, HashKey_32);\n+  gcm_enc_dec_last_avx512(len, in, pos, AAD_HASHx, SHUF_MASK, avx512_subkeyHtbl, ghashin_offset + 16, HashKey_16, false, true);\n+  __ jmp(GHASH_DONE);\n+\n+  __ bind(ENCRYPT_16_BLKS);\n+  ghash16_encrypt_parallel16_avx512(in, out, ct, pos, avx512_subkeyHtbl, CTR_CHECK, rounds, key, CTR_BLOCKx, AAD_HASHx, ADDBE_4x4, ADDBE_1234, ADD_1234, SHUF_MASK,\n+                                    false, true, false, false, false, ghashin_offset, aesout_offset, HashKey_32);\n+\n+  ghash16_avx512(false, true, false, false, true, in, pos, avx512_subkeyHtbl, AAD_HASHx, SHUF_MASK, stack_offset, 16 * 16, 0, HashKey_16);\n+\n+  __ bind(MESG_BELOW_32_BLKS);\n+  __ subl(len, 16 * 16);\n+  __ addl(pos, 16 * 16);\n+  gcm_enc_dec_last_avx512(len, in, pos, AAD_HASHx, SHUF_MASK, avx512_subkeyHtbl, ghashin_offset, HashKey_16, true, true);\n+\n+  __ bind(GHASH_DONE);\n+  \/\/Pre-increment counter for next operation, make sure that counter value is incremented on the LSB\n+  __ vpshufb(CTR_BLOCKx, CTR_BLOCKx, SHUF_MASK, Assembler::AVX_128bit);\n+  __ vpaddd(CTR_BLOCKx, CTR_BLOCKx, ADD_1234, Assembler::AVX_128bit);\n+  __ vpshufb(CTR_BLOCKx, CTR_BLOCKx, SHUF_MASK, Assembler::AVX_128bit);\n@@ -3286,1 +3511,1 @@\n-  \/\/ Load ghash lswap mask\n+  \/\/Load ghash lswap mask\n@@ -3288,1 +3513,1 @@\n-  \/\/ Shuffle ghash using lbswap_mask and store it\n+  \/\/Shuffle ghash using lbswap_mask and store it\n@@ -3291,1 +3516,0 @@\n-  __ jmp(ENC_DEC_DONE);\n@@ -3293,2 +3517,6 @@\n-  __ bind(GENERATE_HTBL_48_BLKS);\n-  generateHtbl_48_block_zmm(subkeyHtbl, avx512_subkeyHtbl, rbx \/*rscratch*\/);\n+  \/\/Zero out sensitive data\n+  __ evpxorq(ZTMP21, ZTMP21, ZTMP21, Assembler::AVX_512bit);\n+  __ evpxorq(ZTMP0, ZTMP0, ZTMP0, Assembler::AVX_512bit);\n+  __ evpxorq(ZTMP1, ZTMP1, ZTMP1, Assembler::AVX_512bit);\n+  __ evpxorq(ZTMP2, ZTMP2, ZTMP2, Assembler::AVX_512bit);\n+  __ evpxorq(ZTMP3, ZTMP3, ZTMP3, Assembler::AVX_512bit);\n@@ -3297,1 +3525,0 @@\n-  __ movq(rax, pos);\n","filename":"src\/hotspot\/cpu\/x86\/stubGenerator_x86_64_aes.cpp","additions":723,"deletions":496,"binary":false,"changes":1219,"status":"modified"},{"patch":"@@ -2,1 +2,1 @@\n-* Copyright (c) 2019, 2021, Intel Corporation. All rights reserved.\n+* Copyright (c) 2019, 2024, Intel Corporation. All rights reserved.\n@@ -60,1 +60,4 @@\n-    0x0000000000000001UL, 0xC200000000000000UL,\n+    0x0000000000000001ULL, 0xC200000000000000ULL,\n+    0x0000000000000001ULL, 0xC200000000000000ULL,\n+    0x0000000000000001ULL, 0xC200000000000000ULL,\n+    0x0000000000000001ULL, 0xC200000000000000ULL\n","filename":"src\/hotspot\/cpu\/x86\/stubGenerator_x86_64_ghash.cpp","additions":5,"deletions":2,"binary":false,"changes":7,"status":"modified"},{"patch":"@@ -75,1 +75,1 @@\n-    private static final int PARALLEL_LEN = 7680;\n+    private static final int PARALLEL_LEN = 512;\n","filename":"src\/java.base\/share\/classes\/com\/sun\/crypto\/provider\/GaloisCounterMode.java","additions":1,"deletions":1,"binary":false,"changes":2,"status":"modified"},{"patch":"@@ -38,1 +38,1 @@\n-    @Param({\"128\"})\n+    @Param({\"128\", \"192\", \"256\"})\n","filename":"test\/micro\/org\/openjdk\/bench\/javax\/crypto\/full\/AESGCMBench.java","additions":1,"deletions":1,"binary":false,"changes":2,"status":"modified"},{"patch":"@@ -48,1 +48,1 @@\n-    @Param({\"1024\", \"1500\", \"4096\", \"16384\"})\n+    @Param({\"128\", \"256\", \"512\", \"1024\", \"1500\", \"4096\", \"16384\"})\n","filename":"test\/micro\/org\/openjdk\/bench\/javax\/crypto\/full\/BenchBase.java","additions":1,"deletions":1,"binary":false,"changes":2,"status":"modified"}]}