{"files":[{"patch":"@@ -156,1 +156,0 @@\n-  _num_dump_regions_used(0),\n@@ -164,0 +163,1 @@\n+  _pz_region(\"pz\", MAX_SHARED_DELTA), \/\/ protection zone\n@@ -326,3 +326,8 @@\n-  _current_dump_region = &_rw_region;\n-  _num_dump_regions_used = 1;\n-  _current_dump_region->init(&_shared_rs, &_shared_vs);\n+\n+  if (CDSConfig::is_dumping_static_archive()) {\n+    _current_dump_region = &_pz_region;\n+    _current_dump_region->init(&_shared_rs, &_shared_vs);\n+  } else {\n+    _current_dump_region = &_rw_region;\n+    _current_dump_region->init(&_shared_rs, &_shared_vs);\n+  }\n@@ -366,1 +371,0 @@\n-#ifdef _LP64\n@@ -368,3 +372,4 @@\n-    \/\/ The region that will be located at the bottom of the encoding range at runtime shall have\n-    \/\/ space for a protection zone.\n-    MetaspaceShared::allocate_and_mark_protection_zone(rw_region());\n+    \/\/ We don't want any valid object to be at the very bottom of the archive.\n+    \/\/ See ArchivePtrMarker::mark_pointer().\n+    _pz_region.allocate(MetaspaceShared::protection_zone_size());\n+    start_dump_region(&_rw_region);\n@@ -372,1 +377,0 @@\n-#endif \/\/ _LP64\n@@ -550,1 +554,0 @@\n-  _num_dump_regions_used ++;\n","filename":"src\/hotspot\/share\/cds\/archiveBuilder.cpp","additions":13,"deletions":10,"binary":false,"changes":23,"status":"modified"},{"patch":"@@ -2,1 +2,1 @@\n- * Copyright (c) 2020, 2024, Oracle and\/or its affiliates. All rights reserved.\n+ * Copyright (c) 2020, 2025, Oracle and\/or its affiliates. All rights reserved.\n@@ -99,1 +99,0 @@\n-  int _num_dump_regions_used;\n@@ -213,0 +212,5 @@\n+  \/\/ The \"pz\" region is used only during static dumps to reserve an unused space between SharedBaseAddress and\n+  \/\/ the bottom of the rw region. During runtime, this space will be filled with a reserved area that disallows\n+  \/\/ read\/write\/exec, so we can track for bad CompressedKlassPointers encoding.\n+  DumpRegion _pz_region;\n+\n@@ -273,3 +277,0 @@\n-\n-  static const int _total_dump_regions = 2;\n-\n@@ -370,0 +371,1 @@\n+  DumpRegion* pz_region() { return &_pz_region; }\n","filename":"src\/hotspot\/share\/cds\/archiveBuilder.hpp","additions":7,"deletions":5,"binary":false,"changes":12,"status":"modified"},{"patch":"@@ -76,2 +76,3 @@\n-  address* rw_bottom = (address*)ArchiveBuilder::current()->rw_region()->base();\n-  address* ro_bottom = (address*)ArchiveBuilder::current()->ro_region()->base();\n+  address* buff_bottom = (address*)ArchiveBuilder::current()->buffer_bottom();\n+  address* rw_bottom   = (address*)ArchiveBuilder::current()->rw_region()->base();\n+  address* ro_bottom   = (address*)ArchiveBuilder::current()->ro_region()->base();\n@@ -79,2 +80,3 @@\n-  _rw_ptrmap = rw_ptrmap;\n-  _ro_ptrmap = ro_ptrmap;\n+  \/\/ The bit in _ptrmap that cover the very first word in the rw\/ro regions.\n+  size_t rw_start = rw_bottom - buff_bottom;\n+  size_t ro_start = ro_bottom - buff_bottom;\n@@ -82,0 +84,3 @@\n+  \/\/ The number of bits used by the rw\/ro ptrmaps. We might have lots of zero\n+  \/\/ bits at the bottom and top of rrw\/ro ptrmaps, but these zeros will be\n+  \/\/ removed by FileMapInfo::write_bitmap_region().\n@@ -84,14 +89,12 @@\n-  \/\/ ro_start is the first bit in _ptrmap that covers the pointer that would sit at ro_bottom.\n-  \/\/ E.g., if rw_bottom = (address*)100\n-  \/\/          ro_bottom = (address*)116\n-  \/\/       then for 64-bit platform:\n-  \/\/          ro_start = ro_bottom - rw_bottom = (116 - 100) \/ sizeof(address) = 2;\n-  size_t ro_start = ro_bottom - rw_bottom;\n-\n-  \/\/ Note: ptrmap is big enough only to cover the last pointer in ro_region.\n-  \/\/ See ArchivePtrMarker::compact()\n-  _rw_ptrmap->initialize(rw_size);\n-  _ro_ptrmap->initialize(_ptrmap->size() - ro_start);\n-\n-  for (size_t rw_bit = 0; rw_bit < _rw_ptrmap->size(); rw_bit++) {\n-    _rw_ptrmap->at_put(rw_bit, _ptrmap->at(rw_bit));\n+\n+  \/\/ The last (exclusive) bit in _ptrmap that covers the rw\/ro regions.\n+  \/\/ Note: _ptrmap is dynamically expanded only when an actual pointer is written, so\n+  \/\/ it may not be as large as we want.\n+  size_t rw_end = MIN2<size_t>(rw_start + rw_size, _ptrmap->size());\n+  size_t ro_end = MIN2<size_t>(ro_start + ro_size, _ptrmap->size());\n+\n+  rw_ptrmap->initialize(rw_size);\n+  ro_ptrmap->initialize(ro_size);\n+\n+  for (size_t rw_bit = rw_start; rw_bit < rw_end; rw_bit++) {\n+    rw_ptrmap->at_put(rw_bit - rw_start, _ptrmap->at(rw_bit));\n@@ -100,2 +103,2 @@\n-  for(size_t ro_bit = ro_start; ro_bit < _ptrmap->size(); ro_bit++) {\n-    _ro_ptrmap->at_put(ro_bit-ro_start, _ptrmap->at(ro_bit));\n+  for(size_t ro_bit = ro_start; ro_bit < ro_end; ro_bit++) {\n+    ro_ptrmap->at_put(ro_bit - ro_start, _ptrmap->at(ro_bit));\n@@ -103,1 +106,3 @@\n-  assert(_ptrmap->size() - ro_start == _ro_ptrmap->size(), \"must be\");\n+\n+  _rw_ptrmap = rw_ptrmap;\n+  _ro_ptrmap = ro_ptrmap;\n","filename":"src\/hotspot\/share\/cds\/archiveUtils.cpp","additions":26,"deletions":21,"binary":false,"changes":47,"status":"modified"},{"patch":"@@ -170,1 +170,0 @@\n-    assert(_num_dump_regions_used == _total_dump_regions, \"must be\");\n","filename":"src\/hotspot\/share\/cds\/dynamicArchive.cpp","additions":0,"deletions":1,"binary":false,"changes":1,"status":"modified"},{"patch":"@@ -540,1 +540,1 @@\n-  char* mapped_base()    const { return first_core_region()->mapped_base(); }\n+  char* mapped_base()    const { return header()->mapped_base_address();    }\n","filename":"src\/hotspot\/share\/cds\/filemap.hpp","additions":1,"deletions":1,"binary":false,"changes":2,"status":"modified"},{"patch":"@@ -148,0 +148,4 @@\n+size_t MetaspaceShared::protection_zone_size() {\n+  return os::cds_core_region_alignment();\n+}\n+\n@@ -1237,0 +1241,1 @@\n+  size_t prot_zone_size = 0;\n@@ -1248,0 +1253,9 @@\n+    if (Metaspace::using_class_space()) {\n+      prot_zone_size = protection_zone_size();\n+#ifdef ASSERT\n+      os::commit_memory(mapped_base_address, prot_zone_size, false);\n+      *(mapped_base_address) = 'P';\n+      *(mapped_base_address + prot_zone_size - 1) = 'P';\n+#endif\n+    }\n+\n@@ -1253,0 +1267,3 @@\n+      assert(archive_space_rs.base() == mapped_base_address &&\n+          archive_space_rs.size() > protection_zone_size(),\n+          \"Archive space must lead and include the protection zone\");\n@@ -1255,1 +1272,1 @@\n-      assert(class_space_rs.is_reserved(),\n+      assert(class_space_rs.is_reserved() && class_space_rs.size() > 0,\n@@ -1268,2 +1285,3 @@\n-    log_info(cds)(\"Reserved archive_space_rs [\" INTPTR_FORMAT \" - \" INTPTR_FORMAT \"] (%zu) bytes\",\n-                   p2i(archive_space_rs.base()), p2i(archive_space_rs.end()), archive_space_rs.size());\n+    log_info(cds)(\"Reserved archive_space_rs [\" INTPTR_FORMAT \" - \" INTPTR_FORMAT \"] (%zu) bytes%s\",\n+                   p2i(archive_space_rs.base()), p2i(archive_space_rs.end()), archive_space_rs.size(),\n+                   (prot_zone_size > 0 ? \" (includes protection zone)\" : \"\"));\n@@ -1341,34 +1359,29 @@\n-        if (Metaspace::using_class_space()) {\n-          \/\/ Set up ccs in metaspace.\n-          Metaspace::initialize_class_space(class_space_rs);\n-\n-          \/\/ Set up compressed Klass pointer encoding: the encoding range must\n-          \/\/  cover both archive and class space.\n-          address cds_base = (address)static_mapinfo->mapped_base();\n-          address ccs_end = (address)class_space_rs.end();\n-          assert(ccs_end > cds_base, \"Sanity check\");\n-          if (INCLUDE_CDS_JAVA_HEAP || UseCompactObjectHeaders) {\n-            \/\/ The CDS archive may contain narrow Klass IDs that were precomputed at archive generation time:\n-            \/\/ - every archived java object header (only if INCLUDE_CDS_JAVA_HEAP)\n-            \/\/ - every archived Klass' prototype   (only if +UseCompactObjectHeaders)\n-            \/\/\n-            \/\/ In order for those IDs to still be valid, we need to dictate base and shift: base should be the\n-            \/\/ mapping start, shift the shift used at archive generation time.\n-            address precomputed_narrow_klass_base = cds_base;\n-            const int precomputed_narrow_klass_shift = ArchiveBuilder::precomputed_narrow_klass_shift();\n-            CompressedKlassPointers::initialize_for_given_encoding(\n-              cds_base, ccs_end - cds_base, \/\/ Klass range\n-              precomputed_narrow_klass_base, precomputed_narrow_klass_shift \/\/ precomputed encoding, see ArchiveBuilder\n-            );\n-          } else {\n-            \/\/ Let JVM freely chose encoding base and shift\n-            CompressedKlassPointers::initialize (\n-              cds_base, ccs_end - cds_base \/\/ Klass range\n-              );\n-          }\n-\n-          \/\/ After narrowKlass encoding scheme is decided: if encoding base points to archive start (can happen\n-          \/\/ for both cases above), establish protection zone.\n-          if (CompressedKlassPointers::base() == cds_base) {\n-            MetaspaceShared::check_and_establish_protection_zone(cds_base);\n-          }\n+    if (Metaspace::using_class_space()) {\n+      assert(*(mapped_base_address) == 'P' &&\n+             *(mapped_base_address + prot_zone_size - 1) == 'P',\n+          \"Protection zone was overwritten?\");\n+\n+      \/\/ Set up ccs in metaspace.\n+      Metaspace::initialize_class_space(class_space_rs);\n+\n+      \/\/ Set up compressed Klass pointer encoding: the encoding range must\n+      \/\/  cover both archive and class space.\n+      const address encoding_base = (address)mapped_base_address;\n+      const address klass_range_start = encoding_base + prot_zone_size;\n+      const size_t klass_range_size = (address)class_space_rs.end() - klass_range_start;\n+      if (INCLUDE_CDS_JAVA_HEAP || UseCompactObjectHeaders) {\n+        \/\/ The CDS archive may contain narrow Klass IDs that were precomputed at archive generation time:\n+        \/\/ - every archived java object header (only if INCLUDE_CDS_JAVA_HEAP)\n+        \/\/ - every archived Klass' prototype   (only if +UseCompactObjectHeaders)\n+        \/\/\n+        \/\/ In order for those IDs to still be valid, we need to dictate base and shift: base should be the\n+        \/\/ mapping start (including protection zone), shift the shift used at archive generation time.\n+        CompressedKlassPointers::initialize_for_given_encoding(\n+          klass_range_start, klass_range_size,\n+          encoding_base, ArchiveBuilder::precomputed_narrow_klass_shift() \/\/ precomputed encoding, see ArchiveBuilder\n+        );\n+      } else {\n+        \/\/ Let JVM freely chose encoding base and shift\n+        CompressedKlassPointers::initialize(klass_range_start, klass_range_size);\n+      }\n+      CompressedKlassPointers::establish_protection_zone(encoding_base, prot_zone_size);\n@@ -1376,4 +1389,4 @@\n-          \/\/ map_or_load_heap_region() compares the current narrow oop and klass encodings\n-          \/\/ with the archived ones, so it must be done after all encodings are determined.\n-          static_mapinfo->map_or_load_heap_region();\n-        }\n+      \/\/ map_or_load_heap_region() compares the current narrow oop and klass encodings\n+      \/\/ with the archived ones, so it must be done after all encodings are determined.\n+      static_mapinfo->map_or_load_heap_region();\n+    }\n@@ -1392,45 +1405,0 @@\n-#ifdef _LP64\n-\n-\/\/ Protection zone handling.\n-\/\/\n-\/\/ A Klass structure must never be located at the encoding base since that would encode to an\n-\/\/ invalid nKlass of zero. At runtime, we set the encoding base to the start of the mapped\n-\/\/ archive. In order to catch accidental accesses via a zero nKlass, we will establis a no-access\n-\/\/ zone there, similar to how the heap does it. Space for that page must be prepared when dumping\n-\/\/ the archive.\n-static constexpr uint64_t protzone_tag       = 0x50524F545A4F4E45ULL; \/\/ \"PROTZONE\"\n-static constexpr uint64_t protzone_tag_start = 0x2D3E2D3E50524F54ULL; \/\/ \"->->PROT\"\n-static constexpr uint64_t protzone_tag_end   = 0x50524F543C2D3C2DULL; \/\/ \"PROT<-<-\"\n-\n-\/\/ Dumptime\n-void MetaspaceShared::allocate_and_mark_protection_zone(DumpRegion* region) {\n-\n-  assert(region->used() == 0, \"Should not have allocations yet\");\n-\n-  \/\/ Note: not page size but core region alignment! Page size can differ at runtime.\n-  const size_t protzone_size = MetaspaceShared::core_region_alignment();\n-  uint64_t* const protzone = (uint64_t*) region->allocate(protzone_size);\n-  const size_t len = protzone_size\/sizeof(uint64_t);\n-  protzone[0] = protzone_tag_start;\n-  protzone[len - 1] = protzone_tag_end;\n-\n-  for (size_t i = 1; i < len - 1; i++) {\n-    protzone[i] = protzone_tag;\n-  }\n-}\n-\n-\/\/ Runtime\n-void MetaspaceShared::check_and_establish_protection_zone(address addr) {\n-  const size_t protzone_size = MetaspaceShared::core_region_alignment();\n-  const uint64_t* const protzone = (const uint64_t*) addr;\n-  const size_t len = protzone_size\/sizeof(uint64_t);\n-  guarantee(protzone[0] == protzone_tag_start, \"Corrupted CDS protection zone\");\n-  guarantee(protzone[len - 1] == protzone_tag_end, \"Corrupted CDS protection zone\");\n-#ifdef ASSERT\n-  for (size_t i = 1; i < len - 1; i++) {\n-    assert(protzone[i] == protzone_tag, \"Corrupted CDS protection zone\");\n-  }\n-#endif\n-  CompressedKlassPointers::establish_protection_zone((address)protzone, protzone_size);\n-}\n-#endif \/\/ _LP64\n@@ -1506,1 +1474,0 @@\n-  assert(static_mapinfo->mapping_base_offset() == 0, \"Must be\");\n","filename":"src\/hotspot\/share\/cds\/metaspaceShared.cpp","additions":54,"deletions":87,"binary":false,"changes":141,"status":"modified"},{"patch":"@@ -37,1 +37,0 @@\n-class DumpRegion;\n@@ -143,0 +142,1 @@\n+  static size_t protection_zone_size();\n@@ -167,8 +167,0 @@\n-  \/\/ Given a dump region (that should not yet have allocations), allocate and prepare a protection\n-  \/\/ at the start of the region.\n-  LP64_ONLY( static void allocate_and_mark_protection_zone(DumpRegion* region); )\n-\n-  \/\/ Given an address that should point to a mapped protection zone, check markers, then protect\n-  \/\/ the zone. Will return false if markers don't match up (misshapen\/tempered archive)\n-  LP64_ONLY( static void check_and_establish_protection_zone(address address) );\n-\n","filename":"src\/hotspot\/share\/cds\/metaspaceShared.hpp","additions":1,"deletions":9,"binary":false,"changes":10,"status":"modified"},{"patch":"@@ -4,1 +4,1 @@\n- * Copyright (c) 2023, 2024, Red Hat, Inc. All rights reserved.\n+ * Copyright (c) 2023, 2025, Red Hat, Inc. All rights reserved.\n","filename":"src\/hotspot\/share\/memory\/metaspace.cpp","additions":1,"deletions":1,"binary":false,"changes":2,"status":"modified"},{"patch":"@@ -166,5 +166,0 @@\n-  \/\/ Note: While it would be technically valid for the encoding base to precede the start of the Klass range,\n-  \/\/ we never do this here. This is used at CDS runtime to re-instate the scheme used to precompute the\n-  \/\/ narrow Klass IDs in the archive, and the requested base should point to the start of the Klass range.\n-  assert(requested_base == addr, \"Invalid requested base\");\n-\n","filename":"src\/hotspot\/share\/oops\/compressedKlass.cpp","additions":0,"deletions":5,"binary":false,"changes":5,"status":"modified"},{"patch":"@@ -2,1 +2,1 @@\n- * Copyright (c) 2019, 2024, Oracle and\/or its affiliates. All rights reserved.\n+ * Copyright (c) 2019, 2025, Oracle and\/or its affiliates. All rights reserved.\n","filename":"src\/hotspot\/share\/oops\/compressedKlass.hpp","additions":1,"deletions":1,"binary":false,"changes":2,"status":"modified"},{"patch":"@@ -2,1 +2,1 @@\n- * Copyright (c) 2017, 2024, Oracle and\/or its affiliates. All rights reserved.\n+ * Copyright (c) 2017, 2025, Oracle and\/or its affiliates. All rights reserved.\n","filename":"src\/hotspot\/share\/oops\/compressedKlass.inline.hpp","additions":1,"deletions":1,"binary":false,"changes":2,"status":"modified"},{"patch":"@@ -26,1 +26,0 @@\n-#include \"oops\/klass.hpp\"\n","filename":"test\/hotspot\/gtest\/oops\/test_compressedKlass.cpp","additions":0,"deletions":1,"binary":false,"changes":1,"status":"modified"},{"patch":"@@ -2,1 +2,1 @@\n- * Copyright (c) 2012, 2024, Oracle and\/or its affiliates. All rights reserved.\n+ * Copyright (c) 2012, 2025, Oracle and\/or its affiliates. All rights reserved.\n","filename":"test\/lib\/jdk\/test\/whitebox\/WhiteBox.java","additions":1,"deletions":1,"binary":false,"changes":2,"status":"modified"}]}